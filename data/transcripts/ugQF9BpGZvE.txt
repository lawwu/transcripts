
[00:00:00.000 --> 00:00:03.400]   staying safe in this very unusual year.
[00:00:03.400 --> 00:00:05.560]   It's very exciting to be here.
[00:00:05.560 --> 00:00:07.640]   So I'm Maitre.
[00:00:07.640 --> 00:00:10.160]   My talk today will be on Do Wide and Deep Neural Networks
[00:00:10.160 --> 00:00:11.600]   Learn the Same Things?
[00:00:11.600 --> 00:00:15.040]   This is based off of a paper of almost the same name
[00:00:15.040 --> 00:00:18.800]   with my amazing collaborators, Tao and Simon.
[00:00:18.800 --> 00:00:21.800]   And we're all at Google AI.
[00:00:21.800 --> 00:00:24.200]   So for this talk, I thought I'd actually first start off
[00:00:24.200 --> 00:00:26.560]   by giving a little bit of background and context
[00:00:26.560 --> 00:00:29.160]   as to what motivated us to look into this direction
[00:00:29.160 --> 00:00:32.560]   in the first place and some of our overarching research
[00:00:32.560 --> 00:00:35.000]   goals in pursuing this project and some
[00:00:35.000 --> 00:00:37.800]   of the other directions we've been working on.
[00:00:37.800 --> 00:00:41.000]   So with that in mind, really big step back.
[00:00:41.000 --> 00:00:44.440]   And I'll just say that, as we all know,
[00:00:44.440 --> 00:00:47.720]   over the past several years, the field of machine learning
[00:00:47.720 --> 00:00:50.080]   has been just an amazing place to be in.
[00:00:50.080 --> 00:00:52.400]   We've seen these incredible breakthroughs,
[00:00:52.400 --> 00:00:55.000]   starting all the way with speech recognition and then computer
[00:00:55.000 --> 00:00:57.240]   vision, machine translation.
[00:00:57.240 --> 00:01:01.720]   More recently, more complex tasks like pose estimation
[00:01:01.720 --> 00:01:03.480]   and dialogue.
[00:01:03.480 --> 00:01:06.320]   And this has all really been made possible
[00:01:06.320 --> 00:01:08.800]   because of the rapid iteration and advancement
[00:01:08.800 --> 00:01:11.160]   of these deep neural network models
[00:01:11.160 --> 00:01:14.080]   on these specific competition benchmarks.
[00:01:14.080 --> 00:01:17.120]   So things like ImageNet and MS COCO and computer vision,
[00:01:17.120 --> 00:01:19.160]   benchmarks in machine translation,
[00:01:19.160 --> 00:01:21.520]   the Glue, Super Glue, and SQUAD benchmarks
[00:01:21.520 --> 00:01:23.560]   in natural language understanding.
[00:01:23.560 --> 00:01:25.640]   There are a huge number of these.
[00:01:25.640 --> 00:01:27.180]   In fact, there are so many of these
[00:01:27.180 --> 00:01:29.360]   that there are all these websites that are actually
[00:01:29.360 --> 00:01:32.580]   dedicated to keeping track of all the benchmarks we have
[00:01:32.580 --> 00:01:34.720]   and how the models and algorithms we're proposing
[00:01:34.720 --> 00:01:37.080]   are slowly getting better and doing well
[00:01:37.080 --> 00:01:39.200]   at all of these tasks.
[00:01:39.200 --> 00:01:42.000]   OK, so that's the field so far.
[00:01:42.000 --> 00:01:44.880]   And what's happened is that all of this progress
[00:01:44.880 --> 00:01:47.280]   and development of these benchmarks
[00:01:47.280 --> 00:01:49.320]   has meant that, looking forward, we're
[00:01:49.320 --> 00:01:51.560]   able to push the boundaries of our field
[00:01:51.560 --> 00:01:53.940]   in brand new ways.
[00:01:53.940 --> 00:01:58.880]   So we're seeing two big different pushes, I think.
[00:01:58.880 --> 00:02:00.520]   Firstly, we're seeing the development
[00:02:00.520 --> 00:02:05.080]   of models of scale larger than we've ever had before
[00:02:05.080 --> 00:02:07.620]   and systems that are very complex in nature,
[00:02:07.620 --> 00:02:10.200]   have many learned and moving parts,
[00:02:10.200 --> 00:02:14.480]   and are capable of tackling all kinds of new tasks.
[00:02:14.480 --> 00:02:15.800]   And so that's one part.
[00:02:15.800 --> 00:02:18.120]   And then on the second side, we're
[00:02:18.120 --> 00:02:20.440]   also seeing just a huge amount of excitement
[00:02:20.440 --> 00:02:23.320]   from all of these different applications and domains
[00:02:23.320 --> 00:02:25.580]   in deploying these systems that we've
[00:02:25.580 --> 00:02:28.680]   been developing to deal with some of the key pain
[00:02:28.680 --> 00:02:30.560]   points in their domain.
[00:02:30.560 --> 00:02:33.440]   And this includes really high-stakes applications,
[00:02:33.440 --> 00:02:35.920]   things like medicine and health care.
[00:02:35.920 --> 00:02:38.920]   So summary is looking back, things are exciting.
[00:02:38.920 --> 00:02:41.000]   And looking forward, things also look
[00:02:41.000 --> 00:02:43.280]   really exciting for the field.
[00:02:43.280 --> 00:02:45.560]   But there's an important caveat here.
[00:02:45.560 --> 00:02:47.140]   And the caveat is that if we want
[00:02:47.140 --> 00:02:49.880]   to continue pushing things forward in this way,
[00:02:49.880 --> 00:02:53.400]   we want to continue developing these very complex models,
[00:02:53.400 --> 00:02:55.640]   we want to continue developing large-scale systems,
[00:02:55.640 --> 00:02:58.400]   we want to be able to safely and reliably deploy
[00:02:58.400 --> 00:03:01.600]   these systems in these other domains,
[00:03:01.600 --> 00:03:03.880]   then we have to be able to design
[00:03:03.880 --> 00:03:08.840]   very robust and high-quality models and algorithms.
[00:03:08.840 --> 00:03:11.000]   And that's where there's this gap.
[00:03:11.000 --> 00:03:13.800]   Despite all of the forward progress we've made,
[00:03:13.800 --> 00:03:16.720]   this design process for these machine learning systems
[00:03:16.720 --> 00:03:21.680]   is still incredibly complex and involved.
[00:03:21.680 --> 00:03:24.600]   So say you're faced with this task of designing
[00:03:24.600 --> 00:03:26.720]   some new machine learning system.
[00:03:26.720 --> 00:03:29.680]   Well, that process is just so complicated.
[00:03:29.680 --> 00:03:32.480]   You're faced with all of these different choices,
[00:03:32.480 --> 00:03:34.480]   ranging from the kind of model you should use,
[00:03:34.480 --> 00:03:37.440]   to questions around the data, how you should train it,
[00:03:37.440 --> 00:03:41.640]   hyperparameters, and questions around the learning algorithm.
[00:03:41.640 --> 00:03:44.320]   And the big problem facing us is that we actually
[00:03:44.320 --> 00:03:46.520]   have to pick carefully amongst these choices
[00:03:46.520 --> 00:03:50.000]   to end up with a high-quality, robust system.
[00:03:50.000 --> 00:03:52.640]   But at the same time, there's very little insight
[00:03:52.640 --> 00:03:55.880]   guiding what a good set of these choices looks like.
[00:03:55.880 --> 00:03:57.520]   So we end up with a design process
[00:03:57.520 --> 00:04:01.920]   that's pretty laborious and computationally expensive,
[00:04:01.920 --> 00:04:03.640]   because people don't know what to do,
[00:04:03.640 --> 00:04:06.560]   so they try out a whole bunch of different things.
[00:04:06.560 --> 00:04:08.000]   And so they will literally build up
[00:04:08.000 --> 00:04:10.400]   this population of neural networks,
[00:04:10.400 --> 00:04:13.240]   each one which has some specific set of design choices
[00:04:13.240 --> 00:04:14.760]   associated with it.
[00:04:14.760 --> 00:04:17.680]   And then the hope is that amongst this population,
[00:04:17.680 --> 00:04:21.760]   there's something there that's doing what they want it to.
[00:04:21.760 --> 00:04:23.360]   Now, this is the place where I think
[00:04:23.360 --> 00:04:25.800]   there's also this interesting gap between science
[00:04:25.800 --> 00:04:27.440]   and engineering.
[00:04:27.440 --> 00:04:29.680]   So on the engineering side, we have actually
[00:04:29.680 --> 00:04:34.360]   seen remarkable progress in making this entire process much
[00:04:34.360 --> 00:04:37.920]   easier, from the development of software packages
[00:04:37.920 --> 00:04:40.920]   to really help us to actually design--
[00:04:40.920 --> 00:04:44.040]   to actually write the code for these models better,
[00:04:44.040 --> 00:04:46.480]   to things like people here at Weights and Biases are doing,
[00:04:46.480 --> 00:04:48.920]   and actually developing tools to monitor and visualize
[00:04:48.920 --> 00:04:53.040]   these systems, to things that help us navigate our compute
[00:04:53.040 --> 00:04:55.640]   architecture and load.
[00:04:55.640 --> 00:04:58.400]   But the place where I see a gap and where we need to catch up
[00:04:58.400 --> 00:05:02.080]   is the science side of it.
[00:05:02.080 --> 00:05:07.440]   So in measuring how our models and deep learning systems
[00:05:07.440 --> 00:05:09.920]   are doing, we're still falling back
[00:05:09.920 --> 00:05:13.600]   to these metrics of loss and accuracy, which
[00:05:13.600 --> 00:05:16.720]   are historically inherited from this history
[00:05:16.720 --> 00:05:20.680]   of developing these systems on these competition benchmarks.
[00:05:20.680 --> 00:05:22.080]   And that's nice.
[00:05:22.080 --> 00:05:23.920]   That is a good place to start.
[00:05:23.920 --> 00:05:25.960]   But there's a limitation to how much
[00:05:25.960 --> 00:05:28.880]   these can offer us if we really care about principled design
[00:05:28.880 --> 00:05:30.280]   processes.
[00:05:30.280 --> 00:05:35.080]   So most obviously, these loss and accuracy metrics
[00:05:35.080 --> 00:05:36.920]   are pretty much concentrated on what's
[00:05:36.920 --> 00:05:39.920]   happening at the very output, the very top layer,
[00:05:39.920 --> 00:05:42.320]   of these deep learning models.
[00:05:42.320 --> 00:05:45.600]   Whereas the bulk of the computation, the parameters,
[00:05:45.600 --> 00:05:49.280]   and the learning happens in all of these hidden layers.
[00:05:49.280 --> 00:05:51.440]   And we still have to take steps.
[00:05:51.440 --> 00:05:54.400]   I mean, ideally, what we want is a suite of techniques
[00:05:54.400 --> 00:05:58.600]   that will let us understand what's going on there.
[00:05:58.600 --> 00:06:00.760]   Now, these hidden layers also pose challenges for us
[00:06:00.760 --> 00:06:01.840]   to analyze in this way.
[00:06:01.840 --> 00:06:02.800]   They're large.
[00:06:02.800 --> 00:06:04.080]   They're complex.
[00:06:04.080 --> 00:06:07.040]   They don't have any nice interpretable mapping.
[00:06:07.040 --> 00:06:11.040]   But the questions that started us off many years ago
[00:06:11.040 --> 00:06:13.560]   into this paper I'm about to present
[00:06:13.560 --> 00:06:15.840]   is just that we at least wanted to be
[00:06:15.840 --> 00:06:19.520]   able to take different members of our population
[00:06:19.520 --> 00:06:22.360]   and understand how these different design choices were
[00:06:22.360 --> 00:06:24.440]   affecting their hidden representations.
[00:06:24.440 --> 00:06:26.200]   What's similar and what's different because
[00:06:26.200 --> 00:06:27.720]   of all these design choices we made?
[00:06:27.720 --> 00:06:30.080]   What's really going on?
[00:06:30.080 --> 00:06:33.520]   And boiling this down, what we are really hoping to do
[00:06:33.520 --> 00:06:36.840]   is meaningfully compare hidden representations
[00:06:36.840 --> 00:06:39.040]   across neural networks.
[00:06:39.040 --> 00:06:41.640]   Now, this seems like a really fundamental question.
[00:06:41.640 --> 00:06:43.400]   But even this fundamental question
[00:06:43.400 --> 00:06:47.720]   has some layers of subtlety and complexity to it.
[00:06:47.720 --> 00:06:50.280]   And I don't know if I can do full justice to it in the time.
[00:06:50.280 --> 00:06:53.880]   But I'll try and give some of the key things
[00:06:53.880 --> 00:06:57.600]   to think about when doing this.
[00:06:57.600 --> 00:07:00.240]   So first of all, there's this definitional question
[00:07:00.240 --> 00:07:04.400]   of how we actually should define a hidden representation.
[00:07:04.400 --> 00:07:05.920]   Normally, we think of neural networks
[00:07:05.920 --> 00:07:07.680]   in terms of their parameters.
[00:07:07.680 --> 00:07:10.320]   But there's lots of redundancy in their parameters.
[00:07:10.320 --> 00:07:12.600]   So it turns out that a more effective approach
[00:07:12.600 --> 00:07:16.840]   is to actually think of the concrete outputs
[00:07:16.840 --> 00:07:19.480]   that a particular hidden neuron is emitting
[00:07:19.480 --> 00:07:21.400]   on some data set of interest.
[00:07:21.400 --> 00:07:24.760]   Or expanding this out to a layer,
[00:07:24.760 --> 00:07:27.400]   we can think of the representation of a layer
[00:07:27.400 --> 00:07:31.400]   as being represented by what we call an activation matrix.
[00:07:31.400 --> 00:07:33.800]   So all of the outputs of all of its neurons
[00:07:33.800 --> 00:07:35.520]   on some data set of interest.
[00:07:35.520 --> 00:07:37.800]   And this summarizes the representation
[00:07:37.800 --> 00:07:40.360]   learned by the layer.
[00:07:40.360 --> 00:07:43.280]   The other challenge in comparing representations,
[00:07:43.280 --> 00:07:46.280]   which I'll just briefly mention, is representations
[00:07:46.280 --> 00:07:48.560]   are distributed across neural networks.
[00:07:48.560 --> 00:07:50.440]   There's also no nice alignment.
[00:07:50.440 --> 00:07:52.280]   So whatever algorithm you come up with
[00:07:52.280 --> 00:07:55.560]   has to take all of those into account.
[00:07:55.560 --> 00:07:57.480]   So bearing these conditions in mind,
[00:07:57.480 --> 00:08:00.000]   in work done a few years ago now,
[00:08:00.000 --> 00:08:01.680]   we proposed some of the first algorithms
[00:08:01.680 --> 00:08:05.480]   to actually do this in a quantitative and reliable way.
[00:08:05.480 --> 00:08:08.240]   And we were able to use that to provide us
[00:08:08.240 --> 00:08:10.280]   lots of interesting insights about what's actually
[00:08:10.280 --> 00:08:11.680]   happening with these systems.
[00:08:11.680 --> 00:08:14.080]   How hidden layers are really converging.
[00:08:14.080 --> 00:08:15.680]   Sort of the differences between networks
[00:08:15.680 --> 00:08:17.840]   that are overfitting or memorizing
[00:08:17.840 --> 00:08:20.520]   versus actually generalizing well.
[00:08:20.520 --> 00:08:22.280]   And sort of building off of this,
[00:08:22.280 --> 00:08:26.240]   more recently there was a paper that really advanced
[00:08:26.240 --> 00:08:28.240]   the core algorithm for performing
[00:08:28.240 --> 00:08:31.560]   these kinds of comparisons of hidden representations
[00:08:31.560 --> 00:08:33.320]   in neural networks.
[00:08:33.320 --> 00:08:36.560]   And so this algorithm is known as the CKA algorithm.
[00:08:36.560 --> 00:08:39.280]   And the way it works is it's going to take in two set--
[00:08:39.280 --> 00:08:41.160]   two two-layer activation matrices.
[00:08:41.160 --> 00:08:43.720]   So some layer one and layer two.
[00:08:43.720 --> 00:08:47.600]   And then it's going to map those into just a scalar similarity
[00:08:47.600 --> 00:08:48.280]   score.
[00:08:48.280 --> 00:08:49.800]   That's just simply telling us, well,
[00:08:49.800 --> 00:08:52.080]   sort of how similar are these representations.
[00:08:52.080 --> 00:08:53.640]   So zero is they're totally different,
[00:08:53.640 --> 00:08:56.440]   and one is sort of they're identical.
[00:08:56.440 --> 00:08:58.920]   And this algorithm is going to be a very important lens
[00:08:58.920 --> 00:09:03.880]   for the question we're about to investigate.
[00:09:03.880 --> 00:09:07.280]   So going back to this kind of picture of design challenges,
[00:09:07.280 --> 00:09:09.120]   the question we really want to investigate
[00:09:09.120 --> 00:09:12.200]   is better understand sort of all these different models
[00:09:12.200 --> 00:09:14.360]   people were playing with and sort of how
[00:09:14.360 --> 00:09:17.480]   the representations actually learned amongst these models
[00:09:17.480 --> 00:09:19.880]   might vary amongst them.
[00:09:19.880 --> 00:09:23.920]   And in particular, one trend that we're seeing as of late
[00:09:23.920 --> 00:09:27.080]   is that people are looking at actually playing
[00:09:27.080 --> 00:09:31.080]   with the total model capacity by varying the depth
[00:09:31.080 --> 00:09:33.240]   and the width of the model.
[00:09:33.240 --> 00:09:37.120]   So we've seen this across all kinds of imaging models,
[00:09:37.120 --> 00:09:40.120]   ResNets, ResNets, efficient nets, efficient debts,
[00:09:40.120 --> 00:09:42.160]   more recently transformers.
[00:09:42.160 --> 00:09:44.560]   And two key hyperparameters people play with here
[00:09:44.560 --> 00:09:48.680]   are actually changing the model depth and the model width.
[00:09:48.680 --> 00:09:51.800]   And oftentimes, the resulting model is very performant.
[00:09:51.800 --> 00:09:54.280]   But we wanted to understand how these might be affecting
[00:09:54.280 --> 00:09:56.840]   the hidden representations for which there
[00:09:56.840 --> 00:09:59.000]   are many open questions.
[00:09:59.000 --> 00:10:01.720]   So sort of concretely, are there any differences
[00:10:01.720 --> 00:10:04.400]   if we just look into a single model
[00:10:04.400 --> 00:10:07.600]   and sort of look at what it's actually learning?
[00:10:07.600 --> 00:10:11.160]   Is kind of depth and width affecting that in any clear way?
[00:10:11.160 --> 00:10:13.400]   If we take models that look very different to each other
[00:10:13.400 --> 00:10:16.600]   architecturally, are they learning the same thing?
[00:10:16.600 --> 00:10:19.520]   And are there even sort of characteristic errors
[00:10:19.520 --> 00:10:22.080]   that might be associated with these kinds
[00:10:22.080 --> 00:10:25.120]   of different neural network architectures?
[00:10:25.120 --> 00:10:28.120]   Those are some of the questions we were interested in.
[00:10:28.120 --> 00:10:32.520]   And so to study this, we turned to a very simple setup.
[00:10:32.520 --> 00:10:36.080]   We took ResNets, which are a family of models
[00:10:36.080 --> 00:10:41.120]   in image classification that have already been in practice,
[00:10:41.120 --> 00:10:44.320]   have their depth and width varied.
[00:10:44.320 --> 00:10:45.960]   So we took a family of models where
[00:10:45.960 --> 00:10:48.760]   we could vary their depth and width in this way.
[00:10:48.760 --> 00:10:50.720]   And we looked at training them on these sorts
[00:10:50.720 --> 00:10:53.440]   of standard image classification tasks, things
[00:10:53.440 --> 00:10:56.920]   like CIFAR-10, CIFAR-100, ImageNet.
[00:10:56.920 --> 00:11:00.520]   And then sort of on these, using CKA as a lens,
[00:11:00.520 --> 00:11:02.560]   we went in to study what was happening
[00:11:02.560 --> 00:11:06.040]   in their hidden representations.
[00:11:06.040 --> 00:11:09.360]   And so in particular, CKA actually
[00:11:09.360 --> 00:11:10.960]   lets us-- it's a very flexible tool
[00:11:10.960 --> 00:11:15.360]   and lets us compare any pair of hidden layers with each other.
[00:11:15.360 --> 00:11:17.360]   And so to represent some of the results,
[00:11:17.360 --> 00:11:20.160]   we kind of turned to showing things as a heat map.
[00:11:20.160 --> 00:11:22.800]   So what we have here is we have a neural network.
[00:11:22.800 --> 00:11:26.120]   And so along the x-axis, we have sort of the layers
[00:11:26.120 --> 00:11:28.680]   corresponding to one neural network.
[00:11:28.680 --> 00:11:31.040]   Along the y-axis, we have the layers corresponding
[00:11:31.040 --> 00:11:32.960]   to another neural network.
[00:11:32.960 --> 00:11:34.800]   And these might be sort of the same network.
[00:11:34.800 --> 00:11:36.120]   These might be different networks.
[00:11:36.120 --> 00:11:37.800]   These might be networks that have two different kinds
[00:11:37.800 --> 00:11:38.760]   of architectures.
[00:11:38.760 --> 00:11:40.080]   It doesn't really matter.
[00:11:40.080 --> 00:11:42.240]   We can kind of do all of these pairwise comparisons.
[00:11:42.240 --> 00:11:45.200]   And the plan is to sort of show it as a heat map.
[00:11:45.200 --> 00:11:46.560]   Sort of by convention, we sort of
[00:11:46.560 --> 00:11:48.680]   had the input in the kind of bottom left corner.
[00:11:48.680 --> 00:11:51.360]   So the output is sort of the top layer on the sort of the right
[00:11:51.360 --> 00:11:53.400]   and sort of upper half.
[00:11:53.400 --> 00:11:56.800]   And then for sort of any pair of layers that we could compare,
[00:11:56.800 --> 00:12:00.000]   we can just sort of go into their corresponding grid cell
[00:12:00.000 --> 00:12:04.920]   and fill it in with their CKA similarity score.
[00:12:04.920 --> 00:12:07.960]   So OK, so that's kind of like the schematic.
[00:12:07.960 --> 00:12:10.080]   Let's see what this actually looks like.
[00:12:10.080 --> 00:12:12.200]   So here are sort of two different ResNets.
[00:12:12.200 --> 00:12:14.440]   So again, along the end-- so this time,
[00:12:14.440 --> 00:12:16.120]   the model we're comparing is actually
[00:12:16.120 --> 00:12:17.200]   the same architecture.
[00:12:17.200 --> 00:12:18.800]   So both in the left and the right,
[00:12:18.800 --> 00:12:20.520]   we're taking the same ResNet model
[00:12:20.520 --> 00:12:23.360]   and sort of just comparing all of its hidden representations
[00:12:23.360 --> 00:12:25.520]   to itself.
[00:12:25.520 --> 00:12:28.480]   So along the x-axis, you have sort of all of the layers.
[00:12:28.480 --> 00:12:31.760]   And then along the y-axis, you sort of have the same layers.
[00:12:31.760 --> 00:12:34.680]   So in this heat map, yellow sort of colored
[00:12:34.680 --> 00:12:37.400]   is sort of maximal CKA similarity 1,
[00:12:37.400 --> 00:12:40.040]   whereas black is 0.
[00:12:40.040 --> 00:12:42.720]   Along the diagonal, kind of reassuringly,
[00:12:42.720 --> 00:12:44.360]   you can see this nice yellow line.
[00:12:44.360 --> 00:12:46.280]   And that's because you're comparing each layer
[00:12:46.280 --> 00:12:47.000]   with itself.
[00:12:47.000 --> 00:12:50.520]   So of course, its representation is identical.
[00:12:50.520 --> 00:12:52.240]   If you kind of peer at this more closely,
[00:12:52.240 --> 00:12:54.720]   you can see other kind of cool artifacts
[00:12:54.720 --> 00:12:56.680]   from the result of this being a ResNet.
[00:12:56.680 --> 00:12:58.800]   So you sort of see this grid-like pattern.
[00:12:58.800 --> 00:13:01.040]   And that's because of the skip connections of the ResNet
[00:13:01.040 --> 00:13:02.080]   coming in.
[00:13:02.080 --> 00:13:03.480]   If you sort of stare even closer,
[00:13:03.480 --> 00:13:07.600]   you can see sort of like a three-block-style structure.
[00:13:07.600 --> 00:13:12.080]   And that's due to the different stages that the ResNet has.
[00:13:12.080 --> 00:13:15.640]   OK, so this is kind of what things look like.
[00:13:15.640 --> 00:13:17.360]   But what happens to these heat maps
[00:13:17.360 --> 00:13:20.080]   as we vary depth and width?
[00:13:20.080 --> 00:13:22.200]   Well, when we do that, the result
[00:13:22.200 --> 00:13:25.120]   is actually really striking.
[00:13:25.120 --> 00:13:27.680]   So this is kind of what happens when we vary depth and width.
[00:13:27.680 --> 00:13:30.280]   So the top row is what happens as we slowly
[00:13:30.280 --> 00:13:31.880]   make the architecture deeper.
[00:13:31.880 --> 00:13:34.420]   Bottom row is what happens as we slowly make the architecture
[00:13:34.420 --> 00:13:35.240]   wider.
[00:13:35.240 --> 00:13:37.400]   And what you can kind of really clearly see
[00:13:37.400 --> 00:13:41.600]   is what we call a block structure emerging
[00:13:41.600 --> 00:13:43.800]   in the representations.
[00:13:43.800 --> 00:13:47.040]   And if you're following along, you'll
[00:13:47.040 --> 00:13:51.640]   sort of see that this block structure, in words,
[00:13:51.640 --> 00:13:54.480]   what it really means is that we sort of have this large set
[00:13:54.480 --> 00:13:59.120]   of contiguous layers that have these really high CKA scores,
[00:13:59.120 --> 00:14:03.840]   so very high representation similarity.
[00:14:03.840 --> 00:14:05.360]   So that was really exciting.
[00:14:05.360 --> 00:14:07.280]   We went ahead and did some work to check
[00:14:07.280 --> 00:14:09.320]   this as a robust phenomenon, things like that.
[00:14:09.320 --> 00:14:12.000]   And it does just very reliably appear
[00:14:12.000 --> 00:14:15.480]   in these sorts of wide and deep models.
[00:14:15.480 --> 00:14:18.480]   So this sort of suggests that this block structure maybe
[00:14:18.480 --> 00:14:22.760]   has some connection to the actual model capacity.
[00:14:22.760 --> 00:14:25.680]   And so we wanted to check whether this is really the case.
[00:14:25.680 --> 00:14:27.400]   Is there some interventional experiment
[00:14:27.400 --> 00:14:30.840]   we could do to really establish this connection?
[00:14:30.840 --> 00:14:32.560]   And one nice experiment you can do
[00:14:32.560 --> 00:14:36.280]   is you can increase model capacity artificially
[00:14:36.280 --> 00:14:40.640]   by reducing the training data set size.
[00:14:40.640 --> 00:14:42.640]   And so we went ahead and did this.
[00:14:42.640 --> 00:14:45.880]   And so as you kind of go down in each column,
[00:14:45.880 --> 00:14:48.680]   you're sort of decreasing the training data set size.
[00:14:48.680 --> 00:14:52.480]   And indeed, what you see is as you decrease the training data
[00:14:52.480 --> 00:14:56.040]   set size, you're sort of seeing this block structure emerge
[00:14:56.040 --> 00:14:58.040]   in models that didn't have a block
[00:14:58.040 --> 00:15:02.680]   structure in their representations anymore.
[00:15:02.680 --> 00:15:05.280]   So that was super interesting to see.
[00:15:05.280 --> 00:15:07.560]   And then based off of this in the paper,
[00:15:07.560 --> 00:15:11.640]   we sort of did further analysis of all of this.
[00:15:11.640 --> 00:15:15.480]   But OK, so although so far, we've kind of developed--
[00:15:15.480 --> 00:15:18.760]   we've kind of talked about some of the motivation for the CKA
[00:15:18.760 --> 00:15:19.760]   algorithm.
[00:15:19.760 --> 00:15:22.520]   We've dived into the representations of a model.
[00:15:22.520 --> 00:15:26.240]   And we've seen that when we make them sort of wider or deeper,
[00:15:26.240 --> 00:15:28.200]   performance-wise, they're still doing great.
[00:15:28.200 --> 00:15:31.160]   But their representations are getting this block structure.
[00:15:31.160 --> 00:15:32.640]   And this block structure is indeed
[00:15:32.640 --> 00:15:35.680]   linked with the actual capacity of the model.
[00:15:35.680 --> 00:15:38.480]   But to us, there was still a gap in sort of all of this,
[00:15:38.480 --> 00:15:41.560]   which is we still didn't understand sort of functionally
[00:15:41.560 --> 00:15:43.280]   what the block structure was really
[00:15:43.280 --> 00:15:45.360]   doing to the representations.
[00:15:45.360 --> 00:15:47.800]   So we were seeing this high representation similarity.
[00:15:47.800 --> 00:15:49.600]   But sort of layer by layer, what was really
[00:15:49.600 --> 00:15:54.440]   going on in the representations through the block structure?
[00:15:54.440 --> 00:15:57.960]   OK, so how should we go about studying that?
[00:15:57.960 --> 00:16:00.520]   Well, if we take a few steps back
[00:16:00.520 --> 00:16:04.080]   and think about what we were doing with the CKA algorithm,
[00:16:04.080 --> 00:16:06.480]   what we were doing is we were taking these different layer
[00:16:06.480 --> 00:16:07.840]   activation matrices.
[00:16:07.840 --> 00:16:10.400]   And then we were sort of performing this CKA algorithm
[00:16:10.400 --> 00:16:12.360]   to compare them.
[00:16:12.360 --> 00:16:14.040]   And so instead, we went for something
[00:16:14.040 --> 00:16:15.800]   kind of even more low level.
[00:16:15.800 --> 00:16:18.880]   We looked at just studying the principal components
[00:16:18.880 --> 00:16:21.960]   of the layer activation matrix.
[00:16:21.960 --> 00:16:25.040]   And it turned out that a very important question to ask
[00:16:25.040 --> 00:16:28.440]   was, how important is the first principal component
[00:16:28.440 --> 00:16:31.280]   of these representations?
[00:16:31.280 --> 00:16:34.080]   And what we found is that the first principal component
[00:16:34.080 --> 00:16:36.480]   was extremely important for models
[00:16:36.480 --> 00:16:40.120]   that had the block structure.
[00:16:40.120 --> 00:16:41.920]   So what we did is you can sort of take
[00:16:41.920 --> 00:16:43.680]   the different layers of the network that's
[00:16:43.680 --> 00:16:46.280]   sort of shown in the x-axes going from input all the way
[00:16:46.280 --> 00:16:47.640]   to output.
[00:16:47.640 --> 00:16:50.200]   And then on the y-axes, you sort of
[00:16:50.200 --> 00:16:52.760]   have the fraction of variance explained.
[00:16:52.760 --> 00:16:55.000]   So for every single layer, we take
[00:16:55.000 --> 00:16:57.680]   sort of the fraction of variance explained by just
[00:16:57.680 --> 00:17:00.160]   the first principal component.
[00:17:00.160 --> 00:17:02.440]   And so what's really obvious here is you can see these two
[00:17:02.440 --> 00:17:04.880]   big sort of spikes.
[00:17:04.880 --> 00:17:08.160]   And these spikes actually correspond perfectly
[00:17:08.160 --> 00:17:11.040]   with where we're seeing the block structure when
[00:17:11.040 --> 00:17:15.200]   we do this sort of the CKA analysis.
[00:17:15.200 --> 00:17:18.320]   And in fact, if we then remove the first principal component
[00:17:18.320 --> 00:17:21.280]   of the representations, well, then the block structure
[00:17:21.280 --> 00:17:24.280]   is significantly reduced.
[00:17:24.280 --> 00:17:26.520]   So with this experiment and sort of others like this,
[00:17:26.520 --> 00:17:29.680]   we were able to really pinpoint the functional property
[00:17:29.680 --> 00:17:31.600]   of the block structure.
[00:17:31.600 --> 00:17:34.080]   So in these sort of high-capacity models,
[00:17:34.080 --> 00:17:37.240]   the block structure worked to preserve and propagate
[00:17:37.240 --> 00:17:42.000]   the first principal component of these representations.
[00:17:42.000 --> 00:17:43.680]   So this is kind of a really nice summary
[00:17:43.680 --> 00:17:48.080]   of what was happening in the internals of a single model.
[00:17:48.080 --> 00:17:50.640]   And so sort of informed by this, we
[00:17:50.640 --> 00:17:53.000]   turned into the question that motivated us
[00:17:53.000 --> 00:17:56.720]   to start this paper in the first place, which was, well, what's
[00:17:56.720 --> 00:17:59.920]   happening in representations across models?
[00:17:59.920 --> 00:18:01.560]   Are wide and deep neural networks
[00:18:01.560 --> 00:18:03.880]   learning the same things?
[00:18:03.880 --> 00:18:06.160]   And it turns out to be a really good starting point
[00:18:06.160 --> 00:18:08.360]   to start by looking at a single model,
[00:18:08.360 --> 00:18:10.680]   because there's an important connection between the block
[00:18:10.680 --> 00:18:14.560]   structure and representations across models.
[00:18:14.560 --> 00:18:16.800]   So as a first simple comparison to see
[00:18:16.800 --> 00:18:18.720]   what was happening across models,
[00:18:18.720 --> 00:18:20.840]   we just looked at different random seeds.
[00:18:20.840 --> 00:18:22.520]   So now we're applying CKA-- we're now
[00:18:22.520 --> 00:18:24.640]   sort of computing these CKA heat maps,
[00:18:24.640 --> 00:18:27.120]   but the CKA heat maps are between models that
[00:18:27.120 --> 00:18:28.480]   are just different random seeds.
[00:18:28.480 --> 00:18:32.120]   So same architecture, but different random seeds.
[00:18:32.120 --> 00:18:34.000]   And there is this kind of important distinction
[00:18:34.000 --> 00:18:36.080]   between models that don't have the block structure
[00:18:36.080 --> 00:18:37.800]   and models that do.
[00:18:37.800 --> 00:18:41.080]   So with models that don't have the block structure,
[00:18:41.080 --> 00:18:43.400]   well, along the diagonal, we're actually comparing sort
[00:18:43.400 --> 00:18:44.560]   of the model to itself.
[00:18:44.560 --> 00:18:46.120]   No block structure here.
[00:18:46.120 --> 00:18:48.280]   But most importantly, in the off-diagonal,
[00:18:48.280 --> 00:18:49.800]   where we're sort of comparing models
[00:18:49.800 --> 00:18:52.000]   of different random seeds, you can actually
[00:18:52.000 --> 00:18:54.560]   see there's a decent amount of similarity.
[00:18:54.560 --> 00:18:57.520]   In fact, between the same random seed and different random
[00:18:57.520 --> 00:19:00.800]   seeds, it looks pretty similar, except for sort
[00:19:00.800 --> 00:19:02.640]   of this strong diagonal line, which
[00:19:02.640 --> 00:19:04.600]   is because in same random seeds, it's
[00:19:04.600 --> 00:19:07.520]   identical representations.
[00:19:07.520 --> 00:19:10.280]   So in summary, we're actually seeing similar features
[00:19:10.280 --> 00:19:12.440]   across these different seeds.
[00:19:12.440 --> 00:19:14.880]   But when models have the block structure,
[00:19:14.880 --> 00:19:16.760]   the story is different.
[00:19:16.760 --> 00:19:18.480]   So of course, across the diagonal,
[00:19:18.480 --> 00:19:20.440]   you're sort of seeing this nice block structure.
[00:19:20.440 --> 00:19:22.840]   But in the off-diagonal, you're seeing
[00:19:22.840 --> 00:19:25.680]   this big sort of lack of similarity
[00:19:25.680 --> 00:19:28.360]   pretty much precisely where the block structure is.
[00:19:28.360 --> 00:19:31.360]   And so sort of put another way, the block structure
[00:19:31.360 --> 00:19:34.120]   is actually unique to each of these seeds.
[00:19:34.120 --> 00:19:37.720]   It's a unique representation learned by these models.
[00:19:37.720 --> 00:19:39.440]   So this is different random seeds.
[00:19:39.440 --> 00:19:41.800]   You can do the sort of same analysis
[00:19:41.800 --> 00:19:43.880]   for different model architectures.
[00:19:43.880 --> 00:19:45.800]   And again, there's sort of this difference
[00:19:45.800 --> 00:19:49.600]   between things that have the block structure and things that
[00:19:49.600 --> 00:19:52.600]   don't have the block structure.
[00:19:52.600 --> 00:19:59.200]   OK, so to summarize things sort of so far,
[00:19:59.200 --> 00:20:01.680]   we've kind of been varying the kind of depth
[00:20:01.680 --> 00:20:03.760]   and width of these different models
[00:20:03.760 --> 00:20:05.720]   by sort of analyzing their internals.
[00:20:05.720 --> 00:20:08.400]   We've gotten sort of this characteristic property
[00:20:08.400 --> 00:20:10.840]   of their representations for these large capacity models,
[00:20:10.840 --> 00:20:11.920]   this block structure.
[00:20:11.920 --> 00:20:14.880]   And we've also gone on to try and understand
[00:20:14.880 --> 00:20:18.040]   how this affects sort of representations learned
[00:20:18.040 --> 00:20:20.680]   across different models and come to this conclusion
[00:20:20.680 --> 00:20:23.520]   that for, well, do wide and deep networks learn the same things?
[00:20:23.520 --> 00:20:25.280]   Well, they learn the same things when they
[00:20:25.280 --> 00:20:26.600]   don't have a block structure.
[00:20:26.600 --> 00:20:28.320]   But when they do have a block structure,
[00:20:28.320 --> 00:20:30.960]   you are seeing that there are some unique set of features
[00:20:30.960 --> 00:20:33.120]   learned by each model.
[00:20:33.120 --> 00:20:34.960]   So to complete this study, we actually
[00:20:34.960 --> 00:20:37.520]   return to thinking about the actual predictions
[00:20:37.520 --> 00:20:38.440]   of the model.
[00:20:38.440 --> 00:20:40.480]   So we're typically evaluating the model just
[00:20:40.480 --> 00:20:42.000]   by looking at its output.
[00:20:42.000 --> 00:20:44.080]   And so as sort of a final kind of question,
[00:20:44.080 --> 00:20:46.080]   we wanted to understand whether there
[00:20:46.080 --> 00:20:48.480]   were clear differences in the actual predictions
[00:20:48.480 --> 00:20:52.600]   and errors being made by models that had
[00:20:52.600 --> 00:20:56.280]   different architectural structures.
[00:20:56.280 --> 00:21:00.160]   And so to do this final study, what we were doing
[00:21:00.160 --> 00:21:02.640]   is we were training a population of networks
[00:21:02.640 --> 00:21:05.160]   that had a certain architecture on some task.
[00:21:05.160 --> 00:21:08.200]   And then we were comparing to a population of networks
[00:21:08.200 --> 00:21:09.680]   also trained on that task, but maybe
[00:21:09.680 --> 00:21:11.840]   with a different architecture.
[00:21:11.840 --> 00:21:15.760]   And so the important kind of quantitative thing to note here
[00:21:15.760 --> 00:21:17.880]   is that when you have a population of architectures
[00:21:17.880 --> 00:21:20.640]   trained on some task, say architecture A,
[00:21:20.640 --> 00:21:25.080]   we can actually think about the accuracy of that population.
[00:21:25.080 --> 00:21:26.720]   So we can take a particular data point,
[00:21:26.720 --> 00:21:29.560]   and then we can compute, say, P A of x,
[00:21:29.560 --> 00:21:33.320]   which is the sort of probability that everything with architecture
[00:21:33.320 --> 00:21:36.640]   A classifies x correctly.
[00:21:36.640 --> 00:21:40.440]   And same for B. And you can do this at a class level also.
[00:21:40.440 --> 00:21:43.520]   So there are sort of two different kind of granularities
[00:21:43.520 --> 00:21:44.000]   here.
[00:21:44.000 --> 00:21:46.800]   There's at a data point level and at a class level.
[00:21:46.800 --> 00:21:48.240]   OK.
[00:21:48.240 --> 00:21:50.640]   And so sort of with these populations trained,
[00:21:50.640 --> 00:21:54.080]   we looked at comparing accuracies and errors
[00:21:54.080 --> 00:21:57.880]   at a data point level and at a class level.
[00:21:57.880 --> 00:21:59.920]   And so the results actually varied a little bit
[00:21:59.920 --> 00:22:01.400]   on the data sets.
[00:22:01.400 --> 00:22:03.120]   So on CIFAR-10, we didn't really see
[00:22:03.120 --> 00:22:06.560]   any statistically significant class level differences.
[00:22:06.560 --> 00:22:09.160]   But when we looked at the per example level,
[00:22:09.160 --> 00:22:13.480]   we did see that architectures that are more similar
[00:22:13.480 --> 00:22:16.720]   in structure have actually more similar predictions.
[00:22:16.720 --> 00:22:20.600]   So here, on the x and y-axes, we are precisely
[00:22:20.600 --> 00:22:24.200]   plotting this P A of x and P B of x
[00:22:24.200 --> 00:22:27.840]   for every single one of these data points x.
[00:22:27.840 --> 00:22:30.520]   And so on the left plot here, we're
[00:22:30.520 --> 00:22:33.400]   kind of comparing a deep network with a wide network.
[00:22:33.400 --> 00:22:36.160]   And you can see that things are very far out from the y
[00:22:36.160 --> 00:22:37.360]   equals x line.
[00:22:37.360 --> 00:22:38.920]   So there are lots of data points which
[00:22:38.920 --> 00:22:43.760]   have a very different accuracy for architecture A
[00:22:43.760 --> 00:22:46.640]   compared to architecture B. Whereas in comparison,
[00:22:46.640 --> 00:22:48.040]   when you compare a deep architecture
[00:22:48.040 --> 00:22:50.080]   to a deep architecture or wide architecture
[00:22:50.080 --> 00:22:53.240]   to a wide architecture, things are much more concentrated
[00:22:53.240 --> 00:22:54.960]   along the y equals x line.
[00:22:54.960 --> 00:22:57.080]   And there are many, many more experiments here
[00:22:57.080 --> 00:22:58.320]   that are in our paper.
[00:22:58.320 --> 00:23:00.120]   So do check it out.
[00:23:00.120 --> 00:23:03.000]   And then finally, on ImageNet, what was interesting
[00:23:03.000 --> 00:23:06.560]   is that we saw class and example level differences.
[00:23:06.560 --> 00:23:09.000]   And because there were these class level differences,
[00:23:09.000 --> 00:23:11.080]   we were actually able to identify
[00:23:11.080 --> 00:23:14.160]   some characteristic errors.
[00:23:14.160 --> 00:23:18.560]   So over here along the x-axis, we
[00:23:18.560 --> 00:23:22.160]   have a very deep network again and a wide network.
[00:23:22.160 --> 00:23:24.040]   The dots in orange are a baseline.
[00:23:24.040 --> 00:23:27.840]   So they're just the deep network minus the deep network.
[00:23:27.840 --> 00:23:32.040]   Whereas the dots in blue are the actual result
[00:23:32.040 --> 00:23:33.440]   we care about measuring.
[00:23:33.440 --> 00:23:37.120]   They're the population accuracy of the wide network
[00:23:37.120 --> 00:23:40.320]   subtracting the population accuracy of the deep network.
[00:23:40.320 --> 00:23:44.280]   So translating all of that, things
[00:23:44.280 --> 00:23:47.760]   with positive y value are places where the wide model is doing
[00:23:47.760 --> 00:23:48.520]   better.
[00:23:48.520 --> 00:23:52.040]   And things with negative y value are places where the deep model
[00:23:52.040 --> 00:23:53.360]   is doing better.
[00:23:53.360 --> 00:23:55.920]   So here, we're broadly seeing this interesting bias
[00:23:55.920 --> 00:23:58.200]   where maybe the wide model is doing a little bit better
[00:23:58.200 --> 00:24:01.080]   on scenes, whereas the deep model is doing better
[00:24:01.080 --> 00:24:01.680]   on objects.
[00:24:01.680 --> 00:24:09.680]   - Vytra, a quick question from YouTube from Tyler.
[00:24:09.680 --> 00:24:12.960]   Back one slide, were the deep, deep, and wide, wide experiments
[00:24:12.960 --> 00:24:16.480]   on ResNets with different seeds?
[00:24:16.480 --> 00:24:17.480]   - Yes, yes, yes.
[00:24:17.480 --> 00:24:18.000]   Yeah.
[00:24:18.000 --> 00:24:19.560]   - So they have the same architecture, just
[00:24:19.560 --> 00:24:20.640]   different seeds?
[00:24:20.640 --> 00:24:21.160]   - Yeah.
[00:24:21.160 --> 00:24:22.720]   So in this case, it's sort of same architecture
[00:24:22.720 --> 00:24:23.440]   as different seeds.
[00:24:23.440 --> 00:24:25.440]   And you're sort of seeing the amount of variance
[00:24:25.440 --> 00:24:28.200]   you'd naturally get thanks to sort of just like, I guess,
[00:24:28.200 --> 00:24:30.000]   sort of sampling differences.
[00:24:30.000 --> 00:24:31.960]   But in the paper, we sort of--
[00:24:31.960 --> 00:24:33.760]   they just didn't fit on the slides.
[00:24:33.760 --> 00:24:35.400]   But we have this nice interpolation
[00:24:35.400 --> 00:24:37.560]   going from sort of things that are
[00:24:37.560 --> 00:24:39.400]   sort of like very deep versus things
[00:24:39.400 --> 00:24:40.960]   that are sort of very wide.
[00:24:40.960 --> 00:24:42.800]   And sort of what does the spread look like?
[00:24:42.800 --> 00:24:45.040]   And what you can broadly see is that architectures
[00:24:45.040 --> 00:24:47.760]   that are sort of both kind of might classify as deep,
[00:24:47.760 --> 00:24:51.200]   but not the same architecture, do look a bit more concentrated
[00:24:51.200 --> 00:24:55.720]   along the y equals x line than kind of going from the deep
[00:24:55.720 --> 00:24:59.120]   to like the very sort of wide architecture, for instance.
[00:24:59.120 --> 00:24:59.640]   - Got it.
[00:24:59.640 --> 00:25:00.720]   That's a great experiment.
[00:25:00.720 --> 00:25:03.360]   Thanks for sharing.
[00:25:03.360 --> 00:25:03.880]   - OK.
[00:25:03.880 --> 00:25:05.400]   So back to this.
[00:25:05.720 --> 00:25:08.000]   So to summarize, in ImageNet, I think
[00:25:08.000 --> 00:25:10.920]   the interesting kind of addition was sort of this class level
[00:25:10.920 --> 00:25:12.080]   difference.
[00:25:12.080 --> 00:25:14.800]   And so I'll just sort of summarize.
[00:25:14.800 --> 00:25:18.240]   So sort of, again, as kind of broader motivation,
[00:25:18.240 --> 00:25:19.920]   we kind of--
[00:25:19.920 --> 00:25:21.680]   this was sort of really inspired by the fact
[00:25:21.680 --> 00:25:23.480]   that we really need measures that give us
[00:25:23.480 --> 00:25:25.560]   more insights into the internals of these systems
[00:25:25.560 --> 00:25:30.080]   to really help inform sort of principled and robust design
[00:25:30.080 --> 00:25:31.080]   processes.
[00:25:31.080 --> 00:25:33.160]   That's what really motivated a whole line of work,
[00:25:33.160 --> 00:25:37.360]   including CKA, that we used to sort of study,
[00:25:37.360 --> 00:25:39.640]   give us insights into internals of these systems
[00:25:39.640 --> 00:25:42.200]   through representational similarity.
[00:25:42.200 --> 00:25:43.840]   And this was kind of really the tool
[00:25:43.840 --> 00:25:46.400]   we used to study lots of properties of networks
[00:25:46.400 --> 00:25:48.400]   as we varied their width and depth.
[00:25:48.400 --> 00:25:51.440]   Specifically, we sort of saw this block structure component
[00:25:51.440 --> 00:25:53.240]   in networks of large capacity.
[00:25:53.240 --> 00:25:55.200]   We're able to kind of really interventionally
[00:25:55.200 --> 00:25:58.640]   link that to the capacity of the network.
[00:25:58.640 --> 00:26:00.800]   And most sort of satisfyingly, I guess,
[00:26:00.800 --> 00:26:03.400]   we're really able to pinpoint what this block structure does
[00:26:03.400 --> 00:26:04.000]   functionally.
[00:26:04.000 --> 00:26:05.520]   It sort of preserves and propagates
[00:26:05.520 --> 00:26:08.160]   this first principle component.
[00:26:08.160 --> 00:26:10.440]   And this turns out to be very important
[00:26:10.440 --> 00:26:13.560]   when we think about what's going on across different models.
[00:26:13.560 --> 00:26:15.640]   Model architectures that without the block structure
[00:26:15.640 --> 00:26:18.800]   do share features, but the block structure sort of uniquely
[00:26:18.800 --> 00:26:20.360]   learned to each model.
[00:26:20.360 --> 00:26:23.040]   And we also observe these sorts of differences
[00:26:23.040 --> 00:26:26.200]   in actual predictions, which are influenced
[00:26:26.200 --> 00:26:28.640]   by the actual architecture of the model.
[00:26:28.640 --> 00:26:31.560]   And on ImageNet, these sorts of characteristic errors.
[00:26:31.560 --> 00:26:33.240]   So that's some of the things we covered.
[00:26:33.240 --> 00:26:36.000]   I think there are lots of really exciting open questions
[00:26:36.000 --> 00:26:36.680]   in this space.
[00:26:36.680 --> 00:26:38.600]   So firstly, I think there's a big open question
[00:26:38.600 --> 00:26:40.440]   on the dynamics of the block structure.
[00:26:40.440 --> 00:26:42.280]   So how is it really learned?
[00:26:42.280 --> 00:26:44.720]   And sort of like, how does it evolve?
[00:26:44.720 --> 00:26:45.880]   And sort of how stable is it?
[00:26:45.880 --> 00:26:47.240]   Things like that.
[00:26:47.240 --> 00:26:50.240]   There are sort of nice questions on other architectures
[00:26:50.240 --> 00:26:51.520]   and other tasks.
[00:26:51.520 --> 00:26:54.280]   We informally got told that the block structure has also
[00:26:54.280 --> 00:26:56.400]   been observed in things like transformers.
[00:26:56.400 --> 00:26:58.760]   And I think it would be really interesting to investigate
[00:26:58.760 --> 00:26:59.800]   that.
[00:26:59.800 --> 00:27:01.560]   And then there are questions, again,
[00:27:01.560 --> 00:27:03.160]   maybe related to the block structure
[00:27:03.160 --> 00:27:05.560]   and sort of is this sort of an overfitting thing?
[00:27:05.560 --> 00:27:06.760]   Is this a key feature?
[00:27:06.760 --> 00:27:10.880]   Sort of why does it exist, basically?
[00:27:10.880 --> 00:27:11.440]   OK.
[00:27:11.440 --> 00:27:13.720]   And so with that, I will wrap up.
[00:27:13.720 --> 00:27:15.840]   Thanks, everybody, for tuning in.
[00:27:15.840 --> 00:27:19.000]   And I guess, Charles, you might have some questions.
[00:27:19.000 --> 00:27:22.760]   Yeah, I'll take a few minutes to ask you some questions.
[00:27:22.760 --> 00:27:25.680]   I want to make sure that we get a chance folks who are online
[00:27:25.680 --> 00:27:28.640]   can ask questions in the Zoom as well.
[00:27:28.640 --> 00:27:30.920]   I guess my first question is, I mean,
[00:27:30.920 --> 00:27:33.960]   so you partly addressed this by mentioning transformers.
[00:27:33.960 --> 00:27:37.240]   How much do you think that this block structure propagating
[00:27:37.240 --> 00:27:42.320]   this first principal component of the Gram matrix,
[00:27:42.320 --> 00:27:45.040]   do you think that that's really a ResNet specific thing?
[00:27:45.040 --> 00:27:50.160]   Because they have this capacity to just pass information
[00:27:50.160 --> 00:27:51.560]   through an identity transformation?
[00:27:51.560 --> 00:27:52.680]   Do you think that that's something
[00:27:52.680 --> 00:27:55.960]   that you might observe in, say, like a VGG type architecture?
[00:27:55.960 --> 00:27:58.360]   Yeah, so the reason we focused on ResNets
[00:27:58.360 --> 00:28:00.600]   is because these skip connections are really
[00:28:00.600 --> 00:28:02.320]   sort of useful and important for, I guess,
[00:28:02.320 --> 00:28:04.640]   stabilizing things as you go deeper.
[00:28:04.640 --> 00:28:06.800]   But in the paper, we actually have additional results
[00:28:06.800 --> 00:28:08.920]   sort of without the residual connections and stuff.
[00:28:08.920 --> 00:28:10.840]   And you do take a bit of a performance hit.
[00:28:10.840 --> 00:28:13.080]   But on the other hand, you do see the same sort of thing
[00:28:13.080 --> 00:28:16.400]   with the block structure and sort of, yeah,
[00:28:16.400 --> 00:28:18.680]   like those sort of very similar properties.
[00:28:18.680 --> 00:28:19.960]   So I do think--
[00:28:19.960 --> 00:28:21.400]   and especially given sort of--
[00:28:21.400 --> 00:28:22.480]   I mean, this is informal.
[00:28:22.480 --> 00:28:25.640]   So I think this is a definite exciting open question that
[00:28:25.640 --> 00:28:27.040]   would be really cool to go after.
[00:28:27.040 --> 00:28:29.880]   But I think transformers would be great to look at.
[00:28:29.880 --> 00:28:31.800]   And the fact that somebody informally
[00:28:31.800 --> 00:28:33.240]   said that they think they'd seen something
[00:28:33.240 --> 00:28:34.320]   like that in transformers.
[00:28:34.320 --> 00:28:37.080]   So I think that would be a really nice sort of thing
[00:28:37.080 --> 00:28:38.680]   to verify on.
[00:28:38.680 --> 00:28:39.480]   Yeah, that's cool.
[00:28:39.480 --> 00:28:41.920]   I think my current mental model for transformers
[00:28:41.920 --> 00:28:44.520]   comes from that Hopfield is all you need paper,
[00:28:44.520 --> 00:28:47.840]   that a lot of what's going on there, like some of the pieces,
[00:28:47.840 --> 00:28:50.040]   the attention heads are kind of pulling out averages.
[00:28:50.040 --> 00:28:51.560]   And then some of them are pulling out
[00:28:51.560 --> 00:28:52.600]   aggregate information.
[00:28:52.600 --> 00:28:54.120]   It'd be interesting to see if there's
[00:28:54.120 --> 00:28:56.520]   a connection between that idea and what's going on here.
[00:28:56.520 --> 00:28:57.520]   Right, right, right.
[00:28:57.520 --> 00:29:00.800]   Yeah, yeah, definitely.
[00:29:00.800 --> 00:29:02.520]   What other hyperparameters-- it seems
[00:29:02.520 --> 00:29:05.640]   like you may have sort of fixed a lot of the hyperparameters
[00:29:05.640 --> 00:29:08.000]   besides the width and depth.
[00:29:08.000 --> 00:29:11.760]   So things like how many max pools and coms you're doing,
[00:29:11.760 --> 00:29:14.800]   the kernel sizes, whether you're using batch norm,
[00:29:14.800 --> 00:29:15.920]   when you're using it.
[00:29:15.920 --> 00:29:19.440]   Do you think any of those other kinds of hyperparameters
[00:29:19.440 --> 00:29:21.720]   that are maybe not quite as architectural still
[00:29:21.720 --> 00:29:25.000]   impact maybe the block structure or other factors
[00:29:25.000 --> 00:29:26.160]   about representations?
[00:29:26.160 --> 00:29:27.760]   Or do you think that this is something
[00:29:27.760 --> 00:29:31.000]   that's going to stay the same no matter what you change there?
[00:29:31.000 --> 00:29:32.360]   Yeah, that's a good question.
[00:29:32.360 --> 00:29:35.560]   So I think some things like probably batch norm,
[00:29:35.560 --> 00:29:37.320]   my prior would be like they don't necessarily
[00:29:37.320 --> 00:29:38.920]   have to affect things too much.
[00:29:38.920 --> 00:29:41.320]   So our goal is-- so we want to vary width and depth
[00:29:41.320 --> 00:29:43.320]   because that felt like a number of sort of things
[00:29:43.320 --> 00:29:47.200]   that were manageable to vary and do experiments with.
[00:29:47.200 --> 00:29:51.400]   And then we kind of want to keep everything else stable.
[00:29:51.400 --> 00:29:54.320]   I think other kind of design things-- yeah,
[00:29:54.320 --> 00:29:57.400]   the other next kind of thing to look at on the vision side
[00:29:57.400 --> 00:29:59.840]   would be sort of like maybe things like kernel size
[00:29:59.840 --> 00:30:01.520]   or like kind of receptive field size.
[00:30:01.520 --> 00:30:04.880]   I guess this is kind of stepping into the actual input
[00:30:04.880 --> 00:30:05.720]   data itself.
[00:30:05.720 --> 00:30:06.840]   But you could look at kind of playing
[00:30:06.840 --> 00:30:09.360]   with the resolution of the input data, for instance,
[00:30:09.360 --> 00:30:11.360]   and try and understand whether--
[00:30:11.360 --> 00:30:13.080]   sort of how that affects things.
[00:30:13.080 --> 00:30:16.640]   My hypothesis would be that somehow if increasing
[00:30:16.640 --> 00:30:19.360]   the resolution of the data somehow really
[00:30:19.360 --> 00:30:22.520]   inflating the kind of effective data set size,
[00:30:22.520 --> 00:30:25.280]   then I think you might need sort of larger models
[00:30:25.280 --> 00:30:27.480]   before you start seeing the block structure.
[00:30:27.480 --> 00:30:29.640]   But I think otherwise, I would still sort of expect
[00:30:29.640 --> 00:30:33.680]   to see something like this.
[00:30:33.680 --> 00:30:37.080]   One idea that's come out of the neural tangent kernel
[00:30:37.080 --> 00:30:40.000]   literature and the people thinking about scaling
[00:30:40.000 --> 00:30:43.480]   limits of neural networks is that it's very easy to end up
[00:30:43.480 --> 00:30:47.520]   with a really big neural network that ends up collapsing down
[00:30:47.520 --> 00:30:49.000]   to a kernel machine.
[00:30:49.000 --> 00:30:51.280]   It's really effectively-- it's like you're only training
[00:30:51.280 --> 00:30:53.280]   that last linear readout layer.
[00:30:53.280 --> 00:30:55.400]   Do you think there might be a connection between what
[00:30:55.400 --> 00:30:57.600]   you're seeing with these big block sizes that
[00:30:57.600 --> 00:31:01.360]   seem to be maybe a collapse in the representation?
[00:31:01.360 --> 00:31:04.040]   And that idea, do you think those are unrelated?
[00:31:04.040 --> 00:31:05.760]   So I'm not sure.
[00:31:05.760 --> 00:31:07.280]   So you'll have to tell me because I
[00:31:07.280 --> 00:31:09.840]   haven't been following that literature sort of extremely
[00:31:09.840 --> 00:31:11.200]   closely as of late.
[00:31:11.200 --> 00:31:14.080]   But I also thought you took a little bit of a performance hit
[00:31:14.080 --> 00:31:15.760]   when you sort of did that.
[00:31:15.760 --> 00:31:17.680]   And I need to check because I know it varies also
[00:31:17.680 --> 00:31:18.920]   depending on the architecture.
[00:31:18.920 --> 00:31:20.840]   Like maybe convolutional networks,
[00:31:20.840 --> 00:31:22.280]   you maybe see a performance hit.
[00:31:22.280 --> 00:31:24.920]   But then maybe for fully connected models, you don't.
[00:31:24.920 --> 00:31:27.800]   And then there's a ton of work out there.
[00:31:27.800 --> 00:31:30.800]   But here, I think they might be different things
[00:31:30.800 --> 00:31:33.760]   because I think here, you're not--
[00:31:33.760 --> 00:31:35.880]   I don't think you're sort of just starting with this
[00:31:35.880 --> 00:31:38.160]   and it's sort of like you're just not sort of like--
[00:31:38.160 --> 00:31:39.640]   it's sort of not training at all.
[00:31:39.640 --> 00:31:42.200]   I think it's more sort of like the--
[00:31:42.200 --> 00:31:44.200]   somehow, something about the dynamics of learning
[00:31:44.200 --> 00:31:46.400]   is resulting in sort of this sort of propagation
[00:31:46.400 --> 00:31:47.960]   sort of happening.
[00:31:47.960 --> 00:31:51.960]   So I think those would be different.
[00:31:51.960 --> 00:31:53.880]   But that's also an interesting question.
[00:31:53.880 --> 00:31:56.840]   So that's worth investigating.
[00:31:56.840 --> 00:31:58.000]   But oh, yeah, sorry.
[00:31:58.000 --> 00:31:59.280]   One other thing is just--
[00:31:59.280 --> 00:32:02.160]   yeah, so the other motivation for doing this is that--
[00:32:02.160 --> 00:32:06.400]   so I guess back on some work on expressiveness and stuff,
[00:32:06.400 --> 00:32:08.120]   depth and width are sort of these parameters
[00:32:08.120 --> 00:32:09.960]   that we've sort of mathematically studied
[00:32:09.960 --> 00:32:10.720]   in various ways.
[00:32:10.720 --> 00:32:12.400]   Like you randomly initialize a network
[00:32:12.400 --> 00:32:14.480]   and look at its depth and width and understand
[00:32:14.480 --> 00:32:16.520]   what's expressive, what's efficient to express
[00:32:16.520 --> 00:32:18.840]   for models of different architectures.
[00:32:18.840 --> 00:32:20.960]   But sort of the real motivation for doing this was,
[00:32:20.960 --> 00:32:23.600]   can we just take a bunch of models that are sort of very
[00:32:23.600 --> 00:32:26.680]   close to what people would kind of be tempted to do in practice
[00:32:26.680 --> 00:32:29.440]   if they wanted to increase model capacity or something?
[00:32:29.440 --> 00:32:31.280]   And can we test them out and sort of then
[00:32:31.280 --> 00:32:34.000]   understand what's happening with their representations?
[00:32:34.000 --> 00:32:36.080]   So indeed, I think one really cool thing to me
[00:32:36.080 --> 00:32:38.040]   is that a whole bunch of these architectures
[00:32:38.040 --> 00:32:40.200]   that we tried with sort of varying depth and width,
[00:32:40.200 --> 00:32:42.840]   performance-wise, they really look pretty identical.
[00:32:42.840 --> 00:32:44.960]   And it's only probing into the representations
[00:32:44.960 --> 00:32:47.320]   that you sort of see this property.
[00:32:47.320 --> 00:32:50.360]   So that was really cool to me.
[00:32:50.360 --> 00:32:52.880]   Yeah, that is interesting that you see the top line
[00:32:52.880 --> 00:32:55.400]   accuracy can be very similar, but then the classes
[00:32:55.400 --> 00:32:56.760]   on which they're getting the best
[00:32:56.760 --> 00:32:58.800]   accuracy and the worst accuracy look different.
[00:32:58.800 --> 00:33:00.000]   That's a really cool result.
[00:33:00.000 --> 00:33:03.520]   Yeah, yeah, yeah.
[00:33:03.520 --> 00:33:04.480]   All right.
[00:33:04.480 --> 00:33:07.400]   Well, that's all the time we have for our first talk.
[00:33:07.400 --> 00:33:10.680]   So thanks a lot, Maitre, for coming through and answering
[00:33:10.680 --> 00:33:13.640]   folks' questions and doing some really excellent work.
[00:33:13.640 --> 00:33:16.320]   OK, thank you so much.
[00:33:16.320 --> 00:33:20.720]   All right, next up, we have Dighanta Mishra,
[00:33:20.720 --> 00:33:24.720]   who will be presenting a couple of different interrelated
[00:33:24.720 --> 00:33:26.600]   ideas.
[00:33:26.600 --> 00:33:31.360]   So Dighanta is maybe best known for the MISH activation,
[00:33:31.360 --> 00:33:37.120]   a self-named self-normalizing activation for neural networks.
[00:33:37.120 --> 00:33:39.760]   And actually, I had seen some of Dighanta's work,
[00:33:39.760 --> 00:33:42.080]   and now Dighanta has come to join Weights of Biases
[00:33:42.080 --> 00:33:43.000]   as an ML engineer.
[00:33:43.000 --> 00:33:44.080]   So that's very exciting.
[00:33:44.080 --> 00:33:46.160]   And I'm looking forward to hear what
[00:33:46.160 --> 00:33:49.560]   Dighanta has to say on smoothness, robustness,
[00:33:49.560 --> 00:33:52.920]   catastrophic forgetting, and more.
[00:33:52.920 --> 00:33:53.520]   Awesome.
[00:33:53.520 --> 00:33:55.960]   Thank you, Charles, for the introduction.
[00:33:55.960 --> 00:33:59.280]   And first of all, thank you for inviting me, especially
[00:33:59.280 --> 00:34:01.560]   with Maitre on the panel.
[00:34:01.560 --> 00:34:05.440]   That's a huge honor for me to share the podium with her.
[00:34:05.440 --> 00:34:09.960]   She's an absolute inspiration for me in my research field.
[00:34:09.960 --> 00:34:15.280]   And I mean, wow, her talk was really, really good.
[00:34:15.280 --> 00:34:19.360]   I personally follow that line of work.
[00:34:19.360 --> 00:34:21.480]   And in fact, some of the points that I'll
[00:34:21.480 --> 00:34:23.720]   put up in my presentation right now
[00:34:23.720 --> 00:34:27.920]   would somewhat correlate to her presentation.
[00:34:27.920 --> 00:34:31.240]   So it might be good to follow up on that.
[00:34:31.240 --> 00:34:33.520]   All right, let me share my screen.
[00:34:34.520 --> 00:34:36.480]   [VIDEO PLAYBACK]
[00:34:36.480 --> 00:34:36.980]   OK.
[00:34:36.980 --> 00:34:41.680]   I guess that should be up.
[00:34:41.680 --> 00:34:50.360]   So in between, I might shift to showcasing
[00:34:50.360 --> 00:34:54.920]   the actual text of a paper just to make things
[00:34:54.920 --> 00:34:56.720]   have more clarity.
[00:34:56.720 --> 00:34:58.880]   But we'll go along with the flow.
[00:34:58.880 --> 00:35:02.960]   The idea of the talk is basically
[00:35:02.960 --> 00:35:07.840]   to provide with questions and hypothetical scenarios that
[00:35:07.840 --> 00:35:11.080]   can possibly happen with current neural network trainings.
[00:35:11.080 --> 00:35:15.680]   So I really invite the audience to actually brainstorm
[00:35:15.680 --> 00:35:18.000]   about those questions and possibly think
[00:35:18.000 --> 00:35:22.200]   of an analytical solution towards it.
[00:35:22.200 --> 00:35:28.640]   So with that, let's get started.
[00:35:28.640 --> 00:35:32.120]   So we'll start by watching a short video.
[00:35:32.120 --> 00:35:38.520]   And we'll come to realize what this video is exactly
[00:35:38.520 --> 00:35:41.920]   discussing about in the next few slides.
[00:35:41.920 --> 00:35:43.560]   So basically, what you're seeing here
[00:35:43.560 --> 00:35:47.720]   are the loss landscapes of a ResNet-20
[00:35:47.720 --> 00:35:50.000]   having different activation functions.
[00:35:50.000 --> 00:35:53.000]   And basically, you're seeing the comparison
[00:35:53.000 --> 00:35:55.360]   between non-smooth and smooth activations
[00:35:55.360 --> 00:35:57.720]   like ReLu, Mesh, and Swish.
[00:35:57.720 --> 00:36:03.520]   So you can see ReLu has a very sharp loss landscape
[00:36:03.520 --> 00:36:07.080]   and has a few saddle points, while Mesh and Swish,
[00:36:07.080 --> 00:36:10.520]   the smoother alternatives, provide a much smoother
[00:36:10.520 --> 00:36:13.560]   landscape, which is obviously very easy to optimize.
[00:36:13.560 --> 00:36:17.120]   And thus, leads to much better generalization
[00:36:17.120 --> 00:36:19.800]   as compared to its non-smooth counterpart.
[00:36:19.800 --> 00:36:23.680]   So this video was done with Javier Arrimi,
[00:36:23.680 --> 00:36:26.160]   so big shout out to him.
[00:36:26.160 --> 00:36:32.000]   So now with that, let's get into the actual stuff.
[00:36:32.000 --> 00:36:34.240]   So what's the idea of smooth activation?
[00:36:34.240 --> 00:36:38.160]   So we have had in the current neural network scenario,
[00:36:38.160 --> 00:36:43.160]   ReLu being probably the most standout activation function
[00:36:43.160 --> 00:36:45.760]   to be used in all the different computer vision tasks
[00:36:45.760 --> 00:36:47.720]   that we can think of.
[00:36:47.720 --> 00:36:50.480]   And that's probably because of two reasons.
[00:36:50.480 --> 00:36:52.160]   One, it works very well.
[00:36:52.160 --> 00:36:54.760]   It's like universally works pretty well
[00:36:54.760 --> 00:36:57.920]   across all the different tasks, like image classification,
[00:36:57.920 --> 00:36:59.800]   semantic segmentation, object detection,
[00:36:59.800 --> 00:37:03.440]   any downstream computer vision tasks that one can think of.
[00:37:03.440 --> 00:37:06.320]   And also, ReLu is super efficient, super lightweight,
[00:37:06.320 --> 00:37:07.520]   and easy to implement.
[00:37:07.520 --> 00:37:11.960]   And it also makes the whole training process much faster.
[00:37:11.960 --> 00:37:16.360]   But currently, we have had some very interesting hype
[00:37:16.360 --> 00:37:19.440]   around smooth activation functions.
[00:37:19.440 --> 00:37:21.000]   And what's the general idea?
[00:37:21.000 --> 00:37:24.560]   The general idea is basically that--
[00:37:24.560 --> 00:37:28.400]   OK, I guess this might be floating.
[00:37:28.400 --> 00:37:30.560]   Right, so the general idea is that we
[00:37:30.560 --> 00:37:35.120]   have activation functions which have a smooth profile.
[00:37:35.120 --> 00:37:39.000]   By smooth here, I mean that they are C1 smooth,
[00:37:39.000 --> 00:37:42.040]   which means that the first order derivative is continuous.
[00:37:42.040 --> 00:37:45.240]   And usually, we would prefer to have a non-monotonic profile
[00:37:45.240 --> 00:37:47.920]   as well with the capability of preserving
[00:37:47.920 --> 00:37:49.280]   small amount of negative weights,
[00:37:49.280 --> 00:37:52.040]   because that's one of the key drawbacks of ReLu,
[00:37:52.040 --> 00:37:55.360]   because it thresholds all the negative information to zero.
[00:37:55.360 --> 00:37:57.760]   So we have a significant loss of information.
[00:37:57.760 --> 00:38:00.360]   And that actually plays a huge role
[00:38:00.360 --> 00:38:03.040]   when we are dealing with deep networks.
[00:38:03.040 --> 00:38:04.280]   So--
[00:38:04.280 --> 00:38:07.040]   - Deganta, I actually figured I'd ask some questions
[00:38:07.040 --> 00:38:08.000]   through this talk.
[00:38:08.000 --> 00:38:09.960]   And I actually invite Maitre.
[00:38:09.960 --> 00:38:13.200]   I guess if you have any questions, you can also pipe up.
[00:38:13.200 --> 00:38:17.680]   So I think of the fact that ReLu sets a lot of values to zero
[00:38:17.680 --> 00:38:20.080]   as kind of actually a feature rather than a bug,
[00:38:20.080 --> 00:38:25.000]   because it sort of says, oh, there's nothing here.
[00:38:25.000 --> 00:38:28.040]   I can only sort of signal the presence of something.
[00:38:28.040 --> 00:38:30.280]   And so there's lots of uses of non-negative matrix
[00:38:30.280 --> 00:38:33.560]   factorization, where positive only is a useful idea.
[00:38:33.560 --> 00:38:35.880]   So I think, could you just expand a little bit more
[00:38:35.880 --> 00:38:38.600]   on why people think this non-monotonicity in things
[00:38:38.600 --> 00:38:41.640]   like SWISH and MISH is so useful?
[00:38:41.640 --> 00:38:42.200]   - Great.
[00:38:42.200 --> 00:38:43.080]   So yeah, perfect.
[00:38:43.080 --> 00:38:46.680]   So actually, there has been significant discussion
[00:38:46.680 --> 00:38:50.400]   around why make it non-monotonous.
[00:38:50.400 --> 00:38:53.360]   If you are actually going to preserve negative weights,
[00:38:53.360 --> 00:38:57.520]   then you can go with the alternative like Leaky ReLu
[00:38:57.520 --> 00:38:59.200]   or like a shifted soft plus.
[00:38:59.200 --> 00:39:01.440]   Or you can go with other alternatives,
[00:39:01.440 --> 00:39:08.840]   which do not collapse down to zero at minus infinity.
[00:39:08.840 --> 00:39:11.720]   It hasn't been very well explained in literature yet.
[00:39:11.720 --> 00:39:14.120]   At least, I haven't found concrete reasoning
[00:39:14.120 --> 00:39:18.760]   behind the fact that why one would want to possibly only
[00:39:18.760 --> 00:39:21.840]   preserve negative weights in a small amount, which
[00:39:21.840 --> 00:39:23.320]   is closer to zero.
[00:39:23.320 --> 00:39:27.080]   But the general idea is that the weights usually
[00:39:27.080 --> 00:39:31.600]   learned in a neural network are very much closer to zero.
[00:39:31.600 --> 00:39:35.480]   And it's considered that weights which are farther apart
[00:39:35.480 --> 00:39:38.320]   are sort of outliers and not necessary.
[00:39:38.320 --> 00:39:42.240]   So to answer that question, there
[00:39:42.240 --> 00:39:45.480]   hasn't been as a concrete reasoning on the fact
[00:39:45.480 --> 00:39:47.960]   that why we would like specifically
[00:39:47.960 --> 00:39:49.680]   non-monotonous profile.
[00:39:49.680 --> 00:39:52.960]   But the whole idea is that we would
[00:39:52.960 --> 00:39:55.720]   like to have a smooth continuous profile for activation
[00:39:55.720 --> 00:39:58.960]   function so that when the optimization happens,
[00:39:58.960 --> 00:40:00.560]   the gradients are smooth.
[00:40:00.560 --> 00:40:03.040]   And that actually plays a huge role,
[00:40:03.040 --> 00:40:05.160]   not just in normal training, but also
[00:40:05.160 --> 00:40:08.080]   in training of adversarial samples,
[00:40:08.080 --> 00:40:09.800]   which we'll get to next.
[00:40:09.800 --> 00:40:13.080]   But that's an open-ended question for the community
[00:40:13.080 --> 00:40:17.520]   to really brainstorm about, to think
[00:40:17.520 --> 00:40:21.280]   why we really prioritize on positive weights
[00:40:21.280 --> 00:40:24.520]   and only give a small amount of priority to negative weights
[00:40:24.520 --> 00:40:29.120]   in examples like SWISH, MISH, or Gaussian error
[00:40:29.120 --> 00:40:32.840]   linear nets, or any other non-monotonic activation
[00:40:32.840 --> 00:40:35.560]   that exists out there.
[00:40:35.560 --> 00:40:37.480]   Yeah, thanks for-- that's a great answer.
[00:40:37.480 --> 00:40:40.920]   I think the fact that the SWISH activation was discovered
[00:40:40.920 --> 00:40:42.800]   by this sort of architecture search,
[00:40:42.800 --> 00:40:47.320]   really, it really demands to be answered why that's so useful.
[00:40:47.320 --> 00:40:48.800]   Yeah, absolutely.
[00:40:48.800 --> 00:40:52.480]   In fact, SWISH, the formulation of SWISH
[00:40:52.480 --> 00:40:56.880]   was earlier proposed in SIGMOD and mostly in it.
[00:40:56.880 --> 00:40:58.160]   It's called SILO.
[00:40:58.160 --> 00:41:03.720]   And even GLE can approximate the form of SWISH as well.
[00:41:03.720 --> 00:41:08.760]   So the profile of activation function being non-monotonic
[00:41:08.760 --> 00:41:14.040]   and having that small amount of negative weights being preserved
[00:41:14.040 --> 00:41:16.880]   is still very much open-ended.
[00:41:16.880 --> 00:41:21.120]   And we haven't yet figured out in absolute detail
[00:41:21.120 --> 00:41:23.520]   about why we are doing that.
[00:41:23.520 --> 00:41:25.680]   But we can clearly see its benefits.
[00:41:25.680 --> 00:41:30.040]   And that's kind of outshining and speaking for itself
[00:41:30.040 --> 00:41:32.640]   as compared to piecewise functions
[00:41:32.640 --> 00:41:37.200]   like ReLU, which threshold the negative information to 0.
[00:41:37.200 --> 00:41:41.760]   So continuing with that, one of the examples
[00:41:41.760 --> 00:41:44.840]   that I'm going to shortly discuss
[00:41:44.840 --> 00:41:47.840]   about in terms of non-monotonic activation functions
[00:41:47.840 --> 00:41:50.240]   is a function that I created.
[00:41:50.240 --> 00:41:51.640]   It's called MISH.
[00:41:51.640 --> 00:41:56.480]   And it's basically a very, very closely related to SWISH,
[00:41:56.480 --> 00:41:59.360]   and in fact, was heavily inspired by it.
[00:41:59.360 --> 00:42:02.920]   And the function is basically input into tanh
[00:42:02.920 --> 00:42:04.600]   of soft list of input.
[00:42:04.600 --> 00:42:08.520]   And as you can see in the plot here,
[00:42:08.520 --> 00:42:12.160]   it somewhat has the same amount of negative information
[00:42:12.160 --> 00:42:17.240]   being kept in the profile of the activation function.
[00:42:17.240 --> 00:42:20.240]   So why is this important?
[00:42:20.240 --> 00:42:22.920]   And why specifically are we looking
[00:42:22.920 --> 00:42:27.560]   at functions which have this profile similar to SWISH
[00:42:27.560 --> 00:42:30.560]   and MISH?
[00:42:30.560 --> 00:42:33.600]   So first of all, these activation functions
[00:42:33.600 --> 00:42:38.400]   which share the same profile as that of MISH and SWISH
[00:42:38.400 --> 00:42:42.200]   are really more robust to increase
[00:42:42.200 --> 00:42:44.200]   in depth of the neural network.
[00:42:44.200 --> 00:42:47.600]   And as I just stated before the start of the talk
[00:42:47.600 --> 00:42:50.200]   that some parts of my presentation
[00:42:50.200 --> 00:42:53.200]   will be highly correlated to that of Mithra's,
[00:42:53.200 --> 00:42:57.360]   I think this is one of them that we can take some caveats off
[00:42:57.360 --> 00:43:01.200]   of it that we can probably experiment with Mithra's idea
[00:43:01.200 --> 00:43:05.640]   and see if this actually holds out in the representation form
[00:43:05.640 --> 00:43:06.760]   that she discussed about.
[00:43:06.760 --> 00:43:10.000]   So as you can see over here, very basic experiment.
[00:43:10.000 --> 00:43:12.600]   We take a six layer neural network
[00:43:12.600 --> 00:43:16.360]   and basically have a ReLU, MISH, and SWISH
[00:43:16.360 --> 00:43:18.160]   as its activation functions.
[00:43:18.160 --> 00:43:21.520]   We keep on increasing the number of layers progressively,
[00:43:21.520 --> 00:43:25.720]   and we see that the test accuracy significantly
[00:43:25.720 --> 00:43:29.240]   drops for ReLU at an alarming rate,
[00:43:29.240 --> 00:43:31.960]   followed by SWISH, followed by MISH.
[00:43:31.960 --> 00:43:36.200]   So ideally, you would want that neural networks which
[00:43:36.200 --> 00:43:40.560]   increase in depth should also have the ability
[00:43:40.560 --> 00:43:43.160]   to preserve its generalization capacity
[00:43:43.160 --> 00:43:45.720]   and not drop at the rate that ReLU demonstrates over here.
[00:43:45.720 --> 00:43:49.680]   So--
[00:43:49.680 --> 00:43:51.640]   So, Deganta, a quick question.
[00:43:51.640 --> 00:43:55.440]   So is the training accuracy for these networks
[00:43:55.440 --> 00:43:56.320]   comparable?
[00:43:56.320 --> 00:44:00.200]   Because one thing with depth, especially with something
[00:44:00.200 --> 00:44:02.840]   like ReLU, there's not that--
[00:44:02.840 --> 00:44:04.720]   it's not self-normalizing at all.
[00:44:04.720 --> 00:44:08.200]   And SWISH is a little bit closer to self-normalizing,
[00:44:08.200 --> 00:44:10.120]   so then maybe it does a little bit better.
[00:44:10.120 --> 00:44:13.200]   But I think there's a bit of a confound of how well are you
[00:44:13.200 --> 00:44:15.120]   able to train these networks that
[00:44:15.120 --> 00:44:19.000]   might be resolved with something like batch norm, for example.
[00:44:19.000 --> 00:44:20.080]   Yeah, that's a good point.
[00:44:20.080 --> 00:44:23.800]   So I should clarify that all these experiments were
[00:44:23.800 --> 00:44:25.440]   done with vanilla settings, which
[00:44:25.440 --> 00:44:28.240]   means that all of the hyperparameters,
[00:44:28.240 --> 00:44:31.000]   the network parameters were shared for all the three
[00:44:31.000 --> 00:44:31.520]   networks.
[00:44:31.520 --> 00:44:36.040]   So we weren't providing SWISH with optimal settings
[00:44:36.040 --> 00:44:38.400]   that it actually requires to thrive.
[00:44:38.400 --> 00:44:40.520]   So we do have certain initializations
[00:44:40.520 --> 00:44:43.360]   that work better with MISH, that works better with SWISH,
[00:44:43.360 --> 00:44:44.840]   and that works better with ReLU.
[00:44:44.840 --> 00:44:46.560]   But we actually chose for this experiment
[00:44:46.560 --> 00:44:51.120]   the vanilla setting, which is the default for ReLU.
[00:44:51.120 --> 00:44:54.040]   And in terms of training accuracy,
[00:44:54.040 --> 00:44:55.440]   it wasn't comparable.
[00:44:55.440 --> 00:44:57.240]   As you're stating, ReLU was still
[00:44:57.240 --> 00:45:03.520]   lagging quite a lot when we increased the number of layers
[00:45:03.520 --> 00:45:05.040]   in our network.
[00:45:05.040 --> 00:45:09.640]   But SWISH and MISH were somewhat comparable.
[00:45:09.640 --> 00:45:11.960]   MISH was usually edging SWISH.
[00:45:11.960 --> 00:45:15.880]   But again, this is not a very comprehensive example
[00:45:15.880 --> 00:45:18.680]   to demonstrate that differentiating factor,
[00:45:18.680 --> 00:45:21.120]   because it's still a very basic neural network
[00:45:21.120 --> 00:45:22.520]   and just on MNIST.
[00:45:22.520 --> 00:45:28.400]   So maybe adding more complexity will reveal some more hidden
[00:45:28.400 --> 00:45:31.720]   details about how big or small the gap is
[00:45:31.720 --> 00:45:35.360]   between the smooth activations, which is MISH and SWISH,
[00:45:35.360 --> 00:45:38.400]   and the smooth activations, MISH, SWISH,
[00:45:38.400 --> 00:45:40.800]   and the non-smooth ones, which is ReLU.
[00:45:40.800 --> 00:45:45.120]   So again, this was not the main focus
[00:45:45.120 --> 00:45:47.160]   for me to experiment with.
[00:45:47.160 --> 00:45:49.800]   But yeah, since it is correlating
[00:45:49.800 --> 00:45:53.000]   to what Maitra discussed about, it's definitely something
[00:45:53.000 --> 00:45:57.280]   that I would be interested to experiment with and see
[00:45:57.280 --> 00:45:59.120]   what the observations reveal.
[00:45:59.120 --> 00:46:07.720]   OK, so what do we get with smooth activation functions?
[00:46:07.720 --> 00:46:10.800]   One of the prime reasons that probably users won't shift
[00:46:10.800 --> 00:46:12.760]   towards using smooth activation functions
[00:46:12.760 --> 00:46:15.240]   is that they are computationally expensive.
[00:46:15.240 --> 00:46:19.520]   And ReLU is just too easy to implement
[00:46:19.520 --> 00:46:22.320]   and comes out of the box with most of the neural network
[00:46:22.320 --> 00:46:22.840]   packages.
[00:46:22.840 --> 00:46:26.560]   So it becomes obvious that one would rather
[00:46:26.560 --> 00:46:29.600]   try to just run the vanilla script
[00:46:29.600 --> 00:46:32.520]   rather than trying to find optimal hyperparameters
[00:46:32.520 --> 00:46:35.640]   for smooth activation and then using it in the neural network.
[00:46:35.640 --> 00:46:37.760]   But we can clearly see that it provides
[00:46:37.760 --> 00:46:41.080]   significant improvement in performance
[00:46:41.080 --> 00:46:46.440]   over piecewise approximations like ReLU and ReGReLU.
[00:46:46.440 --> 00:46:50.680]   And that's also in complex tasks like ImageNet.
[00:46:50.680 --> 00:46:54.560]   And we also see in this experiment
[00:46:54.560 --> 00:46:57.800]   that smooth activations also fare very well when
[00:46:57.800 --> 00:47:01.240]   you use different data augmentation strategies
[00:47:01.240 --> 00:47:04.840]   like mixup or label smoothing or mosaic.
[00:47:04.840 --> 00:47:09.160]   And so this basically speaks that we should rather
[00:47:09.160 --> 00:47:12.960]   transition towards smooth activation functions, which
[00:47:12.960 --> 00:47:16.400]   are comparable in terms of computational complexity
[00:47:16.400 --> 00:47:20.000]   to that of non-smooth alternatives like ReLU
[00:47:20.000 --> 00:47:22.960]   or LeakyReLU because of the significant improvement
[00:47:22.960 --> 00:47:24.000]   in performance.
[00:47:24.000 --> 00:47:26.360]   But that's not the only reason that we
[00:47:26.360 --> 00:47:31.000]   should consider these smooth activation functions.
[00:47:31.000 --> 00:47:33.960]   Before I go into the reason why, here's
[00:47:33.960 --> 00:47:36.240]   another of the downstream computer vision tasks
[00:47:36.240 --> 00:47:39.320]   that most of the users and academicians
[00:47:39.320 --> 00:47:42.040]   use to benchmark their neural networks, so
[00:47:42.040 --> 00:47:43.600]   object detection and MSCOCO.
[00:47:43.600 --> 00:47:46.800]   And you can see that in this example,
[00:47:46.800 --> 00:47:50.120]   mesh is clearly leading by quite a significant margin
[00:47:50.120 --> 00:47:52.560]   as compared to LeakyReLU.
[00:47:52.560 --> 00:47:58.040]   But I should reiterate that all these experiments
[00:47:58.040 --> 00:48:00.040]   we did where we benchmark neural networks
[00:48:00.040 --> 00:48:01.880]   on different data sets, we didn't
[00:48:01.880 --> 00:48:05.640]   try to find out the optimal hyperparameters for LeakyReLU
[00:48:05.640 --> 00:48:07.640]   or mesh specifically.
[00:48:07.640 --> 00:48:11.760]   We ran everything with the normal benchmark settings
[00:48:11.760 --> 00:48:15.600]   that are used and reported in all the default
[00:48:15.600 --> 00:48:17.320]   papers in computer vision.
[00:48:17.320 --> 00:48:22.600]   So probably we expect that if we find optimal hyperparameters,
[00:48:22.600 --> 00:48:26.360]   like as I said earlier, a particular initialization
[00:48:26.360 --> 00:48:28.600]   method that works very well with mesh,
[00:48:28.600 --> 00:48:32.400]   we would probably expect to see a much larger improvement
[00:48:32.400 --> 00:48:35.840]   in performance as compared to what we are observing over
[00:48:35.840 --> 00:48:38.120]   here.
[00:48:38.120 --> 00:48:44.360]   So as I started with a simple video about lost landscapes,
[00:48:44.360 --> 00:48:50.320]   now I think it's time to dive into why I brought that up.
[00:48:50.320 --> 00:48:54.400]   It's really interesting to see the performance improvement
[00:48:54.400 --> 00:48:57.720]   that smooth activation functions are providing as compared
[00:48:57.720 --> 00:48:59.320]   to ReLU or LeakyReLU.
[00:48:59.320 --> 00:49:02.560]   But we wanted to explore what is happening actually
[00:49:02.560 --> 00:49:05.840]   and what's driving that performance improvement.
[00:49:05.840 --> 00:49:08.800]   So first of all, we take this example
[00:49:08.800 --> 00:49:11.600]   where we have a ResNet20 architecture
[00:49:11.600 --> 00:49:15.080]   and we employ it with ReLU and mesh.
[00:49:15.080 --> 00:49:18.480]   And we just visualize its lost landscape over here.
[00:49:18.480 --> 00:49:25.000]   And you can see that mesh has a much smoother lost landscape
[00:49:25.000 --> 00:49:28.320]   as compared to ReLU and also has a lower minima point.
[00:49:28.320 --> 00:49:32.400]   So that essentially reflects to better generalization.
[00:49:32.400 --> 00:49:35.040]   But the smoothness of the lost landscape
[00:49:35.040 --> 00:49:38.920]   correlates to much easier optimization.
[00:49:38.920 --> 00:49:41.640]   Does not only result in performance improvement
[00:49:41.640 --> 00:49:46.120]   as compared to ReLU, but also converges faster just
[00:49:46.120 --> 00:49:51.800]   because of the geometric interpretation of the smoothness
[00:49:51.800 --> 00:49:54.560]   that we are observing by introducing
[00:49:54.560 --> 00:49:57.480]   non-monotonic functions like mesh or swish.
[00:49:57.480 --> 00:50:00.880]   So this is really interesting to see
[00:50:00.880 --> 00:50:06.400]   because sometimes performance metrics is probably
[00:50:06.400 --> 00:50:08.200]   the only standalone metric that is
[00:50:08.200 --> 00:50:11.280]   used to compare activation function performance.
[00:50:11.280 --> 00:50:14.600]   And they might not reflect the actual thing happening
[00:50:14.600 --> 00:50:15.800]   in the background.
[00:50:15.800 --> 00:50:19.960]   So this really consolidates that smooth activation functions
[00:50:19.960 --> 00:50:25.440]   are indeed helpful in not just improving your neural network
[00:50:25.440 --> 00:50:29.480]   training process, but also allowing it to converge faster.
[00:50:29.480 --> 00:50:31.840]   So there's a trade-off where you use
[00:50:31.840 --> 00:50:35.520]   ReLU, which is inherently faster per epoch,
[00:50:35.520 --> 00:50:38.120]   but takes more epoch to train.
[00:50:38.120 --> 00:50:42.480]   But then there's mesh, which is inherently slower than ReLU
[00:50:42.480 --> 00:50:45.840]   per epoch, but takes less epoch to converge.
[00:50:45.840 --> 00:50:49.080]   So it's sort of a perspective that you
[00:50:49.080 --> 00:50:53.280]   have to choose between which one is the fittest
[00:50:53.280 --> 00:50:55.560]   for your own neural network setting.
[00:50:55.560 --> 00:50:58.320]   But usually, smooth activation functions
[00:50:58.320 --> 00:51:01.400]   stand out as compared to non-smooth ones
[00:51:01.400 --> 00:51:04.480]   like ReLU over here.
[00:51:04.480 --> 00:51:06.480]   - Deganta, a couple of questions.
[00:51:06.480 --> 00:51:07.080]   - Yeah.
[00:51:07.080 --> 00:51:12.280]   - So the first one, this is coming in through from a viewer.
[00:51:12.280 --> 00:51:15.200]   What was the motivation for the functional form for mesh?
[00:51:15.200 --> 00:51:16.880]   So there's these nice motivations
[00:51:16.880 --> 00:51:19.440]   you've been giving in terms of smoothness
[00:51:19.440 --> 00:51:22.320]   and in terms of preserving those negative values.
[00:51:22.320 --> 00:51:25.160]   But where did that motivation come from?
[00:51:25.160 --> 00:51:27.240]   And what intuitions may you've gained
[00:51:27.240 --> 00:51:31.280]   in finding that form for mesh?
[00:51:31.280 --> 00:51:34.040]   - So that's a good question, because there's
[00:51:34.040 --> 00:51:37.560]   a really interesting story behind it.
[00:51:37.560 --> 00:51:43.800]   In 2018, I was basically participating
[00:51:43.800 --> 00:51:46.040]   in a machine learning competition,
[00:51:46.040 --> 00:51:47.640]   in-house machine learning competition.
[00:51:47.640 --> 00:51:52.000]   And it was a task to simply build a classifier.
[00:51:52.000 --> 00:51:56.520]   And my classifier that I was training,
[00:51:56.520 --> 00:51:59.360]   although it was really good, it wasn't just
[00:51:59.360 --> 00:52:02.440]   cutting the mark as compared to other submissions
[00:52:02.440 --> 00:52:03.440]   that were coming in.
[00:52:03.440 --> 00:52:06.680]   So I basically randomly searched up
[00:52:06.680 --> 00:52:10.000]   what can I do in my neural network that will probably
[00:52:10.000 --> 00:52:11.840]   improve the performance.
[00:52:11.840 --> 00:52:14.600]   And one of the standout points that came up
[00:52:14.600 --> 00:52:16.080]   were activation functions.
[00:52:16.080 --> 00:52:17.720]   And you can probably play around with it
[00:52:17.720 --> 00:52:20.120]   and change the activation function
[00:52:20.120 --> 00:52:24.040]   from being ReLU to something more smooth like Swish.
[00:52:24.040 --> 00:52:27.040]   So I did play around with Swish then.
[00:52:27.040 --> 00:52:31.000]   And in some of my problems, Swish
[00:52:31.000 --> 00:52:35.360]   didn't perform as well as I would have wanted.
[00:52:35.360 --> 00:52:39.040]   So I basically tried to formulate functions
[00:52:39.040 --> 00:52:43.960]   which basically represent the profile that Swish has,
[00:52:43.960 --> 00:52:46.000]   but at the same time, performs much better.
[00:52:46.000 --> 00:52:48.720]   So as you can see in this graph, the formulations
[00:52:48.720 --> 00:52:51.080]   that I came up with are shown in the graph.
[00:52:51.080 --> 00:52:53.360]   So I came up with mostly a combination
[00:52:53.360 --> 00:52:58.720]   of TAN, Hypotenuse, and Softplus because I found out
[00:52:58.720 --> 00:53:01.200]   that their combinations really provide
[00:53:01.200 --> 00:53:03.680]   a lot of different varieties of activation functions
[00:53:03.680 --> 00:53:05.520]   which can perform really well.
[00:53:05.520 --> 00:53:09.160]   And I did a search on that space.
[00:53:09.160 --> 00:53:10.680]   And from that search, I found out
[00:53:10.680 --> 00:53:14.720]   that MISH was robust across all the tasks
[00:53:14.720 --> 00:53:17.880]   that I trained, all the different activation functions
[00:53:17.880 --> 00:53:19.320]   that I found in that search.
[00:53:19.320 --> 00:53:22.760]   And also, it was consistently better
[00:53:22.760 --> 00:53:24.520]   than what Swish was performing.
[00:53:24.520 --> 00:53:29.120]   So yeah, that's the starting point
[00:53:29.120 --> 00:53:32.720]   of where I basically found the functional form of MISH.
[00:53:32.720 --> 00:53:35.480]   Although there has been future work on top of it
[00:53:35.480 --> 00:53:39.840]   to introduce a beta parameter into the Softplus
[00:53:39.840 --> 00:53:43.600]   component of MISH to allow it to be trainable
[00:53:43.600 --> 00:53:46.520]   so that the non-monotonic depth that we
[00:53:46.520 --> 00:53:48.920]   are seeing in the negative side can be controlled
[00:53:48.920 --> 00:53:51.120]   and can be trained inherently by the model.
[00:53:51.120 --> 00:53:53.480]   But this is the general interpretation.
[00:53:53.480 --> 00:53:55.920]   You can obviously introduce that beta parameter,
[00:53:55.920 --> 00:53:58.480]   but it will obviously come up with this cost.
[00:53:58.480 --> 00:54:02.960]   And personally, I haven't seen much difference
[00:54:02.960 --> 00:54:06.080]   between having the beta parameter introduced
[00:54:06.080 --> 00:54:09.720]   into the functional form, both in MISH and Swish.
[00:54:09.720 --> 00:54:14.760]   So yeah, I hope that answers the question of where
[00:54:14.760 --> 00:54:18.400]   I got the motivation behind designing
[00:54:18.400 --> 00:54:19.640]   the functional form of MISH.
[00:54:19.640 --> 00:54:25.240]   OK.
[00:54:25.240 --> 00:54:27.600]   Is there any other questions?
[00:54:27.600 --> 00:54:28.120]   Yeah.
[00:54:28.120 --> 00:54:30.480]   I actually had one question I wanted to ask about these.
[00:54:30.480 --> 00:54:33.600]   So the loss landscape is, in the end,
[00:54:33.600 --> 00:54:35.960]   really a very high-dimensional object, right?
[00:54:35.960 --> 00:54:38.000]   So there's probably tens of thousands or hundreds
[00:54:38.000 --> 00:54:40.000]   of thousands, maybe millions of parameters
[00:54:40.000 --> 00:54:42.960]   in the neural network that you're considering here.
[00:54:42.960 --> 00:54:45.240]   And right now, we're looking at the loss landscape
[00:54:45.240 --> 00:54:46.560]   in two directions.
[00:54:46.560 --> 00:54:49.920]   So I'm curious, A, where do those two directions come
[00:54:49.920 --> 00:54:51.520]   from in this particular example?
[00:54:51.520 --> 00:54:53.640]   And then B, what attempts have there
[00:54:53.640 --> 00:54:57.800]   been to look at summaries that can take all dimensions
[00:54:57.800 --> 00:55:00.840]   into account to calculate the properties of the loss
[00:55:00.840 --> 00:55:03.160]   landscape, like, for example, its condition number
[00:55:03.160 --> 00:55:05.560]   or something like that?
[00:55:05.560 --> 00:55:06.400]   Right.
[00:55:06.400 --> 00:55:09.080]   Well, that's actually very interesting
[00:55:09.080 --> 00:55:13.080]   because when I was first exploring loss landscape,
[00:55:13.080 --> 00:55:15.560]   I had the same question that it's a huge space.
[00:55:15.560 --> 00:55:19.640]   And it's probably impossible in terms
[00:55:19.640 --> 00:55:22.560]   of computational complexity that we have at hand right
[00:55:22.560 --> 00:55:25.040]   now to visualize that whole functional
[00:55:25.040 --> 00:55:27.360]   space at an efficient level.
[00:55:27.360 --> 00:55:30.880]   So the loss landscape that we are seeing over here right now
[00:55:30.880 --> 00:55:34.320]   is condensed and only representative
[00:55:34.320 --> 00:55:37.320]   of the current minima that the weights are situated at.
[00:55:37.320 --> 00:55:41.400]   And basically, we track the contour
[00:55:41.400 --> 00:55:44.760]   from the weights of the trained neural network.
[00:55:44.760 --> 00:55:47.280]   But we don't interpolate the whole space
[00:55:47.280 --> 00:55:49.640]   because if you interpolate the whole space,
[00:55:49.640 --> 00:55:53.560]   then the problem is that you would lose the details
[00:55:53.560 --> 00:55:56.440]   that we are observing over here pertaining to that local minima
[00:55:56.440 --> 00:55:57.840]   that we are settled in.
[00:55:57.840 --> 00:55:59.720]   But you'll have a general overview
[00:55:59.720 --> 00:56:01.560]   of the whole landscape.
[00:56:01.560 --> 00:56:04.520]   But if you focus only to the local minima
[00:56:04.520 --> 00:56:06.560]   that you're currently at, then you'll
[00:56:06.560 --> 00:56:08.280]   get to know actually the difference
[00:56:08.280 --> 00:56:12.080]   between the geometric structure between the two
[00:56:12.080 --> 00:56:15.640]   landscapes around that local minima
[00:56:15.640 --> 00:56:20.480]   that the network is currently optimized to.
[00:56:20.480 --> 00:56:26.160]   So I think this was put forth in Tom Goldstein papers
[00:56:26.160 --> 00:56:29.160]   at NeurIPS, where he basically investigated
[00:56:29.160 --> 00:56:32.480]   the difference of adding a skip connection
[00:56:32.480 --> 00:56:34.400]   and not adding a skip connection, which
[00:56:34.400 --> 00:56:35.720]   is the motivation behind ResNet.
[00:56:35.720 --> 00:56:38.680]   And we saw a drastic change in terms
[00:56:38.680 --> 00:56:41.560]   of the smoothness of the landscape,
[00:56:41.560 --> 00:56:44.880]   while the one which was not having the skip connection
[00:56:44.880 --> 00:56:46.320]   was pretty rough.
[00:56:46.320 --> 00:56:48.480]   The one which had the skip connection
[00:56:48.480 --> 00:56:52.040]   was extremely smooth and easy to optimize and also
[00:56:52.040 --> 00:56:52.840]   generalize better.
[00:56:52.840 --> 00:56:54.600]   And I think that speaks for itself
[00:56:54.600 --> 00:56:57.360]   that residual connections have been
[00:56:57.360 --> 00:57:00.920]   employed in mostly every given deep neural network
[00:57:00.920 --> 00:57:01.600]   architecture.
[00:57:01.600 --> 00:57:06.520]   So this is still very new, because lost landscapes
[00:57:06.520 --> 00:57:10.960]   aren't potentially explored to the possible limit.
[00:57:10.960 --> 00:57:13.640]   And probably because people think
[00:57:13.640 --> 00:57:15.960]   that we are losing out on a lot of context
[00:57:15.960 --> 00:57:18.680]   by not visualizing the whole space.
[00:57:18.680 --> 00:57:20.520]   But again, it's understandable that we
[00:57:20.520 --> 00:57:22.160]   have to do dimensionality reduction
[00:57:22.160 --> 00:57:24.800]   to condense the space into this representation form
[00:57:24.800 --> 00:57:28.080]   where we are only visualizing the local minima.
[00:57:28.080 --> 00:57:34.080]   Because there are points which exist in a lost landscape
[00:57:34.080 --> 00:57:35.600]   that you can probably connect with.
[00:57:35.600 --> 00:57:38.280]   I think this is relevant to generative neural networks
[00:57:38.280 --> 00:57:40.800]   where you have more connectivity.
[00:57:40.800 --> 00:57:43.440]   But in this case where we are visualizing activation
[00:57:43.440 --> 00:57:47.240]   functions, we only wanted to see the lost landscape surrounding
[00:57:47.240 --> 00:57:50.680]   that minima, because that gives a sense of how easy or difficult
[00:57:50.680 --> 00:57:52.280]   the optimization process was.
[00:57:56.000 --> 00:57:58.560]   So that's probably the reason why
[00:57:58.560 --> 00:58:02.800]   we condense it to such a small area of localization
[00:58:02.800 --> 00:58:06.160]   rather than having a very broader bird's eye view.
[00:58:06.160 --> 00:58:18.000]   So we did differentiate between ReLU and MISH,
[00:58:18.000 --> 00:58:21.200]   which is smooth and non-smooth.
[00:58:21.200 --> 00:58:24.680]   But what's the differentiation between two smooth activation
[00:58:24.680 --> 00:58:27.240]   functions themselves like MISH and SWISH?
[00:58:27.240 --> 00:58:28.880]   So it's kind of tricky.
[00:58:28.880 --> 00:58:30.640]   But in this case, as you can see,
[00:58:30.640 --> 00:58:34.680]   MISH has a much more wider and flatter minima
[00:58:34.680 --> 00:58:37.960]   as compared to SWISH, which is a bit short.
[00:58:37.960 --> 00:58:40.920]   And that has been proven that wider minima actually
[00:58:40.920 --> 00:58:43.720]   leads to better generalization.
[00:58:43.720 --> 00:58:48.480]   And it also speaks for the fact that in our experiments,
[00:58:48.480 --> 00:58:52.280]   we saw that MISH actually was performing consistently
[00:58:52.280 --> 00:58:58.440]   better than SWISH, and that was validated in this visualization.
[00:58:58.440 --> 00:59:04.680]   So why are we discussing about smooth activation functions?
[00:59:04.680 --> 00:59:08.000]   It's not only in the context of performance
[00:59:08.000 --> 00:59:10.360]   in terms of accuracy or generalization
[00:59:10.360 --> 00:59:12.600]   of the neural network, but it actually also
[00:59:12.600 --> 00:59:17.440]   has a huge role in adversarial training of neural networks.
[00:59:17.440 --> 00:59:20.200]   So what's adversarial training?
[00:59:20.200 --> 00:59:23.360]   It's basically we have this image,
[00:59:23.360 --> 00:59:26.080]   and we have corrupted it by some form of perturbation.
[00:59:26.080 --> 00:59:30.240]   And then the network fails to correctly predict
[00:59:30.240 --> 00:59:33.400]   that image, which it was earlier before the perturbation.
[00:59:33.400 --> 00:59:36.040]   But as you can see over in this example here,
[00:59:36.040 --> 00:59:39.880]   the images look pretty much identical to at least
[00:59:39.880 --> 00:59:40.480]   the human eye.
[00:59:40.480 --> 00:59:43.120]   And a human can visually interpret
[00:59:43.120 --> 00:59:46.000]   that both the images are of the same class,
[00:59:46.000 --> 00:59:48.800]   but a neural network fails to do so.
[00:59:48.800 --> 00:59:52.400]   So looks don't matter in terms of neural networks,
[00:59:52.400 --> 00:59:56.160]   as long as you do not corrupt the data
[00:59:56.160 --> 00:59:58.200]   and give clean data to the model, which
[00:59:58.200 --> 01:00:02.240]   can clearly interpret with a much higher accuracy
[01:00:02.240 --> 01:00:06.360]   than ones which are adversarially corrupted.
[01:00:06.360 --> 01:00:10.240]   So why are we discussing about adversarial robustness?
[01:00:10.240 --> 01:00:12.120]   It's because there exists this notion
[01:00:12.120 --> 01:00:16.400]   that generalization is inversely proportional to robustness.
[01:00:16.400 --> 01:00:18.880]   So usually neural networks, which
[01:00:18.880 --> 01:00:23.160]   are adversarially trained with adversarial samples,
[01:00:23.160 --> 01:00:26.360]   do have the incurring cost that they go down
[01:00:26.360 --> 01:00:28.800]   in the generalization capacity.
[01:00:28.800 --> 01:00:31.480]   So they lose a few percent of the accuracy,
[01:00:31.480 --> 01:00:35.800]   but improve in terms of their robustness.
[01:00:35.800 --> 01:00:39.240]   Why is it exactly happening was probably
[01:00:39.240 --> 01:00:44.600]   given in very extensive detail by this paper by Kahn and Allen,
[01:00:44.600 --> 01:00:47.400]   where they basically investigated
[01:00:47.400 --> 01:00:49.800]   into the frequency components of the input
[01:00:49.800 --> 01:00:51.760]   that the neural network was trained on.
[01:00:51.760 --> 01:00:55.760]   So as you can see in the first figure,
[01:00:55.760 --> 01:00:58.640]   the actual natural image is being
[01:00:58.640 --> 01:01:01.240]   predicted by the network, which is a ResNet-18 in this case,
[01:01:01.240 --> 01:01:02.760]   to be a mobile.
[01:01:02.760 --> 01:01:08.640]   But when you reconstruct that image from the low frequency
[01:01:08.640 --> 01:01:13.640]   components only, which is visually highly correlated
[01:01:13.640 --> 01:01:16.400]   to the actual image, the network fails
[01:01:16.400 --> 01:01:19.480]   to predict the correct class and actually predicts
[01:01:19.480 --> 01:01:21.600]   it to be a frog in this case.
[01:01:21.600 --> 01:01:25.440]   But as you can see, if you put a human in this case,
[01:01:25.440 --> 01:01:28.960]   he or she would be able to correctly classify
[01:01:28.960 --> 01:01:32.400]   both the original image and the low frequency reconstructed
[01:01:32.400 --> 01:01:35.080]   image to be the same natural class, which
[01:01:35.080 --> 01:01:36.960]   is mobile in this case.
[01:01:36.960 --> 01:01:41.920]   But as you see over here, if we do reconstruction only
[01:01:41.920 --> 01:01:44.320]   from the high frequency components,
[01:01:44.320 --> 01:01:49.120]   the neural network is still able to maintain its generalization
[01:01:49.120 --> 01:01:51.800]   capacity on that training sample and correctly predicts it
[01:01:51.800 --> 01:01:54.320]   to be mobile in this case.
[01:01:54.320 --> 01:01:58.080]   And that's quite striking, because we
[01:01:58.080 --> 01:02:02.280]   see that this high frequency reconstructed image clearly
[01:02:02.280 --> 01:02:07.200]   doesn't visually correlate with the original image
[01:02:07.200 --> 01:02:09.360]   that we had from the training data set.
[01:02:09.360 --> 01:02:13.040]   And why is that exactly happening
[01:02:13.040 --> 01:02:16.040]   is provided, as I said, in extensive details
[01:02:16.040 --> 01:02:16.960]   in that paper.
[01:02:16.960 --> 01:02:19.640]   But how is this correlated to robustness?
[01:02:19.640 --> 01:02:24.200]   If you see the bottom charts that are shown,
[01:02:24.200 --> 01:02:28.520]   you see on the left side, we have the kernels of a layer
[01:02:28.520 --> 01:02:31.200]   from the convolutional neural network, which is a ResNet-18,
[01:02:31.200 --> 01:02:33.520]   which is not adversarially trained.
[01:02:33.520 --> 01:02:36.960]   And on the right side, we have the same layer
[01:02:36.960 --> 01:02:40.200]   from that same network, but it is adversarially trained.
[01:02:40.200 --> 01:02:43.880]   So we can see that adversarially trained neural networks
[01:02:43.880 --> 01:02:47.600]   have higher robustness, but they lose the generalization
[01:02:47.600 --> 01:02:51.680]   capacity because the kernels that are learned
[01:02:51.680 --> 01:02:54.520]   get smoother, which essentially fail to capture
[01:02:54.520 --> 01:02:56.240]   the high frequency components, which
[01:02:56.240 --> 01:02:58.560]   is essential to maintain the generalization
[01:02:58.560 --> 01:03:00.240]   capacity of the neural network.
[01:03:00.240 --> 01:03:02.980]   So this kind of gives us a good sense
[01:03:02.980 --> 01:03:06.440]   about why there is an inverse proportionality
[01:03:06.440 --> 01:03:08.400]   between generalization and robustness.
[01:03:08.400 --> 01:03:17.360]   But the caveat around that is that we can actually
[01:03:17.360 --> 01:03:20.240]   bypass that inverse proportionality
[01:03:20.240 --> 01:03:23.520]   by using smooth activation functions.
[01:03:23.520 --> 01:03:27.920]   And we can actually see in this result
[01:03:27.920 --> 01:03:32.560]   over here that using smooth activation functions not only
[01:03:32.560 --> 01:03:35.560]   just improve the generalization capacity of the neural network,
[01:03:35.560 --> 01:03:38.760]   but also improve its adversarial robustness.
[01:03:38.760 --> 01:03:41.240]   So it's a win-win situation.
[01:03:41.240 --> 01:03:46.720]   And this happens because the activation functions do not
[01:03:46.720 --> 01:03:52.120]   put a constraint on the kernels to only learn smooth features
[01:03:52.120 --> 01:03:56.160]   and thus will not be able to capture high frequency
[01:03:56.160 --> 01:03:57.160]   components.
[01:03:57.160 --> 01:04:00.360]   But the convolution kernels can still
[01:04:00.360 --> 01:04:02.800]   get the actual feature representation, which
[01:04:02.800 --> 01:04:06.000]   is in the case of a normal neural network training,
[01:04:06.000 --> 01:04:08.680]   and are able to capture the high frequency components, which
[01:04:08.680 --> 01:04:10.720]   thus preserves this generalization
[01:04:10.720 --> 01:04:13.920]   or even improves it when we use smooth activation
[01:04:13.920 --> 01:04:18.760]   functions like MeshWish or SmoothRallu or SoftPlus.
[01:04:18.760 --> 01:04:21.880]   In this paper by Google, where they propose
[01:04:21.880 --> 01:04:24.040]   smooth adversarial training, they actually
[01:04:24.040 --> 01:04:26.320]   discussed various settings of how
[01:04:26.320 --> 01:04:31.280]   you can improve not only the robustness, but also accuracy.
[01:04:31.280 --> 01:04:33.440]   But at least the constraint is that do not
[01:04:33.440 --> 01:04:36.400]   drop the accuracy as we improve the robustness.
[01:04:36.400 --> 01:04:39.520]   And that's basically called free lunch in adversarial robustness.
[01:04:39.520 --> 01:04:42.680]   So you essentially don't incur any cost
[01:04:42.680 --> 01:04:46.360]   of improving your model's robustness in its accuracy
[01:04:46.360 --> 01:04:47.320]   metric.
[01:04:47.320 --> 01:04:50.800]   And they did that by basically keeping the forward pass
[01:04:50.800 --> 01:04:54.000]   of the network to be Rallu, but change the backward pass
[01:04:54.000 --> 01:04:56.560]   to be that of a smoother activation function.
[01:04:56.560 --> 01:04:58.920]   In that case, it was activation function
[01:04:58.920 --> 01:05:01.920]   that they introduced called as SmoothRallu.
[01:05:01.920 --> 01:05:05.040]   So definitely would recommend to check this paper out,
[01:05:05.040 --> 01:05:08.080]   because this provides a very interesting insight
[01:05:08.080 --> 01:05:12.040]   onto why the inverse proportionality
[01:05:12.040 --> 01:05:14.360]   between generalization and robustness
[01:05:14.360 --> 01:05:17.200]   might not always be true.
[01:05:17.200 --> 01:05:19.480]   And there is definitely a work around it.
[01:05:19.480 --> 01:05:22.800]   And the solution is use smoother activation functions.
[01:05:25.720 --> 01:05:29.640]   So based on all the things we have discussed so far,
[01:05:29.640 --> 01:05:33.200]   we'll probably move to another domain
[01:05:33.200 --> 01:05:35.680]   which is kind of gaining a lot of popularity
[01:05:35.680 --> 01:05:38.480]   in these days, which is continual learning.
[01:05:38.480 --> 01:05:42.440]   And that is motivated from this scenario
[01:05:42.440 --> 01:05:45.720]   that occurs in neural networks called as catastrophe
[01:05:45.720 --> 01:05:46.560]   forgetting.
[01:05:46.560 --> 01:05:50.360]   So basically what happens if you have a neural network being
[01:05:50.360 --> 01:05:54.720]   trained sequentially on n number of tasks,
[01:05:54.720 --> 01:05:58.080]   the network will perform well on the current task
[01:05:58.080 --> 01:05:59.240]   that it has been trained on,
[01:05:59.240 --> 01:06:04.240]   but it would have forgot the dataset
[01:06:04.240 --> 01:06:06.840]   that it was trained on in the previous task.
[01:06:06.840 --> 01:06:10.080]   And thus it was not able to maintain the generalization
[01:06:10.080 --> 01:06:15.080]   on the n minus one task that it had been through before.
[01:06:15.080 --> 01:06:20.440]   So this is not really a good problem to have around
[01:06:22.740 --> 01:06:24.860]   because essentially we want our neural networks
[01:06:24.860 --> 01:06:29.260]   to be able to have lifelong learning capacity in them
[01:06:29.260 --> 01:06:32.260]   so that we can introduce new tasks to the same network
[01:06:32.260 --> 01:06:35.880]   and ensure that the network doesn't forget
[01:06:35.880 --> 01:06:38.340]   the previous task that it had earlier trained on.
[01:06:38.340 --> 01:06:43.740]   So why is this important in the context
[01:06:43.740 --> 01:06:45.420]   that we are discussing right now?
[01:06:45.420 --> 01:06:48.620]   Before we get to that answer,
[01:06:48.620 --> 01:06:51.340]   just gonna quickly go through the current scenarios
[01:06:51.340 --> 01:06:53.060]   that we have in continual learning.
[01:06:53.060 --> 01:06:57.580]   So let's say we have a dataset which is pertaining
[01:06:57.580 --> 01:07:00.020]   towards task one for the neural network.
[01:07:00.020 --> 01:07:02.980]   And then we have trained the neural network
[01:07:02.980 --> 01:07:05.580]   on that task one and it performs fairly well.
[01:07:05.580 --> 01:07:09.540]   And then we decide to shift to a new dataset for task two.
[01:07:09.540 --> 01:07:13.700]   We don't want our network to be too plastic,
[01:07:13.700 --> 01:07:17.740]   which means that the network completely forgets the task one
[01:07:17.740 --> 01:07:20.700]   and just generalizes well to task two.
[01:07:20.700 --> 01:07:23.200]   We don't want it to be too rigid as well,
[01:07:23.200 --> 01:07:27.580]   which means that the network not only forgets task one,
[01:07:27.580 --> 01:07:29.180]   but in the process also is not able
[01:07:29.180 --> 01:07:30.420]   to generalize to task two.
[01:07:30.420 --> 01:07:33.380]   So it hangs in the mid-ridge
[01:07:33.380 --> 01:07:37.300]   and is not able to work well in either of the tasks.
[01:07:37.300 --> 01:07:41.220]   We want basically to maintain a good stability
[01:07:41.220 --> 01:07:43.100]   and plasticity of the neural network,
[01:07:43.100 --> 01:07:44.700]   which basically means that the network
[01:07:44.700 --> 01:07:47.940]   should be able to learn new tasks efficiently,
[01:07:47.940 --> 01:07:50.060]   but also should be able to retain memory
[01:07:50.060 --> 01:07:53.060]   of the previous tasks that it had been trained on.
[01:07:53.060 --> 01:07:56.460]   And overall should be able to generalize pretty well.
[01:07:56.460 --> 01:08:01.120]   So based on this, what are we getting towards?
[01:08:01.120 --> 01:08:04.380]   So the stability plasticity dilemma is basically
[01:08:04.380 --> 01:08:07.540]   that we have a neural network which might be too plastic,
[01:08:07.540 --> 01:08:09.940]   which means that it forgets the previous task,
[01:08:09.940 --> 01:08:11.900]   or it might be too stable.
[01:08:11.900 --> 01:08:16.420]   That means it basically is not able
[01:08:16.420 --> 01:08:18.660]   to get a good representation for all the tasks
[01:08:18.660 --> 01:08:23.380]   that it is learning on and like kind of hangs in between.
[01:08:23.380 --> 01:08:27.940]   So what my hypothesis around this concept
[01:08:27.940 --> 01:08:31.900]   is that there exists one more parameter in that criterion,
[01:08:31.900 --> 01:08:33.540]   which is robustness.
[01:08:33.540 --> 01:08:36.940]   So I basically call it a stability plasticity
[01:08:36.940 --> 01:08:38.660]   robustness dilemma.
[01:08:38.660 --> 01:08:42.100]   So the hypothesis is as such that if you have a neural
[01:08:42.100 --> 01:08:44.820]   network which is being trained in continual learning settings
[01:08:44.820 --> 01:08:47.340]   and there exists a lot of them,
[01:08:47.340 --> 01:08:49.380]   which have successfully demonstrated
[01:08:49.380 --> 01:08:54.020]   that they can tackle the problem of catastrophe forgetting
[01:08:54.020 --> 01:08:56.220]   by retaining the generalization capacity
[01:08:56.220 --> 01:08:57.780]   of the neural network on the current
[01:08:57.780 --> 01:08:59.340]   and as well as the previous tasks
[01:08:59.340 --> 01:09:02.500]   which on which the network has been trained.
[01:09:02.500 --> 01:09:06.860]   But the question is, do these training settings
[01:09:06.860 --> 01:09:10.460]   also able to retain the robustness of the model
[01:09:10.460 --> 01:09:12.420]   and not only the generalization.
[01:09:12.420 --> 01:09:17.780]   And if that's so, does there exist an inverse proportionality
[01:09:17.780 --> 01:09:20.060]   between the generalization and robustness
[01:09:20.060 --> 01:09:21.980]   in continual learning settings as well
[01:09:21.980 --> 01:09:24.900]   as they exist in normal neural networks.
[01:09:24.900 --> 01:09:27.460]   So open-ended questions that I would definitely
[01:09:27.460 --> 01:09:31.580]   encourage the audience to consider experimenting
[01:09:31.580 --> 01:09:36.540]   with or brainstorming about is that if this dilemma exists,
[01:09:36.540 --> 01:09:39.540]   which is the stability plasticity robustness dilemma,
[01:09:39.540 --> 01:09:42.100]   then can smooth activation functions
[01:09:42.100 --> 01:09:46.100]   or in general nonlinear dynamics of neural networks
[01:09:46.100 --> 01:09:48.100]   the key to solving this dilemma.
[01:09:48.100 --> 01:09:51.660]   Because we saw earlier that they do
[01:09:51.660 --> 01:09:54.660]   remove that inverse proportionality
[01:09:54.660 --> 01:09:57.740]   by giving you the capacity of not only improving
[01:09:57.740 --> 01:10:00.020]   the generalization but also the robustness of the neural
[01:10:00.020 --> 01:10:05.100]   network in adversarial training settings.
[01:10:05.100 --> 01:10:09.380]   So yeah, that's it from my side.
[01:10:09.380 --> 01:10:12.260]   All the related work and code and pre-trained models
[01:10:12.260 --> 01:10:17.100]   can be made available on this repository of mine.
[01:10:17.100 --> 01:10:19.180]   And you can obviously put forth your ideas
[01:10:19.180 --> 01:10:22.420]   or on the discussion forum that I have in the repository.
[01:10:22.420 --> 01:10:25.500]   And I look forward to taking any questions.
[01:10:25.500 --> 01:10:29.380]   All right.
[01:10:29.380 --> 01:10:31.820]   Thanks, Deganta, for an interesting set
[01:10:31.820 --> 01:10:36.020]   of experiments, papers, and ideas.
[01:10:36.020 --> 01:10:40.780]   I guess the question I have is, so you presented
[01:10:40.780 --> 01:10:43.100]   some interesting intuition for how smoothness
[01:10:43.100 --> 01:10:46.420]   can help with robustness and help
[01:10:46.420 --> 01:10:50.740]   preserve the generalization capacity even when you're
[01:10:50.740 --> 01:10:52.660]   doing adversarial training.
[01:10:52.660 --> 01:10:54.220]   Do you have any intuition for why
[01:10:54.220 --> 01:10:57.260]   it might be the case that it would help with this trilemma
[01:10:57.260 --> 01:11:03.060]   of being able to still help with robustness
[01:11:03.060 --> 01:11:07.620]   even when you're in the continual learning setting?
[01:11:07.620 --> 01:11:10.340]   Well, before I answer that question,
[01:11:10.340 --> 01:11:13.740]   I would rather like to put the hypothesis in more context.
[01:11:13.740 --> 01:11:17.980]   So what can possibly happen when you are training
[01:11:17.980 --> 01:11:19.940]   in a continual learning setting?
[01:11:19.940 --> 01:11:22.980]   Let's say we have only two tasks at hand
[01:11:22.980 --> 01:11:25.220]   and we are sequentially training a deep neural network
[01:11:25.220 --> 01:11:26.660]   on both of the tasks.
[01:11:26.660 --> 01:11:30.980]   So we can expect two things that can happen
[01:11:30.980 --> 01:11:36.140]   is that the natural robustness of the model is lost,
[01:11:36.140 --> 01:11:38.260]   but the generalization is being preserved,
[01:11:38.260 --> 01:11:40.380]   which is courtesy to the continual learning setting
[01:11:40.380 --> 01:11:41.700]   that we're using.
[01:11:41.700 --> 01:11:45.940]   Or it might be the case that the robustness is preserved
[01:11:45.940 --> 01:11:50.380]   for both the tasks or even improved because of the fact
[01:11:50.380 --> 01:11:52.660]   that we are introducing more training samples.
[01:11:52.660 --> 01:11:54.660]   And in return, we would introduce more adversarial
[01:11:54.660 --> 01:11:56.820]   samples to the model.
[01:11:56.820 --> 01:12:03.380]   So the direction about how activation functions
[01:12:03.380 --> 01:12:08.500]   or, in general, smooth functions can improve the situation
[01:12:08.500 --> 01:12:13.860]   to avoid facing this dilemma is around that hypothesis
[01:12:13.860 --> 01:12:16.260]   of which of the results come to be true.
[01:12:16.260 --> 01:12:19.460]   A, that is the robustness is being lost,
[01:12:19.460 --> 01:12:22.340]   or B, the robustness is being preserved
[01:12:22.340 --> 01:12:24.860]   and is actually or even is improved because
[01:12:24.860 --> 01:12:26.700]   of the introduction of new samples.
[01:12:26.700 --> 01:12:31.260]   So I can't really say on which of the either two scenarios
[01:12:31.260 --> 01:12:32.620]   will happen.
[01:12:32.620 --> 01:12:34.180]   And we are currently working on that
[01:12:34.180 --> 01:12:37.300]   to see which of them is more relevant
[01:12:37.300 --> 01:12:39.780]   and actually happens in most of the cases.
[01:12:39.780 --> 01:12:43.420]   But my general idea is that essentially, we
[01:12:43.420 --> 01:12:48.700]   want our neural networks to be able to, as I said,
[01:12:48.700 --> 01:12:51.380]   learn in a lifelong setting and not
[01:12:51.380 --> 01:12:54.660]   have that inverse proportionality
[01:12:54.660 --> 01:12:57.740]   between the generalization and the robustness
[01:12:57.740 --> 01:12:59.660]   parameter of the network.
[01:12:59.660 --> 01:13:02.660]   And the only solution that exists
[01:13:02.660 --> 01:13:08.820]   which can give you that free lunch in adversarial training
[01:13:08.820 --> 01:13:11.540]   is smooth activation functions, as showcased in that paper.
[01:13:11.540 --> 01:13:17.180]   So my hypothesis is if it is the case that robustness is lost,
[01:13:17.180 --> 01:13:19.420]   then introducing smooth activation functions
[01:13:19.420 --> 01:13:22.420]   in the training process of the continual learning setting
[01:13:22.420 --> 01:13:26.340]   will not only help to improve the generalization
[01:13:26.340 --> 01:13:28.500]   capability of the network, but also
[01:13:28.500 --> 01:13:32.180]   help to retain the natural robustness of the model.
[01:13:32.180 --> 01:13:37.820]   But the context of it changes if the other side of the coin
[01:13:37.820 --> 01:13:42.180]   happens, which is also very interesting to know why.
[01:13:42.180 --> 01:13:45.940]   Obviously, it can boil down to simple reasoning
[01:13:45.940 --> 01:13:49.860]   that A, we are introducing more data points to the network
[01:13:49.860 --> 01:13:53.460]   so they are getting more robust because they're
[01:13:53.460 --> 01:13:55.140]   seeing more data points.
[01:13:55.140 --> 01:13:59.940]   But I believe that there might be more things happening
[01:13:59.940 --> 01:14:03.620]   under the hood that probably can explain either of the two
[01:14:03.620 --> 01:14:04.220]   scenarios.
[01:14:04.220 --> 01:14:06.940]   But if I am to put my bet on it, I
[01:14:06.940 --> 01:14:11.540]   would assume that robustness is being lost
[01:14:11.540 --> 01:14:13.860]   while generalization is being preserved
[01:14:13.860 --> 01:14:17.140]   in continual learning settings.
[01:14:17.140 --> 01:14:17.620]   Interesting.
[01:14:17.620 --> 01:14:19.980]   Well, I wanted to hear everything
[01:14:19.980 --> 01:14:23.460]   that you had to say, so we've gone quite a bit over time.
[01:14:23.460 --> 01:14:26.140]   So unfortunately, we can't continue discussing this.
[01:14:26.140 --> 01:14:28.540]   But I want to thank you for coming on
[01:14:28.540 --> 01:14:33.540]   and for sharing your results and your thoughts with us.
[01:14:33.540 --> 01:14:35.340]   And I look forward to talking about this
[01:14:35.340 --> 01:14:38.020]   with you more in the future.
[01:14:38.020 --> 01:14:38.780]   Absolutely.
[01:14:38.780 --> 01:14:42.700]   It was such a pleasure to be here and talk
[01:14:42.700 --> 01:14:45.780]   about my research with you, and especially, as I said,
[01:14:45.780 --> 01:14:48.820]   sharing a podium with Maitra is an absolute honor for me.
[01:14:48.820 --> 01:14:52.580]   So yeah, I would like to really, really thank you
[01:14:52.580 --> 01:14:54.260]   for providing this opportunity.
[01:14:54.260 --> 01:14:59.660]   And I look forward to talking to people who viewed this
[01:14:59.660 --> 01:15:01.980]   and who might have further questions
[01:15:01.980 --> 01:15:05.700]   or want to discuss some new ideas.
[01:15:05.700 --> 01:15:06.200]   Great.
[01:15:06.200 --> 01:15:09.540]   All right, well, to the folks on YouTube following our YouTube
[01:15:09.540 --> 01:15:11.300]   channel, make sure to leave some comments
[01:15:11.300 --> 01:15:13.580]   if you have any thoughts about this research.
[01:15:13.580 --> 01:15:16.940]   I think I'm also supposed to tell you to like and subscribe.
[01:15:16.940 --> 01:15:19.340]   I'm sort of new to this whole YouTube thing.
[01:15:19.340 --> 01:15:22.420]   But thank you all so much for coming, both to our speakers
[01:15:22.420 --> 01:15:23.380]   and to our audience.
[01:15:23.380 --> 01:15:25.900]   And I will catch you around at our future Weights
[01:15:25.900 --> 01:15:27.420]   and Biases salons.
[01:15:27.420 --> 01:15:29.140]   Take care.

