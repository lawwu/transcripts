
[00:00:00.000 --> 00:00:04.080]   One of the interesting things about it that we saw even with the 70 billion is we, we
[00:00:04.080 --> 00:00:07.520]   thought it would get more saturated, you know, it's like we train it on around 15
[00:00:07.520 --> 00:00:08.440]   trillion tokens.
[00:00:08.440 --> 00:00:15.040]   And I guess our prediction going in was that it was going to asymptote more, but even by
[00:00:15.040 --> 00:00:17.680]   the end it was still learning, right?
[00:00:17.680 --> 00:00:22.560]   It's like, we probably could have fed it more tokens and it would have gotten somewhat
[00:00:22.560 --> 00:00:22.840]   better.
[00:00:22.840 --> 00:00:26.280]   But I mean, at some point, you know, you're running a company, you need to do these
[00:00:26.280 --> 00:00:30.440]   meta reasoning questions of like, all right, how do I want to spend our GPUs on like
[00:00:30.440 --> 00:00:32.840]   training the 70 billion model further?
[00:00:32.840 --> 00:00:37.520]   Do we want to kind of get on with it so we can start testing hypotheses for LLAMA4?
[00:00:37.520 --> 00:00:40.560]   So we kind of needed to, to make, to make that call.
[00:00:40.560 --> 00:00:44.200]   And I think we got it, I think we got to a reasonable balance for, for this version of
[00:00:44.200 --> 00:00:45.280]   the 70 billion.
[00:00:45.280 --> 00:00:48.560]   That was, that was fascinating that you could just, that it's the architectures at this
[00:00:48.560 --> 00:00:50.520]   point can just take so much data.
[00:00:50.520 --> 00:00:57.160]   And I do think in the future, it seems quite possible that more of what we call training
[00:00:57.160 --> 00:01:05.000]   for these big models is actually more along the lines of inference generating synthetic
[00:01:05.000 --> 00:01:07.160]   data to then go feed into the model.
[00:01:07.160 --> 00:01:12.760]   So I don't know what that ratio is going to be, but I consider the generation of synthetic
[00:01:12.760 --> 00:01:15.120]   data to be more inference than training today.
[00:01:15.120 --> 00:01:18.520]   But obviously if you're doing it in order to train a model, it's part of the broader
[00:01:18.520 --> 00:01:19.800]   training process.
[00:01:19.800 --> 00:01:25.480]   So I don't know, that's an, that's a, an open question is to, to kind of where, what the
[00:01:25.480 --> 00:01:27.200]   balance of that and how that plays out.
[00:01:27.200 --> 00:01:31.920]   If that's the case, would that potentially also be the case with LLAMA3?
[00:01:31.920 --> 00:01:36.640]   And maybe like LLAMA4 onwards where you put this out and if somebody has a ton of compute,
[00:01:36.640 --> 00:01:40.040]   then using the models that you've put out, you can just keep making these things arbitrarily
[00:01:40.040 --> 00:01:41.040]   smarter.
[00:01:41.040 --> 00:01:47.880]   Like some Kuwait or UAE or some random country has a ton of compute and they can just actually
[00:01:47.880 --> 00:01:50.280]   just use LLAMA4 to just make something much smarter.
[00:01:50.280 --> 00:01:53.560]   I do think that there are going to be dynamics like that.
[00:01:53.560 --> 00:01:57.720]   Actually I found the synthetic data thing really curious.
[00:01:57.720 --> 00:02:01.600]   I'm actually interested in why you don't think, like current models, it makes sense why there
[00:02:01.600 --> 00:02:05.480]   might be an asymptote with just doing the synthetic data again and again, if they get
[00:02:05.480 --> 00:02:09.080]   smarter and use the kind of techniques you talk about in the paper or the blog post that's
[00:02:09.080 --> 00:02:15.120]   coming out on the day this will be released, where it, it goes to the thought chain that
[00:02:15.120 --> 00:02:17.860]   is the most correct.
[00:02:17.860 --> 00:02:22.160]   Why this wouldn't like lead to a loop that, of course it wouldn't be overnight, but over
[00:02:22.160 --> 00:02:25.880]   many months or years of training, potentially with a smarter model, it gets smarter, makes
[00:02:25.880 --> 00:02:28.080]   better output, gets smarter and so forth.
[00:02:28.080 --> 00:02:34.320]   Well, I think it could within the parameter of whatever the model architecture is.
[00:02:34.320 --> 00:02:39.160]   It's just that like at some level, I don't know.
[00:02:39.160 --> 00:02:43.960]   I think like today is 8 billion parameter models.
[00:02:43.960 --> 00:02:49.480]   I just don't think you're going to be able to get to be as good as the state of the art
[00:02:49.480 --> 00:02:54.280]   multi hundred billion parameter models that are incorporating new research into the architecture
[00:02:54.280 --> 00:02:55.400]   itself.
[00:02:55.400 --> 00:02:59.160]   When you're building software, there's like a ton of stuff that you can do with software,
[00:02:59.160 --> 00:03:03.760]   but then at some level you're constrained by the chips that it's running on, right?
[00:03:03.760 --> 00:03:09.720]   So there are always going to be different physical constraints and it's like how big
[00:03:09.720 --> 00:03:14.960]   are the models is going to be constrained by how much energy you can get and use for
[00:03:14.960 --> 00:03:16.360]   inference.
[00:03:16.360 --> 00:03:22.720]   So I guess I'm simultaneously very optimistic that this stuff will continue to improve quickly
[00:03:22.720 --> 00:03:30.600]   and also a little more measured than I think some people are about.
[00:03:30.600 --> 00:03:36.200]   I just don't think the runaway case is like a particularly likely one.
[00:03:36.200 --> 00:03:38.440]   I think it makes sense to keep your options open.
[00:03:38.440 --> 00:03:40.880]   Like there's so much we don't know.
[00:03:40.880 --> 00:03:43.960]   There's a case in which it's really important to keep the balance of power so nobody becomes
[00:03:43.960 --> 00:03:45.560]   like a totalitarian dictator.
[00:03:45.560 --> 00:03:49.200]   There's a case in which you don't want to open source the architecture because like
[00:03:49.200 --> 00:03:54.080]   China can use it to catch up to America's AIs and there is an intelligence explosion
[00:03:54.080 --> 00:03:55.080]   and they win that.
[00:03:55.080 --> 00:04:00.240]   A lot of things seem possible, just keeping your options open, considering all of them
[00:04:00.240 --> 00:04:00.720]   seems reasonable.
[00:04:00.720 --> 00:04:01.720]   [END OF TRANSCRIPT]
[00:04:01.720 --> 00:04:01.720]   1
[00:04:01.720 --> 00:04:03.780]   you
[00:04:03.780 --> 00:04:13.780]   [BLANK_AUDIO]

