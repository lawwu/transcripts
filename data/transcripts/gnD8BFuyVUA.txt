
[00:00:00.000 --> 00:00:03.920]   [MUSIC PLAYING]
[00:00:03.920 --> 00:00:07.720]   Hi, so this is my semantic segmentation project.
[00:00:07.720 --> 00:00:11.600]   It's using UNET with different encoders and FastAI
[00:00:11.600 --> 00:00:13.960]   to identify pixels of a photo as belonging
[00:00:13.960 --> 00:00:19.320]   to a car, a human, buildings in the background, road, et cetera.
[00:00:19.320 --> 00:00:21.360]   And here you can see some examples.
[00:00:21.360 --> 00:00:24.920]   I have the raw photo, the model's prediction,
[00:00:24.920 --> 00:00:26.400]   and then the ground truth.
[00:00:26.400 --> 00:00:29.760]   In my workspace, you can see 395 runs visualized.
[00:00:29.760 --> 00:00:32.040]   That's how many different experiments I've tried.
[00:00:32.040 --> 00:00:34.520]   You can see a bunch of metrics that I care about,
[00:00:34.520 --> 00:00:38.320]   like the overall accuracy, the accuracy specifically
[00:00:38.320 --> 00:00:39.080]   for traffic.
[00:00:39.080 --> 00:00:44.160]   So that's light posts, lights, traffic signs, and signals.
[00:00:44.160 --> 00:00:46.840]   You can also see the validation loss and training loss.
[00:00:46.840 --> 00:00:50.760]   And all of these seem to be doing pretty nicely.
[00:00:50.760 --> 00:00:55.380]   Here I can also expand my table to see the other metrics
[00:00:55.380 --> 00:00:59.880]   that I log-- road accuracy, car accuracy, validation loss,
[00:00:59.880 --> 00:01:00.980]   encoder type, et cetera.
[00:01:00.980 --> 00:01:04.540]   And I can take notes on individual runs here as well.
[00:01:04.540 --> 00:01:07.160]   So right now, my runs are sorted by overall accuracy.
[00:01:07.160 --> 00:01:11.180]   But let's say I want to find out what the most accurate run is
[00:01:11.180 --> 00:01:13.100]   on detecting traffic.
[00:01:13.100 --> 00:01:16.660]   And that looks like it's EfficientShape156,
[00:01:16.660 --> 00:01:18.420]   which I can click through.
[00:01:18.420 --> 00:01:21.960]   And here you can see that the training loss and validation
[00:01:21.960 --> 00:01:26.820]   loss for that run are decreasing nicely and smoothly.
[00:01:26.820 --> 00:01:31.940]   And the overall accuracy in red, the car accuracy in orange,
[00:01:31.940 --> 00:01:33.300]   and the traffic accuracy in blue,
[00:01:33.300 --> 00:01:36.140]   you can see that the overall is in between those two.
[00:01:36.140 --> 00:01:37.780]   It's better at cars than traffic,
[00:01:37.780 --> 00:01:40.660]   because cars are bigger, and there's more of them.
[00:01:40.660 --> 00:01:42.740]   And the lines for traffic signs and posts
[00:01:42.740 --> 00:01:45.580]   are pretty small details.
[00:01:45.580 --> 00:01:48.600]   And unfortunately, you can see that the human accuracy
[00:01:48.600 --> 00:01:52.100]   is stuck at zero here, which is a little surprising.
[00:01:52.100 --> 00:01:57.180]   Some things I can do is inspect the validation data examples.
[00:01:57.180 --> 00:02:01.860]   And here again, you can see the raw photo, the prediction,
[00:02:01.860 --> 00:02:03.580]   and then the ground truth.
[00:02:03.580 --> 00:02:06.660]   And as we scroll through these, maybe you
[00:02:06.660 --> 00:02:09.940]   will notice that there aren't very many examples of humans,
[00:02:09.940 --> 00:02:12.660]   at least in this random sample.
[00:02:12.660 --> 00:02:15.180]   There are some in this image here.
[00:02:15.180 --> 00:02:17.940]   And overall, though, the model is doing very well.
[00:02:17.940 --> 00:02:21.080]   Perhaps it's just not seeing enough humans
[00:02:21.080 --> 00:02:23.980]   to get a good sense of them.
[00:02:23.980 --> 00:02:26.900]   I can also see a bunch more details about this run.
[00:02:26.900 --> 00:02:30.060]   I can see where it ran, what the git state was,
[00:02:30.060 --> 00:02:33.660]   and even I can return to that git state easily.
[00:02:33.660 --> 00:02:36.220]   So I don't need to remember what exact version of the code
[00:02:36.220 --> 00:02:37.420]   it was.
[00:02:37.420 --> 00:02:40.660]   I can see all of my initial settings, my configuration,
[00:02:40.660 --> 00:02:44.620]   such as learning rate, number of training stages, weight decay,
[00:02:44.620 --> 00:02:46.100]   and all of my results here.
[00:02:46.100 --> 00:02:50.460]   So all of those class-specific accuracies that I talked about.
[00:02:50.460 --> 00:02:55.420]   I can also see how my GPU was working while I trained this.
[00:02:55.420 --> 00:02:58.740]   So time spent accessing memory, percent utilization.
[00:02:58.740 --> 00:03:00.180]   This is actually doing pretty well.
[00:03:00.180 --> 00:03:02.860]   It's showing both the GPUs on my system.
[00:03:02.860 --> 00:03:05.740]   But you can see that there's some noisiness.
[00:03:05.740 --> 00:03:09.620]   And in general, I could use this to improve my efficiency
[00:03:09.620 --> 00:03:11.060]   on my GPUs.
[00:03:11.060 --> 00:03:13.620]   You can also see a full description
[00:03:13.620 --> 00:03:15.420]   of the different layers of the model, which
[00:03:15.420 --> 00:03:17.060]   is really useful for debugging, including
[00:03:17.060 --> 00:03:19.860]   the number of parameters and the shapes.
[00:03:19.860 --> 00:03:22.860]   You can see all my terminal logs, any files
[00:03:22.860 --> 00:03:24.980]   that I saved with this run.
[00:03:24.980 --> 00:03:30.380]   And one last awesome detail you can see is the gradients here.
[00:03:30.380 --> 00:03:33.540]   So the weights and biases over time.
[00:03:33.540 --> 00:03:37.620]   You can see how some of their distributions converge to zero,
[00:03:37.620 --> 00:03:40.300]   how some of them are more spread out.
[00:03:40.300 --> 00:03:47.980]   And you can also even search for any particular distribution
[00:03:47.980 --> 00:03:51.180]   if you want to check out a specific layer.
[00:03:51.780 --> 00:03:55.140]   [MUSIC PLAYING]
[00:03:55.500 --> 00:03:58.540]   [MUSIC ENDS]
[00:03:58.540 --> 00:04:03.540]   [MUSIC]

