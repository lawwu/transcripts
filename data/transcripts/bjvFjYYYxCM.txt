
[00:00:00.000 --> 00:00:10.680]   Welcome to this webinar on collaboration for ML teams with Weights & Biases.
[00:00:10.680 --> 00:00:16.000]   The quick agenda, I'll do a quick intro, explain who I am, who Weights & Biases are,
[00:00:16.000 --> 00:00:18.840]   and then what Weights & Biases is in a nutshell.
[00:00:18.840 --> 00:00:20.640]   And then we'll talk about collaboration.
[00:00:20.640 --> 00:00:23.880]   So we'll talk about collaboration using Weights & Biases dashboards.
[00:00:23.880 --> 00:00:27.800]   And then we'll talk about collaboration using Weights & Biases reports.
[00:00:28.040 --> 00:00:30.400]   I'm going to do a little bit of a demo for both of those things.
[00:00:30.400 --> 00:00:32.360]   We'll see some dashboards and reports.
[00:00:32.360 --> 00:00:34.200]   How they work, how they're built.
[00:00:34.200 --> 00:00:39.680]   I used to do research in biology and psychology, and then did my PhD on
[00:00:39.680 --> 00:00:41.600]   neural networks and machine learning.
[00:00:41.600 --> 00:00:45.760]   And when it came time for me to work in machine learning, the question that I
[00:00:45.760 --> 00:00:52.080]   asked coming from my other fields of research was what are the best practices
[00:00:52.080 --> 00:00:53.720]   for research in machine learning?
[00:00:53.720 --> 00:00:56.480]   How do people think about and organize their experiments?
[00:00:56.800 --> 00:01:00.960]   Just as an example for things like studying the effects of drugs in
[00:01:00.960 --> 00:01:05.480]   humans in my research on MDMA, there were tons of protocols around how to
[00:01:05.480 --> 00:01:08.920]   manage human experimental data, how it should be stored, how it should be
[00:01:08.920 --> 00:01:13.920]   shared and lots of really good practices and ideas around how that should be
[00:01:13.920 --> 00:01:16.880]   done working in a large group, as you have to do when you're working
[00:01:16.880 --> 00:01:18.080]   in a scientific laboratory.
[00:01:18.080 --> 00:01:22.840]   So I wanted to find those best practices for research in machine learning.
[00:01:22.840 --> 00:01:26.000]   And when I asked around, what I found the answer was, is that it was the
[00:01:26.000 --> 00:01:30.560]   wild west and that there wasn't really a best practice, there weren't really
[00:01:30.560 --> 00:01:32.560]   good ideas about how to do things.
[00:01:32.560 --> 00:01:37.760]   If weights and biases is anything, I think of it as the sheriff in town
[00:01:37.760 --> 00:01:42.760]   of machine learning come here to clean up the ne'er do welts in the saloon
[00:01:42.760 --> 00:01:44.440]   that is machine learning research.
[00:01:44.440 --> 00:01:48.720]   So the three sort of key value propositions of this system of
[00:01:48.720 --> 00:01:52.400]   record are reproducibility, visualization, and quick integration.
[00:01:52.400 --> 00:01:55.800]   So reproducibility is something that maybe a lot of systems of record
[00:01:55.800 --> 00:02:00.040]   might bring data lineage and Providence version control of your pipeline,
[00:02:00.040 --> 00:02:03.040]   being able to create a central registry of all your trained models.
[00:02:03.040 --> 00:02:06.000]   This is something that you might be able to achieve with a Git repository,
[00:02:06.000 --> 00:02:08.080]   maybe a well-designed spreadsheet.
[00:02:08.080 --> 00:02:12.000]   But the two things I think weights and biases specifically combines with this
[00:02:12.000 --> 00:02:16.360]   and finds lots of excellent synergies is in visualization and
[00:02:16.360 --> 00:02:18.320]   in making that process easy.
[00:02:18.320 --> 00:02:21.400]   So by visualization, some of the things we're going to really focus on today,
[00:02:21.400 --> 00:02:24.840]   like live dashboards that you can do quick experimentation
[00:02:25.000 --> 00:02:26.240]   and collaboration.
[00:02:26.240 --> 00:02:30.280]   This also allows you to sort of manage visibility into those ML projects
[00:02:30.280 --> 00:02:33.920]   across research team stakeholders, maybe a broader community that
[00:02:33.920 --> 00:02:34.640]   you want to share with.
[00:02:34.640 --> 00:02:37.760]   And all of this is done in a manner that allows very quick
[00:02:37.760 --> 00:02:39.440]   integration to existing code.
[00:02:39.440 --> 00:02:40.680]   Our systems are modular.
[00:02:40.680 --> 00:02:41.840]   They have a lightweight setup.
[00:02:41.840 --> 00:02:44.840]   They're designed to play nicely with other features you might
[00:02:44.840 --> 00:02:46.280]   use and never lock you in.
[00:02:46.280 --> 00:02:49.360]   We don't think of ourselves necessarily so much as a platform as a toolkit.
[00:02:49.360 --> 00:02:53.120]   And we built this user experience, both in terms of visualization and in
[00:02:53.120 --> 00:02:57.640]   terms of the integration in partnership with leading research institutions.
[00:02:57.640 --> 00:03:02.000]   That modular toolkit basically comprises our dashboard tool and our reports
[00:03:02.000 --> 00:03:05.480]   tool for experiment tracking and collaboration and reproducibility.
[00:03:05.480 --> 00:03:06.360]   Respectively.
[00:03:06.360 --> 00:03:08.680]   Those are the two things we're going to focus on the most here.
[00:03:08.680 --> 00:03:13.560]   It also includes a tool for hyper parameter optimization and organizing
[00:03:13.560 --> 00:03:17.280]   large sweeps of runs and also for.
[00:03:17.280 --> 00:03:20.760]   Building that registry of models and data sets with the artifacts tools.
[00:03:20.760 --> 00:03:23.920]   We won't focus as much on those today, but we'll, and we're going to
[00:03:23.920 --> 00:03:28.080]   focus on the dashboard and reports, but they're part of this toolkit and they
[00:03:28.080 --> 00:03:29.600]   could be combined with it very easily.
[00:03:29.600 --> 00:03:32.360]   But those tools have been framework agnostic.
[00:03:32.360 --> 00:03:34.840]   You can come in and you can use PyTorch or TensorFlow.
[00:03:34.840 --> 00:03:37.720]   You can use a framework built on top of it, like Keras.
[00:03:37.720 --> 00:03:40.160]   I'm a big fan of PyTorch lightning these days.
[00:03:40.160 --> 00:03:42.040]   That's what I've been using a lot.
[00:03:42.040 --> 00:03:45.800]   And I've found it, it works super well, just as well as the things I was doing
[00:03:45.800 --> 00:03:48.400]   in Keras and, and weights and biases recently.
[00:03:49.000 --> 00:03:52.120]   And we also support lots of other frameworks, including light
[00:03:52.120 --> 00:03:56.040]   GBM and XGBoost and maybe things that are a little bit more built
[00:03:56.040 --> 00:03:57.840]   up like fast.ai and HuggingFace.
[00:03:57.840 --> 00:03:59.760]   You can use any of these things with these tools.
[00:03:59.760 --> 00:04:01.880]   We're also environment agnostic.
[00:04:01.880 --> 00:04:04.880]   You could be running your stuff, you know, on premises.
[00:04:04.880 --> 00:04:08.400]   You could have your bare metal, your, and your laptop, or you could be
[00:04:08.400 --> 00:04:13.200]   scaled all the way up to Kubernetes controlled collections of containers
[00:04:13.200 --> 00:04:15.840]   on AWS and Azure and Google cloud.
[00:04:15.960 --> 00:04:19.400]   So that's weights and biases in a nutshell, from a really broad perspective.
[00:04:19.400 --> 00:04:24.160]   I want to drill down and talk about what the collaboration story is for
[00:04:24.160 --> 00:04:26.240]   teams working with weights and biases.
[00:04:26.240 --> 00:04:40.800]   [Music]

