
[00:00:00.000 --> 00:00:04.160]   we're building up this snowball of like, the models help the models get better and, you know,
[00:00:04.160 --> 00:00:07.920]   can accelerate what the humans do. And eventually, it's mostly the models doing the work. Like,
[00:00:07.920 --> 00:00:14.160]   you zoom out far enough, that's happening. But I'm kind of skeptical of kind of any kind of precise
[00:00:14.160 --> 00:00:19.040]   mathematical or exponential prediction of how it's going to be. I think it's, I think it's,
[00:00:19.040 --> 00:00:24.640]   I think it's all going to be a mess. But I think what we know is it's on a metaphorical exponential,
[00:00:24.640 --> 00:00:28.320]   and it's going to happen fast. We're already at the point where if you look at the loss,
[00:00:28.320 --> 00:00:32.880]   the scaling laws are starting to bend. I mean, we've seen that in, you know, published model cards
[00:00:32.880 --> 00:00:39.520]   offered by multiple companies. So that's not a secret at all. But as, as they start to bend,
[00:00:39.520 --> 00:00:44.480]   each little bit of, of entropy, right, of accurate prediction becomes more important, right? Maybe
[00:00:44.480 --> 00:00:48.960]   these last little bits, bits of entropy are like, well, you know, this is a physics paper, as
[00:00:48.960 --> 00:00:53.040]   Einstein would have written it, as opposed to, you know, as some other physicist would have,
[00:00:53.040 --> 00:00:57.760]   would have, would have written it. And so it's, it's hard to assess significance from this. It's
[00:00:57.760 --> 00:01:04.000]   certainly looks like in terms of practical performance, the metrics keep going up relatively
[00:01:04.000 --> 00:01:08.640]   linearly, although they're always unpredictable. So, so it's, it's hard to see that. And then,
[00:01:08.640 --> 00:01:14.080]   I mean, the thing that I think is driving the most acceleration is just more and more money is going
[00:01:14.080 --> 00:01:19.680]   into the field. Like people are seeing that there's just a huge amount of, you know, of,
[00:01:19.680 --> 00:01:24.640]   of economic value. And so I expect the amount of money spent on the largest models to go up by
[00:01:24.640 --> 00:01:29.440]   like a factor of 100 or something. And for that, that then to be concatenated with the chips are
[00:01:29.440 --> 00:01:34.080]   getting faster, the algorithms are getting better, because there's, there's so many people working on
[00:01:34.080 --> 00:01:38.800]   this now. And so, and so again, I mean that, you know, I'm not making a normative statement here,
[00:01:38.800 --> 00:01:44.240]   this is what should happen. I'm not even saying this necessarily will happen, because I think
[00:01:44.240 --> 00:01:49.120]   there's important safety and government questions here, which we're very actively working on. I'm
[00:01:49.120 --> 00:01:52.640]   just, I'm just saying like left to itself, this is what the economy is going to do.
[00:01:52.640 --> 00:01:56.560]   - So do you think that another thing on the scale of a transformer
[00:01:56.560 --> 00:02:01.760]   is coming down the pike to enable the next, the next great iterations?
[00:02:01.760 --> 00:02:06.000]   - I think it's possible. I mean, people have worked on things like, you know, trying to model
[00:02:06.000 --> 00:02:12.160]   very long time dependencies, or, you know, you know, there's various different ideas where I
[00:02:12.160 --> 00:02:17.680]   could see that we're kind of missing an efficient way of representing or dealing with something.
[00:02:17.680 --> 00:02:22.960]   So I think those inventions are possible. I guess my perspective would be, even if they don't happen,
[00:02:22.960 --> 00:02:29.280]   we're, we're all, we're already on this very, very steep trajectory. And so I'm less, I mean,
[00:02:29.280 --> 00:02:33.520]   we're constantly trying to discover them as are, as are others. But things are already on such a
[00:02:33.520 --> 00:02:38.640]   fast trajectory, all that would do is speed up the trajectory even more. And probably, probably not by
[00:02:38.640 --> 00:02:42.800]   that much, because it's already going so fast. I think we've been relatively responsible in the
[00:02:42.800 --> 00:02:48.000]   sense that, you know, the big acceleration that happened late last year, and beginning of this
[00:02:48.000 --> 00:02:52.640]   year, like, we didn't cause that we weren't, we weren't the ones who did that. And honestly, I
[00:02:52.640 --> 00:02:56.560]   think if you look at the reaction to Google, that that might be 10 times more important than anything
[00:02:56.560 --> 00:03:01.600]   else. And then kind of once it had happened, once the ecosystem had changed, then we did a lot of
[00:03:01.600 --> 00:03:07.120]   things to kind of to kind of stay on the frontier. And so I don't know, it's, it's, I mean, it's like
[00:03:07.120 --> 00:03:11.440]   any other question, right? It's like, you're trying to, you're trying to do the things that have the
[00:03:11.440 --> 00:03:16.800]   lowest costs and the biggest benefits. And you know, that that causes you to have different
[00:03:16.800 --> 00:03:22.480]   strategies at different times. How likely do you think it is that these models will be superhuman
[00:03:22.480 --> 00:03:29.120]   for many years at economically valuable tasks, while there are still below humans in many other
[00:03:29.120 --> 00:03:33.760]   relevant tasks that prevents like an intelligence explosion or something? I think this kind of stuff
[00:03:33.760 --> 00:03:39.360]   is like really hard to know. So I'll give I'll give that caveat that like, you know, again, like
[00:03:39.360 --> 00:03:43.920]   the basic scaling laws, you can kind of predict, and then like this more granular stuff, which we
[00:03:43.920 --> 00:03:49.920]   really want to know to know how this all is going to go is much harder to know. But my guess would
[00:03:49.920 --> 00:03:54.960]   be the scaling laws are going to continue, you know, again, subject to, you know, do people
[00:03:54.960 --> 00:04:01.280]   slow down for safety or for regulatory reasons. But you know, let's just let's just put all that
[00:04:01.280 --> 00:04:05.440]   aside and say, like, we have the economic capability to keep scaling. If we did that,
[00:04:05.440 --> 00:04:11.040]   what would happen? And I think my view is we're going to keep getting better across the board.
[00:04:11.040 --> 00:04:15.920]   And I don't see any area where the models are like, super, super weak or not starting to make
[00:04:15.920 --> 00:04:21.040]   progress like that used to be true of like math and programming. But I think over the last six
[00:04:21.040 --> 00:04:26.960]   months, you know, the 2023 generation of models compared to the 2022 generation has started to
[00:04:26.960 --> 00:04:33.120]   learn that there may be more subtle things we don't know. And so I kind of suspect, even if
[00:04:33.120 --> 00:04:37.680]   it isn't quite even, that the rising tide will lift all the boats, you know, are we going to be
[00:04:37.680 --> 00:04:43.280]   superhuman in some areas and not others? I think it's complicated. I could imagine that we won't
[00:04:43.280 --> 00:04:47.920]   be superhuman in some areas because, for example, they involve like embodiment in the physical
[00:04:47.920 --> 00:04:54.640]   world. And then it's like, what happens like the AIs help us train faster AIs and those faster AIs
[00:04:54.640 --> 00:05:01.120]   wrap around and solve that? Do you not need the physical world? I think the idea that AI systems
[00:05:01.120 --> 00:05:07.120]   become more productive and first they speed up the productivity of humans, then they, you know,
[00:05:07.120 --> 00:05:13.280]   kind of equal the productivity of humans. And, you know, and then they're in some meaningful sense,
[00:05:13.280 --> 00:05:21.360]   the main contributor to scientific progress, that that happens at some point. I think that basic
[00:05:21.360 --> 00:05:26.400]   logic seems likely to me, although I have a suspicion that when we actually go into the
[00:05:26.400 --> 00:05:31.200]   details, it's going to be kind of like weird and different than we expect, that all the detailed
[00:05:31.200 --> 00:05:36.880]   models are kind of, you know, we're thinking about the wrong things or we're right about one
[00:05:36.880 --> 00:05:42.320]   thing and then are wrong about 10 other things. And so I don't know, I think we might end up in
[00:05:42.320 --> 00:05:46.960]   like a weirder world than we expect. - When you add all this together,
[00:05:46.960 --> 00:05:52.480]   like your estimate of when we get something kind of human level, what does that look like?
[00:05:52.480 --> 00:05:59.840]   - You know, in terms of someone looks at these, the model and, you know, even if you talk to it
[00:05:59.840 --> 00:06:07.120]   for, you know, for an hour or so, it's basically, you know, it's basically like a generally well
[00:06:07.120 --> 00:06:14.240]   educated human. That could be not very far away at all, I think. Like that could happen in, you
[00:06:14.240 --> 00:06:18.480]   know, two or three years. That may not be the threshold where the models are existentially
[00:06:18.480 --> 00:06:23.040]   dangerous. In fact, I suspect it's not quite there yet. It may not be the threshold where
[00:06:23.040 --> 00:06:29.440]   the models can take over most AI research. It may not be the threshold where the models, you know,
[00:06:29.440 --> 00:06:34.240]   seriously change how the economy works. I think it gets a little murky after that and all those
[00:06:34.240 --> 00:06:40.960]   thresholds may happen at various times after that. But I think in terms of the base technical
[00:06:40.960 --> 00:06:48.080]   capability of it, it kind of sounds like a reasonably generally educated human across
[00:06:48.080 --> 00:06:50.080]   the board. I think that could be quite close.

