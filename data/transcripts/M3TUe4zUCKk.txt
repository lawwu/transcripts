
[00:00:00.000 --> 00:00:07.440]   So utopia for me just means a kind of profoundly better future. And I think it's important because
[00:00:07.440 --> 00:00:11.680]   I think it's just actually possible. I just think it's actually something that we could do
[00:00:11.680 --> 00:00:18.320]   if we sort of play our cards right. We could just build a world that is radically better than the
[00:00:18.320 --> 00:00:26.000]   world we live in today. Infinite ethics is ethics that tries to grapple with how we should act
[00:00:26.640 --> 00:00:31.360]   with respect to kind of infinite worlds. There's a middle ground between I shall ignore this
[00:00:31.360 --> 00:00:36.640]   completely and I shall be a Jane, which is recognizing that this is a real trade-off,
[00:00:36.640 --> 00:00:40.640]   there's uncertainty here, and taking responsibility for how you're responding to that.
[00:00:40.640 --> 00:00:46.080]   The future is a big thing to try to model with this tiny mind. And so of necessity,
[00:00:46.080 --> 00:00:48.800]   you need to use these extremely lossy abstractions.
[00:00:55.280 --> 00:01:01.360]   Today, I have the pleasure of interviewing Joe Carlsmith, who's a senior research analyst at
[00:01:01.360 --> 00:01:06.560]   Open Philanthropy and a doctoral student in philosophy at the University of Oxford.
[00:01:06.560 --> 00:01:13.680]   Joe has a really interesting blog that I got to check out called Hands and Cities. And that's
[00:01:13.680 --> 00:01:17.280]   the reason that I wanted to have him on the podcast because it has a bunch of thought-provoking
[00:01:17.280 --> 00:01:23.840]   and insightful posts on there about philosophy and morality, ethics, the future. And yeah,
[00:01:23.840 --> 00:01:29.600]   so I really wanted to talk to you, Joe. But do you want to give a bit of a longer intro
[00:01:29.600 --> 00:01:34.240]   on what you're up to? Sure. So I work at Open Philanthropy
[00:01:34.240 --> 00:01:40.800]   on existential risk from artificial intelligence. And so I think about what's going to happen with
[00:01:40.800 --> 00:01:43.920]   AI, how can we make sure it goes well, and in particular, how can we make sure that
[00:01:43.920 --> 00:01:51.760]   advanced AI systems are safe? And then I have a side project, which is this blog
[00:01:51.760 --> 00:01:56.960]   where I write about philosophy and the future and things like that. And that emerges partly
[00:01:56.960 --> 00:02:04.640]   from my background, which is before getting into AI and working at Open Philanthropy,
[00:02:04.640 --> 00:02:09.920]   I was in academic philosophy. Okay. Yeah. That's quite an ambitious
[00:02:09.920 --> 00:02:14.080]   side project. I mean, given the length and the regularity of those posts, it's actually quite
[00:02:14.080 --> 00:02:20.640]   stunning. Do you want to talk more about what you're working on about AI at Open Philanthropy?
[00:02:21.040 --> 00:02:25.840]   Sure. So it's a mix of things. Right now, I'm thinking about AI timelines and what's called
[00:02:25.840 --> 00:02:31.040]   takeoff speeds, how fast the transition is from pretty impressive AI systems to AI systems that
[00:02:31.040 --> 00:02:37.920]   are radically transformative. And I'm trying to use that to provide more perspective on the
[00:02:37.920 --> 00:02:45.520]   probability that everything goes terribly wrong. I see. Okay. So what are the implications? I
[00:02:45.520 --> 00:02:50.560]   suppose it's higher or lower than I would expect. I guess if it's higher, maybe I should work on AI11
[00:02:50.560 --> 00:02:55.600]   but other than that, what are the implications of that figure changing?
[00:02:55.600 --> 00:03:00.560]   I think there are a number of implications just from understanding timelines with respect to how
[00:03:00.560 --> 00:03:05.920]   you prioritize and to some extent, the sooner something is, then you need to be planning for
[00:03:05.920 --> 00:03:13.040]   it coming sooner and cutting more corners or counting less on having more time. And yeah,
[00:03:13.040 --> 00:03:18.160]   I think overall, the higher you think the probability of catastrophe is, the easier it is
[00:03:18.160 --> 00:03:24.320]   for this to become the most important priority. I do think there's a range of probabilities where
[00:03:24.320 --> 00:03:30.560]   it maybe doesn't matter that much, but I think the difference between say one and 10%, I think
[00:03:30.560 --> 00:03:37.920]   is quite substantive. And the difference between 10 and 90 is quite substantive. And I know people
[00:03:37.920 --> 00:03:42.800]   in all of those ranges. Gotcha. Okay. Interesting. Yeah. So
[00:03:42.800 --> 00:03:47.120]   let's back up here and talk a bit more about the philosophy motivating this. So
[00:03:47.120 --> 00:03:52.800]   I think you identify as a long-termist. Yeah. So maybe a broad picture question here is
[00:03:52.800 --> 00:03:58.000]   you have an interesting blog post about why the future looking back on us might think about
[00:03:58.000 --> 00:04:03.920]   the 21st century, given the risk we're taking. So what do you think about the possibility that
[00:04:03.920 --> 00:04:07.920]   we're potentially giving up resources, potentially dedicating, well, I'm not,
[00:04:07.920 --> 00:04:17.760]   you're dedicating your career to building a future that maybe given the fact that you're
[00:04:17.760 --> 00:04:24.000]   alive now, you might find strange or disturbing or disgusting. I mean, so I guess to add more
[00:04:24.000 --> 00:04:27.360]   context to the question, from a utilitarian perspective, the present is clearly much,
[00:04:27.360 --> 00:04:33.040]   much better than the past, but as somebody from the past might think that there's a lot of bad
[00:04:33.040 --> 00:04:36.640]   things about the present that are kind of disturbing. I mean, they might not like the
[00:04:36.640 --> 00:04:42.160]   configuration of how maybe isolating of a modern city might be. They might find that kinds of free
[00:04:42.160 --> 00:04:47.920]   to cheap information that you can access on your phone kind of disturbing. Yeah. So how do you
[00:04:47.920 --> 00:04:56.320]   think about that? So yeah, a few comments there. So one, I do think that if you took, you know,
[00:04:56.320 --> 00:05:00.400]   for most people throughout history, if you brought them to the present day, they would,
[00:05:00.400 --> 00:05:05.360]   my guess is that fairly quickly and depending on exactly the circumstances, they would come to
[00:05:05.360 --> 00:05:10.640]   prefer living in the present day to the past, even if there are sort of a bit of future shock
[00:05:10.640 --> 00:05:18.160]   and a bit of some things are alienating or disturbing. But that said, I think the distance,
[00:05:18.160 --> 00:05:23.760]   the sort of gap between historical humans and the present is actually much, much smaller,
[00:05:23.760 --> 00:05:28.800]   both in terms of time and kind of other factors than the gap I envisioned between present day
[00:05:28.800 --> 00:05:34.640]   humans and the future humans who are living ideally in a kind of radically better situation.
[00:05:34.880 --> 00:05:40.080]   And so I do expect sort of greater distance and possibly greater alienation when you first show
[00:05:40.080 --> 00:05:49.760]   up. My personal view is that the best, the best futures are going to be such that if you really
[00:05:49.760 --> 00:05:54.800]   understood them and if you really experienced what they're like, which, which may be a big
[00:05:54.800 --> 00:06:00.000]   step and might require sort of extensive engagement and possibly sort of changes to your
[00:06:00.000 --> 00:06:06.160]   capacities to understand and experience, then you would think it's really good. And so, and I think
[00:06:06.160 --> 00:06:11.760]   that's the relevant standard. So for me, I worry less if the future is sort of initially alienating.
[00:06:11.760 --> 00:06:16.240]   And the question for me is how do I feel once I've really, really understood what's going on?
[00:06:16.240 --> 00:06:23.040]   I see. So I wonder how much we should value that kind of inside view you would get into the future
[00:06:23.040 --> 00:06:28.960]   from being there. If you think about, I don't know, many, many existing ideologies,
[00:06:28.960 --> 00:06:32.800]   like, I don't know, think of an Islamist or something who might say, listen, if you could
[00:06:32.800 --> 00:06:38.480]   just like come to Iraq and feel the bliss of fighting for the caliphate, you would understand
[00:06:38.480 --> 00:06:41.680]   better than you can understand from the outside view, just, you know, sitting on a couch, eating
[00:06:41.680 --> 00:06:46.000]   Doritos, what, you know, what it's like to fight for a cause. And maybe their experience is kind
[00:06:46.000 --> 00:06:52.000]   of blissful in some kind of way, but I feel like the outside view is more useful than the inside
[00:06:52.000 --> 00:06:58.080]   view there. Well, so I think there's a couple of different questions there. One is what would the
[00:06:58.080 --> 00:07:05.120]   experience be if you had it from the inside? And then there was, I think, a subtly different
[00:07:05.120 --> 00:07:11.040]   question, which is what would your take on this be if you kind of fully understood, where fully
[00:07:11.040 --> 00:07:17.760]   understanding is not just a matter of having the internal experience of being in a certain
[00:07:17.760 --> 00:07:22.160]   situation, but it's also a matter of understanding what that situation is causing, what sort of
[00:07:22.160 --> 00:07:27.120]   beliefs are structuring the ideology, whether those beliefs are true, and all sorts of other
[00:07:27.120 --> 00:07:30.800]   factors. And it's the latter thing that I have in mind. So I'm not just imagining, oh, the future
[00:07:30.800 --> 00:07:35.680]   will feel good if you're there, because, you know, sort of by hypothesis, the people who are there,
[00:07:35.680 --> 00:07:39.520]   at least, one hopes they're enjoying it, or one hopes they're thumbs up. If the people who are
[00:07:39.520 --> 00:07:44.560]   there aren't thumbs up, that's a strange, a strange utopia. But I'm thinking more that in
[00:07:44.560 --> 00:07:49.920]   addition to their perspective, there's a sort of more holistic perspective, which is the sort of
[00:07:49.920 --> 00:07:56.080]   full understanding. And that's the perspective from which you would endorse this situation.
[00:07:56.080 --> 00:08:01.200]   I see. And then, yeah, so another respect in which it's interesting to think about what
[00:08:01.200 --> 00:08:04.880]   they might think of us is, you know, like, well, what would they think of the crazy risk we're
[00:08:04.880 --> 00:08:11.600]   taking by not optimizing for existential risks? And so, you know, one analogy you could offer,
[00:08:11.600 --> 00:08:16.320]   I think, what McCaskill does in his new book is to think of us as, you know, teenagers in our
[00:08:16.320 --> 00:08:20.720]   civilization's history. And then, you know, think of the crazy things you did as a teenager and how
[00:08:22.400 --> 00:08:26.080]   you think of us as adults. And, yeah, so, I mean, maybe there is an aspect to which, like,
[00:08:26.080 --> 00:08:29.360]   one would wish they could take back the crazy things they did as a teenager. But my impression
[00:08:29.360 --> 00:08:36.720]   is that most adults probably think that while the crazy things were kind of risky, they were very
[00:08:36.720 --> 00:08:43.040]   formative and important, and they feel nostalgic about the things they did in the past. Do you
[00:08:43.040 --> 00:08:49.360]   think that the future, looking back, they are going to regret the way we're living in the 21st
[00:08:49.360 --> 00:08:54.480]   century? Or will they look back and think, oh, you know, that was kind of a cool time? I mean,
[00:08:54.480 --> 00:08:57.040]   I guess this is kind of conditional on there being a future, which takes away a lot of the
[00:08:57.040 --> 00:09:09.760]   mystery here, but... I doubt that they will look back with pleasure at the sort of risks and horrors
[00:09:09.760 --> 00:09:14.720]   of the 21st century. I mean, if you just think about how we, or at least I, tend to think about
[00:09:15.440 --> 00:09:22.240]   something like the Cuban Missile Crisis or World War II, I don't personally have a kind of nostalgia,
[00:09:22.240 --> 00:09:26.160]   oh, you know, sure, it was risky, but it made me who I am or something like that.
[00:09:26.160 --> 00:09:30.800]   I also want to say, you know, I think it's true that when you look back on your teenage years,
[00:09:30.800 --> 00:09:34.800]   there is often a sort of, you know, let's say you did something like crazy, you and your friends
[00:09:34.800 --> 00:09:39.840]   used to race, you know, around, and you played chicken or something at the local quarry. And it's
[00:09:39.840 --> 00:09:44.400]   like, oh, right, right, right. But, you know, you survived, right? And the real reason not to do that
[00:09:44.400 --> 00:09:51.200]   is the, like, chunk of probability where you just died. And so I think there's a, you know,
[00:09:51.200 --> 00:09:58.160]   to some extent, the ex post perspective of looking back on certain sorts of risks is not the right
[00:09:58.160 --> 00:10:04.400]   one, especially for death risks. That's not the right perspective to use to kind of calibrate
[00:10:04.400 --> 00:10:08.960]   your understanding of how to feel about it overall. - I see. Okay, so I think you brought
[00:10:08.960 --> 00:10:15.360]   up utopia and you have a really interesting post about the concept of utopia. So do you want to
[00:10:15.360 --> 00:10:20.560]   talk a little bit more about this concept and why it's important? And also why do we have so much
[00:10:20.560 --> 00:10:27.520]   trouble thinking of a compelling utopia? - Yeah, so utopia for me just means a kind of
[00:10:27.520 --> 00:10:33.680]   profoundly better future. And I think it's important because I think it's just actually
[00:10:33.680 --> 00:10:38.160]   possible. I just think it's actually something that we could do. We could make, if we sort of
[00:10:38.160 --> 00:10:46.160]   play our cards right in sort of non-crazy ways, we could just build a world that is radically
[00:10:46.160 --> 00:10:53.120]   better than the world we live in today. And in particular, I think we often, in thinking about
[00:10:53.120 --> 00:10:58.320]   that sort of possibility, underestimate just how big the difference in value could be between
[00:10:58.320 --> 00:11:05.520]   our current situation and kind of what's available. And I think often utopias are kind of
[00:11:06.480 --> 00:11:12.640]   anchored too hard on the status quo and sort of changing it in small ways, but imagining our kind
[00:11:12.640 --> 00:11:18.320]   of fundamental situation basically unaltered. And I think such that it's a little bit like
[00:11:18.320 --> 00:11:24.400]   the difference between, you have a kind of crappy job or like a beach vacation and utopia is like,
[00:11:24.400 --> 00:11:29.440]   everyone has beach vacation. And I don't know how you feel about beach vacations,
[00:11:29.440 --> 00:11:33.440]   but I think it's much, I think the difference is more like being asleep and being awake
[00:11:33.440 --> 00:11:42.400]   or sort of, it's more, yeah, it's sort of, it's like living in a cave or living under the open
[00:11:42.400 --> 00:11:49.120]   sky. I think it's like a really big difference and that that matters a lot. - That's interesting
[00:11:49.120 --> 00:11:56.320]   because I remember in the essay, you had a section where you mentioned that you expect utopia to be
[00:11:56.320 --> 00:12:04.000]   recognizable like to a person alive now. I guess the way you put it just earlier made it seem like
[00:12:04.000 --> 00:12:07.840]   it would be a completely different category of experience than we would be familiar with.
[00:12:07.840 --> 00:12:14.800]   Yeah. So is there a contradiction there or am I missing something? - So I think there's at least
[00:12:14.800 --> 00:12:21.200]   a tension and the way I see the tension playing out or kind of being reconciled is specifically
[00:12:21.760 --> 00:12:28.640]   via the notion I referenced earlier of kind of you would, if you truly understood, come to see
[00:12:28.640 --> 00:12:34.800]   the utopia as genuinely good. But I think that process, I mean, ideally, I think the way we end
[00:12:34.800 --> 00:12:41.040]   up building utopia is we go through a long patient process of becoming wiser and better and more
[00:12:41.040 --> 00:12:46.720]   capable as a species. And it's in virtue of that process kind of culminating that we're in a
[00:12:46.720 --> 00:12:53.200]   position to build a civilization that is sort of profoundly good and radically different.
[00:12:53.200 --> 00:13:00.240]   But that's a long process. And so I do think, as I say, if I just transported you right there and
[00:13:00.240 --> 00:13:08.160]   you skipped the process, then you might not like it. And it is quite alien in some sense, but if
[00:13:08.160 --> 00:13:11.840]   you went through the process of like really understanding and kind of becoming wiser,
[00:13:13.520 --> 00:13:18.080]   you would endorse. - That's interesting to me that you think
[00:13:18.080 --> 00:13:23.440]   the process to get to utopia is more of a sort of, maybe I'm misconstruing it, but when you mentioned
[00:13:23.440 --> 00:13:29.360]   it's a process of us getting wiser. And yeah, so it sounds like it's a more philosophical process
[00:13:29.360 --> 00:13:35.920]   rather than, I don't know, we figure out how to convert everything to hedonium and it's eternal
[00:13:35.920 --> 00:13:41.040]   bliss from then on. Yeah, so am I getting it right that you think it's more of a philosophical
[00:13:41.040 --> 00:13:47.200]   process and then why is it that you think so? - Yeah, so I definitely don't sit around thinking
[00:13:47.200 --> 00:13:53.440]   that utopia, we sort of know what utopia is right now and it's hedonium. I'm not especially into the
[00:13:53.440 --> 00:14:01.600]   notion of hedonium, but I think it's possible to, I think the brand is bad. I think people talk
[00:14:01.600 --> 00:14:06.400]   about pleasure with this kind of dismissive attitude sometimes. And hedonium implies this
[00:14:06.400 --> 00:14:13.040]   kind of sterile uniformity and you're sort of tiling, people are talking about they're gonna
[00:14:13.040 --> 00:14:18.960]   tile the universe with hedonium and it's like, wow, this sounds rough. Whereas I think actually
[00:14:18.960 --> 00:14:22.800]   the relevant perspective when you're thinking about something like hedonium is the kind of
[00:14:22.800 --> 00:14:27.360]   internal perspective from which the sort of experience of the subject is something kind of
[00:14:27.360 --> 00:14:34.720]   joyful and boundless and kind of energizing and whatever pleasure is actually like. Pleasure is
[00:14:34.720 --> 00:14:41.200]   not a trivial thing. I think pleasure is a profound thing in a lot of ways, but I don't assume that
[00:14:41.200 --> 00:14:47.760]   that's what utopia is about at all. I think A, my own values seem to be quite complicated. I don't
[00:14:47.760 --> 00:14:52.640]   think I just value pleasure. I value a lot of different things. And more broadly, I have a
[00:14:52.640 --> 00:14:57.360]   lot of uncertainty about how I will think and feel about things if I were to go through a kind of
[00:14:57.360 --> 00:15:03.600]   process of significantly increasing my capacity to understand. I think sometimes when people
[00:15:03.600 --> 00:15:06.800]   imagine that, they imagine, oh, we're gonna sit around and do a bunch of philosophy and then we'll
[00:15:06.800 --> 00:15:10.160]   have like solve normative ethics and then we'll implement our solution to normative ethics.
[00:15:10.160 --> 00:15:17.760]   And that's not what I'm imagining by kind of wisdom. I'm imagining something richer and also
[00:15:17.760 --> 00:15:24.320]   that involves importantly a kind of enhancement to our cognitive capacity. So it's sort of really,
[00:15:24.320 --> 00:15:28.320]   you know, I think we have very small, we're really limited in our ability to understand
[00:15:28.320 --> 00:15:32.720]   the universe right now. We have kind of, and I think there's just a huge amount of uncharted
[00:15:32.720 --> 00:15:40.240]   territory in terms of what minds can be and do and see. And so I want to sort of chart that
[00:15:40.240 --> 00:15:44.960]   territory before we start making kind of big and irreversible decisions about what sort of
[00:15:44.960 --> 00:15:51.840]   civilization we want to build in the longterm. - I see. And then another maybe concerning part
[00:15:51.840 --> 00:15:59.040]   of utopia is that, yeah, as you mentioned in the piece, many of the worst ideologies in history
[00:15:59.040 --> 00:16:05.360]   have had elements of utopian thinking in them to the extent that EA and utilitarianism generally
[00:16:05.360 --> 00:16:09.120]   are compatible with utopian thinking. Maybe they don't advocate utopian thinking, but they are
[00:16:09.120 --> 00:16:15.600]   compatible with it. Do you see that as a problem for the movement's health and potential impact?
[00:16:15.600 --> 00:16:23.040]   - Is the question something like, is this a red flag? Kind of, you know, we look at other
[00:16:23.040 --> 00:16:28.400]   ideologies throughout history and they've been compatible with utopian thinking and maybe sort of
[00:16:28.400 --> 00:16:33.360]   effective altruism or utilitarianism or something is similarly compatible. So should we worry in
[00:16:33.360 --> 00:16:38.080]   the same way? Is that the question? - Yeah, partly. And also another part is
[00:16:38.080 --> 00:16:45.360]   maybe it's still right that like morally speaking, yeah, utopia is compatible with
[00:16:45.360 --> 00:16:51.520]   this worldview and the worldview is correct, but that the implications are that, you know,
[00:16:51.520 --> 00:16:57.360]   somebody misunderstands what is best, they identify as an EA and this leads to bad consequences when
[00:16:57.360 --> 00:17:02.320]   they try to implement their scheme. - Yeah, so I think there are certainly
[00:17:02.320 --> 00:17:11.120]   reasons to be cautious in this broad vein. I don't see them as very specific to EA or utilitarian,
[00:17:11.120 --> 00:17:18.800]   I don't identify as utilitarian, but to utilitarianism, I see them as more sort of
[00:17:18.800 --> 00:17:26.480]   better understood as risks that come from believing that something is very important at all.
[00:17:27.200 --> 00:17:34.000]   And I think it's true that many acting from a space of conviction, especially where that
[00:17:34.000 --> 00:17:38.400]   conviction has sort of a flavor of, you know, it's interesting what exactly constitutes an ideology,
[00:17:38.400 --> 00:17:42.400]   but I think it's reasonable to look at EA and sort of be like, this looks like an ideology.
[00:17:42.400 --> 00:17:48.160]   And I think, you know, and I think that's right and I think that's sort of important to, you know,
[00:17:48.160 --> 00:17:54.400]   have this sort of relevant red flags about. I think it's pretty hard to have a view of the world
[00:17:55.440 --> 00:18:01.920]   that doesn't in some sense imply that it could be a lot better, or at least a plausible view
[00:18:01.920 --> 00:18:07.360]   of the world. And when I say utopia, I don't really mean anything much different from that.
[00:18:07.360 --> 00:18:13.120]   You know, I think it's sort of, I'm not saying a perfect thing, I'm not, you know, I do have sort
[00:18:13.120 --> 00:18:16.960]   of a more specific view about exactly how much better things could be, but more broadly, it
[00:18:16.960 --> 00:18:20.640]   seems to me many, many people believe in the possibility of a much better world and are
[00:18:20.640 --> 00:18:27.360]   fighting for that in different ways. And so I wouldn't pin the red flag specifically to the
[00:18:27.360 --> 00:18:33.040]   belief that sort of things can be better. I think it would have more to do with sort of what degree
[00:18:33.040 --> 00:18:38.880]   of rigidness are you, you know, relating to that belief with, how are you kind of, how are you
[00:18:38.880 --> 00:18:46.000]   acting on it in the world? How much are you willing to kind of break things or kind of act in uncooperative
[00:18:46.000 --> 00:18:51.680]   ways in virtue of that sort of conviction? And there I think caution is definitely warranted.
[00:18:51.680 --> 00:19:01.680]   I see. Yes, I'm not sure I agree that most people have a view or an ideology that implies
[00:19:01.680 --> 00:19:08.800]   anywhere close to the kind of utopia that one, utopian thinking one can't have. Like if you
[00:19:08.800 --> 00:19:13.680]   think of modern political parties in a developed democracy, like in the United States, for example,
[00:19:13.680 --> 00:19:17.840]   if you think of what is like a utopian vision that either party has, it's like,
[00:19:17.840 --> 00:19:23.600]   it's actually quite, quite banal. It's like, oh, we'll have universal healthcare or I don't know,
[00:19:23.600 --> 00:19:29.520]   GDP will be higher in the next couple of decades, which is, which doesn't seem utopian to me.
[00:19:29.520 --> 00:19:35.280]   It just seems, and it does seem, it does seem like a limited worldview where they're not really
[00:19:35.280 --> 00:19:39.360]   thinking about how much better or worse things could be, but it doesn't exactly seem utopian.
[00:19:40.080 --> 00:19:44.480]   Yeah, I'll let you react to that. I think that's a good point. So maybe the relevant
[00:19:44.480 --> 00:19:50.800]   notion of utopian here is something like, to what extent is a concept of a radically better world
[00:19:50.800 --> 00:19:57.200]   kind of operative in your day-to-day engagement? To some extent, what I meant is that I think if I
[00:19:57.200 --> 00:20:05.680]   sat down and talked with most people, we could eventually, with some kind of constraints on
[00:20:05.680 --> 00:20:09.760]   reasonableness, come to agree that things could be a lot better in the world. Like we could just
[00:20:09.760 --> 00:20:15.040]   cure cancer. We could cure, you know, X, Y, Z disease. We could just go through a few things
[00:20:15.040 --> 00:20:18.960]   like that. We could talk about the degree of abundance that could be available. And I think,
[00:20:18.960 --> 00:20:25.840]   you know, so, but the question is whether that's like a kind of structuring or important dimension
[00:20:25.840 --> 00:20:29.600]   to how people are relating to the world. And I think you're right that it's often not. And that's
[00:20:29.600 --> 00:20:37.680]   part of maybe the thing I'm hoping to kind of push back against with that post is actually,
[00:20:37.680 --> 00:20:41.760]   I think this is a really important feature of our situation. I think it's true that it's,
[00:20:41.760 --> 00:20:45.520]   it can be dangerous. And if you're wrong about it, or if you're acting in the right,
[00:20:45.520 --> 00:20:50.880]   in a sort of unwise way with respect to it, that can be really bad. But I also think it's just,
[00:20:50.880 --> 00:20:54.960]   it's just a really basic fact. And I think we just sort of need to learn to deal with it
[00:20:54.960 --> 00:20:58.000]   maturely and kind of pretending it's not true, I think isn't the way to do that.
[00:20:58.000 --> 00:21:05.840]   I see. But to me, at least utopian or utopia sounds like some sort of peak. And maybe you
[00:21:05.840 --> 00:21:12.480]   didn't mean it this way, but so are you saying in the essay and generally that you think there is
[00:21:12.480 --> 00:21:19.520]   some sort of carrying capacity to how much good things can get or that beyond a certain point,
[00:21:19.520 --> 00:21:25.200]   things can keep getting in indefinitely better. But at this point, we're willing to say that
[00:21:25.200 --> 00:21:30.560]   we have reached utopia. Yeah. So, I mean, I certainly don't have a kind of hard threshold,
[00:21:30.560 --> 00:21:35.120]   uh, you know, here's, here's exactly where, where I'm going to call it utopia. Um, you know,
[00:21:35.120 --> 00:21:42.000]   I mean, something that is profoundly better. Uh, I do think that if you have a finite,
[00:21:42.000 --> 00:21:47.280]   so, you know, the very basic level, if there's only a finite number of states that, uh, the sort
[00:21:47.280 --> 00:21:52.000]   of affectable universe can be in, um, and your, your ranking of these states in terms of how good
[00:21:52.000 --> 00:22:01.040]   they are is, uh, transitive and complete, um, then there will be a sort of top, um, a top.
[00:22:01.040 --> 00:22:07.840]   And, and, uh, you know, I don't think that's an important thing to focus on from the perspective
[00:22:07.840 --> 00:22:13.040]   of just getting it, just, you know, taking seriously that things could be radically better
[00:22:13.040 --> 00:22:17.120]   at all. I think like talking about, ah, but exactly how good, and what's the perfect thing
[00:22:17.120 --> 00:22:21.600]   is, is often kind of, um, distracting in that respect. And it gets into these issues about like,
[00:22:21.600 --> 00:22:25.920]   oh, you know, um, how much suffering is good to have. And, and, and a lot of this sort of
[00:22:25.920 --> 00:22:29.760]   discourse on utopia, I think gets distracted from basic facts about like, at the very least we can
[00:22:29.760 --> 00:22:36.560]   do just a ton better. Um, and that's important to keep in mind. I see. I see you, you pointed
[00:22:36.560 --> 00:22:41.040]   out in the piece that many religions and spiritual movements have done the most amount of thinking
[00:22:41.040 --> 00:22:46.800]   on what a utopia could look like. And, you know, there, there's a very interesting, um, essay by
[00:22:46.800 --> 00:22:52.800]   Nick Bostrom in 2008, where he lays out his vision of what somebody speaking from the future utopia
[00:22:52.800 --> 00:22:57.040]   talking back to us would sound like. And when you read it, it sounds very much like a sort of
[00:22:57.040 --> 00:23:02.320]   mystical, uh, mystical essay, the kind of thing that, uh, change a few words and a Christian
[00:23:02.320 --> 00:23:06.080]   could write like CS Lewis could have written about like what it's like to speak down from heaven.
[00:23:06.720 --> 00:23:12.400]   Yeah. So, so to what extent is there, uh, I, and I don't, I don't mean this pejoratively,
[00:23:12.400 --> 00:23:16.960]   but to, uh, to what extent is there some sort of like a spiritual or religious dimension
[00:23:16.960 --> 00:23:21.680]   to utopian thinking, uh, that relies on some amount of faith that things can get
[00:23:21.680 --> 00:23:25.600]   in sort of, uh, indescribably better in some sort of ephemeral indescribable way.
[00:23:27.760 --> 00:23:36.880]   So I think there are definitely analogs and similarities between some ways of relating to
[00:23:36.880 --> 00:23:42.000]   the notion of utopia and, uh, attitudes and orientations that are common in religious
[00:23:42.000 --> 00:23:48.240]   contexts and spiritual contexts. And I think it's, um, and I think personally, uh, I, so I don't
[00:23:48.240 --> 00:23:51.840]   think it needs to be that like that. I, as I say, I think, I don't think it requires faith. I don't
[00:23:51.840 --> 00:23:58.080]   think it requires anything mystical. Um, I don't think, I think this is, it's just a basic fact,
[00:23:58.080 --> 00:24:04.560]   um, about our kind of current, uh, you know, our current cognitive situation, our current
[00:24:04.560 --> 00:24:11.440]   civilizational situation that, um, things could be radically better. Um, and, uh, it's a, you know,
[00:24:11.440 --> 00:24:15.360]   it's a femoral in the sense that it's quite hard to imagine, especially, you know, for me, an
[00:24:15.360 --> 00:24:20.480]   important, an important source of evidence here is, is sort of variance in the quality of human
[00:24:20.480 --> 00:24:25.360]   experiences. So if you think about your kind of peak experiences, um, they're often, it's,
[00:24:25.360 --> 00:24:30.480]   it's a really big deal. You're kind of, you're kind of sitting there going, wow, this is radically,
[00:24:30.480 --> 00:24:36.080]   this is serious. Um, and, uh, kind of feeling in touch or, or feeling that this is, this is, uh,
[00:24:36.080 --> 00:24:42.400]   in some sense, uh, or, or, um, something you would trade much, much sort of mundane experience for
[00:24:42.400 --> 00:24:46.400]   the sake of. Um, and I think it's important. So the thing that I think we need to do is sort of
[00:24:46.400 --> 00:24:51.200]   extrapolate from there. So you sort of look at the trajectory that your mind moved along as you,
[00:24:51.200 --> 00:24:56.320]   as you moved into some experience or some broader non-experiential, like your community got a lot
[00:24:56.320 --> 00:25:00.160]   better, your relationships got about a lot better, look at that trajectory and then sort of stare
[00:25:00.160 --> 00:25:05.120]   down, you know, where is that going? Um, and I do think that requires a kind of, I don't want
[00:25:05.120 --> 00:25:10.880]   to call it faith. I think it requires a kind of, um, extrapolation into a sort of zone that is in
[00:25:10.880 --> 00:25:15.040]   some sense beyond your experience, but that is sort of deeply worthy and important. And I think
[00:25:15.040 --> 00:25:21.440]   that's, um, something that, uh, is often associated with, with spirituality, um, and religion. And I
[00:25:21.440 --> 00:25:27.680]   think, uh, I think that's okay. Um, but I, I actually think there's a, there are a number
[00:25:27.680 --> 00:25:32.400]   of really important differences between utopia and something like heaven. Um, so, you know,
[00:25:32.400 --> 00:25:40.240]   centrally utopia will be a sort of concrete limited situation there, you know, there are
[00:25:40.240 --> 00:25:44.640]   going to be frictions, there are going to be resource constraints, uh, it's going to be finite.
[00:25:44.640 --> 00:25:50.160]   Um, there's, there's a bunch of, it's still going to be in the real world. Whereas I think, um, uh,
[00:25:50.160 --> 00:25:55.760]   many, you know, most religious visions have, don't have, don't have those constraints and
[00:25:55.760 --> 00:26:00.880]   that's an important, an important feature of their, um, uh, uh, yeah, of their, their situation.
[00:26:00.880 --> 00:26:06.320]   Yeah. Speaking of constraint, uh, constraints, this reminds me of Robin Hanson's theory that,
[00:26:06.320 --> 00:26:11.600]   uh, you know, eventually the universal economy would just be made up of, um, these digital people
[00:26:11.600 --> 00:26:17.920]   Ms. And that because of competition, their wages will be driven down to subsistence levels, uh,
[00:26:17.920 --> 00:26:24.480]   which, um, maybe that's compatible with some engineering in their ability to experience such
[00:26:24.480 --> 00:26:29.520]   that, you know, it's still blissful for them to work at subsistence levels of compute or whatever.
[00:26:29.520 --> 00:26:35.520]   Um, but, uh, yeah, so it seems like this sort of like, uh, the first order economic thinking
[00:26:35.520 --> 00:26:41.120]   implies that there will be no, there'll be no utopia. In fact, things will get, um, things will
[00:26:41.120 --> 00:26:45.360]   get worse for on average, but maybe better, uh, overall, if you just add up all the experience,
[00:26:45.360 --> 00:26:50.960]   but worse on average. Uh, yeah. So, so I dunno, it may, this vision seems incompatible with yours
[00:26:50.960 --> 00:26:58.800]   of a utopia. What do you think? Yeah, I would not call a Robin's world, a utopia. Uh, and so,
[00:26:58.800 --> 00:27:05.280]   you know, a thing I haven't been talking about is what should our overall probability distribution
[00:27:05.280 --> 00:27:11.760]   be with respect to different quality of futures? Um, and what, um, you know, exactly how possible
[00:27:11.760 --> 00:27:15.680]   is it, uh, and how likely is it that we, we build something that is sort of profoundly good as
[00:27:15.680 --> 00:27:24.320]   opposed to, uh, mediocre or much worse. Um, and, uh, I would class Robin's scenario in the, uh,
[00:27:24.320 --> 00:27:32.000]   mediocre or, uh, or much worse zone. So do you have a, uh, a criticism of the logic he uses to
[00:27:32.000 --> 00:27:38.080]   derive that to some extent? I think my main, my main criticism or the first thing that would come
[00:27:38.080 --> 00:27:45.920]   to mind is that I think we will very likely, um, uh, like I think competitive pressures are, uh,
[00:27:45.920 --> 00:27:52.960]   are a source of kind of, kind of pushing, pushing, uh, the world in, in bad directions.
[00:27:52.960 --> 00:27:57.520]   But I also think there are ways in which, um, kind of wise forms of coordination and kind of
[00:27:57.520 --> 00:28:03.200]   preemptive action can, uh, can stave off the sort of bad effects of, of competitive pressures. And,
[00:28:03.200 --> 00:28:08.000]   and, and so that's, that's the sort of, um, that's the way I imagine avoiding, uh, stuff in the
[00:28:08.000 --> 00:28:12.480]   vicinity of, of what Robin is talking about. The, you know, there, there are a lot of complexities
[00:28:12.480 --> 00:28:18.160]   there. Yeah. Yeah. Um, the last few years have not reinforced my, uh, my, my belief in the possibility
[00:28:18.160 --> 00:28:23.040]   of wise coordination, but, uh, yeah, yeah. Uh, anyways. So, um, you know, one thing I want to
[00:28:23.040 --> 00:28:30.320]   talk to you about is you have a paper on what, what it would take to match, uh, humans, brains,
[00:28:30.320 --> 00:28:35.040]   a computational capacity, um, uh, and then associated with that, you have, uh, you know,
[00:28:35.040 --> 00:28:39.680]   a very good summary on open philanthropy. Um, yeah. So d-d-do you want to talk about, uh,
[00:28:39.680 --> 00:28:44.640]   the approach you took to estimate this and then w-why this is a important metric to try to figure
[00:28:44.640 --> 00:28:52.560]   out? Yeah. So, um, the approach I took was to look at the evidence from neuroscience and the
[00:28:52.560 --> 00:28:56.480]   literature on, uh, the kind of computational capacity of the human brain and to talk to a
[00:28:56.480 --> 00:29:01.360]   bunch of neuroscientists and to try to, you know, see, see what we know right now about, uh, the,
[00:29:01.360 --> 00:29:06.560]   uh, the number of floating point operations per second, uh, that would be sufficient to kind of
[00:29:06.560 --> 00:29:13.520]   reproduce the task relevant, uh, aspects of human cognition in a computer. Um, and that's important.
[00:29:13.520 --> 00:29:20.000]   I mean, it's actually not, you know, it's not clear to me exactly how important this parameter
[00:29:20.000 --> 00:29:26.160]   is to our overall picture. Um, I think the way in which it, it's, uh, relevant to thinking that
[00:29:26.160 --> 00:29:31.600]   I've been doing and that OpenPhil has been doing is, um, as an input into an overall methodology
[00:29:31.600 --> 00:29:37.680]   for estimating when we might see, uh, kind of human level AI systems that proceeds by first
[00:29:37.680 --> 00:29:41.600]   trying to estimate roughly the, the kind of computational capacity of the brain or the,
[00:29:41.600 --> 00:29:48.400]   or the sort of, um, uh, the sort of size of, size of a, uh, kind of AI system. And it's,
[00:29:48.400 --> 00:29:54.800]   it's kind of overall parameter count, uh, and, uh, kind of compute capacity. And that would be
[00:29:54.800 --> 00:29:58.320]   sort of analogous to humans. And then you extrapolate from that to the training costs,
[00:29:58.320 --> 00:30:03.840]   the cost to kind of create a system, um, of that kind of using, uh, current methods in machine
[00:30:03.840 --> 00:30:09.840]   learning and kind of current scaling, scaling laws. Uh, and, um, that methodology though,
[00:30:09.840 --> 00:30:13.600]   brings in a number of additional assumptions that I think aren't, um, aren't like just transparent
[00:30:13.600 --> 00:30:16.960]   that that's, oh yeah, of course that's how we would do it or that. And so, um, I think you
[00:30:16.960 --> 00:30:20.960]   have to sort of be a little bit more in the weeds to see exactly how it, um, how it feeds in.
[00:30:20.960 --> 00:30:28.000]   I see. And then, yeah, so what, I think you said it was 10 to the 15 flops, uh, for, um, for human
[00:30:28.000 --> 00:30:33.040]   brain, but like, did you have estimate for how many flops it would take to train, uh, to train
[00:30:33.040 --> 00:30:38.560]   something like the human brain? I know GPT three is like, um, only 175 billion parameters or
[00:30:38.560 --> 00:30:46.160]   something, which is can fit into, uh, you know, like a micro SD card even. Um, but, uh, but yeah,
[00:30:46.160 --> 00:30:51.520]   it was like, oh, a $20 million to train. So, um, yeah, so do you have, were you able to come up
[00:30:51.520 --> 00:30:54.720]   with some sort of estimate for how you, what it would cost to train something like this?
[00:30:54.720 --> 00:31:02.800]   Yeah. So my focus in that report was not on the training extrapolation. That was work, uh, that
[00:31:02.800 --> 00:31:10.240]   Ajaya Katra at OpenPhilanthropy did, um, using my reports estimate as an input and, uh, that her
[00:31:10.240 --> 00:31:15.040]   methodology involves assigning different probabilities to different kind of ways of
[00:31:15.040 --> 00:31:21.680]   using that, that input, uh, to, to, to drive an overall training estimate. Um, and in particular,
[00:31:21.680 --> 00:31:27.440]   an important source of uncertainty there is, uh, the kind of amount of compute required,
[00:31:27.440 --> 00:31:31.840]   or the sort of number of times you need to run a system per data point that it gets.
[00:31:31.840 --> 00:31:36.560]   So in the case of something like GPT three, you get a meaningful data point and a gradient update
[00:31:36.560 --> 00:31:41.040]   as to how well you're performing, um, with each token that you output as you're doing GPT three
[00:31:41.040 --> 00:31:45.040]   style training. So you're, you know, you're predicting text from the internet, you know,
[00:31:45.040 --> 00:31:49.280]   you, you suggest the next token and then your training process says like, nope, do better next
[00:31:49.280 --> 00:31:53.600]   time or something like that. Whereas if you're, uh, say learning to play Go and you have to play,
[00:31:53.600 --> 00:31:57.440]   uh, I mean, this isn't exactly how, or this isn't how it goes as more work, but it's an example.
[00:31:57.440 --> 00:32:02.320]   If you have to play the full game out and that's sort of hundreds of moves, um, then before you
[00:32:02.320 --> 00:32:07.120]   get, uh, an update as to whether, uh, you know, you're playing well or poorly, then, uh, that's
[00:32:07.120 --> 00:32:11.120]   a big multiplier on, on the compute requirement. And so that's, that's one of the central pieces
[00:32:11.120 --> 00:32:16.880]   that's called what Ajay calls the horizon length of, of training. And, um, that's a sort of very
[00:32:16.880 --> 00:32:22.240]   important, uh, source of uncertainty and getting to your overall, overall, uh, training estimate,
[00:32:22.240 --> 00:32:25.200]   I think, but ultimately, you know, she ends up with this big spread out distribution from
[00:32:25.200 --> 00:32:30.880]   something like, I think GPT three was like, um, 10 to the 24, yeah, four times 10 to the 23 or
[00:32:30.880 --> 00:32:35.280]   something like that. And, you know, she's, she spreads out all the way up to the evolution
[00:32:35.280 --> 00:32:40.480]   anchor, I think is something like 10 to the 41. And, uh, I think her distribution is centered
[00:32:40.480 --> 00:32:45.920]   somewhere in the low thirties. Okay. That's, that's still quite a bit, I guess. Um, how much
[00:32:45.920 --> 00:32:51.360]   does this rely on the, you know, the scaling hypothesis? If one thought that the current
[00:32:51.360 --> 00:32:56.400]   efforts and the current approach were not, um, not likely to lead in a, or at least not likely
[00:32:56.400 --> 00:33:00.800]   to the sample efficient way towards, uh, towards human intelligence, you know, it might be analogous
[00:33:00.800 --> 00:33:04.720]   to somebody saying we have, um, enough deuterium on earth to power civilization for millions of
[00:33:04.720 --> 00:33:10.560]   years. Um, uh, but, but if you haven't figured out fusion, then it may be irrelevant, uh, statistic.
[00:33:10.560 --> 00:33:22.080]   Yeah. So I think the approach does assume that you can train a human level or sort of, uh,
[00:33:22.080 --> 00:33:27.120]   transformative AI system, um, with a sort of non-astronomical amount of compute and data
[00:33:27.120 --> 00:33:32.640]   using current, you know, without, without major conceptual or algorithmic breakthroughs relative
[00:33:32.640 --> 00:33:37.760]   to, to what's currently available. Um, now the actual methodology uses allows you to assign
[00:33:37.760 --> 00:33:43.360]   probabilities to that assumption too. So you can, if you want, you know, say I'm only 20% on that.
[00:33:43.360 --> 00:33:49.920]   Um, and then, uh, you have, then there are sort of other, uh, there are a few other options. So
[00:33:49.920 --> 00:33:54.640]   you can also kind of rerun evolution, which is not, uh, and, and, and so that's, that's an anchor
[00:33:54.640 --> 00:33:58.320]   that she provides to sort of, uh, and this is often what people will say as a sort of upper
[00:33:58.320 --> 00:34:02.880]   bound on how hard it is to create, um, to create human level systems is, is to do something,
[00:34:02.880 --> 00:34:07.120]   something analogous to, to, um, to simulating evolution. Um, though that, you know, there are
[00:34:07.120 --> 00:34:13.600]   a lot of open questions as to how, how hard that is. Um, but I do think this methodology, uh, is a
[00:34:13.600 --> 00:34:21.440]   lot more compelling and interesting if you, um, are compelled by the, uh, the kind of available
[00:34:21.440 --> 00:34:26.640]   techniques in deep learning and by, and by kind of scaling hypothesis, like views at least in,
[00:34:26.640 --> 00:34:31.120]   as an upper bound, I think it's important. So, you know, there's different ways of, of
[00:34:31.120 --> 00:34:34.000]   kind of being interested in algorithmic breakthroughs. One is because you think
[00:34:34.000 --> 00:34:38.960]   deep learning isn't enough. Another is because you think they will sort of provide a lot of
[00:34:38.960 --> 00:34:43.600]   efficiency relative to deep learning, such that an estimate like a J is an overestimate because
[00:34:43.600 --> 00:34:46.080]   actually, you know, we won't have to do that. We'll make some sort of breakthrough and it'll
[00:34:46.080 --> 00:34:51.280]   happen a lot earlier. Um, and, uh, uh, and I put, I put weight on that view as well.
[00:34:51.280 --> 00:34:55.920]   Yeah, that's really interesting. So yeah, that, that implies that like, even if you think the
[00:34:55.920 --> 00:35:00.240]   current techniques are not, uh, not optimal, maybe that, maybe that should update you and
[00:35:00.240 --> 00:35:03.680]   take favor of thinking it could happen sooner. That's, that's really interesting. Um, uh,
[00:35:03.680 --> 00:35:10.320]   um, yeah, so yeah. And then how did you go about estimating, uh, like, uh, the amount of flops it
[00:35:10.320 --> 00:35:16.640]   would take to emulate, uh, the interactions that happen in a brain? Uh, obviously it would be
[00:35:16.640 --> 00:35:22.000]   unreasonable to say that you have to emulate every single atomic, uh, atomic interaction.
[00:35:22.000 --> 00:35:26.960]   Um, but then what is, what is your proxy that you think it would be sufficient to emulate?
[00:35:26.960 --> 00:35:34.480]   So I used a few different methodologies and tried to kind of synthesize them. So one was
[00:35:34.480 --> 00:35:39.040]   looking at the kind of mechanisms of the brain and what we know about, uh, the kind of complexity of
[00:35:39.040 --> 00:35:43.040]   what they're doing and how hard it is to capture the kind of task relevant or our best, our best
[00:35:43.040 --> 00:35:48.000]   guess about the task relevant dimensions of the, the, the signaling happening in the brain. Um,
[00:35:48.000 --> 00:35:54.400]   and then I also tried to bring in comparisons with, uh, existing AI systems that are replicating
[00:35:54.400 --> 00:36:00.800]   kind of chunks of functionality, um, that humans, uh, that the human brain has, and in particular
[00:36:00.800 --> 00:36:06.560]   in the context of vision. Um, so sort of, uh, how do our, how do our current, um, vision systems
[00:36:06.560 --> 00:36:11.840]   compare with, uh, the parts of the brain that, uh, are kind of plausibly doing analogous processing,
[00:36:11.840 --> 00:36:15.520]   though they're often, they're often doing other things as well. Um, and then I use the third
[00:36:15.520 --> 00:36:20.160]   method, which has to do with physical limits on the kind of energy consumption per unit computation,
[00:36:20.160 --> 00:36:23.840]   um, that the brain is possibly doing. And then a fourth method I sort of gesture at,
[00:36:23.840 --> 00:36:28.000]   which tries to extrapolate from, uh, the communication capacity of the brain,
[00:36:28.000 --> 00:36:33.360]   uh, to its computational capacity using comparisons with, uh, with current computers.
[00:36:33.360 --> 00:36:36.720]   So it's sort of a triangulation of like a, you look at a bunch of different sources of evidence,
[00:36:36.720 --> 00:36:40.400]   all of which in my opinion are pretty weak. I think we are, um, uh, we're quite well,
[00:36:40.400 --> 00:36:44.080]   that the physical limit stuff is maybe more complicated, but it's sort of a, uh, upper
[00:36:44.080 --> 00:36:48.160]   bound. Um, I think we are significantly uncertain about all of this and, and I,
[00:36:48.160 --> 00:36:53.440]   my distribution is, is, is pretty spread out. Um, but, uh, the hope is that by looking at
[00:36:53.440 --> 00:36:56.880]   a bunch of things at once, you can at least get, um, a sort of educated guess.
[00:36:57.600 --> 00:37:04.240]   And then, yeah, so I'm very curious, um, uh, is there consensus in neuroscience or, uh, other
[00:37:04.240 --> 00:37:08.960]   relevant fields that we understand the signaling mechanisms well enough that we can say like,
[00:37:08.960 --> 00:37:14.080]   basically this is what it's involved. Um, this is what the system is reducible to. Um, and yeah,
[00:37:14.080 --> 00:37:17.360]   so this is how many bits you need to represent. Uh, I don't know all the synaptic connections
[00:37:17.360 --> 00:37:22.720]   here, or I, is there a variance of opinion about like, uh, just how complicated the,
[00:37:22.720 --> 00:37:27.440]   the enterprise is? Uh, there's definitely disagreement. And, um, it was, you know,
[00:37:27.440 --> 00:37:33.280]   interesting and in some sense, disheartening to talk with neuroscientists about just how, uh,
[00:37:33.280 --> 00:37:38.800]   you know, how difficult neuroscience is, you know, it's sort of, I think it's easy,
[00:37:38.800 --> 00:37:45.520]   a consistent message. And I have a section on this in the report, um, was kind of how far we are from
[00:37:45.520 --> 00:37:49.760]   really understanding, uh, what's going on in the brain, um, especially at a kind of algorithmic
[00:37:49.760 --> 00:37:55.440]   level. Um, that said, so in some sense, the report is somewhat opinionated in that, um, you know,
[00:37:55.440 --> 00:37:59.440]   there are experts that I found more compelling than others. Uh, there are experts who are much
[00:37:59.440 --> 00:38:04.560]   more in a sort of agnosticism mode of like, we just don't know. Um, you know, the brain is really,
[00:38:04.560 --> 00:38:10.000]   really complicated who sort of err on the side of a very large compute estimates, a lot of emphasis
[00:38:10.000 --> 00:38:13.840]   on biophysical detail, a lot of emphasis on sort of mysterious things that could be happening that
[00:38:13.840 --> 00:38:20.080]   aren't happening. And then there are other neuroscientists who are more, uh, uh, you know,
[00:38:20.080 --> 00:38:23.840]   more willing to say stuff like, well, we, we kind of basically know what's, what's going on at a
[00:38:23.840 --> 00:38:28.080]   mechanistic level, which isn't the same as knowing kind of the algorithm, the sort of algorithmic
[00:38:28.080 --> 00:38:33.200]   organization overall and how to replicate it. I sort of lean towards the latter view, though,
[00:38:33.200 --> 00:38:38.560]   I give weight to both and, and, and try to, um, yeah, try to synthesize the kind of opinions of
[00:38:38.560 --> 00:38:44.480]   people. I, I, I saw overall just looking at the post itself. I haven't really looked deeper into
[00:38:44.480 --> 00:38:50.320]   the actual, um, the, the, the paper performance is derived. Uh, but it seemed like you were to
[00:38:50.320 --> 00:38:54.960]   estimate the floss mechanistically. You were adding up the different systems that play here.
[00:38:54.960 --> 00:39:00.560]   Um, yeah. So should we expect it to be additive in that way? Or maybe it's like multiplicative
[00:39:00.560 --> 00:39:06.800]   or there's more complicated interaction at like the flops grow super linearly to the inputs.
[00:39:06.800 --> 00:39:11.280]   Uh, I know that probably sounds really naive having studied it, but just like from a, uh,
[00:39:11.280 --> 00:39:18.560]   first glance kind of a way that that's a question I had. Yeah. So the, the way I was understanding,
[00:39:18.560 --> 00:39:25.680]   um, and breaking down the forms of processing that you, you would need to replicate in the brain,
[00:39:25.680 --> 00:39:32.320]   um, made, uh, made them seem not multiplicative in this way. So, uh, you know, an example would
[00:39:32.320 --> 00:39:36.240]   be if you think about, yeah, sort of simple examples, let's suppose we have some neurons
[00:39:36.240 --> 00:39:39.920]   and they're, uh, you know, they're signaling centrally via spikes through synapses or
[00:39:39.920 --> 00:39:45.760]   something like that. And then we have, uh, glial cells as well, which are signaling via like slower
[00:39:45.760 --> 00:39:50.720]   calcium waves. Uh, and it's a sort of separate, uh, separate network. Um, you know, you could
[00:39:50.720 --> 00:39:56.480]   think that if it were something like, you know, the, the rate of calcium signaling is, um, uh,
[00:39:56.480 --> 00:40:00.240]   dependent on the rate of spikes through synapses or something like that, then that's an important
[00:40:00.240 --> 00:40:06.000]   interaction. Uh, uh, but you know, overall, if you, if you sort of imagine like this, this kind
[00:40:06.000 --> 00:40:11.040]   of network processing, um, uh, these are kind of, you can just, you, you can estimate them
[00:40:11.040 --> 00:40:14.400]   independently and then, and then add it up. It's, they're not, they're not actually multiplicative
[00:40:14.400 --> 00:40:19.200]   processes on that, on that conception. Um, I do think there are kind of correlations between the
[00:40:19.200 --> 00:40:24.560]   estimates for, for, um, the different parts, but I, uh, it, it sort of added it at a fundamental
[00:40:24.560 --> 00:40:30.240]   level. I see. Okay. And then, yeah, how, how much credence do you put in, um, these sort of, uh,
[00:40:30.240 --> 00:40:35.520]   almost woo-woo hypotheses that I don't know, Roger Pemner has that thing about,
[00:40:35.520 --> 00:40:38.560]   there's something like, uh, something quantum mechanical happening in the brain. That's very
[00:40:38.560 --> 00:40:44.720]   important in, uh, for understanding cognition. Um, yeah, to what extent, uh, do, do you put
[00:40:44.720 --> 00:40:51.600]   credence in those kinds of hypotheses? I put very little credence in those hypotheses. Um, uh,
[00:40:52.720 --> 00:40:58.640]   yeah, I don't see a lot of reason to think that, um, I see a good amount of reason not to think it,
[00:40:58.640 --> 00:41:05.200]   um, but it wasn't something I dug in on a ton. Okay. Gotcha. All right. So you have this really
[00:41:05.200 --> 00:41:09.680]   interesting blog post about infinite ethics. Um, do you want to talk about why this is an
[00:41:09.680 --> 00:41:15.280]   important topic, why it's important to integrate into our worldview and so on? Sure. So infinite
[00:41:15.280 --> 00:41:24.000]   ethics is ethics that tries to grapple with how we should, uh, act with respect to kind of infinite
[00:41:24.000 --> 00:41:28.320]   worlds. Um, and how should we, you know, how should we rank them? Um, how should they enter
[00:41:28.320 --> 00:41:34.160]   into our, uh, our expected utility calculations or our attitudes towards risk? Um, and I think
[00:41:34.160 --> 00:41:38.720]   this is important for both kind of theoretical and practical reasons. So I think at a theoretical
[00:41:38.720 --> 00:41:43.280]   level, when you, when you try to do this with a lot of common, um, ethical theories and constraints
[00:41:43.280 --> 00:41:49.680]   and principles, um, they just break on, uh, on infinite worlds. Um, and, uh, I think that's a,
[00:41:49.680 --> 00:41:52.960]   that's an important clue as to their viability, because I think infinite worlds are at the very
[00:41:52.960 --> 00:41:58.080]   least possible. Um, even if our world is finite, um, and even if our causal influence is finite
[00:41:58.080 --> 00:42:02.480]   or our influence overall is finite, um, it's possible, uh, to have an infinite worlds and we
[00:42:02.480 --> 00:42:06.240]   have opinions about them, you know, like an infinite heaven is better than an infinite hell.
[00:42:06.240 --> 00:42:11.760]   And, you know, uh, so I think, um, often in ethics, we, we, we expect our ethical principles
[00:42:11.760 --> 00:42:17.360]   to extend to, um, kind of rankings scenarios or, or sort of acting and hypothetical scenarios or
[00:42:17.360 --> 00:42:22.560]   overall kind of talk, um, all possible situations rather than just our actual situation. I think,
[00:42:22.560 --> 00:42:28.240]   um, uh, infinities come in there, but then I think maybe more importantly, um, I think it's a, it's
[00:42:28.240 --> 00:42:33.920]   an issue with practical relevance. Um, and a way to see that is that, you know, I think we should
[00:42:33.920 --> 00:42:38.960]   have non-zero credence that we live in an infinite world. Um, and, uh, you know, it's, it's a very
[00:42:38.960 --> 00:42:43.360]   live, uh, physical hypothesis that the universe is infinite, even if I think the, the mainstream
[00:42:43.360 --> 00:42:48.000]   view is that our causal influence on that universe, um, is finite in virtue of things like entropy
[00:42:48.000 --> 00:42:52.400]   and light speed and stuff like that. Um, but the, the universe itself may well be infinite in,
[00:42:52.400 --> 00:42:58.160]   in, um, uh, you know, uh, and possibly infinite in a number of different ways. Uh, uh, if the sort
[00:42:58.160 --> 00:43:02.480]   of Max Tegmark has some work on all the different kind of like large, you know, ways the universe
[00:43:02.480 --> 00:43:06.320]   can be really very large. There's a number of ways that I think it's just, we should have non-zero
[00:43:06.320 --> 00:43:13.680]   credence that, that we, we can have, um, infinite influence in our actions now. Um, so, uh, you know,
[00:43:13.680 --> 00:43:18.560]   our kind of the causal influence are the, the limitations there could be wrong. It may be that
[00:43:18.560 --> 00:43:21.760]   there are ways, you know, in the future we'll be able to do infinite things. Um, and then I also
[00:43:21.760 --> 00:43:28.080]   think somewhat more, uh, uh, exotically that, um, it's, there, there's sort of ways of having a
[00:43:28.080 --> 00:43:32.960]   causal influence, um, on an, on an infinite universe, even if you are, uh, limited in your
[00:43:32.960 --> 00:43:36.320]   causal influence. And that comes from some additional work I've done on decision theory.
[00:43:36.320 --> 00:43:40.160]   Um, and so if you try to incorporate that, if you're a sort of expected value reasoner,
[00:43:40.160 --> 00:43:45.360]   um, it just very quickly starts to dominate or at least break your expected value calculations.
[00:43:45.360 --> 00:43:50.640]   So, you know, you mentioned long-termism earlier, uh, and, you know, a natural reason, a natural
[00:43:50.640 --> 00:43:53.360]   argument for, for getting an interest in long-termism is, oh, you know, in the future,
[00:43:53.360 --> 00:43:56.640]   there could be all these people, their lives are incredibly important. So if you do the EV
[00:43:56.640 --> 00:44:01.280]   calculation, sort of your effect on them is what dominates. Um, but actually if you have even a
[00:44:01.280 --> 00:44:05.600]   tiny credence that you can do an infinite thing, uh, you know, either that dominates or it breaks.
[00:44:05.600 --> 00:44:08.880]   And then if you have tiny credences on doing different types of infinite things and you need
[00:44:08.880 --> 00:44:13.440]   to compare them, um, you need to know how to do it. Uh, and so I just think this is actually,
[00:44:13.440 --> 00:44:17.840]   you know, it's actually a part of our, of our epistemology now, um, though it's, I think we
[00:44:17.840 --> 00:44:21.840]   often don't, uh, don't treat it that way because we're often not doing EV reasoning or really
[00:44:21.840 --> 00:44:27.120]   thinking, thinking about that, um, that, uh, that these are questions that, that just apply to us.
[00:44:28.160 --> 00:44:35.120]   Yeah. Yeah. So that's, that's super fascinating. Um, I, I, if it is the case that we can only
[00:44:35.120 --> 00:44:39.680]   have an impact on a finite amount of stuff, then maybe it is true that like there's infinite
[00:44:39.680 --> 00:44:45.600]   suffering or happiness in the universe at large, but, uh, the Delta between the best case scenario
[00:44:45.600 --> 00:44:51.120]   for what we do in the worst case scenario is finite. Um, but yeah, I dunno, that still seems
[00:44:51.120 --> 00:44:56.080]   less compelling if the, the, the hell or heaven we're surrounded by is, uh, overall not, uh,
[00:44:56.080 --> 00:45:02.320]   uh, doesn't change. Um, uh, can you talk a bit more, I think you mentioned, uh, your other work
[00:45:02.320 --> 00:45:08.480]   on having impact, uh, having infinite impact beyond, uh, the scope of what light at a speed
[00:45:08.480 --> 00:45:11.520]   and entropy would allow us. Can you talk a bit more about how that might be possible?
[00:45:11.520 --> 00:45:18.000]   Sure. So, um, you know, uh, a common decision theory, um, though it's not, I think the
[00:45:18.000 --> 00:45:21.840]   mainstream decision theory is a contender in the literature is evidential decision theory,
[00:45:21.840 --> 00:45:26.720]   where you should act, um, such that, uh, you would be, you know, roughly speaking,
[00:45:26.720 --> 00:45:34.880]   happiest to learn that you had acted that way for that reason. Um, and, uh, so the reason this
[00:45:34.880 --> 00:45:38.960]   allows you kind of a causal influence, uh, so, you know, a way of thinking about it is suppose
[00:45:38.960 --> 00:45:45.920]   that you are a, um, uh, deterministic simulation, um, and there's a copy of you being run,
[00:45:45.920 --> 00:45:51.440]   uh, sort of too far away for, uh, for you to ever, uh, causally interact with it. Right. Um,
[00:45:51.440 --> 00:45:56.640]   but you know that it's a sort of, um, you know, it's, uh, it's a deterministic copy. And so it'll
[00:45:56.640 --> 00:46:02.080]   do exactly what you do absent some sort of computer malfunction. Um, and now, uh, you're
[00:46:02.080 --> 00:46:07.440]   deciding whether to give, uh, you know, you have two options. You can send a million dollars to
[00:46:07.440 --> 00:46:11.440]   that. Well, it's a little complicated because he's too far away, but, um, uh, you know,
[00:46:11.440 --> 00:46:15.600]   just in general, like if I raise my hand or if I want to write stuff on my whiteboard,
[00:46:15.600 --> 00:46:19.520]   right. Um, or if I'm going to, uh, you know, there's, let's say I have to make some ethical
[00:46:19.520 --> 00:46:23.440]   decision, like whether I should take an expensive vacation, um, or I should donate that money to
[00:46:23.440 --> 00:46:29.200]   save someone's life because that the, the other guy, uh, is going to act just like I do. Um,
[00:46:29.200 --> 00:46:34.240]   even though I can't cause him to do that in some sense, when I, when I make my choice, um, after
[00:46:34.240 --> 00:46:38.400]   doing so, I should think that he made the same choice. And so evidential decision theory treats
[00:46:38.400 --> 00:46:44.000]   his action as in some sense under my control. Um, and so, uh, if you imagine an infinite universe
[00:46:44.000 --> 00:46:47.280]   where there are an infinite number of copies of you, or even not copies, people whose actions
[00:46:47.280 --> 00:46:51.440]   are correlated with you such that when you act a certain way, that gives you evidence about what
[00:46:51.440 --> 00:46:55.760]   they do. Um, in some sense, their actions are under your control. And so if there are an infinite
[00:46:55.760 --> 00:47:01.200]   number of them, uh, on evidential decision theory and a few other decision theories, uh, then, uh,
[00:47:01.200 --> 00:47:06.400]   in some sense, you're having influence on the universe. Yeah, this sounds really similar to,
[00:47:06.400 --> 00:47:11.280]   um, the, the, the solid experiment and quantum mechanics called EPR pair, uh, which, which you
[00:47:11.280 --> 00:47:14.960]   might've heard of, but the basic idea is if you have two entangled bits and you take them very
[00:47:14.960 --> 00:47:19.360]   far away from each other, and then you measure one of them and you do like before they're brought
[00:47:19.360 --> 00:47:24.160]   apart, you come up to some rule that like, Hey, if it's plus, we do this, if it's minus, we did the
[00:47:24.160 --> 00:47:31.440]   other thing. It seems at first glance that measuring something yourself, uh, has an impact
[00:47:31.440 --> 00:47:36.880]   on what the other person does, even though, um, it shouldn't be allowed, uh, uh, by light speed.
[00:47:36.880 --> 00:47:42.640]   Uh, it gets resolved if you take the many worlds view, but, um, um, yeah, yeah. So that's very
[00:47:42.640 --> 00:47:47.440]   interesting. Is this just a thought experiment or is this something that we should anticipate
[00:47:47.440 --> 00:47:52.800]   for, uh, some cosmological reason to actually be a way we could have influence on the world?
[00:47:52.800 --> 00:47:58.880]   So I haven't dug into the cosmology a lot, but my understanding is that it's at the very least
[00:47:58.880 --> 00:48:05.600]   a very live hypothesis that the universe is, um, infinite in the sense that there are, um,
[00:48:05.600 --> 00:48:09.840]   you know, it's just sort of infinite in extent, and there are, uh, you know, suitably far away.
[00:48:09.840 --> 00:48:14.160]   Um, there are copies of us having just this conversation and then, you know, even further
[00:48:14.160 --> 00:48:18.320]   away, there are copies of us having this conversation, but we're in raccoons for hats,
[00:48:18.320 --> 00:48:23.360]   um, and, you know, and, and all the rest, um, which, you know, is itself something to wonder
[00:48:23.360 --> 00:48:26.080]   about and sit with. But I, you know, my understanding is this is, this is just a
[00:48:26.080 --> 00:48:31.040]   live hypothesis and that more broadly, um, kind of infinity is playing, you know, infinite universes
[00:48:31.040 --> 00:48:37.680]   are just sort of a part of, of, uh, of mainstream cosmology at this point. Um, and so, uh, yeah,
[00:48:37.680 --> 00:48:40.880]   I think it, I think, I don't think it's just a thought experiment. I think infinite universes
[00:48:40.880 --> 00:48:46.000]   are, are alive. And then I think, um, uh, you know, these sort of non-causal decision theories
[00:48:46.000 --> 00:48:50.080]   are actually my sort of best guess decision theories, um, though that's not a mainstream view.
[00:48:50.080 --> 00:48:55.440]   Uh, so, uh, it's fairly, um, I think it comes in fairly directly and substantively
[00:48:55.440 --> 00:48:59.440]   if you have that combination of views, but then I also think it comes in, I think everyone should
[00:48:59.440 --> 00:49:03.920]   have non-zero credence in all sorts of different infinity involving hypotheses. And so infinite
[00:49:03.920 --> 00:49:13.600]   ethics gets a grip regardless. I see. Um, and then, so taking that example, um, if, if you're
[00:49:13.600 --> 00:49:18.240]   having an impact on every identical copy of yourself in the infinite universe, it seems that
[00:49:18.240 --> 00:49:23.760]   for any such copy, there's infinite amount of other copies that are slightly different. So it's
[00:49:23.760 --> 00:49:27.280]   not even clear if you're increasing, maybe it makes no sense to talk about proportions in an
[00:49:27.280 --> 00:49:32.560]   infinite universe, but you know, if there is another infinite set of copies that is scribbled,
[00:49:32.560 --> 00:49:39.200]   the exact opposite thing on the whiteboard, then it's, um, it's not clear that you had any impact
[00:49:39.200 --> 00:49:44.160]   on the total amount of good or bad stuff that happened. I don't know, my brain breaks here,
[00:49:44.160 --> 00:49:49.680]   but maybe you can help me understand this. Yeah. So, I mean, there's a general, I think there's a
[00:49:49.680 --> 00:49:56.240]   couple of dimensions here there. So one is, um, trying to understand actually what sort of
[00:49:56.240 --> 00:49:59.920]   difference does it make if you're in this sort of infinite situation and you're thinking about
[00:49:59.920 --> 00:50:06.160]   a causal influence, um, what even did you change, um, at, at a sort of empirical level before you
[00:50:06.160 --> 00:50:10.960]   talk about how to value that. Um, and I think that's a pretty gnarly question. Um, even if
[00:50:10.960 --> 00:50:16.160]   we've settled that question though, in terms of like the empirical, uh, a causal impact, uh,
[00:50:16.160 --> 00:50:21.520]   there's a further question of how to you, how do you rank that or how do you deal with, um, you
[00:50:21.520 --> 00:50:26.640]   know, the sort of the normative dimension here and there, you know, so that's the sort of ethical
[00:50:26.640 --> 00:50:33.520]   question and there are things get really gnarly very fast. Um, and you know, so, uh, and in fact,
[00:50:33.520 --> 00:50:39.920]   there are kind of, um, impossibility results that show that even very basic constraints that you
[00:50:39.920 --> 00:50:45.200]   really would have thought that we could get, um, at the same time in our ethical theories, uh,
[00:50:45.200 --> 00:50:49.680]   you can't get them at the same time, um, when, when you come, when it comes to infinite universes.
[00:50:49.680 --> 00:50:56.240]   Um, and, uh, so we know that something is going to have to go and change if we're going to extend
[00:50:56.240 --> 00:51:04.000]   our ethics to, to infinities. I see. But then, um, so is there some reason you settled on,
[00:51:04.000 --> 00:51:09.840]   I guess you mentioned you're not a utilitarian, but on some version of EA or long-termism as
[00:51:09.840 --> 00:51:15.120]   your tentative moral hypothesis, despite the fact that this seems unresolved and then like,
[00:51:15.120 --> 00:51:19.840]   how do you sit there with that tension while, uh, tentatively remaining in EA?
[00:51:19.840 --> 00:51:24.640]   Yeah. So I think there's a, there's two dimensions there. One is that I think it's,
[00:51:24.640 --> 00:51:31.360]   um, I think it's good practice to not totally upend your life and do, and, and, you know,
[00:51:31.360 --> 00:51:36.400]   if you encounter some destabilizing philosophical idea, especially one that's sort of difficult and,
[00:51:36.400 --> 00:51:40.800]   you know, you don't totally have a grip on it. But isn't, isn't that what long-termism is?
[00:51:40.800 --> 00:51:46.720]   Yeah. So I think there's a real tension there in that. I think, um, many, you know, how seriously
[00:51:46.720 --> 00:51:50.720]   should we take these ideas? At what point should you be making what sorts of changes for your life
[00:51:50.720 --> 00:51:55.600]   on the basis of different, different things that you're, um, uh, you're thinking and believing,
[00:51:55.600 --> 00:51:59.360]   you know, it's a real art. Right. And I think some people go, you know, they grab the first
[00:51:59.360 --> 00:52:04.400]   idea they see, um, and they start doing crazy stuff and, uh, in an unwise way. And some people
[00:52:04.400 --> 00:52:08.960]   are too, um, it's kind of sluggish and they, they're not willing to take ideas seriously and
[00:52:08.960 --> 00:52:12.800]   not willing to reorient their, their life on the basis of, of, uh, changes in what,
[00:52:12.800 --> 00:52:17.920]   in what seems true. Um, but I think, you know, nevertheless, I think especially things that
[00:52:17.920 --> 00:52:22.960]   involve like, ah, it turns out it's fine to, you know, do terrible things or, you know,
[00:52:22.960 --> 00:52:26.880]   there's no reason to eat your lunch or whatever, like things that, you know, sort of really,
[00:52:26.880 --> 00:52:30.560]   really holistically breaking of your ethics views. I think, I think one should,
[00:52:30.560 --> 00:52:35.920]   should tread very cautiously with, um, so that's one aspect at a philosophical level.
[00:52:35.920 --> 00:52:41.760]   Um, the way I resolve it is I think for many of these issues, uh, the right path forward,
[00:52:41.760 --> 00:52:48.240]   or at least a path that looks pretty good is to, um, survive long enough for our civilization to
[00:52:48.240 --> 00:52:54.800]   become much wiser. Um, and if it as, and, and then to use that position of wisdom and empowerment,
[00:52:54.800 --> 00:52:59.360]   uh, to act better with respect to these issues. Um, and so, and that's what I say in the end of
[00:52:59.360 --> 00:53:04.080]   the Infinite Ethics post is that, um, you know, I think future civilization, if all goes well,
[00:53:04.080 --> 00:53:08.560]   will be much better equipped to deal with this. Um, and, you know, we are at, we are at square
[00:53:08.560 --> 00:53:12.800]   one in kind of really understanding how, how, how these issues play out and how to respond.
[00:53:12.800 --> 00:53:18.320]   And so, uh, I think both at an empirical level and at a, at a kind of philosophical level. Um,
[00:53:18.320 --> 00:53:25.520]   and so it, it looks convergently pretty good to me to survive, become wiser, keep your options open,
[00:53:25.520 --> 00:53:30.000]   and then act from there. Um, and that looks, that ends up pretty similar to a lot of long-termism
[00:53:30.000 --> 00:53:35.440]   and existential risk. It's just that it's focused less on, and the main event will be what happens
[00:53:35.440 --> 00:53:38.960]   to future people. And it's, it's more about getting to the point where we are wise enough
[00:53:38.960 --> 00:53:44.080]   to understand and reorient, um, in a better way. Okay. Um, yeah, so what, what I find really
[00:53:44.080 --> 00:53:50.480]   interesting about this is that you can, um, yeah, you, uh, so different people tend to have like
[00:53:50.480 --> 00:53:55.200]   different thresholds for, um, epistemic learned helplessness, where they basically say, this is
[00:53:55.200 --> 00:53:59.200]   too weird. I'm not going to think about this. Uh, let's just stick with my current, uh, current,
[00:53:59.200 --> 00:54:04.160]   uh, moral theories. Um, so for, for somebody else, it might be before they became a long-termist
[00:54:04.160 --> 00:54:06.800]   where it's just like, yeah, surely the future people, what are we talking about here? That's
[00:54:06.800 --> 00:54:11.920]   not changing my mind on stuff. And then, yeah, for, for you, maybe it's before the
[00:54:11.920 --> 00:54:18.640]   infinite ethics stuff. Um, uh, is, is there some principled reason for thinking that this is where
[00:54:18.640 --> 00:54:26.080]   that stop should be, or is it just a matter of like temperament and openness? So I don't think
[00:54:26.080 --> 00:54:30.560]   there's a principled reason. And I should say, I don't think of my attitude towards infinite ethics
[00:54:30.560 --> 00:54:36.800]   as solely, Oh, this has gotten too far down the crazy, the crazy path I'm out. Um, it is this
[00:54:36.800 --> 00:54:42.720]   thing about the wisdom in the future is pretty important to me, um, as a, as a, a reason, uh,
[00:54:42.720 --> 00:54:51.680]   as an, as a mode of orientation, a first pass cut that I use is when do you feel like it's real? Um,
[00:54:51.680 --> 00:54:57.440]   if you feel like a thing is real, uh, as opposed to a kind of abstract fun argument, um, then
[00:54:57.440 --> 00:55:05.600]   that's important or that's, that's a real signal. And I think I, so, um, uh, and I generally
[00:55:05.600 --> 00:55:12.000]   encourage people if, if the sort of mode that I, I, I, I, I don't know, I'm drawn to is something
[00:55:12.000 --> 00:55:17.280]   like if there's an idea that seems compelling intellectually, that's a reason to investigate it
[00:55:17.280 --> 00:55:22.080]   a lot and think about it and really grapple with, you know, if, if this doesn't seem right to you,
[00:55:22.080 --> 00:55:27.040]   or if it seems too crazy, why, um, and really kind of processing, you know, it's a reason to
[00:55:27.040 --> 00:55:30.560]   pay a lot of attention, but if you've paid a lot of attention at the end of the day, you're like,
[00:55:30.560 --> 00:55:34.000]   well, I guess at an abstract level, that's sort of makes sense, but it just doesn't feel to me
[00:55:34.000 --> 00:55:39.280]   like the real world. It just doesn't feel to me like, um, wisdom or like a healthy way of living
[00:55:39.280 --> 00:55:43.600]   or whatever, then I'm like, well, maybe you shouldn't do it. Right. I mean, and, and, uh,
[00:55:43.600 --> 00:55:47.200]   and I think some people will do that wrong and they will end up bouncing off of ideas
[00:55:47.200 --> 00:55:54.960]   that are in fact good. Um, but you know, I think overall these, these are sort of sufficiently
[00:55:54.960 --> 00:56:02.000]   intense and difficult issues that, um, uh, kind of being actually persuaded and not just sort of
[00:56:02.000 --> 00:56:06.960]   chopping off the rest of your epistemology for the sake of some like version of the abstraction,
[00:56:06.960 --> 00:56:11.120]   uh, is, uh, it seems to me important and it's, and it's a sort of a healthier way to relate.
[00:56:11.120 --> 00:56:16.880]   Yeah. So another example of this, um, is that you have this really interesting blog post on ants,
[00:56:16.880 --> 00:56:24.880]   uh, and, uh, your, your, your, your thoughts after, uh, uh, sterilizing a colony of them.
[00:56:24.880 --> 00:56:32.160]   So, um, I, uh, yeah, so this is another example of a thing where almost everybody other than,
[00:56:32.160 --> 00:56:36.880]   I don't know, maybe a Jane who wears a face mask to prevent bugs from going into his mouth.
[00:56:36.880 --> 00:56:42.320]   Would say like, okay, at this point, if we're talking about how many he dons are in a hectare
[00:56:42.320 --> 00:56:48.880]   of forest from all the millions of insects there, um, then, uh, you've lost me. Um, but then, you
[00:56:48.880 --> 00:56:52.720]   know, somebody else might say, okay, well, there's not a strong reason for thinking they have no,
[00:56:52.720 --> 00:56:58.240]   absolutely no capacity to feel suffering. Um, uh, yeah. So I, I, I wonder how you think about
[00:56:58.240 --> 00:57:02.480]   such questions because you can't like stop living and not kill it. You're, you're not
[00:57:02.480 --> 00:57:05.680]   even going to stop going on road trips where you're probably killing hundreds of insects
[00:57:05.680 --> 00:57:10.400]   by just driving. Um, but yeah, so how, what do you think about such conundrums?
[00:57:10.400 --> 00:57:16.720]   The, I have significant uncertainty about, you know, exactly. And I think this is the appropriate
[00:57:16.720 --> 00:57:20.800]   position about exactly how much, uh, kind of consciousness or suffering or other forms of
[00:57:20.800 --> 00:57:24.800]   moral, you know, other ways, other kinds of properties that we associate with moral patienthood,
[00:57:24.800 --> 00:57:30.080]   how much those apply to different, um, different types of insects. Um, I think it's a strange view
[00:57:31.200 --> 00:57:38.080]   to be, you know, extremely confident that, uh, what happens with insects is, uh, totally morally
[00:57:38.080 --> 00:57:41.920]   neutral. And I think it actually doesn't fit with our common sense. So let's say you see,
[00:57:41.920 --> 00:57:48.080]   if you see a child like frying ants, uh, with, uh, with a magnifying glass, I think we, you know,
[00:57:48.080 --> 00:57:52.560]   there is some, uh, you know, what you couldn't say, ah, well, that just indicates that they're
[00:57:52.560 --> 00:57:56.880]   going to be cruel to other things that matter. Um, but, uh, I don't think so. I think, you know,
[00:57:56.880 --> 00:58:01.680]   and you see the ants like, you know, and they're, they're twitching around and, and, um, I, so I
[00:58:01.680 --> 00:58:06.240]   think we aren't, um, you know, as in many cases with an, with animal ethics, I think we're a bit
[00:58:06.240 --> 00:58:11.040]   like kind of schizophrenic about, about what cases we view as sort of morally relevant and, and which,
[00:58:11.040 --> 00:58:15.200]   which not, um, you know, we have, we have, you know, pet treatment laws and then we have factory
[00:58:15.200 --> 00:58:23.600]   farms and stuff like that. Um, so I don't see it as a radical position that ants matter somewhat.
[00:58:23.600 --> 00:58:29.280]   Um, I think there's a further question of what your overall practical response should,
[00:58:29.280 --> 00:58:34.800]   to that should be. And I think, um, the main, uh, and I do think, uh, the kind of costs as,
[00:58:34.800 --> 00:58:39.440]   as in a lot of ethical life, there are trade-offs and you have to make, um, you have to make a call
[00:58:39.440 --> 00:58:43.440]   about what, what sort of constraints you're going to put on yourself at the cost of other goals.
[00:58:43.440 --> 00:58:48.080]   And, um, in, you know, in the case of insects, it's not my, my current moral focus and I don't
[00:58:48.080 --> 00:58:53.280]   pay a lot of costs to kind of, um, uh, to lower my impact on animals. And I don't, you know,
[00:58:53.280 --> 00:59:00.320]   I don't, I don't sweep the sidewalk or anything, or sorry, on, on, on ants in particular. Um, uh,
[00:59:00.320 --> 00:59:04.000]   and so I think it's, I think, and I think that's, you know, that's my best guess response and that,
[00:59:04.000 --> 00:59:08.560]   and that has to do with other ethical priorities in my life. Um, but I think, you know, there's,
[00:59:08.560 --> 00:59:14.160]   there's a middle ground between, um, I shall ignore this completely and I shall, you know,
[00:59:14.160 --> 00:59:18.320]   be a Jane, um, which is recognizing that this is a, this is a real trade-off. There's uncertainty
[00:59:18.320 --> 00:59:22.400]   here and, um, uh, and taking responsibility for how you're responding to that.
[00:59:22.400 --> 00:59:29.040]   Yeah. This seems, um, kind of similar to the infinite ethics example, where if you put any
[00:59:29.040 --> 00:59:36.320]   sort of credence that, um, they have any ability to suffer, then at least if you're, uh, not going
[00:59:36.320 --> 00:59:40.720]   to say that, Oh, it doesn't matter because like the far future trillions and trillions of ants,
[00:59:40.720 --> 00:59:48.000]   um, um, it seems like there should be, uh, a compelling, uh, a compelling thing to think
[00:59:48.000 --> 00:59:54.080]   about. But then the result is, um, yeah, it's, it's not even like become a vegan where it's
[00:59:54.080 --> 00:59:59.760]   like you change your diet. Um, uh, and then, so, you know, as you might know, this is used as a
[00:59:59.760 --> 01:00:05.440]   reducto ad absurdum of veganism where, you know, if you're going to start caring about other, uh,
[01:00:05.440 --> 01:00:09.040]   non-human animals, why not also care about insects? And even if they're worth like a
[01:00:09.040 --> 01:00:13.040]   million of a cow, then, you know, you're probably still killing like a million of them on any given
[01:00:13.040 --> 01:00:17.680]   day from all your activities, uh, indirectly maybe, uh, like, I don't know, like the foods
[01:00:17.680 --> 01:00:21.360]   you're eating, all the pesticides that are used to create that food. I don't know how
[01:00:21.360 --> 01:00:25.520]   you go about resolving that kind of stuff. I mean, I guess I'd want to really hear the
[01:00:25.520 --> 01:00:30.160]   empirical case. I think, um, uh, I think it's true. You know, there are a lot of insects,
[01:00:30.160 --> 01:00:36.880]   uh, and, but you know, I think it's easy. Uh, you know, I think if you want to say like,
[01:00:36.880 --> 01:00:44.400]   uh, taking seriously sort of the idea that, um, there's some reason to, to not like squash,
[01:00:44.400 --> 01:00:51.280]   squash a bug, um, if you see it leads immediately to kind of Jane, like behavior, uh, absent long
[01:00:51.280 --> 01:00:55.360]   termism or something like that. I really, I feel like I want to hear the empirical case about like
[01:00:55.360 --> 01:01:00.000]   exactly what impact you're having and, and how, um, and, and I'm not at all persuaded that that's
[01:01:00.000 --> 01:01:04.480]   the practical upshot. Um, uh, and if it is, if that's a really strong case, then I think that's
[01:01:04.480 --> 01:01:10.480]   an interesting, um, an interesting, uh, uh, you know, that's an interesting kind of implication
[01:01:10.480 --> 01:01:15.120]   of, of this view. Um, and, uh, and, you know, we're, we're concerned, but I wouldn't jump,
[01:01:15.120 --> 01:01:20.400]   it feels to me like it's easy to jump to that almost out of a desire to, to get to the reductio,
[01:01:20.400 --> 01:01:24.960]   um, without kind of, I would try to move slower and, and really see it's like, wait, is that right?
[01:01:24.960 --> 01:01:29.040]   There are a lot of trade-offs here. What's the source of my hesitation about that? Um, and kind
[01:01:29.040 --> 01:01:34.320]   of, uh, and not, not jump too quickly to something that's sufficiently absurd that I can be like,
[01:01:34.320 --> 01:01:37.600]   uh, therefore I get to reject this whole mode of thinking, even though I don't know why.
[01:01:37.600 --> 01:01:43.200]   I see. Yeah. Um, okay. So let's talk about, uh, the two different ways of thinking about
[01:01:43.200 --> 01:01:49.920]   observer effects and their implications. So do, do, do you want to, uh, explain, um, you have
[01:01:49.920 --> 01:01:54.400]   a four-part series on this, but do you want to explain, uh, the self-indication assumption and
[01:01:54.400 --> 01:02:01.440]   the self-sampling assumption? Uh, uh, I know it's a big topic, but, uh, yeah, as much as possible.
[01:02:03.040 --> 01:02:10.880]   Sure. So I think one way to start to get into this debate is by thinking about the following case.
[01:02:10.880 --> 01:02:15.520]   So, um, you wake up in a white room and there's a message written on the wall. And let's say,
[01:02:15.520 --> 01:02:20.480]   you're going to believe this message. And the message says, I, God, it's from God. I, God
[01:02:20.480 --> 01:02:26.080]   created, I, I flipped a coin. Um, and if it was heads, I created one person in a white room. And
[01:02:26.080 --> 01:02:32.640]   if it was tails, I created a million people all in white rooms. And now you are asked to assign
[01:02:32.640 --> 01:02:41.840]   probabilities, uh, to, uh, the coin having come up heads versus tails. Um, and, uh, so one approach
[01:02:41.840 --> 01:02:47.040]   to this question, um, uh, which is the approach I favor, or at least think is better than the other,
[01:02:47.040 --> 01:02:52.640]   uh, is the self, the self-indication assumption. Um, these names are terrible. Um, and, uh,
[01:02:52.640 --> 01:03:00.240]   but you know, so it goes, um, so SIA, uh, says that, uh, your probability that the coin came up
[01:03:00.240 --> 01:03:05.440]   heads should be approximately one in a million. Um, and that's because SIA thinks it's more likely
[01:03:05.440 --> 01:03:11.200]   that you exist in worlds where, uh, there are more people in your epistemic situation or more people
[01:03:11.200 --> 01:03:15.200]   who have your evidence, which in this case is just waking up in this white room. Uh, and that's it.
[01:03:15.200 --> 01:03:19.840]   And so that's, that can be a weird conclusion and go to weird places. Um, but I think, uh, it's a
[01:03:19.840 --> 01:03:24.320]   better conclusion than the alternative SSA, uh, which is the main alternative I consider in that
[01:03:24.320 --> 01:03:30.800]   post, which is the self-sampling assumption, um, uh, says that you should, you think it more likely
[01:03:30.800 --> 01:03:37.760]   that you exist in worlds where people with your evidence are a larger fraction of, uh, something
[01:03:37.760 --> 01:03:44.640]   called your reference class, um, uh, where it's quite opaque what, what a reference class is
[01:03:44.640 --> 01:03:49.440]   supposed to be, but broadly speaking, a reference class is the sort of set of people you could have
[01:03:49.440 --> 01:03:57.520]   been, or that's kind of how it functions in SSA's discourse. So, um, uh, in this case, in both cases,
[01:03:57.520 --> 01:04:05.040]   everyone has your evidence. Um, and so the fraction is the same. Um, uh, and so you, you stick with the
[01:04:05.040 --> 01:04:12.160]   one half prior. Um, but that's not true. So SSA in other contexts, um, not everyone has your evidence.
[01:04:12.160 --> 01:04:18.160]   And so it updates towards worlds, um, where it's a larger fraction. So famously, uh, SSA leads to
[01:04:18.160 --> 01:04:23.680]   what's known as the doomsday argument, um, where you imagine that, uh, there are two possibilities,
[01:04:23.680 --> 01:04:28.480]   either humanity will go extinct very soon, um, or we won't go extinct very soon. And there will
[01:04:28.480 --> 01:04:34.880]   be tons of people in the future. Um, and in the former case, uh, and then you imagine, um, everyone
[01:04:34.880 --> 01:04:40.400]   is sort of ranked in terms of when they're born. Um, uh, in the former case, people born, you know,
[01:04:40.400 --> 01:04:45.760]   at roughly this time, um, are a much larger percentage of all the people who ever lived.
[01:04:45.760 --> 01:04:50.560]   Um, and so if you imagine, you know, God first creates a world and then he inserts you randomly
[01:04:50.560 --> 01:04:55.920]   into like some group, it's much more likely, uh, that you would find yourself in the 21st century,
[01:04:55.920 --> 01:05:01.280]   um, if humanity goes extinct soon than if it's, uh, if there are tons of people in the future,
[01:05:01.280 --> 01:05:04.480]   if God randomly inserted you into these tons of people in the future, then it's like,
[01:05:04.480 --> 01:05:10.800]   really, that's, it's a tiny fraction of them are in the 21st century. Um, so SSA in other contexts,
[01:05:10.800 --> 01:05:14.080]   actually, you know, it has these important implications, namely that in this case,
[01:05:14.080 --> 01:05:18.880]   you update very, very hard towards the future being short. Um, and that matters a lot for
[01:05:18.880 --> 01:05:23.440]   long-termism because, uh, long-termism is all about the future being big in expectation.
[01:05:23.440 --> 01:05:28.400]   Okay. So, and then how, what is it, what does the SIA take on this?
[01:05:28.400 --> 01:05:36.960]   Yeah. So I think a way to think about SIA is kind of story. So I gave this story about SS, SSA,
[01:05:36.960 --> 01:05:42.240]   which is, it's sort of like this. It's like, first God creates a world. This is SSA. First,
[01:05:42.240 --> 01:05:47.200]   he creates a world and then he takes, and he's dead set on, on putting you into this world. So
[01:05:47.200 --> 01:05:51.280]   you, he's got your soul, right? And he really wants, and your soul is going in there no matter
[01:05:51.280 --> 01:05:56.160]   what. Right. Um, but the way he's going to insert your soul into the world is by throwing you
[01:05:56.160 --> 01:06:02.560]   randomly into some set of people, um, the reference class. Uh, and so if you wake, so you
[01:06:02.560 --> 01:06:09.920]   should expect, um, to end up in the world where, uh, the, the kind of person you end up as, uh, is,
[01:06:09.920 --> 01:06:14.880]   is sort of, um, more like a more likely result of that throwing process is a sort of larger
[01:06:14.880 --> 01:06:20.400]   fraction of, of the total people you could have been. What SSA or what SIA thinks is different,
[01:06:20.400 --> 01:06:25.840]   the way the story that I'll use for SIA, though, it doesn't, this isn't the only gloss is God
[01:06:25.840 --> 01:06:32.480]   decides he's going to create a world. And then he, and say, there's like a big line of souls
[01:06:32.480 --> 01:06:38.560]   in heaven and he goes and grabs them kind of randomly out of heaven and puts them into the
[01:06:38.560 --> 01:06:43.520]   world. Right. And so in that case, if there are more people in the world, then you've got more
[01:06:43.520 --> 01:06:48.400]   shots at B and you're one of these souls, you're sort of sitting in heaven, hoping to get creative.
[01:06:48.400 --> 01:06:55.520]   Um, uh, on SIA, God has more chances to grab you out of, out of heaven and put you into the world.
[01:06:55.520 --> 01:07:01.040]   If there are more people, uh, who, uh, more people like you in that world. Um, and so you should
[01:07:01.040 --> 01:07:04.240]   expect to be in a world where there was sort of, there are, there are more such people. And that,
[01:07:04.240 --> 01:07:09.440]   that's, that's kind of SIA's vibe. Doesn't this also imply that you should be in the future,
[01:07:09.440 --> 01:07:14.320]   assuming there'll be more people in the future? Tell me more about why, why would it apply that?
[01:07:14.320 --> 01:07:19.440]   Okay. In an analogous scenario, maybe like, uh, go back to the God tossing the coin scenario
[01:07:19.440 --> 01:07:26.320]   where if you did just substitute for people in right rooms, you substitute, um, being a thing,
[01:07:26.320 --> 01:07:30.320]   uh, a conscious entity. And if there's going to be more conscious entities in the future,
[01:07:30.320 --> 01:07:35.840]   like you, you would really expect to just like in that example of being in that scenario where
[01:07:35.840 --> 01:07:39.200]   there's a lot more rooms, but just as maybe you should expect that you to be in that scenario
[01:07:39.200 --> 01:07:43.280]   where there's a lot more conscious beings, which presumably is the future. So then it's still odd
[01:07:43.280 --> 01:07:49.520]   that you're in the present, uh, under SIA. Yes. So in, in a specific sense, so, um,
[01:07:49.520 --> 01:07:58.000]   it's true that on SIA, uh, say that, um, say that we don't know what room you're in
[01:07:58.560 --> 01:08:04.880]   first, right? So, so, um, you, you wake up in the white room and you're wondering, uh, am I in room
[01:08:04.880 --> 01:08:13.360]   one or am I in rooms two through a million? Right. Um, and on SIA, what you did first, so you woke
[01:08:13.360 --> 01:08:18.000]   up and you don't know what room you're in, but there's a lot more people in the world with lots
[01:08:18.000 --> 01:08:22.160]   of rooms. And so you become very, very confident that you're in that world, right? So you're very,
[01:08:22.160 --> 01:08:28.160]   very confident on tails. And then you're right that, uh, conditional on tails, you think it's,
[01:08:28.160 --> 01:08:32.240]   uh, much more like you, you sort of split your credence evenly between all these rooms. So you
[01:08:32.240 --> 01:08:37.280]   are very confident that you're in one of the sort of two through a million rooms and not, not room
[01:08:37.280 --> 01:08:44.400]   one. Um, but that's before you've seen your room number. Um, once you see your room number, it's
[01:08:44.400 --> 01:08:51.120]   true that you should be quite surprised about your room number. Um, uh, but the, uh, once you get the
[01:08:51.120 --> 01:08:57.200]   room number, you're back, you back to 50, 50 on, uh, heads versus tails because you had sort of
[01:08:57.200 --> 01:09:04.160]   equal credence in being in room one, uh, conditional on tails, um, or sorry, uh, you had equal credence
[01:09:04.160 --> 01:09:10.400]   in being in tails in room one, uh, and, uh, heads in room one. And so when you get rid of all of the
[01:09:10.400 --> 01:09:15.760]   other tails and rooms two through a million, you're left with 50, 50 overall on heads versus tails.
[01:09:15.760 --> 01:09:23.840]   Um, and so, uh, the, the sense in which SIA leaves you back at normality with the doomsday
[01:09:23.840 --> 01:09:28.880]   argument is once you update on being in the 21st century, which admittedly should be surprising.
[01:09:28.880 --> 01:09:32.640]   Like if you didn't know what, that you were in the 21st century and then you learned that you were,
[01:09:32.640 --> 01:09:36.720]   you should be like, wow, that's really unexpected and fair. So, and that's, that's true. But I think
[01:09:36.720 --> 01:09:42.720]   once you do that, you're back at, um, uh, you know, whatever your prior was about, about extinction.
[01:09:42.720 --> 01:09:47.280]   Maybe I'm still not sure on why the fact that you were surprised should not itself
[01:09:47.280 --> 01:09:52.240]   be the doomsday argument. Yeah. I think there's an intuition there. Um, which is sort of like,
[01:09:52.240 --> 01:09:57.920]   yeah, is SIA making a bad prediction? So you, you could, you could kind of update against SIA
[01:09:57.920 --> 01:10:03.200]   because SIA would have predicted that you're in the future. Um, I, I think there's something
[01:10:03.200 --> 01:10:08.080]   there and I think there's a few other analogs. Um, like for example, I think SIA naively predicts
[01:10:08.080 --> 01:10:13.200]   that, um, you know, you, you should find yourself in a situation where there are just
[01:10:13.200 --> 01:10:17.440]   tons of people that, you know, a situation obsessed with creating people with your evidence.
[01:10:17.440 --> 01:10:21.280]   Um, and you know, this is one of the, one of the problems with SIA. So you should expect to find,
[01:10:21.280 --> 01:10:25.040]   you know, in every nook and cranny, a simulation of you, as soon as you like, you know, you open
[01:10:25.040 --> 01:10:29.600]   the door, it's actually this giant bank of simulations of you in like your previous epistemic
[01:10:29.600 --> 01:10:33.680]   state. Um, and so, you know, I think there are, and then you don't see that you might be like,
[01:10:33.680 --> 01:10:37.840]   well, I should update against the entropic theory that predicted, uh, that I would see
[01:10:37.840 --> 01:10:41.920]   that. And I, I, I think there are arguments in that vein. Yeah. So maybe let's back up to go
[01:10:41.920 --> 01:10:49.040]   to the original example, uh, that w that was used to distinguish these two theories. Yeah. So I, I,
[01:10:49.040 --> 01:10:52.960]   I can, can you help me resolve my intuitions here where my, my intuition is very much as I say,
[01:10:52.960 --> 01:10:58.000]   because, um, yeah, it, it seems to me that, uh, you knew you were going to wake up, right. You
[01:10:58.000 --> 01:11:01.520]   knew you were going to wake up in a white room before you actually did wake up and make your
[01:11:01.520 --> 01:11:05.840]   prior should have been like one half of heads or tails. So it's not clear to me why having learned
[01:11:05.840 --> 01:11:11.520]   nothing new, your, uh, your, your posterior probability on either of those scenarios should
[01:11:11.520 --> 01:11:18.800]   change. So I think the, the SIA response to that would be, or at least I think a way of making it
[01:11:18.800 --> 01:11:23.520]   intuitive would be to say that you didn't know that you were going to wake up. Right. So in the,
[01:11:23.520 --> 01:11:30.080]   um, if we go back to that, just so story where God is grabbing you out of the, um, out of heaven,
[01:11:30.080 --> 01:11:35.440]   uh, you know, it's, uh, it's not at all, it's actually incredibly unlikely that he grabs you.
[01:11:35.440 --> 01:11:38.800]   There are so many, so many people. Yeah. I mean, there's a different thing where SIA is in general,
[01:11:38.800 --> 01:11:44.400]   very surprised to exist. Um, and in fact, that's, uh, the, um, so you could make the same arguments
[01:11:44.400 --> 01:11:48.080]   like SIA says you shouldn't exist. Isn't that weird that you exist. Um, and I actually think
[01:11:48.080 --> 01:11:55.440]   that's a, that's a good argument. Um, or is it, uh, so, um, but, uh, once you're in that headspace,
[01:11:55.440 --> 01:11:59.680]   then I think the way, the way to think about it is that it's not a guarantee that you,
[01:11:59.680 --> 01:12:04.560]   you were, God is not dead set on creating you. You are a particular contingent arrangement
[01:12:04.560 --> 01:12:09.600]   of the world. Um, and so that, that you should expect that arrangement to, to come about more
[01:12:09.600 --> 01:12:14.000]   often if there are more arrangements of that type, um, rather than sort of assuming that,
[01:12:14.000 --> 01:12:18.480]   that no matter what existence will include you. Yes. Okay. Can you talk more about the
[01:12:18.480 --> 01:12:23.760]   problems with SSA like scenarios where you think it breaks down, uh, like why you prefer SIA?
[01:12:25.120 --> 01:12:31.280]   Yeah. So, um, an easy, an easy problem, uh, or sort of one of the most dramatic problems is,
[01:12:31.280 --> 01:12:37.840]   is that SIA SSA, um, predicts that, uh, it's possible to have a kind of telekinetic influence
[01:12:37.840 --> 01:12:43.280]   on the world. So imagine that there's a, um, there's a puppy you you're, you're in an, you,
[01:12:43.280 --> 01:12:48.000]   you wake up and you're in an empty universe, except for this puppy and you, and this boulder
[01:12:48.000 --> 01:12:51.840]   that's rolling towards the puppy. Right. And the boulder is inexorably going to kill the puppy.
[01:12:51.840 --> 01:12:55.840]   Um, it's very large boulder. It's basically guaranteed that the puppy is dead meat,
[01:12:55.840 --> 01:13:02.560]   but you have the power to make binding pre-commitments, um, that you will in fact
[01:13:02.560 --> 01:13:07.680]   execute. And, and you have also to your right, a button that would allow you to create tons of
[01:13:07.680 --> 01:13:12.640]   people like zillions and zillions and zillions of people, all of whom, um, are wearing different
[01:13:12.640 --> 01:13:17.280]   clothes from you. Uh, so they would be in a different epistemic state than you, if you,
[01:13:17.280 --> 01:13:23.280]   if you created them. Um, now SSA, uh, so you, so you make the following resolution. You say,
[01:13:23.280 --> 01:13:30.560]   um, if, uh, this boulder does not jump out of the way of this puppy, um, like the boulder leaps in,
[01:13:30.560 --> 01:13:36.000]   you know, in some very weird, very unlikely way. Um, uh, then I will press this button and I will
[01:13:36.000 --> 01:13:40.560]   create zillions and zillions of people. Um, uh, all of whom are in a different epistemic state
[01:13:40.560 --> 01:13:46.320]   than me, but let's assume they were in my reference class. Um, SSA thinks that it's
[01:13:46.320 --> 01:13:52.240]   sufficiently unlikely that you would be in a world with zillions of those people. Um, and, uh,
[01:13:52.240 --> 01:13:55.760]   but you, you know, you at the very beginning, uh, with, with a different, with different colored
[01:13:55.760 --> 01:13:59.200]   clothes, because, you know, that was a tiny fraction of the reference class. If those people
[01:13:59.200 --> 01:14:04.080]   get created, um, that SSA thinks it's actually more likely once you've made that commitment,
[01:14:04.080 --> 01:14:09.200]   that the boulder will jump out of the way. Um, and, uh, and you know, so that, and that looks
[01:14:09.200 --> 01:14:12.640]   weird, right? It's, it's, it seems like that's not going to work. You can't, you can't just make
[01:14:12.640 --> 01:14:16.480]   that commitment and then expect the boulder to jump. Um, and you get to that. So that's the sort
[01:14:16.480 --> 01:14:21.040]   of exotic example. You get, you get similar analogs, even in the God's coin toss case where,
[01:14:21.040 --> 01:14:26.160]   um, like naively, it doesn't actually matter whether God has tossed the coin yet. Right.
[01:14:26.160 --> 01:14:32.640]   So suppose, um, yeah, so like, let's say, let's say you wake up and learn that you're in room one,
[01:14:32.640 --> 01:14:37.520]   right. Um, but God hasn't tossed the coin. It's like he created room one first before he tossed,
[01:14:37.520 --> 01:14:41.360]   and then he's going to toss and that's going to determine whether or not he creates all the rooms
[01:14:41.360 --> 01:14:46.880]   in the future. Um, if you on SSA, once you wake up and learn, um, learn that you're in room one,
[01:14:46.880 --> 01:14:50.720]   you think it's incredibly unlikely that there's going to be these future people. So you, now you
[01:14:50.720 --> 01:14:54.560]   say before that it's a fair coin, God's going to toss it in front of you. You're still going to
[01:14:54.560 --> 01:14:58.880]   say, I'm sorry, God, it's, uh, you know, it's a one in a million chance that this, uh, that this
[01:14:58.880 --> 01:15:04.320]   coin lands tails, um, and, uh, or sorry, one of the million, something like very small number.
[01:15:04.320 --> 01:15:09.760]   I forget exactly. And that's, um, and that's very weird. That's a fair coin. It hasn't been tossed,
[01:15:09.760 --> 01:15:15.040]   but you with the power of SSA have become extremely confident about, about what, how it's
[01:15:15.040 --> 01:15:19.040]   going to land. Um, and that's, uh, so that's, that's another argument. There's a, there's a
[01:15:19.040 --> 01:15:25.040]   number of other, uh, I think really, really bad problems for SSA. Yeah. Well, while I digest that,
[01:15:25.040 --> 01:15:33.120]   um, so let me, uh, let, let me just mention the problems you already pointed out against, uh, SIA,
[01:15:33.120 --> 01:15:38.960]   um, in the post and, uh, earlier where, where if, if, if one thinks SIA is true, one should be
[01:15:38.960 --> 01:15:43.760]   very confident that there are, you're, you're in a universe with many other people who have been
[01:15:43.760 --> 01:15:49.680]   sampled just like you. And so then it's, um, then it's kind of surprising that we're in a universe
[01:15:49.680 --> 01:15:57.200]   that is not filled to the brim with people. Like there's a lot of, um, you could imagine like Mars
[01:15:57.200 --> 01:16:04.400]   is just completely made up of bodies, um, or, you know, like every single star has like, you know,
[01:16:04.400 --> 01:16:11.760]   a simulation of a trillion people inside. Um, the fact that this is not happening seems like,
[01:16:11.760 --> 01:16:15.600]   uh, it seems like very strong evidence against SIA. And then, you know, there's other things
[01:16:15.600 --> 01:16:19.040]   like the presumptuous philosopher that you might want to talk about as well, but, um, yeah. So
[01:16:19.040 --> 01:16:22.960]   do you just bite the bullet on these things or how do you think about these things?
[01:16:24.000 --> 01:16:30.480]   My main claim is that SIA is better than SSA. Um, and, and I think it's just a horrible situation
[01:16:30.480 --> 01:16:38.720]   with, with Amtropics and, and, um, it's, I think overall SIA is an update towards bigger,
[01:16:38.720 --> 01:16:44.400]   more populated universes. Um, I think, you know, the most salient populated universes don't involve
[01:16:44.400 --> 01:16:49.600]   like hidden people on other planets, but they're probably, um, I don't know, maybe we're in a
[01:16:49.600 --> 01:16:53.440]   simulation and people are, you know, obsessed with simulating us or, or, or something like that,
[01:16:53.440 --> 01:16:58.480]   or, and then I think this is actually more important and worrying is I think the way I
[01:16:58.480 --> 01:17:04.400]   see this dialectic is, um, first SIA, I mean, so a big problem with SIA is it immediately becomes
[01:17:04.400 --> 01:17:08.640]   certain naively that you live in an infinite universe or a universe with an infinite number
[01:17:08.640 --> 01:17:15.120]   of people. Um, uh, and that, and then it breaks because, and it doesn't know how to compare,
[01:17:15.120 --> 01:17:20.240]   um, uh, kind of infinite universes. Now, to be fair, SSA also isn't great at comparing
[01:17:20.240 --> 01:17:24.640]   infinite universes. Um, and they both have some, you can do things that are actually quite analogous
[01:17:24.640 --> 01:17:30.080]   to things you can try to do in infinite ethics, where you have like expanding spheres of, uh,
[01:17:30.080 --> 01:17:33.840]   space-time and you, you count, you know, you have some fraction or some density of people
[01:17:33.840 --> 01:17:37.920]   in those spheres. Um, and there's this general problem in cosmology of, of like trying to
[01:17:37.920 --> 01:17:41.600]   understand what, what it means to have like a fraction or a density of, of, of different types
[01:17:41.600 --> 01:17:47.760]   of observers. Um, but you know, my own take is kind of what happens here is you, we hit this
[01:17:47.760 --> 01:17:52.400]   infinity, you hit infinite universes fairly fast, and then they kind of break your anthropics
[01:17:52.400 --> 01:17:58.960]   in analogous ways to how they break your ethics. Um, and that's kind of where I'm currently at.
[01:17:58.960 --> 01:18:04.640]   And I'm, I'm hoping to understand better, uh, how to do anthropics with infinities. Um, and, um,
[01:18:04.640 --> 01:18:09.280]   some of my work on the universal distribution, uh, which is a sort of, I have, I have a couple
[01:18:09.280 --> 01:18:12.720]   blog posts on that was attempting to go a little bit in that direction, though it has its own
[01:18:13.360 --> 01:18:21.520]   giant problems. Okay. Interesting. Um, uh, do you know if, um, I just vaguely seems to be that the
[01:18:21.520 --> 01:18:26.800]   Robin Hanson's grabby aliens thing probably uses SSA. Uh, but did you, did you know if that's the
[01:18:26.800 --> 01:18:33.280]   case? Uh, if he's using SSA in there, I don't, I haven't looked closely at that work. Okay. Okay.
[01:18:33.280 --> 01:18:36.400]   Cool. I don't know. It's hard for me to think about. So maybe it'll take me a few more,
[01:18:36.400 --> 01:18:41.600]   few more weeks before I can, uh, digest it, uh, fully, but, um, yeah. Okay. So that's really
[01:18:41.600 --> 01:18:46.320]   interesting. You have a really interesting blog post about believing in things you cannot see.
[01:18:46.320 --> 01:18:50.880]   Um, and one, I mean, this is almost an aside in the post itself, but I thought it was a really
[01:18:50.880 --> 01:18:56.480]   interesting comment. You make an interesting comment about futurism. Uh, here's what you say
[01:18:56.480 --> 01:19:03.600]   much of futurism in my experience has a distinct flavor of unreality. The concepts mind uploads,
[01:19:03.600 --> 01:19:08.560]   nanotechnology settlement and energy capture in space are, I think, meaningful, even if loosely
[01:19:08.560 --> 01:19:13.280]   defined, but at a certain point, once models become so abstracted and incomplete that the
[01:19:13.280 --> 01:19:18.960]   sense of talking about a real thing, even a possibly real thing is lost. Yeah. So why do
[01:19:18.960 --> 01:19:24.800]   you think that is? And is there a way to do futurism better? I think it comes partly because
[01:19:24.800 --> 01:19:30.960]   imagination is just quite a limited tool and it's just easy. You know, when you're talking about the
[01:19:30.960 --> 01:19:36.240]   whole world, like the future is a big thing to try to model with this tiny mind. And so, you know,
[01:19:36.240 --> 01:19:42.400]   of necessity, you need to use these extremely lossy abstractions. Um, and so, you know, uh,
[01:19:42.400 --> 01:19:50.160]   it puts you in a mode of having these like, you know, really sketchy and gappy maps that you're
[01:19:50.160 --> 01:19:55.280]   trying to, to manipulate. Um, I think that's one dimension. And then I think there's also a way in
[01:19:55.280 --> 01:20:03.680]   which, um, you know, this isn't all that unique to futurism in, in so far as just in general,
[01:20:03.680 --> 01:20:09.920]   I think it's hard sometimes to keep our, uh, intellectual engagement kind of rooted and
[01:20:09.920 --> 01:20:15.440]   grounded in the kind of real world. And I, you know, I think it's just easy to kind of move
[01:20:15.440 --> 01:20:20.400]   into a zone. And especially if that zone is inflected with kind of social dynamics or it's,
[01:20:20.400 --> 01:20:24.080]   you know, it's, it's kind of like a intellectual game where you're enjoying it for its own sake,
[01:20:24.080 --> 01:20:26.960]   or it's, it's like a sort of, there's sort of status dimensions and the way people talk and
[01:20:26.960 --> 01:20:31.600]   other things that I think start to move our discourse and, uh, in directions that aren't
[01:20:31.600 --> 01:20:35.120]   about like, ah, we're talking about the real world right now, let's actually get it right.
[01:20:35.120 --> 01:20:39.680]   And I think that happens with futurism. Um, and, and maybe more so because it can feel like,
[01:20:39.680 --> 01:20:44.400]   like, I think some people, there's sort of topics that they treat as like, ah, that's a real serious
[01:20:44.400 --> 01:20:48.240]   topic. That's about real stuff. And then there are other topics where it's like, this is the chance
[01:20:48.240 --> 01:20:53.600]   to kind of make stuff up. Um, and you know, my experience is sometimes people relate to futurism
[01:20:53.600 --> 01:20:57.840]   that way. There are other topics where people move into a zone of like, one can just say stuff here.
[01:20:57.840 --> 01:21:01.600]   Um, and there were kind of no constraints. Uh, and I think, I think that's actually wrong. And,
[01:21:01.600 --> 01:21:05.680]   and, and with futurism, I think there are, there are important, um, constraints and important
[01:21:05.680 --> 01:21:11.440]   things we can say. Um, but, uh, I think that can, that vibe can seep in nonetheless.
[01:21:11.440 --> 01:21:16.400]   Yeah. And it's interesting that it's true of the future and the past. Um, I recently
[01:21:16.400 --> 01:21:21.040]   interviewed somebody who wrote a book about the Napoleonic war and yeah, it is, I mean,
[01:21:21.040 --> 01:21:25.360]   it's very interesting to talk about in a sort of abstract sense, but then also you can, um,
[01:21:25.360 --> 01:21:31.280]   which is very seldom done. You can like, think of the reality of like a million men marching out of
[01:21:31.280 --> 01:21:36.480]   Russia and freezing and eating the remains of horses and other people and then starving. Um,
[01:21:36.480 --> 01:21:40.160]   and the, the, the, the, the, then the, the concrete reality when you're not, yeah,
[01:21:40.160 --> 01:21:43.520]   when you're not in some abstractions, like, Oh, the border changed so much in these few decades
[01:21:43.520 --> 01:21:50.000]   or something. Um, yeah, just how you think about history changes so much and it becomes, um, yeah,
[01:21:50.000 --> 01:21:56.480]   even recently I was reading this book about, um, the, uh, the, the use of meth by the Nazis.
[01:21:56.480 --> 01:22:02.080]   Um, and if you just, there's, there's this really cynical part of the book where the, the leaders,
[01:22:02.080 --> 01:22:06.960]   um, in the, in the Nazi regime, they're talking about like, Oh, meth is the perfect drug because
[01:22:06.960 --> 01:22:12.640]   it gives them courage to kind of just blitz through an area, um, without any sort of, uh,
[01:22:12.640 --> 01:22:17.520]   without thinking about how cold it is without thinking about how scary it is to just be no
[01:22:17.520 --> 01:22:21.760]   man's land. And just this idea of like this meth up soldier, who's like been forced to
[01:22:21.760 --> 01:22:28.080]   just go out into the middle of nowhere. Um, and yeah, and then all like marching to Russia or
[01:22:28.080 --> 01:22:32.320]   something, uh, in the winter. I don't know if that was going to lead up to a question. I don't
[01:22:32.320 --> 01:22:36.240]   know if you have a reaction, but yeah. Yeah. I mean, I think, so I think that's a great example
[01:22:36.240 --> 01:22:40.880]   of, um, or it's, you know, specifically the sort of image of the difference between relating to
[01:22:40.880 --> 01:22:44.720]   history as this sort of, how is the border changing versus the concreteness of these
[01:22:44.720 --> 01:22:50.240]   people? And, you know, often I think engaging with history is horrifying in this respect is
[01:22:50.240 --> 01:22:54.320]   when you really bring to mind that the lived reality of all these events, it's, um, a really
[01:22:54.320 --> 01:23:01.360]   different, uh, experience. And I think to some extent, one of the reasons that, um, concreteness
[01:23:01.360 --> 01:23:06.880]   might be often lacking from futurism is that you can't, any, any attempt to specify the thing
[01:23:06.880 --> 01:23:11.040]   will be wrong. Um, so, you know, we can, you can, you might be writing about some abstract thing,
[01:23:11.040 --> 01:23:15.440]   like you might be like, oh, we will, you know, we will have, uh, the ability to manipulate matter
[01:23:15.440 --> 01:23:21.920]   at like blob, you know, blah level, uh, you know, uh, you know, uh, scale. But, um, if you try to
[01:23:21.920 --> 01:23:25.760]   dig in and then you're like, and here's what it's like to wake up in the future, you know, and, and
[01:23:25.760 --> 01:23:30.640]   then, you know, the, you're eating the, or whatever, and it's just, you're wrong immediately
[01:23:30.640 --> 01:23:36.480]   that that's not how it's going to be. Um, and so you don't have the, um, the ability to really hone
[01:23:36.480 --> 01:23:41.920]   in on concrete details that are actually true. Um, and so in some sense you need to, there's this
[01:23:41.920 --> 01:23:45.600]   like back and forth where you need to sort of imagine a concrete thing and then be like, okay,
[01:23:45.600 --> 01:23:50.160]   that's wrong, but there will then take the flavor of concreteness that you got from that and say,
[01:23:50.160 --> 01:23:55.600]   but it will be a concrete thing. It just won't be the specific one I imagined. Um, and then keep
[01:23:55.600 --> 01:24:00.400]   that flavor of concreteness, even as you talk in more abstract ways. And that's, I think, a delicate
[01:24:00.400 --> 01:24:07.360]   dance. Yeah. Yeah. As many viewers will know that Peter's Hill has this like very, uh, talking point
[01:24:07.360 --> 01:24:14.400]   that it often brings up about, uh, our, uh, that w we've become indefinite optimist. Um, and that he
[01:24:14.400 --> 01:24:18.320]   prefers a sort of definite optimism where you have a concrete vision of what the future could be.
[01:24:18.320 --> 01:24:23.360]   Um, um, okay. So yeah, uh, I guess to close out what, what, one of the things I wanted to ask you
[01:24:23.360 --> 01:24:29.120]   about was, uh, so you said this was a side project, this blog. Um, I thought it was one of the
[01:24:29.120 --> 01:24:33.600]   actually before you mentioned that you're the one, your main work is AI. I thought this was
[01:24:33.600 --> 01:24:37.120]   at least part of your main work. And so it's surprising. It's really surprising to me then
[01:24:37.120 --> 01:24:41.680]   that, uh, you're able to keep up the regularity. It's like, basically you're publishing a small
[01:24:41.680 --> 01:24:47.120]   book every, I don't know, every week or so. And, um, uh, filled with a lot of insight.
[01:24:47.120 --> 01:24:53.600]   And I mean, it's like, well, so, uh, unlike many other blogs on the internet were just plain style.
[01:24:53.600 --> 01:24:58.800]   Um, yeah, you, you, you've got like great pros. How are like, what is your, uh, how are you able
[01:24:58.800 --> 01:25:03.120]   to like maintain such a productivity on your side project? I should say a few of, a few of my recent,
[01:25:03.120 --> 01:25:07.360]   my most recent posts, which were especially long. Um, I was, I had taken, taken me some time off
[01:25:07.360 --> 01:25:11.520]   from work and I, and I was working on those partly in an academic context. Um, but the first,
[01:25:11.520 --> 01:25:15.680]   the first year and a half or so of the blog was just on the side and I've gone back to having it
[01:25:15.680 --> 01:25:21.520]   be on the side. Now, I think one thing that helps is my blog posts are too long. Um, and, uh,
[01:25:22.400 --> 01:25:29.040]   and so there's, you know, I have dreams of, of writing these, you know, taking my long blog
[01:25:29.040 --> 01:25:35.280]   posts and then really crunching them down and making it into this like pithy, elegant, uh,
[01:25:35.280 --> 01:25:40.880]   statement that that's really concise and condensed. Um, but, uh, that would be more so,
[01:25:40.880 --> 01:25:45.680]   you know, one, one way I sort of, uh, increase my output is by not doing that, uh, editing.
[01:25:45.680 --> 01:25:50.080]   And I feel, I feel bad about that. Um, but that's one, that's one thing at least.
[01:25:50.080 --> 01:25:54.080]   Uh, what is that quote where, I don't know if somebody's asked, like, how did you,
[01:25:54.080 --> 01:25:57.840]   I think it's something like, I would have, you know, I would have written you a long letter,
[01:25:57.840 --> 01:26:00.720]   but I, or I would have, I didn't have time to write you a short letter. So I wrote your long
[01:26:00.720 --> 01:26:04.400]   letter or something like that. Yeah, exactly. I have, I have a friend who says like the actual
[01:26:04.400 --> 01:26:07.680]   thing it should be. I didn't have time to write you a short letter. So I wrote you a bad letter
[01:26:07.680 --> 01:26:14.560]   and you know, I'm like, I hope it's not that bad. Um, but I do, you know, I do think, uh,
[01:26:14.560 --> 01:26:18.480]   if I had more time for these posts, I would, I would try to kind of cut them down. And that's,
[01:26:18.480 --> 01:26:24.240]   so that's one time saving, um, you know, for better or worse. Um, yeah, at least as a reader,
[01:26:24.240 --> 01:26:30.320]   uh, it often seems to me that the people like you who write, um, maybe this is described your
[01:26:30.320 --> 01:26:34.560]   process, but, um, it was Scott Alexander says he kind of just write stream of consciousness
[01:26:34.560 --> 01:26:39.360]   and that, you know, it just turns out to be really readable. Your blog posts are really readable.
[01:26:39.360 --> 01:26:42.480]   Um, and even like the stuff I write, like the things that I write or that are,
[01:26:42.480 --> 01:26:46.240]   I'm like consciously not trying to make edits while I'm going on,
[01:26:46.240 --> 01:26:51.040]   they end up reading much better than the ones where I'm trying to optimize each sentence,
[01:26:51.040 --> 01:26:56.480]   uh, and then taking two steps back for every one I take forward. Um, I, I don't know if it's just,
[01:26:56.480 --> 01:27:00.160]   it could just be like a selection effect of the, the, the things that are harder to convey.
[01:27:00.160 --> 01:27:03.120]   You're spending more time editing, but, um, yeah, it's, it's kind of interesting.
[01:27:03.120 --> 01:27:09.440]   Yeah. I wonder, I wonder, I mean, my, my feeling is that my writing is quite a bit better if I
[01:27:09.440 --> 01:27:14.880]   have a chance to edit it. Um, and it's just, it's just a time thing. Um, but I do think people vary
[01:27:14.880 --> 01:27:18.720]   quite a bit and, and, you know, it's interesting. I don't know if I was recently reading this book.
[01:27:18.720 --> 01:27:23.120]   Um, George Saunders, who I think the writer I really admire has this book about fiction writing
[01:27:23.120 --> 01:27:28.800]   called a swim in the pond in the rain. Um, and the vibe, uh, he tries to convey, and I think this is
[01:27:28.800 --> 01:27:35.840]   relatively common amongst writer types is like this obsessive focus on, you know, even at a
[01:27:35.840 --> 01:27:41.040]   sentence by sentence level, really thinking about what, where is the reader's mind right now?
[01:27:41.040 --> 01:27:46.240]   How are they engaging? Are they interested? Are they surprised? Am I losing them? Um, and, and,
[01:27:46.240 --> 01:27:49.760]   you know, his writing is really, really engaging in ways that it's like not even obvious. You just
[01:27:49.760 --> 01:27:53.520]   sort of start reading along and you're like, oh, wow, I'm really into this. Um, but it's also quite
[01:27:53.520 --> 01:27:57.840]   a daunting picture of the level of attentiveness required. And it's like, wow, if I'm going to
[01:27:57.840 --> 01:28:02.000]   write everything like that, it's like, that's going to cut down a lot on my, um, kind of overall
[01:28:02.000 --> 01:28:06.320]   output. Uh, and so I do think there's a balance there and, and, um, you know, to the extent you're,
[01:28:06.320 --> 01:28:09.200]   you're one of these people who you can just like stream of consciousness and that's like close to
[01:28:09.200 --> 01:28:13.360]   what you would get out of, out of editing, which I'm not sure I am. Um, you know, all the better.
[01:28:13.360 --> 01:28:18.320]   It's sort of like, you're lucky. Yeah. There's also an additional consideration where if you
[01:28:18.320 --> 01:28:22.720]   think there's going to be some kind of power law to like how much, how interesting a piece is or
[01:28:22.720 --> 01:28:29.520]   how, how many, um, how many people see it and how many people, uh, find value in it, then it's not
[01:28:29.520 --> 01:28:35.360]   clear whether that advises you to spend so much time on each piece to increase the odds that that
[01:28:35.360 --> 01:28:38.800]   one piece is going to blow up, given that there's a big difference between the pieces that blow up
[01:28:38.800 --> 01:28:42.800]   and don't, or whether you should just like do a whole bunch and then, uh, kind of just try to
[01:28:42.800 --> 01:28:47.600]   sample, uh, as, as often as possible. Yeah. And I think, I think actually that the blog,
[01:28:47.600 --> 01:28:56.160]   I started the blog partly as an exercise in just getting stuff out there. I think I had had, uh,
[01:28:56.160 --> 01:28:59.920]   I had had the idea that I would one day write up a bunch of stuff that I've been thinking about,
[01:28:59.920 --> 01:29:05.200]   but you know, it was somehow a, uh, and I would write it up in this grand, you know, I would
[01:29:05.200 --> 01:29:09.040]   finally write it up and it'd be this beautiful thing. And I would take all this time. And then,
[01:29:09.040 --> 01:29:16.720]   um, I had ended up, you know, for various reasons, feeling like I was approaching some aspects of my
[01:29:16.720 --> 01:29:21.440]   life with too much perfectionism or too much. And I needed to just like, um, get stuff out there
[01:29:21.440 --> 01:29:27.680]   faster. And so the blog was an exercise in, in, in that. And I think has, uh, you know, I think
[01:29:27.680 --> 01:29:31.280]   that's paid off in ways and that I don't know, I don't think I would have done it otherwise.
[01:29:31.280 --> 01:29:37.760]   I see. All right. Final question. Um, I'm, I'm curious if you have, uh, three book recommendations
[01:29:37.760 --> 01:29:42.080]   that you can give the audience. Probably my primary recommendation that this is somewhat
[01:29:42.080 --> 01:29:46.320]   self-serving because I helped with this, this project is the book, the precipice by Toby Ord.
[01:29:46.320 --> 01:29:50.880]   Um, I, you know, may, may be familiar to many or many of your listeners, but, um, you know,
[01:29:50.880 --> 01:29:58.240]   I think it's, uh, it's a book that really, uh, conveys the ideas that matter, you know,
[01:29:58.240 --> 01:30:02.160]   most to me or that, that I've had, you know, close to the biggest impact in my own life.
[01:30:02.160 --> 01:30:10.960]   Um, uh, other books, I, I love the play angels in America. Um, I think it's just a, I think it's a
[01:30:10.960 --> 01:30:16.160]   epic and amazing. Um, and you know, that's not quite a book, but, um, you know, you can read it.
[01:30:16.160 --> 01:30:21.040]   Uh, I actually recommend watching the HBO mini series. Um, but, uh, that's, you know, that's
[01:30:21.040 --> 01:30:26.880]   something I recommend. Um, and then, uh, I don't know, uh, last, last year I read, I read this
[01:30:26.880 --> 01:30:33.600]   book housekeeping by, um, by Marilyn Robinson and, and, uh, it had this sort of numinous, uh,
[01:30:33.600 --> 01:30:39.120]   quality that, um, I think a lot of her writing does. Um, and so I really like that and recommend
[01:30:39.120 --> 01:30:42.240]   it to people. That's also a piece of fiction. If you're looking for philosophy, I don't know.
[01:30:42.240 --> 01:30:48.160]   A lot of my work is, is in dialogue with Nick Bostrom, um, and, uh, and his, uh, yeah, his,
[01:30:48.160 --> 01:30:52.320]   his overall kind of corpus. And I think that's really, really valuable to engage with.
[01:30:52.320 --> 01:30:55.840]   I see. Cool. Cool. All right. Uh, yeah, Joe, thanks so much for coming on the podcast.
[01:30:55.840 --> 01:30:59.120]   It's a lot of fun, a lot of fun. Yeah. Thanks for having me. Oh, I'll also say, you know,
[01:30:59.120 --> 01:31:03.120]   everything I've said here is just purely my personal opinion. Um, you know, I'm not speaking
[01:31:03.120 --> 01:31:06.880]   for my employer and not speaking for, you know, uh, anyone else, just, just myself. So just,
[01:31:06.880 --> 01:31:12.320]   just keeping that in mind. Cool. Cool. Um, and then where can, uh, people find, uh, your stuff?
[01:31:12.320 --> 01:31:17.440]   So I just, uh, if you want to go over your blog link and then your Twitter link and other things.
[01:31:17.440 --> 01:31:24.240]   Yep. So my blog is hands and cities.com. Um, and my Twitter handle is JK Carl Smith. Um, those are,
[01:31:24.240 --> 01:31:27.760]   those are good places to reach me. And then my personal website is josephcarlsmith.com.
[01:31:27.760 --> 01:31:32.160]   Okay. And then we're going to find yourself on AI, uh, and those kinds of things.
[01:31:32.160 --> 01:31:36.000]   The stuff on AI is linked from my personal website. So that's the best,
[01:31:36.000 --> 01:31:37.760]   that's the best place to go. All right. Cool. Cool.
[01:31:37.760 --> 01:31:44.240]   Thanks for watching. I hope you enjoyed that episode. If you did, and you want to support
[01:31:44.240 --> 01:31:50.240]   the podcast, the most helpful thing you can do is share it on social media and with your friends.
[01:31:50.960 --> 01:31:57.760]   Other than that, please like, and subscribe on YouTube and leave good reviews on podcast platforms.
[01:31:57.760 --> 01:32:09.680]   Cheers. I'll see you next time.

