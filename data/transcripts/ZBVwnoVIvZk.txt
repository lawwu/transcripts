
[00:00:00.000 --> 00:00:02.560]   All right, so you've got your model training,
[00:00:02.560 --> 00:00:05.000]   and you want to make it a little bit better.
[00:00:05.000 --> 00:00:07.680]   So this video is kind of like a sequel
[00:00:07.680 --> 00:00:09.960]   to the video on convolutional neural networks.
[00:00:09.960 --> 00:00:12.840]   And I want to talk about learning rate and batch size
[00:00:12.840 --> 00:00:15.960]   and a couple ways that you can often make your models better.
[00:00:15.960 --> 00:00:19.520]   Now, unlike a lot of other courses on deep learning,
[00:00:19.520 --> 00:00:21.960]   I don't really start with learning rate and batch size
[00:00:21.960 --> 00:00:23.380]   and things like that, because I do
[00:00:23.380 --> 00:00:26.600]   think beginners tend to spend way too much time kind
[00:00:26.600 --> 00:00:29.320]   of optimizing and tweaking these parameters, where they don't
[00:00:29.320 --> 00:00:31.400]   really matter that much.
[00:00:31.400 --> 00:00:33.080]   But they can matter, and they can
[00:00:33.080 --> 00:00:35.840]   be really effective for making models train faster or kind
[00:00:35.840 --> 00:00:39.120]   of getting the last piece of performance out of your model.
[00:00:39.120 --> 00:00:42.000]   So just as a quick refresher, the learning rate
[00:00:42.000 --> 00:00:44.160]   is kind of like the size of the step
[00:00:44.160 --> 00:00:46.160]   that the model makes as it's looking
[00:00:46.160 --> 00:00:47.440]   for the best possible weights.
[00:00:47.440 --> 00:00:49.600]   So really low learning rate will mean
[00:00:49.600 --> 00:00:52.200]   that your model might take a really long time to find
[00:00:52.200 --> 00:00:54.000]   the best set of parameters.
[00:00:54.000 --> 00:00:55.760]   And a really high learning rate might
[00:00:55.760 --> 00:00:58.820]   mean it's kind of jumping over the best possible place
[00:00:58.820 --> 00:01:01.280]   or even sort of jumping into regions where
[00:01:01.280 --> 00:01:04.280]   you get numerical instability.
[00:01:04.280 --> 00:01:07.960]   Batch size, you might remember, is the number of examples
[00:01:07.960 --> 00:01:10.820]   that a model looks at when it decides which direction
[00:01:10.820 --> 00:01:12.200]   to send all the weights.
[00:01:12.200 --> 00:01:14.560]   So a really small batch size means
[00:01:14.560 --> 00:01:17.400]   that each step your model's optimizing only, say,
[00:01:17.400 --> 00:01:19.120]   over one example or two examples,
[00:01:19.120 --> 00:01:21.320]   it could add a lot of noise to your model.
[00:01:21.320 --> 00:01:23.720]   Now, a really big batch size, sometimes it
[00:01:23.720 --> 00:01:25.120]   doesn't have enough noise, and it's
[00:01:25.120 --> 00:01:27.240]   hard for the model to actually find the best place.
[00:01:27.240 --> 00:01:29.620]   So on one hand, smaller batch sizes actually
[00:01:29.620 --> 00:01:32.300]   can ironically help your model train better
[00:01:32.300 --> 00:01:35.860]   by adding a little bit of noise into that search.
[00:01:35.860 --> 00:01:38.540]   But maybe a bigger effect or another important effect
[00:01:38.540 --> 00:01:41.940]   is that a bigger batch size can help your model train faster,
[00:01:41.940 --> 00:01:43.200]   especially on GPUs.
[00:01:43.200 --> 00:01:45.380]   Because on a GPU, you can actually
[00:01:45.380 --> 00:01:50.020]   compute the derivative on all the batches all at once.
[00:01:50.020 --> 00:01:52.820]   So sometimes, as long as you can fit this batch into memory
[00:01:52.820 --> 00:01:55.300]   and an associated model into memory,
[00:01:55.300 --> 00:01:57.180]   a bigger, bigger batch size might not actually
[00:01:57.180 --> 00:01:58.800]   slow down your training by much at all.
[00:01:58.800 --> 00:02:00.560]   In fact, that computation might be
[00:02:00.560 --> 00:02:02.560]   sort of atomic in the same speed,
[00:02:02.560 --> 00:02:04.760]   regardless of your batch size.
[00:02:04.760 --> 00:02:06.800]   Now, if you set your batch size too big,
[00:02:06.800 --> 00:02:08.880]   it just won't fit into memory, and your thing will crash.
[00:02:08.880 --> 00:02:10.460]   So at that point, you'll know that you
[00:02:10.460 --> 00:02:11.840]   need to reduce your batch size.
[00:02:11.840 --> 00:02:14.880]   But one thing I see with a lot of people starting out
[00:02:14.880 --> 00:02:17.480]   is I think they tend to set their batch size too small
[00:02:17.480 --> 00:02:19.040]   and waste more time in their training
[00:02:19.040 --> 00:02:20.280]   than they really need to.
[00:02:20.280 --> 00:02:23.280]   So let's jump into the code and run some experiments.
[00:02:23.280 --> 00:02:26.480]   So I'm going to do these experiments on the CIFAR data
[00:02:26.480 --> 00:02:29.200]   set that we've used in a previous class on data
[00:02:29.200 --> 00:02:32.100]   augmentation that you should probably take a look at if you
[00:02:32.100 --> 00:02:32.820]   haven't already.
[00:02:32.820 --> 00:02:36.600]   So we're going to do the standard imports.
[00:02:36.600 --> 00:02:40.900]   And then we're going to pull in the CIFAR-10 data set.
[00:02:40.900 --> 00:02:43.020]   These things tend to be different on different data
[00:02:43.020 --> 00:02:45.360]   sets, but I wanted to use a data set that's small enough
[00:02:45.360 --> 00:02:47.620]   that we can run a lot of experiments quickly,
[00:02:47.620 --> 00:02:50.200]   but maybe not MNIST, where that's actually so small
[00:02:50.200 --> 00:02:53.460]   that you might get unusual results.
[00:02:53.460 --> 00:02:55.140]   So as usual, I normalize the data
[00:02:55.140 --> 00:02:58.980]   and convert the labels into one hot encoded versions
[00:02:58.980 --> 00:02:59.780]   of themselves.
[00:02:59.780 --> 00:03:02.580]   And I'm going to start with a very, very small
[00:03:02.580 --> 00:03:04.900]   convolutional network, probably smallest possible
[00:03:04.900 --> 00:03:08.340]   convolutional network, with just one convolution and one pooling
[00:03:08.340 --> 00:03:08.840]   layer.
[00:03:08.840 --> 00:03:11.100]   So certainly, you can make this network bigger
[00:03:11.100 --> 00:03:12.420]   and get better performance.
[00:03:12.420 --> 00:03:15.580]   But I want to see what happens as we modify the learning rate
[00:03:15.580 --> 00:03:18.340]   and the batch size first.
[00:03:18.340 --> 00:03:20.180]   So the first thing I want to show you
[00:03:20.180 --> 00:03:23.300]   is what happens if you set the learning rate to something
[00:03:23.300 --> 00:03:24.020]   very small.
[00:03:24.020 --> 00:03:28.460]   So by default, Atom sets the learning rate to 0.001.
[00:03:28.460 --> 00:03:41.620]   So let's see what happens if we set it to 0.0001.
[00:03:41.620 --> 00:03:44.860]   So here, the blue line is the accuracy of the CNN
[00:03:44.860 --> 00:03:46.300]   with the lower learning rate.
[00:03:46.300 --> 00:03:49.260]   And the orange line is the accuracy of CNN
[00:03:49.260 --> 00:03:50.780]   with the default learning rate.
[00:03:50.780 --> 00:03:52.580]   And so you can see, lower learning rate,
[00:03:52.580 --> 00:03:53.780]   nothing really bad happens.
[00:03:53.780 --> 00:03:54.820]   But it learns slower.
[00:03:54.820 --> 00:03:56.580]   And that can be really annoying when you're
[00:03:56.580 --> 00:03:58.020]   training lots of models.
[00:03:58.020 --> 00:04:01.540]   So when I see this blue line, it's
[00:04:01.540 --> 00:04:03.000]   consistently below the orange line,
[00:04:03.000 --> 00:04:07.460]   although I think that'll probably catch up over time.
[00:04:07.460 --> 00:04:09.660]   OK, so suppose you look at this and you say,
[00:04:09.660 --> 00:04:13.060]   you know what, learning rate of 0.0001,
[00:04:13.060 --> 00:04:15.740]   that doesn't work as well as the default 0.001.
[00:04:15.740 --> 00:04:17.980]   Let's raise it up by a few factors.
[00:04:17.980 --> 00:04:19.700]   Let's make it, say, 0.1.
[00:04:19.700 --> 00:04:21.860]   This would be considered a pretty high learning rate.
[00:04:21.860 --> 00:04:26.860]   But who knows, maybe the model will learn faster.
[00:04:26.860 --> 00:04:28.380]   So here we call model.fit.
[00:04:28.380 --> 00:04:29.060]   And look at that.
[00:04:29.060 --> 00:04:31.300]   We see an accuracy of about 10%.
[00:04:31.300 --> 00:04:33.180]   And it's not even getting better.
[00:04:33.180 --> 00:04:36.780]   Another real telltale sign is that our loss is 14.
[00:04:36.780 --> 00:04:38.660]   Now remember, a loss is kind of in log space.
[00:04:38.660 --> 00:04:39.820]   Really, it's like log loss.
[00:04:39.820 --> 00:04:43.020]   So if you see a loss that's above 4 or 5,
[00:04:43.020 --> 00:04:45.420]   you should think of that as a massively huge loss.
[00:04:45.420 --> 00:04:47.140]   I think what's actually happening here
[00:04:47.140 --> 00:04:49.840]   is that the model's returning nans and infinities
[00:04:49.840 --> 00:04:51.560]   and just crazy things.
[00:04:51.560 --> 00:04:53.460]   And so this model is never going to get better.
[00:04:53.460 --> 00:04:58.460]   It just sits at a loss of 14 and an accuracy of 10%.
[00:04:58.460 --> 00:05:00.100]   And remember, we have 10 classes.
[00:05:00.100 --> 00:05:03.380]   So an accuracy of 10% is really the model's just guessing.
[00:05:03.380 --> 00:05:06.380]   So this is what happens when you set the learning rate too high.
[00:05:06.380 --> 00:05:08.060]   You get in a really, really bad place.
[00:05:08.060 --> 00:05:11.420]   So lower learning rates tend to be safer than higher learning
[00:05:11.420 --> 00:05:12.180]   rates.
[00:05:12.180 --> 00:05:13.340]   What about batch size?
[00:05:13.340 --> 00:05:16.260]   So our default batch size is 32.
[00:05:16.260 --> 00:05:19.140]   What happens if we set it to, say, 128?
[00:05:19.140 --> 00:05:21.500]   So this is a 4x increase in batch size.
[00:05:21.500 --> 00:05:24.320]   If we call model.fit, the first thing you'll notice
[00:05:24.320 --> 00:05:26.500]   is that it takes a little bit longer to start
[00:05:26.500 --> 00:05:28.040]   because it's loading more into memory.
[00:05:28.040 --> 00:05:30.940]   But then actually, it runs a lot faster.
[00:05:30.940 --> 00:05:32.700]   And so that's actually pretty nice.
[00:05:32.700 --> 00:05:35.680]   But if it runs faster and we sacrifice accuracy,
[00:05:35.680 --> 00:05:36.840]   that's not very interesting.
[00:05:36.840 --> 00:05:38.180]   And remember, it's running faster
[00:05:38.180 --> 00:05:41.300]   because each step that it's taking is over a bigger chunk.
[00:05:41.300 --> 00:05:43.600]   So it's looking at the same number of training examples,
[00:05:43.600 --> 00:05:46.540]   but it's doing it in larger chunks.
[00:05:46.540 --> 00:05:49.900]   [AUDIO OUT]
[00:05:49.900 --> 00:05:53.360]   [AUDIO OUT]
[00:05:53.360 --> 00:05:56.820]   [AUDIO OUT]
[00:05:56.820 --> 00:06:00.280]   [AUDIO OUT]
[00:06:00.280 --> 00:06:03.740]   [AUDIO OUT]
[00:06:03.740 --> 00:06:07.220]   [AUDIO OUT]
[00:06:07.220 --> 00:06:10.700]   [AUDIO OUT]
[00:06:10.700 --> 00:06:13.680]   [AUDIO OUT]
[00:06:13.680 --> 00:06:16.660]   [AUDIO OUT]
[00:06:16.660 --> 00:06:19.640]   [AUDIO OUT]
[00:06:19.640 --> 00:06:22.620]   [AUDIO OUT]
[00:06:22.620 --> 00:06:25.620]   [AUDIO OUT]
[00:06:25.620 --> 00:06:28.600]   [AUDIO OUT]
[00:06:28.600 --> 00:06:29.520]   All right.
[00:06:29.520 --> 00:06:30.020]   Cool.
[00:06:30.020 --> 00:06:34.420]   So here, I think this is a reason that people often
[00:06:34.420 --> 00:06:36.060]   don't use larger batch sizes.
[00:06:36.060 --> 00:06:40.460]   The CNN with 128 batch size is that each step is faster,
[00:06:40.460 --> 00:06:42.460]   but it's actually performing worse than the CNN
[00:06:42.460 --> 00:06:44.020]   with the smaller batch size.
[00:06:44.020 --> 00:06:45.780]   But there's actually a subtle point there
[00:06:45.780 --> 00:06:47.220]   that's important to know, which is
[00:06:47.220 --> 00:06:49.100]   that as you increase the batch size,
[00:06:49.100 --> 00:06:52.220]   you should also increase the learning rate.
[00:06:52.220 --> 00:06:55.220]   So if you multiply the batch size by 4,
[00:06:55.220 --> 00:06:57.260]   you should also, generally speaking,
[00:06:57.260 --> 00:06:59.140]   multiply the learning rate by 4.
[00:06:59.140 --> 00:07:00.680]   So with bigger batches, because you're
[00:07:00.680 --> 00:07:03.140]   averaging over a larger number of samples,
[00:07:03.140 --> 00:07:05.260]   you can get away with higher learning rates
[00:07:05.260 --> 00:07:06.820]   than you otherwise could.
[00:07:06.820 --> 00:07:09.780]   So let's try this 128 batch size again,
[00:07:09.780 --> 00:07:14.700]   but with a 4x learning rate versus the baseline 32 batch
[00:07:14.700 --> 00:07:16.260]   size.
[00:07:16.260 --> 00:07:45.220]   [AUDIO OUT]
[00:07:45.220 --> 00:07:45.720]   OK.
[00:07:45.720 --> 00:07:47.100]   So actually, what you see here is
[00:07:47.100 --> 00:07:49.480]   that when we 4x the learning rate with the larger batch
[00:07:49.480 --> 00:07:51.540]   size, now we get the best of both worlds.
[00:07:51.540 --> 00:07:54.500]   So we have a CNN that's training faster,
[00:07:54.500 --> 00:07:57.220]   and it's training at about the same level of accuracy
[00:07:57.220 --> 00:08:01.060]   and validation accuracy as our baseline CNN.
[00:08:01.060 --> 00:08:03.260]   So one more fancy thing you can do,
[00:08:03.260 --> 00:08:06.080]   and especially this is really effective when you train
[00:08:06.080 --> 00:08:09.180]   over a large number of epics, is you can reduce the learning
[00:08:09.180 --> 00:08:10.820]   rate on plateau.
[00:08:10.820 --> 00:08:13.660]   So the idea here is that once your model kind of gets stuck
[00:08:13.660 --> 00:08:16.940]   and can't find a better optimum, it lowers the learning rate
[00:08:16.940 --> 00:08:20.540]   so it can kind of fine tune in the area that it's in.
[00:08:20.540 --> 00:08:22.740]   So there's actually-- this is such a common technique
[00:08:22.740 --> 00:08:25.740]   that there's actually a Keras callback that we can just use
[00:08:25.740 --> 00:08:27.060]   that'll do it for us.
[00:08:27.060 --> 00:08:29.920]   It has lots of options, but the defaults, as usual,
[00:08:29.920 --> 00:08:31.820]   are pretty sensible.
[00:08:31.820 --> 00:08:39.300]   So we just add to our callbacks a callback reduce LR on plateau.
[00:08:39.300 --> 00:08:40.900]   And now, in order to get this effect,
[00:08:40.900 --> 00:08:44.220]   we actually have to have our learning actually plateau.
[00:08:44.220 --> 00:08:47.540]   So let's set the epics to, say, 300
[00:08:47.540 --> 00:08:52.740]   and time lapse this a little bit to not
[00:08:52.740 --> 00:08:53.820]   make a really long video.
[00:08:53.820 --> 00:09:01.420]   OK, and so this model's been training for a while,
[00:09:01.420 --> 00:09:05.980]   and you can see what happened is at about the 35th epic,
[00:09:05.980 --> 00:09:07.220]   it stopped improving.
[00:09:07.220 --> 00:09:10.740]   And so our reduce learning rate system
[00:09:10.740 --> 00:09:12.420]   automatically reduced the learning rate,
[00:09:12.420 --> 00:09:15.460]   and you can see that the accuracy and the validation
[00:09:15.460 --> 00:09:19.340]   accuracy actually popped up quickly and then petered out.
[00:09:19.340 --> 00:09:20.960]   And with a lot of models, you actually
[00:09:20.960 --> 00:09:23.940]   see that effect happen a few times, each probably
[00:09:23.940 --> 00:09:27.100]   like a little bit smaller than the time before.
[00:09:27.100 --> 00:09:30.980]   But it can really add some accuracy
[00:09:30.980 --> 00:09:33.260]   on the end of your model training.
[00:09:33.260 --> 00:09:35.400]   So again, I don't think learning rate is always
[00:09:35.400 --> 00:09:38.700]   the first thing to mess with unless your model is really
[00:09:38.700 --> 00:09:41.060]   not training and you're having numerical issues.
[00:09:41.060 --> 00:09:43.620]   Generally, the Keras defaults are pretty good.
[00:09:43.620 --> 00:09:45.620]   And most people go through a phase, I think,
[00:09:45.620 --> 00:09:48.740]   where they spend too much time changing the optimizer
[00:09:48.740 --> 00:09:51.460]   and changing momentum and things like that.
[00:09:51.460 --> 00:09:52.840]   But it definitely does make sense
[00:09:52.840 --> 00:09:55.740]   to understand what learning rate and batch size are
[00:09:55.740 --> 00:09:57.300]   and spend a little time tweaking them.
[00:09:57.300 --> 00:09:59.660]   And I think doing this kind of reduced learning rate
[00:09:59.660 --> 00:10:03.900]   on plateau system is generally best practice for long running,
[00:10:03.900 --> 00:10:05.860]   long training models.
[00:10:05.860 --> 00:10:07.420]   Thanks.
[00:10:07.420 --> 00:10:10.780]   [AUDIO OUT]
[00:10:10.780 --> 00:10:14.240]   [NO AUDIO]
[00:10:14.240 --> 00:10:24.240]   [BLANK_AUDIO]

