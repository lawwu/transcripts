
[00:00:00.000 --> 00:00:07.440]   where I check and freak out. I'm just wait for that. I'm better
[00:00:07.440 --> 00:00:09.360]   at it. Earlier I used to be like, why isn't this starting?
[00:00:09.360 --> 00:00:11.480]   Why isn't this starting? What did I mess up? What did I mess
[00:00:11.480 --> 00:00:17.840]   up? Awesome, that part went through as well. So I look like
[00:00:17.840 --> 00:00:21.600]   an idiot there. But hopefully the awesome presentation will
[00:00:21.600 --> 00:00:25.520]   make up for that funny start up. Hey, everybody, welcome back to
[00:00:25.520 --> 00:00:28.760]   the Jackstock series kindly hosted with the help of
[00:00:28.760 --> 00:00:32.360]   Christian Garcia, who's now I'm not sure if you saw the tweet,
[00:00:32.360 --> 00:00:35.320]   but he's working on Jackson in official capacity still at
[00:00:35.320 --> 00:00:38.120]   Consite. So that's awesome. Christian, welcome back.
[00:00:38.120 --> 00:00:41.600]   Hey, everyone. Hey, Patrick.
[00:00:41.600 --> 00:00:43.560]   Thank you for having me guys.
[00:00:43.560 --> 00:00:50.400]   Awesome. We're super excited to host Patrick. Patrick. Dr.
[00:00:50.400 --> 00:00:53.720]   Kidger, I was corrected before the talk, he's just completed
[00:00:53.720 --> 00:00:58.440]   his PhD, works on stuff that I'll pretend I understand all
[00:00:58.440 --> 00:01:05.200]   of. He is, he's just completed his PhD at Oxford, and he works
[00:01:05.200 --> 00:01:09.280]   on neural ODEs, STEs and time series. I only know bits and
[00:01:09.280 --> 00:01:12.200]   pieces of that. If you want to know more about him, you can go
[00:01:12.200 --> 00:01:15.920]   to his Twitter profile. There he has his thesis pinned on the top
[00:01:15.920 --> 00:01:19.920]   of his profile. You can find his work there. Today he'll be
[00:01:19.920 --> 00:01:23.320]   talking about diffracts. And again, I'll say a few more words
[00:01:23.320 --> 00:01:27.040]   which are quite scary to me. It's a Jax based suite of
[00:01:27.080 --> 00:01:30.080]   ordinary stochastic and controlled differential
[00:01:30.080 --> 00:01:34.400]   equation solvers, roughly analogous to the existing torch
[00:01:34.400 --> 00:01:40.920]   diffract packages. This is available in the Jax ecosystem
[00:01:40.920 --> 00:01:44.040]   and Patrick will be telling us more about it. Patrick, thanks
[00:01:44.040 --> 00:01:45.400]   so much again for joining us.
[00:01:45.400 --> 00:01:48.480]   No worries at all. Thanks for having me guys. Let me start by
[00:01:48.480 --> 00:01:55.480]   sharing my screen and I'll take it away. Okay, you can all see
[00:01:55.480 --> 00:02:03.440]   it. I don't see it now. It popped up but we lost it. Oh,
[00:02:03.440 --> 00:02:05.480]   okay. Fine. What if I do it like this?
[00:02:05.480 --> 00:02:12.760]   I don't think you're sharing it right now. Could you please
[00:02:12.760 --> 00:02:13.200]   reshare?
[00:02:13.200 --> 00:02:20.440]   Not a great start. Okay, so sharing infinite descent. Switch
[00:02:20.440 --> 00:02:21.400]   to this. Can you see it?
[00:02:21.400 --> 00:02:23.080]   Yep. Second time's the charm.
[00:02:23.720 --> 00:02:27.000]   Oh, right. There we go. And then we're good.
[00:02:27.000 --> 00:02:28.760]   Yep, that looks good.
[00:02:28.760 --> 00:02:32.480]   Amazing. Off we go. Okay, so in that case, hello, everyone. It's
[00:02:32.480 --> 00:02:35.080]   great to be here. So as I was introduced, my name is Patrick.
[00:02:35.080 --> 00:02:38.320]   And today I'm going to be talking a bit about diffracts
[00:02:38.320 --> 00:02:40.920]   for the sake of everyone here who is hopefully really
[00:02:40.920 --> 00:02:42.960]   thoroughly invested in the Jax ecosystem. I think it's the
[00:02:42.960 --> 00:02:48.280]   coolest thing ever, just like I do. So program today, I'm going
[00:02:48.280 --> 00:02:50.520]   to give you four bullet points. And that's I'm going to give you
[00:02:50.520 --> 00:02:52.920]   a bit of an introduction. What am I talking about today? Why is
[00:02:52.920 --> 00:02:56.360]   this interesting? We're going to jump into a live coding demo,
[00:02:56.360 --> 00:02:58.720]   always a bit of a scary thing to do in a talk, hopefully it'll
[00:02:58.720 --> 00:03:01.840]   work out. Let's see. And then I'll tell you a little bit about
[00:03:01.840 --> 00:03:05.040]   using diffracts, like why should you care about this as an end
[00:03:05.040 --> 00:03:07.320]   user. But then I think hopefully what should be a bit more
[00:03:07.320 --> 00:03:10.000]   interesting actually towards the end of the talk is I will open
[00:03:10.000 --> 00:03:12.680]   it up. And then we'll look at some of the internals and some
[00:03:12.680 --> 00:03:14.880]   of the cool stuff that's happening inside diffracts that
[00:03:14.880 --> 00:03:21.080]   makes diffracts possible. So obviously, modern auto
[00:03:21.080 --> 00:03:24.560]   differentiation GPU frameworks, these are amazing, not just for
[00:03:24.560 --> 00:03:26.680]   deep learning, but they're also amazing for scientific
[00:03:26.680 --> 00:03:30.200]   computing. So we've got things like PyTorch and Julia, of
[00:03:30.200 --> 00:03:33.960]   course, but of course, also we have Jax. And then what is a
[00:03:33.960 --> 00:03:36.800]   backbone of any scientific computing framework, that is
[00:03:36.800 --> 00:03:39.160]   differential equation solvers, we really need to solve
[00:03:39.160 --> 00:03:43.040]   differential equations all the time in the sciences. So of
[00:03:43.040 --> 00:03:45.000]   course, as you probably guessed off the back of this, we have
[00:03:45.000 --> 00:03:47.240]   diffracts, which is this suite of numerical differential
[00:03:47.240 --> 00:03:52.520]   equation solvers in Jax. Now, why should you care? Well,
[00:03:52.520 --> 00:03:54.440]   first of all, maybe you're a scientist or something like
[00:03:54.440 --> 00:03:56.480]   that, you need to solve differential equations as part
[00:03:56.480 --> 00:03:58.600]   of maybe your model part of whatever you're up to. Well,
[00:03:58.600 --> 00:04:01.240]   then unsurprisingly, something that solves differential
[00:04:01.240 --> 00:04:05.080]   equations might be of interest to you. But maybe you're a
[00:04:05.080 --> 00:04:07.160]   numerical analyst, or maybe you're a software dev who has to
[00:04:07.160 --> 00:04:09.840]   work with differential equation solvers, then diffracts
[00:04:09.840 --> 00:04:12.000]   actually introduces a bunch of pretty carefully thought out
[00:04:12.000 --> 00:04:14.920]   abstractions that makes doing this either very easy to develop
[00:04:14.920 --> 00:04:19.000]   new solvers or to implement existing solvers. And the
[00:04:19.000 --> 00:04:20.680]   really cool thing, this is what we're going to be seeing a
[00:04:20.680 --> 00:04:22.960]   little bit later on, on the more technical mathematical end of
[00:04:22.960 --> 00:04:27.600]   things, is how it solves ODEs and SDEs in a unified way. But
[00:04:27.600 --> 00:04:29.520]   okay, maybe you're not a mathematician, maybe you don't
[00:04:29.520 --> 00:04:32.080]   do numerical analysis, nothing like that, but you just take a
[00:04:32.080 --> 00:04:33.680]   slightly broader stance, you care about scientific
[00:04:33.680 --> 00:04:36.640]   computing in Jax. Well, I'm also going to talk a little bit
[00:04:36.640 --> 00:04:39.640]   about this spin out Equinox library that also introduces
[00:04:39.640 --> 00:04:42.120]   some cool new tools that will be towards the end. And then
[00:04:42.120 --> 00:04:46.080]   finally, hopefully addressing 99% of the audience today, maybe
[00:04:46.080 --> 00:04:48.880]   you just care about doing neural networks and deep learning in
[00:04:48.880 --> 00:04:52.680]   Jax. Well, then actually also we'll see sort of see a couple
[00:04:52.680 --> 00:04:56.880]   of applications towards this end of things as well. So my hope is
[00:04:56.880 --> 00:04:59.320]   that pretty much no matter your background, whether it's really
[00:04:59.320 --> 00:05:01.080]   like all the way down to like needing to solve specific
[00:05:01.080 --> 00:05:03.280]   differential equations, or just more broadly scientific
[00:05:03.280 --> 00:05:06.720]   computing, or just more broadly deep learning, that in any case,
[00:05:06.720 --> 00:05:10.360]   hopefully there's something here for everyone. So this is where
[00:05:10.360 --> 00:05:13.120]   I've written coding demo, let's just jump straight in. And I'll
[00:05:13.120 --> 00:05:18.040]   show you a little bit of what coding into facts looks like. So
[00:05:18.040 --> 00:05:22.520]   I'm only going to give you basically just I'm only going to
[00:05:22.520 --> 00:05:25.120]   give you basically just a just just a sort of glimpse of the
[00:05:25.120 --> 00:05:30.040]   API more than anything else. So let's get started with Python.
[00:05:32.080 --> 00:05:41.280]   Obviously, don't write that in production code. Okay, so let's
[00:05:41.280 --> 00:05:43.640]   suppose that we want to solve, let's say an ordinary
[00:05:43.640 --> 00:05:49.120]   differential equation. So what we're going to solve is this
[00:05:49.120 --> 00:05:51.960]   equation here. So that's just exponential decay. We'll see
[00:05:51.960 --> 00:05:55.560]   what that looks like as a picture in a minute. We're going
[00:05:55.560 --> 00:06:00.280]   to solve it using, let's use an off the shelf method. So all of
[00:06:00.280 --> 00:06:04.720]   this is Jack's code, by the way, not super obvious. But we're
[00:06:04.720 --> 00:06:06.760]   going to start with that. And then let's solve it, say from
[00:06:06.760 --> 00:06:10.120]   this starting point. And probably this doesn't, if you're
[00:06:10.120 --> 00:06:11.680]   not familiar with differential equations, maybe this looks a
[00:06:11.680 --> 00:06:13.720]   little bit like soup, don't worry, we'll have a nice pretty
[00:06:13.720 --> 00:06:15.680]   picture at the end, you'll see, you'll see where we're going
[00:06:15.680 --> 00:06:20.360]   with this. So now what we're going to do is say that we're
[00:06:20.360 --> 00:06:27.960]   solving an ODE. And basically just put all those arguments
[00:06:27.960 --> 00:06:32.800]   together. And then we get our solution. You can see that we've
[00:06:32.800 --> 00:06:37.120]   solved it all the way down to this final endpoint. This is the
[00:06:37.120 --> 00:06:40.000]   value of our solution. But I promised you a pretty picture.
[00:06:40.000 --> 00:06:47.000]   So let's do that. So let's say that we want to save it at 1000
[00:06:47.000 --> 00:06:55.360]   points. Did I miss a place bracket? And now let's just give
[00:06:55.360 --> 00:07:07.480]   that a plot. We can, we've solved our keys, this is all
[00:07:07.480 --> 00:07:11.920]   the points we've solved there. This is all of the values. So
[00:07:11.920 --> 00:07:19.400]   now having saved it, so having plotted it, we can save that and
[00:07:19.400 --> 00:07:24.200]   if I Alt Tab over, you can see that we have in fact solved
[00:07:24.200 --> 00:07:26.320]   exponential decay. So you can see there's a bit of a curve
[00:07:26.320 --> 00:07:30.400]   there. So if you are familiar with differential equations and
[00:07:30.400 --> 00:07:32.680]   ODEs and so on, you know what I mean when I'm talking about
[00:07:32.680 --> 00:07:34.560]   exponential decay, then hopefully this makes some kind
[00:07:34.560 --> 00:07:37.440]   of sense to you, what you've just seen. And if not, then
[00:07:37.440 --> 00:07:39.680]   hopefully you can at least appreciate a pretty picture.
[00:07:39.680 --> 00:07:44.200]   Okay, so that's just a very quick introduction to the API
[00:07:44.200 --> 00:07:46.760]   for ordinary differential equations. And you can see if I
[00:07:46.760 --> 00:07:48.600]   just scroll past all of that, you can see basically it's a
[00:07:48.600 --> 00:07:50.240]   little bit like playing with Lego, right? I've just written
[00:07:50.240 --> 00:07:52.520]   down a bunch of stuff, vector field, you know, 2.0, 2.1, I've
[00:07:52.520 --> 00:07:53.920]   just sort of specified everything and then I've just
[00:07:53.920 --> 00:07:56.480]   bugged it into this differential equation solve call. And then
[00:07:56.480 --> 00:07:59.600]   it saves everything. So it solves the whole thing. And in
[00:07:59.600 --> 00:08:02.800]   this case, it saves out these 1000 points. But I told you as
[00:08:02.800 --> 00:08:06.480]   well, that maybe you do SDEs, say, and you want to be able to
[00:08:06.480 --> 00:08:09.440]   solve an SDE, well, the SDFax does this as well. So let's
[00:08:09.440 --> 00:08:12.120]   take our existing example, we'll modify it, we'll make it a
[00:08:12.120 --> 00:08:15.640]   little bit more complicated. In this case, let's start off with
[00:08:15.640 --> 00:08:18.280]   exactly the same drift. So this is that exponential decay that
[00:08:18.280 --> 00:08:23.080]   we had before. And we'll include a little bit of noise. And
[00:08:23.080 --> 00:08:26.440]   then whereas previously, we had, that was what we had for our
[00:08:26.440 --> 00:08:28.600]   term. On that we're gonna make it slightly more complicated,
[00:08:28.600 --> 00:08:32.840]   we're gonna add on this noise to turn it into an SDE. And
[00:08:32.840 --> 00:08:34.840]   actually, you know what, first of all, I should introduce a
[00:08:34.840 --> 00:08:43.480]   Brownian motion. So this is a instance of Brownian motion. So
[00:08:43.480 --> 00:08:47.000]   this is a single Brownian sample. And then of course, we
[00:08:47.000 --> 00:08:49.400]   need some sort of PR and DP, because we're doing gaps, we
[00:08:49.400 --> 00:08:51.320]   need some sort of randomness, let's just insert that in there.
[00:08:51.320 --> 00:08:56.400]   Right, okay. So now let's define our term. So you can see, we've
[00:08:56.400 --> 00:09:06.760]   got, okay, so that is, yep, there we go. Let's use a
[00:09:06.760 --> 00:09:09.480]   normal method. So now I'm going to go ahead and just call
[00:09:09.480 --> 00:09:17.480]   diff xsolve again. Solve t0, t1, bt0, y0. And then once again,
[00:09:17.480 --> 00:09:21.800]   let's save. And then this time, let's save at all time points,
[00:09:21.800 --> 00:09:24.440]   so you actually output this point. So there's a bit
[00:09:24.440 --> 00:09:26.600]   compilation here happening under the hood. And of course, you
[00:09:26.600 --> 00:09:29.080]   also, you can wrap the whole thing in a JIT call, and that
[00:09:29.080 --> 00:09:31.560]   will also just work. So we've got our output. And then once
[00:09:31.560 --> 00:09:36.920]   again, let's do what we promised. And then we'll
[00:09:38.720 --> 00:09:42.560]   also plot 1000 points, emap the evaluation. So this is what,
[00:09:42.560 --> 00:09:45.200]   that's what this dense is doing. It's giving me this solve
[00:09:45.200 --> 00:09:48.080]   evaluate method, and it allows me to evaluate at all points.
[00:09:48.080 --> 00:09:49.840]   In this case, I'm going to evaluate at these points, get
[00:09:49.840 --> 00:09:56.480]   these values out. Elite.plot these y's. Guilty.savefig,
[00:09:56.480 --> 00:10:02.080]   and /mg. Then hopefully if I go back over there, you can see,
[00:10:02.080 --> 00:10:07.000]   ta-da! Okay, so now you can see this stochastic solution has
[00:10:07.000 --> 00:10:10.880]   been overlaid on top. So first one, that was the ODE we've
[00:10:10.880 --> 00:10:13.840]   solved, and the second one is the SDE we've solved over the
[00:10:13.840 --> 00:10:16.040]   same time interval. And this is the same differential equation,
[00:10:16.040 --> 00:10:18.640]   well, same drift part of the differential equation, we've
[00:10:18.640 --> 00:10:23.720]   just added on this diffusion as well. Okay, so the thing I kind
[00:10:23.720 --> 00:10:27.320]   of want to highlight at this point is how, at this point,
[00:10:27.320 --> 00:10:29.440]   maybe it looks like I've built a DSL, I've built a domain
[00:10:29.440 --> 00:10:31.640]   specific language, right, that I've sort of introduced all of
[00:10:31.640 --> 00:10:34.360]   these concepts, I've introduced all of these solvers and terms
[00:10:34.360 --> 00:10:37.200]   and Brownian motions and diff solvers and blah, blah, blah,
[00:10:37.200 --> 00:10:39.520]   blah, right? And it sort of, it feels like I've invented a whole
[00:10:39.520 --> 00:10:42.400]   new language here. The cool thing is here, actually, is that
[00:10:42.400 --> 00:10:46.240]   all of this is JAX, exactly as normal. When I look at
[00:10:46.240 --> 00:10:48.360]   something like this virtual Brownian tree, this is just a
[00:10:48.360 --> 00:10:52.160]   PyTree. Multiterm is a PyTree. You can see it has ODE term and
[00:10:52.160 --> 00:10:54.320]   control term inside of it. Each of those are PyTrees in turn,
[00:10:54.320 --> 00:10:59.680]   they have drift, diffusion, VM inside of it. If I was to look
[00:10:59.680 --> 00:11:02.800]   at this solution object, also a PyTree. It's a PyTree that
[00:11:02.800 --> 00:11:04.840]   happens to be a class, happens to have a method I can call.
[00:11:04.840 --> 00:11:07.160]   And this is what allows me to then do things like JAX.vmap.
[00:11:07.160 --> 00:11:10.200]   And I can just evaluate this. So this is something which I think
[00:11:10.200 --> 00:11:11.360]   is pretty cool. And it's something I'm going to be
[00:11:11.360 --> 00:11:14.400]   touching on a little bit later on in the talk as well, is the
[00:11:14.400 --> 00:11:17.560]   extent to which all of this is actually just native JAX,
[00:11:17.560 --> 00:11:20.080]   exactly like you're used to thinking about. And this is
[00:11:20.080 --> 00:11:23.600]   what really what allows us to work with these JAX operations
[00:11:23.600 --> 00:11:26.840]   like JAX.vmap and so on, around anything else we're doing with
[00:11:26.840 --> 00:11:30.160]   the facts. So you shouldn't feel like this is built on top of
[00:11:30.160 --> 00:11:32.560]   JAX, you should feel like this is somehow like at the same
[00:11:32.560 --> 00:11:34.560]   level of JAX, I think, you know, it's using exactly the same
[00:11:34.560 --> 00:11:39.560]   stuff. Okay, so that's a quick API demo, mostly just to sort of
[00:11:39.560 --> 00:11:42.560]   give you a sense of what working with this library looks like.
[00:11:42.560 --> 00:11:45.040]   And now I'm going to talk a little bit about some of the
[00:11:45.040 --> 00:11:49.160]   features of JAX and how, sorry, of Diffrax even, and how it has
[00:11:49.160 --> 00:11:51.120]   the kinds of things that we would expect of a differential
[00:11:51.120 --> 00:11:53.600]   equation solving library. And then, as I said, we'll open up
[00:11:53.600 --> 00:11:55.640]   the hood, we'll start seeing some of the new stuff that makes
[00:11:55.640 --> 00:12:00.120]   makes Diffrax tick inside. So first things first, Diffrax has
[00:12:00.120 --> 00:12:02.480]   lots and lots of things that you expect in differential
[00:12:02.480 --> 00:12:06.160]   equation solving library. It can solve ODEs and it can solve SDEs,
[00:12:06.160 --> 00:12:09.120]   that's a big deal. It can also solve what we call controlled
[00:12:09.120 --> 00:12:11.360]   differential equations. So for those of you who aren't
[00:12:11.360 --> 00:12:13.960]   familiar, this is probably very few of you, this is quite a niche
[00:12:13.960 --> 00:12:18.120]   bit of mathematics. This is a generalization of both ODEs and
[00:12:18.120 --> 00:12:20.480]   SDEs and other kinds of differential equations besides,
[00:12:20.480 --> 00:12:22.720]   there's this unifying notion of this thing called a controlled
[00:12:22.720 --> 00:12:25.120]   differential equation that somehow captures everything else
[00:12:25.120 --> 00:12:29.360]   as a special case. So Diffrax actually implements ODE and SDE
[00:12:29.360 --> 00:12:32.120]   solvers by reducing them to CDE solvers and doing everything in
[00:12:32.120 --> 00:12:35.800]   that space, which is something I'll be touching on a bit more
[00:12:35.800 --> 00:12:38.480]   in terms of the theory later. And at this point, it simply
[00:12:38.480 --> 00:12:42.160]   suffices to say that we can solve these very general kinds
[00:12:42.160 --> 00:12:45.240]   of differential equations as well. You've got all the other
[00:12:45.240 --> 00:12:47.400]   things you need, you have high order solvers, you have implicit
[00:12:47.400 --> 00:12:49.120]   solvers for stiff problems, you've got things like
[00:12:49.120 --> 00:12:52.840]   symplectic solvers, dot dot dot dot dot. We have dense solutions,
[00:12:52.840 --> 00:12:57.120]   we saw that earlier. That was when we had, this dense equals
[00:12:57.120 --> 00:12:59.640]   two, this is the thing that gives us evaluate method that
[00:12:59.640 --> 00:13:02.040]   allows us to evaluate at arbitrary time points, get our
[00:13:02.040 --> 00:13:04.280]   solution at arbitrary time points. So Diffrax supports
[00:13:04.280 --> 00:13:08.440]   this. Adjoint methods for backpropagation. So those of you
[00:13:08.440 --> 00:13:10.160]   who follow the neural differential equation
[00:13:10.160 --> 00:13:12.680]   literature, probably, you know, there was this neural ordinary
[00:13:12.680 --> 00:13:17.320]   differential equation paper, won best paper in Europe 2018. And
[00:13:17.320 --> 00:13:21.840]   it did a lot to popularize one of these alternate methods of
[00:13:21.840 --> 00:13:25.240]   backpropagating through a differential equation. And there
[00:13:25.240 --> 00:13:27.360]   are in fact multiple ways of backpropagating through a
[00:13:27.360 --> 00:13:31.200]   differential equation and Diffrax supports these. As I've
[00:13:31.200 --> 00:13:33.360]   mentioned neural differential equations just now, yes, you can
[00:13:33.360 --> 00:13:37.520]   absolutely do these in Diffrax. There's things like step size
[00:13:37.520 --> 00:13:41.320]   controllers. So what we saw in these examples, I just wrote dt
[00:13:41.320 --> 00:13:43.560]   naught equals, I mean, I think it was somewhere above, but what
[00:13:43.560 --> 00:13:48.320]   I wrote was, was this, I wrote dt naught equals 0.0.1. That's
[00:13:48.320 --> 00:13:52.720]   to say it's this constant fixed step size. In practice,
[00:13:52.720 --> 00:13:55.280]   actually, you often want to adapt your step size and do
[00:13:55.280 --> 00:13:57.960]   something smart there. There are particularly advanced ways of
[00:13:57.960 --> 00:13:59.960]   doing that, Diffrax supports these. So we're at the current
[00:13:59.960 --> 00:14:03.800]   state of the art there. This is where I now lean more into the
[00:14:03.800 --> 00:14:06.120]   scientific computing and the more like the JAX and the COBSI
[00:14:06.120 --> 00:14:08.760]   end of things, where I start saying we can Vmap or Grad over
[00:14:08.760 --> 00:14:12.200]   anything. And at this point, maybe you're not surprised
[00:14:12.200 --> 00:14:14.280]   because you do JAX, you know JAX, and of course, you can
[00:14:14.280 --> 00:14:16.760]   Vmap over anything. That's the point of JAX, right? You can do
[00:14:16.760 --> 00:14:19.080]   that. Of course, you can differentiate anything because
[00:14:19.080 --> 00:14:22.960]   it's just JAX and JAX allows you to do that. But from the point
[00:14:22.960 --> 00:14:25.040]   of view of differential equation solving libraries, this is
[00:14:25.040 --> 00:14:29.080]   really new. It's pretty common to be able to batch over, say,
[00:14:29.080 --> 00:14:31.560]   your initial condition. It's pretty common to be able to
[00:14:31.560 --> 00:14:34.800]   differentiate with respect to your initial condition. But it's
[00:14:34.800 --> 00:14:37.200]   much less common to be able to do either of those with respect
[00:14:37.200 --> 00:14:39.240]   to, say, the region of integration that you've chosen
[00:14:39.240 --> 00:14:42.680]   to integrate over, or say, with the tolerances inside your
[00:14:42.680 --> 00:14:44.720]   solver, the things that determine how big your step
[00:14:44.720 --> 00:14:50.040]   sizes are, dot dot dot dot dot. And this is, I think, a really,
[00:14:50.040 --> 00:14:53.120]   really nice example of the fact that we, because Diffrax has
[00:14:53.120 --> 00:14:55.920]   been built inside of JAX, has been built inside of this order
[00:14:55.920 --> 00:14:59.120]   differentiation framework, it allows us to do these things
[00:14:59.120 --> 00:15:02.080]   completely for free. And this is something that in any other
[00:15:02.080 --> 00:15:03.760]   framework basically just doesn't exist because it's almost
[00:15:03.760 --> 00:15:06.880]   impossible. And that I think is just a huge step change with
[00:15:06.880 --> 00:15:09.280]   this ubiquitous differentiable programming that we're starting
[00:15:09.280 --> 00:15:14.520]   to see up here. We can use Pytree as the state. So you saw
[00:15:14.520 --> 00:15:17.840]   that on previously, again, it was somewhere further up, so I
[00:15:17.840 --> 00:15:20.840]   won't type it. I won't go up and find it. But I won't whine
[00:15:20.840 --> 00:15:23.120]   about it. I'll just say, "Hey, I'm going to run this. I'm going
[00:15:23.120 --> 00:15:25.040]   to run this. I'm going to run this. I'm going to run this."
[00:15:25.040 --> 00:15:27.000]   And I'll just say, "Hey, I'm going to run this. I'm going to
[00:15:27.000 --> 00:15:29.200]   run this." And I'll just say, "Hey, I'm going to run this."
[00:15:29.200 --> 00:15:31.200]   And I'll just say, "Hey, I'm going to run this." And I'll
[00:15:31.200 --> 00:15:33.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:15:33.200 --> 00:15:35.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:15:35.200 --> 00:15:37.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:15:37.200 --> 00:15:39.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:15:39.200 --> 00:15:41.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:15:41.200 --> 00:15:43.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:15:43.200 --> 00:15:45.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:15:45.200 --> 00:15:47.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:15:47.200 --> 00:15:49.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:15:49.200 --> 00:15:51.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:15:51.200 --> 00:15:53.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:15:53.200 --> 00:15:55.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:15:55.200 --> 00:15:57.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:15:57.200 --> 00:15:59.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:15:59.200 --> 00:16:01.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:01.200 --> 00:16:03.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:03.200 --> 00:16:05.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:16:05.200 --> 00:16:07.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:16:07.200 --> 00:16:09.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:09.200 --> 00:16:11.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:11.200 --> 00:16:13.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:16:13.200 --> 00:16:15.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:16:15.200 --> 00:16:17.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:17.200 --> 00:16:19.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:19.200 --> 00:16:21.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:16:21.200 --> 00:16:23.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:16:23.200 --> 00:16:25.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:25.200 --> 00:16:27.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:27.200 --> 00:16:29.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:16:29.200 --> 00:16:31.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:16:31.200 --> 00:16:33.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:33.200 --> 00:16:35.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:35.200 --> 00:16:37.200]   just say, "Hey, I'm going to run this." And I'll just say,
[00:16:37.200 --> 00:16:39.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:16:39.200 --> 00:16:41.200]   going to run this." And I'll just say, "Hey, I'm going to run
[00:16:41.200 --> 00:16:43.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:43.200 --> 00:16:45.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey,
[00:16:45.200 --> 00:16:47.200]   I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:16:47.200 --> 00:16:49.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:49.200 --> 00:16:51.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey,
[00:16:51.200 --> 00:16:53.200]   I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:16:53.200 --> 00:16:55.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:16:55.200 --> 00:16:57.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey,
[00:16:57.200 --> 00:16:59.200]   I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:16:59.200 --> 00:17:19.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:17:19.200 --> 00:17:21.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:17:21.200 --> 00:17:23.200]   going to run this." And I'll just say, "Hey, I'm going to run this."
[00:17:23.200 --> 00:17:25.200]   And I'll just say, "Hey, I'm going to run this." And I'll just say,
[00:17:25.200 --> 00:17:27.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to
[00:17:27.200 --> 00:17:29.200]   run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:17:29.200 --> 00:17:31.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:17:31.200 --> 00:17:33.200]   going to run this." And I'll just say, "Hey, I'm going to run this."
[00:17:33.200 --> 00:17:35.200]   And I'll just say, "Hey, I'm going to run this." And I'll just say,
[00:17:35.200 --> 00:17:37.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:17:37.200 --> 00:17:39.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll just
[00:17:39.200 --> 00:17:41.200]   say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going
[00:17:41.200 --> 00:17:43.200]   to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:17:43.200 --> 00:17:55.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going
[00:17:55.200 --> 00:17:57.200]   to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:17:57.200 --> 00:17:59.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going
[00:17:59.200 --> 00:18:01.200]   to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:01.200 --> 00:18:03.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going
[00:18:03.200 --> 00:18:05.200]   to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:05.200 --> 00:18:07.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going
[00:18:07.200 --> 00:18:09.200]   to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:09.200 --> 00:18:13.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to
[00:18:13.200 --> 00:18:15.200]   run this." And I'll just say, "Hey, I'm going to run this." And I'll just
[00:18:15.200 --> 00:18:17.200]   say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:18:17.200 --> 00:18:19.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll just say,
[00:18:19.200 --> 00:18:21.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to run this."
[00:18:21.200 --> 00:18:23.200]   And I'll just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:18:23.200 --> 00:18:25.200]   going to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:25.200 --> 00:18:27.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to
[00:18:27.200 --> 00:18:39.200]   run this." And I'll just say, "Hey, I'm going to run this." And I'll just say,
[00:18:39.200 --> 00:18:41.200]   "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to run this."
[00:18:41.200 --> 00:18:43.200]   And I'll just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm
[00:18:43.200 --> 00:18:45.200]   going to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:45.200 --> 00:18:47.200]   just say, "Hey, I'm going to run this." And I'll just say, "Hey, I'm going to run
[00:18:47.200 --> 00:18:49.200]   this." And I'll just say, "Hey, I'm going to run this." And I'll just say, "Hey,
[00:18:49.200 --> 00:18:51.200]   I'm going to run this." And I'll just say, "Hey, I'm going to run this." And I'll
[00:18:51.200 --> 00:18:55.680]   So you've got db5 but this is an abstract explicit Runge-Kutta method, it's also an
[00:18:55.680 --> 00:19:01.080]   abstract Runge-Kutta method, abstract adapted solver which in turn hits abstract solver.
[00:19:01.080 --> 00:19:05.080]   And so there's this, as I said, it's quite sophisticated collection of different kinds
[00:19:05.080 --> 00:19:10.480]   of solver, and you're free to mix these in as you wish to say your solvers are a certain
[00:19:10.480 --> 00:19:11.480]   type or not.
[00:19:11.480 --> 00:19:13.080]   So this allows you to add custom ops.
[00:19:13.080 --> 00:19:17.200]   So you might notice in particular, by the way, the interface is not ifxsolve solver
[00:19:17.200 --> 00:19:20.320]   equals db5, it doesn't look like that, right?
[00:19:20.320 --> 00:19:24.440]   You are free to define your own solver if you wish, you don't just have to use pre-built
[00:19:24.440 --> 00:19:25.440]   ones.
[00:19:25.440 --> 00:19:28.960]   Here's a really nice example for those of you who know what I mean when I say semi-explicit
[00:19:28.960 --> 00:19:35.560]   differential algebraic equation, which is that when DFATS are being designed, we designed
[00:19:35.560 --> 00:19:40.040]   it to work with ODEs, we designed it to work with SDEs, we designed it to handle both of
[00:19:40.040 --> 00:19:43.000]   these cases in the same unified way.
[00:19:43.000 --> 00:19:46.840]   But as it turns out, this same unified way is also capable of handling these other more
[00:19:46.840 --> 00:19:50.080]   complicated semi-explicit differential algebraic equations.
[00:19:50.080 --> 00:19:52.280]   We didn't even design it to do that.
[00:19:52.280 --> 00:19:55.440]   That is almost a fruit, that just happened.
[00:19:55.440 --> 00:20:00.720]   And the way I like to think about this is simply that I think this is a good indication
[00:20:00.720 --> 00:20:03.920]   that we've hit on the right abstractions.
[00:20:03.920 --> 00:20:08.520]   If you think of like a machine learning model, a machine learning model is good if it generalizes,
[00:20:08.520 --> 00:20:10.800]   if it works on the things you did not train it to work on.
[00:20:10.800 --> 00:20:14.840]   And exactly the same here, we built these abstractions to work with ODEs and SDEs.
[00:20:14.840 --> 00:20:18.200]   And so the fact that it still works in this other much more complicated regime, I think
[00:20:18.200 --> 00:20:22.800]   is a nice indication that probably the abstractions we've introduced are a little bit new, and
[00:20:22.800 --> 00:20:27.240]   I think they're the right ones to be working with.
[00:20:27.240 --> 00:20:31.280]   So I've told you a lot about the features inside DFATS.
[00:20:31.280 --> 00:20:36.080]   Let's open up the hood a little bit, and I'll start talking about some of the things happening
[00:20:36.080 --> 00:20:41.560]   inside DFATS and some of the things that makes it buzz and the things that makes it work.
[00:20:41.560 --> 00:20:43.960]   And why is it cool?
[00:20:43.960 --> 00:20:48.180]   So I mentioned these controlled differential equations earlier.
[00:20:48.180 --> 00:20:53.960]   So as I told you then, ODEs and SDEs, these are reduced to CDs and solved in a unified
[00:20:53.960 --> 00:20:54.960]   way.
[00:20:54.960 --> 00:20:55.960]   So what do I mean by that mathematically?
[00:20:55.960 --> 00:21:00.720]   Well, let's just say if I write down this, this is just an ordinary differential equation.
[00:21:00.720 --> 00:21:04.760]   If you're not familiar with the standard notation, by the way, just be just like divide by DT
[00:21:04.760 --> 00:21:08.400]   on both sides, just quietly pretend that's a thing you can do.
[00:21:08.400 --> 00:21:14.040]   Put that DT on the other side, and it looks like an ODE like you're used to.
[00:21:14.040 --> 00:21:15.640]   And then you write down, say, this SDE.
[00:21:15.640 --> 00:21:18.260]   So again, it's just a standard looking SDE, nothing special there.
[00:21:18.260 --> 00:21:23.340]   Well, in some sense, these are the special cases of this notion of a controlled differential
[00:21:23.340 --> 00:21:24.340]   equation.
[00:21:24.340 --> 00:21:25.340]   And that's this thing I've written down here.
[00:21:25.340 --> 00:21:27.700]   And I'm not expecting anyone here to be familiar with these.
[00:21:27.700 --> 00:21:31.900]   Like I said, these are a relatively esoteric piece of mathematics.
[00:21:31.900 --> 00:21:35.640]   But you can sort of like intuitively see, I think, that if you look at this and write
[00:21:35.640 --> 00:21:41.340]   x of t equals t, if you consider the function f, so if you consider x to be the identity
[00:21:41.340 --> 00:21:45.620]   function, then you will see that actually this thing here, that's just an ODE.
[00:21:45.620 --> 00:21:49.220]   If x is the identity function, this is just an ODE.
[00:21:49.220 --> 00:21:56.900]   In contrast, if x is equal to t comma w of t, so if x is the identity function and the
[00:21:56.900 --> 00:22:01.220]   Brownian motion just put together in a pair, then you can see that what I've written down
[00:22:01.220 --> 00:22:05.200]   here is essentially the same as the SDE as well.
[00:22:05.200 --> 00:22:09.700]   So there's this general notion of controlled differential equation, which generalises ODEs
[00:22:09.700 --> 00:22:14.460]   and SDEs and is a well-formed concept all on its own.
[00:22:14.460 --> 00:22:19.260]   And this is really what we exploit in order to do cool solvers inside of facts in order
[00:22:19.260 --> 00:22:24.380]   to simplify the internals of the facts far, far, far than you could do otherwise.
[00:22:24.380 --> 00:22:26.020]   So here's explicit Euler's method.
[00:22:26.020 --> 00:22:30.140]   This is the simplest possible solver for an ODE.
[00:22:30.140 --> 00:22:32.660]   And so this is probably familiar to many of you in the audience.
[00:22:32.660 --> 00:22:36.940]   But if any of you also consider, say, the same solver for SDEs, and you can see, again,
[00:22:36.940 --> 00:22:39.980]   I've just written down explicit Euler's method, well, you can see that, actually, look, these
[00:22:39.980 --> 00:22:40.980]   have a lot of common structure, right?
[00:22:40.980 --> 00:22:45.100]   Like in both cases, I've evaluated my vector field, I've taken a difference of two things,
[00:22:45.100 --> 00:22:48.260]   and then I have some bilinear operation that combines them.
[00:22:48.260 --> 00:22:51.780]   So I've got this vector field, I've got a difference of two scalars, and then the bilinear
[00:22:51.780 --> 00:22:54.580]   operation is multiplication, f times dt.
[00:22:54.580 --> 00:22:58.660]   Meanwhile, if I go looking at the diffusion, you can see exactly the same thing.
[00:22:58.660 --> 00:23:05.900]   I've got this difference in my control, in my Brownian motion, in my noise process here.
[00:23:05.900 --> 00:23:11.340]   And then f may, say, be a matrix, w may be a vector, and then you can see my bilinear
[00:23:11.340 --> 00:23:13.140]   operation is a matrix vector product.
[00:23:13.140 --> 00:23:18.100]   So you can see that, actually, we have this common structure that describes what's going
[00:23:18.100 --> 00:23:22.060]   on here, that describes this interaction between a vector field and its control.
[00:23:22.060 --> 00:23:26.220]   And so we can hope to try and write code that does all of this in the same unified way.
[00:23:26.220 --> 00:23:29.820]   And so if I go back over here, then this is where we have this thing called an abstract
[00:23:29.820 --> 00:23:30.820]   term.
[00:23:30.820 --> 00:23:34.220]   And this is the abstract object that represents what's going on here.
[00:23:34.220 --> 00:23:37.140]   And so earlier on, you saw me write down things like ODE term.
[00:23:37.140 --> 00:23:40.860]   Well, actually, if we have a look at this, this hierarchy, you can see actually this
[00:23:40.860 --> 00:23:43.660]   is a so-called abstract term.
[00:23:43.660 --> 00:23:48.420]   And all that is capturing is the notion that you have this term, this whole term here.
[00:23:48.420 --> 00:23:52.220]   And the fact that it's an ODE is simply describing the fact that it is time that appears here
[00:23:52.220 --> 00:23:53.220]   as the control on the right-hand side.
[00:23:53.220 --> 00:23:56.220]   And then likewise, also, we saw things like control term.
[00:23:56.220 --> 00:23:58.220]   And it's exactly the same story here.
[00:23:58.220 --> 00:24:02.540]   With the term, it has some sort of bilinear operation between vector field and control.
[00:24:02.540 --> 00:24:06.460]   And we had a vector field, and we had our control that was running in motion.
[00:24:06.460 --> 00:24:11.060]   So you can see there's this new idea of a term, of this abstract term, that captures
[00:24:11.060 --> 00:24:12.060]   what's going on there.
[00:24:12.060 --> 00:24:17.540]   And so what are the implications of this reduction to this piece of mathematics that I've introduced
[00:24:17.540 --> 00:24:18.540]   and sort of drawn in?
[00:24:18.540 --> 00:24:19.540]   Why is this cool?
[00:24:19.540 --> 00:24:20.540]   Why is this useful?
[00:24:20.540 --> 00:24:21.540]   Well, first of all, it's great for me.
[00:24:21.540 --> 00:24:27.220]   Just for me personally, as a library author, I am very, very happy that I get to write
[00:24:27.220 --> 00:24:28.220]   less code.
[00:24:28.220 --> 00:24:32.300]   This makes my life much simpler.
[00:24:32.300 --> 00:24:35.580]   And this actually comes in a few different forms.
[00:24:35.580 --> 00:24:36.580]   I've discussed solvers already.
[00:24:36.580 --> 00:24:39.580]   I've told you I can write one solver, and this one solver will work.
[00:24:39.580 --> 00:24:40.580]   That's great.
[00:24:40.580 --> 00:24:42.580]   But actually, we can go much, much further than this.
[00:24:42.580 --> 00:24:46.180]   So I mentioned that there are multiple ways to back propagate through a differential equation.
[00:24:46.180 --> 00:24:47.180]   This is pretty complicated.
[00:24:47.180 --> 00:24:51.260]   As you might expect, writing custom back propagation is kind of difficult.
[00:24:51.260 --> 00:24:54.380]   It's certainly something you need to do quite carefully to avoid bugs.
[00:24:54.380 --> 00:24:55.380]   And this is great.
[00:24:55.380 --> 00:24:57.620]   I don't have to write it out separately for ODEs and SDEs and so on.
[00:24:57.620 --> 00:24:59.340]   I can just write one implementation.
[00:24:59.340 --> 00:25:03.980]   So you can see that we actually take advantage of this in actually really quite strong, meaningful,
[00:25:03.980 --> 00:25:04.980]   kind of difficult to write ways.
[00:25:04.980 --> 00:25:10.940]   For those of you who are curious, by the way, there is some mathematics here that you can
[00:25:10.940 --> 00:25:11.940]   find in my thesis.
[00:25:11.940 --> 00:25:15.940]   And indeed, the same set of abstractions I keep introducing, I keep discussing, these
[00:25:15.940 --> 00:25:18.260]   can handle lots and lots of different kinds of problems.
[00:25:18.260 --> 00:25:22.260]   So again, these same abstractions handle things like explicit solvers, but they also handle
[00:25:22.260 --> 00:25:24.020]   things like implicit solvers for stiff problems.
[00:25:24.020 --> 00:25:28.580]   They handle things like symplectic solvers for things like Hamiltonian problems, things
[00:25:28.580 --> 00:25:29.580]   like that.
[00:25:29.580 --> 00:25:30.580]   Dot, dot, dot, dot, dot.
[00:25:30.580 --> 00:25:31.580]   You get the idea.
[00:25:31.580 --> 00:25:32.580]   Code is simple.
[00:25:32.580 --> 00:25:33.580]   This is amazing.
[00:25:33.580 --> 00:25:38.220]   So this is great for me, as I said, as a library author.
[00:25:38.220 --> 00:25:40.340]   My hope is that this is a thing that keeps going.
[00:25:40.340 --> 00:25:43.460]   This is a thing that other people use when they write their own differential equation
[00:25:43.460 --> 00:25:44.880]   solving libraries.
[00:25:44.880 --> 00:25:47.060]   Because obviously, diffracts is not the final word.
[00:25:47.060 --> 00:25:51.220]   Someone will have some other cool idea in the next year, five years, 10 years, whenever.
[00:25:51.220 --> 00:25:53.620]   And someone will come along and write their own differential equation solving library,
[00:25:53.620 --> 00:25:54.620]   and it will be amazing.
[00:25:54.620 --> 00:26:01.460]   And my hope is that these insights can be useful to all future software developers and
[00:26:01.460 --> 00:26:06.580]   open source software developers working on similar sorts of problems.
[00:26:06.580 --> 00:26:10.460]   But more than that, for you, as a user, as an end user who doesn't care how the library
[00:26:10.460 --> 00:26:14.500]   does it, you just want to use the library, actually, this reduction has a lot of advantages
[00:26:14.500 --> 00:26:18.020]   for you as well, in particular, with respect to advanced use cases.
[00:26:18.020 --> 00:26:23.020]   So it means, for example, maybe you want to solve an OD and an SD at the same time, maybe
[00:26:23.020 --> 00:26:26.500]   in parallel, or maybe one of them affects the other, or something like that.
[00:26:26.500 --> 00:26:27.500]   You can do that.
[00:26:27.500 --> 00:26:31.020]   This is not difficult, because they all sit under a unified umbrella.
[00:26:31.020 --> 00:26:34.100]   Maybe you want to do something like a controlled stochastic differential equation.
[00:26:34.100 --> 00:26:37.460]   This is a pretty neat kind of thing to see appearing a few times.
[00:26:37.460 --> 00:26:38.460]   You can do that too.
[00:26:38.460 --> 00:26:39.460]   There's no issue with that.
[00:26:39.460 --> 00:26:42.700]   If I go back up here, and then I, where did I write it?
[00:26:42.700 --> 00:26:43.700]   Here we go.
[00:26:43.700 --> 00:26:44.700]   So I wrote it down here.
[00:26:44.700 --> 00:26:47.700]   Term equals drift component plus diffusion component.
[00:26:47.700 --> 00:26:50.860]   But if I wanted to, I could just add in another term here.
[00:26:50.860 --> 00:26:56.460]   And I could do my control vector field, and then whatever that control is.
[00:26:56.460 --> 00:26:58.460]   I could write that out, and that would be totally fine.
[00:26:58.460 --> 00:27:00.460]   I would just have another term.
[00:27:00.460 --> 00:27:01.460]   No issues.
[00:27:01.460 --> 00:27:07.020]   Lots and lots of solvers also demand special structure, need to look at special structure.
[00:27:07.020 --> 00:27:10.380]   So we've already seen this FDW as a matrix vector product.
[00:27:10.380 --> 00:27:14.500]   But actually, there's a bunch of other solvers which decompose this matrix and vector product
[00:27:14.500 --> 00:27:15.940]   and do something clever with it.
[00:27:15.940 --> 00:27:18.780]   So this is something like Nullarabotoir.
[00:27:18.780 --> 00:27:20.140]   And once again, you can implement this.
[00:27:20.140 --> 00:27:21.140]   This is fine.
[00:27:21.140 --> 00:27:25.540]   The abstractions are such that you can work with complicated solvers and complicated advanced
[00:27:25.540 --> 00:27:30.580]   use cases, and you don't need to do complicated stuff.
[00:27:30.580 --> 00:27:32.580]   This isn't weird hackery to make this work.
[00:27:32.580 --> 00:27:38.820]   OK, so I've got one other point on the internals that I want to discuss.
[00:27:38.820 --> 00:27:45.380]   So this is what I refer to as parameterized functions, like how these keep appearing.
[00:27:45.380 --> 00:27:50.460]   So when you think of this DiffXSolve operation, this is an important entry point that we've
[00:27:50.460 --> 00:27:52.780]   seen appearing many, many times.
[00:27:52.780 --> 00:27:54.780]   This consumes parameterized functions as inputs.
[00:27:54.780 --> 00:27:56.420]   So what do I mean by that?
[00:27:56.420 --> 00:28:00.020]   Well, what I mean by that is that, let's say we have a solver.
[00:28:00.020 --> 00:28:02.980]   So in our examples earlier today, we saw, say, Dopey5.
[00:28:02.980 --> 00:28:03.980]   We saw Euler's method.
[00:28:03.980 --> 00:28:06.100]   But we might have some of a solver as well.
[00:28:06.100 --> 00:28:07.340]   So here I've written Caveno3.
[00:28:07.340 --> 00:28:10.580]   That's just another differential equation solver.
[00:28:10.580 --> 00:28:13.900]   And this, in turn, this is some parameterized function.
[00:28:13.900 --> 00:28:17.540]   That is to say, it's a function, say, solver.steps is a function.
[00:28:17.540 --> 00:28:19.100]   It's an operation that does something.
[00:28:19.100 --> 00:28:20.100]   But it's parameterized.
[00:28:20.100 --> 00:28:26.220]   It depends upon these parameters, say, like a butcher tableau, the choice of nonlinear
[00:28:26.220 --> 00:28:27.220]   solver.
[00:28:27.220 --> 00:28:33.100]   All the things that define this particular solver as being this particular solver means
[00:28:33.100 --> 00:28:37.460]   that this step operation is parameterized by these quantities.
[00:28:37.460 --> 00:28:38.980]   We have things like the step size controller.
[00:28:38.980 --> 00:28:41.340]   So today, we just took constant step sizes.
[00:28:41.340 --> 00:28:44.180]   But I told you that we could choose more advanced ones, like PID controllers.
[00:28:44.180 --> 00:28:47.860]   And yes, in this case, once again, we end up with the same thing, that you have some
[00:28:47.860 --> 00:28:51.780]   function, some operation, in which case, this changing of step size.
[00:28:51.780 --> 00:28:54.300]   And this is parameterized by things like the tolerance.
[00:28:54.300 --> 00:28:57.980]   How accurately do you want your differential equation to be solved?
[00:28:57.980 --> 00:29:02.260]   It's parameterized by what are the allowed minimum and maximum sizes of your step sizes.
[00:29:02.260 --> 00:29:06.500]   Any particular places you want to put your step sizes, dot, dot, dot, dot, dot.
[00:29:06.500 --> 00:29:09.660]   So again, what we see is that this is an instance of a parameterized function.
[00:29:09.660 --> 00:29:11.940]   And now, this is going to get even more complicated.
[00:29:11.940 --> 00:29:14.820]   So don't worry if I lose you at this point.
[00:29:14.820 --> 00:29:17.500]   But let's suppose you want to do one of these custom backpropagation methods.
[00:29:17.500 --> 00:29:21.180]   Well, then actually, at this point, what you've got is an entire backward pass, which is in
[00:29:21.180 --> 00:29:26.340]   turn parameterized by these solvers and step size controllers, which are themselves also
[00:29:26.340 --> 00:29:27.340]   parameterized functions.
[00:29:27.340 --> 00:29:31.380]   So you have these hyperparameterized functions, netoperameterized functions, whatever you
[00:29:31.380 --> 00:29:36.580]   want to call them, which are parameterized functions of parameterized functions.
[00:29:36.580 --> 00:29:39.580]   And you can see this starts getting a little bit complicated, a little bit difficult to
[00:29:39.580 --> 00:29:42.060]   reason about.
[00:29:42.060 --> 00:29:45.900]   And it's actually because of this that Diffrax actually resulted in this spin-out project
[00:29:45.900 --> 00:29:49.660]   called Equinox, which kind of gets billed as a neural network library.
[00:29:49.660 --> 00:29:51.140]   But it's actually a lot more than that.
[00:29:51.140 --> 00:29:53.660]   And that's what I'm going to be talking about now.
[00:29:53.660 --> 00:29:57.860]   And don't worry if I lost you on this slide and I started talking about weird differential
[00:29:57.860 --> 00:30:00.460]   equation stuff that you've not seen before.
[00:30:00.460 --> 00:30:05.140]   Because what we're now going to see is a much more closer to home example of a parameterized
[00:30:05.140 --> 00:30:06.140]   function.
[00:30:06.140 --> 00:30:09.060]   And that is to say a neural network.
[00:30:09.060 --> 00:30:11.980]   So what we're going to do is we're going to write a parameterized function.
[00:30:11.980 --> 00:30:16.020]   It is to say some operation, but it has some theta that parameterizes it.
[00:30:16.020 --> 00:30:17.900]   We're going to write this as a pi-tree.
[00:30:17.900 --> 00:30:21.140]   This is the one trick that makes Equinox work.
[00:30:21.140 --> 00:30:25.500]   So let's import this Equinox library, this spin-out Equinox library that Diffrax depends
[00:30:25.500 --> 00:30:29.260]   upon, that came out of Diffrax, and out of a need to solve problems of the previous slide.
[00:30:29.260 --> 00:30:30.260]   So we're going to import that.
[00:30:30.260 --> 00:30:32.860]   We're going to import Jackson on-side it.
[00:30:32.860 --> 00:30:36.620]   And now I'm going to show you how we write down just a simple linear layer, just the
[00:30:36.620 --> 00:30:39.180]   simplest possible neural network using Equinox.
[00:30:39.180 --> 00:30:43.220]   So you can see we start off with this nice familiar looking PyTorch-like syntax.
[00:30:43.220 --> 00:30:45.460]   I think this has been appearing in every library since PyTorch.
[00:30:45.460 --> 00:30:48.460]   There's always some sort of module object.
[00:30:48.460 --> 00:30:49.460]   We have this weight and the bias.
[00:30:49.460 --> 00:30:52.220]   These are the two parts of the pi-tree.
[00:30:52.220 --> 00:30:54.660]   So linear here, this is going to be some pi-tree.
[00:30:54.660 --> 00:30:59.180]   And then weight and bias are the two sub-pi-trees, the two nodes coming off of it.
[00:30:59.180 --> 00:31:00.580]   And then how do we initialize those nodes?
[00:31:00.580 --> 00:31:02.580]   Well, we just define an init method.
[00:31:02.580 --> 00:31:05.860]   And then if you just have a quick read to this, you can see that this is just doing
[00:31:05.860 --> 00:31:08.420]   standard Jack stuff, nothing fancy here.
[00:31:08.420 --> 00:31:12.700]   I've just passed in a couple of integers to say what my weight and my bias should be.
[00:31:12.700 --> 00:31:17.780]   And then I've just initialized those as normal random variables.
[00:31:17.780 --> 00:31:19.380]   But of course, this is a parameterized function.
[00:31:19.380 --> 00:31:21.940]   What I've written down so far is a bag of parameters.
[00:31:21.940 --> 00:31:25.180]   I need the function, I need a forward pass.
[00:31:25.180 --> 00:31:26.980]   And that just might look like this.
[00:31:26.980 --> 00:31:30.740]   And again, just from a Python point of view, all I've done is define a call method.
[00:31:30.740 --> 00:31:31.740]   That's kind of completely expected.
[00:31:31.740 --> 00:31:33.740]   This is how you call an object, call a class.
[00:31:33.740 --> 00:31:40.060]   You can see we do a matrix multiply, a weight against vector, and then an R on the bias.
[00:31:40.060 --> 00:31:45.060]   So you can see that what we end up with here is this very sort of friendly looking, pi-torch
[00:31:45.060 --> 00:31:50.340]   inspired, deliberately pi-torch inspired syntax for creating the neural network.
[00:31:50.340 --> 00:31:53.260]   We're creating any parameterized function, including all the complicated ones we saw
[00:31:53.260 --> 00:31:54.260]   on the previous slide.
[00:31:54.260 --> 00:31:58.220]   I mean, it was a nice, friendly looking pi-torch syntax.
[00:31:58.220 --> 00:32:03.340]   And the really cool thing is that because what it is, when I subclass that class on
[00:32:03.340 --> 00:32:06.980]   module, what I'm doing is registering the linear as a custom pi-tree.
[00:32:06.980 --> 00:32:11.180]   Because that's the only thing that's happening here, the only thing that's happening is that
[00:32:11.180 --> 00:32:12.180]   linear is a pi-tree.
[00:32:12.180 --> 00:32:18.420]   It means I'm now safe to do things like this, where I just instantiate my model.
[00:32:18.420 --> 00:32:19.420]   This is now a pi-tree, model is a pi-tree.
[00:32:19.420 --> 00:32:22.420]   And therefore, I can-- and you know, Jack's.jnr knows how to work with pi-trees.
[00:32:22.420 --> 00:32:23.660]   I can just pass model in.
[00:32:23.660 --> 00:32:27.220]   It's just a pi-tree, and everything just works.
[00:32:27.220 --> 00:32:30.740]   And this is the-- this is one of the big points of XMARKS.
[00:32:30.740 --> 00:32:34.620]   For those of you doing neural networks, you're probably used to using like Flax or Haiku
[00:32:34.620 --> 00:32:35.620]   or something.
[00:32:35.620 --> 00:32:39.340]   Well, Flax and Haiku, you know, these really built these domain-specific languages on top
[00:32:39.340 --> 00:32:40.340]   of Jack's.
[00:32:40.340 --> 00:32:41.700]   They very much sit on top of Jack's.
[00:32:41.700 --> 00:32:47.140]   You can't, you know, fearlessly interchange Jack's and your library.
[00:32:47.140 --> 00:32:50.980]   You can't just put the two of them together freely.
[00:32:50.980 --> 00:32:54.380]   And this is why, you know, you end up with things like Flax.lin and .jit and Flax.lin.scan
[00:32:54.380 --> 00:32:57.140]   and da-da-da-da-da and all the rest of it, right?
[00:32:57.140 --> 00:32:58.140]   But here, you know, we don't need that.
[00:32:58.140 --> 00:33:00.540]   We can just use Jack's.jnr and that just works.
[00:33:00.540 --> 00:33:01.900]   So that's pretty cool, right?
[00:33:01.900 --> 00:33:03.900]   Because we just-- and that's-- this is all coming out as one idea.
[00:33:03.900 --> 00:33:08.860]   It's one idea that we should make our models be pi-trees by using these existing Jack's
[00:33:08.860 --> 00:33:12.100]   abstractions rather than building in new ones.
[00:33:12.100 --> 00:33:14.540]   So that's kind of cool.
[00:33:14.540 --> 00:33:17.940]   Mathematically, what we're doing here is I'm saying that I want my higher-order function
[00:33:17.940 --> 00:33:22.900]   g to see this parameterization theta in f theta.
[00:33:22.900 --> 00:33:26.340]   And this higher-order function, when I say higher-order function, I just mean that it
[00:33:26.340 --> 00:33:28.700]   accepts another function as an input.
[00:33:28.700 --> 00:33:29.700]   And if you're thinking, hang on, wait, what?
[00:33:29.700 --> 00:33:30.700]   Where did those come from?
[00:33:30.700 --> 00:33:31.700]   Well, actually, we already saw that.
[00:33:31.700 --> 00:33:33.900]   That was on the previous slide, right?
[00:33:33.900 --> 00:33:34.900]   This is compute loss.
[00:33:34.900 --> 00:33:36.260]   This is a higher-order function.
[00:33:36.260 --> 00:33:40.060]   All of your loss functions have always been higher-order functions because they've taken
[00:33:40.060 --> 00:33:43.900]   a forward pass, this call of this model.
[00:33:43.900 --> 00:33:46.420]   They've taken in this forward pass as a function.
[00:33:46.420 --> 00:33:49.180]   And then your compute loss is a function of that function.
[00:33:49.180 --> 00:33:50.860]   It's a higher-order function.
[00:33:50.860 --> 00:33:53.300]   So you can see that actually this is exactly what we're doing here.
[00:33:53.300 --> 00:33:56.260]   f theta is my model or my parameterized loss function.
[00:33:56.260 --> 00:33:58.700]   And then g is the evaluation of that loss function.
[00:33:58.700 --> 00:34:00.380]   So we've been doing this all along.
[00:34:00.380 --> 00:34:04.420]   You've all been doing this all along, pretty much no matter which library you've been using.
[00:34:04.420 --> 00:34:08.020]   And so what we're trying to do here is have g see this theta so that we can compute gradients
[00:34:08.020 --> 00:34:12.180]   with respect to it or jit with respect to it and so on and so on.
[00:34:12.180 --> 00:34:14.860]   And these parameterized functions, these keep appearing everywhere.
[00:34:14.860 --> 00:34:15.860]   This is really cool.
[00:34:15.860 --> 00:34:19.540]   So we've already seen just now how neural networks are these parameterized functions
[00:34:19.540 --> 00:34:22.260]   and how this is a useful abstraction.
[00:34:22.260 --> 00:34:27.300]   And then a few slides ago, I also showed you differential equation solvers and how this
[00:34:27.300 --> 00:34:31.980]   needs parameterized functions in lots and lots of places.
[00:34:31.980 --> 00:34:34.300]   But these appear, as I say, these appear absolutely everywhere.
[00:34:34.300 --> 00:34:35.300]   Let's consider, say, a data loader.
[00:34:35.300 --> 00:34:39.020]   So a data loader, you know, you load some data and then just spit it into your model.
[00:34:39.020 --> 00:34:40.700]   So this is like a next batch function.
[00:34:40.700 --> 00:34:42.820]   This goes and gets the next batch.
[00:34:42.820 --> 00:34:44.860]   This is a function that's parameterized by the training data.
[00:34:44.860 --> 00:34:49.500]   Things like optimizers, this is a very, very classic example as well, where making a step
[00:34:49.500 --> 00:34:53.180]   of your optimizer, a step of gradient descent, a step of Adam, something like this.
[00:34:53.180 --> 00:34:55.180]   This is a function parameterized by the learning rate.
[00:34:55.180 --> 00:35:00.940]   Or if it's Adam, maybe it's parameterized by beta one and beta two and things like that.
[00:35:00.940 --> 00:35:03.580]   Things like wrapper functions, you see this appearing quite a lot, right?
[00:35:03.580 --> 00:35:07.460]   That you've got some function here, you've got some function of x, but you're passing
[00:35:07.460 --> 00:35:09.980]   it into some other API that demands two arguments.
[00:35:09.980 --> 00:35:11.980]   You can see x, you've got other argument.
[00:35:11.980 --> 00:35:13.980]   That's quite a common thing to have happen.
[00:35:13.980 --> 00:35:15.660]   You need to match an API somewhere.
[00:35:15.660 --> 00:35:18.420]   So you wrap your function up in something.
[00:35:18.420 --> 00:35:20.700]   And this is a very easy way of doing it, right?
[00:35:20.700 --> 00:35:21.700]   This is exactly what you do.
[00:35:21.700 --> 00:35:24.380]   You can see I've just created this new pytree.
[00:35:24.380 --> 00:35:29.540]   What this pytree does is it wraps this function and has a forward pass that just takes this
[00:35:29.540 --> 00:35:30.540]   call method.
[00:35:30.540 --> 00:35:35.860]   It just evaluates this function whilst respecting the API that we need it to respect.
[00:35:35.860 --> 00:35:39.500]   In particular, by the way, a lot of people have often tried to work around this by just
[00:35:39.500 --> 00:35:42.500]   wrapping into a lambda function or something like that.
[00:35:42.500 --> 00:35:45.980]   But if you do that, then you end up with a different lambda function every time you wrap
[00:35:45.980 --> 00:35:51.260]   it, and maybe that doesn't play well with just grad and so on, where you recompile needlessly.
[00:35:51.260 --> 00:35:53.940]   But as long as you work with the abstractions that Jaxx introduces, as long as you already
[00:35:53.940 --> 00:35:59.820]   do use pytrees, as Jaxx understands, then you can avoid that kind of overhead.
[00:35:59.820 --> 00:36:00.820]   Abstract franchise functions.
[00:36:00.820 --> 00:36:03.260]   So we've already seen some of these.
[00:36:03.260 --> 00:36:04.540]   I've already shown you abstract solve.
[00:36:04.540 --> 00:36:06.020]   I've already shown you abstract term.
[00:36:06.020 --> 00:36:12.300]   So these are examples of abstract franchise functions in the context of differential equations.
[00:36:12.300 --> 00:36:16.540]   So it's very, very common that you need your solver to have a step method.
[00:36:16.540 --> 00:36:18.700]   You need things like that.
[00:36:18.700 --> 00:36:23.020]   And yeah, you can absolutely go ahead and define these just in a normal way using Python.
[00:36:23.020 --> 00:36:25.980]   Because once again, Equinox just integrates with Jaxx, integrates with normal Python in
[00:36:25.980 --> 00:36:28.020]   exactly the way you always expect it to.
[00:36:28.020 --> 00:36:33.220]   This is also a nice moment, by the way, to highlight how this is step.
[00:36:33.220 --> 00:36:36.140]   And on the previous example, we had call.
[00:36:36.140 --> 00:36:37.140]   No method is special case.
[00:36:37.140 --> 00:36:38.140]   It doesn't matter.
[00:36:39.100 --> 00:36:42.100]   You can use whatever you like, and then you can either do my thing dot step, or you can
[00:36:42.100 --> 00:36:44.500]   just do my thing and then just call it with brackets.
[00:36:44.500 --> 00:36:45.500]   It doesn't matter.
[00:36:45.500 --> 00:36:46.500]   This will all work.
[00:36:46.500 --> 00:36:49.080]   And likewise, you could introduce multiple methods.
[00:36:49.080 --> 00:36:52.320]   Maybe you're building a VE, and you have both an encoder and a decoder.
[00:36:52.320 --> 00:36:54.540]   So you have an encoder and a decoder.
[00:36:54.540 --> 00:36:57.840]   You've got two parametrized functions parameterized by the same quantities.
[00:36:57.840 --> 00:36:59.860]   But once again, this just works.
[00:36:59.860 --> 00:37:01.620]   Just define your quantities up here, up at the top.
[00:37:01.620 --> 00:37:04.060]   And then you just do def encode, whatever.
[00:37:04.060 --> 00:37:05.060]   Def decode, whatever.
[00:37:05.060 --> 00:37:06.060]   And again, this works.
[00:37:06.060 --> 00:37:14.580]   So these are lots and lots of examples of parametrized functions.
[00:37:14.580 --> 00:37:17.900]   What's kind of neat, actually, is that this equinox dot module thing can be used sort
[00:37:17.900 --> 00:37:21.660]   of like slightly sneakily for a few other things as well, which I think are kind of
[00:37:21.660 --> 00:37:22.660]   cool.
[00:37:22.660 --> 00:37:24.900]   So for example, maybe you just need a custom collection of objects.
[00:37:24.900 --> 00:37:29.380]   So you can do this in sort of similar fashion using like a tuple or a names tuple or a dictionary
[00:37:29.380 --> 00:37:34.860]   or something to try and pack all of your quantities together in some meaningful operations, in
[00:37:34.860 --> 00:37:38.180]   some meaningful structured type.
[00:37:38.180 --> 00:37:40.980]   You can use equinox dot module just as a convenient way to do this as well.
[00:37:40.980 --> 00:37:44.540]   So you can just use the same thing multiple times without having to use two different
[00:37:44.540 --> 00:37:45.540]   pieces of technology.
[00:37:45.540 --> 00:37:47.500]   So that's kind of neat.
[00:37:47.500 --> 00:37:50.700]   Also, maybe you want to use strings inside a Jax dot JIT region.
[00:37:50.700 --> 00:37:53.180]   But of course, a string is not a Jax type.
[00:37:53.180 --> 00:37:58.940]   Jax does not know how to work with strings until now, until you do this.
[00:37:58.940 --> 00:38:01.700]   And then you can see what I've actually done here is I've just taken some string.
[00:38:01.700 --> 00:38:07.180]   And then I on the fly define some custom Pytree whose wrapper is the string that I want it
[00:38:07.180 --> 00:38:08.180]   to be.
[00:38:08.180 --> 00:38:13.260]   And as a result, now whenever I print out this Jax type M, this is a thing that Jax
[00:38:13.260 --> 00:38:15.060]   knows how to work with because it's a Pytree.
[00:38:15.060 --> 00:38:19.780]   And now whenever I print this out, it appears as this string that I need it to.
[00:38:19.780 --> 00:38:25.620]   So maybe you can use it to represent error messages or something like this inside JIT.
[00:38:25.620 --> 00:38:29.860]   So you can see that there's actually quite a lot of cool things we can do with this single
[00:38:29.860 --> 00:38:32.300]   Equinox dot module abstraction.
[00:38:32.300 --> 00:38:36.940]   So you've seen how this was really motivated by a need to solve problems in Diffrax, by
[00:38:36.940 --> 00:38:41.540]   a need to solve these scientific computing problems in Jax.
[00:38:41.540 --> 00:38:46.580]   And that sort of spun out into neural networks and to do lots and lots of other things besides.
[00:38:46.580 --> 00:38:47.580]   So you have heard of it.
[00:38:47.580 --> 00:38:49.820]   You've probably thought of Equinox as a neural network library.
[00:38:49.820 --> 00:38:51.980]   And I'm here to tell you that's great and it's amazing.
[00:38:51.980 --> 00:38:55.500]   And actually, you can do so much more from that too.
[00:38:55.500 --> 00:38:59.220]   So on that note, I think that brings me to the end of my talk.
[00:38:59.220 --> 00:39:01.140]   So this is my final slide.
[00:39:01.140 --> 00:39:04.420]   So if you want to know more about Diffrax, if you want to go and use Diffrax, if you
[00:39:04.420 --> 00:39:08.020]   want to go and solve some differential equations, that is available on GitHub.
[00:39:08.020 --> 00:39:12.100]   If you want to dig into Equinox, either just for curiosity or because you probably want
[00:39:12.100 --> 00:39:15.380]   to solve some neural networks, likewise available on GitHub.
[00:39:15.380 --> 00:39:20.860]   Please do feel free to just send me an email or put me on Twitter if any of you want to
[00:39:20.860 --> 00:39:22.420]   know more, if you have any questions later.
[00:39:22.420 --> 00:39:26.940]   And of course, finally, if you have any questions now, then go ahead.
[00:39:26.940 --> 00:39:27.940]   So ta-da!
[00:39:27.940 --> 00:39:28.940]   Done.
[00:39:28.940 --> 00:39:31.060]   There was a comment.
[00:39:31.060 --> 00:39:32.940]   So this is being streamed to two places.
[00:39:32.940 --> 00:39:35.420]   One where people had signed up.
[00:39:35.420 --> 00:39:37.100]   The comment was, "Thank you.
[00:39:37.100 --> 00:39:42.100]   Now we can do molecules plus JIT plus Pmap and JAX."
[00:39:42.100 --> 00:39:45.100]   I'm glad to hear it.
[00:39:45.100 --> 00:39:50.420]   Let me see if there are any other questions.
[00:39:50.420 --> 00:39:51.680]   Would it be possible...
[00:39:51.680 --> 00:39:56.220]   So this question is by Pablo Rodriguez.
[00:39:56.220 --> 00:39:57.540]   Great talk and great library.
[00:39:57.540 --> 00:40:00.780]   Would it be possible to solve PDEs with Diffrax?
[00:40:00.780 --> 00:40:01.780]   Right, yes.
[00:40:01.780 --> 00:40:05.540]   It depends on the PDE and depends on the problem, on how you want to solve it.
[00:40:05.540 --> 00:40:10.060]   So if you start with a PDE, now you semi-discretize it via method of lines or something like that.
[00:40:10.060 --> 00:40:13.300]   Well, then, of course, mathematically, all you're doing is discretizing your PDE into
[00:40:13.300 --> 00:40:14.300]   an ODE.
[00:40:14.300 --> 00:40:15.300]   And Diffrax can solve an ODE.
[00:40:15.300 --> 00:40:18.980]   So if you want to do something like that, then yes, absolutely, is the answer.
[00:40:18.980 --> 00:40:23.300]   Of course, the field of PDE numerics is a very large and very complicated one, where
[00:40:23.300 --> 00:40:25.900]   you end up with lots and lots of specialized solvers for special cases.
[00:40:25.900 --> 00:40:30.660]   And it's definitely out of scope for Diffrax to try and do all of those, because there's
[00:40:30.660 --> 00:40:33.340]   hundreds and hundreds and hundreds of those.
[00:40:33.340 --> 00:40:36.820]   So in short, anything that can be sensibly reduced to an ODE, yes, absolutely.
[00:40:36.820 --> 00:40:40.140]   And in practice, that usually means something like finite difference and just not finite
[00:40:40.140 --> 00:40:43.140]   element or finite volume.
[00:40:43.140 --> 00:40:46.260]   Thanks for answering that.
[00:40:46.260 --> 00:40:51.020]   Got a couple of questions.
[00:40:51.020 --> 00:40:54.780]   So we had the JAX-MD library author here.
[00:40:54.780 --> 00:40:57.460]   I don't know if you've seen that.
[00:40:57.460 --> 00:41:06.500]   So when I saw your diffusion example, it was like, OK, maybe you can do similar stuff.
[00:41:06.500 --> 00:41:14.820]   What are the differences between what the MD community does and these SDEs?
[00:41:14.820 --> 00:41:15.820]   They're kind of related.
[00:41:15.820 --> 00:41:16.820]   Right.
[00:41:16.820 --> 00:41:19.860]   I mean, I've got to be honest, I'm probably not well equipped to answer that.
[00:41:19.860 --> 00:41:25.180]   I've never looked that closely at molecular dynamics and molecular MD simulations.
[00:41:25.180 --> 00:41:26.180]   So I don't know.
[00:41:26.180 --> 00:41:29.180]   I'm not the expert on that.
[00:41:29.180 --> 00:41:30.180]   Interesting.
[00:41:30.180 --> 00:41:39.700]   I did take a look, but it seems that they were a little bit more lax on how they advanced
[00:41:39.700 --> 00:41:40.700]   to the next step.
[00:41:40.700 --> 00:41:41.700]   I don't know how to say it.
[00:41:41.700 --> 00:41:42.700]   Yeah.
[00:41:42.700 --> 00:41:46.700]   I mean, for what it's worth, a lot of these things are built on a differential equation
[00:41:46.700 --> 00:41:47.700]   solver somewhere.
[00:41:47.700 --> 00:41:51.260]   I think I remember poking inside JAX-MD at some point and seeing they had some Verlet
[00:41:51.260 --> 00:41:52.260]   integrator or something inside that.
[00:41:52.260 --> 00:41:53.260]   Yeah, that one.
[00:41:53.260 --> 00:41:54.260]   Yeah.
[00:41:54.260 --> 00:42:01.980]   So I mean, if I go back to like...
[00:42:01.980 --> 00:42:08.740]   Seems they care more about the statistics of the whole system more than the accuracy
[00:42:08.740 --> 00:42:09.740]   was what I got.
[00:42:09.740 --> 00:42:14.220]   I mean, what I was actually going to say is this point one here, maybe you want to write
[00:42:14.220 --> 00:42:15.220]   your own simulator.
[00:42:15.220 --> 00:42:18.740]   I mean, this is kind of the sort of example I had in mind where maybe you are writing
[00:42:18.740 --> 00:42:22.500]   down an MD simulator or something like that, and you don't want to write your own Verlet
[00:42:22.500 --> 00:42:27.940]   method or you want to be able to use a single API and switch out your synthetic solver for
[00:42:27.940 --> 00:42:28.940]   some other kind of solver.
[00:42:28.940 --> 00:42:29.940]   Anything like that.
[00:42:29.940 --> 00:42:33.580]   Or maybe you want to adapt your step size as you're going along.
[00:42:33.580 --> 00:42:36.180]   And that's kind of a fact by yourself.
[00:42:36.180 --> 00:42:41.220]   This is exactly the kind of use case which I think this point addresses, where you can
[00:42:41.220 --> 00:42:45.860]   use diffractors as a dependency and then just use that to do all the differential equation
[00:42:45.860 --> 00:42:52.940]   solving you need to do whilst you go ahead and do all the interesting MD stuff.
[00:42:52.940 --> 00:42:53.940]   Can you talk about the...
[00:42:53.940 --> 00:42:59.700]   Well, this is internal, but I chatted with you about this.
[00:42:59.700 --> 00:43:06.740]   The vector-like structure you created and maybe related to Stefan's...
[00:43:06.740 --> 00:43:08.740]   TreeMath thing.
[00:43:08.740 --> 00:43:09.740]   Yeah.
[00:43:09.740 --> 00:43:10.740]   TreeMath.
[00:43:10.740 --> 00:43:11.740]   Yeah, yeah, yeah.
[00:43:11.740 --> 00:43:13.740]   So these are two very, very similar concepts.
[00:43:13.740 --> 00:43:19.020]   And I mean, the thing I've used is, as you say, internal, but in retrospect, everyone
[00:43:19.020 --> 00:43:20.020]   else...
[00:43:20.020 --> 00:43:21.020]   So in practice, everyone else should be using TreeMath.
[00:43:21.020 --> 00:43:22.020]   Yeah, so this is...
[00:43:22.020 --> 00:43:23.020]   Where did I have it?
[00:43:23.020 --> 00:43:31.500]   So I had this thing where I wrote down that we can use pipeleases as the state.
[00:43:31.500 --> 00:43:38.060]   So this is something where you have this pipe tree and you're passing this along.
[00:43:38.060 --> 00:43:39.060]   Your pipe tree is evolving.
[00:43:39.060 --> 00:43:40.060]   Your state is evolving.
[00:43:40.060 --> 00:43:43.460]   Everything is changing and your solver keeps updating it.
[00:43:43.460 --> 00:43:46.980]   But what you'd really like to be able to write down inside your differential equation solver
[00:43:46.980 --> 00:43:53.180]   is just something like state plus DT times change in state.
[00:43:53.180 --> 00:43:55.380]   And you can't write that down if you have a pipe tree.
[00:43:55.380 --> 00:44:02.220]   You end up doing like Jacks.TreeMath plus Jacks.TreeMath multiplication DT times change
[00:44:02.220 --> 00:44:03.220]   in state.
[00:44:03.220 --> 00:44:07.220]   And you end up with all these millions and millions of ugly Jacks.TreeMaths everywhere.
[00:44:07.220 --> 00:44:11.220]   But your code works, but your code isn't readable.
[00:44:11.220 --> 00:44:19.860]   So instead, what we ended up introducing was this thing called Omega, which is literally
[00:44:19.860 --> 00:44:20.860]   just like TreeMath.
[00:44:20.860 --> 00:44:23.580]   It's just like a neater notation, frankly.
[00:44:23.580 --> 00:44:24.580]   So you can write down...
[00:44:24.580 --> 00:44:25.580]   So this is what we want to write down.
[00:44:25.580 --> 00:44:26.580]   This is saying a commutative method in code.
[00:44:26.580 --> 00:44:27.580]   I will write down something like that as a line of code.
[00:44:27.580 --> 00:44:39.460]   But I want to be able to do this with pipe trees here as the state.
[00:44:39.460 --> 00:44:43.540]   And so one option is just to go ahead and wrap all of these objects into a pipe tree,
[00:44:43.540 --> 00:44:45.740]   so into some object that automatically does all this for you.
[00:44:45.740 --> 00:44:46.740]   And that's exactly what we do.
[00:44:46.740 --> 00:44:52.740]   So this is where what I end up doing is introducing this Omega object.
[00:44:52.740 --> 00:44:55.740]   And then that.
[00:44:55.740 --> 00:44:59.740]   So you can see what we do is we somehow take, why not we raise it.
[00:44:59.740 --> 00:45:03.740]   So you can see we've got this notation with the power here, r pow.
[00:45:03.740 --> 00:45:09.740]   We raise it into this Omega object, and then do that for all of the objects.
[00:45:09.740 --> 00:45:11.740]   And then we just lower it again afterwards.
[00:45:11.740 --> 00:45:14.740]   So this now actually does the mathematics that we want it to, whilst having this quite
[00:45:14.740 --> 00:45:15.740]   neat notation.
[00:45:15.740 --> 00:45:18.740]   And for those of you who know what TreeMath is and have seen TreeMath, this is essentially
[00:45:18.740 --> 00:45:19.740]   the same thing.
[00:45:19.740 --> 00:45:22.740]   It's just with a slightly different notation.
[00:45:22.740 --> 00:45:26.740]   So I think that's kind of like it's just a neat trick for writing down these things in
[00:45:26.740 --> 00:45:30.740]   both the way it's easy to read and the way that's, you know, so I can actually handle
[00:45:30.740 --> 00:45:32.740]   the generality if we wanted to.
[00:45:32.740 --> 00:45:35.740]   And by the way, if this r pow notation looks a little bit magic to you, it's just something
[00:45:35.740 --> 00:45:42.740]   that's been deliberately overloaded to just do this.
[00:45:42.740 --> 00:45:44.740]   It just gives us a prettier notation for this one thing.
[00:45:44.740 --> 00:45:45.740]   It's just been overloaded.
[00:45:45.740 --> 00:45:50.740]   The top line's been overloaded to do the same thing as the bottom line.
[00:45:50.740 --> 00:45:53.740]   Yeah, anyway.
[00:45:53.740 --> 00:45:54.740]   Awesome.
[00:45:54.740 --> 00:45:59.740]   Should I take some community questions, Christian?
[00:45:59.740 --> 00:46:00.740]   Yeah, yeah, go ahead.
[00:46:00.740 --> 00:46:02.740]   I just was reading them.
[00:46:02.740 --> 00:46:03.740]   Okay.
[00:46:03.740 --> 00:46:05.740]   So this is by Pablo Aswell.
[00:46:05.740 --> 00:46:11.740]   Did you compare performance of solving ODEs versus approximation with RNNs?
[00:46:11.740 --> 00:46:16.740]   Most people are all skeptical, are still skeptical about performance of differential solvers.
[00:46:16.740 --> 00:46:20.740]   I guess this will change with artifacts and JAX.
[00:46:20.740 --> 00:46:21.740]   Right.
[00:46:21.740 --> 00:46:24.740]   So, I mean, if you're trying to solve an ODE with an RNN, so you're looking at something
[00:46:24.740 --> 00:46:28.740]   like a physics-informed neural network or something coming out of that end of the community.
[00:46:28.740 --> 00:46:32.740]   This is something which really only makes sense in like quite niche scenarios.
[00:46:32.740 --> 00:46:37.740]   It's really only sensible if you're doing like a high dimensional PDE or like a non-local
[00:46:37.740 --> 00:46:41.740]   PDE or something, you know, something awful like that.
[00:46:41.740 --> 00:46:45.740]   In every other respect, traditional mathematical solvers are just, you know, orders of magnitude
[00:46:45.740 --> 00:46:48.740]   more efficient than anything you're going to find coming out of the machine learning
[00:46:48.740 --> 00:46:49.740]   community.
[00:46:49.740 --> 00:46:50.740]   And that makes sense, right?
[00:46:50.740 --> 00:46:53.740]   Because people have spent a lot of time, multiple decades, optimizing these solvers to be very,
[00:46:53.740 --> 00:46:54.740]   very good.
[00:46:54.740 --> 00:46:59.740]   And if you're going to like train a custom solver of loads and loads of training data,
[00:46:59.740 --> 00:47:00.740]   this is a huge hassle.
[00:47:00.740 --> 00:47:03.740]   It actually doesn't get you any better.
[00:47:03.740 --> 00:47:07.740]   So, yeah, you can go ahead and use RNNs if you want to.
[00:47:07.740 --> 00:47:10.740]   But for most use cases, I would say just lean on the standard mathematical ways of handling
[00:47:10.740 --> 00:47:13.740]   these problems.
[00:47:13.740 --> 00:47:20.740]   Sanyam Bhutani: Jonathan asks, great to see you Jonathan in the chat.
[00:47:20.740 --> 00:47:21.740]   Equinox looks very elegant.
[00:47:21.740 --> 00:47:26.740]   Do you have plans for lots of things to add or do you feel that it is close to complete?
[00:47:26.740 --> 00:47:31.740]   Jonathan Pritchard-Cooper: I mean, never say never, but Equinox I think feels relatively
[00:47:31.740 --> 00:47:33.740]   complete to me at this point.
[00:47:33.740 --> 00:47:37.740]   It's got pretty much certainly all of like the basic neural network layers you expect
[00:47:37.740 --> 00:47:38.740]   to see in it.
[00:47:38.740 --> 00:47:41.740]   You know, it's got attention, it's got RNNs, it's got, you know, convolutional layers.
[00:47:41.740 --> 00:47:46.740]   It's got pooling, you know, dot, dot, dot, dot, exactly, etc., etc.
[00:47:46.740 --> 00:47:50.740]   And so in that respect, I think it's, you know, it's already got all of the toolbox
[00:47:50.740 --> 00:47:52.740]   it really needs.
[00:47:52.740 --> 00:47:56.740]   But conversely, you know, if there is some cool layer or something that you think would
[00:47:56.740 --> 00:48:00.740]   make sense to appear in a general purpose library, then, you know, go ahead, write the
[00:48:00.740 --> 00:48:03.740]   layer, open a PR or something like that, a pull request.
[00:48:03.740 --> 00:48:07.740]   You know, this is something I'm very, very happy to take community contributions on and
[00:48:07.740 --> 00:48:11.740]   help grow this tool to its usual purpose.
[00:48:11.740 --> 00:48:13.740]   Aasim, thanks for answering that.
[00:48:13.740 --> 00:48:15.740]   There's one more community question.
[00:48:15.740 --> 00:48:17.740]   This is by Connor.
[00:48:17.740 --> 00:48:19.740]   They say amazing library and amazing talk.
[00:48:19.740 --> 00:48:27.740]   If you're solving an ODE or SDE in the latent layer, example, a neural ODE or neural SDE,
[00:48:27.740 --> 00:48:33.740]   do you have to specify an adjoint method when you set up the solver or will Diffrax automatically
[00:48:33.740 --> 00:48:35.740]   choose one for you?
[00:48:35.740 --> 00:48:36.740]   Right.
[00:48:36.740 --> 00:48:40.740]   So Diffrax has a sensible default in the sense that it will perform what's known as
[00:48:40.740 --> 00:48:42.740]   discretized and optimized.
[00:48:42.740 --> 00:48:47.740]   But if you want to use a different backpropriation method, then you can totally do that.
[00:48:47.740 --> 00:48:51.740]   And in particular, you could switch it out and say, optimize and discretize.
[00:48:51.740 --> 00:48:55.740]   And there's like checkpointing things and things like that floating around there as well.
[00:48:55.740 --> 00:48:59.740]   If you prefer forward mode auto differentiation, then yes, you know, Jack supports this and
[00:48:59.740 --> 00:49:01.740]   therefore Diffrax supports this as well.
[00:49:01.740 --> 00:49:05.740]   So if you're the kind of person who wants to customize this and do the thing you want
[00:49:05.740 --> 00:49:08.740]   to do, yes, you can absolutely go ahead and do that.
[00:49:08.740 --> 00:49:15.740]   And if, by the way, you ever want to write your own, your own adjoint method, then abstract
[00:49:15.740 --> 00:49:17.740]   adjoint is absolutely a thing.
[00:49:17.740 --> 00:49:18.740]   You can subclass that.
[00:49:18.740 --> 00:49:19.740]   You can write your own.
[00:49:19.740 --> 00:49:23.740]   If conversely, you're just a user of these things and you just want to use a particular
[00:49:23.740 --> 00:49:28.740]   one or you're not an expert in this at all and you just want a smart default, you want
[00:49:28.740 --> 00:49:32.740]   Diffrax to do something reasonable for you without you having to worry about it, then
[00:49:32.740 --> 00:49:34.740]   yes, that's absolutely the case.
[00:49:34.740 --> 00:49:38.740]   Diffrax has a sensible default that will just work in like 95% of use cases.
[00:49:38.740 --> 00:49:46.740]   So I see a few more questions, so I'll ask them quickly.
[00:49:46.740 --> 00:49:48.740]   I hope that's okay with you, Christian.
[00:49:48.740 --> 00:49:50.740]   Yeah, yeah, go ahead.
[00:49:50.740 --> 00:49:52.740]   Usually Christian asks a lot of interesting questions.
[00:49:52.740 --> 00:49:56.740]   I want to make sure I let him ask those.
[00:49:56.740 --> 00:50:00.740]   Ian asks, does Diffrax work with stiff systems?
[00:50:00.740 --> 00:50:02.740]   Me, I'm not sure what that is.
[00:50:02.740 --> 00:50:07.740]   Okay, yeah, so for context here for everyone else, a stiff problem is in some sense like
[00:50:07.740 --> 00:50:10.740]   it's a difficult differential equation to solve numerically.
[00:50:10.740 --> 00:50:15.740]   The term stiff is a little bit imprecise, but basically it's just, you know, anything
[00:50:15.740 --> 00:50:17.740]   that's difficult to solve numerically.
[00:50:17.740 --> 00:50:19.740]   So yes, Diffrax totally supports stiff systems.
[00:50:19.740 --> 00:50:25.740]   So the main ones here, we've got things like Coverno 3 and Coverno 4, Coverno 5, etc.
[00:50:25.740 --> 00:50:33.740]   So these are a class of explicit singly diagonal implicit Lanker-Cutter methods, and Diffrax supports these.
[00:50:33.740 --> 00:50:36.740]   I would certainly like to go ahead and add a whole bunch of other stuff.
[00:50:36.740 --> 00:50:39.740]   So things, I don't know, add a special Moulton methods and things like that.
[00:50:39.740 --> 00:50:40.740]   I haven't done that yet.
[00:50:40.740 --> 00:50:43.740]   I just focus specifically on these aesthetic methods.
[00:50:43.740 --> 00:50:45.740]   But there's no reason why you couldn't add anything else.
[00:50:45.740 --> 00:50:47.740]   And this is important to anyone, then, you know, get in touch.
[00:50:47.740 --> 00:50:52.740]   I'm definitely interested to hear from you.
[00:50:52.740 --> 00:50:55.740]   I'll take this last question from the community.
[00:50:55.740 --> 00:50:57.740]   Can Diffrax be used for PTEs?
[00:50:57.740 --> 00:50:59.740]   I think this was already covered.
[00:50:59.740 --> 00:51:01.740]   Yeah, I think we had that one on the stop.
[00:51:01.740 --> 00:51:13.740]   I have a random question, but I think about this every time I see like this kind of things.
[00:51:13.740 --> 00:51:18.740]   And then I think of, OK, JAX has access to TPUs, right?
[00:51:18.740 --> 00:51:33.740]   I mean, if you have a system with a lot of particles, let's call them, like, is it possible to leverage having multiple machines and somehow perform this computation?
[00:51:33.740 --> 00:51:38.740]   Yeah, I mean, honestly, this is just JAX as normal at this point, right?
[00:51:38.740 --> 00:51:47.740]   If you have multiple machines, and you want to do JAX.pmap or JAX.pget and broadcast everything over them, or parallelize it, then yeah, go ahead and do that.
[00:51:47.740 --> 00:51:52.740]   And because Diffrax and Equinox, these are all pure JAX libraries, like, there's nothing magical here.
[00:51:52.740 --> 00:51:56.740]   I encourage you, anyone curious, to just go dig into the source code of these, by the way.
[00:51:56.740 --> 00:52:05.740]   My main question was, like, if you do a Pmap, but then you're doing a scan over all of this, right?
[00:52:05.740 --> 00:52:13.740]   So I'm guessing there's a lot of details of how to synchronize information such that it makes sense.
[00:52:13.740 --> 00:52:15.740]   I don't know if there's a detail here.
[00:52:15.740 --> 00:52:18.740]   I don't think so. I think it should just work.
[00:52:18.740 --> 00:52:22.740]   I mean, fundamentally, you're just running a for loop on multiple different machines.
[00:52:22.740 --> 00:52:27.740]   And then when they're all done, they all get back together and say, I got this result, I got this result.
[00:52:27.740 --> 00:52:36.740]   OK, sorry, maybe I'm thinking if, let's say, you split the state in different ones, then maybe...
[00:52:36.740 --> 00:52:40.740]   Oh, you want the Pmap inside the operation?
[00:52:40.740 --> 00:52:46.740]   Yeah, I was thinking, what if you have so many particles that you have to split?
[00:52:46.740 --> 00:52:55.740]   OK, I mean, first of all, yes, this should just work because, you know, JAX has this amazing sort of composability.
[00:52:55.740 --> 00:53:01.740]   So, yeah, I expect that to work. In practice, if you absolutely can, if it's at all possible,
[00:53:01.740 --> 00:53:06.740]   and in like 99% of use cases, it will be possible, I'd recommend putting the Pmap outside the differential equation.
[00:53:06.740 --> 00:53:12.740]   And then you can swap these two operations. And then just, you know, because you're probably talking about like a large batch size or something.
[00:53:12.740 --> 00:53:20.740]   Yeah, I was imagining, I don't know, I'm not an astrophysicist, but something like, oh, let's simulate a galaxy or...
[00:53:20.740 --> 00:53:24.740]   I don't know how heavy they are, but I was thinking, they require supercomputers.
[00:53:24.740 --> 00:53:31.740]   But what if you can do it with just like one of these TPUs that would be interesting.
[00:53:31.740 --> 00:53:36.740]   This sounds like a cool use case. I've not tried it, you know, I've not tried simulating a galaxy with the FLAX,
[00:53:36.740 --> 00:53:41.740]   but like at least in principle, this sounds possible. And if anyone gives that a try, you know, I'd love to hear from you.
[00:53:41.740 --> 00:53:44.740]   That sounds like a cool project.
[00:53:44.740 --> 00:53:50.740]   Yeah, it would be amazing if somebody comes up with it.
[00:53:50.740 --> 00:53:56.740]   Awesome. We still have a few minutes. So, Christian, if you have any other questions you want to ask?
[00:53:56.740 --> 00:54:02.740]   Yeah, well, it's more of a, I don't know, like a general question.
[00:54:02.740 --> 00:54:12.740]   And it seems like the neural, like, how do you call them? Neural...
[00:54:12.740 --> 00:54:13.740]   Differential equation?
[00:54:13.740 --> 00:54:22.740]   Yeah, neural differential equations, they've been around for a while, right? And I think your thesis has a little bit to do with that.
[00:54:22.740 --> 00:54:26.740]   Yeah, my thesis is a whole thing about that.
[00:54:26.740 --> 00:54:39.740]   Like, I don't know if you can like maybe give a hint of like what is happening in the field and if you can start using the FLEX for that kind of applications.
[00:54:39.740 --> 00:54:48.740]   Yeah, I mean, so the clarity here, this is no longer like a JAXS problem so much as it's, you know, like an academia research sort of discussion.
[00:54:48.740 --> 00:54:59.740]   But I think a lot of what we're starting to see now is people just using neural differential equations and just using the theory that we and everyone else built up over these research papers.
[00:54:59.740 --> 00:55:06.740]   And so, you know, you're seeing people being like, oh, you know, I went and tackled some problem trying to model the internal state of a battery using a neural ODE.
[00:55:06.740 --> 00:55:10.740]   Or I tried to go and model some financial markets using a neural SDE.
[00:55:10.740 --> 00:55:20.740]   And I think what we're starting to see now is a lot of like the dividends of this kind of abstract research now actually starting to get used in practice, which I think is very cool.
[00:55:20.740 --> 00:55:24.740]   Conversely, there's no shortage of open questions in the field of neural differential equations.
[00:55:24.740 --> 00:55:32.740]   And if anyone's really curious to know more about those or wants to work on any of them, then, you know, I'd rather say, please ping me. I'm always happy to hear from people.
[00:55:32.740 --> 00:55:39.740]   But that's getting into like quite specialized stuff where I start talking about like custom numerical solvers, reversible solvers, hyper solvers, da da da da.
[00:55:39.740 --> 00:55:42.740]   That's a very niche discussion.
[00:55:42.740 --> 00:55:58.740]   In Diffrax, I'm guessing what you would do is that your like the functions you are coding instead of just being something simple like minus Y would be like a neural network.
[00:55:58.740 --> 00:56:06.740]   Yeah, yeah, exactly. Exactly. And it's the kind of thing where like if we go up to that linear operation. Yeah, here we go.
[00:56:06.740 --> 00:56:09.740]   Is there like an abstraction for this? Is like a specific one?
[00:56:09.740 --> 00:56:12.740]   No, there's no specific abstraction. You just need to match an API.
[00:56:12.740 --> 00:56:19.740]   So in this case, let's suppose you wanted to use this learned linear layer as your vector field.
[00:56:19.740 --> 00:56:25.740]   That's totally fine. You just need to change your API here from accepting X to accepting T comma Y comma args.
[00:56:25.740 --> 00:56:37.740]   So you need to accept T, the time of your differential gradient solve, Y, the state at that point, time T, and then any args, any like extra information that's held constant throughout the entire solve.
[00:56:37.740 --> 00:56:41.740]   So you change your call method to accept that as an argument and to return an appropriate value.
[00:56:41.740 --> 00:56:45.740]   And then, for example, and then the rest of this code is completely unchanged.
[00:56:45.740 --> 00:56:49.740]   And then I could use a learned linear layer as my vector field.
[00:56:49.740 --> 00:56:59.740]   That's awesome. Like I feel that for certain applications, it's kind of an alternative for recurrent layers, right?
[00:56:59.740 --> 00:57:04.740]   Yeah, absolutely. Absolutely. Yeah, no, I mean, I'm not going to any of this at all.
[00:57:04.740 --> 00:57:11.740]   But no, so like the continuous limit of a recurrent neural network is in fact a neural controlled differential equation.
[00:57:11.740 --> 00:57:14.740]   So I've been discussing these controlled differential equations earlier.
[00:57:14.740 --> 00:57:20.740]   Well, actually, if you put the word neural in front, then actually, yeah, this is also, in fact, a continuous limit of a recurrent neural network.
[00:57:20.740 --> 00:57:24.740]   So, yes, absolutely. There are links to recurrent neural networks.
[00:57:24.740 --> 00:57:34.740]   And if anyone's curious to know more about that, then go ahead and go and have a look at my thesis, which really discusses all of this in ludicrous amounts of detail.
[00:57:34.740 --> 00:57:45.740]   Just like a very, I don't know, like I'm guessing because I mean, recurrent layers and especially LSTMs, they have been around for a while.
[00:57:45.740 --> 00:57:55.740]   But you think you think in practice, people should try to use more like start going towards this neural differential.
[00:57:55.740 --> 00:57:57.740]   Yeah, depends a lot on the problem, honestly.
[00:57:57.740 --> 00:58:04.740]   I mean, this is something where if you are using a recurrent neural network, then somehow secretly you are already using a differential equation.
[00:58:04.740 --> 00:58:11.740]   And so, you know, I don't really care if you use the explicitly discretized R and N or the continuous time differential equation of the defect solver.
[00:58:11.740 --> 00:58:14.740]   There are definitely use cases where each one is more appropriate.
[00:58:14.740 --> 00:58:21.740]   So to answer your question of when would you use a defect differential equation and explicitly use a differential equation,
[00:58:21.740 --> 00:58:26.740]   it's when you have things like irregularly sampled data in a time series, like missing data in a time series.
[00:58:26.740 --> 00:58:32.740]   Or alternatively, maybe very, very densely sampled data in a time series, where if you do that with an R and N,
[00:58:32.740 --> 00:58:35.740]   you'd have to sequence each point sequentially and that would take forever.
[00:58:35.740 --> 00:58:39.740]   But if you lean on the mathematics of differential equations, you can use things like an adaptive step size controller.
[00:58:39.740 --> 00:58:42.740]   That was that PID controller I mentioned earlier.
[00:58:42.740 --> 00:58:47.740]   I forgot that. And then we can now use something like an adaptive step size controller.
[00:58:47.740 --> 00:58:54.740]   Here we go, this thing here. And then you can adaptively take in the right amount of data for each step.
[00:58:54.740 --> 00:59:01.740]   So you see that the continuous time differential equation approach is particularly suitable for these complicated use cases.
[00:59:01.740 --> 00:59:07.740]   And in every other use case, it's just exactly the same as an R and N.
[00:59:07.740 --> 00:59:13.740]   My last question is that this topic seems to touch so many areas.
[00:59:13.740 --> 00:59:23.740]   I think I asked you this once, but lately diffusion methods have become very popular.
[00:59:23.740 --> 00:59:35.740]   It seems that again, there is somehow a link to this. Can you talk about this connection?
[00:59:35.740 --> 00:59:38.740]   Yeah, so absolutely there are connections.
[00:59:38.740 --> 00:59:43.740]   So in terms of the diffracts, equinox, scientific computing end of things,
[00:59:43.740 --> 00:59:48.740]   then go ahead and have a look at the documentation for equinox.
[00:59:48.740 --> 00:59:53.740]   And that actually includes an example of trained score based diffusion just on toy example, just on MNIST.
[00:59:53.740 --> 00:59:59.740]   It's just quite a small example. But it trains a toy example on MNIST, other score based diffusion,
[00:59:59.740 --> 01:00:03.740]   and it solves the differential equations involved using diffracts.
[01:00:03.740 --> 01:00:06.740]   So on that end of things, that is there.
[01:00:06.740 --> 01:00:11.740]   And actually also just as a quick side note, as a quick advert for anyone interested in working with score based diffusions,
[01:00:11.740 --> 01:00:19.740]   then the example in the documentation for equinox is the shortest, simplest, cleanest example of score based diffusions that I know of anywhere.
[01:00:19.740 --> 01:00:26.740]   So that may be helpful for some. And then in terms of the sort of like more theoretical end of things.
[01:00:26.740 --> 01:00:35.740]   Yeah. So score based diffusions are essentially an alternative training method of what we refer to in the neural differential equation community as a continuous normalizing flow.
[01:00:35.740 --> 01:00:47.740]   These two things are somehow the same object. And the real innovation of the score based diffusion compared to what was already happening was that it was a training method that worked in parallel over time.
[01:00:47.740 --> 01:00:50.740]   I don't know how much sense this is going to make to people without the appropriate background,
[01:00:50.740 --> 01:00:55.740]   but we have this way of training these things called continuous normalizing flows that involves solving a differential equation,
[01:00:55.740 --> 01:00:59.740]   which was moderately expensive. And then a score based diffusion was this really,
[01:00:59.740 --> 01:01:06.740]   really smart way of like skipping the solve and then cutting out an expensive component in the training.
[01:01:06.740 --> 01:01:10.740]   So, yeah, absolutely. There were really, really cool things there.
[01:01:10.740 --> 01:01:25.740]   Yeah, that is really cool. I know I hope that eventually the community of diffusion, I'm guessing, because generative models are kind of state of the art, right?
[01:01:25.740 --> 01:01:31.740]   So it would be amazing if they start like binding a little bit together.
[01:01:31.740 --> 01:01:41.740]   I played around a bit and what you showed with the Brownian motion, it makes like a lot of sense.
[01:01:41.740 --> 01:01:49.740]   The only thing I wonder, because I remember there's kind of these beta schedules in some of the literature on diffusion models.
[01:01:49.740 --> 01:01:56.740]   I don't know if it would be possible to somehow include that here with diffrex.
[01:01:56.740 --> 01:01:59.740]   Yeah, I mean, this is essentially what you've just described as a hyper parameter.
[01:01:59.740 --> 01:02:03.740]   So, yeah, go ahead and modify your hyper parameters however you wish.
[01:02:03.740 --> 01:02:07.740]   And this, I think, does appear in the example in the documentation for Epinox.
[01:02:07.740 --> 01:02:12.740]   There is a beta described there, which you could change if you wanted to.
[01:02:12.740 --> 01:02:15.740]   And then actually, I suppose, sort of in respect to what you were saying just at the start there,
[01:02:15.740 --> 01:02:20.740]   whether there's like other connections that are yet to be drawn. Yes, I reckon they probably are.
[01:02:20.740 --> 01:02:23.740]   We've done a whole bunch of work on like neural stochastic differential equations.
[01:02:23.740 --> 01:02:26.740]   We've done a whole bunch of work on continuous normalizing flows.
[01:02:26.740 --> 01:02:28.740]   There's now this whole line of work on score based diffusions.
[01:02:28.740 --> 01:02:32.740]   I think there's going to be all kinds of interesting papers yet to be written combining these approaches.
[01:02:32.740 --> 01:02:37.740]   That just hasn't happened yet. And I think it's going to be very, very exciting to see what happens.
[01:02:37.740 --> 01:02:40.740]   See what the future brings. Amazing.
[01:02:40.740 --> 01:02:46.740]   Yeah, so much stuff related to this concept.
[01:02:46.740 --> 01:02:50.740]   I don't know. Are there other questions, Samnian?
[01:02:50.740 --> 01:02:55.740]   I think we're out of time, so I'll try to wrap up.
[01:02:55.740 --> 01:02:59.740]   I'll quickly again mention Patrick's Twitter handle.
[01:02:59.740 --> 01:03:07.740]   So make sure everyone, you follow him there and check out the GitHub link for all of the stuff we shared today.
[01:03:07.740 --> 01:03:10.740]   If you signed up, we'll be sending an email out where you can find all of this.
[01:03:10.740 --> 01:03:14.740]   So you don't have to worry about anything. Also follow Christian on Twitter.
[01:03:14.740 --> 01:03:23.740]   His handle is C_carcia88. Thank you so much, Patrick and Christian again for another awesome Jax talk.
[01:03:23.740 --> 01:03:30.740]   I understand a few words here and there all the time with these, but it's always I go back and watch and learn so much.
[01:03:30.740 --> 01:03:34.740]   So thanks. Thanks for your time. No worries. Thank you a lot for having me.
[01:03:34.740 --> 01:03:37.740]   Awesome, Patrick. Thanks a lot. It was super interesting.
[01:03:37.740 --> 01:03:47.740]   [BLANK_AUDIO]

