
[00:00:00.000 --> 00:00:07.640]   Next up is from Brian. How does TikTok seem to know so much about me and tailor's contacts so accurately to my interests and behavior?
[00:00:07.640 --> 00:00:27.460]   That's a good question because I think it connects to our deep dive where we talked about content like TikTok being ultra processed that unlike other types of content like television, streaming or games on your phone, it has a way of being hyper palatable and it has a way of pushing your buttons in ways that's really psychologically unhealthy.
[00:00:27.460 --> 00:00:39.460]   So I think it's worth looking into because I actually think I know something about this. I mean, I did some research on this back when there was an internal document that got leaked and this was sort of making its rounds and I read a bunch of accounts of this document.
[00:00:39.460 --> 00:00:48.820]   I have like a generic sense of the techniques used by a TikTok algorithm. It's probably worth briefly going over because I think people have this wrong.
[00:00:48.820 --> 00:00:53.940]   And when you have it wrong, it changes the way you think about this world and what to do about the harms of this world.
[00:00:54.440 --> 00:01:01.900]   I looked up a quote here. I think this sort of establishes the way that we kind of get this wrong.
[00:01:01.900 --> 00:01:10.880]   So here's a quote about TikTok that comes from the founder of an organization called Algo Transparency, which I think kind of gives the wrong idea, but let me read this and then I'll give you the right idea.
[00:01:11.360 --> 00:01:40.960]   Technically, all of that is true, but I think the way that we talk about algorithms here gives us an incomplete understanding of how these things actually work.
[00:01:40.960 --> 00:01:48.480]   I think when we see those type of description of algorithms, what we really imagine is sort of like someone making a list of descriptive information about a user.
[00:01:48.640 --> 00:02:01.900]   Oh, we've learned that we learned that he's into this, not into that. We imagine an algorithm sort of like looking at these semantically rich descriptions of things you're into or not into and, and deciding according to some sort of complicated rules that someone programmed in.
[00:02:02.160 --> 00:02:15.600]   All right. Well, why don't we then take this and mix it with that? And we have some good stuff of that over here. And we're trying to emphasize this type of content. Oh, and I've analyzed, I think this type of content is, is, is, it feels like people like it.
[00:02:15.840 --> 00:02:29.680]   And so it's like a network scheduler. Let's show him more of that. Like, so I think that, that image of an algorithm, it's like a network scheduler looking at a board full of pieces of information on index cards and making a decision. Like, well, what are we going to schedule next? I think this is really going to catch his attention.
[00:02:30.480 --> 00:02:51.560]   We have that idea, which is sort of personified. We imagine like there's a human involved and it gives us the sense that we can turn these various knobs and say things like, Hey, adjust your algorithm to not recommend stuff that is going to be so addictive or adjust your algorithm to like turn down the, the, the outrage meter from like a 10 to a nine.
[00:02:52.280 --> 00:02:59.100]   When we think about the algorithm is like a person with all this information, making scheduling decisions. We think we have all this control over it.
[00:02:59.100 --> 00:03:05.960]   That's not really how this works. I'm going to try here to draw a picture for those who are watching instead of just listing and God help us all for me doing this.
[00:03:05.960 --> 00:03:12.600]   But I want to give you a better sense of how the general class of algorithms that we think powers TikTok actually works.
[00:03:12.600 --> 00:03:22.100]   So you can imagine what we do is that every time you watch a video, there's a piece of information that's gathered, but what's gathered is,
[00:03:22.100 --> 00:03:33.840]   a description of the video. This description though is going to be a bunch of numbers. So there's going to be, you could think of it like a, a lot of different categories you could use to apply to a video.
[00:03:33.840 --> 00:03:40.320]   You know, uh, cats could be a category. If it has a lot of cats, that number is high or low or something like this. Right.
[00:03:40.320 --> 00:03:46.740]   And so you have different numbers in each of these categories sort of gives you a list of numbers that describes the video.
[00:03:46.740 --> 00:03:59.140]   So if we had, for example, just two numbers, we could imagine having a flat surface, like a two dimensional plane where you could put a, each spot in here.
[00:03:59.140 --> 00:04:07.720]   Like if we put a spot right here is associated with, uh, two numbers, like with that number and with that number. Right.
[00:04:07.980 --> 00:04:18.540]   So you can imagine what we do is every time you watch a video, we can put a little dot on here that represents its description and we can make that dot bigger or smaller depending.
[00:04:18.540 --> 00:04:24.840]   And this seems to be what's most crucial for a TikTok, how long you watch it, like what percentage of the video you actually watched.
[00:04:24.840 --> 00:04:30.700]   So like maybe you watched a video over here real briefly, small dot over here though, there's a video you watched all the way through.
[00:04:30.700 --> 00:04:39.440]   There's like a big dot over there and maybe there's a medium dot and you kind of sample some videos over here, right?
[00:04:39.440 --> 00:04:47.120]   And over time, we're kind of just giving you videos to watch and we see how long are you watching each of them.
[00:04:47.120 --> 00:04:49.460]   So now we have a bunch of these numbers. We have this plot.
[00:04:49.460 --> 00:04:54.180]   Technically it's going to be a sort of weighted multidimensional scatter plot.
[00:04:55.440 --> 00:05:00.640]   And now when we want to show you a new video, we being TikTok, the algorithm is super mathematical.
[00:05:00.640 --> 00:05:12.540]   It says, okay, I'm just going to randomly choose a video in this spot of possible videos to show you, but I'm not going to select from every area equally likely.
[00:05:12.540 --> 00:05:19.720]   The more sort of green real estate there is in a certain place, like the more I'm going to be pulled in that direction.
[00:05:20.180 --> 00:05:27.920]   It's like I might select something over here, but I'm way more likely to select something over here because there's so many, not only just so many other dots, but so many big dots.
[00:05:27.920 --> 00:05:31.820]   There's a lot of watch time over in this space of the possible videos.
[00:05:31.820 --> 00:05:37.080]   And what we're implicitly doing here is kind of creating a region.
[00:05:37.080 --> 00:05:40.960]   These are all going to be probably roughly similarly types of videos.
[00:05:40.960 --> 00:05:43.340]   Now again, my algorithm is doing something very simple.
[00:05:43.340 --> 00:05:45.360]   It's just randomly choose a thing, but wait your choice.
[00:05:45.360 --> 00:05:50.020]   You know, the more stuff there is nearby it, more mass you want to put on it.
[00:05:50.020 --> 00:05:52.200]   And there's like a lot of probabilistic ways you could do that.
[00:05:52.200 --> 00:05:59.400]   So I'm going to start selecting more videos near stuff you already like, but like occasionally, you know, you roll a dice enough time.
[00:05:59.400 --> 00:06:01.120]   Sometimes you're going to roll six, three times in a row.
[00:06:01.120 --> 00:06:04.740]   Occasionally I'll pull some stuff over here as well.
[00:06:04.740 --> 00:06:08.460]   And maybe what I'll do, and we think that the TikTok algorithm does this on purpose.
[00:06:08.460 --> 00:06:13.000]   It will on a semi-regular basis say, forget, forget the weights.
[00:06:13.000 --> 00:06:18.480]   And let me just like purely draw randomly just to make sure there's not something we haven't tried yet that you really like.
[00:06:18.480 --> 00:06:27.680]   And then like maybe over here, there is like a whole bunch of Cal network memes and immediately these start getting watched well.
[00:06:27.680 --> 00:06:31.460]   And now it's like, you're getting a lot of Cal network memes.
[00:06:31.460 --> 00:06:37.380]   And then like a lot of the times of the stuff over here and occasionally we'll sample other types of things.
[00:06:38.640 --> 00:06:40.140]   That's what's happening underneath the covers.
[00:06:40.140 --> 00:06:41.760]   It's not just two numbers though.
[00:06:41.760 --> 00:06:44.000]   It'll be like a hundred numbers or a thousand numbers.
[00:06:44.000 --> 00:06:47.200]   So this is each of these spots is in a thousand dimensional space.
[00:06:47.200 --> 00:06:51.100]   So the math is different, but it's the same idea.
[00:06:52.320 --> 00:06:54.740]   What's key about this is the algorithm is stupid.
[00:06:54.740 --> 00:07:01.380]   The algorithm, there is no complicated code in there that humans wrote that's like, how do I balance this versus that?
[00:07:01.380 --> 00:07:06.920]   It doesn't label things necessarily in the way you think, like, is this outrageous or not?
[00:07:06.920 --> 00:07:10.480]   I mean, it does categorization and try to get rid of stuff for safety purposes.
[00:07:10.480 --> 00:07:13.480]   But like it really just mainly treats stuff like these numbers.
[00:07:13.720 --> 00:07:24.280]   And it's just blindly doing this simple repetitive loop, pick another video probabilistically from the space of videos, weight your choice towards stuff that's already been watched.
[00:07:24.280 --> 00:07:30.840]   So you're more likely to choose something near a bunch of other stuff that's been watched a while than other stuff on a regular basis.
[00:07:30.840 --> 00:07:36.040]   They'll randomly just choose something completely different to make sure that we're exploring other types of videos.
[00:07:36.040 --> 00:07:43.540]   They might like this simple loop run tight enough for the only input it's getting from you is how long did you watch each video?
[00:07:43.540 --> 00:07:47.040]   And how do we automatically categorize the video with some sort of neural net?
[00:07:47.040 --> 00:07:55.400]   That is enough to like within 30 or 40 minutes be eerily accurate in showing you stuff that you really like.
[00:07:55.400 --> 00:08:03.460]   Without there ever having to be anywhere like an English language list of like topics and here's what this person likes and complicated algorithms.
[00:08:03.460 --> 00:08:11.640]   So it's a very simple weighted random sampling algorithm with a little bit of exploration or exploitation types of logic.
[00:08:11.760 --> 00:08:14.380]   It's called multi-armed band type stochastic optimization.
[00:08:14.380 --> 00:08:15.300]   You don't have to worry about that.
[00:08:15.300 --> 00:08:17.040]   But like occasionally sampling other stuff.
[00:08:17.040 --> 00:08:22.860]   It's a very simple algorithm run again and again and again that has an eerily effective complex effect.
[00:08:22.860 --> 00:08:29.500]   So anyways, I think that's interesting to know because it tells us that you can't control these things as much as you think.
[00:08:29.500 --> 00:08:39.600]   You can filter like you can say, hey, before we even recommend something, we can like filter is just like really inappropriate content and just like let's not recommend that at all.
[00:08:39.600 --> 00:08:49.880]   You could like manually put shadow bands on things like, okay, anything with this category, I'm going to type into my system, like minimize its weight.
[00:08:49.880 --> 00:08:51.760]   So we're much less likely to show other stuff like that.
[00:08:51.800 --> 00:08:56.940]   You can do that, but there is, for the most part, you don't have knobs to turn, right?
[00:08:56.940 --> 00:08:59.100]   You just say sample stuff near stuff.
[00:08:59.100 --> 00:09:01.320]   They like TikTok works really well.
[00:09:01.320 --> 00:09:02.860]   You could keep that on.
[00:09:02.860 --> 00:09:04.220]   You can turn it off and that's it.
[00:09:04.220 --> 00:09:07.040]   Like we do not have the control over these algorithms that people think.
[00:09:07.040 --> 00:09:08.980]   So anyways, I think that's an important insight that I want to give.
[00:09:08.980 --> 00:09:12.620]   There is a lot of these sort of recommendation algorithms.
[00:09:12.620 --> 00:09:18.940]   The math might be complicated, but the algorithmic logic is not nearly as adjustable as you might think.
[00:09:18.940 --> 00:09:28.400]   A simple routine done again and again with more and more data can work eerily well, and it's pretty hard to have any sort of nuanced control over it.
[00:09:28.400 --> 00:09:30.380]   So there you go.
[00:09:30.380 --> 00:09:34.040]   I still have TikTok on my phone, Jesse, from that New Yorker article I wrote.
[00:09:34.040 --> 00:09:35.120]   Have you been on it?
[00:09:35.120 --> 00:09:36.120]   No, I have no interest.
[00:09:36.120 --> 00:09:36.900]   It's so weird.
[00:09:36.900 --> 00:09:38.100]   Here, look, I'm loading it up right now.
[00:09:38.100 --> 00:09:39.220]   It's such a weird app.
[00:09:39.220 --> 00:09:44.480]   If you're not native to this, if you come to it just for like an article like I did, let's see if this works.
[00:09:44.480 --> 00:09:46.420]   All right, I'm loading up TikTok on my phone.
[00:09:46.420 --> 00:09:48.020]   I haven't been on here since January.
[00:09:48.020 --> 00:09:51.280]   Don't allow.
[00:09:51.280 --> 00:09:54.760]   There's a woman singing.
[00:09:54.760 --> 00:10:07.960]   They're like taking, a shirtless guy is taking blankets off of her, and now she's in like a fancy dress, and there's confetti, and the guy is blowing,
[00:10:07.960 --> 00:10:08.780]   air on her.
[00:10:08.780 --> 00:10:10.700]   Jesse, what are we doing here?
[00:10:10.700 --> 00:10:19.680]   When you see your wife and three daughters see you OTW home, and the kids are giving the middle finger.
[00:10:19.680 --> 00:10:21.180]   What are we doing?
[00:10:21.180 --> 00:10:22.120]   Is this civilization?
[00:10:24.460 --> 00:10:25.360]   Turn long videos.
[00:10:25.360 --> 00:10:26.020]   Oh, my God.
[00:10:26.020 --> 00:10:28.600]   Is this video appropriate for TikTok?
[00:10:28.600 --> 00:10:29.600]   Oh, they're getting data.
[00:10:29.600 --> 00:10:31.680]   They're getting data.
[00:10:31.680 --> 00:10:33.840]   They're forcing me to, all right, one more.
[00:10:33.840 --> 00:10:38.980]   When your teacher used to call home, someone walks into a room.
[00:10:40.140 --> 00:10:48.800]   Oh, it's a skit about like, a guy putting on lots of pants because his dad is going to hit him with a belt for being in trouble at school.
[00:10:48.800 --> 00:10:49.720]   It's a comedy skit.
[00:10:49.720 --> 00:10:50.380]   All right, come on.
[00:10:50.380 --> 00:10:54.060]   Anyways, that's TikTok.
[00:10:54.060 --> 00:10:55.560]   All right, I don't understand that, Jesse.
[00:10:55.560 --> 00:10:58.860]   But if I watch that long enough, see, that's random because I don't use it.
[00:10:58.860 --> 00:11:02.220]   So that's just randomly sampling from like all videos.
[00:11:02.220 --> 00:11:08.060]   But I'm telling you, if I watch this for like, I don't know, 20 minutes, it's all going to, I don't know what it would be.
[00:11:08.060 --> 00:11:11.020]   It'd be like a lot of like Washington Nationals content, I guess.
[00:11:11.020 --> 00:11:12.120]   I don't know what else would be on there.
[00:11:12.120 --> 00:11:17.580]   It'd be like Washington Nationals content and Cal Network memes, I guess.
[00:11:17.580 --> 00:11:18.820]   I don't know.
[00:11:18.820 --> 00:11:21.620]   But that's how that works.
[00:11:21.620 --> 00:11:23.820]   These algorithms just like, it's like a virus.
[00:11:23.820 --> 00:11:30.140]   They're simple, but you let that process run long enough and fast enough, it can cause like complicated, terrible effects.
[00:11:30.140 --> 00:11:31.760]   So there we go.
[00:11:31.760 --> 00:11:34.120]   Tick tock.

