
[00:00:00.000 --> 00:00:03.180]   The following is a conversation with Melanie Mitchell.
[00:00:03.180 --> 00:00:04.860]   She's a professor of computer science
[00:00:04.860 --> 00:00:06.720]   at Portland State University
[00:00:06.720 --> 00:00:10.020]   and an external professor at Santa Fe Institute.
[00:00:10.020 --> 00:00:13.000]   She has worked on and written about artificial intelligence
[00:00:13.000 --> 00:00:14.940]   from fascinating perspectives,
[00:00:14.940 --> 00:00:18.420]   including adaptive complex systems, genetic algorithms,
[00:00:18.420 --> 00:00:20.980]   and the copycat cognitive architecture,
[00:00:20.980 --> 00:00:23.340]   which places the process of analogy making
[00:00:23.340 --> 00:00:26.300]   at the core of human cognition.
[00:00:26.300 --> 00:00:28.500]   From her doctoral work with her advisors,
[00:00:28.500 --> 00:00:30.980]   Douglas Hofstadter and John Holland,
[00:00:30.980 --> 00:00:34.180]   to today, she has contributed a lot of important ideas
[00:00:34.180 --> 00:00:36.940]   to the field of AI, including her recent book,
[00:00:36.940 --> 00:00:39.860]   simply called "Artificial Intelligence,
[00:00:39.860 --> 00:00:42.780]   "A Guide for Thinking Humans."
[00:00:42.780 --> 00:00:45.860]   This is the Artificial Intelligence Podcast.
[00:00:45.860 --> 00:00:48.260]   If you enjoy it, subscribe on YouTube,
[00:00:48.260 --> 00:00:50.260]   give it five stars on Apple Podcast,
[00:00:50.260 --> 00:00:51.700]   support it on Patreon,
[00:00:51.700 --> 00:00:53.780]   or simply connect with me on Twitter
[00:00:53.780 --> 00:00:58.100]   at Lex Friedman, spelled F-R-I-D-M-A-N.
[00:00:58.100 --> 00:01:01.580]   I recently started doing ads at the end of the introduction.
[00:01:01.580 --> 00:01:04.340]   I'll do one or two minutes after introducing the episode
[00:01:04.340 --> 00:01:05.900]   and never any ads in the middle
[00:01:05.900 --> 00:01:08.340]   that can break the flow of the conversation.
[00:01:08.340 --> 00:01:09.580]   I hope that works for you
[00:01:09.580 --> 00:01:12.140]   and doesn't hurt the listening experience.
[00:01:12.140 --> 00:01:14.900]   I provide timestamps for the start of the conversation,
[00:01:14.900 --> 00:01:17.060]   but it helps if you listen to the ad
[00:01:17.060 --> 00:01:18.500]   and support this podcast
[00:01:18.500 --> 00:01:21.260]   by trying out the product or service being advertised.
[00:01:21.260 --> 00:01:24.940]   This show is presented by Cash App,
[00:01:24.940 --> 00:01:27.500]   the number one finance app in the App Store.
[00:01:27.500 --> 00:01:30.260]   I personally use Cash App to send money to friends,
[00:01:30.260 --> 00:01:32.580]   but you can also use it to buy, sell,
[00:01:32.580 --> 00:01:35.020]   and deposit Bitcoin in just seconds.
[00:01:35.020 --> 00:01:38.180]   Cash App also has a new investing feature.
[00:01:38.180 --> 00:01:41.140]   You can buy fractions of a stock, say $1 worth,
[00:01:41.140 --> 00:01:43.380]   no matter what the stock price is.
[00:01:43.380 --> 00:01:46.180]   Brokerage services are provided by Cash App Investing,
[00:01:46.180 --> 00:01:49.940]   a subsidiary of Square and a member of SIPC.
[00:01:49.940 --> 00:01:51.860]   I'm excited to be working with Cash App
[00:01:51.860 --> 00:01:55.260]   to support one of my favorite organizations called FIRST,
[00:01:55.260 --> 00:01:58.900]   best known for their FIRST Robotics and LEGO competitions.
[00:01:58.900 --> 00:02:02.620]   They educate and inspire hundreds of thousands of students
[00:02:02.620 --> 00:02:04.420]   in over 110 countries
[00:02:04.420 --> 00:02:06.940]   and have a perfect rating on Charity Navigator,
[00:02:06.940 --> 00:02:09.020]   which means that donated money is used
[00:02:09.020 --> 00:02:11.660]   to maximum effectiveness.
[00:02:11.660 --> 00:02:14.900]   When you get Cash App from the App Store or Google Play
[00:02:14.900 --> 00:02:18.540]   and use code LEXPODCAST, you'll get $10,
[00:02:18.540 --> 00:02:21.340]   and Cash App will also donate $10 to FIRST,
[00:02:21.340 --> 00:02:23.220]   which again is an organization
[00:02:23.220 --> 00:02:26.100]   that I've personally seen inspire girls and boys
[00:02:26.100 --> 00:02:28.980]   to dream of engineering a better world.
[00:02:28.980 --> 00:02:32.820]   And now, here's my conversation with Melanie Mitchell.
[00:02:32.820 --> 00:02:36.860]   The name of your new book is "Artificial Intelligence,"
[00:02:36.860 --> 00:02:39.700]   subtitle, "A Guide for Thinking Humans."
[00:02:39.700 --> 00:02:42.960]   The name of this podcast is "Artificial Intelligence."
[00:02:42.960 --> 00:02:44.100]   So let me take a step back
[00:02:44.100 --> 00:02:46.940]   and ask the old Shakespeare question about roses.
[00:02:46.940 --> 00:02:51.100]   And what do you think of the term artificial intelligence
[00:02:51.100 --> 00:02:55.500]   for our big and complicated and interesting field?
[00:02:55.500 --> 00:02:57.900]   - I'm not crazy about the term. (laughs)
[00:02:57.900 --> 00:02:59.980]   I think it has a few problems
[00:02:59.980 --> 00:03:04.380]   because it means so many different things
[00:03:04.380 --> 00:03:05.620]   to different people.
[00:03:05.620 --> 00:03:07.460]   And intelligence is one of those words
[00:03:07.460 --> 00:03:10.060]   that isn't very clearly defined either.
[00:03:10.060 --> 00:03:14.420]   There's so many different kinds of intelligence,
[00:03:14.420 --> 00:03:18.900]   degrees of intelligence, approaches to intelligence.
[00:03:18.900 --> 00:03:21.220]   John McCarthy was the one who came up
[00:03:21.220 --> 00:03:23.220]   with the term artificial intelligence.
[00:03:23.220 --> 00:03:24.380]   And from what I read,
[00:03:24.380 --> 00:03:28.780]   he called it that to differentiate it from cybernetics,
[00:03:28.780 --> 00:03:33.700]   which was another related movement at the time.
[00:03:33.700 --> 00:03:38.340]   And he later regretted calling it artificial intelligence.
[00:03:38.340 --> 00:03:40.720]   Herbert Simon was pushing
[00:03:40.720 --> 00:03:43.860]   for calling it complex information processing,
[00:03:43.860 --> 00:03:47.080]   which got nixed,
[00:03:47.080 --> 00:03:52.080]   but probably is equally vague, I guess.
[00:03:52.080 --> 00:03:55.360]   - Is it the intelligence or the artificial
[00:03:55.360 --> 00:03:58.720]   in terms of words that's most problematic, would you say?
[00:03:58.720 --> 00:04:01.040]   - Yeah, I think it's a little of both.
[00:04:01.040 --> 00:04:02.960]   But it has some good sides
[00:04:02.960 --> 00:04:07.060]   because I personally was attracted to the field
[00:04:07.060 --> 00:04:11.280]   because I was interested in phenomenon of intelligence.
[00:04:11.280 --> 00:04:13.620]   And if it was called complex information processing,
[00:04:13.620 --> 00:04:16.200]   maybe I'd be doing something wholly different now.
[00:04:16.200 --> 00:04:17.200]   - What do you think of,
[00:04:17.200 --> 00:04:20.320]   I've heard that term used cognitive systems, for example.
[00:04:20.320 --> 00:04:22.760]   So using cognitive.
[00:04:22.760 --> 00:04:27.760]   - Yeah, I mean, cognitive has certain associations with it.
[00:04:27.760 --> 00:04:29.920]   And people like to separate things
[00:04:29.920 --> 00:04:32.120]   like cognition and perception,
[00:04:32.120 --> 00:04:33.940]   which I don't actually think are separate.
[00:04:33.940 --> 00:04:36.800]   But often people talk about cognition
[00:04:36.800 --> 00:04:41.400]   as being different from other aspects of intelligence.
[00:04:41.400 --> 00:04:42.720]   It's sort of higher level.
[00:04:42.720 --> 00:04:44.640]   - So to you, cognition is this broad,
[00:04:44.640 --> 00:04:47.920]   beautiful mess of things that encompasses the whole thing.
[00:04:47.920 --> 00:04:48.760]   Memory, perception.
[00:04:48.760 --> 00:04:53.040]   - Yeah, I think it's hard to draw lines like that.
[00:04:53.040 --> 00:04:56.640]   When I was coming out of grad school in 1990,
[00:04:56.640 --> 00:04:58.360]   which is when I graduated,
[00:04:58.360 --> 00:05:00.680]   that was during one of the AI winters.
[00:05:00.680 --> 00:05:05.160]   And I was advised to not put AI,
[00:05:05.160 --> 00:05:06.780]   artificial intelligence on my CV,
[00:05:06.780 --> 00:05:09.220]   but instead call it intelligence systems.
[00:05:09.220 --> 00:05:13.420]   So that was kind of a euphemism, I guess.
[00:05:13.900 --> 00:05:18.900]   - What about, to stick briefly on terms and words,
[00:05:18.900 --> 00:05:24.100]   the idea of artificial general intelligence,
[00:05:24.100 --> 00:05:29.100]   or like Jan LeCun prefers human level intelligence.
[00:05:29.100 --> 00:05:32.460]   Sort of starting to talk about ideas
[00:05:32.460 --> 00:05:37.740]   that achieve higher and higher levels of intelligence.
[00:05:37.740 --> 00:05:40.860]   And somehow artificial intelligence seems to be
[00:05:40.860 --> 00:05:43.180]   a term used more for the narrow,
[00:05:43.180 --> 00:05:45.340]   very specific applications of AI.
[00:05:45.340 --> 00:05:50.340]   And sort of, what set of terms appeal to you
[00:05:50.340 --> 00:05:56.020]   to describe the thing that perhaps we strive to create?
[00:05:56.020 --> 00:05:57.420]   - People have been struggling with this
[00:05:57.420 --> 00:05:59.220]   for the whole history of the field.
[00:05:59.220 --> 00:06:03.420]   And defining exactly what it is that we're talking about.
[00:06:03.420 --> 00:06:05.620]   You know, John Searle had this distinction
[00:06:05.620 --> 00:06:08.500]   between strong AI and weak AI.
[00:06:08.500 --> 00:06:10.460]   And weak AI could be general AI,
[00:06:10.460 --> 00:06:14.580]   but his idea was strong AI was the view
[00:06:14.580 --> 00:06:17.180]   that a machine is actually thinking.
[00:06:17.180 --> 00:06:22.580]   That as opposed to simulating thinking
[00:06:22.580 --> 00:06:27.580]   or carrying out processes that we would call intelligent.
[00:06:27.580 --> 00:06:34.460]   - At a high level, if you look at the founding
[00:06:34.460 --> 00:06:37.280]   of the field of McCarthy and Searle and so on,
[00:06:38.940 --> 00:06:43.940]   are we closer to having a better sense of that line
[00:06:43.940 --> 00:06:49.380]   between narrow, weak AI and strong AI?
[00:06:49.380 --> 00:06:55.420]   - Yes, I think we're closer to having a better idea
[00:06:55.420 --> 00:06:57.140]   of what that line is.
[00:06:57.140 --> 00:07:01.660]   Early on, for example, a lot of people thought
[00:07:01.660 --> 00:07:04.460]   that playing chess would be,
[00:07:04.460 --> 00:07:07.980]   you couldn't play chess if you didn't have
[00:07:07.980 --> 00:07:11.140]   sort of general human level intelligence.
[00:07:11.140 --> 00:07:13.340]   And of course, once computers were able
[00:07:13.340 --> 00:07:18.340]   to play chess better than humans, that revised that view.
[00:07:18.340 --> 00:07:21.500]   And people said, okay, well, maybe now we have
[00:07:21.500 --> 00:07:25.260]   to revise what we think of intelligence as.
[00:07:25.260 --> 00:07:28.740]   And so that's kind of been a theme
[00:07:28.740 --> 00:07:30.940]   throughout the history of the field is that
[00:07:30.940 --> 00:07:33.500]   once a machine can do some task,
[00:07:33.500 --> 00:07:36.420]   we then have to look back and say,
[00:07:36.420 --> 00:07:38.740]   oh, well, that changes my understanding
[00:07:38.740 --> 00:07:40.540]   of what intelligence is because I don't think
[00:07:40.540 --> 00:07:43.020]   that machine is intelligent.
[00:07:43.020 --> 00:07:45.580]   At least that's not what I want to call intelligence.
[00:07:45.580 --> 00:07:47.620]   - Do you think that line moves forever?
[00:07:47.620 --> 00:07:51.220]   Or will we eventually really feel as a civilization
[00:07:51.220 --> 00:07:54.020]   like we've crossed the line if it's possible?
[00:07:54.020 --> 00:07:56.460]   - It's hard to predict, but I don't see any reason
[00:07:56.460 --> 00:07:58.640]   why we couldn't, in principle,
[00:07:58.640 --> 00:08:02.140]   create something that we would consider intelligent.
[00:08:03.120 --> 00:08:06.320]   I don't know how we will know for sure.
[00:08:06.320 --> 00:08:10.480]   Maybe our own view of what intelligence is
[00:08:10.480 --> 00:08:13.920]   will be refined more and more until we finally figure out
[00:08:13.920 --> 00:08:15.680]   what we mean when we talk about it.
[00:08:15.680 --> 00:08:22.120]   But I think eventually we will create machines
[00:08:22.120 --> 00:08:24.400]   in a sense that have intelligence.
[00:08:24.400 --> 00:08:28.040]   They may not be the kinds of machines we have now.
[00:08:28.040 --> 00:08:31.800]   And one of the things that that's going to produce
[00:08:31.800 --> 00:08:34.880]   is making us sort of understand
[00:08:34.880 --> 00:08:38.520]   our own machine-like qualities
[00:08:38.520 --> 00:08:43.040]   that we, in a sense, are mechanical
[00:08:43.040 --> 00:08:45.880]   in the sense that like cells,
[00:08:45.880 --> 00:08:47.680]   cells are kind of mechanical.
[00:08:47.680 --> 00:08:52.680]   They have algorithms, they process information by,
[00:08:52.680 --> 00:08:57.040]   and somehow out of this mass of cells,
[00:08:57.040 --> 00:09:01.240]   we get this emergent property that we call intelligence.
[00:09:01.240 --> 00:09:06.240]   But underlying it is really just cellular processing
[00:09:06.240 --> 00:09:10.440]   and lots and lots and lots of it.
[00:09:10.440 --> 00:09:12.220]   - Do you think we'll be able to,
[00:09:12.220 --> 00:09:14.400]   do you think it's possible to create intelligence
[00:09:14.400 --> 00:09:16.440]   without understanding our own mind?
[00:09:16.440 --> 00:09:18.200]   You said sort of in that process
[00:09:18.200 --> 00:09:19.440]   we'll understand more and more,
[00:09:19.440 --> 00:09:22.960]   but do you think it's possible to sort of create
[00:09:22.960 --> 00:09:24.920]   without really fully understanding
[00:09:24.920 --> 00:09:27.560]   from a mechanistic perspective,
[00:09:27.560 --> 00:09:29.120]   sort of from a functional perspective,
[00:09:29.120 --> 00:09:31.880]   how our mysterious mind works?
[00:09:31.880 --> 00:09:36.680]   - If I had to bet on it, I would say no.
[00:09:36.680 --> 00:09:39.440]   We do have to understand our own minds,
[00:09:39.440 --> 00:09:42.840]   at least to some significant extent.
[00:09:42.840 --> 00:09:47.120]   But I think that's a really big open question.
[00:09:47.120 --> 00:09:49.280]   I've been very surprised at how far
[00:09:49.280 --> 00:09:52.840]   kind of brute force approaches based on, say,
[00:09:52.840 --> 00:09:57.360]   big data and huge networks can take us.
[00:09:57.360 --> 00:09:59.080]   I wouldn't have expected that.
[00:09:59.080 --> 00:10:03.080]   And they have nothing to do with the way our minds work.
[00:10:03.080 --> 00:10:06.800]   So that's been surprising to me, so it could be wrong.
[00:10:06.800 --> 00:10:09.560]   - To explore the psychological and the philosophical,
[00:10:09.560 --> 00:10:11.760]   do you think we're okay as a species
[00:10:11.760 --> 00:10:15.960]   with something that's more intelligent than us?
[00:10:15.960 --> 00:10:19.720]   Do you think perhaps the reason we're pushing that line
[00:10:19.720 --> 00:10:23.280]   further and further is we're afraid of acknowledging
[00:10:23.280 --> 00:10:25.800]   that there's something stronger, better,
[00:10:25.800 --> 00:10:29.000]   smarter than us humans?
[00:10:29.000 --> 00:10:31.600]   - Well, I'm not sure we can define intelligence that way
[00:10:31.600 --> 00:10:36.600]   because smarter than is with respect to what,
[00:10:36.600 --> 00:10:42.880]   computers are already smarter than us in some areas.
[00:10:42.880 --> 00:10:45.600]   They can multiply much better than we can.
[00:10:45.600 --> 00:10:50.240]   They can figure out driving routes to take
[00:10:50.240 --> 00:10:51.840]   much faster and better than we can.
[00:10:51.840 --> 00:10:54.400]   They have a lot more information to draw on.
[00:10:54.400 --> 00:10:57.400]   They know about traffic conditions and all that stuff.
[00:10:57.400 --> 00:11:02.240]   So for any given particular task,
[00:11:02.240 --> 00:11:04.680]   sometimes computers are much better than we are
[00:11:04.680 --> 00:11:07.080]   and we're totally happy with that, right?
[00:11:07.080 --> 00:11:08.520]   I'm totally happy with that.
[00:11:08.520 --> 00:11:10.560]   It doesn't bother me at all.
[00:11:10.560 --> 00:11:12.200]   I guess the question is,
[00:11:12.200 --> 00:11:18.280]   which things about our intelligence would we feel
[00:11:18.280 --> 00:11:23.360]   very sad or upset that machines had been able to recreate?
[00:11:24.440 --> 00:11:27.400]   So in the book, I talk about my former PhD advisor,
[00:11:27.400 --> 00:11:30.440]   Douglas Hofstadter, who encountered
[00:11:30.440 --> 00:11:32.920]   a music generation program.
[00:11:32.920 --> 00:11:36.760]   And that was really the line for him,
[00:11:36.760 --> 00:11:40.080]   that if a machine could create beautiful music,
[00:11:40.080 --> 00:11:44.080]   that would be terrifying for him
[00:11:44.080 --> 00:11:48.240]   because that is something he feels is really at the core
[00:11:48.240 --> 00:11:50.160]   of what it is to be human,
[00:11:50.160 --> 00:11:53.360]   creating beautiful music, art, literature.
[00:11:53.360 --> 00:11:57.920]   I don't think, he doesn't like the fact that
[00:11:57.920 --> 00:12:03.840]   machines can recognize spoken language really well.
[00:12:03.840 --> 00:12:09.520]   He personally doesn't like using speech recognition,
[00:12:09.520 --> 00:12:11.600]   but I don't think it bothers him to his core
[00:12:11.600 --> 00:12:15.760]   'cause it's like, okay, that's not at the core of humanity.
[00:12:15.760 --> 00:12:17.920]   But it may be different for every person,
[00:12:17.920 --> 00:12:22.920]   what really they feel would usurp their rights
[00:12:22.920 --> 00:12:25.240]   usurp their humanity.
[00:12:25.240 --> 00:12:27.440]   And I think maybe it's a generational thing also.
[00:12:27.440 --> 00:12:30.720]   Maybe our children or our children's children
[00:12:30.720 --> 00:12:35.720]   will be adapted, they'll adapt to these new devices
[00:12:35.720 --> 00:12:38.680]   that can do all these tasks and say,
[00:12:38.680 --> 00:12:41.560]   yes, this thing is smarter than me in all these areas,
[00:12:41.560 --> 00:12:44.880]   but that's great 'cause it helps me.
[00:12:44.880 --> 00:12:50.520]   - Looking at the broad history of our species,
[00:12:50.520 --> 00:12:52.720]   why do you think so many humans have dreamed
[00:12:52.720 --> 00:12:55.360]   of creating artificial life and artificial intelligence
[00:12:55.360 --> 00:12:57.360]   throughout the history of our civilization?
[00:12:57.360 --> 00:13:00.680]   So not just this century or the 20th century,
[00:13:00.680 --> 00:13:03.840]   but really many, throughout many centuries
[00:13:03.840 --> 00:13:04.740]   that preceded it.
[00:13:04.740 --> 00:13:07.800]   - That's a really good question.
[00:13:07.800 --> 00:13:09.360]   And I have wondered about that.
[00:13:09.360 --> 00:13:15.320]   'Cause I myself was driven by curiosity
[00:13:15.320 --> 00:13:18.720]   about my own thought processes
[00:13:18.720 --> 00:13:20.800]   and thought it would be fantastic
[00:13:20.800 --> 00:13:22.120]   to be able to get a computer
[00:13:22.120 --> 00:13:24.920]   to mimic some of my thought processes.
[00:13:24.920 --> 00:13:28.960]   And I'm not sure why we're so driven.
[00:13:28.960 --> 00:13:33.960]   I think we want to understand ourselves better.
[00:13:33.960 --> 00:13:43.240]   And we also want machines to do things for us.
[00:13:43.240 --> 00:13:46.240]   But I don't know, there's something more to it
[00:13:46.240 --> 00:13:49.560]   because it's so deep in the kind of mythology
[00:13:49.560 --> 00:13:53.360]   or the ethos of our species.
[00:13:53.360 --> 00:13:56.640]   And I don't think other species have this drive.
[00:13:56.640 --> 00:13:57.600]   So I don't know.
[00:13:57.600 --> 00:13:59.960]   - If you were to sort of psychoanalyze yourself
[00:13:59.960 --> 00:14:02.260]   in your own interest in AI,
[00:14:02.260 --> 00:14:07.520]   what excites you about creating intelligence?
[00:14:07.520 --> 00:14:09.800]   You said understanding our own selves?
[00:14:09.800 --> 00:14:13.340]   - Yeah, I think that's what drives me particularly.
[00:14:13.820 --> 00:14:18.820]   I'm really interested in human intelligence.
[00:14:18.820 --> 00:14:25.820]   But I'm also interested in the sort of the phenomenon
[00:14:25.820 --> 00:14:28.340]   of intelligence more generally.
[00:14:28.340 --> 00:14:29.780]   And I don't think humans are the only thing
[00:14:29.780 --> 00:14:34.260]   with intelligence, or even animals.
[00:14:34.260 --> 00:14:39.260]   But I think intelligence is a concept
[00:14:39.660 --> 00:14:43.780]   that encompasses a lot of complex systems.
[00:14:43.780 --> 00:14:47.740]   And if you think of things like insect colonies
[00:14:47.740 --> 00:14:52.000]   or cellular processes or the immune system
[00:14:52.000 --> 00:14:54.200]   or all kinds of different biological
[00:14:54.200 --> 00:14:59.200]   or even societal processes have as an emergent property,
[00:14:59.200 --> 00:15:02.460]   some aspects of what we would call intelligence.
[00:15:02.460 --> 00:15:05.120]   You know, they have memory, they do process information,
[00:15:05.120 --> 00:15:08.500]   they have goals, they accomplish their goals, et cetera.
[00:15:08.500 --> 00:15:12.700]   And to me, the question of what is this thing
[00:15:12.700 --> 00:15:17.700]   we're talking about here was really fascinating to me.
[00:15:17.700 --> 00:15:21.820]   And exploring it using computers seemed to be
[00:15:21.820 --> 00:15:23.980]   a good way to approach the question.
[00:15:23.980 --> 00:15:26.140]   - So do you think kind of of intelligence,
[00:15:26.140 --> 00:15:28.620]   do you think of our universe as a kind of hierarchy
[00:15:28.620 --> 00:15:31.020]   of complex systems and then intelligence
[00:15:31.020 --> 00:15:35.560]   is just the property of any, you can look at any level
[00:15:35.560 --> 00:15:39.300]   and every level has some aspect of intelligence.
[00:15:39.300 --> 00:15:40.940]   So we're just like one little speck
[00:15:40.940 --> 00:15:43.580]   in that giant hierarchy of complex systems.
[00:15:43.580 --> 00:15:47.620]   - I don't know if I would say any system
[00:15:47.620 --> 00:15:49.800]   like that has intelligence.
[00:15:49.800 --> 00:15:53.820]   But I guess what I wanna, I don't have a good enough
[00:15:53.820 --> 00:15:56.780]   definition of intelligence to say that.
[00:15:56.780 --> 00:15:59.380]   - So let me do sort of a multiple choice, I guess.
[00:15:59.380 --> 00:16:02.540]   So you said ant colonies.
[00:16:02.540 --> 00:16:04.540]   So are ant colonies intelligent?
[00:16:04.540 --> 00:16:09.460]   Are the bacteria in our body intelligent?
[00:16:09.460 --> 00:16:12.740]   And then going to the physics world,
[00:16:12.740 --> 00:16:16.260]   molecules and the behavior at the quantum level
[00:16:16.260 --> 00:16:21.260]   of electrons and so on, are those kinds of systems,
[00:16:21.260 --> 00:16:22.940]   do they possess intelligence?
[00:16:22.940 --> 00:16:27.720]   Like where's the line that feels compelling to you?
[00:16:27.720 --> 00:16:30.560]   - I don't know, I mean, I think intelligence is a continuum.
[00:16:30.560 --> 00:16:35.200]   And I think that the ability to, in some sense,
[00:16:35.200 --> 00:16:37.480]   have intention, have a goal,
[00:16:37.480 --> 00:16:45.280]   have some kind of self-awareness is part of it.
[00:16:45.280 --> 00:16:48.520]   So I'm not sure if, it's hard to know
[00:16:48.520 --> 00:16:50.380]   where to draw that line.
[00:16:50.380 --> 00:16:52.420]   I think that's kind of a mystery.
[00:16:52.420 --> 00:16:54.200]   But I wouldn't say that say that,
[00:16:56.400 --> 00:17:00.760]   the planets orbiting the sun is an intelligent system.
[00:17:00.760 --> 00:17:05.080]   I mean, I would find that maybe not the right term
[00:17:05.080 --> 00:17:06.320]   to describe that.
[00:17:06.320 --> 00:17:09.160]   And this is, there's all this debate in the field
[00:17:09.160 --> 00:17:12.600]   of like, what's the right way to define intelligence?
[00:17:12.600 --> 00:17:15.320]   What's the right way to model intelligence?
[00:17:15.320 --> 00:17:16.800]   Should we think about computation?
[00:17:16.800 --> 00:17:18.160]   Should we think about dynamics?
[00:17:18.160 --> 00:17:21.740]   And should we think about free energy
[00:17:21.740 --> 00:17:23.560]   and all of that stuff?
[00:17:23.560 --> 00:17:28.320]   And I think that it's a fantastic time to be in the field
[00:17:28.320 --> 00:17:30.400]   because there's so many questions
[00:17:30.400 --> 00:17:32.080]   and so much we don't understand.
[00:17:32.080 --> 00:17:33.880]   There's so much work to do.
[00:17:33.880 --> 00:17:38.360]   - So are we the most special kind of intelligence
[00:17:38.360 --> 00:17:41.600]   in this kind of, you said there's a bunch
[00:17:41.600 --> 00:17:43.920]   of different elements and characteristics
[00:17:43.920 --> 00:17:47.200]   of intelligence systems and colonies.
[00:17:47.200 --> 00:17:53.080]   Is human intelligence the thing in our brain?
[00:17:53.080 --> 00:17:55.400]   Is that the most interesting kind of intelligence
[00:17:55.400 --> 00:17:57.100]   in this continuum?
[00:17:57.100 --> 00:18:01.480]   - Well, it's interesting to us 'cause it is us.
[00:18:01.480 --> 00:18:03.400]   I mean, interesting to me, yes.
[00:18:03.400 --> 00:18:06.720]   And because I'm part of the human--
[00:18:06.720 --> 00:18:08.800]   - But to understanding the fundamentals of intelligence,
[00:18:08.800 --> 00:18:11.040]   what I'm getting at, is studying the human,
[00:18:11.040 --> 00:18:13.200]   is sort of, everything we've talked about,
[00:18:13.200 --> 00:18:14.400]   what you talk about in your book,
[00:18:14.400 --> 00:18:18.640]   what just the AI field, this notion,
[00:18:18.640 --> 00:18:21.560]   yes, it's hard to define, but it's usually talking
[00:18:21.560 --> 00:18:24.480]   about something that's very akin to human intelligence.
[00:18:24.480 --> 00:18:26.840]   - Yeah, to me it is the most interesting
[00:18:26.840 --> 00:18:29.960]   because it's the most complex, I think.
[00:18:29.960 --> 00:18:32.120]   It's the most self-aware.
[00:18:32.120 --> 00:18:34.960]   It's the only system, at least that I know of,
[00:18:34.960 --> 00:18:37.780]   that reflects on its own intelligence.
[00:18:37.780 --> 00:18:41.040]   - And you talk about the history of AI
[00:18:41.040 --> 00:18:45.000]   and us, in terms of creating artificial intelligence,
[00:18:45.000 --> 00:18:48.480]   being terrible at predicting the future
[00:18:48.480 --> 00:18:50.880]   with AI or with tech in general.
[00:18:50.880 --> 00:18:55.880]   So why do you think we're so bad at predicting the future?
[00:18:55.880 --> 00:18:59.080]   Are we hopelessly bad?
[00:18:59.080 --> 00:19:01.960]   So no matter what, whether it's this decade
[00:19:01.960 --> 00:19:04.880]   or the next few decades, every time we make a prediction,
[00:19:04.880 --> 00:19:06.920]   there's just no way of doing it well?
[00:19:06.920 --> 00:19:10.880]   Or as the field matures, we'll be better and better at it?
[00:19:10.880 --> 00:19:13.720]   - I believe as the field matures, we will be better.
[00:19:13.720 --> 00:19:16.040]   And I think the reason that we've had so much trouble
[00:19:16.040 --> 00:19:18.400]   is that we have so little understanding
[00:19:18.400 --> 00:19:20.320]   of our own intelligence.
[00:19:20.320 --> 00:19:25.320]   So there's the famous story about Marvin Minsky
[00:19:25.320 --> 00:19:32.600]   assigning computer vision as a summer project
[00:19:32.600 --> 00:19:34.640]   to his undergrad students.
[00:19:34.640 --> 00:19:36.640]   And I believe that's actually a true story.
[00:19:36.640 --> 00:19:39.080]   - Yeah, no, there's a write-up on it
[00:19:39.080 --> 00:19:41.000]   that everyone should read.
[00:19:41.000 --> 00:19:42.480]   I think it's like a proposal
[00:19:42.480 --> 00:19:46.000]   that describes everything that should be done
[00:19:46.000 --> 00:19:46.840]   in that project.
[00:19:46.840 --> 00:19:49.920]   And it's hilarious because it, I mean, you can explain it,
[00:19:49.920 --> 00:19:52.600]   but from my recollection, it describes basically
[00:19:52.600 --> 00:19:55.000]   all the fundamental problems of computer vision,
[00:19:55.000 --> 00:19:57.680]   many of which still haven't been solved.
[00:19:57.680 --> 00:19:59.560]   - Yeah, and I don't know how far
[00:19:59.560 --> 00:20:01.400]   they really expect it to get.
[00:20:01.400 --> 00:20:04.320]   But I think that, and they're really,
[00:20:04.320 --> 00:20:06.120]   Marvin Minsky is a super smart guy
[00:20:06.120 --> 00:20:08.400]   and very sophisticated thinker.
[00:20:08.400 --> 00:20:13.880]   But I think that no one really understands or understood,
[00:20:13.880 --> 00:20:17.640]   still doesn't understand how complicated,
[00:20:17.640 --> 00:20:22.160]   how complex the things that we do are
[00:20:22.160 --> 00:20:24.640]   because they're so invisible to us.
[00:20:24.640 --> 00:20:27.640]   To us, vision, being able to look out at the world
[00:20:27.640 --> 00:20:31.680]   and describe what we see, that's just immediate.
[00:20:31.680 --> 00:20:33.380]   It feels like it's no work at all.
[00:20:33.380 --> 00:20:35.920]   So it didn't seem like it would be that hard,
[00:20:35.920 --> 00:20:39.320]   but there's so much going on unconsciously,
[00:20:39.320 --> 00:20:44.320]   sort of invisible to us that I think we overestimate
[00:20:44.480 --> 00:20:49.480]   how easy it will be to get computers to do it.
[00:20:49.480 --> 00:20:53.880]   - And sort of for me to ask an unfair question,
[00:20:53.880 --> 00:20:56.520]   you've done research, you've thought about
[00:20:56.520 --> 00:20:59.920]   many different branches of AI through this book,
[00:20:59.920 --> 00:21:04.060]   widespread looking at where AI has been, where it is today.
[00:21:04.060 --> 00:21:08.800]   If you were to make a prediction,
[00:21:08.800 --> 00:21:12.080]   how many years from now would we as a society
[00:21:12.080 --> 00:21:15.760]   create something that you would say
[00:21:15.760 --> 00:21:18.280]   achieved human level intelligence
[00:21:18.280 --> 00:21:21.720]   or superhuman level intelligence?
[00:21:21.720 --> 00:21:25.080]   - That is an unfair question.
[00:21:25.080 --> 00:21:28.480]   - A prediction that will most likely be wrong.
[00:21:28.480 --> 00:21:29.960]   But it's just your notion because--
[00:21:29.960 --> 00:21:34.280]   - Okay, I'll say more than 100 years.
[00:21:34.280 --> 00:21:35.320]   - More than 100 years.
[00:21:35.320 --> 00:21:38.480]   - And I quoted somebody in my book who said that
[00:21:38.480 --> 00:21:42.560]   human level intelligence is 100 Nobel prizes away.
[00:21:42.560 --> 00:21:44.640]   (laughing)
[00:21:44.640 --> 00:21:48.000]   Which I like 'cause it's a nice way to sort of,
[00:21:48.000 --> 00:21:49.720]   it's a nice unit for prediction.
[00:21:49.720 --> 00:21:55.640]   And it's like that many fantastic discoveries
[00:21:55.640 --> 00:21:56.560]   have to be made.
[00:21:56.560 --> 00:22:00.680]   And of course there's no Nobel Prize in AI.
[00:22:00.680 --> 00:22:01.520]   - Right.
[00:22:01.520 --> 00:22:03.080]   - Not yet at least.
[00:22:03.080 --> 00:22:05.260]   - If we look at that 100 years,
[00:22:05.260 --> 00:22:10.200]   your sense is really the journey to intelligence
[00:22:10.200 --> 00:22:15.200]   has to go through something more complicated
[00:22:15.200 --> 00:22:19.400]   that's akin to our own cognitive systems.
[00:22:19.400 --> 00:22:21.600]   Understanding them, being able to create them
[00:22:21.600 --> 00:22:25.400]   in the artificial systems as opposed to sort of
[00:22:25.400 --> 00:22:28.860]   taking the machine learning approaches of today
[00:22:28.860 --> 00:22:33.520]   and really scaling them and scaling them exponentially
[00:22:33.520 --> 00:22:37.880]   with both compute and hardware and data.
[00:22:37.880 --> 00:22:40.580]   - That would be my guess.
[00:22:40.580 --> 00:22:47.160]   I think that in the sort of going along in the narrow AI
[00:22:47.160 --> 00:22:53.520]   that these current approaches will get better.
[00:22:53.520 --> 00:22:56.800]   I think there's some fundamental limits
[00:22:56.800 --> 00:22:59.320]   to how far they're gonna get.
[00:22:59.320 --> 00:23:01.760]   I might be wrong, but that's what I think.
[00:23:01.760 --> 00:23:06.680]   And there's some fundamental weaknesses that they have
[00:23:06.680 --> 00:23:09.960]   that I talk about in the book
[00:23:09.960 --> 00:23:14.960]   that just comes from this approach of supervised learning,
[00:23:14.960 --> 00:23:23.640]   requiring sort of feed forward networks and so on.
[00:23:23.640 --> 00:23:31.240]   I don't think it's a sustainable approach
[00:23:31.240 --> 00:23:34.200]   to understanding the world.
[00:23:34.200 --> 00:23:36.480]   - Yeah, I'm personally torn on it.
[00:23:36.480 --> 00:23:39.480]   Sort of I've, everything you read about in the book
[00:23:39.480 --> 00:23:43.760]   and sort of we're talking about now, I agree with you,
[00:23:43.760 --> 00:23:47.400]   but I'm more and more, depending on the day,
[00:23:47.400 --> 00:23:50.080]   first of all, I'm deeply surprised by the success
[00:23:50.080 --> 00:23:52.720]   of machine learning and deep learning in general.
[00:23:52.720 --> 00:23:53.840]   From the very beginning,
[00:23:53.840 --> 00:23:57.280]   when I was, it's really been my main focus of work.
[00:23:57.280 --> 00:23:59.360]   I'm just surprised how far it gets.
[00:23:59.400 --> 00:24:03.560]   And I'm also think we're really early on
[00:24:03.560 --> 00:24:07.080]   in these efforts of these narrow AI.
[00:24:07.080 --> 00:24:10.880]   So I think there'll be a lot of surprise of how far it gets.
[00:24:10.880 --> 00:24:14.360]   I think we'll be extremely impressed.
[00:24:14.360 --> 00:24:17.120]   Like my sense is everything I've seen so far,
[00:24:17.120 --> 00:24:19.480]   and we'll talk about autonomous driving and so on.
[00:24:19.480 --> 00:24:21.760]   I think we can get really far,
[00:24:21.760 --> 00:24:24.720]   but I also have a sense that we will discover
[00:24:24.720 --> 00:24:29.240]   just like you said, is that even though we'll get really far
[00:24:29.240 --> 00:24:31.720]   in order to create something like our own intelligence
[00:24:31.720 --> 00:24:34.120]   is actually much farther than we realize.
[00:24:34.120 --> 00:24:34.960]   - Right.
[00:24:34.960 --> 00:24:37.160]   - I think these methods are a lot more powerful
[00:24:37.160 --> 00:24:39.120]   than people give them credit for actually.
[00:24:39.120 --> 00:24:41.160]   So then of course there's the media hype,
[00:24:41.160 --> 00:24:43.720]   but I think there's a lot of researchers in the community,
[00:24:43.720 --> 00:24:46.680]   especially like not undergrads, right?
[00:24:46.680 --> 00:24:48.840]   But like people who've been in AI,
[00:24:48.840 --> 00:24:50.960]   they're skeptical about how far deep learning can get.
[00:24:50.960 --> 00:24:54.840]   And I'm more and more thinking that it can actually get
[00:24:54.840 --> 00:24:56.960]   farther than we realize.
[00:24:56.960 --> 00:24:58.480]   - It's certainly possible.
[00:24:58.480 --> 00:25:00.840]   One thing that surprised me when I was writing the book
[00:25:00.840 --> 00:25:03.800]   is how far apart different people in the field are
[00:25:03.800 --> 00:25:08.440]   on their opinion of how far the field has come
[00:25:08.440 --> 00:25:11.560]   and what is accomplished and what's gonna happen next.
[00:25:11.560 --> 00:25:13.800]   - What's your sense of the different,
[00:25:13.800 --> 00:25:17.560]   who are the different people, groups, mindsets,
[00:25:17.560 --> 00:25:21.820]   thoughts in the community about where AI is today?
[00:25:21.820 --> 00:25:24.120]   - Yeah, they're all over the place.
[00:25:24.120 --> 00:25:29.120]   So there's kind of the singularity transhumanism group.
[00:25:29.120 --> 00:25:33.200]   I don't know exactly how to characterize that approach.
[00:25:33.200 --> 00:25:34.320]   - I agree, first of all.
[00:25:34.320 --> 00:25:37.920]   - Yeah, the sort of exponential progress.
[00:25:37.920 --> 00:25:42.920]   We're on the sort of almost at the hugely accelerating
[00:25:42.920 --> 00:25:45.760]   part of the exponential.
[00:25:45.760 --> 00:25:49.720]   And by in the next 30 years,
[00:25:49.720 --> 00:25:54.120]   we're going to see super intelligent AI and all that,
[00:25:54.120 --> 00:25:57.400]   and we'll be able to upload our brains and that.
[00:25:57.400 --> 00:26:00.520]   So there's that kind of extreme view that most,
[00:26:00.520 --> 00:26:04.640]   I think most people who work in AI don't have.
[00:26:04.640 --> 00:26:06.080]   They disagree with that.
[00:26:06.080 --> 00:26:11.080]   But there are people who are maybe aren't singularity people,
[00:26:11.080 --> 00:26:16.880]   but they do think that the current approach
[00:26:16.880 --> 00:26:20.040]   of deep learning is going to scale
[00:26:20.040 --> 00:26:23.840]   and is going to kind of go all the way basically
[00:26:23.840 --> 00:26:26.760]   and take us to true AI or human level AI
[00:26:26.760 --> 00:26:29.120]   or whatever you wanna call it.
[00:26:29.120 --> 00:26:30.880]   And there's quite a few of them.
[00:26:30.880 --> 00:26:34.800]   And a lot of them, like a lot of the people I've met
[00:26:34.800 --> 00:26:39.800]   who work at big tech companies in AI groups
[00:26:39.800 --> 00:26:45.120]   kind of have this view that we're really not that far.
[00:26:46.200 --> 00:26:47.440]   - Just to linger on that point,
[00:26:47.440 --> 00:26:50.960]   sort of if I can take as an example, like Jan Lekun,
[00:26:50.960 --> 00:26:52.640]   I don't know if you know about his work
[00:26:52.640 --> 00:26:54.480]   and so his viewpoints on this.
[00:26:54.480 --> 00:26:55.320]   - I do.
[00:26:55.320 --> 00:26:57.840]   - He believes that there's a bunch of breakthroughs,
[00:26:57.840 --> 00:27:01.080]   like fundamental, like Nobel prizes that are needed still.
[00:27:01.080 --> 00:27:03.600]   But I think he thinks those breakthroughs
[00:27:03.600 --> 00:27:06.600]   will be built on top of deep learning.
[00:27:06.600 --> 00:27:09.480]   And then there's some people who think we need to kind of
[00:27:09.480 --> 00:27:12.720]   put deep learning to the side a little bit
[00:27:12.720 --> 00:27:15.480]   as just one module that's helpful
[00:27:15.480 --> 00:27:17.800]   in the bigger cognitive framework.
[00:27:17.800 --> 00:27:21.320]   - Right, so I think, from what I understand,
[00:27:21.320 --> 00:27:26.320]   Jan Lekun is rightly saying supervised learning
[00:27:26.320 --> 00:27:28.000]   is not sustainable.
[00:27:28.000 --> 00:27:31.120]   We have to figure out how to do unsupervised learning,
[00:27:31.120 --> 00:27:33.640]   that that's gonna be the key.
[00:27:33.640 --> 00:27:38.320]   And I think that's probably true.
[00:27:38.320 --> 00:27:40.760]   I think unsupervised learning
[00:27:40.760 --> 00:27:42.320]   is gonna be harder than people.
[00:27:42.920 --> 00:27:44.560]   - Think. (laughs)
[00:27:44.560 --> 00:27:47.080]   I mean, the way that we humans do it.
[00:27:47.080 --> 00:27:50.440]   Then there's the opposing view,
[00:27:50.440 --> 00:27:55.440]   you know, there's the Gary Marcus kind of hybrid view
[00:27:55.440 --> 00:27:58.160]   where deep learning is one part,
[00:27:58.160 --> 00:28:02.240]   but we need to bring back kind of these symbolic approaches
[00:28:02.240 --> 00:28:03.440]   and combine them.
[00:28:03.440 --> 00:28:06.680]   Of course, no one knows how to do that very well.
[00:28:06.680 --> 00:28:08.480]   - Which is the more important part.
[00:28:08.480 --> 00:28:09.320]   - Right.
[00:28:09.320 --> 00:28:11.200]   - To emphasize and how do they, yeah,
[00:28:11.200 --> 00:28:12.080]   how do they fit together?
[00:28:12.080 --> 00:28:13.800]   What's the foundation?
[00:28:13.800 --> 00:28:15.400]   What's the thing that's on top?
[00:28:15.400 --> 00:28:16.240]   What's the cake?
[00:28:16.240 --> 00:28:17.080]   What's the icing?
[00:28:17.080 --> 00:28:18.640]   - Right. (laughs)
[00:28:18.640 --> 00:28:22.720]   Then there's people pushing different things.
[00:28:22.720 --> 00:28:24.840]   There's the people, the causality people
[00:28:24.840 --> 00:28:28.720]   who say, you know, deep learning as it's formulated today
[00:28:28.720 --> 00:28:32.080]   completely lacks any notion of causality
[00:28:32.080 --> 00:28:35.160]   and that dooms it.
[00:28:35.160 --> 00:28:37.720]   And therefore, we have to somehow give it
[00:28:37.720 --> 00:28:39.580]   some kind of notion of causality.
[00:28:41.340 --> 00:28:46.340]   There's a lot of push from the more cognitive science crowd
[00:28:46.340 --> 00:28:54.160]   saying we have to look at developmental learning.
[00:28:54.160 --> 00:28:56.740]   We have to look at how babies learn.
[00:28:56.740 --> 00:29:01.000]   We have to look at intuitive physics,
[00:29:01.000 --> 00:29:03.040]   all these things we know about physics.
[00:29:03.040 --> 00:29:05.320]   And as somebody kind of quipped,
[00:29:05.320 --> 00:29:08.840]   we also have to teach machines intuitive metaphysics,
[00:29:08.840 --> 00:29:12.280]   which means like objects exist.
[00:29:12.280 --> 00:29:14.560]   (both laugh)
[00:29:14.560 --> 00:29:16.140]   Causality exists.
[00:29:16.140 --> 00:29:19.280]   You know, these things that maybe we're born with.
[00:29:19.280 --> 00:29:20.200]   I don't know.
[00:29:20.200 --> 00:29:21.400]   That they don't have,
[00:29:21.400 --> 00:29:23.560]   the machines don't have any of that.
[00:29:23.560 --> 00:29:26.600]   You know, they look at a group of pixels
[00:29:26.600 --> 00:29:31.400]   and maybe they get 10 million examples,
[00:29:31.400 --> 00:29:34.360]   but they can't necessarily learn
[00:29:34.360 --> 00:29:36.480]   that there are objects in the world.
[00:29:38.160 --> 00:29:41.160]   So there's just a lot of pieces of the puzzle
[00:29:41.160 --> 00:29:43.080]   that people are promoting
[00:29:43.080 --> 00:29:47.640]   and with different opinions of like how important they are
[00:29:47.640 --> 00:29:52.000]   and how close we are to being able to put them all together
[00:29:52.000 --> 00:29:54.080]   to create general intelligence.
[00:29:54.080 --> 00:29:56.580]   - Looking at this broad field,
[00:29:56.580 --> 00:29:57.800]   what do you take away from it?
[00:29:57.800 --> 00:29:59.580]   Who's the most impressive?
[00:29:59.580 --> 00:30:03.320]   Is it the cognitive folks, the Gary Marcus camp,
[00:30:03.320 --> 00:30:07.000]   the Yon camp, unsupervised and self-supervised?
[00:30:07.000 --> 00:30:08.600]   There's the supervisors
[00:30:08.600 --> 00:30:09.640]   and then there's the engineers
[00:30:09.640 --> 00:30:11.560]   who are actually building systems.
[00:30:11.560 --> 00:30:14.720]   You have sort of the Andrej Karpathy at Tesla
[00:30:14.720 --> 00:30:17.960]   building actual, you know, it's not philosophy,
[00:30:17.960 --> 00:30:21.040]   it's real systems that operate in the real world.
[00:30:21.040 --> 00:30:23.880]   What do you take away from all this beautiful variety?
[00:30:23.880 --> 00:30:25.600]   - I don't know if, you know,
[00:30:25.600 --> 00:30:27.520]   these different views are not necessarily
[00:30:27.520 --> 00:30:29.660]   mutually exclusive.
[00:30:29.660 --> 00:30:33.360]   And I think people like Jan LeCun
[00:30:34.760 --> 00:30:37.160]   agrees with the developmental psychology,
[00:30:37.160 --> 00:30:43.200]   causality, intuitive, physics, et cetera.
[00:30:43.200 --> 00:30:46.000]   But he still thinks that it's learning,
[00:30:46.000 --> 00:30:48.320]   like end-to-end learning is the way to go.
[00:30:48.320 --> 00:30:50.120]   - Will take us perhaps all the way.
[00:30:50.120 --> 00:30:51.120]   - Yeah, and that we don't need,
[00:30:51.120 --> 00:30:55.800]   there's no sort of innate stuff that has to get built in.
[00:30:55.800 --> 00:31:00.500]   This is, you know, it's because it's a hard problem.
[00:31:02.260 --> 00:31:04.300]   I personally, you know,
[00:31:04.300 --> 00:31:07.260]   I'm very sympathetic to the cognitive science side
[00:31:07.260 --> 00:31:10.500]   'cause that's kind of where I came in to the field.
[00:31:10.500 --> 00:31:15.500]   I've become more and more sort of an embodiment adherent
[00:31:15.500 --> 00:31:18.580]   saying that, you know, without having a body,
[00:31:18.580 --> 00:31:20.880]   it's gonna be very hard to learn
[00:31:20.880 --> 00:31:22.780]   what we need to learn about the world.
[00:31:22.780 --> 00:31:26.900]   - That's definitely something I'd love to talk about
[00:31:26.900 --> 00:31:31.580]   in a little bit, to step into the cognitive world.
[00:31:31.580 --> 00:31:32.780]   And if you don't mind,
[00:31:32.780 --> 00:31:34.300]   'cause you've done so many interesting things.
[00:31:34.300 --> 00:31:36.980]   If we look to CopyCat,
[00:31:36.980 --> 00:31:40.300]   taking a couple of decades step back,
[00:31:40.300 --> 00:31:43.380]   you, Douglas Hostetter, and others
[00:31:43.380 --> 00:31:48.380]   have created and developed CopyCat more than 30 years ago.
[00:31:48.380 --> 00:31:49.980]   - That's painful to hear.
[00:31:49.980 --> 00:31:51.940]   - What is it?
[00:31:51.940 --> 00:31:54.340]   What is CopyCat?
[00:31:54.340 --> 00:31:59.340]   - It's a program that makes analogies in an idealized domain,
[00:32:00.740 --> 00:32:03.620]   idealized world of letter strings.
[00:32:03.620 --> 00:32:05.560]   So as you say, 30 years ago, wow.
[00:32:05.560 --> 00:32:07.900]   So I started working on it
[00:32:07.900 --> 00:32:12.620]   when I started grad school in 1984.
[00:32:12.620 --> 00:32:14.380]   Wow.
[00:32:14.380 --> 00:32:15.420]   (both laughing)
[00:32:15.420 --> 00:32:16.260]   Dates me.
[00:32:16.260 --> 00:32:21.460]   And it's based on Doug Hostetter's ideas
[00:32:21.460 --> 00:32:24.660]   that about that analogy
[00:32:24.660 --> 00:32:27.820]   is really a core aspect of thinking.
[00:32:30.280 --> 00:32:32.420]   I remember he has a really nice quote
[00:32:32.420 --> 00:32:36.940]   in the book by himself and Immanuel Sander
[00:32:36.940 --> 00:32:38.820]   called "Surfaces and Essences."
[00:32:38.820 --> 00:32:39.820]   I don't know if you've seen that book,
[00:32:39.820 --> 00:32:42.040]   but it's about analogy.
[00:32:42.040 --> 00:32:46.780]   He says, "Without concepts, there can be no thought,
[00:32:46.780 --> 00:32:51.180]   "and without analogies, there can be no concepts."
[00:32:51.180 --> 00:32:52.620]   So the view is that analogy
[00:32:52.620 --> 00:32:55.060]   is not just this kind of reasoning technique
[00:32:55.060 --> 00:32:59.460]   where we go, shoe is to foot,
[00:32:59.460 --> 00:33:01.020]   glove is to what,
[00:33:01.020 --> 00:33:05.440]   these kinds of things that we have on IQ tests or whatever,
[00:33:05.440 --> 00:33:06.540]   but that it's much deeper,
[00:33:06.540 --> 00:33:10.940]   it's much more pervasive in everything we do,
[00:33:10.940 --> 00:33:14.900]   in our language, our thinking, our perception.
[00:33:14.900 --> 00:33:20.900]   So he had a view that was a very active perception idea.
[00:33:20.900 --> 00:33:25.100]   So the idea was that instead of having
[00:33:25.100 --> 00:33:30.100]   kind of a passive network in which you have input
[00:33:30.100 --> 00:33:35.480]   that's being processed through these feed-forward layers,
[00:33:35.480 --> 00:33:37.060]   and then there's an output at the end,
[00:33:37.060 --> 00:33:41.460]   that perception is really a dynamic process
[00:33:41.460 --> 00:33:43.380]   where our eyes are moving around
[00:33:43.380 --> 00:33:44.780]   and they're getting information,
[00:33:44.780 --> 00:33:47.020]   and that information is feeding back
[00:33:47.020 --> 00:33:51.540]   to what we look at next, influences what we look at next
[00:33:51.540 --> 00:33:53.220]   and how we look at it.
[00:33:53.220 --> 00:33:56.060]   And so CopyCat was trying to do that,
[00:33:56.060 --> 00:33:57.720]   kind of simulate that kind of idea
[00:33:57.720 --> 00:34:02.640]   where you have these agents,
[00:34:02.640 --> 00:34:04.100]   it's kind of an agent-based system,
[00:34:04.100 --> 00:34:08.040]   and you have these agents that are picking things to look at
[00:34:08.040 --> 00:34:11.180]   and deciding whether they were interesting or not,
[00:34:11.180 --> 00:34:13.580]   and whether they should be looked at more,
[00:34:13.580 --> 00:34:15.900]   and that would influence other agents.
[00:34:15.900 --> 00:34:17.580]   - How did they interact?
[00:34:17.580 --> 00:34:19.700]   - So they interacted through this global,
[00:34:19.700 --> 00:34:22.180]   kind of what we call the workspace.
[00:34:22.180 --> 00:34:25.620]   So it was actually inspired by the old blackboard systems
[00:34:25.620 --> 00:34:28.920]   where you would have agents that post information
[00:34:28.920 --> 00:34:30.860]   on a blackboard, a common blackboard.
[00:34:30.860 --> 00:34:32.940]   This is like very old-fashioned AI.
[00:34:32.940 --> 00:34:36.300]   - Is that, are we talking about like in physical space?
[00:34:36.300 --> 00:34:37.380]   Is this a computer program?
[00:34:37.380 --> 00:34:38.300]   - It's a computer program.
[00:34:38.300 --> 00:34:41.940]   - So agents posting concepts on a blackboard kind of thing?
[00:34:41.940 --> 00:34:43.900]   - Yeah, we called it a workspace.
[00:34:43.900 --> 00:34:48.420]   And the workspace is a data structure.
[00:34:48.420 --> 00:34:50.720]   The agents are little pieces of code
[00:34:50.720 --> 00:34:54.100]   that you could think of them as little detectors
[00:34:54.100 --> 00:34:55.940]   or little filters that say,
[00:34:55.940 --> 00:34:57.460]   I'm gonna pick this place to look,
[00:34:57.460 --> 00:34:59.060]   and I'm gonna look for a certain thing,
[00:34:59.060 --> 00:35:01.820]   and is this the thing I think is important?
[00:35:01.820 --> 00:35:03.020]   Is it there?
[00:35:03.020 --> 00:35:06.940]   So it's almost like a convolution in a way,
[00:35:06.940 --> 00:35:10.020]   except a little bit more general,
[00:35:10.020 --> 00:35:13.740]   and then highlighting it in the workspace.
[00:35:13.740 --> 00:35:16.260]   - Once it's in the workspace,
[00:35:16.260 --> 00:35:18.820]   how do the things that are highlighted relate to each other?
[00:35:18.820 --> 00:35:19.660]   Like what's, is it--
[00:35:19.660 --> 00:35:21.520]   - So there's different kinds of agents
[00:35:21.520 --> 00:35:23.840]   that can build connections between different things.
[00:35:23.840 --> 00:35:25.560]   So just to give you a concrete example,
[00:35:25.560 --> 00:35:28.360]   what CopyCat did was it made analogies
[00:35:28.360 --> 00:35:30.320]   between strings of letters.
[00:35:30.320 --> 00:35:31.960]   So here's an example.
[00:35:31.960 --> 00:35:35.360]   ABC changes to ABD.
[00:35:35.360 --> 00:35:38.160]   What does IJK change to?
[00:35:38.160 --> 00:35:42.440]   And the program had some prior knowledge about the alphabet.
[00:35:42.440 --> 00:35:44.280]   It knew the sequence of the alphabet.
[00:35:44.280 --> 00:35:49.300]   It had a concept of letter, of successor of letter.
[00:35:49.300 --> 00:35:50.960]   It had concepts of sameness.
[00:35:50.960 --> 00:35:54.000]   So it had some innate things programmed in.
[00:35:54.000 --> 00:35:58.320]   But then it could do things like, say,
[00:35:58.320 --> 00:36:03.320]   discover that ABC is a group of letters in succession.
[00:36:03.320 --> 00:36:10.120]   And then an agent can mark that.
[00:36:10.120 --> 00:36:16.020]   - So the idea that there could be a sequence of letters,
[00:36:16.020 --> 00:36:18.160]   is that a new concept that's formed,
[00:36:18.160 --> 00:36:19.400]   or that's a concept that's innate?
[00:36:19.400 --> 00:36:21.500]   - That's a concept that's innate.
[00:36:21.500 --> 00:36:23.700]   - Sort of, can you form new concepts,
[00:36:23.700 --> 00:36:25.060]   or are all concepts innate?
[00:36:25.060 --> 00:36:27.500]   - So in this program,
[00:36:27.500 --> 00:36:30.220]   all the concepts of the program were innate.
[00:36:30.220 --> 00:36:32.220]   So, 'cause we weren't, I mean,
[00:36:32.220 --> 00:36:35.600]   obviously that limits it quite a bit.
[00:36:35.600 --> 00:36:37.180]   But what we were trying to do is say,
[00:36:37.180 --> 00:36:39.460]   suppose you have some innate concepts.
[00:36:39.460 --> 00:36:45.140]   How do you flexibly apply them to new situations?
[00:36:45.140 --> 00:36:46.780]   And how do you make analogies?
[00:36:47.820 --> 00:36:49.020]   - Let's step back for a second.
[00:36:49.020 --> 00:36:50.900]   So I really like that quote,
[00:36:50.900 --> 00:36:53.700]   that you say, "Without concepts, there can be no thought,
[00:36:53.700 --> 00:36:56.580]   "and without analogies, there can be no concepts."
[00:36:56.580 --> 00:36:58.460]   In a Santa Fe presentation,
[00:36:58.460 --> 00:37:00.900]   you said that it should be one of the mantras of AI.
[00:37:00.900 --> 00:37:01.900]   - Yes.
[00:37:01.900 --> 00:37:04.260]   - And that you also yourself said,
[00:37:04.260 --> 00:37:06.560]   "How to form and fluidly use concept
[00:37:06.560 --> 00:37:09.860]   "is the most important open problem in AI."
[00:37:09.860 --> 00:37:11.260]   - Yes.
[00:37:11.260 --> 00:37:14.500]   - How to form and fluidly use concepts
[00:37:14.500 --> 00:37:16.980]   is the most important open problem in AI.
[00:37:16.980 --> 00:37:21.880]   So let's, what is a concept and what is an analogy?
[00:37:21.880 --> 00:37:27.120]   - A concept is in some sense a fundamental unit of thought.
[00:37:27.120 --> 00:37:33.260]   So say we have a concept of a dog, okay?
[00:37:33.260 --> 00:37:43.620]   And a concept is embedded in a whole space of concepts
[00:37:45.180 --> 00:37:48.740]   so that there's certain concepts that are closer to it
[00:37:48.740 --> 00:37:50.260]   or farther away from it.
[00:37:50.260 --> 00:37:53.140]   - Are these concepts, are they really like fundamental,
[00:37:53.140 --> 00:37:55.620]   like we mentioned innate, almost like axiomatic,
[00:37:55.620 --> 00:37:57.980]   like very basic, and then there's other stuff
[00:37:57.980 --> 00:37:58.900]   built on top of it?
[00:37:58.900 --> 00:37:59.740]   - Yeah. - Or does this include
[00:37:59.740 --> 00:38:01.100]   everything?
[00:38:01.100 --> 00:38:02.500]   Are they complicated?
[00:38:02.500 --> 00:38:07.020]   - You can certainly form new concepts.
[00:38:07.020 --> 00:38:08.380]   - Right, I guess that's the question I'm asking.
[00:38:08.380 --> 00:38:09.220]   - Yeah.
[00:38:09.220 --> 00:38:10.820]   - Can you form new concepts that are
[00:38:10.820 --> 00:38:14.420]   complex combinations of other concepts?
[00:38:14.420 --> 00:38:15.980]   - Yes, absolutely.
[00:38:15.980 --> 00:38:20.060]   And that's kind of what we do in learning.
[00:38:20.060 --> 00:38:23.020]   - And then what's the role of analogies in that?
[00:38:23.020 --> 00:38:28.020]   - So analogy is when you recognize that one situation
[00:38:28.020 --> 00:38:34.700]   is essentially the same as another situation.
[00:38:34.700 --> 00:38:38.780]   And essentially is kind of the key word there
[00:38:38.780 --> 00:38:40.020]   'cause it's not the same.
[00:38:40.020 --> 00:38:45.020]   So if I say, last week I did a podcast interview
[00:38:45.020 --> 00:38:52.980]   actually like three days ago in Washington, DC.
[00:38:52.980 --> 00:38:56.580]   And that situation was very similar to this situation,
[00:38:56.580 --> 00:38:58.740]   although it wasn't exactly the same.
[00:38:58.740 --> 00:39:00.780]   It was a different person sitting across from me.
[00:39:00.780 --> 00:39:03.380]   We had different kinds of microphones.
[00:39:03.380 --> 00:39:04.740]   The questions were different.
[00:39:04.740 --> 00:39:06.140]   The building was different.
[00:39:06.140 --> 00:39:07.140]   There's all kinds of different things,
[00:39:07.140 --> 00:39:08.980]   but really it was analogous.
[00:39:09.980 --> 00:39:14.460]   Or I can say, so doing a podcast interview,
[00:39:14.460 --> 00:39:15.900]   that's kind of a concept.
[00:39:15.900 --> 00:39:17.100]   It's a new concept.
[00:39:17.100 --> 00:39:19.740]   You know, I never had that concept before.
[00:39:19.740 --> 00:39:21.060]   (both laughing)
[00:39:21.060 --> 00:39:22.780]   I don't know if it's here, essentially.
[00:39:22.780 --> 00:39:26.980]   I mean, and I can make an analogy with it
[00:39:26.980 --> 00:39:31.140]   like being interviewed for a news article in a newspaper.
[00:39:31.140 --> 00:39:35.420]   And I can say, well, you kind of play the same role
[00:39:35.420 --> 00:39:39.860]   that the newspaper reporter played.
[00:39:39.860 --> 00:39:41.780]   It's not exactly the same
[00:39:41.780 --> 00:39:43.660]   'cause maybe they actually emailed me
[00:39:43.660 --> 00:39:45.900]   some written questions rather than talking.
[00:39:45.900 --> 00:39:50.100]   And the writing, the written questions play,
[00:39:50.100 --> 00:39:53.020]   are analogous to your spoken questions.
[00:39:53.020 --> 00:39:54.700]   You know, there's just all kinds of similarities.
[00:39:54.700 --> 00:39:57.180]   - And this somehow probably connects to conversations
[00:39:57.180 --> 00:39:58.580]   you have over Thanksgiving dinner,
[00:39:58.580 --> 00:40:00.860]   just general conversations.
[00:40:00.860 --> 00:40:03.300]   There's like a thread you can probably take
[00:40:03.540 --> 00:40:06.700]   that just stretches out in all aspects of life
[00:40:06.700 --> 00:40:08.420]   that connect to this podcast.
[00:40:08.420 --> 00:40:11.420]   I mean, conversations between humans.
[00:40:11.420 --> 00:40:16.420]   - Sure, and if I go and tell a friend of mine
[00:40:16.420 --> 00:40:19.180]   about this podcast interview,
[00:40:19.180 --> 00:40:22.660]   my friend might say, oh, the same thing happened to me.
[00:40:22.660 --> 00:40:23.820]   You know, let's say, you know,
[00:40:23.820 --> 00:40:26.020]   you ask me some really hard question
[00:40:26.020 --> 00:40:29.260]   and I have trouble answering it.
[00:40:29.260 --> 00:40:31.620]   My friend could say, the same thing happened to me,
[00:40:31.620 --> 00:40:34.060]   but it was like, it wasn't a podcast interview.
[00:40:34.060 --> 00:40:39.060]   It wasn't, it was a completely different situation.
[00:40:39.060 --> 00:40:44.220]   And yet my friend is seeing essentially the same thing.
[00:40:44.220 --> 00:40:45.780]   You know, we say that very fluidly,
[00:40:45.780 --> 00:40:48.380]   the same thing happened to me.
[00:40:48.380 --> 00:40:49.940]   - Essentially the same thing, right.
[00:40:49.940 --> 00:40:51.100]   - But we don't even say that, right?
[00:40:51.100 --> 00:40:52.900]   We just say the same thing. - Right, you imply it, yes.
[00:40:52.900 --> 00:40:56.900]   - Yeah, and the view that kind of went into, say,
[00:40:56.900 --> 00:40:58.980]   a coffee cat, that whole thing is that,
[00:40:58.980 --> 00:41:03.620]   that act of saying the same thing happened to me
[00:41:03.620 --> 00:41:05.540]   is making an analogy.
[00:41:05.540 --> 00:41:10.540]   And in some sense, that's what underlies all of our concepts.
[00:41:10.540 --> 00:41:14.060]   - Why do you think analogy making that you're describing
[00:41:14.060 --> 00:41:17.020]   is so fundamental to cognition?
[00:41:17.020 --> 00:41:20.020]   Like, it seems like it's the main element action
[00:41:20.020 --> 00:41:22.500]   of what we think of as cognition.
[00:41:22.500 --> 00:41:25.940]   - Yeah, so it can be argued that
[00:41:25.940 --> 00:41:30.940]   all of this generalization we do of concepts
[00:41:30.940 --> 00:41:37.540]   and recognizing concepts in different situations
[00:41:37.540 --> 00:41:42.660]   is done by analogy.
[00:41:42.660 --> 00:41:47.660]   That that's, every time I'm recognizing that, say,
[00:41:47.660 --> 00:41:53.780]   you're a person, that's by analogy
[00:41:53.780 --> 00:41:55.700]   'cause I have this concept of what person is
[00:41:55.700 --> 00:41:57.380]   and I'm applying it to you.
[00:41:57.380 --> 00:42:02.380]   And every time I recognize a new situation,
[00:42:02.380 --> 00:42:06.540]   like one of the things I talked about in the book
[00:42:06.540 --> 00:42:09.700]   was the concept of walking a dog,
[00:42:09.700 --> 00:42:11.780]   that that's actually making an analogy
[00:42:11.780 --> 00:42:15.460]   because all of that, the details are very different.
[00:42:15.460 --> 00:42:19.420]   - So reasoning could be reduced down
[00:42:19.420 --> 00:42:21.780]   to essentially analogy making.
[00:42:21.780 --> 00:42:24.000]   So all the things we think of as like,
[00:42:25.220 --> 00:42:26.820]   yeah, like you said, perception.
[00:42:26.820 --> 00:42:29.660]   So what's perception is taking raw sensory input
[00:42:29.660 --> 00:42:33.020]   and it's somehow integrating into our understanding
[00:42:33.020 --> 00:42:34.740]   of the world, updating the understanding.
[00:42:34.740 --> 00:42:39.180]   And all of that has just this giant mess of analogies
[00:42:39.180 --> 00:42:40.180]   that are being made.
[00:42:40.180 --> 00:42:41.260]   - I think so, yeah.
[00:42:41.260 --> 00:42:44.260]   - If you could just linger on it a little bit,
[00:42:44.260 --> 00:42:46.220]   like what do you think it takes
[00:42:46.220 --> 00:42:49.100]   to engineer a process like that
[00:42:49.100 --> 00:42:52.140]   for us in our artificial systems?
[00:42:52.180 --> 00:42:56.940]   - We need to understand better, I think,
[00:42:56.940 --> 00:43:00.940]   how we do it, how humans do it.
[00:43:00.940 --> 00:43:07.740]   And it comes down to internal models, I think.
[00:43:07.740 --> 00:43:11.180]   People talk a lot about mental models,
[00:43:11.180 --> 00:43:13.340]   that concepts are mental models,
[00:43:13.340 --> 00:43:18.340]   that I can, in my head, I can do a simulation
[00:43:19.540 --> 00:43:22.540]   of a situation like walking a dog.
[00:43:22.540 --> 00:43:25.620]   And that there's some work in psychology
[00:43:25.620 --> 00:43:29.460]   that promotes this idea that all of concepts
[00:43:29.460 --> 00:43:31.860]   are really mental simulations.
[00:43:31.860 --> 00:43:35.140]   That whenever you encounter a concept
[00:43:35.140 --> 00:43:37.220]   or situation in the world, or you read about it
[00:43:37.220 --> 00:43:40.740]   or whatever, you do some kind of mental simulation
[00:43:40.740 --> 00:43:43.780]   that allows you to predict what's gonna happen,
[00:43:43.780 --> 00:43:48.020]   to develop expectations of what's gonna happen.
[00:43:48.020 --> 00:43:51.620]   So that's the kind of structure I think we need,
[00:43:51.620 --> 00:43:55.620]   is that kind of mental model that,
[00:43:55.620 --> 00:43:58.100]   and in our brains, somehow these mental models
[00:43:58.100 --> 00:44:00.420]   are very much interconnected.
[00:44:00.420 --> 00:44:03.740]   - Again, so a lot of stuff we're talking about
[00:44:03.740 --> 00:44:05.980]   are essentially open problems, right?
[00:44:05.980 --> 00:44:08.700]   So if I ask a question, I don't mean
[00:44:08.700 --> 00:44:10.020]   that you would know the answer,
[00:44:10.020 --> 00:44:11.380]   I'm really just hypothesizing.
[00:44:11.380 --> 00:44:16.380]   But how big do you think is the network
[00:44:17.380 --> 00:44:22.380]   graph data structure of concepts that's in our head?
[00:44:22.380 --> 00:44:26.500]   Like if we're trying to build that ourselves,
[00:44:26.500 --> 00:44:28.780]   like we take it, that's one of the things
[00:44:28.780 --> 00:44:30.420]   we take for granted, we think, I mean,
[00:44:30.420 --> 00:44:32.580]   that's why we take common sense for granted,
[00:44:32.580 --> 00:44:34.740]   we think common sense is trivial.
[00:44:34.740 --> 00:44:39.740]   But how big of a thing of concepts is
[00:44:39.740 --> 00:44:42.420]   that underlies what we think of as common sense,
[00:44:42.420 --> 00:44:43.260]   for example?
[00:44:43.260 --> 00:44:45.460]   - Yeah, I don't know.
[00:44:45.460 --> 00:44:48.100]   And I don't even know what units to measure it in.
[00:44:48.100 --> 00:44:50.380]   You say how big is it?
[00:44:50.380 --> 00:44:51.980]   - That's beautifully put, right?
[00:44:51.980 --> 00:44:53.100]   What?
[00:44:53.100 --> 00:44:55.660]   - But we have, it's really hard to know.
[00:44:55.660 --> 00:45:00.660]   We have, what, 100 billion neurons or something,
[00:45:00.660 --> 00:45:04.300]   I don't know, and they're connected
[00:45:04.300 --> 00:45:07.860]   via trillions of synapses,
[00:45:07.860 --> 00:45:10.540]   and there's all this chemical processing going on.
[00:45:10.540 --> 00:45:13.740]   There's just a lot of capacity for stuff.
[00:45:13.740 --> 00:45:15.860]   And their information's encoded
[00:45:15.860 --> 00:45:17.180]   in different ways in the brain.
[00:45:17.180 --> 00:45:19.900]   It's encoded in chemical interactions,
[00:45:19.900 --> 00:45:24.220]   it's encoded in electric firing and firing rates.
[00:45:24.220 --> 00:45:25.780]   And nobody really knows how it's encoded,
[00:45:25.780 --> 00:45:29.020]   but it just seems like there's a huge amount of capacity.
[00:45:29.020 --> 00:45:32.460]   So I think it's huge, it's just enormous.
[00:45:32.460 --> 00:45:36.740]   And it's amazing how much stuff we know.
[00:45:36.740 --> 00:45:37.580]   - Yeah.
[00:45:37.580 --> 00:45:42.780]   But we know, and not just know like facts,
[00:45:42.780 --> 00:45:44.820]   but it's all integrated into this thing
[00:45:44.820 --> 00:45:46.500]   that we can make analogies with.
[00:45:46.500 --> 00:45:47.340]   - Yes.
[00:45:47.340 --> 00:45:49.300]   - There's a dream of semantic web,
[00:45:49.300 --> 00:45:52.980]   and there's a lot of dreams from expert systems
[00:45:52.980 --> 00:45:55.420]   of building giant knowledge bases.
[00:45:55.420 --> 00:45:58.940]   Do you see a hope for these kinds of approaches
[00:45:58.940 --> 00:46:01.860]   of converting Wikipedia into something
[00:46:01.860 --> 00:46:05.100]   that could be used in analogy making?
[00:46:05.100 --> 00:46:07.220]   - Sure.
[00:46:07.220 --> 00:46:09.540]   And I think people have made some progress
[00:46:09.540 --> 00:46:10.460]   along those lines.
[00:46:10.700 --> 00:46:13.300]   People have been working on this for a long time.
[00:46:13.300 --> 00:46:15.940]   But the problem is, and this I think
[00:46:15.940 --> 00:46:18.100]   is the problem of common sense.
[00:46:18.100 --> 00:46:20.980]   People have been trying to get these common sense networks.
[00:46:20.980 --> 00:46:24.100]   Here at MIT, there's this concept net project.
[00:46:24.100 --> 00:46:27.460]   But the problem is that, as I said,
[00:46:27.460 --> 00:46:31.860]   most of the knowledge that we have is invisible to us.
[00:46:31.860 --> 00:46:33.180]   It's not in Wikipedia.
[00:46:33.180 --> 00:46:34.860]   (both laughing)
[00:46:34.860 --> 00:46:38.540]   It's very basic things about
[00:46:39.540 --> 00:46:44.100]   intuitive physics, intuitive psychology,
[00:46:44.100 --> 00:46:47.260]   intuitive metaphysics, all that stuff.
[00:46:47.260 --> 00:46:49.220]   - If you were to create a website
[00:46:49.220 --> 00:46:53.500]   that described intuitive physics, intuitive psychology,
[00:46:53.500 --> 00:46:56.500]   would it be bigger or smaller than Wikipedia?
[00:46:56.500 --> 00:46:57.380]   What do you think?
[00:46:57.380 --> 00:47:00.740]   - I guess described to whom?
[00:47:00.740 --> 00:47:02.660]   (both laughing)
[00:47:02.660 --> 00:47:04.140]   I'm sorry, but--
[00:47:04.140 --> 00:47:05.380]   - No, that's really good.
[00:47:05.380 --> 00:47:07.060]   I think it's exactly right, yeah.
[00:47:07.060 --> 00:47:09.060]   That's a hard question because, you know,
[00:47:09.060 --> 00:47:12.100]   how do you represent that knowledge is the question, right?
[00:47:12.100 --> 00:47:15.780]   I can certainly write down F equals MA
[00:47:15.780 --> 00:47:19.700]   and Newton's laws and a lot of physics
[00:47:19.700 --> 00:47:21.180]   can be deduced from that.
[00:47:21.180 --> 00:47:27.060]   But that's probably not the best representation
[00:47:27.060 --> 00:47:32.060]   of that knowledge for doing the kinds of reasoning
[00:47:32.060 --> 00:47:33.580]   we want a machine to do.
[00:47:36.140 --> 00:47:40.340]   - So, I don't know, it's impossible to say now.
[00:47:40.340 --> 00:47:41.380]   (both laughing)
[00:47:41.380 --> 00:47:43.340]   And people, you know, the projects like,
[00:47:43.340 --> 00:47:46.460]   there's a famous psych project, right,
[00:47:46.460 --> 00:47:50.020]   that Douglas Linat did that was trying--
[00:47:50.020 --> 00:47:51.020]   - I think still going.
[00:47:51.020 --> 00:47:52.020]   - I think it's still going.
[00:47:52.020 --> 00:47:54.780]   And the idea was to try and encode
[00:47:54.780 --> 00:47:56.220]   all of common sense knowledge,
[00:47:56.220 --> 00:47:58.460]   including all this invisible knowledge
[00:47:58.460 --> 00:48:03.420]   in some kind of logical representation.
[00:48:03.420 --> 00:48:08.420]   And it just never, I think, could do any of the things
[00:48:08.420 --> 00:48:10.940]   that he was hoping it could do,
[00:48:10.940 --> 00:48:12.940]   because that's just the wrong approach.
[00:48:12.940 --> 00:48:16.740]   - Of course, that's what they always say, you know,
[00:48:16.740 --> 00:48:18.860]   and then the history books will say,
[00:48:18.860 --> 00:48:21.860]   well, the psych project finally found a breakthrough
[00:48:21.860 --> 00:48:24.420]   in 2058 or something.
[00:48:24.420 --> 00:48:27.380]   You know, so much progress has been made
[00:48:27.380 --> 00:48:28.700]   in just a few decades that--
[00:48:28.700 --> 00:48:29.540]   - Yeah, it could be.
[00:48:29.540 --> 00:48:31.940]   - Who knows what the next breakthroughs will be.
[00:48:31.940 --> 00:48:32.780]   - It could be.
[00:48:32.780 --> 00:48:34.700]   It's certainly a compelling notion,
[00:48:34.700 --> 00:48:36.460]   what the psych project stands for.
[00:48:36.460 --> 00:48:39.900]   - I think Linat was one of the earliest people
[00:48:39.900 --> 00:48:43.100]   to say common sense is what we need.
[00:48:43.100 --> 00:48:43.940]   - Important.
[00:48:43.940 --> 00:48:44.780]   - That's what we need.
[00:48:44.780 --> 00:48:46.980]   All this like expert system stuff,
[00:48:46.980 --> 00:48:49.140]   that is not gonna get you to AI.
[00:48:49.140 --> 00:48:50.420]   You need common sense.
[00:48:50.420 --> 00:48:55.420]   And he basically gave up his whole academic career
[00:48:55.420 --> 00:48:57.620]   to go pursue that.
[00:48:57.620 --> 00:48:59.380]   And I totally admire that,
[00:48:59.380 --> 00:49:03.540]   but I think that the approach itself will not,
[00:49:03.540 --> 00:49:08.980]   in 2020 or 2040 or wherever, be successful.
[00:49:08.980 --> 00:49:10.300]   - What do you think is wrong with the approach?
[00:49:10.300 --> 00:49:13.100]   What kind of approach might be successful?
[00:49:13.100 --> 00:49:15.460]   - Well, if I knew that.
[00:49:15.460 --> 00:49:16.940]   - Again, nobody knows the answer, right?
[00:49:16.940 --> 00:49:19.940]   - If I knew that, you know, one of my talks,
[00:49:19.940 --> 00:49:21.180]   one of the people in the audience,
[00:49:21.180 --> 00:49:22.220]   this was a public lecture,
[00:49:22.220 --> 00:49:24.220]   one of the people in the audience said,
[00:49:24.220 --> 00:49:27.020]   what AI companies are you investing in?
[00:49:27.020 --> 00:49:28.300]   (both laughing)
[00:49:28.300 --> 00:49:29.900]   - Investment advice, okay.
[00:49:29.900 --> 00:49:31.860]   - I'm a college professor for one thing,
[00:49:31.860 --> 00:49:34.740]   so I don't have a lot of extra funds to invest,
[00:49:34.740 --> 00:49:39.300]   but also, no one knows what's gonna work in AI, right?
[00:49:39.300 --> 00:49:40.340]   That's the problem.
[00:49:40.340 --> 00:49:43.100]   - Let me ask another impossible question
[00:49:43.100 --> 00:49:44.780]   in case you have a sense.
[00:49:44.780 --> 00:49:46.460]   In terms of data structures
[00:49:46.460 --> 00:49:49.540]   that will store this kind of information,
[00:49:49.540 --> 00:49:51.900]   do you think they've been invented yet,
[00:49:51.900 --> 00:49:53.620]   both in hardware and software?
[00:49:53.620 --> 00:49:57.020]   Or is something else needs to be,
[00:49:57.020 --> 00:49:58.260]   are we totally, you know?
[00:49:58.260 --> 00:50:00.460]   - I think something else has to be invented.
[00:50:00.460 --> 00:50:03.540]   That's my guess.
[00:50:03.540 --> 00:50:06.420]   - Is the breakthroughs that's most promising,
[00:50:06.420 --> 00:50:08.740]   would that be in hardware or in software?
[00:50:08.740 --> 00:50:12.660]   Do you think we can get far with the current computers?
[00:50:12.660 --> 00:50:14.220]   Or do we need to do something?
[00:50:14.220 --> 00:50:16.380]   - I see what you're saying.
[00:50:16.380 --> 00:50:19.860]   I don't know if Turing computation is gonna be sufficient.
[00:50:19.860 --> 00:50:22.020]   Probably, I would guess it will.
[00:50:22.020 --> 00:50:24.860]   I don't see any reason why we need anything else.
[00:50:25.980 --> 00:50:28.980]   So in that sense, we have invented the hardware we need,
[00:50:28.980 --> 00:50:31.860]   but we just need to make it faster and bigger.
[00:50:31.860 --> 00:50:34.260]   And we need to figure out the right algorithms
[00:50:34.260 --> 00:50:38.540]   and the right sort of architecture.
[00:50:38.540 --> 00:50:43.020]   - Turing, that's a very mathematical notion.
[00:50:43.020 --> 00:50:44.900]   When we have to build intelligence,
[00:50:44.900 --> 00:50:46.740]   it's now an engineering notion
[00:50:46.740 --> 00:50:48.260]   where you throw all that stuff.
[00:50:48.260 --> 00:50:51.060]   - Well, I guess it is a question.
[00:50:51.060 --> 00:50:55.260]   People have brought up this question,
[00:50:56.220 --> 00:50:58.980]   and when you asked about, is our current hardware,
[00:50:58.980 --> 00:51:02.220]   will our current hardware work?
[00:51:02.220 --> 00:51:06.980]   Well, Turing computation says that our current hardware
[00:51:06.980 --> 00:51:13.300]   is in principle a Turing machine, right?
[00:51:13.300 --> 00:51:16.500]   So all we have to do is make it faster and bigger.
[00:51:16.500 --> 00:51:20.220]   But there have been people like Roger Penrose,
[00:51:20.220 --> 00:51:22.500]   if you might remember that he said,
[00:51:22.500 --> 00:51:26.420]   "Turing machines cannot produce intelligence
[00:51:26.420 --> 00:51:30.540]   "because intelligence requires continuous valued numbers."
[00:51:30.540 --> 00:51:34.860]   I mean, that was sort of my reading of his argument
[00:51:34.860 --> 00:51:38.500]   and quantum mechanics and what else, whatever.
[00:51:38.500 --> 00:51:41.700]   But I don't see any evidence for that,
[00:51:41.700 --> 00:51:46.580]   that we need new computation paradigms.
[00:51:46.580 --> 00:51:50.460]   But I don't know if we're,
[00:51:50.460 --> 00:51:53.900]   I don't think we're gonna be able to scale up
[00:51:53.900 --> 00:51:58.420]   our current approaches to programming these computers.
[00:51:58.420 --> 00:52:00.780]   - What is your hope for approaches like CopyCat
[00:52:00.780 --> 00:52:02.700]   or other cognitive architectures?
[00:52:02.700 --> 00:52:04.660]   I've talked to the creator of Soar, for example.
[00:52:04.660 --> 00:52:06.020]   I've used Actar myself.
[00:52:06.020 --> 00:52:07.020]   I don't know if you're familiar with it.
[00:52:07.020 --> 00:52:07.860]   - Yeah, I am.
[00:52:07.860 --> 00:52:12.060]   - What's your hope of approaches like that
[00:52:12.060 --> 00:52:16.300]   in helping develop systems of greater and greater
[00:52:16.300 --> 00:52:18.540]   intelligence in the coming decades?
[00:52:19.980 --> 00:52:22.180]   - Well, that's what I'm working on now
[00:52:22.180 --> 00:52:26.100]   is trying to take some of those ideas and extending it.
[00:52:26.100 --> 00:52:30.140]   So I think there's some really promising approaches
[00:52:30.140 --> 00:52:34.140]   that are going on now that have to do with
[00:52:34.140 --> 00:52:39.140]   more active generative models.
[00:52:39.140 --> 00:52:41.980]   So this is the idea of this simulation
[00:52:41.980 --> 00:52:43.620]   in your head of a concept.
[00:52:43.620 --> 00:52:49.860]   When you're perceiving a new situation,
[00:52:49.860 --> 00:52:51.300]   you have some simulations in your head.
[00:52:51.300 --> 00:52:52.540]   Those are generative models.
[00:52:52.540 --> 00:52:54.580]   They're generating your expectations.
[00:52:54.580 --> 00:52:55.940]   They're generating predictions.
[00:52:55.940 --> 00:52:57.220]   - So that's part of a perception.
[00:52:57.220 --> 00:53:00.660]   You have a meta model that generates a prediction
[00:53:00.660 --> 00:53:03.580]   and you compare it with, and then the difference
[00:53:03.580 --> 00:53:05.180]   somehow forms. - And you also,
[00:53:05.180 --> 00:53:08.380]   that generative model is telling you where to look
[00:53:08.380 --> 00:53:11.620]   and what to look at and what to pay attention to.
[00:53:11.620 --> 00:53:14.060]   And I think it affects your perception.
[00:53:14.060 --> 00:53:16.820]   It's not that just you compare it with your perception.
[00:53:18.140 --> 00:53:21.900]   It becomes your perception in a way.
[00:53:21.900 --> 00:53:28.300]   It's kind of a mixture of the bottom-up information
[00:53:28.300 --> 00:53:31.860]   coming from the world and your top-down model
[00:53:31.860 --> 00:53:36.140]   being imposed on the world is what becomes your perception.
[00:53:36.140 --> 00:53:37.380]   - So your hope is something like that
[00:53:37.380 --> 00:53:39.620]   can improve perception systems
[00:53:39.620 --> 00:53:41.780]   and that they can understand things better.
[00:53:41.780 --> 00:53:42.900]   - Yes. - They understand things.
[00:53:42.900 --> 00:53:44.180]   - Yes.
[00:53:44.180 --> 00:53:49.180]   - What's the step, what's the analogy-making step there?
[00:53:49.180 --> 00:53:52.860]   - Well, there, the idea is that you have
[00:53:52.860 --> 00:53:57.100]   this pretty complicated conceptual space.
[00:53:57.100 --> 00:54:00.420]   You can talk about a semantic network or something like that
[00:54:00.420 --> 00:54:05.420]   with these different kinds of concept models in your brain
[00:54:05.420 --> 00:54:07.260]   that are connected.
[00:54:07.260 --> 00:54:10.860]   So let's take the example of walking a dog
[00:54:10.860 --> 00:54:12.380]   'cause we were talking about that.
[00:54:12.380 --> 00:54:15.220]   Okay, let's say I see someone out on the street
[00:54:15.220 --> 00:54:16.620]   walking a cat.
[00:54:16.620 --> 00:54:18.540]   Some people walk their cats, I guess.
[00:54:18.540 --> 00:54:19.860]   Seems like a bad idea, but.
[00:54:19.860 --> 00:54:21.740]   - Yeah. (laughs)
[00:54:21.740 --> 00:54:24.220]   - So my model, there's connections
[00:54:24.220 --> 00:54:28.860]   between my model of a dog and model of a cat.
[00:54:28.860 --> 00:54:32.460]   And I can immediately see the analogy
[00:54:32.460 --> 00:54:38.700]   that those are analogous situations.
[00:54:38.700 --> 00:54:40.820]   But I can also see the differences
[00:54:40.820 --> 00:54:43.260]   and that tells me what to expect.
[00:54:43.260 --> 00:54:48.260]   So also, I have a new situation.
[00:54:48.260 --> 00:54:51.260]   So another example with the walking the dog thing
[00:54:51.260 --> 00:54:54.540]   is sometimes people, I see people riding their bikes
[00:54:54.540 --> 00:54:55.740]   with a leash, holding a leash,
[00:54:55.740 --> 00:54:57.620]   and the dog's running alongside.
[00:54:57.620 --> 00:55:01.100]   Okay, so I know that the, I recognize that
[00:55:01.100 --> 00:55:03.900]   as kind of a dog walking situation
[00:55:03.900 --> 00:55:06.780]   even though the person's not walking, right?
[00:55:06.780 --> 00:55:08.460]   And the dog's not walking.
[00:55:08.460 --> 00:55:12.780]   Because I have these models that say,
[00:55:12.780 --> 00:55:16.580]   okay, riding a bike is sort of similar to walking
[00:55:16.580 --> 00:55:20.180]   or it's connected, it's a means of transportation.
[00:55:20.180 --> 00:55:22.860]   But I, because they have their dog there,
[00:55:22.860 --> 00:55:24.420]   I assume they're not going to work,
[00:55:24.420 --> 00:55:26.380]   but they're going out for exercise.
[00:55:26.380 --> 00:55:30.260]   And these analogies help me to figure out
[00:55:30.260 --> 00:55:33.180]   kind of what's going on, what's likely.
[00:55:33.180 --> 00:55:36.420]   - But sort of these analogies are very human interpretable.
[00:55:37.300 --> 00:55:39.020]   So that's that kind of space.
[00:55:39.020 --> 00:55:41.460]   And then you look at something like
[00:55:41.460 --> 00:55:43.460]   the current deep learning approaches,
[00:55:43.460 --> 00:55:46.700]   they kind of help you to take raw sensory information
[00:55:46.700 --> 00:55:49.460]   and to sort of automatically build up hierarchies
[00:55:49.460 --> 00:55:53.020]   of what you can even call them concepts.
[00:55:53.020 --> 00:55:55.620]   They're just not human interpretable concepts.
[00:55:55.620 --> 00:55:58.660]   What's your, what's the link here?
[00:55:58.660 --> 00:56:03.660]   Do you hope, it's sort of the hybrid system question.
[00:56:05.780 --> 00:56:08.220]   How do you think the two can start to meet each other?
[00:56:08.220 --> 00:56:13.220]   What's the value of learning in this systems of forming,
[00:56:13.220 --> 00:56:16.020]   of analogy making?
[00:56:16.020 --> 00:56:20.580]   - The goal of, you know, the original goal of deep learning
[00:56:20.580 --> 00:56:24.260]   in at least visual perception was that
[00:56:24.260 --> 00:56:27.300]   you would get the system to learn to extract features
[00:56:27.300 --> 00:56:30.340]   that at these different levels of complexities.
[00:56:30.340 --> 00:56:33.340]   It may be edge detection, and that would lead into
[00:56:33.340 --> 00:56:36.660]   learning simple combinations of edges,
[00:56:36.660 --> 00:56:38.100]   and then more complex shapes,
[00:56:38.100 --> 00:56:41.540]   and then whole objects or faces.
[00:56:41.540 --> 00:56:47.740]   And this was based on the ideas of the neuroscientists,
[00:56:47.740 --> 00:56:52.860]   Hubel and Wiesel, who had seen,
[00:56:52.860 --> 00:56:55.300]   laid out this kind of structure in brain.
[00:56:55.300 --> 00:57:02.020]   And I think that's right to some extent.
[00:57:02.020 --> 00:57:05.860]   Of course, people have found that the whole story
[00:57:05.860 --> 00:57:07.340]   is a little more complex than that,
[00:57:07.340 --> 00:57:09.140]   and the brain, of course, always is,
[00:57:09.140 --> 00:57:10.660]   and there's a lot of feedback.
[00:57:10.660 --> 00:57:17.020]   So I see that as absolutely a good brain-inspired approach
[00:57:17.020 --> 00:57:25.660]   to some aspects of perception.
[00:57:25.660 --> 00:57:29.460]   But one thing that it's lacking, for example,
[00:57:29.460 --> 00:57:33.300]   is all of that feedback, which is extremely important.
[00:57:33.300 --> 00:57:35.500]   - The interactive element that you mentioned.
[00:57:35.500 --> 00:57:39.020]   - The expectation, right, the conceptual level.
[00:57:39.020 --> 00:57:42.180]   - Going back and forth with the expectation
[00:57:42.180 --> 00:57:44.220]   and the perception, and just going back and forth.
[00:57:44.220 --> 00:57:47.980]   - So, right, so that is extremely important.
[00:57:47.980 --> 00:57:52.220]   And, you know, one thing about deep neural networks
[00:57:52.220 --> 00:57:55.380]   is that in a given situation, like, you know,
[00:57:55.380 --> 00:57:57.820]   they're trained, right, they get these weights
[00:57:57.820 --> 00:58:01.220]   and everything, but then now I give them a new image,
[00:58:01.220 --> 00:58:02.420]   let's say.
[00:58:02.420 --> 00:58:07.420]   They treat every part of the image in the same way.
[00:58:07.420 --> 00:58:13.540]   You know, they apply the same filters at each layer
[00:58:13.540 --> 00:58:15.900]   to all parts of the image.
[00:58:15.900 --> 00:58:17.580]   There's no feedback to say, like,
[00:58:17.580 --> 00:58:19.940]   oh, this part of the image is irrelevant.
[00:58:19.940 --> 00:58:23.060]   I shouldn't care about this part of the image.
[00:58:23.060 --> 00:58:26.060]   Or this part of the image is the most important part.
[00:58:27.020 --> 00:58:30.140]   And that's kind of what we humans are able to do
[00:58:30.140 --> 00:58:33.420]   because we have these conceptual expectations.
[00:58:33.420 --> 00:58:35.580]   - There's, by the way, a little bit of work in that.
[00:58:35.580 --> 00:58:38.940]   There's certainly a lot more in what's under the--
[00:58:38.940 --> 00:58:40.260]   - Attention. - Called attention
[00:58:40.260 --> 00:58:42.060]   in natural language processing now.
[00:58:42.060 --> 00:58:48.500]   That's exceptionally powerful, and it's a very,
[00:58:48.500 --> 00:58:50.660]   just as you say, it's a really powerful idea.
[00:58:50.660 --> 00:58:53.380]   But again, in sort of machine learning,
[00:58:53.380 --> 00:58:55.740]   it all kind of operates in an automated way
[00:58:55.740 --> 00:58:56.580]   that's not human--
[00:58:56.580 --> 00:58:59.380]   - It's not, okay, so, right.
[00:58:59.380 --> 00:59:01.380]   It's not dynamic, I mean, in the sense that
[00:59:01.380 --> 00:59:06.380]   as a perception of a new example is being processed,
[00:59:06.380 --> 00:59:10.900]   those attention's weights don't change.
[00:59:10.900 --> 00:59:17.540]   - Right, so, I mean, there's a kind of notion
[00:59:17.540 --> 00:59:20.380]   that there's not a memory.
[00:59:20.380 --> 00:59:22.860]   So you're not aggregating.
[00:59:22.860 --> 00:59:25.060]   The idea of this mental model.
[00:59:25.060 --> 00:59:25.900]   - Yes. - Yeah.
[00:59:26.580 --> 00:59:28.620]   I mean, that seems to be a fundamental idea.
[00:59:28.620 --> 00:59:30.980]   There's not a really powerful,
[00:59:30.980 --> 00:59:32.380]   I mean, there's some stuff with memory,
[00:59:32.380 --> 00:59:37.380]   but there's not a powerful way to represent the world
[00:59:37.380 --> 00:59:42.340]   in some sort of way that's deeper than,
[00:59:42.340 --> 00:59:46.180]   I mean, it's so difficult because neural networks
[00:59:46.180 --> 00:59:47.620]   do represent the world.
[00:59:47.620 --> 00:59:50.900]   They do have a mental model, right?
[00:59:50.900 --> 00:59:53.040]   But it just seems to be shallow.
[00:59:55.180 --> 00:59:59.500]   It's hard to criticize them at the fundamental level,
[00:59:59.500 --> 01:00:01.660]   to me at least.
[01:00:01.660 --> 01:00:05.180]   It's easy to criticize them,
[01:00:05.180 --> 01:00:07.180]   well, look, like exactly what you're saying,
[01:00:07.180 --> 01:00:11.660]   mental models sort of almost put a psychology hat on,
[01:00:11.660 --> 01:00:15.860]   say, look, these networks are clearly not able to achieve
[01:00:15.860 --> 01:00:18.060]   what we humans do with forming mental models,
[01:00:18.060 --> 01:00:20.060]   the analogy making, so on.
[01:00:20.060 --> 01:00:22.020]   But that doesn't mean that they fundamentally
[01:00:22.020 --> 01:00:23.300]   cannot do that.
[01:00:24.260 --> 01:00:26.580]   It's very difficult to say that, I mean, at least to me.
[01:00:26.580 --> 01:00:29.860]   Do you have a notion that the learning approaches really,
[01:00:29.860 --> 01:00:33.980]   I mean, they're going to, not only are they limited today,
[01:00:33.980 --> 01:00:38.100]   but they will forever be limited in being able
[01:00:38.100 --> 01:00:41.460]   to construct such mental models.
[01:00:41.460 --> 01:00:47.380]   - I think the idea of the dynamic perception is key here.
[01:00:47.380 --> 01:00:53.800]   The idea that moving your eyes around
[01:00:53.800 --> 01:00:57.900]   and getting feedback, and that's something that,
[01:00:57.900 --> 01:01:00.340]   there's been some models like that.
[01:01:00.340 --> 01:01:02.620]   There's certainly recurrent neural networks
[01:01:02.620 --> 01:01:04.760]   that operate over several time steps.
[01:01:04.760 --> 01:01:10.880]   But the problem is that the actual, the recurrence is,
[01:01:10.880 --> 01:01:18.420]   basically the feedback is, at the next time step,
[01:01:18.420 --> 01:01:23.420]   is the entire hidden state of the network,
[01:01:23.500 --> 01:01:28.500]   which is, and it turns out that that doesn't work very well.
[01:01:28.500 --> 01:01:34.300]   - But see, the thing I'm saying is, mathematically speaking,
[01:01:34.300 --> 01:01:38.700]   it has the information in that recurrence
[01:01:38.700 --> 01:01:39.860]   to capture everything.
[01:01:39.860 --> 01:01:41.380]   It just doesn't seem to work.
[01:01:41.380 --> 01:01:42.540]   - Yeah, right.
[01:01:42.540 --> 01:01:47.440]   - So, it's like, it's the same Turing machine question,
[01:01:47.440 --> 01:01:51.980]   right, yeah, maybe theoretically,
[01:01:53.140 --> 01:01:56.980]   computers, anything that's Turing,
[01:01:56.980 --> 01:01:59.740]   a universal Turing machine can be intelligent,
[01:01:59.740 --> 01:02:03.220]   but practically, the architecture might be
[01:02:03.220 --> 01:02:04.740]   a very specific kind of architecture
[01:02:04.740 --> 01:02:05.940]   to be able to create it.
[01:02:05.940 --> 01:02:08.700]   So, it's just, I guess it sort of asks
[01:02:08.700 --> 01:02:10.180]   almost the same question again,
[01:02:10.180 --> 01:02:14.380]   is how big of a role do you think deep learning
[01:02:14.380 --> 01:02:19.380]   needs, will play, or needs to play in this, in perception?
[01:02:21.140 --> 01:02:26.140]   - I think that deep learning as it currently exists,
[01:02:26.140 --> 01:02:30.340]   that kind of thing will play some role,
[01:02:30.340 --> 01:02:36.820]   but I think that there's a lot more going on in perception.
[01:02:36.820 --> 01:02:40.100]   But who knows, the definition of deep learning,
[01:02:40.100 --> 01:02:42.020]   I mean, it's pretty broad.
[01:02:42.020 --> 01:02:43.700]   It's kind of an umbrella for a lot of different things.
[01:02:43.700 --> 01:02:46.420]   - So, what I mean is purely sort of neural networks.
[01:02:46.420 --> 01:02:48.580]   - Yeah, and a feed-forward neural networks.
[01:02:48.580 --> 01:02:50.780]   - Essentially, or there could be recurrence,
[01:02:50.780 --> 01:02:54.300]   but sometimes it feels like,
[01:02:54.300 --> 01:02:56.260]   for instance, I talked to Gary Marcus,
[01:02:56.260 --> 01:02:58.940]   it feels like the criticism of deep learning
[01:02:58.940 --> 01:03:03.020]   is kind of like us birds criticizing airplanes
[01:03:03.020 --> 01:03:07.740]   for not flying well, or that they're not really flying.
[01:03:07.740 --> 01:03:10.660]   Do you think deep learning,
[01:03:10.660 --> 01:03:12.860]   do you think it could go all the way,
[01:03:12.860 --> 01:03:15.140]   like John Licklund thinks?
[01:03:15.140 --> 01:03:20.140]   Do you think that, yeah, the brute force learning approach
[01:03:20.500 --> 01:03:21.860]   can go all the way?
[01:03:21.860 --> 01:03:23.100]   - I don't think so, no.
[01:03:23.100 --> 01:03:25.460]   I mean, I think it's an open question,
[01:03:25.460 --> 01:03:30.060]   but I tend to be on the innateness side
[01:03:30.060 --> 01:03:35.060]   that there's some things that we've been evolved
[01:03:35.060 --> 01:03:39.660]   to be able to learn,
[01:03:39.660 --> 01:03:44.580]   and that learning just can't happen without them.
[01:03:44.580 --> 01:03:48.100]   So, one example, here's an example I had in the book
[01:03:48.100 --> 01:03:50.620]   that I think is useful to me, at least,
[01:03:50.620 --> 01:03:51.460]   in thinking about this.
[01:03:51.460 --> 01:03:56.180]   So, this has to do with the DeepMind's
[01:03:56.180 --> 01:03:59.340]   Atari game-playing program, okay?
[01:03:59.340 --> 01:04:02.780]   And it learned to play these Atari video games
[01:04:02.780 --> 01:04:07.780]   just by getting input from the pixels of the screen,
[01:04:07.780 --> 01:04:12.940]   and it learned to play the game Breakout
[01:04:15.780 --> 01:04:18.220]   1,000% better than humans, okay?
[01:04:18.220 --> 01:04:20.460]   That was one of their results, and it was great.
[01:04:20.460 --> 01:04:22.980]   And it learned this thing where it tunneled
[01:04:22.980 --> 01:04:26.460]   through the side of the bricks in the Breakout game,
[01:04:26.460 --> 01:04:28.700]   and the ball could bounce off the ceiling,
[01:04:28.700 --> 01:04:30.740]   and then just wipe out bricks.
[01:04:30.740 --> 01:04:35.740]   Okay, so there was a group who did an experiment
[01:04:35.740 --> 01:04:41.540]   where they took the paddle that you move with the joystick
[01:04:41.540 --> 01:04:45.660]   and moved it up two pixels or something like that.
[01:04:45.660 --> 01:04:50.500]   And then they looked at a deep Q-learning system
[01:04:50.500 --> 01:04:52.140]   that had been trained on Breakout and said,
[01:04:52.140 --> 01:04:53.900]   "Could it now transfer its learning
[01:04:53.900 --> 01:04:55.860]   to this new version of the game?"
[01:04:55.860 --> 01:04:59.660]   Of course, a human could, but, and it couldn't.
[01:04:59.660 --> 01:05:01.500]   Maybe that's not surprising, but I guess the point is
[01:05:01.500 --> 01:05:05.220]   it hadn't learned the concept of a paddle.
[01:05:05.220 --> 01:05:08.140]   It hadn't learned the concept of a ball
[01:05:08.140 --> 01:05:09.460]   or the concept of tunneling.
[01:05:09.460 --> 01:05:11.260]   It was learning something, you know,
[01:05:11.860 --> 01:05:16.660]   we, looking at it, kind of anthropomorphized it and said,
[01:05:16.660 --> 01:05:19.220]   "Oh, here's what it's doing in the way we describe it."
[01:05:19.220 --> 01:05:21.500]   But it actually didn't learn those concepts.
[01:05:21.500 --> 01:05:24.020]   And so because it didn't learn those concepts,
[01:05:24.020 --> 01:05:26.940]   it couldn't make this transfer.
[01:05:26.940 --> 01:05:28.980]   - Yeah, so that's a beautiful statement,
[01:05:28.980 --> 01:05:31.640]   but at the same time, by moving the paddle,
[01:05:31.640 --> 01:05:36.460]   we also anthropomorphize flaws to inject into the system
[01:05:36.460 --> 01:05:40.020]   that will then flip how impressed we are by it.
[01:05:40.020 --> 01:05:43.740]   What I mean by that is, to me, the Atari games
[01:05:43.740 --> 01:05:48.740]   were to me deeply impressive that that was possible at all.
[01:05:48.740 --> 01:05:50.780]   So like I have to first pause on that
[01:05:50.780 --> 01:05:53.780]   and people should look at that, just like the game of Go,
[01:05:53.780 --> 01:05:56.060]   which is fundamentally different to me
[01:05:56.060 --> 01:05:59.640]   than what Deep Blue did.
[01:05:59.640 --> 01:06:01.740]   Even though there's still Monte Carlo,
[01:06:01.740 --> 01:06:03.300]   there's still Tree Search.
[01:06:03.300 --> 01:06:07.260]   It's just everything that DeepMind has done
[01:06:07.260 --> 01:06:10.500]   in terms of learning, however limited it is,
[01:06:10.500 --> 01:06:12.100]   is still deeply surprising to me.
[01:06:12.100 --> 01:06:14.540]   - Yeah, I'm not trying to say
[01:06:14.540 --> 01:06:16.540]   that what they did wasn't impressive.
[01:06:16.540 --> 01:06:18.180]   I think it was incredibly impressive.
[01:06:18.180 --> 01:06:19.940]   - To me, it's interesting.
[01:06:19.940 --> 01:06:24.100]   Is moving the board just another thing
[01:06:24.100 --> 01:06:25.100]   that needs to be learned?
[01:06:25.100 --> 01:06:28.340]   So like we've been able to, maybe, maybe,
[01:06:28.340 --> 01:06:30.180]   been able to, through the current neural networks,
[01:06:30.180 --> 01:06:32.860]   learn very basic concepts that are not enough
[01:06:32.860 --> 01:06:34.500]   to do this general reasoning.
[01:06:34.740 --> 01:06:39.740]   And maybe with more data, I mean the data,
[01:06:39.740 --> 01:06:42.380]   the interesting thing about the examples
[01:06:42.380 --> 01:06:45.060]   that you talk about beautifully
[01:06:45.060 --> 01:06:49.020]   is it's often flaws of the data.
[01:06:49.020 --> 01:06:49.980]   - Well, that's the question.
[01:06:49.980 --> 01:06:52.220]   I mean, I think that is the key question,
[01:06:52.220 --> 01:06:53.860]   whether it's a flaw of the data or not.
[01:06:53.860 --> 01:06:54.700]   - Or the mechanics.
[01:06:54.700 --> 01:06:57.100]   - Because the reason I brought up this example
[01:06:57.100 --> 01:06:59.300]   was 'cause you were asking, do I think that
[01:06:59.300 --> 01:07:02.580]   learning from data could go all the way?
[01:07:02.580 --> 01:07:04.900]   And this was why I brought up the example,
[01:07:04.900 --> 01:07:08.260]   because I think, and this is not at all
[01:07:08.260 --> 01:07:12.260]   to take away from the impressive work that they did,
[01:07:12.260 --> 01:07:14.340]   but it's to say that when we look
[01:07:14.340 --> 01:07:16.100]   at what these systems learn,
[01:07:16.100 --> 01:07:23.460]   do they learn the human, the things that we humans
[01:07:23.460 --> 01:07:25.740]   consider to be the relevant concepts?
[01:07:25.740 --> 01:07:29.780]   And in that example, it didn't.
[01:07:29.780 --> 01:07:33.620]   Sure, if you train it on moving,
[01:07:33.620 --> 01:07:36.300]   the paddle being in different places,
[01:07:36.300 --> 01:07:41.180]   maybe it could deal with, maybe it would learn that concept.
[01:07:41.180 --> 01:07:44.460]   I'm not totally sure, but the question is,
[01:07:44.460 --> 01:07:47.460]   scaling that up to more complicated worlds,
[01:07:47.460 --> 01:07:51.820]   to what extent could a machine
[01:07:51.820 --> 01:07:54.820]   that only gets this very raw data
[01:07:54.820 --> 01:07:59.540]   learn to divide up the world into relevant concepts?
[01:07:59.540 --> 01:08:04.020]   And I don't know the answer, but I would bet
[01:08:04.020 --> 01:08:09.020]   that without some innate notion, that it can't do it.
[01:08:09.020 --> 01:08:13.420]   - Yeah, 10 years ago, I 100% agree with you
[01:08:13.420 --> 01:08:16.100]   as the most experts in AI system,
[01:08:16.100 --> 01:08:20.140]   but now I have a glimmer of hope.
[01:08:20.140 --> 01:08:21.820]   - Okay, that's fair enough.
[01:08:21.820 --> 01:08:23.700]   - And I think that's what deep learning did
[01:08:23.700 --> 01:08:25.300]   in the community, is, no, no, no,
[01:08:25.300 --> 01:08:26.940]   I still, if I had to bet all my money,
[01:08:27.140 --> 01:08:30.060]   100% deep learning will not take us all the way,
[01:08:30.060 --> 01:08:35.060]   but there's still, I was so personally sort of surprised
[01:08:35.060 --> 01:08:39.900]   by the Atari games, by Go, by the power of self-play,
[01:08:39.900 --> 01:08:42.180]   of just game playing against each other,
[01:08:42.180 --> 01:08:44.980]   that I was, like many other times,
[01:08:44.980 --> 01:08:46.860]   just humbled of how little I know
[01:08:46.860 --> 01:08:50.060]   about what's possible in this way of approaching it.
[01:08:50.060 --> 01:08:51.740]   - Yeah, I think, fair enough.
[01:08:51.740 --> 01:08:54.220]   Self-play is amazingly powerful.
[01:08:54.260 --> 01:08:58.700]   And that goes way back to Arthur Samuel, right,
[01:08:58.700 --> 01:09:01.140]   with his checker playing program,
[01:09:01.140 --> 01:09:03.540]   and that which was brilliant,
[01:09:03.540 --> 01:09:05.940]   and surprising that it did so well.
[01:09:05.940 --> 01:09:09.700]   - So just for fun, let me ask you
[01:09:09.700 --> 01:09:11.340]   on the topic of autonomous vehicles.
[01:09:11.340 --> 01:09:15.340]   It's the area that I work, at least these days,
[01:09:15.340 --> 01:09:18.100]   most closely on, and it's also area
[01:09:18.100 --> 01:09:20.780]   that I think is a good example that you use
[01:09:20.780 --> 01:09:25.340]   as sort of an example of things we, as humans,
[01:09:25.340 --> 01:09:28.660]   don't always realize how hard it is to do.
[01:09:28.660 --> 01:09:30.900]   It's like the constant trend in AI,
[01:09:30.900 --> 01:09:32.900]   or the different problems that we think are easy
[01:09:32.900 --> 01:09:36.460]   when we first try them, and then realize how hard it is.
[01:09:36.460 --> 01:09:41.460]   Okay, so why, you've talked about this,
[01:09:41.460 --> 01:09:43.540]   autonomous driving being a difficult problem,
[01:09:43.540 --> 01:09:46.860]   more difficult than we realize, humans give it credit for.
[01:09:46.860 --> 01:09:48.140]   Why is it so difficult?
[01:09:48.140 --> 01:09:50.540]   What are the most difficult parts, in your view?
[01:09:50.540 --> 01:09:55.780]   - I think it's difficult because of the world
[01:09:55.780 --> 01:10:00.180]   is so open-ended as to what kinds of things can happen.
[01:10:00.180 --> 01:10:05.180]   So you have sort of what normally happens,
[01:10:05.180 --> 01:10:07.380]   which is just you drive along,
[01:10:07.380 --> 01:10:10.580]   and nothing surprising happens,
[01:10:10.580 --> 01:10:13.420]   and autonomous vehicles can do,
[01:10:13.420 --> 01:10:15.500]   the ones we have now, evidently,
[01:10:15.500 --> 01:10:19.540]   can do really well on most normal situations,
[01:10:19.540 --> 01:10:23.340]   as long as the weather is reasonably good and everything.
[01:10:23.340 --> 01:10:28.900]   But if some, we have this notion of edge case,
[01:10:28.900 --> 01:10:32.700]   or things in the tail of the distribution,
[01:10:32.700 --> 01:10:34.740]   people call it the long tail problem,
[01:10:34.740 --> 01:10:37.900]   which says that there's so many possible things
[01:10:37.900 --> 01:10:42.100]   that can happen that was not in the training data
[01:10:42.100 --> 01:10:47.100]   of the machine that it won't be able to handle it
[01:10:47.100 --> 01:10:50.900]   because it doesn't have common sense.
[01:10:50.900 --> 01:10:54.700]   - Right, it's the old, the paddle moved problem.
[01:10:54.700 --> 01:10:57.860]   - Yeah, it's the paddle moved problem, right.
[01:10:57.860 --> 01:10:59.180]   And so my understanding,
[01:10:59.180 --> 01:11:02.140]   and you probably are more of an expert than I am on this,
[01:11:02.140 --> 01:11:07.140]   is that current self-driving car vision systems
[01:11:07.140 --> 01:11:10.420]   have problems with obstacles,
[01:11:10.460 --> 01:11:13.900]   meaning that they don't know which obstacles,
[01:11:13.900 --> 01:11:16.620]   which quote-unquote obstacles they should stop for
[01:11:16.620 --> 01:11:18.580]   and which ones they shouldn't stop for.
[01:11:18.580 --> 01:11:22.260]   And so a lot of times I read that they tend to slam
[01:11:22.260 --> 01:11:23.900]   on the brakes quite a bit,
[01:11:23.900 --> 01:11:28.140]   and the most common accidents with self-driving cars
[01:11:28.140 --> 01:11:30.500]   are people rear-ending them,
[01:11:30.500 --> 01:11:32.180]   'cause they were surprised,
[01:11:32.180 --> 01:11:35.740]   they weren't expecting the car to stop.
[01:11:35.740 --> 01:11:39.100]   - Yeah, so there's a lot of interesting questions there,
[01:11:39.100 --> 01:11:42.900]   whether, 'cause you mentioned kind of two things.
[01:11:42.900 --> 01:11:45.100]   So one is the problem of perception,
[01:11:45.100 --> 01:11:49.500]   of understanding, of interpreting the objects
[01:11:49.500 --> 01:11:51.540]   that are detected correctly.
[01:11:51.540 --> 01:11:54.380]   And the other one is more like the policy,
[01:11:54.380 --> 01:11:57.740]   the action that you take, how you respond to it.
[01:11:57.740 --> 01:12:02.460]   So a lot of the cars braking is a kind of notion of,
[01:12:02.460 --> 01:12:06.380]   to clarify it, there's a lot of different kind of things
[01:12:06.380 --> 01:12:07.940]   that are people calling autonomous vehicles,
[01:12:07.940 --> 01:12:11.780]   but the L4 vehicles with a safety driver
[01:12:11.780 --> 01:12:15.780]   are the ones like Waymo and Cruise and those companies,
[01:12:15.780 --> 01:12:18.660]   they tend to be very conservative and cautious.
[01:12:18.660 --> 01:12:21.260]   So they tend to be very, very afraid
[01:12:21.260 --> 01:12:22.980]   of hurting anything or anyone
[01:12:22.980 --> 01:12:24.940]   and getting in any kind of accidents.
[01:12:24.940 --> 01:12:27.900]   So their policy is very kind of,
[01:12:27.900 --> 01:12:31.100]   that results in being exceptionally responsive
[01:12:31.100 --> 01:12:33.620]   to anything that could possibly be an obstacle, right?
[01:12:33.620 --> 01:12:37.260]   - Right, which the human drivers around it,
[01:12:37.260 --> 01:12:41.660]   it's unpredictable, it behaves unpredictably.
[01:12:41.660 --> 01:12:44.100]   - Yeah, that's not a very human thing to do, caution.
[01:12:44.100 --> 01:12:46.580]   That's not the thing we're good at, especially in driving.
[01:12:46.580 --> 01:12:49.780]   We're in a hurry, often angry and et cetera,
[01:12:49.780 --> 01:12:50.900]   especially in Boston.
[01:12:50.900 --> 01:12:53.900]   So, and then there's sort of another,
[01:12:53.900 --> 01:12:55.940]   and a lot of times that's,
[01:12:55.940 --> 01:12:58.260]   machine learning is not a huge part of that.
[01:12:58.260 --> 01:13:00.500]   It's becoming more and more unclear to me
[01:13:00.540 --> 01:13:05.540]   how much, sort of speaking to public information,
[01:13:05.540 --> 01:13:09.220]   because a lot of companies say they're doing deep learning
[01:13:09.220 --> 01:13:12.580]   and machine learning just to attract good candidates.
[01:13:12.580 --> 01:13:14.780]   The reality is in many cases,
[01:13:14.780 --> 01:13:18.540]   it's still not a huge part of the perception.
[01:13:18.540 --> 01:13:20.460]   There's LIDAR and there's other sensors
[01:13:20.460 --> 01:13:23.900]   that are much more reliable for obstacle detection.
[01:13:23.900 --> 01:13:27.940]   And then there's Tesla approach, which is vision only.
[01:13:27.940 --> 01:13:30.860]   And there's, I think a few companies doing that,
[01:13:30.860 --> 01:13:33.420]   but Tesla most sort of famously pushing that forward.
[01:13:33.420 --> 01:13:36.140]   - And that's because the LIDAR is too expensive, right?
[01:13:36.140 --> 01:13:41.140]   - Well, I mean, yes, but I would say
[01:13:41.140 --> 01:13:45.260]   if you were to for free give to every Tesla vehicle,
[01:13:45.260 --> 01:13:47.660]   I mean, Elon Musk fundamentally believes
[01:13:47.660 --> 01:13:49.260]   that LIDAR is a crutch, right?
[01:13:49.260 --> 01:13:50.820]   Fantasy said that.
[01:13:50.820 --> 01:13:55.620]   That if you want to solve the problem with machine learning,
[01:13:55.620 --> 01:14:00.620]   LIDAR should not be the primary sensor is the belief.
[01:14:00.620 --> 01:14:04.220]   The camera contains a lot more information.
[01:14:04.220 --> 01:14:08.480]   So if you want to learn, you want that information.
[01:14:08.480 --> 01:14:13.700]   But if you want to not to hit obstacles, you want LIDAR.
[01:14:13.700 --> 01:14:16.340]   Right, it's sort of, it's this weird trade-off
[01:14:16.340 --> 01:14:21.340]   because yeah, so what Tesla vehicles have a lot of,
[01:14:21.700 --> 01:14:26.700]   which is really the thing, the fallback,
[01:14:26.700 --> 01:14:29.740]   the primary fallback sensor is radar,
[01:14:29.740 --> 01:14:32.420]   which is a very crude version of LIDAR.
[01:14:32.420 --> 01:14:35.060]   It's a good detector of obstacles,
[01:14:35.060 --> 01:14:38.140]   except when those things are standing, right?
[01:14:38.140 --> 01:14:39.980]   The stopped vehicle.
[01:14:39.980 --> 01:14:41.380]   - Right, that's why it had problems
[01:14:41.380 --> 01:14:43.620]   with crashing into stopped fire trucks.
[01:14:43.620 --> 01:14:44.900]   - Stopped fire trucks, right?
[01:14:44.900 --> 01:14:47.860]   So the hope there is that the vision sensor
[01:14:47.860 --> 01:14:50.420]   would somehow catch that and infer.
[01:14:50.420 --> 01:14:52.660]   So there's a lot of problems with perception.
[01:14:52.660 --> 01:14:58.500]   They are doing actually some incredible stuff in the,
[01:14:58.500 --> 01:15:02.500]   almost like an active learning space
[01:15:02.500 --> 01:15:06.940]   where it's constantly taking edge cases and pulling back in.
[01:15:06.940 --> 01:15:08.780]   There's this data pipeline.
[01:15:08.780 --> 01:15:13.060]   Another aspect that is really important
[01:15:13.060 --> 01:15:15.860]   that people are studying now is called multitask learning,
[01:15:15.860 --> 01:15:18.420]   which is sort of breaking apart this problem,
[01:15:18.420 --> 01:15:20.620]   whatever the problem is, in this case, driving,
[01:15:20.620 --> 01:15:24.460]   into dozens or hundreds of little problems
[01:15:24.460 --> 01:15:26.260]   that you can turn into learning problems.
[01:15:26.260 --> 01:15:30.300]   So this giant pipeline, it's kind of interesting.
[01:15:30.300 --> 01:15:33.340]   I've been skeptical from the very beginning,
[01:15:33.340 --> 01:15:35.540]   but become less and less skeptical over time
[01:15:35.540 --> 01:15:37.580]   how much of driving can be learned.
[01:15:37.580 --> 01:15:42.060]   I still think it's much farther than the CEO
[01:15:42.060 --> 01:15:44.700]   of that particular company thinks it will be,
[01:15:44.700 --> 01:15:48.140]   but it's constantly surprising
[01:15:48.140 --> 01:15:51.740]   that through good engineering and data collection
[01:15:51.740 --> 01:15:54.180]   and active selection of data,
[01:15:54.180 --> 01:15:56.660]   how you can attack that long tail.
[01:15:56.660 --> 01:15:58.940]   And it's an interesting open question
[01:15:58.940 --> 01:16:00.140]   that you're absolutely right.
[01:16:00.140 --> 01:16:03.060]   There's a much longer tail and all these edge cases
[01:16:03.060 --> 01:16:04.620]   that we don't think about,
[01:16:04.620 --> 01:16:06.500]   but it's a fascinating question
[01:16:06.500 --> 01:16:09.340]   that applies to natural language and all spaces.
[01:16:09.340 --> 01:16:12.100]   How big is that long tail?
[01:16:13.060 --> 01:16:16.940]   And I mean, not to linger on the point,
[01:16:16.940 --> 01:16:19.220]   but what's your sense in driving
[01:16:19.220 --> 01:16:23.780]   in these practical problems of the human experience?
[01:16:23.780 --> 01:16:26.780]   Can it be learned?
[01:16:26.780 --> 01:16:28.180]   So the current, what are your thoughts
[01:16:28.180 --> 01:16:30.620]   of sort of Elon Musk thought,
[01:16:30.620 --> 01:16:32.060]   let's forget the thing that he says
[01:16:32.060 --> 01:16:33.620]   it'd be solved in a year,
[01:16:33.620 --> 01:16:38.620]   but can it be solved in a reasonable timeline
[01:16:38.620 --> 01:16:42.020]   or do fundamentally other methods need to be invented?
[01:16:42.020 --> 01:16:47.020]   - So I don't, I think that ultimately driving,
[01:16:47.020 --> 01:16:49.980]   so it's a trade-off in a way,
[01:16:49.980 --> 01:16:55.220]   being able to drive and deal with any situation
[01:16:55.220 --> 01:17:00.220]   that comes up does require kind of full human intelligence
[01:17:00.220 --> 01:17:03.300]   and even in humans aren't intelligent enough to do it
[01:17:03.300 --> 01:17:06.220]   'cause humans, I mean, most human accidents
[01:17:06.220 --> 01:17:10.020]   are because the human wasn't paying attention
[01:17:10.020 --> 01:17:12.420]   or the humans drunk or whatever.
[01:17:12.420 --> 01:17:14.180]   - And not because they weren't intelligent enough.
[01:17:14.180 --> 01:17:16.900]   - And not because they weren't intelligent enough, right.
[01:17:16.900 --> 01:17:23.340]   Whereas the accidents with autonomous vehicles
[01:17:23.340 --> 01:17:25.740]   is because they weren't intelligent enough.
[01:17:25.740 --> 01:17:26.580]   - They're always paying attention.
[01:17:26.580 --> 01:17:27.620]   - Yeah, they're always paying attention.
[01:17:27.620 --> 01:17:29.540]   So it's a trade-off, you know,
[01:17:29.540 --> 01:17:32.660]   and I think that it's a very fair thing to say
[01:17:32.660 --> 01:17:36.300]   that autonomous vehicles will be ultimately safer
[01:17:36.300 --> 01:17:39.620]   than humans 'cause humans are very unsafe.
[01:17:39.620 --> 01:17:42.340]   It's kind of a low bar.
[01:17:42.340 --> 01:17:45.540]   - But just like you said,
[01:17:45.540 --> 01:17:49.100]   I think humans got a better rap, right?
[01:17:49.100 --> 01:17:50.900]   'Cause we're really good at the common sense thing.
[01:17:50.900 --> 01:17:52.420]   - Yeah, we're great at the common sense thing.
[01:17:52.420 --> 01:17:54.020]   We're bad at the paying attention thing.
[01:17:54.020 --> 01:17:55.100]   - Paying attention thing, right.
[01:17:55.100 --> 01:17:57.180]   - Especially when we're, you know, driving's kind of boring
[01:17:57.180 --> 01:17:59.900]   and we have these phones to play with and everything.
[01:17:59.900 --> 01:18:04.900]   But I think what's gonna happen is that
[01:18:05.100 --> 01:18:08.460]   for many reasons, not just AI reasons,
[01:18:08.460 --> 01:18:11.540]   but also like legal and other reasons,
[01:18:11.540 --> 01:18:16.540]   that the definition of self-driving is gonna change
[01:18:16.540 --> 01:18:18.380]   or autonomous is gonna change.
[01:18:18.380 --> 01:18:21.380]   It's not gonna be just,
[01:18:21.380 --> 01:18:23.820]   I'm gonna go to sleep in the back
[01:18:23.820 --> 01:18:25.380]   and you just drive me anywhere.
[01:18:25.380 --> 01:18:27.740]   It's gonna be more,
[01:18:27.740 --> 01:18:33.180]   certain areas are going to be instrumented
[01:18:33.180 --> 01:18:36.580]   to have the sensors and the mapping
[01:18:36.580 --> 01:18:37.900]   and all of the stuff you need
[01:18:37.900 --> 01:18:40.140]   that the autonomous cars won't have
[01:18:40.140 --> 01:18:42.420]   to have full common sense.
[01:18:42.420 --> 01:18:45.620]   And they'll do just fine in those areas
[01:18:45.620 --> 01:18:48.900]   as long as pedestrians don't mess with them too much.
[01:18:48.900 --> 01:18:50.060]   That's another question.
[01:18:50.060 --> 01:18:50.900]   (laughs)
[01:18:50.900 --> 01:18:51.740]   - That's right.
[01:18:51.740 --> 01:18:52.580]   That's the human.
[01:18:52.580 --> 01:18:57.580]   - But I don't think we will have fully autonomous
[01:18:57.580 --> 01:19:00.540]   self-driving in the way that like most,
[01:19:00.540 --> 01:19:02.260]   the average person thinks of it.
[01:19:02.260 --> 01:19:05.260]   The person thinks of it for a very long time.
[01:19:05.260 --> 01:19:07.580]   - And just to reiterate,
[01:19:07.580 --> 01:19:10.020]   this is the interesting open question
[01:19:10.020 --> 01:19:12.180]   that I think I agree with you on
[01:19:12.180 --> 01:19:14.860]   is to solve fully autonomous driving,
[01:19:14.860 --> 01:19:17.820]   you have to be able to engineer in common sense.
[01:19:17.820 --> 01:19:18.660]   - Yes.
[01:19:18.660 --> 01:19:23.780]   - I think it's an important thing to hear and think about.
[01:19:23.780 --> 01:19:25.060]   I hope that's wrong,
[01:19:25.060 --> 01:19:28.060]   but I currently agree with you
[01:19:28.060 --> 01:19:30.420]   that unfortunately you do have to have,
[01:19:31.580 --> 01:19:32.780]   to be more specific,
[01:19:32.780 --> 01:19:35.300]   sort of these deep understandings of physics
[01:19:35.300 --> 01:19:38.580]   and of the way this world works.
[01:19:38.580 --> 01:19:39.900]   And also the human dynamics.
[01:19:39.900 --> 01:19:41.540]   Like you mentioned, pedestrians and cyclists,
[01:19:41.540 --> 01:19:45.460]   actually that's whatever that nonverbal communication
[01:19:45.460 --> 01:19:47.220]   as some people call it,
[01:19:47.220 --> 01:19:51.380]   there's that dynamic that is also part of this common sense.
[01:19:51.380 --> 01:19:52.220]   - Right.
[01:19:52.220 --> 01:19:55.860]   And we humans are pretty good at predicting
[01:19:55.860 --> 01:19:57.980]   what other humans are gonna do.
[01:19:57.980 --> 01:20:01.380]   - And how our actions impact the behaviors
[01:20:01.380 --> 01:20:03.780]   so there's this weird game theoretic dance
[01:20:03.780 --> 01:20:05.660]   that we're good at somehow.
[01:20:05.660 --> 01:20:07.860]   And the funny thing is,
[01:20:07.860 --> 01:20:12.060]   'cause I've watched countless hours of pedestrian video
[01:20:12.060 --> 01:20:13.140]   and talked to people,
[01:20:13.140 --> 01:20:15.900]   we humans are also really bad at articulating
[01:20:15.900 --> 01:20:17.460]   the knowledge we have.
[01:20:17.460 --> 01:20:18.300]   - Right.
[01:20:18.300 --> 01:20:20.260]   - Which has been a huge challenge.
[01:20:20.260 --> 01:20:21.100]   - Yes.
[01:20:21.100 --> 01:20:24.220]   - So you've mentioned embodied intelligence.
[01:20:24.220 --> 01:20:26.380]   What do you think it takes to build a system
[01:20:26.380 --> 01:20:27.660]   of human level intelligence?
[01:20:27.660 --> 01:20:29.300]   Does it need to have a body?
[01:20:30.460 --> 01:20:35.460]   - I'm not sure, but I'm coming around to that more and more.
[01:20:35.460 --> 01:20:37.620]   - And what does it mean to be,
[01:20:37.620 --> 01:20:40.500]   I don't mean to keep bringing up Yalun Kun.
[01:20:40.500 --> 01:20:42.780]   - He looms very large.
[01:20:42.780 --> 01:20:46.860]   - Well, he certainly has a large personality, yes.
[01:20:46.860 --> 01:20:50.300]   He thinks that the system needs to be grounded,
[01:20:50.300 --> 01:20:53.020]   meaning it needs to sort of be able to interact
[01:20:53.020 --> 01:20:54.340]   with reality,
[01:20:54.340 --> 01:20:56.940]   but doesn't think it necessarily needs to have a body.
[01:20:56.940 --> 01:20:57.780]   So when you think of--
[01:20:57.780 --> 01:20:59.540]   - So what's the difference?
[01:20:59.540 --> 01:21:02.580]   I guess I wanna ask, when you mean body,
[01:21:02.580 --> 01:21:05.300]   do you mean you have to be able to play with the world?
[01:21:05.300 --> 01:21:07.300]   Or do you also mean like there's a body
[01:21:07.300 --> 01:21:09.700]   that you have to preserve?
[01:21:09.700 --> 01:21:12.660]   - Oh, that's a good question.
[01:21:12.660 --> 01:21:13.820]   I haven't really thought about that,
[01:21:13.820 --> 01:21:16.660]   but I think both, I would guess.
[01:21:16.660 --> 01:21:20.300]   Because I think you,
[01:21:20.300 --> 01:21:24.100]   I think intelligence,
[01:21:24.100 --> 01:21:28.580]   it's so hard to separate it from our self,
[01:21:28.580 --> 01:21:32.020]   our desire for self-preservation,
[01:21:32.020 --> 01:21:34.700]   our emotions,
[01:21:34.700 --> 01:21:37.620]   our all that non-rational stuff
[01:21:37.620 --> 01:21:42.620]   that kind of gets in the way of logical thinking.
[01:21:42.620 --> 01:21:45.820]   Because the way,
[01:21:45.820 --> 01:21:48.700]   we're talking about human intelligence
[01:21:48.700 --> 01:21:51.380]   or human level intelligence, whatever that means,
[01:21:51.380 --> 01:21:54.300]   a huge part of it is social.
[01:21:56.820 --> 01:21:58.900]   We were evolved to be social
[01:21:58.900 --> 01:22:01.620]   and to deal with other people.
[01:22:01.620 --> 01:22:04.860]   And that's just so ingrained in us
[01:22:04.860 --> 01:22:08.740]   that it's hard to separate intelligence from that.
[01:22:08.740 --> 01:22:14.700]   I think AI for the last 70 years
[01:22:14.700 --> 01:22:17.100]   or however long it's been around,
[01:22:17.100 --> 01:22:18.700]   it has largely been separated.
[01:22:18.700 --> 01:22:20.860]   There's this idea that there's like,
[01:22:20.860 --> 01:22:23.780]   it's kind of very Cartesian.
[01:22:23.780 --> 01:22:26.580]   There's this thinking thing
[01:22:26.580 --> 01:22:27.820]   that we're trying to create,
[01:22:27.820 --> 01:22:30.900]   but we don't care about all this other stuff.
[01:22:30.900 --> 01:22:35.260]   And I think the other stuff is very fundamental.
[01:22:35.260 --> 01:22:37.820]   - So there's the idea that things like emotion
[01:22:37.820 --> 01:22:40.220]   get in the way of intelligence.
[01:22:40.220 --> 01:22:42.700]   - As opposed to being an integral part of it.
[01:22:42.700 --> 01:22:43.540]   - Integral part of it.
[01:22:43.540 --> 01:22:45.460]   So, I mean, I'm Russian,
[01:22:45.460 --> 01:22:48.460]   so romanticize the notions of emotion and suffering
[01:22:48.460 --> 01:22:51.180]   and all that kind of fear of mortality,
[01:22:51.180 --> 01:22:52.020]   those kinds of things.
[01:22:52.020 --> 01:22:55.500]   So in AI, especially,
[01:22:55.540 --> 01:22:56.660]   so I've--
[01:22:56.660 --> 01:22:57.860]   - By the way, did you see that?
[01:22:57.860 --> 01:23:00.340]   There was this recent thing going around the internet.
[01:23:00.340 --> 01:23:03.860]   This, some, I think he's a Russian or some Slavic
[01:23:03.860 --> 01:23:05.220]   had written this thing,
[01:23:05.220 --> 01:23:08.740]   sort of anti the idea of super intelligence.
[01:23:08.740 --> 01:23:10.460]   I forgot, maybe he's Polish.
[01:23:10.460 --> 01:23:12.460]   Anyway, so he had all these arguments
[01:23:12.460 --> 01:23:15.620]   and one was the argument from Slavic pessimism.
[01:23:15.620 --> 01:23:18.260]   (both laughing)
[01:23:18.260 --> 01:23:19.100]   My favorite.
[01:23:19.100 --> 01:23:22.020]   - Do you remember what the argument is?
[01:23:22.020 --> 01:23:24.020]   - It's like nothing ever works.
[01:23:24.020 --> 01:23:26.500]   - Yeah. - Everything sucks.
[01:23:26.500 --> 01:23:30.100]   - So what do you think is the role,
[01:23:30.100 --> 01:23:32.020]   like that's such a fascinating idea
[01:23:32.020 --> 01:23:36.380]   that what we perceive as sort of the limits of human,
[01:23:36.380 --> 01:23:41.500]   of the human mind, which is emotion and fear
[01:23:41.500 --> 01:23:45.080]   and all those kinds of things are integral to intelligence.
[01:23:45.080 --> 01:23:48.220]   Could you elaborate on that?
[01:23:48.220 --> 01:23:53.020]   Like what, why is that important, do you think?
[01:23:54.020 --> 01:23:57.460]   For human level intelligence?
[01:23:57.460 --> 01:24:00.780]   - At least for the way that humans work,
[01:24:00.780 --> 01:24:04.860]   it's a big part of how it affects how we perceive the world.
[01:24:04.860 --> 01:24:07.780]   It affects how we make decisions about the world.
[01:24:07.780 --> 01:24:10.020]   It affects how we interact with other people.
[01:24:10.020 --> 01:24:13.180]   It affects our understanding of other people.
[01:24:13.180 --> 01:24:16.780]   For me to understand your,
[01:24:16.780 --> 01:24:21.540]   what you're likely to do,
[01:24:21.540 --> 01:24:23.180]   I need to have kind of a theory of mind
[01:24:23.180 --> 01:24:26.660]   and that's very much a theory of emotion
[01:24:26.660 --> 01:24:29.860]   and motivations and goals.
[01:24:29.860 --> 01:24:34.340]   And to understand that,
[01:24:34.340 --> 01:24:41.060]   we have this whole system of mirror neurons.
[01:24:41.060 --> 01:24:45.500]   I sort of understand your motivations
[01:24:45.500 --> 01:24:49.380]   through sort of simulating it myself.
[01:24:49.380 --> 01:24:54.380]   So, it's not something that I can prove that's necessary,
[01:24:54.380 --> 01:24:58.180]   but it seems very likely.
[01:24:58.180 --> 01:25:01.740]   - So, okay.
[01:25:01.740 --> 01:25:04.100]   You've written the op-ed in the New York Times
[01:25:04.100 --> 01:25:07.740]   titled "We Shouldn't Be Scared by Superintelligent AI"
[01:25:07.740 --> 01:25:10.940]   and it criticized a little bit Stuart Russell
[01:25:10.940 --> 01:25:11.900]   and Nick Bostrom.
[01:25:11.900 --> 01:25:16.740]   Can you try to summarize that article's key ideas?
[01:25:18.260 --> 01:25:22.820]   - So, it was spurred by an earlier New York Times op-ed
[01:25:22.820 --> 01:25:26.300]   by Stuart Russell, which was summarizing his book
[01:25:26.300 --> 01:25:28.900]   called "Human Compatible."
[01:25:28.900 --> 01:25:31.420]   And the article was saying,
[01:25:31.420 --> 01:25:37.300]   if we have superintelligent AI,
[01:25:37.300 --> 01:25:40.900]   we need to have its values aligned with our values
[01:25:40.900 --> 01:25:43.940]   and it has to learn about what we really want.
[01:25:43.940 --> 01:25:45.580]   And he gave this example.
[01:25:45.580 --> 01:25:48.900]   What if we have a superintelligent AI
[01:25:48.900 --> 01:25:52.820]   and we give it the problem of solving climate change
[01:25:52.820 --> 01:25:57.100]   and it decides that the best way to lower the carbon
[01:25:57.100 --> 01:25:59.900]   in the atmosphere is to kill all the humans?
[01:25:59.900 --> 01:26:00.740]   Okay.
[01:26:00.740 --> 01:26:02.620]   So, to me, that just made no sense at all
[01:26:02.620 --> 01:26:06.340]   because a superintelligent AI,
[01:26:06.340 --> 01:26:10.860]   first of all, trying to figure out
[01:26:10.860 --> 01:26:13.180]   what superintelligence means.
[01:26:14.180 --> 01:26:19.180]   And it seems that something that's superintelligent
[01:26:19.180 --> 01:26:25.180]   can't just be intelligent along this one dimension of,
[01:26:25.180 --> 01:26:27.180]   okay, I'm gonna figure out all the steps,
[01:26:27.180 --> 01:26:30.620]   the best optimal path to solving climate change
[01:26:30.620 --> 01:26:33.020]   and not be intelligent enough to figure out
[01:26:33.020 --> 01:26:35.820]   that humans don't wanna be killed,
[01:26:35.820 --> 01:26:39.940]   that you could get to one without having the other.
[01:26:39.940 --> 01:26:43.780]   And Bostrom, in his book,
[01:26:43.780 --> 01:26:46.620]   talks about the orthogonality hypothesis
[01:26:46.620 --> 01:26:49.480]   where he says he thinks that a system's,
[01:26:49.480 --> 01:26:52.780]   I can't remember exactly what it is,
[01:26:52.780 --> 01:26:56.980]   but a system's goals and its values
[01:26:56.980 --> 01:26:58.420]   don't have to be aligned.
[01:26:58.420 --> 01:27:00.420]   There's some orthogonality there
[01:27:00.420 --> 01:27:02.580]   which didn't make any sense to me.
[01:27:02.580 --> 01:27:05.180]   - So, you're saying in any system
[01:27:05.180 --> 01:27:07.700]   that's sufficiently, not even superintelligent,
[01:27:07.700 --> 01:27:10.300]   but as it approaches greater and greater intelligence,
[01:27:10.300 --> 01:27:12.380]   there's a holistic nature that will sort of,
[01:27:12.380 --> 01:27:15.420]   a tension that will naturally emerge
[01:27:15.420 --> 01:27:16.580]   that prevents it from sort of
[01:27:16.580 --> 01:27:18.420]   any one dimension running away.
[01:27:18.420 --> 01:27:20.100]   - Yeah, yeah, exactly.
[01:27:20.100 --> 01:27:25.100]   So, Bostrom had this example of the superintelligent AI
[01:27:25.100 --> 01:27:31.140]   that turns the world into paperclips
[01:27:31.140 --> 01:27:34.300]   'cause its job is to make paperclips or something.
[01:27:34.300 --> 01:27:36.540]   And that just, as a thought experiment,
[01:27:36.540 --> 01:27:38.540]   didn't make any sense to me. (laughs)
[01:27:38.540 --> 01:27:40.380]   - Well, as a thought experiment
[01:27:40.380 --> 01:27:43.740]   or as a thing that could possibly be realized?
[01:27:43.740 --> 01:27:44.580]   - Either.
[01:27:44.580 --> 01:27:48.180]   So, I think that what my op-ed was trying to do
[01:27:48.180 --> 01:27:51.380]   was say that intelligence is more complex
[01:27:51.380 --> 01:27:54.940]   than these people are presenting it,
[01:27:54.940 --> 01:27:59.380]   that it's not so separable,
[01:27:59.380 --> 01:28:04.380]   the rationality, the values, the emotions,
[01:28:05.060 --> 01:28:07.900]   all of that, that it's the view
[01:28:07.900 --> 01:28:10.460]   that you could separate all these dimensions
[01:28:10.460 --> 01:28:13.140]   and build a machine that has one of these dimensions
[01:28:13.140 --> 01:28:15.500]   and it's superintelligent in one dimension,
[01:28:15.500 --> 01:28:17.860]   but it doesn't have any of the other dimensions.
[01:28:17.860 --> 01:28:22.860]   That's what I was trying to criticize,
[01:28:22.860 --> 01:28:25.820]   that I don't believe that.
[01:28:25.820 --> 01:28:30.820]   - So, can I read a few sentences from Yoshua Bengio,
[01:28:31.220 --> 01:28:35.140]   who is always super eloquent?
[01:28:35.140 --> 01:28:41.300]   So, he writes, "I have the same impression as Melanie
[01:28:41.300 --> 01:28:43.180]   "that our cognitive biases are linked
[01:28:43.180 --> 01:28:46.380]   "with our ability to learn to solve many problems.
[01:28:46.380 --> 01:28:49.100]   "They may also be a limiting factor for AI.
[01:28:49.100 --> 01:28:54.200]   "However," this is a may in quotes,
[01:28:54.200 --> 01:28:55.800]   "things may also turn out differently
[01:28:55.800 --> 01:28:57.020]   "and there's a lot of uncertainty
[01:28:57.020 --> 01:28:59.320]   "about the capabilities of future machines.
[01:29:00.560 --> 01:29:02.560]   "But more importantly for me,
[01:29:02.560 --> 01:29:05.440]   "the value alignment problem is a problem
[01:29:05.440 --> 01:29:08.900]   "well before we reach some hypothetical superintelligence.
[01:29:08.900 --> 01:29:10.500]   "It is already posing a problem
[01:29:10.500 --> 01:29:12.660]   "in the form of super powerful companies
[01:29:12.660 --> 01:29:17.660]   "whose objective function may not be sufficiently aligned
[01:29:17.660 --> 01:29:19.260]   "with humanity's general well-being,
[01:29:19.260 --> 01:29:21.860]   "creating all kinds of harmful side effects."
[01:29:21.860 --> 01:29:23.820]   So, he goes on to argue that,
[01:29:23.820 --> 01:29:29.500]   the orthogonality and those kinds of things,
[01:29:29.500 --> 01:29:33.020]   the concerns of just aligning values
[01:29:33.020 --> 01:29:35.200]   with the capabilities of the system
[01:29:35.200 --> 01:29:37.740]   is something that might come long
[01:29:37.740 --> 01:29:40.780]   before we reach anything like superintelligence.
[01:29:40.780 --> 01:29:44.500]   So, your criticism is kind of really nice to saying,
[01:29:44.500 --> 01:29:46.980]   this idea of superintelligence systems
[01:29:46.980 --> 01:29:49.100]   seem to be dismissing fundamental parts
[01:29:49.100 --> 01:29:50.820]   of what intelligence would take.
[01:29:50.820 --> 01:29:53.740]   And then Yoshua kind of says, "Yes,
[01:29:53.740 --> 01:29:58.160]   "but if we look at systems that are much less intelligent,
[01:29:58.160 --> 01:30:02.160]   "there might be these same kinds of problems that emerge."
[01:30:02.160 --> 01:30:06.740]   - Sure, but I guess the example that he gives there
[01:30:06.740 --> 01:30:10.100]   of these corporations, that's people, right?
[01:30:10.100 --> 01:30:12.140]   Those are people's values.
[01:30:12.140 --> 01:30:14.420]   I mean, we're talking about people,
[01:30:14.420 --> 01:30:17.260]   the corporations are,
[01:30:17.260 --> 01:30:20.580]   their values are the values of the people
[01:30:20.580 --> 01:30:21.700]   who run those corporations.
[01:30:21.700 --> 01:30:24.740]   - But the idea is the algorithm, that's right.
[01:30:24.740 --> 01:30:26.900]   So, the fundamental person,
[01:30:27.160 --> 01:30:30.600]   the fundamental element of what does the bad thing
[01:30:30.600 --> 01:30:32.280]   is a human being.
[01:30:32.280 --> 01:30:37.120]   But the algorithm kind of controls the behavior
[01:30:37.120 --> 01:30:39.160]   of this mass of human beings.
[01:30:39.160 --> 01:30:40.000]   - Which algorithm?
[01:30:40.000 --> 01:30:42.880]   - For a company that's,
[01:30:42.880 --> 01:30:45.320]   so for example, if it's an advertisement-driven company
[01:30:45.320 --> 01:30:50.320]   that recommends certain things and encourages engagement,
[01:30:50.320 --> 01:30:54.600]   so it gets money by encouraging engagement,
[01:30:54.600 --> 01:30:57.140]   and therefore the company more and more,
[01:30:57.140 --> 01:31:01.420]   it's like the cycle that builds an algorithm
[01:31:01.420 --> 01:31:04.140]   that enforces more engagement
[01:31:04.140 --> 01:31:06.180]   and may perhaps more division in the culture
[01:31:06.180 --> 01:31:07.820]   and so on, so on.
[01:31:07.820 --> 01:31:12.140]   - I guess the question here is sort of who has the agency?
[01:31:12.140 --> 01:31:15.020]   So, you might say, for instance,
[01:31:15.020 --> 01:31:17.500]   we don't want our algorithms to be racist.
[01:31:17.500 --> 01:31:21.940]   And facial recognition,
[01:31:21.940 --> 01:31:24.500]   some people have criticized some facial recognition systems
[01:31:24.500 --> 01:31:27.200]   as being racist 'cause they're not as good
[01:31:27.200 --> 01:31:30.920]   on darker skin than lighter skin.
[01:31:30.920 --> 01:31:33.280]   Okay, but the agency there,
[01:31:33.280 --> 01:31:37.220]   the actual facial recognition algorithm
[01:31:37.220 --> 01:31:39.000]   isn't what has the agency.
[01:31:39.000 --> 01:31:41.440]   It's not the racist thing, right?
[01:31:41.440 --> 01:31:45.040]   It's the, I don't know,
[01:31:45.040 --> 01:31:49.440]   the combination of the training data,
[01:31:49.440 --> 01:31:52.000]   the cameras being used, whatever.
[01:31:52.000 --> 01:31:54.160]   But my understanding of,
[01:31:54.160 --> 01:31:57.420]   and I agree with Benjio there that he,
[01:31:57.420 --> 01:32:00.380]   I think there are these value issues
[01:32:00.380 --> 01:32:03.140]   with our use of algorithms,
[01:32:03.140 --> 01:32:08.140]   but my understanding of what Russell's argument was
[01:32:08.140 --> 01:32:15.020]   is more that the machine itself has the agency now.
[01:32:15.020 --> 01:32:17.660]   It's the thing that's making the decisions,
[01:32:17.660 --> 01:32:20.720]   and it's the thing that has what we would call values.
[01:32:20.720 --> 01:32:23.100]   - Yes.
[01:32:23.920 --> 01:32:26.000]   - Whether that's just a matter of degree,
[01:32:26.000 --> 01:32:27.940]   it's hard to say, right?
[01:32:27.940 --> 01:32:30.340]   But I would say that's sort of qualitatively different
[01:32:30.340 --> 01:32:34.180]   than a face recognition neural network.
[01:32:34.180 --> 01:32:38.720]   - And to broadly linger on that point,
[01:32:38.720 --> 01:32:42.920]   if you look at Elon Musk or Stuart Russell or Bostrom,
[01:32:42.920 --> 01:32:46.300]   people who are worried about existential risks of AI,
[01:32:46.300 --> 01:32:47.900]   however far into the future,
[01:32:47.900 --> 01:32:50.720]   their argument goes is it eventually happens.
[01:32:50.720 --> 01:32:53.580]   We don't know how far, but it eventually happens.
[01:32:53.580 --> 01:32:56.800]   Do you share any of those concerns?
[01:32:56.800 --> 01:33:00.120]   And what kind of concerns in general do you have about AI
[01:33:00.120 --> 01:33:04.340]   that approach anything like existential threat to humanity?
[01:33:04.340 --> 01:33:10.480]   - So I would say, yes, it's possible,
[01:33:10.480 --> 01:33:14.520]   but I think there's a lot more closer-in
[01:33:14.520 --> 01:33:16.040]   existential threats to humanity.
[01:33:16.040 --> 01:33:19.120]   - 'Cause you said like 100 years for, so your time--
[01:33:19.120 --> 01:33:20.280]   - It's more than 100 years.
[01:33:20.280 --> 01:33:22.200]   - More than 100 years, and so that means--
[01:33:22.200 --> 01:33:25.440]   - Maybe even more than 500 years, I don't know.
[01:33:25.440 --> 01:33:28.320]   - So the existential threats are so far out
[01:33:28.320 --> 01:33:31.040]   that the future is, I mean,
[01:33:31.040 --> 01:33:33.240]   there'll be a million different technologies
[01:33:33.240 --> 01:33:34.480]   that we can't even predict now
[01:33:34.480 --> 01:33:37.600]   that will fundamentally change the nature of our behavior,
[01:33:37.600 --> 01:33:39.880]   reality, society, and so on before then.
[01:33:39.880 --> 01:33:41.680]   - I think so, I think so.
[01:33:41.680 --> 01:33:45.880]   And we have so many other pressing existential threats
[01:33:45.880 --> 01:33:46.720]   going on right now.
[01:33:46.720 --> 01:33:48.040]   - Nuclear weapons even.
[01:33:48.040 --> 01:33:50.720]   - Nuclear weapons, climate problems,
[01:33:50.720 --> 01:33:58.280]   poverty, possible pandemics, you can go on and on.
[01:33:58.280 --> 01:34:04.800]   And I think worrying about existential threat from AI
[01:34:04.800 --> 01:34:14.160]   is not the best priority for what we should be worried about.
[01:34:14.160 --> 01:34:15.800]   That's kind of my view, 'cause we're so far away.
[01:34:15.800 --> 01:34:20.800]   But I'm not necessarily criticizing Russell or Bostrom
[01:34:20.800 --> 01:34:26.840]   or whoever for worrying about that.
[01:34:26.840 --> 01:34:30.000]   And I think some people should be worried about it.
[01:34:30.000 --> 01:34:34.040]   It's certainly fine, but I was more sort of getting
[01:34:34.040 --> 01:34:39.040]   at their view of what intelligence is.
[01:34:39.040 --> 01:34:43.320]   So I was more focusing on their view of super intelligence
[01:34:43.320 --> 01:34:48.320]   than just the fact of them worrying.
[01:34:48.320 --> 01:34:52.920]   And the title of the article was written
[01:34:52.920 --> 01:34:54.960]   by the New York Times editors.
[01:34:54.960 --> 01:34:56.520]   I wouldn't have called it that.
[01:34:56.520 --> 01:34:59.520]   - We shouldn't be scared by super intelligence.
[01:34:59.520 --> 01:35:00.360]   - No.
[01:35:00.360 --> 01:35:01.320]   - If you wrote it, it'd be like,
[01:35:01.320 --> 01:35:03.440]   we should redefine what you mean by super intelligence.
[01:35:03.440 --> 01:35:07.440]   - I actually said something like super intelligence
[01:35:07.440 --> 01:35:12.440]   is not a sort of coherent,
[01:35:12.440 --> 01:35:14.080]   sort of coherent idea.
[01:35:14.080 --> 01:35:19.680]   But that's not like something New York Times would put in.
[01:35:19.680 --> 01:35:23.440]   - And the follow-up argument that Yoshua makes also,
[01:35:23.440 --> 01:35:24.920]   not argument, but a statement,
[01:35:24.920 --> 01:35:26.520]   and I've heard him say it before,
[01:35:26.520 --> 01:35:28.480]   and I think I agree.
[01:35:28.480 --> 01:35:30.800]   He kind of has a very friendly way of phrasing it
[01:35:30.800 --> 01:35:33.000]   as it's good for a lot of people
[01:35:33.000 --> 01:35:34.600]   to believe different things.
[01:35:34.600 --> 01:35:36.880]   - He's such a nice guy.
[01:35:36.880 --> 01:35:39.600]   - Yeah, but it's also practically speaking,
[01:35:39.600 --> 01:35:44.600]   like we shouldn't be, like while your article stands,
[01:35:44.600 --> 01:35:47.360]   like Stuart Russell does amazing work,
[01:35:47.360 --> 01:35:50.120]   Bostrom does amazing work, you do amazing work.
[01:35:50.120 --> 01:35:53.200]   And even when you disagree about the definition
[01:35:53.200 --> 01:35:56.840]   of super intelligence or the usefulness of even the term,
[01:35:56.840 --> 01:36:01.720]   it's still useful to have people that like use that term,
[01:36:01.720 --> 01:36:03.000]   right, and then argue.
[01:36:03.000 --> 01:36:06.520]   - Sure, I absolutely agree with Benjo there.
[01:36:06.520 --> 01:36:08.600]   And I think it's great that, you know,
[01:36:08.600 --> 01:36:09.800]   and it's great that New York Times
[01:36:09.800 --> 01:36:10.880]   will publish all this stuff.
[01:36:10.880 --> 01:36:12.560]   - That's right.
[01:36:12.560 --> 01:36:14.520]   It's an exciting time to be here.
[01:36:14.520 --> 01:36:17.320]   What do you think is a good test of intelligence?
[01:36:17.320 --> 01:36:20.680]   Like is natural language ultimately a test
[01:36:20.680 --> 01:36:23.720]   that you find the most compelling, like the original,
[01:36:23.720 --> 01:36:28.720]   or the higher levels of the Turing test kind of, yeah?
[01:36:28.720 --> 01:36:34.560]   - Yeah, I still think the original idea of the Turing test
[01:36:34.560 --> 01:36:37.640]   is a good test for intelligence.
[01:36:37.640 --> 01:36:39.960]   I mean, I can't think of anything better.
[01:36:39.960 --> 01:36:41.640]   You know, the Turing test,
[01:36:41.640 --> 01:36:43.560]   the way that it's been carried out so far
[01:36:43.560 --> 01:36:48.560]   has been very impoverished, if you will.
[01:36:48.560 --> 01:36:52.800]   But I think a real Turing test that really goes into depth,
[01:36:52.800 --> 01:36:54.720]   like the one that I mentioned, I talk about in the book,
[01:36:54.720 --> 01:36:57.880]   I talk about Ray Kurzweil and Mitchell Kapoor
[01:36:57.880 --> 01:37:02.880]   have this bet, right, that in 2029,
[01:37:02.880 --> 01:37:04.440]   I think is the date there,
[01:37:05.800 --> 01:37:07.440]   a machine will pass the Turing test.
[01:37:07.440 --> 01:37:12.440]   And they have a very specific, like how many hours,
[01:37:12.440 --> 01:37:14.920]   expert judges and all of that.
[01:37:14.920 --> 01:37:18.120]   And, you know, Kurzweil says yes, Kapoor says no.
[01:37:18.120 --> 01:37:21.000]   We only have like nine more years to go to see.
[01:37:21.000 --> 01:37:26.960]   But I, you know, if something, a machine could pass that,
[01:37:26.960 --> 01:37:30.360]   I would be willing to call it intelligent.
[01:37:30.360 --> 01:37:32.880]   - Of course, nobody will.
[01:37:33.840 --> 01:37:37.080]   They will say that's just a language model, right?
[01:37:37.080 --> 01:37:38.080]   If it does.
[01:37:38.080 --> 01:37:40.840]   So you would be comfortable, so language,
[01:37:40.840 --> 01:37:45.160]   a long conversation that, well, yeah,
[01:37:45.160 --> 01:37:46.880]   I mean, you're right, because I think probably
[01:37:46.880 --> 01:37:48.920]   to carry out that long conversation,
[01:37:48.920 --> 01:37:50.400]   you would literally need to have
[01:37:50.400 --> 01:37:52.400]   deep common sense understanding of the world.
[01:37:52.400 --> 01:37:54.600]   - I think so, I think so.
[01:37:54.600 --> 01:37:57.600]   - And the conversation is enough to reveal that.
[01:37:57.600 --> 01:37:59.920]   - I think so. - Perhaps it is.
[01:37:59.920 --> 01:38:03.640]   So another super fun topic of complexity
[01:38:03.640 --> 01:38:09.640]   that you have worked on, written about.
[01:38:09.640 --> 01:38:13.360]   Let me ask the basic question, what is complexity?
[01:38:13.360 --> 01:38:16.480]   - So complexity is another one of those terms,
[01:38:16.480 --> 01:38:19.280]   like intelligence, it's perhaps overused.
[01:38:19.280 --> 01:38:24.280]   But my book about complexity was about this wide area
[01:38:28.240 --> 01:38:33.240]   of complex systems, studying different systems in nature,
[01:38:33.240 --> 01:38:38.240]   in technology, in society, in which you have emergence,
[01:38:38.240 --> 01:38:41.880]   kind of like I was talking about with intelligence.
[01:38:41.880 --> 01:38:46.320]   You know, we have the brain, which has billions of neurons,
[01:38:46.320 --> 01:38:49.960]   and each neuron individually could be said
[01:38:49.960 --> 01:38:53.720]   to be not very complex compared to the system as a whole,
[01:38:53.720 --> 01:38:58.160]   but the system, the interactions of those neurons
[01:38:58.160 --> 01:39:01.040]   and the dynamics creates these phenomena
[01:39:01.040 --> 01:39:04.340]   that we call intelligence or consciousness,
[01:39:04.340 --> 01:39:09.440]   that we consider to be very complex.
[01:39:09.440 --> 01:39:13.800]   So the field of complexity is trying to find
[01:39:13.800 --> 01:39:17.080]   general principles that underlie all these systems
[01:39:17.080 --> 01:39:20.120]   that have these kinds of emergent properties.
[01:39:20.120 --> 01:39:23.400]   - And the emergence occurs from,
[01:39:23.400 --> 01:39:26.840]   underlying the complex system is usually simple,
[01:39:26.840 --> 01:39:28.240]   fundamental interactions.
[01:39:28.240 --> 01:39:29.080]   - Yes.
[01:39:29.080 --> 01:39:33.160]   - And the emergence happens when there's just a lot
[01:39:33.160 --> 01:39:34.960]   of these things interacting.
[01:39:34.960 --> 01:39:35.800]   - Yes.
[01:39:35.800 --> 01:39:41.160]   - And then most of science today,
[01:39:41.160 --> 01:39:43.480]   can you talk about what is reductionism?
[01:39:43.480 --> 01:39:50.520]   - Well, reductionism is when you try and take a system
[01:39:50.520 --> 01:39:54.400]   and divide it up into its elements,
[01:39:55.240 --> 01:40:00.240]   whether those be cells or atoms or subatomic particles,
[01:40:00.240 --> 01:40:02.880]   whatever your field is,
[01:40:02.880 --> 01:40:06.480]   and then try and understand those elements
[01:40:06.480 --> 01:40:09.480]   and then try and build up an understanding
[01:40:09.480 --> 01:40:12.480]   of the whole system by looking at sort of the sum
[01:40:12.480 --> 01:40:14.360]   of all the elements.
[01:40:14.360 --> 01:40:16.200]   - Yeah, so what's your sense,
[01:40:16.200 --> 01:40:17.920]   whether we're talking about intelligence
[01:40:17.920 --> 01:40:21.080]   or these kinds of interesting complex systems,
[01:40:21.080 --> 01:40:25.160]   is it possible to understand them in a reductionist way?
[01:40:25.160 --> 01:40:27.000]   Which is probably the approach
[01:40:27.000 --> 01:40:29.240]   of most of science today, right?
[01:40:29.240 --> 01:40:33.400]   - I don't think it's always possible to understand
[01:40:33.400 --> 01:40:35.840]   the things we want to understand the most.
[01:40:35.840 --> 01:40:40.000]   So I don't think it's possible to look at single neurons
[01:40:40.000 --> 01:40:45.880]   and understand what we call intelligence,
[01:40:45.880 --> 01:40:48.360]   to look at sort of summing up.
[01:40:48.360 --> 01:40:53.360]   And the sort of the summing up is the issue here
[01:40:53.440 --> 01:40:57.840]   that we're, you know, one example is that the human genome,
[01:40:57.840 --> 01:41:01.920]   right, so there was a lot of work on excitement
[01:41:01.920 --> 01:41:04.000]   about sequencing the human genome,
[01:41:04.000 --> 01:41:08.680]   because the idea would be that we'd be able to find genes
[01:41:08.680 --> 01:41:11.480]   that underlies diseases.
[01:41:11.480 --> 01:41:15.800]   But it turns out that, and it was a very reductionist idea.
[01:41:15.800 --> 01:41:19.240]   You know, we figure out what all the parts are,
[01:41:19.240 --> 01:41:21.920]   and then we would be able to figure out which parts cause
[01:41:21.920 --> 01:41:23.080]   which things.
[01:41:23.080 --> 01:41:25.440]   But it turns out that the parts don't cause the things
[01:41:25.440 --> 01:41:26.280]   that we're interested in.
[01:41:26.280 --> 01:41:30.920]   It's like the interactions, it's the networks of these parts.
[01:41:30.920 --> 01:41:34.280]   And so that kind of reductionist approach
[01:41:34.280 --> 01:41:37.240]   didn't yield the explanation that we wanted.
[01:41:37.240 --> 01:41:41.840]   - What do you, what to use the most beautiful,
[01:41:41.840 --> 01:41:44.280]   complex system that you've encountered?
[01:41:44.280 --> 01:41:45.880]   - Most beautiful.
[01:41:45.880 --> 01:41:48.040]   - That you've been captivated by?
[01:41:48.040 --> 01:41:52.120]   Is it sort of, I mean, for me,
[01:41:52.120 --> 01:41:55.280]   it's the simplest would be cellular automata.
[01:41:55.280 --> 01:41:56.120]   - Oh, yeah.
[01:41:56.120 --> 01:41:58.680]   So I was very captivated by cellular automata,
[01:41:58.680 --> 01:42:01.880]   and worked on cellular automata for several years.
[01:42:01.880 --> 01:42:05.480]   - Do you find it amazing, or is it surprising
[01:42:05.480 --> 01:42:08.440]   that such simple systems, such simple rules
[01:42:08.440 --> 01:42:12.080]   in cellular automata can create sort of seemingly
[01:42:12.080 --> 01:42:14.760]   unlimited complexity?
[01:42:14.760 --> 01:42:16.800]   - Yeah, that was very surprising to me.
[01:42:16.800 --> 01:42:17.760]   - How do you make sense of it?
[01:42:17.760 --> 01:42:19.160]   How does that make you feel?
[01:42:19.160 --> 01:42:21.720]   Is it just ultimately humbling,
[01:42:21.720 --> 01:42:24.240]   or is there a hope to somehow leverage this
[01:42:24.240 --> 01:42:27.040]   into a deeper understanding,
[01:42:27.040 --> 01:42:30.320]   and even able to engineer things like intelligence?
[01:42:30.320 --> 01:42:32.600]   - It's definitely humbling.
[01:42:32.600 --> 01:42:37.600]   How humbling in that, also kind of awe-inspiring that,
[01:42:37.600 --> 01:42:43.000]   it's that awe-inspiring part of mathematics
[01:42:43.000 --> 01:42:46.240]   that these incredibly simple rules can produce
[01:42:46.240 --> 01:42:51.240]   this very beautiful, complex, hard to understand behavior.
[01:42:52.200 --> 01:42:56.800]   And that's, it's mysterious, you know,
[01:42:56.800 --> 01:43:01.800]   and surprising still, but exciting,
[01:43:01.800 --> 01:43:03.600]   'cause it does give you kind of the hope
[01:43:03.600 --> 01:43:06.600]   that you might be able to engineer complexity
[01:43:06.600 --> 01:43:08.720]   just from simple rules. - From simple rules
[01:43:08.720 --> 01:43:10.000]   from the beginnings.
[01:43:10.000 --> 01:43:12.320]   Can you briefly say what is the Santa Fe Institute?
[01:43:12.320 --> 01:43:14.840]   Its history, its culture, its ideas, its future.
[01:43:14.840 --> 01:43:18.360]   So I've never, as I mentioned to you, I've never been,
[01:43:18.360 --> 01:43:20.800]   but it's always been this, in my mind,
[01:43:20.800 --> 01:43:23.280]   this mystical place where brilliant people
[01:43:23.280 --> 01:43:25.400]   study the edge of chaos.
[01:43:25.400 --> 01:43:26.600]   - Yeah, exactly.
[01:43:26.600 --> 01:43:32.760]   So the Santa Fe Institute was started in 1984,
[01:43:32.760 --> 01:43:37.280]   and it was created by a group of scientists,
[01:43:37.280 --> 01:43:40.000]   a lot of them from Los Alamos National Lab,
[01:43:40.000 --> 01:43:45.000]   which is about a 40-minute drive from Santa Fe Institute.
[01:43:46.920 --> 01:43:49.160]   They were mostly physicists and chemists,
[01:43:49.160 --> 01:43:53.640]   but they were frustrated in their field
[01:43:53.640 --> 01:43:58.480]   because they felt that their field wasn't approaching
[01:43:58.480 --> 01:44:01.680]   kind of big interdisciplinary questions
[01:44:01.680 --> 01:44:04.400]   like the kinds we've been talking about.
[01:44:04.400 --> 01:44:06.120]   And they wanted to have a place
[01:44:06.120 --> 01:44:08.920]   where people from different disciplines
[01:44:08.920 --> 01:44:11.280]   could work on these big questions
[01:44:11.280 --> 01:44:14.600]   without sort of being siloed into physics,
[01:44:14.600 --> 01:44:17.840]   chemistry, biology, whatever.
[01:44:17.840 --> 01:44:20.000]   So they started this institute,
[01:44:20.000 --> 01:44:24.480]   and this was people like George Cowan,
[01:44:24.480 --> 01:44:27.560]   who was a chemist in the Manhattan Project,
[01:44:27.560 --> 01:44:32.560]   and Nicholas Metropolis, who, a mathematician, physicist,
[01:44:32.560 --> 01:44:37.440]   Marie Gelman, physicist in his own,
[01:44:37.440 --> 01:44:39.840]   so some really big names here,
[01:44:39.840 --> 01:44:43.000]   Ken Arrow, a Nobel Prize-winning economist.
[01:44:43.000 --> 01:44:46.240]   And they started having these workshops.
[01:44:46.240 --> 01:44:50.800]   And this whole enterprise kind of grew
[01:44:50.800 --> 01:44:54.120]   into this research institute
[01:44:54.120 --> 01:44:58.440]   that itself has been kind of on the edge of chaos
[01:44:58.440 --> 01:45:01.000]   its whole life because it doesn't have any,
[01:45:01.000 --> 01:45:04.040]   it doesn't have a significant endowment,
[01:45:04.040 --> 01:45:07.600]   and it's just been kind of living on
[01:45:07.600 --> 01:45:12.520]   whatever funding it can raise through donations
[01:45:12.520 --> 01:45:17.520]   and grants and however it can,
[01:45:17.520 --> 01:45:20.840]   you know, business associates and so on.
[01:45:20.840 --> 01:45:23.000]   But it's a great place.
[01:45:23.000 --> 01:45:25.600]   It's a really fun place to go think about ideas
[01:45:25.600 --> 01:45:29.080]   that you wouldn't normally encounter.
[01:45:29.080 --> 01:45:33.120]   - So Sean Carroll, so physicists.
[01:45:33.120 --> 01:45:35.080]   - Yeah, he's on the external faculty.
[01:45:35.080 --> 01:45:36.240]   - And you mentioned that there's,
[01:45:36.240 --> 01:45:37.640]   so there's some external faculty
[01:45:37.640 --> 01:45:38.480]   and there's people that are--
[01:45:38.480 --> 01:45:41.600]   - A very small group of resident faculty.
[01:45:41.600 --> 01:45:46.600]   - Maybe about 10 who are there on five-year terms
[01:45:46.600 --> 01:45:49.320]   that can sometimes get renewed.
[01:45:49.320 --> 01:45:51.280]   And then they have some postdocs,
[01:45:51.280 --> 01:45:53.680]   and then they have this much larger,
[01:45:53.680 --> 01:45:56.040]   on the order of 100, external faculty
[01:45:56.040 --> 01:45:58.240]   or people like me who come and visit
[01:45:58.240 --> 01:45:59.920]   for various periods of time.
[01:45:59.920 --> 01:46:01.120]   - So what do you think is the future
[01:46:01.120 --> 01:46:03.160]   of the Santa Fe Institute?
[01:46:03.160 --> 01:46:04.800]   And if people are interested,
[01:46:04.800 --> 01:46:09.360]   what's there in terms of the public interaction
[01:46:09.360 --> 01:46:13.360]   or students or so on that could be a possible interaction
[01:46:13.360 --> 01:46:15.680]   with the Santa Fe Institute or its ideas?
[01:46:15.680 --> 01:46:18.680]   - Yeah, so there's a few different things they do.
[01:46:18.680 --> 01:46:22.040]   They have a complex system summer school
[01:46:22.040 --> 01:46:23.720]   for graduate students and postdocs,
[01:46:23.720 --> 01:46:25.920]   and sometimes faculty attend too.
[01:46:25.920 --> 01:46:27.800]   And that's a four-week,
[01:46:27.800 --> 01:46:30.040]   very intensive residential program
[01:46:30.040 --> 01:46:32.600]   where you go and you listen to lectures
[01:46:32.600 --> 01:46:36.200]   and you do projects, and people really like that.
[01:46:36.200 --> 01:46:38.200]   I mean, it's a lot of fun.
[01:46:38.200 --> 01:46:42.440]   They also have some specialty summer schools.
[01:46:42.440 --> 01:46:45.600]   There's one on computational social science.
[01:46:45.600 --> 01:46:50.600]   There's one on climate and sustainability,
[01:46:50.600 --> 01:46:51.760]   I think it's called.
[01:46:51.760 --> 01:46:53.800]   There's a few.
[01:46:53.800 --> 01:46:56.200]   And then they have short courses
[01:46:56.200 --> 01:46:58.760]   where just a few days on different topics.
[01:46:58.760 --> 01:47:03.960]   They also have an online education platform
[01:47:03.960 --> 01:47:06.080]   that offers a lot of different courses
[01:47:06.080 --> 01:47:08.320]   and tutorials from SFI faculty,
[01:47:08.320 --> 01:47:12.960]   including an introduction to complexity course that I taught.
[01:47:12.960 --> 01:47:14.320]   (both laughing)
[01:47:14.320 --> 01:47:17.800]   - Awesome, and there's a bunch of talks too online.
[01:47:17.800 --> 01:47:19.600]   There's guest speakers and so on.
[01:47:19.600 --> 01:47:20.440]   They host a lot of different-
[01:47:20.440 --> 01:47:25.440]   - Yeah, they have sort of technical seminars and colloquia,
[01:47:25.440 --> 01:47:28.460]   and they have a community lecture series,
[01:47:28.460 --> 01:47:30.240]   like public lectures,
[01:47:30.240 --> 01:47:32.240]   and they put everything on their YouTube channel
[01:47:32.240 --> 01:47:33.720]   so you can see it all.
[01:47:33.720 --> 01:47:34.880]   - Watch it.
[01:47:34.880 --> 01:47:39.360]   - Douglas Hostadter, author of "Gertl Escherbach,"
[01:47:39.360 --> 01:47:40.600]   was your PhD advisor.
[01:47:40.600 --> 01:47:43.520]   He mentioned a couple of times, and collaborator.
[01:47:43.520 --> 01:47:45.920]   Do you have any favorite lessons or memories
[01:47:45.920 --> 01:47:47.960]   from your time working with him
[01:47:47.960 --> 01:47:50.080]   that continues to this day, I guess?
[01:47:50.080 --> 01:47:52.160]   But just even looking back
[01:47:52.160 --> 01:47:54.440]   throughout your time working with him.
[01:47:54.440 --> 01:47:57.480]   - So one of the things he taught me
[01:47:57.480 --> 01:48:02.480]   was that when you're looking at a complex problem
[01:48:04.120 --> 01:48:07.840]   to idealize it as much as possible,
[01:48:07.840 --> 01:48:12.200]   to try and figure out what is the essence of this problem.
[01:48:12.200 --> 01:48:16.560]   And this is how the CopyCat program came into being,
[01:48:16.560 --> 01:48:19.000]   was by taking analogy making and saying,
[01:48:19.000 --> 01:48:21.320]   "How can we make this as idealized as possible,
[01:48:21.320 --> 01:48:23.960]   but still retain really the important things
[01:48:23.960 --> 01:48:25.680]   we wanna study?"
[01:48:25.680 --> 01:48:30.680]   And that's really been a core theme of my research, I think.
[01:48:34.040 --> 01:48:36.480]   And I continue to try and do that.
[01:48:36.480 --> 01:48:40.200]   And it's really very much kind of physics-inspired.
[01:48:40.200 --> 01:48:42.680]   Hofstadter was a PhD in physics.
[01:48:42.680 --> 01:48:44.200]   That was his background.
[01:48:44.200 --> 01:48:45.840]   - So like first principles kind of thinking,
[01:48:45.840 --> 01:48:48.920]   like you're reduced to the most fundamental aspect
[01:48:48.920 --> 01:48:50.960]   of the problem so that you can focus
[01:48:50.960 --> 01:48:52.280]   on solving that fundamental aspect.
[01:48:52.280 --> 01:48:54.720]   - Yeah, and in AI, that was,
[01:48:54.720 --> 01:48:57.740]   people used to work in these micro-worlds, right?
[01:48:57.740 --> 01:49:01.120]   Like the blocks world was a very early,
[01:49:01.120 --> 01:49:03.080]   important area in AI.
[01:49:03.080 --> 01:49:06.040]   And then that got criticized because they said,
[01:49:06.040 --> 01:49:08.820]   "Oh, you can't scale that to the real world."
[01:49:08.820 --> 01:49:12.080]   And so people started working on much more
[01:49:12.080 --> 01:49:14.600]   real-world-like problems.
[01:49:14.600 --> 01:49:17.920]   But now there's been kind of a return,
[01:49:17.920 --> 01:49:19.800]   even to the blocks world itself.
[01:49:19.800 --> 01:49:22.240]   We've seen a lot of people who are trying to work
[01:49:22.240 --> 01:49:24.600]   on more of these very idealized problems
[01:49:24.600 --> 01:49:29.120]   for things like natural language and common sense.
[01:49:29.120 --> 01:49:32.320]   So that's an interesting evolution of those ideas.
[01:49:32.320 --> 01:49:34.640]   - So perhaps the blocks world represents
[01:49:34.640 --> 01:49:37.760]   the fundamental challenges of the problem of intelligence
[01:49:37.760 --> 01:49:38.960]   more than people realize.
[01:49:38.960 --> 01:49:40.140]   - It might, yeah.
[01:49:40.140 --> 01:49:43.520]   - Is there, sort of, when you look back
[01:49:43.520 --> 01:49:44.960]   at your body of work and your life,
[01:49:44.960 --> 01:49:47.040]   you've worked in so many different fields,
[01:49:47.040 --> 01:49:50.340]   is there something that you're just really proud of
[01:49:50.340 --> 01:49:52.440]   in terms of ideas that you've gotten a chance
[01:49:52.440 --> 01:49:54.340]   to explore, create yourself?
[01:49:54.340 --> 01:49:59.680]   - So I am really proud of my work on the Copycat project.
[01:49:59.680 --> 01:50:01.600]   I think it's really different from what
[01:50:01.600 --> 01:50:04.960]   almost everyone has done in AI.
[01:50:04.960 --> 01:50:08.960]   I think there's a lot of ideas there to be explored.
[01:50:08.960 --> 01:50:12.980]   And I guess one of the happiest days of my life,
[01:50:12.980 --> 01:50:17.460]   aside from the births of my children,
[01:50:17.460 --> 01:50:20.040]   was the birth of Copycat,
[01:50:20.040 --> 01:50:22.720]   when it actually started to be able to make
[01:50:22.720 --> 01:50:25.520]   really interesting analogies.
[01:50:25.520 --> 01:50:27.680]   And I remember that very clearly.
[01:50:27.920 --> 01:50:30.180]   - That was a very exciting time.
[01:50:30.180 --> 01:50:34.360]   - Where you kind of gave life to an artificial system.
[01:50:34.360 --> 01:50:35.200]   - That's right.
[01:50:35.200 --> 01:50:37.240]   - What, in terms of what people can interact,
[01:50:37.240 --> 01:50:40.520]   I saw there's like a, I think it's called MetaCopycat,
[01:50:40.520 --> 01:50:41.640]   or is it-- - MetaCat.
[01:50:41.640 --> 01:50:42.620]   - MetaCat.
[01:50:42.620 --> 01:50:45.600]   And there's a Python 3 implementation.
[01:50:45.600 --> 01:50:47.440]   If people actually wanted to play around with it
[01:50:47.440 --> 01:50:49.000]   and actually get into it and study it
[01:50:49.000 --> 01:50:52.400]   and maybe integrate into, whether it's with deep learning
[01:50:52.400 --> 01:50:54.400]   or any other kind of work they're doing,
[01:50:55.800 --> 01:50:58.160]   what would you suggest they do to learn more about it
[01:50:58.160 --> 01:51:01.360]   and to take it forward in different kinds of directions?
[01:51:01.360 --> 01:51:04.920]   - Yeah, so there's Douglas Hofstadter's book
[01:51:04.920 --> 01:51:07.640]   called "Fluid Concepts and Creative Analogies"
[01:51:07.640 --> 01:51:10.120]   talks in great detail about Copycat.
[01:51:10.120 --> 01:51:13.840]   I have a book called "Analogy Making as Perception,"
[01:51:13.840 --> 01:51:16.960]   which is a version of my PhD thesis on it.
[01:51:16.960 --> 01:51:19.240]   There's also code that's available
[01:51:19.240 --> 01:51:21.200]   that you can get it to run.
[01:51:21.200 --> 01:51:23.480]   I have some links on my webpage
[01:51:23.480 --> 01:51:25.480]   to where people can get the code for it.
[01:51:25.480 --> 01:51:28.640]   And I think that would really be the best way
[01:51:28.640 --> 01:51:29.480]   to get into it. - Just dive in.
[01:51:29.480 --> 01:51:31.160]   - Dive in, yeah. - And play with it.
[01:51:31.160 --> 01:51:33.920]   Well, Melanie, it was an honor talking to you.
[01:51:33.920 --> 01:51:34.760]   I really enjoyed it.
[01:51:34.760 --> 01:51:36.120]   Thank you so much for your time today.
[01:51:36.120 --> 01:51:37.720]   - Thanks, it's been really great.
[01:51:37.720 --> 01:51:40.400]   - Thanks for listening to this conversation
[01:51:40.400 --> 01:51:41.720]   with Melanie Mitchell.
[01:51:41.720 --> 01:51:45.000]   And thank you to our presenting sponsor, Cash App.
[01:51:45.000 --> 01:51:47.840]   Download it, use code LEXPODCAST.
[01:51:47.840 --> 01:51:50.800]   You'll get $10, and $10 will go to FIRST,
[01:51:50.800 --> 01:51:52.560]   a STEM education nonprofit
[01:51:52.560 --> 01:51:55.200]   that inspires hundreds of thousands of young minds
[01:51:55.200 --> 01:51:58.880]   to learn and to dream of engineering our future.
[01:51:58.880 --> 01:52:01.280]   If you enjoy this podcast, subscribe on YouTube,
[01:52:01.280 --> 01:52:03.200]   give it five stars on Apple Podcast,
[01:52:03.200 --> 01:52:06.580]   support it on Patreon, or connect with me on Twitter.
[01:52:06.580 --> 01:52:09.440]   And now, let me leave you with some words of wisdom
[01:52:09.440 --> 01:52:12.520]   from Douglas Hufstadter and Melanie Mitchell.
[01:52:12.520 --> 01:52:15.320]   "Without concepts, there can be no thought.
[01:52:15.320 --> 01:52:18.600]   "And without analogies, there can be no concepts."
[01:52:18.600 --> 01:52:20.460]   And Melanie adds,
[01:52:20.460 --> 01:52:23.560]   "How to form and fluidly use concepts
[01:52:23.560 --> 01:52:26.900]   "is the most important open problem in AI."
[01:52:26.900 --> 01:52:31.000]   Thank you for listening, and hope to see you next time.
[01:52:31.000 --> 01:52:33.580]   (upbeat music)
[01:52:33.580 --> 01:52:36.160]   (upbeat music)
[01:52:36.160 --> 01:52:46.160]   [BLANK_AUDIO]

