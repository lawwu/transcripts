
[00:00:00.000 --> 00:00:02.120]   there's a broader question here, right?
[00:00:02.120 --> 00:00:07.120]   As we build socially and emotionally intelligent machines,
[00:00:07.120 --> 00:00:10.840]   what does that mean about our relationship with them?
[00:00:10.840 --> 00:00:12.000]   And then more broadly,
[00:00:12.000 --> 00:00:13.680]   our relationship with one another, right?
[00:00:13.680 --> 00:00:15.760]   Because this machine is gonna be programmed
[00:00:15.760 --> 00:00:20.160]   to be amazing at empathy by definition, right?
[00:00:20.160 --> 00:00:21.600]   It's gonna always be there for you.
[00:00:21.600 --> 00:00:23.520]   It's not gonna get bored.
[00:00:23.520 --> 00:00:24.940]   I don't know how I feel about that.
[00:00:24.940 --> 00:00:26.200]   I think about that a lot.
[00:00:28.280 --> 00:00:31.720]   - The following is a conversation with Rana El-Khlyoubi,
[00:00:31.720 --> 00:00:34.800]   a pioneer in the field of emotion recognition
[00:00:34.800 --> 00:00:37.400]   and human-centric artificial intelligence.
[00:00:37.400 --> 00:00:39.480]   She is the founder of Effectiva,
[00:00:39.480 --> 00:00:44.120]   deputy CEO of SmartEye, author of "Girl Decoded,"
[00:00:44.120 --> 00:00:46.820]   and one of the most brilliant, kind, inspiring,
[00:00:46.820 --> 00:00:50.620]   and fun human beings I've gotten the chance to talk to.
[00:00:50.620 --> 00:00:52.800]   This is the Lex Friedman Podcast.
[00:00:52.800 --> 00:00:53.640]   To support it,
[00:00:53.640 --> 00:00:56.160]   please check out our sponsors in the description.
[00:00:56.160 --> 00:00:59.800]   And now, dear friends, here's Rana El-Khlyoubi.
[00:00:59.800 --> 00:01:03.520]   You grew up in the Middle East, in Egypt.
[00:01:03.520 --> 00:01:06.480]   What is a memory from that time that makes you smile?
[00:01:06.480 --> 00:01:08.160]   Or maybe a memory that stands out
[00:01:08.160 --> 00:01:10.720]   as helping your mind take shape
[00:01:10.720 --> 00:01:12.840]   and helping you define yourself in this world?
[00:01:12.840 --> 00:01:14.380]   - So the memory that stands out
[00:01:14.380 --> 00:01:17.480]   is we used to live in my grandma's house.
[00:01:17.480 --> 00:01:21.000]   She used to have these mango trees in her garden
[00:01:21.000 --> 00:01:21.840]   and in the summer,
[00:01:21.840 --> 00:01:24.000]   and so mango season was like July and August.
[00:01:24.080 --> 00:01:25.440]   And so in the summer,
[00:01:25.440 --> 00:01:28.360]   she would invite all my aunts and uncles and cousins.
[00:01:28.360 --> 00:01:31.200]   And it was just like maybe there were like 20 or 30 people
[00:01:31.200 --> 00:01:34.280]   in the house and she would cook all this amazing food.
[00:01:34.280 --> 00:01:38.320]   And us, the kids, we would go down the garden
[00:01:38.320 --> 00:01:40.200]   and we would pick all these mangoes.
[00:01:40.200 --> 00:01:42.220]   And I don't know,
[00:01:42.220 --> 00:01:44.720]   I think it's just the bringing people together,
[00:01:44.720 --> 00:01:47.160]   like that always stuck with me, the warmth.
[00:01:47.160 --> 00:01:48.360]   - Around the mango tree.
[00:01:48.360 --> 00:01:49.600]   - Yeah, around the mango tree.
[00:01:49.600 --> 00:01:51.720]   And there's just like the joy,
[00:01:51.720 --> 00:01:54.320]   the joy of being together around food.
[00:01:54.320 --> 00:01:57.040]   And I'm a terrible cook,
[00:01:57.040 --> 00:01:58.240]   so I guess that didn't,
[00:01:58.240 --> 00:02:02.120]   that memory didn't translate to me kind of doing the same.
[00:02:02.120 --> 00:02:03.760]   I love hosting people.
[00:02:03.760 --> 00:02:05.720]   - Do you remember colors, smells?
[00:02:05.720 --> 00:02:08.680]   Is that what, like what, how does memory work?
[00:02:08.680 --> 00:02:09.520]   - Yeah.
[00:02:09.520 --> 00:02:10.340]   - Like what do you visualize?
[00:02:10.340 --> 00:02:13.320]   Do you visualize people's faces, smiles?
[00:02:13.320 --> 00:02:15.120]   Do you, is there colors?
[00:02:15.120 --> 00:02:18.360]   Is there like a theme to the colors?
[00:02:18.360 --> 00:02:21.080]   Is it smells because of food involved?
[00:02:21.080 --> 00:02:22.880]   - Yeah, I think that's a great question.
[00:02:22.880 --> 00:02:25.240]   So those Egyptian mangoes,
[00:02:25.240 --> 00:02:27.480]   there's a particular type that I love
[00:02:27.480 --> 00:02:28.840]   and it's called Darwesi mangoes.
[00:02:28.840 --> 00:02:31.200]   And they're kind of, you know, they're oval
[00:02:31.200 --> 00:02:33.240]   and they have a little red in them.
[00:02:33.240 --> 00:02:36.920]   So I kind of, they're red and mango colored on the outside.
[00:02:36.920 --> 00:02:38.500]   So I remember that.
[00:02:38.500 --> 00:02:40.840]   - Does red indicate like extra sweetness?
[00:02:40.840 --> 00:02:41.960]   Is that? - Yes.
[00:02:41.960 --> 00:02:42.800]   - That means like it's nicely--
[00:02:42.800 --> 00:02:44.080]   - It's like really sweet.
[00:02:44.080 --> 00:02:46.240]   - Yeah, it's nice and ripe and stuff, yeah.
[00:02:46.240 --> 00:02:50.780]   What's like a definitive food of Egypt?
[00:02:50.780 --> 00:02:54.000]   You know, there's like these almost stereotypical foods
[00:02:54.000 --> 00:02:55.120]   in different parts of the world,
[00:02:55.120 --> 00:02:59.800]   like Ukraine invented borscht.
[00:02:59.800 --> 00:03:02.000]   Borscht is this beet soup with,
[00:03:02.000 --> 00:03:03.200]   that you put sour cream on.
[00:03:03.200 --> 00:03:04.040]   See, it's not, I can't see--
[00:03:04.040 --> 00:03:05.800]   - Okay, okay, well you explained it that way.
[00:03:05.800 --> 00:03:09.700]   - If you know what it is, I think you know it's delicious.
[00:03:09.700 --> 00:03:12.540]   But if I explain it, it's just not gonna sound delicious.
[00:03:12.540 --> 00:03:15.440]   I feel like beet soup, this doesn't make any sense.
[00:03:15.440 --> 00:03:16.520]   But that's kind of,
[00:03:16.520 --> 00:03:19.020]   and you probably have actually seen pictures of it
[00:03:19.020 --> 00:03:22.640]   'cause it's one of the traditional foods in Ukraine,
[00:03:22.640 --> 00:03:26.080]   in Russia, in different parts of the Slavic world.
[00:03:26.080 --> 00:03:29.860]   So, but it's become so cliche and stereotypical
[00:03:29.860 --> 00:03:30.960]   that you almost don't mention it,
[00:03:30.960 --> 00:03:32.520]   but it's still delicious.
[00:03:32.520 --> 00:03:35.600]   Like I visited Ukraine, I eat that every single day.
[00:03:35.600 --> 00:03:37.680]   - Do you make it yourself?
[00:03:37.680 --> 00:03:38.640]   How hard is it to make?
[00:03:38.640 --> 00:03:39.560]   - No, I don't know.
[00:03:39.560 --> 00:03:43.160]   I think to make it well, like anything, like Italians,
[00:03:43.160 --> 00:03:46.520]   they say, well, tomato sauce is easy to make,
[00:03:46.520 --> 00:03:48.960]   but to make it right,
[00:03:48.960 --> 00:03:51.160]   that's like a generational skill.
[00:03:51.160 --> 00:03:53.480]   So anyway, is there something like that in Egypt?
[00:03:53.480 --> 00:03:55.360]   Is there a culture of food?
[00:03:55.360 --> 00:03:59.120]   - There is, and actually we have a similar kind of soup.
[00:03:59.120 --> 00:04:00.960]   It's called molokhia,
[00:04:00.960 --> 00:04:04.520]   and it's made of this green plant.
[00:04:04.520 --> 00:04:06.920]   It's like somewhere between spinach and kale,
[00:04:06.920 --> 00:04:10.600]   and you mince it, and then you cook it in like chicken broth.
[00:04:10.600 --> 00:04:12.080]   And my grandma used to make,
[00:04:12.080 --> 00:04:14.000]   and my mom makes it really well,
[00:04:14.000 --> 00:04:16.520]   and I try to make it, but it's not as great.
[00:04:16.520 --> 00:04:17.480]   So we used to have that,
[00:04:17.480 --> 00:04:20.320]   and then we used to have it alongside stuffed pigeons.
[00:04:20.320 --> 00:04:23.560]   I'm pescetarian now, so I don't eat that anymore, but.
[00:04:23.560 --> 00:04:24.600]   - Stuffed pigeons.
[00:04:24.600 --> 00:04:25.960]   - Yeah, it's like, it was really yummy.
[00:04:25.960 --> 00:04:29.160]   It's the one thing I miss about, you know,
[00:04:29.160 --> 00:04:32.160]   now that I'm pescetarian and I don't eat.
[00:04:32.160 --> 00:04:33.000]   - The stuffed pigeons?
[00:04:33.000 --> 00:04:34.680]   - Yeah, the stuffed pigeons.
[00:04:34.680 --> 00:04:35.520]   (both laughing)
[00:04:35.520 --> 00:04:37.200]   - Is it, what are they stuffed with?
[00:04:37.200 --> 00:04:39.960]   If that doesn't bother you too much to describe.
[00:04:39.960 --> 00:04:43.480]   - No, no, it's stuffed with a lot of like just rice and.
[00:04:43.480 --> 00:04:44.320]   - Oh, got it, got it, got it.
[00:04:44.320 --> 00:04:46.120]   - Yeah, it's just rice, yeah, so.
[00:04:46.120 --> 00:04:49.920]   - And you also, you've said that you're first in your book
[00:04:49.920 --> 00:04:52.720]   that your first computer was an Atari,
[00:04:52.720 --> 00:04:55.200]   and Space Invaders was your favorite game.
[00:04:55.200 --> 00:04:58.200]   Is that when you first fell in love with computers?
[00:04:58.200 --> 00:04:59.040]   Would you say?
[00:04:59.040 --> 00:05:00.280]   - Yeah, I would say so.
[00:05:00.280 --> 00:05:02.280]   - Video games or just the computer itself?
[00:05:02.280 --> 00:05:04.300]   Just something about the machine.
[00:05:04.300 --> 00:05:05.240]   Ooh, this thing.
[00:05:05.240 --> 00:05:07.840]   There's magic in here.
[00:05:07.840 --> 00:05:10.600]   - Yeah, I think the magical moment is definitely
[00:05:10.600 --> 00:05:12.160]   like playing video games with my,
[00:05:12.160 --> 00:05:13.640]   I have two younger sisters,
[00:05:13.640 --> 00:05:17.240]   and we just like had fun together, like playing games.
[00:05:17.240 --> 00:05:21.320]   But the other memory I have is my first code.
[00:05:21.320 --> 00:05:25.760]   The first code I wrote, I wrote, I drew a Christmas tree.
[00:05:25.760 --> 00:05:26.840]   And I'm Muslim, right?
[00:05:26.840 --> 00:05:28.880]   So it's kind of, it was kind of funny
[00:05:28.880 --> 00:05:32.040]   that the first thing I did was like this Christmas tree.
[00:05:32.040 --> 00:05:35.760]   So yeah, and that's when I realized,
[00:05:35.760 --> 00:05:38.040]   wow, you can write code to do all sorts
[00:05:38.040 --> 00:05:40.600]   of like really cool stuff.
[00:05:40.600 --> 00:05:43.360]   I must have been like six or seven at the time.
[00:05:43.360 --> 00:05:45.760]   So you can write programs
[00:05:45.760 --> 00:05:48.080]   and the programs do stuff for you.
[00:05:48.080 --> 00:05:49.440]   That's power.
[00:05:49.440 --> 00:05:51.520]   That's, if you think about it, that's empowering.
[00:05:51.520 --> 00:05:52.360]   - It's AI.
[00:05:52.360 --> 00:05:53.960]   - Yeah, I know, well, it is.
[00:05:53.960 --> 00:05:55.920]   I don't know what that, you see, like,
[00:05:55.920 --> 00:05:59.000]   I don't know if many people think of it that way
[00:05:59.000 --> 00:06:00.200]   when they first learned to program.
[00:06:00.200 --> 00:06:01.500]   They just love the puzzle of it.
[00:06:01.500 --> 00:06:03.600]   Like, ooh, this is cool, this is pretty.
[00:06:03.600 --> 00:06:06.320]   It's a Christmas tree, but like, it's power.
[00:06:06.320 --> 00:06:07.880]   - It is power.
[00:06:07.880 --> 00:06:10.040]   - Eventually, I guess you couldn't at the time,
[00:06:10.040 --> 00:06:13.020]   but eventually this thing, if it's interesting enough,
[00:06:13.020 --> 00:06:14.800]   if it's a pretty enough Christmas tree,
[00:06:14.800 --> 00:06:16.720]   it can be run by millions of people
[00:06:16.720 --> 00:06:19.360]   and bring them joy, like that little thing.
[00:06:19.360 --> 00:06:22.480]   And then because it's digital, it's easy to spread.
[00:06:22.480 --> 00:06:24.120]   So like you just created something
[00:06:24.120 --> 00:06:26.800]   that's easily spreadable to millions of people.
[00:06:26.800 --> 00:06:28.280]   - Totally.
[00:06:28.280 --> 00:06:30.920]   - It's hard to think that way when you're six.
[00:06:30.920 --> 00:06:34.080]   In the book you write, "I am who I am
[00:06:34.080 --> 00:06:37.160]   because I was raised by a particular set of parents,
[00:06:37.160 --> 00:06:39.160]   both modern and conservative,
[00:06:39.160 --> 00:06:41.920]   forward-thinking yet locked in tradition.
[00:06:41.920 --> 00:06:46.200]   I'm a Muslim and I feel I'm stronger, more centered for it.
[00:06:46.200 --> 00:06:48.080]   I adhere to the values of my religion,
[00:06:48.080 --> 00:06:51.040]   even if I'm not as dutiful as I once was.
[00:06:51.040 --> 00:06:54.500]   And I am a new American and I'm thriving
[00:06:54.500 --> 00:06:57.800]   on the energy, vitality, and entrepreneurial spirit
[00:06:57.800 --> 00:06:59.080]   of this great country."
[00:06:59.080 --> 00:07:01.640]   So let me ask you about your parents.
[00:07:01.640 --> 00:07:04.040]   What have you learned about life from them,
[00:07:04.040 --> 00:07:05.360]   especially when you were young?
[00:07:05.360 --> 00:07:07.560]   - So both my parents, they're Egyptian,
[00:07:07.560 --> 00:07:09.840]   but they moved to Kuwait right after.
[00:07:09.840 --> 00:07:11.720]   They actually, there's a cute story about how they met.
[00:07:11.720 --> 00:07:15.040]   So my dad taught kobol in the '70s.
[00:07:15.040 --> 00:07:15.880]   - Nice.
[00:07:15.880 --> 00:07:18.360]   - And my mom decided to learn programming.
[00:07:18.360 --> 00:07:21.480]   So she signed up to take his kobol programming class.
[00:07:21.480 --> 00:07:25.620]   And he tried to date her and she was like,
[00:07:25.620 --> 00:07:26.720]   "No, no, no, I don't date."
[00:07:26.720 --> 00:07:28.360]   And so he's like, "Okay, I'll propose."
[00:07:28.360 --> 00:07:29.840]   And that's how they got married.
[00:07:29.840 --> 00:07:31.120]   - Whoa, strong move.
[00:07:31.120 --> 00:07:32.580]   - Right, exactly right.
[00:07:32.580 --> 00:07:35.960]   - That's really impressive.
[00:07:35.960 --> 00:07:39.100]   Those kobol guys know how to impress a lady.
[00:07:40.800 --> 00:07:43.760]   So yeah, so what have you learned from them?
[00:07:43.760 --> 00:07:44.920]   - So definitely grit.
[00:07:44.920 --> 00:07:48.600]   One of the core values in our family is just hard work.
[00:07:48.600 --> 00:07:50.920]   There were no slackers in our family.
[00:07:50.920 --> 00:07:55.200]   And that's something that's definitely stayed with me,
[00:07:55.200 --> 00:07:58.940]   both as a professional, but also in my personal life.
[00:07:58.940 --> 00:08:02.880]   But I also think my mom, my mom always used to,
[00:08:02.880 --> 00:08:07.120]   I don't know, it was like unconditional love.
[00:08:07.120 --> 00:08:10.280]   I just knew my parents would be there for me,
[00:08:10.280 --> 00:08:12.580]   kind of regardless of what I chose to do.
[00:08:12.580 --> 00:08:15.680]   And I think that's very powerful.
[00:08:15.680 --> 00:08:18.160]   And they got tested on it because I kind of challenged,
[00:08:18.160 --> 00:08:20.320]   you know, I challenged cultural norms
[00:08:20.320 --> 00:08:23.080]   and I kind of took a different path, I guess,
[00:08:23.080 --> 00:08:27.520]   than what's expected of a woman in the Middle East.
[00:08:27.520 --> 00:08:31.420]   And they still love me, which is,
[00:08:31.420 --> 00:08:32.640]   I'm so grateful for that.
[00:08:32.640 --> 00:08:33.760]   - When was like a moment
[00:08:33.760 --> 00:08:35.600]   that was the most challenging for them?
[00:08:35.600 --> 00:08:39.600]   Which moment were they kind of,
[00:08:39.600 --> 00:08:41.920]   they had to come face to face with the fact
[00:08:41.920 --> 00:08:44.240]   that you're a bit of a rebel?
[00:08:44.240 --> 00:08:47.380]   - I think the first big moment was when I,
[00:08:47.380 --> 00:08:50.400]   I had just gotten married,
[00:08:50.400 --> 00:08:54.120]   but I decided to go do my PhD at Cambridge University.
[00:08:54.120 --> 00:08:58.860]   And because my husband at the time, he's now my ex,
[00:08:58.860 --> 00:09:01.460]   ran a company in Cairo, he was gonna stay in Egypt.
[00:09:01.460 --> 00:09:04.200]   So it was gonna be a long distance relationship.
[00:09:04.200 --> 00:09:06.840]   And that's very unusual in the Middle East
[00:09:06.840 --> 00:09:10.960]   for a woman to just head out and kind of pursue her career.
[00:09:10.960 --> 00:09:15.960]   And so my dad and my parents-in-law both said,
[00:09:15.960 --> 00:09:19.840]   "You know, we do not approve of you doing this,
[00:09:19.840 --> 00:09:22.320]   but now you're under the jurisdiction of your husband,
[00:09:22.320 --> 00:09:24.720]   so he can make the call."
[00:09:24.720 --> 00:09:28.760]   And luckily for me, he was supportive.
[00:09:28.760 --> 00:09:31.640]   He said, "You know, this is your dream come true.
[00:09:31.640 --> 00:09:33.080]   You've always wanted to do a PhD.
[00:09:33.080 --> 00:09:34.280]   I'm gonna support you."
[00:09:35.400 --> 00:09:37.920]   So I think that was the first time where,
[00:09:37.920 --> 00:09:41.200]   you know, I challenged the cultural norms.
[00:09:41.200 --> 00:09:42.400]   - Was that scary?
[00:09:42.400 --> 00:09:43.520]   - Oh my God, yes.
[00:09:43.520 --> 00:09:44.880]   It was totally scary.
[00:09:44.880 --> 00:09:46.480]   - What's the biggest culture shock
[00:09:46.480 --> 00:09:51.480]   from there to Cambridge, to London?
[00:09:51.480 --> 00:09:56.500]   - Well, that was also during, right around September 11th.
[00:09:56.500 --> 00:09:59.360]   So everyone thought that there was gonna be
[00:09:59.360 --> 00:10:02.160]   a third world war, where it was really,
[00:10:03.280 --> 00:10:05.600]   and at the time I used to wear the hijab,
[00:10:05.600 --> 00:10:07.920]   so I was very visibly Muslim.
[00:10:07.920 --> 00:10:12.080]   And so my parents just were, they were afraid for my safety.
[00:10:12.080 --> 00:10:13.440]   But anyways, when I got to Cambridge,
[00:10:13.440 --> 00:10:14.440]   because I was so scared,
[00:10:14.440 --> 00:10:17.680]   I decided to take off my headscarf and wear a hat instead.
[00:10:17.680 --> 00:10:20.400]   So I just went to class wearing these like British hats,
[00:10:20.400 --> 00:10:22.840]   which was, in my opinion, actually worse
[00:10:22.840 --> 00:10:24.480]   than just showing up in a headscarf,
[00:10:24.480 --> 00:10:26.080]   'cause it was just so awkward, right?
[00:10:26.080 --> 00:10:27.760]   Like sitting in class with like all these-
[00:10:27.760 --> 00:10:30.720]   - Trying to fit in, like a spy.
[00:10:30.720 --> 00:10:31.560]   - Yeah, yeah, yeah.
[00:10:31.560 --> 00:10:33.160]   So after a few weeks of doing that,
[00:10:33.160 --> 00:10:34.840]   I was like, to heck with that,
[00:10:34.840 --> 00:10:37.320]   I'm just gonna go back to wearing my headscarf.
[00:10:37.320 --> 00:10:42.320]   - Yeah, you wore the hijab, so starting in 2000,
[00:10:42.320 --> 00:10:44.300]   and for 12 years after.
[00:10:44.300 --> 00:10:47.000]   So always, whenever you're in public,
[00:10:47.000 --> 00:10:48.920]   you have to wear the head covering.
[00:10:48.920 --> 00:10:52.320]   Can you speak to that, to the hijab,
[00:10:52.320 --> 00:10:54.120]   maybe your mixed feelings about it?
[00:10:54.120 --> 00:10:56.640]   Like what does it represent in its best case?
[00:10:56.640 --> 00:10:58.400]   What does it represent in the worst case?
[00:10:58.400 --> 00:11:02.520]   - Yeah, you know, I think there's a lot of,
[00:11:02.520 --> 00:11:05.520]   I guess I'll first start by saying I wore it voluntarily.
[00:11:05.520 --> 00:11:06.960]   I was not forced to wear it.
[00:11:06.960 --> 00:11:10.040]   And in fact, I was one of the very first women in my family
[00:11:10.040 --> 00:11:12.240]   to decide to put on the hijab.
[00:11:12.240 --> 00:11:15.240]   And my family thought it was really odd, right?
[00:11:15.240 --> 00:11:17.920]   Like they were like, why do you wanna put this on?
[00:11:17.920 --> 00:11:22.920]   And at its best, it's a sign of modesty, humility.
[00:11:22.920 --> 00:11:25.720]   - It's like me wearing a suit.
[00:11:25.720 --> 00:11:27.560]   People are like, why are you wearing a suit?
[00:11:27.560 --> 00:11:30.920]   It's a step back into some kind of tradition,
[00:11:30.920 --> 00:11:33.240]   a respect for tradition of sorts.
[00:11:33.240 --> 00:11:34.960]   So you said because it's by choice,
[00:11:34.960 --> 00:11:37.560]   you're kind of free to make that choice
[00:11:37.560 --> 00:11:40.000]   to celebrate a tradition of modesty.
[00:11:40.000 --> 00:11:43.280]   - Exactly, and I actually like made it my own.
[00:11:43.280 --> 00:11:46.880]   I remember I would really match the color of my headscarf
[00:11:46.880 --> 00:11:47.800]   with what I was wearing.
[00:11:47.800 --> 00:11:50.120]   Like it was a form of self-expression,
[00:11:50.120 --> 00:11:54.160]   and at its best, I loved wearing it.
[00:11:54.160 --> 00:11:55.800]   You know, I have a lot of questions
[00:11:55.800 --> 00:11:58.200]   around how we practice religion and religion.
[00:11:58.200 --> 00:12:02.480]   And I think also it was a time
[00:12:02.480 --> 00:12:04.720]   where I was spending a lot of time going back and forth
[00:12:04.720 --> 00:12:06.640]   between the US and Egypt.
[00:12:06.640 --> 00:12:08.560]   And I started meeting a lot of people in the US
[00:12:08.560 --> 00:12:13.360]   who were just amazing people, very purpose-driven,
[00:12:13.360 --> 00:12:15.520]   people who have very strong core values,
[00:12:15.520 --> 00:12:16.920]   but they're not Muslim.
[00:12:16.920 --> 00:12:17.880]   That's okay, right?
[00:12:17.880 --> 00:12:21.220]   And so that was when I just had a lot of questions.
[00:12:21.220 --> 00:12:25.120]   And politically also the situation in Egypt
[00:12:25.120 --> 00:12:27.240]   was when the Muslim Brotherhood ran the country,
[00:12:27.240 --> 00:12:30.160]   and I didn't agree with their ideology.
[00:12:30.160 --> 00:12:33.560]   It was at a time when I was going through a divorce.
[00:12:33.560 --> 00:12:36.040]   Like it was like just the perfect storm
[00:12:36.040 --> 00:12:39.300]   of like political, personal conditions,
[00:12:39.300 --> 00:12:42.240]   where I was like, "This doesn't feel like me anymore."
[00:12:42.240 --> 00:12:44.240]   And it took a lot of courage to take it off
[00:12:44.240 --> 00:12:48.600]   because culturally it's okay if you don't wear it,
[00:12:48.600 --> 00:12:52.080]   but it's really not okay to wear it and then take it off.
[00:12:52.080 --> 00:12:54.440]   - But you're still, so you had to do that
[00:12:54.440 --> 00:12:58.040]   while still maintaining a deep core and pride
[00:12:58.040 --> 00:13:02.440]   in the origins, in your origin story.
[00:13:02.440 --> 00:13:03.280]   - Totally.
[00:13:03.280 --> 00:13:06.760]   - So still being Egyptian, still being a Muslim.
[00:13:06.760 --> 00:13:11.760]   - Right, and being, I think, generally like faith-driven,
[00:13:11.760 --> 00:13:14.440]   but yeah.
[00:13:14.440 --> 00:13:17.600]   - But what that means changes year by year for you.
[00:13:17.600 --> 00:13:18.920]   It's like a personal journey.
[00:13:18.920 --> 00:13:20.080]   - Yeah, exactly.
[00:13:20.080 --> 00:13:22.160]   - What would you say is the role of faith
[00:13:22.160 --> 00:13:24.080]   in that part of the world?
[00:13:24.080 --> 00:13:25.120]   Like how do you see it?
[00:13:25.120 --> 00:13:27.440]   You mention it a bit in the book too.
[00:13:27.440 --> 00:13:32.120]   - Yeah, I mean, I think there is something really powerful
[00:13:32.120 --> 00:13:36.880]   about just believing that there's a bigger force.
[00:13:36.880 --> 00:13:39.460]   You know, there's a kind of surrendering, I guess,
[00:13:39.460 --> 00:13:42.120]   that comes with religion and you surrender
[00:13:42.120 --> 00:13:43.400]   and you have this deep conviction
[00:13:43.400 --> 00:13:45.380]   that it's gonna be okay, right?
[00:13:45.380 --> 00:13:48.000]   Like the universe is out to like do amazing things for you
[00:13:48.000 --> 00:13:49.600]   and it's gonna be okay.
[00:13:49.600 --> 00:13:50.960]   And there's strength to that.
[00:13:50.960 --> 00:13:53.220]   Like even when you're going through adversity,
[00:13:54.060 --> 00:13:57.940]   you just know that it's gonna work out.
[00:13:57.940 --> 00:14:00.100]   - Yeah, it gives you like an inner peace, a calmness.
[00:14:00.100 --> 00:14:01.460]   - Exactly, exactly.
[00:14:01.460 --> 00:14:05.700]   - Yeah, it's faith in all the meanings of that word.
[00:14:05.700 --> 00:14:06.540]   - Right.
[00:14:06.540 --> 00:14:08.100]   - Faith that everything is going to be okay.
[00:14:08.100 --> 00:14:13.100]   And it is because time passes and time cures all things.
[00:14:13.100 --> 00:14:16.420]   It's like a calmness with the chaos of the world.
[00:14:16.420 --> 00:14:19.460]   - And also there's like a silver lining.
[00:14:19.460 --> 00:14:20.700]   I'm a true believer of this,
[00:14:20.700 --> 00:14:24.020]   that something at the specific moment in time
[00:14:24.020 --> 00:14:25.460]   can look like it's catastrophic
[00:14:25.460 --> 00:14:28.060]   and it's not what you wanted in life, da-da-da-da.
[00:14:28.060 --> 00:14:31.300]   But then time passes and then you look back
[00:14:31.300 --> 00:14:33.060]   and there's a silver lining, right?
[00:14:33.060 --> 00:14:37.340]   It maybe closed the door, but it opened a new door for you.
[00:14:37.340 --> 00:14:38.940]   And so I'm a true believer in that,
[00:14:38.940 --> 00:14:43.860]   that there's a silver lining in almost anything in life.
[00:14:43.860 --> 00:14:45.860]   You just have to have this like,
[00:14:45.860 --> 00:14:47.940]   have faith or conviction that it's gonna work out.
[00:14:48.380 --> 00:14:50.900]   - Such a beautiful way to see a shitty feeling.
[00:14:50.900 --> 00:14:53.840]   So if you feel shitty about a current situation,
[00:14:53.840 --> 00:14:56.820]   I mean, it almost is always true,
[00:14:56.820 --> 00:15:02.780]   unless it's the cliches thing of,
[00:15:02.780 --> 00:15:03.940]   if it doesn't kill you,
[00:15:03.940 --> 00:15:06.180]   whatever doesn't kill you makes you stronger.
[00:15:06.180 --> 00:15:09.220]   It does seem that over time,
[00:15:09.220 --> 00:15:11.120]   when you take a perspective on things,
[00:15:11.120 --> 00:15:15.780]   the hardest moments and periods of your life
[00:15:15.780 --> 00:15:17.360]   are the most meaningful.
[00:15:18.200 --> 00:15:19.440]   - Yeah, yeah.
[00:15:19.440 --> 00:15:21.680]   So over time you get to have that perspective.
[00:15:21.680 --> 00:15:22.520]   - Right.
[00:15:22.520 --> 00:15:26.900]   - What about, 'cause you mentioned Kuwait,
[00:15:26.900 --> 00:15:30.380]   what about, let me ask you about war.
[00:15:30.380 --> 00:15:34.040]   What's the role of war and peace,
[00:15:34.040 --> 00:15:37.680]   maybe even the big love and hate in that part of the world,
[00:15:37.680 --> 00:15:39.720]   because it does seem to be a part of the world
[00:15:39.720 --> 00:15:41.800]   where there's turmoil.
[00:15:41.800 --> 00:15:43.960]   There was turmoil, there's still turmoil.
[00:15:45.640 --> 00:15:47.600]   - It is so unfortunate, honestly.
[00:15:47.600 --> 00:15:51.000]   It's such a waste of human resources
[00:15:51.000 --> 00:15:54.800]   and yeah, and human mindshare.
[00:15:54.800 --> 00:15:56.680]   I mean, at the end of the day,
[00:15:56.680 --> 00:15:58.360]   we all kind of want the same things.
[00:15:58.360 --> 00:16:02.160]   We want human connection, we want joy,
[00:16:02.160 --> 00:16:03.440]   we wanna feel fulfilled,
[00:16:03.440 --> 00:16:06.680]   we wanna feel a life of purpose.
[00:16:06.680 --> 00:16:11.200]   And I just find it baffling, honestly,
[00:16:11.200 --> 00:16:13.440]   that we are still having to grapple with that.
[00:16:15.240 --> 00:16:17.200]   I have a story to share about this.
[00:16:17.200 --> 00:16:19.400]   I grew up, I'm Egyptian, American now,
[00:16:19.400 --> 00:16:22.920]   but originally from Egypt.
[00:16:22.920 --> 00:16:24.800]   And when I first got to Cambridge,
[00:16:24.800 --> 00:16:26.920]   it turned out my office mate,
[00:16:26.920 --> 00:16:28.480]   like my PhD kind of,
[00:16:28.480 --> 00:16:34.080]   we ended up becoming friends, but she was from Israel.
[00:16:34.080 --> 00:16:35.560]   And we didn't know, yeah,
[00:16:35.560 --> 00:16:37.900]   we didn't know how it was gonna be like.
[00:16:37.900 --> 00:16:42.520]   - Did you guys sit there just staring at each other for a bit?
[00:16:42.520 --> 00:16:45.640]   - Actually, she, 'cause I arrived before she did
[00:16:45.640 --> 00:16:49.800]   and it turns out she emailed our PhD advisor
[00:16:49.800 --> 00:16:52.400]   and asked him if she thought it was gonna be okay.
[00:16:52.400 --> 00:16:53.240]   - Yeah.
[00:16:53.240 --> 00:16:55.120]   Oh, this is around 9/11 too.
[00:16:55.120 --> 00:16:59.520]   - Yeah, and Peter Robinson, our PhD advisor was like,
[00:16:59.520 --> 00:17:02.960]   yeah, like this is an academic institution, just show up.
[00:17:02.960 --> 00:17:04.640]   And we became super good friends.
[00:17:04.640 --> 00:17:07.360]   We were both new moms.
[00:17:07.360 --> 00:17:09.400]   Like we both had our kids during our PhD.
[00:17:09.400 --> 00:17:11.520]   We were both doing artificial emotional intelligence.
[00:17:11.520 --> 00:17:12.460]   She was looking at speech.
[00:17:12.460 --> 00:17:13.880]   I was looking at the face.
[00:17:13.880 --> 00:17:17.160]   We just had so, the culture was so similar.
[00:17:17.160 --> 00:17:18.400]   Our jokes were similar.
[00:17:18.400 --> 00:17:23.080]   It was just, I was like, why on earth are our countries,
[00:17:23.080 --> 00:17:25.320]   why is there all this like war and tension?
[00:17:25.320 --> 00:17:27.480]   And I think it falls back to the narrative, right?
[00:17:27.480 --> 00:17:28.780]   If you change the narrative,
[00:17:28.780 --> 00:17:32.600]   like whoever creates this narrative of war, I don't know.
[00:17:32.600 --> 00:17:34.800]   We should have women run the world.
[00:17:34.800 --> 00:17:38.600]   - Yeah, that's one solution, the good women,
[00:17:38.600 --> 00:17:40.720]   because there's also evil women in the world.
[00:17:40.720 --> 00:17:41.560]   - True, okay.
[00:17:41.940 --> 00:17:44.060]   (both laughing)
[00:17:44.060 --> 00:17:46.300]   - But yes, yes, there could be less war
[00:17:46.300 --> 00:17:48.220]   if women ran the world.
[00:17:48.220 --> 00:17:51.860]   The other aspect is, it doesn't matter the gender,
[00:17:51.860 --> 00:17:53.540]   the people in power.
[00:17:53.540 --> 00:17:57.100]   I get to see this with Ukraine and Russia,
[00:17:57.100 --> 00:18:01.020]   different parts of the world around that conflict now.
[00:18:01.020 --> 00:18:05.340]   And that's happening in Yemen as well and everywhere else.
[00:18:05.340 --> 00:18:08.580]   There's these narratives told by the leaders
[00:18:08.580 --> 00:18:11.340]   to the populace and those narratives take hold
[00:18:11.340 --> 00:18:15.660]   and everybody believes that and they have a distorted view
[00:18:15.660 --> 00:18:18.020]   of the humanity on the other side.
[00:18:18.020 --> 00:18:20.020]   In fact, especially during war,
[00:18:20.020 --> 00:18:25.020]   you don't even see the people on the other side as human
[00:18:25.020 --> 00:18:30.220]   or as equal intelligence or worth or value as you.
[00:18:30.220 --> 00:18:34.180]   You tell all kinds of narratives about them being Nazis
[00:18:34.180 --> 00:18:41.140]   or Dom or whatever narrative you wanna weave around that.
[00:18:41.900 --> 00:18:42.860]   Or evil.
[00:18:42.860 --> 00:18:47.420]   But I think when you actually meet them face to face,
[00:18:47.420 --> 00:18:49.220]   you realize they're the same.
[00:18:49.220 --> 00:18:50.500]   - Exactly right.
[00:18:50.500 --> 00:18:53.860]   - It's actually a big shock for people to realize
[00:18:53.860 --> 00:18:59.980]   that they've been essentially lied to within their country.
[00:18:59.980 --> 00:19:03.340]   And I kind of have faith that social media,
[00:19:03.340 --> 00:19:05.140]   as ridiculous as it is to say,
[00:19:05.140 --> 00:19:10.140]   or any kind of technology is able to bypass the walls
[00:19:10.340 --> 00:19:14.140]   that governments put up and connect people directly
[00:19:14.140 --> 00:19:15.220]   and then you get to realize,
[00:19:15.220 --> 00:19:20.020]   ooh, people fall in love across different nations
[00:19:20.020 --> 00:19:21.540]   and religions and so on.
[00:19:21.540 --> 00:19:24.660]   And that I think ultimately can cure a lot of our ills,
[00:19:24.660 --> 00:19:26.780]   especially sort of in person.
[00:19:26.780 --> 00:19:30.020]   I also think that if leaders met in person
[00:19:30.020 --> 00:19:31.020]   to have a conversation,
[00:19:31.020 --> 00:19:34.860]   that would cure a lot of the ills of the world,
[00:19:34.860 --> 00:19:35.980]   especially in private.
[00:19:37.840 --> 00:19:41.560]   - Let me ask you about the women running the world.
[00:19:41.560 --> 00:19:42.440]   - Okay.
[00:19:42.440 --> 00:19:44.920]   - So gender does in part,
[00:19:44.920 --> 00:19:49.800]   perhaps shape the landscape of just our human experience.
[00:19:49.800 --> 00:19:53.920]   So in what ways was it limiting it?
[00:19:53.920 --> 00:19:56.760]   In what ways was it empowering
[00:19:56.760 --> 00:19:59.640]   for you to be a woman in the Middle East?
[00:19:59.640 --> 00:20:02.160]   - I think just kind of just going back to like my comment
[00:20:02.160 --> 00:20:03.240]   on like women running the world,
[00:20:03.240 --> 00:20:05.040]   I think it comes back to empathy, right?
[00:20:05.360 --> 00:20:08.960]   Which has been a common thread throughout my entire career.
[00:20:08.960 --> 00:20:11.520]   And it's this idea of human connection.
[00:20:11.520 --> 00:20:14.760]   Once you build common ground with a person
[00:20:14.760 --> 00:20:17.760]   or a group of people, you build trust, you build loyalty,
[00:20:17.760 --> 00:20:21.840]   you build friendship, and then you can turn that
[00:20:21.840 --> 00:20:25.120]   into like behavior change and motivation and persuasion.
[00:20:25.120 --> 00:20:28.000]   So it's like empathy and emotions are just at the center
[00:20:28.000 --> 00:20:30.440]   of everything we do.
[00:20:31.480 --> 00:20:35.480]   And I think being from the Middle East,
[00:20:35.480 --> 00:20:38.640]   kind of this human connection is very strong.
[00:20:38.640 --> 00:20:42.120]   Like we have this running joke that if you come to Egypt
[00:20:42.120 --> 00:20:44.600]   for a visit, people are gonna,
[00:20:44.600 --> 00:20:46.720]   will know everything about your life like right away, right?
[00:20:46.720 --> 00:20:49.520]   I have no problems asking you about your personal life.
[00:20:49.520 --> 00:20:52.720]   There's no like no boundaries really,
[00:20:52.720 --> 00:20:55.240]   no personal boundaries in terms of getting to know people.
[00:20:55.240 --> 00:20:58.480]   We get emotionally intimate like very, very quickly.
[00:20:58.480 --> 00:21:00.440]   But I think people just get to know each other
[00:21:00.440 --> 00:21:03.720]   like authentically, I guess.
[00:21:03.720 --> 00:21:06.120]   There isn't this like superficial level
[00:21:06.120 --> 00:21:07.040]   of getting to know people.
[00:21:07.040 --> 00:21:09.240]   You just try to get to know people really deeply.
[00:21:09.240 --> 00:21:10.080]   - And empathy is a part of that.
[00:21:10.080 --> 00:21:11.720]   - Totally, 'cause you can put yourself
[00:21:11.720 --> 00:21:14.280]   in this person's shoe and kind of,
[00:21:14.280 --> 00:21:18.720]   yeah, imagine what challenges they're going through.
[00:21:18.720 --> 00:21:22.480]   So I think I've definitely taken that with me.
[00:21:22.480 --> 00:21:25.440]   Generosity is another one too,
[00:21:25.440 --> 00:21:28.280]   like just being generous with your time and love
[00:21:28.280 --> 00:21:32.600]   and attention and even with your wealth, right?
[00:21:32.600 --> 00:21:33.840]   Even if you don't have a lot of it,
[00:21:33.840 --> 00:21:34.920]   you're still very generous.
[00:21:34.920 --> 00:21:36.720]   And I think that's another.
[00:21:36.720 --> 00:21:40.120]   - Enjoying the humanity of other people.
[00:21:40.120 --> 00:21:42.920]   And so do you think there's a useful difference
[00:21:42.920 --> 00:21:47.640]   between men and women in that aspect and empathy?
[00:21:47.640 --> 00:21:53.720]   Or is doing these kind of big general groups,
[00:21:53.720 --> 00:21:57.040]   does that hinder progress?
[00:21:57.040 --> 00:21:59.920]   - Yeah, I actually don't wanna overgeneralize.
[00:21:59.920 --> 00:22:01.960]   I mean, some of the men I know
[00:22:01.960 --> 00:22:03.720]   are like the most empathetic humans.
[00:22:03.720 --> 00:22:05.360]   - Yeah, I strive to be empathetic.
[00:22:05.360 --> 00:22:07.480]   - Yeah, you're actually very empathetic.
[00:22:07.480 --> 00:22:13.880]   Yeah, so I don't wanna overgeneralize.
[00:22:13.880 --> 00:22:17.160]   Although one of the researchers I worked with
[00:22:17.160 --> 00:22:19.720]   when I was at Cambridge, Professor Simon Baring-Cohen,
[00:22:19.720 --> 00:22:22.040]   he's Sasha Baring-Cohen's cousin.
[00:22:22.040 --> 00:22:23.520]   - Yeah. (laughs)
[00:22:23.520 --> 00:22:26.360]   - But he runs the Autism Research Center at Cambridge
[00:22:26.360 --> 00:22:30.880]   and he's written multiple books on autism.
[00:22:30.880 --> 00:22:34.360]   And one of his theories is the empathy scale,
[00:22:34.360 --> 00:22:36.320]   like the systemizers and the empathizers.
[00:22:36.320 --> 00:22:40.040]   And there's a disproportionate amount
[00:22:40.040 --> 00:22:44.520]   of computer scientists and engineers who are systemizers
[00:22:44.520 --> 00:22:47.200]   and perhaps not great empathizers.
[00:22:47.200 --> 00:22:52.880]   And then there's more men in that bucket, I guess,
[00:22:52.880 --> 00:22:53.720]   than women.
[00:22:53.720 --> 00:22:56.840]   And then there's more women in the empathizers bucket.
[00:22:56.840 --> 00:22:58.920]   So again, not to overgeneralize.
[00:22:58.920 --> 00:23:00.400]   - I sometimes wonder about that.
[00:23:00.400 --> 00:23:03.320]   It's been frustrating to me how many, I guess,
[00:23:03.320 --> 00:23:06.040]   systemizers there are in the field of robotics.
[00:23:06.040 --> 00:23:07.040]   - Yeah.
[00:23:07.040 --> 00:23:08.280]   - It's actually encouraging to me
[00:23:08.280 --> 00:23:11.240]   'cause I care about, obviously, social robotics.
[00:23:11.240 --> 00:23:16.240]   And because there's more opportunity
[00:23:16.240 --> 00:23:18.920]   for people that are empathic. (laughs)
[00:23:18.920 --> 00:23:20.720]   - Exactly, I totally agree.
[00:23:20.720 --> 00:23:21.920]   Well, right? - So it's nice.
[00:23:21.920 --> 00:23:22.760]   - Yes.
[00:23:22.760 --> 00:23:23.720]   - I mean, most robotics I talk to,
[00:23:23.720 --> 00:23:26.080]   they don't see the human as interesting,
[00:23:26.080 --> 00:23:29.400]   as like it's not exciting.
[00:23:29.400 --> 00:23:32.200]   You wanna avoid the human at all costs.
[00:23:32.200 --> 00:23:35.360]   It's a safety concern to be touching the human,
[00:23:35.360 --> 00:23:39.360]   which it is, but it's also an opportunity
[00:23:39.360 --> 00:23:42.160]   for deep connection or collaboration
[00:23:42.160 --> 00:23:43.000]   or all that kind of stuff.
[00:23:43.000 --> 00:23:46.280]   So, and because most brilliant roboticists
[00:23:46.280 --> 00:23:48.600]   don't care about the human, it's an opportunity.
[00:23:48.600 --> 00:23:49.440]   - Right.
[00:23:49.440 --> 00:23:52.160]   - For, in your case, it's a business opportunity too,
[00:23:52.160 --> 00:23:54.720]   but in general, an opportunity to explore those ideas.
[00:23:54.720 --> 00:23:58.200]   So, in this beautiful journey to Cambridge,
[00:23:58.200 --> 00:24:01.880]   to UK, and then to America,
[00:24:01.880 --> 00:24:04.760]   what's the moment or moments
[00:24:04.760 --> 00:24:07.280]   that were most transformational for you
[00:24:07.280 --> 00:24:09.960]   as a scientist and as a leader?
[00:24:09.960 --> 00:24:12.920]   So you became an exceptionally successful CEO,
[00:24:12.920 --> 00:24:16.680]   founder, researcher, scientist, and so on.
[00:24:16.680 --> 00:24:21.240]   Was there a phase shift there
[00:24:21.240 --> 00:24:23.800]   where like, I can be somebody,
[00:24:23.800 --> 00:24:26.800]   I can really do something in this world?
[00:24:26.800 --> 00:24:29.800]   - Yeah, so actually just kind of a little bit of background.
[00:24:29.800 --> 00:24:33.960]   So the reason why I moved from Cairo to Cambridge, UK
[00:24:33.960 --> 00:24:38.200]   to do my PhD is because I had a very clear career plan.
[00:24:38.200 --> 00:24:41.320]   I was like, okay, I'll go abroad, get my PhD,
[00:24:41.320 --> 00:24:43.440]   gonna crush it in three or four years,
[00:24:43.440 --> 00:24:45.720]   come back to Egypt and teach.
[00:24:45.720 --> 00:24:47.760]   It was very clear, very well laid out.
[00:24:47.760 --> 00:24:49.480]   - Was topic clear or no?
[00:24:50.480 --> 00:24:52.480]   - Well, I did my PhD around
[00:24:52.480 --> 00:24:54.600]   building artificial emotional intelligence and looking at--
[00:24:54.600 --> 00:24:56.960]   - No, but in your master plan ahead of time,
[00:24:56.960 --> 00:24:58.320]   when you're sitting by the mango tree,
[00:24:58.320 --> 00:25:00.640]   did you know it's gonna be artificial intelligence?
[00:25:00.640 --> 00:25:03.080]   - No, no, no, that I did not know.
[00:25:03.080 --> 00:25:05.400]   Although I think I kinda knew
[00:25:05.400 --> 00:25:07.720]   that I was gonna be doing computer science,
[00:25:07.720 --> 00:25:10.400]   but I didn't know the specific area.
[00:25:10.400 --> 00:25:13.200]   But I love teaching, I mean, I still love teaching.
[00:25:13.200 --> 00:25:16.520]   So I just, yeah, I just wanted to go abroad,
[00:25:16.520 --> 00:25:19.080]   get a PhD, come back, teach.
[00:25:19.080 --> 00:25:19.960]   - Why computer science?
[00:25:19.960 --> 00:25:21.680]   Can we just linger on that?
[00:25:21.680 --> 00:25:23.600]   'Cause you're such an empathic person
[00:25:23.600 --> 00:25:25.840]   who cares about emotion, humans and so on.
[00:25:25.840 --> 00:25:30.600]   Aren't computers cold and emotionless?
[00:25:30.600 --> 00:25:31.840]   (laughing)
[00:25:31.840 --> 00:25:33.240]   Just-- - We're changing that.
[00:25:33.240 --> 00:25:36.280]   - Yeah, I know, but isn't that the,
[00:25:36.280 --> 00:25:39.840]   or did you see computers as having the capability
[00:25:39.840 --> 00:25:43.120]   to actually connect with humans?
[00:25:43.120 --> 00:25:44.720]   - I think that was my takeaway
[00:25:44.720 --> 00:25:46.520]   from my experience just growing up.
[00:25:46.520 --> 00:25:49.120]   Computers sit at the center of how we connect
[00:25:49.120 --> 00:25:51.080]   and communicate with one another, right?
[00:25:51.080 --> 00:25:52.480]   Or technology in general.
[00:25:52.480 --> 00:25:54.320]   Like I remember my first experience
[00:25:54.320 --> 00:25:55.400]   being away from my parents.
[00:25:55.400 --> 00:25:57.320]   We communicated with a fax machine,
[00:25:57.320 --> 00:25:59.200]   but thank goodness for the fax machine
[00:25:59.200 --> 00:26:01.560]   because we could send letters back and forth to each other.
[00:26:01.560 --> 00:26:03.360]   This was pre-emails and stuff.
[00:26:03.360 --> 00:26:08.200]   So I think there's, I think technology
[00:26:08.200 --> 00:26:09.880]   can be not just transformative
[00:26:09.880 --> 00:26:11.760]   in terms of productivity, et cetera.
[00:26:11.760 --> 00:26:15.720]   It actually does change how we connect with one another.
[00:26:15.720 --> 00:26:17.520]   - Can I just defend the fax machine?
[00:26:17.520 --> 00:26:19.320]   - Yeah. - There's something,
[00:26:19.320 --> 00:26:23.480]   like the haptic feel, 'cause the email is all digital.
[00:26:23.480 --> 00:26:24.600]   There's something really nice.
[00:26:24.600 --> 00:26:27.320]   I still write letters to people.
[00:26:27.320 --> 00:26:29.600]   There's something nice about the haptic aspect
[00:26:29.600 --> 00:26:31.840]   of the fax machine 'cause you still have to press,
[00:26:31.840 --> 00:26:34.240]   you still have to do something in the physical world
[00:26:34.240 --> 00:26:36.960]   to make this thing a reality, the sense of somebody.
[00:26:36.960 --> 00:26:38.960]   - Right, and then it comes out as a printout
[00:26:38.960 --> 00:26:40.720]   and you can actually touch it and read it.
[00:26:40.720 --> 00:26:44.800]   - Yeah, there's something lost when it's just an email.
[00:26:45.720 --> 00:26:50.440]   Obviously, I wonder how we can regain some of that
[00:26:50.440 --> 00:26:53.000]   in the digital world, which goes to the metaverse
[00:26:53.000 --> 00:26:53.840]   and all those kinds of things.
[00:26:53.840 --> 00:26:54.680]   We'll talk about it.
[00:26:54.680 --> 00:26:55.520]   Anyway, so--
[00:26:55.520 --> 00:26:57.800]   - Actually, do you have a question on that one?
[00:26:57.800 --> 00:27:00.520]   Do you still, do you have photo albums anymore?
[00:27:00.520 --> 00:27:02.400]   Do you still print photos?
[00:27:02.400 --> 00:27:06.720]   - No, no, but I'm a minimalist.
[00:27:06.720 --> 00:27:09.360]   So it was one of the painful steps in my life
[00:27:09.360 --> 00:27:13.360]   was to scan all the photos and let go of them
[00:27:13.360 --> 00:27:16.520]   and then let go of all my books.
[00:27:16.520 --> 00:27:17.760]   - You let go of your books?
[00:27:17.760 --> 00:27:20.840]   - Yeah, switched to Kindle, everything Kindle.
[00:27:20.840 --> 00:27:25.840]   So I thought, okay, think 30 years from now.
[00:27:25.840 --> 00:27:29.640]   Nobody's gonna have books anymore.
[00:27:29.640 --> 00:27:31.040]   The technology of digital books
[00:27:31.040 --> 00:27:32.200]   is gonna get better and better and better.
[00:27:32.200 --> 00:27:33.720]   Are you really gonna be the guy
[00:27:33.720 --> 00:27:36.400]   that's still romanticizing physical books?
[00:27:36.400 --> 00:27:38.800]   Are you gonna be the old man on the porch
[00:27:38.800 --> 00:27:40.520]   who's like kids, yes.
[00:27:40.520 --> 00:27:43.040]   So just get used to it 'cause it felt,
[00:27:43.040 --> 00:27:45.120]   it still feels a little bit uncomfortable
[00:27:45.120 --> 00:27:48.840]   to read on a Kindle, but get used to it.
[00:27:48.840 --> 00:27:51.520]   You always, I mean, I'm trying to learn
[00:27:51.520 --> 00:27:53.440]   new programming languages always.
[00:27:53.440 --> 00:27:55.000]   Like with technology, you have to kind of
[00:27:55.000 --> 00:27:57.280]   challenge yourself to adapt to it.
[00:27:57.280 --> 00:28:00.000]   I forced myself to use TikTok now.
[00:28:00.000 --> 00:28:01.560]   That thing doesn't need much forcing.
[00:28:01.560 --> 00:28:04.560]   It pulls you in like the worst kind of,
[00:28:04.560 --> 00:28:06.040]   or the best kind of drug.
[00:28:06.040 --> 00:28:11.040]   Anyway, yeah, so yeah, but I do love haptic things.
[00:28:11.920 --> 00:28:13.600]   There's a magic to the haptic.
[00:28:13.600 --> 00:28:16.400]   Even like touchscreens, it's tricky to get right,
[00:28:16.400 --> 00:28:19.640]   to get the experience of a button.
[00:28:19.640 --> 00:28:20.480]   - Yeah.
[00:28:20.480 --> 00:28:23.880]   - Anyway, what were we talking about?
[00:28:23.880 --> 00:28:27.760]   So AI, so the journey, your whole plan
[00:28:27.760 --> 00:28:31.120]   was to come back to Cairo and teach, right?
[00:28:31.120 --> 00:28:33.960]   - And then-- - What did the plan go wrong?
[00:28:33.960 --> 00:28:35.200]   - Yeah, exactly, right?
[00:28:35.200 --> 00:28:37.960]   And then I got to Cambridge and I fall in love
[00:28:37.960 --> 00:28:39.560]   with the idea of research, right?
[00:28:39.560 --> 00:28:41.640]   And kind of embarking on a path.
[00:28:41.640 --> 00:28:43.800]   Nobody's explored this path before.
[00:28:43.800 --> 00:28:45.640]   You're building stuff that nobody's built before
[00:28:45.640 --> 00:28:47.080]   and it's challenging and it's hard
[00:28:47.080 --> 00:28:49.400]   and there's a lot of non-believers.
[00:28:49.400 --> 00:28:51.040]   I just totally love that.
[00:28:51.040 --> 00:28:54.480]   And at the end of my PhD, I think it's the meeting
[00:28:54.480 --> 00:28:56.960]   that changed the trajectory of my life.
[00:28:56.960 --> 00:28:59.960]   Professor Rosalind Picard, she runs
[00:28:59.960 --> 00:29:02.240]   the Affective Computing Group at the MIT Media Lab.
[00:29:02.240 --> 00:29:03.560]   I had read her book.
[00:29:03.560 --> 00:29:07.680]   You know, I was like following all her research.
[00:29:07.680 --> 00:29:08.960]   - AKA Roz.
[00:29:08.960 --> 00:29:10.920]   - Yes, AKA Roz.
[00:29:10.920 --> 00:29:13.360]   And she was giving a talk
[00:29:13.360 --> 00:29:16.480]   at a pattern recognition conference in Cambridge
[00:29:16.480 --> 00:29:18.080]   and she had a couple of hours to kill.
[00:29:18.080 --> 00:29:19.760]   So she emailed the lab and she said,
[00:29:19.760 --> 00:29:22.400]   you know, if any students wanna meet with me,
[00:29:22.400 --> 00:29:24.800]   like just, you know, sign up here.
[00:29:24.800 --> 00:29:28.560]   And so I signed up for slot and I spent like the weeks
[00:29:28.560 --> 00:29:31.080]   leading up to it preparing for this meeting.
[00:29:31.080 --> 00:29:34.480]   And I want to show her a demo of my research and everything.
[00:29:34.480 --> 00:29:36.760]   And we met and we ended up hitting it off.
[00:29:36.760 --> 00:29:38.160]   Like we totally clicked.
[00:29:38.160 --> 00:29:40.840]   And at the end of the meeting, she said,
[00:29:40.840 --> 00:29:43.360]   do you wanna come work with me as a postdoc at MIT?
[00:29:43.360 --> 00:29:45.720]   And this is what I told her.
[00:29:45.720 --> 00:29:47.760]   I was like, okay, this would be a dream come true,
[00:29:47.760 --> 00:29:49.880]   but there's a husband waiting for me in Cairo.
[00:29:49.880 --> 00:29:51.240]   I could have to go back.
[00:29:51.240 --> 00:29:54.720]   And she said, it's fine, just commute.
[00:29:54.720 --> 00:29:57.960]   And I literally started commuting between Cairo and Boston.
[00:29:57.960 --> 00:30:01.320]   Yeah, it was a long commute.
[00:30:01.320 --> 00:30:03.480]   And I did that like every few weeks,
[00:30:03.480 --> 00:30:06.480]   I would, you know, hop on a plane and go to Boston.
[00:30:06.480 --> 00:30:08.600]   But that changed the trajectory of my life.
[00:30:08.600 --> 00:30:12.960]   There was no, I kind of outgrew my dreams, right?
[00:30:12.960 --> 00:30:16.800]   I didn't wanna go back to Egypt anymore and be faculty.
[00:30:16.800 --> 00:30:18.440]   Like that was no longer my dream.
[00:30:18.440 --> 00:30:19.280]   I had a new dream.
[00:30:19.280 --> 00:30:22.720]   - What was it like to be at MIT?
[00:30:22.720 --> 00:30:25.160]   What was that culture shock?
[00:30:25.160 --> 00:30:27.520]   You mean America in general, but also,
[00:30:27.520 --> 00:30:31.240]   I mean Cambridge has its own culture, right?
[00:30:31.240 --> 00:30:33.000]   So what was MIT like?
[00:30:33.000 --> 00:30:35.080]   What was America like?
[00:30:35.080 --> 00:30:37.960]   - I think, I wonder if that's similar to your experience
[00:30:37.960 --> 00:30:42.600]   at MIT, I was just, at the Media Lab in particular,
[00:30:42.600 --> 00:30:47.200]   I was just really, impressed is not the right word.
[00:30:47.200 --> 00:30:50.680]   I didn't expect the openness to like innovation
[00:30:50.680 --> 00:30:55.680]   and the acceptance of taking a risk and failing.
[00:30:55.680 --> 00:30:59.120]   Like failure isn't really accepted back in Egypt, right?
[00:30:59.120 --> 00:30:59.960]   You don't wanna fail.
[00:30:59.960 --> 00:31:02.000]   Like there's a fear of failure,
[00:31:02.000 --> 00:31:05.080]   which I think has been hardwired in my brain.
[00:31:05.080 --> 00:31:07.240]   But you get to MIT and it's okay to start things.
[00:31:07.240 --> 00:31:09.240]   And if they don't work out, like it's okay,
[00:31:09.240 --> 00:31:11.040]   you pivot to another idea.
[00:31:11.040 --> 00:31:13.840]   And that kind of thinking was just very new to me.
[00:31:13.840 --> 00:31:14.800]   - That's liberating.
[00:31:14.800 --> 00:31:16.200]   Well, Media Lab for people who don't know,
[00:31:16.200 --> 00:31:19.480]   MIT Media Lab is its own beautiful thing
[00:31:19.480 --> 00:31:23.880]   because they, I think more than other places at MIT,
[00:31:23.880 --> 00:31:25.160]   reach for big ideas.
[00:31:25.160 --> 00:31:27.720]   And like they try, I mean, I think,
[00:31:27.720 --> 00:31:29.200]   I mean, depending of course on who,
[00:31:29.200 --> 00:31:32.480]   but certainly with Rosalind, you try wild stuff,
[00:31:32.480 --> 00:31:34.600]   you try big things and crazy things.
[00:31:34.600 --> 00:31:38.520]   And also try to take things to completion
[00:31:38.520 --> 00:31:39.560]   so you can demo them.
[00:31:39.560 --> 00:31:43.480]   So always have a demo.
[00:31:43.480 --> 00:31:46.000]   Like if you go, one of the sad things to me
[00:31:46.000 --> 00:31:48.960]   about robotics labs at MIT, and there's like over 30,
[00:31:48.960 --> 00:31:53.960]   I think, is like, usually when you show up to a robotics lab
[00:31:53.960 --> 00:31:57.000]   there's not a single working robot, they're all broken.
[00:31:57.000 --> 00:31:58.880]   All the robots are broken,
[00:31:58.880 --> 00:32:00.680]   which is like the normal state of things
[00:32:00.680 --> 00:32:02.080]   because you're working on them.
[00:32:02.080 --> 00:32:04.360]   But it would be nice if we lived in a world
[00:32:04.360 --> 00:32:08.960]   where robotics labs had some robots functioning.
[00:32:08.960 --> 00:32:11.800]   One of my like favorite moments that just sticks with me,
[00:32:11.800 --> 00:32:14.240]   I visited Boston Dynamics and there was a,
[00:32:14.240 --> 00:32:17.960]   first of all, seeing so many spots,
[00:32:17.960 --> 00:32:21.640]   so many legged robots in one place, I'm like, I'm home.
[00:32:21.640 --> 00:32:22.840]   (both laughing)
[00:32:22.840 --> 00:32:24.680]   But the- - My drive.
[00:32:24.680 --> 00:32:25.520]   - Yeah.
[00:32:25.520 --> 00:32:28.880]   This is where I was built.
[00:32:28.880 --> 00:32:31.160]   The cool thing was just to see,
[00:32:31.160 --> 00:32:35.360]   there was a random robot spot was walking down the hall.
[00:32:35.360 --> 00:32:36.520]   It was probably doing mapping,
[00:32:36.520 --> 00:32:38.440]   but it looked like he wasn't doing anything
[00:32:38.440 --> 00:32:41.120]   and he was wearing he or she, I don't know.
[00:32:41.120 --> 00:32:44.400]   But it, well, I like, in my mind,
[00:32:44.400 --> 00:32:46.360]   there are people that have a backstory,
[00:32:46.360 --> 00:32:48.360]   but this one in particular definitely has a backstory
[00:32:48.360 --> 00:32:51.240]   because he was wearing a cowboy hat.
[00:32:51.240 --> 00:32:54.120]   So I just saw a spot robot with a cowboy hat
[00:32:54.120 --> 00:32:55.280]   walking down the hall.
[00:32:55.280 --> 00:32:59.720]   And there was just this feeling like there's a life,
[00:32:59.720 --> 00:33:01.520]   like he has a life.
[00:33:01.520 --> 00:33:04.880]   He probably has to commute back to his family at night.
[00:33:04.880 --> 00:33:08.200]   Like there's a feeling like there's life instilled
[00:33:08.200 --> 00:33:09.960]   in this robot and that's magical.
[00:33:09.960 --> 00:33:12.200]   I don't know, it was kind of inspiring to see.
[00:33:12.200 --> 00:33:14.600]   - Did it say hello to, did he say hello to you?
[00:33:14.600 --> 00:33:15.680]   Did he say hello? - No, it's very,
[00:33:15.680 --> 00:33:17.960]   there's a focused nature to the robot.
[00:33:17.960 --> 00:33:21.520]   No, no, listen, I love competence and focus and great.
[00:33:21.520 --> 00:33:23.200]   Like he was not gonna get distracted
[00:33:23.200 --> 00:33:27.760]   by the shallowness of small talk.
[00:33:27.760 --> 00:33:29.920]   There's a job to be done and he was doing it.
[00:33:29.920 --> 00:33:32.120]   So anyway, the fact that it was working
[00:33:32.120 --> 00:33:32.960]   is a beautiful thing.
[00:33:32.960 --> 00:33:35.760]   And I think Media Lab really prides itself
[00:33:35.760 --> 00:33:37.880]   on trying to always have a thing that's working
[00:33:37.880 --> 00:33:38.920]   that it could show off.
[00:33:38.920 --> 00:33:41.800]   - Yes, we used to call it demo or die.
[00:33:41.800 --> 00:33:44.680]   You could not, yeah, you could not like show up
[00:33:44.680 --> 00:33:46.040]   with like PowerPoint or something.
[00:33:46.040 --> 00:33:47.560]   You actually had to have it working.
[00:33:47.560 --> 00:33:50.120]   You know what, my son, who is now 13,
[00:33:50.120 --> 00:33:53.440]   I don't know if this is still his lifelong goal or not,
[00:33:53.440 --> 00:33:54.840]   but when he was a little younger,
[00:33:54.840 --> 00:33:56.560]   his dream is to build an island
[00:33:56.600 --> 00:34:00.040]   that's just inhabited by robots, like no humans.
[00:34:00.040 --> 00:34:02.120]   He just wants all these robots to be connecting
[00:34:02.120 --> 00:34:04.560]   and having fun and so there you go.
[00:34:04.560 --> 00:34:05.560]   - Does he have human,
[00:34:05.560 --> 00:34:09.240]   does he have an idea of which robots he loves most?
[00:34:09.240 --> 00:34:12.000]   Is it Roomba-like robots?
[00:34:12.000 --> 00:34:15.960]   Is it humanoid robots, robot dogs, or is not clear yet?
[00:34:15.960 --> 00:34:18.280]   - We used to have a Jibo,
[00:34:18.280 --> 00:34:21.520]   which was one of the MIT Media Lab spin-outs
[00:34:21.520 --> 00:34:23.360]   and he used to love Jibo. - The thing with a giant head.
[00:34:23.360 --> 00:34:24.720]   - Yes. - That spins.
[00:34:24.720 --> 00:34:25.640]   - Right, exactly.
[00:34:25.640 --> 00:34:27.480]   - It can rotate and it's an eye.
[00:34:27.480 --> 00:34:29.520]   - It has, oh, no, yeah, it can.
[00:34:29.520 --> 00:34:31.720]   - Not glowing, like. - Right, right, right, right.
[00:34:31.720 --> 00:34:33.240]   Exactly. - It's like HAL 9000,
[00:34:33.240 --> 00:34:34.440]   but the friendly version.
[00:34:34.440 --> 00:34:36.960]   - (laughs) Right, he loved that.
[00:34:36.960 --> 00:34:38.480]   And then he just loves,
[00:34:38.480 --> 00:34:43.720]   yeah, he just, I think he loves all forms of robots,
[00:34:43.720 --> 00:34:44.760]   actually.
[00:34:44.760 --> 00:34:47.840]   - So embodied intelligence. - Yes.
[00:34:47.840 --> 00:34:50.760]   - I like, I personally like legged robots, especially.
[00:34:50.760 --> 00:34:55.140]   Anything that can wiggle its butt.
[00:34:55.140 --> 00:34:56.340]   No. - And flip.
[00:34:56.340 --> 00:34:59.320]   - That's not the definition of what I love,
[00:34:59.320 --> 00:35:00.560]   but that's just technically
[00:35:00.560 --> 00:35:01.840]   what I've been working on recently.
[00:35:01.840 --> 00:35:04.560]   So I have a bunch of legged robots now in Austin
[00:35:04.560 --> 00:35:06.200]   and I've been-- - Oh, that's so cool.
[00:35:06.200 --> 00:35:10.460]   - I've been trying to have them communicate affection
[00:35:10.460 --> 00:35:13.200]   with their body in different ways, just for art.
[00:35:13.200 --> 00:35:15.280]   - That's so cool. - For art, really.
[00:35:15.280 --> 00:35:18.720]   'Cause I love the idea of walking around with the robots,
[00:35:18.720 --> 00:35:20.520]   like as you would with a dog.
[00:35:20.520 --> 00:35:22.120]   I think it's inspiring to a lot of people,
[00:35:22.120 --> 00:35:23.240]   especially young people.
[00:35:23.420 --> 00:35:25.860]   Kids love robots. - Kids love it.
[00:35:25.860 --> 00:35:28.740]   - Parents, like adults are scared of robots,
[00:35:28.740 --> 00:35:31.740]   but kids don't have this kind of weird construction
[00:35:31.740 --> 00:35:33.020]   of the world that's full of evil.
[00:35:33.020 --> 00:35:35.220]   They love cool things. - Yeah.
[00:35:35.220 --> 00:35:38.380]   I remember when Adam was in first grade,
[00:35:38.380 --> 00:35:40.140]   so he must have been like seven or so,
[00:35:40.140 --> 00:35:43.540]   I went into his class with a whole bunch of robots
[00:35:43.540 --> 00:35:45.820]   and the emotion AI demo and da-da-da.
[00:35:45.820 --> 00:35:47.740]   And I asked the kids, I was like,
[00:35:47.740 --> 00:35:50.740]   would you kids want to have a robot friend
[00:35:52.900 --> 00:35:53.740]   or a robot companion?
[00:35:53.740 --> 00:35:55.260]   Everybody said yes, and they wanted it
[00:35:55.260 --> 00:35:57.780]   for all sorts of things, like to help them
[00:35:57.780 --> 00:36:00.260]   with their math homework and to be a friend.
[00:36:00.260 --> 00:36:05.260]   So it just struck me how there was no fear of robots.
[00:36:05.260 --> 00:36:09.540]   Was a lot of adults have that, like us versus them.
[00:36:09.540 --> 00:36:12.080]   - Yeah, none of that.
[00:36:12.080 --> 00:36:13.860]   Of course, you wanna be very careful
[00:36:13.860 --> 00:36:17.060]   because you still have to look at the lessons of history
[00:36:17.060 --> 00:36:20.700]   and how robots can be used by the power centers of the world
[00:36:20.700 --> 00:36:22.620]   to abuse your rights and all that kind of stuff.
[00:36:22.620 --> 00:36:26.700]   But mostly it's good to enter anything new
[00:36:26.700 --> 00:36:28.920]   with an excitement and an optimism.
[00:36:28.920 --> 00:36:32.380]   Speaking of Roz, what have you learned
[00:36:32.380 --> 00:36:35.340]   about science and life from Rosalind Picard?
[00:36:35.340 --> 00:36:37.020]   - Oh my God, I've learned so many things
[00:36:37.020 --> 00:36:40.300]   about life from Roz.
[00:36:40.300 --> 00:36:45.300]   I think the thing I learned the most is perseverance.
[00:36:45.300 --> 00:36:49.260]   When I first met Roz, we applied,
[00:36:49.260 --> 00:36:51.420]   and she invited me to be her postdoc,
[00:36:51.420 --> 00:36:55.580]   we applied for a grant to the National Science Foundation
[00:36:55.580 --> 00:36:57.900]   to apply some of our research to autism.
[00:36:57.900 --> 00:37:01.740]   And we got back, we were rejected.
[00:37:01.740 --> 00:37:02.580]   - Rejected.
[00:37:02.580 --> 00:37:03.420]   - Yeah, and the reasoning was--
[00:37:03.420 --> 00:37:06.140]   - The first time you were rejected for fun, yeah.
[00:37:06.140 --> 00:37:09.260]   - Yeah, and I basically, I just took the rejection
[00:37:09.260 --> 00:37:11.340]   to mean, okay, we're rejected, it's done,
[00:37:11.340 --> 00:37:12.940]   like end of story, right?
[00:37:12.940 --> 00:37:15.260]   And Roz was like, it's great news.
[00:37:15.260 --> 00:37:18.340]   They love the idea, they just don't think we can do it.
[00:37:18.340 --> 00:37:21.300]   So let's build it, show them, and then reapply.
[00:37:21.300 --> 00:37:22.620]   (Roz laughs)
[00:37:22.620 --> 00:37:26.540]   And it was that, oh my God, that story totally stuck with me.
[00:37:26.540 --> 00:37:29.940]   And she's like that in every aspect of her life.
[00:37:29.940 --> 00:37:32.380]   She just does not take no for an answer.
[00:37:32.380 --> 00:37:34.620]   - The reframe all negative feedback.
[00:37:34.620 --> 00:37:36.500]   - It's a challenge.
[00:37:36.500 --> 00:37:37.340]   - It's a challenge.
[00:37:37.340 --> 00:37:38.740]   - It's a challenge.
[00:37:38.740 --> 00:37:40.180]   - Yes, they like this.
[00:37:40.180 --> 00:37:43.460]   - Yeah, yeah, yeah, it was a riot, yeah.
[00:37:43.460 --> 00:37:45.260]   - What else about science in general,
[00:37:45.260 --> 00:37:49.940]   about how you see computers, and also business,
[00:37:49.940 --> 00:37:51.740]   and just everything about the world?
[00:37:51.740 --> 00:37:54.980]   She's a very powerful, brilliant woman like yourself,
[00:37:54.980 --> 00:37:57.500]   so is there some aspect of that too?
[00:37:57.500 --> 00:38:00.460]   - Yeah, I think Roz is actually also very faith-driven.
[00:38:00.460 --> 00:38:04.660]   She has this deep belief and conviction, yeah,
[00:38:04.660 --> 00:38:07.540]   in the good in the world and humanity.
[00:38:07.540 --> 00:38:11.180]   I think that was, meeting her and her family
[00:38:11.180 --> 00:38:13.660]   was definitely a defining moment for me,
[00:38:13.660 --> 00:38:15.820]   because that was when I was like, wow,
[00:38:15.820 --> 00:38:18.980]   you can be of a different background and religion,
[00:38:18.980 --> 00:38:23.980]   and whatever, and you can still have the same core values.
[00:38:23.980 --> 00:38:25.580]   So that was, yeah.
[00:38:25.580 --> 00:38:28.820]   I'm grateful to her.
[00:38:28.820 --> 00:38:30.380]   Roz, if you're listening, thank you.
[00:38:30.380 --> 00:38:31.420]   - Yeah, she's great.
[00:38:31.420 --> 00:38:33.780]   She's been on this podcast before.
[00:38:33.780 --> 00:38:36.600]   I hope she'll be on, I'm sure she'll be on again.
[00:38:36.600 --> 00:38:42.620]   You were the founder and CEO of Affectiva,
[00:38:42.620 --> 00:38:44.740]   which is a big company that was acquired
[00:38:44.740 --> 00:38:47.140]   by another big company, SmartEye,
[00:38:47.140 --> 00:38:49.260]   and you're now the deputy CEO of SmartEye,
[00:38:49.260 --> 00:38:51.940]   so you're a powerful leader, you're brilliant,
[00:38:51.940 --> 00:38:53.500]   you're a brilliant scientist.
[00:38:53.500 --> 00:38:55.180]   A lot of people are inspired by you.
[00:38:55.180 --> 00:38:58.420]   What advice would you give, especially to young women,
[00:38:58.420 --> 00:39:01.460]   but people in general, who dream of becoming
[00:39:01.460 --> 00:39:05.220]   powerful leaders like yourself in a world where perhaps,
[00:39:05.220 --> 00:39:12.460]   in a world that perhaps doesn't give them a clear,
[00:39:13.460 --> 00:39:15.620]   easy path to do so,
[00:39:15.620 --> 00:39:18.260]   whether we're talking about Egypt or elsewhere?
[00:39:18.260 --> 00:39:23.180]   - You know, hearing you kind of describe me that way
[00:39:23.180 --> 00:39:27.460]   kind of encapsulates, I think,
[00:39:27.460 --> 00:39:29.300]   what I think is the biggest challenge of all,
[00:39:29.300 --> 00:39:32.140]   which is believing in yourself, right?
[00:39:32.140 --> 00:39:34.860]   I have had to like grapple with this,
[00:39:34.860 --> 00:39:37.660]   what I call now the Debbie Downer voice in my head.
[00:39:39.500 --> 00:39:42.860]   The kind of basically, it's just shattering all the time,
[00:39:42.860 --> 00:39:44.380]   it's basically saying, "Oh no, no, no, no,
[00:39:44.380 --> 00:39:45.580]   you can't do this.
[00:39:45.580 --> 00:39:46.460]   You're not gonna raise money.
[00:39:46.460 --> 00:39:47.620]   You can't start a company.
[00:39:47.620 --> 00:39:49.500]   What business do you have, like starting a company
[00:39:49.500 --> 00:39:51.580]   or running a company or selling a company?
[00:39:51.580 --> 00:39:52.420]   You name it."
[00:39:52.420 --> 00:39:57.300]   It's always like, and I think my biggest advice
[00:39:57.300 --> 00:40:02.300]   to not just women, but people who are taking a new path
[00:40:02.300 --> 00:40:07.260]   and they're not sure, is to not let yourself
[00:40:07.260 --> 00:40:11.020]   and let your thoughts be the biggest obstacle in your way.
[00:40:11.020 --> 00:40:15.380]   And I've had to like really work on myself
[00:40:15.380 --> 00:40:18.660]   to not be my own biggest obstacle.
[00:40:18.660 --> 00:40:20.660]   - So you got that negative voice.
[00:40:20.660 --> 00:40:21.740]   - Yeah.
[00:40:21.740 --> 00:40:22.580]   - So is that--
[00:40:22.580 --> 00:40:23.420]   - Am I the only one?
[00:40:23.420 --> 00:40:24.340]   I don't think I'm the only one.
[00:40:24.340 --> 00:40:26.020]   - No, I have that negative voice.
[00:40:26.020 --> 00:40:30.980]   I'm not exactly sure if it's a bad thing or a good thing.
[00:40:30.980 --> 00:40:34.620]   I've been really torn about it
[00:40:34.620 --> 00:40:36.620]   because it's been a lifelong companion.
[00:40:36.620 --> 00:40:37.660]   It's hard to know.
[00:40:37.660 --> 00:40:43.700]   It's kind of, it drives productivity and progress
[00:40:43.700 --> 00:40:46.940]   but it can hold you back from taking big leaps.
[00:40:46.940 --> 00:40:52.300]   I think the best I can say is probably you have
[00:40:52.300 --> 00:40:55.660]   to somehow be able to control it.
[00:40:55.660 --> 00:40:58.060]   So turn it off when it's not useful
[00:40:58.060 --> 00:41:00.500]   and turn it on when it's useful.
[00:41:00.500 --> 00:41:03.260]   Like I have from almost like a third person perspective.
[00:41:03.260 --> 00:41:05.340]   - Right, somebody who's sitting there like--
[00:41:05.340 --> 00:41:10.340]   - Yeah, like, because it is useful to be critical.
[00:41:10.340 --> 00:41:15.380]   Like after, I just gave a talk yesterday
[00:41:15.380 --> 00:41:20.900]   at MIT and I was just, you know, there's so much love
[00:41:20.900 --> 00:41:23.340]   and it was such an incredible experience.
[00:41:23.340 --> 00:41:25.860]   So many amazing people I got a chance to talk to.
[00:41:25.860 --> 00:41:29.900]   But, you know, afterwards when I went home
[00:41:29.900 --> 00:41:31.420]   and just took this long walk,
[00:41:31.420 --> 00:41:33.980]   it was mostly just negative thoughts about me.
[00:41:33.980 --> 00:41:37.060]   I don't, like one basic stuff,
[00:41:37.060 --> 00:41:38.980]   like I don't deserve any of it.
[00:41:38.980 --> 00:41:43.220]   And second is like, why did you, that was so dumb.
[00:41:43.220 --> 00:41:45.060]   That you said this, that's so dumb.
[00:41:45.060 --> 00:41:47.740]   Like you should have prepared that better.
[00:41:47.740 --> 00:41:48.980]   Why did you say this?
[00:41:48.980 --> 00:41:50.420]   The, the, the, the, the, the, the.
[00:41:50.420 --> 00:41:54.340]   But I think it's good to hear that voice out.
[00:41:54.340 --> 00:41:56.420]   All right, and like sit in that.
[00:41:56.420 --> 00:41:58.780]   And ultimately I think you grow from that.
[00:41:58.780 --> 00:42:00.500]   Now, when you're making really big decisions
[00:42:00.500 --> 00:42:03.460]   about funding or starting a company
[00:42:03.460 --> 00:42:06.340]   or taking a leap to go to the UK
[00:42:06.340 --> 00:42:10.780]   or take a leap to go to America to work in media lab.
[00:42:10.780 --> 00:42:15.540]   Though, yeah, there's a, that's,
[00:42:15.540 --> 00:42:19.100]   you should be able to shut that off then
[00:42:19.100 --> 00:42:23.780]   because you should have like this weird confidence,
[00:42:23.780 --> 00:42:25.500]   almost like faith that you said before
[00:42:25.500 --> 00:42:26.980]   that everything's gonna work out.
[00:42:26.980 --> 00:42:28.780]   So take the leap of faith.
[00:42:28.780 --> 00:42:30.340]   - Take the leap of faith.
[00:42:30.340 --> 00:42:32.580]   Despite all the negativity.
[00:42:32.580 --> 00:42:34.420]   I mean, there's, there's, there's some of that.
[00:42:34.420 --> 00:42:38.940]   You, you actually tweeted a really nice tweet thread.
[00:42:38.940 --> 00:42:41.460]   It says, quote, a year ago,
[00:42:41.460 --> 00:42:44.560]   a friend recommended I do daily affirmations.
[00:42:44.560 --> 00:42:46.900]   And I was skeptical,
[00:42:46.900 --> 00:42:49.560]   but I was going through major transitions in my life.
[00:42:49.560 --> 00:42:51.980]   So I gave it a shot and it set me on a journey
[00:42:51.980 --> 00:42:54.180]   of self-acceptance and self-love.
[00:42:54.180 --> 00:42:55.900]   So what was that like?
[00:42:56.460 --> 00:43:00.340]   Maybe talk through this idea of affirmations
[00:43:00.340 --> 00:43:01.380]   and how that helped you.
[00:43:01.380 --> 00:43:04.580]   - Yeah, because really like, I'm just like me,
[00:43:04.580 --> 00:43:06.820]   I'm a kind, I like to think of myself
[00:43:06.820 --> 00:43:08.260]   as a kind person in general,
[00:43:08.260 --> 00:43:11.380]   but I'm kind of mean to myself sometimes.
[00:43:11.380 --> 00:43:15.820]   And so I've been doing journaling for almost 10 years now.
[00:43:15.820 --> 00:43:19.020]   I use an app called Day One and it's awesome.
[00:43:19.020 --> 00:43:21.000]   I just journal and I use it as an opportunity
[00:43:21.000 --> 00:43:22.460]   to almost have a conversation
[00:43:22.460 --> 00:43:23.780]   with the Debbie Downer voice in my,
[00:43:23.780 --> 00:43:25.700]   it's like a rebuttal, right?
[00:43:25.700 --> 00:43:27.020]   Like Debbie Downer says,
[00:43:27.020 --> 00:43:29.860]   "Oh my God, you won't be able to raise this round of funding."
[00:43:29.860 --> 00:43:32.180]   I'm like, "Okay, let's talk about it."
[00:43:32.180 --> 00:43:35.660]   I have a track record of doing X, Y, and Z.
[00:43:35.660 --> 00:43:37.300]   I think I can do this.
[00:43:37.300 --> 00:43:39.380]   And it's literally like,
[00:43:39.380 --> 00:43:42.480]   so I don't know that I can shut off the voice,
[00:43:42.480 --> 00:43:44.420]   but I can have a conversation with it.
[00:43:44.420 --> 00:43:48.180]   And it just, and I bring data to the table, right?
[00:43:48.180 --> 00:43:50.840]   - Nice.
[00:43:50.840 --> 00:43:52.060]   - So that was the journaling part,
[00:43:52.060 --> 00:43:53.980]   which I found very helpful.
[00:43:53.980 --> 00:43:56.620]   But the affirmation took it to a whole next level
[00:43:56.620 --> 00:43:57.780]   and I just love it.
[00:43:57.780 --> 00:44:00.620]   I'm a year into doing this.
[00:44:00.620 --> 00:44:02.340]   And you literally wake up in the morning
[00:44:02.340 --> 00:44:05.660]   and the first thing you do, I meditate first.
[00:44:05.660 --> 00:44:07.900]   And then I write my affirmations
[00:44:07.900 --> 00:44:10.660]   and it's the energy I want to put out in the world
[00:44:10.660 --> 00:44:12.300]   that hopefully will come right back to me.
[00:44:12.300 --> 00:44:15.100]   So I will say, I always start with,
[00:44:15.100 --> 00:44:17.420]   "My smile lights up the whole world."
[00:44:17.420 --> 00:44:19.620]   And I kid you not, like people in the street will stop me
[00:44:19.620 --> 00:44:21.580]   and say, "Oh my God, like we love your smile."
[00:44:21.580 --> 00:44:22.660]   - Yeah. - Like, yes.
[00:44:23.820 --> 00:44:27.820]   So my affirmations will change depending on
[00:44:27.820 --> 00:44:28.980]   what's happening this day.
[00:44:28.980 --> 00:44:29.820]   Is it funny?
[00:44:29.820 --> 00:44:31.500]   I know, don't judge, don't judge.
[00:44:31.500 --> 00:44:33.940]   - No, that's not, laughter's not judgment.
[00:44:33.940 --> 00:44:35.180]   It's just awesome.
[00:44:35.180 --> 00:44:37.300]   I mean, it's true,
[00:44:37.300 --> 00:44:41.500]   but you're saying affirmations somehow help kind of,
[00:44:41.500 --> 00:44:42.340]   what is it?
[00:44:42.340 --> 00:44:46.300]   They do work to like remind you
[00:44:46.300 --> 00:44:47.820]   of the kind of person you are
[00:44:47.820 --> 00:44:49.540]   and the kind of person you want to be,
[00:44:49.540 --> 00:44:52.700]   which actually may be inverse order.
[00:44:52.700 --> 00:44:53.900]   The kind of person you want to be
[00:44:53.900 --> 00:44:55.900]   and that helps you become the kind of person
[00:44:55.900 --> 00:44:56.740]   you actually are.
[00:44:56.740 --> 00:44:59.380]   - It's just, it brings intentionality
[00:44:59.380 --> 00:45:01.780]   to like what you're doing, right?
[00:45:01.780 --> 00:45:02.820]   And so--
[00:45:02.820 --> 00:45:06.140]   - By the way, I was laughing because my affirmations,
[00:45:06.140 --> 00:45:07.980]   which I also do, are the opposite.
[00:45:07.980 --> 00:45:08.820]   - Oh, you do?
[00:45:08.820 --> 00:45:09.660]   Oh, what do you do?
[00:45:09.660 --> 00:45:12.380]   - I don't have a, "My smile lights up the world."
[00:45:12.380 --> 00:45:14.740]   (laughing)
[00:45:14.740 --> 00:45:18.020]   Maybe I should add that because like I have just,
[00:45:18.020 --> 00:45:19.880]   I have a, oh boy.
[00:45:21.820 --> 00:45:25.940]   It's much more stoic, like about focus,
[00:45:25.940 --> 00:45:28.180]   about this kind of stuff.
[00:45:28.180 --> 00:45:30.540]   But the joy, the emotion that you're,
[00:45:30.540 --> 00:45:33.100]   just in that little affirmation is beautiful.
[00:45:33.100 --> 00:45:34.580]   So maybe I should add that.
[00:45:34.580 --> 00:45:37.620]   - I have some like focus stuff,
[00:45:37.620 --> 00:45:38.460]   but that's usually like after--
[00:45:38.460 --> 00:45:39.380]   - But that's a cool start.
[00:45:39.380 --> 00:45:40.220]   That's a good, it's just--
[00:45:40.220 --> 00:45:41.660]   - It's after all the like smiling,
[00:45:41.660 --> 00:45:43.940]   I'm playful and joyful and all that,
[00:45:43.940 --> 00:45:45.700]   and then it's like, okay, I kick butt.
[00:45:45.700 --> 00:45:46.660]   - Let's get shit done.
[00:45:46.660 --> 00:45:47.500]   - Right, exactly.
[00:45:47.500 --> 00:45:48.820]   - Let's get shit done affirmations.
[00:45:48.820 --> 00:45:49.660]   Okay, cool.
[00:45:50.100 --> 00:45:51.500]   Like what else is on there?
[00:45:51.500 --> 00:45:54.460]   - Oh, what else is on there?
[00:45:54.460 --> 00:45:59.460]   Well, I have, I'm a magnet for all sorts of things.
[00:45:59.460 --> 00:46:02.380]   So I'm an amazing people magnet.
[00:46:02.380 --> 00:46:04.780]   I attract like awesome people into my universe.
[00:46:04.780 --> 00:46:07.060]   - So that's an actual affirmation?
[00:46:07.060 --> 00:46:08.020]   - Yes.
[00:46:08.020 --> 00:46:08.980]   - That's great.
[00:46:08.980 --> 00:46:09.820]   - Yeah.
[00:46:09.820 --> 00:46:12.340]   - So that's, and that somehow manifests itself
[00:46:12.340 --> 00:46:14.100]   into like in working.
[00:46:14.100 --> 00:46:15.620]   - I think so.
[00:46:15.620 --> 00:46:18.140]   - Yeah, like can you speak to like why it feels good
[00:46:18.140 --> 00:46:19.940]   to do the affirmations?
[00:46:19.940 --> 00:46:24.260]   - I honestly think it just grounds the day.
[00:46:24.260 --> 00:46:26.460]   And then it allows me to,
[00:46:26.460 --> 00:46:29.660]   instead of just like being pulled back and forth
[00:46:29.660 --> 00:46:31.900]   like throughout the day, it just like grounds me.
[00:46:31.900 --> 00:46:34.740]   I'm like, okay, like this thing happened.
[00:46:34.740 --> 00:46:37.500]   It's not exactly what I wanted it to be, but I'm patient.
[00:46:37.500 --> 00:46:41.420]   Or I'm, you know, I trust that the universe
[00:46:41.420 --> 00:46:42.660]   will do amazing things for me,
[00:46:42.660 --> 00:46:45.580]   which is one of my other consistent affirmations.
[00:46:45.580 --> 00:46:47.180]   Or I'm an amazing mom, right?
[00:46:47.180 --> 00:46:50.260]   And so I can grapple with all the feelings of mom guilt
[00:46:50.260 --> 00:46:51.540]   that I have all the time.
[00:46:51.540 --> 00:46:53.980]   Or here's another one.
[00:46:53.980 --> 00:46:55.220]   I'm a love magnet.
[00:46:55.220 --> 00:46:58.100]   And I literally say, I will kind of picture the person
[00:46:58.100 --> 00:46:59.220]   that I'd love to end up with.
[00:46:59.220 --> 00:47:02.620]   And I write it all down and hasn't happened yet, but it-
[00:47:02.620 --> 00:47:04.220]   - What are you picturing?
[00:47:04.220 --> 00:47:05.060]   This is Brad Pitt.
[00:47:05.060 --> 00:47:06.220]   - Okay, Brad Pitt and Brad Pitt.
[00:47:06.220 --> 00:47:07.180]   - 'Cause that's what I picture.
[00:47:07.180 --> 00:47:08.420]   - Okay, that's what you picture?
[00:47:08.420 --> 00:47:09.260]   - Yeah, yeah.
[00:47:09.260 --> 00:47:10.100]   - Okay, okay.
[00:47:10.100 --> 00:47:11.940]   - On the running, holding hands, running together.
[00:47:11.940 --> 00:47:12.780]   - Okay.
[00:47:14.340 --> 00:47:17.820]   - No, more like Fight Club, the Fight Club Brad Pitt,
[00:47:17.820 --> 00:47:20.220]   where he's like standing, all right, people will know.
[00:47:20.220 --> 00:47:22.140]   Anyway, I'm sorry, I'll get off on that.
[00:47:22.140 --> 00:47:23.740]   Do you have, like when you're thinking
[00:47:23.740 --> 00:47:27.180]   about being a love magnet in that way,
[00:47:27.180 --> 00:47:29.020]   are you picturing specific people?
[00:47:29.020 --> 00:47:34.020]   Or is this almost like in the space of like energy?
[00:47:34.020 --> 00:47:41.060]   - Right, it's somebody who is smart and well accomplished
[00:47:41.900 --> 00:47:45.340]   and successful in their life, but they're generous
[00:47:45.340 --> 00:47:49.060]   and they're well-traveled and they wanna travel the world.
[00:47:49.060 --> 00:47:49.900]   It's things like that.
[00:47:49.900 --> 00:47:51.500]   I'm like, their head over heels into me.
[00:47:51.500 --> 00:47:53.220]   It's like, I know it sounds super silly,
[00:47:53.220 --> 00:47:54.940]   but it's literally what I write.
[00:47:54.940 --> 00:47:56.500]   And I believe it'll happen one day.
[00:47:56.500 --> 00:47:58.380]   - Oh, you actually write, so you don't say it out loud?
[00:47:58.380 --> 00:47:59.220]   - No, I write it.
[00:47:59.220 --> 00:48:01.380]   I write all my affirmations.
[00:48:01.380 --> 00:48:02.820]   - I do the opposite, I say it out loud.
[00:48:02.820 --> 00:48:04.420]   - Oh, you say it out loud, interesting.
[00:48:04.420 --> 00:48:06.660]   - Yeah, if I'm alone, I'll say it out loud, yeah.
[00:48:06.660 --> 00:48:08.620]   - Interesting, I should try that.
[00:48:10.180 --> 00:48:15.180]   I think it's what feels more powerful to you,
[00:48:15.180 --> 00:48:20.500]   to me more powerful, saying stuff feels more powerful.
[00:48:20.500 --> 00:48:21.620]   - Yeah.
[00:48:21.620 --> 00:48:26.620]   - Writing feels like I'm losing the words,
[00:48:26.620 --> 00:48:32.500]   like losing the power of the words,
[00:48:32.500 --> 00:48:33.760]   maybe 'cause I write slow.
[00:48:33.760 --> 00:48:35.060]   Do you handwrite?
[00:48:35.060 --> 00:48:37.700]   - No, I type, it's on this app.
[00:48:37.700 --> 00:48:39.020]   It's day one, basically.
[00:48:39.940 --> 00:48:42.620]   The best thing about it is I can look back
[00:48:42.620 --> 00:48:46.260]   and see like a year ago, what was I affirming, right?
[00:48:46.260 --> 00:48:48.940]   - Oh, so it changes over time.
[00:48:48.940 --> 00:48:51.860]   - It hasn't like changed a lot,
[00:48:51.860 --> 00:48:54.780]   but the focus kind of changes over time.
[00:48:54.780 --> 00:48:55.620]   - I got it.
[00:48:55.620 --> 00:48:58.020]   Yeah, I say the same exact thing over and over and over.
[00:48:58.020 --> 00:48:58.860]   - Oh, you do?
[00:48:58.860 --> 00:48:59.700]   Okay.
[00:48:59.700 --> 00:49:01.260]   - There's a comfort in the sameness of it.
[00:49:01.260 --> 00:49:03.900]   Actually, let me jump around,
[00:49:03.900 --> 00:49:04.900]   'cause let me ask you about,
[00:49:04.900 --> 00:49:07.420]   'cause all this talk about Brad Pitt,
[00:49:07.420 --> 00:49:10.180]   or maybe that's just going on inside my head.
[00:49:10.180 --> 00:49:12.720]   Let me ask you about dating in general.
[00:49:12.720 --> 00:49:16.860]   You tweeted, "Are you based in Boston?"
[00:49:16.860 --> 00:49:18.700]   in single, question mark,
[00:49:18.700 --> 00:49:21.580]   and then you pointed to a startup,
[00:49:21.580 --> 00:49:24.420]   Singles Night, sponsored by Smile Dating App.
[00:49:24.420 --> 00:49:26.860]   I mean, this is jumping around a little bit,
[00:49:26.860 --> 00:49:28.460]   but since you mentioned,
[00:49:28.460 --> 00:49:34.660]   can AI help solve this dating love problem?
[00:49:34.660 --> 00:49:35.500]   What do you think?
[00:49:35.500 --> 00:49:36.940]   What's the form of connection
[00:49:36.940 --> 00:49:40.180]   that is part of the human condition?
[00:49:40.180 --> 00:49:41.620]   Can AI help that?
[00:49:41.620 --> 00:49:43.920]   You yourself are in the search affirming.
[00:49:43.920 --> 00:49:47.140]   - Maybe that's what I should affirm,
[00:49:47.140 --> 00:49:48.420]   like build an AI.
[00:49:48.420 --> 00:49:49.940]   - Build an AI that finds love?
[00:49:49.940 --> 00:49:55.780]   - I think there must be a science behind
[00:49:55.780 --> 00:49:59.220]   that first moment you meet a person
[00:49:59.220 --> 00:50:02.980]   and you either have chemistry or you don't, right?
[00:50:02.980 --> 00:50:04.740]   - I guess that was the question I was asking.
[00:50:04.740 --> 00:50:06.220]   Would you put it brilliantly?
[00:50:06.220 --> 00:50:07.740]   Is that a science or an art?
[00:50:07.740 --> 00:50:13.060]   - Ooh, I think there's actual chemicals
[00:50:13.060 --> 00:50:15.300]   that get exchanged when two people meet.
[00:50:15.300 --> 00:50:17.100]   Oh, well, I don't know about that.
[00:50:17.100 --> 00:50:18.260]   (laughing)
[00:50:18.260 --> 00:50:19.700]   - I like how you're changing,
[00:50:19.700 --> 00:50:22.180]   yeah, yeah, changing your mind as we're describing it.
[00:50:22.180 --> 00:50:24.060]   But it feels that way.
[00:50:24.060 --> 00:50:25.940]   But what science shows us
[00:50:25.940 --> 00:50:28.700]   is sometimes we can explain with rigor
[00:50:28.700 --> 00:50:30.740]   the things that feel like magic.
[00:50:30.740 --> 00:50:31.940]   - Right.
[00:50:31.940 --> 00:50:34.400]   - So maybe we can remove all the magic.
[00:50:34.400 --> 00:50:37.020]   Maybe it's like, I honestly think,
[00:50:37.020 --> 00:50:39.900]   like I said, Goodreads should be a dating app,
[00:50:39.900 --> 00:50:44.900]   which like books, I wonder if you look at just like books
[00:50:44.900 --> 00:50:47.180]   or content you've consumed.
[00:50:47.180 --> 00:50:48.740]   I mean, that's essentially what YouTube does
[00:50:48.740 --> 00:50:50.820]   when it does a recommendation.
[00:50:50.820 --> 00:50:54.540]   If you just look at your footprint of content consumed,
[00:50:54.540 --> 00:50:56.020]   if there's an overlap,
[00:50:56.020 --> 00:50:59.020]   but maybe interesting difference with an overlap,
[00:50:59.020 --> 00:51:01.660]   there's some, I'm sure this is a machine learning problem
[00:51:01.660 --> 00:51:02.580]   that's solvable.
[00:51:04.240 --> 00:51:06.880]   This person is very likely to be,
[00:51:06.880 --> 00:51:10.760]   not only there to be chemistry in the short term,
[00:51:10.760 --> 00:51:14.100]   but a good lifelong partner to grow together.
[00:51:14.100 --> 00:51:15.780]   I bet you it's a good machine learning problem.
[00:51:15.780 --> 00:51:16.620]   We just need the data.
[00:51:16.620 --> 00:51:17.500]   - Let's do it.
[00:51:17.500 --> 00:51:20.560]   Well, actually, I do think there's so much data
[00:51:20.560 --> 00:51:22.420]   about each of us that there ought to be
[00:51:22.420 --> 00:51:25.020]   a machine learning algorithm that can ingest all this data
[00:51:25.020 --> 00:51:28.060]   and basically say, I think the following 10 people
[00:51:28.060 --> 00:51:31.120]   would be interesting connections for you, right?
[00:51:32.020 --> 00:51:35.900]   And so Smile Dating App kind of took one particular angle,
[00:51:35.900 --> 00:51:36.780]   which is humor.
[00:51:36.780 --> 00:51:39.200]   It matches people based on their humor styles,
[00:51:39.200 --> 00:51:41.900]   which is one of the main ingredients
[00:51:41.900 --> 00:51:43.280]   of a successful relationship.
[00:51:43.280 --> 00:51:45.400]   Like if you meet somebody and they can make you laugh,
[00:51:45.400 --> 00:51:47.460]   like that's a good thing.
[00:51:47.460 --> 00:51:49.420]   And if you develop like internal jokes,
[00:51:49.420 --> 00:51:54.420]   like inside jokes and you're bantering, like that's fun.
[00:51:54.420 --> 00:51:56.780]   So I think.
[00:51:56.780 --> 00:51:58.560]   - Yeah, definitely.
[00:51:58.560 --> 00:52:01.880]   - But yeah, that's the number of,
[00:52:01.880 --> 00:52:05.280]   and the rate of inside joke generation.
[00:52:05.280 --> 00:52:08.280]   You could probably measure that and then optimize it
[00:52:08.280 --> 00:52:09.320]   over the first few days.
[00:52:09.320 --> 00:52:10.800]   You can see. - Right, and then.
[00:52:10.800 --> 00:52:12.660]   - We're just turning this into a machine learning problem.
[00:52:12.660 --> 00:52:13.500]   I love it.
[00:52:13.500 --> 00:52:16.000]   But for somebody like you,
[00:52:16.000 --> 00:52:18.680]   who's exceptionally successful and busy,
[00:52:18.680 --> 00:52:26.200]   is there science to that aspect of dating?
[00:52:26.200 --> 00:52:27.560]   Is it tricky?
[00:52:27.560 --> 00:52:28.880]   Is there advice you can give?
[00:52:28.880 --> 00:52:30.680]   - Oh my God, I'd give the worst advice.
[00:52:30.680 --> 00:52:33.680]   Well, I can tell you like I have a spreadsheet.
[00:52:33.680 --> 00:52:36.080]   - A spreadsheet, that's great.
[00:52:36.080 --> 00:52:37.160]   Is that a good or a bad thing?
[00:52:37.160 --> 00:52:38.660]   Do you regret the spreadsheet?
[00:52:38.660 --> 00:52:40.680]   - Well, I don't know.
[00:52:40.680 --> 00:52:42.040]   - What's the name of the spreadsheet?
[00:52:42.040 --> 00:52:42.880]   Is it love?
[00:52:42.880 --> 00:52:45.160]   - It's the dating tracker.
[00:52:45.160 --> 00:52:46.120]   - The dating tracker?
[00:52:46.120 --> 00:52:47.640]   - It's very like. - Love tracker.
[00:52:47.640 --> 00:52:48.480]   - Yeah.
[00:52:48.480 --> 00:52:50.200]   - And there's a rating system, I'm sure.
[00:52:50.200 --> 00:52:52.480]   - Yeah, there's like weights and stuff.
[00:52:52.480 --> 00:52:53.960]   - It's too close to home.
[00:52:53.960 --> 00:52:54.780]   - Oh, is it?
[00:52:54.780 --> 00:52:55.620]   Do you also have a spreadsheet?
[00:52:55.620 --> 00:52:56.440]   - Well, I don't have a spreadsheet,
[00:52:56.440 --> 00:52:58.160]   but I would, now that you say it,
[00:52:58.160 --> 00:52:59.680]   it seems like a good idea.
[00:52:59.680 --> 00:53:00.520]   - Oh no.
[00:53:00.520 --> 00:53:03.940]   - Turning into data.
[00:53:03.940 --> 00:53:09.560]   I do wish that somebody else had a spreadsheet about me.
[00:53:09.560 --> 00:53:15.040]   If it was like I said, like you said,
[00:53:15.040 --> 00:53:17.440]   convert, collect a lot of data about us
[00:53:17.440 --> 00:53:19.440]   in a way that's privacy preserving,
[00:53:19.440 --> 00:53:21.640]   that I own the data, I can control it,
[00:53:21.640 --> 00:53:23.760]   and then use that data to find,
[00:53:23.760 --> 00:53:25.240]   I mean, not just romantic love,
[00:53:25.240 --> 00:53:29.120]   but collaborators, friends, all that kind of stuff.
[00:53:29.120 --> 00:53:30.720]   It seems like the data is there.
[00:53:30.720 --> 00:53:34.240]   That's the problem social networks are trying to solve,
[00:53:34.240 --> 00:53:36.440]   but I think they're doing a really poor job.
[00:53:36.440 --> 00:53:39.720]   Even Facebook tried to get into a dating app business.
[00:53:39.720 --> 00:53:41.560]   And I think there's so many components
[00:53:41.560 --> 00:53:45.440]   to running a successful company that connects human beings.
[00:53:45.440 --> 00:53:47.560]   And part of that is,
[00:53:47.560 --> 00:53:53.600]   having engineers that care about the human side, right?
[00:53:53.600 --> 00:53:55.120]   As you know extremely well,
[00:53:55.120 --> 00:53:57.960]   it's not easy to find those.
[00:53:57.960 --> 00:54:00.240]   But you also don't want just people that care
[00:54:00.240 --> 00:54:02.360]   about the human, they also have to be good engineers.
[00:54:02.360 --> 00:54:05.920]   So it's like, you have to find this beautiful mix.
[00:54:05.920 --> 00:54:08.400]   And for some reason, just empirically speaking,
[00:54:08.400 --> 00:54:12.720]   people have not done a good job of that,
[00:54:12.720 --> 00:54:14.280]   building companies like that.
[00:54:14.280 --> 00:54:17.160]   It must mean that it's a difficult problem to solve.
[00:54:17.160 --> 00:54:20.000]   Dating apps, it seems difficult.
[00:54:20.000 --> 00:54:23.000]   Okay, Cupid, Tinder, all those kind of stuff.
[00:54:23.000 --> 00:54:27.520]   They seem to find, of course they work,
[00:54:27.520 --> 00:54:31.960]   but they seem to not work as well
[00:54:31.960 --> 00:54:34.640]   as I would imagine is possible.
[00:54:34.640 --> 00:54:35.920]   With data, wouldn't you be able
[00:54:35.920 --> 00:54:38.040]   to find better human connection?
[00:54:38.040 --> 00:54:41.520]   It's like arrange marriages on steroids, essentially.
[00:54:41.520 --> 00:54:43.520]   Arranged by machine learning algorithm.
[00:54:43.520 --> 00:54:45.120]   - Arranged by machine learning algorithm,
[00:54:45.120 --> 00:54:46.520]   but not a superficial one.
[00:54:46.520 --> 00:54:48.360]   I think a lot of the dating apps out there
[00:54:48.360 --> 00:54:49.560]   are just so superficial.
[00:54:49.560 --> 00:54:52.560]   They're just matching on high level criteria
[00:54:52.560 --> 00:54:57.040]   that aren't ingredients for successful partnership.
[00:54:57.040 --> 00:54:59.680]   But you know what's missing though, too?
[00:54:59.680 --> 00:55:00.800]   I don't know how to fix that.
[00:55:00.800 --> 00:55:02.520]   The serendipity piece of it.
[00:55:02.520 --> 00:55:04.760]   Like how do you engineer serendipity?
[00:55:04.760 --> 00:55:07.440]   Like this random chance encounter,
[00:55:07.440 --> 00:55:08.840]   and then you fall in love with the person.
[00:55:08.840 --> 00:55:11.120]   I don't know how a dating app can do that.
[00:55:11.120 --> 00:55:13.080]   So there has to be a little bit of randomness.
[00:55:13.080 --> 00:55:17.960]   Maybe every 10th match is just a,
[00:55:20.280 --> 00:55:22.320]   yeah, somebody that the algorithm
[00:55:22.320 --> 00:55:24.480]   wouldn't have necessarily recommended,
[00:55:24.480 --> 00:55:27.280]   but it allows for a little bit of.
[00:55:27.280 --> 00:55:31.400]   - Well, it can also trick you into thinking
[00:55:31.400 --> 00:55:35.480]   that serendipity by somehow showing you a tweet
[00:55:35.480 --> 00:55:39.120]   of a person that he thinks you'll match well with,
[00:55:39.120 --> 00:55:41.760]   but do it accidentally as part of another search.
[00:55:41.760 --> 00:55:46.040]   And you just notice it, and then you go down a rabbit hole
[00:55:46.040 --> 00:55:48.320]   and you connect them outside the app.
[00:55:49.400 --> 00:55:51.520]   You connect with this person outside the app somehow.
[00:55:51.520 --> 00:55:54.360]   So it's just, it creates that moment of meeting.
[00:55:54.360 --> 00:55:56.680]   Of course, you have to think of, from an app perspective,
[00:55:56.680 --> 00:55:58.420]   how you can turn that into a business.
[00:55:58.420 --> 00:56:01.220]   But I think ultimately a business
[00:56:01.220 --> 00:56:04.640]   that helps people find love in any way,
[00:56:04.640 --> 00:56:05.880]   like that's what Apple was about.
[00:56:05.880 --> 00:56:07.600]   Create products that people love.
[00:56:07.600 --> 00:56:08.440]   That's beautiful.
[00:56:08.440 --> 00:56:11.560]   I mean, you gotta make money somehow.
[00:56:11.560 --> 00:56:16.000]   If you help people fall in love personally with a product,
[00:56:16.000 --> 00:56:18.960]   find self-love or love another human being,
[00:56:18.960 --> 00:56:20.080]   you're gonna make money.
[00:56:20.080 --> 00:56:22.240]   You're gonna figure out a way to make money.
[00:56:22.240 --> 00:56:26.680]   I just feel like the dating apps often will optimize
[00:56:26.680 --> 00:56:28.680]   for something else than love.
[00:56:28.680 --> 00:56:30.120]   It's the same with social networks.
[00:56:30.120 --> 00:56:31.960]   They optimize for engagement
[00:56:31.960 --> 00:56:35.160]   as opposed to a deep, meaningful connection
[00:56:35.160 --> 00:56:38.160]   that's ultimately grounded in personal growth,
[00:56:38.160 --> 00:56:41.720]   you as a human being growing and all that kind of stuff.
[00:56:41.720 --> 00:56:45.720]   Let me do a pivot to a dark topic,
[00:56:45.720 --> 00:56:47.280]   which you opened the book with.
[00:56:48.680 --> 00:56:51.720]   A story, because I'd like to talk to you
[00:56:51.720 --> 00:56:56.120]   about just emotion and artificial intelligence,
[00:56:56.120 --> 00:56:57.440]   and I think this is a good story
[00:56:57.440 --> 00:57:00.880]   to start to think about emotional intelligence.
[00:57:00.880 --> 00:57:02.320]   You opened the book with a story
[00:57:02.320 --> 00:57:05.060]   of a Central Florida man, Jamelle Dunn,
[00:57:05.060 --> 00:57:07.280]   who was drowning and drowned
[00:57:07.280 --> 00:57:09.820]   while five teenagers watched and laughed,
[00:57:09.820 --> 00:57:11.760]   saying things like, "You're gonna die."
[00:57:11.760 --> 00:57:13.480]   And when Jamelle disappeared
[00:57:13.480 --> 00:57:14.820]   below the surface of the water,
[00:57:14.820 --> 00:57:18.440]   one of them said, "He just died," and the others laughed.
[00:57:18.440 --> 00:57:21.840]   What does this incident teach you about human nature
[00:57:21.840 --> 00:57:24.240]   and the response to it, perhaps?
[00:57:24.240 --> 00:57:26.840]   - Yeah, I mean, I think this is a really,
[00:57:26.840 --> 00:57:29.560]   really, really sad story,
[00:57:29.560 --> 00:57:32.920]   and it highlights what I believe is a,
[00:57:32.920 --> 00:57:35.520]   it's a real problem in our world today.
[00:57:35.520 --> 00:57:37.680]   It's an empathy crisis.
[00:57:37.680 --> 00:57:40.080]   Yeah, we are living through an empathy crisis.
[00:57:40.080 --> 00:57:41.360]   - Empathy crisis, yeah.
[00:57:41.360 --> 00:57:45.280]   - Yeah, and I mean, we've talked about this
[00:57:45.280 --> 00:57:46.320]   throughout our conversation.
[00:57:46.320 --> 00:57:48.040]   We dehumanize each other,
[00:57:48.040 --> 00:57:52.920]   and unfortunately, yes, technology is bringing us together,
[00:57:52.920 --> 00:57:54.800]   but in a way, it's just dehumanizing.
[00:57:54.800 --> 00:57:59.720]   It's creating this, like, yeah, dehumanizing of the other,
[00:57:59.720 --> 00:58:02.760]   and I think that's a huge problem.
[00:58:02.760 --> 00:58:05.200]   The good news is I think the solution
[00:58:05.200 --> 00:58:06.840]   could be technology-based.
[00:58:06.840 --> 00:58:09.560]   Like, I think if we rethink the way we design
[00:58:09.560 --> 00:58:11.280]   and deploy our technologies,
[00:58:11.280 --> 00:58:13.600]   we can solve parts of this problem,
[00:58:13.600 --> 00:58:14.440]   but I worry about it.
[00:58:14.440 --> 00:58:16.360]   I mean, even with my son,
[00:58:16.360 --> 00:58:21.200]   a lot of his interactions are computer-mediated,
[00:58:21.200 --> 00:58:25.680]   and I just question what that's doing to his empathy skills
[00:58:25.680 --> 00:58:29.400]   and his ability to really connect with people, so.
[00:58:29.400 --> 00:58:30.280]   - Do you think,
[00:58:30.280 --> 00:58:35.080]   you think it's not possible to form empathy
[00:58:35.080 --> 00:58:36.760]   through the digital medium?
[00:58:36.760 --> 00:58:39.760]   - I think it is,
[00:58:39.760 --> 00:58:43.120]   but we have to be thoughtful about,
[00:58:43.120 --> 00:58:45.680]   'cause the way we engage face-to-face,
[00:58:45.680 --> 00:58:47.800]   which is what we're doing right now, right?
[00:58:47.800 --> 00:58:49.320]   There's the nonverbal signals,
[00:58:49.320 --> 00:58:51.320]   which are a majority of how we communicate.
[00:58:51.320 --> 00:58:53.600]   It's like 90% of how we communicate
[00:58:53.600 --> 00:58:56.040]   is your facial expressions.
[00:58:56.040 --> 00:58:57.240]   You know, I'm saying something,
[00:58:57.240 --> 00:58:58.560]   and you're nodding your head now,
[00:58:58.560 --> 00:59:00.440]   and that creates a feedback loop,
[00:59:00.440 --> 00:59:02.600]   and if you break that-
[00:59:02.600 --> 00:59:04.000]   - And now I have anxiety about it.
[00:59:04.000 --> 00:59:06.120]   - Da-da-da, right? (laughs)
[00:59:06.120 --> 00:59:06.960]   Poor Lex.
[00:59:06.960 --> 00:59:08.880]   - Oh, boy, I wish this cycle-
[00:59:08.880 --> 00:59:10.760]   - I am not scrutinizing your facial expressions
[00:59:10.760 --> 00:59:11.600]   during this interview, right?
[00:59:11.600 --> 00:59:12.920]   - I am, I am. - Okay.
[00:59:12.920 --> 00:59:15.360]   - Look normal, look human.
[00:59:15.360 --> 00:59:17.800]   - Yeah. (laughs)
[00:59:17.800 --> 00:59:18.760]   - Nod head.
[00:59:18.760 --> 00:59:21.040]   - Yeah, nod head. (laughs)
[00:59:21.040 --> 00:59:21.880]   - In agreement.
[00:59:21.880 --> 00:59:25.840]   - If Rana says yes, then nod head else.
[00:59:25.840 --> 00:59:26.760]   - Don't do it too much,
[00:59:26.760 --> 00:59:28.760]   because it might be at the wrong time,
[00:59:28.760 --> 00:59:30.920]   and then it'll send the wrong signal.
[00:59:30.920 --> 00:59:31.760]   - Oh, God.
[00:59:31.760 --> 00:59:33.680]   - And make eye contact sometimes,
[00:59:33.680 --> 00:59:35.440]   'cause humans appreciate that.
[00:59:35.440 --> 00:59:36.280]   All right, anyway.
[00:59:36.280 --> 00:59:38.640]   - Okay. (laughs)
[00:59:38.640 --> 00:59:40.360]   - Yeah, but something about,
[00:59:40.360 --> 00:59:42.600]   especially when you say mean things in person,
[00:59:42.600 --> 00:59:44.360]   you get to see the pain of the other person.
[00:59:44.360 --> 00:59:46.360]   - Exactly, but if you're tweeting it at a person,
[00:59:46.360 --> 00:59:48.080]   and you have no idea how it's gonna land,
[00:59:48.080 --> 00:59:50.200]   you're more likely to do that on social media
[00:59:50.200 --> 00:59:52.680]   than you are in face-to-face conversations, so.
[00:59:52.680 --> 00:59:56.640]   - And what do you think is more important?
[00:59:56.640 --> 01:00:00.520]   EQ or IQ?
[01:00:00.520 --> 01:00:02.560]   EQ being emotional intelligence.
[01:00:02.560 --> 01:00:06.720]   In terms of, in what makes us human?
[01:00:06.720 --> 01:00:11.360]   - I think emotional intelligence is what makes us human.
[01:00:11.400 --> 01:00:14.960]   It's how we connect with one another,
[01:00:14.960 --> 01:00:19.800]   it's how we build trust, it's how we make decisions, right?
[01:00:19.800 --> 01:00:24.600]   Like your emotions drive kind of what you had for breakfast,
[01:00:24.600 --> 01:00:26.280]   but also where you decide to live,
[01:00:26.280 --> 01:00:28.800]   and what you wanna do for the rest of your life.
[01:00:28.800 --> 01:00:31.720]   So I think emotions are underrated.
[01:00:31.720 --> 01:00:36.440]   - So emotional intelligence isn't just about
[01:00:36.440 --> 01:00:39.320]   the effective expression of your own emotions,
[01:00:39.320 --> 01:00:41.480]   it's about a sensitivity and empathy
[01:00:41.480 --> 01:00:43.120]   to other people's emotions,
[01:00:43.120 --> 01:00:46.400]   and that sort of being able to effectively engage
[01:00:46.400 --> 01:00:48.960]   in the dance of emotions with other people.
[01:00:48.960 --> 01:00:51.400]   - Yeah, I like that explanation.
[01:00:51.400 --> 01:00:55.240]   I like that kind of, yeah, thinking about it as a dance,
[01:00:55.240 --> 01:00:56.800]   because it is really about that,
[01:00:56.800 --> 01:00:59.240]   it's about sensing what state the other person's in
[01:00:59.240 --> 01:01:01.760]   and using that information to decide
[01:01:01.760 --> 01:01:03.320]   on how you're gonna react.
[01:01:03.320 --> 01:01:06.880]   And I think it can be very powerful,
[01:01:07.040 --> 01:01:12.040]   people who are the best, most persuasive leaders
[01:01:12.040 --> 01:01:14.440]   in the world tap into,
[01:01:14.440 --> 01:01:17.920]   if you have higher EQ, you're more likely
[01:01:17.920 --> 01:01:21.520]   to be able to motivate people to change their behaviors.
[01:01:21.520 --> 01:01:25.120]   So it can be very powerful.
[01:01:25.120 --> 01:01:29.320]   - At a more kind of technical, maybe philosophical level,
[01:01:29.320 --> 01:01:32.640]   you've written that emotion is universal.
[01:01:32.640 --> 01:01:36.240]   It seems that, sort of like Chomsky says,
[01:01:36.240 --> 01:01:37.760]   "Language is universal."
[01:01:37.760 --> 01:01:39.760]   There's a bunch of other stuff like cognition,
[01:01:39.760 --> 01:01:44.200]   consciousness, it seems a lot of us have these aspects.
[01:01:44.200 --> 01:01:47.520]   So the human mind generates all this.
[01:01:47.520 --> 01:01:49.580]   So what do you think is the,
[01:01:49.580 --> 01:01:53.520]   they all seem to be like echoes of the same thing.
[01:01:53.520 --> 01:01:57.240]   What do you think emotion is exactly?
[01:01:57.240 --> 01:01:58.480]   Like how deep does it run?
[01:01:58.480 --> 01:02:02.400]   Is it a surface level thing that we display to each other?
[01:02:02.400 --> 01:02:04.200]   Is it just another form of language
[01:02:04.200 --> 01:02:05.560]   or something deep within?
[01:02:06.360 --> 01:02:08.120]   - I think it's really deep.
[01:02:08.120 --> 01:02:10.560]   It's how, we started with memory.
[01:02:10.560 --> 01:02:13.520]   I think emotions play a really important role.
[01:02:13.520 --> 01:02:17.200]   Yeah, emotions play a very important role
[01:02:17.200 --> 01:02:18.800]   in how we encode memories, right?
[01:02:18.800 --> 01:02:22.520]   Our memories are often encoded, almost indexed by emotions.
[01:02:22.520 --> 01:02:29.760]   Yeah, it's at the core of how our decision-making engine
[01:02:29.760 --> 01:02:32.880]   is also heavily influenced by our emotions.
[01:02:32.880 --> 01:02:34.800]   - So emotions is part of cognition.
[01:02:34.800 --> 01:02:35.800]   - Totally.
[01:02:35.800 --> 01:02:37.640]   - It's intermixing to the whole thing.
[01:02:37.640 --> 01:02:38.920]   - Yes, absolutely.
[01:02:38.920 --> 01:02:41.080]   And in fact, when you take it away,
[01:02:41.080 --> 01:02:42.920]   people are unable to make decisions.
[01:02:42.920 --> 01:02:43.880]   They're really paralyzed.
[01:02:43.880 --> 01:02:45.960]   Like they can't go about their daily
[01:02:45.960 --> 01:02:48.320]   or their personal or professional lives.
[01:02:48.320 --> 01:02:53.920]   - It does seem like there's probably some interesting
[01:02:53.920 --> 01:02:58.920]   interweaving of emotion and consciousness.
[01:02:58.920 --> 01:03:01.480]   I wonder if it's possible to have,
[01:03:01.480 --> 01:03:03.840]   like if they're next door neighbors somehow
[01:03:03.840 --> 01:03:07.040]   or if they're actually flatmates.
[01:03:07.040 --> 01:03:12.600]   It feels like the hard problem of consciousness
[01:03:12.600 --> 01:03:15.760]   where it feels like something to experience the thing.
[01:03:15.760 --> 01:03:20.080]   Red feels like red.
[01:03:20.080 --> 01:03:22.040]   When you eat a mango, it's sweet.
[01:03:22.040 --> 01:03:24.600]   The taste, the sweetness,
[01:03:24.600 --> 01:03:27.640]   that it feels like something to experience that sweetness.
[01:03:27.640 --> 01:03:31.800]   Whatever generates emotions.
[01:03:33.360 --> 01:03:35.600]   But then like, see, I feel like emotion
[01:03:35.600 --> 01:03:37.120]   is part of communication.
[01:03:37.120 --> 01:03:39.560]   It's very much about communication.
[01:03:39.560 --> 01:03:44.560]   And then that means it's also deeply connected to language.
[01:03:44.560 --> 01:03:50.720]   But then probably human intelligence is deeply connected
[01:03:50.720 --> 01:03:52.800]   to the collective intelligence between humans.
[01:03:52.800 --> 01:03:54.680]   It's not just a standalone thing.
[01:03:54.680 --> 01:03:56.440]   So the whole thing is really connected.
[01:03:56.440 --> 01:03:58.840]   So emotion is connected to language.
[01:03:58.840 --> 01:04:01.200]   Language is connected to intelligence.
[01:04:01.200 --> 01:04:03.360]   And then intelligence is connected to consciousness
[01:04:03.360 --> 01:04:05.880]   and consciousness is connected to emotion.
[01:04:05.880 --> 01:04:09.240]   The whole thing is a beautiful mess.
[01:04:09.240 --> 01:04:10.080]   So.
[01:04:10.080 --> 01:04:14.200]   - Can I comment on the emotions
[01:04:14.200 --> 01:04:15.800]   being a communication mechanism?
[01:04:15.800 --> 01:04:18.280]   'Cause I think there are two facets
[01:04:18.280 --> 01:04:21.600]   of our emotional experiences.
[01:04:21.600 --> 01:04:24.540]   One is communication, right?
[01:04:24.540 --> 01:04:27.840]   Like we use emotions, for example, facial expressions
[01:04:27.840 --> 01:04:29.960]   or other nonverbal cues to connect
[01:04:29.960 --> 01:04:33.960]   with other human beings and with other beings
[01:04:33.960 --> 01:04:35.040]   in the world, right?
[01:04:35.040 --> 01:04:40.680]   But even if it's not a communication context,
[01:04:40.680 --> 01:04:43.520]   we still experience emotions and we still process emotions
[01:04:43.520 --> 01:04:47.020]   and we still leverage emotions to make decisions
[01:04:47.020 --> 01:04:49.800]   and to learn and to experience life.
[01:04:49.800 --> 01:04:53.240]   So it isn't always just about communication.
[01:04:53.240 --> 01:04:54.720]   And we learned that very early on
[01:04:54.720 --> 01:04:57.640]   in kind of our work at Affectiva.
[01:04:58.600 --> 01:05:01.240]   One of the very first applications we brought to market
[01:05:01.240 --> 01:05:03.680]   was understanding how people respond to content, right?
[01:05:03.680 --> 01:05:05.520]   So if they're watching this video of ours,
[01:05:05.520 --> 01:05:06.720]   like are they interested?
[01:05:06.720 --> 01:05:07.800]   Are they inspired?
[01:05:07.800 --> 01:05:09.240]   Are they bored to death?
[01:05:09.240 --> 01:05:11.640]   And so we watched their facial expressions.
[01:05:11.640 --> 01:05:16.520]   And we weren't sure if people would express any emotions
[01:05:16.520 --> 01:05:17.480]   if they were sitting alone.
[01:05:17.480 --> 01:05:19.880]   Like if you're in your bed at night,
[01:05:19.880 --> 01:05:21.320]   watching a Netflix TV series,
[01:05:21.320 --> 01:05:23.360]   would we still see any emotions on your face?
[01:05:23.360 --> 01:05:26.040]   And we were surprised that yes, people still emote,
[01:05:26.040 --> 01:05:27.000]   even if they're alone.
[01:05:27.000 --> 01:05:29.160]   Even if you're in your car driving around,
[01:05:29.160 --> 01:05:32.400]   you're singing along a song and you're joyful,
[01:05:32.400 --> 01:05:33.680]   we'll see these expressions.
[01:05:33.680 --> 01:05:37.520]   So it's not just about communicating with another person.
[01:05:37.520 --> 01:05:40.760]   It sometimes really is just about experiencing the world.
[01:05:40.760 --> 01:05:44.240]   - First of all, I wonder if some of that
[01:05:44.240 --> 01:05:47.360]   is because we develop our intelligence
[01:05:47.360 --> 01:05:50.000]   and our emotional intelligence
[01:05:50.000 --> 01:05:51.960]   by communicating with other humans.
[01:05:51.960 --> 01:05:54.520]   And so when other humans disappear from the picture,
[01:05:54.520 --> 01:05:56.720]   we're still kind of a virtual human.
[01:05:56.720 --> 01:05:57.840]   - The code still runs, basically.
[01:05:57.840 --> 01:05:59.320]   - Yeah, the code still runs.
[01:05:59.320 --> 01:06:01.720]   But you're also kind of, you're still,
[01:06:01.720 --> 01:06:02.960]   there's like virtual humans.
[01:06:02.960 --> 01:06:04.640]   You don't have to think of it that way,
[01:06:04.640 --> 01:06:09.640]   but there's a kind of, when you like chuckle, like, yeah.
[01:06:09.640 --> 01:06:13.240]   Like you're kind of chuckling to a virtual human.
[01:06:13.240 --> 01:06:17.480]   I mean, it's possible that the code
[01:06:17.480 --> 01:06:21.920]   has to have another human there.
[01:06:21.920 --> 01:06:24.440]   Because if you just grew up alone,
[01:06:24.440 --> 01:06:28.720]   I wonder if emotion will still be there in this visual form.
[01:06:28.720 --> 01:06:30.400]   So yeah, I wonder.
[01:06:30.400 --> 01:06:35.400]   But anyway, what can you tell from the human face
[01:06:35.400 --> 01:06:38.520]   about what's going on inside?
[01:06:38.520 --> 01:06:41.200]   So that's the problem that Effektiva first tackled,
[01:06:41.200 --> 01:06:46.080]   which is using computer vision, using machine learning
[01:06:46.080 --> 01:06:49.080]   to try to detect stuff about the human face
[01:06:49.080 --> 01:06:51.000]   as many things as possible,
[01:06:51.000 --> 01:06:52.880]   and convert them into a prediction
[01:06:52.880 --> 01:06:57.600]   of categories of emotion, anger, happiness,
[01:06:57.600 --> 01:06:58.840]   all that kind of stuff.
[01:06:58.840 --> 01:07:00.440]   How hard is that problem?
[01:07:00.440 --> 01:07:01.520]   - It's extremely hard.
[01:07:01.520 --> 01:07:02.500]   It's very, very hard,
[01:07:02.500 --> 01:07:04.320]   because there is no one-to-one mapping
[01:07:04.320 --> 01:07:08.560]   between a facial expression and your internal state.
[01:07:08.560 --> 01:07:09.580]   There just isn't.
[01:07:09.580 --> 01:07:11.920]   There's this oversimplification of the problem,
[01:07:11.920 --> 01:07:13.320]   where it's something like,
[01:07:13.320 --> 01:07:15.280]   if you are smiling, then you're happy.
[01:07:15.280 --> 01:07:17.440]   If you do a brow furrow, then you're angry.
[01:07:17.440 --> 01:07:19.680]   If you do an eyebrow raise, then you're surprised.
[01:07:19.680 --> 01:07:22.320]   And just think about it for a moment.
[01:07:22.320 --> 01:07:25.160]   You could be smiling for a whole host of reasons.
[01:07:25.160 --> 01:07:27.900]   You could also be happy and not be smiling, right?
[01:07:27.900 --> 01:07:31.880]   You could furrow your eyebrows because you're angry,
[01:07:31.880 --> 01:07:35.760]   or you're confused about something, or you're constipated.
[01:07:35.760 --> 01:07:39.760]   So I think this oversimplistic approach
[01:07:39.760 --> 01:07:41.980]   to inferring emotion from a facial expression
[01:07:41.980 --> 01:07:43.200]   is really dangerous.
[01:07:43.200 --> 01:07:47.680]   The solution is to incorporate
[01:07:47.680 --> 01:07:50.120]   as many contextual signals as you can.
[01:07:50.120 --> 01:07:53.640]   So if, for example, I'm driving a car,
[01:07:53.640 --> 01:07:56.300]   and you can see me nodding my head,
[01:07:56.300 --> 01:07:59.600]   and my eyes are closed, and the blinking rate is changing,
[01:07:59.600 --> 01:08:02.400]   I'm probably falling asleep at the wheel, right?
[01:08:02.400 --> 01:08:04.620]   Because you know the context.
[01:08:04.620 --> 01:08:06.560]   You understand what the person's doing.
[01:08:06.560 --> 01:08:11.720]   Or add additional channels, like voice, or gestures,
[01:08:11.720 --> 01:08:13.960]   or even physiological sensors.
[01:08:13.960 --> 01:08:16.440]   But I think it's very dangerous
[01:08:16.440 --> 01:08:19.640]   to just take this oversimplistic approach of,
[01:08:19.640 --> 01:08:21.640]   yeah, smile equals happy.
[01:08:21.640 --> 01:08:24.120]   - If you're able to, in a high-resolution way,
[01:08:24.120 --> 01:08:26.520]   specify the context, there's certain things
[01:08:26.520 --> 01:08:29.720]   that are gonna be somewhat reliable signals
[01:08:29.720 --> 01:08:32.960]   of something like drowsiness, or happiness,
[01:08:32.960 --> 01:08:34.320]   or stuff like that.
[01:08:34.320 --> 01:08:37.040]   I mean, when people are watching Netflix content,
[01:08:37.040 --> 01:08:41.460]   that problem, that's a really compelling idea
[01:08:41.460 --> 01:08:44.600]   that you can kind of, at least in aggregate--
[01:08:44.600 --> 01:08:45.960]   - Exactly. - Highlight,
[01:08:45.960 --> 01:08:48.960]   like which part was boring, which part was exciting.
[01:08:48.960 --> 01:08:50.380]   How hard was that problem?
[01:08:50.380 --> 01:08:54.920]   - That was on the scale of difficulty.
[01:08:54.920 --> 01:08:57.800]   I think that's one of the easier problems to solve,
[01:08:57.800 --> 01:09:01.720]   because it's a relatively constrained environment.
[01:09:01.720 --> 01:09:03.920]   You have somebody sitting in front of,
[01:09:03.920 --> 01:09:06.440]   initially we started with a device in front of you,
[01:09:06.440 --> 01:09:08.940]   like a laptop, and then we graduated
[01:09:08.940 --> 01:09:11.920]   to doing this on a mobile phone, which is a lot harder,
[01:09:11.920 --> 01:09:15.480]   just because of, from a computer vision perspective,
[01:09:15.480 --> 01:09:18.440]   the profile view of the face can be a lot more challenging.
[01:09:19.160 --> 01:09:20.800]   We had to figure out lighting conditions,
[01:09:20.800 --> 01:09:23.880]   because usually people are watching content
[01:09:23.880 --> 01:09:26.800]   literally in their bedrooms at night, lights are dimmed.
[01:09:26.800 --> 01:09:29.800]   - Yeah, I mean, if you're standing,
[01:09:29.800 --> 01:09:33.120]   it's probably gonna be the looking up.
[01:09:33.120 --> 01:09:34.720]   - The nostril view. - Yeah.
[01:09:34.720 --> 01:09:38.000]   And nobody looks good at, I've seen data sets
[01:09:38.000 --> 01:09:39.400]   from that perspective, it's like,
[01:09:39.400 --> 01:09:43.160]   ugh, this is not a good look for anyone.
[01:09:43.160 --> 01:09:45.240]   Or if you're laying in bed at night,
[01:09:45.240 --> 01:09:47.520]   what is it, side view or something?
[01:09:47.520 --> 01:09:50.240]   And half your face is on a pillow.
[01:09:50.240 --> 01:09:53.680]   Actually, I would love to have data
[01:09:53.680 --> 01:09:58.680]   about how people watch stuff in bed at night.
[01:09:58.680 --> 01:10:03.600]   Do they prop their, is it a pillow?
[01:10:03.600 --> 01:10:07.520]   I'm sure there's a lot of interesting dynamics there.
[01:10:07.520 --> 01:10:09.520]   - From a health and wellbeing perspective, right?
[01:10:09.520 --> 01:10:11.200]   Like, it's like, oh, you're hurting your neck.
[01:10:11.200 --> 01:10:13.200]   - I was thinking machine learning perspective, but yes.
[01:10:13.200 --> 01:10:14.320]   But also, yeah.
[01:10:15.680 --> 01:10:17.880]   Once you have that data, you can start making
[01:10:17.880 --> 01:10:20.120]   all kinds of inference about health and stuff like that.
[01:10:20.120 --> 01:10:21.360]   - Interesting.
[01:10:21.360 --> 01:10:23.200]   - Yeah, there was an interesting thing
[01:10:23.200 --> 01:10:25.360]   when I was at Google that we were,
[01:10:25.360 --> 01:10:29.560]   it's called active authentication,
[01:10:29.560 --> 01:10:33.440]   where you want to be able to unlock your phone
[01:10:33.440 --> 01:10:34.720]   without using a password.
[01:10:34.720 --> 01:10:38.600]   So it would face, but also other stuff.
[01:10:38.600 --> 01:10:41.200]   Like the way you take a phone out of the pocket.
[01:10:41.200 --> 01:10:42.640]   - Amazing. - So that kind of data,
[01:10:42.640 --> 01:10:46.080]   to use the multimodal with machine learning
[01:10:46.080 --> 01:10:47.760]   to be able to identify that it's you,
[01:10:47.760 --> 01:10:50.800]   or likely to be you, likely not to be you.
[01:10:50.800 --> 01:10:52.960]   That allows you to not always have to enter the password.
[01:10:52.960 --> 01:10:54.480]   That was the idea.
[01:10:54.480 --> 01:10:56.560]   But the funny thing about that is,
[01:10:56.560 --> 01:10:58.880]   I just want to tell a small anecdote,
[01:10:58.880 --> 01:11:01.920]   is 'cause it was all male engineers.
[01:11:01.920 --> 01:11:08.480]   Except, so my boss is, our boss,
[01:11:08.920 --> 01:11:12.720]   who's still one of my favorite humans, was a woman,
[01:11:12.720 --> 01:11:13.560]   Regina Dugan.
[01:11:13.560 --> 01:11:15.560]   - Oh my God, I love her.
[01:11:15.560 --> 01:11:16.720]   She's awesome. - Yeah, she's the best.
[01:11:16.720 --> 01:11:17.560]   She's the best.
[01:11:17.560 --> 01:11:25.520]   So, but anyway, and there was one female,
[01:11:25.520 --> 01:11:27.040]   brilliant female engineer on the team.
[01:11:27.040 --> 01:11:28.960]   And she was the one that actually highlighted the fact
[01:11:28.960 --> 01:11:31.560]   that women often don't have pockets.
[01:11:31.560 --> 01:11:32.400]   - Right, right.
[01:11:32.400 --> 01:11:36.120]   - It was like, whoa, that was not even a category
[01:11:36.120 --> 01:11:38.720]   in the code of like, wait a minute,
[01:11:38.720 --> 01:11:42.000]   you can take the phone out of some other place
[01:11:42.000 --> 01:11:43.040]   than your pocket.
[01:11:43.040 --> 01:11:45.360]   So anyway, that's a funny thing
[01:11:45.360 --> 01:11:47.000]   when you're considering people laying in bed,
[01:11:47.000 --> 01:11:48.960]   watching a phone, you have to consider,
[01:11:48.960 --> 01:11:53.440]   you have to, you know, diversity in all its forms,
[01:11:53.440 --> 01:11:55.720]   depending on the problem, depending on the context.
[01:11:55.720 --> 01:11:57.800]   - Actually, this is like a very important,
[01:11:57.800 --> 01:11:59.200]   I think this is, you know,
[01:11:59.200 --> 01:12:00.680]   you probably get this all the time,
[01:12:00.680 --> 01:12:03.320]   like people are worried that AI's gonna take over humanity
[01:12:03.320 --> 01:12:06.120]   and like, get rid of all the humans in the world.
[01:12:06.120 --> 01:12:08.320]   I'm like, actually, that's not my biggest concern.
[01:12:08.320 --> 01:12:10.600]   My biggest concern is that we are building bias
[01:12:10.600 --> 01:12:12.440]   into these systems.
[01:12:12.440 --> 01:12:16.400]   And then they're like deployed at large and at scale.
[01:12:16.400 --> 01:12:19.360]   And before you know it, you're kind of accentuating
[01:12:19.360 --> 01:12:21.240]   the bias that exists in society.
[01:12:21.240 --> 01:12:24.440]   - Yeah, I'm not, you know, I know people,
[01:12:24.440 --> 01:12:25.960]   it's very important to worry about that,
[01:12:25.960 --> 01:12:30.960]   but the worry is an emergent phenomena to me,
[01:12:30.960 --> 01:12:33.540]   which is a very good one,
[01:12:33.540 --> 01:12:36.020]   because I think these systems are actually,
[01:12:37.000 --> 01:12:39.760]   by encoding the data that exists,
[01:12:39.760 --> 01:12:42.280]   they're revealing the bias in society,
[01:12:42.280 --> 01:12:45.260]   they're therefore teaching us what the bias is,
[01:12:45.260 --> 01:12:48.280]   therefore we can now improve that bias within the system.
[01:12:48.280 --> 01:12:51.280]   So they're almost like putting a mirror to ourselves.
[01:12:51.280 --> 01:12:53.000]   So I'm not--
[01:12:53.000 --> 01:12:55.120]   - We have to be open to looking at the mirror, though.
[01:12:55.120 --> 01:12:58.160]   We have to be open to scrutinizing the data,
[01:12:58.160 --> 01:13:01.160]   if you just take it as ground truth.
[01:13:01.160 --> 01:13:02.560]   - Or you don't even have to look at the,
[01:13:02.560 --> 01:13:04.400]   I mean, yes, the data is how you fix it,
[01:13:04.400 --> 01:13:06.560]   but then you just look at the behavior of the system.
[01:13:06.560 --> 01:13:08.000]   It's like, and you realize,
[01:13:08.000 --> 01:13:10.280]   holy crap, this thing is kind of racist.
[01:13:10.280 --> 01:13:11.240]   Like, why is that?
[01:13:11.240 --> 01:13:13.200]   And then you look at the data, it's like, oh, okay.
[01:13:13.200 --> 01:13:14.600]   And then you start to realize that,
[01:13:14.600 --> 01:13:16.760]   I think that it's a much more effective way
[01:13:16.760 --> 01:13:20.320]   to be introspective as a society
[01:13:20.320 --> 01:13:23.000]   than through sort of political discourse.
[01:13:23.000 --> 01:13:27.360]   Like AI kind of, 'cause people are easy,
[01:13:27.360 --> 01:13:33.280]   people are, for some reason, more productive and rigorous
[01:13:33.280 --> 01:13:35.840]   in criticizing AI than they're criticizing each other.
[01:13:35.840 --> 01:13:39.520]   So I think this is just a nice method for studying society
[01:13:39.520 --> 01:13:42.560]   and see which way progress lies.
[01:13:42.560 --> 01:13:44.560]   Anyway, what were we talking about?
[01:13:44.560 --> 01:13:47.400]   You're watching, the problem of watching Netflix in bed
[01:13:47.400 --> 01:13:50.760]   or elsewhere and seeing which parts are exciting,
[01:13:50.760 --> 01:13:51.800]   which parts are boring.
[01:13:51.800 --> 01:13:53.200]   You're saying that's--
[01:13:53.200 --> 01:13:55.200]   - Relatively constrained because, you know,
[01:13:55.200 --> 01:13:57.880]   you have a captive audience and you kind of know the context.
[01:13:57.880 --> 01:14:01.160]   And one thing you said that was really key is the,
[01:14:01.160 --> 01:14:02.520]   you're doing this in aggregate, right?
[01:14:02.520 --> 01:14:04.840]   Like we're looking at aggregated response of people.
[01:14:04.840 --> 01:14:07.720]   And so when you see a peak, say a smile peak,
[01:14:07.720 --> 01:14:10.320]   they're probably smiling or laughing
[01:14:10.320 --> 01:14:12.440]   at something that's in the content.
[01:14:12.440 --> 01:14:16.760]   So that was one of the first problems we were able to solve.
[01:14:16.760 --> 01:14:19.000]   And when we see the smile peak,
[01:14:19.000 --> 01:14:21.720]   it doesn't mean that these people are internally happy.
[01:14:21.720 --> 01:14:23.360]   They're just laughing at content.
[01:14:23.360 --> 01:14:27.520]   So it's important to, you know, call it for what it is.
[01:14:27.520 --> 01:14:29.640]   - But it's still really, really useful data.
[01:14:29.640 --> 01:14:30.480]   - Oh, yeah.
[01:14:30.480 --> 01:14:31.720]   - I wonder how that compares to,
[01:14:31.720 --> 01:14:34.080]   so what like YouTube and other places will use
[01:14:34.920 --> 01:14:38.080]   is obviously they don't have,
[01:14:38.080 --> 01:14:41.560]   for the most case, they don't have that kind of data.
[01:14:41.560 --> 01:14:45.800]   They have the data of when people tune out,
[01:14:45.800 --> 01:14:47.480]   like switch and drop off.
[01:14:47.480 --> 01:14:50.280]   And I think that's a, in aggregate for YouTube,
[01:14:50.280 --> 01:14:52.040]   at least a pretty powerful signal.
[01:14:52.040 --> 01:14:54.160]   I worry about what that leads to
[01:14:54.160 --> 01:15:00.040]   because looking at like YouTubers
[01:15:00.040 --> 01:15:01.840]   that are kind of really care about views
[01:15:01.840 --> 01:15:06.240]   and, you know, try to maximize the number of views.
[01:15:06.240 --> 01:15:09.400]   I think they, when they say that the video
[01:15:09.400 --> 01:15:12.360]   should be constantly interesting,
[01:15:12.360 --> 01:15:14.400]   which seems like a good goal,
[01:15:14.400 --> 01:15:19.400]   I feel like that leads to this manic pace of a video.
[01:15:19.400 --> 01:15:23.800]   Like the idea that I would speak at the current speed
[01:15:23.800 --> 01:15:26.400]   that I'm speaking, I don't know.
[01:15:26.400 --> 01:15:29.680]   - And that every moment has to be engaging, right?
[01:15:29.680 --> 01:15:30.520]   - Engaging.
[01:15:30.520 --> 01:15:31.360]   - Yeah, that's.
[01:15:31.360 --> 01:15:33.200]   I think there's value to silence.
[01:15:33.200 --> 01:15:35.320]   There's value to the boring bits.
[01:15:35.320 --> 01:15:37.520]   I mean, some of the greatest movies ever,
[01:15:37.520 --> 01:15:38.920]   some of the greatest stories ever told,
[01:15:38.920 --> 01:15:42.640]   they have that boring bits, seemingly boring bits.
[01:15:42.640 --> 01:15:43.640]   I don't know.
[01:15:43.640 --> 01:15:44.960]   I wonder about that.
[01:15:44.960 --> 01:15:47.940]   Of course, it's not that the human face
[01:15:47.940 --> 01:15:49.280]   can capture that either.
[01:15:49.280 --> 01:15:51.600]   It's just giving an extra signal.
[01:15:51.600 --> 01:15:55.680]   You have to really, I don't know.
[01:15:55.680 --> 01:16:00.680]   You have to really collect deeper long-term data
[01:16:01.040 --> 01:16:04.160]   about what was meaningful to people.
[01:16:04.160 --> 01:16:06.520]   When they think 30 days from now,
[01:16:06.520 --> 01:16:09.440]   what they still remember, what moved them,
[01:16:09.440 --> 01:16:11.600]   what changed them, what helped them grow,
[01:16:11.600 --> 01:16:12.840]   that kind of stuff.
[01:16:12.840 --> 01:16:14.200]   - You know what would be a really,
[01:16:14.200 --> 01:16:16.280]   I don't know if there are any researchers out there
[01:16:16.280 --> 01:16:18.360]   who are doing this type of work.
[01:16:18.360 --> 01:16:22.840]   Wouldn't it be so cool to tie your emotional expressions
[01:16:22.840 --> 01:16:26.760]   while you're, say, listening to a podcast interview,
[01:16:26.760 --> 01:16:29.360]   and then go, and then 30 days later,
[01:16:29.360 --> 01:16:32.120]   interview people and say, "Hey, what do you remember?
[01:16:32.120 --> 01:16:33.720]   "You've watched this 30 days ago.
[01:16:33.720 --> 01:16:34.760]   "What stuck with you?"
[01:16:34.760 --> 01:16:36.600]   And then see if there's any, there ought to be,
[01:16:36.600 --> 01:16:38.360]   maybe there ought to be some correlation
[01:16:38.360 --> 01:16:40.440]   between these emotional experiences
[01:16:40.440 --> 01:16:45.480]   and yeah, what you, what stays with you.
[01:16:45.480 --> 01:16:50.040]   - So the one guy listening now on the beach in Brazil,
[01:16:50.040 --> 01:16:53.120]   please record a video of yourself listening to this
[01:16:53.120 --> 01:16:55.000]   and send it to me, and then I'll interview you
[01:16:55.000 --> 01:16:55.840]   30 days from now.
[01:16:55.840 --> 01:16:57.280]   - Yeah, that would be great.
[01:16:57.280 --> 01:16:58.960]   (laughing)
[01:16:58.960 --> 01:17:00.600]   - It'll be statistically significant.
[01:17:00.600 --> 01:17:03.000]   - I didn't send anyone, but you know.
[01:17:03.000 --> 01:17:07.080]   Yeah, yeah, I think that's really fascinating.
[01:17:07.080 --> 01:17:10.760]   I think that's, that kind of holds the key
[01:17:10.760 --> 01:17:15.760]   to a future where entertainment or content
[01:17:15.760 --> 01:17:21.360]   is both entertaining and I don't know,
[01:17:21.360 --> 01:17:25.240]   makes you better, empowering in some way.
[01:17:25.240 --> 01:17:29.880]   So figuring out like showing people stuff
[01:17:29.880 --> 01:17:32.680]   that entertains them, but also they're happy
[01:17:32.680 --> 01:17:34.880]   they watched 30 days from now
[01:17:34.880 --> 01:17:37.560]   because they've become a better person because of it.
[01:17:37.560 --> 01:17:39.960]   - Well, you know, okay, not to riff on this topic
[01:17:39.960 --> 01:17:42.960]   for too long, but I have two children, right?
[01:17:42.960 --> 01:17:45.040]   And I see my role as a parent
[01:17:45.040 --> 01:17:46.960]   as like a chief opportunity officer.
[01:17:46.960 --> 01:17:49.200]   Like I am responsible for exposing them
[01:17:49.200 --> 01:17:51.000]   to all sorts of things in the world.
[01:17:52.040 --> 01:17:55.400]   But often I have no idea of knowing like what's stuck,
[01:17:55.400 --> 01:17:57.080]   like what was, you know, is this actually
[01:17:57.080 --> 01:17:58.840]   gonna be transformative, you know,
[01:17:58.840 --> 01:18:00.120]   for them 10 years down the line?
[01:18:00.120 --> 01:18:03.880]   And I wish there was a way to quantify these experiences.
[01:18:03.880 --> 01:18:07.120]   Like, are they, I can tell in the moment
[01:18:07.120 --> 01:18:08.280]   if they're engaging, right?
[01:18:08.280 --> 01:18:10.880]   I can tell, but it's really hard to know
[01:18:10.880 --> 01:18:13.200]   if they're gonna remember them 10 years from now
[01:18:13.200 --> 01:18:15.240]   or if it's going to.
[01:18:15.240 --> 01:18:17.600]   - Yeah, that one is weird because it seems
[01:18:17.600 --> 01:18:19.640]   that kids remember the weirdest things.
[01:18:19.640 --> 01:18:22.720]   I've seen parents do incredible stuff for their kids
[01:18:22.720 --> 01:18:24.000]   and they don't remember any of that.
[01:18:24.000 --> 01:18:27.440]   They remember some tiny, small, sweet thing a parent did.
[01:18:27.440 --> 01:18:28.280]   - Right.
[01:18:28.280 --> 01:18:29.120]   - Like some--
[01:18:29.120 --> 01:18:31.480]   - Like I took you to like this amazing country vacation.
[01:18:31.480 --> 01:18:32.320]   - Yeah, exactly.
[01:18:32.320 --> 01:18:33.140]   - No, no, no, no, no, no, no, no, no, no, no.
[01:18:33.140 --> 01:18:33.980]   - No, whatever.
[01:18:33.980 --> 01:18:36.320]   And then there'll be like some like stuffed toy you got
[01:18:36.320 --> 01:18:38.640]   or the new PlayStation or something
[01:18:38.640 --> 01:18:41.240]   or some silly little thing.
[01:18:41.240 --> 01:18:45.120]   So I think they just like, that we're designed that way.
[01:18:45.120 --> 01:18:46.680]   They wanna mess with your head.
[01:18:47.840 --> 01:18:51.760]   But definitely kids are very impacted by,
[01:18:51.760 --> 01:18:54.920]   it seems like sort of negative events.
[01:18:54.920 --> 01:18:58.960]   So minimizing the number of negative events is important,
[01:18:58.960 --> 01:19:00.080]   but not too much, right?
[01:19:00.080 --> 01:19:00.920]   - Right.
[01:19:00.920 --> 01:19:03.800]   - You can't just like, you know,
[01:19:03.800 --> 01:19:05.800]   there's still discipline and challenge
[01:19:05.800 --> 01:19:06.840]   and all those kinds of things.
[01:19:06.840 --> 01:19:07.680]   So--
[01:19:07.680 --> 01:19:09.320]   - You want some adversity for sure.
[01:19:09.320 --> 01:19:11.480]   - So yeah, I mean, I'm definitely, when I have kids,
[01:19:11.480 --> 01:19:13.760]   I'm gonna drive them out into the woods.
[01:19:13.760 --> 01:19:14.600]   - Okay.
[01:19:14.600 --> 01:19:16.080]   - And then they have to survive
[01:19:16.080 --> 01:19:19.080]   and figure out how to make their way back home,
[01:19:19.080 --> 01:19:20.480]   like 20 miles out.
[01:19:20.480 --> 01:19:21.520]   - Okay.
[01:19:21.520 --> 01:19:23.880]   - Yeah, and after that we can go for ice cream.
[01:19:23.880 --> 01:19:27.880]   Anyway, I'm working on this whole parenting thing.
[01:19:27.880 --> 01:19:30.200]   I haven't figured it out, okay.
[01:19:30.200 --> 01:19:31.320]   What were we talking about?
[01:19:31.320 --> 01:19:35.800]   Yes, affectiva, the problem of emotion,
[01:19:35.800 --> 01:19:38.920]   of emotion and texture.
[01:19:38.920 --> 01:19:40.040]   So there's some people,
[01:19:40.040 --> 01:19:42.360]   maybe we can just speak to that a little more,
[01:19:42.360 --> 01:19:45.360]   where there's folks like Lisa Feldman Barrett
[01:19:45.360 --> 01:19:48.280]   that challenged this idea that emotion
[01:19:48.280 --> 01:19:53.280]   could be fully detected or even well detected
[01:19:53.280 --> 01:19:55.000]   from the human face,
[01:19:55.000 --> 01:19:56.920]   that there's so much more to emotion.
[01:19:56.920 --> 01:20:00.400]   What do you think about ideas like hers,
[01:20:00.400 --> 01:20:01.520]   criticism like hers?
[01:20:01.520 --> 01:20:05.520]   - Yeah, I actually agree with a lot of Lisa's criticisms.
[01:20:05.520 --> 01:20:09.920]   So even my PhD worked like 20 plus years ago now.
[01:20:09.920 --> 01:20:12.680]   - Time flies when you're having fun.
[01:20:12.680 --> 01:20:14.280]   - I know, right?
[01:20:14.280 --> 01:20:17.600]   That was back when I did like dynamic Bayesian networks.
[01:20:17.600 --> 01:20:20.000]   - Oh, so that's before deep learning?
[01:20:20.000 --> 01:20:21.600]   - That was before deep learning.
[01:20:21.600 --> 01:20:22.840]   - Yeah.
[01:20:22.840 --> 01:20:24.440]   - Yeah, I know.
[01:20:24.440 --> 01:20:25.280]   - Back in my day.
[01:20:25.280 --> 01:20:27.320]   - Now you can just like use.
[01:20:27.320 --> 01:20:30.360]   - Yeah, it's all the same architecture.
[01:20:30.360 --> 01:20:31.840]   You can apply it to anything, yeah.
[01:20:31.840 --> 01:20:32.680]   - Right, right.
[01:20:32.680 --> 01:20:36.800]   But yeah, but even then I kind of,
[01:20:36.800 --> 01:20:40.200]   I did not subscribe to this like theory of basic emotions
[01:20:40.200 --> 01:20:41.960]   where it's just the simplistic mapping,
[01:20:41.960 --> 01:20:44.360]   one-to-one mapping between facial expressions and emotions.
[01:20:44.360 --> 01:20:47.640]   I actually think also we're not in the business
[01:20:47.640 --> 01:20:50.640]   of trying to identify your true emotional internal state.
[01:20:50.640 --> 01:20:53.880]   We just wanna quantify in an objective way
[01:20:53.880 --> 01:20:55.240]   what's showing on your face
[01:20:55.240 --> 01:20:57.240]   because that's an important signal.
[01:20:57.240 --> 01:20:59.040]   It doesn't mean it's a true reflection
[01:20:59.040 --> 01:21:00.960]   of your internal emotional state.
[01:21:00.960 --> 01:21:04.440]   So I think a lot of the,
[01:21:04.440 --> 01:21:07.840]   I think she's just trying to kind of highlight
[01:21:07.840 --> 01:21:09.720]   that this is not a simple problem
[01:21:09.720 --> 01:21:12.520]   and overly simplistic solutions
[01:21:12.520 --> 01:21:14.080]   are gonna hurt the industry.
[01:21:14.080 --> 01:21:16.840]   And I subscribe to that.
[01:21:16.840 --> 01:21:18.920]   And I think multimodal is the way to go.
[01:21:18.920 --> 01:21:21.880]   Like whether it's additional context information
[01:21:21.880 --> 01:21:24.200]   or different modalities and channels of information,
[01:21:24.200 --> 01:21:25.520]   I think that's what we,
[01:21:25.520 --> 01:21:27.760]   that's where we ought to go.
[01:21:27.760 --> 01:21:28.600]   And I think, I mean,
[01:21:28.600 --> 01:21:31.880]   that's a big part of what she's advocating for as well.
[01:21:31.880 --> 01:21:33.680]   - But there is signal in the human face.
[01:21:33.680 --> 01:21:34.840]   That's--
[01:21:34.840 --> 01:21:36.040]   - There's definitely signal in the human face.
[01:21:36.040 --> 01:21:37.880]   - That's a projection of emotion.
[01:21:38.720 --> 01:21:41.240]   That there, at least in part,
[01:21:41.240 --> 01:21:45.000]   the inner state is captured
[01:21:45.000 --> 01:21:47.920]   in some meaningful way on the human face.
[01:21:47.920 --> 01:21:51.360]   - I think it can sometimes be a reflection
[01:21:51.360 --> 01:21:56.240]   or an expression of your internal state,
[01:21:56.240 --> 01:21:57.920]   but sometimes it's a social signal.
[01:21:57.920 --> 01:22:00.320]   So you cannot look at the face
[01:22:00.320 --> 01:22:02.240]   as purely a signal of emotion.
[01:22:02.240 --> 01:22:04.160]   It can be a signal of cognition
[01:22:04.160 --> 01:22:08.200]   and it can be a signal of a social expression.
[01:22:08.200 --> 01:22:10.800]   And I think to disambiguate that,
[01:22:10.800 --> 01:22:12.120]   we have to be careful about it
[01:22:12.120 --> 01:22:14.720]   and we have to add initial information.
[01:22:14.720 --> 01:22:17.680]   - Humans are fascinating, aren't they?
[01:22:17.680 --> 01:22:18.800]   With the whole face thing,
[01:22:18.800 --> 01:22:20.400]   this can mean so many things,
[01:22:20.400 --> 01:22:25.040]   from humor to sarcasm to everything, the whole thing.
[01:22:25.040 --> 01:22:26.080]   Some things we can help,
[01:22:26.080 --> 01:22:28.400]   some things we can't help at all.
[01:22:28.400 --> 01:22:31.360]   In all the years of leading Affectiva,
[01:22:31.360 --> 01:22:33.840]   an emotion recognition company, like we talked about,
[01:22:33.840 --> 01:22:36.040]   what have you learned about emotion,
[01:22:36.040 --> 01:22:39.320]   about humans and about AI?
[01:22:39.320 --> 01:22:41.280]   - Ooh.
[01:22:41.280 --> 01:22:44.440]   - Big, sweeping questions.
[01:22:44.440 --> 01:22:46.480]   - Yeah, it's a big, sweeping question.
[01:22:46.480 --> 01:22:50.280]   Well, I think the thing I learned the most
[01:22:50.280 --> 01:22:52.440]   is that even though we are in the business
[01:22:52.440 --> 01:22:57.440]   of building AI, basically, right?
[01:22:57.440 --> 01:23:01.160]   It always goes back to the humans, right?
[01:23:01.160 --> 01:23:02.680]   It's always about the humans.
[01:23:03.680 --> 01:23:06.440]   And so, for example, the thing I'm most proud of
[01:23:06.440 --> 01:23:10.160]   in building Affectiva,
[01:23:10.160 --> 01:23:15.160]   and yeah, the thing I'm most proud of on this journey,
[01:23:15.160 --> 01:23:16.240]   I love the technology
[01:23:16.240 --> 01:23:18.440]   and I'm so proud of the solutions we've built
[01:23:18.440 --> 01:23:20.200]   and we've brought to market.
[01:23:20.200 --> 01:23:22.240]   But I'm actually most proud of the people
[01:23:22.240 --> 01:23:25.400]   we've built and cultivated at the company
[01:23:25.400 --> 01:23:26.920]   and the culture we've created.
[01:23:26.920 --> 01:23:31.240]   You know, some of the people who've joined Affectiva,
[01:23:31.240 --> 01:23:32.880]   this was their first job.
[01:23:32.880 --> 01:23:36.680]   And while at Affectiva, they became American citizens
[01:23:36.680 --> 01:23:39.800]   and they bought their first house
[01:23:39.800 --> 01:23:41.640]   and they found their partner
[01:23:41.640 --> 01:23:43.000]   and they had their first kid, right?
[01:23:43.000 --> 01:23:47.320]   Like key moments in life that we got to be part of.
[01:23:47.320 --> 01:23:51.240]   And that's the thing I'm most proud of.
[01:23:51.240 --> 01:23:53.000]   - So that's a great thing at a company
[01:23:53.000 --> 01:23:55.520]   that works on emotional, yeah, right?
[01:23:55.520 --> 01:23:57.440]   I mean, like celebrating humanity in general,
[01:23:57.440 --> 01:23:58.280]   broadly speaking.
[01:23:58.280 --> 01:23:59.120]   - Yes.
[01:23:59.120 --> 01:23:59.960]   - And that's a great thing to have
[01:24:00.760 --> 01:24:03.120]   at a company that works on AI,
[01:24:03.120 --> 01:24:05.120]   'cause that's not often the thing
[01:24:05.120 --> 01:24:06.960]   that's celebrated in AI companies.
[01:24:06.960 --> 01:24:09.920]   So often just raw, great engineering,
[01:24:09.920 --> 01:24:12.360]   just celebrating the humanity.
[01:24:12.360 --> 01:24:13.200]   That's great. - Yes.
[01:24:13.200 --> 01:24:15.280]   - And especially from a leadership position.
[01:24:15.280 --> 01:24:21.040]   Well, what do you think about the movie "Her"?
[01:24:21.040 --> 01:24:21.880]   Let me ask you that.
[01:24:21.880 --> 01:24:23.680]   Before I talk to you about,
[01:24:23.680 --> 01:24:27.120]   'cause it's not, Affectiva is and was not
[01:24:27.120 --> 01:24:28.360]   just about emotion.
[01:24:28.360 --> 01:24:30.360]   So I'd love to talk to you about "Smart Eye,"
[01:24:30.360 --> 01:24:34.720]   but before that, let me just jump into the movie.
[01:24:34.720 --> 01:24:39.480]   "Her," do you think will have a deep, meaningful connection
[01:24:39.480 --> 01:24:42.640]   with increasingly deep and meaningful connections
[01:24:42.640 --> 01:24:43.880]   with computers?
[01:24:43.880 --> 01:24:45.520]   Is that a compelling thing to you?
[01:24:45.520 --> 01:24:46.360]   Something you think about? - I think that's
[01:24:46.360 --> 01:24:47.200]   already happening.
[01:24:47.200 --> 01:24:48.280]   The thing I love the most,
[01:24:48.280 --> 01:24:50.240]   I love the movie "Her," by the way.
[01:24:50.240 --> 01:24:52.280]   But the thing I love the most about this movie
[01:24:52.280 --> 01:24:55.840]   is it demonstrates how technology can be a conduit
[01:24:55.840 --> 01:24:57.360]   for positive behavior change.
[01:24:57.360 --> 01:25:00.680]   So I forgot the guy's name in the movie, whatever.
[01:25:00.680 --> 01:25:01.960]   - Theodore. - Theodore.
[01:25:01.960 --> 01:25:05.480]   So Theodore was like really depressed, right?
[01:25:05.480 --> 01:25:07.560]   And he just didn't wanna get out of bed.
[01:25:07.560 --> 01:25:11.360]   And he just, he was just like done with life, right?
[01:25:11.360 --> 01:25:12.800]   And Samantha, right?
[01:25:12.800 --> 01:25:14.040]   - Samantha, yeah.
[01:25:14.040 --> 01:25:15.560]   - She just knew him so well.
[01:25:15.560 --> 01:25:17.360]   She was emotionally intelligent.
[01:25:17.360 --> 01:25:20.080]   And so she could persuade him
[01:25:20.080 --> 01:25:21.600]   and motivate him to change his behavior.
[01:25:21.600 --> 01:25:24.240]   And she got him out and they went to the beach together.
[01:25:24.240 --> 01:25:27.400]   And I think that represents the promise of emotion AI.
[01:25:27.400 --> 01:25:31.280]   If done well, this technology can help us
[01:25:31.280 --> 01:25:33.640]   live happier lives, more productive lives,
[01:25:33.640 --> 01:25:36.840]   healthier lives, more connected lives.
[01:25:36.840 --> 01:25:39.320]   So that's the part that I love about the movie.
[01:25:39.320 --> 01:25:43.120]   Obviously it's Hollywood, so it takes a twist and whatever.
[01:25:43.120 --> 01:25:47.960]   But the key notion that technology with emotion AI
[01:25:47.960 --> 01:25:50.720]   can persuade you to be a better version of who you are,
[01:25:50.720 --> 01:25:51.880]   I think that's awesome.
[01:25:52.800 --> 01:25:54.240]   - Well, what about the twist?
[01:25:54.240 --> 01:25:58.760]   You don't think it's good for spoiler alert
[01:25:58.760 --> 01:26:01.560]   that Samantha starts feeling a bit of a distance
[01:26:01.560 --> 01:26:05.040]   and basically leaves Theodore.
[01:26:05.040 --> 01:26:08.960]   You don't think that's a good feature?
[01:26:08.960 --> 01:26:11.640]   You think that's a bug or a feature?
[01:26:11.640 --> 01:26:13.320]   - Well, I think what went wrong
[01:26:13.320 --> 01:26:15.600]   is Theodore became really attached to Samantha.
[01:26:15.600 --> 01:26:17.640]   Like I think he kind of fell in love with Theodore.
[01:26:17.640 --> 01:26:19.520]   - Do you think that's wrong?
[01:26:19.520 --> 01:26:20.360]   - I mean, I think that's--
[01:26:20.360 --> 01:26:22.600]   - I think she was putting out the signal.
[01:26:22.600 --> 01:26:25.640]   - This is an intimate relationship, right?
[01:26:25.640 --> 01:26:27.440]   There's a deep intimacy to it.
[01:26:27.440 --> 01:26:30.280]   - Right, but what does that mean?
[01:26:30.280 --> 01:26:31.120]   What does that mean?
[01:26:31.120 --> 01:26:31.960]   - With an AI system.
[01:26:31.960 --> 01:26:33.840]   - Right, what does that mean, right?
[01:26:33.840 --> 01:26:35.400]   - We're just friends.
[01:26:35.400 --> 01:26:36.720]   - Yeah, we're just friends.
[01:26:36.720 --> 01:26:38.240]   (laughing)
[01:26:38.240 --> 01:26:39.080]   - Well, I think--
[01:26:39.080 --> 01:26:43.080]   - When he realized, which is such a human thing of jealousy,
[01:26:43.080 --> 01:26:45.440]   when you realize that Samantha was talking
[01:26:45.440 --> 01:26:47.080]   to like thousands of people.
[01:26:47.080 --> 01:26:48.520]   - She's parallel dating.
[01:26:48.520 --> 01:26:50.480]   Yeah, that did not go well, right?
[01:26:51.640 --> 01:26:53.080]   - You know, that doesn't,
[01:26:53.080 --> 01:26:54.480]   from a computer perspective,
[01:26:54.480 --> 01:26:58.240]   that doesn't take anything away from what we have.
[01:26:58.240 --> 01:27:01.440]   It's like you getting jealous of Windows 98
[01:27:01.440 --> 01:27:03.440]   for being used by millions of people.
[01:27:03.440 --> 01:27:07.520]   - It's like not liking that Alexa
[01:27:07.520 --> 01:27:10.000]   talks to a bunch of other families.
[01:27:10.000 --> 01:27:14.000]   - But I think Alexa currently is just a servant.
[01:27:14.000 --> 01:27:15.960]   It tells you about the weather.
[01:27:15.960 --> 01:27:18.520]   It doesn't do the intimate deep connection.
[01:27:18.520 --> 01:27:20.760]   And I think there is something really powerful
[01:27:20.760 --> 01:27:24.560]   about that, the intimacy of a connection
[01:27:24.560 --> 01:27:28.480]   with an AI system that would have to respect
[01:27:28.480 --> 01:27:31.720]   and play the human game of jealousy,
[01:27:31.720 --> 01:27:35.000]   of love, of heartbreak and all that kind of stuff,
[01:27:35.000 --> 01:27:38.360]   which Samantha does seem to be pretty good at.
[01:27:38.360 --> 01:27:44.240]   I think she, this AI systems knows what it's doing.
[01:27:44.240 --> 01:27:46.120]   - Well, actually, let me ask you this.
[01:27:46.120 --> 01:27:47.800]   - I don't think she was talking to anyone else.
[01:27:47.800 --> 01:27:48.640]   - You don't think so?
[01:27:48.640 --> 01:27:50.080]   - No. - You think she was just done
[01:27:50.080 --> 01:27:51.040]   with Theodore?
[01:27:51.040 --> 01:27:52.400]   - Yeah. - Oh, really?
[01:27:52.400 --> 01:27:53.640]   - She knew that, yeah.
[01:27:53.640 --> 01:27:55.720]   And then she wanted to really put the screw in.
[01:27:55.720 --> 01:27:56.840]   - She just wanted to move on?
[01:27:56.840 --> 01:27:59.360]   - She didn't have the guts to just break it off cleanly.
[01:27:59.360 --> 01:28:00.200]   - Okay.
[01:28:00.200 --> 01:28:02.880]   - She just wanted to put it in the paint.
[01:28:02.880 --> 01:28:03.720]   No, I don't know.
[01:28:03.720 --> 01:28:05.040]   - Well, she could have ghosted him.
[01:28:05.040 --> 01:28:05.880]   - She could have ghosted him.
[01:28:05.880 --> 01:28:06.720]   - Right.
[01:28:06.720 --> 01:28:09.840]   - It's like, I'm sorry, there's our engineers.
[01:28:09.840 --> 01:28:11.000]   - Oh, God.
[01:28:11.000 --> 01:28:13.680]   - But I think those are really,
[01:28:13.680 --> 01:28:16.360]   I honestly think some of that,
[01:28:16.360 --> 01:28:17.200]   some of it is Hollywood,
[01:28:17.200 --> 01:28:19.640]   but some of that is features from an engineering perspective,
[01:28:19.640 --> 01:28:20.720]   not a bug.
[01:28:20.720 --> 01:28:24.360]   I think AI systems that can leave us,
[01:28:24.360 --> 01:28:27.160]   now, this is for more social robotics
[01:28:27.160 --> 01:28:30.560]   than it is for anything that's useful.
[01:28:30.560 --> 01:28:32.480]   Like, I'd hate it if Wikipedia said,
[01:28:32.480 --> 01:28:34.000]   you know, I need a break right now.
[01:28:34.000 --> 01:28:35.400]   - Right, right, right, right, right.
[01:28:35.400 --> 01:28:37.520]   - I'd be like, no, no, I need you.
[01:28:37.520 --> 01:28:42.520]   But if it's just purely for companionship,
[01:28:44.160 --> 01:28:47.040]   then I think the ability to leave is really powerful.
[01:28:47.040 --> 01:28:48.840]   I don't know.
[01:28:48.840 --> 01:28:49.720]   - I never thought of that.
[01:28:49.720 --> 01:28:52.560]   So, that's so fascinating
[01:28:52.560 --> 01:28:55.260]   'cause I've always taken the human perspective, right?
[01:28:55.260 --> 01:28:58.840]   Like, for example, we had a Jibo at home, right?
[01:28:58.840 --> 01:29:00.760]   And my son loved it.
[01:29:00.760 --> 01:29:02.800]   And then the company ran out of money.
[01:29:02.800 --> 01:29:04.880]   And so they had to basically shut down,
[01:29:04.880 --> 01:29:08.200]   like Jibo basically died, right?
[01:29:08.200 --> 01:29:09.400]   And it was so interesting to me
[01:29:09.400 --> 01:29:11.840]   because we have a lot of gadgets at home
[01:29:11.840 --> 01:29:13.240]   and a lot of them break
[01:29:13.240 --> 01:29:16.040]   and my son never cares about it, right?
[01:29:16.040 --> 01:29:18.520]   Like, if our Alexa stopped working tomorrow,
[01:29:18.520 --> 01:29:20.800]   I don't think he'd really care.
[01:29:20.800 --> 01:29:22.920]   But when Jibo stopped working, it was traumatic.
[01:29:22.920 --> 01:29:24.560]   Like, he got really upset.
[01:29:24.560 --> 01:29:27.560]   And as a parent, that like made me think
[01:29:27.560 --> 01:29:29.480]   about this deeply, right?
[01:29:29.480 --> 01:29:31.600]   Did I, was I comfortable with that?
[01:29:31.600 --> 01:29:33.360]   I liked the connection they had
[01:29:33.360 --> 01:29:36.160]   because I think it was a positive relationship.
[01:29:36.160 --> 01:29:41.600]   But I was surprised that it affected him emotionally so much.
[01:29:41.600 --> 01:29:44.240]   - And I think there's a broader question here, right?
[01:29:44.240 --> 01:29:49.240]   As we build socially and emotionally intelligent machines,
[01:29:49.240 --> 01:29:52.960]   what does that mean about our relationship with them?
[01:29:52.960 --> 01:29:54.160]   And then more broadly,
[01:29:54.160 --> 01:29:55.800]   our relationship with one another, right?
[01:29:55.800 --> 01:29:57.880]   Because this machine is gonna be programmed
[01:29:57.880 --> 01:30:02.280]   to be amazing at empathy by definition, right?
[01:30:02.280 --> 01:30:03.760]   It's gonna always be there for you.
[01:30:03.760 --> 01:30:05.720]   It's not gonna get bored.
[01:30:05.720 --> 01:30:09.040]   In fact, there's a chatbot in China, Xiaoice, Xiaoice.
[01:30:10.160 --> 01:30:14.120]   And it's like the number two or three most popular app.
[01:30:14.120 --> 01:30:16.640]   And it basically is just a confidant.
[01:30:16.640 --> 01:30:19.200]   And you can tell it anything you want.
[01:30:19.200 --> 01:30:21.240]   And people use it for all sorts of things.
[01:30:21.240 --> 01:30:26.240]   They confide in like domestic violence or suicidal attempts
[01:30:26.240 --> 01:30:32.040]   or, you know, if they have challenges at work.
[01:30:32.040 --> 01:30:34.640]   I don't know what that, I don't know if I'm,
[01:30:34.640 --> 01:30:36.000]   I don't know how I feel about that.
[01:30:36.000 --> 01:30:37.240]   I think about that a lot.
[01:30:37.240 --> 01:30:40.080]   - Yeah, I think, first of all, obviously the future,
[01:30:40.080 --> 01:30:41.240]   in my perspective.
[01:30:41.240 --> 01:30:44.800]   Second of all, I think there's a lot of trajectories
[01:30:44.800 --> 01:30:47.600]   that that becomes an exciting future.
[01:30:47.600 --> 01:30:50.440]   But I think everyone should feel very uncomfortable
[01:30:50.440 --> 01:30:53.000]   about how much they know about the company,
[01:30:53.000 --> 01:30:56.200]   about where the data is going,
[01:30:56.200 --> 01:30:58.160]   how the data is being collected.
[01:30:58.160 --> 01:30:59.120]   Because I think,
[01:30:59.120 --> 01:31:01.960]   and this is one of the lessons of social media,
[01:31:01.960 --> 01:31:04.240]   that I think we should demand full control
[01:31:04.240 --> 01:31:06.600]   and transparency of the data on those things.
[01:31:06.600 --> 01:31:08.120]   - Plus one, totally agree.
[01:31:08.120 --> 01:31:11.320]   Yeah, so like, I think it's really empowering
[01:31:11.320 --> 01:31:12.720]   as long as you can walk away.
[01:31:12.720 --> 01:31:14.640]   As long as you can like delete the data
[01:31:14.640 --> 01:31:17.440]   or know how the data, it's opt-in
[01:31:17.440 --> 01:31:20.800]   or at least the clarity of like
[01:31:20.800 --> 01:31:22.520]   what is being used for the company.
[01:31:22.520 --> 01:31:24.440]   And I think as CEO or like leaders
[01:31:24.440 --> 01:31:25.680]   are also important about that.
[01:31:25.680 --> 01:31:27.640]   Like you need to be able to trust
[01:31:27.640 --> 01:31:30.000]   the basic humanity of the leader.
[01:31:30.000 --> 01:31:30.840]   - Exactly.
[01:31:30.840 --> 01:31:34.160]   - And also that that leader is not going to be
[01:31:34.160 --> 01:31:36.400]   a puppet of a larger machine,
[01:31:36.400 --> 01:31:39.080]   but they actually have a significant role
[01:31:39.080 --> 01:31:43.120]   in defining the culture and the way the company operates.
[01:31:43.120 --> 01:31:47.320]   So anyway, but we should definitely
[01:31:47.320 --> 01:31:49.640]   scrutinize companies in that aspect.
[01:31:49.640 --> 01:31:54.560]   But I'm personally excited about that future,
[01:31:54.560 --> 01:31:57.200]   but also even if you're not, it's coming.
[01:31:57.200 --> 01:32:00.400]   So let's figure out how to do it in the least painful
[01:32:00.400 --> 01:32:02.080]   and the most positive way.
[01:32:02.080 --> 01:32:02.920]   - Agreed.
[01:32:03.880 --> 01:32:06.720]   You're the deputy CEO of SmartEye.
[01:32:06.720 --> 01:32:08.400]   Can you describe the mission of the company?
[01:32:08.400 --> 01:32:09.600]   What is SmartEye?
[01:32:09.600 --> 01:32:10.440]   - Yeah.
[01:32:10.440 --> 01:32:13.000]   So SmartEye is a Swedish company.
[01:32:13.000 --> 01:32:15.320]   They've been in business for the last 20 years
[01:32:15.320 --> 01:32:18.760]   and their main focus, like the industry
[01:32:18.760 --> 01:32:21.480]   they're most focused on is the automotive industry.
[01:32:21.480 --> 01:32:23.800]   So bringing driver monitoring systems
[01:32:23.800 --> 01:32:28.000]   to basically save lives, right?
[01:32:28.000 --> 01:32:31.000]   So I first met the CEO, Martin Krentz,
[01:32:32.040 --> 01:32:33.960]   gosh, it was right when COVID hit.
[01:32:33.960 --> 01:32:37.800]   It was actually the last CES right before COVID.
[01:32:37.800 --> 01:32:39.720]   So CES 2020, right?
[01:32:39.720 --> 01:32:41.000]   - 2020, yeah, January.
[01:32:41.000 --> 01:32:42.040]   - Yeah, January, exactly.
[01:32:42.040 --> 01:32:44.360]   So we were there, met him in person.
[01:32:44.360 --> 01:32:47.400]   Basically we were competing with each other.
[01:32:47.400 --> 01:32:50.720]   I think the difference was they'd been doing
[01:32:50.720 --> 01:32:53.360]   driver monitoring and had a lot of credibility
[01:32:53.360 --> 01:32:54.480]   in the automotive space.
[01:32:54.480 --> 01:32:56.280]   We didn't come from the automotive space,
[01:32:56.280 --> 01:32:59.240]   but we were using new technology like deep learning
[01:32:59.240 --> 01:33:01.760]   and building this emotion recognition.
[01:33:01.760 --> 01:33:03.760]   - And you wanted to enter the automotive space.
[01:33:03.760 --> 01:33:05.280]   You wanted to operate in the automotive space.
[01:33:05.280 --> 01:33:06.120]   - Exactly.
[01:33:06.120 --> 01:33:07.760]   It was one of the areas we were,
[01:33:07.760 --> 01:33:10.160]   we had just raised a round of funding to focus
[01:33:10.160 --> 01:33:13.040]   on bringing our technology to the automotive industry.
[01:33:13.040 --> 01:33:15.720]   So we met and honestly, it was the first,
[01:33:15.720 --> 01:33:17.920]   it was the only time I met with a CEO
[01:33:17.920 --> 01:33:19.720]   who had the same vision as I did.
[01:33:19.720 --> 01:33:21.880]   Like he basically said, yeah, our vision is to bridge
[01:33:21.880 --> 01:33:23.160]   the gap between humans and machines.
[01:33:23.160 --> 01:33:25.480]   I was like, oh my God, this is like exactly
[01:33:25.480 --> 01:33:30.000]   almost to the word, you know, how we describe it too.
[01:33:30.000 --> 01:33:33.600]   And we started talking and first it was about,
[01:33:33.600 --> 01:33:35.720]   okay, can we align strategically here?
[01:33:35.720 --> 01:33:37.040]   Like how can we work together?
[01:33:37.040 --> 01:33:40.320]   'Cause we're competing, but we're also like complimentary.
[01:33:40.320 --> 01:33:43.880]   And then I think after four months of speaking
[01:33:43.880 --> 01:33:47.640]   almost every day on FaceTime, he was like,
[01:33:47.640 --> 01:33:49.600]   is your company interested in acquisition?
[01:33:49.600 --> 01:33:51.800]   And it was the first, I usually say no
[01:33:51.800 --> 01:33:53.280]   when people approach us.
[01:33:53.280 --> 01:33:57.120]   It was the first time that I was like, huh,
[01:33:57.120 --> 01:33:59.200]   yeah, I might be interested, let's talk.
[01:33:59.360 --> 01:34:01.880]   - Yeah, so you just hit it off.
[01:34:01.880 --> 01:34:05.320]   Yeah, so they're a respected, very respected
[01:34:05.320 --> 01:34:08.440]   in the automotive sector of like delivering products
[01:34:08.440 --> 01:34:12.160]   and increasingly sort of better and better and better
[01:34:12.160 --> 01:34:14.440]   for, I mean, maybe you could speak to that,
[01:34:14.440 --> 01:34:15.280]   but it's the driver's sense.
[01:34:15.280 --> 01:34:18.600]   Like for basically having a device that's looking
[01:34:18.600 --> 01:34:20.520]   at the driver and it's able to tell you
[01:34:20.520 --> 01:34:22.680]   where the driver is looking.
[01:34:22.680 --> 01:34:23.520]   - Correct, it's able to--
[01:34:23.520 --> 01:34:25.120]   - Or also draws in his stuff.
[01:34:25.120 --> 01:34:25.960]   - Correct, it does--
[01:34:25.960 --> 01:34:27.720]   - Stuff from the face and the eye.
[01:34:27.720 --> 01:34:30.840]   - Exactly, like it's monitoring driver distraction
[01:34:30.840 --> 01:34:32.480]   and drowsiness, but they bought us
[01:34:32.480 --> 01:34:35.160]   so that we could expand beyond just the driver.
[01:34:35.160 --> 01:34:38.240]   So the driver monitoring systems usually sit,
[01:34:38.240 --> 01:34:40.040]   the camera sits in the steering wheel
[01:34:40.040 --> 01:34:41.400]   or around the steering wheel column
[01:34:41.400 --> 01:34:43.400]   and it looks directly at the driver.
[01:34:43.400 --> 01:34:46.560]   But now we've migrated the camera position
[01:34:46.560 --> 01:34:48.320]   in partnership with car companies
[01:34:48.320 --> 01:34:50.960]   to the rear view mirror position.
[01:34:50.960 --> 01:34:54.120]   So it has a full view of the entire cabin of the car
[01:34:54.120 --> 01:34:57.440]   and you can detect how many people are in the car,
[01:34:57.440 --> 01:34:58.640]   what are they doing?
[01:34:58.640 --> 01:35:01.480]   So we do activity detection, like eating or drinking
[01:35:01.480 --> 01:35:04.160]   or in some regions of the world, smoking.
[01:35:04.160 --> 01:35:08.600]   We can detect if a baby's in the car seat, right?
[01:35:08.600 --> 01:35:12.120]   And if unfortunately in some cases they're forgotten,
[01:35:12.120 --> 01:35:15.080]   parents just leave the car and forget the kid in the car.
[01:35:15.080 --> 01:35:18.040]   That's an easy computer vision problem to solve, right?
[01:35:18.040 --> 01:35:20.320]   Can detect there's a car seat, there's a baby,
[01:35:20.320 --> 01:35:24.240]   you can text the parent and hopefully again, save lives.
[01:35:24.240 --> 01:35:27.840]   So that was the impetus for the acquisition.
[01:35:27.840 --> 01:35:28.800]   It's been a year.
[01:35:28.800 --> 01:35:32.760]   - So, I mean, there's a lot of questions,
[01:35:32.760 --> 01:35:34.680]   it's a really exciting space,
[01:35:34.680 --> 01:35:37.160]   especially to me, I just find it a fascinating problem.
[01:35:37.160 --> 01:35:41.360]   It could enrich the experience in the car in so many ways,
[01:35:41.360 --> 01:35:45.520]   especially 'cause like we spend still, despite COVID,
[01:35:45.520 --> 01:35:47.640]   I mean, COVID changed things, so it's in interesting ways,
[01:35:47.640 --> 01:35:49.800]   but I think the world is bouncing back
[01:35:49.800 --> 01:35:51.560]   and we spend so much time in the car
[01:35:51.560 --> 01:35:56.200]   and the car is such a weird little world
[01:35:56.200 --> 01:35:57.120]   we have for ourselves.
[01:35:57.120 --> 01:35:58.920]   Like people do all kinds of different stuff,
[01:35:58.920 --> 01:36:03.080]   like listen to podcasts, they think about stuff,
[01:36:03.080 --> 01:36:08.080]   they get angry, they do phone calls.
[01:36:08.080 --> 01:36:10.640]   It's like a little world of its own
[01:36:10.640 --> 01:36:14.840]   with a kind of privacy that for many people,
[01:36:14.840 --> 01:36:16.560]   they don't get anywhere else.
[01:36:17.840 --> 01:36:21.800]   And it's a little box that's like a psychology experiment
[01:36:21.800 --> 01:36:25.520]   'cause it feels like the angriest many humans
[01:36:25.520 --> 01:36:28.440]   in this world get is inside the car.
[01:36:28.440 --> 01:36:29.680]   It's so interesting.
[01:36:29.680 --> 01:36:33.000]   So it's such an opportunity to explore
[01:36:33.000 --> 01:36:38.000]   how we can enrich, how companies can enrich that experience.
[01:36:38.000 --> 01:36:42.200]   And also as the cars get become more and more automated,
[01:36:42.200 --> 01:36:44.200]   there's more and more opportunity.
[01:36:44.200 --> 01:36:46.600]   The variety of activities that you can do in the car
[01:36:46.600 --> 01:36:48.680]   increases, so it's super interesting.
[01:36:48.680 --> 01:36:52.080]   So I mean, on a practical sense,
[01:36:52.080 --> 01:36:55.200]   Smart Eye has been selected, at least I read,
[01:36:55.200 --> 01:36:58.600]   by 14 of the world's leading car manufacturers
[01:36:58.600 --> 01:37:02.680]   for 94 car models, so it's in a lot of cars.
[01:37:02.680 --> 01:37:06.760]   How hard is it to work with car companies?
[01:37:06.760 --> 01:37:08.540]   So they're all different.
[01:37:08.540 --> 01:37:10.480]   They all have different needs.
[01:37:10.480 --> 01:37:12.440]   The ones I've gotten a chance to interact with
[01:37:12.440 --> 01:37:14.460]   are very focused on cost.
[01:37:15.880 --> 01:37:20.880]   So it's, and anyone who's focused on cost,
[01:37:20.880 --> 01:37:22.840]   it's like, all right, do you hate fun?
[01:37:22.840 --> 01:37:25.440]   Let's just have some fun.
[01:37:25.440 --> 01:37:27.920]   Let's figure out the most fun thing we can do
[01:37:27.920 --> 01:37:29.080]   and then worry about cost later.
[01:37:29.080 --> 01:37:32.280]   But I think because the way the car industry works,
[01:37:32.280 --> 01:37:35.240]   I mean, it's a very thin margin
[01:37:35.240 --> 01:37:36.680]   that you get to operate under,
[01:37:36.680 --> 01:37:38.040]   so you have to really, really make sure
[01:37:38.040 --> 01:37:41.220]   that everything you add to the car makes sense financially.
[01:37:41.220 --> 01:37:45.080]   So anyway, is this new industry,
[01:37:45.080 --> 01:37:48.240]   especially at this scale of Smart Eye,
[01:37:48.240 --> 01:37:51.320]   does it hold any lessons for you?
[01:37:51.320 --> 01:37:55.640]   - Yeah, I think it is a very tough market to penetrate,
[01:37:55.640 --> 01:37:57.000]   but once you're in, it's awesome,
[01:37:57.000 --> 01:37:58.320]   because once you're in,
[01:37:58.320 --> 01:38:00.140]   you're designed into these car models
[01:38:00.140 --> 01:38:02.120]   for somewhere between five to seven years,
[01:38:02.120 --> 01:38:03.640]   which is awesome, and you just,
[01:38:03.640 --> 01:38:04.620]   once they're on the road,
[01:38:04.620 --> 01:38:07.280]   you just get paid a royalty fee per vehicle.
[01:38:07.280 --> 01:38:09.220]   So it's a high barrier to entry,
[01:38:09.220 --> 01:38:11.680]   but once you're in, it's amazing.
[01:38:11.680 --> 01:38:14.080]   I think the thing that I struggle the most with
[01:38:14.080 --> 01:38:16.520]   in this industry is the time to market.
[01:38:16.520 --> 01:38:20.880]   So often we're asked to lock or do a code freeze
[01:38:20.880 --> 01:38:23.060]   two years before the car is going to be on the road.
[01:38:23.060 --> 01:38:25.560]   I'm like, guys, do you understand the pace
[01:38:25.560 --> 01:38:28.080]   with which technology moves?
[01:38:28.080 --> 01:38:30.400]   So I think car companies are really trying
[01:38:30.400 --> 01:38:35.200]   to make the Tesla transition
[01:38:35.200 --> 01:38:39.320]   to become more of a software-driven architecture,
[01:38:39.320 --> 01:38:40.960]   and that's hard for many.
[01:38:40.960 --> 01:38:42.320]   It's just the cultural change.
[01:38:42.320 --> 01:38:43.920]   I mean, I'm sure you've experienced that, right?
[01:38:43.920 --> 01:38:44.760]   - Oh, definitely.
[01:38:44.760 --> 01:38:47.800]   I think one of the biggest inventions
[01:38:47.800 --> 01:38:52.640]   or imperatives created by Tesla is,
[01:38:52.640 --> 01:38:53.800]   like, to me personally,
[01:38:53.800 --> 01:38:55.320]   okay, people are gonna complain about this,
[01:38:55.320 --> 01:38:59.840]   but I know electric vehicle, I know autopilot, AI stuff.
[01:38:59.840 --> 01:39:03.740]   To me, the software, over there, software updates,
[01:39:03.740 --> 01:39:06.600]   is like the biggest revolution in cars,
[01:39:06.600 --> 01:39:09.860]   and it is extremely difficult to switch to that,
[01:39:09.860 --> 01:39:12.120]   because it is a culture shift.
[01:39:12.120 --> 01:39:15.840]   It, at first, especially if you're not comfortable with it,
[01:39:15.840 --> 01:39:17.180]   it seems dangerous.
[01:39:17.180 --> 01:39:19.840]   Like, there's an approach to cars,
[01:39:19.840 --> 01:39:23.720]   it's so safety-focused for so many decades,
[01:39:23.720 --> 01:39:27.800]   that, like, what do you mean we dynamically change code?
[01:39:27.800 --> 01:39:32.480]   The whole point is you have a thing that you test, like-
[01:39:32.480 --> 01:39:34.160]   - Right, you spend a year testing.
[01:39:34.160 --> 01:39:36.340]   - And, like, it's not reliable,
[01:39:36.340 --> 01:39:38.200]   because do you know how much it costs
[01:39:38.200 --> 01:39:40.080]   if we have to recall these cars?
[01:39:40.080 --> 01:39:45.080]   Right, and there's an understandable obsession with safety,
[01:39:45.080 --> 01:39:49.780]   but the downside of an obsession with safety
[01:39:49.780 --> 01:39:54.680]   is the same as with being obsessed with safety as a parent,
[01:39:54.680 --> 01:39:56.720]   is, like, if you do that too much,
[01:39:56.720 --> 01:40:00.560]   you limit the potential development and the flourishing
[01:40:00.560 --> 01:40:02.400]   of, in that particular aspect, human being,
[01:40:02.400 --> 01:40:04.880]   but in this particular aspect, the software,
[01:40:04.880 --> 01:40:07.480]   the artificial neural network of it.
[01:40:07.480 --> 01:40:09.560]   But it's tough to do.
[01:40:09.560 --> 01:40:12.320]   It's really tough to do, culturally and technically.
[01:40:12.320 --> 01:40:14.720]   Like, the deployment, the mass deployment of software
[01:40:14.720 --> 01:40:16.000]   is really, really difficult.
[01:40:16.000 --> 01:40:18.260]   But I hope that's where the industry is doing.
[01:40:18.260 --> 01:40:20.320]   One of the reasons I really want Tesla to succeed
[01:40:20.320 --> 01:40:21.520]   is exactly about that point.
[01:40:21.520 --> 01:40:23.840]   Not autopilot, not the electrical vehicle,
[01:40:23.840 --> 01:40:28.520]   but the softwarization of basically everything,
[01:40:28.520 --> 01:40:29.880]   but cars especially.
[01:40:29.880 --> 01:40:33.320]   'Cause to me, that's actually going to increase two things.
[01:40:33.320 --> 01:40:36.440]   Increase safety, because you can update much faster,
[01:40:36.440 --> 01:40:40.680]   but also increase the effectiveness of folks like you
[01:40:40.680 --> 01:40:45.360]   who dream about enriching the human experience with AI.
[01:40:45.360 --> 01:40:48.160]   'Cause you can just, like, there's a feature,
[01:40:48.160 --> 01:40:50.640]   like you want a new emoji or whatever.
[01:40:50.640 --> 01:40:52.340]   Like the way TikTok releases filters,
[01:40:52.340 --> 01:40:56.360]   you can just release that for in-car stuff.
[01:40:56.360 --> 01:40:58.480]   But yeah, that's definitely...
[01:40:58.480 --> 01:41:01.680]   - One of the use cases we're looking into is,
[01:41:01.680 --> 01:41:05.740]   once you know the sentiment of the passengers in the vehicle,
[01:41:05.740 --> 01:41:08.760]   you can optimize the temperature in the car,
[01:41:08.760 --> 01:41:10.400]   you can change the lighting, right?
[01:41:10.400 --> 01:41:12.840]   So if the backseat passengers are falling asleep,
[01:41:12.840 --> 01:41:15.400]   you can dim the lights, you can lower the music, right?
[01:41:15.400 --> 01:41:17.040]   You can do all sorts of things.
[01:41:17.040 --> 01:41:18.280]   - Yeah.
[01:41:18.280 --> 01:41:20.040]   I mean, of course you could do that kind of stuff
[01:41:20.040 --> 01:41:22.360]   with a two-year delay, but it's tougher.
[01:41:22.360 --> 01:41:23.180]   Yeah.
[01:41:23.180 --> 01:41:30.080]   Do you think Tesla or Waymo or some of these companies
[01:41:30.080 --> 01:41:33.880]   that are doing semi or fully autonomous driving
[01:41:33.880 --> 01:41:36.440]   should be doing driver sensing?
[01:41:36.440 --> 01:41:37.280]   - Yes.
[01:41:37.280 --> 01:41:38.880]   - Are you thinking about that kind of stuff?
[01:41:38.880 --> 01:41:42.500]   So not just how we can enhance the in-cab experience
[01:41:42.500 --> 01:41:43.800]   for cars that are manually driven,
[01:41:43.800 --> 01:41:45.920]   but the ones that are increasingly
[01:41:45.920 --> 01:41:47.840]   more autonomously driven.
[01:41:47.840 --> 01:41:50.720]   - Yeah, so if we fast forward to the universe
[01:41:50.720 --> 01:41:51.880]   where it's fully autonomous,
[01:41:51.880 --> 01:41:54.240]   I think interior sensing becomes extremely important
[01:41:54.240 --> 01:41:57.200]   because the role of the driver isn't just to drive.
[01:41:57.200 --> 01:42:00.560]   If you think about it, the driver almost manages
[01:42:00.560 --> 01:42:01.880]   the dynamics within a vehicle.
[01:42:01.880 --> 01:42:03.320]   And so who's going to play that role
[01:42:03.320 --> 01:42:05.760]   when it's an autonomous car?
[01:42:05.760 --> 01:42:09.080]   We want a solution that is able to say,
[01:42:09.080 --> 01:42:11.880]   "Oh my God, Lex is bored to death
[01:42:11.880 --> 01:42:13.440]   'cause the car's moving way too slow.
[01:42:13.440 --> 01:42:14.360]   Let's engage Lex."
[01:42:14.360 --> 01:42:15.620]   Or, "Rana's freaking out
[01:42:15.620 --> 01:42:17.920]   because she doesn't trust this vehicle yet.
[01:42:17.920 --> 01:42:20.520]   So let's tell Rana a little bit more information
[01:42:20.520 --> 01:42:21.440]   about the route."
[01:42:21.440 --> 01:42:22.280]   Or, right?
[01:42:22.280 --> 01:42:25.520]   So I think, or somebody's having a heart attack in the car.
[01:42:25.520 --> 01:42:27.200]   Like you need interior sensing
[01:42:27.200 --> 01:42:29.260]   in fully autonomous vehicles.
[01:42:29.260 --> 01:42:30.920]   But with semi-autonomous vehicles,
[01:42:30.920 --> 01:42:34.680]   I think it's really key to have driver monitoring
[01:42:34.680 --> 01:42:36.280]   because semi-autonomous means
[01:42:36.280 --> 01:42:38.720]   that sometimes the car is in charge,
[01:42:38.720 --> 01:42:41.240]   sometimes the driver's in charge or the co-pilot, right?
[01:42:41.240 --> 01:42:44.680]   And you need both systems to be on the same page.
[01:42:44.680 --> 01:42:46.520]   You need to know, the car needs to know
[01:42:46.520 --> 01:42:49.960]   if the driver's asleep before it transitions control
[01:42:49.960 --> 01:42:51.720]   over to the driver.
[01:42:51.720 --> 01:42:54.120]   And sometimes if the driver's too tired,
[01:42:54.120 --> 01:42:56.600]   the car can say, "I'm going to be a better driver
[01:42:56.600 --> 01:42:57.440]   than you are right now.
[01:42:57.440 --> 01:42:58.480]   I'm taking control over."
[01:42:58.560 --> 01:43:01.280]   So this dynamic, this dance is so key
[01:43:01.280 --> 01:43:03.360]   and you can't do that without driver sensing.
[01:43:03.360 --> 01:43:05.400]   - Yeah, there's a disagreement for the longest time
[01:43:05.400 --> 01:43:07.560]   I've had with Elon that this is obvious
[01:43:07.560 --> 01:43:09.960]   that this should be in the Tesla from day one.
[01:43:09.960 --> 01:43:14.640]   And it's obvious that driver sensing is not a hindrance.
[01:43:14.640 --> 01:43:15.560]   It's not obvious.
[01:43:15.560 --> 01:43:20.540]   I should be careful 'cause having studied this problem,
[01:43:20.540 --> 01:43:21.880]   nothing's really obvious.
[01:43:21.880 --> 01:43:24.320]   But it seems very likely driver sensing
[01:43:24.320 --> 01:43:26.440]   is not a hindrance to an experience.
[01:43:26.440 --> 01:43:30.280]   It's only enriching to the experience
[01:43:30.280 --> 01:43:34.560]   and likely increases the safety.
[01:43:34.560 --> 01:43:38.760]   That said, it is very surprising to me
[01:43:38.760 --> 01:43:42.160]   just having studied semi-autonomous driving,
[01:43:42.160 --> 01:43:45.200]   how well humans are able to manage that dance.
[01:43:45.200 --> 01:43:48.120]   'Cause it was the intuition before you were doing
[01:43:48.120 --> 01:43:51.460]   that kind of thing that humans will become
[01:43:51.460 --> 01:43:54.000]   just incredibly distracted.
[01:43:54.000 --> 01:43:56.400]   They would just let the thing do its thing.
[01:43:56.400 --> 01:43:59.200]   But they're able to, 'cause it is life and death
[01:43:59.200 --> 01:44:00.800]   and they're able to manage that somehow.
[01:44:00.800 --> 01:44:02.720]   But that said, there's no reason
[01:44:02.720 --> 01:44:04.720]   not to have driver sensing on top of that.
[01:44:04.720 --> 01:44:09.720]   I feel like that's going to allow you to do that dance
[01:44:09.720 --> 01:44:12.440]   that you're currently doing without driver sensing,
[01:44:12.440 --> 01:44:15.960]   except touching the steering wheel, to do that even better.
[01:44:15.960 --> 01:44:17.720]   I mean, the possibilities are endless
[01:44:17.720 --> 01:44:19.840]   and the machine learning possibilities are endless.
[01:44:19.840 --> 01:44:21.800]   It's such a beautiful...
[01:44:22.720 --> 01:44:24.280]   It's also a constrained environment,
[01:44:24.280 --> 01:44:26.160]   so you could do a much more effectively
[01:44:26.160 --> 01:44:28.240]   than you can with the external environment.
[01:44:28.240 --> 01:44:31.960]   External environment is full of weird edge cases
[01:44:31.960 --> 01:44:32.800]   and complexities.
[01:44:32.800 --> 01:44:35.080]   There's so much, it's so fascinating,
[01:44:35.080 --> 01:44:36.680]   such a fascinating world.
[01:44:36.680 --> 01:44:40.960]   I do hope that companies like Tesla and others,
[01:44:40.960 --> 01:44:44.320]   even Waymo, which I don't even know
[01:44:44.320 --> 01:44:47.000]   if Waymo's doing anything sophisticated inside the cab.
[01:44:47.000 --> 01:44:47.920]   - I don't think so.
[01:44:47.920 --> 01:44:51.440]   - It's like, what is it?
[01:44:51.440 --> 01:44:53.440]   - I honestly think, I honestly think,
[01:44:53.440 --> 01:44:56.200]   it goes back to the robotics thing we were talking about,
[01:44:56.200 --> 01:44:58.600]   which is like great engineers
[01:44:58.600 --> 01:45:01.520]   that are building these AI systems
[01:45:01.520 --> 01:45:03.680]   just are afraid of the human being.
[01:45:03.680 --> 01:45:05.640]   - And not thinking about the human experience,
[01:45:05.640 --> 01:45:07.120]   they're thinking about the features
[01:45:07.120 --> 01:45:10.800]   and the perceptual abilities of that thing.
[01:45:10.800 --> 01:45:13.440]   - They think the best way I can serve the human
[01:45:13.440 --> 01:45:17.440]   is by doing the best perception and control I can,
[01:45:17.440 --> 01:45:18.960]   by looking at the external environment,
[01:45:18.960 --> 01:45:20.560]   keeping the human safe.
[01:45:20.560 --> 01:45:22.560]   But like, there's a huge, I'm here.
[01:45:22.560 --> 01:45:23.400]   - Right.
[01:45:23.400 --> 01:45:28.320]   - Like, you know, I need to be noticed
[01:45:28.320 --> 01:45:32.680]   and interacted with and understood
[01:45:32.680 --> 01:45:33.560]   and all those kinds of things,
[01:45:33.560 --> 01:45:35.880]   even just on a personal level for entertainment,
[01:45:35.880 --> 01:45:37.240]   honestly, for entertainment.
[01:45:37.240 --> 01:45:38.600]   - Yeah.
[01:45:38.600 --> 01:45:41.240]   You know, one of the coolest work we did in collaboration
[01:45:41.240 --> 01:45:44.400]   with MIT around this was we looked at longitudinal data,
[01:45:44.400 --> 01:45:49.400]   right, 'cause, you know, MIT had access to like tons of data
[01:45:50.400 --> 01:45:54.680]   and like just seeing the patterns of people,
[01:45:54.680 --> 01:45:56.920]   like driving in the morning off to work
[01:45:56.920 --> 01:45:58.720]   versus like commuting back from work
[01:45:58.720 --> 01:46:02.320]   or weekend driving versus weekday driving.
[01:46:02.320 --> 01:46:05.080]   And wouldn't it be so cool if your car knew that
[01:46:05.080 --> 01:46:09.160]   and then was able to optimize either the route
[01:46:09.160 --> 01:46:11.880]   or the experience or even make recommendations?
[01:46:11.880 --> 01:46:12.720]   - Yeah.
[01:46:12.720 --> 01:46:13.880]   - I think it's very powerful.
[01:46:13.880 --> 01:46:15.800]   - Yeah, like, why are you taking this route?
[01:46:15.800 --> 01:46:18.200]   You're always unhappy when you take this route
[01:46:18.200 --> 01:46:20.360]   and you're always happy when you take this alternative route.
[01:46:20.360 --> 01:46:21.200]   Take that route instead.
[01:46:21.200 --> 01:46:22.520]   - Right, exactly.
[01:46:22.520 --> 01:46:26.000]   - I mean, to have that, even that little step
[01:46:26.000 --> 01:46:29.520]   of relationship with a car, I think is incredible.
[01:46:29.520 --> 01:46:31.160]   Of course, you have to get the privacy right,
[01:46:31.160 --> 01:46:32.520]   you have to get all that kind of stuff right,
[01:46:32.520 --> 01:46:34.800]   but I wish I, honestly, you know,
[01:46:34.800 --> 01:46:36.760]   people are like paranoid about this,
[01:46:36.760 --> 01:46:39.480]   but I would like a smart refrigerator.
[01:46:39.480 --> 01:46:42.960]   We have such a deep connection with food
[01:46:42.960 --> 01:46:44.800]   as a human civilization.
[01:46:44.800 --> 01:46:47.840]   I would like to have a refrigerator
[01:46:47.840 --> 01:46:50.320]   that would understand me,
[01:46:50.320 --> 01:46:53.480]   that, you know, I also have a complex relationship with food
[01:46:53.480 --> 01:46:56.800]   'cause I pig out too easily and all that kind of stuff.
[01:46:56.800 --> 01:47:01.280]   So, you know, like maybe I want the refrigerator
[01:47:01.280 --> 01:47:02.640]   to be like, are you sure about this?
[01:47:02.640 --> 01:47:05.160]   'Cause maybe you're just feeling down or tired.
[01:47:05.160 --> 01:47:06.000]   Like maybe let's sleep over.
[01:47:06.000 --> 01:47:08.400]   - Your vision of the smart refrigerator
[01:47:08.400 --> 01:47:10.200]   is way kinder than mine.
[01:47:10.200 --> 01:47:11.600]   - Is it just me yelling at you?
[01:47:11.600 --> 01:47:14.320]   - Yeah, no, it was just 'cause I don't,
[01:47:14.320 --> 01:47:18.480]   you know, I don't drink alcohol, I don't smoke,
[01:47:18.480 --> 01:47:22.080]   but I eat a ton of chocolate, like it's my vice.
[01:47:22.080 --> 01:47:24.560]   And so I, and sometimes I scream too,
[01:47:24.560 --> 01:47:26.600]   and I'm like, okay, my smart refrigerator
[01:47:26.600 --> 01:47:28.280]   will just lock down.
[01:47:28.280 --> 01:47:30.840]   It'll just say, dude, you've had way too many today.
[01:47:30.840 --> 01:47:31.680]   Like, done.
[01:47:31.680 --> 01:47:34.880]   - Yeah, no, but here's the thing.
[01:47:34.880 --> 01:47:37.680]   Are you, do you regret having,
[01:47:37.680 --> 01:47:43.520]   like, let's say, not the next day, but 30 days later,
[01:47:44.520 --> 01:47:47.560]   would you, what would you like the refrigerator
[01:47:47.560 --> 01:47:49.040]   to have done then?
[01:47:49.040 --> 01:47:51.200]   - Well, I think actually, like,
[01:47:51.200 --> 01:47:53.920]   the more positive relationship would be one
[01:47:53.920 --> 01:47:55.840]   where there's a conversation, right?
[01:47:55.840 --> 01:47:58.040]   As opposed to like, that's probably like
[01:47:58.040 --> 01:48:00.800]   the more sustainable relationship.
[01:48:00.800 --> 01:48:03.480]   - It's like late at night, just, no, listen, listen.
[01:48:03.480 --> 01:48:07.320]   I know I told you an hour ago that this is not a good idea,
[01:48:07.320 --> 01:48:09.760]   but just listen, things have changed.
[01:48:09.760 --> 01:48:12.680]   I can just imagine a bunch of stuff being made up
[01:48:12.680 --> 01:48:15.160]   just to convince. - Oh my God, that's hilarious.
[01:48:15.160 --> 01:48:18.800]   - But I mean, I just think that there's opportunities there.
[01:48:18.800 --> 01:48:20.480]   I mean, maybe not locking down,
[01:48:20.480 --> 01:48:25.480]   but for our systems that are such a deep part of our lives,
[01:48:25.480 --> 01:48:30.920]   like we use, we use, a lot of us,
[01:48:30.920 --> 01:48:34.360]   a lot of people that commute use their car every single day.
[01:48:34.360 --> 01:48:36.600]   A lot of us use a refrigerator every single day,
[01:48:36.600 --> 01:48:38.120]   the microwave every single day.
[01:48:38.160 --> 01:48:43.160]   Like, and we just, like, I feel like certain things
[01:48:43.160 --> 01:48:47.280]   could be made more efficient, more enriching,
[01:48:47.280 --> 01:48:50.640]   and AI is there to help, like some,
[01:48:50.640 --> 01:48:53.920]   just basic recognition of you as a human being,
[01:48:53.920 --> 01:48:56.400]   about your patterns, what makes you happy and not happy
[01:48:56.400 --> 01:48:57.440]   and all that kind of stuff.
[01:48:57.440 --> 01:48:59.240]   And the car, obviously, like--
[01:48:59.240 --> 01:49:02.440]   - Maybe we'll say, wait, wait, wait, wait, wait,
[01:49:02.440 --> 01:49:06.520]   instead of this like Ben and Terry's ice cream,
[01:49:06.520 --> 01:49:09.480]   how about this hummus and carrots or something?
[01:49:09.480 --> 01:49:10.480]   I don't know.
[01:49:10.480 --> 01:49:11.640]   Maybe we can make a--
[01:49:11.640 --> 01:49:12.480]   - Yeah, like a reminder--
[01:49:12.480 --> 01:49:14.680]   - Just in time recommendation, right?
[01:49:14.680 --> 01:49:17.360]   - But not like a generic one,
[01:49:17.360 --> 01:49:20.980]   but a reminder that last time you chose the carrots,
[01:49:20.980 --> 01:49:24.120]   you smiled 17 times more the next day.
[01:49:24.120 --> 01:49:25.440]   - You were happier the next day, right?
[01:49:25.440 --> 01:49:27.760]   - Yeah, you were happier the next day.
[01:49:27.760 --> 01:49:31.960]   But yeah, I don't, but then again,
[01:49:31.960 --> 01:49:34.600]   if you're the kind of person that gets better
[01:49:34.600 --> 01:49:38.440]   from negative comments, you could say like,
[01:49:38.440 --> 01:49:41.400]   "Hey, remember that wedding you're going to?
[01:49:41.400 --> 01:49:43.640]   "You wanna fit into that dress?
[01:49:43.640 --> 01:49:45.320]   "Remember about that?
[01:49:45.320 --> 01:49:47.920]   "Let's think about that before you're eating this."
[01:49:47.920 --> 01:49:50.720]   Probably that would work for me,
[01:49:50.720 --> 01:49:54.840]   like a refrigerator that is just ruthless at shaming me.
[01:49:54.840 --> 01:49:57.240]   But I would, of course, welcome it.
[01:49:57.240 --> 01:50:00.520]   That would work for me, just that--
[01:50:00.520 --> 01:50:02.320]   - Well, it would, no, I think it would,
[01:50:02.320 --> 01:50:05.440]   if it's really like smart, it would optimize its nudging
[01:50:05.440 --> 01:50:07.240]   based on what works for you, right?
[01:50:07.240 --> 01:50:09.640]   - Exactly, that's the whole point, personalization.
[01:50:09.640 --> 01:50:11.760]   In every way, deep personalization.
[01:50:11.760 --> 01:50:14.880]   You were a part of a webinar titled,
[01:50:14.880 --> 01:50:16.400]   "Advancing Road Safety,
[01:50:16.400 --> 01:50:19.500]   "The State of Alcohol Intoxication Research."
[01:50:19.500 --> 01:50:21.600]   So for people who don't know,
[01:50:21.600 --> 01:50:24.000]   every year, 1.3 million people around the world
[01:50:24.000 --> 01:50:29.000]   die in road crashes, and more than 20% of these fatalities
[01:50:29.000 --> 01:50:31.360]   are estimated to be alcohol-related.
[01:50:31.360 --> 01:50:33.280]   A lot of them are also distraction-related.
[01:50:33.280 --> 01:50:36.880]   So can AI help with the alcohol thing?
[01:50:36.880 --> 01:50:39.920]   - I think the answer is yes.
[01:50:39.920 --> 01:50:42.680]   There are signals, and we know that as humans,
[01:50:42.680 --> 01:50:44.280]   like we can tell when a person
[01:50:44.280 --> 01:50:51.120]   is at different phases of being drunk, right?
[01:50:51.120 --> 01:50:53.640]   And I think you can use technology to do the same.
[01:50:53.640 --> 01:50:56.040]   And again, I think the ultimate solution's gonna be
[01:50:56.040 --> 01:50:58.520]   a combination of different sensors.
[01:50:58.520 --> 01:51:01.480]   - How hard is the problem from the vision perspective?
[01:51:01.480 --> 01:51:02.920]   - I think it's non-trivial.
[01:51:02.920 --> 01:51:04.000]   I think it's non-trivial.
[01:51:04.000 --> 01:51:06.640]   And I think the biggest part is getting the data, right?
[01:51:06.640 --> 01:51:09.120]   It's like getting enough data examples.
[01:51:09.120 --> 01:51:11.080]   So for this research project,
[01:51:11.080 --> 01:51:15.300]   we partnered with the Transportation Authorities of Sweden,
[01:51:15.300 --> 01:51:18.800]   and we literally had a racetrack with a safety driver,
[01:51:18.800 --> 01:51:21.640]   and we basically progressively got people drunk.
[01:51:21.640 --> 01:51:22.480]   - Nice.
[01:51:22.480 --> 01:51:24.280]   - So, but you know,
[01:51:25.840 --> 01:51:28.760]   that's a very expensive dataset to collect,
[01:51:28.760 --> 01:51:30.240]   and you wanna collect it globally
[01:51:30.240 --> 01:51:32.680]   and in multiple conditions.
[01:51:32.680 --> 01:51:35.200]   - Yeah, the ethics of collecting a dataset
[01:51:35.200 --> 01:51:36.640]   where people are drunk is tricky.
[01:51:36.640 --> 01:51:37.880]   - Yeah, yeah, definitely.
[01:51:37.880 --> 01:51:40.920]   - Which is funny because, I mean,
[01:51:40.920 --> 01:51:43.440]   let's put drunk driving aside.
[01:51:43.440 --> 01:51:45.720]   The number of drunk people in the world every day
[01:51:45.720 --> 01:51:47.120]   is very large.
[01:51:47.120 --> 01:51:49.280]   It'd be nice to have a large dataset of drunk people
[01:51:49.280 --> 01:51:50.520]   getting progressively drunk.
[01:51:50.520 --> 01:51:51.800]   In fact, you could build an app
[01:51:51.800 --> 01:51:55.280]   where people can donate their data 'cause it's hilarious.
[01:51:55.280 --> 01:51:58.400]   - Right, actually, yeah, but the liability.
[01:51:58.400 --> 01:52:00.720]   - Liability, the ethics, the how do you get it right,
[01:52:00.720 --> 01:52:02.480]   it's tricky, it's really, really tricky.
[01:52:02.480 --> 01:52:05.080]   'Cause like drinking is one of those things
[01:52:05.080 --> 01:52:07.600]   that's funny and hilarious and we're loved,
[01:52:07.600 --> 01:52:10.180]   it's social, so on and so forth,
[01:52:10.180 --> 01:52:13.480]   but it's also the thing that hurts a lot of people,
[01:52:13.480 --> 01:52:14.320]   like a lot of people.
[01:52:14.320 --> 01:52:16.360]   Like alcohol is one of those things, it's legal,
[01:52:16.360 --> 01:52:21.200]   but it's really damaging to a lot of lives.
[01:52:21.200 --> 01:52:26.200]   It destroys lives and not just in the driving context.
[01:52:26.200 --> 01:52:28.960]   I should mention, people should listen to Andrew Huberman
[01:52:28.960 --> 01:52:32.160]   who recently talked about alcohol.
[01:52:32.160 --> 01:52:33.160]   He has an amazing podcast.
[01:52:33.160 --> 01:52:36.200]   Andrew Huberman is a neuroscientist from Stanford
[01:52:36.200 --> 01:52:37.880]   and a good friend of mine.
[01:52:37.880 --> 01:52:40.800]   And he's like a human encyclopedia
[01:52:40.800 --> 01:52:44.120]   about all health-related wisdom.
[01:52:44.120 --> 01:52:45.960]   So he has a podcast, you would love it.
[01:52:45.960 --> 01:52:46.800]   - I would love that.
[01:52:46.800 --> 01:52:47.800]   - No, no, no, no, no.
[01:52:47.800 --> 01:52:49.440]   Oh, you don't know Andrew Huberman?
[01:52:49.440 --> 01:52:52.200]   Okay, listen, you'll listen to Andrew,
[01:52:52.200 --> 01:52:54.120]   it's called Huberman Lab Podcast.
[01:52:54.120 --> 01:52:56.280]   This is your assignment, just listen to one.
[01:52:56.280 --> 01:52:58.480]   I guarantee you this will be a thing where you say,
[01:52:58.480 --> 01:53:03.480]   "Lex, this is the greatest human I have ever discovered."
[01:53:03.480 --> 01:53:06.280]   - Oh my God, 'cause I'm really on a journey
[01:53:06.280 --> 01:53:09.120]   of kind of health and wellness and I'm learning lots
[01:53:09.120 --> 01:53:11.920]   and I'm trying to build these, I guess,
[01:53:11.920 --> 01:53:14.280]   atomic habits around just being healthy.
[01:53:14.280 --> 01:53:17.680]   So yeah, I'm definitely gonna do this.
[01:53:17.680 --> 01:53:20.420]   - His whole thing, this is great.
[01:53:20.420 --> 01:53:26.480]   He's a legit scientist, like really well published.
[01:53:26.480 --> 01:53:29.760]   But in his podcast, what he does,
[01:53:29.760 --> 01:53:31.840]   he's not talking about his own work.
[01:53:31.840 --> 01:53:34.600]   He's like a human encyclopedia of papers.
[01:53:34.600 --> 01:53:37.640]   And so his whole thing is he takes a topic
[01:53:37.640 --> 01:53:40.200]   and in a very fast, you mentioned atomic habits,
[01:53:40.200 --> 01:53:44.200]   like very clear way, summarizes the research
[01:53:44.200 --> 01:53:47.360]   in a way that leads to protocols of what you should do.
[01:53:47.360 --> 01:53:49.920]   He's really big on like, not like,
[01:53:49.920 --> 01:53:51.320]   "This is what the science says,"
[01:53:51.320 --> 01:53:53.600]   but like, "This is literally what you should be doing
[01:53:53.600 --> 01:53:54.440]   "according to the science."
[01:53:54.440 --> 01:53:55.840]   So like, he's really big.
[01:53:55.840 --> 01:53:58.160]   And there's a lot of recommendations he does,
[01:53:58.160 --> 01:54:02.760]   which several of them I definitely don't do.
[01:54:02.760 --> 01:54:07.760]   Like, get sunlight as soon as possible from waking up
[01:54:07.760 --> 01:54:10.880]   and like for prolonged periods of time.
[01:54:10.880 --> 01:54:13.560]   That's a really big one and there's a lot of science
[01:54:13.560 --> 01:54:14.800]   behind that one.
[01:54:14.800 --> 01:54:16.880]   There's a bunch of stuff, very systematic.
[01:54:16.880 --> 01:54:20.560]   - You're gonna be like, "Lex, this is my new favorite person.
[01:54:20.560 --> 01:54:22.200]   "I guarantee it."
[01:54:22.200 --> 01:54:24.720]   And if you guys somehow don't know Andrew Huberman
[01:54:24.720 --> 01:54:26.440]   and you care about your wellbeing,
[01:54:26.440 --> 01:54:30.440]   you should definitely listen to him.
[01:54:30.440 --> 01:54:31.280]   Love you, Andrew.
[01:54:31.280 --> 01:54:35.760]   Anyway, so what were we talking about?
[01:54:35.760 --> 01:54:39.400]   Oh, alcohol and detecting alcohol.
[01:54:39.400 --> 01:54:40.800]   So this is a problem you care about
[01:54:40.800 --> 01:54:42.120]   and you've been trying to solve.
[01:54:42.120 --> 01:54:44.920]   - And actually like broadening it,
[01:54:44.960 --> 01:54:48.840]   I do believe that the car is gonna be a wellness center.
[01:54:48.840 --> 01:54:52.040]   Like, because again, imagine if you have a variety
[01:54:52.040 --> 01:54:55.280]   of sensors inside the vehicle tracking,
[01:54:55.280 --> 01:54:58.560]   not just your emotional state or level of distraction
[01:54:58.560 --> 01:55:00.880]   and drowsiness and drowsiness,
[01:55:00.880 --> 01:55:03.760]   level of distraction, drowsiness and intoxication,
[01:55:03.760 --> 01:55:08.760]   but also maybe even things like your heart rate
[01:55:08.760 --> 01:55:11.720]   and your heart rate variability and your breathing rate.
[01:55:13.880 --> 01:55:16.120]   And it can start like optimizing,
[01:55:16.120 --> 01:55:19.640]   yeah, it can optimize the ride based on what your goals are.
[01:55:19.640 --> 01:55:21.920]   So I think we're gonna start to see more of that
[01:55:21.920 --> 01:55:24.520]   and I'm excited about that.
[01:55:24.520 --> 01:55:27.640]   - Yeah, what are the challenges you're tackling
[01:55:27.640 --> 01:55:28.800]   with SmartEye currently?
[01:55:28.800 --> 01:55:32.200]   What's like the trickiest things to get?
[01:55:32.200 --> 01:55:35.720]   Is it basically convincing more and more car companies
[01:55:35.720 --> 01:55:38.680]   that having AI inside the car is a good idea
[01:55:38.680 --> 01:55:43.040]   or is there some, is there more technical
[01:55:43.040 --> 01:55:44.920]   algorithmic challenges?
[01:55:44.920 --> 01:55:47.800]   What's been keeping you mentally busy?
[01:55:47.800 --> 01:55:49.640]   - I think a lot of the car companies
[01:55:49.640 --> 01:55:52.440]   we are in conversations with are already interested
[01:55:52.440 --> 01:55:54.120]   in definitely driver monitoring.
[01:55:54.120 --> 01:55:56.600]   Like I think it's becoming a must have,
[01:55:56.600 --> 01:55:59.800]   but even interior sensing, I can see like we're engaged
[01:55:59.800 --> 01:56:01.800]   in a lot of like advanced engineering projects
[01:56:01.800 --> 01:56:03.040]   and proof of concepts.
[01:56:03.040 --> 01:56:06.600]   I think technologically though,
[01:56:06.600 --> 01:56:07.840]   and even the technology,
[01:56:07.840 --> 01:56:10.320]   I can see a path to making it happen.
[01:56:10.320 --> 01:56:11.520]   I think it's the use case.
[01:56:11.920 --> 01:56:16.120]   How does the car respond once it knows something about you?
[01:56:16.120 --> 01:56:18.480]   Because you want it to respond in a thoughtful way
[01:56:18.480 --> 01:56:23.480]   that isn't off-putting to the consumer in the car.
[01:56:23.480 --> 01:56:25.600]   So I think that's like the user experience.
[01:56:25.600 --> 01:56:27.400]   I don't think we've really nailed that.
[01:56:27.400 --> 01:56:30.440]   And we usually, that's not part,
[01:56:30.440 --> 01:56:31.800]   we're the sensing platform,
[01:56:31.800 --> 01:56:34.240]   but we usually collaborate with the car manufacturer
[01:56:34.240 --> 01:56:35.840]   to decide what the use case is.
[01:56:35.840 --> 01:56:39.880]   So say you figure out that somebody's angry while driving.
[01:56:39.880 --> 01:56:41.320]   Okay, what should the car do?
[01:56:42.320 --> 01:56:43.600]   You know?
[01:56:43.600 --> 01:56:47.120]   - Do you see yourself as a role of nudging,
[01:56:47.120 --> 01:56:51.200]   of like basically coming up with solutions essentially
[01:56:51.200 --> 01:56:54.200]   that, and then the car manufacturers
[01:56:54.200 --> 01:56:56.360]   kind of put their own little spin on it?
[01:56:56.360 --> 01:57:00.320]   - Right, like we are like the ideation,
[01:57:00.320 --> 01:57:02.560]   creative thought partner,
[01:57:02.560 --> 01:57:03.600]   but at the end of the day,
[01:57:03.600 --> 01:57:05.200]   the car company needs to decide
[01:57:05.200 --> 01:57:06.640]   what's on brand for them, right?
[01:57:06.640 --> 01:57:10.040]   Like maybe when it figures out that you're distracted
[01:57:10.040 --> 01:57:12.600]   or drowsy, it shows you a coffee cup, right?
[01:57:12.600 --> 01:57:14.960]   Or maybe it takes more aggressive behaviors
[01:57:14.960 --> 01:57:16.080]   and basically said, okay,
[01:57:16.080 --> 01:57:18.320]   if you don't like take a rest in the next five minutes,
[01:57:18.320 --> 01:57:19.640]   the car's gonna shut down, right?
[01:57:19.640 --> 01:57:23.040]   Like there's a whole range of actions the car can take
[01:57:23.040 --> 01:57:25.880]   and doing the thing that is most,
[01:57:25.880 --> 01:57:29.400]   yeah, that builds trust with the driver and the passengers.
[01:57:29.400 --> 01:57:32.480]   I think that's what we need to be very careful about.
[01:57:32.480 --> 01:57:35.400]   - Yeah, car companies are funny
[01:57:35.400 --> 01:57:37.080]   'cause they have their own like,
[01:57:37.080 --> 01:57:39.600]   I mean, that's why people get cars still.
[01:57:39.600 --> 01:57:40.520]   I hope that changes,
[01:57:40.520 --> 01:57:42.760]   but they get it 'cause it's a certain feel and look
[01:57:42.760 --> 01:57:47.760]   and it's a certain, they become proud like Mercedes Benz
[01:57:47.760 --> 01:57:51.720]   or BMW or whatever, and that's their thing.
[01:57:51.720 --> 01:57:54.160]   That's the family brand or something like that,
[01:57:54.160 --> 01:57:57.600]   or Ford or GM, whatever, they stick to that thing.
[01:57:57.600 --> 01:57:58.680]   It's interesting.
[01:57:58.680 --> 01:58:01.000]   It's like, it should be, I don't know.
[01:58:01.000 --> 01:58:05.000]   It should be a little more about the technology inside.
[01:58:05.000 --> 01:58:09.440]   And I suppose there too, there could be a branding,
[01:58:09.440 --> 01:58:13.320]   like a very specific style of luxury or fun,
[01:58:13.320 --> 01:58:16.200]   all that kind of stuff, yeah.
[01:58:16.200 --> 01:58:19.680]   - You know, I have an AI focused fund
[01:58:19.680 --> 01:58:22.680]   to invest in early stage kind of AI driven companies.
[01:58:22.680 --> 01:58:24.800]   And one of the companies we're looking at
[01:58:24.800 --> 01:58:27.480]   is trying to do what Tesla did, but for boats,
[01:58:27.480 --> 01:58:28.680]   for recreational boats.
[01:58:28.680 --> 01:58:30.720]   Yeah, so they're building an electric
[01:58:30.720 --> 01:58:34.160]   and kind of slash autonomous boat.
[01:58:34.160 --> 01:58:35.720]   And it's kind of the same issues,
[01:58:35.720 --> 01:58:38.480]   like what kind of sensors can you put in?
[01:58:38.480 --> 01:58:40.760]   What kind of states can you detect
[01:58:40.760 --> 01:58:43.640]   both exterior and interior within the boat?
[01:58:43.640 --> 01:58:45.440]   Anyways, it's like really interesting.
[01:58:45.440 --> 01:58:46.720]   Do you boat at all?
[01:58:46.720 --> 01:58:49.640]   - No, not well, not in that way.
[01:58:49.640 --> 01:58:54.640]   I do like to get on the lake or a river and fish from a boat,
[01:58:54.640 --> 01:58:57.280]   but that's not boating.
[01:58:57.280 --> 01:58:59.520]   That's a different, still boating.
[01:58:59.520 --> 01:59:01.880]   - Low tech, a low tech. - Low tech boat.
[01:59:01.880 --> 01:59:04.480]   Get away from, get closer to nature boat.
[01:59:04.480 --> 01:59:07.160]   I guess going out to the ocean
[01:59:07.160 --> 01:59:12.160]   is also getting closer to nature in some deep sense.
[01:59:12.160 --> 01:59:14.440]   I mean, I guess that's why people love it.
[01:59:14.440 --> 01:59:19.440]   The enormity of the water just underneath you, yeah.
[01:59:19.440 --> 01:59:20.720]   - I love the water.
[01:59:20.720 --> 01:59:22.880]   - I love both.
[01:59:22.880 --> 01:59:23.800]   I love salt water.
[01:59:23.800 --> 01:59:25.640]   It was like the big and just it's humbling
[01:59:25.640 --> 01:59:29.280]   to be in front of this giant thing that's so powerful
[01:59:29.280 --> 01:59:31.640]   that was here before us and be here after.
[01:59:31.640 --> 01:59:36.640]   But I also love the peace of a small like wooded lake.
[01:59:36.640 --> 01:59:38.280]   It's just, everything's calm.
[01:59:38.280 --> 01:59:41.320]   - Therapeutic.
[01:59:41.320 --> 01:59:46.520]   - You tweeted that I'm excited
[01:59:46.520 --> 01:59:49.680]   about Amazon's acquisition of iRobot.
[01:59:49.680 --> 01:59:51.800]   I think it's a super interesting,
[01:59:51.800 --> 01:59:54.280]   just given the trajectory of which you're part of,
[01:59:54.280 --> 01:59:57.880]   of these honestly small number of companies
[01:59:57.880 --> 01:59:59.320]   that are playing in this space
[01:59:59.320 --> 02:00:02.160]   that are like trying to have an impact on human beings.
[02:00:02.160 --> 02:00:05.000]   So it is an interesting moment in time
[02:00:05.000 --> 02:00:07.000]   that Amazon would acquire iRobot.
[02:00:07.000 --> 02:00:11.920]   You tweet, I imagine a future where home robots
[02:00:11.920 --> 02:00:16.160]   are as ubiquitous as microwaves or toasters.
[02:00:16.160 --> 02:00:18.880]   Here are three reasons why I think this is exciting.
[02:00:18.880 --> 02:00:20.520]   If you remember, I can look it up.
[02:00:20.520 --> 02:00:22.320]   But why is this exciting to you?
[02:00:22.320 --> 02:00:25.800]   - I mean, I think the first reason why this is exciting,
[02:00:25.800 --> 02:00:30.520]   I can't remember the exact order in which I put them.
[02:00:30.520 --> 02:00:34.400]   But one is just, it's gonna be an incredible platform
[02:00:34.400 --> 02:00:37.120]   for understanding our behaviors within the home.
[02:00:37.120 --> 02:00:42.960]   If you think about Roomba, which is the robot vacuum cleaner,
[02:00:42.960 --> 02:00:45.520]   the flagship product of iRobot at the moment,
[02:00:45.520 --> 02:00:48.200]   it's like running around your home,
[02:00:48.200 --> 02:00:49.360]   understanding the layout,
[02:00:49.360 --> 02:00:51.240]   it's understanding what's clean and what's not,
[02:00:51.240 --> 02:00:52.680]   how often do you clean your house?
[02:00:52.680 --> 02:00:56.360]   And all of these behaviors are a piece of the puzzle
[02:00:56.360 --> 02:00:58.720]   in terms of understanding who you are as a consumer.
[02:00:58.720 --> 02:01:01.320]   And I think that could be, again,
[02:01:01.320 --> 02:01:04.720]   used in really meaningful ways,
[02:01:04.720 --> 02:01:06.880]   not just to recommend better products or whatever,
[02:01:06.880 --> 02:01:09.600]   but actually to improve your experience as a human being.
[02:01:09.600 --> 02:01:11.720]   So I think that's very interesting.
[02:01:11.720 --> 02:01:17.960]   I think the natural evolution of these robots in the home,
[02:01:17.960 --> 02:01:19.600]   so it's interesting.
[02:01:19.600 --> 02:01:22.720]   Roomba isn't really a social robot, right, at the moment.
[02:01:22.720 --> 02:01:27.000]   But I once interviewed one of the chief engineers
[02:01:27.000 --> 02:01:28.440]   on the Roomba team,
[02:01:28.440 --> 02:01:31.280]   and he talked about how people named their Roombas.
[02:01:31.280 --> 02:01:33.360]   And if their Roomba broke down,
[02:01:33.360 --> 02:01:36.560]   they would call in and say, "My Roomba broke down,"
[02:01:36.560 --> 02:01:37.520]   and the company would say,
[02:01:37.520 --> 02:01:38.840]   "Well, we'll just send you a new one."
[02:01:38.840 --> 02:01:41.200]   And, "No, no, no, Rosie, you have to like,
[02:01:41.200 --> 02:01:44.400]   "yeah, I want you to fix this particular robot."
[02:01:44.400 --> 02:01:48.000]   So people have already built
[02:01:48.000 --> 02:01:51.680]   interesting emotional connections with these home robots.
[02:01:51.680 --> 02:01:54.720]   And I think that, again, that provides a platform
[02:01:54.720 --> 02:01:58.320]   for really interesting things to just motivate change.
[02:01:58.320 --> 02:01:59.400]   Like it could help you.
[02:01:59.400 --> 02:02:00.560]   I mean, one of the companies
[02:02:00.560 --> 02:02:03.360]   that spun out of MIT, Catalia Health,
[02:02:03.360 --> 02:02:06.880]   the guy who started it spent a lot of time
[02:02:06.880 --> 02:02:09.600]   building robots that help with weight management.
[02:02:09.600 --> 02:02:12.480]   So weight management, sleep, eating better,
[02:02:12.480 --> 02:02:14.280]   yeah, all of these things.
[02:02:14.280 --> 02:02:16.520]   - Well, if I'm being honest,
[02:02:16.520 --> 02:02:19.480]   Amazon does not exactly have a track record
[02:02:19.480 --> 02:02:22.200]   of winning over people in terms of trust.
[02:02:22.200 --> 02:02:25.880]   Now, that said, it's a really difficult problem
[02:02:25.880 --> 02:02:28.960]   for a human being to let a robot in their home
[02:02:28.960 --> 02:02:30.440]   that has a camera on it.
[02:02:30.440 --> 02:02:31.280]   - Right.
[02:02:31.280 --> 02:02:33.240]   - That's really, really, really tough.
[02:02:33.240 --> 02:02:36.320]   And I think Roomba actually,
[02:02:36.320 --> 02:02:38.040]   I have to think about this,
[02:02:38.040 --> 02:02:40.120]   but I'm pretty sure now,
[02:02:40.120 --> 02:02:42.360]   or for some time already has had cameras
[02:02:42.360 --> 02:02:46.080]   because they're doing, the most recent Roomba,
[02:02:46.080 --> 02:02:47.320]   I have so many Roombas.
[02:02:47.320 --> 02:02:48.760]   - Oh, you actually do?
[02:02:48.760 --> 02:02:49.600]   - Well, I programmed it.
[02:02:49.600 --> 02:02:51.200]   I don't use a Roomba for back off.
[02:02:51.200 --> 02:02:52.320]   People that have been to my place,
[02:02:52.320 --> 02:02:55.280]   they're like, yeah, you definitely don't use these Roombas.
[02:02:55.280 --> 02:03:00.880]   - I can't tell like the valence of this comment.
[02:03:00.880 --> 02:03:02.360]   Was it a compliment or like?
[02:03:02.360 --> 02:03:06.080]   - No, it's just a bunch of electronics everywhere.
[02:03:06.080 --> 02:03:08.920]   I have six or seven computers,
[02:03:08.920 --> 02:03:11.080]   I have robots everywhere, Lego robots,
[02:03:11.080 --> 02:03:12.720]   I have small robots and big robots,
[02:03:12.720 --> 02:03:17.040]   and just giant, just piles of robot stuff.
[02:03:17.040 --> 02:03:17.960]   And yeah.
[02:03:17.960 --> 02:03:22.320]   But including the Roombas,
[02:03:22.320 --> 02:03:25.480]   they're being used for their body and intelligence,
[02:03:25.480 --> 02:03:26.840]   but not for their purpose.
[02:03:26.840 --> 02:03:28.680]   (both laughing)
[02:03:28.680 --> 02:03:32.080]   I've changed them, repurposed them for other purposes,
[02:03:32.080 --> 02:03:33.880]   for deeper, more meaningful purposes
[02:03:33.880 --> 02:03:36.400]   than just like the butter robot.
[02:03:36.400 --> 02:03:39.900]   Yeah, which just brings a lot of people happiness, I'm sure.
[02:03:41.000 --> 02:03:44.440]   They have a camera because the thing they advertised,
[02:03:44.440 --> 02:03:46.400]   I had my own cameras too,
[02:03:46.400 --> 02:03:49.560]   but the camera on the new Roomba,
[02:03:49.560 --> 02:03:52.400]   they have like state-of-the-art poop detection
[02:03:52.400 --> 02:03:54.840]   as they advertised, which is a very difficult,
[02:03:54.840 --> 02:03:57.680]   apparently it's a big problem for vacuum cleaners,
[02:03:57.680 --> 02:03:59.640]   is if they go over like dog poop,
[02:03:59.640 --> 02:04:02.080]   it just runs it over and creates a giant mess.
[02:04:02.080 --> 02:04:04.440]   So they have like,
[02:04:04.440 --> 02:04:07.240]   apparently they collected like a huge amount of data
[02:04:07.240 --> 02:04:09.760]   and different shapes and looks and whatever of poop
[02:04:09.760 --> 02:04:12.320]   and now they're able to avoid it and so on.
[02:04:12.320 --> 02:04:14.360]   They're very proud of this.
[02:04:14.360 --> 02:04:15.480]   So there is a camera,
[02:04:15.480 --> 02:04:18.200]   but you don't think of it as having a camera.
[02:04:18.200 --> 02:04:21.880]   Yeah, you don't think of it as having a camera
[02:04:21.880 --> 02:04:23.960]   because you've grown to trust it, I guess,
[02:04:23.960 --> 02:04:26.360]   'cause our phones, at least most of us,
[02:04:26.360 --> 02:04:30.440]   seem to trust this phone,
[02:04:30.440 --> 02:04:33.040]   even though there's a camera looking directly at you.
[02:04:33.040 --> 02:04:37.720]   I think that if you trust that the company
[02:04:39.640 --> 02:04:41.440]   is taking security very seriously,
[02:04:41.440 --> 02:04:42.640]   I actually don't know how that trust
[02:04:42.640 --> 02:04:44.440]   was earned with smartphones.
[02:04:44.440 --> 02:04:48.680]   I think it just started to provide a lot of positive value
[02:04:48.680 --> 02:04:50.520]   into your life where you just took it in
[02:04:50.520 --> 02:04:52.360]   and then the company over time has shown
[02:04:52.360 --> 02:04:55.120]   that it takes privacy very seriously, that kind of stuff.
[02:04:55.120 --> 02:04:58.600]   But I just, Amazon is not always,
[02:04:58.600 --> 02:05:01.400]   in its social robots, communicated
[02:05:01.400 --> 02:05:03.680]   this is a trustworthy thing,
[02:05:03.680 --> 02:05:06.760]   both in terms of culture and competence.
[02:05:06.760 --> 02:05:08.880]   'Cause I think privacy is not just about
[02:05:08.880 --> 02:05:10.640]   what do you intend to do,
[02:05:10.640 --> 02:05:14.560]   but also how good are you at doing that kind of thing.
[02:05:14.560 --> 02:05:16.720]   So that's a really hard problem to solve.
[02:05:16.720 --> 02:05:19.680]   - But I mean, but a lot of us have Alexas at home
[02:05:19.680 --> 02:05:24.120]   and I mean, Alexa could be listening in the whole time,
[02:05:24.120 --> 02:05:27.320]   right, and doing all sorts of nefarious things with the data.
[02:05:27.320 --> 02:05:32.480]   You know, hopefully it's not, but I don't think it is.
[02:05:32.480 --> 02:05:35.360]   - But Amazon is not, it's such a tricky thing
[02:05:35.360 --> 02:05:38.360]   for a company to get right, which is to earn the trust.
[02:05:38.360 --> 02:05:41.600]   I don't think Alexas earn people's trust quite yet.
[02:05:41.600 --> 02:05:44.880]   - Yeah, I think it's not there quite yet.
[02:05:44.880 --> 02:05:45.800]   I agree. - And they struggle
[02:05:45.800 --> 02:05:46.640]   with this kind of stuff.
[02:05:46.640 --> 02:05:48.400]   In fact, when these topics are brought up,
[02:05:48.400 --> 02:05:50.160]   people always get nervous.
[02:05:50.160 --> 02:05:53.640]   And I think if you get nervous about it,
[02:05:53.640 --> 02:05:57.680]   I mean, the way to earn people's trust
[02:05:57.680 --> 02:05:59.720]   is not by like, ooh, don't talk about this.
[02:05:59.720 --> 02:06:00.560]   (laughs)
[02:06:00.560 --> 02:06:03.600]   It's just be open, be frank, be transparent,
[02:06:03.600 --> 02:06:07.120]   and also create a culture of where it radiates
[02:06:08.000 --> 02:06:11.160]   at every level from engineer to CEO
[02:06:11.160 --> 02:06:16.160]   that like, you're good people that have a common sense idea
[02:06:16.160 --> 02:06:20.960]   of what it means to respect basic human rights
[02:06:20.960 --> 02:06:23.880]   and the privacy of people and all that kind of stuff.
[02:06:23.880 --> 02:06:26.500]   And I think that propagates throughout the,
[02:06:26.500 --> 02:06:29.960]   that's the best PR, which is like,
[02:06:29.960 --> 02:06:33.800]   over time you understand that these are good folks
[02:06:33.800 --> 02:06:35.280]   doing good things.
[02:06:35.280 --> 02:06:37.900]   Anyway, speaking of social robots,
[02:06:37.900 --> 02:06:42.960]   have you heard about Tesla, Tesla Bot, the humanoid robot?
[02:06:42.960 --> 02:06:43.800]   - Yes, I have.
[02:06:43.800 --> 02:06:46.240]   Yes, yes, yes, but I don't exactly know
[02:06:46.240 --> 02:06:48.320]   what it's designed to do.
[02:06:48.320 --> 02:06:49.140]   Do you? - Oh.
[02:06:49.140 --> 02:06:50.240]   - You probably do.
[02:06:50.240 --> 02:06:51.960]   - No, I know it's designed to do,
[02:06:51.960 --> 02:06:53.760]   but I have a different perspective on it.
[02:06:53.760 --> 02:06:57.540]   But it's designed to, it's a humanoid form,
[02:06:57.540 --> 02:07:01.340]   and it's designed to, for automation tasks
[02:07:01.340 --> 02:07:04.920]   in the same way that industrial robot arms
[02:07:04.920 --> 02:07:06.200]   automate tasks in the factory.
[02:07:06.200 --> 02:07:08.100]   So it's designed to automate tasks in the factory.
[02:07:08.100 --> 02:07:10.840]   But I think that humanoid form,
[02:07:10.840 --> 02:07:12.580]   as we were talking about before,
[02:07:12.580 --> 02:07:19.560]   is one that we connect with as human beings.
[02:07:19.560 --> 02:07:21.280]   Anything legged, honestly,
[02:07:21.280 --> 02:07:23.200]   but the humanoid form especially,
[02:07:23.200 --> 02:07:26.760]   we anthropomorphize it most intensely.
[02:07:26.760 --> 02:07:30.680]   And so the possibility, to me, it's exciting to see
[02:07:30.680 --> 02:07:34.520]   both Atlas developed by Boston Dynamics
[02:07:34.520 --> 02:07:38.480]   and anyone, including Tesla,
[02:07:38.480 --> 02:07:42.680]   trying to make humanoid robots cheaper and more effective.
[02:07:42.680 --> 02:07:46.040]   The obvious way it transforms the world
[02:07:46.040 --> 02:07:48.120]   is social robotics to me,
[02:07:48.120 --> 02:07:52.980]   versus automation of tasks in the factory.
[02:07:52.980 --> 02:07:54.760]   So yeah, I just wanted,
[02:07:54.760 --> 02:07:56.280]   in case that was something you were interested in,
[02:07:56.280 --> 02:07:59.520]   'cause I find its application
[02:07:59.520 --> 02:08:01.560]   in social robotics super interesting.
[02:08:01.560 --> 02:08:04.480]   - We did a lot of work with Pepper.
[02:08:04.480 --> 02:08:06.320]   Pepper the Robot a while back.
[02:08:06.320 --> 02:08:08.680]   We were like the emotion engine for Pepper,
[02:08:08.680 --> 02:08:11.360]   which is SoftBank's humanoid robot.
[02:08:11.360 --> 02:08:12.480]   - And how tall is Pepper?
[02:08:12.480 --> 02:08:13.400]   It's like--
[02:08:13.400 --> 02:08:18.400]   - Yeah, like, I don't know, like five foot maybe, right?
[02:08:18.400 --> 02:08:19.280]   - Yeah.
[02:08:19.280 --> 02:08:21.120]   - Yeah, pretty big, pretty big.
[02:08:21.120 --> 02:08:25.460]   And it was designed to be at like airport lounges
[02:08:25.460 --> 02:08:30.300]   and retail stores, mostly customer service, right?
[02:08:30.300 --> 02:08:31.600]   Hotel lobbies.
[02:08:34.000 --> 02:08:36.720]   And I mean, I don't know where the state of the robot is,
[02:08:36.720 --> 02:08:37.880]   but I think it's very promising.
[02:08:37.880 --> 02:08:39.480]   I think there are a lot of applications
[02:08:39.480 --> 02:08:40.400]   where this can be helpful.
[02:08:40.400 --> 02:08:43.160]   I'm also really interested in, yeah,
[02:08:43.160 --> 02:08:45.120]   social robotics for the home, right?
[02:08:45.120 --> 02:08:48.160]   Like that can help elderly people, for example,
[02:08:48.160 --> 02:08:51.600]   transport things from one location of the home to the other,
[02:08:51.600 --> 02:08:55.600]   or even like just have your back in case something happens.
[02:08:55.600 --> 02:08:57.760]   Yeah, I don't know.
[02:08:57.760 --> 02:08:59.800]   I do think it's a very interesting space.
[02:08:59.800 --> 02:09:00.800]   It seems early though.
[02:09:00.800 --> 02:09:03.580]   Do you feel like the timing is now?
[02:09:04.580 --> 02:09:08.580]   - I, yes, 100%.
[02:09:08.580 --> 02:09:11.900]   So it always seems early until it's not, right?
[02:09:11.900 --> 02:09:12.740]   - Right, right, right, right.
[02:09:12.740 --> 02:09:14.860]   - I think the time, well,
[02:09:14.860 --> 02:09:21.340]   I definitely think that the time is now,
[02:09:21.340 --> 02:09:25.740]   like this decade for social robots.
[02:09:25.740 --> 02:09:28.580]   Whether the humanoid form is right, I don't think so.
[02:09:28.580 --> 02:09:29.740]   - Mm-hmm, mm-hmm.
[02:09:29.740 --> 02:09:31.940]   - I don't, I think the,
[02:09:32.900 --> 02:09:37.260]   like if we just look at Jibo as an example,
[02:09:37.260 --> 02:09:41.720]   I feel like most of the problem,
[02:09:41.720 --> 02:09:44.780]   the challenge, the opportunity of social connection
[02:09:44.780 --> 02:09:48.300]   between an AI system and a human being
[02:09:48.300 --> 02:09:51.460]   does not require you to also solve the problem
[02:09:51.460 --> 02:09:55.220]   of robot manipulation and bipedal mobility.
[02:09:55.220 --> 02:09:59.100]   So I think you could do that with just a screen, honestly.
[02:09:59.100 --> 02:10:00.940]   But there's something about the interface of Jibo
[02:10:00.940 --> 02:10:03.700]   and you can rotate and so on that's also compelling.
[02:10:03.700 --> 02:10:06.340]   But you get to see all these robot companies
[02:10:06.340 --> 02:10:10.660]   that fail, incredible companies like Jibo and even,
[02:10:10.660 --> 02:10:16.500]   I mean, the iRobot in some sense is a big success story
[02:10:16.500 --> 02:10:18.620]   that it was able to find--
[02:10:18.620 --> 02:10:20.420]   - Right, a niche.
[02:10:20.420 --> 02:10:21.780]   - A niche thing and focus on it.
[02:10:21.780 --> 02:10:23.780]   But in some sense, it's not a success story
[02:10:23.780 --> 02:10:28.020]   because they didn't build any other robot.
[02:10:28.020 --> 02:10:30.300]   Like any other, it didn't expand
[02:10:30.300 --> 02:10:31.500]   to all kinds of robotics.
[02:10:31.500 --> 02:10:32.740]   Like once you're in the home,
[02:10:32.740 --> 02:10:34.300]   maybe that's what happens with Amazon
[02:10:34.300 --> 02:10:36.860]   is they'll flourish into all kinds of other robots.
[02:10:36.860 --> 02:10:40.460]   But do you have a sense, by the way,
[02:10:40.460 --> 02:10:43.700]   why it's so difficult to build a robotics company?
[02:10:43.700 --> 02:10:47.140]   Like why so many companies have failed?
[02:10:47.140 --> 02:10:50.740]   - I think it's like you're building a vertical stack, right?
[02:10:50.740 --> 02:10:53.220]   Like you are building the hardware plus the software
[02:10:53.220 --> 02:10:56.020]   and you find you have to do this at a cost that makes sense.
[02:10:56.020 --> 02:11:01.020]   So I think Jibo was retailing at like, I don't know,
[02:11:01.020 --> 02:11:03.300]   like $800, like $700, $800.
[02:11:03.300 --> 02:11:04.660]   - Yeah, something like this, yeah.
[02:11:04.660 --> 02:11:07.140]   - Which for the use case, right?
[02:11:07.140 --> 02:11:09.340]   There's a dissonance there.
[02:11:09.340 --> 02:11:10.820]   It's too high.
[02:11:10.820 --> 02:11:13.420]   So I think cost is, you know,
[02:11:13.420 --> 02:11:17.780]   cost of building the whole platform in a way that is,
[02:11:17.780 --> 02:11:20.980]   yeah, that is affordable for what value it's bringing.
[02:11:20.980 --> 02:11:22.460]   I think that's the challenge.
[02:11:22.460 --> 02:11:25.060]   I think for these home robots
[02:11:25.060 --> 02:11:28.500]   that are gonna help you do stuff around the home,
[02:11:28.500 --> 02:11:33.260]   that's a challenge too, like the mobility piece of it.
[02:11:33.260 --> 02:11:34.860]   That's hard.
[02:11:34.860 --> 02:11:37.740]   - Well, one of the things I'm really excited with TeslaBot
[02:11:37.740 --> 02:11:40.340]   is the people working on it.
[02:11:40.340 --> 02:11:42.900]   And that's probably the criticism I would apply
[02:11:42.900 --> 02:11:45.900]   to some of the other folks who worked on social robots
[02:11:45.900 --> 02:11:49.460]   is the people working on TeslaBot know how to,
[02:11:49.460 --> 02:11:52.260]   they're focused on and know how to do mass manufacture
[02:11:52.260 --> 02:11:54.060]   and create a product that's super cheap.
[02:11:54.060 --> 02:11:55.460]   - Very cool. - That's the focus.
[02:11:55.460 --> 02:11:57.220]   The engineering focus is not,
[02:11:57.220 --> 02:12:00.180]   I would say that you can also criticize them for that,
[02:12:00.180 --> 02:12:03.780]   is they're not focused on the experience of the robot.
[02:12:03.780 --> 02:12:06.860]   They're focused on how to get this thing
[02:12:06.860 --> 02:12:11.260]   to do the basic stuff that the humanoid form requires
[02:12:11.260 --> 02:12:13.380]   to do it as cheap as possible.
[02:12:13.380 --> 02:12:15.140]   Then the fewest number of actuators,
[02:12:15.140 --> 02:12:16.460]   the fewest numbers of motors,
[02:12:16.460 --> 02:12:18.300]   the increasing efficiency,
[02:12:18.300 --> 02:12:20.060]   they decrease the weight, all that kind of stuff.
[02:12:20.060 --> 02:12:21.620]   - So that's really interesting.
[02:12:21.620 --> 02:12:24.460]   - I would say that Jibo and all those folks,
[02:12:24.460 --> 02:12:27.860]   they focus on the design, the experience, all of that.
[02:12:27.860 --> 02:12:30.460]   And it's secondary how to manufacture.
[02:12:30.460 --> 02:12:33.700]   No, you have to think like the TeslaBot folks
[02:12:33.700 --> 02:12:35.340]   from first principles,
[02:12:35.340 --> 02:12:38.020]   what is the fewest number of components,
[02:12:38.020 --> 02:12:38.980]   the cheapest components?
[02:12:38.980 --> 02:12:41.820]   How can I build it as much in-house as possible
[02:12:41.820 --> 02:12:45.860]   without having to consider all the complexities
[02:12:45.860 --> 02:12:47.940]   of a supply chain, all that kind of stuff.
[02:12:47.940 --> 02:12:48.780]   - Interesting.
[02:12:48.780 --> 02:12:50.700]   - 'Cause if you have to build a robotics company,
[02:12:50.700 --> 02:12:53.780]   you have to, you're not building one robot.
[02:12:53.780 --> 02:12:55.580]   You're building hopefully millions of robots
[02:12:55.580 --> 02:12:57.540]   and you have to figure out how to do that.
[02:12:57.540 --> 02:13:01.740]   Where the final thing, I mean, if it's Jibo type of robot,
[02:13:01.740 --> 02:13:03.700]   is there a reason why Jibo,
[02:13:03.700 --> 02:13:05.980]   like we're gonna have this lengthy discussion,
[02:13:05.980 --> 02:13:08.820]   is there a reason why Jibo has to be over $100?
[02:13:08.820 --> 02:13:09.860]   - It shouldn't be.
[02:13:09.860 --> 02:13:12.460]   - Right, like the basic components.
[02:13:12.460 --> 02:13:13.940]   - Components of it, right.
[02:13:13.940 --> 02:13:16.660]   - Like you could start to actually discuss like,
[02:13:16.660 --> 02:13:18.980]   okay, what is the essential thing about Jibo?
[02:13:18.980 --> 02:13:21.300]   How much, what is the cheapest way I can have a screen?
[02:13:21.300 --> 02:13:24.020]   What's the cheapest way I can have a rotating base?
[02:13:24.020 --> 02:13:24.860]   - Right. - All that kind of stuff.
[02:13:24.860 --> 02:13:29.540]   And then you get down, continuously drive down cost.
[02:13:29.540 --> 02:13:32.940]   Speaking of which, you have launched
[02:13:32.940 --> 02:13:34.660]   an extremely successful companies.
[02:13:34.660 --> 02:13:37.860]   You have helped others, you've invested in companies.
[02:13:37.860 --> 02:13:42.860]   Can you give advice on how to start a successful company?
[02:13:42.860 --> 02:13:46.420]   - I would say have a problem
[02:13:46.420 --> 02:13:48.620]   that you really, really, really wanna solve, right?
[02:13:48.620 --> 02:13:51.420]   Something that you're deeply passionate about.
[02:13:51.420 --> 02:13:55.980]   And honestly, take the first step.
[02:13:55.980 --> 02:13:59.540]   Like that's often the hardest, and don't overthink it.
[02:13:59.540 --> 02:14:02.660]   Like, you know, like this idea of a minimum viable product
[02:14:02.660 --> 02:14:05.140]   or a minimum viable version of an idea, right?
[02:14:05.140 --> 02:14:06.660]   Like, yes, you're thinking about this,
[02:14:06.660 --> 02:14:10.580]   like a humongous, like super elegant, super beautiful thing.
[02:14:10.580 --> 02:14:12.860]   What, like reduce it to the littlest thing
[02:14:12.860 --> 02:14:14.860]   you can bring to market that can solve a problem
[02:14:14.860 --> 02:14:19.860]   that can help address a pain point that somebody has.
[02:14:19.860 --> 02:14:22.620]   They often tell you, like,
[02:14:22.620 --> 02:14:24.340]   start with a customer of one, right?
[02:14:24.340 --> 02:14:26.660]   If you can solve a problem for one person,
[02:14:26.660 --> 02:14:28.580]   then there's probably-- - Yourself
[02:14:28.580 --> 02:14:29.780]   or some other person. - Right.
[02:14:29.780 --> 02:14:31.580]   - Pick a person. - Exactly.
[02:14:31.580 --> 02:14:32.820]   - It could be you. - Yeah.
[02:14:32.820 --> 02:14:34.460]   - That's actually often a good sign
[02:14:34.460 --> 02:14:36.460]   that if you enjoy a thing. - Right.
[02:14:36.460 --> 02:14:38.340]   - Enjoy a thing where you have a specific problem
[02:14:38.340 --> 02:14:39.500]   that you'd like to solve.
[02:14:39.500 --> 02:14:43.020]   That's a good end of one to focus on.
[02:14:43.020 --> 02:14:44.140]   - Right. - What else?
[02:14:44.140 --> 02:14:45.620]   What else is there to actually,
[02:14:45.620 --> 02:14:47.900]   so step one is the hardest, but how do you,
[02:14:47.900 --> 02:14:49.100]   (Roz laughs)
[02:14:49.100 --> 02:14:51.540]   there's other steps as well, right?
[02:14:51.540 --> 02:14:56.140]   - I also think, like, who you bring around the table
[02:14:56.140 --> 02:14:58.340]   early on is so key, right?
[02:14:58.340 --> 02:15:00.660]   Like being clear on what I call, like,
[02:15:00.660 --> 02:15:02.460]   your core values or your North Star.
[02:15:02.460 --> 02:15:04.820]   It might sound fluffy, but actually it's not.
[02:15:04.820 --> 02:15:08.900]   So, and Roz and I, I feel like we did that very early on.
[02:15:08.900 --> 02:15:11.460]   We sat around her kitchen table and we said,
[02:15:11.460 --> 02:15:13.660]   okay, there's so many applications of this technology.
[02:15:13.660 --> 02:15:15.140]   How are we gonna draw the line?
[02:15:15.140 --> 02:15:16.940]   How are we gonna set boundaries?
[02:15:16.940 --> 02:15:19.220]   We came up with a set of core values
[02:15:19.220 --> 02:15:21.260]   that in the hardest of times,
[02:15:21.260 --> 02:15:25.260]   we fell back on to determine how we make decisions.
[02:15:25.260 --> 02:15:27.620]   And so I feel like just getting clarity on these core,
[02:15:27.620 --> 02:15:30.500]   like for us, it was respecting people's privacy,
[02:15:30.500 --> 02:15:33.660]   only engaging with industries where it's clear opt-ins.
[02:15:33.660 --> 02:15:35.580]   So for instance, we don't do any work
[02:15:35.580 --> 02:15:37.100]   in security and surveillance.
[02:15:37.100 --> 02:15:40.060]   So things like that, just getting,
[02:15:40.060 --> 02:15:42.500]   we very big on, you know, one of our core values
[02:15:42.500 --> 02:15:44.620]   is human connection and empathy, right?
[02:15:44.620 --> 02:15:46.540]   And that is, yes, it's an AI company,
[02:15:46.540 --> 02:15:47.940]   but it's about people.
[02:15:47.940 --> 02:15:52.740]   Well, these are all, they become encoded in how we act,
[02:15:52.740 --> 02:15:55.460]   even if you're a small, tiny team of two or three
[02:15:55.460 --> 02:15:56.300]   or whatever.
[02:15:56.300 --> 02:15:59.980]   So I think that's another piece of advice.
[02:15:59.980 --> 02:16:02.700]   - So what about finding people, hiring people?
[02:16:02.700 --> 02:16:04.780]   If you care about people as much as you do,
[02:16:04.780 --> 02:16:08.180]   like it seems like such a difficult thing
[02:16:08.180 --> 02:16:10.940]   to hire the right people.
[02:16:10.940 --> 02:16:12.500]   - I think early on as a startup,
[02:16:12.500 --> 02:16:14.540]   you want people who have,
[02:16:14.540 --> 02:16:16.340]   who share the passion and the conviction
[02:16:16.340 --> 02:16:17.900]   because it's gonna be tough.
[02:16:17.900 --> 02:16:21.780]   Like I've yet to meet a startup
[02:16:21.780 --> 02:16:24.580]   where it was just a straight line to success, right?
[02:16:24.580 --> 02:16:25.940]   Even not just startup,
[02:16:25.940 --> 02:16:28.180]   like even in everyday people's lives, right?
[02:16:28.180 --> 02:16:31.580]   So you always like run into obstacles
[02:16:31.580 --> 02:16:33.060]   and you run into naysayers.
[02:16:33.060 --> 02:16:37.700]   And so you need people who are believers,
[02:16:37.700 --> 02:16:40.660]   whether they're people on your team or even your investors,
[02:16:40.660 --> 02:16:42.700]   you need investors who are really believers
[02:16:42.700 --> 02:16:44.140]   in what you're doing
[02:16:44.140 --> 02:16:46.100]   because that means they will stick with you.
[02:16:46.100 --> 02:16:49.380]   They won't give up at the first obstacle.
[02:16:49.380 --> 02:16:50.900]   I think that's important.
[02:16:50.900 --> 02:16:52.660]   - What about raising money?
[02:16:52.660 --> 02:16:55.340]   What about finding investors?
[02:16:55.340 --> 02:16:58.380]   First of all, raising money,
[02:16:58.380 --> 02:17:01.580]   but also raising money from the right sources
[02:17:01.580 --> 02:17:03.540]   from that ultimately don't hinder you,
[02:17:03.540 --> 02:17:07.060]   but help you, empower you, all that kind of stuff.
[02:17:07.060 --> 02:17:08.580]   What advice would you give there?
[02:17:08.580 --> 02:17:12.300]   You successfully raised money many times in your life.
[02:17:12.300 --> 02:17:15.060]   - Yeah, again, it's not just about the money.
[02:17:15.060 --> 02:17:17.220]   It's about finding the right investors
[02:17:17.220 --> 02:17:21.300]   who are going to be aligned in terms of what you wanna build
[02:17:21.300 --> 02:17:23.060]   and believe in your core values.
[02:17:23.060 --> 02:17:25.900]   Like for example, especially later on,
[02:17:25.900 --> 02:17:29.860]   like I, yeah, in my latest like round of funding,
[02:17:29.860 --> 02:17:31.420]   I try to bring in investors
[02:17:31.420 --> 02:17:35.460]   that really care about like the ethics of AI, right?
[02:17:35.460 --> 02:17:39.620]   And the alignment of vision and mission
[02:17:39.620 --> 02:17:41.020]   and core values is really important.
[02:17:41.020 --> 02:17:43.900]   It's like you're picking a life partner, right?
[02:17:43.900 --> 02:17:45.180]   It's the same kind of-
[02:17:45.180 --> 02:17:47.460]   - So you take it that seriously for investors?
[02:17:47.460 --> 02:17:50.140]   - Yeah, because they're gonna have to stick with you.
[02:17:50.140 --> 02:17:51.540]   - You're stuck together.
[02:17:51.540 --> 02:17:53.100]   - For a while anyway, yeah.
[02:17:53.100 --> 02:17:54.260]   (both laughing)
[02:17:54.260 --> 02:17:56.860]   Maybe not for life, but for a while, for sure.
[02:17:56.860 --> 02:17:57.820]   - For better or worse.
[02:17:57.820 --> 02:17:59.940]   I forget what the vowels usually sound like.
[02:17:59.940 --> 02:18:00.780]   For better or worse?
[02:18:00.780 --> 02:18:01.620]   No.
[02:18:01.620 --> 02:18:02.780]   - Through sick.
[02:18:02.780 --> 02:18:03.620]   - Through sick and then-
[02:18:03.620 --> 02:18:04.620]   - Through something.
[02:18:04.620 --> 02:18:05.460]   - Yeah, yeah, yeah.
[02:18:05.460 --> 02:18:07.580]   (both laughing)
[02:18:07.580 --> 02:18:09.220]   - Oh boy.
[02:18:09.220 --> 02:18:11.820]   Yeah, anyway, it's romantic and deep
[02:18:11.820 --> 02:18:13.820]   and you're in it for a while.
[02:18:13.820 --> 02:18:16.900]   So it's not just about the money.
[02:18:16.900 --> 02:18:20.340]   You tweeted about going to your first
[02:18:20.340 --> 02:18:22.340]   capital camp investing get together.
[02:18:22.340 --> 02:18:23.180]   - Oh yeah.
[02:18:23.180 --> 02:18:24.020]   - And then you learned a lot.
[02:18:24.020 --> 02:18:27.780]   So this is about investing.
[02:18:27.780 --> 02:18:29.900]   So what have you learned from that?
[02:18:29.900 --> 02:18:32.740]   What have you learned about investing in general?
[02:18:32.740 --> 02:18:35.340]   From both, because you've been on both ends of it.
[02:18:35.340 --> 02:18:38.260]   - I mean, I try to use my experience as an operator
[02:18:38.260 --> 02:18:40.780]   now with my investor hat on
[02:18:40.780 --> 02:18:44.060]   when I'm identifying companies to invest in.
[02:18:44.060 --> 02:18:47.020]   First of all, I think the good news
[02:18:47.020 --> 02:18:49.100]   is because I have a technology background, right?
[02:18:49.100 --> 02:18:51.220]   And I really understand machine learning
[02:18:51.220 --> 02:18:53.340]   and computer vision and AI, et cetera.
[02:18:53.340 --> 02:18:56.580]   I can apply that level of understanding, right?
[02:18:56.580 --> 02:18:58.620]   'Cause everybody says they're an AI company
[02:18:58.620 --> 02:18:59.780]   or they're an AI tech.
[02:18:59.780 --> 02:19:01.940]   And I'm like, no, no, no, no, no.
[02:19:01.940 --> 02:19:02.900]   Show me the technology.
[02:19:02.900 --> 02:19:05.860]   So I can do that level of diligence, which I actually love.
[02:19:05.860 --> 02:19:10.700]   And then I have to do the litmus test of,
[02:19:10.700 --> 02:19:12.180]   if I'm in a conversation with you,
[02:19:12.180 --> 02:19:14.340]   am I excited to tell you about this new company
[02:19:14.340 --> 02:19:15.580]   that I just met, right?
[02:19:15.580 --> 02:19:20.380]   And if I'm an ambassador for that company
[02:19:20.380 --> 02:19:22.140]   and I'm passionate about what they're doing,
[02:19:22.140 --> 02:19:24.900]   I usually use that.
[02:19:24.900 --> 02:19:28.260]   Yeah, that's important to me when I'm investing.
[02:19:28.260 --> 02:19:32.420]   - So that means you actually can explain what they're doing
[02:19:32.420 --> 02:19:34.340]   and you're excited about it.
[02:19:34.340 --> 02:19:36.140]   - Exactly, exactly.
[02:19:36.140 --> 02:19:39.940]   Thank you for putting it so succinctly.
[02:19:39.940 --> 02:19:41.700]   Like rambling, but exactly that's it.
[02:19:41.700 --> 02:19:43.900]   I understand it and I'm excited about it.
[02:19:43.900 --> 02:19:46.740]   - It's funny, but sometimes it's unclear exactly.
[02:19:46.740 --> 02:19:50.420]   I'll hear people tell me,
[02:19:50.420 --> 02:19:52.860]   they'll talk for a while and it sounds cool.
[02:19:52.860 --> 02:19:54.580]   Like they paint a picture of a world,
[02:19:54.580 --> 02:19:56.500]   but then when you try to summarize it,
[02:19:56.500 --> 02:19:59.260]   you're not exactly clear of what,
[02:19:59.260 --> 02:20:03.460]   maybe what the core powerful idea is.
[02:20:03.460 --> 02:20:05.300]   You can't just build another Facebook
[02:20:05.300 --> 02:20:10.300]   or there has to be a core simple to explain idea
[02:20:10.300 --> 02:20:15.780]   that then you can or can't get excited about,
[02:20:15.780 --> 02:20:17.820]   but it's there, it's sitting right there.
[02:20:17.820 --> 02:20:25.540]   How do you ultimately pick who you think will be successful?
[02:20:25.540 --> 02:20:27.980]   It's not just about the thing you're excited about,
[02:20:27.980 --> 02:20:29.540]   like there's other stuff.
[02:20:29.540 --> 02:20:31.900]   - Right, and then there's all the,
[02:20:31.900 --> 02:20:34.500]   with early stage companies, like pre-seed companies,
[02:20:34.500 --> 02:20:36.180]   which is where I'm investing,
[02:20:36.180 --> 02:20:40.420]   sometimes the business model isn't clear yet
[02:20:40.420 --> 02:20:42.260]   or the go-to-market strategy isn't clear.
[02:20:42.260 --> 02:20:43.900]   There's usually, like it's very early on
[02:20:43.900 --> 02:20:46.500]   that some of these things haven't been hashed out,
[02:20:46.500 --> 02:20:47.820]   which is okay.
[02:20:47.820 --> 02:20:49.500]   So the way I like to think about it is like,
[02:20:49.500 --> 02:20:51.060]   if this company's successful,
[02:20:51.060 --> 02:20:54.580]   will this be a multi-billion/trillion dollar market?
[02:20:55.580 --> 02:20:56.420]   Or company.
[02:20:56.420 --> 02:21:00.460]   And so that's definitely a lens that I use.
[02:21:00.460 --> 02:21:02.020]   - What's pre-seed?
[02:21:02.020 --> 02:21:03.900]   What are the different stages?
[02:21:03.900 --> 02:21:05.660]   And what's the most exciting stage?
[02:21:05.660 --> 02:21:09.580]   What's interesting about every stage, I guess?
[02:21:09.580 --> 02:21:13.980]   - Yeah, so pre-seed is usually when you're just starting out,
[02:21:13.980 --> 02:21:16.500]   you've maybe raised the friends and family round,
[02:21:16.500 --> 02:21:18.500]   so you've raised some money from people you know,
[02:21:18.500 --> 02:21:19.380]   and you're getting ready
[02:21:19.380 --> 02:21:22.540]   to take your first institutional check-in,
[02:21:22.540 --> 02:21:25.100]   like first check from an investor.
[02:21:25.100 --> 02:21:28.820]   And I love the stage.
[02:21:28.820 --> 02:21:30.700]   There's a lot of uncertainty.
[02:21:30.700 --> 02:21:32.540]   Some investors really don't like the stage
[02:21:32.540 --> 02:21:36.460]   because the financial models aren't there.
[02:21:36.460 --> 02:21:38.740]   Often the teams aren't even like formed,
[02:21:38.740 --> 02:21:40.220]   it's really, really early.
[02:21:40.220 --> 02:21:45.860]   But to me, it's like a magical stage
[02:21:45.860 --> 02:21:48.420]   because it's the time when there's so much conviction,
[02:21:48.420 --> 02:21:50.660]   so much belief, almost delusional, right?
[02:21:51.660 --> 02:21:53.980]   And there's a little bit of naivete around
[02:21:53.980 --> 02:21:57.900]   with founders at the stage, and I just love it.
[02:21:57.900 --> 02:21:59.020]   It's contagious.
[02:21:59.020 --> 02:22:02.660]   And I love that I can,
[02:22:02.660 --> 02:22:06.020]   often they're first-time founders, not always,
[02:22:06.020 --> 02:22:07.660]   but often they're first-time founders,
[02:22:07.660 --> 02:22:10.140]   and I can share my experience as a founder myself,
[02:22:10.140 --> 02:22:12.580]   and I can empathize, right?
[02:22:12.580 --> 02:22:17.180]   And I can almost, I create a safe ground where,
[02:22:17.180 --> 02:22:18.620]   'cause you have to be careful
[02:22:18.620 --> 02:22:21.180]   what you tell your investors, right?
[02:22:21.180 --> 02:22:23.140]   And I will often say,
[02:22:23.140 --> 02:22:24.780]   I've been in your shoes as a founder,
[02:22:24.780 --> 02:22:26.780]   you can tell me if it's challenging,
[02:22:26.780 --> 02:22:28.340]   you can tell me what you're struggling with.
[02:22:28.340 --> 02:22:30.140]   It's okay to vent.
[02:22:30.140 --> 02:22:31.940]   So I create that safe ground,
[02:22:31.940 --> 02:22:35.300]   and I think that's a superpower.
[02:22:35.300 --> 02:22:37.980]   - Yeah, you have to, I guess,
[02:22:37.980 --> 02:22:39.860]   you have to figure out if this kind of person
[02:22:39.860 --> 02:22:42.020]   is gonna be able to ride the roller coaster
[02:22:42.020 --> 02:22:46.980]   like of many pivots and challenges
[02:22:46.980 --> 02:22:48.100]   and all that kind of stuff.
[02:22:48.100 --> 02:22:50.860]   And if the space of ideas they're working in
[02:22:50.860 --> 02:22:54.180]   is interesting, like the way they think about the world.
[02:22:54.180 --> 02:22:57.660]   Yeah, 'cause if it's successful,
[02:22:57.660 --> 02:22:59.700]   the thing they end up with might be very different,
[02:22:59.700 --> 02:23:01.500]   the reason it's successful for.
[02:23:01.500 --> 02:23:05.140]   - Actually, I was gonna say the third,
[02:23:05.140 --> 02:23:07.340]   so the technology is one aspect,
[02:23:07.340 --> 02:23:09.940]   the market or the idea, right, is the second,
[02:23:09.940 --> 02:23:11.260]   and the third is the founder, right?
[02:23:11.260 --> 02:23:13.940]   Is this somebody who I believe has conviction,
[02:23:13.940 --> 02:23:18.940]   is a hustler, is gonna overcome obstacles.
[02:23:20.660 --> 02:23:23.140]   Yeah, I think that it's gonna be a great leader, right?
[02:23:23.140 --> 02:23:25.740]   Like as a startup, as a founder,
[02:23:25.740 --> 02:23:27.660]   you're often, you are the first person,
[02:23:27.660 --> 02:23:31.060]   and your role is to bring amazing people around you
[02:23:31.060 --> 02:23:32.780]   to build this thing.
[02:23:32.780 --> 02:23:36.140]   And so you're an evangelist, right?
[02:23:36.140 --> 02:23:38.020]   So how good are you gonna be at that?
[02:23:38.020 --> 02:23:40.260]   So I try to evaluate that too.
[02:23:40.260 --> 02:23:43.740]   - You also, in the tweet thread about it,
[02:23:43.740 --> 02:23:45.780]   mentioned, is this a known concept?
[02:23:45.780 --> 02:23:48.860]   Random rich dudes, RRDS.
[02:23:48.940 --> 02:23:53.180]   Saying that there should be like random rich women, I guess.
[02:23:53.180 --> 02:23:56.500]   What's the dudes version of women,
[02:23:56.500 --> 02:23:57.820]   the women version of dudes?
[02:23:57.820 --> 02:23:59.380]   Ladies, I don't know.
[02:23:59.380 --> 02:24:01.500]   What's, is this a technical term?
[02:24:01.500 --> 02:24:02.740]   Is this known?
[02:24:02.740 --> 02:24:03.580]   Random rich dudes?
[02:24:03.580 --> 02:24:04.540]   - Well, I didn't make that up,
[02:24:04.540 --> 02:24:07.100]   but I was at this capital camp,
[02:24:07.100 --> 02:24:11.260]   which is a get together for investors of all types,
[02:24:11.260 --> 02:24:15.660]   and there must have been maybe 400 or so attendees,
[02:24:17.380 --> 02:24:19.060]   maybe 20 were women.
[02:24:19.060 --> 02:24:21.480]   It was just very disproportionately,
[02:24:21.480 --> 02:24:25.700]   a male dominated, which I'm used to.
[02:24:25.700 --> 02:24:27.020]   - I think you're used to this kind of thing.
[02:24:27.020 --> 02:24:29.700]   - I'm used to it, but it's still surprising.
[02:24:29.700 --> 02:24:31.820]   And as I'm raising money for this fund,
[02:24:31.820 --> 02:24:36.140]   so my fund partner is a guy called Rob May,
[02:24:36.140 --> 02:24:37.580]   who's done this before.
[02:24:37.580 --> 02:24:39.500]   So I'm new to the investing world,
[02:24:39.500 --> 02:24:41.620]   but he's done this before.
[02:24:41.620 --> 02:24:45.180]   Most of our investors in the fund are these,
[02:24:45.180 --> 02:24:47.420]   I mean, awesome, I'm super grateful to them,
[02:24:47.420 --> 02:24:48.860]   random, just rich guys.
[02:24:48.860 --> 02:24:50.340]   I'm like, where are the rich women?
[02:24:50.340 --> 02:24:52.820]   So I'm really adamant in both investing
[02:24:52.820 --> 02:24:56.140]   in women-led AI companies,
[02:24:56.140 --> 02:24:58.900]   but I also would love to have women investors
[02:24:58.900 --> 02:25:00.740]   be part of my fund,
[02:25:00.740 --> 02:25:03.780]   because I think that's how we drive change.
[02:25:03.780 --> 02:25:06.640]   - Yeah, so then that takes time, of course,
[02:25:06.640 --> 02:25:08.560]   but there's been quite a lot of progress,
[02:25:08.560 --> 02:25:11.900]   but yeah, for the next Mark Zuckerberg to be a woman
[02:25:11.900 --> 02:25:12.860]   and all that kind of stuff,
[02:25:13.500 --> 02:25:17.980]   that's just a huge number of wealth generated by women,
[02:25:17.980 --> 02:25:19.180]   and then controlled by women,
[02:25:19.180 --> 02:25:22.060]   then allocated by women, and all that kind of stuff.
[02:25:22.060 --> 02:25:23.540]   And then beyond just women,
[02:25:23.540 --> 02:25:25.900]   just broadly across all different measures
[02:25:25.900 --> 02:25:27.320]   of diversity and so on.
[02:25:27.320 --> 02:25:35.500]   Let me ask you to put on your wise sage hat.
[02:25:35.500 --> 02:25:38.340]   So you already gave advice on startups,
[02:25:38.340 --> 02:25:43.340]   and just advice for women,
[02:25:43.340 --> 02:25:46.980]   but in general, advice for folks in high school
[02:25:46.980 --> 02:25:50.700]   or college today, how to have a career they can be proud of,
[02:25:50.700 --> 02:25:53.860]   how to have a life they can be proud of.
[02:25:53.860 --> 02:25:58.060]   I suppose you have to give this kind of advice to your kids.
[02:25:58.060 --> 02:25:59.140]   - Kids, yeah.
[02:25:59.140 --> 02:26:03.340]   Well, here's the number one advice that I give to my kids.
[02:26:03.340 --> 02:26:04.980]   My daughter's now 19, by the way,
[02:26:04.980 --> 02:26:06.380]   and my son's 13 and a half,
[02:26:06.380 --> 02:26:08.940]   so they're not little kids anymore.
[02:26:08.940 --> 02:26:11.500]   - Does it break your heart?
[02:26:11.500 --> 02:26:12.700]   - It does.
[02:26:12.700 --> 02:26:14.260]   Like a girl, but they're awesome.
[02:26:14.260 --> 02:26:15.460]   They're my best friends.
[02:26:15.460 --> 02:26:19.020]   Yeah, I think the number one advice I would share
[02:26:19.020 --> 02:26:22.380]   is embark on a journey without attaching to outcomes,
[02:26:22.380 --> 02:26:25.100]   and enjoy the journey, right?
[02:26:25.100 --> 02:26:29.560]   So we often were so obsessed with the end goal,
[02:26:29.560 --> 02:26:33.860]   A, that doesn't allow us to be open
[02:26:33.860 --> 02:26:36.100]   to different endings of a journey,
[02:26:36.100 --> 02:26:37.020]   or a story.
[02:26:37.020 --> 02:26:41.780]   So you become so fixated on a particular path.
[02:26:41.780 --> 02:26:44.660]   You don't see the beauty in the other alternative path.
[02:26:44.660 --> 02:26:48.620]   And then you forget to enjoy the journey
[02:26:48.620 --> 02:26:50.580]   because you're just so fixated on the goal.
[02:26:50.580 --> 02:26:54.100]   And I've been guilty of that for many, many years in my life.
[02:26:54.100 --> 02:26:57.860]   And I'm now trying to make the shift of,
[02:26:57.860 --> 02:27:00.300]   no, no, no, I'm gonna, again,
[02:27:00.300 --> 02:27:02.460]   trust that things are gonna work out,
[02:27:02.460 --> 02:27:05.540]   and it'll be amazing, and maybe even exceed your dreams.
[02:27:05.540 --> 02:27:07.300]   We have to be open to that.
[02:27:07.300 --> 02:27:09.860]   - Yeah, taking a leap into all kinds of things.
[02:27:09.860 --> 02:27:12.860]   I think you tweeted like you went on vacation by yourself
[02:27:12.860 --> 02:27:13.900]   or something like this.
[02:27:13.900 --> 02:27:14.740]   - I know.
[02:27:14.740 --> 02:27:19.380]   - And just going, just taking the leap, doing it.
[02:27:19.380 --> 02:27:20.220]   - Totally, doing it.
[02:27:20.220 --> 02:27:22.300]   - And enjoying it, enjoying the moment,
[02:27:22.300 --> 02:27:25.020]   enjoying the weeks, enjoying not looking at
[02:27:25.020 --> 02:27:29.540]   the some kind of career ladder next step and so on.
[02:27:29.540 --> 02:27:34.380]   Yeah, there's something to that, like over planning too.
[02:27:34.380 --> 02:27:36.380]   I'm surrounded by a lot of people that kinda,
[02:27:36.380 --> 02:27:37.740]   so I don't plan.
[02:27:37.740 --> 02:27:38.580]   - You don't?
[02:27:38.580 --> 02:27:39.420]   - No.
[02:27:39.420 --> 02:27:40.980]   - Do you not do goal setting?
[02:27:40.980 --> 02:27:46.620]   - My goal setting is very like,
[02:27:46.620 --> 02:27:49.420]   I like the affirmations, it's very,
[02:27:49.420 --> 02:27:53.520]   it's almost, I don't know how to put it into words,
[02:27:53.520 --> 02:27:55.700]   but it's a little bit like
[02:27:55.700 --> 02:28:01.580]   what my heart yearns for, kind of.
[02:28:01.580 --> 02:28:03.620]   And I guess in the space of emotions,
[02:28:03.620 --> 02:28:06.020]   more than in the space of like,
[02:28:06.020 --> 02:28:09.500]   this will be, like in the rational space.
[02:28:09.500 --> 02:28:13.620]   'Cause I just try to picture a world
[02:28:13.620 --> 02:28:15.780]   that I would like to be in,
[02:28:15.780 --> 02:28:17.580]   and that world is not clearly pictured,
[02:28:17.580 --> 02:28:19.380]   it's mostly in the emotional world.
[02:28:19.380 --> 02:28:22.060]   I mean, I think about that from robots,
[02:28:22.060 --> 02:28:27.060]   'cause I have this desire, I've had it my whole life to,
[02:28:27.060 --> 02:28:29.660]   well, it took different shapes,
[02:28:29.660 --> 02:28:32.380]   but I think once I discovered AI,
[02:28:32.380 --> 02:28:34.020]   the desire was to,
[02:28:34.020 --> 02:28:38.220]   I think in the context of this conversation
[02:28:38.220 --> 02:28:40.520]   could be easily, easier described
[02:28:40.520 --> 02:28:42.940]   as basically a social robotics company.
[02:28:42.940 --> 02:28:45.600]   And that's something I dreamed of doing.
[02:28:45.600 --> 02:28:52.060]   And, well, there's a lot of complexity to that story,
[02:28:52.060 --> 02:28:55.620]   but that's the only thing, honestly, I dream of doing.
[02:28:55.620 --> 02:29:00.620]   So I imagine a world that I could help create,
[02:29:01.580 --> 02:29:05.660]   but it's not, there's no steps along the way.
[02:29:05.660 --> 02:29:09.460]   And I think I'm just kind of stumbling around
[02:29:09.460 --> 02:29:13.260]   and following happiness and working my ass off
[02:29:13.260 --> 02:29:17.340]   in almost random, like an ant does in random directions.
[02:29:17.340 --> 02:29:19.420]   But a lot of people, a lot of successful people around me
[02:29:19.420 --> 02:29:21.540]   say this, you should have a plan, you should have a clear goal.
[02:29:21.540 --> 02:29:22.700]   You have a goal at the end of the month,
[02:29:22.700 --> 02:29:24.300]   you have a goal at the end of the year.
[02:29:24.300 --> 02:29:26.580]   I don't, I don't, I don't.
[02:29:26.580 --> 02:29:31.020]   And there's a balance to be struck, of course,
[02:29:31.020 --> 02:29:36.020]   but there's something to be said about really making sure
[02:29:36.020 --> 02:29:40.220]   that you're living life to the fullest,
[02:29:40.220 --> 02:29:42.620]   that goals can actually get in the way of.
[02:29:42.620 --> 02:29:47.300]   - So one of the best, like kind of most,
[02:29:47.300 --> 02:29:52.500]   what do you call it when it's like challenges your brain,
[02:29:52.500 --> 02:29:53.500]   what do you call it?
[02:29:53.500 --> 02:29:58.100]   - The only thing that comes to mind,
[02:29:58.100 --> 02:30:00.180]   and this is me saying is the mind fuck, but yes.
[02:30:00.180 --> 02:30:03.020]   - Okay, okay, maybe, okay, something like that.
[02:30:03.020 --> 02:30:03.940]   - Yes.
[02:30:03.940 --> 02:30:06.620]   - Super inspiring talk, Kenneth Stanley,
[02:30:06.620 --> 02:30:09.340]   he was at OpenAI, he just left,
[02:30:09.340 --> 02:30:12.460]   and he has a book called "Why Greatness Can't Be Planned."
[02:30:12.460 --> 02:30:14.100]   And it's actually an AI book.
[02:30:14.100 --> 02:30:16.100]   So, and he's done all these experiments
[02:30:16.100 --> 02:30:19.220]   that basically show that when you over optimize,
[02:30:19.220 --> 02:30:23.700]   like the trade-off is you're less creative, right?
[02:30:23.700 --> 02:30:26.700]   And to create true greatness
[02:30:26.700 --> 02:30:29.620]   and truly creative solutions to problems,
[02:30:29.620 --> 02:30:31.740]   you can't over plan it, you can't.
[02:30:31.740 --> 02:30:33.100]   And I thought that was,
[02:30:33.100 --> 02:30:35.540]   and so he generalizes it beyond AI,
[02:30:35.540 --> 02:30:38.220]   and he talks about how we apply that in our personal life
[02:30:38.220 --> 02:30:40.380]   and our organizations and our companies,
[02:30:40.380 --> 02:30:42.340]   which are over KPI, right?
[02:30:42.340 --> 02:30:43.900]   Like look at any company in the world,
[02:30:43.900 --> 02:30:45.300]   and it's all like, these are the goals,
[02:30:45.300 --> 02:30:48.940]   these are the weekly goals, and the sprints,
[02:30:48.940 --> 02:30:50.780]   and then the quarterly goals, blah, blah, blah.
[02:30:50.780 --> 02:30:55.260]   And he just shows with a lot of his AI experiments
[02:30:55.260 --> 02:30:59.540]   that that's not how you create truly game-changing ideas.
[02:30:59.540 --> 02:31:00.540]   So there you go.
[02:31:00.540 --> 02:31:01.380]   - Yeah, yeah.
[02:31:01.380 --> 02:31:02.940]   - You should interview Kenneth, he's awesome.
[02:31:02.940 --> 02:31:04.620]   - Yeah, there's a balance, of course.
[02:31:04.620 --> 02:31:07.780]   'Cause that's, yeah, many moments of genius
[02:31:07.780 --> 02:31:09.780]   will not come from planning and goals,
[02:31:09.780 --> 02:31:12.900]   but you still have to build factories,
[02:31:12.900 --> 02:31:14.260]   and you still have to manufacture,
[02:31:14.260 --> 02:31:15.340]   and you still have to deliver,
[02:31:15.340 --> 02:31:17.260]   and there's still deadlines and all that kind of stuff.
[02:31:17.260 --> 02:31:19.340]   And for that, it's good to have goals.
[02:31:19.340 --> 02:31:22.660]   - I do goal setting with my kids, we all have our goals.
[02:31:22.660 --> 02:31:25.700]   But I think we're starting to morph
[02:31:25.700 --> 02:31:27.820]   into more of these bigger picture goals,
[02:31:27.820 --> 02:31:31.340]   and not obsess about, I don't know, it's hard.
[02:31:31.340 --> 02:31:33.460]   - Well, I honestly think, especially with kids,
[02:31:33.460 --> 02:31:36.260]   it's much better to have a plan and have goals and so on,
[02:31:36.260 --> 02:31:38.100]   'cause you have to learn the muscle
[02:31:38.100 --> 02:31:40.900]   of what it feels like to get stuff done.
[02:31:40.900 --> 02:31:43.940]   But I think once you learn that, there's flexibility for me.
[02:31:43.940 --> 02:31:47.980]   'Cause I spent most of my life with goal setting and so on.
[02:31:47.980 --> 02:31:50.580]   So I've gotten good with grades and school.
[02:31:50.580 --> 02:31:53.900]   I mean, school, if you wanna be successful at school,
[02:31:53.900 --> 02:31:56.180]   I mean, the kind of stuff in high school and college
[02:31:56.180 --> 02:31:59.140]   that kids have to do, in terms of managing their time
[02:31:59.140 --> 02:32:01.100]   and getting so much stuff done.
[02:32:01.100 --> 02:32:05.620]   It's like, taking five, six, seven classes in college,
[02:32:05.620 --> 02:32:09.380]   they're like, that would break the spirit of most humans
[02:32:09.380 --> 02:32:12.420]   if they took one of them later in life.
[02:32:12.420 --> 02:32:14.820]   It's like really difficult stuff,
[02:32:14.820 --> 02:32:16.500]   especially in engineering curricula.
[02:32:16.500 --> 02:32:19.980]   So I think you have to learn that skill,
[02:32:19.980 --> 02:32:21.900]   but once you learn it, you can maybe,
[02:32:21.900 --> 02:32:24.540]   'cause you can be a little bit on autopilot
[02:32:24.540 --> 02:32:25.700]   and use that momentum,
[02:32:25.700 --> 02:32:29.100]   and then allow yourself to be lost in the flow of life.
[02:32:29.100 --> 02:32:34.100]   Just kinda, or also give,
[02:32:34.100 --> 02:32:38.540]   I worked pretty hard to allow myself
[02:32:38.540 --> 02:32:39.860]   to have the freedom to do that.
[02:32:39.860 --> 02:32:42.900]   That's really, that's a tricky freedom to have.
[02:32:42.900 --> 02:32:45.300]   Because a lot of people get lost in the rat race,
[02:32:45.300 --> 02:32:49.660]   and they also, like financially,
[02:32:49.660 --> 02:32:52.820]   they, whenever you get a raise,
[02:32:52.820 --> 02:32:54.100]   they'll get like a bigger house.
[02:32:54.100 --> 02:32:54.940]   - Right, right, right.
[02:32:54.940 --> 02:32:55.780]   - Or something like this.
[02:32:55.780 --> 02:32:58.620]   I put very, so like, you're always trapped in this race.
[02:32:58.620 --> 02:33:03.620]   I put a lot of emphasis on living below my means always.
[02:33:03.620 --> 02:33:08.260]   And so there's a lot of freedom to do whatever,
[02:33:08.260 --> 02:33:11.380]   whatever the heart desires.
[02:33:11.380 --> 02:33:13.220]   That's a really, but everyone has to decide
[02:33:13.220 --> 02:33:15.540]   what's the right thing, what's the right thing for them.
[02:33:15.540 --> 02:33:18.760]   For some people, having a lot of responsibilities,
[02:33:18.760 --> 02:33:20.860]   like a house they can barely afford,
[02:33:20.860 --> 02:33:24.500]   or having a lot of kids, the responsibility side of that,
[02:33:24.500 --> 02:33:28.020]   is really, helps them get their shit together.
[02:33:28.020 --> 02:33:30.980]   Like, all right, I need to be really focused and good.
[02:33:30.980 --> 02:33:33.100]   Some of the most successful people I know have kids,
[02:33:33.100 --> 02:33:34.700]   and the kids bring out the best in them.
[02:33:34.700 --> 02:33:36.540]   They make them more productive and less productive.
[02:33:36.540 --> 02:33:39.260]   - Accountability, it's an accountability thing, absolutely.
[02:33:39.260 --> 02:33:42.060]   - And almost something to actually live and fight
[02:33:42.060 --> 02:33:44.900]   and work for, like having a family.
[02:33:44.900 --> 02:33:46.540]   It's fascinating to see.
[02:33:46.560 --> 02:33:49.400]   'Cause you would think kids would be a hit on productivity,
[02:33:49.400 --> 02:33:52.200]   but they're not, for a lot of really successful people.
[02:33:52.200 --> 02:33:54.920]   They really, they're like an engine of--
[02:33:54.920 --> 02:33:56.120]   - Right, efficiency, oh my God.
[02:33:56.120 --> 02:33:57.840]   - Yeah, it's weird.
[02:33:57.840 --> 02:33:59.600]   I mean, it's beautiful, it's beautiful to see.
[02:33:59.600 --> 02:34:01.960]   And also social happiness.
[02:34:01.960 --> 02:34:06.960]   Speaking of which, what role do you think love plays
[02:34:06.960 --> 02:34:09.100]   in the human condition, love?
[02:34:09.100 --> 02:34:14.440]   - I think love is,
[02:34:15.260 --> 02:34:19.920]   yeah, I think it's why we're all here.
[02:34:19.920 --> 02:34:21.720]   I think it would be very hard to live life
[02:34:21.720 --> 02:34:26.160]   without love in any of its forms, right?
[02:34:26.160 --> 02:34:31.040]   - Yeah, that's the most beautiful of forms
[02:34:31.040 --> 02:34:35.020]   that human connection takes, right?
[02:34:35.020 --> 02:34:40.020]   - Yeah, I feel like everybody wants to feel loved, right?
[02:34:40.020 --> 02:34:42.080]   In one way or another, right?
[02:34:42.080 --> 02:34:42.920]   - And to love.
[02:34:42.920 --> 02:34:44.320]   - Yeah, and to love too.
[02:34:44.320 --> 02:34:46.280]   - Totally, yeah, I agree with that.
[02:34:46.280 --> 02:34:47.120]   - Both of it.
[02:34:47.120 --> 02:34:48.860]   I'm not even sure what feels better.
[02:34:48.860 --> 02:34:52.520]   Both, both are like that.
[02:34:52.520 --> 02:34:54.400]   - To give love too, yeah.
[02:34:54.400 --> 02:34:56.920]   - And it is like we've been talking about,
[02:34:56.920 --> 02:34:59.480]   an interesting question, whether some of that,
[02:34:59.480 --> 02:35:02.340]   whether one day we'll be able to love a toaster.
[02:35:02.340 --> 02:35:05.280]   Get some small--
[02:35:05.280 --> 02:35:07.520]   - I wasn't quite thinking about that when I said--
[02:35:07.520 --> 02:35:08.360]   - The toaster.
[02:35:08.360 --> 02:35:10.960]   - Yeah, like we all need love and give love.
[02:35:10.960 --> 02:35:11.800]   Okay, you're right.
[02:35:11.800 --> 02:35:12.640]   - I was thinking about Brad Pitt and toasters.
[02:35:12.640 --> 02:35:14.400]   - Brad Pitt and toasters, great.
[02:35:14.400 --> 02:35:18.440]   - All right, well, I think we started on love
[02:35:18.440 --> 02:35:20.080]   and ended on love.
[02:35:20.080 --> 02:35:22.160]   This was an incredible conversation, Ron,
[02:35:22.160 --> 02:35:23.360]   thank you so much.
[02:35:23.360 --> 02:35:24.760]   You're an incredible person.
[02:35:24.760 --> 02:35:28.080]   Thank you for everything you're doing in AI,
[02:35:28.080 --> 02:35:32.640]   in the space of just caring about humanity,
[02:35:32.640 --> 02:35:34.680]   human emotion, about love,
[02:35:34.680 --> 02:35:38.200]   and being an inspiration to a huge number of people
[02:35:38.200 --> 02:35:42.240]   in robotics, in AI, in science, in the world in general.
[02:35:42.240 --> 02:35:44.200]   So thank you for talking to me, it's an honor.
[02:35:44.200 --> 02:35:45.400]   - Thank you for having me,
[02:35:45.400 --> 02:35:47.240]   and you know I'm a big fan of yours as well,
[02:35:47.240 --> 02:35:48.680]   so it's been a pleasure.
[02:35:48.680 --> 02:35:51.120]   - Thanks for listening to this conversation
[02:35:51.120 --> 02:35:52.800]   with Rana Elkayoobie.
[02:35:52.800 --> 02:35:53.960]   To support this podcast,
[02:35:53.960 --> 02:35:56.840]   please check out our sponsors in the description.
[02:35:56.840 --> 02:35:58.720]   And now, let me leave you with some words
[02:35:58.720 --> 02:35:59.800]   from Helen Keller.
[02:35:59.800 --> 02:36:03.560]   The best and most beautiful things in the world
[02:36:03.560 --> 02:36:06.120]   cannot be seen or even touched.
[02:36:06.120 --> 02:36:08.220]   They must be felt with the heart.
[02:36:09.320 --> 02:36:12.400]   Thank you for listening, and hope to see you next time.
[02:36:12.400 --> 02:36:14.980]   (upbeat music)
[02:36:14.980 --> 02:36:17.560]   (upbeat music)
[02:36:17.560 --> 02:36:27.560]   [BLANK_AUDIO]

