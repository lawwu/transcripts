
[00:00:00.000 --> 00:00:04.480]   I'll take the usual 30 seconds to make sure I'm live.
[00:00:04.480 --> 00:00:07.520]   There's this delay to people that are new.
[00:00:07.520 --> 00:00:11.080]   Between the time when I hit start the live stream
[00:00:11.080 --> 00:00:12.200]   and when it goes live.
[00:00:12.200 --> 00:00:16.960]   So if you sense a few seconds of awkward silence,
[00:00:16.960 --> 00:00:19.360]   that's what I'm trying to make sure that it lives.
[00:00:19.360 --> 00:00:27.360]   I can hear myself on one spot.
[00:00:27.360 --> 00:00:30.000]   Let me make sure the registration page is good.
[00:00:30.000 --> 00:00:45.680]   This is worrying, I don't hear,
[00:00:45.680 --> 00:00:47.400]   I don't see the other place being live.
[00:00:47.400 --> 00:00:49.240]   Let me look into that real quick.
[00:00:49.240 --> 00:00:54.240]  , I'll take one minute to debug this.
[00:00:54.240 --> 00:01:07.480]   If not, I'll continue live on YouTube.
[00:01:07.480 --> 00:01:11.080]   So please give me one quick minute to figure this out.
[00:01:11.080 --> 00:01:14.160]   I have run into an issue with the live stream.
[00:01:14.160 --> 00:01:16.460]   So that's what I'm trying to solve real quick.
[00:01:16.460 --> 00:01:18.620]   (silence)
[00:01:18.620 --> 00:01:19.620]   there we go.
[00:01:19.620 --> 00:01:20.620]  , there we go.
[00:01:20.620 --> 00:01:22.780]   (silence)
[00:01:22.780 --> 00:01:27.780]  , (silence)
[00:01:27.780 --> 00:01:37.780]  , (silence)
[00:01:37.780 --> 00:01:57.380]   all right, I put in a fix.
[00:01:57.380 --> 00:01:59.780]   Let me see if that worked.
[00:01:59.780 --> 00:02:04.420]   I'm hoping that should have worked.
[00:02:04.420 --> 00:02:08.740]   To anyone else who's waiting,
[00:02:08.740 --> 00:02:11.700]   there's been a mistake in setting up the live stream.
[00:02:11.700 --> 00:02:13.700]   That's what I'm trying to fix right now.
[00:02:13.700 --> 00:02:21.020]   Okay, I'll continue on YouTube because
[00:02:21.020 --> 00:02:25.140]   I wanted to make sure it's live for the registrants,
[00:02:25.140 --> 00:02:28.100]   but it's not, so I don't want to waste Amit's time
[00:02:28.100 --> 00:02:31.300]   who's saying, hello, hey Amit, thanks for joining us.
[00:02:31.300 --> 00:02:32.140]   Let's get started.
[00:02:32.140 --> 00:02:33.900]   Sorry about that everyone.
[00:02:33.900 --> 00:02:36.540]   We also have a registration page
[00:02:36.540 --> 00:02:38.180]   which sends you weekly reminders.
[00:02:38.180 --> 00:02:41.140]   And for some reason, I'm not able to live stream there.
[00:02:41.140 --> 00:02:44.980]   I was trying to debug that, but that didn't go well.
[00:02:44.980 --> 00:02:47.380]   So I'm continuing regardless.
[00:02:47.380 --> 00:02:48.820]   Welcome back to the paper reading group.
[00:02:48.820 --> 00:02:52.060]   I am excited to be back in my Chai office.
[00:02:52.060 --> 00:02:54.900]   I'm live streaming for the Chai office now for anyone.
[00:02:54.900 --> 00:02:55.900]   That's the way I was traveling.
[00:02:55.900 --> 00:02:59.980]   That's why we didn't have the last paper reading group.
[00:02:59.980 --> 00:03:02.460]   For today, we're reading the paper titled
[00:03:02.460 --> 00:03:07.380]   Politeness Transfer, a Tag and Generate Approach.
[00:03:07.380 --> 00:03:09.900]   That is the title of the paper.
[00:03:09.900 --> 00:03:14.740]   This is by Aman Madan, sorry, et al.
[00:03:14.740 --> 00:03:16.060]   There are a few co-authors.
[00:03:16.060 --> 00:03:18.900]   I've actually interviewed the lead author,
[00:03:18.900 --> 00:03:21.460]   Aman Madan on Chai Time Data Science.
[00:03:21.460 --> 00:03:24.060]   So you're welcome to check that interview out.
[00:03:24.060 --> 00:03:25.780]   For today, we'll be covering this paper.
[00:03:25.780 --> 00:03:30.100]   And like always, I have received feedback
[00:03:30.100 --> 00:03:32.340]   about having a shorter summary of the papers.
[00:03:32.340 --> 00:03:34.980]   Let me real quick summarize the paper
[00:03:34.980 --> 00:03:36.100]   for anyone that's watching
[00:03:36.100 --> 00:03:38.780]   and doesn't want to spend an hour watching this video.
[00:03:38.780 --> 00:03:41.580]   Let me share the correct screen.
[00:03:41.580 --> 00:03:47.460]   And this should be the one.
[00:03:47.460 --> 00:03:50.180]   Scrolling down to the summary area.
[00:03:50.180 --> 00:03:52.020]   This figure inside the paper,
[00:03:52.020 --> 00:03:53.420]   again, this is from the archive
[00:03:53.420 --> 00:03:56.060]   and I'll point everyone to that link.
[00:03:56.060 --> 00:03:58.020]   But summarizing the paper,
[00:03:58.020 --> 00:04:02.060]   this paper aims at making sentences polite.
[00:04:02.060 --> 00:04:06.900]   So taking a sentence that's not exactly polite
[00:04:06.900 --> 00:04:08.860]   or maybe sometimes it's rude
[00:04:08.860 --> 00:04:10.580]   and you want to make it more polite,
[00:04:10.580 --> 00:04:12.820]   that's a very interesting problem to solve
[00:04:12.820 --> 00:04:14.540]   and we learn where that is.
[00:04:14.540 --> 00:04:16.700]   But here's the summary of how the paper does it.
[00:04:16.700 --> 00:04:18.140]   So it takes a sentence.
[00:04:19.500 --> 00:04:24.060]   As you can see here, X1 is the nonpolite sentence.
[00:04:24.060 --> 00:04:26.740]   Then it's adding a tagger.
[00:04:26.740 --> 00:04:29.980]   So it tags the sentences that it, or words, sorry.
[00:04:29.980 --> 00:04:32.340]   So it takes the sentence that's nonpolite,
[00:04:32.340 --> 00:04:35.060]   puts in a tag.
[00:04:35.060 --> 00:04:39.380]   So here's the intermediate representation, right?
[00:04:39.380 --> 00:04:42.860]   Let me pick up my pen.
[00:04:42.860 --> 00:04:43.820]   This is to draw.
[00:04:43.820 --> 00:04:45.900]   Awesome.
[00:04:48.460 --> 00:04:53.340]   So it creates this intermediate representation, right?
[00:04:53.340 --> 00:04:55.460]   And from there, there's a stage two
[00:04:55.460 --> 00:04:57.540]   where you use a generator.
[00:04:57.540 --> 00:05:00.500]   And now you have this tag, right?
[00:05:00.500 --> 00:05:01.740]   And this could be similar to,
[00:05:01.740 --> 00:05:03.980]   so if you're familiar with NLP,
[00:05:03.980 --> 00:05:06.300]   if you're not, look at our Hugging Face Study Group.
[00:05:06.300 --> 00:05:08.300]   Wade has taught a lot of stuff there.
[00:05:08.300 --> 00:05:11.740]   But this is how tagging is usually done in NLP,
[00:05:11.740 --> 00:05:15.580]   you see unknown or similar tags.
[00:05:15.580 --> 00:05:20.140]   The generator generates a few words and adds it there.
[00:05:20.140 --> 00:05:21.380]   So I know this is blurred.
[00:05:21.380 --> 00:05:25.540]   I think one node doesn't like having high risk.
[00:05:25.540 --> 00:05:28.060]   So I'll read out the sentence for everyone.
[00:05:28.060 --> 00:05:31.340]   The input sentence is send me the data.
[00:05:31.340 --> 00:05:34.700]   We add a tag in front, and then the generator adds,
[00:05:34.700 --> 00:05:37.460]   could you please send me the data?
[00:05:37.460 --> 00:05:39.780]   So we've made the sentence more polite.
[00:05:39.780 --> 00:05:44.580]   Another approach, another example.
[00:05:45.580 --> 00:05:49.860]   We want to replace negative words and make them positive.
[00:05:49.860 --> 00:05:53.020]   So it says, the chips are okay, but the sales are really,
[00:05:53.020 --> 00:05:55.740]   sorry, the salsa is really bland.
[00:05:55.740 --> 00:05:58.620]   It's really low resolution, so even I can't read.
[00:05:58.620 --> 00:06:00.460]   The salsa is really bland.
[00:06:00.460 --> 00:06:03.460]   Salsa shouldn't be bland.
[00:06:03.460 --> 00:06:07.260]   So the intermediate representation is their chips are tag,
[00:06:07.260 --> 00:06:10.540]   but the salsa is really tag, right?
[00:06:10.540 --> 00:06:15.540]   So the tagger has identified the negative words, right?
[00:06:15.540 --> 00:06:19.820]   Okay and bland, and replace them with the tag.
[00:06:19.820 --> 00:06:21.900]   Then in the stage two, we take a generator,
[00:06:21.900 --> 00:06:24.780]   which replaces this with, the chips are great,
[00:06:24.780 --> 00:06:26.660]   but their salsa is really delicious.
[00:06:26.660 --> 00:06:30.940]   Maybe not a good idea for a restaurant
[00:06:30.940 --> 00:06:34.740]   that wants honest feedback,
[00:06:34.740 --> 00:06:37.860]   but if you're trying to change your review
[00:06:37.860 --> 00:06:39.940]   and create more positive impact in the world,
[00:06:39.940 --> 00:06:41.180]   that's how you change that.
[00:06:41.180 --> 00:06:43.660]   Although in this case, you should be more honest.
[00:06:43.660 --> 00:06:47.220]   That's an ethical debate.
[00:06:47.220 --> 00:06:48.660]   This is how the model works.
[00:06:48.660 --> 00:06:51.700]   So this was a summary for anyone who had two minutes.
[00:06:51.700 --> 00:06:52.860]   I summarized the paper.
[00:06:52.860 --> 00:06:54.380]   I hope you stick to the end of the video
[00:06:54.380 --> 00:06:56.460]   if you're watching the recap on livestream.
[00:06:56.460 --> 00:06:58.620]   But this is just to tell you what we'll be learning.
[00:06:58.620 --> 00:07:00.300]   And Amit is asking a great question.
[00:07:00.300 --> 00:07:04.060]   Is it like end of sentence tag?
[00:07:04.060 --> 00:07:06.060]   Yes, it's similar to that.
[00:07:06.060 --> 00:07:11.060]   Now, Wits and Biases is an MLOps company, broadly.
[00:07:11.060 --> 00:07:14.900]   More in, broadly speaking, it's a machine learning company
[00:07:14.900 --> 00:07:16.820]   and we come to paper reading groups
[00:07:16.820 --> 00:07:18.420]   to learn about machine learning techniques.
[00:07:18.420 --> 00:07:20.260]   So, "Syed, why the hell are we learning
[00:07:20.260 --> 00:07:21.980]   "about how to make sentences polite?
[00:07:21.980 --> 00:07:22.900]   "It's a nice thing to know,
[00:07:22.900 --> 00:07:25.060]   "but where's the machine learning?"
[00:07:25.060 --> 00:07:27.900]   We learn about this, this tagger,
[00:07:27.900 --> 00:07:33.860]   and this generator are, of course, transformer models.
[00:07:33.860 --> 00:07:36.620]   And the way this is evaluated is also,
[00:07:36.620 --> 00:07:40.260]   it's actually a nice mix of a few techniques.
[00:07:40.260 --> 00:07:43.100]   So, TF-IDF is being used.
[00:07:43.100 --> 00:07:45.820]   We learned what that is, if you don't know.
[00:07:45.820 --> 00:07:47.580]   Don't worry about it.
[00:07:47.580 --> 00:07:50.460]   And for the generator approach,
[00:07:50.460 --> 00:07:54.020]   we are using transformer models.
[00:07:54.020 --> 00:07:55.860]   And I'll show that quickly as well.
[00:07:55.860 --> 00:07:57.140]   We'll jump around a bit.
[00:07:57.140 --> 00:07:59.340]   I'm giving the summary upfront
[00:07:59.340 --> 00:08:01.780]   just so that everyone is a bit prepared of what's coming.
[00:08:01.780 --> 00:08:04.660]   So, let's find where that is.
[00:08:04.660 --> 00:08:14.580]   Oops, I scrolled past it.
[00:08:14.580 --> 00:08:15.900]   I'm trying to get to it fast
[00:08:15.900 --> 00:08:18.660]   and I always worry about my scrolling speed
[00:08:18.660 --> 00:08:21.180]   because I know it blurs in the live stream.
[00:08:21.180 --> 00:08:23.380]   So, pardon that, please.
[00:08:23.380 --> 00:08:29.580]   Here is the implementation detail.
[00:08:29.580 --> 00:08:31.620]   Again, giving the summary upfront
[00:08:31.620 --> 00:08:34.100]   and we'll come back and take a look at it.
[00:08:34.100 --> 00:08:36.740]   We use, the authors, not me,
[00:08:36.740 --> 00:08:39.500]   they use a four-layer transformer
[00:08:39.500 --> 00:08:43.100]   to train both tagger and generator models.
[00:08:43.100 --> 00:08:44.420]   And then they have the details of,
[00:08:44.420 --> 00:08:46.340]   okay, this has that many attention here,
[00:08:46.340 --> 00:08:48.300]   this many embedding layers, hidden states,
[00:08:48.300 --> 00:08:49.540]   so on and so forth.
[00:08:49.540 --> 00:08:51.740]   So, if you were curious about
[00:08:51.740 --> 00:08:52.900]   where is the machine learning
[00:08:52.900 --> 00:08:54.940]   in making the world a better place
[00:08:54.940 --> 00:08:58.140]   by making sentences more polite, this is how.
[00:08:59.660 --> 00:09:02.180]   So, let me start with another question.
[00:09:02.180 --> 00:09:04.420]   Since Amit has been quite active.
[00:09:04.420 --> 00:09:08.260]   Amit or anyone else watching this,
[00:09:08.260 --> 00:09:10.940]   where else do you think this approach can be useful?
[00:09:10.940 --> 00:09:14.820]   Alternate question, why do you think I picked this paper?
[00:09:14.820 --> 00:09:17.660]   And this is a cool example, right?
[00:09:17.660 --> 00:09:19.220]   Making sentences more polite.
[00:09:19.220 --> 00:09:23.940]   But where else do you think this approach,
[00:09:23.940 --> 00:09:26.940]   not knowing what it is exactly, can be useful?
[00:09:26.940 --> 00:09:28.700]   I'll just wait for one answer, if at all,
[00:09:28.700 --> 00:09:30.300]   and if not, I'll continue.
[00:09:30.300 --> 00:09:36.180]   Meanwhile, let me switch to the correct screen.
[00:09:36.180 --> 00:09:39.340]   Again, there are no wrong answers or wrong questions,
[00:09:39.340 --> 00:09:41.980]   so please feel free to say anything at all.
[00:09:41.980 --> 00:09:48.660]   The question was, where else can this approach be useful?
[00:09:48.660 --> 00:09:51.980]   Maybe YouTube Live Chat?
[00:09:51.980 --> 00:09:55.480]   (computer mouse clicking)
[00:09:55.480 --> 00:10:02.540]   Yeah, that is a place where this can be applied.
[00:10:02.540 --> 00:10:04.180]   I was thinking more in terms of,
[00:10:04.180 --> 00:10:11.140]   you know, examples where it can be demonstrated.
[00:10:11.140 --> 00:10:13.540]   So, at the end of the paper,
[00:10:13.540 --> 00:10:16.460]   the authors actually show a few examples.
[00:10:16.460 --> 00:10:18.740]   I see Yuvraj is one of our most active members.
[00:10:18.740 --> 00:10:19.700]   Welcome back, Yuvraj.
[00:10:19.700 --> 00:10:23.620]   His answer is, "If something is offensive,
[00:10:23.620 --> 00:10:24.900]   "it can be mellowed down."
[00:10:24.900 --> 00:10:26.220]   That's a great answer.
[00:10:26.220 --> 00:10:30.800]   So, yes, for politeness, answer yes.
[00:10:30.800 --> 00:10:32.620]   But what I was trying to allude to,
[00:10:32.620 --> 00:10:34.500]   maybe I didn't frame the question correctly.
[00:10:34.500 --> 00:10:37.020]   So, this can also be used
[00:10:37.020 --> 00:10:41.780]   to make sentences less political and similar areas.
[00:10:41.780 --> 00:10:44.020]   So, where you can tag parts of sentences
[00:10:44.020 --> 00:10:46.220]   and remove those few things.
[00:10:46.220 --> 00:10:48.100]   So, again, your applications are correct.
[00:10:48.100 --> 00:10:51.180]   I probably didn't frame my question correctly,
[00:10:51.180 --> 00:10:54.540]   but it's not just limited to politeness to answer.
[00:10:54.540 --> 00:10:59.220]   It can be transferring anything within certain limits.
[00:10:59.220 --> 00:11:00.060]   But, yeah.
[00:11:00.060 --> 00:11:04.340]   And Amit is asking, "How do I join the study group?"
[00:11:04.340 --> 00:11:06.540]   Anyone and everyone is welcome to join.
[00:11:06.540 --> 00:11:10.460]   At least all of the study groups I host.
[00:11:10.460 --> 00:11:13.060]   And if my colleagues don't allow you
[00:11:13.060 --> 00:11:16.060]   into any of the study groups they are hosting,
[00:11:16.060 --> 00:11:17.760]   send me a message and I'll add you.
[00:11:18.440 --> 00:11:19.520]   I'm kidding about that.
[00:11:19.520 --> 00:11:21.800]   Our study groups are open to everyone.
[00:11:21.800 --> 00:11:24.520]   So, let's get started with a detailed walkthrough.
[00:11:24.520 --> 00:11:26.920]   I've already introduced the paper and summarized it.
[00:11:26.920 --> 00:11:27.840]   That's the target.
[00:11:27.840 --> 00:11:28.800]   You can look it up.
[00:11:28.800 --> 00:11:30.240]   If you've received the email
[00:11:30.240 --> 00:11:31.440]   or if you're watching on YouTube,
[00:11:31.440 --> 00:11:33.740]   it should be linked in the description
[00:11:33.740 --> 00:11:35.740]   or in the email you all have received.
[00:11:35.740 --> 00:11:39.400]   Just to remind you, last paper reading group,
[00:11:39.400 --> 00:11:40.960]   we looked at Swin Transformer.
[00:11:40.960 --> 00:11:41.800]   And this was in March,
[00:11:41.800 --> 00:11:44.200]   because in April I was not in my chai office.
[00:11:44.200 --> 00:11:47.200]   And before that, we looked at Convex.
[00:11:47.200 --> 00:11:49.760]   So, if you're curious about these recent papers,
[00:11:49.760 --> 00:11:51.200]   you can also check them out.
[00:11:51.200 --> 00:11:56.280]   The agenda for today, I've already covered the first point,
[00:11:56.280 --> 00:11:57.480]   but the next few are,
[00:11:57.480 --> 00:12:00.360]   we learn what Politeness Transfer exactly is.
[00:12:00.360 --> 00:12:02.920]   We'll get an overview of the implementation
[00:12:02.920 --> 00:12:06.280]   and we'll get a few suggested homework.
[00:12:06.280 --> 00:12:08.040]   I always suggest that.
[00:12:08.040 --> 00:12:09.500]   Sometimes some people do it.
[00:12:09.500 --> 00:12:12.360]   Yorosh definitely does it.
[00:12:12.360 --> 00:12:14.520]   I always look forward to his papers.
[00:12:14.520 --> 00:12:16.200]   Sorry, his blog posts.
[00:12:16.200 --> 00:12:18.920]   Maybe I can convince Amit as well to write blog posts now.
[00:12:18.920 --> 00:12:19.880]   So, we'll see.
[00:12:19.880 --> 00:12:22.760]   We'll see in a few days if he writes a blog post or not.
[00:12:22.760 --> 00:12:26.720]   Amit has another great answer to my previous question.
[00:12:26.720 --> 00:12:30.760]   Maybe converting English text to proper Hindi sentence.
[00:12:30.760 --> 00:12:32.400]   That's a great answer.
[00:12:32.400 --> 00:12:35.040]   See, as you can see, it's this technique.
[00:12:35.040 --> 00:12:36.520]   The reason I picked this paper was,
[00:12:36.520 --> 00:12:39.240]   it's not just this one example where it's useful.
[00:12:39.240 --> 00:12:40.760]   It extends beyond that.
[00:12:40.760 --> 00:12:42.720]   And the authors also cover this in the paper.
[00:12:42.720 --> 00:12:46.320]   So, inside this glossary, I think,
[00:12:46.320 --> 00:12:47.800]   maybe the glossary, maybe before that,
[00:12:47.800 --> 00:12:50.240]   the part that no one actually gets to most of the time
[00:12:50.240 --> 00:12:51.080]   while reading a paper,
[00:12:51.080 --> 00:12:55.000]   they show it extends well beyond this.
[00:12:55.000 --> 00:12:56.840]   So, it's not just the silo technique.
[00:12:56.840 --> 00:12:59.580]   And before we start with this,
[00:12:59.580 --> 00:13:02.600]   I also want to quickly mention to everyone,
[00:13:02.600 --> 00:13:06.200]   Kaggle Grandmaster, the first notebooks Grandmaster,
[00:13:06.200 --> 00:13:08.600]   heads or tails, is currently hosting a competition
[00:13:08.600 --> 00:13:10.800]   around storytelling.
[00:13:10.800 --> 00:13:12.800]   Weights and Biases is sponsoring that
[00:13:12.800 --> 00:13:15.080]   and also has been supporting the competition.
[00:13:15.080 --> 00:13:17.120]   So, if you'd like to get started on that,
[00:13:17.120 --> 00:13:20.760]   there are two videos or two live streams,
[00:13:20.760 --> 00:13:22.440]   one with Grandmaster and Dada
[00:13:22.440 --> 00:13:26.860]   and one with heads or tails, Martin Hens himself.
[00:13:26.860 --> 00:13:28.560]   So, please check those out if you're interested
[00:13:28.560 --> 00:13:33.380]   in taking part in a storytelling Kaggle competition.
[00:13:33.380 --> 00:13:36.320]   Coming back to the reading group.
[00:13:39.600 --> 00:13:42.160]   The resources I've used are, we'll go to the paper,
[00:13:42.160 --> 00:13:45.880]   we'll go to the official repository.
[00:13:45.880 --> 00:13:48.200]   I re-watched my interview from two years ago
[00:13:48.200 --> 00:13:49.040]   with the lead author.
[00:13:49.040 --> 00:13:51.840]   So, you can literally go to my channel
[00:13:51.840 --> 00:13:53.480]   and look the paper's name up.
[00:13:53.480 --> 00:13:55.160]   It should show up,
[00:13:55.160 --> 00:13:57.080]   although I don't try to promote my stuff too much.
[00:13:57.080 --> 00:13:59.280]   So, if you don't find the link, reach out,
[00:13:59.280 --> 00:14:02.680]   but I don't think I put the link in the emails.
[00:14:02.680 --> 00:14:06.920]   So, let me start by asking a question to everyone.
[00:14:06.920 --> 00:14:09.600]   What is transfer learning
[00:14:09.600 --> 00:14:14.600]   and how can we do style transfer in text?
[00:14:14.600 --> 00:14:16.160]   I'll answer these questions in the next slide.
[00:14:16.160 --> 00:14:17.760]   Let me actually hide that.
[00:14:17.760 --> 00:14:19.480]   Can't scroll past that.
[00:14:19.480 --> 00:14:21.200]   But the question to everyone is,
[00:14:21.200 --> 00:14:25.080]   what is transfer learning in the case of NLP?
[00:14:25.080 --> 00:14:28.640]   How can we do style transfer
[00:14:28.640 --> 00:14:30.800]   and how does text generation works?
[00:14:30.800 --> 00:14:32.960]   (silence)
[00:14:32.960 --> 00:14:48.560]   Amit says he'll reach out on Twitter.
[00:14:48.560 --> 00:14:49.600]   Please do.
[00:14:49.600 --> 00:14:57.160]   So, for me, these few seconds are always freakish
[00:14:57.160 --> 00:15:00.080]   because by the time I'm saying something
[00:15:00.080 --> 00:15:03.760]   and by the time I get a reply, it's 20 seconds,
[00:15:03.760 --> 00:15:06.120]   and I'm trying to make sure if I should wait or answer that.
[00:15:06.120 --> 00:15:09.640]   So, I'll wait a few more seconds before I embarrass myself.
[00:15:09.640 --> 00:15:11.480]   These are the questions that I'm setting up
[00:15:11.480 --> 00:15:12.560]   to answer later on
[00:15:12.560 --> 00:15:17.280]   to help set up the paper walkthrough better.
[00:15:17.280 --> 00:15:21.440]   (silence)
[00:15:21.440 --> 00:15:23.600]   (silence)
[00:15:23.600 --> 00:15:48.360]   Okay, awesome.
[00:15:48.360 --> 00:15:49.680]   I'll continue still
[00:15:49.720 --> 00:15:51.680]   because I assume a few answers will come in,
[00:15:51.680 --> 00:15:53.360]   but I'll get started regardless.
[00:15:53.360 --> 00:15:56.080]   So, the first question was a bit of a trick question
[00:15:56.080 --> 00:15:58.560]   because this is not any transfer learning.
[00:15:58.560 --> 00:16:01.280]   In transfer learning,
[00:16:01.280 --> 00:16:04.800]   usually you take a pre-trained model
[00:16:04.800 --> 00:16:06.600]   and you apply that.
[00:16:06.600 --> 00:16:09.800]   We already have an answer from our most active users,
[00:16:09.800 --> 00:16:11.440]   so I'll highlight that.
[00:16:11.440 --> 00:16:13.720]   Yuvraj says transfer learning in NLP
[00:16:13.720 --> 00:16:17.280]   is when you have a larger network
[00:16:17.280 --> 00:16:19.080]   trained on a large corpus.
[00:16:20.080 --> 00:16:21.480]   And then you fine tune it.
[00:16:21.480 --> 00:16:24.120]   That's the perfect answer.
[00:16:24.120 --> 00:16:25.560]   That's the textbook answer.
[00:16:25.560 --> 00:16:26.400]   Thanks, Yuvraj.
[00:16:26.400 --> 00:16:31.120]   Amit answers...
[00:16:31.120 --> 00:16:35.840]   I'm trying to read and summarize it.
[00:16:35.840 --> 00:16:38.040]   It's also the correct answer, so thanks.
[00:16:38.040 --> 00:16:38.880]   Thanks, Amit.
[00:16:38.880 --> 00:16:40.720]   Both of these answers are right.
[00:16:40.720 --> 00:16:43.720]   So, the point I was trying to make here is
[00:16:43.720 --> 00:16:45.560]   in this paper, I was quite confused
[00:16:45.560 --> 00:16:47.320]   between transfer learning and what's happening
[00:16:47.320 --> 00:16:49.600]   in this politeness transfer thing.
[00:16:49.600 --> 00:16:51.520]   Just to make it clear to everyone
[00:16:51.520 --> 00:16:53.080]   who doesn't understand this,
[00:16:53.080 --> 00:16:56.480]   in transfer learning, as Yuvraj and Amit have answered,
[00:16:56.480 --> 00:16:59.120]   you take a model that's been trained
[00:16:59.120 --> 00:17:03.800]   and you apply it to a target problem.
[00:17:03.800 --> 00:17:08.800]   Here, we're doing something similar, but not exactly.
[00:17:08.800 --> 00:17:10.600]   So, we're not using a pre-trained model
[00:17:10.600 --> 00:17:12.560]   and throwing it at a different problem.
[00:17:13.800 --> 00:17:18.600]   We are transferring one format of one format.
[00:17:18.600 --> 00:17:20.200]   Let me move this out of the way.
[00:17:20.200 --> 00:17:23.000]   One format of a sentence to another.
[00:17:23.000 --> 00:17:26.000]   And we're using two models, as I showed earlier, to do that.
[00:17:26.000 --> 00:17:28.800]   So, this is different from transfer learning.
[00:17:28.800 --> 00:17:30.800]   It's not transfer learning,
[00:17:30.800 --> 00:17:34.120]   although the models are pre-trained.
[00:17:34.120 --> 00:17:37.640]   So, the entire process is not transfer learning,
[00:17:37.640 --> 00:17:39.360]   but parts of it involve that.
[00:17:39.360 --> 00:17:41.880]   Just to make the jargon clear,
[00:17:41.880 --> 00:17:44.280]   it's not quite relevant, but it's good to know.
[00:17:44.280 --> 00:17:48.120]   Here, the style transfer is quite different.
[00:17:48.120 --> 00:17:51.440]   So, style transfer, again, is something else,
[00:17:51.440 --> 00:17:53.400]   if you look up the textbook definition.
[00:17:53.400 --> 00:17:59.720]   But we are making sentences more polite.
[00:17:59.720 --> 00:18:04.720]   So, we are transferring the style of polite sentences.
[00:18:04.720 --> 00:18:09.920]   Somewhat similar, exact textbook definition is different.
[00:18:09.920 --> 00:18:11.600]   Just to make it clear to everyone.
[00:18:12.560 --> 00:18:15.920]   The third question, how does text generation work?
[00:18:15.920 --> 00:18:18.760]   So, let me again move the screen around.
[00:18:18.760 --> 00:18:22.920]   Yuvraj has an answer, so I'll just use that.
[00:18:22.920 --> 00:18:29.040]   When you prompt a large language model with a text input
[00:18:29.040 --> 00:18:32.280]   and end up having the model complete the sentence,
[00:18:32.280 --> 00:18:33.640]   that's the correct answer.
[00:18:33.640 --> 00:18:36.400]   And how does this happen is,
[00:18:36.400 --> 00:18:40.200]   the model checks the probability
[00:18:41.200 --> 00:18:43.920]   and continues generating words.
[00:18:43.920 --> 00:18:46.160]   There are a few more complications than this.
[00:18:46.160 --> 00:18:48.720]   Simplified version, you give a prompt,
[00:18:48.720 --> 00:18:51.880]   the model looks at the prompt, looks at the words,
[00:18:51.880 --> 00:18:54.240]   it checks the probability of what could be possibly
[00:18:54.240 --> 00:18:56.960]   the next word, a few more things beyond that,
[00:18:56.960 --> 00:19:00.360]   gives the word, continues on words.
[00:19:00.360 --> 00:19:04.400]   Sometimes inside of this, there are a few tags generated,
[00:19:04.400 --> 00:19:06.480]   there are a few tokens generated,
[00:19:06.480 --> 00:19:10.080]   and it continues like so.
[00:19:10.080 --> 00:19:12.200]   So the reason why I mentioned that was,
[00:19:12.200 --> 00:19:17.200]   we looked at having that tag inside of the sentence
[00:19:17.200 --> 00:19:19.720]   while performing politeness transfer.
[00:19:19.720 --> 00:19:23.320]   It's somewhat similar to text generation.
[00:19:23.320 --> 00:19:26.160]   So it's good to have that relevant background.
[00:19:26.160 --> 00:19:28.800]   If again, you are new to all of this,
[00:19:28.800 --> 00:19:33.960]   I can't possibly cover all of this,
[00:19:33.960 --> 00:19:37.280]   but we have already on our wonderful YouTube channel.
[00:19:37.280 --> 00:19:39.440]   Let me hide my subscriptions.
[00:19:40.120 --> 00:19:42.320]   (mouse clicking)
[00:19:42.320 --> 00:19:45.520]   Okay, now I can recommend these channels for entertainment,
[00:19:45.520 --> 00:19:49.440]   nothing interesting going on there, thankfully.
[00:19:49.440 --> 00:19:53.440]   So many of these.
[00:19:53.440 --> 00:19:56.840]   I'd encourage you to check out our Hugging Face study group
[00:19:56.840 --> 00:20:00.000]   where we have covered the basics of NLP and beyond.
[00:20:00.000 --> 00:20:02.600]   So if you're unaware of this,
[00:20:02.600 --> 00:20:04.760]   it's completely fine to ask that right now.
[00:20:04.760 --> 00:20:06.720]   I can't possibly teach it in one hour,
[00:20:06.720 --> 00:20:09.080]   so you're welcome to ask those questions still
[00:20:09.080 --> 00:20:11.960]   or go to those videos and ask again later.
[00:20:11.960 --> 00:20:18.920]   Yuvraj adds, "There are multiple ways
[00:20:18.920 --> 00:20:20.360]   "to completing the sentence.
[00:20:20.360 --> 00:20:24.320]   "It can be a greedy search or beam search,
[00:20:24.320 --> 00:20:27.520]   "or simply sampling the vocab available."
[00:20:27.520 --> 00:20:28.680]   That's a great answer.
[00:20:28.680 --> 00:20:35.840]   For this case, I believe the authors use,
[00:20:35.840 --> 00:20:37.080]   I had it written.
[00:20:37.080 --> 00:20:38.560]   I think it was beam search.
[00:20:38.560 --> 00:20:40.720]   I might be wrong about that.
[00:20:40.720 --> 00:20:42.920]   I'll double check once we read that section.
[00:20:42.920 --> 00:20:47.360]   So the reason I asked these questions,
[00:20:47.360 --> 00:20:49.600]   was again, I'm going to the latest version of FastAI,
[00:20:49.600 --> 00:20:54.040]   and there I was reminded, asking questions first
[00:20:54.040 --> 00:20:55.520]   and then getting to the answers
[00:20:55.520 --> 00:20:57.520]   helps you remember things better.
[00:20:57.520 --> 00:21:00.520]   So I'm always trying to improve our approach here
[00:21:00.520 --> 00:21:01.840]   with the paper reading group.
[00:21:01.840 --> 00:21:02.680]   Here are the answers.
[00:21:02.680 --> 00:21:06.120]   We've already discussed how style transfer works.
[00:21:06.120 --> 00:21:09.400]   I'll elaborate a bit on why is this an interesting task.
[00:21:09.400 --> 00:21:11.840]   And then we'll discuss the last two points
[00:21:11.840 --> 00:21:13.600]   before reading the paper.
[00:21:13.600 --> 00:21:15.600]   Why am I going through all of this?
[00:21:15.600 --> 00:21:19.080]   This is the relevant background you need for the paper.
[00:21:19.080 --> 00:21:23.080]   And if you go through the paper that it links off to,
[00:21:23.080 --> 00:21:26.240]   so the citation graph, you'll learn this.
[00:21:26.240 --> 00:21:27.200]   But since you're here,
[00:21:27.200 --> 00:21:28.840]   I don't want you to spend that time
[00:21:28.840 --> 00:21:31.640]   and I want you to go with as much learning in one hour.
[00:21:31.640 --> 00:21:33.520]   So I'll summarize all of these points.
[00:21:34.760 --> 00:21:37.200]   Why is this again, an interesting task?
[00:21:37.200 --> 00:21:42.840]   Could you possibly throw an end-to-end
[00:21:42.840 --> 00:21:45.040]   transformer model to do all of this?
[00:21:45.040 --> 00:21:48.040]   So the authors actually create a dataset,
[00:21:48.040 --> 00:21:51.000]   again, having a non-polite sentence
[00:21:51.000 --> 00:21:52.700]   and target polite sentence.
[00:21:52.700 --> 00:21:56.920]   Could you possibly throw a transformer model
[00:21:56.920 --> 00:21:59.040]   and make this an end-to-end task?
[00:21:59.040 --> 00:22:01.860]   Maybe with enough compute, most definitely.
[00:22:03.280 --> 00:22:06.440]   But here they're using a tagging and generator approach.
[00:22:06.440 --> 00:22:08.320]   And as we have discussed,
[00:22:08.320 --> 00:22:15.060]   this is quite effective beyond these few tasks.
[00:22:15.060 --> 00:22:17.220]   So for example, we discussed,
[00:22:17.220 --> 00:22:19.320]   this can make sentences less political,
[00:22:19.320 --> 00:22:22.740]   which is quite more important these days.
[00:22:22.740 --> 00:22:26.760]   It can convert sentence from English to Hindi,
[00:22:26.760 --> 00:22:28.720]   as Amit had pointed out.
[00:22:28.720 --> 00:22:30.000]   This is quite useful.
[00:22:30.000 --> 00:22:32.680]   And why is this an interesting task?
[00:22:32.680 --> 00:22:36.060]   Again, sorry about that.
[00:22:36.060 --> 00:22:41.800]   It is quite challenging to have a model
[00:22:41.800 --> 00:22:45.400]   identify a specific part of the sentence
[00:22:45.400 --> 00:22:48.560]   that is non-polite or polite,
[00:22:48.560 --> 00:22:53.040]   tag that and replace that without translating
[00:22:53.040 --> 00:22:56.400]   or changing the complete meaning of the sentence.
[00:22:56.400 --> 00:23:00.040]   That is the challenge that they have accepted here.
[00:23:00.040 --> 00:23:01.440]   This paper is from two years ago,
[00:23:01.440 --> 00:23:03.720]   larger models and much higher accuracy stuff
[00:23:03.720 --> 00:23:06.600]   has come out since, but the basis is still important.
[00:23:06.600 --> 00:23:09.280]   So that's why this, in my opinion,
[00:23:09.280 --> 00:23:11.820]   is an interesting problem.
[00:23:11.820 --> 00:23:15.320]   Let's look at TF-IDF.
[00:23:15.320 --> 00:23:20.320]   I won't go into the mathematical definition of it.
[00:23:20.320 --> 00:23:21.880]   You can look it up.
[00:23:21.880 --> 00:23:24.880]   I understand it every time I read.
[00:23:24.880 --> 00:23:28.280]   And as soon as I close that tab, I forget the formula.
[00:23:28.280 --> 00:23:30.320]   So I won't embarrass myself.
[00:23:30.320 --> 00:23:33.400]   But as you're going through a document
[00:23:33.400 --> 00:23:37.880]   and as you're building a vocab for your model,
[00:23:37.880 --> 00:23:44.160]   you would like to assign importance to different words.
[00:23:44.160 --> 00:23:52.600]   With TF-IDF, you perform a certain division
[00:23:52.600 --> 00:23:57.280]   based on frequency, throwing broad jargon out there
[00:23:57.280 --> 00:23:59.800]   because I don't want to remember the formula.
[00:23:59.800 --> 00:24:00.960]   I can look it up.
[00:24:00.960 --> 00:24:04.200]   And it's quite intuitive once you've worked
[00:24:04.200 --> 00:24:05.040]   through a problem.
[00:24:05.040 --> 00:24:07.080]   So if you actually take a piece of paper
[00:24:07.080 --> 00:24:09.760]   and write all of this down, you can understand it.
[00:24:09.760 --> 00:24:14.960]   But let's say in your sentence or in your document,
[00:24:14.960 --> 00:24:17.660]   chai appears once.
[00:24:17.660 --> 00:24:21.260]   I said it thrice already in this video,
[00:24:21.260 --> 00:24:23.280]   but it is the most important thing, right?
[00:24:23.280 --> 00:24:25.200]   So how do you give it that importance
[00:24:25.200 --> 00:24:28.280]   covered to all of the other words you compare?
[00:24:28.280 --> 00:24:32.960]   One of the obvious ways is you divide it
[00:24:32.960 --> 00:24:37.280]   by the general appearances of, by the frequency
[00:24:37.280 --> 00:24:40.280]   or general number of appearances of other words.
[00:24:40.280 --> 00:24:42.840]   Again, I'm highlighting how it works
[00:24:42.840 --> 00:24:44.240]   and not exactly the formulas.
[00:24:44.240 --> 00:24:46.320]   It might be slightly different.
[00:24:46.320 --> 00:24:50.600]   But by dividing that, for example, in a novel,
[00:24:50.600 --> 00:24:52.920]   how do you understand who's the hero
[00:24:52.920 --> 00:24:57.760]   or who's the protagonist of the story?
[00:24:57.760 --> 00:25:01.040]   GFIDF is helpful for that.
[00:25:01.040 --> 00:25:02.200]   And that is used in the paper.
[00:25:02.200 --> 00:25:03.360]   That's why I'm talking about it.
[00:25:03.360 --> 00:25:04.600]   It's not irrelevant.
[00:25:04.600 --> 00:25:09.320]   And we've already covered the tag and generator approach.
[00:25:09.320 --> 00:25:12.020]   So here is the background knowledge again to summarize.
[00:25:12.020 --> 00:25:17.300]   We are doing somewhat of a style transfer,
[00:25:17.300 --> 00:25:20.520]   somewhat of transfer learning,
[00:25:20.520 --> 00:25:23.340]   but the problem statement as framed
[00:25:23.340 --> 00:25:24.780]   is not transfer learning.
[00:25:24.780 --> 00:25:27.220]   So making sentence more polite,
[00:25:27.220 --> 00:25:30.000]   I don't think the textbook definition is transfer learning.
[00:25:30.000 --> 00:25:32.020]   We are using pre-trained models,
[00:25:32.020 --> 00:25:35.200]   but don't mix the jargon because I think in the paper,
[00:25:35.200 --> 00:25:38.040]   the literature gets confusing at some point.
[00:25:38.040 --> 00:25:41.420]   I understand most of the people know this,
[00:25:41.420 --> 00:25:43.560]   but this is just to clarify it for people
[00:25:43.560 --> 00:25:45.480]   that are still new to machine learning.
[00:25:45.480 --> 00:25:48.000]   So I have to make sure that they don't come back
[00:25:48.000 --> 00:25:49.200]   and start screaming at me.
[00:25:49.200 --> 00:25:52.280]   So I'm sorry if you already knew this.
[00:25:52.280 --> 00:25:56.840]   This is an interesting task because it extends beyond,
[00:25:56.840 --> 00:25:58.640]   well beyond politeness transfer.
[00:25:58.640 --> 00:26:01.680]   And it's a two-stage approach,
[00:26:01.680 --> 00:26:03.560]   which now we'll read in depth.
[00:26:03.560 --> 00:26:07.860]   Now I'll switch sharing screen and go to the paper.
[00:26:07.860 --> 00:26:11.160]   I'll give 30 seconds or I'll sip my chai for any questions.
[00:26:11.160 --> 00:26:12.580]   So any questions so far?
[00:26:12.580 --> 00:26:22.680]   We also have the big boss from Weights and Biases,
[00:26:22.680 --> 00:26:24.320]   Lavanya says I'm doing a great job.
[00:26:24.320 --> 00:26:25.520]   So thank you, Lavanya.
[00:26:26.360 --> 00:26:29.120]   (mouse clicking)
[00:26:29.120 --> 00:26:33.580]   Any questions so far with what we've discussed?
[00:26:33.580 --> 00:26:54.040]   Okay, I'll continue further,
[00:26:54.040 --> 00:26:59.040]   but just to emphasize, since I see some new names here,
[00:26:59.040 --> 00:27:01.680]   there are no stupid questions.
[00:27:01.680 --> 00:27:03.840]   So please feel free to ask as many.
[00:27:03.840 --> 00:27:06.720]   If we run out of time, I'll reply to your questions
[00:27:06.720 --> 00:27:09.480]   on Twitter or LinkedIn or email
[00:27:09.480 --> 00:27:11.280]   or wherever you want to write them to me.
[00:27:11.280 --> 00:27:13.480]   So please, please keep the questions coming.
[00:27:13.480 --> 00:27:18.500]   Quick recap on style transfer.
[00:27:19.920 --> 00:27:23.400]   Style transfer in my understanding
[00:27:23.400 --> 00:27:28.400]   is used more for GANs, historically.
[00:27:28.400 --> 00:27:31.560]   That might not be true for NLP.
[00:27:31.560 --> 00:27:33.360]   But I wanted to emphasize that
[00:27:33.360 --> 00:27:38.720]   usually it's about taking a picture by a certain artist
[00:27:38.720 --> 00:27:41.280]   and having your photo and applying this style
[00:27:41.280 --> 00:27:43.240]   to that picture.
[00:27:43.240 --> 00:27:46.480]   We're doing somewhat of a similar task here.
[00:27:48.480 --> 00:27:51.480]   And the thing we are not doing is
[00:27:51.480 --> 00:27:58.520]   transfer learning in the complete pipeline.
[00:27:58.520 --> 00:28:01.600]   I'm not sure if that clarifies it
[00:28:01.600 --> 00:28:03.200]   or makes it more confusing.
[00:28:03.200 --> 00:28:06.200]   Feel free to ask again, Amit,
[00:28:06.200 --> 00:28:08.000]   if that doesn't answer the question.
[00:28:08.000 --> 00:28:13.520]   So I'll start, like always, how I read these papers.
[00:28:13.520 --> 00:28:15.960]   It usually, I'm a slow reader.
[00:28:15.960 --> 00:28:17.600]   I hope that's useful for everyone,
[00:28:17.600 --> 00:28:21.160]   but sometimes you would want to watch these at 2x speed.
[00:28:21.160 --> 00:28:24.480]   I pre-read the paper and make my notes.
[00:28:24.480 --> 00:28:27.200]   I have since switched to a digital tablet.
[00:28:27.200 --> 00:28:29.480]   This is not a promotion, I really like this one.
[00:28:29.480 --> 00:28:30.520]   It's remarkable.
[00:28:30.520 --> 00:28:34.480]   And I take my notes and then again highlight those
[00:28:34.480 --> 00:28:36.640]   as I read through the important points here.
[00:28:36.640 --> 00:28:37.960]   So that's what we'll do now.
[00:28:37.960 --> 00:28:40.760]   I'll go through the important bits, in my opinion,
[00:28:40.760 --> 00:28:42.520]   and highlight those while we read through
[00:28:42.520 --> 00:28:45.440]   this paper's important sections.
[00:28:46.440 --> 00:28:51.440]   So I don't like the highlighter, let's use the pen.
[00:28:51.440 --> 00:28:59.440]   So this paper introduces a new task of politeness transfer,
[00:28:59.440 --> 00:29:04.440]   which involves, I should actually use the highlighter,
[00:29:04.440 --> 00:29:09.560]   converting non-polite sentences to polite sentences
[00:29:09.560 --> 00:29:11.600]   while preserving the meaning.
[00:29:13.720 --> 00:29:16.320]   And they have also created a dataset.
[00:29:16.320 --> 00:29:18.320]   So one of the suggested homeworks from me
[00:29:18.320 --> 00:29:23.320]   would be to download this dataset and actually work on it.
[00:29:23.320 --> 00:29:31.640]   Amit, I see your question, I'll come back to it afterwards.
[00:29:31.640 --> 00:29:37.880]   I would encourage you to, sorry, not do style transfer,
[00:29:37.880 --> 00:29:40.280]   but actually play around with this dataset.
[00:29:42.040 --> 00:29:44.440]   The approach, as I mentioned earlier,
[00:29:44.440 --> 00:29:46.960]   is a tag and generate pipeline
[00:29:46.960 --> 00:29:54.200]   that identifies attributes and generates a new target.
[00:29:54.200 --> 00:29:58.640]   So in that sentence, it would identify that
[00:29:58.640 --> 00:30:03.320]   this is a number, let's say we are trying to convert
[00:30:03.320 --> 00:30:08.160]   written numbers to word representations for some reason,
[00:30:08.160 --> 00:30:10.360]   because you work at a fancy company
[00:30:10.360 --> 00:30:12.520]   and you don't want to do the obvious programming.
[00:30:12.520 --> 00:30:14.360]   So now you'd use a transform model
[00:30:14.360 --> 00:30:17.320]   and spend a month doing it.
[00:30:17.320 --> 00:30:21.800]   Again, this is a joke and not intended to offend anyone.
[00:30:21.800 --> 00:30:27.040]   But in the sentence, your model would highlight the fact that
[00:30:27.040 --> 00:30:32.480]   this is a number, and then the generator would generate
[00:30:32.480 --> 00:30:37.360]   words that are literally the numbers written in words here.
[00:30:38.240 --> 00:30:41.000]   (mouse clicking)
[00:30:41.000 --> 00:30:44.080]   Later, the authors, of course,
[00:30:44.080 --> 00:30:47.640]   analyzed this beyond politeness transfer,
[00:30:47.640 --> 00:30:50.160]   and they say for five other tasks,
[00:30:50.160 --> 00:30:54.080]   their model outperforms the SOTA models at that time.
[00:30:54.080 --> 00:30:56.280]   Usually for anyone that's been attending
[00:30:56.280 --> 00:30:57.280]   the paper reading groups,
[00:30:57.280 --> 00:31:02.280]   I don't emphasize too much about the results percentages,
[00:31:03.440 --> 00:31:07.440]   because this paper is two year old papers,
[00:31:07.440 --> 00:31:09.760]   machine learning moves really fast,
[00:31:09.760 --> 00:31:14.760]   and I don't see a point in emphasizing on percentages,
[00:31:14.760 --> 00:31:18.280]   although it's of course really amazing
[00:31:18.280 --> 00:31:21.080]   that the authors have achieved such great results.
[00:31:21.080 --> 00:31:22.920]   So you can go through the results,
[00:31:22.920 --> 00:31:26.120]   but I won't highlight the numbers in absolute detail.
[00:31:26.120 --> 00:31:30.320]   So in the introduction, they talk about
[00:31:30.320 --> 00:31:31.840]   why is this an important task,
[00:31:31.840 --> 00:31:36.000]   and the history of how this has been done.
[00:31:36.000 --> 00:31:45.120]   The authors in this paper study the task of converting
[00:31:45.120 --> 00:31:50.560]   non-polite sentences to polite sentences,
[00:31:50.560 --> 00:31:55.560]   the key being, while preserving the meaning.
[00:31:55.560 --> 00:32:00.600]   So as you have seen already,
[00:32:00.600 --> 00:32:04.960]   the tagger only replaces or only tags a part of the sentence
[00:32:04.960 --> 00:32:06.200]   and you don't have a,
[00:32:06.200 --> 00:32:11.440]   I wanted to say RNN or LSTM, that's not true nowadays,
[00:32:11.440 --> 00:32:13.120]   but you don't have a deep learning model
[00:32:13.120 --> 00:32:15.280]   replace the complete sentence word by word,
[00:32:15.280 --> 00:32:16.840]   it's only a part of it.
[00:32:16.840 --> 00:32:22.800]   That's how it differs from the modern approaches.
[00:32:22.800 --> 00:32:27.760]   Why is this an interesting problem?
[00:32:27.760 --> 00:32:29.960]   Apart from what we've discussed already,
[00:32:30.640 --> 00:32:35.640]   this is a rich and multifaceted problem.
[00:32:35.640 --> 00:32:41.920]   And depending on if you're a native speaker or not,
[00:32:41.920 --> 00:32:44.640]   it changes a bit.
[00:32:44.640 --> 00:32:48.240]   So they allude to the fact that in North America,
[00:32:48.240 --> 00:32:53.240]   some ads would be rude compared to the Arab culture.
[00:32:53.240 --> 00:32:58.160]   So it also really varies from culture to culture.
[00:32:58.160 --> 00:33:02.000]   And depending on this problem, especially,
[00:33:02.000 --> 00:33:06.120]   if you're a native or non-native speaker, things change.
[00:33:06.120 --> 00:33:13.680]   So they circumscribe the scope,
[00:33:13.680 --> 00:33:16.840]   I was trying not to mess that word up, I did not.
[00:33:16.840 --> 00:33:21.720]   They circumscribe the scope of politeness by two things.
[00:33:21.720 --> 00:33:26.080]   They adapt a data-driven definition.
[00:33:27.080 --> 00:33:31.440]   And they use this data set.
[00:33:31.440 --> 00:33:34.360]   What is this data set?
[00:33:34.360 --> 00:33:35.480]   You can look it up.
[00:33:35.480 --> 00:33:40.840]   But it's a collection of emails from American corporation
[00:33:40.840 --> 00:33:41.760]   a few years ago.
[00:33:41.760 --> 00:33:44.080]   So hopefully the world was more polite then
[00:33:44.080 --> 00:33:46.160]   and not less casual in emails.
[00:33:46.160 --> 00:33:47.960]   I think I've at least gotten less,
[00:33:47.960 --> 00:33:49.680]   more casual in emails now.
[00:33:49.680 --> 00:33:54.680]   So in 2004, I would expect emails
[00:33:54.840 --> 00:33:59.160]   would be much more formal, so more polite, I hope.
[00:33:59.160 --> 00:34:02.600]   Most of them, unless someone wrote an upset email.
[00:34:02.600 --> 00:34:14.120]   So later on, they point out, based on a paper,
[00:34:14.120 --> 00:34:21.560]   it is easy to pinpoint the signals of politeness.
[00:34:21.560 --> 00:34:26.560]   However, that signal, the absence of politeness,
[00:34:26.560 --> 00:34:34.120]   like direct question, do not explicitly appear
[00:34:34.120 --> 00:34:35.160]   in a sentence.
[00:34:35.160 --> 00:34:38.760]   And those are hard to objectify.
[00:34:38.760 --> 00:34:45.600]   So words like please,
[00:34:45.600 --> 00:34:48.600]   or I don't know, I don't know,
[00:34:49.280 --> 00:34:54.280]   so words like please, or what else could be it?
[00:34:54.280 --> 00:34:55.800]   Thank you.
[00:34:55.800 --> 00:35:00.000]   Such words clearly make a sentence polite.
[00:35:00.000 --> 00:35:04.240]   But their absence is hard to identify, right?
[00:35:04.240 --> 00:35:06.200]   So like, how is it a non-polite sentence?
[00:35:06.200 --> 00:35:08.440]   Because you're not saying please.
[00:35:08.440 --> 00:35:13.000]   Or I would mark every sentence that has coffee in it
[00:35:13.000 --> 00:35:14.160]   as non-polite.
[00:35:14.160 --> 00:35:17.560]   So that's a strong signal for me.
[00:35:17.560 --> 00:35:20.440]   But apart from these extreme cases,
[00:35:20.440 --> 00:35:22.840]   how do you do that, right?
[00:35:22.840 --> 00:35:28.800]   In their study, again, what I'm doing here
[00:35:28.800 --> 00:35:31.400]   for transparency is I'm looking at my notes
[00:35:31.400 --> 00:35:33.560]   from the paper and just pointing those out.
[00:35:33.560 --> 00:35:36.480]   So this is literally the paper in my hand
[00:35:36.480 --> 00:35:38.160]   and I'm pointing things out here.
[00:35:38.160 --> 00:35:41.560]   They not are the authors.
[00:35:41.560 --> 00:35:44.560]   Their study, they focus on, in their study,
[00:35:44.560 --> 00:35:47.400]   they focus on the task of transferring
[00:35:47.400 --> 00:35:50.080]   the non-polite sentences to polite sentences
[00:35:50.080 --> 00:35:53.400]   where they simply define non-politeness
[00:35:53.400 --> 00:35:58.080]   to be the absence of both politeness and impoliteness.
[00:35:58.080 --> 00:36:00.200]   We've discussed this multiple times
[00:36:00.200 --> 00:36:02.080]   what they're doing here,
[00:36:02.080 --> 00:36:06.240]   but here's how they define these things.
[00:36:06.240 --> 00:36:09.560]   So if you had literally thought about this for a minute,
[00:36:09.560 --> 00:36:11.160]   you would have immediately come to the point,
[00:36:11.160 --> 00:36:12.560]   oh, how do you define this, right?
[00:36:12.560 --> 00:36:14.800]   So here's how the author have done it.
[00:36:14.800 --> 00:36:17.680]   There could be a better approach, I think,
[00:36:17.680 --> 00:36:22.560]   but I couldn't come up with one, so I trust the authors.
[00:36:22.560 --> 00:36:27.120]   Again, their proposed approach
[00:36:27.120 --> 00:36:29.960]   is a tag and generate pipeline.
[00:36:29.960 --> 00:36:33.160]   The tagger identifies the words or phrases
[00:36:33.160 --> 00:36:37.000]   which belong to original style and replaces them.
[00:36:41.160 --> 00:36:44.240]   The generator takes this as an input
[00:36:44.240 --> 00:36:47.720]   and generates a sentence in the target style.
[00:36:47.720 --> 00:36:50.520]   So the generator,
[00:36:50.520 --> 00:36:54.440]   let's write here,
[00:36:54.440 --> 00:36:56.640]   or,
[00:36:56.640 --> 00:36:59.800]   okay, let's see if I can fit my handwriting
[00:36:59.800 --> 00:37:00.760]   in this margin.
[00:37:00.760 --> 00:37:02.520]   New challenge.
[00:37:02.520 --> 00:37:06.240]   The tagger takes the sentence
[00:37:08.640 --> 00:37:11.880]   and gives an output where parts of the sentence
[00:37:11.880 --> 00:37:13.040]   have a tag in them.
[00:37:13.040 --> 00:37:18.360]   Then the generator takes this output
[00:37:18.360 --> 00:37:23.880]   and replaces this with the styled word.
[00:37:23.880 --> 00:37:25.640]   So it's still the same sentence,
[00:37:25.640 --> 00:37:28.440]   but now it has, let's say, please, or something like that,
[00:37:28.440 --> 00:37:32.240]   or, I don't know, chai, something like that.
[00:37:32.240 --> 00:37:35.740]   (computer mouse clicking)
[00:37:35.740 --> 00:37:51.920]   Sorry, I opened this up, I opened a new tab up.
[00:37:51.920 --> 00:37:53.160]   I didn't want to do that.
[00:38:01.840 --> 00:38:03.240]   Sorry about that.
[00:38:03.240 --> 00:38:11.440]   So you have the target sentence
[00:38:11.440 --> 00:38:14.800]   where the generator has replaced that particular word.
[00:38:14.800 --> 00:38:17.520]   Now let's skip over the other details
[00:38:17.520 --> 00:38:19.880]   because I don't think these are relevant here.
[00:38:19.880 --> 00:38:23.360]   And they have also created a dataset, like I mentioned,
[00:38:23.360 --> 00:38:25.280]   with a large number of examples.
[00:38:25.280 --> 00:38:30.960]   You can also take a look at the source code.
[00:38:30.960 --> 00:38:33.520]   I did, and today I won't be highlighting it
[00:38:33.520 --> 00:38:36.480]   because it's extremely readable.
[00:38:36.480 --> 00:38:39.900]   And again, the paper pretty much covers everything.
[00:38:39.900 --> 00:38:46.080]   So before I move on to the next section,
[00:38:46.080 --> 00:38:48.820]   I'll see, I'll answer the questions that I see here.
[00:38:48.820 --> 00:38:52.080]   So going back to Amit's question,
[00:38:52.080 --> 00:38:54.000]   he had asked about style transfer,
[00:38:54.000 --> 00:38:57.680]   and he says, "Jeff Heaton's channel has covered this."
[00:38:57.680 --> 00:38:59.840]   Yes, that's the one we were talking about.
[00:39:00.680 --> 00:39:02.680]   Amit's style transfer.
[00:39:02.680 --> 00:39:04.680]   "Jeff Heaton's channel is pretty amazing.
[00:39:04.680 --> 00:39:06.480]   "I would highly encourage it to everyone."
[00:39:06.480 --> 00:39:07.320]   So,
[00:39:07.320 --> 00:39:11.860]   highly recommend it to everyone.
[00:39:11.860 --> 00:39:17.280]   Yuvraj says, "This is what apps like Grammarly do
[00:39:17.280 --> 00:39:18.840]   "with our sentences."
[00:39:18.840 --> 00:39:23.040]   I think he's right, especially if you have Grammarly Pro
[00:39:23.040 --> 00:39:24.440]   because I have that.
[00:39:24.440 --> 00:39:27.960]   Not an advertisement for Grammarly.
[00:39:27.960 --> 00:39:31.120]   This is a company channel, so I have to say this.
[00:39:31.120 --> 00:39:34.520]   But yeah, if you have the Grammarly Pro subscription,
[00:39:34.520 --> 00:39:37.960]   it actually tells you the tone of your
[00:39:37.960 --> 00:39:44.060]   sentence or document or whatever.
[00:39:44.060 --> 00:39:49.400]   Maybe these apps have rules instead of background.
[00:39:49.400 --> 00:39:52.380]   I think Grammarly has written about this.
[00:39:52.380 --> 00:39:56.720]   I'm not sure, but I think they use machine learning
[00:39:56.720 --> 00:40:01.400]   because it wasn't as good as it is now.
[00:40:01.400 --> 00:40:05.040]   And that, I think, only happens when you're using ML models.
[00:40:05.040 --> 00:40:06.880]   Again, Grammarly's pretty good now.
[00:40:06.880 --> 00:40:09.720]   I remember when it wasn't as good,
[00:40:09.720 --> 00:40:11.400]   meaning it's gotten better now.
[00:40:11.400 --> 00:40:15.240]   I assume that's because they use machine learning.
[00:40:15.240 --> 00:40:18.280]   An approach similar to the one
[00:40:18.280 --> 00:40:22.720]   can potentially be used to make our sentences more eloquent.
[00:40:24.720 --> 00:40:27.360]   Yeah, I think that's a great idea as well.
[00:40:27.360 --> 00:40:31.720]   One of the other ideas I had just based on your response was
[00:40:31.720 --> 00:40:34.880]   and this could be really useful.
[00:40:34.880 --> 00:40:38.020]   You need to figure out the dataset for this or whatnot.
[00:40:38.020 --> 00:40:40.360]   Taking a research paper
[00:40:40.360 --> 00:40:44.360]   and converting it into a blog post,
[00:40:44.360 --> 00:40:46.200]   meaning you replace the mathy jargon
[00:40:46.200 --> 00:40:48.120]   with stuff that is more readable.
[00:40:48.120 --> 00:40:53.120]   Sell that as a software and basically become a millionaire.
[00:40:53.120 --> 00:40:55.220]   I would buy that, everyone would buy that.
[00:40:55.220 --> 00:40:57.400]   Jokes apart, I think that would be
[00:40:57.400 --> 00:41:00.500]   a really cool implementation if anyone wants to look into it.
[00:41:00.500 --> 00:41:07.960]   So again, I'm looking at my notes
[00:41:07.960 --> 00:41:11.280]   to check out what I want to highlight.
[00:41:11.280 --> 00:41:17.640]   So they talk about the related work here
[00:41:17.640 --> 00:41:19.600]   and they've covered a few things.
[00:41:21.480 --> 00:41:23.000]   One of the things to highlight
[00:41:23.000 --> 00:41:27.100]   and this is about from previous covered work.
[00:41:27.100 --> 00:41:31.400]   They say the older work focuses
[00:41:31.400 --> 00:41:35.560]   on contextual dialogue response generation
[00:41:35.560 --> 00:41:38.300]   as opposed to content preserving style transfer.
[00:41:38.300 --> 00:41:45.120]   So these approaches take in the context and give a response
[00:41:45.120 --> 00:41:51.120]   and don't ensure that the original tone or style
[00:41:51.120 --> 00:41:53.320]   or whatnot is not changed.
[00:41:53.320 --> 00:41:55.520]   This is how their work differs.
[00:41:55.520 --> 00:41:57.480]   So people have of course worked on this problem
[00:41:57.480 --> 00:42:00.280]   earlier as well, but the author's approach
[00:42:00.280 --> 00:42:05.280]   is slightly different compared to that.
[00:42:05.280 --> 00:42:07.920]   So again, going further,
[00:42:07.920 --> 00:42:11.940]   there is another approach I want to highlight,
[00:42:11.940 --> 00:42:17.280]   which is delete, retrieve and generate, DRG.
[00:42:19.360 --> 00:42:20.920]   They say these are effective methods
[00:42:20.920 --> 00:42:23.560]   to generate outputs in target style
[00:42:23.560 --> 00:42:28.240]   with a high rate of source content preservation.
[00:42:28.240 --> 00:42:33.240]   However, so these approaches delete,
[00:42:33.240 --> 00:42:36.900]   I assume the words, retrieve them and again generate them.
[00:42:36.900 --> 00:42:41.160]   I just read those words out, but that's what it sounds like.
[00:42:41.160 --> 00:42:45.240]   The limitation of the said older approaches is
[00:42:48.480 --> 00:42:51.400]   they often delete important words.
[00:42:51.400 --> 00:42:56.720]   For example, in a sentence, you don't want to delete please
[00:42:56.720 --> 00:42:57.920]   or something like that.
[00:42:57.920 --> 00:43:03.580]   And it gets deleted, right?
[00:43:03.580 --> 00:43:06.400]   That's one of the things.
[00:43:06.400 --> 00:43:12.440]   I always get distracted with reading the chart
[00:43:12.440 --> 00:43:14.760]   and making sure I don't miss anything relevant.
[00:43:14.760 --> 00:43:16.600]   So I'm always reading the chart
[00:43:16.600 --> 00:43:18.600]   and that's why I get a bit lost.
[00:43:18.600 --> 00:43:25.640]   The second limitation is the retrieve step
[00:43:25.640 --> 00:43:28.440]   relies on presence of similar content
[00:43:28.440 --> 00:43:31.060]   in both source and target style,
[00:43:31.060 --> 00:43:34.040]   which of course, as you think of an example,
[00:43:34.040 --> 00:43:36.280]   I've thought this through, we have 15 more minutes,
[00:43:36.280 --> 00:43:39.080]   so I won't elaborate, but I'll try to think of this
[00:43:39.080 --> 00:43:41.900]   and you'll see that it's not always common.
[00:43:43.440 --> 00:43:46.680]   The retrieve step is time consuming for large datasets.
[00:43:46.680 --> 00:43:52.520]   We are contrasting and picking out the limitations
[00:43:52.520 --> 00:43:55.600]   of an older approach that is more similar to this.
[00:43:55.600 --> 00:44:01.320]   The pipeline makes assumption that style can be transferred
[00:44:01.320 --> 00:44:03.680]   by deleting markers and replacing them
[00:44:03.680 --> 00:44:05.240]   with target style phrases.
[00:44:05.240 --> 00:44:11.400]   And then the method relies on fixed corpus
[00:44:11.400 --> 00:44:13.700]   of style attribute markers.
[00:44:13.700 --> 00:44:19.000]   And this is limited to the ability to not being able
[00:44:19.000 --> 00:44:21.640]   to generalize to unseen data.
[00:44:21.640 --> 00:44:26.400]   So I'm trying to think of examples
[00:44:26.400 --> 00:44:28.900]   where use words other than please.
[00:44:28.900 --> 00:44:32.960]   I can't, but if you do, the older approach won't work.
[00:44:39.200 --> 00:44:41.920]   So reading further, I don't think I want to highlight
[00:44:41.920 --> 00:44:43.760]   anything important from here.
[00:44:43.760 --> 00:44:48.040]   Again, I only highlight things that can maximize
[00:44:48.040 --> 00:44:51.200]   your learning in one hour and glance over other details.
[00:44:51.200 --> 00:44:53.560]   If you want, you can always go back and read the paper
[00:44:53.560 --> 00:44:56.920]   and ask me anywhere, any questions that you have.
[00:44:56.920 --> 00:44:59.320]   I'm always happy to answer, ask me any questions.
[00:44:59.320 --> 00:45:06.440]   Looking at the chart, Jeff says there is a web app
[00:45:06.440 --> 00:45:09.800]   that makes simple language abstracts from complex text.
[00:45:09.800 --> 00:45:12.440]   Please do share it, I'd be curious to check this out.
[00:45:12.440 --> 00:45:16.280]   Sounds like my business idea was already taken guys.
[00:45:16.280 --> 00:45:17.600]   (laughs)
[00:45:17.600 --> 00:45:19.660]   I'm not becoming a founder anytime soon.
[00:45:19.660 --> 00:45:26.600]   Is it like BERT training process in DRG?
[00:45:26.600 --> 00:45:30.280]   I need to check the BERT training process.
[00:45:30.280 --> 00:45:36.260]   Oh, sorry, you mean is the DRG similar to BERT training?
[00:45:36.860 --> 00:45:41.860]   Uh, somewhat, because you do use the QKV combination there.
[00:45:41.860 --> 00:45:47.540]   I think that's what you're referring to.
[00:45:47.540 --> 00:45:49.020]   And it does sound similar to that.
[00:45:49.020 --> 00:45:51.940]   And this was after the transformers came out.
[00:45:51.940 --> 00:45:54.820]   So the paper, said paper is from 2018.
[00:45:54.820 --> 00:45:56.140]   I assume it's important.
[00:45:56.140 --> 00:46:00.100]   Okay, let's continue further.
[00:46:04.540 --> 00:46:09.300]   So here they talk about how they have prepared the dataset.
[00:46:09.300 --> 00:46:15.260]   And they say the Enron corpus consists of large set
[00:46:15.260 --> 00:46:18.700]   of email conversations exchanged by the employees.
[00:46:18.700 --> 00:46:23.700]   And this is the dataset, how do they pre-process it?
[00:46:23.700 --> 00:46:28.940]   They use spaCy first to pre-process the data.
[00:46:28.940 --> 00:46:31.900]   And in the sub-nodes, I remember they had written this.
[00:46:31.900 --> 00:46:34.540]   So they tokenize using spaCy
[00:46:34.540 --> 00:46:37.380]   and then convert to lowercase.
[00:46:37.380 --> 00:46:41.460]   Then they prune the corpus.
[00:46:41.460 --> 00:46:45.860]   So again, the dataset comes from Enron corpus,
[00:46:45.860 --> 00:46:48.260]   which is a bunch of emails.
[00:46:48.260 --> 00:46:51.100]   They pre-process it and then remove sentences
[00:46:51.100 --> 00:46:54.460]   that are short or have mostly numbers
[00:46:54.460 --> 00:46:58.980]   or have email addresses and had repeated occurrences.
[00:46:58.980 --> 00:47:00.820]   So basically they remove all the garbage
[00:47:00.820 --> 00:47:03.100]   in pre-processing, that's what it sounds like.
[00:47:03.100 --> 00:47:11.820]   And then they use a politeness classifier
[00:47:11.820 --> 00:47:16.380]   to assign politeness scores into 10 buckets.
[00:47:16.380 --> 00:47:22.100]   So they read these from P0 to P9,
[00:47:22.100 --> 00:47:26.300]   P9 being the most polite, P0 being the least polite.
[00:47:26.300 --> 00:47:28.700]   And they do this by using a classifier there.
[00:47:29.220 --> 00:47:34.220]   So to be conscious of time, I'm moving further,
[00:47:34.220 --> 00:47:38.900]   but this was all I wanted to cover here as well.
[00:47:38.900 --> 00:47:40.940]   If you're curious, you should check out this graph
[00:47:40.940 --> 00:47:43.100]   where they show the P0 bucket.
[00:47:43.100 --> 00:47:44.860]   Again, it's a bit blurred
[00:47:44.860 --> 00:47:47.180]   because OneNote doesn't like high resolution
[00:47:47.180 --> 00:47:50.740]   and here's the P9 bucket, here's the P0 bucket.
[00:47:50.740 --> 00:47:55.660]   So for some reason, the classifier doesn't like the word par.
[00:47:57.180 --> 00:47:59.220]   California, interestingly enough,
[00:47:59.220 --> 00:48:02.540]   gas company, electricity, prize as well.
[00:48:02.540 --> 00:48:06.660]   Polite words are quite obvious to me,
[00:48:06.660 --> 00:48:11.660]   great, hope, thanks, good, no, stuff like that.
[00:48:11.660 --> 00:48:13.060]   So all of this makes sense.
[00:48:13.060 --> 00:48:20.220]   Just checking my notes to make sure
[00:48:20.220 --> 00:48:22.340]   is there anything else I want to highlight here?
[00:48:22.340 --> 00:48:23.260]   I don't think so.
[00:48:23.260 --> 00:48:25.500]   So continue further.
[00:48:25.940 --> 00:48:29.740]   So here they've highlighted
[00:48:29.740 --> 00:48:31.260]   how they worked on the other tasks.
[00:48:31.260 --> 00:48:34.100]   This is quite straightforward, so I'll continue further.
[00:48:34.100 --> 00:48:39.100]   I'll talk about the methodology
[00:48:39.100 --> 00:48:41.020]   with a suggestion for everyone.
[00:48:41.020 --> 00:48:44.260]   So because of the nature of how papers are written,
[00:48:44.260 --> 00:48:48.140]   the authors use notation like X1, X1.
[00:48:48.140 --> 00:48:51.500]   I'm even forgetting how this is called,
[00:48:51.500 --> 00:48:53.980]   what this is called, but X1 cap.
[00:48:54.980 --> 00:48:59.140]   I think because of the limitations
[00:48:59.140 --> 00:49:04.140]   or the way how a paper is written,
[00:49:04.140 --> 00:49:06.260]   the authors use this.
[00:49:06.260 --> 00:49:07.540]   If you're trying to understand it,
[00:49:07.540 --> 00:49:10.500]   just write on a piece of paper,
[00:49:10.500 --> 00:49:16.580]   polite sentence, non-polite, target polite.
[00:49:16.580 --> 00:49:18.660]   Things will instantly start to make sense.
[00:49:18.660 --> 00:49:20.740]   Maybe all are smarter and you don't need to do that.
[00:49:20.740 --> 00:49:22.220]   You can keep this in your head.
[00:49:22.220 --> 00:49:25.780]   For me, I just couldn't understand what's,
[00:49:25.780 --> 00:49:28.860]   sorry about that.
[00:49:28.860 --> 00:49:31.580]   I press Control + Z too many times
[00:49:31.580 --> 00:49:33.060]   and it scrolled to the bottom.
[00:49:33.060 --> 00:49:42.220]   So when you're trying to understand this,
[00:49:42.220 --> 00:49:44.220]   just if you have a problem,
[00:49:44.220 --> 00:49:45.780]   just replace it with the set words
[00:49:45.780 --> 00:49:47.580]   and it'll start to make sense.
[00:49:47.580 --> 00:49:50.420]   So they say we are given,
[00:49:50.420 --> 00:49:52.060]   here's the method that they use.
[00:49:52.060 --> 00:49:57.020]   Again, one of the differences that I was expecting
[00:49:57.020 --> 00:49:59.300]   to come out from the question was,
[00:49:59.300 --> 00:50:02.180]   usually when you're doing a transfer learning
[00:50:02.180 --> 00:50:04.540]   or style transfer or translation,
[00:50:04.540 --> 00:50:07.780]   you have a parallel set of words.
[00:50:07.780 --> 00:50:10.140]   So English to French, English to German,
[00:50:10.140 --> 00:50:12.340]   I think that's the standard dataset.
[00:50:12.340 --> 00:50:13.340]   That's not the case here.
[00:50:13.340 --> 00:50:17.220]   So it's a non-parallel collection of sentences.
[00:50:17.220 --> 00:50:22.220]   They are given a non-parallel samples of sentences
[00:50:22.220 --> 00:50:28.260]   from styles, non-polite and polite.
[00:50:28.260 --> 00:50:34.940]   So you have two samples and your goal is
[00:50:34.940 --> 00:50:38.420]   to take this and convert this to P hat.
[00:50:38.420 --> 00:50:40.740]   Thanks for that answer, Amit.
[00:50:40.740 --> 00:50:42.060]   Or P hyphen.
[00:50:42.060 --> 00:50:43.460]   Let's just write P hat here.
[00:50:43.460 --> 00:50:48.460]   And they said this is done by using style SV
[00:50:48.460 --> 00:50:55.900]   where V belongs to one or two.
[00:50:55.900 --> 00:50:56.740]   What is one?
[00:50:56.740 --> 00:50:59.260]   Let's say this, let's say it's this.
[00:50:59.260 --> 00:51:00.300]   And two is this.
[00:51:00.300 --> 00:51:09.060]   So they begin by learning a set of phrases
[00:51:09.060 --> 00:51:13.020]   which characterize either this style or this style.
[00:51:14.020 --> 00:51:18.500]   And then they propose the approach
[00:51:18.500 --> 00:51:20.420]   of having a two-stage model,
[00:51:20.420 --> 00:51:23.020]   which we've already looked at, so I'll skip further.
[00:51:23.020 --> 00:51:29.020]   And this allows them to predict polite sentences.
[00:51:29.020 --> 00:51:33.500]   The key thing here is,
[00:51:33.500 --> 00:51:38.540]   they train two independent systems
[00:51:38.540 --> 00:51:41.340]   for the tagger and generator.
[00:51:42.340 --> 00:51:45.260]   So these are two different models
[00:51:45.260 --> 00:51:47.220]   and it's not an end-to-end process.
[00:51:47.220 --> 00:51:52.300]   We've already covered this, so I'm just reading ahead
[00:51:52.300 --> 00:51:54.620]   if there's anything I want to highlight.
[00:51:54.620 --> 00:51:57.620]   But no, we've talked about this extensively.
[00:51:57.620 --> 00:52:02.140]   Key thing, tagger and generator are two separate systems.
[00:52:02.140 --> 00:52:07.460]   I'm rushing through because we're out of time
[00:52:07.460 --> 00:52:09.700]   and I don't want to go over.
[00:52:09.700 --> 00:52:12.620]   But again, ask any question you might want to.
[00:52:12.620 --> 00:52:16.740]   So again, we've gone through this approach.
[00:52:16.740 --> 00:52:19.340]   I'll glance over it again real quick.
[00:52:19.340 --> 00:52:23.820]   Stage one, the tagger adds,
[00:52:23.820 --> 00:52:26.140]   so there are two different possibilities.
[00:52:26.140 --> 00:52:28.060]   In the first sentence, which is really blurred,
[00:52:28.060 --> 00:52:29.460]   so I'll read it out.
[00:52:29.460 --> 00:52:31.220]   It says, "Send me the data."
[00:52:31.220 --> 00:52:34.340]   And here the tagger adds a tag ahead of it.
[00:52:34.340 --> 00:52:36.420]   And then you replace it.
[00:52:37.900 --> 00:52:40.380]   The other one is a replace tagger.
[00:52:40.380 --> 00:52:42.900]   So it identifies words that are negative.
[00:52:42.900 --> 00:52:48.620]   And then the replace tagger replaces these words
[00:52:48.620 --> 00:52:50.700]   with the set target.
[00:52:50.700 --> 00:52:53.780]   So there are two possibilities
[00:52:53.780 --> 00:52:56.180]   of how the tagger and generator work.
[00:52:56.180 --> 00:53:06.740]   Okay, now continuing further into the implementation.
[00:53:07.740 --> 00:53:11.740]   They talk about how do they estimate the style phrases?
[00:53:11.740 --> 00:53:16.740]   How do they do the style invariant tag sentence generation?
[00:53:16.740 --> 00:53:23.060]   And then how do they do the style targeted generation?
[00:53:23.060 --> 00:53:27.180]   I will rush through this a bit.
[00:53:27.180 --> 00:53:29.780]   If you end up reading the paper, which I hope you should,
[00:53:29.780 --> 00:53:32.100]   it's a great paper.
[00:53:32.100 --> 00:53:34.900]   What was helpful for me was just glancing over the math
[00:53:34.900 --> 00:53:36.660]   and looking at the source code.
[00:53:36.660 --> 00:53:39.260]   So the source code is linked at the top of the paper.
[00:53:39.260 --> 00:53:44.260]   If you read it, TF-IDF, for me, at least,
[00:53:44.260 --> 00:53:46.100]   made more sense in the Pythonic code
[00:53:46.100 --> 00:53:49.460]   than this scary jargon.
[00:53:49.460 --> 00:53:53.900]   So this defines how this works in math,
[00:53:53.900 --> 00:53:57.260]   how this approach works for style,
[00:53:57.260 --> 00:53:58.900]   estimating style phrases.
[00:53:58.900 --> 00:54:00.780]   But if you look at the code,
[00:54:00.780 --> 00:54:02.540]   for me, it was more intuitive like that.
[00:54:02.540 --> 00:54:05.780]   The code, again, is very well documented by the authors.
[00:54:05.780 --> 00:54:08.980]   So for the given corpus pair,
[00:54:08.980 --> 00:54:12.580]   polite and nonpolite sentences in the set style,
[00:54:12.580 --> 00:54:16.020]   they compute a probability distribution
[00:54:16.020 --> 00:54:19.420]   present in both the corpora.
[00:54:19.420 --> 00:54:23.180]   And they do this by using some math magic,
[00:54:23.180 --> 00:54:25.940]   which involves TF-IDF and styles.
[00:54:25.940 --> 00:54:28.700]   (mouse clicking)
[00:54:28.700 --> 00:54:38.340]   I'll go, I'll skip the style invariant tag sentence as well,
[00:54:38.340 --> 00:54:42.620]   because again, it's obvious as long as you skip the math.
[00:54:42.620 --> 00:54:46.620]   I did that, it worked okay for me.
[00:54:46.620 --> 00:54:49.780]   And here they talk about how are they creating
[00:54:49.780 --> 00:54:51.780]   training data for R-Tagger.
[00:54:53.740 --> 00:54:55.100]   So for this image,
[00:54:55.100 --> 00:54:58.900]   they take the input sentence
[00:54:58.900 --> 00:55:04.820]   and remove the words where they want to add a tag.
[00:55:04.820 --> 00:55:07.580]   And then in the target output,
[00:55:07.580 --> 00:55:11.060]   they replace it with the tag phrase.
[00:55:11.060 --> 00:55:13.140]   So this happens inside the model.
[00:55:13.140 --> 00:55:15.940]   And we only see the output,
[00:55:15.940 --> 00:55:19.380]   but internally, it first removes the sentences
[00:55:19.380 --> 00:55:20.540]   and then adds a tag.
[00:55:20.540 --> 00:55:22.820]   Basically, that's how replacement works.
[00:55:22.820 --> 00:55:24.020]   If it's a two-step process,
[00:55:24.020 --> 00:55:25.620]   that is how it's being replaced.
[00:55:25.620 --> 00:55:33.100]   I'll skip over style targeted generation
[00:55:33.100 --> 00:55:35.620]   because it's not that relevant.
[00:55:35.620 --> 00:55:37.900]   So coming to experiments and results,
[00:55:37.900 --> 00:55:45.420]   they compare the systems against DRG,
[00:55:45.420 --> 00:55:47.980]   style transfer through back translation,
[00:55:47.980 --> 00:55:50.540]   and style transfer from non-parallel text
[00:55:50.540 --> 00:55:51.660]   by cross-alignment.
[00:55:52.660 --> 00:55:55.660]   I haven't read these papers.
[00:55:55.660 --> 00:55:58.660]   So I understand they are relevant to this one,
[00:55:58.660 --> 00:56:03.060]   but again, I don't have the relevant context to share here.
[00:56:03.060 --> 00:56:05.580]   If you want, you're welcome to read this.
[00:56:05.580 --> 00:56:07.260]   The reason I said that was,
[00:56:07.260 --> 00:56:09.860]   one thing I've learned by following the citation graph
[00:56:09.860 --> 00:56:11.260]   of most papers is,
[00:56:11.260 --> 00:56:14.420]   it's not always helpful.
[00:56:14.420 --> 00:56:16.060]   Most of the times, if you just read
[00:56:16.060 --> 00:56:19.020]   that one sentence summary inside the paper,
[00:56:19.020 --> 00:56:20.780]   you don't need to go off and read the paper.
[00:56:20.780 --> 00:56:22.860]   I've had a phase where I would try to read
[00:56:22.860 --> 00:56:24.220]   the relevant papers.
[00:56:24.220 --> 00:56:27.540]   One thing I've learned is,
[00:56:27.540 --> 00:56:29.300]   most of the times you don't have to,
[00:56:29.300 --> 00:56:31.260]   if you just read that one line
[00:56:31.260 --> 00:56:34.940]   where the paper is summarized,
[00:56:34.940 --> 00:56:35.780]   that's good enough.
[00:56:35.780 --> 00:56:37.660]   So don't go off and read the paper,
[00:56:37.660 --> 00:56:40.660]   unless you're curious, then by all means.
[00:56:40.660 --> 00:56:42.220]   I'm sure there are great papers.
[00:56:42.220 --> 00:56:44.340]   I'm not saying they're not good papers.
[00:56:44.340 --> 00:56:46.980]   If you're short on time, you don't have to read them.
[00:56:49.660 --> 00:56:52.220]   So I've covered this already.
[00:56:52.220 --> 00:56:54.380]   They use four-layer transformer
[00:56:54.380 --> 00:56:57.500]   for both tagger and generator modules.
[00:56:57.500 --> 00:56:59.740]   Each has four retention heads
[00:56:59.740 --> 00:57:02.420]   and five and two dimensional embeddings
[00:57:02.420 --> 00:57:04.900]   and hidden state size.
[00:57:04.900 --> 00:57:07.180]   They also say the dropout value,
[00:57:07.180 --> 00:57:08.540]   which is really cool to see that
[00:57:08.540 --> 00:57:12.180]   they're actually sharing so many details here.
[00:57:12.180 --> 00:57:16.380]   And to highlight the numbers real quick,
[00:57:17.700 --> 00:57:18.660]   as you can see,
[00:57:18.660 --> 00:57:24.140]   it is, at that time at least,
[00:57:24.140 --> 00:57:26.260]   beating all of the three set approaches.
[00:57:26.260 --> 00:57:33.780]   During inference, as Yuvraj had already shared,
[00:57:33.780 --> 00:57:35.380]   some people use BeamSearch.
[00:57:35.380 --> 00:57:40.060]   And here they're using BeamSearch
[00:57:40.060 --> 00:57:42.700]   to decode tag sentences and targeted generation
[00:57:42.700 --> 00:57:44.860]   for tagger and generator respectively.
[00:57:45.860 --> 00:57:48.860]   Now they use a few approaches for evaluating.
[00:57:48.860 --> 00:57:52.060]   One is an automated evaluation approach.
[00:57:52.060 --> 00:57:55.260]   And the user classifier that is an AWD-LSTM.
[00:57:55.260 --> 00:57:59.180]   If you are familiar with this,
[00:57:59.180 --> 00:58:01.060]   maybe you're coming from FastAI,
[00:58:01.060 --> 00:58:04.100]   and if you are, they actually use the FastAI implementation.
[00:58:04.100 --> 00:58:07.260]   So they use the implementation provided by FastAI
[00:58:07.260 --> 00:58:08.980]   to do the automated evaluation.
[00:58:13.060 --> 00:58:15.540]   They also do a human evaluation
[00:58:15.540 --> 00:58:17.780]   where it's given to judges
[00:58:17.780 --> 00:58:21.220]   who look at content preservation,
[00:58:21.220 --> 00:58:23.260]   grammatical correctness,
[00:58:23.260 --> 00:58:25.900]   and target attribute match and give scores.
[00:58:25.900 --> 00:58:30.420]   And then they also do qualitative analysis.
[00:58:30.420 --> 00:58:32.620]   So they try to evaluate their model
[00:58:32.620 --> 00:58:35.060]   by a lot of approaches, as you can see.
[00:58:35.060 --> 00:58:37.140]   And I think it's,
[00:58:37.140 --> 00:58:39.940]   I think it's a really good example
[00:58:39.940 --> 00:58:42.780]   and I think it's great
[00:58:42.780 --> 00:58:45.180]   that they have covered all of these approaches as well
[00:58:45.180 --> 00:58:50.180]   apart from just covering older models.
[00:58:50.180 --> 00:58:55.060]   I will skip this again,
[00:58:55.060 --> 00:58:57.020]   but you can take a read through this.
[00:58:57.020 --> 00:59:00.860]   If you want, so they are comparing the DRG output
[00:59:00.860 --> 00:59:02.300]   and their model output.
[00:59:02.300 --> 00:59:06.460]   And also the strategy that's being used here.
[00:59:07.660 --> 00:59:10.580]   I found that it's mostly readable text
[00:59:10.580 --> 00:59:13.860]   and it's not a weird text here.
[00:59:13.860 --> 00:59:16.300]   So you're welcome to read it through if you want.
[00:59:16.300 --> 00:59:25.260]   Also, I don't think there's anything else to summarize
[00:59:25.260 --> 00:59:26.380]   in the remaining two minutes.
[00:59:26.380 --> 00:59:28.220]   So to conclude,
[00:59:28.220 --> 00:59:32.340]   the authors have introduced a task of politeness transfer.
[00:59:35.180 --> 00:59:37.700]   And they use the Enron dataset corpus
[00:59:37.700 --> 00:59:41.260]   by introducing a tag and generate approach,
[00:59:41.260 --> 00:59:42.980]   which is a two-staged approach
[00:59:42.980 --> 00:59:44.980]   for content preserving style transfer.
[00:59:44.980 --> 00:59:48.620]   Where the source
[00:59:48.620 --> 00:59:53.100]   is neutral, like non-polite,
[00:59:53.100 --> 00:59:57.060]   class and the,
[00:59:57.060 --> 00:59:59.780]   class in the cases of politeness transfer.
[00:59:59.780 --> 01:00:04.260]   Automatic and human evaluation
[01:00:04.260 --> 01:00:07.300]   show that the approach outperforms other SOTA models
[01:00:07.300 --> 01:00:10.180]   at that time, might have changed now.
[01:00:10.180 --> 01:00:11.900]   And their model works really well.
[01:00:11.900 --> 01:00:17.140]   I think there was one point that was relevant here.
[01:00:17.140 --> 01:00:20.900]   So they also have additional qualitative examples
[01:00:20.900 --> 01:00:23.540]   and other things that they have done here.
[01:00:23.540 --> 01:00:26.980]   So I think they've converted from
[01:00:26.980 --> 01:00:32.020]   one gender to the other in the sentences.
[01:00:32.020 --> 01:00:35.980]   They have, I think, converted from facts to,
[01:00:35.980 --> 01:00:40.140]   I think it's romanticized,
[01:00:40.140 --> 01:00:42.260]   and then facts to humors.
[01:00:42.260 --> 01:00:43.820]   So these are the said approaches
[01:00:43.820 --> 01:00:47.100]   that it can be applied to beyond what the model does.
[01:00:47.100 --> 01:00:56.580]   Coming back to this.
[01:00:56.580 --> 01:01:01.620]   So I always end up with some suggested homework.
[01:01:01.620 --> 01:01:05.860]   And I would say try this approach on different problems.
[01:01:05.860 --> 01:01:10.220]   Implement the same as an end-to-end process.
[01:01:10.220 --> 01:01:12.900]   So just throw a transformer at this and see what happens.
[01:01:12.900 --> 01:01:16.060]   Maybe it outperforms, and you could write a paper
[01:01:16.060 --> 01:01:18.660]   that's better than Cornell's papers.
[01:01:18.660 --> 01:01:21.780]   I think all of the authors are from Cornell's research labs.
[01:01:21.780 --> 01:01:24.940]   I don't think that might be true
[01:01:24.940 --> 01:01:27.900]   because they would have thought for it, thought about it.
[01:01:27.900 --> 01:01:29.220]   I would encourage you to download
[01:01:29.220 --> 01:01:31.700]   and play with the dataset as well.
[01:01:31.700 --> 01:01:33.460]   And what do you do after you've done all of this?
[01:01:33.460 --> 01:01:36.100]   Please consider writing about it.
[01:01:36.100 --> 01:01:38.780]   We at Weights & Biases really like to support this.
[01:01:38.780 --> 01:01:40.820]   When you write it, share it on social,
[01:01:40.820 --> 01:01:42.300]   tag me or tag Weights & Biases,
[01:01:42.300 --> 01:01:45.420]   and we'll send some swag love your way.
[01:01:45.420 --> 01:01:50.660]   I have asked the team to send Yuvraj multiple swag
[01:01:50.660 --> 01:01:51.980]   over the few weeks, and they're like,
[01:01:51.980 --> 01:01:54.540]   "No, we've sent him some stuff already."
[01:01:54.540 --> 01:01:58.260]   So, Yuvraj, I'll still continue reading your paper
[01:01:58.260 --> 01:02:00.740]   and your blog posts and fighting for that.
[01:02:00.740 --> 01:02:04.660]   But for others, you should check out Yuvraj's work
[01:02:04.660 --> 01:02:07.300]   or consider writing blog posts,
[01:02:07.300 --> 01:02:11.380]   not just for the sake of swag.
[01:02:11.380 --> 01:02:12.980]   That's more to encourage you,
[01:02:12.980 --> 01:02:15.460]   but that's the reason why we push for it
[01:02:15.460 --> 01:02:17.980]   is to really help you build a portfolio
[01:02:17.980 --> 01:02:21.420]   of technical writing, which I think is really helpful.
[01:02:21.420 --> 01:02:23.620]   That's a discussion for another time.
[01:02:23.620 --> 01:02:25.820]   But that's it for the paper reading group.
[01:02:25.820 --> 01:02:28.180]   I'll wait for 30 seconds.
[01:02:28.180 --> 01:02:29.380]   We've gone two minutes over.
[01:02:29.380 --> 01:02:30.220]   Sorry about that.
[01:02:30.220 --> 01:02:32.220]   I'll wait for 30 seconds to answer any questions.
[01:02:32.220 --> 01:02:33.860]   If not, I'll wrap up.
[01:02:33.860 --> 01:02:42.140]   And to the people that might be here still,
[01:02:42.140 --> 01:02:45.660]   just announcing this here, keep an eye out.
[01:02:45.660 --> 01:02:47.100]   We'll be reading this book next
[01:02:47.100 --> 01:02:50.220]   in part of our paper reading groups.
[01:02:50.220 --> 01:02:52.580]   Not paper reading groups, book reading groups.
[01:02:52.580 --> 01:02:55.100]   I'm anxious because we're out of time
[01:02:55.100 --> 01:02:57.900]   and I absolutely hate not being on time.
[01:02:57.900 --> 01:03:01.780]   So we'll be studying the NLP with Transformers book
[01:03:01.780 --> 01:03:03.060]   from the Hugging Face team.
[01:03:03.060 --> 01:03:07.460]   I think it's, I'm not sure if Leandro is from Hugging Face.
[01:03:07.460 --> 01:03:08.300]   Most probably they are,
[01:03:08.300 --> 01:03:11.620]   but really great authors have written this book.
[01:03:11.620 --> 01:03:14.740]   I'm four chapters through at this point.
[01:03:14.740 --> 01:03:16.380]   I like to read the book at least once
[01:03:16.380 --> 01:03:19.220]   before starting a book reading group.
[01:03:19.220 --> 01:03:21.140]   It'll be announced in one or two weeks.
[01:03:21.140 --> 01:03:22.780]   We'll start in one or two weeks.
[01:03:22.780 --> 01:03:25.460]   If you're interested, keep an eye out.
[01:03:25.460 --> 01:03:26.500]   For the paper reading group,
[01:03:26.500 --> 01:03:29.300]   we'll meet again in one month.
[01:03:29.300 --> 01:03:31.500]   Please consider writing about this stuff.
[01:03:31.500 --> 01:03:32.340]   Thanks for watching.
[01:03:32.340 --> 01:03:33.340]   I don't see any questions.
[01:03:33.340 --> 01:03:38.060]   So maybe I can wrap up.
[01:03:38.060 --> 01:03:40.020]   Amit is asking for your Rogers blog post.
[01:03:40.020 --> 01:03:42.500]   So let me see if I can find that thing real quick.
[01:03:42.500 --> 01:03:51.260]   Share my screen, share the window.
[01:03:54.860 --> 01:03:59.420]   This is his Twitter feed where he shares most of the stuff.
[01:03:59.420 --> 01:04:00.260]   Here's the link.
[01:04:00.260 --> 01:04:03.020]   Everyone watching, please follow him there
[01:04:03.020 --> 01:04:05.020]   because his stuff is absolutely amazing.
[01:04:05.020 --> 01:04:09.300]   Yuvraj, can you share your blog's link here?
[01:04:09.300 --> 01:04:12.220]   I don't see it in your bio.
[01:04:12.220 --> 01:04:17.220]   But if you follow Yuvraj on Twitter,
[01:04:17.220 --> 01:04:19.540]   you'll be sorted because that's where he tweets his stuff.
[01:04:19.540 --> 01:04:21.980]   So I don't want to scroll through his timeline
[01:04:21.980 --> 01:04:25.500]   and embarrass him because I don't think
[01:04:25.500 --> 01:04:27.940]   cricket is embarrassing, but yeah.
[01:04:27.940 --> 01:04:29.940]   Please follow him on Twitter
[01:04:29.940 --> 01:04:31.900]   and you'll find his blog post there.
[01:04:31.900 --> 01:04:35.180]   Amazing, I don't see any questions.
[01:04:35.180 --> 01:04:36.580]   So thank you for joining everyone.
[01:04:36.580 --> 01:04:38.020]   I'll see you in one month
[01:04:38.020 --> 01:04:39.980]   if you just joined the paper reading group
[01:04:39.980 --> 01:04:42.940]   or in two weeks if you join the book reading group
[01:04:42.940 --> 01:04:46.540]   or this week if you join the Chai livestream.
[01:04:46.540 --> 01:04:48.420]   I'll be doing a Chai livestream this week.
[01:04:48.420 --> 01:04:50.460]   So thanks for joining.
[01:04:50.460 --> 01:04:54.180]   Hope you have a good remaining Sunday
[01:04:54.180 --> 01:04:56.140]   or Sunday night if you're from India.
[01:04:56.140 --> 01:05:00.780]   And yeah, I'll see you in any of our livestreams.
[01:05:00.780 --> 01:05:01.780]   Thanks for watching.
[01:05:01.780 --> 01:05:04.180]   (Music ends)

