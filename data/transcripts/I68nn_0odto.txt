
[00:00:00.000 --> 00:00:05.000]   Should be fine.
[00:00:05.000 --> 00:00:06.000]   Cool.
[00:00:06.000 --> 00:00:12.480]   So, hey everybody and welcome to another paper reading group today and I'm so excited to
[00:00:12.480 --> 00:00:16.760]   share that this is the first multimodal paper that we're discussing and we're also discussing
[00:00:16.760 --> 00:00:22.760]   MDTR, which is modulated detection for end-to-end multimodal understanding.
[00:00:22.760 --> 00:00:28.680]   So the last time in paper reading group we discussed DETR, which is the detection transformer.
[00:00:28.680 --> 00:00:33.880]   And today we're doing MDTR, which is a step further, which takes DETR to another level
[00:00:33.880 --> 00:00:38.000]   and it starts to mix computer vision and NLP.
[00:00:38.000 --> 00:00:40.680]   And we have Aishwarya Kamath joining us.
[00:00:40.680 --> 00:00:46.280]   So she's the paper author for MDTR and hey Aishwarya.
[00:00:46.280 --> 00:00:47.820]   Hi everyone.
[00:00:47.820 --> 00:00:48.820]   Thanks for joining.
[00:00:48.820 --> 00:00:54.920]   So Aishwarya, I'm so impressed and so excited as well to have Aishwarya here today.
[00:00:54.920 --> 00:01:00.000]   She's a second year PhD student at New York University Center of Data Science and she's
[00:01:00.000 --> 00:01:03.920]   advised by Professor Yann LeCun and Professor Kyung-Hoon Cho.
[00:01:03.920 --> 00:01:04.920]   Is that a correct pronunciation?
[00:01:04.920 --> 00:01:05.920]   We can correct me later.
[00:01:05.920 --> 00:01:08.120]   Yeah, it's Kyung-Hoon Cho.
[00:01:08.120 --> 00:01:12.760]   And has a master's degree in computer science from University of Massachusetts Amherst.
[00:01:12.760 --> 00:01:18.280]   And she's working at the intersection of NLP and computer vision and her interests lie
[00:01:18.280 --> 00:01:23.880]   in leveraging information from multiple sources, such as text, images, video, and speech to
[00:01:23.880 --> 00:01:28.240]   improve common sense reasoning capabilities of machines.
[00:01:28.240 --> 00:01:31.240]   So basically you've been working on multimodal for quite some time Aishwarya.
[00:01:31.240 --> 00:01:33.320]   That's so exciting to hear.
[00:01:33.320 --> 00:01:38.080]   And she's previously completed internships at Google Research and Facebook AI Research
[00:01:38.080 --> 00:01:43.680]   as well as worked full time as an ML engineer at Oracle's Machine Learning Research Group.
[00:01:43.680 --> 00:01:48.840]   Her work has received a Best Paper Award at the Representation Learning for NLP Workshop
[00:01:48.840 --> 00:01:51.960]   at ACL 2019.
[00:01:51.960 --> 00:01:53.520]   That's really a lot Aishwarya.
[00:01:53.520 --> 00:01:56.600]   So please tell us about your journey.
[00:01:56.600 --> 00:01:57.600]   I'm so excited.
[00:01:57.600 --> 00:02:04.120]   Before we get started with the paper, maybe tell us how that journey started and how you
[00:02:04.120 --> 00:02:05.120]   got into multimodal.
[00:02:05.120 --> 00:02:06.120]   Yeah, sure.
[00:02:06.120 --> 00:02:09.120]   So I guess journey starting was in India.
[00:02:09.120 --> 00:02:16.040]   I did my undergrad at Manipal Institute of Technology in electronics.
[00:02:16.040 --> 00:02:18.040]   So not much programming.
[00:02:18.040 --> 00:02:20.600]   It was more on the hardware side.
[00:02:20.600 --> 00:02:26.160]   Towards the end of the undergrad, I started doing these online Coursera courses and things
[00:02:26.160 --> 00:02:27.160]   like that.
[00:02:27.160 --> 00:02:28.160]   It was like, oh, machine learning is cool.
[00:02:28.160 --> 00:02:30.320]   TensorFlow was out there by then.
[00:02:30.320 --> 00:02:37.080]   So yeah, I applied to Computer Science Masters and that's where I got my first experience
[00:02:37.080 --> 00:02:38.360]   with research.
[00:02:38.360 --> 00:02:45.640]   So Andrew McCallum was my research mentor at UMass and he basically just changed what
[00:02:45.640 --> 00:02:47.960]   I wanted to do in life.
[00:02:47.960 --> 00:02:50.920]   Because at that point, I was like, yeah, I'm going to do a master's and I get a job and
[00:02:50.920 --> 00:02:51.920]   that'd be fine.
[00:02:51.920 --> 00:02:54.520]   But and you know, that's going to be my life.
[00:02:54.520 --> 00:02:57.920]   But basically, he was like, oh, no, no, do research.
[00:02:57.920 --> 00:02:58.920]   This is really fun.
[00:02:58.920 --> 00:03:04.760]   And he like, you know, mentored me and like, also helped me go to like, NeurIPS, which
[00:03:04.760 --> 00:03:09.640]   was like my first conference in like 2017 and introduced me to a lot of cool people
[00:03:09.640 --> 00:03:13.560]   and showed me like the study life of research.
[00:03:13.560 --> 00:03:15.360]   I was like, wow, this is really awesome.
[00:03:15.360 --> 00:03:19.120]   There's so many cool and smart people who also like know how to have a good time and
[00:03:19.120 --> 00:03:20.120]   stuff like that.
[00:03:20.120 --> 00:03:26.440]   So yeah, it was that was like the beginning and I got my first paper at UMass, which was
[00:03:26.440 --> 00:03:31.840]   a short paper, but that was like mostly NLP and at NACL in 2018.
[00:03:31.840 --> 00:03:36.400]   And then once I started working at Oracle, I explored more like on the industry side
[00:03:36.400 --> 00:03:41.520]   how to apply things and I realized I wanted a bit more freedom in like in terms of what
[00:03:41.520 --> 00:03:42.840]   I like to work on.
[00:03:42.840 --> 00:03:48.400]   So I applied for a PhD after like working on a bunch of research projects with some
[00:03:48.400 --> 00:03:52.840]   friends, you know, to like improve my PhD application.
[00:03:52.840 --> 00:03:58.200]   So yeah, NYU happened after that because I had an interview with Jan and we discussed
[00:03:58.200 --> 00:04:01.160]   a bunch of stuff about like common sense reasoning and multimodality.
[00:04:01.160 --> 00:04:05.720]   And he was like super excited about like, you know, going beyond just image or just
[00:04:05.720 --> 00:04:10.120]   text and using self supervised learning and also putting it together with like multimodality
[00:04:10.120 --> 00:04:13.280]   and yeah, going along this path.
[00:04:13.280 --> 00:04:15.960]   So I started working on this after I joined NYU.
[00:04:15.960 --> 00:04:17.840]   So yeah, that's my.
[00:04:17.840 --> 00:04:18.840]   That's really exciting.
[00:04:18.840 --> 00:04:24.080]   I've always meant to ask, but never got an option to, but how's Jan like in real life?
[00:04:24.080 --> 00:04:25.640]   He's super chill.
[00:04:25.640 --> 00:04:30.360]   He's like, yeah, he's, he's like, really, really nice and understanding and like, you
[00:04:30.360 --> 00:04:34.560]   know, I never feel like in the beginning I was like, oh my God, during award winner,
[00:04:34.560 --> 00:04:36.560]   how am I ever going to talk to him, right?
[00:04:36.560 --> 00:04:42.040]   Like this year, especially like that interview, which I had with him where he told me like
[00:04:42.040 --> 00:04:44.880]   during the interview, oh, you know, I'm going to be taking a PhD student.
[00:04:44.880 --> 00:04:46.760]   Do you want to be a PhD student with me?
[00:04:46.760 --> 00:04:47.760]   And I was like, what?
[00:04:47.760 --> 00:04:48.760]   Did you just say that?
[00:04:48.760 --> 00:04:49.760]   Cause I was totally not expecting it.
[00:04:49.760 --> 00:04:50.760]   And it was just like out of the blue.
[00:04:50.760 --> 00:04:51.760]   And yeah, so I was super excited.
[00:04:51.760 --> 00:04:58.640]   And since then, I think like we have a pretty good relationship.
[00:04:58.640 --> 00:05:04.600]   He is very open to like me trying out ideas or like, you know, pursuing my own direction.
[00:05:04.600 --> 00:05:09.240]   So he like has some high level of interest that, you know, these energy-based models
[00:05:09.240 --> 00:05:13.800]   and regularize energy-based models for like different kinds of tasks.
[00:05:13.800 --> 00:05:16.720]   So he doesn't really care about what task you work on.
[00:05:16.720 --> 00:05:21.800]   So, you know, at that point he wasn't like that much into NLP and I was, so he was like,
[00:05:21.800 --> 00:05:23.800]   yeah, we can like find common interests.
[00:05:23.800 --> 00:05:28.400]   And he did try to like find a project which, you know, overlap with his interests and mine,
[00:05:28.400 --> 00:05:29.720]   which was super nice.
[00:05:29.720 --> 00:05:34.760]   And then during my internship last year at FAIR with him, I started exploring more of
[00:05:34.760 --> 00:05:36.920]   the multi-modal area.
[00:05:36.920 --> 00:05:37.920]   So yeah.
[00:05:37.920 --> 00:05:38.920]   Wow.
[00:05:38.920 --> 00:05:41.560]   That is really exciting to hear.
[00:05:41.560 --> 00:05:42.560]   Thanks for that introduction, Aishwarya.
[00:05:42.560 --> 00:05:43.880]   I really appreciate it.
[00:05:43.880 --> 00:05:49.720]   So for everybody, if you have questions for Aishwarya like I have, this is the link 1db.me/mdetr.
[00:05:49.720 --> 00:05:55.280]   So let me just show you where that will take you.
[00:05:55.280 --> 00:05:58.000]   So if I go to that link, that's the wrong place.
[00:05:58.000 --> 00:05:59.000]   One second.
[00:05:59.000 --> 00:06:00.000]   So 1db.me/mdtr.
[00:06:00.000 --> 00:06:06.400]   So that should take you to the Fast.ai forums.
[00:06:06.400 --> 00:06:08.400]   That's where we've been hosting all our discussions.
[00:06:08.400 --> 00:06:11.120]   So you'll see this link mdtr.
[00:06:11.120 --> 00:06:13.640]   And then if you go there, you can just click reply.
[00:06:13.640 --> 00:06:17.480]   And I'm just going to do this test and click reply.
[00:06:17.480 --> 00:06:18.960]   I think this will make an error.
[00:06:18.960 --> 00:06:20.480]   Yeah, at least 20 characters.
[00:06:20.480 --> 00:06:23.280]   I'm going to make this 20 characters.
[00:06:23.280 --> 00:06:24.280]   All right.
[00:06:24.560 --> 00:06:29.040]   So this is because we're also live on YouTube and we also have Zoom chats.
[00:06:29.040 --> 00:06:31.360]   So it's really hard for me to monitor questions everywhere.
[00:06:31.360 --> 00:06:36.760]   So Aishwarya, what we do is basically as we're going through the papers, we're just going
[00:06:36.760 --> 00:06:40.360]   to have a look at this forum and then that's where all the questions will be.
[00:06:40.360 --> 00:06:43.280]   So this is for the Ask Me Anything section right at the end.
[00:06:43.280 --> 00:06:44.280]   OK, sounds good.
[00:06:44.280 --> 00:06:45.280]   All right.
[00:06:45.280 --> 00:06:52.200]   With that being said, let's start and let's start to dig in this paper, which is MDETR.
[00:06:52.200 --> 00:06:54.760]   So I guess we'll start with an introduction.
[00:06:54.760 --> 00:06:59.120]   And then the first thing when I was reading this paper, figure one was like, that's the
[00:06:59.120 --> 00:07:00.120]   first thing I ever saw.
[00:07:00.120 --> 00:07:04.240]   And I'm like a pink elephant and the model can recognize a pink elephant.
[00:07:04.240 --> 00:07:09.240]   So please tell us, I guess it would be really helpful if you could tell us what is multimodal
[00:07:09.240 --> 00:07:17.080]   understanding and how is MDETR different from DETR, which is just object detection and how
[00:07:17.080 --> 00:07:19.160]   did the work come about?
[00:07:19.160 --> 00:07:20.160]   Sure.
[00:07:20.160 --> 00:07:25.480]   So DETR was like the first object detection model, which went like end to end.
[00:07:25.480 --> 00:07:28.920]   So you can train it like fully end to end.
[00:07:28.920 --> 00:07:31.280]   And I mean, it's solving a different task.
[00:07:31.280 --> 00:07:36.940]   So detection is a classical vision, you know, task it to localize and recognize objects
[00:07:36.940 --> 00:07:37.940]   in the image.
[00:07:37.940 --> 00:07:43.280]   Whereas multimodal reasoning or like multimodal understanding is much more broad term, I would
[00:07:43.280 --> 00:07:46.080]   say, because it like has a lot of more components to it.
[00:07:46.080 --> 00:07:49.960]   You have to understand like if you're right here, we're learning about text and image
[00:07:49.960 --> 00:07:54.500]   multimodal, but of course you can also, you know, do other modalities like speech.
[00:07:54.500 --> 00:07:57.400]   And then if you have videos, you can learn from that as well.
[00:07:57.400 --> 00:08:04.840]   But yeah, so the main idea of our paper was like to extend from what previous work was
[00:08:04.840 --> 00:08:05.840]   based on.
[00:08:05.840 --> 00:08:11.520]   So most of the previous work for multimodal approaches all did object detection as some
[00:08:11.520 --> 00:08:13.320]   sort of pre-processing step.
[00:08:13.320 --> 00:08:16.720]   So it's always like a pipeline approach where you first do detection, you find a bunch of
[00:08:16.720 --> 00:08:21.320]   objects of interest, and then you have this multimodal reasoning or like understanding
[00:08:21.320 --> 00:08:24.340]   phase where you actually align image and text.
[00:08:24.340 --> 00:08:29.640]   So this is problematic because the object detectors all have like a fixed label set,
[00:08:29.640 --> 00:08:30.640]   right?
[00:08:30.640 --> 00:08:34.960]   So they're trained on like Cocoa or VG or whatever, which have a fixed number of object
[00:08:34.960 --> 00:08:35.960]   classes.
[00:08:35.960 --> 00:08:41.640]   So if you want to reason about objects and attributes, which suppose this pink elephant,
[00:08:41.640 --> 00:08:44.360]   right, Cocoa does not have a pink elephant.
[00:08:44.360 --> 00:08:50.000]   So in like your VQA task, suppose you have some question, which is about a pink elephant
[00:08:50.000 --> 00:08:53.960]   and the image has it, there's really no way you can reason about it.
[00:08:53.960 --> 00:08:59.000]   On the other hand, if you like make the object detection part more central to your approach,
[00:08:59.000 --> 00:09:03.960]   which is what we try to do, then you can basically detect anything that's found in free form
[00:09:03.960 --> 00:09:04.960]   text.
[00:09:04.960 --> 00:09:10.280]   So as long as you know, you have some aligned text and object, like you have the alignment
[00:09:10.280 --> 00:09:15.800]   between some text and the object in the image, you can recognize it and you can like reason
[00:09:15.800 --> 00:09:19.000]   over it in the VQA or whatever other tasks that you're interested in.
[00:09:19.000 --> 00:09:23.040]   I mean, there's a whole bunch of multimodal tasks, there's image captioning, there's VQA,
[00:09:23.040 --> 00:09:25.520]   you know, there's phase grounding and segmentation.
[00:09:25.520 --> 00:09:28.320]   So yeah, it's like phrase based segmentation.
[00:09:28.320 --> 00:09:35.720]   So yeah, MDeter was trying to get away from this limitation basically where you have to
[00:09:35.720 --> 00:09:39.760]   have this object detector trained on a bunch of classes and you can only do stuff with
[00:09:39.760 --> 00:09:40.760]   those classes.
[00:09:40.760 --> 00:09:44.760]   That was the motivating idea.
[00:09:44.760 --> 00:09:48.360]   And like in terms of that idea, it's very, I guess, a similar approach to what DETR tried
[00:09:48.360 --> 00:09:52.720]   to do with object detection as a whole, because there were two steps.
[00:09:52.720 --> 00:09:56.160]   You had a region of interest and then that's when the detection would happen.
[00:09:56.160 --> 00:09:59.880]   And then you see MDETR, which is kind of like taking away that bottleneck.
[00:09:59.880 --> 00:10:03.440]   So it's really good to see like things being built one after the other.
[00:10:03.440 --> 00:10:09.240]   I mean, it was like collaboration with Nico and Gab as well, who are like Nico is the
[00:10:09.240 --> 00:10:11.480]   one who wrote the DETR paper.
[00:10:11.480 --> 00:10:16.640]   And he was like basically my mentor during my internship along with Jan, because he was
[00:10:16.640 --> 00:10:19.240]   like working on this for a whole while now.
[00:10:19.240 --> 00:10:22.600]   And yeah, he has a lot of experience.
[00:10:22.600 --> 00:10:28.280]   And initially, actually, when I started working on MDETR, it wasn't called that because I
[00:10:28.280 --> 00:10:30.680]   wasn't trying to do it as modulator detection.
[00:10:30.680 --> 00:10:33.060]   It was more of a multitask model.
[00:10:33.060 --> 00:10:36.480]   So I think there's been like a bunch of papers actually, which ended up doing that in the
[00:10:36.480 --> 00:10:37.960]   last few months.
[00:10:37.960 --> 00:10:42.240]   But that's the first thing I had tried basically to do image captioning, like have a decoder
[00:10:42.240 --> 00:10:46.720]   for captioning and the DETR decoder, which does detection so that there's some sort of
[00:10:46.720 --> 00:10:50.080]   synergy between the two and you have better performance.
[00:10:50.080 --> 00:10:51.080]   It did okay.
[00:10:51.080 --> 00:10:55.920]   Like I would say the results were decent, but it wasn't like clear how to make this
[00:10:55.920 --> 00:10:58.160]   help downstream tasks such as VQA.
[00:10:58.160 --> 00:11:04.720]   And I guess the main, like my main motivation was like learn to make a model where you can
[00:11:04.720 --> 00:11:09.400]   apply it to like a whole bunch of tasks and like, which is in a way that's useful to downstream
[00:11:09.400 --> 00:11:10.400]   tasks.
[00:11:10.400 --> 00:11:15.440]   So that's why we kind of pivoted at some point after the internship, I started looking at
[00:11:15.440 --> 00:11:16.680]   referring expressions.
[00:11:16.680 --> 00:11:20.880]   And then I was like, wait, what if like, you know, you find stuff which is actually referred
[00:11:20.880 --> 00:11:27.280]   to and that's basically what made the direction shift from just using DETR as a detector and
[00:11:27.280 --> 00:11:31.680]   doing detection and captioning to like actually doing modulator detection.
[00:11:31.680 --> 00:11:35.000]   Yeah, that's really exciting to hear.
[00:11:35.000 --> 00:11:38.560]   And I guess this is another thing, just so you mentioned, I just want to highlight that
[00:11:38.560 --> 00:11:39.560]   in the paper as well.
[00:11:39.560 --> 00:11:44.400]   Like exactly what you said, there's like several downstream tasks like phrase counting, referring
[00:11:44.400 --> 00:11:49.280]   expansion, comprehension, segmentation, and then you've achieved this, this paper's achieved
[00:11:49.280 --> 00:11:53.840]   state of the art on a lot of popular benchmarks, which is very exciting.
[00:11:53.840 --> 00:11:59.040]   And I'm really keen to see like what's coming next after MDTR and like where this will lead
[00:11:59.040 --> 00:12:00.040]   to.
[00:12:00.040 --> 00:12:03.600]   So I'm going to leave all of that for the AMA towards the end on what your plans are
[00:12:03.600 --> 00:12:06.280]   after MDTR.
[00:12:06.280 --> 00:12:11.920]   So I guess from the app, so what we do is we just go through the paper section by section.
[00:12:11.920 --> 00:12:16.880]   So in terms of abstract, I guess this was just about providing the main idea about the
[00:12:16.880 --> 00:12:17.880]   paper.
[00:12:17.880 --> 00:12:23.960]   And one thing maybe that I do want to highlight is the pre-training of the network on 1.3
[00:12:23.960 --> 00:12:29.480]   text image pairs, which, and that was a data set that was curated at Facebook.
[00:12:29.480 --> 00:12:30.480]   Is that correct?
[00:12:30.480 --> 00:12:33.920]   Well, we just use existing public data sets.
[00:12:33.920 --> 00:12:38.440]   So for our images, we use Cocoa and VG and Flickr.
[00:12:38.440 --> 00:12:41.640]   So there's some overlap between Cocoa and VG.
[00:12:41.640 --> 00:12:44.640]   And then there's like a few extra images, which are only in VG.
[00:12:44.640 --> 00:12:48.080]   And then there's like, I don't know, not that many images in Flickr.
[00:12:48.080 --> 00:12:50.200]   So like, yeah, I'm not sure.
[00:12:50.200 --> 00:12:52.160]   I don't remember the exact number, but yeah.
[00:12:52.160 --> 00:12:57.760]   So these data sets all existed and we just went and looked for data sets where you have
[00:12:57.760 --> 00:13:02.720]   an explicit alignment between words in the text and the object.
[00:13:02.720 --> 00:13:06.920]   So we need this kind of alignment because we use it in our loss functions.
[00:13:06.920 --> 00:13:11.600]   Of course, like it's a strong requirement and like future work or like subsequent work
[00:13:11.600 --> 00:13:15.000]   is going to try and like reduce this strong requirement.
[00:13:15.000 --> 00:13:17.000]   But yeah, we didn't make any new data sets.
[00:13:17.000 --> 00:13:21.280]   It was just about taking it from VG regions.
[00:13:21.280 --> 00:13:27.480]   It's actually a very rich source of information because they have, it's extremely noisy.
[00:13:27.480 --> 00:13:34.000]   So we did do like quite a lot of like filtering or like, you know, noise reduction in the
[00:13:34.000 --> 00:13:38.120]   region to box kind of annotations.
[00:13:38.120 --> 00:13:42.640]   But yeah, there's a lot of like really good data out there, which is like extremely long
[00:13:42.640 --> 00:13:43.920]   tailed and very interesting.
[00:13:43.920 --> 00:13:51.160]   For example, one of the images, like I remember there was a tiny glass which had a, you know,
[00:13:51.160 --> 00:13:53.960]   an inscription of a tree on it.
[00:13:53.960 --> 00:13:55.760]   And they even like annotate that.
[00:13:55.760 --> 00:13:59.000]   Like it's crazy how detailed the annotations are.
[00:13:59.000 --> 00:14:02.840]   So it's a very rich source of information and VG is just like amazing.
[00:14:02.840 --> 00:14:07.800]   So we use basically just existing stuff, nothing like made at Facebook basically.
[00:14:07.800 --> 00:14:08.800]   Yeah.
[00:14:08.800 --> 00:14:09.800]   Right.
[00:14:09.800 --> 00:14:10.800]   Interesting.
[00:14:10.800 --> 00:14:11.800]   Thank you for that background.
[00:14:11.800 --> 00:14:13.800]   So I guess then we'll move on to the introduction.
[00:14:13.800 --> 00:14:19.360]   Is there anything else you want to add in abstract or we good for introduction?
[00:14:19.360 --> 00:14:25.600]   I guess about the 1.3 million image text pairs, the main difference from prior work like Oscar
[00:14:25.600 --> 00:14:31.560]   and, you know, all of these recent papers, they all are trained on conceptual captions,
[00:14:31.560 --> 00:14:33.360]   which is, which has a lot more images.
[00:14:33.360 --> 00:14:38.600]   It's like 3.3 million images, whereas we use only 200,000 images, which is like an order
[00:14:38.600 --> 00:14:40.320]   of magnitude smaller.
[00:14:40.320 --> 00:14:46.080]   But we of course have like a stronger supervision in terms of like, you know, the actual part
[00:14:46.080 --> 00:14:51.080]   of the text that is annotated, whereas they use a lot more data, but it's sort of weaker
[00:14:51.080 --> 00:14:52.800]   because it's just image and text.
[00:14:52.800 --> 00:14:54.520]   It's not like object level.
[00:14:54.520 --> 00:14:58.920]   So yeah, just to make that point, because some of the reviewers were a bit confused
[00:14:58.920 --> 00:15:04.120]   about like if it's 1.3 million or 200,000, because we say that interchangeably and yeah,
[00:15:04.120 --> 00:15:07.160]   200,000 is images just to be clear.
[00:15:07.160 --> 00:15:08.160]   Right.
[00:15:08.160 --> 00:15:11.560]   And then the 1.3 million is an image and text pairs because you can, for one image you can
[00:15:11.560 --> 00:15:12.560]   add multiple texts.
[00:15:12.560 --> 00:15:13.560]   Yeah, exactly.
[00:15:13.560 --> 00:15:14.560]   Correct.
[00:15:14.560 --> 00:15:15.560]   That makes sense.
[00:15:15.560 --> 00:15:16.560]   All right.
[00:15:16.560 --> 00:15:22.560]   So I guess when I was first looking at the paper, what I find too is I just go through
[00:15:22.560 --> 00:15:27.200]   the architecture and just trying to understand the paper from an architecture perspective.
[00:15:27.200 --> 00:15:33.240]   So we know that MDTI is then from an introduction perspective, we know that MDTI is trying to
[00:15:33.240 --> 00:15:37.800]   mix the text or basically the text modality with the image modality.
[00:15:37.800 --> 00:15:40.040]   So it's mixing text and vision.
[00:15:40.040 --> 00:15:45.160]   And then I guess maybe it will be helpful if we could go through figure two from here
[00:15:45.160 --> 00:15:46.360]   on Eshwarya.
[00:15:46.360 --> 00:15:52.520]   So did you want to share on what's going on in figure two and explain how that works?
[00:15:52.520 --> 00:15:57.600]   So on the left, we're going to look at the image going from left to right.
[00:15:57.600 --> 00:16:02.560]   So on the left, we have this picture of a cat jumping over a fence and you have the
[00:16:02.560 --> 00:16:06.600]   caption which is aligned with it, which says a cat with white paws jumps over a fence in
[00:16:06.600 --> 00:16:08.040]   front of a yellow tree.
[00:16:08.040 --> 00:16:15.720]   Now in our annotations, we have explicit mapping that certain boxes in the image are mapped
[00:16:15.720 --> 00:16:17.800]   to certain spans of text.
[00:16:17.800 --> 00:16:22.680]   So that's what we show in the absolute right image and the color coded text.
[00:16:22.680 --> 00:16:28.760]   It's just to show you that the box corresponding to cat, for example, the orange one, we know
[00:16:28.760 --> 00:16:36.480]   from the ground truth that it is mapped to a cat, the span of the text.
[00:16:36.480 --> 00:16:42.280]   So the image is passed through a CNN, just like in Dita, you'd get a bunch of image features,
[00:16:42.280 --> 00:16:45.120]   which is just linearized.
[00:16:45.120 --> 00:16:50.680]   So it's flattened, the empty box is meant to show that it's a flattening.
[00:16:50.680 --> 00:16:55.800]   Similarly from the text, you get Roberta embeddings, apply a linear to make it the same size.
[00:16:55.800 --> 00:16:58.460]   And then you have like the sequence of text features.
[00:16:58.460 --> 00:17:02.400]   On the image side, there's no positional embedding that's already there.
[00:17:02.400 --> 00:17:06.400]   Like Roberta has sequence embeddings already, so we don't add an extra one.
[00:17:06.400 --> 00:17:10.080]   But on the image side, we use 2D positional embedding so that it knows which part of the
[00:17:10.080 --> 00:17:11.920]   image these features are from.
[00:17:11.920 --> 00:17:15.240]   And then we concatenate these on the sequence dimension.
[00:17:15.240 --> 00:17:22.000]   So you basically get this really long sequence of like image features and then text features.
[00:17:22.000 --> 00:17:26.680]   So the part after that is basically just a Dita transformer.
[00:17:26.680 --> 00:17:32.160]   So you have an encoder in Dita, here we call it a cross encoder because it takes in both
[00:17:32.160 --> 00:17:33.840]   image and text features.
[00:17:33.840 --> 00:17:35.920]   But otherwise, there's like nothing different about it.
[00:17:35.920 --> 00:17:40.280]   And then the decoder basically predicts boxes just as in Dita.
[00:17:40.280 --> 00:17:46.720]   The main difference is that the predicted boxes are not mapped to explicit class labels.
[00:17:46.720 --> 00:17:51.400]   So there's no more notion of like, you know, 81 or whatever class labels in Cocoa.
[00:17:51.400 --> 00:17:56.400]   Here it's just like, since we know in the ground truth that that orange box is mapped
[00:17:56.400 --> 00:18:02.040]   to a cat, we will supervise it to predict the actual like a soft distribution over the
[00:18:02.040 --> 00:18:07.080]   span where it's uniform over all the parts that correspond to that box.
[00:18:07.080 --> 00:18:08.080]   So yeah.
[00:18:08.080 --> 00:18:09.320]   That makes complete sense.
[00:18:09.320 --> 00:18:13.520]   Let's try and, I guess when I was first looking at this, that was the first difference that
[00:18:13.520 --> 00:18:18.120]   I saw is like, and you've mentioned that in the paper as well, is like how DTR maps the
[00:18:18.120 --> 00:18:20.480]   boxes to the class IDs.
[00:18:20.480 --> 00:18:23.600]   And then instead where over here, because we're doing multi-modal, what you want to
[00:18:23.600 --> 00:18:27.600]   do is you want to have the red box correspond to the tokens, a cat, whereas you want to
[00:18:27.600 --> 00:18:31.960]   have the purple one, which is like the four bounding boxes for the pause, which corresponds
[00:18:31.960 --> 00:18:32.960]   to the white pause.
[00:18:32.960 --> 00:18:35.680]   So I guess that was really helpful to see.
[00:18:35.680 --> 00:18:40.200]   And one thing that has always helped me is if it's okay, if we maybe start going through
[00:18:40.200 --> 00:18:44.960]   dimensions or just maybe tensor shapes, and then maybe that will help clear that understanding,
[00:18:44.960 --> 00:18:48.000]   or maybe we can move on and not go through the tensor shapes.
[00:18:48.000 --> 00:18:49.000]   Sure.
[00:18:49.000 --> 00:18:53.160]   So I guess like the box, you will have like, yeah, go ahead.
[00:18:53.160 --> 00:18:58.320]   Do you mind if I just go through, maybe we have our, I was just going to do it very quickly
[00:18:58.320 --> 00:19:03.080]   just for anybody who's new to like DTR or MDTR.
[00:19:03.080 --> 00:19:10.680]   So I just wanted to do it very quickly is, so if we have an input of say one by three
[00:19:10.680 --> 00:19:15.440]   by five and two, it would be something like that, which is one image and three channels
[00:19:15.440 --> 00:19:17.320]   and five and two by five and two.
[00:19:17.320 --> 00:19:22.120]   And then we have this whole text over here, which let's say is 10 words.
[00:19:22.120 --> 00:19:27.120]   So what Roberta is, and please correct me at any point if you feel like my understanding
[00:19:27.120 --> 00:19:31.720]   is different, but then what Roberta would do is for each of these 10, it would convert
[00:19:31.720 --> 00:19:35.220]   it into a, say a 10 by 196.
[00:19:35.220 --> 00:19:38.000]   I'm not sure what the embedding dimension is.
[00:19:38.000 --> 00:19:39.000]   768.
[00:19:39.000 --> 00:19:40.000]   Okay.
[00:19:40.000 --> 00:19:42.400]   So it would do 768.
[00:19:42.400 --> 00:19:47.120]   So that's going to convert those 10 words into an embedding of 10 by 768.
[00:19:47.120 --> 00:19:53.060]   And then from a CNN, what we would get is something like one by say 64 channels and
[00:19:53.060 --> 00:19:56.520]   maybe like a 16 by 16 spatial dimension.
[00:19:56.520 --> 00:19:59.280]   So far, is that correct?
[00:19:59.280 --> 00:20:03.480]   And then we just flattened this out so you could have reshaping and flattening things
[00:20:03.480 --> 00:20:04.480]   out.
[00:20:04.480 --> 00:20:08.240]   And then this is where this dimension would go in and they both get concatenated.
[00:20:08.240 --> 00:20:09.240]   So we have.
[00:20:09.240 --> 00:20:14.320]   So before you, when you flatten it, you're going to have like some feature dimension.
[00:20:14.320 --> 00:20:19.000]   That's why there's a linear so that you convert it to the dimension of the transformer basically,
[00:20:19.000 --> 00:20:20.840]   which was 256 for us.
[00:20:20.840 --> 00:20:26.240]   So before we add the positional embedding and concat, the text and image features are
[00:20:26.240 --> 00:20:30.120]   both like basically just, you know, 256 dimensional.
[00:20:30.120 --> 00:20:32.560]   And then we concat it on the sequence dimension.
[00:20:32.560 --> 00:20:37.360]   So it becomes length of the image features plus the length of the text features.
[00:20:37.360 --> 00:20:40.280]   And the last dimension is always 256 basically.
[00:20:40.280 --> 00:20:41.280]   All right.
[00:20:41.280 --> 00:20:42.280]   Okay.
[00:20:42.280 --> 00:20:43.280]   That makes good.
[00:20:43.280 --> 00:20:44.480]   That does really help.
[00:20:44.480 --> 00:20:48.760]   And then finally the transformer over here, because now we have the concatenated list.
[00:20:48.760 --> 00:20:52.080]   So the transformer over here, that's very similar to the DETR.
[00:20:52.080 --> 00:20:53.880]   And we just pass that through the transformer.
[00:20:53.880 --> 00:21:00.400]   So now the transformer learns to find that, basically find that relationship between text
[00:21:00.400 --> 00:21:01.500]   and images.
[00:21:01.500 --> 00:21:06.680]   So that's how this model then starts to learn that, okay, a cat are these, because now the
[00:21:06.680 --> 00:21:10.880]   DETR or the transformer itself is going to start predicting the bounding boxes.
[00:21:10.880 --> 00:21:14.680]   So this is where the model starts to learn, okay, a cat is linked to this red bounding
[00:21:14.680 --> 00:21:16.880]   box coordinates.
[00:21:16.880 --> 00:21:20.560]   Is that a good summary of the understanding so far?
[00:21:20.560 --> 00:21:21.560]   So yeah.
[00:21:21.560 --> 00:21:25.720]   So there is like a matching step which happens during prediction, right?
[00:21:25.720 --> 00:21:26.840]   Just as in DETR.
[00:21:26.840 --> 00:21:33.040]   And basically during Ground Truth, we're going to like know which box to map to which, like
[00:21:33.040 --> 00:21:37.140]   instead of the label here, you have a soft token distribution that you're predicting
[00:21:37.140 --> 00:21:39.800]   and it's like everything else remains the same basically.
[00:21:39.800 --> 00:21:40.800]   Yeah.
[00:21:40.800 --> 00:21:41.800]   Absolutely.
[00:21:41.800 --> 00:21:46.360]   And the matching, that's the bipartite matching criterion that's part of DETR?
[00:21:46.360 --> 00:21:47.360]   It's the same.
[00:21:47.360 --> 00:21:51.520]   We have the same, like, yeah, this was something which the reviewers asked us to overhaul.
[00:21:51.520 --> 00:21:54.840]   So I'll add if it's confusing to anyone.
[00:21:54.840 --> 00:21:57.560]   The DETR matching step exists here too.
[00:21:57.560 --> 00:22:01.000]   We also have the DETR losses for the box prediction.
[00:22:01.000 --> 00:22:05.240]   In addition, we have like the contrastive and soft token prediction.
[00:22:05.240 --> 00:22:06.240]   So yeah.
[00:22:06.240 --> 00:22:07.240]   Right.
[00:22:07.240 --> 00:22:08.240]   Okay.
[00:22:08.240 --> 00:22:09.240]   Thanks very much.
[00:22:09.240 --> 00:22:15.360]   So I guess then that understanding sort of, I believe, covers somewhat of the method and
[00:22:15.360 --> 00:22:17.400]   we've already looked at the architecture.
[00:22:17.400 --> 00:22:21.840]   That covers the overall architecture and it covers like how the shapes would propagate
[00:22:21.840 --> 00:22:22.840]   through this transformer.
[00:22:22.840 --> 00:22:25.440]   Is there anything you want to add so far?
[00:22:25.440 --> 00:22:28.440]   Because I was thinking we'd get started with the losses.
[00:22:28.440 --> 00:22:29.440]   Yeah, sure.
[00:22:29.440 --> 00:22:30.440]   No worries.
[00:22:30.440 --> 00:22:32.800]   I'm like answering questions also in the chat.
[00:22:32.800 --> 00:22:33.800]   Oh, okay.
[00:22:33.800 --> 00:22:35.520]   Thanks very much for doing that.
[00:22:35.520 --> 00:22:36.520]   Nice.
[00:22:36.520 --> 00:22:38.400]   I'm very bad at doing multitasking.
[00:22:38.400 --> 00:22:42.840]   So I guess because you come from a multimodal model, but you're good at multitasking.
[00:22:42.840 --> 00:22:45.080]   While you're talking, I can answer the questions.
[00:22:45.080 --> 00:22:46.840]   So it's no worries.
[00:22:46.840 --> 00:22:47.840]   That's great to see.
[00:22:47.840 --> 00:22:48.840]   Okay.
[00:22:48.840 --> 00:22:54.600]   So then as Eshwarya just mentioned that in training, we also have two additional loss
[00:22:54.600 --> 00:22:55.600]   functions.
[00:22:55.600 --> 00:23:01.000]   So for anybody that's new to DETR, I would again mention that this is something that
[00:23:01.000 --> 00:23:05.680]   if you want to have a look at the DETR, when we keep referring to DETR, we're just saying
[00:23:05.680 --> 00:23:11.680]   the DETR paper, which is the end to end detection with transformers.
[00:23:11.680 --> 00:23:15.040]   And last week at Wits and Mices, we also had a paper reading group for this.
[00:23:15.040 --> 00:23:20.840]   And I also have written a blog post about this, which is the annotated transformer.
[00:23:20.840 --> 00:23:25.360]   So we still have all of those loss functions, as you're saying, Eshwarya, which is set criterion,
[00:23:25.360 --> 00:23:29.160]   which is still the matching and everything's happening as is.
[00:23:29.160 --> 00:23:33.240]   But what you've done is because this is different from predicting class ID.
[00:23:33.240 --> 00:23:38.560]   So instead of like for every bounding box, where in DETR, we would predict for this cat,
[00:23:38.560 --> 00:23:43.360]   we would just say, predict a class ID of maybe 81.
[00:23:43.360 --> 00:23:45.520]   Yeah, I'm just putting in numbers.
[00:23:45.520 --> 00:23:46.520]   Right, yeah.
[00:23:46.520 --> 00:23:51.840]   So yeah, so we would have the class ID say 81 for the cat, or we would have a class ID
[00:23:51.840 --> 00:23:55.840]   of say 29, which is just how DETR would do it.
[00:23:55.840 --> 00:23:59.320]   And it would try and match these bounding boxes to those IDs.
[00:23:59.320 --> 00:24:05.160]   Here what we want to do is 29 needs to be matched to these three tokens.
[00:24:05.160 --> 00:24:06.880]   Is that correct?
[00:24:06.880 --> 00:24:11.200]   And then 81 needs to be matched to these two tokens.
[00:24:11.200 --> 00:24:16.880]   And I mentioned that it's also like, it's a many to many mapping, right?
[00:24:16.880 --> 00:24:20.960]   Because you can have many, like in white paws, for example, there are four boxes that are
[00:24:20.960 --> 00:24:22.960]   mapped to the same text.
[00:24:22.960 --> 00:24:27.920]   And it can also be possible that you have two boxes, which are referred to by the same
[00:24:27.920 --> 00:24:30.480]   like co-referent text, for example.
[00:24:30.480 --> 00:24:34.680]   Like in Flickr, for example, they have like some sentences like a couple.
[00:24:34.680 --> 00:24:38.880]   So a couple can be shown with like the full box.
[00:24:38.880 --> 00:24:41.940]   And then, you know, they again refer to the individual people.
[00:24:41.940 --> 00:24:45.960]   So they also have like a reference to the person, you know, it could be like the two
[00:24:45.960 --> 00:24:49.160]   different boxes, but the same like text, or you should know that they're talking about
[00:24:49.160 --> 00:24:52.440]   the same people in like two references in the text because of the box.
[00:24:52.440 --> 00:24:53.440]   Yeah.
[00:24:53.440 --> 00:24:57.360]   So I was just saying that it can have many boxes for one text and many text for one box,
[00:24:57.360 --> 00:24:58.360]   like both ways.
[00:24:58.360 --> 00:25:01.280]   Yeah, I guess that's a really, really good point.
[00:25:01.280 --> 00:25:02.280]   Thanks for sharing that.
[00:25:02.280 --> 00:25:06.700]   So I guess even in this image, if this wasn't a cat, if this was say something like many
[00:25:06.700 --> 00:25:11.640]   cats with white paws are jumping over a fence, so then this very image could have multiple
[00:25:11.640 --> 00:25:12.640]   cats in there.
[00:25:12.640 --> 00:25:19.240]   So two, like for the word cats, for one word, you still have many objects in the image.
[00:25:19.240 --> 00:25:24.040]   So the important part here would be like, if there is a sentence which has two women,
[00:25:24.040 --> 00:25:28.600]   right, like we, there's an example later in the paper where we show like a woman wearing
[00:25:28.600 --> 00:25:33.520]   a watch, whereas there's another woman who has like white hair.
[00:25:33.520 --> 00:25:40.460]   So if there was no qualifier, the box for women should have like map to all, like, there's
[00:25:40.460 --> 00:25:43.100]   no like, there's nothing which is distinguishing them, right?
[00:25:43.100 --> 00:25:44.980]   So it should be all women basically.
[00:25:44.980 --> 00:25:48.740]   But if you say women who has white hair, you know that there's something qualifying it.
[00:25:48.740 --> 00:25:50.260]   So it like distinguishes the two.
[00:25:50.260 --> 00:25:54.260]   So yeah, that's, that's important in like how we also construct the data later and it
[00:25:54.260 --> 00:25:56.700]   will come up for the pre-training data.
[00:25:56.700 --> 00:25:58.260]   So just mentioning it now.
[00:25:58.260 --> 00:25:59.260]   Yeah.
[00:25:59.260 --> 00:26:00.260]   Thanks very much, Aishwarya.
[00:26:00.260 --> 00:26:03.100]   That's really, really helpful.
[00:26:03.100 --> 00:26:07.600]   So I guess then with that understanding, I guess we're okay to start having a look at
[00:26:07.600 --> 00:26:08.600]   the losses.
[00:26:08.600 --> 00:26:13.340]   So again, just to repeat, what we want to do is we want to have the bounding boxes be
[00:26:13.340 --> 00:26:16.060]   assigned to some tokens of the text.
[00:26:16.060 --> 00:26:21.260]   So the two loss functions that we have, as Aishwarya mentioned, that have been added
[00:26:21.260 --> 00:26:26.900]   to detail and MDTR adds them is the soft token prediction and the contrastive alignment.
[00:26:26.900 --> 00:26:32.200]   So Aishwarya, maybe did you want to share some things about, or just provide an introduction
[00:26:32.200 --> 00:26:34.700]   to soft token prediction to start with?
[00:26:34.700 --> 00:26:38.760]   You want me to like share my screen and like show a slide because it might be easier to
[00:26:38.760 --> 00:26:40.120]   explain with that.
[00:26:40.120 --> 00:26:41.120]   Yeah, please.
[00:26:41.120 --> 00:26:44.200]   You can, you should be able to share your screen now.
[00:26:44.200 --> 00:26:45.200]   One second.
[00:26:45.200 --> 00:26:46.200]   Let me find the window.
[00:26:46.200 --> 00:26:47.200]   Sorry, one second.
[00:26:47.200 --> 00:26:48.200]   That's okay.
[00:26:48.200 --> 00:26:57.200]   Meanwhile, I'll just have a look at any questions that we might have.
[00:26:57.200 --> 00:26:58.200]   Yeah.
[00:26:58.200 --> 00:27:20.040]   Can you see my screen?
[00:27:20.040 --> 00:27:21.040]   It's coming up now.
[00:27:21.040 --> 00:27:23.400]   So just going to, yeah, now we can.
[00:27:23.400 --> 00:27:24.400]   Okay.
[00:27:24.400 --> 00:27:25.400]   So yeah.
[00:27:25.400 --> 00:27:28.600]   So the first one is the soft token prediction loss.
[00:27:28.600 --> 00:27:32.820]   This image helps because it's like the same one as we've been looking at.
[00:27:32.820 --> 00:27:39.440]   So to tell you concretely what the ground truth looks like, like how we construct it,
[00:27:39.440 --> 00:27:45.520]   we have a map which basically says it's zero for all the places, you know, all the token
[00:27:45.520 --> 00:27:49.160]   positions where this object doesn't have anything to do with it.
[00:27:49.160 --> 00:27:54.880]   And whenever this object is referred to, you need to have like a uniform distribution over
[00:27:54.880 --> 00:27:55.880]   it.
[00:27:55.880 --> 00:27:57.760]   So it's going to be like 0.5, 0.5 here.
[00:27:57.760 --> 00:28:03.800]   So in terms of, in case of like this, the pause, all of these four boxes will have the
[00:28:03.800 --> 00:28:06.600]   same distribution or the white balls.
[00:28:06.600 --> 00:28:10.400]   And in the, in case of no object, basically you're saying it doesn't correspond to any
[00:28:10.400 --> 00:28:11.400]   of this.
[00:28:11.400 --> 00:28:16.880]   It's going to have like, you know, all of it's a mass on this no object token in detail.
[00:28:16.880 --> 00:28:23.160]   Also this no object token existed for when you don't have a matching object for a particular,
[00:28:23.160 --> 00:28:24.480]   you know, prediction.
[00:28:24.480 --> 00:28:31.040]   But here we, instead of having a label, it's just implemented as like the last token basically.
[00:28:31.040 --> 00:28:36.760]   So if you have like a max length of 255, it's like 256, if that makes sense.
[00:28:36.760 --> 00:28:41.720]   And so, yeah, what I was saying earlier is like, suppose this cat is referred to again,
[00:28:41.720 --> 00:28:46.520]   and if the sentence was a cat with white balls jumps over a fence and the cat is like really
[00:28:46.520 --> 00:28:51.560]   cute or something like that, if that cat is referred to again, it would have like, you
[00:28:51.560 --> 00:28:57.080]   know, 0.25, 0.25 and 0.25, 0.25 again for the remaining, for the other place that the
[00:28:57.080 --> 00:28:58.120]   cat is referred.
[00:28:58.120 --> 00:29:02.640]   So I'm just trying to point out that the span that you're aligning to doesn't have to be
[00:29:02.640 --> 00:29:03.720]   contiguous.
[00:29:03.720 --> 00:29:07.680]   It's anywhere in the sentence, wherever that, you know, object is referred to, you're going
[00:29:07.680 --> 00:29:11.720]   to have a uniform distribution over all of the token positions.
[00:29:11.720 --> 00:29:13.080]   So I hope that's clear.
[00:29:13.080 --> 00:29:15.520]   If you have any questions you can ask.
[00:29:15.520 --> 00:29:17.760]   And then this is the soft token prediction.
[00:29:17.760 --> 00:29:22.220]   And then the other one is the contrastive alignment.
[00:29:22.220 --> 00:29:29.060]   So here, the idea is, we need to figure out which objects and which parts of the text
[00:29:29.060 --> 00:29:32.240]   should have representations that are closer together.
[00:29:32.240 --> 00:29:39.160]   So in the first case, in the soft token alignment, sorry, in the soft token prediction, you're
[00:29:39.160 --> 00:29:43.680]   only getting like positional information, but there's nothing that's really being done
[00:29:43.680 --> 00:29:48.240]   about the actual embeddings of the objects and the actual embeddings of the text, right?
[00:29:48.240 --> 00:29:53.040]   Because the prediction is just like, okay, I'm going to give you a span over the positions
[00:29:53.040 --> 00:29:54.040]   of the text.
[00:29:54.040 --> 00:30:00.400]   But what about like the actual representations of the image and text of the objects and text?
[00:30:00.400 --> 00:30:03.720]   So that we fix by using this contrastive alignment.
[00:30:03.720 --> 00:30:09.160]   So if you're not familiar with it, it's a very standard thing now in like sub-supervised
[00:30:09.160 --> 00:30:10.160]   learning.
[00:30:10.160 --> 00:30:12.520]   It's called an info-NCE loss.
[00:30:12.520 --> 00:30:15.600]   NCE stands for noise contrastive estimation.
[00:30:15.600 --> 00:30:21.840]   The equation we can look at again in the paper, but at a very high level, what you want is
[00:30:21.840 --> 00:30:29.000]   things that are referred to in text and in the image, like which correspond to each other.
[00:30:29.000 --> 00:30:35.480]   For example, this red box around this blue ball and the text is ball or yellow.
[00:30:35.480 --> 00:30:44.400]   So the first token P1 basically, which corresponds to this word ball should have like a one for
[00:30:44.400 --> 00:30:48.840]   the box, which is red, as well as this other ball, right?
[00:30:48.840 --> 00:30:52.640]   Because you have like the word ball, you know that these are both balls.
[00:30:52.640 --> 00:30:56.880]   So if you have four objects, both of these have to be positive, whereas the rest have
[00:30:56.880 --> 00:30:58.320]   to be like negative.
[00:30:58.320 --> 00:31:03.800]   So essentially what you're doing here is taking the embedding for the object ball, which both
[00:31:03.800 --> 00:31:08.280]   of these are, and trying to make them as close as possible to the embedding for the word
[00:31:08.280 --> 00:31:09.280]   ball.
[00:31:09.280 --> 00:31:11.240]   So yeah, that's the idea.
[00:31:11.240 --> 00:31:13.420]   And this is a symmetric loss.
[00:31:13.420 --> 00:31:17.480]   So this is what I showed you is from token to objects, but the same way you can go from
[00:31:17.480 --> 00:31:19.720]   objects to tokens.
[00:31:19.720 --> 00:31:25.560]   So for the box, for the red box, it basically is this ball, right?
[00:31:25.560 --> 00:31:26.560]   The first word.
[00:31:26.560 --> 00:31:32.480]   And yeah, so like you can basically look at all of the other Xs I have and figure out
[00:31:32.480 --> 00:31:33.880]   how to make them.
[00:31:33.880 --> 00:31:37.200]   And this is used in the contrastive alignment loss.
[00:31:37.200 --> 00:31:41.520]   After making this matrix, we can go back to the equations.
[00:31:41.520 --> 00:31:43.520]   Maybe I can talk to her.
[00:31:43.520 --> 00:31:46.520]   No, I think that was really, really helpful.
[00:31:46.520 --> 00:31:54.040]   Looking at the equation is a bit like daunting, but if you just like break it down, it's like,
[00:31:54.040 --> 00:31:58.120]   what do you want to do is just like maybe embeddings of things that should be similar
[00:31:58.120 --> 00:32:03.000]   close together and whatever should not be like a ball in a cylinder should not be, the
[00:32:03.000 --> 00:32:04.560]   embeddings for that should not be similar.
[00:32:04.560 --> 00:32:05.560]   So you like push them apart.
[00:32:05.560 --> 00:32:06.560]   So yeah.
[00:32:06.560 --> 00:32:07.560]   Absolutely.
[00:32:07.560 --> 00:32:12.840]   So maybe let's just go into some details now that we have a really good understanding of
[00:32:12.840 --> 00:32:13.840]   those concepts.
[00:32:13.840 --> 00:32:18.000]   So maybe I will try and summarize and then maybe I'm sure you could correct me if I'm
[00:32:18.000 --> 00:32:20.000]   wrong somewhere.
[00:32:20.000 --> 00:32:25.240]   So again, for soft token prediction, as you said, what we want to do is we want to try
[00:32:25.240 --> 00:32:28.160]   and match a cat to these two tokens.
[00:32:28.160 --> 00:32:35.080]   So if we have, say in soft token prediction, let's say if we have, let's say we have the
[00:32:35.080 --> 00:32:40.880]   cat object and then I've got, so this, this I'm referring to as the object.
[00:32:40.880 --> 00:32:43.400]   And let's say I've got words, one, two, three, four.
[00:32:43.400 --> 00:32:48.160]   So the words one and two are the one that needs to be assigned to this, to this object.
[00:32:48.160 --> 00:32:53.320]   So what we're trying to do is for this object, then there's this loss, because these are
[00:32:53.320 --> 00:32:58.880]   ones which is every word or every token that's corresponding to the object that's valued
[00:32:58.880 --> 00:33:01.960]   at one, which is the mask, as you mentioned, or the ground truth.
[00:33:01.960 --> 00:33:06.240]   For the soft token prediction, it's normalized by the length of the span.
[00:33:06.240 --> 00:33:07.640]   So basically this would be 0.5.
[00:33:07.640 --> 00:33:08.640]   Oh, sorry.
[00:33:08.640 --> 00:33:09.640]   That's a good point.
[00:33:09.640 --> 00:33:10.640]   Sorry.
[00:33:10.640 --> 00:33:11.640]   Yes.
[00:33:11.640 --> 00:33:13.180]   So that will be 0.5.5 and then everything's going to be zero.
[00:33:13.180 --> 00:33:18.500]   So as many tokens as we have, you would divide it uniformly over those tokens.
[00:33:18.500 --> 00:33:23.620]   So if this was something like, if I'm referring to the object, say, if I'm referring to the
[00:33:23.620 --> 00:33:29.800]   object, say tree, then, and I've got three tokens that refer to the word three in my
[00:33:29.800 --> 00:33:34.400]   sentence this time, then this would be 0.33 and 0.33 and then zero.
[00:33:34.400 --> 00:33:39.420]   And then what we're trying to do is then this matrix would become kind of the ground truth
[00:33:39.420 --> 00:33:40.420]   for this.
[00:33:40.420 --> 00:33:42.940]   And then could we just use BCE or binary cross entropy?
[00:33:42.940 --> 00:33:45.260]   It's a soft cross entropy, basically.
[00:33:45.260 --> 00:33:46.260]   Soft cross entropy.
[00:33:46.260 --> 00:33:47.260]   Exactly.
[00:33:47.260 --> 00:33:51.180]   So we could use something like a soft cross entropy to then make the model match.
[00:33:51.180 --> 00:33:56.700]   So in this loss, I guess the model is still learning some information about, because it
[00:33:56.700 --> 00:33:57.940]   knows where the tree is.
[00:33:57.940 --> 00:34:00.980]   It has some information about the bounding box.
[00:34:00.980 --> 00:34:06.620]   It is learning to associate somewhat where the tree is in the image to then those three
[00:34:06.620 --> 00:34:12.620]   tokens, like one, two, three tokens that we have that are corresponding to the tree.
[00:34:12.620 --> 00:34:15.260]   Is that correct so far?
[00:34:15.260 --> 00:34:20.660]   This is mostly like a non-parametric alignment because the actual embeddings of the objects
[00:34:20.660 --> 00:34:25.300]   and text, there's nothing like forcing them to be similar through this loss.
[00:34:25.300 --> 00:34:29.380]   It's just learning about positional information, which is why we need another loss, which is
[00:34:29.380 --> 00:34:30.380]   the contrastive.
[00:34:30.380 --> 00:34:31.380]   Yeah.
[00:34:31.380 --> 00:34:32.380]   OK.
[00:34:32.380 --> 00:34:38.460]   So then, just as you said, so contrastive alignment then is slightly different and it's
[00:34:38.460 --> 00:34:44.460]   a bit more powerful in the sense that now this is what is going to match the tree to
[00:34:44.460 --> 00:34:45.460]   the object.
[00:34:45.460 --> 00:34:51.940]   So because if I, I think if I go back to the image, because we're going to pass the, we're
[00:34:51.940 --> 00:34:55.500]   going to, let's just take the example of the cat.
[00:34:55.500 --> 00:34:59.660]   So because we're going to pass this cat through the CNN and we're going to get some embedding,
[00:34:59.660 --> 00:35:03.220]   let's just say we're going to get an embedding of 768 for this cat.
[00:35:03.220 --> 00:35:07.160]   That's just represents that representation of that cat.
[00:35:07.160 --> 00:35:09.080]   And similarly, we're going to get some embedding.
[00:35:09.080 --> 00:35:12.980]   So when we concatenate, or let's just say it's just some embedding, which is again of
[00:35:12.980 --> 00:35:18.700]   say dimension 768, we can reshape or I'm not exactly sure what the dimensions are that
[00:35:18.700 --> 00:35:25.660]   we can not go into the details, but just for a high level, both of these now are represented
[00:35:25.660 --> 00:35:26.660]   by vectors.
[00:35:26.660 --> 00:35:29.660]   So cat is represented by 768 long vector.
[00:35:29.660 --> 00:35:33.200]   And similarly, those two words are again, this vector.
[00:35:33.200 --> 00:35:35.820]   And then what this loss is trying to do is it's trying to make sure.
[00:35:35.820 --> 00:35:36.820]   Sorry, sorry to interrupt.
[00:35:36.820 --> 00:35:42.520]   For the contrastive, I just wanted to say that we take the output of this and the cross
[00:35:42.520 --> 00:35:43.520]   encoder.
[00:35:43.520 --> 00:35:45.700]   So there's this concat and transformer, right?
[00:35:45.700 --> 00:35:52.180]   The transformer has an encoder and decoder, the output of that cross encoder, the representation
[00:35:52.180 --> 00:35:53.180]   of the text there.
[00:35:53.180 --> 00:35:58.880]   So there's already been some like cross attention or like self attention to the image features
[00:35:58.880 --> 00:36:00.260]   inside that cross encoder.
[00:36:00.260 --> 00:36:04.380]   And we're getting the output like somewhere in the middle of this transformer for the
[00:36:04.380 --> 00:36:05.700]   text features.
[00:36:05.700 --> 00:36:08.260]   And for the image features, it's the output of the boxes.
[00:36:08.260 --> 00:36:10.660]   Yeah, that would help, I guess.
[00:36:10.660 --> 00:36:15.040]   Yeah, I was just going to quickly bring up that image that you're referring to.
[00:36:15.040 --> 00:36:20.280]   So let me just quickly do that.
[00:36:20.280 --> 00:36:21.280]   When it loads.
[00:36:21.280 --> 00:36:25.160]   I'm in Australia, so the Wi-Fi is really bad.
[00:36:25.160 --> 00:36:31.440]   They say the Qantas Airlines has a better Wi-Fi than all of Australia.
[00:36:31.440 --> 00:36:37.240]   So that's just an Australian joke.
[00:36:37.240 --> 00:36:43.420]   Thanks for having this earlier this week, by the way, to accommodate me.
[00:36:43.420 --> 00:36:45.000]   No, no problems at all.
[00:36:45.000 --> 00:36:48.240]   Thanks for joining us.
[00:36:48.240 --> 00:36:49.480]   So I was just going to go.
[00:36:49.480 --> 00:36:52.960]   Yeah, that will do.
[00:36:52.960 --> 00:36:57.840]   Or maybe what about the, there was this really nice image about the transformer.
[00:36:57.840 --> 00:36:58.840]   How about this one?
[00:36:58.840 --> 00:36:59.840]   Yeah, sure.
[00:36:59.840 --> 00:37:03.240]   I mean, the other one is like more compact because we don't really care about.
[00:37:03.240 --> 00:37:06.240]   Let's go about that.
[00:37:06.240 --> 00:37:07.920]   There we go.
[00:37:07.920 --> 00:37:08.920]   So this is.
[00:37:08.920 --> 00:37:12.640]   Yeah, so this is the.
[00:37:12.640 --> 00:37:16.080]   In detail, you have the image features which go into this encoder.
[00:37:16.080 --> 00:37:20.700]   So the bottom part of the transformer encoder is basically just this flattened representation
[00:37:20.700 --> 00:37:21.700]   of the image.
[00:37:21.700 --> 00:37:26.560]   So in addition to that flattened representation of the image, we also have the Roberta features
[00:37:26.560 --> 00:37:31.520]   as like, just like imagine a little longer vector instead of whatever you have here.
[00:37:31.520 --> 00:37:35.120]   But at the output, the encoder is basically going to have like a one to one mapping.
[00:37:35.120 --> 00:37:38.020]   So you're going to also get hidden features of the same length.
[00:37:38.020 --> 00:37:42.040]   So the text features that we use for the contrast of alignment are basically what you get as
[00:37:42.040 --> 00:37:44.080]   the output of the encoder.
[00:37:44.080 --> 00:37:49.000]   And for the image features, which are the object, we get the output of the decoder.
[00:37:49.000 --> 00:37:54.400]   So what we see as the output here is exactly what we use and try to like basically make
[00:37:54.400 --> 00:37:55.800]   those embeddings close together.
[00:37:55.800 --> 00:38:02.800]   They do go through like a linear mapping to make them a smaller dimension because it's
[00:38:02.800 --> 00:38:07.040]   been shown that when you're doing this contrastive learning or like contrastive loss, you don't
[00:38:07.040 --> 00:38:12.320]   need to do the dot product in like 768 or whatever, even 256 dimensional space.
[00:38:12.320 --> 00:38:17.360]   So we project down to 64 dimensions and then do the dot product for the contrast.
[00:38:17.360 --> 00:38:18.360]   No, perfect.
[00:38:18.360 --> 00:38:19.360]   Perfect.
[00:38:19.360 --> 00:38:21.000]   Thanks very much, Ishwarya.
[00:38:21.000 --> 00:38:27.160]   So I guess then that's why you have the two losses here, which is one for, because we
[00:38:27.160 --> 00:38:33.160]   could have multiple objects for a single token, or we could have multiple tokens for a single
[00:38:33.160 --> 00:38:34.160]   object.
[00:38:34.160 --> 00:38:37.840]   That's why you have these two losses, which is token and object.
[00:38:37.840 --> 00:38:41.880]   And you just add them up or take the average of these two losses.
[00:38:41.880 --> 00:38:45.080]   The image which I had shown earlier with the clever example.
[00:38:45.080 --> 00:38:49.120]   So we have this matrix, which is like objects on one side and tokens on the other.
[00:38:49.120 --> 00:38:53.120]   So basically you're normalizing row wise for one of them and normalizing column wise for
[00:38:53.120 --> 00:38:54.120]   the other.
[00:38:54.120 --> 00:38:57.120]   And those are the two, like L, O and L, D.
[00:38:57.120 --> 00:38:58.120]   Perfect.
[00:38:58.120 --> 00:38:59.120]   Thanks very much.
[00:38:59.120 --> 00:39:05.040]   I guess then that's the losses and we're next to pre-training, I believe.
[00:39:05.040 --> 00:39:11.280]   So we will find, so getting into pre-training then, did you want to give us an, you've already
[00:39:11.280 --> 00:39:16.400]   told us that there's 1.5 million text pairs and about 200,000 images.
[00:39:16.400 --> 00:39:19.760]   So how did the, so what exactly is the pre-training?
[00:39:19.760 --> 00:39:22.160]   How is that different from what we're doing?
[00:39:22.160 --> 00:39:25.200]   And then how do we use that model for further downstream tasks?
[00:39:25.200 --> 00:39:26.200]   Sure.
[00:39:26.200 --> 00:39:32.560]   So generally when we have multimodal models, which have object detection as a pre-processing
[00:39:32.560 --> 00:39:34.520]   thing, they usually get the object.
[00:39:34.520 --> 00:39:39.760]   And then the pre-training tasks are generally things like masking out part of the text and
[00:39:39.760 --> 00:39:44.560]   like predicting the image or masking out some regions and trying to predict them and stuff
[00:39:44.560 --> 00:39:45.560]   like that.
[00:39:45.560 --> 00:39:48.840]   Or like image text matching is a very popular one as well.
[00:39:48.840 --> 00:39:52.680]   So we kind of go away from all of these.
[00:39:52.680 --> 00:39:55.120]   We don't, like maybe it'll be even better if we try that.
[00:39:55.120 --> 00:40:01.040]   We actually didn't try language modeling or master whatever on any of our encoders or
[00:40:01.040 --> 00:40:02.040]   decoders.
[00:40:02.040 --> 00:40:10.040]   But what we want to like kind of establish is that if you want to like reason about something
[00:40:10.040 --> 00:40:14.880]   in a downstream task, you have to like make the central task of pre-training actually
[00:40:14.880 --> 00:40:18.920]   figuring out which text is referring to which part of the image.
[00:40:18.920 --> 00:40:24.680]   So this task of like phrase grounding is what we make central to our approach.
[00:40:24.680 --> 00:40:30.320]   And our pre-training only does this one task, which is, you know, phrase grounding.
[00:40:30.320 --> 00:40:31.680]   So what is phrase grounding?
[00:40:31.680 --> 00:40:35.640]   For example, like you have this, can you make the image a bit bigger?
[00:40:35.640 --> 00:40:36.640]   Yeah, sure.
[00:40:36.640 --> 00:40:37.640]   Yeah.
[00:40:37.640 --> 00:40:43.520]   So for example, if you have this, let's just look at the first part of the sentence, the
[00:40:43.520 --> 00:40:47.160]   person in the gray shirt with a watch on their wrist.
[00:40:47.160 --> 00:40:53.200]   You can see that this person in the image, which is in the purple box, would be the person
[00:40:53.200 --> 00:40:54.920]   being referred to here.
[00:40:54.920 --> 00:41:02.400]   So if the annotations came from VG or yeah, like Flickr, there's a different like level
[00:41:02.400 --> 00:41:06.980]   of density in annotations across the different datasets that we use.
[00:41:06.980 --> 00:41:12.200]   So in the referring expression annotations, which we use for Cocoa images, it basically
[00:41:12.200 --> 00:41:14.820]   just gives you one box for the entire text.
[00:41:14.820 --> 00:41:18.440]   So like the person in gray shirt with a watch on their wrist, whereas in Flickr, they would
[00:41:18.440 --> 00:41:23.380]   actually also annotate the gray shirt and the watch, maybe even the wrist.
[00:41:23.380 --> 00:41:28.600]   So yeah, basically all the name entities would be like, would be annotated.
[00:41:28.600 --> 00:41:34.040]   So in our case, when we want to try and combine, because we don't have that much data of this
[00:41:34.040 --> 00:41:38.200]   kind, so we had to like make do with like these different levels of annotations.
[00:41:38.200 --> 00:41:42.480]   So we just use Flickr the way it is, Cocoa referring expressions.
[00:41:42.480 --> 00:41:47.120]   And on VG, we use VG regions as well as GQA balanced dataset.
[00:41:47.120 --> 00:41:50.600]   So it's like a subset of the huge 22 million questions.
[00:41:50.600 --> 00:41:58.200]   So the GQA questions are basically just used as is, they're not used for question answering.
[00:41:58.200 --> 00:42:04.760]   So at this stage, it'll be like, what is the color of the banana, you know, in the bottom
[00:42:04.760 --> 00:42:06.460]   right part of the image or whatever.
[00:42:06.460 --> 00:42:12.160]   So basically GQA gives you alignment between the objects referred to in the question and
[00:42:12.160 --> 00:42:13.680]   you know, boxes in the image.
[00:42:13.680 --> 00:42:17.720]   So we decided to use that as well.
[00:42:17.720 --> 00:42:24.680]   An important point that I want to make about like the data combination is that if we had
[00:42:24.680 --> 00:42:30.640]   these separate sentences, oh, so to combine Flickr, RefCocoa and like all of these datasets,
[00:42:30.640 --> 00:42:36.080]   what we did was if the sentence only has one box for the entire text, where we can see
[00:42:36.080 --> 00:42:41.480]   that there are more, you know, objects that could be referred to in the sentence, then
[00:42:41.480 --> 00:42:43.560]   we extract the root of the sentence.
[00:42:43.560 --> 00:42:47.560]   This is not optimal, obviously, because like, this is just because we had to like combine
[00:42:47.560 --> 00:42:49.200]   multiple datasets.
[00:42:49.200 --> 00:42:55.480]   But what we do is like use like a spacey parser and just say, what is the root of the sentence?
[00:42:55.480 --> 00:42:56.920]   It's the person in the gray shirt.
[00:42:56.920 --> 00:43:01.480]   So basically we just use that as the annotation, which is why you have this, the person as
[00:43:01.480 --> 00:43:04.400]   the only thing that's mapped to this entire sentence.
[00:43:04.400 --> 00:43:12.000]   So now another important point is for 200,000 images, we have many texts.
[00:43:12.000 --> 00:43:14.720]   Does it make sense to just have one image in one text?
[00:43:14.720 --> 00:43:19.080]   Like, and just like, you know, repeat the same image and have the different text each
[00:43:19.080 --> 00:43:20.080]   time.
[00:43:20.080 --> 00:43:24.360]   Instead of doing that, we realized that our soft token prediction loss, if we were just
[00:43:24.360 --> 00:43:27.760]   using the root would be utterly useless.
[00:43:27.760 --> 00:43:32.360]   For example, here, like the person in the gray shirt with a watch on their wrist, the
[00:43:32.360 --> 00:43:38.560]   like, the root would be the person and it's like, kind of trivial to always just predict
[00:43:38.560 --> 00:43:39.560]   the root, right?
[00:43:39.560 --> 00:43:43.680]   So then you're not doing anything about the positional information, like it's not helping
[00:43:43.680 --> 00:43:44.680]   you.
[00:43:44.680 --> 00:43:48.800]   On the other hand, if we like combine multiple sentences where the rest of the sentence has
[00:43:48.800 --> 00:43:52.920]   the other person wearing a blue sweater, the third person in a gray coat and scarf.
[00:43:52.920 --> 00:43:58.780]   Now to like make, to figure out if the box corresponds to which part of the sentence
[00:43:58.780 --> 00:44:04.040]   or like the root of which part of the sentence, it really needs to like reason about giving
[00:44:04.040 --> 00:44:08.200]   the token positions to like the person from the first sentence or the other person from
[00:44:08.200 --> 00:44:10.020]   the second sentence, et cetera.
[00:44:10.020 --> 00:44:15.860]   So this density increase of like information is extremely important to make the soft token
[00:44:15.860 --> 00:44:17.060]   prediction useful.
[00:44:17.060 --> 00:44:21.020]   And also like for the contrast of alignment, because the annotation would be the same,
[00:44:21.020 --> 00:44:22.020]   right?
[00:44:22.020 --> 00:44:26.260]   So yeah, that, that was like the thought behind the different stages that we went through
[00:44:26.260 --> 00:44:28.580]   for our data collection and combination.
[00:44:28.580 --> 00:44:32.540]   Like initially we were like, yeah, let's just like combine the data, but then, you know,
[00:44:32.540 --> 00:44:38.460]   it really makes a difference if some of the data has only like the whole sentence for
[00:44:38.460 --> 00:44:39.460]   one box.
[00:44:39.460 --> 00:44:44.500]   So we also went through a phase where we were trying to have a unlikelihood loss kind of
[00:44:44.500 --> 00:44:50.460]   thing where we were like avoiding penalizing for finding boxes that in sentences where
[00:44:50.460 --> 00:44:53.380]   there are other entities, but we don't have annotations for.
[00:44:53.380 --> 00:44:57.860]   So it got really messy and we were like, okay, let's not go down this like messy road and
[00:44:57.860 --> 00:44:59.360]   we'll just like extract the root.
[00:44:59.360 --> 00:45:00.360]   So yeah.
[00:45:00.360 --> 00:45:01.580]   No, that makes sense.
[00:45:01.580 --> 00:45:05.980]   I guess for someone like me, I'll be very honest, like, cause I'm not like, I've been
[00:45:05.980 --> 00:45:09.580]   part of the computer vision side of things, but not so much on the multimodal side of
[00:45:09.580 --> 00:45:10.580]   things.
[00:45:10.580 --> 00:45:15.140]   And then until I hit MDTR or started reading about it, that's when it struck me, like the
[00:45:15.140 --> 00:45:18.980]   amount of detail that goes into getting a model to train.
[00:45:18.980 --> 00:45:25.660]   So tell me like when you were first experimenting with MDTR, were there lots of field experiments?
[00:45:25.660 --> 00:45:30.620]   And cause I don't think the model would have just started to train and like match the,
[00:45:30.620 --> 00:45:31.900]   and that's when you would go back and forth.
[00:45:31.900 --> 00:45:35.300]   It's like, do we change the pre-training and then you come back and you train your model
[00:45:35.300 --> 00:45:36.300]   and things like that.
[00:45:36.300 --> 00:45:42.660]   It was even crazier because so I did my internship in summer where I was doing a bunch of experiments,
[00:45:42.660 --> 00:45:46.740]   but this pivot towards modulated detection happened much later.
[00:45:46.740 --> 00:45:50.820]   And I didn't have compute to do these large scale experiments anymore.
[00:45:50.820 --> 00:45:56.700]   So most of our loss designing and all of this stuff happened on Clever because that's the
[00:45:56.700 --> 00:45:59.500]   only thing we could run on NYU compute.
[00:45:59.500 --> 00:46:05.500]   So at some point we started collaborating with Gab and Gabrielle and Ishan and Manat
[00:46:05.500 --> 00:46:10.260]   and they also started pitching in, in like running experiments and like helping us with
[00:46:10.260 --> 00:46:13.100]   infrastructure side of things.
[00:46:13.100 --> 00:46:16.180]   And that made it easier to like run more large scale experiments.
[00:46:16.180 --> 00:46:20.900]   But I'll tell you that for pre-training, it was like one shot.
[00:46:20.900 --> 00:46:26.580]   Like we didn't, we didn't have multiple chances because like we were in a, we had a pretty
[00:46:26.580 --> 00:46:31.580]   good preliminary results and on Flickr even without pre-training, like we'll see in the
[00:46:31.580 --> 00:46:36.140]   table that, you know, we had like a eight or nine point increase on the best state of
[00:46:36.140 --> 00:46:38.580]   the art on Flickr entities without pre-training.
[00:46:38.580 --> 00:46:42.780]   So we were like convinced that this would work with pre-training as well.
[00:46:42.780 --> 00:46:49.860]   So we basically just had one shot before the ICV deadline and we just ran it for one week,
[00:46:49.860 --> 00:46:52.300]   the pre-training and then like ran the downstream.
[00:46:52.300 --> 00:46:57.380]   So yeah, we didn't have much space because of the time constraints as well for the deadlines.
[00:46:57.380 --> 00:47:01.540]   But yeah, a lot of experimentation happened on Clever.
[00:47:01.540 --> 00:47:03.140]   So that's what I'd say.
[00:47:03.140 --> 00:47:06.380]   I think that's also a good point that you make.
[00:47:06.380 --> 00:47:10.500]   Like again, this is something we could take from the paper as well.
[00:47:10.500 --> 00:47:14.460]   Like the experimentation, even though it was done on a smaller subset, it kind of generalizes
[00:47:14.460 --> 00:47:16.980]   to a bigger, but then also there's this.
[00:47:16.980 --> 00:47:19.980]   I don't know if that always happens.
[00:47:19.980 --> 00:47:24.780]   I remember like some of the people from our team were like very wary of like spending
[00:47:24.780 --> 00:47:28.260]   too much time on Clever because they were like, yeah, you know, I've done this before.
[00:47:28.260 --> 00:47:31.900]   I spent a whole summer doing Clever stuff, assuming it will generalize to real data and
[00:47:31.900 --> 00:47:33.900]   then nothing worked.
[00:47:33.900 --> 00:47:38.260]   So we were like biting our nails when we ran the pre-training, but thankfully it worked
[00:47:38.260 --> 00:47:39.260]   out.
[00:47:39.260 --> 00:47:40.980]   How many days of that nail biting was it?
[00:47:40.980 --> 00:47:43.540]   I think it was like, I don't know, nine days.
[00:47:43.540 --> 00:47:45.380]   That would have been tough.
[00:47:45.380 --> 00:47:47.860]   Tough nine days, I guess.
[00:47:47.860 --> 00:47:48.860]   Exciting.
[00:47:48.860 --> 00:47:52.140]   That's really, I guess this is really good to see.
[00:47:52.140 --> 00:47:54.860]   Like when you read the paper, these are the stories you miss.
[00:47:54.860 --> 00:47:58.980]   Like, oh, this is like how much effort or there's like, because in the real world, when
[00:47:58.980 --> 00:48:02.780]   we, when the paper hits us, when we're reading the paper, we don't really see, oh, the paper
[00:48:02.780 --> 00:48:06.780]   was written within a deadline and they were like these compute resources or the person
[00:48:06.780 --> 00:48:10.740]   was actually living a life and real human being who's writing this paper.
[00:48:10.740 --> 00:48:13.140]   So there's like all of these constraints that come into it.
[00:48:13.140 --> 00:48:17.100]   And it's really good to see those stories and hear those stories.
[00:48:17.100 --> 00:48:21.420]   And of course, like as PhD students, you want like, you know, papers, you want to like get
[00:48:21.420 --> 00:48:26.300]   to the deadline because like, yeah, you know, it, it feels good when you like submit a paper.
[00:48:26.300 --> 00:48:31.340]   So every, of course, like it would not have been possible, like without Nikola's help,
[00:48:31.340 --> 00:48:36.500]   basically we like stayed up and like worked on it like crazy for like the, you know, month
[00:48:36.500 --> 00:48:38.180]   or and a half before the deadline.
[00:48:38.180 --> 00:48:39.940]   And it would have been impossible without his help.
[00:48:39.940 --> 00:48:41.700]   He's also a really good software engineer.
[00:48:41.700 --> 00:48:45.540]   So that like, I don't have that much software engineering experience.
[00:48:45.540 --> 00:48:50.820]   So like I can, you know, write a reasonably fast code, but like to make it actually work
[00:48:50.820 --> 00:48:54.020]   and performant, like he helped a lot with all of this stuff.
[00:48:54.020 --> 00:48:55.020]   Yeah.
[00:48:55.020 --> 00:48:59.980]   And I think that experience from Bita in the past would come in really handy when, when
[00:48:59.980 --> 00:49:00.980]   we're doing this.
[00:49:00.980 --> 00:49:01.980]   So that's really exciting to hear.
[00:49:01.980 --> 00:49:07.620]   So I guess in terms of the pre-training, this, the Roberta model was just straight from Hugging
[00:49:07.620 --> 00:49:08.620]   Face.
[00:49:08.620 --> 00:49:09.620]   Yeah.
[00:49:09.620 --> 00:49:10.860]   It's a, thanks to Hugging Face.
[00:49:10.860 --> 00:49:15.100]   So shout out to them for like making everything so easy to use.
[00:49:15.100 --> 00:49:16.100]   Amazing.
[00:49:16.100 --> 00:49:17.100]   That's great.
[00:49:17.100 --> 00:49:21.460]   And I guess the, then the last thing that I, in this section, I was really excited to
[00:49:21.460 --> 00:49:23.420]   see was the Tim library.
[00:49:23.420 --> 00:49:30.460]   So someone who's been involved in Tim for quite some time and to see Ross and then that library.
[00:49:30.460 --> 00:49:32.340]   The backbones as well then are customizable.
[00:49:32.340 --> 00:49:36.420]   So you could use any backbone that offers feature extraction from Tim.
[00:49:36.420 --> 00:49:37.420]   Is that correct?
[00:49:37.420 --> 00:49:38.420]   Exactly.
[00:49:38.420 --> 00:49:39.420]   Yeah.
[00:49:39.420 --> 00:49:42.300]   And this is very different from all of the other multi-modal approaches because they've
[00:49:42.300 --> 00:49:46.980]   been using like, you know, the same bottom up, top down extractor from like whichever
[00:49:46.980 --> 00:49:49.140]   2017, which was trained once on VG.
[00:49:49.140 --> 00:49:53.060]   And basically like there's a Docker, which everybody uses to extract images and nobody
[00:49:53.060 --> 00:49:54.980]   ever thought of like improving the detector.
[00:49:54.980 --> 00:50:00.260]   I mean, Vinville did of course, they, you know, applied, they used a lot more detection
[00:50:00.260 --> 00:50:01.260]   data sets.
[00:50:01.260 --> 00:50:03.620]   I don't, I don't think they like improve their detector as such.
[00:50:03.620 --> 00:50:09.620]   They just like use more data, but I'm just saying that like nobody else has like, I guess
[00:50:09.620 --> 00:50:16.980]   made the effort or like try to plug in, you know, better and yeah, just better backbones.
[00:50:16.980 --> 00:50:18.700]   And there's no reason not to, right.
[00:50:18.700 --> 00:50:24.380]   And Dieter made that like extremely easy for us because they had like a very smooth pipeline
[00:50:24.380 --> 00:50:27.780]   for just like plugging in anything, any pipeline and any backbone.
[00:50:27.780 --> 00:50:31.260]   And we're like, yeah, we're not efficient at B3, B5.
[00:50:31.260 --> 00:50:32.260]   Yeah.
[00:50:32.260 --> 00:50:33.260]   Yeah.
[00:50:33.260 --> 00:50:34.580]   Let's keep trying and let's keep doing that.
[00:50:34.580 --> 00:50:36.980]   And then I see you use the noisy students.
[00:50:36.980 --> 00:50:41.620]   I think that would help as well for the backbone because we want to extract really good features
[00:50:41.620 --> 00:50:42.620]   from the image itself.
[00:50:42.620 --> 00:50:43.620]   And then when there's like.
[00:50:43.620 --> 00:50:46.020]   We don't have to use this stuff, right?
[00:50:46.020 --> 00:50:50.460]   Like it's like somebody just mentioned that using bottom up, top down detector is such
[00:50:50.460 --> 00:50:51.460]   a pain.
[00:50:51.460 --> 00:50:52.460]   Yeah.
[00:50:52.460 --> 00:50:55.180]   Because like you have to like extract the features, dump them and then like do all of
[00:50:55.180 --> 00:50:56.180]   this stuff.
[00:50:56.180 --> 00:50:59.660]   And then just part of your model, you train everything end to end and it's just like clean.
[00:50:59.660 --> 00:51:00.660]   So yeah.
[00:51:00.660 --> 00:51:02.780]   No, that's the benefit of it.
[00:51:02.780 --> 00:51:03.780]   And that's what we see.
[00:51:03.780 --> 00:51:08.020]   Like if Robot is trained on like so many million tokens, then there's no reason not to have
[00:51:08.020 --> 00:51:10.740]   the vision backbones be trained on that.
[00:51:10.740 --> 00:51:11.740]   Yeah, exactly.
[00:51:11.740 --> 00:51:13.620]   Why only do the index.
[00:51:13.620 --> 00:51:14.620]   Yeah, exactly.
[00:51:14.620 --> 00:51:15.620]   That's wonderful to hear.
[00:51:15.620 --> 00:51:17.860]   So I guess now we're just down to the last sections.
[00:51:17.860 --> 00:51:19.580]   I'm also conscious of time.
[00:51:19.580 --> 00:51:24.220]   So maybe we'll go through the, maybe Aishwarya you could just quickly go through the downstream
[00:51:24.220 --> 00:51:29.260]   tasks and if you have some show reasons you want to share with us about any of the downstream
[00:51:29.260 --> 00:51:30.260]   fine tuning.
[00:51:30.260 --> 00:51:31.260]   Yeah.
[00:51:31.260 --> 00:51:37.220]   So, so for phrase grounding, basically because our data and our pre-training task was exactly
[00:51:37.220 --> 00:51:40.660]   just phrase grounding, we didn't fine tune any further.
[00:51:40.660 --> 00:51:46.540]   We just use our pre-trained model after 40 epochs that trained for about a week on 32
[00:51:46.540 --> 00:51:50.500]   GPUs and just evaluated it on the Flickr dataset.
[00:51:50.500 --> 00:51:58.540]   So there's, it's the one below the, sorry, the table is the one below, not the, yeah.
[00:51:58.540 --> 00:51:59.540]   This one?
[00:51:59.540 --> 00:52:00.540]   Yeah.
[00:52:00.540 --> 00:52:01.540]   So there's two protocols.
[00:52:01.540 --> 00:52:02.540]   This is a bit weird.
[00:52:02.540 --> 00:52:06.820]   We had to report on two different protocols because we actually spent quite a bit of time
[00:52:06.820 --> 00:52:10.700]   trying to make sure we're evaluating it in the right way, because there seems to be like
[00:52:10.700 --> 00:52:16.220]   two line of work, like the ones in the bottom half of the table don't cite the ones in the
[00:52:16.220 --> 00:52:18.980]   top and vice versa for some reason.
[00:52:18.980 --> 00:52:22.300]   So the one in the top are like the image tags, multimodal stuff.
[00:52:22.300 --> 00:52:26.540]   And the one in the bottom are more pure vision where they just like condition on the text
[00:52:26.540 --> 00:52:30.580]   and try to do detection, but they don't really do anything other than just like detecting
[00:52:30.580 --> 00:52:31.580]   the box.
[00:52:31.580 --> 00:52:32.580]   Right.
[00:52:32.580 --> 00:52:38.620]   So most box protocol just means that if there is one text, which refers to multiple boxes,
[00:52:38.620 --> 00:52:41.820]   then you have to like merge all the ground truth.
[00:52:41.820 --> 00:52:48.420]   Now this is problematic if you think about it, because if you pre-extract faster R-CNN
[00:52:48.420 --> 00:52:53.820]   boxes and try to like rank them using like, you know, whatever, how other people do it,
[00:52:53.820 --> 00:52:57.700]   you're not going to have boxes that are merged because the object detector boxes are supposed
[00:52:57.700 --> 00:53:00.860]   to be like separate boxes for each thing in the image.
[00:53:00.860 --> 00:53:07.260]   So yeah, that's why like these approaches like only cite among themselves, I guess.
[00:53:07.260 --> 00:53:10.940]   On the other hand, any box protocol does not mess with the ground truth.
[00:53:10.940 --> 00:53:13.340]   They say like, yeah, you have separate boxes.
[00:53:13.340 --> 00:53:19.580]   So if you just count it as a positive or like a correct thing, if you have more than 0.5
[00:53:19.580 --> 00:53:22.300]   IOU with any of the boxes.
[00:53:22.300 --> 00:53:24.020]   So that's why we call it any box protocol.
[00:53:24.020 --> 00:53:28.620]   So just to be clear about like what these two things are, but yeah, on both of these,
[00:53:28.620 --> 00:53:35.820]   basically we have like, yeah, we went from 71.3 to 83 and this was like with pre-training,
[00:53:35.820 --> 00:53:39.580]   but you can see the one without pre-training before that on Val, because we didn't like
[00:53:39.580 --> 00:53:40.580]   test it.
[00:53:40.580 --> 00:53:42.840]   It went from 70 to 78.
[00:53:42.840 --> 00:53:48.580]   So yeah, there was a huge boost in performance on this task and we were like super excited.
[00:53:48.580 --> 00:53:52.980]   And this is what gave us the confidence to run the large pre-training run and like get
[00:53:52.980 --> 00:53:54.540]   results on that.
[00:53:54.540 --> 00:54:00.460]   And then the table above is the referring expressions comprehension.
[00:54:00.460 --> 00:54:01.460]   Can you go up a bit, please?
[00:54:01.460 --> 00:54:02.460]   Yeah, one second.
[00:54:02.460 --> 00:54:03.460]   There we go.
[00:54:03.460 --> 00:54:04.460]   Yeah.
[00:54:04.460 --> 00:54:11.380]   So this one is, it seems like more papers report on this task.
[00:54:11.380 --> 00:54:13.980]   It's more popular than the Flickr.
[00:54:13.980 --> 00:54:20.960]   But the main idea here is that you have an image which has like multiple objects, which
[00:54:20.960 --> 00:54:25.620]   are often of the same class, but there will be a text which distinguishes the two.
[00:54:25.620 --> 00:54:29.660]   So like there'll be a man in a red shirt, for example, where there'll be like two men
[00:54:29.660 --> 00:54:30.660]   in the image.
[00:54:30.660 --> 00:54:33.880]   So you need to like take the context and figure out which of the two you need to put a box
[00:54:33.880 --> 00:54:34.880]   around.
[00:54:34.880 --> 00:54:35.880]   Like the pink elephant.
[00:54:35.880 --> 00:54:36.880]   Yeah, the pink elephant.
[00:54:36.880 --> 00:54:42.860]   Yeah, so the RefCoco, RefCoco+ and RefCocoG are three of these.
[00:54:42.860 --> 00:54:47.640]   RefCocoG is, I think, the hardest of these three because it has much longer sentences
[00:54:47.640 --> 00:54:53.600]   and like they're not allowed to use like just, like the RefCoco is pretty silly sometimes
[00:54:53.600 --> 00:54:58.320]   actually because it'll be like left or man right or something like that.
[00:54:58.320 --> 00:55:02.760]   Like the sentence is extremely short and not very interesting, but RefCocoG is like much
[00:55:02.760 --> 00:55:07.160]   more flowery language because these datasets were collected like one after the other with
[00:55:07.160 --> 00:55:11.440]   like refinement and how the process was used to like collect.
[00:55:11.440 --> 00:55:16.880]   In terms of results, you can see on RefCocoG, we have like a good five points increase also
[00:55:16.880 --> 00:55:20.360]   on this, on this, on this task.
[00:55:20.360 --> 00:55:21.360]   So that's really exciting.
[00:55:21.360 --> 00:55:28.980]   Vila is a paper which does like adversarial learning on top of really good image text
[00:55:28.980 --> 00:55:30.360]   models that already exist.
[00:55:30.360 --> 00:55:32.240]   And it's, I think, from NeurIPS last year.
[00:55:32.240 --> 00:55:34.680]   So it was pretty recent when we submitted this.
[00:55:34.680 --> 00:55:38.040]   So we were really excited when we saw that, you know, our results are like substantially
[00:55:38.040 --> 00:55:42.920]   better than all of the baselines, even though they like train on like what 4.6 million images
[00:55:42.920 --> 00:55:44.360]   and stuff like that.
[00:55:44.360 --> 00:55:50.520]   So yeah, this was, I think this is just like a consequence of the fact that our pre-training
[00:55:50.520 --> 00:55:55.640]   is basically doing the right kind of learning for this kind of task.
[00:55:55.640 --> 00:56:02.000]   Like, you know, all these other papers, they take faster CNN boxes and then kind of rank
[00:56:02.000 --> 00:56:04.240]   the boxes according to the text.
[00:56:04.240 --> 00:56:06.120]   They don't actually predict the box.
[00:56:06.120 --> 00:56:11.120]   So their performance is kind of upper bounded by the boxes that they have, which might not
[00:56:11.120 --> 00:56:12.440]   be good in the first place.
[00:56:12.440 --> 00:56:17.320]   So yeah, I think like the way we do it is like kind of pushing people to like change
[00:56:17.320 --> 00:56:19.160]   the way that they approach these tasks.
[00:56:19.160 --> 00:56:20.160]   And that's important.
[00:56:20.160 --> 00:56:21.160]   Yeah.
[00:56:21.160 --> 00:56:23.440]   No, I think that's a really, really good point.
[00:56:23.440 --> 00:56:31.440]   And having this end to end MD that makes things, I guess, makes things so easy instead of having
[00:56:31.440 --> 00:56:35.160]   like that two stage approach, because then you can have reason and you can match all
[00:56:35.160 --> 00:56:37.360]   of these different bounding boxes to one another.
[00:56:37.360 --> 00:56:39.680]   And that's really exciting to hear.
[00:56:39.680 --> 00:56:43.240]   So I guess that's in terms of the downstream tasks.
[00:56:43.240 --> 00:56:47.800]   There's also like a couple more, which maybe we can go over very quickly.
[00:56:47.800 --> 00:56:49.480]   Which is the segmentation?
[00:56:49.480 --> 00:56:54.280]   Yeah, there's GQA, which is question answering, which I guess is like a really popular data
[00:56:54.280 --> 00:57:00.120]   set because it's a bit cooler than VQA in my, yeah, you can put this on.
[00:57:00.120 --> 00:57:02.480]   I was going to put this on because I think this is important.
[00:57:02.480 --> 00:57:06.760]   I was going to ask question about the QA specific queries and how that's different.
[00:57:06.760 --> 00:57:09.440]   Thanks for bringing this up.
[00:57:09.440 --> 00:57:14.960]   So for question answering, the main difference is apart from just detecting the boxes and
[00:57:14.960 --> 00:57:19.200]   knowing which box corresponds to which part of the text, you also have to give an answer.
[00:57:19.200 --> 00:57:24.200]   So we had to fine tune on the GQA data set.
[00:57:24.200 --> 00:57:27.280]   In addition to the existing architecture, which, you know, everything would have been
[00:57:27.280 --> 00:57:32.080]   the same as like for pre-training, except for these QA specific queries, like you just
[00:57:32.080 --> 00:57:33.440]   pointed out.
[00:57:33.440 --> 00:57:36.160]   So this is just like something we call it.
[00:57:36.160 --> 00:57:39.600]   It's just a few more object queries.
[00:57:39.600 --> 00:57:40.600]   Nothing special about them.
[00:57:40.600 --> 00:57:45.160]   All of these things at the bottom of the decoder are just like learnable embeddings.
[00:57:45.160 --> 00:57:47.360]   So they're just initialized from scratch.
[00:57:47.360 --> 00:57:51.600]   During pre-training, obviously we don't have this QA specific query.
[00:57:51.600 --> 00:57:56.480]   So yeah, during the fine tuning, we initialize from scratch and the output of these is used
[00:57:56.480 --> 00:58:01.280]   to predict first the question type, where in GQA, we just like divide it by semantic
[00:58:01.280 --> 00:58:02.600]   types.
[00:58:02.600 --> 00:58:09.240]   And then according to like which type it is, you'll have like a softmax over the answers
[00:58:09.240 --> 00:58:10.560]   for that type of question.
[00:58:10.560 --> 00:58:16.120]   So if it's like, you know, very, like for clever, like it's like count questions, you'll
[00:58:16.120 --> 00:58:17.760]   have like numbers or whatever.
[00:58:17.760 --> 00:58:18.760]   So yeah, it's a...
[00:58:18.760 --> 00:58:20.440]   That makes sense.
[00:58:20.440 --> 00:58:24.600]   It's very similar to the existing pre-training model.
[00:58:24.600 --> 00:58:29.240]   So that makes like the new components of the model that need to be trained are basically
[00:58:29.240 --> 00:58:34.440]   just these heads on top of the queries, QA specific queries.
[00:58:34.440 --> 00:58:37.880]   I missed the details, but in terms of like then QA specific, do you remember off the
[00:58:37.880 --> 00:58:39.360]   top of your head, like how many queries?
[00:58:39.360 --> 00:58:43.040]   Because I think Dita had a hundred object queries as a standard.
[00:58:43.040 --> 00:58:45.920]   So in GQA, I think there's five answer types.
[00:58:45.920 --> 00:58:50.840]   So basically we'll have, you know, six of them because like one to predict the type
[00:58:50.840 --> 00:58:54.000]   and the five, which are the actual going to predict the answer.
[00:58:54.000 --> 00:58:55.000]   Right.
[00:58:55.000 --> 00:58:58.920]   So then I guess then when we're doing the fine tuning, you just add those six to the,
[00:58:58.920 --> 00:59:02.520]   instead of like initializing the embedding as a hundred, you just initialize it as a
[00:59:02.520 --> 00:59:05.600]   hundred and six, which is the difference here.
[00:59:05.600 --> 00:59:06.600]   Okay.
[00:59:06.600 --> 00:59:09.600]   That makes, that's really good to hear.
[00:59:09.600 --> 00:59:13.760]   So I guess then we just found at the conclusion, which is MDTR.
[00:59:13.760 --> 00:59:14.760]   I am so...
[00:59:14.760 --> 00:59:17.880]   If we look at the image once before we go to that, like I...
[00:59:17.880 --> 00:59:18.880]   Oh, I'm so sorry.
[00:59:18.880 --> 00:59:19.880]   The segmentation?
[00:59:19.880 --> 00:59:20.880]   No, no.
[00:59:20.880 --> 00:59:23.400]   This example of the question answering.
[00:59:23.400 --> 00:59:24.400]   Oh yes.
[00:59:24.400 --> 00:59:25.400]   Yeah.
[00:59:25.400 --> 00:59:30.680]   So yeah, I guess another cool thing that I want to point out about like MDTR is basically,
[00:59:30.680 --> 00:59:37.320]   you know, you, because you have such dense grounding, you can really figure out what
[00:59:37.320 --> 00:59:40.920]   it's looking at or what it's reasoning using.
[00:59:40.920 --> 00:59:46.040]   So what is on the table here, it has a box around the laptop for the word what, and you
[00:59:46.040 --> 00:59:47.920]   know, a box on the table for table.
[00:59:47.920 --> 00:59:51.280]   So it gives you a lot of like interpretability for question answering.
[00:59:51.280 --> 00:59:56.720]   And it's really exciting that we can like actually tell you exactly what it's looking
[00:59:56.720 --> 01:00:01.440]   at or like, sure, we used to have attention maps, but they were never like very clean,
[01:00:01.440 --> 01:00:02.440]   right.
[01:00:02.440 --> 01:00:07.600]   It's very different from having like explicit boxes, which are clean and like, you know,
[01:00:07.600 --> 01:00:09.680]   very tight around the object.
[01:00:09.680 --> 01:00:10.680]   So yeah.
[01:00:10.680 --> 01:00:11.680]   Yeah.
[01:00:11.680 --> 01:00:16.080]   And we'll go back to segmentation as well, because now I realized that I skipped segmentation.
[01:00:16.080 --> 01:00:21.640]   I think there was an image somewhere about, is it in this version of the paper, Aishwarya?
[01:00:21.640 --> 01:00:26.360]   I think we don't have a, yeah, model thing, but we can look at it in the colab once we're
[01:00:26.360 --> 01:00:27.360]   done with the paper.
[01:00:27.360 --> 01:00:28.360]   Oh, I know that.
[01:00:28.360 --> 01:00:30.720]   I know a really good place to find that.
[01:00:30.720 --> 01:00:31.720]   It's your GitHub repository.
[01:00:31.720 --> 01:00:34.480]   There's this link to the website.
[01:00:34.480 --> 01:00:39.520]   And I think there was a, I'll just go to the segmentation part, because I think that's
[01:00:39.520 --> 01:00:40.960]   really cool to see as well.
[01:00:40.960 --> 01:00:43.040]   Like that's just another, oh, there it is.
[01:00:43.040 --> 01:00:45.760]   Like referring this street lamp, which MDTR can do.
[01:00:45.760 --> 01:00:49.120]   Yeah, it's pretty cool because you know, that street lamp is kind of odd looking and
[01:00:49.120 --> 01:00:53.320]   it's far away in the background, but we still able to like give a very tight mask around
[01:00:53.320 --> 01:00:54.320]   it.
[01:00:54.320 --> 01:00:58.560]   Even the next one, like major league logo, that seems like not a very straightforward
[01:00:58.560 --> 01:00:59.560]   thing to like find.
[01:00:59.560 --> 01:01:01.080]   And it does a very good job at it.
[01:01:01.080 --> 01:01:02.080]   So yeah.
[01:01:02.080 --> 01:01:04.200]   I mean, of course we have, it's not perfect, right?
[01:01:04.200 --> 01:01:08.120]   Like these are like good examples that work well.
[01:01:08.120 --> 01:01:12.440]   It's not like we have a hundred percent accuracy on this, but it's exciting to see that it
[01:01:12.440 --> 01:01:15.680]   can do really well on a lot of images.
[01:01:15.680 --> 01:01:19.560]   Well the fact that you have examples to cherry pick, I think that's an accomplishment in
[01:01:19.560 --> 01:01:20.560]   itself.
[01:01:20.560 --> 01:01:23.640]   So you have examples of major league logo that can do this.
[01:01:23.640 --> 01:01:25.640]   I think that's quite a good accomplishment.
[01:01:25.640 --> 01:01:32.920]   I just feel like it's important to also like state your limitations, not be like, you know,
[01:01:32.920 --> 01:01:35.520]   this is the best thing which can like solve every task.
[01:01:35.520 --> 01:01:36.520]   Yeah.
[01:01:36.520 --> 01:01:39.800]   I mean, there's still more work to be done, but yeah.
[01:01:39.800 --> 01:01:40.800]   Absolutely.
[01:01:40.800 --> 01:01:41.800]   Thanks, Aras.
[01:01:41.800 --> 01:01:46.720]   So in terms of, I guess we'll head to the EMA section now then, is there anything else
[01:01:46.720 --> 01:01:51.400]   you want to add about the paper or you could for the EMA section, Aishwarya?
[01:01:51.400 --> 01:01:52.400]   Yeah, no.
[01:01:52.400 --> 01:01:55.680]   I was just sending somebody the GitHub link, but yeah.
[01:01:55.680 --> 01:02:01.680]   Yeah, no, I just want to say that overall, like this was a really fun project and, you
[01:02:01.680 --> 01:02:08.600]   know, I hope the code is like easy to use and we made sure to like release the code
[01:02:08.600 --> 01:02:15.000]   and release like colabs so you can also like try and set up like add URLs or any images
[01:02:15.000 --> 01:02:19.320]   from the web and like try to do face detection, try to do segmentation, try to do question
[01:02:19.320 --> 01:02:21.080]   answering and see what you get.
[01:02:21.080 --> 01:02:25.440]   So it like will give you a really good sense of how the model is doing.
[01:02:25.440 --> 01:02:28.840]   Maybe if you have time in the, after the EMA, we can like show them an example.
[01:02:28.840 --> 01:02:34.160]   But yeah, I just want to say that don't feel scared to like play with this.
[01:02:34.160 --> 01:02:39.360]   I think there's a lot more that can be done once, yeah, once you get used to like, you
[01:02:39.360 --> 01:02:43.840]   know, these kind of models and it's an exciting area to work in.
[01:02:43.840 --> 01:02:46.240]   And the one thing I do want to, I think that's a great point to highlight.
[01:02:46.240 --> 01:02:50.160]   Another thing is that at Wits and Biases, then there's something new that we're going
[01:02:50.160 --> 01:02:54.720]   to try as well, that after the paper reading groups, we're going to start going into the
[01:02:54.720 --> 01:02:56.080]   code for those paper reading groups.
[01:02:56.080 --> 01:02:57.920]   I guess it's really handy.
[01:02:57.920 --> 01:03:02.440]   It comes from the idea of like, it's really handy to know what the paper is about, but
[01:03:02.440 --> 01:03:04.400]   it's equally handy to then apply that paper.
[01:03:04.400 --> 01:03:08.320]   So we're going to try, actually, we're going to try and go into the details of how MDTR
[01:03:08.320 --> 01:03:09.320]   is implemented.
[01:03:09.320 --> 01:03:13.200]   And we might fail at doing that, but we at least are going to try and get our hands dirty
[01:03:13.200 --> 01:03:16.680]   and look at all the loss functions and everything in PyTorch code.
[01:03:16.680 --> 01:03:20.880]   So that's something that we will do very soon.
[01:03:20.880 --> 01:03:24.400]   So that being said, let me just quickly go to the forums for EMA.
[01:03:24.400 --> 01:03:26.360]   All right.
[01:03:26.360 --> 01:03:28.640]   So we've got a few questions.
[01:03:28.640 --> 01:03:31.200]   So we'll start with this one.
[01:03:31.200 --> 01:03:36.160]   Question by Ramesh, if there's no fixed number of classes in the output, the predicted boxes
[01:03:36.160 --> 01:03:38.760]   is doing soft distribution over input tokens.
[01:03:38.760 --> 01:03:40.320]   I answered this in the chat.
[01:03:40.320 --> 01:03:41.320]   Oh, okay.
[01:03:41.320 --> 01:03:42.320]   Oh yes.
[01:03:42.320 --> 01:03:43.320]   Okay.
[01:03:43.320 --> 01:03:45.080]   I did actually answer it in the chat.
[01:03:45.080 --> 01:03:48.280]   Sorry, I should read the whole thing first.
[01:03:48.280 --> 01:03:50.000]   Let's move on to the next one.
[01:03:50.000 --> 01:03:54.680]   Can you go over the flattening and concatenating of image features and text features?
[01:03:54.680 --> 01:03:57.960]   I didn't quite understand what the final shape is expected to be.
[01:03:57.960 --> 01:04:03.960]   So if it was like batch size by sequence length by feature dimension, by the time you apply
[01:04:03.960 --> 01:04:08.200]   the linearity to project to the size of the transformer.
[01:04:08.200 --> 01:04:10.680]   So the transformer size we use is 256.
[01:04:10.680 --> 01:04:13.080]   This is another thing that we never adulated, by the way.
[01:04:13.080 --> 01:04:14.560]   We never tried bigger sizes.
[01:04:14.560 --> 01:04:18.680]   It's very possible that if you increase the size, you'll get even better performance.
[01:04:18.680 --> 01:04:23.720]   But yeah, there was limited time and we didn't want to change too many things because Ditter
[01:04:23.720 --> 01:04:25.440]   has a lot of knobs.
[01:04:25.440 --> 01:04:29.840]   We know a setting that works, so we thought we'll just go with that.
[01:04:29.840 --> 01:04:33.640]   But yeah, so the CNN output is linearized.
[01:04:33.640 --> 01:04:40.600]   So you'll have batch size by a really long sequence of the linearized image features
[01:04:40.600 --> 01:04:44.120]   by whatever the feature dimension was at that layer.
[01:04:44.120 --> 01:04:49.040]   And that's going to be down projected basically to 256 or I don't know, down or up projected,
[01:04:49.040 --> 01:04:50.560]   basically changed to 256.
[01:04:50.560 --> 01:04:52.840]   And on the reverse- Sorry to cut you.
[01:04:52.840 --> 01:04:53.840]   Just a quick one then.
[01:04:53.840 --> 01:04:55.440]   So I'm just going to do this flattening.
[01:04:55.440 --> 01:04:57.320]   So this then just becomes 256, right?
[01:04:57.320 --> 01:05:01.400]   So then I've got 64 by 256 because then that becomes my number of channels.
[01:05:01.400 --> 01:05:04.240]   Or do you flatten this out as well, the whole thing?
[01:05:04.240 --> 01:05:08.320]   No, it's like sequence length remains the same.
[01:05:08.320 --> 01:05:09.320]   The sequence length stays.
[01:05:09.320 --> 01:05:10.320]   All right.
[01:05:10.320 --> 01:05:11.320]   Okay.
[01:05:11.320 --> 01:05:12.320]   So keep going.
[01:05:12.320 --> 01:05:16.160]   So I guess the 16 by 16 is what becomes the sequence length and the 64 is your feature
[01:05:16.160 --> 01:05:17.160]   dimension.
[01:05:17.160 --> 01:05:18.160]   The 64 will become also 256 basically.
[01:05:18.160 --> 01:05:19.160]   Okay.
[01:05:19.160 --> 01:05:22.320]   So it's just a reshaping operation that happens after.
[01:05:22.320 --> 01:05:24.680]   You end up being like 256 by 64.
[01:05:24.680 --> 01:05:25.680]   Yeah.
[01:05:25.680 --> 01:05:31.160]   And then 64 will go through a linear and then you like to be the size of the transformer.
[01:05:31.160 --> 01:05:32.800]   And on the other side, yeah.
[01:05:32.800 --> 01:05:38.080]   And then the other side, the 10 by 768 will become 10 by 256, which is the size of the
[01:05:38.080 --> 01:05:39.080]   transformer.
[01:05:39.080 --> 01:05:40.600]   And then you could reshape that again.
[01:05:40.600 --> 01:05:41.600]   Yeah.
[01:05:41.600 --> 01:05:46.680]   So the 10 by 256 will be concat with the 256 by 256 basically.
[01:05:46.680 --> 01:05:50.080]   So you'll get like 266 by 256.
[01:05:50.080 --> 01:05:51.080]   Yes.
[01:05:51.080 --> 01:05:52.080]   That makes sense.
[01:05:52.080 --> 01:05:53.080]   Yeah.
[01:05:53.080 --> 01:05:54.080]   Absolutely.
[01:05:54.080 --> 01:05:55.080]   Okay.
[01:05:55.080 --> 01:05:56.880]   For this question, I guess it will be helpful when we go through the code.
[01:05:56.880 --> 01:06:01.280]   So that's something we do plan on doing next week anyway, but that was really good.
[01:06:01.280 --> 01:06:03.040]   I think that should help it.
[01:06:03.040 --> 01:06:06.880]   So we have 10 by 256 for these words.
[01:06:06.880 --> 01:06:11.120]   And then, because this 64 is going through the linear layer, which will then just change
[01:06:11.120 --> 01:06:12.480]   the dimension to be 256.
[01:06:12.480 --> 01:06:16.340]   So you have 256 by 256 coming from here.
[01:06:16.340 --> 01:06:20.160]   And when you concatenate these two features, the top one being the image, the bottom one
[01:06:20.160 --> 01:06:26.400]   being the text, you get something like 266 by 256, which then goes into the transformer.
[01:06:26.400 --> 01:06:27.400]   Cool.
[01:06:27.400 --> 01:06:34.040]   Fromish, I'm still not very clear on contrastive loss alignment.
[01:06:34.040 --> 01:06:38.240]   Is it like a triplet loss of minimizing the cosine distance between similar things?
[01:06:38.240 --> 01:06:39.240]   How do you handle negative examples?
[01:06:39.240 --> 01:06:40.240]   Yes.
[01:06:40.240 --> 01:06:44.080]   You can think of it as like a margin loss where the thing that is being optimized, like
[01:06:44.080 --> 01:06:49.520]   the cosine distance between the two, between the image and text embeddings.
[01:06:49.520 --> 01:06:54.080]   So if you open up the equation that we have in the paper, you'll get two terms.
[01:06:54.080 --> 01:06:58.720]   Maybe you can just look at it quickly.
[01:06:58.720 --> 01:06:59.720]   So yeah.
[01:06:59.720 --> 01:07:06.000]   So if you look at the numerator, if you take a log of A by B, you're going to get log A
[01:07:06.000 --> 01:07:12.680]   minus like log of A minus log B. So basically, the log and the X will cancel.
[01:07:12.680 --> 01:07:17.120]   So you'll get one term, which will have all of this summation stuff outside.
[01:07:17.120 --> 01:07:23.380]   And then something which is just minus of OI transpose TJ divided by temperature.
[01:07:23.380 --> 01:07:30.220]   So that OI transpose TJ will need to become big so that this entire term is small because
[01:07:30.220 --> 01:07:31.900]   you want to minimize the loss.
[01:07:31.900 --> 01:07:37.100]   So minus of this term needs to be big so that you have the whole term go down.
[01:07:37.100 --> 01:07:43.240]   So that's why the embeddings of OI and TJ need to be similar so that their dot product
[01:07:43.240 --> 01:07:45.880]   basically needs to be big.
[01:07:45.880 --> 01:07:50.640]   And keep in mind that this OI and TJ, if you look at the code, you'll see that it's normalized
[01:07:50.640 --> 01:07:51.860]   before this dot product.
[01:07:51.860 --> 01:07:55.500]   So it's a cosine similarity and not just a dot product.
[01:07:55.500 --> 01:07:59.700]   Because if it was just a dot product, the norms of the vectors can basically just increase
[01:07:59.700 --> 01:08:02.840]   as much as possible to make this big.
[01:08:02.840 --> 01:08:05.380]   And you don't want that because you'll get instability in training.
[01:08:05.380 --> 01:08:10.060]   So it's important to normalize these two vectors, take the dot product, and that's the first
[01:08:10.060 --> 01:08:11.060]   part of the term.
[01:08:11.060 --> 01:08:17.940]   The second term, you'll notice, is a log sum x, which the denominator, when it comes up,
[01:08:17.940 --> 01:08:23.120]   you'll get minus of log of 1 by something so that negative cancels.
[01:08:23.120 --> 01:08:26.440]   So you'll get log of sum of x.
[01:08:26.440 --> 01:08:34.800]   So that term and the summation outside, you'll notice, has a J, which doesn't depend on the
[01:08:34.800 --> 01:08:36.600]   inner term of the denominator.
[01:08:36.600 --> 01:08:37.820]   Doesn't have any Js.
[01:08:37.820 --> 01:08:41.740]   So basically, your summation and the 1 by TI plus will cancel.
[01:08:41.740 --> 01:08:46.200]   And you'll just have a positive log sum x of the denominator.
[01:08:46.200 --> 01:08:52.460]   So that term is basically the thing which is all the positive and negative.
[01:08:52.460 --> 01:08:56.140]   So in general, yes, you can also have just the negatives.
[01:08:56.140 --> 01:08:58.220]   The summation could have been just over the negatives.
[01:08:58.220 --> 01:09:01.740]   But experimentally, we tried both.
[01:09:01.740 --> 01:09:06.460]   And it seemed like if you have all the terms instead of just the negative terms, it's more
[01:09:06.460 --> 01:09:07.460]   stable.
[01:09:07.460 --> 01:09:08.460]   So you can have it like this.
[01:09:08.460 --> 01:09:12.340]   I just have a quick question.
[01:09:12.340 --> 01:09:16.900]   When you mentioned J, what did you mean when you say the denominator?
[01:09:16.900 --> 01:09:17.900]   Just this bit here?
[01:09:17.900 --> 01:09:18.900]   Yeah, yeah.
[01:09:18.900 --> 01:09:25.940]   So when you expand the log of the numerator by denominator, if you do log A by B, it's
[01:09:25.940 --> 01:09:27.140]   log A minus log B, right?
[01:09:27.140 --> 01:09:33.780]   So when the second term will have all of the summation terms and will also have log of
[01:09:33.780 --> 01:09:35.020]   the denominator.
[01:09:35.020 --> 01:09:38.700]   But if you notice, the denominator does not have any J terms.
[01:09:38.700 --> 01:09:41.140]   It has no J index.
[01:09:41.140 --> 01:09:45.140]   So basically, you can take that out of the summation and the 1 by TI plus.
[01:09:45.140 --> 01:09:46.140]   Oh, of course.
[01:09:46.140 --> 01:09:47.140]   OK, that makes sense.
[01:09:47.140 --> 01:09:48.140]   Got it.
[01:09:48.140 --> 01:09:49.140]   Thank you very much.
[01:09:49.140 --> 01:09:56.040]   So I explain it like this because that's explicitly how it's implemented in the code.
[01:09:56.040 --> 01:09:59.340]   So it might help to hear this explanation when you read the code.
[01:09:59.340 --> 01:10:02.860]   It's like-- because we implemented it line by line from the equation.
[01:10:02.860 --> 01:10:06.100]   So you'll be like, oh, why are you doing it like this?
[01:10:06.100 --> 01:10:11.340]   And maybe after-- if it doesn't make sense next week when you're reading the code, try
[01:10:11.340 --> 01:10:13.020]   opening up the equation like this.
[01:10:13.020 --> 01:10:14.820]   And that's exactly what is implemented.
[01:10:14.820 --> 01:10:15.820]   So yeah.
[01:10:15.820 --> 01:10:16.820]   Thank you very much.
[01:10:16.820 --> 01:10:20.380]   Yeah, for context for everybody, I emailed Aishwarya before this event.
[01:10:20.380 --> 01:10:23.780]   And I'm like, Aishwarya, the equations 1 and 2 could be really daunting.
[01:10:23.780 --> 01:10:25.700]   So maybe it might help to go through the code.
[01:10:25.700 --> 01:10:28.380]   But that's something we will do next week anyway.
[01:10:28.380 --> 01:10:32.980]   But after that explanation, I don't think those equations are daunting at all.
[01:10:32.980 --> 01:10:34.900]   So thanks very much for that.
[01:10:34.900 --> 01:10:38.460]   So I guess Ramesh is asking, what downstream tasks can we use?
[01:10:38.460 --> 01:10:41.780]   MDT4, can it be image-only downstream tasks?
[01:10:41.780 --> 01:10:45.700]   Or does it need to be image text in GQA?
[01:10:45.700 --> 01:10:49.660]   Does the labels need to be as detailed as the initial pre-training?
[01:10:49.660 --> 01:10:52.420]   How much data is required for downstream tasks?
[01:10:52.420 --> 01:10:56.500]   So the first question, I'll just say that, I mean, it's supposed to be a multimodal model.
[01:10:56.500 --> 01:11:01.860]   So I'm not sure why you would want to use it for image or text only.
[01:11:01.860 --> 01:11:04.420]   For the GQA, that's a good question.
[01:11:04.420 --> 01:11:09.260]   Yes, like GQA, when we were training, when we were fine-tuning, we first fine-tuned where
[01:11:09.260 --> 01:11:14.600]   we actually used the alignment between the image-- sorry, the objects and the text.
[01:11:14.600 --> 01:11:17.060]   So because GQA provides this, we were like, why not?
[01:11:17.060 --> 01:11:22.780]   We will continue having this phrase grounding loss, like the normal loss from pre-training,
[01:11:22.780 --> 01:11:27.020]   in addition to the fine-tuning on the actual question answering.
[01:11:27.020 --> 01:11:29.180]   And how much data, it depends.
[01:11:29.180 --> 01:11:33.540]   Like for Flickr, we didn't fine-tune to it because we basically just used all of the
[01:11:33.540 --> 01:11:37.100]   data for pre-training and just reported the results.
[01:11:37.100 --> 01:11:42.100]   For RefCoCo, it's-- I don't remember anymore.
[01:11:42.100 --> 01:11:46.060]   Sorry, it's been-- my last few months have been working on a completely different project.
[01:11:46.060 --> 01:11:50.380]   So like the exact numbers of the data have slipped from my mind.
[01:11:50.380 --> 01:11:52.660]   But yeah, I would say it's not very big.
[01:11:52.660 --> 01:11:58.380]   So that's also a reason why we only fine-tune the ResNet model and the B3, but not the B5
[01:11:58.380 --> 01:12:02.740]   on the RefCoCo because the model is much bigger, and we didn't want it to overfit on the small
[01:12:02.740 --> 01:12:03.780]   data sets.
[01:12:03.780 --> 01:12:08.060]   So I would say if you don't have that much fine-tuning data for your downstream task,
[01:12:08.060 --> 01:12:09.900]   use the smaller model.
[01:12:09.900 --> 01:12:13.780]   And if you have lots of data, then just use the B5 because it's the best one.
[01:12:13.780 --> 01:12:19.200]   And yeah, so we also have-- if you look at the appendix, we have results on VQA.
[01:12:19.200 --> 01:12:22.880]   We didn't put it in the main paper because they're not stunning results.
[01:12:22.880 --> 01:12:31.480]   We just did it to see what if we don't have object to text span alignment, but we only
[01:12:31.480 --> 01:12:38.240]   have higher level, only at the entire-- so we pre-train on phrase grounding and then
[01:12:38.240 --> 01:12:40.640]   fine-tune only on question answering, basically.
[01:12:40.640 --> 01:12:44.080]   So the supervision only comes from the answers.
[01:12:44.080 --> 01:12:46.080]   And we got a decent score.
[01:12:46.080 --> 01:12:50.620]   It's not state of the art, but it was 70-something, which is not a joke.
[01:12:50.620 --> 01:12:52.320]   So we were happy with that score.
[01:12:52.320 --> 01:12:57.760]   So yeah, that is the one downstream task that we tried where we don't have that much dense
[01:12:57.760 --> 01:13:00.440]   annotation to answer your question.
[01:13:00.440 --> 01:13:02.480]   Thank you.
[01:13:02.480 --> 01:13:07.240]   And then there's this question about, OK, so are there any specific data augmentation
[01:13:07.240 --> 01:13:11.480]   on images and text that you ended up using for fine-tuning?
[01:13:11.480 --> 01:13:15.380]   Yeah, so for data augmentation, the only thing you need to be careful about when you're using
[01:13:15.380 --> 01:13:17.560]   image text is flips.
[01:13:17.560 --> 01:13:23.120]   Because if you flip the image, you need to also flip the left, right, and things like
[01:13:23.120 --> 01:13:25.680]   that, which was something that I didn't do in the beginning.
[01:13:25.680 --> 01:13:29.520]   And it kind of bit me when I was doing my very early experiments.
[01:13:29.520 --> 01:13:31.360]   It's something to be careful about.
[01:13:31.360 --> 01:13:36.240]   But other than that, on the image side, we used mostly the same transforms as Ditter,
[01:13:36.240 --> 01:13:41.080]   so random crops and things like that, jitters, whatever was in Ditter.
[01:13:41.080 --> 01:13:44.560]   So yeah, I didn't change much in that.
[01:13:44.560 --> 01:13:45.740]   Thank you.
[01:13:45.740 --> 01:13:50.700]   And then the last question is, is the Roberta frozen, or is that also fine-tuned?
[01:13:50.700 --> 01:13:51.700]   It's fine-tuned.
[01:13:51.700 --> 01:13:53.760]   Yeah, we train on everything.
[01:13:53.760 --> 01:13:58.540]   It has a different learning rate, so that was a bit hard because it was a lot of part--
[01:13:58.540 --> 01:14:03.320]   like, already in Ditter, you have the encoder and the decoder and the backbone.
[01:14:03.320 --> 01:14:06.640]   And now we were adding one more part to the model, which also needed to-- we had to figure
[01:14:06.640 --> 01:14:08.380]   out what learning rate to use it.
[01:14:08.380 --> 01:14:14.060]   So at some point, we had-- so the text encoder has a different learning rate schedule from
[01:14:14.060 --> 01:14:15.820]   the rest of the model, actually.
[01:14:15.820 --> 01:14:22.580]   We use a linear warm-up with decay, as is common in NLP for the Roberta model.
[01:14:22.580 --> 01:14:27.260]   But for the Ditter part, we were using a different step alert, kind of constant for filler point,
[01:14:27.260 --> 01:14:29.420]   and then drop for parts of the model.
[01:14:29.420 --> 01:14:34.660]   So yeah, there was a lot of hyperparameter tuning that went into that.
[01:14:34.660 --> 01:14:39.060]   In my head, I'm getting excited for getting my hands dirty with all this code.
[01:14:39.060 --> 01:14:41.860]   So everything you're saying, I'm like, oh, yes, that's another thing I want to look at
[01:14:41.860 --> 01:14:42.860]   in code.
[01:14:42.860 --> 01:14:45.940]   And then you say something else, I'm like, oh, yes, that's another thing I want to look
[01:14:45.940 --> 01:14:46.940]   at in code.
[01:14:46.940 --> 01:14:50.500]   So I guess that's where we'll stop.
[01:14:50.500 --> 01:14:54.060]   Thanks so much, Aishwarya, for finding the time and joining us today.
[01:14:54.060 --> 01:14:58.940]   I think it's been a really good experience learning about MDitter and learning about
[01:14:58.940 --> 01:15:03.780]   everything that's-- the wonderful things that the paper can do and the wonderful experiments
[01:15:03.780 --> 01:15:06.060]   that you run and that you ran.
[01:15:06.060 --> 01:15:10.500]   And it's really good to see all those stories that are behind the scenes of the paper.
[01:15:10.500 --> 01:15:12.540]   And it's really helpful, at least.
[01:15:12.540 --> 01:15:16.060]   So I guess thanks for finding the time and joining us on your evening.
[01:15:16.060 --> 01:15:17.060]   Of course.
[01:15:17.060 --> 01:15:18.900]   And we'll let you go now.
[01:15:18.900 --> 01:15:19.900]   And thanks, Aishwarya.
[01:15:19.900 --> 01:15:20.900]   So everybody, we will--
[01:15:20.900 --> 01:15:21.900]   Have a good day.
[01:15:21.900 --> 01:15:22.900]   Thank you.
[01:15:22.900 --> 01:15:28.460]   And everybody, for Weights and Biases context, and what we're going to start from here on
[01:15:28.460 --> 01:15:31.620]   is we're going to start also digging into our beginner-friendly papers.
[01:15:31.620 --> 01:15:34.900]   So we're going to start with ResNets or DenseNets and similar papers.
[01:15:34.900 --> 01:15:37.700]   So the details will be out very soon.
[01:15:37.700 --> 01:15:41.820]   But we're also going to look into the-- there's really a strong plan to start looking into
[01:15:41.820 --> 01:15:46.340]   the code for MData and run every experiment or whatever we've learned today.
[01:15:46.340 --> 01:15:47.740]   We're going to run all those experiments.
[01:15:47.740 --> 01:15:51.020]   And we're going to see how that thing looks like in code.
[01:15:51.020 --> 01:15:54.780]   And then we're going to try and match the equation to the code implementation.
[01:15:54.780 --> 01:15:57.860]   And we're going to try and match what-- the good things that Aishwarya has explained to
[01:15:57.860 --> 01:16:02.620]   us today and then try and match that to the code implementations and see if we can fine
[01:16:02.620 --> 01:16:04.020]   tune to our own data sets.
[01:16:04.020 --> 01:16:05.020]   Those are the plans.
[01:16:05.020 --> 01:16:06.020]   Next.
[01:16:06.020 --> 01:16:08.580]   Aishwarya, before you go, actually, I just had last question.
[01:16:08.580 --> 01:16:13.860]   I'm really sorry, but you did mention you have-- you're working on other projects.
[01:16:13.860 --> 01:16:15.740]   So what's next for MData?
[01:16:15.740 --> 01:16:18.700]   So what's-- is that something you could share with us?
[01:16:18.700 --> 01:16:19.700]   Yeah, sure.
[01:16:19.700 --> 01:16:25.060]   So, yeah, we're-- like, I haven't had much time to work on this during the summer because
[01:16:25.060 --> 01:16:27.060]   I've been at Google for the summer.
[01:16:27.060 --> 01:16:30.980]   And I'm working on, like, vision language navigation, not really using MData, so like
[01:16:30.980 --> 01:16:33.820]   other kind of approach.
[01:16:33.820 --> 01:16:38.540]   But now that the semester is starting again, I'll be going back to working on some follow-ups
[01:16:38.540 --> 01:16:46.300]   and it's mostly in, like, trying to make MData be more versatile in terms of what tasks you
[01:16:46.300 --> 01:16:50.980]   can approach, like more generation-based tasks, like captioning and things like that, and
[01:16:50.980 --> 01:16:54.140]   which will also help us, like, kind of use more data.
[01:16:54.140 --> 01:17:00.300]   Because I still feel bad that, you know, Oscar and all of these approaches use so much more
[01:17:00.300 --> 01:17:01.300]   data.
[01:17:01.300 --> 01:17:05.260]   So, like, we need to figure out how to, like, make MData also scale up like that.
[01:17:05.260 --> 01:17:07.100]   And, yeah, that's the next plan.
[01:17:07.100 --> 01:17:08.100]   Excellent.
[01:17:08.100 --> 01:17:09.100]   Thanks, Aishwarya.
[01:17:09.100 --> 01:17:10.100]   Thank you.
[01:17:10.100 --> 01:17:11.100]   Have a nice evening.
[01:17:11.100 --> 01:17:12.100]   And you.
[01:17:12.100 --> 01:17:12.100]   [END OF TRANSCRIPT]
[01:17:12.100 --> 01:17:14.100]   you.
[01:17:14.100 --> 01:17:16.100]   >> Thank you.
[01:17:16.100 --> 01:17:18.100]   >> Have a nice evening.
[01:17:18.100 --> 01:17:20.100]   >> And you.
[01:17:20.100 --> 01:17:22.100]   >> Thank you.
[01:17:22.100 --> 01:17:24.100]   >> Thank you.
[01:17:24.100 --> 01:17:26.100]   >> Thank you.
[01:17:26.100 --> 01:17:36.100]   [BLANK_AUDIO]

