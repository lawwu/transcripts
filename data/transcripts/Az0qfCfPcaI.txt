
[00:00:00.000 --> 00:00:06.640]   We would love to go in, have the business partner say, "Here's my task.
[00:00:06.640 --> 00:00:09.120]   Here's my baseline performance.
[00:00:09.120 --> 00:00:10.120]   Let's see if this is viable.
[00:00:10.120 --> 00:00:15.160]   If you can increase performance on this baseline by X percent or whatever."
[00:00:15.160 --> 00:00:19.240]   I don't think I've ever seen an instance where that happens in real life.
[00:00:19.240 --> 00:00:23.620]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:23.620 --> 00:00:25.960]   and I'm your host, Lukas Biewald.
[00:00:25.960 --> 00:00:33.440]   Catherine Hume runs Borealis AI, which is the AI arm of the Royal Bank of Canada.
[00:00:33.440 --> 00:00:42.000]   She works on a slew of machine learning applications that we get into, but also she has a background
[00:00:42.000 --> 00:00:46.960]   in comparative literature and speaks Latin.
[00:00:46.960 --> 00:00:51.320]   That's surprisingly relevant to the work that she does and to our conversation and to find
[00:00:51.320 --> 00:00:53.640]   out why you're going to need to listen to this one.
[00:00:53.640 --> 00:00:57.440]   All right, so why don't we start with what you work on?
[00:00:57.440 --> 00:01:00.880]   You have a really interesting job and an interesting organization.
[00:01:00.880 --> 00:01:05.440]   Maybe you could describe it and talk about what a day in the life is like.
[00:01:05.440 --> 00:01:07.120]   Yeah, for sure.
[00:01:07.120 --> 00:01:13.080]   I lead up a group called Borealis AI, which is the machine learning research lab for the
[00:01:13.080 --> 00:01:14.080]   Royal Bank of Canada.
[00:01:14.080 --> 00:01:17.680]   For those listening in the States or outside of Canada, you might not know of it, but it's
[00:01:17.680 --> 00:01:19.240]   actually the largest bank in Canada.
[00:01:19.240 --> 00:01:21.320]   I think it's the ninth largest bank in the world.
[00:01:21.320 --> 00:01:25.880]   It's a pretty big shop, there's like 90,000 employees in the company.
[00:01:25.880 --> 00:01:32.760]   Borealis was founded in 2016 as just the ML research center for the bank.
[00:01:32.760 --> 00:01:38.520]   Day to day in my team, I think like many other ML shops, we learned over the years that it
[00:01:38.520 --> 00:01:42.840]   takes more than just scientists to really make ML production, ML systems work.
[00:01:42.840 --> 00:01:45.400]   We've got a group of machine learning scientists.
[00:01:45.400 --> 00:01:50.560]   We have ML engineers who do a lot of the work in taking the code from the scientists and
[00:01:50.560 --> 00:01:52.880]   really building out production ML systems.
[00:01:52.880 --> 00:01:57.520]   We have product managers who do what product managers do, figure out what we should build
[00:01:57.520 --> 00:02:00.080]   and really collaborate to make sure we ship things on time.
[00:02:00.080 --> 00:02:05.080]   Then we have a group of business development experts who work with our business partners.
[00:02:05.080 --> 00:02:08.200]   In the bank, there's mini markets, if you will.
[00:02:08.200 --> 00:02:12.320]   There's the retail bank, which works with people like you and I, so checking accounts,
[00:02:12.320 --> 00:02:14.000]   savings accounts.
[00:02:14.000 --> 00:02:17.240]   There's wealth managers who help manage people's assets.
[00:02:17.240 --> 00:02:20.920]   Then there's capital markets, which is the institutional investing.
[00:02:20.920 --> 00:02:24.880]   They partner with those various teams and help us find ML use cases.
[00:02:24.880 --> 00:02:25.880]   Yeah.
[00:02:25.880 --> 00:02:29.560]   Can you describe what those use cases are?
[00:02:29.560 --> 00:02:32.040]   Yeah, for sure.
[00:02:32.040 --> 00:02:36.880]   I'll talk about some of the products that we've worked on and various use cases that
[00:02:36.880 --> 00:02:37.880]   we see.
[00:02:37.880 --> 00:02:42.040]   It's a broad variety, I'd say, of applications.
[00:02:42.040 --> 00:02:46.600]   If we go into the retail bank, so things that are helpful for people like you and I, one
[00:02:46.600 --> 00:02:54.000]   of our recent applications was a cash flow forecasting application for day-to-day customers.
[00:02:54.000 --> 00:02:57.960]   Probably there's lots of customers out there who have missed a bill payment or potentially
[00:02:57.960 --> 00:03:02.840]   overdrawn their account and gotten fees from the bank for having insufficient funds in
[00:03:02.840 --> 00:03:03.840]   their account.
[00:03:03.840 --> 00:03:08.520]   We built something that's trying to predict upcoming payments in the next seven days.
[00:03:08.520 --> 00:03:12.040]   We stuck at a target of about a week out to give people reminders that say, "Hey, this
[00:03:12.040 --> 00:03:13.800]   thing is coming.
[00:03:13.800 --> 00:03:19.400]   You might want to either pay it now or take different kinds of actions in managing moving
[00:03:19.400 --> 00:03:23.280]   money from savings to checking, et cetera, to be able to cover those expenses."
[00:03:23.280 --> 00:03:25.240]   Wow, is that really live?
[00:03:25.240 --> 00:03:26.240]   That really works?
[00:03:26.240 --> 00:03:27.240]   Yeah, it's live.
[00:03:27.240 --> 00:03:28.880]   I've never gotten a message like that from my bank.
[00:03:28.880 --> 00:03:31.140]   Maybe I should switch banks.
[00:03:31.140 --> 00:03:35.680]   It's live as of maybe a month ago we went into production, so it's really new.
[00:03:35.680 --> 00:03:37.120]   Wow, how cool.
[00:03:37.120 --> 00:03:40.200]   But yeah, that's one of the latest ones we put out.
[00:03:40.200 --> 00:03:46.240]   It's an interesting ML problem because if we think about ML models, they're going to
[00:03:46.240 --> 00:03:49.920]   take a series of time series data, things that you've paid in the past, and then try
[00:03:49.920 --> 00:03:53.040]   to generalize a prediction, so what's that going to look like in the future?
[00:03:53.040 --> 00:03:56.840]   But if you have something like, say it's your electricity bill or your phone bill, my phone
[00:03:56.840 --> 00:04:01.320]   provider decides every once in a while that they're going to increase my rate arbitrarily,
[00:04:01.320 --> 00:04:03.680]   and I don't know why this is coming, but it just happens.
[00:04:03.680 --> 00:04:11.680]   It goes from $75 a month to $85 a month, and I have to just pay that extra fee.
[00:04:11.680 --> 00:04:16.940]   One of the cool things in our model was we needed to use an intention mechanism so as
[00:04:16.940 --> 00:04:20.520]   to basically not overgeneralize that trend.
[00:04:20.520 --> 00:04:25.760]   It's not going to see that it's 75, 75, 75, 85, 85, and then imagine that three months
[00:04:25.760 --> 00:04:26.760]   later it's going to go to 95.
[00:04:26.760 --> 00:04:33.240]   We had to sort of correct for those kind of stepwise changes that a provider might make
[00:04:33.240 --> 00:04:36.360]   that an algorithm might incorrectly generalize as a trend.
[00:04:36.360 --> 00:04:41.600]   That was one of the many micro nuances to actually making this thing work.
[00:04:41.600 --> 00:04:44.520]   That's cool how your energy bill might go up in the winter, or I guess in the summer
[00:04:44.520 --> 00:04:45.520]   if you're in a hot place.
[00:04:45.520 --> 00:04:48.000]   Yeah, there's a lot with seasonality for sure.
[00:04:48.000 --> 00:04:49.000]   Interesting.
[00:04:49.000 --> 00:04:51.840]   You can't just take the average trend over a year or else you're going to end up with
[00:04:51.840 --> 00:04:58.120]   somewhere in the midpoint in September when it's actually, it tends to be more stepwise.
[00:04:58.120 --> 00:05:00.560]   Can you talk about how you came to make that?
[00:05:00.560 --> 00:05:05.960]   I always assume that banks kind of liked charging really high overdraft fees and would kind
[00:05:05.960 --> 00:05:09.080]   of want you to maybe do that, but I guess that's wrong.
[00:05:09.080 --> 00:05:12.760]   How did you kind of come up with that?
[00:05:12.760 --> 00:05:16.800]   Working with the business to know the business wants that and then also realizing that's
[00:05:16.800 --> 00:05:20.920]   a feasible ML problem that you could actually solve.
[00:05:20.920 --> 00:05:23.840]   I was actually quite proud of this.
[00:05:23.840 --> 00:05:27.320]   At the Royal Bank of Canada, there's a little bit of like a, it might have to do with the
[00:05:27.320 --> 00:05:32.680]   Canadian banking mindset, but there's part of it is be profitable institution, but then
[00:05:32.680 --> 00:05:38.440]   equal in part is sort of be like a good citizen, like be good to the Canadian citizens.
[00:05:38.440 --> 00:05:43.520]   And so slightly different than in some of the US environment in that the Canadian population
[00:05:43.520 --> 00:05:45.840]   is very, it's not underbanked.
[00:05:45.840 --> 00:05:49.880]   So underbanked is a term that we use for people who are not within the sort of recognized
[00:05:49.880 --> 00:05:50.880]   banking system.
[00:05:50.880 --> 00:05:55.280]   So like a Chase customer, a Bank of America customer, a Morgan Stanley customer.
[00:05:55.280 --> 00:05:58.480]   I think in the US, I don't know exactly what the statistics are, but it's something like
[00:05:58.480 --> 00:06:04.400]   30 ish percent of the population is actually not using a bank, like a registered bank.
[00:06:04.400 --> 00:06:08.360]   So they use things like payday loans and sort of like the sort of on the side type banking
[00:06:08.360 --> 00:06:09.360]   products.
[00:06:09.360 --> 00:06:13.960]   But in Canada, I think it's like 98% of the population actually is in the main banking
[00:06:13.960 --> 00:06:18.480]   community, which then has implications for sort of social responsibility from these pretty
[00:06:18.480 --> 00:06:22.360]   large institutions because you've got the whole population represented.
[00:06:22.360 --> 00:06:31.960]   So I think I found it quite promising that like the executives making the decisions were
[00:06:31.960 --> 00:06:35.480]   felt that it would be better for customer loyalty to provide a service that helped versus
[00:06:35.480 --> 00:06:40.360]   like do this sort of nickel and diming on these kind of fees.
[00:06:40.360 --> 00:06:43.440]   Finding the use case, it's always an iteration.
[00:06:43.440 --> 00:06:50.040]   So I think in the ideal, like coming from the academic ML world, we would love to go
[00:06:50.040 --> 00:06:53.560]   in, have the business partner say, "Here's my task.
[00:06:53.560 --> 00:06:56.040]   Here's my baseline performance.
[00:06:56.040 --> 00:06:57.040]   Let's see if this is viable.
[00:06:57.040 --> 00:07:02.080]   If you can increase performance upon this baseline by X percent or whatever."
[00:07:02.080 --> 00:07:06.520]   I don't think I've ever seen an instance where that happens in real life.
[00:07:06.520 --> 00:07:10.080]   And so it's more like, "Hey, we have this idea.
[00:07:10.080 --> 00:07:11.440]   What do you guys think?"
[00:07:11.440 --> 00:07:15.200]   And then we're like, "All right, let's play around with some of the data and see what
[00:07:15.200 --> 00:07:18.540]   we can find and see if like there is a there there."
[00:07:18.540 --> 00:07:21.400]   And so we'll come and we'll say, "All right, we think this is the task."
[00:07:21.400 --> 00:07:24.160]   And then it's like, "What timeframe do you need?"
[00:07:24.160 --> 00:07:28.480]   And so when we originally started this, I thought cashflow was like six months out.
[00:07:28.480 --> 00:07:30.880]   And then our partners were like, "No, no, no, one week."
[00:07:30.880 --> 00:07:33.880]   And I was like, "Okay, well that's, that helps."
[00:07:33.880 --> 00:07:39.680]   So there was sort of iterations on just narrowing down, yeah, the scope of the prediction, what
[00:07:39.680 --> 00:07:41.640]   qualified as decent performance.
[00:07:41.640 --> 00:07:47.240]   So I often find with the business, like the preferences, it's accurate every time.
[00:07:47.240 --> 00:07:49.240]   That would be the preference.
[00:07:49.240 --> 00:07:50.240]   It's absolutely, yeah.
[00:07:50.240 --> 00:07:53.680]   The preference is always like, actually there's no machine learning involved and this is just
[00:07:53.680 --> 00:07:57.080]   a rules-based system that works like clockwork.
[00:07:57.080 --> 00:08:01.440]   So then it's sort of iterations where it's, all right, well, what if this edge case, the
[00:08:01.440 --> 00:08:02.440]   prediction is off?
[00:08:02.440 --> 00:08:05.480]   Like what might that, how might that impact customer experience?
[00:08:05.480 --> 00:08:09.880]   And it's sort of like iterative negotiation to get to the point of, yeah, we're comfortable
[00:08:09.880 --> 00:08:13.240]   with this as like a starting point.
[00:08:13.240 --> 00:08:18.920]   And then there's sort of the selling and pitching and telling the story, getting the various
[00:08:18.920 --> 00:08:21.520]   people involved to get it to market.
[00:08:21.520 --> 00:08:27.520]   And I'd say with our group, since we're a machine learning team, but we partner with
[00:08:27.520 --> 00:08:34.640]   other groups in the bank that do design, that do just a lot of the sort of ticks, like ticking
[00:08:34.640 --> 00:08:36.320]   the boxes around all the business processes.
[00:08:36.320 --> 00:08:39.720]   There's a lot of back and forth and stakeholder management to really get something live as
[00:08:39.720 --> 00:08:41.080]   well.
[00:08:41.080 --> 00:08:44.240]   And what ended up, I'm just curious, what ended up being like the level of accuracy
[00:08:44.240 --> 00:08:49.400]   that you could get with that sort of seven day prediction if I'm going to overcharge
[00:08:49.400 --> 00:08:50.400]   my account?
[00:08:50.400 --> 00:08:52.720]   It varied per payment type.
[00:08:52.720 --> 00:08:58.160]   So if you had a pre-approved payment, so like your Spotify subscription or whatever, it
[00:08:58.160 --> 00:09:02.640]   was quite high accuracy and also the day, like the day that the payment would come out
[00:09:02.640 --> 00:09:03.640]   was quite high.
[00:09:03.640 --> 00:09:08.080]   So we had a multi-objective supervised learning algorithm that predicted the one task was
[00:09:08.080 --> 00:09:11.560]   how much and the second task was when.
[00:09:11.560 --> 00:09:13.440]   And so with those, it could get pretty high.
[00:09:13.440 --> 00:09:19.280]   I think within three days range, we were at, I don't remember, I think it goes down to
[00:09:19.280 --> 00:09:21.080]   like 88 or 89%.
[00:09:21.080 --> 00:09:23.840]   If it was, no, sorry, within three days, it was up to 98.
[00:09:23.840 --> 00:09:28.320]   And if it were the exact day, it was more like 88, 89.
[00:09:28.320 --> 00:09:29.920]   So with those pre-approved, it was quite high.
[00:09:29.920 --> 00:09:36.920]   Once we got into things like loan payments, anything that's sort of an arbitrary e-transfer
[00:09:36.920 --> 00:09:44.760]   like a Venmo payment, those are harder because there's just not a lot of predictive, there's
[00:09:44.760 --> 00:09:45.840]   more variability.
[00:09:45.840 --> 00:09:48.920]   There's not that kind of like just standardization.
[00:09:48.920 --> 00:09:53.200]   So yeah, so very per payment type, but sometimes it was as high as like 98 and then it would
[00:09:53.200 --> 00:09:57.960]   skew down to about like in the high 80s.
[00:09:57.960 --> 00:10:00.920]   And you were using an attention, a model that included attention.
[00:10:00.920 --> 00:10:04.680]   It sounds like a lot of machinery for this kind of problem.
[00:10:04.680 --> 00:10:05.680]   Did that really matter?
[00:10:05.680 --> 00:10:07.440]   Was it too much better than like a simpler baseline?
[00:10:07.440 --> 00:10:09.120]   Yeah, it's a great question.
[00:10:09.120 --> 00:10:12.600]   I pose the same question to the team being like, wow, this is a lot of machinery for
[00:10:12.600 --> 00:10:14.320]   this kind of problem.
[00:10:14.320 --> 00:10:18.360]   I think it came down to, again, the seasonal variability.
[00:10:18.360 --> 00:10:22.160]   It's like there's, you'd imagine that it's something where it's like, it seems like it
[00:10:22.160 --> 00:10:24.840]   could just be a pretty standard approach.
[00:10:24.840 --> 00:10:31.680]   But yeah, just with these things that creep up like seasons, like variation in payment
[00:10:31.680 --> 00:10:34.000]   time per individual.
[00:10:34.000 --> 00:10:37.880]   So some people are like, they've set up automated stuff and the minute it goes, it comes.
[00:10:37.880 --> 00:10:42.320]   Other people, they haven't, so they do it manually and there'll be these like lags and
[00:10:42.320 --> 00:10:43.480]   when they pay.
[00:10:43.480 --> 00:10:47.640]   And so that has implications not only on this thing is due, but like how that impacts their
[00:10:47.640 --> 00:10:49.040]   balance.
[00:10:49.040 --> 00:10:55.080]   So once you get into the details, it becomes more messy and needs more machinery.
[00:10:55.080 --> 00:10:58.880]   Can you describe some of the other applications that seems like such a surprising and cool
[00:10:58.880 --> 00:11:03.120]   one, but like what are the kind of the main like bread and butter, like bank applications
[00:11:03.120 --> 00:11:04.120]   of ML?
[00:11:04.120 --> 00:11:07.040]   I want to talk about another cool one.
[00:11:07.040 --> 00:11:08.040]   Oh yeah.
[00:11:08.040 --> 00:11:09.920]   I want to do another cool one.
[00:11:09.920 --> 00:11:11.280]   I'll do a cool one first.
[00:11:11.280 --> 00:11:17.920]   So another, this one is really artful because you have to scope it down really small, but
[00:11:17.920 --> 00:11:18.920]   it's cool.
[00:11:18.920 --> 00:11:21.900]   So it's using reinforcement learning for trade execution.
[00:11:21.900 --> 00:11:24.400]   So here's the problem.
[00:11:24.400 --> 00:11:28.560]   Imagine you're a big hedge fund and you trade every day, right?
[00:11:28.560 --> 00:11:32.680]   You just come into the equities markets and you're like, you order millions of orders
[00:11:32.680 --> 00:11:35.160]   of some sort of stock and you trade it through the day.
[00:11:35.160 --> 00:11:42.440]   So the question at hand is you come in, you decide that you want to execute a million
[00:11:42.440 --> 00:11:46.040]   orders of Google shares over the course of the day.
[00:11:46.040 --> 00:11:48.440]   Stock market opens at nine, closes at 430.
[00:11:48.440 --> 00:11:53.840]   The problem, the question is how do you distribute that order optimally throughout the day so
[00:11:53.840 --> 00:11:57.680]   as to achieve your desired returns targets?
[00:11:57.680 --> 00:12:01.800]   So there's a common historical algorithmic approach to solving this problem, which is
[00:12:01.800 --> 00:12:03.240]   called a VWAP algorithm.
[00:12:03.240 --> 00:12:06.280]   VWAP stands for volume weighted average price.
[00:12:06.280 --> 00:12:10.320]   So it's the average price of the stock throughout the day weighted by the volume that's traded,
[00:12:10.320 --> 00:12:11.320]   right?
[00:12:11.320 --> 00:12:12.320]   As the name suggests.
[00:12:12.320 --> 00:12:16.560]   I don't know when these kind of algorithms came into being, but I think they're like
[00:12:16.560 --> 00:12:18.080]   date back to the nineties or something.
[00:12:18.080 --> 00:12:19.080]   Sorry to interrupt.
[00:12:19.080 --> 00:12:23.080]   That seems like it would be a number.
[00:12:23.080 --> 00:12:25.000]   How is that an algorithm?
[00:12:25.000 --> 00:12:26.000]   So it's a curve.
[00:12:26.000 --> 00:12:27.920]   It's a number, but it's a number.
[00:12:27.920 --> 00:12:32.560]   If you trace it throughout the day, it'll change slightly.
[00:12:32.560 --> 00:12:36.920]   So the algorithm we took was what trades do you place to hit that number?
[00:12:36.920 --> 00:12:39.280]   Oh, I see.
[00:12:39.280 --> 00:12:44.760]   And you can, I won't go into the complexities of how the limit order system works, but effectively
[00:12:44.760 --> 00:12:48.460]   think do I sell, hold, or buy the stock?
[00:12:48.460 --> 00:12:52.720]   It's not exactly that, but I think for the sake of, we don't have to go into the arcane
[00:12:52.720 --> 00:12:54.280]   detail.
[00:12:54.280 --> 00:12:56.480]   You could buy or sell or hold, right?
[00:12:56.480 --> 00:12:58.920]   So this is where reinforcement learning comes in.
[00:12:58.920 --> 00:13:04.360]   You have a sequence of a couple of kinds of actions.
[00:13:04.360 --> 00:13:10.400]   You might sell, you might buy, but yeah, you're trading stocks during the day.
[00:13:10.400 --> 00:13:12.080]   And so what would the simple algorithm be?
[00:13:12.080 --> 00:13:15.240]   Because you don't know what the stock price is going to be, right?
[00:13:15.240 --> 00:13:18.700]   You don't know what the stock price is going to be, but the simple algorithm from what
[00:13:18.700 --> 00:13:26.360]   I understand is you've got historical price curves, how the stock performed yesterday,
[00:13:26.360 --> 00:13:29.320]   two weeks ago, a month ago, et cetera.
[00:13:29.320 --> 00:13:36.520]   So you use that to make a guess on how much you should buy at a certain time of the day
[00:13:36.520 --> 00:13:41.880]   in order to achieve your target goals, the target money making goals that you have for
[00:13:41.880 --> 00:13:42.880]   the day.
[00:13:42.880 --> 00:13:43.880]   Okay.
[00:13:43.880 --> 00:13:48.240]   So the algorithm basically releases, buys, or holds.
[00:13:48.240 --> 00:13:49.240]   Buy or doesn't release.
[00:13:49.240 --> 00:13:53.040]   And it partitions that at timestamps.
[00:13:53.040 --> 00:13:56.660]   At 9am, I'm going to do X at 9.15.
[00:13:56.660 --> 00:14:01.160]   What you don't want to do is say if it's a large order, like a million orders of stock,
[00:14:01.160 --> 00:14:07.940]   if at 9am you say, "Buy a million," that has a big impact on the market price and it'll
[00:14:07.940 --> 00:14:10.480]   sort of shake things off, right?
[00:14:10.480 --> 00:14:16.240]   So you try to dose it just a little bit at a time.
[00:14:16.240 --> 00:14:19.120]   So without disrupting the ship, right?
[00:14:19.120 --> 00:14:21.200]   With keeping the market relatively stable.
[00:14:21.200 --> 00:14:24.800]   Because you're one of the participants, but there's going to be however many others on
[00:14:24.800 --> 00:14:27.840]   the exchange at that time.
[00:14:27.840 --> 00:14:32.800]   So what we did here is we said, "Can we..."
[00:14:32.800 --> 00:14:35.160]   We're trying to hug that number, right?
[00:14:35.160 --> 00:14:40.920]   So the price number across these timestamps.
[00:14:40.920 --> 00:14:46.160]   What we said is, "Can we use reinforcement learning to optimally distribute, dose our
[00:14:46.160 --> 00:14:49.960]   buy decisions to stay as close as possible to that curve?"
[00:14:49.960 --> 00:14:53.820]   So the value function was basically just minimize the distance.
[00:14:53.820 --> 00:14:55.200]   We can observe that number.
[00:14:55.200 --> 00:14:58.200]   Sorry, the curve is the volume?
[00:14:58.200 --> 00:15:01.600]   The curve is the average price weighted per volume.
[00:15:01.600 --> 00:15:05.040]   It's the price weighted by the volume.
[00:15:05.040 --> 00:15:06.040]   Okay.
[00:15:06.040 --> 00:15:07.040]   Yeah.
[00:15:07.040 --> 00:15:09.480]   And you want the curve to do what?
[00:15:09.480 --> 00:15:14.060]   What you want to do, there's a trading strategy where you want to hit that curve.
[00:15:14.060 --> 00:15:19.760]   So you want to sell or buy in such a way that the price that you arrive at matches that
[00:15:19.760 --> 00:15:20.760]   curve.
[00:15:20.760 --> 00:15:21.760]   Exactly.
[00:15:21.760 --> 00:15:22.760]   And does it work?
[00:15:22.760 --> 00:15:23.760]   Yeah.
[00:15:23.760 --> 00:15:27.840]   And the cool thing is it works quite well even when there's a lot of volatility in the
[00:15:27.840 --> 00:15:28.960]   market.
[00:15:28.960 --> 00:15:38.640]   So in March of 2020, when COVID hit, it was just much more volatile than the stock market
[00:15:38.640 --> 00:15:39.640]   normally was.
[00:15:39.640 --> 00:15:42.000]   And what was nice is it adapted.
[00:15:42.000 --> 00:15:49.320]   I don't know the exact time it took to adapt and nonetheless get superior trading returns.
[00:15:49.320 --> 00:15:50.320]   It may have taken a day.
[00:15:50.320 --> 00:15:51.320]   It may have taken a couple of weeks.
[00:15:51.320 --> 00:15:58.040]   I'm not sure on that timeframe, but I know that it adapted much better than a standard
[00:15:58.040 --> 00:15:59.560]   trading algorithm would.
[00:15:59.560 --> 00:16:03.080]   And I guess you're constantly retraining the algorithm then.
[00:16:03.080 --> 00:16:06.600]   Constantly retraining the algorithm and a different team, not our team now, but the
[00:16:06.600 --> 00:16:14.760]   team that now owns that algorithm are working on adapting the task to different kinds of
[00:16:14.760 --> 00:16:15.760]   trading styles.
[00:16:15.760 --> 00:16:21.960]   So not this hub that VWAP curve that I described, but there's other approaches and strategies
[00:16:21.960 --> 00:16:24.360]   that one could take when trading.
[00:16:24.360 --> 00:16:27.680]   And they're retuning it to see if it could work there.
[00:16:27.680 --> 00:16:33.480]   I think there's a lesson though in reinforcement learning and that you can't just scale it
[00:16:33.480 --> 00:16:34.480]   to a new use case.
[00:16:34.480 --> 00:16:39.160]   It requires significant effort to write a new algorithm that will work with a different
[00:16:39.160 --> 00:16:40.160]   task.
[00:16:40.160 --> 00:16:41.160]   Right.
[00:16:41.160 --> 00:16:42.160]   Interesting.
[00:16:42.160 --> 00:16:45.720]   So what are the other kind of important applications to you?
[00:16:45.720 --> 00:16:46.720]   Yeah.
[00:16:46.720 --> 00:16:49.040]   So other more bread and butter applications.
[00:16:49.040 --> 00:16:52.360]   Other cool ones, I guess, if you've got other ones you want to talk about.
[00:16:52.360 --> 00:16:55.360]   There's another cool one we can talk about down the line that's a little less related
[00:16:55.360 --> 00:16:56.360]   to banking.
[00:16:56.360 --> 00:16:58.480]   I like to think about it this way.
[00:16:58.480 --> 00:16:59.480]   So what does a bank do?
[00:16:59.480 --> 00:17:03.160]   A bank takes in money at one rate.
[00:17:03.160 --> 00:17:07.520]   So you put money in your checking and savings account and it loans out money at a different
[00:17:07.520 --> 00:17:10.200]   interest rate and it makes money on the spread.
[00:17:10.200 --> 00:17:11.200]   Right.
[00:17:11.200 --> 00:17:16.200]   That's kind of basically what a bank does.
[00:17:16.200 --> 00:17:22.520]   Historically when banks have used models, statistical models to decide how much they
[00:17:22.520 --> 00:17:29.520]   should lend to a given customer, there's plenty of background models that are using linear
[00:17:29.520 --> 00:17:31.720]   regressions, et cetera, to do this.
[00:17:31.720 --> 00:17:36.360]   But there's a lot of opportunities to sort of upgrade some of those decisions using ML
[00:17:36.360 --> 00:17:42.600]   with more data, different types of data, but sort of the basics of who should we give a
[00:17:42.600 --> 00:17:43.840]   loan to?
[00:17:43.840 --> 00:17:45.480]   How much should that loan be?
[00:17:45.480 --> 00:17:50.760]   What is the risk that the person will, that we incur that the person might default on
[00:17:50.760 --> 00:17:51.840]   this loan?
[00:17:51.840 --> 00:17:55.920]   If they do default, when should we call them?
[00:17:55.920 --> 00:18:03.240]   You know, ranking that order queue, there's process optimization.
[00:18:03.240 --> 00:18:06.140]   How do we, like we have our call center.
[00:18:06.140 --> 00:18:09.760]   Very often we go into our, especially today we have digital banking.
[00:18:09.760 --> 00:18:15.640]   You go on, your password doesn't work, something happens, you're stuck, you have to call somebody.
[00:18:15.640 --> 00:18:22.000]   There's a lot of applications in call center automation, the conversational AI work, automating
[00:18:22.000 --> 00:18:26.840]   some of that queue, rank ordering queues, whose call should we take first to approach
[00:18:26.840 --> 00:18:27.840]   this?
[00:18:27.840 --> 00:18:32.560]   There's which product, you know, banks often have a series of products, which product offering
[00:18:32.560 --> 00:18:35.600]   do we send to which customer next?
[00:18:35.600 --> 00:18:37.600]   Those are kind of more standard industry problems.
[00:18:37.600 --> 00:18:38.600]   I think that exists everywhere.
[00:18:38.600 --> 00:18:43.720]   It's not unique to banking, sort of the next best product offering optimization.
[00:18:43.720 --> 00:18:45.400]   And do you work on all those problems?
[00:18:45.400 --> 00:18:47.920]   We don't work on all those problems, no.
[00:18:47.920 --> 00:18:51.760]   We work on a subset, we do a lot of work in credit, but there's other teams in the bank
[00:18:51.760 --> 00:18:55.480]   who work on various other data science problems like this.
[00:18:55.480 --> 00:18:56.480]   Well, tell me about credit.
[00:18:56.480 --> 00:19:01.840]   I mean, that's something I don't know a lot about, but I do know that we've had various
[00:19:01.840 --> 00:19:09.040]   sort of like mortgage crises that I think were like, at least in the public, the publicly
[00:19:09.040 --> 00:19:13.400]   available information seemed to indicate that it was like too much machinery kind of leading
[00:19:13.400 --> 00:19:15.520]   to bad decisions.
[00:19:15.520 --> 00:19:17.200]   Do you think that's accurate?
[00:19:17.200 --> 00:19:20.800]   And then like, how do you think about machine learning in that context?
[00:19:20.800 --> 00:19:26.720]   Well, this is, to be honest, I'm not, I'll speak on this from the perspective of somebody
[00:19:26.720 --> 00:19:32.320]   who was not a banking expert in the 2007, 2008, you know, crisis, credit crisis.
[00:19:32.320 --> 00:19:38.280]   Yeah, but I mean, so this whole collateralized debt obligations, right?
[00:19:38.280 --> 00:19:44.800]   It's like, I think at the, there's the tip of the spear, which is two people deciding
[00:19:44.800 --> 00:19:49.720]   to like, let's say I decided to lend you 10 bucks.
[00:19:49.720 --> 00:19:52.200]   And I think about whether or not you're going to give that back to me.
[00:19:52.200 --> 00:19:54.800]   And I say, if he doesn't pay it, it's also okay.
[00:19:54.800 --> 00:19:55.800]   Right.
[00:19:55.800 --> 00:20:03.880]   All the way to, you know, I've got a set of mortgages and I'm a different institution
[00:20:03.880 --> 00:20:08.440]   who's going to pedge a strategy on some other institutions, mortgages, and I don't have
[00:20:08.440 --> 00:20:09.840]   insight into the quality of them.
[00:20:09.840 --> 00:20:16.640]   That's when you get into like the, you know, these sort of layered risk management strategies.
[00:20:16.640 --> 00:20:23.680]   So when you go in terms of MLs engagement here, I think the big thing is the regulators
[00:20:23.680 --> 00:20:25.240]   are have caught up.
[00:20:25.240 --> 00:20:29.600]   You know, there's always the, some action happened in activities.
[00:20:29.600 --> 00:20:35.160]   And then there's been a lot of regulatory oversight since 2007, 2008, to try to protect
[00:20:35.160 --> 00:20:38.360]   the economy by putting some limits on banks.
[00:20:38.360 --> 00:20:45.720]   Like there's a thing in Canada, we call it the CET1 ratio, which is a ratio that's used
[00:20:45.720 --> 00:20:51.520]   to manage the liquidity that a bank has to sort of overall cashflow over risk weighted
[00:20:51.520 --> 00:20:52.520]   assets.
[00:20:52.520 --> 00:20:55.720]   So these are something like a mortgage, right?
[00:20:55.720 --> 00:20:59.000]   The asset that a bank might hold that has some risk associated with it.
[00:20:59.000 --> 00:21:04.560]   And a bank has to manage that ratio in such a way that if something bad happens, there's
[00:21:04.560 --> 00:21:08.560]   still like relative stability.
[00:21:08.560 --> 00:21:11.760]   If you think about adding an ML into this mix, right?
[00:21:11.760 --> 00:21:17.280]   So let's say we were to use ML to calculate that, the risk factor in the denominator of
[00:21:17.280 --> 00:21:19.680]   that one equation.
[00:21:19.680 --> 00:21:22.680]   They want a lot of transparency and explainability, right?
[00:21:22.680 --> 00:21:26.760]   So there's a lot of governance oversight that's like, we're not just going to put in a black
[00:21:26.760 --> 00:21:29.000]   box neural network and see what happens.
[00:21:29.000 --> 00:21:36.120]   So there's a high need to select models for those kinds of use cases that are quite transparent
[00:21:36.120 --> 00:21:42.680]   and audible and where you can clearly understand how input feature is leading to output.
[00:21:42.680 --> 00:21:45.760]   So what kinds of models do you end up using?
[00:21:45.760 --> 00:21:48.720]   Various per use case and context.
[00:21:48.720 --> 00:21:55.840]   So ranging from the cashflow one that I talked about is a deep LSTM.
[00:21:55.840 --> 00:22:02.680]   There's an LSTM backbone also in the reinforcement learning one for trade execution.
[00:22:02.680 --> 00:22:07.440]   To sometimes, I'm not sure if my team has done much, but there's a lot of decision trees.
[00:22:07.440 --> 00:22:08.440]   You know what I mean?
[00:22:08.440 --> 00:22:12.120]   There's a lot of extra boost models for some of the credit work.
[00:22:12.120 --> 00:22:18.640]   We have a governance tool that we've built that is optimized for decision trees because
[00:22:18.640 --> 00:22:21.840]   there's a lot of models in the bank that use those.
[00:22:21.840 --> 00:22:25.280]   And this is like a single tree or like a boosted set of trees?
[00:22:25.280 --> 00:22:28.880]   Various per use case again.
[00:22:28.880 --> 00:22:30.480]   And I guess, how do you...
[00:22:30.480 --> 00:22:37.400]   It seems like probably a lot of applications, but mortgages specifically has a long history
[00:22:37.400 --> 00:22:41.240]   of at least racial inequality.
[00:22:41.240 --> 00:22:43.120]   How do you think about that?
[00:22:43.120 --> 00:22:49.000]   Are you able to look at the models and get some sense if they're being fair?
[00:22:49.000 --> 00:22:52.560]   And how do you even define what fairness would mean?
[00:22:52.560 --> 00:22:53.560]   Yeah, great question.
[00:22:53.840 --> 00:22:59.400]   We haven't done any work on mortgage predictions in particular, but we have done some work
[00:22:59.400 --> 00:23:02.880]   with credit and we do fairness.
[00:23:02.880 --> 00:23:07.760]   There's a lot of fairness tests prior to putting a model into production.
[00:23:07.760 --> 00:23:11.360]   At the bank, there's a group called Enterprise Model Risk Management.
[00:23:11.360 --> 00:23:14.840]   And it's interesting.
[00:23:14.840 --> 00:23:20.800]   I don't actually know if there's a preference for individual or group level fairness testing.
[00:23:20.800 --> 00:23:28.080]   I do know that there's a tool we've built that focuses on individual fairness.
[00:23:28.080 --> 00:23:30.200]   Sorry, what would that mean?
[00:23:30.200 --> 00:23:31.640]   Individual fairness versus group fairness?
[00:23:31.640 --> 00:23:32.640]   Yeah.
[00:23:32.640 --> 00:23:40.960]   So group fairness is if you've got two groups where a group is defined by some similarity
[00:23:40.960 --> 00:23:44.320]   on a feature.
[00:23:44.320 --> 00:23:45.840]   Let's take the example of race.
[00:23:45.840 --> 00:23:51.480]   So you've got the black group and the white group.
[00:23:51.480 --> 00:23:59.600]   The group level fairness is the error rate on the black group proportionate to the error
[00:23:59.600 --> 00:24:04.920]   rate on the white group for some prediction task.
[00:24:04.920 --> 00:24:09.200]   If you go into individual level fairness, it's like if you have a set of features that
[00:24:09.200 --> 00:24:16.960]   are similar to my set of features, then if I get a $5,000 loan, you too get a $5,000
[00:24:16.960 --> 00:24:17.960]   loan.
[00:24:17.960 --> 00:24:21.760]   So we have tools, but I still believe there's a decent amount of subjective interpretation
[00:24:21.760 --> 00:24:27.800]   that goes into what aspects are we trying to calibrate as fair?
[00:24:27.800 --> 00:24:28.800]   Yeah.
[00:24:28.800 --> 00:24:34.880]   I mean, sometimes it seems to me with machine learning, it kind of forces us to be more
[00:24:34.880 --> 00:24:41.320]   clear about what we mean by fairness and that can...
[00:24:41.320 --> 00:24:49.200]   The way it's easier to quantify, I guess the unfairness sort of leads to a lot of debate.
[00:24:49.200 --> 00:24:58.320]   How do you account for features that are correlated with group fairness?
[00:24:58.320 --> 00:24:59.320]   It seems always challenging.
[00:24:59.320 --> 00:25:04.000]   What does it mean to really prove that your model is being completely fair?
[00:25:04.000 --> 00:25:07.200]   It seems like a hard thing to rigorously define.
[00:25:07.200 --> 00:25:08.200]   Although I'm sure a lot of...
[00:25:08.200 --> 00:25:12.320]   I mean, we should get people to thought about it deeply on this podcast for sure.
[00:25:12.320 --> 00:25:14.160]   On the podcast, yeah.
[00:25:14.160 --> 00:25:15.160]   Yeah.
[00:25:15.160 --> 00:25:20.760]   The last I checked, there were 21 current interpretations, like technical interpretations
[00:25:20.760 --> 00:25:21.760]   of what fair means.
[00:25:21.760 --> 00:25:22.760]   Wow.
[00:25:22.760 --> 00:25:23.760]   Is there a list somewhere?
[00:25:23.760 --> 00:25:24.760]   Do we have a link that you could get?
[00:25:24.760 --> 00:25:28.680]   Yeah, I can definitely find it.
[00:25:28.680 --> 00:25:30.000]   There's a paper from...
[00:25:30.000 --> 00:25:32.840]   This is from 2019, so maybe it's been...
[00:25:32.840 --> 00:25:33.840]   Or something like that.
[00:25:33.840 --> 00:25:36.080]   I definitely sent the link after this call.
[00:25:36.080 --> 00:25:41.640]   Yeah, I've seen things like at the bank, there was one where these proxy correlations, you
[00:25:41.640 --> 00:25:44.280]   might want to say we don't want to discriminate by gender.
[00:25:44.280 --> 00:25:49.480]   This one for a business loan, I remember.
[00:25:49.480 --> 00:25:51.920]   But they kept in the business code type.
[00:25:51.920 --> 00:25:54.720]   So it was like restaurant, retail, manufacturing, blah, blah, blah.
[00:25:54.720 --> 00:25:57.040]   One of them was beauty and spas.
[00:25:57.040 --> 00:26:04.240]   And as it happens, X, I don't know, some very high percentage of the proportion in Ontario
[00:26:04.240 --> 00:26:06.160]   that are beauty and spa owners are women.
[00:26:06.160 --> 00:26:07.160]   So this is proxy encoded.
[00:26:07.160 --> 00:26:09.000]   So they sneak up all the time.
[00:26:09.000 --> 00:26:14.480]   I think it's also kind of like, if you really dig into it, you can keep going and uncover
[00:26:14.480 --> 00:26:17.560]   these potentially unfair variables.
[00:26:17.560 --> 00:26:19.520]   Right, right.
[00:26:19.520 --> 00:26:20.520]   Yeah.
[00:26:20.520 --> 00:26:21.520]   Interesting.
[00:26:21.520 --> 00:26:26.320]   I think we found some other interesting applications that you've talked about or your teams talked
[00:26:26.320 --> 00:26:29.520]   about like a text to SQL database interface.
[00:26:29.520 --> 00:26:31.480]   Would you want to talk about that at all?
[00:26:31.480 --> 00:26:36.280]   Yes, we built this tool called it, we called it Allen, which was a language, a listening
[00:26:36.280 --> 00:26:41.160]   answering neural network, I think is what the acronym standard for in the beginning.
[00:26:41.160 --> 00:26:43.720]   But yeah, it's a text to SQL interface.
[00:26:43.720 --> 00:26:52.800]   So basically, user comes in, poses a question, like, you know, find the highest rated stocks
[00:26:52.800 --> 00:26:55.320]   in my portfolio or something like that.
[00:26:55.320 --> 00:27:04.280]   And the system takes that query, goes into an SQL database, and one parses from a natural
[00:27:04.280 --> 00:27:06.360]   language utterance into something that's a little bit more structured.
[00:27:06.360 --> 00:27:08.800]   So it looks like a SQL field.
[00:27:08.800 --> 00:27:13.440]   And then two can actually go and compute the operation and output an answer.
[00:27:13.440 --> 00:27:17.680]   So you know, Google is the stock that has the highest rated portfolio, whatever it was
[00:27:17.680 --> 00:27:19.440]   that I said is the potential question.
[00:27:19.440 --> 00:27:20.440]   Right, right.
[00:27:20.440 --> 00:27:23.880]   And so how did you frame that even as a machine learning problem?
[00:27:23.880 --> 00:27:26.520]   Like, how did you get training data?
[00:27:26.520 --> 00:27:30.680]   Did you view it as like an NLP, like a sequence to sequence model type thing?
[00:27:30.680 --> 00:27:32.040]   Or how did you think about that?
[00:27:32.040 --> 00:27:34.440]   Yeah, it's a great question.
[00:27:34.440 --> 00:27:37.800]   The person on the team who built it, Yanshui, would be better equipped to answer it than
[00:27:37.800 --> 00:27:39.160]   I.
[00:27:39.160 --> 00:27:40.900]   But it was framed that way.
[00:27:40.900 --> 00:27:44.720]   So framed as a sequence to sequence mapping problem.
[00:27:44.720 --> 00:27:51.000]   When we started the application, transformers hadn't really taken off yet, but midway, they
[00:27:51.000 --> 00:27:52.480]   had.
[00:27:52.480 --> 00:27:59.120]   And it ended up being sort of this, how can we adapt transformers to very small data sets?
[00:27:59.120 --> 00:28:05.440]   Because we have very small, like there's close to no training data mapping natural utterance
[00:28:05.440 --> 00:28:09.840]   to extremely structured, like pseudo SQL.
[00:28:09.840 --> 00:28:16.840]   So we built this, we kind of bootstrapped this pseudo SQL database, and had a bunch
[00:28:16.840 --> 00:28:20.600]   of labelers come in and be like, yes, this is what this is.
[00:28:20.600 --> 00:28:22.240]   This is what this, it was sort of a pick list.
[00:28:22.240 --> 00:28:26.200]   It was like, if you say this question, does it mean X, Y, or Z?
[00:28:26.200 --> 00:28:30.440]   And they labeled the pick list, and we had that as our bootstrap training data set.
[00:28:30.440 --> 00:28:38.200]   And decided on the application because there's a lot of SQL databases in the bank, and in
[00:28:38.200 --> 00:28:40.520]   a lot of large enterprises.
[00:28:40.520 --> 00:28:46.200]   And often you've got like a handful of folks who are the analysts who are called upon to
[00:28:46.200 --> 00:28:49.640]   go and do these queries and find answers.
[00:28:49.640 --> 00:28:57.240]   So we'll build dashboards, like a Tableau type dashboard where that's sort of like commonly
[00:28:57.240 --> 00:28:58.240]   posed questions.
[00:28:58.240 --> 00:29:00.120]   They're kind of like FAQs, right?
[00:29:00.120 --> 00:29:04.160]   Where it makes sense to automate, like every month you see the chart.
[00:29:04.160 --> 00:29:08.280]   But our original hypothesis was there's probably lots of long tail questions that it doesn't
[00:29:08.280 --> 00:29:10.480]   make sense to program, but that would be really nice.
[00:29:10.480 --> 00:29:13.880]   But you also don't want to have sort of call in the data analyst to do the work on.
[00:29:13.880 --> 00:29:17.960]   So can we just have people ask those questions to the tools?
[00:29:17.960 --> 00:29:18.960]   Yeah, yeah.
[00:29:18.960 --> 00:29:19.960]   Interesting.
[00:29:19.960 --> 00:29:24.960]   Well, I guess switching gears a little bit, I was hoping to hear a little bit about your
[00:29:24.960 --> 00:29:27.960]   career and how you came to this really interesting job.
[00:29:27.960 --> 00:29:33.480]   I mean, I think you talked about coming up through humanities, although I think you do
[00:29:33.480 --> 00:29:39.960]   have a math degree, which is kind of a technical side of humanities.
[00:29:39.960 --> 00:29:45.400]   And then you did a grad school in comparative literature, right?
[00:29:45.400 --> 00:29:48.240]   Which is a little bit of an interesting switch maybe.
[00:29:48.240 --> 00:29:52.320]   Although I feel like I had a couple of friends in college that did math and conflict, but
[00:29:52.320 --> 00:29:53.640]   I was always sort of struck by that.
[00:29:53.640 --> 00:29:57.640]   I wonder if you could talk about kind of what you're thinking at the time and how that informs
[00:29:57.640 --> 00:29:58.640]   your work today.
[00:29:58.640 --> 00:29:59.640]   Yeah, great.
[00:29:59.640 --> 00:30:05.960]   I'm glad you noticed that I also have a math background because people often are like,
[00:30:05.960 --> 00:30:08.960]   "How does literature and then machine learning?"
[00:30:08.960 --> 00:30:15.520]   And I'm like, "Yes, but I did do a lot of work in linear algebra, so I can imagine functions."
[00:30:15.520 --> 00:30:17.680]   Yeah, it's a great question.
[00:30:17.680 --> 00:30:25.080]   I wish I had a master plan, but I didn't have a master plan.
[00:30:25.080 --> 00:30:29.000]   I actually intended originally to be a physics and philosophy major.
[00:30:29.000 --> 00:30:31.560]   Those were the things that interested me most.
[00:30:31.560 --> 00:30:35.160]   And I was kind of a klutz in the lab.
[00:30:35.160 --> 00:30:36.760]   I really didn't like the lab.
[00:30:36.760 --> 00:30:37.760]   So I was like, "You know what?
[00:30:37.760 --> 00:30:40.520]   None of this physics stuff, I'm just going to do the part where you don't have to go
[00:30:40.520 --> 00:30:43.560]   into the lab and just do math."
[00:30:43.560 --> 00:30:49.880]   And then I always loved the humanities and I spent my junior abroad in Paris and I didn't
[00:30:49.880 --> 00:30:53.000]   have to take any math courses because I had enough sort of standing credits.
[00:30:53.000 --> 00:30:58.440]   So I took courses in philosophy, film, literature, and I really loved it.
[00:30:58.440 --> 00:31:03.560]   So yeah, I decided to change my major my fourth year in college and instead of just doing
[00:31:03.560 --> 00:31:07.400]   math, do a double major in math in complete.
[00:31:07.400 --> 00:31:11.560]   And the good thing about complete is that it's kind of...
[00:31:11.560 --> 00:31:12.920]   Well, the good and bad thing.
[00:31:12.920 --> 00:31:16.680]   The bad thing is that it kind of lacks identity as a discipline.
[00:31:16.680 --> 00:31:19.560]   It's kind of like a grab bag of like...
[00:31:19.560 --> 00:31:20.560]   It used to be...
[00:31:20.560 --> 00:31:21.560]   So why comparative?
[00:31:21.560 --> 00:31:22.560]   It used to be...
[00:31:22.560 --> 00:31:27.400]   Imagine you take a theme like love and then you say, "How did the French write about it?
[00:31:27.400 --> 00:31:28.400]   How did the Germans write about it?"
[00:31:28.400 --> 00:31:32.920]   And you find these sort of cultural overlaps, which was the comparison.
[00:31:32.920 --> 00:31:37.940]   As the discipline has evolved, it's kind of become like some people focus on philosophy
[00:31:37.940 --> 00:31:42.120]   and literature, some people do cultural studies, some people do rigorous sort of history of
[00:31:42.120 --> 00:31:43.960]   the national literature.
[00:31:43.960 --> 00:31:47.320]   So the ambiguity was good for somebody like me because it was kind of like, "Sure, you
[00:31:47.320 --> 00:31:51.680]   want to do math, history of philosophy, history of literature, languages, semiot.
[00:31:51.680 --> 00:31:52.680]   Great.
[00:31:52.680 --> 00:31:54.680]   Great place for you."
[00:31:54.680 --> 00:31:55.680]   So I went into it.
[00:31:55.680 --> 00:32:00.080]   I really liked languages and I thought it provided a lot of freedom to explore.
[00:32:00.080 --> 00:32:05.880]   I wrote my dissertation on 17th century epistemology.
[00:32:05.880 --> 00:32:10.400]   So basically what was knowledge at the time.
[00:32:10.400 --> 00:32:14.300]   And I focused on Descartes, Leibniz, Newton.
[00:32:14.300 --> 00:32:18.080]   So sort of the old dead white guys.
[00:32:18.080 --> 00:32:19.080]   Classic math guys.
[00:32:19.080 --> 00:32:20.080]   Classic math guys.
[00:32:20.080 --> 00:32:21.080]   Exactly.
[00:32:21.080 --> 00:32:22.080]   Yeah.
[00:32:22.080 --> 00:32:24.560]   I know a lot about 17th century math that's not really as relevant today.
[00:32:24.560 --> 00:32:28.440]   Tell me some stuff about 17th century math.
[00:32:28.440 --> 00:32:30.720]   So like favorite things in 17th century math.
[00:32:30.720 --> 00:32:33.880]   So I mean the beginning, it's a dawning of calculus, right?
[00:32:33.880 --> 00:32:40.600]   So you've got Newton in particular is really, really fascinating.
[00:32:40.600 --> 00:32:43.000]   Leibniz and Newton, both of them.
[00:32:43.000 --> 00:32:48.000]   So like Leibniz was, he had this thing called Kojitachi Okaika, blind thought.
[00:32:48.000 --> 00:32:52.000]   He really thought that basically we could just let the symbols do all the work and it
[00:32:52.000 --> 00:32:56.900]   doesn't matter if we can visually represent some mathematical concept or if like it really
[00:32:56.900 --> 00:32:58.680]   has a tie to the real world.
[00:32:58.680 --> 00:33:00.960]   It was just like, let's go calculate stuff.
[00:33:00.960 --> 00:33:05.320]   And with that sort of focus on formalism, he did a lot of, he had a lot of development
[00:33:05.320 --> 00:33:11.200]   of like thinking about infinitesimal ratios and some of the mechanisms that go into making
[00:33:11.200 --> 00:33:16.360]   differentiation and integration possible that just kind of worked.
[00:33:16.360 --> 00:33:20.660]   Newton on the flip side kind of started off more on this formal track, but then he was
[00:33:20.660 --> 00:33:27.000]   influenced by a bunch of this sort of traditional focus on Greek math that was really prominent
[00:33:27.000 --> 00:33:29.920]   in 17th century England.
[00:33:29.920 --> 00:33:35.440]   And there they were like, you have to visualize, like you have to, it all comes back to geometry.
[00:33:35.440 --> 00:33:42.320]   Like geometry started with farmers out trying to measure distances in a field and like it
[00:33:42.320 --> 00:33:44.440]   needs to be grounded.
[00:33:44.440 --> 00:33:51.640]   And he grappled a lot with thinking about the gap between a limit and zero, right?
[00:33:51.640 --> 00:33:52.960]   And you see that through the Principia.
[00:33:52.960 --> 00:33:57.920]   I wrote a paper at one point on his notion of, they called them first and last ratios,
[00:33:57.920 --> 00:34:01.200]   which were basically like proto limits.
[00:34:01.200 --> 00:34:09.760]   And he kind of held himself back because he was really so focused on keeping things tangible,
[00:34:09.760 --> 00:34:12.080]   which I found really interesting between the two of them.
[00:34:12.080 --> 00:34:16.160]   So yeah, one 17th century math tidbit.
[00:34:16.160 --> 00:34:21.200]   Did you kind of continue this line of research in grad school or was it something else?
[00:34:21.200 --> 00:34:26.680]   I continued the line of research on 17th century math and philosophy in grad school, wrote a
[00:34:26.680 --> 00:34:31.360]   dissertation that five people have read on this topic.
[00:34:31.360 --> 00:34:39.160]   And yeah, and then afterwards, basically with complete, so word of the wise for any listeners
[00:34:39.160 --> 00:34:43.200]   who decide to be complete grad students, there's not a lot of complete departments.
[00:34:43.200 --> 00:34:45.680]   There's a lot of national language departments.
[00:34:45.680 --> 00:34:47.560]   And there's not a lot of ability.
[00:34:47.560 --> 00:34:51.160]   Like I think if I had been able to become a philosophy, like history of philosophy professor,
[00:34:51.160 --> 00:34:55.280]   I probably would have stayed in academic, but I was sort of prepared to be like a French
[00:34:55.280 --> 00:34:57.400]   literature 18th century professor, 17.
[00:34:57.400 --> 00:34:59.880]   And I was like, I don't know if that's really me.
[00:34:59.880 --> 00:35:01.120]   And there's not a lot of jobs.
[00:35:01.120 --> 00:35:05.880]   So it's like, you know, do I go to Nebraska and fight for my assistant professorship or
[00:35:05.880 --> 00:35:07.360]   do I go into tech?
[00:35:07.360 --> 00:35:09.720]   So I was out at Stanford.
[00:35:09.720 --> 00:35:13.840]   So just decided to switch careers.
[00:35:13.840 --> 00:35:17.920]   And I think something though, that like, you know, what is the humanities training?
[00:35:17.920 --> 00:35:22.360]   Does that, is that still with me besides having arcane knowledge that not many people want
[00:35:22.360 --> 00:35:24.360]   to talk about, but I'm glad you do.
[00:35:24.360 --> 00:35:26.720]   Normally it's a liability for me at work.
[00:35:26.720 --> 00:35:30.440]   Cause like I get feedback on performance reviews that are like, Catherine's really great, but
[00:35:30.440 --> 00:35:35.560]   sometimes she like goes off on these philosophical digressions and we're not really sure why.
[00:35:35.560 --> 00:35:41.880]   But I think one thing that I've brought with me is I was, I trained as like an intellectual
[00:35:41.880 --> 00:35:43.440]   historian in grad school.
[00:35:43.440 --> 00:35:47.840]   And so if you're a philosopher often today, you're like evaluating arguments for like,
[00:35:47.840 --> 00:35:48.840]   is this right?
[00:35:48.840 --> 00:35:49.840]   Right.
[00:35:49.840 --> 00:35:50.840]   Is this true?
[00:35:50.840 --> 00:35:53.240]   And then there's, you know, rel, people come in and say, well, there's no such thing as
[00:35:53.240 --> 00:35:55.720]   truth in the first place, everything's relative.
[00:35:55.720 --> 00:36:00.160]   I think as an intellectual historian, I didn't care if Descartes was right about, you know,
[00:36:00.160 --> 00:36:01.880]   the motion of planets in space.
[00:36:01.880 --> 00:36:06.840]   I was really interested in understanding what he thought he was thinking, like why this,
[00:36:06.840 --> 00:36:07.960]   what was he reading?
[00:36:07.960 --> 00:36:08.960]   What was happening around the time?
[00:36:08.960 --> 00:36:13.080]   Like what sort of staying, all right, I'm reading this as a 21st century reader and
[00:36:13.080 --> 00:36:18.120]   I'm coming with all of my prejudices and predispositions of thinking like somebody who's on the internet
[00:36:18.120 --> 00:36:22.280]   and viewing the world in a certain way and thinks that, you know, universal gravitation
[00:36:22.280 --> 00:36:24.560]   is second nature, but for him it was not.
[00:36:24.560 --> 00:36:30.960]   So it's, I think there was a lot of training in like, um, suspending disbelief, sort of
[00:36:30.960 --> 00:36:34.780]   ensuring that one didn't bring in one's own subjective predispositions and like really
[00:36:34.780 --> 00:36:36.840]   understanding a foreign thinker.
[00:36:36.840 --> 00:36:40.520]   Um, and actually I think that's really good training for product management.
[00:36:40.520 --> 00:36:44.980]   Um, I think it's really good training for executive work, like, cause you're constantly
[00:36:44.980 --> 00:36:49.480]   in situations like with a customer, it's, it's not, here's how I want to use my cashflow
[00:36:49.480 --> 00:36:50.480]   forecasting app.
[00:36:50.480 --> 00:36:54.560]   It's, there's going to be a distribution of millions of customers who are like totally
[00:36:54.560 --> 00:36:55.840]   different from me.
[00:36:55.840 --> 00:37:02.280]   Um, so it's, I guess I'm always approaching problems from the perspective of like, I'm
[00:37:02.280 --> 00:37:05.800]   not going to assume that there's one right answer and I'm not going to assume that this
[00:37:05.800 --> 00:37:09.920]   person thinks any, this is like similar for me or it comes from a similar place.
[00:37:09.920 --> 00:37:14.880]   Um, and I think that's been really good, like good training and, and doing product work
[00:37:14.880 --> 00:37:15.880]   eventually.
[00:37:15.880 --> 00:37:21.160]   I mean, you didn't just sort of study like any, you know, conflict.
[00:37:21.160 --> 00:37:27.120]   It's like, it's very kind of like, like different technical points of view.
[00:37:27.120 --> 00:37:30.560]   And I, I feel like you, you see that in ML too, right?
[00:37:30.560 --> 00:37:37.400]   Like I feel like there's this, I had a boss who always said he preferred to hire biologists
[00:37:37.400 --> 00:37:39.320]   over physicists.
[00:37:39.320 --> 00:37:44.640]   And I think what he meant by that is he liked people that sort of didn't really try to like
[00:37:44.640 --> 00:37:48.320]   figure out the underlying structure of models, but just sort of like examine them from the
[00:37:48.320 --> 00:37:51.480]   outside of like what they, what they do.
[00:37:51.480 --> 00:37:52.480]   Right.
[00:37:52.480 --> 00:37:55.280]   So like, just sort of like take this like kind of open mind, you know, we're not going
[00:37:55.280 --> 00:37:56.280]   to make assumptions.
[00:37:56.280 --> 00:38:02.240]   But then I think about Newton actually, and it's like, I feel like it seems to me like
[00:38:02.240 --> 00:38:06.440]   you tell me actually, it seems like Newton made this kind of like leap into like a lot
[00:38:06.440 --> 00:38:11.600]   of structure and really, so you must've wanted to put like an underlying structure on the
[00:38:11.600 --> 00:38:18.160]   world, like really badly to come up with such a, you know, such an amazing structure.
[00:38:18.160 --> 00:38:24.560]   And I wonder if, I mean, do you think Newton doing ML, it would have driven him nuts that
[00:38:24.560 --> 00:38:29.080]   like we kind of, this point of view of like looking at the models from the outside and
[00:38:29.080 --> 00:38:33.120]   just examining what they do and maybe not worrying about exactly how they work and making
[00:38:33.120 --> 00:38:34.120]   them more and more complicated.
[00:38:34.120 --> 00:38:36.680]   Yeah, that's a great question.
[00:38:36.680 --> 00:38:41.120]   I think there probably would be aspects of ML that would have driven Newton crazy.
[00:38:41.120 --> 00:38:47.040]   There's other aspects where I think there's some like kinship or, you know, predestined
[00:38:47.040 --> 00:38:48.040]   thinking.
[00:38:48.040 --> 00:38:54.520]   So, and I'm influenced here by one of my dear friends and mentors, a man named George Smith.
[00:38:54.520 --> 00:38:58.720]   He's a professor at Tufts who if you really want to know about Newton, like talk to George,
[00:38:58.720 --> 00:39:00.560]   like he's the guy.
[00:39:00.560 --> 00:39:05.920]   He's taught this course on basically how Newton changed the standards for high quality evidence
[00:39:05.920 --> 00:39:09.120]   for like 25 years and really knows a lot on this topic.
[00:39:09.120 --> 00:39:13.440]   But one of the things I learned from him is that Newton always assumed that the system
[00:39:13.440 --> 00:39:19.200]   that he was trying to model was infinitely more complex than the deductive, you know,
[00:39:19.200 --> 00:39:21.640]   mathematical model that he could apply to it.
[00:39:21.640 --> 00:39:27.960]   And so there's a lot in sort of the Newtonian scientific paradigm that's like, all right,
[00:39:27.960 --> 00:39:32.400]   we're going to put this hypothesis out there, this deductive model, then we're going to
[00:39:32.400 --> 00:39:36.240]   make observations and there's going to be a gap between what we observe and what we've
[00:39:36.240 --> 00:39:42.360]   modeled and, you know, the progress of this paradigm is to continuously see where there's
[00:39:42.360 --> 00:39:49.920]   a, to watch that gap and close it when possible, you know, by refining our mathematical model,
[00:39:49.920 --> 00:39:53.440]   but sometimes realize where it's just completely off the mark and we might need to sort of
[00:39:53.440 --> 00:39:54.880]   shift our thinking.
[00:39:54.880 --> 00:39:59.880]   And so to that extent, I think there's some, there's more affinity within sort of the ML
[00:39:59.880 --> 00:40:05.120]   mindset than like a sort of traditional like rules-based computer programming mindset,
[00:40:05.120 --> 00:40:07.800]   or even the Gophi type mindset, right.
[00:40:07.800 --> 00:40:12.400]   And like, we can sort of, as long as we can articulate the structure of the thinking,
[00:40:12.400 --> 00:40:14.640]   we can model the world.
[00:40:14.640 --> 00:40:16.560]   What is a Gophi type mindset?
[00:40:16.560 --> 00:40:19.680]   Like the good old fashioned AI, these sort of expert systems.
[00:40:19.680 --> 00:40:20.680]   Yeah.
[00:40:20.680 --> 00:40:21.680]   Right, right.
[00:40:21.680 --> 00:40:22.680]   Yeah.
[00:40:22.680 --> 00:40:23.680]   Nice ending with an acronym.
[00:40:23.680 --> 00:40:24.680]   But yeah, yeah.
[00:40:24.680 --> 00:40:30.200]   I'd say, I don't know, it's always been a plight of mine, like definitely spent a lot
[00:40:30.200 --> 00:40:36.760]   of my time in complete working on rationalist, hyper-structured 17th century thinkers and
[00:40:36.760 --> 00:40:38.600]   drove my conflict colleagues crazy.
[00:40:38.600 --> 00:40:42.320]   Cause I kind of came from the math background and my papers were like proofs versus like
[00:40:42.320 --> 00:40:43.320]   more exploratory.
[00:40:43.320 --> 00:40:48.600]   So I sometimes, I envy the ML mindset too, because I think coming from more of that,
[00:40:48.600 --> 00:40:53.680]   like always trying to prove things, it's not always the best, best approach to running
[00:40:53.680 --> 00:40:55.760]   a company either.
[00:40:55.760 --> 00:41:02.560]   Do you think Descartes would have had a different point of view on ML?
[00:41:02.560 --> 00:41:03.560]   Okay.
[00:41:03.560 --> 00:41:09.040]   So this is another like loose analogy, but basically this whole, like, you know, the
[00:41:09.040 --> 00:41:12.880]   famous, I think therefore I am, Kojito Ergosum.
[00:41:12.880 --> 00:41:19.600]   He phrased it that way in 1637, a discourse on method, which is sort of like, he has this
[00:41:19.600 --> 00:41:21.080]   like, here's my method.
[00:41:21.080 --> 00:41:22.960]   And then he runs it through three examples.
[00:41:22.960 --> 00:41:24.960]   One of which being the geometry.
[00:41:24.960 --> 00:41:25.960]   And one was on.
[00:41:25.960 --> 00:41:28.040]   Sorry, could you explain what that means?
[00:41:28.040 --> 00:41:32.280]   I've heard that a zillion times, but I don't think I know the implication of, I think therefore
[00:41:32.280 --> 00:41:33.280]   I am.
[00:41:33.280 --> 00:41:34.280]   Yeah.
[00:41:34.280 --> 00:41:35.280]   So, okay.
[00:41:35.280 --> 00:41:42.360]   So when he first stated it, he, what he was trying to do was big, bold 17th century work,
[00:41:42.360 --> 00:41:47.880]   prove that God exists, A, but then B sort of put forth a new way of thinking and doing
[00:41:47.880 --> 00:41:51.600]   science that was cleaner and upon which one could actually feel like they had to sort
[00:41:51.600 --> 00:41:57.080]   of like, they could believe these statements and propositions and truths versus the predecessors,
[00:41:57.080 --> 00:42:00.880]   which were like always like citing the ancient.
[00:42:00.880 --> 00:42:02.320]   So it's like, why is something true?
[00:42:02.320 --> 00:42:06.680]   It's true because Aristotle said it was true versus like, it's true because I have used
[00:42:06.680 --> 00:42:10.960]   logic to come to a propositional type of truth.
[00:42:10.960 --> 00:42:13.800]   So when he was starting his, all right, let's prove that God exists.
[00:42:13.800 --> 00:42:15.560]   He says, well, where do I start?
[00:42:15.560 --> 00:42:20.440]   Why don't I start by proving that, you know, I there's, there's some, there's some clear
[00:42:20.440 --> 00:42:24.600]   point that I can stand upon where it's like, I know that this is what truth looks like.
[00:42:24.600 --> 00:42:30.520]   And so the, the Cogito Ergo Sum was, was that point where it was like, basically he's like,
[00:42:30.520 --> 00:42:35.760]   no matter how hard I try, if I try to pretend I don't exist, there's gotta be somebody there
[00:42:35.760 --> 00:42:38.320]   doing that thinking, therefore I must exist.
[00:42:38.320 --> 00:42:39.800]   So it's kind of this proof by contradiction.
[00:42:39.800 --> 00:42:43.640]   Like, you know, there's gotta be some voice there.
[00:42:43.640 --> 00:42:47.640]   What's interesting is he rewrote this in his second attempt in it.
[00:42:47.640 --> 00:42:48.640]   He got rid of the thinking.
[00:42:48.640 --> 00:42:49.960]   So he didn't say Cogito.
[00:42:49.960 --> 00:42:53.520]   He just said, I am, I exist.
[00:42:53.520 --> 00:42:58.720]   And then he said, and if you want to understand how this truth works, he didn't use these
[00:42:58.720 --> 00:43:06.800]   words, but you know, my paraphrase, he's basically like, go sit in a room and meditate for days,
[00:43:06.800 --> 00:43:11.680]   like do it, like repeat it and do this for like 30 days.
[00:43:11.680 --> 00:43:17.800]   And eventually you will, you will have trained your mind to think clearly.
[00:43:17.800 --> 00:43:20.920]   So I looked at that and I was like, well, that's different.
[00:43:20.920 --> 00:43:24.760]   That's not quite what I thought Descartes was about.
[00:43:24.760 --> 00:43:29.320]   Like go sit in a room and repeat things, you know, until you, you train your mind to think
[00:43:29.320 --> 00:43:30.320]   that way.
[00:43:30.320 --> 00:43:31.320]   So that was really interesting to me.
[00:43:31.320 --> 00:43:36.520]   And this is a loose analogy, but I think there's something sort of similar to supervised learning
[00:43:36.520 --> 00:43:37.920]   when it's like, is this a dog?
[00:43:37.920 --> 00:43:39.160]   Is this a cat?
[00:43:39.160 --> 00:43:45.280]   It's just like, show me 50 examples and repeat until it, until it becomes like, you've established
[00:43:45.280 --> 00:43:48.840]   the input output pattern.
[00:43:48.840 --> 00:43:50.840]   That's not really there, but I think it's kind of there.
[00:43:50.840 --> 00:43:55.200]   And I think it's kind of interesting that there's sort of this like intellectual foundation
[00:43:55.200 --> 00:43:57.160]   for supervised learning and Descartes.
[00:43:57.160 --> 00:44:01.480]   So, although it seems like with Descartes, it's like, there's maybe no input if you're
[00:44:01.480 --> 00:44:02.480]   meditating.
[00:44:02.480 --> 00:44:08.760]   There's no input besides your own trading your mind to like rewire, it's like rewire
[00:44:08.760 --> 00:44:10.680]   your mind to think this way.
[00:44:10.680 --> 00:44:11.680]   I see.
[00:44:11.680 --> 00:44:12.680]   I see.
[00:44:12.680 --> 00:44:13.680]   Yeah.
[00:44:13.680 --> 00:44:14.680]   Interesting.
[00:44:14.680 --> 00:44:15.680]   Yeah.
[00:44:15.680 --> 00:44:19.880]   Do you then have thoughts on AI being sentient?
[00:44:19.880 --> 00:44:27.800]   Like do you have opinions on, I guess, things like the, the Turing test or what are the
[00:44:27.800 --> 00:44:32.840]   those classic ones that you learned in your first AI class on like the, the room with
[00:44:32.840 --> 00:44:36.680]   the person in it and the books like doing Chinese or something?
[00:44:36.680 --> 00:44:37.680]   Yeah.
[00:44:37.680 --> 00:44:39.200]   The Chinese, the John Searle Chinese room.
[00:44:39.200 --> 00:44:40.200]   Yeah.
[00:44:40.200 --> 00:44:41.200]   Yeah.
[00:44:41.200 --> 00:44:42.560]   To be honest, not really.
[00:44:42.560 --> 00:44:51.240]   I've always like, I find the Turing test interesting conceptually, but I struggle to see, I struggle
[00:44:51.240 --> 00:44:56.880]   with the arguments that are like, you know, the sort of singularity type arguments, like
[00:44:56.880 --> 00:45:01.280]   computation is rising and the models are more complex and these models are going to get
[00:45:01.280 --> 00:45:03.640]   to the point where they're, they come into consciousness.
[00:45:03.640 --> 00:45:08.200]   I, I just don't, I don't, I don't really see it.
[00:45:08.200 --> 00:45:09.200]   Do you?
[00:45:09.200 --> 00:45:11.520]   Like, I don't know.
[00:45:11.520 --> 00:45:15.520]   Well we have this existence proof with humans, right?
[00:45:15.520 --> 00:45:20.400]   That seems like consciousness kind of comes from some type of process.
[00:45:20.400 --> 00:45:26.600]   So it seems to me like, unless you think there's something really like, you know, something
[00:45:26.600 --> 00:45:32.160]   like God in there, in the physics somehow there must, it sort of must come from like
[00:45:32.160 --> 00:45:35.320]   increasingly complicated computation.
[00:45:35.320 --> 00:45:36.320]   Right.
[00:45:36.320 --> 00:45:37.320]   Yeah, for sure.
[00:45:37.320 --> 00:45:38.320]   Yeah.
[00:45:38.320 --> 00:45:39.320]   And I'm, I sort of fall into the materialist.
[00:45:39.320 --> 00:45:46.160]   I, while I spent a lot of time with the 17th century philosophers, I don't share the, the
[00:45:46.160 --> 00:45:49.080]   sense of, you know, the soul and there's a God in there that makes any difference.
[00:45:49.080 --> 00:45:50.080]   Fair enough.
[00:45:50.080 --> 00:45:51.080]   Fair enough.
[00:45:51.080 --> 00:45:52.920]   But it's interesting.
[00:45:52.920 --> 00:45:57.720]   There's still that gap, you know, between, and I don't know if it's the plasticity of
[00:45:57.720 --> 00:46:04.400]   our neurons, the fact that there's just thousands of, you know, billions, trillions of very
[00:46:04.400 --> 00:46:06.720]   plastic processes going on in there.
[00:46:06.720 --> 00:46:10.320]   Or if it's like a Dan, I don't know if you have Dan Dennett type argument where the self
[00:46:10.320 --> 00:46:14.400]   is basically a user illusion, right.
[00:46:14.400 --> 00:46:20.200]   In the same way that we interface with, you know, I'm looking at you on a zoom screen
[00:46:20.200 --> 00:46:25.640]   right now, my iOS operating system, which makes it easy for me to engage with the computer
[00:46:25.640 --> 00:46:29.620]   versus seeing the, the, the nitty gritty insides.
[00:46:29.620 --> 00:46:36.240]   Maybe it is a useful illusion, you know, that we've gained through evolution, but that is
[00:46:36.240 --> 00:46:37.880]   not, it's not really real.
[00:46:37.880 --> 00:46:39.880]   I kind of buy that argument.
[00:46:39.880 --> 00:46:45.520]   So it's all, it's a red, basically consciousness is a red herring is kind of what that argument
[00:46:45.520 --> 00:46:46.520]   would be.
[00:46:46.520 --> 00:46:50.960]   So are you the kind of person that would, would get into like a transporter that would
[00:46:50.960 --> 00:46:54.000]   like disintegrate your body and reassemble it somewhere else?
[00:46:54.000 --> 00:46:57.240]   What do you feel like that's a safe thing to do?
[00:46:57.240 --> 00:47:01.240]   There's some very strong opinions on different sides of that at Weights and Biases.
[00:47:01.240 --> 00:47:04.680]   That's a question I've never really thought about that.
[00:47:04.680 --> 00:47:10.760]   I might change now that I have a son, you know, now that I'm a mom, if you met me a
[00:47:10.760 --> 00:47:13.920]   year ago, maybe I'd say like, yeah, sure.
[00:47:13.920 --> 00:47:18.240]   But now it's almost like, would that mean that there's some implication on my relationship
[00:47:18.240 --> 00:47:19.240]   to my child?
[00:47:19.240 --> 00:47:20.240]   I'm not sure I want that.
[00:47:20.240 --> 00:47:21.240]   Yeah.
[00:47:21.240 --> 00:47:22.240]   That's funny.
[00:47:22.240 --> 00:47:25.600]   I have a small daughter too, but I think I've my whole life, I would just never get in that
[00:47:25.600 --> 00:47:26.600]   machine to me.
[00:47:26.600 --> 00:47:27.600]   It's incredibly unsafe.
[00:47:27.600 --> 00:47:32.200]   I don't know if I could justify it, but I just would not do that.
[00:47:32.200 --> 00:47:33.200]   All right.
[00:47:33.200 --> 00:47:39.440]   We always end with two questions that are a little more on the practical side, but this
[00:47:39.440 --> 00:47:40.440]   is really fun.
[00:47:40.440 --> 00:47:50.040]   So, so what is, I guess, what's a topic in ML that you think is understudied or underappreciated?
[00:47:50.040 --> 00:47:54.800]   So I don't know if it's understudied, but I, I think it's, it has been underappreciated
[00:47:54.800 --> 00:47:58.680]   and now is becoming more appreciated, but this is causal inference.
[00:47:58.680 --> 00:48:01.760]   So like the Judea pearl do calculus type work.
[00:48:01.760 --> 00:48:07.080]   This is something we're starting to really look into at the bank because, because of
[00:48:07.080 --> 00:48:14.400]   this need for sort of more interpretable models and lots of conditional probabilities where
[00:48:14.400 --> 00:48:19.360]   if we could understand what happens in one variable and how that relates to another variable,
[00:48:19.360 --> 00:48:23.540]   it'd be really, really useful for sort of macroeconomic modeling.
[00:48:23.540 --> 00:48:28.720]   So that's, it's something that I think it also is, it's a topic that a person like me
[00:48:28.720 --> 00:48:32.160]   is going to be interested in because it's like philosopher candy as well.
[00:48:32.160 --> 00:48:37.360]   Like there's lots of inter, you know, interdisciplinary approaches to this problem.
[00:48:37.360 --> 00:48:42.560]   What is a cause if we can even really define it well and, you know, it's represented formally
[00:48:42.560 --> 00:48:45.240]   in the, in machine learning in one particular way.
[00:48:45.240 --> 00:48:47.400]   But I think there's, I think it's going to be interesting over the next couple of years
[00:48:47.400 --> 00:48:52.640]   to see these sort of traditional causal inference methods interacting with deep learning and
[00:48:52.640 --> 00:48:53.640]   the deep learning community.
[00:48:53.640 --> 00:48:58.040]   So that's, that's one, one thing that we're, I'm personally excited about, but also Borealis
[00:48:58.040 --> 00:48:59.400]   is looking into these days.
[00:48:59.400 --> 00:49:00.400]   Interesting.
[00:49:00.400 --> 00:49:01.400]   Super cool.
[00:49:01.400 --> 00:49:06.640]   Is there like a paper that you could point people to if they were interested in learning
[00:49:06.640 --> 00:49:07.640]   more about?
[00:49:07.640 --> 00:49:08.640]   Yeah, yeah.
[00:49:08.640 --> 00:49:10.200]   I'll send a link after.
[00:49:10.200 --> 00:49:14.320]   There's a paper recently, I know Elias Barenboim, who's one of the, he was one of the Perl students
[00:49:14.320 --> 00:49:15.320]   and he's at Columbia.
[00:49:15.320 --> 00:49:18.960]   He was a coauthor, Joshua Benjio is a coauthor, and then there's two others that I know as
[00:49:18.960 --> 00:49:19.960]   well.
[00:49:19.960 --> 00:49:22.280]   That's all about sort of deep learning and causal inference.
[00:49:22.280 --> 00:49:24.320]   That's probably a great place to start.
[00:49:24.320 --> 00:49:25.320]   Cool.
[00:49:25.320 --> 00:49:29.240]   So the final question we always ask, and you've seen so many different applications, I think
[00:49:29.240 --> 00:49:33.800]   they'll have a really interesting perspective on this, is basically like kind of going from
[00:49:33.800 --> 00:49:38.040]   like wanting to, to build a model for some purpose and kind of getting it deployed in
[00:49:38.040 --> 00:49:40.640]   production actually doing that purpose.
[00:49:40.640 --> 00:49:45.880]   Like where do you feel like there's the biggest painful or most painful bottlenecks?
[00:49:45.880 --> 00:49:48.560]   Yeah, there's often a lot.
[00:49:48.560 --> 00:49:50.840]   I would say the most painful.
[00:49:50.840 --> 00:49:58.960]   So I think, I think at the highest level, it's really deep integration into the full
[00:49:58.960 --> 00:50:00.400]   business process.
[00:50:00.400 --> 00:50:05.240]   So and this is really coming also from an enterprise ML perspective versus like a sort
[00:50:05.240 --> 00:50:06.760]   of ML for a software company.
[00:50:06.760 --> 00:50:13.400]   But I've seen tons of projects fail where you might have a good, like given a task,
[00:50:13.400 --> 00:50:20.400]   given a model, if it's just handed over to the business without considerations of like,
[00:50:20.400 --> 00:50:24.520]   all right, well, where's, where does the production data sit?
[00:50:24.520 --> 00:50:29.920]   How do we get that data from that environment to the environment where our model sits to
[00:50:29.920 --> 00:50:30.920]   do inference?
[00:50:30.920 --> 00:50:36.120]   There's always questions on like, just the timeframe, or is this, you know, is this batch
[00:50:36.120 --> 00:50:39.800]   monthly, weekly, real time?
[00:50:39.800 --> 00:50:45.160]   And making sure I've seen stuff where like, there's, we think we can easily do a batch
[00:50:45.160 --> 00:50:48.640]   output, like it's just monthly, you know, output a set of predictions that are going
[00:50:48.640 --> 00:50:51.680]   to go into some call center list.
[00:50:51.680 --> 00:50:58.040]   But there's like, some nuance in the process where the third week of every month, they
[00:50:58.040 --> 00:51:00.600]   do this to the data, and that's going to mess this up.
[00:51:00.600 --> 00:51:04.240]   And so it's, it's always in the details of like, what the what that full flow will look
[00:51:04.240 --> 00:51:05.240]   like.
[00:51:05.240 --> 00:51:09.000]   And then the third, yeah, with the business processes, all right, now you've output the
[00:51:09.000 --> 00:51:11.040]   prediction, but how does the process change?
[00:51:11.040 --> 00:51:15.760]   Because if people just use it, and like, they continue to do what they're doing, I don't
[00:51:15.760 --> 00:51:20.640]   think you're really taking advantage of right now that we have this, like, we can shift
[00:51:20.640 --> 00:51:24.000]   our approach, let's say it's a call center automation thing, like, we can shift the number
[00:51:24.000 --> 00:51:29.320]   of people we have on staff at a given time, we can collect the following new data to improve
[00:51:29.320 --> 00:51:30.440]   the process in some way.
[00:51:30.440 --> 00:51:35.920]   So I think you have to think about it holistically, right, in terms of like, what's the end result?
[00:51:35.920 --> 00:51:36.920]   Where's the set?
[00:51:36.920 --> 00:51:37.920]   How do you measure it?
[00:51:37.920 --> 00:51:42.160]   So that's kind of all of the production ML pipeline, but I actually think it's all there
[00:51:42.160 --> 00:51:43.160]   and like, it all matters.
[00:51:43.160 --> 00:51:48.160]   It's spoken like someone who's done a bunch of production pipelines, I think.
[00:51:48.160 --> 00:51:49.160]   Thank you.
[00:51:49.160 --> 00:51:50.160]   Thank you very much.
[00:51:50.160 --> 00:51:51.160]   That's really fun.
[00:51:51.160 --> 00:51:56.280]   If you're enjoying these interviews, and you want to learn more, please click on the link
[00:51:56.280 --> 00:52:01.000]   to the show notes in the description where you can find links to all the papers that
[00:52:01.000 --> 00:52:05.400]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:52:05.400 --> 00:52:06.400]   produce.
[00:52:06.400 --> 00:52:06.400]   So check it out.
[00:52:06.400 --> 00:52:06.900]   out.

