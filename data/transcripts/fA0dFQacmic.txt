
[00:00:00.000 --> 00:00:03.360]   I want to introduce you to this course I've been working on.
[00:00:03.360 --> 00:00:11.840]   I've just released it and I wanted to give a lot of you guys who subscribed and follow
[00:00:11.840 --> 00:00:20.240]   me on Medium or Twitter, I wanted to give you guys a chance to get this course for free.
[00:00:20.240 --> 00:00:24.160]   So for the next three days it is completely free, you just use this code.
[00:00:24.160 --> 00:00:28.940]   But I just want to talk very quickly about what it actually covers.
[00:00:28.940 --> 00:00:35.440]   Now obviously you can see from the title it's NLP and it's with Transformers and Python.
[00:00:35.440 --> 00:00:40.880]   Now if we scroll down a little bit we come to this course overview video and I'll just
[00:00:40.880 --> 00:00:47.440]   quickly go through this because it's quite long and I don't want to take too much of
[00:00:47.440 --> 00:00:50.360]   your time and we cover a lot of things.
[00:00:50.360 --> 00:00:56.840]   So first thing is NLP and Transformers where I give a quick summary of NLP in general,
[00:00:56.840 --> 00:01:02.240]   the history of NLP leading up to Transformers.
[00:01:02.240 --> 00:01:04.880]   Then we move into a bit of pre-processing for NLP.
[00:01:04.880 --> 00:01:11.880]   Now this is just your basic stuff, I think the most relevant one here for us in Transformers
[00:01:11.880 --> 00:01:17.660]   is Unicode normalization and tokenization special tokens.
[00:01:17.660 --> 00:01:25.080]   Then I move through a few lectures on attention, how attention works and describing the logic
[00:01:25.080 --> 00:01:28.720]   behind it.
[00:01:28.720 --> 00:01:34.120]   I always see this as like the hello world of NLP which is sentiment analysis.
[00:01:34.120 --> 00:01:44.120]   I think it's a great introduction and we introduce Transformers in this section here.
[00:01:44.120 --> 00:01:48.440]   And it's worth pointing out as well that I use a lot of different frameworks throughout
[00:01:48.440 --> 00:01:49.440]   this course.
[00:01:49.440 --> 00:01:55.440]   So Flare is the very first one, we also use Hockey Face Transformers, that's obviously
[00:01:55.440 --> 00:02:04.280]   the primary one that we'll be using throughout the course, TensorFlow, PyTorch, NLTK, Spacey
[00:02:04.280 --> 00:02:06.680]   and many others as well.
[00:02:06.680 --> 00:02:13.400]   So there's a lot in there, of course using a lot of BERT.
[00:02:13.400 --> 00:02:21.040]   So there's two projects in the course as well, the first of those is sentiment analysis,
[00:02:21.040 --> 00:02:23.320]   the second one is question answering.
[00:02:23.320 --> 00:02:27.000]   Both of them I think are great because they take you all the way through from the very
[00:02:27.000 --> 00:02:31.080]   start of your project, so getting data, all the way through to actually building your
[00:02:31.080 --> 00:02:36.360]   model and applying it to your data.
[00:02:36.360 --> 00:02:46.400]   So moving on to named entity recognition, question answering, how we measure the performance
[00:02:46.400 --> 00:02:55.760]   of our models which is of course very important, a full question answering stack using another
[00:02:55.760 --> 00:02:59.440]   library called Haystack which I think this is one of the coolest things in the course
[00:02:59.440 --> 00:03:07.880]   in my opinion and in NLP in general, this sort of stuff is incredibly cool.
[00:03:07.880 --> 00:03:12.480]   Then like I said, there's that second project, the Q&A project.
[00:03:12.480 --> 00:03:19.400]   Before we move on to similarity, now similarity is super important in NLP and I think probably
[00:03:19.400 --> 00:03:26.480]   one of the most promising areas in the future for further research and just impact that
[00:03:26.480 --> 00:03:29.520]   it could have on industry.
[00:03:29.520 --> 00:03:34.080]   I think this is really a super cool place to be.
[00:03:34.080 --> 00:03:38.160]   Then finally we move on to fine tuning.
[00:03:38.160 --> 00:03:48.000]   So that's the course in a nutshell, all together there's 11 hours of content so it's I think
[00:03:48.000 --> 00:03:57.600]   comparatively long when you look at other NLP courses, so we see this 11, 10, 10, 3
[00:03:57.600 --> 00:04:06.640]   and 6 and as far as I'm aware it's the first course that focuses on Transformers, on Udemy.
[00:04:06.640 --> 00:04:14.280]   So if you're into NLP, obviously Transformers are really the models that you want to be
[00:04:14.280 --> 00:04:21.040]   using, check out the course in the next few days, it's completely free using this code.
[00:04:21.040 --> 00:04:25.600]   So thank you for watching and I hope you enjoy the course.

