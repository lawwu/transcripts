
[00:00:00.000 --> 00:00:23.520]   Do you buy this narrow window framing of the intelligence explosion of given that you have to be two orders of magnitude bigger at every generation, if you don't get AGI by GPT-7 that can help you catapult an intelligence explosion, like, you're kind of just fucked as far as like, much smarter intelligences go, and you're kind of stuck with GPT-7 level models for a long time.
[00:00:23.560 --> 00:00:30.360]   Because at that point, you're just like consuming significant fractions of the economy to make that model. And we just don't have the wherewithal to like make GPT-8.
[00:00:30.360 --> 00:00:43.680]   Yeah, I mean, I generally buy that increases in order of magnitude of compute by like, in absolute terms, almost like diminishing returns on like capability, right? Like we've seen over a couple orders of magnitude models go from being unable to do anything to be able to like do huge amounts.
[00:00:43.880 --> 00:01:01.600]   And it feels to me like, each incremental order of magnitude, like it's more nines of reliability of things. And so it unlocks things like agents, but at least at the moment, I haven't seen like, transformatively, like, it doesn't feel like reasoning improves, like linearly, so to speak, but rather like somewhat sub-linearly.
[00:01:01.640 --> 00:01:14.920]   If there is this diminishing increase in capabilities, and, and that increased cost exponentially more to get, that's actually a bearish sign on like, what 4.5 will be able to do or what 5 will unlock in terms of economic impact.
[00:01:14.920 --> 00:01:30.480]   That being said, for me, the jump between 3.5 and 4 is like pretty huge. And so like, even if I was like, another 3.5 to 4 jump is like ridiculous, right? Like if you if you imagine 5 as being a 3.5 to 4 jump, like straight off the bat in terms of like ability to do SATs and this kind of stuff.
[00:01:30.520 --> 00:01:32.920]   Yeah, the LSAT performance was particularly striking.
[00:01:32.920 --> 00:01:56.040]   Exactly. You go from like, not super smart to like, very smart to like, utter genius in the next generation instantly. And it doesn't, at least like, to me feel like we're, we're gonna sort of jump to utter genius in the next generation, but it does feel like we'll get very smart plus lots of reliability. And then like, we'll see TBD, what that continues to look like, the jumps that we've seen so far are huge.
[00:01:56.840 --> 00:02:09.480]   And even if those continue on like a smaller scale, we're still in for extremely smart, like very reliable agents, like over the next couple of orders of magnitude. And so like, we didn't sort of fully close the thread on the narrow window thing.
[00:02:10.600 --> 00:02:39.400]   When you you think like, let's say, GPT forecast, I know, let's call it $100 million or whatever. You have what the 1B run, the 10B run, the 100B run all seem very plausible by, you know, private company standards. And then you can also imagine even like a 1T run being part of like a national consortium or like a national level thing, but much harder on the behalf of an individual company. But Sammy is out there trying to raise $7 trillion.
[00:02:39.400 --> 00:02:47.480]   He's already preparing for like, whole order of magnitude more than the, he's shifting the orders of magnitude here beyond the national level.
[00:02:47.480 --> 00:02:58.600]   So I want to point out the one we have a lot more jumps. And even if those jumps are relatively smaller, that's still a pretty stark improvement in capability.
[00:02:58.840 --> 00:03:17.560]   Not only that, but if you believe claims that GPT-4 is around 1 trillion parameter count, I mean, the human brain is between 30 and 300 trillion synapses. And so that's obviously not a one to one mapping and we can debate the numbers. But it seems pretty plausible that we're below brain scale, still.
[00:03:18.120 --> 00:03:42.760]   So crucially, the point being that the algorithmic overhead is really high in the sense that, and maybe this is something we should touch on explicitly of, even if you can't keep dumping more compute beyond the models that cost a trillion dollars or something, the fact that the brain is so much more data efficient implies that if you can, we have the compute, if we had like the brain's algorithm to train.
[00:03:44.280 --> 00:03:49.880]   If we could like train as a sample efficient as humans train from birth, we could make the AGI.
[00:03:49.880 --> 00:04:07.880]   Yeah. But the sample efficiency stuff, I never know exactly how to think about it because obviously a lot of things are hardwired in certain ways. Right. And they're like the co-evolution of language and the brain structure. So it's hard to say. Also, there are some results that if you make your model bigger, it becomes more sample efficient.
[00:04:09.240 --> 00:04:21.800]   Yeah. The original scaling was paper, right? Right. So, so maybe that also just solves it. Like you don't have to be more data efficient, but if your model is bigger than you also just are more data efficient.

