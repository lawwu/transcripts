
[00:00:00.000 --> 00:00:03.080]   The following is a conversation with Daniel Schmachtenberger,
[00:00:03.080 --> 00:00:05.780]   a founding member of the Consilience Project
[00:00:05.780 --> 00:00:09.320]   that is aimed at improving public sensemaking and dialogue.
[00:00:09.320 --> 00:00:12.640]   He's interested in understanding how we humans
[00:00:12.640 --> 00:00:15.400]   can be the best version of ourselves as individuals
[00:00:15.400 --> 00:00:18.800]   and as collectives at all scales.
[00:00:18.800 --> 00:00:20.480]   Quick mention of our sponsors,
[00:00:20.480 --> 00:00:23.560]   Ground News, NetSuite, Four Sigmatic,
[00:00:23.560 --> 00:00:25.800]   Magic Spoon, and BetterHelp.
[00:00:25.800 --> 00:00:29.480]   Check them out in the description to support this podcast.
[00:00:29.480 --> 00:00:31.520]   As a side note, let me say that I got a chance
[00:00:31.520 --> 00:00:34.700]   to talk to Daniel on and off the mic for a couple of days.
[00:00:34.700 --> 00:00:37.920]   We took a long walk the day before our conversation.
[00:00:37.920 --> 00:00:40.680]   I really enjoyed meeting him, just on a basic human level.
[00:00:40.680 --> 00:00:42.940]   We talked about the world around us
[00:00:42.940 --> 00:00:46.200]   with words that carried hope for us individual ants
[00:00:46.200 --> 00:00:50.000]   actually contributing something of value to the colony.
[00:00:50.000 --> 00:00:52.920]   These conversations are the reasons I love human beings,
[00:00:52.920 --> 00:00:56.640]   our insatiable striving to lessen the suffering in the world.
[00:00:56.640 --> 00:00:59.080]   But more than that, there's a simple magic
[00:00:59.080 --> 00:01:01.680]   to two strangers meeting for the first time
[00:01:01.680 --> 00:01:04.440]   and sharing ideas, becoming fast friends,
[00:01:04.440 --> 00:01:06.320]   and creating something that is far greater
[00:01:06.320 --> 00:01:08.280]   than the sum of our parts.
[00:01:08.280 --> 00:01:10.040]   I've gotten to experience some of that same magic
[00:01:10.040 --> 00:01:12.440]   here in Austin with a few new friends
[00:01:12.440 --> 00:01:16.060]   and in random bars in my travels across this country,
[00:01:16.060 --> 00:01:17.800]   where a conversation leaves me
[00:01:17.800 --> 00:01:19.760]   with a big stupid smile on my face
[00:01:19.760 --> 00:01:24.220]   and a new appreciation of this too short, too beautiful life.
[00:01:24.220 --> 00:01:26.280]   This is the Lex Friedman Podcast,
[00:01:26.280 --> 00:01:30.120]   and here is my conversation with Daniel Schmachtenberger.
[00:01:30.120 --> 00:01:33.680]   If aliens were observing Earth
[00:01:33.680 --> 00:01:36.900]   through the entire history, just watching us,
[00:01:36.900 --> 00:01:40.240]   and were tasked with summarizing what happened until now,
[00:01:40.240 --> 00:01:41.880]   what do you think they would say?
[00:01:41.880 --> 00:01:43.880]   What do you think they would write up in that summary?
[00:01:43.880 --> 00:01:46.960]   Like it has to be pretty short, less than a page.
[00:01:46.960 --> 00:01:49.040]   Like in "Hitchhiker's Guide,"
[00:01:49.040 --> 00:01:50.040]   (Daniel laughing)
[00:01:50.040 --> 00:01:52.960]   there's I think like a paragraph or a couple sentences.
[00:01:52.960 --> 00:01:56.000]   How would you summarize, sorry,
[00:01:56.000 --> 00:01:58.320]   how would the alien summarize, do you think,
[00:01:58.320 --> 00:01:59.780]   all of human civilization?
[00:01:59.780 --> 00:02:04.400]   - My first thoughts take more than a page.
[00:02:04.400 --> 00:02:05.760]   They'd probably distill it.
[00:02:05.760 --> 00:02:10.040]   'Cause if they watched, well, I mean, first,
[00:02:10.040 --> 00:02:12.440]   I have no idea if their senses are even attuned
[00:02:12.440 --> 00:02:15.280]   to similar stuff to what our senses are attuned to,
[00:02:15.280 --> 00:02:17.380]   or what the nature of their consciousness is like
[00:02:17.380 --> 00:02:18.280]   relative to ours.
[00:02:18.280 --> 00:02:21.100]   And so let's assume that they're kind of like us,
[00:02:21.100 --> 00:02:22.440]   just technologically more advanced
[00:02:22.440 --> 00:02:23.880]   to get here from wherever they are.
[00:02:23.880 --> 00:02:25.200]   That's the first kind of constraint
[00:02:25.200 --> 00:02:27.280]   on the thought experiment.
[00:02:27.280 --> 00:02:29.780]   And then if they've watched throughout all of history,
[00:02:29.780 --> 00:02:32.680]   they saw the burning of Alexandria,
[00:02:32.680 --> 00:02:36.600]   they saw that 2000 years ago in Greece,
[00:02:36.600 --> 00:02:38.080]   we were producing things like clocks,
[00:02:38.080 --> 00:02:39.360]   the Antikythera mechanism,
[00:02:39.360 --> 00:02:40.880]   and then that technology got lost.
[00:02:40.880 --> 00:02:44.240]   They saw that there wasn't just a steady dialectic
[00:02:44.240 --> 00:02:45.280]   of progress.
[00:02:45.280 --> 00:02:46.200]   - So every once in a while,
[00:02:46.200 --> 00:02:49.760]   there's a giant fire that destroys a lot of things.
[00:02:49.760 --> 00:02:53.020]   There's a giant like commotion
[00:02:53.020 --> 00:02:54.260]   that destroys a lot of things.
[00:02:54.260 --> 00:02:56.640]   - Yeah, and it's usually self-induced.
[00:02:56.640 --> 00:03:00.640]   They would have seen that.
[00:03:00.640 --> 00:03:03.840]   And so as they're looking at us now,
[00:03:03.840 --> 00:03:07.040]   as we move past the nuclear weapons age
[00:03:07.040 --> 00:03:08.800]   into the full globalization,
[00:03:08.800 --> 00:03:11.460]   Anthropocene exponential tech age,
[00:03:11.460 --> 00:03:15.800]   still making our decisions relatively similarly
[00:03:15.800 --> 00:03:18.280]   to how we did in the stone age
[00:03:18.280 --> 00:03:20.240]   as far as rivalry game theory type stuff.
[00:03:20.240 --> 00:03:22.960]   I think they would think that this is probably
[00:03:22.960 --> 00:03:24.380]   most likely one of the planets
[00:03:24.380 --> 00:03:26.340]   that is not gonna make it to being intergalactic
[00:03:26.340 --> 00:03:27.340]   'cause we blow ourselves up
[00:03:27.340 --> 00:03:29.460]   in the technological adolescence.
[00:03:29.460 --> 00:03:30.580]   And if we are going to,
[00:03:30.580 --> 00:03:35.580]   we're gonna need some major progress rapidly
[00:03:35.580 --> 00:03:39.780]   in the social technologies that can guide
[00:03:39.780 --> 00:03:42.700]   and bind and direct the physical technologies
[00:03:42.700 --> 00:03:44.120]   so that we are safe vessels
[00:03:44.120 --> 00:03:45.980]   for the amount of power we're getting.
[00:03:45.980 --> 00:03:50.820]   - Actually, "Hitchhiker's Guide" has a estimation
[00:03:50.820 --> 00:03:54.780]   about how much of a risk this particular thing poses
[00:03:54.780 --> 00:03:57.540]   to the rest of the galaxy.
[00:03:57.540 --> 00:04:00.020]   And I think, I forget what it was,
[00:04:00.020 --> 00:04:02.380]   I think it was medium or low.
[00:04:02.380 --> 00:04:05.860]   So their estimation would be that
[00:04:05.860 --> 00:04:08.540]   this species of ant-like creatures
[00:04:08.540 --> 00:04:10.220]   is not gonna survive long.
[00:04:10.220 --> 00:04:13.840]   There's ups and downs in terms of technological innovation.
[00:04:13.840 --> 00:04:16.180]   The fundamental nature of their behavior
[00:04:16.180 --> 00:04:18.940]   from a game theory perspective hasn't really changed.
[00:04:18.940 --> 00:04:21.420]   They have not learned in any fundamental way
[00:04:21.420 --> 00:04:26.120]   how to control and properly incentivize
[00:04:26.120 --> 00:04:30.220]   or properly do the mechanism design of games
[00:04:30.220 --> 00:04:32.660]   to ensure long-term survival.
[00:04:32.660 --> 00:04:35.660]   And then they move on to another planet.
[00:04:35.660 --> 00:04:38.980]   Do you think there is,
[00:04:38.980 --> 00:04:41.660]   in a more, slightly more serious question,
[00:04:41.660 --> 00:04:44.140]   do you think there's some number
[00:04:44.140 --> 00:04:46.720]   or perhaps a very, very large number
[00:04:46.720 --> 00:04:49.360]   of intelligent alien civilizations out there?
[00:04:49.360 --> 00:04:53.980]   - Yes, would be hard to think otherwise.
[00:04:53.980 --> 00:04:58.000]   I know, I think Postrom had a new article not that long ago
[00:04:58.000 --> 00:04:59.380]   on why that might not be the case,
[00:04:59.380 --> 00:05:01.760]   that the Drake equation might not be
[00:05:01.760 --> 00:05:03.780]   the kind of end story on it.
[00:05:03.780 --> 00:05:07.380]   But when I look at the total number of Kepler planets
[00:05:07.380 --> 00:05:09.720]   just that we're aware of just galactically,
[00:05:09.720 --> 00:05:13.780]   and also like when those life forms
[00:05:13.780 --> 00:05:15.080]   were discovered in Mono Lake
[00:05:15.080 --> 00:05:17.500]   that didn't have the same six primary atoms,
[00:05:17.500 --> 00:05:19.420]   I think it had arsenic replacing phosphorus
[00:05:19.420 --> 00:05:23.080]   as one of the primary aspects of its energy metabolism,
[00:05:23.080 --> 00:05:24.760]   we get to think about that the building blocks
[00:05:24.760 --> 00:05:26.300]   might be more different.
[00:05:26.300 --> 00:05:27.420]   So the physical constraints
[00:05:27.420 --> 00:05:30.660]   even that the planets have to have might be more different.
[00:05:30.660 --> 00:05:32.460]   It seems really unlikely,
[00:05:32.460 --> 00:05:36.860]   not to mention interesting things that we've observed
[00:05:36.860 --> 00:05:38.200]   that are still unexplained.
[00:05:38.200 --> 00:05:42.500]   As you've had guests on your show discussing Tic Tac and--
[00:05:42.500 --> 00:05:44.300]   - Oh, the ones that have visited.
[00:05:44.300 --> 00:05:45.140]   - Yeah.
[00:05:45.140 --> 00:05:46.820]   - Well, let's dive right into that.
[00:05:46.820 --> 00:05:51.820]   What do you make sense of the rich human psychology
[00:05:51.820 --> 00:05:55.920]   of there being hundreds of thousands,
[00:05:55.920 --> 00:05:58.820]   probably millions of witnesses of UFOs
[00:05:58.820 --> 00:06:00.580]   of different kinds on Earth,
[00:06:00.580 --> 00:06:05.100]   most of which I presume are conjured up by the human mind
[00:06:05.100 --> 00:06:07.180]   through the perceptual system.
[00:06:07.180 --> 00:06:09.140]   Some number might be true,
[00:06:09.140 --> 00:06:12.680]   some number might be reflective of actual physical objects,
[00:06:12.680 --> 00:06:17.080]   whether it's drones or testing military technology
[00:06:17.080 --> 00:06:20.980]   that's secret or other worldly technology.
[00:06:20.980 --> 00:06:22.300]   What do you make sense of all of that?
[00:06:22.300 --> 00:06:25.460]   Because it's gained quite a bit of popularity recently.
[00:06:25.460 --> 00:06:31.380]   There's some sense of which that's us humans being hopeful
[00:06:31.380 --> 00:06:38.380]   and dreaming of other worldly creatures
[00:06:38.380 --> 00:06:40.940]   as a way to escape the dreariness
[00:06:40.940 --> 00:06:43.180]   of the human condition.
[00:06:43.180 --> 00:06:47.280]   But in another sense, it really could be
[00:06:47.280 --> 00:06:49.560]   something truly exciting that science
[00:06:49.560 --> 00:06:53.520]   should turn its eye towards.
[00:06:53.520 --> 00:06:56.000]   So where do you place it?
[00:06:56.000 --> 00:06:57.460]   - Speaking of turning eye towards,
[00:06:57.460 --> 00:07:00.500]   this is one of those super fascinating,
[00:07:00.500 --> 00:07:02.480]   actually super consequential possibly,
[00:07:02.480 --> 00:07:05.240]   topics that I wish I had more time to study
[00:07:05.240 --> 00:07:06.280]   and just haven't allocated.
[00:07:06.280 --> 00:07:08.200]   So I don't have firm beliefs on this
[00:07:08.200 --> 00:07:09.580]   'cause I haven't got to study it as much as I want.
[00:07:09.580 --> 00:07:12.480]   So what I'm gonna say comes from a superficial assessment.
[00:07:12.480 --> 00:07:18.160]   While we know there are plenty of things
[00:07:18.160 --> 00:07:20.160]   that people thought of as UFO sightings
[00:07:20.160 --> 00:07:21.520]   that we can fully write off,
[00:07:21.520 --> 00:07:24.120]   we have other better explanations for them.
[00:07:24.120 --> 00:07:25.640]   What we're interested in is the ones
[00:07:25.640 --> 00:07:27.160]   that we don't have better explanations for
[00:07:27.160 --> 00:07:28.760]   and then not just immediately jumping
[00:07:28.760 --> 00:07:31.560]   to a theory of what it is,
[00:07:31.560 --> 00:07:33.160]   but holding it as unidentified
[00:07:33.160 --> 00:07:35.600]   and being curious and earnest.
[00:07:36.560 --> 00:07:39.900]   I think the Tic Tac one is quite interesting
[00:07:39.900 --> 00:07:42.180]   and made it in major media recently,
[00:07:42.180 --> 00:07:45.540]   but I don't know if you ever saw the disclosure project.
[00:07:45.540 --> 00:07:49.160]   A guy named Stephen Greer organized
[00:07:49.160 --> 00:07:53.660]   a bunch of mostly US military and some commercial flight,
[00:07:53.660 --> 00:07:57.460]   people who had direct observation
[00:07:57.460 --> 00:08:01.980]   and classified information disclosing it at a CNN briefing.
[00:08:01.980 --> 00:08:05.060]   And so you saw high-ranking generals,
[00:08:05.060 --> 00:08:07.380]   admirals, fighter pilots all describing things
[00:08:07.380 --> 00:08:10.660]   that they saw on radar with visual,
[00:08:10.660 --> 00:08:14.740]   with their own eyes or cameras,
[00:08:14.740 --> 00:08:17.420]   and also describing some phenomena
[00:08:17.420 --> 00:08:20.500]   that had some consistency across different people.
[00:08:20.500 --> 00:08:23.060]   And I find this interesting enough
[00:08:23.060 --> 00:08:25.560]   that I think it would be silly to just dismiss it.
[00:08:25.560 --> 00:08:30.320]   And specifically, we can ask the question,
[00:08:30.320 --> 00:08:31.860]   how much of it is natural phenomena,
[00:08:31.860 --> 00:08:34.260]   ball lightning or something like that?
[00:08:34.260 --> 00:08:35.900]   And this is why I'm more interested
[00:08:35.900 --> 00:08:38.940]   in what fighter pilots and astronauts
[00:08:38.940 --> 00:08:43.420]   and people who are trained in being able
[00:08:43.420 --> 00:08:46.260]   to identify flying objects
[00:08:46.260 --> 00:08:49.780]   and atmospheric phenomena have to say about it.
[00:08:49.780 --> 00:08:53.560]   I think the thing, then you could say,
[00:08:53.560 --> 00:08:57.220]   well, are they more advanced military craft?
[00:08:57.220 --> 00:08:59.140]   Is it some kind of human craft?
[00:08:59.140 --> 00:09:02.540]   The interesting thing that a number of them describe
[00:09:02.540 --> 00:09:06.220]   is something that's kind of like right angles at speed,
[00:09:06.220 --> 00:09:08.580]   or if not right angles, acute angles at speed,
[00:09:08.580 --> 00:09:11.220]   but something that looks like a different relationship
[00:09:11.220 --> 00:09:13.480]   to inertia than physics makes sense for us.
[00:09:13.480 --> 00:09:17.460]   I don't think that there are any human technologies
[00:09:17.460 --> 00:09:19.380]   that are doing that even in really deep
[00:09:19.380 --> 00:09:22.260]   underground black projects.
[00:09:22.260 --> 00:09:25.700]   Now, one could say, okay, well, could it be a hologram?
[00:09:25.700 --> 00:09:28.060]   Well, would it show up on radar if radar is also seeing it?
[00:09:28.060 --> 00:09:30.820]   And so I don't know.
[00:09:30.820 --> 00:09:32.220]   I think there's enough.
[00:09:32.220 --> 00:09:36.180]   I mean, and for that to be a massive coordinated PSYOP
[00:09:36.180 --> 00:09:40.980]   is as interesting and ridiculous in a way
[00:09:40.980 --> 00:09:43.500]   as the idea that it's UFOs
[00:09:43.500 --> 00:09:45.880]   from some extra planetary source.
[00:09:45.880 --> 00:09:48.460]   So it's up there on the interesting topics.
[00:09:48.460 --> 00:09:53.460]   - To me, if it is at all alien technology,
[00:09:53.460 --> 00:09:57.420]   it is the dumbest version of alien technologies.
[00:09:57.420 --> 00:09:58.460]   It's so far away.
[00:09:58.460 --> 00:10:02.920]   It's like the old, old crappy VHS tapes of alien technology.
[00:10:02.920 --> 00:10:05.620]   These are like crappy drones that just floated
[00:10:05.620 --> 00:10:09.500]   or even like space to the level of like space junk
[00:10:09.500 --> 00:10:14.160]   because it is so close to our human technology.
[00:10:14.160 --> 00:10:17.540]   We talk about it moves in ways that's unlike
[00:10:17.540 --> 00:10:18.700]   what we understand about physics,
[00:10:18.700 --> 00:10:23.700]   but it still has very similar kind of geometric notions
[00:10:23.700 --> 00:10:27.640]   and something that we humans can perceive with our eyes,
[00:10:27.640 --> 00:10:28.500]   all those kinds of things.
[00:10:28.500 --> 00:10:31.820]   I feel like alien technology most likely
[00:10:31.820 --> 00:10:35.660]   would be something that we would not be able to perceive,
[00:10:35.660 --> 00:10:36.700]   not because they're hiding,
[00:10:36.700 --> 00:10:38.820]   but because it's so far advanced
[00:10:38.820 --> 00:10:42.300]   that it would be much,
[00:10:42.300 --> 00:10:45.060]   it would be beyond the cognitive capabilities of us humans.
[00:10:45.060 --> 00:10:46.300]   Just as you were saying,
[00:10:46.300 --> 00:10:50.240]   as per your answer for aliens summarizing earth,
[00:10:50.240 --> 00:10:53.980]   the starting assumption
[00:10:53.980 --> 00:10:56.200]   is they have similar perception systems.
[00:10:56.200 --> 00:10:58.980]   They have similar cognitive capabilities
[00:10:58.980 --> 00:11:01.420]   and that very well may not be the case.
[00:11:01.420 --> 00:11:05.180]   Let me ask you about staying in aliens
[00:11:05.180 --> 00:11:06.900]   for just a little longer
[00:11:06.900 --> 00:11:09.180]   because I think it's a good transition
[00:11:09.180 --> 00:11:11.840]   in talking about governments and human societies.
[00:11:11.840 --> 00:11:19.320]   Do you think if a US government or any government
[00:11:19.320 --> 00:11:24.460]   was in possession of an alien spacecraft
[00:11:24.460 --> 00:11:28.240]   or of information related to alien spacecraft,
[00:11:28.240 --> 00:11:33.240]   they would have the capacity structurally?
[00:11:33.240 --> 00:11:38.720]   Would they have the processes?
[00:11:38.720 --> 00:11:42.720]   Would they be able to communicate
[00:11:42.720 --> 00:11:45.840]   that to the public effectively?
[00:11:45.840 --> 00:11:47.800]   Or would they keep it secret in a room
[00:11:47.800 --> 00:11:49.440]   and do nothing with it,
[00:11:49.440 --> 00:11:53.200]   both to try to preserve military secrets,
[00:11:53.200 --> 00:11:54.900]   but also because of the incompetence
[00:11:54.900 --> 00:11:59.060]   that's inherent to bureaucracies or either?
[00:11:59.060 --> 00:12:03.500]   - Well, we can certainly see
[00:12:03.500 --> 00:12:06.260]   when certain things become declassified
[00:12:06.260 --> 00:12:07.860]   25 or 50 years later,
[00:12:07.860 --> 00:12:09.140]   that there were things
[00:12:09.140 --> 00:12:11.500]   that the public might've wanted to know
[00:12:11.500 --> 00:12:14.340]   that were kept secret for a very long time
[00:12:14.340 --> 00:12:18.160]   for reasons of at least supposedly national security,
[00:12:18.160 --> 00:12:21.940]   which is also a nice source of plausible deniability
[00:12:21.940 --> 00:12:25.920]   for people covering their ass
[00:12:25.920 --> 00:12:28.040]   for doing things that would be problematic
[00:12:28.040 --> 00:12:30.560]   and other purposes.
[00:12:30.560 --> 00:12:37.960]   There's a scientist at Stanford
[00:12:37.960 --> 00:12:42.120]   who supposedly got some material
[00:12:42.120 --> 00:12:45.480]   that was recovered from Area 51 type area,
[00:12:45.480 --> 00:12:48.720]   did analysis on it using, I believe, electron microscopy
[00:12:48.720 --> 00:12:50.160]   and a couple other methods
[00:12:50.160 --> 00:12:54.080]   and came to the idea that it was a nanotech alloy
[00:12:54.080 --> 00:12:58.080]   that was something we didn't currently have the ability
[00:12:58.080 --> 00:12:59.780]   to do, was not naturally occurring.
[00:12:59.780 --> 00:13:03.000]   So I've heard some things.
[00:13:03.000 --> 00:13:03.840]   And again, like I said,
[00:13:03.840 --> 00:13:05.520]   I'm not gonna stand behind any of these
[00:13:05.520 --> 00:13:07.040]   'cause I haven't done the level of study
[00:13:07.040 --> 00:13:08.600]   to have high confidence.
[00:13:08.600 --> 00:13:14.460]   I think what you said also about
[00:13:14.460 --> 00:13:18.920]   would it be super low-tech alien craft,
[00:13:18.920 --> 00:13:22.120]   like would they necessarily move their atoms around in space
[00:13:22.120 --> 00:13:24.640]   or might they do something more interesting than that?
[00:13:24.640 --> 00:13:27.920]   Might they be able to have a different relationship
[00:13:27.920 --> 00:13:31.280]   to the concept of space or information or consciousness?
[00:13:31.280 --> 00:13:35.000]   One of the things that the craft supposedly do
[00:13:35.000 --> 00:13:37.840]   is not only accelerate and turn in a way
[00:13:37.840 --> 00:13:40.440]   that looks non-inertial, but also disappear.
[00:13:40.440 --> 00:13:42.000]   So there's a question as to,
[00:13:42.000 --> 00:13:44.880]   like the two are not necessarily mutually exclusive
[00:13:44.880 --> 00:13:46.640]   and it could be possible to,
[00:13:46.640 --> 00:13:48.700]   some people run a hypothesis
[00:13:48.700 --> 00:13:51.480]   that they create intentional amounts of exposure
[00:13:51.480 --> 00:13:54.100]   as an invitation of a particular kind.
[00:13:54.100 --> 00:13:57.000]   Who knows?
[00:13:57.000 --> 00:13:58.680]   Interesting field.
[00:13:58.680 --> 00:14:00.560]   - We tend to assume like SETI,
[00:14:00.560 --> 00:14:03.180]   that's listening out for aliens out there.
[00:14:03.180 --> 00:14:06.720]   I've just been recently reading more and more
[00:14:06.720 --> 00:14:08.240]   about gravitational waves
[00:14:08.240 --> 00:14:13.240]   and you have orbiting black holes that orbit each other.
[00:14:13.240 --> 00:14:17.060]   They generate ripples in space-time.
[00:14:17.680 --> 00:14:21.520]   For fun at night when I lay in bed,
[00:14:21.520 --> 00:14:23.920]   I think about what it would be like to ride those waves
[00:14:23.920 --> 00:14:27.320]   when they, not the low magnitude they are
[00:14:27.320 --> 00:14:28.360]   as when they reach Earth,
[00:14:28.360 --> 00:14:30.400]   but get closer to the black holes
[00:14:30.400 --> 00:14:32.820]   because it would basically be shrinking
[00:14:32.820 --> 00:14:37.820]   and expanding us in all dimensions, including time.
[00:14:37.820 --> 00:14:41.200]   So it's actually ripples through space-time
[00:14:41.200 --> 00:14:42.800]   that they generate.
[00:14:42.800 --> 00:14:46.360]   Why is it that you couldn't use that
[00:14:46.360 --> 00:14:48.160]   as it travels at the speed of light?
[00:14:48.160 --> 00:14:53.660]   Travels at a speed, which is a very weird thing to say
[00:14:53.660 --> 00:14:57.460]   when you're morphing space-time.
[00:14:57.460 --> 00:15:02.080]   You could argue it's faster than the speed of light.
[00:15:02.080 --> 00:15:04.500]   So if you're able to communicate by,
[00:15:04.500 --> 00:15:08.580]   to summon enough energy to generate black holes
[00:15:08.580 --> 00:15:12.440]   and to force them to orbit each other,
[00:15:12.440 --> 00:15:17.440]   why not travel as the ripples in space-time,
[00:15:17.440 --> 00:15:19.600]   whatever the hell that means,
[00:15:19.600 --> 00:15:21.400]   somehow combined with wormholes.
[00:15:21.400 --> 00:15:23.320]   So if you're able to communicate through,
[00:15:23.320 --> 00:15:26.940]   like we don't think of gravitational waves
[00:15:26.940 --> 00:15:28.600]   as something you can communicate with
[00:15:28.600 --> 00:15:31.520]   because the radio will have to be
[00:15:31.520 --> 00:15:35.880]   a very large size and very dense, but perhaps that's it.
[00:15:35.880 --> 00:15:39.120]   Perhaps that's one way to communicate.
[00:15:39.120 --> 00:15:40.800]   It's a very effective way.
[00:15:40.800 --> 00:15:43.640]   And that would explain,
[00:15:43.640 --> 00:15:46.960]   we wouldn't even be able to make sense of that,
[00:15:46.960 --> 00:15:50.360]   of the physics that results in an alien species
[00:15:50.360 --> 00:15:53.380]   that's able to control gravity at that scale.
[00:15:53.380 --> 00:15:56.760]   - I think you just jumped up the Kardashev scale so far.
[00:15:56.760 --> 00:15:59.080]   You're not just harnessing the power of a star,
[00:15:59.080 --> 00:16:02.660]   but harnessing the power of mutually rotating black holes.
[00:16:02.660 --> 00:16:08.900]   That's way above my physics pay grade to think about,
[00:16:08.900 --> 00:16:13.900]   including even non-rotating black hole versions
[00:16:13.900 --> 00:16:15.600]   of transwarp travel.
[00:16:15.600 --> 00:16:21.560]   I think, you can talk with Eric more about that.
[00:16:21.560 --> 00:16:23.800]   I think he has better ideas on it than I do.
[00:16:23.800 --> 00:16:26.200]   My hope for the future of humanity
[00:16:26.200 --> 00:16:29.280]   mostly does not rest in the near term
[00:16:29.280 --> 00:16:32.840]   on our ability to get to other habitable planets in time.
[00:16:32.840 --> 00:16:34.560]   - And even more than that,
[00:16:34.560 --> 00:16:36.280]   in the list of possible solutions
[00:16:36.280 --> 00:16:39.340]   of how to improve human civilization,
[00:16:39.340 --> 00:16:42.780]   orbiting black holes is not on the first page for you.
[00:16:42.780 --> 00:16:44.860]   - Not on the first page.
[00:16:44.860 --> 00:16:46.500]   - Okay, I bet you did not expect us
[00:16:46.500 --> 00:16:48.960]   to start this conversation here.
[00:16:48.960 --> 00:16:50.560]   But I'm glad the places it went.
[00:16:50.560 --> 00:16:55.460]   I am excited on a much smaller scale
[00:16:55.460 --> 00:16:59.740]   of Mars, Europa, or Titan,
[00:16:59.740 --> 00:17:04.740]   Venus potentially having very bacteria-like life forms.
[00:17:05.660 --> 00:17:10.620]   Just on a small human level,
[00:17:10.620 --> 00:17:13.220]   it's a little bit scary, but mostly really exciting
[00:17:13.220 --> 00:17:15.880]   that there might be life elsewhere,
[00:17:15.880 --> 00:17:17.960]   in the volcanoes, in the oceans,
[00:17:17.960 --> 00:17:22.920]   all around us, teeming, having little societies.
[00:17:22.920 --> 00:17:26.680]   And whether there's properties about that kind of life,
[00:17:26.680 --> 00:17:28.380]   that's somehow different than ours.
[00:17:28.380 --> 00:17:29.660]   I don't know what would be more exciting
[00:17:29.660 --> 00:17:34.040]   if those colonies of single-cell type organisms,
[00:17:35.140 --> 00:17:36.420]   what would be more exciting,
[00:17:36.420 --> 00:17:39.260]   if they're different or if they're the same?
[00:17:39.260 --> 00:17:41.780]   If they're the same, that means
[00:17:41.780 --> 00:17:45.420]   through the rest of the universe,
[00:17:45.420 --> 00:17:47.420]   there's life forms like us,
[00:17:47.420 --> 00:17:51.240]   something like us everywhere.
[00:17:51.240 --> 00:17:53.900]   If they're different, that's also really exciting
[00:17:53.900 --> 00:17:58.860]   'cause there's life forms everywhere that are not like us.
[00:17:58.860 --> 00:18:01.860]   That's a little bit scary.
[00:18:01.860 --> 00:18:03.620]   - I don't know what's scarier, actually.
[00:18:03.620 --> 00:18:04.660]   (both laughing)
[00:18:04.660 --> 00:18:05.620]   I think-- - It's both scary
[00:18:05.620 --> 00:18:07.700]   and exciting no matter what, right?
[00:18:07.700 --> 00:18:09.940]   - The idea that they could be very different
[00:18:09.940 --> 00:18:11.420]   is philosophically very interesting
[00:18:11.420 --> 00:18:14.780]   for us to open our aperture on what life and consciousness
[00:18:14.780 --> 00:18:18.220]   and self-replicating possibilities could look like.
[00:18:18.220 --> 00:18:21.500]   The question on are they different or the same,
[00:18:21.500 --> 00:18:22.660]   obviously there's lots of life here
[00:18:22.660 --> 00:18:25.560]   that is the same in some ways and different in other ways.
[00:18:25.560 --> 00:18:29.300]   When you take the thing that we call an invasive species,
[00:18:29.300 --> 00:18:30.980]   it's something that's still pretty the same,
[00:18:30.980 --> 00:18:32.760]   hydrocarbon-based thing,
[00:18:32.800 --> 00:18:35.560]   but co-evolved with co-selective pressures
[00:18:35.560 --> 00:18:36.400]   in a certain environment,
[00:18:36.400 --> 00:18:37.440]   we move it to another environment,
[00:18:37.440 --> 00:18:39.660]   it might be devastating to that whole ecosystem
[00:18:39.660 --> 00:18:40.980]   'cause it's just different enough
[00:18:40.980 --> 00:18:44.160]   that it messes up the self-stabilizing dynamics
[00:18:44.160 --> 00:18:45.000]   of that ecosystem.
[00:18:45.000 --> 00:18:50.000]   So the question of would they be different
[00:18:50.000 --> 00:18:53.020]   in ways where we could still figure out a way
[00:18:53.020 --> 00:18:56.980]   to inhabit a biosphere together or fundamentally not,
[00:18:56.980 --> 00:19:00.480]   fundamentally the nature of how they operate
[00:19:00.480 --> 00:19:01.820]   and the nature of how we operate
[00:19:01.820 --> 00:19:05.020]   would be incommensurable is a deep question.
[00:19:05.020 --> 00:19:09.740]   - Well, we offline talked about mimetic theory, right?
[00:19:09.740 --> 00:19:12.460]   It seems like if there were sufficiently different
[00:19:12.460 --> 00:19:14.580]   where we would not even,
[00:19:14.580 --> 00:19:17.660]   we can coexist on different planes,
[00:19:17.660 --> 00:19:19.540]   it seems like a good thing.
[00:19:19.540 --> 00:19:23.200]   If we're close enough together to where we'd be competing,
[00:19:23.200 --> 00:19:26.220]   then you're getting into the world of viruses and pathogens
[00:19:26.220 --> 00:19:28.700]   and all those kinds of things to where we would,
[00:19:29.820 --> 00:19:32.180]   one of us would die off quickly
[00:19:32.180 --> 00:19:36.100]   through basically mass murder without--
[00:19:36.100 --> 00:19:38.780]   - Even accidentally.
[00:19:38.780 --> 00:19:40.060]   - Even accidentally.
[00:19:40.060 --> 00:19:42.180]   - If we just had a self-replicating,
[00:19:42.180 --> 00:19:43.980]   single-celled kind of creature
[00:19:43.980 --> 00:19:48.980]   that happened to not work well
[00:19:48.980 --> 00:19:50.900]   for the hydrocarbon life that was here,
[00:19:50.900 --> 00:19:54.140]   that got introduced because it either output something
[00:19:54.140 --> 00:19:56.620]   that was toxic or utilized up the same resource too quickly
[00:19:56.620 --> 00:19:59.980]   and it just replicated faster and mutated faster.
[00:19:59.980 --> 00:20:03.620]   It wouldn't be a mimetic theory,
[00:20:03.620 --> 00:20:04.940]   conflict theory kind of harm,
[00:20:04.940 --> 00:20:08.360]   it would just be a von Neumann machine,
[00:20:08.360 --> 00:20:09.820]   a self-replicating machine
[00:20:09.820 --> 00:20:12.060]   that was fundamentally incompatible
[00:20:12.060 --> 00:20:14.460]   with these kinds of self-replicating systems
[00:20:14.460 --> 00:20:16.180]   with faster OODA loops.
[00:20:16.180 --> 00:20:19.900]   - For one final time, putting your alien/god hat on
[00:20:19.900 --> 00:20:22.780]   and you look at human civilization,
[00:20:24.260 --> 00:20:28.100]   do you think about the 7.8 billion people on Earth
[00:20:28.100 --> 00:20:30.420]   as individual little creatures,
[00:20:30.420 --> 00:20:32.500]   individual little organisms,
[00:20:32.500 --> 00:20:36.500]   or do you think of us as one organism
[00:20:36.500 --> 00:20:40.240]   with a collective intelligence?
[00:20:40.240 --> 00:20:45.460]   What's the proper framework through which to analyze it,
[00:20:45.460 --> 00:20:46.540]   again, as an alien?
[00:20:46.540 --> 00:20:47.940]   - So that I know where you're coming from
[00:20:47.940 --> 00:20:50.420]   when you have asked the question the same way
[00:20:50.420 --> 00:20:51.900]   before the Industrial Revolution,
[00:20:51.900 --> 00:20:53.260]   before the Agricultural Revolution
[00:20:53.260 --> 00:20:54.540]   when there were half a billion people
[00:20:54.540 --> 00:20:56.640]   and no telecommunications connecting them?
[00:20:56.640 --> 00:21:01.860]   - I would indeed ask the question the same way,
[00:21:01.860 --> 00:21:06.660]   but I would be less confident about your conclusions.
[00:21:06.660 --> 00:21:11.020]   It would be an actually more interesting way
[00:21:11.020 --> 00:21:12.580]   to ask the question at that time,
[00:21:12.580 --> 00:21:15.300]   but I would nevertheless ask it the same way, yes.
[00:21:15.300 --> 00:21:17.980]   - Well, let's go back further and smaller then.
[00:21:17.980 --> 00:21:19.780]   Rather than just a single human
[00:21:19.780 --> 00:21:21.260]   or the entire human species,
[00:21:21.260 --> 00:21:26.260]   let's look at a relatively isolated tribe.
[00:21:26.260 --> 00:21:29.140]   In the relatively isolated,
[00:21:29.140 --> 00:21:34.020]   probably sub Dunbar number, sub 150 people tribe,
[00:21:34.020 --> 00:21:37.260]   do I look at that as one entity
[00:21:37.260 --> 00:21:38.860]   where evolution is selecting for it
[00:21:38.860 --> 00:21:39.940]   based on group selection,
[00:21:39.940 --> 00:21:42.540]   or do I think of it as 150 individuals
[00:21:42.540 --> 00:21:45.140]   that are interacting in some way?
[00:21:45.140 --> 00:21:50.020]   Well, could those individuals exist without the group?
[00:21:50.020 --> 00:21:50.860]   No.
[00:21:51.820 --> 00:21:56.100]   The evolutionary adaptiveness of humans
[00:21:56.100 --> 00:21:59.940]   was involved critically group selection
[00:21:59.940 --> 00:22:01.980]   and individual humans alone
[00:22:01.980 --> 00:22:04.620]   trying to figure out stone tools and protection
[00:22:04.620 --> 00:22:07.660]   and whatever aren't what was selected for.
[00:22:07.660 --> 00:22:13.540]   And so I think the or is the wrong frame.
[00:22:13.540 --> 00:22:18.420]   I think it's individuals are affecting
[00:22:18.420 --> 00:22:20.140]   the group that they're a part of.
[00:22:20.140 --> 00:22:22.020]   They're also dependent upon
[00:22:22.020 --> 00:22:24.900]   and being affected by the group that they're part of.
[00:22:24.900 --> 00:22:27.300]   And so this now starts to get in deep
[00:22:27.300 --> 00:22:28.900]   into political theories also,
[00:22:28.900 --> 00:22:32.340]   which is theories that orient towards the collective
[00:22:32.340 --> 00:22:33.180]   at different scales,
[00:22:33.180 --> 00:22:34.900]   whether a tribal scale or an empire
[00:22:34.900 --> 00:22:36.380]   or a nation state or something.
[00:22:36.380 --> 00:22:38.660]   And ones that orient towards the individual liberalism
[00:22:38.660 --> 00:22:39.980]   and stuff like that.
[00:22:39.980 --> 00:22:41.860]   And I think there's very obvious failure modes
[00:22:41.860 --> 00:22:43.180]   on both sides.
[00:22:43.180 --> 00:22:45.260]   And so the relationship between them
[00:22:45.260 --> 00:22:47.020]   is more interesting to me than either of them.
[00:22:47.020 --> 00:22:48.540]   The relationship between the individual
[00:22:48.540 --> 00:22:49.540]   and the collective and the question
[00:22:49.540 --> 00:22:52.060]   around how to have a virtuous process between those.
[00:22:52.060 --> 00:22:54.420]   So a good social system would be one
[00:22:54.420 --> 00:22:56.060]   where the organism of the individual
[00:22:56.060 --> 00:22:59.740]   and the organism of the group of individuals
[00:22:59.740 --> 00:23:01.940]   is they're both synergistic to each other.
[00:23:01.940 --> 00:23:03.260]   So what is best for the individuals
[00:23:03.260 --> 00:23:05.700]   and what's best for the whole is aligned.
[00:23:05.700 --> 00:23:08.940]   - But there is nevertheless an individual.
[00:23:08.940 --> 00:23:11.820]   They're not, it's a matter of degrees, I suppose.
[00:23:11.820 --> 00:23:16.820]   But what defines a human more?
[00:23:18.740 --> 00:23:23.740]   The social network within which they've been brought up
[00:23:23.740 --> 00:23:27.340]   through which they've developed their intelligence
[00:23:27.340 --> 00:23:30.880]   or is it their own sovereign individual self?
[00:23:30.880 --> 00:23:35.280]   Like what's your intuition of how much,
[00:23:35.280 --> 00:23:38.340]   not just for evolutionary survival,
[00:23:38.340 --> 00:23:41.100]   but as intellectual beings,
[00:23:41.100 --> 00:23:44.460]   how much do we need others for our development?
[00:23:44.460 --> 00:23:48.500]   - Yeah, I think we have a weird sense of this today
[00:23:48.500 --> 00:23:53.340]   relative to most previous periods of sapien history.
[00:23:53.340 --> 00:23:56.900]   I think the vast majority of sapien history is tribal.
[00:23:56.900 --> 00:24:00.580]   Like depending upon your early human model,
[00:24:00.580 --> 00:24:05.500]   two or 300,000 years of Homo sapiens in little tribes
[00:24:05.500 --> 00:24:08.420]   where they depended upon that tribe for survival
[00:24:08.420 --> 00:24:12.820]   and excommunication from the tribe was fatal.
[00:24:12.820 --> 00:24:16.120]   I think they, and our whole evolutionary genetic history
[00:24:16.120 --> 00:24:17.020]   is in that environment.
[00:24:17.020 --> 00:24:18.300]   And the amount of time we've been out of it
[00:24:18.300 --> 00:24:20.660]   is relatively so tiny.
[00:24:20.660 --> 00:24:24.460]   And then we still depended upon extended families
[00:24:24.460 --> 00:24:26.100]   and local communities more.
[00:24:26.100 --> 00:24:28.980]   And the big kind of giant market complex
[00:24:28.980 --> 00:24:33.020]   where I can provide something to the market to get money,
[00:24:33.020 --> 00:24:34.440]   to be able to get other things from the market
[00:24:34.440 --> 00:24:35.900]   where it seems like I don't need anyone.
[00:24:35.900 --> 00:24:38.140]   It's almost like disintermediating our sense of need,
[00:24:38.140 --> 00:24:43.140]   even though your and my ability to talk to each other
[00:24:43.140 --> 00:24:45.900]   using these mics and the phones that we coordinated on
[00:24:45.900 --> 00:24:48.060]   took millions of people over six continents
[00:24:48.060 --> 00:24:49.540]   to be able to run the supply chains
[00:24:49.540 --> 00:24:50.860]   that made all the stuff that we depend on,
[00:24:50.860 --> 00:24:52.380]   but we don't notice that we depend upon them.
[00:24:52.380 --> 00:24:53.540]   They all seem fungible.
[00:24:53.540 --> 00:24:58.140]   If you take a baby,
[00:24:58.140 --> 00:25:00.260]   obviously that you didn't even get to a baby without a mom.
[00:25:00.260 --> 00:25:02.560]   Was it dependent, we depended upon each other, right?
[00:25:02.560 --> 00:25:04.680]   Without two parents at minimum
[00:25:04.680 --> 00:25:06.480]   and they depended upon other people.
[00:25:06.480 --> 00:25:09.780]   But if we take that baby and we put it out in the wild,
[00:25:09.780 --> 00:25:11.320]   it obviously dies.
[00:25:11.320 --> 00:25:13.220]   So if we let it grow up for a little while,
[00:25:13.220 --> 00:25:14.220]   the minimum amount of time
[00:25:14.220 --> 00:25:15.660]   where it starts to have some autonomy
[00:25:15.660 --> 00:25:16.780]   and then we put it out in the wild
[00:25:16.780 --> 00:25:19.180]   and this has happened a few times,
[00:25:19.180 --> 00:25:20.460]   it doesn't learn language
[00:25:20.460 --> 00:25:26.660]   and it doesn't learn the small motor articulation
[00:25:26.660 --> 00:25:27.740]   that we learn.
[00:25:27.740 --> 00:25:31.180]   It doesn't learn the type of consciousness
[00:25:31.180 --> 00:25:33.220]   that we end up having that is socialized.
[00:25:33.220 --> 00:25:37.260]   So I think we take for granted
[00:25:37.260 --> 00:25:39.440]   how much conditioning affects us.
[00:25:39.440 --> 00:25:46.180]   - Is it possible that it affects basically 90% of us
[00:25:46.180 --> 00:25:50.020]   or basically 99.9 or maybe the whole thing?
[00:25:50.020 --> 00:25:52.580]   The whole thing is the connection between us humans
[00:25:52.580 --> 00:25:56.420]   and that we're no better than apes
[00:25:56.420 --> 00:25:58.980]   without our human connections.
[00:25:58.980 --> 00:26:01.500]   Because thinking of it that way
[00:26:01.500 --> 00:26:05.580]   forces us to think very differently about human society
[00:26:05.580 --> 00:26:07.580]   and how to progress forward
[00:26:07.580 --> 00:26:09.940]   if the connections are fundamental.
[00:26:09.940 --> 00:26:12.900]   - I just have to object to the no better than apes
[00:26:12.900 --> 00:26:15.060]   'cause better here, I think you mean a specific thing
[00:26:15.060 --> 00:26:16.260]   which means have capacities
[00:26:16.260 --> 00:26:17.340]   that are fundamentally different
[00:26:17.340 --> 00:26:19.880]   than I think apes also depend upon troops.
[00:26:19.880 --> 00:26:26.540]   And I think the idea of humans as better than nature
[00:26:26.540 --> 00:26:29.140]   in some kind of ethical sense
[00:26:29.140 --> 00:26:30.300]   ends up having heaps of problems.
[00:26:30.300 --> 00:26:32.340]   We'll table that, we can come back to it.
[00:26:32.340 --> 00:26:35.380]   But when we say what is unique about homo sapien capacity
[00:26:35.380 --> 00:26:36.600]   relative to the other animals
[00:26:36.600 --> 00:26:39.700]   we currently inhabit the biosphere with?
[00:26:39.700 --> 00:26:40.820]   And I'm saying it that way
[00:26:40.820 --> 00:26:42.500]   because there were other early hominids
[00:26:42.500 --> 00:26:44.200]   that had some of these capacities.
[00:26:45.200 --> 00:26:50.040]   We believe our tool creation and our language creation
[00:26:50.040 --> 00:26:52.120]   and our coordination are all kind of the results
[00:26:52.120 --> 00:26:55.040]   of a certain type of capacity for abstraction.
[00:26:55.040 --> 00:26:58.080]   And other animals will use tools
[00:26:58.080 --> 00:26:59.760]   but they don't evolve the tools they use.
[00:26:59.760 --> 00:27:01.360]   They keep using the same types of tools
[00:27:01.360 --> 00:27:03.280]   that they basically can find.
[00:27:03.280 --> 00:27:06.760]   So a chimp will notice that a rock can cut a vine
[00:27:06.760 --> 00:27:07.600]   that it wants to.
[00:27:07.600 --> 00:27:09.240]   And it'll even notice that a sharper rock
[00:27:09.240 --> 00:27:10.080]   will cut it better.
[00:27:10.080 --> 00:27:12.400]   And experientially it'll use the sharper rock.
[00:27:12.400 --> 00:27:13.560]   And if you even give it a knife
[00:27:13.560 --> 00:27:14.600]   it'll probably use the knife
[00:27:14.600 --> 00:27:17.040]   'cause it's experiencing the effectiveness.
[00:27:17.040 --> 00:27:19.140]   But it doesn't make stone tools
[00:27:19.140 --> 00:27:21.240]   because that requires understanding
[00:27:21.240 --> 00:27:22.880]   why one is sharper than the other.
[00:27:22.880 --> 00:27:25.920]   What is the abstract principle called sharpness
[00:27:25.920 --> 00:27:28.200]   to then be able to invent a sharper thing?
[00:27:28.200 --> 00:27:30.780]   That same abstraction makes language
[00:27:30.780 --> 00:27:33.640]   and the ability for abstract representation
[00:27:33.640 --> 00:27:35.880]   which makes the ability to coordinate
[00:27:35.880 --> 00:27:38.880]   in a more advanced set of ways.
[00:27:38.880 --> 00:27:41.320]   So I do think our ability to coordinate with each other
[00:27:41.320 --> 00:27:43.440]   is pretty fundamental to the selection
[00:27:43.440 --> 00:27:45.640]   of what we are as a species.
[00:27:45.640 --> 00:27:49.320]   - I wonder if that coordination, that connection
[00:27:49.320 --> 00:27:52.080]   is actually the thing that gives birth to consciousness.
[00:27:52.080 --> 00:27:55.960]   That gives birth to, well let's start with self-awareness.
[00:27:55.960 --> 00:27:57.200]   - More like theory of mind.
[00:27:57.200 --> 00:27:58.640]   - Theory of mind, yeah.
[00:27:58.640 --> 00:28:01.560]   I mean I suppose there's experiments
[00:28:01.560 --> 00:28:03.040]   that show that there's other mammals
[00:28:03.040 --> 00:28:05.480]   that have a very crude theory of mind.
[00:28:05.480 --> 00:28:08.240]   I'm not sure, maybe dogs, something like that.
[00:28:08.240 --> 00:28:09.920]   But actually dogs probably has to do with
[00:28:09.920 --> 00:28:12.400]   that they co-evolved with humans.
[00:28:12.400 --> 00:28:15.120]   See it'd be interesting if that theory of mind
[00:28:15.120 --> 00:28:20.120]   is what leads to consciousness in the way we think about it.
[00:28:20.120 --> 00:28:23.560]   Is the richness of the subjective experience
[00:28:23.560 --> 00:28:24.920]   that is consciousness.
[00:28:24.920 --> 00:28:28.400]   I have an inkling sense that that only exists
[00:28:28.400 --> 00:28:30.880]   because we're social creatures.
[00:28:30.880 --> 00:28:34.880]   That doesn't come with the hardware and the software
[00:28:34.880 --> 00:28:36.400]   in the beginning.
[00:28:36.400 --> 00:28:41.400]   That's learned as an effective tool
[00:28:41.400 --> 00:28:43.160]   for communication almost.
[00:28:43.160 --> 00:28:48.200]   I think we think that consciousness is fundamental.
[00:28:48.200 --> 00:28:52.480]   Maybe it's not.
[00:28:52.480 --> 00:28:57.640]   A bunch of folks kind of criticize the idea
[00:28:57.640 --> 00:29:00.440]   that the illusion of consciousness is consciousness.
[00:29:00.440 --> 00:29:03.240]   That it is just a facade we use
[00:29:03.240 --> 00:29:08.240]   to help us construct theories of mind.
[00:29:08.240 --> 00:29:10.120]   You almost put yourself in the world
[00:29:10.120 --> 00:29:12.040]   as a subjective being.
[00:29:12.040 --> 00:29:14.800]   And that experience, you want to richly experience it
[00:29:14.800 --> 00:29:16.440]   as an individual person
[00:29:16.440 --> 00:29:20.000]   so that I could empathize with your experience.
[00:29:20.000 --> 00:29:22.420]   I find that notion compelling.
[00:29:22.420 --> 00:29:25.780]   Mostly because it allows you to then create robots
[00:29:25.780 --> 00:29:30.300]   that become conscious not by being quote unquote conscious
[00:29:30.300 --> 00:29:34.160]   but by just learning to fake it 'til they make it.
[00:29:34.160 --> 00:29:39.480]   Present a facade of consciousness
[00:29:39.480 --> 00:29:43.640]   with the task of making that facade
[00:29:43.640 --> 00:29:45.840]   very convincing to us humans
[00:29:45.840 --> 00:29:48.160]   and thereby it will become conscious.
[00:29:48.160 --> 00:29:51.640]   I have a sense that in some way
[00:29:51.640 --> 00:29:53.320]   that will make them conscious
[00:29:53.320 --> 00:29:56.300]   if they're sufficiently convincing to humans.
[00:29:58.780 --> 00:30:00.380]   Is there some element of that
[00:30:00.380 --> 00:30:05.100]   that you find convincing?
[00:30:05.100 --> 00:30:09.420]   This is a much harder set of questions
[00:30:09.420 --> 00:30:10.900]   and deep end of the pool
[00:30:10.900 --> 00:30:12.940]   than starting with the aliens was.
[00:30:12.940 --> 00:30:16.860]   We went from aliens to consciousness.
[00:30:16.860 --> 00:30:21.140]   This is not the trajectory I was expecting, nor you.
[00:30:21.140 --> 00:30:23.820]   But let us walk a while.
[00:30:23.820 --> 00:30:24.900]   We can walk a while
[00:30:24.900 --> 00:30:26.980]   and I don't think we will do it justice.
[00:30:26.980 --> 00:30:30.720]   So what do we mean by consciousness
[00:30:30.720 --> 00:30:34.240]   versus conscious self-reflective awareness?
[00:30:34.240 --> 00:30:38.060]   What do we mean by awareness, qualia, theory of mind?
[00:30:38.060 --> 00:30:39.680]   There's a lot of terms that we think of
[00:30:39.680 --> 00:30:41.620]   as slightly different things
[00:30:41.620 --> 00:30:45.740]   and subjectivity, first person.
[00:30:45.740 --> 00:30:50.080]   I don't remember exactly the quote
[00:30:50.080 --> 00:30:52.300]   but I remember when reading
[00:30:52.300 --> 00:30:54.040]   when Sam Harris wrote the book "Free Will"
[00:30:54.040 --> 00:30:56.280]   and then Dennett critiqued it.
[00:30:56.280 --> 00:30:58.080]   And then there was some writing back and forth
[00:30:58.080 --> 00:30:59.160]   between the two
[00:30:59.160 --> 00:31:02.060]   because normally they're on the same side
[00:31:02.060 --> 00:31:05.960]   of kind of arguing for critical thinking
[00:31:05.960 --> 00:31:08.560]   and logical fallacies and philosophy of science
[00:31:08.560 --> 00:31:11.100]   against supernatural ideas.
[00:31:11.100 --> 00:31:14.200]   And here Dennett believed
[00:31:14.200 --> 00:31:15.600]   there is something like free will.
[00:31:15.600 --> 00:31:17.740]   He is a determinist compatibilist
[00:31:17.740 --> 00:31:21.240]   but no consciousness and radical limitivist.
[00:31:21.240 --> 00:31:23.220]   And Sam was saying, no, there is consciousness
[00:31:23.220 --> 00:31:24.240]   but there's no free will.
[00:31:24.240 --> 00:31:26.600]   And that's like the most fundamental kinds
[00:31:26.600 --> 00:31:28.940]   of axiomatic senses they disagreed on
[00:31:28.940 --> 00:31:29.780]   but neither of them could say
[00:31:29.780 --> 00:31:30.880]   it was 'cause the other one didn't understand
[00:31:30.880 --> 00:31:33.400]   the philosophy of science or logical fallacies.
[00:31:33.400 --> 00:31:35.400]   And they kind of spoke past each other.
[00:31:35.400 --> 00:31:36.800]   And at the end, if I remember correctly,
[00:31:36.800 --> 00:31:39.360]   Sam said something that I thought was quite insightful
[00:31:39.360 --> 00:31:42.200]   which was to the effect of, it seems
[00:31:42.200 --> 00:31:43.760]   'cause they weren't making any progress
[00:31:43.760 --> 00:31:45.100]   in shared understanding.
[00:31:45.100 --> 00:31:48.520]   It seems that we simply have different intuitions about this.
[00:31:48.520 --> 00:31:52.360]   And what you could see was that
[00:31:52.360 --> 00:31:54.280]   what the words meant, right?
[00:31:54.280 --> 00:31:57.400]   At the level of symbol grounding might be quite different.
[00:31:57.400 --> 00:32:01.640]   One of them might've had deeply different
[00:32:01.640 --> 00:32:04.180]   enough life experiences that what is being referenced
[00:32:04.180 --> 00:32:05.760]   and then also different associations
[00:32:05.760 --> 00:32:06.640]   of what the words mean.
[00:32:06.640 --> 00:32:09.280]   This is why when trying to address these things
[00:32:09.280 --> 00:32:10.920]   Charles Sanders Peirce said,
[00:32:10.920 --> 00:32:13.080]   "The first philosophy has to be semiotics
[00:32:13.080 --> 00:32:14.900]   "because if you don't get semiotics right,
[00:32:14.900 --> 00:32:17.680]   "we end up importing different ideas and bad ideas
[00:32:17.680 --> 00:32:20.240]   "right into the nature of the language that we're using."
[00:32:20.240 --> 00:32:21.840]   And then it's very hard to do epistemology
[00:32:21.840 --> 00:32:22.920]   or ontology together.
[00:32:22.920 --> 00:32:25.920]   So I'm saying this to say
[00:32:25.920 --> 00:32:27.400]   why I don't think we're gonna get very far
[00:32:27.400 --> 00:32:30.280]   is I think we would have to go very slowly
[00:32:30.280 --> 00:32:32.820]   in terms of defining what we mean by words
[00:32:32.820 --> 00:32:34.760]   and fundamental concepts.
[00:32:34.760 --> 00:32:38.280]   - Well, and also allowing our minds to drift together
[00:32:38.280 --> 00:32:42.840]   for a time so that our definitions of these terms align.
[00:32:42.840 --> 00:32:47.840]   I think there's a beauty that some people enjoy with Sam
[00:32:49.460 --> 00:32:54.460]   that he is quite stubborn on his definitions of terms
[00:32:54.460 --> 00:32:59.540]   without often clearly revealing that definition.
[00:32:59.540 --> 00:33:01.460]   So in his mind, he can, like,
[00:33:01.460 --> 00:33:03.620]   you could sense that he can deeply understand
[00:33:03.620 --> 00:33:06.500]   what he means exactly by a term
[00:33:06.500 --> 00:33:08.180]   like free will and consciousness.
[00:33:08.180 --> 00:33:09.020]   And you're right.
[00:33:09.020 --> 00:33:12.460]   He's very specific in fascinating ways
[00:33:12.460 --> 00:33:17.320]   that not only does he think that free will is an illusion,
[00:33:18.420 --> 00:33:21.820]   he thinks he's able, not thinks, he says
[00:33:21.820 --> 00:33:24.020]   he's able to just remove himself
[00:33:24.020 --> 00:33:26.020]   from the experience of free will
[00:33:26.020 --> 00:33:31.020]   and just be like for minutes at a time, hours at a time,
[00:33:31.020 --> 00:33:35.700]   like really experience as if he has no free will.
[00:33:35.700 --> 00:33:39.480]   Like he's a leaf flowing down the river.
[00:33:39.480 --> 00:33:43.020]   And given that,
[00:33:43.020 --> 00:33:45.820]   he's very sure that consciousness is fundamental.
[00:33:45.820 --> 00:33:48.300]   So here's this conscious leaf
[00:33:48.300 --> 00:33:51.500]   that's subjectively experiencing the floating
[00:33:51.500 --> 00:33:54.260]   and yet has no ability to control
[00:33:54.260 --> 00:33:56.660]   and make any decisions for itself.
[00:33:56.660 --> 00:34:01.300]   It's only the decisions have all been made.
[00:34:01.300 --> 00:34:04.440]   There's some aspect to which the terminology there
[00:34:04.440 --> 00:34:06.500]   perhaps is the problem.
[00:34:06.500 --> 00:34:09.020]   - So that's a particular kind of meditative experience.
[00:34:09.020 --> 00:34:11.940]   And the people in the Vedantic tradition
[00:34:11.940 --> 00:34:13.620]   and some of the Buddhist traditions
[00:34:13.620 --> 00:34:15.820]   thousands of years ago described similar experiences
[00:34:15.820 --> 00:34:17.180]   and somewhat similar conclusions,
[00:34:17.180 --> 00:34:18.380]   some slightly different.
[00:34:18.380 --> 00:34:23.780]   There are other types of phenomenal experience
[00:34:23.780 --> 00:34:28.660]   that are the phenomenal experience of pure agency.
[00:34:28.660 --> 00:34:31.780]   And like the Catholic theologian,
[00:34:31.780 --> 00:34:33.020]   but evolutionary theorist,
[00:34:33.020 --> 00:34:35.300]   Tarek Deshardan describes this.
[00:34:35.300 --> 00:34:38.980]   And that rather than a creator agent God in the beginning,
[00:34:38.980 --> 00:34:41.340]   there's a creative impulse or a creative process.
[00:34:41.340 --> 00:34:43.740]   And he would go into a type of meditation
[00:34:43.740 --> 00:34:45.380]   that identified as the pure essence
[00:34:45.380 --> 00:34:47.080]   of that kind of creative process.
[00:34:47.080 --> 00:34:53.140]   And I think the types of experiences we've had,
[00:34:53.140 --> 00:34:55.860]   and then one, the types of experience we've had
[00:34:55.860 --> 00:34:58.600]   make a big deal to the nature of how we do symbol grounding.
[00:34:58.600 --> 00:35:01.300]   The other thing is the types of experiences we have
[00:35:01.300 --> 00:35:02.980]   can't not be interpreted
[00:35:02.980 --> 00:35:05.220]   through our existing interpretive frames.
[00:35:05.220 --> 00:35:06.980]   And most of the time our interpretive frames
[00:35:06.980 --> 00:35:09.340]   are unknown even to us, some of them.
[00:35:09.340 --> 00:35:13.740]   And so this is a tricky topic.
[00:35:14.340 --> 00:35:17.260]   So I guess there's a bunch of directions
[00:35:17.260 --> 00:35:18.100]   we could go with it,
[00:35:18.100 --> 00:35:21.420]   but I wanna come back to what the impulse was
[00:35:21.420 --> 00:35:24.180]   that was interesting around what is consciousness
[00:35:24.180 --> 00:35:26.580]   and how does it relate to us as social beings?
[00:35:26.580 --> 00:35:29.260]   And how does it relate to the possibility
[00:35:29.260 --> 00:35:31.340]   of consciousness with AIs?
[00:35:31.340 --> 00:35:34.860]   - Right, you're keeping us on track, which is wonderful.
[00:35:34.860 --> 00:35:36.500]   You're a wonderful hiking partner.
[00:35:36.500 --> 00:35:37.900]   - Okay. - Yes.
[00:35:37.900 --> 00:35:40.180]   Let's go back to the initial impulse
[00:35:40.180 --> 00:35:41.560]   of what is consciousness
[00:35:41.560 --> 00:35:44.760]   and how does the social impulse connect to consciousness?
[00:35:44.760 --> 00:35:50.700]   Is consciousness a consequence of that social connection?
[00:35:50.700 --> 00:35:53.040]   - I'm gonna state a position and not argue it
[00:35:53.040 --> 00:35:56.300]   'cause it's honestly, like it's a long, hard thing to argue
[00:35:56.300 --> 00:35:58.800]   and we can totally do it another time if you want.
[00:35:58.800 --> 00:36:04.220]   I don't subscribe to consciousness
[00:36:04.220 --> 00:36:08.900]   as an emergent property of biology or neural networks.
[00:36:10.760 --> 00:36:12.340]   Obviously, a lot of people do.
[00:36:12.340 --> 00:36:15.220]   Obviously, the philosophy of science
[00:36:15.220 --> 00:36:18.520]   orients towards that in,
[00:36:18.520 --> 00:36:21.940]   not absolutely, but largely.
[00:36:21.940 --> 00:36:27.880]   I think of the nature of first person,
[00:36:27.880 --> 00:36:31.180]   the universe of first person, of qualia,
[00:36:31.180 --> 00:36:36.180]   as experience, sensation, desire, emotion, phenomenology,
[00:36:38.900 --> 00:36:41.220]   but the felt sense, not the, we say emotion
[00:36:41.220 --> 00:36:42.800]   and we think of a neurochemical pattern
[00:36:42.800 --> 00:36:44.160]   or an endocrine pattern.
[00:36:44.160 --> 00:36:47.920]   But all of the physical stuff, the third person stuff,
[00:36:47.920 --> 00:36:51.600]   has position and momentum and charge and stuff like that
[00:36:51.600 --> 00:36:54.460]   that is measurable, repeatable.
[00:36:54.460 --> 00:36:57.840]   I think of the nature of first person and third person
[00:36:57.840 --> 00:37:01.480]   as ontologically orthogonal to each other,
[00:37:01.480 --> 00:37:03.500]   not reducible to each other.
[00:37:03.500 --> 00:37:05.160]   They're different kinds of stuff.
[00:37:06.760 --> 00:37:08.940]   So I think about the evolution of third person
[00:37:08.940 --> 00:37:10.780]   that we're quite used to thinking about
[00:37:10.780 --> 00:37:14.460]   from subatomic particles to atoms to molecules to on and on.
[00:37:14.460 --> 00:37:17.480]   I think about a similar kind of and corresponding evolution
[00:37:17.480 --> 00:37:18.680]   in the domain of first person
[00:37:18.680 --> 00:37:20.840]   from the way Whitehead talked about
[00:37:20.840 --> 00:37:23.520]   kind of prehension or proto qualia
[00:37:23.520 --> 00:37:25.460]   in earlier phases of self-organization
[00:37:25.460 --> 00:37:28.500]   and to higher orders of it and that there's correspondence,
[00:37:28.500 --> 00:37:31.820]   but that neither like the idealists
[00:37:31.820 --> 00:37:34.300]   do we reduce third person to first person,
[00:37:34.300 --> 00:37:35.800]   which is what idealists do.
[00:37:35.800 --> 00:37:37.300]   Or neither like the physicalists
[00:37:37.300 --> 00:37:40.340]   or do we reduce first person to third person.
[00:37:40.340 --> 00:37:44.920]   Obviously, Bohm talked about an implicate order
[00:37:44.920 --> 00:37:46.340]   that was deeper than and gave rise
[00:37:46.340 --> 00:37:48.380]   to the explicate order of both.
[00:37:48.380 --> 00:37:49.960]   Nagel talks about something like that.
[00:37:49.960 --> 00:37:51.880]   I have a slightly different sense of that,
[00:37:51.880 --> 00:37:53.980]   but again, I'll just kind of not argue
[00:37:53.980 --> 00:37:56.180]   how that occurs for a moment.
[00:37:56.180 --> 00:37:59.280]   So rather than say, does consciousness emerge from,
[00:37:59.280 --> 00:38:03.500]   I'll talk about do higher capacities of consciousness
[00:38:04.740 --> 00:38:07.260]   emerge in relationship with?
[00:38:07.260 --> 00:38:09.560]   So it's not first person as a category
[00:38:09.560 --> 00:38:10.940]   emerging from third person,
[00:38:10.940 --> 00:38:13.780]   but increased complexity within the nature of first person
[00:38:13.780 --> 00:38:15.260]   and third person co-evolving.
[00:38:15.260 --> 00:38:19.480]   Do I think that it seems relatively likely
[00:38:19.480 --> 00:38:21.140]   that more advanced neural networks
[00:38:21.140 --> 00:38:24.580]   have deeper phenomenology, more complex,
[00:38:24.580 --> 00:38:29.380]   where it goes just from basic sensation to emotion,
[00:38:29.380 --> 00:38:33.100]   to social awareness, to abstract cognition,
[00:38:33.100 --> 00:38:34.900]   to self-reflexive abstract cognition?
[00:38:34.900 --> 00:38:36.520]   Yeah, but I wouldn't say
[00:38:36.520 --> 00:38:37.940]   that's the emergence of consciousness.
[00:38:37.940 --> 00:38:39.680]   I would say it's increased complexity
[00:38:39.680 --> 00:38:41.140]   within the domain of first person
[00:38:41.140 --> 00:38:43.380]   corresponding to increased complexity.
[00:38:43.380 --> 00:38:44.780]   And the correspondent should not
[00:38:44.780 --> 00:38:46.340]   automatically be seen as causal.
[00:38:46.340 --> 00:38:47.660]   We can get into the arguments
[00:38:47.660 --> 00:38:49.880]   for why that often is the case.
[00:38:49.880 --> 00:38:54.300]   So would I say that obviously the sapien brain
[00:38:54.300 --> 00:38:58.060]   is pretty unique and a single sapien now has that, right?
[00:38:58.060 --> 00:39:01.100]   Even if it took sapiens evolving in tribes
[00:39:01.100 --> 00:39:03.680]   based on group selection to make that brain.
[00:39:03.680 --> 00:39:06.020]   So the group made it, now that brain is there.
[00:39:06.020 --> 00:39:08.380]   Now, if I take a single person with that brain
[00:39:08.380 --> 00:39:10.780]   out of the group and try to raise them in a box,
[00:39:10.780 --> 00:39:13.500]   they'll still not be very interesting even with the brain.
[00:39:13.500 --> 00:39:16.860]   But the brain does give hardware capacities
[00:39:16.860 --> 00:39:18.980]   that if conditioned in relationship
[00:39:18.980 --> 00:39:21.860]   can have interesting things emerge.
[00:39:21.860 --> 00:39:26.180]   So do I think that the human biology,
[00:39:26.180 --> 00:39:28.640]   types of human consciousness
[00:39:28.640 --> 00:39:30.500]   and types of social interaction
[00:39:30.500 --> 00:39:32.320]   all co-emerged and co-evolved?
[00:39:32.320 --> 00:39:33.160]   Yes.
[00:39:33.160 --> 00:39:36.880]   - As a small aside, as you're talking about the biology,
[00:39:36.880 --> 00:39:40.140]   let me comment that I spent, this is what I do.
[00:39:40.140 --> 00:39:41.460]   This is what I do with my life.
[00:39:41.460 --> 00:39:43.180]   This is why I will never accomplish anything
[00:39:43.180 --> 00:39:44.940]   is I spent much of the morning
[00:39:44.940 --> 00:39:49.260]   trying to do research on how many computations
[00:39:49.260 --> 00:39:52.820]   the brain performs and how much energy it uses
[00:39:52.820 --> 00:39:55.420]   versus the state of the ER, CPUs and GPUs.
[00:39:56.540 --> 00:39:59.960]   Arriving at about 20 quadrillion.
[00:39:59.960 --> 00:40:03.480]   So that's two to the 10 to the 16 computations.
[00:40:03.480 --> 00:40:07.680]   So synaptic firings per second that the brain does.
[00:40:07.680 --> 00:40:11.100]   And that's about a million times faster than the,
[00:40:11.100 --> 00:40:17.600]   let's say the 20 thread state of the arts Intel CPU,
[00:40:17.600 --> 00:40:21.200]   the 10th generation.
[00:40:21.200 --> 00:40:25.760]   And then there's similar calculation for the GPU
[00:40:25.760 --> 00:40:28.700]   and all ended up also trying to compute
[00:40:28.700 --> 00:40:32.440]   that it takes 10 watts to run the brain about.
[00:40:32.440 --> 00:40:34.760]   And then what does that mean in terms of calories per day,
[00:40:34.760 --> 00:40:39.760]   kilocalories, that's about, for an average human brain,
[00:40:39.760 --> 00:40:44.460]   that's 250 to 300 calories a day.
[00:40:44.460 --> 00:40:48.100]   And so it ended up being a calculation
[00:40:48.100 --> 00:40:53.100]   where you're doing about 20 quadrillion calculations
[00:40:54.620 --> 00:40:56.580]   that are fueled by something like,
[00:40:56.580 --> 00:40:59.180]   depending on your diet, three bananas.
[00:40:59.180 --> 00:41:03.600]   So three bananas results in a computation
[00:41:03.600 --> 00:41:05.820]   that's about a million times more powerful
[00:41:05.820 --> 00:41:08.580]   than the current state of the art computers.
[00:41:08.580 --> 00:41:10.620]   - Now let's take that one step further.
[00:41:10.620 --> 00:41:12.380]   There's some assumptions built in there.
[00:41:12.380 --> 00:41:14.580]   The assumption is that one,
[00:41:14.580 --> 00:41:17.540]   what the brain is doing is just computation.
[00:41:17.540 --> 00:41:20.940]   Two, the relevant computations are synaptic firings
[00:41:20.940 --> 00:41:22.620]   and that there's nothing other than synaptic firings
[00:41:22.620 --> 00:41:24.980]   that we have to factor.
[00:41:24.980 --> 00:41:27.940]   So I'm forgetting his name right now.
[00:41:27.940 --> 00:41:32.820]   There's a very famous neuroscientist at Stanford
[00:41:32.820 --> 00:41:34.060]   just passed away recently
[00:41:34.060 --> 00:41:37.280]   who did a lot of the pioneering work on glial cells
[00:41:37.280 --> 00:41:40.060]   and showed that his assessment glial cells
[00:41:40.060 --> 00:41:42.260]   did a huge amount of the thinking, not just neurons.
[00:41:42.260 --> 00:41:44.300]   And it opened up this entirely different field
[00:41:44.300 --> 00:41:46.620]   of like what the brain is and what consciousness is.
[00:41:46.620 --> 00:41:48.960]   You look at Damasio's work on embodied cognition
[00:41:48.960 --> 00:41:52.060]   and how much of what we would consider consciousness
[00:41:52.060 --> 00:41:52.900]   or feeling is happening
[00:41:52.900 --> 00:41:54.860]   outside of the nervous system completely,
[00:41:54.860 --> 00:41:56.820]   happening in endocrine process
[00:41:56.820 --> 00:41:59.540]   involving lots of other cells and signal communication.
[00:41:59.540 --> 00:42:01.860]   You talk to somebody like Penrose
[00:42:01.860 --> 00:42:03.420]   who you've had on the show.
[00:42:03.420 --> 00:42:05.940]   And even though the Penrose-Hammerhoff conjecture
[00:42:05.940 --> 00:42:08.320]   is probably not right, is there something like that
[00:42:08.320 --> 00:42:09.340]   that might be the case
[00:42:09.340 --> 00:42:11.180]   where we're actually having to look at stuff happening
[00:42:11.180 --> 00:42:13.820]   at the level of quantum computation and microtubules.
[00:42:13.820 --> 00:42:16.820]   I'm not arguing for any of those.
[00:42:16.820 --> 00:42:18.820]   I'm arguing that we don't know
[00:42:18.820 --> 00:42:21.160]   how big the unknown unknown set is.
[00:42:21.160 --> 00:42:22.700]   - Well, at the very least,
[00:42:22.700 --> 00:42:25.700]   this has become like an infomercial for the human brain.
[00:42:25.700 --> 00:42:28.500]   But wait, there's more.
[00:42:28.500 --> 00:42:31.700]   At the very least, the three bananas
[00:42:31.700 --> 00:42:33.380]   buys you a million times--
[00:42:33.380 --> 00:42:34.220]   - At the very least.
[00:42:34.220 --> 00:42:35.620]   - At the very least. - It's impressive.
[00:42:35.620 --> 00:42:38.340]   - And then you could have,
[00:42:38.340 --> 00:42:40.580]   and then the synaptic firings we're referring to
[00:42:40.580 --> 00:42:42.540]   is strictly the electrical signals.
[00:42:42.540 --> 00:42:44.580]   It could be the mechanical transmission of information,
[00:42:44.580 --> 00:42:46.500]   there's chemical transmission of information,
[00:42:46.500 --> 00:42:49.300]   there's all kinds of other stuff going on.
[00:42:49.300 --> 00:42:51.060]   And there's memory that's built in
[00:42:51.060 --> 00:42:52.380]   that's also all tied in.
[00:42:52.380 --> 00:42:55.400]   Not to mention, which I'm learning more and more about,
[00:42:55.400 --> 00:42:58.640]   it's not just about the neurons.
[00:42:58.640 --> 00:43:00.460]   It's also about the immune system
[00:43:00.460 --> 00:43:02.260]   that's somehow helping with the computation.
[00:43:02.260 --> 00:43:05.460]   So it's the entirety and the entire body
[00:43:05.460 --> 00:43:06.900]   is helping with the computation.
[00:43:06.900 --> 00:43:08.840]   So the three bananas--
[00:43:08.840 --> 00:43:10.020]   - It could buy you a lot.
[00:43:10.020 --> 00:43:12.100]   - It could buy you a lot.
[00:43:12.100 --> 00:43:17.100]   But on the topic of sort of the greater degrees
[00:43:18.340 --> 00:43:22.260]   of complexity emerging in consciousness,
[00:43:22.260 --> 00:43:26.140]   I think few things are as beautiful and inspiring
[00:43:26.140 --> 00:43:28.940]   as taking a step outside of the human brain,
[00:43:28.940 --> 00:43:32.540]   just looking at systems or simple rules
[00:43:32.540 --> 00:43:36.260]   create incredible complexity.
[00:43:36.260 --> 00:43:40.020]   Not create, incredible complexity emerges.
[00:43:40.020 --> 00:43:44.100]   So one of the simplest things to do that with
[00:43:44.100 --> 00:43:46.260]   is cellular automata.
[00:43:46.260 --> 00:43:49.580]   And there's, I don't know what it is,
[00:43:49.580 --> 00:43:51.100]   and maybe you can speak to it.
[00:43:51.100 --> 00:43:54.020]   We can certainly, we will certainly
[00:43:54.020 --> 00:43:55.500]   talk about the implications of this,
[00:43:55.500 --> 00:44:00.500]   but there's so few things that are as awe-inspiring to me
[00:44:00.500 --> 00:44:05.860]   as knowing the rules of a system
[00:44:05.860 --> 00:44:08.380]   and not being able to predict what the heck it looks like.
[00:44:08.380 --> 00:44:11.740]   And it creates incredibly beautiful complexity
[00:44:11.740 --> 00:44:14.320]   that when zoomed out on,
[00:44:14.320 --> 00:44:18.140]   looks like there's actual organisms doing things
[00:44:18.140 --> 00:44:23.140]   that are much, that operate on a scale much higher
[00:44:23.140 --> 00:44:27.820]   than the underlying mechanism.
[00:44:27.820 --> 00:44:28.940]   So with cellular automata,
[00:44:28.940 --> 00:44:31.860]   that's cells that are born and die, born and die,
[00:44:31.860 --> 00:44:34.420]   and they only know about each other's neighbors.
[00:44:34.420 --> 00:44:35.780]   And there's simple rules that govern
[00:44:35.780 --> 00:44:38.000]   that interaction of birth and death.
[00:44:38.000 --> 00:44:40.580]   And then they create, at scale,
[00:44:40.580 --> 00:44:43.740]   organisms that look like they take up
[00:44:43.740 --> 00:44:46.900]   hundreds or thousands of cells, and they're moving.
[00:44:46.900 --> 00:44:48.580]   They're moving around, they're communicating,
[00:44:48.580 --> 00:44:50.860]   they're sending signals to each other.
[00:44:50.860 --> 00:44:54.380]   And you forget, at moments at a time,
[00:44:54.380 --> 00:44:59.060]   before you remember, that the simple rules on cells
[00:44:59.060 --> 00:45:00.980]   is all that it took to create that.
[00:45:00.980 --> 00:45:08.240]   It's sad in that we can't come up
[00:45:08.240 --> 00:45:10.420]   with a simple description of that system
[00:45:11.420 --> 00:45:16.420]   that generalizes the behavior of the large organisms.
[00:45:16.420 --> 00:45:21.060]   We can only come up, we can only hope to come up
[00:45:21.060 --> 00:45:23.100]   with the thing, the fundamental physics,
[00:45:23.100 --> 00:45:25.460]   or the fundamental rules of that system, I suppose.
[00:45:25.460 --> 00:45:27.360]   It's sad that we can't predict.
[00:45:27.360 --> 00:45:29.840]   Everything we know about the mathematics of those systems,
[00:45:29.840 --> 00:45:32.320]   it seems like we can't, really in a nice way,
[00:45:32.320 --> 00:45:34.040]   like economics tries to do,
[00:45:34.040 --> 00:45:37.060]   to predict how this whole thing will unroll.
[00:45:37.060 --> 00:45:39.940]   But it's beautiful because how simple it is
[00:45:39.940 --> 00:45:40.940]   underneath it all.
[00:45:40.940 --> 00:45:47.300]   So what do you make of the emergence of complexity
[00:45:47.300 --> 00:45:48.900]   from simple rules?
[00:45:48.900 --> 00:45:50.820]   What the hell is that about?
[00:45:50.820 --> 00:45:53.180]   - Yeah, well, we can see that something like
[00:45:53.180 --> 00:45:56.980]   flocking behavior, the murmuration, can be computer-coded.
[00:45:56.980 --> 00:45:59.580]   It's not a very hard set of rules to be able to see
[00:45:59.580 --> 00:46:03.060]   some of those really amazing types of complexity.
[00:46:03.060 --> 00:46:06.420]   And the whole field of complexity science
[00:46:06.420 --> 00:46:08.620]   and some of the sub-disciplines like stigmur G
[00:46:08.620 --> 00:46:13.100]   are studying how following fairly simple responses
[00:46:13.100 --> 00:46:16.140]   to a pheromone signal do ant colonies do this amazing thing
[00:46:16.140 --> 00:46:18.740]   where what you might describe as the organizational
[00:46:18.740 --> 00:46:20.860]   or computational capacity of the colony
[00:46:20.860 --> 00:46:25.140]   is so profound relative to what each individual ant is doing.
[00:46:25.140 --> 00:46:29.240]   I am not anywhere near as well versed
[00:46:29.240 --> 00:46:31.180]   in the cutting edge of cellular automatas.
[00:46:31.180 --> 00:46:33.900]   I would like, unfortunately, in terms of topics
[00:46:33.900 --> 00:46:35.260]   that I would like to get to and haven't,
[00:46:35.260 --> 00:46:39.260]   like ET's more, Wolfram's a new kind of science
[00:46:39.260 --> 00:46:42.740]   I have only skimmed and read reviews of
[00:46:42.740 --> 00:46:45.380]   and not read the whole thing or his newer work since.
[00:46:45.380 --> 00:46:50.460]   But his idea of the four basic kind of categories
[00:46:50.460 --> 00:46:53.820]   of emergent phenomena that can come from cellular automata
[00:46:53.820 --> 00:46:55.420]   and that one of them is kind of interesting
[00:46:55.420 --> 00:46:58.900]   and looks a lot like complexity,
[00:46:58.900 --> 00:47:01.980]   rather than just chaos or homogeneity
[00:47:01.980 --> 00:47:05.100]   or self-termination or whatever.
[00:47:05.100 --> 00:47:10.180]   I think this is very interesting.
[00:47:10.180 --> 00:47:15.460]   It does not instantly make me think that biology
[00:47:15.460 --> 00:47:18.100]   is operating on a similarly small set of rules
[00:47:18.100 --> 00:47:19.340]   or that human consciousness is.
[00:47:19.340 --> 00:47:22.060]   I'm not that reductionistly oriented.
[00:47:22.060 --> 00:47:27.060]   And so if you look at say Santa Fe Institute,
[00:47:27.060 --> 00:47:30.740]   one of the co-founders, Stuart Kauffman,
[00:47:30.740 --> 00:47:33.100]   his work, you should really get him on your show.
[00:47:33.100 --> 00:47:35.420]   So a lot of the questions that you like,
[00:47:35.420 --> 00:47:39.140]   one of Kauffman's more recent books after investigations
[00:47:39.140 --> 00:47:40.420]   and some of the real fundamental stuff
[00:47:40.420 --> 00:47:41.740]   was called "Reinventing the Sacred"
[00:47:41.740 --> 00:47:44.980]   and it had to do with some of these exact questions
[00:47:44.980 --> 00:47:46.700]   in kind of non-reductionist approach,
[00:47:46.700 --> 00:47:49.180]   but that is not just silly hippie-ism.
[00:47:49.180 --> 00:47:53.020]   And he was very interested in highly non-ergotic systems
[00:47:53.020 --> 00:47:55.740]   where you couldn't take a lot of behavior
[00:47:55.740 --> 00:47:57.140]   over a small period of time and predict
[00:47:57.140 --> 00:47:58.620]   what the behavior of subsets
[00:47:58.620 --> 00:48:00.460]   over a longer period of time would do.
[00:48:01.460 --> 00:48:03.300]   And then going further,
[00:48:03.300 --> 00:48:05.180]   someone who spent some time at Santa Fe Institute
[00:48:05.180 --> 00:48:06.740]   and then kind of made a whole new field
[00:48:06.740 --> 00:48:09.020]   that you should have on, Dave Snowden,
[00:48:09.020 --> 00:48:12.420]   who some people call the father of anthro-complexity
[00:48:12.420 --> 00:48:14.580]   or what is the complexity unique to humans?
[00:48:14.580 --> 00:48:17.820]   He says something to the effect of
[00:48:17.820 --> 00:48:20.420]   that modeling humans as termites really doesn't cut it.
[00:48:20.420 --> 00:48:24.100]   Like we don't respond exactly identically
[00:48:24.100 --> 00:48:26.980]   to the same pheromone stimulus using stigmergy
[00:48:26.980 --> 00:48:28.620]   like it works for flows of traffic
[00:48:28.620 --> 00:48:30.500]   and some very simple human behaviors,
[00:48:30.500 --> 00:48:32.900]   but it really doesn't work for trying to make sense
[00:48:32.900 --> 00:48:34.660]   of the Sistine Chapel and Picasso
[00:48:34.660 --> 00:48:37.580]   and general relativity creation and stuff like that.
[00:48:37.580 --> 00:48:41.500]   And it's because the termites are not doing abstraction,
[00:48:41.500 --> 00:48:43.420]   forecasting deep into the future
[00:48:43.420 --> 00:48:46.100]   and making choices now based on forecasts of the future,
[00:48:46.100 --> 00:48:47.700]   not just adaptive signals in the moment
[00:48:47.700 --> 00:48:49.660]   and evolutionary code from history.
[00:48:49.660 --> 00:48:51.060]   That's really different, right?
[00:48:51.060 --> 00:48:53.140]   Like making choices now that can factor
[00:48:53.140 --> 00:48:54.540]   deep modeling of the future.
[00:48:56.100 --> 00:49:00.220]   And with humans, our uniqueness one to the next
[00:49:00.220 --> 00:49:02.300]   in terms of response to similar stimuli
[00:49:02.300 --> 00:49:04.340]   is much higher than it is with a termite.
[00:49:04.340 --> 00:49:07.300]   One of the interesting things there
[00:49:07.300 --> 00:49:08.940]   is that their uniqueness is extremely low.
[00:49:08.940 --> 00:49:11.140]   They're basically fungible within a class, right?
[00:49:11.140 --> 00:49:12.100]   There's different classes,
[00:49:12.100 --> 00:49:13.700]   but within a class they're basically fungible
[00:49:13.700 --> 00:49:15.100]   and their system uses that,
[00:49:15.100 --> 00:49:19.580]   very high numbers and lots of loss, right?
[00:49:19.580 --> 00:49:20.420]   Lots of death and loss.
[00:49:20.420 --> 00:49:21.980]   - Do you think the termite feels that way?
[00:49:21.980 --> 00:49:23.980]   Don't you think we humans are deceiving ourselves
[00:49:23.980 --> 00:49:24.980]   about our uniqueness?
[00:49:24.980 --> 00:49:27.140]   Perhaps it doesn't just,
[00:49:27.140 --> 00:49:28.940]   isn't there some sense in which this emergence
[00:49:28.940 --> 00:49:31.220]   just creates different higher and higher levels
[00:49:31.220 --> 00:49:33.460]   of abstraction where at every layer,
[00:49:33.460 --> 00:49:35.820]   each organism feels unique?
[00:49:35.820 --> 00:49:36.820]   Is that possible?
[00:49:36.820 --> 00:49:40.340]   That we're all equally dumb but at different scales?
[00:49:40.340 --> 00:49:42.180]   - No, I think uniqueness is evolving.
[00:49:42.180 --> 00:49:48.580]   I think that hydrogen atoms are more similar to each other
[00:49:48.580 --> 00:49:51.260]   than cells of the same type are.
[00:49:51.260 --> 00:49:53.340]   And I think that cells are more similar to each other
[00:49:53.340 --> 00:49:54.740]   than humans are.
[00:49:54.740 --> 00:49:58.340]   And I think that highly K-selected species
[00:49:58.340 --> 00:50:00.740]   are more unique than R-selected species.
[00:50:00.740 --> 00:50:03.020]   So they're different evolutionary processes.
[00:50:03.020 --> 00:50:05.780]   The R-selected species where you have a whole,
[00:50:05.780 --> 00:50:09.460]   a lot of death and very high birth rates,
[00:50:09.460 --> 00:50:14.060]   you're not looking for as much individuality within
[00:50:14.060 --> 00:50:16.060]   or individual possible expression
[00:50:16.060 --> 00:50:18.660]   to cover the evolutionary search space within an individual.
[00:50:18.660 --> 00:50:21.300]   You're looking at it more in terms of a numbers game.
[00:50:22.740 --> 00:50:25.100]   So yeah, I would say there's probably more difference
[00:50:25.100 --> 00:50:26.820]   between one orca and the next
[00:50:26.820 --> 00:50:29.620]   than there is between one Cape buffalo and the next.
[00:50:29.620 --> 00:50:32.340]   - Given that, it would be interesting to get your thoughts
[00:50:32.340 --> 00:50:36.580]   about mimetic theory where we're imitating each other
[00:50:36.580 --> 00:50:42.140]   in the context of this idea of uniqueness.
[00:50:42.140 --> 00:50:46.060]   How much truth is there to that?
[00:50:46.060 --> 00:50:48.700]   How compelling is this worldview to you
[00:50:49.580 --> 00:50:54.220]   of Girardian mimetic theory of desire
[00:50:54.220 --> 00:50:57.940]   where maybe you can explain it from your perspective,
[00:50:57.940 --> 00:51:00.020]   but it seems like imitating each other
[00:51:00.020 --> 00:51:04.140]   is the fundamental property of the behavior
[00:51:04.140 --> 00:51:06.660]   of human civilization.
[00:51:06.660 --> 00:51:09.100]   - Well, imitation is not unique to humans, right?
[00:51:09.100 --> 00:51:10.140]   Monkeys imitate.
[00:51:10.140 --> 00:51:15.740]   So a certain amount of learning through observing
[00:51:15.740 --> 00:51:17.660]   is not unique to humans.
[00:51:17.660 --> 00:51:19.100]   Humans do more of it.
[00:51:19.100 --> 00:51:24.060]   It's actually kind of worth speaking to this for a moment.
[00:51:24.060 --> 00:51:26.620]   Monkeys can learn new behaviors, new...
[00:51:26.620 --> 00:51:29.980]   We've even seen teaching an ape sign language
[00:51:29.980 --> 00:51:33.100]   and then the ape teaching other apes sign language.
[00:51:33.100 --> 00:51:34.660]   So that's a kind of mimesis, right?
[00:51:34.660 --> 00:51:36.660]   Kind of learning through imitation.
[00:51:36.660 --> 00:51:40.820]   And that needs to happen if they need to learn
[00:51:40.820 --> 00:51:42.900]   or develop capacities that are not just coded
[00:51:42.900 --> 00:51:44.260]   by their genetics, right?
[00:51:44.260 --> 00:51:45.860]   So within the same genome,
[00:51:45.860 --> 00:51:48.700]   they're learning new things based on the environment.
[00:51:48.700 --> 00:51:51.580]   And so based on someone else learned something first.
[00:51:51.580 --> 00:51:53.220]   And so let's pick it up.
[00:51:53.220 --> 00:51:57.100]   How much a creature is the result
[00:51:57.100 --> 00:51:58.500]   of just its genetic programming
[00:51:58.500 --> 00:52:02.180]   and how much it's learning is a very interesting question.
[00:52:02.180 --> 00:52:04.900]   And I think this is a place where humans really show up
[00:52:04.900 --> 00:52:06.860]   radically different than everything else.
[00:52:06.860 --> 00:52:09.660]   And you can see it in the neoteny,
[00:52:09.660 --> 00:52:12.180]   how long we're basically fetal.
[00:52:13.580 --> 00:52:17.940]   That the closest ancestors to us, if we look at a chimp,
[00:52:17.940 --> 00:52:20.300]   a chimp can hold on to its mother's fur
[00:52:20.300 --> 00:52:22.700]   while she moves around day one.
[00:52:22.700 --> 00:52:24.260]   And obviously we see horses up
[00:52:24.260 --> 00:52:26.620]   and walking within 20 minutes.
[00:52:26.620 --> 00:52:29.380]   The fact that it takes a human a year to be walking
[00:52:29.380 --> 00:52:30.540]   and it takes a horse 20 minutes
[00:52:30.540 --> 00:52:32.220]   and you say how many multiples of 20 minutes
[00:52:32.220 --> 00:52:33.060]   go into a year?
[00:52:33.060 --> 00:52:35.660]   Like that's a long period of helplessness
[00:52:35.660 --> 00:52:37.300]   that wouldn't work for a horse, right?
[00:52:37.300 --> 00:52:39.980]   Like they or anything else.
[00:52:40.980 --> 00:52:44.980]   And not only can we not hold on to mom in the first day,
[00:52:44.980 --> 00:52:48.540]   it's three months before we can move our head volitionally.
[00:52:48.540 --> 00:52:51.940]   So it's like, why are we embryonic for so long?
[00:52:51.940 --> 00:52:56.780]   Basically it's like it's still fetal on the outside.
[00:52:56.780 --> 00:52:59.420]   Had to be because couldn't keep growing inside
[00:52:59.420 --> 00:53:01.420]   and actually ever get out with big heads
[00:53:01.420 --> 00:53:03.260]   and narrower hips from going upright.
[00:53:03.260 --> 00:53:07.700]   So here's a place where there's a co-evolution
[00:53:07.700 --> 00:53:09.380]   of the pattern of humans,
[00:53:09.380 --> 00:53:12.300]   specifically here are our neoteny
[00:53:12.300 --> 00:53:15.100]   and what that portends to learning
[00:53:15.100 --> 00:53:17.660]   with our being tool making
[00:53:17.660 --> 00:53:19.900]   and environment modifying creatures.
[00:53:19.900 --> 00:53:23.500]   Which is because we have the abstraction to make tools,
[00:53:23.500 --> 00:53:25.620]   we change our environments more than other creatures
[00:53:25.620 --> 00:53:26.700]   change their environments.
[00:53:26.700 --> 00:53:29.340]   The next most environment modifying creature to us
[00:53:29.340 --> 00:53:30.180]   is like a beaver.
[00:53:30.180 --> 00:53:32.940]   And then you were in LA,
[00:53:32.940 --> 00:53:36.620]   you fly into LAX and you look at the just orthogonal grid
[00:53:36.620 --> 00:53:38.940]   going on forever in all directions.
[00:53:38.940 --> 00:53:41.860]   And we've recently come into the Anthropocene
[00:53:41.860 --> 00:53:43.340]   where the surface of the earth is changing
[00:53:43.340 --> 00:53:46.100]   more from human activity than geological activity
[00:53:46.100 --> 00:53:46.940]   and then beavers.
[00:53:46.940 --> 00:53:47.780]   And you're like, okay, wow,
[00:53:47.780 --> 00:53:49.460]   we're really in a class of our own
[00:53:49.460 --> 00:53:51.740]   in terms of environment modifying.
[00:53:51.740 --> 00:53:57.620]   So as soon as we started tool making,
[00:53:57.620 --> 00:54:00.020]   we were able to change our environments
[00:54:00.020 --> 00:54:02.060]   much more radically.
[00:54:02.060 --> 00:54:05.060]   We could put on clothes and go to a cold place.
[00:54:05.060 --> 00:54:07.980]   And this is really important because we actually went
[00:54:07.980 --> 00:54:10.420]   and became apex predators in every environment.
[00:54:10.420 --> 00:54:12.020]   We functioned like apex predators.
[00:54:12.020 --> 00:54:14.460]   The polar bear can't leave the Arctic, right?
[00:54:14.460 --> 00:54:16.900]   And the lion can't leave the Savannah
[00:54:16.900 --> 00:54:18.220]   and an orca can't leave the ocean.
[00:54:18.220 --> 00:54:19.540]   And we went and became apex predators
[00:54:19.540 --> 00:54:20.500]   in all those environments
[00:54:20.500 --> 00:54:21.940]   because of our tool creation capacity.
[00:54:21.940 --> 00:54:23.140]   We could become better predators
[00:54:23.140 --> 00:54:24.660]   than them adapted to the environment
[00:54:24.660 --> 00:54:27.260]   or at least with our tools adapted to the environment.
[00:54:27.260 --> 00:54:31.380]   - So then every aspect towards any organism
[00:54:31.380 --> 00:54:33.060]   in any environment,
[00:54:33.060 --> 00:54:35.900]   we're incredibly good at becoming apex predators.
[00:54:35.900 --> 00:54:39.540]   - Yes, and nothing else can do that kind of thing.
[00:54:39.540 --> 00:54:42.980]   There is no other apex predator that,
[00:54:42.980 --> 00:54:44.260]   but see the other apex predator
[00:54:44.260 --> 00:54:45.980]   is only getting better at being a predator
[00:54:45.980 --> 00:54:48.180]   through evolutionary process that's super slow.
[00:54:48.180 --> 00:54:49.420]   And that super slow process
[00:54:49.420 --> 00:54:52.140]   creates co-selective process with their environment.
[00:54:52.140 --> 00:54:54.420]   So as the predator becomes a tiny bit faster,
[00:54:54.420 --> 00:54:55.820]   it eats more of the slow prey,
[00:54:55.820 --> 00:54:57.500]   the genes of the fast prey and breed
[00:54:57.500 --> 00:54:58.860]   and the prey becomes faster.
[00:54:58.860 --> 00:55:01.220]   And so there's this kind of balancing.
[00:55:01.220 --> 00:55:03.220]   We in, because of our tool making,
[00:55:03.220 --> 00:55:05.620]   we increased our predatory capacity faster
[00:55:05.620 --> 00:55:08.460]   than anything else could increase its resilience to it.
[00:55:08.460 --> 00:55:10.940]   As a result, we started outstripping the environment
[00:55:10.940 --> 00:55:14.060]   and extincting species following stone tools
[00:55:14.060 --> 00:55:15.580]   and going and becoming apex predator everywhere.
[00:55:15.580 --> 00:55:17.700]   This is why we can't keep applying apex predator theories
[00:55:17.700 --> 00:55:18.740]   'cause we're not an apex predator.
[00:55:18.740 --> 00:55:19.580]   We're an apex predator,
[00:55:19.580 --> 00:55:21.500]   but we're something much more than that.
[00:55:21.500 --> 00:55:23.860]   Like just for an example,
[00:55:23.860 --> 00:55:26.140]   the top apex predator in the world, an orca.
[00:55:26.140 --> 00:55:30.180]   An orca can eat one big fish at a time, like one tuna,
[00:55:30.180 --> 00:55:32.620]   and it'll miss most of the time or one seal.
[00:55:32.620 --> 00:55:37.420]   And we can put a mile long drift net out on a single boat
[00:55:37.420 --> 00:55:40.140]   and pull up an entire school of them, right?
[00:55:40.140 --> 00:55:41.940]   We can deplete the entire oceans of them.
[00:55:41.940 --> 00:55:43.020]   That's not an orca, right?
[00:55:43.020 --> 00:55:44.660]   Like that's not an apex predator.
[00:55:44.660 --> 00:55:46.700]   And that's not even including
[00:55:46.700 --> 00:55:49.620]   that we can then genetically engineer different creatures.
[00:55:49.620 --> 00:55:52.860]   We can extinct species, we can devastate whole ecosystems.
[00:55:52.860 --> 00:55:55.060]   We can make built worlds that have no natural things
[00:55:55.060 --> 00:55:56.260]   that are just human built worlds.
[00:55:56.260 --> 00:55:59.180]   We can build new types of natural creatures, synthetic life.
[00:55:59.180 --> 00:56:01.180]   So we are much more like little gods
[00:56:01.180 --> 00:56:02.660]   than we are like apex predators now,
[00:56:02.660 --> 00:56:04.260]   but we're still behaving as apex predators.
[00:56:04.260 --> 00:56:06.100]   And little gods that behave as apex predators
[00:56:06.100 --> 00:56:09.780]   causes a problem, kind of core to my assessment of the world.
[00:56:09.780 --> 00:56:13.020]   - So what does it mean to be a predator?
[00:56:13.020 --> 00:56:18.020]   So a predator is somebody that effectively can mine
[00:56:18.020 --> 00:56:22.220]   the resources from a place, so for their survival,
[00:56:22.220 --> 00:56:27.220]   or is it also just purely like higher level objectives
[00:56:27.660 --> 00:56:31.060]   of violence and what is, can predators be predators
[00:56:31.060 --> 00:56:34.740]   towards the same, each other towards the same species?
[00:56:34.740 --> 00:56:37.980]   Like are we using the word predator sort of generally,
[00:56:37.980 --> 00:56:41.900]   which then connects to conflict and military conflict,
[00:56:41.900 --> 00:56:45.700]   violent conflict in the space of human species?
[00:56:45.700 --> 00:56:47.900]   - Obviously we can say that plants are mining
[00:56:47.900 --> 00:56:50.260]   the resources of their environment in a particular way,
[00:56:50.260 --> 00:56:52.820]   using photosynthesis to be able to pull minerals
[00:56:52.820 --> 00:56:55.660]   out of the soil and nitrogen and carbon out of the air
[00:56:55.660 --> 00:56:57.300]   and like that.
[00:56:57.300 --> 00:57:00.460]   And we can say herbivores are being able to mine
[00:57:00.460 --> 00:57:01.360]   and concentrate that.
[00:57:01.360 --> 00:57:03.060]   So I wouldn't say mining the environment
[00:57:03.060 --> 00:57:04.300]   is unique to predator.
[00:57:04.300 --> 00:57:07.460]   Predator is, you know,
[00:57:07.460 --> 00:57:12.900]   - Violence. - Generally being defined
[00:57:12.900 --> 00:57:16.740]   as mining other animals, right?
[00:57:16.740 --> 00:57:19.740]   We don't consider herbivores predators,
[00:57:19.740 --> 00:57:24.300]   but animal, which requires some type of violence capacity
[00:57:24.300 --> 00:57:26.940]   because animals move, plants don't move.
[00:57:26.940 --> 00:57:31.060]   So it requires some capacity to overtake something
[00:57:31.060 --> 00:57:32.820]   that can move and try to get away.
[00:57:32.820 --> 00:57:35.740]   We'll go back to the Gerard thing,
[00:57:35.740 --> 00:57:37.500]   then we'll come back here.
[00:57:37.500 --> 00:57:38.940]   Why are we neotenous?
[00:57:38.940 --> 00:57:40.580]   Why are we embryonic for so long?
[00:57:40.580 --> 00:57:45.780]   Because are we, did we just move from the Savannah
[00:57:45.780 --> 00:57:48.100]   to the Arctic and we need to learn new stuff?
[00:57:48.100 --> 00:57:49.780]   If we came genetically programmed,
[00:57:49.780 --> 00:57:51.580]   we would not be able to do that.
[00:57:51.580 --> 00:57:53.820]   Are we throwing spears or are we fishing
[00:57:53.820 --> 00:57:56.480]   or are we running an industrial supply chain
[00:57:56.480 --> 00:57:57.320]   or are we texting?
[00:57:57.320 --> 00:57:59.420]   What is the adaptive behavior?
[00:57:59.420 --> 00:58:02.180]   Horses today in the wild and horses 10,000 years ago
[00:58:02.180 --> 00:58:04.300]   were doing pretty much the same stuff.
[00:58:04.300 --> 00:58:08.260]   And so since we make tools and we evolve our tools
[00:58:08.260 --> 00:58:10.380]   and then change our environment so quickly
[00:58:10.380 --> 00:58:12.300]   and other animals are largely the result
[00:58:12.300 --> 00:58:13.140]   of their environment,
[00:58:13.140 --> 00:58:16.040]   but we're environment modifying so rapidly,
[00:58:16.040 --> 00:58:18.000]   we need to come without too much programming
[00:58:18.000 --> 00:58:20.100]   so we can learn the environment we're in,
[00:58:20.100 --> 00:58:21.620]   learn the language, right?
[00:58:21.620 --> 00:58:25.940]   Which is gonna be very important to learn the toolmaking,
[00:58:25.940 --> 00:58:29.380]   learn the, and so we have a very long period
[00:58:29.380 --> 00:58:31.420]   of relative helplessness
[00:58:31.420 --> 00:58:33.340]   because we aren't coded how to behave yet
[00:58:33.340 --> 00:58:35.580]   because we're imprinting a lot of software
[00:58:35.580 --> 00:58:38.660]   on how to behave that is useful to that particular time.
[00:58:38.660 --> 00:58:41.420]   So our mimesis is not unique to humans,
[00:58:41.420 --> 00:58:44.340]   but the total amount of it is really unique.
[00:58:44.340 --> 00:58:46.980]   And this is also where the uniqueness can go up, right?
[00:58:46.980 --> 00:58:49.660]   Is because we are less just the result of the genetics
[00:58:49.660 --> 00:58:52.460]   and that means the kind of learning through history
[00:58:52.460 --> 00:58:53.820]   that they got coded in genetics
[00:58:53.820 --> 00:58:55.380]   and more the result of,
[00:58:55.380 --> 00:59:00.020]   it's almost like our hardware selected for software, right?
[00:59:00.020 --> 00:59:02.120]   Like if evolution is kind of doing these,
[00:59:02.120 --> 00:59:04.020]   think of as a hardware selection.
[00:59:04.020 --> 00:59:06.140]   I have problems with computer metaphors for biology,
[00:59:06.140 --> 00:59:07.500]   but I'll use this one here.
[00:59:07.500 --> 00:59:14.000]   That we have not had hardware changes
[00:59:14.000 --> 00:59:15.760]   since the beginning of sapiens,
[00:59:15.760 --> 00:59:18.260]   but our world is really, really different.
[00:59:18.260 --> 00:59:20.480]   And that's all changes in software, right?
[00:59:20.480 --> 00:59:24.840]   Changes in on the same fundamental genetic substrate,
[00:59:24.840 --> 00:59:27.880]   what we're doing with these brains and minds and bodies
[00:59:27.880 --> 00:59:30.560]   and social groups and like that.
[00:59:30.560 --> 00:59:35.560]   And so now Gerard specifically was looking at
[00:59:35.560 --> 00:59:41.480]   when we watch other people talking, so we learn language,
[00:59:41.480 --> 00:59:43.760]   you and I would have a hard time learning Mandarin today
[00:59:43.760 --> 00:59:44.680]   or it'd take a lot of work.
[00:59:44.680 --> 00:59:46.600]   We'd be learning how to conjugate verbs and stuff,
[00:59:46.600 --> 00:59:47.880]   but a baby learns it instantly
[00:59:47.880 --> 00:59:49.520]   without anyone even really trying to teach it
[00:59:49.520 --> 00:59:50.360]   just through mimesis.
[00:59:50.360 --> 00:59:52.440]   So it's a powerful thing.
[00:59:52.440 --> 00:59:54.460]   They're obviously more neuroplastic than we are
[00:59:54.460 --> 00:59:55.300]   when they're doing that
[00:59:55.300 --> 00:59:57.160]   and all their attention is allocated to that.
[00:59:57.160 --> 00:59:59.560]   But they're also learning how to move their bodies
[00:59:59.560 --> 01:00:02.440]   and they're learning all kinds of stuff through mimesis.
[01:00:02.440 --> 01:00:03.760]   One of the things that Gerard says
[01:00:03.760 --> 01:00:05.600]   is they're also learning what to want
[01:00:05.600 --> 01:00:07.880]   and they learn what to want.
[01:00:07.880 --> 01:00:10.440]   They learn desire by watching what other people want.
[01:00:10.440 --> 01:00:11.780]   And so intrinsic to this,
[01:00:11.780 --> 01:00:13.940]   people end up wanting what other people want.
[01:00:13.940 --> 01:00:18.020]   And if we can't have what other people have
[01:00:18.020 --> 01:00:20.040]   without taking it away from them,
[01:00:20.040 --> 01:00:21.680]   then that becomes a source of conflict.
[01:00:21.680 --> 01:00:24.160]   So the mimesis of desire
[01:00:24.160 --> 01:00:26.120]   is the fundamental generator of conflict
[01:00:26.120 --> 01:00:29.880]   and that then the conflict energy
[01:00:29.880 --> 01:00:32.840]   within a group of people will build over time.
[01:00:32.840 --> 01:00:36.140]   This is a very, very crude interpretation of the theory.
[01:00:36.140 --> 01:00:37.680]   - Can we just pause on that?
[01:00:37.680 --> 01:00:39.240]   For people who are not familiar
[01:00:39.240 --> 01:00:41.360]   and for me who hasn't,
[01:00:41.360 --> 01:00:43.700]   I'm loosely familiar but haven't internalized it,
[01:00:43.700 --> 01:00:44.540]   but every time I think about it,
[01:00:44.540 --> 01:00:46.520]   it's a very compelling view of the world,
[01:00:46.520 --> 01:00:48.520]   whether it's true or not.
[01:00:48.520 --> 01:00:51.840]   It's quite, it's like when you take
[01:00:51.840 --> 01:00:54.120]   everything Freud says as truth,
[01:00:54.120 --> 01:00:56.560]   it's a very interesting way to think about the world.
[01:00:56.560 --> 01:00:57.840]   In the same way,
[01:00:57.840 --> 01:01:03.580]   thinking about the mimetic theory of desire,
[01:01:03.580 --> 01:01:05.520]   that everything we want
[01:01:05.520 --> 01:01:11.280]   is imitation of other people's wants.
[01:01:11.280 --> 01:01:13.320]   We don't have any original wants.
[01:01:13.320 --> 01:01:15.760]   We're constantly imitating others.
[01:01:15.760 --> 01:01:18.640]   And so, and not just others,
[01:01:18.640 --> 01:01:21.360]   but others we're exposed to.
[01:01:21.360 --> 01:01:23.360]   So there's these little local pockets,
[01:01:23.360 --> 01:01:25.040]   however defined local,
[01:01:25.040 --> 01:01:27.400]   of people imitating each other.
[01:01:27.400 --> 01:01:29.540]   And one that's super empowering
[01:01:29.540 --> 01:01:33.160]   because then you can pick which group you can join.
[01:01:33.160 --> 01:01:35.060]   Like, what do you wanna imitate?
[01:01:35.060 --> 01:01:35.960]   (laughs)
[01:01:35.960 --> 01:01:39.880]   It's the old, whoever your friends are,
[01:01:39.880 --> 01:01:42.440]   that's what your life is gonna be like.
[01:01:42.440 --> 01:01:43.680]   That's really powerful.
[01:01:43.680 --> 01:01:46.360]   I mean, it's depressing that we're so unoriginal,
[01:01:46.360 --> 01:01:49.000]   but it's also liberating in that,
[01:01:49.000 --> 01:01:51.040]   if this holds true,
[01:01:51.040 --> 01:01:52.560]   that we can choose our life
[01:01:52.560 --> 01:01:54.600]   by choosing the people we hang out with.
[01:01:54.600 --> 01:01:56.840]   - So, okay.
[01:01:56.840 --> 01:01:58.520]   Thoughts that are very compelling,
[01:01:58.520 --> 01:02:00.200]   that seem like they're more absolute
[01:02:00.200 --> 01:02:01.320]   than they actually are,
[01:02:01.320 --> 01:02:02.800]   end up also being dangerous.
[01:02:02.800 --> 01:02:03.840]   We wanna-- - Communism?
[01:02:03.840 --> 01:02:04.680]   (laughs)
[01:02:04.680 --> 01:02:07.620]   - I'm gonna discuss here where I think we need
[01:02:07.620 --> 01:02:10.240]   to amend this particular theory.
[01:02:10.240 --> 01:02:11.920]   But specifically, you just said something
[01:02:11.920 --> 01:02:14.120]   that everyone who's paid attention
[01:02:14.120 --> 01:02:15.760]   knows is true experientially,
[01:02:15.760 --> 01:02:18.680]   which is who you're around affects who you become.
[01:02:18.680 --> 01:02:22.560]   And as libertarian and self-determining
[01:02:22.560 --> 01:02:24.620]   and sovereign as we'd like to be,
[01:02:24.620 --> 01:02:28.640]   everybody, I think, knows that if you got put
[01:02:28.640 --> 01:02:30.320]   in a maximum security prison,
[01:02:30.320 --> 01:02:32.660]   aspects of your personality would have to adapt
[01:02:32.660 --> 01:02:34.520]   or you wouldn't survive there, right?
[01:02:34.520 --> 01:02:35.360]   You would become different.
[01:02:35.360 --> 01:02:38.960]   If you grew up in Darfur versus Finland,
[01:02:38.960 --> 01:02:40.680]   you would be different with your same genetics.
[01:02:40.680 --> 01:02:43.360]   Like, just, there's no real question about that.
[01:02:43.360 --> 01:02:47.520]   And that even today, if you hang out in a place
[01:02:47.520 --> 01:02:50.360]   with ultra marathoners as your roommates
[01:02:50.360 --> 01:02:53.660]   or all people who are obese as your roommates,
[01:02:53.660 --> 01:02:55.480]   the statistical likelihood of what happens
[01:02:55.480 --> 01:02:56.980]   to your fitness is pretty clear, right?
[01:02:56.980 --> 01:02:59.360]   Like the behavioral science of this is pretty clear.
[01:02:59.360 --> 01:03:02.320]   So, the whole saying we are the average
[01:03:02.320 --> 01:03:04.320]   of the five people we spend the most time around.
[01:03:04.320 --> 01:03:06.560]   I think the more self-reflective someone is
[01:03:06.560 --> 01:03:07.920]   and the more time they spend by themselves
[01:03:07.920 --> 01:03:09.720]   in self-reflection, the less this is true,
[01:03:09.720 --> 01:03:10.720]   but it's still true.
[01:03:10.720 --> 01:03:13.980]   So, one of the best things someone can do
[01:03:13.980 --> 01:03:16.760]   to become more self-determined is be self-determined
[01:03:16.760 --> 01:03:18.880]   about the environments they wanna put themselves in.
[01:03:18.880 --> 01:03:21.340]   Because to the degree that there is some self-determination
[01:03:21.340 --> 01:03:23.360]   and some determination by the environment,
[01:03:23.360 --> 01:03:24.900]   don't be fighting an environment
[01:03:24.900 --> 01:03:27.040]   that is predisposing you in bad directions.
[01:03:27.040 --> 01:03:28.500]   Try to put yourself in an environment
[01:03:28.500 --> 01:03:30.920]   that is predisposing the things that you want.
[01:03:30.920 --> 01:03:32.760]   In turn, try to affect the environment
[01:03:32.760 --> 01:03:34.080]   in ways that predispose positive things
[01:03:34.080 --> 01:03:35.120]   for those around you.
[01:03:35.120 --> 01:03:38.800]   - Or perhaps also to, there's probably interesting ways
[01:03:38.800 --> 01:03:39.640]   to play with this.
[01:03:39.640 --> 01:03:41.480]   You could probably put yourself,
[01:03:41.480 --> 01:03:46.040]   like form connections that have this perfect tension
[01:03:46.040 --> 01:03:48.320]   in all directions to where you're actually free
[01:03:48.320 --> 01:03:49.720]   to decide whatever the heck you want,
[01:03:49.720 --> 01:03:54.720]   because the set of wants within your circle of interactions
[01:03:54.720 --> 01:03:59.140]   is so conflicting that you're free to choose whichever one.
[01:03:59.140 --> 01:04:00.380]   So, if there's enough tension,
[01:04:00.380 --> 01:04:03.600]   as opposed to everybody aligned like a flock of birds.
[01:04:03.600 --> 01:04:05.200]   - Yeah, I mean, you definitely want
[01:04:05.200 --> 01:04:08.500]   that all of the dialectics would be balanced.
[01:04:09.340 --> 01:04:14.340]   So, if you have someone who is extremely oriented
[01:04:14.340 --> 01:04:17.980]   to self-empowerment and someone who's extremely oriented
[01:04:17.980 --> 01:04:19.360]   to kind of empathy and compassion,
[01:04:19.360 --> 01:04:21.300]   both the dialectic of those is better
[01:04:21.300 --> 01:04:22.940]   than either of them on their own.
[01:04:22.940 --> 01:04:27.740]   If you have both of them being inhabited better than you
[01:04:27.740 --> 01:04:29.660]   by the same person, spending time around that person
[01:04:29.660 --> 01:04:31.400]   will probably do well for you.
[01:04:31.400 --> 01:04:34.700]   I think the thing you just mentioned is super important
[01:04:34.700 --> 01:04:36.880]   when it comes to cognitive schools,
[01:04:36.880 --> 01:04:41.860]   which is I think one of the fastest things people can do
[01:04:41.860 --> 01:04:43.380]   to improve their learning
[01:04:43.380 --> 01:04:46.260]   and their not just cognitive learning,
[01:04:46.260 --> 01:04:50.860]   but their meaningful problem-solving communication
[01:04:50.860 --> 01:04:52.140]   and civic capacity,
[01:04:52.140 --> 01:04:54.540]   capacity to participate as a citizen with other people
[01:04:54.540 --> 01:04:56.260]   and making the world better,
[01:04:56.260 --> 01:04:59.340]   is to be seeking dialectical synthesis all the time.
[01:04:59.340 --> 01:05:03.580]   And so, in the Hegelian sense, if you have a thesis,
[01:05:03.580 --> 01:05:06.240]   you have an antithesis.
[01:05:06.240 --> 01:05:08.580]   So, maybe we have libertarianism on one side
[01:05:08.580 --> 01:05:10.700]   and Marxist kind of communism on the other side.
[01:05:10.700 --> 01:05:13.820]   And one is arguing that the individual
[01:05:13.820 --> 01:05:16.620]   is the unit of choice.
[01:05:16.620 --> 01:05:20.060]   And so, we want to increase the freedom
[01:05:20.060 --> 01:05:21.480]   and support of individual choice,
[01:05:21.480 --> 01:05:23.420]   because as they make more agentic choices,
[01:05:23.420 --> 01:05:25.260]   it'll produce a better whole for everybody.
[01:05:25.260 --> 01:05:26.100]   The other side saying,
[01:05:26.100 --> 01:05:27.980]   well, the individuals are conditioned by their environment
[01:05:27.980 --> 01:05:30.940]   who would choose to be born into Darfur rather than Finland.
[01:05:30.940 --> 01:05:35.780]   So, we actually need to collectively make environments
[01:05:36.380 --> 01:05:37.280]   that are good,
[01:05:37.280 --> 01:05:40.000]   because that the environment conditions individuals.
[01:05:40.000 --> 01:05:42.280]   So, you have a thesis and an antithesis.
[01:05:42.280 --> 01:05:44.560]   And then Hegel's ideas, you have a synthesis,
[01:05:44.560 --> 01:05:46.040]   which is a kind of higher order truth
[01:05:46.040 --> 01:05:48.000]   that understands how those relate
[01:05:48.000 --> 01:05:50.120]   in a way that neither of them do.
[01:05:50.120 --> 01:05:52.740]   And so, it is actually at a higher order of complexity.
[01:05:52.740 --> 01:05:53.720]   So, the first part would be,
[01:05:53.720 --> 01:05:55.480]   can I steel man each of these?
[01:05:55.480 --> 01:05:57.080]   Can I argue each one well enough
[01:05:57.080 --> 01:05:59.920]   that the proponents of it are like, totally, you got that?
[01:05:59.920 --> 01:06:01.640]   And not just argue it rhetorically,
[01:06:01.640 --> 01:06:04.420]   but can I inhabit it where I can try to see
[01:06:04.420 --> 01:06:06.280]   and feel the world the way someone
[01:06:06.280 --> 01:06:08.480]   seeing and feeling the world that way would?
[01:06:08.480 --> 01:06:09.740]   'Cause once I do,
[01:06:09.740 --> 01:06:11.360]   then I don't want to screw those people
[01:06:11.360 --> 01:06:13.160]   because there's truth in it, right?
[01:06:13.160 --> 01:06:14.560]   And I'm not gonna go back to war with them.
[01:06:14.560 --> 01:06:16.140]   I'm gonna go to finding solutions
[01:06:16.140 --> 01:06:18.380]   that could actually work at a higher order.
[01:06:18.380 --> 01:06:21.380]   If I don't go to a higher order, then there's war.
[01:06:21.380 --> 01:06:23.280]   And, but then the higher order thing would be,
[01:06:23.280 --> 01:06:27.720]   well, it seems like the individual does affect the commons
[01:06:27.720 --> 01:06:28.880]   and the collective and other people.
[01:06:28.880 --> 01:06:31.900]   It also seems like the collective conditions individuals,
[01:06:31.900 --> 01:06:33.100]   at least statistically.
[01:06:33.100 --> 01:06:34.700]   And I can cherry pick out the one guy
[01:06:34.700 --> 01:06:36.340]   who got out of the ghetto
[01:06:36.340 --> 01:06:38.480]   and pulled himself up by his bootstraps.
[01:06:38.480 --> 01:06:39.880]   But I can also say statistically
[01:06:39.880 --> 01:06:41.400]   that most people born into the ghetto
[01:06:41.400 --> 01:06:44.200]   show up differently than most people born into the Hamptons.
[01:06:44.200 --> 01:06:47.880]   And so, unless you wanna argue that
[01:06:47.880 --> 01:06:49.460]   and have you take your child from the Hamptons
[01:06:49.460 --> 01:06:50.300]   and put them in the ghetto,
[01:06:50.300 --> 01:06:52.880]   then like, come on, be realistic about this thing.
[01:06:52.880 --> 01:06:54.960]   So how do we make,
[01:06:54.960 --> 01:06:56.340]   we don't want social systems
[01:06:56.340 --> 01:07:00.040]   that make weak dependent individuals, right?
[01:07:00.040 --> 01:07:01.120]   The welfare argument,
[01:07:01.120 --> 01:07:04.000]   but we also don't want no social system
[01:07:04.000 --> 01:07:06.600]   that supports individuals to do better.
[01:07:06.600 --> 01:07:09.120]   We don't want individuals
[01:07:09.120 --> 01:07:11.480]   where their self-expression and agency
[01:07:11.480 --> 01:07:13.220]   fucks the environment and everybody else
[01:07:13.220 --> 01:07:15.600]   and employs slave labor and whatever.
[01:07:15.600 --> 01:07:20.520]   So can we make it to where individuals are creating holes
[01:07:20.520 --> 01:07:22.500]   that are better for conditioning other individuals?
[01:07:22.500 --> 01:07:23.680]   Can we make it to where we have holes
[01:07:23.680 --> 01:07:26.960]   that are conditioning increased agency and sovereignty?
[01:07:26.960 --> 01:07:28.240]   Right, that would be the synthesis.
[01:07:28.240 --> 01:07:30.820]   So the thing that I'm coming to here is,
[01:07:30.820 --> 01:07:32.800]   if people have that as a frame,
[01:07:32.800 --> 01:07:34.960]   and sometimes it's not just thesis and antithesis,
[01:07:34.960 --> 01:07:37.520]   it's like eight different views, right?
[01:07:37.520 --> 01:07:39.400]   Can I steel man each view?
[01:07:39.400 --> 01:07:41.280]   This is not just, can I take the perspective,
[01:07:41.280 --> 01:07:42.220]   but am I seeking them?
[01:07:42.220 --> 01:07:46.400]   Am I actively trying to inhabit other people's perspective?
[01:07:46.400 --> 01:07:49.440]   Then can I really try to essentialize it
[01:07:49.440 --> 01:07:51.480]   and argue the best points of it,
[01:07:51.480 --> 01:07:54.480]   both the sense-making about reality and the values,
[01:07:54.480 --> 01:07:56.120]   why these values actually matter?
[01:07:56.120 --> 01:07:59.740]   Then, just like I wanna seek those perspectives,
[01:07:59.740 --> 01:08:02.800]   then I wanna seek, is there a higher order
[01:08:02.800 --> 01:08:07.160]   set of understandings that could fulfill the values of
[01:08:07.160 --> 01:08:08.680]   and synthesize the sense-making
[01:08:08.680 --> 01:08:10.140]   of all of them simultaneously?
[01:08:10.140 --> 01:08:12.000]   Maybe I won't get it, but I wanna be seeking it,
[01:08:12.000 --> 01:08:14.460]   and I wanna be seeking progressively better ones.
[01:08:14.460 --> 01:08:18.740]   So this is perspective seeking, driving perspective taking,
[01:08:18.740 --> 01:08:22.000]   and then seeking synthesis.
[01:08:22.000 --> 01:08:27.000]   I think that one cognitive disposition
[01:08:27.800 --> 01:08:30.220]   might be the most helpful thing.
[01:08:30.220 --> 01:08:34.020]   - Would you put a title of dialectic synthesis
[01:08:34.020 --> 01:08:34.860]   on that process?
[01:08:34.860 --> 01:08:36.440]   'Cause that seems to be such a part,
[01:08:36.440 --> 01:08:38.480]   so like this rigorous empathy.
[01:08:38.480 --> 01:08:44.040]   Like, it's not just empathy, it's empathy with rigor.
[01:08:44.040 --> 01:08:46.380]   Like you really want to understand
[01:08:46.380 --> 01:08:48.380]   and embody different worldviews,
[01:08:48.380 --> 01:08:50.880]   and then try to find a higher order synthesis.
[01:08:50.880 --> 01:08:54.340]   - Okay, so I remember last night you told me,
[01:08:54.340 --> 01:08:57.220]   when we first met, you said
[01:08:57.220 --> 01:08:59.060]   that you looked in somebody's eyes
[01:08:59.060 --> 01:09:01.100]   and you felt that you had suffered in some ways
[01:09:01.100 --> 01:09:03.580]   that they had suffered, and so you could trust them.
[01:09:03.580 --> 01:09:05.800]   Shared pathos, right, creates a certain sense
[01:09:05.800 --> 01:09:07.940]   of kind of shared bonding and shared intimacy.
[01:09:07.940 --> 01:09:10.800]   So empathy is actually feeling the suffering
[01:09:10.800 --> 01:09:14.620]   of somebody else, and feeling the depth of their sentience.
[01:09:14.620 --> 01:09:16.880]   I don't wanna fuck them anymore, I don't wanna hurt them.
[01:09:16.880 --> 01:09:18.180]   I don't want to behave,
[01:09:18.180 --> 01:09:20.940]   I don't want my proposition to go through
[01:09:20.940 --> 01:09:23.140]   when I go and inhabit the perspective of the other people,
[01:09:23.140 --> 01:09:25.700]   if they feel that's really gonna mess them up, right?
[01:09:25.700 --> 01:09:27.900]   And so the rigorous empathy,
[01:09:27.900 --> 01:09:29.300]   it's different than just compassion,
[01:09:29.300 --> 01:09:31.060]   which is I generally care.
[01:09:31.060 --> 01:09:32.500]   Like I have a generalized care,
[01:09:32.500 --> 01:09:34.320]   but I don't know what it's like to be them.
[01:09:34.320 --> 01:09:36.340]   I can never know what it's like to be them perfectly,
[01:09:36.340 --> 01:09:38.200]   and there's a humility you have to have,
[01:09:38.200 --> 01:09:42.440]   which is my most rigorous attempt is still not it.
[01:09:42.440 --> 01:09:44.620]   My most rigorous attempt, mine,
[01:09:44.620 --> 01:09:46.860]   to know what it's like to be a woman is still not it.
[01:09:46.860 --> 01:09:48.540]   I have no question that if I was actually a woman,
[01:09:48.540 --> 01:09:50.460]   it would be different than my best guesses.
[01:09:50.460 --> 01:09:52.100]   I have no question if I was actually black,
[01:09:52.100 --> 01:09:54.500]   it'd be different than my best guesses.
[01:09:54.500 --> 01:09:56.980]   So there's a humility in that which keeps me listening,
[01:09:56.980 --> 01:09:58.740]   'cause I don't think that I know fully,
[01:09:58.740 --> 01:10:01.460]   but I want to, and I'm gonna keep trying better to.
[01:10:01.460 --> 01:10:03.680]   And then I want to across them,
[01:10:03.680 --> 01:10:04.520]   and then I wanna say,
[01:10:04.520 --> 01:10:06.140]   is there a way we can forward together
[01:10:06.140 --> 01:10:07.880]   and not have to be in war?
[01:10:07.880 --> 01:10:10.540]   It has to be something that could meet the values
[01:10:10.540 --> 01:10:11.660]   that everyone holds,
[01:10:11.660 --> 01:10:13.880]   that could reconcile the partial sensemaking
[01:10:13.880 --> 01:10:15.060]   that everyone holds,
[01:10:15.060 --> 01:10:18.820]   and that could offer a way forward that is more agreeable
[01:10:18.820 --> 01:10:21.220]   than the partial perspectives at war with each other.
[01:10:21.220 --> 01:10:24.100]   - But so the more you succeed at this empathy
[01:10:24.100 --> 01:10:25.540]   with humility,
[01:10:25.540 --> 01:10:27.140]   the more you're carrying the burden
[01:10:27.140 --> 01:10:30.860]   of other people's pain, essentially.
[01:10:30.860 --> 01:10:32.520]   - Now this goes back to the question of,
[01:10:32.520 --> 01:10:36.140]   do I see us as one being or 7.8 billion?
[01:10:36.140 --> 01:10:39.900]   I think the,
[01:10:39.900 --> 01:10:44.320]   if I'm overwhelmed with my own pain,
[01:10:44.320 --> 01:10:46.600]   I can't empathize that much,
[01:10:46.600 --> 01:10:47.900]   because I don't have the bandwidth,
[01:10:47.900 --> 01:10:49.600]   I don't have the capacity.
[01:10:49.600 --> 01:10:51.280]   If I don't feel like I can do something
[01:10:51.280 --> 01:10:52.820]   about a particular problem in the world,
[01:10:52.820 --> 01:10:55.900]   it's hard to feel it 'cause it's just too devastating.
[01:10:55.900 --> 01:10:58.980]   And so a lot of people go numb and even go nihilistic
[01:10:58.980 --> 01:11:01.420]   because they just don't feel the agency.
[01:11:01.420 --> 01:11:04.120]   So as I actually become more empowered as an individual
[01:11:04.120 --> 01:11:05.660]   and have more sense of agency,
[01:11:05.660 --> 01:11:08.380]   I also become more empowered to be more empathetic for others
[01:11:08.380 --> 01:11:10.940]   and be more connected to that shared burden
[01:11:10.940 --> 01:11:12.740]   and want to be able to make choices
[01:11:12.740 --> 01:11:15.620]   on behalf of and in benefit of.
[01:11:15.620 --> 01:11:18.940]   - So this way of living
[01:11:19.900 --> 01:11:22.100]   seems like a way of living
[01:11:22.100 --> 01:11:25.300]   that would solve a lot of problems in society
[01:11:25.300 --> 01:11:27.320]   from a cellular automata perspective.
[01:11:27.320 --> 01:11:32.580]   So if you have a bunch of little agents behaving in this way,
[01:11:32.580 --> 01:11:35.020]   my intuition, there'll be interesting complexities
[01:11:35.020 --> 01:11:37.460]   that emerge, but my intuition is
[01:11:37.460 --> 01:11:39.780]   it will create a society that's very different
[01:11:39.780 --> 01:11:43.740]   and recognizably better than the one we have today.
[01:11:43.740 --> 01:11:48.460]   How much, oh wait, hold that question,
[01:11:48.460 --> 01:11:49.300]   'cause I want to come back to it,
[01:11:49.300 --> 01:11:51.580]   but this brings us back to Gerard, which we didn't answer.
[01:11:51.580 --> 01:11:52.580]   The conflict theory.
[01:11:52.580 --> 01:11:53.400]   - Yes.
[01:11:53.400 --> 01:11:54.980]   - 'Cause about how to get past the conflict theory.
[01:11:54.980 --> 01:11:57.300]   - Yes, you know the Robert Frost poem about the two paths
[01:11:57.300 --> 01:11:59.780]   and you never had time to turn back to the other.
[01:11:59.780 --> 01:12:01.340]   We're gonna have to do that quite a lot.
[01:12:01.340 --> 01:12:05.540]   We're gonna be living that poem over and over again.
[01:12:05.540 --> 01:12:09.540]   But yes, how to, let's return back.
[01:12:09.540 --> 01:12:11.700]   - Okay, so the rest of the argument goes,
[01:12:11.700 --> 01:12:13.620]   you learn to want what other people want,
[01:12:13.620 --> 01:12:16.580]   therefore fundamental conflict based in our desire
[01:12:16.580 --> 01:12:18.840]   because we want the thing that somebody else has.
[01:12:18.840 --> 01:12:22.100]   And then people are, they're in conflict
[01:12:22.100 --> 01:12:23.600]   over trying to get the same stuff,
[01:12:23.600 --> 01:12:25.980]   power, status, attention, physical stuff,
[01:12:25.980 --> 01:12:27.620]   a mate, whatever it is.
[01:12:27.620 --> 01:12:30.620]   And then we learn the conflict by watching.
[01:12:30.620 --> 01:12:32.180]   And so then the conflict becomes mimetic.
[01:12:32.180 --> 01:12:36.180]   So the, and you know, we become on the Palestinian side
[01:12:36.180 --> 01:12:38.100]   or the Israeli side or the communist or capitalist side
[01:12:38.100 --> 01:12:40.860]   or the left or right politically or whatever it is.
[01:12:40.860 --> 01:12:43.860]   And until eventually the conflict energy in the system
[01:12:43.860 --> 01:12:47.420]   builds up so much that some type of violence
[01:12:47.420 --> 01:12:48.980]   is needed to get the bad guy,
[01:12:48.980 --> 01:12:50.460]   whoever it is that we're gonna blame.
[01:12:50.460 --> 01:12:52.940]   And you know, George talks about why scapegoating
[01:12:52.940 --> 01:12:55.700]   was kind of a mechanism to minimize the amount of violence.
[01:12:55.700 --> 01:12:59.820]   Let's blame a scapegoat as being more relevant
[01:12:59.820 --> 01:13:00.640]   than they really were.
[01:13:00.640 --> 01:13:02.060]   But if we all believe it, then we can all kind of
[01:13:02.060 --> 01:13:03.820]   calm down with the conflict energy.
[01:13:03.820 --> 01:13:06.300]   - It's a really interesting concept, by the way.
[01:13:06.300 --> 01:13:08.660]   I mean, you went, you beautifully summarized it,
[01:13:08.660 --> 01:13:10.340]   but the idea that there's a scapegoat,
[01:13:10.340 --> 01:13:11.900]   that there's a, this kind of thing
[01:13:11.900 --> 01:13:13.460]   naturally leads to a conflict.
[01:13:13.460 --> 01:13:17.020]   And then they find the other, some group that's the other,
[01:13:17.020 --> 01:13:18.860]   that's either real or artificial
[01:13:18.860 --> 01:13:20.940]   as the cause of the conflict.
[01:13:20.940 --> 01:13:22.460]   - Well, it's always artificial
[01:13:22.460 --> 01:13:23.900]   because the cause of the conflict,
[01:13:23.900 --> 01:13:25.980]   and Gerard is the mimesis of desire itself,
[01:13:25.980 --> 01:13:27.580]   and how do we attack that?
[01:13:27.580 --> 01:13:30.180]   How do we attack that it's our own desire?
[01:13:30.180 --> 01:13:32.760]   So this now gets to something more like Buddha said, right?
[01:13:32.760 --> 01:13:34.860]   Which was desire is the cause of suffering.
[01:13:34.860 --> 01:13:38.980]   Gerard and Buddha would kind of agree in this way.
[01:13:38.980 --> 01:13:43.420]   - So, but that explains, I mean, again,
[01:13:43.420 --> 01:13:46.780]   it's a compelling description of human history
[01:13:46.780 --> 01:13:49.740]   that we do tend to come up with the other.
[01:13:49.740 --> 01:13:50.580]   And--
[01:13:50.580 --> 01:13:51.420]   - Okay, kind of.
[01:13:51.420 --> 01:13:54.380]   I just had such a funny experience
[01:13:54.380 --> 01:13:56.020]   with someone critiquing Gerard the other day
[01:13:56.020 --> 01:13:59.700]   in such a elegant and beautiful and simple way.
[01:13:59.700 --> 01:14:04.700]   It's a friend who's grew up Aboriginal Australian,
[01:14:04.700 --> 01:14:12.660]   is a scholar of Aboriginal social technologies.
[01:14:12.660 --> 01:14:15.660]   And he's like, nah, man, Gerard just made shit up
[01:14:15.660 --> 01:14:16.880]   about how tribes work.
[01:14:16.880 --> 01:14:18.140]   Like we come from a tribe,
[01:14:18.140 --> 01:14:20.260]   we've got tens of thousands of years,
[01:14:20.260 --> 01:14:22.100]   and we didn't have increasing conflict
[01:14:22.100 --> 01:14:23.740]   and then scapegoat and kill someone.
[01:14:23.740 --> 01:14:24.940]   We'd have a little bit of conflict
[01:14:24.940 --> 01:14:27.940]   and then we would dance and then everybody'd be fine.
[01:14:27.940 --> 01:14:29.260]   Like we'd dance around the campfire,
[01:14:29.260 --> 01:14:31.100]   everyone would like kind of physically get the energy out.
[01:14:31.100 --> 01:14:32.100]   We'd look in each other's eyes,
[01:14:32.100 --> 01:14:34.580]   we'd have positive bonding, and then we're fine.
[01:14:34.580 --> 01:14:36.260]   And nobody, no scapegoats.
[01:14:36.260 --> 01:14:37.100]   And--
[01:14:37.100 --> 01:14:40.300]   - I think that's called the Joe Rogan theory of desire,
[01:14:40.300 --> 01:14:43.540]   which is he's like all of human problems
[01:14:43.540 --> 01:14:44.420]   have to do with the fact
[01:14:44.420 --> 01:14:46.900]   that you don't do enough hard shit in your day.
[01:14:46.900 --> 01:14:49.140]   So maybe you could just dance it,
[01:14:49.140 --> 01:14:50.900]   'cause he says like doing exercise
[01:14:50.900 --> 01:14:53.180]   and running on the treadmill gets all the demons out.
[01:14:53.180 --> 01:14:55.220]   Maybe just dancing gets all the demons out.
[01:14:55.220 --> 01:14:57.100]   - So this is why I say we have to be careful
[01:14:57.100 --> 01:15:00.460]   with taking an idea that seems too explanatory
[01:15:00.460 --> 01:15:02.900]   and then taking it as a given and then saying,
[01:15:02.900 --> 01:15:04.500]   well, now that we're stuck with the fact
[01:15:04.500 --> 01:15:08.940]   that conflict is inexorable because mimetic desire
[01:15:08.940 --> 01:15:10.940]   and therefore how do we deal with the inexorability
[01:15:10.940 --> 01:15:12.940]   of the conflict and how to sublimate violence?
[01:15:12.940 --> 01:15:15.740]   Well, no, the whole thing might be actually gibberish.
[01:15:15.740 --> 01:15:17.780]   Meaning it's only true in certain conditions
[01:15:17.780 --> 01:15:19.100]   and other conditions it's not true.
[01:15:19.100 --> 01:15:21.620]   So the deeper question is under which conditions
[01:15:21.620 --> 01:15:22.460]   is that true?
[01:15:22.460 --> 01:15:23.820]   Under which conditions is it not true?
[01:15:23.820 --> 01:15:26.300]   What do those other conditions make possible and look like?
[01:15:26.300 --> 01:15:27.500]   - And in general, we should stay away
[01:15:27.500 --> 01:15:30.300]   from really compelling models of reality
[01:15:30.300 --> 01:15:33.580]   because there's something about our brains
[01:15:33.580 --> 01:15:35.260]   that these models become sticky
[01:15:35.260 --> 01:15:37.340]   and we can't even think outside of them.
[01:15:37.340 --> 01:15:38.180]   So--
[01:15:38.180 --> 01:15:39.020]   - It's not that we stay away from them,
[01:15:39.020 --> 01:15:40.540]   it's that we know that the model of reality
[01:15:40.540 --> 01:15:41.440]   is never reality.
[01:15:42.340 --> 01:15:43.820]   That's the key thing.
[01:15:43.820 --> 01:15:46.340]   - Humility again, it goes back to just having the humility
[01:15:46.340 --> 01:15:48.380]   that you don't have a perfect model of reality.
[01:15:48.380 --> 01:15:52.020]   - There's an, the model of reality could never be reality.
[01:15:52.020 --> 01:15:56.020]   The process of modeling is inherently information reduction.
[01:15:56.020 --> 01:16:00.140]   And I can never show that the unknown,
[01:16:00.140 --> 01:16:01.840]   unknown set has been factored.
[01:16:01.840 --> 01:16:05.700]   - It's back to the cellular automata.
[01:16:05.700 --> 01:16:10.180]   You can't put the genie back in the bottle.
[01:16:10.180 --> 01:16:13.540]   Like when you realize it's unfortunately,
[01:16:13.540 --> 01:16:18.540]   sadly impossible to create a model of cellular automata,
[01:16:18.540 --> 01:16:22.860]   even if you know the basic rules
[01:16:22.860 --> 01:16:26.220]   that predict to even any degree of accuracy
[01:16:26.220 --> 01:16:30.700]   what, how that system will evolve,
[01:16:30.700 --> 01:16:32.740]   which is fascinating mathematically, sorry.
[01:16:32.740 --> 01:16:34.660]   I think about it quite a lot.
[01:16:34.660 --> 01:16:36.100]   It's very annoying.
[01:16:36.100 --> 01:16:37.840]   Wolfram has this rule 30,
[01:16:39.300 --> 01:16:41.860]   like you should be able to predict it.
[01:16:41.860 --> 01:16:46.380]   It's so simple, but you can't predict what's going to be,
[01:16:46.380 --> 01:16:48.860]   like there's a problem he defines,
[01:16:48.860 --> 01:16:50.680]   they try to predict some aspect
[01:16:50.680 --> 01:16:53.140]   of the middle column of the system.
[01:16:53.140 --> 01:16:55.780]   Just anything about it, what's gonna happen in the future.
[01:16:55.780 --> 01:16:58.300]   And you can't, you can't.
[01:16:58.300 --> 01:17:01.860]   It sucks.
[01:17:01.860 --> 01:17:03.900]   'Cause then we can't make sense of this world,
[01:17:03.900 --> 01:17:07.620]   you know, of reality in a definitive way.
[01:17:07.620 --> 01:17:10.180]   It's always like in the striving,
[01:17:10.180 --> 01:17:12.620]   like we're always striving.
[01:17:12.620 --> 01:17:14.260]   - Yeah, I don't think this sucks.
[01:17:14.260 --> 01:17:17.500]   - So that's a feature, not a bug?
[01:17:17.500 --> 01:17:20.380]   - Well, that's assuming a designer.
[01:17:20.380 --> 01:17:23.300]   I would say, I don't think it sucks.
[01:17:23.300 --> 01:17:25.540]   I think it's not only beautiful,
[01:17:25.540 --> 01:17:27.360]   but maybe necessary for beauty.
[01:17:27.360 --> 01:17:29.980]   - The mess.
[01:17:29.980 --> 01:17:33.580]   So you're, so you're, you disagree with Jordan Peterson,
[01:17:33.580 --> 01:17:35.140]   you should clean up your room.
[01:17:35.140 --> 01:17:36.580]   You like the rooms messy.
[01:17:36.580 --> 01:17:39.500]   - It's essential for the, for beauty.
[01:17:39.500 --> 01:17:42.700]   - It's not, it's not that, it's, okay.
[01:17:42.700 --> 01:17:45.860]   I take, I have no idea if it was intended this way.
[01:17:45.860 --> 01:17:48.300]   And so I'm just interpreting it a way I like.
[01:17:48.300 --> 01:17:51.140]   The commandment about having no false idols.
[01:17:51.140 --> 01:17:55.940]   To me, the way I interpret that is meaningful,
[01:17:55.940 --> 01:17:58.660]   is that reality is sacred to me.
[01:17:58.660 --> 01:18:00.580]   I have a reverence for reality,
[01:18:00.580 --> 01:18:03.420]   but I know my best understanding of it is never complete.
[01:18:04.940 --> 01:18:08.100]   I know my best model of it is a model
[01:18:08.100 --> 01:18:12.020]   where I tried to make some kind of predictive capacity
[01:18:12.020 --> 01:18:13.860]   by reducing the complexity of it
[01:18:13.860 --> 01:18:15.640]   to a set of stuff that I could observe.
[01:18:15.640 --> 01:18:17.200]   And then a subset of that stuff
[01:18:17.200 --> 01:18:18.860]   that I thought was the causal dynamics
[01:18:18.860 --> 01:18:21.980]   and then some set of mechanisms that are involved.
[01:18:21.980 --> 01:18:24.740]   And what we find is that it can be super useful.
[01:18:24.740 --> 01:18:28.380]   Like Newtonian gravity can help us do ballistic curves
[01:18:28.380 --> 01:18:30.020]   and all kinds of super useful stuff.
[01:18:30.020 --> 01:18:32.960]   And then we get to the place where it doesn't explain
[01:18:32.960 --> 01:18:34.980]   what's happening at the cosmological scale
[01:18:34.980 --> 01:18:36.840]   or at a quantum scale.
[01:18:36.840 --> 01:18:41.840]   And at each time, what we're finding is we excluded stuff.
[01:18:41.840 --> 01:18:44.360]   And it also doesn't explain the reconciliation
[01:18:44.360 --> 01:18:46.000]   of gravity with quantum mechanics
[01:18:46.000 --> 01:18:48.240]   and the other kinds of fundamental laws.
[01:18:48.240 --> 01:18:50.040]   So models can be useful,
[01:18:50.040 --> 01:18:52.140]   but they're never true with a capital T.
[01:18:52.140 --> 01:18:56.500]   Meaning they're never an actual real full,
[01:18:56.500 --> 01:18:57.980]   they're never a complete description
[01:18:57.980 --> 01:18:59.620]   of what's happening in real systems.
[01:18:59.620 --> 01:19:01.460]   They can be a complete description of what's happening
[01:19:01.460 --> 01:19:03.240]   in an artificial system that was the result
[01:19:03.240 --> 01:19:04.600]   of applying a model.
[01:19:04.600 --> 01:19:05.880]   So the model of a circuit board
[01:19:05.880 --> 01:19:07.500]   and the circuit board are the same thing.
[01:19:07.500 --> 01:19:08.940]   But I would argue that the model of a cell
[01:19:08.940 --> 01:19:11.180]   and the cell are not the same thing.
[01:19:11.180 --> 01:19:15.120]   And I would say this is key to what we call complexity
[01:19:15.120 --> 01:19:16.400]   versus the complicated,
[01:19:16.400 --> 01:19:19.220]   which is a distinction Dave Snowden made well
[01:19:19.220 --> 01:19:23.060]   in defining the difference between simple,
[01:19:23.060 --> 01:19:25.940]   complicated, complex and chaotic systems.
[01:19:25.940 --> 01:19:28.380]   But one of the definers in complex systems
[01:19:28.380 --> 01:19:30.560]   is that no matter how you model the complex system,
[01:19:30.560 --> 01:19:32.300]   it will still have some emergent behavior
[01:19:32.300 --> 01:19:34.020]   not predicted by the model.
[01:19:34.020 --> 01:19:37.100]   - Can you elaborate on the complex versus the complicated?
[01:19:37.100 --> 01:19:40.020]   - Complicated means we can fully explicate
[01:19:40.020 --> 01:19:41.820]   the phase space of all the things that it can do.
[01:19:41.820 --> 01:19:42.900]   We can program it.
[01:19:42.900 --> 01:19:47.440]   All human, not all, for the most part,
[01:19:47.440 --> 01:19:48.980]   human built things are complicated.
[01:19:48.980 --> 01:19:50.180]   They don't self-organize.
[01:19:50.180 --> 01:19:52.380]   They don't self-repair.
[01:19:52.380 --> 01:19:53.420]   They're not self-evolving.
[01:19:53.420 --> 01:19:56.300]   And we can make a blueprint for them.
[01:19:56.300 --> 01:19:58.020]   - Sorry, for human systems?
[01:19:58.020 --> 01:19:59.580]   - For human technologies.
[01:19:59.580 --> 01:20:01.080]   - Human technologies, I'm sorry.
[01:20:01.080 --> 01:20:01.920]   Okay, so non-biological systems.
[01:20:01.920 --> 01:20:04.560]   - That are basically the application of models.
[01:20:04.560 --> 01:20:09.080]   And engineering is kind of applied science,
[01:20:09.080 --> 01:20:10.980]   science as the modeling process.
[01:20:10.980 --> 01:20:14.460]   But with-
[01:20:14.460 --> 01:20:15.820]   - But humans are complex.
[01:20:15.820 --> 01:20:18.800]   - Complex stuff, with biological type stuff
[01:20:18.800 --> 01:20:20.520]   and sociological type stuff,
[01:20:20.520 --> 01:20:23.060]   it more has generator functions.
[01:20:23.060 --> 01:20:25.300]   And even those can't be fully explicated
[01:20:25.300 --> 01:20:28.580]   than it has, or our explanation can't prove
[01:20:28.580 --> 01:20:30.300]   that it has closure of what would be
[01:20:30.300 --> 01:20:31.500]   in the unknown, unknown set.
[01:20:31.500 --> 01:20:33.560]   Where we keep finding like, oh, it's just the genome.
[01:20:33.560 --> 01:20:35.220]   Oh, well now it's the genome and the epigenome.
[01:20:35.220 --> 01:20:37.180]   And then a recursive change on the epigenome
[01:20:37.180 --> 01:20:38.000]   'cause of the proteome.
[01:20:38.000 --> 01:20:39.260]   And then there's mitochondrial DNA
[01:20:39.260 --> 01:20:41.700]   and then virus is affected and fuck, right?
[01:20:41.700 --> 01:20:43.940]   So it's like we get overexcited
[01:20:43.940 --> 01:20:45.580]   when we think we found the thing.
[01:20:45.580 --> 01:20:48.220]   - So on Facebook, you know how you can list
[01:20:48.220 --> 01:20:49.700]   your relationship as complicated?
[01:20:49.700 --> 01:20:52.900]   It should actually say it's complex.
[01:20:52.900 --> 01:20:55.540]   That's the more accurate description.
[01:20:55.540 --> 01:20:57.540]   Self-terminating is a really interesting idea
[01:20:57.540 --> 01:20:59.180]   that you talk about quite a bit.
[01:20:59.180 --> 01:21:03.860]   First of all, what is a self-terminating system?
[01:21:03.860 --> 01:21:07.540]   And I think you have a sense, correct me if I'm wrong,
[01:21:07.540 --> 01:21:11.620]   that human civilization as it currently is
[01:21:11.620 --> 01:21:13.320]   is a self-terminating system.
[01:21:13.320 --> 01:21:17.700]   Why do you have that intuition?
[01:21:17.700 --> 01:21:18.860]   Combine it with the definition
[01:21:18.860 --> 01:21:21.020]   of what self-terminating means.
[01:21:22.760 --> 01:21:27.760]   - Okay, so if we look at human societies historically,
[01:21:27.760 --> 01:21:29.840]   human civilizations,
[01:21:29.840 --> 01:21:33.880]   it's not that hard to realize
[01:21:33.880 --> 01:21:35.880]   that most of the major civilizations
[01:21:35.880 --> 01:21:37.840]   and empires of the past don't exist anymore.
[01:21:37.840 --> 01:21:40.560]   So they had a life cycle, they died for some reason.
[01:21:40.560 --> 01:21:44.120]   So we don't still have the early Egyptian empire
[01:21:44.120 --> 01:21:47.080]   or Inca or Maya or Aztec or any of those, right?
[01:21:47.080 --> 01:21:50.400]   And so they terminated.
[01:21:50.400 --> 01:21:51.920]   Sometimes it seems like they were terminated
[01:21:51.920 --> 01:21:53.000]   from the outside and more,
[01:21:53.000 --> 01:21:54.520]   sometimes it seems like they self-terminate.
[01:21:54.520 --> 01:21:57.640]   When we look at Easter Island, it was a self-termination.
[01:21:57.640 --> 01:22:00.700]   So let's go ahead and take an island situation.
[01:22:00.700 --> 01:22:03.240]   If I have an island and we are consuming the resources
[01:22:03.240 --> 01:22:05.280]   on that island faster than the resources
[01:22:05.280 --> 01:22:08.200]   can replicate themselves and there's a finite space there,
[01:22:08.200 --> 01:22:09.840]   that system is gonna self-terminate.
[01:22:09.840 --> 01:22:12.080]   It's not gonna be able to keep doing that thing
[01:22:12.080 --> 01:22:15.200]   'cause you'll get to a place of there's no resources left
[01:22:15.200 --> 01:22:16.840]   and then you get a...
[01:22:16.840 --> 01:22:20.040]   So now if I'm utilizing the resources faster
[01:22:20.040 --> 01:22:21.240]   than they can replicate,
[01:22:21.920 --> 01:22:23.760]   or faster than they can replenish,
[01:22:23.760 --> 01:22:25.840]   and I'm actually growing our population in the process,
[01:22:25.840 --> 01:22:27.320]   I'm even increasing the rate
[01:22:27.320 --> 01:22:29.080]   of the utilization of resources,
[01:22:29.080 --> 01:22:32.520]   I might get an exponential curve and then hit a wall
[01:22:32.520 --> 01:22:34.760]   and then just collapse the exponential curve
[01:22:34.760 --> 01:22:37.740]   rather than do an S curve or some other kind of thing.
[01:22:37.740 --> 01:22:43.080]   So self-terminating system is any system
[01:22:43.080 --> 01:22:46.360]   that depends upon a substrate system
[01:22:46.360 --> 01:22:48.700]   that is debasing its own substrate,
[01:22:48.700 --> 01:22:51.080]   that is debasing what it depends upon.
[01:22:51.080 --> 01:22:54.800]   - So you're right that if you look at empires,
[01:22:54.800 --> 01:22:57.640]   they rise and fall throughout human history,
[01:22:57.640 --> 01:22:59.500]   but not this time, bro.
[01:22:59.500 --> 01:23:03.440]   This one's gonna last forever.
[01:23:03.440 --> 01:23:06.560]   - I like that idea.
[01:23:06.560 --> 01:23:07.920]   I think that if we don't understand
[01:23:07.920 --> 01:23:11.000]   why all the previous ones failed, we can't ensure that.
[01:23:11.000 --> 01:23:12.840]   And so I think it's very important to understand it well
[01:23:12.840 --> 01:23:16.400]   so that we can have that be a designed outcome
[01:23:16.400 --> 01:23:18.680]   with somewhat decent probability.
[01:23:18.680 --> 01:23:22.040]   - So where it's sort of in terms of consuming the resources
[01:23:22.040 --> 01:23:24.560]   on the island, we're a clever bunch,
[01:23:24.560 --> 01:23:29.560]   and we keep coming up, especially when on the horizon,
[01:23:29.560 --> 01:23:33.720]   there is a termination point.
[01:23:33.720 --> 01:23:37.440]   We keep coming up with clever ways of avoiding disaster,
[01:23:37.440 --> 01:23:40.640]   of avoiding collapse, of constructing,
[01:23:40.640 --> 01:23:42.240]   this is where technological innovation,
[01:23:42.240 --> 01:23:43.820]   this is where growth comes in,
[01:23:43.820 --> 01:23:46.240]   coming up with different ways to improve productivity
[01:23:46.240 --> 01:23:48.160]   and the way society functions
[01:23:48.160 --> 01:23:50.160]   such that we consume less resources
[01:23:50.160 --> 01:23:52.400]   or get a lot more from the resources we have.
[01:23:52.400 --> 01:23:58.160]   So there's some sense in which there is a,
[01:23:58.160 --> 01:24:01.760]   human ingenuity is a source for optimism
[01:24:01.760 --> 01:24:03.640]   about the future of this particular system
[01:24:03.640 --> 01:24:07.240]   that may not be self-terminating.
[01:24:07.240 --> 01:24:11.160]   If there's more innovation than there is consumption.
[01:24:11.160 --> 01:24:15.720]   - So overconsumption of resources
[01:24:15.720 --> 01:24:17.320]   is just one way a thing can self-terminate.
[01:24:17.320 --> 01:24:18.720]   We're just kind of starting here,
[01:24:18.720 --> 01:24:23.720]   but there are reasons for optimism and pessimism
[01:24:23.720 --> 01:24:27.400]   then they're both worth understanding.
[01:24:27.400 --> 01:24:29.040]   And there's failure modes on understanding
[01:24:29.040 --> 01:24:30.280]   either without the other.
[01:24:30.280 --> 01:24:33.120]   As we mentioned previously,
[01:24:33.120 --> 01:24:37.720]   there's what I would call naive techno-optimism,
[01:24:37.720 --> 01:24:39.760]   naive techno-capital optimism
[01:24:39.760 --> 01:24:43.000]   that says stuff just has been getting better and better
[01:24:43.000 --> 01:24:44.600]   and we wouldn't wanna live in the dark ages
[01:24:44.600 --> 01:24:46.240]   and tech has done all this awesome stuff.
[01:24:46.240 --> 01:24:50.400]   And we know the proponents of those models
[01:24:50.400 --> 01:24:52.200]   and that stuff is gonna kind of keep getting better.
[01:24:52.200 --> 01:24:53.160]   Of course there are problems,
[01:24:53.160 --> 01:24:54.720]   but human ingenuity rises to it.
[01:24:54.720 --> 01:24:56.720]   Supply and demand will solve the problems, whatever.
[01:24:56.720 --> 01:25:01.040]   - Would you put a rake as well on that or in that bucket?
[01:25:01.040 --> 01:25:04.240]   Is there some specific people you have in mind
[01:25:04.240 --> 01:25:06.440]   or naive optimism is truly naive
[01:25:06.440 --> 01:25:09.480]   to where you're essentially just have an optimism
[01:25:09.480 --> 01:25:11.320]   that's blind to any kind of realities
[01:25:11.320 --> 01:25:14.040]   of the way technology progresses?
[01:25:14.040 --> 01:25:19.040]   - I don't think that anyone who thinks about it
[01:25:19.040 --> 01:25:22.440]   and writes about it is perfectly naive.
[01:25:22.440 --> 01:25:23.280]   - Gotcha.
[01:25:23.280 --> 01:25:24.120]   - But there might be--
[01:25:24.120 --> 01:25:25.680]   - It's a platonic ideal.
[01:25:25.680 --> 01:25:29.720]   - There might be a bias in the nature of the assessment.
[01:25:29.720 --> 01:25:33.160]   I would also say there's kind of naive techno-pessimism
[01:25:33.160 --> 01:25:36.160]   and there are critics of technology.
[01:25:36.160 --> 01:25:43.800]   I mean, you read the Unabomber's manifesto
[01:25:43.800 --> 01:25:47.080]   on why technology can't not result in our self-termination.
[01:25:47.080 --> 01:25:49.560]   So we have to take it out before it gets any further.
[01:25:49.560 --> 01:25:54.280]   But also if you read a lot of the X-risk community,
[01:25:54.280 --> 01:25:56.560]   Bostrom and friends,
[01:25:56.560 --> 01:26:00.400]   it's like our total number of existential risks
[01:26:00.400 --> 01:26:03.480]   and the total probability of them is going up.
[01:26:03.480 --> 01:26:07.080]   And so I think that there are,
[01:26:07.080 --> 01:26:11.960]   we have to hold together where our positive possibilities
[01:26:11.960 --> 01:26:14.440]   and our risk possibilities are both increasing
[01:26:14.440 --> 01:26:17.080]   and then say for the positive possibilities
[01:26:17.080 --> 01:26:19.100]   to be realized long-term,
[01:26:19.100 --> 01:26:21.560]   all of the catastrophic risks have to not happen.
[01:26:21.560 --> 01:26:25.040]   Any of the catastrophic risks happening is enough
[01:26:25.040 --> 01:26:27.320]   to keep that positive outcome from occurring.
[01:26:27.320 --> 01:26:29.980]   So how do we ensure that none of them happen?
[01:26:29.980 --> 01:26:31.160]   If we want to say,
[01:26:31.160 --> 01:26:33.040]   let's have a civilization that doesn't collapse.
[01:26:33.040 --> 01:26:35.480]   So again, collapse theory.
[01:26:35.480 --> 01:26:37.440]   It's worth looking at books like
[01:26:37.440 --> 01:26:40.240]   "The Collapse of Complex Societies" by Joseph Tainter.
[01:26:40.240 --> 01:26:45.240]   It does an analysis of that many of the societies fell
[01:26:45.240 --> 01:26:48.960]   for internal institutional decay,
[01:26:48.960 --> 01:26:51.400]   civilizational decay reasons.
[01:26:51.400 --> 01:26:53.960]   Baudrillard in "Simulation and Simulacra"
[01:26:53.960 --> 01:26:55.600]   looks at a very different way of looking at
[01:26:55.600 --> 01:26:57.960]   how institutional decay in the collective intelligence
[01:26:57.960 --> 01:26:59.760]   of a system happens and it becomes kind of
[01:26:59.760 --> 01:27:02.360]   more internally parasitic on itself.
[01:27:02.360 --> 01:27:04.560]   Obviously, Jared Diamond made a more popular book
[01:27:04.560 --> 01:27:05.500]   called "Collapse."
[01:27:05.500 --> 01:27:07.640]   And as we were mentioning,
[01:27:07.640 --> 01:27:10.480]   the Antikythera mechanism has been getting attention
[01:27:10.480 --> 01:27:11.320]   in the news lately.
[01:27:11.320 --> 01:27:13.640]   It's like a 2000 year old clock, right?
[01:27:13.640 --> 01:27:15.600]   Like metal gears.
[01:27:15.600 --> 01:27:20.600]   And does that mean we lost like 1500 years
[01:27:20.600 --> 01:27:22.220]   of technological progress?
[01:27:22.220 --> 01:27:25.600]   And from a society that was relatively
[01:27:25.600 --> 01:27:27.040]   technologically advanced.
[01:27:27.040 --> 01:27:32.560]   So what I'm interested in here is being able to say,
[01:27:32.560 --> 01:27:36.420]   okay, well, why did previous societies fail?
[01:27:37.420 --> 01:27:40.660]   Can we understand that abstractly enough
[01:27:40.660 --> 01:27:44.100]   that we can make a civilizational model
[01:27:44.100 --> 01:27:47.140]   that isn't just trying to solve one type of failure,
[01:27:47.140 --> 01:27:48.980]   but solve the underlying things
[01:27:48.980 --> 01:27:51.500]   that generate the failures as a whole?
[01:27:51.500 --> 01:27:55.060]   Are there some underlying generator functions or patterns
[01:27:55.060 --> 01:27:57.140]   that would make a system self-terminating?
[01:27:57.140 --> 01:27:59.200]   And can we solve those and have that be the kernel
[01:27:59.200 --> 01:28:02.420]   of a new civilizational model that is not self-terminating?
[01:28:02.420 --> 01:28:05.180]   And can we then be able to actually look at
[01:28:05.180 --> 01:28:06.860]   the categories of excerpts we're aware of
[01:28:06.860 --> 01:28:09.220]   and see that we actually have resilience
[01:28:09.220 --> 01:28:10.280]   in the presence of those,
[01:28:10.280 --> 01:28:12.700]   not just resilience, but anti-fragility.
[01:28:12.700 --> 01:28:16.820]   And I would say for the optimism to be grounded,
[01:28:16.820 --> 01:28:20.340]   it has to actually be able to understand the risk space well
[01:28:20.340 --> 01:28:22.460]   and have adequate solutions for it.
[01:28:22.460 --> 01:28:27.460]   - So can we try to dig into some basic intuitions
[01:28:27.460 --> 01:28:32.380]   about the underlying sources of catastrophic failures
[01:28:32.380 --> 01:28:35.020]   of the system and overconsumption
[01:28:35.020 --> 01:28:37.420]   that's built in into self-terminating systems?
[01:28:37.420 --> 01:28:41.000]   So both the overconsumption, which is like the slow death,
[01:28:41.000 --> 01:28:44.220]   and then there's the fast death of nuclear war
[01:28:44.220 --> 01:28:45.820]   and all those kinds of things,
[01:28:45.820 --> 01:28:49.420]   AGI, biotech, bioengineering, nanotechnology,
[01:28:49.420 --> 01:28:50.820]   my favorite, nanobots.
[01:28:50.820 --> 01:28:58.220]   Nanobots are my favorite because it sounds so cool to me
[01:28:58.220 --> 01:29:01.140]   that I could just know that I would be one of the scientists
[01:29:01.140 --> 01:29:04.020]   that would be full steam ahead in building them
[01:29:04.020 --> 01:29:07.300]   without sufficiently thinking
[01:29:07.300 --> 01:29:08.700]   about the negative consequences.
[01:29:08.700 --> 01:29:11.020]   I would definitely be, I would be podcasting
[01:29:11.020 --> 01:29:12.940]   all about the negative consequences,
[01:29:12.940 --> 01:29:17.460]   but when I go back home, I'd be just in my heart,
[01:29:17.460 --> 01:29:21.180]   know the amount of excitement is a dumb descendant of ape,
[01:29:21.180 --> 01:29:22.220]   no offense to apes.
[01:29:22.220 --> 01:29:27.620]   So I wanna backtrack on my previous comments
[01:29:27.620 --> 01:29:31.560]   about negative comments about apes.
[01:29:32.560 --> 01:29:36.640]   I have that sense of excitement
[01:29:36.640 --> 01:29:38.880]   that would result in problems.
[01:29:38.880 --> 01:29:40.360]   So sorry, a lot of things said,
[01:29:40.360 --> 01:29:43.560]   but can we start to pull it at a thread?
[01:29:43.560 --> 01:29:46.940]   'Cause you've also provided a kind of a beautiful,
[01:29:46.940 --> 01:29:50.760]   general approach to this, which is this dialectic synthesis
[01:29:50.760 --> 01:29:54.360]   or just rigorous empathy.
[01:29:54.360 --> 01:29:56.960]   Whatever word we wanna put to it,
[01:29:56.960 --> 01:29:59.080]   that seems to be from the individual perspective
[01:29:59.080 --> 01:30:01.560]   as one way to sort of live in the world
[01:30:01.560 --> 01:30:02.720]   as we try to figure out
[01:30:02.720 --> 01:30:06.120]   how to construct non-self-terminating systems.
[01:30:06.120 --> 01:30:08.040]   So what are some underlying sources?
[01:30:08.040 --> 01:30:10.720]   - Yeah, first I have to say,
[01:30:10.720 --> 01:30:16.640]   I actually really respect Drexler for emphasizing Grey Goo
[01:30:16.640 --> 01:30:18.640]   in "Engines of Creation" back in the day
[01:30:18.640 --> 01:30:23.780]   to make sure the world was paying adequate attention
[01:30:23.780 --> 01:30:26.380]   to the risks of the nanotech.
[01:30:26.380 --> 01:30:28.680]   As someone who was right at the cutting edge
[01:30:28.680 --> 01:30:29.680]   of what could be,
[01:30:29.680 --> 01:30:35.080]   there's definitely game theoretic advantage
[01:30:35.080 --> 01:30:36.980]   to those who focus on the opportunities
[01:30:36.980 --> 01:30:39.920]   and don't focus on the risks or pretend there aren't risks
[01:30:39.920 --> 01:30:45.640]   because they get to market first
[01:30:45.640 --> 01:30:49.400]   and then they externalize all of the costs
[01:30:49.400 --> 01:30:51.460]   through limited liability or whatever it is
[01:30:51.460 --> 01:30:53.240]   to the commons or wherever happen to have it.
[01:30:53.240 --> 01:30:54.640]   Other people are gonna have to solve those,
[01:30:54.640 --> 01:30:56.640]   but now they have the power and capital associated.
[01:30:56.640 --> 01:30:57.860]   The person who looked at the risks
[01:30:57.860 --> 01:31:00.000]   and tried to do better design and go slower
[01:31:00.000 --> 01:31:02.960]   is probably not gonna move into positions
[01:31:02.960 --> 01:31:04.480]   of as much power or influence as quickly.
[01:31:04.480 --> 01:31:05.960]   So this is one of the issues we have to deal with
[01:31:05.960 --> 01:31:08.800]   is some of the bad game theoretic dispositions
[01:31:08.800 --> 01:31:12.680]   in the system relative to its own stability.
[01:31:12.680 --> 01:31:15.080]   - And the key aspect to that, sorry to interrupt,
[01:31:15.080 --> 01:31:17.120]   is the externalities generated.
[01:31:17.120 --> 01:31:18.480]   - Yes.
[01:31:18.480 --> 01:31:20.380]   - What flavors of catastrophic risk
[01:31:20.380 --> 01:31:21.680]   are we talking about here?
[01:31:21.680 --> 01:31:25.000]   What's your favorite flavor in terms of ice cream?
[01:31:25.000 --> 01:31:25.880]   So mine is coconut.
[01:31:25.880 --> 01:31:27.900]   - Nobody seems to like coconut ice cream.
[01:31:27.900 --> 01:31:30.600]   So ice cream aside,
[01:31:30.600 --> 01:31:35.780]   what do you most worry about in terms of catastrophic risk
[01:31:35.780 --> 01:31:40.180]   that will help us kind of make concrete
[01:31:40.180 --> 01:31:42.980]   the discussion we're having about
[01:31:42.980 --> 01:31:44.700]   how to fix this whole thing?
[01:31:44.700 --> 01:31:46.540]   - Yeah, I think it's worth taking
[01:31:46.540 --> 01:31:48.420]   a historical perspective briefly
[01:31:48.420 --> 01:31:49.940]   to just kind of orient everyone to it.
[01:31:49.940 --> 01:31:53.620]   We don't have to go all the way back to the aliens
[01:31:53.620 --> 01:31:54.880]   who've seen all of civilization,
[01:31:54.880 --> 01:31:59.580]   but to just recognize that for all of human history,
[01:31:59.580 --> 01:32:00.880]   as far as we're aware,
[01:32:00.880 --> 01:32:05.840]   there were existential risks to civilizations
[01:32:05.840 --> 01:32:07.300]   and they happened, right?
[01:32:07.300 --> 01:32:10.600]   Like there were civilizations that were killed in war,
[01:32:10.600 --> 01:32:13.760]   that tribes that were killed in tribal warfare, whatever.
[01:32:13.760 --> 01:32:15.940]   So people faced existential risk
[01:32:15.940 --> 01:32:18.120]   to the group that they identified with.
[01:32:18.120 --> 01:32:20.500]   It's just, those were local phenomena, right?
[01:32:20.500 --> 01:32:21.980]   It wasn't a fully global phenomena.
[01:32:21.980 --> 01:32:23.580]   So an empire could fall
[01:32:23.580 --> 01:32:25.260]   and surrounding empires didn't fall.
[01:32:25.260 --> 01:32:27.260]   Maybe they came in and filled the space.
[01:32:27.260 --> 01:32:33.220]   The first time that we were able to think
[01:32:33.220 --> 01:32:36.120]   about catastrophic risk, not from like a solar flare
[01:32:36.120 --> 01:32:37.380]   or something that we couldn't control,
[01:32:37.380 --> 01:32:39.580]   but from something that humans would actually create
[01:32:39.580 --> 01:32:42.900]   at a global level was World War II and the bomb.
[01:32:42.900 --> 01:32:45.180]   Because it was the first time that we had tech big enough
[01:32:45.180 --> 01:32:48.740]   that could actually mess up everything at a global level.
[01:32:48.740 --> 01:32:50.060]   It could mess up habitability.
[01:32:50.060 --> 01:32:52.940]   We just weren't powerful enough to do that before.
[01:32:52.940 --> 01:32:54.500]   It's not that we didn't behave in ways
[01:32:54.500 --> 01:32:55.340]   that would have done it.
[01:32:55.340 --> 01:32:57.180]   We just only behaved in those ways
[01:32:57.180 --> 01:32:59.020]   at the scale we could affect.
[01:32:59.020 --> 01:33:01.020]   And so it's important to get
[01:33:01.020 --> 01:33:04.360]   that there's the entire world before World War II,
[01:33:04.360 --> 01:33:05.780]   where we don't have the ability
[01:33:05.780 --> 01:33:09.220]   to make a non-habitable biosphere, non-habitable for us.
[01:33:09.220 --> 01:33:10.420]   And then there's World War II
[01:33:10.420 --> 01:33:13.040]   and the beginning of a completely new phase
[01:33:13.040 --> 01:33:16.580]   where global human induced catastrophic risk
[01:33:16.580 --> 01:33:17.580]   is now a real thing.
[01:33:17.580 --> 01:33:19.820]   And that was such a big deal
[01:33:19.820 --> 01:33:21.400]   that it changed the entire world
[01:33:21.400 --> 01:33:23.220]   in a really fundamental way,
[01:33:23.220 --> 01:33:26.420]   which is when you study history,
[01:33:26.420 --> 01:33:28.340]   it's amazing how big a percentage of history
[01:33:28.340 --> 01:33:29.420]   is studying war, right?
[01:33:29.420 --> 01:33:30.700]   And the history of wars,
[01:33:30.700 --> 01:33:32.660]   you said European history, whatever.
[01:33:32.660 --> 01:33:35.660]   It's generals and wars and empire expansions.
[01:33:35.660 --> 01:33:38.580]   And so the major empires near each other
[01:33:38.580 --> 01:33:40.540]   never had really long periods of time
[01:33:40.540 --> 01:33:41.820]   where they weren't engaged in war
[01:33:41.820 --> 01:33:44.100]   or preparation for war or something like that.
[01:33:44.100 --> 01:33:47.360]   That was, humans don't have a good precedent
[01:33:47.360 --> 01:33:49.660]   in the post tribal phase,
[01:33:49.660 --> 01:33:52.000]   the civilization phase of being able to solve conflicts
[01:33:52.000 --> 01:33:53.300]   without war for very long.
[01:33:53.300 --> 01:33:56.320]   World War II was the first time
[01:33:56.320 --> 01:33:59.240]   where we could have a war that no one could win.
[01:33:59.240 --> 01:34:02.800]   And so the superpowers couldn't fight again.
[01:34:02.800 --> 01:34:04.200]   They couldn't do a real kinetic war.
[01:34:04.200 --> 01:34:07.240]   They could do diplomatic wars and cold war type stuff,
[01:34:07.240 --> 01:34:09.380]   and they could fight proxy wars through other countries
[01:34:09.380 --> 01:34:11.180]   that didn't have the big weapons.
[01:34:11.180 --> 01:34:12.900]   And so mutually assured destruction
[01:34:12.900 --> 01:34:15.240]   and like coming out of World War II,
[01:34:15.240 --> 01:34:17.240]   we actually realized that nation states
[01:34:17.240 --> 01:34:19.520]   couldn't prevent world war.
[01:34:19.520 --> 01:34:22.560]   And so we needed a new type of supervening government
[01:34:22.560 --> 01:34:23.720]   in addition to nation states,
[01:34:23.720 --> 01:34:25.080]   which was the whole Bretton Woods world,
[01:34:25.080 --> 01:34:28.760]   the United Nations, the World Bank, the IMF,
[01:34:28.760 --> 01:34:31.980]   the globalization trade type agreements,
[01:34:31.980 --> 01:34:33.640]   mutually assured destruction.
[01:34:33.640 --> 01:34:36.320]   That was, how do we have some coordination
[01:34:36.320 --> 01:34:38.240]   beyond just nation states between them
[01:34:38.240 --> 01:34:41.800]   since we have to stop war between at least the superpowers?
[01:34:41.800 --> 01:34:44.120]   And it was pretty successful,
[01:34:44.120 --> 01:34:45.920]   given that we've had like 75 years
[01:34:45.920 --> 01:34:48.340]   of no superpower on superpower war.
[01:34:49.340 --> 01:34:52.940]   We've had lots of proxy wars during that time.
[01:34:52.940 --> 01:34:54.360]   We've had cold war.
[01:34:54.360 --> 01:34:57.940]   And I would say we're in a new phase now
[01:34:57.940 --> 01:35:02.020]   where the Bretton Woods solution is basically over,
[01:35:02.020 --> 01:35:03.300]   or almost over.
[01:35:03.300 --> 01:35:05.260]   - Can you describe the Bretton Woods solution?
[01:35:05.260 --> 01:35:07.240]   - Yeah, so the Bretton Woods,
[01:35:07.240 --> 01:35:12.240]   the series of agreements for how the nations
[01:35:12.240 --> 01:35:16.100]   would be able to engage with each other
[01:35:16.100 --> 01:35:19.780]   in a solution other than war was these IGOs,
[01:35:19.780 --> 01:35:22.020]   these intergovernmental organizations,
[01:35:22.020 --> 01:35:24.780]   and was the idea of globalization.
[01:35:24.780 --> 01:35:26.020]   Since we could have global effects,
[01:35:26.020 --> 01:35:28.380]   we needed to be able to think about things globally,
[01:35:28.380 --> 01:35:30.360]   where we had trade relationships with each other,
[01:35:30.360 --> 01:35:33.340]   where it would not be profitable to war with each other.
[01:35:33.340 --> 01:35:34.900]   It'd be more profitable to actually be able
[01:35:34.900 --> 01:35:35.820]   to trade with each other.
[01:35:35.820 --> 01:35:38.420]   So our own self-interest was gonna drive
[01:35:38.420 --> 01:35:40.040]   our non-war interest.
[01:35:40.040 --> 01:35:44.180]   And so this started to look like,
[01:35:44.180 --> 01:35:46.620]   and obviously this couldn't have happened
[01:35:46.620 --> 01:35:47.600]   that much earlier either,
[01:35:47.600 --> 01:35:49.740]   because industrialization hadn't gotten far enough
[01:35:49.740 --> 01:35:52.420]   to be able to do massive global industrial supply chains
[01:35:52.420 --> 01:35:54.700]   and ship stuff around quickly.
[01:35:54.700 --> 01:35:56.300]   But like we were mentioning earlier,
[01:35:56.300 --> 01:35:58.900]   almost all the electronics that we use today,
[01:35:58.900 --> 01:36:00.540]   just basic cheap stuff for us,
[01:36:00.540 --> 01:36:03.100]   is made on six continents, made in many countries.
[01:36:03.100 --> 01:36:04.380]   There's no single country in the world
[01:36:04.380 --> 01:36:06.460]   that could actually make many of the things that we have,
[01:36:06.460 --> 01:36:08.740]   and from the raw material extraction
[01:36:08.740 --> 01:36:12.620]   to the plastics and polymers and the et cetera.
[01:36:12.620 --> 01:36:15.020]   And so the idea that we made a world
[01:36:15.020 --> 01:36:17.420]   that could do that kind of trade
[01:36:17.420 --> 01:36:19.140]   and create massive GDP growth,
[01:36:19.140 --> 01:36:20.540]   we could all work together to be able
[01:36:20.540 --> 01:36:23.740]   to mine natural resources and grow stuff.
[01:36:23.740 --> 01:36:25.380]   With the rapid GDP growth,
[01:36:25.380 --> 01:36:28.060]   there was the idea that everybody could keep having more
[01:36:28.060 --> 01:36:30.580]   without having to take each other's stuff.
[01:36:30.580 --> 01:36:33.980]   And so that was part of kind of the Bretton Woods
[01:36:33.980 --> 01:36:35.460]   post-World War II model.
[01:36:35.460 --> 01:36:38.220]   The other was that we'd be so economically interdependent
[01:36:38.220 --> 01:36:41.000]   that blowing each other up would never make sense.
[01:36:41.000 --> 01:36:42.500]   That worked for a while.
[01:36:42.500 --> 01:36:48.500]   Now, it also brought us up into planetary boundaries faster,
[01:36:48.500 --> 01:36:51.500]   the unrenewable use of resource
[01:36:51.500 --> 01:36:54.040]   and turning those resources into pollution
[01:36:54.040 --> 01:36:56.580]   on the other side of the supply chain.
[01:36:56.580 --> 01:36:58.780]   So obviously that faster GDP growth
[01:36:58.780 --> 01:37:02.500]   meant the overfishing of the oceans
[01:37:02.500 --> 01:37:04.980]   and the cutting down of the trees and the climate change
[01:37:04.980 --> 01:37:08.980]   and the toxic mining tailings going into the water
[01:37:08.980 --> 01:37:10.780]   and the mountaintop removal mining
[01:37:10.780 --> 01:37:11.780]   and all those types of things.
[01:37:11.780 --> 01:37:13.280]   - That's the overconsumption side
[01:37:13.280 --> 01:37:15.340]   of the risk that we're talking about.
[01:37:15.340 --> 01:37:19.060]   - And so the answer of let's do positive GDP
[01:37:19.060 --> 01:37:23.280]   is the answer rapidly and exponentially
[01:37:23.280 --> 01:37:26.960]   obviously accelerated the planetary boundary side.
[01:37:26.960 --> 01:37:30.140]   And that was thought about for a long time,
[01:37:30.140 --> 01:37:32.940]   but it started to be modeled with the Club of Rome
[01:37:32.940 --> 01:37:34.040]   and limits of growth.
[01:37:34.040 --> 01:37:39.380]   But it's just very obvious to say
[01:37:39.380 --> 01:37:40.940]   if you have a linear materials economy
[01:37:40.940 --> 01:37:43.400]   where you take stuff out of the earth faster,
[01:37:43.400 --> 01:37:46.620]   whether it's fish or trees or oil,
[01:37:46.620 --> 01:37:47.960]   you take it out of the earth faster
[01:37:47.960 --> 01:37:49.940]   than it can replenish itself.
[01:37:49.940 --> 01:37:51.860]   And you turn it into trash
[01:37:51.860 --> 01:37:53.240]   after using it for a short period of time,
[01:37:53.240 --> 01:37:55.060]   you put the trash in the environment faster
[01:37:55.060 --> 01:37:56.740]   than it can process itself.
[01:37:56.740 --> 01:37:59.900]   And there's toxicity associated with both sides of this.
[01:37:59.900 --> 01:38:02.500]   You can't run an exponentially growing
[01:38:02.500 --> 01:38:05.080]   linear materials economy on a finite planet forever.
[01:38:05.080 --> 01:38:06.700]   That's not a hard thing to figure out.
[01:38:06.700 --> 01:38:08.500]   And it has to be exponential
[01:38:08.500 --> 01:38:11.660]   if there's an exponentiation in the monetary supply
[01:38:11.660 --> 01:38:14.500]   because of interest and then fractional reserve banking.
[01:38:14.500 --> 01:38:16.140]   And to then be able to keep up
[01:38:16.140 --> 01:38:17.400]   with the growing monetary supply,
[01:38:17.400 --> 01:38:19.300]   you have to have growth of goods and services.
[01:38:19.300 --> 01:38:21.700]   So that's that kind of thing that has happened.
[01:38:21.700 --> 01:38:27.140]   But you also see that when you get these supply chains
[01:38:27.140 --> 01:38:29.060]   that are so interconnected across the world,
[01:38:29.060 --> 01:38:30.380]   you get increased fragility
[01:38:30.380 --> 01:38:32.740]   'cause a collapse or a problem in one area
[01:38:32.740 --> 01:38:35.060]   then affects the whole world in a much bigger area
[01:38:35.060 --> 01:38:37.500]   as opposed to the issues being local.
[01:38:37.500 --> 01:38:39.540]   So we got to see with COVID
[01:38:39.540 --> 01:38:42.780]   and an issue that started in one part of China
[01:38:42.780 --> 01:38:45.660]   affecting the whole world so much more rapidly
[01:38:45.660 --> 01:38:48.580]   than would have happened before Bretton Woods, right?
[01:38:48.580 --> 01:38:50.980]   Before international travel supply chains,
[01:38:50.980 --> 01:38:52.460]   you know, that whole kind of thing.
[01:38:52.460 --> 01:38:54.300]   And with a bunch of second and third order effects
[01:38:54.300 --> 01:38:55.500]   that people wouldn't have predicted, okay?
[01:38:55.500 --> 01:38:57.940]   We have to stop certain kinds of travel
[01:38:57.940 --> 01:38:59.300]   because of viral contaminants,
[01:38:59.300 --> 01:39:02.820]   but the countries doing agriculture
[01:39:02.820 --> 01:39:04.700]   depend upon fertilizer they don't produce
[01:39:04.700 --> 01:39:05.660]   that is shipped into them
[01:39:05.660 --> 01:39:07.420]   and depend upon pesticides they don't produce.
[01:39:07.420 --> 01:39:09.580]   So we got both crop failures
[01:39:09.580 --> 01:39:11.420]   and crops being eaten by locusts
[01:39:11.420 --> 01:39:13.820]   in scale in Northern Africa and Iran and things like that
[01:39:13.820 --> 01:39:15.520]   because they couldn't get the supplies of stuff in.
[01:39:15.520 --> 01:39:17.460]   So then you get massive starvation
[01:39:17.460 --> 01:39:19.140]   or future kind of hunger issues
[01:39:19.140 --> 01:39:21.980]   because of supply chain shutdowns.
[01:39:21.980 --> 01:39:25.100]   So you get this increased fragility and cascade dynamics
[01:39:25.100 --> 01:39:28.900]   where a small problem can end up leading to cascade effects.
[01:39:28.900 --> 01:39:33.900]   And also we went from two superpowers
[01:39:33.900 --> 01:39:35.620]   with one catastrophe weapon
[01:39:37.100 --> 01:39:40.980]   to now that same catastrophe weapon
[01:39:40.980 --> 01:39:44.620]   is there's more countries that have it,
[01:39:44.620 --> 01:39:46.500]   eight or nine countries that have it.
[01:39:46.500 --> 01:39:50.220]   And there's a lot more types of catastrophe weapons.
[01:39:50.220 --> 01:39:53.580]   We now have catastrophe weapons with weaponized drones
[01:39:53.580 --> 01:39:56.180]   that can hit infrastructure targets with bio with,
[01:39:56.180 --> 01:39:59.680]   in fact, every new type of tech has created an arms race.
[01:39:59.680 --> 01:40:02.060]   So we have not with the UN
[01:40:02.060 --> 01:40:04.140]   or the other kind of intergovernmental organizations,
[01:40:04.140 --> 01:40:07.620]   we haven't been able to really do nuclear deproliferation.
[01:40:07.620 --> 01:40:09.860]   We've actually had more countries get nukes
[01:40:09.860 --> 01:40:11.260]   and keep getting faster nukes,
[01:40:11.260 --> 01:40:13.920]   the race to hypersonics and things like that.
[01:40:13.920 --> 01:40:17.700]   And every new type of technology that has emerged
[01:40:17.700 --> 01:40:19.560]   has created an arms race.
[01:40:19.560 --> 01:40:23.820]   And so you can't do mutually assured destruction
[01:40:23.820 --> 01:40:26.780]   with multiple agents the way you can with two agents.
[01:40:26.780 --> 01:40:28.700]   Two agents, it's much easier
[01:40:28.700 --> 01:40:31.620]   to create a stable Nash equilibrium that's forced.
[01:40:31.620 --> 01:40:32.780]   But the ability to monitor and say,
[01:40:32.780 --> 01:40:34.020]   if these guys shoot, who do I shoot?
[01:40:34.020 --> 01:40:34.840]   Do I shoot them?
[01:40:34.840 --> 01:40:36.100]   Do I shoot everybody?
[01:40:36.100 --> 01:40:37.700]   And so you get a three body problem.
[01:40:37.700 --> 01:40:39.520]   You get a very complex type of thing
[01:40:39.520 --> 01:40:40.540]   when you have multiple agents
[01:40:40.540 --> 01:40:42.900]   and multiple different types of catastrophe weapons,
[01:40:42.900 --> 01:40:45.380]   including ones that can be much more easily produced
[01:40:45.380 --> 01:40:46.220]   than nukes.
[01:40:46.220 --> 01:40:47.040]   Nukes are really hard to produce.
[01:40:47.040 --> 01:40:48.500]   There's only uranium in a few areas.
[01:40:48.500 --> 01:40:50.020]   Uranium enrichment is hard.
[01:40:50.020 --> 01:40:51.600]   ICBMs are hard.
[01:40:51.600 --> 01:40:55.380]   But weaponized drones hitting smart targets is not so hard.
[01:40:55.380 --> 01:40:57.560]   There's a lot of other things where basically the scale
[01:40:57.560 --> 01:40:59.940]   at being able to manufacture them is going way, way down
[01:40:59.940 --> 01:41:02.680]   to where even non-state actors can have them.
[01:41:02.680 --> 01:41:06.560]   And so when we talk about exponential tech
[01:41:06.560 --> 01:41:09.500]   and the decentralization of exponential tech,
[01:41:09.500 --> 01:41:13.060]   what that means is decentralized catastrophe weapon capacity.
[01:41:13.060 --> 01:41:16.420]   And especially in a world of increasing numbers
[01:41:16.420 --> 01:41:19.300]   of people feeling disenfranchised, frantic,
[01:41:19.300 --> 01:41:21.340]   whatever, for different reasons.
[01:41:21.340 --> 01:41:26.340]   So I would say the Bretton Woods world doesn't prepare us
[01:41:26.340 --> 01:41:29.260]   to be able to deal with lots of different agents,
[01:41:29.260 --> 01:41:31.540]   having lots of different types of catastrophe weapons
[01:41:31.540 --> 01:41:33.680]   you can't put mutually assured destruction on,
[01:41:33.680 --> 01:41:36.840]   where you can't keep doing growth of the materials economy
[01:41:36.840 --> 01:41:40.900]   in the same way because of hitting planetary boundaries
[01:41:40.900 --> 01:41:44.060]   and where the fragility dynamics are actually now
[01:41:44.060 --> 01:41:46.100]   their own source of catastrophic risk.
[01:41:46.100 --> 01:41:48.220]   So now we're, so like there was all the world
[01:41:48.220 --> 01:41:49.220]   until World War II.
[01:41:49.220 --> 01:41:52.240]   And World War II is just from a civilization timescale
[01:41:52.240 --> 01:41:54.620]   point of view, it was just a second ago.
[01:41:54.620 --> 01:41:56.700]   It seems like a long time, but it is really not.
[01:41:56.700 --> 01:41:58.900]   We get a short period of relative peace at the level
[01:41:58.900 --> 01:42:01.500]   of superpowers while building up the military capacity
[01:42:01.500 --> 01:42:04.340]   for much, much, much worse war the entire time.
[01:42:04.340 --> 01:42:07.600]   And then now we're at this new phase where the things
[01:42:07.600 --> 01:42:10.940]   that allowed us to make it through the nuclear power
[01:42:10.940 --> 01:42:13.100]   are not the same systems that will let us make it
[01:42:13.100 --> 01:42:14.660]   through the next stage.
[01:42:14.660 --> 01:42:17.460]   So what is this next post Bretton Woods?
[01:42:17.460 --> 01:42:22.460]   How do we become safe vessels, safe stewards
[01:42:22.460 --> 01:42:26.220]   of many different types of exponential technology
[01:42:26.220 --> 01:42:30.020]   is a key question when we're thinking about X-Risk.
[01:42:30.020 --> 01:42:35.020]   - Okay, so, and I'd like to try to answer the how
[01:42:35.020 --> 01:42:40.980]   a few ways, but first on the mutually assured destruction.
[01:42:40.980 --> 01:42:46.460]   Do you give credit to the idea of two superpowers
[01:42:46.460 --> 01:42:49.740]   not blowing each other up with nuclear weapons
[01:42:49.740 --> 01:42:51.980]   to the simple game theoretic model
[01:42:51.980 --> 01:42:53.300]   of mutually assured destruction,
[01:42:53.300 --> 01:42:56.460]   or something you've said previously,
[01:42:56.460 --> 01:42:59.060]   this idea of inverse correlation,
[01:42:59.060 --> 01:43:02.300]   which I tend to believe between,
[01:43:02.300 --> 01:43:05.820]   now you were talking about tech,
[01:43:05.820 --> 01:43:09.500]   but I think it's maybe broadly true.
[01:43:09.500 --> 01:43:11.860]   The inverse correlation between competence
[01:43:11.860 --> 01:43:13.900]   and propensity for destruction.
[01:43:13.900 --> 01:43:18.900]   So the bigger your weapons,
[01:43:18.900 --> 01:43:22.260]   not because you're afraid of mutually assured
[01:43:22.260 --> 01:43:24.460]   self-destruction, but because we're human beings
[01:43:24.460 --> 01:43:28.700]   and there's a deep moral fortitude there
[01:43:28.700 --> 01:43:30.820]   that somehow aligned with competence
[01:43:30.820 --> 01:43:32.260]   and being good at your job.
[01:43:32.260 --> 01:43:37.260]   That like, it's very hard to be a psychopath
[01:43:37.260 --> 01:43:41.860]   and be good at killing at scale.
[01:43:41.860 --> 01:43:46.420]   Do you share any of that intuition?
[01:43:46.420 --> 01:43:47.260]   - Kind of.
[01:43:47.260 --> 01:43:51.820]   I think most people would say that Alexander the Great
[01:43:51.820 --> 01:43:55.220]   and Genghis Khan and Napoleon were effective people
[01:43:55.220 --> 01:43:56.620]   that were good at their job.
[01:43:58.180 --> 01:44:01.620]   That were actually maybe asymmetrically good
[01:44:01.620 --> 01:44:03.820]   at being able to organize people
[01:44:03.820 --> 01:44:06.180]   and do certain kinds of things
[01:44:06.180 --> 01:44:08.460]   that were pretty oriented
[01:44:08.460 --> 01:44:11.100]   towards certain types of destruction.
[01:44:11.100 --> 01:44:13.020]   Or pretty willing to,
[01:44:13.020 --> 01:44:14.220]   maybe they would say they were oriented
[01:44:14.220 --> 01:44:15.340]   towards empire expansion,
[01:44:15.340 --> 01:44:18.300]   but pretty willing to commit certain acts of destruction
[01:44:18.300 --> 01:44:19.340]   in the name of it.
[01:44:19.340 --> 01:44:20.780]   - What are you worried about?
[01:44:20.780 --> 01:44:25.780]   The Genghis Khan, or you could argue he's not a psychopath.
[01:44:27.580 --> 01:44:30.140]   Are you worried about Genghis Khan,
[01:44:30.140 --> 01:44:31.180]   are you worried about Hitler,
[01:44:31.180 --> 01:44:33.900]   or are you worried about a terrorist
[01:44:33.900 --> 01:44:37.900]   who has a very different ethic,
[01:44:37.900 --> 01:44:41.100]   which is not even for,
[01:44:41.100 --> 01:44:43.820]   it's not trying to preserve and build
[01:44:43.820 --> 01:44:46.500]   and expand my community.
[01:44:46.500 --> 01:44:50.460]   It's more about just the destruction in itself is the goal.
[01:44:50.460 --> 01:44:53.540]   - I think the thing that you're looking at
[01:44:53.540 --> 01:44:54.540]   that I do agree with
[01:44:54.620 --> 01:44:57.740]   is that there's a psychological disposition
[01:44:57.740 --> 01:44:59.100]   towards construction
[01:44:59.100 --> 01:45:02.740]   and a psychological disposition more towards destruction.
[01:45:02.740 --> 01:45:05.900]   Obviously everybody has both and can toggle between both.
[01:45:05.900 --> 01:45:09.180]   And oftentimes one is willing to destroy certain things.
[01:45:09.180 --> 01:45:10.860]   We have this idea of creative destruction, right?
[01:45:10.860 --> 01:45:13.700]   Willing to destroy certain things to create other things.
[01:45:13.700 --> 01:45:16.460]   And utilitarianism and trolley problems
[01:45:16.460 --> 01:45:18.020]   are all about exploring that space.
[01:45:18.020 --> 01:45:20.700]   And the idea of war is all about that.
[01:45:20.700 --> 01:45:23.220]   I am trying to create something for our people
[01:45:23.220 --> 01:45:25.460]   and it requires destroying some other people.
[01:45:25.460 --> 01:45:30.100]   Sociopathy is a funny topic
[01:45:30.100 --> 01:45:32.060]   'cause it's possible to have very high fealty
[01:45:32.060 --> 01:45:35.100]   to your in-group and work on perfecting the methods
[01:45:35.100 --> 01:45:38.300]   of torture to the out-group at the same time
[01:45:38.300 --> 01:45:40.860]   'cause you can dehumanize and then remove empathy.
[01:45:40.860 --> 01:45:47.780]   And I would also say that there are types.
[01:45:47.780 --> 01:45:51.300]   So the reason, the thing that gives hope
[01:45:51.300 --> 01:45:55.220]   about the orientation towards construction and destruction
[01:45:55.220 --> 01:45:56.980]   being a little different in psychologies
[01:45:56.980 --> 01:46:01.500]   is what it takes to build really catastrophic tech,
[01:46:01.500 --> 01:46:03.260]   even today where it doesn't take what it took
[01:46:03.260 --> 01:46:05.860]   to make a nuke, a small group of people could do it,
[01:46:05.860 --> 01:46:09.260]   takes still some real technical knowledge
[01:46:09.260 --> 01:46:11.420]   that required having studied for a while
[01:46:11.420 --> 01:46:13.900]   and some then building capacity.
[01:46:13.900 --> 01:46:15.500]   And there's a question of,
[01:46:15.500 --> 01:46:18.260]   is that psychologically inversely correlated
[01:46:18.260 --> 01:46:22.100]   with the desire to damage civilization meaningfully?
[01:46:22.100 --> 01:46:26.860]   A little bit, a little bit, I think.
[01:46:26.860 --> 01:46:29.180]   - I think a lot.
[01:46:29.180 --> 01:46:31.580]   I think it's actually, I mean,
[01:46:31.580 --> 01:46:33.340]   this is the conversation I had,
[01:46:33.340 --> 01:46:35.220]   I think offline with Dan Carlin,
[01:46:35.220 --> 01:46:39.620]   which is like, it's pretty easy to come up with ways
[01:46:39.620 --> 01:46:42.960]   that any competent, I can come up with a lot of ways
[01:46:42.960 --> 01:46:45.000]   to hurt a lot of people.
[01:46:45.000 --> 01:46:46.300]   And it's pretty easy.
[01:46:46.300 --> 01:46:48.220]   Like I alone could do it.
[01:46:48.220 --> 01:46:53.220]   And there's a lot of people as smart or smarter than me,
[01:46:53.220 --> 01:46:57.820]   at least in their creation of explosives.
[01:46:57.820 --> 01:47:02.820]   Why are we not seeing more insane mass murder?
[01:47:02.820 --> 01:47:06.380]   - I think there is something fascinating
[01:47:06.380 --> 01:47:08.500]   and beautiful about this.
[01:47:08.500 --> 01:47:09.340]   - Yes.
[01:47:09.340 --> 01:47:12.700]   - And it does have to do with some deeply pro-social
[01:47:12.700 --> 01:47:14.640]   types of characteristics in humans.
[01:47:17.340 --> 01:47:20.620]   But when you're dealing with very large numbers,
[01:47:20.620 --> 01:47:23.180]   you don't need a whole lot of a phenomena.
[01:47:23.180 --> 01:47:24.260]   And so then you start to say,
[01:47:24.260 --> 01:47:26.980]   well, what's the probability that X won't happen this year,
[01:47:26.980 --> 01:47:28.500]   then won't happen in the next two years,
[01:47:28.500 --> 01:47:30.060]   three years, four years.
[01:47:30.060 --> 01:47:33.020]   And then how many people are doing destructive things
[01:47:33.020 --> 01:47:33.940]   with lower tech?
[01:47:33.940 --> 01:47:36.220]   And then how many of them can get access to higher tech
[01:47:36.220 --> 01:47:39.020]   that they didn't have to figure out how to build?
[01:47:39.020 --> 01:47:43.580]   So when I can get commercial tech,
[01:47:43.580 --> 01:47:46.300]   and maybe I don't understand tech very well,
[01:47:46.300 --> 01:47:48.520]   but I understand it well enough to utilize it,
[01:47:48.520 --> 01:47:51.340]   not to create it, and I can repurpose it.
[01:47:51.340 --> 01:47:54.860]   When we saw that commercial drone
[01:47:54.860 --> 01:47:55.980]   with a homemade thermite bomb
[01:47:55.980 --> 01:47:58.340]   hit the Ukrainian munitions factory
[01:47:58.340 --> 01:48:02.540]   and do the equivalent of an incendiary bomb level of damage,
[01:48:02.540 --> 01:48:04.180]   that was just home tech.
[01:48:04.180 --> 01:48:06.340]   That's just simple kind of thing.
[01:48:06.340 --> 01:48:09.800]   And so the question is not,
[01:48:09.800 --> 01:48:14.060]   does it stay being a small percentage of the population?
[01:48:14.060 --> 01:48:17.660]   The question is, can you bind that phenomena
[01:48:17.660 --> 01:48:18.980]   nearly completely?
[01:48:18.980 --> 01:48:25.740]   And especially now as you start to get into bigger things,
[01:48:25.740 --> 01:48:29.460]   CRISPR gene drive technologies and various things like that,
[01:48:29.460 --> 01:48:33.240]   can you bind it completely long-term?
[01:48:33.240 --> 01:48:35.860]   Over what period of time?
[01:48:35.860 --> 01:48:37.100]   - Not perfectly though.
[01:48:37.100 --> 01:48:37.940]   That's the thing.
[01:48:37.940 --> 01:48:40.020]   I'm trying to say that there is some,
[01:48:41.260 --> 01:48:45.540]   let's call it, let's, a random word, love,
[01:48:45.540 --> 01:48:50.460]   that's inherent and that's core to human nature,
[01:48:50.460 --> 01:48:54.060]   that's preventing destruction at scale.
[01:48:54.060 --> 01:48:57.700]   And you're saying, yeah, but there's a lot of humans.
[01:48:57.700 --> 01:48:59.860]   There's gonna be eight plus billion,
[01:48:59.860 --> 01:49:01.760]   and then there's a lot of seconds in the day
[01:49:01.760 --> 01:49:02.820]   to come up with stuff.
[01:49:02.820 --> 01:49:04.260]   There's a lot of pain in the world
[01:49:04.260 --> 01:49:07.420]   that can lead to a distorted view of the world
[01:49:07.420 --> 01:49:10.560]   such that you want to channel that pain into the destruction.
[01:49:10.560 --> 01:49:11.580]   All those kinds of things.
[01:49:11.580 --> 01:49:13.180]   And it's only a matter of time
[01:49:13.180 --> 01:49:15.460]   that any one individual could do large damage,
[01:49:15.460 --> 01:49:20.460]   especially as we create more and more democratized,
[01:49:20.460 --> 01:49:22.620]   decentralized ways to deliver that damage,
[01:49:22.620 --> 01:49:25.740]   even if you don't know how to build the initial weapon.
[01:49:25.740 --> 01:49:30.060]   But the thing is, it seems like it's a race
[01:49:30.060 --> 01:49:35.060]   between the cheapening of destructive weapons
[01:49:36.820 --> 01:49:41.820]   and the capacity of humans to express their love
[01:49:41.820 --> 01:49:43.160]   towards each other.
[01:49:43.160 --> 01:49:48.160]   And it's a race that so far, I know on Twitter,
[01:49:48.160 --> 01:49:51.900]   it's not popular to say, but love is winning, okay?
[01:49:51.900 --> 01:49:55.200]   So what is the argument that love is going to lose here
[01:49:55.200 --> 01:50:00.200]   against nuclear weapons and biotech and AI and drones?
[01:50:00.200 --> 01:50:05.520]   - Okay, I'm gonna come at the end of this
[01:50:05.520 --> 01:50:07.200]   to a how love wins.
[01:50:07.200 --> 01:50:09.560]   So I just want you to know that that's where I'm oriented.
[01:50:09.560 --> 01:50:10.400]   - That's the end, okay.
[01:50:10.400 --> 01:50:14.640]   - But I'm gonna argue against why that is a given
[01:50:14.640 --> 01:50:19.040]   because it's not a given.
[01:50:19.040 --> 01:50:20.080]   I don't believe.
[01:50:20.080 --> 01:50:20.920]   And I think that-
[01:50:20.920 --> 01:50:22.560]   - This is like a good romantic comedy.
[01:50:22.560 --> 01:50:25.120]   So you're gonna create drama right now,
[01:50:25.120 --> 01:50:27.000]   but it will end in a happy ending.
[01:50:27.000 --> 01:50:28.720]   - Well, it's because it's only a happy ending
[01:50:28.720 --> 01:50:30.760]   if we actually understand the issues well enough
[01:50:30.760 --> 01:50:32.520]   and take responsibility to shift it.
[01:50:32.520 --> 01:50:34.480]   Do I believe, like, there's a reason
[01:50:34.480 --> 01:50:36.560]   why there's so much more dystopic sci-fi
[01:50:36.560 --> 01:50:38.040]   than protopic sci-fi.
[01:50:38.040 --> 01:50:41.880]   And the some protopic sci-fi usually requires magic
[01:50:41.880 --> 01:50:46.880]   is because, or at least magical tech, right?
[01:50:46.880 --> 01:50:49.360]   Dilithium crystals and warp drives and stuff.
[01:50:49.360 --> 01:50:53.040]   Because it's very hard to imagine people
[01:50:53.040 --> 01:50:55.480]   like the people we have been in the history books
[01:50:55.480 --> 01:51:01.480]   with exponential type technology and power
[01:51:01.480 --> 01:51:03.920]   that don't eventually blow themselves up,
[01:51:03.920 --> 01:51:05.480]   that make good enough choices
[01:51:05.480 --> 01:51:07.600]   as stewards of their environment and their commons
[01:51:07.600 --> 01:51:09.720]   and each other and et cetera.
[01:51:09.720 --> 01:51:11.920]   So like, it's easier to think of scenarios
[01:51:11.920 --> 01:51:12.960]   where we blow ourselves up
[01:51:12.960 --> 01:51:14.000]   than it is to think of scenarios
[01:51:14.000 --> 01:51:15.520]   where we avoid every single scenario
[01:51:15.520 --> 01:51:16.680]   where we blow ourselves up.
[01:51:16.680 --> 01:51:17.680]   And when I say blow ourselves up,
[01:51:17.680 --> 01:51:20.440]   I mean the environmental versions,
[01:51:20.440 --> 01:51:22.800]   the terrorist versions, the war versions,
[01:51:22.800 --> 01:51:25.420]   the cumulative externalities versions.
[01:51:25.420 --> 01:51:31.200]   - And I'm sorry if I'm interrupting your flow of thought,
[01:51:31.200 --> 01:51:32.680]   but why is it easier?
[01:51:33.640 --> 01:51:35.680]   Could it be a weird psychological thing
[01:51:35.680 --> 01:51:38.000]   where we either are just more capable
[01:51:38.000 --> 01:51:41.120]   to visualize explosions and destruction?
[01:51:41.120 --> 01:51:42.760]   And then the sicker thought,
[01:51:42.760 --> 01:51:45.760]   which is like we kind of enjoy for some weird reason
[01:51:45.760 --> 01:51:47.000]   thinking about that kind of stuff,
[01:51:47.000 --> 01:51:49.840]   even though we wouldn't actually act on it.
[01:51:49.840 --> 01:51:51.640]   It's almost like some weird,
[01:51:51.640 --> 01:51:53.600]   like I love playing shooter games,
[01:51:53.600 --> 01:51:56.200]   first person shooters.
[01:51:56.200 --> 01:51:59.720]   And like, especially if it's like murdering zombie doom,
[01:51:59.720 --> 01:52:01.440]   you're shooting demons.
[01:52:01.440 --> 01:52:03.400]   I played one of my favorite games, Diablos,
[01:52:03.400 --> 01:52:05.880]   like slashing through different monsters
[01:52:05.880 --> 01:52:08.360]   and the screaming and pain and the hellfire.
[01:52:08.360 --> 01:52:11.000]   And then I go out into the real world
[01:52:11.000 --> 01:52:13.360]   to eat my coconut ice cream and I'm all about love.
[01:52:13.360 --> 01:52:17.560]   So like, can we trust our ability to visualize
[01:52:17.560 --> 01:52:19.720]   how it all goes to shit
[01:52:19.720 --> 01:52:22.640]   as an actual rational way of thinking?
[01:52:22.640 --> 01:52:25.920]   - I think it's a fair question to say to what degree
[01:52:25.920 --> 01:52:28.760]   is there just kind of perverse fantasy
[01:52:28.760 --> 01:52:32.000]   and morbid exploration
[01:52:32.000 --> 01:52:34.520]   and whatever else that happens in our imagination.
[01:52:34.520 --> 01:52:38.040]   But I don't think that's the whole of it.
[01:52:38.040 --> 01:52:41.080]   I think there is also a reality
[01:52:41.080 --> 01:52:43.720]   to the combinatorial possibility space
[01:52:43.720 --> 01:52:45.280]   and the difference in the probabilities
[01:52:45.280 --> 01:52:48.760]   that there's a lot of ways I could try to put
[01:52:48.760 --> 01:52:50.920]   the 70 trillion cells of your body together
[01:52:50.920 --> 01:52:52.000]   that don't make you.
[01:52:52.000 --> 01:52:54.800]   There's not that many ways I can put them together
[01:52:54.800 --> 01:52:55.640]   that make you.
[01:52:55.640 --> 01:52:56.480]   There's a lot of ways I could try
[01:52:56.480 --> 01:52:57.720]   to connect the organs together
[01:52:57.720 --> 01:53:00.600]   that make some weird kind of group of organs
[01:53:00.600 --> 01:53:02.840]   on a desk but that doesn't actually
[01:53:02.840 --> 01:53:04.320]   make a functioning human.
[01:53:04.320 --> 01:53:08.400]   And you can kill an adult human in a second,
[01:53:08.400 --> 01:53:09.720]   but you can't get one in a second.
[01:53:09.720 --> 01:53:11.280]   It takes 20 years to grow one
[01:53:11.280 --> 01:53:12.680]   and a lot of things to happen right.
[01:53:12.680 --> 01:53:14.480]   I could destroy this building
[01:53:14.480 --> 01:53:17.880]   in a couple minutes with demolition,
[01:53:17.880 --> 01:53:20.320]   but it took a year or a couple years to build it.
[01:53:20.320 --> 01:53:21.160]   There is--
[01:53:21.160 --> 01:53:25.160]   - Calm down, Cole, this is just an example.
[01:53:25.160 --> 01:53:26.600]   It's not, he doesn't mean it.
[01:53:27.640 --> 01:53:31.200]   - There's a gradient where entropy is easier
[01:53:31.200 --> 01:53:34.000]   and that there's a lot more ways
[01:53:34.000 --> 01:53:36.840]   to put a set of things together that don't work
[01:53:36.840 --> 01:53:40.520]   than the few that really do produce higher order synergies.
[01:53:40.520 --> 01:53:42.440]   And so,
[01:53:42.440 --> 01:53:46.920]   when we look at a history of war
[01:53:46.920 --> 01:53:51.360]   and then we look at exponentially more powerful warfare,
[01:53:51.360 --> 01:53:53.600]   an arms race that drives that in all these directions,
[01:53:53.600 --> 01:53:56.000]   and when we look at a history of environmental destruction
[01:53:56.000 --> 01:53:57.560]   and exponentially more powerful tech
[01:53:57.560 --> 01:53:59.400]   that makes exponential externalities
[01:53:59.400 --> 01:54:01.120]   multiplied by the total number of agents
[01:54:01.120 --> 01:54:03.520]   that are doing it and the cumulative effects,
[01:54:03.520 --> 01:54:05.600]   there's a lot of ways the whole thing can break,
[01:54:05.600 --> 01:54:07.400]   like a lot of different ways.
[01:54:07.400 --> 01:54:08.720]   And for it to get ahead,
[01:54:08.720 --> 01:54:10.520]   it has to have none of those happen.
[01:54:10.520 --> 01:54:15.080]   And so, there's just a probability space
[01:54:15.080 --> 01:54:16.480]   where it's easier to imagine that thing.
[01:54:16.480 --> 01:54:20.000]   So, to say how do we have a protopic future,
[01:54:20.000 --> 01:54:23.240]   we have to say, well, one criteria must be
[01:54:23.240 --> 01:54:25.680]   that it avoids all of the catastrophic risks.
[01:54:25.680 --> 01:54:26.840]   So, can we understand,
[01:54:26.840 --> 01:54:28.600]   can we inventory all the catastrophic risks?
[01:54:28.600 --> 01:54:30.480]   Can we inventory the patterns of human behavior
[01:54:30.480 --> 01:54:32.040]   that give rise to them?
[01:54:32.040 --> 01:54:34.920]   And could we try to solve for that?
[01:54:34.920 --> 01:54:37.160]   And could we have that be the essence
[01:54:37.160 --> 01:54:39.840]   of the social technology that we're thinking about
[01:54:39.840 --> 01:54:41.680]   to be able to guide, bind, and direct
[01:54:41.680 --> 01:54:42.800]   a new physical technology?
[01:54:42.800 --> 01:54:45.200]   'Cause so far, our physical technology,
[01:54:45.200 --> 01:54:48.120]   like we were talking about the Genghis Khans and like that,
[01:54:48.120 --> 01:54:51.240]   that obviously use certain kinds of physical technology
[01:54:51.240 --> 01:54:53.640]   and armaments and also social technology
[01:54:53.640 --> 01:54:57.760]   and unconventional warfare for a particular set of purposes.
[01:54:57.760 --> 01:54:59.880]   But we have things that don't look like warfare,
[01:54:59.880 --> 01:55:03.680]   like Rockefeller and standard oil.
[01:55:03.680 --> 01:55:07.040]   And it looked like a constructive mindset
[01:55:07.040 --> 01:55:12.040]   to be able to bring this new energy resource to the world.
[01:55:12.040 --> 01:55:13.840]   And it did.
[01:55:13.840 --> 01:55:18.840]   And the second order effects of that are climate change
[01:55:18.840 --> 01:55:23.240]   and all of the oil spills that have happened and will happen.
[01:55:23.240 --> 01:55:25.880]   And all of the wars in the Middle East
[01:55:25.880 --> 01:55:27.720]   over the oil that had been there
[01:55:27.720 --> 01:55:30.800]   and the massive political clusterfuck
[01:55:30.800 --> 01:55:33.520]   and human life issues that are associated with it
[01:55:33.520 --> 01:55:35.000]   and on and on, right?
[01:55:35.000 --> 01:55:41.400]   And so it's also not just the orientation
[01:55:41.400 --> 01:55:44.280]   to construct a thing can have a narrow focus
[01:55:44.280 --> 01:55:45.480]   on what I'm trying to construct,
[01:55:45.480 --> 01:55:47.160]   but be affecting a lot of other things
[01:55:47.160 --> 01:55:48.480]   through second and third order effects
[01:55:48.480 --> 01:55:50.200]   I'm not taking responsibility for.
[01:55:50.200 --> 01:55:53.560]   - And you often, on another tangent,
[01:55:53.560 --> 01:55:57.160]   mentioned second, third, and fourth order effects.
[01:55:57.160 --> 01:55:58.240]   - And the order.
[01:55:58.240 --> 01:55:59.560]   - And the order. - Cascading.
[01:55:59.560 --> 01:56:01.880]   - Which is really fascinating.
[01:56:01.880 --> 01:56:04.680]   Like starting with the third order,
[01:56:04.680 --> 01:56:08.280]   plus it gets really interesting.
[01:56:08.280 --> 01:56:10.280]   'Cause we don't even acknowledge
[01:56:10.280 --> 01:56:11.800]   like the second order effects.
[01:56:11.800 --> 01:56:12.960]   - Right.
[01:56:12.960 --> 01:56:14.680]   - But like thinking, 'cause those,
[01:56:14.680 --> 01:56:17.040]   it could get bigger and bigger and bigger
[01:56:17.040 --> 01:56:18.920]   in ways we were not anticipating.
[01:56:18.920 --> 01:56:21.720]   So how do we make those, so it sounds like part of the,
[01:56:21.720 --> 01:56:26.000]   part of the thing that you are thinking through
[01:56:26.000 --> 01:56:30.280]   in terms of a solution, how to create an anti-fragile,
[01:56:30.280 --> 01:56:34.920]   a resilient society, is to make explicit,
[01:56:34.920 --> 01:56:39.520]   acknowledge, understand the externalities,
[01:56:39.520 --> 01:56:42.560]   the second order, third order, fourth order,
[01:56:42.560 --> 01:56:44.720]   and the order effects.
[01:56:44.720 --> 01:56:46.960]   How do we start to think about those effects?
[01:56:47.920 --> 01:56:50.320]   - Yeah, the war application is harm we're trying to cause,
[01:56:50.320 --> 01:56:53.120]   or that we're aware we're causing, right?
[01:56:53.120 --> 01:56:55.960]   The externality is harm that, at least supposedly,
[01:56:55.960 --> 01:56:57.080]   we're not aware we're causing,
[01:56:57.080 --> 01:56:59.360]   or at minimum, it's not our intention, right?
[01:56:59.360 --> 01:57:01.280]   Maybe we're either totally unaware of it,
[01:57:01.280 --> 01:57:02.120]   or we're aware of it,
[01:57:02.120 --> 01:57:04.480]   but it is a side effect of what our intention is.
[01:57:04.480 --> 01:57:06.400]   It's not the intention itself.
[01:57:06.400 --> 01:57:08.920]   There are catastrophic risks from both types.
[01:57:08.920 --> 01:57:12.640]   The direct application of increased technological power
[01:57:12.640 --> 01:57:14.920]   to a rivalrous intent,
[01:57:16.080 --> 01:57:18.360]   which is gonna cause harm for some out-group
[01:57:18.360 --> 01:57:19.520]   for some in-group to win,
[01:57:19.520 --> 01:57:21.840]   but the out-group is also working on growing the tech,
[01:57:21.840 --> 01:57:23.640]   and if they don't lose completely,
[01:57:23.640 --> 01:57:25.800]   they reverse engineer the tech, upregulate it,
[01:57:25.800 --> 01:57:27.760]   come back with more capacity.
[01:57:27.760 --> 01:57:31.520]   So there's the exponential tech arms race side
[01:57:31.520 --> 01:57:34.520]   of in-group, out-group rivalry using exponential tech
[01:57:34.520 --> 01:57:36.240]   that is one set of risks.
[01:57:36.240 --> 01:57:40.480]   And the other set of risks is the application
[01:57:40.480 --> 01:57:42.760]   of exponentially more powerful tech,
[01:57:42.760 --> 01:57:46.520]   not intentionally to try and beat an out-group,
[01:57:46.520 --> 01:57:48.580]   but to try to achieve some goal that we have,
[01:57:48.580 --> 01:57:51.160]   but to produce a second and third order effects
[01:57:51.160 --> 01:57:55.240]   that do have harm to the commons, to other people,
[01:57:55.240 --> 01:57:57.220]   to environment, to other groups,
[01:57:57.220 --> 01:58:01.680]   that might actually be bigger problems
[01:58:01.680 --> 01:58:03.360]   than the problem we were originally trying to solve
[01:58:03.360 --> 01:58:05.240]   with the thing we were building.
[01:58:05.240 --> 01:58:09.320]   When Facebook was building a dating app
[01:58:09.320 --> 01:58:10.480]   and then building a social app
[01:58:10.480 --> 01:58:12.080]   where people could tag pictures,
[01:58:12.920 --> 01:58:17.040]   they weren't trying to build a democracy destroying app
[01:58:17.040 --> 01:58:23.200]   that would maximize time on site as part of its ad model
[01:58:23.200 --> 01:58:27.480]   through AI optimization of a newsfeed
[01:58:27.480 --> 01:58:29.460]   to the thing that made people spend most time on site,
[01:58:29.460 --> 01:58:31.840]   which is usually them being limbically hijacked
[01:58:31.840 --> 01:58:33.240]   more than something else,
[01:58:33.240 --> 01:58:35.920]   which ends up appealing to people's cognitive biases
[01:58:35.920 --> 01:58:37.440]   and group identities,
[01:58:37.440 --> 01:58:39.880]   and creates no sense of shared reality.
[01:58:39.880 --> 01:58:40.880]   They weren't trying to do that,
[01:58:40.880 --> 01:58:42.920]   but it was a second order effect.
[01:58:42.920 --> 01:58:46.940]   And it's a pretty fucking powerful second order effect,
[01:58:46.940 --> 01:58:49.920]   and a pretty fast one,
[01:58:49.920 --> 01:58:52.760]   'cause the rate of tech is obviously able to get distributed
[01:58:52.760 --> 01:58:54.520]   to much larger scale, much faster,
[01:58:54.520 --> 01:58:58.560]   and with a bigger jump in terms of total vertical capacity,
[01:58:58.560 --> 01:59:00.680]   then that's what it means to get to the verticalizing part
[01:59:00.680 --> 01:59:01.880]   of an exponential curve.
[01:59:01.880 --> 01:59:07.440]   So just like we can see that oil
[01:59:07.440 --> 01:59:09.100]   had these second order environmental effects,
[01:59:09.100 --> 01:59:10.760]   and also social and political effects,
[01:59:10.760 --> 01:59:14.680]   and war and so much of the whole,
[01:59:14.680 --> 01:59:17.080]   like the total amount of oil used
[01:59:17.080 --> 01:59:20.960]   has a proportionality to total global GDP.
[01:59:20.960 --> 01:59:23.200]   And this way we have this, the petrodollar,
[01:59:23.200 --> 01:59:29.120]   and so the oil thing also had the externalities
[01:59:29.120 --> 01:59:30.940]   of a major aspect of what happened
[01:59:30.940 --> 01:59:33.640]   with military industrial complex and things like that.
[01:59:33.640 --> 01:59:35.680]   So, but we can see the same thing
[01:59:35.680 --> 01:59:37.920]   with more current technologies,
[01:59:37.920 --> 01:59:41.000]   with Facebook and Google and other things.
[01:59:41.000 --> 01:59:44.800]   So I don't think we can run,
[01:59:44.800 --> 01:59:46.780]   and the more powerful the tech is,
[01:59:46.780 --> 01:59:51.000]   we build it for reason X, whatever reason X is.
[01:59:51.000 --> 01:59:53.800]   Maybe X is three things, maybe it's one thing, right?
[01:59:53.800 --> 01:59:57.680]   We're doing the oil thing because we wanna make cars
[01:59:57.680 --> 02:00:00.040]   because it's a better method of individual transportation.
[02:00:00.040 --> 02:00:01.120]   We're building the Facebook thing
[02:00:01.120 --> 02:00:02.800]   'cause we're gonna connect people socially
[02:00:02.800 --> 02:00:04.440]   in a personal sphere.
[02:00:04.440 --> 02:00:08.760]   But it interacts with complex systems,
[02:00:08.760 --> 02:00:12.960]   with ecologies, economies, psychologies, cultures.
[02:00:12.960 --> 02:00:14.000]   And so it has effects
[02:00:14.000 --> 02:00:16.400]   on other than the thing we're intending.
[02:00:16.400 --> 02:00:19.800]   Some of those effects can end up being negative effects,
[02:00:19.800 --> 02:00:21.760]   but because this technology,
[02:00:21.760 --> 02:00:24.480]   if we make it to solve a problem,
[02:00:24.480 --> 02:00:25.920]   it has to overcome the problem.
[02:00:25.920 --> 02:00:27.080]   The problem's been around for a while,
[02:00:27.080 --> 02:00:28.340]   it's gonna overcome in a short period of time.
[02:00:28.340 --> 02:00:30.120]   So it usually has greater scale,
[02:00:30.120 --> 02:00:32.560]   greater rate of magnitude in some way.
[02:00:32.560 --> 02:00:35.520]   That also means that the externalities that it creates
[02:00:35.520 --> 02:00:37.480]   might be bigger problems.
[02:00:37.480 --> 02:00:39.960]   And you can say, well, but then that's the new problem
[02:00:39.960 --> 02:00:41.740]   and humanity will innovate its way out of that.
[02:00:41.740 --> 02:00:43.560]   Well, I don't think that's paying attention
[02:00:43.560 --> 02:00:44.980]   to the fact that we can't keep up
[02:00:44.980 --> 02:00:47.440]   with exponential curves like that,
[02:00:47.440 --> 02:00:49.320]   nor do finite spaces allow
[02:00:49.320 --> 02:00:51.200]   exponential externalities forever.
[02:00:51.200 --> 02:00:55.140]   And this is why a lot of the smartest people
[02:00:55.140 --> 02:00:56.680]   thinking about this are thinking,
[02:00:56.680 --> 02:00:59.840]   well, no, I think we're totally screwed
[02:00:59.840 --> 02:01:02.200]   unless we can make a benevolent AI singleton
[02:01:02.200 --> 02:01:03.260]   that rules all of us.
[02:01:03.260 --> 02:01:08.080]   Guys like Ostrom and others thinking in those directions,
[02:01:08.080 --> 02:01:12.080]   'cause they're like, how do humans try to do
[02:01:12.080 --> 02:01:14.280]   multipolarity and make it work?
[02:01:14.280 --> 02:01:17.880]   And I have a different answer of what I think it looks like
[02:01:17.880 --> 02:01:20.680]   that does have more to do with the love,
[02:01:20.680 --> 02:01:23.720]   but some applied social tech aligned with love.
[02:01:23.720 --> 02:01:26.240]   - 'Cause I have a bunch of really dumb ideas.
[02:01:26.240 --> 02:01:28.240]   I'd prefer to hear-
[02:01:28.240 --> 02:01:30.080]   - I'd like to hear some of them first.
[02:01:30.080 --> 02:01:34.240]   I think the idea I would have is to be a bit more rigorous
[02:01:34.240 --> 02:01:39.240]   in trying to measure the amount of love you add
[02:01:39.240 --> 02:01:42.600]   or subtract from the world
[02:01:42.600 --> 02:01:46.400]   in second, third, fourth, fifth order effects.
[02:01:46.400 --> 02:01:49.600]   It's actually, I think, especially in the world of tech,
[02:01:49.600 --> 02:01:50.580]   quite doable.
[02:01:50.580 --> 02:01:53.720]   You just might not like,
[02:01:53.720 --> 02:01:58.280]   the shareholders may not like that kind of metric,
[02:01:58.280 --> 02:01:59.980]   but it's pretty easy to measure.
[02:01:59.980 --> 02:02:02.120]   That's not even,
[02:02:02.120 --> 02:02:06.520]   I'm perhaps half joking about love,
[02:02:06.520 --> 02:02:08.680]   but we could talk about just happiness and well-being,
[02:02:08.680 --> 02:02:09.920]   long-term well-being.
[02:02:09.920 --> 02:02:12.840]   That's pretty easy for Facebook, for YouTube,
[02:02:12.840 --> 02:02:15.240]   for all these companies to measure that.
[02:02:15.240 --> 02:02:18.840]   They do a lot of kinds of surveys.
[02:02:18.840 --> 02:02:21.200]   I mean, there's very simple solutions here
[02:02:21.200 --> 02:02:23.920]   that you could just survey how,
[02:02:23.920 --> 02:02:26.760]   I mean, servers are in some sense useless
[02:02:26.760 --> 02:02:31.280]   because they're a subset of the population.
[02:02:31.280 --> 02:02:32.520]   You're just trying to get a sense,
[02:02:32.520 --> 02:02:34.440]   it's very loose kind of understanding,
[02:02:34.440 --> 02:02:36.960]   but integrated deeply as part of the technology.
[02:02:36.960 --> 02:02:39.280]   Most of our tech is recommender systems.
[02:02:39.280 --> 02:02:41.000]   Most of the, sorry, not tech,
[02:02:41.000 --> 02:02:45.120]   online interaction is driven by recommender systems
[02:02:45.120 --> 02:02:48.040]   that learn very little data about you
[02:02:48.040 --> 02:02:50.680]   and use that data based on,
[02:02:50.680 --> 02:02:52.960]   mostly based on traces of your previous behavior
[02:02:52.960 --> 02:02:54.600]   to suggest future things.
[02:02:54.600 --> 02:02:56.800]   This is how Twitter, this is how Facebook works,
[02:02:56.800 --> 02:03:00.280]   this is how AdSense for Google, AdSense works,
[02:03:00.280 --> 02:03:02.520]   this is how Netflix, YouTube work, and so on.
[02:03:02.520 --> 02:03:06.440]   And for them to just track, as opposed to engagement,
[02:03:06.440 --> 02:03:08.360]   how much you spend on a particular video,
[02:03:08.360 --> 02:03:12.040]   a particular site, is also track,
[02:03:12.040 --> 02:03:15.520]   give you the technology to do self-report
[02:03:15.520 --> 02:03:17.040]   of what makes you feel good,
[02:03:17.040 --> 02:03:19.480]   of what makes you grow as a person,
[02:03:19.480 --> 02:03:20.900]   of what makes you,
[02:03:23.120 --> 02:03:24.880]   the best version of yourself,
[02:03:24.880 --> 02:03:30.960]   the Rogan idea of the hero of your own movie.
[02:03:30.960 --> 02:03:34.040]   And just add that little bit of information.
[02:03:34.040 --> 02:03:37.560]   If you have people, you have this happiness surveys
[02:03:37.560 --> 02:03:40.140]   of how you feel about the last five days,
[02:03:40.140 --> 02:03:42.840]   how would you report your experience?
[02:03:42.840 --> 02:03:44.920]   You can lay out a set of videos.
[02:03:44.920 --> 02:03:45.960]   This is kind of fascinating to watch.
[02:03:45.960 --> 02:03:47.400]   I don't know if you ever look at YouTube,
[02:03:47.400 --> 02:03:49.540]   the history of videos you've looked at.
[02:03:49.540 --> 02:03:52.160]   It's fascinating, it's very embarrassing for me.
[02:03:52.160 --> 02:03:54.000]   Like, it'll be like a lecture,
[02:03:54.000 --> 02:03:56.480]   and then like a set of videos
[02:03:56.480 --> 02:03:58.640]   that I don't want anyone to know about,
[02:03:58.640 --> 02:04:02.400]   which is, which would be like, I don't know,
[02:04:02.400 --> 02:04:04.120]   maybe like five videos in a row
[02:04:04.120 --> 02:04:05.760]   where it looks like I watched the whole thing,
[02:04:05.760 --> 02:04:08.320]   which I probably did, about like how to cook a steak,
[02:04:08.320 --> 02:04:11.320]   even though, or just like the best chefs in the world
[02:04:11.320 --> 02:04:15.080]   cooking steaks, and I'm just like sitting there watching it
[02:04:15.080 --> 02:04:17.840]   for no purpose whatsoever, wasting away my life,
[02:04:17.840 --> 02:04:21.160]   or like funny cat videos, or like legit,
[02:04:21.160 --> 02:04:23.480]   that's always a good one.
[02:04:23.480 --> 02:04:26.560]   And I could look back and rate which videos
[02:04:26.560 --> 02:04:29.160]   made me a better person and not.
[02:04:29.160 --> 02:04:31.800]   And I mean, on a more serious note,
[02:04:31.800 --> 02:04:34.040]   there's a bunch of conversations, podcasts,
[02:04:34.040 --> 02:04:37.400]   or lectures I've watched which made me a better person,
[02:04:37.400 --> 02:04:40.120]   and some of them made me a worse person.
[02:04:40.120 --> 02:04:43.720]   Quite honestly, not for stupid reasons, like I feel dumber,
[02:04:43.720 --> 02:04:47.200]   but because I do have a sense that that started me
[02:04:47.200 --> 02:04:52.200]   on a path of not being kind to other people.
[02:04:52.200 --> 02:04:57.160]   For example, I'll give you, for my own,
[02:04:57.160 --> 02:04:59.560]   and I'm sorry for ranting, but maybe there's some usefulness
[02:04:59.560 --> 02:05:01.360]   to this kind of exploration of self.
[02:05:01.360 --> 02:05:07.180]   When I focus on creating, on programming, on science,
[02:05:07.180 --> 02:05:11.640]   I become a much deeper thinker
[02:05:11.640 --> 02:05:14.380]   and a kinder person to others.
[02:05:14.380 --> 02:05:16.960]   When I listen to too many, a little bit is good,
[02:05:16.960 --> 02:05:20.240]   but too many podcasts or videos
[02:05:20.240 --> 02:05:24.280]   about how our world is melting down,
[02:05:24.280 --> 02:05:27.240]   or criticizing ridiculous people,
[02:05:27.240 --> 02:05:30.240]   the worst of the quote-unquote woke, for example.
[02:05:30.240 --> 02:05:33.560]   There's all these groups that are misbehaving
[02:05:33.560 --> 02:05:37.040]   in fascinating ways because they've been corrupted by power.
[02:05:37.040 --> 02:05:42.040]   The more I watch criticism of them, the worse I become.
[02:05:44.120 --> 02:05:47.080]   And I'm aware of this, but I'm also aware
[02:05:47.080 --> 02:05:50.920]   that for some reason it's pleasant to watch those sometimes.
[02:05:50.920 --> 02:05:54.040]   And so for me to be able to self-report
[02:05:54.040 --> 02:05:57.400]   that to the YouTube algorithm, to the systems around me,
[02:05:57.400 --> 02:05:59.640]   and they ultimately try to optimize
[02:05:59.640 --> 02:06:03.160]   to make me the best version of myself,
[02:06:03.160 --> 02:06:05.400]   which I personally believe would make YouTube
[02:06:05.400 --> 02:06:07.420]   a lot more money because I'd be much more willing
[02:06:07.420 --> 02:06:09.440]   to spend time on YouTube and give YouTube
[02:06:09.440 --> 02:06:12.160]   a lot more of my money.
[02:06:12.160 --> 02:06:15.880]   That's great for business and great for humanity
[02:06:15.880 --> 02:06:18.040]   because it'll make me a kinder person.
[02:06:18.040 --> 02:06:23.000]   It'll increase the love quotient, the love metric,
[02:06:23.000 --> 02:06:25.420]   and it'll make them a lot of money.
[02:06:25.420 --> 02:06:27.000]   I feel like everything's aligned.
[02:06:27.000 --> 02:06:30.000]   And so you should do that not just for YouTube algorithm,
[02:06:30.000 --> 02:06:32.000]   but also for military strategy
[02:06:32.000 --> 02:06:33.740]   and whether to go to war or not,
[02:06:33.740 --> 02:06:35.880]   because one externality you can think of
[02:06:35.880 --> 02:06:40.080]   about going to war, which I think we talked about offline,
[02:06:40.080 --> 02:06:42.800]   is we often go to war with kind of governments,
[02:06:42.800 --> 02:06:46.000]   not with the people.
[02:06:46.000 --> 02:06:49.960]   You have to think about the kids of countries
[02:06:49.960 --> 02:06:56.840]   that see a soldier and because of what they experience,
[02:06:56.840 --> 02:07:00.820]   their interaction with the soldier, hate is born.
[02:07:00.820 --> 02:07:04.200]   When you're like eight years old, six years old,
[02:07:04.200 --> 02:07:06.200]   you lose your dad, you lose your mom,
[02:07:06.200 --> 02:07:09.160]   you lose a friend, somebody close to you
[02:07:09.160 --> 02:07:11.920]   that wanna really powerful externality
[02:07:11.920 --> 02:07:15.280]   that could be reduced to love, positive and negative,
[02:07:15.280 --> 02:07:19.360]   is the hate that's born when you make decisions.
[02:07:19.360 --> 02:07:22.640]   And that's going to take fruition,
[02:07:22.640 --> 02:07:25.180]   that that little seed is going to become a tree
[02:07:25.180 --> 02:07:27.120]   that then leads to the kind of destruction
[02:07:27.120 --> 02:07:28.340]   that we talk about.
[02:07:28.340 --> 02:07:33.600]   So, but in my sense, it's possible to reduce everything
[02:07:33.600 --> 02:07:36.440]   to a measure of how much love does this add to the world.
[02:07:38.560 --> 02:07:41.160]   All that to say, do you have ideas
[02:07:41.160 --> 02:07:46.000]   of how we practically build systems
[02:07:46.000 --> 02:07:49.300]   that create a resilient society?
[02:07:49.300 --> 02:07:51.800]   - There were a lot of good things that you shared
[02:07:51.800 --> 02:07:55.040]   where there's like 15 different ways
[02:07:55.040 --> 02:07:57.140]   that we could enter this that are all interesting.
[02:07:57.140 --> 02:07:59.880]   So I'm trying to see which one will probably be most useful.
[02:07:59.880 --> 02:08:03.520]   - Pick the one or two things that are least ridiculous.
[02:08:03.520 --> 02:08:07.240]   - When you were mentioning if we could see
[02:08:07.240 --> 02:08:11.440]   some of the second order effects or externalities
[02:08:11.440 --> 02:08:12.320]   that we aren't used to seeing,
[02:08:12.320 --> 02:08:15.200]   specifically the one of a kid being radicalized
[02:08:15.200 --> 02:08:18.080]   somewhere else, which engenders enmity in them towards us,
[02:08:18.080 --> 02:08:20.240]   which decreases our own future security.
[02:08:20.240 --> 02:08:21.400]   Even if you don't care about the kid,
[02:08:21.400 --> 02:08:23.480]   if you care about the kid, it's a whole other thing.
[02:08:23.480 --> 02:08:27.820]   Yeah, I mean, I think when we saw this,
[02:08:27.820 --> 02:08:30.120]   when Jane Fonda and others went to Vietnam
[02:08:30.120 --> 02:08:32.760]   and took photos and videos of what was happening,
[02:08:32.760 --> 02:08:33.920]   and you got to see the pictures
[02:08:33.920 --> 02:08:35.840]   of the kids with napalm on them.
[02:08:36.680 --> 02:08:41.600]   That like the anti-war effort was bolstered by that
[02:08:41.600 --> 02:08:43.600]   in a way it couldn't have been without that.
[02:08:43.600 --> 02:08:46.840]   There's a, until we can see the images,
[02:08:46.840 --> 02:08:50.080]   you can't have a mere neuron effect in the same way.
[02:08:50.080 --> 02:08:53.300]   And when you can, that starts to have a powerful effect.
[02:08:53.300 --> 02:08:54.600]   I think there's a deep principle
[02:08:54.600 --> 02:08:59.600]   that you're sharing there, which is that if we,
[02:08:59.600 --> 02:09:05.480]   we can have a rivalrous intent where our in-group,
[02:09:05.480 --> 02:09:07.520]   whatever it is, maybe it's our political party
[02:09:07.520 --> 02:09:08.920]   wanting to win within the US,
[02:09:08.920 --> 02:09:12.880]   maybe it's our nation state wanting to win in a war
[02:09:12.880 --> 02:09:15.660]   or an economic war over resource or whatever it is,
[02:09:15.660 --> 02:09:19.840]   that if we don't obliterate the other people completely,
[02:09:19.840 --> 02:09:21.400]   they don't go away.
[02:09:21.400 --> 02:09:24.300]   They're not engendered to like us more.
[02:09:24.300 --> 02:09:27.160]   They didn't become less smart.
[02:09:27.160 --> 02:09:28.800]   So they have more enmity towards us
[02:09:28.800 --> 02:09:31.280]   and whatever technologies we employed to be successful,
[02:09:31.280 --> 02:09:33.160]   they will now reverse engineer,
[02:09:33.160 --> 02:09:35.380]   make iterations on and come back.
[02:09:35.380 --> 02:09:37.280]   And so you drive an arms race,
[02:09:37.280 --> 02:09:42.280]   which is why you can see that the wars were over history,
[02:09:42.280 --> 02:09:45.840]   employing more lethal weaponry.
[02:09:45.840 --> 02:09:47.480]   And not just the kinetic war,
[02:09:47.480 --> 02:09:51.680]   the information war and the narrative war
[02:09:51.680 --> 02:09:53.600]   and the economic war, right?
[02:09:53.600 --> 02:09:56.940]   Like it just increased capacity in all of those fronts.
[02:09:56.940 --> 02:10:02.760]   And so what seems like a win to us on the short term
[02:10:02.760 --> 02:10:05.380]   might actually really produce losses in the long term.
[02:10:05.380 --> 02:10:07.780]   And what's even in our own best interest in the long term
[02:10:07.780 --> 02:10:09.300]   is probably more aligned with everyone else
[02:10:09.300 --> 02:10:10.780]   'cause we inter-affect each other.
[02:10:10.780 --> 02:10:13.780]   And I think the thing about globalism,
[02:10:13.780 --> 02:10:16.060]   globalization and exponential tech
[02:10:16.060 --> 02:10:17.740]   and the rate at which we affect each other
[02:10:17.740 --> 02:10:19.100]   and the rate at which we affect the biosphere
[02:10:19.100 --> 02:10:20.860]   that we're all affected by
[02:10:20.860 --> 02:10:25.860]   is that this kind of proverbial spiritual idea
[02:10:25.860 --> 02:10:28.100]   that we're all interconnected
[02:10:28.100 --> 02:10:30.180]   and need to think about that in some way,
[02:10:30.180 --> 02:10:32.620]   that it was easy for tribes to get,
[02:10:32.620 --> 02:10:34.740]   because everyone in the tribe so clearly
[02:10:34.740 --> 02:10:37.720]   saw their interconnection and dependence on each other.
[02:10:37.720 --> 02:10:39.480]   But in terms of a global level,
[02:10:39.480 --> 02:10:43.720]   the speed at which we are actually interconnected,
[02:10:43.720 --> 02:10:46.860]   the speed at which the harm happening to something in Wuhan
[02:10:46.860 --> 02:10:48.000]   affects the rest of the world
[02:10:48.000 --> 02:10:50.360]   or a new technology developed somewhere
[02:10:50.360 --> 02:10:52.440]   affects the entire world or an environmental issue
[02:10:52.440 --> 02:10:54.880]   or whatever is making it to where
[02:10:54.880 --> 02:10:57.140]   we either actually all get,
[02:10:57.140 --> 02:10:59.560]   not as a spiritual idea, just even as physics, right?
[02:10:59.560 --> 02:11:01.800]   We all get the interconnectedness of everything
[02:11:01.800 --> 02:11:04.120]   and that we either all consider that
[02:11:04.120 --> 02:11:06.940]   and see how to make it through more effectively together
[02:11:06.940 --> 02:11:10.160]   or failures anywhere end up becoming
[02:11:10.160 --> 02:11:11.800]   decreased quality of life and failures
[02:11:11.800 --> 02:11:13.240]   and increased risk everywhere.
[02:11:13.240 --> 02:11:15.280]   - Don't you think people are beginning to experience that
[02:11:15.280 --> 02:11:16.480]   at the individual level?
[02:11:16.480 --> 02:11:18.320]   So governments are resisting it.
[02:11:18.320 --> 02:11:20.880]   They're trying to make us not empathize with each other,
[02:11:20.880 --> 02:11:21.720]   feel connected.
[02:11:21.720 --> 02:11:23.000]   But don't you think people are beginning
[02:11:23.000 --> 02:11:25.000]   to feel more and more connected?
[02:11:25.000 --> 02:11:27.880]   Like, isn't that exactly what the technology is enabling?
[02:11:27.880 --> 02:11:30.040]   Like social networks, we tend to criticize them,
[02:11:30.040 --> 02:11:35.040]   but isn't there a sense which we're experiencing?
[02:11:35.040 --> 02:11:39.380]   - When you watch those videos that are criticizing,
[02:11:39.380 --> 02:11:42.680]   whether it's the woke Antifa side
[02:11:42.680 --> 02:11:46.040]   or the QAnon Trump supporter side,
[02:11:46.040 --> 02:11:49.300]   does it seem like they have increased empathy
[02:11:49.300 --> 02:11:51.560]   for people that are outside of their ideologic camp?
[02:11:51.560 --> 02:11:52.400]   - No, not at all.
[02:11:52.400 --> 02:11:57.400]   So I may be conflating my own experience
[02:11:58.200 --> 02:12:01.200]   of the world and that of the populace.
[02:12:01.200 --> 02:12:09.640]   I tend to see those videos as feeding something
[02:12:09.640 --> 02:12:12.060]   that's a relic of the past.
[02:12:12.060 --> 02:12:16.360]   They figured out that drama fuels clicks,
[02:12:16.360 --> 02:12:19.440]   but whether I'm right or wrong, I don't know.
[02:12:19.440 --> 02:12:23.400]   But I tend to sense that that is not,
[02:12:23.400 --> 02:12:27.320]   that hunger for drama is not fundamental to human beings.
[02:12:27.320 --> 02:12:29.080]   That we want to actually,
[02:12:29.080 --> 02:12:33.240]   that we want to understand Antifa
[02:12:33.240 --> 02:12:34.840]   and we want to empathize.
[02:12:34.840 --> 02:12:37.200]   We want to take radical ideas
[02:12:37.200 --> 02:12:41.840]   and be able to empathize with them and synthesize it all.
[02:12:41.840 --> 02:12:46.840]   - Okay, let's look at cultural outliers
[02:12:46.840 --> 02:12:51.920]   in terms of violence versus compassion.
[02:12:51.920 --> 02:12:53.660]   We can see that a lot of cultures
[02:12:53.660 --> 02:12:56.980]   have relatively lower in-group violence,
[02:12:56.980 --> 02:12:58.700]   bigger out-group violence.
[02:12:58.700 --> 02:13:00.020]   And there's some variance in them
[02:13:00.020 --> 02:13:01.180]   and variance at different times
[02:13:01.180 --> 02:13:02.980]   based on the scarcity or abundance of resource
[02:13:02.980 --> 02:13:04.380]   and other things.
[02:13:04.380 --> 02:13:09.060]   But you can look at, say, Jains,
[02:13:09.060 --> 02:13:11.820]   whose whole religion is around nonviolence
[02:13:11.820 --> 02:13:13.740]   so much so that they don't even hurt plants.
[02:13:13.740 --> 02:13:16.620]   They only take fruits that fall off them and stuff.
[02:13:16.620 --> 02:13:19.820]   Or to go to a larger population, you take Buddhists,
[02:13:19.820 --> 02:13:22.140]   where for the most part, with a few exceptions,
[02:13:22.140 --> 02:13:24.540]   for the most part, across three millennia
[02:13:24.540 --> 02:13:26.320]   and across lots of different countries
[02:13:26.320 --> 02:13:28.120]   and geographies and whatever,
[02:13:28.120 --> 02:13:30.320]   you have 10 million people, plus or minus,
[02:13:30.320 --> 02:13:31.460]   who don't hurt bugs.
[02:13:31.460 --> 02:13:34.820]   The whole spectrum of genetic variance
[02:13:34.820 --> 02:13:37.980]   that is happening within a culture of that many people
[02:13:37.980 --> 02:13:41.900]   and head traumas and whatever, and nobody hurts bugs.
[02:13:41.900 --> 02:13:44.480]   And then you look at a group where the kids grew up
[02:13:44.480 --> 02:13:47.580]   as child soldiers in Liberia or Darfur,
[02:13:47.580 --> 02:13:48.660]   where to make it to adulthood,
[02:13:48.660 --> 02:13:51.620]   pretty much everybody's killed people, hand to hand,
[02:13:51.620 --> 02:13:53.260]   and killed people who were civilian
[02:13:53.260 --> 02:13:55.460]   or innocent type of people.
[02:13:55.460 --> 02:13:58.620]   And you say, okay, so we were very neotenous.
[02:13:58.620 --> 02:14:00.260]   We can be conditioned by our environment,
[02:14:00.260 --> 02:14:02.900]   and humans can be conditioned,
[02:14:02.900 --> 02:14:04.940]   where almost all the humans show up
[02:14:04.940 --> 02:14:06.420]   in these two different bell curves.
[02:14:06.420 --> 02:14:08.200]   It doesn't mean that the Buddhists had no violence.
[02:14:08.200 --> 02:14:09.940]   It doesn't mean that these people had no compassion,
[02:14:09.940 --> 02:14:13.820]   but they're very different Gaussian distributions.
[02:14:13.820 --> 02:14:17.940]   And so I think one of the important things
[02:14:17.940 --> 02:14:20.640]   that I like to do is look at the examples
[02:14:20.640 --> 02:14:24.580]   of the populations, what Buddhism shows regarding compassion
[02:14:24.580 --> 02:14:28.900]   or what Judaism shows around education,
[02:14:28.900 --> 02:14:31.260]   the average level of education that everybody gets
[02:14:31.260 --> 02:14:32.540]   'cause of a culture that is really working
[02:14:32.540 --> 02:14:35.260]   on conditioning it or various cultures.
[02:14:35.260 --> 02:14:37.940]   What are the positive deviance
[02:14:37.940 --> 02:14:40.440]   outside of the statistical deviance
[02:14:40.440 --> 02:14:42.720]   to see what is actually possible?
[02:14:42.720 --> 02:14:45.600]   And then say, what are the conditioning factors?
[02:14:45.600 --> 02:14:46.860]   And can we condition those
[02:14:46.860 --> 02:14:48.900]   across a few of them simultaneously?
[02:14:48.900 --> 02:14:50.880]   And could we build a civilization like that?
[02:14:50.880 --> 02:14:53.400]   Becomes a very interesting question.
[02:14:53.400 --> 02:14:55.580]   So there's this kind of real politic idea
[02:14:55.580 --> 02:14:58.440]   that humans are violent.
[02:14:58.440 --> 02:15:00.920]   Large groups of humans become violent.
[02:15:00.920 --> 02:15:02.980]   They become irrational, specifically those two things,
[02:15:02.980 --> 02:15:05.020]   rivalrous and violent and irrational.
[02:15:05.020 --> 02:15:07.900]   And so in order to minimize the total amount of violence
[02:15:07.900 --> 02:15:10.440]   and have some good decisions, they need ruled somehow.
[02:15:10.440 --> 02:15:13.960]   And that not getting that is some kind of naive utopianism
[02:15:13.960 --> 02:15:16.020]   that doesn't understand human nature yet.
[02:15:16.020 --> 02:15:18.100]   This gets back to like mimesis of desire
[02:15:18.100 --> 02:15:19.280]   as an inexorable thing.
[02:15:19.280 --> 02:15:22.080]   I think the idea of the masses
[02:15:22.080 --> 02:15:24.160]   is actually a kind of propaganda
[02:15:24.160 --> 02:15:28.060]   that is useful for the classes that control
[02:15:28.060 --> 02:15:34.540]   to popularize the idea that most people are too violent,
[02:15:34.540 --> 02:15:39.540]   lazy, undisciplined and irrational to make good choices.
[02:15:39.540 --> 02:15:42.040]   And therefore their choices should be sublimated
[02:15:42.040 --> 02:15:43.440]   in some kind of way.
[02:15:43.440 --> 02:15:44.700]   I think that if we look back
[02:15:44.700 --> 02:15:46.480]   at these conditioning environments,
[02:15:47.660 --> 02:15:51.060]   we can say, okay, so the kids,
[02:15:51.060 --> 02:15:56.060]   they go to a really fancy school
[02:15:56.060 --> 02:15:57.820]   and have a good developmental environment
[02:15:57.820 --> 02:15:59.620]   like Exeter Academy.
[02:15:59.620 --> 02:16:00.980]   There's still a Gaussian distribution
[02:16:00.980 --> 02:16:03.420]   of how well they do on any particular metric,
[02:16:03.420 --> 02:16:06.100]   but on average, they become senators.
[02:16:06.100 --> 02:16:09.180]   And the worst ones become high-end lawyers or whatever.
[02:16:09.180 --> 02:16:10.700]   And then I look at the inner city school
[02:16:10.700 --> 02:16:12.100]   with a totally different set of things.
[02:16:12.100 --> 02:16:13.980]   And I see a very, very differently displaced
[02:16:13.980 --> 02:16:14.820]   Gaussian distribution,
[02:16:14.820 --> 02:16:16.400]   but a very different set of conditioning factors.
[02:16:16.400 --> 02:16:18.020]   So then I say the masses,
[02:16:18.020 --> 02:16:18.940]   well, if all those kids
[02:16:18.940 --> 02:16:20.460]   who were one of the parts of the masses
[02:16:20.460 --> 02:16:22.420]   got to go to Exeter and have that family and whatever,
[02:16:22.420 --> 02:16:23.980]   would they still be the masses?
[02:16:23.980 --> 02:16:29.940]   Could we actually condition more social virtue,
[02:16:29.940 --> 02:16:30.900]   more civic virtue,
[02:16:30.900 --> 02:16:33.460]   more orientation towards dialectical synthesis,
[02:16:33.460 --> 02:16:37.320]   more empathy, more rationality widely?
[02:16:37.320 --> 02:16:38.160]   Yes.
[02:16:38.160 --> 02:16:42.280]   Would that lead to better capacity
[02:16:42.280 --> 02:16:44.540]   for something like participatory governance,
[02:16:44.540 --> 02:16:45.680]   democracy or republic
[02:16:45.680 --> 02:16:47.920]   or some kind of participatory governance?
[02:16:47.920 --> 02:16:49.760]   - Yes. - Yes.
[02:16:49.760 --> 02:16:51.860]   Is it necessary for it, actually?
[02:16:51.860 --> 02:16:53.500]   Yes.
[02:16:53.500 --> 02:16:56.960]   And is it good for class interests?
[02:16:56.960 --> 02:16:58.140]   Not really.
[02:16:58.140 --> 02:17:01.200]   - By the way, when you say class interests,
[02:17:01.200 --> 02:17:04.120]   this is the powerful leading over the less powerful,
[02:17:04.120 --> 02:17:05.020]   that kind of idea?
[02:17:05.020 --> 02:17:08.880]   - Anyone that benefits from asymmetries of power
[02:17:08.880 --> 02:17:11.800]   doesn't necessarily benefit
[02:17:11.800 --> 02:17:14.100]   from decreasing those asymmetries of power
[02:17:14.100 --> 02:17:19.100]   and kind of increasing the capacity of people more widely.
[02:17:19.100 --> 02:17:24.760]   And so when we talk about power,
[02:17:24.760 --> 02:17:26.940]   we're talking about asymmetries
[02:17:26.940 --> 02:17:29.480]   in agency, influence and control.
[02:17:29.480 --> 02:17:31.160]   - You think that hunger for power
[02:17:31.160 --> 02:17:33.260]   is fundamental to human nature?
[02:17:33.260 --> 02:17:34.720]   I think we should get that straight
[02:17:34.720 --> 02:17:35.980]   before we talk about other stuff.
[02:17:35.980 --> 02:17:40.980]   So like this pickup line that I use at a bar often,
[02:17:41.560 --> 02:17:44.100]   which is power corrupts and absolute power corrupts,
[02:17:44.100 --> 02:17:45.500]   absolutely.
[02:17:45.500 --> 02:17:48.620]   Is that true or is that just a fancy thing to say?
[02:17:48.620 --> 02:17:51.460]   In modern society, there's something to be said,
[02:17:51.460 --> 02:17:54.140]   have we changed as societies over time
[02:17:54.140 --> 02:17:56.900]   in terms of how much we crave power?
[02:17:56.900 --> 02:18:01.220]   - That there is an impulse towards power
[02:18:01.220 --> 02:18:03.340]   that is innate in people and can be conditioned
[02:18:03.340 --> 02:18:04.420]   one way or the other, yes.
[02:18:04.420 --> 02:18:06.100]   But you can see that Buddhist society
[02:18:06.100 --> 02:18:08.880]   does a very different thing with it at scale.
[02:18:08.880 --> 02:18:11.000]   That you don't end up seeing the emergence
[02:18:11.000 --> 02:18:16.000]   of the same types of sociopathic behavior
[02:18:16.000 --> 02:18:19.460]   and particularly then creating sociopathic institutions.
[02:18:19.460 --> 02:18:26.300]   And so it's like, is eating the foods
[02:18:26.300 --> 02:18:28.340]   that were rare in our evolutionary environment
[02:18:28.340 --> 02:18:30.220]   that give us more dopamine hit because they were rare
[02:18:30.220 --> 02:18:33.180]   and they're not anymore, salt, fat, sugar.
[02:18:33.180 --> 02:18:35.020]   Is there something pleasurable about those
[02:18:35.020 --> 02:18:38.260]   where humans have an orientation to overeat if they can?
[02:18:38.260 --> 02:18:40.620]   Well, the fact that there is that possibility
[02:18:40.620 --> 02:18:42.880]   doesn't mean everyone will obligately be obese
[02:18:42.880 --> 02:18:43.920]   and die of obesity, right?
[02:18:43.920 --> 02:18:48.420]   Like it's possible to have a particular impulse
[02:18:48.420 --> 02:18:49.600]   and to be able to understand it,
[02:18:49.600 --> 02:18:52.220]   have other ones and be able to balance them.
[02:18:52.220 --> 02:18:57.220]   And so to say that power dynamics are obligate in humans
[02:18:57.220 --> 02:19:01.680]   and we can't do anything about it is very similar to me
[02:19:01.680 --> 02:19:05.880]   to saying like everyone is gonna be obligately obese.
[02:19:05.880 --> 02:19:08.040]   - Yeah, so there's some degree to which those,
[02:19:08.040 --> 02:19:10.000]   the control of those impulses has to do
[02:19:10.000 --> 02:19:11.440]   with the conditioning early in life.
[02:19:11.440 --> 02:19:15.040]   - Yes, and the culture that creates the environment
[02:19:15.040 --> 02:19:17.760]   to be able to do that and then the recursion on that.
[02:19:17.760 --> 02:19:21.640]   - Okay, so what if we were to, just bear with me,
[02:19:21.640 --> 02:19:22.880]   just asking for a friend,
[02:19:22.880 --> 02:19:26.780]   if we were to kill all humans on earth and then start over,
[02:19:26.780 --> 02:19:32.100]   is there ideas about how to build up,
[02:19:32.100 --> 02:19:33.080]   okay, we don't have to kill it,
[02:19:33.080 --> 02:19:35.320]   let's leave the humans on earth, they're fine,
[02:19:35.320 --> 02:19:37.380]   and go to Mars and start a new society.
[02:19:39.040 --> 02:19:42.080]   Is there ways to construct systems of conditioning,
[02:19:42.080 --> 02:19:45.800]   education, of how we live with each other
[02:19:45.800 --> 02:19:52.020]   that would incentivize us properly?
[02:19:52.020 --> 02:19:56.440]   To not seek power, to not construct systems
[02:19:56.440 --> 02:20:00.200]   that are of asymmetry of power
[02:20:00.200 --> 02:20:01.720]   and to create systems that are resilient
[02:20:01.720 --> 02:20:03.400]   to all kinds of terrorist attacks,
[02:20:03.400 --> 02:20:05.380]   to all kinds of destructions?
[02:20:05.380 --> 02:20:08.140]   - I believe so.
[02:20:08.140 --> 02:20:10.240]   - So is there some inklings we could,
[02:20:10.240 --> 02:20:13.160]   of course, you probably don't have all the answers,
[02:20:13.160 --> 02:20:15.640]   but you have insights about what that looks like.
[02:20:15.640 --> 02:20:20.160]   I mean, is it just rigorous practice of dialectic synthesis
[02:20:20.160 --> 02:20:23.520]   as essentially conversations with assholes
[02:20:23.520 --> 02:20:26.400]   of various flavors until they're not assholes anymore
[02:20:26.400 --> 02:20:28.520]   because you've become deeply empathetic
[02:20:28.520 --> 02:20:29.600]   with their experience?
[02:20:29.600 --> 02:20:33.080]   - Okay, so there's a lot of things
[02:20:33.080 --> 02:20:36.920]   that we would need to construct to come back to this,
[02:20:36.920 --> 02:20:39.560]   like what is the basis of rivalry?
[02:20:39.560 --> 02:20:40.940]   How do you bind it?
[02:20:40.940 --> 02:20:43.400]   How does it relate to tech?
[02:20:43.400 --> 02:20:46.020]   If you have a culture that is doing less rivalry,
[02:20:46.020 --> 02:20:48.920]   does it always lose in war to those who do war better?
[02:20:48.920 --> 02:20:51.040]   And how do you make something on the enactment
[02:20:51.040 --> 02:20:53.240]   of how to get there from here?
[02:20:53.240 --> 02:20:54.080]   - Great, great.
[02:20:54.080 --> 02:20:55.440]   So what's rivalry?
[02:20:55.440 --> 02:20:57.020]   Is rivalry bad or good?
[02:20:57.020 --> 02:21:01.680]   So is another word for rivalry competition?
[02:21:01.680 --> 02:21:05.460]   - Yes, I think, roughly yes.
[02:21:05.460 --> 02:21:10.460]   I think bad and good are kind of silly concepts here.
[02:21:10.460 --> 02:21:12.640]   Good for some things, bad for other things.
[02:21:12.640 --> 02:21:15.680]   - For resilience. - Some contexts and others.
[02:21:15.680 --> 02:21:16.520]   Even that.
[02:21:16.520 --> 02:21:19.800]   Let me give you an example that relates back
[02:21:19.800 --> 02:21:21.280]   to the Facebook measuring thing
[02:21:21.280 --> 02:21:22.920]   you were mentioning a moment ago.
[02:21:22.920 --> 02:21:26.320]   First, I think what you're saying is actually aligned
[02:21:26.320 --> 02:21:27.200]   with the right direction
[02:21:27.200 --> 02:21:29.220]   and what I wanna get to in a moment,
[02:21:29.220 --> 02:21:32.180]   but it's not, the devil is in the details here.
[02:21:32.180 --> 02:21:34.360]   So-- - I enjoy praise.
[02:21:34.360 --> 02:21:35.360]   It feeds my ego.
[02:21:35.360 --> 02:21:37.720]   I grow stronger, so I appreciate that.
[02:21:37.720 --> 02:21:39.560]   - I will make sure to include one piece
[02:21:39.560 --> 02:21:42.280]   every 15 minutes as we go. - Thank you.
[02:21:42.280 --> 02:21:47.280]   - So it's easier to measure,
[02:21:47.280 --> 02:21:52.760]   there are problems with this argument,
[02:21:52.760 --> 02:21:54.220]   but there's also utility to it.
[02:21:54.220 --> 02:21:56.960]   So let's take it for the utility it has first.
[02:21:56.960 --> 02:22:00.800]   It's harder to measure happiness
[02:22:00.800 --> 02:22:02.300]   than it is to measure comfort.
[02:22:04.840 --> 02:22:07.360]   We can measure with technology
[02:22:07.360 --> 02:22:10.600]   that the shocks in a car are making the car bounce less,
[02:22:10.600 --> 02:22:14.900]   that the bed is softer and material science
[02:22:14.900 --> 02:22:16.200]   and those types of things.
[02:22:16.200 --> 02:22:19.660]   And happiness is actually hard for philosophers to define
[02:22:19.660 --> 02:22:22.840]   because some people find
[02:22:22.840 --> 02:22:24.760]   that there's certain kinds of overcoming suffering
[02:22:24.760 --> 02:22:26.120]   that are necessary for happiness.
[02:22:26.120 --> 02:22:27.960]   There's happiness that feels more like contentment
[02:22:27.960 --> 02:22:30.080]   and happiness that feels more like passion.
[02:22:30.080 --> 02:22:31.580]   Is passion the source of all suffering
[02:22:31.580 --> 02:22:33.340]   or the source of all creativity?
[02:22:33.340 --> 02:22:35.800]   There's deep stuff and it's mostly first person,
[02:22:35.800 --> 02:22:37.200]   not measurable third person stuff,
[02:22:37.200 --> 02:22:40.120]   even if maybe it corresponds to third person stuff
[02:22:40.120 --> 02:22:42.460]   to some degree, but we also see examples of,
[02:22:42.460 --> 02:22:44.020]   some of our favorite examples is people
[02:22:44.020 --> 02:22:45.300]   who are in the worst environments
[02:22:45.300 --> 02:22:46.640]   who end up finding happiness, right?
[02:22:46.640 --> 02:22:49.180]   Where the third person stuff looks to be less conducive
[02:22:49.180 --> 02:22:52.520]   and there's some Viktor Frankl, Nelson Mandela, whatever.
[02:22:52.520 --> 02:22:55.700]   But it's pretty easy to measure comfort
[02:22:55.700 --> 02:22:57.060]   and it's pretty universal.
[02:22:57.060 --> 02:22:59.980]   And I think we can see that the industrial revolution
[02:22:59.980 --> 02:23:03.760]   started to replace happiness with comfort quite heavily
[02:23:03.760 --> 02:23:05.360]   as the thing it was optimizing for.
[02:23:05.360 --> 02:23:07.860]   And we can see that when increased comfort is given,
[02:23:07.860 --> 02:23:10.340]   maybe because of the evolutionary disposition
[02:23:10.340 --> 02:23:12.680]   that expending extra calories
[02:23:12.680 --> 02:23:14.140]   when for the majority of our history,
[02:23:14.140 --> 02:23:17.320]   we didn't have extra calories was not a safe thing to do.
[02:23:17.320 --> 02:23:18.280]   Who knows why?
[02:23:18.280 --> 02:23:21.280]   When extra comfort is given,
[02:23:21.280 --> 02:23:24.060]   it's very easy to take that path,
[02:23:24.060 --> 02:23:26.520]   even if it's not the path that supports
[02:23:26.520 --> 02:23:28.680]   overall wellbeing long-term.
[02:23:28.680 --> 02:23:33.680]   And so we can see that,
[02:23:33.680 --> 02:23:37.140]   when you look at the techno-optimist idea
[02:23:37.140 --> 02:23:39.940]   that we have better lives than Egyptian pharaohs
[02:23:39.940 --> 02:23:42.820]   and kings and whatever, what they're largely looking at
[02:23:42.820 --> 02:23:45.960]   is how comfortable our beds are
[02:23:45.960 --> 02:23:48.060]   and how comfortable the transportation systems are
[02:23:48.060 --> 02:23:48.900]   and things like that,
[02:23:48.900 --> 02:23:50.280]   in which case there's massive improvement.
[02:23:50.280 --> 02:23:52.660]   But we also see that in some of the nations
[02:23:52.660 --> 02:23:54.340]   where people have access to the most comfort,
[02:23:54.340 --> 02:23:57.140]   suicide and mental illness are the highest.
[02:23:57.140 --> 02:24:00.540]   And we also see that some of the happiest cultures
[02:24:00.540 --> 02:24:01.380]   are actually some of the ones
[02:24:01.380 --> 02:24:04.080]   that are in materially lame environments.
[02:24:04.080 --> 02:24:06.220]   And so there's a very interesting question here.
[02:24:06.220 --> 02:24:09.980]   And if I understand correctly, you do cold showers.
[02:24:09.980 --> 02:24:12.440]   And Joe Rogan was talking about how he needs to do
[02:24:12.440 --> 02:24:16.720]   some fairly intensive kind of struggle
[02:24:16.720 --> 02:24:18.080]   that is a non-comfort
[02:24:18.080 --> 02:24:20.520]   to actually induce being better as a person,
[02:24:20.520 --> 02:24:22.440]   this concept of hormesis,
[02:24:22.440 --> 02:24:26.400]   that it's actually stressing an adaptive system
[02:24:26.400 --> 02:24:28.420]   that increases its adaptive capacity
[02:24:28.420 --> 02:24:32.180]   and that there's something that the happiness of a system
[02:24:32.180 --> 02:24:34.540]   has something to do with its adaptive capacity,
[02:24:34.540 --> 02:24:36.480]   its overall resilience, health, wellbeing,
[02:24:36.480 --> 02:24:39.440]   which requires a decent bit of discomfort.
[02:24:39.440 --> 02:24:44.080]   And yet in the presence of the comfort solution,
[02:24:44.080 --> 02:24:45.720]   it's very hard to not choose it.
[02:24:45.720 --> 02:24:47.600]   And then as you're choosing it regularly
[02:24:47.600 --> 02:24:51.280]   to actually down-regulate your overall adaptive capacity.
[02:24:51.280 --> 02:24:55.000]   And so when we start saying,
[02:24:55.000 --> 02:25:00.000]   can we make tech where we're measuring
[02:25:00.000 --> 02:25:01.320]   for the things that it produces
[02:25:01.320 --> 02:25:03.120]   beyond just the measure of GDP
[02:25:03.120 --> 02:25:06.600]   or whatever particular measures look like
[02:25:06.600 --> 02:25:09.560]   the revenue generation or profit generation of my business,
[02:25:09.560 --> 02:25:14.200]   are all the meaningful things measurable?
[02:25:14.200 --> 02:25:17.700]   And what are the right measures?
[02:25:17.700 --> 02:25:20.140]   And what are the externalities of optimizing
[02:25:20.140 --> 02:25:21.240]   for that measurement set?
[02:25:21.240 --> 02:25:22.800]   What meaningful things aren't included
[02:25:22.800 --> 02:25:23.880]   in that measurement set
[02:25:23.880 --> 02:25:25.960]   that might have their own externalities?
[02:25:25.960 --> 02:25:26.880]   These are some of the questions
[02:25:26.880 --> 02:25:28.160]   we actually have to take seriously.
[02:25:28.160 --> 02:25:31.120]   - Yeah, and I think they're answerable questions, right?
[02:25:31.120 --> 02:25:33.120]   - Progressively better, not perfect.
[02:25:33.120 --> 02:25:34.720]   - Right, so first of all,
[02:25:34.720 --> 02:25:37.480]   let me throw out happiness and comfort out of the discussion.
[02:25:37.480 --> 02:25:39.280]   Those seem like useless.
[02:25:39.280 --> 02:25:42.320]   The distinction, 'cause I said they're useful,
[02:25:42.320 --> 02:25:45.780]   wellbeing is useful, but I think I take it back.
[02:25:45.780 --> 02:25:52.040]   I proposed new metrics in this brainstorm session,
[02:25:52.400 --> 02:25:57.400]   which is, so one is like personal growth,
[02:25:57.400 --> 02:26:01.080]   which is intellectual growth.
[02:26:01.080 --> 02:26:05.640]   I think we're able to make that concrete for ourselves.
[02:26:05.640 --> 02:26:10.360]   Like you're a better person than you were a week ago,
[02:26:10.360 --> 02:26:12.200]   or a worse person than you were a week ago.
[02:26:12.200 --> 02:26:15.760]   I think we can ourselves report that
[02:26:15.760 --> 02:26:17.840]   and understand what that means.
[02:26:17.840 --> 02:26:20.480]   It's this gray area, and we try to define it,
[02:26:20.480 --> 02:26:22.680]   but I think we humans are pretty good at that,
[02:26:22.680 --> 02:26:25.040]   because we have a sense, an idealistic sense
[02:26:25.040 --> 02:26:27.120]   of the person we might be able to become.
[02:26:27.120 --> 02:26:29.320]   We all dream of becoming a certain kind of person,
[02:26:29.320 --> 02:26:32.600]   and I think we have a sense of getting closer
[02:26:32.600 --> 02:26:34.040]   and not towards that person.
[02:26:34.040 --> 02:26:36.220]   Maybe this is not a great metric, fine.
[02:26:36.220 --> 02:26:38.440]   The other one is love, actually.
[02:26:38.440 --> 02:26:42.900]   Fuck if you're happy or not, or you're comfortable or not,
[02:26:42.900 --> 02:26:47.120]   how much love do you have towards your fellow human beings?
[02:26:47.120 --> 02:26:48.920]   I feel like if you try to optimize that
[02:26:48.920 --> 02:26:51.400]   and increasing that, that's going to have,
[02:26:51.400 --> 02:26:52.500]   that's a good metric.
[02:26:52.500 --> 02:26:58.800]   How many times a day, sorry, if I can quantify,
[02:26:58.800 --> 02:27:00.920]   how many times a day have you thought positively
[02:27:00.920 --> 02:27:02.560]   of another human being?
[02:27:02.560 --> 02:27:06.400]   Just put that down as a number, and increase that number.
[02:27:06.400 --> 02:27:08.960]   - I think the process of saying,
[02:27:08.960 --> 02:27:13.200]   okay, so let's not take GDP or GDP per capita
[02:27:13.200 --> 02:27:14.680]   as the metric we wanna optimize for,
[02:27:14.680 --> 02:27:16.500]   because GDP goes up during war,
[02:27:16.500 --> 02:27:18.500]   and it goes up with more healthcare spending
[02:27:18.500 --> 02:27:20.040]   from sicker people and various things
[02:27:20.040 --> 02:27:22.700]   that we wouldn't say correlate to quality of life.
[02:27:22.700 --> 02:27:24.680]   Addiction drives GDP awesomely.
[02:27:24.680 --> 02:27:28.200]   - By the way, when I said growth, I wasn't referring to GDP.
[02:27:28.200 --> 02:27:30.120]   - I know, I'm giving an example now
[02:27:30.120 --> 02:27:32.080]   of the primary metric we use
[02:27:32.080 --> 02:27:33.880]   and why it's not an adequate metric,
[02:27:33.880 --> 02:27:35.280]   'cause we're exploring other ones.
[02:27:35.280 --> 02:27:38.360]   So the idea of saying, what would the metrics
[02:27:38.360 --> 02:27:41.640]   for a good civilization be?
[02:27:41.640 --> 02:27:43.160]   If I had to pick a set of metrics,
[02:27:43.160 --> 02:27:44.080]   what would the best ones be,
[02:27:44.080 --> 02:27:46.200]   if I was gonna optimize for those?
[02:27:46.200 --> 02:27:49.140]   And then really try to run the thought experiment
[02:27:49.140 --> 02:27:51.140]   more deeply and say, okay,
[02:27:51.140 --> 02:27:53.320]   so what happens if we optimize for that?
[02:27:53.320 --> 02:27:56.360]   Try to think through the first and second
[02:27:56.360 --> 02:27:58.880]   and third order effects of what happens that's positive,
[02:27:58.880 --> 02:28:01.580]   and then also say what negative things
[02:28:01.580 --> 02:28:02.820]   can happen from optimizing that?
[02:28:02.820 --> 02:28:05.540]   What actually matters that is not included in that
[02:28:05.540 --> 02:28:06.820]   or in that way of defining it?
[02:28:06.820 --> 02:28:09.860]   Because love versus number of positive thoughts per day,
[02:28:09.860 --> 02:28:11.740]   I could just make a long list of names
[02:28:11.740 --> 02:28:13.780]   and just say positive thing about each one.
[02:28:13.780 --> 02:28:15.580]   It's all very superficial.
[02:28:15.580 --> 02:28:17.620]   Not include animals or the rest of life,
[02:28:17.620 --> 02:28:20.780]   have a very shallow total amount of it,
[02:28:20.780 --> 02:28:21.900]   but I'm optimizing the number,
[02:28:21.900 --> 02:28:23.580]   and if I get some credit for the number.
[02:28:23.580 --> 02:28:25.580]   So, and this is when I said
[02:28:25.580 --> 02:28:27.340]   the model of reality isn't reality.
[02:28:27.340 --> 02:28:30.700]   When you make a set of metrics,
[02:28:30.700 --> 02:28:32.200]   say we're gonna optimize for this,
[02:28:32.200 --> 02:28:35.420]   whatever reality is that is not included in those metrics
[02:28:35.420 --> 02:28:36.800]   can be the areas where harm occurs,
[02:28:36.800 --> 02:28:40.620]   which is why I would say that wisdom is something like
[02:28:43.780 --> 02:28:47.280]   the discernment that leads to right choices
[02:28:47.280 --> 02:28:52.940]   beyond what metrics-based optimization would offer.
[02:28:52.940 --> 02:28:57.060]   - Yeah, but another way to say that is
[02:28:57.060 --> 02:29:03.100]   wisdom is a constantly expanding
[02:29:03.100 --> 02:29:04.980]   and evolving set of metrics.
[02:29:04.980 --> 02:29:07.940]   - Which means that there is something in you
[02:29:07.940 --> 02:29:10.100]   that is recognizing a new metric that's important
[02:29:10.100 --> 02:29:11.420]   that isn't part of that metric set.
[02:29:11.420 --> 02:29:13.820]   So, there's a certain kind of connection,
[02:29:13.820 --> 02:29:17.420]   discernment, awareness, and this is--
[02:29:17.420 --> 02:29:18.940]   - It's iterative game theory.
[02:29:18.940 --> 02:29:20.700]   - There's a Girdles incompleteness theorem, right?
[02:29:20.700 --> 02:29:23.400]   Which is if the set of things is consistent,
[02:29:23.400 --> 02:29:24.240]   it won't be complete.
[02:29:24.240 --> 02:29:25.460]   So, we're gonna keep adding to it,
[02:29:25.460 --> 02:29:27.540]   which is why we were saying earlier,
[02:29:27.540 --> 02:29:29.180]   I don't think it's not beautiful.
[02:29:29.180 --> 02:29:31.340]   And especially if you were just saying
[02:29:31.340 --> 02:29:32.700]   one of the metrics you wanna optimize for
[02:29:32.700 --> 02:29:34.500]   at the individual level is becoming, right?
[02:29:34.500 --> 02:29:35.340]   That we're becoming more.
[02:29:35.340 --> 02:29:37.300]   Well, that then becomes true for the civilization
[02:29:37.300 --> 02:29:38.740]   and our metric sets as well.
[02:29:39.620 --> 02:29:42.420]   And our definition of how to think about a meaningful life
[02:29:42.420 --> 02:29:44.220]   and a meaningful civilization.
[02:29:44.220 --> 02:29:46.540]   I can tell you what some of my favorite metrics are.
[02:29:46.540 --> 02:29:47.380]   - What's that?
[02:29:47.380 --> 02:29:52.380]   - Well, love is obviously not a metric.
[02:29:52.380 --> 02:29:53.860]   - Oh, what you can bench?
[02:29:53.860 --> 02:29:55.060]   - Yeah. - It's a good metric.
[02:29:55.060 --> 02:29:57.460]   - Yeah, I wanna optimize that across the entire population,
[02:29:57.460 --> 02:29:58.560]   starting with infants.
[02:29:58.560 --> 02:30:05.260]   So, in the same way that love isn't a metric,
[02:30:05.260 --> 02:30:07.380]   but you could make metrics that look at certain parts of it.
[02:30:07.380 --> 02:30:08.980]   The thing I'm about to say isn't a metric,
[02:30:08.980 --> 02:30:11.260]   but it's a consideration.
[02:30:11.260 --> 02:30:12.340]   'Cause I thought about this a lot.
[02:30:12.340 --> 02:30:14.700]   I don't think there is a metric, a right one.
[02:30:14.700 --> 02:30:18.420]   I think that every metric by itself,
[02:30:18.420 --> 02:30:19.660]   without this thing we talked about,
[02:30:19.660 --> 02:30:20.740]   of the continuous improvement
[02:30:20.740 --> 02:30:22.620]   becomes a paperclip maximizer.
[02:30:22.620 --> 02:30:26.540]   I think that's what the idea of false idol means
[02:30:26.540 --> 02:30:29.700]   in terms of the model of reality not being reality.
[02:30:29.700 --> 02:30:32.460]   Then my sacred relationship is to reality itself,
[02:30:32.460 --> 02:30:34.940]   which also binds me to the unknown forever.
[02:30:34.940 --> 02:30:36.420]   To the known, but also to the unknown.
[02:30:36.420 --> 02:30:39.660]   And there's a sense of sacredness connected to the unknown
[02:30:39.660 --> 02:30:41.300]   that creates an epistemic humility
[02:30:41.300 --> 02:30:43.860]   that is always seeking not just to optimize the thing I know,
[02:30:43.860 --> 02:30:45.740]   but to learn new stuff.
[02:30:45.740 --> 02:30:47.540]   And to be open to perceive reality directly.
[02:30:47.540 --> 02:30:48.980]   So, my model never becomes sacred.
[02:30:48.980 --> 02:30:49.940]   My model is useful.
[02:30:49.940 --> 02:30:52.980]   - So, the model can't be the false idol.
[02:30:52.980 --> 02:30:54.540]   - Correct.
[02:30:54.540 --> 02:30:57.780]   And this is why the first verse of the Tao Te Ching
[02:30:57.780 --> 02:31:00.540]   is the Tao that is nameable is not the eternal Tao.
[02:31:00.540 --> 02:31:03.100]   The naming then can become the source of the 10,000 things
[02:31:03.100 --> 02:31:04.620]   that if you get too carried away with it,
[02:31:04.620 --> 02:31:07.260]   can actually obscure you from paying attention to reality
[02:31:07.260 --> 02:31:09.300]   beyond the models.
[02:31:09.300 --> 02:31:11.540]   - It sounds a lot like Stephen Wolfram,
[02:31:11.540 --> 02:31:14.020]   but in a different language, much more poetic.
[02:31:14.020 --> 02:31:15.420]   - I can imagine that.
[02:31:15.420 --> 02:31:17.820]   - No, I'm referring to, I'm joking.
[02:31:17.820 --> 02:31:19.820]   But there's echoes of cellular automata,
[02:31:19.820 --> 02:31:21.260]   which you can't name.
[02:31:21.260 --> 02:31:24.300]   You can't construct a good model of cellular automata.
[02:31:24.300 --> 02:31:25.920]   You can only watch in awe.
[02:31:25.920 --> 02:31:27.940]   I apologize.
[02:31:27.940 --> 02:31:29.580]   I'm distracting your train of thought
[02:31:29.580 --> 02:31:31.140]   horribly and miserably.
[02:31:31.140 --> 02:31:31.980]   Making a difference.
[02:31:31.980 --> 02:31:34.380]   By the way, something robots aren't good at.
[02:31:34.380 --> 02:31:37.420]   And dealing with the uncertainty of uneven ground.
[02:31:37.420 --> 02:31:38.980]   You've been okay so far.
[02:31:38.980 --> 02:31:40.300]   You've been doing wonderfully.
[02:31:40.300 --> 02:31:41.660]   So what's your favorite metrics?
[02:31:41.660 --> 02:31:42.500]   - Okay, so-- - That's why I know
[02:31:42.500 --> 02:31:43.320]   you're not a robot.
[02:31:43.320 --> 02:31:44.220]   You passed an interim test.
[02:31:44.220 --> 02:31:48.220]   - So one metric, and there are problems with this,
[02:31:48.220 --> 02:31:50.300]   but one metric that I like to just,
[02:31:50.300 --> 02:31:52.420]   as a thought experiment to consider,
[02:31:52.420 --> 02:31:55.380]   is 'cause you're actually asking,
[02:31:55.380 --> 02:31:58.420]   we're, I mean, I know you ask your guests
[02:31:58.420 --> 02:31:59.380]   about the meaning of life.
[02:31:59.380 --> 02:32:01.220]   'Cause ultimately when you're saying
[02:32:01.220 --> 02:32:03.660]   what is a desirable civilization,
[02:32:03.660 --> 02:32:05.740]   you can't answer that without answering
[02:32:05.740 --> 02:32:07.220]   what is a meaningful human life?
[02:32:07.220 --> 02:32:10.140]   And to say what is a good civilization?
[02:32:10.140 --> 02:32:12.740]   'Cause it's gonna be in relationship to that, right?
[02:32:12.740 --> 02:32:19.020]   And then you have whatever your answer is,
[02:32:19.020 --> 02:32:19.860]   how do you know?
[02:32:19.860 --> 02:32:23.940]   What is the epistemic basis for postulating that?
[02:32:23.940 --> 02:32:26.420]   - There's also a whole 'nother reason
[02:32:26.420 --> 02:32:27.540]   for asking that question.
[02:32:27.540 --> 02:32:31.700]   I don't, I mean, that doesn't even apply to you whatsoever,
[02:32:31.700 --> 02:32:34.900]   which is it's interesting how few people
[02:32:34.900 --> 02:32:39.300]   have been asked questions like it.
[02:32:39.300 --> 02:32:43.820]   We joke about these questions as silly.
[02:32:43.820 --> 02:32:44.820]   - Right.
[02:32:44.820 --> 02:32:47.940]   - It's funny to watch a person.
[02:32:47.940 --> 02:32:49.420]   And if I was more of an asshole,
[02:32:49.420 --> 02:32:51.620]   I would really stick on that question.
[02:32:51.620 --> 02:32:52.580]   - Right.
[02:32:52.580 --> 02:32:54.580]   - It's a silly question in some sense,
[02:32:54.580 --> 02:32:58.140]   but we haven't really considered what it means.
[02:32:58.140 --> 02:33:00.580]   Just a more concrete version of that question
[02:33:00.580 --> 02:33:02.900]   is what is a better world?
[02:33:02.900 --> 02:33:04.900]   What is the kind of world we're trying to create?
[02:33:04.900 --> 02:33:06.140]   Really?
[02:33:06.140 --> 02:33:08.460]   Have you really thought about the kind of world?
[02:33:08.460 --> 02:33:10.900]   - I'll give you some kind of simple answers to that
[02:33:10.900 --> 02:33:12.940]   that are meaningful to me.
[02:33:12.940 --> 02:33:14.580]   But let me do the societal indices first,
[02:33:14.580 --> 02:33:15.420]   'cause they're fun.
[02:33:15.420 --> 02:33:16.260]   - Yes.
[02:33:16.260 --> 02:33:19.100]   - We should take a note of this meaningful thing,
[02:33:19.100 --> 02:33:21.180]   'cause it's important to come back to.
[02:33:21.180 --> 02:33:22.420]   - Are you reminding me to ask you
[02:33:22.420 --> 02:33:23.380]   about the meaning of life?
[02:33:23.380 --> 02:33:24.220]   Noted.
[02:33:24.220 --> 02:33:25.060]   - I know.
[02:33:25.060 --> 02:33:27.460]   - Let me jot that down.
[02:33:28.460 --> 02:33:31.540]   - So, 'cause I think I stopped tracking it
[02:33:31.540 --> 02:33:33.180]   like 25 open threads.
[02:33:33.180 --> 02:33:35.220]   Okay.
[02:33:35.220 --> 02:33:36.060]   - Let it all burn.
[02:33:36.060 --> 02:33:40.060]   - One index that I find very interesting
[02:33:40.060 --> 02:33:43.020]   is the inverse correlation of addiction within the society.
[02:33:43.020 --> 02:33:47.460]   The more a society produces addiction
[02:33:47.460 --> 02:33:49.180]   within the people in it,
[02:33:49.180 --> 02:33:52.300]   the less healthy I think the society is
[02:33:52.300 --> 02:33:54.740]   as a pretty fundamental metric.
[02:33:54.740 --> 02:33:56.980]   And so the more the individuals feel
[02:33:56.980 --> 02:34:00.380]   that there are less compulsive things
[02:34:00.380 --> 02:34:02.900]   in compelling them to behave in ways
[02:34:02.900 --> 02:34:05.100]   that are destructive to their own values.
[02:34:05.100 --> 02:34:09.260]   And insofar as a civilization is conditioning
[02:34:09.260 --> 02:34:12.140]   and influencing the individuals within it,
[02:34:12.140 --> 02:34:13.660]   the inverse of addiction.
[02:34:13.660 --> 02:34:15.620]   - Broadly defined.
[02:34:15.620 --> 02:34:16.460]   - Correct.
[02:34:16.460 --> 02:34:17.300]   - Addiction.
[02:34:17.300 --> 02:34:18.540]   What's it?
[02:34:18.540 --> 02:34:21.220]   - Compulsive behavior that is destructive
[02:34:21.220 --> 02:34:22.700]   towards things that we value.
[02:34:26.900 --> 02:34:28.180]   - Yeah.
[02:34:28.180 --> 02:34:29.940]   - I think that's a very interesting one to think about.
[02:34:29.940 --> 02:34:31.100]   - That's a really interesting one, yeah.
[02:34:31.100 --> 02:34:32.740]   - And this is then also where comfort
[02:34:32.740 --> 02:34:35.460]   and addiction start to get very close.
[02:34:35.460 --> 02:34:38.900]   And the ability to go in the other direction from addiction
[02:34:38.900 --> 02:34:41.660]   is the ability to be exposed to hypernormal stimuli
[02:34:41.660 --> 02:34:46.460]   and not go down the path of desensitizing to other stimuli
[02:34:46.460 --> 02:34:48.540]   and needing that hypernormal stimuli,
[02:34:48.540 --> 02:34:50.980]   which does involve a kind of hormesis.
[02:34:50.980 --> 02:34:54.500]   So I do think the civilization of the future
[02:34:54.500 --> 02:34:58.740]   has to create something like ritualized discomfort.
[02:34:58.740 --> 02:35:01.380]   And...
[02:35:01.380 --> 02:35:05.100]   - Ritualized discomfort.
[02:35:05.100 --> 02:35:06.260]   - Yeah.
[02:35:06.260 --> 02:35:09.500]   I think that's what the sweat lodge and the vision quest
[02:35:09.500 --> 02:35:11.980]   and the solo journey and the ayahuasca journey
[02:35:11.980 --> 02:35:13.420]   and the Sundance were.
[02:35:13.420 --> 02:35:16.020]   I think it's even a big part of what yoga asana was,
[02:35:16.020 --> 02:35:21.020]   is to make beings that are resilient and strong,
[02:35:21.020 --> 02:35:23.060]   they have to overcome some things.
[02:35:23.060 --> 02:35:25.580]   To make beings that can control their own mind and fear,
[02:35:25.580 --> 02:35:27.500]   they have to face some fears.
[02:35:27.500 --> 02:35:31.140]   But we don't want to put everybody in war or real trauma.
[02:35:31.140 --> 02:35:34.780]   And yet we can see that the most fucked up people we know
[02:35:34.780 --> 02:35:36.380]   had childhoods of a lot of trauma,
[02:35:36.380 --> 02:35:38.340]   but some of the most incredible people we know
[02:35:38.340 --> 02:35:40.060]   had childhoods of a lot of trauma,
[02:35:40.060 --> 02:35:41.980]   whether or not they happened to make it through
[02:35:41.980 --> 02:35:43.220]   and overcome that or not.
[02:35:43.220 --> 02:35:48.220]   So how do we get the benefits of the stealing of character
[02:35:48.220 --> 02:35:49.820]   and the resilience and the whatever
[02:35:49.820 --> 02:35:51.140]   that happened from the difficulty
[02:35:51.140 --> 02:35:52.940]   without traumatizing people?
[02:35:52.940 --> 02:35:56.300]   A certain kind of ritualized discomfort
[02:35:56.300 --> 02:36:01.620]   that not only has us overcome something by ourselves,
[02:36:01.620 --> 02:36:03.220]   but overcome it together with each other
[02:36:03.220 --> 02:36:04.820]   where nobody bails when it gets hard
[02:36:04.820 --> 02:36:05.780]   'cause the other people are there.
[02:36:05.780 --> 02:36:08.060]   So it's both a resilience of the individuals
[02:36:08.060 --> 02:36:09.660]   and a resilience of the bonding.
[02:36:09.660 --> 02:36:12.780]   So I think we'll keep getting more and more
[02:36:12.780 --> 02:36:16.300]   comfortable stuff, but we have to also develop resilience
[02:36:16.300 --> 02:36:21.100]   in the presence of that for the anti-addiction direction
[02:36:21.100 --> 02:36:23.060]   and the fullness of character
[02:36:23.060 --> 02:36:24.900]   and the trustworthiness to others.
[02:36:24.900 --> 02:36:29.260]   - So you have to be consistently injecting discomfort
[02:36:29.260 --> 02:36:31.180]   into the system, ritualize.
[02:36:31.180 --> 02:36:35.300]   I mean, this sounds like you have to imagine Sisyphus happy.
[02:36:35.300 --> 02:36:39.140]   You have to imagine Sisyphus with his rock,
[02:36:39.140 --> 02:36:46.060]   optimally resilient from a metrics perspective in society.
[02:36:47.020 --> 02:36:52.020]   So we want to constantly be throwing rocks at ourselves.
[02:36:52.020 --> 02:36:53.400]   - Not constantly.
[02:36:53.400 --> 02:36:56.140]   You didn't have to do- - Frequently.
[02:36:56.140 --> 02:36:58.860]   - Periodically. - Periodically.
[02:36:58.860 --> 02:37:00.740]   - And there's different levels of intensity,
[02:37:00.740 --> 02:37:01.620]   different periodacies.
[02:37:01.620 --> 02:37:04.720]   Now, I do not think this should be imposed by states.
[02:37:04.720 --> 02:37:07.900]   I think it should emerge from cultures.
[02:37:07.900 --> 02:37:11.020]   And I think the cultures are developing people
[02:37:11.020 --> 02:37:12.500]   that understand the value of it.
[02:37:13.340 --> 02:37:17.340]   So there is both a cultural cohesion to it,
[02:37:17.340 --> 02:37:20.340]   but there's also a voluntarism because the people value
[02:37:20.340 --> 02:37:22.380]   the thing that is being developed, they understand it.
[02:37:22.380 --> 02:37:24.300]   - And that's what conditioning, so it's conditioning.
[02:37:24.300 --> 02:37:28.140]   It's conditioning some of these values.
[02:37:28.140 --> 02:37:29.300]   - Conditioning is a bad word
[02:37:29.300 --> 02:37:30.900]   because we like our idea of sovereignty,
[02:37:30.900 --> 02:37:34.380]   but when we recognize the language that we speak
[02:37:34.380 --> 02:37:35.980]   and the words that we think in
[02:37:35.980 --> 02:37:39.220]   and the patterns of thought built into that language
[02:37:39.220 --> 02:37:40.380]   and the aesthetics that we like,
[02:37:40.380 --> 02:37:42.340]   and so much is conditioned in us
[02:37:42.340 --> 02:37:43.780]   just based on where we're born,
[02:37:43.780 --> 02:37:45.380]   you can't not condition people.
[02:37:45.380 --> 02:37:47.340]   So all you can do is take more responsibility
[02:37:47.340 --> 02:37:48.860]   for what the conditioning factors are,
[02:37:48.860 --> 02:37:50.380]   and then you have to think about this question
[02:37:50.380 --> 02:37:51.700]   of what is a meaningful human life?
[02:37:51.700 --> 02:37:55.140]   Because we're, unlike the other animals born into environment
[02:37:55.140 --> 02:37:56.680]   that they're genetically adapted for,
[02:37:56.680 --> 02:37:58.140]   we're building new environments
[02:37:58.140 --> 02:37:59.700]   that we were not adapted for,
[02:37:59.700 --> 02:38:02.100]   and then we're becoming affected by those.
[02:38:02.100 --> 02:38:04.400]   So then we have to say, well, what kinds of environments,
[02:38:04.400 --> 02:38:06.460]   digital environments, physical environments,
[02:38:06.460 --> 02:38:10.820]   social environments would we want to create
[02:38:10.820 --> 02:38:13.820]   that would develop the healthiest, happiest,
[02:38:13.820 --> 02:38:16.460]   most moral, noble, meaningful people?
[02:38:16.460 --> 02:38:18.220]   What are even those sets of things that matter?
[02:38:18.220 --> 02:38:21.680]   So you end up getting deep existential consideration
[02:38:21.680 --> 02:38:23.380]   at the heart of civilization design
[02:38:23.380 --> 02:38:25.200]   when you start to realize how powerful we're becoming
[02:38:25.200 --> 02:38:27.100]   and how much what we're building it
[02:38:27.100 --> 02:38:29.180]   in service towards matters.
[02:38:29.180 --> 02:38:33.020]   - Before I pull, I think, three threads you just laid down,
[02:38:33.020 --> 02:38:35.740]   is there another metric index that you're interested in?
[02:38:35.740 --> 02:38:37.700]   - I'll tell you one more that I really like.
[02:38:37.700 --> 02:38:42.800]   There's a number, but the next one that comes to mind is,
[02:38:42.800 --> 02:38:48.200]   I have to make a very quick model.
[02:38:48.200 --> 02:38:53.100]   Healthy human bonding,
[02:38:53.100 --> 02:38:56.100]   say we were in a tribal type setting,
[02:38:56.100 --> 02:38:58.360]   my positive emotional states
[02:38:58.360 --> 02:39:00.580]   and your positive emotional states
[02:39:00.580 --> 02:39:02.740]   would most of the time be correlated,
[02:39:02.740 --> 02:39:04.480]   your negative emotional states and mine.
[02:39:04.480 --> 02:39:07.160]   And so you start laughing, I start laughing,
[02:39:07.160 --> 02:39:09.780]   you start crying, my eyes might tear up.
[02:39:09.780 --> 02:39:14.780]   And we would call that the compassion-compersion axis.
[02:39:14.780 --> 02:39:18.780]   I would, this is a model I find useful.
[02:39:18.780 --> 02:39:20.860]   So compassion is when you're feeling something negative,
[02:39:20.860 --> 02:39:22.900]   I feel some pain, I feel some empathy,
[02:39:22.900 --> 02:39:24.400]   something in relationship.
[02:39:24.400 --> 02:39:27.260]   Compersion is when you do well, I'm stoked for you.
[02:39:27.260 --> 02:39:29.740]   Right, like I actually feel happiness at your happiness.
[02:39:29.740 --> 02:39:31.220]   - I like compersion.
[02:39:31.220 --> 02:39:34.060]   - Yeah, the fact that it's such an uncommon word in English
[02:39:34.060 --> 02:39:36.600]   is actually a problem culturally.
[02:39:36.600 --> 02:39:37.440]   - 'Cause I feel that often
[02:39:37.440 --> 02:39:39.760]   and I think that's a really good feeling to feel
[02:39:39.760 --> 02:39:40.900]   and maximize for actually.
[02:39:40.900 --> 02:39:42.620]   - That's actually the metric I'm gonna say.
[02:39:42.620 --> 02:39:43.500]   - Oh, wow.
[02:39:43.500 --> 02:39:46.020]   - Is the compassion-compersion axis
[02:39:46.020 --> 02:39:47.540]   is the thing I would optimize for.
[02:39:47.540 --> 02:39:51.400]   Now, there is a state where my emotional states
[02:39:51.400 --> 02:39:54.680]   and your emotional states are just totally decoupled.
[02:39:54.680 --> 02:39:57.260]   And that is like sociopathy.
[02:39:57.260 --> 02:39:59.700]   I don't want to hurt you, but I don't care if I do
[02:39:59.700 --> 02:40:01.480]   or for you to do well or whatever.
[02:40:01.480 --> 02:40:03.500]   But there's a worse state and it's extremely common,
[02:40:03.500 --> 02:40:05.980]   which is where they're inversely coupled,
[02:40:05.980 --> 02:40:09.620]   where my positive emotions correspond to your negative ones
[02:40:09.620 --> 02:40:11.120]   and vice versa.
[02:40:11.120 --> 02:40:16.060]   And that is the, I would call it the jealousy-sadism axis.
[02:40:16.060 --> 02:40:19.460]   The jealousy axis is when you're doing really well,
[02:40:19.460 --> 02:40:20.700]   I feel something bad.
[02:40:20.700 --> 02:40:25.700]   I feel taken away from, less than, upset, envious, whatever.
[02:40:25.700 --> 02:40:28.920]   And that's so common.
[02:40:30.580 --> 02:40:34.380]   But I think of it as kind of a low-grade psychopathology
[02:40:34.380 --> 02:40:35.740]   that we've just normalized.
[02:40:35.740 --> 02:40:39.540]   The idea that I'm actually upset at the happiness
[02:40:39.540 --> 02:40:40.920]   or fulfillment or success of another
[02:40:40.920 --> 02:40:43.360]   is like a profoundly fucked up thing.
[02:40:43.360 --> 02:40:45.440]   No, we shouldn't shame it and repress it so it gets worse.
[02:40:45.440 --> 02:40:46.500]   We should study it.
[02:40:46.500 --> 02:40:47.340]   Where does it come from?
[02:40:47.340 --> 02:40:50.580]   And it comes from our own insecurities and stuff.
[02:40:50.580 --> 02:40:52.480]   But then the next part that everybody knows
[02:40:52.480 --> 02:40:55.100]   is really fucked up is just on the same axis.
[02:40:55.100 --> 02:40:58.540]   It's the same inverted, which is to the jealousy
[02:40:58.540 --> 02:41:02.120]   or the envy is the, I feel badly when you're doing well.
[02:41:02.120 --> 02:41:05.620]   The sadism side is I actually feel good when you lose.
[02:41:05.620 --> 02:41:06.560]   Or when you're in pain,
[02:41:06.560 --> 02:41:07.980]   I feel some happiness that's associated.
[02:41:07.980 --> 02:41:10.020]   And you can see when someone feels jealous,
[02:41:10.020 --> 02:41:12.420]   sometimes they feel jealous with a partner
[02:41:12.420 --> 02:41:14.820]   and then they feel they want that partner to get it.
[02:41:14.820 --> 02:41:17.460]   Revenge comes up or something.
[02:41:17.460 --> 02:41:21.260]   So sadism is really, jealousy is one step on the path
[02:41:21.260 --> 02:41:23.940]   to sadism from the healthy compassion-compersion axis.
[02:41:23.940 --> 02:41:28.240]   So I would like to see a society that is inversely,
[02:41:28.240 --> 02:41:32.660]   that is conditioning sadism and jealousy inversely, right?
[02:41:32.660 --> 02:41:34.300]   The lower that amount
[02:41:34.300 --> 02:41:36.020]   and the more the compassion-compersion.
[02:41:36.020 --> 02:41:37.780]   And if I had to summarize that very simply,
[02:41:37.780 --> 02:41:39.820]   I'd say it would optimize for compersion.
[02:41:39.820 --> 02:41:43.980]   Which is, 'cause notice,
[02:41:43.980 --> 02:41:46.140]   that's not just saying love for you
[02:41:46.140 --> 02:41:48.800]   where I might be self-sacrificing and miserable
[02:41:48.800 --> 02:41:51.220]   and I love people, but I kill myself,
[02:41:51.220 --> 02:41:52.780]   which I don't think anybody thinks a great idea.
[02:41:52.780 --> 02:41:55.000]   Or happiness where I might be sociopathically happy
[02:41:55.000 --> 02:41:56.860]   where I'm causing problems all over the place
[02:41:56.860 --> 02:42:00.600]   or even sadistically happy, but it's a coupling, right?
[02:42:00.600 --> 02:42:03.200]   That I'm actually feeling happiness in relationship to yours
[02:42:03.200 --> 02:42:05.460]   and even in causal relationship where I,
[02:42:05.460 --> 02:42:07.280]   my own agentic desire to get happier
[02:42:07.280 --> 02:42:08.760]   wants to support you too.
[02:42:08.760 --> 02:42:12.260]   - That's actually, speaking of another pickup line,
[02:42:12.260 --> 02:42:15.660]   that's quite honestly what I,
[02:42:15.660 --> 02:42:17.240]   as a guy who's single,
[02:42:17.240 --> 02:42:19.860]   this is gonna come out very ridiculous
[02:42:19.860 --> 02:42:22.620]   'cause it's like, oh yeah, where's your girlfriend, bro?
[02:42:22.620 --> 02:42:27.620]   But that's what I look for in a relationship
[02:42:27.620 --> 02:42:30.340]   'cause it's like, it's so much,
[02:42:30.340 --> 02:42:33.500]   it's so, it's such an amazing life
[02:42:33.500 --> 02:42:37.920]   where you actually get joy from another person's success
[02:42:37.920 --> 02:42:40.020]   and they get joy from your success.
[02:42:40.020 --> 02:42:41.660]   And then it becomes like,
[02:42:41.660 --> 02:42:44.100]   you don't actually need to succeed much
[02:42:44.100 --> 02:42:45.820]   for that to have a, like a,
[02:42:45.820 --> 02:42:49.660]   like a cycle of just like happiness
[02:42:49.660 --> 02:42:52.280]   that just increases like exponentially.
[02:42:52.280 --> 02:42:53.140]   It's weird.
[02:42:53.140 --> 02:42:56.300]   So like just be, just enjoying the happiness of others,
[02:42:56.300 --> 02:42:58.040]   the success of others.
[02:42:58.040 --> 02:43:01.020]   So this is like the, let's call this,
[02:43:01.020 --> 02:43:03.020]   'cause the first person that drilled this into my head
[02:43:03.020 --> 02:43:04.860]   is Rogan, Joe Rogan.
[02:43:04.860 --> 02:43:06.180]   He was the embodiment of that
[02:43:06.180 --> 02:43:11.180]   'cause I saw somebody who was successful, rich,
[02:43:11.180 --> 02:43:15.100]   and nonstop, true, I mean,
[02:43:15.100 --> 02:43:16.700]   you could tell when somebody's full of shit
[02:43:16.700 --> 02:43:18.180]   and somebody's not,
[02:43:18.180 --> 02:43:21.940]   really genuinely enjoying the success of his friends.
[02:43:21.940 --> 02:43:23.060]   That was weird to me.
[02:43:23.060 --> 02:43:23.900]   That was interesting.
[02:43:23.900 --> 02:43:27.340]   And I mean, the way you're kind of speaking to it,
[02:43:27.340 --> 02:43:30.020]   the reason Joe stood out to me
[02:43:30.020 --> 02:43:33.020]   is I guess I haven't witnessed genuine expression
[02:43:33.020 --> 02:43:35.380]   of that often in this culture,
[02:43:35.380 --> 02:43:38.660]   of just real joy for others.
[02:43:38.660 --> 02:43:39.660]   I mean, part of that has to do,
[02:43:39.660 --> 02:43:43.640]   there hasn't been many channels where you can watch
[02:43:43.640 --> 02:43:46.300]   or listen to people being their authentic selves.
[02:43:46.300 --> 02:43:47.660]   So I'm sure there's a bunch of people
[02:43:47.660 --> 02:43:49.580]   who live life with compersion.
[02:43:49.580 --> 02:43:52.420]   They probably don't seek public attention also,
[02:43:52.420 --> 02:43:56.780]   but that was, yeah, if there was any word
[02:43:56.780 --> 02:43:58.740]   that could express what I've learned from Joe
[02:43:58.740 --> 02:44:01.340]   and why he's been a really inspiring figure
[02:44:01.340 --> 02:44:02.820]   is that compersion.
[02:44:02.820 --> 02:44:07.820]   And I wish our world had a lot more of that
[02:44:07.820 --> 02:44:12.580]   'cause then, I mean, my own,
[02:44:12.580 --> 02:44:14.380]   sorry to go in a small tangent,
[02:44:14.380 --> 02:44:19.060]   but you're speaking how society should function,
[02:44:19.060 --> 02:44:21.780]   but I feel like if you optimize for that metric
[02:44:21.780 --> 02:44:23.540]   in your own personal life,
[02:44:23.540 --> 02:44:27.860]   you're going to live a truly fulfilling life.
[02:44:27.860 --> 02:44:30.020]   I don't know what the right word to use,
[02:44:30.020 --> 02:44:32.180]   but that's a really good way to live life.
[02:44:32.180 --> 02:44:36.140]   - You will also learn what gets in the way of it
[02:44:36.140 --> 02:44:38.300]   and how to work with it that if you wanted to help
[02:44:38.300 --> 02:44:40.900]   try to build systems at scale or apply Facebook
[02:44:40.900 --> 02:44:42.820]   or exponential technologies to do that,
[02:44:42.820 --> 02:44:45.540]   you would have more actual depth of real knowledge
[02:44:45.540 --> 02:44:46.860]   of what that takes.
[02:44:48.660 --> 02:44:50.420]   And this is, as you mentioned,
[02:44:50.420 --> 02:44:51.740]   that there's this virtuous cycle
[02:44:51.740 --> 02:44:53.740]   between when you get stoked on other people doing well
[02:44:53.740 --> 02:44:55.700]   and then they have a similar relationship to you
[02:44:55.700 --> 02:44:58.300]   and everyone is in the process of building each other up.
[02:44:58.300 --> 02:45:02.340]   And this is what I would say the healthy version
[02:45:02.340 --> 02:45:04.740]   of competition is versus the unhealthy version.
[02:45:04.740 --> 02:45:08.540]   The healthy version, right, the root,
[02:45:08.540 --> 02:45:11.940]   I believe it's a Latin word that means to strive together.
[02:45:11.940 --> 02:45:14.140]   And it's that impulse of becoming
[02:45:14.140 --> 02:45:15.220]   where I want to become more,
[02:45:15.220 --> 02:45:17.740]   but I recognize that there's actually a hormesis,
[02:45:17.740 --> 02:45:19.340]   there's a challenge that is needed
[02:45:19.340 --> 02:45:21.260]   for me to be able to do that.
[02:45:21.260 --> 02:45:22.980]   But that means that, yes,
[02:45:22.980 --> 02:45:24.620]   there's an impulse where I'm trying to get ahead,
[02:45:24.620 --> 02:45:25.780]   maybe I'm even trying to win,
[02:45:25.780 --> 02:45:27.740]   but I actually want a good opponent
[02:45:27.740 --> 02:45:29.260]   and I want them to get ahead too
[02:45:29.260 --> 02:45:31.620]   because that is where my ongoing becoming happens.
[02:45:31.620 --> 02:45:34.460]   And the win itself will get boring very quickly.
[02:45:34.460 --> 02:45:37.260]   The ongoing becoming is where there's aliveness.
[02:45:37.260 --> 02:45:39.780]   And for the ongoing becoming, they need to have it too.
[02:45:39.780 --> 02:45:41.260]   And that's the strive together.
[02:45:41.260 --> 02:45:42.460]   So in the healthy competition,
[02:45:42.460 --> 02:45:44.340]   I'm stoked when they're doing really well
[02:45:44.340 --> 02:45:47.140]   'cause my becoming is supported by it.
[02:45:47.140 --> 02:45:48.780]   - Now, this is actually a very nice segue
[02:45:48.780 --> 02:45:53.780]   into a model I like about what a meaningful human life is,
[02:45:53.780 --> 02:45:57.900]   if you want to go there.
[02:45:57.900 --> 02:46:00.940]   - Let's go there.
[02:46:00.940 --> 02:46:02.820]   - We can go somewhere else if you want.
[02:46:02.820 --> 02:46:05.820]   - Well, I have three things I'm going elsewhere with,
[02:46:05.820 --> 02:46:07.460]   but if we were first,
[02:46:07.460 --> 02:46:09.500]   let us take a short stroll
[02:46:09.500 --> 02:46:12.220]   through the park of the meaning of life.
[02:46:12.220 --> 02:46:15.100]   Daniel, what is a meaningful life?
[02:46:16.300 --> 02:46:20.300]   - I think the semantics end up mattering
[02:46:20.300 --> 02:46:24.300]   'cause a lot of people will take the word meaning
[02:46:24.300 --> 02:46:27.340]   and the word purpose almost interchangeably.
[02:46:27.340 --> 02:46:30.980]   And they'll think kind of what is the meaning of my life?
[02:46:30.980 --> 02:46:32.100]   What is the meaning of human life?
[02:46:32.100 --> 02:46:32.940]   What is the meaning of life?
[02:46:32.940 --> 02:46:34.780]   What's the meaning of the universe?
[02:46:34.780 --> 02:46:38.620]   And what is the meaning of existence rather than non-existence?
[02:46:38.620 --> 02:46:41.780]   So there's a lot of kind of existential considerations there.
[02:46:41.780 --> 02:46:44.340]   And I think there's some cognitive mistakes
[02:46:44.340 --> 02:46:45.940]   that are very easy.
[02:46:45.940 --> 02:46:48.860]   Like taking the idea of purpose.
[02:46:48.860 --> 02:46:50.300]   - Which is like a goal.
[02:46:50.300 --> 02:46:51.980]   - Which is a utilitarian concept.
[02:46:51.980 --> 02:46:54.500]   The purpose of one thing is defined in relationship
[02:46:54.500 --> 02:46:56.500]   to other things that have assumed value.
[02:46:56.500 --> 02:47:00.780]   And to say, what is the purpose of everything?
[02:47:00.780 --> 02:47:03.380]   Well, it's a purpose is too small of a question.
[02:47:03.380 --> 02:47:05.740]   It's fundamentally a relative question within everything.
[02:47:05.740 --> 02:47:07.660]   What is the purpose of one thing relative to another?
[02:47:07.660 --> 02:47:08.860]   What is the purpose of everything?
[02:47:08.860 --> 02:47:11.100]   And there's nothing outside of it with which to say it.
[02:47:11.100 --> 02:47:14.100]   You actually just got to the limits of the utility
[02:47:14.100 --> 02:47:16.100]   of the concept of purpose.
[02:47:16.100 --> 02:47:17.860]   It doesn't mean it's purposeless in the sense
[02:47:17.860 --> 02:47:19.420]   of something inside of it being purposeless.
[02:47:19.420 --> 02:47:21.380]   It means the concept is too small.
[02:47:21.380 --> 02:47:23.180]   Which is why you end up getting to,
[02:47:23.180 --> 02:47:27.380]   like in Taoism talking about the nature of it.
[02:47:27.380 --> 02:47:30.940]   Rather, there's a fundamental what,
[02:47:30.940 --> 02:47:32.460]   where the why can't go deeper.
[02:47:32.460 --> 02:47:33.620]   It is the nature of it.
[02:47:33.620 --> 02:47:39.180]   But I'm gonna try to speak to a much simpler part,
[02:47:39.180 --> 02:47:40.460]   which is when people think about
[02:47:40.460 --> 02:47:42.300]   what is a meaningful human life?
[02:47:42.300 --> 02:47:44.780]   And kind of if we were to optimize for something
[02:47:44.780 --> 02:47:46.580]   at the level of individual life,
[02:47:46.580 --> 02:47:49.340]   but also how does optimizing for this
[02:47:49.340 --> 02:47:50.620]   at the level of the individual life
[02:47:50.620 --> 02:47:55.380]   lead to the best society for insofar
[02:47:55.380 --> 02:47:57.460]   as people living that way affects others
[02:47:57.460 --> 02:47:59.780]   and long-term to the world as a whole?
[02:47:59.780 --> 02:48:02.300]   And how would we then make a civilization
[02:48:02.300 --> 02:48:04.500]   that was trying to think about these things?
[02:48:04.500 --> 02:48:09.740]   'Cause you can see that there are a lot of dialectics
[02:48:09.780 --> 02:48:13.020]   where there's value on two sides,
[02:48:13.020 --> 02:48:14.620]   individualism and collectivism,
[02:48:14.620 --> 02:48:19.460]   or the ability to accept things
[02:48:19.460 --> 02:48:21.940]   and the ability to push harder and whatever.
[02:48:21.940 --> 02:48:24.260]   And there's failure modes on both sides.
[02:48:24.260 --> 02:48:27.260]   And so when you were starting to say,
[02:48:27.260 --> 02:48:28.780]   okay, individual happiness,
[02:48:28.780 --> 02:48:30.420]   and you're like, wait, fuck, sadists can be happy
[02:48:30.420 --> 02:48:31.260]   while hurting people.
[02:48:31.260 --> 02:48:32.620]   It's not individual happiness, it's love.
[02:48:32.620 --> 02:48:35.500]   But wait, some people can self-sacrifice out of love
[02:48:35.500 --> 02:48:36.500]   in a way that actually ends up
[02:48:36.500 --> 02:48:38.980]   just creating codependency for everybody.
[02:48:38.980 --> 02:48:41.940]   Or, okay, so how do we think about
[02:48:41.940 --> 02:48:43.340]   all those things together?
[02:48:43.340 --> 02:48:51.220]   This kind of came to me as a simple way
[02:48:51.220 --> 02:48:52.780]   that I kind of relate to it is
[02:48:52.780 --> 02:48:57.420]   that a meaningful life involves the mode of being,
[02:48:57.420 --> 02:49:00.060]   the mode of doing, and the mode of becoming.
[02:49:00.060 --> 02:49:02.500]   And it involves a virtuous relationship
[02:49:02.500 --> 02:49:03.620]   between those three.
[02:49:04.460 --> 02:49:09.100]   And that any of those modes on their own
[02:49:09.100 --> 02:49:12.420]   also have failure modes that are not a meaningful life.
[02:49:12.420 --> 02:49:15.180]   The mode of being, the way I would describe it,
[02:49:15.180 --> 02:49:21.180]   if we're talking about the essence of it,
[02:49:21.180 --> 02:49:23.900]   is about taking in and appreciating
[02:49:23.900 --> 02:49:25.980]   the beauty of life that is now.
[02:49:25.980 --> 02:49:28.340]   It's a mode that is in the moment,
[02:49:28.340 --> 02:49:31.300]   and that is largely about being with what is.
[02:49:32.940 --> 02:49:35.260]   It's fundamentally grounded in the nature of experience
[02:49:35.260 --> 02:49:37.340]   and the meaningfulness of experience.
[02:49:37.340 --> 02:49:38.940]   The prima facie meaningfulness
[02:49:38.940 --> 02:49:40.860]   of when I'm having this experience,
[02:49:40.860 --> 02:49:44.060]   I'm not actually asking what the meaning of life is.
[02:49:44.060 --> 02:49:45.020]   I'm actually full of it.
[02:49:45.020 --> 02:49:47.020]   I'm full of experiencing it.
[02:49:47.020 --> 02:49:48.260]   - The momentary experience.
[02:49:48.260 --> 02:49:49.100]   - Yes.
[02:49:49.100 --> 02:49:52.940]   So, taking in the beauty of life.
[02:49:52.940 --> 02:49:56.460]   Doing is adding to the beauty of life.
[02:49:56.460 --> 02:49:57.700]   I'm gonna produce some art.
[02:49:57.700 --> 02:49:58.940]   I'm gonna produce some technology
[02:49:58.940 --> 02:50:01.260]   that will make life easier, more beautiful for somebody else.
[02:50:01.260 --> 02:50:05.340]   I'm going to do some science
[02:50:05.340 --> 02:50:07.420]   that will end up leading to better insights
[02:50:07.420 --> 02:50:09.980]   or other people's ability to appreciate the beauty of life
[02:50:09.980 --> 02:50:11.420]   more because they understand more about it,
[02:50:11.420 --> 02:50:13.260]   or whatever it is, or protect it.
[02:50:13.260 --> 02:50:14.940]   I'm gonna protect it in some way.
[02:50:14.940 --> 02:50:17.580]   But that's adding to, or being in service
[02:50:17.580 --> 02:50:19.700]   of the beauty of life through our doing.
[02:50:19.700 --> 02:50:22.940]   And becoming is getting better at both of those.
[02:50:22.940 --> 02:50:24.540]   Being able to deepen our being,
[02:50:24.540 --> 02:50:26.580]   which is to be able to take in the beauty of life
[02:50:26.580 --> 02:50:29.980]   more profoundly, be more moved by it, touched by it.
[02:50:31.060 --> 02:50:32.820]   And increasing our capacity with doing
[02:50:32.820 --> 02:50:34.540]   to add to the beauty of life more.
[02:50:34.540 --> 02:50:40.420]   And so, I hold that a meaningful life
[02:50:40.420 --> 02:50:42.420]   has to be all three of those.
[02:50:42.420 --> 02:50:46.420]   And where they're not in conflict with each other,
[02:50:46.420 --> 02:50:48.620]   ultimately, it grounds in being.
[02:50:48.620 --> 02:50:52.020]   It grounds in the intrinsic meaningfulness of experience.
[02:50:52.020 --> 02:50:54.340]   And then my doing is ultimately something
[02:50:54.340 --> 02:50:57.540]   that will be able to increase the possibility
[02:50:57.540 --> 02:51:00.180]   of the quality of experience for others.
[02:51:00.180 --> 02:51:03.140]   And my becoming is a deepening on those.
[02:51:03.140 --> 02:51:04.700]   So it grounds in experience,
[02:51:04.700 --> 02:51:07.780]   and also the evolutionary possibility of experience.
[02:51:07.780 --> 02:51:13.540]   - And the point is to oscillate between these,
[02:51:13.540 --> 02:51:16.360]   never getting stuck on any one.
[02:51:16.360 --> 02:51:18.100]   - Yeah.
[02:51:18.100 --> 02:51:18.920]   - Or I suppose in parallel,
[02:51:18.920 --> 02:51:22.020]   well, you can't really, attention is a thing.
[02:51:22.020 --> 02:51:23.860]   You can only allocate attention.
[02:51:26.900 --> 02:51:30.820]   I want moments where I am absorbed in the sunset
[02:51:30.820 --> 02:51:32.940]   and I'm not thinking about what to do next.
[02:51:32.940 --> 02:51:37.260]   And then the fullness of that can make it
[02:51:37.260 --> 02:51:40.780]   to where my doing doesn't come from what's in it for me.
[02:51:40.780 --> 02:51:44.260]   'Cause I actually feel overwhelmingly full already.
[02:51:44.260 --> 02:51:49.160]   And then it's like, how can I make life better
[02:51:49.160 --> 02:51:50.300]   for other people that don't have
[02:51:50.300 --> 02:51:51.820]   as much opportunities I had?
[02:51:51.820 --> 02:51:53.540]   How can I add something wonderful?
[02:51:53.540 --> 02:51:56.460]   How can I just be in the creative process?
[02:51:56.460 --> 02:52:00.460]   And so I think where the doing comes from matters.
[02:52:00.460 --> 02:52:02.940]   And if the doing comes from a fullness of being,
[02:52:02.940 --> 02:52:04.860]   it's inherently going to be paying attention
[02:52:04.860 --> 02:52:05.940]   to externalities.
[02:52:05.940 --> 02:52:08.900]   Or it's more oriented to do that
[02:52:08.900 --> 02:52:10.440]   than if it comes from some emptiness
[02:52:10.440 --> 02:52:12.100]   that is trying to get full in some way
[02:52:12.100 --> 02:52:14.180]   that is willing to cause sacrifices other places
[02:52:14.180 --> 02:52:17.020]   and where a chunk of its attention is internally focused.
[02:52:17.020 --> 02:52:19.860]   And so when Buddha said,
[02:52:19.860 --> 02:52:21.420]   "Desire is the cause of all suffering,"
[02:52:21.420 --> 02:52:24.620]   then later the vow of the Bodhisattva,
[02:52:24.620 --> 02:52:26.620]   which was to show up for all sentient beings
[02:52:26.620 --> 02:52:28.120]   in universe forever,
[02:52:28.120 --> 02:52:30.560]   is a pretty intense thing like desire.
[02:52:30.560 --> 02:52:34.540]   I would say there is a kind of desire,
[02:52:34.540 --> 02:52:36.240]   if we think of desire as a basis for movement,
[02:52:36.240 --> 02:52:37.580]   like a flow or a gradient,
[02:52:37.580 --> 02:52:39.020]   there's a kind of desire that comes
[02:52:39.020 --> 02:52:40.420]   from something missing inside,
[02:52:40.420 --> 02:52:43.060]   seeking fulfillment of that in the world.
[02:52:43.060 --> 02:52:45.100]   That ends up being the cause of actions
[02:52:45.100 --> 02:52:46.820]   that perpetuate suffering.
[02:52:46.820 --> 02:52:49.080]   But there's also not just non-desire,
[02:52:49.080 --> 02:52:52.460]   there's a kind of desire that comes from feeling full
[02:52:52.460 --> 02:52:54.940]   at the beauty of life and wanting to add to it
[02:52:54.940 --> 02:52:56.580]   that is a flow this direction.
[02:52:56.580 --> 02:52:59.860]   And I don't think that is the cause of suffering.
[02:52:59.860 --> 02:53:01.700]   I think that is,
[02:53:01.700 --> 02:53:03.360]   and the Western traditions, right?
[02:53:03.360 --> 02:53:05.260]   The Eastern traditions focused on that
[02:53:05.260 --> 02:53:07.860]   and kind of unconditional happiness outside of them,
[02:53:07.860 --> 02:53:09.220]   in the moment, outside of time.
[02:53:09.220 --> 02:53:10.160]   Western tradition said,
[02:53:10.160 --> 02:53:12.380]   "No, actually desire is the source of creativity
[02:53:12.380 --> 02:53:14.300]   "and we are here to be made in the image
[02:53:14.300 --> 02:53:15.480]   "and likeness of the creator.
[02:53:15.480 --> 02:53:17.560]   "We're here to be fundamentally creative."
[02:53:17.560 --> 02:53:21.120]   But creating from where and in service of what?
[02:53:21.120 --> 02:53:23.080]   Creating from a sense of connection to everything
[02:53:23.080 --> 02:53:25.540]   and wholeness in service of the wellbeing of all of it
[02:53:25.540 --> 02:53:26.440]   is very different.
[02:53:26.440 --> 02:53:31.700]   Which is back to that compassion, compersion axis.
[02:53:31.700 --> 02:53:33.400]   - Being, doing, becoming.
[02:53:33.400 --> 02:53:35.620]   It's pretty powerful.
[02:53:35.620 --> 02:53:40.580]   Also could potentially be algorithmatized
[02:53:40.580 --> 02:53:42.120]   into a robot just saying,
[02:53:49.540 --> 02:53:51.800]   where does death come into that?
[02:53:51.800 --> 02:53:56.000]   Being is forgetting, I mean,
[02:53:56.000 --> 02:53:58.880]   the concept of time completely.
[02:53:58.880 --> 02:54:01.140]   There's a sense to doing and becoming
[02:54:01.140 --> 02:54:05.020]   that has a deadline built in,
[02:54:05.020 --> 02:54:07.440]   the urgency built in.
[02:54:07.440 --> 02:54:09.860]   Do you think death is fundamental to this,
[02:54:09.860 --> 02:54:12.980]   to a meaningful life?
[02:54:12.980 --> 02:54:18.000]   Acknowledging or
[02:54:18.340 --> 02:54:23.340]   feeling the terror of death, like Ernest Becker,
[02:54:23.340 --> 02:54:26.860]   or just acknowledging the uncertainty, the mystery,
[02:54:26.860 --> 02:54:31.300]   the melancholy nature of the fact that the ride ends?
[02:54:31.300 --> 02:54:34.700]   Is that part of this equation or it's not necessary?
[02:54:34.700 --> 02:54:37.540]   - Okay, look at how it could be related.
[02:54:37.540 --> 02:54:39.240]   I've experienced fear of death.
[02:54:39.240 --> 02:54:41.980]   I've also experienced
[02:54:41.980 --> 02:54:46.540]   times where I thought I was gonna die.
[02:54:46.540 --> 02:54:48.660]   It felt extremely peaceful and beautiful.
[02:54:48.660 --> 02:54:54.540]   And it's funny because
[02:54:54.540 --> 02:54:58.620]   if we, we can be afraid of death
[02:54:58.620 --> 02:55:00.580]   because we're afraid of hell or bad reincarnation
[02:55:00.580 --> 02:55:02.980]   or the Bardo or some kind of idea of the afterlife we have,
[02:55:02.980 --> 02:55:05.460]   or we're projecting some kind of sentient suffering.
[02:55:05.460 --> 02:55:07.880]   But if we're afraid of just non-experience,
[02:55:07.880 --> 02:55:12.800]   I noticed that every time I stay up late enough
[02:55:12.800 --> 02:55:14.600]   that I'm really tired,
[02:55:14.600 --> 02:55:18.040]   I'm longing for deep sleep and non-experience, right?
[02:55:18.040 --> 02:55:21.060]   Like I'm actually longing for experience to stop.
[02:55:21.060 --> 02:55:24.040]   And it's not morbid, it's not a bummer.
[02:55:24.040 --> 02:55:27.780]   It's, and I don't mind falling asleep.
[02:55:27.780 --> 02:55:30.880]   And I sometimes when I wake up, wanna go back into it.
[02:55:30.880 --> 02:55:33.980]   And then when it's done, I'm happy to come out of it.
[02:55:33.980 --> 02:55:38.980]   So when we think about death and having finite time here,
[02:55:42.500 --> 02:55:45.680]   and we could talk about if we live for a thousand years
[02:55:45.680 --> 02:55:47.400]   instead of a hundred or something like that,
[02:55:47.400 --> 02:55:48.900]   it would still be finite time.
[02:55:48.900 --> 02:55:52.340]   The one bummer with the age we die is that I generally find
[02:55:52.340 --> 02:55:55.240]   that people mostly start to emotionally mature
[02:55:55.240 --> 02:55:56.700]   just shortly before they die.
[02:55:56.700 --> 02:56:02.960]   But there's,
[02:56:02.960 --> 02:56:07.260]   if I get to live forever,
[02:56:07.260 --> 02:56:11.060]   I can just stay focused on what's in it for me forever.
[02:56:12.060 --> 02:56:17.060]   And if life continues and consciousness and sentience
[02:56:17.060 --> 02:56:21.740]   and people appreciating beauty and adding to it
[02:56:21.740 --> 02:56:23.820]   and becoming continues, my life doesn't,
[02:56:23.820 --> 02:56:27.080]   but my life can have effects that continue well beyond it.
[02:56:27.080 --> 02:56:31.240]   Then life with a capital L starts mattering more to me
[02:56:31.240 --> 02:56:32.160]   than my life.
[02:56:32.160 --> 02:56:34.500]   My life gets to be a part of an in service too.
[02:56:34.500 --> 02:56:38.320]   And the whole thing about when old men plant trees,
[02:56:38.320 --> 02:56:41.300]   the shade of which they'll never get to be in.
[02:56:41.300 --> 02:56:45.680]   I remember the first time I read this poem by Hafez,
[02:56:45.680 --> 02:56:50.000]   the Sufi poet written in like 13th century
[02:56:50.000 --> 02:56:51.160]   or something like that.
[02:56:51.160 --> 02:56:56.160]   And he talked about that if you're lonely to think about him
[02:56:56.160 --> 02:56:58.320]   and he was kind of leaning his spirit
[02:56:58.320 --> 02:57:01.840]   into yours across the distance of a millennium
[02:57:01.840 --> 02:57:04.640]   and would come for you with these poems.
[02:57:04.640 --> 02:57:07.080]   And I was thinking about people a millennium from now
[02:57:07.080 --> 02:57:08.640]   and caring about their experience
[02:57:08.640 --> 02:57:10.440]   and what they'd be suffering if they'd be lonely
[02:57:10.440 --> 02:57:13.000]   and could he offer something that could touch them.
[02:57:13.000 --> 02:57:14.920]   And it's just fucking beautiful.
[02:57:14.920 --> 02:57:18.960]   And so like the most beautiful parts of humans
[02:57:18.960 --> 02:57:20.540]   have to do with something that transcends
[02:57:20.540 --> 02:57:21.540]   what's in it for me.
[02:57:21.540 --> 02:57:25.280]   And death forces you to that.
[02:57:25.280 --> 02:57:27.600]   - So not only does death create the urgency,
[02:57:27.600 --> 02:57:32.600]   it urgency of doing it.
[02:57:32.600 --> 02:57:33.520]   You're very right.
[02:57:33.520 --> 02:57:36.960]   It does have a sense in which it incentivizes
[02:57:36.960 --> 02:57:39.820]   the compersion and the compassion.
[02:57:39.820 --> 02:57:45.040]   - And the widening, you remember Einstein had that quote,
[02:57:45.040 --> 02:57:47.360]   "Something to the effect of it's an optical delusion
[02:57:47.360 --> 02:57:50.160]   "of consciousness to believe there are separate things."
[02:57:50.160 --> 02:57:52.500]   There's this one thing we call universe
[02:57:52.500 --> 02:57:57.500]   and something about us being inside of a prison of perception
[02:57:57.500 --> 02:58:01.200]   that can only see a very narrow little bit of it.
[02:58:02.080 --> 02:58:07.080]   But this might be just some weird disposition of mine,
[02:58:07.080 --> 02:58:11.360]   but when I think about the future after I'm dead
[02:58:11.360 --> 02:58:14.880]   and I think about consciousness,
[02:58:14.880 --> 02:58:19.020]   I think about young people falling in love
[02:58:19.020 --> 02:58:20.760]   for the first time and their experience.
[02:58:20.760 --> 02:58:22.800]   And I think about people being awed by sunsets
[02:58:22.800 --> 02:58:26.620]   and I think about all of it, right?
[02:58:26.620 --> 02:58:30.200]   I can't not feel connected to that.
[02:58:30.200 --> 02:58:35.200]   - Do you feel some sadness to the very high likelihood
[02:58:35.200 --> 02:58:37.520]   that you will be forgotten completely
[02:58:37.520 --> 02:58:39.080]   by all of human history?
[02:58:39.080 --> 02:58:44.080]   You, Daniel, the name, that which cannot be named?
[02:58:44.080 --> 02:58:48.120]   - Systems like to self perpetuate.
[02:58:48.120 --> 02:58:52.460]   Egos do that.
[02:58:52.460 --> 02:58:54.960]   The idea that I might do something meaningful
[02:58:54.960 --> 02:58:56.480]   that future people will appreciate,
[02:58:56.480 --> 02:58:59.280]   of course there's like a certain sweetness to that idea.
[02:59:00.120 --> 02:59:03.480]   But I know how many people did something,
[02:59:03.480 --> 02:59:05.560]   did things that I wouldn't be here without
[02:59:05.560 --> 02:59:06.840]   and that my life would be less without
[02:59:06.840 --> 02:59:08.340]   whose names I will never know.
[02:59:08.340 --> 02:59:12.000]   And I feel a gratitude to them.
[02:59:12.000 --> 02:59:12.900]   I feel a closeness.
[02:59:12.900 --> 02:59:14.000]   I feel touched by that.
[02:59:14.000 --> 02:59:17.760]   And I think to the degree that the future people
[02:59:17.760 --> 02:59:20.800]   are conscious enough, there is a,
[02:59:20.800 --> 02:59:23.320]   you know, a lot of traditions had this kind of,
[02:59:23.320 --> 02:59:25.660]   are we being good ancestors and respect for the ancestors
[02:59:25.660 --> 02:59:27.120]   beyond the names?
[02:59:27.120 --> 02:59:28.860]   I think that's a very healthy idea.
[02:59:29.860 --> 02:59:32.820]   - But let me return to a much less beautiful
[02:59:32.820 --> 02:59:36.460]   and much less pleasant conversation.
[02:59:36.460 --> 02:59:37.500]   You mentioned prison.
[02:59:37.500 --> 02:59:38.800]   - Back to X-Risk, okay.
[02:59:38.800 --> 02:59:42.160]   - And conditioning.
[02:59:42.160 --> 02:59:45.560]   You mentioned something about the state.
[02:59:45.560 --> 02:59:52.580]   So what role, let's talk about companies,
[02:59:52.580 --> 02:59:56.400]   governments, parents, all the mechanisms
[02:59:56.400 --> 02:59:58.720]   that can be a source of conditioning.
[02:59:58.720 --> 03:00:01.820]   Which flavor of ice cream do you like?
[03:00:01.820 --> 03:00:05.620]   Do you think the state is the right thing for the future?
[03:00:05.620 --> 03:00:08.340]   So governments that are elected, democratic systems
[03:00:08.340 --> 03:00:11.580]   that are representing representative democracy.
[03:00:11.580 --> 03:00:16.140]   Is there some kind of political system of governance
[03:00:16.140 --> 03:00:17.860]   that you find appealing?
[03:00:17.860 --> 03:00:22.860]   Is it parents, meaning a very close-knit tribes
[03:00:22.860 --> 03:00:25.860]   of conditioning that's the most essential?
[03:00:25.860 --> 03:00:30.860]   And then you and Michael Malice would happily agree
[03:00:30.860 --> 03:00:35.580]   that it's anarchy, where the state should be dissolved
[03:00:35.580 --> 03:00:38.360]   or destroyed or burned to the ground,
[03:00:38.360 --> 03:00:40.160]   if you're Michael Malice, giggling,
[03:00:40.160 --> 03:00:46.040]   holding the torch as the fire burns.
[03:00:46.040 --> 03:00:47.400]   So which is it?
[03:00:47.400 --> 03:00:49.680]   Is the state, can the state be good
[03:00:49.680 --> 03:00:53.760]   or is the state bad for the conditioning
[03:00:53.760 --> 03:00:55.820]   of a beautiful world?
[03:00:55.820 --> 03:00:58.860]   A or B, this is like an S and T test.
[03:00:58.860 --> 03:01:01.700]   - You like to give these simplified good or bad things.
[03:01:01.700 --> 03:01:06.300]   Would I like the state that we live in currently,
[03:01:06.300 --> 03:01:07.900]   the United States federal government,
[03:01:07.900 --> 03:01:09.700]   to stop existing today?
[03:01:09.700 --> 03:01:11.640]   No, I would really not like that.
[03:01:11.640 --> 03:01:13.060]   I think that'd be not quite bad
[03:01:13.060 --> 03:01:15.340]   for the world in a lot of ways.
[03:01:15.340 --> 03:01:20.980]   Do I think that it's a optimal social system
[03:01:20.980 --> 03:01:23.860]   and maximally just and humane and all those things?
[03:01:23.860 --> 03:01:25.060]   And I want it to continue as is.
[03:01:25.060 --> 03:01:26.980]   No, also not that.
[03:01:26.980 --> 03:01:29.900]   But I am much more interested in it being able to evolve
[03:01:29.900 --> 03:01:34.900]   to a better thing without going through the catastrophe phase
[03:01:34.900 --> 03:01:38.300]   that I think it's just non-existence would give.
[03:01:38.300 --> 03:01:42.520]   - So what size of state is good?
[03:01:42.520 --> 03:01:45.480]   In a sense, like, should we as a human society,
[03:01:45.480 --> 03:01:47.100]   as this world becomes more globalized,
[03:01:47.100 --> 03:01:49.620]   should we be constantly striving to reduce
[03:01:50.460 --> 03:01:54.940]   the, we can put on a map, like right now, literally,
[03:01:54.940 --> 03:01:59.660]   like the centers of power in the world.
[03:01:59.660 --> 03:02:01.820]   Some of them are tech companies,
[03:02:01.820 --> 03:02:03.420]   some of them are governments.
[03:02:03.420 --> 03:02:05.780]   Should we be trying to, as much as possible,
[03:02:05.780 --> 03:02:09.180]   decentralize the power to where it's very difficult
[03:02:09.180 --> 03:02:12.500]   to point on the map the centers of power?
[03:02:12.500 --> 03:02:15.780]   And that means making the state,
[03:02:15.780 --> 03:02:17.500]   however, there's a bunch of different ways
[03:02:17.500 --> 03:02:19.860]   to make the government much smaller.
[03:02:19.860 --> 03:02:24.700]   That could be reducing, in the United States,
[03:02:24.700 --> 03:02:28.540]   reducing the funding for the government,
[03:02:28.540 --> 03:02:29.380]   all those kinds of things,
[03:02:29.380 --> 03:02:33.420]   the set of responsibilities, the set of powers.
[03:02:33.420 --> 03:02:36.720]   It could be, I mean, this is far out,
[03:02:36.720 --> 03:02:41.220]   but making more nations, or maybe nations not in the space
[03:02:41.220 --> 03:02:43.740]   that are defined by geographic location,
[03:02:43.740 --> 03:02:45.700]   but rather in the space of ideas,
[03:02:45.700 --> 03:02:47.360]   which is what anarchy is about.
[03:02:47.360 --> 03:02:49.220]   So anarchy is about forming collectives
[03:02:49.220 --> 03:02:52.860]   based on their set of ideas, and doing so dynamically,
[03:02:52.860 --> 03:02:55.060]   not based on where you were born, and so on.
[03:02:55.060 --> 03:03:00.560]   - I think we can say that the natural state of humans,
[03:03:00.560 --> 03:03:03.020]   if we want to describe such a thing,
[03:03:03.020 --> 03:03:07.820]   was to live in tribes that were below the Dunbar number,
[03:03:07.820 --> 03:03:11.520]   meaning that for a few hundred thousand years
[03:03:11.520 --> 03:03:15.140]   of human history, all of the groups of humans
[03:03:15.140 --> 03:03:16.900]   mostly stayed under that size.
[03:03:16.900 --> 03:03:18.180]   And whenever it would get up to that size,
[03:03:18.180 --> 03:03:19.620]   it would end up cleaving.
[03:03:19.620 --> 03:03:22.260]   And so it seems like there's a pretty strong,
[03:03:22.260 --> 03:03:24.260]   but there weren't individual humans out in the wild
[03:03:24.260 --> 03:03:25.180]   doing really well, right?
[03:03:25.180 --> 03:03:26.940]   So we were a group animal,
[03:03:26.940 --> 03:03:28.700]   but with groups that had a specific size.
[03:03:28.700 --> 03:03:30.580]   So we could say, in a way,
[03:03:30.580 --> 03:03:32.600]   humans were being domesticated by those groups.
[03:03:32.600 --> 03:03:34.340]   They were learning how to have certain rules
[03:03:34.340 --> 03:03:35.620]   to participate with the group
[03:03:35.620 --> 03:03:36.860]   without which you'd get kicked out,
[03:03:36.860 --> 03:03:40.300]   but that's still the wild state of people.
[03:03:40.300 --> 03:03:43.660]   - And maybe it's useful to do as a side statement,
[03:03:43.660 --> 03:03:45.740]   which I've recently looked at a bunch of papers
[03:03:45.740 --> 03:03:49.260]   around Dunbar's number, where the mean is actually 150.
[03:03:49.260 --> 03:03:50.820]   If you actually look at the original papers--
[03:03:50.820 --> 03:03:51.660]   - It's a range.
[03:03:51.660 --> 03:03:53.140]   - It's really a range.
[03:03:53.140 --> 03:03:56.060]   So it's actually somewhere under a thousand.
[03:03:56.060 --> 03:03:59.060]   So it's a range of like two to 500 or whatever it is.
[03:03:59.060 --> 03:04:01.300]   But you could argue that the,
[03:04:01.300 --> 03:04:03.940]   I think it actually is exactly two,
[03:04:03.940 --> 03:04:08.240]   the range is two to 520, something like that.
[03:04:08.240 --> 03:04:11.940]   And this is the mean that's taken crudely.
[03:04:11.940 --> 03:04:13.300]   It's not a very good paper
[03:04:14.380 --> 03:04:18.260]   in terms of the actual numerically speaking.
[03:04:18.260 --> 03:04:21.180]   But it'd be interesting if there's a bunch
[03:04:21.180 --> 03:04:24.580]   of Dunbar numbers that could be computed
[03:04:24.580 --> 03:04:26.980]   for particular environments, particular conditions, so on.
[03:04:26.980 --> 03:04:30.420]   It is very true that they're likely to be something small,
[03:04:30.420 --> 03:04:32.220]   you know, under a million.
[03:04:32.220 --> 03:04:34.700]   But it'd be interesting if we could expand that number
[03:04:34.700 --> 03:04:37.380]   in interesting ways that will change the fabric
[03:04:37.380 --> 03:04:38.200]   of this conversation.
[03:04:38.200 --> 03:04:39.740]   I just want to kind of throw that in there.
[03:04:39.740 --> 03:04:42.460]   I don't know if the 150 is baked in somehow
[03:04:42.460 --> 03:04:43.820]   into the hardware.
[03:04:43.820 --> 03:04:45.020]   We can talk about some of the things
[03:04:45.020 --> 03:04:46.980]   that it probably has to do with.
[03:04:46.980 --> 03:04:50.080]   Up to a certain number of people,
[03:04:50.080 --> 03:04:51.740]   and this is gonna be variable based
[03:04:51.740 --> 03:04:54.540]   on the social technologies that mediate it to some degree.
[03:04:54.540 --> 03:04:56.300]   We can talk about that in a minute.
[03:04:56.300 --> 03:05:01.100]   Up to a certain number of people,
[03:05:01.100 --> 03:05:04.260]   everybody can know everybody else pretty intimately.
[03:05:04.260 --> 03:05:07.980]   So let's go ahead and just take 150 as an average number.
[03:05:12.020 --> 03:05:14.420]   Everybody can know everyone intimately enough
[03:05:14.420 --> 03:05:18.740]   that if your actions made anyone else do poorly,
[03:05:18.740 --> 03:05:20.300]   it's your extended family,
[03:05:20.300 --> 03:05:21.780]   and you're stuck living with them,
[03:05:21.780 --> 03:05:22.700]   and you know who they are,
[03:05:22.700 --> 03:05:24.260]   and there's no anonymous people.
[03:05:24.260 --> 03:05:26.640]   There's no just them and over there.
[03:05:26.640 --> 03:05:29.900]   And that's one part of what leads
[03:05:29.900 --> 03:05:32.260]   to a kind of tribal process where what's good
[03:05:32.260 --> 03:05:34.980]   for the individual and good for the whole has a coupling.
[03:05:34.980 --> 03:05:39.460]   Also below that scale, everyone is somewhat aware
[03:05:39.460 --> 03:05:40.840]   of what everybody else is doing.
[03:05:40.840 --> 03:05:44.220]   There's not groups that are very siloed.
[03:05:44.220 --> 03:05:46.140]   And as a result, it's actually very hard
[03:05:46.140 --> 03:05:47.860]   to get away with bad behavior.
[03:05:47.860 --> 03:05:50.540]   There's a forced kind of transparency.
[03:05:50.540 --> 03:05:55.460]   And so you don't need kind of like the state in that way,
[03:05:55.460 --> 03:05:57.940]   but lying to people doesn't actually get you ahead.
[03:05:57.940 --> 03:05:59.740]   Sociopathic behavior doesn't get you ahead
[03:05:59.740 --> 03:06:01.020]   because it gets seen.
[03:06:01.020 --> 03:06:04.920]   And so there's a conditioning environment
[03:06:04.920 --> 03:06:06.860]   where the individual's behaving in a way
[03:06:06.860 --> 03:06:09.260]   that is aligned with the interests of the tribe
[03:06:09.260 --> 03:06:11.320]   is what gets conditioned.
[03:06:11.320 --> 03:06:13.600]   When it gets to be a much larger system,
[03:06:13.600 --> 03:06:16.800]   it becomes easier to hide certain things
[03:06:16.800 --> 03:06:18.000]   from the group as a whole,
[03:06:18.000 --> 03:06:20.040]   as well as to be less emotionally bound
[03:06:20.040 --> 03:06:21.640]   to a bunch of anonymous people.
[03:06:21.640 --> 03:06:26.020]   I would say there's also a communication protocol
[03:06:26.020 --> 03:06:29.300]   where up to about that number of people,
[03:06:29.300 --> 03:06:31.400]   we could all sit around a tribal council
[03:06:31.400 --> 03:06:34.120]   and be part of a conversation around a really big decision.
[03:06:34.120 --> 03:06:34.960]   Do we migrate?
[03:06:34.960 --> 03:06:35.780]   Do we not migrate?
[03:06:35.780 --> 03:06:37.200]   Do we, you know, something like that.
[03:06:37.200 --> 03:06:38.960]   Do we get rid of this person?
[03:06:38.960 --> 03:06:43.960]   And why would I want to agree to be a part of a larger group
[03:06:43.960 --> 03:06:48.980]   where everyone can't be part of that council?
[03:06:48.980 --> 03:06:52.600]   And so I am going to now be subject to law
[03:06:52.600 --> 03:06:53.820]   that I have no say in.
[03:06:53.820 --> 03:06:55.140]   If I could be part of a smaller group
[03:06:55.140 --> 03:06:57.100]   that could still survive and I get a say in the law
[03:06:57.100 --> 03:06:58.060]   that I'm subject to.
[03:06:58.060 --> 03:06:59.860]   So I think the cleaving,
[03:06:59.860 --> 03:07:02.740]   and a way we can look at it beyond the Dunbar number two
[03:07:02.740 --> 03:07:06.560]   is we can look at that a civilization has binding energy
[03:07:06.560 --> 03:07:08.660]   that is holding them together and has cleaving energy.
[03:07:08.660 --> 03:07:10.860]   And if the binding energy exceeds the cleaving energy,
[03:07:10.860 --> 03:07:12.840]   that civilization will last.
[03:07:12.840 --> 03:07:14.500]   And so there are things that we can do
[03:07:14.500 --> 03:07:16.480]   to decrease the cleaving energy within the society,
[03:07:16.480 --> 03:07:18.260]   things we can do to increase the binding energy.
[03:07:18.260 --> 03:07:20.940]   I think naturally we saw that had certain characteristics
[03:07:20.940 --> 03:07:22.940]   up to a certain size kind of tribalism.
[03:07:22.940 --> 03:07:25.780]   That ended with a few things.
[03:07:25.780 --> 03:07:27.660]   It ended with people having migrated enough
[03:07:27.660 --> 03:07:31.420]   that when you started to get resource wars,
[03:07:31.420 --> 03:07:33.260]   you couldn't just migrate away easily.
[03:07:33.260 --> 03:07:35.380]   And so tribal warfare became more obligated.
[03:07:35.380 --> 03:07:37.340]   It involved the plow
[03:07:37.340 --> 03:07:39.540]   in the beginning of real economic surplus.
[03:07:39.540 --> 03:07:43.200]   So there were a few different kind of forcing functions.
[03:07:43.200 --> 03:07:48.380]   But we're talking about what size should it be, right?
[03:07:48.380 --> 03:07:49.900]   What size should a society be?
[03:07:49.900 --> 03:07:51.940]   And I think the idea,
[03:07:51.940 --> 03:07:54.780]   like if we think about your body for a moment
[03:07:54.780 --> 03:07:58.260]   as a self-organizing complex system that is multi-scaled,
[03:07:58.260 --> 03:08:00.380]   we think about-- - Our body is a wonderland.
[03:08:00.380 --> 03:08:02.580]   - Our body is a wonderland, yeah.
[03:08:02.580 --> 03:08:05.460]   You have--
[03:08:05.460 --> 03:08:07.580]   - That's a John Mayer song, I apologize.
[03:08:07.580 --> 03:08:10.360]   But yes, so if we think about our body
[03:08:10.360 --> 03:08:12.260]   and the billions of cells that are in it.
[03:08:12.260 --> 03:08:13.660]   - Well, you don't have,
[03:08:13.660 --> 03:08:15.300]   like think about how ridiculous it would be
[03:08:15.300 --> 03:08:18.800]   to try to have all the tens of trillions of cells in it
[03:08:18.800 --> 03:08:21.720]   with no internal organization structure, right?
[03:08:21.720 --> 03:08:24.540]   Just like a sea of protoplasm, it wouldn't work.
[03:08:24.540 --> 03:08:25.660]   - Pure democracy.
[03:08:25.660 --> 03:08:29.140]   - And so you have cells and tissues,
[03:08:29.140 --> 03:08:31.260]   and then you have tissues and organs
[03:08:31.260 --> 03:08:32.700]   and organs and organ systems.
[03:08:32.700 --> 03:08:34.500]   And so you have these layers of organization.
[03:08:34.500 --> 03:08:37.180]   And then obviously the individual and a tribe
[03:08:37.180 --> 03:08:39.260]   and a ecosystem.
[03:08:39.260 --> 03:08:42.000]   And each of the higher layers
[03:08:42.000 --> 03:08:43.460]   are both based on the lower layers,
[03:08:43.460 --> 03:08:45.260]   but also influencing them.
[03:08:45.260 --> 03:08:48.300]   I think the future of civilization will be similar,
[03:08:48.300 --> 03:08:49.900]   which is there's a level of governance
[03:08:49.900 --> 03:08:51.400]   that happens at the level of the individual,
[03:08:51.400 --> 03:08:54.620]   my own governance of my own choice.
[03:08:54.620 --> 03:08:56.140]   I think there's a level that happens
[03:08:56.140 --> 03:08:57.620]   at the level of a family.
[03:08:57.620 --> 03:08:59.140]   We're making decisions together,
[03:08:59.140 --> 03:09:00.540]   we're inter-influencing each other
[03:09:00.540 --> 03:09:03.780]   and affecting each other, taking responsibility for.
[03:09:03.780 --> 03:09:05.100]   The idea of an extended family.
[03:09:05.100 --> 03:09:06.940]   And you can see that for a lot of human history,
[03:09:06.940 --> 03:09:08.060]   we had an extended family.
[03:09:08.060 --> 03:09:10.000]   We had a local community, a local church,
[03:09:10.000 --> 03:09:10.980]   or whatever it was.
[03:09:10.980 --> 03:09:13.440]   We had these intermediate structures.
[03:09:13.440 --> 03:09:16.100]   Whereas right now there's kind of like the individual
[03:09:16.100 --> 03:09:19.860]   producer, consumer, taxpayer, voter,
[03:09:19.860 --> 03:09:22.520]   and the massive nation state global complex.
[03:09:22.520 --> 03:09:25.020]   And not that much in the way of intermediate structures
[03:09:25.020 --> 03:09:25.860]   that we relate with,
[03:09:25.860 --> 03:09:27.960]   and not that much in the way of real personal dynamics,
[03:09:27.960 --> 03:09:30.900]   all impersonalized, made fungible.
[03:09:30.900 --> 03:09:35.900]   And so I think that we have to have global governance.
[03:09:35.900 --> 03:09:39.580]   Meaning, I think we have to have governance
[03:09:39.580 --> 03:09:40.820]   at the scale we affect stuff.
[03:09:40.820 --> 03:09:43.140]   And if anybody is messing up the oceans,
[03:09:43.140 --> 03:09:44.020]   that matters for everybody.
[03:09:44.020 --> 03:09:48.020]   So that can't only be national or only local.
[03:09:48.020 --> 03:09:49.900]   Everyone is scared of the idea of global governance
[03:09:49.900 --> 03:09:52.620]   'cause we think about some top-down system of imposition
[03:09:52.620 --> 03:09:54.980]   that now has no checks and balances on power.
[03:09:54.980 --> 03:09:56.340]   I'm scared of that same version.
[03:09:56.340 --> 03:09:58.980]   So I'm not talking about that kind of global governance.
[03:10:00.020 --> 03:10:02.540]   It's why I'm even using the word governance as a process
[03:10:02.540 --> 03:10:06.300]   rather than government as an imposed phenomena.
[03:10:06.300 --> 03:10:09.340]   So I think we have to have global governance,
[03:10:09.340 --> 03:10:11.540]   but I think we also have to have local governance.
[03:10:11.540 --> 03:10:13.260]   And there has to be relationships between them
[03:10:13.260 --> 03:10:16.980]   that each, where there are both checks and balances
[03:10:16.980 --> 03:10:18.920]   and power flows of information.
[03:10:18.920 --> 03:10:21.620]   So I think governance at the level of cities
[03:10:21.620 --> 03:10:24.260]   will be a bigger deal in the future
[03:10:24.260 --> 03:10:26.300]   than governance at the level of nation states.
[03:10:26.300 --> 03:10:30.380]   'Cause I think nation states are largely fictitious things
[03:10:30.380 --> 03:10:33.740]   that are defined by wars and agreements to stop wars
[03:10:33.740 --> 03:10:34.580]   and like that.
[03:10:34.580 --> 03:10:36.700]   I think cities are based on real things
[03:10:36.700 --> 03:10:37.540]   that will keep being real
[03:10:37.540 --> 03:10:40.500]   where the proximity of certain things together,
[03:10:40.500 --> 03:10:42.140]   the physical proximity of things together
[03:10:42.140 --> 03:10:44.560]   gives increased value of those things.
[03:10:44.560 --> 03:10:47.500]   So you look at like Jeffrey West's work on scale
[03:10:47.500 --> 03:10:50.520]   and finding that companies and nation states
[03:10:50.520 --> 03:10:52.540]   and things that have a kind of complicated
[03:10:52.540 --> 03:10:54.940]   agreement structure get diminishing return
[03:10:54.940 --> 03:10:56.860]   of production per capita
[03:10:56.860 --> 03:10:58.380]   as the total number of people increases
[03:10:58.380 --> 03:10:59.600]   beyond about the tribal scale,
[03:10:59.600 --> 03:11:01.980]   but the city actually gets increasing productivity
[03:11:01.980 --> 03:11:04.000]   per capita, but it's not designed.
[03:11:04.000 --> 03:11:06.080]   It's kind of this organic thing, right?
[03:11:06.080 --> 03:11:08.500]   So there should be governance at the level of cities
[03:11:08.500 --> 03:11:09.820]   because people can sense
[03:11:09.820 --> 03:11:11.940]   and actually have some agency there.
[03:11:11.940 --> 03:11:14.140]   Probably neighborhoods and smaller scales within it
[03:11:14.140 --> 03:11:15.140]   and also verticals.
[03:11:15.140 --> 03:11:16.500]   And some of it won't be geographic,
[03:11:16.500 --> 03:11:17.580]   it'll be network based, right?
[03:11:17.580 --> 03:11:18.920]   Networks of affinities.
[03:11:18.920 --> 03:11:22.200]   So I don't think the future is one type of governance.
[03:11:22.200 --> 03:11:25.200]   Now, what we can say more broadly is say,
[03:11:25.200 --> 03:11:26.600]   when we're talking about groups of people
[03:11:26.600 --> 03:11:27.840]   that inter-affect each other,
[03:11:27.840 --> 03:11:30.360]   the idea of a civilization is that we can figure out
[03:11:30.360 --> 03:11:32.800]   how to coordinate our choice-making
[03:11:32.800 --> 03:11:34.480]   to not be at war with each other
[03:11:34.480 --> 03:11:38.120]   and hopefully increase total productive capacity
[03:11:38.120 --> 03:11:39.360]   in a way that's good for everybody.
[03:11:39.360 --> 03:11:41.140]   Division of labor and specialty,
[03:11:41.140 --> 03:11:44.820]   so we all get more, better stuff and whatever.
[03:11:44.820 --> 03:11:47.560]   But it's a coordination of our choice-making.
[03:11:49.840 --> 03:11:52.520]   I think we can look at civilizations failing
[03:11:52.520 --> 03:11:55.320]   on the side of not having enough coordination
[03:11:55.320 --> 03:11:57.440]   of choice-making, so they fail on the side of chaos
[03:11:57.440 --> 03:11:58.260]   and then they cleave
[03:11:58.260 --> 03:12:00.840]   and an internal war comes about or whatever.
[03:12:00.840 --> 03:12:04.100]   Or they can't make smart decisions
[03:12:04.100 --> 03:12:06.800]   and they overuse their resources or whatever.
[03:12:06.800 --> 03:12:10.720]   Or it can fail on the side of trying to get order
[03:12:10.720 --> 03:12:14.000]   via imposition, via force.
[03:12:14.000 --> 03:12:15.560]   And so it fails on the side of oppression,
[03:12:15.560 --> 03:12:18.240]   which ends up being for a while,
[03:12:18.240 --> 03:12:21.360]   functional-ish for the thing as a whole,
[03:12:21.360 --> 03:12:22.880]   but miserable for most people in it
[03:12:22.880 --> 03:12:25.260]   until it fails either because of revolt
[03:12:25.260 --> 03:12:28.160]   or because it can't innovate enough or something like that.
[03:12:28.160 --> 03:12:29.560]   And so there's this toggling
[03:12:29.560 --> 03:12:34.240]   between order via oppression and chaos.
[03:12:34.240 --> 03:12:37.120]   And I think the idea of democracy,
[03:12:37.120 --> 03:12:38.580]   not the way we've implemented it,
[03:12:38.580 --> 03:12:39.680]   but the idea of it,
[03:12:39.680 --> 03:12:42.140]   whether we're talking about a representative democracy
[03:12:42.140 --> 03:12:43.640]   or a direct digital democracy,
[03:12:43.640 --> 03:12:46.600]   liquid democracy, a republic, or whatever,
[03:12:46.600 --> 03:12:50.320]   the idea of an open society, participatory governance,
[03:12:50.320 --> 03:12:55.000]   is can we have order that is emergent rather than imposed
[03:12:55.000 --> 03:12:57.800]   so that we aren't stuck with chaos
[03:12:57.800 --> 03:13:00.680]   and infighting and inability to coordinate,
[03:13:00.680 --> 03:13:03.400]   and we're also not stuck with oppression?
[03:13:03.400 --> 03:13:07.600]   And what would it take to have emergent order?
[03:13:07.600 --> 03:13:10.520]   This is the most kind of central question
[03:13:10.520 --> 03:13:12.020]   for me these days,
[03:13:12.020 --> 03:13:17.020]   because if we look at what different nation states
[03:13:17.020 --> 03:13:18.380]   are doing around the world,
[03:13:18.380 --> 03:13:21.820]   and we see nation states that are more authoritarian,
[03:13:21.820 --> 03:13:24.740]   that in some ways are actually coordinating
[03:13:24.740 --> 03:13:26.340]   much more effectively.
[03:13:26.340 --> 03:13:30.960]   So for instance, we can see that China
[03:13:30.960 --> 03:13:33.220]   has built high-speed rail, not just through its country,
[03:13:33.220 --> 03:13:34.220]   but around the world,
[03:13:34.220 --> 03:13:36.780]   and the US hasn't built any high-speed rail yet.
[03:13:36.780 --> 03:13:38.820]   You can see that it brought 300 million people
[03:13:38.820 --> 03:13:39.960]   out of poverty in a time
[03:13:39.960 --> 03:13:43.840]   where we've had increasing economic inequality happening.
[03:13:43.840 --> 03:13:48.100]   You can see that if there was a single country
[03:13:48.100 --> 03:13:49.380]   that could make all of its own stuff,
[03:13:49.380 --> 03:13:51.180]   if the global supply chains failed,
[03:13:51.180 --> 03:13:53.180]   China would be the closest one to being able
[03:13:53.180 --> 03:13:56.480]   to start to go closed loop on fundamental things.
[03:13:56.480 --> 03:13:59.540]   Belt and Road Initiative,
[03:13:59.540 --> 03:14:02.700]   supply chain on rare earth metals,
[03:14:02.700 --> 03:14:04.020]   transistor manufacturing,
[03:14:04.020 --> 03:14:06.220]   that is like, oh, they're actually coordinating
[03:14:06.220 --> 03:14:08.800]   more effectively in some important ways.
[03:14:08.800 --> 03:14:10.400]   In the last, call it 30 years.
[03:14:10.400 --> 03:14:14.280]   And that's imposed order.
[03:14:14.280 --> 03:14:15.620]   - Imposed order.
[03:14:15.620 --> 03:14:20.620]   - And we can see that if in the US,
[03:14:20.620 --> 03:14:22.780]   let's look at why real quick.
[03:14:22.780 --> 03:14:26.340]   We know why we created term limits
[03:14:26.340 --> 03:14:28.940]   so that we wouldn't have forever monarchs.
[03:14:28.940 --> 03:14:30.860]   That's the thing we were trying to get away from,
[03:14:30.860 --> 03:14:32.980]   and that there would be checks and balances on power
[03:14:32.980 --> 03:14:34.420]   and that kind of thing.
[03:14:34.420 --> 03:14:37.580]   But that also has created a negative second order effect,
[03:14:37.580 --> 03:14:39.840]   which is nobody does long-term planning.
[03:14:39.840 --> 03:14:41.820]   Because somebody comes in who's got four years,
[03:14:41.820 --> 03:14:43.140]   they want reelected.
[03:14:43.140 --> 03:14:45.000]   They don't do anything that doesn't create a return
[03:14:45.000 --> 03:14:48.240]   within four years that will end up getting them elected,
[03:14:48.240 --> 03:14:49.080]   reelected.
[03:14:49.080 --> 03:14:51.240]   And so the 30 year industrial development
[03:14:51.240 --> 03:14:54.720]   to build high speed trains or the new kind of fusion energy
[03:14:54.720 --> 03:14:57.100]   or whatever it is, just doesn't get invested in.
[03:14:57.100 --> 03:15:00.420]   And then if you have left versus right,
[03:15:00.420 --> 03:15:02.720]   where whatever someone does for four years,
[03:15:02.720 --> 03:15:05.680]   then the other guy gets in and undoes it for four years.
[03:15:05.680 --> 03:15:07.740]   And most of the energy goes into campaigning
[03:15:07.740 --> 03:15:08.580]   against each other.
[03:15:08.580 --> 03:15:11.300]   This system is just dissipating as heat.
[03:15:11.300 --> 03:15:12.580]   It's just burning up as heat.
[03:15:12.580 --> 03:15:14.300]   And the system that has no term limits
[03:15:14.300 --> 03:15:15.920]   and no internal friction in fighting
[03:15:15.920 --> 03:15:18.100]   because they got rid of those people,
[03:15:18.100 --> 03:15:19.800]   can actually coordinate better.
[03:15:19.800 --> 03:15:24.800]   But I would argue it has its own fail states eventually
[03:15:24.800 --> 03:15:28.740]   and dystopic properties that are not the thing we want.
[03:15:28.740 --> 03:15:30.740]   - So the goal is to accomplish,
[03:15:30.740 --> 03:15:33.600]   to create a system that does long-term planning
[03:15:33.600 --> 03:15:37.140]   without the negative effects of a monarch or dictator
[03:15:37.140 --> 03:15:39.660]   that stays there for the long-term
[03:15:39.660 --> 03:15:46.720]   and accomplish that through,
[03:15:46.720 --> 03:15:52.180]   not through the imposition of a single leader,
[03:15:52.180 --> 03:15:54.100]   but through emergence.
[03:15:54.100 --> 03:15:58.580]   So that doesn't, that perhaps, first of all,
[03:15:58.580 --> 03:16:02.540]   the technology in itself seems to maybe disagree,
[03:16:02.540 --> 03:16:04.900]   allow for different possibilities here,
[03:16:04.900 --> 03:16:08.280]   which is make primary the system, not the humans.
[03:16:08.280 --> 03:16:13.280]   So the basic, the medium on which the democracy happens,
[03:16:13.280 --> 03:16:21.220]   like a platform where people can make decisions,
[03:16:21.220 --> 03:16:26.980]   do the choice-making, the coordination of the choice-making,
[03:16:26.980 --> 03:16:30.980]   where emerges some kind of order
[03:16:30.980 --> 03:16:32.540]   to where like something that applies
[03:16:32.540 --> 03:16:35.460]   at the scale of the family, the extended family,
[03:16:35.460 --> 03:16:40.460]   the city, the country, the continent, the whole world.
[03:16:40.460 --> 03:16:43.180]   And then does that so dynamically,
[03:16:43.180 --> 03:16:45.940]   constantly changing based on the needs of the people,
[03:16:45.940 --> 03:16:47.240]   sort of always evolving.
[03:16:47.240 --> 03:16:50.600]   And it would all be owned by Google.
[03:16:50.600 --> 03:16:56.080]   Like doesn't this, is there a way to,
[03:16:56.080 --> 03:16:58.780]   so first of all, you're optimistic
[03:16:58.780 --> 03:17:00.780]   that you could basically create,
[03:17:00.780 --> 03:17:02.420]   that technology can save us.
[03:17:02.420 --> 03:17:05.020]   Technology at creating platforms,
[03:17:05.020 --> 03:17:08.020]   by technology I mean like software network platforms
[03:17:08.020 --> 03:17:11.280]   that allows humans to deliberate,
[03:17:11.280 --> 03:17:14.700]   like make government together dynamically
[03:17:14.700 --> 03:17:16.500]   without the need for a leader
[03:17:16.500 --> 03:17:19.220]   that's on a podium screaming stuff.
[03:17:19.220 --> 03:17:20.260]   That's one.
[03:17:20.260 --> 03:17:23.340]   And two, if you're optimistic about that,
[03:17:23.340 --> 03:17:27.600]   are you also optimistic about the CEOs of such platforms?
[03:17:27.600 --> 03:17:32.100]   The idea that technology is values neutral,
[03:17:32.100 --> 03:17:35.340]   values agnostic, and people can use it
[03:17:35.340 --> 03:17:37.940]   for constructive or destructive purposes,
[03:17:37.940 --> 03:17:40.100]   but it doesn't predispose anything.
[03:17:40.100 --> 03:17:43.460]   It's just silly and naive.
[03:17:43.460 --> 03:17:46.700]   Technology elicits patterns of human behavior
[03:17:46.700 --> 03:17:49.300]   because those who utilize it and get ahead
[03:17:49.300 --> 03:17:50.380]   end up behaving differently
[03:17:50.380 --> 03:17:51.780]   because of their utilization of it.
[03:17:51.780 --> 03:17:53.580]   And then other people,
[03:17:53.580 --> 03:17:54.900]   then they end up shaping the world
[03:17:54.900 --> 03:17:56.740]   or other people race to also get the power
[03:17:56.740 --> 03:17:57.740]   of the technology.
[03:17:57.740 --> 03:18:00.120]   And so there's whole schools of anthropology
[03:18:00.120 --> 03:18:02.800]   that look at the effect on social systems
[03:18:02.800 --> 03:18:05.920]   and the minds of people of the change in our tooling.
[03:18:05.920 --> 03:18:08.200]   Marvin Harris's work called "Cultural Materialism"
[03:18:08.200 --> 03:18:09.080]   looked at this deeply.
[03:18:09.080 --> 03:18:11.220]   Obviously, Marshall McLuhan looked specifically
[03:18:11.220 --> 03:18:12.760]   at the way that information technologies
[03:18:12.760 --> 03:18:14.480]   change the nature of our beliefs,
[03:18:14.480 --> 03:18:17.040]   minds, values, social systems.
[03:18:17.040 --> 03:18:21.040]   I will not try to do this rigorously
[03:18:21.040 --> 03:18:22.840]   because there are academics
[03:18:22.840 --> 03:18:24.260]   who will disagree on the subtle details,
[03:18:24.260 --> 03:18:26.440]   but I'll do it kind of like illustratively.
[03:18:27.380 --> 03:18:29.580]   You think about the emergence of the plow,
[03:18:29.580 --> 03:18:31.660]   the ox drawn plow in the beginning of agriculture
[03:18:31.660 --> 03:18:32.500]   that came with it,
[03:18:32.500 --> 03:18:34.600]   where before that you had hunter-gatherer
[03:18:34.600 --> 03:18:36.260]   and then you had horticulture,
[03:18:36.260 --> 03:18:40.220]   kind of a digging stick, but not the plow.
[03:18:40.220 --> 03:18:43.180]   Well, the world changed a lot with that, right?
[03:18:43.180 --> 03:18:48.180]   And a few of the changes
[03:18:48.180 --> 03:18:51.580]   that at least some theorists believe in
[03:18:51.580 --> 03:18:55.580]   is when the ox drawn plow started to proliferate,
[03:18:55.580 --> 03:18:56.800]   any culture that utilized it
[03:18:56.800 --> 03:18:58.720]   was able to start to actually cultivate grain
[03:18:58.720 --> 03:19:00.040]   'cause just with a digging stick,
[03:19:00.040 --> 03:19:01.580]   you couldn't get enough grain for it to matter.
[03:19:01.580 --> 03:19:03.360]   Grain was a storable caloric surplus.
[03:19:03.360 --> 03:19:04.480]   They could make it through the famines.
[03:19:04.480 --> 03:19:05.600]   They could grow their population.
[03:19:05.600 --> 03:19:07.560]   So the ones that used it got so much ahead
[03:19:07.560 --> 03:19:10.400]   that it became obligate and everybody used it.
[03:19:10.400 --> 03:19:13.640]   That corresponding with the use of a plow,
[03:19:13.640 --> 03:19:16.120]   animism went away everywhere, that it existed,
[03:19:16.120 --> 03:19:18.640]   because you can't talk about the spirit of the buffalo
[03:19:18.640 --> 03:19:22.000]   while beating the cow all day long to pull a plow.
[03:19:22.000 --> 03:19:24.560]   So the moment that we do animal husbandry of that kind,
[03:19:24.560 --> 03:19:25.680]   where you have to beat the cow all day,
[03:19:25.680 --> 03:19:27.560]   you have to say, it's just a dumb animal.
[03:19:27.560 --> 03:19:28.820]   Man has dominion over earth.
[03:19:28.820 --> 03:19:30.200]   And the nature of even our religious
[03:19:30.200 --> 03:19:31.820]   and spiritual ideas change.
[03:19:31.820 --> 03:19:34.960]   You went from women primarily using the digging stick
[03:19:34.960 --> 03:19:37.760]   to do the horticulture or gathering before that,
[03:19:37.760 --> 03:19:40.240]   men doing the hunting stuff to now men had to use the plow
[03:19:40.240 --> 03:19:42.440]   because the upper body strength actually really mattered.
[03:19:42.440 --> 03:19:44.160]   Women would have miscarriages when they would do it
[03:19:44.160 --> 03:19:45.160]   when they were pregnant.
[03:19:45.160 --> 03:19:47.940]   So all the caloric supply started to come from men
[03:19:47.940 --> 03:19:49.320]   where it had been from both before
[03:19:49.320 --> 03:19:51.820]   and the ratio of male, female gods changed
[03:19:51.820 --> 03:19:54.540]   to being mostly male gods following that.
[03:19:54.540 --> 03:19:57.360]   Obviously we went from very,
[03:19:57.360 --> 03:20:01.080]   that particular line of thought then also says
[03:20:01.080 --> 03:20:03.240]   that feminism followed the tractor
[03:20:03.240 --> 03:20:08.760]   and that the rise of feminism in the West
[03:20:08.760 --> 03:20:10.520]   started to follow women being able to say,
[03:20:10.520 --> 03:20:12.200]   we can do what men can
[03:20:12.200 --> 03:20:15.560]   because the male upper body strength wasn't differential
[03:20:15.560 --> 03:20:18.400]   once the internal combustion engine was much stronger
[03:20:18.400 --> 03:20:20.620]   and we can drive a tractor.
[03:20:20.620 --> 03:20:24.280]   So I don't think to try to trace complex things
[03:20:24.280 --> 03:20:25.700]   to one cause is a good idea.
[03:20:25.700 --> 03:20:27.220]   So I think this is a reductionist view,
[03:20:27.220 --> 03:20:29.240]   but it has truth in it.
[03:20:29.240 --> 03:20:34.240]   And so the idea that technology is values agnostic is silly.
[03:20:34.240 --> 03:20:36.640]   Technology codes patterns of behavior
[03:20:36.640 --> 03:20:39.280]   that code rationalizing those patterns of behavior
[03:20:39.280 --> 03:20:40.320]   and believing in them.
[03:20:40.320 --> 03:20:43.320]   The plow also is the beginning of the Anthropocene, right?
[03:20:43.320 --> 03:20:45.400]   It was the beginning of us changing the environment
[03:20:45.400 --> 03:20:47.720]   radically to clear cut areas
[03:20:47.720 --> 03:20:49.280]   to just make them useful for people,
[03:20:49.280 --> 03:20:51.040]   which also meant the change of the view
[03:20:51.040 --> 03:20:54.160]   of where the web of life, we're just a part of it, et cetera.
[03:20:54.160 --> 03:20:55.720]   So all those types of things.
[03:20:55.720 --> 03:20:58.720]   - So that's brilliantly put,
[03:20:58.720 --> 03:21:01.300]   but by the way, that was just brilliant.
[03:21:01.300 --> 03:21:05.900]   But the question is, so it's not agnostic, but.
[03:21:05.900 --> 03:21:08.720]   - So we have to look at what the psychological effects
[03:21:08.720 --> 03:21:11.600]   of specific tech applied certain ways are
[03:21:11.600 --> 03:21:13.640]   and be able to say,
[03:21:13.640 --> 03:21:16.460]   it's not just doing the first order thing you intended.
[03:21:16.460 --> 03:21:20.480]   It's doing like the effect on patriarchy and animism
[03:21:20.480 --> 03:21:22.840]   and the end of tribal culture
[03:21:22.840 --> 03:21:24.800]   and the beginning of empire and the class systems
[03:21:24.800 --> 03:21:25.880]   that came with that.
[03:21:25.880 --> 03:21:28.640]   We can go on and on about what the plow did.
[03:21:28.640 --> 03:21:30.640]   The beginning of surplus was inheritance,
[03:21:30.640 --> 03:21:34.440]   which then became the capital model and like lots of things.
[03:21:34.440 --> 03:21:36.880]   So we have to say, when we're looking at the tech,
[03:21:36.880 --> 03:21:39.320]   how is, what are the values built
[03:21:39.320 --> 03:21:42.720]   into the way the tech is being built that are not obvious?
[03:21:42.720 --> 03:21:44.840]   - Right, so you always have to consider externalities.
[03:21:44.840 --> 03:21:45.680]   - Yes.
[03:21:45.680 --> 03:21:46.500]   - And this is no matter what.
[03:21:46.500 --> 03:21:47.340]   - And the externalities are not just physical
[03:21:47.340 --> 03:21:48.180]   to the environment,
[03:21:48.180 --> 03:21:49.760]   they're also to how the people are being conditioned
[03:21:49.760 --> 03:21:51.920]   and how the relationality between them is being conditioned.
[03:21:51.920 --> 03:21:53.120]   - The question I'm asking you,
[03:21:53.120 --> 03:21:56.240]   so I personally would rather be led by a plow
[03:21:56.240 --> 03:21:58.760]   and a tractor than Stalin, okay?
[03:21:58.760 --> 03:22:00.600]   That's the question I'm asking you.
[03:22:00.600 --> 03:22:06.740]   In creating an emergent government where people,
[03:22:06.740 --> 03:22:09.320]   where there's a democracy that's dynamic,
[03:22:09.320 --> 03:22:11.600]   that makes choices, that does governance
[03:22:11.600 --> 03:22:17.520]   at like a very kind of liquid,
[03:22:17.520 --> 03:22:21.640]   like there's a bunch of fine resolution layers
[03:22:21.640 --> 03:22:26.160]   of abstraction of governance happening at all scales,
[03:22:26.160 --> 03:22:28.080]   right, and doing so dynamically
[03:22:28.080 --> 03:22:30.720]   where no one person has power at any one time
[03:22:30.720 --> 03:22:34.200]   that can dominate and impose rule, okay?
[03:22:34.200 --> 03:22:35.840]   That's the Stalin version.
[03:22:35.840 --> 03:22:40.840]   I'm saying isn't the alternative that's emergent,
[03:22:40.840 --> 03:22:48.120]   empowered or made possible by the plow and the tractor,
[03:22:48.120 --> 03:22:50.920]   which is the modern version of that,
[03:22:50.920 --> 03:22:54.240]   is like the internet, the digital space
[03:22:54.240 --> 03:22:56.840]   where we can, the monetary system,
[03:22:56.840 --> 03:22:58.720]   where you have the cryptocurrency and so on,
[03:22:58.720 --> 03:23:01.280]   but you have much more importantly,
[03:23:01.280 --> 03:23:03.360]   to me at least, is just basic social interaction,
[03:23:03.360 --> 03:23:05.960]   the mechanisms of human transacting with each other
[03:23:05.960 --> 03:23:07.200]   in the space of ideas.
[03:23:07.200 --> 03:23:12.160]   So yes, it's not agnostic, definitely not agnostic.
[03:23:12.160 --> 03:23:14.280]   You've had a brilliant rant there.
[03:23:14.280 --> 03:23:16.200]   The tractor has effects,
[03:23:16.200 --> 03:23:19.440]   but isn't that the way we achieve an emergent system
[03:23:19.440 --> 03:23:21.200]   of governance?
[03:23:21.200 --> 03:23:23.240]   - Yes, but I wouldn't say we're on track.
[03:23:23.240 --> 03:23:29.080]   - You haven't seen anything promising.
[03:23:29.080 --> 03:23:30.520]   - It's not that I haven't seen anything promising.
[03:23:30.520 --> 03:23:32.840]   It's that to be on track requires understanding
[03:23:32.840 --> 03:23:34.240]   and guiding some of the things differently
[03:23:34.240 --> 03:23:36.480]   than is currently happening, and it's possible.
[03:23:36.480 --> 03:23:38.720]   That's actually what I really care about.
[03:23:38.720 --> 03:23:43.720]   So you couldn't have had a Stalin
[03:23:43.720 --> 03:23:46.480]   without having certain technologies emerge.
[03:23:46.480 --> 03:23:47.920]   He couldn't have ruled such a big area
[03:23:47.920 --> 03:23:51.000]   without transportation technologies, without the train,
[03:23:51.000 --> 03:23:55.120]   without the communication tech that made it possible.
[03:23:55.120 --> 03:23:58.280]   So when you say you'd rather have a tractor
[03:23:58.280 --> 03:23:59.480]   or a plow than a Stalin,
[03:23:59.480 --> 03:24:02.720]   there's a relationship between them that is more recursive,
[03:24:02.720 --> 03:24:07.720]   which is new physical technologies allow rulers
[03:24:07.720 --> 03:24:12.320]   to rule with more power over larger distances, historically.
[03:24:16.680 --> 03:24:19.440]   Some things are more responsible for that than others.
[03:24:19.440 --> 03:24:22.160]   Like Stalin also ate stuff for breakfast,
[03:24:22.160 --> 03:24:24.680]   but the thing he ate for breakfast is less responsible
[03:24:24.680 --> 03:24:28.180]   for the starvation of millions than the train.
[03:24:28.180 --> 03:24:30.160]   The train is more responsible for that.
[03:24:30.160 --> 03:24:32.400]   And then the weapons of war are more responsible.
[03:24:32.400 --> 03:24:36.160]   So some technology, let's not throw it all in the,
[03:24:36.160 --> 03:24:39.560]   you're saying technology has a responsibility here,
[03:24:39.560 --> 03:24:42.080]   but some is better than others.
[03:24:42.080 --> 03:24:45.000]   - I'm saying that people's use of technology
[03:24:45.000 --> 03:24:46.160]   will change their behavior.
[03:24:46.160 --> 03:24:48.880]   So it has behavioral dispositions built in.
[03:24:48.880 --> 03:24:51.560]   The change of the behavior will also change the values
[03:24:51.560 --> 03:24:52.520]   in the society.
[03:24:52.520 --> 03:24:53.520]   - It's very complicated, right?
[03:24:53.520 --> 03:24:55.520]   - It will also, as a result,
[03:24:55.520 --> 03:24:58.680]   both make people who have different kinds of predispositions
[03:24:58.680 --> 03:25:00.320]   with regard to rulership
[03:25:00.320 --> 03:25:03.260]   and different kinds of new capacities.
[03:25:03.260 --> 03:25:06.300]   And so we have to think about these things.
[03:25:06.300 --> 03:25:09.360]   It's kind of well understood that the printing press
[03:25:09.360 --> 03:25:13.120]   and then in early industrialism ended feudalism
[03:25:13.120 --> 03:25:15.720]   and created kind of nation states.
[03:25:15.720 --> 03:25:19.320]   So one thing I would say as a long trend
[03:25:19.320 --> 03:25:22.520]   that we can look at is that whenever there is a step
[03:25:22.520 --> 03:25:26.440]   function, a major leap in technology, physical technology,
[03:25:26.440 --> 03:25:28.320]   the underlying techno-industrial base
[03:25:28.320 --> 03:25:30.240]   with which we do stuff,
[03:25:30.240 --> 03:25:32.080]   it ends up coding for,
[03:25:32.080 --> 03:25:33.760]   it ends up predisposing a whole bunch
[03:25:33.760 --> 03:25:36.000]   of human behavioral patterns
[03:25:36.000 --> 03:25:37.800]   that the previous social system
[03:25:37.800 --> 03:25:40.320]   had not emerged to try to solve.
[03:25:40.320 --> 03:25:42.100]   And so it usually ends up breaking
[03:25:42.100 --> 03:25:43.520]   the previous social systems,
[03:25:43.520 --> 03:25:45.300]   the way the plow broke the tribal system,
[03:25:45.300 --> 03:25:46.640]   the way that the industrial revolution
[03:25:46.640 --> 03:25:48.080]   broke the feudal system.
[03:25:48.080 --> 03:25:50.320]   And then new social systems have to emerge
[03:25:50.320 --> 03:25:51.400]   that can deal with that,
[03:25:51.400 --> 03:25:54.040]   the new powers, the new dispositions,
[03:25:54.040 --> 03:25:55.360]   whatever with that tech.
[03:25:55.360 --> 03:25:58.120]   Obviously the nuke broke nation state governance
[03:25:58.120 --> 03:25:59.920]   being adequate and said,
[03:25:59.920 --> 03:26:00.880]   we can't ever have that again.
[03:26:00.880 --> 03:26:03.360]   So then it created this international
[03:26:03.360 --> 03:26:05.680]   governance apparatus world.
[03:26:05.680 --> 03:26:11.680]   So I guess what I'm saying is that
[03:26:12.680 --> 03:26:17.680]   the solution is not exponential tech
[03:26:17.680 --> 03:26:22.140]   following the current path
[03:26:22.140 --> 03:26:24.860]   of what the market incentivizes exponential tech to do,
[03:26:24.860 --> 03:26:27.040]   market being a previous social tech.
[03:26:27.040 --> 03:26:33.180]   I would say that exponential tech,
[03:26:33.180 --> 03:26:38.420]   if we look at different types of social tech,
[03:26:38.420 --> 03:26:40.100]   so let's just briefly look at
[03:26:41.360 --> 03:26:44.560]   that democracy tried to do the emergent order thing.
[03:26:44.560 --> 03:26:47.960]   At least that's the story.
[03:26:47.960 --> 03:26:53.060]   And this is why if you look,
[03:26:53.060 --> 03:26:57.300]   this important part to build first.
[03:26:57.300 --> 03:26:58.300]   - It's kind of doing it,
[03:26:58.300 --> 03:27:00.860]   it's just doing it poorly, you're saying.
[03:27:00.860 --> 03:27:03.320]   I mean, it is emergent order in some sense.
[03:27:03.320 --> 03:27:04.500]   I mean, that's the hope of democracy
[03:27:04.500 --> 03:27:06.220]   versus other forms of government.
[03:27:06.220 --> 03:27:07.060]   - Correct.
[03:27:07.060 --> 03:27:08.820]   I mean, I said at least the story
[03:27:08.820 --> 03:27:10.420]   because obviously it didn't do it
[03:27:10.420 --> 03:27:11.880]   for women and slaves early on,
[03:27:11.880 --> 03:27:14.420]   it doesn't do it for all classes equally, et cetera.
[03:27:14.420 --> 03:27:19.420]   But the idea of democracy is participatory governance.
[03:27:19.420 --> 03:27:22.720]   And so you notice that the modern democracies
[03:27:22.720 --> 03:27:26.220]   emerged out of the European enlightenment.
[03:27:26.220 --> 03:27:29.900]   And specifically, because the idea that a lot of people,
[03:27:29.900 --> 03:27:31.380]   some huge number, not a tribal number,
[03:27:31.380 --> 03:27:33.540]   huge number of anonymous people who don't know each other
[03:27:33.540 --> 03:27:35.660]   are not bonded to each other,
[03:27:35.660 --> 03:27:36.860]   who believe different things,
[03:27:36.860 --> 03:27:38.000]   who grew up in different ways
[03:27:38.000 --> 03:27:40.200]   can all work together to make collective decisions.
[03:27:40.200 --> 03:27:41.940]   Well, that affect everybody.
[03:27:41.940 --> 03:27:43.620]   And where some of them will make compromises
[03:27:43.620 --> 03:27:44.620]   and the thing that matters to them
[03:27:44.620 --> 03:27:46.440]   for what matters to other strangers.
[03:27:46.440 --> 03:27:47.820]   That's actually wild.
[03:27:47.820 --> 03:27:50.660]   Like it's a wild idea that that would even be possible.
[03:27:50.660 --> 03:27:52.220]   And it was kind of the result
[03:27:52.220 --> 03:27:54.460]   of this high enlightenment idea
[03:27:54.460 --> 03:27:58.860]   that we could all do the philosophy of science
[03:27:58.860 --> 03:28:02.860]   and we could all do the Hegelian dialectic.
[03:28:02.860 --> 03:28:04.300]   Those ideas had emerged, right?
[03:28:04.300 --> 03:28:08.940]   And it was that we could all,
[03:28:08.940 --> 03:28:10.160]   so our choice-making,
[03:28:10.160 --> 03:28:11.200]   'cause we've said a society
[03:28:11.200 --> 03:28:12.520]   is trying to coordinate choice-making.
[03:28:12.520 --> 03:28:14.960]   The emergent order is the order of the choices
[03:28:14.960 --> 03:28:15.840]   that we're making,
[03:28:15.840 --> 03:28:17.040]   not just at the level of the individuals,
[03:28:17.040 --> 03:28:18.280]   but what groups of individuals,
[03:28:18.280 --> 03:28:20.440]   corporations, nations, states, whatever do.
[03:28:20.440 --> 03:28:23.240]   Our choices are based on,
[03:28:23.240 --> 03:28:25.080]   our choice-making is based on our sense-making
[03:28:25.080 --> 03:28:26.400]   and our meaning-making.
[03:28:26.400 --> 03:28:27.720]   Our sense-making is what do we believe
[03:28:27.720 --> 03:28:29.460]   is happening in the world?
[03:28:29.460 --> 03:28:30.600]   And what do we believe the effects
[03:28:30.600 --> 03:28:31.600]   of a particular thing would be?
[03:28:31.600 --> 03:28:33.480]   Our meaning-making is what do we care about, right?
[03:28:33.480 --> 03:28:34.900]   Our values generation, what do we care about
[03:28:34.900 --> 03:28:37.200]   that we're trying to move the world in the direction of?
[03:28:37.200 --> 03:28:39.260]   If you ultimately are trying to move the world
[03:28:39.260 --> 03:28:41.620]   in a direction that is really, really different
[03:28:41.620 --> 03:28:42.760]   than the direction I'm trying to,
[03:28:42.760 --> 03:28:44.620]   we have very different values,
[03:28:44.620 --> 03:28:46.060]   we're gonna have a hard time.
[03:28:46.060 --> 03:28:48.180]   And if you think the world is a very different world,
[03:28:48.180 --> 03:28:51.220]   right, if you think that systemic racism
[03:28:51.220 --> 03:28:54.220]   is rampant everywhere and one of the worst problems,
[03:28:54.220 --> 03:28:55.860]   and I think it's not even a thing,
[03:28:55.860 --> 03:28:58.740]   if you think climate change is almost existential
[03:28:58.740 --> 03:29:00.220]   and I think it's not even a thing,
[03:29:00.220 --> 03:29:02.580]   we're gonna have a really hard time coordinating.
[03:29:02.580 --> 03:29:05.700]   And so we have to be able to have shared sense-making
[03:29:05.700 --> 03:29:07.280]   of can we come to understand
[03:29:07.280 --> 03:29:10.640]   just what is happening together?
[03:29:10.640 --> 03:29:12.600]   And then can we do shared values generation?
[03:29:12.600 --> 03:29:14.600]   Okay, maybe I'm emphasizing a particular value
[03:29:14.600 --> 03:29:16.300]   more than you, but I can see how,
[03:29:16.300 --> 03:29:17.440]   I can take your perspective
[03:29:17.440 --> 03:29:18.900]   and I can see how the thing that you value
[03:29:18.900 --> 03:29:20.000]   is worth valuing,
[03:29:20.000 --> 03:29:22.240]   and I can see how it's affected by this thing.
[03:29:22.240 --> 03:29:23.900]   So can we take all the values
[03:29:23.900 --> 03:29:25.360]   and try to come up with a proposition
[03:29:25.360 --> 03:29:27.280]   that benefits all of them better
[03:29:27.280 --> 03:29:28.480]   than the proposition I created
[03:29:28.480 --> 03:29:30.480]   just to benefit these ones that harms the ones
[03:29:30.480 --> 03:29:31.860]   that you care about,
[03:29:31.860 --> 03:29:34.320]   which is why you're opposing my proposition.
[03:29:34.320 --> 03:29:35.920]   We don't even try in the process
[03:29:35.920 --> 03:29:39.040]   of crafting a proposition currently to see,
[03:29:39.040 --> 03:29:41.060]   and this is the reason that the proposition
[03:29:41.060 --> 03:29:43.440]   when we vote on it gets half the votes almost all the time.
[03:29:43.440 --> 03:29:45.920]   It almost never gets 90% of the votes
[03:29:45.920 --> 03:29:48.180]   is because it benefits some things and harms other things.
[03:29:48.180 --> 03:29:49.760]   We can say all theory of trade-offs,
[03:29:49.760 --> 03:29:51.140]   but we didn't even try to say,
[03:29:51.140 --> 03:29:53.820]   could we see what everybody cares about
[03:29:53.820 --> 03:29:55.780]   and see if there was a better solution?
[03:29:55.780 --> 03:29:56.620]   So-
[03:29:56.620 --> 03:29:57.820]   - How do we fix that try?
[03:29:57.820 --> 03:29:59.880]   I wonder, is it as simple
[03:29:59.880 --> 03:30:02.120]   as the social technology education?
[03:30:02.120 --> 03:30:02.960]   - Well, no.
[03:30:04.280 --> 03:30:07.080]   The proposition crafting and refinement process
[03:30:07.080 --> 03:30:09.440]   has to be key to a democracy
[03:30:09.440 --> 03:30:11.840]   or to a government, and it's not currently.
[03:30:11.840 --> 03:30:15.780]   - But isn't that the humans creating that situation?
[03:30:15.780 --> 03:30:19.880]   So one way, there's two ways to fix that.
[03:30:19.880 --> 03:30:21.740]   One is to fix the individual humans,
[03:30:21.740 --> 03:30:23.760]   which is the education early in life.
[03:30:23.760 --> 03:30:26.480]   And the second is to create somehow systems that-
[03:30:26.480 --> 03:30:28.040]   - Yeah, it's both.
[03:30:28.040 --> 03:30:30.440]   - So I understand the education part,
[03:30:30.440 --> 03:30:31.400]   but creating systems,
[03:30:31.400 --> 03:30:34.040]   that's why I mentioned the technologies,
[03:30:34.040 --> 03:30:36.240]   is creating social networks, essentially.
[03:30:36.240 --> 03:30:37.880]   - Yes, that's actually necessary.
[03:30:37.880 --> 03:30:39.160]   Okay, so let's go to the first part
[03:30:39.160 --> 03:30:40.760]   and then we'll come to the second part.
[03:30:40.760 --> 03:30:45.440]   So democracy emerged as an enlightenment era idea
[03:30:45.440 --> 03:30:49.000]   that we could all do a dialectic
[03:30:49.000 --> 03:30:51.440]   and come to understand what other people valued.
[03:30:51.440 --> 03:30:54.160]   And so that we could actually come up
[03:30:54.160 --> 03:30:57.640]   with a cooperative solution rather than just,
[03:30:57.640 --> 03:31:00.360]   fuck you, we're gonna get our thing in war, right?
[03:31:00.360 --> 03:31:01.640]   And that we could sense make together.
[03:31:01.640 --> 03:31:03.280]   We could all apply the philosophy of science
[03:31:03.280 --> 03:31:05.160]   and you weren't gonna stick to your guns
[03:31:05.160 --> 03:31:06.840]   on what the speed of sound is if we measured it
[03:31:06.840 --> 03:31:07.800]   and we found out what it was.
[03:31:07.800 --> 03:31:09.680]   And there's a unifying element
[03:31:09.680 --> 03:31:11.880]   to the objectivity in that way.
[03:31:11.880 --> 03:31:15.600]   And so this is why I believe Jefferson said,
[03:31:15.600 --> 03:31:17.200]   if you could give me a perfect newspaper
[03:31:17.200 --> 03:31:18.080]   and a broken government,
[03:31:18.080 --> 03:31:19.280]   or I'm paraphrasing,
[03:31:19.280 --> 03:31:20.900]   or a broken government and perfect newspaper,
[03:31:20.900 --> 03:31:22.480]   I wouldn't hesitate to take the perfect newspaper.
[03:31:22.480 --> 03:31:24.380]   Because if the people understand what's going on,
[03:31:24.380 --> 03:31:25.960]   they can build a new government.
[03:31:25.960 --> 03:31:27.360]   If they don't understand what's going on,
[03:31:27.360 --> 03:31:29.600]   they can't possibly make good choices.
[03:31:29.600 --> 03:31:33.680]   And Washington, I'm paraphrasing again,
[03:31:33.680 --> 03:31:34.960]   first president said,
[03:31:34.960 --> 03:31:36.720]   the number one aim of the federal government
[03:31:36.720 --> 03:31:39.640]   should be the comprehensive education of every citizen
[03:31:39.640 --> 03:31:41.080]   in the science of government.
[03:31:41.080 --> 03:31:42.720]   Science of government was the term of art.
[03:31:42.720 --> 03:31:43.800]   Think about what that means, right?
[03:31:43.800 --> 03:31:47.760]   Science of government would be game theory,
[03:31:47.760 --> 03:31:49.240]   coordination theory, history,
[03:31:49.240 --> 03:31:51.240]   it wouldn't call it game theory yet.
[03:31:51.240 --> 03:31:53.600]   History, sociology, economics, right?
[03:31:53.600 --> 03:31:54.880]   All the things that lead to
[03:31:54.880 --> 03:31:57.360]   how we understand human coordination.
[03:31:57.360 --> 03:32:00.400]   I think it's so profound that he didn't say
[03:32:00.400 --> 03:32:02.120]   the number one aim of the federal government
[03:32:02.120 --> 03:32:03.020]   is rule of law.
[03:32:03.020 --> 03:32:07.200]   And he didn't say it's protecting the border from enemies.
[03:32:07.200 --> 03:32:08.720]   Because if the number one aim
[03:32:08.720 --> 03:32:11.000]   was to protect the border from enemies,
[03:32:11.000 --> 03:32:14.440]   it could do that as military dictatorship quite effectively.
[03:32:14.440 --> 03:32:16.360]   And if the goal was rule of law,
[03:32:16.360 --> 03:32:19.200]   it could do it as a dictatorship, as a police state.
[03:32:19.200 --> 03:32:23.520]   And so if the number one goal is anything other
[03:32:23.520 --> 03:32:25.640]   than the comprehensive education of all the citizens
[03:32:25.640 --> 03:32:26.480]   in the science of government,
[03:32:26.480 --> 03:32:28.240]   it won't stay democracy long.
[03:32:28.240 --> 03:32:31.960]   You can see, so both education and the fourth estate,
[03:32:31.960 --> 03:32:33.000]   the fourth estate being the,
[03:32:33.000 --> 03:32:34.840]   so education, can I make sense of the world?
[03:32:34.840 --> 03:32:36.080]   Am I trained to make sense of the world?
[03:32:36.080 --> 03:32:38.320]   The fourth estate is what's actually going on currently,
[03:32:38.320 --> 03:32:41.280]   the news, do I have good unbiased information about it?
[03:32:41.280 --> 03:32:43.920]   Those are both considered prerequisite institutions
[03:32:43.920 --> 03:32:46.520]   for democracy to even be a possibility.
[03:32:46.520 --> 03:32:49.340]   And then at the scale it was initially suggested here,
[03:32:49.340 --> 03:32:51.920]   the town hall was the key phenomena
[03:32:51.920 --> 03:32:54.440]   where there wasn't a special interest group
[03:32:54.440 --> 03:32:55.440]   crafted a proposition.
[03:32:55.440 --> 03:32:58.000]   The first thing I ever saw was the proposition,
[03:32:58.000 --> 03:32:58.840]   didn't know anything about it,
[03:32:58.840 --> 03:33:00.240]   and I got to vote yes or no.
[03:33:00.240 --> 03:33:02.160]   It was in the town hall, we all got to talk about it,
[03:33:02.160 --> 03:33:04.640]   and the proposition could get crafted in real time
[03:33:04.640 --> 03:33:05.940]   through the conversation,
[03:33:05.940 --> 03:33:08.320]   which is why there was that founding father statement
[03:33:08.320 --> 03:33:10.760]   that voting is the death of democracy.
[03:33:10.760 --> 03:33:13.240]   Voting fundamentally is polarizing the population
[03:33:13.240 --> 03:33:15.120]   in some kind of sublimated war,
[03:33:15.120 --> 03:33:17.400]   and we'll do that as the last step.
[03:33:17.400 --> 03:33:18.920]   But what we wanna do first is to say,
[03:33:18.920 --> 03:33:20.760]   how does the thing that you care about
[03:33:20.760 --> 03:33:22.600]   that seems damaged by this proposition,
[03:33:22.600 --> 03:33:24.560]   how could that turn into a solution
[03:33:24.560 --> 03:33:26.400]   to make this proposition better?
[03:33:26.400 --> 03:33:27.920]   Or this proposition still tends to the thing
[03:33:27.920 --> 03:33:29.560]   it's trying to tend to and tends to that better.
[03:33:29.560 --> 03:33:31.000]   Can we work on this together?
[03:33:31.000 --> 03:33:32.880]   And in a town hall, we could have that.
[03:33:32.880 --> 03:33:35.920]   As the scale increased, we lost the ability to do that.
[03:33:35.920 --> 03:33:37.880]   Now, as you mentioned, the internet could change that.
[03:33:37.880 --> 03:33:40.200]   The fact that we had representatives
[03:33:40.200 --> 03:33:41.720]   that had to ride a horse from one town hall
[03:33:41.720 --> 03:33:44.680]   to the other one to see what the colony would do,
[03:33:44.680 --> 03:33:48.120]   that we stopped having this kind of developmental,
[03:33:48.120 --> 03:33:51.960]   propositional development process when the town hall ended.
[03:33:51.960 --> 03:33:53.660]   The fact that we have not used the internet
[03:33:53.660 --> 03:33:58.480]   to recreate this is somewhere between insane
[03:33:58.480 --> 03:34:03.480]   and aligned with class interests.
[03:34:03.480 --> 03:34:06.920]   - I would push back to say that the internet
[03:34:06.920 --> 03:34:09.760]   has those things, it just has a lot of other things.
[03:34:09.760 --> 03:34:11.640]   I feel like the internet has places
[03:34:11.640 --> 03:34:16.040]   where that encourage synthesis of competing ideas
[03:34:16.040 --> 03:34:19.800]   and sense-making, which is what we're talking about.
[03:34:19.800 --> 03:34:21.720]   It's just that it's also flooded
[03:34:21.720 --> 03:34:23.400]   with a bunch of other systems
[03:34:23.400 --> 03:34:26.460]   that perhaps are out competing it under current incentives,
[03:34:26.460 --> 03:34:28.860]   perhaps has to do with capitalism and the market.
[03:34:28.860 --> 03:34:31.460]   - Sure, Linux is awesome, right?
[03:34:31.460 --> 03:34:34.680]   And Wikipedia and places where you have,
[03:34:34.680 --> 03:34:36.280]   and they have problems, but places where you have
[03:34:36.280 --> 03:34:39.060]   open source sharing of information,
[03:34:39.060 --> 03:34:41.620]   vetting of information towards collective building.
[03:34:41.620 --> 03:34:44.640]   Is that building something like,
[03:34:44.640 --> 03:34:47.440]   how much has that affected our court systems
[03:34:47.440 --> 03:34:50.600]   or our policing systems or our military systems or our--
[03:34:50.600 --> 03:34:53.000]   - First of all, I think a lot, but not enough.
[03:34:53.000 --> 03:34:56.200]   I think that's something I told you offline yesterday
[03:34:56.200 --> 03:34:59.920]   is perhaps it's a whole nother discussion,
[03:34:59.920 --> 03:35:02.960]   but I don't think we're quite quantifying
[03:35:02.960 --> 03:35:04.840]   the impact on the world,
[03:35:04.840 --> 03:35:06.640]   the positive impact of Wikipedia.
[03:35:06.640 --> 03:35:09.360]   You said the policing, I mean,
[03:35:09.360 --> 03:35:14.360]   I just think the amount of empathy that,
[03:35:14.360 --> 03:35:19.540]   like knowledge, I think can't help but lead to empathy.
[03:35:22.840 --> 03:35:26.360]   Just knowing, okay, just knowing, okay,
[03:35:26.360 --> 03:35:28.640]   I'll give you some pieces of information.
[03:35:28.640 --> 03:35:30.920]   Knowing how many people died in various wars,
[03:35:30.920 --> 03:35:32.680]   that already, that delta,
[03:35:32.680 --> 03:35:35.480]   when you have millions of people have that knowledge,
[03:35:35.480 --> 03:35:37.520]   it's like, it's a little like slap in the face,
[03:35:37.520 --> 03:35:42.360]   like, oh, like my boyfriend or girlfriend breaking up with me
[03:35:42.360 --> 03:35:43.960]   is not such a big deal
[03:35:43.960 --> 03:35:46.560]   when millions of people were tortured,
[03:35:46.560 --> 03:35:48.000]   you know, like just a little bit.
[03:35:48.000 --> 03:35:50.700]   And when a lot of people know that because of Wikipedia
[03:35:51.920 --> 03:35:55.280]   or the effect, there's second order effects of Wikipedia,
[03:35:55.280 --> 03:35:58.680]   which is it's not that necessarily people read Wikipedia,
[03:35:58.680 --> 03:36:03.680]   it's like YouTubers who don't really know stuff that well
[03:36:03.680 --> 03:36:07.480]   will thoroughly read a Wikipedia article
[03:36:07.480 --> 03:36:09.160]   and create a compelling video
[03:36:09.160 --> 03:36:10.600]   describing that Wikipedia article
[03:36:10.600 --> 03:36:12.880]   that then millions of people watch
[03:36:12.880 --> 03:36:15.520]   and they understand that, holy shit, a lot of,
[03:36:15.520 --> 03:36:16.840]   there was such, first of all,
[03:36:16.840 --> 03:36:19.600]   there was such a thing as World War II and World War I,
[03:36:19.600 --> 03:36:22.680]   okay, like they can at least like learn about it,
[03:36:22.680 --> 03:36:25.480]   they can learn about, this was like recent,
[03:36:25.480 --> 03:36:26.520]   they can learn about slavery,
[03:36:26.520 --> 03:36:30.080]   they can learn about all kinds of injustices in the world.
[03:36:30.080 --> 03:36:33.720]   And that I think has a lot of effects to our,
[03:36:33.720 --> 03:36:36.640]   to the way, whether you're a police officer,
[03:36:36.640 --> 03:36:40.320]   a lawyer, a judge, and the jury,
[03:36:40.320 --> 03:36:44.520]   or just the regular civilian citizen,
[03:36:44.520 --> 03:36:46.880]   the way you approach the,
[03:36:47.800 --> 03:36:50.080]   every other communication you engage in,
[03:36:50.080 --> 03:36:52.000]   even if the system of that communication
[03:36:52.000 --> 03:36:53.360]   is very much flawed.
[03:36:53.360 --> 03:36:55.880]   So I think there's a huge positive effect on Wikipedia.
[03:36:55.880 --> 03:36:57.160]   That's my case for Wikipedia.
[03:36:57.160 --> 03:36:59.520]   So you should donate to Wikipedia.
[03:36:59.520 --> 03:37:02.760]   I'm a huge fan, but there's very few systems like it,
[03:37:02.760 --> 03:37:04.720]   which is sad to me.
[03:37:04.720 --> 03:37:08.520]   - So I think it would be a useful exercise
[03:37:08.520 --> 03:37:11.960]   for any listener of the show
[03:37:11.960 --> 03:37:16.880]   to really try to run the dialectical synthesis process
[03:37:16.880 --> 03:37:20.640]   with regard to a topic like this,
[03:37:20.640 --> 03:37:25.640]   and take the techno concern perspective
[03:37:25.640 --> 03:37:29.320]   with regard to information tech
[03:37:29.320 --> 03:37:32.240]   that folks like Tristan Harris take,
[03:37:32.240 --> 03:37:35.320]   and say, what are all of the things that are getting worse?
[03:37:35.320 --> 03:37:38.680]   And what, and are any of them following an exponential curve
[03:37:38.680 --> 03:37:40.980]   and how much worse, how quickly could that be?
[03:37:40.980 --> 03:37:45.920]   And then, and do that fully without mitigating it.
[03:37:46.920 --> 03:37:48.680]   Then take the techno optimist perspective
[03:37:48.680 --> 03:37:50.320]   and see what things are getting better
[03:37:50.320 --> 03:37:54.800]   in a way that Kurzweil or Diamandis or someone might do,
[03:37:54.800 --> 03:37:57.560]   and try to take that perspective fully
[03:37:57.560 --> 03:37:59.040]   and say, are some of those things exponential?
[03:37:59.040 --> 03:38:00.280]   And what could that portend?
[03:38:00.280 --> 03:38:03.080]   And then try to hold all that at the same time.
[03:38:03.080 --> 03:38:07.400]   And I think there are ways in which,
[03:38:07.400 --> 03:38:10.520]   depending upon the metrics we're looking at,
[03:38:10.520 --> 03:38:13.160]   things are getting worse on exponential curves
[03:38:13.160 --> 03:38:14.760]   and better on exponential curves
[03:38:14.760 --> 03:38:16.800]   for different metrics at the same time,
[03:38:16.800 --> 03:38:20.760]   which I hold as the destabilization of previous system.
[03:38:20.760 --> 03:38:22.720]   And either an emergence to a better system
[03:38:22.720 --> 03:38:25.520]   or a collapse to a lower order are both possible.
[03:38:25.520 --> 03:38:31.880]   And so I want my optimism not to be about my assessment.
[03:38:31.880 --> 03:38:34.600]   I want my assessment to be just as fucking clear
[03:38:34.600 --> 03:38:35.440]   as it can be.
[03:38:35.440 --> 03:38:40.000]   I want my optimism to be what inspires the solution process
[03:38:40.000 --> 03:38:41.300]   on that clear assessment.
[03:38:42.200 --> 03:38:45.720]   So I never want to apply optimism in the sense making.
[03:38:45.720 --> 03:38:47.560]   I want to just try to be clear.
[03:38:47.560 --> 03:38:49.120]   If anything, I want to make sure
[03:38:49.120 --> 03:38:52.480]   that the challenges are really well understood.
[03:38:52.480 --> 03:38:54.680]   But that's in service of an optimism
[03:38:54.680 --> 03:38:57.480]   that there are good potentials,
[03:38:57.480 --> 03:39:00.600]   even if I don't know what they are, that are worth seeking.
[03:39:00.600 --> 03:39:03.800]   There's kind of a, there is some sense of optimism
[03:39:03.800 --> 03:39:04.760]   that's required to even try
[03:39:04.760 --> 03:39:06.460]   to innovate really hard problems.
[03:39:06.460 --> 03:39:09.400]   But then I want to take my pessimism
[03:39:09.400 --> 03:39:11.200]   and red team my own optimism
[03:39:11.200 --> 03:39:13.040]   to see is that solution not gonna work?
[03:39:13.040 --> 03:39:14.600]   Does it have second order effects?
[03:39:14.600 --> 03:39:16.940]   And then not get upset by that
[03:39:16.940 --> 03:39:19.600]   because I then come back to how to make it better.
[03:39:19.600 --> 03:39:22.280]   So just a relationship between optimism and pessimism
[03:39:22.280 --> 03:39:25.120]   and the dialectic of how they can work.
[03:39:25.120 --> 03:39:27.720]   So when I, of course, we can say
[03:39:27.720 --> 03:39:32.560]   that Wikipedia is a pretty awesome example of a thing.
[03:39:32.560 --> 03:39:34.960]   We can look at the places where it has limits
[03:39:34.960 --> 03:39:39.960]   or has failed where on a celebrity topic
[03:39:40.200 --> 03:39:42.000]   or corporate interest topic,
[03:39:42.000 --> 03:39:45.080]   you can pay Wikipedia editors to edit more frequently
[03:39:45.080 --> 03:39:46.920]   and various things like that.
[03:39:46.920 --> 03:39:49.160]   But you can also see where there's a lot of information
[03:39:49.160 --> 03:39:51.360]   that was kind of decentrally created,
[03:39:51.360 --> 03:39:52.360]   that is good information,
[03:39:52.360 --> 03:39:54.060]   that is more easily accessible to people
[03:39:54.060 --> 03:39:56.200]   than everybody buying their own Encyclopedia Britannica
[03:39:56.200 --> 03:39:57.800]   or walking down to the library
[03:39:57.800 --> 03:40:00.840]   and that can be updated in real time faster.
[03:40:00.840 --> 03:40:04.140]   And I think you're very right
[03:40:04.140 --> 03:40:08.080]   that the business model is a big difference
[03:40:08.080 --> 03:40:11.400]   because Wikipedia is not a for-profit corporation.
[03:40:11.400 --> 03:40:15.260]   It is a, it's tending to the information commons
[03:40:15.260 --> 03:40:16.960]   and it doesn't have an agenda
[03:40:16.960 --> 03:40:19.640]   other than tending to the information commons.
[03:40:19.640 --> 03:40:23.720]   And I think the two masters issue is a tricky one
[03:40:23.720 --> 03:40:24.960]   when I'm trying to optimize
[03:40:24.960 --> 03:40:26.920]   for very different kinds of things
[03:40:26.920 --> 03:40:31.360]   where I have to sacrifice one for the other
[03:40:31.360 --> 03:40:34.360]   and I can't find synergistic satisfiers, which one?
[03:40:34.360 --> 03:40:37.280]   And if I have a fiduciary responsibility to shareholder
[03:40:37.840 --> 03:40:41.780]   profit maximization and what does that end up creating?
[03:40:41.780 --> 03:40:46.800]   I think the ad model that Silicon Valley took,
[03:40:46.800 --> 03:40:49.840]   I think Jaron Lanier,
[03:40:49.840 --> 03:40:51.120]   I don't know if you've had him on the show,
[03:40:51.120 --> 03:40:52.720]   but he has interesting assessment
[03:40:52.720 --> 03:40:54.860]   of the nature of the ad model.
[03:40:54.860 --> 03:41:00.280]   Silicon Valley wanting to support capitalism
[03:41:00.280 --> 03:41:01.620]   and entrepreneurs to make things,
[03:41:01.620 --> 03:41:05.280]   but also the belief that information should be free
[03:41:05.280 --> 03:41:07.480]   and also the network dynamics
[03:41:07.480 --> 03:41:08.680]   where the more people you got on,
[03:41:08.680 --> 03:41:11.560]   you got increased value per user per capita
[03:41:11.560 --> 03:41:12.440]   as more people got on.
[03:41:12.440 --> 03:41:13.320]   So you didn't want to do anything
[03:41:13.320 --> 03:41:15.400]   to slow the rate of adoption.
[03:41:15.400 --> 03:41:18.600]   Some places actually, PayPal paying people money
[03:41:18.600 --> 03:41:21.760]   to join the network because the value of the network
[03:41:21.760 --> 03:41:23.840]   would be, there'd be a Metcalf-like dynamic
[03:41:23.840 --> 03:41:26.600]   proportional to the square of the total number of users.
[03:41:26.600 --> 03:41:31.480]   So the ad model made sense of how do we make it free,
[03:41:31.480 --> 03:41:34.120]   but also be a business, get everybody on,
[03:41:34.120 --> 03:41:37.600]   but not really thinking about what it would mean to,
[03:41:37.600 --> 03:41:38.840]   and this is now the whole idea
[03:41:38.840 --> 03:41:40.440]   that if you aren't paying for the product,
[03:41:40.440 --> 03:41:41.440]   you are the product.
[03:41:41.440 --> 03:41:45.940]   If they have a fiduciary responsibility
[03:41:45.940 --> 03:41:47.700]   to their shareholder to maximize profit,
[03:41:47.700 --> 03:41:50.260]   their customer is the advertiser,
[03:41:50.260 --> 03:41:52.500]   the user who it's being built for
[03:41:52.500 --> 03:41:56.400]   is to do behavioral mod for them for advertisers.
[03:41:56.400 --> 03:41:58.440]   That's a whole different thing
[03:41:58.440 --> 03:42:00.680]   than that same type of tech could have been
[03:42:00.680 --> 03:42:02.400]   if applied with a different business model
[03:42:02.400 --> 03:42:03.660]   or a different purpose.
[03:42:03.660 --> 03:42:10.800]   I think there's, because Facebook and Google
[03:42:10.800 --> 03:42:14.580]   and other information and communication platforms
[03:42:14.580 --> 03:42:17.180]   end up harvesting data about user behavior
[03:42:17.180 --> 03:42:19.520]   that allows them to model who the people are
[03:42:19.520 --> 03:42:20.920]   in a way that gives them more,
[03:42:20.920 --> 03:42:25.160]   sometimes specific information and behavioral information
[03:42:25.160 --> 03:42:29.920]   than even a therapist or a doctor or a lawyer
[03:42:29.920 --> 03:42:31.820]   or a priest might have in a different setting.
[03:42:31.820 --> 03:42:35.080]   They basically are accessing privileged information.
[03:42:35.080 --> 03:42:38.160]   There should be a fiduciary responsibility.
[03:42:38.160 --> 03:42:40.520]   And in normal fiduciary law,
[03:42:40.520 --> 03:42:42.200]   if there's this principal agent thing,
[03:42:42.200 --> 03:42:47.200]   if you are a principal and I'm an agent on your behalf,
[03:42:47.200 --> 03:42:49.600]   I don't have a game theoretic relationship with you.
[03:42:49.600 --> 03:42:50.840]   If you're sharing something with me
[03:42:50.840 --> 03:42:52.800]   and I'm the priest or I'm the therapist,
[03:42:52.800 --> 03:42:54.280]   I'm never gonna use that information
[03:42:54.280 --> 03:42:58.140]   to try to sell you a used car or whatever the thing is.
[03:42:58.140 --> 03:43:00.960]   But Facebook is gathering massive amounts
[03:43:00.960 --> 03:43:02.460]   of privileged information and using it
[03:43:02.460 --> 03:43:04.680]   to modify people's behavior for a behavior
[03:43:04.680 --> 03:43:07.160]   that they didn't sign up for wanting the behavior
[03:43:07.160 --> 03:43:08.660]   but what the corporation did.
[03:43:08.660 --> 03:43:12.720]   So I think this is an example of the physical tech
[03:43:12.720 --> 03:43:15.960]   evolving in the context of the previous social tech
[03:43:15.960 --> 03:43:18.480]   where it's being shaped in particular ways.
[03:43:18.480 --> 03:43:20.360]   And here, unlike Wikipedia that evolved
[03:43:20.360 --> 03:43:22.840]   for the information commons,
[03:43:22.840 --> 03:43:26.720]   this evolved for fulfilling particular agentic purpose.
[03:43:26.720 --> 03:43:28.120]   Most people, when they're on Facebook,
[03:43:28.120 --> 03:43:29.600]   think it's just a tool that they're using.
[03:43:29.600 --> 03:43:31.600]   They don't realize it's an agent.
[03:43:31.600 --> 03:43:33.920]   It is a corporation with a profit motive
[03:43:33.920 --> 03:43:36.640]   and as I'm interacting with it,
[03:43:36.640 --> 03:43:39.840]   it has a goal for me different than my goal for myself.
[03:43:39.840 --> 03:43:41.720]   And I might wanna be on for a short period of time.
[03:43:41.720 --> 03:43:43.600]   Its goal is maximize time on site.
[03:43:43.600 --> 03:43:46.840]   And so there is a rivalry that is take,
[03:43:46.840 --> 03:43:50.000]   but where there should be a fiduciary contract.
[03:43:50.000 --> 03:43:52.200]   I think that's actually a huge deal.
[03:43:52.200 --> 03:43:54.040]   And I think if we said,
[03:43:54.040 --> 03:43:56.760]   could we apply Facebook-like technology
[03:43:58.080 --> 03:44:03.080]   to develop people's citizenry capacity, right?
[03:44:03.080 --> 03:44:09.240]   To develop their personal health and wellbeing and habits,
[03:44:09.240 --> 03:44:13.320]   as well as their cognitive understanding,
[03:44:13.320 --> 03:44:15.880]   the complexity with which they can process
[03:44:15.880 --> 03:44:17.600]   the health of their relationships,
[03:44:17.600 --> 03:44:22.200]   that would be amazing to start to explore.
[03:44:22.200 --> 03:44:23.520]   And this is now the thesis
[03:44:23.520 --> 03:44:26.180]   that we started to discuss before is,
[03:44:27.140 --> 03:44:29.720]   every time there is a major step function
[03:44:29.720 --> 03:44:30.980]   in the physical tech,
[03:44:30.980 --> 03:44:33.880]   it obsoletes the previous social tech
[03:44:33.880 --> 03:44:35.780]   and the new social tech has to emerge.
[03:44:35.780 --> 03:44:38.200]   What I would say is that when we look
[03:44:38.200 --> 03:44:40.580]   at the nation state level of the world today,
[03:44:40.580 --> 03:44:44.000]   the more top-down authoritarian nation states
[03:44:44.000 --> 03:44:47.160]   are as the exponential tech started to emerge,
[03:44:47.160 --> 03:44:49.960]   the digital technology started to emerge.
[03:44:49.960 --> 03:44:53.420]   They were in a position for better long-term planning
[03:44:53.420 --> 03:44:55.300]   and better coordination.
[03:44:55.300 --> 03:44:57.520]   And so the authoritarian states started applying
[03:44:57.520 --> 03:44:59.240]   the exponential tech intentionally
[03:44:59.240 --> 03:45:01.680]   to make more effective authoritarian states.
[03:45:01.680 --> 03:45:03.960]   And that's everything from like an internet of things,
[03:45:03.960 --> 03:45:08.180]   surveillance system, going into machine learning systems,
[03:45:08.180 --> 03:45:11.600]   to the Sesame Credit system, to all those types of things.
[03:45:11.600 --> 03:45:14.080]   And so they're upgrading their social tech
[03:45:14.080 --> 03:45:16.200]   using the exponential tech.
[03:45:16.200 --> 03:45:19.100]   Otherwise, within a nation state like the US,
[03:45:19.100 --> 03:45:21.840]   but democratic open societies,
[03:45:22.800 --> 03:45:26.380]   the countries, the states are not directing the technology
[03:45:26.380 --> 03:45:28.180]   in a way that makes a better open society,
[03:45:28.180 --> 03:45:29.980]   meaning better emergent order.
[03:45:29.980 --> 03:45:32.500]   They're saying, well, the corporations are doing that
[03:45:32.500 --> 03:45:34.820]   and the state is doing the relatively little thing
[03:45:34.820 --> 03:45:36.880]   it would do aligned with the previous corporate law
[03:45:36.880 --> 03:45:38.060]   that no longer is relevant
[03:45:38.060 --> 03:45:39.820]   'cause there wasn't fiduciary responsibility
[03:45:39.820 --> 03:45:40.660]   for things like that.
[03:45:40.660 --> 03:45:42.180]   There wasn't antitrust
[03:45:42.180 --> 03:45:44.580]   because this creates functional monopolies
[03:45:44.580 --> 03:45:46.080]   because of network dynamics, right?
[03:45:46.080 --> 03:45:49.020]   Where YouTube has more users than Vimeo
[03:45:49.020 --> 03:45:50.580]   and every other video player together.
[03:45:50.580 --> 03:45:52.700]   Amazon has a bigger percentage of market share
[03:45:52.700 --> 03:45:54.560]   than all of the other markets together.
[03:45:54.560 --> 03:45:59.300]   You get one big dog per vertical because of network effect,
[03:45:59.300 --> 03:46:00.700]   which is a kind of organic monopoly
[03:46:00.700 --> 03:46:02.900]   that the previous antitrust law didn't even have a place.
[03:46:02.900 --> 03:46:04.420]   That wasn't the thing.
[03:46:04.420 --> 03:46:06.900]   Anti-monopoly was only something that emerged
[03:46:06.900 --> 03:46:08.780]   in the space of government contracts.
[03:46:08.780 --> 03:46:13.740]   So what we see is the new exponential technology
[03:46:13.740 --> 03:46:16.680]   is being directed by authoritarian nation states
[03:46:16.680 --> 03:46:18.200]   to make better authoritarian nation states
[03:46:18.200 --> 03:46:21.100]   and by corporations to make more powerful corporations.
[03:46:21.100 --> 03:46:23.020]   And powerful corporations,
[03:46:23.020 --> 03:46:24.940]   when we think about the Scottish enlightenment,
[03:46:24.940 --> 03:46:27.020]   when the idea of markets was being advanced,
[03:46:27.020 --> 03:46:28.980]   the modern kind of ideas of markets,
[03:46:28.980 --> 03:46:32.260]   the biggest corporation was tiny
[03:46:32.260 --> 03:46:35.020]   compared to what the biggest corporation today is.
[03:46:35.020 --> 03:46:37.880]   So the asymmetry of it relative to people was tiny.
[03:46:37.880 --> 03:46:41.840]   And the asymmetry now in terms of the total technology
[03:46:41.840 --> 03:46:43.620]   it employs, total amount of money,
[03:46:43.620 --> 03:46:45.700]   total amount of information processing
[03:46:45.700 --> 03:46:48.380]   is so many orders of magnitude.
[03:46:48.380 --> 03:46:53.380]   And rather than there be demand for an authentic thing
[03:46:53.380 --> 03:46:55.740]   that creates a basis for supply,
[03:46:55.740 --> 03:46:58.660]   as supply started to get way more coordinated and powerful
[03:46:58.660 --> 03:46:59.780]   and the demand wasn't coordinated
[03:46:59.780 --> 03:47:00.860]   'cause you don't have a labor union
[03:47:00.860 --> 03:47:02.860]   of all the customers working together,
[03:47:02.860 --> 03:47:04.900]   but you do have a coordination on the supply side,
[03:47:04.900 --> 03:47:06.240]   supply started to recognize
[03:47:06.240 --> 03:47:08.020]   that it could manufacture demand.
[03:47:08.020 --> 03:47:09.980]   It could make people want shit that they didn't want before
[03:47:09.980 --> 03:47:12.000]   that maybe wouldn't increase their happiness
[03:47:12.000 --> 03:47:14.140]   in a meaningful way, might increase addiction.
[03:47:14.140 --> 03:47:17.100]   Addiction is a very good way to manufacture demand.
[03:47:17.100 --> 03:47:21.400]   And so as soon as manufactured demand started
[03:47:21.400 --> 03:47:23.420]   through this is the cool thing
[03:47:23.420 --> 03:47:25.900]   and you have to have it for status or whatever it is,
[03:47:25.900 --> 03:47:28.720]   the intelligence of the market was breaking.
[03:47:28.720 --> 03:47:30.900]   Now it's no longer a collective intelligence system
[03:47:30.900 --> 03:47:32.940]   that is upregulating real desire
[03:47:32.940 --> 03:47:34.260]   for things that are really meaningful.
[03:47:34.260 --> 03:47:37.200]   You were able to hijack the lower angels of our nature
[03:47:37.200 --> 03:47:38.160]   rather than the higher ones,
[03:47:38.160 --> 03:47:40.260]   the addictive patterns drive those
[03:47:40.260 --> 03:47:41.340]   and have people want shit
[03:47:41.340 --> 03:47:42.540]   that doesn't actually make them happier,
[03:47:42.540 --> 03:47:44.140]   make the world better.
[03:47:44.140 --> 03:47:48.980]   And so we really also have to update our theory of markets
[03:47:48.980 --> 03:47:52.360]   because behavioral econ showed that homo economicus,
[03:47:52.360 --> 03:47:54.500]   the rational actor is not really a thing,
[03:47:54.500 --> 03:47:56.860]   but particularly at greater and greater scale
[03:47:56.860 --> 03:47:57.980]   can't really be a thing.
[03:47:57.980 --> 03:48:00.800]   Voluntarism isn't a thing where if my company
[03:48:00.800 --> 03:48:02.420]   doesn't wanna advertise on Facebook,
[03:48:02.420 --> 03:48:04.260]   I just will lose to the companies that do
[03:48:04.260 --> 03:48:06.420]   'cause that's where all the fucking attention is.
[03:48:06.420 --> 03:48:08.300]   And so then I can say it's voluntary,
[03:48:08.300 --> 03:48:10.940]   but it's not really if there's a functional monopoly.
[03:48:11.820 --> 03:48:14.660]   Same if I'm gonna sell on Amazon or things like that.
[03:48:14.660 --> 03:48:18.460]   So what I would say is these corporations
[03:48:18.460 --> 03:48:23.460]   are becoming more powerful than nation states in some ways.
[03:48:23.460 --> 03:48:28.460]   And they are also debasing the integrity
[03:48:28.460 --> 03:48:33.900]   of the nation states, the open societies.
[03:48:33.900 --> 03:48:35.900]   So the democracies are getting weaker
[03:48:35.900 --> 03:48:38.040]   as a result of exponential tech
[03:48:38.040 --> 03:48:40.460]   and the kind of new tech companies
[03:48:40.460 --> 03:48:42.860]   that are kind of a new feudalism, tech feudalism,
[03:48:42.860 --> 03:48:45.140]   'cause it's not a democracy inside of a tech company
[03:48:45.140 --> 03:48:46.940]   or the supply and demand relationship
[03:48:46.940 --> 03:48:49.480]   when you have manufactured demand
[03:48:49.480 --> 03:48:52.120]   and kind of monopoly type functions.
[03:48:52.120 --> 03:48:54.820]   And so we have basically a new feudalism
[03:48:54.820 --> 03:48:55.980]   controlling exponential tech
[03:48:55.980 --> 03:48:58.060]   and authoritarian nation states controlling it.
[03:48:58.060 --> 03:49:00.060]   And those attractors are both shitty.
[03:49:00.060 --> 03:49:05.340]   And so I'm interested in the application of exponential tech
[03:49:05.340 --> 03:49:07.300]   to making better social tech
[03:49:07.300 --> 03:49:09.140]   that makes emergent order possible.
[03:49:10.020 --> 03:49:12.580]   And where then that emergent order can bind
[03:49:12.580 --> 03:49:16.660]   and direct the exponential tech in fundamentally healthy,
[03:49:16.660 --> 03:49:19.100]   not X risk oriented directions.
[03:49:19.100 --> 03:49:21.220]   I think the relationship of social tech
[03:49:21.220 --> 03:49:22.700]   and physical tech can make it.
[03:49:22.700 --> 03:49:24.420]   I think we can actually use the physical tech
[03:49:24.420 --> 03:49:27.860]   to make better social tech, but it's not given that we do.
[03:49:27.860 --> 03:49:30.240]   If we don't make better social tech,
[03:49:30.240 --> 03:49:31.500]   then I think the physical tech
[03:49:31.500 --> 03:49:33.000]   empowers really shitty social tech
[03:49:33.000 --> 03:49:35.080]   that is not a world that we want.
[03:49:35.080 --> 03:49:37.820]   - I don't know if it's the road we wanna go down,
[03:49:37.820 --> 03:49:39.680]   but I tend to believe that the market
[03:49:39.680 --> 03:49:42.200]   will create exactly the thing you're talking about,
[03:49:42.200 --> 03:49:44.560]   which I feel like there's a lot of money to be made
[03:49:44.560 --> 03:49:49.560]   in creating a social tech that creates a better citizen,
[03:49:49.560 --> 03:49:56.500]   that creates a better human being.
[03:49:56.500 --> 03:50:02.800]   Your description of Facebook and so on,
[03:50:02.800 --> 03:50:05.600]   which is a system that creates addiction,
[03:50:05.600 --> 03:50:10.600]   which manufactures demand, is not obviously inherently
[03:50:10.600 --> 03:50:14.600]   the consequence of the markets.
[03:50:14.600 --> 03:50:17.400]   I feel like that's the first stage of us,
[03:50:17.400 --> 03:50:20.560]   like baby deer trying to figure out how to use the internet.
[03:50:20.560 --> 03:50:23.320]   I feel like there's much more money to be made
[03:50:23.320 --> 03:50:28.320]   with something that creates compersion and love, honestly.
[03:50:28.320 --> 03:50:33.960]   I mean, I really, we can have this,
[03:50:33.960 --> 03:50:35.360]   I can make the business case for it.
[03:50:35.360 --> 03:50:39.040]   I don't think we wanna really have that discussion,
[03:50:39.040 --> 03:50:41.440]   but do you have some hope that that's the case?
[03:50:41.440 --> 03:50:44.720]   And I guess if not, then how do we fix the system
[03:50:44.720 --> 03:50:46.000]   of markets that work so well
[03:50:46.000 --> 03:50:48.960]   for the United States for so long?
[03:50:48.960 --> 03:50:51.600]   - Like I said, every social tech worked for a while.
[03:50:51.600 --> 03:50:55.480]   Tribalism worked well for 200,000 or 300,000 years.
[03:50:55.480 --> 03:50:57.960]   I think social tech has to keep evolving.
[03:50:57.960 --> 03:51:00.840]   The social technologies with which we organize
[03:51:00.840 --> 03:51:03.440]   and coordinate our behavior have to keep evolving
[03:51:03.440 --> 03:51:05.720]   as our physical tech does.
[03:51:05.720 --> 03:51:09.320]   So I think the thing that we call markets,
[03:51:09.320 --> 03:51:12.000]   of course we can try to say,
[03:51:12.000 --> 03:51:14.480]   oh, even biology runs on markets,
[03:51:14.480 --> 03:51:18.080]   but the thing that we call markets,
[03:51:18.080 --> 03:51:20.600]   the underlying theory, homo economicus,
[03:51:20.600 --> 03:51:23.560]   demand, driving supply, that thing broke.
[03:51:23.560 --> 03:51:28.160]   It broke with scale in particular and a few other things.
[03:51:28.160 --> 03:51:30.720]   So it needs updated in a really fundamental way.
[03:51:32.600 --> 03:51:34.200]   I think there's something even deeper
[03:51:34.200 --> 03:51:35.560]   than making money happening
[03:51:35.560 --> 03:51:39.080]   that in some ways will obsolete money-making.
[03:51:39.080 --> 03:51:44.560]   I think capitalism is not about business.
[03:51:44.560 --> 03:51:48.280]   So if you think about business,
[03:51:48.280 --> 03:51:50.880]   I'm gonna produce a good or a service that people want
[03:51:50.880 --> 03:51:53.080]   and bring it to the market
[03:51:53.080 --> 03:51:55.160]   so that people get access to that good or service.
[03:51:55.160 --> 03:51:58.280]   That's the world of business, but that's not capitalism.
[03:51:58.280 --> 03:52:02.160]   Capitalism is the management and allocation of capital.
[03:52:03.000 --> 03:52:06.560]   Which financial services was a tiny percentage
[03:52:06.560 --> 03:52:08.360]   of the total market has become a huge percentage
[03:52:08.360 --> 03:52:09.200]   of the total market.
[03:52:09.200 --> 03:52:10.360]   It's a different creature.
[03:52:10.360 --> 03:52:12.400]   So if I was in business
[03:52:12.400 --> 03:52:13.920]   and I was producing a good or service
[03:52:13.920 --> 03:52:15.080]   and I was saving up enough money
[03:52:15.080 --> 03:52:17.120]   that I started to be able to invest that money
[03:52:17.120 --> 03:52:19.840]   and gain interest or do things like that,
[03:52:19.840 --> 03:52:23.600]   I start realizing I'm making more money on my money
[03:52:23.600 --> 03:52:26.120]   than I'm making on producing the goods and services.
[03:52:26.120 --> 03:52:28.240]   So I stop even paying attention to goods and services
[03:52:28.240 --> 03:52:30.880]   and start paying attention to making money on money.
[03:52:30.880 --> 03:52:34.040]   And how do I utilize capital to create more capital?
[03:52:34.040 --> 03:52:36.280]   And capital gives me more optionality
[03:52:36.280 --> 03:52:37.400]   'cause I can buy anything with it
[03:52:37.400 --> 03:52:40.200]   than a particular good or service that only some people want.
[03:52:40.200 --> 03:52:47.240]   Capitalism, more capital ended up meaning more control.
[03:52:47.240 --> 03:52:51.680]   I could put more people under my employment.
[03:52:51.680 --> 03:52:54.800]   I could buy larger pieces of land,
[03:52:54.800 --> 03:52:56.640]   novel access to resource, mines,
[03:52:56.640 --> 03:52:58.200]   and put more technology under my employment.
[03:52:58.200 --> 03:53:00.960]   So it meant increased agency and also increased control.
[03:53:00.960 --> 03:53:05.300]   I think attentionalism is even more powerful.
[03:53:05.300 --> 03:53:10.680]   So rather than enslave people
[03:53:10.680 --> 03:53:14.000]   where the people kind of always want to get away
[03:53:14.000 --> 03:53:16.320]   and put in the least work they can,
[03:53:16.320 --> 03:53:18.160]   there's a way in which economic servitude
[03:53:18.160 --> 03:53:21.640]   was just more profitable than slavery, right?
[03:53:21.640 --> 03:53:24.000]   Have the people work even harder voluntarily
[03:53:24.000 --> 03:53:25.640]   'cause they wanna get ahead
[03:53:25.640 --> 03:53:27.960]   and nobody has to be there to whip them
[03:53:27.960 --> 03:53:29.360]   or control them or whatever.
[03:53:29.360 --> 03:53:34.380]   This is a cynical take, but a meaningful take.
[03:53:34.380 --> 03:53:40.680]   So capital ends up being a way
[03:53:40.680 --> 03:53:43.160]   to influence human behavior, right?
[03:53:43.160 --> 03:53:48.160]   And yet where people still feel free in some meaningful way,
[03:53:48.160 --> 03:53:51.760]   they're not feeling like they're gonna be punished
[03:53:51.760 --> 03:53:53.480]   by the state if they don't do something.
[03:53:53.480 --> 03:53:54.800]   It's like punished by the market
[03:53:54.800 --> 03:53:56.600]   via homelessness or something.
[03:53:56.600 --> 03:53:58.160]   But the market is this invisible thing
[03:53:58.160 --> 03:54:01.120]   I can't put an agent on, so it feels like free.
[03:54:01.120 --> 03:54:06.120]   And so if you want to affect people's behavior
[03:54:06.120 --> 03:54:10.280]   and still have them feel free,
[03:54:10.280 --> 03:54:12.480]   capital ends up being a way to do that.
[03:54:12.480 --> 03:54:15.880]   But I think affecting their attention is even deeper
[03:54:15.880 --> 03:54:18.500]   'cause if I can affect their attention,
[03:54:18.500 --> 03:54:20.880]   I can both affect what they want
[03:54:20.880 --> 03:54:22.880]   and what they believe and what they feel.
[03:54:22.880 --> 03:54:24.600]   And we statistically know this very clearly.
[03:54:24.600 --> 03:54:27.320]   Facebook has done studies that based on changing the feed,
[03:54:27.320 --> 03:54:31.160]   it can change beliefs, emotional dispositions, et cetera.
[03:54:31.160 --> 03:54:35.920]   And so I think there's a way that the harvest
[03:54:35.920 --> 03:54:38.960]   and directing of attention is even a more powerful system
[03:54:38.960 --> 03:54:39.800]   than capitalism.
[03:54:39.800 --> 03:54:42.780]   It is effective in capitalism to generate capital,
[03:54:42.780 --> 03:54:44.680]   but I think it also generates influence
[03:54:44.680 --> 03:54:46.640]   beyond what capital can do.
[03:54:46.640 --> 03:54:51.640]   And so do we want to have some groups
[03:54:52.800 --> 03:54:55.960]   utilizing that type of tech
[03:54:55.960 --> 03:54:57.680]   to direct other people's attention?
[03:54:57.680 --> 03:55:02.680]   If so, towards what?
[03:55:02.680 --> 03:55:05.520]   Towards what metrics of what a good civilization
[03:55:05.520 --> 03:55:07.160]   and good human life would be?
[03:55:07.160 --> 03:55:08.800]   What's the oversight process?
[03:55:08.800 --> 03:55:09.960]   What is the-
[03:55:09.960 --> 03:55:10.800]   - Transparency.
[03:55:10.800 --> 03:55:13.480]   I can answer all the things you're mentioning.
[03:55:13.480 --> 03:55:19.520]   I guarantee you, if I'm not such a lazy ass,
[03:55:19.520 --> 03:55:21.840]   I'll be part of the many people doing this,
[03:55:21.840 --> 03:55:23.880]   is transparency and control,
[03:55:23.880 --> 03:55:26.240]   like giving control to individual people.
[03:55:26.240 --> 03:55:31.240]   - Okay, so maybe the corporation has coordination
[03:55:31.240 --> 03:55:36.120]   on its goals that all of its customers
[03:55:36.120 --> 03:55:37.600]   or users together don't have.
[03:55:37.600 --> 03:55:42.600]   So there's some asymmetry of its goals,
[03:55:42.600 --> 03:55:45.840]   but maybe I could actually help all of the customers
[03:55:45.840 --> 03:55:49.000]   to coordinate almost like a labor union or whatever
[03:55:49.000 --> 03:55:51.840]   by informing and educating them adequately
[03:55:51.840 --> 03:55:54.840]   about the effects, the externalities on them.
[03:55:54.840 --> 03:55:57.320]   This is not toxic waste going into the ocean
[03:55:57.320 --> 03:55:59.160]   or the atmosphere, it's their minds,
[03:55:59.160 --> 03:56:02.280]   their beings, their families, their relationships,
[03:56:02.280 --> 03:56:05.560]   such that they will in group change their behavior.
[03:56:05.560 --> 03:56:08.760]   And I think the,
[03:56:08.760 --> 03:56:13.040]   one way of saying what you're saying, I think,
[03:56:13.040 --> 03:56:17.120]   is that you think that you can rescue homo economicus
[03:56:18.040 --> 03:56:21.080]   from the rational actor
[03:56:21.080 --> 03:56:22.720]   that will pursue all the goods and services
[03:56:22.720 --> 03:56:24.600]   and choose the best one at the best price,
[03:56:24.600 --> 03:56:26.440]   the kind of Rand, von Mises Hayek,
[03:56:26.440 --> 03:56:28.520]   that you can rescue that from Dan Ariely
[03:56:28.520 --> 03:56:29.840]   and behavioral econ that says
[03:56:29.840 --> 03:56:31.400]   that's actually not how people make choices,
[03:56:31.400 --> 03:56:32.960]   they make it based on status hacking,
[03:56:32.960 --> 03:56:36.080]   largely whether it's good for them or not in the long term.
[03:56:36.080 --> 03:56:40.320]   And the large asymmetric corporation can run propaganda
[03:56:40.320 --> 03:56:42.600]   and narrative warfare that hits people's status buttons
[03:56:42.600 --> 03:56:45.640]   and their limbic hijacks and their lots of other things
[03:56:45.640 --> 03:56:47.880]   in ways that they can't even perceive
[03:56:47.880 --> 03:56:48.880]   that are happening.
[03:56:48.880 --> 03:56:51.480]   They're not paying attention to that,
[03:56:51.480 --> 03:56:53.240]   the site is employing psychologists
[03:56:53.240 --> 03:56:55.140]   and split testing and whatever else.
[03:56:55.140 --> 03:56:59.800]   So you're saying, I think we can recover homo economicus.
[03:56:59.800 --> 03:57:02.400]   - And not just through a single mechanism technology,
[03:57:02.400 --> 03:57:05.640]   there's the, not to keep mentioning the guy,
[03:57:05.640 --> 03:57:08.760]   but platforms like Joe Rogan and so on
[03:57:08.760 --> 03:57:12.800]   that help make viral the ways
[03:57:14.960 --> 03:57:17.880]   the education of negative externalities
[03:57:17.880 --> 03:57:20.840]   can become viral in this world.
[03:57:20.840 --> 03:57:25.240]   - So interestingly, I actually agree with you
[03:57:25.240 --> 03:57:27.120]   that--
[03:57:27.120 --> 03:57:30.560]   - Got 'em, four and a half hours in.
[03:57:30.560 --> 03:57:32.280]   - That we can--
[03:57:32.280 --> 03:57:33.480]   - Tech can do some good.
[03:57:33.480 --> 03:57:35.800]   - Well, see what you're talking about
[03:57:35.800 --> 03:57:38.640]   is the application of tech here, broadcast tech,
[03:57:38.640 --> 03:57:40.280]   where you can speak to a lot of people
[03:57:40.280 --> 03:57:42.280]   and that's not gonna be strong enough
[03:57:42.280 --> 03:57:44.160]   'cause the different people need spoken to differently,
[03:57:44.160 --> 03:57:45.600]   which means it has to be different voices
[03:57:45.600 --> 03:57:47.080]   that get amplified to those audiences,
[03:57:47.080 --> 03:57:48.160]   more like Facebook's tech,
[03:57:48.160 --> 03:57:50.520]   but nonetheless, we'll start with broadcast tech.
[03:57:50.520 --> 03:57:52.680]   - Plants the first seed and then the word of mouth
[03:57:52.680 --> 03:57:53.880]   is a powerful thing.
[03:57:53.880 --> 03:57:56.200]   You need to do the first broadcast shotgun
[03:57:56.200 --> 03:57:59.480]   and then it like lands, a catapult or whatever,
[03:57:59.480 --> 03:58:01.760]   I don't know what the right weapon is,
[03:58:01.760 --> 03:58:03.480]   but then it just spreads the word of mouth
[03:58:03.480 --> 03:58:06.240]   through all kinds of tech, including Facebook.
[03:58:06.240 --> 03:58:08.160]   - So let's come back to the fundamental thing.
[03:58:08.160 --> 03:58:11.280]   The fundamental thing is we want a kind of order
[03:58:11.280 --> 03:58:15.680]   at various scales from the conflicting parts of ourself,
[03:58:15.680 --> 03:58:19.880]   actually having more harmony than they might have
[03:58:19.880 --> 03:58:24.880]   to family, extended family, local, all the way up to global.
[03:58:24.880 --> 03:58:28.600]   We want emergent order where our choices
[03:58:28.600 --> 03:58:32.600]   have more alignment, right?
[03:58:32.600 --> 03:58:36.360]   We want that to be emergent rather than imposed
[03:58:36.360 --> 03:58:38.640]   or rather than we want fundamentally different things
[03:58:38.640 --> 03:58:40.240]   or make totally different sense of the world
[03:58:40.240 --> 03:58:43.640]   where warfare of some kind becomes the only solution.
[03:58:43.640 --> 03:58:47.480]   Emergent order requires us in our choice-making,
[03:58:47.480 --> 03:58:51.440]   requires us being able to have related sense-making
[03:58:51.440 --> 03:58:53.480]   and related meaning-making processes.
[03:58:53.480 --> 03:58:59.720]   Can we apply digital technologies and exponential tech
[03:58:59.720 --> 03:59:03.120]   in general to try to increase the capacity to do that?
[03:59:03.120 --> 03:59:04.920]   Where the technology called a town hall,
[03:59:04.920 --> 03:59:06.720]   the social tech that we'd all get together and talk,
[03:59:06.720 --> 03:59:08.600]   obviously is very scale limited
[03:59:08.600 --> 03:59:10.880]   and it's also oriented to geography
[03:59:10.880 --> 03:59:13.080]   rather than networks of aligned interest.
[03:59:13.080 --> 03:59:16.240]   Can we build new, better versions of those types of things?
[03:59:16.240 --> 03:59:20.280]   And going back to the idea that a democracy
[03:59:20.280 --> 03:59:22.720]   or participatory governance depends upon
[03:59:22.720 --> 03:59:24.960]   comprehensive education in the science of government,
[03:59:24.960 --> 03:59:27.000]   which include being able to understand things
[03:59:27.000 --> 03:59:28.640]   like asymmetric information warfare
[03:59:28.640 --> 03:59:30.000]   on the side of governments
[03:59:30.000 --> 03:59:32.200]   and how the people can organize adequately.
[03:59:32.200 --> 03:59:35.880]   Can you utilize some of the technologies now
[03:59:35.880 --> 03:59:40.120]   to be able to support increased comprehensive education
[03:59:40.120 --> 03:59:43.240]   of the people and maybe comprehensive informant-ness?
[03:59:43.240 --> 03:59:45.880]   So both fixing the decay in both education
[03:59:45.880 --> 03:59:47.360]   and the fourth estate that have happened
[03:59:47.360 --> 03:59:49.240]   so that people can start self-organizing
[03:59:49.240 --> 03:59:52.760]   to then influence the corporations,
[03:59:52.760 --> 03:59:55.280]   the nation states to do different things
[03:59:55.280 --> 03:59:57.400]   and/or build new ones themselves.
[03:59:57.400 --> 04:00:00.720]   Yeah, fundamentally, that's the thing that has to happen.
[04:00:00.720 --> 04:00:03.480]   The exponential tech gives us a novel problem landscape
[04:00:03.480 --> 04:00:04.640]   that the world never had.
[04:00:04.640 --> 04:00:07.000]   The nuke gave us a novel problem landscape.
[04:00:07.000 --> 04:00:10.480]   And so that required this whole Bretton Woods world.
[04:00:10.480 --> 04:00:13.480]   The exponential tech gives us novel problem landscape.
[04:00:13.480 --> 04:00:15.440]   Our existing problem-solving processes
[04:00:15.440 --> 04:00:16.520]   aren't doing a good job.
[04:00:16.520 --> 04:00:18.680]   We have had more countries get nukes.
[04:00:18.680 --> 04:00:20.360]   We haven't had nuclear deproliferation.
[04:00:20.360 --> 04:00:22.200]   We haven't achieved any of the UN
[04:00:22.200 --> 04:00:24.600]   sustainable development goals.
[04:00:24.600 --> 04:00:26.640]   We haven't kept any of the new categories of tech
[04:00:26.640 --> 04:00:27.720]   from making arms races.
[04:00:27.720 --> 04:00:29.880]   So our global coordination is not adequate
[04:00:29.880 --> 04:00:32.200]   to the problem landscape.
[04:00:32.200 --> 04:00:35.120]   So we need fundamentally better problem-solving processes,
[04:00:35.120 --> 04:00:37.240]   a market or a state as a problem-solving process.
[04:00:37.240 --> 04:00:39.400]   We need better ones that can do the speed
[04:00:39.400 --> 04:00:41.440]   and scale of the current issues.
[04:00:41.440 --> 04:00:43.360]   Right now, speed is one of the other big things,
[04:00:43.360 --> 04:00:46.920]   is that by the time we regulated DDT out of existence
[04:00:46.920 --> 04:00:49.080]   or cigarettes not for people under 18,
[04:00:49.080 --> 04:00:50.800]   they'd already killed so many people
[04:00:50.800 --> 04:00:52.720]   and we let the market do the thing.
[04:00:52.720 --> 04:00:56.280]   But as Elon has made the point, that won't work for AI.
[04:00:56.280 --> 04:00:59.460]   By the time we recognize afterwards
[04:00:59.460 --> 04:01:01.440]   that we have an autopoetic AI that's a problem,
[04:01:01.440 --> 04:01:02.600]   you won't be able to reverse it,
[04:01:02.600 --> 04:01:04.720]   that there's a number of things that
[04:01:04.720 --> 04:01:05.680]   when you're dealing with tech
[04:01:05.680 --> 04:01:07.840]   that is either self-replicating
[04:01:07.840 --> 04:01:09.760]   and disintermediates humans to keep going,
[04:01:09.760 --> 04:01:11.560]   doesn't need humans to keep going,
[04:01:11.560 --> 04:01:15.600]   or you have tech that just has exponentially fast effects,
[04:01:15.600 --> 04:01:17.680]   your regulation has to come early.
[04:01:17.680 --> 04:01:21.640]   It can't come after the effects have happened,
[04:01:21.640 --> 04:01:23.120]   the negative effects have happened
[04:01:23.120 --> 04:01:25.400]   'cause the negative effects could be too big too quickly.
[04:01:25.400 --> 04:01:27.800]   So we basically need new problem-solving processes
[04:01:27.800 --> 04:01:32.800]   that do better at being able to internalize externality,
[04:01:32.800 --> 04:01:34.800]   solve the problems on the right time scale
[04:01:34.800 --> 04:01:37.680]   and the right geographic scale.
[04:01:37.680 --> 04:01:40.480]   And those new processes to not be imposed
[04:01:40.480 --> 04:01:42.520]   have to emerge from people wanting them
[04:01:42.520 --> 04:01:46.440]   and being able to participate in their development,
[04:01:46.440 --> 04:01:47.720]   which is what I would call kind of
[04:01:47.720 --> 04:01:49.640]   a new cultural enlightenment or renaissance
[04:01:49.640 --> 04:01:50.920]   that has to happen,
[04:01:50.920 --> 04:01:54.400]   where people start understanding the new power
[04:01:54.400 --> 04:01:56.600]   that exponential tech offers,
[04:01:56.600 --> 04:01:59.720]   the way that it is actually damaging
[04:01:59.720 --> 04:02:02.880]   current governance structures that we care about
[04:02:02.880 --> 04:02:04.760]   and creating an ex-risk landscape,
[04:02:04.760 --> 04:02:09.760]   but could also be redirected towards more pro-topic purposes
[04:02:09.760 --> 04:02:11.080]   and then saying,
[04:02:11.080 --> 04:02:13.080]   how do we rebuild new social institutions?
[04:02:13.080 --> 04:02:14.840]   What are adequate social institutions
[04:02:14.840 --> 04:02:18.780]   where we can do participatory governance at scale and time?
[04:02:18.780 --> 04:02:21.660]   And how can the people actually participate
[04:02:21.660 --> 04:02:22.880]   to build those things?
[04:02:22.880 --> 04:02:25.680]   The solution that I see working
[04:02:25.680 --> 04:02:27.140]   requires a process like that.
[04:02:27.140 --> 04:02:32.280]   - And the result maximizes love.
[04:02:32.280 --> 04:02:36.120]   So again, Elon, you'd be right that love is the answer.
[04:02:36.120 --> 04:02:39.280]   Let me take you back from the scale of societies
[04:02:39.280 --> 04:02:42.640]   to the scale that's far, far more important,
[04:02:42.640 --> 04:02:47.300]   which is the scale of family.
[04:02:47.300 --> 04:02:50.080]   You've written a blog post about your dad.
[04:02:50.080 --> 04:02:55.080]   We have various flavors of relationships with our fathers.
[04:02:55.920 --> 04:02:59.600]   What have you learned about life from your dad?
[04:02:59.600 --> 04:03:04.040]   - Well, people can read the blog post
[04:03:04.040 --> 04:03:06.680]   and see a lot of individual things that I learned
[04:03:06.680 --> 04:03:08.800]   that I really appreciated.
[04:03:08.800 --> 04:03:11.200]   If I was to kind of summarize at a high level,
[04:03:11.200 --> 04:03:16.360]   I had a really incredible dad,
[04:03:16.360 --> 04:03:23.040]   very, very unusually positive set of experiences.
[04:03:23.160 --> 04:03:25.120]   He was committed, we were homeschooled,
[04:03:25.120 --> 04:03:27.640]   and he was committed to work from home to be available
[04:03:27.640 --> 04:03:30.340]   and prioritize fathering in a really deep way.
[04:03:30.340 --> 04:03:40.240]   And as a super gifted, super loving, very unique man,
[04:03:40.240 --> 04:03:41.820]   he also had his unique issues
[04:03:41.820 --> 04:03:43.800]   that were part of what crafted the unique brilliance
[04:03:43.800 --> 04:03:45.680]   and those things often go together.
[04:03:45.680 --> 04:03:50.340]   And I say that because I think I had some unusual gifts
[04:03:50.340 --> 04:03:52.100]   and also some unusual difficulties.
[04:03:52.100 --> 04:03:55.520]   And I think it's useful for everybody to know
[04:03:55.520 --> 04:03:57.420]   their path probably has both of those.
[04:03:57.420 --> 04:04:03.640]   But if I was to say kind of at the essence
[04:04:03.640 --> 04:04:05.720]   of one of the things my dad taught me
[04:04:05.720 --> 04:04:07.360]   across a lot of lessons was like
[04:04:07.360 --> 04:04:11.560]   the intersection of self-empowerment,
[04:04:11.560 --> 04:04:14.840]   ideas and practices that self-empower
[04:04:14.840 --> 04:04:17.960]   towards collective good,
[04:04:17.960 --> 04:04:20.800]   towards some virtuous purpose beyond the self.
[04:04:20.800 --> 04:04:23.960]   And he both said that a million different ways,
[04:04:23.960 --> 04:04:25.560]   taught it in a million different ways
[04:04:25.560 --> 04:04:26.920]   when we were doing construction
[04:04:26.920 --> 04:04:30.160]   and he was teaching me how to build a house.
[04:04:30.160 --> 04:04:32.900]   We were putting the wires to the walls
[04:04:32.900 --> 04:04:34.060]   before the drywall went on.
[04:04:34.060 --> 04:04:36.440]   He made sure that the way that we put the wires
[04:04:36.440 --> 04:04:37.600]   through was beautiful.
[04:04:37.600 --> 04:04:42.600]   Like that the height of the holes was similar,
[04:04:42.600 --> 04:04:44.880]   that we twisted the wires in a particular way.
[04:04:44.880 --> 04:04:46.920]   And it's like, no one's ever gonna see it.
[04:04:46.920 --> 04:04:48.720]   And he's like, if a job's worth doing,
[04:04:48.720 --> 04:04:50.920]   it's worth doing well and excellence is its own reward
[04:04:50.920 --> 04:04:52.040]   and those types of ideas.
[04:04:52.040 --> 04:04:53.600]   And if there was a really shitty job to do,
[04:04:53.600 --> 04:04:55.720]   he'd say, see the job, do the job, stay out of the misery.
[04:04:55.720 --> 04:04:57.160]   Just don't indulge any negativity,
[04:04:57.160 --> 04:04:59.040]   do the things that need done.
[04:04:59.040 --> 04:05:01.160]   And so there's like a,
[04:05:01.160 --> 04:05:04.180]   there's an empowerment and a nobility together.
[04:05:04.180 --> 04:05:10.560]   And yeah, extraordinarily fortunate.
[04:05:10.560 --> 04:05:13.400]   - Is there ways you think you could have been a better son?
[04:05:13.400 --> 04:05:16.320]   Is there things you regret?
[04:05:17.680 --> 04:05:19.520]   That's an interesting question.
[04:05:19.520 --> 04:05:23.620]   - Let me first say, just as a bit of a criticism,
[04:05:23.620 --> 04:05:28.560]   that what kind of man do you think you are
[04:05:28.560 --> 04:05:30.560]   not wearing a suit and tie?
[04:05:30.560 --> 04:05:31.600]   A real man should.
[04:05:31.600 --> 04:05:33.760]   (laughing)
[04:05:33.760 --> 04:05:36.680]   Exactly, I agree with your dad on that point.
[04:05:36.680 --> 04:05:39.880]   You mentioned offline that he suggested
[04:05:39.880 --> 04:05:41.760]   a real man should wear a suit and tie.
[04:05:41.760 --> 04:05:45.480]   But outside of that,
[04:05:45.480 --> 04:05:48.440]   is there ways you could have been a better son?
[04:05:48.440 --> 04:05:51.040]   - Maybe next time on your show, I'll wear a suit and tie.
[04:05:51.040 --> 04:05:52.560]   (laughing)
[04:05:52.560 --> 04:05:54.200]   My dad would be happy about that.
[04:05:54.200 --> 04:05:56.920]   - Please.
[04:06:12.600 --> 04:06:15.740]   - I can answer the question later in life, not early.
[04:06:15.740 --> 04:06:19.560]   I had just a huge amount of respect and reverence
[04:06:19.560 --> 04:06:20.640]   for my dad when I was young.
[04:06:20.640 --> 04:06:23.840]   So I was asking myself that question a lot.
[04:06:23.840 --> 04:06:26.280]   So there weren't a lot of things I knew
[04:06:26.280 --> 04:06:28.380]   that I wasn't seeking to apply.
[04:06:28.380 --> 04:06:34.040]   There was a phase when I went through
[04:06:34.040 --> 04:06:38.040]   my kind of individuation differentiation
[04:06:38.040 --> 04:06:41.080]   where I had to make him excessively wrong
[04:06:41.080 --> 04:06:42.240]   about too many things.
[04:06:42.240 --> 04:06:45.480]   I don't think I had to, but I did.
[04:06:45.480 --> 04:06:50.480]   And he had a lot of kind of non-standard model beliefs
[04:06:50.480 --> 04:06:55.480]   about things, whether early kind of ancient civilizations
[04:06:55.480 --> 04:06:58.800]   or ideas on evolutionary theory
[04:06:58.800 --> 04:07:00.360]   or alternate models of physics.
[04:07:00.360 --> 04:07:06.200]   They weren't irrational,
[04:07:06.200 --> 04:07:08.080]   but they didn't all have the standard
[04:07:08.080 --> 04:07:12.000]   of epistemic proof that I would need.
[04:07:12.000 --> 04:07:16.200]   And I went through, and some of them
[04:07:16.200 --> 04:07:18.680]   were kind of spiritual ideas as well.
[04:07:18.680 --> 04:07:23.120]   I went through a phase in my early 20s
[04:07:23.120 --> 04:07:28.120]   where I kind of had the attitude that Dawkins
[04:07:28.120 --> 04:07:33.440]   or Christopher Hitchens has that can kind of be
[04:07:36.920 --> 04:07:39.560]   excessively certain and sanctimonious,
[04:07:39.560 --> 04:07:43.200]   applying their reductionist philosophy of science
[04:07:43.200 --> 04:07:46.740]   to everything and kind of brutally dismissive.
[04:07:46.740 --> 04:07:49.500]   I'm embarrassed by that phase.
[04:07:49.500 --> 04:07:55.100]   Not to say anything about those men and their path,
[04:07:55.100 --> 04:07:56.840]   but for myself.
[04:07:56.840 --> 04:08:01.840]   And so during that time, I was more dismissive
[04:08:01.840 --> 04:08:04.640]   of my dad's epistemology
[04:08:04.640 --> 04:08:06.760]   than I would have liked to have been.
[04:08:06.760 --> 04:08:08.960]   I got to correct that later, apologize for it.
[04:08:08.960 --> 04:08:11.920]   But that was the first thought that came to mind.
[04:08:11.920 --> 04:08:14.280]   - You've written the following.
[04:08:14.280 --> 04:08:17.960]   I've had the experience countless times,
[04:08:17.960 --> 04:08:22.280]   making love, watching a sunset, listening to music,
[04:08:22.280 --> 04:08:27.280]   feeling the breeze, that I would sign up for this whole life
[04:08:27.280 --> 04:08:32.480]   and all of its pains just to experience this exact moment.
[04:08:33.400 --> 04:08:35.600]   This is a kind of worldless knowing.
[04:08:35.600 --> 04:08:40.120]   It's the most important and real truth I know,
[04:08:40.120 --> 04:08:43.960]   that experience itself is infinitely meaningful
[04:08:43.960 --> 04:08:45.720]   and pain is temporary.
[04:08:45.720 --> 04:08:50.760]   And seen clearly, even the suffering is filled with beauty.
[04:08:50.760 --> 04:08:54.280]   I've experienced countless lives worth of moments
[04:08:54.280 --> 04:08:58.500]   worthy of life, such an unreasonable fortune.
[04:09:00.560 --> 04:09:03.680]   A few words of gratitude from you, beautifully written.
[04:09:03.680 --> 04:09:05.920]   Is there some beautiful moments?
[04:09:05.920 --> 04:09:08.880]   Now you have experienced countless lives
[04:09:08.880 --> 04:09:10.440]   worth of those moments,
[04:09:10.440 --> 04:09:14.880]   but is there some things that if you could,
[04:09:14.880 --> 04:09:19.540]   in your darker moments, you can go to to relive,
[04:09:19.540 --> 04:09:22.720]   to remind yourself that the whole ride is worthwhile?
[04:09:22.720 --> 04:09:24.320]   Maybe skip the making love part.
[04:09:24.320 --> 04:09:25.880]   We don't wanna know about that.
[04:09:27.920 --> 04:09:32.840]   - I mean, I feel unreasonably fortunate
[04:09:32.840 --> 04:09:39.920]   that it is such a humongous list because,
[04:09:39.920 --> 04:09:47.080]   I mean, I feel fortunate to have had exposure to practices
[04:09:47.080 --> 04:09:48.680]   and philosophies and way of seeing things
[04:09:48.680 --> 04:09:50.020]   that makes me see things that way.
[04:09:50.020 --> 04:09:53.200]   So I can take responsibility for seeing things in that way
[04:09:53.200 --> 04:09:55.980]   and not taking for granted really wonderful things,
[04:09:55.980 --> 04:09:57.800]   but I can't take credit for being exposed
[04:09:57.800 --> 04:10:00.500]   to the philosophies that even gave me that possibility.
[04:10:00.500 --> 04:10:06.760]   It's not just with my wife,
[04:10:06.760 --> 04:10:10.880]   it's with every person who I really love when we're talking,
[04:10:10.880 --> 04:10:11.920]   I look at their face.
[04:10:11.920 --> 04:10:14.160]   I, in the context of a conversation,
[04:10:14.160 --> 04:10:17.600]   feel overwhelmed by how lucky I am to get to know them.
[04:10:17.600 --> 04:10:19.940]   And like there's never been someone like them
[04:10:19.940 --> 04:10:22.000]   in all of history and there never will be again.
[04:10:22.000 --> 04:10:23.800]   And they might be gone tomorrow, I might be gone tomorrow.
[04:10:23.800 --> 04:10:25.360]   And like, I get this moment with them.
[04:10:25.360 --> 04:10:28.320]   And when you take in the uniqueness of that fully
[04:10:28.320 --> 04:10:30.920]   and the beauty of it, it's overwhelmingly beautiful.
[04:10:30.920 --> 04:10:38.360]   And I remember the first time I did a big dose of mushrooms
[04:10:38.360 --> 04:10:43.780]   and I was looking at a tree for a long time.
[04:10:43.780 --> 04:10:46.580]   And I was just crying with overwhelm at how beautiful
[04:10:46.580 --> 04:10:47.420]   the tree was.
[04:10:47.420 --> 04:10:48.700]   And it was a tree outside the front of my house
[04:10:48.700 --> 04:10:50.000]   that I'd walked by a million times
[04:10:50.000 --> 04:10:52.200]   and never looked at like this.
[04:10:52.200 --> 04:10:57.080]   And it wasn't the dose of mushrooms where I was hallucinating
[04:10:57.080 --> 04:10:59.000]   like where the tree was purple.
[04:10:59.000 --> 04:11:01.040]   Like the tree still looked like, if I had to describe it,
[04:11:01.040 --> 04:11:03.320]   say it's green and it has leaves, looks like this,
[04:11:03.320 --> 04:11:05.720]   but it was way fucking more beautiful,
[04:11:05.720 --> 04:11:08.560]   like capturing than it normally was.
[04:11:08.560 --> 04:11:09.840]   And I'm like, why is it so beautiful
[04:11:09.840 --> 04:11:12.020]   if I would describe it the same way?
[04:11:12.020 --> 04:11:15.120]   And I realized I had no thoughts taking me anywhere else.
[04:11:15.120 --> 04:11:15.960]   - Yeah.
[04:11:15.960 --> 04:11:17.720]   - Like what it seemed like the mushrooms were doing
[04:11:17.720 --> 04:11:19.640]   was just actually shutting the narrative off
[04:11:19.640 --> 04:11:20.800]   that would have me be distracted
[04:11:20.800 --> 04:11:22.840]   so I could really see the tree.
[04:11:22.840 --> 04:11:24.480]   And then I'm like, fuck, when I get off these mushrooms,
[04:11:24.480 --> 04:11:26.280]   I'm gonna practice seeing the tree
[04:11:26.280 --> 04:11:29.200]   because it's always that beautiful and I just miss it.
[04:11:29.200 --> 04:11:30.720]   And so I practice being with it
[04:11:30.720 --> 04:11:32.240]   and quieting the rest of the mind
[04:11:32.240 --> 04:11:33.560]   and then being like, wow.
[04:11:33.560 --> 04:11:35.120]   And if it's not mushrooms,
[04:11:35.120 --> 04:11:37.320]   like people will have peak experiences
[04:11:37.320 --> 04:11:40.000]   where they'll see life and how incredible it is.
[04:11:40.000 --> 04:11:41.880]   It's always there.
[04:11:41.880 --> 04:11:44.960]   - It's funny that I had this exact same experience
[04:11:44.960 --> 04:11:47.800]   on quite a lot of mushrooms,
[04:11:47.800 --> 04:11:50.320]   just sitting alone and looking at a tree
[04:11:50.320 --> 04:11:52.920]   and exactly as you described it,
[04:11:52.920 --> 04:11:56.080]   appreciating the undistorted beauty of it.
[04:11:56.080 --> 04:12:00.160]   And it's funny to me that here's two humans,
[04:12:00.160 --> 04:12:02.560]   very different with very different journeys
[04:12:02.560 --> 04:12:03.840]   where at some moment in time,
[04:12:03.840 --> 04:12:06.360]   both looking at a tree like idiots for hours
[04:12:06.360 --> 04:12:07.960]   (both laughing)
[04:12:07.960 --> 04:12:10.520]   and just in awe and happy to be alive.
[04:12:10.520 --> 04:12:14.960]   And yeah, even just that moment alone is worth living for.
[04:12:14.960 --> 04:12:17.200]   But you did say humans
[04:12:17.240 --> 04:12:21.120]   and we have a moment together as two humans
[04:12:21.120 --> 04:12:22.520]   and you mentioned shots.
[04:12:22.520 --> 04:12:23.360]   (both laughing)
[04:12:23.360 --> 04:12:26.960]   That's a, I have to ask, what are we looking at?
[04:12:26.960 --> 04:12:30.480]   - When I went to go get a smoothie before coming here,
[04:12:30.480 --> 04:12:32.480]   I got you a keto smoothie that you didn't want
[04:12:32.480 --> 04:12:34.880]   'cause you're not just keto but fasting.
[04:12:34.880 --> 04:12:37.160]   But I saw the thing with you and your dad
[04:12:37.160 --> 04:12:39.520]   where you did shots together.
[04:12:39.520 --> 04:12:41.240]   And this place happened to have shots
[04:12:41.240 --> 04:12:45.920]   of ginger turmeric cayenne juice of some kind.
[04:12:45.920 --> 04:12:47.400]   - With some Himalayan salt.
[04:12:47.400 --> 04:12:49.960]   - I didn't necessarily plan it for being on the show.
[04:12:49.960 --> 04:12:51.200]   I just brought it.
[04:12:51.200 --> 04:12:52.040]   - Wow.
[04:12:52.040 --> 04:12:52.880]   - But we can do it that way.
[04:12:52.880 --> 04:12:57.880]   - I think we shall toast like heroes, Daniel.
[04:12:57.880 --> 04:12:59.920]   It's a huge honor.
[04:12:59.920 --> 04:13:00.760]   - What do we toast to?
[04:13:00.760 --> 04:13:01.640]   What do we toast to?
[04:13:01.640 --> 04:13:03.320]   - We toast to this moment,
[04:13:03.320 --> 04:13:07.000]   this unique moment that we get to share together.
[04:13:07.000 --> 04:13:09.040]   - I'm very grateful to be here in this moment with you.
[04:13:09.040 --> 04:13:12.680]   And yeah, I'm grateful that you invited me here.
[04:13:12.680 --> 04:13:16.200]   We met for the first time and I will never be the same
[04:13:16.200 --> 04:13:18.880]   for the good and the bad.
[04:13:18.880 --> 04:13:19.720]   - I am.
[04:13:19.720 --> 04:13:24.240]   - That is really interesting.
[04:13:24.240 --> 04:13:26.480]   That feels way healthier than the vodka
[04:13:26.480 --> 04:13:27.840]   my dad and I were drinking.
[04:13:27.840 --> 04:13:31.160]   So I feel like a better man already.
[04:13:31.160 --> 04:13:34.200]   Daniel, this is one of the best conversations I've ever had.
[04:13:34.200 --> 04:13:36.120]   I can't wait to have many more.
[04:13:36.120 --> 04:13:37.120]   - Likewise.
[04:13:37.120 --> 04:13:39.120]   - This has been an amazing experience.
[04:13:39.120 --> 04:13:40.960]   Thank you for wasting all your time today.
[04:13:40.960 --> 04:13:43.920]   I wanna say in terms of what you're mentioning about,
[04:13:43.920 --> 04:13:48.760]   like the, that you work in machine learning
[04:13:48.760 --> 04:13:52.960]   and the optimism that wants to look at the issues,
[04:13:52.960 --> 04:13:56.040]   but wants to look at how this increased technological power
[04:13:56.040 --> 04:13:57.800]   could be applied to solving them.
[04:13:57.800 --> 04:14:01.760]   And that even thinking about the broadcast of like,
[04:14:01.760 --> 04:14:04.160]   can I help people understand the issues better
[04:14:04.160 --> 04:14:05.520]   and help organize them?
[04:14:05.520 --> 04:14:09.400]   Like fundamentally you're oriented like Wikipedia.
[04:14:09.400 --> 04:14:13.960]   What I see to really try to tend to the information commons
[04:14:13.960 --> 04:14:16.800]   without another agentic interest distorting it.
[04:14:16.800 --> 04:14:22.120]   And for you to be able to get guys like Lee Smolin
[04:14:22.120 --> 04:14:25.440]   and Roger Penrose and like the greatest thinkers
[04:14:25.440 --> 04:14:29.040]   that are alive and have them on the show.
[04:14:29.040 --> 04:14:30.760]   And most people would never be exposed to them
[04:14:30.760 --> 04:14:33.840]   and talk about it in a way that people can understand.
[04:14:33.840 --> 04:14:35.760]   I think it's an incredible service.
[04:14:35.760 --> 04:14:36.920]   I think you're doing great work.
[04:14:36.920 --> 04:14:39.360]   So I was really happy to hear from you.
[04:14:39.360 --> 04:14:41.240]   - Thank you, Daniel.
[04:14:41.240 --> 04:14:42.680]   Thanks for listening to this conversation
[04:14:42.680 --> 04:14:44.080]   with Daniel Schmachtenberger.
[04:14:44.080 --> 04:14:47.520]   And thank you to Ground News, NetSuite,
[04:14:47.520 --> 04:14:51.640]   Four Sigmatic, Magic Spoon and BetterHelp.
[04:14:51.640 --> 04:14:55.480]   Check them out in the description to support this podcast.
[04:14:55.480 --> 04:14:57.560]   And now let me leave you with some words
[04:14:57.560 --> 04:14:59.600]   from Albert Einstein.
[04:14:59.600 --> 04:15:03.320]   "I know not with what weapons World War III will be fought,
[04:15:03.320 --> 04:15:07.120]   "but World War IV will be fought with sticks and stones."
[04:15:08.040 --> 04:15:11.120]   Thank you for listening and hope to see you next time.
[04:15:11.120 --> 04:15:13.720]   (upbeat music)
[04:15:13.720 --> 04:15:16.320]   (upbeat music)
[04:15:16.320 --> 04:15:26.320]   [BLANK_AUDIO]

