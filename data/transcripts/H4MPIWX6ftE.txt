
[00:00:00.000 --> 00:00:03.240]   All the examples that we think about when we talk about machine learning,
[00:00:03.240 --> 00:00:05.680]   from hot dog, not hot dog,
[00:00:05.680 --> 00:00:08.080]   to predicting the stock market,
[00:00:08.080 --> 00:00:10.600]   to understanding speech,
[00:00:10.600 --> 00:00:14.760]   all these things that you really think about classifying things that you've already seen.
[00:00:14.760 --> 00:00:16.520]   When we look at the MNIST dataset,
[00:00:16.520 --> 00:00:20.440]   we're talking about labeling numbers 0-9,
[00:00:20.440 --> 00:00:24.920]   but we're not talking about recognizing other digits that we've maybe never seen before.
[00:00:24.920 --> 00:00:27.800]   In the real world, it's actually common that you want to classify
[00:00:27.800 --> 00:00:30.640]   something where you've literally never seen it.
[00:00:30.640 --> 00:00:32.840]   Humans can do this really well.
[00:00:32.840 --> 00:00:34.600]   When I see a spatula for the first time,
[00:00:34.600 --> 00:00:36.960]   I recognize it as a new object.
[00:00:36.960 --> 00:00:38.880]   Maybe I don't know the name for it,
[00:00:38.880 --> 00:00:40.280]   but I recognize that it's a thing,
[00:00:40.280 --> 00:00:42.080]   and I can recognize if I see it again,
[00:00:42.080 --> 00:00:44.520]   that's that thing that I saw before.
[00:00:44.520 --> 00:00:51.280]   So how do we make computers do this classification?
[00:00:51.280 --> 00:00:54.240]   This is an example that's called sometimes one shot,
[00:00:54.240 --> 00:00:56.080]   or zero shot, or sometimes few shot,
[00:00:56.080 --> 00:00:57.640]   if you have a couple examples.
[00:00:57.640 --> 00:01:02.760]   It's really been a challenge for machine learning in general to make this work.
[00:01:02.760 --> 00:01:07.080]   One of the approaches that I think is really exciting that I want to talk about today,
[00:01:07.080 --> 00:01:09.320]   is a reframing of the problem.
[00:01:09.320 --> 00:01:11.760]   So instead of looking at one object and saying,
[00:01:11.760 --> 00:01:13.640]   what is this object and training on that,
[00:01:13.640 --> 00:01:16.480]   what I want to do is train on pairs of objects,
[00:01:16.480 --> 00:01:19.400]   where the question is, are these two objects the same?
[00:01:19.400 --> 00:01:21.400]   Now, what's so cool about that is that,
[00:01:21.400 --> 00:01:24.520]   if instead of building a classifier of one thing at a time,
[00:01:24.520 --> 00:01:26.680]   I build a classifier of pairs of things,
[00:01:26.680 --> 00:01:28.640]   I can actually look at something new,
[00:01:28.640 --> 00:01:30.080]   and look at one example,
[00:01:30.080 --> 00:01:32.000]   maybe a canonical example of that thing,
[00:01:32.000 --> 00:01:33.760]   and say, is it that thing?
[00:01:33.760 --> 00:01:36.320]   So my classifier can potentially generalize,
[00:01:36.320 --> 00:01:39.200]   not just to classify the things that it's seen in the training data,
[00:01:39.200 --> 00:01:43.440]   but maybe to classify anything that it might see out there.
[00:01:43.440 --> 00:01:45.720]   This technique is really generalizable.
[00:01:45.720 --> 00:01:47.200]   I'm going to do it on images,
[00:01:47.200 --> 00:01:49.800]   but the same approach can be used on video,
[00:01:49.800 --> 00:01:50.880]   it can be used on audio,
[00:01:50.880 --> 00:01:53.880]   it can be used in tons and tons of different examples.
[00:01:53.880 --> 00:01:56.600]   Let's get to it. All right.
[00:01:56.600 --> 00:01:59.640]   So let's walk through an example of how we're going to do this.
[00:01:59.640 --> 00:02:02.640]   I'm going to do this on the MNIST dataset at first,
[00:02:02.640 --> 00:02:06.080]   just because it's a dataset you're probably familiar with from previous videos,
[00:02:06.080 --> 00:02:08.560]   and it's really fast to run experiments on.
[00:02:08.560 --> 00:02:12.280]   So first, we have the requisite lots of imports,
[00:02:12.280 --> 00:02:13.840]   and then we're going to load the data,
[00:02:13.840 --> 00:02:17.080]   and we're going to load it exactly the same as we've done in previous videos,
[00:02:17.080 --> 00:02:19.320]   where we load the data into the train,
[00:02:19.320 --> 00:02:20.880]   X train is the images,
[00:02:20.880 --> 00:02:23.320]   Y train is the labels on the training data,
[00:02:23.320 --> 00:02:25.200]   X test is the images in the test data,
[00:02:25.200 --> 00:02:28.680]   and then Y train is the labels for the test data.
[00:02:28.680 --> 00:02:31.240]   Then we're going to normalize just like we've done in a lot of other videos,
[00:02:31.240 --> 00:02:33.680]   where we just divide the values by 255,
[00:02:33.680 --> 00:02:35.760]   so that our pixels are between zero and one,
[00:02:35.760 --> 00:02:39.680]   instead of being between zero and 255.
[00:02:39.680 --> 00:02:42.040]   But now, we're going to do something new.
[00:02:42.040 --> 00:02:45.760]   We're going to call this function that I wrote called make_pairs.
[00:02:45.760 --> 00:02:49.840]   What make_pairs does is it takes in input data and labels,
[00:02:49.840 --> 00:02:52.400]   and it makes a new dataset,
[00:02:52.400 --> 00:02:55.240]   where the dataset is actually pairs of images,
[00:02:55.240 --> 00:02:57.600]   and here the label is actually one,
[00:02:57.600 --> 00:03:00.800]   if the two images correspond to the same category of thing,
[00:03:00.800 --> 00:03:05.160]   and zero if they correspond to different categories of thing.
[00:03:05.160 --> 00:03:07.840]   So I just wrote a little bit of code here that actually just
[00:03:07.840 --> 00:03:11.680]   randomly walks through digits and then picks other digits that match,
[00:03:11.680 --> 00:03:14.360]   and then adds a label of is the same thing,
[00:03:14.360 --> 00:03:16.080]   and then it finds two that don't match,
[00:03:16.080 --> 00:03:17.920]   and adds a label of not the same thing.
[00:03:17.920 --> 00:03:20.160]   So what comes out of this is
[00:03:20.160 --> 00:03:23.280]   a dataset where half the images are
[00:03:23.280 --> 00:03:26.480]   same things and half the images are different things.
[00:03:26.480 --> 00:03:30.680]   So this function at the bottom creates a new variable called pairs_train,
[00:03:30.680 --> 00:03:33.840]   which is going to be the pairs of images,
[00:03:33.840 --> 00:03:36.520]   and labels_train, which is going to be either
[00:03:36.520 --> 00:03:40.360]   zero for not the same thing or one for the same thing.
[00:03:40.360 --> 00:03:43.760]   So we can run this here.
[00:03:46.480 --> 00:03:52.280]   We got to load the data first,
[00:03:52.280 --> 00:03:54.640]   then we can run this guy.
[00:03:54.640 --> 00:03:58.000]   As usual, I always recommend taking a little peek at the data.
[00:03:58.000 --> 00:04:00.640]   So why don't we look at pairs_train here,
[00:04:00.640 --> 00:04:04.280]   four, zero, and it turns out that is a number four,
[00:04:04.280 --> 00:04:05.440]   that's just an accident.
[00:04:05.440 --> 00:04:08.040]   The fourth dataset is a four.
[00:04:08.040 --> 00:04:10.080]   Then if we look at pairs_train four,
[00:04:10.080 --> 00:04:13.840]   one, that's actually a different looking four.
[00:04:13.840 --> 00:04:18.680]   So we would expect then labels_train four to be a one,
[00:04:18.680 --> 00:04:20.680]   meaning that they're the same image.
[00:04:20.680 --> 00:04:24.600]   So why don't we just print that out,
[00:04:24.600 --> 00:04:32.320]   labels_train four, and yep, they're the same image.
[00:04:32.320 --> 00:04:36.960]   We can look at maybe the 400th example of pairs_train.
[00:04:36.960 --> 00:04:40.920]   So here that's a one.
[00:04:40.920 --> 00:04:44.880]   Then if we look at the adjacent one, it's another one.
[00:04:44.880 --> 00:04:48.000]   A little bit different writing, but the same thing.
[00:04:48.000 --> 00:04:50.760]   So we've transformed our data.
[00:04:50.760 --> 00:04:53.280]   Now, what are we going to do with it?
[00:04:53.280 --> 00:04:57.440]   So naively, one thing we could do is actually just pass
[00:04:57.440 --> 00:05:01.480]   in each image into a separate dense network,
[00:05:01.480 --> 00:05:03.600]   and then concatenate those and have
[00:05:03.600 --> 00:05:08.000]   a final dense layer to predict same image or a different image.
[00:05:08.000 --> 00:05:09.840]   So that's what we're going to do here.
[00:05:09.840 --> 00:05:12.640]   So here our first sequential model
[00:05:12.640 --> 00:05:14.760]   is just a flatten and then a dense layer.
[00:05:14.760 --> 00:05:17.360]   So this is just the perceptron that you might be used to,
[00:05:17.360 --> 00:05:19.920]   but we're going to use a ReLU activation function because it's
[00:05:19.920 --> 00:05:21.760]   an intermediate piece.
[00:05:21.760 --> 00:05:24.280]   And then we're actually going to have the exact same layer,
[00:05:24.280 --> 00:05:25.920]   but a different set of weights.
[00:05:25.920 --> 00:05:28.320]   And now here's a new layer you might not have seen before,
[00:05:28.320 --> 00:05:29.280]   but it's super useful.
[00:05:29.280 --> 00:05:30.920]   It's called concatenate.
[00:05:30.920 --> 00:05:33.920]   So what that does is actually just takes two layers,
[00:05:33.920 --> 00:05:36.080]   the outputs of two layers, and puts them together
[00:05:36.080 --> 00:05:38.920]   into a single set of activations.
[00:05:38.920 --> 00:05:40.240]   So no parameters.
[00:05:40.240 --> 00:05:42.240]   It just combines the two.
[00:05:42.240 --> 00:05:44.440]   And then the final layer, I'm calling dense layer,
[00:05:44.440 --> 00:05:47.800]   takes as inputs the things from the merge layer
[00:05:47.800 --> 00:05:49.400]   and then outputs a single number.
[00:05:49.400 --> 00:05:53.640]   And hopefully that's going to be a 1 if the images are the same
[00:05:53.640 --> 00:05:55.640]   and a 0 if the images are not the same.
[00:05:55.640 --> 00:05:57.280]   So we use a sigmoid activation function
[00:05:57.280 --> 00:06:00.360]   because it's kind of a binary classification.
[00:06:00.360 --> 00:06:02.960]   And then we use the Keras functional definition
[00:06:02.960 --> 00:06:05.460]   to define this because it's actually not a sequential model,
[00:06:05.460 --> 00:06:07.800]   because we have two inputs and then we're combining them.
[00:06:07.800 --> 00:06:09.960]   It's not just a simple sequential model
[00:06:09.960 --> 00:06:10.920]   we might be used to.
[00:06:10.920 --> 00:06:14.920]   And so we use a more complicated way of defining it.
[00:06:14.920 --> 00:06:16.520]   Then we compile the model.
[00:06:16.520 --> 00:06:18.400]   We use binary cross-entropy because we're
[00:06:18.400 --> 00:06:20.760]   doing a single binary classification.
[00:06:20.760 --> 00:06:23.400]   And we use our standard atom optimizer,
[00:06:23.400 --> 00:06:25.820]   and we're going to output the accuracy.
[00:06:25.820 --> 00:06:28.080]   So let's take a quick look at what this model looks like
[00:06:28.080 --> 00:06:31.200]   before we run it here.
[00:06:31.200 --> 00:06:34.480]   And you can see here that we have 100,000 parameters
[00:06:34.480 --> 00:06:36.840]   in our dense layer that corresponds to image 1
[00:06:36.840 --> 00:06:39.680]   and 100,000 parameters that corresponds to our image 2.
[00:06:39.680 --> 00:06:42.880]   And then each of those fully connected layers,
[00:06:42.880 --> 00:06:44.760]   they output 128 numbers.
[00:06:44.760 --> 00:06:47.100]   We combine those into 256 numbers.
[00:06:47.100 --> 00:06:49.080]   And then we have a single perceptron
[00:06:49.080 --> 00:06:52.400]   with 256 inputs and one single output
[00:06:52.400 --> 00:06:54.880]   at the bottom of our network.
[00:06:54.880 --> 00:06:59.100]   So in total, it's about 200,000 parameters.
[00:06:59.100 --> 00:07:01.480]   And we can call fit here.
[00:07:01.480 --> 00:07:05.200]   And now again, we'll call fit on actually pairs train 0.
[00:07:05.200 --> 00:07:06.660]   So that's one of the input images.
[00:07:06.660 --> 00:07:09.880]   Pairs train 1, which is the other set of input images.
[00:07:09.880 --> 00:07:13.320]   And then labels train, which is again, the binary number 0
[00:07:13.320 --> 00:07:18.640]   if the images don't match and 1 if they do match.
[00:07:18.640 --> 00:07:23.520]   So let's set that to say 10 epics and let our model train.
[00:07:23.520 --> 00:07:33.440]   So this architecture does work barely.
[00:07:33.440 --> 00:07:35.800]   So you can see that at every step,
[00:07:35.800 --> 00:07:40.560]   it actually is improving the accuracy by about 0.5%.
[00:07:40.560 --> 00:07:43.500]   And it's starting at a 50% accuracy.
[00:07:43.500 --> 00:07:45.720]   So better than random, which is better
[00:07:45.720 --> 00:07:48.440]   than a lot of the networks that I've made in my life.
[00:07:48.440 --> 00:07:50.000]   And we're kind of onto something good.
[00:07:50.000 --> 00:07:51.480]   But it seems pretty clear that we're
[00:07:51.480 --> 00:07:53.720]   going to need to make this work better.
[00:07:53.720 --> 00:07:56.600]   So what we've done so far doesn't work super well.
[00:07:56.600 --> 00:07:58.560]   It's unclear actually how well it'll ever work.
[00:07:58.560 --> 00:07:59.640]   It does work better than random.
[00:07:59.640 --> 00:08:01.060]   But it's not working super well.
[00:08:01.060 --> 00:08:02.440]   And it's not typically what people
[00:08:02.440 --> 00:08:04.480]   do when they encounter the situation where they
[00:08:04.480 --> 00:08:06.200]   want to do one-shot learning.
[00:08:06.200 --> 00:08:08.880]   What they really do is they share weights
[00:08:08.880 --> 00:08:10.800]   across the model.
[00:08:10.800 --> 00:08:13.520]   So sharing weights across layers is actually
[00:08:13.520 --> 00:08:16.320]   pretty common in more advanced architectures.
[00:08:16.320 --> 00:08:17.560]   But we haven't done it yet.
[00:08:17.560 --> 00:08:18.760]   So it's a good thing to know.
[00:08:18.760 --> 00:08:21.640]   And it's actually really effective in this case.
[00:08:21.640 --> 00:08:24.520]   It's one of the things we have to do to make this thing really
[00:08:24.520 --> 00:08:25.760]   work well.
[00:08:25.760 --> 00:08:28.360]   And the intuition is that the model that we're
[00:08:28.360 --> 00:08:31.040]   running on the first input image and the model
[00:08:31.040 --> 00:08:32.840]   that we're running on the second input image, really,
[00:08:32.840 --> 00:08:34.880]   it seems like they should be the same model.
[00:08:34.880 --> 00:08:38.400]   Because the images are drawn from the same set
[00:08:38.400 --> 00:08:39.560]   of overall images.
[00:08:39.560 --> 00:08:41.760]   And so the transform that we want to do on one image
[00:08:41.760 --> 00:08:43.960]   seems like it should really be the transform that we
[00:08:43.960 --> 00:08:45.720]   do on the other image.
[00:08:45.720 --> 00:08:47.760]   So in order to do this, in order to share weights
[00:08:47.760 --> 00:08:50.800]   across the model, we have to actually use
[00:08:50.800 --> 00:08:54.120]   more of Keras' functional model definition.
[00:08:54.120 --> 00:08:55.960]   And I think this gets a little confusing,
[00:08:55.960 --> 00:09:01.040]   because when we define a layer in the functional definition,
[00:09:01.040 --> 00:09:04.000]   we actually just set up the specification for the layer.
[00:09:04.000 --> 00:09:06.640]   And it doesn't actually really attach it to some input
[00:09:06.640 --> 00:09:10.680]   until we call a function on that layer once specified.
[00:09:10.680 --> 00:09:13.240]   So then we set up a model.
[00:09:13.240 --> 00:09:15.840]   And so we actually say that the input is going to be this input.
[00:09:15.840 --> 00:09:17.440]   And actually, what the model does
[00:09:17.440 --> 00:09:20.360]   is going to be this flatten step and then this dense step.
[00:09:20.360 --> 00:09:24.240]   But now we haven't actually attached this model
[00:09:24.240 --> 00:09:24.840]   to any input.
[00:09:24.840 --> 00:09:26.540]   So what we're going to do, we're actually
[00:09:26.540 --> 00:09:28.340]   going to attach it to two different inputs.
[00:09:28.340 --> 00:09:31.200]   We're going to attach it to input 1 and input 2.
[00:09:31.200 --> 00:09:34.440]   And so I call the model that's attached to input 1 dense 1,
[00:09:34.440 --> 00:09:37.320]   and the model that's attached to input 2 dense 2.
[00:09:37.320 --> 00:09:39.360]   So we have two separate models, but they're
[00:09:39.360 --> 00:09:41.920]   attached to different inputs.
[00:09:41.920 --> 00:09:43.460]   So we can actually take those, and we
[00:09:43.460 --> 00:09:45.640]   can use that same concatenate layer that we
[00:09:45.640 --> 00:09:47.960]   used before to combine them.
[00:09:47.960 --> 00:09:50.720]   And then we can add that same dense layer that we had before.
[00:09:50.720 --> 00:09:53.240]   And that's going to output a single number.
[00:09:53.240 --> 00:09:55.560]   We're going to use a sigmoid activation layer.
[00:09:55.560 --> 00:09:57.320]   And that number is obviously going to be 1
[00:09:57.320 --> 00:09:59.580]   if we think that these two images are corresponding
[00:09:59.580 --> 00:10:01.980]   to the same number, and 0 if they're
[00:10:01.980 --> 00:10:04.280]   like two different numbers.
[00:10:04.280 --> 00:10:06.740]   So we compile the model the same way we did before.
[00:10:06.740 --> 00:10:08.720]   And then we can take a quick look at it.
[00:10:08.720 --> 00:10:10.800]   And we can see that actually this model should
[00:10:10.800 --> 00:10:12.680]   have about half the number of parameters
[00:10:12.680 --> 00:10:14.180]   of the previous model because we're
[00:10:14.180 --> 00:10:15.960]   sharing those parameters.
[00:10:15.960 --> 00:10:17.920]   So whereas before we had two layers,
[00:10:17.920 --> 00:10:19.880]   each with 100,000 parameters, now we only
[00:10:19.880 --> 00:10:22.280]   have one set of 100,000 parameters.
[00:10:22.280 --> 00:10:24.160]   But it's actually two different layers
[00:10:24.160 --> 00:10:25.800]   that are getting called, but each
[00:10:25.800 --> 00:10:28.680]   with those shared parameters.
[00:10:28.680 --> 00:10:30.400]   So we can run this model too.
[00:10:30.400 --> 00:10:33.720]   And spoiler alert, it works a little bit better
[00:10:33.720 --> 00:10:36.880]   than the last thing we did, but not a lot better
[00:10:36.880 --> 00:10:40.480]   because there's actually one more fancy optimization
[00:10:40.480 --> 00:10:41.480]   that we need to add in.
[00:10:41.480 --> 00:10:43.960]   And then we'll have the typical setup
[00:10:43.960 --> 00:10:46.180]   of what's called a Siamese network, which is actually
[00:10:46.180 --> 00:10:48.680]   an old concept.
[00:10:48.680 --> 00:10:50.920]   It was talked about in the '90s.
[00:10:50.920 --> 00:10:53.800]   But I feel like it's had renewed interest in various forms
[00:10:53.800 --> 00:10:56.480]   as people have gotten more and more excited about deep
[00:10:56.480 --> 00:10:59.960]   learning and this one-shot learning problem specifically.
[00:10:59.960 --> 00:11:02.320]   So you do from Keras import backend as K.
[00:11:02.320 --> 00:11:04.120]   And this harkens back to the time
[00:11:04.120 --> 00:11:06.720]   when Keras had typically multiple backends.
[00:11:06.720 --> 00:11:09.360]   These days, really, it's almost always TensorFlow.
[00:11:09.360 --> 00:11:12.560]   So I just look at any TensorFlow operation
[00:11:12.560 --> 00:11:14.800]   as something that I can run here.
[00:11:14.800 --> 00:11:17.080]   And now I define a function where
[00:11:17.080 --> 00:11:18.760]   it takes in inputs, which are actually
[00:11:18.760 --> 00:11:20.520]   going to be TensorFlow tensors.
[00:11:20.520 --> 00:11:24.720]   And then I can call K dot and then any TensorFlow operation
[00:11:24.720 --> 00:11:25.680]   that I can find.
[00:11:25.680 --> 00:11:30.360]   So here I'm using sum and square and square root and maximum.
[00:11:30.360 --> 00:11:32.760]   Really, all this is doing is it's basically
[00:11:32.760 --> 00:11:36.560]   looking at the sum of the squares of the differences
[00:11:36.560 --> 00:11:39.240]   between the two inputs.
[00:11:39.240 --> 00:11:41.720]   So it's kind of a simple-- they call it Euclidean distance.
[00:11:41.720 --> 00:11:43.980]   It's really just how different are the outputs of my two
[00:11:43.980 --> 00:11:44.840]   different networks.
[00:11:44.840 --> 00:11:46.420]   That's what we're going to do with it.
[00:11:46.420 --> 00:11:49.240]   We're going to feed in the outputs of each network.
[00:11:49.240 --> 00:11:52.840]   And then we're going to compare them using Euclidean distance.
[00:11:52.840 --> 00:11:56.320]   So we define this nice little Euclidean distance function.
[00:11:56.320 --> 00:11:57.960]   And then we add a new layer.
[00:11:57.960 --> 00:11:59.680]   It's called a lambda layer, which
[00:11:59.680 --> 00:12:01.560]   implies kind of a lambda function.
[00:12:01.560 --> 00:12:04.840]   And we actually pass in our Euclidean distance function.
[00:12:04.840 --> 00:12:07.160]   And so now we're actually building our own custom
[00:12:07.160 --> 00:12:10.240]   TensorFlow operations as a layer.
[00:12:10.240 --> 00:12:12.040]   And what this does is it basically
[00:12:12.040 --> 00:12:14.160]   lets the network, instead of trying to figure out
[00:12:14.160 --> 00:12:16.620]   what it should do with the outputs of these two networks
[00:12:16.620 --> 00:12:19.040]   that we've defined, it just knows that really what I want
[00:12:19.040 --> 00:12:22.200]   is the outputs of these two networks to be similar.
[00:12:22.200 --> 00:12:24.880]   So the more similar the outputs of my two networks are,
[00:12:24.880 --> 00:12:27.080]   the more likely the model thinks that the two inputs
[00:12:27.080 --> 00:12:29.400]   are the same, corresponding to the same number.
[00:12:29.400 --> 00:12:31.280]   So let's run this network.
[00:12:31.280 --> 00:12:34.640]   And we can compile it and look at it just like we did before.
[00:12:34.640 --> 00:12:37.000]   And we see that it's very similar to the previous model,
[00:12:37.000 --> 00:12:39.360]   but we don't have that last big dense layer
[00:12:39.360 --> 00:12:40.480]   to figure things out.
[00:12:40.480 --> 00:12:44.800]   And then when we run this network,
[00:12:44.800 --> 00:12:46.400]   we actually see a marked improvement.
[00:12:46.400 --> 00:12:48.040]   In the first epoch, we're already
[00:12:48.040 --> 00:12:50.640]   seeing accuracy above 70%.
[00:12:50.640 --> 00:12:53.240]   So by taking out some of the complexity
[00:12:53.240 --> 00:12:55.760]   and pushing the complexity into the code,
[00:12:55.760 --> 00:12:58.880]   we've actually made a much more effective Siamese network.
[00:12:58.880 --> 00:13:01.520]   So the real reason to do this is not the MNIST dataset.
[00:13:01.520 --> 00:13:02.980]   I mean, it seems unlikely that you'd
[00:13:02.980 --> 00:13:04.800]   want to generalize to some other digit
[00:13:04.800 --> 00:13:06.520]   that we haven't seen before.
[00:13:06.520 --> 00:13:09.140]   But there actually are lots of cases where you'd want to do it.
[00:13:09.140 --> 00:13:11.280]   And one case is in handwriting, where
[00:13:11.280 --> 00:13:13.640]   you might see characters that you haven't seen before.
[00:13:13.640 --> 00:13:15.640]   And a super cool dataset to do this on
[00:13:15.640 --> 00:13:19.280]   that's really fun and a lot like MNIST is the Omniglot dataset.
[00:13:19.280 --> 00:13:21.920]   So I've actually left in a little bit of code
[00:13:21.920 --> 00:13:24.400]   to load in the Omniglot dataset, which actually loads
[00:13:24.400 --> 00:13:26.200]   in lots and lots of different characters
[00:13:26.200 --> 00:13:27.940]   from lots and lots of different languages.
[00:13:27.940 --> 00:13:30.080]   And so I think a fun next step to do
[00:13:30.080 --> 00:13:32.800]   would be to run this exact same architecture
[00:13:32.800 --> 00:13:36.080]   on the Omniglot dataset and see if you can recognize characters.
[00:13:36.080 --> 00:13:37.420]   And actually see if you can build
[00:13:37.420 --> 00:13:40.360]   a system that can recognize characters in one alphabet
[00:13:40.360 --> 00:13:42.380]   and generalize to other alphabets,
[00:13:42.380 --> 00:13:44.740]   because that is really magical and powerful
[00:13:44.740 --> 00:13:48.040]   and really shows off why one-shot learning can be
[00:13:48.040 --> 00:13:50.440]   really effective, and especially why Siamese networks work
[00:13:50.440 --> 00:13:53.520]   really well for this application.
[00:13:53.520 --> 00:13:55.080]   [END PLAYBACK]
[00:13:55.080 --> 00:13:58.140]   [AUDIO OUT]
[00:13:58.140 --> 00:14:01.200]   [AUDIO OUT]
[00:14:01.200 --> 00:14:03.200]   You

