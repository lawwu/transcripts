
[00:00:00.000 --> 00:00:03.160]   [MUSIC PLAYING]
[00:00:03.160 --> 00:00:03.800]   Hi, all.
[00:00:03.800 --> 00:00:05.080]   My name is Alex Avrila.
[00:00:05.080 --> 00:00:07.480]   I am co-director at Carper AI.
[00:00:07.480 --> 00:00:10.320]   And today, I will be talking about TRLX, a framework
[00:00:10.320 --> 00:00:12.160]   for open source RLHF.
[00:00:12.160 --> 00:00:13.800]   So what is RLHF?
[00:00:13.800 --> 00:00:16.560]   RLHF is an acronym standing for Reinforcement Learning
[00:00:16.560 --> 00:00:17.800]   from Human Feedback.
[00:00:17.800 --> 00:00:19.440]   And you can think of it as a technique
[00:00:19.440 --> 00:00:21.880]   for fine-tuning language models to incorporate
[00:00:21.880 --> 00:00:23.220]   human preferences.
[00:00:23.220 --> 00:00:26.120]   And it can be roughly broken down into three stages.
[00:00:26.120 --> 00:00:28.840]   Number one, first, you collect pairwise comparisons
[00:00:28.840 --> 00:00:31.280]   among model responses to a given task.
[00:00:31.280 --> 00:00:33.960]   So for example, let's say I want to write happy stories.
[00:00:33.960 --> 00:00:35.840]   Look up the right-hand side.
[00:00:35.840 --> 00:00:38.080]   And the model generates two potential responses,
[00:00:38.080 --> 00:00:40.920]   one about a happy goose and one about a sad goose.
[00:00:40.920 --> 00:00:44.360]   Since I want to write stories which are happy,
[00:00:44.360 --> 00:00:46.880]   I'm going to select the response which
[00:00:46.880 --> 00:00:49.400]   is about a happy goose as more preferable to the response
[00:00:49.400 --> 00:00:51.240]   which is about a sad goose.
[00:00:51.240 --> 00:00:53.120]   These pairwise comparisons can then
[00:00:53.120 --> 00:00:56.240]   be used to train a reward model, which
[00:00:56.240 --> 00:00:58.480]   you can think of as an adapted language model which
[00:00:58.480 --> 00:01:03.360]   assigns a higher score or reward to the response which
[00:01:03.360 --> 00:01:05.480]   is preferred by the user.
[00:01:05.480 --> 00:01:06.900]   You can think of this reward model
[00:01:06.900 --> 00:01:09.400]   as an offline proxy for the human preferences which
[00:01:09.400 --> 00:01:11.240]   were captured in stage one.
[00:01:11.240 --> 00:01:13.640]   Then once this reward model has been learned,
[00:01:13.640 --> 00:01:16.800]   we optimize against it using a reinforcement learning
[00:01:16.800 --> 00:01:20.160]   algorithm, most commonly proximal policy optimization
[00:01:20.160 --> 00:01:21.360]   or PPO.
[00:01:21.360 --> 00:01:24.280]   And this results in a downstream student model
[00:01:24.280 --> 00:01:28.120]   which is able to write happy stories or more generally,
[00:01:28.120 --> 00:01:30.240]   it's aligned to the preferences which
[00:01:30.240 --> 00:01:32.120]   were collected in stage one and distilled
[00:01:32.120 --> 00:01:34.400]   into a reward model in stage two.
[00:01:34.400 --> 00:01:36.520]   And this makes up the RLHF pipeline.
[00:01:36.520 --> 00:01:41.480]   So some examples of popular RLHF models in the wild.
[00:01:41.480 --> 00:01:43.960]   JTPT, I think we're all familiar with.
[00:01:43.960 --> 00:01:46.160]   Most people have heard it by now.
[00:01:46.160 --> 00:01:49.360]   This is an example of what we believe to be an RLHF model.
[00:01:49.360 --> 00:01:51.960]   It has been trained using reinforcement learning
[00:01:51.960 --> 00:01:54.480]   and optimized against a reward model which
[00:01:54.480 --> 00:01:55.880]   incorporates human preferences.
[00:01:55.880 --> 00:01:59.000]   However, there are other examples of RLHF models
[00:01:59.000 --> 00:02:00.000]   in the wild as well.
[00:02:00.000 --> 00:02:03.120]   So Anthropic AI, a competitor to JTPT,
[00:02:03.120 --> 00:02:05.000]   has their own model called Collod which
[00:02:05.000 --> 00:02:06.600]   is trained in a similar way.
[00:02:06.600 --> 00:02:09.240]   And then the research group DeepMind also
[00:02:09.240 --> 00:02:11.600]   has their own version of this called Sparrow which
[00:02:11.600 --> 00:02:13.880]   optimizes against a reward model specifically
[00:02:13.880 --> 00:02:16.040]   for web-based retrieval.
[00:02:16.040 --> 00:02:17.960]   However, one important caveat here
[00:02:17.960 --> 00:02:22.040]   is that these are all examples of closed source RLHF models.
[00:02:22.040 --> 00:02:24.800]   We don't have access to the underlying model weights.
[00:02:24.800 --> 00:02:27.200]   We don't have access to the training data which
[00:02:27.200 --> 00:02:29.960]   was used to produce these models or the training framework that
[00:02:29.960 --> 00:02:32.160]   was used to produce these models for that matter.
[00:02:32.160 --> 00:02:35.400]   And as a result, it's very hard to reproduce them or even
[00:02:35.400 --> 00:02:39.280]   go about studying them in any relatively rigorous way.
[00:02:39.280 --> 00:02:43.120]   In order to do this, we need open source RLHF models.
[00:02:43.120 --> 00:02:45.200]   But the problem here is that really currently there
[00:02:45.200 --> 00:02:49.840]   are no good large scale open source RLHF models, yet
[00:02:49.840 --> 00:02:50.320]   at least.
[00:02:50.320 --> 00:02:53.480]   So at Carpre AI, I along with many others
[00:02:53.480 --> 00:02:56.520]   are working to produce one of the first open source RLHF
[00:02:56.520 --> 00:02:57.280]   models.
[00:02:57.280 --> 00:03:01.440]   We plan to open source the model itself as well as the training
[00:03:01.440 --> 00:03:05.280]   framework and the data that was used to train the model.
[00:03:05.280 --> 00:03:10.120]   And we anticipate this to be hugely impactful to a bunch
[00:03:10.120 --> 00:03:12.960]   of people, developers, researchers,
[00:03:12.960 --> 00:03:16.560]   even everyday folk who are interested in trying out
[00:03:16.560 --> 00:03:19.560]   some of these models, probably researchers and developers
[00:03:19.560 --> 00:03:20.480]   for the most part.
[00:03:20.480 --> 00:03:22.440]   So why does open source RLHF matter?
[00:03:22.440 --> 00:03:24.800]   I mean, I kind of already touched
[00:03:24.800 --> 00:03:26.960]   on some of these points, but I think
[00:03:26.960 --> 00:03:30.280]   there are many people who feel that equal access for all
[00:03:30.280 --> 00:03:33.200]   to this kind of potentially world changing technology
[00:03:33.200 --> 00:03:34.000]   is crucial.
[00:03:34.000 --> 00:03:37.720]   Carpre AI is fortunate enough to be supported by Stability AI,
[00:03:37.720 --> 00:03:41.040]   who provides compute for us to train these RLHF models.
[00:03:41.040 --> 00:03:44.720]   And democratization of AI is central to Stability's mission.
[00:03:44.720 --> 00:03:47.480]   Giving people access to this kind of technology
[00:03:47.480 --> 00:03:49.440]   is crucial, I believe, and should not
[00:03:49.440 --> 00:03:51.200]   be left in the hands of the few.
[00:03:51.200 --> 00:03:54.280]   But perhaps most importantly, in my belief,
[00:03:54.280 --> 00:03:58.560]   is closed source RLHF makes good science fundamentally
[00:03:58.560 --> 00:03:59.680]   impossible.
[00:03:59.680 --> 00:04:02.880]   If researchers don't have access to the underlying model weights
[00:04:02.880 --> 00:04:04.840]   or to the underlying data, then we really
[00:04:04.840 --> 00:04:07.760]   have no idea how these models were really trained.
[00:04:07.760 --> 00:04:10.720]   And so it is hard for us to draw meaningful comparisons
[00:04:10.720 --> 00:04:13.080]   or study these models in any rigorous way.
[00:04:13.080 --> 00:04:15.600]   And due to the huge impact these models have been having
[00:04:15.600 --> 00:04:18.440]   in media in general, it's very important
[00:04:18.440 --> 00:04:20.280]   to be able to study them and understand them
[00:04:20.280 --> 00:04:21.320]   in a deeper way.
[00:04:21.320 --> 00:04:23.320]   So blockers to open source RLHF.
[00:04:23.320 --> 00:04:25.160]   What makes this problem difficult?
[00:04:25.160 --> 00:04:28.280]   So first, number one, high quality supervised fine tuning
[00:04:28.280 --> 00:04:30.720]   data is generally difficult to find.
[00:04:30.720 --> 00:04:32.240]   There's kind of a step zero, which
[00:04:32.240 --> 00:04:35.760]   I didn't mention beforehand, before you collect preferences
[00:04:35.760 --> 00:04:37.880]   among various model outputs, which
[00:04:37.880 --> 00:04:43.400]   is you collect a bunch of high quality responses,
[00:04:43.400 --> 00:04:47.120]   kind of telling your model what you want it to roughly do.
[00:04:47.120 --> 00:04:49.000]   And then you fine tune it on these responses.
[00:04:49.000 --> 00:04:51.840]   And this kind of gets the model more in distribution.
[00:04:51.840 --> 00:04:53.880]   If you are from the RL domain and are
[00:04:53.880 --> 00:04:55.800]   familiar with the concept of behavior cloning,
[00:04:55.800 --> 00:04:57.840]   you can think of it as analogous to that.
[00:04:57.840 --> 00:05:01.040]   Number two, once you have a supervised fine tune model,
[00:05:01.040 --> 00:05:03.760]   you still need access to high quality preference data, which
[00:05:03.760 --> 00:05:07.880]   is you present various responses to users
[00:05:07.880 --> 00:05:10.480]   and then have the users rank or choose which responses are
[00:05:10.480 --> 00:05:11.560]   most preferable.
[00:05:11.560 --> 00:05:13.620]   This is also relatively expensive to collect.
[00:05:13.620 --> 00:05:15.920]   And there are not that many examples out there
[00:05:15.920 --> 00:05:16.700]   in the wild.
[00:05:16.700 --> 00:05:18.760]   Number three, and perhaps most underrated,
[00:05:18.760 --> 00:05:20.600]   you need a scalable training framework
[00:05:20.600 --> 00:05:23.040]   for language model fine tuning with real reinforcement
[00:05:23.040 --> 00:05:24.160]   learning.
[00:05:24.160 --> 00:05:26.280]   The dominant language model training paradigm
[00:05:26.280 --> 00:05:27.740]   over the last couple of years has
[00:05:27.740 --> 00:05:32.760]   been a supervised objective, or semi-supervised objective,
[00:05:32.760 --> 00:05:36.160]   such as next token prediction, where
[00:05:36.160 --> 00:05:38.600]   you don't have to query an outside reward
[00:05:38.600 --> 00:05:40.200]   model or something like that.
[00:05:40.200 --> 00:05:43.000]   And so for particularly training models at scale,
[00:05:43.000 --> 00:05:46.080]   this makes adapting these scaling frameworks
[00:05:46.080 --> 00:05:48.120]   difficult to the RL domain, since you
[00:05:48.120 --> 00:05:50.400]   have many more moving parts of reinforcement learning.
[00:05:50.400 --> 00:05:52.040]   You need to host the reward model.
[00:05:52.040 --> 00:05:53.600]   You need to host the student model.
[00:05:53.600 --> 00:05:56.280]   And for online RL, you need to host other things as well.
[00:05:56.280 --> 00:05:58.720]   And this makes the compute and memory requirements
[00:05:58.720 --> 00:06:00.680]   that much more difficult to satisfy,
[00:06:00.680 --> 00:06:03.800]   which were already difficult for 70 billion parameter models.
[00:06:03.800 --> 00:06:05.760]   So where does TRLx come in?
[00:06:05.760 --> 00:06:09.080]   So TRLx addresses number three, a scalable RL training
[00:06:09.080 --> 00:06:09.880]   framework.
[00:06:09.880 --> 00:06:12.160]   This was a training framework originally developed
[00:06:12.160 --> 00:06:15.160]   by myself and a couple others at Kupfer AI.
[00:06:15.160 --> 00:06:19.760]   And as of right now, it is the leader in open source RLHF
[00:06:19.760 --> 00:06:21.280]   training at scale.
[00:06:21.280 --> 00:06:23.000]   It's an actively developed library, which
[00:06:23.000 --> 00:06:24.400]   gets regular updates every week.
[00:06:24.400 --> 00:06:27.920]   And we hope for this repo to continue
[00:06:27.920 --> 00:06:32.680]   to be pushing forward open source RLHF training at scale.
[00:06:32.680 --> 00:06:34.300]   And we're excited to see where it goes.
[00:06:34.300 --> 00:06:35.920]   So what are some of the capabilities
[00:06:35.920 --> 00:06:37.400]   that TRLx supports?
[00:06:37.400 --> 00:06:38.980]   When we were designing the library,
[00:06:38.980 --> 00:06:40.920]   we were cognizant of the fact that we
[00:06:40.920 --> 00:06:43.720]   would have users with a wide range of various user
[00:06:43.720 --> 00:06:45.200]   profiles using it.
[00:06:45.200 --> 00:06:47.960]   And so we tried to build in various training frameworks
[00:06:47.960 --> 00:06:50.080]   which accommodate each of these users separately.
[00:06:50.080 --> 00:06:52.280]   These are roughly divided into three cases.
[00:06:52.280 --> 00:06:54.480]   So first of all, you have your independent researchers
[00:06:54.480 --> 00:06:56.680]   with access to maybe a single GPU.
[00:06:56.680 --> 00:06:59.840]   And these can train up to about a billion parameter model
[00:06:59.840 --> 00:07:00.880]   using native pipeline.
[00:07:00.880 --> 00:07:02.440]   But this is still pretty decent.
[00:07:02.440 --> 00:07:04.040]   You can get pretty good RLHF results
[00:07:04.040 --> 00:07:05.560]   with a one billion parameter model.
[00:07:05.560 --> 00:07:08.420]   If you have some more resources, so maybe multiple GPUs
[00:07:08.420 --> 00:07:10.480]   on a single node, you can consider
[00:07:10.480 --> 00:07:13.320]   training using Hugging Face Accelerate integration
[00:07:13.320 --> 00:07:15.880]   plus Deep Speed, which allows you to train about up
[00:07:15.880 --> 00:07:17.400]   to 20 billion parameter models.
[00:07:17.400 --> 00:07:18.880]   And this is quite good.
[00:07:18.880 --> 00:07:20.840]   If you are a particularly well-endowed user
[00:07:20.840 --> 00:07:24.160]   with multiple nodes of eight 8.1 hundreds or something,
[00:07:24.160 --> 00:07:26.680]   then we have tested our integration
[00:07:26.680 --> 00:07:28.960]   with the NVIDIA NEMO training framework,
[00:07:28.960 --> 00:07:31.760]   fine tuning models up to 70 billion parameters.
[00:07:31.760 --> 00:07:34.440]   And this is competitive with the size of models
[00:07:34.440 --> 00:07:37.640]   you see in many of these Anthropx papers, for example.
[00:07:37.640 --> 00:07:39.720]   And we're excited to see where this takes us.
[00:07:39.720 --> 00:07:42.000]   So what are some features that TRLx supports?
[00:07:42.000 --> 00:07:43.360]   We have a good number.
[00:07:43.360 --> 00:07:45.680]   So first of all, we're compatible with most encoder
[00:07:45.680 --> 00:07:49.600]   decoders, such as T5 model, and decoder only models,
[00:07:49.600 --> 00:07:52.880]   such as DPT NeoX, which are supported on Hugging Face.
[00:07:52.880 --> 00:07:55.000]   This makes loading and saving models
[00:07:55.000 --> 00:07:56.360]   relatively easy and efficient.
[00:07:56.360 --> 00:07:58.680]   Also, like I said, we were cognizant of the fact
[00:07:58.680 --> 00:08:01.160]   that we want this library to be accessible
[00:08:01.160 --> 00:08:03.880]   to many different users and use cases,
[00:08:03.880 --> 00:08:05.480]   particularly independent researchers
[00:08:05.480 --> 00:08:07.280]   who have limited resources.
[00:08:07.280 --> 00:08:10.240]   So we've incorporated several memory saving/compute
[00:08:10.240 --> 00:08:13.760]   efficient features, including low rank adaptation, or LoRa,
[00:08:13.760 --> 00:08:15.800]   8-bit atom, layer freezing, and Hydra.
[00:08:15.800 --> 00:08:17.680]   And we'll go into these in more detail later.
[00:08:17.680 --> 00:08:19.240]   But in general, you can think of these
[00:08:19.240 --> 00:08:24.160]   as reducing the memory requirements to do RLHF.
[00:08:24.160 --> 00:08:27.920]   We've also incorporated various online and offline reinforcement
[00:08:27.920 --> 00:08:30.360]   learning algorithms, which have their various trade-offs
[00:08:30.360 --> 00:08:32.520]   and benefits, which I'll talk about later.
[00:08:32.520 --> 00:08:37.040]   And perhaps most importantly, we have support for multi-GPU
[00:08:37.040 --> 00:08:39.120]   hyperparameter sweeps, which are very
[00:08:39.120 --> 00:08:41.040]   important for reinforcement learning, which
[00:08:41.040 --> 00:08:43.920]   are integrated with weights and biases, which
[00:08:43.920 --> 00:08:45.240]   we use for experiment tracking.
[00:08:45.240 --> 00:08:46.360]   And this is very useful.
[00:08:46.360 --> 00:08:47.760]   We use it in the future every day.
[00:08:47.760 --> 00:08:50.080]   Additionally, we have 10-plus built-in examples
[00:08:50.080 --> 00:08:53.280]   of varying complexity, which users can run and get started
[00:08:53.280 --> 00:08:55.000]   with and start building their own.
[00:08:55.000 --> 00:08:57.280]   So let's dive in a little more and look
[00:08:57.280 --> 00:08:59.280]   at the anatomy of language model fine-tuning
[00:08:59.280 --> 00:09:02.160]   via online reinforcement learning at scale.
[00:09:02.160 --> 00:09:04.720]   So this is kind of a complex process.
[00:09:04.720 --> 00:09:07.840]   And at training time, you have many different models
[00:09:07.840 --> 00:09:09.600]   which you need to hold in memory.
[00:09:09.600 --> 00:09:12.040]   So first, you need the model that you're fine-tuning,
[00:09:12.040 --> 00:09:15.240]   the student model, which you can think of as your policy.
[00:09:15.240 --> 00:09:17.760]   And you also need the value head of this policy,
[00:09:17.760 --> 00:09:20.960]   which is used for proximal policy optimization, or PPO.
[00:09:20.960 --> 00:09:23.460]   And if you're not familiar, you can think of this value head
[00:09:23.460 --> 00:09:26.720]   as kind of trying to learn and approximate the reward
[00:09:26.720 --> 00:09:29.280]   that a given response will generate.
[00:09:29.280 --> 00:09:31.640]   Additionally, we have this reference model,
[00:09:31.640 --> 00:09:33.560]   frozen reference model, which is used
[00:09:33.560 --> 00:09:37.480]   to kind of produce some kind of KL control penalty,
[00:09:37.480 --> 00:09:39.200]   enforcing the student model.
[00:09:39.200 --> 00:09:42.500]   It doesn't kind of overfit to the reward too hard.
[00:09:42.500 --> 00:09:44.760]   And additionally, we need to hold in memory the reward
[00:09:44.760 --> 00:09:47.320]   models themselves, which can be quite large.
[00:09:47.320 --> 00:09:49.880]   And so altogether, if you were to treat each of these models
[00:09:49.880 --> 00:09:52.600]   as separate, a value network separate from a student
[00:09:52.600 --> 00:09:54.480]   network, separate from a reference network,
[00:09:54.480 --> 00:09:57.200]   separate from a reward network, that's a high bar.
[00:09:57.200 --> 00:10:00.040]   It requires a lot of compute and a lot of memory.
[00:10:00.040 --> 00:10:03.620]   However, TROx leverages a recent architecture
[00:10:03.620 --> 00:10:06.480]   introduced by Sparrow, DeepMind's paper Sparrow,
[00:10:06.480 --> 00:10:09.080]   called the Hydra architecture, which fuses together
[00:10:09.080 --> 00:10:11.880]   all these models into kind of one shared
[00:10:11.880 --> 00:10:13.520]   trunk with shared layers.
[00:10:13.520 --> 00:10:15.060]   And this is made feasible by the fact
[00:10:15.060 --> 00:10:18.880]   that most of these models have their layers frozen already.
[00:10:18.880 --> 00:10:20.640]   For example, the reference model is frozen.
[00:10:20.640 --> 00:10:21.960]   It's not being optimized.
[00:10:21.960 --> 00:10:23.220]   The reward models are frozen.
[00:10:23.220 --> 00:10:24.500]   They're just being inferenced.
[00:10:24.500 --> 00:10:26.460]   Really, it's just the student that we're training.
[00:10:26.460 --> 00:10:27.920]   And in many cases, it doesn't even
[00:10:27.920 --> 00:10:30.720]   make sense to train the majority of the student anyway.
[00:10:30.720 --> 00:10:32.160]   We want to freeze most of the layers
[00:10:32.160 --> 00:10:33.480]   to provide more stability.
[00:10:33.480 --> 00:10:35.240]   And we find that in some cases, this
[00:10:35.240 --> 00:10:38.720]   can save up to 75% of the memory required otherwise
[00:10:38.720 --> 00:10:40.880]   if we were to treat these all in separate networks.
[00:10:40.880 --> 00:10:43.280]   And this really makes it more accessible to all
[00:10:43.280 --> 00:10:44.360]   our different user groups.
[00:10:44.360 --> 00:10:46.080]   And we definitely recommend this feature.
[00:10:46.080 --> 00:10:48.920]   To download the TROx library, let's do a warm up.
[00:10:48.920 --> 00:10:51.560]   Let's say we're fine tuning GPTJ,
[00:10:51.560 --> 00:10:53.320]   which is a 6 billion parameter model,
[00:10:53.320 --> 00:10:55.840]   to produce positive movie reviews.
[00:10:55.840 --> 00:10:57.120]   How would we do this?
[00:10:57.120 --> 00:10:59.040]   Well, we're going to use a reward model, which
[00:10:59.040 --> 00:11:02.080]   is Distobird IMDB, 55 million parameters.
[00:11:02.080 --> 00:11:04.200]   And it's been trained to classify movie reviews
[00:11:04.200 --> 00:11:06.120]   into positive and negative sentiment.
[00:11:06.120 --> 00:11:08.520]   And then we're going to use proximal policy optimization,
[00:11:08.520 --> 00:11:11.080]   or PPO, which is an online RL algorithm.
[00:11:11.080 --> 00:11:16.280]   The environment we'll be training in is using 40GB 8A100s.
[00:11:16.280 --> 00:11:18.600]   And we'll be using DSP 0.2 as our trainer.
[00:11:18.600 --> 00:11:19.960]   And what are the results?
[00:11:19.960 --> 00:11:22.000]   So we see that things look pretty good.
[00:11:22.000 --> 00:11:24.160]   After a couple hundred steps, the reward that we get
[00:11:24.160 --> 00:11:25.000]   is quite high.
[00:11:25.000 --> 00:11:28.360]   Note the reward from Distobird is between 0 and 1.
[00:11:28.360 --> 00:11:30.920]   It is effectively just calculating the probability
[00:11:30.920 --> 00:11:33.120]   that the result of text is positive or negative.
[00:11:33.120 --> 00:11:35.960]   And yeah, the reward looks stable.
[00:11:35.960 --> 00:11:37.680]   Things increase quickly.
[00:11:37.680 --> 00:11:39.480]   Life is good.
[00:11:39.480 --> 00:11:41.320]   Now, drawing your attention to the graphs
[00:11:41.320 --> 00:11:44.920]   on the right-hand side, we have the top, which is explore time.
[00:11:44.920 --> 00:11:47.160]   And we have the bottom, which is backward time.
[00:11:47.160 --> 00:11:49.680]   And these represent the time that the algorithm
[00:11:49.680 --> 00:11:55.000]   takes to do the two main components of the reinforcement
[00:11:55.000 --> 00:11:57.360]   learning training loop with language model.
[00:11:57.360 --> 00:12:00.360]   Explore time represents the time it takes to generate rollouts
[00:12:00.360 --> 00:12:03.920]   and allow the model to generate new experiences, which
[00:12:03.920 --> 00:12:07.560]   are then stored in a replay buffer and used at train time.
[00:12:07.560 --> 00:12:09.560]   And then we see that this takes a while.
[00:12:09.560 --> 00:12:11.240]   I mean, a 6-billion-per-emeter model
[00:12:11.240 --> 00:12:14.520]   is relatively expensive to infer, right?
[00:12:14.520 --> 00:12:16.280]   And we're trying to generate a story, which
[00:12:16.280 --> 00:12:20.640]   means it needs to be inferred up to 40 times for each mover
[00:12:20.640 --> 00:12:21.440]   review.
[00:12:21.440 --> 00:12:22.320]   And this is expensive.
[00:12:22.320 --> 00:12:23.680]   It can take up to three seconds.
[00:12:23.680 --> 00:12:25.960]   And it's definitely the bottleneck of the computation.
[00:12:25.960 --> 00:12:28.800]   So reducing or eliminating entirely the rollout time
[00:12:28.800 --> 00:12:30.920]   is very ideal if we're trying to speed up training.
[00:12:30.920 --> 00:12:32.640]   Comparing this to the amount of time
[00:12:32.640 --> 00:12:34.760]   it takes to do a backward pass on the model,
[00:12:34.760 --> 00:12:38.200]   and we see that the backward time is a fraction of the time
[00:12:38.200 --> 00:12:39.320]   that rollout takes.
[00:12:39.320 --> 00:12:41.280]   So optimizing rollout is quite ideal.
[00:12:41.280 --> 00:12:44.000]   This slide is just to allow you to qualitatively evaluate
[00:12:44.000 --> 00:12:47.280]   the results of the optimization.
[00:12:47.280 --> 00:12:49.480]   The reward that you see on the right, like I said,
[00:12:49.480 --> 00:12:50.680]   is between 0 and 1.
[00:12:50.680 --> 00:12:53.560]   So a reward of 0.99 is basically as good as you can get.
[00:12:53.560 --> 00:12:55.280]   So pause here if you want to examine some
[00:12:55.280 --> 00:12:56.760]   of the qualitative results.
[00:12:56.760 --> 00:12:59.200]   So some takeaways from fine-tuning for Sonsman.
[00:12:59.200 --> 00:13:01.840]   Pros-- relatively sample efficient.
[00:13:01.840 --> 00:13:05.960]   Like I said, you see that reward goes up pretty fast.
[00:13:05.960 --> 00:13:07.360]   Also, the reward is stable, which
[00:13:07.360 --> 00:13:10.400]   means that reward is going up reliably.
[00:13:10.400 --> 00:13:13.760]   You don't see it fluctuating up and down stochastically.
[00:13:13.760 --> 00:13:18.480]   But cons-- so online RL is pretty difficult to scale.
[00:13:18.480 --> 00:13:21.960]   You have to have several different copies of the model
[00:13:21.960 --> 00:13:22.960]   in memory at times.
[00:13:22.960 --> 00:13:24.120]   You need the value network.
[00:13:24.120 --> 00:13:25.080]   You need the policy.
[00:13:25.080 --> 00:13:26.440]   You need the reference model.
[00:13:26.440 --> 00:13:28.040]   And you need the reward model.
[00:13:28.040 --> 00:13:30.560]   And this is somewhat alleviated by the Hydra architecture,
[00:13:30.560 --> 00:13:32.360]   like I said, but still expensive.
[00:13:32.360 --> 00:13:35.760]   And then it's also somewhat prone to reward overfitting.
[00:13:35.760 --> 00:13:37.400]   And what I mean when I say this is,
[00:13:37.400 --> 00:13:39.320]   if you look at a couple of these samples here,
[00:13:39.320 --> 00:13:42.200]   you notice that just even of these six samples,
[00:13:42.200 --> 00:13:45.480]   the first generated token is about music.
[00:13:45.480 --> 00:13:47.480]   And this is kind of representative
[00:13:47.480 --> 00:13:49.240]   of an underlying phenomenon you will see,
[00:13:49.240 --> 00:13:52.000]   which is the more you optimize against a reward,
[00:13:52.000 --> 00:13:54.680]   the more overfit you will become and the less diversity
[00:13:54.680 --> 00:13:56.360]   of generated text you will see.
[00:13:56.360 --> 00:13:59.480]   And this is particularly bad with online algorithms
[00:13:59.480 --> 00:14:01.120]   versus offline algorithms.
[00:14:01.120 --> 00:14:03.080]   I would say it's a significant drawback.
[00:14:03.080 --> 00:14:06.160]   So you want your reward to be pretty high so that you know
[00:14:06.160 --> 00:14:07.800]   you've done a good job optimizing.
[00:14:07.800 --> 00:14:10.280]   But you don't want your reward to be so high that it's obvious
[00:14:10.280 --> 00:14:11.200]   that you've overfitted.
[00:14:11.200 --> 00:14:13.920]   And the resulting text is obviously wrong,
[00:14:13.920 --> 00:14:15.880]   but rated highly by the reward model
[00:14:15.880 --> 00:14:19.080]   because the reward model is an imperfect representation
[00:14:19.080 --> 00:14:21.160]   of the human preferences it's trying to capture.
[00:14:21.160 --> 00:14:23.760]   So now with that in mind, let's take a look
[00:14:23.760 --> 00:14:25.560]   at a more advanced case study where
[00:14:25.560 --> 00:14:29.040]   we are training a 20 billion parameter model, GPT-NeoX,
[00:14:29.040 --> 00:14:32.200]   to generate summarizations of some Reddit stories.
[00:14:32.200 --> 00:14:35.200]   The reward model will be a fine-tuned GPT-J.
[00:14:35.200 --> 00:14:37.920]   And we'll be using instead an offline algorithm here,
[00:14:37.920 --> 00:14:39.920]   which is implicit language queue learning.
[00:14:39.920 --> 00:14:43.760]   The environment we're in will be 40 gigabyte A100s again.
[00:14:43.760 --> 00:14:46.040]   And the trainer we'll be using is DeepSpeed02.
[00:14:46.040 --> 00:14:48.600]   Well, we see that the reward that we compute
[00:14:48.600 --> 00:14:49.600]   goes up pretty quickly.
[00:14:49.600 --> 00:14:51.120]   Looks like training is still stable.
[00:14:51.120 --> 00:14:55.280]   And we converge to a reward of about 2.5.
[00:14:55.280 --> 00:14:58.240]   And perhaps more notably, the training time
[00:14:58.240 --> 00:15:00.920]   which it takes to do a full forward-backward pass
[00:15:00.920 --> 00:15:04.160]   is only three seconds, which is, when you compare it
[00:15:04.160 --> 00:15:06.400]   to the amount of time it takes to generate rollouts
[00:15:06.400 --> 00:15:08.840]   in the online case, very competitive,
[00:15:08.840 --> 00:15:10.400]   especially when you consider the fact
[00:15:10.400 --> 00:15:12.920]   that the model we're fine-tuning here
[00:15:12.920 --> 00:15:15.320]   is three times the size of the student
[00:15:15.320 --> 00:15:18.680]   model in the sentiments case and with a much larger context.
[00:15:18.680 --> 00:15:20.360]   And I just want to point out here
[00:15:20.360 --> 00:15:22.320]   that because we're doing offline RL,
[00:15:22.320 --> 00:15:24.120]   no rollouts are required, which is
[00:15:24.120 --> 00:15:26.680]   a component of the fine-tuning procedure, which takes--
[00:15:26.680 --> 00:15:27.520]   it's the bottleneck.
[00:15:27.520 --> 00:15:29.400]   It takes the most amount of time.
[00:15:29.400 --> 00:15:32.360]   And so because we've managed to eliminate that bottleneck,
[00:15:32.360 --> 00:15:34.400]   we find that offline reinforcement learning
[00:15:34.400 --> 00:15:38.320]   is much more efficient in terms of compute and walk-on time.
[00:15:38.320 --> 00:15:42.920]   And I let you survey some qualitative results here.
[00:15:42.920 --> 00:15:44.380]   I'm not going to read you the story,
[00:15:44.380 --> 00:15:46.160]   but you can pause if you want to.
[00:15:46.160 --> 00:15:47.680]   And if you look at the summary, I'd
[00:15:47.680 --> 00:15:49.680]   say the summary is pretty decent.
[00:15:49.680 --> 00:15:51.480]   In order to properly evaluate these models,
[00:15:51.480 --> 00:15:53.520]   you would need to deploy them to users,
[00:15:53.520 --> 00:15:58.240]   have them rank their responses, and then compute win rates.
[00:15:58.240 --> 00:16:00.520]   But just looking at the reward here
[00:16:00.520 --> 00:16:03.480]   and examining, doing a qualitative analysis
[00:16:03.480 --> 00:16:04.480]   is pretty good as well.
[00:16:04.480 --> 00:16:06.760]   And it lets you get a feel for how well these models were
[00:16:06.760 --> 00:16:07.400]   performing.
[00:16:07.400 --> 00:16:10.160]   So some takeaways for the second fine-tuning
[00:16:10.160 --> 00:16:11.760]   for summarization example.
[00:16:11.760 --> 00:16:15.280]   Pros-- relative to online RL, offline RL
[00:16:15.280 --> 00:16:16.600]   is compute efficient.
[00:16:16.600 --> 00:16:18.560]   Training time is much faster.
[00:16:18.560 --> 00:16:22.440]   It's easier to scale, meaning that the offline RL
[00:16:22.440 --> 00:16:24.720]   objective and training framework is much more
[00:16:24.720 --> 00:16:28.320]   similar to the kind of training that you see doing supervised
[00:16:28.320 --> 00:16:28.920]   fine-tuning.
[00:16:28.920 --> 00:16:34.840]   So it's much easier to implement code-wise and architecture-wise
[00:16:34.840 --> 00:16:36.200]   than online.
[00:16:36.200 --> 00:16:38.280]   And it's also relatively robust to overfitting,
[00:16:38.280 --> 00:16:40.560]   because we're doing it offline and we're not actively
[00:16:40.560 --> 00:16:43.280]   interacting with the reward model.
[00:16:43.280 --> 00:16:45.000]   However, cons are, if you're really
[00:16:45.000 --> 00:16:47.760]   trying to get a high reward and optimize your reward model
[00:16:47.760 --> 00:16:51.120]   as far as you can, online is going to do better than offline.
[00:16:51.120 --> 00:16:52.840]   And to decide between the two, you're
[00:16:52.840 --> 00:16:54.400]   just going to have to judge yourself
[00:16:54.400 --> 00:16:55.600]   and mix and match, perhaps.
[00:16:55.600 --> 00:16:57.360]   So kind of summarizing the takeaways here,
[00:16:57.360 --> 00:16:58.920]   we have this nice little table.
[00:16:58.920 --> 00:17:01.440]   On the left, PPO and IoQL.
[00:17:01.440 --> 00:17:03.760]   And then on the top, we have ease to scale.
[00:17:03.760 --> 00:17:05.520]   One is better, one is not.
[00:17:05.520 --> 00:17:07.840]   Compute efficiency-- offline is a little better,
[00:17:07.840 --> 00:17:09.200]   online is a little worse.
[00:17:09.200 --> 00:17:11.200]   Robustness to overfitting-- we have observed
[00:17:11.200 --> 00:17:14.040]   online is a little worse, offline is a little better.
[00:17:14.040 --> 00:17:15.960]   And then really maximizing the reward.
[00:17:15.960 --> 00:17:18.280]   So in this case, online does pretty well,
[00:17:18.280 --> 00:17:19.880]   offline not quite as good.
[00:17:19.880 --> 00:17:21.960]   I do want to say here, though, that these are just
[00:17:21.960 --> 00:17:23.160]   the results of our study.
[00:17:23.160 --> 00:17:26.000]   And this isn't very much so an active area of research.
[00:17:26.000 --> 00:17:29.240]   So these could be domain-specific phenomena
[00:17:29.240 --> 00:17:30.320]   which we are discovering.
[00:17:30.320 --> 00:17:32.680]   And so I would definitely recommend
[00:17:32.680 --> 00:17:34.360]   doing your own experiments as well to get
[00:17:34.360 --> 00:17:35.480]   a sense of what works well.
[00:17:35.480 --> 00:17:40.160]   So in conclusion, TRLx provides a scalable, flexible framework
[00:17:40.160 --> 00:17:42.880]   for training models up to 70 billion parameters
[00:17:42.880 --> 00:17:47.600]   with reinforcement learning, an online and an offline trainer
[00:17:47.600 --> 00:17:50.960]   for various use cases and different scenarios.
[00:17:50.960 --> 00:17:53.040]   10-plus examples of varying complexity,
[00:17:53.040 --> 00:17:55.080]   ranging from beginner to research level,
[00:17:55.080 --> 00:17:58.360]   which users can get started with and start plugging in.
[00:17:58.360 --> 00:18:00.800]   And finally, integration with weights and biases
[00:18:00.800 --> 00:18:03.320]   for experiment tracking and hyperparameter sweeps,
[00:18:03.320 --> 00:18:04.920]   which I, again, want to emphasize,
[00:18:04.920 --> 00:18:06.760]   are crucial for reinforcement learning.
[00:18:06.760 --> 00:18:08.720]   Hyperparameters are quite finicky.
[00:18:08.720 --> 00:18:10.600]   And to get, really, the maximum performance,
[00:18:10.600 --> 00:18:14.240]   you need to do a thorough search space of your hyperparameters.
[00:18:14.240 --> 00:18:17.280]   [MUSIC PLAYING]
[00:18:17.280 --> 00:18:19.860]   (upbeat music)
[00:18:19.860 --> 00:18:22.620]   (logo whooshing)

