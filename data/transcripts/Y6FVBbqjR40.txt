
[00:00:00.000 --> 00:00:02.000]   *Gasp*
[00:00:02.000 --> 00:00:04.000]   *Gasp*
[00:00:04.000 --> 00:00:09.500]   Welcome to this new video where we're gonna build a robot and train it in the real world using deep reinforcement learning.
[00:00:09.500 --> 00:00:14.000]   I've been wanting to learn more about deep reinforcement learning for a while now so that's the perfect opportunity.
[00:00:14.000 --> 00:00:18.500]   The first thing many reinforcement learning tutorials make you do is solve the cartpole environment.
[00:00:18.500 --> 00:00:23.500]   The task is simple, the agent can move the cart left or right and it has to balance the pole.
[00:00:23.500 --> 00:00:28.500]   We're gonna build a rotary robot version of the cartpole which is also known as a Furutappendulum.
[00:00:28.500 --> 00:00:30.500]   That's my friend Pierre also balancing a pole.
[00:00:30.500 --> 00:00:34.000]   We gave ourselves the challenge to build a robot under 48 hours.
[00:00:34.000 --> 00:00:38.000]   We spent about one full day in advance carefully selecting components for the robot.
[00:00:38.000 --> 00:00:42.500]   We were not able to find a lot of documentation on these kinds of robots so we made a lot of guesses.
[00:00:42.500 --> 00:00:45.000]   And at this point we had no idea if any of this would even work.
[00:00:45.000 --> 00:00:47.000]   But we took the bet and ordered the parts.
[00:00:47.000 --> 00:00:52.500]   One goal of this project is for it to be reproducible so you'll find a bill of materials and build instructions on GitHub
[00:00:52.500 --> 00:00:58.500]   along with the hyperparameters for each run, some pre-trained models and any note from the project in Waste & Biases.
[00:00:58.500 --> 00:01:02.500]   We started by unboxing the electronics and making sure that everything was working.
[00:01:02.500 --> 00:01:08.000]   It took us way too long to understand the documentation so we were quite excited when we finally got value from our sensors
[00:01:08.000 --> 00:01:10.000]   and managed to get the motor to spin.
[00:01:10.000 --> 00:01:18.000]   Then it was time to work on the mechanical aspect of the robot.
[00:01:18.000 --> 00:01:23.500]   We chose to go with 3D printing for most of the parts so that the build wouldn't require too many or too expensive tools.
[00:01:24.500 --> 00:01:27.500]   [Music]
[00:01:49.500 --> 00:01:52.500]   After a few iterations, here's what we got.
[00:01:52.500 --> 00:01:54.000]   The final robot is quite simple.
[00:01:54.000 --> 00:01:58.000]   This is the motor, it has an angle sensor attached to it and it is controlled by this board.
[00:01:58.000 --> 00:02:00.000]   Then this is the pendulum angle sensor.
[00:02:00.000 --> 00:02:04.000]   Both the motor and the pendulum sensors are read by these two counter chips.
[00:02:04.000 --> 00:02:08.000]   And finally everything is plugged into a Jetson Nano which acts as the robot brain.
[00:02:08.000 --> 00:02:11.000]   At this point we needed telemetry to display our sensor value over time
[00:02:11.000 --> 00:02:14.000]   and understand if everything inside the robot was working as expected.
[00:02:14.000 --> 00:02:16.500]   So we used Waste & Biases' experiment tracking.
[00:02:16.500 --> 00:02:20.000]   Experiment tracking lets you log metrics such as your model loss and accuracy.
[00:02:20.000 --> 00:02:23.000]   This way you don't run the risk of losing insights and progress.
[00:02:23.000 --> 00:02:26.000]   What's nice is that we can use this tool to log anything.
[00:02:26.000 --> 00:02:30.000]   So we logged our robot state which got us a nice telemetry dashboard with no setup.
[00:02:30.000 --> 00:02:33.000]   One detail I really like is the custom expression feature.
[00:02:33.000 --> 00:02:36.000]   It lets you create a new metric based on what you log.
[00:02:36.000 --> 00:02:42.000]   In our case the pendulum and motor angles are in radians which is useful for computation but not so much for debugging.
[00:02:42.000 --> 00:02:45.000]   Using expressions I can easily convert radians to degrees like so.
[00:02:45.500 --> 00:02:54.500]   [Construction sounds]
[00:02:54.500 --> 00:02:58.500]   With everything working it was time to move on to the software.
[00:02:58.500 --> 00:03:02.500]   One talk I watched suggested using a continuity cost to incentivize a smooth policy.
[00:03:02.500 --> 00:03:05.500]   I wanted to try it out to gauge if it would make sense for my robot.
[00:03:05.500 --> 00:03:09.500]   To try out different combinations of those parameters I used Waste & Biases' SWIPS.
[00:03:09.500 --> 00:03:11.500]   SWIPS is a hyperparameter tuning tool.
[00:03:11.500 --> 00:03:15.000]   We usually use it to fine tune our learning rate, model architecture and such.
[00:03:15.000 --> 00:03:18.000]   But I used it to run a grid search over my environment parameters.
[00:03:18.000 --> 00:03:20.000]   Let's look at the results.
[00:03:20.000 --> 00:03:24.000]   Ok so let's start by looking at the run where the continuity cost was false.
[00:03:24.000 --> 00:03:29.000]   By looking at the videos it seems like a lot of these runs converge to a policy that's kind of jittery.
[00:03:29.000 --> 00:03:33.000]   So let's look at the run where the continuity cost was true to see if there is a difference.
[00:03:33.000 --> 00:03:36.000]   And indeed there seems to be a difference.
[00:03:36.000 --> 00:03:39.000]   Most of those look more smooth than the previous ones.
[00:03:39.000 --> 00:03:41.500]   So I think that's worth trying on the robot.
[00:03:41.500 --> 00:03:45.500]   With everything working in simulation I moved on to the robot to start training.
[00:03:45.500 --> 00:03:53.500]   And it didn't work and the robot looked very angry.
[00:03:53.500 --> 00:03:56.500]   At this point I still had no idea if this could even work.
[00:03:56.500 --> 00:04:01.500]   Maybe the mechanical assembly wasn't good enough or the angle sensor not precise enough.
[00:04:01.500 --> 00:04:04.500]   Maybe there were delays in the system I didn't know about.
[00:04:04.500 --> 00:04:06.500]   That's where Waste & Biases came in handy.
[00:04:06.500 --> 00:04:09.500]   I was clueless but I had been logging everything from the start.
[00:04:09.500 --> 00:04:12.500]   Model weights, training runs, experiment notes, code.
[00:04:12.500 --> 00:04:16.500]   This meant I was able to ask my team and nice people on the internet for advice
[00:04:16.500 --> 00:04:20.500]   while providing rich context around the project and current issues.
[00:04:20.500 --> 00:04:26.000]   Waste & Biases Teams makes it easy to share insight, collaborate on experiments or in my case ask for help.
[00:04:26.000 --> 00:04:29.500]   Alternatively you can also make a solo project public to easily share runs.
[00:04:29.500 --> 00:04:33.500]   So I asked questions in a Discord server linking to my Waste & Biases run.
[00:04:33.500 --> 00:04:35.000]   And this answer got me on stock.
[00:04:35.000 --> 00:04:37.000]   It turns out the issue was the exploration noise.
[00:04:37.000 --> 00:04:41.500]   During the exploration phase we have the agent taking random actions by sending Gaussian noise to the motor.
[00:04:41.500 --> 00:04:45.000]   And while this works well in simulation, it's more complicated in the real world.
[00:04:45.000 --> 00:04:48.500]   With Gaussian noise the action can go from full torque to the right at one time step
[00:04:48.500 --> 00:04:51.500]   to full torque to the left one fraction of a second later.
[00:04:51.500 --> 00:04:53.500]   It's kind of like reversing gear on the highway.
[00:04:53.500 --> 00:04:55.000]   The robot can't execute the action.
[00:04:55.000 --> 00:04:59.500]   In effect this makes the system act as a low pass filter and reenters the exploration ineffective.
[00:04:59.500 --> 00:05:02.500]   The solution is to use generalized state-dependent exploration
[00:05:02.500 --> 00:05:05.500]   which derives the exploration action from the current system state.
[00:05:05.500 --> 00:05:07.500]   This makes the action less volatile.
[00:05:07.500 --> 00:05:11.000]   Thankfully GSD is implemented in stable baseline 3.
[00:05:11.000 --> 00:05:13.000]   So this was as simple as changing a parameter.
[00:05:13.000 --> 00:05:15.000]   So I started training again.
[00:05:16.000 --> 00:05:20.000]   [Music]
[00:05:21.000 --> 00:05:24.000]   [Music]
[00:05:24.000 --> 00:05:53.880]   This is it for this video, i hope you enjoyed it.
[00:05:53.880 --> 00:05:57.400]   I'm grateful that i got to work on this as my internship project, this was really fun.
[00:05:57.400 --> 00:06:00.800]   If you wanna learn more about reinforcement learning, i recommend Costa's video about
[00:06:00.800 --> 00:06:06.680]   PPO and Antonin Raphain's talk at RLVS 2021 about reinforcement learning in the real world.
[00:06:06.680 --> 00:06:09.800]   Don't forget to subscribe for more interviews, tutorials and talks.
[00:06:09.800 --> 00:06:10.560]   Thanks for watching!
[00:06:10.800 --> 00:06:22.400]   [Music]

