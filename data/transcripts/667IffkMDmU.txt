
[00:00:00.000 --> 00:00:05.040]   smart people really underrate espionage, right? And, you know, I think part of the security issue
[00:00:05.040 --> 00:00:08.880]   is I think people don't realize like how intense state level espionage can be, right? Like,
[00:00:08.880 --> 00:00:13.840]   you know, you know, this Israeli company had had software that could just zero click hack any
[00:00:13.840 --> 00:00:17.440]   iPhone, right? They just put in your number, and then it's just like straight download of everything,
[00:00:17.440 --> 00:00:22.320]   right? Like, the United States infiltrated an air gaffed atomic weapons program, right? Wild,
[00:00:22.320 --> 00:00:27.920]   you know, like, yeah, you know, the, you know, you know, intelligence agencies have just stockpiles
[00:00:27.920 --> 00:00:32.240]   of zero days, you know, when things get really hot, you know, I don't know, maybe we'll send
[00:00:32.240 --> 00:00:35.520]   special forces, right? To like, you know, get go to the data center or something that's, you know,
[00:00:35.520 --> 00:00:39.680]   or, you know, I mean, China does this, they threaten people's families, right? And they're
[00:00:39.680 --> 00:00:45.440]   like, look, if you don't cooperate, if you don't give us the Intel, there's a good book, you know,
[00:00:45.440 --> 00:00:51.760]   along the lines of the Gulag, you know, the inside the aquarium, which is by a Soviet GRU defector,
[00:00:51.760 --> 00:00:56.240]   GRU was like military intelligence, Ilya recommended this book to me, you know,
[00:00:56.240 --> 00:00:59.280]   I think reading that, I was just kind of like shocked at how intense sort of state level
[00:00:59.280 --> 00:01:03.040]   espionage is. The whole book was about like, they go to these European countries, and they try to
[00:01:03.040 --> 00:01:08.480]   like get all the technology and recruit all these people to get the technology. I mean, yeah, maybe
[00:01:08.480 --> 00:01:12.640]   one anecdote, you know, so when so the spy, you know, this eventual defector, you know, so he's
[00:01:12.640 --> 00:01:17.360]   being trained, he goes to the kind of GRU spy academy. And so then to graduate from the spy
[00:01:17.360 --> 00:01:21.200]   academy, sort of before you're sent abroad, you kind of had to pass a test to show that you can
[00:01:21.200 --> 00:01:27.200]   do this. And the test was, you know, you had to in Moscow, recruit a Soviet scientist and recruit
[00:01:27.200 --> 00:01:33.280]   them to give you information sort of like you would do in the foreign country. But of course,
[00:01:33.280 --> 00:01:39.280]   for whomever you recruited, the penalty for giving away sort of secret information was death. And so
[00:01:39.280 --> 00:01:44.880]   to graduate from the Soviet spy, this GRU spy academy, you had to condemn a countryman to death.
[00:01:46.320 --> 00:01:50.640]   States do this stuff. You know, you've got to have Russia proof security, right? It's like you
[00:01:50.640 --> 00:01:54.000]   can't you can't just have Russia steal your stuff. And like, maybe their clusters aren't going to be
[00:01:54.000 --> 00:01:57.520]   as big, but like, they're still going to be able to make the crazy bioweapons and the you know,
[00:01:57.520 --> 00:02:03.520]   the mosquito sized drone swarm, you know, and so on. And so I mean, I think I think I think the
[00:02:03.520 --> 00:02:08.320]   security component is just actually a pretty large component of the project in the sense of like,
[00:02:08.320 --> 00:02:12.560]   I currently do not see another way where we don't kind of like instantly proliferate this to
[00:02:12.560 --> 00:02:18.560]   everybody. So like, yeah, how easy would it be for for an actor to steal the things that are like,
[00:02:18.560 --> 00:02:24.880]   not the things that are released about Scarlett Johansson's voice, but the the RL things are
[00:02:24.880 --> 00:02:28.800]   talking about the unhoblings. I mean, I mean, all extremely easy, right? You know, I, you know,
[00:02:28.800 --> 00:02:32.880]   DeepMind, even like, you know, they, they don't make a claim that it's hard, right? DeepMind put
[00:02:32.880 --> 00:02:36.320]   out there, like whatever, frontier safety, something, and they like layout security levels,
[00:02:36.320 --> 00:02:40.080]   and they, you know, security levels zero to four, and four is the zone resistant to state actors,
[00:02:40.080 --> 00:02:43.680]   and they say we're at level zero, right? And then, you know, I mean, just recently, there's
[00:02:43.680 --> 00:02:47.680]   like an indictment of a guy who just like stole the code, a bunch of like really important AI
[00:02:47.680 --> 00:02:51.760]   code and went to China with it. And, you know, all he had to do to steal the code was, you know,
[00:02:51.760 --> 00:02:55.680]   copy the code and put it into Apple Notes, and then export it as PDF. And that got past their
[00:02:55.680 --> 00:02:59.440]   monitoring, right? And, you know, Google is the best security of any of the AI labs, probably
[00:02:59.440 --> 00:03:02.400]   because they have the, you know, the Google infrastructure. I mean, I think, I don't know,
[00:03:02.400 --> 00:03:06.240]   roughly, I would think of this as like, you know, security of a startup, right? And like,
[00:03:06.240 --> 00:03:11.280]   what does security of a startup look like, right? You know, it's not that good. It's easy to steal.
[00:03:11.280 --> 00:03:14.720]   - Is it just a matter of getting the right pen drive, and you plug it into the gigawatt data
[00:03:14.720 --> 00:03:17.360]   center next to the Three Gorges Dam, and then you're off to the races?
[00:03:17.360 --> 00:03:21.680]   - I mean, look, there's a few different things, right? So one threat model is just stealing the
[00:03:21.680 --> 00:03:25.920]   weights themselves. And the weights one is sort of particularly insane, right? Because they can
[00:03:25.920 --> 00:03:29.920]   just like steal the literal like end product, right? Just like make a replica of the atomic
[00:03:29.920 --> 00:03:33.920]   bomb, and then they're just like ready to go. And, you know, I think that one just is, you know,
[00:03:33.920 --> 00:03:37.920]   extremely important around the time we have AGI and super intelligence, right? Because it's, you
[00:03:37.920 --> 00:03:42.480]   know, China can build a big cluster. By default, we'd have a big lead, right? Because we have the
[00:03:42.480 --> 00:03:45.440]   better scientists, but we make the super intelligence, they just steal it, they're off to
[00:03:45.440 --> 00:03:49.920]   the races. Weights are a little bit less important right now. Because, you know, who cares if they
[00:03:49.920 --> 00:03:55.600]   steal the GP4 weights, right? Like, whatever. And so, you know, we still have to get started on
[00:03:55.600 --> 00:03:58.800]   weight security now. Because, you know, look, if we think AGI by 27, you know, this stuff is going
[00:03:58.800 --> 00:04:01.840]   to take a while. And it, you know, it doesn't, you know, it's not just going to be like, oh,
[00:04:01.840 --> 00:04:05.040]   we do some access control. It's going to, you know, if you actually want to be resistant to
[00:04:05.040 --> 00:04:10.000]   sort of Chinese espionage, you know, it needs to be much more intense. The thing, though, that I
[00:04:10.000 --> 00:04:13.680]   think, you know, people aren't paying enough attention to is the secrets, as you say. And,
[00:04:13.680 --> 00:04:18.720]   you know, I think this is, you know, the compute stuff is sexy. You know, we talk about it. But,
[00:04:18.720 --> 00:04:23.600]   you know, I think that, you know, I think people underrate the secrets because they're, you know,
[00:04:23.600 --> 00:04:26.320]   I think they're, you know, half an order of magnitude a year just by default sort of
[00:04:26.320 --> 00:04:29.840]   algorithmic progress. That's huge. You know, if we have a few year lead by default, you know,
[00:04:29.840 --> 00:04:34.720]   that's 10, 30 X, 100 X bigger cluster. If we protected them, you know, I don't think anyone
[00:04:34.720 --> 00:04:38.400]   has really gotten up in front of these people and been like, look, you know, the thing you're
[00:04:38.400 --> 00:04:42.880]   building is the most important thing for like the national security of the United States for like,
[00:04:42.880 --> 00:04:46.560]   whether, you know, like, you know, the free world will have another century ahead of it. Like,
[00:04:46.560 --> 00:04:50.400]   this is the thing you're doing is really important, like for your country, for democracy.
[00:04:50.400 --> 00:04:55.520]   And, you know, don't talk about the secrets. And it's not just about, you know,
[00:04:55.520 --> 00:05:00.240]   oh, DeepMind or whatever it's about. It's about, you know, these really important things.
[00:05:00.240 --> 00:05:10.240]   [BLANK_AUDIO]

