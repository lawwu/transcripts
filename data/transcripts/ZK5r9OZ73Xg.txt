
[00:00:00.000 --> 00:00:05.000]   - This idea of DISC, right,
[00:00:05.000 --> 00:00:07.920]   Distributed Idea Suppression Complex.
[00:00:07.920 --> 00:00:08.760]   - Yeah.
[00:00:08.760 --> 00:00:12.960]   - Is that what's bringing the Elon's of the world down?
[00:00:12.960 --> 00:00:13.800]   - You know, it's so funny,
[00:00:13.800 --> 00:00:17.480]   it's like he's asking Joe Rogan, like, is that a joint?
[00:00:17.480 --> 00:00:19.680]   You know, it's like, well, what will happen if I smoke it?
[00:00:19.680 --> 00:00:21.400]   What will happen to the stock price?
[00:00:21.400 --> 00:00:24.320]   What will happen if I scratch myself in public?
[00:00:24.320 --> 00:00:27.760]   What will happen if I say what I think about Thailand
[00:00:27.760 --> 00:00:30.280]   or COVID or who knows what?
[00:00:30.280 --> 00:00:32.480]   And everybody's like, don't say that, say this,
[00:00:32.480 --> 00:00:33.880]   go do this, go do that.
[00:00:33.880 --> 00:00:35.240]   Well, it's crazy making.
[00:00:35.240 --> 00:00:37.960]   It's absolutely crazy making.
[00:00:37.960 --> 00:00:40.920]   And if you think about what we put people through,
[00:00:40.920 --> 00:00:47.980]   we need to get people who can use FU money,
[00:00:47.980 --> 00:00:51.760]   the FU money they need to insulate themselves
[00:00:51.760 --> 00:00:53.880]   from all of the people who know better.
[00:00:53.880 --> 00:00:58.880]   'Cause my nightmare is that why did we only get one Elon?
[00:00:58.880 --> 00:01:00.920]   What if we were supposed to have thousands
[00:01:00.920 --> 00:01:02.360]   and thousands of Elon's?
[00:01:02.360 --> 00:01:06.160]   And the weird thing is like, this is all that remains.
[00:01:06.160 --> 00:01:10.360]   You're looking at like Obi-Wan and Yoda,
[00:01:10.360 --> 00:01:13.080]   and it's like, this is all that's left
[00:01:13.080 --> 00:01:17.840]   after Order 66 has been executed.
[00:01:17.840 --> 00:01:19.640]   And that's the thing that's really upsetting to me
[00:01:19.640 --> 00:01:22.120]   is we used to have Elon's five deep
[00:01:22.120 --> 00:01:24.040]   and then we could talk about Elon
[00:01:24.040 --> 00:01:26.960]   in the context of his cohort.
[00:01:26.960 --> 00:01:29.780]   But this is like, if you were to see a giraffe
[00:01:29.780 --> 00:01:31.660]   in the Arctic with no trees around,
[00:01:31.660 --> 00:01:33.800]   you'd think, why the long neck?
[00:01:33.800 --> 00:01:35.000]   What a strange sight.
[00:01:35.000 --> 00:01:37.900]   - How do we get more Elon's?
[00:01:37.900 --> 00:01:40.120]   How do we change these?
[00:01:40.120 --> 00:01:45.120]   So I think that you've, so we know MIT and Harvard.
[00:01:45.120 --> 00:01:48.840]   So maybe returning to our previous conversation,
[00:01:48.840 --> 00:01:51.360]   my sense is that the Elon's of the world
[00:01:51.360 --> 00:01:53.360]   are supposed to come from MIT and Harvard.
[00:01:53.360 --> 00:01:54.320]   - Right.
[00:01:54.320 --> 00:01:56.280]   - And how do you change?
[00:01:56.280 --> 00:01:58.760]   - Let's think of one that MIT sort of killed.
[00:01:58.760 --> 00:02:01.820]   Have any names in mind?
[00:02:01.820 --> 00:02:05.920]   Aaron Schwartz leaps to my mind.
[00:02:05.920 --> 00:02:06.760]   - Yeah.
[00:02:06.760 --> 00:02:11.760]   - Okay, are we MIT supposed to shield the Aaron Schwartz's
[00:02:11.760 --> 00:02:16.960]   from, I don't know, journal publishers?
[00:02:16.960 --> 00:02:19.560]   Or are we supposed to help the journal publishers
[00:02:19.560 --> 00:02:22.340]   so that we can throw 35 year sentences in his face
[00:02:22.340 --> 00:02:25.080]   or whatever it is that we did that depressed him?
[00:02:25.080 --> 00:02:27.400]   Okay, so here's my point.
[00:02:27.400 --> 00:02:32.400]   I want MIT to go back to being the home of Aaron Schwartz.
[00:02:32.400 --> 00:02:38.400]   And if you wanna send Aaron Schwartz to a state
[00:02:38.400 --> 00:02:41.320]   where he's looking at 35 years in prison
[00:02:41.320 --> 00:02:44.520]   or something like that, you are my sworn enemy.
[00:02:44.520 --> 00:02:45.720]   You are not MIT.
[00:02:45.720 --> 00:02:47.560]   - Yeah.
[00:02:47.560 --> 00:02:49.600]   - You are the traitorous,
[00:02:49.600 --> 00:02:56.560]   irresponsible, middle brow, pencil pushing,
[00:02:56.560 --> 00:03:00.440]   green eye shade fool that needs to not be in the seat
[00:03:00.440 --> 00:03:03.160]   at the presidency of MIT, period, the end.
[00:03:03.160 --> 00:03:04.720]   Get the fuck out of there
[00:03:04.720 --> 00:03:07.080]   and let one of our people sit in that chair.
[00:03:07.080 --> 00:03:09.880]   - And the thing that you've articulated is that
[00:03:09.880 --> 00:03:13.880]   the people in those chairs are not the way they are
[00:03:13.880 --> 00:03:17.040]   because they're evil or somehow morally compromised
[00:03:17.040 --> 00:03:20.880]   is that it's just that that's the distributed nature.
[00:03:20.880 --> 00:03:22.880]   Is that there's some kind of aspect of the system that--
[00:03:22.880 --> 00:03:26.640]   - These are people who wed themselves to the system.
[00:03:26.640 --> 00:03:28.640]   They adapt every instinct.
[00:03:28.640 --> 00:03:32.040]   And the fact is is that they're not going to be
[00:03:32.040 --> 00:03:34.060]   on Joe Rogan smoking a blunt.
[00:03:34.060 --> 00:03:36.280]   - Let me ask a silly question.
[00:03:36.280 --> 00:03:37.960]   Do you think institutions generally
[00:03:37.960 --> 00:03:39.580]   just tend to become that?
[00:03:39.580 --> 00:03:43.720]   - No, we get some of the institutions.
[00:03:43.720 --> 00:03:45.440]   We get Caltech.
[00:03:45.440 --> 00:03:46.480]   Here's what we're supposed to have.
[00:03:46.480 --> 00:03:47.840]   We're supposed to have Caltech.
[00:03:47.840 --> 00:03:49.840]   We're supposed to have Reed.
[00:03:49.840 --> 00:03:51.680]   We're supposed to have Deep Springs.
[00:03:51.680 --> 00:03:54.240]   We're supposed to have MIT.
[00:03:54.240 --> 00:03:56.320]   We're supposed to have a part of Harvard.
[00:03:56.320 --> 00:03:59.440]   And when the sharp elbow crowd comes after
[00:03:59.440 --> 00:04:02.560]   the sharp mind crowd, we're supposed to break
[00:04:02.560 --> 00:04:05.880]   those sharp elbows and say, don't come around here again.
[00:04:05.880 --> 00:04:07.880]   - So what are the weapons that the sharp minds
[00:04:07.880 --> 00:04:10.720]   are supposed to use in our modern day?
[00:04:10.720 --> 00:04:15.200]   So to reclaim MIT, what is the, what's the future?
[00:04:15.200 --> 00:04:16.480]   - Are you kidding me?
[00:04:16.480 --> 00:04:20.040]   First of all, assume that this is being seen at MIT.
[00:04:20.040 --> 00:04:21.280]   Hey, everybody. - It definitely is.
[00:04:21.280 --> 00:04:22.800]   - Okay.
[00:04:22.800 --> 00:04:24.200]   Hey, everybody.
[00:04:24.200 --> 00:04:26.300]   Try to remember who you are.
[00:04:26.300 --> 00:04:28.080]   You're the guys who put the police car
[00:04:28.080 --> 00:04:29.760]   on top of the great dome.
[00:04:29.760 --> 00:04:32.160]   You guys came up with the great breast of knowledge.
[00:04:32.160 --> 00:04:35.440]   You created a Tetris game in the green building.
[00:04:35.440 --> 00:04:38.560]   Now, what is your problem?
[00:04:38.560 --> 00:04:40.480]   They killed one of your own.
[00:04:40.480 --> 00:04:43.120]   You should make their life a living hell.
[00:04:43.960 --> 00:04:46.720]   You should be the ones who keep the memory
[00:04:46.720 --> 00:04:49.880]   of Aaron Schwartz alive and all of those hackers
[00:04:49.880 --> 00:04:51.340]   and all of those mutants.
[00:04:51.340 --> 00:04:57.160]   It's like, it's either our place or it isn't.
[00:04:57.160 --> 00:05:02.160]   And if we have to throw 12 more pianos off of the roof,
[00:05:02.160 --> 00:05:07.800]   if Harold Edgerton was taking those photographs
[00:05:07.800 --> 00:05:11.260]   with slow-mo back in the 40s,
[00:05:13.560 --> 00:05:16.840]   if Noam Chomsky's on your faculty,
[00:05:16.840 --> 00:05:19.160]   what the hell is wrong with you kids?
[00:05:19.160 --> 00:05:21.800]   You are the most creative and insightful people
[00:05:21.800 --> 00:05:24.320]   and you can't figure out how to defend Aaron Schwartz?
[00:05:24.320 --> 00:05:25.600]   That's on you guys.
[00:05:25.600 --> 00:05:28.240]   - So some of that is giving more power to the young,
[00:05:28.240 --> 00:05:30.640]   like you said, to the brave, to the bold.
[00:05:30.640 --> 00:05:33.720]   - Taking power from the feeble and the middle-brow.
[00:05:33.720 --> 00:05:35.560]   - Yeah, but what is the mechanism?
[00:05:35.560 --> 00:05:36.400]   To me-- - I don't know.
[00:05:36.400 --> 00:05:39.040]   You have some nine-volt batteries?
[00:05:39.040 --> 00:05:40.440]   You have some copper wire?
[00:05:41.240 --> 00:05:44.760]   - I tend to-- - Do you have a capacitor?
[00:05:44.760 --> 00:05:47.720]   - I tend to believe you have to create an alternative
[00:05:47.720 --> 00:05:51.000]   and make the alternative so much better
[00:05:51.000 --> 00:05:55.720]   that it makes MIT obsolete unless they change.
[00:05:55.720 --> 00:05:58.000]   And that's what forces change.
[00:05:58.000 --> 00:05:59.400]   So as opposed to somehow--
[00:05:59.400 --> 00:06:02.060]   - Okay, so you use projection mapping.
[00:06:02.060 --> 00:06:03.300]   - What's projection mapping?
[00:06:03.300 --> 00:06:05.520]   - Where you take some complicated edifice
[00:06:05.520 --> 00:06:07.320]   and you map all of its planes
[00:06:07.320 --> 00:06:10.240]   and then you actually project some unbelievable graphics,
[00:06:10.240 --> 00:06:12.320]   re-skinning a building, let's say, at night.
[00:06:12.320 --> 00:06:13.160]   - That's right, yeah.
[00:06:13.160 --> 00:06:15.240]   - Okay, so you wanna do some graffiti art with light.
[00:06:15.240 --> 00:06:16.720]   - You basically wanna hack the system?
[00:06:16.720 --> 00:06:19.480]   - No, I'm saying, look, listen to me, Liv.
[00:06:19.480 --> 00:06:21.680]   We're smarter than they are.
[00:06:21.680 --> 00:06:23.440]   And you know what they say?
[00:06:23.440 --> 00:06:27.160]   They say things like, I think we need some geeks.
[00:06:27.160 --> 00:06:28.680]   Get me two PhDs.
[00:06:28.680 --> 00:06:33.880]   You treat PhDs like that, that's a bad move.
[00:06:33.880 --> 00:06:36.200]   'Cause PhDs are capable.
[00:06:36.200 --> 00:06:40.640]   And we act like our job is to peel grapes for our betters.
[00:06:40.640 --> 00:06:42.400]   - Yeah, that's a strange thing.
[00:06:42.400 --> 00:06:44.560]   You speak about it very eloquently.
[00:06:44.560 --> 00:06:49.480]   It's how we treat basically the greatest minds in the world,
[00:06:49.480 --> 00:06:52.940]   which is like at their prime, which is PhD students.
[00:06:52.940 --> 00:06:56.020]   We pay them nothing.
[00:06:56.020 --> 00:06:58.960]   - I'm done with it.
[00:06:58.960 --> 00:06:59.780]   - Yeah.
[00:06:59.780 --> 00:07:01.880]   - Right, we gotta take what's ours.
[00:07:01.880 --> 00:07:04.720]   So take back MIT.
[00:07:04.720 --> 00:07:07.080]   Become ungovernable.
[00:07:07.080 --> 00:07:08.620]   Become ungovernable.
[00:07:08.620 --> 00:07:11.960]   And by the way, when you become ungovernable,
[00:07:11.960 --> 00:07:13.820]   don't do it by throwing food.
[00:07:13.820 --> 00:07:19.040]   Don't do it by pouring salt on the lawn like a jerk.
[00:07:19.040 --> 00:07:20.540]   Do it through brilliance.
[00:07:20.540 --> 00:07:23.280]   Because what you, Caltech and MIT can do,
[00:07:23.280 --> 00:07:26.640]   and maybe Rensselaer Polytechnic or Worcester Polytech,
[00:07:26.640 --> 00:07:28.800]   I don't know, Lehigh.
[00:07:28.800 --> 00:07:31.000]   God damn it, what's wrong with you technical people?
[00:07:31.000 --> 00:07:34.080]   You act like you're a servant class.
[00:07:34.080 --> 00:07:36.080]   - It's unclear to me how you reclaim it,
[00:07:36.080 --> 00:07:38.120]   except with brilliance, like you said.
[00:07:38.120 --> 00:07:41.680]   But to me, the way you reclaim it with brilliance
[00:07:41.680 --> 00:07:43.040]   is to go outside the system.
[00:07:43.040 --> 00:07:45.800]   - Aaron Schwartz came from the Elon Musk class.
[00:07:45.800 --> 00:07:48.160]   What you guys gonna do about it?
[00:07:48.160 --> 00:07:49.000]   Right?
[00:07:49.000 --> 00:07:53.120]   The super capable people need to flex,
[00:07:53.120 --> 00:07:55.320]   need to be individual, they need to stop giving away
[00:07:55.320 --> 00:07:58.120]   all their power to a zeitgeist or a community
[00:07:58.120 --> 00:07:59.520]   or this or that.
[00:07:59.520 --> 00:08:02.080]   You're not indoor cats, you're outdoor cats.
[00:08:02.080 --> 00:08:02.960]   Go be outdoor cats.
[00:08:02.960 --> 00:08:05.240]   - Do you think we're gonna see this kind of change?
[00:08:05.240 --> 00:08:06.880]   - You were the one asking me before,
[00:08:06.880 --> 00:08:09.400]   like what about the World War II generation?
[00:08:09.400 --> 00:08:10.480]   What I'm trying to say is that there's
[00:08:10.480 --> 00:08:12.240]   a technical revolt coming.
[00:08:12.240 --> 00:08:14.200]   You wanna talk about--
[00:08:14.200 --> 00:08:15.400]   - But I'm trying to lead it, right?
[00:08:15.400 --> 00:08:16.520]   I'm trying to see--
[00:08:16.520 --> 00:08:17.360]   - No, you're not trying to lead it.
[00:08:17.360 --> 00:08:18.800]   - I'm trying to get a blueprint here.
[00:08:18.800 --> 00:08:20.280]   - All right, Lex.
[00:08:20.280 --> 00:08:23.520]   How angry are you about our country pretending
[00:08:23.520 --> 00:08:26.920]   that you and I can't actually do technical subjects
[00:08:26.920 --> 00:08:30.340]   so that they need an army of kids coming in
[00:08:30.340 --> 00:08:32.280]   from four countries in Asia?
[00:08:32.280 --> 00:08:34.080]   It's not about the four countries in Asia,
[00:08:34.080 --> 00:08:35.800]   it's not about those kids.
[00:08:35.800 --> 00:08:38.240]   It's about lying about us, that we don't care enough
[00:08:38.240 --> 00:08:42.000]   about science and technology, that we're incapable of it.
[00:08:42.000 --> 00:08:44.440]   As if we don't have Chinese and Russians
[00:08:44.440 --> 00:08:49.400]   and Koreans and Croatians, like we've got everybody here.
[00:08:49.400 --> 00:08:51.480]   The only reason you're looking outside
[00:08:51.480 --> 00:08:53.500]   is that you wanna hire cheap people
[00:08:53.500 --> 00:08:55.760]   from the family business because you don't wanna pass
[00:08:55.760 --> 00:08:57.520]   the family business on.
[00:08:57.520 --> 00:08:59.160]   And you know what?
[00:08:59.160 --> 00:09:01.160]   You didn't really build the family business.
[00:09:01.160 --> 00:09:03.440]   It's not yours to decide.
[00:09:03.440 --> 00:09:05.900]   You the boomers and you the silent generation,
[00:09:05.900 --> 00:09:09.300]   you did your bit, but you also fouled a lot of stuff up.
[00:09:09.300 --> 00:09:11.580]   And you're custodians.
[00:09:11.580 --> 00:09:12.800]   You are caretakers.
[00:09:12.800 --> 00:09:14.440]   You are supposed to hand something.
[00:09:14.440 --> 00:09:17.960]   What you did instead was to gorge yourself
[00:09:17.960 --> 00:09:21.120]   on cheap foreign labor, which you then held up
[00:09:21.120 --> 00:09:23.500]   as being much more brilliant than your own children,
[00:09:23.500 --> 00:09:24.660]   which was never true.
[00:09:24.660 --> 00:09:26.880]   - See, but I'm trying to understand
[00:09:26.880 --> 00:09:29.200]   how we create a better system without anger,
[00:09:29.200 --> 00:09:30.440]   without revolution.
[00:09:30.440 --> 00:09:31.280]   - Ah.
[00:09:31.280 --> 00:09:36.280]   - Not by kissing and hugs, but by,
[00:09:36.280 --> 00:09:39.840]   I mean, I don't understand within MIT
[00:09:39.840 --> 00:09:42.920]   what the mechanism of building a better MIT is.
[00:09:42.920 --> 00:09:44.720]   - We're not gonna pay Elsevier.
[00:09:44.720 --> 00:09:45.960]   Aaron Schwartz was right.
[00:09:45.960 --> 00:09:48.520]   JSTOR is an abomination.
[00:09:48.520 --> 00:09:52.320]   - But why, who within MIT, who within institutions
[00:09:52.320 --> 00:09:55.000]   is going to do that when, just like you said,
[00:09:55.000 --> 00:09:57.880]   the people who are running the show are more senior.
[00:09:57.880 --> 00:10:00.320]   Why did I get Frank Wilczek to speak out?
[00:10:00.320 --> 00:10:03.840]   - So you're, it's basically individuals that step up.
[00:10:03.840 --> 00:10:06.000]   I mean, one of the surprising things about Elon
[00:10:06.000 --> 00:10:09.640]   is that one person can inspire so much.
[00:10:09.640 --> 00:10:11.240]   - He's got academic freedom.
[00:10:11.240 --> 00:10:12.480]   It just comes from money.
[00:10:12.480 --> 00:10:16.600]   - I don't agree with that.
[00:10:16.600 --> 00:10:20.480]   Do you think money, okay, so yes, certainly--
[00:10:20.480 --> 00:10:23.080]   - Sorry, and testicles.
[00:10:23.080 --> 00:10:25.080]   - You've, yes, but I think that testicles
[00:10:25.080 --> 00:10:26.440]   is more important than money.
[00:10:26.440 --> 00:10:27.280]   - Right.
[00:10:27.280 --> 00:10:29.000]   - Or guts.
[00:10:29.000 --> 00:10:31.360]   I think, I do agree with you, you speak about this a lot,
[00:10:31.360 --> 00:10:34.080]   that because the money in the academic institutions
[00:10:34.080 --> 00:10:37.520]   has been so constrained that people are misbehaving
[00:10:37.520 --> 00:10:39.560]   in horrible ways.
[00:10:39.560 --> 00:10:42.760]   But I don't think that if we reverse that
[00:10:42.760 --> 00:10:44.120]   and give a huge amount of money,
[00:10:44.120 --> 00:10:45.640]   people will all of a sudden behave well.
[00:10:45.640 --> 00:10:46.880]   I think it also takes guts.
[00:10:46.880 --> 00:10:49.080]   - No, you need to give people security.
[00:10:49.080 --> 00:10:49.920]   - Security, yes.
[00:10:49.920 --> 00:10:54.120]   - Like you need to know that you have a job on Monday
[00:10:54.120 --> 00:10:56.600]   when on Friday you say, "I'm not so sure
[00:10:56.600 --> 00:10:59.160]   "I really love diversity and inclusion."
[00:10:59.160 --> 00:11:00.840]   And somebody's like, "Wait, what?
[00:11:00.840 --> 00:11:02.360]   "You didn't love diversity?
[00:11:02.360 --> 00:11:03.880]   "We had a statement on diversity and inclusion
[00:11:03.880 --> 00:11:05.040]   "and you wouldn't sign?
[00:11:05.040 --> 00:11:06.880]   "Are you against the inclusion part
[00:11:06.880 --> 00:11:08.040]   "or are you against diversity?
[00:11:08.040 --> 00:11:10.040]   "Do you just not like people like you?"
[00:11:10.040 --> 00:11:12.920]   You're like, "Actually, that has nothing to do with anything.
[00:11:12.920 --> 00:11:14.960]   "You're making this into something that it isn't.
[00:11:14.960 --> 00:11:17.640]   "I don't wanna sign your goddamn stupid statement.
[00:11:17.640 --> 00:11:19.680]   "And get out of my lab."
[00:11:19.680 --> 00:11:22.920]   Get out of my lab, it all begins from the middle finger.
[00:11:22.920 --> 00:11:24.320]   Get out of my lab.
[00:11:25.480 --> 00:11:28.460]   The administrators need to find other work.
[00:11:28.460 --> 00:11:31.200]   - Yeah, listen, I agree with you
[00:11:31.200 --> 00:11:36.040]   and I hope to seek your advice and wisdom
[00:11:36.040 --> 00:11:38.880]   as we change this because I'd love to see--
[00:11:38.880 --> 00:11:42.400]   - I will visit you in prison if that's what you're asking.
[00:11:42.400 --> 00:11:45.000]   - I have no, I think prison is great.
[00:11:45.000 --> 00:11:49.040]   You get a lot of reading done and good working out.
[00:11:49.040 --> 00:11:54.200]   Well, let me ask, something I brought up before
[00:11:54.200 --> 00:11:55.800]   is the Nietzsche quote of,
[00:11:55.800 --> 00:11:57.880]   "Beware that when fighting monsters,
[00:11:57.880 --> 00:12:00.040]   "you yourself do not become a monster.
[00:12:00.040 --> 00:12:02.440]   "For when you gaze long into the abyss,
[00:12:02.440 --> 00:12:04.240]   "the abyss gazes into you."
[00:12:04.240 --> 00:12:08.400]   Are you worried that your focus on the flaws in the system
[00:12:08.400 --> 00:12:12.360]   that we've just been talking about has damaged your mind
[00:12:12.360 --> 00:12:15.760]   or the part of your mind that's able to see the beauty
[00:12:15.760 --> 00:12:18.560]   in the world in the system?
[00:12:18.560 --> 00:12:23.120]   That because you have so sharply been able
[00:12:23.120 --> 00:12:25.480]   to see the flaws in the system,
[00:12:25.480 --> 00:12:28.880]   you can no longer step back and appreciate its beauty?
[00:12:28.880 --> 00:12:33.180]   - Look, I'm the one who's trying to get the institutions
[00:12:33.180 --> 00:12:36.080]   to save themselves by getting rid of their inhabitants
[00:12:36.080 --> 00:12:38.760]   but leaving the institution, like a neutron bomb
[00:12:38.760 --> 00:12:43.760]   that removes the unworkable leadership class
[00:12:43.760 --> 00:12:45.400]   but leaves the structures.
[00:12:45.400 --> 00:12:48.320]   - So the leadership class is really the problem.
[00:12:48.320 --> 00:12:49.160]   - The leadership class is the problem.
[00:12:49.160 --> 00:12:50.920]   - But the individual, like the professors,
[00:12:50.920 --> 00:12:51.760]   the individual scholars--
[00:12:51.760 --> 00:12:54.200]   - Well, the professors are gonna have to go back
[00:12:54.200 --> 00:12:57.840]   into training to remember how to be professors.
[00:12:57.840 --> 00:12:59.640]   Like people are cowards at the moment
[00:12:59.640 --> 00:13:02.240]   because if they're not cowards, they're unemployed.
[00:13:02.240 --> 00:13:05.660]   - Yeah, that's one of the disappointing things
[00:13:05.660 --> 00:13:08.280]   I've encountered is to me, tenure--
[00:13:08.280 --> 00:13:11.200]   - But nobody has tenure now.
[00:13:11.200 --> 00:13:17.800]   - Whether they do or not, they certainly don't have
[00:13:20.920 --> 00:13:23.280]   the kind of character and fortitude
[00:13:23.280 --> 00:13:25.120]   that I was hoping to see.
[00:13:25.120 --> 00:13:25.960]   To me--
[00:13:25.960 --> 00:13:26.960]   - But they'd be gone.
[00:13:26.960 --> 00:13:30.800]   See, you're dreaming about the people
[00:13:30.800 --> 00:13:34.180]   who used to live at MIT.
[00:13:34.180 --> 00:13:38.440]   You're dreaming about the previous inhabitants
[00:13:38.440 --> 00:13:40.240]   of your university.
[00:13:40.240 --> 00:13:42.320]   And if you looked at somebody like,
[00:13:42.320 --> 00:13:46.520]   Isidore Singer is very old, I don't know what state he's in
[00:13:46.520 --> 00:13:49.560]   but that guy was absolutely the real deal.
[00:13:49.560 --> 00:13:51.720]   And if you look at Noam Chomsky,
[00:13:51.720 --> 00:13:55.200]   tell me that Noam Chomsky has been muzzled, right?
[00:13:55.200 --> 00:13:59.220]   Now, what I'm trying to get at is you're talking
[00:13:59.220 --> 00:14:02.120]   about younger energetic people, but those people,
[00:14:02.120 --> 00:14:03.840]   like when I say something like,
[00:14:03.840 --> 00:14:08.840]   I'm against, I'm for inclusion and I'm for diversity
[00:14:08.840 --> 00:14:12.920]   but I'm against diversity and inclusion TM,
[00:14:12.920 --> 00:14:13.820]   like the movement.
[00:14:13.820 --> 00:14:17.800]   Well, I couldn't say that if I was a professor.
[00:14:19.400 --> 00:14:21.880]   Oh my God, he's against our sacred document.
[00:14:21.880 --> 00:14:24.840]   Okay, well, in that kind of a world,
[00:14:24.840 --> 00:14:27.520]   do you wanna know how many things I don't agree with you on?
[00:14:27.520 --> 00:14:29.440]   Like we could go on for days and days and days,
[00:14:29.440 --> 00:14:31.640]   all of the nonsense that you've parroted
[00:14:31.640 --> 00:14:33.840]   inside of the institution.
[00:14:33.840 --> 00:14:36.320]   Any sane person has no need for it.
[00:14:36.320 --> 00:14:38.560]   They have no want or desire.
[00:14:38.560 --> 00:14:44.740]   - Do you think you have to have some patience for nonsense
[00:14:44.740 --> 00:14:47.280]   when many people work together in a system?
[00:14:47.280 --> 00:14:48.960]   - How long has string theory gone on for
[00:14:48.960 --> 00:14:51.240]   and how long have I been patient?
[00:14:51.240 --> 00:14:52.080]   Okay, so you're talking about--
[00:14:52.080 --> 00:14:53.400]   - There's a limit to patience, I imagine.
[00:14:53.400 --> 00:14:55.760]   - You're talking about like 36 years
[00:14:55.760 --> 00:14:58.000]   of modern nonsense in string theory.
[00:14:58.000 --> 00:15:00.800]   - So you can do like eight to 10 years, but not more.
[00:15:00.800 --> 00:15:03.020]   - I can do 40 minutes.
[00:15:03.020 --> 00:15:05.280]   This is 36 years.
[00:15:05.280 --> 00:15:06.880]   - Well, you've done that over two hours already.
[00:15:06.880 --> 00:15:08.320]   - No, but it's-- - I appreciate.
[00:15:08.320 --> 00:15:10.640]   - But it's been 36 years of nonsense
[00:15:10.640 --> 00:15:14.000]   since the anomaly cancellation in string theory.
[00:15:14.000 --> 00:15:16.800]   It's like, what are you talking about about patience?
[00:15:16.800 --> 00:15:20.160]   I mean, Lex, you're not even acting like yourself.
[00:15:20.160 --> 00:15:23.520]   Well, you're trying to stay in the system.
[00:15:23.520 --> 00:15:25.080]   - I'm not trying, I'm not.
[00:15:25.080 --> 00:15:28.640]   I'm trying to see if perhaps,
[00:15:28.640 --> 00:15:32.640]   so my hope is that the system just has a few assholes in it,
[00:15:32.640 --> 00:15:35.200]   which you highlight,
[00:15:35.200 --> 00:15:38.600]   and the fundamentals of the system are broken,
[00:15:38.600 --> 00:15:41.640]   because if the fundamentals of the systems are broken,
[00:15:41.640 --> 00:15:44.980]   then I just don't see a way for MIT to succeed.
[00:15:45.740 --> 00:15:50.260]   Like, I don't see how young people take over MIT.
[00:15:50.260 --> 00:15:51.300]   I don't see how--
[00:15:51.300 --> 00:15:54.460]   - By inspiring us.
[00:15:54.460 --> 00:15:58.460]   You know, the great part about being at MIT,
[00:15:58.460 --> 00:16:02.260]   like when you saw the genius in these pranks,
[00:16:02.260 --> 00:16:06.380]   the heart, the irreverence, it's like, don't,
[00:16:06.380 --> 00:16:08.700]   we were talking about Tom Lehrer the last time.
[00:16:08.700 --> 00:16:12.340]   Tom Lehrer was as naughty as the day is long, agreed?
[00:16:12.340 --> 00:16:13.220]   - Agreed.
[00:16:13.220 --> 00:16:14.740]   - Was he also a genius?
[00:16:14.740 --> 00:16:15.740]   Was he well-spoken?
[00:16:15.740 --> 00:16:17.900]   Was he highly cultured?
[00:16:17.900 --> 00:16:20.140]   He was so talented, so intellectual,
[00:16:20.140 --> 00:16:23.380]   that he could just make fart jokes morning, noon, and night.
[00:16:23.380 --> 00:16:27.180]   Okay, well, in part, the right to make fart jokes,
[00:16:27.180 --> 00:16:30.440]   the right to, for example, put a functioning phone booth
[00:16:30.440 --> 00:16:33.540]   that was ringing on top of the Great Dome at MIT
[00:16:33.540 --> 00:16:35.460]   has to do with we are such badasses
[00:16:35.460 --> 00:16:37.900]   that we can actually do this stuff.
[00:16:37.900 --> 00:16:39.740]   Well, don't tell me about it anymore.
[00:16:39.740 --> 00:16:41.740]   Go break the law.
[00:16:41.740 --> 00:16:44.100]   Go break the law in a way that inspires us
[00:16:44.100 --> 00:16:46.700]   and makes us not want to prosecute you.
[00:16:46.700 --> 00:16:49.540]   Break the law in a way that lets us know
[00:16:49.540 --> 00:16:51.060]   that you're calling us out on our bullshit,
[00:16:51.060 --> 00:16:53.260]   that you're filled with love,
[00:16:53.260 --> 00:16:57.260]   and that our technical talent has not gone to sleep,
[00:16:57.260 --> 00:17:00.660]   it's not incapable, and if the idea is
[00:17:00.660 --> 00:17:03.360]   is that you're gonna dig a moat around the university
[00:17:03.360 --> 00:17:07.420]   and fill it with tiger sharks, that's awesome,
[00:17:07.420 --> 00:17:08.780]   'cause I don't know how you're gonna do it,
[00:17:08.780 --> 00:17:10.840]   but if you actually manage to do that,
[00:17:10.840 --> 00:17:14.800]   I'm not gonna prosecute you under a reckless endangerment.
[00:17:14.800 --> 00:17:18.120]   - That's beautifully put.
[00:17:18.120 --> 00:17:20.840]   I hope those, first of all, they'll listen.
[00:17:20.840 --> 00:17:22.920]   I hope young people at MIT will take over
[00:17:22.920 --> 00:17:24.260]   in this kind of way.
[00:17:24.260 --> 00:17:27.640]   In the introduction to your podcast episode
[00:17:27.640 --> 00:17:32.640]   on Jeffrey Epstein, you give to me a really moving story,
[00:17:32.640 --> 00:17:37.300]   but unfortunately for me, too brief,
[00:17:37.300 --> 00:17:40.140]   about your experience with a therapist
[00:17:40.140 --> 00:17:42.440]   and a lasting terror that permeated your mind.
[00:17:42.440 --> 00:17:47.080]   Can you go there?
[00:17:47.080 --> 00:17:48.240]   Can you tell?
[00:17:48.240 --> 00:17:49.080]   - I don't think so.
[00:17:49.080 --> 00:17:50.820]   I mean, I appreciate what you're saying.
[00:17:50.820 --> 00:17:51.960]   I said it obliquely.
[00:17:51.960 --> 00:17:52.820]   I said enough.
[00:17:52.820 --> 00:17:57.080]   There are bad people who cross our paths,
[00:17:57.080 --> 00:18:02.080]   and the current vogue is to say, oh, I'm a survivor.
[00:18:02.080 --> 00:18:05.940]   I'm a victim.
[00:18:05.940 --> 00:18:07.540]   I can do anything I want.
[00:18:08.600 --> 00:18:11.500]   This is a broken person, and I don't know why
[00:18:11.500 --> 00:18:14.100]   I was sent to a broken person as a kid.
[00:18:14.100 --> 00:18:17.160]   And to be honest with you, I also felt like in that story,
[00:18:17.160 --> 00:18:19.260]   I say that I was able to say no,
[00:18:19.260 --> 00:18:22.860]   and this was like the entire weight of authority,
[00:18:22.860 --> 00:18:26.780]   and he was misusing his position,
[00:18:26.780 --> 00:18:29.800]   and I was also able to say no.
[00:18:29.800 --> 00:18:32.920]   What I couldn't say no to
[00:18:32.920 --> 00:18:35.360]   was having him re-inflicted in my life.
[00:18:35.360 --> 00:18:38.380]   - Right, so you were sent back.
[00:18:38.380 --> 00:18:39.220]   - Yeah, second time.
[00:18:39.220 --> 00:18:41.180]   I tried to complain about what had happened,
[00:18:41.180 --> 00:18:43.260]   and I tried to do it in a way that did not
[00:18:43.260 --> 00:18:48.700]   immediately cause horrific consequences
[00:18:48.700 --> 00:18:50.340]   to both this person and myself,
[00:18:50.340 --> 00:18:52.780]   because we don't have the tools
[00:18:52.780 --> 00:18:58.340]   to deal with sexual misbehavior.
[00:18:58.340 --> 00:19:00.260]   We have nuclear weapons.
[00:19:00.260 --> 00:19:02.560]   We don't have any way of saying,
[00:19:02.560 --> 00:19:05.720]   this is probably not a good place
[00:19:05.720 --> 00:19:09.920]   or a role for you at this moment as an authority figure,
[00:19:09.920 --> 00:19:11.680]   and something needs to be worked on.
[00:19:11.680 --> 00:19:13.840]   So in general, when we see somebody
[00:19:13.840 --> 00:19:17.000]   who is misbehaving in that way,
[00:19:17.000 --> 00:19:22.000]   our immediate instinct is to treat the person as Satan,
[00:19:22.000 --> 00:19:26.000]   and we understand why.
[00:19:26.000 --> 00:19:28.720]   We don't want our children to be at risk.
[00:19:28.720 --> 00:19:34.920]   Now, I personally believe that I fell down on the job
[00:19:34.920 --> 00:19:38.200]   and did not call out the Jeffrey Epstein thing early enough
[00:19:38.200 --> 00:19:41.080]   because I was terrified of what Jeffrey Epstein represents,
[00:19:41.080 --> 00:19:44.080]   and this recapitulated the old terror,
[00:19:44.080 --> 00:19:48.120]   trying to tell the world, this therapist is out of control.
[00:19:48.120 --> 00:19:51.880]   And when I said that, the world responded by saying,
[00:19:51.880 --> 00:19:53.480]   well, you have two appointments booked,
[00:19:53.480 --> 00:19:55.440]   and you have to go for the second one.
[00:19:55.440 --> 00:19:59.800]   So I got re-inflicted into this office on this person
[00:19:59.800 --> 00:20:01.400]   who was now convinced that I was about to tear down
[00:20:01.400 --> 00:20:02.840]   his career and his reputation
[00:20:02.840 --> 00:20:04.720]   and might have been on the verge of suicide for all I know.
[00:20:04.720 --> 00:20:06.200]   I don't know.
[00:20:06.200 --> 00:20:08.120]   But he was very, very angry,
[00:20:08.120 --> 00:20:09.520]   and he was furious with me
[00:20:09.520 --> 00:20:12.480]   that I had breached the sacred confidence of his office.
[00:20:12.480 --> 00:20:16.520]   - What kind of ripple effects does that have,
[00:20:16.520 --> 00:20:19.220]   has that had to the rest of your life?
[00:20:19.220 --> 00:20:23.200]   The absurdity and the cruelty of that.
[00:20:23.200 --> 00:20:24.760]   I mean, there's no sense to it.
[00:20:24.760 --> 00:20:27.960]   - Well, see, this is the thing
[00:20:27.960 --> 00:20:29.900]   people don't really grasp, I think.
[00:20:31.880 --> 00:20:35.760]   There's an academic who I got to know many years ago
[00:20:35.760 --> 00:20:42.160]   named Jennifer Fried, who has a theory of betrayal,
[00:20:42.160 --> 00:20:44.320]   which she calls institutional betrayal.
[00:20:44.320 --> 00:20:46.800]   And her gambit is that when you were betrayed
[00:20:46.800 --> 00:20:50.480]   by an institution that is sort of like a fiduciary
[00:20:50.480 --> 00:20:53.700]   or a parental obligation to take care of you,
[00:20:53.700 --> 00:20:58.720]   that you find yourself in a far different situation
[00:20:58.720 --> 00:21:01.240]   with respect to trauma than if you were betrayed
[00:21:01.240 --> 00:21:03.600]   by somebody who's a peer.
[00:21:03.600 --> 00:21:08.560]   And so I think that in my situation,
[00:21:08.560 --> 00:21:15.480]   I kind of repeat a particular dynamic with authority.
[00:21:15.480 --> 00:21:20.180]   I come in not following all the rules,
[00:21:20.180 --> 00:21:23.340]   trying to do some things, not trying to do others,
[00:21:23.340 --> 00:21:24.560]   blah, blah, blah.
[00:21:24.560 --> 00:21:28.520]   And then I get into a weird relationship with authority.
[00:21:28.520 --> 00:21:30.120]   And so I have more experience
[00:21:30.120 --> 00:21:32.520]   with what I would call institutional betrayal.
[00:21:32.520 --> 00:21:35.920]   Now, the funny part about it is that
[00:21:35.920 --> 00:21:39.080]   when you don't have masks or PPE
[00:21:39.080 --> 00:21:42.720]   in a influenza-like pandemic,
[00:21:42.720 --> 00:21:45.440]   and you're missing ICU beds and ventilators,
[00:21:45.440 --> 00:21:49.880]   that is ubiquitous institutional betrayal.
[00:21:49.880 --> 00:21:53.440]   So I believe that in a weird way, I was very early.
[00:21:53.440 --> 00:21:57.420]   The idea of, and this is like the really hard concept,
[00:21:58.680 --> 00:22:02.680]   pervasive or otherwise universal institutional betrayal,
[00:22:02.680 --> 00:22:04.200]   where all of the institutions,
[00:22:04.200 --> 00:22:07.860]   you can count on any hospital to not charge you properly
[00:22:07.860 --> 00:22:09.920]   for what their services are.
[00:22:09.920 --> 00:22:12.520]   You can count on no pharmaceutical company
[00:22:12.520 --> 00:22:15.600]   to produce the drug that will be maximally beneficial
[00:22:15.600 --> 00:22:17.480]   to the people who take it.
[00:22:17.480 --> 00:22:20.000]   You know that your financial professionals
[00:22:20.000 --> 00:22:22.800]   are not simply working in your best interest.
[00:22:22.800 --> 00:22:25.160]   And that issue had to do with
[00:22:25.160 --> 00:22:28.240]   the way in which growth left our system.
[00:22:28.240 --> 00:22:29.960]   So I think that the weird thing is
[00:22:29.960 --> 00:22:33.640]   is that this first institutional betrayal by a therapist
[00:22:33.640 --> 00:22:35.840]   left me very open to the idea of,
[00:22:35.840 --> 00:22:37.500]   okay, well, maybe the schools are bad.
[00:22:37.500 --> 00:22:38.760]   Maybe the hospitals are bad.
[00:22:38.760 --> 00:22:40.120]   Maybe the drug companies are bad.
[00:22:40.120 --> 00:22:41.800]   Maybe our food is off.
[00:22:41.800 --> 00:22:44.960]   Maybe our journalists are not serving journalistic ends.
[00:22:44.960 --> 00:22:46.920]   And that was what allowed me
[00:22:46.920 --> 00:22:49.560]   to sort of go all the distance and say,
[00:22:49.560 --> 00:22:52.560]   huh, I wonder if our problem is that something
[00:22:52.560 --> 00:22:57.240]   is causing all of our sense-making institutions to be off.
[00:22:57.240 --> 00:22:58.360]   That was the big insight.
[00:22:58.360 --> 00:23:02.320]   And tying that to a single ideology,
[00:23:02.320 --> 00:23:03.680]   what if it's just about growth?
[00:23:03.680 --> 00:23:05.080]   They were all built on growth,
[00:23:05.080 --> 00:23:08.160]   and now we've promoted people who are capable
[00:23:08.160 --> 00:23:11.640]   of keeping quiet that their institutions aren't working.
[00:23:11.640 --> 00:23:16.040]   So the privileged, silent aristocracy,
[00:23:16.040 --> 00:23:18.000]   the people who can be counted upon,
[00:23:18.000 --> 00:23:20.320]   not to mention a fire when a raging fire
[00:23:20.320 --> 00:23:21.820]   is tearing through a building.
[00:23:23.520 --> 00:23:28.520]   - But nevertheless, how big of a psychological burden is that?
[00:23:28.520 --> 00:23:29.400]   - It's huge.
[00:23:29.400 --> 00:23:30.240]   It's terrible.
[00:23:30.240 --> 00:23:31.080]   It's crushing.
[00:23:31.080 --> 00:23:33.040]   It's very-- - It's very comforting
[00:23:33.040 --> 00:23:34.880]   to be the parental.
[00:23:34.880 --> 00:23:37.320]   I mean, I don't know.
[00:23:37.320 --> 00:23:41.240]   I treasure, I mean, we were just talking about MIT.
[00:23:41.240 --> 00:23:44.120]   I can intellectualize and agree
[00:23:44.120 --> 00:23:45.360]   with everything you're saying,
[00:23:45.360 --> 00:23:47.600]   but there's a comfort, a warm blanket
[00:23:47.600 --> 00:23:49.960]   of being within the institution.
[00:23:49.960 --> 00:23:53.100]   And up until Aaron Schwartz, let's say.
[00:23:53.100 --> 00:23:56.880]   In other words, now, if I look at the provost
[00:23:56.880 --> 00:23:59.160]   and the president as mommy and daddy,
[00:23:59.160 --> 00:24:01.080]   you did what to my big brother?
[00:24:01.080 --> 00:24:06.440]   You did what to our family?
[00:24:06.440 --> 00:24:08.340]   You sold us out in which way?
[00:24:08.340 --> 00:24:11.940]   What secrets left for China?
[00:24:11.940 --> 00:24:13.720]   You hired which workforce?
[00:24:13.720 --> 00:24:16.120]   You did what to my wages?
[00:24:16.120 --> 00:24:18.660]   You took this portion of my grant for what purpose?
[00:24:18.660 --> 00:24:21.320]   You just stole my retirement through a fringe rate?
[00:24:21.320 --> 00:24:22.680]   What did you do?
[00:24:22.680 --> 00:24:24.720]   - But can you still, I mean,
[00:24:24.720 --> 00:24:28.560]   the thing is about this view you have
[00:24:28.560 --> 00:24:31.420]   is it often turns out to be sadly correct.
[00:24:31.420 --> 00:24:32.920]   - But this is the thing.
[00:24:32.920 --> 00:24:37.440]   - But let me just, in this silly hopeful thing,
[00:24:37.440 --> 00:24:39.880]   do you still have hope in institutions?
[00:24:39.880 --> 00:24:41.160]   Can you within your-- - Yes.
[00:24:41.160 --> 00:24:42.640]   - Psychologically. - Yes.
[00:24:42.640 --> 00:24:44.540]   - I'm referring not intellectually.
[00:24:44.540 --> 00:24:46.600]   Because you have to carry this burden,
[00:24:46.600 --> 00:24:50.360]   can you still have a hope within you, Jake?
[00:24:50.360 --> 00:24:52.640]   When you sit at home alone,
[00:24:52.640 --> 00:24:55.200]   and as opposed to seeing the darkness
[00:24:55.200 --> 00:24:57.560]   within these institutions, seeing a hope.
[00:24:57.560 --> 00:24:58.760]   - Well, but this is the thing.
[00:24:58.760 --> 00:25:03.760]   I want to confront, not for the purpose of a dust up.
[00:25:03.760 --> 00:25:08.320]   I believe, for example, if you've heard episode 19,
[00:25:08.320 --> 00:25:13.320]   that the best outcome is for Carol Greider to come forward,
[00:25:13.320 --> 00:25:15.880]   as we discussed in episode 19.
[00:25:15.880 --> 00:25:17.360]   - With your brother, Brett Einstein.
[00:25:17.360 --> 00:25:19.120]   - And say, you know what? - It's a great episode.
[00:25:19.120 --> 00:25:20.680]   - I screwed up.
[00:25:20.680 --> 00:25:23.960]   He did call, he did suggest the experiment.
[00:25:23.960 --> 00:25:26.120]   I didn't understand that it was his theory
[00:25:26.120 --> 00:25:27.200]   that was producing it.
[00:25:27.200 --> 00:25:30.260]   Maybe I was slow to grasp it.
[00:25:30.260 --> 00:25:35.260]   But my bad, and I don't want to pay for this bad choice
[00:25:35.260 --> 00:25:42.440]   on my part, let's say, for the rest of my career.
[00:25:42.440 --> 00:25:44.680]   I want to own up, and I want to help make sure
[00:25:44.680 --> 00:25:48.200]   that we do what's right with what's left.
[00:25:48.200 --> 00:25:50.640]   - And that's one little case within the institution
[00:25:50.640 --> 00:25:51.800]   that you would like to see made.
[00:25:51.800 --> 00:25:55.400]   - I would like to see MIT very clearly come out
[00:25:55.400 --> 00:25:57.520]   and say, you know, Margot O'Toole was right
[00:25:57.520 --> 00:25:59.960]   when she said David Baltimore's lab here
[00:25:59.960 --> 00:26:05.720]   produced some stuff that was not reproducible
[00:26:05.720 --> 00:26:08.700]   with Teresa Imanishikari's research.
[00:26:08.700 --> 00:26:11.840]   I want to see the courageous people.
[00:26:11.840 --> 00:26:14.880]   I would like to see the Aaron Schwartz wing
[00:26:14.880 --> 00:26:17.720]   of the computer science department.
[00:26:17.720 --> 00:26:20.520]   Yeah, let's think about it.
[00:26:20.520 --> 00:26:21.920]   Wouldn't that be great if they said,
[00:26:21.920 --> 00:26:23.800]   you know, an injustice was done,
[00:26:23.800 --> 00:26:25.900]   and we're gonna write that wrong
[00:26:25.900 --> 00:26:27.740]   just as if this was Alan Turing?
[00:26:27.740 --> 00:26:31.840]   - Which I don't think they've written that wrong.
[00:26:31.840 --> 00:26:34.400]   - Well, then let's have the Turing-Schwartz wing.
[00:26:34.400 --> 00:26:36.680]   - The Turing-Schwartz, they're starting
[00:26:36.680 --> 00:26:37.880]   a new college of computing.
[00:26:37.880 --> 00:26:40.360]   It wouldn't be wonderful to call it the Turing-Schwartz.
[00:26:40.360 --> 00:26:42.520]   - I would like to have the Madame Wu wing
[00:26:42.520 --> 00:26:44.000]   of the physics department,
[00:26:44.000 --> 00:26:47.240]   and I'd love to have the Emmy Noether statue
[00:26:47.240 --> 00:26:48.520]   in front of the math department.
[00:26:48.520 --> 00:26:49.840]   I mean, like, you want to get excited
[00:26:49.840 --> 00:26:52.560]   about actual diversity and inclusion?
[00:26:52.560 --> 00:26:54.720]   Well, let's go with our absolute best people
[00:26:54.720 --> 00:26:57.560]   who never got theirs, 'cause there is structural bigotry.
[00:26:57.560 --> 00:27:01.880]   But if we don't actually start celebrating
[00:27:01.880 --> 00:27:03.880]   the beautiful stuff that we're capable of
[00:27:03.880 --> 00:27:07.660]   when we're handed heroes and we fumble them into the trash,
[00:27:07.660 --> 00:27:08.500]   what the hell?
[00:27:08.500 --> 00:27:13.500]   I mean, Lex, this is such nonsense.
[00:27:13.500 --> 00:27:17.220]   Just pulling our head out.
[00:27:17.220 --> 00:27:22.340]   You know, on everyone's cecum should be tattooed,
[00:27:22.340 --> 00:27:24.520]   if you can read this, you're too close.
[00:27:24.520 --> 00:27:32.160]   - Beautifully put, and I'm a dreamer just like you.
[00:27:32.160 --> 00:27:36.400]   So I don't see as much of the darkness,
[00:27:36.400 --> 00:27:39.940]   genetically or due to my life experience,
[00:27:39.940 --> 00:27:43.820]   but I do share the hope for MIT,
[00:27:43.820 --> 00:27:45.620]   the institution that we care a lot about.
[00:27:45.620 --> 00:27:46.600]   - We both do.
[00:27:46.600 --> 00:27:48.660]   - Yeah, and Harvard, the institution
[00:27:48.660 --> 00:27:50.960]   I don't give a damn about, but you do.
[00:27:50.960 --> 00:27:52.000]   - I love Harvard.
[00:27:52.000 --> 00:27:52.840]   - I'm just kidding.
[00:27:52.840 --> 00:27:54.800]   - I love Harvard, but Harvard and I
[00:27:54.800 --> 00:27:56.480]   have a very difficult relationship,
[00:27:56.480 --> 00:27:57.760]   and part of what, you know,
[00:27:57.760 --> 00:27:59.920]   when you love a family that isn't working,
[00:27:59.920 --> 00:28:04.080]   I don't want to trash, I didn't bring up the name
[00:28:04.080 --> 00:28:07.160]   of the president of MIT during the Aaron Schwartz period.
[00:28:07.160 --> 00:28:12.080]   It's not vengeance, I want the rot cleared out.
[00:28:12.080 --> 00:28:14.720]   I don't need to go after human beings.
[00:28:14.720 --> 00:28:19.480]   - Yeah, just like you said, with the disk formulation,
[00:28:19.480 --> 00:28:24.000]   the individual human beings don't necessarily carry the--
[00:28:24.000 --> 00:28:28.200]   - It's those chairs that are so powerful
[00:28:28.200 --> 00:28:29.520]   in which they sit.
[00:28:29.520 --> 00:28:31.240]   - It's the chairs, not the humans.
[00:28:31.240 --> 00:28:32.520]   - It's not the humans.
[00:28:32.720 --> 00:28:34.880]   (silence)
[00:28:34.880 --> 00:28:37.040]   (silence)
[00:28:37.040 --> 00:28:39.200]   (silence)
[00:28:39.200 --> 00:28:41.360]   (silence)
[00:28:41.360 --> 00:28:43.520]   (silence)
[00:28:43.520 --> 00:28:45.680]   (silence)
[00:28:45.680 --> 00:28:47.840]   (silence)
[00:28:47.840 --> 00:28:57.840]   [BLANK_AUDIO]

