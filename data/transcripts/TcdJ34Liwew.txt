
[00:00:00.880 --> 00:00:01.720]   - Hey, everyone.
[00:00:01.720 --> 00:00:03.520]   Welcome to the Latent Space Podcast.
[00:00:03.520 --> 00:00:05.460]   This is Alessio, partner and CTO
[00:00:05.460 --> 00:00:07.040]   on Resident and Decibel Partners.
[00:00:07.040 --> 00:00:09.960]   And I'm joined by my co-host Swiggs, founder of Small.ai.
[00:00:09.960 --> 00:00:12.320]   - Hey, and today in the studio we have Soheil Doshi.
[00:00:12.320 --> 00:00:13.140]   Welcome.
[00:00:13.140 --> 00:00:14.320]   - Yeah, thanks for having me.
[00:00:14.320 --> 00:00:15.960]   - Among many things, you're a CEO and co-founder
[00:00:15.960 --> 00:00:16.800]   of Mixpanel.
[00:00:16.800 --> 00:00:17.620]   - Yep.
[00:00:17.620 --> 00:00:19.800]   - And I think about three years ago,
[00:00:19.800 --> 00:00:21.400]   you left to start Mighty.
[00:00:21.400 --> 00:00:22.640]   - Mm-hmm.
[00:00:22.640 --> 00:00:25.920]   - And more recently, I think about a year ago,
[00:00:25.920 --> 00:00:27.680]   transitioned into Playground.
[00:00:27.680 --> 00:00:31.160]   And you've just announced your new round.
[00:00:31.160 --> 00:00:33.120]   I'd just like to start, touch on Mixpanel a little bit,
[00:00:33.120 --> 00:00:35.200]   'cause it's obviously one of the more
[00:00:35.200 --> 00:00:37.920]   sort of successful analytics companies
[00:00:37.920 --> 00:00:40.640]   we previously had amplitude on.
[00:00:40.640 --> 00:00:43.800]   And I'm curious if you had any sort of reflections
[00:00:43.800 --> 00:00:46.480]   on just that overall,
[00:00:46.480 --> 00:00:49.880]   the interaction of that amount of data
[00:00:49.880 --> 00:00:52.120]   that people would want to use for AI.
[00:00:52.120 --> 00:00:54.760]   Like, I don't know if there's still a part of you
[00:00:54.760 --> 00:00:56.320]   that stays in touch with that world.
[00:00:56.320 --> 00:00:58.560]   - Yeah, I mean, it's, I mean, you know,
[00:00:58.560 --> 00:01:03.560]   the short version is that maybe back in like 2015 or '16,
[00:01:03.560 --> 00:01:05.800]   I don't really remember exactly,
[00:01:05.800 --> 00:01:08.900]   'cause it was a while ago, we had an ML team at Mixpanel.
[00:01:08.900 --> 00:01:13.060]   And I think this is like when maybe deep learning
[00:01:13.060 --> 00:01:14.680]   or something like really just started
[00:01:14.680 --> 00:01:16.160]   getting kind of exciting.
[00:01:16.160 --> 00:01:17.800]   And we were thinking that maybe we,
[00:01:17.800 --> 00:01:20.200]   given that we had such vast amounts of data,
[00:01:20.200 --> 00:01:22.460]   perhaps we could predict things.
[00:01:22.460 --> 00:01:24.480]   So we built, you know, two or three different features.
[00:01:24.480 --> 00:01:26.960]   I think we built a feature where we could predict
[00:01:26.960 --> 00:01:30.200]   whether users would churn from your product.
[00:01:30.200 --> 00:01:32.040]   We made a feature that could predict
[00:01:32.040 --> 00:01:33.840]   whether users would convert.
[00:01:33.840 --> 00:01:35.320]   We tried to be built a feature
[00:01:35.320 --> 00:01:36.840]   that could do anomaly detection.
[00:01:36.840 --> 00:01:40.080]   Like if something occurred in your product,
[00:01:40.080 --> 00:01:41.160]   that was just very surprising,
[00:01:41.160 --> 00:01:43.840]   maybe a spike in traffic in a particular region.
[00:01:43.840 --> 00:01:45.040]   Could we tell you in advance?
[00:01:45.040 --> 00:01:46.360]   Could we tell you that that happened?
[00:01:46.360 --> 00:01:47.880]   'Cause it's really hard to like know everything
[00:01:47.880 --> 00:01:49.080]   that's going on with your data.
[00:01:49.080 --> 00:01:51.580]   Could we tell you something surprising about your data?
[00:01:51.580 --> 00:01:53.700]   And we tried all of these various features.
[00:01:53.700 --> 00:01:55.440]   Most of it boiled down to just like, you know,
[00:01:55.440 --> 00:01:58.480]   using logistic regression.
[00:01:58.480 --> 00:02:03.280]   And it never quite seemed very groundbreaking in the end.
[00:02:03.280 --> 00:02:07.240]   And so I think, you know, we had a four or five person ML team
[00:02:07.240 --> 00:02:10.220]   and I think we never expanded it from there.
[00:02:10.220 --> 00:02:12.060]   And I did all these fast AI courses
[00:02:12.060 --> 00:02:13.280]   trying to learn about ML.
[00:02:13.280 --> 00:02:15.200]   And that was the, that's the--
[00:02:15.200 --> 00:02:16.640]   - That's the first time you did fast AI.
[00:02:16.640 --> 00:02:18.320]   - Yeah, that was the first time I did fast AI.
[00:02:18.320 --> 00:02:20.400]   Yeah, I think I've done it now three times maybe.
[00:02:20.400 --> 00:02:21.240]   - Oh, okay.
[00:02:21.240 --> 00:02:22.060]   I didn't know it was a third.
[00:02:22.060 --> 00:02:23.160]   Okay.
[00:02:23.160 --> 00:02:25.440]   - No, no, just me reviewing it is maybe three times,
[00:02:25.440 --> 00:02:26.280]   but yeah.
[00:02:26.280 --> 00:02:27.120]   - Yeah, yeah, yeah.
[00:02:27.120 --> 00:02:29.160]   I mean, I think you mentioned prediction,
[00:02:29.160 --> 00:02:31.760]   but honestly like it's also just about the feedback, right?
[00:02:31.760 --> 00:02:34.900]   The quality of feedback from users,
[00:02:34.900 --> 00:02:38.400]   I think it's useful for anyone building AI applications.
[00:02:38.400 --> 00:02:39.320]   - Yeah.
[00:02:39.320 --> 00:02:40.560]   - Yeah, self-evident.
[00:02:40.560 --> 00:02:42.760]   - Yeah, I think I haven't spent a lot of time
[00:02:42.760 --> 00:02:44.440]   thinking about Mixpanel 'cause it's been a long time,
[00:02:44.440 --> 00:02:47.680]   but yeah, I wonder now, given everything that's happened,
[00:02:47.680 --> 00:02:50.400]   like, you know, sometimes I'm like,
[00:02:50.400 --> 00:02:51.920]   oh, I wonder what we could do now.
[00:02:51.920 --> 00:02:54.360]   And then I kind of like move on to whatever I'm working on,
[00:02:54.360 --> 00:02:56.720]   but things have changed significantly since.
[00:02:56.720 --> 00:02:57.880]   So yeah.
[00:02:57.880 --> 00:02:58.720]   - Yeah.
[00:02:58.720 --> 00:02:59.560]   Awesome.
[00:02:59.560 --> 00:03:01.660]   And then maybe we'll touch on Mighty a little bit.
[00:03:01.660 --> 00:03:03.400]   Mighty was very, very bold.
[00:03:03.400 --> 00:03:06.440]   It was basically, well, my framing of it was,
[00:03:06.440 --> 00:03:07.920]   you will run our browsers for us
[00:03:07.920 --> 00:03:10.820]   because everyone has too many tabs open.
[00:03:10.820 --> 00:03:12.680]   I have too many tabs open and slowing down your machines
[00:03:12.680 --> 00:03:14.160]   that you can do it better for us
[00:03:14.160 --> 00:03:15.480]   in a centralized data center.
[00:03:15.480 --> 00:03:17.960]   - Yeah, we were first trying to make a browser
[00:03:17.960 --> 00:03:21.120]   that we would stream from a data center to your computer
[00:03:21.120 --> 00:03:22.680]   at extremely low latency.
[00:03:22.680 --> 00:03:27.080]   But the real objective wasn't trying to make a browser
[00:03:27.080 --> 00:03:27.920]   or anything like that.
[00:03:27.920 --> 00:03:29.080]   The real objective was to try to make
[00:03:29.080 --> 00:03:30.760]   a new kind of computer.
[00:03:30.760 --> 00:03:32.720]   And the thought was just that, like, you know,
[00:03:32.720 --> 00:03:35.080]   we have these computers in front of us today
[00:03:35.080 --> 00:03:37.840]   and we upgrade them or they run out of RAM
[00:03:37.840 --> 00:03:39.640]   or they don't have enough RAM or not enough disk,
[00:03:39.640 --> 00:03:43.240]   or, you know, there's some limitation with our computers,
[00:03:43.240 --> 00:03:46.360]   perhaps like data locality is a problem.
[00:03:46.360 --> 00:03:48.880]   Could we, you know, why do I need to think about
[00:03:48.880 --> 00:03:50.760]   upgrading my computer ever?
[00:03:50.760 --> 00:03:52.800]   And so, you know, we just had to kind of observe that,
[00:03:52.800 --> 00:03:55.920]   like, well, actually, it seems like a lot of applications
[00:03:55.920 --> 00:03:57.600]   are just now in the browser.
[00:03:57.600 --> 00:04:00.800]   You know, it's like how many real desktop applications
[00:04:00.800 --> 00:04:02.960]   do we use relative to the number of applications
[00:04:02.960 --> 00:04:03.800]   we use in the browser?
[00:04:03.800 --> 00:04:05.920]   So it was just this realization that actually, like,
[00:04:05.920 --> 00:04:08.280]   you know, the browser was effectively becoming
[00:04:08.280 --> 00:04:10.840]   more or less our operating system over time.
[00:04:10.840 --> 00:04:13.120]   And so then that's why we kind of decided to go,
[00:04:13.120 --> 00:04:14.680]   hmm, maybe we can stream the browser.
[00:04:14.680 --> 00:04:15.760]   Fortunately, the idea did not work
[00:04:15.760 --> 00:04:18.040]   for a couple of different reasons,
[00:04:18.040 --> 00:04:21.200]   but the objective is to try to make a true new computer.
[00:04:21.200 --> 00:04:22.480]   - Yeah, very bold, very bold.
[00:04:22.480 --> 00:04:25.200]   - Yeah, and I was there at YC Demo Day
[00:04:25.200 --> 00:04:26.040]   when you first announced it.
[00:04:26.040 --> 00:04:26.960]   - Oh, okay.
[00:04:26.960 --> 00:04:29.880]   - I think the last, or one of the last in-person ones,
[00:04:29.880 --> 00:04:32.120]   or like the PR 34 in Mission Bay.
[00:04:32.120 --> 00:04:34.280]   - Yeah, before COVID.
[00:04:34.280 --> 00:04:36.080]   - How do you think about that now
[00:04:36.080 --> 00:04:38.240]   when everybody wants to put some of these models
[00:04:38.240 --> 00:04:39.760]   in people's machines and some of them
[00:04:39.760 --> 00:04:40.960]   want to stream them in?
[00:04:40.960 --> 00:04:44.320]   Do you think there's maybe another wave of the same problem
[00:04:44.320 --> 00:04:46.200]   before it was like browser apps too slow,
[00:04:46.200 --> 00:04:49.000]   and now it's like models too slow to run on device?
[00:04:49.000 --> 00:04:51.000]   - Yeah, I think, you know,
[00:04:51.000 --> 00:04:52.520]   we obviously pivoted away from Mighty,
[00:04:52.520 --> 00:04:57.040]   but a lot of what I somewhat believed at Mighty
[00:04:57.040 --> 00:04:58.760]   is like somewhat very true.
[00:04:58.760 --> 00:05:02.000]   Maybe why I'm so excited about AI and what's happening.
[00:05:02.000 --> 00:05:03.440]   A lot of what Mighty was about
[00:05:03.440 --> 00:05:06.800]   was like moving compute somewhere else, right?
[00:05:06.800 --> 00:05:07.920]   Right now applications,
[00:05:07.920 --> 00:05:12.280]   they get limited quantities of memory, disk, networking,
[00:05:12.280 --> 00:05:14.920]   whatever your home network has, et cetera.
[00:05:14.920 --> 00:05:17.280]   You know, what if these applications could somehow,
[00:05:17.280 --> 00:05:18.440]   if we could shift compute,
[00:05:18.440 --> 00:05:20.560]   and then these applications have vastly more compute
[00:05:20.560 --> 00:05:22.120]   than they do today.
[00:05:22.120 --> 00:05:24.920]   Right now it's just like client backend services,
[00:05:24.920 --> 00:05:27.280]   but you know, what if we could change the shape
[00:05:27.280 --> 00:05:31.040]   of how applications could interact with things?
[00:05:31.040 --> 00:05:33.400]   And it's changed my thinking.
[00:05:33.400 --> 00:05:36.680]   In some ways, AI has like a bit of a continuation
[00:05:36.680 --> 00:05:38.120]   of my belief that like,
[00:05:38.120 --> 00:05:41.560]   perhaps we can really shift compute somewhere else.
[00:05:41.560 --> 00:05:43.120]   One of the problems with Mighty
[00:05:43.120 --> 00:05:47.720]   was that JavaScript is single-threaded in the browser.
[00:05:47.720 --> 00:05:49.280]   And what we learned, you know,
[00:05:49.280 --> 00:05:51.240]   the reason why we kind of abandoned Mighty
[00:05:51.240 --> 00:05:52.520]   was because I didn't believe
[00:05:52.520 --> 00:05:53.760]   we could make a new kind of computer.
[00:05:53.760 --> 00:05:56.080]   We could have made some kind of enterprise business,
[00:05:56.080 --> 00:05:59.000]   probably could have made maybe a lot of money,
[00:05:59.000 --> 00:06:01.520]   but it wasn't going to be what I hoped it was going to be.
[00:06:01.520 --> 00:06:05.440]   And so once I realized that most of a web app
[00:06:05.440 --> 00:06:07.240]   is just going to be single-threaded JavaScript,
[00:06:07.240 --> 00:06:10.160]   then the only thing you could do largely
[00:06:10.160 --> 00:06:11.480]   withstanding changing JavaScript,
[00:06:11.480 --> 00:06:14.560]   which is a fool's errand most likely,
[00:06:14.560 --> 00:06:18.080]   is make a better CPU, right?
[00:06:18.080 --> 00:06:20.480]   And there's like three CPU manufacturers,
[00:06:20.480 --> 00:06:23.120]   two of which sell, you know, big ones, you know,
[00:06:23.120 --> 00:06:26.440]   AMD, Intel, and then of course, like Apple made the M1.
[00:06:26.440 --> 00:06:30.240]   And it's not like single-threaded CPU core performance.
[00:06:30.240 --> 00:06:33.240]   Single core performance was like increasing very fast.
[00:06:33.240 --> 00:06:35.080]   It's plateauing rapidly.
[00:06:35.080 --> 00:06:36.800]   And even these different like companies
[00:06:36.800 --> 00:06:38.560]   were not doing as good of a job, you know,
[00:06:38.560 --> 00:06:40.640]   sort of with the continuation of Moore's law.
[00:06:40.640 --> 00:06:43.480]   But what happened in AI was that you got like,
[00:06:43.480 --> 00:06:46.920]   like if you think of the AI model as like a computer program
[00:06:46.920 --> 00:06:48.560]   which is like a compiled computer program,
[00:06:48.560 --> 00:06:50.120]   it is literally built and designed
[00:06:50.120 --> 00:06:53.360]   to do massive parallel computations.
[00:06:53.360 --> 00:06:55.120]   And so if you could take like
[00:06:55.120 --> 00:06:56.520]   the universal approximation theorem
[00:06:56.520 --> 00:07:00.280]   to its like kind of logical complete point,
[00:07:00.280 --> 00:07:01.120]   you know, you're like, wow,
[00:07:01.120 --> 00:07:04.360]   I can get make computation happen really rapidly
[00:07:04.360 --> 00:07:06.040]   and parallel somewhere else.
[00:07:06.040 --> 00:07:09.200]   You know, so you end up with these like
[00:07:09.200 --> 00:07:11.920]   really amazing models that can like do anything.
[00:07:11.920 --> 00:07:14.080]   It just turned out like perhaps,
[00:07:14.080 --> 00:07:16.320]   perhaps the new kind of computer
[00:07:16.320 --> 00:07:19.000]   would just simply be shifted, you know,
[00:07:19.000 --> 00:07:22.600]   into these like really amazing AI models in reality.
[00:07:22.600 --> 00:07:23.440]   - Yeah.
[00:07:23.440 --> 00:07:25.760]   Like I think Andrej Karpathy has always been,
[00:07:25.760 --> 00:07:28.640]   has been making a lot of analogies with the LLMOS.
[00:07:28.640 --> 00:07:30.480]   - Yeah, I saw his, yeah, I saw his video
[00:07:30.480 --> 00:07:31.880]   and I watched that, you know,
[00:07:31.880 --> 00:07:33.320]   maybe two weeks ago or something like that.
[00:07:33.320 --> 00:07:35.000]   And I was like, oh man, this,
[00:07:35.000 --> 00:07:37.000]   I very much resonate with this like idea.
[00:07:37.000 --> 00:07:38.680]   - Why didn't I see this three years ago?
[00:07:38.680 --> 00:07:40.760]   - Yeah, I think, I think there still will be,
[00:07:40.760 --> 00:07:42.240]   you know, local models
[00:07:42.240 --> 00:07:43.760]   and then there'll be these very large models
[00:07:43.760 --> 00:07:45.760]   that have to be run in data centers.
[00:07:45.760 --> 00:07:47.320]   Yeah, I think it just depends on kind of like
[00:07:47.320 --> 00:07:48.440]   the right tool for the job,
[00:07:48.440 --> 00:07:52.000]   like any, like any engineer would probably care about.
[00:07:52.000 --> 00:07:54.400]   But I think that, you know, by and large,
[00:07:54.400 --> 00:07:57.520]   like if the models continue to kind of keep getting bigger,
[00:07:57.520 --> 00:07:58.720]   you know, it's gonna, it's,
[00:07:58.720 --> 00:07:59.760]   you're always going to be wondering
[00:07:59.760 --> 00:08:01.880]   whether you should use the big thing or the small,
[00:08:01.880 --> 00:08:03.880]   you know, the tiny little model.
[00:08:03.880 --> 00:08:05.800]   And it might just depend on like, you know,
[00:08:05.800 --> 00:08:08.320]   do you need 30 FPS or 60 FPS?
[00:08:08.320 --> 00:08:12.240]   Maybe that would be hard to do, you know, over a network.
[00:08:12.240 --> 00:08:16.520]   - Yeah, you tackle the much harder problem latency-wise,
[00:08:16.520 --> 00:08:19.080]   you know, than the AI models actually require.
[00:08:19.080 --> 00:08:20.880]   - Yeah, yeah, you can do quite well.
[00:08:20.880 --> 00:08:22.360]   You can do quite well.
[00:08:22.360 --> 00:08:26.320]   You know, you definitely did 30 FPS video streaming,
[00:08:26.320 --> 00:08:28.440]   did very crazy things to make that work.
[00:08:28.440 --> 00:08:30.720]   So I'm actually quite bullish
[00:08:30.720 --> 00:08:33.120]   on the kinds of things you can do with networking.
[00:08:33.120 --> 00:08:33.960]   - Yeah, right.
[00:08:33.960 --> 00:08:37.520]   Maybe someday you'll come back to that at some point.
[00:08:37.520 --> 00:08:39.360]   - But so for those that don't know,
[00:08:39.360 --> 00:08:41.200]   you're very transparent on Twitter.
[00:08:41.200 --> 00:08:43.840]   Very good to follow you just to learn your insights.
[00:08:43.840 --> 00:08:45.800]   And you actually published a postmortem on Mighty
[00:08:45.800 --> 00:08:48.400]   that people can read up on if they're willing to.
[00:08:48.400 --> 00:08:50.760]   And so there was a bit of an overlap.
[00:08:50.760 --> 00:08:56.560]   You started exploring the AI stuff in June, 2022,
[00:08:56.560 --> 00:08:57.600]   which is when you started saying like,
[00:08:57.600 --> 00:08:59.480]   "I'm taking Fast.ai again."
[00:08:59.480 --> 00:09:02.440]   Maybe, was there more context around that?
[00:09:02.440 --> 00:09:05.480]   - Yeah, I think I was kind of like waiting
[00:09:05.480 --> 00:09:08.640]   for the team at Mighty to finish up something.
[00:09:08.640 --> 00:09:11.240]   And I was like, "Okay, well, what can I do?
[00:09:11.240 --> 00:09:15.240]   "I guess I will make some kind of like address bar predictor
[00:09:15.240 --> 00:09:16.080]   "in the browser."
[00:09:16.080 --> 00:09:18.560]   So we had forked Chrome and Chromium.
[00:09:18.560 --> 00:09:22.420]   And I was like, "You know, one thing that's kind of lame
[00:09:22.420 --> 00:09:24.600]   "is that like this browser should be like a lot better
[00:09:24.600 --> 00:09:28.480]   "at predicting what I might do, where I might wanna go."
[00:09:28.480 --> 00:09:30.160]   You know, it struck me as really odd
[00:09:30.160 --> 00:09:32.720]   that Chrome had very little AI actually,
[00:09:32.720 --> 00:09:34.680]   or ML inside this browser.
[00:09:34.680 --> 00:09:37.320]   And for a company like Google, you'd think there's a lot,
[00:09:37.320 --> 00:09:41.200]   but it's actually just like the code is actually just very,
[00:09:41.200 --> 00:09:43.240]   you know, it's just a bunch of if then statements
[00:09:43.240 --> 00:09:45.160]   is more or less the address bar.
[00:09:45.160 --> 00:09:47.600]   So it seemed like a pretty big opportunity.
[00:09:47.600 --> 00:09:50.040]   And that's also where a lot of people interact
[00:09:50.040 --> 00:09:50.880]   with the browser.
[00:09:50.880 --> 00:09:52.360]   So in a long story short, I was like,
[00:09:52.360 --> 00:09:55.160]   "Hmm, I wonder what I could build here."
[00:09:55.160 --> 00:09:57.840]   So I started to, yeah, take some AI courses
[00:09:57.840 --> 00:10:00.620]   and try to review the material again
[00:10:00.620 --> 00:10:02.520]   and get back to figuring it out.
[00:10:02.520 --> 00:10:05.240]   But I think that was somewhat serendipitous
[00:10:05.240 --> 00:10:08.640]   because right around April was, I think,
[00:10:08.640 --> 00:10:10.320]   a very big watershed moment in AI
[00:10:10.320 --> 00:10:12.200]   'cause that's when "Dolly 2" came out.
[00:10:12.200 --> 00:10:15.560]   And I think that was the first like truly big viral moment
[00:10:15.560 --> 00:10:17.760]   for generative AI.
[00:10:17.760 --> 00:10:19.680]   - Because of the avocado chair.
[00:10:19.680 --> 00:10:24.680]   - Because of the avocado chair and yeah, exactly.
[00:10:24.680 --> 00:10:26.040]   Yeah, it was just so novel.
[00:10:26.040 --> 00:10:28.000]   - It wasn't as big for me as "Stable Diffusion."
[00:10:28.000 --> 00:10:28.840]   - Really?
[00:10:28.840 --> 00:10:29.660]   - Yeah, I don't know.
[00:10:29.660 --> 00:10:31.220]   People was like, "All right, that's cool."
[00:10:31.220 --> 00:10:32.460]   I don't know. (laughs)
[00:10:32.460 --> 00:10:33.300]   - Yeah.
[00:10:33.300 --> 00:10:34.460]   - I mean, they had some flashy videos,
[00:10:34.460 --> 00:10:37.900]   but I never really, it didn't really register me as--
[00:10:37.900 --> 00:10:39.460]   - But just that moment of images
[00:10:39.460 --> 00:10:41.860]   was just such a viral, novel moment.
[00:10:41.860 --> 00:10:44.040]   I think it just blew people's mind.
[00:10:44.040 --> 00:10:46.540]   - Yeah, I mean, it was the first time
[00:10:46.540 --> 00:10:47.980]   I encountered Sam Altman
[00:10:47.980 --> 00:10:50.140]   'cause they had this "Dolly 2" hackathon.
[00:10:50.140 --> 00:10:51.620]   They opened up the OpenAI office
[00:10:51.620 --> 00:10:56.100]   for developers to walk in back when it wasn't as,
[00:10:56.100 --> 00:11:00.200]   I guess, much of a security issue as it is today.
[00:11:00.200 --> 00:11:01.600]   Maybe take us through the journey
[00:11:01.600 --> 00:11:03.940]   to decide to pivot into this.
[00:11:03.940 --> 00:11:06.060]   And also, choosing images.
[00:11:06.060 --> 00:11:08.580]   Obviously, you were inspired by "Dolly,"
[00:11:08.580 --> 00:11:13.140]   but there could be any number of AI companies
[00:11:13.140 --> 00:11:16.500]   and businesses that you could start in the widest one, right?
[00:11:16.500 --> 00:11:17.340]   - Yeah.
[00:11:17.340 --> 00:11:20.500]   - So there must be an idea maze from June to September.
[00:11:20.500 --> 00:11:22.500]   - Yeah, yeah, there definitely was.
[00:11:22.500 --> 00:11:24.300]   So I think at that time,
[00:11:24.300 --> 00:11:29.300]   Mighty, OpenAI was not quite as popular
[00:11:29.300 --> 00:11:32.300]   as it is all of a sudden now these days.
[00:11:32.300 --> 00:11:36.180]   But back then, I think they were more than happy.
[00:11:36.180 --> 00:11:38.900]   They had a lot more bandwidth to help anybody.
[00:11:38.900 --> 00:11:42.140]   And so we had been talking with the team there
[00:11:42.140 --> 00:11:43.820]   around trying to see if we could do
[00:11:43.820 --> 00:11:47.140]   really fast, low-latency address bar prediction
[00:11:47.140 --> 00:11:51.180]   with GPT-3 and 3.5 and that kind of thing.
[00:11:51.180 --> 00:11:54.220]   And so we were sort of figuring out
[00:11:54.220 --> 00:11:56.020]   how could we make that low-latency.
[00:11:56.020 --> 00:11:59.140]   I think that just being able to talk to them
[00:11:59.140 --> 00:12:01.660]   and kind of being involved gave me a bird's-eye view
[00:12:01.660 --> 00:12:03.960]   into a bunch of things that started to happen.
[00:12:03.960 --> 00:12:07.620]   Obviously, first was the "Dolly 2" moment,
[00:12:07.620 --> 00:12:09.280]   but then "Stable Diffusion" came out,
[00:12:09.280 --> 00:12:12.060]   and that was a big moment for me as well.
[00:12:12.060 --> 00:12:16.080]   And I remember just kind of sitting up one night thinking,
[00:12:16.080 --> 00:12:18.540]   I was like, "What are the kinds of companies
[00:12:18.540 --> 00:12:19.380]   "one could build?
[00:12:19.380 --> 00:12:20.740]   "What matters right now?"
[00:12:20.740 --> 00:12:23.420]   One thing that I observed is that I find a lot of great,
[00:12:23.420 --> 00:12:26.260]   I find a lot of inspiration when I'm working
[00:12:26.260 --> 00:12:27.500]   in a field in something,
[00:12:27.500 --> 00:12:29.620]   and then I can identify a bunch of problems.
[00:12:29.620 --> 00:12:32.340]   Like for Mixpanel, I was an intern at a company,
[00:12:32.340 --> 00:12:33.620]   and I just noticed that they were doing
[00:12:33.620 --> 00:12:34.660]   all this data analysis.
[00:12:34.660 --> 00:12:37.000]   And so I thought, "Hmm, I wonder if I could make a product,
[00:12:37.000 --> 00:12:38.500]   "and then maybe they would use it."
[00:12:38.500 --> 00:12:41.680]   And in this case, the same thing kind of occurred.
[00:12:41.680 --> 00:12:42.660]   It was like, okay, there are a bunch
[00:12:42.660 --> 00:12:46.640]   of infrastructure companies that are doing,
[00:12:46.640 --> 00:12:49.500]   they put a model up, and then you can use their API,
[00:12:49.500 --> 00:12:52.620]   like Replicate is a really good example of that.
[00:12:52.620 --> 00:12:54.620]   There are a bunch of companies that are helping you
[00:12:54.620 --> 00:12:59.620]   with training, model optimization, Mosaic at the time,
[00:12:59.620 --> 00:13:03.180]   and probably still was doing stuff like that.
[00:13:03.180 --> 00:13:06.340]   So I just started listing out every category of everything,
[00:13:06.340 --> 00:13:08.100]   of every company that was doing something interesting.
[00:13:08.100 --> 00:13:09.560]   Obviously, Weights & Biases.
[00:13:09.560 --> 00:13:12.100]   I was like, "Oh man, Weights & Biases
[00:13:12.100 --> 00:13:14.000]   "is this great company.
[00:13:14.000 --> 00:13:15.440]   "Do I want to compete with that company?
[00:13:15.440 --> 00:13:17.940]   "I might be really good at competing with that company."
[00:13:17.940 --> 00:13:21.380]   Because of Mixpanel, 'cause it's so much of analysis.
[00:13:21.380 --> 00:13:23.780]   I was like, "No, I don't want to do anything related to that.
[00:13:23.780 --> 00:13:26.480]   "I think that would be too boring now at this point."
[00:13:26.480 --> 00:13:30.300]   But, so I started to list out all these ideas,
[00:13:30.300 --> 00:13:32.820]   and one thing I observed was that at OpenAI,
[00:13:32.820 --> 00:13:35.620]   they have a playground for GPT-3, right?
[00:13:35.620 --> 00:13:38.060]   And all it was was just a text box, more or less.
[00:13:38.060 --> 00:13:39.540]   And then there were some settings on the right,
[00:13:39.540 --> 00:13:41.140]   like temperature and whatever.
[00:13:41.140 --> 00:13:43.340]   - Top K, Top N. - Yeah, Top K.
[00:13:43.340 --> 00:13:44.940]   What's your end stop sequence?
[00:13:44.940 --> 00:13:48.140]   I mean, that was like their product before chat GPT.
[00:13:48.140 --> 00:13:49.500]   You know, really difficult to use,
[00:13:49.500 --> 00:13:51.400]   but fun if you're like an engineer.
[00:13:51.400 --> 00:13:53.060]   And I just noticed that their product
[00:13:53.060 --> 00:13:54.460]   kind of was evolving a little bit,
[00:13:54.460 --> 00:13:56.700]   where the interface kind of was getting more and more,
[00:13:56.700 --> 00:13:58.020]   a little bit more complex.
[00:13:58.020 --> 00:13:59.420]   They had like a way where you could like,
[00:13:59.420 --> 00:14:01.340]   generate something in the middle of a sentence,
[00:14:01.340 --> 00:14:02.820]   and all those kinds of things.
[00:14:02.820 --> 00:14:04.140]   And I just thought to myself, I was like,
[00:14:04.140 --> 00:14:05.200]   "You know, there's not,
[00:14:05.200 --> 00:14:07.460]   "everything is just like this text box,
[00:14:07.460 --> 00:14:09.620]   "and you generate something, and that's about it."
[00:14:09.620 --> 00:14:11.220]   And Stable Diffusion had kind of come out,
[00:14:11.220 --> 00:14:13.540]   and it was all like hugging face and code.
[00:14:13.540 --> 00:14:15.820]   Nobody was really building any UI.
[00:14:15.820 --> 00:14:18.540]   And so I had this kind of thing where I wrote prompt dash,
[00:14:18.540 --> 00:14:20.460]   like question mark in my notes.
[00:14:20.460 --> 00:14:23.780]   And I didn't know what was like the product for that,
[00:14:23.780 --> 00:14:24.820]   at the time.
[00:14:24.820 --> 00:14:27.380]   I mean, it seems kind of trite now.
[00:14:27.380 --> 00:14:29.180]   But yeah, I just like wrote prompt.
[00:14:29.180 --> 00:14:30.020]   What's the thing for that?
[00:14:30.020 --> 00:14:32.100]   - Manager, prompt. - Prompt manager.
[00:14:32.100 --> 00:14:33.560]   Do you organize them?
[00:14:33.560 --> 00:14:35.860]   Like, do you like have a UI that can like--
[00:14:35.860 --> 00:14:37.280]   - Library. - Play with them?
[00:14:37.280 --> 00:14:38.340]   Yeah, like a library.
[00:14:38.340 --> 00:14:40.260]   What would you make?
[00:14:40.260 --> 00:14:41.700]   And so then of course, then you thought about,
[00:14:41.700 --> 00:14:44.420]   what would the modalities be, given that?
[00:14:44.420 --> 00:14:47.260]   How would you build a UI for each kind of modality?
[00:14:47.260 --> 00:14:48.620]   And so there were a couple people
[00:14:48.620 --> 00:14:51.100]   working on some pretty cool things.
[00:14:51.100 --> 00:14:54.300]   And I basically chose graphics
[00:14:54.300 --> 00:14:57.760]   because it seemed like the most obvious place
[00:14:57.760 --> 00:15:02.220]   where you could build a really powerful, complex UI
[00:15:02.220 --> 00:15:05.260]   that's not just only typing in a box.
[00:15:05.260 --> 00:15:07.820]   That it would very much evolve beyond that.
[00:15:07.820 --> 00:15:08.900]   Like, what would be the best thing
[00:15:08.900 --> 00:15:09.900]   for something that's visual?
[00:15:09.900 --> 00:15:11.300]   Probably something visual.
[00:15:11.300 --> 00:15:17.360]   So yeah, I think that just that progression kind of happened
[00:15:17.360 --> 00:15:19.860]   and it just seemed like there was a lot of effort
[00:15:19.860 --> 00:15:21.220]   going into language,
[00:15:21.220 --> 00:15:24.220]   but not a lot of effort going into graphics.
[00:15:24.220 --> 00:15:26.300]   And then maybe the very last thing was,
[00:15:26.300 --> 00:15:29.020]   I think I was talking to Aditya Ramesh,
[00:15:29.020 --> 00:15:32.780]   who is the co-creator of Dolly 2 and Sam.
[00:15:32.780 --> 00:15:34.180]   And I just kind of went to these guys
[00:15:34.180 --> 00:15:35.860]   and I was just like, hey,
[00:15:35.860 --> 00:15:38.660]   are you gonna make like a UI for this thing?
[00:15:38.660 --> 00:15:40.780]   Like a true UI, are you gonna go for this?
[00:15:40.780 --> 00:15:42.020]   Are you gonna make a product?
[00:15:42.020 --> 00:15:43.100]   - For Dolly, yeah.
[00:15:43.100 --> 00:15:44.700]   - For Dolly, yeah.
[00:15:44.700 --> 00:15:46.420]   Are you gonna do anything here?
[00:15:46.420 --> 00:15:47.940]   'Cause if you're not gonna do it,
[00:15:47.940 --> 00:15:49.100]   if you are gonna do it, just let me know
[00:15:49.100 --> 00:15:51.500]   and I will stop and I'll go do something else.
[00:15:51.500 --> 00:15:54.460]   But if you're not gonna do anything, I'll just do it.
[00:15:54.460 --> 00:15:55.780]   And so we had a couple of conversations
[00:15:55.780 --> 00:15:58.020]   around what that would look like.
[00:15:58.020 --> 00:15:59.620]   And then I think ultimately they decided
[00:15:59.620 --> 00:16:03.220]   that they were gonna focus on language primarily.
[00:16:03.220 --> 00:16:05.780]   And yeah, I just felt like
[00:16:05.780 --> 00:16:07.860]   it was gonna be very underinvested in.
[00:16:07.860 --> 00:16:11.260]   - Yes, there's that sort of underinvestment
[00:16:11.260 --> 00:16:14.420]   from OpenAI, which I can see that.
[00:16:14.420 --> 00:16:18.100]   But also it's a different type of customer
[00:16:18.100 --> 00:16:19.380]   than you're used to.
[00:16:19.380 --> 00:16:22.100]   Presumably, and Mixpanel are very good
[00:16:22.100 --> 00:16:24.620]   at selling to B2B developers.
[00:16:24.620 --> 00:16:26.180]   With Fairground, you're not.
[00:16:26.180 --> 00:16:27.020]   - Yeah.
[00:16:27.020 --> 00:16:28.540]   - Was that not a concern?
[00:16:28.540 --> 00:16:32.620]   - Well, not so much, because I think that right now
[00:16:32.620 --> 00:16:34.740]   I would say graphics is in this very nascent phase.
[00:16:34.740 --> 00:16:37.500]   Like most of the customers are just like hobbyists, right?
[00:16:37.500 --> 00:16:40.140]   Like it's a little bit of like a novel toy
[00:16:40.140 --> 00:16:42.980]   as opposed to being this like very high utility thing.
[00:16:42.980 --> 00:16:45.220]   But I think ultimately if you believe
[00:16:45.220 --> 00:16:47.260]   that you could make it very high utility,
[00:16:47.260 --> 00:16:50.460]   then probably the next customers will end up being B2B.
[00:16:50.460 --> 00:16:52.180]   It'll probably not be like consumer.
[00:16:52.180 --> 00:16:53.860]   Like there will certainly be a variation
[00:16:53.860 --> 00:16:55.500]   of this idea that's in consumer.
[00:16:55.500 --> 00:17:00.220]   If your quest is to kind of make like a super,
[00:17:00.220 --> 00:17:03.660]   something that surpasses human ability for graphics,
[00:17:03.660 --> 00:17:06.540]   like ultimately it will end up being used for business.
[00:17:06.540 --> 00:17:08.540]   So I think it's maybe more of a progression.
[00:17:08.540 --> 00:17:09.980]   In fact, for me, it's maybe more like
[00:17:09.980 --> 00:17:11.940]   Mixpanel started out as SMB,
[00:17:11.940 --> 00:17:13.340]   and then very much like ended up
[00:17:13.340 --> 00:17:14.940]   starting to grow up towards enterprise.
[00:17:14.940 --> 00:17:16.420]   So for me, it's a little,
[00:17:16.420 --> 00:17:18.340]   I think it will be a very similar progression.
[00:17:18.340 --> 00:17:19.540]   - Yeah, yeah.
[00:17:19.540 --> 00:17:21.340]   - But yeah, I mean, the reason why I was excited about it
[00:17:21.340 --> 00:17:22.860]   is 'cause it was a creative tool.
[00:17:22.860 --> 00:17:26.100]   I make music and it's AI.
[00:17:26.100 --> 00:17:28.100]   It's like something that I know I could stay up
[00:17:28.100 --> 00:17:30.400]   till three o'clock in the morning doing.
[00:17:30.400 --> 00:17:33.140]   Those are kind of like very simple bars for me.
[00:17:33.140 --> 00:17:33.980]   - Yeah. - Yeah.
[00:17:33.980 --> 00:17:35.900]   It's good decision criteria.
[00:17:35.900 --> 00:17:38.780]   - So you mentioned DALI, Stable Diffusion.
[00:17:38.780 --> 00:17:42.020]   You just had Playground V2 come out two days ago?
[00:17:42.020 --> 00:17:42.920]   - Yeah, two days ago, yeah.
[00:17:42.920 --> 00:17:43.760]   - Two days ago.
[00:17:43.760 --> 00:17:46.580]   So this is a model you train completely from scratch.
[00:17:46.580 --> 00:17:49.480]   So it's not a cheap fine tune on something.
[00:17:49.480 --> 00:17:52.740]   You open source everything, including the weights.
[00:17:52.740 --> 00:17:54.200]   Why did you decide to do it?
[00:17:54.200 --> 00:17:56.560]   I know you supported Stable Diffusion XL
[00:17:56.560 --> 00:17:58.220]   in Playground before, right?
[00:17:58.220 --> 00:17:59.380]   - Yep.
[00:17:59.380 --> 00:18:02.020]   - Yeah, what made you want to come up with V2
[00:18:02.020 --> 00:18:04.320]   and maybe some of the interesting,
[00:18:04.320 --> 00:18:06.180]   technical research work you've done?
[00:18:06.180 --> 00:18:12.100]   - Yeah, so I think that we continue to feel like graphics
[00:18:12.100 --> 00:18:16.900]   and these foundation models for anything really related
[00:18:16.900 --> 00:18:18.980]   to pixels, but also definitely images,
[00:18:18.980 --> 00:18:21.060]   continues to be very under-invested.
[00:18:21.060 --> 00:18:24.460]   It feels a little like graphics is in this GPT-2 moment,
[00:18:24.460 --> 00:18:27.140]   right, like even GPT-3.
[00:18:27.140 --> 00:18:29.320]   Even when GPT-3 came out, it was exciting.
[00:18:29.320 --> 00:18:30.980]   But it was like, what are you gonna use this for?
[00:18:30.980 --> 00:18:33.060]   You know, yeah, we'll do some text classification
[00:18:33.060 --> 00:18:34.740]   and some semantic analysis,
[00:18:34.740 --> 00:18:37.460]   and maybe it'll sometimes make a summary of something
[00:18:37.460 --> 00:18:38.500]   and it'll hallucinate.
[00:18:38.500 --> 00:18:41.120]   But no one really had a very significant
[00:18:41.120 --> 00:18:42.960]   business application for GPT-3.
[00:18:42.960 --> 00:18:46.500]   And in images, we're kind of stuck in the same place.
[00:18:46.500 --> 00:18:49.080]   We're kind of like, okay, I write this thing in a box
[00:18:49.080 --> 00:18:50.860]   and I get some cool piece of artwork
[00:18:50.860 --> 00:18:52.180]   and the hands are kind of messed up
[00:18:52.180 --> 00:18:54.500]   and sometimes the eyes are a little weird.
[00:18:54.500 --> 00:18:58.280]   Maybe I'll use it for a blog post, that kind of thing.
[00:18:58.280 --> 00:18:59.840]   The utility feels so limited.
[00:18:59.840 --> 00:19:02.320]   And so, you know, and then you sort of look
[00:19:02.320 --> 00:19:04.740]   at stable diffusion and we definitely use that model
[00:19:04.740 --> 00:19:07.260]   in our product and our users like it and use it
[00:19:07.260 --> 00:19:08.540]   and love it and enjoy it.
[00:19:08.540 --> 00:19:12.420]   But it hasn't gone nearly far enough.
[00:19:12.420 --> 00:19:14.500]   So we were kind of faced with the choice of, you know,
[00:19:14.500 --> 00:19:16.100]   do we wait for progress to occur
[00:19:16.100 --> 00:19:18.340]   or do we make that progress happen?
[00:19:18.340 --> 00:19:21.180]   So, yeah, we kind of embarked on a plan
[00:19:21.180 --> 00:19:24.380]   to just decide to go train these things from scratch.
[00:19:24.380 --> 00:19:27.020]   And I think the community has given us so much.
[00:19:27.020 --> 00:19:28.740]   The community for stable diffusion, I think,
[00:19:28.740 --> 00:19:31.900]   is one of the most vibrant communities on the internet.
[00:19:31.900 --> 00:19:33.360]   It's like amazing.
[00:19:33.360 --> 00:19:36.680]   It feels like, I hope this is what Homebrew Club felt like
[00:19:36.680 --> 00:19:39.220]   when computers showed up because it's like amazing
[00:19:39.220 --> 00:19:40.460]   what that community will do.
[00:19:40.460 --> 00:19:42.060]   And it moves so fast.
[00:19:42.060 --> 00:19:44.540]   I've never seen anything in my life where so far,
[00:19:44.540 --> 00:19:46.540]   and heard other people's stories around this,
[00:19:46.540 --> 00:19:50.200]   where a research, an academic research paper comes out
[00:19:50.200 --> 00:19:53.660]   and then like two days later, someone has sample code for it
[00:19:53.660 --> 00:19:55.180]   and then two days later, there's a model
[00:19:55.180 --> 00:19:57.780]   and then two days later, it's like in nine products.
[00:19:57.780 --> 00:19:58.620]   - Yeah.
[00:19:58.620 --> 00:19:59.540]   - Competing with each other.
[00:19:59.540 --> 00:20:00.380]   - Yeah.
[00:20:00.380 --> 00:20:01.780]   - It's incredible to see like math symbols
[00:20:01.780 --> 00:20:04.960]   on an academic paper go to features,
[00:20:04.960 --> 00:20:06.980]   well-designed features in a product.
[00:20:06.980 --> 00:20:10.020]   So I think the community has done so much.
[00:20:10.020 --> 00:20:12.180]   So I think we wanted to give back to the community
[00:20:12.180 --> 00:20:13.020]   kind of on our way.
[00:20:13.020 --> 00:20:15.300]   We knew it wasn't going to be,
[00:20:15.300 --> 00:20:17.140]   we knew it was not ever going to be,
[00:20:17.140 --> 00:20:18.500]   certainly we would train a better model
[00:20:18.500 --> 00:20:21.540]   than what we gave out on Tuesday.
[00:20:21.540 --> 00:20:24.220]   But we definitely felt like there needs to be
[00:20:24.220 --> 00:20:27.740]   some kind of progress in these open source models.
[00:20:27.740 --> 00:20:30.260]   The last kind of milestone was in July
[00:20:30.260 --> 00:20:31.900]   when Stable Diffusion Excel came out,
[00:20:31.900 --> 00:20:34.500]   but there hasn't been anything really since, right?
[00:20:34.500 --> 00:20:36.380]   - And there's Excel Turbo now.
[00:20:36.380 --> 00:20:38.900]   - Well, Excel Turbo is like this distilled model, right?
[00:20:38.900 --> 00:20:40.780]   So it's like lower quality, but fast.
[00:20:40.780 --> 00:20:43.460]   You have to decide what your trade-off is there.
[00:20:43.460 --> 00:20:46.100]   - And it's also a consistency model?
[00:20:46.100 --> 00:20:48.140]   - It's not, I don't think it's a consistency model.
[00:20:48.140 --> 00:20:50.260]   It's like, they did like a different thing.
[00:20:50.260 --> 00:20:51.100]   - Yeah.
[00:20:51.100 --> 00:20:51.940]   - Yeah, I think it's like,
[00:20:51.940 --> 00:20:53.460]   I don't want to get quoted for this,
[00:20:53.460 --> 00:20:54.900]   but it's like something called ad,
[00:20:54.900 --> 00:20:56.460]   like adversarial something or another.
[00:20:56.460 --> 00:20:58.340]   - That's exactly right.
[00:20:58.340 --> 00:21:00.980]   - Yeah, I think it's, I've read something about that.
[00:21:00.980 --> 00:21:02.380]   Maybe it's like closer to GANs or something,
[00:21:02.380 --> 00:21:04.020]   but I didn't really read the full paper.
[00:21:04.020 --> 00:21:06.820]   But yeah, there hasn't been quite enough progress
[00:21:06.820 --> 00:21:09.780]   in terms of, you know, there's no multitask image model.
[00:21:09.780 --> 00:21:11.180]   You know, the closest thing would be something called
[00:21:11.180 --> 00:21:13.940]   like EmuEdit, but there's no model for that.
[00:21:13.940 --> 00:21:16.140]   It's just a paper that's within meta.
[00:21:16.140 --> 00:21:20.780]   So we did that and we also gave out pre-trained weights,
[00:21:20.780 --> 00:21:22.300]   which is very rare.
[00:21:22.300 --> 00:21:24.020]   Usually you just get the aligned model
[00:21:24.020 --> 00:21:25.180]   and then you have to like,
[00:21:25.180 --> 00:21:26.840]   see if you can do anything with it.
[00:21:26.840 --> 00:21:28.260]   We actually gave out,
[00:21:28.260 --> 00:21:32.460]   there's like a 256 pixel pre-trained stage and a 512.
[00:21:32.460 --> 00:21:34.100]   And we did that for academic research,
[00:21:34.100 --> 00:21:35.020]   'cause there's a whole bunch of,
[00:21:35.020 --> 00:21:36.780]   we come across people all the time in academia
[00:21:36.780 --> 00:21:37.620]   and they have like,
[00:21:37.620 --> 00:21:42.060]   they have access to like one A100 or eight at best.
[00:21:42.060 --> 00:21:45.220]   And so if we can give them kind of like a 512
[00:21:45.220 --> 00:21:47.740]   pre-trained model, it might,
[00:21:47.740 --> 00:21:50.340]   our hope is that there'll be interesting novel research
[00:21:50.340 --> 00:21:51.660]   that occurs from that.
[00:21:51.660 --> 00:21:53.900]   - What research do you want to happen?
[00:21:53.900 --> 00:21:56.620]   - I would love to see more research around,
[00:21:56.620 --> 00:21:57.900]   you know, things that users care about
[00:21:57.900 --> 00:22:00.660]   tend to be things like character consistency.
[00:22:00.660 --> 00:22:02.180]   - Between frames?
[00:22:02.180 --> 00:22:03.900]   - More like if you have like a face.
[00:22:03.900 --> 00:22:05.420]   Yeah, yeah, basically between frames,
[00:22:05.420 --> 00:22:06.300]   but more just like, you know,
[00:22:06.300 --> 00:22:08.620]   you have your face and it's in, you know,
[00:22:08.620 --> 00:22:10.980]   one image and then you want it to be like in another.
[00:22:10.980 --> 00:22:12.940]   And users are very particular
[00:22:12.940 --> 00:22:14.260]   and sensitive to faces changing.
[00:22:14.260 --> 00:22:16.900]   'Cause we know, we know what, you know,
[00:22:16.900 --> 00:22:19.140]   we're trained on faces as humans.
[00:22:19.140 --> 00:22:21.900]   And, you know, that's something I don't,
[00:22:21.900 --> 00:22:23.380]   I'm not seeing a lot of innovation,
[00:22:23.380 --> 00:22:26.820]   enough innovation around multitask editing.
[00:22:26.820 --> 00:22:28.860]   You know, there are two things like instruct pics to pics
[00:22:28.860 --> 00:22:33.140]   and then the emu edit paper that are maybe very interesting,
[00:22:33.140 --> 00:22:36.460]   but we certainly are not pushing the fold on that
[00:22:36.460 --> 00:22:37.340]   in that regard.
[00:22:37.340 --> 00:22:43.220]   It just, all kinds of things like around that rotation,
[00:22:43.220 --> 00:22:46.740]   you know, being able to keep coherence across images,
[00:22:46.740 --> 00:22:48.740]   style transfer is still very limited.
[00:22:48.740 --> 00:22:52.100]   Just even reasoning around images, you know,
[00:22:52.100 --> 00:22:54.820]   what's going on in an image, that kind of thing.
[00:22:54.820 --> 00:22:57.820]   Things are still very, very underpowered, very nascent.
[00:22:57.820 --> 00:23:01.140]   So therefore the utility is very, very limited.
[00:23:01.140 --> 00:23:02.780]   - On the 1K Prompt Benchmark,
[00:23:02.780 --> 00:23:06.740]   you are 2.5X prefer to stable diffusion Excel.
[00:23:06.740 --> 00:23:07.580]   How do you get there?
[00:23:07.580 --> 00:23:10.540]   Is it better images in the training corpus?
[00:23:10.540 --> 00:23:13.660]   Is it, yeah, can you maybe talk through
[00:23:13.660 --> 00:23:15.660]   the improvements in the model?
[00:23:15.660 --> 00:23:18.140]   - I think they're still very early on in the recipe,
[00:23:18.140 --> 00:23:21.620]   but I think it's a lot of like little things.
[00:23:21.620 --> 00:23:22.780]   And, you know, every now and then
[00:23:22.780 --> 00:23:24.260]   there are some big important things.
[00:23:24.260 --> 00:23:26.860]   Like certainly your data quality
[00:23:26.860 --> 00:23:28.020]   is really, really important.
[00:23:28.020 --> 00:23:30.900]   So we spend a lot of time thinking about that.
[00:23:30.900 --> 00:23:34.140]   But I would say it's a lot of things
[00:23:34.140 --> 00:23:35.660]   that you kind of clean up along the way
[00:23:35.660 --> 00:23:37.020]   as you train your model.
[00:23:37.020 --> 00:23:40.980]   Everything from captions to the data that you align with
[00:23:40.980 --> 00:23:44.380]   after pre-train to how you're picking your data sets,
[00:23:44.380 --> 00:23:46.920]   how you filter your data sets.
[00:23:46.920 --> 00:23:49.700]   There's a lot, I feel like there's a lot of work in AI
[00:23:49.700 --> 00:23:52.060]   that's like, doesn't really feel like AI.
[00:23:52.060 --> 00:23:55.220]   It just really feels like just data set filtering
[00:23:55.220 --> 00:23:56.260]   and systems engineering.
[00:23:56.260 --> 00:23:58.460]   And just like, you know, and the recipe is all there,
[00:23:58.460 --> 00:24:01.580]   but it's like a lot of extra work to do that.
[00:24:01.580 --> 00:24:04.420]   So I think these models, I think whatever version,
[00:24:04.420 --> 00:24:08.220]   I think we plan to do a Playground V 2.1,
[00:24:08.220 --> 00:24:10.940]   maybe either by the end of the year or early next year.
[00:24:10.940 --> 00:24:13.100]   And we're just like watching what the community does
[00:24:13.100 --> 00:24:14.300]   with the model.
[00:24:14.300 --> 00:24:16.060]   And then we're just gonna take a lot of the things
[00:24:16.060 --> 00:24:18.560]   that they're unhappy about and just like fix them.
[00:24:19.520 --> 00:24:23.560]   You know, so for example, like maybe the eyes of people
[00:24:23.560 --> 00:24:25.840]   in an image don't feel right.
[00:24:25.840 --> 00:24:27.800]   They feel like they're a little misshapen
[00:24:27.800 --> 00:24:29.600]   or they're kind of blurry feeling.
[00:24:29.600 --> 00:24:31.320]   That's something that we already know we wanna fix.
[00:24:31.320 --> 00:24:34.600]   So I think in that case, it's gonna be about data quality.
[00:24:34.600 --> 00:24:37.600]   Or maybe you wanna improve the kind of the dynamic range
[00:24:37.600 --> 00:24:38.440]   of color.
[00:24:38.440 --> 00:24:40.280]   You know, we wanna make sure that that's like got a good
[00:24:40.280 --> 00:24:41.300]   range in any image.
[00:24:41.300 --> 00:24:43.000]   So what technique can we use there?
[00:24:43.000 --> 00:24:45.960]   There's different things like offset noise, pyramid noise,
[00:24:45.960 --> 00:24:47.120]   terminal zero SNR.
[00:24:47.120 --> 00:24:49.080]   Like there are all these various interesting things
[00:24:49.080 --> 00:24:49.920]   that you can do.
[00:24:49.920 --> 00:24:52.200]   So I think it's like a lot of just like tricks.
[00:24:52.200 --> 00:24:53.360]   Some are tricks, some are data,
[00:24:53.360 --> 00:24:55.880]   and some is just like cleaning.
[00:24:55.880 --> 00:24:57.220]   Yeah.
[00:24:57.220 --> 00:25:01.320]   - Specifically for faces, it's very common to use a pipeline
[00:25:01.320 --> 00:25:05.400]   rather than just train the base model more.
[00:25:05.400 --> 00:25:08.440]   Do you have a strong belief either way on like,
[00:25:08.440 --> 00:25:10.720]   oh, they should be separated out to different stages
[00:25:10.720 --> 00:25:12.640]   for like improving the eyes, improving the face
[00:25:12.640 --> 00:25:14.440]   or enhance or whatever?
[00:25:14.440 --> 00:25:17.440]   Or do you think like it can all be done in one model?
[00:25:17.440 --> 00:25:19.320]   - I think we will make a unified model.
[00:25:19.320 --> 00:25:20.160]   - Okay.
[00:25:20.160 --> 00:25:21.680]   - Yeah, I think we'll certainly in the end,
[00:25:21.680 --> 00:25:23.320]   ultimately make a unified model.
[00:25:23.320 --> 00:25:29.960]   There's not enough research about this.
[00:25:29.960 --> 00:25:32.220]   Maybe there is something out there that we haven't read.
[00:25:32.220 --> 00:25:35.800]   There are some bottlenecks, like for example, in the VAE,
[00:25:35.800 --> 00:25:38.120]   like the VAEs are ultimately like compressing these things.
[00:25:38.120 --> 00:25:39.880]   And so you don't know, and then you might have
[00:25:39.880 --> 00:25:42.800]   like a big information bottleneck.
[00:25:42.800 --> 00:25:45.520]   So maybe you would use a pixel based model, perhaps.
[00:25:45.520 --> 00:25:48.280]   You know, there's a lot of belief.
[00:25:48.280 --> 00:25:51.300]   I think we've talked to people, everyone from like Rombach
[00:25:51.300 --> 00:25:54.520]   to various people, Rombach trained stable diffusion.
[00:25:54.520 --> 00:25:56.760]   You know, I think there's like a big question
[00:25:56.760 --> 00:25:59.360]   around the architecture of these things.
[00:25:59.360 --> 00:26:01.360]   It's still kind of unknown, right?
[00:26:01.360 --> 00:26:03.400]   Like we've got transformers
[00:26:03.400 --> 00:26:06.440]   and we've got like a GPT architecture model,
[00:26:06.440 --> 00:26:07.800]   but then there's this like weird thing
[00:26:07.800 --> 00:26:10.240]   that's also seemingly working with diffusion.
[00:26:10.240 --> 00:26:12.520]   And so, you know, are we going to use vision transformers?
[00:26:12.520 --> 00:26:14.340]   Are we going to move to pixel based models?
[00:26:14.340 --> 00:26:16.360]   Is there a different kind of architecture?
[00:26:16.360 --> 00:26:17.800]   We don't really, I don't think there have been
[00:26:17.800 --> 00:26:19.320]   enough experiments in this regard.
[00:26:19.320 --> 00:26:21.200]   - Still? Oh my God.
[00:26:21.200 --> 00:26:22.680]   - Yeah. - That's surprising.
[00:26:22.680 --> 00:26:25.120]   - Yeah, I think it's very computationally expensive
[00:26:25.120 --> 00:26:28.080]   to do a pipeline model where you're like fixing the eyes
[00:26:28.080 --> 00:26:28.920]   and you're fixing the mouth and you're fixing the hands.
[00:26:28.920 --> 00:26:31.340]   - That's what everyone does as far as I understand.
[00:26:31.340 --> 00:26:33.320]   - Well, I'm not sure, I'm not exactly sure what you mean,
[00:26:33.320 --> 00:26:35.260]   but if you mean like you get an image
[00:26:35.260 --> 00:26:37.280]   and then you will like make another model
[00:26:37.280 --> 00:26:38.940]   specifically to fix a face.
[00:26:38.940 --> 00:26:40.640]   Yeah, I think that's a very computationally,
[00:26:40.640 --> 00:26:42.200]   that's fairly computationally expensive.
[00:26:42.200 --> 00:26:43.320]   And I think it's like not,
[00:26:43.320 --> 00:26:45.320]   probably not the right thing, right way.
[00:26:45.320 --> 00:26:46.160]   - Yeah. - Yeah.
[00:26:46.160 --> 00:26:47.760]   And it doesn't generalize very well.
[00:26:47.760 --> 00:26:49.280]   Now you have to pick all these different things.
[00:26:49.280 --> 00:26:51.120]   - Yeah, you're just kind of glomming things on together.
[00:26:51.120 --> 00:26:54.380]   Like when I look at AI artists, like that's what they do.
[00:26:54.380 --> 00:26:55.640]   - Ah, yeah, yeah, yeah.
[00:26:55.640 --> 00:26:57.760]   They'll do things like, you know,
[00:26:57.760 --> 00:26:59.320]   I think a lot of ARs will do, you know,
[00:26:59.320 --> 00:27:01.920]   control net tiling to do kind of generative upscaling
[00:27:01.920 --> 00:27:04.140]   of all these different pieces of the image.
[00:27:04.140 --> 00:27:05.920]   Yeah, I mean, to me, these are all just like,
[00:27:05.920 --> 00:27:08.280]   they're all hacks, ultimately in the end.
[00:27:08.280 --> 00:27:09.480]   I mean, it just, to me, it's like,
[00:27:09.480 --> 00:27:12.240]   let's go back to where we were just three years,
[00:27:12.240 --> 00:27:14.920]   four years ago with where deep learning was at
[00:27:14.920 --> 00:27:16.600]   and where language was at.
[00:27:16.600 --> 00:27:17.440]   You know, it's the same thing.
[00:27:17.440 --> 00:27:18.360]   It's like, we were like, okay,
[00:27:18.360 --> 00:27:21.200]   well, I'll just train these very narrow models
[00:27:21.200 --> 00:27:23.200]   to try to do these things and kind of ensemble them
[00:27:23.200 --> 00:27:25.600]   or pipeline them to try to get to a best-in-class result.
[00:27:25.600 --> 00:27:29.400]   And here we are with like where the models are gigantic
[00:27:29.400 --> 00:27:33.440]   and like very capable of solving huge amounts of tasks
[00:27:33.440 --> 00:27:35.200]   when given like lots of great data.
[00:27:35.200 --> 00:27:38.520]   So, yeah. - Makes sense.
[00:27:38.520 --> 00:27:42.480]   You also released a new benchmark called MJHQ-30K
[00:27:42.480 --> 00:27:45.960]   for automatic evaluation of a model's aesthetic quality.
[00:27:45.960 --> 00:27:48.680]   I have one question.
[00:27:48.680 --> 00:27:51.020]   The dataset that you use for the benchmark
[00:27:51.020 --> 00:27:52.440]   is from MidJourney. - Yes.
[00:27:52.440 --> 00:27:54.120]   - You have 10 categories.
[00:27:54.120 --> 00:27:58.720]   How do you think about the Playground model, MidJourney?
[00:27:58.720 --> 00:27:59.840]   - You know, there are a lot of people,
[00:27:59.840 --> 00:28:02.500]   a lot of people in research like to come up with,
[00:28:02.500 --> 00:28:03.640]   they like to compare themselves
[00:28:03.640 --> 00:28:06.760]   to something they know they can beat, right?
[00:28:06.760 --> 00:28:09.800]   But maybe this is the best reason why
[00:28:09.800 --> 00:28:12.840]   it can be helpful to not be a researcher also sometimes.
[00:28:12.840 --> 00:28:15.320]   Like I'm not like trained as a researcher.
[00:28:15.320 --> 00:28:19.120]   I don't have a PhD in anything AI related, for example.
[00:28:19.120 --> 00:28:21.880]   But I think if you care about products
[00:28:21.880 --> 00:28:23.400]   and you care about your users,
[00:28:23.400 --> 00:28:25.720]   then the most important thing that you wanna figure out
[00:28:25.720 --> 00:28:28.080]   is like everyone has to acknowledge
[00:28:28.080 --> 00:28:30.080]   that MidJourney is very good.
[00:28:30.080 --> 00:28:32.760]   You know, they are the best at this thing.
[00:28:32.760 --> 00:28:34.840]   We would, I would happily, I'm happy to admit that.
[00:28:34.840 --> 00:28:37.520]   I have no problem admitting that.
[00:28:37.520 --> 00:28:38.760]   It's just easy.
[00:28:38.760 --> 00:28:40.680]   It's very visual to tell.
[00:28:40.680 --> 00:28:43.720]   So, you know, I think it's incumbent on us
[00:28:43.720 --> 00:28:45.720]   to try to compare ourselves to the thing that's best,
[00:28:45.720 --> 00:28:50.040]   even if we lose, even if we're not the best, right?
[00:28:50.040 --> 00:28:53.060]   And, you know, at some point,
[00:28:53.060 --> 00:28:55.440]   if we are able to surpass MidJourney,
[00:28:55.440 --> 00:28:58.360]   then we only have ourselves to compare ourselves to.
[00:28:58.360 --> 00:29:00.020]   But on first blush, you know,
[00:29:00.020 --> 00:29:01.360]   I think it's worth comparing yourself
[00:29:01.360 --> 00:29:03.860]   to maybe the best thing and try to find
[00:29:03.860 --> 00:29:06.320]   like a really fair way of doing that.
[00:29:06.320 --> 00:29:08.680]   So I think more people should try to do that.
[00:29:08.680 --> 00:29:09.960]   I definitely don't think you should be
[00:29:09.960 --> 00:29:13.480]   kind of comparing yourself on like some Google model
[00:29:13.480 --> 00:29:16.680]   or some old SD, you know, stable diffusion model
[00:29:16.680 --> 00:29:19.640]   and be like, look, we beat, you know, stable diffusion 1.5.
[00:29:19.640 --> 00:29:23.380]   I think users ultimately want care, you know,
[00:29:23.380 --> 00:29:24.520]   how close are you getting to the thing
[00:29:24.520 --> 00:29:28.380]   that like I also mostly, people mostly agree with.
[00:29:28.380 --> 00:29:31.280]   So we put out that benchmark not because,
[00:29:31.280 --> 00:29:32.840]   for no other reason to say like,
[00:29:32.840 --> 00:29:35.280]   this seems like a worthy thing for us to at least try,
[00:29:35.280 --> 00:29:37.600]   you know, for people to try to get to.
[00:29:37.600 --> 00:29:38.760]   And then if we surpass it, great,
[00:29:38.760 --> 00:29:40.080]   we'll come up with another one.
[00:29:40.080 --> 00:29:41.000]   - Yeah, no, that's awesome.
[00:29:41.000 --> 00:29:45.240]   And you kill stable diffusion Excel and everything.
[00:29:45.240 --> 00:29:47.960]   In the benchmark chart,
[00:29:47.960 --> 00:29:51.680]   it says Playground V2 1024 pixel dash aesthetic.
[00:29:51.680 --> 00:29:53.720]   - Yes. - You have kind of like,
[00:29:53.720 --> 00:29:55.680]   yeah, style fine tunes or like,
[00:29:55.680 --> 00:29:57.960]   what's the dash aesthetic for?
[00:29:57.960 --> 00:30:00.080]   - We debated this, maybe we named it wrong or something,
[00:30:00.080 --> 00:30:03.400]   but we were like, how do we help people realize
[00:30:03.400 --> 00:30:06.520]   the model that's aligned versus the models that weren't.
[00:30:06.520 --> 00:30:09.120]   So because we gave out pre-trained models,
[00:30:09.120 --> 00:30:11.920]   we didn't want people to like use those.
[00:30:11.920 --> 00:30:13.520]   So that's why they're called base.
[00:30:13.520 --> 00:30:15.120]   And then the aesthetic model, yeah,
[00:30:15.120 --> 00:30:16.600]   we wanted people to pick up the thing
[00:30:16.600 --> 00:30:18.560]   that we thought would be like the thing
[00:30:18.560 --> 00:30:19.980]   that makes things pretty.
[00:30:19.980 --> 00:30:22.680]   Who wouldn't want the thing that's aesthetic?
[00:30:22.680 --> 00:30:25.040]   But if there's a better name,
[00:30:25.040 --> 00:30:26.840]   we definitely are open to feedback.
[00:30:26.840 --> 00:30:28.040]   - No, no, that's cool.
[00:30:28.040 --> 00:30:29.000]   I was using the product.
[00:30:29.000 --> 00:30:31.080]   You also have the style filter
[00:30:31.080 --> 00:30:33.000]   and you have all these different style.
[00:30:33.000 --> 00:30:35.920]   And it seems like the styles are tied to the model.
[00:30:35.920 --> 00:30:38.800]   So there's some like SDXL styles,
[00:30:38.800 --> 00:30:41.320]   there's some Playground V2 styles.
[00:30:41.320 --> 00:30:45.120]   Can you maybe give listeners an overview of how that works?
[00:30:45.120 --> 00:30:49.040]   Because in language, there's not this idea of like style,
[00:30:49.040 --> 00:30:52.640]   right, versus like in vision model there is,
[00:30:52.640 --> 00:30:55.640]   and you cannot get certain styles in different models.
[00:30:55.640 --> 00:30:56.920]   How do styles emerge
[00:30:56.920 --> 00:30:59.160]   and how do you categorize them and find them?
[00:30:59.160 --> 00:31:01.560]   - Yeah, I mean, it's so fun having a community
[00:31:01.560 --> 00:31:03.360]   where people are just trying a model.
[00:31:03.360 --> 00:31:05.880]   Like it's only been two days for Playground V2
[00:31:05.880 --> 00:31:09.680]   and we actually don't know what the model's capable of
[00:31:09.680 --> 00:31:10.600]   and not capable of.
[00:31:10.600 --> 00:31:12.600]   You know, we certainly see problems with it,
[00:31:12.600 --> 00:31:16.520]   but we have yet to see what emergent behavior is.
[00:31:16.520 --> 00:31:17.960]   I mean, we've just sort of discovered
[00:31:17.960 --> 00:31:19.680]   that it takes about like a week
[00:31:19.680 --> 00:31:21.880]   before you start to see like new things.
[00:31:21.880 --> 00:31:24.080]   But I think like a lot of that style
[00:31:24.080 --> 00:31:26.400]   kind of emerges after that week
[00:31:26.400 --> 00:31:28.640]   where you start to see, you know,
[00:31:28.640 --> 00:31:30.560]   there's some styles that are very like well-known to us,
[00:31:30.560 --> 00:31:33.560]   like maybe like pixel art is a well-known style.
[00:31:33.560 --> 00:31:34.720]   But then there's some style,
[00:31:34.720 --> 00:31:36.160]   photo realism is like another one
[00:31:36.160 --> 00:31:38.200]   that's like well-known to us.
[00:31:38.200 --> 00:31:41.880]   But there are some styles that cannot be easily named.
[00:31:41.880 --> 00:31:43.880]   You know, it's not as simple as like,
[00:31:43.880 --> 00:31:45.800]   okay, that's an anime style.
[00:31:45.800 --> 00:31:47.840]   It's very visual.
[00:31:47.840 --> 00:31:50.760]   And in the end, you end up making up the name
[00:31:50.760 --> 00:31:52.040]   for what that style represents.
[00:31:52.040 --> 00:31:55.040]   And so the community kind of shapes itself
[00:31:55.040 --> 00:31:56.320]   around these different things.
[00:31:56.320 --> 00:31:58.920]   And so if anyone that's into stable diffusion
[00:31:58.920 --> 00:32:01.960]   and into building anything with graphics and stuff
[00:32:01.960 --> 00:32:03.240]   with these models, you know,
[00:32:03.240 --> 00:32:07.080]   you might've heard of like ProtoVision or DreamShaper,
[00:32:07.080 --> 00:32:09.200]   some of these weird names.
[00:32:09.200 --> 00:32:11.120]   But they're just, you know, invented by these authors,
[00:32:11.120 --> 00:32:13.000]   but they have a sort of je ne sais quoi
[00:32:13.000 --> 00:32:14.960]   that, you know, appeals to users.
[00:32:14.960 --> 00:32:18.640]   - Because it like roughly embeds to what you want.
[00:32:18.640 --> 00:32:21.240]   - I guess so.
[00:32:21.240 --> 00:32:22.080]   I mean, it's like, you know,
[00:32:22.080 --> 00:32:24.400]   there's this one of my favorite ones that's fine-tuned.
[00:32:24.400 --> 00:32:25.560]   It's not made by us.
[00:32:25.560 --> 00:32:28.080]   It's called like Starlight XL.
[00:32:28.080 --> 00:32:30.160]   It's just this beautiful model.
[00:32:30.160 --> 00:32:33.960]   It's got really great color contrast and visual elements.
[00:32:33.960 --> 00:32:35.280]   And the users love it.
[00:32:35.280 --> 00:32:36.240]   I love it.
[00:32:36.240 --> 00:32:38.960]   And yeah, it's so hard.
[00:32:38.960 --> 00:32:41.280]   I think that's like a very big open question with graphics
[00:32:41.280 --> 00:32:44.040]   that I'm not totally sure how we'll solve.
[00:32:44.040 --> 00:32:47.040]   Yeah, I think a lot of styles are sort of,
[00:32:47.040 --> 00:32:49.560]   I don't know, it's like an evolving situation too,
[00:32:49.560 --> 00:32:51.320]   'cause styles get boring, right?
[00:32:51.320 --> 00:32:52.160]   They get fatigued.
[00:32:52.160 --> 00:32:55.400]   It's like listening to the same style of pop song.
[00:32:55.400 --> 00:32:57.920]   I kind of, I try to relate to graphics
[00:32:57.920 --> 00:32:59.240]   a little bit like with music,
[00:32:59.240 --> 00:33:01.400]   because I think it gives you a little bit
[00:33:01.400 --> 00:33:02.600]   of a different shape to things.
[00:33:02.600 --> 00:33:04.760]   Like in music, it's not just,
[00:33:04.760 --> 00:33:06.440]   it's not as if we just have pop music
[00:33:06.440 --> 00:33:09.040]   and, you know, rap music and country music.
[00:33:09.040 --> 00:33:10.680]   Like they're all of these,
[00:33:10.680 --> 00:33:14.040]   like the EDM genre alone has like sub genres.
[00:33:14.040 --> 00:33:16.160]   And I think that's very true in graphics
[00:33:16.160 --> 00:33:19.080]   and painting and art and anything that we're doing.
[00:33:19.080 --> 00:33:20.400]   There's just these sub genres,
[00:33:20.400 --> 00:33:22.760]   even if we can't quite always name them.
[00:33:22.760 --> 00:33:24.760]   But I think they are emergent from the community,
[00:33:24.760 --> 00:33:26.120]   which is why we're so always happy
[00:33:26.120 --> 00:33:27.160]   to work with the community.
[00:33:27.160 --> 00:33:29.680]   - Yeah, that is a struggle, you know,
[00:33:29.680 --> 00:33:32.480]   coming back to this, like B2B versus B2C thing.
[00:33:32.480 --> 00:33:35.040]   B2C, you're gonna have a huge amount of diversity
[00:33:35.040 --> 00:33:36.920]   and then it's gonna reduce as you get towards
[00:33:36.920 --> 00:33:38.560]   more sort of B2B type use cases.
[00:33:38.560 --> 00:33:41.280]   I'm making this up here, tell me if you disagree.
[00:33:41.280 --> 00:33:44.040]   So like you might be optimizing for a thing
[00:33:44.040 --> 00:33:45.840]   that you may eventually not need.
[00:33:45.840 --> 00:33:46.960]   - Yeah, possibly.
[00:33:46.960 --> 00:33:48.320]   Yeah, possibly.
[00:33:48.320 --> 00:33:49.320]   Yeah, I try not to share,
[00:33:49.320 --> 00:33:51.120]   I think like a simple thing with startups
[00:33:51.120 --> 00:33:55.040]   is that I worry sometimes by trying to be
[00:33:55.040 --> 00:33:59.440]   overly ambitious and like really scrutinizing
[00:33:59.440 --> 00:34:01.440]   like what something is in its most nascent phase
[00:34:01.440 --> 00:34:03.960]   that you miss the most ambitious thing you could have done.
[00:34:03.960 --> 00:34:06.840]   Like just having like very basic curiosity
[00:34:06.840 --> 00:34:09.600]   with something very small
[00:34:09.600 --> 00:34:13.040]   can like kind of lead you to something amazing.
[00:34:13.040 --> 00:34:14.280]   Like Einstein definitely did that.
[00:34:14.280 --> 00:34:16.480]   And then when, and then he like, you know,
[00:34:16.480 --> 00:34:17.880]   he basically won all the prizes
[00:34:17.880 --> 00:34:19.080]   and got everything he wanted
[00:34:19.080 --> 00:34:20.240]   and then basically did like kind,
[00:34:20.240 --> 00:34:24.080]   didn't really, he kind of dismissed quantum
[00:34:24.080 --> 00:34:26.960]   and then just kind of was still searching, you know,
[00:34:26.960 --> 00:34:28.200]   for the unifying theory.
[00:34:28.200 --> 00:34:29.760]   And he like had this quest.
[00:34:29.760 --> 00:34:31.760]   I think that happens a lot with like Nobel prize people.
[00:34:31.760 --> 00:34:34.200]   I think there's like a term for it that I forget.
[00:34:34.200 --> 00:34:39.180]   I actually wanted to go after a toy almost intentionally.
[00:34:39.180 --> 00:34:42.040]   So long as that I could see,
[00:34:42.040 --> 00:34:45.360]   I could imagine that it would lead to something
[00:34:45.360 --> 00:34:47.080]   very, very large later.
[00:34:47.080 --> 00:34:50.760]   And so, yeah, it's a very, like I said, it's very hobbyist,
[00:34:50.760 --> 00:34:53.200]   but you need to start somewhere.
[00:34:53.200 --> 00:34:54.400]   You need to start with something
[00:34:54.400 --> 00:34:58.220]   that has a big gravitational pull,
[00:34:58.220 --> 00:35:01.220]   even if these hobbyists aren't likely to be the people
[00:35:01.220 --> 00:35:04.080]   that, you know, have a way to monetize it or whatever,
[00:35:04.080 --> 00:35:05.460]   even if they're, but they're doing it for fun.
[00:35:05.460 --> 00:35:07.160]   So there's something there
[00:35:07.160 --> 00:35:08.700]   that I think is really important.
[00:35:08.700 --> 00:35:11.160]   But I agree with you that, you know, in time,
[00:35:11.160 --> 00:35:12.000]   we're gonna have to focus,
[00:35:12.000 --> 00:35:16.400]   we will absolutely focus on more utilitarian things,
[00:35:16.400 --> 00:35:18.760]   like things that are more related to editing feats
[00:35:18.760 --> 00:35:20.060]   that are much harder.
[00:35:20.060 --> 00:35:23.360]   But, and so I think like a very simple use case is just,
[00:35:23.360 --> 00:35:26.000]   you know, I'm not a graphics designer.
[00:35:26.000 --> 00:35:28.680]   I don't know if, I don't know if you guys are,
[00:35:28.680 --> 00:35:31.080]   but it's sure, you know, it seems like very simple
[00:35:31.080 --> 00:35:33.080]   that like you, if we could give you the ability
[00:35:33.080 --> 00:35:37.520]   to do really complex graphics without skill,
[00:35:37.520 --> 00:35:39.000]   wouldn't you want that?
[00:35:39.000 --> 00:35:41.020]   You know, like my wife the other day was set, you know,
[00:35:41.020 --> 00:35:43.080]   said, ah, I wish Playground was better
[00:35:43.080 --> 00:35:45.560]   because I wish that, you know, don't you,
[00:35:45.560 --> 00:35:46.840]   when are you guys gonna have a feature
[00:35:46.840 --> 00:35:48.880]   where like we could make my son, his name's Devin,
[00:35:48.880 --> 00:35:50.800]   smile when he was not smiling in the picture
[00:35:50.800 --> 00:35:53.040]   for the holiday card, right?
[00:35:53.040 --> 00:35:55.080]   You know, just being able to highlight his mouth
[00:35:55.080 --> 00:35:56.480]   and just say like, make him smile.
[00:35:56.480 --> 00:35:58.040]   Like, why can't we do that
[00:35:58.040 --> 00:36:00.600]   with like high fidelity and coherence?
[00:36:00.600 --> 00:36:03.920]   Little things like that, all the way to, you know,
[00:36:03.920 --> 00:36:06.200]   putting you in completely different scenarios.
[00:36:06.200 --> 00:36:07.040]   - Is that true?
[00:36:07.040 --> 00:36:08.760]   Can we not do that in painting?
[00:36:08.760 --> 00:36:10.200]   - You can do in painting,
[00:36:10.200 --> 00:36:12.840]   but it's the quality is just so bad.
[00:36:12.840 --> 00:36:16.240]   Yeah, it's just really terrible quality.
[00:36:16.240 --> 00:36:18.480]   You know, it's like, you'll do it five times
[00:36:18.480 --> 00:36:20.440]   and it'll still like kind of look like crooked
[00:36:20.440 --> 00:36:21.720]   or just the artifact.
[00:36:21.720 --> 00:36:24.360]   Part of it's like, you know, the lips on the face are so,
[00:36:24.360 --> 00:36:26.720]   there's such, there's such little information there.
[00:36:26.720 --> 00:36:29.500]   It's so small that the models really struggle with it.
[00:36:29.500 --> 00:36:30.520]   Yeah.
[00:36:30.520 --> 00:36:32.360]   - Make the picture smaller and you won't see it.
[00:36:32.360 --> 00:36:34.760]   - Wait, I think, I think that's my trick, I don't know.
[00:36:34.760 --> 00:36:35.640]   - Yeah, yeah, that's true.
[00:36:35.640 --> 00:36:37.200]   Or, you know, you could take that region
[00:36:37.200 --> 00:36:39.520]   and make it really big and then like say it's a mouth
[00:36:39.520 --> 00:36:40.920]   and then like shrink it.
[00:36:40.920 --> 00:36:43.120]   It feels like you're wrestling with it
[00:36:43.120 --> 00:36:47.640]   more than it's doing something that kind of surprises you.
[00:36:47.640 --> 00:36:48.480]   Yeah.
[00:36:48.480 --> 00:36:50.600]   - It feels like you are very much the internal tastemaker.
[00:36:50.600 --> 00:36:53.320]   Like you carry in your head this vision
[00:36:53.320 --> 00:36:56.200]   for what a good art model should look like.
[00:36:56.200 --> 00:36:59.520]   Is it, do you find it hard to like communicate it
[00:36:59.520 --> 00:37:02.960]   to like your team and, you know, other people?
[00:37:02.960 --> 00:37:04.840]   'Cause obviously it's hard to put into words
[00:37:04.840 --> 00:37:06.100]   like we just said.
[00:37:06.100 --> 00:37:10.140]   - Yeah, it's very hard to explain.
[00:37:10.140 --> 00:37:14.360]   Like images have such, like such high bit rate
[00:37:14.360 --> 00:37:15.700]   compared to just words.
[00:37:15.700 --> 00:37:19.900]   And we don't have enough words to describe these things.
[00:37:19.900 --> 00:37:21.740]   Difficult, I think everyone on the team,
[00:37:21.740 --> 00:37:25.180]   if they don't have good kind of like judgment taste
[00:37:25.180 --> 00:37:27.300]   or like an eye for some of these things,
[00:37:27.300 --> 00:37:28.860]   they're like steadily building it
[00:37:28.860 --> 00:37:30.820]   'cause they have no choice, right?
[00:37:30.820 --> 00:37:33.820]   So in that realm, I don't worry too much, actually.
[00:37:33.820 --> 00:37:35.860]   Like everyone is kind of like learning
[00:37:35.860 --> 00:37:39.980]   to get the eye is what I would call it.
[00:37:39.980 --> 00:37:41.740]   But I also have, you know, my own narrow taste.
[00:37:41.740 --> 00:37:43.220]   Like I'm at my, you know, I'm not,
[00:37:43.220 --> 00:37:45.220]   I don't represent the whole population either.
[00:37:45.220 --> 00:37:46.060]   - True, true.
[00:37:46.060 --> 00:37:47.580]   - So.
[00:37:47.580 --> 00:37:49.780]   - When you benchmark models, you know,
[00:37:49.780 --> 00:37:51.060]   like this benchmark we're talking about,
[00:37:51.060 --> 00:37:53.720]   we use FID for efficient input distance.
[00:37:53.720 --> 00:37:56.500]   Okay, that's one measure,
[00:37:56.500 --> 00:37:57.700]   but like doesn't capture anything
[00:37:57.700 --> 00:37:59.380]   you just said about smiles.
[00:37:59.380 --> 00:38:02.660]   - Yeah, FID is generally a bad metric.
[00:38:02.660 --> 00:38:04.460]   You know, it's good up to a point
[00:38:04.460 --> 00:38:06.580]   and then it kind of like is irrelevant.
[00:38:06.580 --> 00:38:07.420]   - Yeah. - Yeah.
[00:38:07.420 --> 00:38:11.060]   - And then, so are there any other metrics that you like
[00:38:11.060 --> 00:38:11.980]   apart from vibes?
[00:38:11.980 --> 00:38:13.940]   I'm always looking for alternatives to vibes.
[00:38:13.940 --> 00:38:15.500]   'Cause vibes don't scale, you know?
[00:38:15.500 --> 00:38:18.300]   - You know, it might be fun to kind of talk about this
[00:38:18.300 --> 00:38:20.300]   because it's actually kind of fresh.
[00:38:20.300 --> 00:38:22.860]   So up till now, we haven't needed to do
[00:38:22.860 --> 00:38:24.540]   a ton of like benchmarking
[00:38:24.540 --> 00:38:26.540]   because we hadn't trained our own model
[00:38:26.540 --> 00:38:27.380]   and then now we have.
[00:38:27.380 --> 00:38:28.340]   So now what?
[00:38:28.340 --> 00:38:29.180]   What does that mean?
[00:38:29.180 --> 00:38:30.380]   How do we evaluate it?
[00:38:30.380 --> 00:38:31.460]   You know, we're kind of like living
[00:38:31.460 --> 00:38:33.980]   with the last 48, 72 hours of going,
[00:38:33.980 --> 00:38:37.340]   did the way that we benchmark actually succeed?
[00:38:37.340 --> 00:38:38.180]   Did it deliver?
[00:38:38.180 --> 00:38:39.020]   Right?
[00:38:39.020 --> 00:38:40.500]   You know, like I think Gemini just came out.
[00:38:40.500 --> 00:38:42.340]   They just put out a bunch of benchmarks,
[00:38:42.340 --> 00:38:45.100]   but all these benchmarks are just an approximation
[00:38:45.100 --> 00:38:46.340]   of how you think it's gonna end up
[00:38:46.340 --> 00:38:47.420]   with real world performance.
[00:38:47.420 --> 00:38:50.260]   And I think that's like very fascinating to me.
[00:38:50.260 --> 00:38:53.360]   So if you fake that benchmark,
[00:38:53.360 --> 00:38:55.500]   you'll still end up in a really bad scenario
[00:38:55.500 --> 00:38:56.540]   at the end of the day.
[00:38:56.540 --> 00:38:58.340]   And so, you know, one of the benchmarks we did
[00:38:58.340 --> 00:39:01.300]   was we did a, we kind of curated like a thousand prompts.
[00:39:01.300 --> 00:39:03.940]   That's what we published in our blog post, you know,
[00:39:03.940 --> 00:39:05.140]   of all these tasks that we,
[00:39:05.140 --> 00:39:07.100]   a lot of them, some of them are curated by our team
[00:39:07.100 --> 00:39:09.340]   where we know the models all suck at it.
[00:39:09.340 --> 00:39:12.900]   Like my favorite prompt that no model's really capable of
[00:39:12.900 --> 00:39:15.600]   is a horse riding an astronaut.
[00:39:15.600 --> 00:39:16.440]   - Yeah.
[00:39:16.440 --> 00:39:17.260]   - The inverse one.
[00:39:17.260 --> 00:39:19.900]   And it's really, really hard to do.
[00:39:19.900 --> 00:39:20.900]   - Not in data.
[00:39:20.900 --> 00:39:22.720]   - You know, another one is like a giraffe
[00:39:22.720 --> 00:39:24.420]   underneath a microwave.
[00:39:24.420 --> 00:39:25.260]   How does that work?
[00:39:25.260 --> 00:39:26.780]   (laughing)
[00:39:26.780 --> 00:39:27.620]   Right?
[00:39:27.620 --> 00:39:29.620]   There's so many of these little funny ones.
[00:39:29.620 --> 00:39:31.060]   We do, we have prompts that are just like
[00:39:31.060 --> 00:39:32.740]   misspellings of things, right?
[00:39:32.740 --> 00:39:35.260]   Just to see if the models will figure it out.
[00:39:35.260 --> 00:39:36.100]   - So that's easy.
[00:39:36.100 --> 00:39:38.780]   That should embed to the same space.
[00:39:38.780 --> 00:39:39.620]   - Yeah.
[00:39:39.620 --> 00:39:42.260]   And just like all these very interesting, weird,
[00:39:42.260 --> 00:39:43.080]   weirdo things.
[00:39:43.080 --> 00:39:44.120]   And so we have so many of these
[00:39:44.120 --> 00:39:46.300]   and then we kind of like evaluate whether the models
[00:39:46.300 --> 00:39:47.140]   are any good at it.
[00:39:47.140 --> 00:39:48.940]   And the reality is that they're all bad at it.
[00:39:48.940 --> 00:39:51.440]   And so then you're just picking the most aesthetic image.
[00:39:51.440 --> 00:39:53.500]   But I think, you know, we're just,
[00:39:53.500 --> 00:39:55.420]   we're still at the beginning of building like our,
[00:39:55.420 --> 00:39:56.980]   like the best benchmark we can
[00:39:56.980 --> 00:40:01.980]   that aligns most with just user happiness, I think.
[00:40:01.980 --> 00:40:03.780]   'Cause we're not, we're not like putting these in papers
[00:40:03.780 --> 00:40:05.900]   and trying to like win, you know, I don't know,
[00:40:05.900 --> 00:40:07.980]   awards at ICCV or something if they have awards.
[00:40:07.980 --> 00:40:09.740]   Sorry if they don't.
[00:40:09.740 --> 00:40:11.340]   And you could.
[00:40:11.340 --> 00:40:12.980]   - Well, that's absolutely a valid strategy.
[00:40:12.980 --> 00:40:14.020]   - Yeah, you could.
[00:40:14.020 --> 00:40:15.860]   I don't think it could correlate necessarily
[00:40:15.860 --> 00:40:18.100]   with the impact we want to have on humanity.
[00:40:18.100 --> 00:40:20.460]   I think we're still evolving whatever our benchmarks are.
[00:40:20.460 --> 00:40:23.020]   So the first benchmark was just like very difficult tasks
[00:40:23.020 --> 00:40:24.300]   that we know the models are bad at.
[00:40:24.300 --> 00:40:26.700]   Can we come up with a thousand of these?
[00:40:26.700 --> 00:40:27.540]   Whether they're hand-written
[00:40:27.540 --> 00:40:28.980]   and some of them are generated.
[00:40:28.980 --> 00:40:31.900]   And then can we ask the users, like, how do we do?
[00:40:31.900 --> 00:40:34.380]   And then we wanted to use a benchmark like party prompts
[00:40:34.380 --> 00:40:36.020]   so that people in academia,
[00:40:36.020 --> 00:40:37.740]   we mostly did that so people in academia
[00:40:37.740 --> 00:40:40.580]   could measure their models against ours versus others.
[00:40:40.580 --> 00:40:45.080]   And, but yeah, I mean, fit is pretty bad.
[00:40:45.080 --> 00:40:49.380]   And I think, yeah, in terms of vibes,
[00:40:49.380 --> 00:40:50.880]   it's like when you put out the model
[00:40:50.880 --> 00:40:52.980]   and then you try to see like what users make.
[00:40:52.980 --> 00:40:55.220]   And I think my sense is that we're gonna take all the things
[00:40:55.220 --> 00:40:58.060]   that we noticed that the users kind of were failing at
[00:40:58.060 --> 00:41:01.020]   and try to find like new ways to measure that,
[00:41:01.020 --> 00:41:03.740]   whether that's like a smile or, you know,
[00:41:03.740 --> 00:41:06.260]   color contrast or lighting.
[00:41:06.260 --> 00:41:07.900]   One benefit of Playground is that
[00:41:07.900 --> 00:41:12.900]   we have users making millions of images every single day.
[00:41:12.900 --> 00:41:15.900]   And so we can just ask them.
[00:41:15.900 --> 00:41:20.260]   - And they go for like a post-generation feedback.
[00:41:20.260 --> 00:41:21.500]   - Yeah, we can just ask them.
[00:41:21.500 --> 00:41:23.740]   We can just say like, how good was the lighting here?
[00:41:23.740 --> 00:41:25.620]   How was the subject?
[00:41:25.620 --> 00:41:26.740]   How was the background?
[00:41:26.740 --> 00:41:30.460]   - Oh, like a proper form of like.
[00:41:30.460 --> 00:41:32.300]   - It's just like, you make it,
[00:41:32.300 --> 00:41:33.700]   you come to our site, you make an image
[00:41:33.700 --> 00:41:35.660]   and then we say, and then maybe randomly you just say,
[00:41:35.660 --> 00:41:37.540]   hey, you know, like how was the color
[00:41:37.540 --> 00:41:38.700]   and contrast of this image?
[00:41:38.700 --> 00:41:40.460]   And you say, it was not very good.
[00:41:40.460 --> 00:41:41.760]   And then you just tell us.
[00:41:41.760 --> 00:41:45.460]   So I think we can get like tens of thousands
[00:41:45.460 --> 00:41:49.100]   of these evaluations every single day
[00:41:49.100 --> 00:41:52.200]   to truly measure real world performance
[00:41:52.200 --> 00:41:54.140]   as opposed to just like benchmark performance.
[00:41:54.140 --> 00:41:56.940]   Hopefully next year, I think we will try to publish
[00:41:56.940 --> 00:42:01.640]   kind of like a benchmark that anyone could use,
[00:42:01.640 --> 00:42:04.580]   that we evaluate ourselves on and that other people can,
[00:42:04.580 --> 00:42:06.660]   that we think does a good job
[00:42:06.660 --> 00:42:08.420]   of approximating real world performance
[00:42:08.420 --> 00:42:10.940]   because we've tried it and done it and noticed that it did.
[00:42:10.940 --> 00:42:12.580]   Yeah, I think we will do that.
[00:42:12.580 --> 00:42:14.060]   - Yeah.
[00:42:14.060 --> 00:42:15.500]   I think we're going to ask a few more
[00:42:15.500 --> 00:42:17.540]   like sort of product-y questions.
[00:42:17.540 --> 00:42:20.020]   I personally have a few like categories
[00:42:20.020 --> 00:42:22.860]   that I consider special among, you know,
[00:42:22.860 --> 00:42:25.060]   you have like animals, art, fashion, food.
[00:42:25.060 --> 00:42:28.640]   There are some categories which I consider
[00:42:28.640 --> 00:42:30.680]   like a different tier of image.
[00:42:30.680 --> 00:42:33.420]   So the top among them is text in images.
[00:42:33.420 --> 00:42:36.600]   How do you think about that?
[00:42:36.600 --> 00:42:38.720]   So one of the big wild ones for me,
[00:42:38.720 --> 00:42:40.720]   something I've been looking out for the entire year
[00:42:40.720 --> 00:42:42.520]   is just the progress of text and images.
[00:42:42.520 --> 00:42:44.480]   Like, do you, can you write in an image?
[00:42:44.480 --> 00:42:45.320]   - Yeah.
[00:42:45.320 --> 00:42:48.440]   - Or an ideogram, I think, came out recently,
[00:42:48.440 --> 00:42:52.280]   which had decent but not perfect text and images.
[00:42:52.280 --> 00:42:55.140]   Dottie3 had improved some
[00:42:55.140 --> 00:42:58.500]   and all they said in their paper was that
[00:42:58.500 --> 00:43:00.000]   they just included more text in the dataset
[00:43:00.000 --> 00:43:01.200]   and it just worked.
[00:43:01.200 --> 00:43:03.000]   I was like, that's just, that's just lazy.
[00:43:03.000 --> 00:43:04.320]   (laughing)
[00:43:04.320 --> 00:43:06.200]   But anyway, do you care about that?
[00:43:06.200 --> 00:43:08.360]   'Cause I don't see any of that in like your sample.
[00:43:08.360 --> 00:43:09.200]   - Yeah, yeah.
[00:43:09.200 --> 00:43:14.200]   Yeah, the V2 model was mostly focused on image quality
[00:43:14.200 --> 00:43:18.120]   versus like the feature of text synthesis.
[00:43:18.120 --> 00:43:20.280]   'Cause I, well, as a business user,
[00:43:20.280 --> 00:43:21.120]   I care a lot about that.
[00:43:21.120 --> 00:43:21.940]   - Yeah. - Right.
[00:43:21.940 --> 00:43:23.520]   - Yeah, I'm very excited about text synthesis
[00:43:23.520 --> 00:43:26.720]   and yeah, I think ideogram has done a good job
[00:43:26.720 --> 00:43:28.080]   of maybe the best job.
[00:43:28.080 --> 00:43:31.920]   Dottie kind of has like a, it has like a hit rate.
[00:43:31.920 --> 00:43:33.520]   You know, you don't want just text effects.
[00:43:33.520 --> 00:43:36.620]   I think where this has to go is it has to be like,
[00:43:36.620 --> 00:43:39.000]   you could like write little tiny pieces of text
[00:43:39.000 --> 00:43:41.040]   like on like a milk carton.
[00:43:41.040 --> 00:43:41.880]   - Yeah.
[00:43:41.880 --> 00:43:43.600]   - That's maybe not even the focal point of a scene.
[00:43:43.600 --> 00:43:44.440]   - Yeah.
[00:43:44.440 --> 00:43:46.360]   - I think that's like a very hard task
[00:43:46.360 --> 00:43:48.600]   that if you could do something like that,
[00:43:48.600 --> 00:43:50.360]   then there's a lot of other possibilities.
[00:43:50.360 --> 00:43:51.400]   - Well, you don't have to zero shot it.
[00:43:51.400 --> 00:43:54.080]   You can just be like here and focus on this.
[00:43:54.080 --> 00:43:55.520]   - Sure, yeah, yeah, definitely.
[00:43:55.520 --> 00:43:56.360]   Yeah, yeah.
[00:43:56.360 --> 00:43:58.320]   So I think text synthesis would be very exciting.
[00:43:58.320 --> 00:43:59.160]   - Yeah.
[00:43:59.160 --> 00:44:02.860]   And then also flag that Max Wolf, Minimax here,
[00:44:02.860 --> 00:44:04.960]   which you must have come across his work.
[00:44:04.960 --> 00:44:08.700]   He's done a lot of stuff about using like logo masks
[00:44:08.700 --> 00:44:13.440]   that then map onto like food or vegetables
[00:44:13.440 --> 00:44:15.720]   and it looks like text,
[00:44:15.720 --> 00:44:17.280]   which can be pretty fun.
[00:44:17.280 --> 00:44:18.280]   - Yeah, yeah.
[00:44:18.280 --> 00:44:20.280]   I mean, it's very interesting to,
[00:44:20.280 --> 00:44:21.720]   that's the wonderful thing about like
[00:44:21.720 --> 00:44:23.600]   the open source community is that you get things
[00:44:23.600 --> 00:44:25.880]   like control net and then you see all these people
[00:44:25.880 --> 00:44:28.360]   do these just amazing things with control net
[00:44:28.360 --> 00:44:31.480]   and then you wonder, I think from our point of view,
[00:44:31.480 --> 00:44:33.400]   we sort of go, that's really wonderful,
[00:44:33.400 --> 00:44:35.520]   but how do we end up with like a unified model
[00:44:35.520 --> 00:44:36.400]   that can do that?
[00:44:36.400 --> 00:44:37.320]   What are the bottlenecks?
[00:44:37.320 --> 00:44:39.040]   What are the issues?
[00:44:39.040 --> 00:44:40.280]   Because the community ultimately
[00:44:40.280 --> 00:44:41.720]   has very limited resources.
[00:44:41.720 --> 00:44:42.560]   - Yeah.
[00:44:42.560 --> 00:44:44.960]   - And so they need these kinds of like work around
[00:44:45.720 --> 00:44:50.520]   work around research ideas to get there, but yeah.
[00:44:50.520 --> 00:44:52.480]   - Are techniques like control net
[00:44:52.480 --> 00:44:54.240]   portable to your architecture?
[00:44:54.240 --> 00:44:55.440]   - Definitely, yeah.
[00:44:55.440 --> 00:44:58.520]   We kept the Playground v2 exactly the same as SDXL,
[00:44:58.520 --> 00:45:00.080]   not because, not out of laziness,
[00:45:00.080 --> 00:45:01.720]   but just because we wanted,
[00:45:01.720 --> 00:45:03.880]   we knew that the community already had tools.
[00:45:03.880 --> 00:45:04.720]   - Yeah.
[00:45:04.720 --> 00:45:06.080]   - It's, you know, all you have to do
[00:45:06.080 --> 00:45:08.600]   is maybe change a string in your code
[00:45:08.600 --> 00:45:10.520]   and then, you know, retrain a control net for it.
[00:45:10.520 --> 00:45:12.040]   So it was very intentional to do that.
[00:45:12.040 --> 00:45:13.320]   We didn't want to fragment the community
[00:45:13.320 --> 00:45:14.520]   with different architectures.
[00:45:14.520 --> 00:45:15.360]   - Yeah.
[00:45:15.360 --> 00:45:16.200]   Yeah.
[00:45:16.200 --> 00:45:17.240]   I have more questions about that.
[00:45:17.240 --> 00:45:18.080]   I don't know.
[00:45:18.080 --> 00:45:21.200]   I don't want to DDoS you with topics, but okay.
[00:45:21.200 --> 00:45:23.640]   I was basically going to go over three more categories.
[00:45:23.640 --> 00:45:27.720]   One is UIs, like app UIs, like mock UIs.
[00:45:27.720 --> 00:45:32.120]   Third is not safe for work, obviously.
[00:45:32.120 --> 00:45:34.000]   And then copyrighted stuff.
[00:45:34.000 --> 00:45:36.440]   I don't know if you care to comment on any of those.
[00:45:36.440 --> 00:45:39.840]   - The NSFW kind of like safety stuff is really important.
[00:45:39.840 --> 00:45:44.360]   Part of, I kind of think that one of the biggest risks
[00:45:44.360 --> 00:45:47.200]   kind of going into maybe the U.S. election year
[00:45:47.200 --> 00:45:49.400]   will probably be very interrelated
[00:45:49.400 --> 00:45:53.760]   with like graphics, audio, video.
[00:45:53.760 --> 00:45:56.200]   I think it's going to be very hard to explain,
[00:45:56.200 --> 00:45:58.480]   you know, to a family relative
[00:45:58.480 --> 00:46:00.880]   who's not kind of in our world.
[00:46:00.880 --> 00:46:02.800]   And our world is like sometimes very, you know,
[00:46:02.800 --> 00:46:04.680]   we think it's very big, but it's very tiny
[00:46:04.680 --> 00:46:05.520]   compared to the rest of the world.
[00:46:05.520 --> 00:46:07.280]   Some people are like, there's still lots of humanity
[00:46:07.280 --> 00:46:09.320]   who have no idea what chat GPT is.
[00:46:09.320 --> 00:46:12.080]   And I think it's going to be very hard to explain,
[00:46:12.080 --> 00:46:14.960]   you know, to your uncle, aunt, whoever,
[00:46:14.960 --> 00:46:16.200]   you know, hey, I saw, you know,
[00:46:16.200 --> 00:46:19.800]   I saw President Biden say this thing on a video.
[00:46:19.800 --> 00:46:22.960]   You know, I can't believe, you know, he said that.
[00:46:22.960 --> 00:46:25.440]   I think that's going to be a very troubling thing
[00:46:25.440 --> 00:46:29.720]   going into the world next year, the year after.
[00:46:29.720 --> 00:46:32.280]   - Oh, I didn't, that's more like a risk thing.
[00:46:32.280 --> 00:46:33.840]   - Yeah. - Or like deep fakes.
[00:46:33.840 --> 00:46:35.800]   Well, faking, political faking.
[00:46:35.800 --> 00:46:39.680]   But there's just, there's a lot of studies on how,
[00:46:40.520 --> 00:46:42.080]   yeah, for most businesses,
[00:46:42.080 --> 00:46:44.480]   you don't want to train on not safe for work images,
[00:46:44.480 --> 00:46:47.560]   except that it makes you really good at bodies.
[00:46:47.560 --> 00:46:51.040]   - Yeah, I mean, yeah, I mean, we personally,
[00:46:51.040 --> 00:46:55.760]   we filter out NSFW type of images in our data set
[00:46:55.760 --> 00:46:58.440]   so that it's, you know, so our safety filter stuff
[00:46:58.440 --> 00:46:59.640]   doesn't have to work as hard.
[00:46:59.640 --> 00:47:01.600]   - But you've heard this argument that it gets,
[00:47:01.600 --> 00:47:04.160]   it makes you worse at, because obviously,
[00:47:04.160 --> 00:47:08.200]   not safe for work images are very good at human anatomy,
[00:47:08.200 --> 00:47:09.640]   which you do want to be good at.
[00:47:09.640 --> 00:47:11.280]   - Yeah, it's not about like,
[00:47:11.280 --> 00:47:14.120]   it's not like necessarily a bad thing to train on that data.
[00:47:14.120 --> 00:47:16.160]   It's more about like how you go and use it.
[00:47:16.160 --> 00:47:18.200]   That's why I was kind of talking about safety.
[00:47:18.200 --> 00:47:19.480]   - Yeah, I see. - You know, in part,
[00:47:19.480 --> 00:47:20.920]   because there are very terrible things
[00:47:20.920 --> 00:47:21.760]   that can happen in the world.
[00:47:21.760 --> 00:47:23.480]   If you have a sufficiently, you know,
[00:47:23.480 --> 00:47:25.280]   extremely powerful graphics model, you know,
[00:47:25.280 --> 00:47:27.840]   suddenly like you can kind of imagine, you know,
[00:47:27.840 --> 00:47:30.040]   now if you can like generate nudes and then there's like,
[00:47:30.040 --> 00:47:32.520]   you can do very character consistent things with faces,
[00:47:32.520 --> 00:47:33.560]   like what does that lead to?
[00:47:33.560 --> 00:47:35.480]   - Yeah. - I think it's like more
[00:47:35.480 --> 00:47:37.600]   what occurs after that, right?
[00:47:37.600 --> 00:47:40.880]   Even if you train on, let's say, you know, new data,
[00:47:40.880 --> 00:47:42.280]   if it does something to kind of help,
[00:47:42.280 --> 00:47:44.760]   there's nothing wrong with the human anatomy.
[00:47:44.760 --> 00:47:47.200]   It's very valid for a model to learn that,
[00:47:47.200 --> 00:47:49.440]   but then it's kind of like, how does that get used?
[00:47:49.440 --> 00:47:52.280]   And, you know, I won't bring up all of the very,
[00:47:52.280 --> 00:47:55.360]   very unsavory, terrible things that we see
[00:47:55.360 --> 00:47:57.640]   on a daily basis on the site.
[00:47:57.640 --> 00:48:00.320]   I think it's more about what occurs.
[00:48:00.320 --> 00:48:03.520]   And so we, you know, we just recently did like a big sprint
[00:48:03.520 --> 00:48:05.760]   on safety internally around,
[00:48:05.760 --> 00:48:08.560]   and it's very difficult with graphics and art, right?
[00:48:08.560 --> 00:48:12.940]   Because there is tasteful art that has nudity, right?
[00:48:12.940 --> 00:48:15.440]   They're all over in museums, like, you know,
[00:48:15.440 --> 00:48:18.120]   it's very, very valid situations for that.
[00:48:18.120 --> 00:48:19.920]   And then there's, you know,
[00:48:19.920 --> 00:48:22.280]   there's the things that are the gray line of that.
[00:48:22.280 --> 00:48:23.960]   You know, what I might not find tasteful,
[00:48:23.960 --> 00:48:26.840]   someone might be like, that is completely tasteful, right?
[00:48:26.840 --> 00:48:29.880]   And then there's things that are way over the line.
[00:48:29.880 --> 00:48:31.400]   And then there are things that are, you know,
[00:48:31.400 --> 00:48:35.600]   maybe you or, you know, maybe I would be okay with,
[00:48:35.600 --> 00:48:37.720]   but society isn't.
[00:48:37.720 --> 00:48:39.600]   I think it's really hard with art.
[00:48:39.600 --> 00:48:41.360]   I think it's really, really hard.
[00:48:41.360 --> 00:48:43.320]   Sometimes even if you have like,
[00:48:43.320 --> 00:48:45.440]   even if you have things that are not nude,
[00:48:45.440 --> 00:48:48.920]   if a child goes to your site, scrolls down some images,
[00:48:48.920 --> 00:48:52.040]   you know, classrooms of kids, you know, using our product,
[00:48:52.040 --> 00:48:53.640]   it's a really difficult problem.
[00:48:53.640 --> 00:48:57.040]   And it stretches mostly culture, society,
[00:48:57.040 --> 00:48:59.040]   politics, everything, yeah.
[00:48:59.040 --> 00:49:00.720]   - Okay.
[00:49:02.160 --> 00:49:06.880]   Another favorite topic of our listeners is UX and AI.
[00:49:06.880 --> 00:49:09.800]   And I think you're probably one of the best
[00:49:09.800 --> 00:49:12.040]   all-inclusive editors for these things.
[00:49:12.040 --> 00:49:14.680]   So you don't just have the, you know,
[00:49:14.680 --> 00:49:17.360]   prompt images come out, you pray,
[00:49:17.360 --> 00:49:19.240]   and now you do it again.
[00:49:19.240 --> 00:49:21.880]   First, you let people pick a seed
[00:49:21.880 --> 00:49:25.080]   so they can kind of have semi-repeatable generation.
[00:49:25.080 --> 00:49:28.840]   You also have, yeah, you can pick how many images,
[00:49:28.840 --> 00:49:31.280]   and then you leave all of them in the canvas,
[00:49:31.280 --> 00:49:33.720]   and then you have kind of like this box,
[00:49:33.720 --> 00:49:37.080]   the generation box, and you can even cross between them
[00:49:37.080 --> 00:49:39.040]   and outpaint, there's all these things.
[00:49:39.040 --> 00:49:41.920]   How did you get here?
[00:49:41.920 --> 00:49:43.800]   You know, most people are kind of like,
[00:49:43.800 --> 00:49:45.360]   give me text, I give you image.
[00:49:45.360 --> 00:49:47.680]   You know, you're like, these are all the tools for you.
[00:49:47.680 --> 00:49:50.200]   - Even though we were trying to make
[00:49:50.200 --> 00:49:52.600]   a graphics foundation model,
[00:49:52.600 --> 00:49:57.840]   I think we think that we're also trying to like re-imagine
[00:49:57.840 --> 00:49:59.680]   like what a graphics editor might look like
[00:49:59.680 --> 00:50:02.240]   given the change in technology.
[00:50:02.240 --> 00:50:06.160]   So, you know, I don't think we're trying to build Photoshop,
[00:50:06.160 --> 00:50:07.960]   but it's the only thing that we could say
[00:50:07.960 --> 00:50:10.000]   that people are, you know, largely familiar with.
[00:50:10.000 --> 00:50:11.520]   Oh, okay, there's Photoshop.
[00:50:11.520 --> 00:50:14.400]   I think, you know, I don't think you would think
[00:50:14.400 --> 00:50:16.840]   of Photoshop without like the, you know,
[00:50:16.840 --> 00:50:19.640]   you wouldn't think, what would Photoshop compare itself
[00:50:19.640 --> 00:50:22.040]   to pre-computer, I don't know, right?
[00:50:22.040 --> 00:50:24.440]   It's like, or kind of like a canvas,
[00:50:24.440 --> 00:50:26.360]   but, you know, there's these menu options,
[00:50:26.360 --> 00:50:28.520]   and you can use your mouse, what's a mouse?
[00:50:29.520 --> 00:50:31.640]   So I think that we're trying to make like,
[00:50:31.640 --> 00:50:32.600]   we're trying to re-imagine
[00:50:32.600 --> 00:50:34.120]   what a graphics editor might look like.
[00:50:34.120 --> 00:50:35.800]   Not just for the fun of it,
[00:50:35.800 --> 00:50:37.160]   but because we kind of have no choice.
[00:50:37.160 --> 00:50:39.560]   Like there's this idea in image generation
[00:50:39.560 --> 00:50:41.440]   where you can generate images.
[00:50:41.440 --> 00:50:42.760]   That's like a super weird thing.
[00:50:42.760 --> 00:50:44.440]   What is that in Photoshop, right?
[00:50:44.440 --> 00:50:46.920]   You have to wait right now for the time being,
[00:50:46.920 --> 00:50:50.600]   but the wait is worth it often for a lot of people
[00:50:50.600 --> 00:50:52.560]   because they can't make that with their own skills.
[00:50:52.560 --> 00:50:54.760]   So I think it goes back to, you know,
[00:50:54.760 --> 00:50:56.560]   how we started the company,
[00:50:56.560 --> 00:51:00.720]   which was kind of looking at GPT-3's Playground,
[00:51:00.720 --> 00:51:02.240]   that the reason why we're named Playground
[00:51:02.240 --> 00:51:04.120]   is a homage to that, actually.
[00:51:04.120 --> 00:51:06.480]   And, you know, it's like,
[00:51:06.480 --> 00:51:09.320]   shouldn't these products be more visual?
[00:51:09.320 --> 00:51:11.440]   Shouldn't, you know, shouldn't they,
[00:51:11.440 --> 00:51:15.080]   these prompt boxes are like a terminal window, right?
[00:51:15.080 --> 00:51:17.400]   We're kind of at this weird point where it's just like CLI.
[00:51:17.400 --> 00:51:18.400]   It's like MS-DOS.
[00:51:18.400 --> 00:51:20.400]   I remember my mom using MS-DOS,
[00:51:20.400 --> 00:51:23.160]   and I memorized the keywords, like D-I-R-L-S,
[00:51:23.160 --> 00:51:24.520]   all those things, right?
[00:51:24.520 --> 00:51:26.080]   It feels a little like we're there, right?
[00:51:26.080 --> 00:51:27.560]   Prompt engineering is just like--
[00:51:27.560 --> 00:51:29.480]   - The shirt I'm wearing, you know, it's a bug,
[00:51:29.480 --> 00:51:30.320]   not a feature.
[00:51:30.320 --> 00:51:31.160]   - Yeah, exactly.
[00:51:31.160 --> 00:51:33.120]   Parentheses to say beautiful or whatever,
[00:51:33.120 --> 00:51:37.160]   which waits the word token more in the model or whatever.
[00:51:37.160 --> 00:51:40.000]   Yeah, it's, that's like super strange.
[00:51:40.000 --> 00:51:42.880]   I think that's not, I think everybody,
[00:51:42.880 --> 00:51:45.240]   I think a large portion of humanity would agree
[00:51:45.240 --> 00:51:47.720]   that that's not user-friendly, right?
[00:51:47.720 --> 00:51:49.480]   So how do we think about the products
[00:51:49.480 --> 00:51:50.520]   to be more user-friendly?
[00:51:50.520 --> 00:51:52.000]   Well, sure, you know, sure it would be nice
[00:51:52.000 --> 00:51:53.680]   if I could like, you know,
[00:51:53.840 --> 00:51:56.360]   if I wanted to get rid of like the headphones on my head,
[00:51:56.360 --> 00:51:57.640]   you know, it'd be nice to mask it,
[00:51:57.640 --> 00:52:00.640]   and then say, you know, can you remove the headphones?
[00:52:00.640 --> 00:52:03.240]   You know, if I want to grow the, expand the image,
[00:52:03.240 --> 00:52:06.240]   it should, you know, how can we make that feel easier
[00:52:06.240 --> 00:52:09.320]   without typing lots of words and being really confused?
[00:52:09.320 --> 00:52:11.400]   And by no, by no stretch of the imagination,
[00:52:11.400 --> 00:52:14.480]   I don't even think we've nailed the UI/UX yet.
[00:52:14.480 --> 00:52:18.160]   Part of that is because we don't,
[00:52:18.160 --> 00:52:19.480]   we're still experimenting.
[00:52:19.480 --> 00:52:21.760]   And part of that is because the model
[00:52:21.760 --> 00:52:24.000]   and the technology is going to get better.
[00:52:24.000 --> 00:52:27.760]   And whatever felt like the right UX six months ago
[00:52:27.760 --> 00:52:29.600]   is going to feel very broken now.
[00:52:29.600 --> 00:52:34.920]   And so that's a little bit of how we got there,
[00:52:34.920 --> 00:52:37.120]   is kind of saying, does everything have to be
[00:52:37.120 --> 00:52:38.280]   like a prompt in a box?
[00:52:38.280 --> 00:52:39.960]   Or can we do, can we do things
[00:52:39.960 --> 00:52:42.080]   that make it very intuitive for users?
[00:52:42.080 --> 00:52:44.960]   - How do you decide what to give access to?
[00:52:44.960 --> 00:52:47.720]   So you have things like Expand Prompt,
[00:52:47.720 --> 00:52:51.280]   which Dali 3 just does, it doesn't let you decide
[00:52:51.280 --> 00:52:52.580]   whether you should or not.
[00:52:52.580 --> 00:52:55.560]   - As in like, rewrites your prompts for you.
[00:52:55.560 --> 00:52:56.400]   - Yeah.
[00:52:56.400 --> 00:52:59.920]   - Yeah, for that feature, I think we'll probably,
[00:52:59.920 --> 00:53:02.720]   I think once we get it to be cheaper,
[00:53:02.720 --> 00:53:03.760]   we'll probably just give it up,
[00:53:03.760 --> 00:53:04.840]   we'll probably just give it away.
[00:53:04.840 --> 00:53:07.600]   But we also decided something that,
[00:53:07.600 --> 00:53:08.760]   that might be a little bit different.
[00:53:08.760 --> 00:53:10.760]   We noticed that most of image generation
[00:53:10.760 --> 00:53:12.920]   is just like kind of casual.
[00:53:12.920 --> 00:53:14.840]   You know, it's in WhatsApp, it's, you know,
[00:53:14.840 --> 00:53:17.200]   it's in a Discord bot somewhere with Majorny,
[00:53:17.200 --> 00:53:19.240]   it's in ChatGPT.
[00:53:19.240 --> 00:53:21.480]   One of the differentiators I think we provide
[00:53:21.480 --> 00:53:26.480]   is at the expense of just lots of users necessarily,
[00:53:26.480 --> 00:53:29.800]   mainstream consumers, is that we provide as much like power
[00:53:29.800 --> 00:53:33.080]   and tweakability and configurability as possible.
[00:53:33.080 --> 00:53:35.000]   So the only reason why it's a toggle,
[00:53:35.000 --> 00:53:37.560]   because we know that users might want to use it
[00:53:37.560 --> 00:53:39.480]   and might not want to use it, right?
[00:53:39.480 --> 00:53:42.640]   There are some really powerful power user hobbyists
[00:53:42.640 --> 00:53:44.080]   that know what they're doing.
[00:53:44.080 --> 00:53:45.940]   And then there's a lot of people that,
[00:53:45.940 --> 00:53:49.120]   you know, just want something that looks cool,
[00:53:49.120 --> 00:53:50.080]   but they don't know how to prompt.
[00:53:50.080 --> 00:53:53.040]   And so I think a lot of Playground is more about
[00:53:53.040 --> 00:53:57.040]   going after that core user base that like knows,
[00:53:57.040 --> 00:53:59.160]   has a little bit more savviness
[00:53:59.160 --> 00:54:01.520]   and how to use these tools, yeah.
[00:54:01.520 --> 00:54:03.280]   So they might not use like these users probably,
[00:54:03.280 --> 00:54:04.360]   you know, the average Dell user
[00:54:04.360 --> 00:54:05.720]   is probably not going to use ControlNet.
[00:54:05.720 --> 00:54:08.360]   They probably don't even know what that is.
[00:54:08.360 --> 00:54:11.040]   And so I think that like, as the models get more powerful,
[00:54:11.040 --> 00:54:13.680]   as there's more tooling, yeah,
[00:54:13.680 --> 00:54:15.080]   I think you could imagine it,
[00:54:15.080 --> 00:54:17.040]   hopefully you'll imagine a new sort of
[00:54:17.040 --> 00:54:20.360]   AI first graphics editor that's
[00:54:20.360 --> 00:54:24.400]   just as like powerful and configurable as Photoshop.
[00:54:24.400 --> 00:54:27.360]   And you might have to master a new kind of tool.
[00:54:27.360 --> 00:54:28.720]   - Yeah, yeah, well.
[00:54:28.720 --> 00:54:33.640]   There's so many things I could bounce off of that.
[00:54:33.640 --> 00:54:35.820]   One, what you mentioned about waiting.
[00:54:35.820 --> 00:54:39.560]   We have to kind of somewhat address
[00:54:39.560 --> 00:54:40.760]   the elephant in the room.
[00:54:40.760 --> 00:54:45.640]   Consistency models have been blowing up the past month.
[00:54:45.640 --> 00:54:48.560]   Is that, like, how do you think about integrating that?
[00:54:48.560 --> 00:54:50.040]   Obviously there's a lot of other companies
[00:54:50.040 --> 00:54:52.960]   also trying to beat you to that space as well.
[00:54:52.960 --> 00:54:55.320]   - I think we were the first company to integrate it.
[00:54:55.320 --> 00:54:57.240]   Well, we integrated it in a different way.
[00:54:57.240 --> 00:54:58.600]   There are like 10 companies right now
[00:54:58.600 --> 00:55:00.880]   that have kind of tried to do like interactive editing
[00:55:00.880 --> 00:55:03.040]   where you can like draw on the left side
[00:55:03.040 --> 00:55:04.560]   and then you get an image on the right side.
[00:55:04.560 --> 00:55:06.480]   We decided to kind of like wait and see
[00:55:06.480 --> 00:55:09.160]   whether there's like true utility on that.
[00:55:09.160 --> 00:55:11.320]   We have a different feature that's like unique
[00:55:11.320 --> 00:55:15.520]   in our product that's called preview rendering.
[00:55:15.520 --> 00:55:18.760]   And so you go to the product and you say,
[00:55:18.760 --> 00:55:20.120]   we're like, what is the most common use case?
[00:55:20.120 --> 00:55:22.280]   The most common use case is you write a prompt
[00:55:22.280 --> 00:55:23.180]   and then you get an image.
[00:55:23.180 --> 00:55:24.960]   But what's the most annoying thing about that?
[00:55:24.960 --> 00:55:26.300]   The most annoying thing is like,
[00:55:26.300 --> 00:55:28.160]   it feels like a slot machine, right?
[00:55:28.160 --> 00:55:29.400]   You're like, okay, I'm gonna put it in
[00:55:29.400 --> 00:55:31.480]   and maybe I'll get something cool.
[00:55:31.480 --> 00:55:34.320]   So we did something that seemed a lot simpler
[00:55:34.320 --> 00:55:36.960]   but a lot more relevant to how users already use this
[00:55:36.960 --> 00:55:38.240]   product, which is preview rendering.
[00:55:38.240 --> 00:55:40.560]   You toggle it on and it will show you a render of the image.
[00:55:40.560 --> 00:55:44.840]   And then it's just like, graphics tools already have this.
[00:55:44.840 --> 00:55:47.480]   Like if you use Cinema 4D or After Effects or something,
[00:55:47.480 --> 00:55:49.600]   it's called viewport rendering.
[00:55:49.600 --> 00:55:52.280]   And so we try to take something that exists
[00:55:52.280 --> 00:55:54.200]   in the real world that has familiarity and say,
[00:55:54.200 --> 00:55:56.380]   okay, you're gonna get a rough sense
[00:55:56.380 --> 00:55:57.780]   of an early preview of this thing.
[00:55:57.780 --> 00:55:59.640]   And then when you're ready to generate,
[00:55:59.640 --> 00:56:01.720]   we're gonna try to be as coherent
[00:56:01.720 --> 00:56:03.440]   about that image that you saw.
[00:56:03.440 --> 00:56:05.300]   That way you're not spending so much time
[00:56:05.300 --> 00:56:08.900]   just like pulling down the slot machine lever.
[00:56:08.900 --> 00:56:11.160]   So we were actually the first company,
[00:56:11.160 --> 00:56:12.080]   I think we were the first company
[00:56:12.080 --> 00:56:16.160]   to actually ship a quick LCM thing, yeah.
[00:56:16.160 --> 00:56:17.000]   - Okay.
[00:56:17.000 --> 00:56:18.080]   (laughing)
[00:56:18.080 --> 00:56:19.120]   - We were very excited about it.
[00:56:19.120 --> 00:56:20.760]   So we shipped it very quick, yeah.
[00:56:20.760 --> 00:56:23.800]   - Yeah, I think like the other,
[00:56:23.800 --> 00:56:27.840]   well the demos I've been seeing it's also, I guess,
[00:56:27.840 --> 00:56:30.000]   it's not like a preview necessarily.
[00:56:30.000 --> 00:56:34.640]   They're almost using it to animate their generations,
[00:56:34.640 --> 00:56:36.240]   because you can kind of move shapes over.
[00:56:36.240 --> 00:56:37.840]   - Yeah, yeah, they're like doing it.
[00:56:37.840 --> 00:56:39.400]   They're like animating it,
[00:56:39.400 --> 00:56:41.520]   but they're sort of showing like if I move a moon,
[00:56:41.520 --> 00:56:42.640]   you know, can I, yeah.
[00:56:42.640 --> 00:56:43.960]   - Yeah, I don't know.
[00:56:43.960 --> 00:56:46.560]   To me it unlocks video in a way.
[00:56:46.560 --> 00:56:47.400]   - Yeah.
[00:56:47.400 --> 00:56:48.240]   - That--
[00:56:48.240 --> 00:56:49.480]   - But the video models are already
[00:56:49.480 --> 00:56:50.600]   so much better than that.
[00:56:50.600 --> 00:56:51.440]   Yeah, so.
[00:56:51.440 --> 00:56:53.440]   (laughing)
[00:56:53.440 --> 00:56:55.400]   - There's another one which I think is,
[00:56:55.400 --> 00:57:01.760]   like how about the just general ecosystem of Loras, right?
[00:57:01.760 --> 00:57:06.200]   That Civit is obviously the most popular repository of Loras.
[00:57:06.200 --> 00:57:09.680]   How do you think about sort of interacting
[00:57:09.680 --> 00:57:11.500]   with that ecosystem?
[00:57:11.500 --> 00:57:14.080]   - Yeah, I mean, the guy that did Lora,
[00:57:14.080 --> 00:57:15.280]   not the guy that invented Loras,
[00:57:15.280 --> 00:57:19.120]   but the person that brought Loras to Stable Diffusion
[00:57:19.120 --> 00:57:23.200]   actually works with us on some projects.
[00:57:23.200 --> 00:57:24.560]   His name is Simu.
[00:57:24.560 --> 00:57:26.360]   Shout out to Simu.
[00:57:26.360 --> 00:57:30.160]   And I think Loras are wonderful.
[00:57:30.160 --> 00:57:33.480]   Obviously fine tuning all these dream booth models
[00:57:33.480 --> 00:57:35.480]   and such, it's just so heavy.
[00:57:35.480 --> 00:57:38.240]   And giving, and it's obvious in our conversation
[00:57:38.240 --> 00:57:42.800]   around styles and vibes and it's very hard
[00:57:42.800 --> 00:57:44.860]   to evaluate the artistry of these things.
[00:57:44.860 --> 00:57:48.860]   Loras give people this wonderful opportunity
[00:57:48.860 --> 00:57:51.860]   to create sub-genres of art.
[00:57:51.860 --> 00:57:52.900]   And I think they're amazing.
[00:57:52.900 --> 00:57:54.880]   And so any graphics tool, any kind of thing
[00:57:54.880 --> 00:57:57.900]   that's expressing art has to provide
[00:57:57.900 --> 00:58:01.340]   some level of customization to its user base
[00:58:01.340 --> 00:58:04.980]   that goes beyond just typing Greg Rakowski in a prompt.
[00:58:04.980 --> 00:58:06.940]   Right, we have to give more than that.
[00:58:08.180 --> 00:58:11.200]   It's not like users want to type these real artist names.
[00:58:11.200 --> 00:58:12.960]   It's that they don't know how else to get an image
[00:58:12.960 --> 00:58:14.280]   that looks interesting.
[00:58:14.280 --> 00:58:16.720]   They truly want originality and uniqueness.
[00:58:16.720 --> 00:58:18.040]   And I think Loras provide that.
[00:58:18.040 --> 00:58:21.320]   And they provide it in a very nice scalable way.
[00:58:21.320 --> 00:58:24.040]   I hope that we find something even better than Loras
[00:58:24.040 --> 00:58:26.060]   in the long term.
[00:58:26.060 --> 00:58:31.000]   'Cause there are still weaknesses to Loras,
[00:58:31.000 --> 00:58:32.560]   but I think they do a good job for now.
[00:58:32.560 --> 00:58:34.440]   - Yeah, and so you don't want to be the,
[00:58:34.440 --> 00:58:36.320]   like you wouldn't ever compete with Civet.
[00:58:36.320 --> 00:58:37.920]   You would just kind of--
[00:58:37.920 --> 00:58:39.320]   - Civet's a site where like all these things
[00:58:39.320 --> 00:58:41.880]   get kind of hosted by the community, right?
[00:58:41.880 --> 00:58:43.960]   And so yeah, we'll often pull down
[00:58:43.960 --> 00:58:46.600]   like some of the best things there.
[00:58:46.600 --> 00:58:51.440]   I think when we have a significantly better model,
[00:58:51.440 --> 00:58:53.360]   we will certainly build something.
[00:58:53.360 --> 00:58:55.200]   - I see. - That gets closer to that.
[00:58:55.200 --> 00:58:57.080]   I still, again, I go back to saying just,
[00:58:57.080 --> 00:58:59.120]   I still think this is like very nascent.
[00:58:59.120 --> 00:59:00.920]   Things are very underpowered, right?
[00:59:00.920 --> 00:59:05.640]   Loras are not easy for people to train.
[00:59:05.640 --> 00:59:07.800]   You know, they're easy for an engineer,
[00:59:07.800 --> 00:59:10.160]   but they're not easy, you know,
[00:59:10.160 --> 00:59:11.920]   it sure would be nicer if I could just pick,
[00:59:11.920 --> 00:59:14.440]   you know, five or six reference images, right?
[00:59:14.440 --> 00:59:17.480]   And then say, hey, you know, this is,
[00:59:17.480 --> 00:59:19.080]   and they might even be five or six different
[00:59:19.080 --> 00:59:20.400]   reference images that are not,
[00:59:20.400 --> 00:59:22.200]   they're just very different, actually.
[00:59:22.200 --> 00:59:24.220]   Like they're, they communicate a style,
[00:59:24.220 --> 00:59:27.560]   but they're actually like, it's like a mood board, right?
[00:59:27.560 --> 00:59:30.480]   And it takes, you have to be kind of an engineer almost
[00:59:30.480 --> 00:59:32.120]   to train these Loras or go to some site
[00:59:32.120 --> 00:59:33.980]   and be technically savvy at least.
[00:59:33.980 --> 00:59:37.280]   It seems like it'd be much better if I could say,
[00:59:37.280 --> 00:59:38.500]   I love this style.
[00:59:38.500 --> 00:59:43.640]   I love this style, here are five images.
[00:59:43.640 --> 00:59:45.680]   And you tell the model, like, this is what I want.
[00:59:45.680 --> 00:59:48.320]   And the model gives you something that's very aligned
[00:59:48.320 --> 00:59:50.320]   with what your style is, what you're talking about.
[00:59:50.320 --> 00:59:52.400]   And it's a style you couldn't even communicate, right?
[00:59:52.400 --> 00:59:54.400]   There's no word, you know, this is,
[00:59:54.400 --> 00:59:56.040]   you know, if you have a Tron image, it's not just Tron,
[00:59:56.040 --> 00:59:57.980]   it's like Tron plus like four or five
[00:59:57.980 --> 00:59:59.480]   different weird things. - Cyberpunk, yeah.
[00:59:59.480 --> 01:00:03.360]   - Yeah, even cyberpunk can have its like sub-genre, right?
[01:00:03.360 --> 01:00:05.680]   But I just think training Loras and doing that
[01:00:05.680 --> 01:00:08.800]   is very heavy, so I hope we can do better than that.
[01:00:08.800 --> 01:00:09.640]   - Cool. - Yeah.
[01:00:09.640 --> 01:00:13.640]   - We have Sharif from Lexica on the podcast before.
[01:00:13.640 --> 01:00:14.600]   - Oh, nice.
[01:00:14.600 --> 01:00:17.360]   - Both of you have like a landing page
[01:00:17.360 --> 01:00:19.320]   with just a bunch of images
[01:00:19.320 --> 01:00:20.960]   where you can like explore things.
[01:00:20.960 --> 01:00:22.820]   - Yeah, yeah, we have a feed.
[01:00:22.820 --> 01:00:25.880]   - Yeah, yeah, is that something you see more and more of
[01:00:25.880 --> 01:00:27.660]   in terms of like coming up with these styles?
[01:00:27.660 --> 01:00:30.540]   Is that why you have that as the starting point
[01:00:30.540 --> 01:00:32.680]   versus a lot of other products, you just go in,
[01:00:32.680 --> 01:00:34.340]   you have the generation prompt,
[01:00:34.340 --> 01:00:36.160]   you don't see a lot of examples?
[01:00:36.160 --> 01:00:38.520]   - Our feed is a little different than their feed.
[01:00:38.520 --> 01:00:41.000]   Our feed is more about community.
[01:00:41.000 --> 01:00:43.800]   So we have kind of like a Reddit thing going on
[01:00:43.800 --> 01:00:47.200]   where it's a kind of a competition like every day,
[01:00:47.200 --> 01:00:49.640]   loose competition, mostly fun competition
[01:00:49.640 --> 01:00:51.460]   of like making things.
[01:00:51.460 --> 01:00:53.760]   And there's just this wonderful community of people
[01:00:53.760 --> 01:00:55.120]   where they're liking each other's images
[01:00:55.120 --> 01:00:58.440]   and just showing their genuine interest in each other.
[01:00:58.440 --> 01:01:01.700]   And I think we definitely learn about styles that way.
[01:01:01.700 --> 01:01:03.400]   One of the funniest polls,
[01:01:03.400 --> 01:01:06.640]   if you go to the Mid-Journey polls,
[01:01:06.640 --> 01:01:08.400]   they'll sometimes put these polls out and they'll say,
[01:01:08.400 --> 01:01:10.040]   you know, what do you wish you could like learn more from?
[01:01:10.040 --> 01:01:12.760]   And like one of the things that people vote the most for
[01:01:12.760 --> 01:01:16.080]   is like learning how to prompt, right?
[01:01:16.080 --> 01:01:17.520]   And so I think like, you know,
[01:01:17.520 --> 01:01:19.560]   if you put away your research hat for a minute
[01:01:19.560 --> 01:01:22.160]   and you just put on like your product hat for a second,
[01:01:22.160 --> 01:01:23.000]   you're kind of like, well,
[01:01:23.000 --> 01:01:25.400]   why do people want to learn how to prompt, right?
[01:01:25.400 --> 01:01:28.160]   It's because they want to get higher quality images.
[01:01:28.160 --> 01:01:29.600]   Well, what's higher quality composition,
[01:01:29.600 --> 01:01:32.660]   lighting, aesthetics, so on and so forth.
[01:01:32.660 --> 01:01:35.560]   And I think that the community on our feed,
[01:01:35.560 --> 01:01:38.300]   I think we might have the biggest community
[01:01:38.300 --> 01:01:43.300]   and it gives all of the users a way to learn how to prompt
[01:01:43.300 --> 01:01:47.300]   because they're just seeing this huge rising tide
[01:01:47.300 --> 01:01:49.980]   of all these images that are super cool and interesting
[01:01:49.980 --> 01:01:51.780]   and they can kind of like take each other's prompts
[01:01:51.780 --> 01:01:53.680]   and like kind of learn how to do that.
[01:01:53.680 --> 01:01:57.180]   I think that'll be short-lived
[01:01:57.180 --> 01:01:58.540]   because I think the complexity of these things
[01:01:58.540 --> 01:01:59.780]   is going to get higher,
[01:01:59.780 --> 01:02:03.800]   but that's more about why we have that feed
[01:02:03.800 --> 01:02:05.840]   is to help each other, help teach users
[01:02:05.840 --> 01:02:08.600]   and then also just celebrate people's art.
[01:02:08.600 --> 01:02:09.960]   - You run your own infra.
[01:02:09.960 --> 01:02:10.800]   - We do.
[01:02:10.800 --> 01:02:12.360]   - Yeah, that's unusual.
[01:02:12.360 --> 01:02:14.560]   (laughs)
[01:02:14.560 --> 01:02:15.480]   - It's necessary.
[01:02:15.480 --> 01:02:16.480]   - It's necessary.
[01:02:16.480 --> 01:02:19.360]   What have you learned running DevOps for GPUs?
[01:02:19.360 --> 01:02:22.680]   You had a tweet about like how many A100s you have,
[01:02:22.680 --> 01:02:24.780]   but I feel like it's out of date probably.
[01:02:28.020 --> 01:02:29.200]   - I mean, it just comes down to cost.
[01:02:29.200 --> 01:02:30.400]   These things are very expensive.
[01:02:30.400 --> 01:02:33.160]   So we just want to make it as affordable
[01:02:33.160 --> 01:02:34.960]   for everybody as possible.
[01:02:34.960 --> 01:02:40.320]   I find the DevOps for inference to be relatively easy.
[01:02:40.320 --> 01:02:42.360]   It doesn't feel that different than,
[01:02:42.360 --> 01:02:44.840]   I think we had thousands and thousands of servers
[01:02:44.840 --> 01:02:47.720]   at Mixpanel just for dealing with the API
[01:02:47.720 --> 01:02:50.200]   had such huge quantities of volume that I didn't find it.
[01:02:50.200 --> 01:02:52.400]   I don't find it particularly very different.
[01:02:53.680 --> 01:02:57.700]   I do find model optimization performance
[01:02:57.700 --> 01:02:58.660]   is very new to me.
[01:02:58.660 --> 01:03:01.140]   So I think that I find that very difficult at the moment.
[01:03:01.140 --> 01:03:02.620]   So that's very interesting.
[01:03:02.620 --> 01:03:05.820]   But scaling inference is not terrible.
[01:03:05.820 --> 01:03:08.860]   Scaling a training cluster is much, much harder
[01:03:08.860 --> 01:03:11.840]   than I perhaps anticipated.
[01:03:11.840 --> 01:03:12.980]   - Why is that?
[01:03:12.980 --> 01:03:16.660]   - Well, it's just like a very large distributed system
[01:03:16.660 --> 01:03:20.100]   with if you have like a node that goes down
[01:03:20.100 --> 01:03:21.820]   then your training run crashes
[01:03:21.820 --> 01:03:23.560]   and then you have to somehow be resilient to that.
[01:03:23.560 --> 01:03:28.260]   And I would say training in for a software is very early.
[01:03:28.260 --> 01:03:29.820]   It feels very broken.
[01:03:29.820 --> 01:03:32.260]   I can tell in 10 years, it would be a lot better.
[01:03:32.260 --> 01:03:34.340]   - Like a mosaic or whatever.
[01:03:34.340 --> 01:03:35.180]   - Yeah, we don't even know.
[01:03:35.180 --> 01:03:39.020]   I think we use very basic tools like Slurm for scheduling
[01:03:39.020 --> 01:03:41.340]   and just normal PyTorch, PyTorch Lightning,
[01:03:41.340 --> 01:03:42.160]   that kind of thing.
[01:03:42.160 --> 01:03:43.780]   I think our tooling is an ascent.
[01:03:43.780 --> 01:03:45.740]   I think I talked to a friend that's over at XAI.
[01:03:45.740 --> 01:03:48.540]   They just, they like built their own scheduler
[01:03:48.540 --> 01:03:50.140]   and doing things with Kubernetes.
[01:03:50.140 --> 01:03:51.900]   When people are building out tools
[01:03:51.900 --> 01:03:54.000]   because the existing open source stuff doesn't work
[01:03:54.000 --> 01:03:55.600]   and everyone's doing their own bespoke thing,
[01:03:55.600 --> 01:03:58.040]   you know there's a valuable company to be formed.
[01:03:58.040 --> 01:03:59.840]   - Yeah, I think it's Mosaic.
[01:03:59.840 --> 01:04:01.360]   I don't know.
[01:04:01.360 --> 01:04:03.680]   - Well, with Mosaic, yeah, it's tough with Mosaic
[01:04:03.680 --> 01:04:06.240]   'cause anyway, I won't go into the details why,
[01:04:06.240 --> 01:04:09.200]   but yeah, we found it difficult to do it.
[01:04:09.200 --> 01:04:10.640]   It might be worth like wondering
[01:04:10.640 --> 01:04:13.160]   like why not everyone is going to Mosaic.
[01:04:13.160 --> 01:04:15.720]   Perhaps it's still, I just think it's nascent
[01:04:15.720 --> 01:04:17.520]   and perhaps Mosaic will come through.
[01:04:17.520 --> 01:04:18.920]   - Cool, anything for you?
[01:04:18.920 --> 01:04:20.880]   - No, no, this was great.
[01:04:20.880 --> 01:04:22.940]   And just to wrap, we talked about
[01:04:22.940 --> 01:04:25.040]   some of the pivotal moments in your mind
[01:04:25.040 --> 01:04:27.140]   with like DALI and whatnot.
[01:04:27.140 --> 01:04:30.120]   If you were not doing this,
[01:04:30.120 --> 01:04:33.360]   what's the most interesting unsolved question in AI
[01:04:33.360 --> 01:04:34.960]   that you would try and build in?
[01:04:34.960 --> 01:04:38.160]   - Oh man, coming up with startup ideas
[01:04:38.160 --> 01:04:39.900]   is very hard on the spot.
[01:04:39.900 --> 01:04:42.580]   - You shoot, you have to have them.
[01:04:42.580 --> 01:04:45.440]   I mean, you're a founder, you're a repeat founder.
[01:04:45.440 --> 01:04:48.040]   - I'm very picky about my startup ideas.
[01:04:49.140 --> 01:04:51.620]   So I don't have any great ones.
[01:04:51.620 --> 01:04:54.900]   The only thing that I, I don't have an idea per se
[01:04:54.900 --> 01:04:57.300]   as much as a curiosity.
[01:04:57.300 --> 01:05:00.820]   And I suppose I'll pose it to you guys.
[01:05:00.820 --> 01:05:04.600]   Right now, we sort of think that a lot of the modalities
[01:05:04.600 --> 01:05:09.600]   just kind of feel like they're vision, language, audio,
[01:05:09.600 --> 01:05:11.880]   that's roughly it.
[01:05:11.880 --> 01:05:14.420]   And somehow all this will like turn into something,
[01:05:14.420 --> 01:05:18.740]   it'll be multimodal and then we'll end up with AGI perhaps.
[01:05:18.740 --> 01:05:22.580]   And I just think that there are probably far more modalities
[01:05:22.580 --> 01:05:25.540]   than maybe we, than meets the eye.
[01:05:25.540 --> 01:05:28.760]   And it just seems hard for us to see it right now
[01:05:28.760 --> 01:05:31.260]   because it's sort of like we have tunnel vision
[01:05:31.260 --> 01:05:32.100]   on the moment.
[01:05:32.100 --> 01:05:34.700]   - We're just like code, image, audio, video.
[01:05:34.700 --> 01:05:35.540]   - Yeah, I think--
[01:05:35.540 --> 01:05:36.660]   - Very, very broad categories.
[01:05:36.660 --> 01:05:39.840]   - I think we are lacking imagination as a species
[01:05:39.840 --> 01:05:40.680]   in this regard.
[01:05:40.680 --> 01:05:43.580]   And I think like, you know, just like, you know,
[01:05:43.580 --> 01:05:45.300]   it's not, I don't know what company would form
[01:05:45.300 --> 01:05:47.220]   as a result of this, but you know,
[01:05:47.220 --> 01:05:49.420]   like there's some very difficult problems,
[01:05:49.420 --> 01:05:52.940]   like just like a true actual, like not a meta world model,
[01:05:52.940 --> 01:05:56.860]   but an actual world model that truly maps everything
[01:05:56.860 --> 01:06:00.140]   that's going in terms of like physics and fluids
[01:06:00.140 --> 01:06:02.700]   and all these various kinds of interactions.
[01:06:02.700 --> 01:06:04.660]   And what does that kind of model,
[01:06:04.660 --> 01:06:07.340]   like a true physics foundation model of sorts
[01:06:07.340 --> 01:06:09.040]   that represents earth.
[01:06:09.040 --> 01:06:13.060]   And that in of itself seems very difficult, you know,
[01:06:13.060 --> 01:06:15.460]   but we just think of, but we're kind of stuck on like
[01:06:15.460 --> 01:06:17.020]   thinking that we can approximate everything
[01:06:17.020 --> 01:06:20.820]   with like, you know, a word or a token, if you will.
[01:06:20.820 --> 01:06:22.300]   And I went, you know, I had a dinner last night
[01:06:22.300 --> 01:06:24.580]   where we were kind of debating this philosophically.
[01:06:24.580 --> 01:06:26.380]   And I think someone, you know, said something
[01:06:26.380 --> 01:06:27.780]   that I also believe in, which is like,
[01:06:27.780 --> 01:06:29.260]   at the end of the day, it doesn't really matter
[01:06:29.260 --> 01:06:31.180]   that it's like a token or a byte.
[01:06:31.180 --> 01:06:33.620]   At the end of the day, it's just like some, you know,
[01:06:33.620 --> 01:06:36.100]   unit of information that it emits.
[01:06:36.100 --> 01:06:38.780]   But, you know, I do wonder if there are more,
[01:06:38.780 --> 01:06:42.520]   far more modalities than meets the eye.
[01:06:42.520 --> 01:06:45.300]   And if you could create that, then what would that,
[01:06:45.300 --> 01:06:47.220]   what would that company become?
[01:06:47.220 --> 01:06:48.940]   What problems could you solve?
[01:06:48.940 --> 01:06:52.700]   So I don't know yet, so I don't have a great company for it.
[01:06:52.700 --> 01:06:53.540]   - I don't know.
[01:06:53.540 --> 01:06:56.180]   Maybe you would just inspire somebody to try.
[01:06:56.180 --> 01:06:57.720]   - Yeah, hopefully.
[01:06:57.720 --> 01:06:59.860]   - My personal response to that is I'm less interested
[01:06:59.860 --> 01:07:01.780]   in physics and more interested in people.
[01:07:01.780 --> 01:07:04.220]   Like how do I mind upload?
[01:07:04.220 --> 01:07:07.940]   Because that is teleportation, that is immortality,
[01:07:07.940 --> 01:07:08.980]   that is everything.
[01:07:08.980 --> 01:07:11.660]   - Yeah, yeah, can we model our own,
[01:07:11.660 --> 01:07:13.300]   rather than trying to create consciousness,
[01:07:13.300 --> 01:07:15.040]   could we model our own?
[01:07:15.040 --> 01:07:18.500]   Even if it was lossy to some extent, yeah.
[01:07:18.500 --> 01:07:19.780]   - Yeah.
[01:07:19.780 --> 01:07:22.180]   Well, we won't solve that here.
[01:07:22.180 --> 01:07:27.180]   If I were to take a Bill Gates book trip and had a week,
[01:07:27.180 --> 01:07:29.820]   what should I take with me to learn AI?
[01:07:29.820 --> 01:07:32.700]   - Oh man, oh gosh.
[01:07:32.700 --> 01:07:35.540]   You shouldn't take a book, you should just go to YouTube
[01:07:35.540 --> 01:07:40.540]   and visit Karpathy's class and just do it, do it,
[01:07:40.540 --> 01:07:41.820]   grind through it.
[01:07:41.820 --> 01:07:43.300]   That's actually the most useful thing for you?
[01:07:43.300 --> 01:07:46.220]   - I wish it came out when I started back last year.
[01:07:46.220 --> 01:07:49.460]   I'm as bummed that I didn't get to take it
[01:07:49.460 --> 01:07:53.140]   at the beginning, but I did do a few of his classes
[01:07:53.140 --> 01:07:53.980]   regardless.
[01:07:53.980 --> 01:07:57.300]   I don't think books, every time I buy a programming book,
[01:07:57.300 --> 01:07:58.220]   I never read it.
[01:07:58.220 --> 01:08:00.500]   I always find that just writing code
[01:08:00.500 --> 01:08:02.300]   helps cement my internal understanding.
[01:08:02.300 --> 01:08:04.820]   - Yeah, so more generally, advice for founders
[01:08:04.820 --> 01:08:07.420]   who are not PhDs and are effectively self-taught
[01:08:07.420 --> 01:08:11.000]   like you are, what should they do, what should they avoid?
[01:08:11.000 --> 01:08:14.100]   - Same thing that I would advise if you're programming.
[01:08:14.100 --> 01:08:16.700]   Pick a project that seems very exciting to you,
[01:08:16.700 --> 01:08:19.060]   but doesn't have to be too serious,
[01:08:19.060 --> 01:08:22.420]   and build it and learn every detail of it while you do it.
[01:08:22.420 --> 01:08:24.740]   - And it must be, should you train?
[01:08:24.740 --> 01:08:29.180]   Or can you go far enough not training, just fine-tuning?
[01:08:29.180 --> 01:08:31.500]   - It depends, I would just follow your curiosity.
[01:08:31.500 --> 01:08:33.660]   If what you want to do is something
[01:08:33.660 --> 01:08:35.980]   that requires fundamental understanding of training models,
[01:08:35.980 --> 01:08:37.820]   then you should learn it.
[01:08:37.820 --> 01:08:39.300]   You don't have to be a PhD, you don't have to get
[01:08:39.300 --> 01:08:41.940]   to become a five-year, whatever, PhD,
[01:08:41.940 --> 01:08:44.700]   but if that's necessary, I would do it.
[01:08:44.700 --> 01:08:46.860]   If it's not necessary, then go as far as you need to go,
[01:08:46.860 --> 01:08:48.940]   but I would learn, pick something that motivates.
[01:08:48.940 --> 01:08:51.420]   I think most people tap out on motivation,
[01:08:51.420 --> 01:08:52.780]   but they're deeply curious.
[01:08:52.780 --> 01:08:55.180]   - Cool. - Cool.
[01:08:55.180 --> 01:08:56.380]   - Thank you so much for coming out, man.
[01:08:56.380 --> 01:08:58.980]   - Thank you for having me, appreciate it.
[01:08:58.980 --> 01:09:01.560]   (upbeat music)
[01:09:01.560 --> 01:09:04.140]   (upbeat music)
[01:09:04.140 --> 01:09:06.720]   (upbeat music)
[01:09:06.720 --> 01:09:09.300]   (upbeat music)
[01:09:09.300 --> 01:09:11.880]   (upbeat music)
[01:09:11.880 --> 01:09:14.460]   (upbeat music)
[01:09:14.460 --> 01:09:17.040]   (upbeat music)
[01:09:17.400 --> 01:09:19.980]   (upbeat music)
[01:09:19.980 --> 01:09:21.940]   (gentle music)

