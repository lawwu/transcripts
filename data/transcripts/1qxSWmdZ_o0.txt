
[00:00:00.000 --> 00:00:04.480]   Hi everyone. As Fliga said, my name is Alex. I'm a member of the robotics team at OpenAI.
[00:00:04.480 --> 00:00:11.680]   Tonight I'll be talking about our progress towards learning to solve dexterous manipulation tasks with a humanoid robotic hand.
[00:00:11.680 --> 00:00:14.040]   So,
[00:00:14.040 --> 00:00:21.000]   specifically, I'll begin by discussing the motivation for this line of research, kind of what it means, this research on dexterity,
[00:00:21.000 --> 00:00:26.760]   and we'll then describe our two most recent big releases, Learning Dexterity from summer of 2018 and
[00:00:26.760 --> 00:00:29.480]   Solving Rubik's Cube, released this past fall.
[00:00:29.760 --> 00:00:35.840]   At the end, I'll add a small update with some more recent research we did kind of along the same research agenda.
[00:00:35.840 --> 00:00:41.640]   Okay, so let's begin with the motivation for this line of research overall.
[00:00:41.640 --> 00:00:47.000]   So the goal of robotics at OpenAI is to build a general-purpose robot.
[00:00:47.000 --> 00:00:53.200]   That is one that can operate in the complex environment of the real world and carry out most tasks that humans can do.
[00:00:54.720 --> 00:01:01.400]   Today's robots are still pretty far away from this. So most consumer-oriented robots are either very simple toys or
[00:01:01.400 --> 00:01:03.960]   tend to focus on
[00:01:03.960 --> 00:01:09.520]   primarily obstacle avoidance, which are things like Roombas, self-driving cars, or autonomous drones.
[00:01:09.520 --> 00:01:15.640]   Though these are challenging in their own right and certainly very difficult problems, especially with self-driving cars,
[00:01:15.640 --> 00:01:19.280]   they do not interact with their environment to the level that we're after.
[00:01:19.960 --> 00:01:27.400]   Kind of on the other end of the spectrum are complicated robots, which do interact with their environment such as in manufacturing or surgical robotics.
[00:01:27.400 --> 00:01:36.120]   However, today almost all of these are either executing just hard-coded behavior, pre-recorded trajectories, or are being controlled by a human.
[00:01:36.120 --> 00:01:42.480]   Building a general-purpose robot is a daunting task though, so we had to pick somewhere to start.
[00:01:42.480 --> 00:01:48.840]   We chose to focus on dexterity and specifically dexterity of humanoid robotic hands for a couple of reasons.
[00:01:49.440 --> 00:01:55.120]   First, the human hand is able to solve a huge array of tasks and the world we've constructed is
[00:01:55.120 --> 00:01:58.160]   designed to a large extent around the human hand.
[00:01:58.160 --> 00:02:01.240]   Second, while the necessary hardware for
[00:02:01.240 --> 00:02:04.280]   doing this, so anthropomorphic
[00:02:04.280 --> 00:02:10.960]   robotic hands, has existed for a while, it has found little real-world use due to the very high difficulty in creating software
[00:02:10.960 --> 00:02:13.160]   capable of controlling this hardware.
[00:02:14.840 --> 00:02:20.840]   So why is it hard? The main source of difficulty stems from the high complexity of the robots that are involved.
[00:02:20.840 --> 00:02:25.280]   For example, the robot that we've been using through this line of research, the Shadow Hand,
[00:02:25.280 --> 00:02:29.000]   has 24 degrees of freedom and 20 actuators.
[00:02:29.000 --> 00:02:36.040]   This is much higher than the like seven or eight degrees of freedom that you typically see with robotic arms used in manufacturing.
[00:02:36.040 --> 00:02:43.960]   And also when coupled with a relatively small form factor, this leads to a very complicated and generally less precise hardware,
[00:02:44.120 --> 00:02:46.720]   which in turn is very difficult to simulate accurately.
[00:02:46.720 --> 00:02:54.440]   So that's a very quick motivation. With that, we can go ahead and start talking about the first set of results that I want to
[00:02:54.440 --> 00:02:55.880]   describe to you all tonight.
[00:02:55.880 --> 00:02:59.960]   This is learning dexterity, which we released in summer of 2018.
[00:02:59.960 --> 00:03:05.440]   Work done by all the fine folks that are listed here. Note that I hadn't joined OpenAI yet at the time.
[00:03:05.440 --> 00:03:10.200]   So this is our first project using the Shadow Hand.
[00:03:10.580 --> 00:03:17.100]   So we decided to start with a relatively simple task of reorienting a wooden block as pictured here.
[00:03:17.100 --> 00:03:20.240]   So,
[00:03:20.240 --> 00:03:22.700]   now we can go ahead and start talking about how we did this.
[00:03:22.700 --> 00:03:29.480]   So our high-level approach was to use reinforcement learning to train a policy which then controls the robot.
[00:03:29.480 --> 00:03:32.000]   So over the past five or so years,
[00:03:32.000 --> 00:03:35.340]   reinforcement learning has proven to be incredibly powerful
[00:03:36.140 --> 00:03:40.700]   from the success with AlphaGo, some more recently with the success of OpenAI 5,
[00:03:40.700 --> 00:03:43.460]   from where I'm working.
[00:03:43.460 --> 00:03:50.180]   But these approaches are incredibly data hungry. So you need a huge amount of trained data to get these approaches to work.
[00:03:50.180 --> 00:03:56.860]   And for robotics, this is particularly challenging since collecting this huge amount of data
[00:03:56.860 --> 00:03:59.380]   for training the robots is both difficult and expensive.
[00:03:59.380 --> 00:04:04.540]   So here we have a video from I think Google's so-called Arm Farm,
[00:04:05.100 --> 00:04:10.580]   where they basically did try to build out a huge array of robots and train in the real world.
[00:04:10.580 --> 00:04:14.140]   And I guess this is a sped up video, but you see there's
[00:04:14.140 --> 00:04:20.360]   kind of a lot going on here. It's a very expensive setup to maintain and it's still kind of difficult to get working.
[00:04:20.360 --> 00:04:26.500]   So instead of going down that route, we've decided to take what is called the sim-to-real approach,
[00:04:26.500 --> 00:04:32.620]   which means that we train our control policy entirely in simulation using a physics simulator,
[00:04:32.620 --> 00:04:35.260]   and then deploy it onto the physical robot.
[00:04:35.260 --> 00:04:39.820]   This is much cheaper and far more scalable than training directly in the real world,
[00:04:39.820 --> 00:04:43.420]   but it does come with its own set of challenges, which we'll get to in a bit.
[00:04:43.420 --> 00:04:47.780]   So here we have a video
[00:04:47.780 --> 00:04:50.740]   from this physics simulator that we use. So specifically,
[00:04:50.740 --> 00:04:56.380]   this is from the MuJoCo physics engine, and the video is showing our policy controlling the shadow hand here,
[00:04:56.380 --> 00:04:59.300]   trying to solve the block reorientation task.
[00:04:59.820 --> 00:05:03.900]   So on the right hand side of the video, you see this kind of transparent block
[00:05:03.900 --> 00:05:06.700]   that's like kind of moving, looks like it's moving around.
[00:05:06.700 --> 00:05:10.940]   What's happening here is that this is the desired orientation for the block,
[00:05:10.940 --> 00:05:15.620]   and once the hand is able to manipulate the block into this position,
[00:05:15.620 --> 00:05:20.540]   we sample a new goal for it to achieve. And this kind of continues
[00:05:20.540 --> 00:05:27.860]   until we get to either like 50 successes, so 50 goals achieved, or we drop the block.
[00:05:28.860 --> 00:05:34.740]   That's our simulation environment. So now let's take a quick look at our real world setup.
[00:05:34.740 --> 00:05:39.140]   So the picture here is of what we call our cages
[00:05:39.140 --> 00:05:42.980]   for running robotic experiments. So
[00:05:42.980 --> 00:05:49.500]   the hand here is in the middle. You can see it's surrounded by a very large number of cameras.
[00:05:49.500 --> 00:05:54.460]   Most of the cameras here are for the face-based motion tracking system.
[00:05:54.460 --> 00:06:00.540]   I think there's like 24 in total, and that's used for some additional like sensory input into the policy.
[00:06:00.540 --> 00:06:03.220]   Only three of the cameras pictured here are
[00:06:03.220 --> 00:06:07.580]   normal RGB cameras. So these are the ones that are circled on the image.
[00:06:07.580 --> 00:06:14.020]   The most relevant piece for this talk is that these three RGB cameras are ultimately fed into a vision model,
[00:06:14.020 --> 00:06:18.340]   which is then used to produce an estimate of the location and the orientation of the block,
[00:06:18.340 --> 00:06:22.100]   which is then passed on to the policy when we deploy the real world.
[00:06:22.500 --> 00:06:28.500]   So what this means is that the vision model we train must also be capable of achieving the sim-to-real transfer.
[00:06:28.500 --> 00:06:33.580]   So the problem with the sim-to-real approach is that it's hard.
[00:06:33.580 --> 00:06:39.820]   It's very hard. It's hard because it's impossible to perfectly model the complicated robotic systems accurately,
[00:06:39.820 --> 00:06:44.500]   which means that policies trained in simulation generally perform very poorly in the real world.
[00:06:44.500 --> 00:06:48.660]   In general, this problem is known as the sim-to-real or reality gap.
[00:06:50.980 --> 00:06:55.300]   So rather than working endlessly to make the physics simulation closer to reality,
[00:06:55.300 --> 00:07:00.660]   we can instead employ a technique called domain randomization to force the policy to learn behaviors
[00:07:00.660 --> 00:07:04.980]   which generalize to a wide range of environments rather than overfitting to any single environment.
[00:07:04.980 --> 00:07:10.500]   One of the earliest examples of this technique from Sadagi and Levin is pictured here.
[00:07:10.500 --> 00:07:16.300]   They trained a policy to fly a drone in simulation on environments with a wide variety of textures
[00:07:16.700 --> 00:07:20.140]   rendered on the furniture and the walls and the environment.
[00:07:20.140 --> 00:07:25.460]   And this allowed the policy to then transfer and work in the real world without having seen any real data.
[00:07:25.460 --> 00:07:32.460]   At OpenAI in 2017, we used roughly the same approach to train a vision model
[00:07:32.460 --> 00:07:36.740]   to predict an object's position and orientation
[00:07:36.740 --> 00:07:43.700]   entirely in simulation. As you can see from this video, we're able to actually use kind of pretty crazy looking
[00:07:44.540 --> 00:07:48.700]   textures randomized like onto the objects of everything in the scene.
[00:07:48.700 --> 00:07:55.380]   And thanks to kind of the magic of domain randomization, this model is then able to transfer to the real world.
[00:07:55.380 --> 00:08:00.940]   So we took a similar approach then with learning dexterity.
[00:08:00.940 --> 00:08:08.380]   So for this release, we had two different types of domain randomizations that you see displayed here.
[00:08:08.380 --> 00:08:11.700]   So on the left you have the physics randomizations.
[00:08:12.460 --> 00:08:19.100]   These are things like the friction coefficient, the object sizes, even like gravitational force, etc.
[00:08:19.100 --> 00:08:23.100]   And on the right you see the visual randomizations, which are
[00:08:23.100 --> 00:08:26.980]   critical for training the vision model, which I previously mentioned.
[00:08:26.980 --> 00:08:34.060]   Here we use a different rendering approach than the previous video. So we use the Unity game engine here to render
[00:08:34.060 --> 00:08:38.100]   more realistic looking images and kind of higher resolution, higher
[00:08:38.100 --> 00:08:40.460]   fidelity.
[00:08:40.460 --> 00:08:47.140]   And each column you see on the right hand side, it represents one single sample fed into the model.
[00:08:47.140 --> 00:08:50.420]   So from each of the three cameras that I pointed out earlier,
[00:08:50.420 --> 00:08:57.380]   the job of the vision model is then to predict the pose of the box, the position and the orientation given this image.
[00:08:57.380 --> 00:09:03.500]   So with this approach to training and simulation with domain randomization in mind,
[00:09:03.500 --> 00:09:07.300]   we now talk about how we ultimately train our models.
[00:09:07.660 --> 00:09:12.580]   So this diagram here is kind of describing the rough like flow of how it works.
[00:09:12.580 --> 00:09:19.060]   So both the policy and the vision model training use an internal framework called Rapid, which was
[00:09:19.060 --> 00:09:22.220]   originally developed for the OpenAI 5 bot,
[00:09:22.220 --> 00:09:24.820]   but has since been used
[00:09:24.820 --> 00:09:27.740]   throughout OpenAI for a lot of reinforcement learning projects.
[00:09:27.740 --> 00:09:36.420]   Note that we train the control policy here using reinforcement learning with state-based observations rather than from images.
[00:09:37.140 --> 00:09:42.180]   The reason we do this is basically because it's easy to get these state-based observations.
[00:09:42.180 --> 00:09:44.420]   So that is like the position and orientation of the block
[00:09:44.420 --> 00:09:46.540]   from the simulator.
[00:09:46.540 --> 00:09:49.740]   And it's much much easier in terms of
[00:09:49.740 --> 00:09:55.260]   just like kind of complexity of setup as well as the amount of compute required to train the LSTM policy
[00:09:55.260 --> 00:09:57.660]   from state rather than from images.
[00:09:57.660 --> 00:10:00.900]   However, as I mentioned before we have this,
[00:10:00.900 --> 00:10:06.380]   in the real world we have to use a vision model to predict the pose of the block.
[00:10:06.500 --> 00:10:09.780]   So we also use the same Rapid framework to
[00:10:09.780 --> 00:10:13.860]   train this vision model using again a highly highly similar
[00:10:13.860 --> 00:10:16.580]   distributed system setup.
[00:10:16.580 --> 00:10:20.140]   And then once we've trained
[00:10:20.140 --> 00:10:24.420]   using this setup for long enough, we can then deploy it to the real robot. To do this
[00:10:24.420 --> 00:10:28.580]   we finally combine the vision model with the policy. So we have the
[00:10:28.580 --> 00:10:31.300]   vision model process, the real frames
[00:10:31.300 --> 00:10:34.460]   images from the cameras mounted around the cage.
[00:10:35.060 --> 00:10:40.500]   And this predicts, this produces an estimated pose which is then passed along to the LSTM policy
[00:10:40.500 --> 00:10:43.780]   which ultimately produces actions which control the robotic hand.
[00:10:43.780 --> 00:10:48.780]   So that's how it works. Now, let's see the system in action.
[00:10:48.780 --> 00:10:52.100]   So there's audio.
[00:10:52.100 --> 00:10:58.580]   So yeah, here's an example of our system performing object reorientation on a physical hand. On the right
[00:10:58.580 --> 00:11:03.580]   is the desired orientation of the block. Once it's achieved, we randomly sample a different goal.
[00:11:04.660 --> 00:11:06.820]   Sorry, I did not realize there's audio for this video.
[00:11:06.820 --> 00:11:09.420]   But basically
[00:11:09.420 --> 00:11:16.220]   this goes on for a long time. Seven full minutes of 50 successful goals achieved. You can view the whole thing on YouTube later if you're curious.
[00:11:16.220 --> 00:11:23.340]   So to take away the music now. So we also quantified the performance of our system.
[00:11:23.340 --> 00:11:31.820]   So we measured the median and maximum number of successfully achieved goals over 10 trials on the real robot with a few different setups
[00:11:31.820 --> 00:11:33.500]   considered.
[00:11:33.500 --> 00:11:40.220]   So we can see here that the domain randomization is absolutely critical to success. We get basically no
[00:11:40.220 --> 00:11:44.100]   transfer performance even from an LSTM based policy
[00:11:44.100 --> 00:11:52.540]   without domain randomization. We also find that the LSTM greatly outperforms a standard feed-forward model.
[00:11:52.540 --> 00:11:59.500]   We believe this is due to the memory present in an LSTM which when coupled with our training approach allows for what we've been calling
[00:11:59.500 --> 00:12:02.940]   emergent meta-learning or domain adaptation.
[00:12:03.140 --> 00:12:05.140]   So a bit more on that later.
[00:12:05.140 --> 00:12:12.780]   Okay, so that's it for the learning dexterity release. Now we'll fast forward to fall of last year when we released our work on
[00:12:12.780 --> 00:12:15.780]   manipulating Rubik's Cube using the same physical setup.
[00:12:15.780 --> 00:12:20.140]   So in this work, we use a shadow hand to solve Rubik's Cube.
[00:12:20.140 --> 00:12:23.900]   We chose this specific task because it built directly on the previous work
[00:12:23.900 --> 00:12:28.140]   while also taking the difficulty of the manipulation problem up several notches.
[00:12:29.820 --> 00:12:32.220]   Before we dive into the details here, I just want to clarify
[00:12:32.220 --> 00:12:36.020]   the goals that we had with this project when we set out to do it,
[00:12:36.020 --> 00:12:37.620]   which you can see here.
[00:12:37.620 --> 00:12:43.900]   So the primary goal was to push the limits of symptom-to-real transfer on an incredibly difficult dexterous manipulation task.
[00:12:43.900 --> 00:12:51.620]   Note that we didn't care for this project about learning to solve the Rubik's Cube symbolically as this only seemed tangential to our real research
[00:12:51.620 --> 00:12:53.620]   agenda around dexterity.
[00:12:53.620 --> 00:12:56.900]   Okay, now for some details. So,
[00:12:57.740 --> 00:13:03.300]   just to start, at a high level we leveraged the same approach from the previous release on learning dexterity.
[00:13:03.300 --> 00:13:06.460]   So basically we tried to combine symptom-to-real domain randomization.
[00:13:06.460 --> 00:13:10.020]   We used an LSTM policy and a vision model
[00:13:10.020 --> 00:13:12.620]   separately trained in simulation
[00:13:12.620 --> 00:13:14.140]   and all of this.
[00:13:14.140 --> 00:13:15.300]   So,
[00:13:15.300 --> 00:13:20.700]   one early surprise of the project was that it was surprisingly easy to solve this task in the simulator
[00:13:20.700 --> 00:13:24.220]   using a very simplified model of Rubik's Cube as you can see here.
[00:13:25.740 --> 00:13:29.060]   However, it turned out to be incredibly difficult
[00:13:29.060 --> 00:13:35.020]   to transfer this policy to the real robot as you can see from the video struggling there.
[00:13:35.020 --> 00:13:39.900]   So basically we had a much harder symptom-to-real problem compared to the previous work.
[00:13:39.900 --> 00:13:49.340]   So at first, like I mentioned, we were just trying the same combination of reinforcement learning and domain randomization from learning dexterity.
[00:13:50.700 --> 00:13:55.940]   However, we quickly found that for this project domain randomization just was not scaling well enough.
[00:13:55.940 --> 00:14:03.060]   So we ended up having to continue to add so many more parameters that needed to be randomized such that we couldn't feasibly
[00:14:03.060 --> 00:14:08.460]   set the correct ranges for each of them correctly by hand. It just simply wasn't scalable.
[00:14:08.460 --> 00:14:13.580]   So instead we introduced what we call automatic domain randomization or ADR
[00:14:13.580 --> 00:14:17.980]   to discover the correct ranges for each parameter for us automatically.
[00:14:18.900 --> 00:14:21.620]   So now I'll quickly describe what ADR does.
[00:14:21.620 --> 00:14:26.020]   So to begin we can consider a non-domain randomization approach
[00:14:26.020 --> 00:14:33.860]   which would consist of a single point in the space of domain randomization parameters corresponding to our best guess for the real value of these parameters.
[00:14:33.860 --> 00:14:36.340]   So what this means is you might try to like measure
[00:14:36.340 --> 00:14:43.260]   the friction coefficients of the surfaces, you would use like the actual value of gravity on Earth, things like this.
[00:14:44.220 --> 00:14:49.900]   With domain randomization, we would define a fixed box sorts around the starting point
[00:14:49.900 --> 00:14:58.180]   in domain randomization parameter space. So these the box unit corresponding to the fixed ranges in the high dimensional space.
[00:14:58.180 --> 00:15:02.140]   With ADR we're able to start with
[00:15:02.140 --> 00:15:07.660]   conservative initial ranges and then grow them automatically according to how well the policy performs
[00:15:07.660 --> 00:15:12.660]   rather than trying to initialize them to the perfect like maximal values.
[00:15:13.900 --> 00:15:18.460]   Specifically, it does this by occasionally sampling each domain randomization parameter
[00:15:18.460 --> 00:15:23.940]   at its lower or upper bound and then evaluates the performance of the policy on the resulting environment.
[00:15:23.940 --> 00:15:28.820]   If the policy does well enough, ADR will then expand this bound in the direction that it was sampling.
[00:15:28.820 --> 00:15:37.100]   So if you run this for long enough, it kind of gradually grows this box and parameter space
[00:15:37.100 --> 00:15:41.620]   much farther than we had previously been able to tune it by hand.
[00:15:42.580 --> 00:15:50.100]   In part because ADR also kind of gives us an implicit curriculum over these environments. So as things get more like over time that
[00:15:50.100 --> 00:15:54.260]   generally the environments that you're trying to solve become more and more difficult.
[00:15:54.260 --> 00:16:02.500]   So here we see an example of ADR at work on the domain randomization parameter governing the size of the cube.
[00:16:02.500 --> 00:16:10.500]   So you can see ADR allows us to initialize this parameter just to the actual size of the cube and then over time, the policy is able to
[00:16:11.500 --> 00:16:17.020]   work with not just this size of the cube, but also one that's quite a bit bigger and quite a bit smaller than the real size.
[00:16:17.020 --> 00:16:24.420]   But this is just one parameter. So again, the point of ADR is that we have so many parameters to tune that we can't do it by hand.
[00:16:24.420 --> 00:16:36.780]   So here you see the complete set listed out from some internal analytics. It would be quite a pain to tune all of these perfectly by hand and it's probably just not really a feasible thing to do given compute constraints.
[00:16:38.220 --> 00:16:44.940]   So I can talk a little bit about how we actually trained it. So it's largely the same setup. So again, we use the internal framework we call RAPID
[00:16:44.940 --> 00:16:54.380]   to separately train an LSTM policy and a convolutional neural net vision model and simulation. We then combine them the same way to deploy to the real robot.
[00:16:54.380 --> 00:17:03.780]   So now for the results. So after much trial and error, basically much, much trial and error, we finally got it working
[00:17:04.460 --> 00:17:11.620]   in the early summer of last year. So here we have a video of the Shadowhands successfully solving Rubik's Cube from a fair scramble.
[00:17:11.620 --> 00:17:22.060]   So as the last one, it takes a few minutes at real time speed. So I won't show the whole thing now, but the video is also on YouTube and is also linked from our blog. So I encourage you to check it out there if you're curious.
[00:17:22.060 --> 00:17:32.620]   So once we did this, we also ran some fun robustness experiments to kind of test the limits of the domain randomization we employed,
[00:17:33.380 --> 00:17:42.660]   including things like tying the fingers together, putting a rubber glove on the hand, or the perennial fan favorite, the plush giraffe perturbation.
[00:17:42.660 --> 00:17:49.780]   Note that none of these perturbations were included in the training data, particularly the plush giraffe. That would be way too much work.
[00:17:49.780 --> 00:17:57.100]   We also did some analysis to better understand how ADR related to performance on the real robot.
[00:17:58.020 --> 00:18:09.620]   So here we measure the size of the domain randomization parameter box that I mentioned earlier by the entropy of the distribution. So basically a higher entropy here means that ADR has expanded
[00:18:09.620 --> 00:18:16.580]   the parameter space more. So we've covered the policy has effectively solved more environments.
[00:18:16.580 --> 00:18:24.460]   Encouragingly, when we ran this analysis, we found a correlation between this ADR entropy and the mean number of successes achieved in real roll outs.
[00:18:25.220 --> 00:18:36.420]   So since we did this in the real world, we unfortunately don't have a huge amount of data, but I think that the trend here we see is pretty clear and it's pretty exciting to us that ADR has this property.
[00:18:36.420 --> 00:18:44.900]   We also did a lot of analysis for the paper studying the emergence of meta learning capabilities in our policy.
[00:18:45.500 --> 00:18:55.620]   So in our case, what we mean by meta learning is that the policy has learned how to infer the parameters of its environment so that it can adapt and more efficiently solve the task.
[00:18:55.620 --> 00:19:04.620]   So one piece of evidence for this meta learning is presented here, where we attempt to measure whether the hidden state of the LSTM, so basically
[00:19:04.620 --> 00:19:13.580]   at the end of a rollout, we capture this specter that's the memory of the model, like what is the state of it at the end of a rollout.
[00:19:14.340 --> 00:19:20.420]   And try to determine if it has information about the size of the cube that it's tasked with solving.
[00:19:20.420 --> 00:19:29.500]   So of course we did this in simulation where we can easily vary the size of the cube automatically across a wider range.
[00:19:29.500 --> 00:19:39.260]   So in this table, the rightmost column here is the accuracy that we obtained from a linear classifier that's fit atop this hidden state, this memory,
[00:19:40.020 --> 00:19:44.780]   and we can predict whether the cube size is above or below the mean size of the cube.
[00:19:44.780 --> 00:19:55.900]   So this being just a binary classifier, the random prediction accuracy would be 0.5. So the fact that we see all these above, well above 0.5,
[00:19:55.900 --> 00:20:00.980]   you know, the confidence interval being above 0.5, it's pretty encouraging. It tells us that
[00:20:01.740 --> 00:20:09.780]   the accuracy is contained within the memory. So the policy has learned how to learn about the environment and first state about the environment.
[00:20:09.780 --> 00:20:21.540]   And again, it was exciting to see that this accuracy only increased as we ran for longer and basically had a higher ADR volume. So
[00:20:23.740 --> 00:20:33.620]   that wraps up the discussion of our most recent two big releases. For more info on each, I definitely just skimmed the surface here. There's tons more in both papers that are on archive
[00:20:33.620 --> 00:20:37.380]   and also more information in our blog. So I encourage you to check those out.
[00:20:37.380 --> 00:20:49.900]   Finally, we'll now discuss a smaller result from some work completed at the beginning of this year, which was published recently in a Weights and Biases report. There's a picture on the right here and linked below.
[00:20:51.860 --> 00:21:02.260]   So for this, once we had shipped the Rubik's Cube results, we decided to do a brief investigation into whether we could train a policy directly from images, aka end to end,
[00:21:02.260 --> 00:21:10.660]   cutting out the need for a separate vision model. This means the policy architecture, both in training and when run on the real robot, looks like this, where
[00:21:11.380 --> 00:21:27.500]   you can predictively, instead of having the vision convolutional neural network produce an estimate of the block pose, you have it produce some embedding in a higher dimensional space representing the state of the environment, which is then directly fed into the policy,
[00:21:27.500 --> 00:21:40.100]   which then again produces actions. So the important property here is that when you're training the model, you're able to backprop all the way through the vision stack to learn the optimal representation for the policy.
[00:21:41.100 --> 00:21:51.420]   So for this investigation, we chose the block reorientation test from the learning dexterity release that I talked about, just because it's much simpler to solve relative to the Rubik's Cube.
[00:21:51.420 --> 00:22:05.420]   We decided to start by trying behavioral cloning to more quickly train such a policy. So behavioral cloning works by first training a state based policy, as was done in the learning dexterity release,
[00:22:06.140 --> 00:22:20.140]   which is then used to effectively teach a new end to end student policy, how to behave. So concretely, what this means is you allow the end to end policy to
[00:22:20.140 --> 00:22:30.620]   do its rollout, so it'll try to act in the environment, just as you would in a typical RL setting. But then when you do optimization, instead of using something like
[00:22:31.140 --> 00:22:49.500]   PPO or any kind of like policy gradient technique like we we've been using for our past two releases, you instead sample the kind of optimal actions from your teacher model. So in this case, the state based model and then coerce the student
[00:22:49.500 --> 00:22:54.260]   policy to behave like the teacher would have in any given situation.
[00:22:55.620 --> 00:23:05.660]   So this effectively replaces a reinforcement learning problem with a supervised learning problem where the policy gets direct supervision on its actions at every single time step.
[00:23:05.660 --> 00:23:16.860]   And with this kind of swaps is supervised learning, you get a number of benefits related to training speed and just the general ease of the problem, which I should mention here. So
[00:23:17.700 --> 00:23:34.180]   basically this made this whole investigation much quicker and cheaper to perform. And we already knew that behavioral cloning had a good shot at working because we've been using this technique within the team for over a year now to do other kinds of experimentation.
[00:23:34.180 --> 00:23:43.100]   And long story short, again, after a lot of debugging, which is honestly 90% of this job, we were finally able to get this working.
[00:23:44.740 --> 00:23:52.260]   So once we had a basic set of working we then used behavioral cloning to run a number of ablations to find the optimal setup.
[00:23:52.260 --> 00:24:02.300]   So cloning was key here as it allowed us to more quickly run these ablations. So there are a few more that are listed in the report and a few that require a bit more context to set up.
[00:24:02.300 --> 00:24:12.100]   But I think the most interesting and quick to explain ablation we ran here was on using a pre trained vision model. So here in green, we have a
[00:24:13.260 --> 00:24:21.740]   kind of a from scratch behavioral cloning run with a randomly initialized vision model and then in pink, we have a run where we initialize the vision model
[00:24:21.740 --> 00:24:30.180]   with the parameters from an already trained vision model that was taken basically from the same vision model we used for the learning dexterity release.
[00:24:30.180 --> 00:24:37.180]   So interestingly, we see a 4x speed up. I think it's not surprising we would see some speed up, but I think it's interesting that
[00:24:37.860 --> 00:24:46.540]   the nature of the speed up, it's basically eliminating this kind of flat like long stretch that you see with the green curve here.
[00:24:46.540 --> 00:24:51.380]   During which time we're hypothesizing that maybe the policy is struggling to
[00:24:51.380 --> 00:24:56.620]   kind of learn the right representation of its environment.
[00:24:59.060 --> 00:25:08.420]   So once we had this optimal setup for doing behavioral cloning, we then kind of used it to run a single very large reinforcement learning experiment.
[00:25:08.420 --> 00:25:13.060]   So again, like basically from scratch, except for using a pre trained vision stack.
[00:25:13.060 --> 00:25:20.420]   So in green here we have the RL experiment and in pink we had the behavioral cloning.
[00:25:21.540 --> 00:25:29.340]   Oh, sorry, I realized I forgot to mention what these plots denote, but on the y axis here we have the mean episodic reward.
[00:25:29.340 --> 00:25:37.020]   And on the x axis, we have the amount of frames collected. So basically the amount of training experience that has gone into the model.
[00:25:37.020 --> 00:25:45.180]   So from this plot, you can see that there's a huge gap in how much experience it takes to converge. So
[00:25:46.460 --> 00:25:53.780]   the total amount that it comes out to is about 30x more compute for the reinforcement learning experiment than behavioral cloning.
[00:25:53.780 --> 00:25:59.340]   So this is just kind of a useful result to keep in mind for us for future projects.
[00:25:59.340 --> 00:26:03.460]   And that's it. Thanks for listening.
[00:26:03.460 --> 00:26:08.300]   Nice. Thanks, Alex.
[00:26:09.300 --> 00:26:19.260]   Do people have questions? If you can pop it up in the chat or the QA button if you're on Zoom. I know Charles has a question.
[00:26:19.260 --> 00:26:35.380]   Yeah, so it's really cool work and very impressive, like getting, you know, there's lots of difficult aspects to reinforcement learning and to robotics and to get them to both work together is really cool. The question I have is
[00:26:36.620 --> 00:26:46.500]   one of the salient features of the human motor system, especially the dexterity, is haptic feedback. Like, I get a bunch of information from my joints about the amount of tension,
[00:26:46.500 --> 00:26:52.020]   about their position, about their relative orientation, and like, you know, whatever
[00:26:52.020 --> 00:27:02.380]   massive data stream we get from our skin. So I'm curious, to what degree do folks at, are folks at OpenAI interested in incorporating haptic feedback to future versions
[00:27:02.700 --> 00:27:09.940]   of this? Or was its exclusion like a very, like, yeah, because it's not helpful or something like that?
[00:27:09.940 --> 00:27:18.980]   Yeah, um, yeah, that's a good question. So I think there's a few aspects to this. The first is that the hardware, I think, for
[00:27:18.980 --> 00:27:27.500]   this kind of sensing is a little more finicky to work with. So the Shadow Hand actually does come with some of these sensors. I believe they're like,
[00:27:29.460 --> 00:27:34.340]   I can't remember the exact term, but there's basically pressure sensors in the fingers, like at each digit.
[00:27:34.340 --> 00:27:41.940]   But they're very, like, kind of sensitive to all kinds of factors like air pressure or like
[00:27:41.940 --> 00:27:51.780]   other like things that lead to them being very noisy, which has made them like more difficult to incorporate. And then I think
[00:27:52.620 --> 00:28:02.780]   basically, because of this, like, it's also a little more difficult to exactly simulate the dynamics with them. So like, I think the way the Shadow Hand work is there is these little kind of like balloons of
[00:28:02.780 --> 00:28:11.300]   I don't know if it was actually just air or some other gas that would kind of compress slightly and it would measure that. So that throws off the dynamics a little bit.
[00:28:11.300 --> 00:28:18.980]   I think longer term, it's definitely a thing that we're interested in, because clearly it's very critical to humans. Like we've, we've even tried, I think,
[00:28:19.700 --> 00:28:29.060]   we joked around on the team of like trying to like these like numbing agents, you can put on your hand and see if we can still do some of these tasks and like we're definitely thinking like it's a lot more difficult.
[00:28:29.060 --> 00:28:34.900]   I guess the other
[00:28:34.900 --> 00:28:40.380]   we also have a tremendous amount of experience working with
[00:28:40.980 --> 00:28:47.300]   vision like in the ML community. Like we have convolutional neural nets that are this like, you know, sort of super weapon for handling vision tasks.
[00:28:47.300 --> 00:28:58.580]   So do you also think it's a matter of algorithmic development that we need better tools for handling the kinds of input streams that things like haptic sensors provide? Or do you think that we have that technology figured out?
[00:28:58.580 --> 00:29:00.300]   It's interesting.
[00:29:00.300 --> 00:29:09.460]   I would guess that that won't be the blocker anytime soon for this, but we haven't, we haven't tested it as well.
[00:29:10.060 --> 00:29:20.700]   All right, we've got a couple questions here from the audience. So lame asks, did you run behavioral coding for the same number of frames as RL?
[00:29:20.700 --> 00:29:23.500]   Oh, yeah.
[00:29:23.500 --> 00:29:32.140]   I see. I think I see the question. So like to 2 billion or 3 billion. Maybe it'll get better. I think there's another question I see related to
[00:29:33.060 --> 00:29:43.140]   the RL being better at the end. So this was a kind of a difficulty in constructing the plot. So I was showing the mean reward obtained during training.
[00:29:43.140 --> 00:29:50.060]   But we also have like a separate set of kind of evaluation on like a kind of the maximally difficult environment.
[00:29:50.060 --> 00:29:59.740]   And we find that the reason we stopped the behavioral coding experiment when we did is because it had already fully solved the environment. So with the median
[00:30:00.580 --> 00:30:06.180]   performance was 50 successes out of 50 so that the reward can go a little bit higher because you get
[00:30:06.180 --> 00:30:10.740]   benefits for other things, but it didn't matter for us at this point.
[00:30:10.740 --> 00:30:24.060]   Someone, Matisse asks, how's it like to work on such big projects? Isn't it hard to get the full picture of what you're doing sometimes?
[00:30:24.060 --> 00:30:26.180]   Yeah, um,
[00:30:28.460 --> 00:30:35.300]   yeah, it's a good question. I think I personally really enjoy working on these big projects. I think for robotics,
[00:30:35.300 --> 00:30:44.300]   I can't really see like a way of solving really hard problems without having like a bunch of people focusing on it, just because there's so much to figure out on the stack.
[00:30:44.300 --> 00:30:55.020]   From, you know, getting the firmware right, getting the physical hardware setup right, and all the way up to like, you know, designing the right, like using the right reinforcement learning algorithms.
[00:30:56.180 --> 00:31:07.860]   So I agree, it can be harder to like understand fully what's going on. I mean, I don't have full context and everything the mechanical engineers do on our team. I also think that's kind of fun because you there's more opportunity to learn new stuff.
[00:31:07.860 --> 00:31:13.460]   What are some of the tools you're using to understand what's going on with these models?
[00:31:13.460 --> 00:31:18.700]   Um, yeah. So I guess the tooling we use
[00:31:19.860 --> 00:31:28.740]   Yeah, like I mentioned, we use Waves and Biases a lot for for sharing the reports and kind of like discussing reports results internally.
[00:31:28.740 --> 00:31:43.580]   The like tech stack, I guess, if that's the question, is that we're using PyTorch to train these these models, which you've really enjoyed using. And then we have like a bunch of internal stuff as well.
[00:31:46.860 --> 00:31:58.620]   So Han asks, how does domain randomization relate to data augmentation schedules and how would the search and the reinforcement learning space translate to image tasks?
[00:31:58.620 --> 00:32:09.500]   Interesting. So I think, yeah, there is, I guess, a sense in which there's a relation between domain randomization and data augmentation.
[00:32:10.660 --> 00:32:16.940]   I don't know as much about the schedules here that Han refers to. I'm not like that expert in that field, but
[00:32:16.940 --> 00:32:36.220]   I guess the the key difference here is that with data augmentation, I guess you're starting with some real world data and then you're you're perturbing it in some way to get basically more data. Whereas in our case, everything is coming from a simulation. So you don't really need to view it as like augmentation as much. But there's definitely a relationship there.
[00:32:39.620 --> 00:32:43.300]   What's the team vision with this project, maybe in a year or two?
[00:32:43.300 --> 00:32:50.940]   Yeah, so I can't really talk too much about like future work beyond like what I shared with the motivation.
[00:32:50.940 --> 00:32:56.500]   And aside from just like our overarching goal around burning building a general purpose robot.
[00:32:56.500 --> 00:33:05.900]   Does OpenAI plan on building Keras like OpenAPI for reinforcement learning?
[00:33:07.460 --> 00:33:21.860]   Um, I don't. Well, I don't think we'd have something like a Keras. I think that the closest answer maybe is that we have this release called OpenAI Baselines, which are kind of like good standard implementations of RL algorithms.
[00:33:21.860 --> 00:33:25.940]   But yeah, that's the closest maybe.
[00:33:26.940 --> 00:33:39.940]   Nice. I also encourage the YouTube folks to ask questions right now the Zoom folks are asking all the questions, but this one question does come from YouTube. What was your most important learning in this project?
[00:33:39.940 --> 00:33:53.820]   Yeah, I think for me, so I only joined kind of March of last year. I think the most interesting thing to see was that a lot of these
[00:33:56.140 --> 00:34:04.140]   the problems that we ended up solving, they felt at the end of the day, kind of like more like engineering problems that we had to figure out. So what was really cool to see was that
[00:34:04.140 --> 00:34:14.700]   basically when we go from learning dexterity to solving the Rubik's Cube, the same, roughly the same formula worked, right? So we used reinforcement learning with domain randomization.
[00:34:15.980 --> 00:34:33.820]   We introduced ADR, which is critical to success too. But at a high level, the same approach worked. We just had to figure out how to run it at the right scale and kind of solve a lot of engineering problems. So I think that was really encouraging because I think like the promise for these approaches is very high for challenging tasks.
[00:34:33.820 --> 00:34:36.060]   Nice.
[00:34:37.060 --> 00:34:50.380]   Last question. This is just me wondering, did working on this project change how you do machine learning or change what you think of as best practices, but are not? Were you discovering new best practices?
[00:34:50.380 --> 00:34:53.540]   Yeah, it's interesting.
[00:34:55.620 --> 00:35:05.140]   So I think the interesting thing for me, jumping on this project. So my background before this was more in applied machine learning, working on sort of fraud detection.
[00:35:05.140 --> 00:35:17.220]   So there's far less emphasis on the algorithms themselves. Basically, all of the work that you end up doing is in getting better data in or like better features.
[00:35:18.460 --> 00:35:26.700]   So I think what's been interesting to see is that with this work, we also like generally don't worry too much about the algorithms themselves. Like, you know, we've been using
[00:35:26.700 --> 00:35:31.420]   PBO for all of these results is a great, you know, policy gradient method. It's
[00:35:31.420 --> 00:35:39.540]   so that works really well. The main thing that we, you know, do end up having to worry about is again the data that we're training on.
[00:35:40.140 --> 00:35:50.620]   Only in this case, we have complete control over it. It's all coming from a simulator. I think it was just, it was interesting to note that it's still just like the data that you're trading on that mattering most
[00:35:50.620 --> 00:36:02.300]   I have one more question and then we'll end it. So why did you pick the Rubik's Cube as the go to problem to prove that the robot hand worked?
[00:36:02.300 --> 00:36:08.020]   Yeah, so I wasn't around for when we actually picked this. My understanding is that
[00:36:09.900 --> 00:36:20.420]   it's basically something that I think is roughly like the most difficult like single handed manipulation tasks that you can think of. So, like
[00:36:20.420 --> 00:36:26.100]   with basically if you have like your wrist in a fixed position and you like have to do something
[00:36:26.100 --> 00:36:30.300]   with just like your fingers like it's about as hard as you can imagine.
[00:36:30.300 --> 00:36:38.780]   And it's also something that kind of, I think, has been floated around, like the idea of it has been like mentioned in the robotics community and you'll sometimes see like
[00:36:39.180 --> 00:36:52.500]   you know stock photos like a robot, it can, like a Rubik's Cube next to it, but no one's like really made a serious effort towards solving it. So I think, you know, the kind of assumption that it would be too difficult to solve was like attractive.
[00:36:52.500 --> 00:36:59.380]   I have actually one little quick more follow up. So in
[00:37:01.060 --> 00:37:11.660]   one closely related world that I'm more familiar with is the neuro prosthetics world, like the goal of generating, of human neural networks to control robotic hands.
[00:37:11.660 --> 00:37:19.380]   And one of the tasks that turned out to be most difficult for that and that required haptic feedback was the manipulation of soft objects and like
[00:37:19.660 --> 00:37:35.140]   things that are much more deformable than a Rubik's Cube. So can you say anything about your experience using this setup and these algorithms for those kinds of tasks like, you know, handling an egg or a playdough or something like that?
[00:37:35.140 --> 00:37:37.900]   Yeah, that's a good point. So one of the
[00:37:37.900 --> 00:37:44.700]   difficult parts with training entirely in simulation is that simulators today at least are
[00:37:45.740 --> 00:37:53.260]   not as good at modeling deformable objects. It's really difficult. You end up having way too many contact points and it becomes like really expensive.
[00:37:53.260 --> 00:37:56.900]   So we haven't really focused on this quite as much.
[00:37:56.900 --> 00:38:07.140]   I think it's something where we believe like once we push our sim to real approach far enough, we'll be able to do it. But yeah, it's not a thing we've attempted too much yet.

