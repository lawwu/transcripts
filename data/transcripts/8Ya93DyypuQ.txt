
[00:00:00.000 --> 00:00:06.920]   Alright, so thank you guys for the invitation to speak today. I'm really excited to tell you about some of the work that came out of
[00:00:06.920 --> 00:00:08.800]   some of my thesis
[00:00:08.800 --> 00:00:11.700]   as Lucas said from Stanford in genetics
[00:00:11.700 --> 00:00:18.500]   with a heavy focus on biophysics and how we can apply machine learning methods and where they might fit in best to
[00:00:18.500 --> 00:00:22.720]   the overall experimental pipeline of biotechnology
[00:00:22.720 --> 00:00:29.400]   So a little bit more about me. I did my undergraduate degree in biology at the University of Utah focused
[00:00:29.680 --> 00:00:35.380]   pretty heavily on wet lab biology at the time and then transitioned in 2015 to my PhD in
[00:00:35.380 --> 00:00:37.740]   genetics at Stanford
[00:00:37.740 --> 00:00:43.960]   Where I made the switch from a primarily wet lab background to a primarily dry lab or computational background
[00:00:43.960 --> 00:00:49.740]   And started learning and applying machine learning methods to the problems that we were facing in our lab at Stanford
[00:00:49.740 --> 00:00:54.920]   And then recently defended my thesis about three weeks ago now and shortly thereafter
[00:00:55.320 --> 00:01:03.040]   founded a biotech company called Trident Bioscience to take some of the methods and techniques that I developed at Stanford and
[00:01:03.040 --> 00:01:07.760]   Transition them to commercial applications for the discovery and refinement of useful proteins
[00:01:07.760 --> 00:01:11.840]   And now I want to start my talk with kind of a very general overview
[00:01:11.840 --> 00:01:16.000]   Take you guys back to high school science class and the central dogma of molecular biology
[00:01:16.000 --> 00:01:23.560]   And the central dogma states that the information flows from DNA to RNA to protein is generally kind of one
[00:01:24.120 --> 00:01:25.560]   directional
[00:01:25.560 --> 00:01:26.840]   movement
[00:01:26.840 --> 00:01:34.080]   through transcription and translation transcription of DNA into RNA and then translation of the information contained in that RNA into
[00:01:34.080 --> 00:01:39.520]   Protein, but that's really only half the story this bit that you kind of get in high school science class
[00:01:39.520 --> 00:01:45.440]   What happens to the protein is kind of lost in that process of explaining this central dogma
[00:01:45.440 --> 00:01:52.680]   But that protein goes on to actually fold into a three-dimensional structure and that three-dimensional structure is
[00:01:52.960 --> 00:01:59.800]   Dynamic with time it actually changes and the shape shifts so that it can perform different functions for the cell
[00:01:59.800 --> 00:02:08.320]   In addition, you can get these post translational modifications or chemical group additions to these protein structures that can further modify their function
[00:02:08.320 --> 00:02:10.160]   and then
[00:02:10.160 --> 00:02:17.880]   This ensemble of all these different structures are what actually carry out the various functions for a single protein sequence with any given cell
[00:02:18.120 --> 00:02:21.400]   And when I say protein sequence or DNA sequence without this talk
[00:02:21.400 --> 00:02:26.760]   What I mean is that each of these molecules are composed of different monomeric blocks
[00:02:26.760 --> 00:02:31.920]   So they're polymers strung together with different Lego like building blocks in a linear chain
[00:02:31.920 --> 00:02:36.680]   and it's the three-dimensional folding of that linear chain that really determines what the
[00:02:36.680 --> 00:02:40.000]   structure and ultimate function is going to be and
[00:02:40.000 --> 00:02:46.200]   In the context of this talk when I say function what I really mean is binding. So the binding of
[00:02:46.720 --> 00:02:54.480]   Proteins to each other proteins to smaller peptides. So small chains of amino acids or proteins to DNA or RNA
[00:02:54.480 --> 00:02:56.920]   So you might be asking yourself
[00:02:56.920 --> 00:02:58.720]   Why are we really focused on this?
[00:02:58.720 --> 00:03:05.040]   Phenotype or this this idea of binding from one protein to another protein or one protein to DNA?
[00:03:05.040 --> 00:03:13.600]   And the reason is that this is really how biology controls or tunes the kind of rheostat for different interactions and different cellular processes
[00:03:13.600 --> 00:03:15.320]   and this
[00:03:15.320 --> 00:03:21.360]   Biological rheostat can be defined by these two main axes the first being affinity or how tightly
[00:03:21.360 --> 00:03:28.920]   the different interactions take place and the second being specificity or how specific a protein recognizes its binding partner and
[00:03:28.920 --> 00:03:33.400]   So if we divide this risk that in two different quadrants
[00:03:33.400 --> 00:03:40.120]   We can start with the bottom left where we have non-specific interactions essentially proteins and DNA just bumping into each other randomly in the cell
[00:03:40.120 --> 00:03:45.040]   If we then go to the other extreme a really high affinity high specificity interaction
[00:03:45.160 --> 00:03:47.560]   We have something like antibody antigen systems
[00:03:47.560 --> 00:03:53.480]   These systems bind almost irreversibly to each other and with a really high specificity
[00:03:53.480 --> 00:03:56.240]   So an antibody can recognize its target protein
[00:03:56.240 --> 00:03:58.960]   in a sea of other proteins
[00:03:58.960 --> 00:04:01.880]   If we then move down the affinity axis
[00:04:01.880 --> 00:04:08.280]   We run into transcription factors or proteins that bind DNA to regulate the transcription of downstream genes
[00:04:08.280 --> 00:04:13.680]   So these the proteins that play an integral role in determining which genes are expressed in which cells
[00:04:14.240 --> 00:04:20.860]   And their binding is really important, but it tends to be pretty low affinity, but relatively high specificity
[00:04:20.860 --> 00:04:27.760]   To fill out the quadrant we can think about nucleosomes or these proteins that the DNA in your cells is actually wrapped around
[00:04:27.760 --> 00:04:34.680]   And these proteins play an integral role in actually packing the DNA into the nucleus of each individual cell
[00:04:34.680 --> 00:04:37.160]   And they're a really high affinity interaction
[00:04:37.160 --> 00:04:44.160]   But they have a low specificity that generally bind DNA with lower sequence specificity than the transcription factors
[00:04:45.160 --> 00:04:51.680]   And as you might expect these types of interactions are really important for a lot of different applications in biotechnology
[00:04:51.680 --> 00:04:56.000]   for instance one thing that might be familiar with is CRISPR. CRISPR relies on
[00:04:56.000 --> 00:04:59.200]   the base pairing between the
[00:04:59.200 --> 00:05:05.200]   RNA as part of the CRISPR complex and the DNA sequence that is trying to modify
[00:05:05.200 --> 00:05:10.200]   And this has been commercialized by a bunch of different companies at a toss and caribou or just two of these
[00:05:11.080 --> 00:05:16.720]   Another type of interaction is particularly important is antibodies and with all the COVID related research
[00:05:16.720 --> 00:05:20.240]   I'm sure you guys are probably more familiar with antibodies than you might like to be
[00:05:20.240 --> 00:05:25.920]   But they're used in a variety of different applications. They're of course used as therapeutics as you've been hearing about lately
[00:05:25.920 --> 00:05:32.000]   But they also are the basis for a lot of different pregnancy tests. So these antibodies are recognizing
[00:05:32.000 --> 00:05:36.520]   components in the urine that are indicative of a positive pregnancy and
[00:05:37.360 --> 00:05:39.200]   Finally we have transcription factors
[00:05:39.200 --> 00:05:44.760]   Which is where I'm going to start my talk because these interactions are really important for a lot of applications in synthetic biology
[00:05:44.760 --> 00:05:48.680]   And companies like Amaris or Zymergen which you may not have even heard of
[00:05:48.680 --> 00:05:55.320]   Are really interested in how transcription factors are binding and regulating downstream genes and how they might be able to modify that
[00:05:55.320 --> 00:05:59.120]   to regulate gene expression for their desired traits
[00:05:59.120 --> 00:06:07.160]   And so as you might expect given the importance of designing these different intermolecular interactions or these kind of binding interactions
[00:06:07.960 --> 00:06:15.160]   We might want a set of tools that would allow us to develop and modify different interactions to suit a particular need
[00:06:15.160 --> 00:06:21.000]   So if we think about what tools we might need to actually design these intermolecular interactions
[00:06:21.000 --> 00:06:23.440]   They kind of break down into four general categories
[00:06:23.440 --> 00:06:30.360]   The first and the ones that I think you guys came to this talk for is a model of how these interactions are actually taking place
[00:06:31.620 --> 00:06:37.840]   We then need a method to translate those predicted interactions or the predicted mechanism of the interaction
[00:06:37.840 --> 00:06:40.320]   into a way to actually
[00:06:40.320 --> 00:06:46.960]   Test the hypothesis that's made by that model. And so what this looks like in a lot of wet lab
[00:06:46.960 --> 00:06:54.440]   wet lab biology techniques is generating a set of sequences what we call a library that you can then run through a certain experiment and
[00:06:55.160 --> 00:07:02.160]   Test the actual outcome of the DNA sequence or protein being produced by that library of input sequences
[00:07:02.160 --> 00:07:04.600]   Taking this a step further
[00:07:04.600 --> 00:07:09.040]   We need to actually test that pool or that library of sequences that we've generated
[00:07:09.040 --> 00:07:14.200]   via some experimental method to get a measurement about how well they're performing a certain task and
[00:07:14.200 --> 00:07:19.720]   Then finally and ideally we would like some sort of method to update our
[00:07:19.720 --> 00:07:23.640]   our initial model given the new experimental data and
[00:07:24.640 --> 00:07:28.000]   Hopefully we're able to build a closed-loop system where we can get continuous
[00:07:28.000 --> 00:07:33.480]   Improvement a better and better model of how these interactions are taking place and the ways in which we might be able to change them
[00:07:33.480 --> 00:07:35.480]   to generate new
[00:07:35.480 --> 00:07:38.560]   functionally refined protein or DNA variants
[00:07:38.560 --> 00:07:44.880]   And what I'm going to focus on for the purposes of this talk is kind of this upper corner here
[00:07:44.880 --> 00:07:46.680]   I'm gonna define
[00:07:46.680 --> 00:07:49.600]   what the model of these types of interactions might look like and
[00:07:49.720 --> 00:07:54.760]   Then start to think about how we might generate a method for making these these
[00:07:54.760 --> 00:07:58.120]   hypotheses actually testable and
[00:07:58.120 --> 00:08:05.320]   The reason that this is such a big problem in biology is that sequence space in general and especially for proteins is incredibly massive
[00:08:05.320 --> 00:08:10.840]   It's really hard for us to fathom how large sequence space is and how quickly it grows
[00:08:10.840 --> 00:08:12.840]   so for
[00:08:12.840 --> 00:08:13.880]   DNA sequences
[00:08:13.880 --> 00:08:19.880]   There are four possible nucleotides at each position. And so as you get a longer and longer chain of these nucleotides
[00:08:19.880 --> 00:08:26.440]   The number of potential sequences that can be that can that that length of chain can give rise to
[00:08:26.440 --> 00:08:31.440]   Grows as four to the L where L is the length of chain. So it grows exponentially
[00:08:31.440 --> 00:08:35.400]   For proteins on the other hand you have 20 choices per position
[00:08:35.400 --> 00:08:39.040]   And so the number of possible sequences grows as 20 to the L
[00:08:39.200 --> 00:08:41.200]   And so here what I'm showing is
[00:08:41.200 --> 00:08:49.000]   given a certain protein sequence that we want to start with we can mutate a certain number of positions to all 19 other choices and
[00:08:49.000 --> 00:08:57.320]   Here on the y-axis is the number of total mutants or we would be required to screen if you want to do this in a
[00:08:57.320 --> 00:09:01.040]   saturation mutagenesis or exhaustive fashion
[00:09:01.040 --> 00:09:02.840]   and unfortunately
[00:09:02.840 --> 00:09:10.320]   Our most advanced experimental methods have a limitation of about 10 to the 11 sequences that they can screen in a single experiment
[00:09:10.320 --> 00:09:12.320]   And so this limitation
[00:09:12.320 --> 00:09:18.280]   Really defines our upper boundary for what we can do in an experimental wet lab technique
[00:09:18.280 --> 00:09:25.640]   So to some extent we must be selective in the design of these protein sequences that we want to screen in high throughput libraries
[00:09:25.640 --> 00:09:26.560]   and
[00:09:26.560 --> 00:09:34.440]   To me this really defines the best place for a home for deep learning methods because we have these high throughput screening methods available to us
[00:09:34.440 --> 00:09:39.260]   The cost of any given prediction being incorrect is relatively low
[00:09:39.260 --> 00:09:43.040]   We're not, you know using this to diagnose a disease or to immediately treat a disease
[00:09:43.040 --> 00:09:47.880]   But the value of the informed search through pattern recognition is really high
[00:09:47.880 --> 00:09:53.460]   So if we can recognize patterns in the existing data and use that to inform the sequences that we include in our library
[00:09:53.460 --> 00:09:55.460]   It could be really valuable
[00:09:56.320 --> 00:10:01.480]   And as a case study, I want to present the the first project that I worked on during my PhD which is
[00:10:01.480 --> 00:10:07.560]   Characterizing and modeling the behavior of transcription factors and how they bind to DNA sequences
[00:10:07.560 --> 00:10:13.800]   So as I mentioned earlier in the talk these transcription factors bind to DNA and they do so in a sequence specific manner
[00:10:13.800 --> 00:10:17.640]   And so when I say sequence specific manner, I mean that the given protein is
[00:10:17.640 --> 00:10:25.080]   Binding to DNA with a preference for certain short DNA motifs an example of one is given here
[00:10:25.080 --> 00:10:32.000]   So this is foe4. It's a transcription factor involved in helping yeast fungi respond to phosphate starvation
[00:10:32.000 --> 00:10:39.520]   hence the name foe4 and the sequence preference is given here and so the dominant nucleotides are these ones at the middle
[00:10:39.520 --> 00:10:42.160]   CACGTG
[00:10:42.160 --> 00:10:49.920]   Also known and you might hear me say Ebox motif. Ebox motif generally refers to this CACGTG pattern here at the center
[00:10:49.920 --> 00:10:52.840]   but unfortunately
[00:10:52.840 --> 00:10:58.320]   Or fortunately for biology families of transcription factors often bind very similar sequences
[00:10:58.320 --> 00:11:05.160]   And so here we have a related transcription vector CBF1 that you can see also likes to bind this CACGTG motif
[00:11:05.160 --> 00:11:12.600]   And so this raises an interesting question, which is you know, if they have very similar binding preferences
[00:11:12.600 --> 00:11:16.540]   Do they actually just regulate the same genes? Are they fully redundant in their function?
[00:11:16.540 --> 00:11:20.920]   And if we look at the gene regulatory networks, so here
[00:11:22.000 --> 00:11:27.680]   The transcription factor is given as these kind of central nodes and all of the genes that they regulate are the ones
[00:11:27.680 --> 00:11:34.320]   connected to those central nodes and what we see is that there's actually a very small amount of overlap between the genes regulated by foe4
[00:11:34.320 --> 00:11:37.200]   and the genes regulated by CBF1 and
[00:11:37.200 --> 00:11:46.040]   So the question that we wanted to answer was if these core binding motifs for that CACGTG the Ebox motif is nearly identical
[00:11:46.040 --> 00:11:50.760]   What differentiates the target binding sites for one transcription factor over the other?
[00:11:51.320 --> 00:11:53.320]   in the actual genome of a living organism
[00:11:53.320 --> 00:11:55.920]   And
[00:11:55.920 --> 00:12:01.160]   So what we have to understand is that these binding site sequences sit in different contexts within the genome
[00:12:01.160 --> 00:12:05.600]   They they're not just floating out as a you know, six or eight mer sequence
[00:12:05.600 --> 00:12:11.600]   In the cell, they're actually connected to a bunch of other DNA and that DNA has its own sequence
[00:12:11.600 --> 00:12:16.840]   And so our hypothesis was that it might be these sequences that sit outside of that main
[00:12:18.280 --> 00:12:22.920]   motif that actually govern whether the transcription factor is going to bind or not and
[00:12:22.920 --> 00:12:27.720]   So we wanted to look at all possible
[00:12:27.720 --> 00:12:34.520]   arrangements so all possible genomic configurations for these given Ebox motifs and then measure the binding for foe4 and
[00:12:34.520 --> 00:12:37.680]   CBF1 against that target library and
[00:12:37.680 --> 00:12:45.360]   The way we did this was we've made this DNA library that we're able to use high throughput sequencing on so we're able to actually
[00:12:45.440 --> 00:12:51.080]   Measure and read out each of these individual sequences where we fix that Ebox motif at the center
[00:12:51.080 --> 00:12:57.920]   CACGTG here and then we allowed the bases next to it to be any of the four other nucleotides
[00:12:57.920 --> 00:13:00.360]   so ACG or T
[00:13:00.360 --> 00:13:07.000]   This generated about a million sequences in total and represents an attempt to characterize binding
[00:13:07.000 --> 00:13:08.960]   That's actually relatively large
[00:13:08.960 --> 00:13:14.680]   so we don't have a lot of really good methods to measure a million sequences with the resolution that we wanted to and
[00:13:14.960 --> 00:13:17.640]   So along with a postdoc and another grad student in the lab
[00:13:17.640 --> 00:13:21.880]   I developed this BET-seq method which stands for binding energy
[00:13:21.880 --> 00:13:27.600]   Topographies by sequencing and it allows us to measure these transcription factor binding specificities in high throughput
[00:13:27.600 --> 00:13:30.960]   Importantly with energetic information behind it
[00:13:30.960 --> 00:13:37.000]   So it has to be a biophysical measurement in order for us to be really interested in a poly's lab
[00:13:37.000 --> 00:13:44.400]   My PI's lab and so we wanted to do this with really high resolution and generate these physically meaningful values for each measured
[00:13:44.840 --> 00:13:46.840]   transcription factor DNA sequence interaction
[00:13:46.840 --> 00:13:53.240]   and the way that we did this was we have a small microfluidic device that you can see here on the left hand side and
[00:13:53.240 --> 00:14:00.680]   This device is about the size of a poacher postage stamp and has a bunch of these inputs and these hydraulically activated valves
[00:14:00.680 --> 00:14:07.080]   So here's kind of a schematic of how the individual chambers within this microfluidic device look
[00:14:07.080 --> 00:14:11.560]   and each one has what we call this button valve at the top that allows us to
[00:14:11.960 --> 00:14:18.440]   Compress and actually stop the interaction of a transcription factor with a bunch of different DNA sequences
[00:14:18.440 --> 00:14:23.760]   So this allows us to do is bind transcription factor to the surface here flow in DNA
[00:14:23.760 --> 00:14:27.600]   We're going to select for the things that bind they're going to bind
[00:14:27.600 --> 00:14:31.680]   We're then going to trap them and separate them for this from the sequences that don't bind
[00:14:31.680 --> 00:14:35.480]   And so this separation is what allows us to make a measurement
[00:14:35.480 --> 00:14:40.720]   An energetic measurement of the affinity of different transcription factor DNA interactions
[00:14:41.600 --> 00:14:43.600]   So then we can put each library
[00:14:43.600 --> 00:14:50.800]   So here we have the bound and the unbound libraries go through high throughput DNA sequencing on a machine made by usually Illumina
[00:14:50.800 --> 00:14:52.760]   and then
[00:14:52.760 --> 00:14:59.400]   This generates a library where we can actually order these sequences based on the preference of each given transcription factor
[00:14:59.400 --> 00:15:01.720]   to these
[00:15:01.720 --> 00:15:07.320]   Kind of context sequences outside the ebox domain and so here you can see we can
[00:15:07.840 --> 00:15:15.440]   Potentially look at these all of these DNA sequences in the library and rank them from highest affinity which in Delta Delta G space
[00:15:15.440 --> 00:15:21.280]   so Delta Delta G is just a measurement for how high affinity is sequence binds and it has a
[00:15:21.280 --> 00:15:23.800]   physical meaning behind it and
[00:15:23.800 --> 00:15:29.640]   The more negative that value is the more tightly it's bound to the transcription factor
[00:15:29.640 --> 00:15:33.440]   So more negative equals tighter binding more positive equals looser binding
[00:15:33.440 --> 00:15:37.040]   And so we can do this for the entire library of sequences
[00:15:37.960 --> 00:15:41.880]   But unfortunately as I said before the methods that already exist for doing this
[00:15:41.880 --> 00:15:46.720]   Don't really scale well to the million sequences that we have in our library
[00:15:46.720 --> 00:15:53.240]   And the reason is because experimental noise really inhibits these energetic measurements
[00:15:53.240 --> 00:15:57.960]   The two reasons behind that are that we can't calculate a binding energy for something we don't observe
[00:15:57.960 --> 00:15:59.400]   so for a million sequences
[00:15:59.400 --> 00:16:05.240]   you actually have to use quite a bit of sequencing in order to see every sequence in that library and
[00:16:06.720 --> 00:16:13.520]   For low count values so things where we only observe a few of those of that individual sequence a few of those molecules
[00:16:13.520 --> 00:16:18.560]   The values are extremely noisy which make our ultimate energetic measurements really noisy
[00:16:18.560 --> 00:16:24.800]   And so we thought about this for a while and the solution that we came up with was to simulate this process
[00:16:24.800 --> 00:16:31.020]   So we the advantage of working in a biophysically meaningful space is that we know how these things should
[00:16:31.120 --> 00:16:37.440]   Interact in a perfect world and we can just simulate the experiment and see how well we do for different depths of sequencing
[00:16:37.440 --> 00:16:43.360]   And the important thing to recognize here is that as you go to a deeper depth of sequencing you're getting more reads
[00:16:43.360 --> 00:16:46.560]   You're getting better data, but it's also more and more expensive
[00:16:46.560 --> 00:16:53.960]   And so we simulated one of these Betsy experiments and here on the bottom we get the mean reads per sequence
[00:16:53.960 --> 00:16:59.600]   So the number of times we're observing each individual DNA sequence and then on the y-axis here
[00:17:00.160 --> 00:17:03.280]   We're showing the the median value for two different
[00:17:03.280 --> 00:17:07.640]   Measurements of success and the first is the fraction of the library that we're observing
[00:17:07.640 --> 00:17:13.640]   We want to be able to see all million sequences and then the second is the R squared value with the true
[00:17:13.640 --> 00:17:15.560]   defined
[00:17:15.560 --> 00:17:19.800]   Value that we put into the simulation and we can see that it was we started out on the left
[00:17:19.800 --> 00:17:25.280]   We end up with these really kind of garbage data sets where we're only observing a fraction of our library
[00:17:25.280 --> 00:17:28.960]   And the measurements that we're getting don't actually correlate that well with reality
[00:17:29.480 --> 00:17:36.400]   On the other side we have a really expensive experiment that yields beautiful data where we're observing nearly all the library and
[00:17:36.400 --> 00:17:43.000]   Our data quality is really good as well. The R squared value is really high with the expected values
[00:17:43.000 --> 00:17:48.960]   But we figured that we might be able to sit kind of in this Goldilocks zone here in the middle where we're observing
[00:17:48.960 --> 00:17:52.000]   Almost all of our library or all of our library
[00:17:52.000 --> 00:17:55.040]   But the R squared values might not be perfect
[00:17:55.040 --> 00:18:01.600]   And so that really set up the expectation that if we sequence one of these libraries if we perform the experiment at really low read depth
[00:18:01.600 --> 00:18:05.080]   What we're gonna get out is kind of not reproducible
[00:18:05.080 --> 00:18:09.120]   And is going to look kind of like a circular pattern
[00:18:09.120 --> 00:18:15.040]   And in fact when we ran the experiment we ran it twice two different replicates and we do see this this indicative circle pattern
[00:18:15.040 --> 00:18:20.320]   Where each measurement does not correlate with the same measurement made in a different replicate
[00:18:20.320 --> 00:18:24.120]   But since we had the simulation data in hand, we were able to say that we expected that
[00:18:24.720 --> 00:18:29.120]   Made the change to a higher read depth and we end up getting better data out
[00:18:29.120 --> 00:18:33.400]   But again, the problem with this is that if we run all these experiments at really high read depth
[00:18:33.400 --> 00:18:36.640]   It's gonna cost us a lot of money and we'd really like to avoid that
[00:18:36.640 --> 00:18:43.920]   so we thought to ourselves we might be able to use some type of modeling to extract and separate the
[00:18:43.920 --> 00:18:48.720]   signal in the data from the low read depth experiments from the noise and
[00:18:48.720 --> 00:18:52.600]   so we wanted to see if we could do that and the
[00:18:53.640 --> 00:18:59.040]   Thought behind this was that we already used transcription factor binding models. In fact, I presented one earlier in this talk
[00:18:59.040 --> 00:19:02.120]   So this motif that I showed you at the very beginning
[00:19:02.120 --> 00:19:07.460]   It's important to ask how we actually got to that point and it's through a very simple linear model
[00:19:07.460 --> 00:19:11.600]   That we can go from a bunch of different sequences to one of these motifs
[00:19:11.600 --> 00:19:14.260]   So we asked how do we get here?
[00:19:14.260 --> 00:19:20.760]   Someone previously had run a bunch of DNA sequences did the same kind of enrichments enrichment step that we did earlier
[00:19:22.320 --> 00:19:25.160]   Pulled out a bunch of sequences that bound to the transcription factor
[00:19:25.160 --> 00:19:30.520]   Performed alignment and then noticed that in all the sequences that bound there was a common pattern
[00:19:30.520 --> 00:19:37.600]   this CAC GTG pattern and potentially this flanking nucleotide here upstream and downstream and
[00:19:37.600 --> 00:19:43.720]   From that they were able to generate what we call a consensus binding site. So a single sequence that is
[00:19:43.720 --> 00:19:48.060]   Generally enriched in the binding sites for these transcription factors
[00:19:48.800 --> 00:19:56.640]   But an important assumption in this methodology is that the contribution of each nucleotide to binding is totally independent
[00:19:56.640 --> 00:19:58.760]   and so
[00:19:58.760 --> 00:20:01.160]   We didn't think that that assumption was that good
[00:20:01.160 --> 00:20:05.500]   We wanted to ask is this assumption of independence actually valid and we knew
[00:20:05.500 --> 00:20:09.860]   Because the time that we were doing this work neural network models were coming into vogue
[00:20:09.860 --> 00:20:15.780]   We knew that nonlinear models would enable us to do the same type of modeling but in an assumption free fashion
[00:20:15.780 --> 00:20:22.060]   So we wouldn't be forced into assuming that the contribution of each nucleotide in the binding site was contributing independently
[00:20:22.060 --> 00:20:23.520]   and
[00:20:23.520 --> 00:20:25.520]   So we built ourselves a nice little
[00:20:25.520 --> 00:20:31.120]   Neural network model we feed in the DNA sequence here as a one hot encoded vector
[00:20:31.120 --> 00:20:35.060]   Have a bun have a couple intermediate layers with nonlinearities
[00:20:35.060 --> 00:20:41.240]   and then we're going to output the predicted Delta Delta G value and train on our experimentally measured values and
[00:20:41.760 --> 00:20:45.620]   when you work on deep learning and biology you run into a bunch of people that
[00:20:45.620 --> 00:20:52.360]   Aren't so aren't too keen to trust that your magic neural network box model is actually accurate
[00:20:52.360 --> 00:20:57.600]   And so you have to do something to prove to them that the data that you get out actually reflects reality
[00:20:57.600 --> 00:21:00.360]   And we had another grad student in the lab
[00:21:00.360 --> 00:21:02.880]   origin who was kind enough to
[00:21:02.880 --> 00:21:08.840]   Measure a bunch of the sequences that we had looked at on an orthogonal platform. So based on the same technology
[00:21:09.280 --> 00:21:12.960]   but a much higher resolution technique this much lower throughput and
[00:21:12.960 --> 00:21:15.960]   What he was able to find and show
[00:21:15.960 --> 00:21:22.360]   Was that if we take our unprocessed or raw data the measurements that we had made without any type of correction
[00:21:22.360 --> 00:21:26.440]   They don't correlate super well with measurements that he had made in this orthogonal method
[00:21:26.440 --> 00:21:30.820]   But if we first run it through our neural network and take the prediction output from that
[00:21:30.820 --> 00:21:35.080]   And compare it to his orthogonal measurement. They do correlate really well
[00:21:35.080 --> 00:21:40.680]   So this gave us a lot of confidence that what we were seeing was good separation of signal from noise
[00:21:40.680 --> 00:21:48.000]   But we also understood that in exchange for the flexibility of neural network models. We pay a price in interpretability
[00:21:48.000 --> 00:21:52.120]   So we're not able to have that nice little motif anymore where we can say, okay
[00:21:52.120 --> 00:21:54.120]   These are the contributions to binding
[00:21:54.120 --> 00:21:57.360]   and so we were looking for a way around that and we wanted to
[00:21:57.360 --> 00:22:02.280]   Take a look at how well each of the existing models that were based on
[00:22:02.600 --> 00:22:07.000]   Principled biophysical approaches can explain what's learned by the neural network models
[00:22:07.000 --> 00:22:14.320]   So we basically wanted to ask how far can a mononucleotide model or that kind of binding motif model that I showed you in the very?
[00:22:14.320 --> 00:22:15.200]   beginning
[00:22:15.200 --> 00:22:19.160]   How can how far can that get us in explaining what the neural network has learned?
[00:22:19.160 --> 00:22:23.760]   And so if we plot the mononucleotides the linear model prediction down here
[00:22:23.760 --> 00:22:30.880]   On the x-axis and the neural network prediction on the y-axis. We see that for for that linear approximation is actually pretty good
[00:22:30.880 --> 00:22:36.640]   It explains binding behavior pretty well, but for CBF one on the other hand, it doesn't do so
[00:22:36.640 --> 00:22:44.840]   Well, there's a lot of variability for any given mononucleotide predicted Delta Delta G that can't be explained only by those mononucleotide effects
[00:22:44.840 --> 00:22:46.960]   and so
[00:22:46.960 --> 00:22:51.660]   There's a another method for doing this that adds a little bit more
[00:22:51.660 --> 00:22:56.040]   Biophysical information injects a bit more domain knowledge into this modeling process
[00:22:56.360 --> 00:23:03.940]   And that's considering dinucleotide effects and dinucleotide effects are just combinations of different mononucleotides at different positions
[00:23:03.940 --> 00:23:05.880]   and
[00:23:05.880 --> 00:23:10.680]   These dinucleotide or effects are really important for a bunch of different functions
[00:23:10.680 --> 00:23:16.120]   And but there's reason to believe that for fo for and CBF one, especially they might be important
[00:23:16.120 --> 00:23:21.360]   And so what I'm showing here is the actual structure of the fo for protein
[00:23:21.360 --> 00:23:28.880]   Which is are these kind of blue squiggly lines up here and then the DNA that is bound to the green helix down here
[00:23:28.880 --> 00:23:33.360]   and I'm showing kind of two views of that offset by 90 degrees and
[00:23:33.360 --> 00:23:37.760]   There's two different types of dinucleotide effects that we wanted to consider
[00:23:37.760 --> 00:23:41.920]   The first were adjacent or nearest neighbor dinucleotide effects
[00:23:41.920 --> 00:23:46.800]   these effects are a single base and the base pair right next to it and
[00:23:47.120 --> 00:23:51.400]   These are really important for determining the actual three-dimensional shape of DNA
[00:23:51.400 --> 00:23:55.580]   And so I know that I've been presenting DNA here as a linear sequence of these letters
[00:23:55.580 --> 00:24:02.360]   But it actually exists in your cells as this three-dimensional helix that can bend rotate compact expand
[00:24:02.360 --> 00:24:08.640]   And a lot of those shape properties are determined by the nucleotides and how they stack right next to each other
[00:24:08.640 --> 00:24:14.640]   On the other hand, we realize that these gaps nucleotides or nucleotides that are present
[00:24:15.280 --> 00:24:19.200]   dinucleotide effects that are not present in adjacent positions might be really important because
[00:24:19.200 --> 00:24:22.520]   Foufour and CBF1 are actually dimer proteins
[00:24:22.520 --> 00:24:30.240]   and so there's two individual protein chains that bind together and co-bind that DNA complex and because these tails of
[00:24:30.240 --> 00:24:35.260]   Foufour and CBF1 sit within these grooves of the DNA
[00:24:35.260 --> 00:24:40.520]   The contacts that are distal to one another might be actually important in the binding process
[00:24:41.680 --> 00:24:46.840]   So if we add these dinucleotide features to our linear models and then compare them back to the neural network models
[00:24:46.840 --> 00:24:51.560]   What we see is that for Foufour it nearly perfectly explains binding at this point
[00:24:51.560 --> 00:24:56.920]   There's very little variation left and for CBF1 it does a pretty significant job indicating
[00:24:56.920 --> 00:25:00.720]   this a significant improvement on the
[00:25:00.720 --> 00:25:03.760]   explanation of binding
[00:25:03.760 --> 00:25:07.280]   If we add in all of the dinucleotide effects at this point
[00:25:07.880 --> 00:25:11.760]   We can explain nearly all of the variability in the predictions from the neural network for Foufour
[00:25:11.760 --> 00:25:14.920]   And the same is almost true for CBF1 as well
[00:25:14.920 --> 00:25:22.840]   So what we found from this work was that Foufour and CBF1 clearly bind in these two very different modes
[00:25:22.840 --> 00:25:26.600]   So one has a strong preference for the mononucleotide effects
[00:25:26.600 --> 00:25:32.240]   So it seems to be that each individual nucleotide is actually contributing independently to binding
[00:25:32.240 --> 00:25:36.400]   But CBF1 that doesn't seem to be the case indicating that for CBF1
[00:25:36.400 --> 00:25:41.480]   There might be a much larger effect of something like DNA shape to how it binds
[00:25:41.480 --> 00:25:45.080]   But if we want to ask the question why
[00:25:45.080 --> 00:25:49.600]   The work that I've presented so far doesn't really get us there and we're a bit limited
[00:25:49.600 --> 00:25:55.840]   going from DNA sequence to functional predictions directly via a neural network model and
[00:25:55.840 --> 00:25:59.280]   the reason for that if we go back to the
[00:25:59.840 --> 00:26:05.520]   initial presentation of the central dogma of molecular biology is that instead of
[00:26:05.520 --> 00:26:13.320]   Predicting each step along this process from protein to the folding and conformational dynamics the post translational modifications and then binding
[00:26:13.320 --> 00:26:19.800]   We're basically black boxing this entire intermediate step and going directly from the protein and DNA sequences
[00:26:19.800 --> 00:26:22.360]   directly to their functions
[00:26:22.360 --> 00:26:27.640]   It's important to note here that we're not explicitly modeling the structure of the protein or the DNA here
[00:26:27.640 --> 00:26:31.840]   We're just taking a proxy measurement of their sequences for their structures
[00:26:31.840 --> 00:26:35.520]   But why would we want to model the structure at all?
[00:26:35.520 --> 00:26:39.120]   Aside from the fact that I just told you that it was critically important
[00:26:39.120 --> 00:26:44.880]   and I chose an example from the literature that I think illustrates this really well and
[00:26:44.880 --> 00:26:54.280]   The principle here is that structural changes in the protein actually drive really important functional effects for the protein being targeted
[00:26:55.040 --> 00:27:00.880]   And so here what I'm showing on the left hand side is green fluorescent protein also known as GFP
[00:27:00.880 --> 00:27:06.340]   And this is a really important protein in molecular biology because if you hit it with the right wavelength of light
[00:27:06.340 --> 00:27:11.960]   it glows green and it allows us to visualize proteins and motion within the cell and
[00:27:11.960 --> 00:27:14.560]   on the right hand side are
[00:27:14.560 --> 00:27:21.340]   Two different nanobodies. So these are really cool single domain antibodies isolated from camelid species
[00:27:21.340 --> 00:27:28.560]   So llamas alpacas camels, they have these really cool single domain antibodies and and a lab in Europe isolated
[00:27:28.560 --> 00:27:30.560]   two of these different nanobodies
[00:27:30.560 --> 00:27:33.420]   that bound this GFP protein and
[00:27:33.420 --> 00:27:39.600]   They were doing some basic characterization of this and they realized that as they added more and more of each nanobody
[00:27:39.600 --> 00:27:42.360]   They actually saw a very real functional effect
[00:27:42.360 --> 00:27:48.060]   And so the one on top in red as they added more nanobody, it suppressed the fluorescence of GFP
[00:27:48.680 --> 00:27:52.880]   But the one on the bottom as they added more and more of it it enhanced the fluorescence of GFP
[00:27:52.880 --> 00:27:56.660]   And so they named these the GFP minimizer and enhancer nanobodies
[00:27:56.660 --> 00:28:02.420]   Respectively and all they really differ in is the site on GFP that they're binding
[00:28:02.420 --> 00:28:10.120]   And what's interesting about this is that if we look at the GFP minimizer and enhancer nanobodies and start to ask like, okay
[00:28:10.120 --> 00:28:14.160]   What are the functional effects? What are the structural reasons behind these functional effects?
[00:28:14.520 --> 00:28:16.960]   We can look at the contact map of GFP
[00:28:16.960 --> 00:28:22.960]   So if we look at all of the amino acids by all the amino acids and then just highlight the ones that are in contact
[00:28:22.960 --> 00:28:26.720]   With each other we see that when the GFP minimizer nanobody is bound
[00:28:26.720 --> 00:28:29.760]   This is what the contact map looks like when the enhancer nanobody is bound
[00:28:29.760 --> 00:28:36.920]   This is what the contact map looks like and they look generally very similar, which is to be expected GFP structure is pretty stable
[00:28:36.920 --> 00:28:42.860]   But if we start to look at the difference between these two we can start to see really important functional effects pop out
[00:28:43.240 --> 00:28:48.000]   and so here what I'm showing are in red the enhancer specific contacts, so the
[00:28:48.000 --> 00:28:53.600]   Contacts that are only present in the GFP bound to the minimizer nanobody
[00:28:53.600 --> 00:28:58.560]   Or the enhancer nanobody and then the minimizer specific ones bound to the minimizer
[00:28:58.560 --> 00:29:04.880]   and so it would be really useful if we had some sort of model to actually predict the function of the
[00:29:04.880 --> 00:29:08.420]   structural consequences of these types of binding interactions
[00:29:09.400 --> 00:29:14.800]   So how might we go about doing that? If we think about two different amino acid residues
[00:29:14.800 --> 00:29:18.240]   So here I'm showing two of the chemical structures each for alanine
[00:29:18.240 --> 00:29:22.520]   what we want is some model that takes as input those two residues and
[00:29:22.520 --> 00:29:29.880]   gives us back the probability of them being in contact with one another and if we run this over all of the potential pairs of
[00:29:29.880 --> 00:29:35.680]   amino acids in a in a protein structure we can generate this probabilistic contact map and
[00:29:36.600 --> 00:29:39.400]   There are a bunch of different ways to do this and in fact
[00:29:39.400 --> 00:29:44.880]   Alpha fold has been very successful at generating these predicted contact maps for single chain structures
[00:29:44.880 --> 00:29:47.720]   but one of the most promising I think is
[00:29:47.720 --> 00:29:52.480]   reframing this problem as a graph neural network problem and
[00:29:52.480 --> 00:29:56.840]   so we can treat these proteins as a molecular graph because as I said before
[00:29:56.840 --> 00:30:01.000]   They're just a bunch of these amino acids chained together linearly
[00:30:01.000 --> 00:30:03.920]   And so this as I said before is the structure of alanine
[00:30:04.520 --> 00:30:08.320]   And what you might notice is that it looks a bit like a graph structure
[00:30:08.320 --> 00:30:14.080]   you have your nodes and you have your edges that connect atoms with covalent bonds and
[00:30:14.080 --> 00:30:19.280]   So if we apply neural network graph neural network models
[00:30:19.280 --> 00:30:22.940]   We might be able to actually model this process
[00:30:22.940 --> 00:30:27.880]   incorporating all of the atoms not just the amino acids or the
[00:30:27.880 --> 00:30:31.120]   nucleotides like we did before for the
[00:30:31.920 --> 00:30:34.120]   fo4 and cbf1 modeling
[00:30:34.120 --> 00:30:42.600]   and an important side note here is that using these atom level representations and not relying on the representations just of the amino acids as
[00:30:42.600 --> 00:30:52.000]   Kind of elements of your alphabet is that it allows us a straightforward method for encoding post translational modifications of these chemical modifications
[00:30:52.000 --> 00:30:56.640]   That aren't amino acids, but are added to the protein after it's been translated
[00:30:58.160 --> 00:31:03.440]   And so in general graph neural networks operate over these graph structures through message passing
[00:31:03.440 --> 00:31:07.640]   And I tried out two different ones in very early stage development
[00:31:07.640 --> 00:31:11.040]   These graph attention networks and position where graph neural networks
[00:31:11.040 --> 00:31:18.480]   To perform this exact task where we're going to input atom level features and the covalent adjacency matrix
[00:31:18.480 --> 00:31:26.040]   perform graph convolutions for the two different residues and then output the probability of contact generating that contact map and
[00:31:27.080 --> 00:31:33.080]   So we don't just generate a single contact map, but actually we generate different types of potential interactions
[00:31:33.080 --> 00:31:40.780]   And so those of you familiar with chemistry might understand why we might want to predict hydrogen bonds or van der Waals interactions
[00:31:40.780 --> 00:31:44.440]   We actually predict a set of eight different types of interactions
[00:31:44.440 --> 00:31:48.200]   And so this work is still very early stage
[00:31:48.200 --> 00:31:51.480]   But I want to present some really preliminary results
[00:31:51.560 --> 00:31:58.800]   just overfitting one of these graph neural network models to a single structure to see if it has the capacity to do that overfitting and
[00:31:58.800 --> 00:32:05.920]   So if we look at one small protein structure here, we can use the different types of interactions
[00:32:05.920 --> 00:32:09.660]   So here hydrophobic hydrogen bond van der Waals and water bridges
[00:32:09.660 --> 00:32:12.200]   as our target and
[00:32:12.200 --> 00:32:14.480]   Then if we try a bunch of different models
[00:32:14.480 --> 00:32:21.160]   We can see varying degrees of success overfitting to that single structure with one of the models actually perfectly fitting the data
[00:32:21.360 --> 00:32:29.560]   Indicating that this might be a feasible approach for modeling protein structure and importantly modeling the interaction of different protein structures
[00:32:29.560 --> 00:32:31.640]   so complexes of different proteins and
[00:32:31.640 --> 00:32:38.200]   So what I've shown you here is that these graph neural network models are really powerful and
[00:32:38.200 --> 00:32:42.280]   directly applicable and directly translatable to chemical prediction tasks
[00:32:42.280 --> 00:32:46.560]   That by using all atom models of protein structure
[00:32:46.560 --> 00:32:53.480]   we're able to predict the effect of post translational modifications or things that are added to the protein after it's been translated and
[00:32:53.480 --> 00:32:58.240]   That by using a deep learning model to do this type of prediction
[00:32:58.240 --> 00:33:00.520]   We'll be able to do it in really high throughput
[00:33:00.520 --> 00:33:06.480]   Allowing us to generate those protein libraries that I mentioned at the beginning of the talk that would allow us to do really high throughput
[00:33:06.480 --> 00:33:14.200]   Screening for functional proteins and so if we combine these types of methods with existing methods based on physics simulations
[00:33:14.640 --> 00:33:21.040]   And experimental processes we can kind of find the needle in the haystack a lot quicker than previously possible
[00:33:21.040 --> 00:33:26.920]   And with that I want to acknowledge a bunch of people that were really integral in this work
[00:33:26.920 --> 00:33:31.640]   So Dan Lan Arjun were the two members of the Fordyce lab that really helped me develop
[00:33:31.640 --> 00:33:39.080]   The Bet-Seq assay I got a lot of help from Anshul Kondaji's lab in particular these individuals listed here
[00:33:39.240 --> 00:33:45.600]   on developing some of the early neural network models for this. The Leskovec lab has been super helpful in
[00:33:45.600 --> 00:33:50.920]   Shaping my thinking about how to apply graph neural networks to the protein structure prediction problem
[00:33:50.920 --> 00:33:58.000]   And then these two collaborators down here at the bottom Yaron and Thomas were very helpful on on some of the other graph neural network
[00:33:58.000 --> 00:34:00.360]   And some work that I didn't talk about
[00:34:00.360 --> 00:34:04.280]   And with that I'll open it up for any questions
[00:34:04.280 --> 00:34:08.200]   Great, thanks Tyler
[00:34:08.640 --> 00:34:14.400]   So I see one question and people both in on YouTube and people in the zoom
[00:34:14.400 --> 00:34:20.120]   Can you please start asking your questions either in the Q&A or in the chat and I'll come get you
[00:34:20.120 --> 00:34:24.600]   The YouTube stream does run about two minutes behind
[00:34:24.600 --> 00:34:28.160]   So we'll just take that into account
[00:34:28.160 --> 00:34:35.320]   I do see one question here though, which is what is the data for doing your network model look like?
[00:34:37.000 --> 00:34:44.080]   So I'll assume but correct me if it's wrong that you're you're asking about what the inputs and outputs to that network look like
[00:34:44.080 --> 00:34:45.640]   and
[00:34:45.640 --> 00:34:53.720]   So the input to the network is just the one hot encoded DNA sequence. And so it's just a 4 by L matrix
[00:34:53.720 --> 00:34:59.680]   where the individual rows represent a T G and C and then the
[00:34:59.680 --> 00:35:03.600]   Length of that matrix represents the the length of the DNA sequence
[00:35:03.600 --> 00:35:07.960]   So that's the input and then the output is the predicted Delta Delta G value
[00:35:07.960 --> 00:35:14.640]   So that energetic value that we're actually measuring we have a noisy estimate of that from our experiment. That's what we're using as the
[00:35:14.640 --> 00:35:17.200]   the target for the training data
[00:35:17.200 --> 00:35:19.160]   Cool
[00:35:19.160 --> 00:35:22.200]   This is not a question, but Daniel Cooper on
[00:35:22.200 --> 00:35:25.160]   YouTube says his mind is blown and he's
[00:35:25.160 --> 00:35:28.600]   Someone dabbed
[00:35:28.600 --> 00:35:32.160]   So that was cool. I don't see. Oh
[00:35:32.680 --> 00:35:34.600]   I see another question
[00:35:34.600 --> 00:35:38.520]   Have you been using any of the state-of-the-art NLP libraries?
[00:35:38.520 --> 00:35:43.360]   Hugging face maybe to parse and analyze the DNA sequences
[00:35:43.360 --> 00:35:50.760]   So we we haven't been using that and what a lot of the other groups at Stanford have found really successful are actually the 2d
[00:35:50.760 --> 00:35:53.720]   CNN models so treating that 4 by L matrix
[00:35:53.720 --> 00:35:57.280]   Not as a sequence and applying NLP stuff
[00:35:57.360 --> 00:36:03.080]   But applying a lot more of the computer vision techniques as a 4 by L image basically
[00:36:03.080 --> 00:36:09.320]   So they're using that to train and test networks over a full genome sequences
[00:36:09.320 --> 00:36:16.240]   So the sequences that I was looking at here were only 10 base pairs. They routinely train on thousands or hundreds of thousands
[00:36:16.240 --> 00:36:19.200]   long sequences
[00:36:19.200 --> 00:36:20.920]   so
[00:36:20.920 --> 00:36:27.080]   Really cool talk. I was wondering so it sounds like you're you you think that the sort of 2d CNN
[00:36:27.280 --> 00:36:29.280]   Approach is gonna work better for these
[00:36:29.280 --> 00:36:31.760]   one hot and coatings
[00:36:31.760 --> 00:36:34.240]   DNA sequences. Do you have any thoughts about why?
[00:36:34.240 --> 00:36:37.480]   something like an embedding based model isn't
[00:36:37.480 --> 00:36:41.040]   what works there even though that's been so successful in NLP and
[00:36:41.040 --> 00:36:46.720]   Yeah, also, why do you think graph networks are so successful for the other
[00:36:46.720 --> 00:36:54.920]   Tasks that you've used them for. Yeah. So I think that the the convolutional networks versus recurrent networks or transformers
[00:36:54.920 --> 00:37:02.200]   I think that a lot of that comes back to not necessarily that they wouldn't be successful, but that the
[00:37:02.200 --> 00:37:08.160]   2d CNNs are they generate predictions in a way that's much more interpretable for
[00:37:08.160 --> 00:37:10.920]   biologists and for bioinformaticians
[00:37:10.920 --> 00:37:14.920]   and so we're used to thinking about these things in terms of motifs and if you think about each
[00:37:14.920 --> 00:37:22.040]   Kernel in a CNN as basically a motif finder. It's gonna apply a weight to each nucleotide at each position
[00:37:22.480 --> 00:37:27.060]   Then people have already done a lot of the heavy lifting of kind of generating
[00:37:27.060 --> 00:37:32.900]   Interpretation methods for 2d CNNs. So I think a lot of it stems from that you might see
[00:37:32.900 --> 00:37:39.600]   Greater success using some of the NLP techniques, but the other factor is that the data tend to be really noisy
[00:37:39.600 --> 00:37:45.160]   And the trick is that depending on who collects the data which group how they do it
[00:37:45.160 --> 00:37:48.160]   You run into a lot of these batch effects and things that you know
[00:37:48.160 --> 00:37:53.580]   Aren't necessarily normalized over different experimental techniques or different experimental protocols
[00:37:53.580 --> 00:38:03.120]   Cool and then Han asked what type of linear models or techniques we use in the sequencing and how has
[00:38:03.120 --> 00:38:06.320]   modeling the sequences using linear models
[00:38:06.320 --> 00:38:09.080]   Helped in designing your neural network
[00:38:09.080 --> 00:38:16.440]   Yeah, so the the linear models basically use the same inputs and outputs. And so it's just
[00:38:17.080 --> 00:38:18.520]   essentially a
[00:38:18.520 --> 00:38:22.040]   You know 40 dimensional vector where we're flattening that matrix
[00:38:22.040 --> 00:38:28.440]   So it's the same one hot encoded matrix and the same Delta Delta G values same inputs and outputs as the neural network models
[00:38:28.440 --> 00:38:32.740]   And I'll have to admit that it didn't really shape our thinking about
[00:38:32.740 --> 00:38:38.220]   Exactly how to design the neural network model. We kind of worked from the neural network model backwards
[00:38:38.220 --> 00:38:41.120]   and so we built a neural network model that fit our data and
[00:38:41.480 --> 00:38:48.840]   Was validated via these orthogonal methods and then we use the linear models to really interpret what the neural network was learning and how that
[00:38:48.840 --> 00:38:50.840]   fit with our assumptions based on
[00:38:50.840 --> 00:38:53.040]   The data sets that had already been collected
[00:38:53.040 --> 00:39:03.120]   Thanks, and then I think the last question is have you tried experimenting with graph neural networks in addition to convolutional networks?
[00:39:03.120 --> 00:39:06.520]   And if so, is any of this stuff publicly available?
[00:39:06.520 --> 00:39:10.260]   Yeah, so the the graph neural network
[00:39:10.800 --> 00:39:14.240]   Stuff that I presented at the very end is still really early stages
[00:39:14.240 --> 00:39:20.600]   We're not seeing we're not seeing the same level of prediction accuracy yet as something like AlphaGo or these
[00:39:20.600 --> 00:39:23.360]   recurrent geometric networks
[00:39:23.360 --> 00:39:28.520]   So if you're looking for the literature, I would point you to either the AlphaGo
[00:39:28.520 --> 00:39:29.680]   We're not the AlphaGo
[00:39:29.680 --> 00:39:36.040]   I'm the Alpha fold paper or a lot of the work from Muhammad al-kara she's group at Harvard does a really nice job with some
[00:39:36.040 --> 00:39:37.760]   of the recurrent networks
[00:39:37.760 --> 00:39:44.440]   They seem to be more accurate in general so far, but I think there's promise for the the graph neural network techniques
[00:39:44.440 --> 00:39:51.760]   All right, I think that's it thank you so much this is really cool, of course, thank you guys
[00:39:51.760 --> 00:39:56.240]   All right up next we have Kostov
[00:39:56.240 --> 00:40:04.920]   So Kostov is a research PhD intern at Facebook AI research and his interests lie at the intersection of
[00:40:05.640 --> 00:40:08.240]   Generative language modeling and...

