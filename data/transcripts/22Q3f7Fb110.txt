
[00:00:00.000 --> 00:00:03.440]   So I just graduated from the University of Virginia
[00:00:03.440 --> 00:00:06.080]   and joined the author's program.
[00:00:06.080 --> 00:00:11.080]   I'm, yeah, coming to you live from Arlington, Virginia.
[00:00:11.080 --> 00:00:14.480]   And I'm now basically a full-time researcher
[00:00:14.480 --> 00:00:16.280]   at the University of Virginia
[00:00:16.280 --> 00:00:17.840]   in the machine learning lab.
[00:00:17.840 --> 00:00:21.540]   We're studying this idea of adversarial attacks
[00:00:21.540 --> 00:00:24.200]   or adversarial examples in natural language
[00:00:24.200 --> 00:00:26.000]   and what those even are.
[00:00:26.000 --> 00:00:30.120]   And so through our research, we've created this library
[00:00:30.120 --> 00:00:32.640]   called TextAttack, which sort of started
[00:00:32.640 --> 00:00:37.220]   as like a shared implementation of the adversarial attacks
[00:00:37.220 --> 00:00:38.960]   from a bunch of different NLP papers.
[00:00:38.960 --> 00:00:40.880]   And it's grown to be this like framework
[00:00:40.880 --> 00:00:43.560]   or way of thinking about adversarial attacks
[00:00:43.560 --> 00:00:45.240]   and their construction.
[00:00:45.240 --> 00:00:46.840]   So I have a presentation.
[00:00:46.840 --> 00:00:49.060]   I'm just about to share my screen.
[00:00:49.060 --> 00:00:53.160]   And I'm really excited about this
[00:00:53.160 --> 00:00:56.320]   because we actually hadn't open sourced TextAttack
[00:00:56.320 --> 00:00:58.720]   until like a couple of weeks ago.
[00:00:58.720 --> 00:01:00.620]   So you guys are gonna be some of the first people
[00:01:00.620 --> 00:01:02.500]   to actually see it.
[00:01:02.500 --> 00:01:04.100]   Lavanya, can you see my screen?
[00:01:04.100 --> 00:01:06.680]   - Yes, we can. - Okay, awesome.
[00:01:06.680 --> 00:01:07.520]   We're set.
[00:01:07.520 --> 00:01:10.520]   So the title of this presentation is TextAttack,
[00:01:10.520 --> 00:01:13.400]   a Python framework for NLP attacks.
[00:01:13.400 --> 00:01:17.700]   And it's available on GitHub under Qdata/TextAttack.
[00:01:17.700 --> 00:01:19.700]   Actually, I'll show that really quickly.
[00:01:19.700 --> 00:01:22.620]   Yeah, this is our GitHub repository
[00:01:22.620 --> 00:01:24.480]   if you wanna check it out later
[00:01:24.480 --> 00:01:27.120]   and like download it, install it.
[00:01:27.120 --> 00:01:29.320]   You can run pip install TextAttack,
[00:01:29.320 --> 00:01:32.100]   run some NLP attacks for yourself.
[00:01:32.100 --> 00:01:35.280]   I wanted to give this disclaimer before I forget.
[00:01:35.280 --> 00:01:38.320]   To run an attack, you're gonna need at least a model
[00:01:38.320 --> 00:01:39.560]   and probably some data.
[00:01:39.560 --> 00:01:41.840]   So if you're running it on your laptop,
[00:01:41.840 --> 00:01:45.760]   just be warned that models like state-of-the-art NLP models
[00:01:45.760 --> 00:01:47.160]   can take up a few gigabytes.
[00:01:47.160 --> 00:01:50.960]   And just like TensorFlow Hub or any other library
[00:01:50.960 --> 00:01:52.240]   that comes with its own models,
[00:01:52.240 --> 00:01:54.680]   it's gonna download them and just save them to your hard drive.
[00:01:54.680 --> 00:01:57.360]   So yeah, just a fair word of warning.
[00:01:57.360 --> 00:02:00.400]   So I'm gonna do a couple introduction slides
[00:02:00.400 --> 00:02:03.840]   about TextAttack, talk about adversarial examples in NLP,
[00:02:03.840 --> 00:02:05.920]   then go through like a sample use case
[00:02:05.920 --> 00:02:09.440]   of an NLP attack implemented in TextAttack.
[00:02:09.440 --> 00:02:11.600]   And then at the end, I'll do a demo
[00:02:11.600 --> 00:02:13.120]   and talk about like a use case
[00:02:13.120 --> 00:02:14.880]   that might be more useful for you all,
[00:02:14.880 --> 00:02:18.080]   which is using TextAttack for data augmentation.
[00:02:18.080 --> 00:02:21.440]   So some things you can do with TextAttack
[00:02:21.440 --> 00:02:23.960]   are build and run NLP attacks.
[00:02:23.960 --> 00:02:28.220]   But instead of providing just basically
[00:02:28.220 --> 00:02:30.640]   like the implementations of a bunch of attacks,
[00:02:30.640 --> 00:02:33.920]   we've come up with this framework of four components
[00:02:33.920 --> 00:02:36.600]   that you can use to construct an NLP attack.
[00:02:36.600 --> 00:02:38.420]   So you can mix and match those components
[00:02:38.420 --> 00:02:40.160]   to customize your own attacks
[00:02:40.160 --> 00:02:42.900]   or recreate some from the literature.
[00:02:42.900 --> 00:02:44.640]   And once you've created an attack,
[00:02:44.640 --> 00:02:48.140]   you can run it via the command line or the Python interface
[00:02:48.140 --> 00:02:51.280]   and run it on a custom model or a dataset
[00:02:51.280 --> 00:02:53.760]   and visualize it using Visdom
[00:02:53.760 --> 00:02:56.480]   or just printing things to the command line,
[00:02:56.480 --> 00:02:59.820]   saving things to CSV if you wanna do data analysis
[00:02:59.820 --> 00:03:03.520]   or maybe reuse for adversarial training or whatever.
[00:03:03.520 --> 00:03:05.860]   And also now, as of last week,
[00:03:05.860 --> 00:03:09.740]   we support Weights and Biases, which is really cool.
[00:03:09.740 --> 00:03:12.060]   So you can run an attack and then automatically
[00:03:12.060 --> 00:03:14.560]   send the data off to your Weights and Biases account
[00:03:14.560 --> 00:03:16.280]   and then you'll have like a link you can send around
[00:03:16.280 --> 00:03:18.200]   or just a webpage you can go
[00:03:18.200 --> 00:03:19.680]   and see the results of the attack
[00:03:19.680 --> 00:03:22.580]   and actually look at the attack results.
[00:03:22.580 --> 00:03:26.640]   Or if you're not interested in running attacks,
[00:03:26.640 --> 00:03:29.400]   you can use TextAttack or the components of TextAttack
[00:03:29.400 --> 00:03:30.920]   to develop your own attacks,
[00:03:30.920 --> 00:03:32.800]   benchmark the attacks from the literature
[00:03:32.800 --> 00:03:35.040]   or compare them in a shared framework,
[00:03:35.040 --> 00:03:36.680]   which is one of like the main motivations
[00:03:36.680 --> 00:03:38.400]   we had for developing it.
[00:03:38.400 --> 00:03:41.200]   Or you can just take some components of TextAttack
[00:03:41.200 --> 00:03:42.480]   for whatever you need.
[00:03:42.480 --> 00:03:44.840]   So I think one of the most popular uses for TextAttack
[00:03:44.840 --> 00:03:48.520]   is the procedure for running an attack on an NLP model
[00:03:48.520 --> 00:03:52.160]   is pretty similar to just generating extra training data
[00:03:52.160 --> 00:03:53.580]   using data augmentation.
[00:03:53.580 --> 00:03:55.960]   So you can just use some components of TextAttack
[00:03:55.960 --> 00:03:57.680]   and we have some built in features for that
[00:03:57.680 --> 00:03:59.400]   that I'll talk about at the end.
[00:03:59.400 --> 00:04:02.120]   So TextAttack is for everyone,
[00:04:02.120 --> 00:04:05.600]   but I wrote down three different like popular use cases
[00:04:05.600 --> 00:04:06.760]   for TextAttack.
[00:04:06.760 --> 00:04:09.060]   So anyone who's doing research on robustness
[00:04:09.060 --> 00:04:11.560]   or adversarial examples in natural language
[00:04:11.560 --> 00:04:15.180]   can use TextAttack to implement attacks or compare them.
[00:04:15.180 --> 00:04:17.160]   If you're just a machine learning practitioner
[00:04:17.160 --> 00:04:18.760]   who's interested in these ideas,
[00:04:18.760 --> 00:04:23.000]   you can download TextAttack and run it, look in the data.
[00:04:23.000 --> 00:04:28.000]   We're very big proponents of getting into the data
[00:04:28.000 --> 00:04:30.840]   and actually looking at examples
[00:04:30.840 --> 00:04:35.160]   like scrolling through 10s or 100s of adversarial examples
[00:04:35.160 --> 00:04:37.120]   produced to understand your model's output
[00:04:37.120 --> 00:04:39.620]   and especially the attack procedure.
[00:04:39.620 --> 00:04:41.200]   And the third use case I wrote down
[00:04:41.200 --> 00:04:42.880]   was anyone who's training an NLP model,
[00:04:42.880 --> 00:04:44.340]   even if you're not interested in
[00:04:44.340 --> 00:04:45.780]   maybe your model's robustness
[00:04:45.780 --> 00:04:49.700]   or exploring what your model's blind spots are,
[00:04:49.700 --> 00:04:51.680]   you might just be looking for some way
[00:04:51.680 --> 00:04:53.720]   to increase your test set accuracy.
[00:04:53.720 --> 00:04:56.200]   And depending on the task,
[00:04:56.200 --> 00:04:59.520]   data augmentation can generally give you
[00:04:59.520 --> 00:05:01.840]   like a one or 2% test set increase
[00:05:01.840 --> 00:05:04.640]   just for free on whatever task you're training on.
[00:05:04.640 --> 00:05:08.120]   So even if you're not interested in like security
[00:05:08.120 --> 00:05:10.560]   or robustness, data augmentation with TextAttack
[00:05:10.560 --> 00:05:11.400]   is really easy.
[00:05:11.400 --> 00:05:15.080]   So I'm gonna talk about adversarial examples
[00:05:15.080 --> 00:05:16.740]   and thankfully my previous presenter
[00:05:16.740 --> 00:05:17.880]   did some of the work for me
[00:05:17.880 --> 00:05:20.360]   so I can just skip some of my stuff.
[00:05:20.360 --> 00:05:23.820]   And what the idea of adversarial examples are in NLP,
[00:05:23.820 --> 00:05:26.940]   then how that ties into our framework,
[00:05:26.940 --> 00:05:30.300]   like the four components that make up an NLP attack
[00:05:30.300 --> 00:05:32.320]   as in TextAttack.
[00:05:32.320 --> 00:05:33.640]   Then I'm gonna do a short demo,
[00:05:33.640 --> 00:05:36.080]   just like run some stuff on the command line.
[00:05:36.080 --> 00:05:37.960]   And then I'll talk about data augmentation
[00:05:37.960 --> 00:05:39.200]   and take some questions.
[00:05:40.300 --> 00:05:44.700]   So a little bit of terminology before we get started.
[00:05:44.700 --> 00:05:49.740]   We're gonna refer to changes that fool the model
[00:05:49.740 --> 00:05:53.280]   as adversarial examples or adversarial perturbations.
[00:05:53.280 --> 00:05:55.360]   Personally, I prefer the terminology
[00:05:55.360 --> 00:05:58.260]   adversarial perturbation because that implies
[00:05:58.260 --> 00:05:59.900]   that there's some sort of starting point
[00:05:59.900 --> 00:06:01.540]   that's been perturbed.
[00:06:01.540 --> 00:06:03.260]   And I think it's a little more clear
[00:06:03.260 --> 00:06:04.460]   than adversarial examples,
[00:06:04.460 --> 00:06:07.540]   which sometimes are things that the adversary
[00:06:07.540 --> 00:06:08.500]   creates from scratch,
[00:06:08.500 --> 00:06:11.260]   which is not something that like TextAttack does.
[00:06:11.260 --> 00:06:13.380]   We're talking about adversarial perturbations.
[00:06:13.380 --> 00:06:15.460]   And the program that creates adversarial examples
[00:06:15.460 --> 00:06:17.620]   is known as the adversarial attack.
[00:06:17.620 --> 00:06:20.740]   And the model's resistance for adversarial examples
[00:06:20.740 --> 00:06:23.820]   or like basically the model's level of security
[00:06:23.820 --> 00:06:26.820]   in this context is called the robustness.
[00:06:26.820 --> 00:06:29.820]   So a model that's more susceptible to adversarial examples
[00:06:29.820 --> 00:06:31.420]   is less robust.
[00:06:31.420 --> 00:06:33.060]   And so that's some really common terminology,
[00:06:33.060 --> 00:06:35.820]   whether you're talking about NLP models or vision models.
[00:06:35.820 --> 00:06:37.380]   We actually just talked about this.
[00:06:37.380 --> 00:06:39.740]   This is a picture of a pig
[00:06:39.740 --> 00:06:42.060]   that when a small amount of noise is added,
[00:06:42.060 --> 00:06:46.220]   it's grossly misclassified by some ImageNet classifier.
[00:06:46.220 --> 00:06:48.460]   And this is like the idea of an adversarial example,
[00:06:48.460 --> 00:06:50.900]   which made a big splash when the first paper
[00:06:50.900 --> 00:06:53.620]   was published by Ian Goodfellow in 2014.
[00:06:53.620 --> 00:06:56.700]   And a lot of people have been pursuing
[00:06:56.700 --> 00:06:58.420]   this line of research since then.
[00:06:58.420 --> 00:07:01.940]   But it's still sort of an open question,
[00:07:01.940 --> 00:07:04.020]   what are adversarial examples in NLP?
[00:07:04.020 --> 00:07:06.620]   Because like, if you look at this picture,
[00:07:06.620 --> 00:07:10.540]   this pig on the left and the pig on the right,
[00:07:10.540 --> 00:07:12.460]   these are indistinguishable images.
[00:07:12.460 --> 00:07:17.260]   Like this perturbation, 0.005, is so small
[00:07:17.260 --> 00:07:19.700]   that it's literally too small
[00:07:19.700 --> 00:07:21.580]   for the human eye to distinguish.
[00:07:21.580 --> 00:07:25.540]   There's no analogous example in text.
[00:07:25.540 --> 00:07:27.980]   You can't produce a sequence of text on the left,
[00:07:27.980 --> 00:07:29.660]   a sequence of text on the right
[00:07:29.660 --> 00:07:31.660]   that two humans can't tell apart,
[00:07:31.660 --> 00:07:33.140]   or one human can't tell apart,
[00:07:33.140 --> 00:07:37.020]   unless those sequences of text are the same.
[00:07:37.020 --> 00:07:39.140]   So one idea researchers have proposed
[00:07:39.140 --> 00:07:41.380]   are adversarial examples in text
[00:07:41.380 --> 00:07:44.780]   are two sequences that are almost visually indistinguishable.
[00:07:44.780 --> 00:07:47.740]   So maybe they differ by a few words or characters,
[00:07:47.740 --> 00:07:49.340]   and someone who's reading really quickly
[00:07:49.340 --> 00:07:50.380]   can't tell the difference.
[00:07:50.380 --> 00:07:52.740]   So let's look at an example.
[00:07:52.740 --> 00:07:54.140]   This is an input.
[00:07:54.140 --> 00:07:58.220]   It's a movie review, inputted to a sentiment classifier.
[00:07:58.220 --> 00:08:00.700]   It says, "True Grit was the best movie I've seen
[00:08:00.700 --> 00:08:03.020]   "since I was a small boy."
[00:08:03.020 --> 00:08:05.500]   So under this definition of an adversarial example,
[00:08:05.500 --> 00:08:08.020]   anything that makes a small character change
[00:08:08.020 --> 00:08:10.900]   or maybe a few character changes
[00:08:10.900 --> 00:08:13.180]   is considered an imperceptible change,
[00:08:13.180 --> 00:08:16.540]   and one that introduces a misclassification
[00:08:16.540 --> 00:08:18.980]   is called an adversarial example.
[00:08:18.980 --> 00:08:19.820]   So on the right here,
[00:08:19.820 --> 00:08:23.220]   we see the string has been edited a slight bit.
[00:08:23.220 --> 00:08:25.380]   Actually, no characters have been added or removed.
[00:08:25.380 --> 00:08:26.660]   They've just been moved around,
[00:08:26.660 --> 00:08:29.940]   so it's pretty easy to parse,
[00:08:29.940 --> 00:08:32.420]   but the classification is negative.
[00:08:32.420 --> 00:08:34.420]   So if you're training a model and you see this example,
[00:08:34.420 --> 00:08:35.860]   that's pretty interesting,
[00:08:35.860 --> 00:08:38.580]   but you can also probably defend your system
[00:08:38.580 --> 00:08:39.620]   from this type of attack.
[00:08:39.620 --> 00:08:43.780]   Like one way to do it would be just install a spell checker,
[00:08:43.780 --> 00:08:47.060]   and before passing any input to your model,
[00:08:47.060 --> 00:08:48.860]   whether it's like a sentiment classifier
[00:08:48.860 --> 00:08:50.340]   or a translation model,
[00:08:50.340 --> 00:08:54.540]   you make sure it passes a simple grammar check.
[00:08:54.540 --> 00:08:55.820]   Some other work has come out
[00:08:55.820 --> 00:08:58.700]   that did a more complicated thing
[00:08:58.700 --> 00:09:01.020]   where they trained like a recurrent neural network
[00:09:01.020 --> 00:09:04.140]   to take the input on the right and then correct it.
[00:09:04.140 --> 00:09:05.980]   So like it would take this input
[00:09:05.980 --> 00:09:08.100]   and then produce hopefully the input on the left,
[00:09:08.100 --> 00:09:09.540]   basically just a spelling corrector
[00:09:09.540 --> 00:09:12.700]   and a correct for character deletions and insertions,
[00:09:12.700 --> 00:09:14.860]   and that was proposed as another defense
[00:09:14.860 --> 00:09:15.860]   against this sort of attack,
[00:09:15.860 --> 00:09:18.220]   and actually they work really well.
[00:09:18.220 --> 00:09:19.980]   And I'm gonna talk about one other idea
[00:09:19.980 --> 00:09:21.420]   of adversarial examples in NLP,
[00:09:21.420 --> 00:09:26.420]   which is probably more relevant to the recent literature
[00:09:26.420 --> 00:09:28.860]   and like we implement more of these approaches
[00:09:28.860 --> 00:09:29.940]   in text attack.
[00:09:29.940 --> 00:09:33.220]   So this idea is that adversarial examples in NLP
[00:09:33.220 --> 00:09:35.220]   are sequences that are indistinguishable
[00:09:35.220 --> 00:09:37.340]   in meaning to the original input.
[00:09:37.340 --> 00:09:40.660]   So if we look at the same input from the previous slide,
[00:09:40.660 --> 00:09:42.940]   the movie review about true grit,
[00:09:42.940 --> 00:09:45.500]   we could imagine some different paraphrases
[00:09:45.500 --> 00:09:47.500]   that contain all the same meaning,
[00:09:47.500 --> 00:09:51.540]   but change the output of the classifier.
[00:09:51.540 --> 00:09:54.100]   So this is an example that I made up,
[00:09:54.100 --> 00:09:55.700]   but could well be true,
[00:09:55.700 --> 00:10:00.540]   where we substituted the word small and boy for we lad.
[00:10:00.540 --> 00:10:01.740]   So the whole sentence,
[00:10:01.740 --> 00:10:03.500]   true grit was the best movie I've seen
[00:10:03.500 --> 00:10:05.460]   since I was a we lad,
[00:10:05.460 --> 00:10:10.460]   was predicted a negative sentiment by the model.
[00:10:10.460 --> 00:10:15.820]   I'm not exactly sure why that would be,
[00:10:15.820 --> 00:10:18.540]   maybe because the model hasn't seen enough examples
[00:10:18.540 --> 00:10:20.220]   of these words or this phraseage,
[00:10:20.220 --> 00:10:22.580]   or for some reason it thinks these two words
[00:10:22.580 --> 00:10:23.780]   have a negative connotation,
[00:10:23.780 --> 00:10:25.060]   but in any event,
[00:10:25.060 --> 00:10:27.340]   if you trained a sentiment classifier and you saw this,
[00:10:27.340 --> 00:10:28.980]   this is a bug.
[00:10:28.980 --> 00:10:32.020]   You would use this as an example for retraining
[00:10:32.020 --> 00:10:35.140]   or just some data for understanding
[00:10:35.140 --> 00:10:37.820]   how your model misclassifies some test points.
[00:10:37.820 --> 00:10:43.700]   So this is actually not like a full sentence paraphrase.
[00:10:43.700 --> 00:10:46.220]   This is just by swapping out small for we,
[00:10:46.220 --> 00:10:48.540]   which is a direct synonym substitution,
[00:10:48.540 --> 00:10:49.660]   and boy for lad,
[00:10:49.660 --> 00:10:52.220]   which is also a direct synonym substitution.
[00:10:52.220 --> 00:10:55.020]   It turns out that doing actual paraphrases
[00:10:55.020 --> 00:10:57.300]   with machine learning models is really hard
[00:10:57.300 --> 00:10:59.500]   and no one's done it yet
[00:10:59.500 --> 00:11:01.700]   or done it well enough to be applied
[00:11:01.700 --> 00:11:03.820]   in this adversarial example context
[00:11:03.820 --> 00:11:05.980]   where the model can pick up on any small change
[00:11:05.980 --> 00:11:07.700]   to the original input.
[00:11:07.700 --> 00:11:09.460]   I think the closest thing I ever heard of
[00:11:09.460 --> 00:11:11.740]   that was actual neural paraphrase
[00:11:11.740 --> 00:11:14.580]   was this paper that Google came out with.
[00:11:14.580 --> 00:11:16.860]   It's called Ask the Right Questions,
[00:11:16.860 --> 00:11:21.100]   question reformulation using reinforcement learning
[00:11:21.100 --> 00:11:22.540]   or something like that,
[00:11:22.540 --> 00:11:26.100]   where they trained one model to paraphrase inputs
[00:11:26.100 --> 00:11:29.300]   for another model to try and optimize,
[00:11:29.300 --> 00:11:31.940]   to try and, like basically it took a question
[00:11:31.940 --> 00:11:34.740]   and rephrase the question so that another model
[00:11:34.740 --> 00:11:35.700]   would have the best chance
[00:11:35.700 --> 00:11:37.380]   of answering the question correctly.
[00:11:37.380 --> 00:11:39.620]   But anyways, paraphrase is hard
[00:11:39.620 --> 00:11:43.980]   and most NLP attacks focus on this idea
[00:11:43.980 --> 00:11:45.740]   of a synonym substitution,
[00:11:45.740 --> 00:11:48.540]   like here taking small and replacing it with we.
[00:11:48.540 --> 00:11:50.620]   So there's some different ways to do this.
[00:11:50.620 --> 00:11:54.380]   I think the most obvious would be use a thesaurus,
[00:11:54.380 --> 00:11:58.020]   like take something that lists the synonyms for words,
[00:11:58.020 --> 00:12:00.540]   look up the word and then try replacing all the synonyms
[00:12:00.540 --> 00:12:03.660]   and see how that affects the classification score.
[00:12:03.660 --> 00:12:07.140]   This would work, but as it turns out,
[00:12:07.140 --> 00:12:08.820]   it's just not gonna be enough.
[00:12:08.820 --> 00:12:12.180]   I think that no matter how much time you spend on it,
[00:12:12.180 --> 00:12:14.500]   your thesaurus isn't gonna have that many entries
[00:12:14.500 --> 00:12:17.420]   and it's not gonna be able to keep a current list
[00:12:17.420 --> 00:12:20.540]   of all the synonyms for all the usages of every word.
[00:12:20.540 --> 00:12:24.260]   So a more feasible approach is by using word embeddings.
[00:12:24.260 --> 00:12:25.860]   If you take the embeddings of words
[00:12:25.860 --> 00:12:27.460]   that are trained by a model
[00:12:27.460 --> 00:12:29.300]   and look at their nearest neighbors,
[00:12:29.300 --> 00:12:30.980]   you're gonna get a pretty good idea
[00:12:30.980 --> 00:12:32.580]   of what their synonyms are.
[00:12:32.580 --> 00:12:35.060]   Like for example, small might have neighbors
[00:12:35.060 --> 00:12:39.660]   like little and tiny, but this introduces a problem
[00:12:39.660 --> 00:12:43.020]   because another neighbor to small in the embedding space
[00:12:43.020 --> 00:12:46.460]   might be big because they're used in very similar contexts,
[00:12:46.460 --> 00:12:49.100]   even though they have different meanings when used.
[00:12:49.100 --> 00:12:51.820]   So people dealt with this problem
[00:12:51.820 --> 00:12:54.060]   by creating a new type of embedding.
[00:12:54.060 --> 00:12:55.380]   They take the original embeddings
[00:12:55.380 --> 00:12:58.740]   and apply this post-processing step called counterfeiting,
[00:12:58.740 --> 00:13:02.100]   where they take the thesaurus words
[00:13:02.100 --> 00:13:04.740]   and try to make synonyms really close together
[00:13:04.740 --> 00:13:06.660]   and antonyms far apart.
[00:13:06.660 --> 00:13:09.580]   So there's actually one set of counterfeited word vectors
[00:13:09.580 --> 00:13:12.340]   from this paper, 2016 paper,
[00:13:12.340 --> 00:13:16.340]   that is used in a lot of different applications,
[00:13:16.340 --> 00:13:20.940]   including some of the most prominent NLP attacks
[00:13:20.940 --> 00:13:21.980]   in the literature.
[00:13:21.980 --> 00:13:24.420]   So counterfeited vectors are something that we use often
[00:13:24.420 --> 00:13:26.820]   in text attack for doing word replacements.
[00:13:26.820 --> 00:13:33.180]   This step by going from the sentence about the small boy
[00:13:33.180 --> 00:13:35.020]   to the sentence about the wee lad
[00:13:35.020 --> 00:13:38.620]   is within the text attack context, a transformation,
[00:13:38.620 --> 00:13:40.620]   which is one of our four components.
[00:13:40.620 --> 00:13:42.820]   A transformation takes an input
[00:13:42.820 --> 00:13:46.060]   and produces a list of potential adversarial examples.
[00:13:46.060 --> 00:13:49.980]   But this brings us to the next question,
[00:13:49.980 --> 00:13:51.860]   like what happens when it messes up?
[00:13:51.860 --> 00:13:54.580]   I put two examples of maybe questionable
[00:13:54.580 --> 00:13:56.860]   adversarial perturbations on the right.
[00:13:56.860 --> 00:14:00.540]   One says, "True Grit was the worst film I've seen
[00:14:00.540 --> 00:14:02.260]   "since I was a small boy."
[00:14:02.260 --> 00:14:05.300]   And one says, "True Grit was the best movie I've seen
[00:14:05.300 --> 00:14:07.260]   "since me were a small boy."
[00:14:07.260 --> 00:14:10.220]   These are both reasonable transformations
[00:14:10.220 --> 00:14:12.660]   that could be proposed by the thesaurus
[00:14:12.660 --> 00:14:15.380]   or the embedding or the counterfeited embedding word
[00:14:15.380 --> 00:14:17.420]   or synonym swap method.
[00:14:17.420 --> 00:14:19.660]   So they could all be like feasibly produced
[00:14:19.660 --> 00:14:22.060]   by what we just suggested.
[00:14:22.060 --> 00:14:23.900]   The first one's a problem because I mean,
[00:14:23.900 --> 00:14:25.020]   it changes the sentiment.
[00:14:25.020 --> 00:14:26.780]   This is actually correctly classified.
[00:14:26.780 --> 00:14:28.860]   If it says, "True Grit was the worst film I've seen
[00:14:28.860 --> 00:14:31.700]   "since I was a boy," that's a very negative review
[00:14:31.700 --> 00:14:35.060]   and the model's producing it or predicting it correctly.
[00:14:35.060 --> 00:14:38.620]   The second one is maybe still a positive review.
[00:14:38.620 --> 00:14:40.140]   Yeah, it's probably a positive review,
[00:14:40.140 --> 00:14:43.540]   but maybe you don't care how your model behaves on this
[00:14:43.540 --> 00:14:45.980]   because this is just grammatically incorrect.
[00:14:45.980 --> 00:14:47.980]   Either way, it's definitely in more of it
[00:14:47.980 --> 00:14:51.220]   in the space of inputs.
[00:14:51.220 --> 00:14:54.380]   And so the first one violates the semantics
[00:14:54.380 --> 00:14:56.620]   of the original input and the second one
[00:14:56.620 --> 00:14:58.340]   violates the grammaticality.
[00:14:58.340 --> 00:15:02.540]   In either case, we're gonna wanna like filter out
[00:15:02.540 --> 00:15:04.420]   things like this during our NLP attack.
[00:15:04.420 --> 00:15:08.100]   So when our embedding suggests replacing best with worst,
[00:15:08.100 --> 00:15:09.940]   we wanna have some kind of catch
[00:15:09.940 --> 00:15:11.700]   that'll prevent us from making that replacement.
[00:15:11.700 --> 00:15:13.580]   Same with making these grammatical mistakes
[00:15:13.580 --> 00:15:15.340]   in the second example.
[00:15:15.340 --> 00:15:18.060]   So here's two ideas that could have prevented
[00:15:18.060 --> 00:15:20.220]   those two cases from the last slide.
[00:15:20.220 --> 00:15:23.100]   One, we could have produced sentence embeddings
[00:15:23.100 --> 00:15:26.140]   from the input on the left and the input on the right
[00:15:26.140 --> 00:15:29.780]   or X and Xab and compare their sentence embeddings.
[00:15:29.780 --> 00:15:33.580]   So the idea is that the first input and the second input
[00:15:33.580 --> 00:15:37.820]   would have a low correlation or a small cosine similarity
[00:15:37.820 --> 00:15:39.140]   between their sentence embeddings
[00:15:39.140 --> 00:15:41.380]   because they mean very different things.
[00:15:41.380 --> 00:15:43.220]   And then the second idea proposed here
[00:15:43.220 --> 00:15:45.060]   is just use a grammar checker.
[00:15:45.060 --> 00:15:46.900]   This sentence is grammatically correct.
[00:15:46.900 --> 00:15:48.020]   This sentence is not.
[00:15:48.020 --> 00:15:50.660]   It shouldn't be allowed in our NLP attack.
[00:15:50.660 --> 00:15:54.660]   So these are very common to have this kind of like catch
[00:15:54.660 --> 00:15:59.660]   or like to have some function that you make sure
[00:15:59.660 --> 00:16:02.500]   that you ensure your adversarial example meets
[00:16:02.500 --> 00:16:04.060]   as part of your attack.
[00:16:04.060 --> 00:16:06.380]   And we see this all the time in the NLP attack literature,
[00:16:06.380 --> 00:16:08.060]   whether it's using the sentence encoder
[00:16:08.060 --> 00:16:09.460]   or a language model to make sure
[00:16:09.460 --> 00:16:12.780]   that the adversarial example actually makes sense.
[00:16:12.780 --> 00:16:15.140]   So this is the second component of the text attack framework
[00:16:15.140 --> 00:16:16.900]   which we call constraints.
[00:16:16.900 --> 00:16:19.660]   So once you produce your set of transformations,
[00:16:19.660 --> 00:16:21.420]   you check all the constraints to make sure
[00:16:21.420 --> 00:16:22.980]   that the transformations are valid.
[00:16:22.980 --> 00:16:26.380]   So hopefully that'll filter out the bad examples
[00:16:26.380 --> 00:16:27.940]   that we've shown on this slide.
[00:16:27.940 --> 00:16:31.100]   Next slide.
[00:16:31.100 --> 00:16:32.900]   So these are the last two pieces
[00:16:32.900 --> 00:16:34.540]   of the text attack framework.
[00:16:34.540 --> 00:16:36.980]   Once you've taken your set of transformations
[00:16:36.980 --> 00:16:38.900]   and filtered it by the constraints,
[00:16:38.900 --> 00:16:41.980]   you still need a way to actually search the transformations
[00:16:41.980 --> 00:16:43.580]   and apply them over and over again
[00:16:43.580 --> 00:16:46.140]   until you find a valid adversarial example.
[00:16:46.140 --> 00:16:49.500]   So this is what we call the search method in text attack.
[00:16:49.500 --> 00:16:51.380]   There are different search methods that are common.
[00:16:51.380 --> 00:16:54.860]   You could just use some like greedy method
[00:16:54.860 --> 00:16:57.700]   that tries the most promising word to swap
[00:16:57.700 --> 00:16:58.540]   over and over again.
[00:16:58.540 --> 00:17:00.460]   That would be like a fast way to do it.
[00:17:00.460 --> 00:17:03.700]   Or you could use something slower like theme search.
[00:17:03.700 --> 00:17:06.500]   And the last component we need is actually a way
[00:17:06.500 --> 00:17:08.420]   to know whether our example is successful.
[00:17:08.420 --> 00:17:11.980]   So once we apply the search or iteratively search
[00:17:11.980 --> 00:17:13.340]   and get a list of transformations,
[00:17:13.340 --> 00:17:15.540]   how do we know when to stop?
[00:17:15.540 --> 00:17:17.660]   A common case, like the case we said before,
[00:17:17.660 --> 00:17:19.860]   is when the class has changed.
[00:17:19.860 --> 00:17:24.460]   So the positive review was perturbed to a negative review.
[00:17:24.460 --> 00:17:25.860]   The original class was positive.
[00:17:25.860 --> 00:17:27.860]   Once that class changed, we stop.
[00:17:27.860 --> 00:17:31.020]   That's generally called like untargeted classification.
[00:17:31.020 --> 00:17:34.020]   Another common one is trying to induce a particular class,
[00:17:34.020 --> 00:17:36.060]   which would be targeted classification.
[00:17:36.940 --> 00:17:39.660]   In any event, this is the fourth component.
[00:17:39.660 --> 00:17:42.540]   And so this is called the goal function in text attack.
[00:17:42.540 --> 00:17:44.940]   And these are all pretty similarly named in the code.
[00:17:44.940 --> 00:17:47.740]   So before we had text attack dot transformations,
[00:17:47.740 --> 00:17:49.500]   text attack dot constraints,
[00:17:49.500 --> 00:17:51.540]   this is search methods and goal functions.
[00:17:51.540 --> 00:17:55.620]   And the idea is that we implement a lot of search methods,
[00:17:55.620 --> 00:17:57.620]   a lot of goal functions, a lot of transformations,
[00:17:57.620 --> 00:17:59.500]   and a lot of constraints along the way
[00:17:59.500 --> 00:18:02.700]   we recreate as many attacks as possible.
[00:18:02.700 --> 00:18:05.540]   And then we can mix and match these components
[00:18:05.540 --> 00:18:07.500]   to try and learn things about them
[00:18:07.500 --> 00:18:09.940]   and learn things about our models.
[00:18:09.940 --> 00:18:11.460]   So now I'm gonna explain the framework
[00:18:11.460 --> 00:18:13.180]   a little more formally.
[00:18:13.180 --> 00:18:15.700]   These are the four pieces I talked about before.
[00:18:15.700 --> 00:18:18.500]   The text attack framework says that NLP attacks
[00:18:18.500 --> 00:18:20.420]   are constructed from four components.
[00:18:20.420 --> 00:18:24.660]   Transformation, which is like the synonym substitutions
[00:18:24.660 --> 00:18:25.940]   that we've mentioned before,
[00:18:25.940 --> 00:18:28.860]   generates a list of potential perturbations,
[00:18:28.860 --> 00:18:30.940]   constraints that filter out perturbations
[00:18:30.940 --> 00:18:32.220]   that we don't want,
[00:18:32.220 --> 00:18:34.980]   the goal function, which tells us when we can stop,
[00:18:34.980 --> 00:18:37.220]   and the search method that if the goal function
[00:18:37.220 --> 00:18:38.300]   isn't fulfilled,
[00:18:38.300 --> 00:18:40.260]   takes the list of potential perturbations
[00:18:40.260 --> 00:18:41.940]   and applies the transformation again
[00:18:41.940 --> 00:18:44.060]   to the most promising ones.
[00:18:44.060 --> 00:18:46.060]   So now we're gonna look at like an attack
[00:18:46.060 --> 00:18:48.020]   from the literature and how we would implement it
[00:18:48.020 --> 00:18:50.380]   within text attack, and then we'll do the demo.
[00:18:50.380 --> 00:18:53.740]   Oh yeah, and I mentioned before,
[00:18:53.740 --> 00:18:56.180]   these are just the classes in text attack.
[00:18:56.180 --> 00:18:59.740]   So goal functions actually extend the goal function class,
[00:18:59.740 --> 00:19:01.540]   same with constraints and transformations.
[00:19:01.540 --> 00:19:04.260]   We're trying to make the API as straightforward as possible
[00:19:04.260 --> 00:19:08.780]   so that we can continue to think within this framework.
[00:19:08.780 --> 00:19:11.780]   So this is a graph of like,
[00:19:11.780 --> 00:19:13.420]   or a chart of all the things
[00:19:13.420 --> 00:19:15.620]   within the text attack ecosystem.
[00:19:15.620 --> 00:19:17.340]   You can see the four components here,
[00:19:17.340 --> 00:19:19.020]   transformations, constraints,
[00:19:19.020 --> 00:19:20.780]   search methods and goal functions,
[00:19:20.780 --> 00:19:22.700]   which vary by task.
[00:19:22.700 --> 00:19:24.700]   And we don't have to go through everything here,
[00:19:24.700 --> 00:19:27.740]   but we support things like synonym substitutions,
[00:19:27.740 --> 00:19:28.860]   swapping characters,
[00:19:28.860 --> 00:19:31.500]   like in the first definition of adversarial examples,
[00:19:31.500 --> 00:19:33.140]   and then all sorts of constraints,
[00:19:33.140 --> 00:19:35.740]   like constraining just based on edit distance
[00:19:35.740 --> 00:19:38.420]   or based on the sentence encoder similarity,
[00:19:38.420 --> 00:19:40.260]   based on language model predictions
[00:19:40.260 --> 00:19:43.380]   or the grammatical errors from a free grammar checker,
[00:19:43.380 --> 00:19:46.180]   different search methods and goal functions.
[00:19:46.180 --> 00:19:48.260]   So the idea is that this will keep growing
[00:19:48.260 --> 00:19:49.260]   and along the way,
[00:19:49.260 --> 00:19:52.260]   we'll be able to construct examples from the literature
[00:19:52.260 --> 00:19:53.660]   from these components.
[00:19:53.660 --> 00:19:56.460]   So here's an example paper.
[00:19:56.460 --> 00:19:59.060]   This paper is called, "Is BERT really robust?"
[00:19:59.060 --> 00:20:01.180]   And it was published about a year ago.
[00:20:01.180 --> 00:20:04.620]   It proposes a greedy algorithm called TextFooler
[00:20:04.620 --> 00:20:06.540]   for attacking NLP models.
[00:20:06.540 --> 00:20:10.940]   And they released their code
[00:20:10.940 --> 00:20:14.020]   and I think somewhere they show
[00:20:14.020 --> 00:20:16.380]   that they have a 97% success rate
[00:20:16.380 --> 00:20:18.780]   attacking BERT within their constraints.
[00:20:18.780 --> 00:20:20.340]   So on page two, they have this,
[00:20:20.340 --> 00:20:23.140]   which is the pseudocode for TextFooler.
[00:20:23.140 --> 00:20:25.620]   What we did is go through the pseudocode
[00:20:25.620 --> 00:20:26.660]   and the actual code
[00:20:26.660 --> 00:20:29.780]   and trying to still out the four components of text attack,
[00:20:29.780 --> 00:20:31.580]   the transformation, constraints,
[00:20:31.580 --> 00:20:33.500]   goal function and search method.
[00:20:33.500 --> 00:20:35.740]   So I already did it for you.
[00:20:35.740 --> 00:20:37.740]   If we look at the pseudocode,
[00:20:37.740 --> 00:20:39.700]   we're gonna be able to spot everything.
[00:20:39.700 --> 00:20:42.820]   So they suggest a greedy search method,
[00:20:42.820 --> 00:20:44.700]   which with word importance ranking,
[00:20:44.700 --> 00:20:46.540]   which is something like what I said before,
[00:20:46.540 --> 00:20:48.660]   is they come up with an order
[00:20:48.660 --> 00:20:52.780]   to find the most salient or important words to the model
[00:20:52.780 --> 00:20:55.500]   and substitute those in order to try and save time
[00:20:55.500 --> 00:20:57.540]   and minimize the number of words swapped.
[00:20:57.540 --> 00:20:59.060]   They swap words with their neighbors
[00:20:59.060 --> 00:21:01.140]   in the counterfeited embedding space.
[00:21:01.140 --> 00:21:02.420]   They have three constraints,
[00:21:02.420 --> 00:21:05.820]   one on the cosine similarity between the swap words,
[00:21:05.820 --> 00:21:09.060]   one to ensure the part of speech is the same,
[00:21:09.060 --> 00:21:11.300]   one to ensure small sentence embedding
[00:21:11.300 --> 00:21:14.020]   or a large sentence embedding cosine similarity.
[00:21:14.020 --> 00:21:17.140]   And they go until the YK,
[00:21:17.140 --> 00:21:20.220]   the results, the prediction of the candidate
[00:21:20.220 --> 00:21:22.780]   is different than the prediction of the original input.
[00:21:22.780 --> 00:21:24.220]   So this is basically like going
[00:21:24.220 --> 00:21:26.740]   until the class scores change.
[00:21:26.740 --> 00:21:29.340]   This is what we'd call untargeted classification.
[00:21:29.340 --> 00:21:33.220]   And if you go back to the chart from before,
[00:21:33.220 --> 00:21:34.900]   we've implemented all these things,
[00:21:34.900 --> 00:21:36.700]   all three constraints, the transformation,
[00:21:36.700 --> 00:21:38.900]   the search method and the goal function.
[00:21:38.900 --> 00:21:41.460]   So if we just take those components,
[00:21:41.460 --> 00:21:43.900]   the transformation, the three constraints,
[00:21:43.900 --> 00:21:46.300]   the search method and the goal function,
[00:21:46.300 --> 00:21:48.100]   we can just recreate this attack
[00:21:48.100 --> 00:21:50.420]   in text attack with a few lines of code.
[00:21:50.420 --> 00:21:54.020]   So we actually have a part of our package,
[00:21:54.020 --> 00:21:57.180]   like a module called textattack.attackrecipes,
[00:21:57.180 --> 00:21:59.180]   which includes code like this,
[00:21:59.180 --> 00:22:01.420]   which is a re-implementation of the attack
[00:22:01.420 --> 00:22:03.540]   based on those four components.
[00:22:03.540 --> 00:22:06.860]   And this shows like how little code it takes.
[00:22:06.860 --> 00:22:07.940]   So this is word swap
[00:22:07.940 --> 00:22:10.100]   and the counterfeited embedding space.
[00:22:10.100 --> 00:22:11.900]   These are the three constraints,
[00:22:11.900 --> 00:22:13.540]   word embedding distance, part of speech
[00:22:13.540 --> 00:22:15.260]   and universal sentence encoder.
[00:22:15.260 --> 00:22:16.460]   Here's the goal function,
[00:22:16.460 --> 00:22:19.180]   which is untargeted classification.
[00:22:19.180 --> 00:22:20.900]   And lastly, the search method,
[00:22:20.900 --> 00:22:23.620]   the attack base is just greedy word swap
[00:22:23.620 --> 00:22:24.820]   with word importance ranking.
[00:22:24.820 --> 00:22:26.420]   So this is the actual syntax
[00:22:26.420 --> 00:22:29.100]   for creating the attack within text attack.
[00:22:29.100 --> 00:22:31.020]   This is the code in GitHub.
[00:22:31.020 --> 00:22:34.020]   And now I'm gonna do a quick demo
[00:22:34.020 --> 00:22:36.020]   of actually running that recipe.
[00:22:36.020 --> 00:22:41.020]   So I realized I shared a screen that's not my terminal.
[00:22:41.020 --> 00:22:44.940]   Okay, now I'm sharing my terminal.
[00:22:44.940 --> 00:22:47.740]   Okay, cool.
[00:22:47.740 --> 00:22:49.420]   So I've already installed textattack
[00:22:49.420 --> 00:22:51.500]   via pip install textattack.
[00:22:51.500 --> 00:22:55.180]   So to run it, I call the Python module,
[00:22:55.180 --> 00:22:58.900]   Python-M, type textattack.
[00:22:58.900 --> 00:23:01.420]   And then I'm gonna type --help here.
[00:23:01.420 --> 00:23:04.940]   I have it installed like the editable version,
[00:23:04.940 --> 00:23:06.860]   which means that it takes a few seconds
[00:23:06.860 --> 00:23:08.580]   to load and everything.
[00:23:08.580 --> 00:23:10.380]   But by typing --help,
[00:23:10.380 --> 00:23:12.900]   what I wanna do is see a list of all the arguments
[00:23:12.900 --> 00:23:15.900]   for calling textattack from the command line.
[00:23:15.900 --> 00:23:17.140]   The idea is that like,
[00:23:17.140 --> 00:23:20.060]   it'll tell me the possible recipes I can use,
[00:23:20.060 --> 00:23:22.420]   the attack recipes, the transformations,
[00:23:22.420 --> 00:23:23.820]   goal functions and constraints.
[00:23:23.820 --> 00:23:25.260]   So I'm not gonna go through everything here.
[00:23:25.260 --> 00:23:27.140]   This is pretty much like the same
[00:23:27.140 --> 00:23:29.580]   as the visualization I showed before.
[00:23:29.580 --> 00:23:32.740]   But here's all the transformations.
[00:23:32.740 --> 00:23:35.100]   Down here, we have goal functions.
[00:23:35.100 --> 00:23:37.540]   And also, instead of choosing
[00:23:37.540 --> 00:23:40.340]   the transformation constraint goal function,
[00:23:40.340 --> 00:23:42.940]   I could just apply an attack recipe,
[00:23:42.940 --> 00:23:44.060]   which is what I'm gonna do now.
[00:23:44.060 --> 00:23:46.700]   So Python-M textattack.
[00:23:46.700 --> 00:23:49.580]   The recipe, our name for it is just textfooler,
[00:23:49.580 --> 00:23:52.980]   which is like what they called it in the paper.
[00:23:52.980 --> 00:23:55.940]   And then I'm gonna attack one of the pre-trained models.
[00:23:55.940 --> 00:23:59.580]   So I'm gonna use BERT-MR.
[00:23:59.580 --> 00:24:02.420]   That's BERT fine-tuned for that MR dataset,
[00:24:02.420 --> 00:24:06.340]   which is just a small sentiment classification dataset.
[00:24:06.340 --> 00:24:10.900]   And it's just fast and easy.
[00:24:10.900 --> 00:24:14.220]   And then maybe let's do 20 examples.
[00:24:14.220 --> 00:24:18.580]   This textfooler is really fast, so that should be okay.
[00:24:18.580 --> 00:24:20.460]   What else do we wanna do?
[00:24:20.460 --> 00:24:25.260]   Let's enable CSV so I can see my results in CSV later.
[00:24:25.260 --> 00:24:29.620]   And most importantly, enable weights and biases.
[00:24:29.620 --> 00:24:31.940]   So again, takes a second to load
[00:24:31.940 --> 00:24:34.780]   and also to initialize the attack, as you might imagine.
[00:24:34.780 --> 00:24:37.420]   I mean, BERT itself is a few gigabytes,
[00:24:37.420 --> 00:24:39.660]   so it loads BERT onto the GPU.
[00:24:39.660 --> 00:24:43.020]   One of the constraints is the universal sentence encoder,
[00:24:43.020 --> 00:24:44.180]   which is also very large,
[00:24:44.180 --> 00:24:46.820]   and it has to load that onto the GPU.
[00:24:46.820 --> 00:24:48.340]   It has to get the word embeddings,
[00:24:48.340 --> 00:24:50.900]   which are a few gigabytes.
[00:24:50.900 --> 00:24:53.780]   And luckily, the nearest neighbors are pre-computed,
[00:24:53.780 --> 00:24:56.420]   so it's not like it has to do
[00:24:56.420 --> 00:25:01.780]   some kind of nearest neighbor algorithm,
[00:25:01.780 --> 00:25:04.100]   but it still takes a few seconds to set up.
[00:25:04.100 --> 00:25:09.740]   So I guess we'll wait a second for this to initialize.
[00:25:09.740 --> 00:25:11.980]   The text attack has this cache folder
[00:25:11.980 --> 00:25:13.460]   that it saves everything to.
[00:25:13.460 --> 00:25:15.620]   This output is from TensorFlow Hub.
[00:25:15.620 --> 00:25:19.420]   So they use .cache/tensorflowhub.
[00:25:19.420 --> 00:25:21.300]   Well, here it's calling TensorFlow Hub
[00:25:21.300 --> 00:25:23.820]   because TensorFlow provides us
[00:25:23.820 --> 00:25:25.580]   the universal sentence encoder,
[00:25:25.580 --> 00:25:27.780]   but text attacks own models are saved
[00:25:27.780 --> 00:25:31.420]   to .cache/textattack by default,
[00:25:31.420 --> 00:25:32.860]   and you can override that.
[00:25:32.860 --> 00:25:34.540]   But it automatically downloads everything
[00:25:34.540 --> 00:25:35.780]   on the first time you use it.
[00:25:35.780 --> 00:25:38.540]   So we host our models on Amazon S3,
[00:25:38.540 --> 00:25:42.300]   and we just query them using like a WGET basically.
[00:25:42.300 --> 00:25:43.780]   And there we go.
[00:25:44.980 --> 00:25:46.420]   Wait for it.
[00:25:46.420 --> 00:25:48.020]   And then you have to wait for it to download
[00:25:48.020 --> 00:25:50.340]   on the first time you use it, that's all.
[00:25:50.340 --> 00:25:52.500]   So at the beginning, it prints out the attack.
[00:25:52.500 --> 00:25:54.940]   This is the same syntax as like
[00:25:54.940 --> 00:25:56.980]   if you print out a module in PyTorch.
[00:25:56.980 --> 00:25:59.020]   So again, you can see the four components.
[00:25:59.020 --> 00:26:00.660]   The attack is the search method.
[00:26:00.660 --> 00:26:04.060]   So this is greedy word swap with word importance ranking.
[00:26:04.060 --> 00:26:06.260]   Untargeted classification.
[00:26:06.260 --> 00:26:08.580]   The transformation is counterfeited embeddings.
[00:26:08.580 --> 00:26:10.420]   That's what this means.
[00:26:10.420 --> 00:26:12.340]   There's three constraints here,
[00:26:12.340 --> 00:26:14.420]   and it's a black box attack.
[00:26:14.420 --> 00:26:16.740]   So yeah, this is basically gonna do the same thing
[00:26:16.740 --> 00:26:19.460]   as the pseudocode that's described in that paper.
[00:26:19.460 --> 00:26:21.620]   And this is gonna run 20 times,
[00:26:21.620 --> 00:26:26.620]   and it's gonna log to this Weights and Biases dashboard.
[00:26:26.620 --> 00:26:30.980]   I'm gonna copy that so we can look at it at the end.
[00:26:30.980 --> 00:26:32.380]   Oh, wow, that was fast.
[00:26:32.380 --> 00:26:37.180]   Okay, well, yeah, so this is the attack results.
[00:26:37.180 --> 00:26:40.460]   The attack was originally 80% accurate.
[00:26:40.460 --> 00:26:45.460]   So it predicted, I guess, 16 out of 20 correctly.
[00:26:45.460 --> 00:26:48.100]   And then afterwards, it was only 5% accurate.
[00:26:48.100 --> 00:26:50.100]   So that means the attack must have succeeded
[00:26:50.100 --> 00:26:52.500]   15 out of the 16 times.
[00:26:52.500 --> 00:26:55.940]   Oh, yeah, so 93.75% successful.
[00:26:55.940 --> 00:26:58.980]   And on average, it perturbed 21.12 words.
[00:26:58.980 --> 00:27:01.460]   So let's look at an example here.
[00:27:01.460 --> 00:27:06.100]   MR is lowercase by default, which is annoying to me.
[00:27:08.020 --> 00:27:11.220]   Here, the jokes are flat and the action looks fake.
[00:27:11.220 --> 00:27:15.620]   That's a negative review, though not that negative.
[00:27:15.620 --> 00:27:18.660]   Then the perturbation is the jokes are flat
[00:27:18.660 --> 00:27:20.620]   and the strides look forged.
[00:27:20.620 --> 00:27:22.340]   That's a pretty good one.
[00:27:22.340 --> 00:27:24.820]   You'll see the universal sentence encoder similarity
[00:27:24.820 --> 00:27:26.140]   and the word embedding constraint
[00:27:26.140 --> 00:27:27.820]   are actually relatively lenient.
[00:27:27.820 --> 00:27:29.740]   So if you didn't notice already,
[00:27:29.740 --> 00:27:34.740]   a decent amount of these are pretty lax, I'd say.
[00:27:34.740 --> 00:27:36.140]   I'm gonna go back to Chrome
[00:27:36.140 --> 00:27:39.060]   and we can look at the weights and biases run.
[00:27:39.060 --> 00:27:41.620]   Oh, no, I copied the percentage.
[00:27:41.620 --> 00:27:43.900]   Here we go.
[00:27:43.900 --> 00:27:49.340]   So since I added that flag --enable,
[00:27:49.340 --> 00:27:53.300]   W and B had automatically printed to weights and biases.
[00:27:53.300 --> 00:27:55.420]   So this is the same table we saw before
[00:27:55.420 --> 00:27:57.580]   of the statistics from the attack.
[00:27:57.580 --> 00:28:02.180]   And then here, it's all the results visualized in HTML.
[00:28:02.180 --> 00:28:03.620]   So let's look at one more.
[00:28:03.620 --> 00:28:06.060]   This didn't get changed.
[00:28:06.060 --> 00:28:08.580]   So this means this is like a failed attack result.
[00:28:08.580 --> 00:28:10.940]   So I guess substituting, this is a stop word.
[00:28:10.940 --> 00:28:14.460]   So I probably tried substituting each of these and failed.
[00:28:14.460 --> 00:28:17.180]   Woisterous and daft documentary
[00:28:17.180 --> 00:28:20.300]   changed to rambunctious and daft documentary.
[00:28:20.300 --> 00:28:23.460]   That one's questionably supposed to be negative as well.
[00:28:23.460 --> 00:28:25.220]   But yeah, so you could run the attack
[00:28:25.220 --> 00:28:27.260]   with more than 20 examples
[00:28:27.260 --> 00:28:29.140]   and probably spend all day looking at these,
[00:28:29.140 --> 00:28:31.420]   trying to understand your model's output.
[00:28:31.420 --> 00:28:34.220]   And we also, if you add a dash dash parallel,
[00:28:34.220 --> 00:28:35.820]   which I should have and forgot,
[00:28:35.820 --> 00:28:38.740]   it'll distribute across all the GPUs on your machine.
[00:28:38.740 --> 00:28:41.380]   Whoops, this is the wrong tab.
[00:28:41.380 --> 00:28:44.780]   So you can get a pretty big speed up.
[00:28:44.780 --> 00:28:45.980]   Okay, so that was the demo.
[00:28:45.980 --> 00:28:49.260]   And lastly, I'm gonna talk about data augmentation.
[00:28:49.260 --> 00:28:53.820]   I think this is probably the most important use case
[00:28:53.820 --> 00:28:55.340]   to the general public.
[00:28:55.340 --> 00:28:58.660]   So if you're training an NLP model,
[00:28:58.660 --> 00:29:00.620]   try, like seriously, try this.
[00:29:00.620 --> 00:29:01.780]   Send me an email.
[00:29:01.780 --> 00:29:04.260]   Change a few lines in your training script.
[00:29:04.260 --> 00:29:06.740]   And instead of using your regular training set,
[00:29:06.740 --> 00:29:07.860]   especially if it's small,
[00:29:07.860 --> 00:29:10.460]   so a few thousand or maybe,
[00:29:10.460 --> 00:29:13.620]   probably fewer than 20,000 examples,
[00:29:13.620 --> 00:29:15.580]   it would be most effective.
[00:29:15.580 --> 00:29:18.140]   Augment your training set with a few times the size
[00:29:18.140 --> 00:29:22.220]   using text attack and see how your performance increases.
[00:29:22.220 --> 00:29:26.620]   So here's embedding augmenting in a few lines.
[00:29:26.620 --> 00:29:28.980]   We have a few recipes for augmentation.
[00:29:28.980 --> 00:29:30.260]   This one replaces words
[00:29:30.260 --> 00:29:32.580]   with their counterfeited nearest neighbors.
[00:29:32.580 --> 00:29:34.300]   The sentence is, "What I cannot create,
[00:29:34.300 --> 00:29:36.940]   "I do not understand," a Richard Feynman quote.
[00:29:36.940 --> 00:29:38.940]   And by default, this embedding augmenter
[00:29:38.940 --> 00:29:42.220]   is just gonna return all of the potential augmentations.
[00:29:42.220 --> 00:29:43.980]   So since I didn't add any parameters,
[00:29:43.980 --> 00:29:45.900]   it's just gonna swap out one word
[00:29:45.900 --> 00:29:49.100]   and return all the things that it could possibly do.
[00:29:49.100 --> 00:29:50.940]   So these are pretty good.
[00:29:50.940 --> 00:29:53.420]   "What I cannot create, I do not fathom.
[00:29:53.420 --> 00:29:55.940]   "What I cannot create, I do not realize.
[00:29:55.940 --> 00:29:58.100]   "What I cannot create, I do not understood."
[00:29:58.100 --> 00:29:59.220]   That's not good grammar,
[00:29:59.220 --> 00:30:03.100]   but I didn't add the grammar checking constraints,
[00:30:03.100 --> 00:30:04.740]   so that makes sense.
[00:30:04.740 --> 00:30:07.980]   Anyways, you could append these to your training set
[00:30:07.980 --> 00:30:11.580]   along with the correct ID for the correct output for S
[00:30:11.580 --> 00:30:13.700]   and train on a larger training set
[00:30:13.700 --> 00:30:15.340]   and see what your results are.
[00:30:15.340 --> 00:30:17.780]   So I actually did that earlier today.
[00:30:17.780 --> 00:30:20.620]   The dataset I just tested on is the MR dataset,
[00:30:20.620 --> 00:30:24.700]   which by default has 9,595 training examples,
[00:30:24.700 --> 00:30:27.340]   which is pretty, I'd say it's on the small side
[00:30:27.340 --> 00:30:28.700]   for an NLP task.
[00:30:28.700 --> 00:30:31.660]   And also BERT is surprisingly bad at it
[00:30:31.660 --> 00:30:34.060]   for how easy you would think it is.
[00:30:34.060 --> 00:30:36.740]   It's, I think, oh, well, this shows it.
[00:30:36.740 --> 00:30:40.220]   So originally it's 86.4% accurate,
[00:30:40.220 --> 00:30:41.820]   which is kind of surprising,
[00:30:41.820 --> 00:30:43.060]   just you would think it would be easier.
[00:30:43.060 --> 00:30:46.580]   That's probably due to just the short length
[00:30:46.580 --> 00:30:47.420]   of the sentences.
[00:30:47.420 --> 00:30:48.860]   There's just not enough information,
[00:30:48.860 --> 00:30:50.060]   maybe a little bit of noise.
[00:30:50.060 --> 00:30:52.260]   Some of them are misclassified, I don't know.
[00:30:52.260 --> 00:30:55.860]   But so I instantiated an embedding augmenter
[00:30:55.860 --> 00:30:57.260]   just like this.
[00:30:57.260 --> 00:30:59.820]   And for each training example,
[00:30:59.820 --> 00:31:02.020]   I added eight augmented examples.
[00:31:02.020 --> 00:31:05.120]   So I guess, oh no, sorry.
[00:31:05.120 --> 00:31:10.060]   This is, yeah, yeah.
[00:31:10.060 --> 00:31:13.540]   So this is 14 times, not nine.
[00:31:13.540 --> 00:31:18.140]   Anyways, here's a bar chart produced by Weights and Biases
[00:31:18.140 --> 00:31:22.420]   of the sizes, the relative sizes of the datasets.
[00:31:22.420 --> 00:31:24.580]   And you can see the augmented one performs,
[00:31:24.580 --> 00:31:27.580]   outperforms the vanilla one by about 1%.
[00:31:27.580 --> 00:31:32.220]   I bet if I ran this over more hyperparameters,
[00:31:32.220 --> 00:31:34.300]   tried different amounts of things to change
[00:31:34.300 --> 00:31:36.140]   and tried swapping out different words,
[00:31:36.140 --> 00:31:38.060]   maybe tried the thesaurus instead,
[00:31:38.060 --> 00:31:40.080]   I could push this up a little more.
[00:31:40.080 --> 00:31:42.060]   But it was about, I don't know,
[00:31:42.060 --> 00:31:44.220]   eight lines of code that I wrote,
[00:31:44.220 --> 00:31:48.580]   and it increased my performance by 1%.
[00:31:48.580 --> 00:31:50.180]   So that's a pretty good bargain.
[00:31:50.180 --> 00:31:53.100]   Oh, that was supposed to go before.
[00:31:53.100 --> 00:31:55.260]   And then the last slide is,
[00:31:55.260 --> 00:31:57.120]   this is a screenshot of actually the paper
[00:31:57.120 --> 00:31:58.340]   we wrote about text attacks.
[00:31:58.340 --> 00:32:03.340]   So if you're interested in the more theoretical details
[00:32:03.340 --> 00:32:05.040]   of how we define NLP attacks,
[00:32:05.040 --> 00:32:08.660]   or maybe you wanna see how some other attacks
[00:32:08.660 --> 00:32:11.780]   from the literature can get implemented in text attack,
[00:32:11.780 --> 00:32:14.780]   or some more things about what goes on behind the scenes
[00:32:14.780 --> 00:32:17.380]   in text attack, you should definitely check out the paper.
[00:32:17.380 --> 00:32:20.620]   I was hoping, we released it to archive a few weeks ago.
[00:32:20.620 --> 00:32:22.060]   I was hoping it would be published today,
[00:32:22.060 --> 00:32:23.480]   but there's been a big delay.
[00:32:23.480 --> 00:32:25.220]   Apparently it's coming out in the morning.
[00:32:25.220 --> 00:32:27.000]   So you can't look at it tonight.
[00:32:27.000 --> 00:32:28.460]   But if you go on archive in the morning
[00:32:28.460 --> 00:32:29.460]   and type in text attack,
[00:32:29.460 --> 00:32:31.360]   you should be able to find this paper.
[00:32:31.360 --> 00:32:35.700]   And if you're not interested in the paper, that's okay.
[00:32:35.700 --> 00:32:38.040]   You can install it, just pip install text attack,
[00:32:38.040 --> 00:32:42.260]   or go to this GitHub repository and clone it for Git.
[00:32:42.260 --> 00:32:45.600]   You can always contact me, let me know what you think,
[00:32:45.600 --> 00:32:49.500]   if you have suggestions or find a bug, could happen.
[00:32:49.500 --> 00:32:51.820]   Yeah, so now I'm ready for questions.
[00:32:52.500 --> 00:32:54.940]   - Cool, that was really good, Jack.
[00:32:54.940 --> 00:32:56.340]   Thank you.
[00:32:56.340 --> 00:32:58.340]   So I have a question first.
[00:32:58.340 --> 00:33:01.900]   What models and datasets does text attack provide?
[00:33:01.900 --> 00:33:04.260]   - Good question.
[00:33:04.260 --> 00:33:07.980]   So we're working on adding more tasks,
[00:33:07.980 --> 00:33:10.600]   but I think I should talk about the tasks first.
[00:33:10.600 --> 00:33:14.520]   So what I just showed was sentiment classification.
[00:33:14.520 --> 00:33:17.080]   So we support some sentiment classification models
[00:33:17.080 --> 00:33:18.600]   and other classification tasks.
[00:33:18.600 --> 00:33:21.540]   Like I thought this was in the read me.
[00:33:21.860 --> 00:33:25.300]   Yeah, so AG news topic classification.
[00:33:25.300 --> 00:33:28.700]   Then we have three different entailment datasets,
[00:33:28.700 --> 00:33:31.700]   SNLI, MNLI, matched and unmatched.
[00:33:31.700 --> 00:33:33.900]   And then we have one translation dataset,
[00:33:33.900 --> 00:33:35.900]   which is English to German.
[00:33:35.900 --> 00:33:38.100]   We have pre-trained models for all of these
[00:33:38.100 --> 00:33:40.580]   for a few different model types.
[00:33:40.580 --> 00:33:43.060]   So like for each of these classification types,
[00:33:43.060 --> 00:33:45.820]   you can just, instead of typing BERT-MR,
[00:33:45.820 --> 00:33:48.740]   you could type LSTM-MR and attack an LSTM,
[00:33:48.740 --> 00:33:52.780]   or I'd have to look at the help to make sure,
[00:33:52.780 --> 00:33:56.100]   but there's a convolutional neural network
[00:33:56.100 --> 00:33:59.380]   and LSTM and BERT train for all of these things,
[00:33:59.380 --> 00:34:01.300]   plus a Google T5,
[00:34:01.300 --> 00:34:03.400]   which is actually provided by Hugging Face.
[00:34:03.400 --> 00:34:06.300]   Yeah.
[00:34:06.300 --> 00:34:10.180]   - Sayak also asked, what frameworks do you guys support?
[00:34:10.180 --> 00:34:11.960]   - Yeah, I should have mentioned that.
[00:34:11.960 --> 00:34:13.380]   That's a really good question, Sayak.
[00:34:13.380 --> 00:34:17.420]   So most of the code is PyTorch.
[00:34:17.420 --> 00:34:21.300]   So like when we're doing operations with tensors
[00:34:21.300 --> 00:34:25.500]   or matrix multiplication,
[00:34:25.500 --> 00:34:27.980]   stuff like that happens in PyTorch,
[00:34:27.980 --> 00:34:29.980]   but it generally doesn't matter.
[00:34:29.980 --> 00:34:32.340]   Like TextAttack is just gonna call your model
[00:34:32.340 --> 00:34:33.320]   for prediction score.
[00:34:33.320 --> 00:34:35.900]   So it works on TensorFlow models, PyTorch,
[00:34:35.900 --> 00:34:38.720]   Jacks, whatever, SKLearn.
[00:34:38.720 --> 00:34:42.260]   But there are some things like, for example,
[00:34:42.260 --> 00:34:46.260]   this attack recipe, which is called seek to sick.
[00:34:46.260 --> 00:34:47.180]   Where is it?
[00:34:47.180 --> 00:34:50.820]   They swap words by looking at the gradient
[00:34:50.820 --> 00:34:53.540]   and taking the words that have the maximum gradient
[00:34:53.540 --> 00:34:56.760]   like of the one hot encoding at different steps.
[00:34:56.760 --> 00:34:59.380]   So the gradient stuff is implemented in PyTorch.
[00:34:59.380 --> 00:35:01.740]   So it has to be a PyTorch model,
[00:35:01.740 --> 00:35:03.220]   but yeah, it doesn't matter.
[00:35:03.220 --> 00:35:05.700]   And we also require TensorFlow.
[00:35:05.700 --> 00:35:07.380]   So some things are TensorFlow,
[00:35:07.380 --> 00:35:10.380]   like the universal sentence encoder that I mentioned before.
[00:35:10.380 --> 00:35:15.860]   - Boris asked, would it be easy to use with Hugging Face?
[00:35:15.860 --> 00:35:18.060]   And I feel like you've kind of answered that, but.
[00:35:18.060 --> 00:35:20.200]   - Yeah, it definitely would.
[00:35:20.200 --> 00:35:22.220]   I mean, the thing that I showed,
[00:35:22.220 --> 00:35:26.940]   the graph is models that I trained using Hugging Face.
[00:35:26.940 --> 00:35:31.180]   Like I just used the BERT, what's it called?
[00:35:31.180 --> 00:35:33.180]   Like the BERT for fine tuning class.
[00:35:33.180 --> 00:35:34.820]   And then I just provided my own data set.
[00:35:34.820 --> 00:35:35.800]   I didn't use their script.
[00:35:35.800 --> 00:35:37.020]   I wrote my own training script,
[00:35:37.020 --> 00:35:39.780]   but yeah, you could definitely use it with Hugging Face.
[00:35:39.780 --> 00:35:41.540]   Just increase the data set size.
[00:35:41.540 --> 00:35:43.100]   All of the behind the scenes stuff,
[00:35:43.100 --> 00:35:46.820]   like the pre-trained models and the tokenization and things,
[00:35:46.820 --> 00:35:50.140]   they all use Hugging Face transformers and tokenizers.
[00:35:50.140 --> 00:35:54.780]   - All right, Joseph asks,
[00:35:54.780 --> 00:35:57.480]   does this work well for all languages?
[00:35:57.480 --> 00:35:59.740]   - Good question, Joseph.
[00:35:59.740 --> 00:36:02.460]   Unfortunately, no.
[00:36:02.460 --> 00:36:06.180]   Right now we're just supporting the perturbation stuff
[00:36:06.180 --> 00:36:11.180]   as in English, just because it's hard
[00:36:11.400 --> 00:36:16.300]   to get like a good set of synonyms in another language
[00:36:16.300 --> 00:36:20.840]   and actually like get good results is pretty difficult.
[00:36:20.840 --> 00:36:21.940]   But hopefully in the future,
[00:36:21.940 --> 00:36:25.200]   we're gonna have like embeddings and synonyms at least
[00:36:25.200 --> 00:36:26.300]   for multiple languages.
[00:36:26.300 --> 00:36:29.440]   Right now we just support English as the starting language.
[00:36:29.440 --> 00:36:32.600]   And then like you can run it on translation models
[00:36:32.600 --> 00:36:35.260]   that translate from English to another language.
[00:36:35.260 --> 00:36:39.640]   - Sayak has all the best questions right now.
[00:36:39.640 --> 00:36:43.480]   She also asks, what effect does it have on test labels
[00:36:43.480 --> 00:36:46.920]   associated with the text pieces?
[00:36:46.920 --> 00:36:49.320]   Is there a hyper parameter that you can tweak
[00:36:49.320 --> 00:36:52.800]   that would let users control the amount of augmentation?
[00:36:52.800 --> 00:36:55.080]   - Yeah, yeah, that's a really good question.
[00:36:55.080 --> 00:36:59.520]   So that's something that a lot of papers disagree about,
[00:36:59.520 --> 00:37:01.760]   like where to set those knobs.
[00:37:01.760 --> 00:37:04.120]   Like for example, I said before,
[00:37:04.120 --> 00:37:08.000]   we require a cosine similarity between words of 0.8
[00:37:08.000 --> 00:37:09.680]   for the embedding augmenter.
[00:37:09.680 --> 00:37:12.360]   You can change that and you can change the number of words
[00:37:12.360 --> 00:37:17.040]   to swap or the number of examples you'd like to include
[00:37:17.040 --> 00:37:18.680]   in your augmented data set.
[00:37:18.680 --> 00:37:20.680]   So those are all like hyper parameters
[00:37:20.680 --> 00:37:22.120]   that you can adjust yourself.
[00:37:22.120 --> 00:37:27.120]   - How would you formulate validation metrics
[00:37:27.120 --> 00:37:29.080]   for measuring the performance of the models
[00:37:29.080 --> 00:37:33.520]   that are adversarially robust in this case?
[00:37:33.520 --> 00:37:35.560]   Sayak says he understands the context
[00:37:35.560 --> 00:37:38.280]   of adversarial robustness might change
[00:37:38.280 --> 00:37:39.680]   from domain to domain.
[00:37:39.680 --> 00:37:41.240]   - Yeah, absolutely.
[00:37:41.240 --> 00:37:43.280]   So the first thing you have to do
[00:37:43.280 --> 00:37:44.720]   is define your threat model.
[00:37:44.720 --> 00:37:46.200]   And if you know what kinds of threats
[00:37:46.200 --> 00:37:48.000]   that you're trying to defend against,
[00:37:48.000 --> 00:37:51.680]   you can create an attack that tests for,
[00:37:51.680 --> 00:37:54.480]   that creates those kinds of adversaries.
[00:37:54.480 --> 00:37:57.280]   So basically you settle on the attack
[00:37:57.280 --> 00:37:58.520]   that you wanna defend against,
[00:37:58.520 --> 00:38:00.000]   and then you can measure robustness
[00:38:00.000 --> 00:38:01.880]   by the attack success rate.
[00:38:01.880 --> 00:38:03.360]   So in the example I showed,
[00:38:03.360 --> 00:38:07.880]   the attack was successful 93.75% of the time,
[00:38:07.880 --> 00:38:09.480]   which is like relatively high.
[00:38:09.480 --> 00:38:11.560]   So if you trained a model that was more robust
[00:38:11.560 --> 00:38:14.040]   against this type of synonym substitution,
[00:38:14.040 --> 00:38:18.120]   then hopefully the attack would be less than 93% successful.
[00:38:18.120 --> 00:38:21.960]   But I guess like to answer your question succinctly,
[00:38:21.960 --> 00:38:24.600]   like attack success rate is the main metric.
[00:38:24.600 --> 00:38:29.040]   - I wanted to follow that question up actually.
[00:38:29.040 --> 00:38:33.280]   So first I wanted to say that I really like this work, Jack.
[00:38:33.280 --> 00:38:34.400]   I've done some research
[00:38:34.400 --> 00:38:37.160]   on adversarial perturbations for images,
[00:38:37.160 --> 00:38:40.720]   and it was massively helped by Foolbox,
[00:38:40.720 --> 00:38:43.000]   which is I think primarily targeted images.
[00:38:43.000 --> 00:38:44.640]   I don't know all the features,
[00:38:44.640 --> 00:38:47.880]   but it's great to see that people are doing that in NLP.
[00:38:47.880 --> 00:38:52.520]   But one thing that image folks really benefit from
[00:38:52.520 --> 00:38:54.720]   is that there are objective metrics
[00:38:54.720 --> 00:38:57.920]   like L infinity distance, which is the maximum change,
[00:38:57.920 --> 00:39:01.280]   L2 distance, which is sort of the mean squared error
[00:39:01.280 --> 00:39:02.760]   of the perturbation.
[00:39:02.760 --> 00:39:04.880]   And so this has allowed people to come up with things
[00:39:04.880 --> 00:39:07.320]   like Lipschitz constrained networks
[00:39:07.320 --> 00:39:10.400]   that can be provably adversarially robust.
[00:39:10.400 --> 00:39:12.480]   So I'm curious, like to what extent
[00:39:12.480 --> 00:39:14.600]   do you think that's even possible in NLP?
[00:39:14.600 --> 00:39:16.760]   What have people done to try and get that level
[00:39:16.760 --> 00:39:18.880]   of adversarial protection in NLP
[00:39:18.880 --> 00:39:20.680]   where you wouldn't need to know a threat model
[00:39:20.680 --> 00:39:22.720]   'cause you actually have the right notion
[00:39:22.720 --> 00:39:25.520]   of what it means to attack objectively?
[00:39:25.520 --> 00:39:27.640]   - Yeah, that's a super good question.
[00:39:27.640 --> 00:39:30.800]   So I think like I didn't really talk about this
[00:39:30.800 --> 00:39:32.600]   in the presentation 'cause it was more about
[00:39:32.600 --> 00:39:35.040]   like the framework, I guess, than a research background,
[00:39:35.040 --> 00:39:38.120]   but that's like the main research question
[00:39:38.120 --> 00:39:40.680]   in this line of work and like adversarial examples in NLP
[00:39:40.680 --> 00:39:45.000]   is like what is the equivalent of the L infinity norm
[00:39:45.000 --> 00:39:48.760]   for images, which maybe everyone can agree on for text.
[00:39:48.760 --> 00:39:52.760]   And unfortunately, I think the problem is that
[00:39:52.760 --> 00:39:56.000]   like if you think of maybe you're just concerned
[00:39:56.000 --> 00:39:59.040]   with semantic perturbations,
[00:39:59.040 --> 00:40:01.360]   you would have to come up with some metric
[00:40:01.360 --> 00:40:05.760]   of an exact metric of semantics between two text inputs.
[00:40:05.760 --> 00:40:07.480]   And that question is like,
[00:40:07.480 --> 00:40:09.240]   I guess I would call it like NLP hard.
[00:40:09.240 --> 00:40:12.320]   Like if you could exactly quantify the semantic difference
[00:40:12.320 --> 00:40:16.400]   between two inputs, you could solve like any NLP task.
[00:40:16.400 --> 00:40:20.440]   So I guess like to measure that kind of L infinity norm,
[00:40:20.440 --> 00:40:22.040]   you would need a model or something,
[00:40:22.040 --> 00:40:23.800]   which is why people try to use, for example,
[00:40:23.800 --> 00:40:25.560]   the universal sentence encoder.
[00:40:25.560 --> 00:40:27.480]   And then they get into these disagreements about like,
[00:40:27.480 --> 00:40:31.400]   oh, I think, you know, I think 0.9 is the right distance.
[00:40:31.400 --> 00:40:33.840]   And I think 0.8 is the right distance or whatever.
[00:40:33.840 --> 00:40:36.160]   So there's not really a perfect metric.
[00:40:36.160 --> 00:40:39.200]   And we created text attacks so that hopefully we can
[00:40:39.200 --> 00:40:40.880]   kind of like stay ahead of the curve
[00:40:40.880 --> 00:40:43.800]   with these types of like constraints.
[00:40:43.800 --> 00:40:46.000]   And like whenever someone comes up with maybe like
[00:40:46.000 --> 00:40:48.160]   a better way to quantify it, we'll implement it
[00:40:48.160 --> 00:40:49.840]   and we can test all the previous attacks
[00:40:49.840 --> 00:40:51.040]   within that constraint.
[00:40:51.040 --> 00:40:55.360]   - Great, that's super great answer.
[00:40:55.360 --> 00:40:56.720]   And it also seems like something like
[00:40:56.720 --> 00:41:01.680]   the universal sentence encoder is itself subject
[00:41:01.680 --> 00:41:03.440]   to adversarial attacks, right?
[00:41:03.440 --> 00:41:06.720]   And so because you don't have a simple function
[00:41:06.720 --> 00:41:09.920]   for measuring similarity, you end up like,
[00:41:09.920 --> 00:41:12.280]   yeah, there's a chicken and egg kind of problem.
[00:41:12.280 --> 00:41:13.520]   - Yeah, absolutely.
[00:41:13.520 --> 00:41:16.560]   Like maybe you're not finding a hole in the model.
[00:41:16.560 --> 00:41:19.160]   You're just finding some place where the sentence encoder
[00:41:19.160 --> 00:41:21.720]   thinks they're really similar, even though they're not.
[00:41:26.600 --> 00:41:28.800]   - Also, if you guys haven't met already,
[00:41:28.800 --> 00:41:31.800]   I don't think you have, Charles is a deep learning engineer,
[00:41:31.800 --> 00:41:35.280]   teacher at Weights and Biases,
[00:41:35.280 --> 00:41:37.240]   and Jack is one of our newest authors.
[00:41:37.240 --> 00:41:39.440]   So you guys will run into each other soon.
[00:41:39.440 --> 00:41:41.800]   - Cool. - Right over to Slack.
[00:41:41.800 --> 00:41:43.400]   Cool, all right, thank you, Jack.
[00:41:43.400 --> 00:41:44.560]   That was really cool.
[00:41:44.560 --> 00:41:45.720]   I really enjoyed it.
[00:41:45.720 --> 00:41:47.880]   - Yeah, thanks for the questions, everybody.

