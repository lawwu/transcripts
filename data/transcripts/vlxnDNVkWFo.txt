
[00:00:00.000 --> 00:00:03.880]   [MUSIC PLAYING]
[00:00:03.880 --> 00:00:04.960]   Hi, everyone.
[00:00:04.960 --> 00:00:07.160]   We are Deployed team.
[00:00:07.160 --> 00:00:09.760]   We will tell you about large language modeling
[00:00:09.760 --> 00:00:13.080]   for text-to-image models and about
[00:00:13.080 --> 00:00:16.560]   if deployed, if new text-to-image diffusion
[00:00:16.560 --> 00:00:19.720]   model that will be open source very soon.
[00:00:19.720 --> 00:00:23.200]   And we're happy to say it's a new state-of-the-art model
[00:00:23.200 --> 00:00:24.880]   for text-to-image.
[00:00:24.880 --> 00:00:28.640]   And first of all, I want to tell about our team.
[00:00:28.640 --> 00:00:34.160]   We're working as a research lab as a part of Stability Eye.
[00:00:34.160 --> 00:00:37.880]   Here, Daria, she is a senior researcher.
[00:00:37.880 --> 00:00:44.440]   And Alex is one of the main guys who are behind IF model.
[00:00:44.440 --> 00:00:47.760]   Also me, team lead of Deployed Lab,
[00:00:47.760 --> 00:00:51.120]   and Ksenia, our data scientist.
[00:00:51.120 --> 00:00:54.920]   So first of all, we are talking about large language models.
[00:00:54.920 --> 00:00:56.320]   But they are language models.
[00:00:56.320 --> 00:01:01.160]   So why should we talk about them in context of text-to-image?
[00:01:01.160 --> 00:01:03.080]   So the question is very interesting
[00:01:03.080 --> 00:01:06.840]   because nowadays everyone know about DALI 2
[00:01:06.840 --> 00:01:09.960]   and especially about stable diffusion, the most popular
[00:01:09.960 --> 00:01:12.160]   text-to-image model ever.
[00:01:12.160 --> 00:01:17.000]   One of the greatest things that we have in the open source.
[00:01:17.000 --> 00:01:20.520]   And I don't want to spend time about telling you what is it.
[00:01:20.520 --> 00:01:24.680]   But it's a diffusion model that can convert the text
[00:01:24.680 --> 00:01:29.320]   to the image in several seconds on almost any PC and laptops
[00:01:29.320 --> 00:01:31.160]   and maybe mobile phone.
[00:01:31.160 --> 00:01:32.760]   So it's very popular nowadays.
[00:01:32.760 --> 00:01:37.760]   And one of the things that people maybe not
[00:01:37.760 --> 00:01:41.760]   talk a lot about is that it consists of several parts.
[00:01:41.760 --> 00:01:44.560]   And the language model inside is a huge part
[00:01:44.560 --> 00:01:46.480]   of the stable diffusion model.
[00:01:46.480 --> 00:01:50.000]   So I will tell you briefly about DALI 2.
[00:01:50.000 --> 00:01:53.600]   It's one of the first popular text-to-image model
[00:01:53.600 --> 00:01:56.800]   that blow the people's mind.
[00:01:56.800 --> 00:02:00.040]   And it can generate almost anything, not just avocado
[00:02:00.040 --> 00:02:04.160]   on chair, but maybe some complex scenes, for example,
[00:02:04.160 --> 00:02:07.880]   dog playing the trumpet in a jazz band, et cetera.
[00:02:07.880 --> 00:02:13.000]   So if you see the paper and you look at the images,
[00:02:13.000 --> 00:02:15.880]   you see, OK, this is a huge scheme
[00:02:15.880 --> 00:02:18.160]   that describe how DALI 2 works.
[00:02:18.160 --> 00:02:22.240]   But it starts from the text, a Corby playing a trumpet.
[00:02:22.240 --> 00:02:25.400]   So this text is converted to some embedding space
[00:02:25.400 --> 00:02:26.720]   using text encoder.
[00:02:26.720 --> 00:02:30.120]   And the text encoder is the language model
[00:02:30.120 --> 00:02:32.920]   what we are talking about now.
[00:02:32.920 --> 00:02:35.280]   And this is a frozen language model
[00:02:35.280 --> 00:02:40.560]   that are not trained during the DALI 2 training phase.
[00:02:40.560 --> 00:02:45.040]   I mean that this is text clip tower that
[00:02:45.040 --> 00:02:47.800]   have been trained before trained DALI 2.
[00:02:47.800 --> 00:02:51.120]   And it can convert this text to some embedding space.
[00:02:51.120 --> 00:02:55.040]   And after that, DALI 2 approach use the prior
[00:02:55.040 --> 00:02:58.960]   to move this vector to the image embedding space.
[00:02:58.960 --> 00:03:04.920]   And after that, unclip it to the low-resolution pixel space.
[00:03:04.920 --> 00:03:07.880]   So Stable Diffusion, this network
[00:03:07.880 --> 00:03:12.000]   that have been done by Robin and his team
[00:03:12.000 --> 00:03:16.560]   as a part of Stability AI, is a very popular network nowadays.
[00:03:16.560 --> 00:03:19.960]   And it also has a very complex scheme,
[00:03:19.960 --> 00:03:21.240]   maybe not the best one.
[00:03:21.240 --> 00:03:25.760]   But you can see this is the frozen clip part here too.
[00:03:25.760 --> 00:03:26.600]   What does it mean?
[00:03:26.600 --> 00:03:33.120]   It means that as same as DALI 2 have the frozen text encoder,
[00:03:33.120 --> 00:03:38.040]   Stable Diffusion also have the frozen text encoder.
[00:03:38.040 --> 00:03:41.440]   But it doesn't make this kind of mapping
[00:03:41.440 --> 00:03:45.120]   from language embedding space to image embedding space.
[00:03:45.120 --> 00:03:48.280]   But it still have this frozen text encoder.
[00:03:48.280 --> 00:03:53.280]   And this information comes from text to embedding space.
[00:03:53.280 --> 00:03:57.000]   And after that, this information fuse through cross attention
[00:03:57.000 --> 00:03:59.040]   to the unit.
[00:03:59.040 --> 00:04:01.520]   And there the magic begins.
[00:04:01.520 --> 00:04:04.520]   So the diffusion in the latent space
[00:04:04.520 --> 00:04:07.760]   and then decoding backward to the pixel space.
[00:04:07.760 --> 00:04:13.560]   The interesting part here that this is frozen clip model
[00:04:13.560 --> 00:04:18.960]   without prior that converts or map the text embedding
[00:04:18.960 --> 00:04:20.200]   to image embedding.
[00:04:20.200 --> 00:04:24.280]   So this idea initially comes from ImageN,
[00:04:24.280 --> 00:04:27.040]   the network we will tell you now.
[00:04:27.040 --> 00:04:32.080]   It's obvious that there are some pros in using a clip
[00:04:32.080 --> 00:04:36.320]   as a text encoder because Clip was training
[00:04:36.320 --> 00:04:40.000]   using text image pairs.
[00:04:40.000 --> 00:04:44.960]   And obviously, it contains some visual features
[00:04:44.960 --> 00:04:48.480]   inside its text embeddings.
[00:04:48.480 --> 00:04:53.520]   But there are also some cons with this approach
[00:04:53.520 --> 00:04:59.040]   because actually parallel text image corpora,
[00:04:59.040 --> 00:05:03.040]   they're not so big as, for example,
[00:05:03.040 --> 00:05:09.440]   corpora on which large language models are trained on.
[00:05:09.440 --> 00:05:15.760]   So there is another idea with the model called ImageN.
[00:05:15.760 --> 00:05:21.360]   They take T5XXL language model.
[00:05:21.360 --> 00:05:25.960]   It's a great, huge language model
[00:05:25.960 --> 00:05:31.920]   which was trained on a great amount of data on C4 corpus.
[00:05:31.920 --> 00:05:35.840]   And they use it as a text encoder.
[00:05:35.840 --> 00:05:37.200]   And it's frozen.
[00:05:37.200 --> 00:05:43.400]   So no weights are changing through the training process
[00:05:43.400 --> 00:05:45.320]   of ImageN itself.
[00:05:45.320 --> 00:05:50.400]   Actually, ImageN is our main source of inspiration.
[00:05:50.400 --> 00:05:53.200]   And I guess it's worth mentioning
[00:05:53.200 --> 00:05:59.240]   that ImageN works in pixel space unlike stable diffusion,
[00:05:59.240 --> 00:06:01.120]   which works in latent space.
[00:06:01.120 --> 00:06:07.880]   And so our model says it works in pixel space as well.
[00:06:07.880 --> 00:06:13.320]   T5 is a large language model which
[00:06:13.320 --> 00:06:17.200]   is trained with the Spun Corruption Objective.
[00:06:17.200 --> 00:06:21.480]   Basically, it's an encoder-decoder-transformer
[00:06:21.480 --> 00:06:22.480]   model.
[00:06:22.480 --> 00:06:24.480]   As you can see on the right, it's
[00:06:24.480 --> 00:06:28.520]   written that it was trained on C4 corpus.
[00:06:28.520 --> 00:06:33.280]   And the model itself, like the biggest model,
[00:06:33.280 --> 00:06:37.920]   XXL, has 11 billion parameters.
[00:06:37.920 --> 00:06:43.440]   And actually, this is the very language model
[00:06:43.440 --> 00:06:48.520]   which we use as a text encoder.
[00:06:48.520 --> 00:06:56.760]   Now we are approaching to our model to deploy it if model.
[00:06:56.760 --> 00:07:02.320]   And we just want to start with the results,
[00:07:02.320 --> 00:07:07.840]   actually, with some examples of our trained model, which
[00:07:07.840 --> 00:07:11.000]   can demonstrate that the language
[00:07:11.000 --> 00:07:16.400]   understanding of our model is really pretty good.
[00:07:16.400 --> 00:07:21.600]   As we all know, stable diffusion doesn't generate text well.
[00:07:21.600 --> 00:07:24.200]   It makes many mistakes.
[00:07:24.200 --> 00:07:30.720]   And it's really hard to get the desired image
[00:07:30.720 --> 00:07:35.880]   if you have some sophisticated complex prompt.
[00:07:35.880 --> 00:07:43.640]   But here, you can see that with T5 XXL as a text encoder,
[00:07:43.640 --> 00:07:47.840]   you can really get what you want in many cases,
[00:07:47.840 --> 00:07:51.680]   even if your prompt is pretty much complex.
[00:07:51.680 --> 00:07:58.640]   And here, we also can observe that it has a basic knowledge
[00:07:58.640 --> 00:08:00.720]   of cardinality.
[00:08:00.720 --> 00:08:04.760]   So through the prompt, we can set how many objects
[00:08:04.760 --> 00:08:06.840]   we'd like to generate.
[00:08:06.840 --> 00:08:13.080]   And well, it works fine with some amount of objects,
[00:08:13.080 --> 00:08:16.600]   actually, up to five.
[00:08:16.600 --> 00:08:19.720]   So it really understands that if you
[00:08:19.720 --> 00:08:23.960]   have a prompt of four bottles of wine next to each other,
[00:08:23.960 --> 00:08:27.400]   it can really generate these four bottles.
[00:08:27.400 --> 00:08:33.200]   And moreover, it doesn't duplicate or clone objects.
[00:08:33.200 --> 00:08:37.080]   It makes some variations in the object
[00:08:37.080 --> 00:08:40.320]   so they look more realistic.
[00:08:40.320 --> 00:08:44.480]   The last image, I think, is the most comprehensive
[00:08:44.480 --> 00:08:50.760]   and complicated one, where we can see three objects.
[00:08:50.760 --> 00:08:58.080]   And in prompt, we describe the particular space relationship
[00:08:58.080 --> 00:09:03.720]   between this object as well as their color and their texture.
[00:09:03.720 --> 00:09:06.320]   As I mentioned, we are a deployed team.
[00:09:06.320 --> 00:09:10.600]   And when we began to do this new model,
[00:09:10.600 --> 00:09:13.080]   we thought about that we are living
[00:09:13.080 --> 00:09:16.280]   in the age of not one neural network,
[00:09:16.280 --> 00:09:20.240]   but several neural networks that combine with each other.
[00:09:20.240 --> 00:09:22.520]   So this approach, you know, comes
[00:09:22.520 --> 00:09:25.200]   from a different type of architecture and design.
[00:09:25.200 --> 00:09:28.480]   For example, modular synthesizers
[00:09:28.480 --> 00:09:31.880]   give the opportunity to not buy the final synthesizer
[00:09:31.880 --> 00:09:33.200]   of your dream.
[00:09:33.200 --> 00:09:36.600]   But you can buy different parts and construct
[00:09:36.600 --> 00:09:38.520]   the synthesizer of your--
[00:09:38.520 --> 00:09:40.560]   I don't know, the best synthesizer you want
[00:09:40.560 --> 00:09:43.240]   would fit your needs.
[00:09:43.240 --> 00:09:47.200]   And also, you can construct your home, your ideal room,
[00:09:47.200 --> 00:09:48.800]   or even buildings.
[00:09:48.800 --> 00:09:51.880]   And we thought, OK, we can construct
[00:09:51.880 --> 00:09:56.600]   this kind of architecture like Imagen.
[00:09:56.600 --> 00:10:00.760]   But also, we can separate our parts
[00:10:00.760 --> 00:10:03.320]   and use, for example, first two stages,
[00:10:03.320 --> 00:10:05.760]   but not use the third one, for example,
[00:10:05.760 --> 00:10:07.360]   as a super resolution stage.
[00:10:07.360 --> 00:10:10.840]   I will tell you in the second several slides.
[00:10:10.840 --> 00:10:14.680]   But I think this is idea, the core idea of our lab,
[00:10:14.680 --> 00:10:17.040]   to construct a lot of neural networks
[00:10:17.040 --> 00:10:20.560]   that are not like separate, own things,
[00:10:20.560 --> 00:10:24.040]   but they come with the model approach.
[00:10:24.040 --> 00:10:26.920]   And you can combine them and see what happens next.
[00:10:26.920 --> 00:10:29.400]   And I hope in several weeks, we will
[00:10:29.400 --> 00:10:31.760]   share with you some additional idea that
[00:10:31.760 --> 00:10:34.400]   comes from model approach.
[00:10:34.400 --> 00:10:37.840]   In our case, we use several stages.
[00:10:37.840 --> 00:10:41.080]   First of them is made by Google.
[00:10:41.080 --> 00:10:43.960]   It's D5, frozen language model.
[00:10:43.960 --> 00:10:44.840]   It's a function.
[00:10:44.840 --> 00:10:51.720]   It takes the text prompt and return the embeddings,
[00:10:51.720 --> 00:10:53.520]   depending on how many tokens you use.
[00:10:53.520 --> 00:10:56.480]   In our case, we use 77 tokens.
[00:10:56.480 --> 00:11:01.400]   And we have 4,096 dimension space.
[00:11:01.400 --> 00:11:05.760]   And after that, that textual information came to UNET.
[00:11:05.760 --> 00:11:08.600]   And our first stage model return your image
[00:11:08.600 --> 00:11:11.000]   in 64 by 64 resolution.
[00:11:11.000 --> 00:11:13.320]   In future, we will tell you that in reality, we
[00:11:13.320 --> 00:11:15.680]   can choose any resolution you want.
[00:11:15.680 --> 00:11:19.920]   But this is the default resolution
[00:11:19.920 --> 00:11:21.560]   we use for training.
[00:11:21.560 --> 00:11:23.920]   And after that, we have different super resolution
[00:11:23.920 --> 00:11:24.400]   phases.
[00:11:24.400 --> 00:11:27.680]   And this image, it looks like amplification of audio signal.
[00:11:27.680 --> 00:11:31.920]   But in reality, we just increase the resolution of the image.
[00:11:31.920 --> 00:11:36.440]   And in the several two upscaler cascades,
[00:11:36.440 --> 00:11:44.320]   we came from 64 by 64 to 124 resolution.
[00:11:44.320 --> 00:11:47.600]   In our approach, we can use this model in separate.
[00:11:47.600 --> 00:11:51.520]   But the ideal pipeline for inference
[00:11:51.520 --> 00:11:54.560]   is to use them all together.
[00:11:54.560 --> 00:11:56.680]   So the difference between our model
[00:11:56.680 --> 00:12:01.760]   and its image gen is like a devil in the details.
[00:12:01.760 --> 00:12:05.680]   So in our case, we're using additional type
[00:12:05.680 --> 00:12:09.320]   of low and upper resolution blocks
[00:12:09.320 --> 00:12:12.840]   in the second and third stage model
[00:12:12.840 --> 00:12:15.880]   with a lot of additional cross-attention blocks
[00:12:15.880 --> 00:12:18.480]   to increase the text information that
[00:12:18.480 --> 00:12:22.120]   fuse from text model to the final image.
[00:12:22.120 --> 00:12:24.440]   So if you're interested in the user
[00:12:24.440 --> 00:12:27.360]   information on the low resolution blocks,
[00:12:27.360 --> 00:12:33.360]   every skip connection blocks with cross and self-attention.
[00:12:33.360 --> 00:12:36.320]   And we call this approach optimal UNet
[00:12:36.320 --> 00:12:40.240]   because we thought it's more optimal to use
[00:12:40.240 --> 00:12:44.920]   this kind of cross-attention per block approach.
[00:12:44.920 --> 00:12:46.600]   And it gives more gain.
[00:12:46.600 --> 00:12:50.360]   So about training data, we use open source.
[00:12:50.360 --> 00:12:51.520]   We love open source.
[00:12:51.520 --> 00:12:54.480]   We use line 5b data set.
[00:12:54.480 --> 00:12:57.480]   But it's not only English data set.
[00:12:57.480 --> 00:12:59.480]   It consists of many languages.
[00:12:59.480 --> 00:13:03.040]   And Christopher from Lion made a huge work
[00:13:03.040 --> 00:13:05.920]   to create line 2b data set.
[00:13:05.920 --> 00:13:10.160]   And also his team and himself made a large work.
[00:13:10.160 --> 00:13:12.680]   He and especially Roam--
[00:13:12.680 --> 00:13:15.480]   thank you, Christopher and Roam, for doing that--
[00:13:15.480 --> 00:13:23.200]   did several things to do this data set better for users.
[00:13:23.200 --> 00:13:26.400]   And they make aesthetic scores.
[00:13:26.400 --> 00:13:29.120]   So it's basically clip score that
[00:13:29.120 --> 00:13:35.000]   have been trained based on human evaluation and human markups
[00:13:35.000 --> 00:13:37.960]   that show how beautiful data is.
[00:13:37.960 --> 00:13:41.720]   So for example, you can see that these lowest static score
[00:13:41.720 --> 00:13:44.760]   is very unpleasant images.
[00:13:44.760 --> 00:13:49.040]   But if you increase these aesthetic score,
[00:13:49.040 --> 00:13:53.440]   you will see that maybe more pleasant, more stock-like image
[00:13:53.440 --> 00:13:55.680]   comes to these thresholds.
[00:13:55.680 --> 00:13:59.760]   And in the end of the day, you will come with very beautiful--
[00:13:59.760 --> 00:14:02.840]   or maybe not, depending on your taste--
[00:14:02.840 --> 00:14:04.080]   images.
[00:14:04.080 --> 00:14:10.680]   So we made a decision to threshold aesthetics by 4.5.
[00:14:10.680 --> 00:14:17.240]   And it allow us to use 1.2 billion images.
[00:14:17.240 --> 00:14:21.240]   And after that, we found another strange behavior
[00:14:21.240 --> 00:14:22.640]   on stable diffusion.
[00:14:22.640 --> 00:14:25.600]   And this is one of the example you can generate
[00:14:25.600 --> 00:14:28.400]   by typing value 002.
[00:14:28.400 --> 00:14:30.800]   And this is very strange image that
[00:14:30.800 --> 00:14:34.040]   came almost every time with a lot of artifacts
[00:14:34.040 --> 00:14:35.400]   and a lot of noises.
[00:14:35.560 --> 00:14:41.320]   But the same yellow chair and these paintings of the wall,
[00:14:41.320 --> 00:14:43.480]   we thought why it happened.
[00:14:43.480 --> 00:14:46.440]   And after several hours of research,
[00:14:46.440 --> 00:14:51.840]   they understood what almost 0.1% of the data set
[00:14:51.840 --> 00:14:54.200]   are consists of the one similar images,
[00:14:54.200 --> 00:14:56.400]   because these data set haven't been read,
[00:14:56.400 --> 00:15:00.040]   deduct, or duplicated as need.
[00:15:00.040 --> 00:15:04.360]   We also made a lot of analysis how much data set
[00:15:04.360 --> 00:15:06.080]   we have been deduct.
[00:15:06.080 --> 00:15:10.240]   And I will tell you, this is a lot, a lot of data.
[00:15:10.240 --> 00:15:15.520]   It's almost 25% of data have some duplications.
[00:15:15.520 --> 00:15:19.120]   And also, I don't want to give you
[00:15:19.120 --> 00:15:22.400]   a lot of textual examples.
[00:15:22.400 --> 00:15:28.720]   But as you can see, the most popular textual description
[00:15:28.720 --> 00:15:31.960]   of the image that have been deduct is value 2.
[00:15:31.960 --> 00:15:34.640]   So because of that, stable diffusion
[00:15:34.640 --> 00:15:37.360]   have been overfed on this image, and the behavior
[00:15:37.360 --> 00:15:39.880]   was very, very strange.
[00:15:39.880 --> 00:15:41.200]   OK.
[00:15:41.200 --> 00:15:43.720]   Also, it's very interesting that even on the text,
[00:15:43.720 --> 00:15:48.520]   on the image collision type, we have this type of behavior
[00:15:48.520 --> 00:15:50.080]   known as Z-Flow.
[00:15:50.080 --> 00:15:53.240]   But it's usual, as we know.
[00:15:53.240 --> 00:15:56.080]   So this is when a very popular case that we
[00:15:56.080 --> 00:15:57.840]   have been delete from data set.
[00:15:57.840 --> 00:16:00.760]   And you can see that stable diffusion
[00:16:00.760 --> 00:16:05.160]   can generate it precisely, because it's overfed on it.
[00:16:05.160 --> 00:16:10.880]   But in the case there, you have almost 0.1% of the same images.
[00:16:10.880 --> 00:16:12.640]   I mean, in every batch you train,
[00:16:12.640 --> 00:16:16.720]   you have some images of the same yellow chair.
[00:16:16.720 --> 00:16:22.560]   It allow model to not just remember it by heart,
[00:16:22.560 --> 00:16:26.400]   but also to hallucinate it in a very strange manner
[00:16:26.400 --> 00:16:29.360]   because of this damage to the weights of the model.
[00:16:29.360 --> 00:16:31.160]   So we don't want to.
[00:16:31.160 --> 00:16:33.560]   This stuff, almost more than half images
[00:16:33.560 --> 00:16:36.520]   from stable diffusion have this strange behavior.
[00:16:36.520 --> 00:16:42.320]   I want to tell you about how to train and decide
[00:16:42.320 --> 00:16:44.840]   which thing you should use or not
[00:16:44.840 --> 00:16:49.560]   use during the train, these kind of big image models.
[00:16:49.560 --> 00:16:54.480]   So on the early days of doing IF model,
[00:16:54.480 --> 00:16:57.840]   we made several experiments.
[00:16:57.840 --> 00:17:01.800]   They took from 10 hours to maybe a day and a half.
[00:17:01.800 --> 00:17:05.040]   And after finishing them, we want
[00:17:05.040 --> 00:17:08.520]   to decide which model are better.
[00:17:08.520 --> 00:17:11.560]   But we didn't solve the good results on FID
[00:17:11.560 --> 00:17:15.320]   because there was a big stochasticity in FID score,
[00:17:15.320 --> 00:17:18.000]   clip score, and this training.
[00:17:18.000 --> 00:17:21.360]   But we saw that on the train loads,
[00:17:21.360 --> 00:17:24.560]   you cannot decide which model is better.
[00:17:24.560 --> 00:17:28.560]   You have just noisy, almost plain line,
[00:17:28.560 --> 00:17:30.080]   with no information at all.
[00:17:30.080 --> 00:17:32.960]   It maybe looks a little bit noisier in the end.
[00:17:32.960 --> 00:17:35.360]   So it gives you no information.
[00:17:35.360 --> 00:17:39.320]   So we made a decision to do static steps.
[00:17:39.320 --> 00:17:45.680]   For example, if you have 1,000 discrete diffusion,
[00:17:45.680 --> 00:17:51.160]   so you can hold or decide to evaluate on the specific steps
[00:17:51.160 --> 00:17:56.960]   to avoid the stochasticity in the validation loss.
[00:17:56.960 --> 00:18:00.440]   And we're using Bayesian biases to log this using just
[00:18:00.440 --> 00:18:02.240]   several lines and changes.
[00:18:02.240 --> 00:18:06.360]   We made a good approach that was very representable,
[00:18:06.360 --> 00:18:11.200]   as you can compare with previous stochastic loss
[00:18:11.200 --> 00:18:13.880]   that I have shown you.
[00:18:13.880 --> 00:18:16.120]   You can see here that, for example,
[00:18:16.120 --> 00:18:23.200]   this model, the instance normalization,
[00:18:23.200 --> 00:18:28.240]   it works much better than without it by this loss.
[00:18:28.240 --> 00:18:30.800]   And it's very visible.
[00:18:30.800 --> 00:18:34.360]   And we understood, OK, let's do this kind of approach.
[00:18:34.360 --> 00:18:38.120]   So we have three stages of our model.
[00:18:38.120 --> 00:18:41.400]   And by using Bayesian biases, we saw how--
[00:18:41.400 --> 00:18:44.880]   not stages, so three sizes of first stage.
[00:18:44.880 --> 00:18:49.520]   I mean, first stage is the text to 64 by 64 model.
[00:18:49.520 --> 00:18:53.720]   And you can see that the smallest model is not
[00:18:53.720 --> 00:18:55.560]   as good as the largest two.
[00:18:55.560 --> 00:18:57.600]   So Bayesian biases give us opportunity
[00:18:57.600 --> 00:18:59.880]   to validate and see what happened.
[00:18:59.880 --> 00:19:02.640]   And when we have some spikes in losses,
[00:19:02.640 --> 00:19:06.280]   we can decide, should we stop experiment or not?
[00:19:06.280 --> 00:19:08.840]   This was very, very representative.
[00:19:08.840 --> 00:19:14.120]   But without this idea of holding some discrete steps
[00:19:14.120 --> 00:19:17.840]   and validate them, we were blind.
[00:19:17.840 --> 00:19:21.800]   So this is a good idea maybe for future trainings
[00:19:21.800 --> 00:19:24.160]   to use these kind of steps.
[00:19:24.160 --> 00:19:29.080]   But this is cool to see what loss show us.
[00:19:29.080 --> 00:19:36.520]   But the main idea of evaluation and comparing the model
[00:19:36.520 --> 00:19:40.240]   is to use mathematical metrics such as FID.
[00:19:40.240 --> 00:19:47.640]   And we're actually proud to announce that, at least for now,
[00:19:47.640 --> 00:19:57.960]   our model has the best FID score on zero shot COCO validation
[00:19:57.960 --> 00:20:02.640]   subset of 30,000 of images.
[00:20:02.640 --> 00:20:08.840]   We've actually conducted really many experiments deciding
[00:20:08.840 --> 00:20:11.720]   on the parameters.
[00:20:11.720 --> 00:20:14.800]   Because once you generate image, there
[00:20:14.800 --> 00:20:20.440]   are several parameters you can choose from.
[00:20:20.440 --> 00:20:25.240]   It's classifier-free guidance on the first stage.
[00:20:25.240 --> 00:20:31.040]   It's the level of augmentation and also classifier-free
[00:20:31.040 --> 00:20:33.280]   guidance on the second stage.
[00:20:33.280 --> 00:20:38.960]   And you can imagine that we can't actually
[00:20:38.960 --> 00:20:43.160]   test the ultimate set of all these combinations
[00:20:43.160 --> 00:20:45.920]   of all these parameters.
[00:20:45.920 --> 00:20:49.400]   But through the random search, we've
[00:20:49.400 --> 00:20:55.000]   been trying to find the optimal set of parameters
[00:20:55.000 --> 00:20:59.680]   with which it's better to generate images.
[00:20:59.680 --> 00:21:05.680]   And after that, we've got these results.
[00:21:05.680 --> 00:21:11.960]   So on the left, you can see the table,
[00:21:11.960 --> 00:21:19.560]   which represents the FID scores of many text-to-image models.
[00:21:19.560 --> 00:21:22.560]   You can see, for instance, DALY2,
[00:21:22.560 --> 00:21:24.720]   which we've already mentioned.
[00:21:24.720 --> 00:21:34.680]   And it has 10.39 zero shot FID 30K.
[00:21:34.680 --> 00:21:38.160]   Also, you can see IMAGEN model here.
[00:21:38.160 --> 00:21:40.320]   It's 7.27.
[00:21:40.320 --> 00:21:43.240]   So it's like our parent--
[00:21:43.240 --> 00:21:44.800]   [INAUDIBLE]
[00:21:44.800 --> 00:21:47.440]   --ancestor.
[00:21:47.440 --> 00:21:51.040]   FID score, the lower is the better.
[00:21:51.040 --> 00:21:58.200]   And we can see that our result is 6.66.
[00:21:58.200 --> 00:22:02.240]   And at least for now, it's the best result.
[00:22:02.240 --> 00:22:08.640]   Yes, and on the right side, you can
[00:22:08.640 --> 00:22:14.760]   observe several Pareto curves of FID score and clip score.
[00:22:14.760 --> 00:22:18.760]   FID score, it's the lower it is, the better.
[00:22:18.760 --> 00:22:24.760]   So it compares visual features of real images
[00:22:24.760 --> 00:22:29.760]   from the COCOA data set and the generated ones.
[00:22:29.760 --> 00:22:38.680]   And clip score says how much the text description, the prompt,
[00:22:38.680 --> 00:22:43.720]   and the generated images are close to each other
[00:22:43.720 --> 00:22:45.880]   in semantically.
[00:22:45.880 --> 00:22:48.960]   And the higher is the better.
[00:22:48.960 --> 00:22:53.840]   And so there are these Pareto curves of these two metrics
[00:22:53.840 --> 00:23:01.720]   which allow us to understand how good are our generated images
[00:23:01.720 --> 00:23:08.080]   and how do they fit the prompts, the captions.
[00:23:08.080 --> 00:23:13.680]   And so we can see that we have really good results.
[00:23:13.680 --> 00:23:19.760]   So clip score, here we can observe our biggest
[00:23:19.760 --> 00:23:26.240]   and our best model, if 2.3 billion parameters.
[00:23:26.240 --> 00:23:39.240]   And so it has FID score 6.66 and clip score around 0.306 or 7.
[00:23:39.240 --> 00:23:43.160]   Yes, so it's really a good result.
[00:23:43.160 --> 00:23:47.080]   During our evaluation, we've not only
[00:23:47.080 --> 00:23:50.640]   conducted quantitative evaluation,
[00:23:50.640 --> 00:23:55.160]   but we've also made a qualitative evaluation,
[00:23:55.160 --> 00:24:01.200]   meaning that we took some well-established benchmarks,
[00:24:01.200 --> 00:24:05.360]   for instance, party prompts, draw bench,
[00:24:05.360 --> 00:24:10.960]   and then we generated images based on this prompt.
[00:24:10.960 --> 00:24:16.040]   And then we just evaluated by our own eyes.
[00:24:16.040 --> 00:24:21.640]   And we also actually hope to conduct human evaluation.
[00:24:21.640 --> 00:24:29.040]   But having only our own eyes, we discovered really
[00:24:29.040 --> 00:24:32.400]   interesting properties of our model.
[00:24:32.400 --> 00:24:38.560]   For instance, taking draw bench prompts,
[00:24:38.560 --> 00:24:42.800]   we've discovered that if we translate them
[00:24:42.800 --> 00:24:47.480]   into several languages, in this case,
[00:24:47.480 --> 00:24:53.920]   we mean European languages, for instance, German and French,
[00:24:53.920 --> 00:24:59.600]   and put these prompts to the model, it really--
[00:24:59.600 --> 00:25:01.920]   it understands it.
[00:25:01.920 --> 00:25:05.840]   For instance, here you can see that if we take the prompt
[00:25:05.840 --> 00:25:10.400]   "a black-colored car" and translate it
[00:25:10.400 --> 00:25:15.640]   into German and French, the results of the model
[00:25:15.640 --> 00:25:17.880]   is consistent.
[00:25:17.880 --> 00:25:24.120]   So it really-- generated images fit the captions,
[00:25:24.120 --> 00:25:31.080]   even in other languages, because we used T5 as text encoder.
[00:25:31.080 --> 00:25:34.040]   There were no non-Latin languages
[00:25:34.040 --> 00:25:38.240]   or non-European languages.
[00:25:38.240 --> 00:25:41.240]   Yes, but we definitely would like
[00:25:41.240 --> 00:25:48.520]   to dive into this property more to evaluate it
[00:25:48.520 --> 00:25:56.760]   and to understand to which extent this property hold.
[00:25:56.760 --> 00:25:59.440]   Yes, and here, for instance, it's
[00:25:59.440 --> 00:26:04.840]   interesting to observe with some--
[00:26:04.840 --> 00:26:10.640]   that in French language, the generated images,
[00:26:10.640 --> 00:26:16.200]   they look quite different comparing
[00:26:16.200 --> 00:26:21.840]   with the English, with the one generated with English prompt
[00:26:21.840 --> 00:26:24.280]   or with German prompt.
[00:26:24.280 --> 00:26:28.280]   Yes, because here we see "sec ados."
[00:26:28.280 --> 00:26:34.200]   It's a backpack in French, but in this French word,
[00:26:34.200 --> 00:26:38.840]   the part of this French word is the "sec," is a bag.
[00:26:38.840 --> 00:26:46.760]   And we see that in French, the model understands it not so
[00:26:46.760 --> 00:26:47.400]   well.
[00:26:47.400 --> 00:26:55.560]   It's just pick this word "sec" as a bag and generates "bag,"
[00:26:55.560 --> 00:26:57.360]   not "backpacks."
[00:26:57.360 --> 00:27:02.160]   Another great and very useful ability of our model
[00:27:02.160 --> 00:27:06.960]   is zero-shot image-to-image translation,
[00:27:06.960 --> 00:27:11.400]   which allows to make some alterations,
[00:27:11.400 --> 00:27:16.480]   some modifications of the initial image
[00:27:16.480 --> 00:27:20.120]   to change its style or to add some details.
[00:27:20.120 --> 00:27:25.840]   For instance, here we have an image of some sketch
[00:27:25.840 --> 00:27:30.200]   we'd like to use, and we'd like to get it
[00:27:30.200 --> 00:27:32.520]   in the style of Minecraft.
[00:27:32.520 --> 00:27:35.320]   So what can we do with that?
[00:27:35.320 --> 00:27:39.080]   We can use the forward diffusion process,
[00:27:39.080 --> 00:27:46.440]   whether we can infuse noise in our initial image
[00:27:46.440 --> 00:27:49.960]   in several steps, add, edit.
[00:27:49.960 --> 00:27:54.520]   And then we can stop at some point
[00:27:54.520 --> 00:28:00.280]   so that we see that it's not a complete noise.
[00:28:00.280 --> 00:28:07.000]   Yes, so we can see that some geometry of the image
[00:28:07.000 --> 00:28:08.600]   is preserved.
[00:28:08.600 --> 00:28:16.840]   And from this step, we can put our text prompt, for instance,
[00:28:16.840 --> 00:28:17.960]   Minecraft.
[00:28:17.960 --> 00:28:24.520]   We put it into our text encoder, our T5XXL,
[00:28:24.520 --> 00:28:31.800]   and then put it to the model and thus reverse
[00:28:31.800 --> 00:28:37.280]   the process of diffusion with this text embedding.
[00:28:37.280 --> 00:28:44.040]   And here you see that we, more and more,
[00:28:44.040 --> 00:28:53.320]   we really get the image which fits our text description.
[00:28:53.320 --> 00:28:57.560]   At the same time, the final image
[00:28:57.560 --> 00:29:04.120]   really preserves the geometry of the initial image.
[00:29:04.120 --> 00:29:08.520]   So for instance, we can use not only Minecraft,
[00:29:08.520 --> 00:29:17.280]   or we can put some other styles like zombie or origami or anime
[00:29:17.280 --> 00:29:19.160]   and so on.
[00:29:19.160 --> 00:29:25.480]   So with the knowledge of this large language model of T5XXL
[00:29:25.480 --> 00:29:29.680]   and the capabilities of diffusion models,
[00:29:29.680 --> 00:29:34.400]   we can really get this result.
[00:29:34.400 --> 00:29:40.080]   Also, we can change not only the whole picture,
[00:29:40.080 --> 00:29:45.880]   as we've just observed, but we can change and modify
[00:29:45.880 --> 00:29:48.480]   some parts of the image.
[00:29:48.480 --> 00:29:55.040]   So here, for example, we can have some special zones,
[00:29:55.040 --> 00:30:01.160]   like here are the ears of this beautiful young woman.
[00:30:01.160 --> 00:30:05.800]   And we can change her ears, for instance.
[00:30:05.800 --> 00:30:10.480]   And it can be controlled with text prompts.
[00:30:10.480 --> 00:30:12.680]   So I think that's all.
[00:30:12.680 --> 00:30:17.000]   This is Deployed Research Lab from Stability.
[00:30:17.000 --> 00:30:20.000]   This is Deployed Eve text-to-image model.
[00:30:20.000 --> 00:30:23.960]   I hope that the covers pretty everything from the train
[00:30:23.960 --> 00:30:26.400]   to the inference and the evaluation.
[00:30:26.400 --> 00:30:31.400]   And we will happy to see your feedbacks.
[00:30:31.400 --> 00:30:38.120]   And we really want you to try our model in the several time
[00:30:38.120 --> 00:30:43.840]   when it will be open source and everyone can try it.
[00:30:43.880 --> 00:30:47.240]   [MUSIC PLAYING]
[00:30:47.840 --> 00:30:49.680]   [MUSIC ENDS]
[00:30:50.320 --> 00:30:53.280]   [AUDIO OUT]
[00:30:53.280 --> 00:30:56.340]   [AUDIO OUT]
[00:30:56.340 --> 00:30:59.340]   [AUDIO OUT]
[00:30:59.340 --> 00:31:02.340]   [AUDIO OUT]
[00:31:02.340 --> 00:31:05.340]   [AUDIO OUT]
[00:31:05.340 --> 00:31:08.340]   [AUDIO OUT]
[00:31:08.340 --> 00:31:11.340]   [AUDIO OUT]
[00:31:11.340 --> 00:31:14.340]   [AUDIO OUT]
[00:31:14.340 --> 00:31:17.340]   [AUDIO OUT]
[00:31:17.340 --> 00:31:20.340]   [AUDIO OUT]
[00:31:20.340 --> 00:31:23.340]   [AUDIO OUT]
[00:31:23.340 --> 00:31:26.340]   [AUDIO OUT]
[00:31:26.340 --> 00:31:29.340]   [AUDIO OUT]
[00:31:29.340 --> 00:31:32.340]   [AUDIO OUT]
[00:31:32.340 --> 00:31:35.340]   [AUDIO OUT]
[00:31:35.340 --> 00:31:38.340]   [AUDIO OUT]
[00:31:38.340 --> 00:31:41.340]   [AUDIO OUT]
[00:31:41.340 --> 00:31:51.340]   [BLANK_AUDIO]

