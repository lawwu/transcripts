
[00:00:00.000 --> 00:00:01.000]   All right.
[00:00:01.000 --> 00:00:02.000]   Welcome, everybody.
[00:00:02.000 --> 00:00:07.000]   Welcome to this week's edition of paper reading group at Weights and Biases.
[00:00:07.000 --> 00:00:10.000]   And today we're going to be looking at the Kite paper.
[00:00:10.000 --> 00:00:14.000]   It's basically a transformer that came after the vision transformer.
[00:00:14.000 --> 00:00:21.000]   So there's been some progress around the vision transformer ever since the vision transformer was released in October 2020.
[00:00:21.000 --> 00:00:27.000]   New papers have come out and the ones that are very relevant to our paper reading group today are the Kite,
[00:00:27.000 --> 00:00:31.000]   which is the going deeper with image transformers. And there's another paper.
[00:00:31.000 --> 00:00:39.000]   So hopefully, can everybody see my screen? OK.
[00:00:39.000 --> 00:00:45.000]   Can everybody see my screen? OK. Yes. Perfect.
[00:00:45.000 --> 00:00:52.000]   So then one second, I just need to stop that.
[00:00:52.000 --> 00:00:57.000]   So we're looking at the Kite paper today and the Kite paper.
[00:00:57.000 --> 00:01:02.000]   So the ones that have been extremely relevant that we're going to discuss today,
[00:01:02.000 --> 00:01:08.000]   this is the one paper that we're going to discuss. But then there's also been the training data efficient image transformers,
[00:01:08.000 --> 00:01:14.000]   also referred to as Kite. And you'll see it's by the same author, similar authors from Facebook.
[00:01:14.000 --> 00:01:21.000]   You've got Hugo de Bruin and the others. And they've been making really good progress in terms of vision and transformer.
[00:01:21.000 --> 00:01:24.000]   So that's the one that's the paper we're going to be looking at today.
[00:01:24.000 --> 00:01:29.000]   And something we're going to do a little bit differently is as we go through this paper,
[00:01:29.000 --> 00:01:35.000]   I'm also going to showcase how things are built in code and we're going to refer things from the team library.
[00:01:35.000 --> 00:01:41.000]   So team is by Ross Whitman. It's PyTorch image models.
[00:01:41.000 --> 00:01:48.000]   Sorry, not the docs. One second.
[00:01:48.000 --> 00:01:54.000]   So it's the PyTorch image models and this PyTorch image models has got pretty much a long list of pre-trained models
[00:01:54.000 --> 00:02:00.000]   and everything that you see in terms of state of art that's available in PyTorch image models.
[00:02:00.000 --> 00:02:05.000]   So we're going to be referencing code towards the end or as we keep going through and through towards the paper,
[00:02:05.000 --> 00:02:11.000]   we're going to be referencing code from this repo. And that has already implemented the Kite paper.
[00:02:11.000 --> 00:02:16.000]   So with that being said, let's get started. One last thing, sorry, is that if you go to one,
[00:02:16.000 --> 00:02:23.000]   I think if you have been part of our paper reading groups before, if you go to 1db.me/guide,
[00:02:23.000 --> 00:02:30.000]   that's that will take you to a report. And this is where we're going to be asking questions or interacting with each other as comments.
[00:02:30.000 --> 00:02:35.000]   Yes. If I go, I say write a comment and I just type a comment and post a comment.
[00:02:35.000 --> 00:02:39.000]   The only reason for that is because we're also live on YouTube.
[00:02:39.000 --> 00:02:42.000]   So it just makes sense for me to have everything in one place.
[00:02:42.000 --> 00:02:46.000]   And then this is where we can have all our discussions and pretty much everything.
[00:02:46.000 --> 00:02:51.000]   And it's there for a longer time. So if anybody's looking at this video in six months from now,
[00:02:51.000 --> 00:02:56.000]   then they know where to look at the comments. So that's that.
[00:02:56.000 --> 00:03:01.000]   So let's get started then. What's the what's the main thing about the Kite paper?
[00:03:01.000 --> 00:03:06.000]   So the Kite paper, we've already looked at the vision transformer. So what is the vision transformer?
[00:03:06.000 --> 00:03:11.000]   That's the vision transformer that we have already looked at before.
[00:03:11.000 --> 00:03:17.000]   And the one thing that was found with this vision transformer is if you go deeper and deeper,
[00:03:17.000 --> 00:03:21.000]   if you have more number of layers in the transformer encoder, which is basically this.
[00:03:21.000 --> 00:03:26.000]   So this vision transformer has an input image. You break it into patches.
[00:03:26.000 --> 00:03:31.000]   Then you linearly project those patches, add a class embedding layer over here.
[00:03:31.000 --> 00:03:37.000]   And then you have multiple transformer encoder layers until you finally have an MLP head and then you classify.
[00:03:37.000 --> 00:03:44.000]   So one thing that was noticed as part of the vision transformer is if you go deeper and deeper or you have more number of layers,
[00:03:44.000 --> 00:03:50.000]   then training sort of becomes a little bit unstable. When I say a little bit unstable,
[00:03:50.000 --> 00:03:55.000]   that's that's we will look at what exactly that means as part of the paper today.
[00:03:55.000 --> 00:04:01.000]   But that was one of the main findings is like it's really hard to train deeper vision transformers.
[00:04:01.000 --> 00:04:06.000]   And the same thing kind of was noticed with the Kite paper. So these are the two papers that are very relevant to today.
[00:04:06.000 --> 00:04:12.000]   So when I say Kite, I'm referring to the training data efficient image transformers paper.
[00:04:12.000 --> 00:04:15.000]   And the main difference, I'm just providing context for now.
[00:04:15.000 --> 00:04:21.000]   The main difference in the Kite paper and the vision transformer paper is this distillation token.
[00:04:21.000 --> 00:04:27.000]   So what was different in the Kite paper is they added this distillation token over here.
[00:04:27.000 --> 00:04:51.000]   So let me try and explain things in one note.
[00:04:51.000 --> 00:04:57.000]   So what happened in the Kite paper is they took basically an L teacher. So what does this L teacher do?
[00:04:57.000 --> 00:05:02.000]   So you have basically you have like a very big, say, convolutional neural network.
[00:05:02.000 --> 00:05:11.000]   So you have a massive CNN. Say you have a large efficient net B7 or you have really big CNN that,
[00:05:11.000 --> 00:05:15.000]   you know, has a really good performance on the image net.
[00:05:15.000 --> 00:05:20.000]   So what you do is you take this efficient at B7. You have your image net data set.
[00:05:20.000 --> 00:05:29.000]   So this I'm just going to call it IM. But this is my image net data set of 1.1 million or say around that number.
[00:05:29.000 --> 00:05:34.000]   I think it's 1.4 million. But anyway, around the 1 million mark, that's how many images you have.
[00:05:34.000 --> 00:05:38.000]   So what you have is for these 1.4 million, you also have the 1.4 million labels.
[00:05:38.000 --> 00:05:47.000]   So if I go have a look at the labels, that's my 1.4 million labels for each of the image in image net.
[00:05:47.000 --> 00:05:56.000]   And then what the what this paper did in in diet is like they took this CNN and they made predictions on this 1.4 million images.
[00:05:56.000 --> 00:06:01.000]   So you have something which is again 1.4 million. But this time, these are the predictions.
[00:06:01.000 --> 00:06:07.000]   I'm just going to call it Pred. But these are the predictions from the CNN. So this becomes your teacher.
[00:06:07.000 --> 00:06:11.000]   So I'm just providing context in what exactly was going on in the diet paper.
[00:06:11.000 --> 00:06:22.000]   So what happens is, as we know from wit, you have your patch tokens over here and then you add your class token, which which is what's predicting on the label.
[00:06:22.000 --> 00:06:29.000]   So you have towards the end, you have like a cross entropy loss between your class token or the predictions from the class token.
[00:06:29.000 --> 00:06:37.000]   And then you you're trying to compare it with the labels. But what was different in the diet paper is you also have this another cross entropy loss.
[00:06:37.000 --> 00:06:42.000]   But this time you're trying to minimize the difference between the predictions from this distillation token.
[00:06:42.000 --> 00:06:48.000]   So they added two tokens instead of one. So in Vision Transformer, you only have the class token.
[00:06:48.000 --> 00:06:57.000]   But in diet, they also added this distillation token, which is the teacher trying to teach the student model, in which case the student is the wit.
[00:06:57.000 --> 00:07:03.000]   And then what they do is they're trying to minimize the difference in the loss in this L teacher.
[00:07:03.000 --> 00:07:12.000]   And they're trying to minimize the difference on both of these. So you're trying to minimize the labels and you're trying to minimize the loss on the predictions from the teacher model.
[00:07:12.000 --> 00:07:25.000]   So that was what was different in the diet paper. So in basically when we have a look at wit, which is here, you see how we only have the one token, which is the class token.
[00:07:25.000 --> 00:07:31.000]   So we only have the class token. But in diet, you have two tokens, which is this class token here and then this distillation token.
[00:07:31.000 --> 00:07:38.000]   So that's the main difference between the two. If there's any questions so far, I'll just go to that paper reading group.
[00:07:38.000 --> 00:07:46.000]   So there's no questions so far. Excellent. So that's the background in terms of wit and diet.
[00:07:46.000 --> 00:07:52.000]   So then what's different in going deeper with Image Transformers, which is the kite paper today?
[00:07:52.000 --> 00:08:03.000]   So the one thing they noticed, the one thing they're saying is that we're doing is we build and optimize deeper transformer networks for image classification.
[00:08:03.000 --> 00:08:17.000]   So what they mean by deeper, they typically train transformers for 24 or 36 layers, which is much deeper than what it used to be for wit and what it used to be for the diet paper.
[00:08:17.000 --> 00:08:27.000]   And the next thing they say, which is really, really key, is they obtained 86.5 percent top one accuracy on ImageNet, which is the state of the art at the time.
[00:08:27.000 --> 00:08:34.000]   And then what they say is we achieve 86.5 percent top one accuracy. And the main thing is with no external data.
[00:08:34.000 --> 00:08:41.000]   So that's really key because generally, wits have a tendency to require a lot of data.
[00:08:41.000 --> 00:08:52.000]   So that's what's different in this case. So that being said, what they're trying to say now is just in this part,
[00:08:52.000 --> 00:08:58.000]   what they're trying to say is they're trying to relate the vision transformer to a typical ResNet.
[00:08:58.000 --> 00:09:07.000]   So what we have, say in a ResNet, what you typically have is you have your input here.
[00:09:07.000 --> 00:09:13.000]   So let's say that's my input X. So that input goes all the way.
[00:09:13.000 --> 00:09:23.000]   And then that's your that's your shortcut or. And then you have your basically this is the residual branch.
[00:09:23.000 --> 00:09:29.000]   This is where all the convolution sits. And then you add the two together. So that that was ResNet.
[00:09:29.000 --> 00:09:37.000]   So in what they kind of say, just in this part of introduction, is that's what a ResNet looks like, where this shortcut branch is called G of L.
[00:09:37.000 --> 00:09:40.000]   And then this residual branch is called F of L.
[00:09:40.000 --> 00:09:48.000]   So the main difference is in how different networks have this R of L implemented or like this, the CNN part of the network.
[00:09:48.000 --> 00:09:55.000]   But then you have these repeated layers over and over again, which is what makes the ResNet work.
[00:09:55.000 --> 00:10:01.000]   But what they say is this, what they say is that, hey, coming here, basically,
[00:10:01.000 --> 00:10:10.000]   there was another paper that said that the ResNets are so easy to train and they are one of the best in the world because not because of that,
[00:10:10.000 --> 00:10:16.000]   they offer any better representational power. That is not because they learn anything more than the other networks.
[00:10:16.000 --> 00:10:26.000]   But that's because they're easier to train. So the authors of this paper, they took sort of inspiration from what went on with ResNets.
[00:10:26.000 --> 00:10:31.000]   And they said, OK, what if we make the vision transformer easier to train?
[00:10:31.000 --> 00:10:38.000]   As you can see, they basically in equation two, then they're just trying to compare the vision transformer with ResNet.
[00:10:38.000 --> 00:11:01.000]   So how do they do that? Let me show you. So see how what they have is you have your inputs,
[00:11:01.000 --> 00:11:08.000]   which is your embedding patches here that goes through a normalization layer, then it goes to self-attention and you add them two together.
[00:11:08.000 --> 00:11:13.000]   So this could be this branch on the right is the shortcut and this branch on the left,
[00:11:13.000 --> 00:11:19.000]   which is the norm and the multi head attention that could be compared to basically your residual branch.
[00:11:19.000 --> 00:11:24.000]   So this could be F of L and this could be R of L. And then similar thing happens again.
[00:11:24.000 --> 00:11:28.000]   You have your shortcut and then you have your residual branch in the center.
[00:11:28.000 --> 00:11:36.000]   So this all of this transformer encoder can be represented by two equations, which is this one.
[00:11:36.000 --> 00:11:40.000]   So you have your input. The first thing you do is you normalize your input,
[00:11:40.000 --> 00:11:43.000]   then you pass it through self-attention layer and then you add the two together.
[00:11:43.000 --> 00:11:47.000]   So if you have a look here, you have your input. So let's call it X.
[00:11:47.000 --> 00:11:53.000]   Then you then you normalize it. So you have your eta X or you normalize it basically.
[00:11:53.000 --> 00:12:02.000]   And then you add you add the multi head self-attention to your output becomes self-attention of your normalized input.
[00:12:02.000 --> 00:12:05.000]   So you apply that. And then finally, you add the two together.
[00:12:05.000 --> 00:12:11.000]   So your first thing then becomes X plus self-attention normalized input.
[00:12:11.000 --> 00:12:16.000]   So as you can see, then that becomes this first this becomes this first equation.
[00:12:16.000 --> 00:12:22.000]   And then if you follow the same thing for the next part, which is normalizing and adding things to a feed forward new network,
[00:12:22.000 --> 00:12:30.000]   then you get the second equation. So that's how they're trying to compare this to a typical resident or basically that architecture.
[00:12:30.000 --> 00:12:40.000]   And then the next thing that they say is. What the first thing we want to see, as you can see in Vision Transformer.
[00:12:40.000 --> 00:12:48.000]   The normalization is happening before you have your normalization first and then you have the self-attention.
[00:12:48.000 --> 00:12:52.000]   So you have your normalization first and then you have things going to the MLP.
[00:12:52.000 --> 00:12:57.000]   And what they're saying in this part of the paper is we want to do things differently.
[00:12:57.000 --> 00:13:02.000]   Like we revisit this topic for transformer architectures and what this topic refers to.
[00:13:02.000 --> 00:13:09.000]   This topic is the topic that's about normalization. So they say we look at different normalization techniques.
[00:13:09.000 --> 00:13:13.000]   We look at fix up, we look at T fix up, we look at B zero and skip in it.
[00:13:13.000 --> 00:13:19.000]   And what was different in these architectures? These are again their own separate papers.
[00:13:19.000 --> 00:13:23.000]   But what was different in these architectures? We'll have a look now.
[00:13:23.000 --> 00:13:31.000]   And then they say. So this is just introduction. I'm just trying to so far give you an introduction on how they went about releasing the kite architecture.
[00:13:31.000 --> 00:13:38.000]   So that's the first thing that we're going to look at. So that's the very first change that they make to the transformer architecture.
[00:13:38.000 --> 00:13:43.000]   Is this say we're going to normalize our inputs differently. That's the first thing.
[00:13:43.000 --> 00:13:49.000]   And then they call that approach of normalizing the inputs differently as layer scale.
[00:13:49.000 --> 00:13:57.000]   So that's the first thing. So they introduced two things. One is layer scale. And the second thing is class attention layer, which is here.
[00:13:57.000 --> 00:14:09.000]   So the second thing that they input is this class. So the two main contributions in terms of architectural changes are this layer scale and class attention layers.
[00:14:09.000 --> 00:14:14.000]   So what's the difference in class attention layer? So let's see.
[00:14:14.000 --> 00:14:24.000]   So what you have, you have this is my transformer encoder. This is one layer on the right is one layer of the transformer encoder.
[00:14:24.000 --> 00:14:30.000]   And this transformer encoder has multiple of these layers, maybe like 6, 12, 24, so on.
[00:14:30.000 --> 00:14:39.000]   That's what these these layers are repeated six times, 12 times and so on to give you the transformer encoder or basically the transformer architecture.
[00:14:39.000 --> 00:14:51.000]   And what's happening is in in this paper, what they say is you have like your first you have your input, you convert them to patches, then everything becomes like a vector.
[00:14:51.000 --> 00:14:56.000]   So this patch becomes this vector, this patch becomes this vector, this patch becomes the third vector and so on.
[00:14:56.000 --> 00:15:01.000]   So you have a total of nine vectors and then you add the class token.
[00:15:01.000 --> 00:15:06.000]   So this is this idea, I think it comes from natural language processing from BERT.
[00:15:06.000 --> 00:15:17.000]   And you add the class token over here. And when you're trying to do the classification in the end is you take this class token.
[00:15:17.000 --> 00:15:24.000]   So you you kind of not looking at any of these vectors or what's happening on on the patches.
[00:15:24.000 --> 00:15:28.000]   That's something that's going on in the transformer encoder on the side.
[00:15:28.000 --> 00:15:36.000]   But what transformer encoder is then trying to do is it's trying to interact between each of these patches.
[00:15:36.000 --> 00:15:42.000]   So the transformer encoder is trying to find interaction between all of these patches.
[00:15:42.000 --> 00:15:48.000]   Right. And it's trying to first interact. I'm just going to call it.
[00:15:48.000 --> 00:16:01.000]   Sorry about that. One second. So I'm just going to call it INT, which is it's trying to find interactions because, you know, when you're trying to classify, say, this image, you need to know that, OK, this is the building here.
[00:16:01.000 --> 00:16:05.000]   There's a tree over here. There's like greenery over here. There's like a peak.
[00:16:05.000 --> 00:16:12.000]   So this this is what the transformer needs to know, like the various positions of these things are called as patches or tokens.
[00:16:12.000 --> 00:16:19.000]   So what it's trying to do is it's trying to look at, OK, it's just trying to find the information in the various parts of the image.
[00:16:19.000 --> 00:16:27.000]   And it's trying to interact or basically trying to have like a combination of the various positions in the image.
[00:16:27.000 --> 00:16:35.000]   And then what it's trying to do is it's trying to have a look at everything that's in the image and it's trying to summarize that information in this class token.
[00:16:35.000 --> 00:16:44.000]   So let me draw it with a different color. So it's trying to take everything that's in the image and it's trying to summarize that to the class token.
[00:16:44.000 --> 00:16:55.000]   So when you're trying to do the classification and you pass the class token to the MLP head, then the class token contains a summary of the information that's in the image.
[00:16:55.000 --> 00:17:05.000]   This is very important. And what the kite authors are saying is that if you have a class token right from the start,
[00:17:05.000 --> 00:17:10.000]   then the transformer is kind of doing two things at once.
[00:17:10.000 --> 00:17:16.000]   It's trying to first interact between all the various different parts of the image.
[00:17:16.000 --> 00:17:23.000]   And it's also trying to summarize what you've learned or what the transformer can see in the image in this class token.
[00:17:23.000 --> 00:17:40.000]   So that's the main thing that the kite papers authors are saying is you have a look at over here.
[00:17:40.000 --> 00:17:45.000]   OK, I think it's introduced in this detail in the next section. But that's the main idea.
[00:17:45.000 --> 00:17:49.000]   It's not I can't see if it's written over here in this part of the paper.
[00:17:49.000 --> 00:17:59.000]   But that's definitely referred to later as we go down. But that's the main thing that they say is the kite is the transformer is then trying to do two objectives at once.
[00:17:59.000 --> 00:18:03.000]   And what we want to do is we want to separate the objectives.
[00:18:03.000 --> 00:18:18.000]   So that's the main introduction to the paper in terms of what a layer scale does. A layer scale just is trying to change the way things are normalized and whether it's normalized before,
[00:18:18.000 --> 00:18:25.000]   whether the input is first normalized and then or it's normalized after. There's like different various ways of doing that.
[00:18:25.000 --> 00:18:38.000]   And then the second thing that they do is they add this class attention. So is there any questions so far?
[00:18:38.000 --> 00:18:45.000]   OK, doesn't look like. Excellent. So let's keep going forward.
[00:18:45.000 --> 00:18:59.000]   So that's that. So then you want to still be able to like we still haven't found a way of how we want to train these deeper image transformers using layer scale.
[00:18:59.000 --> 00:19:15.000]   So what the next thing that they say is, if you have a look at this figure one, let me copy paste this figure one. One second.
[00:19:15.000 --> 00:19:30.000]   Let me take that and put it somewhere here. Not there.
[00:19:30.000 --> 00:19:46.000]   So what they're trying to say is. If you compare this A to the vision transformer, I'm to have a look at the vision transformer to see how you have your inputs, then it's normalized, then self attention.
[00:19:46.000 --> 00:19:54.000]   But basically, the main thing is your normalization is happening before things are passed.
[00:19:54.000 --> 00:20:06.000]   So this is your typical vid or diet is where the normalization is happening before things are passed to either the feed forward network or the self attention module.
[00:20:06.000 --> 00:20:13.000]   And what they say is then they take reference from other papers that have tried to make things or do things differently.
[00:20:13.000 --> 00:20:18.000]   So these are papers like fix up this papers like re zero, skipping it.
[00:20:18.000 --> 00:20:24.000]   And what these papers are trying to do is they've tried to change the way a convolution neural network works.
[00:20:24.000 --> 00:20:33.000]   So they've tried to take away basically batch normalization and they've tried to introduce different ways of doing batch norm or basically different ways of doing normalization.
[00:20:33.000 --> 00:20:37.000]   So I won't go into the details of skipping it or re zero or fix up.
[00:20:37.000 --> 00:20:48.000]   But the main idea is that they take away this normalization layer, and then you have your input say X, it goes through your self attention feed forward neural network.
[00:20:48.000 --> 00:20:57.000]   And then you have some parameter alpha. And this parameter alpha is learned, it could be initialized at zero but it's learned as you train the model.
[00:20:57.000 --> 00:21:11.000]   And so what this is doing is it's making the model learn the weight, or it's making the model learn how much importance it needs to give to this feed forward neural network or this self attention.
[00:21:11.000 --> 00:21:14.000]   So remember what happening is you have your input.
[00:21:14.000 --> 00:21:24.000]   It goes straight, or it goes through this feed forward neural network or the self attention, and then they both get added so if this is my say one which is my shortcut.
[00:21:24.000 --> 00:21:30.000]   And then this is my two, when you have here your one and two get added together.
[00:21:30.000 --> 00:21:36.000]   And what this parameter alpha is doing is it's giving the weight to this two.
[00:21:36.000 --> 00:21:46.000]   So it's trying to say is like this alpha, when the model is training it learns using two it could say okay, this weight is point one or it could say okay this weight is point two or all of these different weights.
[00:21:46.000 --> 00:21:52.000]   That's the main difference in skipping it, and this early normalization.
[00:21:52.000 --> 00:22:10.000]   So the next thing that they try as part of the experimentation is they add this alpha, and they noticed, as you'll see in the paper, what they noticed was this part or the B part of normalizing or doing things differently, or introducing this parameter
[00:22:10.000 --> 00:22:21.000]   alpha, it performed worse than this a part. So what they said is okay we'll bring back a, which is your layer normalization, and we'll also have alpha so that's your C.
[00:22:21.000 --> 00:22:40.000]   But that's all the experiments that they did and this came up with layer scale. So this is layer scale. So imagine this, you have your input, basically in transformer, your input could be 196 patches of say 768 channels, where each patch is
[00:22:40.000 --> 00:22:57.000]   so you have your basically your image, which is say divided into nine patches, then each patch is a 768 long vector representation, right? That's what happens in a width when you pass things to the input.
[00:22:57.000 --> 00:23:11.000]   And this that I'm referring to is right here, like this is right now, these 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, these are the various patches. I mean, each batch is say 768 long vector.
[00:23:11.000 --> 00:23:30.000]   And what they're saying is, instead of, because when you have your output, you have something like alpha that's being multiplied to this output, which is again the same dimension. What they're saying is instead of alpha being one number, let each channel get its own weight.
[00:23:30.000 --> 00:23:43.000]   So this channel has its own weight, this channel has its own weight, because these are all 768 channels. This channel has its own weight or for that matter, this last channel has its own weight and so on.
[00:23:43.000 --> 00:23:56.000]   So what they're saying is, okay, let all of them have different weights. And this then gets combined to my input. And this is called layer scale.
[00:23:56.000 --> 00:24:08.000]   This is the idea of layer scale. So let's go back to the paper.
[00:24:08.000 --> 00:24:15.000]   So going back to the paper.
[00:24:15.000 --> 00:24:28.000]   So what do we see here? I'm just trying to show you how they said. First, in figure one, depicts the main variance that we compare for helping the optimization. That's there.
[00:24:28.000 --> 00:24:32.000]   Sorry, one second.
[00:24:32.000 --> 00:24:43.000]   Figure one depicts the main variance, and then they say, okay, the first thing that we try, which is, they cover recent choices from the literature.
[00:24:43.000 --> 00:24:51.000]   The choices are fix up, re0, skip init. And what they do is they introduce this parameter alpha n instead of the normalization.
[00:24:51.000 --> 00:25:00.000]   The main difference is like re0 initializes things to zero, initializes this parameter to zero, fix up initializes this one.
[00:25:00.000 --> 00:25:10.000]   And then they say, our empirical observation is that removing the warm up and the layer normalization, which is the difference between this part B and C.
[00:25:10.000 --> 00:25:16.000]   So see how this thing does not have, the B does not have this normalization.
[00:25:16.000 --> 00:25:22.000]   So what they're saying here is, removing that layer normalization is what makes training unstable.
[00:25:22.000 --> 00:25:27.000]   Therefore, we reintroduce these two ingredients so that fix up and T fix up converge.
[00:25:27.000 --> 00:25:32.000]   So that's the difference in 1A is just like normal normalization.
[00:25:32.000 --> 00:25:36.000]   Then you have this alpha. B performs worse. So in C you have both.
[00:25:36.000 --> 00:25:40.000]   And then they introduce layer scale. So this is what their proposal is.
[00:25:40.000 --> 00:25:47.000]   Our proposal, layer scale is a per channel multiplication of the vector produced by each residual block.
[00:25:47.000 --> 00:25:55.000]   So as I said, in layer scale, each channel gets its own weight.
[00:25:55.000 --> 00:25:59.000]   That's the main thing that happens in layer scale.
[00:25:59.000 --> 00:26:06.000]   So let me see. That's the layer scale part of things. After this, we're just left with understanding class attention.
[00:26:06.000 --> 00:26:10.000]   But let's see if there's any questions so far.
[00:26:10.000 --> 00:26:15.000]   Can you once again explain class attention also wasn't it existent in wit?
[00:26:15.000 --> 00:26:22.000]   I haven't explained class attention in detail yet. So don't worry about it. It will happen in very soon.
[00:26:22.000 --> 00:26:31.000]   Wasn't it existent in wit? OK, so there's the difference. Again, I will I will touch upon what was in wit and what is the difference in guide.
[00:26:31.000 --> 00:26:37.000]   Next question. If I have two images of two classes of cat and dog, both have sky patches.
[00:26:37.000 --> 00:26:41.000]   As I understand, we had the class token for the sky patch in both cat and dog image.
[00:26:41.000 --> 00:26:46.000]   Will that lead to confusion, especially if dog or cat is not dominant?
[00:26:46.000 --> 00:26:51.000]   I think you've got a misunderstanding of what gets added to the image.
[00:26:51.000 --> 00:26:57.000]   So actually, without touching into this, because this is, again, part of the Vision Transformer,
[00:26:57.000 --> 00:27:05.000]   I would refer you to a previous blog of mine, which is this wit. So have a look at this and see what this class token is.
[00:27:05.000 --> 00:27:13.000]   So instead of adding like things for a cat or dog or like separate patches for various parts of the image,
[00:27:13.000 --> 00:27:20.000]   I'm just posting this in the chat. So have a look at this blog and you will see the class embedding that you add is
[00:27:20.000 --> 00:27:26.000]   for the whole image. So it's irrespective of the sky, like whatever is there in the image,
[00:27:26.000 --> 00:27:30.000]   the two images are still different. One image is a cat, the second image is dog.
[00:27:30.000 --> 00:27:35.000]   So the class token is still going to produce or going to have a different representation of the image.
[00:27:35.000 --> 00:27:43.000]   So the token is a representation of the image, not the sky or like the parts of the image.
[00:27:43.000 --> 00:27:52.000]   Is this hyperparameter search for alpha and did they went from problem to solution similar to how authors went in discovering the present paper?
[00:27:52.000 --> 00:27:58.000]   What is the problem of wit that they saw and created this new architecture? I'm not getting the problem solution.
[00:27:58.000 --> 00:28:04.000]   OK, the problem is you can't train very deep Vision Transformers.
[00:28:04.000 --> 00:28:20.000]   So when they say very deep, remember this again. I think I just need a fresh one.
[00:28:20.000 --> 00:28:32.000]   So what happens is this is the transformer encoder layer. OK, and this layer, the same layer is repeated.
[00:28:32.000 --> 00:28:39.000]   Multiple times. One sorry, one second.
[00:28:39.000 --> 00:28:48.000]   I'm just trying to show you on what's going on. So this transformer encoder layer.
[00:28:48.000 --> 00:28:56.000]   Why is it not letting me change the position of that? Sorry, OneNote is trying to play up with me.
[00:28:56.000 --> 00:29:07.000]   Anyway, let me do this again. OK, OneNote is really playing up.
[00:29:07.000 --> 00:29:18.000]   Ignore me for now. I'll just draw it with hand. What do you have in your wit is you have the transformer encoder layer one.
[00:29:18.000 --> 00:29:26.000]   Then you have the transformer encoder layer two. So on transformer encoder layer three and so on as many as you want.
[00:29:26.000 --> 00:29:32.000]   And then that whole thing becomes your this whole thing becomes your transformer encoder.
[00:29:32.000 --> 00:29:37.000]   The problem is you can't have many of these transformer encoder layers.
[00:29:37.000 --> 00:29:46.000]   Like see how ResNet can be like ResNet 34, 101, 150, 101, 152, ResNet 152 and so on.
[00:29:46.000 --> 00:29:51.000]   The problem is that you can't have very deep architectures.
[00:29:51.000 --> 00:29:56.000]   You can't have a very deep width. So what you do then, that's the problem.
[00:29:56.000 --> 00:30:05.000]   Like it can't be very deep. So they're trying to find solution is like we want to find a solution to be able to train very deep widths.
[00:30:05.000 --> 00:30:13.000]   And that's what the Chiat paper is trying to do. And in terms of like finding solution, the first thing that they're checking at is this normalization or like this.
[00:30:13.000 --> 00:30:17.000]   They're revisiting how things happen in width in terms of normalization.
[00:30:17.000 --> 00:30:26.000]   So they introduce this layer scale, which is the first solution towards being able to train very deep architectures.
[00:30:26.000 --> 00:30:35.000]   Does that help Ramesh? Could you maybe post or reply to this comment and just let me know in case that helps?
[00:30:35.000 --> 00:30:40.000]   Perfect. Yes. Excellent. Excellent. Excellent. Thanks.
[00:30:40.000 --> 00:30:49.000]   Okay. So that's the main idea. And then that's the layer scale. In layer scale, then repeating what happened in layer scale is I think I've lost everything.
[00:30:49.000 --> 00:30:55.000]   Okay. Here it is. In layer scale, then the one thing that happened is each channel gets its own weight.
[00:30:55.000 --> 00:31:04.000]   So, for example, in easier terms, then maybe the weight for this is 0.01.
[00:31:04.000 --> 00:31:09.000]   The weight for this channel is 0.9. Or that's too high. Actually, maybe it's 0.3, 0.2 and so on.
[00:31:09.000 --> 00:31:18.000]   So each channel now, because your patch is 768 length vector, each channel is its own weight.
[00:31:18.000 --> 00:31:21.000]   That's the difference in kite and width.
[00:31:21.000 --> 00:31:28.000]   So let's see how one thing I did promise this time is that I'm going to also show you how things look like in code.
[00:31:28.000 --> 00:31:35.000]   So let's have a look at layer scale. This is here, a layer scale block.
[00:31:35.000 --> 00:31:40.000]   As you can see, you have your gamma one and your gamma two. It's just been called gamma.
[00:31:40.000 --> 00:31:47.000]   But basically, it's your initialization values times torch ones times dim.
[00:31:47.000 --> 00:31:53.000]   So if I go and maybe set, I've already got a set trace here.
[00:31:53.000 --> 00:32:02.000]   I just want to show you how these things look like.
[00:32:02.000 --> 00:32:09.000]   So see how I just had, I basically had a random input, 1 by 3 by 2 to 4 by 2 to 4, which represents a three channel image.
[00:32:09.000 --> 00:32:14.000]   I created my kite model. And then I put a set trace in this layer scale.
[00:32:14.000 --> 00:32:25.000]   I just want to show you what this layer scale is. In this case, instead of my, because it's 2 to 4 by 2 to 4 and a patch size is 16 by 16,
[00:32:25.000 --> 00:32:31.000]   which means you're going to have 196 patches. If none of this makes sense, this is, again, past things that have been explained in GWT.
[00:32:31.000 --> 00:32:38.000]   And each patch is then represented by a 384 long vector. So that's what's happening.
[00:32:38.000 --> 00:32:46.000]   So this is why my X shape is this 1 by 196 by 384. But what I wanted to show you is what's happening in this.
[00:32:46.000 --> 00:32:53.000]   This is what the self dot gamma one times this. This very small change is a layer scale.
[00:32:53.000 --> 00:33:01.000]   Like in code, you're just adding this one piece of multiplication. And that's what, and that's like the, everything been explained so far.
[00:33:01.000 --> 00:33:09.000]   It's just this one piece of change. So your self dot gamma one, if you have a look at self dot gamma one,
[00:33:09.000 --> 00:33:17.000]   it's basically initialized to a really small value. And then if you check the shape of it, it's 384,
[00:33:17.000 --> 00:33:25.000]   which means each channel, because when you multiply this to your input of shape 1 by 196 by 384,
[00:33:25.000 --> 00:33:32.000]   it just means that each channel of the patch is going to get a certain weight. So that's what is layer scale.
[00:33:32.000 --> 00:33:37.000]   And then in terms of like architecture, they add this when your things are going through self attention.
[00:33:37.000 --> 00:33:42.000]   And then it's also added when things are going through to the feed forward neural network.
[00:33:42.000 --> 00:33:50.000]   That's the main thing. Like this is the only thing is like you're having basically instead of like, if this was width,
[00:33:50.000 --> 00:33:55.000]   this would just look like this. That's how width would look like. That's it.
[00:33:55.000 --> 00:34:04.000]   This is now, this is the vision transformer block. And then with this small change where each channel gets its own weight is the layer scale block.
[00:34:04.000 --> 00:34:14.000]   So I hope that that makes things even clearer. What is the intuition behind adding weights to the channel?
[00:34:14.000 --> 00:34:21.000]   It's exactly what's been mentioned in the paper is if you go have a look at the paper and when you go back and have a look at the read of this,
[00:34:21.000 --> 00:34:25.000]   what they're trying to say is our objective is to group the weights associated.
[00:34:25.000 --> 00:34:28.000]   Sorry, one sec. Associated with the same output channel.
[00:34:28.000 --> 00:34:34.000]   So basically they're saying like each channel could have a different kind of importance.
[00:34:34.000 --> 00:34:40.000]   And that's the intuition that I sort of felt in like having different weights.
[00:34:40.000 --> 00:34:46.000]   What would be the class token at test time? Assuming we cannot use the ground truth class label inference?
[00:34:46.000 --> 00:34:51.000]   That is incorrect. We're still using ground truth labels. So don't worry about class tokens.
[00:34:51.000 --> 00:34:58.000]   I'm not explaining class tokens. Let me go and explain class token next.
[00:34:58.000 --> 00:35:03.000]   Cool. So that's the first change. That's the, that's layer scale.
[00:35:03.000 --> 00:35:09.000]   That's the main thing that we looked at today. The next thing we're looking at is class attention.
[00:35:09.000 --> 00:35:19.000]   So providing a summary of what's going on with class attention is.
[00:35:19.000 --> 00:35:27.000]   Again, going to copy paste this image multiple times. Let me go back here.
[00:35:27.000 --> 00:35:31.000]   I think that didn't work very well.
[00:35:31.000 --> 00:35:35.000]   I'm going to copy it again.
[00:35:35.000 --> 00:35:40.000]   And paste it here. OK, perfect. This time it works.
[00:35:40.000 --> 00:35:48.000]   All right. So again, what you have in wit is you have this class token.
[00:35:48.000 --> 00:35:57.000]   What the wit model is trying to do is it's trying to summarize the information basically from all of the patches to this class token.
[00:35:57.000 --> 00:36:04.000]   That's what the wit is trying to do. But it's also trying to then the earlier layers, or at least transformer encoder layers.
[00:36:04.000 --> 00:36:09.000]   They're also trying to find interaction between these various patches.
[00:36:09.000 --> 00:36:16.000]   Right. That's the main starting point for Kite.
[00:36:16.000 --> 00:36:27.000]   And what they say in Kite is.
[00:36:27.000 --> 00:36:36.000]   When we have basically there's a discrepancy on the first layers of the transformers, which are.
[00:36:36.000 --> 00:36:42.000]   I'm just trying to find the text where it says the first layers of.
[00:36:42.000 --> 00:36:53.000]   Sorry, one second. I'm just trying to find the text in the paper.
[00:36:53.000 --> 00:37:01.000]   Especially, always a contradictory. OK, here it is. The contradictory objective of guiding the attention process while processing the class embedding.
[00:37:01.000 --> 00:37:05.000]   In simple terms, that just means like the objective is like having two objectives.
[00:37:05.000 --> 00:37:16.000]   One is processing the summarizing the information in the class token and then also like guiding the attention process, which means it's trying to attend to all of these different different patches.
[00:37:16.000 --> 00:37:25.000]   And what they try and do as a solution is here. This is the main solution.
[00:37:25.000 --> 00:37:38.000]   So let me again paste that so I can explain things. So as a solution, what happens is on your left is wit.
[00:37:38.000 --> 00:37:48.000]   So this is the wit. So in wit you have the class token really early on. Right. And then the class token propagates throughout.
[00:37:48.000 --> 00:37:56.000]   But what they're saying is instead of having the class token introduced really early on, don't do it like this.
[00:37:56.000 --> 00:38:06.000]   How about you do it like this on your right. You have your first layers without the class token and then you introduce the class token later.
[00:38:06.000 --> 00:38:13.000]   OK, this is important to understand what they're trying to say is now what's going to happen is.
[00:38:13.000 --> 00:38:32.000]   These earlier layers, specifically the ones without the class token, they're just going to perform the self attention, the self attention operation or it's basically then just going to find interaction between these various patches of the image.
[00:38:32.000 --> 00:38:41.000]   So like all it's trying to do now is just have a look at the various parts of the image and it's just trying to process that information.
[00:38:41.000 --> 00:38:50.000]   It's not trying to summarize that information into a class token so far. That happens in these two layers.
[00:38:50.000 --> 00:38:57.000]   Then what these two layers are going to do is it's going to take whatever the earlier layers processed.
[00:38:57.000 --> 00:39:04.000]   So it's going to take that information and then it's going to put all of that information in this class token.
[00:39:04.000 --> 00:39:08.000]   So that's the difference between.
[00:39:08.000 --> 00:39:19.000]   That's the main difference between Kite and Vision Transformer. And that's why this paper is called class.
[00:39:19.000 --> 00:39:26.000]   Class attention layer. There was somewhere. That's why they called it Kite. Class attention.
[00:39:26.000 --> 00:39:40.000]   Here it is. Class attention in Image Transformers. That's why this is called Kite because this is what the class attention is doing. It's like it's being introduced later instead of being very early on.
[00:39:40.000 --> 00:39:46.000]   So that's the main intuition of Kite. There's not much.
[00:39:46.000 --> 00:39:52.000]   There's not a lot to go from here.
[00:39:52.000 --> 00:39:57.000]   As long as you get the intuition, you've got the paper. These are the only two main changes.
[00:39:57.000 --> 00:40:04.000]   And if we have a look at the code, it's really, really simple. Once we have a look at the code. So I'm just going to have a look at.
[00:40:04.000 --> 00:40:14.000]   If there's any questions on the intuition behind Kite. I hope now it makes sense. What is the class token? What's the difference in Vid and Kite?
[00:40:14.000 --> 00:40:25.000]   In Vid you have the class token early on. In Kite you have the class token at a later stage.
[00:40:25.000 --> 00:40:33.000]   I'm not sure. I think it has been. I think there's a few. I can't exactly remember which research paper did it.
[00:40:33.000 --> 00:40:45.000]   There definitely would have been previous papers. I can't find or from the top of my head, I don't really remember which paper did it or where it's been tried.
[00:40:45.000 --> 00:40:54.000]   Oh, the SCNet is one of those, right? The SCNet. So if you have a look at.
[00:40:54.000 --> 00:41:03.000]   I'm just trying to answer this Bharat's question on like, has this been tried in the past? I think the squeeze and excitation network would be the closest.
[00:41:03.000 --> 00:41:08.000]   So in squeeze and excitation, basically each channel has its own weight.
[00:41:08.000 --> 00:41:14.000]   I think the closest that I can remember for now is SCNet.
[00:41:14.000 --> 00:41:17.000]   Okay, that's that.
[00:41:17.000 --> 00:41:21.000]   What else is left? Not much. Not much.
[00:41:21.000 --> 00:41:28.000]   Now let's have a look at the difference in class attention in terms of like the implementation.
[00:41:28.000 --> 00:41:36.000]   So this is the class Kite model in TIM. As you can see, there's this class attention.
[00:41:36.000 --> 00:41:42.000]   In class attention, the main thing it's trying to do is like you have your attention being performed.
[00:41:42.000 --> 00:41:57.000]   So you take the bat size, the number of patches and the number of channels you have, you extract the query and key, perform the, basically this is just getting the values.
[00:41:57.000 --> 00:42:03.000]   We perform the attention operation. And finally, now this is where this class attention is happening.
[00:42:03.000 --> 00:42:09.000]   It's like it's trying to now summarize everything that it's learned so far into this class token.
[00:42:09.000 --> 00:42:18.000]   And then you finally have like a projection, which is just a linear layer, which is just extra processing or computation on top.
[00:42:18.000 --> 00:42:20.000]   And then you finally return that class token.
[00:42:20.000 --> 00:42:32.000]   But in case if you have a look in terms of like the difference of this class token, so there's now, they just called it two kinds of attention.
[00:42:32.000 --> 00:42:39.000]   I'm just having a look at what the second type is called, which is talking head attention.
[00:42:39.000 --> 00:42:43.000]   So the first part, I think, is the talking attention. This one does not have the class token.
[00:42:43.000 --> 00:42:51.000]   So that's the main difference in terms of like implementation is like the earlier layers of attention don't have the class token.
[00:42:51.000 --> 00:42:56.000]   I haven't mentioned talking heads attention so far. Let me quickly do that.
[00:42:56.000 --> 00:43:04.000]   Okay.
[00:43:04.000 --> 00:43:09.000]   So let's have a look at the, let's read this part of the kite models.
[00:43:09.000 --> 00:43:12.000]   Our kite models are built upon width.
[00:43:12.000 --> 00:43:22.000]   The only difference is that we incorporate layer scale and the two stages architectures with class attention layers described in section three.
[00:43:22.000 --> 00:43:30.000]   So this two stage architectures that we've had a look at are, if you go back to section three, there's a few things I want to touch upon again.
[00:43:30.000 --> 00:43:39.000]   So if you go back to section three, this is the two stage architecture. It's like the first stage is without the class token.
[00:43:39.000 --> 00:43:48.000]   And then the second stage is with the class token. So that's why they just called it two stage.
[00:43:48.000 --> 00:43:52.000]   That's the majority of it.
[00:43:52.000 --> 00:43:59.000]   And then going back to the, I'll come back to like what these figures mean in the paper. Just give me two minutes. I just want to first summarize.
[00:43:59.000 --> 00:44:11.000]   So then this is the class attention layers. If you now go back and you read this part, this will say, like, what we want to do is the performance increases when we insert the class embedding later in the transformer.
[00:44:11.000 --> 00:44:20.000]   This is the main thing. It is maximized. And they say, like, when do you insert, like, if you have a 12-layer long vision transformer, when do you insert it?
[00:44:20.000 --> 00:44:25.000]   They say, like, the best position to insert is just two layers before the input.
[00:44:25.000 --> 00:44:36.000]   If you have, like, four layers, it's like the performance sort of saturates or like there's no major difference in having the class token two layers before the input/output or four layers before the output.
[00:44:36.000 --> 00:44:38.000]   That's the main thing.
[00:44:38.000 --> 00:44:44.000]   And then these are the various kite models. You can have a look. It's really simple now to read all of these.
[00:44:44.000 --> 00:44:54.000]   Like for the S24, you have 24 layers of first self-attention, which is without the class token. And then you have two layers of class attention.
[00:44:54.000 --> 00:45:02.000]   Then the dimension is 384. That's the number of params. And then that's just the accuracies that you get.
[00:45:02.000 --> 00:45:10.000]   It's really simple now to understand. So if you go back and now if you have a look at the paper, it will make sense and everything should be easy to read.
[00:45:10.000 --> 00:45:20.000]   I just want to sort of touch upon some parts of this, some parts of, like, some other experiments that they did or some other changes that this paper introduces.
[00:45:20.000 --> 00:45:23.000]   One of them is the stochastic depth coefficient.
[00:45:23.000 --> 00:45:35.000]   If you don't know what stochastic depth is, I think that's been again, that's something in the past, but I should have that should have a reference somewhere that explains stochastic depth.
[00:45:35.000 --> 00:45:44.000]   It's in here, I believe.
[00:45:44.000 --> 00:45:45.000]   Yeah.
[00:45:45.000 --> 00:45:46.000]   So here it is.
[00:45:46.000 --> 00:45:55.000]   So as part of this blog post, let me share that in the chat. If you're new to stochastic depth, have a look at what the difference is over here.
[00:45:55.000 --> 00:46:01.000]   But what they say is, because the main idea they want to do is they want to train deeper and deeper architectures.
[00:46:01.000 --> 00:46:15.000]   What they say is, they introduced this stochastic depth as part of the vision transformer. And what they say is, as we go deeper and deeper, so let's say, initially, the stochastic depth coefficient is zero.
[00:46:15.000 --> 00:46:20.000]   Then if you're training a model with 18 blocks, it gets about like 81% accuracy.
[00:46:20.000 --> 00:46:24.000]   If you're training a model with 12 blocks, it gets about 80.5.
[00:46:24.000 --> 00:46:30.000]   But if you're training a very deep block, 36 blocks, this is why they say, like, the vision transformer is unstable.
[00:46:30.000 --> 00:46:36.000]   When you have a lot of layers, when you have 36 blocks, then it gets 77.9 or something like that accuracy.
[00:46:36.000 --> 00:46:42.000]   But see how this accuracy increases as the stochastic depth is introduced.
[00:46:42.000 --> 00:46:53.000]   So what they're trying to do is then, since the main idea is to train deeper architectures, when you have 36 blocks, then you add a point to stochastic depth coefficient.
[00:46:53.000 --> 00:47:01.000]   And then that's when they say, OK, for deeper architectures, you need to have some stochastic depth as part of that.
[00:47:01.000 --> 00:47:14.000]   The next thing that they mention in this paper in terms of a key difference in vision transformer and the Kite paper is this idea of the contribution of the residual branches.
[00:47:14.000 --> 00:47:35.000]   So if you have a 34 layer deep architecture, in vision transformer, what typically happens is like some residual branches, like in the 34th layer, the contribution of the residual branch is a lot more than, say, for like this 11th layer or 30th layer.
[00:47:35.000 --> 00:47:53.000]   And by adding layer scale or by having effective optimization, by adding all of these steps that we've looked at so far, having class attention, having layer scale, having stochastic depth, what that does is it's sort of, it kind of is easier to optimize the Kite paper.
[00:47:53.000 --> 00:48:06.000]   And they say now have a look like in terms of the contribution of the residual branch, now it's more stable or it's like around the same 20% as opposed to the shortcut.
[00:48:06.000 --> 00:48:09.000]   So that's the main thing in part of this image.
[00:48:09.000 --> 00:48:23.000]   In terms of like this, what they're trying to say is, okay, when should we insert the class embedding, there's like different layers where they insert, they either insert it after a second or they insert after 11th layer.
[00:48:23.000 --> 00:48:40.000]   They do it for the same with diet, and they say, okay, these are the various parts where we introduce, and they say like the performance is maximized if the class token is just inserted two layers before the output.
[00:48:40.000 --> 00:48:43.000]   So that's another part of this.
[00:48:43.000 --> 00:48:49.000]   Then this is just the various Kite models, the stable tree is just these various Kite models that have been defined.
[00:48:49.000 --> 00:49:01.000]   And that's pretty much it about the paper, then you have results, they compare the results to all of these different NF nets, efficient nets, basically your CNNs.
[00:49:01.000 --> 00:49:12.000]   And you can see how the Kite is able to have like 86.5 emission at top one but this is again a really big architecture. There's some parts that have skipped today.
[00:49:12.000 --> 00:49:25.000]   The part that I've skipped is this idea of distillation, I did kind of touch upon it very briefly when I was saying, but this, this is just dependent on the diet paper so because we didn't look at the diet paper today.
[00:49:25.000 --> 00:49:33.000]   I just kind of touched upon this part of like having token distillation, or like having a teacher, teacher student.
[00:49:33.000 --> 00:49:45.000]   And then this is just saying okay step by step from diet to Kite. So there's just say okay when we have more heads, we start with 79.9% accuracy in the diet.
[00:49:45.000 --> 00:50:04.000]   And then as we keep adding layer scale, you see how, so you see, this is kind of interesting to note, it's like when you have 79.9%, and you have now you increase the depth to 36 blocks, the accuracy goes down from 80 to 70.
[00:50:04.000 --> 00:50:14.000]   When they add layer scale with proper initialization, the accuracy then goes back up to 80.5. Then you add stochastic depth.
[00:50:14.000 --> 00:50:26.000]   Then you add the class attention list, see how I'm just adding the class attention towards later part increases accuracy by 0.2%, then you have longer training, and so on and you keep doing that.
[00:50:26.000 --> 00:50:34.000]   So just crop ratio is just towards inference, instead of inferencing on like a smaller image you inference on a larger image.
[00:50:34.000 --> 00:50:38.000]   And then by the end you have like a really good 85.4% accuracy.
[00:50:38.000 --> 00:50:49.000]   In terms of like crop ratio, this should be really easy and simple to read but that's it. That's the, that's the main parts of the paper.
[00:50:49.000 --> 00:50:56.000]   So let's see, let's take questions for the last 10 minutes.
[00:50:56.000 --> 00:51:08.000]   If there are any.
[00:51:08.000 --> 00:51:10.000]   Alright, there's a few questions.
[00:51:10.000 --> 00:51:28.000]   Great session loving it the second last question was misread. Okay, sorry. The question was, what is the class token set to add test time for inference, since we obviously don't have access to the ground truth class.
[00:51:28.000 --> 00:51:33.000]   I see what you're trying to say is like, okay.
[00:51:33.000 --> 00:51:37.000]   So,
[00:51:37.000 --> 00:51:47.000]   I think what you're trying to say, and please correct me again if I didn't get it right this time as well. But what you're trying to say is, I'm sorry I copied the wrong architecture.
[00:51:47.000 --> 00:51:50.000]   One second.
[00:51:50.000 --> 00:52:00.000]   The vision transformer should be enough to answer this. So let's just copy the vision transformer, because kites just does things similarly.
[00:52:00.000 --> 00:52:04.000]   So what you're saying is,
[00:52:04.000 --> 00:52:07.000]   when you're trying to do inference.
[00:52:07.000 --> 00:52:19.000]   Actually, that was the sorry, I'm really sorry for. I'll go back to this, I'll go back to this image it's a, it's a nicer image to explain this.
[00:52:19.000 --> 00:52:20.000]   I'm going down.
[00:52:20.000 --> 00:52:21.000]   Paste.
[00:52:21.000 --> 00:52:22.000]   All right.
[00:52:22.000 --> 00:52:33.000]   I guess the question is, ignore the distillation token we're not looking at this over here. I guess the question is,
[00:52:33.000 --> 00:52:37.000]   I did something again.
[00:52:37.000 --> 00:52:53.000]   I guess the question is, this class token, what happens to this class token during inference. So what happens is, essentially, because what are you trying to look at is, what happens is when you have this class token.
[00:52:53.000 --> 00:53:06.000]   This class token is not the label, I think you're mistaking the class token as a label. The class token is first initialized as just a vector of whatever your dimension is.
[00:53:06.000 --> 00:53:10.000]   So if all of these patches are say.
[00:53:10.000 --> 00:53:12.000]   So you have your image.
[00:53:12.000 --> 00:53:25.000]   This is how this image looks like and I have say nine patches, then each of these nine patches like this on the ninth batch over here are represented by a vector of some dimension.
[00:53:25.000 --> 00:53:31.000]   So let's say that dimension is 784. So let's say this, this dimension is 784.
[00:53:31.000 --> 00:53:46.000]   Okay. And so what do you have before things are fed to the transformer, you have a nine cross 784 matrix, which kind of is now a representation of your image.
[00:53:46.000 --> 00:53:48.000]   Okay.
[00:53:48.000 --> 00:54:02.000]   Now, what happens is you add this class token, which is one cross 784. So your overall input is 10 cross 784.
[00:54:02.000 --> 00:54:18.000]   Now this class token has nothing to do with the label like you don't need to know the label. But when you're trying to do your classification, the transformer model is going to try and take information from your patches.
[00:54:18.000 --> 00:54:34.000]   It's going to try and put that information into this 784 long class token. And then you use that. So you have a MLP at the top, which classifies say goes from 784 to whatever your number of classes is 10, 20, whatever.
[00:54:34.000 --> 00:54:39.000]   And then you have your cross entropy. And that's where you have a look at the label.
[00:54:39.000 --> 00:54:51.000]   So this is where the label comes in. But when you're doing inference, you don't calculate the loss, like you don't need to do cross entropy because you're not training your model anymore.
[00:54:51.000 --> 00:55:01.000]   So you still just have an MLP over here, which then classifies to your 10 classes and then you take the softmax so you have the final label.
[00:55:01.000 --> 00:55:05.000]   I hope that explains.
[00:55:05.000 --> 00:55:11.000]   So please reply to this comment if this time I kind of got the question.
[00:55:11.000 --> 00:55:19.000]   In CNN models we add global average pool and linear layer at the end. So there's no concept of class token and gradient flows.
[00:55:19.000 --> 00:55:26.000]   Why do transformer architectures use class token instead of combining all the outputs? Great question.
[00:55:26.000 --> 00:55:31.000]   Where is the paper?
[00:55:31.000 --> 00:55:39.000]   There's the paper.
[00:55:39.000 --> 00:55:46.000]   The answer is, where is the answer? I'm trying to find the table that will answer your question.
[00:55:46.000 --> 00:55:48.000]   Here's the answer.
[00:55:48.000 --> 00:55:57.000]   In transformer, if you do average pooling, you get a top one accuracy of, say, 80 or 79%.
[00:55:57.000 --> 00:56:03.000]   But if you have your class attention or you have your class token, you get a better validation accuracy.
[00:56:03.000 --> 00:56:09.000]   So if something's making your validation accuracy go up, do that instead of something that's not.
[00:56:09.000 --> 00:56:13.000]   Thanks for the clarification. Okay, perfect. We solved that.
[00:56:13.000 --> 00:56:22.000]   So I hope, Ramesh, that answers. The only reason why you have your class attention or like the idea came from BERT transformer.
[00:56:22.000 --> 00:56:28.000]   And that's what I think. I think that's -- I'm a little weak on the literature in terms of NLP side of things.
[00:56:28.000 --> 00:56:32.000]   But I think the idea was introduced in BERT first.
[00:56:32.000 --> 00:56:44.000]   And as you will see in this table, if you have your average pooling instead of having the class attention or like the class token, you just have a lower accuracy.
[00:56:44.000 --> 00:56:47.000]   So why not have that?
[00:56:47.000 --> 00:56:50.000]   Class attention equal to self-attention.
[00:56:50.000 --> 00:57:00.000]   Just adding class token to the previous layer's output. Is that understanding correct?
[00:57:00.000 --> 00:57:04.000]   No, that's not the correct understanding, I think.
[00:57:04.000 --> 00:57:08.000]   Class attention equals just adding class token to the previous layer's output.
[00:57:08.000 --> 00:57:15.000]   No, that's incorrect. The main difference is the position.
[00:57:15.000 --> 00:57:21.000]   Okay, maybe I didn't explain this very well. But the main difference is the position where you add the class token.
[00:57:21.000 --> 00:57:28.000]   If you add the class token early on, then that's wit.
[00:57:28.000 --> 00:57:33.000]   But if you add the class token later on, then that's kiet.
[00:57:33.000 --> 00:57:40.000]   But I guess class attention equals self-attention. Just adding class token to the previous layer's output.
[00:57:40.000 --> 00:57:42.000]   I don't really get the question very well.
[00:57:42.000 --> 00:57:47.000]   Like class attention and self-attention, it's just attention. It's just trying to attend.
[00:57:47.000 --> 00:57:52.000]   Class attention is trying to summarize everything that is learned into this class token.
[00:57:52.000 --> 00:57:57.000]   In self-attention, it's like in kiet paper, you'll see it's without the class token.
[00:57:57.000 --> 00:58:05.000]   So I don't really get the question very well. So I'm sorry, I'm not able to explain the answer very well.
[00:58:05.000 --> 00:58:12.000]   Will the addition of layer scale and class attention...
[00:58:12.000 --> 00:58:17.000]   I haven't worked much. Will it lead to improvement in terms of explainability and interpretability of the network?
[00:58:17.000 --> 00:58:24.000]   I think the standard explainability or interpretation techniques would still work.
[00:58:24.000 --> 00:58:28.000]   Like you still have your attention.
[00:58:28.000 --> 00:58:36.000]   In a way, I think if you have a look at... Where is this?
[00:58:36.000 --> 00:58:42.000]   I'm trying to find... There's really nice visualizations of...
[00:58:42.000 --> 00:58:46.000]   In terms of interpretation, I'm not sure if it will make things better.
[00:58:46.000 --> 00:58:52.000]   But there's these nice visualizations where they just show the attention maps.
[00:58:52.000 --> 00:58:59.000]   So you can have a look at this part. I haven't checked that much in detail in terms of interpretation.
[00:58:59.000 --> 00:59:02.000]   But yeah, the answer is I don't know.
[00:59:02.000 --> 00:59:07.000]   Bharath, if you want to post or repost the question...
[00:59:07.000 --> 00:59:11.000]   I'll just try and summarize self-attention and class attention for you.
[00:59:11.000 --> 00:59:14.000]   In self-attention, you don't have the class embedding token.
[00:59:14.000 --> 00:59:21.000]   So in the Kite paper, then the earlier layers, which are called... I'll go back to the paper architecture.
[00:59:21.000 --> 00:59:32.000]   Okay, here it is. So then you have your self-attention layers early on.
[00:59:32.000 --> 00:59:36.000]   That's what Kite calls them. And you have a class attention layer later on.
[00:59:36.000 --> 00:59:38.000]   In self-attention, you don't have the class embedding.
[00:59:38.000 --> 00:59:44.000]   So what the self-attention is trying to do is just trying to summarize the information in these various different patches.
[00:59:44.000 --> 00:59:48.000]   In class attention, you're not summarizing the information in these various different patches.
[00:59:48.000 --> 00:59:50.000]   Because it's already been summarized.
[00:59:50.000 --> 00:59:55.000]   What you're trying to do is you're trying to take that summary and you're trying to put that all in this class token.
[00:59:55.000 --> 00:59:58.000]   Because remember, then that's what happens.
[00:59:58.000 --> 01:00:04.000]   It's like this class token, then one 784 long vector becomes the representation of the whole image.
[01:00:04.000 --> 01:00:08.000]   So that's the difference in class attention and self-attention.
[01:00:08.000 --> 01:00:13.000]   This class attention has this class token. Self-attention does not have this class token.
[01:00:13.000 --> 01:00:15.000]   So I hope that helps.
[01:00:15.000 --> 01:00:19.000]   With that being said, I guess that's all the questions.
[01:00:19.000 --> 01:00:21.000]   And that's Kite. So have a look.
[01:00:21.000 --> 01:00:23.000]   Guys, everybody, have a read of this paper.
[01:00:23.000 --> 01:00:31.000]   I hope it will be really easy to read from after today's session.
[01:00:31.000 --> 01:00:34.000]   But do let me know if you still have any questions.
[01:00:34.000 --> 01:00:36.000]   And thanks, everybody, for attending.
[01:00:36.000 --> 01:00:46.000]   And going forward, we're looking at one of the toughest papers I've read on object detection.
[01:00:46.000 --> 01:00:52.000]   But there's a few things that will have to be covered as part of this Transformers.
[01:00:52.000 --> 01:01:00.000]   So in the coming, say, next two weeks on Twitter, I'm going to be announcing blogs on this end-to-end object detection with Transformers.
[01:01:00.000 --> 01:01:06.000]   And we will start looking into this part of how do we do object detection using Transformers.
[01:01:06.000 --> 01:01:10.000]   So that's what we're going to have a look at in two weeks from now.
[01:01:10.000 --> 01:01:13.000]   So thanks, everybody, and see you next time.
[01:01:13.000 --> 01:01:25.000]   [MUSIC PLAYING]

