
[00:00:00.000 --> 00:00:04.080]   It's wonderful to be here, wonderful to see so many faces
[00:00:04.080 --> 00:00:08.880]   that I've come to love over the years.
[00:00:08.880 --> 00:00:13.880]   My advisor, my family's here, my mom, brother.
[00:00:13.880 --> 00:00:20.200]   You know, I did ask security to make sure my dad doesn't,
[00:00:20.200 --> 00:00:26.520]   is not allowed in, but he somehow found his way in,
[00:00:26.520 --> 00:00:28.480]   so good job.
[00:00:28.640 --> 00:00:30.040]   (audience laughing)
[00:00:30.040 --> 00:00:33.640]   The topic of today's talk reminds me
[00:00:33.640 --> 00:00:35.440]   of something my dad once told me.
[00:00:35.440 --> 00:00:37.340]   I wrote it down.
[00:00:37.340 --> 00:00:40.920]   Fewer those who see with their own eyes
[00:00:40.920 --> 00:00:42.600]   and feel with their own hearts.
[00:00:42.600 --> 00:00:46.840]   No, wait, that actually was Albert Einstein,
[00:00:46.840 --> 00:00:51.240]   different Jew, similar haircut for those of you.
[00:00:51.240 --> 00:00:52.440]   (audience laughing)
[00:00:52.440 --> 00:00:53.280]   Similar.
[00:00:53.280 --> 00:00:58.040]   You know, there's a saying, there's an old saying
[00:00:58.040 --> 00:01:02.120]   that goes, "Give a man a fish and you feed him for a day,
[00:01:02.120 --> 00:01:05.040]   "teach a man to fish and you feed him for a lifetime."
[00:01:05.040 --> 00:01:09.800]   A little known fact, it actually goes on to say,
[00:01:09.800 --> 00:01:13.960]   "So that he may never discover how much he loves steak."
[00:01:13.960 --> 00:01:16.120]   Or vegetarian lasagna for those of you
[00:01:16.120 --> 00:01:18.680]   who are vegetarian in the audience.
[00:01:18.680 --> 00:01:23.680]   And the key there, the key idea is society tries to,
[00:01:24.280 --> 00:01:29.280]   impose lessons to teach, to drive the human being,
[00:01:29.280 --> 00:01:33.240]   each of us, but it's you discovering your own passion
[00:01:33.240 --> 00:01:34.560]   is the key, and that's what the talk
[00:01:34.560 --> 00:01:36.000]   I'd like to talk about today.
[00:01:36.000 --> 00:01:39.000]   And there'll be a lot of poems throughout.
[00:01:39.000 --> 00:01:43.560]   And the central poem by Shel Silverstein called "The Voice"
[00:01:43.560 --> 00:01:48.080]   is one I think that will resonate throughout the talk.
[00:01:48.080 --> 00:01:50.040]   There's a voice that's in the air,
[00:01:50.040 --> 00:01:55.040]   it's a voice that's in the air, it's a voice that's in the air.
[00:01:55.040 --> 00:02:00.040]   It's a voice that's in the air, it's a voice that's in the air.
[00:02:00.040 --> 00:02:05.200]   It's a voice that's in the air, it's a voice that's in the air.
[00:02:05.200 --> 00:02:10.280]   It's a voice that's in the air, it's a voice that's in the air.
[00:02:10.280 --> 00:02:15.600]   It's a voice that's in the air, it's a voice that's in the air.
[00:02:15.760 --> 00:02:20.760]   And that's the poem we have together over two small topics,
[00:02:20.760 --> 00:02:23.840]   life and artificial intelligence.
[00:02:23.840 --> 00:02:29.000]   Now, from an optimization perspective,
[00:02:29.000 --> 00:02:32.880]   and one of my co-advisors has always told me
[00:02:32.880 --> 00:02:36.040]   when you show a plot, you have to describe the X axis
[00:02:36.040 --> 00:02:38.800]   and the Y axis as a good engineer.
[00:02:38.800 --> 00:02:41.040]   There you go, that's lesson number one.
[00:02:41.040 --> 00:02:45.520]   The X axis is competence, the Y axis is confidence.
[00:02:45.520 --> 00:02:51.240]   And there's something called the Dunning-Kruger effect,
[00:02:51.240 --> 00:02:53.320]   which is captured by this plot.
[00:02:53.320 --> 00:02:57.080]   And that is at the beginning of your journey of competence,
[00:02:57.080 --> 00:02:58.560]   when you're not very good at something,
[00:02:58.560 --> 00:03:01.000]   when you're first taking the first steps
[00:03:01.000 --> 00:03:03.440]   of learning something, as some of you here are
[00:03:03.440 --> 00:03:06.720]   in the engineering fields, you're overly confident.
[00:03:06.720 --> 00:03:08.480]   It's the peak of confidence,
[00:03:08.480 --> 00:03:11.440]   and you're at the lowest stage of actually
[00:03:11.440 --> 00:03:14.280]   of your abilities, of your expertise.
[00:03:14.280 --> 00:03:19.280]   And it's funny that I'm speaking here before you today
[00:03:19.280 --> 00:03:27.640]   in a place of a complete sort of self-doubt and despair
[00:03:27.640 --> 00:03:30.680]   and not knowing what I'm doing at all.
[00:03:30.680 --> 00:03:33.960]   And I feel like I have zero expertise to impart on you.
[00:03:33.960 --> 00:03:38.120]   And so in that sense, it's a funny position
[00:03:38.120 --> 00:03:41.040]   to be speaking with, especially some of the lessons,
[00:03:41.040 --> 00:03:42.920]   some of the advice I'll try to give.
[00:03:42.920 --> 00:03:44.720]   So take that with a grain of salt.
[00:03:44.720 --> 00:03:48.600]   And some of you sitting in the audience today
[00:03:48.600 --> 00:03:50.720]   may be at the very peak, especially if you're
[00:03:50.720 --> 00:03:53.880]   at the beginning of the college journey, university journey.
[00:03:53.880 --> 00:03:57.840]   And I'd say to me, the biggest positive,
[00:03:57.840 --> 00:04:01.580]   the biggest impact of college and university education
[00:04:01.580 --> 00:04:06.580]   is the dismantling of the ego that's involved
[00:04:07.440 --> 00:04:10.680]   in going from that peak overconfidence
[00:04:10.680 --> 00:04:13.220]   to the valley of despair that I'm currently in.
[00:04:13.220 --> 00:04:17.880]   Oh, and I should mention that this is also the time for me
[00:04:17.880 --> 00:04:21.800]   and perhaps for you where folks like Dostoevsky
[00:04:21.800 --> 00:04:23.760]   start making a lot of sense,
[00:04:23.760 --> 00:04:27.120]   talking about suffering and pain
[00:04:27.120 --> 00:04:31.400]   and how the really great men and women must,
[00:04:31.400 --> 00:04:35.160]   I think as he says, have great sadness on earth.
[00:04:35.160 --> 00:04:36.880]   This resonates with everybody
[00:04:36.880 --> 00:04:39.080]   in their undergraduate years in engineering.
[00:04:39.080 --> 00:04:42.480]   Now, the real thing I'd like to talk about
[00:04:42.480 --> 00:04:44.680]   is the broader optimization problem
[00:04:44.680 --> 00:04:48.480]   formed by the Dunning-Kruger effect,
[00:04:48.480 --> 00:04:52.120]   which is after the peak of confidence
[00:04:52.120 --> 00:04:55.320]   and the valley of despair, there's a gradient
[00:04:55.320 --> 00:04:58.720]   provided to you by your advisors, by your parents,
[00:04:58.720 --> 00:05:03.040]   by your friends, your loved ones, society in general.
[00:05:03.040 --> 00:05:05.880]   The gradient over which you're optimized
[00:05:05.880 --> 00:05:10.280]   to achieve some definition of success.
[00:05:10.280 --> 00:05:12.360]   This is what I call the local optimum.
[00:05:12.360 --> 00:05:15.020]   What everybody else tells you you're supposed to do.
[00:05:15.020 --> 00:05:18.280]   What everybody else at the small scale, on a daily scale,
[00:05:18.280 --> 00:05:20.720]   and on the weekly scale, monthly, yearly,
[00:05:20.720 --> 00:05:22.120]   and for the rest of your life
[00:05:22.120 --> 00:05:24.620]   tells you what the definition of success is.
[00:05:24.620 --> 00:05:26.240]   That's the local optimum.
[00:05:26.240 --> 00:05:29.400]   What I'd like to argue is some ideas
[00:05:29.400 --> 00:05:32.000]   of how to break out of that convention,
[00:05:32.000 --> 00:05:37.000]   of how to listen just enough to hear the lessons
[00:05:37.000 --> 00:05:40.720]   in society, advisors, friends, and parents,
[00:05:40.720 --> 00:05:43.480]   but for the rest of it, ignore their voices
[00:05:43.480 --> 00:05:45.260]   and only listen to your own voice.
[00:05:45.260 --> 00:05:50.420]   And I'll tell you through my own story here.
[00:05:50.420 --> 00:05:53.180]   So I was introduced as a research scientist at MIT.
[00:05:53.180 --> 00:05:57.560]   And very recently, I decided to step down from MIT
[00:05:57.560 --> 00:05:59.640]   to do my own startup.
[00:05:59.640 --> 00:06:00.880]   I'm still affiliated there,
[00:06:00.880 --> 00:06:04.640]   but sort of give up the salary, give up everything,
[00:06:04.640 --> 00:06:06.840]   give up what I'm supposed to be,
[00:06:06.840 --> 00:06:11.360]   the definition under academic colleagues of what success is,
[00:06:11.360 --> 00:06:15.500]   of what the pursuit of the academic life is,
[00:06:15.500 --> 00:06:17.600]   because I'm listening to the voice inside.
[00:06:17.600 --> 00:06:20.700]   And so I'm speaking to you
[00:06:20.700 --> 00:06:22.880]   at the very beginning of this journey,
[00:06:22.880 --> 00:06:24.940]   again, full of self-doubt.
[00:06:24.940 --> 00:06:26.680]   And so take with a grain of salt,
[00:06:26.680 --> 00:06:30.240]   but perhaps it's interesting to speak from this position,
[00:06:30.240 --> 00:06:33.320]   'cause I would argue it's the most beautiful position
[00:06:33.320 --> 00:06:34.640]   to be in in life.
[00:06:34.640 --> 00:06:39.280]   The opportunity, the freedom in the struggles
[00:06:39.280 --> 00:06:43.100]   that I'm undergoing now is really a gift
[00:06:43.100 --> 00:06:46.060]   that comes at the end of this journey of college.
[00:06:46.060 --> 00:06:49.400]   Now, who am I?
[00:06:49.400 --> 00:06:53.160]   And what is the dream that I mentioned there at the end?
[00:06:53.160 --> 00:06:54.500]   The global optimum.
[00:06:54.500 --> 00:06:58.080]   For me, that's understanding the human mind
[00:06:58.080 --> 00:07:01.520]   and engineering artificial intelligence systems.
[00:07:01.520 --> 00:07:07.120]   Visualized on the left here is just 3% of the neurons
[00:07:07.120 --> 00:07:08.160]   in the human brain.
[00:07:08.160 --> 00:07:10.020]   It's a mysterious, beautiful thing.
[00:07:10.020 --> 00:07:14.720]   It's easy to forget how little we know about this mystery
[00:07:14.720 --> 00:07:16.760]   that's just between our two ears.
[00:07:16.760 --> 00:07:20.160]   And engineering machines that can reason,
[00:07:20.160 --> 00:07:22.680]   that can think, that can perceive the world
[00:07:22.680 --> 00:07:24.480]   is one of the ways we can understand
[00:07:26.960 --> 00:07:30.680]   this mysterious, beautiful thing that brings to life
[00:07:30.680 --> 00:07:31.820]   everything around us.
[00:07:31.820 --> 00:07:36.800]   And the dream of creating intelligence systems,
[00:07:36.800 --> 00:07:40.040]   companions, ones that you can have a deep connection with.
[00:07:40.040 --> 00:07:41.340]   That's what drives me.
[00:07:41.340 --> 00:07:43.040]   That's my startup work.
[00:07:43.040 --> 00:07:45.200]   That's what my entrepreneurship work,
[00:07:45.200 --> 00:07:47.320]   that's my research work is focused on.
[00:07:47.320 --> 00:07:49.720]   Most of the work at MIT and before that
[00:07:49.720 --> 00:07:52.180]   has been on robotics and autonomous vehicles.
[00:07:52.260 --> 00:07:57.020]   But now the dream is to create a system that you can love
[00:07:57.020 --> 00:07:58.300]   and it can love you back.
[00:07:58.300 --> 00:08:02.540]   A brief history of artificial intelligence
[00:08:02.540 --> 00:08:05.260]   to give you a sense, to give you a quick review
[00:08:05.260 --> 00:08:07.140]   if this is a totally new field.
[00:08:07.140 --> 00:08:09.460]   Again, if you're undergraduate,
[00:08:09.460 --> 00:08:11.900]   perhaps this is a field that you want to,
[00:08:11.900 --> 00:08:17.180]   that you want to take on as your journey.
[00:08:17.180 --> 00:08:21.020]   So it started on the theoretical end with Alan Turing
[00:08:21.020 --> 00:08:24.140]   and many of the ideas from philosophy to mathematics
[00:08:24.140 --> 00:08:27.680]   that he presented and from whom the field was born.
[00:08:27.680 --> 00:08:31.620]   And on the engineering side, Frank Crosonblatt
[00:08:31.620 --> 00:08:34.220]   in building the Perceptron, the first machine.
[00:08:34.220 --> 00:08:37.420]   So engineering machines that can do some aspect of learning,
[00:08:37.420 --> 00:08:39.580]   some aspect of search that we associate
[00:08:39.580 --> 00:08:41.020]   with artificial intelligence.
[00:08:41.020 --> 00:08:44.220]   And then there's been accomplishments throughout,
[00:08:44.220 --> 00:08:47.400]   none greater, at least to me, than in this,
[00:08:47.400 --> 00:08:49.920]   at least for now in a span of games.
[00:08:49.920 --> 00:08:52.900]   There's been two branches of artificial intelligence
[00:08:52.900 --> 00:08:54.740]   that have dominated the field.
[00:08:54.740 --> 00:08:58.380]   The early days have been, you can think of a search,
[00:08:58.380 --> 00:08:59.740]   as brute force search.
[00:08:59.740 --> 00:09:03.740]   It's not quite as captivating to our imagination.
[00:09:03.740 --> 00:09:05.780]   It doesn't quite feel like intelligence
[00:09:05.780 --> 00:09:08.940]   because it's brute force searching through possible answers
[00:09:08.940 --> 00:09:11.460]   until you find one that's optimal.
[00:09:11.460 --> 00:09:14.220]   It's converting every single problem to a search problem
[00:09:14.220 --> 00:09:16.840]   and then bringing computational power to it
[00:09:16.840 --> 00:09:18.340]   to try to solve it.
[00:09:18.340 --> 00:09:21.040]   But nevertheless, the peak of that,
[00:09:21.040 --> 00:09:22.440]   especially for those who play chess,
[00:09:22.440 --> 00:09:24.040]   especially for those who might be a Russian,
[00:09:24.040 --> 00:09:29.040]   is when IBM D-Blue defeated Garry Kasparov in 1997.
[00:09:29.040 --> 00:09:31.940]   This is a seminal moment in artificial intelligence
[00:09:31.940 --> 00:09:34.760]   where the game that was associated with thought,
[00:09:34.760 --> 00:09:38.320]   with intelligence, with reason, was overcome,
[00:09:38.320 --> 00:09:42.440]   was the greatest champion and human champion
[00:09:42.440 --> 00:09:44.240]   was defeated by a machine.
[00:09:44.240 --> 00:09:47.160]   And the seminal moment on the second branch
[00:09:47.160 --> 00:09:49.960]   of artificial intelligence, which is learning systems,
[00:09:49.960 --> 00:09:51.460]   systems that learn from scratch,
[00:09:51.460 --> 00:09:55.640]   knowing nothing, with zero human assistance,
[00:09:55.640 --> 00:10:01.560]   was able to defeat the greatest player in the world.
[00:10:01.560 --> 00:10:06.200]   Little side note, the first moment did have human assistance
[00:10:06.200 --> 00:10:09.520]   in the AlphaGo system from DeepMind and Google DeepMind.
[00:10:09.520 --> 00:10:11.900]   And then the follow on a few months later,
[00:10:11.900 --> 00:10:15.540]   the system called AlphaZero was able to learn from scratch
[00:10:15.540 --> 00:10:16.800]   by playing itself.
[00:10:16.800 --> 00:10:19.160]   This is, to me, the greatest accomplishment
[00:10:19.160 --> 00:10:20.520]   of artificial intelligence.
[00:10:20.520 --> 00:10:22.320]   And I'll mention when I discuss it
[00:10:22.320 --> 00:10:24.320]   about open problems in the field.
[00:10:24.320 --> 00:10:27.140]   And then in a real world application,
[00:10:27.140 --> 00:10:29.200]   like I said, I worked a lot in autonomous vehicles.
[00:10:29.200 --> 00:10:31.240]   This is one of the most exciting applications
[00:10:31.240 --> 00:10:33.600]   with autonomous and semi-autonomous vehicles.
[00:10:33.600 --> 00:10:38.340]   There's been deployments, lessons, explorations,
[00:10:38.340 --> 00:10:39.840]   a lot of different debates.
[00:10:39.840 --> 00:10:42.400]   This is the most exciting space of artificial intelligence.
[00:10:42.400 --> 00:10:44.720]   If you wanna have an impact as an engineer,
[00:10:44.720 --> 00:10:47.160]   autonomous vehicles is the space you will do so
[00:10:47.160 --> 00:10:50.480]   in the next, in the 2020s.
[00:10:50.480 --> 00:10:54.920]   And a quick whirlwind overview of key ideas
[00:10:54.920 --> 00:10:58.040]   in artificial intelligence that were key breakthroughs.
[00:10:58.040 --> 00:11:00.120]   So neural networks and Perceptron, like I said,
[00:11:00.120 --> 00:11:03.160]   was born in the '40s, '50s, and '60s.
[00:11:03.160 --> 00:11:05.280]   With the algorithms that dominate today's world
[00:11:05.280 --> 00:11:07.120]   of deep learning and machine learning
[00:11:07.120 --> 00:11:10.840]   have been invented in many, many decades ago,
[00:11:10.840 --> 00:11:13.080]   in the '70s and '80s,
[00:11:13.080 --> 00:11:16.040]   with convolutional for the computer vision aspect
[00:11:16.040 --> 00:11:20.000]   of things in the '80s and '90s,
[00:11:20.000 --> 00:11:23.400]   with LSTM, recurrent neural networks,
[00:11:23.400 --> 00:11:26.440]   they work with language, work with sequence of data,
[00:11:26.440 --> 00:11:31.000]   were developed in the '90s and proven out in the aughts.
[00:11:31.000 --> 00:11:33.640]   And then the deep learning quote unquote revolution,
[00:11:33.640 --> 00:11:36.960]   the term and the ideas of large-scale machine learning
[00:11:36.960 --> 00:11:41.760]   using neural networks was reborn in 2006
[00:11:41.760 --> 00:11:43.160]   in the early aughts,
[00:11:43.160 --> 00:11:46.560]   and then proven out in the seminal ImageNet moment
[00:11:46.560 --> 00:11:49.920]   when computer vision systems were able to,
[00:11:49.920 --> 00:11:54.040]   in the challenge of object recognition,
[00:11:54.040 --> 00:11:56.840]   image recognition, and the ImageNet data set,
[00:11:56.840 --> 00:11:58.560]   and the ImageNet challenge,
[00:11:58.560 --> 00:12:03.560]   neural networks were able to far outperform the competition
[00:12:03.560 --> 00:12:07.120]   and do so easily from just learning from data.
[00:12:07.120 --> 00:12:10.880]   And a few other developments.
[00:12:10.880 --> 00:12:12.760]   There's a lot of unsupervised learning,
[00:12:12.760 --> 00:12:14.200]   self-supervised learning ideas
[00:12:14.200 --> 00:12:18.040]   that were born in the '14, '15, '16, just a few years ago,
[00:12:18.040 --> 00:12:20.760]   and a lot of exciting ideas in the past few years.
[00:12:20.760 --> 00:12:22.440]   The past few years have been dominated
[00:12:22.440 --> 00:12:24.480]   by ideas in natural language processing
[00:12:24.480 --> 00:12:26.120]   with ideas of transformers.
[00:12:26.120 --> 00:12:28.000]   Anyway, this might be outside the scope
[00:12:28.000 --> 00:12:29.840]   of what you're familiar with.
[00:12:29.840 --> 00:12:31.480]   I encourage you to look into it.
[00:12:31.480 --> 00:12:34.880]   Transformers in particular, with natural languages,
[00:12:34.880 --> 00:12:36.960]   some of the most beautiful and exciting ideas
[00:12:36.960 --> 00:12:39.000]   that without any human supervision,
[00:12:39.000 --> 00:12:42.920]   you can learn to model language sufficiently well
[00:12:42.920 --> 00:12:46.920]   to outperform anything we've done previously,
[00:12:46.920 --> 00:12:49.000]   to do things like machine translation
[00:12:49.000 --> 00:12:51.480]   to a level that's unprecedented.
[00:12:51.480 --> 00:12:52.840]   It's really exciting.
[00:12:52.840 --> 00:12:56.760]   And especially exciting is that bigger is better,
[00:12:56.760 --> 00:12:59.320]   meaning that as long as we can scale compute,
[00:12:59.320 --> 00:13:01.480]   we can perform better and better and better.
[00:13:01.480 --> 00:13:03.360]   And it's a totally open question
[00:13:03.360 --> 00:13:05.560]   how, what the ceiling of that is.
[00:13:05.560 --> 00:13:07.680]   And finally, the most exciting thing
[00:13:07.680 --> 00:13:11.240]   in artificial intelligence is the idea,
[00:13:11.240 --> 00:13:16.320]   there's a concept of Big Bang for the start of the universe,
[00:13:16.320 --> 00:13:20.480]   a silly name for one of the most incredible mysteries
[00:13:20.480 --> 00:13:22.320]   of our human existence.
[00:13:22.320 --> 00:13:25.240]   Same way, self-play is one of the silliest names
[00:13:25.240 --> 00:13:27.160]   for one of the most powerful ideas
[00:13:27.160 --> 00:13:28.640]   in artificial intelligence.
[00:13:28.640 --> 00:13:30.960]   It's the mechanism behind alpha zero.
[00:13:30.960 --> 00:13:32.960]   It's a system playing against itself
[00:13:32.960 --> 00:13:36.360]   to improve continuously without any human supervision.
[00:13:36.360 --> 00:13:38.440]   That is the most exciting aspect,
[00:13:38.440 --> 00:13:41.480]   the most exciting area that I'm excited
[00:13:41.480 --> 00:13:44.880]   and I recommend if you love learning that you explore.
[00:13:44.880 --> 00:13:49.680]   So the open problems in artificial intelligence
[00:13:49.680 --> 00:13:50.920]   and possible solutions.
[00:13:50.920 --> 00:13:53.160]   And one of the things, and I'll focus on number four,
[00:13:53.160 --> 00:13:57.160]   which is something that is my dream,
[00:13:57.160 --> 00:14:02.160]   that is sort of my life aspiration,
[00:14:02.160 --> 00:14:04.040]   but I'll give a whirlwind introduction.
[00:14:04.040 --> 00:14:08.360]   Learning to understand, learning to act, reason,
[00:14:08.360 --> 00:14:11.880]   and a deep connection between humans and AI systems.
[00:14:11.880 --> 00:14:13.040]   So learning to understand,
[00:14:13.040 --> 00:14:15.480]   there's a lot of exciting possibilities here.
[00:14:15.480 --> 00:14:18.320]   This is a lot of the breakthroughs in machine learning
[00:14:18.320 --> 00:14:21.360]   have been in something called supervised learning,
[00:14:21.360 --> 00:14:24.640]   where you have a set of data and you have a neural network
[00:14:24.640 --> 00:14:28.000]   or a model that's able to learn from that data
[00:14:28.000 --> 00:14:31.080]   in order to generalize sufficiently to infer
[00:14:31.080 --> 00:14:33.600]   on cases it hasn't seen before.
[00:14:33.600 --> 00:14:35.960]   You could recognize cat versus dog.
[00:14:35.960 --> 00:14:37.720]   In the case of domain,
[00:14:37.720 --> 00:14:39.760]   in the domains of like autonomous driving,
[00:14:39.760 --> 00:14:41.920]   you can recognize lane markings,
[00:14:41.920 --> 00:14:44.320]   you could recognize other vehicles, pedestrians,
[00:14:44.320 --> 00:14:47.040]   all the different subtasks involved
[00:14:47.040 --> 00:14:49.360]   in solving a particular problem.
[00:14:49.360 --> 00:14:53.000]   Now that's all good, but to solve real world problems,
[00:14:53.000 --> 00:14:55.640]   you have to actually, you have to deal
[00:14:55.640 --> 00:14:58.640]   with endless edge cases that we human beings
[00:14:58.640 --> 00:15:01.760]   effortlessly take care of,
[00:15:01.760 --> 00:15:05.040]   that our ability to do reasoning and common sense reasoning
[00:15:05.040 --> 00:15:06.520]   effortlessly takes care of.
[00:15:06.520 --> 00:15:08.320]   So to be able to learn over those edge cases,
[00:15:08.320 --> 00:15:10.920]   you have to do much larger scale learning.
[00:15:10.920 --> 00:15:13.600]   And for that, you have to be much more selective
[00:15:13.600 --> 00:15:16.720]   and clever about which data you annotate with human beings.
[00:15:16.720 --> 00:15:18.680]   And that's the idea of active learning.
[00:15:18.680 --> 00:15:22.600]   Same way with, as children, we explore the world,
[00:15:22.600 --> 00:15:25.520]   we interact with the world to pick up the lessons from it.
[00:15:25.520 --> 00:15:28.560]   The same way you can interact with a dataset
[00:15:28.560 --> 00:15:31.520]   to select only small parts of it to learn from.
[00:15:31.520 --> 00:15:34.000]   And I'll take Tesla, which is a car company
[00:15:34.000 --> 00:15:38.240]   that's using autonomous driving and its system autopilot
[00:15:38.240 --> 00:15:40.560]   that uses deep learning to learn
[00:15:40.560 --> 00:15:43.400]   how to solve all these different problems.
[00:15:43.400 --> 00:15:45.200]   I'll use them as a case study.
[00:15:45.200 --> 00:15:47.040]   What they're doing is quite interesting
[00:15:47.040 --> 00:15:49.120]   in the space of active learning.
[00:15:49.120 --> 00:15:51.880]   They're creating a pipeline for each individual task.
[00:15:51.880 --> 00:15:54.560]   They take the task of driving and break it apart
[00:15:54.560 --> 00:15:57.080]   into now over a hundred different subtasks.
[00:15:59.200 --> 00:16:04.200]   Each subtask gets its own pipeline, its own dataset.
[00:16:04.200 --> 00:16:06.240]   And there's a machine learning system
[00:16:06.240 --> 00:16:07.840]   that learns from that dataset
[00:16:07.840 --> 00:16:12.000]   and is then deployed back into the vehicles.
[00:16:12.000 --> 00:16:15.440]   And when the vehicle fails in a particular case,
[00:16:15.440 --> 00:16:19.400]   that's an edge case that's marked for the system
[00:16:19.400 --> 00:16:22.800]   and is brought back to the pipeline to annotate.
[00:16:22.800 --> 00:16:27.320]   So there's ongoing pipeline that continuously goes on.
[00:16:27.320 --> 00:16:29.280]   The system is not very good in the beginning,
[00:16:29.280 --> 00:16:32.920]   but the whole purpose of it is to discover edge cases.
[00:16:32.920 --> 00:16:37.160]   In the same way that us humans learn something,
[00:16:37.160 --> 00:16:40.880]   and you can think of our actually existence in the world
[00:16:40.880 --> 00:16:44.600]   as an edge case discovery mechanism.
[00:16:44.600 --> 00:16:46.080]   So you learn something,
[00:16:46.080 --> 00:16:48.280]   you construct a mental model of the world,
[00:16:48.280 --> 00:16:49.640]   and you move about the world
[00:16:49.640 --> 00:16:52.680]   until you run up against a case,
[00:16:52.680 --> 00:16:55.200]   a situation that you totally didn't expect.
[00:16:55.200 --> 00:16:59.080]   And we do that thousands of times a day still,
[00:16:59.080 --> 00:17:00.200]   and we learn from those.
[00:17:00.200 --> 00:17:02.960]   And that pipeline of active learning
[00:17:02.960 --> 00:17:04.760]   is a really exciting area
[00:17:04.760 --> 00:17:06.400]   that very few people are working on,
[00:17:06.400 --> 00:17:08.320]   especially in the space of research.
[00:17:08.320 --> 00:17:11.400]   To me, that's the most exciting
[00:17:11.400 --> 00:17:14.840]   in terms of scale impact area in the next few years.
[00:17:14.840 --> 00:17:17.880]   Learning to act,
[00:17:17.880 --> 00:17:20.920]   the second set of open problems in artificial intelligence.
[00:17:20.920 --> 00:17:23.560]   This is where the idea of self-play comes in,
[00:17:24.440 --> 00:17:27.560]   is learning to build systems,
[00:17:27.560 --> 00:17:29.720]   whether through a reinforcement learning mechanism
[00:17:29.720 --> 00:17:33.920]   or otherwise, that are actually acting in the world.
[00:17:33.920 --> 00:17:36.000]   In the case of self-play,
[00:17:36.000 --> 00:17:38.880]   the idea is that you have a really dumb system
[00:17:38.880 --> 00:17:40.600]   in the beginning that knows nothing.
[00:17:40.600 --> 00:17:43.120]   Again, no human supervision.
[00:17:43.120 --> 00:17:46.280]   And through randomization,
[00:17:46.280 --> 00:17:48.560]   you have other systems that also know nothing,
[00:17:48.560 --> 00:17:50.840]   but know a different set of nothing.
[00:17:50.840 --> 00:17:52.360]   And they compete against each other.
[00:17:52.360 --> 00:17:56.120]   So you formulate the problem as a competitive setting.
[00:17:56.120 --> 00:17:58.680]   And when you have two dumb systems
[00:17:58.680 --> 00:18:00.440]   that compete against each other,
[00:18:00.440 --> 00:18:02.440]   a magical thing happens.
[00:18:02.440 --> 00:18:05.520]   The one that's slightly less dumb starts winning.
[00:18:05.520 --> 00:18:11.280]   And this little incremental step
[00:18:11.280 --> 00:18:13.320]   can be repeated arbitrarily
[00:18:13.320 --> 00:18:16.680]   and without any constraints
[00:18:16.680 --> 00:18:20.240]   on human supervision, annotation costs,
[00:18:20.240 --> 00:18:22.320]   without any constraints on having to
[00:18:22.320 --> 00:18:24.920]   sort of bring the human in the loop
[00:18:24.920 --> 00:18:26.280]   or bring the physical world in the loop.
[00:18:26.280 --> 00:18:29.240]   It can all be done in computation in a distributed sense.
[00:18:29.240 --> 00:18:32.400]   So you can, in a matter of hours
[00:18:32.400 --> 00:18:34.160]   on a distributed compute setting,
[00:18:34.160 --> 00:18:37.480]   create a system that beats the world champion at go.
[00:18:37.480 --> 00:18:39.840]   And in fact, with DeepMind and all the games
[00:18:39.840 --> 00:18:42.720]   that have they've defeated the world champion in chess,
[00:18:42.720 --> 00:18:44.440]   not just the world champion,
[00:18:44.440 --> 00:18:48.800]   is the best chess playing program, Stockfish,
[00:18:48.800 --> 00:18:51.520]   in a matter of hours of training.
[00:18:51.520 --> 00:18:54.680]   And the ceiling hasn't yet been reached.
[00:18:54.680 --> 00:18:56.720]   This is both the exciting and the scary thing
[00:18:56.720 --> 00:19:00.160]   about self-play is very few times
[00:19:00.160 --> 00:19:02.840]   is the ceiling ever reached.
[00:19:02.840 --> 00:19:06.720]   What we hit is the limits of our computational power,
[00:19:06.720 --> 00:19:08.840]   which is computation power,
[00:19:08.840 --> 00:19:11.880]   especially the kind of mechanisms that are happening now,
[00:19:11.880 --> 00:19:13.880]   developments happening now.
[00:19:13.880 --> 00:19:16.440]   The Moore's law is continuing in many ways.
[00:19:16.440 --> 00:19:18.400]   So computation, if you just wait a few years,
[00:19:18.400 --> 00:19:19.480]   computation is increasing.
[00:19:19.480 --> 00:19:23.960]   So we were yet to see the ceiling of the capabilities
[00:19:23.960 --> 00:19:26.400]   that these approaches are able to achieve.
[00:19:26.400 --> 00:19:29.320]   This should be both exciting and terrifying.
[00:19:29.320 --> 00:19:32.160]   Okay, the total biggest open problem
[00:19:32.160 --> 00:19:34.080]   that nobody even knows how to do.
[00:19:34.080 --> 00:19:40.800]   This is an example of a state-of-the-art
[00:19:40.800 --> 00:19:47.080]   dog intelligence system solving a particular problem.
[00:19:48.120 --> 00:19:53.120]   So we know nothing how to do reasoning systems
[00:19:53.120 --> 00:19:54.840]   in artificial intelligence.
[00:19:54.840 --> 00:19:58.760]   This is the actually not very often talked about area
[00:19:58.760 --> 00:20:01.760]   because nobody knows what to do about it.
[00:20:01.760 --> 00:20:06.120]   There's been subsets called program synthesis,
[00:20:06.120 --> 00:20:10.120]   communities that kind of try to formulate a subset
[00:20:10.120 --> 00:20:11.920]   of the reasoning problem and try to solve it,
[00:20:11.920 --> 00:20:13.720]   but we don't know much to do,
[00:20:13.720 --> 00:20:16.320]   particularly common sense reasoning,
[00:20:16.320 --> 00:20:18.480]   how to formulate enough about the world
[00:20:18.480 --> 00:20:21.160]   to be able to reason about the physics of the world,
[00:20:21.160 --> 00:20:24.120]   about the basic, especially with human beings,
[00:20:24.120 --> 00:20:27.600]   human to human, human to physical world dynamics.
[00:20:27.600 --> 00:20:30.080]   Just there's millions of facts seemingly
[00:20:30.080 --> 00:20:35.080]   that are intricately connected that we learn
[00:20:35.080 --> 00:20:38.480]   and we accumulate in a knowledge base.
[00:20:38.480 --> 00:20:42.560]   This process is a really exciting area of research
[00:20:42.560 --> 00:20:44.280]   that nobody knows what to do with.
[00:20:45.640 --> 00:20:47.360]   The things I've described previously
[00:20:47.360 --> 00:20:51.160]   don't really have anything to do with humans necessarily.
[00:20:51.160 --> 00:20:56.160]   The by-passion in my interest is that space
[00:20:56.160 --> 00:20:58.800]   between machine and human.
[00:20:58.800 --> 00:21:01.320]   The community broadly could be called
[00:21:01.320 --> 00:21:03.120]   human-robot interaction,
[00:21:03.120 --> 00:21:05.160]   but there's a lot of different areas
[00:21:05.160 --> 00:21:09.880]   in which there's a deep connection
[00:21:09.880 --> 00:21:11.120]   between the human and machine
[00:21:11.120 --> 00:21:13.200]   that you all experience every day.
[00:21:13.200 --> 00:21:15.800]   So recommender systems from Netflix
[00:21:15.800 --> 00:21:18.760]   to much more importantly, social networks,
[00:21:18.760 --> 00:21:21.320]   the recommendation engines behind social networks,
[00:21:21.320 --> 00:21:23.000]   recommending what you see next
[00:21:23.000 --> 00:21:24.280]   in terms of both advertisement
[00:21:24.280 --> 00:21:29.280]   and about the content of your friends that you see,
[00:21:29.280 --> 00:21:33.280]   which friends you get to see more from.
[00:21:33.280 --> 00:21:37.600]   The personalization of IOT, of smart systems,
[00:21:37.600 --> 00:21:41.600]   semi-autonomous systems like Tesla Autopilot
[00:21:41.600 --> 00:21:44.240]   and different semi-autonomous vehicles
[00:21:44.240 --> 00:21:47.000]   like the Cadillac Super Cruise systems.
[00:21:47.000 --> 00:21:51.600]   Whenever you have AI systems between you and a machine.
[00:21:51.600 --> 00:21:53.160]   So there's a machine that does,
[00:21:53.160 --> 00:21:55.000]   that automates some particular task.
[00:21:55.000 --> 00:21:58.440]   There's you human that are tasked with sitting there
[00:21:58.440 --> 00:21:59.960]   and supervising the machine.
[00:21:59.960 --> 00:22:03.680]   And there is an AI system in the middle that manages that.
[00:22:03.680 --> 00:22:05.880]   It manages the tension, the dance, the uncertainty,
[00:22:05.880 --> 00:22:09.360]   the human, all the T word, the trust,
[00:22:09.360 --> 00:22:12.680]   all the mess of human beings, it manages that.
[00:22:12.680 --> 00:22:15.680]   That's a really exciting space
[00:22:15.680 --> 00:22:18.520]   that is in the very early days.
[00:22:18.520 --> 00:22:23.520]   What I show there is where my sense is, where we stand.
[00:22:23.520 --> 00:22:28.720]   In 1998, there was a lot of search engines.
[00:22:28.720 --> 00:22:31.720]   Some of you may even be old enough to have used them.
[00:22:31.720 --> 00:22:35.360]   AltaVista, Excite, AskG is like us and so on.
[00:22:35.360 --> 00:22:38.680]   Then Google came along, the Google search engine
[00:22:38.680 --> 00:22:40.440]   and blew them all out of the water.
[00:22:40.440 --> 00:22:43.240]   They were all working on a very interesting,
[00:22:43.240 --> 00:22:46.960]   very important problem, but the approach
[00:22:46.960 --> 00:22:50.000]   and the fundamental ideas behind their approach was flawed.
[00:22:50.000 --> 00:22:54.820]   I believe that personal assistance
[00:22:54.820 --> 00:22:57.760]   and a personal deep, meaningful connection
[00:22:57.760 --> 00:23:01.520]   between an AI system and a human being
[00:23:01.520 --> 00:23:03.200]   that's exactly where we're at.
[00:23:03.200 --> 00:23:05.920]   Many people have in their home an Alexa device,
[00:23:05.920 --> 00:23:07.280]   a Google home device.
[00:23:08.280 --> 00:23:11.600]   But most people don't use it for almost anything
[00:23:11.600 --> 00:23:13.700]   except to play music or check the weather.
[00:23:13.700 --> 00:23:18.720]   Many of you use Twitter and social networks,
[00:23:18.720 --> 00:23:21.640]   but artificial intelligence plays a minimal role
[00:23:21.640 --> 00:23:24.360]   and understands almost nothing about you
[00:23:24.360 --> 00:23:27.640]   in recommending how you interact with the platform
[00:23:27.640 --> 00:23:29.940]   or the advertisements you see.
[00:23:29.940 --> 00:23:33.400]   And autonomous vehicles, robotics platforms
[00:23:33.400 --> 00:23:36.280]   know almost nothing about you.
[00:23:36.280 --> 00:23:38.920]   So shown there is the Tesla vehicle.
[00:23:38.920 --> 00:23:41.860]   It knows almost nothing about you
[00:23:41.860 --> 00:23:44.800]   except whether your hands are on the steering wheel or not.
[00:23:44.800 --> 00:23:50.000]   I believe it'll be obvious in retrospect
[00:23:50.000 --> 00:23:54.160]   how much opportunity there is to learn about human beings
[00:23:54.160 --> 00:23:56.280]   from the devices and from that
[00:23:56.280 --> 00:23:58.180]   to form a deep, meaningful connection.
[00:23:58.180 --> 00:24:03.000]   So now to return to my valley of despair
[00:24:03.000 --> 00:24:06.120]   to give some words of advice.
[00:24:06.120 --> 00:24:08.760]   And again, take them with a grain of salt.
[00:24:08.760 --> 00:24:14.800]   So in this context, in this optimization context,
[00:24:14.800 --> 00:24:19.240]   my first piece of advice is to listen to your inner voice.
[00:24:19.240 --> 00:24:20.880]   I think a lot of people,
[00:24:20.880 --> 00:24:24.200]   including a lot of very smart professors, advisors,
[00:24:24.200 --> 00:24:29.200]   parents, friends, significant others,
[00:24:29.200 --> 00:24:34.360]   have in them a kind of mutually agreed upon gradient
[00:24:34.560 --> 00:24:36.060]   along which they push you.
[00:24:36.060 --> 00:24:42.320]   It's so difficult for me to articulate this in a clear way.
[00:24:42.320 --> 00:24:46.960]   But early on, I heard within myself
[00:24:46.960 --> 00:24:52.340]   a silly sounding, crazy voice that told me to do things.
[00:24:52.340 --> 00:24:57.680]   One of which was to try to put a robot in every home.
[00:24:57.680 --> 00:25:00.840]   There's dreams that are difficult for me to articulate.
[00:25:00.840 --> 00:25:03.840]   But if you allow your mind to be quiet enough,
[00:25:03.840 --> 00:25:07.480]   you'll hear such voices, you'll hear such dreams.
[00:25:07.480 --> 00:25:12.480]   And it's important to really listen and to pursue them.
[00:25:12.480 --> 00:25:17.840]   Advice number two is carve your own path.
[00:25:17.840 --> 00:25:23.880]   And if that means taking a few detours, take the detours.
[00:25:23.880 --> 00:25:27.780]   Again, this is coming from the valley of despair.
[00:25:27.780 --> 00:25:31.200]   (audience laughing)
[00:25:31.200 --> 00:25:34.700]   So I hope this pans out in the end.
[00:25:34.700 --> 00:25:36.640]   But I had many detours.
[00:25:36.640 --> 00:25:39.400]   In music, I was in a band, I had long hair.
[00:25:39.400 --> 00:25:48.440]   I gave a lot of myself to the practice of martial arts.
[00:25:48.440 --> 00:25:53.360]   And both music and martial arts have given me,
[00:25:53.360 --> 00:25:55.140]   again, very difficult to put into words,
[00:25:55.140 --> 00:25:59.060]   but it have given me something quite profound.
[00:25:59.060 --> 00:26:03.480]   It gave flavor and color to the pursuit of that dream
[00:26:03.480 --> 00:26:04.960]   that's hard to articulate.
[00:26:04.960 --> 00:26:07.280]   It's because I listened to my instinct,
[00:26:07.280 --> 00:26:09.600]   listened to my heart in pursuing these detours.
[00:26:09.600 --> 00:26:13.860]   From poetry to excessive reading, like I mentioned,
[00:26:13.860 --> 00:26:16.400]   I took a James Joyce course here.
[00:26:16.400 --> 00:26:19.840]   So pursuing these avenues of knowledge
[00:26:19.840 --> 00:26:21.020]   through philosophy and history
[00:26:21.020 --> 00:26:24.200]   that seemingly have nothing to do with the main pursuit.
[00:26:24.200 --> 00:26:29.200]   And starting the silliest of pursuits, starting a podcast.
[00:26:29.200 --> 00:26:35.780]   Advice number three is to measure passion, not progress.
[00:26:35.780 --> 00:26:42.280]   So most of us get an average of about 27,000 days of life.
[00:26:42.280 --> 00:26:47.000]   I think a good metric by which you should live
[00:26:47.000 --> 00:26:49.800]   is to maximize the number of those days
[00:26:49.800 --> 00:26:53.400]   that are filled with a passionate pursuit of something.
[00:26:53.400 --> 00:26:55.780]   Not by how much you've progressed
[00:26:55.780 --> 00:26:58.020]   towards a particular goal.
[00:26:58.020 --> 00:27:00.040]   Because goals are grounded in your comparison
[00:27:00.040 --> 00:27:01.640]   to other human beings,
[00:27:01.640 --> 00:27:04.880]   to something that's already been done before.
[00:27:04.880 --> 00:27:08.320]   Passionate pursuit of something
[00:27:08.320 --> 00:27:10.740]   is the way you achieve something totally new.
[00:27:10.740 --> 00:27:17.380]   And a quick warning about passion.
[00:27:17.380 --> 00:27:19.760]   Again, I'm a little bit of Russian,
[00:27:19.760 --> 00:27:21.720]   so maybe I romanticize this whole suffering
[00:27:21.720 --> 00:27:22.680]   and passion thing.
[00:27:22.920 --> 00:27:24.960]   (audience laughing)
[00:27:24.960 --> 00:27:26.960]   But the people who love you,
[00:27:26.960 --> 00:27:28.640]   the people who care for you,
[00:27:28.640 --> 00:27:32.680]   like I mentioned, your friends, your family,
[00:27:32.680 --> 00:27:35.200]   should not be trusted.
[00:27:35.200 --> 00:27:41.160]   Accept their love, but not their advice.
[00:27:41.160 --> 00:27:43.960]   Parents and significant others
[00:27:43.960 --> 00:27:45.880]   will tell you to find a secure job
[00:27:45.880 --> 00:27:48.780]   because passion looks dangerous.
[00:27:48.780 --> 00:27:51.720]   It looks insecure.
[00:27:51.720 --> 00:27:54.600]   Advisors, colleagues will tell you to be pragmatic
[00:27:54.600 --> 00:27:57.120]   because passion looks like a distraction
[00:27:57.120 --> 00:28:00.440]   from the main effort that you should be focusing on.
[00:28:00.440 --> 00:28:05.260]   And society will tell you to find balance,
[00:28:05.260 --> 00:28:07.660]   work-life balance in your life
[00:28:07.660 --> 00:28:10.800]   because passion looks unhealthy.
[00:28:10.800 --> 00:28:14.640]   Advice number four,
[00:28:14.640 --> 00:28:18.180]   continuing on the unhealthy part,
[00:28:18.180 --> 00:28:19.920]   is work hard.
[00:28:19.920 --> 00:28:23.960]   Make a habit of working hard every day,
[00:28:23.960 --> 00:28:26.560]   putting in the hours.
[00:28:26.560 --> 00:28:30.560]   There's a lot of books and a lot of advice
[00:28:30.560 --> 00:28:33.440]   that have been written on working smart
[00:28:33.440 --> 00:28:34.700]   and not working hard.
[00:28:34.700 --> 00:28:38.320]   I'm yet to meet anyone
[00:28:38.320 --> 00:28:42.160]   who has not truly worked hard for thousands of hours
[00:28:42.160 --> 00:28:44.120]   in order to accomplish something great.
[00:28:44.120 --> 00:28:47.640]   In order to work smart,
[00:28:47.640 --> 00:28:50.160]   you first have to put in those few tens of thousands
[00:28:50.160 --> 00:28:52.400]   of hours of really dumb, brute force,
[00:28:52.400 --> 00:28:54.000]   hard work of all-nighters.
[00:28:54.000 --> 00:28:58.480]   The key there is to minimize stress,
[00:28:58.480 --> 00:29:02.000]   not to minimize the amount of hours of work.
[00:29:02.000 --> 00:29:07.280]   And to do that is you have to love what you do.
[00:29:07.280 --> 00:29:09.000]   And the final piece of advice,
[00:29:09.000 --> 00:29:12.120]   I love that picture, okay,
[00:29:12.120 --> 00:29:13.840]   is to look up to the stars
[00:29:13.840 --> 00:29:17.400]   and appreciate every single moment you're alive.
[00:29:17.400 --> 00:29:19.240]   At the mystery of this world,
[00:29:19.240 --> 00:29:21.100]   at the beauty of this world.
[00:29:21.100 --> 00:29:24.200]   Again, this is my perspective,
[00:29:24.200 --> 00:29:26.100]   take it with a grain of salt,
[00:29:26.100 --> 00:29:28.200]   but I advise to forever oscillate
[00:29:28.200 --> 00:29:33.200]   between deep, profound doubt and self-dissatisfaction
[00:29:33.200 --> 00:29:37.360]   and a deep gratitude for the moment,
[00:29:37.360 --> 00:29:38.960]   for just being alive,
[00:29:38.960 --> 00:29:42.160]   for all the people around you that give you their love,
[00:29:42.160 --> 00:29:44.280]   with whom you get to share those moments
[00:29:44.280 --> 00:29:45.220]   and share the love.
[00:29:46.280 --> 00:29:47.800]   A poem by Stephen Crane
[00:29:47.800 --> 00:29:49.680]   that I especially like in the desert.
[00:29:49.680 --> 00:29:54.000]   In the desert, I saw a creature, a naked bestial,
[00:29:54.000 --> 00:29:55.480]   who squatting up on the ground,
[00:29:55.480 --> 00:29:57.720]   held his heart in his hands and ate of it.
[00:29:57.720 --> 00:30:01.640]   I said, "Is it good, friend?"
[00:30:01.640 --> 00:30:04.000]   "It is bitter."
[00:30:04.000 --> 00:30:05.900]   "Bitter," he answered.
[00:30:05.900 --> 00:30:09.280]   "But I like it, because it is bitter,
[00:30:09.280 --> 00:30:11.040]   "and because it is my heart."
[00:30:11.040 --> 00:30:15.360]   So I would say the bitter is the self-dissatisfaction,
[00:30:15.360 --> 00:30:19.120]   and that's the restless energy that drives us forward.
[00:30:19.120 --> 00:30:22.120]   And then enjoying that bitterness
[00:30:22.120 --> 00:30:24.400]   and enjoying the moment
[00:30:24.400 --> 00:30:27.000]   and enjoying the sweetness that comes
[00:30:27.000 --> 00:30:31.600]   from eating your own heart in this poem
[00:30:31.600 --> 00:30:37.600]   is a thing that makes life worthwhile.
[00:30:37.600 --> 00:30:42.540]   And that is, to me, happiness.
[00:30:44.060 --> 00:30:47.960]   So with those silly few pieces of advice,
[00:30:47.960 --> 00:30:52.160]   I'd like to continue on the gratitude and say thank you.
[00:30:52.160 --> 00:30:55.440]   Thank you to my advisor.
[00:30:55.440 --> 00:30:57.840]   Thank you to this university
[00:30:57.840 --> 00:31:00.320]   for giving me a helping hand.
[00:31:00.320 --> 00:31:01.680]   There you go.
[00:31:01.680 --> 00:31:03.400]   And thank you to my family
[00:31:03.400 --> 00:31:06.120]   and all the friends that I've had along the way.
[00:31:06.120 --> 00:31:07.200]   Thank you for their love.
[00:31:07.200 --> 00:31:08.040]   I appreciate it.
[00:31:08.040 --> 00:31:11.200]   (audience applauding)
[00:31:12.720 --> 00:31:14.580]   I've never been introduced with this much energy.
[00:31:14.580 --> 00:31:15.680]   I really appreciate it.
[00:31:15.680 --> 00:31:18.260]   (audience laughing)
[00:31:18.260 --> 00:31:20.060]   - You're hanging out at the wrong places, man.
[00:31:20.060 --> 00:31:21.420]   (laughing)
[00:31:21.420 --> 00:31:22.260]   - Yes.
[00:31:22.260 --> 00:31:25.820]   - First of all, great to see you in person, Dr. Kristine.
[00:31:25.820 --> 00:31:27.100]   Big fan of your lectures,
[00:31:27.100 --> 00:31:29.420]   big fan of your show, "Tell Me" podcast.
[00:31:29.420 --> 00:31:30.580]   Just listening to your conversation
[00:31:30.580 --> 00:31:31.680]   on these phones this morning,
[00:31:31.680 --> 00:31:33.700]   my way to my phone.
[00:31:33.700 --> 00:31:35.140]   My question for you was,
[00:31:35.140 --> 00:31:38.460]   is your perspective in any way influenced
[00:31:38.460 --> 00:31:40.700]   by the ultimate being-less-ness of it all?
[00:31:40.700 --> 00:31:42.380]   (laughing)
[00:31:42.380 --> 00:31:44.500]   - By the way, thank you for that question.
[00:31:44.500 --> 00:31:47.000]   How is your daily life affected
[00:31:47.000 --> 00:31:48.940]   by the meaninglessness of it all?
[00:31:48.940 --> 00:31:51.940]   (audience laughing)
[00:31:51.940 --> 00:31:57.500]   So the answer is yes.
[00:31:57.500 --> 00:32:00.700]   And it's hard to use reason to justify
[00:32:00.700 --> 00:32:02.100]   that life is meaningful.
[00:32:02.100 --> 00:32:03.740]   I think you have to listen to,
[00:32:03.740 --> 00:32:09.500]   there's something in you that makes life beautiful.
[00:32:09.500 --> 00:32:11.580]   So if you look at somebody like Elon Musk,
[00:32:11.580 --> 00:32:14.900]   he believes that interplanetary,
[00:32:14.900 --> 00:32:16.460]   so colonizing Mars,
[00:32:16.460 --> 00:32:21.820]   that's one of the most exciting things
[00:32:21.820 --> 00:32:23.320]   we human beings can do.
[00:32:23.320 --> 00:32:26.180]   And so if you allow yourself to think,
[00:32:26.180 --> 00:32:28.320]   what is the most exciting thing
[00:32:28.320 --> 00:32:29.940]   that we human beings can do?
[00:32:29.940 --> 00:32:35.460]   And see that the work you're doing is part of that.
[00:32:35.460 --> 00:32:40.260]   For me, if I were to psychoanalyze myself,
[00:32:40.260 --> 00:32:43.660]   there's something in me that's deeply fulfilling
[00:32:43.660 --> 00:32:47.300]   about creating intelligent systems.
[00:32:47.300 --> 00:32:50.180]   That's so exciting to me,
[00:32:50.180 --> 00:32:53.140]   that we human beings can create intelligent systems.
[00:32:53.140 --> 00:32:54.580]   I see artificial intelligence
[00:32:54.580 --> 00:32:57.900]   as the next evolution of human civilization.
[00:32:57.900 --> 00:33:01.820]   And to me, that makes it somehow deeply exciting,
[00:33:01.820 --> 00:33:05.300]   even though eventually the whole universe
[00:33:05.300 --> 00:33:06.340]   will collapse on itself
[00:33:06.340 --> 00:33:10.620]   or the other cold death of the universe.
[00:33:10.620 --> 00:33:13.060]   There's something within that that's so exciting.
[00:33:13.060 --> 00:33:16.860]   - There was an interview with Elon Musk
[00:33:16.860 --> 00:33:20.660]   and he basically said that we're in a civilization,
[00:33:20.660 --> 00:33:23.980]   so this might not be actual reality.
[00:33:23.980 --> 00:33:25.340]   What's your take on that?
[00:33:25.340 --> 00:33:27.220]   - So my first take is,
[00:33:27.220 --> 00:33:30.460]   I love it how much fellow colleagues
[00:33:30.460 --> 00:33:33.820]   and scientists are uncomfortable with this question.
[00:33:33.820 --> 00:33:34.900]   So I love it.
[00:33:34.900 --> 00:33:37.700]   I love to ask it just 'cause it makes them uncomfortable.
[00:33:37.700 --> 00:33:39.900]   (audience laughing)
[00:33:39.900 --> 00:33:41.780]   Yeah, I appreciate it.
[00:33:41.780 --> 00:33:48.660]   It's a good, I don't know, maybe in French cuisine,
[00:33:48.660 --> 00:33:50.860]   you have to cleanse the palate.
[00:33:50.860 --> 00:33:52.660]   It's a good question to ask.
[00:33:52.660 --> 00:33:55.980]   We're not now talking about the latest paper.
[00:33:55.980 --> 00:33:58.620]   We're now talking about the bigger questions of life.
[00:33:58.620 --> 00:34:02.680]   The simulation question is a nice one to do that.
[00:34:02.680 --> 00:34:05.180]   In terms of actually practically,
[00:34:05.180 --> 00:34:09.500]   I think there's two interesting things to say.
[00:34:09.500 --> 00:34:12.620]   So one, it's interesting to me,
[00:34:12.620 --> 00:34:14.260]   I'm a big fan of virtual reality.
[00:34:14.260 --> 00:34:19.140]   I love entering worlds,
[00:34:19.140 --> 00:34:21.580]   even primitive as they are now that are virtual.
[00:34:21.580 --> 00:34:24.420]   I can already imagine that more and more people
[00:34:24.420 --> 00:34:25.940]   would wanna live in those worlds.
[00:34:25.940 --> 00:34:27.820]   It's an interesting question to me,
[00:34:27.820 --> 00:34:30.300]   how real do those worlds need to become
[00:34:30.300 --> 00:34:32.100]   in order for you to wanna stay there
[00:34:32.100 --> 00:34:34.120]   and not return to the real world?
[00:34:34.120 --> 00:34:36.080]   So the question of the simulation is,
[00:34:36.080 --> 00:34:38.720]   how real do we need to simulate the world
[00:34:38.720 --> 00:34:41.220]   in order for you to enjoy it better than this one?
[00:34:41.220 --> 00:34:43.660]   That's a computer science question.
[00:34:43.660 --> 00:34:44.500]   That's really interesting.
[00:34:44.500 --> 00:34:45.980]   That's a, it's a,
[00:34:45.980 --> 00:34:48.100]   it's like practical engineering question
[00:34:48.100 --> 00:34:49.820]   'cause you can create virtual reality systems
[00:34:49.820 --> 00:34:51.880]   that'll make a lot of money,
[00:34:51.880 --> 00:34:53.860]   perhaps have a detrimental effect on society
[00:34:53.860 --> 00:34:56.860]   by having people wanna stay in the virtual worlds.
[00:34:56.860 --> 00:34:59.580]   And then the other question is the physics question
[00:34:59.580 --> 00:35:01.380]   of quantum mechanics of,
[00:35:01.380 --> 00:35:04.300]   like what is the fundamental fabric of reality?
[00:35:04.300 --> 00:35:05.220]   And is it,
[00:35:05.220 --> 00:35:10.340]   what does it take to simulate that reality?
[00:35:10.340 --> 00:35:12.500]   And that's like a physics question.
[00:35:12.500 --> 00:35:15.000]   How, is it finite, is it infinite?
[00:35:15.000 --> 00:35:18.220]   What are the mechanisms, the underlying mechanisms?
[00:35:18.220 --> 00:35:19.620]   Does it go as low as string theory?
[00:35:19.620 --> 00:35:21.420]   Does it go below string theory?
[00:35:21.420 --> 00:35:24.220]   And there's actually people that written papers
[00:35:24.220 --> 00:35:26.180]   on how big a computer needs to be
[00:35:26.180 --> 00:35:28.860]   in order to simulate that kind of system.
[00:35:28.860 --> 00:35:31.100]   And now quantum computers are coming forward,
[00:35:31.100 --> 00:35:33.060]   which is one of the exciting applications
[00:35:33.060 --> 00:35:34.860]   of quantum computing is to be able
[00:35:34.860 --> 00:35:37.780]   to simulate quantum mechanical systems.
[00:35:37.780 --> 00:35:38.780]   And this is the question,
[00:35:38.780 --> 00:35:40.420]   how big does a quantum computer have to be
[00:35:40.420 --> 00:35:41.680]   to simulate the universe?
[00:35:41.680 --> 00:35:46.180]   It's a fun, but a real physics question,
[00:35:46.180 --> 00:35:48.780]   way out of reach of our engineering capabilities.
[00:35:48.780 --> 00:35:51.340]   But it's just, it's a nice party over the beer,
[00:35:51.340 --> 00:35:55.140]   over beers thing to bring up with scientists.
[00:35:55.140 --> 00:35:59.740]   There's two things that make scientists uncomfortable
[00:35:59.740 --> 00:36:00.940]   that I love bringing up.
[00:36:00.940 --> 00:36:03.820]   One is the simulation question.
[00:36:03.820 --> 00:36:06.940]   And the other is, what do you think about the idea
[00:36:06.940 --> 00:36:08.540]   that's become popular recently
[00:36:08.540 --> 00:36:10.280]   that the earth might be flat?
[00:36:10.280 --> 00:36:12.980]   (audience laughing)
[00:36:12.980 --> 00:36:15.260]   They get really, they get angry actually.
[00:36:15.260 --> 00:36:17.540]   So.
[00:36:17.540 --> 00:36:20.180]   - I wanna say, I appreciate your work
[00:36:20.180 --> 00:36:23.660]   and I love the podcast and stuff like that.
[00:36:23.660 --> 00:36:26.660]   So people talk about athletes and academics
[00:36:26.660 --> 00:36:28.420]   being the greatest of their field.
[00:36:28.420 --> 00:36:29.780]   People consider Jesse Ellens
[00:36:29.780 --> 00:36:32.100]   to be one of the greatest runners of all time,
[00:36:32.100 --> 00:36:34.900]   even though he's quite outpaced by the runners today.
[00:36:34.900 --> 00:36:37.420]   People consider scientists like Isaac Newton
[00:36:37.420 --> 00:36:38.900]   one of the greatest science evers
[00:36:38.900 --> 00:36:40.980]   because of his advancements in classical mechanics
[00:36:40.980 --> 00:36:42.900]   and calculus, which is considered
[00:36:42.900 --> 00:36:44.420]   pretty basic physics nowadays.
[00:36:44.420 --> 00:36:47.260]   What do you define greatness as
[00:36:47.260 --> 00:36:49.620]   when it comes to the pursuit of an endeavor?
[00:36:49.620 --> 00:36:52.320]   Does it involve looking for the most advancements
[00:36:52.320 --> 00:36:54.940]   in the field given your starting point?
[00:36:54.940 --> 00:36:57.820]   Does it come from the journey and the work associated
[00:36:57.820 --> 00:36:59.660]   or the destination?
[00:36:59.660 --> 00:37:01.420]   Is it a personal concept
[00:37:01.420 --> 00:37:04.300]   or is it something you understand across humanity?
[00:37:04.300 --> 00:37:07.380]   - So thank you for that question.
[00:37:07.380 --> 00:37:10.580]   Very well written out and thought out.
[00:37:10.580 --> 00:37:13.020]   There's a personal greatness
[00:37:13.020 --> 00:37:15.540]   from the perspective of the individual for me.
[00:37:15.540 --> 00:37:18.340]   Like for me, greatness is doing what I love.
[00:37:18.340 --> 00:37:21.060]   That ignores the rest of society.
[00:37:21.060 --> 00:37:25.340]   It's just like, to me, I'm the greatest human
[00:37:25.340 --> 00:37:28.500]   to have ever lived in my own little world
[00:37:28.500 --> 00:37:31.020]   for having to do the things I love.
[00:37:31.020 --> 00:37:33.340]   And that's from my perspective.
[00:37:33.340 --> 00:37:36.860]   And I love the craftsmanship of it.
[00:37:36.860 --> 00:37:38.500]   Anything, it could be anything.
[00:37:38.500 --> 00:37:40.940]   It's just doing the skill.
[00:37:40.940 --> 00:37:42.380]   So that's not about accomplishment.
[00:37:42.380 --> 00:37:43.220]   That's not about anything.
[00:37:43.220 --> 00:37:45.540]   That's about just doing the things you love.
[00:37:45.540 --> 00:37:47.100]   From the perspective of society,
[00:37:47.100 --> 00:37:50.980]   they tend to then tell stories about these pursuits.
[00:37:50.980 --> 00:37:54.980]   And they like to, like greatness is something
[00:37:54.980 --> 00:37:56.260]   that people invent.
[00:37:56.260 --> 00:37:59.540]   They give Nobel prize, they give prizes to accomplishment.
[00:37:59.540 --> 00:38:01.900]   They kind of tell stories about human beings,
[00:38:01.900 --> 00:38:06.860]   about Steve Jobs, about different icons.
[00:38:06.860 --> 00:38:09.460]   And some are completely ignored through history.
[00:38:09.460 --> 00:38:13.620]   Some are glorified through history, like over glorified.
[00:38:13.620 --> 00:38:18.620]   I recently found out that the Pythagorean theorem
[00:38:18.620 --> 00:38:23.740]   was not developed by Pythagoras.
[00:38:24.740 --> 00:38:27.180]   But I read it on Wikipedia.
[00:38:27.180 --> 00:38:28.540]   I don't know if it's true.
[00:38:28.540 --> 00:38:32.140]   But that's an example of somebody I at least thought
[00:38:32.140 --> 00:38:34.740]   was kind of an actual entity, an actual human being
[00:38:34.740 --> 00:38:37.660]   that was great and associated with this idea.
[00:38:37.660 --> 00:38:42.660]   So to me, I think greatness is doing the things you love.
[00:38:42.660 --> 00:38:45.740]   And the rest is just luck,
[00:38:45.740 --> 00:38:48.500]   whether they tell a good story about you or not.
[00:38:48.500 --> 00:38:50.740]   - Give it up for our speaker, Dr. Leslie Kuhn.
[00:38:50.740 --> 00:38:53.700]   (audience applauding)
[00:38:53.700 --> 00:38:56.700]   (audience cheering)
[00:38:56.700 --> 00:38:59.700]   (audience cheering)
[00:38:59.700 --> 00:39:02.700]   (audience cheering)
[00:39:02.700 --> 00:39:05.700]   (audience cheering)
[00:39:05.700 --> 00:39:08.700]   (audience cheering)
[00:39:08.700 --> 00:39:11.700]   (audience cheering)
[00:39:11.700 --> 00:39:21.700]   [BLANK_AUDIO]

