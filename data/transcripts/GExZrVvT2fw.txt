
[00:00:00.000 --> 00:00:04.080]   - Let's jump back to it, artificial intelligence.
[00:00:04.080 --> 00:00:06.960]   What's your thought of the state of where we are at
[00:00:06.960 --> 00:00:08.720]   currently with artificial intelligence?
[00:00:08.720 --> 00:00:11.680]   And what do you think it takes to build human level
[00:00:11.680 --> 00:00:13.740]   or superhuman level intelligence?
[00:00:13.740 --> 00:00:16.200]   - I don't know what intelligence means.
[00:00:16.200 --> 00:00:18.620]   That's my biggest question at the moment.
[00:00:18.620 --> 00:00:20.840]   And I think it's 'cause my instinct is always to go,
[00:00:20.840 --> 00:00:23.760]   well, what are the foundations here of our discussion?
[00:00:23.760 --> 00:00:26.840]   What does it mean to be intelligent?
[00:00:26.840 --> 00:00:28.680]   How do we measure the intelligence
[00:00:28.680 --> 00:00:33.240]   of an artificial machine or a program or something?
[00:00:33.240 --> 00:00:35.640]   - Can we say that humans are intelligent?
[00:00:35.640 --> 00:00:38.360]   Because there's also a fascinating field
[00:00:38.360 --> 00:00:40.040]   of how do you measure human intelligence?
[00:00:40.040 --> 00:00:41.160]   - Of course.
[00:00:41.160 --> 00:00:42.840]   - But if we just take that for granted,
[00:00:42.840 --> 00:00:45.800]   saying that whatever this fuzzy intelligence thing
[00:00:45.800 --> 00:00:48.000]   we're talking about, humans kind of have it.
[00:00:48.000 --> 00:00:52.440]   What would be a good test for you?
[00:00:52.440 --> 00:00:54.120]   So Turing developed a test
[00:00:54.120 --> 00:00:56.200]   that's natural language conversation.
[00:00:56.200 --> 00:00:57.360]   Would that impress you?
[00:00:57.360 --> 00:00:59.120]   A chat bot that you'd want to hang out
[00:00:59.120 --> 00:01:02.360]   and have a beer with for a bunch of hours
[00:01:02.360 --> 00:01:04.120]   or have dinner plans with.
[00:01:04.120 --> 00:01:05.960]   Is that a good test, natural language conversation?
[00:01:05.960 --> 00:01:08.000]   Is there something else that would impress you?
[00:01:08.000 --> 00:01:09.400]   Or is that also too difficult to think about?
[00:01:09.400 --> 00:01:11.600]   - Oh yeah, I'm pretty much impressed by everything.
[00:01:11.600 --> 00:01:13.480]   I think that if-
[00:01:13.480 --> 00:01:14.480]   - Roomba?
[00:01:14.480 --> 00:01:17.900]   - If there was a chat bot that was like incredibly,
[00:01:17.900 --> 00:01:20.200]   I don't know, really had a personality.
[00:01:20.200 --> 00:01:23.680]   And if I didn't, the Turing test, right?
[00:01:23.680 --> 00:01:28.680]   Like if I'm unable to tell that it's not another person,
[00:01:28.680 --> 00:01:32.320]   but then I was shown a bunch of wires
[00:01:32.320 --> 00:01:34.840]   and mechanical components,
[00:01:34.840 --> 00:01:38.440]   and it was like, that's actually what you're talking to.
[00:01:38.440 --> 00:01:42.200]   I don't know if I would feel that guilty destroying it.
[00:01:42.200 --> 00:01:45.000]   I would feel guilty because clearly it's well-made
[00:01:45.000 --> 00:01:46.820]   and it's a really cool thing.
[00:01:46.820 --> 00:01:49.560]   It's like destroying a really cool car or something.
[00:01:49.560 --> 00:01:52.040]   But I would not feel like I was a murderer.
[00:01:52.040 --> 00:01:54.560]   So yeah, at what point would I start to feel that way?
[00:01:54.560 --> 00:01:58.440]   And this is such a subjective psychological question.
[00:01:58.440 --> 00:02:00.760]   If you give it movement,
[00:02:00.760 --> 00:02:04.360]   or if you have it act as though,
[00:02:04.360 --> 00:02:07.600]   or perhaps really feel pain as I destroy it
[00:02:07.600 --> 00:02:11.400]   and scream and resist, then I'd feel bad.
[00:02:11.400 --> 00:02:12.440]   - Yeah, it's beautifully put.
[00:02:12.440 --> 00:02:16.280]   And let's just say act like it's a pain.
[00:02:16.280 --> 00:02:21.280]   So if you just have a robot that not screams,
[00:02:21.400 --> 00:02:24.200]   just like moans in pain, if you kick it,
[00:02:24.200 --> 00:02:28.640]   that immediately just puts it in a class that we humans,
[00:02:28.640 --> 00:02:31.040]   it becomes, it anthropomorphizes it.
[00:02:31.040 --> 00:02:33.680]   It almost immediately becomes human.
[00:02:33.680 --> 00:02:35.120]   So that's a psychology question
[00:02:35.120 --> 00:02:36.720]   as opposed to sort of a physics question.
[00:02:36.720 --> 00:02:38.880]   - Right, I think that's a really good instinct to have.
[00:02:38.880 --> 00:02:39.840]   If the robot-
[00:02:39.840 --> 00:02:42.360]   - Screams?
[00:02:42.360 --> 00:02:44.200]   - Screams and moans,
[00:02:44.200 --> 00:02:48.440]   even if you don't believe that it has the mental experience,
[00:02:48.440 --> 00:02:51.080]   the qualia of pain and suffering,
[00:02:51.080 --> 00:02:52.560]   I think it's still a good instinct to say,
[00:02:52.560 --> 00:02:55.760]   you know what, I'd rather not hurt it.
[00:02:55.760 --> 00:02:58.360]   - The problem is that instinct can get us in trouble
[00:02:58.360 --> 00:03:01.360]   because then robots can manipulate that.
[00:03:01.360 --> 00:03:04.040]   And there's different kinds of robots.
[00:03:04.040 --> 00:03:06.440]   There's robots like the Facebook and the YouTube algorithm
[00:03:06.440 --> 00:03:07.720]   that recommends the video,
[00:03:07.720 --> 00:03:10.120]   and they can manipulate it in the same kind of way.
[00:03:10.120 --> 00:03:12.720]   (upbeat music)
[00:03:12.720 --> 00:03:15.320]   (upbeat music)
[00:03:15.320 --> 00:03:17.920]   (upbeat music)
[00:03:17.920 --> 00:03:20.520]   (upbeat music)
[00:03:20.520 --> 00:03:23.120]   (upbeat music)
[00:03:23.120 --> 00:03:25.720]   (upbeat music)
[00:03:25.720 --> 00:03:35.720]   [BLANK_AUDIO]

