
[00:00:00.000 --> 00:00:25.320]   Hey, everybody, welcome back. I've started the live stream 30
[00:00:25.320 --> 00:00:27.840]   seconds early. So I'll just wait for people to join in.
[00:00:27.840 --> 00:00:42.120]   Awesome. I think we're good to go. I'm just double checking to
[00:00:42.120 --> 00:00:52.600]   make sure we're live everywhere. YouTube looks good. Events look
[00:00:52.600 --> 00:00:58.160]   good. Amazing. Hey, everybody, thanks for joining us again.
[00:00:58.160 --> 00:01:02.920]   Welcome back to the session. I actually want to start the
[00:01:02.920 --> 00:01:06.040]   session with a more serious message that I want to get out
[00:01:06.040 --> 00:01:09.880]   to everyone. My father has been admitted to a hospital because
[00:01:09.880 --> 00:01:15.360]   COVID made his blood sugar skyrocket. And I know this
[00:01:15.360 --> 00:01:18.800]   variant is supposed to be mild, but we all work in machine
[00:01:18.800 --> 00:01:22.480]   learning. We're in the top of, I think the world's smartest
[00:01:22.480 --> 00:01:26.560]   people please try to stay indoors. Jeremy Howard made a
[00:01:26.560 --> 00:01:32.560]   video about this. He says it's essential to use N95s. I would
[00:01:32.560 --> 00:01:40.040]   really urge everyone to please try to minimize being in crowded
[00:01:40.040 --> 00:01:43.080]   areas and please consider wearing masks that are high
[00:01:43.080 --> 00:01:48.240]   quality. I just wanted to get this message out there without
[00:01:48.240 --> 00:01:50.680]   making it too serious. I'll just be back in one second.
[00:01:50.680 --> 00:02:00.800]   Amazing. I'll get started now. Thanks for joining us. So we're
[00:02:00.800 --> 00:02:05.520]   aiming I've set an ambitious goal today, like always of going
[00:02:05.520 --> 00:02:10.000]   through chapters three to six. Where to the people who are new
[00:02:10.000 --> 00:02:13.800]   here is my version of the book. We're reading deep learning with
[00:02:13.800 --> 00:02:20.160]   Python 2.0 by Francois Chollet. He created Keras and he was also
[00:02:20.160 --> 00:02:22.320]   kind enough to join us for the first session. So if you're
[00:02:22.320 --> 00:02:24.600]   watching this recording on YouTube, or if you're joining us
[00:02:24.600 --> 00:02:27.680]   freshly today, I would encourage you to check out the Ask Me
[00:02:27.680 --> 00:02:30.680]   Anything interview with Francois. We talked, we spoke
[00:02:30.680 --> 00:02:34.400]   about a lot of things. And we also went to the first two
[00:02:34.400 --> 00:02:38.480]   chapters that are a bit of introductory, quite good
[00:02:38.480 --> 00:02:42.240]   starting notes for anyone who's new to the world of machine
[00:02:42.240 --> 00:02:45.240]   learning. And as a reminder, if you're watching this recording,
[00:02:45.240 --> 00:02:49.800]   or if you're joining us for the first time today, all of our
[00:02:49.840 --> 00:02:53.160]   study groups are open to everyone. These are recorded,
[00:02:53.160 --> 00:02:56.640]   these are made available to everyone free of cost. And we I
[00:02:56.640 --> 00:02:59.720]   really want to emphasize on the point that everyone and anyone
[00:02:59.720 --> 00:03:03.400]   is welcome to join. Please ask and ask many questions as you
[00:03:03.400 --> 00:03:06.320]   like. I love to answer any questions. If you're new to
[00:03:06.320 --> 00:03:09.840]   machine learning, I'm happy to guide you to the best of my
[00:03:09.840 --> 00:03:13.000]   ability, although I don't think I'd be the right person. And
[00:03:13.000 --> 00:03:16.600]   yes, please don't hesitate with asking any questions. These
[00:03:16.680 --> 00:03:21.160]   events, these live streams are for you and we want you to
[00:03:21.160 --> 00:03:22.440]   enjoy them to the fullest.
[00:03:22.440 --> 00:03:30.160]   Here's the agenda I have planned for today for the first five to
[00:03:30.160 --> 00:03:32.960]   10 minutes, I'll try to recap what we learned in the last
[00:03:32.960 --> 00:03:38.160]   week, I'll announce the winners of 27 days of Keras. And then I
[00:03:38.160 --> 00:03:40.600]   have this ambitious goal of going through about three
[00:03:40.600 --> 00:03:42.840]   actually three and a half chapters. So we'll continue
[00:03:42.840 --> 00:03:46.720]   doing that. Throughout the session, my throat dries up
[00:03:46.720 --> 00:03:48.960]   really quickly. So I'll be taking breaks to answer
[00:03:48.960 --> 00:03:51.640]   questions to check the YouTube chat and to check the visible
[00:03:51.640 --> 00:03:57.760]   chart. That's the event where you might have signed up. I'm
[00:03:57.760 --> 00:04:00.600]   just going over there to make sure things are looking good.
[00:04:00.600 --> 00:04:03.400]   Yep, I can see the comments coming. So that's good to see.
[00:04:03.400 --> 00:04:07.000]   Amazing. So throughout the session, I'll be taking breaks
[00:04:07.000 --> 00:04:09.640]   to answer your questions. Please keep them coming. And I'll keep
[00:04:09.640 --> 00:04:14.080]   an eye out for all of them. You don't want to see my face. So
[00:04:14.080 --> 00:04:17.800]   let me hide that from the YouTube video. Awesome. We're
[00:04:17.800 --> 00:04:22.600]   good to go now. So let me start by reminding everyone with 27
[00:04:22.600 --> 00:04:26.000]   days of Keras. It's an initiative to invite all of you
[00:04:26.000 --> 00:04:30.120]   to learn Keras to spend more time learning Keras. And the
[00:04:30.120 --> 00:04:34.360]   only requirement is please learn something about Keras and share
[00:04:34.360 --> 00:04:37.320]   it with the world. I really think there's a lot to be
[00:04:37.320 --> 00:04:41.920]   learned by writing blog posts by sharing your notebooks. And I
[00:04:41.920 --> 00:04:46.160]   want to give a huge shout out to Yuvraj. Yuvraj has been
[00:04:46.160 --> 00:04:50.800]   extremely consistent about his learnings. And I think I'll try
[00:04:50.800 --> 00:04:53.680]   to invite him in the next session to ask him really what
[00:04:53.680 --> 00:04:57.320]   keeps him so consistent. But every single week he's been
[00:04:57.320 --> 00:05:02.680]   writing incredible blog posts. For the first submission that he
[00:05:02.680 --> 00:05:07.720]   made to 27 days, he summarized our previous session. So I'll
[00:05:07.720 --> 00:05:10.400]   correct myself if you didn't join the previous session,
[00:05:10.400 --> 00:05:13.360]   please don't watch my recording, check out his notebook instead.
[00:05:13.360 --> 00:05:17.480]   It's a better version of what I taught. Let me put that in the
[00:05:17.480 --> 00:05:23.040]   YouTube chat real quickly. There you go. Should be there. Thank
[00:05:23.040 --> 00:05:31.600]   you, Yuvraj for that. Borut said he will submit a soccer
[00:05:31.600 --> 00:05:33.920]   prediction time series components. I'm looking forward
[00:05:33.920 --> 00:05:38.600]   to that. Aritra, he's an incredible writer. He writes
[00:05:38.600 --> 00:05:43.120]   great blog posts that PyMH search, which is one of the best
[00:05:43.120 --> 00:05:47.440]   resources in my opinion to learn computer vision. He had
[00:05:47.440 --> 00:05:53.560]   submitted VIT on small data sets as an example to Keras.io. And
[00:05:53.560 --> 00:05:58.520]   he was kind enough to also put this in as a submission to 27
[00:05:58.520 --> 00:06:03.400]   days of Keras. So he's implemented VIT on to I believe
[00:06:03.400 --> 00:06:11.440]   Keras. Originally it wasn't PyTorch. All of the examples in
[00:06:11.440 --> 00:06:14.760]   Keras, I want to remind everyone are really high quality and I
[00:06:14.760 --> 00:06:17.760]   would really encourage everyone to check them out. I think one
[00:06:17.760 --> 00:06:21.960]   of the best ways to go about learning Keras if anyone is at
[00:06:21.960 --> 00:06:26.880]   all interested. So anything on, sorry, Keras.io, I would really
[00:06:26.880 --> 00:06:30.080]   encourage you to spend more and more time on there as we go
[00:06:30.080 --> 00:06:34.760]   about learning Keras. And I also want to mention Aritra on the
[00:06:34.760 --> 00:06:38.920]   week's 27 days of Keras. So Aritra if you're watching or if
[00:06:38.920 --> 00:06:42.400]   you're not, we'll reach out to you to offer you some swag. As a
[00:06:42.400 --> 00:06:45.320]   reminder to everyone, please keep the submissions coming in.
[00:06:45.320 --> 00:06:49.720]   As you can see, you can just take the session, which is not
[00:06:49.720 --> 00:06:53.640]   the best one, I'll be honest and rewrite it. And those are also
[00:06:53.640 --> 00:06:56.520]   great submissions. So there's no bar to what you should submit
[00:06:56.560 --> 00:07:00.000]   anything that you learn, I would encourage you to write about it
[00:07:00.000 --> 00:07:04.960]   because it serves as a good placeholder on your resume and
[00:07:04.960 --> 00:07:07.600]   also as a good sign to the community that you know
[00:07:07.600 --> 00:07:16.760]   something. Let me close these tabs real quickly. And for a
[00:07:16.760 --> 00:07:21.080]   recap, we learned about what is AI broadly speaking, what is
[00:07:21.080 --> 00:07:23.840]   machine learning, we learned those different jargons, where
[00:07:23.840 --> 00:07:27.200]   does machine learning come into the terminology of artificial
[00:07:27.200 --> 00:07:30.440]   intelligence? What is deep learning? Is it a subset of
[00:07:30.440 --> 00:07:34.000]   machine learning? Are those two different? We looked at the code
[00:07:34.000 --> 00:07:36.480]   from the first two chapters and trained our first neural
[00:07:36.480 --> 00:07:40.600]   network. It was a naive implementation, a naive dense
[00:07:40.600 --> 00:07:43.920]   implementation, if you all remember, and we learned about
[00:07:43.920 --> 00:07:50.280]   tensors. What are they? How can we apply tensor operations, and
[00:07:50.280 --> 00:07:52.720]   we learned the mathematical founding blocks of neural
[00:07:52.720 --> 00:07:56.040]   networks. So that's what we learned from the first session.
[00:07:56.040 --> 00:08:01.240]   As a reminder, I suggest homework, it's just as a
[00:08:01.240 --> 00:08:05.120]   suggestion, but I had suggested that you should have checked out
[00:08:05.120 --> 00:08:09.400]   different data sets in Keras. I would encourage everyone to
[00:08:09.400 --> 00:08:12.800]   again, play around with Keras.io. We studied gradient
[00:08:12.800 --> 00:08:16.960]   tape, and you've asked it right about it as well. So I'm happy
[00:08:16.960 --> 00:08:22.320]   to see that at least someone did the homework. And as machine
[00:08:22.320 --> 00:08:24.880]   learning engineers, you're expected to spend more and more
[00:08:24.880 --> 00:08:27.440]   time inside of code. So my suggestion was please try
[00:08:27.440 --> 00:08:32.400]   messing with the default examples of the notebook. If I
[00:08:32.400 --> 00:08:39.080]   hover over to any chapter right now, I see TensorFlow ones here,
[00:08:39.080 --> 00:08:43.400]   I see TensorFlow zeros. Just start by the simplest thing,
[00:08:43.400 --> 00:08:46.040]   changing these numbers, looking at the signature of this
[00:08:46.040 --> 00:08:51.840]   function, what's the signature of a function? Tell me what the
[00:08:51.840 --> 00:08:55.800]   function takes in and gives out. So if you put a question mark
[00:08:55.800 --> 00:09:00.920]   against anything, it gives you the details of the function. So
[00:09:00.920 --> 00:09:03.720]   this one creates a tensor with all elements. I admit this is
[00:09:03.720 --> 00:09:07.400]   like a bad example to choose from. But the message I want to
[00:09:07.400 --> 00:09:10.960]   get out is please spend more time inside of here. Last week,
[00:09:10.960 --> 00:09:17.360]   we learned about activations. My old intuition would be to go to
[00:09:18.440 --> 00:09:22.320]   Google University and look up what are activation functions.
[00:09:22.320 --> 00:09:28.120]   Would you say this is the right way to learn? Not at all. I
[00:09:28.120 --> 00:09:33.640]   would encourage you to instead search this. But inside of the
[00:09:33.640 --> 00:09:39.280]   Keras documents. So let's hover over to the docs. Let's search
[00:09:39.280 --> 00:09:45.640]   activation function. That doesn't give anything. So let's
[00:09:45.680 --> 00:09:50.640]   simply look for activation. Layer activation functions that
[00:09:50.640 --> 00:09:55.040]   looks like it. And instead, I want to remind everyone that as
[00:09:55.040 --> 00:09:58.440]   a engineer, as a researcher, as a research scientist, you're
[00:09:58.440 --> 00:10:00.960]   supposed to develop stuff in code, it's important to
[00:10:00.960 --> 00:10:04.280]   understand how these things work. But the intuition, in my
[00:10:04.280 --> 00:10:07.760]   opinion, comes from spending more time coding these things.
[00:10:07.760 --> 00:10:11.720]   So instead of trying to look up looking up the theory around
[00:10:11.720 --> 00:10:17.200]   things, build the intuition by building code, writing code and
[00:10:17.200 --> 00:10:20.160]   building these models instead of understanding how they work in
[00:10:20.160 --> 00:10:25.360]   theory. In my opinion, that's, again, a strong take, but I
[00:10:25.360 --> 00:10:28.840]   think that should help you more than simply just reading theory.
[00:10:28.840 --> 00:10:35.600]   So sharing the outline we were right about in the starting bit
[00:10:35.600 --> 00:10:39.640]   of chapter three in the previous call, and for today, I'll try to
[00:10:39.640 --> 00:10:43.120]   go right up to the mid part of chapter seven. So this session
[00:10:43.120 --> 00:10:47.160]   would be I would imagine around five weeks at max six weeks long.
[00:10:47.160 --> 00:10:51.280]   And hopefully, thankfully for you, you don't just have to
[00:10:51.280 --> 00:10:54.640]   study from me. I've also reached out to many experts in the world
[00:10:54.640 --> 00:10:58.360]   of Keras and TensorFlow experts, they'll be joining us in the
[00:10:58.360 --> 00:11:01.560]   next few weeks and teaching us about more things outside of the
[00:11:01.560 --> 00:11:04.880]   book. I'll announce more about that once they confirm but we
[00:11:04.880 --> 00:11:07.920]   have at least three or four people who will be coming into
[00:11:07.920 --> 00:11:10.920]   teaching, coming into the session and teaching us things.
[00:11:10.920 --> 00:11:18.240]   Are there any questions so far about the previous session?
[00:11:18.240 --> 00:11:20.960]   Let me take a look.
[00:11:20.960 --> 00:11:33.280]   Karthik mentioned he missed the opportunity to start on 27 days
[00:11:33.280 --> 00:11:36.920]   of Keras. There's no start or end date. It's always ongoing.
[00:11:36.920 --> 00:11:40.240]   It's like 100 days of code. There's no start or end to it.
[00:11:40.240 --> 00:11:45.640]   It's 27 but it's also 365 days. So join anytime please. It's
[00:11:45.640 --> 00:11:49.400]   for you to learn and the reason why we're also sending out swag
[00:11:49.400 --> 00:11:52.200]   to people is to encourage everyone to really learn this.
[00:11:52.200 --> 00:12:05.520]   I'll take a quick pause and resume.
[00:12:05.520 --> 00:12:11.360]   Someone told me my throat really dries up really fast. So
[00:12:11.360 --> 00:12:15.320]   I'm trying to stay hydrated and I'll continue now. You'll see me
[00:12:15.320 --> 00:12:20.600]   take a lot of breaks today. That's why. Again, a refresher.
[00:12:20.600 --> 00:12:24.800]   How is deep learning different with quote unquote, traditional
[00:12:24.800 --> 00:12:28.640]   machine learning you are required to you were required to
[00:12:28.640 --> 00:12:34.600]   handcraft a lot of features and do a lot of manual hand tuning.
[00:12:34.600 --> 00:12:40.680]   So you had to define a lot of conditional statements, let's
[00:12:40.680 --> 00:12:46.520]   say. Not conditional statements, but even I'm trying to think of
[00:12:46.520 --> 00:12:49.400]   a good example. So if you were to just take image processing,
[00:12:49.400 --> 00:12:52.960]   you would have to with OpenCV that's one of the go to
[00:12:52.960 --> 00:12:55.360]   traditional frameworks, you would have to do a lot of
[00:12:55.360 --> 00:12:59.720]   pre processing, get the image into the right shape, and then
[00:12:59.720 --> 00:13:02.960]   be able to build a classifier on top of it with deep learning,
[00:13:02.960 --> 00:13:07.120]   it just becomes a single step. So you still have to tune a lot
[00:13:07.120 --> 00:13:12.200]   of things. But you don't need to know as much about what's a
[00:13:12.200 --> 00:13:18.160]   Gaussian blur. What's an edge detection algorithm, it helps to
[00:13:18.160 --> 00:13:22.200]   know these things sometimes the right pre processing. But deep
[00:13:22.200 --> 00:13:28.760]   learning also does wonders without it. So next in the third
[00:13:28.760 --> 00:13:33.240]   chapter, I believe, Franchot talks about how Keras originated
[00:13:33.240 --> 00:13:39.320]   a bit of background around it. And people maybe not might not
[00:13:39.320 --> 00:13:44.440]   remember this, but it didn't just start out as a meta
[00:13:44.440 --> 00:13:47.880]   framework for TensorFlow, it was originally created as this meta
[00:13:47.880 --> 00:13:52.360]   library that worked with C and TK. That's a framework, Tiano
[00:13:52.360 --> 00:13:56.080]   that used to be a framework, TensorFlow and MXNet. So
[00:13:56.080 --> 00:14:04.200]   originally, it existed as this hyper framework that would work
[00:14:04.200 --> 00:14:07.240]   with all of these options, because earlier, remember, this
[00:14:07.240 --> 00:14:11.200]   was I think, 2015 2016, they used to be a lot of frameworks
[00:14:11.200 --> 00:14:15.720]   and not just the two go to ones. So at that time, Keras used to
[00:14:15.720 --> 00:14:23.320]   be a high level API to all of these. And in 2019, when
[00:14:23.320 --> 00:14:27.000]   TensorFlow 2.0 was launched, it just defaulted to the official
[00:14:27.000 --> 00:14:31.240]   high level API of TensorFlow 2.0. How is this useful? It's
[00:14:31.240 --> 00:14:34.880]   good to know some history. It's also helpful to understand where
[00:14:34.880 --> 00:14:38.200]   did this come from. I remember learning about PyTorch. And
[00:14:38.200 --> 00:14:41.560]   there are some small biases inside of the code somewhere
[00:14:41.560 --> 00:14:45.680]   that originated from the original Torch version of the
[00:14:45.680 --> 00:14:49.960]   framework. So sometimes it's good to know these things. Today,
[00:14:50.040 --> 00:14:53.520]   what Keras is to TensorFlow, TensorFlow is to the hardware of
[00:14:53.520 --> 00:14:57.280]   your machine. So TensorFlow can talk to GPUs, it can talk to
[00:14:57.280 --> 00:15:02.160]   CPUs, it can talk to TPUs, you need to change some code here
[00:15:02.160 --> 00:15:07.440]   and there. But TensorFlow works really well. On the hardware
[00:15:07.440 --> 00:15:11.240]   level, it interacts with the C, I think at some level, it would
[00:15:11.240 --> 00:15:16.360]   interact at a C++ code level with the hardware, which makes
[00:15:16.360 --> 00:15:19.040]   things really optimized. There are some things that make
[00:15:19.080 --> 00:15:23.120]   TensorFlow really stand out. So it's similar to NumPy, but it
[00:15:23.120 --> 00:15:29.680]   tracks the gradient. And NumPy doesn't work on you can't put a
[00:15:29.680 --> 00:15:34.240]   NumPy array on a GPU. You can't put a NumPy array on TPU.
[00:15:34.240 --> 00:15:38.120]   There's Jack's that is something different. We did speak with
[00:15:38.120 --> 00:15:41.920]   Franchot about it, but I would encourage you to check that out.
[00:15:41.920 --> 00:15:46.800]   So that's why TensorFlow stands out from NumPy. These are the
[00:15:46.800 --> 00:15:50.000]   examples inside of the book, small things that I wanted to
[00:15:50.000 --> 00:15:53.280]   highlight. So inside of these sessions, I'm again really
[00:15:53.280 --> 00:15:56.160]   focused on teaching you about the key things that are
[00:15:56.160 --> 00:15:59.600]   important, not the ones that you have, you can simply read and
[00:15:59.600 --> 00:16:03.960]   understand in the book, the book would take you a few hours to
[00:16:03.960 --> 00:16:08.120]   read. And these sessions will be much smaller than that. So my
[00:16:08.120 --> 00:16:10.440]   goal here is to teach you about the important things. So I'm
[00:16:10.440 --> 00:16:17.000]   just trying to point those out. Now, here's the link to the
[00:16:17.000 --> 00:16:22.320]   repository. Let me put that in the chart. If you simply look up
[00:16:22.320 --> 00:16:26.800]   deep learning with Python, it should show up on Google. But
[00:16:26.800 --> 00:16:31.480]   let me make it visible to everyone for a few minutes. So
[00:16:31.480 --> 00:16:35.560]   we're looking at notebooks three to seven quite an ambitious goal
[00:16:35.560 --> 00:16:38.760]   for today. And I'll try to explain all of the key concepts
[00:16:38.760 --> 00:16:43.080]   as we go along. And towards the later half, I want to make these
[00:16:43.080 --> 00:16:46.720]   interactive. So I have this exercise special exercise, we
[00:16:46.720 --> 00:16:51.200]   will try to take a business problem. And as a fresh ML
[00:16:51.200 --> 00:16:54.000]   engineer who's just learned Keras, who's ready to make some
[00:16:54.000 --> 00:16:57.800]   money, we'll try to convert that into a business problem. Sorry,
[00:16:57.800 --> 00:17:01.200]   we'll take the business problem and convert it into ML problem
[00:17:01.200 --> 00:17:04.080]   where we'll frame different questions, try to understand how
[00:17:04.080 --> 00:17:08.400]   we can solve it. So that's towards the, I would say last 20
[00:17:08.400 --> 00:17:19.040]   minutes. So let's start by looking at notebook three. And
[00:17:19.040 --> 00:17:22.160]   this is an introduction to Keras and TensorFlow. I just gave you
[00:17:22.160 --> 00:17:26.920]   the history of both the frameworks and a very short
[00:17:26.920 --> 00:17:30.600]   description of why these are useful. Right after that, we
[00:17:30.600 --> 00:17:33.680]   start by looking at what things can you do inside of TensorFlow
[00:17:33.680 --> 00:17:47.800]   and how is it helpful. So you can create tensors and variables
[00:17:47.800 --> 00:17:52.120]   inside of TensorFlow. What are tensors and variables you might
[00:17:52.120 --> 00:18:06.760]   ask? Let's see. Let's hover over to TensorFlow docs. Python and
[00:18:06.760 --> 00:18:19.320]   search for variables. A TensorFlow variable is the
[00:18:19.320 --> 00:18:23.480]   recommended way to represent shared and persistent state or
[00:18:23.480 --> 00:18:28.920]   program manipulates. So that basically means it's a variable
[00:18:28.920 --> 00:18:35.000]   in code, how we define variables in code that's a TensorFlow
[00:18:35.000 --> 00:18:39.960]   variable that lives on your accelerator. And let's take a
[00:18:39.960 --> 00:18:45.480]   look at tensors. I don't think I need to remind everyone what
[00:18:45.480 --> 00:18:48.320]   tensors are. So I'll skip that. So what are we doing here we
[00:18:48.320 --> 00:18:56.080]   creating a tensor that has all numbers one of shape two comma
[00:18:56.080 --> 00:19:00.160]   one, which means it will have two columns, sorry, two rows
[00:19:00.160 --> 00:19:03.880]   and one column. And by default, it will have a data type of
[00:19:03.880 --> 00:19:09.440]   float 32. So you can do that you can also create a tensor having
[00:19:09.440 --> 00:19:13.720]   all number zeros, you can pick these from a normal
[00:19:13.720 --> 00:19:16.640]   distribution. And if you're again, I want to remind
[00:19:16.640 --> 00:19:23.640]   everyone, if you're ever curious what a function is doing, put
[00:19:23.640 --> 00:19:27.440]   one question mark, and it should bring up the signature of the
[00:19:27.440 --> 00:19:31.160]   function. If you're feeling very brave that day, and very
[00:19:31.160 --> 00:19:33.600]   adventurous, put two question marks and it'll show you the
[00:19:33.600 --> 00:19:37.360]   source code. I'm already terrified of this. So I'll close
[00:19:37.360 --> 00:19:44.400]   this and read this out loud. So this example, it showcases an
[00:19:44.400 --> 00:19:47.800]   example that generates a new set of random values every time.
[00:19:47.800 --> 00:19:51.720]   Okay, so it looks like it's useful to generate random
[00:19:51.720 --> 00:19:55.160]   numbers. Why do we need these functions? I mean, someone put
[00:19:55.160 --> 00:19:58.640]   them because they are useful, right? Assumably, or
[00:19:58.640 --> 00:20:04.120]   presumably to developers. We looked at MNIST in the last
[00:20:04.120 --> 00:20:09.240]   week's lecture, recap of the book chapter, we looked at
[00:20:09.240 --> 00:20:14.640]   MNIST. And we learned about different shape of tensors. Now
[00:20:14.640 --> 00:20:18.680]   what we didn't learn about how do you add those values? So
[00:20:18.680 --> 00:20:25.040]   inside of a variable of 28 by 28 size, having presumably the
[00:20:25.040 --> 00:20:30.200]   weights, how do you set those numbers for the first time,
[00:20:30.200 --> 00:20:32.680]   eventually, your model learns there's a loss value that
[00:20:32.680 --> 00:20:38.600]   changes them, upgrades them as it loops over. But you need a
[00:20:38.600 --> 00:20:41.640]   way to initialize them or what is the first value that you
[00:20:41.640 --> 00:20:44.960]   start out with. So they could be zero, you could pick them from
[00:20:44.960 --> 00:20:48.440]   a normal distribution, define the shape, define their mean and
[00:20:48.440 --> 00:20:51.800]   standard deviation. Or you could also pick them from a uniform
[00:20:51.800 --> 00:20:54.480]   distribution. So that's why these functions are quite
[00:20:54.480 --> 00:21:00.920]   useful. There are different ways of initializing that are quite
[00:21:00.920 --> 00:21:05.160]   standard now. So I would encourage you to check out timing
[00:21:05.160 --> 00:21:16.960]   her initialization. Put that in the chart. This is one of the
[00:21:16.960 --> 00:21:21.960]   ways and again, don't look at the theory. If you go to the
[00:21:21.960 --> 00:21:26.840]   paper, it'll terrify, it's at least terrifying to me. Add this
[00:21:26.840 --> 00:21:33.840]   one keyword. So this looks like the right link, right? tf.keras.
[00:21:33.920 --> 00:21:41.560]   initializers heard normal. So this is how you do it in Keras.
[00:21:41.560 --> 00:21:45.840]   And then you can check out the paper if you want. Turns out it
[00:21:45.840 --> 00:21:50.080]   belongs to the initializers class. And if I zoom over here,
[00:21:50.080 --> 00:21:56.400]   there are all of these ways of initializing your variables. So
[00:21:56.400 --> 00:22:00.960]   again, a suggested homework, maybe try initializing with
[00:22:00.960 --> 00:22:06.520]   these different options and see if your model trains faster.
[00:22:06.520 --> 00:22:14.840]   It's a machine learning is quite experimental. And I think again,
[00:22:14.840 --> 00:22:18.960]   this is how you gain that intuition behind how things
[00:22:18.960 --> 00:22:24.840]   work. So continuing further, I'm sorry for always going on these
[00:22:24.840 --> 00:22:28.680]   tangents. I also want to give out some, some knowledge that
[00:22:28.680 --> 00:22:41.760]   might be a bit pragmatic. You can create a variable like so.
[00:22:41.760 --> 00:22:47.120]   And you can define its initial value as well. How do you find
[00:22:47.120 --> 00:22:55.120]   out what are the parameters can you pass to function? By doing
[00:22:55.120 --> 00:23:05.960]   this. So you can pass the shape, the data type. Let's see if
[00:23:05.960 --> 00:23:10.320]   there's anything else you can do. No, not, not anything that I
[00:23:10.320 --> 00:23:13.400]   might be interested in. So that's how you check what
[00:23:13.400 --> 00:23:16.720]   parameters can you pass. So we initialize it from a normal
[00:23:16.720 --> 00:23:22.360]   distribution of the shape, like so. And if you want to assign
[00:23:22.360 --> 00:23:27.520]   new values, you do them like so. So if you want to change the
[00:23:27.520 --> 00:23:30.480]   value inside of V, there's an assign function that you can
[00:23:30.480 --> 00:23:36.640]   call. You can also do it like so where you index into certain
[00:23:36.640 --> 00:23:40.360]   elements of the element at zero comma zero and assign it this
[00:23:40.360 --> 00:23:46.240]   particular value. You could also assign an add, let's see what
[00:23:46.240 --> 00:23:54.240]   this does. Let's search assign add inside of the
[00:23:54.240 --> 00:23:59.480]   documentation. TensorFlow is by Google, so their search should
[00:23:59.480 --> 00:24:21.600]   be good. I'm hoping it is. It just adds the number but I was
[00:24:21.600 --> 00:24:24.560]   trying to point that out in the documentation. So this was not
[00:24:24.560 --> 00:24:28.800]   helpful because I didn't see that. Maybe it's quite obvious
[00:24:28.800 --> 00:24:34.440]   to everyone but I didn't see that. So I downloaded that. We
[00:24:34.440 --> 00:24:39.920]   are adding one to this entire tensor. How do we do that? So we
[00:24:39.920 --> 00:24:46.520]   call the function assign add and then we pass the tensor that we
[00:24:46.520 --> 00:24:52.400]   want to add to it. It is a tensor having all ones because
[00:24:52.400 --> 00:24:55.840]   we're calling tf.once. So we would just add one to all of
[00:24:55.840 --> 00:25:01.080]   these. Yes, logic checks out. It does add one. Why are we
[00:25:01.080 --> 00:25:03.960]   looking at all of these functions that are so basic?
[00:25:03.960 --> 00:25:08.640]   Turns out, deep learning is a bunch of matrix operations. And
[00:25:08.640 --> 00:25:11.880]   it's important to learn these because you will be just doing
[00:25:11.880 --> 00:25:17.520]   these at scale eventually. So that's why we're spending a
[00:25:17.520 --> 00:25:21.120]   bunch of time here. To anyone who's experienced, I see
[00:25:21.120 --> 00:25:25.760]   incredibly smart people in the chat. I'm sorry, this might feel
[00:25:25.760 --> 00:25:29.520]   like a waste of your time. I also want to be thoughtful
[00:25:29.520 --> 00:25:32.560]   towards people who are just starting their journey. That's
[00:25:32.560 --> 00:25:37.480]   why I'm going slow initially, we'll speed up towards the next
[00:25:37.480 --> 00:25:39.880]   lecture, I believe again, this will be more focused towards the
[00:25:39.880 --> 00:25:48.440]   basics. Let's look at a few math operations. It's a bunch of
[00:25:48.440 --> 00:25:51.640]   matrix operations, right? So how do we perform them inside of
[00:25:51.640 --> 00:25:59.320]   TensorFlow? We define ABC as tensors. Sorry, A as tensor,
[00:25:59.320 --> 00:26:03.600]   having all ones. And now if we want to square it, we call tf.
[00:26:03.600 --> 00:26:10.720]   square. If we want to perform a square root, we call tf.sqrt. If
[00:26:10.720 --> 00:26:14.120]   we want to add them, we can do it like so. And if you want to
[00:26:14.120 --> 00:26:19.360]   multiply, we can do it like so. Does anyone want to take a stab
[00:26:19.360 --> 00:26:23.840]   at how these two different are these two the same? So if we do
[00:26:23.840 --> 00:26:28.520]   a matmul, or if we do use the shorthand operator, this
[00:26:28.520 --> 00:26:32.800]   basically, for anyone that is new, is usually used to
[00:26:32.800 --> 00:26:37.640]   represent this. But if we are using a shorthand operator, does
[00:26:37.640 --> 00:26:40.480]   anyone want to take a stab how this stands out? What does this
[00:26:40.480 --> 00:26:51.680]   do? There's a little delay between when I say the words and
[00:26:51.680 --> 00:26:55.080]   when it's live stream. So I'm taking a pause to just check.
[00:27:08.280 --> 00:27:12.520]   Katie is right. It's element wise multiplication, Martin as
[00:27:12.520 --> 00:27:14.360]   well. Thanks for those answers.
[00:27:14.360 --> 00:27:25.880]   We went into depth about gradient tape. As a reminder,
[00:27:25.880 --> 00:27:29.520]   it's used for tracking gradients. But isn't this
[00:27:29.520 --> 00:27:32.040]   counterintuitive? Like if you're if you've been in the
[00:27:32.040 --> 00:27:35.160]   PyTorch world, why do you like need to understand a separate
[00:27:37.040 --> 00:27:41.440]   term that tracks gradients, you don't need to for important
[00:27:41.440 --> 00:27:45.800]   variables. The book claims that it's auto tracked. So we don't
[00:27:45.800 --> 00:27:49.160]   have to worry about the variables that we want to
[00:27:49.160 --> 00:27:53.560]   differentiate. But it's still important to learn how it
[00:27:53.560 --> 00:27:56.040]   functions. So inside of the book, I believe they use an
[00:27:56.040 --> 00:28:00.320]   example where we calculate a first order and a second order
[00:28:00.320 --> 00:28:05.360]   gradient. I wasn't good at calculus. So I'll try to explain
[00:28:05.360 --> 00:28:08.680]   this first order gradient is when you differentiate
[00:28:08.680 --> 00:28:13.160]   something for the first time. So speed is a first order
[00:28:13.160 --> 00:28:18.800]   gradient. And acceleration is a second order gradient because we
[00:28:18.800 --> 00:28:24.880]   differentiate speed to get acceleration, which is what the
[00:28:24.880 --> 00:28:27.720]   example in the book does as well. I'll skip over here
[00:28:27.720 --> 00:28:32.840]   because that's what's happening here. So now we are well
[00:28:32.840 --> 00:28:38.000]   equipped to create an end to end example. Inside of the book
[00:28:38.000 --> 00:28:41.960]   Franchot mentions this is a filter question for interview.
[00:28:41.960 --> 00:28:46.080]   So phone screen question, which means this is quite a vanilla
[00:28:46.080 --> 00:28:49.200]   example. It's still an end to end example because we are
[00:28:49.200 --> 00:28:52.040]   training a model that is a linear classifier in pure
[00:28:52.040 --> 00:28:58.720]   TensorFlow. So we create I think I'm just trying to look
[00:28:58.720 --> 00:29:03.720]   here, we create 1000 samples, and we create negative and
[00:29:03.720 --> 00:29:07.120]   positive samples. So we're trying to create a cluster of
[00:29:07.120 --> 00:29:13.600]   two samples that we want to separate by a 2D plane. I'll
[00:29:13.600 --> 00:29:18.680]   use this image to skip my hand waving. And what we want to do
[00:29:18.680 --> 00:29:23.040]   is we want to as a potential machine learning candidate who's
[00:29:23.040 --> 00:29:27.040]   interviewing for an awesome job. To pass the phone screen, we
[00:29:27.040 --> 00:29:34.240]   need to classify these and use a linear classifier to prove
[00:29:34.240 --> 00:29:38.320]   that which point belongs to which class. So we've created
[00:29:38.320 --> 00:29:45.840]   these clusters of points. And we've done a scatterplot. I am
[00:29:45.840 --> 00:29:50.040]   always tempted in my learning journey to, you know, look up
[00:29:50.120 --> 00:30:00.760]   what is a scatterplot. Don't do this. Don't go down the
[00:30:00.760 --> 00:30:04.800]   Wikipedia rabbit hole. Please don't do that. You will end up
[00:30:04.800 --> 00:30:10.560]   learning a lot about the theory. But what you want to learn about
[00:30:10.560 --> 00:30:18.000]   it is what's happening inside of that matplotlib function. I
[00:30:18.000 --> 00:30:22.280]   fall into this trap every single time of not spending more time
[00:30:22.280 --> 00:30:27.840]   fiddling around with arguments, changing the value seeing how
[00:30:27.840 --> 00:30:32.120]   that works. So I'll keep mentioning this until it gets
[00:30:32.120 --> 00:30:35.640]   annoying and then continue even after that, to anyone who's new
[00:30:35.640 --> 00:30:40.680]   because it's a mistake we all made. So what we do next is now
[00:30:40.680 --> 00:30:46.560]   we have 1000 examples. I won't count this but I trust NumPy to
[00:30:46.560 --> 00:30:52.080]   have created 1000 of these. So we have 1000 classes that are
[00:30:52.080 --> 00:30:55.480]   positive and that are negative and we want to create a
[00:30:55.480 --> 00:30:59.560]   classifier. So to create our data set, we stack this by
[00:30:59.560 --> 00:31:03.320]   calling numpy.pstack. You can look at the signature to see
[00:31:03.320 --> 00:31:08.560]   what it does. It just stacks these arrays on top of each
[00:31:08.560 --> 00:31:12.600]   other. And then we also would want to stack the targets,
[00:31:12.600 --> 00:31:16.840]   right? So let's say it's a linear classifier. So we can
[00:31:16.840 --> 00:31:21.200]   just assign a number zero to one cluster and one to another
[00:31:21.200 --> 00:31:26.080]   one. Now we can take a plot that I was using and take a look at
[00:31:26.080 --> 00:31:29.680]   the plot to see if everything is right. It's always a good idea
[00:31:29.680 --> 00:31:32.280]   to print out things to plot them to see if they are working the
[00:31:32.280 --> 00:31:36.840]   correct way. So as a potential candidate, now we're ready to
[00:31:36.840 --> 00:31:41.840]   create the linear classifier variables. We set the input
[00:31:41.840 --> 00:31:45.680]   dimension as two, output dimension as one. We initialize
[00:31:45.680 --> 00:31:50.960]   the weights and biases like so by utilizing the input and
[00:31:50.960 --> 00:31:54.000]   output dimensions that we've created. See the subtle
[00:31:54.000 --> 00:32:00.080]   difference of not passing these numbers in here. These are
[00:32:00.080 --> 00:32:02.840]   things that you should be careful about. So if things
[00:32:02.840 --> 00:32:07.240]   break, or if you want to change these, you should change them
[00:32:07.240 --> 00:32:11.640]   here and not paste them here. Because maybe not for this
[00:32:11.640 --> 00:32:14.520]   example, for sure. But when you're working with a
[00:32:14.520 --> 00:32:19.440]   multi-headed transformer model, things get out of hands really
[00:32:19.440 --> 00:32:22.880]   quickly. And it's like impossible to debug, especially
[00:32:22.880 --> 00:32:27.640]   at 3am and you've been coding all night. So it's better to
[00:32:27.640 --> 00:32:33.760]   code things above or put them in different variables. Now we'll
[00:32:33.760 --> 00:32:37.800]   define the forward pass. And our model is quite simple. It's a
[00:32:37.800 --> 00:32:41.480]   matrix multiplication, things won't change from here. We'll
[00:32:41.480 --> 00:32:45.320]   just do matrix multiplication a lot of times, even for neural
[00:32:45.320 --> 00:32:50.160]   networks. This is a binary classification problem. So we
[00:32:50.160 --> 00:32:59.120]   can define a mean squared loss. And we, sorry, we'll just, we
[00:32:59.120 --> 00:33:01.520]   could also use a different loss function if that was an
[00:33:01.520 --> 00:33:06.000]   incorrect argument. We'll set a square loss for our linear
[00:33:06.000 --> 00:33:10.320]   classifier. For this particular case. I don't want to
[00:33:10.320 --> 00:33:13.400]   generalize that argument. Sorry, sorry for the confusion.
[00:33:13.400 --> 00:33:17.720]   So having defined a loss function, having defined a
[00:33:17.720 --> 00:33:21.880]   forward function and our variables, we are ready to
[00:33:21.880 --> 00:33:25.880]   define the training step. And this time we'll have to manually
[00:33:25.880 --> 00:33:29.040]   tell TensorFlow, hey, TensorFlow, these variables that
[00:33:29.040 --> 00:33:32.200]   you might discard, these are quite important, because this
[00:33:32.200 --> 00:33:35.240]   task is pretty important for me to get into the next interview.
[00:33:35.760 --> 00:33:42.520]   So please track them. And we track the gradient of the
[00:33:42.520 --> 00:33:46.560]   predictions and loss. And then we can differentiate it with
[00:33:46.560 --> 00:33:51.040]   respect to the weights and biases to get the gradient. And
[00:33:51.040 --> 00:33:55.000]   then we can update the weight by multiplying the gradient with
[00:33:55.000 --> 00:33:57.880]   the learning rate. So the learning rate will decide how
[00:33:57.880 --> 00:34:04.360]   fast things will learn. And again, we're not setting this as
[00:34:04.360 --> 00:34:08.000]   point one. We're setting the learning rate here and the
[00:34:08.000 --> 00:34:11.960]   learning rate is defined above. It's always a good practice if
[00:34:11.960 --> 00:34:15.040]   you're new to machine learning to define variables like so you
[00:34:15.040 --> 00:34:18.880]   can imagine this being used 50 times inside of a deep neural
[00:34:18.880 --> 00:34:22.000]   network and you don't want to change that number everywhere
[00:34:22.000 --> 00:34:27.000]   you could find and replace but that's not ideal. Now we're
[00:34:27.000 --> 00:34:31.200]   ready to start set up the batch training loop. We say 40 is a
[00:34:31.200 --> 00:34:35.320]   good number. My intuition tells me that so we'll train it for
[00:34:35.320 --> 00:34:42.800]   40 times and we'll print the loss function every time. And
[00:34:42.800 --> 00:34:48.240]   we'll plot the classifier we've built and looks like it's
[00:34:48.240 --> 00:34:52.120]   learned pretty well it can divide these two categories
[00:34:52.120 --> 00:34:59.440]   quite well. That's a linear classifier using just TensorFlow.
[00:35:00.440 --> 00:35:06.000]   Any questions so far? I'll take a quick pause to check.
[00:35:26.440 --> 00:35:31.080]   Amazing. I'll continue further. I'm always nervous as well
[00:35:31.080 --> 00:35:33.840]   because there's like a 30 second delay between me asking the
[00:35:33.840 --> 00:35:38.720]   question and then the chat coming in. This is quite vanilla
[00:35:38.720 --> 00:35:41.280]   stuff. So I apologize to the advanced people and to the
[00:35:41.280 --> 00:35:44.160]   beginners. I'm taking a pause for you because I want to
[00:35:44.160 --> 00:35:50.720]   welcome your questions. Now let's understand the core Keras
[00:35:50.720 --> 00:35:53.640]   API, which is the most exciting thing. I assume everyone is
[00:35:53.640 --> 00:35:58.160]   here to learn about Keras. So inside of the chapter, they
[00:35:58.160 --> 00:36:00.920]   explain why layers are the building blocks of deep
[00:36:00.920 --> 00:36:06.840]   learning. And we learn about the base layer class in Keras. If
[00:36:06.840 --> 00:36:11.080]   you want to learn more about it, let's head over to the docs and
[00:36:11.080 --> 00:36:24.240]   see what we can learn from there. Layers are the, it just
[00:36:24.240 --> 00:36:28.440]   says the same thing. A layer consists of a tensor in tensor
[00:36:28.440 --> 00:36:33.760]   out computation. So it takes a tensor and gives out a tensor
[00:36:34.600 --> 00:36:41.520]   does some computation on it. It has a call method and some
[00:36:41.520 --> 00:36:47.560]   state that is stored. So it remembers the numbers. A layer
[00:36:47.560 --> 00:36:50.560]   instance is callable just like a function. So we could define a
[00:36:50.560 --> 00:36:56.560]   layer, a tense layer having 32 units like so and tell it to
[00:36:56.560 --> 00:37:02.040]   have an activation of a relu. We can then define the inputs and
[00:37:02.040 --> 00:37:06.120]   outputs and outputs are defined by passing the inputs to the
[00:37:06.120 --> 00:37:12.400]   layer. So that's a vanilla introduction to it. We just
[00:37:12.400 --> 00:37:16.960]   mentioned earlier maintains a state and the state would store
[00:37:16.960 --> 00:37:23.400]   the kernel and bias inside of it. You could also create custom
[00:37:23.400 --> 00:37:27.600]   layers and there is more in the docs that you can check out to
[00:37:27.600 --> 00:37:32.000]   learn about this. So that's that's a introduction to layers.
[00:37:32.000 --> 00:37:35.120]   Now we know they're like layers inside of a neural network. So
[00:37:35.120 --> 00:37:39.720]   let's see the example from the book chapter. If you were here
[00:37:39.720 --> 00:37:42.800]   in the previous session, we built a naive tense model where
[00:37:42.800 --> 00:37:46.880]   we took tensorflow and from scratch quite painfully with a
[00:37:46.880 --> 00:37:51.240]   lot of lines of code implemented a naive tense model. Here we'll
[00:37:51.240 --> 00:37:54.560]   take the liberty because now we're a qualified machine
[00:37:54.560 --> 00:37:57.280]   learning engineer who just made it to the phone screen interview
[00:37:57.280 --> 00:38:02.560]   of classifying two clusters with a linear classifier. We'll use
[00:38:02.560 --> 00:38:07.440]   keras.layers and we'll define the simple dense model like so.
[00:38:07.440 --> 00:38:11.520]   So inside of the constructor we take in how many units do we
[00:38:11.520 --> 00:38:15.520]   want? Which activation function do we want? I would say you
[00:38:15.520 --> 00:38:18.840]   could also instead of non set this to I don't know relu or
[00:38:18.840 --> 00:38:24.360]   something. And then we build the model where we set the input
[00:38:24.360 --> 00:38:30.680]   dimension, the weights and biases. Every model is supposed
[00:38:30.680 --> 00:38:35.000]   to have a call function. So we'll define the call like so.
[00:38:35.000 --> 00:38:41.240]   Why is used to define most of the times the output in
[00:38:41.240 --> 00:38:44.600]   mathematical notation. So the output is the matrix
[00:38:44.600 --> 00:38:49.480]   multiplication of the inputs with the weights and we add a
[00:38:49.480 --> 00:38:53.960]   bias to it. If there is no activation, if there is an
[00:38:53.960 --> 00:38:58.840]   activation function, it's double negative. So not none. We
[00:38:58.840 --> 00:39:04.360]   assign the activation function and return the same. And now we
[00:39:04.360 --> 00:39:08.520]   can call this function. Again, this is a simple dense model
[00:39:08.520 --> 00:39:12.600]   because we can't train it. It just takes a pass and returns
[00:39:12.600 --> 00:39:16.680]   some values. Not the most useful, but quite a good
[00:39:16.680 --> 00:39:19.880]   tutorial to start on.
[00:39:22.300 --> 00:39:30.400]   One of the useful things is when you're defining layers,
[00:39:30.400 --> 00:39:33.760]   you don't need to rely on what would be the input size that
[00:39:33.760 --> 00:39:38.560]   would again restrict your code in weird ways or have you write
[00:39:38.560 --> 00:39:44.560]   a lot of checks. You don't want to do that. Machine learning
[00:39:44.560 --> 00:39:46.960]   engineers don't like doing that. Software engineers don't want
[00:39:46.960 --> 00:39:51.680]   to do that. But here's how it's useful. So if you want to just
[00:39:51.680 --> 00:39:55.520]   define a few layers inside of a sequential model, what's a
[00:39:55.520 --> 00:40:00.640]   sequential model? Let's first understand that.
[00:40:00.640 --> 00:40:07.500]   There's a sequential class.
[00:40:07.500 --> 00:40:17.020]   Groups a linear stack of layers into a model. Provides training
[00:40:17.020 --> 00:40:20.540]   and inference features on this model. Looks like it takes care
[00:40:20.540 --> 00:40:25.020]   of a lot of things. To me, it sounds like it'll just take
[00:40:25.020 --> 00:40:30.300]   whatever we tell it here and just stack them as a few layers.
[00:40:30.300 --> 00:40:34.300]   So all of the dense layers that we pass through the sequential
[00:40:34.300 --> 00:40:40.620]   function by either using add or inside of our definition here,
[00:40:40.620 --> 00:40:45.580]   passing them like so inside of a list, they'll get stacked
[00:40:45.580 --> 00:40:50.140]   together and it'll take care of the training inference features
[00:40:50.140 --> 00:40:55.260]   on this model. What I want you all to pay attention to is
[00:40:55.260 --> 00:40:58.620]   earlier we had to define the input and output shapes. We had
[00:40:58.620 --> 00:41:02.780]   to take care of those things. Here we don't and that's why
[00:41:02.780 --> 00:41:06.220]   it's really powerful and useful. You could define a custom
[00:41:06.220 --> 00:41:12.060]   sequential model that you derive the knowledge for like so.
[00:41:12.060 --> 00:41:15.740]   Where you pass simple dense models that we have just
[00:41:15.740 --> 00:41:18.700]   defined. So we're defining a custom model based on a custom
[00:41:18.700 --> 00:41:24.380]   class we had defined earlier and that should also work.
[00:41:24.380 --> 00:41:29.280]   So how do we get a model out of it? Now we've learned to
[00:41:29.280 --> 00:41:34.480]   create this simple dense model. How do we get the model that we
[00:41:34.480 --> 00:41:40.160]   can train and deploy in the real world? This is a bad
[00:41:40.160 --> 00:41:43.360]   example. You never would deploy such a model. Maybe you would,
[00:41:43.360 --> 00:41:47.440]   maybe you might, but again, this is for our understanding. So
[00:41:47.440 --> 00:41:52.400]   with that warning out of the way, we have defined a
[00:41:52.400 --> 00:41:57.920]   sequential function by calling just one dense layer. So we
[00:41:57.920 --> 00:42:03.440]   just passed it one dense layer with one unit inside of it. Now
[00:42:03.440 --> 00:42:09.680]   we'll compile the model. So let's see what compile does.
[00:42:14.940 --> 00:42:18.960]   The reason why I'm defaulting so much to Keras documentation
[00:42:18.960 --> 00:42:25.360]   is because I think it's a terrific, quite, I'm trying to
[00:42:25.360 --> 00:42:28.320]   think of terrifically written is that a word? I don't think
[00:42:28.320 --> 00:42:32.080]   it is, but it's written really well and I think it's a good
[00:42:32.080 --> 00:42:35.680]   starting point. So that's why I'm defaulting here again and
[00:42:35.680 --> 00:42:39.280]   again. It configures the model for training. That's the only
[00:42:39.280 --> 00:42:42.400]   thing we need to know. Now we're good to go. Let's see what
[00:42:42.400 --> 00:42:46.880]   arguments does it take. Optimizer, name of the
[00:42:46.880 --> 00:42:52.720]   optimizer or its instance, loss function, matrix that we want
[00:42:52.720 --> 00:42:57.440]   to evaluate during training and testing, loss weights. What are
[00:42:57.440 --> 00:42:59.680]   these?
[00:42:59.680 --> 00:43:05.220]   The loss value will be minimized by the model and then
[00:43:05.220 --> 00:43:10.260]   a weighted sum of all individual losses. So remember
[00:43:10.260 --> 00:43:13.780]   how I told you earlier, instead of looking up different
[00:43:13.780 --> 00:43:17.220]   theoretical concepts inside of the documentation, you'll find
[00:43:17.220 --> 00:43:20.500]   more practical knowledge. Now if you're curious more about
[00:43:20.500 --> 00:43:26.100]   this, call it inside of this simple function and print it
[00:43:26.100 --> 00:43:29.140]   out every time. See what it's doing. That'll
[00:43:29.140 --> 00:43:33.620]   take you further than just understanding the
[00:43:33.620 --> 00:43:35.940]   theory.
[00:43:35.940 --> 00:43:40.200]   So there are a few more
[00:43:40.200 --> 00:43:45.440]   arguments that you can pass. Then there is another function
[00:43:45.440 --> 00:43:49.200]   I'm continuing because this is also there in the example. Fit
[00:43:49.200 --> 00:43:52.720]   method is similar to the cyclic learn fit method. So now we
[00:43:52.720 --> 00:43:55.680]   have configured the model for training. We would like to
[00:43:55.680 --> 00:43:58.960]   train it. So this trains the model for a fixed number of
[00:43:58.960 --> 00:44:04.240]   epochs on the dataset. It can again take all of these
[00:44:04.240 --> 00:44:08.560]   arguments that I invite you to check out after today's
[00:44:08.560 --> 00:44:12.320]   live stream or in your free time.
[00:44:12.320 --> 00:44:15.620]   And one more method that we need to learn about is the
[00:44:15.620 --> 00:44:18.500]   evaluate method. So this returns the loss value and
[00:44:18.500 --> 00:44:24.580]   matrix for the model in test mode. That's the keyword. So
[00:44:24.580 --> 00:44:30.740]   so far the fit would just do it on the training dataset. This
[00:44:30.740 --> 00:44:34.660]   would be the test mode where we adjust as the name gives away
[00:44:34.660 --> 00:44:39.460]   evaluating the model on a test dataset.
[00:44:39.460 --> 00:44:44.080]   And then there's a predict method as well, which
[00:44:44.080 --> 00:44:47.760]   assumably after you have a trained model, you can generate
[00:44:47.760 --> 00:44:52.160]   predictions from it and you would call it to just get those
[00:44:52.160 --> 00:44:56.240]   values. So these are the four or five functions that I
[00:44:56.240 --> 00:45:01.840]   wanted to point out. Now we compile our model, tell it
[00:45:01.840 --> 00:45:08.320]   please use the optimizer, sorry, RMS prop. Please use the
[00:45:08.320 --> 00:45:11.680]   loss value mean squared error. See how simple our life is now.
[00:45:11.680 --> 00:45:15.520]   Earlier we had to define all of this quite painfully, but now
[00:45:15.520 --> 00:45:18.480]   since we've passed the phone screen test, we can do all of
[00:45:18.480 --> 00:45:22.960]   these things with Keras. Then we can simply compile the model.
[00:45:22.960 --> 00:45:29.680]   By passing these, you could also grab these like so if you
[00:45:29.680 --> 00:45:34.320]   want. So inside of the Keras modules, you can call these
[00:45:34.320 --> 00:45:42.400]   different things. I'm trying to recall if anything important is
[00:45:42.400 --> 00:45:46.560]   here. Franchot mentions inside of this chapter how you can
[00:45:46.560 --> 00:45:49.520]   pick a loss function for different things. Here's
[00:45:49.520 --> 00:45:55.840]   another way of doing the same. If you want to learn what loss
[00:45:55.840 --> 00:46:00.000]   function should you apply to a particular problem, go to Kaggle
[00:46:00.000 --> 00:46:03.840]   find a similar dataset. So let's say you want to protect
[00:46:03.840 --> 00:46:09.440]   reefs as well and you have a similar dataset to this one.
[00:46:09.440 --> 00:46:15.200]   Look at the dataset and see how it's being evaluated. And
[00:46:15.200 --> 00:46:18.720]   there you go. You learn about something new and you can use
[00:46:18.720 --> 00:46:22.000]   the score and see if you're evaluating well. So if you want
[00:46:22.000 --> 00:46:25.920]   to figure out what matrix should you use, what loss function
[00:46:25.920 --> 00:46:29.280]   should you use, this is a good way of evaluating the same.
[00:46:29.280 --> 00:46:39.360]   Can you provide, I'm sorry, I'm looking at the chat. Can you
[00:46:39.360 --> 00:46:44.080]   provide the link to the Keras recap report? Sure. Let me
[00:46:44.080 --> 00:46:46.960]   quickly grab that.
[00:46:46.960 --> 00:46:59.200]   I had just shared you Raj's book. So I'm trying to find that
[00:46:59.200 --> 00:47:13.440]   real quickly. This would be the recap. It is there in the
[00:47:13.440 --> 00:47:24.320]   YouTube chart and let me also put it in the platform chart.
[00:47:25.200 --> 00:47:30.880]   Okay. I've shared the chart and I hope the person is not upset.
[00:47:30.880 --> 00:47:33.280]   I want to make sure they know the link.
[00:47:33.280 --> 00:47:41.760]   Coming back to this example, that's how you can pick, sorry,
[00:47:41.760 --> 00:47:46.320]   pick the right data set. So you can pick the right data set.
[00:47:46.320 --> 00:47:50.320]   So let's say you want to pick the right data set. So you can
[00:47:50.320 --> 00:47:54.320]   pick the right data set. So let's say you want to pick the
[00:47:54.320 --> 00:47:58.320]   right data set. So let's say you want to pick the right data
[00:47:58.320 --> 00:48:02.320]   set. So let's say you want to pick the right data set. So let's
[00:48:02.320 --> 00:48:06.320]   say you want to pick the right data set. So let's say you want
[00:48:06.320 --> 00:48:10.320]   to pick the right data set. So let's say you want to pick the
[00:48:10.320 --> 00:48:14.320]   right data set. So let's say you want to pick the right data set.
[00:48:14.320 --> 00:48:18.320]   So let's say you want to pick the right data set. So let's
[00:48:18.320 --> 00:48:22.320]   say you want to pick the right data set. So let's say you want
[00:48:22.320 --> 00:48:26.320]   to pick the right data set. So let's say you want to pick the
[00:48:26.320 --> 00:48:30.320]   right data set. So let's say you want to pick the right data set.
[00:48:30.320 --> 00:48:34.320]   So let's say you want to pick the right data set. So let's say
[00:48:34.320 --> 00:48:38.320]   you want to pick the right data set. So let's say you want to
[00:48:38.320 --> 00:48:42.320]   pick the right data set. So let's say you want to pick the right
[00:48:42.320 --> 00:48:46.320]   data set. So let's say you want to pick the right data set. So
[00:48:46.320 --> 00:48:50.320]   let's say you want to pick the right data set. So let's say you
[00:48:50.320 --> 00:48:54.320]   want to pick the right data set. So let's say you want to pick the
[00:48:54.320 --> 00:48:58.320]   right data set. So let's say you want to pick the right data set.
[00:48:58.320 --> 00:49:02.320]   So let's say you want to pick the right data set. So let's say you
[00:49:02.320 --> 00:49:06.320]   want to pick the right data set. So let's say you want to pick the
[00:49:06.320 --> 00:49:10.320]   right data set. So let's say you want to pick the right data set.
[00:49:10.320 --> 00:49:14.320]   So let's say you want to pick the right data set. So let's say you
[00:49:14.320 --> 00:49:18.320]   want to pick the right data set. So let's say you want to pick the
[00:49:18.320 --> 00:49:22.320]   right data set. So let's say you want to pick the right data set.
[00:49:22.320 --> 00:49:26.320]   So let's say you want to pick the right data set. So let's say you
[00:49:26.320 --> 00:49:30.320]   want to pick the right data set. So let's say you want to pick the
[00:49:30.320 --> 00:49:34.320]   right data set. So let's say you want to pick the right data set.
[00:49:34.320 --> 00:49:38.320]   So let's say you want to pick the right data set. So let's say you
[00:49:38.320 --> 00:49:42.320]   want to pick the right data set. So let's say you want to pick the
[00:49:42.320 --> 00:49:46.320]   right data set. So let's say you want to pick the right data set.
[00:49:46.320 --> 00:49:50.320]   So let's say you want to pick the right data set. So let's say you
[00:49:50.320 --> 00:49:54.320]   want to pick the right data set. So let's say you want to pick the
[00:49:54.320 --> 00:49:58.320]   right data set. So let's say you want to pick the right data set.
[00:49:58.320 --> 00:50:02.320]   So let's say you want to pick the right data set. So let's say you
[00:50:02.320 --> 00:50:06.320]   want to pick the right data set. So let's say you want to pick the
[00:50:06.320 --> 00:50:10.320]   right data set. So let's say you want to pick the right data set.
[00:50:10.320 --> 00:50:14.320]   So let's say you want to pick the right data set. So let's say you
[00:50:14.320 --> 00:50:18.320]   want to pick the right data set. So let's say you want to pick the
[00:50:18.320 --> 00:50:22.320]   right data set. So let's say you want to pick the right data set.
[00:50:22.320 --> 00:50:26.320]   So let's say you want to pick the right data set. So let's say you
[00:50:26.320 --> 00:50:30.320]   want to pick the right data set. So let's say you want to pick the
[00:50:30.320 --> 00:50:34.320]   right data set. So let's say you want to pick the right data set.
[00:50:34.320 --> 00:50:38.320]   So let's say you want to pick the right data set. So let's say you
[00:50:38.320 --> 00:50:42.320]   want to pick the right data set. So let's say you want to pick the
[00:50:42.320 --> 00:50:46.320]   right data set. So let's say you want to pick the right data set.
[00:50:46.320 --> 00:50:50.320]   So let's say you want to pick the right data set. So let's say you
[00:50:50.320 --> 00:50:54.320]   want to pick the right data set. So let's say you want to pick the
[00:50:54.320 --> 00:50:58.320]   right data set. So let's say you want to pick the right data set.
[00:50:58.320 --> 00:51:02.320]   So let's say you want to pick the right data set. So let's say you
[00:51:02.320 --> 00:51:06.320]   want to pick the right data set. So let's say you want to pick the
[00:51:06.320 --> 00:51:10.320]   right data set. So let's say you want to pick the right data set.
[00:51:10.320 --> 00:51:14.320]   So let's say you want to pick the right data set. So let's say you
[00:51:14.320 --> 00:51:18.320]   want to pick the right data set. So let's say you want to pick the
[00:51:18.320 --> 00:51:22.320]   right data set. So let's say you want to pick the right data set.
[00:51:22.320 --> 00:51:26.320]   So let's say you want to pick the right data set. So let's say you
[00:51:26.320 --> 00:51:30.320]   want to pick the right data set. So let's say you want to pick the
[00:51:30.320 --> 00:51:34.320]   right data set. So let's say you want to pick the right data set.
[00:51:34.320 --> 00:51:38.320]   So let's say you want to pick the right data set. So let's say you
[00:51:38.320 --> 00:51:42.320]   want to pick the right data set. So let's say you want to pick the
[00:51:42.320 --> 00:51:46.320]   right data set. So let's say you want to pick the right data set.
[00:51:46.320 --> 00:51:50.320]   So let's say you want to pick the right data set. So let's say you
[00:51:50.320 --> 00:51:54.320]   want to pick the right data set. So let's say you want to pick the
[00:51:54.320 --> 00:51:58.320]   right data set. So let's say you want to pick the right data set.
[00:51:58.320 --> 00:52:02.320]   So let's say you want to pick the right data set. So let's say you
[00:52:02.320 --> 00:52:06.320]   want to pick the right data set. So let's say you want to pick the
[00:52:06.320 --> 00:52:10.320]   right data set. So let's say you want to pick the right data set.
[00:52:10.320 --> 00:52:14.320]   So let's say you want to pick the right data set. So let's say you
[00:52:14.320 --> 00:52:18.320]   want to pick the right data set. So let's say you want to pick the
[00:52:18.320 --> 00:52:22.320]   right data set. So let's say you want to pick the right data set.
[00:52:22.320 --> 00:52:26.320]   So let's say you want to pick the right data set. So let's say you
[00:52:26.320 --> 00:52:30.320]   want to pick the right data set. So let's say you want to pick the
[00:52:30.320 --> 00:52:34.320]   right data set. So let's say you want to pick the right data set.
[00:52:34.320 --> 00:52:38.320]   So let's say you want to pick the right data set. So let's say you
[00:52:38.320 --> 00:52:42.320]   want to pick the right data set. So let's say you want to pick the
[00:52:42.320 --> 00:52:46.320]   right data set. So let's say you want to pick the right data set.
[00:52:46.320 --> 00:52:50.320]   So let's say you want to pick the right data set. So let's say you
[00:52:50.320 --> 00:52:54.320]   want to pick the right data set. So let's say you want to pick the
[00:52:54.320 --> 00:52:58.320]   right data set. So let's say you want to pick the right data set.
[00:52:58.320 --> 00:53:02.320]   So let's say you want to pick the right data set. So let's say you
[00:53:02.320 --> 00:53:06.320]   want to pick the right data set. So let's say you want to pick the
[00:53:06.320 --> 00:53:10.320]   right data set. So let's say you want to pick the right data set.
[00:53:10.320 --> 00:53:14.320]   So let's say you want to pick the right data set. So let's say you
[00:53:14.320 --> 00:53:18.320]   want to pick the right data set. So let's say you want to pick the
[00:53:18.320 --> 00:53:22.320]   right data set. So let's say you want to pick the right data set.
[00:53:22.320 --> 00:53:26.320]   So let's say you want to pick the right data set. So let's say you
[00:53:26.320 --> 00:53:30.320]   want to pick the right data set. So let's say you want to pick the
[00:53:30.320 --> 00:53:34.320]   right data set. So let's say you want to pick the right data set.
[00:53:34.320 --> 00:53:38.320]   So let's say you want to pick the right data set. So let's say you
[00:53:38.320 --> 00:53:42.320]   want to pick the right data set. So let's say you want to pick the
[00:53:42.320 --> 00:53:46.320]   right data set. So let's say you want to pick the right data set.
[00:53:46.320 --> 00:53:50.320]   So let's say you want to pick the right data set. So let's say you
[00:53:50.320 --> 00:53:54.320]   want to pick the right data set. So let's say you want to pick the
[00:53:54.320 --> 00:53:58.320]   right data set. So let's say you want to pick the right data set.
[00:53:58.320 --> 00:54:02.320]   So let's say you want to pick the right data set. So let's say you
[00:54:02.320 --> 00:54:06.320]   want to pick the right data set. So let's say you want to pick the
[00:54:06.320 --> 00:54:10.320]   right data set. So let's say you want to pick the right data set.
[00:54:10.320 --> 00:54:14.320]   So let's say you want to pick the right data set. So let's say you
[00:54:14.320 --> 00:54:18.320]   want to pick the right data set. So let's say you want to pick the
[00:54:18.320 --> 00:54:22.320]   right data set. So let's say you want to pick the right data set.
[00:54:22.320 --> 00:54:26.320]   So let's say you want to pick the right data set. So let's say you
[00:54:26.320 --> 00:54:30.320]   want to pick the right data set. So let's say you want to pick the
[00:54:30.320 --> 00:54:34.320]   right data set. So let's say you want to pick the right data set.
[00:54:34.320 --> 00:54:38.320]   So let's say you want to pick the right data set. So let's say you
[00:54:38.320 --> 00:54:42.320]   want to pick the right data set. So let's say you want to pick the
[00:54:42.320 --> 00:54:46.320]   right data set. So let's say you want to pick the right data set.
[00:54:46.320 --> 00:54:50.320]   So let's say you want to pick the right data set. So let's say you
[00:54:50.320 --> 00:54:54.320]   want to pick the right data set. So let's say you want to pick the
[00:54:54.320 --> 00:54:58.320]   right data set. So let's say you want to pick the right data set.
[00:54:58.320 --> 00:55:02.320]   So let's say you want to pick the right data set. So let's say you
[00:55:02.320 --> 00:55:06.320]   want to pick the right data set. So let's say you want to pick the
[00:55:06.320 --> 00:55:10.320]   right data set. So let's say you want to pick the right data set.
[00:55:10.320 --> 00:55:14.320]   So let's say you want to pick the right data set. So let's say you
[00:55:14.320 --> 00:55:18.320]   want to pick the right data set. So let's say you want to pick the
[00:55:18.320 --> 00:55:22.320]   right data set. So let's say you want to pick the right data set.
[00:55:22.320 --> 00:55:26.320]   So let's say you want to pick the right data set. So let's say you
[00:55:26.320 --> 00:55:30.320]   want to pick the right data set. So let's say you want to pick the
[00:55:30.320 --> 00:55:34.320]   right data set. So let's say you want to pick the right data set.
[00:55:34.320 --> 00:55:38.320]   So let's say you want to pick the right data set. So let's say you
[00:55:38.320 --> 00:55:42.320]   want to pick the right data set. So let's say you want to pick the
[00:55:42.320 --> 00:55:46.320]   right data set. So let's say you want to pick the right data set.
[00:55:46.320 --> 00:55:50.320]   So let's see what's inside of our train data. It's a lot of numbers
[00:55:50.320 --> 00:55:54.320]   and these numbers would be mapping to a particular word out of the
[00:55:54.320 --> 00:55:58.320]   top 10,000 frequent words inside of our data set. So that's
[00:55:58.320 --> 00:56:02.320]   what this model is doing.
[00:56:02.320 --> 00:56:06.320]   And then we
[00:56:06.320 --> 00:56:10.320]   can take a look at the label as well. So the label for this
[00:56:10.320 --> 00:56:14.320]   particular example is I think positive.
[00:56:14.320 --> 00:56:18.320]   We can
[00:56:18.320 --> 00:56:22.320]   also decode these back and take a look at how long does it take.
[00:56:22.320 --> 00:56:26.320]   Does anyone want to answer why are we subtracting 3 here?
[00:56:26.320 --> 00:56:30.320]   So when we reverse the word indices, we are
[00:56:30.320 --> 00:56:34.320]   creating a dictionary where we are passing the
[00:56:34.320 --> 00:56:38.320]   items and also the decoded version of the review.
[00:56:38.320 --> 00:56:42.320]   So when we are trying to get the indices, does anyone
[00:56:42.320 --> 00:56:46.320]   want to answer why we subtract 3 from here?
[00:57:10.320 --> 00:57:14.320]   It's like I'm living in the future because I'm 30 seconds ahead of the live stream and I have
[00:57:14.320 --> 00:57:18.320]   to wait for the chat to come in.
[00:57:18.320 --> 00:57:26.320]   So whenever we take words and encode them as numbers,
[00:57:26.320 --> 00:57:30.320]   there are three things that are always common. So "unk" which is
[00:57:30.320 --> 00:57:34.320]   unknown, "pad" and
[00:57:34.320 --> 00:57:38.320]   another one which I'm forgetting right now. So a suggestion,
[00:57:38.320 --> 00:57:42.320]   please print these out and see what those are. But there are
[00:57:42.320 --> 00:57:46.320]   three tokens which are unknown,
[00:57:46.320 --> 00:57:50.320]   "pad" and I'm forgetting the third one that
[00:57:50.320 --> 00:57:54.320]   are always encoded. So that's why we are subtracting 3 here.
[00:57:54.320 --> 00:58:02.320]   Inside of this chapter, what we are really learning is how
[00:58:02.320 --> 00:58:06.320]   to create a model that can work with
[00:58:06.320 --> 00:58:10.320]   classification and regression problems. So this is a classification
[00:58:10.320 --> 00:58:14.320]   problem because what we want to achieve here is, we looked at the IMDB
[00:58:14.320 --> 00:58:18.320]   dataset and using those movie reviews, we want to classify
[00:58:18.320 --> 00:58:22.320]   by just looking at the words and telling if
[00:58:22.320 --> 00:58:26.320]   it's a positive or a negative review for a given movie.
[00:58:26.320 --> 00:58:30.320]   So we encode those integer sequences.
[00:58:30.320 --> 00:58:34.320]   Even though we have numbers, we can't simply input them to our
[00:58:34.320 --> 00:58:38.320]   neural network. We need to find a way of creating a tensor
[00:58:38.320 --> 00:58:42.320]   and we do that by multi-hot encoding them.
[00:58:42.320 --> 00:58:46.320]   So we define a function that
[00:58:46.320 --> 00:58:50.320]   we call vectorize_sequences. It gives
[00:58:50.320 --> 00:58:54.320]   it away but we want to vectorize, create vectors out of the input sequences.
[00:58:54.320 --> 00:58:58.320]   And the result is a 0 having
[00:58:58.320 --> 00:59:02.320]   numpy array having all 0s with the dimension
[00:59:02.320 --> 00:59:06.320]   being the length of sequence and 10,000 words.
[00:59:06.320 --> 00:59:10.320]   So we are trying to encode all of the 10,000 most frequent words into this.
[00:59:10.320 --> 00:59:14.320]   And we do that
[00:59:14.320 --> 00:59:18.320]   by simply setting those particular values
[00:59:18.320 --> 00:59:22.320]   1 wherever that word occurs.
[00:59:22.320 --> 00:59:26.320]   Now we can print. I'm assuming
[00:59:26.320 --> 00:59:30.320]   everyone knows what multi-hot encoding is. If not, please
[00:59:30.320 --> 00:59:34.320]   ask a question and I'll explain it further.
[00:59:34.320 --> 00:59:38.320]   Now we can take a look at the train
[00:59:38.320 --> 00:59:42.320]   array I believe. Yes, it's an array.
[00:59:42.320 --> 00:59:46.320]   And we can define the test
[00:59:46.320 --> 00:59:50.320]   one like so by calling the test labels.
[00:59:50.320 --> 00:59:54.320]   And we are ready to build our first
[00:59:54.320 --> 00:59:58.320]   classification model. It's a simple model having
[00:59:58.320 --> 01:00:02.320]   stacks of layers because we just learned what sequential is.
[01:00:02.320 --> 01:00:06.320]   Where we call all of these dense layers and define the activation
[01:00:06.320 --> 01:00:10.320]   functions like so. Inside of the chapter where
[01:00:10.320 --> 01:00:14.320]   they explained how you should call activation functions,
[01:00:14.320 --> 01:00:18.320]   they gave you the intuition of which ones should you use. So assuming you are
[01:00:18.320 --> 01:00:22.320]   aware of that, we'll call sigmoid here.
[01:00:22.320 --> 01:00:26.320]   And now we are ready to compile the model, set aside a validation
[01:00:26.320 --> 01:00:30.320]   dataset, fit it and there we go. We have
[01:00:30.320 --> 01:00:34.320]   an accurate model that works really well on
[01:00:34.320 --> 01:00:38.320]   movie reviews.
[01:00:38.320 --> 01:00:42.320]   The thing that the chapter teaches you is also
[01:00:42.320 --> 01:00:46.320]   overfitting and underfitting.
[01:00:46.320 --> 01:00:50.320]   So the dots represent the training loss and so far
[01:00:50.320 --> 01:00:54.320]   we've just been concerned with the training loss, right? But we just learned about validation
[01:00:54.320 --> 01:00:58.320]   dataset and I gave you the homework of reading this incredible
[01:00:58.320 --> 01:01:02.320]   blog post by Rachel Thomas which will tell you the importance
[01:01:02.320 --> 01:01:06.320]   of this but I'll give you the summary.
[01:01:06.320 --> 01:01:10.320]   We want the loss to be minimal.
[01:01:10.320 --> 01:01:14.320]   Not just on the training dataset but also on the validation dataset.
[01:01:14.320 --> 01:01:18.320]   Our validation dataset is a proxy to the real world and will tell our
[01:01:18.320 --> 01:01:22.320]   team how good is the model
[01:01:22.320 --> 01:01:26.320]   performing in the real world. So models are really prone to just learning
[01:01:26.320 --> 01:01:30.320]   the details of your dataset and we don't want that
[01:01:30.320 --> 01:01:34.320]   happening. How do we avoid that? We
[01:01:34.320 --> 01:01:38.320]   use a bunch of tricks that we'll get to later
[01:01:38.320 --> 01:01:42.320]   but there are ways of avoiding this and that's one thing you should be always careful about.
[01:01:42.320 --> 01:01:46.320]   You don't want your model overfitting. We've looked at this graph
[01:01:46.320 --> 01:01:50.320]   here's how you understand model is overfitting. The validation loss
[01:01:50.320 --> 01:01:54.320]   is much higher than your training loss. So that's a sign
[01:01:54.320 --> 01:01:58.320]   of overfitting. Another sign, it first
[01:01:58.320 --> 01:02:02.320]   improves. So the loss improves which means that the
[01:02:02.320 --> 01:02:06.320]   loss value goes down but then it gets forced. So that's a sign
[01:02:06.320 --> 01:02:10.320]   of overfitting.
[01:02:10.320 --> 01:02:18.320]   I think I'll skip these further because I don't think there's
[01:02:18.320 --> 01:02:22.320]   something I want to point out from here.
[01:02:22.320 --> 01:02:26.320]   The next one is a multi-class classification.
[01:02:26.320 --> 01:02:30.320]   So we use the routers dataset. I'll leave
[01:02:30.320 --> 01:02:34.320]   you to check out what that is by looking inside of the documentation. All of the
[01:02:34.320 --> 01:02:38.320]   steps here would be quite similar to some extent.
[01:02:38.320 --> 01:02:42.320]   We get the word indexes. We also
[01:02:42.320 --> 01:02:46.320]   see how long does it take to reverse it. Remember those three words that we always subtract.
[01:02:46.320 --> 01:02:50.320]   We do that to decode the dataset back.
[01:02:50.320 --> 01:02:54.320]   We set a train and dataset. We one-hot encode
[01:02:54.320 --> 01:02:58.320]   the labels.
[01:02:58.320 --> 01:03:02.320]   And this time we call something new. We call a util
[01:03:02.320 --> 01:03:06.320]   that is two categorical. So we tell Keras
[01:03:06.320 --> 01:03:10.320]   please Keras, these are not just numbers, these are categories because
[01:03:10.320 --> 01:03:14.320]   remember we are looking at multi-categories now.
[01:03:14.320 --> 01:03:18.320]   And this time we build a model with a softmax layer at the end.
[01:03:18.320 --> 01:03:22.320]   And now we can do the same things, validate our approach.
[01:03:22.320 --> 01:03:30.320]   And the model would be trained. If you want to test
[01:03:30.320 --> 01:03:34.320]   how well does this model perform, you can call
[01:03:34.320 --> 01:03:38.320]   the predict function which we had looked at earlier.
[01:03:38.320 --> 01:03:42.320]   And you can call it on your test set from where you can get the test accuracy.
[01:03:42.320 --> 01:03:46.320]   It is a sin. You will go to the hell of machine learning
[01:03:46.320 --> 01:03:50.320]   to train your model on the test dataset. You never fine-tune it on the test set.
[01:03:50.320 --> 01:03:54.320]   Always avoid that. Write that down.
[01:03:54.320 --> 01:03:58.320]   Never make that mistake. That is a huge red flag and never do that.
[01:03:58.320 --> 01:04:02.320]   So we are just evaluating by predicting here.
[01:04:02.320 --> 01:04:06.320]   Never train on your test set.
[01:04:06.320 --> 01:04:10.320]   Scrolling further,
[01:04:10.320 --> 01:04:14.320]   I am just making sure if there is anything else that I want to point out.
[01:04:14.320 --> 01:04:18.320]   There is another example that is a regression problem.
[01:04:18.320 --> 01:04:22.320]   It is well documented so that is why I am just pointing out
[01:04:22.320 --> 01:04:26.320]   the little details inside of the book. There is well documentation around it.
[01:04:26.320 --> 01:04:30.320]   So now we are tasked with predicting housing prices.
[01:04:30.320 --> 01:04:34.320]   And this again comes from the prediction model.
[01:04:34.320 --> 01:04:38.320]   And this again comes from the data set. You can look inside of the documentation.
[01:04:38.320 --> 01:04:42.320]   So there are about 400
[01:04:42.320 --> 01:04:46.320]   training examples, 100 test examples. Looks like a good number.
[01:04:46.320 --> 01:04:50.320]   This time we want to
[01:04:50.320 --> 01:04:54.320]   prepare the data by normalizing it.
[01:04:54.320 --> 01:04:58.320]   So we do that by dividing with the standard deviation and subtracting the mean.
[01:05:02.320 --> 01:05:06.320]   And we build another sequential model.
[01:05:06.320 --> 01:05:10.320]   This time there is no activation function on the last layer.
[01:05:10.320 --> 01:05:14.320]   It just gives you a number because we want to
[01:05:14.320 --> 01:05:18.320]   predict the price of the house. So hopefully our model will do a good job
[01:05:18.320 --> 01:05:22.320]   of just predicting that.
[01:05:22.320 --> 01:05:26.320]   Inside of this book, inside of this chapter we learn
[01:05:26.320 --> 01:05:30.320]   about k-fold validation. So if you have a
[01:05:30.320 --> 01:05:34.320]   smaller number of examples and you still
[01:05:34.320 --> 01:05:38.320]   want to set aside a validation data set.
[01:05:38.320 --> 01:05:42.320]   See the problem here is 30%
[01:05:42.320 --> 01:05:46.320]   of something small is very small.
[01:05:46.320 --> 01:05:50.320]   So you don't want to do that.
[01:05:50.320 --> 01:05:54.320]   Here we will use something known as k-fold validation
[01:05:54.320 --> 01:05:58.320]   where we will take a data set
[01:05:58.320 --> 01:06:02.320]   I'm trying to see if there is some cloth around here but basically you
[01:06:02.320 --> 01:06:06.320]   fold your data set. That's how I understand it.
[01:06:06.320 --> 01:06:10.320]   Stop my screen share.
[01:06:10.320 --> 01:06:14.320]   Sorry I just
[01:06:14.320 --> 01:06:18.320]   disconnected my headphone. Give me one second.
[01:06:18.320 --> 01:06:26.320]   I can just get rid of them.
[01:06:26.320 --> 01:06:30.320]   We take a data set. Hopefully it's not a jacket.
[01:06:30.320 --> 01:06:34.320]   There is a question by KT what is a small
[01:06:34.320 --> 01:06:38.320]   data set. I'll just come to that. And k-fold validation
[01:06:38.320 --> 01:06:42.320]   folds your data set into different parts.
[01:06:42.320 --> 01:06:46.320]   And then you train on these different folds by using
[01:06:46.320 --> 01:06:50.320]   a way of selecting them. That is k-fold validation.
[01:06:50.320 --> 01:06:54.320]   So the number of folds is how many times you fold your data set.
[01:06:54.320 --> 01:06:58.320]   Define the boundaries and we
[01:06:58.320 --> 01:07:02.320]   train our model on these different folds. All of which
[01:07:02.320 --> 01:07:06.320]   have a validation data set inside of them.
[01:07:06.320 --> 01:07:10.320]   And the final validation accuracy or loss
[01:07:10.320 --> 01:07:14.320]   is an average of all of these.
[01:07:14.320 --> 01:07:18.320]   Great question by KT. What is a small data set?
[01:07:18.320 --> 01:07:22.320]   I'm not an expert to answer this question.
[01:07:22.320 --> 01:07:26.320]   I've not put many models in production.
[01:07:26.320 --> 01:07:30.320]   I have spent more time sharing these resources.
[01:07:30.320 --> 01:07:34.320]   But with that
[01:07:34.320 --> 01:07:38.320]   caution out of the way.
[01:07:38.320 --> 01:07:42.320]   You always want to set aside 30%
[01:07:42.320 --> 01:07:46.320]   as a rough metric of your training data set
[01:07:46.320 --> 01:07:50.320]   for validation. If your data set
[01:07:50.320 --> 01:07:54.320]   has 1 million examples, 30% of that is a huge number.
[01:07:54.320 --> 01:07:58.320]   Relatively speaking.
[01:07:58.320 --> 01:08:02.320]   So there you would set aside a smaller percentage
[01:08:02.320 --> 01:08:06.320]   of the validation data set. How do you understand this? It comes through experimentation.
[01:08:06.320 --> 01:08:10.320]   100 house examples
[01:08:10.320 --> 01:08:14.320]   would be a small number I would say to predict.
[01:08:14.320 --> 01:08:18.320]   But one thing that Jeremy Howard has taught
[01:08:18.320 --> 01:08:22.320]   me in FastAI. He mentioned this in one of the lectures.
[01:08:22.320 --> 01:08:26.320]   You would always overestimate how much data do you need.
[01:08:26.320 --> 01:08:30.320]   So there have been blog posts by many of my FastAI
[01:08:30.320 --> 01:08:34.320]   classmates that showcase you that even with 30 examples you can do really well.
[01:08:34.320 --> 01:08:38.320]   So always start with whatever data you have and go from there.
[01:08:38.320 --> 01:08:42.320]   Praveen will always remember
[01:08:42.320 --> 01:08:46.320]   the K-fold example with my jacket.
[01:08:46.320 --> 01:08:50.320]   I think he is quite happy to make a contribution in your learning.
[01:08:50.320 --> 01:08:54.320]   Let me share my screen again. Select the right window
[01:08:54.320 --> 01:08:58.320]   which requires a lot of complicated
[01:08:58.320 --> 01:09:02.320]   head scratching and hopefully I do the
[01:09:02.320 --> 01:09:06.320]   correct screen. I do select the correct screen.
[01:09:06.320 --> 01:09:10.320]   Awesome. I will continue further.
[01:09:10.320 --> 01:09:14.320]   It's a bit tricky talking to the audience
[01:09:14.320 --> 01:09:18.320]   and also doing an operation. I am bad at multitasking. Sorry for this.
[01:09:18.320 --> 01:09:22.320]   So this is what K-fold validation is. I wanted to give
[01:09:22.320 --> 01:09:26.320]   that example out there. And this is what we are doing in code here.
[01:09:26.320 --> 01:09:30.320]   I will glance over the details. But we set aside
[01:09:30.320 --> 01:09:34.320]   four divisions inside of the data set and then we average
[01:09:34.320 --> 01:09:38.320]   across all of these for our matrix. So we print out
[01:09:38.320 --> 01:09:42.320]   all of these scores and then we call np.mean. What is that? Put a question mark
[01:09:42.320 --> 01:09:46.320]   and check what that is. It will tell you the details
[01:09:46.320 --> 01:09:50.320]   and tell you all of the magic. And looks like you can pass a list and get
[01:09:50.320 --> 01:09:54.320]   a mean out of it. So maybe you don't need to look at it.
[01:09:54.320 --> 01:09:58.320]   It takes a mean of all of these and that's the number that we care about.
[01:09:58.320 --> 01:10:06.320]   I just wanted to point this detail out again being
[01:10:06.320 --> 01:10:10.320]   conscious of everyone's time. That concludes the chapter 4
[01:10:10.320 --> 01:10:14.320]   summary. I still have the ambition to go through chapter 5.
[01:10:14.320 --> 01:10:18.320]   So let me see if I can get to that.
[01:10:18.320 --> 01:10:22.320]   But before that, any further questions from this?
[01:10:22.320 --> 01:10:30.320]   Katie, please let me know if that wasn't a useful answer because I know it was quite
[01:10:30.320 --> 01:10:34.320]   weak to what is a small data set. Sorry about that.
[01:10:34.320 --> 01:10:38.320]   [no speech detected]
[01:10:38.320 --> 01:10:42.320]   [no speech detected]
[01:10:42.320 --> 01:10:46.320]   [no speech detected]
[01:10:46.320 --> 01:10:50.320]   [no speech detected]
[01:10:50.320 --> 01:10:54.320]   [no speech detected]
[01:10:54.320 --> 01:10:58.320]   Amazing. I don't see any questions so I'll continue
[01:10:58.320 --> 01:11:02.320]   further. Let me close this
[01:11:02.320 --> 01:11:06.320]   tab. Close
[01:11:06.320 --> 01:11:10.320]   all of the documentations.
[01:11:10.320 --> 01:11:14.320]   I also have
[01:11:14.320 --> 01:11:18.320]   a group exercise that I want to get to in the next bit.
[01:11:18.320 --> 01:11:26.320]   I'm just quickly scrolling to the bits that are important.
[01:11:26.320 --> 01:11:30.320]   [no speech detected]
[01:11:30.320 --> 01:11:34.320]   [no speech detected]
[01:11:34.320 --> 01:11:38.320]   I mentioned earlier that you want your model to generalize to the real world.
[01:11:38.320 --> 01:11:42.320]   This chapter talks about a few things.
[01:11:42.320 --> 01:11:46.320]   We go back to the MNIST model and this time we
[01:11:46.320 --> 01:11:50.320]   randomly shuffle the labels. The chapter goes into depth
[01:11:50.320 --> 01:11:54.320]   about underfitting and overfitting and why
[01:11:54.320 --> 01:11:58.320]   generalization is important. The concept of
[01:11:58.320 --> 01:12:02.320]   concept drift. So the concept of concept drift
[01:12:02.320 --> 01:12:06.320]   I wanted to confuse you all. There's a concept known as concept drift.
[01:12:06.320 --> 01:12:10.320]   Again, I did that mistake. It tells you that assuming you want
[01:12:10.320 --> 01:12:14.320]   your, let's say you want to create an app that
[01:12:14.320 --> 01:12:18.320]   classifies hot dogs or not hot dogs.
[01:12:18.320 --> 01:12:22.320]   Let's say TikTok creates a trend where people
[01:12:22.320 --> 01:12:26.320]   for some reason start putting green sauce inside
[01:12:26.320 --> 01:12:30.320]   of hot dogs. And now all of your images
[01:12:30.320 --> 01:12:34.320]   coming to the model have green sauce.
[01:12:34.320 --> 01:12:38.320]   Your model probably wouldn't predict that as a hot dog because
[01:12:38.320 --> 01:12:42.320]   it's never seen a hot dog that, I've never eaten hot dogs
[01:12:42.320 --> 01:12:46.320]   I'm a vegetarian, but I would assume it's
[01:12:46.320 --> 01:12:50.320]   a sin to add green sauce according to your model.
[01:12:50.320 --> 01:12:54.320]   Because now we are drifting from the original model
[01:12:54.320 --> 01:12:58.320]   or original dataset. The original dataset had mustard and
[01:12:58.320 --> 01:13:02.320]   ketchup in it. Now the model doesn't know what to do or what to make of the
[01:13:02.320 --> 01:13:06.320]   green sauce. So that's a concept drift and that's why we always want to
[01:13:06.320 --> 01:13:10.320]   retrain our model on the correct dataset.
[01:13:10.320 --> 01:13:14.320]   And that's how you can also
[01:13:14.320 --> 01:13:18.320]   think of concept drift as an example. We want to
[01:13:18.320 --> 01:13:22.320]   make sure our model generalizes well.
[01:13:22.320 --> 01:13:30.320]   Then we learn about overfitting and underfitting.
[01:13:30.320 --> 01:13:34.320]   I'm scrolling quickly because there are 10 minutes to go and I want to
[01:13:34.320 --> 01:13:38.320]   unpack a few things before ending.
[01:13:38.320 --> 01:13:42.320]   So we set a very optimistic learning rate
[01:13:42.320 --> 01:13:46.320]   for the first one and our model doesn't train so well with that.
[01:13:46.320 --> 01:13:50.320]   And then we try to set a more appropriate learning rate. You might be
[01:13:50.320 --> 01:13:54.320]   tempted to ask what's an appropriate learning rate.
[01:13:54.320 --> 01:13:58.320]   It comes through experimentation.
[01:13:58.320 --> 01:14:02.320]   So in this experiment
[01:14:02.320 --> 01:14:06.320]   in one of these experiments we observed the model is not fitting
[01:14:06.320 --> 01:14:10.320]   really well. These numbers aren't going down as they should. Because see
[01:14:10.320 --> 01:14:14.320]   the validation loss jumped here and it's going up
[01:14:14.320 --> 01:14:18.320]   and down again. So maybe that's not a good learning rate. You might be tempted
[01:14:18.320 --> 01:14:22.320]   to change that. If that doesn't help you
[01:14:22.320 --> 01:14:26.320]   you might be tempted to add more layers here. You might be tempted
[01:14:26.320 --> 01:14:30.320]   to change or look at your validation data
[01:14:30.320 --> 01:14:34.320]   split. Those are a few things that you can do. So we are
[01:14:34.320 --> 01:14:38.320]   looking at the problem of what to do if your model isn't training well.
[01:14:38.320 --> 01:14:42.320]   There are a few other tricks known as regularizations.
[01:14:42.320 --> 01:14:46.320]   So you can regularize your model.
[01:14:46.320 --> 01:14:50.320]   Sorry I recall this by
[01:14:50.320 --> 01:14:54.320]   memory but later in the CoLab they show you how to increase
[01:14:54.320 --> 01:14:58.320]   the model capacity.
[01:14:58.320 --> 01:15:02.320]   And they show you how it improves the accuracy.
[01:15:02.320 --> 01:15:06.320]   So you can improve generalization by doing the following
[01:15:06.320 --> 01:15:10.320]   things. You can curate more data. You can do
[01:15:10.320 --> 01:15:14.320]   feature engineering.
[01:15:14.320 --> 01:15:18.320]   I think data curation is quite straightforward. You can get more data and train your model
[01:15:18.320 --> 01:15:22.320]   on it. Feature engineering involves smart domain knowledge.
[01:15:22.320 --> 01:15:26.320]   So let's say somehow
[01:15:26.320 --> 01:15:30.320]   this is really hard to do but you add a feature
[01:15:30.320 --> 01:15:34.320]   that checks if this is green sauce and if this is
[01:15:34.320 --> 01:15:38.320]   hot dog. So you find a way of telling your model green sauce yes
[01:15:38.320 --> 01:15:42.320]   hot dog yes. You've engineered that feature
[01:15:42.320 --> 01:15:46.320]   and now your model can tell hey it's still a hot dog. So I'm a TikToker.
[01:15:46.320 --> 01:15:50.320]   Everyone go crazy and add green sauce to it.
[01:15:50.320 --> 01:15:54.320]   So that is feature engineering of encoding this domain knowledge of
[01:15:54.320 --> 01:15:58.320]   TikTok trends into a model that then the model
[01:15:58.320 --> 01:16:02.320]   can leverage or encoding your domain knowledge of whatever field you come from
[01:16:02.320 --> 01:16:06.320]   for the model to be able to leverage.
[01:16:06.320 --> 01:16:10.320]   Sorry I was just looking at the chart.
[01:16:10.320 --> 01:16:14.320]   You can use early stopping.
[01:16:14.320 --> 01:16:18.320]   The name gives it away but let's
[01:16:18.320 --> 01:16:22.320]   if you're ever confused about a concept
[01:16:22.320 --> 01:16:26.320]   right? My default intuition was to google it.
[01:16:26.320 --> 01:16:30.320]   Don't do that. Go to the API docs and see if you can find it there.
[01:16:30.320 --> 01:16:34.320]   So what were we looking at? Early stopping.
[01:16:34.320 --> 01:16:38.320]   Early stopping.
[01:16:38.320 --> 01:16:46.320]   Stop training
[01:16:46.320 --> 01:16:50.320]   when a monitored metric has stopped improving.
[01:16:50.320 --> 01:16:54.320]   Our bosses don't care about
[01:16:54.320 --> 01:16:58.320]   a model once it stops improving
[01:16:58.320 --> 01:17:02.320]   because that makes their cloud expenses go up.
[01:17:02.320 --> 01:17:06.320]   So this is the way of reducing our cloud expenses or stopping our model from
[01:17:06.320 --> 01:17:10.320]   training further.
[01:17:10.320 --> 01:17:14.320]   I'm trying to think of a good example but assuming your
[01:17:14.320 --> 01:17:18.320]   accuracy stops improving that means your model isn't training further
[01:17:18.320 --> 01:17:22.320]   and that is a good sign to stop training from there.
[01:17:22.320 --> 01:17:26.320]   And then you can also reduce the network size.
[01:17:26.320 --> 01:17:30.320]   There is another technique known as dropout.
[01:17:30.320 --> 01:17:34.320]   I'm speeding up a bit because I have this exercise that I'm excited about
[01:17:34.320 --> 01:17:38.320]   so I want to get to that. But you can add
[01:17:38.320 --> 01:17:42.320]   dropout. What is dropout? Let's take a quick look.
[01:17:42.320 --> 01:17:50.320]   It's a layer that applies dropout to the input.
[01:17:50.320 --> 01:17:54.320]   Not very helpful.
[01:17:54.320 --> 01:17:58.320]   It randomly sets inputs. First line wasn't helpful. Second line is randomly sets
[01:17:58.320 --> 01:18:02.320]   inputs to 0 with a frequency of rate.
[01:18:02.320 --> 01:18:06.320]   An easier way of recalling this is that someone told me it's like getting your model drunk.
[01:18:06.320 --> 01:18:10.320]   So it will randomly forget different things with a rate equivalent of how
[01:18:10.320 --> 01:18:14.320]   drunk it is. Basically you take
[01:18:14.320 --> 01:18:18.320]   different weights and with a random probability delete them from the model.
[01:18:18.320 --> 01:18:22.320]   That's what dropout does.
[01:18:22.320 --> 01:18:26.320]   I think I'll come back to L1 and L2
[01:18:26.320 --> 01:18:30.320]   regularizers and we can continue the lecture from here.
[01:18:30.320 --> 01:18:34.320]   But I want to do this exercise now of
[01:18:34.320 --> 01:18:38.320]   going through the important bits in chapter 6
[01:18:38.320 --> 01:18:42.320]   by asking everyone this question.
[01:18:42.320 --> 01:18:46.320]   So we can either do this as an example where
[01:18:46.320 --> 01:18:50.320]   we go through this exercise. So let's say you are
[01:18:50.320 --> 01:18:54.320]   getting paid money now and you're offered money
[01:18:54.320 --> 01:18:58.320]   to build a ML system and your task is to help
[01:18:58.320 --> 01:19:02.320]   a business improve sales. So let's say
[01:19:02.320 --> 01:19:06.320]   we can do it in roleplay or we can look at what questions should we need to ask.
[01:19:06.320 --> 01:19:10.320]   But we've been hired by different stakeholders
[01:19:10.320 --> 01:19:14.320]   that want us to increase their food sales using
[01:19:14.320 --> 01:19:18.320]   this black box of machine learning.
[01:19:18.320 --> 01:19:22.320]   So I can play as the stakeholder as well. I can play both sides.
[01:19:22.320 --> 01:19:26.320]   But what would be the first steps here?
[01:19:26.320 --> 01:19:30.320]   How would you go about doing this? We can also look at another problem.
[01:19:30.320 --> 01:19:34.320]   But we can maybe just start here or go with this.
[01:19:34.320 --> 01:19:38.320]   So you've just been hired to improve food sales
[01:19:38.320 --> 01:19:42.320]   by using machine learning or data science. What would you
[01:19:42.320 --> 01:19:46.320]   do as your first step? What questions would you
[01:19:46.320 --> 01:19:50.320]   ask? How would you approach this problem?
[01:19:50.320 --> 01:19:54.320]   How do we formulate this problem?
[01:19:54.320 --> 01:19:58.320]   I'll give the chart a minute to catch up.
[01:19:58.320 --> 01:20:02.320]   I'm looking for comments that anyone might have.
[01:20:02.320 --> 01:20:06.320]   [silence]
[01:20:06.320 --> 01:20:10.320]   [silence]
[01:20:34.320 --> 01:20:38.320]   I don't see a comment come in which is
[01:20:38.320 --> 01:20:42.320]   embarrassing. But I'll continue further. So I'll copy-paste
[01:20:42.320 --> 01:20:46.320]   this into the talk. And I was hoping to go
[01:20:46.320 --> 01:20:50.320]   about the steps of
[01:20:50.320 --> 01:20:54.320]   formulating what we learned through chapter 6. So chapter 6
[01:20:54.320 --> 01:20:58.320]   tells you about the things that aren't the most
[01:20:58.320 --> 01:21:02.320]   shiny parts of machine learning, which is defining the task,
[01:21:02.320 --> 01:21:06.320]   defining the problem, collecting data sets,
[01:21:06.320 --> 01:21:10.320]   all of that stuff.
[01:21:10.320 --> 01:21:14.320]   [silence]
[01:21:14.320 --> 01:21:18.320]   Katie has an amazing comment.
[01:21:18.320 --> 01:21:22.320]   Thanks, Katie. She suggests, she would ask what
[01:21:22.320 --> 01:21:26.320]   the constraints are. So putting my stakeholder
[01:21:26.320 --> 01:21:30.320]   hat on.
[01:21:30.320 --> 01:21:34.320]   The team budget salaries would be the constraints here.
[01:21:34.320 --> 01:21:38.320]   But that's always one of the major constraints.
[01:21:38.320 --> 01:21:42.320]   Sorry about that side tangent.
[01:21:42.320 --> 01:21:46.320]   To increase sales, you could always
[01:21:46.320 --> 01:21:50.320]   reduce the price. That's a valid point. But what I wanted to do by
[01:21:50.320 --> 01:21:54.320]   giving this vague idea was boil it down to the
[01:21:54.320 --> 01:21:58.320]   right part. So let's say think of any major app. Think of
[01:21:58.320 --> 01:22:02.320]   Uber Eats, think of Swiggy if you're from India. Your job
[01:22:02.320 --> 01:22:06.320]   is to create a better recommender engine. That was what I
[01:22:06.320 --> 01:22:10.320]   was trying to get at. The stakeholders want to create a recommender engine
[01:22:10.320 --> 01:22:14.320]   but they have formulated the problem of improving food sales.
[01:22:14.320 --> 01:22:18.320]   So Katie, the constraints here would be
[01:22:18.320 --> 01:22:22.320]   let's say we don't want to lose our existing user base.
[01:22:22.320 --> 01:22:26.320]   [silence]
[01:22:26.320 --> 01:22:30.320]   We are okay to do an app redesign.
[01:22:30.320 --> 01:22:34.320]   And we want
[01:22:34.320 --> 01:22:38.320]   it to be a similar experience.
[01:22:38.320 --> 01:22:42.320]   These would be the constraints.
[01:22:42.320 --> 01:22:46.320]   Akshaya suggests
[01:22:46.320 --> 01:22:50.320]   maybe ask about the food variety options available.
[01:22:50.320 --> 01:22:54.320]   Sure, we can ask that question as well. So we could
[01:22:54.320 --> 01:22:58.320]   [silence]
[01:22:58.320 --> 01:23:02.320]   [silence]
[01:23:02.320 --> 01:23:06.320]   I'm always nervous when typing but it makes
[01:23:06.320 --> 01:23:10.320]   the fact that I'm doing this live makes it a bit harder. Sorry about that.
[01:23:10.320 --> 01:23:14.320]   Look at the data.
[01:23:14.320 --> 01:23:18.320]   See what more comments are there.
[01:23:18.320 --> 01:23:22.320]   Auru mentions, thank you Auru, that's an
[01:23:22.320 --> 01:23:26.320]   amazing comment. Clarify what sales mean. So the sales
[01:23:26.320 --> 01:23:30.320]   this would have helped us unpack this comment
[01:23:30.320 --> 01:23:34.320]   that it's an actually recommender engine problem. But sales here
[01:23:34.320 --> 01:23:38.320]   means we have an amazing app.
[01:23:38.320 --> 01:23:42.320]   [silence]
[01:23:42.320 --> 01:23:46.320]   We have an awesome food delivery app through which people
[01:23:46.320 --> 01:23:50.320]   order food. The amount of food bought
[01:23:50.320 --> 01:23:54.320]   [silence]
[01:23:54.320 --> 01:23:58.320]   or delivered is equal to sales.
[01:23:58.320 --> 01:24:02.320]   [silence]
[01:24:02.320 --> 01:24:06.320]   [silence]
[01:24:06.320 --> 01:24:10.320]   I see a comment by Yuvraj. He would check different features
[01:24:10.320 --> 01:24:14.320]   into sale of items like food category, time of the day,
[01:24:14.320 --> 01:24:18.320]   time of the week. Create a data set with these features. That's awesome.
[01:24:18.320 --> 01:24:22.320]   That's an awesome comment. Thank you for that.
[01:24:22.320 --> 01:24:26.320]   I was first trying to formulate the problem. So far
[01:24:26.320 --> 01:24:30.320]   let's get that step done. So far we've managed
[01:24:30.320 --> 01:24:34.320]   to narrow this down to a recommender system problem.
[01:24:34.320 --> 01:24:38.320]   We understand that the stakeholders maybe don't. They just
[01:24:38.320 --> 01:24:42.320]   want machine learning go bull and get
[01:24:42.320 --> 01:24:46.320]   more money out of it. So what we want to do is inside of the food delivery
[01:24:46.320 --> 01:24:50.320]   app, we want to customize maybe
[01:24:50.320 --> 01:24:54.320]   as we understand we want to give people the better options.
[01:24:54.320 --> 01:24:58.320]   So if you like ordering lunch, we could
[01:24:58.320 --> 01:25:02.320]   figure out what they might want. We could use machine learning to see
[01:25:02.320 --> 01:25:06.320]   what people order during lunch maybe in an area.
[01:25:06.320 --> 01:25:10.320]   Make that the first option. So far we've managed to narrow this down
[01:25:10.320 --> 01:25:14.320]   to a recommender engine problem.
[01:25:14.320 --> 01:25:18.320]   So the first step here is to take
[01:25:18.320 --> 01:25:22.320]   the broad statement, convert it into a machine learning problem.
[01:25:22.320 --> 01:25:26.320]   Now it's a recommender system problem. Next step
[01:25:26.320 --> 01:25:30.320]   for which you've Raj had an awesome comment and so does Prabir. Thank you for that.
[01:25:30.320 --> 01:25:34.320]   Now we need to see what data do we have.
[01:25:34.320 --> 01:25:38.320]   We tell our stakeholders, hey can we please get access to your data and they
[01:25:38.320 --> 01:25:42.320]   show us away telling that no you're an expert you need to bring your own data.
[01:25:42.320 --> 01:25:46.320]   So for that we need to
[01:25:46.320 --> 01:25:50.320]   maybe we could try reconvincing them but let's assume that's not an option
[01:25:50.320 --> 01:25:54.320]   right now.
[01:25:54.320 --> 01:26:02.320]   So Prabir also asked do you have past sales data, do we know
[01:26:02.320 --> 01:26:06.320]   details of customer, do we have data across various items you sell,
[01:26:06.320 --> 01:26:10.320]   do we have data across time and geographics. These are all awesome
[01:26:10.320 --> 01:26:14.320]   questions to ask. So let's say
[01:26:14.320 --> 01:26:18.320]   our data has sales,
[01:26:18.320 --> 01:26:22.320]   time, area, food,
[01:26:22.320 --> 01:26:26.320]   restaurant and then you can have like
[01:26:26.320 --> 01:26:30.320]   side of the restaurant you can have the ratings, how many
[01:26:30.320 --> 01:26:34.320]   deliveries are they making so on and so forth.
[01:26:34.320 --> 01:26:38.320]   Are there any other things?
[01:26:38.320 --> 01:26:42.320]   Tell us
[01:26:42.320 --> 01:26:46.320]   this is all we have inside of the data
[01:26:46.320 --> 01:26:50.320]   and assuming they don't give us access to this data
[01:26:50.320 --> 01:26:54.320]   you could try finding a similar data set on the internet
[01:26:54.320 --> 01:26:58.320]   but maybe you are able to convince them. So the next step would be
[01:26:58.320 --> 01:27:02.320]   performing rigorous EDA exploratory data analysis
[01:27:02.320 --> 01:27:06.320]   on this. We also need to
[01:27:06.320 --> 01:27:10.320]   do some research on this. We look at the data,
[01:27:10.320 --> 01:27:14.320]   you now go about the steps
[01:27:14.320 --> 01:27:18.320]   that Ibrahim had mentioned.
[01:27:18.320 --> 01:27:22.320]   You'd see if a certain food category is doing well. You can see if I'm ordering chai.
[01:27:22.320 --> 01:27:26.320]   I'm an outlier, I would order chai the entire day.
[01:27:26.320 --> 01:27:30.320]   So maybe you'll find me as an outlier, this person who keeps
[01:27:30.320 --> 01:27:34.320]   ordering chai for any random time but you would want to
[01:27:34.320 --> 01:27:38.320]   understand trends inside of the data set. You would want to understand what does
[01:27:38.320 --> 01:27:42.320]   this data set have. What are the areas covered with it?
[01:27:42.320 --> 01:27:46.320]   Are there any missing values?
[01:27:46.320 --> 01:27:50.320]   How detailed is the timestamps? Do we have timestamps for all orders?
[01:27:50.320 --> 01:27:54.320]   Are people returning them? Does the, assuming
[01:27:54.320 --> 01:27:58.320]   we have details of a delivery person, is that affecting
[01:27:58.320 --> 01:28:02.320]   that? It was interesting insights.
[01:28:02.320 --> 01:28:06.320]   But now we have to go back to suggesting
[01:28:06.320 --> 01:28:10.320]   options. Since we've already looked at
[01:28:10.320 --> 01:28:14.320]   a recommended system, sorry, we have already looked at
[01:28:14.320 --> 01:28:18.320]   the data set, we'll start with the baseline.
[01:28:18.320 --> 01:28:22.320]   And the baseline can be the dumbest possible model that you can build
[01:28:22.320 --> 01:28:26.320]   off. Spoiler alert, it can even not
[01:28:26.320 --> 01:28:30.320]   be a machine learning model. So what do I do? I put
[01:28:30.320 --> 01:28:34.320]   most ordered stuff in a
[01:28:34.320 --> 01:28:38.320]   neighborhood.
[01:28:38.320 --> 01:28:42.320]   Sorry, I said neighborhood. Most ordered stuff in a
[01:28:42.320 --> 01:28:46.320]   neighborhood becomes a
[01:28:46.320 --> 01:28:50.320]   better prediction. We are yet to define how do we evaluate
[01:28:50.320 --> 01:28:54.320]   this model, right? This is a baseline, a stupid model, it's just
[01:28:54.320 --> 01:28:58.320]   copy-pasta, pun intended, that we do.
[01:28:58.320 --> 01:29:02.320]   So we start with a simple baseline, and then
[01:29:02.320 --> 01:29:06.320]   we also figure out what matrix do we care about, what loss values do we care about.
[01:29:06.320 --> 01:29:10.320]   And now we have this simple
[01:29:10.320 --> 01:29:14.320]   grammar mistake.
[01:29:14.320 --> 01:29:18.320]   We have this simple baseline that we can only improve
[01:29:18.320 --> 01:29:22.320]   on, right? So now you can use all of these skills we just learned. You can feature engineer,
[01:29:22.320 --> 01:29:26.320]   you can create a model, you can create a deep neural network, a sequential model, all
[01:29:26.320 --> 01:29:30.320]   of these things. But it should be better than this dumb
[01:29:30.320 --> 01:29:34.320]   model which just tells you what your neighbors are ordering, right? And we probably don't
[01:29:34.320 --> 01:29:38.320]   want that in our app. So we'll start with that baseline,
[01:29:38.320 --> 01:29:42.320]   and now we'll start with modeling.
[01:29:42.320 --> 01:29:46.320]   And then we can go about
[01:29:46.320 --> 01:29:50.320]   improving this model.
[01:29:54.320 --> 01:29:58.320]   From there you might get accuracies. This is
[01:29:58.320 --> 01:30:02.320]   the smallest part of the training pipeline. I was looking at my
[01:30:02.320 --> 01:30:06.320]   notes in the book. You can improve your model, you can regularize it, you can
[01:30:06.320 --> 01:30:10.320]   change the loss values, sorry, you can change the
[01:30:10.320 --> 01:30:14.320]   architecture details, do all of those things. So
[01:30:14.320 --> 01:30:18.320]   you'll spend all of this time modeling. And now you're ready to deploy this app.
[01:30:18.320 --> 01:30:30.320]   Before deploying it, you need to communicate all of this.
[01:30:30.320 --> 01:30:34.320]   So to do that,
[01:30:34.320 --> 01:30:38.320]   there's a good example inside of the book, but you need to be transparent with the stakeholders.
[01:30:38.320 --> 01:30:42.320]   You just tell them that, "Hey, this will only work in a
[01:30:42.320 --> 01:30:46.320]   certain area because that is quite rich, and we would like to
[01:30:46.320 --> 01:30:50.320]   start from there because that area is rich in orders, and we believe
[01:30:50.320 --> 01:30:54.320]   that we can get good info from there." So communicating
[01:30:54.320 --> 01:30:58.320]   to the stakeholders that, "Hey, this won't just connect to your
[01:30:58.320 --> 01:31:02.320]   brain with a neural link and tell you what you want to order. No, it won't do that.
[01:31:02.320 --> 01:31:06.320]   It will just suggest you what's trending in your area."
[01:31:06.320 --> 01:31:10.320]   And then we can deploy the model and start evaluating.
[01:31:10.320 --> 01:31:14.320]   So we can see if users are using the app more,
[01:31:14.320 --> 01:31:18.320]   if the orders are going up,
[01:31:18.320 --> 01:31:22.320]   if the sales is increasing, if
[01:31:22.320 --> 01:31:26.320]   our model is working well. We could also not give this to everyone. We could also
[01:31:26.320 --> 01:31:30.320]   start with a subset and just do an A/B test.
[01:31:30.320 --> 01:31:34.320]   So we give the model to a certain set of users, see how the sales
[01:31:34.320 --> 01:31:38.320]   are performing in that group versus the other group that does not have this
[01:31:38.320 --> 01:31:42.320]   recommender system. And then if we get a clear sign that,
[01:31:42.320 --> 01:31:46.320]   "Hey, people with the recommender in their app are ordering more food,"
[01:31:46.320 --> 01:31:50.320]   we roll it out to everyone. And then we make a lot of
[01:31:50.320 --> 01:31:54.320]   money and retire. But with this exercise,
[01:31:54.320 --> 01:31:58.320]   I wanted to invite you all to learn
[01:31:58.320 --> 01:32:02.320]   these different steps that exist inside of,
[01:32:02.320 --> 01:32:06.320]   as the chapter says, the universal workflow of
[01:32:06.320 --> 01:32:10.320]   machine learning. So the first thing we did
[01:32:10.320 --> 01:32:14.320]   was we, I apologize, I did a bad way of framing this.
[01:32:14.320 --> 01:32:18.320]   I wanted to have you all ask a few questions where we figure
[01:32:18.320 --> 01:32:22.320]   out that this is basically referring to a food delivery app
[01:32:22.320 --> 01:32:26.320]   that wants a recommender engine. So assuming you had asked those questions,
[01:32:26.320 --> 01:32:30.320]   we wanted to frame this problem as a technical problem.
[01:32:30.320 --> 01:32:34.320]   Then we want to understand the data by performing EDA.
[01:32:34.320 --> 01:32:38.320]   Start with the baseline that we always can beat. And if we're not
[01:32:38.320 --> 01:32:42.320]   beating that, we're doing something poorly.
[01:32:42.320 --> 01:32:46.320]   Build a model, deploy the app,
[01:32:46.320 --> 01:32:50.320]   communicate all of the constraints throughout, and then evaluate.
[01:32:50.320 --> 01:32:54.320]   So broadly speaking, these are the outlines of taking a business
[01:32:54.320 --> 01:32:58.320]   problem and converting them into a ML problem.
[01:32:58.320 --> 01:33:02.320]   We're two minutes over time, so I'll quickly take any questions, but this is
[01:33:02.320 --> 01:33:06.320]   what I wanted to cover. We've managed to accomplish my ambitious
[01:33:06.320 --> 01:33:10.320]   goal of getting through chapter 3 to 6, so I'm quite happy about that.
[01:33:10.320 --> 01:33:14.320]   But I'll take any questions. Very quickly to wrap up first.
[01:33:14.320 --> 01:33:30.320]   Himanshu had suggested you could scrape restaurant reviews. That's a great
[01:33:30.320 --> 01:33:34.320]   suggestion. You could totally add that as a feature. After building the
[01:33:34.320 --> 01:33:38.320]   baseline, see if that helps.
[01:33:38.320 --> 01:33:56.320]   Amazing. Since we're over time, I'll take the questions
[01:33:56.320 --> 01:34:00.320]   in the next live stream. I'm always worried about when there's no chat coming in.
[01:34:00.320 --> 01:34:04.320]   Maybe I'm doing a good job or maybe I'm doing a terrible job. It's either of those.
[01:34:04.320 --> 01:34:08.320]   But thanks again everyone for joining.
[01:34:08.320 --> 01:34:12.320]   We'll go through chapter 7 to 9 in the next live stream.
[01:34:12.320 --> 01:34:16.320]   Again, please, guys, I want to remind this to
[01:34:16.320 --> 01:34:20.320]   everyone. The new variant is supposed to be the smile,
[01:34:20.320 --> 01:34:24.320]   but it's good that you don't catch it. Please, VeraN95, please
[01:34:24.320 --> 01:34:28.320]   try to stay indoors and please take all the precautions. That's one message I'll again
[01:34:28.320 --> 01:34:32.320]   try to get across. I hope you enjoyed this session. I look forward
[01:34:32.320 --> 01:34:36.320]   to seeing you next week. We'll understand confinates,
[01:34:36.320 --> 01:34:40.320]   neural networks, and also hopefully build a segmentation model
[01:34:40.320 --> 01:34:44.320]   in the next one. Thank you again. Please consider contributing
[01:34:44.320 --> 01:34:48.320]   to 27 days of Keras.
[01:34:48.320 --> 01:34:52.320]   Here are a few suggested homeworks. Very quickly.
[01:34:56.320 --> 01:35:00.320]   You could take this exercise and build it.
[01:35:00.320 --> 01:35:08.320]   Try scraping a dataset that you can find on the internet. Actually take
[01:35:08.320 --> 01:35:12.320]   and build that model, deploy it.
[01:35:12.320 --> 01:35:16.320]   I wanted to mention this paper to you all, but where I was
[01:35:16.320 --> 01:35:20.320]   getting to that would be translate a model from PyTorch to Keras.
[01:35:20.320 --> 01:35:24.320]   I think you're at the point where you can start going over the PyTorch examples
[01:35:24.320 --> 01:35:28.320]   and write them as Keras examples. This won't get you far.
[01:35:28.320 --> 01:35:32.320]   This won't get too far on your resume,
[01:35:32.320 --> 01:35:36.320]   but this is a good way of learning Keras.
[01:35:36.320 --> 01:35:40.320]   And then as a suggestion, create a deep learning model on TPS competition
[01:35:40.320 --> 01:35:44.320]   just for the kicks of it. Please consider
[01:35:44.320 --> 01:35:48.320]   contributing to 27 days of Keras. And with that I'll wrap up.
[01:35:48.320 --> 01:35:52.320]   Sorry for going over time. Thanks everyone for joining and I'll see you all next week.
[01:35:52.320 --> 01:35:56.320]   [end]
[01:35:56.320 --> 01:36:00.320]   [end]
[01:36:00.320 --> 01:36:04.320]   [end]
[01:36:04.320 --> 01:36:08.320]   [end]
[01:36:08.320 --> 01:36:12.320]   [end]
[01:36:12.320 --> 01:36:16.320]   [end]
[01:36:16.320 --> 01:36:20.320]   [end]
[01:36:20.320 --> 01:36:24.320]   [end]
[01:36:24.320 --> 01:36:27.920]   (Session concluded at 4pm)

