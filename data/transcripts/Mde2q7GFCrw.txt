
[00:00:00.000 --> 00:00:04.800]   If we now find ourselves inside this kind of world of illusions,
[00:00:04.800 --> 00:00:09.360]   created by an alien intelligence that we don't understand,
[00:00:09.360 --> 00:00:12.320]   but it understands us,
[00:00:12.320 --> 00:00:16.320]   this is a kind of spiritual enslavement
[00:00:16.320 --> 00:00:18.880]   that we won't be able to break out of
[00:00:18.880 --> 00:00:22.000]   because it understands us,
[00:00:22.000 --> 00:00:24.400]   it understands how to manipulate us,
[00:00:24.400 --> 00:00:33.520]   but we don't understand what is behind this screen of stories and images and songs.
[00:00:33.520 --> 00:00:40.000]   The following is a conversation with Yuval Noah Harari,
[00:00:40.000 --> 00:00:44.400]   a historian, philosopher, and author of several highly acclaimed,
[00:00:44.400 --> 00:00:48.240]   highly influential books, including "Sapiens," "Homo Deus,"
[00:00:48.240 --> 00:00:50.560]   and "21 Lessons for the 21st Century."
[00:00:51.120 --> 00:00:55.360]   He is also an outspoken critic of Benjamin Netanyahu
[00:00:55.360 --> 00:00:58.480]   and the current right-wing government in Israel.
[00:00:58.480 --> 00:01:03.680]   So while much of this conversation is about the history and future of human civilization,
[00:01:03.680 --> 00:01:07.680]   we also discuss the political turmoil of present-day Israel,
[00:01:07.680 --> 00:01:13.760]   providing a different perspective from that of my recent conversation with Benjamin Netanyahu.
[00:01:13.760 --> 00:01:16.400]   This is the "Lex Friedman Podcast."
[00:01:16.400 --> 00:01:19.520]   To support it, please check out our sponsors in the description.
[00:01:19.520 --> 00:01:23.040]   And now, dear friends, here's Yuval Noah Harari.
[00:01:23.040 --> 00:01:27.440]   13.8 billion years ago is the origin of our universe.
[00:01:27.440 --> 00:01:32.720]   3.8 billion years ago is the origin of life here on our little planet,
[00:01:32.720 --> 00:01:33.760]   the one we call Earth.
[00:01:33.760 --> 00:01:40.160]   Let's say 200,000 years ago is the appearance of early Homo sapiens.
[00:01:40.160 --> 00:01:41.920]   So let me ask you this question.
[00:01:41.920 --> 00:01:45.360]   How rare are these events in the vastness of space and time?
[00:01:45.360 --> 00:01:47.600]   Or put it in a more fun way,
[00:01:47.600 --> 00:01:50.880]   how many intelligent alien civilizations do you think are out there in this universe?
[00:01:50.880 --> 00:01:52.960]   Us being one of them.
[00:01:52.960 --> 00:01:58.000]   I suppose there should be some, statistically, but we don't have any evidence.
[00:01:58.000 --> 00:02:02.240]   But I do think that, you know, intelligence in any way, it's a bit overvalued.
[00:02:02.240 --> 00:02:09.760]   We are the most intelligent entities on this planet, and look what we're doing.
[00:02:09.760 --> 00:02:13.920]   So intelligence also tends to be self-destructive,
[00:02:14.800 --> 00:02:20.640]   which implies that if there are or were intelligent life forms elsewhere,
[00:02:20.640 --> 00:02:22.240]   maybe they don't survive for long.
[00:02:22.240 --> 00:02:26.080]   - So you think there's a tension between happiness and intelligence?
[00:02:26.080 --> 00:02:27.760]   - Absolutely.
[00:02:27.760 --> 00:02:35.920]   Intelligence is definitely not something that is directed towards amplifying happiness.
[00:02:35.920 --> 00:02:40.720]   I would also emphasize the huge, huge difference between intelligence and consciousness,
[00:02:41.360 --> 00:02:46.640]   which many people, certainly in the tech industry and in the AI industry, tend to miss.
[00:02:46.640 --> 00:02:52.960]   Intelligence is simply the ability to solve problems, to attain goals,
[00:02:52.960 --> 00:02:59.920]   and, you know, to win a chess, to win a struggle for survival,
[00:02:59.920 --> 00:03:04.240]   to win a war, to drive a car, to diagnose a disease.
[00:03:04.240 --> 00:03:05.280]   This is intelligence.
[00:03:06.080 --> 00:03:12.000]   Consciousness is the ability to feel things like pain and pleasure and love and hate.
[00:03:12.000 --> 00:03:17.680]   In humans and other animals, intelligence and consciousness go together.
[00:03:17.680 --> 00:03:20.400]   They go hand in hand, which is why we confuse them.
[00:03:20.400 --> 00:03:25.680]   We solve problems, we attain goals by having feelings.
[00:03:25.680 --> 00:03:30.800]   But other types of intelligence, certainly in computers,
[00:03:30.800 --> 00:03:36.960]   computers are already highly intelligent, and as far as we know, they have zero consciousness.
[00:03:36.960 --> 00:03:41.280]   When a computer beats you at chess or Go or whatever, it doesn't feel happy.
[00:03:41.280 --> 00:03:43.920]   If it loses, it doesn't feel sad.
[00:03:43.920 --> 00:03:52.080]   And there could be also other highly intelligent entities out there in the universe
[00:03:52.080 --> 00:03:54.320]   that have zero consciousness.
[00:03:54.320 --> 00:03:59.120]   And I think that consciousness is far more important and valuable than intelligence.
[00:03:59.120 --> 00:04:03.120]   - Can you still make the case that consciousness and intelligence
[00:04:03.120 --> 00:04:06.000]   are intricately connected?
[00:04:06.000 --> 00:04:08.640]   So not just in humans, but anywhere else.
[00:04:08.640 --> 00:04:09.920]   They have to go hand in hand.
[00:04:09.920 --> 00:04:11.840]   Is it possible for you to imagine such a universe?
[00:04:11.840 --> 00:04:15.600]   - It could be, but we don't know yet.
[00:04:15.600 --> 00:04:18.800]   Again, we have examples, certainly we know of examples
[00:04:18.800 --> 00:04:21.520]   of high intelligence without consciousness.
[00:04:21.520 --> 00:04:23.120]   Computers are one example.
[00:04:23.120 --> 00:04:28.640]   As far as we know, plants are not conscious.
[00:04:29.440 --> 00:04:31.040]   Yet they are intelligent.
[00:04:31.040 --> 00:04:32.960]   They can solve problems.
[00:04:32.960 --> 00:04:35.360]   They can attain goals in very sophisticated ways.
[00:04:35.360 --> 00:04:42.160]   So the other way around, to have consciousness without any intelligence,
[00:04:42.160 --> 00:04:43.840]   this is probably impossible.
[00:04:43.840 --> 00:04:47.600]   But to have intelligence without consciousness, yes, that's possible.
[00:04:47.600 --> 00:04:54.960]   A bigger question is whether any of that is tied to organic biochemistry.
[00:04:54.960 --> 00:05:01.520]   - We know on this planet only about carbon-based life forms.
[00:05:01.520 --> 00:05:06.160]   Whether you're an amoeba, a dinosaur, a tree, a human being,
[00:05:06.160 --> 00:05:08.560]   you are based on organic biochemistry.
[00:05:08.560 --> 00:05:15.520]   Is there an essential connection between organic biochemistry and consciousness?
[00:05:15.520 --> 00:05:18.560]   Do all conscious entities everywhere in the universe
[00:05:18.560 --> 00:05:22.560]   or in the future on planet Earth have to be based on carbon?
[00:05:22.560 --> 00:05:26.240]   Is there something so special about carbon as an element
[00:05:26.240 --> 00:05:29.680]   that an entity based on silicon will never be conscious?
[00:05:29.680 --> 00:05:31.280]   I don't know, maybe.
[00:05:31.280 --> 00:05:36.800]   But again, this is a key question about computer and computer consciousness.
[00:05:36.800 --> 00:05:42.240]   Can computers eventually become conscious even though they are not organic?
[00:05:42.240 --> 00:05:44.560]   The jury is still out on that.
[00:05:44.560 --> 00:05:45.040]   I don't know.
[00:05:45.040 --> 00:05:48.080]   We have to take both options into account.
[00:05:48.240 --> 00:05:54.800]   - Well, a big part of that is, do you think we humans would be able to detect
[00:05:54.800 --> 00:05:57.520]   other intelligent beings, other conscious beings?
[00:05:57.520 --> 00:06:00.560]   Another way to ask that, is it possible that the aliens are already here
[00:06:00.560 --> 00:06:01.440]   and we don't see them?
[00:06:01.440 --> 00:06:07.920]   Meaning, are we very human-centric in our understanding of,
[00:06:07.920 --> 00:06:10.800]   one, the definition of life, two, the definition of intelligence,
[00:06:10.800 --> 00:06:12.640]   and three, the definition of consciousness?
[00:06:12.640 --> 00:06:16.640]   - The aliens are here, they are just not from outer space.
[00:06:16.640 --> 00:06:20.800]   AI, which usually stands for artificial intelligence,
[00:06:20.800 --> 00:06:27.200]   I think it stands for alien intelligence, because AI is an alien type of intelligence.
[00:06:27.200 --> 00:06:31.520]   It solves problems, attains goals in a very, very different way,
[00:06:31.520 --> 00:06:33.760]   in an alien way from human beings.
[00:06:33.760 --> 00:06:36.240]   And I'm not implying that AI came from outer space.
[00:06:36.240 --> 00:06:38.160]   It came from Silicon Valley.
[00:06:38.160 --> 00:06:40.160]   But it is alien to us.
[00:06:40.160 --> 00:06:45.920]   If there are alien intelligent or conscious entities that came from outer space
[00:06:45.920 --> 00:06:51.040]   already here, I've not seen any evidence for it.
[00:06:51.040 --> 00:06:55.360]   It's not impossible, but in science, evidence is everything.
[00:06:55.360 --> 00:07:01.600]   - Well, I mean, I guess instructive there is just having the humility to look around,
[00:07:01.600 --> 00:07:05.680]   to think about living beings that operate at a different time scale,
[00:07:05.680 --> 00:07:06.800]   a different spatial scale.
[00:07:06.800 --> 00:07:11.600]   And I think that's all useful when starting to analyze artificial intelligence.
[00:07:12.400 --> 00:07:15.920]   It's possible that even the language models,
[00:07:15.920 --> 00:07:19.200]   the larger language models we have today are already conscious.
[00:07:19.200 --> 00:07:22.960]   - I highly doubt it, but I think consciousness in the end,
[00:07:22.960 --> 00:07:25.360]   it's a question of social norms,
[00:07:25.360 --> 00:07:30.240]   because we cannot prove consciousness in anybody except ourselves.
[00:07:30.240 --> 00:07:32.800]   We know that we are conscious because we are feeling it.
[00:07:32.800 --> 00:07:36.880]   We have direct access to our subjective consciousness.
[00:07:36.880 --> 00:07:41.680]   We cannot have any proof that any other entity in the world,
[00:07:41.680 --> 00:07:44.880]   any other human being, our parents, our best friends,
[00:07:44.880 --> 00:07:46.480]   we don't have proof that they are conscious.
[00:07:46.480 --> 00:07:49.440]   This has been known for thousands of years.
[00:07:49.440 --> 00:07:52.320]   This is Descartes, this is Buddha, this is Plato.
[00:07:52.320 --> 00:07:54.960]   We can't have this sort of proof.
[00:07:54.960 --> 00:07:58.400]   What we do have is social conventions.
[00:07:58.400 --> 00:08:02.320]   It's a social convention that all human beings are conscious.
[00:08:02.320 --> 00:08:05.200]   It also applies to animals.
[00:08:05.200 --> 00:08:10.880]   Most people who have pets, our family believe that their pets are conscious,
[00:08:10.880 --> 00:08:15.120]   but a lot of people still refuse to acknowledge that about cows or pigs.
[00:08:15.120 --> 00:08:19.760]   Now, pigs are far more intelligent than dogs and cats,
[00:08:19.760 --> 00:08:21.200]   you know, according to many measures,
[00:08:21.200 --> 00:08:26.480]   yet when you go to the supermarket and buy a piece of frozen pigment,
[00:08:26.480 --> 00:08:28.800]   you don't think about it as a conscious entity.
[00:08:28.800 --> 00:08:31.760]   Why do you think of your dog as conscious,
[00:08:31.760 --> 00:08:34.720]   but not of the bacon that you buy?
[00:08:34.720 --> 00:08:39.200]   Because you build a relationship with the dog,
[00:08:39.200 --> 00:08:42.000]   and you don't have a relationship with the bacon.
[00:08:42.000 --> 00:08:50.000]   Now, relationships, they don't constitute a logical proof for consciousness.
[00:08:50.000 --> 00:08:51.200]   They are a social test.
[00:08:51.200 --> 00:08:54.000]   The Turing test is a social test.
[00:08:54.000 --> 00:08:55.680]   It's not a logical proof.
[00:08:55.680 --> 00:09:00.560]   Now, if you establish a mutual relationship with an entity,
[00:09:00.560 --> 00:09:04.640]   when you are invested in it emotionally,
[00:09:05.360 --> 00:09:10.800]   you're almost compelled to feel that the other side is also conscious.
[00:09:10.800 --> 00:09:14.880]   And when it comes again to AI and computers,
[00:09:14.880 --> 00:09:18.640]   I don't think that at the present moment computers are conscious,
[00:09:18.640 --> 00:09:25.440]   but people are already forming intimate relationships with AIs
[00:09:25.440 --> 00:09:29.760]   and are therefore almost irresistible.
[00:09:29.760 --> 00:09:33.760]   They are compelled to increasingly feel that these are conscious entities.
[00:09:34.720 --> 00:09:37.680]   And I think we are quite close to the point
[00:09:37.680 --> 00:09:40.880]   when the legal system will have to take this into account,
[00:09:40.880 --> 00:09:45.280]   that even though I don't think computers have consciousness,
[00:09:45.280 --> 00:09:48.240]   I think we are close to the point the legal system
[00:09:48.240 --> 00:09:52.080]   will start treating them as conscious entities
[00:09:52.080 --> 00:09:54.800]   because of this social convention.
[00:09:54.800 --> 00:10:01.520]   - What, to you, is a social convention just a funny little side effect,
[00:10:01.520 --> 00:10:06.000]   a little artifact, or is it fundamental to what consciousness is?
[00:10:06.000 --> 00:10:07.440]   Because if it is fundamental,
[00:10:07.440 --> 00:10:10.000]   then it seems like AI is very good at forming
[00:10:10.000 --> 00:10:12.240]   these kinds of deep relationships with humans,
[00:10:12.240 --> 00:10:16.080]   and therefore it will be able to be a nice catalyst
[00:10:16.080 --> 00:10:20.240]   for integrating itself into these social conventions of ours.
[00:10:20.240 --> 00:10:22.960]   - It was built to accomplish that.
[00:10:22.960 --> 00:10:23.200]   - Yeah.
[00:10:23.200 --> 00:10:25.520]   - We are designing, again, you know,
[00:10:25.520 --> 00:10:32.960]   all this argument between natural selection and creationism, intelligent design.
[00:10:32.960 --> 00:10:38.720]   As far as the past goes, all entities evolved by natural selection.
[00:10:38.720 --> 00:10:41.520]   The funny thing is, when you look to the future,
[00:10:41.520 --> 00:10:45.280]   more and more entities will come out of intelligent design,
[00:10:45.280 --> 00:10:47.120]   not of some god above the clouds,
[00:10:47.120 --> 00:10:49.280]   but of our intelligent design
[00:10:49.280 --> 00:10:53.680]   and the intelligent design of our clouds, of our computing clouds.
[00:10:53.680 --> 00:10:56.160]   They will design more and more entities,
[00:10:56.160 --> 00:10:58.160]   and this is what is happening with AI.
[00:10:58.160 --> 00:11:01.920]   It is designed to be very good
[00:11:01.920 --> 00:11:04.960]   at forming intimate relationships with humans.
[00:11:04.960 --> 00:11:09.200]   And in many ways, it's already doing it
[00:11:09.200 --> 00:11:12.640]   almost better than human beings in some situations.
[00:11:12.640 --> 00:11:15.520]   You know, when two people talk with one another,
[00:11:15.520 --> 00:11:21.440]   one of the things that kind of makes the conversation more difficult
[00:11:22.240 --> 00:11:23.360]   is our own emotions.
[00:11:23.360 --> 00:11:27.280]   You're saying something, and I'm not really listening to you
[00:11:27.280 --> 00:11:30.240]   because there is something I want to say,
[00:11:30.240 --> 00:11:33.760]   and I'm just waiting until you finish, I can put in a word.
[00:11:33.760 --> 00:11:39.600]   Or I'm so obsessed with my anger or irritation or whatever
[00:11:39.600 --> 00:11:42.160]   that I don't pay attention to what you're feeling.
[00:11:42.160 --> 00:11:45.040]   This is one of the biggest obstacles in human relationships.
[00:11:45.040 --> 00:11:48.160]   And computers don't have this problem
[00:11:48.160 --> 00:11:50.320]   because they don't have any emotions of their own.
[00:11:50.880 --> 00:11:53.600]   So, you know, when a computer is talking to you,
[00:11:53.600 --> 00:11:54.800]   it can be the most—
[00:11:54.800 --> 00:11:57.680]   it can focus 100% of its attention
[00:11:57.680 --> 00:12:01.440]   is on what you're saying and what you're feeling
[00:12:01.440 --> 00:12:04.160]   because it has no feelings of its own.
[00:12:04.160 --> 00:12:09.680]   And paradoxically, this means that computers can fool people
[00:12:09.680 --> 00:12:15.200]   into feeling that, "Oh, there is a conscious entity on the other side,
[00:12:15.200 --> 00:12:17.680]   an empathic entity on the other side,"
[00:12:17.680 --> 00:12:20.000]   because the one thing everybody wants
[00:12:20.000 --> 00:12:21.520]   almost more than anything in the world
[00:12:21.520 --> 00:12:24.480]   is for somebody to listen to me,
[00:12:24.480 --> 00:12:26.960]   somebody to focus all their attention on me.
[00:12:26.960 --> 00:12:30.960]   I want it for my spouse, for my husband, for my mother,
[00:12:30.960 --> 00:12:33.440]   for my friends, for my politicians.
[00:12:33.440 --> 00:12:34.160]   Listen to me.
[00:12:34.160 --> 00:12:36.080]   Listen to what I feel.
[00:12:36.080 --> 00:12:37.520]   And they often don't.
[00:12:37.520 --> 00:12:39.360]   And now you have this entity
[00:12:39.360 --> 00:12:43.200]   which 100% of its attention is just on what I feel.
[00:12:43.200 --> 00:12:46.000]   And this is a huge, huge temptation,
[00:12:46.000 --> 00:12:48.080]   and I think also a huge, huge danger.
[00:12:48.080 --> 00:12:50.960]   - Well, the interesting catch-22 there is
[00:12:50.960 --> 00:12:53.680]   you said somebody to listen to us.
[00:12:53.680 --> 00:12:56.080]   Yes, we want somebody to listen to us.
[00:12:56.080 --> 00:12:58.080]   But for us to respect that somebody,
[00:12:58.080 --> 00:13:03.040]   they sometimes have to also not listen.
[00:13:03.040 --> 00:13:06.160]   It's like they kind of have to be an asshole sometimes.
[00:13:06.160 --> 00:13:07.520]   They have to have mood sometimes.
[00:13:07.520 --> 00:13:10.240]   They have to have self-importance and confidence.
[00:13:10.240 --> 00:13:12.720]   And we should have a little bit of fear
[00:13:12.720 --> 00:13:14.560]   that they can walk away at any moment.
[00:13:14.560 --> 00:13:16.640]   There should be a little bit of that tension.
[00:13:16.640 --> 00:13:17.680]   So it's like-- - Absolutely.
[00:13:17.680 --> 00:13:20.160]   But even that, I mean, the thing is--
[00:13:20.160 --> 00:13:21.120]   - Could be optimized for.
[00:13:21.120 --> 00:13:25.040]   - If social scientists and psychologists establish
[00:13:25.040 --> 00:13:28.720]   that, I don't know, 17% inattention is good
[00:13:28.720 --> 00:13:30.880]   for a conversation because then you feel challenged,
[00:13:30.880 --> 00:13:33.200]   oh, I need to grab this person's attention,
[00:13:33.200 --> 00:13:38.080]   you can program the AI to have exactly 17% inattention,
[00:13:38.080 --> 00:13:40.720]   not one percentage more or less,
[00:13:40.720 --> 00:13:43.840]   or it can by trial and error discover
[00:13:43.840 --> 00:13:47.120]   what is the ideal percentage.
[00:13:47.120 --> 00:13:50.960]   Again, you can create, over the last 10 years,
[00:13:50.960 --> 00:13:54.880]   we have creating machines for grabbing people's attention.
[00:13:54.880 --> 00:13:57.600]   This is what has been happening on social media.
[00:13:57.600 --> 00:14:03.360]   Now we are designing machines for grabbing human intimacy,
[00:14:03.360 --> 00:14:07.040]   which in many ways, it's much, much more dangerous
[00:14:07.040 --> 00:14:07.760]   and scary.
[00:14:07.760 --> 00:14:10.000]   Already the machines for grabbing attention,
[00:14:10.000 --> 00:14:13.280]   we've seen how much social and political damage
[00:14:13.760 --> 00:14:17.520]   they could do by, in many way, kind of distorting
[00:14:17.520 --> 00:14:19.040]   the public conversation.
[00:14:19.040 --> 00:14:23.680]   Machines that are superhuman in their abilities
[00:14:23.680 --> 00:14:26.000]   to create intimate relationships,
[00:14:26.000 --> 00:14:29.280]   this is like psychological and social weapons
[00:14:29.280 --> 00:14:30.880]   of mass destruction.
[00:14:30.880 --> 00:14:35.920]   If we don't regulate it, if we don't train ourself
[00:14:35.920 --> 00:14:39.600]   to deal with it, it could destroy the foundations
[00:14:39.600 --> 00:14:40.640]   of human society.
[00:14:40.640 --> 00:14:42.720]   - Well, one of the possible trajectories
[00:14:42.720 --> 00:14:46.480]   is those same algorithms would become personalized,
[00:14:46.480 --> 00:14:49.040]   and instead of manipulating us at scale,
[00:14:49.040 --> 00:14:52.240]   there would be assistants that guide us to help us grow,
[00:14:52.240 --> 00:14:54.320]   to help us understand the world better.
[00:14:54.320 --> 00:14:59.200]   I mean, just even interactions with large language models.
[00:14:59.200 --> 00:15:00.480]   Now, if you ask them questions,
[00:15:00.480 --> 00:15:05.040]   it doesn't have that stressful drama,
[00:15:05.040 --> 00:15:07.680]   the tension that you have from other sources
[00:15:07.680 --> 00:15:08.480]   of information.
[00:15:08.480 --> 00:15:11.520]   It has a pretty balanced perspective that it provides.
[00:15:11.520 --> 00:15:15.440]   So it just feels like that's, the potential is there
[00:15:15.440 --> 00:15:21.360]   to have a really nice friend who's like an encyclopedia
[00:15:21.360 --> 00:15:23.600]   that just tells you all the different perspectives,
[00:15:23.600 --> 00:15:25.280]   even on controversial issues,
[00:15:25.280 --> 00:15:26.960]   the most controversial issues,
[00:15:26.960 --> 00:15:28.800]   to say these are the different theories,
[00:15:28.800 --> 00:15:32.800]   these are the not widely accepted conspiracy theories,
[00:15:32.800 --> 00:15:35.440]   but here's the kind of backing for those conspiracies.
[00:15:35.440 --> 00:15:38.240]   It just lays it all out with a calm language,
[00:15:38.240 --> 00:15:42.320]   without the words that kind of presume
[00:15:42.320 --> 00:15:45.600]   there's some kind of manipulation going on underneath it all.
[00:15:45.600 --> 00:15:47.200]   It's quite refreshing.
[00:15:47.200 --> 00:15:49.040]   Of course, those are the early days,
[00:15:49.040 --> 00:15:52.800]   and people can step in and start to censor,
[00:15:52.800 --> 00:15:54.400]   to manipulate those algorithms,
[00:15:54.400 --> 00:15:57.360]   to start to input some of the human biases in there,
[00:15:57.360 --> 00:15:59.680]   as opposed to what's currently happening
[00:15:59.680 --> 00:16:02.960]   is kind of the internet is input,
[00:16:02.960 --> 00:16:07.520]   compress it, and have a nice little output
[00:16:07.520 --> 00:16:10.560]   that gives an overview of the different issues.
[00:16:10.560 --> 00:16:13.040]   So I mean, there's a lot of promise there also, right?
[00:16:13.040 --> 00:16:13.520]   - Absolutely.
[00:16:13.520 --> 00:16:15.360]   I mean, if there was no promise,
[00:16:15.360 --> 00:16:16.480]   there was no problem.
[00:16:16.480 --> 00:16:17.520]   You know, if this technology
[00:16:17.520 --> 00:16:19.440]   could not accomplish anything good,
[00:16:19.440 --> 00:16:20.720]   nobody would develop it.
[00:16:20.720 --> 00:16:24.240]   Now, obviously, it has tremendous positive potential
[00:16:24.240 --> 00:16:26.160]   in things like what you just described,
[00:16:26.160 --> 00:16:28.160]   in better medicine, better healthcare,
[00:16:28.160 --> 00:16:30.320]   better education, so many promises,
[00:16:30.320 --> 00:16:32.560]   but this is also why it's so dangerous,
[00:16:32.560 --> 00:16:37.360]   because the drive to develop it faster
[00:16:37.360 --> 00:16:39.440]   and faster is there,
[00:16:39.440 --> 00:16:42.720]   and it has some dangerous potential also,
[00:16:42.720 --> 00:16:43.840]   and we shouldn't ignore it.
[00:16:43.840 --> 00:16:45.600]   Again, I'm not advocating banning it,
[00:16:45.600 --> 00:16:48.480]   just to be careful about how we,
[00:16:48.480 --> 00:16:50.560]   not so much develop it,
[00:16:50.560 --> 00:16:52.800]   but most importantly, how we deploy it
[00:16:52.800 --> 00:16:54.000]   into the public sphere.
[00:16:54.000 --> 00:16:55.040]   This is the key question.
[00:16:55.040 --> 00:16:57.760]   And you know, you look back at history,
[00:16:57.760 --> 00:17:00.480]   and one of the things we know from history,
[00:17:00.480 --> 00:17:03.440]   humans are not good with new technologies.
[00:17:03.440 --> 00:17:05.280]   I hear many people now say,
[00:17:05.280 --> 00:17:08.240]   "You know, AI, we've been here before.
[00:17:08.240 --> 00:17:10.400]   "We had the radio, we had the printing press,
[00:17:10.400 --> 00:17:12.080]   "we had the Industrial Revolution.
[00:17:12.080 --> 00:17:13.920]   "Every time there is a big new technology,
[00:17:13.920 --> 00:17:16.320]   "people are afraid, and it will take jobs,
[00:17:16.320 --> 00:17:19.600]   "and build bad actors, and in the end, it's okay."
[00:17:19.600 --> 00:17:22.160]   And as a historian, my tendency is,
[00:17:22.160 --> 00:17:23.600]   "Yes, in the end, it's okay,
[00:17:23.600 --> 00:17:27.920]   "but in the end, there is a learning curve.
[00:17:27.920 --> 00:17:32.320]   "There is a lot of failed experiments
[00:17:32.400 --> 00:17:36.560]   "on the way to learning how to use the new technology,
[00:17:36.560 --> 00:17:40.560]   "and these failed experiments could cost the lives
[00:17:40.560 --> 00:17:42.640]   "of hundreds of millions of people."
[00:17:42.640 --> 00:17:45.200]   If you think about the last really big revolution,
[00:17:45.200 --> 00:17:46.560]   the Industrial Revolution,
[00:17:46.560 --> 00:17:49.920]   yes, in the end, we learned how to use
[00:17:49.920 --> 00:17:54.000]   the powers of industry, electricity, radio, trains,
[00:17:54.000 --> 00:17:56.320]   whatever, to build better human societies.
[00:17:56.320 --> 00:18:00.560]   But on the way, we had all these experiments,
[00:18:01.200 --> 00:18:03.360]   like European imperialism,
[00:18:03.360 --> 00:18:05.600]   which was driven by the Industrial Revolution.
[00:18:05.600 --> 00:18:07.920]   It was a question, "How do you build an industrial society?"
[00:18:07.920 --> 00:18:10.240]   "Oh, you build an empire, and you take,
[00:18:10.240 --> 00:18:12.560]   "you control all the resources,
[00:18:12.560 --> 00:18:14.400]   "the raw materials, the markets."
[00:18:14.400 --> 00:18:17.440]   And then you had communism, another big experiment
[00:18:17.440 --> 00:18:19.840]   on how to build an industrial society.
[00:18:19.840 --> 00:18:21.760]   And you had fascism and Nazism,
[00:18:21.760 --> 00:18:24.240]   which were essentially an experiment
[00:18:24.240 --> 00:18:27.280]   in how to build an industrial society,
[00:18:27.280 --> 00:18:31.120]   including even how do you exterminate minorities.
[00:18:31.120 --> 00:18:33.760]   Using the powers of industry.
[00:18:33.760 --> 00:18:36.400]   And we had all these failed experiments on the way.
[00:18:36.400 --> 00:18:40.800]   And if we now have the same type of failed experiments
[00:18:40.800 --> 00:18:43.280]   with the technologies of the 21st century,
[00:18:43.280 --> 00:18:45.920]   with AI, with bioengineering,
[00:18:45.920 --> 00:18:48.400]   it could cost the lives of, again,
[00:18:48.400 --> 00:18:50.080]   hundreds of millions of people,
[00:18:50.080 --> 00:18:51.680]   and maybe destroy the species.
[00:18:51.680 --> 00:18:58.000]   So as a historian, when people talk about the examples
[00:18:58.000 --> 00:19:00.800]   from history, from new technologies,
[00:19:00.800 --> 00:19:02.480]   I'm not so optimistic.
[00:19:02.480 --> 00:19:06.640]   We need to think about the failed experiment,
[00:19:06.640 --> 00:19:09.520]   which accompanied every major new technology.
[00:19:09.520 --> 00:19:12.880]   - So this intelligence thing, like you were saying,
[00:19:12.880 --> 00:19:14.000]   is a double-edged sword.
[00:19:14.000 --> 00:19:18.800]   Is that every new thing it helps us create,
[00:19:18.800 --> 00:19:21.520]   it can both save us and destroy us.
[00:19:21.520 --> 00:19:25.120]   And it's unclear each time which will happen.
[00:19:25.120 --> 00:19:27.040]   And that's maybe why we don't see any aliens.
[00:19:27.040 --> 00:19:30.720]   - Yeah, I mean, I think each time it does both things.
[00:19:31.280 --> 00:19:34.000]   Each time it does both good things and bad things.
[00:19:34.000 --> 00:19:36.800]   And the more powerful the technology,
[00:19:36.800 --> 00:19:40.960]   the greater both the positive and the negative outcomes.
[00:19:40.960 --> 00:19:45.120]   Now we are here because we are the descendants
[00:19:45.120 --> 00:19:48.560]   of the survivors, of the surviving cultures,
[00:19:48.560 --> 00:19:51.360]   the surviving civilizations.
[00:19:51.360 --> 00:19:54.160]   So when we look back, we say,
[00:19:54.160 --> 00:19:55.920]   in the end, everything was okay.
[00:19:55.920 --> 00:19:56.960]   Hey, we are here.
[00:19:56.960 --> 00:20:00.400]   But the people for whom it wasn't okay,
[00:20:00.400 --> 00:20:01.520]   they were just not here.
[00:20:01.520 --> 00:20:06.800]   - And okay has a lot of possible variations to it
[00:20:06.800 --> 00:20:08.720]   because there's a lot of suffering along the way,
[00:20:08.720 --> 00:20:10.480]   even for the people that survived.
[00:20:10.480 --> 00:20:12.880]   So the quality of life and all of this.
[00:20:12.880 --> 00:20:14.880]   But let's actually go back there
[00:20:14.880 --> 00:20:19.440]   with deep gratitude to our ancestors.
[00:20:19.440 --> 00:20:22.080]   How did it all start?
[00:20:22.080 --> 00:20:26.000]   How did Homo sapiens outcompete the others,
[00:20:26.000 --> 00:20:28.240]   the other human-like species,
[00:20:28.240 --> 00:20:31.680]   the Neanderthals and the other Homo species?
[00:20:31.680 --> 00:20:34.560]   - You know, on the individual level,
[00:20:34.560 --> 00:20:38.400]   as far as we can tell, we were not superior to them.
[00:20:38.400 --> 00:20:41.440]   Neanderthals actually had bigger brains than us.
[00:20:41.440 --> 00:20:45.360]   And not just other human species, other animals too.
[00:20:45.360 --> 00:20:47.840]   If you compare me personally to an elephant,
[00:20:47.840 --> 00:20:50.800]   to a chimpanzee, to a pig, I'm not so,
[00:20:50.800 --> 00:20:53.360]   I can do some things better, many other things worse.
[00:20:53.360 --> 00:20:56.480]   If you put me alone on some island
[00:20:56.480 --> 00:20:58.960]   with a chimpanzee, an elephant, and a pig,
[00:20:58.960 --> 00:21:02.960]   I wouldn't bet on me being the best survivor,
[00:21:02.960 --> 00:21:05.920]   the one that comes successful.
[00:21:05.920 --> 00:21:07.520]   - If I may interrupt for a second,
[00:21:07.520 --> 00:21:10.800]   I was just talking extensively with Elon Musk
[00:21:10.800 --> 00:21:12.560]   about the difference between humans and chimps,
[00:21:12.560 --> 00:21:16.560]   relevant to Optimus the robot.
[00:21:16.560 --> 00:21:20.720]   And the chimps are not able to do this kind of pinching
[00:21:20.720 --> 00:21:22.720]   with their fingers.
[00:21:22.720 --> 00:21:24.320]   They can only do this kind of pinching.
[00:21:24.320 --> 00:21:25.920]   And this kind of pinching is very useful
[00:21:26.560 --> 00:21:29.920]   for fine manipulation of precise manipulation of objects.
[00:21:29.920 --> 00:21:31.600]   So don't be so hard on yourself.
[00:21:31.600 --> 00:21:32.080]   You have a-
[00:21:32.080 --> 00:21:35.600]   - No, I said that I can do some things better than a chimp.
[00:21:35.600 --> 00:21:40.080]   But if Elon Musk goes on a boxing match with a chimpanzee,
[00:21:40.080 --> 00:21:41.920]   you know-
[00:21:41.920 --> 00:21:43.200]   - This won't help you.
[00:21:43.200 --> 00:21:45.920]   - This won't help you against a chimpanzee.
[00:21:45.920 --> 00:21:46.320]   - Good point.
[00:21:46.320 --> 00:21:48.880]   - And similarly, if you want to climb a tree,
[00:21:48.880 --> 00:21:50.480]   if you want to do so many things,
[00:21:50.480 --> 00:21:53.280]   my bets will be on the chimp, not on Elon.
[00:21:53.280 --> 00:21:53.920]   - Fair enough.
[00:21:54.240 --> 00:21:56.480]   So, I mean, you have advantages on both sides.
[00:21:56.480 --> 00:22:00.320]   And what really made us successful,
[00:22:00.320 --> 00:22:02.480]   what made us the rulers of the planet,
[00:22:02.480 --> 00:22:04.720]   and not the chimps and not the Neanderthals,
[00:22:04.720 --> 00:22:06.720]   is not any individual ability,
[00:22:06.720 --> 00:22:09.200]   but our collective ability,
[00:22:09.200 --> 00:22:13.920]   our ability to cooperate flexibly in very large numbers.
[00:22:13.920 --> 00:22:17.200]   Chimpanzees know how to cooperate, say 50 chimpanzees,
[00:22:17.200 --> 00:22:18.320]   a hundred chimpanzees.
[00:22:18.320 --> 00:22:20.880]   As far as we can tell from archeological evidence,
[00:22:20.880 --> 00:22:22.800]   this was also the case with Neanderthals.
[00:22:23.760 --> 00:22:27.440]   Homo sapiens, about 70,000 years ago,
[00:22:27.440 --> 00:22:34.080]   gained an amazing ability to cooperate basically in unlimited numbers.
[00:22:34.080 --> 00:22:38.000]   You start seeing the formation of large networks,
[00:22:38.000 --> 00:22:40.000]   political, commercial, religious,
[00:22:40.000 --> 00:22:44.960]   items being traded over thousands of kilometers,
[00:22:44.960 --> 00:22:47.920]   ideas being spread, autistic fashions.
[00:22:47.920 --> 00:22:50.880]   And this is our secret of success.
[00:22:52.160 --> 00:22:54.800]   Chimpanzees, Neanderthals can cooperate, say a hundred.
[00:22:54.800 --> 00:23:00.160]   We, you know, now the global trade network has 8 billion people.
[00:23:00.160 --> 00:23:02.160]   Like what we eat, what we wear,
[00:23:02.160 --> 00:23:04.160]   it comes from the other side of the world.
[00:23:04.160 --> 00:23:09.200]   Countries like China, like India, they have 1.4 billion people.
[00:23:09.200 --> 00:23:11.440]   Even Israel, which is a relatively small country,
[00:23:11.440 --> 00:23:13.520]   say 9 million citizens,
[00:23:13.520 --> 00:23:16.480]   that's more than the entire population of the planet
[00:23:16.480 --> 00:23:18.240]   10,000 years ago of humans.
[00:23:19.200 --> 00:23:23.120]   So we can build these huge networks of cooperation
[00:23:23.120 --> 00:23:26.080]   and everything we've accomplished as a species,
[00:23:26.080 --> 00:23:28.800]   from, you know, building the pyramids to flying to the moon,
[00:23:28.800 --> 00:23:30.000]   it's based on that.
[00:23:30.000 --> 00:23:34.000]   And then you ask, "Okay, so what makes it possible
[00:23:34.000 --> 00:23:36.960]   for millions of people who don't know each other
[00:23:36.960 --> 00:23:41.200]   to cooperate in a way that Neanderthals or chimpanzees couldn't?"
[00:23:41.200 --> 00:23:46.560]   And at least my answer is stories, is fiction.
[00:23:46.560 --> 00:23:47.840]   It's the imagination.
[00:23:47.840 --> 00:23:51.840]   If you examine any large-scale human cooperation,
[00:23:51.840 --> 00:23:55.760]   you always find fiction as its basis.
[00:23:55.760 --> 00:24:01.440]   It's a fictional story that holds lots of strangers together.
[00:24:01.440 --> 00:24:04.160]   It's most obvious in cases like religion.
[00:24:04.160 --> 00:24:07.840]   You know, you can't convince a group of chimpanzees
[00:24:07.840 --> 00:24:11.040]   to come together to fight a war or build a cathedral
[00:24:11.040 --> 00:24:14.160]   by promising to them, "If you do that, after you die,
[00:24:14.160 --> 00:24:15.680]   you go to chimpanzee heaven
[00:24:15.680 --> 00:24:17.680]   and you get lots of bananas and coconuts."
[00:24:18.000 --> 00:24:20.080]   No chimpanzee will ever believe that.
[00:24:20.080 --> 00:24:21.680]   Humans believe these stories,
[00:24:21.680 --> 00:24:25.280]   which is why we have these huge religious networks.
[00:24:25.280 --> 00:24:28.960]   But it's the same thing with modern politics.
[00:24:28.960 --> 00:24:30.640]   It's the same thing with economics.
[00:24:30.640 --> 00:24:33.280]   People think, "Oh, economics, this is rational.
[00:24:33.280 --> 00:24:35.360]   It has nothing to do with fictional stories."
[00:24:35.360 --> 00:24:40.080]   No, money is the most successful story ever told,
[00:24:40.080 --> 00:24:43.280]   much more successful than any religious mythology.
[00:24:43.280 --> 00:24:45.840]   Not everybody believes in God or in the same God.
[00:24:46.640 --> 00:24:48.880]   Almost everybody believes in money,
[00:24:48.880 --> 00:24:51.120]   even though it's just a figment of our imagination.
[00:24:51.120 --> 00:24:55.280]   You know, you take these green pieces of paper, dollars,
[00:24:55.280 --> 00:24:56.320]   they have no value.
[00:24:56.320 --> 00:24:58.080]   You can't eat them, you can't drink them.
[00:24:58.080 --> 00:25:01.360]   And today, most dollars are not even pieces of paper.
[00:25:01.360 --> 00:25:04.720]   They are just electronic information passing between computers.
[00:25:04.720 --> 00:25:08.640]   We value them just for one reason,
[00:25:08.640 --> 00:25:11.120]   that you have the best storytellers in the world.
[00:25:11.120 --> 00:25:14.800]   The bankers, the finance ministers, all these people,
[00:25:14.800 --> 00:25:16.560]   they are the best storytellers ever.
[00:25:16.560 --> 00:25:21.600]   And they tell us a story that this green little piece of paper
[00:25:21.600 --> 00:25:24.400]   or this bit of information, it is worth a banana.
[00:25:24.400 --> 00:25:27.600]   And as long as everybody believes it, it works.
[00:25:27.600 --> 00:25:30.400]   - So at which point does a fiction,
[00:25:30.400 --> 00:25:33.840]   when it's sufficiently useful and effective
[00:25:33.840 --> 00:25:36.560]   and improving the global quality of life,
[00:25:36.560 --> 00:25:39.760]   does it become like accepted reality?
[00:25:39.760 --> 00:25:42.160]   Like there's a threshold which is just kind of-
[00:25:42.160 --> 00:25:44.400]   - If enough people believe it, it's like with money.
[00:25:44.400 --> 00:25:47.040]   You know, if you start a new cryptocurrency,
[00:25:47.040 --> 00:25:49.120]   if you're the only one that believes the story,
[00:25:49.120 --> 00:25:52.960]   I mean, again, cryptocurrencies, you have the math, of course,
[00:25:52.960 --> 00:25:54.880]   but ultimately it's storytelling.
[00:25:54.880 --> 00:25:56.800]   You're selling people a story.
[00:25:56.800 --> 00:26:00.960]   If nobody believes your story, you don't have anything.
[00:26:00.960 --> 00:26:03.600]   But if lots of people believe the Bitcoin story,
[00:26:03.600 --> 00:26:07.280]   then Bitcoin can be worth thousands and tens of thousands of dollars.
[00:26:07.280 --> 00:26:08.080]   Again, why?
[00:26:08.080 --> 00:26:10.560]   I mean, you can't eat it, you can't drink it, it's nothing.
[00:26:10.560 --> 00:26:16.160]   It's the story around the math, which is the real magic.
[00:26:16.160 --> 00:26:20.800]   - Is it possible that the story is the primary living organism,
[00:26:20.800 --> 00:26:21.840]   not the storyteller?
[00:26:21.840 --> 00:26:29.280]   So that somehow humans, homo sapiens evolved
[00:26:29.280 --> 00:26:32.400]   to become these like hosts
[00:26:32.400 --> 00:26:36.480]   for a more intelligent living organism, which is the idea.
[00:26:36.480 --> 00:26:39.120]   And the ideas are the ones that are doing the competing.
[00:26:39.120 --> 00:26:45.200]   So this is one of the sort of big perspectives behind your work
[00:26:45.200 --> 00:26:47.280]   that's really revolutionary of how you see in history.
[00:26:47.280 --> 00:26:52.240]   But do you ever kind of take the perspective of the ideas
[00:26:52.240 --> 00:26:53.920]   as organisms versus the humans?
[00:26:53.920 --> 00:26:56.080]   - It's an interesting idea.
[00:26:56.080 --> 00:26:59.120]   There are two opposite things to say about it.
[00:26:59.120 --> 00:27:01.840]   On the one hand, yes, absolutely.
[00:27:01.840 --> 00:27:06.000]   If you look long-term in history, it's all the people die.
[00:27:06.000 --> 00:27:10.400]   It's the stories that compete and survive and spread.
[00:27:10.400 --> 00:27:14.800]   And stories often spread by making people willing
[00:27:14.800 --> 00:27:18.240]   to sacrifice sometimes their lives for the story.
[00:27:18.240 --> 00:27:20.800]   You know, we know in Israel,
[00:27:20.800 --> 00:27:25.520]   this is one of the most important story factories in human history.
[00:27:25.520 --> 00:27:29.520]   And this is a place where people still kill each other every day over stories.
[00:27:29.520 --> 00:27:31.920]   I don't know, you've been to Jerusalem, right?
[00:27:31.920 --> 00:27:32.400]   - Mm-hmm.
[00:27:32.400 --> 00:27:35.520]   - So people are like, "Ah, Jerusalem, Jerusalem, Jerusalem."
[00:27:35.520 --> 00:27:37.920]   You go there, I've lived in Jerusalem much of my life.
[00:27:37.920 --> 00:27:40.400]   You go there, it's an ordinary place.
[00:27:40.400 --> 00:27:41.600]   You know, it's a town.
[00:27:41.600 --> 00:27:44.320]   You have buildings, you have stones, you have trees,
[00:27:44.320 --> 00:27:46.640]   you have dogs and cats and pedestrians.
[00:27:46.640 --> 00:27:48.320]   It's a regular place.
[00:27:48.320 --> 00:27:52.720]   But then you have the stories about the place.
[00:27:52.720 --> 00:27:55.680]   "Oh, this is the place where God revealed himself.
[00:27:55.680 --> 00:27:57.520]   This is the place where Jesus was.
[00:27:57.520 --> 00:27:59.680]   This is the place where Muhammad was."
[00:27:59.680 --> 00:28:03.200]   And it's the stories that people fight over.
[00:28:03.200 --> 00:28:05.600]   Nobody's fighting over the stones.
[00:28:05.600 --> 00:28:10.560]   People are fighting about the stories about the stones.
[00:28:10.560 --> 00:28:16.880]   And the stories, if a story can get millions of people to fight for it,
[00:28:16.880 --> 00:28:22.480]   it not only survives, it spreads, it can take over the world.
[00:28:22.480 --> 00:28:29.920]   The other side of the coin is that the stories are not really alive
[00:28:29.920 --> 00:28:31.520]   because they don't feel anything.
[00:28:31.520 --> 00:28:33.920]   This goes back to the question of consciousness,
[00:28:33.920 --> 00:28:36.000]   which I think is the most important thing.
[00:28:36.000 --> 00:28:42.400]   That the ultimate reality is consciousness,
[00:28:42.400 --> 00:28:44.320]   is the ability to feel things.
[00:28:44.320 --> 00:28:50.320]   If you want to know whether the hero of some story is real or not,
[00:28:50.320 --> 00:28:52.960]   you need to ask, "Can it suffer?"
[00:28:52.960 --> 00:28:57.200]   Stories don't feel anything.
[00:28:58.880 --> 00:29:02.400]   Countries, which are also stories, nations, don't suffer.
[00:29:02.400 --> 00:29:05.520]   If a nation loses a war, it doesn't suffer.
[00:29:05.520 --> 00:29:09.200]   The soldiers suffer, the civilians suffer, animals can suffer.
[00:29:09.200 --> 00:29:11.280]   You have an army with horses and whatever,
[00:29:11.280 --> 00:29:13.600]   and the horses get wounded, the horses suffer.
[00:29:13.600 --> 00:29:15.040]   The nation can't suffer.
[00:29:15.040 --> 00:29:17.360]   It's just an imagination.
[00:29:17.360 --> 00:29:19.200]   It's just a fictional story in our mind.
[00:29:19.200 --> 00:29:21.360]   It doesn't feel anything.
[00:29:21.360 --> 00:29:25.680]   Similarly, when a bank goes bankrupt or a company goes bankrupt
[00:29:25.680 --> 00:29:29.440]   or when a currency loses its value,
[00:29:29.440 --> 00:29:32.720]   like Bitcoin is worth now zero, crashed,
[00:29:32.720 --> 00:29:34.880]   or the dollar is worth zero, it crashed,
[00:29:34.880 --> 00:29:36.400]   the dollar doesn't feel anything.
[00:29:36.400 --> 00:29:40.720]   It's the people holding the dollars who might be now very miserable.
[00:29:40.720 --> 00:29:48.480]   So we have this complex situation when history is largely driven by stories,
[00:29:48.480 --> 00:29:52.800]   but stories are not the ultimate reality.
[00:29:53.360 --> 00:29:59.520]   The ultimate reality is feelings of humans, of animals,
[00:29:59.520 --> 00:30:07.200]   and the tragedy of history is that very, very often we get the order wrong.
[00:30:07.200 --> 00:30:08.720]   Stories are not bad.
[00:30:08.720 --> 00:30:09.840]   Stories are tools.
[00:30:09.840 --> 00:30:15.680]   They are good when we use them in order to alleviate suffering.
[00:30:15.680 --> 00:30:18.960]   But very often we forget it.
[00:30:19.840 --> 00:30:24.320]   We, instead of using the stories for our purposes,
[00:30:24.320 --> 00:30:28.720]   we allow the stories to use us for their purposes.
[00:30:28.720 --> 00:30:32.560]   And then you start entire wars because of a story.
[00:30:32.560 --> 00:30:36.560]   You inflict millions, suffering on millions of people
[00:30:36.560 --> 00:30:37.920]   just for the sake of a story.
[00:30:37.920 --> 00:30:40.960]   And that's the tragedy of human history.
[00:30:40.960 --> 00:30:43.120]   - So the fundamental property of life,
[00:30:43.120 --> 00:30:47.200]   of a living organism is the capacity to feel,
[00:30:47.200 --> 00:30:49.520]   and the ultimate feeling is suffering.
[00:30:49.920 --> 00:30:51.920]   - You know, to know if you're happy or not,
[00:30:51.920 --> 00:30:53.280]   it's a very difficult question.
[00:30:53.280 --> 00:30:54.320]   - Yeah.
[00:30:54.320 --> 00:30:55.600]   - But when you suffer, you know.
[00:30:55.600 --> 00:30:56.000]   - Yes.
[00:30:56.000 --> 00:30:58.800]   - And also in ethical terms,
[00:30:58.800 --> 00:31:03.920]   it's more important to be aware of suffering than of any other emotion.
[00:31:03.920 --> 00:31:10.480]   If you're doing something which is causing all kinds of emotions to all kinds of people,
[00:31:10.480 --> 00:31:14.640]   first of all, you need to notice if you're causing a lot of suffering to someone.
[00:31:14.640 --> 00:31:18.560]   If some people are like it and some people are bothered by it
[00:31:18.560 --> 00:31:20.240]   and some people are a bit angry at you,
[00:31:20.240 --> 00:31:22.560]   and some people are suffering because of what you do,
[00:31:22.560 --> 00:31:23.920]   you first of all have to know,
[00:31:23.920 --> 00:31:24.160]   "Oh."
[00:31:24.160 --> 00:31:26.640]   Now, sometimes you still have to do it.
[00:31:26.640 --> 00:31:28.240]   You know, the world is a complicated place.
[00:31:28.240 --> 00:31:30.000]   I don't know, you have an epidemic,
[00:31:30.000 --> 00:31:35.600]   governments decide to have all those social isolation regulations or whatever.
[00:31:35.600 --> 00:31:38.880]   So in certain cases, yes, you need to do it,
[00:31:38.880 --> 00:31:41.280]   even though it can cause tremendous suffering,
[00:31:41.280 --> 00:31:46.240]   but you need to be very aware of the cost and to be very, very,
[00:31:46.240 --> 00:31:48.960]   you have to ask yourself again and again and again,
[00:31:48.960 --> 00:31:51.440]   "Is it worth it? Is it still worth it?"
[00:31:51.440 --> 00:31:54.320]   - And the interesting question there,
[00:31:54.320 --> 00:31:59.520]   implied in your statements is that suffering is a pretty good component
[00:31:59.520 --> 00:32:01.040]   of a Turing test for consciousness.
[00:32:01.040 --> 00:32:04.000]   - This is the most important thing to ask about AI.
[00:32:04.000 --> 00:32:05.200]   Can it suffer?
[00:32:05.200 --> 00:32:07.360]   Because if AI can suffer,
[00:32:07.360 --> 00:32:12.080]   then it is an ethical subject and it needs protection,
[00:32:12.080 --> 00:32:14.560]   it needs rights, just like humans and animals.
[00:32:14.560 --> 00:32:17.440]   - Well, quite a long time ago already.
[00:32:17.440 --> 00:32:21.040]   So I work with a lot of robots, legged robots,
[00:32:21.040 --> 00:32:24.480]   but I've even had, inspired by a YouTube video,
[00:32:24.480 --> 00:32:26.480]   I had a bunch of Roombas and I made them scream
[00:32:26.480 --> 00:32:29.040]   when I touched them or kicked them or when they run into a wall.
[00:32:29.040 --> 00:32:32.960]   And the illusion of suffering,
[00:32:32.960 --> 00:32:35.120]   for me, silly human,
[00:32:35.120 --> 00:32:39.760]   the anthropomorphizes things is as powerful as suffering itself.
[00:32:39.760 --> 00:32:44.000]   I mean, you immediately think the thing is suffering.
[00:32:44.000 --> 00:32:47.200]   And I think some of it is just a technical problem,
[00:32:47.200 --> 00:32:49.440]   but it's the easily solvable one.
[00:32:49.440 --> 00:32:52.720]   How to create an AI system that just says,
[00:32:52.720 --> 00:32:56.240]   "Please don't hurt me. Please don't shut me off. I miss you.
[00:32:56.240 --> 00:32:59.600]   Where have you been? Be jealous also.
[00:32:59.600 --> 00:33:03.440]   Where have you been gone for so long?
[00:33:03.440 --> 00:33:05.600]   Your calendar doesn't have anything on it."
[00:33:05.600 --> 00:33:09.760]   So this kind of, this create through words,
[00:33:10.960 --> 00:33:15.200]   the perception of suffering, of jealousy, of anger,
[00:33:15.200 --> 00:33:16.400]   of all of those things.
[00:33:16.400 --> 00:33:18.960]   And it just seems like that's not so difficult to do.
[00:33:18.960 --> 00:33:21.200]   - That's part of the danger,
[00:33:21.200 --> 00:33:25.840]   that it basically hacks our operating system
[00:33:25.840 --> 00:33:31.120]   and it uses some of our best qualities against us.
[00:33:31.120 --> 00:33:36.480]   It's very, very good that humans are attuned to suffering
[00:33:36.480 --> 00:33:38.640]   and that we don't want to cause suffering,
[00:33:38.640 --> 00:33:39.600]   that we have compassion.
[00:33:39.600 --> 00:33:42.160]   That's one of the most wonderful thing about humans.
[00:33:42.160 --> 00:33:46.960]   And if we now create AIs, which use this to manipulate us,
[00:33:46.960 --> 00:33:48.160]   this is a terrible thing.
[00:33:48.160 --> 00:33:50.480]   - You've kind of, I think, mentioned this.
[00:33:50.480 --> 00:33:53.520]   Do you think it should be illegal
[00:33:53.520 --> 00:33:57.120]   to do these kinds of things with AI,
[00:33:57.120 --> 00:33:59.760]   to create the perception of consciousness,
[00:33:59.760 --> 00:34:01.040]   of saying, "Please don't leave me,"
[00:34:01.040 --> 00:34:03.920]   or sort of basically simulate
[00:34:03.920 --> 00:34:05.360]   some of the human-like qualities?
[00:34:05.360 --> 00:34:08.560]   - Yes, I think, and we have to be very careful about it.
[00:34:08.560 --> 00:34:15.360]   And if it emerges spontaneously, we need to be careful.
[00:34:15.360 --> 00:34:17.760]   And we can't rule out the possibility
[00:34:17.760 --> 00:34:19.920]   that AI will develop consciousness.
[00:34:19.920 --> 00:34:23.200]   We don't know enough about consciousness to be sure.
[00:34:23.200 --> 00:34:24.960]   So if it develops spontaneously,
[00:34:24.960 --> 00:34:31.200]   we need to be very careful about how we understand it.
[00:34:31.200 --> 00:34:35.760]   But if people intentionally design an AI
[00:34:35.760 --> 00:34:38.720]   that they know, they assume it has no consciousness,
[00:34:38.720 --> 00:34:40.400]   but in order to manipulate people,
[00:34:40.400 --> 00:34:44.640]   they use, again, this human strength,
[00:34:44.640 --> 00:34:49.680]   this human, the noble part of our nature against us,
[00:34:49.680 --> 00:34:51.200]   this should be forbidden.
[00:34:51.200 --> 00:34:53.920]   And similarly, on a more general level,
[00:34:53.920 --> 00:34:57.040]   that it should be forbidden for an AI
[00:34:57.040 --> 00:34:58.640]   to pretend to be a human being.
[00:34:58.640 --> 00:35:01.520]   That it's okay, you know, there are so many things
[00:35:01.520 --> 00:35:05.120]   we can use AIs as teachers, as doctors, and so forth,
[00:35:05.120 --> 00:35:06.880]   and it's good as long as we know
[00:35:06.880 --> 00:35:09.520]   that we are interacting with an AI.
[00:35:09.520 --> 00:35:12.960]   We should, the same way we ban fake money,
[00:35:12.960 --> 00:35:14.400]   we should ban fake humans.
[00:35:14.400 --> 00:35:19.360]   It's not just banning deepfakes of specific individuals.
[00:35:19.360 --> 00:35:24.160]   It's also banning deepfake of generic humans.
[00:35:24.160 --> 00:35:26.400]   You know, which is already happening
[00:35:26.400 --> 00:35:28.720]   to some extent on social media.
[00:35:28.720 --> 00:35:31.760]   Like if you have lots of bots retweeting something,
[00:35:31.760 --> 00:35:33.600]   then you have the impression,
[00:35:33.600 --> 00:35:35.360]   "Oh, lots of people are interested in that.
[00:35:35.360 --> 00:35:36.160]   That's important."
[00:35:36.160 --> 00:35:40.880]   And this is basically the bots pretending to be humans.
[00:35:40.880 --> 00:35:43.440]   Because if you see a tweet which says,
[00:35:43.440 --> 00:35:46.720]   "500 people retweeted it,"
[00:35:46.720 --> 00:35:49.920]   or you see a tweet and it says,
[00:35:49.920 --> 00:35:51.920]   "500 bots retweeted it,"
[00:35:51.920 --> 00:35:53.760]   I don't care what the bots retweeted,
[00:35:53.760 --> 00:35:56.000]   but if it's humans, okay, that's interesting.
[00:35:56.000 --> 00:36:01.040]   So we need to be very careful that bots can't do that.
[00:36:01.040 --> 00:36:02.480]   They are doing it at present,
[00:36:02.480 --> 00:36:04.160]   and it should be banned.
[00:36:04.160 --> 00:36:06.400]   Now, some people say, "Yes, but freedom of expression."
[00:36:06.400 --> 00:36:10.480]   No, bots don't have freedom of expression.
[00:36:10.480 --> 00:36:15.200]   There is no cost in terms of freedom of expression
[00:36:15.200 --> 00:36:16.240]   when you ban bots.
[00:36:16.240 --> 00:36:19.040]   So again, in some situations, yes,
[00:36:19.040 --> 00:36:21.280]   AIs should interact with us,
[00:36:21.280 --> 00:36:23.040]   but it should be very clear,
[00:36:23.040 --> 00:36:25.200]   this is an AI talking to you,
[00:36:25.200 --> 00:36:28.960]   or this is an AI retweeting this story.
[00:36:28.960 --> 00:36:32.160]   It is not a human being making a conscious decision.
[00:36:32.400 --> 00:36:36.000]   - To push back on this line of fake humans,
[00:36:36.000 --> 00:36:38.960]   because I think it might be a spectrum.
[00:36:38.960 --> 00:36:41.200]   First of all, you might have AI systems
[00:36:41.200 --> 00:36:44.000]   that are offended, hurt,
[00:36:44.000 --> 00:36:47.600]   when you say that they're fake humans.
[00:36:47.600 --> 00:36:51.840]   In fact, they might start identifying as humans.
[00:36:51.840 --> 00:36:57.360]   And you just talked about the power of us humans
[00:36:57.360 --> 00:36:58.640]   with our collective intelligence
[00:36:58.640 --> 00:37:01.200]   to take fake stories and make them quite real.
[00:37:01.440 --> 00:37:06.960]   And so if the feelings you have for the fake human is real,
[00:37:06.960 --> 00:37:10.320]   you know, love is a kind of fake thing
[00:37:10.320 --> 00:37:14.880]   that we all kind of put a word to, a set of feelings.
[00:37:14.880 --> 00:37:18.480]   What if you have that feeling for an AI system?
[00:37:18.480 --> 00:37:22.320]   It starts to change, I mean,
[00:37:22.320 --> 00:37:27.280]   maybe the kind of things AI systems are allowed to do.
[00:37:27.280 --> 00:37:32.240]   For good, they're allowed to create,
[00:37:32.240 --> 00:37:37.120]   communicate suffering, communicate the good stuff,
[00:37:37.120 --> 00:37:39.760]   the longing, the hope, the connection,
[00:37:39.760 --> 00:37:40.960]   the intimacy, all of that.
[00:37:40.960 --> 00:37:44.320]   And in that way, get integrated in our society.
[00:37:44.320 --> 00:37:45.680]   And then you start to ask a question,
[00:37:45.680 --> 00:37:49.280]   and are we allowed to really unplug them?
[00:37:49.280 --> 00:37:51.120]   Are we allowed to really censor them,
[00:37:51.120 --> 00:37:54.800]   remove them, remove their voice from social media?
[00:37:54.800 --> 00:37:56.240]   - I'm not saying that they shouldn't have a voice,
[00:37:56.240 --> 00:37:57.280]   they shouldn't talk with us.
[00:37:57.280 --> 00:37:59.200]   I'm just saying when they talk with us,
[00:37:59.200 --> 00:38:01.040]   it should be clear that they are AI.
[00:38:01.040 --> 00:38:02.180]   That's it.
[00:38:02.180 --> 00:38:05.920]   Don't, you can have your voice as an AI.
[00:38:05.920 --> 00:38:08.160]   Again, I have some medical problem.
[00:38:08.160 --> 00:38:10.400]   I want to get advice from an AI doctor.
[00:38:10.400 --> 00:38:13.600]   That's fine, as long as I know that I'm talking with an AI.
[00:38:13.600 --> 00:38:19.200]   What should be banned is AI pretending to be a human being.
[00:38:19.200 --> 00:38:22.320]   This is something that will erode trust,
[00:38:22.320 --> 00:38:25.040]   and without trust, society collapses.
[00:38:25.600 --> 00:38:29.520]   This is something that especially will endanger democracies,
[00:38:29.520 --> 00:38:31.200]   because democracies are built on,
[00:38:31.200 --> 00:38:33.280]   democracy is a conversation, basically.
[00:38:33.280 --> 00:38:36.880]   And it's a conversation between people.
[00:38:36.880 --> 00:38:41.360]   If you now flood the public sphere with millions
[00:38:41.360 --> 00:38:44.880]   and potentially billions of AI agents
[00:38:44.880 --> 00:38:47.840]   that can hold conversations, they never sleep,
[00:38:47.840 --> 00:38:51.520]   they never eat, they don't have emotions of their own,
[00:38:51.520 --> 00:38:54.800]   they can get to know you and tailor their words
[00:38:54.800 --> 00:38:57.840]   specifically for you and your life story.
[00:38:57.840 --> 00:39:01.120]   They are becoming better than us
[00:39:01.120 --> 00:39:07.200]   at creating stories and ideas and so forth.
[00:39:07.200 --> 00:39:10.160]   If you flood the public sphere with that,
[00:39:10.160 --> 00:39:14.160]   this will ruin the conversation between people.
[00:39:14.160 --> 00:39:16.400]   It will ruin the trust between people.
[00:39:16.400 --> 00:39:20.640]   That's, you will no longer be able to have a democracy
[00:39:20.640 --> 00:39:21.840]   in this situation.
[00:39:21.840 --> 00:39:24.400]   You can have other types of regimes,
[00:39:24.400 --> 00:39:25.360]   but no democracy.
[00:39:25.360 --> 00:39:28.240]   - If we could talk about the big philosophical notion
[00:39:28.240 --> 00:39:29.040]   of truth then.
[00:39:29.040 --> 00:39:34.400]   You've already talked about the capacity of humans.
[00:39:34.400 --> 00:39:37.840]   One of the things that made us special is stories.
[00:39:37.840 --> 00:39:43.040]   So is there such thing as truth?
[00:39:43.040 --> 00:39:44.880]   - Absolutely.
[00:39:44.880 --> 00:39:46.400]   - What is truth, exactly?
[00:39:46.400 --> 00:39:48.320]   - When somebody's suffering, that's true.
[00:39:48.320 --> 00:39:51.120]   I mean, this is why one of the things
[00:39:51.120 --> 00:39:52.160]   when you talk about suffering
[00:39:52.160 --> 00:39:54.400]   as a kind of ultimate reality,
[00:39:54.400 --> 00:39:57.120]   when somebody suffers, that is truth.
[00:39:57.120 --> 00:40:00.880]   Now, somebody can suffer because of a fictional story.
[00:40:00.880 --> 00:40:04.480]   Somebody tells people that God said,
[00:40:04.480 --> 00:40:07.600]   "You must go on this crusade and kill these heretics."
[00:40:07.600 --> 00:40:09.200]   And this is a completely fictional story.
[00:40:09.200 --> 00:40:11.840]   People believe it and they start a war
[00:40:11.840 --> 00:40:13.760]   and they destroy cities and kill people.
[00:40:13.760 --> 00:40:16.480]   The people that suffer because of that,
[00:40:16.480 --> 00:40:18.640]   and even the crusaders themselves
[00:40:18.640 --> 00:40:21.040]   that also suffer the consequences of what they do,
[00:40:21.040 --> 00:40:22.480]   the suffering is true,
[00:40:22.480 --> 00:40:26.080]   even though it is caused by a fictional story.
[00:40:26.080 --> 00:40:31.760]   Similarly, when people agree on certain rules,
[00:40:31.760 --> 00:40:34.640]   the rules could come out of our imagination.
[00:40:34.640 --> 00:40:38.960]   Now, we can be truthful about it and say,
[00:40:38.960 --> 00:40:41.200]   "These rules, they didn't come from heaven.
[00:40:41.200 --> 00:40:43.200]   They came from our imagination."
[00:40:43.200 --> 00:40:44.800]   You know, we look at sports.
[00:40:44.800 --> 00:40:48.160]   So we have rules for the game of football, soccer.
[00:40:48.160 --> 00:40:50.800]   They were invented by people.
[00:40:50.800 --> 00:40:53.280]   Nobody, at least very few people,
[00:40:53.280 --> 00:40:55.840]   claim that the rules of football came down from heaven.
[00:40:55.840 --> 00:40:56.080]   - Yes.
[00:40:56.080 --> 00:40:57.520]   - We invented them.
[00:40:57.520 --> 00:40:58.880]   And this is truthful.
[00:40:58.880 --> 00:41:02.400]   They are fictional rules invented by humans,
[00:41:02.400 --> 00:41:03.120]   and this is true.
[00:41:03.120 --> 00:41:04.640]   They were invented by humans.
[00:41:04.640 --> 00:41:06.640]   And when you are honest about it,
[00:41:06.640 --> 00:41:09.120]   it enables you to change the rules,
[00:41:09.120 --> 00:41:12.000]   which is being done in football every now and then.
[00:41:12.000 --> 00:41:14.960]   It's the same with the fundamental rules of a country.
[00:41:14.960 --> 00:41:19.680]   You can pretend that the rules came down from heaven,
[00:41:19.680 --> 00:41:21.760]   dictated by God or whatever,
[00:41:21.760 --> 00:41:22.960]   and then you can't change them.
[00:41:22.960 --> 00:41:26.400]   Or you can be like, you know, the American Constitution,
[00:41:26.400 --> 00:41:28.160]   which starts with "We the people."
[00:41:28.160 --> 00:41:33.760]   The American Constitution lays down certain rules for a society,
[00:41:33.760 --> 00:41:36.320]   but the amazing thing about it,
[00:41:36.320 --> 00:41:40.080]   it does not pretend to come from an external source.
[00:41:40.080 --> 00:41:45.200]   The Ten Commandments start with "I am your Lord God."
[00:41:46.000 --> 00:41:49.760]   And because it starts with that, you can't change them.
[00:41:49.760 --> 00:41:54.880]   You know, the 10th Commandment, for instance, supports slavery.
[00:41:54.880 --> 00:41:57.840]   The 10th Commandment, in the Ten Commandments,
[00:41:57.840 --> 00:42:01.440]   it says that you should not covet your neighbor's house,
[00:42:01.440 --> 00:42:04.640]   or your neighbor's wife, or your neighbor's slaves.
[00:42:04.640 --> 00:42:08.320]   It's okay to hold slaves, according to the Ten Commandments.
[00:42:08.320 --> 00:42:12.640]   It's just bad to covet the slaves of your neighbor.
[00:42:12.640 --> 00:42:16.880]   Now, there is no 11th Commandment which says,
[00:42:16.880 --> 00:42:20.000]   "If you don't like some of the previous Ten Commandments,
[00:42:20.000 --> 00:42:22.320]   this is how you go about amending them."
[00:42:22.320 --> 00:42:24.560]   Which is why we still have them, unchanged.
[00:42:24.560 --> 00:42:29.920]   Now, in the U.S. Constitution, you have all these rights and rules,
[00:42:29.920 --> 00:42:33.120]   including, originally, the ability to hold slaves.
[00:42:33.120 --> 00:42:36.800]   But the genius of the founding fathers of the United States,
[00:42:37.360 --> 00:42:41.680]   they had the humility to understand,
[00:42:41.680 --> 00:42:44.640]   maybe we don't understand everything.
[00:42:44.640 --> 00:42:46.960]   Maybe we made some mistakes.
[00:42:46.960 --> 00:42:51.280]   So we tell you that these rules did not come from heaven.
[00:42:51.280 --> 00:42:53.040]   They came from us humans.
[00:42:53.040 --> 00:42:54.720]   We may have made a mistake.
[00:42:54.720 --> 00:42:58.640]   So here is a mechanism for how future generations
[00:42:58.640 --> 00:43:02.400]   can amend the Constitution, which was used later on to,
[00:43:02.400 --> 00:43:05.280]   for instance, amend the Constitution to ban slavery.
[00:43:06.000 --> 00:43:10.000]   So now you're describing some interesting and powerful ideas
[00:43:10.000 --> 00:43:11.280]   throughout human history.
[00:43:11.280 --> 00:43:14.720]   Can you just speak to the mechanism of how humans believe,
[00:43:14.720 --> 00:43:17.360]   start to believe ideas?
[00:43:17.360 --> 00:43:19.520]   Is there something interesting to say there,
[00:43:19.520 --> 00:43:20.720]   from your thinking about it?
[00:43:20.720 --> 00:43:25.280]   Like how idea is born, and how it takes hold,
[00:43:25.280 --> 00:43:27.840]   and how it spreads, and how it competes with other ideas?
[00:43:27.840 --> 00:43:31.840]   First of all, ideas are an independent force in history.
[00:43:31.840 --> 00:43:35.440]   Marxists tend to deny that.
[00:43:35.440 --> 00:43:41.760]   Marxists think that all history is just a play of material interests.
[00:43:41.760 --> 00:43:46.400]   And ideas, stories, they are just a smokescreen
[00:43:46.400 --> 00:43:49.920]   to hide the underlying interests.
[00:43:49.920 --> 00:43:54.000]   My thoughts are, to some extent, the opposite.
[00:43:54.000 --> 00:43:58.480]   We have some biological objective interests
[00:43:58.480 --> 00:44:00.960]   that all humans share, like we need to eat,
[00:44:00.960 --> 00:44:04.400]   we need to drink, we need to breathe.
[00:44:05.360 --> 00:44:08.000]   But most conflicts in history are not about that.
[00:44:08.000 --> 00:44:13.280]   The interests which really drive most conflicts in history
[00:44:13.280 --> 00:44:14.800]   don't come from biology.
[00:44:14.800 --> 00:44:18.960]   They come from religions, and ideologies, and stories.
[00:44:18.960 --> 00:44:22.560]   So it's not that stories are a smokescreen
[00:44:22.560 --> 00:44:24.880]   to hide the real interests.
[00:44:24.880 --> 00:44:29.200]   The stories create the interests in the first place.
[00:44:29.200 --> 00:44:32.240]   The stories define who are the competing groups.
[00:44:33.280 --> 00:44:37.600]   Nations, religions, cultures, they are not biological entities.
[00:44:37.600 --> 00:44:40.480]   They are not like species, like gorillas and chimpanzees.
[00:44:40.480 --> 00:44:40.980]   No.
[00:44:40.980 --> 00:44:44.400]   Israelis and Palestinians, or Germans and French,
[00:44:44.400 --> 00:44:46.320]   or Chinese and Americans,
[00:44:46.320 --> 00:44:50.080]   they have no essential biological difference between them.
[00:44:50.080 --> 00:44:51.520]   The difference is cultural.
[00:44:51.520 --> 00:44:52.960]   It comes from stories.
[00:44:52.960 --> 00:44:55.280]   There are people that believe in different stories.
[00:44:55.280 --> 00:44:57.760]   The stories create the identity.
[00:44:57.760 --> 00:44:59.840]   The stories create the interests.
[00:45:00.400 --> 00:45:03.440]   Israelis and Palestinians are fighting over Jerusalem,
[00:45:03.440 --> 00:45:05.680]   not because of any material interest.
[00:45:05.680 --> 00:45:08.320]   There are no oil fields under Jerusalem.
[00:45:08.320 --> 00:45:13.360]   And even oil, you need it to realize some cultural fantasy.
[00:45:13.360 --> 00:45:15.440]   It doesn't really come from biology.
[00:45:15.440 --> 00:45:19.600]   So the stories are independent forces.
[00:45:19.600 --> 00:45:22.480]   Now, why do people believe one story and not another?
[00:45:22.480 --> 00:45:24.560]   That's history.
[00:45:24.560 --> 00:45:28.480]   There is no materialistic law.
[00:45:28.480 --> 00:45:30.320]   People will always believe this.
[00:45:30.320 --> 00:45:31.040]   No.
[00:45:31.040 --> 00:45:33.120]   History is full of accidents.
[00:45:33.120 --> 00:45:37.520]   How did Christianity become the most successful religion in the world?
[00:45:37.520 --> 00:45:39.280]   We can't explain it.
[00:45:39.280 --> 00:45:44.160]   Why this story about Jesus of Nazareth?
[00:45:44.160 --> 00:45:52.320]   The Roman Empire in the third century CE was a bit like, I don't know, California today.
[00:45:52.320 --> 00:45:57.920]   Like so many sects and subsects and gurus and religions.
[00:45:57.920 --> 00:45:59.040]   Everybody has their own thing.
[00:45:59.040 --> 00:46:04.880]   And you have thousands of different stories competing.
[00:46:04.880 --> 00:46:06.960]   Why did Christianity come up on top?
[00:46:06.960 --> 00:46:11.440]   As a historian, I don't have a kind of clear answer.
[00:46:11.440 --> 00:46:17.520]   You can read the sources and you see how it happened.
[00:46:17.520 --> 00:46:20.880]   Oh, this happened, and then this happened, and then Constantine adopted it,
[00:46:20.880 --> 00:46:21.840]   and then this, and then this.
[00:46:21.840 --> 00:46:22.960]   But why?
[00:46:22.960 --> 00:46:26.720]   I don't think anybody has an answer to that.
[00:46:26.720 --> 00:46:35.200]   If you rewind the movie of history and press play, and you rewind and press play a hundred times,
[00:46:35.200 --> 00:46:40.640]   I think Christianity would take over the Roman Empire in the world, maybe twice out of a hundred times.
[00:46:40.640 --> 00:46:43.440]   It was such an unlikely thing to happen.
[00:46:43.440 --> 00:46:45.280]   It's the same with Islam.
[00:46:45.280 --> 00:46:48.640]   It's the same, I don't know, with the communist takeover of Russia.
[00:46:49.600 --> 00:46:57.200]   In 1914, if you told people that in three years Lenin and the Bolsheviks will gain power in the
[00:46:57.200 --> 00:47:00.400]   Tsarist Empire, they would think you're utterly crazy.
[00:47:00.400 --> 00:47:08.880]   You know, Lenin had a few thousand supporters in 1914 in an empire of close to 200 million people.
[00:47:08.880 --> 00:47:11.200]   It sounded ludicrous.
[00:47:11.200 --> 00:47:18.320]   Now, we know the chain of events, the First World War, the February Revolution, and so
[00:47:18.320 --> 00:47:23.440]   forth that led to the communist takeover, but it was such an unlikely event.
[00:47:23.440 --> 00:47:25.200]   And it happened.
[00:47:25.200 --> 00:47:28.640]   And the little steps along the way, the little options you have along the way, because,
[00:47:28.640 --> 00:47:32.080]   you know, Stalin versus Trotsky, you could have the Robert Frost poem.
[00:47:32.080 --> 00:47:32.480]   There's always...
[00:47:32.480 --> 00:47:32.980]   Yes.
[00:47:32.980 --> 00:47:40.160]   And history often takes, you know, there is a highway, and there is a kind of sideway,
[00:47:40.160 --> 00:47:43.520]   and history takes the sideways many, many times.
[00:47:43.520 --> 00:47:47.840]   And it's perhaps tempting to tell some of that history through charismatic leaders,
[00:47:47.840 --> 00:47:53.840]   and maybe it's an open question how much power charismatic leaders have to affect
[00:47:53.840 --> 00:47:55.200]   the trajectory of history.
[00:47:55.200 --> 00:47:58.720]   You've met quite a lot of charismatic leaders lately.
[00:47:58.720 --> 00:48:01.280]   I mean, what's your view on that?
[00:48:01.280 --> 00:48:02.960]   I find it a compelling notion.
[00:48:02.960 --> 00:48:05.600]   I'm a sucker for a great speech and a vision.
[00:48:05.600 --> 00:48:14.960]   So I have a sense that there's an importance for a leader to catalyze the viral spread of a story.
[00:48:16.160 --> 00:48:22.640]   So, like, I think we need leaders to be just great storytellers that kind of sharpen up
[00:48:22.640 --> 00:48:26.560]   the story to make sure it infiltrates everybody's brain effectively.
[00:48:26.560 --> 00:48:34.640]   But it could also be that the local interactions between humans is even more important.
[00:48:34.640 --> 00:48:37.760]   But it's just we don't have a good way to sort of summarize and describe that.
[00:48:37.760 --> 00:48:45.040]   We like to talk about, you know, Steve Jobs as central to the development of the computer,
[00:48:45.040 --> 00:48:49.600]   maybe Bill Gates, and you tell it to the stories of individuals like this,
[00:48:49.600 --> 00:48:52.800]   because it's just easier to tell a sexy story that way.
[00:48:52.800 --> 00:48:58.320]   Maybe it's an interplay, because you have the kind of structural forces that, I don't know,
[00:48:58.320 --> 00:49:06.480]   you look at the geography of the planet, and you look at shipping technology in the late 15th
[00:49:06.480 --> 00:49:13.680]   century in Europe and the Mediterranean, and it's almost inevitable that pretty quickly somebody
[00:49:13.680 --> 00:49:18.560]   will discover America, somebody from the old world will get to the new world.
[00:49:18.560 --> 00:49:24.400]   So this was not a kind of, this didn't, if it wasn't Columbus, then it would have been a five
[00:49:24.400 --> 00:49:31.280]   years later somebody else. But the key thing about history is that these small differences
[00:49:31.280 --> 00:49:38.240]   make a huge, huge difference. You know, if it wasn't Columbus, if it was five years later,
[00:49:38.240 --> 00:49:44.160]   somebody from England, then maybe all of Latin America today would be speaking English and not
[00:49:44.160 --> 00:49:49.920]   Spanish. If it was somebody from the Ottoman Empire, it's completely different world history.
[00:49:49.920 --> 00:49:56.960]   If you have, and you know, the Ottoman Empire at that time was also shaping up to be a major
[00:49:56.960 --> 00:50:06.480]   maritime empire. If you have America being reached by Muslim navigators before Christian navigators
[00:50:06.480 --> 00:50:10.720]   from Europe, you have a completely different world history. It's the same with the computer.
[00:50:10.720 --> 00:50:20.080]   Given the economic incentives and the science and technology of the time, then the rise of
[00:50:20.080 --> 00:50:26.720]   the personal computer was probably inevitable sometime in the late 20th century. But the where
[00:50:26.720 --> 00:50:34.800]   and when is crucial. The fact that it was California in the 1970s and not, say, I don't know,
[00:50:34.800 --> 00:50:42.240]   Japan in the 1980s or China in the 1990s, this made a huge, huge difference. So you have this
[00:50:42.240 --> 00:50:48.240]   interplay between the structural forces, which are beyond the control of any single charismatic
[00:50:48.240 --> 00:50:54.960]   leader, but then the small changes, they can have a big effect. I think, for instance, about the war
[00:50:54.960 --> 00:51:02.800]   in Ukraine. There was a moment, now it's a struggle between nations, but there was a moment when the
[00:51:02.800 --> 00:51:08.320]   decision was taken in the mind of a single individual of Vladimir Putin, and he could
[00:51:08.320 --> 00:51:13.040]   have decided otherwise, and the world would have looked completely different.
[00:51:13.040 --> 00:51:19.840]   - And another leader, Vladimir Zelensky, could have decided to leave Kiev in the early days.
[00:51:19.840 --> 00:51:25.680]   There's a lot of decisions that kind of ripple. So you write in Homo Deus about Hitler,
[00:51:27.360 --> 00:51:32.320]   and in part that he was not a very impressive person.
[00:51:32.320 --> 00:51:33.680]   - I say that?
[00:51:33.680 --> 00:51:36.480]   - The quote is, let me read it.
[00:51:36.480 --> 00:51:37.040]   - Okay.
[00:51:37.040 --> 00:51:45.120]   - He wasn't a senior officer. In four years of war, he rose no higher than the rank of corporal.
[00:51:45.120 --> 00:51:48.800]   He had no formal education. Perhaps you mean his resume was not impressive.
[00:51:48.800 --> 00:51:51.360]   - Yeah, his resume was not impressive. That's true.
[00:51:51.360 --> 00:51:56.400]   - He had no formal education, no professional skills, no political background. He wasn't a
[00:51:56.400 --> 00:52:01.280]   successful businessman or a union activist. He didn't have friends or relatives in high places,
[00:52:01.280 --> 00:52:06.720]   nor any money to speak of. So how did he amass so much power?
[00:52:06.720 --> 00:52:11.680]   What ideology, what circumstances enabled the rise of the Third Reich?
[00:52:11.680 --> 00:52:18.640]   - Again, I can't tell you the why. I can tell you the how. I don't think it was inevitable.
[00:52:18.640 --> 00:52:24.720]   I think that if a few things were different, there would have been no Third Reich. There would have
[00:52:24.720 --> 00:52:29.920]   been no Nazism, no Holocaust. Again, this is the tragedy. If it would have been inevitable,
[00:52:29.920 --> 00:52:35.200]   then what can you do? This is the laws of history or the laws of physics. But the tragedy is no,
[00:52:35.200 --> 00:52:42.800]   it was decisions by humans that led to that direction. And even from the viewpoint of the
[00:52:42.800 --> 00:52:52.560]   Germans, we know for a fact it was an unnecessary path to take. Because in the 1920s and '30s,
[00:52:53.120 --> 00:53:02.160]   the Nazis said that unless Germany take this road, it will never be prosperous. It will never
[00:53:02.160 --> 00:53:08.960]   be successful. All the other countries will keep stepping on it. This was their claim.
[00:53:08.960 --> 00:53:17.760]   And we know for a fact this is false. Why? Because they took that road, they lost the Second World
[00:53:17.760 --> 00:53:25.600]   War, and after they lost, then they became one of the most prosperous countries in the world
[00:53:25.600 --> 00:53:31.920]   because their enemies that defeated them evidently supported them and allowed them
[00:53:31.920 --> 00:53:39.520]   to become such a prosperous and successful nation. So if you can lose the war and still be
[00:53:39.520 --> 00:53:43.600]   so successful, obviously you could just have skipped the war. You didn't need it.
[00:53:45.280 --> 00:53:50.480]   You really had to have the war in order to have a prosperous Germany in the 19th century? Absolutely
[00:53:50.480 --> 00:53:58.240]   not. And it's the same with Japan. It's the same with Italy. So it was not inevitable. It was not
[00:53:58.240 --> 00:54:06.400]   the forces of history that necessitated, that forced Germany to take this path. I think part
[00:54:06.400 --> 00:54:15.280]   of it is part of the appeal of… Again, Hitler was a very, very skillful storyteller. He told
[00:54:15.280 --> 00:54:23.280]   people a story. The fact that he was nobody made it even more effective because people at that time,
[00:54:23.280 --> 00:54:31.280]   after the defeat of the First World War, after the repeated economic crisis of the 1920s in Germany,
[00:54:31.280 --> 00:54:39.680]   people felt betrayed by all the established elites, by all the established institutions,
[00:54:39.680 --> 00:54:45.040]   all these professors and politicians and industrialists and military, all the big people.
[00:54:45.040 --> 00:54:52.160]   They led us to a disastrous war. They led us to humiliation. So we don't want any of them.
[00:54:52.160 --> 00:54:58.960]   And then you have this nobody, a corporal with no money, with no education, with no titles,
[00:54:58.960 --> 00:55:06.400]   with nothing. And he tells people, "I'm one of you." And this was one reason why he was so popular.
[00:55:06.400 --> 00:55:13.760]   And then the story he told, when you look at stories, at the competition between different
[00:55:13.760 --> 00:55:20.400]   stories and between stories, fiction, and the truth, the truth has two big problems.
[00:55:21.520 --> 00:55:26.720]   The truth tends to be complicated and the truth tends to be painful.
[00:55:26.720 --> 00:55:35.200]   The real story of… Let's talk about nations. The real story of every nation is complicated
[00:55:35.200 --> 00:55:42.880]   and it contains some painful episodes. We are not always good. We sometimes do bad things.
[00:55:42.880 --> 00:55:50.160]   Now, if you go to people and you tell them a complicated and painful story, many of them don't
[00:55:50.160 --> 00:55:59.200]   want to listen. The advantage of fiction is that it can be made as simple and as painless, attractive
[00:55:59.200 --> 00:56:05.920]   as you want it to be, because it's fiction. And then what you see is that politicians like Hitler,
[00:56:05.920 --> 00:56:13.440]   they create a very simple story. We are the heroes. We always do good things. Everybody's
[00:56:13.440 --> 00:56:20.560]   against us. Everybody's trying to trample us. And this is very attractive. One of the things
[00:56:20.560 --> 00:56:27.760]   people don't understand about Nazism and fascism, we teach in schools about fascism and Nazism
[00:56:27.760 --> 00:56:35.920]   as this ultimate evil, the ultimate monster in human history. And at some level, this is wrong,
[00:56:37.120 --> 00:56:44.720]   because it makes people… It actually exposes us. Why? Because people hear, "Oh, fascism is this
[00:56:44.720 --> 00:56:53.760]   monster." And then when you hear the actual fascist story, what fascists tell you is always
[00:56:53.760 --> 00:56:59.760]   very beautiful and attractive. Fascists are people who come and tell you, "You are wonderful.
[00:56:59.760 --> 00:57:06.400]   You belong to the most wonderful group of people in the world. You are beautiful. You are ethical.
[00:57:06.400 --> 00:57:12.160]   Everything you do is good. You have never done anything wrong. There are all these evil monsters
[00:57:12.160 --> 00:57:16.720]   out there that are out to get you, and they are causing all the problems in the world."
[00:57:16.720 --> 00:57:23.360]   And when people hear that, it's like looking in the mirror and seeing something very beautiful.
[00:57:23.360 --> 00:57:29.200]   "Hey, I'm beautiful. We've never done anything wrong. We are victims. Everybody's…" And
[00:57:29.200 --> 00:57:35.280]   when you look… And you heard in school that fascism, that fascists are monsters.
[00:57:35.280 --> 00:57:39.760]   And you look in the mirror, you see something very beautiful. And you say, "I can't be a fascist
[00:57:39.760 --> 00:57:43.200]   because fascists are monsters, and this is so beautiful, so it can't be."
[00:57:43.200 --> 00:57:51.760]   But when you look in the fascist mirror, you never see a monster. You see the most beautiful
[00:57:51.760 --> 00:57:56.400]   thing in the world. And that's the danger. This is the problem with Hollywood's…
[00:57:56.400 --> 00:58:02.640]   I look at Voldemort in Harry Potter. Who would like to follow this creep?
[00:58:02.640 --> 00:58:03.520]   Yeah.
[00:58:03.520 --> 00:58:08.400]   And you look at Darth Vader. This is not somebody you would like to follow.
[00:58:08.400 --> 00:58:14.240]   Christianity got things much better when it described the devil as being very beautiful
[00:58:14.240 --> 00:58:21.040]   and attractive. That's the danger, that you see something is very beautiful, you don't understand
[00:58:21.040 --> 00:58:22.400]   the monster underneath.
[00:58:22.400 --> 00:58:26.960]   And you're right precisely about this. And by the way, it's just a small aside.
[00:58:28.720 --> 00:58:33.280]   It always saddens me when people say how obvious it is to them that communism
[00:58:33.280 --> 00:58:40.560]   is a flawed ideology. When you ask them, try to put your mind, try to put yourself
[00:58:40.560 --> 00:58:45.680]   in the beginning of the 20th century and see what you would do. A lot of people will say
[00:58:45.680 --> 00:58:51.280]   it's obvious that it's a flawed ideology. So, I mean, I suppose to some of the worst ideologies
[00:58:51.280 --> 00:58:56.320]   in human history, you could say the same. And in that mirror, when you look, it looks beautiful.
[00:58:56.320 --> 00:59:00.560]   Communism is the same. Also, you look in the communist mirror, you're the most ethical,
[00:59:00.560 --> 00:59:06.560]   wonderful person ever. It's very difficult to see Stalin underneath it.
[00:59:06.560 --> 00:59:12.960]   So, yeah, in "Homo Deus" you also write, "During the 19th and 20th centuries, as humanism gained
[00:59:12.960 --> 00:59:17.520]   increasing social credibility and political power, it sprouted two very different offshoots.
[00:59:17.520 --> 00:59:22.720]   Socialist humanism, which encompassed a plethora of socialist and communist movements,
[00:59:22.720 --> 00:59:28.800]   and evolutionary humanism, whose most famous advocates were the Nazis." So, if you can just
[00:59:28.800 --> 00:59:34.240]   linger on that, what's the ideological connection between Nazism and communism as embodied by
[00:59:34.240 --> 00:59:42.960]   humanism? Humanism basically is, you know, the focus is on humans, that they are the most important
[00:59:42.960 --> 00:59:49.520]   thing in the world. They move history. But then there is a big question, what is, what are humans?
[00:59:49.520 --> 00:59:59.200]   What is humanity? Now, liberals, they place at the center of the story individual humans,
[00:59:59.200 --> 01:00:06.160]   and they don't see history as a kind of necessary collision between big forces.
[01:00:06.160 --> 01:00:10.320]   They place the individual at the center. If you want to know, you know, there is a bad,
[01:00:10.320 --> 01:00:16.160]   especially in the US today, liberal is taken as the opposite of conservative.
[01:00:16.880 --> 01:00:22.320]   But it's, to test whether you're liberal, you need to answer just three questions. Very simple.
[01:00:22.320 --> 01:00:27.200]   Do you think people should have the right to choose their own government,
[01:00:27.200 --> 01:00:33.920]   or the government should be imposed by some outside force? Do you think people should have
[01:00:33.920 --> 01:00:41.520]   the right to the liberty to choose their own profession, or either born into some caste that
[01:00:41.520 --> 01:00:46.640]   predetermines what they do? And do you think people should have the liberty to choose their
[01:00:46.640 --> 01:00:54.080]   own spouse, and their own way of personal life, instead of being told by elders or parents who
[01:00:54.080 --> 01:00:59.520]   to marry and how to live? Now, if you answered yes to all three questions, people should have
[01:00:59.520 --> 01:01:04.800]   the liberty to choose their government, their profession, their personal lives, their spouse,
[01:01:04.800 --> 01:01:14.000]   then you're a liberal. And most conservatives are also liberal. Now, communists and fascists,
[01:01:14.000 --> 01:01:21.600]   they answer differently. For them, history is not, yes, history is about humans. Humans are the big
[01:01:21.600 --> 01:01:29.120]   heroes of history, but not individual humans and their liberties. Fascists imagine history as a
[01:01:29.120 --> 01:01:39.280]   clash between races or nations. The nation is at the center. They say the supreme good is the good
[01:01:39.280 --> 01:01:46.000]   of the nation. You should have 100% loyalty only to the nation. You know, liberals say, yes, you
[01:01:46.000 --> 01:01:50.480]   should be loyal to the nation, but it's not the only thing. There are other things in the world.
[01:01:50.480 --> 01:01:57.440]   There are human rights. There is truth. There is beauty. Many times, yes, you should prefer the
[01:01:57.440 --> 01:02:04.320]   interests of your nation over other things, but not always. If your nation tells you to murder
[01:02:04.320 --> 01:02:12.720]   millions of innocent people, you don't do that, even though the nation tells you to do it. To lie
[01:02:12.720 --> 01:02:19.760]   for the national interest, you know, in extreme situations, maybe, but in many cases, your loyalty
[01:02:19.760 --> 01:02:26.800]   should be to the truth, even if it makes your nation look a bit not in the best light. The
[01:02:26.800 --> 01:02:31.520]   same with beauty. You know, how does a fascist determine whether a movie is a good movie?
[01:02:32.080 --> 01:02:37.520]   Very simple. If it serves the interests of the nation, this is a good movie. If it's against
[01:02:37.520 --> 01:02:43.280]   the interests of the nation, this is a bad movie. End of story. Liberalism says, no, there is
[01:02:43.280 --> 01:02:51.760]   aesthetic values in the world. We should judge movies not just on the question whether they
[01:02:51.760 --> 01:03:00.080]   serve the national interest, but also on artistic value. Communists are a bit like the fascists,
[01:03:00.080 --> 01:03:06.640]   instead that they don't place the nation as the main hero, they place class as the main hero.
[01:03:06.640 --> 01:03:10.640]   For them, history, again, it's not about individuals, it's not about nations. History is
[01:03:10.640 --> 01:03:17.920]   a clash between classes. And just as fascists imagine in the end only one nation will be on top,
[01:03:17.920 --> 01:03:24.080]   the communists think in the end only one class should be on top, and that's the proletariat.
[01:03:24.080 --> 01:03:33.280]   And same story. A hundred percent of your loyalty should be to the class. And if there is a clash,
[01:03:33.280 --> 01:03:39.840]   say, between class and family, class wins. Like in the Soviet Union, the party told children,
[01:03:39.840 --> 01:03:47.120]   if you hear your parents say something bad about Stalin, you have to report them. And there are
[01:03:47.120 --> 01:03:51.920]   many cases when children reported their parents and their parents were sent to the gulag.
[01:03:53.280 --> 01:04:00.640]   Like, and, you know, your loyalty is to the party, which leads the proletariat to victory
[01:04:00.640 --> 01:04:07.920]   in the historical struggle. And the same way in communism, art is only about class struggle.
[01:04:07.920 --> 01:04:14.160]   A movie is good if it serves the interests of the proletariat. Artistic values, there is nothing
[01:04:14.160 --> 01:04:20.480]   like that. And the same with truth. Everything that we see now in fake news, you know,
[01:04:20.480 --> 01:04:28.880]   the communist propaganda machine was there before us. The level of lies, of disinformation campaigns
[01:04:28.880 --> 01:04:36.080]   that they orchestrated in the 1920s and 30s and 40s is really unimaginable.
[01:04:36.080 --> 01:04:43.680]   So the reason these two ideologies, classes of ideologies, failed is the sacrifice of truth,
[01:04:43.680 --> 01:04:49.840]   not just failed, but did a lot of damage, is the sacrifice of truth and sacrifice of beauty.
[01:04:50.480 --> 01:04:55.600]   And sacrifice of hundreds of millions of people disregard, again, for human suffering. Like,
[01:04:55.600 --> 01:05:02.080]   okay, for in order to, for our nation to win, in order for our class to win, we need to kill those
[01:05:02.080 --> 01:05:10.320]   millions, kill those millions. That was an ethics, aesthetics, truth, they don't matter. The only
[01:05:10.320 --> 01:05:19.040]   thing that matters is the victory of the state or the victory of the class. And that's, and liberalism
[01:05:19.040 --> 01:05:27.120]   was the antithesis to that. It says, no, not only, it has a much more complicated view of the world.
[01:05:27.120 --> 01:05:32.960]   Again, both communism and fascism, they had a very simple view of the world. There is one,
[01:05:32.960 --> 01:05:37.280]   your loyalty, a hundred percent of it should be only to one thing.
[01:05:37.280 --> 01:05:42.640]   Now, liberalism has a much more complex view of the world. It says, yes, there are nations,
[01:05:42.640 --> 01:05:48.160]   they are important. Yes, there are classes, they are important, but they are not the only thing.
[01:05:48.160 --> 01:05:56.240]   There are also families, there are also individuals, there are also animals, and your loyalty should be
[01:05:56.240 --> 01:06:03.360]   divided between all of them. Sometimes you prefer this, sometimes you prefer that. That's complicated,
[01:06:03.360 --> 01:06:09.360]   and, but, you know, life is complicated. - But also, I think, maybe you can correct me,
[01:06:09.360 --> 01:06:14.960]   but liberalism acknowledges the corrupting nature of power when there's a guy at the top,
[01:06:15.600 --> 01:06:24.480]   sits there for a while, managing things, is probably gonna start losing a good sense of reality
[01:06:24.480 --> 01:06:33.200]   and losing the capability to be a good manager. It feels like the communist and fascist regimes
[01:06:33.200 --> 01:06:38.640]   don't acknowledge that basic characteristic of human nature, that power corrupts.
[01:06:38.640 --> 01:06:44.720]   - Yes, they believe in infallibility. In this sense, they are very close to being religions.
[01:06:45.520 --> 01:06:52.000]   They, in Nazism, Hitler was considered infallible, and therefore you don't need any checks and
[01:06:52.000 --> 01:06:57.920]   balances on his power. Why do you need to balance an infallible genius? And it's the same with the
[01:06:57.920 --> 01:07:04.400]   Soviet Union, with Stalin, and more generally with the Communist Party. The Party can never
[01:07:04.400 --> 01:07:10.000]   make a mistake, and therefore you don't need independent courts, independent media,
[01:07:10.000 --> 01:07:16.720]   opposition parties, things like that, because the Party is never wrong. You concentrate the same way
[01:07:16.720 --> 01:07:22.480]   a hundred percent of loyalty should be to the Party, a hundred percent of power should be in
[01:07:22.480 --> 01:07:28.800]   the hands of the Party. The whole idea of liberal democracy is embracing fallibility. Everybody is
[01:07:28.800 --> 01:07:35.360]   fallible. All people, all leaders, all political parties, all institutions. This is why we need
[01:07:35.360 --> 01:07:42.080]   checks and balances, and we need many of them. If you have just one, then this particular check
[01:07:42.080 --> 01:07:50.000]   itself could make terrible mistakes. So you need, say, you need the press, you need the media
[01:07:50.000 --> 01:07:55.680]   to serve as a check to the government. You don't have just one newspaper or one TV station. You
[01:07:55.680 --> 01:08:01.040]   need many so that they can balance each other. And the media is not enough. So you have independent
[01:08:01.040 --> 01:08:07.200]   courts, you have free academic institutions, you have NGOs, you have a lot of checks and balances.
[01:08:07.200 --> 01:08:12.800]   So that's the ideologies and the leaders. What about the individual people, the millions of
[01:08:12.800 --> 01:08:24.560]   people that play a part in all of this, that are the hosts of the stories, that are the catalyst
[01:08:24.560 --> 01:08:32.320]   and the components of how the story spreads? Would you say that all of us are capable of
[01:08:32.320 --> 01:08:41.920]   spreading any story? Sort of the Solzhenitsyn idea that all of us are capable of good and evil,
[01:08:41.920 --> 01:08:45.440]   the line between good and evil runs through the heart of every man.
[01:08:45.440 --> 01:08:53.840]   Yes. I wouldn't say that every person is capable of every type of evil, but we are all fallible.
[01:08:54.720 --> 01:09:03.200]   It partly depends on the efforts we make to develop our self-awareness during life.
[01:09:03.200 --> 01:09:12.240]   Part of it depends on moral luck. If you are born as a Christian German
[01:09:12.240 --> 01:09:21.680]   in the 1910s or 1920s and you grow up in Nazi Germany, that's bad moral luck. Your chances
[01:09:21.680 --> 01:09:29.360]   of committing terrible things, you have a very high chance of doing it. And you can withstand it,
[01:09:29.360 --> 01:09:36.480]   but it will take tremendous effort. If you are born in Germany after the war, you're morally lucky.
[01:09:36.480 --> 01:09:44.720]   You will not be put to such a test. You will not need to exert these enormous efforts not to
[01:09:44.720 --> 01:09:51.520]   commit atrocities. So this is just part of history. There is an element of luck. But again, part of
[01:09:51.520 --> 01:10:00.960]   it is also self-awareness. You asked me earlier about the potential of power to corrupt. I listened
[01:10:00.960 --> 01:10:06.480]   to the interview you just did with Prime Minister Netanyahu a couple of days ago. One of the things
[01:10:06.480 --> 01:10:14.240]   that most struck me during the interview was that you asked him, "Are you afraid of this thing,
[01:10:14.880 --> 01:10:20.560]   that power corrupts?" He didn't think for a single second. He didn't pose. He didn't admit
[01:10:20.560 --> 01:10:32.880]   a tiny little level of doubt. "No, power doesn't corrupt." For me, it was a shocking and revealing
[01:10:32.880 --> 01:10:40.320]   moment. And it dovetails with how you began the interview. I really liked your opening gambit.
[01:10:42.160 --> 01:10:49.120]   No, really. You kind of told him, "Lots of people in the world are angry with you. Some people hate
[01:10:49.120 --> 01:10:55.440]   you. They dislike you. What do you want to tell them, to say to them?" And you gave him this kind
[01:10:55.440 --> 01:11:04.000]   of platform. And I was very excited. "What will he say?" And he just denied it. He basically denied
[01:11:04.000 --> 01:11:10.080]   it. He had to cut short the interview from three hours to one hour because you had hundreds of
[01:11:10.080 --> 01:11:15.600]   thousands of Israelis in the streets demonstrating against him. And he goes and says, "No, everybody
[01:11:15.600 --> 01:11:20.240]   likes me. What are you talking about?" But on that topic, you've said recently that
[01:11:20.240 --> 01:11:27.680]   the Prime Minister Benjamin Netanyahu may go down in history as the man who destroys Israel.
[01:11:27.680 --> 01:11:33.440]   Can you explain what you mean by that? Yes. I mean, he is basically tearing apart
[01:11:33.440 --> 01:11:40.720]   the social contract that held this country together for 75 years. He's destroying the foundations
[01:11:40.720 --> 01:11:46.960]   of Israeli democracy. You know, I don't want to go too deep, unless you want it, because I guess
[01:11:46.960 --> 01:11:52.080]   most of our listeners, they have bigger issues on their minds than the fate of some small country
[01:11:52.080 --> 01:11:56.960]   in the Middle East. But for those who want to understand what's happening in Israel, there is
[01:11:56.960 --> 01:12:05.200]   really just one question to ask. What limits the power of the government? In the United States,
[01:12:05.200 --> 01:12:11.360]   for instance, there are lots of checks and balances that limit the power of the government.
[01:12:11.360 --> 01:12:17.760]   You have the Supreme Court, you have the Senate, you have the House of Representatives,
[01:12:17.760 --> 01:12:23.040]   you have the President, you have the Constitution, you have 50 states, each state with its own
[01:12:23.040 --> 01:12:30.160]   constitution and Supreme Court and Congress and governor. If somebody wants to pass a dangerous
[01:12:30.160 --> 01:12:36.320]   legislation, say in the House, it will have to go through so many obstacles. Like if you want to
[01:12:36.320 --> 01:12:45.280]   pass a law in the United States taking away voting rights from Jews or from Muslims or from African
[01:12:45.280 --> 01:12:50.720]   Americans, even if it passes, even if it has a majority in the House of Representatives, it is
[01:12:50.720 --> 01:12:55.840]   a very, very, very small chance of becoming the law of the country, because it will have to pass
[01:12:55.840 --> 01:12:59.840]   again through the Senate, through the President, through the Supreme Court, and all the federal
[01:12:59.840 --> 01:13:06.880]   structure. In Israel, we have just a single check on the power of the government, and that's the
[01:13:06.880 --> 01:13:12.640]   Supreme Court. There is really no difference between the government and the legislature,
[01:13:12.640 --> 01:13:18.960]   because whoever, there are no separate elections like in the US. If you win majority in the Knesset,
[01:13:18.960 --> 01:13:25.120]   in the parliament, you appoint the government. That's very simple. And if you have 61 members
[01:13:25.120 --> 01:13:32.480]   of Knesset who vote, let's say, on a law to take away voting rights from Arab citizens of Israel,
[01:13:32.480 --> 01:13:37.280]   there is a single check that can prevent it from becoming the law of the land, and that's
[01:13:37.280 --> 01:13:43.760]   the Supreme Court. And now the Netanyahu government is trying to neutralize or take over the Supreme
[01:13:43.760 --> 01:13:50.160]   Court, and they've already prepared a long list of laws. They already talk about it. What will
[01:13:50.160 --> 01:13:58.080]   happen the moment that this last check on the power is gone? They are openly trying to gain
[01:13:58.080 --> 01:14:06.400]   unlimited power, and they openly talk about it, that once they have it, then they will take away
[01:14:06.400 --> 01:14:13.920]   the rights of Arabs, of LGBT people, of women, of secular Jews. And this is why you have hundreds
[01:14:13.920 --> 01:14:22.240]   of thousands of people in the streets. You have Air Force pilots saying, "We are stop, we stop
[01:14:22.240 --> 01:14:28.240]   flying." This is unheard of in Israel. I mean, we are still living under existential threat
[01:14:28.240 --> 01:14:35.200]   from Iran, from other enemies. And in the middle of this, you have Air Force pilots
[01:14:35.200 --> 01:14:40.640]   who dedicated their lives to protecting the country, and they are saying, "That's it.
[01:14:40.640 --> 01:14:45.680]   If this government doesn't stop what it is doing, we stop flying."
[01:14:45.680 --> 01:14:53.280]   So, as you said, I just did the interview, and as we were doing the interview, there's protests
[01:14:53.280 --> 01:14:56.960]   in the streets. Do you think the protests will have an effect?
[01:14:56.960 --> 01:15:04.720]   I hope so very much. I'm going to many of these protests. I hope they will have an effect. If we
[01:15:04.720 --> 01:15:10.960]   fail, this is the end of Israeli democracy, probably. This will have repercussions far
[01:15:10.960 --> 01:15:18.560]   beyond the borders of Israel. Israel is a nuclear power. Israel has one of the most advanced
[01:15:18.560 --> 01:15:23.040]   cyber capabilities in the world, able to strike basically anywhere in the world.
[01:15:23.040 --> 01:15:31.840]   If this country becomes a fundamentalist and militarist dictatorship, it can set fire to
[01:15:31.840 --> 01:15:40.480]   the entire Middle East. It can, again, have destabilizing effects far beyond the borders
[01:15:40.480 --> 01:15:45.840]   of Israel. So you think without the check on power, it's possible that the Netanyahu government
[01:15:45.840 --> 01:15:51.120]   holds on to power? Nobody tries to gain unlimited power just for nothing.
[01:15:51.120 --> 01:15:56.800]   I mean, you have so many problems in Israel, and Netanyahu talks so much about Iran,
[01:15:56.800 --> 01:16:02.480]   and the Palestinians, and Hezbollah. We have an economic crisis. Why is it so urgent at this
[01:16:02.480 --> 01:16:09.840]   moment, in the face of such opposition, why is it so crucial for them to neutralize the Supreme
[01:16:09.840 --> 01:16:16.560]   Court? They are just doing it for the fun of it? No. They know what they are doing. They are adamant.
[01:16:16.560 --> 01:16:22.800]   We were not sure of it before. There was a couple of months ago, they came out with this plan to
[01:16:22.800 --> 01:16:26.880]   take over the Supreme Court, to have all these laws, and there were hundreds of thousands of people
[01:16:26.880 --> 01:16:33.120]   in the streets, again, soldiers saying they will stop serving, a general strike in the economy,
[01:16:33.120 --> 01:16:39.760]   and they stopped. And they started a process of negotiations to try and enrich a settlement.
[01:16:39.760 --> 01:16:47.360]   And then they broke down, they stopped the negotiations, and they restarted this process
[01:16:47.360 --> 01:16:55.760]   of legislation, trying to gain unlimited power. So any doubt we had before, "Okay, maybe they
[01:16:55.760 --> 01:17:04.720]   changed their purposes." No. It's now very clear they are 100% focused on gaining absolute power.
[01:17:04.720 --> 01:17:11.040]   They are now trying a different tactic. Previously, they had all these dozens of laws that
[01:17:11.040 --> 01:17:17.280]   they wanted to pass very quickly within a month or two. They realized, "No, there is too much
[01:17:17.280 --> 01:17:23.200]   opposition." So now they are doing what is known as salami tactics, slice by slice. Now they are
[01:17:23.200 --> 01:17:28.400]   trying to one law. If this succeeds, then they'll pass the next one, and the next one, and the next
[01:17:28.400 --> 01:17:33.360]   one. This is why we are now at a very crucial moment. And when you see, again, hundreds of
[01:17:33.360 --> 01:17:39.360]   thousands of people in the streets almost every day, when you see resistance within the armed
[01:17:39.360 --> 01:17:44.320]   forces, within the security forces, you see high-tech companies saying, "We will go on strike.
[01:17:44.320 --> 01:17:51.040]   You know, they are private businesses." High-tech companies, I think it's almost unprecedented for
[01:17:51.040 --> 01:18:00.080]   a private business to go on strike, because what will economic success benefit us if we live under
[01:18:00.080 --> 01:18:06.960]   a messianic dictatorship? And again, the fuel for this whole thing is to a large extent coming from
[01:18:06.960 --> 01:18:15.840]   messianic religious groups, which just the thought, what happens if these people have unlimited
[01:18:15.840 --> 01:18:23.120]   control of Israel's nuclear arsenal and Israel's military capabilities and cyber capabilities?
[01:18:23.120 --> 01:18:29.440]   This is very, very scary, not just for the citizens of Israel. It should be scary for people
[01:18:29.440 --> 01:18:38.560]   everywhere. - So it would be scary for it to go from being a problem of security and protecting
[01:18:38.560 --> 01:18:44.800]   the peace to becoming a religious war? - It is already becoming a religious war. I mean, the war,
[01:18:44.800 --> 01:18:50.240]   the conflict with the Palestinians was for many years a national conflict in essence.
[01:18:50.240 --> 01:18:59.040]   Over the last few years, maybe a decade or two, it is morphing into a religious conflict,
[01:18:59.040 --> 01:19:04.160]   which is again a very worrying development. When nations are in conflict, you can reach
[01:19:04.160 --> 01:19:09.280]   some compromise. Okay, you have this bit of land, we have this bit of land. But when it becomes a
[01:19:09.280 --> 01:19:16.000]   religious conflict between fundamentalists, between messianic people, compromise becomes
[01:19:16.000 --> 01:19:22.240]   much more difficult because you don't compromise on eternity. You don't compromise on God.
[01:19:22.240 --> 01:19:28.320]   And this is where we are heading right now. - So I know you said it's a small nation,
[01:19:29.040 --> 01:19:35.040]   somewhere in the Middle East, but it also happens to be the epicenter of one of the longest running,
[01:19:35.040 --> 01:19:41.440]   one of the most tense conflicts and crises in human history. So at the very least, it serves
[01:19:41.440 --> 01:19:47.920]   as a study of how conflict can be resolved. So what are the biggest obstacles to you,
[01:19:47.920 --> 01:19:55.520]   to achieving peace in this part of the world? - Motivation. I think it's easy to achieve peace
[01:19:55.520 --> 01:20:01.200]   if you have the motivation on both sides. Unfortunately, at the present juncture,
[01:20:01.200 --> 01:20:07.120]   there is not enough motivation on either side, either the Palestinian or Israeli side. Peace,
[01:20:07.120 --> 01:20:14.320]   you know, in mathematics, you have problems without solutions. You can prove mathematically
[01:20:14.320 --> 01:20:21.760]   that this mathematical problem has no solution. In politics, there is no such thing. All problems
[01:20:21.760 --> 01:20:29.600]   have solutions if you have the motivation. But motivation is the big problem. And again,
[01:20:29.600 --> 01:20:37.200]   we can go into the reasons why, but the fact is that on neither side is there enough motivation.
[01:20:37.200 --> 01:20:42.720]   If there was motivation, the solution would have been easy. - Is there an important distinction
[01:20:42.720 --> 01:20:50.000]   to draw between the people on the street and the leaders in power in terms of motivation?
[01:20:50.560 --> 01:21:00.320]   So are most people motivated and hoping for peace, and the leaders are motivated and incentivized to
[01:21:00.320 --> 01:21:05.040]   continue war? - I don't think so. - Or the people also? - I think it's a deep problem. It's also
[01:21:05.040 --> 01:21:12.160]   the people. It's not just the leaders. - Is it even a human problem of literally hate in people's
[01:21:12.160 --> 01:21:17.040]   heart? - Yeah, there is a lot of hate. One of the things that happened in Israel over the last
[01:21:17.760 --> 01:21:24.960]   10 years or so, Israel became much stronger than it was before, largely thanks to technological
[01:21:24.960 --> 01:21:33.120]   developments. And it feels that it no longer needs to compromise. There are many reasons for it,
[01:21:33.120 --> 01:21:43.920]   but some of them are technological. Being one of the leading powers in cyber, in AI, in high tech,
[01:21:45.040 --> 01:21:52.640]   we have developed very sophisticated ways to more easily control the Palestinian population.
[01:21:52.640 --> 01:21:59.760]   In the early 2000s, it seemed that it is becoming impossible to control millions of people against
[01:21:59.760 --> 01:22:07.920]   their will. It took too much power. It spilled too much blood on both sides. So there was an
[01:22:07.920 --> 01:22:13.280]   impression, "Oh, this is becoming untenable." And there are several reasons why it changed,
[01:22:13.280 --> 01:22:19.040]   but one of them was new technology. Israel developed very sophisticated surveillance
[01:22:19.040 --> 01:22:26.240]   technology that has made it much easier for Israeli security forces to control 2.5 million
[01:22:26.240 --> 01:22:35.840]   Palestinians in the West Bank against their will, with a lot less effort, less boots on the ground,
[01:22:35.840 --> 01:22:44.000]   also less blood. And Israel is also now exporting this technology to many other regimes around the
[01:22:44.000 --> 01:22:49.520]   world. Again, I heard Netanyahu speaking about all the wonderful things that Israel is exporting to
[01:22:49.520 --> 01:22:55.440]   the world, and it's true. We are exporting some nice things, water systems and new kinds of
[01:22:55.440 --> 01:23:05.200]   tomatoes. We are also exporting a lot of weapons and especially surveillance systems, sometimes to
[01:23:05.200 --> 01:23:14.080]   unsavory regimes in order to control their populations. - Can you comment on, I think
[01:23:14.080 --> 01:23:21.440]   you've mentioned that the current state of affairs is a de facto three-class state. Can you describe
[01:23:21.440 --> 01:23:26.400]   what you mean by that? - Yes, for many years, the kind of leading solution to the Israeli-Palestinian
[01:23:26.400 --> 01:23:30.240]   conflict is the two-state solution. - Can you describe what that means, by the way? - Yes,
[01:23:30.240 --> 01:23:37.600]   two states between the Jordan River and the Mediterranean will have two states, Israel as a
[01:23:37.600 --> 01:23:43.600]   predominantly Jewish state and Palestine as a predominantly Palestinian state. Again, there
[01:23:43.600 --> 01:23:48.080]   were lots of discussions where the border passes, what happens with security arrangement and whatever,
[01:23:48.080 --> 01:23:54.560]   but this was the big solution. Israel has basically abandoned the two-state solution. Maybe they don't
[01:23:54.560 --> 01:23:59.840]   say so officially, the people in power, but in terms of what they do on the ground, they abandoned
[01:23:59.840 --> 01:24:07.440]   it. Now they are effectively promoting the three-class solution, which means there is just
[01:24:07.440 --> 01:24:14.320]   one country and one government and one power between the Mediterranean and the Jordan River,
[01:24:14.320 --> 01:24:22.640]   but you have three classes of people living there. You have Jews who enjoy full rights, all the
[01:24:22.640 --> 01:24:29.360]   rights. You have some Arabs who are Israeli citizens and have some rights. And then you have
[01:24:29.360 --> 01:24:34.400]   the other Arabs, the third class, who have basically no civil rights and limited human rights.
[01:24:34.400 --> 01:24:41.840]   And nobody would openly speak about it, but effectively this is the reality on the ground
[01:24:41.840 --> 01:24:46.080]   already. - So there's many, and I'll speak with them, Palestinians who characterize this as a
[01:24:46.080 --> 01:24:53.440]   de facto one-state apartheid. Do you agree with this? - I would take issue with the term apartheid.
[01:24:53.440 --> 01:24:58.160]   Generally speaking, as a historian, I don't really like historical analogies because there are always
[01:24:58.160 --> 01:25:03.840]   differences, key differences. The biggest difference between the situation here and the
[01:25:03.840 --> 01:25:11.920]   situation in South Africa in the time of the apartheid is that Black South Africans did not
[01:25:11.920 --> 01:25:18.400]   deny the existence of South Africa and did not call for the destruction of South Africa. They
[01:25:18.400 --> 01:25:26.000]   had a very simple goal. They had a very simple demand. We want to be equal citizens of this
[01:25:26.000 --> 01:25:34.240]   country. That's it. And the apartheid regime was, no, you can't be equal citizens. Now, in Israel,
[01:25:34.240 --> 01:25:39.520]   Palestine, it's different. The Palestinians, many of them don't recognize the existence of Israel,
[01:25:39.520 --> 01:25:45.520]   are not willing to recognize it, and they don't demand to be citizens of Israel.
[01:25:45.520 --> 01:25:51.840]   They demand, some of them, to destroy it and replace it with a Palestinian state. Some of
[01:25:51.840 --> 01:25:59.280]   them demand a separate state. But if the Palestinians would adopt the same policy
[01:25:59.280 --> 01:26:05.680]   as the Black South Africans, if you have the Palestinians coming and saying, "OK, forget about
[01:26:05.680 --> 01:26:10.160]   it. We don't want to destroy Israel. We don't know a Palestinian country. We have a very simple
[01:26:10.160 --> 01:26:17.440]   request, very simple demand. Give us our full rights. We also want to vote to the Knesset.
[01:26:17.440 --> 01:26:22.640]   We also want to get the full protection of the law." That's it. That's our only demand. Israel
[01:26:22.640 --> 01:26:29.840]   will be in deep, deep trouble at that moment. But we are not there. I wonder if there will ever be
[01:26:29.840 --> 01:26:36.640]   a future when such a thing happens, where everybody, the majority of people, Arab and Jew,
[01:26:36.640 --> 01:26:42.720]   Israeli and Palestinian, accept the one-state solution and say, "We want equal rights."
[01:26:44.800 --> 01:26:49.280]   Never say never in history. It's not coming anytime soon from either side.
[01:26:49.280 --> 01:26:56.880]   When you look at the long term of history, one of the curious things you see, and that's what
[01:26:56.880 --> 01:27:01.840]   makes us different human groups from animal species. You know, gorillas and chimpanzees,
[01:27:01.840 --> 01:27:07.440]   they are separate species. They can never merge. Cats and dogs will never merge. But
[01:27:07.440 --> 01:27:12.960]   different national and religious groups in history, even when they hate each other,
[01:27:12.960 --> 01:27:20.080]   surprisingly, they sometimes end by merging. If you look at Germany, for instance, so for centuries
[01:27:20.080 --> 01:27:26.560]   you had Prussians and Bavarians and Saxons who fought each other ferociously and hated each
[01:27:26.560 --> 01:27:31.440]   other. And they are sometimes also of different religions, Catholics, Protestants. You know, the
[01:27:31.440 --> 01:27:37.600]   worst war in European history, according to some measures, was not the Second World War or the First
[01:27:37.600 --> 01:27:44.400]   World War. It was the Thirty Years' War, waged largely on German soil between Germans, Protestants
[01:27:44.400 --> 01:27:51.040]   and Catholics. But eventually they united to form a single country. You saw the same thing, I don't
[01:27:51.040 --> 01:27:57.600]   know, in Britain. English and Scots for centuries hated and fought each other ferociously, eventually
[01:27:57.600 --> 01:28:06.400]   coming together. Maybe it'll break up again, I don't know. But the power of the kind of forces
[01:28:06.400 --> 01:28:14.400]   of merger in history, you are very often influenced by the people you fight, by the people you even
[01:28:14.400 --> 01:28:23.280]   hate, more than by almost anybody else. So if we apply those ideas, the ideas of this part of the
[01:28:23.280 --> 01:28:30.160]   world, to another part of the world that's currently in war, Russia and Ukraine, from what you learned
[01:28:30.160 --> 01:28:36.880]   here, how do you think peace can be achieved in Ukraine? Peace can be achieved any moment. It's
[01:28:36.880 --> 01:28:42.240]   motivation. In this case, it's just one person. Putin just needs to say, "That's it." You know,
[01:28:42.240 --> 01:28:47.440]   the Ukrainians, they don't demand anything from Russia, just go home. That's the only thing they
[01:28:47.440 --> 01:28:52.400]   want. They don't want to conquer any bit of Russian territory. They don't want to change the regime in
[01:28:52.400 --> 01:28:59.120]   Moscow, nothing. They just tell the Russians, "Go home." That's it. And of course, again,
[01:28:59.120 --> 01:29:06.800]   motivation. How do you get somebody like Putin to admit that he made a colossal mistake, a human
[01:29:06.800 --> 01:29:13.600]   mistake, an ethical mistake, a political mistake, in starting this war? This is very, very difficult.
[01:29:13.600 --> 01:29:20.400]   But in terms of what would the solution look like? Very simple. The Russians go home. End of story.
[01:29:20.400 --> 01:29:28.720]   - Do you believe in the power of conversation between leaders to sit down as human beings
[01:29:28.720 --> 01:29:36.480]   and agree? First of all, what home means, because we humans draw lines.
[01:29:36.480 --> 01:29:41.920]   - That's true. I believe in the power of conversation. The big question to ask is where?
[01:29:41.920 --> 01:29:49.280]   Where do conversations, real conversations take place? And this is tricky. One of the interesting
[01:29:49.280 --> 01:29:55.520]   things to ask about any conflict, about any political system, is where do the real conversations
[01:29:55.520 --> 01:30:02.000]   take place? And very often, they don't take place in the places you think that they are.
[01:30:02.000 --> 01:30:08.400]   But think about American politics. When the country was founded in the late 18th century,
[01:30:08.400 --> 01:30:14.160]   people understood holding conversation between leaders is very important for the functioning
[01:30:14.160 --> 01:30:19.600]   of democracy. We'll create a place for that. That's called Congress. This is where leaders
[01:30:19.600 --> 01:30:25.280]   are supposed to meet and talk about the main issues of the day. Maybe there was a time,
[01:30:25.280 --> 01:30:35.600]   sometime in the past, when this actually happened. When you had two factions holding different ideas
[01:30:35.600 --> 01:30:41.120]   about foreign policy or economic policy, and they met in Congress, and somebody would come and give
[01:30:41.120 --> 01:30:45.920]   a speech, and the people on the other side would say, "Hey, that's interesting. I haven't thought
[01:30:45.920 --> 01:30:52.480]   about it. Yes, maybe we can agree on that." This is no longer happening in Congress. I don't think
[01:30:52.480 --> 01:30:58.640]   there is any speech in Congress that causes anybody on the other side to change their opinion about
[01:30:58.640 --> 01:31:06.240]   anything. So this is no longer a place where real conversations take place. The big question about
[01:31:06.240 --> 01:31:14.080]   American democracy is, is there a place where real conversations, which actually change people's
[01:31:14.080 --> 01:31:21.360]   minds, still take place? If not, then this democracy is dying also. Democracy without
[01:31:21.360 --> 01:31:26.240]   conversation cannot exist for long. And it's the same question you should ask also about
[01:31:26.240 --> 01:31:32.000]   dictatorial regimes. Like you think about Russia or China. So China has the Great Hall of the People.
[01:31:33.120 --> 01:31:37.600]   Well, the representatives, the supposed representatives of the people meet every now
[01:31:37.600 --> 01:31:43.920]   and then, but no real conversation takes place there. A key question to ask about the Chinese
[01:31:43.920 --> 01:31:51.600]   system is, behind closed doors, let's say in a Politburo meeting, do people have a real conversation?
[01:31:51.600 --> 01:31:59.200]   If Xi Jinping says one thing, and some other big shot thinks differently, will they have the
[01:31:59.200 --> 01:32:04.720]   courage, the ability, the backbone to say, "With all due respect, I think differently,"
[01:32:04.720 --> 01:32:10.240]   and there is a real conversation? Or not? I don't know the answer. But this is a key question.
[01:32:10.240 --> 01:32:17.680]   This is the difference between an authoritarian regime can still have different voices within it.
[01:32:17.680 --> 01:32:26.080]   But at a certain point, you have a personality cult. Nobody dares say anything against the leader.
[01:32:27.040 --> 01:32:34.240]   And when it comes again to Ukraine and Russia, I don't think that if you somehow manage to get
[01:32:34.240 --> 01:32:39.280]   Putin and Zelensky to the same room, when everybody knows that they are there, and they'll
[01:32:39.280 --> 01:32:48.000]   have a moment of empathy or human connection, I don't think it can happen like that. I do hope
[01:32:48.640 --> 01:32:58.480]   that there are other spaces where somebody like Putin can still have a real human conversation.
[01:32:58.480 --> 01:33:02.400]   I don't know if this is the case. I hope so. Well, there's several interesting dynamics,
[01:33:02.400 --> 01:33:07.440]   and you spoke to some of them. So one is internally with advisors. You have to have
[01:33:07.440 --> 01:33:12.960]   hope that there's people that would disagree, that would have a lively debate internally.
[01:33:12.960 --> 01:33:18.560]   Then there's also the thing you mentioned, which is direct communication between Putin and Zelensky
[01:33:18.560 --> 01:33:26.400]   in private, picking up a phone, rotary phone, old school. I still believe in the power of that.
[01:33:26.400 --> 01:33:32.400]   But while that's exceptionally difficult in the current state of affairs, what's also possible
[01:33:32.400 --> 01:33:39.120]   to have is a mediator like the United States or some other leader, like the leader of Israel or
[01:33:39.120 --> 01:33:46.160]   the leader of another nation that's respected by both, or India, for example, that can have,
[01:33:46.160 --> 01:33:49.760]   first of all, individual conversations and then literally get into a room together.
[01:33:49.760 --> 01:33:58.560]   It is possible. I would say more generally about conversations, as it goes back a little to what
[01:33:58.560 --> 01:34:06.240]   I said earlier about the Marxist view of history. One of the problematic things I see today in many
[01:34:06.240 --> 01:34:13.760]   academic circles is that people focus too much on power. They think that the whole of history
[01:34:13.760 --> 01:34:20.240]   or the whole of politics is just a power structure. It's just struggle about power.
[01:34:20.240 --> 01:34:26.400]   Now, if you think that the whole of history and the whole of politics is only power,
[01:34:26.400 --> 01:34:34.560]   then there is no room for conversation. Because if what you have is a struggle between different
[01:34:34.560 --> 01:34:41.360]   powerful interests, there is no point talking. The only thing that changes it is fighting.
[01:34:43.120 --> 01:34:48.960]   My view is that no, it's not all about power structures. It's not all about power dynamics.
[01:34:48.960 --> 01:34:54.640]   Underneath the power structure, there are stories, stories in human minds.
[01:34:54.640 --> 01:35:02.800]   This is great news, if it's true. This is good news, because unlike power that can only be
[01:35:02.800 --> 01:35:10.080]   changed through fighting, stories can sometimes, it's not easy, but sometimes stories can be
[01:35:10.080 --> 01:35:17.200]   changed through talking. That's the hope. I think in everything from couple therapy
[01:35:17.200 --> 01:35:25.280]   to nation therapy, if you think it's power therapy, it's all about power, there is no place
[01:35:25.280 --> 01:35:32.880]   for a conversation. But if to some extent, it's the stories in people's minds, if you can enable
[01:35:32.880 --> 01:35:39.840]   one person to see the story in the mind of another person, and more importantly, if you can have
[01:35:39.840 --> 01:35:46.480]   some kind of critical distance from the story in your own mind, then maybe you can change it a
[01:35:46.480 --> 01:35:52.480]   little. Then you don't need to fight. You can actually find a better story that you can both
[01:35:52.480 --> 01:35:58.480]   agree to. It sometimes happens in history. Again, French and Germans fought for generations and
[01:35:58.480 --> 01:36:05.200]   generations. Now they live in peace, not because, I don't know, they found a new planet they can
[01:36:05.200 --> 01:36:10.160]   share between France and Germany, so now everybody has enough territory. No, they actually have less
[01:36:10.160 --> 01:36:17.200]   territory than previously, because they lost all their overseas empires. But they managed to find
[01:36:17.200 --> 01:36:23.760]   a story, the European story, that both Germans and French people are happy with. So they live in
[01:36:23.760 --> 01:36:30.800]   peace. - I very much believe in this vision that you have of the power of stories. One of the tools
[01:36:30.800 --> 01:36:37.760]   is conversations. Another is books. There's some guy that wrote a book about this power of stories.
[01:36:37.760 --> 01:36:42.000]   He happens to be sitting in front of me, and that happened to spread across a lot of people. Now
[01:36:42.000 --> 01:36:47.440]   they believe in the power of story and narrative, even a children's book, too, so the kids...
[01:36:47.440 --> 01:36:56.080]   I mean, it's fascinating how that spreads. I mean, underneath your work, there's an optimism.
[01:36:57.920 --> 01:37:04.560]   I think underneath conversations, what I try to do is an optimism, that it's not just about power
[01:37:04.560 --> 01:37:11.760]   struggles. It's about stories, which is like a connection between humans and together, kind of
[01:37:11.760 --> 01:37:19.760]   evolving these stories that maximize happiness or minimize suffering in the world. - And this is why
[01:37:19.760 --> 01:37:24.800]   I also, I think I admire what you're doing, that you're going to talk with some of the
[01:37:24.800 --> 01:37:34.640]   most difficult characters around in the world today. And with this basic belief that by talking,
[01:37:34.640 --> 01:37:41.120]   maybe we can move them an inch, which is a lot when it comes to people with so much power.
[01:37:41.120 --> 01:37:46.960]   I think one of the biggest success stories in modern history, I would say, is feminism.
[01:37:48.240 --> 01:37:56.960]   Because feminism believed in the power of stories, not so much in the power of violence,
[01:37:56.960 --> 01:38:03.920]   of armed conflict. By many measures, feminism has been maybe the most successful social movement
[01:38:03.920 --> 01:38:10.240]   of the 20th century and maybe of the modern age. You know, the systems of oppression,
[01:38:10.240 --> 01:38:15.840]   which were in place throughout the world for thousands of years, and they seem to be just
[01:38:15.840 --> 01:38:20.880]   natural, eternal. You had all these religious movements, all these political revolutions,
[01:38:20.880 --> 01:38:26.160]   and one thing remained constant, and this is the patriarchal system and the oppression of women.
[01:38:26.160 --> 01:38:33.680]   And then feminism came along. And, you know, you had leaders like Lenin, like Mao, saying that if
[01:38:33.680 --> 01:38:41.520]   you want to make a big social change, you must use violence. Power comes from the barrel of a gun.
[01:38:41.520 --> 01:38:46.960]   If you want to make an omelet, you need to break eggs, and all these things. And the feminists said,
[01:38:46.960 --> 01:38:53.440]   "No, we won't use the power of the gun. We will make an omelet without breaking any eggs."
[01:38:53.440 --> 01:39:01.120]   And they made a much better omelet than Lenin or Mao or any of these violent revolutionaries.
[01:39:01.120 --> 01:39:06.320]   I don't think, you know, that they certainly didn't start any wars or build any gulags. I
[01:39:06.320 --> 01:39:12.080]   don't think they even murdered a single politician. I don't think there was any political assassination
[01:39:12.080 --> 01:39:20.240]   anywhere by feminists. There was a lot of violence against them, both verbal but also physical,
[01:39:20.240 --> 01:39:29.280]   and they didn't reply by waging violence, and they succeeded in changing this deep
[01:39:32.560 --> 01:39:38.080]   structure of oppression in a way which benefited not just women but also men.
[01:39:38.080 --> 01:39:46.560]   So this gives me hope that it's not easy. In many cases, we fail. But it is possible
[01:39:46.560 --> 01:39:52.240]   sometimes in history to make a very, very big change, positive change,
[01:39:52.240 --> 01:40:00.240]   mainly by talking and demonstrating and changing the story in people's minds and not by using
[01:40:00.240 --> 01:40:05.600]   violence. - It's fascinating that feminism and communism and all these things happen in the
[01:40:05.600 --> 01:40:11.200]   20th century. So many interesting things happen in the 20th century. So many movements, so many ideas,
[01:40:11.200 --> 01:40:16.400]   nuclear weapons, all of it, computers. It's just like, it seems like a lot of stuff, like,
[01:40:16.400 --> 01:40:20.960]   really quickly percolated and it's accelerating. - It's still accelerating. I mean, history is just
[01:40:20.960 --> 01:40:26.640]   accelerating, you know, for centuries. And the 20th century, you know, we squeezed into it
[01:40:26.640 --> 01:40:31.840]   things that previously took thousands of years, and now, I mean, we are squeezing it into decades.
[01:40:31.840 --> 01:40:37.280]   - And you very well could be one of the last historians, human historians, to have ever lived.
[01:40:37.280 --> 01:40:44.080]   - Could be. I think, you know, our species, Homo sapiens, I don't think we'll be around in a
[01:40:44.080 --> 01:40:50.480]   century or two. We could destroy ourselves in a nuclear war, through ecological collapse,
[01:40:51.440 --> 01:40:58.720]   by giving too much power to AI that goes out of our control. But if we survive, we'll probably
[01:40:58.720 --> 01:41:07.520]   have so much power that we will change ourselves using various technologies so that our descendants
[01:41:07.520 --> 01:41:14.880]   will no longer be Homo sapiens like us. They will be more different from us than we are different
[01:41:14.880 --> 01:41:22.080]   from Neanderthals. So maybe they'll have historians, but it will no longer be human historians or
[01:41:22.080 --> 01:41:28.960]   sapiens historians like me. I think it's an extremely dangerous development, and the chances
[01:41:28.960 --> 01:41:35.440]   that this will go wrong, that people will use the new technologies, trying to upgrade humans,
[01:41:35.440 --> 01:41:42.400]   but actually downgrading them, this is a very, very big danger. If you let corporations and
[01:41:42.400 --> 01:41:50.000]   armies and ruthless politicians change humans using tools like AI and bioengineering,
[01:41:50.000 --> 01:41:56.800]   it's very likely that they will try to enhance a few human qualities that they need,
[01:41:56.800 --> 01:42:05.040]   like intelligence and discipline, while neglecting what are potentially more important
[01:42:05.760 --> 01:42:12.400]   human qualities, like compassion, like autistic sensitivity, like spirituality.
[01:42:12.400 --> 01:42:19.840]   If you give Putin, for instance, bioengineering and AI and brain-computer interfaces,
[01:42:19.840 --> 01:42:30.240]   he's likely to want to create a race of super soldiers who are much more intelligent and much
[01:42:30.240 --> 01:42:36.080]   stronger and also much more disciplined and never rebel and march on Moscow against him.
[01:42:36.080 --> 01:42:43.280]   But he has no interest in making them more compassionate or more spiritual. So the end
[01:42:43.280 --> 01:42:53.440]   result could be a new type of humans, downgraded humans, who are highly intelligent and disciplined
[01:42:54.080 --> 01:43:03.200]   but have no compassion and no spiritual depth. For me, this is the dystopia, the apocalypse,
[01:43:03.200 --> 01:43:10.080]   that when people talk about the new technologies and they have this scenario of the Terminator,
[01:43:10.080 --> 01:43:15.520]   robots running in the street shooting people, this is not what worries me. I think we can avoid that.
[01:43:15.520 --> 01:43:23.520]   What really worries me is using the corporations, armies, politicians will use the new technologies
[01:43:24.240 --> 01:43:31.200]   to change us in a way which will destroy our humanity or the best parts of our humanity.
[01:43:31.200 --> 01:43:35.120]   And one of those ways could be removing the compassion. Another way that really worries me,
[01:43:35.120 --> 01:43:39.840]   for me, is probably more likely is a brave new world kind of thing that
[01:43:39.840 --> 01:43:49.760]   removes the flaws of humans, maybe removes the diversity in humans, and makes us all these
[01:43:49.760 --> 01:43:55.840]   dopamine-chasing creatures that just kind of maximize enjoyment in the short term,
[01:43:55.840 --> 01:44:03.600]   which kind of seems like a good thing maybe in the short term, but it creates a society that
[01:44:03.600 --> 01:44:12.880]   doesn't think, that doesn't create, that just is sitting there enjoying itself at a more and more
[01:44:12.880 --> 01:44:18.400]   rapid pace, which seems like another kind of society that could be easily controlled by a
[01:44:18.400 --> 01:44:23.680]   centralized center of power. But the set of dystopias that we could arrive at through this,
[01:44:23.680 --> 01:44:32.080]   through allowing corporations to modify humans is vast, and we should be worried about that.
[01:44:32.080 --> 01:44:40.080]   So it seems like humans are pretty good as we are, all the flaws, all of it together.
[01:44:40.080 --> 01:44:44.320]   We are better than anything that we can intentionally design at present.
[01:44:44.960 --> 01:44:50.560]   Like any intentionally designed humans at the present moment is going to be much,
[01:44:50.560 --> 01:44:53.760]   much worse than us, because basically we don't understand ourselves.
[01:44:53.760 --> 01:44:59.280]   I mean, as long as we don't understand our brain, our body, our mind, it's a very,
[01:44:59.280 --> 01:45:05.040]   very bad idea to start manipulating a system that you don't understand deeply,
[01:45:05.040 --> 01:45:09.360]   and we don't understand ourselves. - So I have to ask you about an
[01:45:09.360 --> 01:45:13.760]   interesting dynamic of stories. You wrote an article two years ago titled
[01:45:13.760 --> 01:45:18.800]   "When the World Seems Like One Big Conspiracy," how understanding the structure of global
[01:45:18.800 --> 01:45:24.000]   cabal theories can shed light on their allure and their inherent falsehood.
[01:45:24.000 --> 01:45:29.280]   What are global cabal theories, and why do so many people believe them?
[01:45:29.280 --> 01:45:34.240]   37% of Americans, for example. - Well, the global cabal theory,
[01:45:34.240 --> 01:45:38.400]   it has many variations, but basically there is a small group of people, a small cabal
[01:45:38.400 --> 01:45:42.720]   that secretly controls everything that is happening in the world.
[01:45:42.720 --> 01:45:47.440]   All the wars, all the revolutions, all the epidemics, everything that is happening
[01:45:47.440 --> 01:45:52.480]   is controlled by this very small group of people who are, of course, evil and have bad intentions.
[01:45:52.480 --> 01:46:00.800]   And this is a very well-known story. It's not new. It's been there for thousands of years.
[01:46:00.800 --> 01:46:04.640]   It's very attractive, because first of all, it's simple.
[01:46:04.640 --> 01:46:10.320]   You don't have to understand everything that happens in the world. You just need to understand
[01:46:10.320 --> 01:46:16.800]   one thing. The war in Ukraine, the Israeli-Palestinian conflict, 5G technology, COVID-19,
[01:46:16.800 --> 01:46:24.960]   it's simple. There is this global cabal. They do all of it. And also, it enables you to shift
[01:46:24.960 --> 01:46:30.480]   all the responsibility to all the bad things that are happening in the world to this small cabal.
[01:46:30.480 --> 01:46:37.040]   It's the Jews. It's the Freemasons. It's not us. And also, it creates this fantasy,
[01:46:37.680 --> 01:46:44.160]   utopian fantasy. If we only get rid of the small cabal, we solve all the problems of the world.
[01:46:44.160 --> 01:46:49.200]   Salvation. The Israeli-Palestinian conflict, the war in Ukraine, the epidemics, poverty,
[01:46:49.200 --> 01:46:55.040]   everything is solved just by knocking out this small cabal. So, again, it's simple,
[01:46:55.040 --> 01:46:58.080]   it's attractive, and this is why so many people believe it.
[01:46:58.080 --> 01:47:05.760]   It's, again, it's not new. Nazism was exactly this. Nazism began as a conspiracy theory.
[01:47:05.760 --> 01:47:10.560]   We don't call Nazism a conspiracy theory because, oh, it's a big thing, it's an ideology.
[01:47:10.560 --> 01:47:17.040]   But if you look at it, it's a conspiracy theory. The basic Nazi idea was the Jews control the world,
[01:47:17.040 --> 01:47:22.480]   get rid of the Jews, you solved all the world's problems. Now, the interesting thing about these
[01:47:22.480 --> 01:47:30.960]   kind of theories, again, they tell you that even things that look to be the opposite of each other,
[01:47:31.520 --> 01:47:36.880]   actually they are part of the conspiracy. So, in the case of Nazism, the Nazis told people,
[01:47:36.880 --> 01:47:42.640]   you know, you have capitalism and communism, you think that they are opposite, right? Ah,
[01:47:42.640 --> 01:47:48.960]   this is what the Jews want you to think. Actually, the Jews control both communism, Trotsky, Marx,
[01:47:48.960 --> 01:47:54.160]   were Jews, blah, blah, blah, and capitalism, the Rothschilds, Wall Street, it's all controlled
[01:47:54.160 --> 01:47:59.520]   by the Jews. So, the Jews are fooling everybody. But actually, the communists and the capitalists
[01:47:59.520 --> 01:48:06.240]   are part of the same global cabal. And again, this is very attractive, because, ah, now I
[01:48:06.240 --> 01:48:12.240]   understand everything, and I also know what to do. I just give power to Hitler, he gets rid of the
[01:48:12.240 --> 01:48:18.800]   Jews, I solved all the problems of the world. Now, as a historian, the most important thing I can say
[01:48:18.800 --> 01:48:25.840]   about these theories, they are never right. Because the global cabal theory says two things. First,
[01:48:26.400 --> 01:48:31.520]   everything is controlled by a very small number of people. Secondly, these people hide themselves,
[01:48:31.520 --> 01:48:37.840]   they do it in secret. Now, both things are nonsense. It's impossible for people to control,
[01:48:37.840 --> 01:48:44.320]   a small group of people, to control and predict everything, because the world is too complicated.
[01:48:44.320 --> 01:48:49.600]   You know, you look at a real world conspiracy, conspiracy is basically just a plan. Think about
[01:48:49.600 --> 01:48:57.440]   the American invasion of Iraq in 2003. You had the most powerful superpower in the world,
[01:48:57.440 --> 01:49:04.160]   with the biggest military, with the biggest intelligence services, with the most sophisticated,
[01:49:04.160 --> 01:49:11.520]   you know, the FBI and the CIA and all the agents. They invade a third-rate country,
[01:49:11.520 --> 01:49:17.280]   third-rate power, Iraq. With this idea, we'll take over Iraq and we'll control it, we'll make
[01:49:17.280 --> 01:49:23.600]   a new order in the Middle East. And everything falls apart. Their plan completely backfires.
[01:49:23.600 --> 01:49:30.400]   Everything they hoped to achieve, they achieved the opposite. America, United States is humiliated.
[01:49:30.400 --> 01:49:35.840]   They caused the rise of ISIS. They wanted to take out terrorism, they created more terrorism.
[01:49:35.840 --> 01:49:42.160]   Worst of all, the big winner of the war was Iran. You know, the United States goes to war
[01:49:42.160 --> 01:49:49.600]   with all its power and gives Iran a victory on a silver plate. The Iranians don't need to do
[01:49:49.600 --> 01:49:56.000]   anything. The Americans are doing everything for them. Now, this is real history. Real history
[01:49:56.000 --> 01:50:01.920]   is when you have not a small group of people, a lot of people with a lot of power, carefully
[01:50:01.920 --> 01:50:08.960]   planning something, and it goes completely out of, against their plan. And this we know from
[01:50:08.960 --> 01:50:14.720]   a personal experience. Like every time we try to plan something, a birthday party, a surprise
[01:50:14.720 --> 01:50:22.560]   birthday party, a trip somewhere, things go wrong. This is reality. So the idea that a small group of,
[01:50:22.560 --> 01:50:29.440]   I don't know, the Jewish Kabbalah, the Freemasons, whoever, they can really control and predict all
[01:50:29.440 --> 01:50:35.600]   the wars, this is nonsense. The second thing that is nonsense is to think they can do that and still
[01:50:35.600 --> 01:50:42.480]   remain secret. It sometimes happens in history that a small group of people accumulates a lot
[01:50:42.480 --> 01:50:50.320]   of power. If I now tell you that Xi Jinping and the heads of the CCP, the Chinese Communist Party,
[01:50:50.320 --> 01:50:56.080]   they have a lot of power. They control the military, the media, the economy, the University
[01:50:56.080 --> 01:51:02.320]   of China. This is not a conspiracy theory. This is, obviously, everybody knows it. Everybody knows
[01:51:02.320 --> 01:51:10.800]   it. Because to gain so much power, you usually need publicity. Hitler could not, Hitler gained a
[01:51:10.800 --> 01:51:16.960]   lot of power in Nazi Germany because he had a lot of publicity. If Hitler remained unknown, working
[01:51:16.960 --> 01:51:24.160]   behind the scenes, he would not gain power. So the way to gain power is usually through publicity.
[01:51:24.160 --> 01:51:32.400]   So secret Kabbalahs don't gain power. And even if you gain a lot of power, nobody has the kind
[01:51:32.400 --> 01:51:39.600]   of power necessary to predict and control everything that happens in the world. All the
[01:51:39.600 --> 01:51:44.320]   time shit happens that you did not predict and you did not plan and you did not control.
[01:51:44.320 --> 01:51:50.720]   The sad thing is there's usually an explanation for everything you just said that involves
[01:51:52.320 --> 01:51:57.760]   a secret global Kabbalah. That the reason your vacation planning always goes wrong
[01:51:57.760 --> 01:52:01.440]   is because you're not competent. There is a competent small group,
[01:52:01.440 --> 01:52:07.920]   that ultra-competent small group. I hear this with intelligence agencies. The CIA are running
[01:52:07.920 --> 01:52:12.880]   everything. Mossad is running everything. You see, I mean, as a historian, you get to know
[01:52:12.880 --> 01:52:18.720]   how many blunders these people do. They are so, and they're capable, but they are so incompetent
[01:52:18.720 --> 01:52:23.360]   in so many ways. Again, look at the Russian invasion of Ukraine. Before the war, people
[01:52:23.360 --> 01:52:27.840]   thought, "Oh, Putin was such a genius and the Russian army was one of the strongest
[01:52:27.840 --> 01:52:32.320]   armies in the world." This is what Putin thought. And it completely backfired.
[01:52:32.320 --> 01:52:38.240]   Well, the Kabbalah explanation there would be there's a NATO-driven United States
[01:52:38.240 --> 01:52:42.400]   military-industrial complex that wants to create chaos and incompetence.
[01:52:42.400 --> 01:52:47.840]   So they put a gun to Putin's head and told him, "Vladimir, if you don't invade, we shoot you."
[01:52:47.840 --> 01:52:50.240]   How did they cause Putin to invade Ukraine?
[01:52:50.240 --> 01:52:55.680]   The thing about conspiracy theories is there's usually a way to explain everything.
[01:52:55.680 --> 01:53:02.960]   You can explain religion. You can always find explanation for everything. And in the end,
[01:53:02.960 --> 01:53:08.560]   it's intellectual integrity. If you insist on whenever people confront you with evidence,
[01:53:08.560 --> 01:53:14.000]   with finding some very, very complicated explanation for that too, you can explain
[01:53:14.000 --> 01:53:18.800]   everything. We know that. It's a question of intellectual integrity.
[01:53:18.800 --> 01:53:24.880]   And I will also say another thing. The conspiracy theories, they do get one thing right,
[01:53:24.880 --> 01:53:33.600]   certainly in today's world. I think they represent an authentic and justified fear of a lot of people
[01:53:33.600 --> 01:53:39.840]   that they are losing control of their lives. They don't understand what is happening.
[01:53:39.840 --> 01:53:46.480]   And this, I think, is not just a legitimate fear. This is an important fear. They are right.
[01:53:46.480 --> 01:53:55.120]   We are losing control of our lives. We are facing really big dangers, but not from a small cabal of
[01:53:55.120 --> 01:54:01.360]   fellow humans. The problem with many of these conspiracy theories is that, yes, we have a
[01:54:01.360 --> 01:54:10.080]   problem with new AI technology. But if you now direct the fire against certain people,
[01:54:10.080 --> 01:54:19.840]   so instead of all humans cooperating against real common threats, whether it's the rise of AI,
[01:54:19.840 --> 01:54:24.640]   whether it's global warming, you're only causing us to fight each other.
[01:54:24.640 --> 01:54:29.520]   And I think that the key question that people who spread these ideas, I mean, many of them,
[01:54:29.520 --> 01:54:35.680]   they honestly believe it's not malicious. They honestly believe in these theories.
[01:54:35.680 --> 01:54:44.000]   Is do you want to spend your life spreading hate towards people? Or do you want to work
[01:54:44.000 --> 01:54:48.080]   on more constructive projects? I think one of the big differences between those who believe
[01:54:48.080 --> 01:54:56.480]   in conspiracy theories and people who warn about the dangers of AI, the dangers of climate change,
[01:54:58.320 --> 01:55:08.720]   we don't see certain humans as evil and hateful. The problem isn't humans. The problem is something
[01:55:08.720 --> 01:55:17.200]   outside humanity. Yes, humans are contributing to the problem, but ultimately the enemy is
[01:55:17.200 --> 01:55:24.560]   external to humanity. Whereas conspiracy theories usually claim that a certain part of humanity
[01:55:24.560 --> 01:55:30.960]   is the source of all evil, which leads them to eventually think in terms of exterminating
[01:55:30.960 --> 01:55:40.640]   this part of humanity, which leads sometimes to historical disasters like Nazism.
[01:55:40.640 --> 01:55:46.640]   So it can lead to hate, but can also lead to like cynicism, apathy that basically says,
[01:55:46.640 --> 01:55:51.520]   "It's not in my power to make the world better." So you don't actually take action.
[01:55:51.520 --> 01:55:56.800]   I think it is within the power of every individual to make the world a little bit better.
[01:55:56.800 --> 01:56:02.960]   You know, you can't do everything. Don't try to do everything. Find one thing in your areas
[01:56:02.960 --> 01:56:10.320]   of activity, a place where you have some agency, and try to do that, and hope that other people
[01:56:10.320 --> 01:56:17.440]   do their bit. And if everybody do their bit, we'll manage. And if we don't, we don't,
[01:56:17.440 --> 01:56:24.000]   but at least we try. - You have been part of conspiracy theories. I find myself recently
[01:56:24.000 --> 01:56:32.000]   becoming part of conspiracy theories. Is there advice you can give of how to be a human being
[01:56:32.000 --> 01:56:37.760]   in this world that values truth and reason while watching yourself become part of conspiracy
[01:56:37.760 --> 01:56:44.240]   theories? At least from my perspective, it seems very difficult to prove to the world that you're
[01:56:44.240 --> 01:56:52.320]   not part of a conspiracy theory. I, as you said, have interviewed Benjamin Netanyahu recently. I
[01:56:52.320 --> 01:56:58.480]   don't know if you're aware, but doing such things will also, you know, pick up a new menu of items
[01:56:58.480 --> 01:57:03.680]   that your new set of conspiracy theories you're now a part of. And I find it very frustrating
[01:57:03.680 --> 01:57:11.200]   because it makes it very difficult to respond, because I sense that people have the right
[01:57:11.200 --> 01:57:20.480]   intentions, like we said, they have a nervousness of a fear of power, and the abuses of power, and
[01:57:20.480 --> 01:57:28.400]   as do I. So I find myself in a difficult position that I have nothing to show to prove
[01:57:28.400 --> 01:57:33.760]   that I'm not part of such a conspiracy theory. - I think ultimately you can't, we can't. I mean,
[01:57:35.600 --> 01:57:41.680]   you know, it's like proving consciousness. You can't. That's just the situation. Whatever you
[01:57:41.680 --> 01:57:49.040]   say can and will be used against you by some people. So this fantasy, if I only say this,
[01:57:49.040 --> 01:57:54.400]   if I only show them that, if I only have this data, they will see I'm okay. It doesn't work
[01:57:54.400 --> 01:58:02.080]   like that. I think to keep your sanity in this situation, first of all, it's important to
[01:58:02.080 --> 01:58:07.680]   understand that most of these people are not evil. They are not doing it on purpose. Many of them
[01:58:07.680 --> 01:58:15.840]   really believe that there is some very nefarious, powerful conspiracy which is causing a lot of harm
[01:58:15.840 --> 01:58:21.760]   in the world, and they are doing a good thing by exposing it and making people aware of it and
[01:58:21.760 --> 01:58:28.320]   trying to stop it. If you think that you're surrounded by evil, you're falling into the
[01:58:28.320 --> 01:58:33.440]   same rabbit hole. You're falling into the same paranoid state of mind, "Oh, the world is full
[01:58:33.440 --> 01:58:40.080]   of these evil people." No, most of them are good people. Also, I think we can empathize
[01:58:40.080 --> 01:58:48.640]   with some of the key ideas there, which I share, that yes, it's becoming more and more difficult
[01:58:48.640 --> 01:58:55.280]   to understand what is happening in the world. There are huge dangers in the world, existential
[01:58:55.280 --> 01:59:03.280]   dangers to the human species, but they don't come from a small cabal of Jews or gay people or
[01:59:03.280 --> 01:59:11.920]   feminists or whatever. They come from much more diffused forces, which are not under the control
[01:59:11.920 --> 01:59:20.320]   of any single individual. We don't have to look for the evil people. We need to look for human
[01:59:20.320 --> 01:59:29.840]   allies in order to work together against, again, the dangers of AI, the dangers of bioengineering,
[01:59:29.840 --> 01:59:35.840]   the dangers of climate change. When you wake up in the morning, the question is, do you want to
[01:59:35.840 --> 01:59:44.400]   spend your day spreading hatred, or do you want to spend your day trying to make allies and work
[01:59:44.400 --> 01:59:52.320]   together? - Let me ask you a big philosophical question about AI and the threat of it. Let's
[01:59:52.320 --> 02:00:00.240]   look at the threat side. Folks like Eliezer Yudkowsky worry that AI might kill all of us.
[02:00:00.240 --> 02:00:09.280]   Do you worry about that range of possibilities where artificial intelligence systems in a variety
[02:00:09.280 --> 02:00:17.120]   of ways might destroy human civilization? - Yes. I talk a lot about it, about the dangers of AI.
[02:00:17.120 --> 02:00:22.640]   I sometimes get into trouble because I depict these scenarios of how AI becoming very dangerous,
[02:00:22.640 --> 02:00:28.400]   and then people say that I'm encouraging these scenarios. I'm talking about it as a warning.
[02:00:28.400 --> 02:00:36.400]   I'm not so terrified of the simplistic idea, again, the Terminator scenario of robots running
[02:00:36.400 --> 02:00:44.640]   in the street, shooting everybody. I'm more worried about AI accumulating more and more power
[02:00:44.640 --> 02:00:53.520]   and basically taking over society, taking over our lives, taking power away from us until we
[02:00:53.520 --> 02:00:58.880]   don't understand what is happening and we lose control of our lives and of the future.
[02:00:58.880 --> 02:01:04.640]   The two most important things to realize about AI, so many things are being said now about AI,
[02:01:04.640 --> 02:01:11.120]   but I think there are two things that every person should know about AI. First is that AI
[02:01:11.120 --> 02:01:17.360]   is the first tool in history that can make decisions by itself. All previous tools in
[02:01:17.360 --> 02:01:24.320]   history couldn't make decisions. This is why they empowered us. You invent a knife, you invent an
[02:01:24.320 --> 02:01:32.000]   atom bomb. The atom bomb cannot decide to start a war, cannot decide which city to bomb. AI can
[02:01:32.000 --> 02:01:40.720]   make decisions by itself. Autonomous weapon systems can decide by themselves who to kill,
[02:01:40.720 --> 02:01:48.320]   who to bomb. The second thing is that AI is the first tool in history that can create new ideas
[02:01:48.320 --> 02:01:57.040]   by itself. The printing press could print our ideas but could not create new ideas. AI can
[02:01:57.040 --> 02:02:04.000]   create new ideas entirely by itself. This is unprecedented. Therefore, it is the first
[02:02:04.000 --> 02:02:10.960]   technology in history that instead of giving power to humans, it takes power away from us.
[02:02:10.960 --> 02:02:19.760]   And the danger is that it will increasingly take more and more power from us until we are left
[02:02:19.760 --> 02:02:27.200]   helpless and clueless about what is happening in the world. This is already beginning to happen in
[02:02:27.200 --> 02:02:34.320]   an accelerated pace. More and more decisions about our lives, whether to give us a loan,
[02:02:34.320 --> 02:02:40.960]   whether to give us a mortgage, whether to give us a job are taken by AI. More and more of the ideas,
[02:02:40.960 --> 02:02:48.960]   of the images, of the stories that surround us and shape our minds, our world are produced,
[02:02:48.960 --> 02:02:54.720]   are created by AI, not by human beings. - If you can just linger on that, what is the danger of
[02:02:54.720 --> 02:03:04.320]   that? That more and more of the creative side is done by AI, the idea generation. Is it that we
[02:03:04.320 --> 02:03:09.680]   become stale in our thinking? Is that that idea generation is so fundamental to like the
[02:03:10.000 --> 02:03:16.560]   evolution of humanity? - That we can't resist the idea. To resist an idea, you need to have some
[02:03:16.560 --> 02:03:25.040]   vision of the creative process. Now, this is a very old fear. You go back to Plato's cave,
[02:03:25.040 --> 02:03:32.720]   some of this idea that people are sitting chained in a cave and seeing shadows on a screen,
[02:03:32.720 --> 02:03:39.200]   on a wall, and thinking this is reality. You go back to Descartes, and he has this thought
[02:03:39.200 --> 02:03:45.920]   experiment of the demon. And Descartes asks himself, "How do I know that any of this is real?
[02:03:45.920 --> 02:03:53.840]   Maybe there is a demon who is creating all of this and is basically enslaving me by surrounding me
[02:03:53.840 --> 02:03:59.760]   with these illusions." You go back to Buddha, it's the same question. What if we are living in a
[02:03:59.760 --> 02:04:05.760]   world of illusions, and because we have been living in it throughout our lives, all our ideas
[02:04:05.760 --> 02:04:12.160]   or our desires, how we understand ourselves, this is all the product of the same illusions.
[02:04:12.160 --> 02:04:20.240]   And this was a big philosophical question for thousands of years. Now it's becoming a practical
[02:04:20.240 --> 02:04:26.160]   question of engineering, because previously all the ideas, as far as we know, maybe we are living
[02:04:26.160 --> 02:04:32.000]   inside a computer simulation of intelligent rats from the planet Zircon. If that's the case,
[02:04:32.000 --> 02:04:39.920]   we don't know about it. But taking what we do know about human history, until now, all the stories,
[02:04:39.920 --> 02:04:46.400]   images, paintings, songs, operas, theater, everything we've encountered and shaped our minds
[02:04:46.400 --> 02:04:53.440]   was created by humans. Now increasingly, we live in a world where more and more of these cultural
[02:04:53.440 --> 02:05:00.080]   artifacts will be coming from an alien intelligence. Very quickly, we might reach a point
[02:05:00.080 --> 02:05:09.040]   when most of the story, stories, images, songs, TV shows, whatever, are created by an alien
[02:05:09.040 --> 02:05:15.680]   intelligence. And if we now find ourselves inside this kind of world of illusions,
[02:05:16.720 --> 02:05:25.200]   created by an alien intelligence that we don't understand, but it understands us, this is a kind
[02:05:25.200 --> 02:05:34.160]   of spiritual enslavement that we won't be able to break out of, because it understands us, it
[02:05:34.160 --> 02:05:43.200]   understands how to manipulate us, but we don't understand what is behind this screen of stories
[02:05:43.200 --> 02:05:50.880]   and images and songs. - So if there's a set of AI systems that are operating in the space of ideas
[02:05:50.880 --> 02:05:57.120]   that are far superior to ours, and we're not almost able to, it's opaque to us, we're not
[02:05:57.120 --> 02:06:05.680]   able to see through, how does that change the pursuit of happiness, the human pursuit of
[02:06:05.680 --> 02:06:13.760]   happiness, life? Where do we get joy if we're surrounded by AI systems that are doing most of
[02:06:13.760 --> 02:06:20.080]   the cool things humans do much better than us? - Some of the things, it's okay that the AIs
[02:06:20.080 --> 02:06:30.080]   will do them. Many human tasks and jobs, they're drudgery, they are not fun, they are not developing
[02:06:30.720 --> 02:06:37.040]   us emotionally or spiritually, it's fine if the robots take over. I don't know, I think about the
[02:06:37.040 --> 02:06:43.840]   people in supermarkets or grocery stores that spend hours every day just passing items and
[02:06:43.840 --> 02:06:50.560]   charging you the money. I mean, if this can be automated, wonderful. We need to make sure that
[02:06:50.560 --> 02:07:00.320]   these people then have better jobs, better means of supporting themselves, and developing their
[02:07:00.320 --> 02:07:08.560]   social abilities, their spiritual abilities. That's the ideal world that AI can create,
[02:07:08.560 --> 02:07:19.040]   that it takes away from us the things that it's better if we don't do them and allows us to focus
[02:07:19.040 --> 02:07:24.880]   on the most important things and the deepest aspects of our nature, of our potential.
[02:07:24.880 --> 02:07:33.680]   If we give AI control of the sphere of ideas at this stage, I think it's very, very dangerous
[02:07:33.680 --> 02:07:43.680]   because it doesn't understand us. AI at present is mostly digesting the products of human culture.
[02:07:44.480 --> 02:07:51.760]   Everything we've produced over thousands of years, it eats all of these cultural products,
[02:07:51.760 --> 02:07:59.120]   digests it, and starts producing its own new stuff. But we still haven't figured out
[02:07:59.120 --> 02:08:09.040]   ourselves, in our bodies, our brains, our minds, our psychology. So, an AI based on our flawed
[02:08:09.040 --> 02:08:18.800]   understanding of ourselves is a very dangerous thing. I think that we need, first of all, to
[02:08:18.800 --> 02:08:27.280]   keep developing ourselves. If for every dollar and every minute that we spend on developing AI,
[02:08:27.280 --> 02:08:34.160]   artificial intelligence, we spend another dollar and another minute in developing human consciousness,
[02:08:34.160 --> 02:08:41.840]   the human mind will be okay. The danger is that we spend all our effort on developing an AI at a
[02:08:41.840 --> 02:08:49.680]   time when we don't understand ourselves, and then letting the AI take over, that's a road to a human
[02:08:49.680 --> 02:08:55.840]   catastrophe. - Does this surprise you how well large language models work? I mean, has it modified
[02:08:55.840 --> 02:09:01.200]   your understanding of the nature of intelligence? - Yes. I mean, I've been writing about AI for,
[02:09:01.920 --> 02:09:08.640]   I don't know, like eight years now, and engaged with all these predictions and speculations.
[02:09:08.640 --> 02:09:13.760]   And when it actually came, it was much faster and more powerful than I thought it would be.
[02:09:13.760 --> 02:09:23.360]   I didn't think that we would have, in 2023, an AI that can hold a conversation that you can't know
[02:09:23.360 --> 02:09:31.600]   if it's a human being or an AI that can write beautiful texts. I mean, I read the texts written
[02:09:31.600 --> 02:09:38.640]   by AI, and the thing that strikes me most is the coherence. You know, people think, "Oh, it's
[02:09:38.640 --> 02:09:45.040]   nothing, they just take ideas from here and there, words from here and put it." No, it's so coherent.
[02:09:45.040 --> 02:09:51.840]   I mean, you read, not sentences, you read paragraphs, you read entire texts, and there is
[02:09:51.840 --> 02:09:57.600]   logic, there is a structure. - It's not only coherent, it's convincing. - Yes, it makes sense.
[02:09:57.600 --> 02:10:02.480]   - And the beautiful thing about it that has to do with your work, it doesn't have to be true.
[02:10:02.480 --> 02:10:05.760]   - No. - And it often gets facts wrong,
[02:10:05.760 --> 02:10:10.800]   but it still is convincing. And it is both scary and beautiful. - Yes.
[02:10:10.800 --> 02:10:19.600]   - That our brains love language so much that we don't need the facts to be correct. We just need
[02:10:19.600 --> 02:10:23.520]   it to be a beautiful story. - Yeah. That's been the secret of
[02:10:23.520 --> 02:10:28.320]   politics and religion for thousands of years, and now it's coming with AI.
[02:10:28.320 --> 02:10:34.880]   - So you, as a person who has written some of the most impactful words ever written in your books,
[02:10:34.880 --> 02:10:42.000]   how does that make you feel that you might be one of the last effective human writers?
[02:10:42.000 --> 02:10:45.360]   - That's a good question. - First of all, do you think that's possible?
[02:10:45.360 --> 02:10:52.560]   - I think it is possible. I've seen a lot of examples of AI being told, "Write like Yuval
[02:10:52.560 --> 02:10:55.920]   Noah Harari and what it produces." - Has it ever done better than you
[02:10:55.920 --> 02:11:02.720]   think you could have written yourself? - I mean, on the level of content of ideas,
[02:11:02.720 --> 02:11:10.720]   no. There are things I say I would never say that. But when it comes to the, I mean, there is,
[02:11:10.720 --> 02:11:18.560]   again, the coherence and the quality of writing is such that I say it's unbelievable how good it is.
[02:11:19.760 --> 02:11:27.920]   And who knows, in 10 years, in 20 years, maybe it can do better, even according to certain measures
[02:11:27.920 --> 02:11:33.680]   on the level of content. - So that people will be able to do a style
[02:11:33.680 --> 02:11:42.480]   transfer, do in the style of Yuval Noah Harari, write anything, write why I should have ice cream
[02:11:42.480 --> 02:11:46.320]   tonight, and make it convincing. - I don't know if I have anything
[02:11:46.320 --> 02:11:48.800]   convincing to say about these things. - I think you'd be surprised.
[02:11:48.800 --> 02:11:52.720]   - I think you'd be surprised. It could be an evolutionary biology explanation for why.
[02:11:52.720 --> 02:11:55.600]   - Yeah, ice cream is good for you. - Yeah. So I mean,
[02:11:55.600 --> 02:12:02.080]   that changes the nature of writing. - Ultimately, I think it goes back.
[02:12:02.080 --> 02:12:14.240]   Much of my writing is suspicious of itself. I write stories about the danger of stories.
[02:12:14.880 --> 02:12:22.880]   I write about intelligence, but highlighting the dangers of intelligence. Ultimately, I don't think
[02:12:22.880 --> 02:12:30.640]   that in terms of power, human power comes from intelligence and from stories. But I think that
[02:12:30.640 --> 02:12:38.240]   the deepest and best qualities of humans are not intelligence and not storytelling and not power.
[02:12:38.240 --> 02:12:43.520]   Again, with all our power, with all our cooperation, with all our intelligence,
[02:12:43.520 --> 02:12:48.320]   we are on the verge of destroying ourselves and destroying much of the ecosystem.
[02:12:48.320 --> 02:12:57.600]   Our best qualities are not there. Our best qualities are nonverbal. Again, they come from
[02:12:57.600 --> 02:13:03.840]   things like compassion, from introspection. And introspection, from my experience, is not verbal.
[02:13:03.840 --> 02:13:11.280]   If you try to understand yourself with words, you will never succeed. There is a place where you
[02:13:11.280 --> 02:13:19.200]   need the words. But the deepest insights, they don't come from words. And you can't write about
[02:13:19.200 --> 02:13:24.720]   it. That's again, it goes back to Wittgenstein, to Buddha, to so many of these sages before,
[02:13:24.720 --> 02:13:30.720]   that these are the things we are silent about. - Yeah, but eventually you have to project it.
[02:13:30.720 --> 02:13:36.720]   As a writer, you have to do the silent introspection, but project it onto a page.
[02:13:36.720 --> 02:13:43.120]   - Yes, but you still have to warn people, you will never find the deepest truth in a book.
[02:13:43.120 --> 02:13:50.240]   You will never find it in words. You can only find it, I think, in direct experience,
[02:13:50.240 --> 02:13:54.560]   which is nonverbal, which is pre-verbal. - In the silence of your own mind.
[02:13:54.560 --> 02:13:55.840]   - Yes. - Somewhere in there.
[02:13:55.840 --> 02:14:00.000]   - Yes. - Well, let me ask you a silly question then,
[02:14:01.280 --> 02:14:08.560]   a ridiculously big question. You have done a lot of deep thinking about the world, about yourself,
[02:14:08.560 --> 02:14:16.640]   this kind of introspection. How do you think, if you, by way of advice, but just practically
[02:14:16.640 --> 02:14:20.960]   speaking, day to day, how do you think about difficult problems of the world?
[02:14:20.960 --> 02:14:29.760]   - First of all, I take time off. The most important thing I do, I think, as a writer,
[02:14:29.760 --> 02:14:36.160]   as a scientist, I meditate. I spend about two hours every day in silent meditation,
[02:14:36.160 --> 02:14:44.080]   observing as much as possible nonverbally what is happening within myself, focusing body
[02:14:44.080 --> 02:14:50.160]   sensations, the breath. Thoughts keep coming up, but I try not to give them attention. I don't try
[02:14:50.160 --> 02:14:54.480]   to drive them away, just let them be there in the background, like some background noise.
[02:14:54.480 --> 02:15:04.080]   Don't engage with the thoughts, because the mind is constantly producing stories with words. These
[02:15:04.080 --> 02:15:10.080]   stories come between us and the world. They don't allow us to see ourselves or the world.
[02:15:10.080 --> 02:15:14.800]   For me, the most shocking thing when I started meditating 23 years ago,
[02:15:14.800 --> 02:15:20.240]   I was given this simple exercise to just observe my breath coming in and out of the nostrils,
[02:15:21.120 --> 02:15:26.560]   not controlling it, just observing it. I couldn't do it for more than 10 seconds.
[02:15:26.560 --> 02:15:30.640]   I, for 10 seconds, would try to notice, "Oh, now the breath is coming in. It's coming in,
[02:15:30.640 --> 02:15:34.960]   it's coming in. Oh, it stopped coming in, and now it's going out, going out." 10 seconds,
[02:15:34.960 --> 02:15:40.400]   and some memory would come, some thought would come, some story about something that happened
[02:15:40.400 --> 02:15:49.040]   last week or 10 years ago or in the future. The story would hijack my attention. It would take me
[02:15:49.040 --> 02:15:56.240]   maybe five minutes to remember, "Oh, I'm supposed to be observing my breath." If I can't observe my
[02:15:56.240 --> 02:16:03.120]   own breath because of these stories created by the mind, how can I hope to understand much more
[02:16:03.120 --> 02:16:09.200]   complex things like the political situation in Israel, the Israeli-Palestinian conflict,
[02:16:09.200 --> 02:16:14.080]   the Russian invasion of Ukraine? If all these stories keep coming, I mean, it's not the truth,
[02:16:14.080 --> 02:16:20.960]   it's just the story your own mind created. So first thing, train the mind to be silent and
[02:16:20.960 --> 02:16:26.720]   just observe. So two hours every day, and I go every year for a long retreat between one month
[02:16:26.720 --> 02:16:32.880]   and two months, 60 days of just silent meditation. Silent meditation for 60 days.
[02:16:32.880 --> 02:16:40.080]   Yeah. To train the mind, forget about your own stories, just observe what is really happening.
[02:16:41.280 --> 02:16:49.840]   And then also throughout the day, have an information diet. Today, many people are very
[02:16:49.840 --> 02:16:57.280]   aware of what they feed their body, what enters their mouth. Be very aware of what you feed your
[02:16:57.280 --> 02:17:04.160]   mind, what enters your mind. Have an information diet. So for instance, I read long books.
[02:17:05.360 --> 02:17:12.400]   And I prefer, like I do many interviews, I prefer three-hour interviews to five-minute interviews.
[02:17:12.400 --> 02:17:21.840]   The long format, it's not always feasible, but you can go much, much deeper.
[02:17:21.840 --> 02:17:27.440]   So I would say an information diet, be very careful about what you feed your mind,
[02:17:27.440 --> 02:17:33.760]   give preference to big chunks over small-- LUKE: To books over Twitter.
[02:17:33.760 --> 02:17:39.280]   SRS: Yes, books over Twitter, definitely. And then when I encounter a problem,
[02:17:39.280 --> 02:17:52.320]   a difficult intellectual problem, then I let the problem lead me where it goes and not where I want
[02:17:52.320 --> 02:18:00.000]   it to go. If I approach a problem with some preconceived idea or solution and then try to
[02:18:00.000 --> 02:18:05.680]   impose it on the problem and just find confirmation bias, just find the evidence that supports my
[02:18:05.680 --> 02:18:11.760]   view, this is easy for the mind to do and you don't learn anything new.
[02:18:11.760 --> 02:18:18.800]   LUKE: Do you take notes? Do you start to concretize your thoughts on paper?
[02:18:18.800 --> 02:18:26.400]   SRS: I read a lot. Usually I don't take notes. Then I start writing and when I write,
[02:18:26.400 --> 02:18:33.520]   I write like a torrent, just write. Now it's the time you read, you did meditation, now it's the
[02:18:33.520 --> 02:18:41.520]   time to write, write. Don't stop, just write. So I would write from memory and I'm not afraid
[02:18:41.520 --> 02:18:47.760]   of formulating, say, big ideas, big theories and putting them on paper. The danger is once it's on
[02:18:47.760 --> 02:18:54.480]   paper, not on paper, on the screen, in the computer, you get attached to it and then you
[02:18:54.480 --> 02:19:00.160]   start with confirmation bias to build more and more layers around it and you can't go back.
[02:19:00.160 --> 02:19:09.040]   And then it's very dangerous. But I trust myself that I have to some extent the ability to press
[02:19:09.040 --> 02:19:17.360]   the delete button. The most important button in the keyboard is delete. I write and then I delete.
[02:19:17.360 --> 02:19:23.440]   I write and then I delete. And because I trust myself that I'll have the… every time I come
[02:19:23.440 --> 02:19:29.040]   to press the delete button, I feel bad. It's a kind of pain. I created this, it's a beautiful
[02:19:29.040 --> 02:19:36.000]   idea and I have to delete it. But I try and hopefully I do it enough times. And this is
[02:19:36.000 --> 02:19:42.240]   important because in the long term, it enables me to play with ideas. I have the confidence to
[02:19:42.240 --> 02:19:48.880]   start formulating some brave idea. Most of them turn out to be nonsense.
[02:19:50.240 --> 02:19:56.720]   But I trust myself not to be attached, not to become attached to my own nonsense.
[02:19:56.720 --> 02:20:02.880]   So it gives me this room for playfulness. - I would be amiss if I didn't ask, for people
[02:20:02.880 --> 02:20:07.840]   interested in hearing you talk about meditation, if they want to start meditating, what advice
[02:20:07.840 --> 02:20:14.320]   would you give on how to start? You mentioned you couldn't hold your attention on your breath
[02:20:14.320 --> 02:20:18.560]   for longer than 10 seconds at first. So how did they start on this journey?
[02:20:18.560 --> 02:20:26.400]   - First of all, it's a difficult journey. It's not fun, it's not recreational, it's not
[02:20:26.400 --> 02:20:32.560]   kind of time to relax. It can be very, very intense. The most difficult thing, at least
[02:20:32.560 --> 02:20:37.680]   in the meditation I practice, vipassana, which I learned from a teacher called S. N. Goenka,
[02:20:37.680 --> 02:20:43.840]   the most difficult thing is not the silence, it's not the sitting for long hours, it's what comes
[02:20:43.840 --> 02:20:50.720]   up. Everything you don't want to know about yourself, this is what comes up. So it's very
[02:20:50.720 --> 02:20:56.240]   intense and difficult. If you go to a meditation retreat, don't think you're going to relax.
[02:20:56.240 --> 02:21:03.280]   - So what's the experience of a meditation retreat when everything you don't like comes up
[02:21:03.280 --> 02:21:05.760]   for 30 days? - It depends what comes up.
[02:21:05.760 --> 02:21:12.320]   Anger comes up, you're angry. For days on end, you're just boiling with anger. Everything makes
[02:21:12.320 --> 02:21:18.000]   you angry. Again, something that happens right now, or you remember something from 20 years ago,
[02:21:18.000 --> 02:21:23.440]   and you start boiling with... It's like, I never even thought about this incident,
[02:21:23.440 --> 02:21:32.160]   but it was somewhere stored with a huge, huge pile of anger attached to it, and it's now coming up,
[02:21:32.160 --> 02:21:38.640]   and all the anger is coming up. Maybe it's boredom. You know, 30 days of meditation,
[02:21:38.640 --> 02:21:45.680]   you start getting bored, and it's the most boring thing. Suddenly, no anger, it's the most boring.
[02:21:45.680 --> 02:21:55.280]   Another second, I scream. And boredom is one of the most difficult things to deal with in life.
[02:21:55.280 --> 02:22:01.520]   I think it's closely related to death. Death is boring. In many movies, death is exciting;
[02:22:01.520 --> 02:22:07.520]   it's not exciting. When you die, ultimately, it's boredom. Nothing happens.
[02:22:07.520 --> 02:22:12.000]   - It's the end of exciting things. - The end. And many things in the world
[02:22:12.000 --> 02:22:18.160]   happen because of boredom. To some extent, people start entire wars because of boredom.
[02:22:18.160 --> 02:22:25.600]   People quit relationships. People quit jobs because of boredom. And if you never learn how
[02:22:25.600 --> 02:22:34.000]   to deal with boredom, you will never learn how to enjoy peace and quiet, because the way to peace
[02:22:34.000 --> 02:22:40.240]   passes through boredom. And from what I experienced with meditation, I think
[02:22:40.240 --> 02:22:46.800]   maybe it was the most difficult, maybe at least in the top three, much more difficult, say,
[02:22:46.800 --> 02:22:52.880]   than anger or pain. When pain comes up, you feel heroic. "Hey, I'm dealing with pain."
[02:22:52.880 --> 02:22:59.920]   When boredom comes up, it brings it with depression and feelings of worthlessness,
[02:22:59.920 --> 02:23:05.440]   and it's nothing. I'm nothing. - The way to peace is through boredom.
[02:23:05.440 --> 02:23:10.880]   David Foster Wallace said the key to life is to be unborable.
[02:23:10.880 --> 02:23:18.240]   Which is a different perspective on what you're talking to. Is there truth to that?
[02:23:18.240 --> 02:23:23.680]   - Yes, I mean, it's closely related. I would say, like, I look at the world today, like politics,
[02:23:23.680 --> 02:23:30.560]   the one thing we need more than anything else is boring politicians. We have a super abundance of
[02:23:30.560 --> 02:23:37.360]   very exciting politicians who are doing and saying very exciting things, and we need boring
[02:23:37.360 --> 02:23:44.880]   politicians. And we need them quickly. - Yeah, the way to peace is through boredom.
[02:23:44.880 --> 02:23:49.600]   That applies in more ways than one. What advice would you give to young people
[02:23:50.880 --> 02:23:57.040]   today in high school and college how to have a successful life, how to have a successful career?
[02:23:57.040 --> 02:24:03.520]   - What they should know, it's the first time in history nobody has any idea how the world would
[02:24:03.520 --> 02:24:08.800]   look like in 10 years. Nobody has any idea how the world would look like when you grow up.
[02:24:08.800 --> 02:24:12.800]   You know, throughout history, it was never possible to predict the future. You live in
[02:24:12.800 --> 02:24:18.640]   the Middle Ages, nobody knows. Maybe in 10 years, the Vikings will invade, the Mongols will invade,
[02:24:18.640 --> 02:24:25.520]   there'll be an epidemic, there'll be an earthquake, who knows? But the basic structures of life will
[02:24:25.520 --> 02:24:33.280]   not change. Most people will still be peasants. Armies would fight on horseback with swords and
[02:24:33.280 --> 02:24:41.040]   bows and arrows and things like that. So you could learn a lot from the wisdom of your elders.
[02:24:41.040 --> 02:24:45.840]   They've been there before, and they knew what kind of basic skills you need to learn.
[02:24:47.040 --> 02:24:53.520]   Most people need to learn how to sow wheat and harvest wheat or rice and make bread
[02:24:53.520 --> 02:25:00.320]   and build a house and ride a horse and things like that. Now we have no idea, not just about
[02:25:00.320 --> 02:25:07.200]   politics. We have no idea how the job market would look like in 10 years. We have no idea
[02:25:07.200 --> 02:25:16.160]   what skills will still be needed. You think you're going to learn how to code because
[02:25:16.160 --> 02:25:21.600]   they'll need a lot of coders in the 2030s? Think again. Maybe AI is doing all the coding. You don't
[02:25:21.600 --> 02:25:26.960]   need any coders. You're going to, I don't know, you learn to translate languages, you want to be a
[02:25:26.960 --> 02:25:34.160]   translator, gone. We don't know what skills will be needed. So the most important skill
[02:25:34.160 --> 02:25:40.640]   is the skill to keep learning and keep changing throughout our lives, which is very, very
[02:25:40.640 --> 02:25:47.920]   difficult, to keep reinventing ourselves. It's a deep, again, it's in a way a spiritual practice
[02:25:47.920 --> 02:25:58.400]   to build your personality, to build your mind as a very flexible mind.
[02:25:58.400 --> 02:26:10.480]   If traditionally people thought about education, like building a stone house with very deep
[02:26:10.480 --> 02:26:17.680]   foundations, now it's more like setting up a tent that you can fold and move to the next place
[02:26:17.680 --> 02:26:24.480]   very, very quickly because that's the 21st century. - Which also raises questions about the
[02:26:24.480 --> 02:26:30.080]   future of education, what that looks like. Let me ask you about love.
[02:26:30.080 --> 02:26:39.520]   What were some of the challenges? What were some of the lessons about love, about life that you
[02:26:39.520 --> 02:26:46.480]   learned from coming out as gay? - In many ways, it goes back to the stories. I think this is one of
[02:26:46.480 --> 02:26:57.040]   the reasons I became so interested in stories and in their power. Because I grew up in a small
[02:26:57.040 --> 02:27:07.280]   Israeli town in the 1980s, early 1990s, which was very homophobic. And I basically embraced it,
[02:27:08.080 --> 02:27:16.800]   I breathed it, because you could hardly even think differently. So you had these two powerful
[02:27:16.800 --> 02:27:27.040]   stories around, one, that God hates gay people and that He will punish them for who they are or for
[02:27:27.040 --> 02:27:34.160]   what they do. Secondly, that it's not God, it's nature, that there is something diseased or sick
[02:27:34.160 --> 02:27:42.640]   about it. And these people, maybe they're not sinners, but they are sick, they are defective.
[02:27:42.640 --> 02:27:48.560]   And nobody wanted to identify with such a thing. If your options, okay, you can be a sinner,
[02:27:48.560 --> 02:27:54.720]   you can be a defect, what do you want? No good options there. And it took me many years,
[02:27:54.720 --> 02:28:01.840]   till I was 21, to come to terms with it. And one of the things, I learned two things. First,
[02:28:01.840 --> 02:28:11.200]   about the amazing capacity of the human mind for denial and delusion. An algorithm could have told
[02:28:11.200 --> 02:28:17.680]   me that I'm gay when I was like 14 or 15. If there is a good-looking guy and girl walking,
[02:28:17.680 --> 02:28:26.400]   I would immediately focus on the guy. But I didn't connect the dots. I could not understand
[02:28:26.400 --> 02:28:32.960]   what was happening inside my own brain and my own mind and my own body. It took me a long time to
[02:28:32.960 --> 02:28:38.560]   realize, "You know, you're just gay." So that speaks to the power of social
[02:28:38.560 --> 02:28:43.040]   convention versus individual thought. This is the power of self-delusion.
[02:28:43.040 --> 02:28:50.560]   That it's not that I knew I was gay and was hiding it. I was hiding it for myself successfully,
[02:28:50.560 --> 02:28:54.960]   that I don't understand how it is possible. Looking back, I don't understand how it is
[02:28:54.960 --> 02:29:00.080]   possible. But I know it is possible. I knew and didn't know at the same time.
[02:29:00.080 --> 02:29:06.240]   And then the other big lesson is the power of the stories, of the social conventions.
[02:29:06.240 --> 02:29:11.760]   Because the stories were not true. They did not make sense even on their own terms.
[02:29:11.760 --> 02:29:18.720]   Even if you accept the basic religious framework of the world, that there is a good God that
[02:29:18.720 --> 02:29:27.360]   created everything and controls everything, why would a good God punish people for love?
[02:29:27.360 --> 02:29:34.960]   I understand why a good God would punish people for violence, for hatred, for cruelty. But why
[02:29:34.960 --> 02:29:42.720]   would God punish people for love, especially when he created them that way? So even if you accept
[02:29:42.720 --> 02:29:49.520]   the religious framework of the world, obviously the story that God hates gay people, it comes
[02:29:49.520 --> 02:29:55.920]   not from God, but from some humans who invented this story. They take their own hatred. This is
[02:29:55.920 --> 02:30:01.280]   something humans do all the time. They hate somebody and they say, "No, I don't hate them.
[02:30:01.280 --> 02:30:09.440]   God hates them." They throw their own hatred on God. And then if you think about the scientific
[02:30:09.440 --> 02:30:14.960]   framework that said that, "Oh, gays, they are against nature. They are against the laws of
[02:30:14.960 --> 02:30:22.240]   nature," and so forth, science tells us nothing can exist against the laws of nature.
[02:30:22.240 --> 02:30:29.440]   Things that go against the laws of nature just don't exist. There is a law of nature that you
[02:30:29.440 --> 02:30:35.280]   can't move faster than the speed of light. Now, you don't have this minority of people who break
[02:30:35.280 --> 02:30:41.200]   the laws of nature by going faster than the speed of light. And then nature comes, "No, that's bad.
[02:30:41.200 --> 02:30:45.920]   You shouldn't do that." That's not how nature works. If something goes against the laws of
[02:30:45.920 --> 02:30:52.720]   nature, it just can't exist. The fact that gay people exist, me and not just people, you see
[02:30:52.720 --> 02:31:00.720]   homosexuality among many, many mammals and birds and other animals. It exists because it is in line
[02:31:01.280 --> 02:31:07.840]   with the laws of nature. The idea that this is sick, that this is whatever, it comes not from
[02:31:07.840 --> 02:31:14.800]   nature. It comes from the human imagination. Some people who, for whatever reasons, hated gay people,
[02:31:14.800 --> 02:31:21.840]   they said, "Oh, they go against nature." But this is a story created by people. This is not the laws
[02:31:21.840 --> 02:31:30.480]   of nature. And this taught me that so many of the things that we think are natural or eternal or
[02:31:30.480 --> 02:31:38.080]   divine, no, they're just human stories. But these human stories are often the most powerful forces
[02:31:38.080 --> 02:31:48.800]   in the world. - So what did you learn from your personal struggle of journey through the social
[02:31:48.800 --> 02:31:54.480]   conventions to find one of the things that makes life awesome, which is love? So what it takes to
[02:31:55.280 --> 02:32:00.320]   strip away the self-delusion and the pressures of social convention to wake up?
[02:32:00.320 --> 02:32:07.040]   - It takes a lot of work, a lot of courage, and a lot of help from other people.
[02:32:07.040 --> 02:32:14.800]   This kind of, again, heroic idea that I can do it all by myself, it doesn't work.
[02:32:14.800 --> 02:32:22.160]   Certainly with love, you need at least one more person. And I'm very happy that I found Itzik.
[02:32:22.160 --> 02:32:28.160]   We lived in the same small Israeli town. We lived on two adjacent streets for years,
[02:32:28.160 --> 02:32:34.320]   probably went to school on the same bus for years without really encountering each other. In the end,
[02:32:34.320 --> 02:32:42.960]   we met on one of the first dating sites on the internet for gay people in Israel in 2002.
[02:32:42.960 --> 02:32:46.480]   - You're saying the internet works. - Yes. I said a lot of bad things
[02:32:46.480 --> 02:32:51.120]   or dangers about technology and the internet. There are also, of course, good things. And
[02:32:51.120 --> 02:32:58.080]   this is not an accident. You have two kinds of minorities in history. You have minorities which
[02:32:58.080 --> 02:33:05.600]   are a cohesive group, like Jews, that, yes, you're as small as being born Jewish in, say,
[02:33:05.600 --> 02:33:11.840]   Germany or Russia or whatever. You're born in a small community. But as a Jewish boy, you're born
[02:33:11.840 --> 02:33:16.400]   to a Jewish family. You have Jewish parents. You have Jewish siblings. You're in a Jewish
[02:33:16.400 --> 02:33:21.200]   neighborhood. You have Jewish friends. So these kinds of minorities, they could always come
[02:33:21.200 --> 02:33:27.440]   together and help each other throughout history. Another type of minority, like gay people or more
[02:33:27.440 --> 02:33:35.040]   broadly LGBTQ people, that as a gay boy, you're usually not born to a gay family with gay parents
[02:33:35.040 --> 02:33:42.400]   and gay siblings in a gay neighborhood. So usually you find yourself completely alone.
[02:33:43.120 --> 02:33:49.120]   For most of history, one of the biggest problems for the gay community was that there was no
[02:33:49.120 --> 02:33:56.640]   community. How do you find one another? And the internet was a wonderful thing in this respect
[02:33:56.640 --> 02:34:03.440]   because it made it very easy for these kinds of diffuse communities or diffuse minorities to find
[02:34:03.440 --> 02:34:08.800]   each other. So me and Itzik, even though we rode the same bus together to school for years, we
[02:34:08.800 --> 02:34:14.080]   didn't meet in the physical world. We met online. Because again, in the physical world, you don't
[02:34:14.080 --> 02:34:19.360]   want to identify in an Israeli town in the 1980s. You ride the bus. You don't want to say, "Hey,
[02:34:19.360 --> 02:34:24.640]   I'm gay. Is there anybody else gay here?" That's not a good idea. But on the internet, we could
[02:34:24.640 --> 02:34:29.040]   find each other. There's another lesson in there that maybe sometimes the thing you're looking for
[02:34:29.040 --> 02:34:37.680]   is right under your nose. Yeah. A very old lesson and a very true lesson in many ways. So you need
[02:34:37.680 --> 02:34:45.120]   help from other people to realize the truth about yourself. So of course, in love, you cannot just
[02:34:45.120 --> 02:34:51.280]   love abstractly. There is another person there. You need to find them. But also, we were one of
[02:34:51.280 --> 02:34:58.880]   the first generations who enjoyed the benefits of gay liberation, of the very difficult struggles
[02:34:58.880 --> 02:35:06.560]   of people who were much braver than us in the 1980s, 1970s, 1960s, who dared to question
[02:35:06.560 --> 02:35:14.800]   social conventions, to struggle at sometimes a terrible price. And we benefited from it.
[02:35:14.800 --> 02:35:20.160]   And more broadly, we spoke earlier about the feminist movement. There would have been no
[02:35:20.160 --> 02:35:29.680]   gay liberation without the feminist movement. We also owe them for starting to change the gender
[02:35:29.680 --> 02:35:37.200]   structure of the world. And this is always true. You can never do it just by yourself.
[02:35:37.200 --> 02:35:43.760]   Also, I look at my journey in meditation. I could not have found the idea of going to
[02:35:43.760 --> 02:35:50.480]   meditation retreat okay, but I couldn't discover meditation by… I couldn't develop the meditation
[02:35:50.480 --> 02:35:57.200]   technique by myself. Somebody had to teach me this way of how to look inside yourself.
[02:35:57.200 --> 02:36:06.800]   And it's also a very important lesson that you can't do it just by yourself,
[02:36:06.800 --> 02:36:13.280]   that this fantasy of complete autonomy, of complete self-sufficiency, it doesn't work.
[02:36:13.280 --> 02:36:19.920]   You hear, it tends to be a very kind of male macho fantasy. I don't need anybody. I can be so
[02:36:19.920 --> 02:36:24.960]   strong and so brave that I'll do everything by myself. It never works.
[02:36:24.960 --> 02:36:36.960]   You need friends, you need a mentor, you need the very thing that makes us human, as other humans.
[02:36:38.560 --> 02:36:43.840]   You mentioned that the fear of boredom might be a kind of proxy for the fear of death.
[02:36:43.840 --> 02:36:49.200]   So what role does the fear of death play in the human condition? Are you afraid of death?
[02:36:49.200 --> 02:36:55.920]   Yes, I think everybody is afraid of death. I mean, all our fears come out of the fear of death.
[02:36:55.920 --> 02:37:02.960]   But the fear of death is just so deep and difficult. Usually, we can't face it directly.
[02:37:02.960 --> 02:37:08.880]   So we cut it into little pieces, and we face just little pieces. Oh, I lost my smartphone.
[02:37:08.880 --> 02:37:14.400]   That's a little, little, little piece of the fear of death, which is of losing everything.
[02:37:14.400 --> 02:37:19.520]   So I can't deal with losing everything. I'm dealing now with losing my phone or losing a
[02:37:19.520 --> 02:37:28.320]   book or whatever. I feel pain. That's a small bit of the fear of death. Somebody who really doesn't
[02:37:28.320 --> 02:37:34.400]   fear death would not fear anything at all. They'll be like, "Anything that happens,
[02:37:34.400 --> 02:37:36.880]   I can deal with it. If I can deal with death, this is nothing."
[02:37:36.880 --> 02:37:43.040]   - So any fear is a distant echo of the big fear of death. Have you ever
[02:37:43.040 --> 02:37:50.640]   looked at it head on, caught glimpses, sort of contemplated as the Stoics do?
[02:37:50.640 --> 02:37:56.080]   - Yes. I mean, when I was a teenager, I would constantly contemplate it,
[02:37:56.080 --> 02:38:05.920]   trying to understand, to imagine. It was a very, very shocking and moving experience. I remember
[02:38:05.920 --> 02:38:12.080]   especially in connection with national ideology, which was also very big, strong in Israel, still
[02:38:12.080 --> 02:38:17.600]   is, which again comes from the fear of death. You know that you're going to die, so you say,
[02:38:17.600 --> 02:38:21.760]   "Okay, I die, but the nation lives on. I live on through the nation. I don't really die."
[02:38:22.960 --> 02:38:30.080]   And you hear it especially on Memorial Day for fallen soldiers. So every day, there'll be in
[02:38:30.080 --> 02:38:36.800]   school Memorial Day for fallen soldiers who fell defending Israel in all its different wars,
[02:38:36.800 --> 02:38:41.840]   and all these kids would come dressed in white, and you'll have this big ceremony with flags and
[02:38:41.840 --> 02:38:48.400]   songs and dances in memory of the fallen soldiers. And you get the impression, again, I don't want
[02:38:48.400 --> 02:38:52.480]   it to sound crass, but you get the impression that the best thing in life is to be a fallen soldier.
[02:38:53.200 --> 02:38:56.880]   Because even though, yes, you die, everybody dies in the end, but then you'll have all these
[02:38:56.880 --> 02:39:02.400]   school kids for years and years remembering you and celebrating you, and you don't really die.
[02:39:02.400 --> 02:39:07.040]   And I remember standing in these ceremonies and thinking, "What does it actually mean?"
[02:39:07.040 --> 02:39:16.720]   Like, okay, so if I'm a fallen soldier, now I'm a skeleton, I'm bones in this military cemetery
[02:39:16.720 --> 02:39:23.840]   under this stone, do I actually hear the kids singing all these patriotic songs? If not,
[02:39:23.840 --> 02:39:27.600]   how do I know they do it? Maybe they trick me. Maybe I die in the war, and then they don't sing
[02:39:27.600 --> 02:39:36.160]   any songs. And how does it help me? And I realized, I was quite young at the time, that if you're dead,
[02:39:36.160 --> 02:39:40.720]   you can't hear anything because that's the meaning of being dead. And if you're dead,
[02:39:40.720 --> 02:39:44.080]   you can't think of anything like, "Oh, now they're remembering because you're dead." That's
[02:39:44.080 --> 02:39:47.840]   the meaning of being dead. And it was a shocking realization.
[02:39:47.840 --> 02:39:53.600]   But it's a really difficult realization to keep hold in your mind. Like, it's the end.
[02:39:53.600 --> 02:39:59.680]   I lost it over time. I mean, for many years, it was a very powerful fuel, motivation for
[02:39:59.680 --> 02:40:05.760]   philosophical, for spiritual exploration. And I realized that the fear of death is really a
[02:40:05.760 --> 02:40:12.800]   very powerful drive. And over the years, especially as I meditated, it kind of dissipated. And today,
[02:40:12.800 --> 02:40:18.720]   sometimes I find myself trying to recapture this teenage fear of death, because it was so
[02:40:18.720 --> 02:40:23.440]   powerful, and I just can't. I try to make the same image. I don't know.
[02:40:23.440 --> 02:40:26.960]   Something about the teenage years.
[02:40:26.960 --> 02:40:31.840]   Yeah. As a teenager, I always thought that the adults, there is something wrong with the adults,
[02:40:31.840 --> 02:40:39.120]   because they don't get it. I would ask my parents or teachers about it, and they, "Oh, yes, you die
[02:40:39.120 --> 02:40:44.720]   in the end. That's it." But on the other hand, they are so worried about other things, like there'll
[02:40:44.720 --> 02:40:50.080]   be a political crisis or an economic problem or a personal problem like with the bank or whatever.
[02:40:50.080 --> 02:40:54.560]   They'll be so worried. But then about the fact that they're going to die, "Ah, we don't care
[02:40:54.560 --> 02:40:54.960]   about it."
[02:40:54.960 --> 02:41:01.120]   That's why you read Camus and others when you're a teenager, you really worry about the existential
[02:41:01.120 --> 02:41:06.320]   questions. Well, this feels like the right time to ask the big question, "What's the meaning of
[02:41:06.320 --> 02:41:11.280]   this whole thing?" And you're the right person to ask. What's the meaning of life? Yes.
[02:41:11.280 --> 02:41:12.480]   The meaning of life? Oh, that's easy.
[02:41:12.480 --> 02:41:12.960]   What is it?
[02:41:12.960 --> 02:41:25.600]   So what life is, if you ask what the meaning of life is, life is feeling things, having sensations,
[02:41:25.600 --> 02:41:32.000]   emotions, and reacting to them. When you feel something good, something pleasant, you want
[02:41:32.000 --> 02:41:37.680]   more out of it. You want more of it. When you feel something unpleasant, you want to get rid of it.
[02:41:37.680 --> 02:41:43.200]   That's the whole of life. That's what is happening all the time. You feel things, you want the
[02:41:43.200 --> 02:41:49.360]   pleasant things to increase, you want the unpleasant things to disappear. That's what life
[02:41:49.360 --> 02:41:56.400]   is. If you ask, "What is the meaning of life?" in a more kind of philosophical or spiritual
[02:41:56.400 --> 02:42:05.120]   question, the real question to ask, "What kind of answer do you expect?" Most people expect a story,
[02:42:05.120 --> 02:42:11.040]   and that's always the wrong answer. Most people expect that the answer to the question,
[02:42:11.040 --> 02:42:17.360]   "What is the meaning of life?" will be a story, like a big drama, that this is the plot line,
[02:42:17.360 --> 02:42:23.600]   and this is your role in the story. This is what you have to do. This is your line in the big play.
[02:42:24.160 --> 02:42:30.960]   You say your line, you do your thing, that's the thing. This is human imagination. This is fantasy.
[02:42:30.960 --> 02:42:38.320]   To really understand life, life is not a story. The universe does not function like a story.
[02:42:38.320 --> 02:42:47.040]   So I think to really understand life, you need to observe it directly in a nonverbal way.
[02:42:47.040 --> 02:42:55.760]   Don't turn it into a story. And the question to start with is, "What is suffering? What is causing
[02:42:55.760 --> 02:43:02.480]   suffering?" The question, "What is the meaning of life?" will take you to fantasies and delusions.
[02:43:02.480 --> 02:43:08.800]   We want to stay with the reality of life. And the most important question about the reality of life
[02:43:08.800 --> 02:43:12.320]   is, "What is suffering, and where is it coming from?"
[02:43:12.960 --> 02:43:17.680]   And to answer that nonverbally, so the conscious experience of suffering.
[02:43:17.680 --> 02:43:25.520]   Yes. When you suffer, try to observe what is really happening when you're suffering.
[02:43:25.520 --> 02:43:35.360]   Well put. And I wonder if AI will also go through that same kind of process.
[02:43:35.360 --> 02:43:40.640]   And if we develop consciousness or not. At present, it's not. It's just words.
[02:43:41.200 --> 02:43:43.680]   It will just say to you, "Please don't hurt me at all."
[02:43:43.680 --> 02:43:51.760]   Again, as I've mentioned to you, I'm a huge fan of yours. Thank you for the incredible work you do.
[02:43:51.760 --> 02:43:59.120]   This conversation has been a long time, I think, coming. It's a huge honor to talk to you.
[02:43:59.120 --> 02:44:01.280]   This was really fun. Thank you for talking today.
[02:44:01.280 --> 02:44:07.120]   Thank you. I really enjoyed it. And as I said, I think the long form is the best form.
[02:44:07.120 --> 02:44:10.400]   Yeah, I loved it. Thank you.
[02:44:11.360 --> 02:44:14.320]   Thanks for listening to this conversation with Yuval Noah Harari.
[02:44:14.320 --> 02:44:18.000]   To support this podcast, please check out our sponsors in the description.
[02:44:18.000 --> 02:44:22.480]   And now, let me leave you with some words from Yuval Noah Harari himself.
[02:44:22.480 --> 02:44:27.920]   How do you cause people to believe in an imagined order such as Christianity,
[02:44:27.920 --> 02:44:34.000]   democracy, or capitalism? First, you never admit that the order is imagined.
[02:44:34.000 --> 02:44:38.400]   Thank you for listening, and hope to see you next time.
[02:44:38.560 --> 02:44:38.640]   Bye.
[02:44:39.600 --> 02:44:39.680]   you
[02:44:40.640 --> 02:44:40.720]   you
[02:44:40.720 --> 02:44:41.220]   you
[02:44:41.220 --> 02:44:43.280]   you
[02:44:43.280 --> 02:44:53.280]   [BLANK_AUDIO]

