
[00:00:00.000 --> 00:00:14.720]   I think that worked.
[00:00:14.720 --> 00:00:21.160]   Let me make sure we're live.
[00:00:21.160 --> 00:00:24.600]   This is a slight delay, but I have to make sure we're live.
[00:00:24.600 --> 00:00:25.600]   Awesome.
[00:00:25.600 --> 00:00:27.040]   Sorry for the minor delay, everyone.
[00:00:27.040 --> 00:00:28.800]   I know I'm three minutes late.
[00:00:28.800 --> 00:00:32.280]   There were some issues with setting up the live stream to YouTube, so I do apologize
[00:00:32.280 --> 00:00:33.280]   for that.
[00:00:33.280 --> 00:00:35.560]   Anyways, I can see that I'm live now.
[00:00:35.560 --> 00:00:41.520]   If my audio and video and slides are looking good, could someone please confirm?
[00:00:41.520 --> 00:00:43.120]   And with that, we'll quickly get started.
[00:00:43.120 --> 00:00:45.920]   Since I'm late, I'm anxious to jump into it.
[00:00:45.920 --> 00:00:48.960]   Welcome back to the fifth session in the Keras.
[00:00:48.960 --> 00:00:49.960]   Keras.
[00:00:49.960 --> 00:00:52.320]   I always mix up the pronunciation.
[00:00:52.320 --> 00:00:53.320]   Sorry.
[00:00:53.320 --> 00:01:01.160]   Welcome back to the fifth part of this reading group.
[00:01:01.160 --> 00:01:08.080]   We're almost at the end of the book because I think this will be the last reading part
[00:01:08.080 --> 00:01:09.680]   where we'll be going through the book.
[00:01:09.680 --> 00:01:14.760]   We'll continue for, I think, two more weeks because I have, as I've been promising everyone,
[00:01:14.760 --> 00:01:19.160]   I have been reaching out to different people that I really want to host to bring more knowledge
[00:01:19.160 --> 00:01:20.160]   to everyone.
[00:01:20.160 --> 00:01:21.880]   Those are being confirmed.
[00:01:21.880 --> 00:01:23.680]   So that's why the dates aren't announced.
[00:01:23.680 --> 00:01:27.460]   We were supposed to host someone today, but I was a bit overwhelmed.
[00:01:27.460 --> 00:01:29.280]   So we didn't get that chance today.
[00:01:29.280 --> 00:01:30.280]   We had to push back.
[00:01:30.280 --> 00:01:34.440]   That date is being scheduled as well.
[00:01:34.440 --> 00:01:35.440]   Someone confirmed.
[00:01:35.440 --> 00:01:36.440]   Yes, all is good.
[00:01:36.440 --> 00:01:37.440]   So thanks for that.
[00:01:37.440 --> 00:01:40.080]   So for today, we'll continue with the book regardless.
[00:01:40.080 --> 00:01:43.240]   And here's what I have planned for the agenda.
[00:01:43.240 --> 00:01:48.480]   So we'll be taking another look at attention, transformers and NLP in Keras.
[00:01:48.480 --> 00:01:56.560]   We'll also further be going through this next chapter after that, which is generative stuff,
[00:01:56.560 --> 00:02:01.320]   broadly speaking, in Keras, which includes GANs, variable autoencoders, and even text
[00:02:01.320 --> 00:02:02.320]   generators.
[00:02:02.320 --> 00:02:07.600]   So we'll be going through that.
[00:02:07.600 --> 00:02:12.400]   This is the, like I mentioned, fifth part in the running series.
[00:02:12.400 --> 00:02:16.480]   So before this, we saw implementing convex and intro to time series.
[00:02:16.480 --> 00:02:20.920]   In the previous session, we also got a very vanilla introduction to attention.
[00:02:20.920 --> 00:02:27.720]   We have seen and learned about convex nets and advanced computer vision.
[00:02:27.720 --> 00:02:30.360]   I keep saying convex because that's happening tomorrow.
[00:02:30.360 --> 00:02:35.040]   I've been reading that paper just to polish my understanding.
[00:02:35.040 --> 00:02:39.960]   And we of course started with the awesome AMA with Franchot, who's kind enough to join
[00:02:39.960 --> 00:02:41.680]   with us.
[00:02:41.680 --> 00:02:47.160]   So for today's agenda, here are the broad topics that I plan to cover attention, transformers
[00:02:47.160 --> 00:02:52.360]   and generative deep learning.
[00:02:52.360 --> 00:02:56.400]   These would be from chapter 11 and 12.
[00:02:56.400 --> 00:03:00.160]   So as I mentioned, I won't be going through chapter 13 and 14.
[00:03:00.160 --> 00:03:05.080]   Throughout this study group, it's been an active thought of how to best share the knowledge
[00:03:05.080 --> 00:03:06.220]   in the book.
[00:03:06.220 --> 00:03:10.520]   Because again, this book is not freely available to everyone.
[00:03:10.520 --> 00:03:16.160]   So we of course have to be very respectful not to cross that line.
[00:03:16.160 --> 00:03:17.680]   So I won't be going through those chapters.
[00:03:17.680 --> 00:03:19.240]   I believe they're really straightforward.
[00:03:19.240 --> 00:03:23.120]   In my preparation, I go through the book at least once or twice.
[00:03:23.120 --> 00:03:24.280]   So I have gone through them.
[00:03:24.280 --> 00:03:25.840]   I invite everyone to read them.
[00:03:25.840 --> 00:03:28.200]   We'll still be meeting next week.
[00:03:28.200 --> 00:03:29.920]   And you're still welcome to ask any questions.
[00:03:29.920 --> 00:03:35.040]   We won't be actively going through chapter 13 and 14 because they're quite self sufficient
[00:03:35.040 --> 00:03:38.960]   and they don't have much to discuss outside.
[00:03:38.960 --> 00:03:46.740]   So and I'll announce a blog competition that we're planning to host around this series.
[00:03:46.740 --> 00:03:52.080]   So I'll announce the people that will be hosting really soon Akash Kumar Nain.
[00:03:52.080 --> 00:03:57.600]   He's a Google Dev expert in machine learning and really one of the Keras heroes to the
[00:03:57.600 --> 00:03:58.600]   broader community.
[00:03:58.600 --> 00:04:03.320]   We'll be hosting him soon again, dates are to be declared.
[00:04:03.320 --> 00:04:06.720]   And Murr from Hugging Face will be joining us as well.
[00:04:06.720 --> 00:04:11.720]   And dates are yet to be finalized.
[00:04:11.720 --> 00:04:13.880]   But we'll be hosting her really soon.
[00:04:13.880 --> 00:04:15.840]   And it won't be on a Saturday.
[00:04:15.840 --> 00:04:20.720]   So I'll keep everyone posted of when we will be hosting her.
[00:04:20.720 --> 00:04:26.220]   This will be a workshop of Hugging Face and Weights and Biases.
[00:04:26.220 --> 00:04:30.520]   So we'll be integrate both of the frameworks, you'll get to learn about NLP and as well
[00:04:30.520 --> 00:04:34.880]   as how can you use Weights and Biases along with Hugging Face.
[00:04:34.880 --> 00:04:37.700]   That is the planned agenda for the talk with Murr.
[00:04:37.700 --> 00:04:43.320]   And for Akash, he'll be speaking potentially about the Keras API.
[00:04:43.320 --> 00:04:49.960]   He writes really well documented code and he writes a extremely Pythonic code that I
[00:04:49.960 --> 00:04:56.680]   don't know anyone, I don't know many people in the field who are at that caliber, especially
[00:04:56.680 --> 00:04:58.000]   in the Keras world.
[00:04:58.000 --> 00:05:02.200]   So we'll get to learn about the Keras API and more around that.
[00:05:02.200 --> 00:05:05.360]   So just wanted to mention these sessions.
[00:05:05.360 --> 00:05:06.740]   Again, these won't be on the Saturdays.
[00:05:06.740 --> 00:05:11.000]   If you've signed up for the group, whenever these are supposed to happen, you'll get an
[00:05:11.000 --> 00:05:14.280]   email just as a reminder if you want to join live.
[00:05:14.280 --> 00:05:16.600]   Or you can always watch the recordings.
[00:05:16.600 --> 00:05:19.480]   These are like all of our sessions live stream to YouTube.
[00:05:19.480 --> 00:05:22.040]   So you can always go back, watch the recording, leave a comment.
[00:05:22.040 --> 00:05:27.880]   We'll try to answer anyone, any questions that you might have.
[00:05:27.880 --> 00:05:32.320]   Here's the recap from the previous session.
[00:05:32.320 --> 00:05:38.320]   So we looked at attention, we looked at convex implementation, and we took a look around
[00:05:38.320 --> 00:05:41.280]   advanced computer vision.
[00:05:41.280 --> 00:05:48.840]   So that included image segmentation, object detection, object classification as well.
[00:05:48.840 --> 00:05:53.960]   All of those things were looked at.
[00:05:53.960 --> 00:05:57.840]   The homework from the previous session, again, as a reminder, was to implement a paper in
[00:05:57.840 --> 00:06:05.320]   Keras, just take an implementation translated to Keras literally, and play with Gradcam,
[00:06:05.320 --> 00:06:07.880]   train a time series RNN or LSTM.
[00:06:07.880 --> 00:06:09.800]   So those are the things we had looked at.
[00:06:09.800 --> 00:06:16.720]   Now we're at the point of mid half, I would say, or at least we've read through the first
[00:06:16.720 --> 00:06:19.920]   quarter of chapter 11.
[00:06:19.920 --> 00:06:23.600]   And for today, we have a short agenda, which is to cover chapter 11 and 12.
[00:06:23.600 --> 00:06:26.360]   I also wanted to cover 13 and 14.
[00:06:26.360 --> 00:06:29.000]   That's what was announced for today.
[00:06:29.000 --> 00:06:34.480]   But after reading through the book, I've learned that there's not much more value that I can
[00:06:34.480 --> 00:06:35.480]   add.
[00:06:35.480 --> 00:06:36.480]   So we'll be skipping through that.
[00:06:36.480 --> 00:06:42.280]   So here's one theoretical concept that I need to introduce to you through this slide.
[00:06:42.280 --> 00:06:49.200]   And then I'll be switching over to Microsoft OneNote with my trusty annotation method to
[00:06:49.200 --> 00:06:52.760]   explain a few more.
[00:06:52.760 --> 00:06:56.200]   One thing we'll be learning in chapter 12 is GANs.
[00:06:56.200 --> 00:07:03.480]   So just as a refresher to anyone who's new to the field, GANs are used to do a lot of
[00:07:03.480 --> 00:07:04.480]   interesting things.
[00:07:04.480 --> 00:07:06.120]   First of all, let me say that.
[00:07:06.120 --> 00:07:11.720]   Let me hop over to this website, which is thispersondoesnotexist.com.
[00:07:11.720 --> 00:07:16.160]   And like the URL says it, this person really does not exist.
[00:07:16.160 --> 00:07:21.200]   So these images are being generated by GANs.
[00:07:21.200 --> 00:07:24.240]   Every time you refresh this website, you see a new person.
[00:07:24.240 --> 00:07:27.000]   And there's an independent discussion of ethics.
[00:07:27.000 --> 00:07:28.000]   And should you do that?
[00:07:28.000 --> 00:07:30.560]   Should you not do that?
[00:07:30.560 --> 00:07:32.600]   That is a really open question.
[00:07:32.600 --> 00:07:34.640]   And we won't be covering that, of course.
[00:07:34.640 --> 00:07:39.440]   That's a very serious discussion that everyone in the field needs to take.
[00:07:39.440 --> 00:07:40.840]   This is one of the applications.
[00:07:40.840 --> 00:07:47.640]   And another one that has recently surfaced, it's called VQGAN+clip.
[00:07:47.640 --> 00:07:49.640]   So this is really cool.
[00:07:49.640 --> 00:07:55.680]   You can put in words that you want to, or a prompt based on which you want to generate
[00:07:55.680 --> 00:07:57.120]   an image.
[00:07:57.120 --> 00:08:01.520]   And this model by itself can generate those images.
[00:08:01.520 --> 00:08:06.440]   And it's, I don't want to do any injustice by trying to describe it.
[00:08:06.440 --> 00:08:10.360]   But let me see if I can quickly find an example.
[00:08:10.360 --> 00:08:19.080]   So this, I think this might be good for that.
[00:08:19.080 --> 00:08:20.840]   I don't think so.
[00:08:20.840 --> 00:08:28.200]   Let's try the second link.
[00:08:28.200 --> 00:08:36.540]   I was trying to find a link where we can find any interesting generated image just to get
[00:08:36.540 --> 00:08:37.540]   my point across.
[00:08:37.540 --> 00:08:38.840]   Let's try this blog.
[00:08:38.840 --> 00:08:43.040]   So yep, this looks like an output from a VQGAN.
[00:08:43.040 --> 00:08:49.080]   So the author, I believe Angus would have put in geometric glass city from future at
[00:08:49.080 --> 00:08:52.960]   dusk, just with this input.
[00:08:52.960 --> 00:08:58.840]   And throwing that into a collab, there's an open collab available if you just Google VQGAN+clip.
[00:08:58.840 --> 00:09:00.520]   You can generate an image like so.
[00:09:00.520 --> 00:09:07.360]   I don't know what fascinates, what could potentially fascinate anyone more.
[00:09:07.360 --> 00:09:11.240]   Just apart from just showing this to them.
[00:09:11.240 --> 00:09:21.520]   So all of this, I would say broadly comes under GANs or generative deep learning's umbrella.
[00:09:21.520 --> 00:09:29.920]   I'm just quickly catching up on the question, then I'll come back to that.
[00:09:29.920 --> 00:09:31.440]   There's a question, where is the group?
[00:09:31.440 --> 00:09:35.700]   So if you go to our Twitter, we regularly mention this out.
[00:09:35.700 --> 00:09:37.820]   You can sign up through that link.
[00:09:37.820 --> 00:09:40.380]   Is this deep learning?
[00:09:40.380 --> 00:09:41.460]   Which book is this?
[00:09:41.460 --> 00:09:46.500]   This is the deep learning with Python group to answer your question.
[00:09:46.500 --> 00:09:50.580]   It's this one by Manning publication.
[00:09:50.580 --> 00:09:53.140]   It's by Francois Chollet and it covers Keras.
[00:09:53.140 --> 00:09:58.380]   We're almost at the end of the group, but you're still welcome to join us and ask any
[00:09:58.380 --> 00:10:02.460]   questions.
[00:10:02.460 --> 00:10:04.300]   You could not find any other videos.
[00:10:04.300 --> 00:10:12.620]   I'll show that real quickly as well.
[00:10:12.620 --> 00:10:19.940]   I usually get to questions towards the break or whenever we try to take a break, but these
[00:10:19.940 --> 00:10:25.420]   seem to be these questions require addressing right now.
[00:10:25.420 --> 00:10:26.420]   So I'll do that.
[00:10:26.420 --> 00:10:34.620]   If you go to this playlist, you should be able to see all of the videos in the playlist.
[00:10:34.620 --> 00:10:36.580]   So let me put that link in the YouTube chart.
[00:10:36.580 --> 00:10:39.300]   And again, you can find it on the Weights and Biases YouTube channel.
[00:10:39.300 --> 00:10:40.620]   There you go.
[00:10:40.620 --> 00:10:43.740]   There's the link in the YouTube chart.
[00:10:43.740 --> 00:10:47.080]   Again, you can find these on our YouTube page as well.
[00:10:47.080 --> 00:10:48.660]   So what is a GAN?
[00:10:48.660 --> 00:10:56.680]   I just showcase to at least to me fascinating applications of it.
[00:10:56.680 --> 00:11:04.760]   Inside of a GAN, you have these two independent models that are fighting against each other.
[00:11:04.760 --> 00:11:12.360]   So there's a generator and its mission is to generate an image that is as good looking
[00:11:12.360 --> 00:11:14.320]   as it can be.
[00:11:14.320 --> 00:11:23.960]   And its task is to fool this detective or discriminator which wants to tell if it's
[00:11:23.960 --> 00:11:26.360]   real or fake.
[00:11:26.360 --> 00:11:31.260]   So generator generates a fake image that should fool the discriminator into thinking this
[00:11:31.260 --> 00:11:33.600]   is a real image.
[00:11:33.600 --> 00:11:37.160]   And then both of these models improve upon each other.
[00:11:37.160 --> 00:11:43.760]   They keep improving based on each other's outputs to continuously give us better and
[00:11:43.760 --> 00:11:44.760]   better looking images.
[00:11:44.760 --> 00:11:50.280]   That's the foundational theory behind GANs.
[00:11:50.280 --> 00:11:55.880]   And let me also switch sharing and share my OneNote screen.
[00:11:55.880 --> 00:12:03.000]   I'm always a bit anxious when I'm doing this because it takes me a second to find the right
[00:12:03.000 --> 00:12:04.000]   screen.
[00:12:04.000 --> 00:12:05.400]   I always have too many things open.
[00:12:05.400 --> 00:12:08.240]   Let's see if I can pick the right one.
[00:12:08.240 --> 00:12:09.240]   I think I did.
[00:12:09.240 --> 00:12:10.240]   Awesome.
[00:12:10.240 --> 00:12:12.880]   So you can see me reading a paper.
[00:12:12.880 --> 00:12:14.600]   We have a paper reading group happening tomorrow.
[00:12:14.600 --> 00:12:17.600]   So I'm just prepping for that.
[00:12:17.600 --> 00:12:25.280]   The next concept I want to quickly highlight is variable autoencoders.
[00:12:25.280 --> 00:12:32.400]   So these work on one concept, one key concept, which is having a latent space.
[00:12:32.400 --> 00:12:40.480]   So inside of a latent space, you would have many images of a face, let's say.
[00:12:40.480 --> 00:12:44.160]   And these would have some level of a smile.
[00:12:44.160 --> 00:12:50.560]   So let's say inside of a latent vector, you're trying to find different smiles or trying
[00:12:50.560 --> 00:12:58.260]   to adjust the level of smiles.
[00:12:58.260 --> 00:13:03.920]   So whenever you see those animations going on, where you can see a face morphing into
[00:13:03.920 --> 00:13:11.240]   different things or the face changing its appearance with getting more and more emphasis,
[00:13:11.240 --> 00:13:15.000]   that's being looked up inside of a latent space.
[00:13:15.000 --> 00:13:21.080]   And variable autoencoders have worked strongly based on that concept.
[00:13:21.080 --> 00:13:26.400]   I'm trying to find the right page to make sure I can cover that.
[00:13:26.400 --> 00:13:38.480]   So you have a, broadly speaking, you have an input image that goes into an encoder,
[00:13:38.480 --> 00:13:50.880]   from which you pick out a compressed representation that goes through a decoder and you get an
[00:13:50.880 --> 00:13:52.280]   output.
[00:13:52.280 --> 00:14:03.400]   This is the broad, higher level theoretical, I want to say, overview of how these work.
[00:14:03.400 --> 00:14:09.920]   What's really happening is inside of a VAE, again, I'm directly taking this from the book.
[00:14:09.920 --> 00:14:12.520]   So I'm looking at chapter 12.
[00:14:12.520 --> 00:14:23.480]   You have an input image that goes through an encoder, which randomly samples a point
[00:14:23.480 --> 00:14:25.600]   over a latent space.
[00:14:25.600 --> 00:14:29.340]   So remember this space that I had defined above.
[00:14:29.340 --> 00:14:36.760]   So let's say you want to pull a randomly sampled image from here.
[00:14:36.760 --> 00:14:48.360]   You sample a random point that will go through the decoder and give you an output.
[00:14:48.360 --> 00:14:55.420]   So this by definition also becomes a compressed representation of sorts of the image.
[00:14:55.420 --> 00:15:04.920]   This at a very meta level or a broad overview level is how variable autoencoders work.
[00:15:04.920 --> 00:15:12.240]   So so far I've covered two concepts, which are GANs and variable autoencoders.
[00:15:12.240 --> 00:15:14.680]   Those are the theoretical concepts I wanted to cover.
[00:15:14.680 --> 00:15:18.040]   We were also looking at attention last week.
[00:15:18.040 --> 00:15:25.880]   So I'll come back to attention and we'll try to quickly cover that as well right now.
[00:15:25.880 --> 00:15:28.960]   Here's what we were looking at last week.
[00:15:28.960 --> 00:15:34.680]   So when I mentioned, can I have your attention, please?
[00:15:34.680 --> 00:15:36.240]   Maybe you're not looking at your screen almost.
[00:15:36.240 --> 00:15:40.880]   I never I find it really difficult to look at the screen while watching YouTube videos.
[00:15:40.880 --> 00:15:46.640]   I always pull out my phone and start watching TikToks.
[00:15:46.640 --> 00:15:51.380]   But when I said that phrase, I did get your attention.
[00:15:51.380 --> 00:15:57.960]   So the question then becomes how can we take this concept and bring it over to neural network?
[00:15:57.960 --> 00:16:02.960]   That is the question that attention module broadly answers.
[00:16:02.960 --> 00:16:10.800]   So let's say you have a set of words.
[00:16:10.800 --> 00:16:18.800]   And let's say you were generating a sentence with chai.
[00:16:18.800 --> 00:16:25.280]   So when you're doing that, the challenge with a lot of the older sequence to sequence model
[00:16:25.280 --> 00:16:32.480]   becomes how do you give this context awareness?
[00:16:32.480 --> 00:16:42.080]   Which means how do you associate chai with me or Kaggle or data science?
[00:16:42.080 --> 00:16:43.080]   Right?
[00:16:43.080 --> 00:16:46.720]   Chai could also just be a beverage.
[00:16:46.720 --> 00:16:49.440]   So it could just be a drink right?
[00:16:49.440 --> 00:16:53.880]   Now you get to see my exceptional drawing skills at this point.
[00:16:53.880 --> 00:17:01.880]   But how do you give this context awareness to the model, broadly speaking?
[00:17:01.880 --> 00:17:07.000]   So there are three ways of how this is addressed.
[00:17:07.000 --> 00:17:19.200]   You have three things that the model is looking at which is query, key and value.
[00:17:19.200 --> 00:17:24.740]   So the model broadly speaking, this is again just to explain things there's more nuances
[00:17:24.740 --> 00:17:25.800]   to it.
[00:17:25.800 --> 00:17:35.920]   The model would query a database about chai and from there it will pull out key and value
[00:17:35.920 --> 00:17:37.560]   pairs.
[00:17:37.560 --> 00:17:47.200]   So let's say you have an x and y axis where this is chai and this is beverage.
[00:17:47.200 --> 00:17:55.520]   So these are absolutely perpendicular which means they have a zero component over z axis.
[00:17:55.520 --> 00:17:59.360]   So this is chai and this is beverage.
[00:17:59.360 --> 00:18:08.020]   Since these are totally perpendicular, they would have a zero overlap in the y axis.
[00:18:08.020 --> 00:18:15.200]   But in the case of, I don't know, Saiyam, he would probably lie here.
[00:18:15.200 --> 00:18:20.380]   So when you do a dot product, you get some z component.
[00:18:20.380 --> 00:18:28.120]   So these key value pairs actually have an overlap where one of these is chai and one
[00:18:28.120 --> 00:18:31.080]   of these is Saiyam.
[00:18:31.080 --> 00:18:38.280]   Whenever you represent any words, we've looked at this while we were working on word embeddings.
[00:18:38.280 --> 00:18:44.020]   But whenever you represent information in neural networks, these are stored in high
[00:18:44.020 --> 00:18:47.380]   dimensional spaces.
[00:18:47.380 --> 00:18:49.020]   So this is a 3D representation.
[00:18:49.020 --> 00:18:54.420]   I learned this through Yannick Kilcher's incredible video called, basically called
[00:18:54.420 --> 00:18:57.500]   what the paper is called, "Attention is all you need, paper explained", something like
[00:18:57.500 --> 00:18:59.380]   that.
[00:18:59.380 --> 00:19:01.580]   He showcased this concept.
[00:19:01.580 --> 00:19:06.380]   So through dot producting key and value, you get this component through which you can understand
[00:19:06.380 --> 00:19:09.500]   how much importance to give different things.
[00:19:09.500 --> 00:19:14.900]   So using this concept, we have achieved something incredible, which is we are now able to give
[00:19:14.900 --> 00:19:19.140]   context awareness to our model about chai.
[00:19:19.140 --> 00:19:25.520]   So based on how these are represented, we can tell the model, no, this is not a beverage.
[00:19:25.520 --> 00:19:31.660]   This is associated with these three words.
[00:19:31.660 --> 00:19:39.220]   And we are able to pay attention to different words accordingly.
[00:19:39.220 --> 00:19:42.660]   So that is how the attention module at a very broad level works.
[00:19:42.660 --> 00:19:47.500]   I'm again, I'll remind everyone to please keep the questions coming while I switch my
[00:19:47.500 --> 00:19:48.500]   screens.
[00:19:48.500 --> 00:19:52.460]   So let me find the correct screen again.
[00:19:52.460 --> 00:19:56.860]   And at this point, we are also good to look at sequence to sequence model.
[00:19:56.860 --> 00:20:02.900]   So I'll hop over to my blog because I wrote a blog summary of this.
[00:20:02.900 --> 00:20:16.540]   Let me search for sequence to sequence, which doesn't bring anything, sadly speaking.
[00:20:16.540 --> 00:20:22.780]   So now I'll have to scroll because I've managed to break the search feature on my blog post.
[00:20:22.780 --> 00:20:26.940]   Absolutely incredible time for it not to not work, but the audience is usually forgiving
[00:20:26.940 --> 00:20:27.940]   to me.
[00:20:27.940 --> 00:20:32.620]   So this is a summary of the original sequence to sequence learning with neural networks
[00:20:32.620 --> 00:20:37.300]   paper.
[00:20:37.300 --> 00:20:44.980]   I had done this series of summaries on my blog where I would cover one paper, I think,
[00:20:44.980 --> 00:20:47.320]   once every week, twice every week, I forget.
[00:20:47.320 --> 00:20:49.500]   So I'll drop the link again in the YouTube chat.
[00:20:49.500 --> 00:20:51.420]   But again, you don't have to go through this.
[00:20:51.420 --> 00:20:54.260]   I'll quickly summarize whatever is important from here.
[00:20:54.260 --> 00:20:58.620]   This is covering the original sequence to sequence learning with neural network paper
[00:20:58.620 --> 00:21:01.740]   by Ilya Satsukevour.
[00:21:01.740 --> 00:21:06.740]   I think they're currently at OpenAI, if I'm not too wrong.
[00:21:06.740 --> 00:21:13.340]   So this was the first paper, or first pioneer paper to show that neural networks can be
[00:21:13.340 --> 00:21:20.580]   used to do end to end translation on sentences.
[00:21:20.580 --> 00:21:25.280]   If you remember, and if you joined us last week, this is basically what Jay Alamur's
[00:21:25.280 --> 00:21:27.980]   blog post showcased to us as well.
[00:21:27.980 --> 00:21:32.380]   So how do we perform neural machine translation using a sequence to sequence model?
[00:21:32.380 --> 00:21:34.680]   Now we're peeling the onion another layer.
[00:21:34.680 --> 00:21:39.300]   And we're looking at sequence to sequence models, what the hell are even these things?
[00:21:39.300 --> 00:21:42.940]   I always miss up the tab.
[00:21:42.940 --> 00:21:47.220]   So let's come back to that.
[00:21:47.220 --> 00:21:50.780]   And again, even in the blog, as I mentioned, the paper is from 2014.
[00:21:50.780 --> 00:21:56.860]   So there's, there's no interesting, not too much interesting key details to highlight.
[00:21:56.860 --> 00:22:01.260]   This was one of the pioneers and there was no tensorflow or PyTorch at that time.
[00:22:01.260 --> 00:22:07.100]   So a lot of the things mentioned there were, how do you fit on a 6 GB GPU?
[00:22:07.100 --> 00:22:09.300]   Using this magic, you can work on that.
[00:22:09.300 --> 00:22:14.420]   And that was really incredible to see because right now even Colab has, I hope GPUs have
[00:22:14.420 --> 00:22:19.220]   a larger capacity than that.
[00:22:19.220 --> 00:22:25.380]   So there are two LSTM networks inside of this.
[00:22:25.380 --> 00:22:26.880]   Last week we had looked at LSTM.
[00:22:26.880 --> 00:22:30.180]   So I hope everyone understands what those are.
[00:22:30.180 --> 00:22:33.340]   The first one acts as an encoder.
[00:22:33.340 --> 00:22:38.660]   It takes your inputs and maps them to a dimension vector.
[00:22:38.660 --> 00:22:46.980]   And then you have another set of LSTM networks, which from the vector map it to an output.
[00:22:46.980 --> 00:22:51.880]   So how that becomes an encoder and the other one becomes a decoder.
[00:22:51.880 --> 00:22:58.540]   The mission of the LSTM is to predict the conditional probability of a target sequence.
[00:22:58.540 --> 00:23:01.100]   Let's break this down.
[00:23:01.100 --> 00:23:07.620]   So we were just looking at probability of Siam being associated with Chai.
[00:23:07.620 --> 00:23:11.660]   Here the probability should be really high.
[00:23:11.660 --> 00:23:19.460]   But the problem is if you have trained on a very broad set of rules, it will also be
[00:23:19.460 --> 00:23:23.260]   high for Chai and beverage.
[00:23:23.260 --> 00:23:28.180]   So how do you distinguish between these and how do you make the model pay attention to
[00:23:28.180 --> 00:23:29.180]   that?
[00:23:29.180 --> 00:23:30.180]   That's what attention solves.
[00:23:30.180 --> 00:23:37.300]   With LSTMs, that context awareness isn't very wide spanning and isn't always so domain specific
[00:23:37.300 --> 00:23:39.660]   also sometimes.
[00:23:39.660 --> 00:23:45.020]   That is somewhat addressed in the other approach.
[00:23:45.020 --> 00:23:48.140]   This paper has a very deep neural network of four layers.
[00:23:48.140 --> 00:23:49.660]   That's how they call it in the paper.
[00:23:49.660 --> 00:23:53.340]   I actually smiled when I read that.
[00:23:53.340 --> 00:24:00.140]   One of the tricks to make making it work again, you don't even realize this most of the times.
[00:24:00.140 --> 00:24:06.300]   But whenever you're sending an input, you would want to reverse the sequence of inputs
[00:24:06.300 --> 00:24:14.100]   so that the distance between the first output and input is smallest.
[00:24:14.100 --> 00:24:21.940]   Because if I'm sending a sentence, the first word needs to get translated first.
[00:24:21.940 --> 00:24:28.780]   And the easiest way to reduce the distance between the first input and first output is
[00:24:28.780 --> 00:24:32.200]   to reverse the input sentence.
[00:24:32.200 --> 00:24:35.100]   So these become really close now.
[00:24:35.100 --> 00:24:42.060]   If you completely reverse the input sentence, the last word comes first and first word comes
[00:24:42.060 --> 00:24:49.060]   last, which means it's closer to the output.
[00:24:49.060 --> 00:24:54.540]   So these are the tricks of making sequence to sequence model work.
[00:24:54.540 --> 00:25:01.860]   Again, that is covered inside of this chapter.
[00:25:01.860 --> 00:25:07.020]   And you can use an LSTM model here inside of sequence to sequence models.
[00:25:07.020 --> 00:25:08.700]   We had always also looked at it.
[00:25:08.700 --> 00:25:11.060]   You can use an RNN model.
[00:25:11.060 --> 00:25:30.780]   And you can also use a tension module here or a transformer module.
[00:25:30.780 --> 00:25:37.060]   I wanted to also mention quickly, you should also check out JR Lemmer's illustrated transformer
[00:25:37.060 --> 00:25:41.020]   blog for better understanding.
[00:25:41.020 --> 00:25:45.460]   Here's another paper that I would suggest everyone to check out.
[00:25:45.460 --> 00:25:51.240]   It's called visualizing attention in transformer based language representation modules.
[00:25:51.240 --> 00:25:55.980]   It's by J. C. Wigg.
[00:25:55.980 --> 00:26:00.180]   But the thing we want to understand here is how the heck does this thing called attention
[00:26:00.180 --> 00:26:05.460]   work so that we can then move on to implementing it in Keras.
[00:26:05.460 --> 00:26:07.780]   So this gives us the best view.
[00:26:07.780 --> 00:26:10.900]   This is a neuron view of GPT-2.
[00:26:10.900 --> 00:26:13.700]   So slightly older module.
[00:26:13.700 --> 00:26:23.620]   For one of the later layers, positive values are blue and negative values are orange.
[00:26:23.620 --> 00:26:33.740]   So the more orange or stronger shade of orange means there's a positive weight being given.
[00:26:33.740 --> 00:26:39.500]   And the more blue it is, sorry, the more blue it is, the more positive weight and the more
[00:26:39.500 --> 00:26:44.740]   orange it is, the more negative weight it's being given to that.
[00:26:44.740 --> 00:26:51.820]   So now we have this super nice representation where we can query and multiply that with
[00:26:51.820 --> 00:26:52.820]   keys.
[00:26:52.820 --> 00:26:58.740]   So for all of these values, we get a positive or negative output.
[00:26:58.740 --> 00:27:04.780]   Basically by multiplying these two sets of matrices, having these words, we can then
[00:27:04.780 --> 00:27:09.540]   see which would be the likeliest output here.
[00:27:09.540 --> 00:27:11.740]   But now there's like another question.
[00:27:11.740 --> 00:27:14.420]   How do you pick from these values?
[00:27:14.420 --> 00:27:15.420]   Right?
[00:27:15.420 --> 00:27:20.220]   So I'll also request you all to answer this in the chart.
[00:27:20.220 --> 00:27:26.760]   Once you have these outputs, how would you pick which one to predict from here?
[00:27:26.760 --> 00:27:34.700]   So our mission is to generate a sentence and you have the set of query values and key values.
[00:27:34.700 --> 00:27:37.740]   Let's say you get all of these as somewhat positive.
[00:27:37.740 --> 00:28:00.860]   So how do you pick which value to produce from here?
[00:28:00.860 --> 00:28:30.540]   I'll just wait for any, any suggestion to come in.
[00:28:30.540 --> 00:28:31.820]   I don't see any charts coming in.
[00:28:31.820 --> 00:28:34.820]   So I'll avoid the awkward pause.
[00:28:34.820 --> 00:28:35.820]   I'll continue.
[00:28:35.820 --> 00:28:40.060]   Let me quickly refresh to make sure.
[00:28:40.060 --> 00:28:44.680]   There's always this weird dance that happens inside of my head where I'm anxiously waiting
[00:28:44.680 --> 00:28:47.940]   for replies and trying not to waste anyone's time.
[00:28:47.940 --> 00:28:49.500]   So okay, I don't see any questions.
[00:28:49.500 --> 00:28:50.740]   I'll continue regardless.
[00:28:50.740 --> 00:28:54.640]   My question was, how could you possibly pick the output from here?
[00:28:54.640 --> 00:28:58.860]   So you have these query and key values that give you suggestions for which values could
[00:28:58.860 --> 00:29:01.220]   probably fit in the sentence.
[00:29:01.220 --> 00:29:04.460]   And you get a bunch of positive responses from here, right?
[00:29:04.460 --> 00:29:13.600]   Now either you could pick the most positive suggestion from here.
[00:29:13.600 --> 00:29:17.180]   But there's a problem with that.
[00:29:17.180 --> 00:29:21.760]   You always end up getting stuck in a prediction loop.
[00:29:21.760 --> 00:29:25.460]   So most of the times I would recommend everyone to try this.
[00:29:25.460 --> 00:29:29.900]   If you follow that approach, your model would be predicting the same word over and over
[00:29:29.900 --> 00:29:30.900]   and over again.
[00:29:30.900 --> 00:29:33.780]   And it becomes quite broken.
[00:29:33.780 --> 00:29:35.620]   Sometimes it starts predicting.
[00:29:35.620 --> 00:29:40.520]   I think for one of the large language models I was reading somewhere, it was just predicting
[00:29:40.520 --> 00:29:45.140]   slashes because they were apparently a few hundred thousand slashes inside of the input
[00:29:45.140 --> 00:29:48.100]   data set.
[00:29:48.100 --> 00:29:55.420]   The other approach could be, if you look at these, you could randomly pick above a
[00:29:55.420 --> 00:29:56.420]   certain threshold.
[00:29:56.420 --> 00:29:57.420]   That's totally possible.
[00:29:57.420 --> 00:30:07.420]   Or you could again, think of another heuristic using which you can pick these.
[00:30:07.420 --> 00:30:15.020]   So now I think we have covered all of the ground basics for chapter 11 and 12.
[00:30:15.020 --> 00:30:17.100]   I think we've covered sequence to sequence model.
[00:30:17.100 --> 00:30:20.040]   What is attention that covered chapter 11.
[00:30:20.040 --> 00:30:23.140]   And we've looked at generative deep learning for chapter 12.
[00:30:23.140 --> 00:30:28.300]   So now we'll hop over to code walkthrough of all of these parts.
[00:30:28.300 --> 00:30:41.180]   So I think we've gone through chapter 11, but I'll quickly skim through these as well.
[00:30:41.180 --> 00:30:45.700]   One of the things we've learned is how to vectorize the text.
[00:30:45.700 --> 00:30:52.220]   So this is covered inside of the vectorization, text vectorization layer that we create.
[00:30:52.220 --> 00:30:58.260]   And for that, we have to define a class vectorizer, which will first of all lower all of the text
[00:30:58.260 --> 00:31:00.420]   and join the characters.
[00:31:00.420 --> 00:31:08.180]   So from there, we'll tokenize all of the text and we'll split them.
[00:31:08.180 --> 00:31:11.580]   And using that we can then create a vocab.
[00:31:11.580 --> 00:31:14.020]   And how do we define the vocab size?
[00:31:14.020 --> 00:31:17.540]   You could just pick the top 10,000 used words.
[00:31:17.540 --> 00:31:19.340]   That's one of the approaches.
[00:31:19.340 --> 00:31:24.780]   But what this will lead to is you'll have a certain set of dictionary elements that
[00:31:24.780 --> 00:31:29.340]   will have a number and associated word with it.
[00:31:29.340 --> 00:31:33.780]   So let's say if you have a text sentence, I write, rewrite and still rewrite again,
[00:31:33.780 --> 00:31:39.140]   you would have all of these numbers connected with every single word.
[00:31:39.140 --> 00:31:47.940]   So here we've managed to create a mapping for every word with a number.
[00:31:47.940 --> 00:31:53.700]   You could also do this for individual characters, as you would imagine that would eat up a lot
[00:31:53.700 --> 00:31:55.500]   of your memory.
[00:31:55.500 --> 00:32:07.100]   So this is why I think NLP models eat up more memory compared to broadly speaking, computer
[00:32:07.100 --> 00:32:12.460]   vision models, because these was as you can see it up a lot of memory.
[00:32:12.460 --> 00:32:17.460]   And in that case, you'll also have a lot of unknown predictions in the case of using character
[00:32:17.460 --> 00:32:21.740]   level tokenization, you might get a lot of unknowns.
[00:32:21.740 --> 00:32:30.940]   If you create a threshold, for example, if you say, I just want the top N thousand characters,
[00:32:30.940 --> 00:32:34.180]   you might get a lot of unknowns in those outputs.
[00:32:34.180 --> 00:32:41.300]   So this is one of the ways where you can create the vocab and map them like so.
[00:32:41.300 --> 00:32:45.200]   There are two approaches to representing groups.
[00:32:45.200 --> 00:32:47.020]   These could be sets and sequences.
[00:32:47.020 --> 00:32:50.340]   So you could create a set or a dictionary.
[00:32:50.340 --> 00:32:54.700]   We had covered this in the previous session, I'm just giving a very quick refresher.
[00:32:54.700 --> 00:32:59.700]   We had looked at the IMDB data set where you were tasked to predict positive and negative
[00:32:59.700 --> 00:33:00.460]   movie reviews.
[00:33:00.460 --> 00:33:11.860]   So we looked at processing words using the back of words approach, where we just use
[00:33:11.860 --> 00:33:15.300]   unigrams with binary encoding.
[00:33:15.300 --> 00:33:19.340]   And then we had inspected these outputs.
[00:33:19.340 --> 00:33:25.660]   From there, we had also looked at using bigrams.
[00:33:25.660 --> 00:33:31.140]   And last week, we had already gone through this, I'm just reminding everyone.
[00:33:31.140 --> 00:33:38.900]   We had looked at bigrams with TF-IDF.
[00:33:38.900 --> 00:33:47.080]   And from there, we had started looking at using sequence to sequence models.
[00:33:47.080 --> 00:33:56.600]   So this is the traditional way of creating NLP models, which is using these n-gram models
[00:33:56.600 --> 00:34:03.280]   or TF-IDF weighted outputs for vectorization.
[00:34:03.280 --> 00:34:05.840]   That is one way of generating outputs.
[00:34:05.840 --> 00:34:09.720]   The other way is throwing in a neural network somewhere in here.
[00:34:09.720 --> 00:34:12.820]   And how do you decide which model to use?
[00:34:12.820 --> 00:34:18.340]   So I believe Franchois linked off a study where his team, I believe the team that's
[00:34:18.340 --> 00:34:21.980]   working on Keras would have intensively studied all of these.
[00:34:21.980 --> 00:34:27.980]   And a certain rule that they had suggested was if you have more than 15,000 words in
[00:34:27.980 --> 00:34:34.980]   your training data set, you should just use a neural network based approach.
[00:34:34.980 --> 00:34:39.340]   If it's smaller than that, a back of words approach performs better.
[00:34:39.340 --> 00:34:45.140]   This is not to say that if you just download a pre-trained NLP model from somewhere, it
[00:34:45.140 --> 00:34:46.540]   won't perform well.
[00:34:46.540 --> 00:34:55.500]   You can totally do that as well.
[00:34:55.500 --> 00:34:57.980]   Let me close this tab.
[00:34:57.980 --> 00:35:06.700]   And we can continue looking at sequence to sequence models.
[00:35:06.700 --> 00:35:10.620]   So again, we define the helper functions to load in the data set.
[00:35:10.620 --> 00:35:13.420]   We're still looking at the IMDB data set.
[00:35:13.420 --> 00:35:22.520]   We set up the train and test split, which would be 20% that goes into 80% that goes
[00:35:22.520 --> 00:35:26.720]   into the training data set and the rest 20% is divided.
[00:35:26.720 --> 00:35:32.020]   From there, we import the text vectorization layer.
[00:35:32.020 --> 00:35:49.100]   Let's see what that does.
[00:35:49.100 --> 00:35:51.100]   We're taking a look at what does this layer do.
[00:35:51.100 --> 00:35:55.540]   A pre-processing layer which maps text features to integer sequence.
[00:35:55.540 --> 00:36:00.140]   So the thing we had done manually earlier by defining those two classes, I would have
[00:36:00.140 --> 00:36:02.700]   shown the collab but I closed that link.
[00:36:02.700 --> 00:36:04.940]   And now I remember I shouldn't have done that.
[00:36:04.940 --> 00:36:07.940]   This layer takes care of that and it takes all of these inputs.
[00:36:07.940 --> 00:36:09.300]   Let's see what inputs are we passing.
[00:36:09.300 --> 00:36:13.540]   We're passing max tokens, output mode and output sequence length.
[00:36:13.540 --> 00:36:16.140]   So let's see what do these two.
[00:36:16.140 --> 00:36:19.440]   Max tokens is the maximum size of vocab for this layer.
[00:36:19.440 --> 00:36:22.440]   That makes sense.
[00:36:22.440 --> 00:36:28.140]   Output mode is the specification where it can be integers, multi-hot encoded, count
[00:36:28.140 --> 00:36:32.140]   or tf-idf, all of these things we had manually defined.
[00:36:32.140 --> 00:36:33.980]   That's one thing I really love about this book.
[00:36:33.980 --> 00:36:40.580]   First of all, you hand define things and then you bring in characters, make your life easy.
[00:36:40.580 --> 00:36:43.580]   And I believe this was the last input.
[00:36:43.580 --> 00:36:45.940]   Yep.
[00:36:45.940 --> 00:36:54.060]   So we pass an output sequence length, which is the output will have its dimension exactly
[00:36:54.060 --> 00:36:55.220]   matched to this.
[00:36:55.220 --> 00:36:59.980]   And if the model predicts a smaller output, it will be padded.
[00:36:59.980 --> 00:37:03.420]   Or if it's longer than this, it will be truncated to this length.
[00:37:03.420 --> 00:37:09.640]   So if you want a particular answer of a given length, you could, from me, you could either
[00:37:09.640 --> 00:37:15.020]   make me shut up if I'm continuously speaking, or you could add some awkward silence.
[00:37:15.020 --> 00:37:20.220]   That's how this model works.
[00:37:20.220 --> 00:37:26.020]   Instead of here, we'll call this layer and set all of these values, we're setting the
[00:37:26.020 --> 00:37:34.140]   max tokens to 20,000, maximum length of generated output to be 600 words.
[00:37:34.140 --> 00:37:39.940]   And then we create a mapping for training, data set validation data set and test data
[00:37:39.940 --> 00:37:40.940]   set.
[00:37:40.940 --> 00:37:47.900]   Pro tip here, if you set this to minus one, this will actually use up all of your CPU
[00:37:47.900 --> 00:37:48.900]   cores.
[00:37:48.900 --> 00:37:52.060]   So I have a 28 core CPU.
[00:37:52.060 --> 00:37:55.020]   And if you use this, it will actually use up all of the threads.
[00:37:55.020 --> 00:37:59.260]   If you use four, it will just use two cores of your CPU because those would have four
[00:37:59.260 --> 00:38:01.860]   threads.
[00:38:01.860 --> 00:38:08.660]   So now we're looking at a sequence model built on one hot encoded vector sequences.
[00:38:08.660 --> 00:38:18.740]   To do that, we create a one hot encoding that takes in the inputs and the depth, which would
[00:38:18.740 --> 00:38:22.100]   be the maximum number of tokens that we want to predict.
[00:38:22.100 --> 00:38:25.420]   And here we define a bidirectional LSTM.
[00:38:25.420 --> 00:38:29.860]   Because we just looked at a sequence to sequence model and we've learned these are bidirectional
[00:38:29.860 --> 00:38:30.860]   LSTMs.
[00:38:30.860 --> 00:38:33.860]   We'll add a dropout of 50%.
[00:38:33.860 --> 00:38:37.300]   I have seen dropout values of 0.8 as well.
[00:38:37.300 --> 00:38:45.460]   So if you're surprised by this, well, there are bigger things to some bigger numbers used
[00:38:45.460 --> 00:38:46.460]   as well.
[00:38:46.460 --> 00:38:48.340]   I see someone mentioning what book is this?
[00:38:48.340 --> 00:38:51.660]   I'll quickly mention again, I've mentioned this a few times.
[00:38:51.660 --> 00:38:54.900]   This is the book we're looking at, Deep Learning with Python by Francois Joliot.
[00:38:54.900 --> 00:38:58.500]   We're almost at the end of the book and you can find the other videos on our channel inside
[00:38:58.500 --> 00:39:03.940]   of a playlist.
[00:39:03.940 --> 00:39:11.340]   From there, we create a dense output and now we can create a model and compile it.
[00:39:11.340 --> 00:39:14.220]   This is still training.
[00:39:14.220 --> 00:39:18.460]   But from here we can train our basic sequence model.
[00:39:18.460 --> 00:39:27.500]   To do that, we'll just have to pass one callback, which is to checkpoint the model with this
[00:39:27.500 --> 00:39:34.380]   particular name and now we can fit it to the training and validation data set, which we
[00:39:34.380 --> 00:39:42.460]   had defined by creating a text vectorizer with a maximum token limit of 20,000 and a
[00:39:42.460 --> 00:39:44.980]   maximum length of 600.
[00:39:44.980 --> 00:39:48.540]   So this was our first sequence to sequence models.
[00:39:48.540 --> 00:39:56.300]   Later, we also learn about how to create an embedding layer.
[00:39:56.300 --> 00:40:00.020]   And we had also learned how to pad and mask the outputs.
[00:40:00.020 --> 00:40:02.100]   I'll skip over these details.
[00:40:02.100 --> 00:40:08.420]   I think these are quite straightforward.
[00:40:08.420 --> 00:40:13.620]   And now we're good to hop on to the transformer train.
[00:40:13.620 --> 00:40:18.780]   So again, I would encourage everyone to read these parts, but I have read ahead of time
[00:40:18.780 --> 00:40:22.540]   and these are quite straightforward to understand and we'd also covered this in the previous
[00:40:22.540 --> 00:40:23.540]   week.
[00:40:23.540 --> 00:40:27.980]   So that is the reason why I'm skipping this, not to keep things moving fast.
[00:40:27.980 --> 00:40:28.980]   That's not the reason.
[00:40:28.980 --> 00:40:35.660]   I don't want to repeat anything for anyone who's joining us for the second time.
[00:40:35.660 --> 00:40:40.220]   So now I've explained the transformer architecture real quickly to everyone.
[00:40:40.220 --> 00:40:46.460]   This is the part where we define a sequence to sequence model for the transformer architecture.
[00:40:46.460 --> 00:40:49.460]   So again, we download the exact same data set.
[00:40:49.460 --> 00:40:56.380]   The idea is to compare accuracies against all of these models.
[00:40:56.380 --> 00:40:59.900]   But really what we're doing here is trying to understand the concept, right?
[00:40:59.900 --> 00:41:05.100]   By definition or just by being on Twitter, you would know transformer models would perform
[00:41:05.100 --> 00:41:09.180]   really, really, really better on these things.
[00:41:09.180 --> 00:41:13.260]   But what we're trying to again, do here is understand how do these work.
[00:41:13.260 --> 00:41:17.460]   So please pay attention to that and note the accuracy values.
[00:41:17.460 --> 00:41:18.660]   So we do the exact same thing.
[00:41:18.660 --> 00:41:23.460]   We prepare the data with the exact same split as earlier validation would be 20% of the
[00:41:23.460 --> 00:41:24.460]   data.
[00:41:24.460 --> 00:41:30.300]   I think even the seed here is the same as earlier for shuffling all of the files.
[00:41:30.300 --> 00:41:32.820]   We split the data like so.
[00:41:32.820 --> 00:41:38.140]   We vectorize the data in the exact same fashion as we had done earlier.
[00:41:38.140 --> 00:41:43.180]   And now we'll implement the transformer encoder.
[00:41:43.180 --> 00:41:45.900]   So transformers are still a sequence to sequence model.
[00:41:45.900 --> 00:41:53.180]   The only difference then becomes we had looked about attention and how does that work?
[00:41:53.180 --> 00:41:58.060]   That goes into both the encoder and decoder parts of the model.
[00:41:58.060 --> 00:42:02.840]   So sequence to sequence models always have an encoder and decoder.
[00:42:02.840 --> 00:42:09.060]   These could be RNN, LSTM, a group based model, a transformer.
[00:42:09.060 --> 00:42:12.980]   Maybe you're watching this in six months and we have a totally different thing come out.
[00:42:12.980 --> 00:42:14.840]   It could be that as well.
[00:42:14.840 --> 00:42:19.100]   Sequence to sequence models always have an encoder and decoder.
[00:42:19.100 --> 00:42:24.900]   So in this case, we'll have a transformer encoder and we'll implement that by subclassing
[00:42:24.900 --> 00:42:27.820]   the layer class.
[00:42:27.820 --> 00:42:32.740]   So we inherit the layer slot layer class from Keras.
[00:42:32.740 --> 00:42:35.620]   You imported it here.
[00:42:35.620 --> 00:42:44.740]   And we define the embedding dimension, the dense dimension, the number of heads.
[00:42:44.740 --> 00:42:51.580]   And the dense proj layer would be a sequential model of a dense layer followed by another
[00:42:51.580 --> 00:42:54.100]   dense layer.
[00:42:54.100 --> 00:42:56.260]   And after that, we'll add a layer norm.
[00:42:56.260 --> 00:42:57.380]   Why are we doing this?
[00:42:57.380 --> 00:43:04.300]   Because this is how it's suggested to implement an encoder layer inside of a transformer.
[00:43:04.300 --> 00:43:10.540]   If you read the original paper, I believe they had used a layer norm as a normalizer
[00:43:10.540 --> 00:43:11.540]   instead of batch norm.
[00:43:11.540 --> 00:43:15.000]   I think that helps train better.
[00:43:15.000 --> 00:43:20.500]   If you were feeling adventurous, you should switch this out for batch normalization here
[00:43:20.500 --> 00:43:22.580]   and tell us how that performs.
[00:43:22.580 --> 00:43:27.700]   As a reminder, you have to put your model in evaluation mode if you're working with
[00:43:27.700 --> 00:43:31.780]   batch normalization and trying to get outputs from there.
[00:43:31.780 --> 00:43:37.700]   So now we're good to define the call function inside of the model.
[00:43:37.700 --> 00:43:47.020]   And if we don't have a mask, it'll quickly, sorry, I'm getting confused.
[00:43:47.020 --> 00:43:55.380]   If there is a mask value, it will map it like so.
[00:43:55.380 --> 00:44:04.820]   So the new axis gets added to the mask value.
[00:44:04.820 --> 00:44:11.500]   And from there, the attention output will call the attention module and you'll pass
[00:44:11.500 --> 00:44:16.500]   the inputs and attention mask here.
[00:44:16.500 --> 00:44:23.020]   And you'll run this through the first layer norm layer and the dense branch layer.
[00:44:23.020 --> 00:44:26.740]   And from there, you can return the second output.
[00:44:26.740 --> 00:44:32.920]   Furthermore, you can define the config where you'll have the embedding dimension, number
[00:44:32.920 --> 00:44:37.580]   of heads and dense dimension, and you can return this.
[00:44:37.580 --> 00:44:42.140]   So how do we use this for text classification?
[00:44:42.140 --> 00:44:48.860]   Now since we've already done the heavy lifting, we can quickly define an embedding layer and
[00:44:48.860 --> 00:44:52.660]   call a transformer encoder on top of that.
[00:44:52.660 --> 00:44:57.620]   So we call an input layer which gets captured inside of input.
[00:44:57.620 --> 00:45:03.500]   We pass this input to an embedding layer and we pass these embeddings to the transformer
[00:45:03.500 --> 00:45:05.400]   encoder.
[00:45:05.400 --> 00:45:09.340]   From there, we just add a global max pooling and a drop out of 50%.
[00:45:09.340 --> 00:45:12.740]   Actually, it's even higher sometimes.
[00:45:12.740 --> 00:45:15.820]   We pass this through a dense layer.
[00:45:15.820 --> 00:45:19.900]   And now we can create a model and train it.
[00:45:19.900 --> 00:45:25.500]   So this is a transformer encoder being used for text classification.
[00:45:25.500 --> 00:45:30.740]   Sorry, I want to look at the chat and instead of switching windows, I always hop over to
[00:45:30.740 --> 00:45:31.740]   my presentation.
[00:45:31.740 --> 00:45:35.740]   Okay, I don't see any messages.
[00:45:35.740 --> 00:45:36.740]   I'll continue.
[00:45:36.740 --> 00:45:44.740]   As a reminder, please keep asking questions in the chat if you want.
[00:45:44.740 --> 00:45:49.660]   So now we're good to evaluate this as well.
[00:45:49.660 --> 00:45:55.860]   To do that, we'll fit the model again to all of these data sets we've defined earlier,
[00:45:55.860 --> 00:46:01.140]   which would be the integer mappings of the training data set, the validation data set
[00:46:01.140 --> 00:46:07.140]   and train it for 20 epochs.
[00:46:07.140 --> 00:46:08.820]   And I'll let this continue for a bit.
[00:46:08.820 --> 00:46:11.100]   I should have used a GPU module.
[00:46:11.100 --> 00:46:12.940]   Colab is not being nice to me these days.
[00:46:12.940 --> 00:46:14.540]   It doesn't give me GPU access.
[00:46:14.540 --> 00:46:19.540]   Google overlords, if you're listening, please, please send more GPUs.
[00:46:19.540 --> 00:46:25.140]   But if you use a accelerator, this would train faster.
[00:46:25.140 --> 00:46:29.900]   So you can also use a positional encoding to reject order information.
[00:46:29.900 --> 00:46:33.580]   What does that mean?
[00:46:33.580 --> 00:46:41.060]   You could once you're training the module, you need to provide context to it again.
[00:46:41.060 --> 00:46:49.860]   So if you've trained a positional encoding, that can reorder things inside of the module
[00:46:49.860 --> 00:46:57.620]   to tell it, hey, look at this word, or bring attention to a specific word.
[00:46:57.620 --> 00:47:06.020]   So for that, we define a positional embedding class, which is again subclass from layer,
[00:47:06.020 --> 00:47:12.120]   which will take in the sequence length, input dimension and output dimension.
[00:47:12.120 --> 00:47:19.860]   So the token embeddings are a simple embedding having the dimensions defined like so.
[00:47:19.860 --> 00:47:25.460]   And from there, we can pick up the position embeddings.
[00:47:25.460 --> 00:47:28.620]   And these would become the inputs that go into the model.
[00:47:28.620 --> 00:47:34.860]   So again, the input dimension, sequence length and output dimension are being passed here.
[00:47:34.860 --> 00:47:44.420]   For the model call, we'll define the length as the length of inputs.
[00:47:44.420 --> 00:47:50.900]   The positions could be in a range of the entire sequence.
[00:47:50.900 --> 00:47:56.580]   And the embedded tokens then get picked from the token embeddings.
[00:47:56.580 --> 00:48:03.660]   And positions would be picked from the position embedding.
[00:48:03.660 --> 00:48:08.540]   We can then compute the mask and return the configuration.
[00:48:08.540 --> 00:48:14.100]   So using all of these, we can now create a text classification transformer, which will
[00:48:14.100 --> 00:48:15.500]   just call all of these layers.
[00:48:15.500 --> 00:48:21.980]   So the only thing we've switched here is we've called positional embedding, and then transformer
[00:48:21.980 --> 00:48:26.440]   encoder and the definition remains similar to earlier.
[00:48:26.440 --> 00:48:34.420]   So now we can use this transformer based approach to do classification.
[00:48:34.420 --> 00:48:40.540]   And I've used my knowledge from the book to cover this section, which answers the question
[00:48:40.540 --> 00:48:43.240]   when to use a sequence model over a bag of words.
[00:48:43.240 --> 00:48:48.980]   I believe inside of the book, the 15,000 word mark is the suggested limit.
[00:48:48.980 --> 00:49:03.540]   So let me take a sip of water and check the chart real quick.
[00:49:03.540 --> 00:49:09.500]   Awesome.
[00:49:09.500 --> 00:49:10.500]   I don't see any questions.
[00:49:10.500 --> 00:49:33.380]   I'll just continue in one second.
[00:49:33.380 --> 00:49:43.300]   I think we're now at the point where we can also tackle the one last mission in NLP models
[00:49:43.300 --> 00:49:44.300]   covered in the book.
[00:49:44.300 --> 00:49:49.160]   There are more things that happen outside of it, which is machine translation.
[00:49:49.160 --> 00:49:53.460]   So for that, we'll download a Spanish to English data set.
[00:49:53.460 --> 00:49:59.260]   We'll open that file, read it in, vectorize the input and output pairs.
[00:49:59.260 --> 00:50:07.060]   And again, we'll call it X vectorization layer to map the source and target vectors here.
[00:50:07.060 --> 00:50:13.780]   We'll create those pairs, prepare our data sets for translation.
[00:50:13.780 --> 00:50:16.500]   And first we'll use the group based encoder.
[00:50:16.500 --> 00:50:19.300]   So remember that model we had looked at last week?
[00:50:19.300 --> 00:50:20.940]   Don't forget about it just yet.
[00:50:20.940 --> 00:50:22.180]   It's still in play.
[00:50:22.180 --> 00:50:25.820]   So look at the group based encoder first.
[00:50:25.820 --> 00:50:27.380]   That is quite straightforward.
[00:50:27.380 --> 00:50:35.380]   You just define an embedding layer followed by a group layer and you're good to go.
[00:50:35.380 --> 00:50:41.620]   So you take the input inside of source, which will take the shape, the data type and it'll
[00:50:41.620 --> 00:50:43.660]   be in English.
[00:50:43.660 --> 00:50:50.220]   From there you map this to embeddings, which would take your vocab size, embedding dimension
[00:50:50.220 --> 00:50:52.340]   and the input.
[00:50:52.340 --> 00:51:03.060]   We then take this input or this embedding layer and pass it to a bidirectional group
[00:51:03.060 --> 00:51:12.540]   model having the dimensions of the latent dimension.
[00:51:12.540 --> 00:51:19.740]   We can then create an end to end model based on this.
[00:51:19.740 --> 00:51:25.180]   So we take the input, pass it to a decoded group.
[00:51:25.180 --> 00:51:31.660]   So this time we're taking an input and trying to produce a translated sentence.
[00:51:31.660 --> 00:51:38.620]   We again add a dropout of 50% followed by a dense layer.
[00:51:38.620 --> 00:51:45.140]   And then this becomes a seek to seek RNN module, which you can then compile and train.
[00:51:45.140 --> 00:51:54.940]   So we have two group layers, one that works as an encoder, another that works as a decoder.
[00:51:54.940 --> 00:52:04.220]   Combining both of these, we are able to translate a model from Spanish to English.
[00:52:04.220 --> 00:52:11.980]   Next we look at how can we use transformers for the same.
[00:52:11.980 --> 00:52:17.300]   I'm skipping over this part where you translate new sentences.
[00:52:17.300 --> 00:52:20.820]   But now we're excited to apply attention here.
[00:52:20.820 --> 00:52:27.620]   So to do that, we make our lives easy by calling multi head attention this time.
[00:52:27.620 --> 00:52:33.740]   So we define two attention layers apart from embedding dimensions, dense dimensions and
[00:52:33.740 --> 00:52:36.340]   number of heads.
[00:52:36.340 --> 00:52:41.180]   And we create another sequential layer, which will have just a bunch of dense layers, followed
[00:52:41.180 --> 00:52:52.660]   by which we normalize and add the normalization layers like so.
[00:52:52.660 --> 00:52:57.940]   So in the call here, we'll have to make a few adjustments because now we're working
[00:52:57.940 --> 00:52:59.720]   with attention.
[00:52:59.720 --> 00:53:07.180]   So we'll need to define Q, K and V. We had looked at these and these were query, key
[00:53:07.180 --> 00:53:09.900]   and value pairs.
[00:53:09.900 --> 00:53:13.440]   So we'll define them like so.
[00:53:13.440 --> 00:53:17.860]   And we'll just make them equal to inputs.
[00:53:17.860 --> 00:53:22.060]   Inside of the call, we'll define a casual mask, which will be a casual attention mask
[00:53:22.060 --> 00:53:26.500]   over the inputs.
[00:53:26.500 --> 00:53:35.820]   And from there, we'll create a padding and grab the TF minimum value from padding mask
[00:53:35.820 --> 00:53:38.260]   and casual mask.
[00:53:38.260 --> 00:53:44.460]   From there, we'll try to grab the first output of attention, which is self attention one.
[00:53:44.460 --> 00:53:48.900]   This will take in the casual mask.
[00:53:48.900 --> 00:53:55.140]   And then we can normalize this output by adding it to the inputs.
[00:53:55.140 --> 00:54:06.380]   And from there, we can now use or generate the second output by passing again, the output
[00:54:06.380 --> 00:54:10.260]   from the first model as the query.
[00:54:10.260 --> 00:54:15.180]   The value becomes the encoder outputs and same for the keys.
[00:54:15.180 --> 00:54:21.540]   And then the attention is just padding mask this time.
[00:54:21.540 --> 00:54:29.780]   So by doing all of this, we can now grab the attention output to after normalizing it by
[00:54:29.780 --> 00:54:33.000]   concatenating both of these.
[00:54:33.000 --> 00:54:37.820]   If you haven't read the paper or watch the previous live stream, we looked at the architecture.
[00:54:37.820 --> 00:54:54.500]   I am assuming everyone knows this architecture.
[00:54:54.500 --> 00:54:56.500]   It's also drawn inside of the book chapter.
[00:54:56.500 --> 00:55:00.980]   So if you're reading the book, you don't need to go through this paper, but I'll give it
[00:55:00.980 --> 00:55:07.820]   a second to load.
[00:55:07.820 --> 00:55:11.460]   Why are we doing all of these weird concatenation and different things?
[00:55:11.460 --> 00:55:17.260]   If you actually look at this transformer model architecture, things would start to make sense
[00:55:17.260 --> 00:55:18.940]   through that.
[00:55:18.940 --> 00:55:23.380]   So that's why all of these steps are being found.
[00:55:23.380 --> 00:55:27.160]   These are actually defined in this original paper and this architecture.
[00:55:27.160 --> 00:55:31.380]   There are hundreds of resources on there and also the previous live stream where we had
[00:55:31.380 --> 00:55:34.700]   covered this.
[00:55:34.700 --> 00:55:39.740]   So now we need a positional embedding layer.
[00:55:39.740 --> 00:55:44.580]   As you can see, inside of this, there is an input that goes to input embeddings, which
[00:55:44.580 --> 00:55:51.780]   are positionally encode and there's an output which has its own output embedding and a positional
[00:55:51.780 --> 00:55:53.820]   encoding layer.
[00:55:53.820 --> 00:56:01.820]   So we define the positional embedding layer and now this becomes an end to end transformer
[00:56:01.820 --> 00:56:10.300]   where we can grab the encoder outputs through the transformer encoder and we do the same
[00:56:10.300 --> 00:56:14.160]   to grab the transformer decoder outputs.
[00:56:14.160 --> 00:56:19.580]   And finally, we create a transformer model by combining both of these and we're ready
[00:56:19.580 --> 00:56:24.540]   to train this model.
[00:56:24.540 --> 00:56:31.140]   Then following the same approach as earlier, we can translate sentences or new sentences
[00:56:31.140 --> 00:56:34.180]   with our transformer model.
[00:56:34.180 --> 00:56:38.340]   So that I think was the last concept I wanted to cover in chapter 11.
[00:56:38.340 --> 00:56:59.300]   Again, let me take a peek at the questions.
[00:56:59.300 --> 00:57:00.620]   Amazing I don't see any questions.
[00:57:00.620 --> 00:57:02.620]   So I'll continue.
[00:57:02.620 --> 00:57:10.340]   Chapter 12 talks about generative deep learning, as I mentioned, broadly speaking.
[00:57:10.340 --> 00:57:14.700]   So one of these, if you look at the Colab is text generation.
[00:57:14.700 --> 00:57:22.420]   The other one is disconnected, but it's a disconnected deep dream.
[00:57:22.420 --> 00:57:28.780]   And you have neural style transfer followed by variable autoencoders and GANs.
[00:57:28.780 --> 00:57:33.740]   So again, a lot of things to cover, but again, the chapter is quite brief as well.
[00:57:33.740 --> 00:57:36.820]   And the idea is to give you a broad taste of all of these things.
[00:57:36.820 --> 00:57:40.420]   So I'll try to do justice to all of these concepts.
[00:57:40.420 --> 00:57:47.180]   Text generation by itself has become this incredible field revolutionized by GPT-3,
[00:57:47.180 --> 00:57:57.580]   Copilot, all of these have attention and transformers working at the base layer.
[00:57:57.580 --> 00:58:04.580]   So inside of this, again, you will have to create a sequence to sequence model.
[00:58:04.580 --> 00:58:08.940]   But this time, the mission of the model changes.
[00:58:08.940 --> 00:58:10.940]   Sorry.
[00:58:10.940 --> 00:58:20.180]   We are trying to generate text out of the model.
[00:58:20.180 --> 00:58:29.220]   So for that, the main thing that changes is we'll have to define a text generation callback.
[00:58:29.220 --> 00:58:35.980]   And I believe we, I think inside of the book, we have a pseudocode that first does it in
[00:58:35.980 --> 00:58:43.060]   NumPy and then we use the power of Keras.
[00:58:43.060 --> 00:58:51.100]   So to do that, we define a text generator class.
[00:58:51.100 --> 00:58:57.140]   To do that, we define a text generator class, which will take in a prompt, a length that
[00:58:57.140 --> 00:59:04.620]   we want to generate, the input length, temperature and printing frequency.
[00:59:04.620 --> 00:59:10.180]   Printing frequency is a convenience for understanding how is the model being trained.
[00:59:10.180 --> 00:59:16.220]   And now we get introduced with this concept called temperature, which basically defines
[00:59:16.220 --> 00:59:21.580]   how interesting of outputs in your model throughout.
[00:59:21.580 --> 00:59:31.820]   So when you're working with generator models, a higher temperature gives it more creativity.
[00:59:31.820 --> 00:59:39.080]   And a lower temperature gives boring sets of outputs that are always repeated, not as
[00:59:39.080 --> 00:59:42.540]   interesting to look at.
[00:59:42.540 --> 00:59:49.900]   And most of the times they end up getting stuck in the loop.
[00:59:49.900 --> 00:59:56.920]   So now we can, inside of a text gen callback, simply pass in the text generator.
[00:59:56.920 --> 01:00:00.860]   And we can train the model by just using that one callback.
[01:00:00.860 --> 01:00:07.340]   So the key thing that changed here is we've had to define this temperature class, which
[01:00:07.340 --> 01:00:12.260]   will determine how much creative can our model be.
[01:00:12.260 --> 01:00:18.620]   And again, inside of the book, they actually investigate how do these different values
[01:00:18.620 --> 01:00:19.980]   affect the output.
[01:00:19.980 --> 01:00:24.940]   I would encourage you to just download any model, any pre-trained model and actually
[01:00:24.940 --> 01:00:33.460]   look at how does the temperature affect the different outputs that come into play here.
[01:00:33.460 --> 01:00:39.460]   So that's it, I think that's the key part that I wanted to cover in text generation.
[01:00:39.460 --> 01:00:46.340]   Please increase font size, I apologize, let me make sure it's zoomed in this time.
[01:00:46.340 --> 01:00:55.500]   Again, if you're new here, these are from the books repository, word book, it's the
[01:00:55.500 --> 01:01:00.820]   book by Francio Achiolle for the third time today.
[01:01:00.820 --> 01:01:10.180]   So our deep dream is this concept of making very trippy images that came out in 2015.
[01:01:10.180 --> 01:01:12.100]   Let's see if this model got to an output.
[01:01:12.100 --> 01:01:14.100]   No, I don't think it did.
[01:01:14.100 --> 01:01:20.980]   So I won't get to showcase the output here.
[01:01:20.980 --> 01:01:23.380]   I didn't want to do that.
[01:01:23.380 --> 01:01:31.980]   I wanted to go to a new tab and look up deep dream.
[01:01:31.980 --> 01:01:35.500]   We can take a look here.
[01:01:35.500 --> 01:01:45.080]   So deep dream creates these psychedelic images like so that have a lot of eyes and dog like
[01:01:45.080 --> 01:01:52.780]   features in them because dogs are over represented in ImageNet.
[01:01:52.780 --> 01:01:55.900]   This could be one possible output.
[01:01:55.900 --> 01:01:59.540]   And these are done by doing something known as gradient ascent.
[01:01:59.540 --> 01:02:06.020]   So now we're in the opposite territory, exact opposite territory of training models.
[01:02:06.020 --> 01:02:18.500]   With here you try to maximize the activation of entire layers rather than that of filters.
[01:02:18.500 --> 01:02:25.700]   So that allows you to mix visualizations and create these really trippy images.
[01:02:25.700 --> 01:02:29.140]   Again I'm just reading that off of the book.
[01:02:29.140 --> 01:02:34.820]   And I'm also looking at the chapter just trying to make sure if there are any things that
[01:02:34.820 --> 01:02:38.300]   I want to emphasize on.
[01:02:38.300 --> 01:02:44.380]   So here we define or first of all we import inception v3 that will form the backbone of
[01:02:44.380 --> 01:02:45.900]   our mission here.
[01:02:45.900 --> 01:02:51.980]   We'll download this image which was the ghost image that would have gone now since I managed
[01:02:51.980 --> 01:02:54.940]   to reload this image.
[01:02:54.940 --> 01:02:58.940]   Also I'm hoping to whoever commented that this font size is good enough for you.
[01:02:58.940 --> 01:03:01.860]   If it's not I'll again zoom in a bit more.
[01:03:01.860 --> 01:03:04.900]   So here we'll define the layer settings.
[01:03:04.900 --> 01:03:09.260]   These are the layers for which we are trying to maximize the activations.
[01:03:09.260 --> 01:03:13.740]   So mixed 4, mixed 5, mixed 6 and mixed 7.
[01:03:13.740 --> 01:03:21.060]   And we can tweak these values to play around with the visuals quite a bit.
[01:03:21.060 --> 01:03:29.060]   This would be the symbolic outputs and the feature extractor is the model that returns
[01:03:29.060 --> 01:03:36.180]   the activation values for all of the targets.
[01:03:36.180 --> 01:03:40.660]   Here we'll have to define the compute loss like so.
[01:03:40.660 --> 01:03:50.580]   So we'll first of all extract the activations and initialize the loss to zero.
[01:03:50.580 --> 01:03:54.980]   There's one problem if you don't set this correctly which is you'll have artifacts if
[01:03:54.980 --> 01:04:01.700]   you pick along the edges your outputs will start to look like they're from an old PC
[01:04:01.700 --> 01:04:04.580]   that would have these bars coming across.
[01:04:04.580 --> 01:04:17.260]   So to avoid that you only take the images from the middle of the inputs I want to say.
[01:04:17.260 --> 01:04:25.500]   And now we define the gradient ascent process.
[01:04:25.500 --> 01:04:32.700]   So inside of this we'll compute the gradients of deep dream loss with respect to current
[01:04:32.700 --> 01:04:46.860]   image inside of here.
[01:04:46.860 --> 01:04:55.540]   And then we normalize the gradients by calling L2 normalize.
[01:04:55.540 --> 01:05:06.680]   From there we can define a gradient ascent loop which runs this for an entire octave
[01:05:06.680 --> 01:05:12.320]   inside of an image.
[01:05:12.320 --> 01:05:21.100]   And we repeatedly update inside of this loop.
[01:05:21.100 --> 01:05:34.580]   We repeatedly try to minimize the deep dream loss which we had defined just above.
[01:05:34.580 --> 01:05:45.180]   And we break out of this loop if the loss crosses a certain threshold.
[01:05:45.180 --> 01:05:53.460]   So just using these few steps we have defined the gradient ascent loop and the oops I didn't
[01:05:53.460 --> 01:05:59.580]   want to do that gradient ascent loss.
[01:05:59.580 --> 01:06:06.700]   From there we can define some utilities to pre-process the images.
[01:06:06.700 --> 01:06:09.820]   These are quite straightforward so again I'll jump over this.
[01:06:09.820 --> 01:06:18.300]   The next thing we want to do is run this algorithm over successive octaves inside of the image
[01:06:18.300 --> 01:06:21.660]   to generate those trippy outputs.
[01:06:21.660 --> 01:06:28.180]   So we define all of these functions here which take care of these.
[01:06:28.180 --> 01:06:31.580]   Again the code here is quite annotated.
[01:06:31.580 --> 01:06:38.260]   So the key thing to take away inside of deep dream is we define gradient ascent and we
[01:06:38.260 --> 01:06:43.580]   compute the losses over these extracted features.
[01:06:43.580 --> 01:06:49.380]   So first of all we extract the features and then we define a way to compute the deep dream
[01:06:49.380 --> 01:06:56.760]   loss over the feature extractor or the extracted features.
[01:06:56.760 --> 01:07:04.100]   And then we define the gradient ascent process which then allows us to pull out features
[01:07:04.100 --> 01:07:19.700]   from the images and create these super trippy visualizations.
[01:07:19.700 --> 01:07:25.420]   I was hoping I had run this notebook but apparently it has crashed as well.
[01:07:25.420 --> 01:07:38.460]   So let me try to reconnect it and see if I can get it to train real quick.
[01:07:38.460 --> 01:07:50.580]   And of course the other notebook would have disconnected as well.
[01:07:50.580 --> 01:07:57.140]   The next thing we are trying to learn here is apply neural style transfer.
[01:07:57.140 --> 01:08:04.980]   That is a fancy way of saying can we use an existing style of an image and just apply
[01:08:04.980 --> 01:08:07.260]   it to another image.
[01:08:07.260 --> 01:08:10.580]   So can we make an image animated?
[01:08:10.580 --> 01:08:13.580]   Can we make it trippy looking?
[01:08:13.580 --> 01:08:20.540]   Can we just take style of a painter and throw it at another image to make it look like that?
[01:08:20.540 --> 01:08:26.140]   And if yes, how do we do that?
[01:08:26.140 --> 01:08:31.900]   I was hoping I had printed these somewhere but I think I lost that since I didn't save
[01:08:31.900 --> 01:08:32.900]   it.
[01:08:32.900 --> 01:08:40.060]   So the thing we are looking at is we are downloading an image of I think San Francisco and we are
[01:08:40.060 --> 01:08:47.500]   applying the Vincent van Gogh starry night over top of it.
[01:08:47.500 --> 01:08:54.340]   So how do we achieve this process of neural style transfer?
[01:08:54.340 --> 01:09:02.900]   The main thing here is you have a reference image over which you are applying a certain
[01:09:02.900 --> 01:09:04.660]   style.
[01:09:04.660 --> 01:09:17.940]   So that should tell you that if you simply define a loss function that can find a way
[01:09:17.940 --> 01:09:27.120]   to maximize, sorry, loss should be minimized but the resemblance should be maximized.
[01:09:27.120 --> 01:09:33.040]   So we will have two loss components here which will look at the transferred image and also
[01:09:33.040 --> 01:09:37.820]   the original style being applied.
[01:09:37.820 --> 01:09:43.460]   And that is done in the original implementation with the VGG19 backbone.
[01:09:43.460 --> 01:09:48.480]   You are welcome to try with other feature extractors as well.
[01:09:48.480 --> 01:09:57.120]   So use a VGG19 image net backbone and we grab the feature extractor from here.
[01:09:57.120 --> 01:10:10.700]   So now we have two losses which are the content loss and style loss.
[01:10:10.700 --> 01:10:23.100]   So the final loss that we want to minimize becomes a combination of all of these.
[01:10:23.100 --> 01:10:36.680]   So the loss would be the sum of content weight and content loss and style loss would contain
[01:10:36.680 --> 01:10:39.840]   both of these.
[01:10:39.840 --> 01:10:50.020]   And finally the loss becomes a sum of total variation weight multiplied by the total variation
[01:10:50.020 --> 01:10:59.180]   loss over the combination of images and that gets concatenated across all of the input
[01:10:59.180 --> 01:11:04.220]   tensors over which we are trying to compute the loss function.
[01:11:04.220 --> 01:11:13.540]   And then we can simply iterate over this to minimize a combination of both of style losses
[01:11:13.540 --> 01:11:15.800]   and content losses.
[01:11:15.800 --> 01:11:24.440]   So to be able to effectively apply a style to an image we define two different loss components
[01:11:24.440 --> 01:11:32.900]   and we try to define a combination of both of these that allows us to work on these style
[01:11:32.900 --> 01:11:35.640]   transfer applications.
[01:11:35.640 --> 01:11:41.660]   And by minimizing that we are telling the model to bring it closer to style to the original
[01:11:41.660 --> 01:11:49.500]   image and also making sure that it remains comprehensive or it doesn't totally change
[01:11:49.500 --> 01:11:55.680]   the image.
[01:11:55.680 --> 01:12:03.760]   That was an extremely quick rundown of neural style transfer.
[01:12:03.760 --> 01:12:08.180]   The next thing inside of the book is a VAE.
[01:12:08.180 --> 01:12:13.720]   I think this is really well annotated in the book and I really don't have anything to add
[01:12:13.720 --> 01:12:21.600]   here apart from the theoretical definition that I had done and I would say the same for
[01:12:21.600 --> 01:12:23.240]   GANs.
[01:12:23.240 --> 01:12:29.840]   So again I could run through this implementation but what really happens is I have to cover
[01:12:29.840 --> 01:12:35.720]   or recap these minute details inside of these small sessions where we meet every week and
[01:12:35.720 --> 01:12:39.260]   I'm always in this internal debate of trying to understand what should be covered and what
[01:12:39.260 --> 01:12:41.780]   should be skipped.
[01:12:41.780 --> 01:12:47.380]   So the devil is in the details for a lot of these things and GANs are really frustratingly
[01:12:47.380 --> 01:12:50.020]   hard to train.
[01:12:50.020 --> 01:12:54.400]   I would encourage you to really sit down with the implementation and play through it especially
[01:12:54.400 --> 01:12:57.800]   for chapter 12 where I've run through all of the details.
[01:12:57.800 --> 01:13:03.680]   You really understand those by playing around with different values and seeing how those
[01:13:03.680 --> 01:13:04.680]   affect that.
[01:13:04.680 --> 01:13:09.120]   Sometimes your model just wouldn't train because you would forget this one silly thing that
[01:13:09.120 --> 01:13:15.080]   really makes your life easy or super hard at times if you forget it.
[01:13:15.080 --> 01:13:20.120]   And these are the tricks that lie in the implementation details.
[01:13:20.120 --> 01:13:23.960]   So please spend some time going through these collabs.
[01:13:23.960 --> 01:13:31.560]   I don't think I can do justice to them but these are quite short and yet have a lot of
[01:13:31.560 --> 01:13:35.480]   details packed into them.
[01:13:35.480 --> 01:13:41.560]   So for now we've come to the end of the book because we're skipping over chapter 13 and
[01:13:41.560 --> 01:13:42.560]   14.
[01:13:42.560 --> 01:13:47.720]   But like I mentioned earlier, we'll be having at least three more sessions or at least two
[01:13:47.720 --> 01:13:49.400]   more sessions.
[01:13:49.400 --> 01:13:55.960]   I also want to announce a Keras competition today where I invite everyone to submit a
[01:13:55.960 --> 01:13:59.080]   Keras blog or implementation.
[01:13:59.080 --> 01:14:06.840]   The only way to submit is just share your blog on Twitter and use the hashtag #27DaysofKeras
[01:14:06.840 --> 01:14:13.160]   or simply write a blog and tag @SynbiasisOrMe.
[01:14:13.160 --> 01:14:18.760]   The blog could also be for anyone who's been participating earlier if you just explain
[01:14:18.760 --> 01:14:20.880]   any concept at all.
[01:14:20.880 --> 01:14:26.800]   And from today's session, today was very rushed or maybe an earlier session, those are welcomed
[01:14:26.800 --> 01:14:27.800]   as well.
[01:14:27.800 --> 01:14:32.600]   But the idea is your blog should add value.
[01:14:32.600 --> 01:14:38.440]   And for any blog posts that are interesting enough, next week I'll announce the winners
[01:14:38.440 --> 01:14:40.640]   and invite them to present their solution.
[01:14:40.640 --> 01:14:47.280]   The winners will be getting swag and GPU credits if the submission really stands out.
[01:14:47.280 --> 01:14:51.680]   Again we can decide on these details.
[01:14:51.680 --> 01:14:55.760]   We'll announce these details once we have the submissions coming in.
[01:14:55.760 --> 01:15:04.080]   But this is again really a way for weights and biases to encourage all of you to really
[01:15:04.080 --> 01:15:10.560]   have a go at Keras and make sure you get started in your journey of sharing your learnings.
[01:15:10.560 --> 01:15:16.140]   Once you're done with all of this, and hopefully you've read through the book, you should go
[01:15:16.140 --> 01:15:18.280]   back and watch the AMA.
[01:15:18.280 --> 01:15:25.080]   I've learned with really successful people, they give out so much information in one minute
[01:15:25.080 --> 01:15:28.960]   that just listening to it multiple times really helps you.
[01:15:28.960 --> 01:15:34.400]   Personally, I've gone back to the podcast that I have recorded the episodes many times
[01:15:34.400 --> 01:15:37.040]   and learned many new things.
[01:15:37.040 --> 01:15:38.240]   Same for fast day lectures.
[01:15:38.240 --> 01:15:43.280]   I believe there was a lot of depth to the questions answered by Francois and I would
[01:15:43.280 --> 01:15:46.160]   request you to go back to that.
[01:15:46.160 --> 01:15:49.240]   So now we've come to an end of the book.
[01:15:49.240 --> 01:15:52.560]   But I think we're just starting our Keras journey.
[01:15:52.560 --> 01:15:58.720]   If at all, anything that I have covered would just leave you in some confusion and you'll
[01:15:58.720 --> 01:16:02.160]   be spending more time just learning this stuff.
[01:16:02.160 --> 01:16:08.840]   We'll still meet, not on Saturday for the workshops I mentioned, but we'll be hosting
[01:16:08.840 --> 01:16:15.200]   a Hugging Face workshop with Moor who is absolutely incredible.
[01:16:15.200 --> 01:16:20.560]   And we'll also be hosting Akash Nain hopefully to learn about the Keras API.
[01:16:20.560 --> 01:16:30.400]   I have another person from the Keras world, whom I'll announce real soon if they confirm.
[01:16:30.400 --> 01:16:36.240]   So we have two or three sessions planned and I want to invite everyone to write blog posts
[01:16:36.240 --> 01:16:38.400]   as a part of a competition.
[01:16:38.400 --> 01:16:44.680]   And the best submissions will be invited next week just to present what they have learned.
[01:16:44.680 --> 01:16:50.720]   And of course, to give you recognition, we'll be sending swag over to you or even credits
[01:16:50.720 --> 01:16:55.240]   if the submission is really exceptional.
[01:16:55.240 --> 01:16:59.440]   I really want to thank everyone for making this journey so awesome.
[01:16:59.440 --> 01:17:06.560]   This book is one of the most fluid books in a way where you can really sit down and go
[01:17:06.560 --> 01:17:12.920]   through it in a sequential order without your brain getting fried as it does with technical
[01:17:12.920 --> 01:17:13.920]   books.
[01:17:13.920 --> 01:17:20.000]   And if that happened to you, I just get so derailed after reading two or three chapters.
[01:17:20.000 --> 01:17:24.800]   This is one of those books that I feel I can sit down and read in one go if I had 50 hours
[01:17:24.800 --> 01:17:25.800]   in my day.
[01:17:25.800 --> 01:17:27.680]   I'm a bit slow at reading.
[01:17:27.680 --> 01:17:31.280]   But thank you everyone for joining all of these sessions.
[01:17:31.280 --> 01:17:34.800]   Or if you're new here, I would request you to watch all of these recordings.
[01:17:34.800 --> 01:17:39.280]   We have as a group completed this book.
[01:17:39.280 --> 01:17:45.200]   But again, if you're new to Keras, I think really this is the start of your Keras journey
[01:17:45.200 --> 01:17:48.280]   and we all have a lot of work to do there.
[01:17:48.280 --> 01:17:54.560]   So thank you again, everyone who's been joining these sessions and I'll be seeing you next
[01:17:54.560 --> 01:17:56.680]   week if we have submissions coming in.
[01:17:56.680 --> 01:18:01.960]   If not, we'll announce whenever we are hosting any workshop, the ones that mention or any
[01:18:01.960 --> 01:18:03.840]   guest lectures.
[01:18:03.840 --> 01:18:06.200]   And I would love to see you there.
[01:18:06.200 --> 01:18:08.280]   So thank you so much for joining all of these sessions.
[01:18:08.280 --> 01:18:14.480]   And I would also request you all to really thank Franscho for writing this incredible
[01:18:14.480 --> 01:18:16.200]   book.
[01:18:16.200 --> 01:18:20.600]   My contribution or Fritz and Bias's contribution was to basically create a reading group around
[01:18:20.600 --> 01:18:21.600]   it.
[01:18:21.600 --> 01:18:25.880]   But it's all thanks to him for writing this book and also creating one of the first deep
[01:18:25.880 --> 01:18:29.160]   learning frameworks.
[01:18:29.160 --> 01:18:32.400]   That's the end of my TEDx talk.
[01:18:32.400 --> 01:18:36.000]   But thanks everyone for joining and look out for an email.
[01:18:36.000 --> 01:18:39.640]   Please consider submitting to the competition.
[01:18:39.640 --> 01:18:43.720]   If you'd like to present your work, I'd love to invite you next week.
[01:18:43.720 --> 01:18:49.600]   If not, just keep an eye out and we'll send over the details for the workshops or future
[01:18:49.600 --> 01:18:53.440]   Keras meetings.
[01:18:53.440 --> 01:18:57.040]   Thank you for going through this book together with us.

