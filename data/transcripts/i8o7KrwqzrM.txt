
[00:00:00.000 --> 00:00:06.320]   You're listening to Gradient Dissent, a show about making machine learning work in the
[00:00:06.320 --> 00:00:07.620]   real world.
[00:00:07.620 --> 00:00:10.500]   And I'm your host, Lukas Biewald.
[00:00:10.500 --> 00:00:16.400]   Joe Spizak is Director of Product Management of Generative AI at Meta, but I've known him
[00:00:16.400 --> 00:00:19.540]   a long time and he's been in the space a long time.
[00:00:19.540 --> 00:00:27.000]   He's done product management for AI at AWS and Google and Intel and Udacity and a number
[00:00:27.000 --> 00:00:28.220]   of others.
[00:00:28.220 --> 00:00:32.120]   He has a lot of learnings to share over the years and this is a super fun conversation.
[00:00:32.120 --> 00:00:33.520]   I hope you enjoy it.
[00:00:33.520 --> 00:00:39.160]   All right, Joe, why don't we start with what you're working on these days?
[00:00:39.160 --> 00:00:43.040]   I see you're back in Meta.
[00:00:43.040 --> 00:00:44.040]   What's going on?
[00:00:44.040 --> 00:00:45.040]   Oh my God.
[00:00:45.040 --> 00:00:46.040]   Yeah, it's...
[00:00:46.040 --> 00:00:52.280]   Yeah, I jumped back in July, end of July, and back in Meta.
[00:00:52.280 --> 00:00:56.920]   And I mean, it's Gen AI, man.
[00:00:56.920 --> 00:00:59.520]   It's everywhere, it's everything.
[00:00:59.520 --> 00:01:05.280]   I was working on Gen AI at Google, working Gen AI back here at Meta.
[00:01:05.280 --> 00:01:06.280]   It's a wild and crazy time.
[00:01:06.280 --> 00:01:10.120]   I mean, it feels great to be back, to be honest.
[00:01:10.120 --> 00:01:14.560]   Like I think there's never been a company that I've experienced that embraces open source
[00:01:14.560 --> 00:01:19.800]   and open science and just like open innovation like Meta has.
[00:01:19.800 --> 00:01:24.160]   And so for me, it's kind of amazing to my ears being back here.
[00:01:24.160 --> 00:01:25.160]   So love it.
[00:01:25.160 --> 00:01:26.160]   Nice.
[00:01:26.160 --> 00:01:30.480]   And last time, I think we talked, at least at length, when you were at Meta, you were
[00:01:30.480 --> 00:01:32.840]   working on PyTorch, I think, right?
[00:01:32.840 --> 00:01:35.080]   So this is a little different, isn't it?
[00:01:35.080 --> 00:01:36.080]   Yeah.
[00:01:36.080 --> 00:01:37.080]   Yeah.
[00:01:37.080 --> 00:01:38.080]   It is.
[00:01:38.080 --> 00:01:41.920]   Well, I mean, it's interesting because like, you know, Sumit and I, I mean, I saw you,
[00:01:41.920 --> 00:01:47.320]   Sumit did one of these a few months back and it's like him and I have kind of like rekindled
[00:01:47.320 --> 00:01:48.320]   the partnership, right?
[00:01:48.320 --> 00:01:53.960]   Because we worked together for five years on PyTorch and now it's like we're kind of
[00:01:53.960 --> 00:01:56.160]   like two layers of the stack, so to speak.
[00:01:56.160 --> 00:01:59.160]   Like I'm, and it's like, it's, it's interesting.
[00:01:59.160 --> 00:02:04.440]   It's actually really fun to have the PyTorch team inside Meta and, and kind of at the framework
[00:02:04.440 --> 00:02:08.560]   level and all the way down to the hardware level and I'm building the models and actually
[00:02:08.560 --> 00:02:10.960]   thinking about everything around the models.
[00:02:10.960 --> 00:02:16.200]   And so it actually is actually really interesting working together now as kind of two big components
[00:02:16.200 --> 00:02:18.600]   in this JAI ecosystem.
[00:02:18.600 --> 00:02:22.740]   Very different, but, but, but still feels sort of similar too.
[00:02:22.740 --> 00:02:26.160]   So you're working on the foundation models that you're releasing.
[00:02:26.160 --> 00:02:28.920]   That's like your, your day to day.
[00:02:28.920 --> 00:02:31.200]   And were you involved in Llama too?
[00:02:31.200 --> 00:02:35.800]   Or is that, was that before you're, you joining?
[00:02:35.800 --> 00:02:43.680]   It was, it was actually, so I was, I was involved in, in like we built Llama one in one of my
[00:02:43.680 --> 00:02:48.840]   teams in, so I was in, so after PyTorch, I was in FAIR working on math and science and
[00:02:48.840 --> 00:02:53.120]   building that up and one of the teams was our theorem proving team, which is basically
[00:02:53.120 --> 00:02:58.480]   looking at kind of AI guided formal mathematics, you know, eventually for like software verification
[00:02:58.480 --> 00:02:59.640]   and program proving.
[00:02:59.640 --> 00:03:03.280]   And there's some cool stuff happening there in parallel to that.
[00:03:03.280 --> 00:03:07.960]   If you remember Guillermo and he's actually just in town this past week, we had grabbed
[00:03:07.960 --> 00:03:12.200]   dinner with a few folks and it's interesting.
[00:03:12.200 --> 00:03:14.840]   It basically grabbed a bunch of FAIR compute and built Llama one.
[00:03:14.840 --> 00:03:16.360]   And that's kind of where that came from.
[00:03:16.360 --> 00:03:19.160]   And that was like right around the time I left for Google.
[00:03:19.160 --> 00:03:23.240]   And so then I came back and Llama two was out and now I'm basically working on Llama three.
[00:03:23.240 --> 00:03:26.000]   So that's nice.
[00:03:26.000 --> 00:03:28.080]   So I skipped, skipped a generation.
[00:03:28.080 --> 00:03:30.360]   Skipped a generation of Llama.
[00:03:30.360 --> 00:03:31.360]   Well, cool.
[00:03:31.360 --> 00:03:36.200]   I mean, can you talk at kind of like a high level, I guess, like, you know, what is the,
[00:03:36.200 --> 00:03:43.760]   like what is, what does the company get out of this process of putting out these, these
[00:03:43.760 --> 00:03:48.080]   models that are presumably really expensive to train?
[00:03:48.080 --> 00:03:50.120]   Yeah, it's interesting.
[00:03:50.120 --> 00:03:55.440]   This is a very similar story in a lot of ways to George's story.
[00:03:55.440 --> 00:04:00.520]   And like I always think about goals in open source.
[00:04:00.520 --> 00:04:05.880]   Yeah I'm not an open source zealot for a lot of people may think I am given I've spent
[00:04:05.880 --> 00:04:11.000]   a lot of time, less than years in open source AI, but I actually deeply believe that open
[00:04:11.000 --> 00:04:14.640]   source serves a really strong purpose and there are really clear goals to open source
[00:04:14.640 --> 00:04:15.640]   things.
[00:04:15.640 --> 00:04:21.960]   And, you know, I think thematically like Llama actually, I would say echoes a lot of, of
[00:04:21.960 --> 00:04:22.960]   what we saw with PyTorch.
[00:04:22.960 --> 00:04:27.000]   With PyTorch when, you know, Smith and I put our heads together back in like, you know,
[00:04:27.000 --> 00:04:29.840]   early 2018, we're like, what are we, you know, what are we going to do here?
[00:04:29.840 --> 00:04:32.280]   Like are we going to converge?
[00:04:32.280 --> 00:04:37.480]   We have this, this really high entropy, like ML community around us.
[00:04:37.480 --> 00:04:41.200]   And we were just thinking about, you know, what is the best way to like capture that
[00:04:41.200 --> 00:04:47.320]   community to, to, to, in order to, you know, build better products to, you know, when,
[00:04:47.320 --> 00:04:52.560]   when innovations happened in the CV space or an NLP, like how do we capture that quickly
[00:04:52.560 --> 00:04:57.080]   and be able to leverage that for internal usage or, you know, build it into PyTorch,
[00:04:57.080 --> 00:04:58.080]   make it a better framework.
[00:04:58.080 --> 00:05:05.920]   And that, when I think this similar story, like applies here for, for models as well.
[00:05:05.920 --> 00:05:09.680]   Like I think, you know, we're, we're using these models in production, like we're using,
[00:05:09.680 --> 00:05:14.440]   or at least the technology, the underlying technology in our production models.
[00:05:14.440 --> 00:05:20.640]   And so when you have like crazy high entropy spaces, like safety, for example, or just
[00:05:20.640 --> 00:05:27.080]   model evaluation, you know, more generally, you know, having the role to use your models
[00:05:27.080 --> 00:05:28.080]   is really helpful.
[00:05:28.080 --> 00:05:33.380]   We've learned a lot and we've learned a lot really fast by other people using our models.
[00:05:33.380 --> 00:05:40.000]   So that's a huge motivator for us, but there are other reasons, certainly.
[00:05:40.000 --> 00:05:45.400]   And what are your, what are your goals associated with, with releasing it?
[00:05:45.400 --> 00:05:51.520]   Like do you track usage somehow or like, like how do you know if you've had a successful
[00:05:51.520 --> 00:05:52.520]   rollout?
[00:05:52.520 --> 00:05:56.960]   Can you talk about like kind of what you might be optimizing for with, you know, like subsequent
[00:05:56.960 --> 00:05:58.560]   versions of Lambas?
[00:05:58.560 --> 00:05:59.560]   Yeah.
[00:05:59.740 --> 00:06:04.980]   I mean, open source has always been one of those tricky things, you know, to go a lot
[00:06:04.980 --> 00:06:06.660]   because you can't really set a metric.
[00:06:06.660 --> 00:06:12.140]   I remember when, you know, Robert Nishihara over at AnyScale and he was building Ray and
[00:06:12.140 --> 00:06:16.540]   he kind of called me up and he's like, Hey, I don't know how to define success in open
[00:06:16.540 --> 00:06:18.140]   source.
[00:06:18.140 --> 00:06:23.260]   And I remember sending him like my detailed thoughts and like an email and, and, and we've
[00:06:23.260 --> 00:06:27.780]   been kind of friends, I think ever since like he was a fair intern and, and he's got a great
[00:06:27.780 --> 00:06:28.780]   hand for open source.
[00:06:28.780 --> 00:06:29.780]   Oh, no way.
[00:06:29.780 --> 00:06:32.300]   We should put that email in the show notes.
[00:06:32.300 --> 00:06:33.300]   That would be amazing.
[00:06:33.300 --> 00:06:34.780]   Do you, do you have that?
[00:06:34.780 --> 00:06:35.780]   Is that private?
[00:06:35.780 --> 00:06:39.100]   I would love to, I mean, if people ask me about all the time, I would love to have a
[00:06:39.100 --> 00:06:40.900]   thoughtful response there.
[00:06:40.900 --> 00:06:44.540]   It's, well, I mean, it depends on your goals, right?
[00:06:44.540 --> 00:06:48.060]   I mean, like ultimately like everything, well, first of all, with open source, everything's
[00:06:48.060 --> 00:06:49.500]   a proxy, right?
[00:06:49.500 --> 00:06:55.340]   You're, you're very rarely able to basically measure directly like what success is with
[00:06:55.340 --> 00:06:56.620]   open source.
[00:06:56.620 --> 00:07:01.140]   So, you know, when we think about like in the Pytorch days, when we thought about research,
[00:07:01.140 --> 00:07:04.740]   research was actually like easiest thing to measure because like if my plain language
[00:07:04.740 --> 00:07:10.460]   goal is I want, you know, our framework to be the foundation of research because you
[00:07:10.460 --> 00:07:13.580]   know, it generates like new algorithms that we can leverage.
[00:07:13.580 --> 00:07:18.020]   Like it speeds up inference, whatever it is that, that the value that gets created and
[00:07:18.020 --> 00:07:20.900]   it gets kind of brought into our platform.
[00:07:20.900 --> 00:07:23.780]   Like there's you know, you can measure citations, right?
[00:07:23.780 --> 00:07:25.860]   You can, you can measure like papers with code.
[00:07:25.860 --> 00:07:31.740]   It's a great, great way to kind of show that that nice little index that shows, you know,
[00:07:31.740 --> 00:07:33.620]   over time the adoption of papers and code.
[00:07:33.620 --> 00:07:37.460]   So that's like the easiest thing to measure, but like, how do you measure like whether
[00:07:37.460 --> 00:07:42.440]   your production, whether production usage is actually occurring, right?
[00:07:42.440 --> 00:07:46.060]   No one puts their code out on GitHub in production.
[00:07:46.060 --> 00:07:50.660]   So like ultimately you're dealing with proxies no matter what you're doing.
[00:07:50.660 --> 00:07:54.380]   And I mean, even the cloud providers themselves don't actually have a lot of direct, like
[00:07:54.380 --> 00:07:59.620]   they can, they can get, you know, some customers that tell them, you know, you, you would wait
[00:07:59.620 --> 00:08:03.460]   to advise this, for example, can, could get actually probably more direct signal, even
[00:08:03.460 --> 00:08:07.220]   then I would say cloud providers, given you're so much closer to the user.
[00:08:07.220 --> 00:08:11.100]   A lot of the usage on cloud, for example, is like obfuscated through like VMs and, and
[00:08:11.100 --> 00:08:13.820]   Omnis and such, whereas like you have actually direct.
[00:08:13.820 --> 00:08:18.580]   So it's anything, anything in success, like definition in open source is always a proxy.
[00:08:18.580 --> 00:08:19.900]   I've learned to live with that.
[00:08:19.900 --> 00:08:27.100]   It's, you know, I've embraced it basically.
[00:08:27.100 --> 00:08:32.620]   When you like, when you build these LLAMA models, like, like the next generation one,
[00:08:32.620 --> 00:08:34.300]   how do you measure the quality of them?
[00:08:34.300 --> 00:08:39.620]   Like, are you looking to kind of beat major benchmarks or what, what do you actually like,
[00:08:39.620 --> 00:08:42.060]   how do you even decide like which one to ship?
[00:08:42.060 --> 00:08:48.180]   Yeah, it's, it's interesting because there's, I would say that there's two kind of two layers
[00:08:48.180 --> 00:08:53.260]   to North stars when it comes to these frontier or to foundation models.
[00:08:53.260 --> 00:08:55.140]   I almost slipped up and said frontier.
[00:08:55.140 --> 00:08:58.500]   I spent the last several months, you know, now I'm all, now we're supposed to be talking
[00:08:58.500 --> 00:08:59.900]   dual use, right.
[00:08:59.900 --> 00:09:07.460]   But but you know, foundation models ultimately like in my mind should show emerging capabilities
[00:09:07.460 --> 00:09:10.260]   or they should be, you know, really capable on.
[00:09:10.260 --> 00:09:15.740]   And this is where eval is like really interesting because, you know, the capabilities and evaluation
[00:09:15.740 --> 00:09:16.860]   kind of go hand in hand.
[00:09:16.860 --> 00:09:21.220]   Like you either, you know, it's, it's almost like the chicken and the egg, which one comes
[00:09:21.220 --> 00:09:22.220]   first?
[00:09:22.220 --> 00:09:25.540]   You know, did you, did you actually generate in the model or did you evaluate it and find
[00:09:25.540 --> 00:09:26.540]   it?
[00:09:26.540 --> 00:09:31.700]   So I do think like, but ultimately like pushing on things like reasoning, you know, pushing
[00:09:31.700 --> 00:09:35.740]   on capabilities, you know, you can look at in the image space, I guess if you can generate
[00:09:35.740 --> 00:09:41.380]   super, you know, photo, photo, realistic images or, or that, I think you can maybe call that
[00:09:41.380 --> 00:09:42.380]   a capability.
[00:09:42.380 --> 00:09:46.100]   But ultimately I would say there's like a dual, dual North star.
[00:09:46.100 --> 00:09:51.900]   One is, is obviously the foundation model capability, but the second is like actual,
[00:09:51.900 --> 00:09:53.700]   I would say usability of a model.
[00:09:53.700 --> 00:09:58.660]   Because if, for example, I build a trillion parameter MOE, I'm just throwing numbers out
[00:09:58.660 --> 00:09:59.660]   there.
[00:09:59.660 --> 00:10:00.660]   It is too trying.
[00:10:00.660 --> 00:10:04.140]   But at this massive MOE, like who's going to be able to use that ultimately, right?
[00:10:04.140 --> 00:10:07.420]   It's just like a, like, you know, are we going to be able to use it?
[00:10:07.420 --> 00:10:11.620]   Even, even better Google, I will struggle to kind of deploy something at that scale.
[00:10:11.620 --> 00:10:15.460]   I mean, they could probably do it, but it's like how efficient, how cost effective, like
[00:10:15.460 --> 00:10:20.020]   are you going to be able to scale that in search or in our case, ads and feed and other
[00:10:20.020 --> 00:10:21.020]   things.
[00:10:21.020 --> 00:10:24.340]   So I do think that the second North star is really kind of like adoption.
[00:10:24.340 --> 00:10:29.780]   And ultimately you need to be able to, to take what you're building and ultimately make
[00:10:29.780 --> 00:10:34.220]   it consumable, you know, for kind of at scale deployment.
[00:10:34.220 --> 00:10:39.180]   And so that might mean distilling these models into, you know, sizes that are kind of tractable
[00:10:39.180 --> 00:10:41.860]   for different compute envelopes.
[00:10:41.860 --> 00:10:45.020]   You know, it could be pruning them and so on.
[00:10:45.020 --> 00:10:48.540]   But ultimately like you, you really want to push on capabilities and be able to waterfall
[00:10:48.540 --> 00:10:52.020]   that, that, you know, to actual impact.
[00:10:52.020 --> 00:10:55.860]   And so there's a lot of thought that goes into that and how you actually kind of stage
[00:10:55.860 --> 00:10:57.740]   that process.
[00:10:57.740 --> 00:11:01.540]   How did you pick the particular sizes that you released?
[00:11:01.540 --> 00:11:04.260]   There's like three versions, right?
[00:11:04.260 --> 00:11:06.980]   How did you come up with the 6 billion parameters?
[00:11:06.980 --> 00:11:08.820]   And yeah.
[00:11:08.820 --> 00:11:13.000]   I mean, like in early days, obviously it's, you know, it's a little bit of, we don't know,
[00:11:13.000 --> 00:11:14.000]   we don't know.
[00:11:14.000 --> 00:11:19.820]   We're just kind of picking, you know, different sizes based on, I would say like compute envelope
[00:11:19.820 --> 00:11:26.500]   and memory, you know, based on memory capacity of, of, of what the platforms are that you
[00:11:26.500 --> 00:11:27.500]   target.
[00:11:27.500 --> 00:11:31.340]   So, you know, we kind of wish we, for example, would have released 34 B because it's, it
[00:11:31.340 --> 00:11:34.780]   kind of fits nicely into the GPU memory.
[00:11:34.780 --> 00:11:39.260]   You know, then obviously the Kaifu released a really nice model.
[00:11:39.260 --> 00:11:40.780]   You know, it's 34 B and that's awesome.
[00:11:40.780 --> 00:11:45.220]   It's I have a real problem with the licensing, it's interesting license, but like but the
[00:11:45.220 --> 00:11:47.860]   model itself seems to be pretty capable.
[00:11:47.860 --> 00:11:51.520]   But I think like in some ways, like, like I said, if you, if you start with really the
[00:11:51.520 --> 00:11:56.140]   foundational capabilities and then you, you think about the different, you know, this
[00:11:56.140 --> 00:11:59.520]   is where product science and your, your product, having a product manager thinking about these
[00:11:59.520 --> 00:12:03.940]   things, you know, it does help where you think about like, what is it like ultimately that
[00:12:03.940 --> 00:12:04.940]   you're trying to achieve?
[00:12:04.940 --> 00:12:09.340]   Is it, you know, if you're trying to achieve like on device, like an on device LLM to be
[00:12:09.340 --> 00:12:13.860]   able to run kind of in tandem, say with a larger model in the cloud, like you need to
[00:12:13.860 --> 00:12:17.740]   think about the envelope and the compute footprint and the memory footprint.
[00:12:17.740 --> 00:12:22.180]   It actually most importantly, these days in mobile, it's actually memory bandwidth that
[00:12:22.180 --> 00:12:24.100]   actually is, is actually the limiting factor.
[00:12:24.100 --> 00:12:29.060]   So thinking through like what type of environments, what type of experiences you want to enable,
[00:12:29.060 --> 00:12:35.340]   I think is, is I think probably the, the most important thing in terms of sizes.
[00:12:35.340 --> 00:12:37.300]   So I think we've learned a lot, right?
[00:12:37.300 --> 00:12:38.300]   We have data now, right?
[00:12:38.300 --> 00:12:39.300]   We're releasing things.
[00:12:39.300 --> 00:12:43.060]   We know what people are downloading, what we know internally at scale, what people are
[00:12:43.060 --> 00:12:46.340]   doing with these models and we can add now I can zero it in.
[00:12:46.340 --> 00:12:50.860]   You know, the next generation I can zero in much more precisely and say, you know what,
[00:12:50.860 --> 00:12:54.660]   this is actually a good size to aim at for say like the server side.
[00:12:54.660 --> 00:12:59.180]   Here's a nice, you know, size for maybe the other, other types of devices and so on.
[00:12:59.180 --> 00:13:03.340]   So I think like we're learning, I think it was a short answer.
[00:13:03.340 --> 00:13:08.420]   Is it hard to release models of different new sizes?
[00:13:08.420 --> 00:13:12.860]   Like I sort of imagine you distilling from one bigger model or are you training them
[00:13:12.860 --> 00:13:13.860]   all from scratch?
[00:13:13.860 --> 00:13:15.540]   Like how does that actually work?
[00:13:15.540 --> 00:13:19.740]   I think this is actually a bit of an open question of, cause it's funny, you actually
[00:13:19.740 --> 00:13:23.460]   get different, different sides of that camp.
[00:13:23.460 --> 00:13:28.620]   You know, people will say that, you know, to get the best small model or a smaller model
[00:13:28.620 --> 00:13:32.660]   is best to start with a larger model and distill from there.
[00:13:32.660 --> 00:13:36.380]   Others will say, well, the scaling laws are different for smaller models versus larger
[00:13:36.380 --> 00:13:37.380]   models.
[00:13:37.380 --> 00:13:41.740]   So, you know, you should basically adjust your scaling laws and you should, you know,
[00:13:41.740 --> 00:13:46.660]   train from scratch and then kind of saturate a smaller, you know, smaller parameter count
[00:13:46.660 --> 00:13:48.740]   and go from there.
[00:13:48.740 --> 00:13:55.100]   So I think honestly, I mean, we'll probably try both ways and see what works best because
[00:13:55.100 --> 00:14:01.540]   you know, the amount of data we're using, you know, the amount of compute we're using
[00:14:01.540 --> 00:14:04.060]   is changing from generation to generation.
[00:14:04.060 --> 00:14:08.860]   And I think the use cases and I would say the envelopes of like, you know, the actual
[00:14:08.860 --> 00:14:11.900]   environment these things are getting deployed in is actually where we target as well.
[00:14:11.900 --> 00:14:18.420]   So I think we'll, I mean, we'll probably try everything and see what works best, honestly.
[00:14:18.420 --> 00:14:19.780]   Nice.
[00:14:19.780 --> 00:14:27.540]   And with, you have a couple, I think, different versions also with maybe different RLHF strategies.
[00:14:27.540 --> 00:14:30.540]   There's like a chat one and an instructs one, right?
[00:14:30.540 --> 00:14:31.940]   Like how did you end up at that?
[00:14:31.940 --> 00:14:33.340]   And what are you thinking with that?
[00:14:33.340 --> 00:14:38.380]   I feel like at some point there's maybe too many options and it starts to get confusing.
[00:14:38.380 --> 00:14:43.860]   Like how are you, how are you imagining modifying that going forward?
[00:14:43.860 --> 00:14:44.860]   Yeah.
[00:14:44.860 --> 00:14:48.900]   I mean, we're, we're thinking about it.
[00:14:48.900 --> 00:14:52.500]   You know what it looks like, cause there's, you know, I think I have a really innovative
[00:14:52.500 --> 00:14:55.580]   team that's like super excited to kind of work on different things.
[00:14:55.580 --> 00:15:00.020]   And, and we also have a research organization in FAIR who, you know, for example, like they
[00:15:00.020 --> 00:15:05.620]   built Code Llama, which was a, you know, a fine tuned version of Llama 2 for, for not
[00:15:05.620 --> 00:15:08.860]   only like generating code, but also like, you know, just conversing about code.
[00:15:08.860 --> 00:15:13.860]   And so, yeah, the number of variants like is, is kind of crazy.
[00:15:13.860 --> 00:15:18.380]   And you start to get this, you know, you're like, what do I use now?
[00:15:18.380 --> 00:15:21.900]   And so I do, I do instruct, do I use the Python version?
[00:15:21.900 --> 00:15:26.740]   Do I, you know, do I just take the pre-trained one and ultimately like customize it for,
[00:15:26.740 --> 00:15:28.700]   you know, for what I want to do?
[00:15:28.700 --> 00:15:31.580]   I think, you know, honestly, I think we're just in that space right now where we're going
[00:15:31.580 --> 00:15:35.780]   to be trying a lot of things and we may put out a lot of different models.
[00:15:35.780 --> 00:15:39.340]   So we're finding like certain data mixtures, like generate really good reasoning, for example,
[00:15:39.340 --> 00:15:45.620]   do we want to release a reasoning version or, you know, do we, like we did with code,
[00:15:45.620 --> 00:15:48.140]   you know, do we want to continue to release code versions?
[00:15:48.140 --> 00:15:53.820]   And like, ultimately I think it's, it is space right now that's like fast moving and we're
[00:15:53.820 --> 00:15:57.580]   not sure like, which, you know, can you just put all of these capabilities into like one
[00:15:57.580 --> 00:16:00.580]   foundation model and like have the best of all worlds?
[00:16:00.580 --> 00:16:08.180]   Maybe, you know, there's also like the question of like how the capabilities come out versus
[00:16:08.180 --> 00:16:13.540]   like, or in terms of like pre-training versus fine tuning versus, you know, RLHF, like,
[00:16:13.540 --> 00:16:17.820]   is, are you getting most of your capabilities and like in this pre-training, like, you know,
[00:16:17.820 --> 00:16:23.060]   phase or are you getting like everything in, in SFTE, like, you know, do you need RLHF?
[00:16:23.060 --> 00:16:24.540]   Like that's still a little bit of an open debate.
[00:16:24.540 --> 00:16:29.820]   I think, I think we did all three and obviously our models are pretty good.
[00:16:29.820 --> 00:16:33.580]   Like, you know, OpenAI does do all three as well.
[00:16:33.580 --> 00:16:37.500]   So I think like, we'll just look at what people are doing, what, what's, you know, we might,
[00:16:37.500 --> 00:16:41.880]   some of our models might end up being like for showcase, like to show different capabilities
[00:16:41.880 --> 00:16:45.160]   and others might be more like, you know, built for customization.
[00:16:45.160 --> 00:16:49.180]   So I mean, we're still looking at all, all types of models.
[00:16:49.180 --> 00:16:50.180]   Do you have any advice?
[00:16:50.180 --> 00:16:54.700]   I get this question a lot of like where to start, like what, what model to start with
[00:16:54.700 --> 00:16:56.460]   or how to think about that.
[00:16:56.460 --> 00:16:57.740]   I was kind of surprised myself.
[00:16:57.740 --> 00:17:02.660]   I was trying, the chat model seemed to work a little better for me in cases that felt
[00:17:02.660 --> 00:17:04.700]   like I was like telling the model to do something.
[00:17:04.700 --> 00:17:06.700]   They worked significantly better than the instruct model.
[00:17:06.700 --> 00:17:10.140]   And I was kind of wondering, okay, how common is that?
[00:17:10.140 --> 00:17:14.540]   Or like, is there like a repository that I could go to, to sort of see what general best
[00:17:14.540 --> 00:17:15.540]   practices are?
[00:17:15.540 --> 00:17:19.060]   I mean, can you speak to the best practice today or kind of point people to where they
[00:17:19.060 --> 00:17:20.660]   could learn more about that?
[00:17:20.660 --> 00:17:25.420]   Yeah, I mean, there's some really, there's some really great resources, honestly.
[00:17:25.420 --> 00:17:30.460]   Our team just put out a, like a massive getting started guide on Lama just a couple of weeks
[00:17:30.460 --> 00:17:31.460]   ago.
[00:17:31.460 --> 00:17:36.540]   It's on the, on the ai.meta.com/lama site.
[00:17:36.540 --> 00:17:37.540]   We still need our own website.
[00:17:37.540 --> 00:17:39.180]   I'm working on that.
[00:17:39.180 --> 00:17:44.340]   But you know, there's everything from like using, you know, Lama with rag, although,
[00:17:44.340 --> 00:17:49.900]   you know, to using a line chain, to prompt engineering, to fine tuning, you know, all
[00:17:49.900 --> 00:17:55.300]   these things like there's basically integrations and, and kind of really nice detailed instructions
[00:17:55.300 --> 00:17:58.500]   that kind of give you an idea of how to use the models.
[00:17:58.500 --> 00:18:02.900]   I personally love using like the small models and just like, I mean, like you can go to
[00:18:02.900 --> 00:18:05.940]   Hugging Face, for example, and just go into like the model zoo and grab like the 7B or
[00:18:05.940 --> 00:18:08.140]   even, even like prompted directly.
[00:18:08.140 --> 00:18:09.380]   It's super, super easy.
[00:18:09.380 --> 00:18:13.740]   Like that's probably the first thing I do or, or I'll start like, I'll just grab like
[00:18:13.740 --> 00:18:17.220]   a Jupyter notebook or like a Colab and like load up a GPU.
[00:18:17.220 --> 00:18:20.820]   And you know, when like the early Falcon models came out, I, you know, and I saw them and
[00:18:20.820 --> 00:18:22.300]   I'm like, this is pretty cool.
[00:18:22.300 --> 00:18:26.140]   Like I could just grab one, like, and I was up and running with on the 40B within like
[00:18:26.140 --> 00:18:30.180]   a couple of minutes and just kind of seeing very quickly, like how it compared to like
[00:18:30.180 --> 00:18:32.140]   the smaller version they had.
[00:18:32.140 --> 00:18:36.620]   And like, was it more coherent when it responded to me or, or would it like have a higher false
[00:18:36.620 --> 00:18:39.660]   refusal rate if I like ask it to do stuff.
[00:18:39.660 --> 00:18:45.780]   I think like, to me, like, you know, there's a bunch of like services and things out there.
[00:18:45.780 --> 00:18:50.340]   My like go-to is either just to like set up a Jupyter notebook and Colab and start to
[00:18:50.340 --> 00:18:54.580]   grab a model or, you know, just go like Hugging Face and prompt directly.
[00:18:54.580 --> 00:18:56.700]   If the model fits in GPU memory, I guess.
[00:18:56.700 --> 00:19:01.060]   Otherwise you'll like run out of memory, but that's, those are the things I do anyway.
[00:19:01.060 --> 00:19:02.060]   Interesting.
[00:19:02.060 --> 00:19:05.820]   So you recommend just sort of like trying them and getting a feel for how well they,
[00:19:05.820 --> 00:19:06.820]   they work?
[00:19:06.820 --> 00:19:07.820]   That to me.
[00:19:07.820 --> 00:19:08.820]   Yeah.
[00:19:08.820 --> 00:19:12.460]   I mean, I think I like the kind of chat you know, just to understand like how to converse
[00:19:12.460 --> 00:19:13.460]   with the model.
[00:19:13.460 --> 00:19:15.900]   I think that's that's pretty interesting.
[00:19:15.900 --> 00:19:17.780]   That's like a quick way to do it.
[00:19:17.780 --> 00:19:21.980]   But there's also like, if you look on like on Hugging Face, there's like 11,000 derivative
[00:19:21.980 --> 00:19:27.300]   Lama models that you can also like download and people have like built Chinese versions.
[00:19:27.300 --> 00:19:30.500]   They built German language versions.
[00:19:30.500 --> 00:19:34.180]   They've made them fashionista versions, or I think I saw her fashion versions.
[00:19:34.180 --> 00:19:35.980]   And we'll converse with you about fashion.
[00:19:35.980 --> 00:19:42.140]   I'm not a very fashionable guy, but you know, but but you know, you can, I guess play with
[00:19:42.140 --> 00:19:43.140]   those as well.
[00:19:43.140 --> 00:19:47.340]   So I think, you know, the community is kind of innovating here and they're taking like
[00:19:47.340 --> 00:19:51.900]   advantage of the fact that you can like adapt these models pretty quickly with very little
[00:19:51.900 --> 00:19:56.180]   data or even prompt engineering and be able to kind of have them do different things.
[00:19:56.180 --> 00:20:00.340]   So yeah, I mean, it's pretty fun to kind of play with what the community is doing.
[00:20:00.340 --> 00:20:01.340]   Totally.
[00:20:01.340 --> 00:20:02.340]   Totally.
[00:20:02.340 --> 00:20:08.660]   I mean, I think the 6B model is, is pretty impressive, although kind of challenging to
[00:20:08.660 --> 00:20:11.500]   deploy like truly for a lot of edge use cases.
[00:20:11.500 --> 00:20:14.180]   Have you thought about like building even kind of smaller ones?
[00:20:14.180 --> 00:20:18.100]   I feel like there's a lot of appetite for that and various strategies that I've seen
[00:20:18.100 --> 00:20:19.100]   out there.
[00:20:19.100 --> 00:20:23.660]   Yeah, there's, I mean, we've, you know, we released the 7B actually.
[00:20:23.660 --> 00:20:24.660]   The 7B, sorry.
[00:20:24.660 --> 00:20:25.660]   Yeah, right, right.
[00:20:25.660 --> 00:20:32.100]   But it's, and it's, you know, Qualcomm, for example, is able to deploy the 7B Lama 2 on
[00:20:32.100 --> 00:20:34.620]   the new Snapdragon.
[00:20:34.620 --> 00:20:38.260]   That was like announced a couple of weeks ago, like MediaTek is able to do it.
[00:20:38.260 --> 00:20:42.820]   And I think that's going to be like a fairly premium like feature on a phone.
[00:20:42.820 --> 00:20:47.500]   So it's like the high end phones, which you can imagine like smaller versions.
[00:20:47.500 --> 00:20:50.260]   I think there's a, there was a paper I just saw that came out this past week called Baby
[00:20:50.260 --> 00:20:53.140]   Lama, which was pretty cool.
[00:20:53.140 --> 00:20:56.220]   You know, there's also like Tiny Lama on GitHub.
[00:20:56.220 --> 00:20:59.980]   Somebody's basically releasing incremental checkpoints and it's like a 1 billion.
[00:20:59.980 --> 00:21:00.980]   It's up to a 1 billion.
[00:21:00.980 --> 00:21:03.180]   They're releasing all the checkpoints up to 1 billion.
[00:21:03.180 --> 00:21:05.420]   So there's actually quite a bit of, of innovation.
[00:21:05.420 --> 00:21:09.900]   I mean, it's not that expensive to train or, or to pre-train from scratch, right?
[00:21:09.900 --> 00:21:14.340]   When you get into the small, the small models it's just like really how coherent you can
[00:21:14.340 --> 00:21:16.620]   make them.
[00:21:16.620 --> 00:21:23.220]   And and I think the more you can like basically fine tuning or, or like adapt them for a specific
[00:21:23.220 --> 00:21:26.300]   task, I think the more coherent they can be, which is really cool.
[00:21:26.300 --> 00:21:29.860]   So you can't really expect like a 1B or a 3B for example, to like be as good as like
[00:21:29.860 --> 00:21:34.260]   a 7DB or, or bigger just because it doesn't have the model capacity.
[00:21:34.260 --> 00:21:40.460]   But, but all that said, like we actually haven't, I think yet saturated those, those model sizes.
[00:21:40.460 --> 00:21:44.280]   So actually I do think there's like opportunity for us in the community to kind of build better
[00:21:44.280 --> 00:21:45.500]   smaller models.
[00:21:45.500 --> 00:21:47.060]   So I'm pretty bullish here actually.
[00:21:47.060 --> 00:21:53.860]   I mean, one thing that I was kind of wondering about, you know, like, like it was a little
[00:21:53.860 --> 00:21:58.740]   bit irritating to have to like ask permission to, to just download the model.
[00:21:58.740 --> 00:22:01.380]   Like what was the thinking behind that?
[00:22:01.380 --> 00:22:05.780]   Like that, that it's a product manager, I'm sure you want to remove friction and the sort
[00:22:05.780 --> 00:22:09.420]   of like waiting for a few hours to get access to the model is kind of like, wait, what's
[00:22:09.420 --> 00:22:10.420]   going on here?
[00:22:10.420 --> 00:22:13.420]   No, this drives me crazy.
[00:22:13.420 --> 00:22:17.500]   And I'm, I'm working on some things I think to make it easier certainly.
[00:22:17.500 --> 00:22:22.780]   But yeah, I mean, I, as a product manager, like I, I mean, as a, as a, as a llama guy,
[00:22:22.780 --> 00:22:26.580]   whatever you want to call me, you know, if you, if you follow me at GitHub, you'll see
[00:22:26.580 --> 00:22:29.140]   that I I'm on there a lot and responding to issues.
[00:22:29.140 --> 00:22:32.540]   And I see a lot of what the community is, is dealing with.
[00:22:32.540 --> 00:22:37.420]   And it's, I mean, I don't know how many issues are of there are for like on hugging face,
[00:22:37.420 --> 00:22:41.580]   like, you know, cause right now we do like we do a check on the meta side.
[00:22:41.580 --> 00:22:43.860]   We do a check on the, on the hugging face side.
[00:22:43.860 --> 00:22:47.420]   You know, we have a license that is, you know, it's not a crazy license.
[00:22:47.420 --> 00:22:50.940]   It's not crazy bespoke, but it is a modified license.
[00:22:50.940 --> 00:22:55.940]   We have an acceptable use policy, which I think is a good, good thing to have.
[00:22:55.940 --> 00:22:59.220]   But yeah, ultimately it does add friction, which I'm not happy about.
[00:22:59.220 --> 00:23:02.860]   So I'm, I'm hoping with some of the changes that we have upcoming that we can streamline
[00:23:02.860 --> 00:23:04.720]   that quite a bit.
[00:23:04.720 --> 00:23:09.380]   Because I think ultimately like we want this technology in everyone's hands.
[00:23:09.380 --> 00:23:13.020]   I don't think the license we've selected precludes any of that.
[00:23:13.020 --> 00:23:16.780]   I think though there are some things we can do with like hugging face, Kaggle and other
[00:23:16.780 --> 00:23:20.900]   platforms though, to just make it easier to access the models and more streamlined.
[00:23:20.900 --> 00:23:22.900]   So yeah, I mean, we're definitely working on it.
[00:23:22.900 --> 00:23:24.500]   But I'm as frustrated as you are.
[00:23:25.500 --> 00:23:32.500]   Do you feel at all competitive with other open source projects to build foundation models?
[00:23:32.500 --> 00:23:34.620]   Or do you just feel like, Hey, this is great.
[00:23:34.620 --> 00:23:35.860]   Like let's just collaborate with them.
[00:23:35.860 --> 00:23:37.300]   Like what's that like?
[00:23:37.300 --> 00:23:41.060]   I mean, I'm a competitive guy.
[00:23:41.060 --> 00:23:42.060]   I love that.
[00:23:42.060 --> 00:23:43.060]   Nice.
[00:23:43.060 --> 00:23:47.060]   I mean, I'm of two minds here.
[00:23:47.060 --> 00:23:48.860]   I love, I love being competitive.
[00:23:48.860 --> 00:23:50.980]   I love seeing our models on top of the leaderboards.
[00:23:50.980 --> 00:23:56.580]   I love hearing about what people build with our models and, and the stuff.
[00:23:56.580 --> 00:24:01.260]   I mean, I've, I'm just shocked, for example, like how like enterprise even has it like
[00:24:01.260 --> 00:24:06.020]   adopted like Lama models from mid July to now, like thousands of enterprises are using
[00:24:06.020 --> 00:24:08.460]   like Lama models in products.
[00:24:08.460 --> 00:24:12.060]   On the other hand though, and by the way, that's a huge source of pride for like, you
[00:24:12.060 --> 00:24:14.060]   know, me and a lot of other folks that met it here.
[00:24:14.060 --> 00:24:19.580]   But at the same time, like I love when other models are released into the open.
[00:24:19.580 --> 00:24:21.900]   Because I think that it's, it can't be just Meta, right.
[00:24:21.900 --> 00:24:27.900]   It can't be just like us and our team, like carrying the flag for the open, you know,
[00:24:27.900 --> 00:24:30.180]   for openness and, and transparency.
[00:24:30.180 --> 00:24:33.980]   I think it's like, you know, I love, I've met with the Falcon team.
[00:24:33.980 --> 00:24:34.980]   They're super cool.
[00:24:34.980 --> 00:24:36.660]   I love what they're doing.
[00:24:36.660 --> 00:24:40.180]   You know, the, the minstrel team in Paris, I mean, they're all of our friends, right.
[00:24:40.180 --> 00:24:44.620]   I mean, I, they're all my colleagues, Tim and DL and the team.
[00:24:44.620 --> 00:24:47.340]   So like when new models come out, I'm huge fan.
[00:24:47.340 --> 00:24:51.380]   So I'll go play with them and I'll compare them and I'll be like, I'll, I'll evaluate
[00:24:51.380 --> 00:24:56.060]   them and it's just like the more honestly that the better gets out there.
[00:24:56.060 --> 00:24:59.740]   But yeah, I mean, I'm a little bit competitive, obviously I want us to have the best models.
[00:24:59.740 --> 00:25:05.980]   Are there, is there like secret sauce in the training that you want to keep to yourself
[00:25:05.980 --> 00:25:11.500]   or are you just totally open about the learnings and the process of building these models?
[00:25:11.500 --> 00:25:12.780]   I mean, we're pretty transparent.
[00:25:12.780 --> 00:25:15.500]   I mean, the paper, how many pages was our paper?
[00:25:15.500 --> 00:25:16.500]   48 pages.
[00:25:16.500 --> 00:25:21.300]   So like, I, I think there's obvious, every company has secret sauce that you're going
[00:25:21.300 --> 00:25:24.980]   to kind of keep to yourself.
[00:25:24.980 --> 00:25:29.820]   You know, but at the same time, like, I think if you read the paper, like we were, we, the
[00:25:29.820 --> 00:25:36.020]   RLHF like ablations studying that the team did was like, holy cow.
[00:25:36.020 --> 00:25:40.500]   When I read that, I was just like, I mean, I've never seen anything like this.
[00:25:40.500 --> 00:25:43.540]   So that paper was an absolute masterpiece.
[00:25:43.540 --> 00:25:47.820]   And that's just not, I'm not saying that because I work at Meta and I know the team, but it
[00:25:47.820 --> 00:25:49.100]   really was an amazing paper.
[00:25:49.100 --> 00:25:52.260]   It was as detailed as I've ever seen it.
[00:25:52.260 --> 00:25:57.900]   And like it's transparent and everything from like the, you know, the, I don't know, just
[00:25:57.900 --> 00:26:05.700]   like the learning curves and, and, and RLHF the ablation study to, you know, how we thought
[00:26:05.700 --> 00:26:07.500]   about SFT versus pre-training.
[00:26:07.500 --> 00:26:09.660]   It was like, it was just really, really detailed.
[00:26:09.660 --> 00:26:10.660]   And like, yeah.
[00:26:10.660 --> 00:26:16.340]   I mean, so I think we've, we've been as probably, probably as transparent as we possibly can
[00:26:16.340 --> 00:26:19.260]   be, I guess, maybe that point.
[00:26:19.260 --> 00:26:26.740]   Can you talk about what you're working on now in the, in the next version of this?
[00:26:26.740 --> 00:26:28.180]   Like how are you thinking about that?
[00:26:28.180 --> 00:26:29.180]   Like, okay.
[00:26:29.180 --> 00:26:30.180]   Yeah.
[00:26:30.180 --> 00:26:36.300]   I mean, like, what are you, like, what changes are you thinking about?
[00:26:36.300 --> 00:26:39.220]   I mean, we've learned a lot.
[00:26:39.220 --> 00:26:44.620]   I mean, I think that, I think there's a bunch of other challenges besides the model.
[00:26:44.620 --> 00:26:49.500]   I think that are, yeah, I think I kind of look at the, what's interesting is, you know,
[00:26:49.500 --> 00:26:54.820]   I saw, you know, Karpathy had this post, you know, last week around kind of LLM operating
[00:26:54.820 --> 00:26:55.820]   system.
[00:26:55.820 --> 00:27:00.260]   And it was interesting cause I had been thinking, you know, on this line for, for a while as
[00:27:00.260 --> 00:27:05.940]   well, it's, it feels very much like the model is the kernel and, you know, you kind of have
[00:27:05.940 --> 00:27:09.180]   in the operating system, you have things like your antivirus, which is kind of like your
[00:27:09.180 --> 00:27:10.620]   trust and safety, I guess.
[00:27:10.620 --> 00:27:15.740]   And you know, you have like file systems and your access to kind of information or like
[00:27:15.740 --> 00:27:18.900]   through embeddings or rag and databases and so on.
[00:27:18.900 --> 00:27:25.580]   And so like, I think I'm thinking about like, not just the model, I'm thinking about the
[00:27:25.580 --> 00:27:27.340]   whole ecosystem.
[00:27:27.340 --> 00:27:33.100]   I'm thinking about, you know, levels of abstraction for how you access these models, whether it's,
[00:27:33.100 --> 00:27:37.980]   you know, through an API, through our partnerships or, you know, through like direct access or
[00:27:37.980 --> 00:27:43.020]   integration with SDKs or, you know, we, we work for example, with a lot of open source
[00:27:43.020 --> 00:27:47.740]   projects like Lanshane and AutoGPT and, and a bunch of others.
[00:27:47.740 --> 00:27:51.620]   And so I'm right now, I mean, my biggest focus is like, how do I think about the community
[00:27:51.620 --> 00:27:57.860]   and how do I, how do I build around, you know, LLM models as kind of like this kernel of
[00:27:57.860 --> 00:28:02.980]   an operating system and, and bring a more holistic, like, you know, platform for others
[00:28:02.980 --> 00:28:03.980]   to build on.
[00:28:03.980 --> 00:28:09.300]   Cause ultimately like with PyTorch, like that's, that was kind of our strategy with PyTorch
[00:28:09.300 --> 00:28:13.460]   is we kind of, you know, when we thought about like, what do we make, what do we buy?
[00:28:13.460 --> 00:28:17.420]   You know, we, we didn't want to king make in places where it didn't make sense for us
[00:28:17.420 --> 00:28:18.420]   to king make.
[00:28:18.420 --> 00:28:24.340]   I think probably Sumit talked about this in his, his podcast, but you know, in areas of,
[00:28:24.340 --> 00:28:28.540]   of high entropy, you know, it's, it's kind of weird to go and put something there and,
[00:28:28.540 --> 00:28:32.780]   and then, you know, be kind of competitive with everything going on in high entropy space.
[00:28:32.780 --> 00:28:37.420]   So, so we want to build basically things that others can build on ultimately and grow a
[00:28:37.420 --> 00:28:39.940]   community around that and create that flywheel.
[00:28:39.940 --> 00:28:43.740]   And so like, I'm looking at areas around lava and thinking, I mean, like trust and safety
[00:28:43.740 --> 00:28:48.500]   is one area where in evaluation, like how do we actually grow the community around us
[00:28:48.500 --> 00:28:55.420]   so we can basically ultimately build safer, generative AI experiences.
[00:28:55.420 --> 00:28:58.860]   Like I think ultimately there's this chasm to cross right in products.
[00:28:58.860 --> 00:29:03.340]   How do I take this open model, which looks, looks cool and does some cool stuff, but ultimately
[00:29:03.340 --> 00:29:06.020]   build responsible and safe products from it.
[00:29:06.020 --> 00:29:09.540]   And I think, so this is, this is kind of like a, and, and, you know, some of the things
[00:29:09.540 --> 00:29:14.860]   we're doing, obviously we're, we're involved in AML Commons safety working group and and
[00:29:14.860 --> 00:29:20.500]   working with the community there to help standardize and build, build tools and evals.
[00:29:20.500 --> 00:29:23.260]   But yeah, I mean, generally speaking, I'm thinking about the whole product here and
[00:29:23.260 --> 00:29:24.260]   not just the model.
[00:29:24.260 --> 00:29:30.980]   Well, like what kinds of things are you thinking about building in the realm of safety or encouraging
[00:29:30.980 --> 00:29:31.980]   others to build?
[00:29:31.980 --> 00:29:35.940]   Like what kinds of problems you see people running into and how are you fixing that?
[00:29:35.940 --> 00:29:36.940]   Yeah.
[00:29:36.940 --> 00:29:41.900]   I mean, you can look at the executive order as, as, as one, I mean, cybersecurity certainly,
[00:29:41.900 --> 00:29:43.340]   you know, generating insecure code.
[00:29:43.340 --> 00:29:48.260]   I think if you, that was one of the, it was the top thing on the, on the executive order.
[00:29:48.260 --> 00:29:52.060]   There's just, you know, general safety around, you know, input output, you know, to your,
[00:29:52.060 --> 00:29:53.060]   to your prompts.
[00:29:53.060 --> 00:29:56.780]   I'll cut, I mean, there's a number of different harms and we, you know, we have to deal with
[00:29:56.780 --> 00:30:02.380]   obviously as a company, we have a massive platform on Facebook and Instagram and WhatsApp.
[00:30:02.380 --> 00:30:05.380]   And so there's, there's kind of harms that we deal with on a daily basis.
[00:30:05.380 --> 00:30:07.500]   And so internally we're looking at those things.
[00:30:07.500 --> 00:30:11.260]   And I think what we want to go, what we want to do is ultimately, you know, help grow the
[00:30:11.260 --> 00:30:16.780]   community around, you know, our model so they can basically help build these things as well
[00:30:16.780 --> 00:30:18.220]   in the open.
[00:30:18.220 --> 00:30:26.180]   And so you can imagine taking, for example, a model, you know, from say Amazon and then
[00:30:26.180 --> 00:30:27.700]   be able to adapt it to your application.
[00:30:27.700 --> 00:30:31.620]   But then ultimately I don't know if that model is actually safe or not.
[00:30:31.620 --> 00:30:36.220]   You know, how do I actually understand whether it's going to be generating you know, outputs
[00:30:36.220 --> 00:30:38.340]   that's that will create ultimately harms.
[00:30:38.340 --> 00:30:42.060]   I mean, I wouldn't go far as like go crazy with like bio weapons.
[00:30:42.060 --> 00:30:46.900]   I know it's been like the topic, there's your right of like the summit last week.
[00:30:46.900 --> 00:30:51.580]   I think there's like nothing's, nothing's impossible from these models.
[00:30:51.580 --> 00:30:55.980]   But you know, understanding what these risks are and ultimately finding ways to mitigate
[00:30:55.980 --> 00:31:02.100]   them and build that into your products and make your products basically as safe as possible.
[00:31:02.100 --> 00:31:03.100]   That's ultimately my goal.
[00:31:03.100 --> 00:31:06.740]   And I want to not only do that at Meta, but also do that in the open.
[00:31:06.740 --> 00:31:10.380]   I guess when you talk about like safety of inputs and outputs, you know, I feel like
[00:31:10.380 --> 00:31:12.140]   everyone says that, but it's a little vague.
[00:31:12.140 --> 00:31:14.700]   Like how do you, like, can you be like a little more concrete?
[00:31:14.700 --> 00:31:21.500]   Well, I mean, I think this is part of the challenge, you know, we have, I think.
[00:31:21.500 --> 00:31:26.740]   So for example, if I were to hypothetically put out tools that allowed you to understand,
[00:31:26.740 --> 00:31:28.700]   you know, the input safety of a prompt.
[00:31:28.700 --> 00:31:34.380]   So say, you know, you, you know, Lucas Biewald wanted to ask my model here, ask Lama to build
[00:31:34.380 --> 00:31:36.100]   a bomb.
[00:31:36.100 --> 00:31:38.420]   You know, that's, that's obviously an unsafe input.
[00:31:38.420 --> 00:31:44.020]   I think everyone would probably agree that that's like, you know, pretty generally speaking,
[00:31:44.020 --> 00:31:47.540]   something that was bad or a bad, bad thing to ask for or a bad thing to generate an output
[00:31:47.540 --> 00:31:48.540]   for.
[00:31:48.540 --> 00:31:51.900]   But there's a lot of gray area here too.
[00:31:51.900 --> 00:31:57.600]   And like, our policy isn't, you know, may not be like Amazon's policy or may not be
[00:31:57.600 --> 00:31:58.980]   like someone else's policy.
[00:31:58.980 --> 00:32:04.540]   So unfortunately, like, yeah, I mean, it's going to vary from, from party to party, but
[00:32:04.540 --> 00:32:07.740]   I think this is also why we want to work with the community and especially ML Commons to
[00:32:07.740 --> 00:32:12.500]   standardize because like our perspective may not be everyone's perspective on what a harm
[00:32:12.500 --> 00:32:13.500]   is.
[00:32:13.500 --> 00:32:16.540]   I mean, we deal with all kinds of crazy stuff on our platform today.
[00:32:16.540 --> 00:32:23.700]   I mean, you know, sexual content and, and human trafficking and all the terrible things
[00:32:23.700 --> 00:32:25.740]   that, you know, that we try and mitigate.
[00:32:25.740 --> 00:32:30.660]   I mean, these are, these are just horrendous things that, you know, we want to, we obviously
[00:32:30.660 --> 00:32:36.420]   want to keep off our platform and, and, and, and we deal with integrity issues on a constant
[00:32:36.420 --> 00:32:37.420]   basis.
[00:32:37.420 --> 00:32:40.340]   And the question is like, you know, how do we do that?
[00:32:40.340 --> 00:32:43.500]   Obviously we have tools internally, but then, you know, what is the community dealing with?
[00:32:43.500 --> 00:32:47.420]   How do we create a framework so that your policy, that you can adapt to custom policy
[00:32:47.420 --> 00:32:48.620]   and so on.
[00:32:48.620 --> 00:32:52.420]   So I think that's, and this again, it goes back to like ML Commons and us working to
[00:32:52.420 --> 00:32:56.740]   standardize these things and ultimately giving, you know, companies the freedom to define
[00:32:56.740 --> 00:33:00.740]   their policies themselves.
[00:33:00.740 --> 00:33:06.700]   But I guess, I mean, Lama 2 probably has embedded in it some of these safety mechanisms, right?
[00:33:06.700 --> 00:33:08.860]   That you're probably trying to make hard to override.
[00:33:08.860 --> 00:33:12.500]   Like could I, could I get Lama 2 to tell me how to make a bomb?
[00:33:12.500 --> 00:33:17.900]   Like, does it know that somewhere inside of it or, or, or not?
[00:33:17.900 --> 00:33:22.020]   I mean, ultimately it's training a lot of data, so it can generate things, you know,
[00:33:22.020 --> 00:33:25.740]   you, any, with any of these models, I think you could prompt it in a way, or you could,
[00:33:25.740 --> 00:33:30.700]   you could probably prompt it in ways that would, would eventually allow you to do that.
[00:33:30.700 --> 00:33:39.060]   I think it's, it's a matter of, of, of maybe determination or, you know, there's, if there's
[00:33:39.060 --> 00:33:41.180]   a ton of ways to jailbreak these models.
[00:33:41.180 --> 00:33:45.180]   I think we have, we have a lot of data though, that shows how safe we are.
[00:33:45.180 --> 00:33:47.500]   I think that we've done internal studies.
[00:33:47.500 --> 00:33:50.900]   We've obviously, this is one of the reasons why we love having our models out there is
[00:33:50.900 --> 00:33:54.900]   because researchers are obviously are, are going to be kind of crowdsourcing.
[00:33:54.900 --> 00:33:59.780]   We have a crowdsource, you know, kind of researchers who are, are banging on our models, doing
[00:33:59.780 --> 00:34:04.060]   publishing papers, understanding the bias in our models, understanding, you know, how
[00:34:04.060 --> 00:34:05.500]   to, how to break down.
[00:34:05.500 --> 00:34:11.420]   I mean, how to manipulate the system problems, which is a problem we had, you know, for,
[00:34:11.420 --> 00:34:13.900]   for a while until we moved that.
[00:34:13.900 --> 00:34:17.900]   So I think like there, this is why, why I think being open actually helpful.
[00:34:17.900 --> 00:34:21.000]   And then you can understand these things better and then you can mitigate them and either
[00:34:21.000 --> 00:34:23.140]   the current generation and the next generation.
[00:34:23.140 --> 00:34:27.100]   So, but yeah, I mean, ultimately like these models can be manipulated.
[00:34:27.100 --> 00:34:31.980]   I think this is why I think in my opinion, like trust and safety is like a, is one of
[00:34:31.980 --> 00:34:34.540]   the most important things that we need to deal with.
[00:34:34.540 --> 00:34:39.580]   And we need to do that in the open and how we, how we build these tools, how we evaluate
[00:34:39.580 --> 00:34:45.060]   these, these, these models, you know, how we do it, not only unilaterally, but as a
[00:34:45.060 --> 00:34:46.580]   community.
[00:34:46.580 --> 00:34:51.700]   But isn't there like a danger in open sourcing a model that you can't take it back?
[00:34:51.700 --> 00:34:56.300]   Like if somebody does find a way to get this kind of, you know, maybe unsafe information
[00:34:56.300 --> 00:35:00.220]   out of the model, there's no way to, to put the genie back in the bottle.
[00:35:00.220 --> 00:35:01.220]   Right.
[00:35:01.220 --> 00:35:05.540]   I mean, it's, it's kind of the nature of open source in a lot of ways.
[00:35:05.540 --> 00:35:11.380]   So I think ultimately like the harms, the, the good I'd say, you know, should outweigh
[00:35:11.380 --> 00:35:12.380]   the harms.
[00:35:12.380 --> 00:35:21.380]   I think that if you look at, you know, kind of the open versus closed argument, you know,
[00:35:21.380 --> 00:35:25.900]   I would, I would much rather have a world where we have a, you know, maybe, maybe I'll
[00:35:25.900 --> 00:35:29.020]   use the OS analogy here.
[00:35:29.020 --> 00:35:30.700]   You know, Windows versus Linux, right.
[00:35:30.700 --> 00:35:35.940]   Am I, am I going to be operating and, and, and working in a Linux environment, you know,
[00:35:35.940 --> 00:35:39.780]   where I can inspect the kernel, I can, you know, I can build on it.
[00:35:39.780 --> 00:35:41.660]   It's transparent.
[00:35:41.660 --> 00:35:44.940]   Or am I going to be like operating in, in say like a Windows closed source environment
[00:35:44.940 --> 00:35:51.660]   where, you know, I, I trust my, my overlords or, or, or my paternalistic you know, owner
[00:35:51.660 --> 00:35:54.940]   of that platform to, to make sure that everything is okay.
[00:35:54.940 --> 00:36:00.220]   And that, I mean, there's a, there's a case I think maybe to be made for both certainly.
[00:36:00.220 --> 00:36:06.700]   And I like, I really think transparent and open is kind of the way, and I think the good
[00:36:06.700 --> 00:36:11.900]   that comes out of this, like the democratization that happens when, when you build in the open,
[00:36:11.900 --> 00:36:16.100]   I think is I mean, it's it's amazing.
[00:36:16.100 --> 00:36:20.380]   Like the number of startups that are building on a lot of models I'm hearing numbers like
[00:36:20.380 --> 00:36:24.540]   in the thousands, like and I actually don't think that's hyperbole.
[00:36:24.540 --> 00:36:29.140]   I think enterprises are building on it because they can't build their own foundation models
[00:36:29.140 --> 00:36:32.420]   and they don't want to ship data, for example, into others.
[00:36:32.420 --> 00:36:37.860]   They'd rather maybe run this on the edge or run it in their own data centers in places
[00:36:37.860 --> 00:36:42.140]   where they don't have to send data back to, to another cloud or to another service.
[00:36:42.140 --> 00:36:48.380]   So like, I, there's just so many positives in, in being open about this.
[00:36:48.380 --> 00:36:55.220]   And yeah, I think it's, it's such an important thing that we have a, we have good balance
[00:36:55.220 --> 00:36:59.780]   I would say in the discussion between open and closed.
[00:36:59.780 --> 00:37:02.180]   What about the, the data sets?
[00:37:02.180 --> 00:37:07.420]   I think you don't actually release the data set it's trained on if I remember right.
[00:37:07.420 --> 00:37:11.460]   So, so would you ever open up the data set or how do you, like, how does that, how do
[00:37:11.460 --> 00:37:12.460]   you think about that?
[00:37:12.460 --> 00:37:18.660]   Yeah, I mean, we've released quite a few data sets over the years.
[00:37:18.660 --> 00:37:22.900]   In this case, like, you know, we were not for, we're not releasing our data sets, you
[00:37:22.900 --> 00:37:27.300]   know, for a number of reasons, you know, competitive reasons.
[00:37:27.300 --> 00:37:30.540]   I think that that's being one I would say.
[00:37:30.540 --> 00:37:35.180]   So but like, I think you can, you can look at the number of data sets and things we've
[00:37:35.180 --> 00:37:37.420]   opened and we may open data sets in the future.
[00:37:37.420 --> 00:37:39.020]   I mean, we're always talking about it.
[00:37:39.020 --> 00:37:40.020]   It's, it's a lot of energy.
[00:37:40.020 --> 00:37:45.500]   If you've ever worked in a big company and you've and you want to release something,
[00:37:45.500 --> 00:37:50.140]   and I would say Meta is probably like the, one of the best companies in, in, in that
[00:37:50.140 --> 00:37:51.140]   terms.
[00:37:51.140 --> 00:37:55.100]   I mean, if you look at Google, Amazon and the companies I've worked for I would say
[00:37:55.100 --> 00:37:58.020]   Meta is definitely the easiest to, to do that.
[00:37:58.020 --> 00:38:00.580]   But we may, may release in the future.
[00:38:00.580 --> 00:38:02.220]   You know, it's, it's up for discussion.
[00:38:02.220 --> 00:38:04.980]   It's a matter of goals, I would say.
[00:38:04.980 --> 00:38:08.620]   What do you think about multilingual capabilities?
[00:38:08.620 --> 00:38:11.740]   Multilingual is interesting.
[00:38:11.740 --> 00:38:18.100]   I mean, I think we, we've seen, I would say Lama models be fine tuned on a ton of different
[00:38:18.100 --> 00:38:20.260]   languages.
[00:38:20.260 --> 00:38:23.780]   I think it's, it's obviously there's a demand for multilingual.
[00:38:23.780 --> 00:38:27.220]   There's, you know, not, not everything is English, right?
[00:38:27.220 --> 00:38:30.860]   We, we have a very North American centric, you know, view where you and I are sitting
[00:38:30.860 --> 00:38:36.100]   here in Silicon Valley and San Francisco and, and so everything could, you know, is English
[00:38:36.100 --> 00:38:37.500]   and very Americanized.
[00:38:37.500 --> 00:38:39.420]   But that's not the case, right?
[00:38:39.420 --> 00:38:41.500]   It's, it's like the world is super diverse.
[00:38:41.500 --> 00:38:44.740]   There's a lot of languages that are used.
[00:38:44.740 --> 00:38:50.460]   I think I've been pleasantly surprised on how many, how many languages people have kind
[00:38:50.460 --> 00:38:54.300]   of fine tuned on top of Lama then release models.
[00:38:54.300 --> 00:38:58.180]   I think it's the, the challenge of course is like, how do you, how do you do that at
[00:38:58.180 --> 00:39:03.780]   like if we were, for example, to build multilingual capabilities into our models in the future,
[00:39:03.780 --> 00:39:09.100]   like evaluation is obviously you know, something that's interesting and, and language issue.
[00:39:09.100 --> 00:39:13.940]   If you get something slightly wrong, it can say things that maybe we didn't want to say,
[00:39:13.940 --> 00:39:14.940]   right?
[00:39:14.940 --> 00:39:20.220]   So like, I think just being thoughtful about that evaluation again is, is one of the, one
[00:39:20.220 --> 00:39:22.940]   of the biggest concerns.
[00:39:22.940 --> 00:39:29.900]   So like having a robust eval platform, having obviously a diverse data set and it gets harder
[00:39:29.900 --> 00:39:34.860]   and harder as you kind of get into like the low resource languages getting, getting that.
[00:39:34.860 --> 00:39:39.380]   And obviously like having people who speak all those languages is really, really helpful.
[00:39:39.380 --> 00:39:41.540]   So it's, it's definitely a hard problem.
[00:39:41.540 --> 00:39:42.540]   It takes scale.
[00:39:42.540 --> 00:39:46.860]   It takes, you know, you have to be thoughtful about it.
[00:39:46.860 --> 00:39:51.860]   And but I think these models, if you want them to truly be world models in the future,
[00:39:51.860 --> 00:39:57.580]   I think multilinguality is, is, I mean, kind of a basic, basic requirement.
[00:39:57.580 --> 00:40:02.460]   So totally.
[00:40:02.460 --> 00:40:04.100]   What about the tone of the model?
[00:40:04.100 --> 00:40:05.100]   Like it's kind of interesting.
[00:40:05.100 --> 00:40:11.700]   I feel like Lama Chat has a very distinct tone.
[00:40:11.700 --> 00:40:15.380]   Like how much thought was, was put into that?
[00:40:15.380 --> 00:40:20.060]   Tell me, tell me more, tell me more about what do you, what do you think the tone, is
[00:40:20.060 --> 00:40:21.060]   it too chatty?
[00:40:21.060 --> 00:40:24.020]   Is it like, or is it too terse?
[00:40:24.020 --> 00:40:27.100]   Or is it like, I'm kind of curious, like you're, you're, what do you, what do you think?
[00:40:27.100 --> 00:40:33.140]   Oh, well, I mean, this is my only my impression, but I sort of feel like the, the GPT models
[00:40:33.140 --> 00:40:37.780]   feel like, kind of like I'm talking to a boy scout or something like just feels very sort
[00:40:37.780 --> 00:40:41.460]   of like affable and direct.
[00:40:41.460 --> 00:40:46.540]   And I feel like the Lama models feel a little kind of like sillier to me and crazier.
[00:40:46.540 --> 00:40:50.660]   Like I feel like it, it feels like kind of more friendly, kind of like a little bit more
[00:40:50.660 --> 00:40:53.980]   of like a personality, but like less serious.
[00:40:53.980 --> 00:40:56.460]   I'm curious if that was like intentional.
[00:40:56.460 --> 00:40:58.500]   I don't even know if other people have that same impression.
[00:40:58.500 --> 00:41:03.180]   That's just, you know, I've been talking to a lot of these models over the last couple
[00:41:03.180 --> 00:41:09.340]   of months and that's, that's one man's, uh, one man's impression, I guess.
[00:41:09.340 --> 00:41:10.340]   That's interesting.
[00:41:10.340 --> 00:41:13.420]   I mean, I've definitely, I mean, I've played obviously with a lot with Lama and I played
[00:41:13.420 --> 00:41:19.940]   with Bard and played with, with the GPT models and I don't know, I've, I found Lama to be
[00:41:19.940 --> 00:41:20.940]   pretty good.
[00:41:20.940 --> 00:41:25.100]   I think it's, it's like, you always like balance, like helpfulness and safety and all these
[00:41:25.100 --> 00:41:28.820]   different factors and yeah, you have reward models.
[00:41:28.820 --> 00:41:34.580]   And so, uh, I actually found like the GPT models to be fairly terse when it came to
[00:41:34.580 --> 00:41:38.820]   like, when it came to like certain things, like it will, um, like if, if it's going to
[00:41:38.820 --> 00:41:43.420]   do a, like a refusal, like it will like be pretty, pretty terse when it refuses things
[00:41:43.420 --> 00:41:46.260]   like, you know, one sentence, like I can't talk about that or something like that.
[00:41:46.260 --> 00:41:47.260]   I can't remember.
[00:41:47.260 --> 00:41:50.460]   And, uh, like some other models are a little more chatty.
[00:41:50.460 --> 00:41:56.300]   Um, I think, uh, Lama models tend to be a little, little chatty, but actually, you know,
[00:41:56.300 --> 00:41:59.900]   when they refuse, they tend to be more helpful too, from my experience.
[00:41:59.900 --> 00:42:03.820]   So if like, if we were, if you mentioned, for example, um, you know, something, I don't
[00:42:03.820 --> 00:42:09.900]   know, bad happening or something like it will, it, it won't just like, um, it will just like
[00:42:09.900 --> 00:42:14.060]   flat out refuse you actually will like provide, um, I don't know, here's like a hotline or
[00:42:14.060 --> 00:42:17.300]   something like here's a, here's like, you know, where you can call for it to get help
[00:42:17.300 --> 00:42:18.540]   or whatever it is.
[00:42:18.540 --> 00:42:22.500]   So it's, so even when it's, it's like actually kind of refusing, it actually will come in
[00:42:22.500 --> 00:42:23.500]   and provide something helpful.
[00:42:23.500 --> 00:42:26.580]   And I think that was like a balance that you have to strike.
[00:42:26.580 --> 00:42:31.900]   And that's, in some ways that's probably a lot of like the, like the, the inputs you
[00:42:31.900 --> 00:42:35.740]   got from humans when you're generating your, you know, your reward model and the RLH, uh,
[00:42:35.740 --> 00:42:36.740]   early Jeff.
[00:42:36.740 --> 00:42:40.340]   So like, um, in some ways it's, it's, it's harder.
[00:42:40.340 --> 00:42:44.580]   I think maybe ultimately these models like reflect, you know, what they were trained
[00:42:44.580 --> 00:42:50.740]   on and, and, and you know, the SFT data and, and everything that you put into them in terms
[00:42:50.740 --> 00:42:54.340]   of a reward model and so on, they, they ultimately reflect those things.
[00:42:54.340 --> 00:42:58.620]   Um, so I think we just maybe had a different sampling of people, maybe that other models,
[00:42:58.620 --> 00:42:59.620]   I'm not sure.
[00:42:59.620 --> 00:43:03.180]   Um, but I think it's actually interesting that he found that observation.
[00:43:03.180 --> 00:43:08.140]   Or maybe different guidelines in the, um, the early Jeff, like, um, do you actually
[00:43:08.140 --> 00:43:11.500]   release the, the guidelines that you use?
[00:43:11.500 --> 00:43:14.980]   I'd imagine that's a pretty important piece.
[00:43:14.980 --> 00:43:19.420]   Yeah, we talk, I think we talked a lot about it in the paper, um, but the reward models
[00:43:19.420 --> 00:43:20.420]   are not open.
[00:43:20.420 --> 00:43:21.420]   Um, well, okay.
[00:43:21.420 --> 00:43:28.420]   I guess this is a question I get constantly.
[00:43:28.420 --> 00:43:34.580]   I'm curious how you, uh, what, what are like, what do you feel like are the biggest kind
[00:43:34.580 --> 00:43:42.060]   of working applications that you see of, of Lama too, especially in a business context?
[00:43:42.060 --> 00:43:49.900]   Yeah, it's not the sexiest applications, uh, to be honest, like, um, you know what I mean?
[00:43:49.900 --> 00:43:55.740]   It's like, it's, it's like summarization is like, honestly, one of like the, it's like
[00:43:55.740 --> 00:43:58.020]   the unsung hero of Gen AI.
[00:43:58.020 --> 00:44:03.140]   Um, I was like chatting with somebody, um, the CTO of a medical company and we were having
[00:44:03.140 --> 00:44:08.740]   dinner at the same, uh, like last month and they're using Lama too.
[00:44:08.740 --> 00:44:10.740]   And I'm like, so what do you, what are you doing?
[00:44:10.740 --> 00:44:11.740]   What are they doing?
[00:44:11.740 --> 00:44:12.740]   Some crazy stuff.
[00:44:12.740 --> 00:44:16.820]   Like, um, are you, you know, and he's like, honestly, like, if you think about your medical
[00:44:16.820 --> 00:44:21.740]   records and maybe some people have really long medical records, um, they have like 500
[00:44:21.740 --> 00:44:22.740]   pages of records.
[00:44:22.740 --> 00:44:27.900]   And if you just want to summarize it, like in plain language, um, in a way that you can
[00:44:27.900 --> 00:44:34.900]   easily read it as a, you know, either as a doctor or ultimately, or as a patient, I mean,
[00:44:34.900 --> 00:44:36.740]   you could do that with, with an LLM and it's great.
[00:44:36.740 --> 00:44:39.020]   Um, and you can ask a question then.
[00:44:39.020 --> 00:44:43.580]   Um, and so like, I really think like, there's like, we're, while we're getting super excited
[00:44:43.580 --> 00:44:49.940]   about like a lot of the more far flung applications of Gen AI, like we're kind of forgetting that
[00:44:49.940 --> 00:44:53.100]   it brings some level of value just for basic things like that.
[00:44:53.100 --> 00:44:54.700]   So, um, I see a lot of that.
[00:44:54.700 --> 00:44:59.220]   I think, um, you know, like zoom, for example, like, you know, they, they have agents that
[00:44:59.220 --> 00:45:03.060]   deploy when you're, you know, you have meetings and it'll summarize your meeting for you and
[00:45:03.060 --> 00:45:05.980]   give you like the salient, you know, highlights.
[00:45:05.980 --> 00:45:08.820]   I was actually just talking about this with someone, um, yesterday.
[00:45:08.820 --> 00:45:12.940]   Um, um, like over the weekend, I was having coffee with somebody and it's like, damn,
[00:45:12.940 --> 00:45:16.220]   I wish there was like a, an agent that, you know, it was like private and all that.
[00:45:16.220 --> 00:45:21.700]   It would just like sit with me in my meeting and summarize and just, just take notes and
[00:45:21.700 --> 00:45:26.380]   in a private way, you know, summarize things at the end of the day, like, cause I'm tired
[00:45:26.380 --> 00:45:27.380]   at the end of the day.
[00:45:27.380 --> 00:45:29.340]   I have the, I started my days at like six or six 30.
[00:45:29.340 --> 00:45:33.900]   Um, I go to a late and like, I would love to have a two page, just like summary of all
[00:45:33.900 --> 00:45:37.860]   the notes, all the salient things, all the actions, everything that someone's expecting
[00:45:37.860 --> 00:45:41.580]   from me, like, you know, at the end of the day, like, Oh my God, that would be like a
[00:45:41.580 --> 00:45:42.580]   lifesaver.
[00:45:42.580 --> 00:45:48.100]   Um, cause you know, when you're in back to back meetings for 10 hours or nine hours and,
[00:45:48.100 --> 00:45:51.860]   uh, yeah, whatever, take notes or whatever, you can't remember everything.
[00:45:51.860 --> 00:45:54.160]   So, um, especially when you're multitasking, right.
[00:45:54.160 --> 00:45:58.500]   People ping you like I get all these chats and, and people call on me or whatever.
[00:45:58.500 --> 00:46:02.460]   Uh, so like even basic things like that I think would be like incredibly valuable people.
[00:46:02.460 --> 00:46:07.740]   So like I'm biasing right now towards like really like utility, but obviously the innovation
[00:46:07.740 --> 00:46:08.740]   is still coming.
[00:46:08.740 --> 00:46:12.900]   So, so you worked on, you worked on PyTorch and then you worked with, with the Jax team.
[00:46:12.900 --> 00:46:19.460]   Was there any like, um, was there anything that you learned or saw, um, from Jax that
[00:46:19.460 --> 00:46:24.220]   you would want to like take into PyTorch?
[00:46:24.220 --> 00:46:28.300]   You know, it's funny, like the Jax, I, first of all, I love the Jax team.
[00:46:28.300 --> 00:46:29.620]   Like they're, they're so cool.
[00:46:29.620 --> 00:46:35.500]   Like they remind me very much of like the early days of, of PyTorch, small team, uh,
[00:46:35.500 --> 00:46:39.900]   four researchers by researchers, you know, Sky and Matt and the team, they're just like
[00:46:39.900 --> 00:46:41.460]   so cool to hang out with.
[00:46:41.460 --> 00:46:46.980]   So I would go up to San Francisco just to try and find time and hang out with them and
[00:46:46.980 --> 00:46:49.900]   James Bradbury, James Bradbury is my late anthropic.
[00:46:49.900 --> 00:46:53.140]   And so like, I, I'm trying to hang out with Sky cause she's super cool.
[00:46:53.140 --> 00:46:54.500]   And, and just like chat.
[00:46:54.500 --> 00:47:01.900]   And I think the, like, I don't know, it's, it's, um, on one hand, like I loved the way
[00:47:01.900 --> 00:47:06.460]   they just were able to just shut everything out around them and just keep building their
[00:47:06.460 --> 00:47:09.060]   core framework no matter what happens around them.
[00:47:09.060 --> 00:47:14.540]   They just, they had this like uncanny ability to just like push away any distractions that,
[00:47:14.540 --> 00:47:19.660]   that, you know, like the production teams or like anyone, um, would, would come and
[00:47:19.660 --> 00:47:21.980]   like tell them what to build, maybe like, okay, yeah, whatever.
[00:47:21.980 --> 00:47:25.220]   Um, and they would just keep building what they thought was awesome.
[00:47:25.220 --> 00:47:26.980]   And uh, it is awesome.
[00:47:26.980 --> 00:47:32.380]   Um, I think the, you know, like, so I think like, to me, that's like one of the, one of
[00:47:32.380 --> 00:47:37.860]   the best like learnings I had is like, just if you have like some of the best teams in
[00:47:37.860 --> 00:47:43.620]   my opinion are the ones that are like built by like four years, built, you know, like
[00:47:43.620 --> 00:47:44.620]   built by user.
[00:47:44.620 --> 00:47:48.620]   A lot of what you did, for example, with Weights & Biases, frankly, like, I mean, you yourself,
[00:47:48.620 --> 00:47:51.300]   like, you know, or a user in building Weights & Biases.
[00:47:51.300 --> 00:47:55.660]   And when you demoed, like, I mean, I'm trying to think how many years ago that was when
[00:47:55.660 --> 00:47:57.260]   you demoed to Simith and I, right.
[00:47:57.260 --> 00:47:59.940]   And we were like, we looked at each other after the meeting and were like, holy shit.
[00:47:59.940 --> 00:48:00.940]   Oh, really?
[00:48:00.940 --> 00:48:01.940]   I didn't know that.
[00:48:01.940 --> 00:48:03.820]   I actually didn't know that landed.
[00:48:03.820 --> 00:48:04.820]   That's so sweet.
[00:48:04.820 --> 00:48:05.820]   Okay.
[00:48:05.820 --> 00:48:06.820]   You did.
[00:48:06.820 --> 00:48:07.820]   No, it was awesome.
[00:48:07.820 --> 00:48:08.820]   We were like, we looked at each other.
[00:48:08.820 --> 00:48:09.820]   Like, this is, this is awesome.
[00:48:09.820 --> 00:48:11.500]   Like, this was such a cool platform.
[00:48:11.500 --> 00:48:17.540]   Like, and it's, it just like, it felt like it was built by someone who had a lot of user
[00:48:17.540 --> 00:48:18.820]   empathy.
[00:48:18.820 --> 00:48:20.660]   And that's pretty rare in these days.
[00:48:20.660 --> 00:48:21.660]   Right.
[00:48:21.660 --> 00:48:26.300]   Cause you like product managers typically don't like, aren't typically hands-on with these
[00:48:26.300 --> 00:48:30.960]   things or like, you know, a lot of these startups that I see building platforms, like a lot
[00:48:30.960 --> 00:48:35.660]   of these people have never actually shipped production LLMs yet they're building systems
[00:48:35.660 --> 00:48:38.740]   and platforms that are, you know, supposed to be great.
[00:48:38.740 --> 00:48:39.740]   Right.
[00:48:39.740 --> 00:48:43.500]   So like, you know, so Jax, I think it was really interesting.
[00:48:43.500 --> 00:48:49.220]   I think Jax though, where Jax's challenges were is actually, it's funny because I look
[00:48:49.220 --> 00:48:55.020]   at Jax where it is today and it builds to me, like, at least in, in terms of like processes
[00:48:55.020 --> 00:49:00.140]   and like instability, it reminds me of like where PyGeorge was maybe two, three years
[00:49:00.140 --> 00:49:01.140]   ago.
[00:49:01.140 --> 00:49:05.620]   Um, a lot of ways it's, it's probably further ahead than I'm giving it credit for, but.
[00:49:05.620 --> 00:49:10.500]   You know, things like feature maturity, things like forward and backward compatibility.
[00:49:10.500 --> 00:49:16.460]   Um, you know, I think, you know, last I checked it, like it was, you know, there was, there
[00:49:16.460 --> 00:49:22.140]   was no, like there was no semblance, I would say of like backward compatibility.
[00:49:22.140 --> 00:49:24.860]   So they were, you know, as a user, like I'm pretty annoying cause I'm constantly being
[00:49:24.860 --> 00:49:26.980]   broken, right.
[00:49:26.980 --> 00:49:31.300]   Breaking changes are really annoying, but this is stuff you kind of like learn along
[00:49:31.300 --> 00:49:32.300]   the way.
[00:49:32.300 --> 00:49:37.900]   Like when we first like started releasing, you know, like the early versions of PyGeorge,
[00:49:37.900 --> 00:49:42.460]   like 0.4 onward that were like built for, for like large scale usage.
[00:49:42.460 --> 00:49:44.420]   Like we had no semblance of like breaking changes.
[00:49:44.420 --> 00:49:46.900]   Um, like we, we would just break people, right.
[00:49:46.900 --> 00:49:49.540]   We didn't know, but then we like really adapted.
[00:49:49.540 --> 00:49:55.100]   We understood like, there you go, man.
[00:49:55.100 --> 00:49:56.100]   But you learn, right.
[00:49:56.100 --> 00:50:00.020]   And the only way to learn these lessons, I sadly think is like by actually like living
[00:50:00.020 --> 00:50:04.020]   through that and like getting yelled at by users, like, Hey, why did you break me?
[00:50:04.020 --> 00:50:05.020]   Okay.
[00:50:05.020 --> 00:50:06.020]   Okay.
[00:50:06.020 --> 00:50:07.020]   Okay.
[00:50:07.020 --> 00:50:08.020]   What do we do next?
[00:50:08.020 --> 00:50:10.420]   Like, and we just like learned that, like, you know, we, we issue a warning, right.
[00:50:10.420 --> 00:50:14.220]   When you use an API, then the next time we break you, um, and at least you have a heads
[00:50:14.220 --> 00:50:17.060]   up and you can have a contract with a user.
[00:50:17.060 --> 00:50:22.260]   And so we learned that like over, you know, over time, you know, um, and I think that's
[00:50:22.260 --> 00:50:26.100]   like the, like what I would say the Jax team still needs to grow up a little bit in that
[00:50:26.100 --> 00:50:27.100]   regard.
[00:50:27.100 --> 00:50:30.420]   But it's an awesome framework and it's got incredible user empathy.
[00:50:30.420 --> 00:50:34.700]   Do you not feel like, um, there's also advantages to breaking changes?
[00:50:34.700 --> 00:50:37.860]   Like, I'm a little surprised to hear you, you say it like that.
[00:50:37.860 --> 00:50:42.740]   Cause I kind of thought that PyTorch at the time was taking an intentional point of view
[00:50:42.740 --> 00:50:46.380]   of like, Hey, we're not going to be like saddled by the past.
[00:50:46.380 --> 00:50:50.140]   And we're just going to like, you know, move forward and make, you know, weights and biases
[00:50:50.140 --> 00:50:55.980]   life incredibly hard with every version of like basic primitives changing in each, uh,
[00:50:55.980 --> 00:50:56.980]   day release.
[00:50:56.980 --> 00:51:00.340]   I mean, certainly PyTorch was successful, right?
[00:51:00.340 --> 00:51:05.660]   So do you think you would have been more successful if you had done more backwards compatibility?
[00:51:05.660 --> 00:51:14.020]   Um, I think, you know, I would say backward and forward compatibility are, are, I mean,
[00:51:14.020 --> 00:51:18.260]   they're kind of like a product of the level of maturity, um, for a project.
[00:51:18.260 --> 00:51:21.580]   Like I think in the, in the early days of a project, I think it's totally cool to be
[00:51:21.580 --> 00:51:22.580]   breaking constantly.
[00:51:22.580 --> 00:51:27.540]   Like, I think, um, you know, when it's, when it's a research project, when you're, when
[00:51:27.540 --> 00:51:31.980]   you're in kind of this high entropy state where you're still figuring things out and
[00:51:31.980 --> 00:51:33.540]   you kind of have something that's interesting.
[00:51:33.540 --> 00:51:38.100]   Um, but like the community, like, but, but the ecosystem around you is like, is innovating
[00:51:38.100 --> 00:51:39.340]   rapidly.
[00:51:39.340 --> 00:51:41.300]   Like actually think that's totally fine.
[00:51:41.300 --> 00:51:43.900]   Um, and as long as you like communicate, right.
[00:51:43.900 --> 00:51:49.140]   As long as you like, are in constant communication with your users and like you tell them you're
[00:51:49.140 --> 00:51:50.140]   going to break.
[00:51:50.140 --> 00:51:53.380]   The worst thing is when you don't tell your users that you're going to break down and
[00:51:53.380 --> 00:51:57.500]   they they're surprised as a user, that's super entertaining.
[00:51:57.500 --> 00:51:58.500]   Right.
[00:51:58.500 --> 00:51:59.500]   That's so frustrating.
[00:51:59.500 --> 00:52:01.380]   So I think you're like, it is a product of maturity.
[00:52:01.380 --> 00:52:06.580]   Like it is as PyTorch became more and more used by production, not only inside like meta,
[00:52:06.580 --> 00:52:12.220]   but like, and in the world, like you can't just break production users, like, because
[00:52:12.220 --> 00:52:17.140]   they've, you know, they've, they've taken a bet on you implicitly and you know, their
[00:52:17.140 --> 00:52:21.820]   products and I remember when Microsoft like made a bet on, on PyTorch and you know, they
[00:52:21.820 --> 00:52:26.180]   obviously have Onyx and it's doing these Onyx today, but they made this huge bet on it.
[00:52:26.180 --> 00:52:31.620]   And you know, to be a $2 trillion company, taking a bet on a framework that another company
[00:52:31.620 --> 00:52:33.980]   is, is like is building.
[00:52:33.980 --> 00:52:35.500]   That was a huge leap of faith for Microsoft.
[00:52:35.500 --> 00:52:41.020]   I mean, open AI as well, when they, they took a bet on PyTorch with us and yeah, I remember
[00:52:41.020 --> 00:52:44.860]   meeting with David LeBlanc and the team in San Francisco and they're like, yep, we're
[00:52:44.860 --> 00:52:49.980]   betting on PyTorch and like, obviously like, you know, they built GPD 3 on it and GPD 4
[00:52:49.980 --> 00:52:54.660]   and you know, so it's like, you know, it's, it's a, it's a leap of faith at the end of
[00:52:54.660 --> 00:52:55.660]   the day.
[00:52:55.660 --> 00:53:02.260]   And, um, and so like, ultimately it, it, um, you have to think not only about your needs
[00:53:02.260 --> 00:53:05.700]   and what you, what your company needs, but also like what, you know, what the community
[00:53:05.700 --> 00:53:10.780]   needs and that forces you to think a little bit different and take different risks or,
[00:53:10.780 --> 00:53:15.700]   or push some of the entropy into like repos that are outside of the core where you can
[00:53:15.700 --> 00:53:17.180]   kind of, you know, figure things out.
[00:53:17.180 --> 00:53:21.580]   Cause we spent a lot of time with PyTorch, like modularizing, uh, the code base.
[00:53:21.580 --> 00:53:25.740]   And it's still to this day, you know, we spend a lot of time basically every time we think
[00:53:25.740 --> 00:53:30.420]   about a new paradigm or a new API or something, you know, it sits outside the core.
[00:53:30.420 --> 00:53:33.540]   We can understand whether there's value, we can graduate it.
[00:53:33.540 --> 00:53:39.060]   Um, you know, if we see that we want to support a longterm, um, but ultimately like people
[00:53:39.060 --> 00:53:44.540]   now are so relying on PyTorch, it's, you know, breaking it down is so hard, right?
[00:53:44.540 --> 00:53:47.620]   It's a, it's not, you need to be thoughtful about it.
[00:53:47.620 --> 00:53:49.500]   And so, you know, processes evolve, right?
[00:53:49.500 --> 00:53:51.580]   We, we learn how to deal with it.
[00:53:51.580 --> 00:53:56.180]   Um, and the community adapts to it and they, they come to expect, okay, well I got a warning.
[00:53:56.180 --> 00:53:59.420]   Ah, I better change my code cause next release, right?
[00:53:59.420 --> 00:54:01.140]   Things are going to change and people do it.
[00:54:01.140 --> 00:54:02.140]   So
[00:54:02.140 --> 00:54:08.740]   What are other like major differences in culture that you really feel like?
[00:54:08.740 --> 00:54:13.020]   What's, what is it, what is it like to be working at Meta versus working at Google on
[00:54:13.020 --> 00:54:16.220]   these, these big open source projects?
[00:54:16.220 --> 00:54:25.740]   Um, I mean, culturally I can tell you, uh, in terms of open source, a very different
[00:54:25.740 --> 00:54:35.180]   say, um, at Google, it was more of a question of open source on something like, as in, okay,
[00:54:35.180 --> 00:54:39.260]   we built this saying, like, you know, maybe after the fact, like, should we, should we
[00:54:39.260 --> 00:54:40.260]   open source it?
[00:54:40.260 --> 00:54:41.260]   I don't know.
[00:54:41.260 --> 00:54:42.260]   Maybe, I don't know.
[00:54:42.260 --> 00:54:43.260]   It's gonna be hard.
[00:54:43.260 --> 00:54:49.780]   Let's align like a bunch of VPs because you know, you know, um, no one agrees on what
[00:54:49.780 --> 00:54:50.780]   to do.
[00:54:50.780 --> 00:54:54.620]   Um, whereas Meta, I think the going assumption for just about everything is that it's going
[00:54:54.620 --> 00:54:58.900]   to be open source or we will, we will think about open sourcing it.
[00:54:58.900 --> 00:55:01.100]   Now let's talk about, you know, number one, our goals.
[00:55:01.100 --> 00:55:03.700]   Um, number two, like what success looks like.
[00:55:03.700 --> 00:55:07.340]   Um, you know, what, what kind of a community should we build?
[00:55:07.340 --> 00:55:10.500]   Like, um, like what, how would it license it based on our goals?
[00:55:10.500 --> 00:55:14.540]   Um, so it's kind of like a built in assumption here that we're going to be open about a lot
[00:55:14.540 --> 00:55:16.340]   of our technology.
[00:55:16.340 --> 00:55:20.740]   Whereas like that assumption at Google, uh, in my experience anyway, and this is just
[00:55:20.740 --> 00:55:25.980]   my time there is that it's, it's not, it's not an assumption that things will be open.
[00:55:25.980 --> 00:55:29.380]   It's, you know, it, it definitely, it was harder.
[00:55:29.380 --> 00:55:30.380]   Right.
[00:55:30.380 --> 00:55:32.380]   It's just a, it's just a cultural difference.
[00:55:32.380 --> 00:55:36.740]   I think that in the companies, um, and, and how they view these things, um, again, just
[00:55:36.740 --> 00:55:40.060]   probably my experience in a lot of ways, but, um, it was a lot harder.
[00:55:40.060 --> 00:55:41.620]   Open source was a lot harder there.
[00:55:41.620 --> 00:55:46.340]   Um, and I think the, the other thing that was different that I found is, and this is
[00:55:46.340 --> 00:55:49.540]   more of an ethos that I've adopted over the years is, you know, if you're going to open
[00:55:49.540 --> 00:55:54.020]   source something, you know, you absolutely need to support it and you need the team that's
[00:55:54.020 --> 00:55:56.980]   developing it to, to be there to support it.
[00:55:56.980 --> 00:55:59.180]   And I'm not saying Lama is perfect.
[00:55:59.180 --> 00:56:04.420]   I mean, I'm, we're, you know, we're, we're fixing issues as quickly as we can and we're
[00:56:04.420 --> 00:56:07.020]   trying to support the community as best we can.
[00:56:07.020 --> 00:56:08.900]   Um, but we do have processes, right?
[00:56:08.900 --> 00:56:13.300]   We have weekly triage meetings that, you know, I've been running and then, you know, like,
[00:56:13.300 --> 00:56:17.340]   um, in that, I think the Google is a little bit different.
[00:56:17.340 --> 00:56:20.500]   I think in certain parts of it had that, that level of empathy.
[00:56:20.500 --> 00:56:24.660]   Like I love, for example, the Tensor, uh, Tensor board team, like Nick Felt and those
[00:56:24.660 --> 00:56:28.860]   folks over there, we partnered so closely with them on PyTorch because, you know, we're
[00:56:28.860 --> 00:56:32.940]   thinking about, Hey, we need to, you know, we need a really nice, like visualization
[00:56:32.940 --> 00:56:38.140]   tool, um, like Tensor board was like becoming standard, like how do we remove the TensorFlow
[00:56:38.140 --> 00:56:42.860]   dependency and, and support PyTorch because we don't really want to build something brand
[00:56:42.860 --> 00:56:43.860]   new.
[00:56:43.860 --> 00:56:47.780]   And I remember we removed the TF dependency and, and then basically got it.
[00:56:47.780 --> 00:56:51.700]   So you can just basically import it into PyTorch, like with a single line of code.
[00:56:51.700 --> 00:56:52.700]   It was so, so awesome.
[00:56:52.700 --> 00:56:57.140]   I remember I brought over like half a dozen bottles of Scotch over to Mountain View, um,
[00:56:57.140 --> 00:57:00.740]   gave them out to the folks over there that landed the PRs and we were all like, you know,
[00:57:00.740 --> 00:57:02.820]   having to having a drink and it was so cool.
[00:57:02.820 --> 00:57:04.140]   Um, that team is great.
[00:57:04.140 --> 00:57:06.940]   Um, other parts, like they've struggled, right.
[00:57:06.940 --> 00:57:13.060]   They, um, like they have internal priorities that are overriding support externally, or,
[00:57:13.060 --> 00:57:15.060]   you know, they, they don't prioritize it or whatever.
[00:57:15.060 --> 00:57:19.500]   So it's definitely like, it's a little harder over there, um, than I think it is.
[00:57:19.500 --> 00:57:20.500]   Hmm.
[00:57:20.500 --> 00:57:21.500]   Makes sense.
[00:57:21.500 --> 00:57:25.700]   Um, you didn't give us any Scotch and we integrated with you.
[00:57:25.700 --> 00:57:36.340]   I'm glad to buy you a glass of Scotch, uh, or a bottle.
[00:57:36.340 --> 00:57:37.340]   That's cool.
[00:57:37.340 --> 00:57:38.340]   Either way.
[00:57:38.340 --> 00:57:39.340]   No worries.
[00:57:39.340 --> 00:57:40.340]   We can share a bottle.
[00:57:40.340 --> 00:57:41.340]   All right.
[00:57:41.340 --> 00:57:42.340]   I'm looking forward to that.
[00:57:42.340 --> 00:57:44.500]   Did you want to talk about the, um, AI Learning Alliance?
[00:57:44.500 --> 00:57:49.260]   Um, I saw that you're the co-founder of that and, and I was curious to know, like, how
[00:57:49.260 --> 00:57:51.260]   you think about that and what it does.
[00:57:51.260 --> 00:57:52.260]   Yeah.
