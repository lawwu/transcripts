
[00:00:00.000 --> 00:00:03.240]   The following is a conversation with Regina Barsley.
[00:00:03.240 --> 00:00:06.720]   She's a professor at MIT and a world-class researcher
[00:00:06.720 --> 00:00:08.360]   in natural language processing
[00:00:08.360 --> 00:00:12.480]   and applications of deep learning to chemistry and oncology
[00:00:12.480 --> 00:00:15.360]   or the use of deep learning for early diagnosis,
[00:00:15.360 --> 00:00:18.320]   prevention and treatment of cancer.
[00:00:18.320 --> 00:00:21.040]   She has also been recognized for teaching
[00:00:21.040 --> 00:00:24.720]   of several successful AI related courses at MIT,
[00:00:24.720 --> 00:00:26.840]   including the popular introduction
[00:00:26.840 --> 00:00:28.920]   to machine learning course.
[00:00:28.920 --> 00:00:32.160]   This is the Artificial Intelligence Podcast.
[00:00:32.160 --> 00:00:34.560]   If you enjoy it, subscribe on YouTube,
[00:00:34.560 --> 00:00:36.400]   give it five stars on iTunes,
[00:00:36.400 --> 00:00:39.840]   support it on Patreon or simply connect with me on Twitter
[00:00:39.840 --> 00:00:43.760]   at Lex Friedman spelled F-R-I-D-M-A-N.
[00:00:43.760 --> 00:00:47.760]   And now here's my conversation with Regina Barsley.
[00:00:47.760 --> 00:00:50.320]   In an interview, you've mentioned
[00:00:50.320 --> 00:00:51.960]   that if there's one course you would take,
[00:00:51.960 --> 00:00:54.600]   it would be a literature course with a friend of yours
[00:00:54.600 --> 00:00:56.360]   that a friend of yours teaches.
[00:00:56.360 --> 00:01:00.240]   Just out of curiosity, 'cause I couldn't find anything on it,
[00:01:00.240 --> 00:01:04.400]   are there books or ideas that had profound impact
[00:01:04.400 --> 00:01:06.760]   on your life journey, books and ideas,
[00:01:06.760 --> 00:01:10.760]   perhaps outside of computer science and the technical fields?
[00:01:10.760 --> 00:01:14.680]   - I think because I'm spending a lot of my time at MIT
[00:01:14.680 --> 00:01:18.280]   and previously in other institutions where I was a student,
[00:01:18.280 --> 00:01:21.040]   I have a limited ability to interact with people.
[00:01:21.040 --> 00:01:22.640]   So a lot of what I know about the world
[00:01:22.640 --> 00:01:24.200]   actually comes from books.
[00:01:25.240 --> 00:01:27.240]   And there were quite a number of books
[00:01:27.240 --> 00:01:31.360]   that had profound impact on me and how I view the world.
[00:01:31.360 --> 00:01:35.800]   Let me just give you one example of such a book.
[00:01:35.800 --> 00:01:39.680]   I've maybe a year ago read a book
[00:01:39.680 --> 00:01:42.480]   called "The Emperor of All Melodies."
[00:01:42.480 --> 00:01:45.760]   It's a book about, it's kind of a history of science book
[00:01:45.760 --> 00:01:50.760]   on how the treatments and drugs for cancer were developed.
[00:01:50.760 --> 00:01:53.600]   And that book, despite the fact
[00:01:53.600 --> 00:01:55.480]   that I am in the business of science,
[00:01:55.480 --> 00:01:59.800]   really opened my eyes on how imprecise
[00:01:59.800 --> 00:02:03.080]   and imperfect the discovery process is
[00:02:03.080 --> 00:02:05.840]   and how imperfect our current solutions
[00:02:05.840 --> 00:02:11.080]   and what makes science succeed and be implemented.
[00:02:11.080 --> 00:02:14.120]   And sometimes it's actually not the strength of the idea,
[00:02:14.120 --> 00:02:17.440]   but devotion of the person who wants to see it implemented.
[00:02:17.440 --> 00:02:19.800]   So this is one of the books that,
[00:02:19.800 --> 00:02:22.280]   at least for the last year, quite changed the way
[00:02:22.280 --> 00:02:24.920]   I'm thinking about scientific process
[00:02:24.920 --> 00:02:26.680]   just from the historical perspective
[00:02:26.680 --> 00:02:28.920]   and what do I need to do
[00:02:28.920 --> 00:02:33.440]   to make my ideas really implemented.
[00:02:33.440 --> 00:02:36.040]   Let me give you an example of a book,
[00:02:36.040 --> 00:02:39.560]   which is not kind of, which is a fiction book,
[00:02:39.560 --> 00:02:43.200]   is a book called "Americana."
[00:02:43.200 --> 00:02:48.760]   And this is a book about a young female student
[00:02:48.760 --> 00:02:53.240]   who comes from Africa to study in the United States.
[00:02:53.240 --> 00:02:57.760]   And it describes her path within her studies
[00:02:57.760 --> 00:03:00.600]   and her life transformation that,
[00:03:00.600 --> 00:03:06.560]   in a new country and kind of adaptation to a new culture.
[00:03:06.560 --> 00:03:09.520]   And when I read this book,
[00:03:09.520 --> 00:03:13.520]   I saw myself in many different points of it,
[00:03:14.920 --> 00:03:19.920]   but it also kind of gave me the lens on different events.
[00:03:19.920 --> 00:03:22.080]   And some events that I never actually paid attention,
[00:03:22.080 --> 00:03:24.720]   one of the funny stories in this book
[00:03:24.720 --> 00:03:29.720]   is how she arrives to her new college
[00:03:29.720 --> 00:03:32.920]   and she starts speaking in English
[00:03:32.920 --> 00:03:35.720]   and she had this beautiful British accent
[00:03:35.720 --> 00:03:39.840]   because that's how she was educated in her country.
[00:03:39.840 --> 00:03:40.960]   This is not my case.
[00:03:40.960 --> 00:03:44.640]   And then she notices that the person who talks to her,
[00:03:44.640 --> 00:03:48.280]   talks to her in a very funny way, in a very slow way.
[00:03:48.280 --> 00:03:50.560]   And she's thinking that this woman is disabled
[00:03:50.560 --> 00:03:54.440]   and she's also trying to kind of to accommodate her.
[00:03:54.440 --> 00:03:56.640]   And then after a while, when she finishes her discussion
[00:03:56.640 --> 00:03:58.560]   with this officer from her college,
[00:03:58.560 --> 00:04:02.040]   she sees how she interacts with other students,
[00:04:02.040 --> 00:04:06.920]   with American students, and she discovers that actually
[00:04:06.920 --> 00:04:09.640]   she talked to her this way
[00:04:09.640 --> 00:04:12.160]   because she saw that she doesn't understand English.
[00:04:12.160 --> 00:04:15.040]   And I thought, wow, this is a funny experience.
[00:04:15.040 --> 00:04:17.960]   And literally within a few weeks,
[00:04:17.960 --> 00:04:21.880]   I went to LA to a conference
[00:04:21.880 --> 00:04:24.320]   and I asked somebody in the airport
[00:04:24.320 --> 00:04:26.560]   how to find a cab or something.
[00:04:26.560 --> 00:04:29.360]   And then I noticed that this person is talking
[00:04:29.360 --> 00:04:30.280]   in a very strange way.
[00:04:30.280 --> 00:04:32.080]   And my first thought was that this person
[00:04:32.080 --> 00:04:35.480]   have some pronunciation issues or something.
[00:04:35.480 --> 00:04:37.160]   And I'm trying to talk very slowly to him
[00:04:37.160 --> 00:04:39.640]   and I was with another professor, Ernst Frankel,
[00:04:39.640 --> 00:04:43.160]   and he's like laughing because it's funny
[00:04:43.160 --> 00:04:45.720]   that I don't get that the guy is talking in this way
[00:04:45.720 --> 00:04:47.000]   because he thinks that I cannot speak.
[00:04:47.000 --> 00:04:50.080]   So it was really kind of mirroring experience
[00:04:50.080 --> 00:04:54.320]   and it led me think a lot about my own experiences
[00:04:54.320 --> 00:04:57.000]   moving from different countries.
[00:04:57.000 --> 00:05:00.240]   So I think that books play a big role
[00:05:00.240 --> 00:05:02.720]   in my understanding of the world.
[00:05:02.720 --> 00:05:05.600]   - On the science question,
[00:05:05.600 --> 00:05:07.520]   you mentioned that it made you discover
[00:05:07.520 --> 00:05:09.840]   that personalities of human beings
[00:05:09.840 --> 00:05:12.480]   are more important than perhaps ideas.
[00:05:12.480 --> 00:05:13.720]   Is that what I heard?
[00:05:13.720 --> 00:05:15.760]   - It's not necessarily that they are more important
[00:05:15.760 --> 00:05:19.200]   than ideas, but I think that ideas on their own
[00:05:19.200 --> 00:05:20.520]   are not sufficient.
[00:05:20.520 --> 00:05:24.720]   And many times, at least at the local horizon,
[00:05:24.720 --> 00:05:29.200]   it's the personalities and their devotion to their ideas
[00:05:29.200 --> 00:05:33.040]   is really that locally changes the landscape.
[00:05:33.040 --> 00:05:37.680]   If you're looking at AI, like let's say 30 years ago,
[00:05:37.680 --> 00:05:40.640]   dark ages of AI or whatever word is symbolic times,
[00:05:40.640 --> 00:05:41.840]   you can use any word.
[00:05:41.840 --> 00:05:44.640]   There were some people,
[00:05:44.640 --> 00:05:46.600]   now we're looking at a lot of that work
[00:05:46.600 --> 00:05:48.720]   and we're kind of thinking this was not really
[00:05:48.720 --> 00:05:50.640]   maybe a relevant work,
[00:05:50.640 --> 00:05:53.040]   but you can see that some people managed to take it
[00:05:53.040 --> 00:05:58.040]   and to make it so shiny and dominate the academic world
[00:05:58.040 --> 00:06:02.320]   and make it to be the standard.
[00:06:02.320 --> 00:06:05.160]   If you look at the area of natural language processing,
[00:06:05.160 --> 00:06:09.120]   it is well-known fact that the reason that statistics
[00:06:09.120 --> 00:06:13.960]   in NLP took such a long time to become mainstream
[00:06:13.960 --> 00:06:16.800]   because there were quite a number of personalities
[00:06:16.800 --> 00:06:18.400]   which didn't believe in this idea
[00:06:18.400 --> 00:06:22.040]   and didn't stop research progress in this area.
[00:06:22.040 --> 00:06:27.040]   So I do not think that kind of asymptotically
[00:06:27.040 --> 00:06:28.880]   maybe personalities matters,
[00:06:28.920 --> 00:06:33.920]   but I think locally it does make quite a bit of impact
[00:06:33.920 --> 00:06:38.920]   and it's generally speeds up the rate of adoption
[00:06:38.920 --> 00:06:41.360]   of the new ideas.
[00:06:41.360 --> 00:06:43.480]   - Yeah, and the other interesting question
[00:06:43.480 --> 00:06:46.480]   is in the early days of particular discipline,
[00:06:46.480 --> 00:06:49.920]   I think you mentioned in that book
[00:06:49.920 --> 00:06:52.320]   was ultimately a book of cancer.
[00:06:52.320 --> 00:06:55.080]   - It's called "The Emperor of All Melodies."
[00:06:55.080 --> 00:06:57.840]   - Yeah, and those melodies included
[00:06:57.840 --> 00:07:00.520]   the trying to, the medicine, was it centered on--
[00:07:00.520 --> 00:07:04.920]   - So it was actually centered on
[00:07:04.920 --> 00:07:07.200]   how people thought of curing cancer.
[00:07:07.200 --> 00:07:10.680]   Like for me, it was really a discovery how people,
[00:07:10.680 --> 00:07:14.120]   what was the science of chemistry behind drug development,
[00:07:14.120 --> 00:07:17.240]   that it actually grew up out of dyeing,
[00:07:17.240 --> 00:07:21.920]   like coloring industry, that people who develop chemistry
[00:07:21.920 --> 00:07:25.080]   in 19th century in Germany and Britain
[00:07:25.080 --> 00:07:28.120]   to do the really new dyes,
[00:07:28.120 --> 00:07:30.160]   they looked at the molecules and identified
[00:07:30.160 --> 00:07:32.120]   that they do certain things to cells.
[00:07:32.120 --> 00:07:34.480]   And from there, the process started
[00:07:34.480 --> 00:07:36.880]   and like historically, yeah, this is fascinating
[00:07:36.880 --> 00:07:38.680]   that they managed to make the connection
[00:07:38.680 --> 00:07:42.280]   and look under the microscope and do all this discovery.
[00:07:42.280 --> 00:07:44.440]   But as you continue reading about it
[00:07:44.440 --> 00:07:48.760]   and you read about how chemotherapy drugs
[00:07:48.760 --> 00:07:50.520]   were actually developed in Boston
[00:07:50.520 --> 00:07:52.480]   and some of them were developed
[00:07:52.480 --> 00:07:57.480]   and Farber, Dr. Farber from Dana-Farber,
[00:07:57.480 --> 00:08:00.480]   you know, how the experiments were done,
[00:08:00.480 --> 00:08:03.320]   that there was some miscalculation,
[00:08:03.320 --> 00:08:05.960]   let's put it this way, and they tried it on the patients
[00:08:05.960 --> 00:08:10.000]   and those were children with leukemia and they died.
[00:08:10.000 --> 00:08:11.640]   And then they tried another modification.
[00:08:11.640 --> 00:08:15.000]   You look at the process, how imperfect is this process?
[00:08:15.000 --> 00:08:17.480]   And you know, like if we're again looking back
[00:08:17.480 --> 00:08:19.160]   like 60 years ago, 70 years ago,
[00:08:19.160 --> 00:08:20.760]   you can kind of understand it.
[00:08:20.760 --> 00:08:23.000]   But some of the stories in this book,
[00:08:23.000 --> 00:08:24.600]   which were really shocking to me,
[00:08:24.600 --> 00:08:28.000]   were really happening, you know, maybe decades ago.
[00:08:28.000 --> 00:08:30.640]   And we still don't have a vehicle
[00:08:30.640 --> 00:08:33.520]   to do it much more fast and effective
[00:08:33.520 --> 00:08:35.640]   and you know, scientific,
[00:08:35.640 --> 00:08:38.200]   the way I'm thinking computer science, scientific.
[00:08:38.200 --> 00:08:40.400]   - So from the perspective of computer science,
[00:08:40.400 --> 00:08:43.160]   you've gotten a chance to work the application
[00:08:43.160 --> 00:08:44.880]   to cancer and to medicine in general.
[00:08:44.880 --> 00:08:48.440]   From a perspective of an engineer and a computer scientist,
[00:08:48.440 --> 00:08:51.800]   how far along are we from understanding the human body,
[00:08:51.800 --> 00:08:55.160]   biology, of being able to manipulate it
[00:08:55.160 --> 00:08:57.920]   in a way we can cure some of the maladies,
[00:08:57.920 --> 00:08:59.720]   some of the diseases?
[00:08:59.720 --> 00:09:02.220]   - So this is very interesting question.
[00:09:02.220 --> 00:09:06.040]   And if you're thinking as a computer scientist
[00:09:06.040 --> 00:09:09.840]   about this problem, I think one of the reasons
[00:09:09.840 --> 00:09:11.880]   that we succeeded in the areas
[00:09:11.880 --> 00:09:14.000]   we as a computer scientist succeeded
[00:09:14.000 --> 00:09:16.280]   is because we don't have,
[00:09:16.280 --> 00:09:19.000]   we are not trying to understand in some ways.
[00:09:19.000 --> 00:09:22.280]   Like if you're thinking about like e-commerce, Amazon,
[00:09:22.280 --> 00:09:24.240]   Amazon doesn't really understand you
[00:09:24.240 --> 00:09:27.720]   and that's why it recommends you certain books
[00:09:27.720 --> 00:09:29.600]   or certain products, correct?
[00:09:29.600 --> 00:09:34.240]   And in, you know, traditionally,
[00:09:34.240 --> 00:09:36.160]   when people were thinking about marketing,
[00:09:36.160 --> 00:09:37.800]   you know, they divided the population
[00:09:37.800 --> 00:09:39.800]   to different kind of subgroups,
[00:09:39.800 --> 00:09:41.760]   identify the features of the subgroup
[00:09:41.760 --> 00:09:43.120]   and come up with a strategy
[00:09:43.120 --> 00:09:45.560]   which is specific to that subgroup.
[00:09:45.560 --> 00:09:47.320]   If you're looking about recommendation system,
[00:09:47.320 --> 00:09:50.600]   they're not claiming that they're understanding somebody,
[00:09:50.600 --> 00:09:52.680]   they're just managing to,
[00:09:52.680 --> 00:09:54.760]   from the patterns of your behavior,
[00:09:54.760 --> 00:09:57.560]   to recommend you a product.
[00:09:57.560 --> 00:09:59.600]   Now, if you look at the traditional biology,
[00:09:59.600 --> 00:10:03.200]   and obviously I wouldn't say that I,
[00:10:03.200 --> 00:10:06.160]   at any way, you know, educated in this field,
[00:10:06.160 --> 00:10:07.040]   but you know, what I see,
[00:10:07.040 --> 00:10:09.280]   there's really a lot of emphasis
[00:10:09.280 --> 00:10:10.640]   on mechanistic understanding.
[00:10:10.640 --> 00:10:12.560]   And it was very surprising to me
[00:10:12.560 --> 00:10:13.800]   coming from computer science
[00:10:13.800 --> 00:10:17.600]   how much emphasis is on this understanding.
[00:10:17.600 --> 00:10:20.720]   And given the complexity of the system,
[00:10:20.720 --> 00:10:23.200]   maybe the deterministic full understanding
[00:10:23.200 --> 00:10:27.360]   of this process is, you know, beyond our capacity.
[00:10:27.360 --> 00:10:29.440]   And the same way as in computer science,
[00:10:29.440 --> 00:10:30.520]   when we're doing recognition,
[00:10:30.520 --> 00:10:32.760]   when you do recommendation in many other areas,
[00:10:32.760 --> 00:10:35.960]   it's just probabilistic matching process.
[00:10:35.960 --> 00:10:40.080]   And in some way, maybe in certain cases,
[00:10:40.080 --> 00:10:42.920]   we shouldn't even attempt to understand,
[00:10:42.920 --> 00:10:44.480]   though we can attempt to understand,
[00:10:44.480 --> 00:10:48.040]   but in parallel, we can actually do this kind of matching
[00:10:48.040 --> 00:10:51.080]   that would help us to find hero
[00:10:51.080 --> 00:10:54.160]   to do early diagnostics and so on.
[00:10:54.160 --> 00:10:55.880]   And I know that in these communities,
[00:10:55.880 --> 00:10:59.080]   it's really important to understand,
[00:10:59.080 --> 00:11:00.720]   but I'm sometimes wondering,
[00:11:00.720 --> 00:11:02.960]   what exactly does it mean to understand here?
[00:11:02.960 --> 00:11:05.520]   - Well, there's stuff that works,
[00:11:05.520 --> 00:11:07.640]   but that can be, like you said,
[00:11:07.640 --> 00:11:10.360]   separate from this deep human desire
[00:11:10.360 --> 00:11:12.720]   to uncover the mysteries of the universe,
[00:11:12.720 --> 00:11:16.160]   of science, of the way the body works,
[00:11:16.160 --> 00:11:17.600]   the way the mind works.
[00:11:17.600 --> 00:11:19.560]   It's the dream of symbolic AI,
[00:11:19.560 --> 00:11:24.000]   of being able to reduce human knowledge into logic
[00:11:24.000 --> 00:11:26.880]   and be able to play with that logic
[00:11:26.880 --> 00:11:28.680]   in a way that's very explainable
[00:11:28.680 --> 00:11:30.280]   and understandable for us humans.
[00:11:30.280 --> 00:11:31.760]   I mean, that's a beautiful dream.
[00:11:31.760 --> 00:11:34.840]   So I understand it, but it seems that
[00:11:34.840 --> 00:11:37.880]   what seems to work today, and we'll talk about it more,
[00:11:37.880 --> 00:11:40.780]   is as much as possible, reduce stuff into data,
[00:11:40.780 --> 00:11:43.880]   reduce whatever problem you're interested in to data
[00:11:43.880 --> 00:11:47.040]   and try to apply statistical methods,
[00:11:47.040 --> 00:11:49.120]   apply machine learning to that.
[00:11:49.120 --> 00:11:51.120]   On a personal note,
[00:11:51.120 --> 00:11:54.160]   you were diagnosed with breast cancer in 2014.
[00:11:54.160 --> 00:11:58.400]   What did facing your mortality make you think about?
[00:11:58.400 --> 00:12:00.240]   How did it change you?
[00:12:00.240 --> 00:12:01.840]   - You know, this is a great question,
[00:12:01.840 --> 00:12:03.800]   and I think that I was interviewed many times,
[00:12:03.800 --> 00:12:05.720]   so nobody actually asked me this question.
[00:12:05.720 --> 00:12:09.680]   I think I was 43 at the time,
[00:12:09.680 --> 00:12:11.480]   and it's the first time I realized in my life
[00:12:11.480 --> 00:12:14.480]   that I may die, and I never thought about it before.
[00:12:14.480 --> 00:12:17.280]   And there was a long time since you diagnosed
[00:12:17.280 --> 00:12:18.600]   until you actually know what you have
[00:12:18.600 --> 00:12:20.160]   and how severe is your disease.
[00:12:20.160 --> 00:12:23.520]   For me, it was like maybe two and a half months.
[00:12:23.520 --> 00:12:28.320]   And I didn't know where I am during this time
[00:12:28.320 --> 00:12:30.640]   because I was getting different tests,
[00:12:30.640 --> 00:12:32.600]   and one would say it's bad, and I would say,
[00:12:32.600 --> 00:12:33.420]   no, it is not.
[00:12:33.420 --> 00:12:34.840]   So until I knew where I am,
[00:12:34.840 --> 00:12:36.280]   I really was thinking about
[00:12:36.280 --> 00:12:38.240]   all these different possible outcomes.
[00:12:38.240 --> 00:12:39.720]   - Were you imagining the worst,
[00:12:39.720 --> 00:12:41.960]   or were you trying to be optimistic?
[00:12:41.960 --> 00:12:45.640]   - It would be really, I don't remember
[00:12:45.640 --> 00:12:47.400]   what was my thinking.
[00:12:47.400 --> 00:12:51.120]   It was really a mixture with many components at the time,
[00:12:51.120 --> 00:12:54.120]   speaking in our terms.
[00:12:54.120 --> 00:12:59.120]   And one thing that I remember,
[00:12:59.120 --> 00:13:01.560]   and every test comes, and then you think,
[00:13:01.560 --> 00:13:03.360]   oh, it could be this, or it may not be this,
[00:13:03.360 --> 00:13:04.720]   and you're hopeful, and then you're desperate.
[00:13:04.720 --> 00:13:07.720]   So it's like there is a whole slew of emotions
[00:13:07.720 --> 00:13:08.720]   that go through you.
[00:13:08.720 --> 00:13:14.800]   But what I remember is that when I came back to MIT,
[00:13:14.800 --> 00:13:17.160]   I was kind of going the whole time
[00:13:17.160 --> 00:13:18.280]   through the treatment to MIT,
[00:13:18.280 --> 00:13:19.800]   but my brain was not really there.
[00:13:19.800 --> 00:13:21.800]   But when I came back, really finished my treatment,
[00:13:21.800 --> 00:13:24.560]   and I was here teaching and everything,
[00:13:24.560 --> 00:13:27.080]   you know, I look back at what my group was doing,
[00:13:27.080 --> 00:13:28.840]   what other groups was doing,
[00:13:28.840 --> 00:13:30.820]   and I saw these trivialities.
[00:13:30.820 --> 00:13:33.240]   It's like people are building their careers
[00:13:33.240 --> 00:13:36.200]   on improving some parts, around 2% or 3% or whatever.
[00:13:36.920 --> 00:13:38.400]   I was, it's like, seriously,
[00:13:38.400 --> 00:13:40.760]   I did a work on how to decipher Ugaritic,
[00:13:40.760 --> 00:13:42.880]   like a language that nobody speak, and whatever.
[00:13:42.880 --> 00:13:46.160]   Like, what is significance?
[00:13:46.160 --> 00:13:49.000]   When all of a sudden, you know, I walked out of MIT,
[00:13:49.000 --> 00:13:51.600]   which is, you know, when people really do care,
[00:13:51.600 --> 00:13:54.240]   you know, what happened to your Eclair paper,
[00:13:54.240 --> 00:13:56.640]   you know, what is your next publication,
[00:13:56.640 --> 00:13:59.920]   to ACL, to the world where people, you know,
[00:13:59.920 --> 00:14:01.920]   people, you see a lot of sufferings,
[00:14:01.920 --> 00:14:04.880]   and I'm kind of totally shielded on it on daily basis.
[00:14:04.880 --> 00:14:06.840]   And it's like the first time I've seen, like,
[00:14:06.840 --> 00:14:08.680]   real life and real suffering.
[00:14:08.680 --> 00:14:10.720]   And I was thinking,
[00:14:10.720 --> 00:14:13.280]   why are we trying to improve the parts there,
[00:14:13.280 --> 00:14:16.120]   or deal with some trivialities
[00:14:16.120 --> 00:14:20.720]   when we have capacity to really make a change?
[00:14:20.720 --> 00:14:23.480]   And it was really challenging to me,
[00:14:23.480 --> 00:14:24.600]   because on one hand, you know,
[00:14:24.600 --> 00:14:25.760]   I have my graduate students
[00:14:25.760 --> 00:14:28.720]   who really want to do their papers and their work,
[00:14:28.720 --> 00:14:30.840]   and they want to continue to do what they were doing,
[00:14:30.840 --> 00:14:31.920]   which was great.
[00:14:31.920 --> 00:14:36.320]   And then it was me who really kind of reevaluated
[00:14:36.320 --> 00:14:37.480]   what is the importance.
[00:14:37.480 --> 00:14:38.560]   And also at that point,
[00:14:38.560 --> 00:14:40.280]   because I had to take some break,
[00:14:40.280 --> 00:14:47.520]   I look back into, like, my years in science,
[00:14:47.520 --> 00:14:49.600]   and I was thinking, you know, like,
[00:14:49.600 --> 00:14:51.680]   10 years ago, this was the biggest thing.
[00:14:51.680 --> 00:14:52.960]   I don't know, topic models.
[00:14:52.960 --> 00:14:55.360]   We have, like, millions of papers on topic models
[00:14:55.360 --> 00:14:56.520]   and variation of topics models,
[00:14:56.520 --> 00:14:58.600]   and I was totally, like, irrelevant.
[00:14:58.600 --> 00:15:01.480]   And you start looking at this, you know,
[00:15:01.480 --> 00:15:03.240]   what do you perceive as important
[00:15:03.240 --> 00:15:04.520]   at different point of time,
[00:15:04.520 --> 00:15:08.920]   and how, you know, it fades over time.
[00:15:08.920 --> 00:15:13.000]   And since we have a limited time,
[00:15:13.000 --> 00:15:14.960]   all of us have limited time on Earth,
[00:15:14.960 --> 00:15:17.840]   it's really important to prioritize
[00:15:17.840 --> 00:15:19.760]   things that really matter to you,
[00:15:19.760 --> 00:15:22.040]   maybe matter to you at that particular point,
[00:15:22.040 --> 00:15:24.400]   but it's important to take some time
[00:15:24.400 --> 00:15:26.960]   and understand what matters to you,
[00:15:26.960 --> 00:15:28.880]   which may not necessarily be the same
[00:15:28.880 --> 00:15:31.720]   as what matters to the rest of your scientific community,
[00:15:31.720 --> 00:15:34.600]   and pursue that vision.
[00:15:34.600 --> 00:15:38.480]   - So that moment, did it make you cognizant,
[00:15:38.480 --> 00:15:39.600]   you mentioned suffering,
[00:15:39.600 --> 00:15:44.400]   of just the general amount of suffering in the world.
[00:15:44.400 --> 00:15:45.680]   Is that what you're referring to?
[00:15:45.680 --> 00:15:47.440]   So as opposed to topic models
[00:15:47.440 --> 00:15:50.840]   and specific detailed problems in NLP,
[00:15:50.840 --> 00:15:54.480]   did you start to think about other people
[00:15:54.480 --> 00:15:56.960]   who have been diagnosed with cancer?
[00:15:56.960 --> 00:16:00.040]   Is that the way you started to see the world, perhaps?
[00:16:00.040 --> 00:16:02.440]   - Oh, absolutely, and it actually creates,
[00:16:02.440 --> 00:16:04.960]   because like, for instance, you know,
[00:16:04.960 --> 00:16:06.040]   there is parts of the treatment
[00:16:06.040 --> 00:16:08.520]   where you need to go to the hospital every day,
[00:16:08.520 --> 00:16:11.600]   and you see, you know, the community of people that you see,
[00:16:11.600 --> 00:16:16.080]   and many of them are much worse than I was at the time,
[00:16:16.080 --> 00:16:20.480]   and you all of a sudden see it all.
[00:16:20.480 --> 00:16:23.920]   And people who are happier some day
[00:16:23.920 --> 00:16:25.320]   just because they feel better,
[00:16:25.320 --> 00:16:28.480]   and for people who are in our normal realm,
[00:16:28.480 --> 00:16:30.800]   you take it totally for granted that you feel well,
[00:16:30.800 --> 00:16:32.920]   that if you decide to go running, you can go running,
[00:16:32.920 --> 00:16:35.880]   and you can, you know, you're pretty much free
[00:16:35.880 --> 00:16:37.600]   to do whatever you want with your body.
[00:16:37.600 --> 00:16:40.200]   Like, I saw like a community,
[00:16:40.200 --> 00:16:42.800]   my community became those people.
[00:16:42.800 --> 00:16:47.480]   And I remember one of my friends, Dina Khatabi,
[00:16:47.480 --> 00:16:50.400]   took me to Prudential to buy me a gift for my birthday,
[00:16:50.400 --> 00:16:52.320]   and it was like the first time in months
[00:16:52.320 --> 00:16:54.960]   that I went to kind of to see other people,
[00:16:54.960 --> 00:16:57.080]   and I was like, wow, first of all,
[00:16:57.080 --> 00:16:58.960]   these people, you know, they are happy,
[00:16:58.960 --> 00:17:00.640]   and they are laughing, and they're very different
[00:17:00.640 --> 00:17:02.600]   from these other my people.
[00:17:02.600 --> 00:17:04.640]   And second, I think it's totally crazy,
[00:17:04.640 --> 00:17:06.360]   they're like laughing and wasting their money
[00:17:06.360 --> 00:17:11.360]   on some stupid gifts, and, you know, they may die.
[00:17:11.360 --> 00:17:15.960]   They already may have cancer, and they don't understand it.
[00:17:15.960 --> 00:17:20.080]   So you can really see how the mind changes,
[00:17:20.080 --> 00:17:22.360]   that you can see that, you know,
[00:17:22.360 --> 00:17:23.680]   before that you can, as didn't you know
[00:17:23.680 --> 00:17:25.280]   that you're gonna die, of course I knew,
[00:17:25.280 --> 00:17:28.320]   but it was kind of a theoretical notion,
[00:17:28.320 --> 00:17:31.040]   it wasn't something which was concrete.
[00:17:31.040 --> 00:17:33.880]   And at that point, when you really see it,
[00:17:33.880 --> 00:17:37.640]   and see how little means sometimes the system
[00:17:37.640 --> 00:17:41.000]   has to harm them, you really feel that we need
[00:17:41.000 --> 00:17:45.440]   to take a lot of our brilliance that we have here at MIT
[00:17:45.440 --> 00:17:48.040]   and translate it into something useful.
[00:17:48.040 --> 00:17:50.520]   - Yeah, and useful can have a lot of definitions,
[00:17:50.520 --> 00:17:52.320]   but of course, alleviating suffering,
[00:17:52.320 --> 00:17:57.320]   alleviating trying to cure cancer is a beautiful mission.
[00:17:57.320 --> 00:18:01.920]   So I of course know theoretically the notion of cancer,
[00:18:01.920 --> 00:18:04.680]   but just reading more and more about it,
[00:18:04.680 --> 00:18:08.000]   it's 1.7 million new cancer cases
[00:18:08.000 --> 00:18:09.840]   in the United States every year,
[00:18:09.840 --> 00:18:13.480]   600,000 cancer-related deaths every year.
[00:18:13.480 --> 00:18:17.580]   So this has a huge impact, United States globally.
[00:18:19.340 --> 00:18:24.340]   When broadly, before we talk about how machine learning,
[00:18:24.340 --> 00:18:29.380]   how MIT can help, when do you think we as a civilization
[00:18:29.380 --> 00:18:32.100]   will cure cancer?
[00:18:32.100 --> 00:18:34.620]   How hard of a problem is it from everything
[00:18:34.620 --> 00:18:36.220]   you've learned from it recently?
[00:18:36.220 --> 00:18:39.340]   - I cannot really assess it.
[00:18:39.340 --> 00:18:42.100]   What I do believe will happen with the advancement
[00:18:42.100 --> 00:18:45.940]   in machine learning, that a lot of types of cancer
[00:18:45.940 --> 00:18:48.500]   we will be able to predict way early
[00:18:48.500 --> 00:18:53.420]   and more effectively utilize existing treatments.
[00:18:53.420 --> 00:18:57.540]   I think, I hope at least, that with all the advancements
[00:18:57.540 --> 00:19:01.180]   in AI and drug discovery, we would be able
[00:19:01.180 --> 00:19:04.700]   to much faster find relevant molecules.
[00:19:04.700 --> 00:19:08.220]   What I'm not sure about is how long it will take
[00:19:08.220 --> 00:19:11.940]   the medical establishment and regulatory bodies
[00:19:11.940 --> 00:19:14.780]   to kind of catch up and to implement it.
[00:19:14.780 --> 00:19:17.460]   And I think this is a very big piece of puzzles
[00:19:17.460 --> 00:19:20.420]   that is currently not addressed.
[00:19:20.420 --> 00:19:21.820]   - That's a really interesting question.
[00:19:21.820 --> 00:19:25.460]   So first, a small detail that I think the answer is yes,
[00:19:25.460 --> 00:19:30.460]   but is cancer one of the diseases that when detected earlier
[00:19:30.460 --> 00:19:37.120]   that significantly improves the outcomes?
[00:19:37.120 --> 00:19:41.020]   'Cause we will talk about there's the cure
[00:19:41.020 --> 00:19:43.020]   and then there is detection.
[00:19:43.020 --> 00:19:45.180]   And I think where machine learning can really help
[00:19:45.180 --> 00:19:46.660]   is earlier detection.
[00:19:46.660 --> 00:19:48.540]   So does detection help?
[00:19:48.540 --> 00:19:49.660]   - Detection is crucial.
[00:19:49.660 --> 00:19:53.940]   For instance, the vast majority of pancreatic cancer patients
[00:19:53.940 --> 00:19:57.300]   are detected at the stage that they are incurable.
[00:19:57.300 --> 00:20:02.300]   That's why they have such a terrible survival rate.
[00:20:02.300 --> 00:20:07.340]   It's like just few percent over five years.
[00:20:07.340 --> 00:20:09.820]   It's pretty much today a death sentence.
[00:20:09.820 --> 00:20:13.600]   But if you can discover this disease early,
[00:20:14.500 --> 00:20:16.740]   there are mechanisms to treat it.
[00:20:16.740 --> 00:20:20.740]   And in fact, I know a number of people who were diagnosed
[00:20:20.740 --> 00:20:23.600]   and saved just because they had food poisoning.
[00:20:23.600 --> 00:20:25.020]   They had terrible food poisoning.
[00:20:25.020 --> 00:20:28.540]   They went to ER and they got scan.
[00:20:28.540 --> 00:20:30.660]   There were early signs on the scan
[00:20:30.660 --> 00:20:33.540]   and that what saved their lives.
[00:20:33.540 --> 00:20:35.820]   But this wasn't really an accidental case.
[00:20:35.820 --> 00:20:40.820]   So as we become better, we would be able to help
[00:20:41.220 --> 00:20:46.220]   too many more people that are likely to develop diseases.
[00:20:46.220 --> 00:20:50.980]   And I just want to say that as I got more into this field,
[00:20:50.980 --> 00:20:53.580]   I realized that cancer is of course terrible disease,
[00:20:53.580 --> 00:20:56.660]   but there are really the whole slew of terrible diseases
[00:20:56.660 --> 00:21:00.780]   out there like neurodegenerative diseases and others.
[00:21:00.780 --> 00:21:04.540]   So we, of course, a lot of us are fixated on cancer
[00:21:04.540 --> 00:21:06.500]   just because it's so prevalent in our society
[00:21:06.500 --> 00:21:07.500]   and you see these people,
[00:21:07.500 --> 00:21:08.620]   but there are a lot of patients
[00:21:08.620 --> 00:21:10.400]   with neurodegenerative diseases
[00:21:10.400 --> 00:21:12.580]   and the kind of aging diseases
[00:21:12.580 --> 00:21:17.180]   that we still don't have a good solution for.
[00:21:17.180 --> 00:21:22.180]   And we, you know, and I felt as a computer scientist,
[00:21:22.180 --> 00:21:25.540]   we kind of decided that it's other people's job
[00:21:25.540 --> 00:21:26.940]   to treat these diseases
[00:21:26.940 --> 00:21:30.460]   because it's like traditionally people in biology
[00:21:30.460 --> 00:21:33.660]   or in chemistry or MDs are the ones who are thinking
[00:21:33.660 --> 00:21:37.460]   about it and have to kind of start paying attention.
[00:21:37.460 --> 00:21:40.380]   I think that it's really a wrong assumption.
[00:21:40.380 --> 00:21:43.020]   And we all need to join the battle.
[00:21:43.020 --> 00:21:46.540]   - So how, it seems like in cancer specifically,
[00:21:46.540 --> 00:21:49.200]   that there's a lot of ways that machine learning can help.
[00:21:49.200 --> 00:21:51.940]   So what's the role of machine learning
[00:21:51.940 --> 00:21:54.160]   in the diagnosis of cancer?
[00:21:54.160 --> 00:21:57.260]   - So for many cancers today,
[00:21:57.260 --> 00:22:02.260]   we really don't know what is your likelihood to get cancer.
[00:22:02.260 --> 00:22:06.380]   And for the vast majority of patients,
[00:22:06.380 --> 00:22:08.020]   especially on the younger patients,
[00:22:08.020 --> 00:22:09.620]   it really comes as a surprise.
[00:22:09.620 --> 00:22:11.180]   Like for instance, for breast cancer,
[00:22:11.180 --> 00:22:13.900]   80% of the patients are first in their families.
[00:22:13.900 --> 00:22:15.400]   It's like me.
[00:22:15.400 --> 00:22:18.500]   And I never saw that I had any increased risk
[00:22:18.500 --> 00:22:20.860]   because, you know, nobody had it in my family.
[00:22:20.860 --> 00:22:22.300]   And for some reason in my head,
[00:22:22.300 --> 00:22:24.840]   it was kind of inherited disease.
[00:22:24.840 --> 00:22:28.380]   But even if I would pay attention,
[00:22:28.380 --> 00:22:30.220]   the models that currently,
[00:22:30.220 --> 00:22:32.420]   there's very simplistic statistical models
[00:22:32.420 --> 00:22:34.540]   that are currently used and in clinical practice,
[00:22:34.540 --> 00:22:35.820]   they really don't give you an answer.
[00:22:35.820 --> 00:22:37.460]   So you don't know.
[00:22:37.460 --> 00:22:40.380]   And the same true for pancreatic cancer,
[00:22:40.380 --> 00:22:45.380]   the same true for non-smoking lung cancer and many others.
[00:22:45.380 --> 00:22:47.340]   So what machine learning can do here
[00:22:47.340 --> 00:22:51.620]   is utilize all this data to tell us early
[00:22:51.620 --> 00:22:53.140]   who is likely to be susceptible
[00:22:53.140 --> 00:22:55.980]   and using all the information that is already there,
[00:22:55.980 --> 00:22:59.980]   be it imaging, be it your other tests,
[00:22:59.980 --> 00:23:04.860]   and, you know, eventually liquid biopsies and others
[00:23:04.860 --> 00:23:08.180]   where the signal itself is not sufficiently strong
[00:23:08.180 --> 00:23:11.300]   for human eye to do good discrimination
[00:23:11.300 --> 00:23:12.900]   because the signal may be weak.
[00:23:12.900 --> 00:23:15.620]   But by combining many sources,
[00:23:15.620 --> 00:23:18.100]   a machine which is trained on large volumes of data
[00:23:18.100 --> 00:23:20.700]   can really detect it early.
[00:23:20.700 --> 00:23:22.500]   And that's what we've seen with breast cancer
[00:23:22.500 --> 00:23:25.900]   and people are reporting it in other diseases as well.
[00:23:25.900 --> 00:23:28.260]   - That really boils down to data, right?
[00:23:28.260 --> 00:23:30.980]   And in the different kinds of sources of data.
[00:23:30.980 --> 00:23:33.720]   And you mentioned regulatory challenges.
[00:23:33.720 --> 00:23:36.500]   So what are the challenges in gathering
[00:23:36.500 --> 00:23:39.220]   large data sets in this space?
[00:23:39.220 --> 00:23:42.660]   - Again, another great question.
[00:23:42.660 --> 00:23:45.500]   So it took me after I decided that I want to work on it
[00:23:45.500 --> 00:23:48.740]   two years to get access to data.
[00:23:48.740 --> 00:23:50.580]   - Any data, like any significant data set?
[00:23:50.580 --> 00:23:51.420]   - Any significant amount.
[00:23:51.420 --> 00:23:53.580]   Like right now in this country,
[00:23:53.580 --> 00:23:58.060]   there is no publicly available data set of modern mammogram
[00:23:58.060 --> 00:23:59.540]   that you can just go on your computer,
[00:23:59.540 --> 00:24:01.860]   sign a document and get it.
[00:24:01.860 --> 00:24:03.180]   It just doesn't exist.
[00:24:03.180 --> 00:24:05.280]   I mean, obviously every hospital
[00:24:05.280 --> 00:24:07.560]   has its own collection of mammograms.
[00:24:07.560 --> 00:24:11.300]   There are data that came out of clinical trials.
[00:24:11.300 --> 00:24:13.220]   But we're talking about you as a computer scientist
[00:24:13.220 --> 00:24:17.120]   who just want to run his or her model
[00:24:17.120 --> 00:24:19.060]   and see how it works.
[00:24:19.060 --> 00:24:22.900]   This data, like ImageNet, doesn't exist.
[00:24:22.900 --> 00:24:27.900]   And there is a set which is called like Florida Dataset,
[00:24:27.900 --> 00:24:30.860]   which is a film mammogram from '90s,
[00:24:30.860 --> 00:24:32.460]   which is totally not representative
[00:24:32.460 --> 00:24:33.880]   of the current developments.
[00:24:33.880 --> 00:24:35.800]   Whatever you're learning on them doesn't scale up.
[00:24:35.800 --> 00:24:39.340]   This is the only resource that is available.
[00:24:39.340 --> 00:24:42.820]   And today there are many agencies
[00:24:42.820 --> 00:24:44.460]   that govern access to data.
[00:24:44.460 --> 00:24:46.300]   Like the hospital holds your data
[00:24:46.300 --> 00:24:49.280]   and the hospital decides whether they would give it
[00:24:49.280 --> 00:24:52.220]   to the researcher to work with his data.
[00:24:52.220 --> 00:24:54.180]   - An individual hospital?
[00:24:54.180 --> 00:24:56.120]   - Yeah, I mean, the hospital may,
[00:24:56.120 --> 00:24:59.220]   assuming that you're doing research collaboration,
[00:24:59.220 --> 00:25:03.500]   you can submit, there is a proper approval process
[00:25:03.500 --> 00:25:05.060]   guided by our RPE.
[00:25:05.060 --> 00:25:07.820]   And if you go through all the processes,
[00:25:07.820 --> 00:25:10.140]   you can eventually get access to the data.
[00:25:10.140 --> 00:25:13.540]   But if you yourself know, our AI community,
[00:25:13.540 --> 00:25:14.740]   there are not that many people
[00:25:14.740 --> 00:25:16.620]   who actually ever got access to data
[00:25:16.620 --> 00:25:20.220]   because it's a very challenging process.
[00:25:20.220 --> 00:25:22.780]   - And sorry, just a quick comment.
[00:25:22.780 --> 00:25:27.780]   MGH or any kind of hospital, are they scanning the data?
[00:25:28.100 --> 00:25:29.700]   Are they digitally storing it?
[00:25:29.700 --> 00:25:31.580]   - Oh, it is already digitally stored.
[00:25:31.580 --> 00:25:34.160]   You don't need to do any extra processing steps.
[00:25:34.160 --> 00:25:36.340]   It's already there in the right format.
[00:25:36.340 --> 00:25:39.820]   Is that right now there are a lot of issues
[00:25:39.820 --> 00:25:41.180]   that govern access to the data
[00:25:41.180 --> 00:25:46.180]   because the hospital is legally responsible for the data.
[00:25:46.180 --> 00:25:51.100]   And they have a lot to lose
[00:25:51.100 --> 00:25:53.220]   if they give the data to the wrong person,
[00:25:53.220 --> 00:25:56.540]   but they may not have a lot to gain if they give it,
[00:25:56.540 --> 00:26:00.660]   as a hospital, as a legal entity is giving it to you.
[00:26:00.660 --> 00:26:02.820]   And the way, you know, what I would imagine
[00:26:02.820 --> 00:26:05.300]   happening in the future is the same thing that happens
[00:26:05.300 --> 00:26:06.860]   when you're getting your driving license.
[00:26:06.860 --> 00:26:09.900]   You can decide whether you want to donate your organs.
[00:26:09.900 --> 00:26:12.900]   You can imagine that whenever a person goes to the hospital,
[00:26:12.900 --> 00:26:18.100]   it should be easy for them to donate their data for research
[00:26:18.100 --> 00:26:19.500]   and it can be different kind of,
[00:26:19.500 --> 00:26:21.340]   do they only give you a test results
[00:26:21.340 --> 00:26:25.940]   or only imaging data or the whole medical record?
[00:26:26.780 --> 00:26:29.020]   Because at the end,
[00:26:29.020 --> 00:26:33.900]   we all will benefit from all this insights.
[00:26:33.900 --> 00:26:36.100]   And it's not like you say, I want to keep my data private,
[00:26:36.100 --> 00:26:38.820]   but I would really love to get it from other people
[00:26:38.820 --> 00:26:40.780]   because other people are thinking the same way.
[00:26:40.780 --> 00:26:45.780]   So if there is a mechanism to do this donation
[00:26:45.780 --> 00:26:48.060]   and the patient has an ability to say
[00:26:48.060 --> 00:26:50.860]   how they want to use their data for research,
[00:26:50.860 --> 00:26:54.140]   it would be really a game changer.
[00:26:54.140 --> 00:26:56.500]   - People, when they think about this problem,
[00:26:56.500 --> 00:26:58.500]   there's a, it depends on the population,
[00:26:58.500 --> 00:27:00.180]   depends on the demographics,
[00:27:00.180 --> 00:27:02.460]   but there's some privacy concerns.
[00:27:02.460 --> 00:27:05.900]   Generally, not just medical data, just any kind of data.
[00:27:05.900 --> 00:27:09.660]   It's what you said, my data, it should belong kind of to me.
[00:27:09.660 --> 00:27:11.660]   I'm worried how it's going to be misused.
[00:27:11.660 --> 00:27:15.660]   How do we alleviate those concerns?
[00:27:15.660 --> 00:27:19.500]   Because that seems like a problem that needs to be,
[00:27:19.500 --> 00:27:21.660]   that problem of trust, of transparency,
[00:27:21.660 --> 00:27:25.180]   needs to be solved before we build large datasets
[00:27:25.180 --> 00:27:27.300]   that help detect cancer,
[00:27:27.300 --> 00:27:30.220]   help save those very people in the future.
[00:27:30.220 --> 00:27:31.980]   - So I think there are two things that could be done.
[00:27:31.980 --> 00:27:34.460]   There is a technical solutions
[00:27:34.460 --> 00:27:38.220]   and there are societal solutions.
[00:27:38.220 --> 00:27:40.180]   So on the technical end,
[00:27:40.180 --> 00:27:46.440]   we today have ability to improve disambiguation,
[00:27:46.440 --> 00:27:49.860]   like for instance, for imaging,
[00:27:51.380 --> 00:27:55.620]   for imaging, you can do it pretty well.
[00:27:55.620 --> 00:27:56.780]   - What's disambiguation?
[00:27:56.780 --> 00:27:58.540]   - And disambiguation, sorry, disambiguation,
[00:27:58.540 --> 00:27:59.860]   removing the identification,
[00:27:59.860 --> 00:28:02.220]   removing the names of the people.
[00:28:02.220 --> 00:28:04.860]   There are other data, like if it is a raw text,
[00:28:04.860 --> 00:28:08.180]   you cannot really achieve 99.9%,
[00:28:08.180 --> 00:28:10.060]   but there are all these techniques,
[00:28:10.060 --> 00:28:12.500]   and actually some of them are developed at MIT,
[00:28:12.500 --> 00:28:15.460]   how you can do learning on the encoded data,
[00:28:15.460 --> 00:28:17.420]   where you locally encode the image,
[00:28:17.420 --> 00:28:19.900]   you train a network which only works
[00:28:19.900 --> 00:28:22.420]   on the encoded images,
[00:28:22.420 --> 00:28:24.940]   and then you send the outcome back to the hospital
[00:28:24.940 --> 00:28:26.580]   and you can open it up.
[00:28:26.580 --> 00:28:28.020]   So those are the technical solution.
[00:28:28.020 --> 00:28:30.660]   There are a lot of people who are working in this space
[00:28:30.660 --> 00:28:33.780]   where the learning happens in the encoded form.
[00:28:33.780 --> 00:28:35.300]   We are still early,
[00:28:35.300 --> 00:28:39.220]   but this is an interesting research area
[00:28:39.220 --> 00:28:41.860]   where I think we'll make more progress.
[00:28:41.860 --> 00:28:45.180]   There is a lot of work in natural language
[00:28:45.180 --> 00:28:48.560]   processing community, how to do the identification better.
[00:28:49.560 --> 00:28:54.000]   But even today, there are already a lot of data
[00:28:54.000 --> 00:28:55.880]   which can be de-identified perfectly,
[00:28:55.880 --> 00:28:58.720]   like your test data, for instance, correct?
[00:28:58.720 --> 00:29:00.960]   Where you can just, you know the name of the patient,
[00:29:00.960 --> 00:29:04.280]   you just want to extract the part with the numbers.
[00:29:04.280 --> 00:29:07.440]   The big problem here is again,
[00:29:07.440 --> 00:29:10.400]   hospitals don't see much incentive
[00:29:10.400 --> 00:29:12.600]   to give this data away on one hand,
[00:29:12.600 --> 00:29:14.200]   and then there is general concern.
[00:29:14.200 --> 00:29:17.680]   Now when I'm talking about societal benefits
[00:29:17.680 --> 00:29:19.640]   and about the education,
[00:29:19.640 --> 00:29:23.640]   the public needs to understand,
[00:29:23.640 --> 00:29:27.880]   and I think that there are situations,
[00:29:27.880 --> 00:29:31.560]   and I still remember myself when I really needed an answer.
[00:29:31.560 --> 00:29:33.360]   I had to make a choice,
[00:29:33.360 --> 00:29:35.280]   and there was no information to make a choice.
[00:29:35.280 --> 00:29:36.720]   You're just guessing.
[00:29:36.720 --> 00:29:41.100]   And at that moment, you feel that your life is at stake,
[00:29:41.100 --> 00:29:44.840]   but you just don't have information to make the choice.
[00:29:44.840 --> 00:29:48.760]   And many times when I give talks,
[00:29:48.760 --> 00:29:51.320]   I get emails from women who say,
[00:29:51.320 --> 00:29:52.840]   you know, I'm in this situation,
[00:29:52.840 --> 00:29:55.980]   can you please run statistics and see what are the outcomes?
[00:29:55.980 --> 00:30:00.080]   We get almost every week a mammogram
[00:30:00.080 --> 00:30:03.500]   that comes by mail to my office at MIT, I'm serious.
[00:30:03.500 --> 00:30:07.920]   That people ask to run because they need to make
[00:30:07.920 --> 00:30:10.060]   life-changing decisions.
[00:30:10.060 --> 00:30:13.000]   And of course, I'm not planning to open a clinic here,
[00:30:13.000 --> 00:30:16.640]   but we do run and give them the results for their doctors.
[00:30:16.640 --> 00:30:20.080]   But the point that I'm trying to make,
[00:30:20.080 --> 00:30:23.760]   that we all at some point, or our loved ones,
[00:30:23.760 --> 00:30:26.640]   will be in the situation where you need information
[00:30:26.640 --> 00:30:28.880]   to make the best choice.
[00:30:28.880 --> 00:30:31.880]   And if this information is not available,
[00:30:31.880 --> 00:30:35.120]   you would feel vulnerable and unprotected.
[00:30:35.120 --> 00:30:37.880]   And then the question is, you know, what do I care more?
[00:30:37.880 --> 00:30:40.360]   Because at the end, everything is a trade-off, correct?
[00:30:40.360 --> 00:30:41.680]   - Yeah, exactly.
[00:30:41.680 --> 00:30:43.560]   Just out of curiosity,
[00:30:43.560 --> 00:30:45.600]   it seems like one possible solution,
[00:30:45.600 --> 00:30:47.440]   I'd like to see what you think of it,
[00:30:47.440 --> 00:30:50.680]   based on what you just said,
[00:30:50.680 --> 00:30:52.520]   based on wanting to know answers
[00:30:52.520 --> 00:30:55.060]   for when you're yourself in that situation.
[00:30:55.060 --> 00:30:58.400]   Is it possible for patients to own their data
[00:30:58.400 --> 00:31:01.040]   as opposed to hospitals owning their data?
[00:31:01.040 --> 00:31:04.120]   Of course, theoretically, I guess patients own their data,
[00:31:04.120 --> 00:31:06.640]   but can you walk out there with a USB stick
[00:31:06.640 --> 00:31:10.620]   containing everything, or upload it to the cloud,
[00:31:10.620 --> 00:31:13.000]   where a company, you know,
[00:31:13.000 --> 00:31:15.680]   I remember Microsoft had a service,
[00:31:15.680 --> 00:31:17.800]   like I was really excited about,
[00:31:17.800 --> 00:31:19.280]   and Google Health was there.
[00:31:19.280 --> 00:31:21.880]   I tried to give, I was excited about it.
[00:31:21.880 --> 00:31:24.780]   Basically, companies helping you upload your data
[00:31:24.780 --> 00:31:27.960]   to the cloud, so that you can move from hospital to hospital,
[00:31:27.960 --> 00:31:29.240]   from doctor to doctor.
[00:31:29.240 --> 00:31:32.680]   Do you see a promise of that kind of possibility?
[00:31:32.680 --> 00:31:34.680]   - I absolutely think this is, you know,
[00:31:34.680 --> 00:31:38.160]   the right way to exchange the data.
[00:31:38.160 --> 00:31:41.700]   I don't know now who's the biggest player in this field,
[00:31:41.700 --> 00:31:44.620]   but I can clearly see that even for,
[00:31:44.620 --> 00:31:46.820]   even for totally selfish health reasons,
[00:31:46.820 --> 00:31:49.300]   when you are going to a new facility,
[00:31:49.300 --> 00:31:52.620]   and many of us are sent to some specialized treatment,
[00:31:52.620 --> 00:31:55.740]   they don't easily have access to your data.
[00:31:55.740 --> 00:31:57.940]   And today, you know,
[00:31:57.940 --> 00:31:59.420]   we would want to send this mammogram,
[00:31:59.420 --> 00:32:01.780]   need to go to the hospital, find some small office,
[00:32:01.780 --> 00:32:04.820]   which gives them the CD, and they ship us the CD,
[00:32:04.820 --> 00:32:06.460]   so you can imagine we're looking
[00:32:06.460 --> 00:32:10.140]   at kind of decades old mechanism of data exchange.
[00:32:10.140 --> 00:32:14.060]   So I definitely think this is an area
[00:32:14.060 --> 00:32:18.260]   where hopefully all the right regulatory
[00:32:18.260 --> 00:32:20.380]   and technical forces will align,
[00:32:20.380 --> 00:32:23.220]   and we will see it actually implemented.
[00:32:23.220 --> 00:32:25.740]   - It's sad because unfortunately,
[00:32:25.740 --> 00:32:28.420]   and I need to research why that happened,
[00:32:28.420 --> 00:32:30.620]   but I'm pretty sure Google Health
[00:32:30.620 --> 00:32:32.940]   and Microsoft Health Vault, or whatever it's called,
[00:32:32.940 --> 00:32:36.100]   both closed down, which means that there was
[00:32:36.100 --> 00:32:39.100]   either regulatory pressure, or there's not a business case,
[00:32:39.100 --> 00:32:41.820]   or there's challenges from hospitals,
[00:32:41.820 --> 00:32:43.260]   which is very disappointing.
[00:32:43.260 --> 00:32:44.540]   So when you say you don't know
[00:32:44.540 --> 00:32:46.520]   what the biggest players are,
[00:32:46.520 --> 00:32:50.540]   the two biggest that I was aware of closed their doors.
[00:32:50.540 --> 00:32:53.140]   So I'm hoping, I'd love to see why,
[00:32:53.140 --> 00:32:54.780]   and I'd love to see who else can come up.
[00:32:54.780 --> 00:32:59.620]   It seems like one of those Elon Musk style problems
[00:32:59.620 --> 00:33:01.300]   that are obvious needs to be solved,
[00:33:01.300 --> 00:33:02.380]   and somebody needs to step up
[00:33:02.380 --> 00:33:05.180]   and actually do this large scale data,
[00:33:05.180 --> 00:33:07.540]   you know, data collection.
[00:33:07.540 --> 00:33:09.660]   - So I know there is an initiative in Massachusetts,
[00:33:09.660 --> 00:33:11.780]   I think, actually led by the governor,
[00:33:11.780 --> 00:33:15.500]   to try to create this kind of health exchange system,
[00:33:15.500 --> 00:33:17.260]   where at least to help people who kind of,
[00:33:17.260 --> 00:33:18.700]   when you show up in emergency room
[00:33:18.700 --> 00:33:20.580]   and there is no information about
[00:33:20.580 --> 00:33:22.620]   what are your allergies and other things.
[00:33:22.620 --> 00:33:26.180]   So I don't know how far it will go.
[00:33:26.180 --> 00:33:28.220]   But another thing that you said,
[00:33:28.220 --> 00:33:30.340]   and I find it very interesting,
[00:33:30.340 --> 00:33:33.820]   is actually who are the successful players in this space,
[00:33:33.820 --> 00:33:37.260]   and the whole implementation, how does it go?
[00:33:37.260 --> 00:33:40.300]   To me, it is from the anthropological perspective,
[00:33:40.300 --> 00:33:44.140]   it's more fascinating that AI that today goes in healthcare.
[00:33:44.140 --> 00:33:47.900]   You know, we've seen so many attempts
[00:33:47.900 --> 00:33:50.380]   and so very little successes.
[00:33:50.380 --> 00:33:53.860]   And it's interesting to understand that I by no means,
[00:33:53.860 --> 00:33:56.680]   you know, have knowledge to assess it,
[00:33:56.680 --> 00:33:59.620]   why we are in the position where we are.
[00:33:59.620 --> 00:34:02.940]   - Yeah, it's interesting, 'cause data is really fuel
[00:34:02.940 --> 00:34:04.980]   for a lot of successful applications.
[00:34:04.980 --> 00:34:08.500]   And when that data requires regulatory approval,
[00:34:08.500 --> 00:34:12.440]   like the FDA or any kind of approval,
[00:34:12.440 --> 00:34:15.740]   it seems that the computer scientists
[00:34:15.740 --> 00:34:17.460]   are not quite there yet in being able
[00:34:17.460 --> 00:34:18.900]   to play the regulatory game,
[00:34:18.900 --> 00:34:21.220]   understanding the fundamentals of it.
[00:34:21.220 --> 00:34:23.700]   - I think that in many cases,
[00:34:23.700 --> 00:34:26.500]   when even people do have data,
[00:34:26.500 --> 00:34:31.500]   we still don't know what exactly do you need to demonstrate
[00:34:31.580 --> 00:34:33.880]   to change the standard of care.
[00:34:33.880 --> 00:34:37.220]   Like, let me give you an example
[00:34:37.220 --> 00:34:41.100]   related to my breast cancer research.
[00:34:41.100 --> 00:34:45.500]   So in traditional breast cancer risk assessment,
[00:34:45.500 --> 00:34:47.100]   there is something called density,
[00:34:47.100 --> 00:34:50.500]   which determines the likelihood of a woman to get cancer.
[00:34:50.500 --> 00:34:52.800]   And this pretty much says how much white
[00:34:52.800 --> 00:34:54.220]   do you see on the mammogram?
[00:34:54.220 --> 00:34:58.980]   The whiter it is, the more likely the tissue is dense.
[00:34:58.980 --> 00:35:03.660]   And the idea behind density, it's not a bad idea.
[00:35:03.660 --> 00:35:06.860]   In 1967, a radiologist called Wolf
[00:35:06.860 --> 00:35:09.780]   decided to look back at women who were diagnosed
[00:35:09.780 --> 00:35:12.420]   and see what is special in their images.
[00:35:12.420 --> 00:35:14.700]   Can we look back and say that they're likely to develop?
[00:35:14.700 --> 00:35:16.180]   So he come up with some patterns.
[00:35:16.180 --> 00:35:20.620]   It was the best that his human eye can identify.
[00:35:20.620 --> 00:35:22.660]   Then it was kind of formalized and coded
[00:35:22.660 --> 00:35:26.940]   into four categories, and that's what we are using today.
[00:35:26.940 --> 00:35:30.980]   And today, this density assessment
[00:35:30.980 --> 00:35:34.580]   is actually a federal law from 2019,
[00:35:34.580 --> 00:35:36.140]   approved by President Trump
[00:35:36.140 --> 00:35:38.780]   and for the previous FDA commissioner,
[00:35:38.780 --> 00:35:43.620]   where women are supposed to be advised by their providers
[00:35:43.620 --> 00:35:45.100]   if they have high density,
[00:35:45.100 --> 00:35:47.260]   putting them into higher risk category.
[00:35:47.260 --> 00:35:50.220]   And in some states, you can actually get
[00:35:50.220 --> 00:35:52.460]   supplementary screening paid by your insurance
[00:35:52.460 --> 00:35:53.700]   because you're in this category.
[00:35:53.700 --> 00:35:56.780]   Now you can say, how much science do we have behind it?
[00:35:56.780 --> 00:36:00.860]   Whatever, biological science or epidemiological evidence.
[00:36:00.860 --> 00:36:05.180]   So it turns out that between 40 and 50% of women
[00:36:05.180 --> 00:36:06.660]   have dense breast.
[00:36:06.660 --> 00:36:11.140]   So about 40% of patients are coming out of their screening
[00:36:11.140 --> 00:36:15.060]   and somebody tells them, you are in high risk.
[00:36:15.060 --> 00:36:16.900]   Now, what exactly does it mean
[00:36:16.900 --> 00:36:19.620]   if you as half of the population are in high risk?
[00:36:19.620 --> 00:36:22.060]   It's from saying, maybe I'm not,
[00:36:22.060 --> 00:36:23.700]   or what do I really need to do with it?
[00:36:23.700 --> 00:36:27.220]   Because the system doesn't provide me
[00:36:27.220 --> 00:36:28.340]   a lot of the solutions
[00:36:28.340 --> 00:36:30.180]   because there are so many people like me,
[00:36:30.180 --> 00:36:34.620]   we cannot really provide very expensive solutions for them.
[00:36:34.620 --> 00:36:38.740]   And the reason this whole density became this big deal,
[00:36:38.740 --> 00:36:40.820]   it's actually advocated by the patients
[00:36:40.820 --> 00:36:42.500]   who felt very unprotected
[00:36:42.500 --> 00:36:44.900]   because many women went in the mammograms,
[00:36:44.900 --> 00:36:46.260]   which were normal.
[00:36:46.260 --> 00:36:49.460]   And then it turns out that they already had cancer,
[00:36:49.460 --> 00:36:50.580]   quite developed cancer.
[00:36:50.580 --> 00:36:54.420]   So they didn't have a way to know who is really at risk
[00:36:54.420 --> 00:36:56.300]   and what is the likelihood that when the doctor tells you,
[00:36:56.300 --> 00:36:58.060]   you're okay, you are not okay.
[00:36:58.060 --> 00:37:02.140]   So at the time, and it was 15 years ago,
[00:37:02.140 --> 00:37:06.820]   this maybe was the best piece of science that we had.
[00:37:06.820 --> 00:37:11.820]   And it took quite 15, 16 years to make it federal law.
[00:37:11.820 --> 00:37:15.660]   But now this is a standard.
[00:37:15.660 --> 00:37:17.620]   Now with a deep learning model,
[00:37:17.620 --> 00:37:19.660]   we can so much more accurately predict
[00:37:19.660 --> 00:37:21.620]   who is gonna develop breast cancer
[00:37:21.620 --> 00:37:23.700]   just because you are trained on a logical thing.
[00:37:23.700 --> 00:37:26.060]   And instead of describing how much white
[00:37:26.060 --> 00:37:27.380]   and what kind of white machine
[00:37:27.380 --> 00:37:30.140]   can systematically identify the patterns,
[00:37:30.140 --> 00:37:31.980]   which was the original idea
[00:37:31.980 --> 00:37:34.020]   behind the sort of the tradiologist machine
[00:37:34.020 --> 00:37:35.700]   is can do it much more systematically
[00:37:35.700 --> 00:37:38.260]   and predict the risk when you're training the machine
[00:37:38.260 --> 00:37:39.100]   to look at the image
[00:37:39.100 --> 00:37:42.140]   and to say the risk in one, two, five years.
[00:37:42.140 --> 00:37:45.020]   Now you can ask me how long it will take
[00:37:45.020 --> 00:37:46.460]   to substitute this density,
[00:37:46.460 --> 00:37:48.620]   which is broadly used across the country.
[00:37:48.620 --> 00:37:53.620]   And I really it's not helping to bring this new models.
[00:37:53.620 --> 00:37:56.700]   And I would say it's not a matter of the algorithm.
[00:37:56.700 --> 00:37:58.780]   Algorithms already orders of magnitude better
[00:37:58.780 --> 00:38:00.460]   than what is currently in practice.
[00:38:00.460 --> 00:38:02.500]   I think it's really the question,
[00:38:02.500 --> 00:38:04.380]   who do you need to convince?
[00:38:04.380 --> 00:38:07.540]   How many hospitals do you need to run the experiment?
[00:38:07.540 --> 00:38:11.660]   What, you know, all this mechanism of adoption
[00:38:11.660 --> 00:38:15.180]   and how do you explain to patients
[00:38:15.180 --> 00:38:17.620]   and to women across the country
[00:38:17.620 --> 00:38:20.460]   that this is really a better measure?
[00:38:20.460 --> 00:38:22.740]   And again, I don't think it's an AI question.
[00:38:22.740 --> 00:38:25.940]   We can work more and make the algorithm even better,
[00:38:25.940 --> 00:38:28.140]   but I don't think that this is the current,
[00:38:28.140 --> 00:38:29.980]   you know, the barrier.
[00:38:29.980 --> 00:38:32.060]   The barrier is really this other piece
[00:38:32.060 --> 00:38:35.260]   that for some reason is not really explored.
[00:38:35.260 --> 00:38:36.860]   It's like anthropological piece.
[00:38:36.860 --> 00:38:39.860]   And coming back to your question about books,
[00:38:39.860 --> 00:38:42.980]   there is a book that I'm reading.
[00:38:42.980 --> 00:38:47.980]   It's called "American Sickness" by Elizabeth Rosenthal.
[00:38:47.980 --> 00:38:51.580]   And I got this book from my clinical collaborator,
[00:38:51.580 --> 00:38:53.100]   Dr. Connie Lehman.
[00:38:53.100 --> 00:38:54.820]   And I said, I know everything that I need to know
[00:38:54.820 --> 00:38:56.020]   about American health system,
[00:38:56.020 --> 00:38:59.220]   but you know, every page doesn't fail to surprise me.
[00:38:59.220 --> 00:39:03.140]   And I think that there is a lot of interesting
[00:39:03.140 --> 00:39:06.860]   and really deep lessons for people like us
[00:39:06.860 --> 00:39:09.620]   from computer science who are coming into this field
[00:39:09.620 --> 00:39:11.540]   to really understand how complex
[00:39:11.540 --> 00:39:14.820]   is the system of incentives in the system
[00:39:14.820 --> 00:39:17.660]   to understand how you really need to play
[00:39:17.660 --> 00:39:18.780]   to drive adoption.
[00:39:18.780 --> 00:39:21.160]   - You just said it's complex,
[00:39:21.160 --> 00:39:23.980]   but if we're trying to simplify it,
[00:39:23.980 --> 00:39:27.380]   who do you think most likely would be successful
[00:39:27.380 --> 00:39:29.540]   if we push on this group of people?
[00:39:29.540 --> 00:39:30.740]   Is it the doctors?
[00:39:30.740 --> 00:39:31.820]   Is it the hospitals?
[00:39:31.820 --> 00:39:34.300]   Is it the governments or policy makers?
[00:39:34.300 --> 00:39:37.380]   Is it the individual patients, consumers?
[00:39:38.860 --> 00:39:43.860]   Who needs to be inspired to most likely lead to adoption?
[00:39:43.860 --> 00:39:47.100]   Or is there no simple answer?
[00:39:47.100 --> 00:39:48.260]   - There's no simple answer,
[00:39:48.260 --> 00:39:51.980]   but I think there is a lot of good people in medical system
[00:39:51.980 --> 00:39:55.200]   who do want, you know, to make a change.
[00:39:55.200 --> 00:40:01.480]   And I think a lot of power will come from us as consumers,
[00:40:01.480 --> 00:40:04.260]   because we all are consumers or future consumers
[00:40:04.260 --> 00:40:06.500]   of healthcare services.
[00:40:06.500 --> 00:40:11.500]   And I think we can do so much more
[00:40:11.500 --> 00:40:15.540]   in explaining the potential and not in the hype terms
[00:40:15.540 --> 00:40:17.900]   and not saying that we now cured all Alzheimer
[00:40:17.900 --> 00:40:19.500]   and, you know, I'm really sick of reading
[00:40:19.500 --> 00:40:22.100]   these kind of articles which make these claims,
[00:40:22.100 --> 00:40:24.780]   but really to show with some examples
[00:40:24.780 --> 00:40:29.060]   what this implementation does and how it changes the care.
[00:40:29.060 --> 00:40:30.020]   Because I can't imagine,
[00:40:30.020 --> 00:40:32.620]   it doesn't matter what kind of politician it is,
[00:40:32.620 --> 00:40:35.220]   you know, we all are susceptible to these diseases.
[00:40:35.220 --> 00:40:37.740]   There is no one who is free.
[00:40:37.740 --> 00:40:41.060]   And eventually, you know, we all are humans
[00:40:41.060 --> 00:40:44.860]   and we're looking for a way to alleviate the suffering.
[00:40:44.860 --> 00:40:47.260]   And this is one possible way
[00:40:47.260 --> 00:40:49.300]   where we currently are underutilizing,
[00:40:49.300 --> 00:40:50.940]   which I think can help.
[00:40:50.940 --> 00:40:55.100]   - So it sounds like the biggest problems are outside of AI
[00:40:55.100 --> 00:40:57.960]   in terms of the biggest impact at this point.
[00:40:57.960 --> 00:41:00.420]   But are there any open problems
[00:41:00.420 --> 00:41:03.780]   in the application of ML to oncology in general?
[00:41:03.780 --> 00:41:07.540]   So improving the detection or any other creative methods,
[00:41:07.540 --> 00:41:09.620]   whether it's on the detection segmentations
[00:41:09.620 --> 00:41:11.780]   or the vision perception side
[00:41:11.780 --> 00:41:16.260]   or some other clever inference.
[00:41:16.260 --> 00:41:18.500]   Yeah, what in general in your view
[00:41:18.500 --> 00:41:20.340]   are the open problems in this space?
[00:41:20.340 --> 00:41:22.420]   - Yeah, I just want to mention that beside detection,
[00:41:22.420 --> 00:41:24.820]   another area where I am kind of quite active
[00:41:24.820 --> 00:41:28.580]   and I think it's really an increasingly important area
[00:41:28.580 --> 00:41:30.960]   in healthcare is drug design.
[00:41:30.960 --> 00:41:33.100]   - Absolutely.
[00:41:33.100 --> 00:41:36.900]   - Because, you know, it's fine if you detect something early
[00:41:36.900 --> 00:41:41.100]   but you still need to get drugs
[00:41:41.100 --> 00:41:43.860]   and new drugs for these conditions.
[00:41:43.860 --> 00:41:48.300]   And today, all of the drug design, ML is non-existent there.
[00:41:48.300 --> 00:41:52.980]   We don't have any drug that was developed by the ML model
[00:41:52.980 --> 00:41:56.220]   or even not developed, but at least even knew
[00:41:56.220 --> 00:41:59.260]   that ML model plays some significant role.
[00:41:59.260 --> 00:42:03.300]   I think this area was all the new ability
[00:42:03.300 --> 00:42:05.780]   to generate molecules with desired properties
[00:42:05.780 --> 00:42:10.780]   to do in silica screening is really a big open area.
[00:42:10.780 --> 00:42:12.740]   To be totally honest with you,
[00:42:12.740 --> 00:42:14.940]   when we are doing diagnostics and imaging,
[00:42:14.940 --> 00:42:17.260]   primarily taking the ideas that were developed
[00:42:17.260 --> 00:42:20.460]   for other areas and you applying them with some adaptation,
[00:42:20.460 --> 00:42:25.460]   the area of drug design is really technically interesting
[00:42:25.460 --> 00:42:27.980]   and exciting area.
[00:42:27.980 --> 00:42:30.380]   You need to work a lot with graphs
[00:42:30.380 --> 00:42:34.620]   and capture various 3D properties.
[00:42:34.620 --> 00:42:37.420]   There are lots and lots of opportunities
[00:42:37.420 --> 00:42:39.820]   to be technically creative.
[00:42:39.820 --> 00:42:44.820]   And I think there are a lot of open questions in this area.
[00:42:44.820 --> 00:42:48.820]   You know, we're already getting a lot of successes
[00:42:48.820 --> 00:42:52.700]   even with the kind of the first generation of these models,
[00:42:52.700 --> 00:42:56.500]   but there is much more new creative things that you can do.
[00:42:56.500 --> 00:42:59.300]   And what's very nice to see is that actually
[00:42:59.300 --> 00:43:04.180]   the more powerful, the more interesting models
[00:43:04.180 --> 00:43:05.460]   actually do do better.
[00:43:05.460 --> 00:43:10.100]   So there is a place to innovate
[00:43:10.100 --> 00:43:12.520]   in machine learning in this area.
[00:43:12.520 --> 00:43:16.660]   And some of these techniques are really unique
[00:43:16.660 --> 00:43:19.700]   to let's say to graph generation and other things.
[00:43:19.700 --> 00:43:23.980]   - Just to interrupt really quick, I'm sorry.
[00:43:23.980 --> 00:43:28.980]   Graph generation or graphs, drug discovery in general,
[00:43:28.980 --> 00:43:31.980]   how do you discover a drug?
[00:43:31.980 --> 00:43:33.340]   Is this chemistry?
[00:43:33.340 --> 00:43:37.500]   Is this trying to predict different chemical reactions?
[00:43:37.500 --> 00:43:41.180]   Or is it some kind of, what do graphs even represent
[00:43:41.180 --> 00:43:42.020]   in this space?
[00:43:42.020 --> 00:43:43.980]   - Oh, sorry, sorry.
[00:43:43.980 --> 00:43:45.300]   - And what's a drug?
[00:43:45.300 --> 00:43:47.100]   - Okay, so let's say you're thinking
[00:43:47.100 --> 00:43:48.500]   there are many different types of drugs,
[00:43:48.500 --> 00:43:50.540]   but let's say you're gonna talk about small molecules
[00:43:50.540 --> 00:43:52.820]   because I think today the majority of drugs
[00:43:52.820 --> 00:43:53.660]   are small molecules.
[00:43:53.660 --> 00:43:55.020]   So small molecule is a graph.
[00:43:55.020 --> 00:44:00.020]   The molecule is just where the node in the graph is an atom
[00:44:00.020 --> 00:44:01.500]   and then you have the bond.
[00:44:01.500 --> 00:44:03.220]   So it's really a graph representation
[00:44:03.220 --> 00:44:05.540]   if you're looking at it in 2D, correct?
[00:44:05.540 --> 00:44:07.460]   You can do it 3D, but let's say,
[00:44:07.460 --> 00:44:09.560]   let's keep it simple and stick in 2D.
[00:44:09.560 --> 00:44:14.740]   So pretty much my understanding today
[00:44:14.740 --> 00:44:17.740]   how it is done at scale in the companies
[00:44:17.740 --> 00:44:20.220]   you're without machine learning,
[00:44:20.220 --> 00:44:22.100]   you have high throughput screening.
[00:44:22.100 --> 00:44:23.740]   So you know that you are interested
[00:44:23.740 --> 00:44:26.540]   to get certain biological activity of the compound.
[00:44:26.540 --> 00:44:28.860]   So you scan a lot of compounds,
[00:44:28.860 --> 00:44:30.700]   like maybe hundreds of thousands,
[00:44:30.700 --> 00:44:33.020]   some really big number of compounds.
[00:44:33.020 --> 00:44:36.100]   You identify some compounds which have the right activity
[00:44:36.100 --> 00:44:39.260]   and then at this point, the chemists come
[00:44:39.260 --> 00:44:44.260]   and they're trying to now to optimize this original heat
[00:44:44.260 --> 00:44:46.340]   to different properties that you want it to be,
[00:44:46.340 --> 00:44:49.100]   maybe soluble, you want it to decrease toxicity,
[00:44:49.100 --> 00:44:51.660]   you want it to decrease the side effects.
[00:44:51.660 --> 00:44:54.060]   - Are those, sorry again to interrupt,
[00:44:54.060 --> 00:44:55.500]   can that be done in simulation
[00:44:55.500 --> 00:44:57.700]   or just by looking at the molecules
[00:44:57.700 --> 00:44:59.860]   or do you need to actually run reactions
[00:44:59.860 --> 00:45:02.500]   in real labs with laptops and stuff?
[00:45:02.500 --> 00:45:04.020]   - So when you do high throughput screening,
[00:45:04.020 --> 00:45:07.060]   you really do screening, it's in the lab.
[00:45:07.060 --> 00:45:09.140]   It's really the lab screening.
[00:45:09.140 --> 00:45:10.980]   You screen the molecules, correct?
[00:45:10.980 --> 00:45:12.620]   - I don't know what screening is.
[00:45:12.620 --> 00:45:15.100]   - The screening is just check them for certain property.
[00:45:15.100 --> 00:45:17.300]   - Like in the physical space, in the physical world,
[00:45:17.300 --> 00:45:18.780]   like actually there's a machine probably
[00:45:18.780 --> 00:45:21.460]   that's doing some, that's actually running the reaction.
[00:45:21.460 --> 00:45:22.900]   - Actually running the reactions, yeah.
[00:45:22.900 --> 00:45:25.420]   So there is a process where you can run
[00:45:25.420 --> 00:45:27.100]   and that's why it's called high throughput,
[00:45:27.100 --> 00:45:30.060]   it become cheaper and faster to do it
[00:45:30.060 --> 00:45:33.820]   on very big number of molecules.
[00:45:33.820 --> 00:45:37.660]   You run the screening, you identify potential,
[00:45:37.660 --> 00:45:42.340]   potential good starts and then when the chemists come in
[00:45:42.340 --> 00:45:44.060]   who have done it many times
[00:45:44.060 --> 00:45:46.180]   and then they can try to look at it and say,
[00:45:46.180 --> 00:45:48.260]   how can you change the molecule
[00:45:48.260 --> 00:45:53.260]   to get the desired profile in terms of all other properties?
[00:45:53.260 --> 00:45:56.500]   So maybe how do I make it more bioactive and so on?
[00:45:56.500 --> 00:45:59.460]   And there, the creativity of the chemists
[00:45:59.460 --> 00:46:04.460]   really is the one that determines the success of this design
[00:46:04.460 --> 00:46:09.300]   because again, they have a lot of domain knowledge
[00:46:09.300 --> 00:46:12.900]   of what works, how do you decrease the CCD and so on
[00:46:12.900 --> 00:46:15.020]   and that's what they do.
[00:46:15.020 --> 00:46:19.780]   So all the drugs that are currently in the FDA approved
[00:46:19.780 --> 00:46:22.140]   drugs or even drugs that are in clinical trials,
[00:46:22.140 --> 00:46:27.140]   they are designed using these domain experts
[00:46:27.140 --> 00:46:30.060]   which goes through this combinatorial space
[00:46:30.060 --> 00:46:31.980]   of molecules or graphs or whatever
[00:46:31.980 --> 00:46:35.180]   and find the right one or adjust it to be the right ones.
[00:46:35.180 --> 00:46:38.060]   - Sounds like the breast density heuristic
[00:46:38.060 --> 00:46:40.500]   from '67, the same echoes.
[00:46:40.500 --> 00:46:44.260]   - It's not necessarily that, it's really driven
[00:46:44.260 --> 00:46:46.860]   by deep understanding, it's not like they just observe it.
[00:46:46.860 --> 00:46:48.580]   I mean, they do deeply understand chemistry
[00:46:48.580 --> 00:46:50.460]   and they do understand how different groups
[00:46:50.460 --> 00:46:53.140]   and how does it change the properties.
[00:46:53.140 --> 00:46:56.660]   So there is a lot of science that gets into it
[00:46:56.660 --> 00:46:58.740]   and a lot of kind of simulation,
[00:46:58.740 --> 00:47:00.940]   how do you want it to behave?
[00:47:00.940 --> 00:47:03.900]   It's very, very complex.
[00:47:03.900 --> 00:47:06.140]   - So they're quite effective at this design, obviously.
[00:47:06.140 --> 00:47:08.420]   - Now, effective, yeah, we have drugs.
[00:47:08.420 --> 00:47:10.780]   Like depending on how do you measure effective?
[00:47:10.780 --> 00:47:13.940]   If you measure it in terms of cost, it's prohibitive.
[00:47:13.940 --> 00:47:15.780]   If you measure it in terms of times,
[00:47:15.780 --> 00:47:18.380]   we have lots of diseases for which we don't have any drugs
[00:47:18.380 --> 00:47:20.020]   and we don't even know how to approach
[00:47:20.020 --> 00:47:23.420]   and don't need to mention few drugs
[00:47:23.420 --> 00:47:27.100]   or neurodegenerative disease drugs that fail.
[00:47:27.100 --> 00:47:32.100]   So there are lots of trials that fail in later stages
[00:47:32.100 --> 00:47:35.140]   which is really catastrophic from the financial perspective.
[00:47:35.140 --> 00:47:39.500]   So is it the effective, the most effective mechanism?
[00:47:39.500 --> 00:47:41.420]   Absolutely no, but this is the only one
[00:47:41.420 --> 00:47:42.700]   that currently works.
[00:47:43.700 --> 00:47:47.340]   - And I was closely interacting
[00:47:47.340 --> 00:47:48.660]   with people in pharmaceutical industry.
[00:47:48.660 --> 00:47:50.780]   I was really fascinated on how sharp
[00:47:50.780 --> 00:47:54.700]   and what a deep understanding of the domain do they have.
[00:47:54.700 --> 00:47:56.460]   It's not observation driven.
[00:47:56.460 --> 00:47:59.620]   There is really a lot of science behind what they do.
[00:47:59.620 --> 00:48:01.700]   But if you ask me, can machine learning change it?
[00:48:01.700 --> 00:48:04.700]   I firmly believe yes,
[00:48:04.700 --> 00:48:07.260]   because even the most experienced chemists
[00:48:07.260 --> 00:48:10.500]   cannot hold in their memory and understanding
[00:48:10.500 --> 00:48:13.700]   everything that you can learn from millions
[00:48:13.700 --> 00:48:15.460]   of molecules and reactions.
[00:48:15.460 --> 00:48:19.940]   - And the space of graphs is a totally new space.
[00:48:19.940 --> 00:48:22.100]   I mean, it's a really interesting space
[00:48:22.100 --> 00:48:24.020]   for machine learning to explore, graph generation.
[00:48:24.020 --> 00:48:26.300]   - Yeah, so there are a lot of things that you can do here.
[00:48:26.300 --> 00:48:28.780]   So we do a lot of work.
[00:48:28.780 --> 00:48:31.660]   So the first tool that we started with
[00:48:31.660 --> 00:48:36.340]   was the tool that can predict properties of the molecules.
[00:48:36.340 --> 00:48:39.460]   So you can just give the molecule and the property.
[00:48:39.460 --> 00:48:41.340]   It can be bioactivity property
[00:48:41.340 --> 00:48:44.300]   or it can be some other property.
[00:48:44.300 --> 00:48:46.500]   And you train the molecules
[00:48:46.500 --> 00:48:50.060]   and you can now take a new molecule
[00:48:50.060 --> 00:48:52.220]   and predict this property.
[00:48:52.220 --> 00:48:54.940]   Now, when people started working in this area,
[00:48:54.940 --> 00:48:56.020]   it is something very simple.
[00:48:56.020 --> 00:48:58.620]   They do kind of existing fingerprints,
[00:48:58.620 --> 00:49:00.780]   which is kind of handcrafted features of the molecule
[00:49:00.780 --> 00:49:03.020]   when you break the graph to substructures
[00:49:03.020 --> 00:49:06.020]   and then you run, I don't know, feed forward neural network.
[00:49:06.020 --> 00:49:08.540]   And what was interesting to see that clearly,
[00:49:08.540 --> 00:49:11.060]   this was not the most effective way to proceed.
[00:49:11.060 --> 00:49:14.140]   And you need to have much more complex models
[00:49:14.140 --> 00:49:16.340]   that can induce the representation,
[00:49:16.340 --> 00:49:19.260]   which can translate this graph into the embeddings
[00:49:19.260 --> 00:49:21.340]   and do these predictions.
[00:49:21.340 --> 00:49:23.260]   So this is one direction.
[00:49:23.260 --> 00:49:25.340]   Then another direction, which is kind of related
[00:49:25.340 --> 00:49:29.260]   is not only to stop by looking at the embedding itself,
[00:49:29.260 --> 00:49:32.860]   but actually modify it to produce better molecules.
[00:49:32.860 --> 00:49:36.060]   So you can think about it as machine translation,
[00:49:36.060 --> 00:49:38.220]   that you can start with a molecule
[00:49:38.220 --> 00:49:40.660]   and then there is an improved version of molecule.
[00:49:40.660 --> 00:49:42.460]   And you can again, within code,
[00:49:42.460 --> 00:49:43.940]   translate it into the hidden space
[00:49:43.940 --> 00:49:46.140]   and then learn how to modify it to improve
[00:49:46.140 --> 00:49:49.420]   the in some ways version of the molecules.
[00:49:49.420 --> 00:49:52.700]   So that's, it's kind of really exciting.
[00:49:52.700 --> 00:49:54.780]   We already have seen that the property prediction
[00:49:54.780 --> 00:49:56.220]   works pretty well.
[00:49:56.220 --> 00:49:59.820]   And now we are generating molecules
[00:49:59.820 --> 00:50:01.900]   and there is actually labs
[00:50:01.900 --> 00:50:04.260]   which are manufacturing this molecule.
[00:50:04.260 --> 00:50:06.420]   So we'll see where it will get us.
[00:50:06.420 --> 00:50:07.820]   - Okay, that's really exciting.
[00:50:07.820 --> 00:50:08.940]   That's a lot of promise.
[00:50:08.940 --> 00:50:11.900]   Speaking of machine translation and embeddings,
[00:50:11.900 --> 00:50:16.180]   you have done a lot of really great research in NLP,
[00:50:16.180 --> 00:50:17.580]   natural language processing.
[00:50:17.580 --> 00:50:21.540]   Can you tell me your journey through NLP?
[00:50:21.540 --> 00:50:25.100]   What ideas, problems, approaches were you working on?
[00:50:25.100 --> 00:50:26.500]   Were you fascinated with?
[00:50:26.500 --> 00:50:31.380]   Did you explore before this magic of deep learning
[00:50:31.380 --> 00:50:34.060]   re-emerged and after?
[00:50:34.060 --> 00:50:35.940]   - So when I started my work in NLP,
[00:50:35.940 --> 00:50:38.140]   it was in '97.
[00:50:38.140 --> 00:50:39.420]   This was very interesting time.
[00:50:39.420 --> 00:50:42.540]   It was exactly the time that I came to ACL
[00:50:42.540 --> 00:50:46.100]   and the time I could barely understand English.
[00:50:46.100 --> 00:50:48.460]   But it was exactly like the transition point
[00:50:48.460 --> 00:50:51.420]   because half of the papers were really,
[00:50:51.420 --> 00:50:53.460]   you know, rule-based approaches
[00:50:53.460 --> 00:50:56.140]   where people took more kind of heavy linguistic approaches
[00:50:56.140 --> 00:51:00.020]   for small domains and try to build up from there.
[00:51:00.020 --> 00:51:02.180]   And then there were the first generation of papers
[00:51:02.180 --> 00:51:04.460]   which were corpus-based papers.
[00:51:04.460 --> 00:51:06.420]   And they were very simple in our terms
[00:51:06.420 --> 00:51:07.900]   when you collect some statistics
[00:51:07.900 --> 00:51:10.060]   and do prediction based on them.
[00:51:10.060 --> 00:51:12.340]   But I found it really fascinating that, you know,
[00:51:12.340 --> 00:51:15.620]   one community can think so very differently
[00:51:15.620 --> 00:51:19.260]   about the problem.
[00:51:19.260 --> 00:51:22.860]   And I remember my first paper that I wrote,
[00:51:22.860 --> 00:51:24.500]   it didn't have a single formula.
[00:51:24.500 --> 00:51:25.740]   It didn't have evaluation.
[00:51:25.740 --> 00:51:28.340]   It just had examples of outputs.
[00:51:28.340 --> 00:51:32.060]   And this was a standard of the field at the time.
[00:51:32.060 --> 00:51:34.940]   In some ways, I mean, people maybe just started
[00:51:34.940 --> 00:51:37.860]   emphasizing their empirical evaluation,
[00:51:37.860 --> 00:51:40.100]   but for many applications like summarization,
[00:51:40.100 --> 00:51:42.780]   you just show some examples of outputs.
[00:51:42.780 --> 00:51:44.660]   And then increasingly, you can see that
[00:51:44.660 --> 00:51:48.300]   how the statistical approaches dominated the field.
[00:51:48.300 --> 00:51:52.100]   And we've seen, you know, increased performance
[00:51:52.100 --> 00:51:56.020]   across many basic tasks.
[00:51:56.020 --> 00:51:59.300]   The sad part of the story may be that
[00:51:59.300 --> 00:52:01.580]   if you look again through this journey,
[00:52:01.580 --> 00:52:05.100]   we see that the role of linguistics
[00:52:05.100 --> 00:52:07.460]   in some ways greatly diminishes.
[00:52:07.460 --> 00:52:11.580]   And I think that you really need to look
[00:52:11.580 --> 00:52:14.540]   through the whole proceeding to find one or two papers
[00:52:14.540 --> 00:52:17.260]   which make some interesting linguistic references.
[00:52:17.260 --> 00:52:18.460]   - You mean today.
[00:52:18.460 --> 00:52:19.740]   - Today, today.
[00:52:19.740 --> 00:52:20.580]   This was definitely--
[00:52:20.580 --> 00:52:21.620]   - Things like syntactic trees,
[00:52:21.620 --> 00:52:24.420]   just even basically against our conversation
[00:52:24.420 --> 00:52:27.540]   about human understanding of language,
[00:52:27.540 --> 00:52:30.300]   which I guess what linguistics would be,
[00:52:30.300 --> 00:52:34.300]   structured, hierarchical, representing language
[00:52:34.300 --> 00:52:37.140]   in a way that's human explainable, understandable,
[00:52:37.140 --> 00:52:39.340]   is missing today.
[00:52:39.340 --> 00:52:41.140]   - I don't know if it is,
[00:52:41.140 --> 00:52:43.620]   what is explainable and understandable.
[00:52:43.620 --> 00:52:45.940]   In the end, you know, we perform functions,
[00:52:45.940 --> 00:52:50.180]   and it's okay to have a machine which performs a function.
[00:52:50.180 --> 00:52:53.220]   Like when you're thinking about your calculator, correct?
[00:52:53.220 --> 00:52:56.100]   Your calculator can do calculation very different
[00:52:56.100 --> 00:52:57.620]   from you would do the calculation,
[00:52:57.620 --> 00:52:58.860]   but it's very effective in it.
[00:52:58.860 --> 00:53:02.540]   And this is fine if we can achieve certain tasks
[00:53:02.540 --> 00:53:04.460]   with high accuracy,
[00:53:04.460 --> 00:53:07.180]   it doesn't necessarily mean that it has to understand
[00:53:07.180 --> 00:53:09.260]   in the same way as we understand.
[00:53:09.260 --> 00:53:11.260]   In some ways, it's even naive to request
[00:53:11.260 --> 00:53:14.940]   because you have so many other sources of information
[00:53:14.940 --> 00:53:17.900]   that are absent when you are training your system.
[00:53:17.900 --> 00:53:20.020]   So it's okay as it delivers it.
[00:53:20.020 --> 00:53:21.500]   And I will tell you one application
[00:53:21.500 --> 00:53:22.780]   that is really fascinating.
[00:53:22.780 --> 00:53:24.260]   In '97, when it came to ACL,
[00:53:24.260 --> 00:53:25.860]   there were some papers on machine translation.
[00:53:25.860 --> 00:53:27.420]   They were like primitive,
[00:53:27.420 --> 00:53:31.020]   like people were trying really, really simple.
[00:53:31.020 --> 00:53:34.220]   And the feeling, my feeling was that, you know,
[00:53:34.220 --> 00:53:36.220]   to make real machine translation system,
[00:53:36.220 --> 00:53:39.540]   it's like to fly in the moon and build a house there
[00:53:39.540 --> 00:53:41.540]   and a garden and live happily ever after.
[00:53:41.540 --> 00:53:42.580]   I mean, it's like impossible.
[00:53:42.580 --> 00:53:46.700]   I never could imagine that within, you know, 10 years,
[00:53:46.700 --> 00:53:48.500]   we would already see the system working.
[00:53:48.500 --> 00:53:51.380]   And now, you know, nobody is even surprised
[00:53:51.380 --> 00:53:54.420]   to utilize the system on daily basis.
[00:53:54.420 --> 00:53:56.220]   So this was like a huge, huge progress
[00:53:56.220 --> 00:53:57.860]   in the sense that people for a very long time
[00:53:57.860 --> 00:54:00.820]   tried to solve using other mechanisms
[00:54:00.820 --> 00:54:03.180]   and they were unable to solve it.
[00:54:03.180 --> 00:54:06.140]   That's why coming back to a question about biology,
[00:54:06.140 --> 00:54:10.780]   that in linguistics, people try to go this way
[00:54:10.780 --> 00:54:13.500]   and try to write the syntactic trees
[00:54:13.500 --> 00:54:17.860]   and try to obstruct it and to find the right representation.
[00:54:17.860 --> 00:54:22.220]   And, you know, they couldn't get very far
[00:54:22.220 --> 00:54:25.980]   with this understanding while these models,
[00:54:25.980 --> 00:54:28.740]   using, you know, other sources,
[00:54:28.740 --> 00:54:31.700]   actually capable to make a lot of progress.
[00:54:31.700 --> 00:54:33.940]   Now, I'm not naive to think
[00:54:33.940 --> 00:54:36.820]   that we are in this paradise space in NLP.
[00:54:36.820 --> 00:54:38.580]   And I'm sure as you know,
[00:54:38.580 --> 00:54:40.860]   that when we slightly change the domain
[00:54:40.860 --> 00:54:42.620]   and when we decrease the amount of training,
[00:54:42.620 --> 00:54:44.740]   it can do like really bizarre and funny thing.
[00:54:44.740 --> 00:54:46.500]   But I think it's just a matter
[00:54:46.500 --> 00:54:48.540]   of improving generalization capacity,
[00:54:48.540 --> 00:54:51.500]   which is just a technical question.
[00:54:51.500 --> 00:54:54.300]   - Wow, so that's the question.
[00:54:54.300 --> 00:54:57.020]   How much of language understanding
[00:54:57.020 --> 00:54:59.180]   can be solved with deep neural networks?
[00:54:59.180 --> 00:55:03.740]   In your intuition, I mean, it's unknown, I suppose.
[00:55:03.740 --> 00:55:07.620]   But as we start to creep towards romantic notions
[00:55:07.620 --> 00:55:10.620]   of the spirit of the Turing test
[00:55:10.620 --> 00:55:14.220]   and conversation and dialogue
[00:55:14.220 --> 00:55:19.020]   and something that maybe to me or to us silly humans
[00:55:19.020 --> 00:55:21.620]   feels like it needs real understanding,
[00:55:21.620 --> 00:55:23.500]   how much can that be achieved
[00:55:23.500 --> 00:55:27.180]   with these neural networks or statistical methods?
[00:55:27.180 --> 00:55:33.060]   - So I guess I am very much driven by the outcomes.
[00:55:33.060 --> 00:55:35.420]   Can we achieve the performance,
[00:55:35.420 --> 00:55:40.420]   which would be satisfactory for us for different tasks?
[00:55:40.420 --> 00:55:43.100]   Now, if you again look at machine translation systems,
[00:55:43.100 --> 00:55:46.020]   which are, you know, trained on large amounts of data,
[00:55:46.020 --> 00:55:48.780]   they really can do a remarkable job
[00:55:48.780 --> 00:55:51.340]   relatively to where they've been a few years ago.
[00:55:51.340 --> 00:55:54.580]   And if you project into the future,
[00:55:54.580 --> 00:55:56.940]   if it will be the same speed of improvement,
[00:55:56.940 --> 00:56:00.020]   you know, this is great.
[00:56:00.020 --> 00:56:01.900]   Now, does it bother me that it's not doing
[00:56:01.900 --> 00:56:04.820]   the same translation as we are doing?
[00:56:04.820 --> 00:56:06.580]   Now, if you go to cognitive science,
[00:56:06.580 --> 00:56:09.420]   we still don't really understand what we are doing.
[00:56:09.420 --> 00:56:11.820]   I mean, there are a lot of theories
[00:56:11.820 --> 00:56:13.820]   and there is obviously a lot of progress in studying,
[00:56:13.820 --> 00:56:16.380]   but our understanding what exactly goes on,
[00:56:16.380 --> 00:56:18.820]   you know, in our brains when we process language
[00:56:18.820 --> 00:56:21.740]   is still not crystal clear and precise
[00:56:21.740 --> 00:56:25.420]   that we can translate it into machines.
[00:56:25.420 --> 00:56:29.740]   What does bother me is that, you know, again,
[00:56:29.740 --> 00:56:31.660]   that machines can be extremely brittle
[00:56:31.660 --> 00:56:33.940]   when you go out of your comfort zone
[00:56:33.940 --> 00:56:36.020]   of when there is a distributional shift
[00:56:36.020 --> 00:56:37.260]   between training and testing.
[00:56:37.260 --> 00:56:38.980]   And it have been years and years,
[00:56:38.980 --> 00:56:41.300]   every year when I teach an LP class,
[00:56:41.300 --> 00:56:43.540]   you know, I show them some examples of translation
[00:56:43.540 --> 00:56:47.260]   from some newspaper in Hebrew or whatever, it was perfect.
[00:56:47.260 --> 00:56:51.300]   And then I have a recipe that Tommy Yakala's sister
[00:56:51.300 --> 00:56:53.900]   sent me a while ago and it was written in Finnish
[00:56:53.900 --> 00:56:55.700]   of Carilion pies.
[00:56:55.700 --> 00:56:59.260]   And it's just a terrible translation.
[00:56:59.260 --> 00:57:01.460]   You cannot understand anything, what it does.
[00:57:01.460 --> 00:57:04.300]   It's not like some syntactic mistakes, it's just terrible.
[00:57:04.300 --> 00:57:07.020]   And year after year I try it and it will translate,
[00:57:07.020 --> 00:57:08.980]   and year after year it does this terrible work
[00:57:08.980 --> 00:57:10.980]   because I guess, you know, the recipes
[00:57:10.980 --> 00:57:14.580]   are not a big part of the training repertoire.
[00:57:15.460 --> 00:57:18.020]   - So, but in terms of outcomes,
[00:57:18.020 --> 00:57:21.140]   that's a really clean, good way to look at it.
[00:57:21.140 --> 00:57:23.200]   I guess the question I was asking is,
[00:57:23.200 --> 00:57:27.740]   do you think, imagine a future,
[00:57:27.740 --> 00:57:29.820]   do you think the current approaches
[00:57:29.820 --> 00:57:32.500]   can pass the Turing test in the way,
[00:57:32.500 --> 00:57:37.060]   in the best possible formulation of the Turing test?
[00:57:37.060 --> 00:57:39.500]   Which is, would you wanna have a conversation
[00:57:39.500 --> 00:57:42.380]   with a neural network for an hour?
[00:57:42.380 --> 00:57:43.220]   - Oh God, no.
[00:57:44.780 --> 00:57:45.820]   There are not that many people
[00:57:45.820 --> 00:57:48.020]   that I would wanna talk for an hour.
[00:57:48.020 --> 00:57:49.700]   But-- - There are some people
[00:57:49.700 --> 00:57:51.500]   in this world, alive or not,
[00:57:51.500 --> 00:57:53.260]   that you would like to talk to for an hour.
[00:57:53.260 --> 00:57:56.700]   Could a neural network achieve that outcome?
[00:57:56.700 --> 00:57:58.180]   - So I think it would be really hard
[00:57:58.180 --> 00:58:01.100]   to create a successful training set
[00:58:01.100 --> 00:58:03.820]   which would enable it to have a conversation,
[00:58:03.820 --> 00:58:06.700]   a contextual conversation for an hour.
[00:58:06.700 --> 00:58:08.140]   - So you think it's a problem of data, perhaps?
[00:58:08.140 --> 00:58:09.940]   - I think in some ways it's a problem of data.
[00:58:09.940 --> 00:58:12.460]   It's a problem both of data and the problem
[00:58:12.460 --> 00:58:15.660]   of the way we're training our systems,
[00:58:15.660 --> 00:58:18.060]   their ability to truly to generalize,
[00:58:18.060 --> 00:58:19.300]   to be very compositional.
[00:58:19.300 --> 00:58:20.460]   In some ways it's limited.
[00:58:20.460 --> 00:58:24.140]   You know, in the current capacity, at least,
[00:58:24.140 --> 00:58:27.980]   you know, we can translate well,
[00:58:27.980 --> 00:58:31.340]   we can, you know, find information well,
[00:58:31.340 --> 00:58:32.540]   we can extract information.
[00:58:32.540 --> 00:58:35.180]   So there are many capacities in which it's doing very well.
[00:58:35.180 --> 00:58:38.000]   And you can ask me, would you trust the machine
[00:58:38.000 --> 00:58:39.780]   to translate for you and use it as a source?
[00:58:39.780 --> 00:58:40.820]   I would say absolutely,
[00:58:40.820 --> 00:58:43.540]   especially if we're talking about newspaper data
[00:58:43.540 --> 00:58:46.740]   or other data, which is in the realm of its own training set,
[00:58:46.740 --> 00:58:47.880]   I would say yes.
[00:58:47.880 --> 00:58:52.900]   But, you know, having conversations with the machine,
[00:58:52.900 --> 00:58:56.460]   it's not something that I would choose to do.
[00:58:56.460 --> 00:58:58.140]   But you know, I would tell you something,
[00:58:58.140 --> 00:58:59.420]   talking about Turing tests
[00:58:59.420 --> 00:59:02.940]   and about all this kind of ELISA conversations,
[00:59:02.940 --> 00:59:05.540]   I remember visiting Tencent in China
[00:59:05.540 --> 00:59:06.940]   and they have this chat board
[00:59:06.940 --> 00:59:09.520]   and they claim that it's like really humongous amount
[00:59:09.520 --> 00:59:11.820]   of the local population, which like for hours
[00:59:11.820 --> 00:59:12.900]   talks to the chat board.
[00:59:12.900 --> 00:59:15.320]   To me it was, I cannot believe it,
[00:59:15.320 --> 00:59:17.100]   but apparently it's like documented
[00:59:17.100 --> 00:59:20.760]   that there are some people who enjoy this conversation.
[00:59:20.760 --> 00:59:24.540]   And you know, it brought to me another MIT story
[00:59:24.540 --> 00:59:26.940]   about ELISA and Weizenbaum.
[00:59:26.940 --> 00:59:29.340]   I don't know if you're familiar with this story.
[00:59:29.340 --> 00:59:31.020]   So Weizenbaum was a professor at MIT
[00:59:31.020 --> 00:59:32.580]   and when he developed this ELISA,
[00:59:32.580 --> 00:59:34.620]   which was just doing string matching,
[00:59:34.620 --> 00:59:38.540]   very trivial, like restating of what you said
[00:59:38.540 --> 00:59:41.260]   with very few rules, no syntax.
[00:59:41.260 --> 00:59:43.740]   Apparently there were secretaries at MIT
[00:59:43.740 --> 00:59:46.560]   that would sit for hours and converse
[00:59:46.560 --> 00:59:48.180]   with this trivial thing.
[00:59:48.180 --> 00:59:50.180]   And at the time there was no beautiful interfaces,
[00:59:50.180 --> 00:59:51.820]   so you actually need to go through the pain
[00:59:51.820 --> 00:59:53.540]   of communicating.
[00:59:53.540 --> 00:59:56.900]   And Weizenbaum himself was so horrified by this phenomena
[00:59:56.900 --> 00:59:59.260]   that people can believe enough to the machine
[00:59:59.260 --> 01:00:00.820]   that you just need to give them the hint
[01:00:00.820 --> 01:00:03.940]   that machine understands you and you can complete the rest.
[01:00:03.940 --> 01:00:05.420]   Then he kind of stopped this research
[01:00:05.460 --> 01:00:08.660]   and went into kind of trying to understand
[01:00:08.660 --> 01:00:11.460]   what this artificial intelligence can do to our brains.
[01:00:11.460 --> 01:00:15.400]   So my point is, you know, how much,
[01:00:15.400 --> 01:00:19.340]   it's not how good is the technology,
[01:00:19.340 --> 01:00:22.620]   it's how ready we are to believe
[01:00:22.620 --> 01:00:25.580]   that it delivers the good that we are trying to get.
[01:00:25.580 --> 01:00:27.220]   - That's a really beautiful way to put it.
[01:00:27.220 --> 01:00:29.800]   I, by the way, I'm not horrified by that possibility,
[01:00:29.800 --> 01:00:33.740]   but inspired by it because, I mean,
[01:00:34.780 --> 01:00:37.060]   human connection, whether it's through language
[01:00:37.060 --> 01:00:42.060]   or through love, it seems like it's very amenable
[01:00:42.060 --> 01:00:46.060]   to machine learning and the rest is just
[01:00:46.060 --> 01:00:49.340]   challenges of psychology.
[01:00:49.340 --> 01:00:52.460]   Like you said, the secretaries who enjoy spending hours.
[01:00:52.460 --> 01:00:55.020]   I would say I would describe most of our lives
[01:00:55.020 --> 01:00:58.060]   as enjoying spending hours with those we love
[01:00:58.060 --> 01:01:00.820]   for very silly reasons.
[01:01:00.820 --> 01:01:02.780]   All we're doing is keyword matching as well.
[01:01:02.780 --> 01:01:05.660]   So I'm not sure how much intelligence we exhibit
[01:01:05.660 --> 01:01:08.140]   to each other with the people we love
[01:01:08.140 --> 01:01:09.820]   that we're close with.
[01:01:09.820 --> 01:01:12.660]   So it's a very interesting point
[01:01:12.660 --> 01:01:16.020]   of what it means to pass the Turing test with language.
[01:01:16.020 --> 01:01:18.220]   I think you're right in terms of conversation.
[01:01:18.220 --> 01:01:23.140]   I think machine translation has very clear performance
[01:01:23.140 --> 01:01:24.420]   and improvement, right?
[01:01:24.420 --> 01:01:28.020]   What it means to have a fulfilling conversation
[01:01:28.020 --> 01:01:32.660]   is very, very person dependent and context dependent
[01:01:32.660 --> 01:01:33.580]   and so on.
[01:01:33.580 --> 01:01:36.340]   That's, yeah, it's very well put.
[01:01:36.340 --> 01:01:40.740]   But in your view, what's a benchmark in natural language,
[01:01:40.740 --> 01:01:43.640]   a test that's just out of reach right now,
[01:01:43.640 --> 01:01:46.060]   but we might be able to, that's exciting?
[01:01:46.060 --> 01:01:49.100]   Is it in perfecting machine translation
[01:01:49.100 --> 01:01:51.920]   or is there other, is it summarization?
[01:01:51.920 --> 01:01:52.760]   What's out there?
[01:01:52.760 --> 01:01:55.820]   - I think it goes across specific application.
[01:01:55.820 --> 01:01:59.500]   It's more about the ability to learn from few examples
[01:01:59.500 --> 01:02:01.460]   for real, what we call few short learning
[01:02:01.460 --> 01:02:04.980]   and all these cases, because the way we publish
[01:02:04.980 --> 01:02:07.540]   these papers today, we say, if we have,
[01:02:07.540 --> 01:02:11.340]   like naively we get 55, but now we had a few example
[01:02:11.340 --> 01:02:12.500]   and we can move to 65.
[01:02:12.500 --> 01:02:14.760]   None of these methods actually are realistically
[01:02:14.760 --> 01:02:15.980]   doing anything useful.
[01:02:15.980 --> 01:02:18.540]   You cannot use them today.
[01:02:18.540 --> 01:02:23.540]   And the ability to be able to generalize
[01:02:23.540 --> 01:02:28.580]   and to move or to be autonomous in finding the data
[01:02:28.940 --> 01:02:33.180]   that you need to learn, to be able to perfect new task
[01:02:33.180 --> 01:02:37.220]   or new language, this is an area where I think
[01:02:37.220 --> 01:02:40.940]   we really need to move forward to,
[01:02:40.940 --> 01:02:43.020]   and we are not yet there.
[01:02:43.020 --> 01:02:46.540]   - Are you at all excited, curious by the possibility
[01:02:46.540 --> 01:02:48.500]   of creating human level intelligence?
[01:02:48.500 --> 01:02:52.540]   Is this, 'cause you've been very, in your discussion,
[01:02:52.540 --> 01:02:56.360]   so if we look at oncology, you're trying to use
[01:02:56.360 --> 01:02:58.100]   machine learning to help the world
[01:02:58.100 --> 01:02:59.660]   in terms of alleviating suffering.
[01:02:59.660 --> 01:03:02.340]   If you look at natural language processing,
[01:03:02.340 --> 01:03:04.500]   you're focused on the outcomes of improving
[01:03:04.500 --> 01:03:06.820]   practical things like machine translation.
[01:03:06.820 --> 01:03:09.860]   But human level intelligence is a thing
[01:03:09.860 --> 01:03:13.800]   that our civilization has dreamed about creating,
[01:03:13.800 --> 01:03:15.740]   super human level intelligence.
[01:03:15.740 --> 01:03:16.940]   Do you think about this?
[01:03:16.940 --> 01:03:19.040]   Do you think it's at all within our reach?
[01:03:19.040 --> 01:03:23.720]   - So as you said yourself earlier, talking about,
[01:03:25.220 --> 01:03:28.980]   how do you perceive our communications with each other,
[01:03:28.980 --> 01:03:31.940]   that we're matching keywords and certain behaviors
[01:03:31.940 --> 01:03:32.780]   and so on.
[01:03:32.780 --> 01:03:37.220]   And then whenever one assesses, let's say,
[01:03:37.220 --> 01:03:39.900]   relations with another person, you have separate
[01:03:39.900 --> 01:03:42.420]   kind of measurements and outcomes inside your head
[01:03:42.420 --> 01:03:45.860]   that determine what is the status of the relation.
[01:03:45.860 --> 01:03:48.620]   So one way, this is this classical level,
[01:03:48.620 --> 01:03:49.600]   what is the intelligence?
[01:03:49.600 --> 01:03:51.860]   Is it the fact that now we are gonna do the same way
[01:03:51.860 --> 01:03:53.980]   as human is doing when we don't even understand
[01:03:53.980 --> 01:03:55.460]   what the human is doing?
[01:03:55.460 --> 01:03:59.100]   Or we now have an ability to deliver these outcomes,
[01:03:59.100 --> 01:04:01.260]   but not in one area, not in NLP,
[01:04:01.260 --> 01:04:03.940]   not just to translate or just to answer questions,
[01:04:03.940 --> 01:04:06.920]   but across many, many areas that we can achieve
[01:04:06.920 --> 01:04:09.740]   the functionalities that humans can achieve
[01:04:09.740 --> 01:04:12.380]   with the ability to learn and do other things.
[01:04:12.380 --> 01:04:15.500]   I think this is, and this we can actually measure
[01:04:15.500 --> 01:04:17.560]   how far we are.
[01:04:17.560 --> 01:04:21.580]   And that's what makes me excited that we,
[01:04:21.580 --> 01:04:23.780]   in my lifetime, at least so far what we've seen,
[01:04:23.780 --> 01:04:26.260]   it's like tremendous progress across
[01:04:26.260 --> 01:04:28.740]   with these different functionalities.
[01:04:28.740 --> 01:04:33.660]   And I think it will be really exciting to see
[01:04:33.660 --> 01:04:35.540]   where we will be.
[01:04:35.540 --> 01:04:39.340]   And again, one way to think about it,
[01:04:39.340 --> 01:04:41.860]   there are machines which are improving their functionality.
[01:04:41.860 --> 01:04:44.940]   Another one is to think about us with our brains,
[01:04:44.940 --> 01:04:49.060]   which are imperfect, how they can be accelerated
[01:04:49.060 --> 01:04:54.060]   by this technology as it becomes stronger and stronger.
[01:04:54.060 --> 01:04:58.580]   Coming back to another book that I love,
[01:04:58.580 --> 01:05:00.900]   "Flowers for Algernon."
[01:05:00.900 --> 01:05:02.100]   Have you read this book?
[01:05:02.100 --> 01:05:02.940]   - Yes.
[01:05:02.940 --> 01:05:05.700]   - So there is this point that the patient gets
[01:05:05.700 --> 01:05:07.980]   this miracle cure which changes his brain,
[01:05:07.980 --> 01:05:11.020]   and all of a sudden they see life in a different way
[01:05:11.020 --> 01:05:13.300]   and can do certain things better,
[01:05:13.300 --> 01:05:14.860]   but certain things much worse.
[01:05:16.460 --> 01:05:21.460]   So you can imagine this kind of computer augmented cognition
[01:05:21.460 --> 01:05:24.780]   where it can bring you that now in the same way
[01:05:24.780 --> 01:05:28.100]   as the cars enable us to get to places
[01:05:28.100 --> 01:05:30.040]   where we've never been before,
[01:05:30.040 --> 01:05:31.580]   can we think differently?
[01:05:31.580 --> 01:05:33.580]   Can we think faster?
[01:05:33.580 --> 01:05:36.660]   And we already see a lot of it happening
[01:05:36.660 --> 01:05:38.220]   in how it impacts us,
[01:05:38.220 --> 01:05:42.180]   but I think we have a long way to go there.
[01:05:42.180 --> 01:05:44.980]   - So that's sort of artificial intelligence
[01:05:44.980 --> 01:05:49.980]   and technology augmenting our intelligence as humans.
[01:05:49.980 --> 01:05:55.420]   Yesterday, a company called Neuralink announced,
[01:05:55.420 --> 01:05:56.780]   they did this whole demonstration,
[01:05:56.780 --> 01:05:58.900]   I don't know if you saw it.
[01:05:58.900 --> 01:06:02.660]   They demonstrated brain, computer, brain machine interface,
[01:06:02.660 --> 01:06:06.100]   where there's like a sewing machine for the brain.
[01:06:06.100 --> 01:06:11.100]   A lot of that is quite out there
[01:06:11.100 --> 01:06:14.020]   in terms of things that some people would say
[01:06:14.020 --> 01:06:16.300]   are impossible, but they're dreamers
[01:06:16.300 --> 01:06:18.060]   and want to engineer systems like that.
[01:06:18.060 --> 01:06:20.340]   Do you see, based on what you just said,
[01:06:20.340 --> 01:06:23.780]   a hope for that more direct interaction with the brain?
[01:06:23.780 --> 01:06:27.020]   - I think there are different ways.
[01:06:27.020 --> 01:06:28.980]   One is a direct interaction with the brain,
[01:06:28.980 --> 01:06:30.860]   and again, there are lots of companies
[01:06:30.860 --> 01:06:32.240]   that work in this space,
[01:06:32.240 --> 01:06:35.060]   and I think there will be a lot of developments.
[01:06:35.060 --> 01:06:36.540]   But I'm just thinking that many times
[01:06:36.540 --> 01:06:39.860]   we are not aware of our feelings of motivation
[01:06:39.860 --> 01:06:41.420]   of what drives us.
[01:06:41.420 --> 01:06:44.120]   Like let me give you a trivial example, our attention.
[01:06:44.120 --> 01:06:47.260]   There are a lot of studies that demonstrate
[01:06:47.260 --> 01:06:49.220]   that it takes a while to a person to understand
[01:06:49.220 --> 01:06:51.060]   that they are not attentive anymore.
[01:06:51.060 --> 01:06:52.180]   And we know that there are people
[01:06:52.180 --> 01:06:54.540]   who really have strong capacity to hold attention.
[01:06:54.540 --> 01:06:55.980]   There are another end of the spectrum,
[01:06:55.980 --> 01:06:57.980]   people with ADD and other issues
[01:06:57.980 --> 01:07:00.740]   that they have problem to regulate their attention.
[01:07:00.740 --> 01:07:03.540]   Imagine to yourself that you have like a cognitive aid
[01:07:03.540 --> 01:07:06.260]   that just alerts you based on your gaze,
[01:07:06.260 --> 01:07:09.300]   that your attention is now not on what you are doing,
[01:07:09.300 --> 01:07:10.580]   and instead of writing a paper,
[01:07:10.580 --> 01:07:12.780]   you're now dreaming of what you're gonna do in the evening.
[01:07:12.780 --> 01:07:16.420]   So even this kind of simple measurement things,
[01:07:16.420 --> 01:07:18.060]   how they can change us,
[01:07:18.060 --> 01:07:22.460]   and I see it even in simple ways with myself.
[01:07:22.460 --> 01:07:26.540]   I have my zone up that I got in MIT gym,
[01:07:26.540 --> 01:07:28.820]   it kind of records how much did you run,
[01:07:28.820 --> 01:07:29.860]   and you have some points,
[01:07:29.860 --> 01:07:32.560]   and you can get some status, whatever.
[01:07:32.560 --> 01:07:35.860]   I said, "What is this ridiculous thing?
[01:07:35.860 --> 01:07:38.880]   "Who would ever care about some status in some app?"
[01:07:38.880 --> 01:07:41.620]   Guess what, so to maintain the status,
[01:07:41.620 --> 01:07:44.700]   you have to do set a number of points every month.
[01:07:44.700 --> 01:07:48.100]   And not only is it I do it every single month
[01:07:48.100 --> 01:07:50.620]   for the last 18 months,
[01:07:50.620 --> 01:07:54.220]   it went to the point that I was injured.
[01:07:54.220 --> 01:07:56.220]   And when I could run again,
[01:07:56.220 --> 01:08:02.500]   in two days, I did like some humongous amount of running
[01:08:02.500 --> 01:08:04.180]   just to complete the points.
[01:08:04.180 --> 01:08:05.980]   It was like really not safe.
[01:08:05.980 --> 01:08:08.500]   It's like, I'm not gonna lose my status
[01:08:08.500 --> 01:08:10.240]   because I want to get there.
[01:08:10.240 --> 01:08:13.320]   So you can already see that this direct measurement
[01:08:13.320 --> 01:08:14.880]   and the feedback is,
[01:08:14.880 --> 01:08:16.320]   you know, we're looking at video games
[01:08:16.320 --> 01:08:18.680]   and see why the addiction aspect of it,
[01:08:18.680 --> 01:08:20.460]   but you can imagine that the same idea
[01:08:20.460 --> 01:08:23.640]   can be expanded to many other areas of our life
[01:08:23.640 --> 01:08:25.960]   when we really can get feedback.
[01:08:25.960 --> 01:08:28.480]   And imagine in your case in relations,
[01:08:28.480 --> 01:08:31.200]   when we are doing keyword matching,
[01:08:31.200 --> 01:08:36.120]   imagine that the person who is generating the keywords,
[01:08:36.120 --> 01:08:37.740]   that person gets direct feedback
[01:08:37.740 --> 01:08:39.580]   before the whole thing explodes.
[01:08:39.580 --> 01:08:41.980]   Is it maybe at this happy point,
[01:08:41.980 --> 01:08:44.020]   we are going in the wrong direction.
[01:08:44.020 --> 01:08:48.020]   Maybe it will be really behavior-modifying moment.
[01:08:48.020 --> 01:08:51.340]   - So yeah, it's relationship management too.
[01:08:51.340 --> 01:08:54.220]   So yeah, that's a fascinating whole area
[01:08:54.220 --> 01:08:56.140]   of psychology actually as well,
[01:08:56.140 --> 01:08:58.260]   of seeing how our behavior has changed
[01:08:58.260 --> 01:09:00.860]   with basically all human relations
[01:09:00.860 --> 01:09:05.860]   now have other non-human entities helping us out.
[01:09:06.240 --> 01:09:09.480]   - So you teach a large,
[01:09:09.480 --> 01:09:12.640]   a huge machine learning course here at MIT.
[01:09:12.640 --> 01:09:15.360]   I could ask you a million questions,
[01:09:15.360 --> 01:09:17.600]   but you've seen a lot of students.
[01:09:17.600 --> 01:09:20.940]   What ideas do students struggle with the most
[01:09:20.940 --> 01:09:23.940]   as they first enter this world of machine learning?
[01:09:23.940 --> 01:09:28.040]   - Actually this year was the first time
[01:09:28.040 --> 01:09:30.120]   I started teaching a small machine learning class
[01:09:30.120 --> 01:09:32.880]   and it came as a result of what I saw
[01:09:32.880 --> 01:09:35.720]   in my big machine learning class at Tomiakala
[01:09:35.720 --> 01:09:38.360]   and I built maybe six years ago.
[01:09:38.360 --> 01:09:41.500]   What we've seen that as this area
[01:09:41.500 --> 01:09:42.900]   become more and more popular,
[01:09:42.900 --> 01:09:47.000]   more and more people at MIT want to take this class.
[01:09:47.000 --> 01:09:50.000]   And while we designed it for computer science majors,
[01:09:50.000 --> 01:09:51.080]   there were a lot of people
[01:09:51.080 --> 01:09:52.920]   who really are interested to learn it,
[01:09:52.920 --> 01:09:55.720]   but unfortunately their background
[01:09:55.720 --> 01:09:58.820]   was not enabling them to do well in the class.
[01:09:58.820 --> 01:10:01.040]   And many of them associated machine learning
[01:10:01.040 --> 01:10:03.020]   with the word struggle and failure.
[01:10:04.420 --> 01:10:06.460]   Primarily for non-majors.
[01:10:06.460 --> 01:10:08.720]   And that's why we actually started a new class
[01:10:08.720 --> 01:10:12.700]   which we call machine learning from algorithms to modeling,
[01:10:12.700 --> 01:10:16.820]   which emphasizes more the modeling aspects of it
[01:10:16.820 --> 01:10:21.780]   and focuses on, it has majors and non-majors.
[01:10:21.780 --> 01:10:25.380]   So we kind of try to extract the relevant parts
[01:10:25.380 --> 01:10:27.460]   and make it more accessible
[01:10:27.460 --> 01:10:29.740]   because the fact that we're teaching 20 classifiers
[01:10:29.740 --> 01:10:31.100]   in standard machine learning class
[01:10:31.100 --> 01:10:34.180]   is really a big question we really needed.
[01:10:34.180 --> 01:10:36.380]   - But it was interesting to see this
[01:10:36.380 --> 01:10:38.300]   from first generation of students,
[01:10:38.300 --> 01:10:40.900]   when they came back from their internships
[01:10:40.900 --> 01:10:43.940]   and from their jobs,
[01:10:43.940 --> 01:10:47.460]   what different and exciting things they can do
[01:10:47.460 --> 01:10:48.300]   that I would never think
[01:10:48.300 --> 01:10:51.100]   that you can even apply machine learning to.
[01:10:51.100 --> 01:10:53.740]   Some of them are like matching their relations
[01:10:53.740 --> 01:10:55.780]   and other things like variety of different applications.
[01:10:55.780 --> 01:10:58.020]   - Everything is amenable to machine learning.
[01:10:58.020 --> 01:11:00.260]   That actually brings up an interesting point
[01:11:00.260 --> 01:11:02.580]   of computer science in general.
[01:11:02.580 --> 01:11:05.420]   It almost seems, maybe I'm crazy,
[01:11:05.420 --> 01:11:08.420]   but it almost seems like everybody needs to learn
[01:11:08.420 --> 01:11:10.060]   how to program these days.
[01:11:10.060 --> 01:11:13.340]   If you're 20 years old or if you're starting school,
[01:11:13.340 --> 01:11:15.900]   even if you're an English major,
[01:11:15.900 --> 01:11:19.500]   it seems like programming unlocks
[01:11:19.500 --> 01:11:21.860]   so much possibility in this world.
[01:11:21.860 --> 01:11:24.980]   So when you interacted with those non-majors,
[01:11:24.980 --> 01:11:29.980]   is there skills that they were simply lacking at the time
[01:11:30.260 --> 01:11:32.020]   that you wish they had
[01:11:32.020 --> 01:11:34.660]   and that they learned in high school and so on?
[01:11:34.660 --> 01:11:37.460]   Like how should education change
[01:11:37.460 --> 01:11:41.260]   in this computerized world that we live in?
[01:11:41.260 --> 01:11:42.100]   - Same because I knew
[01:11:42.100 --> 01:11:44.780]   that there is a Python component in the class.
[01:11:44.780 --> 01:11:47.020]   Their Python skills were okay
[01:11:47.020 --> 01:11:49.140]   and the class is not really heavy on programming.
[01:11:49.140 --> 01:11:52.420]   They primarily kind of add parts to the programs.
[01:11:52.420 --> 01:11:55.420]   I think it was more of the mathematical barriers
[01:11:55.420 --> 01:11:58.220]   and the class, again, with a design on the majors
[01:11:58.220 --> 01:12:01.180]   was using the notation like big O for complexity
[01:12:01.180 --> 01:12:04.540]   and others, people who come from different backgrounds
[01:12:04.540 --> 01:12:05.780]   just don't have it in the lexical.
[01:12:05.780 --> 01:12:09.100]   It's not necessarily very challenging notion,
[01:12:09.100 --> 01:12:11.460]   but they were just not aware.
[01:12:11.460 --> 01:12:16.220]   So I think that kind of linear algebra and probability,
[01:12:16.220 --> 01:12:17.620]   the basics, the calculus,
[01:12:17.620 --> 01:12:20.820]   multivariate calculus are things that can help.
[01:12:20.820 --> 01:12:23.540]   - What advice would you give to students
[01:12:23.540 --> 01:12:25.260]   interested in machine learning,
[01:12:25.260 --> 01:12:28.460]   interested, you've talked about
[01:12:28.460 --> 01:12:31.380]   detecting, curing cancer, drug design.
[01:12:31.380 --> 01:12:34.520]   If they want to get into that field, what should they do?
[01:12:34.520 --> 01:12:40.620]   Get into it and succeed as researchers and entrepreneurs.
[01:12:40.620 --> 01:12:45.220]   - The first good piece of news is right now
[01:12:45.220 --> 01:12:47.380]   there are lots of resources
[01:12:47.380 --> 01:12:50.140]   that are created at different levels
[01:12:50.140 --> 01:12:54.780]   and you can find online or in your school classes
[01:12:54.780 --> 01:12:57.540]   which are more mathematical, more applied and so on.
[01:12:57.540 --> 01:13:01.300]   So you can find a kind of a preacher
[01:13:01.300 --> 01:13:02.740]   which preaches your own language
[01:13:02.740 --> 01:13:04.500]   where you can enter the field
[01:13:04.500 --> 01:13:06.700]   and you can make many different types of contribution
[01:13:06.700 --> 01:13:09.580]   depending of what is your strengths.
[01:13:09.580 --> 01:13:13.660]   And the second point, I think it's really important
[01:13:13.660 --> 01:13:18.100]   to find some area which you really care about
[01:13:18.100 --> 01:13:20.180]   and it can motivate your learning.
[01:13:20.180 --> 01:13:22.540]   And it can be for somebody curing cancer
[01:13:22.540 --> 01:13:25.340]   or doing self-driving cars or whatever
[01:13:25.340 --> 01:13:29.620]   but to find an area where there is data,
[01:13:29.620 --> 01:13:31.300]   where you believe there are strong patterns
[01:13:31.300 --> 01:13:33.540]   and we should be doing it and we're still not doing it
[01:13:33.540 --> 01:13:37.820]   or you can do it better and just start there
[01:13:37.820 --> 01:13:39.660]   and see where it can bring you.
[01:13:39.660 --> 01:13:45.460]   - So you've been very successful in many directions in life
[01:13:45.460 --> 01:13:48.580]   but you also mentioned "Flowers of Argonaut"
[01:13:51.140 --> 01:13:53.100]   and I think I've read or listened to you
[01:13:53.100 --> 01:13:55.300]   mention somewhere that researchers often get lost
[01:13:55.300 --> 01:13:56.680]   in the details of their work.
[01:13:56.680 --> 01:14:00.180]   This is per our original discussion with cancer and so on
[01:14:00.180 --> 01:14:02.140]   and don't look at the bigger picture,
[01:14:02.140 --> 01:14:04.280]   bigger questions of meaning and so on.
[01:14:04.280 --> 01:14:07.380]   So let me ask you the impossible question
[01:14:07.380 --> 01:14:11.540]   of what's the meaning of this thing,
[01:14:11.540 --> 01:14:16.540]   of life, of your life, of research.
[01:14:16.660 --> 01:14:21.420]   Why do you think we descendant of great apes
[01:14:21.420 --> 01:14:24.460]   are here on this spinning ball?
[01:14:24.460 --> 01:14:29.100]   - You know, I don't think that I have really
[01:14:29.100 --> 01:14:31.140]   a global answer, you know, maybe that's why
[01:14:31.140 --> 01:14:32.780]   I didn't go to humanities
[01:14:32.780 --> 01:14:36.440]   and I didn't take humanities classes in my undergrad.
[01:14:36.440 --> 01:14:43.540]   But the way I'm thinking about it,
[01:14:43.540 --> 01:14:48.220]   each one of us inside of them have their own set of,
[01:14:48.220 --> 01:14:51.140]   you know, things that we believe are important
[01:14:51.140 --> 01:14:53.380]   and it just happens that we are busy
[01:14:53.380 --> 01:14:56.260]   with achieving various goal, busy listening to others
[01:14:56.260 --> 01:14:59.500]   and to kind of try to conform and to be part of the crowd
[01:14:59.500 --> 01:15:03.740]   that we don't listen to that part.
[01:15:03.740 --> 01:15:09.620]   And, you know, we all should find some time to understand
[01:15:09.620 --> 01:15:11.860]   what is our own individual missions
[01:15:11.860 --> 01:15:14.100]   and we may have very different missions
[01:15:14.100 --> 01:15:18.220]   and to make sure that while we are running 10,000 things,
[01:15:18.220 --> 01:15:21.940]   we are not, you know, missing out
[01:15:21.940 --> 01:15:24.420]   and we're putting all the resources
[01:15:24.420 --> 01:15:28.500]   to satisfy our own mission.
[01:15:28.500 --> 01:15:32.460]   And if I look over my time, when I was younger,
[01:15:32.460 --> 01:15:35.060]   most of these missions, you know,
[01:15:35.060 --> 01:15:38.620]   I was primarily driven by the external stimulus,
[01:15:38.620 --> 01:15:41.540]   you know, to achieve this or to be that.
[01:15:41.540 --> 01:15:46.540]   And now a lot of what I do is driven by really thinking
[01:15:46.540 --> 01:15:51.340]   what is important for me to achieve independently
[01:15:51.340 --> 01:15:55.140]   of the external recognition.
[01:15:55.140 --> 01:16:00.100]   And, you know, I don't mind to be viewed in certain ways.
[01:16:00.100 --> 01:16:05.740]   The most important thing for me is to be true to myself,
[01:16:05.740 --> 01:16:07.500]   to what I think is right.
[01:16:07.500 --> 01:16:09.900]   - How long did it take, how hard was it
[01:16:09.900 --> 01:16:13.220]   to find the you that you have to be true to?
[01:16:13.220 --> 01:16:16.780]   - So it takes time and even now,
[01:16:16.780 --> 01:16:19.420]   sometimes, you know, the vanity and the triviality
[01:16:19.420 --> 01:16:21.660]   can take, you know. - At MIT.
[01:16:21.660 --> 01:16:25.100]   - Yeah, it can everywhere, you know,
[01:16:25.100 --> 01:16:26.940]   it's just the vanity at MIT is different,
[01:16:26.940 --> 01:16:28.140]   the vanity in different places,
[01:16:28.140 --> 01:16:30.940]   but we all have our piece of vanity.
[01:16:30.940 --> 01:16:35.940]   But I think actually, for me, many times,
[01:16:37.820 --> 01:16:41.700]   the place to get back to it is, you know,
[01:16:41.700 --> 01:16:45.820]   when I'm alone and also when I read.
[01:16:45.820 --> 01:16:47.740]   And I think by selecting the right books,
[01:16:47.740 --> 01:16:49.940]   you can get the right questions
[01:16:49.940 --> 01:16:53.700]   and learn from what you read.
[01:16:53.700 --> 01:16:58.460]   So, but again, it's not perfect, like,
[01:16:58.460 --> 01:17:02.020]   vanity sometimes dominates. - Nothing is.
[01:17:02.020 --> 01:17:04.780]   Well, that's a beautiful way to end.
[01:17:04.780 --> 01:17:06.340]   Thank you so much for talking today.
[01:17:06.340 --> 01:17:07.860]   - Thank you. - That was fun.
[01:17:07.860 --> 01:17:08.700]   - Oh, it was fun.
[01:17:08.700 --> 01:17:11.300]   (upbeat music)
[01:17:11.300 --> 01:17:13.900]   (upbeat music)
[01:17:13.900 --> 01:17:16.500]   (upbeat music)
[01:17:16.500 --> 01:17:19.100]   (upbeat music)
[01:17:19.100 --> 01:17:21.700]   (upbeat music)
[01:17:21.700 --> 01:17:24.300]   (upbeat music)
[01:17:24.300 --> 01:17:34.300]   [BLANK_AUDIO]

