
[00:00:00.000 --> 00:00:03.440]   All right, welcome back to 6S094,
[00:00:03.440 --> 00:00:05.440]   Deep Learning for Self-Driving Cars.
[00:00:05.440 --> 00:00:07.860]   Today we have Carl and Yema,
[00:00:07.860 --> 00:00:12.160]   and Oscar Bayboom from Aptiv.
[00:00:12.160 --> 00:00:15.240]   Carl is the president of Aptiv Autonomous Mobility,
[00:00:15.240 --> 00:00:18.080]   where Oscar is the machine learning lead.
[00:00:18.080 --> 00:00:22.280]   Carl founded Neutonomy, as many of you know, in 2013.
[00:00:22.280 --> 00:00:25.960]   It's a Boston-based autonomous vehicle company,
[00:00:25.960 --> 00:00:29.160]   and Neutonomy was acquired by Aptiv in 2017,
[00:00:29.160 --> 00:00:31.040]   and now is part of Aptiv.
[00:00:31.040 --> 00:00:33.000]   Carl and team are one of the leaders
[00:00:33.000 --> 00:00:36.080]   in autonomous vehicle development and deployment,
[00:00:36.080 --> 00:00:38.120]   with cars on roads all over the United States,
[00:00:38.120 --> 00:00:39.440]   several sites.
[00:00:39.440 --> 00:00:43.180]   But most importantly, Carl is MIT through and through,
[00:00:43.180 --> 00:00:46.480]   as also some of you may know, getting his PhD here.
[00:00:46.480 --> 00:00:48.640]   He led a robotics group here
[00:00:48.640 --> 00:00:50.720]   as a research scientist for many years.
[00:00:50.720 --> 00:00:54.280]   So it's really a pleasure to have both Carl
[00:00:54.280 --> 00:00:55.400]   and Oscar with us today.
[00:00:55.400 --> 00:00:57.200]   Please give them a warm welcome.
[00:00:57.200 --> 00:01:00.360]   (audience applauding)
[00:01:00.360 --> 00:01:02.760]   - All right, thanks, Lex.
[00:01:02.760 --> 00:01:05.160]   Yeah, very glad to be back at MIT.
[00:01:05.160 --> 00:01:08.040]   Very impressed that you guys are here during IAP.
[00:01:08.040 --> 00:01:13.080]   My course load during IAP was usually ice skating,
[00:01:13.080 --> 00:01:15.880]   and sometimes there was a wine tasting course.
[00:01:15.880 --> 00:01:17.520]   This was now almost 20 years ago,
[00:01:17.520 --> 00:01:18.800]   and that was pretty much it.
[00:01:18.800 --> 00:01:20.720]   That's where the academic work stopped.
[00:01:20.720 --> 00:01:22.660]   So you guys are here to learn something,
[00:01:22.660 --> 00:01:25.520]   so I'm gonna do my best and try something radical, actually.
[00:01:25.520 --> 00:01:28.160]   Since I'm president now of Aptiv Autonomous Driving,
[00:01:28.160 --> 00:01:29.040]   I'm not allowed to talk about
[00:01:29.040 --> 00:01:30.840]   anything technical or interesting.
[00:01:30.840 --> 00:01:32.240]   I'm gonna flout that a little bit
[00:01:32.240 --> 00:01:35.440]   and raise some topics that we think about
[00:01:35.440 --> 00:01:38.640]   that I think are interesting questions
[00:01:38.640 --> 00:01:40.920]   to keep in the back of your mind
[00:01:40.920 --> 00:01:42.080]   as you're thinking about deep learning
[00:01:42.080 --> 00:01:42.940]   and autonomous driving.
[00:01:42.940 --> 00:01:44.960]   So I'll raise some of those questions.
[00:01:44.960 --> 00:01:46.960]   And then Oscar will actually present
[00:01:46.960 --> 00:01:49.340]   some real-life technology
[00:01:49.340 --> 00:01:51.580]   and some of the work that he has been doing.
[00:01:51.580 --> 00:01:53.200]   Oscar's our machine learning lead.
[00:01:53.200 --> 00:01:55.180]   Some of the work that he and his outstanding team
[00:01:55.180 --> 00:02:00.180]   have been doing around machine learning-based detectors
[00:02:00.180 --> 00:02:03.560]   for the perception problem.
[00:02:03.560 --> 00:02:05.360]   So let me first introduce Aptiv a little bit,
[00:02:05.360 --> 00:02:08.240]   'cause people usually ask me, like,
[00:02:08.240 --> 00:02:11.000]   what's an Aptiv when I say I work for Aptiv?
[00:02:11.000 --> 00:02:13.200]   Aptiv's actually been around for a long time,
[00:02:13.200 --> 00:02:14.680]   but in a different form.
[00:02:14.680 --> 00:02:17.200]   Aptiv was previously Delphi Technologies,
[00:02:17.200 --> 00:02:19.720]   which was previously part of General Motors.
[00:02:19.720 --> 00:02:21.120]   So everybody's heard of General Motors.
[00:02:21.120 --> 00:02:23.480]   Some of you may have heard of Delphi,
[00:02:23.480 --> 00:02:28.480]   Aptiv spun from Delphi about 14 months ago.
[00:02:28.480 --> 00:02:30.800]   And so Aptiv's a tier one supplier.
[00:02:30.800 --> 00:02:32.160]   They're an automotive company
[00:02:32.160 --> 00:02:34.160]   that industrializes technology.
[00:02:34.160 --> 00:02:37.960]   Essentially, they take software and hardware,
[00:02:37.960 --> 00:02:39.880]   they industrialize it and put it on cars
[00:02:39.880 --> 00:02:42.200]   so it can run for many, many hundreds of thousands of miles
[00:02:42.200 --> 00:02:44.960]   without failing, which is a useful thing
[00:02:44.960 --> 00:02:46.220]   when we think about autonomous driving.
[00:02:46.220 --> 00:02:48.080]   So the themes for Aptiv,
[00:02:48.080 --> 00:02:50.720]   they develop what they say is safer, greener,
[00:02:50.720 --> 00:02:52.960]   and more connected solutions.
[00:02:52.960 --> 00:02:55.520]   Safer means safety systems, active safety,
[00:02:55.520 --> 00:02:58.320]   autonomous driving systems of the type that we're building.
[00:02:58.320 --> 00:03:02.440]   Greener, systems to enable electrification
[00:03:02.440 --> 00:03:03.720]   and kind of green vehicles.
[00:03:03.720 --> 00:03:06.160]   And then more connected connectivity solutions,
[00:03:06.160 --> 00:03:07.420]   both within the vehicle,
[00:03:07.420 --> 00:03:08.800]   transmitting data around the vehicle,
[00:03:08.800 --> 00:03:11.680]   and then externally, wireless communication.
[00:03:11.680 --> 00:03:14.200]   All of these things, as you can imagine,
[00:03:14.200 --> 00:03:17.720]   feed very, very nicely into the future
[00:03:17.720 --> 00:03:19.960]   transportation systems that the software
[00:03:19.960 --> 00:03:21.600]   will actually only be a part of.
[00:03:21.600 --> 00:03:23.580]   So Aptiv is in a really interesting spot
[00:03:23.580 --> 00:03:27.020]   when you think about the future of autonomous driving.
[00:03:27.020 --> 00:03:29.600]   And to give you a sense of scale,
[00:03:29.600 --> 00:03:32.780]   still kind of amazes me.
[00:03:32.780 --> 00:03:34.520]   The biggest my research group ever was at MIT
[00:03:34.520 --> 00:03:36.620]   was like 18, 18 people.
[00:03:36.620 --> 00:03:40.580]   Aptiv is 156,000 employees,
[00:03:40.580 --> 00:03:42.960]   so significant sized organization,
[00:03:42.960 --> 00:03:45.160]   about a $13 billion company by revenue
[00:03:45.160 --> 00:03:47.740]   in about 50 countries around the world.
[00:03:47.740 --> 00:03:51.140]   My group's about 700 people,
[00:03:51.140 --> 00:03:53.640]   so of which Oscar is one very important person.
[00:03:53.640 --> 00:03:55.840]   We're about 700 working on autonomous driving.
[00:03:55.840 --> 00:03:58.280]   We've got about 120 cars on the road
[00:03:58.280 --> 00:03:59.680]   in different countries,
[00:03:59.680 --> 00:04:01.480]   and I'll show you some examples of that.
[00:04:01.480 --> 00:04:04.880]   But first, let me take a trip down memory lane
[00:04:04.880 --> 00:04:07.060]   and show you a couple of snapshots
[00:04:07.060 --> 00:04:10.400]   about where we were not too long ago
[00:04:10.400 --> 00:04:13.960]   kind of as a community, but also me personally.
[00:04:13.960 --> 00:04:16.120]   And this will either inspire or horrify you,
[00:04:16.120 --> 00:04:17.880]   I'm not sure which.
[00:04:17.880 --> 00:04:22.140]   The fact is 2007, there were groups driving around
[00:04:22.140 --> 00:04:24.940]   with cars like running blade servers in the trunk
[00:04:24.940 --> 00:04:26.580]   that were generating so much heat,
[00:04:26.580 --> 00:04:28.920]   you had to install another air conditioner,
[00:04:28.920 --> 00:04:30.220]   which then was drawing so much power,
[00:04:30.220 --> 00:04:32.060]   you had to add another alternator,
[00:04:32.060 --> 00:04:33.700]   and then kind of rinse and repeat.
[00:04:33.700 --> 00:04:35.740]   So it wasn't a great situation.
[00:04:35.740 --> 00:04:40.740]   But people did enough algorithmically, computationally,
[00:04:40.740 --> 00:04:43.560]   to enable these cars,
[00:04:43.560 --> 00:04:45.220]   and this is the DARPA Urban Challenge
[00:04:45.220 --> 00:04:46.740]   for those of you that may be familiar,
[00:04:46.740 --> 00:04:48.680]   to enable these cars to do something useful
[00:04:48.680 --> 00:04:50.440]   and interesting on a closed course.
[00:04:50.440 --> 00:04:53.920]   And it kind of convinced enough people
[00:04:53.920 --> 00:04:57.960]   that given enough devotion of thought and resources
[00:04:57.960 --> 00:05:00.860]   that this might actually become a real thing someday.
[00:05:00.860 --> 00:05:03.940]   So I was one of those people that got convinced.
[00:05:03.940 --> 00:05:09.560]   2010, this is now, I'm gonna crib from my co-founder Emilio
[00:05:09.560 --> 00:05:12.320]   who was a former MIT faculty member in AeroAstro.
[00:05:12.320 --> 00:05:15.140]   Emilio started up an operation in Singapore through Smart,
[00:05:15.140 --> 00:05:16.640]   who some of you have probably worked with.
[00:05:16.640 --> 00:05:19.000]   So this is some folks from Smart.
[00:05:19.000 --> 00:05:21.440]   That's James, who looks really young in that picture.
[00:05:21.440 --> 00:05:23.200]   He was one of Emilio's students
[00:05:23.200 --> 00:05:26.200]   who was basically taking a golf cart
[00:05:26.200 --> 00:05:29.520]   and turning it into an autonomous shuttle.
[00:05:29.520 --> 00:05:31.000]   It turned out to work pretty well,
[00:05:31.000 --> 00:05:33.300]   and it got people in Singapore excited,
[00:05:33.300 --> 00:05:35.300]   which in turn got us further excited.
[00:05:35.300 --> 00:05:39.020]   2014, they did a demo where they let people of Singapore
[00:05:39.020 --> 00:05:42.240]   come and ride around in these carts in a garden,
[00:05:42.240 --> 00:05:45.800]   and that worked great over the course of a weekend.
[00:05:45.800 --> 00:05:48.080]   Around this time, we'd started Newtonomy.
[00:05:48.080 --> 00:05:49.800]   We'd actually started a commercial enterprise.
[00:05:49.800 --> 00:05:51.440]   It kind of stepped at least partly away
[00:05:51.440 --> 00:05:53.480]   from MIT at that point.
[00:05:53.480 --> 00:05:55.480]   2015, we had cars on the road.
[00:05:55.480 --> 00:05:58.300]   This is a Mitsubishi IMEV electric vehicle.
[00:05:58.300 --> 00:06:00.400]   When we had all of our equipment in it,
[00:06:00.400 --> 00:06:02.080]   the front seat was pushed forward so far
[00:06:02.080 --> 00:06:04.000]   that me, I'm about six foot three,
[00:06:04.000 --> 00:06:05.920]   actually couldn't sit in the front seat,
[00:06:05.920 --> 00:06:07.880]   so I couldn't actually accompany people on rides.
[00:06:07.880 --> 00:06:09.820]   It wasn't very practical.
[00:06:09.820 --> 00:06:13.760]   We ended up switching cars to a Renault Zoe platform,
[00:06:13.760 --> 00:06:14.780]   which is the one you see here,
[00:06:14.780 --> 00:06:16.440]   which had a little more leg room.
[00:06:16.440 --> 00:06:17.520]   We were giving, at that point,
[00:06:17.520 --> 00:06:21.040]   open to the public rides in our cars in Singapore
[00:06:21.040 --> 00:06:24.040]   in the part of the city that we were allowed to operate in.
[00:06:24.040 --> 00:06:26.260]   It was a quick transition.
[00:06:26.260 --> 00:06:28.840]   As you can see, just even visually,
[00:06:28.840 --> 00:06:31.480]   the evolution of these systems has come a long way
[00:06:31.480 --> 00:06:34.340]   in a short time, and we're just a point example
[00:06:34.340 --> 00:06:38.160]   of this phenomenon, which is kind of, broadly speaking,
[00:06:38.160 --> 00:06:39.720]   similar across the industry.
[00:06:39.720 --> 00:06:43.840]   But 2017, we joined Aptiv, and we were excited by that
[00:06:43.840 --> 00:06:47.520]   because we, as primarily scientists and technologists,
[00:06:47.520 --> 00:06:48.560]   didn't have a great idea
[00:06:48.560 --> 00:06:50.240]   how we were gonna industrialize this technology
[00:06:50.240 --> 00:06:51.760]   and actually bring it to market
[00:06:51.760 --> 00:06:55.120]   and make it reliable and robust and make it safe,
[00:06:55.120 --> 00:06:57.600]   which is what I'm gonna talk about a little bit here today.
[00:06:57.600 --> 00:07:00.040]   So we joined Aptiv with its global footprint.
[00:07:00.040 --> 00:07:02.200]   Today, we're primarily in Pittsburgh,
[00:07:02.200 --> 00:07:04.960]   Boston, Singapore, and Vegas,
[00:07:04.960 --> 00:07:08.160]   and we've got connectivity to Aptiv's other sites
[00:07:08.160 --> 00:07:10.160]   in Shanghai and Wolfsburg.
[00:07:10.160 --> 00:07:11.000]   Let me tell you a little bit
[00:07:11.000 --> 00:07:11.840]   about what's happening in Vegas.
[00:07:11.840 --> 00:07:13.080]   I think people were here,
[00:07:13.080 --> 00:07:14.920]   when was Luke talking?
[00:07:14.920 --> 00:07:16.100]   Couple days ago, yesterday.
[00:07:16.100 --> 00:07:17.760]   So Luke from Lyft, Luke Vincent,
[00:07:17.760 --> 00:07:20.360]   probably talked a little bit about Vegas.
[00:07:20.360 --> 00:07:22.960]   Vegas is really an interesting place for us.
[00:07:22.960 --> 00:07:24.080]   We've got a big operation there,
[00:07:24.080 --> 00:07:26.080]   130,000 square foot garage.
[00:07:26.080 --> 00:07:28.340]   We've got about 75 cars.
[00:07:28.340 --> 00:07:30.980]   We've got 30 of those cars on the Lyft network.
[00:07:30.980 --> 00:07:32.200]   So Aptiv technology,
[00:07:32.200 --> 00:07:33.720]   but connecting to the customer through Lyft.
[00:07:33.720 --> 00:07:36.320]   So if you go to Vegas and you open your Lyft app,
[00:07:36.320 --> 00:07:39.640]   it'll ask you, do you wanna take a ride in an autonomous car?
[00:07:39.640 --> 00:07:41.800]   You can opt in, you can opt out, it's up to you.
[00:07:41.800 --> 00:07:43.520]   If you opt in, there's a reasonable chance
[00:07:43.520 --> 00:07:46.520]   one of our cars will pick you up if you call for a ride.
[00:07:46.520 --> 00:07:48.120]   So anybody can do this,
[00:07:48.120 --> 00:07:50.600]   competitors, innocent bystanders,
[00:07:50.600 --> 00:07:52.360]   totally up to you, we have nothing to hide.
[00:07:52.360 --> 00:07:54.280]   Our cars are on the road 20 hours a day,
[00:07:54.280 --> 00:07:55.600]   seven days a week.
[00:07:55.600 --> 00:07:57.600]   If you take a ride, when you get out of the car,
[00:07:57.600 --> 00:07:58.800]   just like any Lyft ride,
[00:07:58.800 --> 00:08:00.920]   you gotta give us a star rating, one through five.
[00:08:00.920 --> 00:08:02.720]   And that, to us, is actually really interesting
[00:08:02.720 --> 00:08:06.560]   because it's a scaler, it's not too rich,
[00:08:06.560 --> 00:08:08.520]   but that star rating, to me,
[00:08:08.520 --> 00:08:11.000]   says something about the ride quality,
[00:08:11.000 --> 00:08:12.600]   meaning the comfort of the trip,
[00:08:12.600 --> 00:08:13.800]   the safety that you felt,
[00:08:13.800 --> 00:08:16.720]   and the efficiency of getting to where you wanted to go.
[00:08:16.720 --> 00:08:21.120]   Our star rating today is 4.95, which is pretty good.
[00:08:21.120 --> 00:08:24.280]   Key numbers, we've given, at this point,
[00:08:24.280 --> 00:08:27.680]   over 30,000 rides to more than 50,000 passengers.
[00:08:27.680 --> 00:08:30.160]   We've driven over a million miles in Vegas
[00:08:30.160 --> 00:08:34.320]   and a little bit additional, but primarily there.
[00:08:34.320 --> 00:08:37.000]   And as I mentioned, the 4.95.
[00:08:37.000 --> 00:08:38.280]   So what does it look like on the road?
[00:08:38.280 --> 00:08:40.120]   I'll show just one video today.
[00:08:40.120 --> 00:08:41.680]   I think Oscar has a few more.
[00:08:41.680 --> 00:08:43.920]   This one's actually in Singapore,
[00:08:43.920 --> 00:08:46.240]   but it's all kind of morally equivalent.
[00:08:46.240 --> 00:08:51.240]   You'll see a sped up, slightly sped up view of a run from,
[00:08:51.240 --> 00:08:54.200]   this is now probably six, seven months old,
[00:08:54.200 --> 00:08:55.040]   on the road in Singapore,
[00:08:55.040 --> 00:08:56.800]   but it's got some interesting stuff
[00:08:56.800 --> 00:08:59.400]   in a fairly typical run.
[00:08:59.400 --> 00:09:02.000]   Some of you may recognize these roads.
[00:09:02.000 --> 00:09:03.080]   We're on the wrong side of the road,
[00:09:03.080 --> 00:09:04.600]   remember, 'cause we're in Singapore.
[00:09:04.600 --> 00:09:07.460]   But to give you an example of some of the types of problems
[00:09:07.460 --> 00:09:09.380]   we have to solve on a daily basis.
[00:09:10.380 --> 00:09:11.660]   So let me run this thing.
[00:09:11.660 --> 00:09:16.660]   And you'll see as this car is cruising down the road,
[00:09:16.660 --> 00:09:20.460]   you have obstacles that we have to avoid,
[00:09:20.460 --> 00:09:23.340]   sometimes in the face of oncoming traffic.
[00:09:23.340 --> 00:09:26.340]   We've got to deal with sometimes situations
[00:09:26.340 --> 00:09:29.460]   where other road users are maybe not perfectly behaving
[00:09:29.460 --> 00:09:30.300]   by the rules.
[00:09:30.300 --> 00:09:33.100]   We've got to manage that in a natural way.
[00:09:33.100 --> 00:09:35.780]   Construction in Singapore, like everywhere else,
[00:09:35.780 --> 00:09:37.220]   is pretty ubiquitous.
[00:09:37.220 --> 00:09:38.500]   And so you have to navigate
[00:09:38.500 --> 00:09:40.680]   through these less structured environments.
[00:09:40.680 --> 00:09:44.660]   People who are sometimes doing things
[00:09:44.660 --> 00:09:46.660]   or indicating some future action,
[00:09:46.660 --> 00:09:49.220]   which you have to make inferences about,
[00:09:49.220 --> 00:09:51.140]   that can be tricky to navigate.
[00:09:51.140 --> 00:09:55.060]   So typical day, a route that any one of us as humans
[00:09:55.060 --> 00:09:58.340]   would drive through without batting an eye, no problem,
[00:09:58.340 --> 00:10:02.740]   is actually presents some really, really complex problems
[00:10:02.740 --> 00:10:04.260]   for autonomous vehicles.
[00:10:04.260 --> 00:10:05.780]   But it's the table stakes these days.
[00:10:05.780 --> 00:10:07.300]   These are the things you have to do if you want to be
[00:10:07.300 --> 00:10:09.740]   on the road, and certainly if you want to drive
[00:10:09.740 --> 00:10:12.340]   millions of miles with very few accidents,
[00:10:12.340 --> 00:10:13.660]   which is what we're doing.
[00:10:13.660 --> 00:10:15.220]   So that's an introduction to Aptiv
[00:10:15.220 --> 00:10:17.500]   and a little bit of background.
[00:10:17.500 --> 00:10:20.700]   So let me talk about, we're gonna talk about learning
[00:10:20.700 --> 00:10:22.460]   and how we think about learning
[00:10:22.460 --> 00:10:25.260]   in the context of autonomous driving.
[00:10:25.260 --> 00:10:27.060]   So there was a period a few years ago
[00:10:27.060 --> 00:10:29.100]   where I think as a community,
[00:10:29.100 --> 00:10:30.980]   people thought that we would be able to go
[00:10:30.980 --> 00:10:33.180]   from pixels to actuator commands
[00:10:33.180 --> 00:10:35.540]   with a single learned architecture,
[00:10:35.540 --> 00:10:36.980]   a single black box.
[00:10:36.980 --> 00:10:39.260]   I'll say, generally speaking,
[00:10:39.260 --> 00:10:40.660]   we no longer believe that's true.
[00:10:40.660 --> 00:10:43.060]   And I shouldn't include we in that.
[00:10:43.060 --> 00:10:44.460]   I didn't believe that was ever true.
[00:10:44.460 --> 00:10:46.340]   But some of us maybe thought that was true.
[00:10:46.340 --> 00:10:48.580]   And I'll tell you part of the reason why,
[00:10:48.580 --> 00:10:50.540]   in part of this talk,
[00:10:50.540 --> 00:10:53.060]   a big part of it comes down to safety.
[00:10:53.060 --> 00:10:54.700]   A big part of it comes down to safety.
[00:10:54.700 --> 00:10:57.940]   And the question of safety, convincing ourselves
[00:10:57.940 --> 00:10:59.660]   that that system, that black box,
[00:10:59.660 --> 00:11:03.340]   even if we could train it to accurately approximate
[00:11:03.340 --> 00:11:05.900]   this massively complex underlying function
[00:11:05.900 --> 00:11:07.900]   that we're trying to approximate,
[00:11:07.900 --> 00:11:10.340]   can we convince ourselves that it's safe?
[00:11:10.340 --> 00:11:11.940]   And it's very, very hard to answer
[00:11:11.940 --> 00:11:13.140]   that question affirmatively.
[00:11:13.140 --> 00:11:17.940]   And I'll raise some of the issues around why that is.
[00:11:17.940 --> 00:11:19.860]   This is not to say that learning methods
[00:11:19.860 --> 00:11:21.820]   are not incredibly useful for autonomous driving
[00:11:21.820 --> 00:11:23.580]   because they absolutely are.
[00:11:23.580 --> 00:11:25.900]   And Oscar will show you examples of why that is
[00:11:25.900 --> 00:11:28.900]   and how Aptiv is using some learning methods today.
[00:11:28.900 --> 00:11:30.700]   But this safety dimension is tricky
[00:11:30.700 --> 00:11:34.620]   because there's actually two axes here.
[00:11:34.620 --> 00:11:36.860]   One is the actual technical safety of the system,
[00:11:36.860 --> 00:11:39.660]   which is to say, can we build a system that's safe,
[00:11:39.660 --> 00:11:41.860]   that's provably in some sense safe,
[00:11:41.860 --> 00:11:45.300]   that we can validate, which we can convince ourselves,
[00:11:45.300 --> 00:11:47.500]   achieves the intended functionality
[00:11:47.500 --> 00:11:49.580]   in our operational design domain,
[00:11:49.580 --> 00:11:52.980]   that adheres to whatever regulatory requirements
[00:11:52.980 --> 00:11:56.220]   might be imposed on our jurisdictions that we're operating.
[00:11:56.220 --> 00:11:59.340]   And there's a whole longer list related to technical safety.
[00:11:59.340 --> 00:12:02.220]   But these are technical problems primarily.
[00:12:02.220 --> 00:12:03.300]   But there's another dimension,
[00:12:03.300 --> 00:12:05.980]   which up here is called perceived safety,
[00:12:05.980 --> 00:12:08.300]   which is to say, when you ride in a car,
[00:12:08.300 --> 00:12:11.900]   even if it's safe, do you believe that it's safe?
[00:12:11.900 --> 00:12:14.420]   And therefore, will you wanna take another trip?
[00:12:14.420 --> 00:12:16.300]   Which sounds kinda squishy,
[00:12:16.300 --> 00:12:18.980]   and as engineers, we're typically uncomfortable
[00:12:18.980 --> 00:12:19.820]   with that kind of stuff,
[00:12:19.820 --> 00:12:21.260]   but it turns out to be really important
[00:12:21.260 --> 00:12:22.900]   and probably harder to solve
[00:12:22.900 --> 00:12:24.860]   because it's a little bit squishy.
[00:12:24.860 --> 00:12:26.940]   And quite obviously, we gotta sit up here, right?
[00:12:26.940 --> 00:12:28.540]   We gotta be in this upper right-hand corner
[00:12:28.540 --> 00:12:30.980]   where we have not only a very safe car
[00:12:30.980 --> 00:12:32.060]   from a technical perspective,
[00:12:32.060 --> 00:12:34.940]   but one that feels safe, that inspires confidence
[00:12:34.940 --> 00:12:38.020]   in riders, in regulators, and in everybody else.
[00:12:38.020 --> 00:12:40.940]   So how do we get there in the context
[00:12:40.940 --> 00:12:44.340]   of elements of this system that may be black boxes,
[00:12:44.340 --> 00:12:46.340]   for lack of a better word?
[00:12:46.340 --> 00:12:48.180]   What's required is trust.
[00:12:48.180 --> 00:12:49.420]   You know, how do we get to this point
[00:12:49.420 --> 00:12:51.060]   where we can trust neural networks
[00:12:51.060 --> 00:12:53.180]   in the context of safety-critical systems,
[00:12:53.180 --> 00:12:55.180]   which is what an autonomous vehicle is?
[00:12:55.180 --> 00:12:58.300]   It really comes down to this question of,
[00:12:58.300 --> 00:12:59.740]   how do we convince ourselves
[00:12:59.740 --> 00:13:01.220]   that we can validate these systems?
[00:13:01.220 --> 00:13:03.100]   Again, validating the system,
[00:13:03.100 --> 00:13:07.140]   ensuring that it can meet the requirements,
[00:13:07.140 --> 00:13:09.700]   the operational requirements in the domain of interest
[00:13:09.700 --> 00:13:12.660]   that are imposed by the user, all right?
[00:13:12.660 --> 00:13:17.100]   There's three dimensions to this key question
[00:13:17.100 --> 00:13:18.220]   of understanding how to validate,
[00:13:18.220 --> 00:13:19.820]   and I'm gonna just briefly introduce
[00:13:19.820 --> 00:13:22.500]   some topics of interest around each of these.
[00:13:22.500 --> 00:13:25.900]   But the first one, trusting the data.
[00:13:25.900 --> 00:13:27.660]   Trusting the data.
[00:13:27.660 --> 00:13:29.700]   Do we actually have confidence
[00:13:29.700 --> 00:13:32.260]   about what goes into this algorithm?
[00:13:32.260 --> 00:13:35.020]   I mean, everybody knows garbage in, garbage out.
[00:13:35.020 --> 00:13:38.340]   There's various ways that we can make this garbage.
[00:13:38.340 --> 00:13:42.180]   We can have data which is insufficiently covering our domain,
[00:13:42.180 --> 00:13:43.540]   not representative of the domain.
[00:13:43.540 --> 00:13:45.620]   We can have data that's poorly annotated
[00:13:45.620 --> 00:13:47.300]   by our third-party trusted partners
[00:13:47.300 --> 00:13:50.420]   who we've trusted to label certain things of interest.
[00:13:50.420 --> 00:13:52.620]   So do we trust the data that's going in
[00:13:52.620 --> 00:13:53.900]   to the algorithm itself?
[00:13:53.900 --> 00:13:56.460]   Do we trust the implementation?
[00:13:56.460 --> 00:13:58.060]   We've got a beautiful algorithm,
[00:13:58.060 --> 00:14:00.100]   super descriptive, super robust,
[00:14:00.100 --> 00:14:02.300]   not brittle at all, well-trained,
[00:14:02.300 --> 00:14:03.940]   and we're running it on poor hardware.
[00:14:03.940 --> 00:14:05.500]   We've coded it poorly.
[00:14:05.500 --> 00:14:07.540]   We've got buffer overruns right and left.
[00:14:07.540 --> 00:14:09.260]   Do we trust the implementation
[00:14:09.260 --> 00:14:11.340]   to actually execute in a safe manner?
[00:14:11.340 --> 00:14:14.620]   And do we trust the algorithm?
[00:14:14.620 --> 00:14:15.740]   Again, generally speaking,
[00:14:15.740 --> 00:14:19.020]   we're trying to approximate really complicated functions.
[00:14:19.020 --> 00:14:21.300]   I don't think we typically use neural networks
[00:14:21.300 --> 00:14:23.340]   to approximate linear systems.
[00:14:23.340 --> 00:14:25.340]   So this is a gnarly, nasty function
[00:14:25.340 --> 00:14:30.340]   which has problems of critical interest
[00:14:30.340 --> 00:14:33.100]   which are really rare.
[00:14:33.100 --> 00:14:35.180]   In fact, they're the only ones of interest.
[00:14:35.180 --> 00:14:38.380]   So there's these events that happen very, very infrequently
[00:14:38.380 --> 00:14:40.300]   that we absolutely have to get right.
[00:14:40.300 --> 00:14:43.020]   It's a hard problem to convince ourselves
[00:14:43.020 --> 00:14:45.660]   that the algorithm is gonna perform properly
[00:14:45.660 --> 00:14:48.660]   in these unexpected and rare situations.
[00:14:48.660 --> 00:14:51.740]   So these are the sorts of things that we think about
[00:14:51.740 --> 00:14:55.220]   and that we have to answer in an intelligent way
[00:14:55.220 --> 00:14:56.780]   to convince ourselves that we have
[00:14:56.780 --> 00:15:00.060]   a validated neural network-based system.
[00:15:00.060 --> 00:15:05.020]   Okay, let me just step through each of these topics
[00:15:05.020 --> 00:15:06.100]   really quickly.
[00:15:06.100 --> 00:15:09.020]   So the topic of validation,
[00:15:09.020 --> 00:15:11.300]   what do we mean by that and why it is hard?
[00:15:11.300 --> 00:15:13.900]   There's a number of different dimensions here.
[00:15:13.900 --> 00:15:15.380]   The first is that we don't have insight
[00:15:15.380 --> 00:15:16.500]   into the nature of the function
[00:15:16.500 --> 00:15:18.860]   that we're trying to approximate.
[00:15:18.860 --> 00:15:21.580]   The underlying phenomena is really complicated.
[00:15:21.580 --> 00:15:24.940]   Again, if it weren't, we'd probably possibly be modeling it
[00:15:24.940 --> 00:15:25.940]   using different techniques.
[00:15:25.940 --> 00:15:28.400]   We'd write a closed-form equation to describe it.
[00:15:28.400 --> 00:15:30.180]   So that's a problem.
[00:15:30.180 --> 00:15:33.620]   Second, again, the accidents,
[00:15:33.620 --> 00:15:35.420]   the actual crashes on the road,
[00:15:35.420 --> 00:15:37.340]   what's going crashes and not accidents,
[00:15:37.340 --> 00:15:38.600]   these are rare.
[00:15:38.600 --> 00:15:40.380]   Luckily, they're very rare.
[00:15:40.380 --> 00:15:42.580]   But it makes the statistical argument
[00:15:42.580 --> 00:15:44.540]   around these accidents
[00:15:44.540 --> 00:15:45.820]   and being able to avoid these accidents
[00:15:45.820 --> 00:15:47.060]   really, really difficult.
[00:15:47.060 --> 00:15:51.460]   If you believe Rand, and they're pretty smart folks,
[00:15:51.460 --> 00:15:54.260]   they say you gotta drive 275 million miles
[00:15:54.260 --> 00:15:55.980]   without accident, without a crash,
[00:15:55.980 --> 00:15:57.660]   to claim a lower fatality rate
[00:15:57.660 --> 00:16:00.260]   than a human with 95% confidence.
[00:16:00.260 --> 00:16:01.820]   Well, how are we gonna do that?
[00:16:01.820 --> 00:16:06.080]   Can we think about using some correlated incident,
[00:16:06.080 --> 00:16:08.180]   maybe some kind of close call,
[00:16:08.180 --> 00:16:10.620]   as a proxy for accidents, which may be more frequent,
[00:16:10.620 --> 00:16:12.500]   and maybe back in that way?
[00:16:12.500 --> 00:16:14.020]   There's a lot of questions here,
[00:16:14.020 --> 00:16:16.180]   which I won't say we don't have any answers to,
[00:16:16.180 --> 00:16:17.180]   'cause I wouldn't go that far,
[00:16:17.180 --> 00:16:19.060]   but they're hard questions.
[00:16:19.060 --> 00:16:21.860]   They're not questions with obvious answers.
[00:16:21.860 --> 00:16:23.140]   So this is one of them,
[00:16:23.140 --> 00:16:24.580]   this issue of rare events.
[00:16:24.580 --> 00:16:29.940]   The regulatory dimension is one of these known unknowns.
[00:16:29.940 --> 00:16:31.700]   How do we validate a system
[00:16:31.700 --> 00:16:34.100]   if the requirements that may be imposed upon us
[00:16:34.100 --> 00:16:38.620]   from outside regulatory bodies are still to be written?
[00:16:38.620 --> 00:16:40.140]   That's difficult.
[00:16:40.140 --> 00:16:42.680]   So there's a lack of consensus
[00:16:42.680 --> 00:16:46.180]   on what the safety target should be for these systems.
[00:16:46.180 --> 00:16:47.540]   This is obviously evolving.
[00:16:47.540 --> 00:16:49.420]   Smart people are thinking about this.
[00:16:49.420 --> 00:16:51.860]   But today, it's not at all clear.
[00:16:51.860 --> 00:16:53.220]   If you're driving in Las Vegas,
[00:16:53.220 --> 00:16:54.620]   if you're driving in Singapore,
[00:16:54.620 --> 00:16:56.020]   if you're driving in San Francisco,
[00:16:56.020 --> 00:16:58.820]   or anywhere in between, what this target needs to be.
[00:16:58.820 --> 00:17:03.880]   And then lastly, and this is a really interesting one,
[00:17:03.880 --> 00:17:06.940]   we can get through a validation process for a bill to code.
[00:17:06.940 --> 00:17:08.360]   Let's assume we can do that.
[00:17:08.360 --> 00:17:10.480]   Well, what happens when we wanna update the code?
[00:17:10.480 --> 00:17:11.860]   'Cause obviously we will.
[00:17:11.860 --> 00:17:13.940]   Does that mean we have to start that validation process
[00:17:13.940 --> 00:17:14.820]   again from scratch,
[00:17:14.820 --> 00:17:18.520]   which will unavoidably be expensive and lengthy?
[00:17:18.520 --> 00:17:20.260]   Well, what if we only change a little bit of the code?
[00:17:20.260 --> 00:17:22.020]   What if we only change one line?
[00:17:22.020 --> 00:17:24.740]   But what if that one line is the most important line of code
[00:17:24.740 --> 00:17:25.940]   in the whole code base?
[00:17:25.940 --> 00:17:29.700]   This is one that I can tell you
[00:17:29.700 --> 00:17:30.980]   keeps a lot of people up at night,
[00:17:30.980 --> 00:17:33.260]   this question of revalidation.
[00:17:33.260 --> 00:17:35.940]   And then not even, again, keep that code base fixed.
[00:17:35.940 --> 00:17:38.200]   What if we move from one city to the next?
[00:17:38.200 --> 00:17:39.620]   And let's say that city is quite similar
[00:17:39.620 --> 00:17:42.380]   to your previous city, but not exactly the same.
[00:17:42.380 --> 00:17:44.020]   How do we think about validation
[00:17:44.020 --> 00:17:46.280]   in the context of new environments?
[00:17:46.280 --> 00:17:49.780]   So this continuous development issue is a challenge.
[00:17:50.780 --> 00:17:53.540]   All right, let me move on to talking about the data.
[00:17:53.540 --> 00:17:54.620]   There's probably people in this room
[00:17:54.620 --> 00:17:57.020]   who are doing active research in this area
[00:17:57.020 --> 00:17:59.360]   'cause it's a really interesting one.
[00:17:59.360 --> 00:18:02.780]   But there's a couple of obvious questions, I would say,
[00:18:02.780 --> 00:18:06.340]   that we think about when we think about data.
[00:18:06.340 --> 00:18:07.740]   We can have a great algorithm,
[00:18:07.740 --> 00:18:09.940]   and if we're training it on poor data
[00:18:09.940 --> 00:18:12.800]   for one reason or another, we won't have a great output.
[00:18:12.800 --> 00:18:16.660]   So one thing we think about is the sufficiency,
[00:18:16.660 --> 00:18:18.180]   the completeness of the data,
[00:18:19.100 --> 00:18:21.420]   and the bias that may be inherent in the data
[00:18:21.420 --> 00:18:23.820]   for our operational domain.
[00:18:23.820 --> 00:18:26.900]   If we wanna operate 24 hours a day,
[00:18:26.900 --> 00:18:29.840]   and we only train on data collected during daytime,
[00:18:29.840 --> 00:18:31.580]   we're probably gonna have an issue.
[00:18:31.580 --> 00:18:36.420]   Annotating the data is another dimension of the problem.
[00:18:36.420 --> 00:18:38.420]   We can collect raw data that's sufficient,
[00:18:38.420 --> 00:18:41.100]   that covers our space, but when we annotate it,
[00:18:41.100 --> 00:18:42.500]   when we hand it off to a third party,
[00:18:42.500 --> 00:18:44.540]   'cause it's typically a third party,
[00:18:44.540 --> 00:18:47.920]   to mark up the interesting aspects of it,
[00:18:47.920 --> 00:18:49.340]   we provide them some specifications,
[00:18:49.340 --> 00:18:51.780]   but we put a lot of trust in that third party,
[00:18:51.780 --> 00:18:55.900]   and trust that they're gonna do a good job
[00:18:55.900 --> 00:18:57.340]   annotating the interesting parts,
[00:18:57.340 --> 00:18:58.880]   and not the uninteresting parts,
[00:18:58.880 --> 00:19:00.540]   that they're gonna catch all the interesting parts
[00:19:00.540 --> 00:19:02.640]   that we've asked them to catch, et cetera.
[00:19:02.640 --> 00:19:06.500]   So this annotation part, which seems very mundane,
[00:19:06.500 --> 00:19:10.420]   very easy to manage, and kind of like low-hanging fruit,
[00:19:10.420 --> 00:19:13.120]   is in fact another key aspect
[00:19:13.120 --> 00:19:15.020]   of ensuring that we can trust the data.
[00:19:16.020 --> 00:19:19.020]   Okay, and this reference just kind of points to the fact
[00:19:19.020 --> 00:19:21.300]   that there are, again, smart people
[00:19:21.300 --> 00:19:22.540]   thinking about this problem,
[00:19:22.540 --> 00:19:24.620]   which rears its head in many domains
[00:19:24.620 --> 00:19:26.540]   beyond autonomous driving.
[00:19:26.540 --> 00:19:31.060]   Now what about the algorithms themselves?
[00:19:31.060 --> 00:19:34.800]   So moving on from the data to the actual algorithm,
[00:19:34.800 --> 00:19:38.420]   how do we convince ourselves that that algorithm,
[00:19:38.420 --> 00:19:41.980]   that like any kind of learning-based algorithm,
[00:19:41.980 --> 00:19:44.420]   we've trained on a training set,
[00:19:44.420 --> 00:19:48.120]   is gonna do well on some unknown test set?
[00:19:48.120 --> 00:19:52.220]   Well, there's a couple kind of properties
[00:19:52.220 --> 00:19:53.780]   of the algorithm that we can look at,
[00:19:53.780 --> 00:19:55.860]   that we can kind of interrogate,
[00:19:55.860 --> 00:19:58.740]   and kind of poke at to convince ourselves
[00:19:58.740 --> 00:20:00.880]   that that algorithm will perform well.
[00:20:00.880 --> 00:20:03.580]   You know, one is invariance,
[00:20:03.580 --> 00:20:06.260]   and the other one, we can say, is stability.
[00:20:06.260 --> 00:20:10.000]   If we make small perturbations to this function,
[00:20:10.000 --> 00:20:11.180]   does it behave well?
[00:20:11.180 --> 00:20:13.660]   Given kind of, let's say, a bounded input,
[00:20:13.660 --> 00:20:15.460]   do we see a bounded output?
[00:20:15.460 --> 00:20:17.640]   Or do we see some wild response?
[00:20:17.640 --> 00:20:22.060]   You know, I'm sure you've all heard of examples
[00:20:22.060 --> 00:20:26.700]   of adversarial images that can confuse
[00:20:26.700 --> 00:20:28.460]   learning-based classifiers.
[00:20:28.460 --> 00:20:31.060]   So it's a turtle.
[00:20:31.060 --> 00:20:33.460]   You show it a turtle, it says, "Well, that's a turtle."
[00:20:33.460 --> 00:20:35.180]   And then you show it a turtle that's maybe fuzzed
[00:20:35.180 --> 00:20:38.320]   with a little bit of noise that the human eye can't perceive.
[00:20:38.320 --> 00:20:39.940]   So it still looks like a turtle,
[00:20:39.940 --> 00:20:41.740]   and it tells you it's a machine gun.
[00:20:41.740 --> 00:20:44.860]   Obviously, for us in the driving domain,
[00:20:44.860 --> 00:20:46.940]   we want a stop sign to be correctly identified
[00:20:46.940 --> 00:20:49.380]   as a stop sign 100 times of 100.
[00:20:49.380 --> 00:20:51.380]   We don't want that stop sign,
[00:20:51.380 --> 00:20:53.180]   if somebody goes up and puts a piece of duct tape
[00:20:53.180 --> 00:20:54.220]   in the lower right-hand corner,
[00:20:54.220 --> 00:20:58.000]   to be interpreted as a yield sign, for example.
[00:20:58.000 --> 00:21:02.300]   So this question of the properties of the algorithm,
[00:21:02.300 --> 00:21:04.260]   its invariance, its stability,
[00:21:04.260 --> 00:21:06.280]   is something of high interest.
[00:21:08.460 --> 00:21:12.540]   And then lastly, to add one more point to this,
[00:21:12.540 --> 00:21:14.620]   this notion of interpretability.
[00:21:14.620 --> 00:21:17.880]   So interpretability, understanding why an algorithm
[00:21:17.880 --> 00:21:19.500]   made a decision that it made.
[00:21:19.500 --> 00:21:23.220]   This is the sort of thing that may not be a nice-to-have,
[00:21:23.220 --> 00:21:25.100]   may actually be a requirement,
[00:21:25.100 --> 00:21:26.700]   and would likely to be a requirement
[00:21:26.700 --> 00:21:27.900]   from the regulatory groups
[00:21:27.900 --> 00:21:29.740]   that I was referring to a minute ago.
[00:21:29.740 --> 00:21:32.540]   So let's say, imagine the case of a crash,
[00:21:32.540 --> 00:21:34.580]   where the system that was governing
[00:21:34.580 --> 00:21:38.620]   your trajectory generator was a data-driven system,
[00:21:38.620 --> 00:21:42.460]   was a deep-learning-based trajectory generator.
[00:21:42.460 --> 00:21:44.900]   Well, you may need to explain to someone
[00:21:44.900 --> 00:21:47.740]   exactly why that particular trajectory
[00:21:47.740 --> 00:21:50.020]   was generated at that particular moment.
[00:21:50.020 --> 00:21:52.340]   And this may be a hard thing to do,
[00:21:52.340 --> 00:21:55.180]   if the generator was a data-driven model.
[00:21:55.180 --> 00:21:56.580]   Now, obviously, there are people working
[00:21:56.580 --> 00:21:59.680]   and doing active research into this specific question
[00:21:59.680 --> 00:22:02.940]   of interpretable learning methods,
[00:22:02.940 --> 00:22:05.420]   but it's a thorny one.
[00:22:05.420 --> 00:22:07.420]   It's a very, very difficult topic,
[00:22:07.420 --> 00:22:11.180]   and it's not at all clear to me when and if
[00:22:11.180 --> 00:22:13.140]   we'll get to the stage where we can,
[00:22:13.140 --> 00:22:16.260]   to even a technical audience,
[00:22:16.260 --> 00:22:18.580]   but beyond that, to a lay jury,
[00:22:18.580 --> 00:22:22.340]   be able to explain why algorithm X made decision Y.
[00:22:22.340 --> 00:22:25.820]   Okay, so with all that in mind,
[00:22:25.820 --> 00:22:30.920]   let me talk a little bit about safety.
[00:22:32.580 --> 00:22:34.180]   That all maybe sounds pretty bleak.
[00:22:34.180 --> 00:22:35.820]   You think, well, man, why are we taking this course
[00:22:35.820 --> 00:22:37.700]   with Lex, 'cause we're never gonna really use this stuff.
[00:22:37.700 --> 00:22:39.980]   But in fact, we can.
[00:22:39.980 --> 00:22:42.780]   We can and will, as a community.
[00:22:42.780 --> 00:22:45.720]   There's a lot of tools we can bring to bear
[00:22:45.720 --> 00:22:48.780]   to think about neural networks,
[00:22:48.780 --> 00:22:49.820]   and they're, generally speaking,
[00:22:49.820 --> 00:22:52.860]   within the context of a broader safety argument.
[00:22:52.860 --> 00:22:53.940]   I think that's the key.
[00:22:53.940 --> 00:22:57.000]   We tend not to think about using a neural network
[00:22:57.000 --> 00:23:00.500]   as a holistic system to drive a car,
[00:23:00.500 --> 00:23:02.780]   but we'll think about it as a submodule
[00:23:02.780 --> 00:23:05.600]   that we can build other systems around,
[00:23:05.600 --> 00:23:07.980]   generally speaking, that which we can say,
[00:23:07.980 --> 00:23:10.700]   maybe make more rigorous claims about their performance,
[00:23:10.700 --> 00:23:13.000]   their underlying properties,
[00:23:13.000 --> 00:23:14.900]   and then therefore make a convincing,
[00:23:14.900 --> 00:23:19.180]   holistic safety argument that this end-to-end system is safe.
[00:23:19.180 --> 00:23:22.860]   We have tools, functional safety is,
[00:23:22.860 --> 00:23:24.140]   maybe familiar to some of you.
[00:23:24.140 --> 00:23:25.060]   It's something we think about a lot
[00:23:25.060 --> 00:23:26.360]   in the automotive domain.
[00:23:26.360 --> 00:23:29.740]   And SOTIF, which stands for
[00:23:29.740 --> 00:23:31.460]   Safety of the Intended Functionality,
[00:23:31.460 --> 00:23:34.100]   we're basically asking ourselves the question,
[00:23:34.100 --> 00:23:38.580]   is this overall function doing what it's intended to do?
[00:23:38.580 --> 00:23:39.740]   Is it operating safely?
[00:23:39.740 --> 00:23:41.520]   And is it meeting its specifications?
[00:23:41.520 --> 00:23:43.340]   There's kind of an analogy here
[00:23:43.340 --> 00:23:47.180]   to validation and verification, if you will.
[00:23:47.180 --> 00:23:48.860]   And we have to answer these questions
[00:23:48.860 --> 00:23:52.720]   around functional safety and SOTIF affirmatively,
[00:23:52.720 --> 00:23:57.020]   even when we have neural network-based elements
[00:23:57.020 --> 00:24:00.260]   in order to eventually put this car on the road.
[00:24:00.260 --> 00:24:02.940]   All right, so I mentioned that we need to do some embedding.
[00:24:02.940 --> 00:24:05.240]   This is an example of what it might look like.
[00:24:05.240 --> 00:24:07.620]   We refer to this as,
[00:24:07.620 --> 00:24:10.060]   sometimes we call this caging the learning.
[00:24:10.060 --> 00:24:11.580]   So we put the learning in a box.
[00:24:11.580 --> 00:24:14.360]   It's this powerful animal we wanna control.
[00:24:14.360 --> 00:24:17.340]   And in this case, it's up there at the top in red.
[00:24:17.340 --> 00:24:21.360]   That might be that trajectory proposer I was talking about.
[00:24:21.360 --> 00:24:23.780]   So let's say we've got a powerful trajectory proposer.
[00:24:23.780 --> 00:24:24.740]   We wanna use this thing.
[00:24:24.740 --> 00:24:26.980]   We've got it on what we call our performance compute,
[00:24:26.980 --> 00:24:28.380]   our high-powered compute.
[00:24:28.380 --> 00:24:29.820]   It's maybe not automotive grade.
[00:24:29.820 --> 00:24:31.460]   It's got some potential failure modes,
[00:24:31.460 --> 00:24:34.060]   but it's generally speaking, good performance.
[00:24:34.060 --> 00:24:35.300]   Let's go there.
[00:24:35.300 --> 00:24:38.060]   And we've got our neural network-based generator on it,
[00:24:38.060 --> 00:24:39.380]   which we can say some things about,
[00:24:39.380 --> 00:24:41.280]   but maybe not everything we'd like to.
[00:24:41.280 --> 00:24:44.780]   Well, we make the argument that if we can surround that,
[00:24:44.780 --> 00:24:47.860]   so if we can cage it, kind of underpin it
[00:24:47.860 --> 00:24:50.180]   with a safety system that we can say
[00:24:50.180 --> 00:24:54.320]   very rigorous things about its performance,
[00:24:54.320 --> 00:24:56.060]   then generally speaking, we may be okay.
[00:24:56.060 --> 00:24:58.460]   There may be a path to using neural networks
[00:24:58.460 --> 00:25:01.680]   on autonomous vehicles if we can wrap them
[00:25:01.680 --> 00:25:03.900]   in a safety architecture that we can say
[00:25:03.900 --> 00:25:05.700]   a lot of good things about.
[00:25:05.700 --> 00:25:08.140]   And this is exactly what this represents.
[00:25:08.140 --> 00:25:10.420]   So I'm gonna conclude my part of the talk here,
[00:25:10.420 --> 00:25:15.420]   hand it over to Oscar, with kind of a quote, an assertion.
[00:25:15.420 --> 00:25:18.480]   One of my engineers insisted I show today.
[00:25:18.480 --> 00:25:20.460]   The argument is the following.
[00:25:20.460 --> 00:25:22.980]   Engineering is inching closer to the natural sciences.
[00:25:22.980 --> 00:25:24.920]   I won't say how much closer, but closer.
[00:25:24.920 --> 00:25:27.460]   We're creating things that we don't fully understand,
[00:25:27.460 --> 00:25:30.320]   and then we're investigating the properties of our creation.
[00:25:30.320 --> 00:25:33.100]   We're not writing down closed-form functions.
[00:25:33.100 --> 00:25:35.440]   That would be too easy.
[00:25:35.440 --> 00:25:38.100]   We're generating these immensely complex
[00:25:38.100 --> 00:25:40.980]   functional approximators, and then we're just poking at 'em
[00:25:40.980 --> 00:25:42.020]   in different ways and saying, boy, well,
[00:25:42.020 --> 00:25:44.500]   what does this thing do under these situations?
[00:25:44.500 --> 00:25:46.580]   And I'll leave you with one image,
[00:25:46.580 --> 00:25:48.100]   which I'll present without comment,
[00:25:48.100 --> 00:25:50.000]   and then hand it over to Oscar.
[00:25:50.000 --> 00:25:51.060]   All right, thank you.
[00:25:51.980 --> 00:25:55.140]   (audience applauding)
[00:25:55.140 --> 00:25:58.700]   - So thanks a lot, Carl.
[00:25:58.700 --> 00:26:00.220]   Thanks, Lex, for the invite.
[00:26:00.220 --> 00:26:02.120]   Yes, my name is Oscar.
[00:26:02.120 --> 00:26:05.660]   I run the machine learning team at Aptiv Neutronomy.
[00:26:05.660 --> 00:26:08.920]   So let me begin with this slide.
[00:26:08.920 --> 00:26:13.140]   You know, not long ago, image classification was,
[00:26:13.140 --> 00:26:14.380]   you know, quite literally a joke.
[00:26:14.380 --> 00:26:17.420]   So this is an actual comic.
[00:26:17.420 --> 00:26:18.980]   How many have seen this before?
[00:26:20.060 --> 00:26:22.820]   Okay, well, I was doing my PhD in this era
[00:26:22.820 --> 00:26:26.820]   where, you know, building a bird classifier
[00:26:26.820 --> 00:26:28.660]   was like a PhD project, right?
[00:26:28.660 --> 00:26:32.500]   And it was, you know, it's funny 'cause it's true.
[00:26:32.500 --> 00:26:34.660]   And then, of course, as you well know,
[00:26:34.660 --> 00:26:36.340]   the deep learning revolution happened,
[00:26:36.340 --> 00:26:38.940]   and Lex, you know, previous introductory slides
[00:26:38.940 --> 00:26:40.780]   gives a great overview.
[00:26:40.780 --> 00:26:42.140]   I don't wanna redo that.
[00:26:42.140 --> 00:26:44.420]   I just wanna say sort of a straight line
[00:26:44.420 --> 00:26:46.740]   from what I consider the breakthrough paper
[00:26:46.740 --> 00:26:48.900]   by Krzyzewski et al.
[00:26:48.900 --> 00:26:51.020]   To the work I'll be talking about today,
[00:26:51.020 --> 00:26:51.900]   I'll start with these three.
[00:26:51.900 --> 00:26:54.580]   So you had the, you know, deep learning,
[00:26:54.580 --> 00:26:57.180]   end-to-end learning for image net classification
[00:26:57.180 --> 00:26:58.300]   by Krzyzewski et al.
[00:26:58.300 --> 00:27:00.940]   That paper's been cited 35,000 times.
[00:27:00.940 --> 00:27:01.980]   I checked yesterday.
[00:27:01.980 --> 00:27:06.180]   Then, 2014, Ross Gershick et al. at Berkeley
[00:27:06.180 --> 00:27:08.820]   basically showed how to, you know,
[00:27:08.820 --> 00:27:11.540]   repurpose the deep learning architecture
[00:27:11.540 --> 00:27:13.860]   to do detection in images.
[00:27:13.860 --> 00:27:14.700]   And that was the first time
[00:27:14.700 --> 00:27:16.780]   when the visual community really started seeing,
[00:27:16.780 --> 00:27:18.460]   okay, so classification is more general.
[00:27:18.460 --> 00:27:19.460]   You can classify anything,
[00:27:19.460 --> 00:27:21.780]   an image, an audio signal, whatever, right?
[00:27:21.780 --> 00:27:23.900]   But detection in images was very intimate
[00:27:23.900 --> 00:27:25.100]   to the computer vision community.
[00:27:25.100 --> 00:27:27.260]   We thought we were best in the world, right?
[00:27:27.260 --> 00:27:28.620]   So when this paper came out,
[00:27:28.620 --> 00:27:32.220]   that was sort of the final argument for like,
[00:27:32.220 --> 00:27:34.320]   okay, we all need to do deep learning now.
[00:27:34.320 --> 00:27:37.860]   Right, and then 2016, this paper came out,
[00:27:37.860 --> 00:27:40.020]   the single-shot multi-box detector,
[00:27:40.020 --> 00:27:43.460]   which I think is a great paper by Liu et al.
[00:27:43.460 --> 00:27:46.220]   So if you haven't looked at this paper,
[00:27:46.220 --> 00:27:48.020]   by all means, read them carefully.
[00:27:48.020 --> 00:27:51.180]   So as a result,
[00:27:51.180 --> 00:27:54.980]   you know, performance is no longer a joke, right?
[00:27:54.980 --> 00:27:59.460]   So this is a network that we developed in my group.
[00:27:59.460 --> 00:28:04.460]   So it's a joint image classification segmentation network.
[00:28:04.460 --> 00:28:07.780]   This thing, we can run this at 200 hertz on a single GPU.
[00:28:07.780 --> 00:28:11.840]   And in this video, in this rendering,
[00:28:11.840 --> 00:28:13.940]   there's no tracking applied.
[00:28:13.940 --> 00:28:15.260]   There's no temporal smoothing.
[00:28:15.260 --> 00:28:17.420]   Every single frame is analyzed
[00:28:17.420 --> 00:28:20.300]   independently from the other one.
[00:28:20.300 --> 00:28:23.300]   And you can see that we can model several different classes,
[00:28:23.300 --> 00:28:29.480]   you know, both boxes and the surfaces at the same time.
[00:28:29.480 --> 00:28:32.660]   Here's my cartoon drawing of a perception system
[00:28:32.660 --> 00:28:33.660]   on an autonomous vehicle.
[00:28:33.660 --> 00:28:38.000]   So you have the three different main sensibilities.
[00:28:38.000 --> 00:28:41.300]   Typically have some module that does detection and tracking.
[00:28:41.300 --> 00:28:44.200]   You know, there's tons of variations of this, of course,
[00:28:44.200 --> 00:28:46.600]   but you have some sort of sensor pipelines,
[00:28:46.600 --> 00:28:49.460]   and then in the end, you have a tracking and fusion step.
[00:28:49.460 --> 00:28:52.380]   So what I showed you in the previous video
[00:28:52.380 --> 00:28:53.260]   is basically this part.
[00:28:53.260 --> 00:28:55.140]   So like I said, there was no tracking,
[00:28:55.140 --> 00:28:58.600]   but it's like going from the camera to detections.
[00:28:58.600 --> 00:29:01.920]   And if you look, you know, when I started,
[00:29:01.920 --> 00:29:04.240]   so I come strict from the computer science
[00:29:04.240 --> 00:29:06.980]   learning community, so when I started looking
[00:29:06.980 --> 00:29:09.300]   at this pipeline, I'm like, why are there so many steps?
[00:29:09.300 --> 00:29:11.740]   Why aren't we optimizing things end to end?
[00:29:11.740 --> 00:29:14.160]   So obviously, there's a real temptation
[00:29:14.160 --> 00:29:15.540]   to just wrap everything in a kernel.
[00:29:15.540 --> 00:29:18.620]   It's a very well-defined input/output function.
[00:29:18.620 --> 00:29:22.460]   And like Carl alluded to, it's one that can be verified
[00:29:22.460 --> 00:29:25.520]   quite well, assuming you have the right data.
[00:29:25.520 --> 00:29:28.260]   I'm not gonna be talking about this.
[00:29:28.260 --> 00:29:30.540]   I am gonna talk about this,
[00:29:30.540 --> 00:29:33.860]   namely the building a deep learning kernel
[00:29:33.860 --> 00:29:35.120]   for the LiDAR pipeline.
[00:29:35.120 --> 00:29:37.980]   And LiDAR pipeline is arguably the backbone
[00:29:37.980 --> 00:29:39.620]   of the perception system
[00:29:39.620 --> 00:29:43.860]   for most autonomous driving systems.
[00:29:43.860 --> 00:29:44.980]   So what we're gonna do is,
[00:29:44.980 --> 00:29:47.580]   so this is basically gonna be the goal here.
[00:29:47.580 --> 00:29:49.380]   So we're gonna have a point cloud,
[00:29:49.380 --> 00:29:52.940]   it's input, and we're gonna have a neural network
[00:29:52.940 --> 00:29:54.980]   that takes that as input and then generates
[00:29:54.980 --> 00:29:57.360]   3D bounding boxes that are in a well-coordinated system.
[00:29:57.360 --> 00:29:59.720]   So it's like 20 meters that way,
[00:29:59.720 --> 00:30:01.780]   it's two meters wide, so long,
[00:30:01.780 --> 00:30:04.180]   this rotation and this orientation and so on.
[00:30:04.180 --> 00:30:09.420]   So yeah, so that's what this talk is about.
[00:30:09.420 --> 00:30:10.860]   So I'm gonna talk about point pillars,
[00:30:10.860 --> 00:30:13.100]   which is a new method we developed for this,
[00:30:13.100 --> 00:30:16.460]   and new scenes, which is a benchmark data that we released.
[00:30:16.460 --> 00:30:18.260]   Okay, so what is point pillars?
[00:30:18.260 --> 00:30:21.300]   Well, it's a novel point cloud encoder.
[00:30:21.300 --> 00:30:23.100]   So what we do is we learn a representation
[00:30:23.100 --> 00:30:25.180]   that is suitable for downstream detection.
[00:30:25.180 --> 00:30:27.100]   It's almost like a, the main innovation
[00:30:27.100 --> 00:30:29.260]   is the translation from the point cloud
[00:30:29.260 --> 00:30:31.620]   to a canvas that can then be processed
[00:30:31.620 --> 00:30:35.780]   by a similar architecture that you would use in an image.
[00:30:35.780 --> 00:30:37.620]   And I'll show you how it performs,
[00:30:37.620 --> 00:30:39.300]   you know, all published measurement on KITTI
[00:30:39.300 --> 00:30:42.980]   by a large margin, especially with respect
[00:30:42.980 --> 00:30:45.860]   to inference speed.
[00:30:45.860 --> 00:30:48.820]   And there's a pre-printout and some code available
[00:30:48.820 --> 00:30:50.660]   if you guys wanna play around with it.
[00:30:50.660 --> 00:30:54.260]   So the architecture that we're gonna use
[00:30:54.260 --> 00:30:56.300]   looks something like this.
[00:30:56.300 --> 00:31:00.900]   And I should say, most papers in this space
[00:31:00.900 --> 00:31:02.780]   use this architecture.
[00:31:02.780 --> 00:31:04.860]   So it's kind of a natural design, right?
[00:31:04.860 --> 00:31:06.880]   So you have the point cloud at the top,
[00:31:06.880 --> 00:31:09.380]   you have this encoder, and that's where we introduce
[00:31:09.380 --> 00:31:10.860]   the point pillars, but you can have,
[00:31:10.860 --> 00:31:14.540]   I'll show you guys, you can have various types of encoders.
[00:31:14.540 --> 00:31:16.600]   And then after that, that feeds into a backbone,
[00:31:16.600 --> 00:31:19.780]   which is now a standard convolutional 2D backbone.
[00:31:19.780 --> 00:31:22.220]   You have a detection head, and you might have,
[00:31:22.220 --> 00:31:25.100]   you may or may not have a segmentation head on that.
[00:31:25.100 --> 00:31:26.620]   The point is that after the encoder,
[00:31:26.620 --> 00:31:28.900]   everything looks just like, the architecture's
[00:31:28.900 --> 00:31:31.060]   very, very similar to the SSD architecture
[00:31:31.060 --> 00:31:32.540]   or the RCNN architecture.
[00:31:32.540 --> 00:31:38.020]   So let's go into a little bit more detail, right?
[00:31:38.020 --> 00:31:40.420]   So the range, so what you're given here
[00:31:40.420 --> 00:31:43.340]   is a range of D meters, so you wanna model,
[00:31:43.340 --> 00:31:45.740]   you know, 40 meters, a 40 meter circle
[00:31:45.740 --> 00:31:47.740]   around the vehicle, for example.
[00:31:47.740 --> 00:31:51.500]   You have certain resolution of your bins,
[00:31:51.500 --> 00:31:53.820]   and then a number of output channels, right?
[00:31:53.820 --> 00:31:55.900]   So your input is a set of pillars,
[00:31:55.900 --> 00:31:59.100]   or in the pillar here is a vertical column, right?
[00:31:59.100 --> 00:32:03.060]   So you have N, M of those that are non-empty in the space.
[00:32:03.060 --> 00:32:05.500]   And you say a pillar P contains all the points,
[00:32:05.500 --> 00:32:08.060]   which are a lot of point X, Y, C, and intensity.
[00:32:09.100 --> 00:32:13.700]   And there's N sub, M indexed by M points in each pillar,
[00:32:13.700 --> 00:32:16.940]   right, so just to say that it varies, right?
[00:32:16.940 --> 00:32:19.580]   So it could be one single point at a particular location,
[00:32:19.580 --> 00:32:20.980]   it could be 200 points.
[00:32:20.980 --> 00:32:23.220]   And then it's centered around this bin.
[00:32:23.220 --> 00:32:27.540]   And the goal here is to produce a tensor as a fixed size.
[00:32:27.540 --> 00:32:29.780]   So it's height, which is, you know,
[00:32:29.780 --> 00:32:33.100]   range of a resolution, width, range of a resolution,
[00:32:33.100 --> 00:32:35.340]   and then this parameter C.
[00:32:35.340 --> 00:32:38.420]   C is the number of channels, so in an image,
[00:32:38.420 --> 00:32:39.820]   C will be three.
[00:32:39.820 --> 00:32:41.400]   We don't necessarily care about that.
[00:32:41.400 --> 00:32:43.740]   We call it a pseudo-image, but it's the same thing.
[00:32:43.740 --> 00:32:45.220]   It's a fixed number of channels
[00:32:45.220 --> 00:32:47.140]   that the backbone can then operate on.
[00:32:47.140 --> 00:32:52.860]   Yeah, so here's the same thing without math, right?
[00:32:52.860 --> 00:32:55.140]   So you have a lot of points, and then you have this space
[00:32:55.140 --> 00:32:58.700]   where you just grid it up in these pillars, right?
[00:32:58.700 --> 00:33:00.900]   Some are empty, some are not empty.
[00:33:00.900 --> 00:33:02.900]   So in this sort of, with this notation,
[00:33:02.900 --> 00:33:05.900]   let me give a little bit of a literature review.
[00:33:05.900 --> 00:33:07.980]   What people tend to do is you take each pillar,
[00:33:07.980 --> 00:33:09.580]   and you divide it into voxels, right?
[00:33:09.580 --> 00:33:11.780]   So now you have a 3D voxel grid, right?
[00:33:11.780 --> 00:33:13.140]   And then you say, I'm gonna extract
[00:33:13.140 --> 00:33:14.460]   some sort of features for each voxel.
[00:33:14.460 --> 00:33:16.620]   For example, how many points are in this voxel?
[00:33:16.620 --> 00:33:18.640]   Or what is the maximum intensity
[00:33:18.640 --> 00:33:20.740]   of all the points in this voxel?
[00:33:20.740 --> 00:33:23.580]   Then you extract features for the whole pillar, right?
[00:33:23.580 --> 00:33:26.660]   What is the max intensity across all the points
[00:33:26.660 --> 00:33:28.420]   in the whole pillar, right?
[00:33:28.420 --> 00:33:31.500]   All of these are hand-engineered functions
[00:33:31.500 --> 00:33:33.820]   that generates the fixed length output.
[00:33:33.820 --> 00:33:36.380]   So what you can do is you can now concatenate them,
[00:33:36.380 --> 00:33:40.940]   and their output is this tensor x, y, z.
[00:33:40.940 --> 00:33:49.420]   So then, VoxelNet came around, I'd say, a year or so ago.
[00:33:49.420 --> 00:33:51.740]   Maybe a little bit more by now.
[00:33:51.740 --> 00:33:54.620]   So they do the first, the first step is similar, right?
[00:33:54.620 --> 00:33:56.520]   So you divide each pillar into voxels,
[00:33:56.520 --> 00:34:00.700]   and then you take, you map the points in each voxels.
[00:34:00.700 --> 00:34:02.380]   And the novel thing here is that
[00:34:02.380 --> 00:34:03.980]   they got rid of the feature engineering.
[00:34:03.980 --> 00:34:06.940]   So they said, we'll map it from a voxel
[00:34:06.940 --> 00:34:10.300]   to features using a PointNet.
[00:34:10.300 --> 00:34:12.220]   And I'm not gonna get into the details of a PointNet,
[00:34:12.220 --> 00:34:15.780]   but it's basically a network architecture
[00:34:15.780 --> 00:34:18.820]   that allows you to take a point cloud
[00:34:18.820 --> 00:34:21.920]   and map it to, again, a fixed length representation.
[00:34:21.920 --> 00:34:27.220]   So it's a series of 1D convolutions and max pooling layers.
[00:34:27.220 --> 00:34:29.240]   It's a very neat paper, right?
[00:34:29.240 --> 00:34:30.420]   So what they did is they, okay,
[00:34:30.420 --> 00:34:32.340]   we say we apply that to each voxel,
[00:34:32.340 --> 00:34:34.620]   but now I end up with this awkward four-dimensional tensor
[00:34:34.620 --> 00:34:37.660]   'cause I still have XYZ from the voxels,
[00:34:37.660 --> 00:34:41.860]   and then I have this C-dimensional output
[00:34:41.860 --> 00:34:42.900]   from the PointNet.
[00:34:42.900 --> 00:34:45.540]   So then they have to consolidate the Z dimension
[00:34:45.540 --> 00:34:47.740]   through a 3D convolution, right?
[00:34:47.740 --> 00:34:50.980]   And now you achieve your XYZ tensor.
[00:34:50.980 --> 00:34:52.060]   So now you're ready to go.
[00:34:52.060 --> 00:34:54.800]   So it's very nice in the sense that it's end-to-end method.
[00:34:54.800 --> 00:34:57.020]   They showed good performance,
[00:34:57.020 --> 00:34:58.260]   but at the end of the day, it was very slow.
[00:34:58.260 --> 00:35:00.620]   They got like five hertz runtime.
[00:35:00.620 --> 00:35:03.700]   And the culprit here is this last step,
[00:35:03.700 --> 00:35:05.780]   so the 3D convolution.
[00:35:05.780 --> 00:35:09.040]   It's much, much slower than a standard 2D convolution.
[00:35:09.040 --> 00:35:12.800]   All right, so here's what we did.
[00:35:12.800 --> 00:35:15.940]   We basically said, let's just forget about voxels.
[00:35:15.940 --> 00:35:17.900]   We'll take all the points in the pillar
[00:35:17.900 --> 00:35:21.700]   and we'll put it straight through PointNet.
[00:35:21.700 --> 00:35:22.540]   That's it.
[00:35:22.540 --> 00:35:29.260]   So just that single change gave a 10- to 100-fold speedup
[00:35:29.260 --> 00:35:30.940]   from VoxelNet.
[00:35:30.940 --> 00:35:33.260]   And then we simplified the PointNet.
[00:35:33.260 --> 00:35:34.380]   So now, instead of having,
[00:35:34.380 --> 00:35:35.980]   so PointNet can have several layers
[00:35:35.980 --> 00:35:37.780]   and several modules inside it.
[00:35:37.780 --> 00:35:40.300]   So we simplified it to a single 1D convolution
[00:35:40.300 --> 00:35:41.400]   and max pooling layer.
[00:35:41.400 --> 00:35:45.380]   And then we showed you can get a really fast implementation
[00:35:45.380 --> 00:35:48.140]   by taking all your pillars that are not empty,
[00:35:48.140 --> 00:35:50.420]   stack them together into a nice, dense tensor
[00:35:50.420 --> 00:35:52.620]   with a little bit of padding here and there.
[00:35:52.620 --> 00:35:56.780]   And then you can run the forward pass with a single,
[00:35:56.780 --> 00:35:59.020]   you can pose it as a 2D convolution
[00:35:59.020 --> 00:36:00.460]   with a one-by-one kernel.
[00:36:00.460 --> 00:36:06.180]   So the final encoder runtime is now 1.3 milliseconds,
[00:36:06.180 --> 00:36:08.340]   which is really, really fast.
[00:36:08.340 --> 00:36:12.540]   So the full method looks like this.
[00:36:12.540 --> 00:36:14.300]   So you have the point cloud,
[00:36:14.300 --> 00:36:17.580]   you have this pillar feature net, which is the encoder.
[00:36:17.580 --> 00:36:20.540]   So the different steps there,
[00:36:20.540 --> 00:36:22.820]   that feeds straight into the backbone
[00:36:22.820 --> 00:36:24.100]   and your detection heads.
[00:36:24.100 --> 00:36:25.180]   And there you go.
[00:36:25.180 --> 00:36:28.700]   So it's still a multi-stage architecture,
[00:36:28.700 --> 00:36:32.100]   but of course the key is that none of the steps are,
[00:36:32.100 --> 00:36:34.780]   all the steps are fully parameterized.
[00:36:34.780 --> 00:36:37.980]   And we can back propagate through the whole thing
[00:36:37.980 --> 00:36:38.820]   and learn it.
[00:36:38.820 --> 00:36:43.500]   So putting these things together,
[00:36:43.500 --> 00:36:46.100]   these were the results we got on the Qt Benchmark.
[00:36:46.100 --> 00:36:51.340]   So if you look at the car class, right,
[00:36:51.340 --> 00:36:53.580]   we actually got the highest performance,
[00:36:53.580 --> 00:36:56.340]   so this is I think the bird's eye view metric.
[00:36:56.340 --> 00:36:58.820]   And we even outperformed the methods
[00:36:58.820 --> 00:37:00.420]   that relied on LiDAR and vision.
[00:37:00.420 --> 00:37:05.780]   And we did that running at a little bit over 60 hertz.
[00:37:05.780 --> 00:37:15.100]   And this is, like I said, this is in terms of bird's eye view
[00:37:15.100 --> 00:37:17.780]   we can also measure the 3D benchmark
[00:37:17.780 --> 00:37:20.300]   and we get the same, very similar performance.
[00:37:23.580 --> 00:37:28.340]   Yeah, so, you know, car did well, cyclist did well,
[00:37:28.340 --> 00:37:30.780]   pedestrian there was one or two methods,
[00:37:30.780 --> 00:37:32.700]   future methods that did a little bit better.
[00:37:32.700 --> 00:37:35.300]   But then in aggregate on the top left,
[00:37:35.300 --> 00:37:36.900]   we ended up on top.
[00:37:36.900 --> 00:37:37.740]   So, yeah.
[00:37:37.740 --> 00:37:40.660]   And I put a little asterisk here,
[00:37:40.660 --> 00:37:43.020]   this is compared to published methods
[00:37:43.020 --> 00:37:44.940]   at the time of submission.
[00:37:44.940 --> 00:37:47.500]   And so many things happening so quickly.
[00:37:47.500 --> 00:37:49.620]   So there's tons of, you know,
[00:37:49.620 --> 00:37:51.180]   submissions on the Qt leaderboard
[00:37:51.180 --> 00:37:52.980]   that are completely anonymous,
[00:37:52.980 --> 00:37:55.180]   so we don't even know, you know,
[00:37:55.180 --> 00:37:57.940]   what was the input, what data did they use.
[00:37:57.940 --> 00:38:00.140]   So we only compare it to published methods.
[00:38:00.140 --> 00:38:04.660]   So here's some qualitative results.
[00:38:04.660 --> 00:38:07.300]   You have the, you know, just for visibilization
[00:38:07.300 --> 00:38:08.460]   you can project them into the image.
[00:38:08.460 --> 00:38:10.260]   So you see the gray boxes are the ground truth
[00:38:10.260 --> 00:38:13.500]   and the colored ones are the predictions.
[00:38:13.500 --> 00:38:21.220]   And yeah, some challenging ones,
[00:38:21.220 --> 00:38:22.700]   it's so small here.
[00:38:22.700 --> 00:38:25.420]   So we have, for example, the person on the right there,
[00:38:25.420 --> 00:38:29.260]   that's a person with a little stand
[00:38:29.260 --> 00:38:30.820]   got interpreted as a bicycle.
[00:38:30.820 --> 00:38:33.060]   We have this man on the ladder,
[00:38:33.060 --> 00:38:34.540]   which is an actual annotation error.
[00:38:34.540 --> 00:38:36.460]   So we discovered it as a person,
[00:38:36.460 --> 00:38:38.260]   but it wasn't annotated in the data.
[00:38:38.260 --> 00:38:44.340]   Here's a child on a bicycle that didn't get detected.
[00:38:44.340 --> 00:38:49.180]   So that's a, you know, that's a bummer.
[00:38:50.020 --> 00:38:54.340]   Okay, so that was KITTI,
[00:38:54.340 --> 00:38:56.940]   and then I just wanted to show you guys,
[00:38:56.940 --> 00:39:00.140]   of course we can run this on our vehicles.
[00:39:00.140 --> 00:39:01.660]   So this is a rendering.
[00:39:01.660 --> 00:39:04.980]   We just deploy the network at two hertz
[00:39:04.980 --> 00:39:08.700]   on the full 360 sensor suite.
[00:39:08.700 --> 00:39:12.180]   Input is still alive, you know, a few lighter sweeps,
[00:39:12.180 --> 00:39:16.140]   but just projected into the images for visualization.
[00:39:17.660 --> 00:39:19.740]   And again, no tracking or smoothing applied here.
[00:39:19.740 --> 00:39:24.020]   So it's every single frame is analyzed independently.
[00:39:24.020 --> 00:39:28.500]   See those arrows sticking out?
[00:39:28.500 --> 00:39:30.700]   That's the velocity estimate.
[00:39:30.700 --> 00:39:32.260]   So we actually show how you can,
[00:39:32.260 --> 00:39:37.060]   yeah, you can actually accumulate multiple point clouds
[00:39:37.060 --> 00:39:37.940]   into this method,
[00:39:37.940 --> 00:39:40.660]   and now you can start reasoning about velocity as well.
[00:39:40.660 --> 00:39:42.980]   (no audio)
[00:39:42.980 --> 00:39:52.660]   So the second part I want to talk about is NuScenes,
[00:39:52.660 --> 00:39:57.660]   which is a new benchmark data set that we have published.
[00:39:57.660 --> 00:39:58.700]   So what is NuScenes?
[00:39:58.700 --> 00:40:01.820]   So it's 1,020 second scenes
[00:40:01.820 --> 00:40:05.140]   that we collected with our development platform.
[00:40:05.140 --> 00:40:08.740]   So it's a full, it's the same platform that Carl showed,
[00:40:08.740 --> 00:40:12.060]   or a sort of previous generation platform, the Zoe vehicle.
[00:40:12.060 --> 00:40:15.660]   So it's full, you know, the full automotive sensor suite,
[00:40:15.660 --> 00:40:20.620]   data is registered and synced in 360 degree view.
[00:40:20.620 --> 00:40:22.860]   And it's also fully annotated with 3D bounding boxes.
[00:40:22.860 --> 00:40:27.020]   I think there's over one million 3D bounding boxes.
[00:40:27.020 --> 00:40:29.580]   And we actually make this freely available for research.
[00:40:29.580 --> 00:40:32.820]   So you can go to nuscenes.org right now
[00:40:32.820 --> 00:40:37.820]   and download a teaser release, which is 100 scenes,
[00:40:38.260 --> 00:40:40.380]   the full release will be in about a month.
[00:40:40.380 --> 00:40:44.940]   And of course the motivation is straightforward, right?
[00:40:44.940 --> 00:40:48.060]   So, you know, the whole field is driven by benchmark,
[00:40:48.060 --> 00:40:51.020]   and you know, without image, I don't think none of it,
[00:40:51.020 --> 00:40:52.780]   it may be the case that none of us are here,
[00:40:52.780 --> 00:40:53.620]   we're here, right?
[00:40:53.620 --> 00:40:54.860]   Because they may never have been able
[00:40:54.860 --> 00:40:56.740]   to write that first paper
[00:40:56.740 --> 00:40:58.960]   and sort of start this whole thing going.
[00:40:58.960 --> 00:41:01.980]   And when I started looking at 3D,
[00:41:01.980 --> 00:41:03.140]   I looked at the Kili benchmark,
[00:41:03.140 --> 00:41:05.540]   which is truly groundbreaking.
[00:41:05.540 --> 00:41:07.060]   I don't want to take anything away,
[00:41:07.060 --> 00:41:08.580]   but it was becoming outdated.
[00:41:08.580 --> 00:41:13.380]   They don't have full 3D view, they don't have any radar.
[00:41:13.380 --> 00:41:15.980]   So I think this offers an opportunity
[00:41:15.980 --> 00:41:18.880]   to sort of push the field forward a little bit.
[00:41:18.880 --> 00:41:22.500]   Right, and just as a comparison,
[00:41:22.500 --> 00:41:25.980]   this is sort of the most similar benchmark.
[00:41:25.980 --> 00:41:30.020]   And really the only one that you can really compare to
[00:41:30.020 --> 00:41:30.860]   is Kiti.
[00:41:30.860 --> 00:41:35.540]   But so there's other data sets that have maybe LIDAR only,
[00:41:35.540 --> 00:41:39.740]   tons of data sets that have image only, of course.
[00:41:39.740 --> 00:41:43.420]   But it's quite a big step up from Kiti.
[00:41:43.420 --> 00:41:46.180]   Yeah, some details.
[00:41:46.180 --> 00:41:51.100]   So you see the layout with the radars along the edge,
[00:41:51.100 --> 00:41:55.060]   all the cameras on the roof and the top LIDAR,
[00:41:55.060 --> 00:41:56.940]   and some of the receptive fields.
[00:41:56.940 --> 00:41:59.160]   And this data is all on the website.
[00:41:59.160 --> 00:42:03.180]   The taxonomy, so we model several different subcategories
[00:42:03.180 --> 00:42:05.620]   of pedestrians, several types of vehicles,
[00:42:05.620 --> 00:42:08.460]   some static objects, barrier cones.
[00:42:08.460 --> 00:42:11.020]   And then in addition, a bunch of attributes
[00:42:11.020 --> 00:42:12.980]   on the vehicles and on the pedestrians.
[00:42:12.980 --> 00:42:15.380]   All right, so without further ado,
[00:42:15.380 --> 00:42:16.640]   let's just look at some data.
[00:42:16.640 --> 00:42:18.820]   So this is one of the thousand scenes, right?
[00:42:18.820 --> 00:42:23.700]   So all I'm showing here is just playing the frames
[00:42:23.700 --> 00:42:25.680]   one by one of all the images.
[00:42:25.680 --> 00:42:30.700]   And again, the annotations live
[00:42:30.700 --> 00:42:32.300]   in the world coordinate system, right?
[00:42:32.300 --> 00:42:34.580]   So they are full 3D boxes.
[00:42:34.580 --> 00:42:36.580]   I've just projected them into the image.
[00:42:36.580 --> 00:42:38.580]   And that's what's so neat.
[00:42:38.580 --> 00:42:41.660]   So we're not really annotating the LIDAR
[00:42:41.660 --> 00:42:43.380]   or the camera or the radar.
[00:42:43.380 --> 00:42:45.420]   We're annotating the actual objects
[00:42:45.420 --> 00:42:46.900]   and put them in a world coordinate system
[00:42:46.900 --> 00:42:48.060]   and give all the transformations
[00:42:48.060 --> 00:42:52.140]   so you guys can play around with it how you like.
[00:42:52.140 --> 00:42:54.420]   So just to show that, so I can,
[00:42:54.420 --> 00:42:55.420]   because everything is ready,
[00:42:55.420 --> 00:42:56.740]   so I can now take the LIDAR sweeps
[00:42:56.740 --> 00:42:58.780]   and I can just project them into the images
[00:42:58.780 --> 00:42:59.780]   at the same time.
[00:42:59.780 --> 00:43:02.540]   So here I'm showing just colored by distance.
[00:43:02.540 --> 00:43:06.700]   So now you have some sort of sparse density measurement
[00:43:06.700 --> 00:43:09.660]   on the images, distance measurement, sorry.
[00:43:09.660 --> 00:43:11.780]   So that's all I wanted to talk about.
[00:43:11.780 --> 00:43:12.620]   Thank you.
[00:43:12.620 --> 00:43:15.780]   (audience applauding)
[00:43:15.780 --> 00:43:19.500]   - Hi, I was really, really interested
[00:43:19.500 --> 00:43:21.700]   in your discussion around validation
[00:43:21.700 --> 00:43:23.300]   and particularly continuous development
[00:43:23.300 --> 00:43:24.340]   and that sort of thing.
[00:43:24.340 --> 00:43:26.020]   And so my question was basically
[00:43:26.020 --> 00:43:27.320]   is this new scenes dataset,
[00:43:27.320 --> 00:43:29.580]   is this enough to guarantee
[00:43:29.580 --> 00:43:31.900]   that your model is going to generalize to unseen data
[00:43:31.900 --> 00:43:33.980]   and not hit pedestrians and that stuff
[00:43:33.980 --> 00:43:36.100]   or do you have other validation that you need to do?
[00:43:36.100 --> 00:43:36.940]   - No, no, no, I mean,
[00:43:36.940 --> 00:43:40.100]   so the new scenes effort is purely an academic effort.
[00:43:40.100 --> 00:43:43.580]   So we wanna share our data with academic community
[00:43:43.580 --> 00:43:46.300]   to drive the field forward.
[00:43:46.300 --> 00:43:47.620]   We're not making any claims
[00:43:47.620 --> 00:43:49.340]   that this is somehow a sufficient dataset
[00:43:49.340 --> 00:43:51.460]   for any safety case.
[00:43:51.460 --> 00:43:54.620]   It's a small subset of our data.
[00:43:56.100 --> 00:43:59.620]   Yeah, I would say, obviously,
[00:43:59.620 --> 00:44:01.200]   my background is in the academic world.
[00:44:01.200 --> 00:44:03.400]   One of the hardest things was always collecting data
[00:44:03.400 --> 00:44:05.240]   because it's difficult and expensive.
[00:44:05.240 --> 00:44:08.960]   And so having access to a dataset like that,
[00:44:08.960 --> 00:44:12.520]   which was expensive to collect and annotate,
[00:44:12.520 --> 00:44:15.000]   but which we thought we would make available
[00:44:15.000 --> 00:44:18.400]   because, well, we hope that it would spark
[00:44:18.400 --> 00:44:20.820]   academic interests and smart people
[00:44:20.820 --> 00:44:22.280]   like the people in this room
[00:44:22.280 --> 00:44:23.840]   coming up with new and better algorithms,
[00:44:23.840 --> 00:44:25.120]   which could benefit the whole community
[00:44:25.120 --> 00:44:26.660]   and then maybe some of you would even wanna
[00:44:26.660 --> 00:44:28.120]   come work with us at Aptiv.
[00:44:28.120 --> 00:44:31.700]   So not totally, a little bit of self-interest there.
[00:44:31.700 --> 00:44:33.220]   Wasn't intended to be for validation,
[00:44:33.220 --> 00:44:34.380]   it was more for research.
[00:44:34.380 --> 00:44:36.940]   To give you a sense of the scale of validation,
[00:44:36.940 --> 00:44:39.020]   there was one quote there at RAND
[00:44:39.020 --> 00:44:42.000]   saying you gotta drive 275 million miles or more,
[00:44:42.000 --> 00:44:47.000]   depending on the certainty you wanna impose.
[00:44:47.000 --> 00:44:48.680]   But to date as an industry,
[00:44:48.680 --> 00:44:51.540]   we've driven about 12 million miles
[00:44:51.540 --> 00:44:54.220]   to 12 to 14 million miles in sum,
[00:44:54.220 --> 00:44:56.900]   all participants in autonomous mode,
[00:44:56.900 --> 00:44:59.460]   under over hundreds of different bills of code
[00:44:59.460 --> 00:45:01.020]   and many different environments.
[00:45:01.020 --> 00:45:02.100]   So this would now be saying
[00:45:02.100 --> 00:45:04.560]   you're supposed to drive hundreds of millions of miles
[00:45:04.560 --> 00:45:07.360]   in a particular environment on a single bill of code,
[00:45:07.360 --> 00:45:08.980]   a single platform.
[00:45:08.980 --> 00:45:11.020]   Now obviously we're probably not gonna do that.
[00:45:11.020 --> 00:45:13.480]   What we'll end up doing is supplementing the driving
[00:45:13.480 --> 00:45:15.820]   with quite a lot of simulation
[00:45:15.820 --> 00:45:18.560]   and then other methodologies to convince ourselves
[00:45:18.560 --> 00:45:20.260]   that we can make a statistical,
[00:45:20.260 --> 00:45:22.420]   ultimately a statistical argument for safety.
[00:45:22.420 --> 00:45:25.220]   So there'll be use of data sets like this.
[00:45:25.220 --> 00:45:26.820]   We'll be doing lots of regression testing
[00:45:26.820 --> 00:45:30.140]   on supersized version of data sets like that
[00:45:30.140 --> 00:45:32.120]   and other kind of morally equivalent versions
[00:45:32.120 --> 00:45:33.500]   to test different parts of the systems.
[00:45:33.500 --> 00:45:34.640]   Now not just classification,
[00:45:34.640 --> 00:45:36.380]   but different aspects of the system.
[00:45:36.380 --> 00:45:38.740]   Our motion planning, decision making,
[00:45:38.740 --> 00:45:42.580]   localization, all aspects of the system.
[00:45:42.580 --> 00:45:45.480]   And then augment that with on-road driving
[00:45:45.480 --> 00:45:46.900]   and augment that with simulation.
[00:45:46.900 --> 00:45:49.480]   So the safety case is really quite a bit broader,
[00:45:49.480 --> 00:45:51.940]   unfortunately, than any single data set
[00:45:51.940 --> 00:45:54.620]   would allow you to kind of speak to.
[00:45:54.620 --> 00:45:56.340]   - From an industrial perspective,
[00:45:56.340 --> 00:46:00.920]   what do you think can 5G offer for autonomous vehicles?
[00:46:00.920 --> 00:46:04.000]   - 5G, yeah, it's an interesting one.
[00:46:04.000 --> 00:46:06.540]   Well, these vehicles are connected.
[00:46:06.540 --> 00:46:08.980]   You know, that's a requirement.
[00:46:08.980 --> 00:46:12.620]   Certainly when you think about operating them as a fleet.
[00:46:12.620 --> 00:46:14.820]   When the day comes when you have an autonomous vehicle
[00:46:14.820 --> 00:46:16.900]   that is personally owned,
[00:46:16.900 --> 00:46:19.180]   and that day will come in some point in the future,
[00:46:19.180 --> 00:46:20.420]   it may or may not be connected,
[00:46:20.420 --> 00:46:22.220]   it will almost certainly then be too.
[00:46:22.220 --> 00:46:24.380]   But when you have a fleet of vehicles
[00:46:24.380 --> 00:46:27.380]   and you wanna coordinate the activity of that fleet
[00:46:27.380 --> 00:46:30.100]   in a way to maximize the efficiency of that network,
[00:46:30.100 --> 00:46:31.660]   that transportation network,
[00:46:31.660 --> 00:46:33.180]   they're certainly connected.
[00:46:33.180 --> 00:46:35.740]   The requirements of that connectivity is fairly relaxed
[00:46:35.740 --> 00:46:37.380]   if you're talking about just passing back and forth
[00:46:37.380 --> 00:46:40.500]   the position of the car and maybe some status indicators.
[00:46:40.500 --> 00:46:42.280]   You know, are you in autonomous mode, manual mode,
[00:46:42.280 --> 00:46:43.980]   are all systems go, or do you have a fault code,
[00:46:43.980 --> 00:46:45.240]   and what is it?
[00:46:45.240 --> 00:46:48.220]   Now, there's some interesting requirements
[00:46:48.220 --> 00:46:49.580]   that become a little bit more stringent
[00:46:49.580 --> 00:46:51.740]   if you think about what we call teleoperation
[00:46:51.740 --> 00:46:53.900]   or remote operation of the car.
[00:46:53.900 --> 00:46:56.380]   The case where if the car encounters a situation
[00:46:56.380 --> 00:46:58.620]   it doesn't recognize, can't figure out,
[00:46:58.620 --> 00:47:00.340]   gets stuck or confused,
[00:47:00.340 --> 00:47:02.900]   you may kind of phone a human operator
[00:47:02.900 --> 00:47:05.080]   who's sitting remotely to intervene.
[00:47:05.080 --> 00:47:06.580]   And in that case, you know,
[00:47:06.580 --> 00:47:07.900]   that human operator will wanna have
[00:47:07.900 --> 00:47:09.380]   some situational awareness.
[00:47:09.380 --> 00:47:13.660]   There may be a demand of high bandwidth, low latency,
[00:47:13.660 --> 00:47:16.940]   high reliability of the sort that maybe 5G
[00:47:16.940 --> 00:47:19.540]   is better suited to than 4G.
[00:47:19.540 --> 00:47:21.420]   Or LT or whatever you've got.
[00:47:21.420 --> 00:47:25.700]   Broadly speaking, we see it as a very nice to have,
[00:47:25.700 --> 00:47:28.420]   but like any infrastructure,
[00:47:28.420 --> 00:47:30.660]   we understand that it's gonna arrive
[00:47:30.660 --> 00:47:32.420]   on a timeline of its own
[00:47:32.420 --> 00:47:34.940]   and be maintained by someone who's not us.
[00:47:34.940 --> 00:47:37.380]   So it's very much outside our control.
[00:47:37.380 --> 00:47:39.700]   And so for that reason, we design a system
[00:47:39.700 --> 00:47:43.260]   such that we don't rely on kind of the coming 5G wave,
[00:47:43.260 --> 00:47:45.220]   but we'll certainly welcome it when it arrives.
[00:47:45.220 --> 00:47:48.260]   - So you said you have presence in 45 countries.
[00:47:48.260 --> 00:47:51.140]   So did you observe any interesting patterns from that?
[00:47:51.140 --> 00:47:55.500]   Like your car, your same self-driving car model
[00:47:55.500 --> 00:47:57.900]   that is deployed in Vegas as well as Singapore
[00:47:57.900 --> 00:47:59.940]   was able to perform equally well
[00:47:59.940 --> 00:48:01.180]   in both Vegas and Singapore,
[00:48:01.180 --> 00:48:03.700]   or the model was able to perform very well in Singapore
[00:48:03.700 --> 00:48:05.580]   compared to Vegas?
[00:48:05.580 --> 00:48:06.420]   - To speak to your question
[00:48:06.420 --> 00:48:08.260]   about like country to country variation,
[00:48:08.260 --> 00:48:10.540]   you know, we touched on that for a moment
[00:48:10.540 --> 00:48:12.620]   in the validation discussion.
[00:48:12.620 --> 00:48:14.180]   But obviously driving in Singapore
[00:48:14.180 --> 00:48:15.540]   and driving in Vegas is pretty different.
[00:48:15.540 --> 00:48:18.340]   I mean, you're on the other side of the road for starters,
[00:48:18.340 --> 00:48:20.900]   but different traffic rules
[00:48:20.900 --> 00:48:23.300]   and it's sort of underappreciated people drive differently.
[00:48:23.300 --> 00:48:25.700]   There's slightly different traffic norms.
[00:48:25.700 --> 00:48:27.380]   So one of the things that,
[00:48:27.380 --> 00:48:30.500]   well, if anyone was in this class last year,
[00:48:30.500 --> 00:48:31.900]   my co-founder Emilio gave a talk
[00:48:31.900 --> 00:48:33.700]   about something we call rule books,
[00:48:33.700 --> 00:48:35.860]   which is a structure that we've designed
[00:48:35.860 --> 00:48:37.460]   around what we call the driving policy
[00:48:37.460 --> 00:48:38.580]   or the decision-making engine,
[00:48:38.580 --> 00:48:42.500]   which tries to admit in a general and fairly flexible way
[00:48:44.020 --> 00:48:47.300]   the ability to reprioritize rules, reassign rules,
[00:48:47.300 --> 00:48:50.300]   change weights on rules to enable us to drive
[00:48:50.300 --> 00:48:52.300]   in one community and then another
[00:48:52.300 --> 00:48:53.980]   in a fairly seamless manner.
[00:48:53.980 --> 00:48:55.020]   So to give you an example,
[00:48:55.020 --> 00:48:57.820]   when we wanted to get on the road in Singapore,
[00:48:57.820 --> 00:48:59.460]   if you can imagine you've got a,
[00:48:59.460 --> 00:49:02.100]   so let's say you're a autonomy engineer
[00:49:02.100 --> 00:49:03.820]   who was tasked with writing the decision-making engine
[00:49:03.820 --> 00:49:06.100]   and you decide I'm gonna do a finite state architecture,
[00:49:06.100 --> 00:49:07.860]   I'm gonna write down some transition rules,
[00:49:07.860 --> 00:49:09.820]   I'm gonna do them by hand, it's gonna be great.
[00:49:09.820 --> 00:49:12.060]   And then you did that for the right-hand driving
[00:49:12.060 --> 00:49:13.100]   and your boss came in and said,
[00:49:13.100 --> 00:49:15.020]   "Oh yeah, next Monday we're gonna be left-hand driving,
[00:49:15.020 --> 00:49:18.580]   "so just flip all that and get it ready to go."
[00:49:18.580 --> 00:49:21.620]   That could be a huge pain, pain to do,
[00:49:21.620 --> 00:49:23.300]   'cause it's generally speaking you're doing it manually
[00:49:23.300 --> 00:49:24.780]   and then very difficult to validate,
[00:49:24.780 --> 00:49:27.380]   to ensure that the outputs are correct
[00:49:27.380 --> 00:49:29.740]   across the entire spectrum of possibilities.
[00:49:29.740 --> 00:49:31.020]   So we wanted to avoid that.
[00:49:31.020 --> 00:49:33.580]   And so the long story short,
[00:49:33.580 --> 00:49:36.940]   we actually quite carefully designed the system
[00:49:36.940 --> 00:49:41.780]   such that we can scale to different cities and countries.
[00:49:41.780 --> 00:49:45.060]   And one of the ways you do that is by thinking carefully
[00:49:45.060 --> 00:49:47.100]   around the architectural design
[00:49:47.100 --> 00:49:48.900]   of the decision-making engine.
[00:49:48.900 --> 00:49:51.860]   But it's quite different.
[00:49:51.860 --> 00:49:53.940]   There's four cities I mentioned which are our primary sites,
[00:49:53.940 --> 00:49:56.700]   Boston, Pittsburgh, Vegas, and Singapore,
[00:49:56.700 --> 00:49:58.760]   spans a wide spectrum of driving conditions.
[00:49:58.760 --> 00:50:02.700]   I mean, everybody knows Boston, which is pretty bad.
[00:50:02.700 --> 00:50:07.700]   Vegas is warm weather, mid-density urban, but it's Vegas.
[00:50:07.700 --> 00:50:11.080]   So I mean, all kinds of stuff.
[00:50:11.080 --> 00:50:13.260]   And then Singapore is interesting,
[00:50:13.260 --> 00:50:16.380]   perfect infrastructure, good weather, flat.
[00:50:16.380 --> 00:50:18.380]   People, generally speaking, obey the rules,
[00:50:18.380 --> 00:50:21.860]   so it's kind of close to the ideal case.
[00:50:21.860 --> 00:50:25.340]   So that exposure to this different spectrum of data,
[00:50:25.340 --> 00:50:27.900]   I think, I'll speak for Oscar, maybe, is pretty valuable.
[00:50:27.900 --> 00:50:30.020]   I know for other parts of the development team,
[00:50:30.020 --> 00:50:30.860]   quite valuable.
[00:50:30.860 --> 00:50:33.100]   - Singapore is ideal except there are,
[00:50:33.100 --> 00:50:34.820]   there's constant construction zones.
[00:50:34.820 --> 00:50:37.620]   So every time we drive out, there's a new construction zone.
[00:50:37.620 --> 00:50:39.580]   So we focused, have a lot of work
[00:50:39.580 --> 00:50:41.720]   on construction zone detection in Singapore.
[00:50:41.720 --> 00:50:42.880]   - And the torrential rain.
[00:50:42.880 --> 00:50:44.720]   - Yeah, and the jaywalkers.
[00:50:44.720 --> 00:50:46.440]   - And the jaywalkers, right.
[00:50:46.440 --> 00:50:47.680]   Yeah, they do jaywalk.
[00:50:47.680 --> 00:50:50.500]   People don't break the rules, but they jaywalk.
[00:50:50.500 --> 00:50:52.480]   Other than that, it's perfect.
[00:50:52.480 --> 00:50:54.120]   So which country's fully equipped?
[00:50:54.120 --> 00:50:55.880]   That's a really good question, yeah.
[00:50:55.880 --> 00:50:59.120]   Well, it's interesting because there's other dimensions.
[00:50:59.120 --> 00:51:01.640]   So when we look at which countries are interesting to us
[00:51:01.640 --> 00:51:03.920]   to be in as a market,
[00:51:03.920 --> 00:51:05.960]   there's the infrastructure conditions,
[00:51:05.960 --> 00:51:08.400]   there's the driving patterns and properties,
[00:51:08.400 --> 00:51:10.620]   the density, is it Times Square at rush hour
[00:51:10.620 --> 00:51:12.460]   or is it Dubuque, Iowa?
[00:51:12.460 --> 00:51:14.400]   There is the regulatory environment,
[00:51:14.400 --> 00:51:15.780]   which is incredibly important.
[00:51:15.780 --> 00:51:17.220]   You may have a perfectly well-suited city
[00:51:17.220 --> 00:51:18.700]   from a technical perspective
[00:51:18.700 --> 00:51:21.560]   and they may not allow you to drive there.
[00:51:21.560 --> 00:51:24.460]   So it's really all of these things put together.
[00:51:24.460 --> 00:51:26.280]   And so we kind of have a matrix.
[00:51:26.280 --> 00:51:28.420]   We analyze which cities check these boxes
[00:51:28.420 --> 00:51:31.700]   and assign them scores and then try to understand
[00:51:31.700 --> 00:51:34.040]   then also the economics of that market.
[00:51:34.040 --> 00:51:36.020]   Is that city, check all these boxes,
[00:51:36.020 --> 00:51:38.780]   but there's no one using mobility services there.
[00:51:38.780 --> 00:51:41.420]   There's no opportunity to actually generate revenue
[00:51:41.420 --> 00:51:43.020]   from the service.
[00:51:43.020 --> 00:51:45.160]   So you factor in all of those things.
[00:51:45.160 --> 00:51:48.100]   - Yeah, and I think, I mean, one thing to keep in mind
[00:51:48.100 --> 00:51:50.500]   that it's always the first thing I tell candidates
[00:51:50.500 --> 00:51:51.940]   when I interview them.
[00:51:51.940 --> 00:51:54.100]   There's a huge difference in the advantage
[00:51:54.100 --> 00:51:55.700]   to the business model we're proposing, right?
[00:51:55.700 --> 00:51:57.260]   The ride-hailing service.
[00:51:57.260 --> 00:52:00.180]   So we can choose, even if we commit to a certain city,
[00:52:00.180 --> 00:52:03.540]   we can still select the routes that we feel comfortable
[00:52:03.540 --> 00:52:05.180]   and we can roll it out sort of piece by piece.
[00:52:05.180 --> 00:52:07.260]   We can say, okay, we don't feel comfortable
[00:52:07.260 --> 00:52:09.460]   when driving at night in the city yet.
[00:52:09.460 --> 00:52:12.060]   So we just won't accept any rides, right?
[00:52:12.060 --> 00:52:16.020]   So there's like that decision space as well.
[00:52:16.020 --> 00:52:17.500]   - Hi, thank you very much for coming
[00:52:17.500 --> 00:52:18.780]   and giving us this talk today.
[00:52:18.780 --> 00:52:19.660]   It was very, very interesting.
[00:52:19.660 --> 00:52:21.420]   I have a question which might reveal more
[00:52:21.420 --> 00:52:24.340]   about how naive I am than anything else.
[00:52:24.340 --> 00:52:28.220]   I was comparing your point pillar approach
[00:52:28.220 --> 00:52:31.740]   to the earlier approach where you were,
[00:52:31.740 --> 00:52:34.500]   which is the Voxel-based approach
[00:52:34.500 --> 00:52:37.100]   to interpreting the LIDAR results.
[00:52:37.100 --> 00:52:40.900]   And in the Voxels, you had a four-dimensional tensor
[00:52:40.900 --> 00:52:42.900]   that you were starting with, and your point pillar,
[00:52:42.900 --> 00:52:43.900]   you only have three dimensions.
[00:52:43.900 --> 00:52:46.340]   You're throwing away the Z, as I understood it.
[00:52:46.340 --> 00:52:49.100]   So when you do that, are you concerned
[00:52:49.100 --> 00:52:51.460]   that you're losing information about potential occlusions
[00:52:51.460 --> 00:52:53.420]   or transparencies or semi-occlusions?
[00:52:53.420 --> 00:52:56.540]   Is this a concern?
[00:52:56.540 --> 00:52:57.380]   - I see.
[00:52:57.380 --> 00:53:00.980]   So I may have been a little bit sloppy there.
[00:53:00.980 --> 00:53:03.500]   So we're certainly not throwing away the Z.
[00:53:03.500 --> 00:53:05.660]   All we're saying is that we're learning
[00:53:05.660 --> 00:53:08.060]   the embedding in the Z dimension
[00:53:08.060 --> 00:53:10.340]   jointly with everything else.
[00:53:10.340 --> 00:53:14.220]   So VoxelNet, if you want, sort of felt,
[00:53:14.220 --> 00:53:15.900]   when I first signed that paper,
[00:53:15.900 --> 00:53:18.260]   I felt the need to spoon-feed the network a little bit
[00:53:18.260 --> 00:53:21.300]   and say, let's learn everything stratified
[00:53:21.300 --> 00:53:24.820]   in this height dimension.
[00:53:24.820 --> 00:53:26.940]   And then we'll have a second step
[00:53:26.940 --> 00:53:30.820]   where we learn to consolidate that into a single vector.
[00:53:30.820 --> 00:53:33.660]   We just said, why not just learn those things together?
[00:53:33.660 --> 00:53:35.100]   So, yeah.
[00:53:35.100 --> 00:53:36.740]   - Thanks for your talk.
[00:53:36.740 --> 00:53:38.060]   I have a question for Carl.
[00:53:38.060 --> 00:53:42.180]   You mentioned that if people make change to the code,
[00:53:42.180 --> 00:53:45.580]   do we need another validation or not?
[00:53:45.580 --> 00:53:48.700]   So I work in the industry of nuclear power.
[00:53:48.700 --> 00:53:51.180]   So we do nuclear power simulations.
[00:53:51.180 --> 00:53:56.180]   So when we make any change to our simulation code,
[00:53:56.180 --> 00:53:57.860]   and to make it commercialized,
[00:53:57.860 --> 00:54:01.260]   we need to submit a request for the NRC,
[00:54:01.260 --> 00:54:04.660]   which is the Nuclear Regulation Committee.
[00:54:04.660 --> 00:54:08.260]   So in your opinion, do you think for self-driving,
[00:54:08.260 --> 00:54:14.140]   we need another third-party validation committee or not?
[00:54:14.140 --> 00:54:19.580]   Or should that be a third party, or is just self-check?
[00:54:19.580 --> 00:54:22.540]   - Yeah, that's a really good question.
[00:54:22.540 --> 00:54:24.140]   So I don't know the answer.
[00:54:24.140 --> 00:54:26.020]   I wouldn't be surprised, let me put it this way.
[00:54:26.020 --> 00:54:27.740]   I would not be surprised either way
[00:54:27.740 --> 00:54:29.340]   if the automotive industry ended up
[00:54:29.340 --> 00:54:33.860]   with third-party regulatory oversight, or it didn't.
[00:54:33.860 --> 00:54:34.700]   And I'll tell you why.
[00:54:34.700 --> 00:54:37.780]   There's great precedence for what you just described.
[00:54:37.780 --> 00:54:40.980]   Nuclear, aerospace, there's external bodies
[00:54:40.980 --> 00:54:43.700]   who have deep technical competence,
[00:54:43.700 --> 00:54:45.780]   who can come in, they can do investigations,
[00:54:45.780 --> 00:54:50.020]   they can impose strict regulation, or advise regulation,
[00:54:50.020 --> 00:54:55.020]   and they can partner or define requirements
[00:54:56.600 --> 00:54:58.880]   for certification of various types.
[00:54:58.880 --> 00:55:01.680]   The automotive industry has largely been self-certifying.
[00:55:01.680 --> 00:55:05.080]   There's an argument, which is certainly not unreasonable,
[00:55:05.080 --> 00:55:08.800]   that you have a real alignment of incentive
[00:55:08.800 --> 00:55:11.640]   within the industry and with the public
[00:55:11.640 --> 00:55:13.520]   to be as safe as possible.
[00:55:13.520 --> 00:55:17.900]   Simply put, the cost of a crash is enormous,
[00:55:17.900 --> 00:55:20.720]   economically, socially, everything else.
[00:55:20.720 --> 00:55:22.840]   But whether it continues along that path,
[00:55:22.840 --> 00:55:23.880]   I couldn't tell you.
[00:55:25.060 --> 00:55:28.580]   It's an interesting space because it's one
[00:55:28.580 --> 00:55:31.140]   where the federal government is actually moving
[00:55:31.140 --> 00:55:32.020]   very, very quickly.
[00:55:32.020 --> 00:55:33.900]   I mean, I would say carefully, too,
[00:55:33.900 --> 00:55:36.620]   not overstepping and not trying to impose
[00:55:36.620 --> 00:55:38.660]   too much regulation around an industry
[00:55:38.660 --> 00:55:41.620]   that has never generated a dollar of revenue.
[00:55:41.620 --> 00:55:43.620]   It's still quite nascent.
[00:55:43.620 --> 00:55:45.680]   But if you would have told me a few years ago
[00:55:45.680 --> 00:55:48.820]   that there would have been very thoughtfully defined
[00:55:48.820 --> 00:55:52.620]   draft regulatory guidelines or advice,
[00:55:52.620 --> 00:55:55.540]   I mean, let's say, it's not firm regulation,
[00:55:55.540 --> 00:55:58.220]   around this industry, I probably wouldn't have believed you.
[00:55:58.220 --> 00:55:59.180]   But in fact, that exists.
[00:55:59.180 --> 00:56:01.140]   There's a third version that was released this summer
[00:56:01.140 --> 00:56:04.300]   by the Department of Transportation.
[00:56:04.300 --> 00:56:07.700]   So there's intense interest on the regulatory side.
[00:56:07.700 --> 00:56:11.880]   In terms of how far the process goes
[00:56:11.880 --> 00:56:13.520]   in terms of formation of an external body,
[00:56:13.520 --> 00:56:15.060]   I think really remains to be seen.
[00:56:15.060 --> 00:56:16.660]   I don't know the answer.
[00:56:16.660 --> 00:56:18.740]   - Thanks for your insightful talk.
[00:56:18.740 --> 00:56:21.760]   Looking at this slide, I'm wondering how easy
[00:56:21.760 --> 00:56:25.100]   and effective your train models are
[00:56:25.100 --> 00:56:27.340]   to transfer across different weathers
[00:56:27.340 --> 00:56:29.300]   and whether you need, for example,
[00:56:29.300 --> 00:56:32.780]   if it is snowing, do you need specific trainings
[00:56:32.780 --> 00:56:36.100]   for specifically for your lidars to work effectively
[00:56:36.100 --> 00:56:39.500]   or you don't see any issues in that regard?
[00:56:39.500 --> 00:56:41.700]   - No, I mean, I think the same rules apply
[00:56:41.700 --> 00:56:44.380]   to this method as any other machine learning-based method.
[00:56:44.380 --> 00:56:46.220]   You wanna have support in your training data
[00:56:46.220 --> 00:56:49.220]   for the situation you wanna deploy in.
[00:56:49.220 --> 00:56:51.760]   So if we have no snow in our training data,
[00:56:51.760 --> 00:56:54.120]   I wouldn't go and deploy this in snow.
[00:56:54.120 --> 00:56:57.680]   I do like, one thing I like after having worked
[00:56:57.680 --> 00:57:00.220]   so much with vision though is that the lidar point cloud
[00:57:00.220 --> 00:57:05.060]   is really easy to augment and play around with.
[00:57:05.060 --> 00:57:07.720]   So for example, if you wanna say,
[00:57:07.720 --> 00:57:11.720]   you wanna be robust in really rare events, right?
[00:57:11.720 --> 00:57:13.680]   So let's say there's a piano on the road.
[00:57:13.680 --> 00:57:15.560]   I really wanna detect that.
[00:57:15.560 --> 00:57:17.460]   But it's hard because I have very few examples
[00:57:17.460 --> 00:57:19.200]   of pianos on the road, right?
[00:57:19.200 --> 00:57:21.260]   Now if you think about augmenting your visual dataset
[00:57:21.260 --> 00:57:23.720]   with that data, it's actually quite tricky.
[00:57:23.720 --> 00:57:26.320]   So that easy to have a photorealistic piano
[00:57:26.320 --> 00:57:27.480]   in your training data.
[00:57:27.480 --> 00:57:30.640]   But it is quite easy to do that in your lidar data, right?
[00:57:30.640 --> 00:57:33.200]   So you have a 3D model of your piano,
[00:57:33.200 --> 00:57:35.160]   you have the model for your lidar
[00:57:35.160 --> 00:57:37.040]   and you can get a pretty accurate,
[00:57:37.040 --> 00:57:40.520]   fairly realistic point cloud return from that, right?
[00:57:40.520 --> 00:57:42.080]   So I like that part about working with lidar.
[00:57:42.080 --> 00:57:44.360]   You can augment, you can play around with it.
[00:57:44.360 --> 00:57:47.500]   In fact, one of the things we do when we train this model
[00:57:47.500 --> 00:57:51.820]   is that we copy and paste samples from,
[00:57:51.820 --> 00:57:53.840]   or like objects from different samples.
[00:57:53.840 --> 00:57:56.240]   So you can take a car that I saw yesterday,
[00:57:56.240 --> 00:57:59.420]   take the point returns on that car,
[00:57:59.420 --> 00:58:02.280]   you can just paste it into your current lidar sweep.
[00:58:02.280 --> 00:58:03.860]   You have to be a little bit careful, right?
[00:58:03.860 --> 00:58:07.100]   And this was actually proposed by another,
[00:58:07.100 --> 00:58:08.500]   by a previous paper.
[00:58:08.500 --> 00:58:11.680]   And we found that that was a really useful data.
[00:58:11.680 --> 00:58:14.420]   It sounds absurd, but it actually works.
[00:58:14.420 --> 00:58:18.280]   And it speaks to the ability to do that with lidar point cloud.
[00:58:18.280 --> 00:58:19.120]   - Okay, great.
[00:58:19.120 --> 00:58:20.920]   Please give Carl and Oscar a big hand.
[00:58:20.920 --> 00:58:21.760]   Thank you so much.
[00:58:21.760 --> 00:58:24.920]   (audience applauding)
[00:58:25.540 --> 00:58:26.540]   - Excellent.
[00:58:26.540 --> 00:58:29.120]   (upbeat music)
[00:58:29.120 --> 00:58:31.700]   (upbeat music)
[00:58:31.700 --> 00:58:34.280]   (upbeat music)
[00:58:34.280 --> 00:58:36.860]   (upbeat music)
[00:58:36.860 --> 00:58:39.440]   (upbeat music)
[00:58:39.440 --> 00:58:42.020]   (upbeat music)
[00:58:42.020 --> 00:58:52.020]   [BLANK_AUDIO]

