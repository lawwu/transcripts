
[00:00:00.000 --> 00:00:02.540]   The following is a conversation with Tyler Cohen,
[00:00:02.540 --> 00:00:04.900]   an economist at George Mason University
[00:00:04.900 --> 00:00:08.220]   and co-creator of an amazing economics blog
[00:00:08.220 --> 00:00:10.300]   called Marginal Revolution.
[00:00:10.300 --> 00:00:13.460]   Author of many books, including The Great Stagnation,
[00:00:13.460 --> 00:00:17.280]   Average is Over, and his most recent, Big Business,
[00:00:17.280 --> 00:00:20.260]   a love letter to an American antihero.
[00:00:20.260 --> 00:00:22.540]   He's truly a polymath in his work,
[00:00:22.540 --> 00:00:24.300]   including his love for food,
[00:00:24.300 --> 00:00:26.460]   which makes this amazing podcast
[00:00:26.460 --> 00:00:28.460]   called Conversations with Tyler
[00:00:28.460 --> 00:00:29.960]   really fun to listen to.
[00:00:29.960 --> 00:00:32.680]   Quick mention of our sponsors,
[00:00:32.680 --> 00:00:37.380]   Linode, ExpressVPN, SimpliSafe, and Public Goods.
[00:00:37.380 --> 00:00:41.020]   Check them out in the description to support this podcast.
[00:00:41.020 --> 00:00:45.180]   As a side note, given Tyler's culinary explorations,
[00:00:45.180 --> 00:00:48.380]   let me say that one of the things that makes me sad
[00:00:48.380 --> 00:00:51.020]   about my love-hate relationship with food
[00:00:51.020 --> 00:00:53.860]   is that while I've found a simple diet,
[00:00:53.860 --> 00:00:55.520]   plain meat and veggies,
[00:00:55.520 --> 00:00:58.220]   that makes me happy in day-to-day life,
[00:00:58.220 --> 00:01:00.140]   I sometimes wish I had the mental ability
[00:01:00.140 --> 00:01:02.060]   to moderate consumption of food
[00:01:02.060 --> 00:01:03.780]   so that I could truly enjoy meals
[00:01:03.780 --> 00:01:06.140]   that go way outside of that diet.
[00:01:06.140 --> 00:01:07.540]   I've seen my mom, for example,
[00:01:07.540 --> 00:01:09.700]   enjoy a single piece of chocolate,
[00:01:09.700 --> 00:01:12.520]   and yet, if I were to eat one piece of chocolate,
[00:01:12.520 --> 00:01:15.960]   the odds are high that I would end up eating the whole box.
[00:01:15.960 --> 00:01:18.200]   This is definitely something I would like to fix
[00:01:18.200 --> 00:01:21.860]   because some of the amazing artistry in this world
[00:01:21.860 --> 00:01:23.260]   happens in the kitchen,
[00:01:23.260 --> 00:01:25.380]   and some of the richest human experiences
[00:01:25.380 --> 00:01:27.560]   happen over a unique meal.
[00:01:27.560 --> 00:01:29.380]   I recently was eating cheeseburgers
[00:01:29.380 --> 00:01:33.540]   with Joe Rogan and John Donaher late at night in Austin,
[00:01:33.540 --> 00:01:35.700]   talking about jiu-jitsu and life,
[00:01:35.700 --> 00:01:40.180]   and I was distinctly aware of the magic of that experience,
[00:01:40.180 --> 00:01:41.720]   magic made possible
[00:01:41.720 --> 00:01:44.480]   by the incredibly delicious cheeseburgers.
[00:01:44.480 --> 00:01:47.140]   This is the Lex Friedman Podcast,
[00:01:47.140 --> 00:01:50.060]   and here is my conversation with Tyler Cohen.
[00:01:50.060 --> 00:01:54.400]   Would you say economics is more art or science
[00:01:54.400 --> 00:01:57.080]   or philosophy or even magic?
[00:01:57.080 --> 00:01:58.100]   What is it?
[00:01:58.100 --> 00:02:01.440]   - Economics is interesting because it's all of the above.
[00:02:01.440 --> 00:02:02.720]   To start with magic,
[00:02:02.720 --> 00:02:04.800]   the notion that you can make some change
[00:02:04.800 --> 00:02:06.620]   and simply everyone's better off,
[00:02:06.620 --> 00:02:08.420]   that is a kind of modern magic
[00:02:08.420 --> 00:02:11.020]   that has replaced old-style magic.
[00:02:11.020 --> 00:02:14.620]   It's an art in the sense that the models are not very exact.
[00:02:14.620 --> 00:02:15.900]   It's a science in the sense
[00:02:15.900 --> 00:02:18.540]   that occasionally propositions are falsified.
[00:02:18.540 --> 00:02:20.860]   There are a few basic things we know,
[00:02:20.860 --> 00:02:22.340]   and however trivial they may sound,
[00:02:22.340 --> 00:02:24.580]   if you don't know them, you're out of luck.
[00:02:24.580 --> 00:02:26.480]   So all of the above.
[00:02:26.480 --> 00:02:28.580]   But from my outsider's perspective,
[00:02:28.580 --> 00:02:33.580]   economics is sometimes able to formulate very simple,
[00:02:33.580 --> 00:02:36.100]   almost like E equals MC squared,
[00:02:36.100 --> 00:02:40.020]   general models of how our human society will function
[00:02:40.020 --> 00:02:41.740]   when you do a certain thing.
[00:02:41.740 --> 00:02:47.420]   But it seems impossible or almost way too optimistic
[00:02:47.420 --> 00:02:49.420]   to think that a single formula
[00:02:49.420 --> 00:02:51.620]   or just a set of simple principles
[00:02:51.620 --> 00:02:56.040]   can describe behavior of billions of human beings.
[00:02:56.040 --> 00:02:58.580]   Well, with all the complexity that we have involved.
[00:02:58.580 --> 00:03:01.220]   So do you have a sense there's a hope for economics
[00:03:01.220 --> 00:03:06.220]   to have those kinds of physics-level descriptions
[00:03:06.220 --> 00:03:08.080]   and models of the world?
[00:03:08.080 --> 00:03:10.060]   Or is it just our desperate attempts as humans
[00:03:10.060 --> 00:03:10.900]   to make sense of it,
[00:03:10.900 --> 00:03:12.980]   even though it's more desperate
[00:03:12.980 --> 00:03:17.980]   than rigorous and serious and actually predictable
[00:03:17.980 --> 00:03:20.540]   like a physics-type science?
[00:03:20.540 --> 00:03:24.160]   - I don't think economics will ever be very predictive.
[00:03:24.160 --> 00:03:27.260]   It's most useful for helping you ask better questions.
[00:03:27.260 --> 00:03:29.420]   You look at something like game theory.
[00:03:29.420 --> 00:03:32.660]   Well, game theory never predicted USA and USSR
[00:03:32.660 --> 00:03:35.060]   would have a war, would not have a war.
[00:03:35.060 --> 00:03:38.140]   But trying to think through the logic of strategic conflict,
[00:03:38.140 --> 00:03:39.480]   if you know game theory,
[00:03:39.480 --> 00:03:42.500]   it's just a much more interesting discussion.
[00:03:42.500 --> 00:03:44.540]   - Are you surprised that we,
[00:03:44.540 --> 00:03:47.420]   speaking of the Soviet Union and United States,
[00:03:47.420 --> 00:03:48.780]   and speaking of game theory,
[00:03:48.780 --> 00:03:50.900]   are you surprised that we haven't destroyed ourselves
[00:03:50.900 --> 00:03:52.020]   with nuclear weapons yet?
[00:03:52.020 --> 00:03:54.280]   Like that simple formulation
[00:03:54.280 --> 00:03:56.320]   of mutually assured destruction,
[00:03:56.320 --> 00:03:58.960]   that's a good example of an explanation
[00:03:58.960 --> 00:04:02.360]   that perhaps allows us to ask better questions,
[00:04:02.360 --> 00:04:06.120]   but it seems to have actually described the reality
[00:04:06.120 --> 00:04:08.340]   of why we haven't destroyed ourselves
[00:04:08.340 --> 00:04:10.840]   with these ultra-powerful weapons.
[00:04:10.840 --> 00:04:12.280]   Are you surprised?
[00:04:12.280 --> 00:04:14.440]   Do you think the game theoretic explanation
[00:04:14.440 --> 00:04:16.880]   is at all accurate there?
[00:04:16.880 --> 00:04:19.920]   - I think we will destroy each other with those weapons.
[00:04:19.920 --> 00:04:20.760]   - Eventually?
[00:04:20.760 --> 00:04:21.920]   - Eventually.
[00:04:21.920 --> 00:04:24.320]   Look, it's a very low probability event,
[00:04:24.320 --> 00:04:26.720]   so I'm not surprised it hasn't happened yet.
[00:04:26.720 --> 00:04:29.480]   I'm a little surprised it came as close as it did.
[00:04:29.480 --> 00:04:31.000]   You know, your general thinking,
[00:04:31.000 --> 00:04:33.080]   realizing it might have just been a flock of birds
[00:04:33.080 --> 00:04:35.680]   or it wasn't a first strike attack from the USA,
[00:04:35.680 --> 00:04:38.040]   we got very lucky on that one.
[00:04:38.040 --> 00:04:39.960]   But if you just keep on running the clock
[00:04:39.960 --> 00:04:42.920]   on a low probability event, it will happen.
[00:04:42.920 --> 00:04:47.160]   And it may not be USA and China, USA and Russia, whatever.
[00:04:47.160 --> 00:04:49.200]   You know, it could be the Saudis and Turkey.
[00:04:49.200 --> 00:04:50.560]   - And it might not be nuclear weapons,
[00:04:50.560 --> 00:04:51.560]   it might be some other destruction.
[00:04:51.560 --> 00:04:52.720]   - Bio weapons.
[00:04:52.720 --> 00:04:55.560]   But it simply will happen is my view.
[00:04:55.560 --> 00:04:58.800]   And I've argued at best we have 700 or 800 years,
[00:04:58.800 --> 00:05:00.240]   and that's being generous.
[00:05:00.240 --> 00:05:03.040]   - At worst, how long we got?
[00:05:03.040 --> 00:05:06.400]   - Well, maybe it's like a Poisson arrival process, right?
[00:05:06.400 --> 00:05:09.560]   So tiny probability, could come any time.
[00:05:09.560 --> 00:05:12.000]   Probably not in your lifetime.
[00:05:12.000 --> 00:05:15.720]   But the chance presumably increases
[00:05:15.720 --> 00:05:18.760]   the cheaper weapons of mass destruction are.
[00:05:18.760 --> 00:05:22.400]   - So the Poisson process description doesn't take
[00:05:22.400 --> 00:05:24.360]   in consideration the game theoretic aspect.
[00:05:24.360 --> 00:05:28.080]   So another way to consider is repeated games,
[00:05:28.080 --> 00:05:29.720]   iterative games.
[00:05:29.720 --> 00:05:33.440]   So is there something about us, our human nature
[00:05:33.440 --> 00:05:38.200]   that allows us to fight against probability, reduce?
[00:05:38.200 --> 00:05:40.920]   Like the closer we get to trouble,
[00:05:40.920 --> 00:05:43.840]   the more we're able to figure out how to avoid trouble.
[00:05:43.840 --> 00:05:46.120]   The same thing is for when you take exams
[00:05:46.120 --> 00:05:49.320]   or you go and take classes, the closer,
[00:05:49.320 --> 00:05:52.200]   or paper deadlines, the closer you get to a deadline,
[00:05:52.200 --> 00:05:53.480]   the better you start to perform
[00:05:53.480 --> 00:05:56.200]   and get your shit together and actually get stuff done.
[00:05:56.200 --> 00:05:58.720]   - I'm really not so negative on human nature.
[00:05:58.720 --> 00:06:00.600]   And as an economist, I very much see
[00:06:00.600 --> 00:06:02.880]   the gains from cooperation.
[00:06:02.880 --> 00:06:05.240]   But if you just ask, are there outliers in history?
[00:06:05.240 --> 00:06:08.160]   Like was there a Hitler, for instance, obviously.
[00:06:08.160 --> 00:06:10.000]   And again, you let the clock tick,
[00:06:10.000 --> 00:06:12.280]   another Hitler with nuclear weapons,
[00:06:12.280 --> 00:06:15.160]   doesn't per se care about his own destruction,
[00:06:15.160 --> 00:06:16.480]   it will happen.
[00:06:16.480 --> 00:06:19.340]   - So your sense is fundamentally people are good,
[00:06:19.340 --> 00:06:22.240]   but outliers-- - A trembling hand equilibrium
[00:06:22.240 --> 00:06:23.520]   is what we would call it.
[00:06:23.520 --> 00:06:25.440]   - Trembling hand equilibrium?
[00:06:25.440 --> 00:06:28.520]   - That the basic logic is for cooperation,
[00:06:28.520 --> 00:06:32.480]   which is mostly what we've seen, even between enemies.
[00:06:32.480 --> 00:06:35.040]   But every now and then someone does something crazy
[00:06:35.040 --> 00:06:36.840]   and you don't know how to react to it.
[00:06:36.840 --> 00:06:39.540]   And you can't always beat Hitler.
[00:06:39.540 --> 00:06:41.720]   Sometimes Hitler drags you down.
[00:06:41.720 --> 00:06:44.680]   - To push back, is it possible
[00:06:44.680 --> 00:06:49.680]   that the crazier the person, the less likely they are,
[00:06:49.680 --> 00:06:52.960]   and in a way where we're safe?
[00:06:52.960 --> 00:06:56.520]   Meaning like, this is the kind of proposition,
[00:06:56.520 --> 00:06:59.620]   I had the discussion with my dad as a physicist about this,
[00:06:59.620 --> 00:07:04.620]   where he thinks that, like if you have a graph,
[00:07:04.620 --> 00:07:08.240]   like evil people can't also be geniuses.
[00:07:08.240 --> 00:07:11.080]   So this is his defense why evil people
[00:07:11.080 --> 00:07:13.420]   will not get control of nuclear weapons,
[00:07:13.420 --> 00:07:15.040]   because to be truly evil.
[00:07:15.040 --> 00:07:19.680]   But evil meaning, sort of you can argue that,
[00:07:19.680 --> 00:07:22.440]   not even the evil of Hitler we're talking about,
[00:07:22.440 --> 00:07:24.480]   because Hitler had a kind of view of Germany
[00:07:24.480 --> 00:07:26.480]   and all those kinds of, there's like,
[00:07:26.480 --> 00:07:29.560]   he probably deluded himself and the people around him
[00:07:29.560 --> 00:07:31.360]   to think that he's actually doing good for the world,
[00:07:31.360 --> 00:07:33.200]   similar with Stalin and so on.
[00:07:33.200 --> 00:07:35.760]   By evil, I mean more like, almost like terrorists,
[00:07:35.760 --> 00:07:39.800]   to where they want to destroy themselves and the world.
[00:07:39.800 --> 00:07:41.800]   Like those people will never be able
[00:07:41.800 --> 00:07:44.800]   to be actually skilled enough to do,
[00:07:44.800 --> 00:07:47.880]   to deliver that kind of mass scale destruction.
[00:07:47.880 --> 00:07:52.720]   So the hope is that it's very unlikely
[00:07:52.720 --> 00:07:54.640]   that the kind of evil that would lead
[00:07:54.640 --> 00:07:57.720]   to extinctions of humans or mass destruction
[00:07:57.720 --> 00:08:00.960]   is so unlikely that we're able to last
[00:08:00.960 --> 00:08:03.680]   way longer than 700, 800 years.
[00:08:03.680 --> 00:08:06.040]   - I agree it's very unlikely.
[00:08:06.040 --> 00:08:08.180]   In that sense, I accept the argument.
[00:08:08.180 --> 00:08:10.520]   But that's why you need to let the clock tick.
[00:08:10.520 --> 00:08:13.200]   It's also the best argument for bureaucracy.
[00:08:13.200 --> 00:08:14.760]   To negotiate a bureaucracy,
[00:08:14.760 --> 00:08:17.000]   it actually selects against pure evil,
[00:08:17.000 --> 00:08:18.880]   because you need to build alliances.
[00:08:18.880 --> 00:08:21.000]   So bureaucracy in that regard is great, right?
[00:08:21.000 --> 00:08:23.320]   It keeps out the worst apples.
[00:08:23.320 --> 00:08:24.340]   But look, put it this way,
[00:08:24.340 --> 00:08:26.980]   could you imagine 35 years from now,
[00:08:26.980 --> 00:08:29.680]   the Osama bin Laden of the future
[00:08:29.680 --> 00:08:32.560]   has nukes or very bad bio weapons?
[00:08:32.560 --> 00:08:34.160]   It seems to me you can.
[00:08:34.160 --> 00:08:36.000]   And Osama was pretty evil.
[00:08:36.000 --> 00:08:38.720]   And actually even he failed, right?
[00:08:38.720 --> 00:08:41.000]   But nonetheless, that's what the 700
[00:08:41.000 --> 00:08:43.000]   or 800 years is there for.
[00:08:43.000 --> 00:08:44.960]   - And there might be destructive technologies
[00:08:44.960 --> 00:08:48.660]   that don't have such a high cost of production
[00:08:48.660 --> 00:08:50.760]   or such a high learning curve.
[00:08:50.760 --> 00:08:53.960]   Like cyber attacks or artificial intelligence,
[00:08:53.960 --> 00:08:55.600]   all those kinds of things.
[00:08:55.600 --> 00:08:56.440]   So yeah.
[00:08:56.440 --> 00:08:57.600]   - I mean, let me ask you a question.
[00:08:57.600 --> 00:09:00.080]   Let's say you could as an act of will,
[00:09:00.080 --> 00:09:01.680]   by spending a million dollars,
[00:09:01.680 --> 00:09:05.320]   obliterate any city on Earth and everyone in it dies.
[00:09:05.320 --> 00:09:06.420]   And you'll get caught.
[00:09:06.420 --> 00:09:08.480]   And you'll be sentenced to death.
[00:09:08.480 --> 00:09:11.000]   But you can make it happen just by willing it.
[00:09:11.000 --> 00:09:14.000]   How many months does it take before that happens?
[00:09:14.000 --> 00:09:18.040]   - So the obvious answer is like very soon.
[00:09:18.040 --> 00:09:19.440]   That's probably a good answer for that
[00:09:19.440 --> 00:09:21.480]   'cause you can consider how many millionaires there are,
[00:09:21.480 --> 00:09:22.760]   how many, you can look at that, right?
[00:09:22.760 --> 00:09:23.680]   - Right.
[00:09:23.680 --> 00:09:26.600]   - I have a sense that there's just people
[00:09:26.600 --> 00:09:28.620]   that have a million dollars.
[00:09:28.620 --> 00:09:31.420]   I mean, there's a certain amount,
[00:09:31.420 --> 00:09:33.640]   but have a million dollars,
[00:09:33.640 --> 00:09:36.840]   have other interests that will outweigh
[00:09:37.800 --> 00:09:42.080]   the interest of destroying the entire city.
[00:09:42.080 --> 00:09:43.520]   Like there's a particular,
[00:09:43.520 --> 00:09:46.680]   you know, like maybe that's a hope.
[00:09:46.680 --> 00:09:49.040]   - It's why we should be nice to the wealthy too, right?
[00:09:49.040 --> 00:09:50.360]   (laughing)
[00:09:50.360 --> 00:09:51.320]   - Yeah.
[00:09:51.320 --> 00:09:53.960]   Yeah, all that trash talking is Bill Gates.
[00:09:53.960 --> 00:09:57.120]   We should stop that 'cause that doesn't inspire
[00:09:57.120 --> 00:10:00.160]   the other future Bill Gates is to be nice to the world.
[00:10:00.160 --> 00:10:02.000]   It's true.
[00:10:02.000 --> 00:10:05.440]   But your sense is the cheaper it gets to destroy the world,
[00:10:05.440 --> 00:10:07.480]   the more likely it becomes.
[00:10:07.480 --> 00:10:08.960]   - Now when I say destroy the world,
[00:10:08.960 --> 00:10:09.780]   there's a trick in there.
[00:10:09.780 --> 00:10:12.760]   I don't think literally every human will die,
[00:10:12.760 --> 00:10:14.960]   but it would set back civilization
[00:10:14.960 --> 00:10:16.400]   by an extraordinary degree.
[00:10:16.400 --> 00:10:18.880]   It's then just hard to predict what comes next.
[00:10:18.880 --> 00:10:19.720]   - Yeah.
[00:10:19.720 --> 00:10:21.600]   - But a catastrophe where everyone dies,
[00:10:21.600 --> 00:10:24.120]   that probably has to be something more like an asteroid
[00:10:24.120 --> 00:10:26.120]   or supernova.
[00:10:26.120 --> 00:10:29.640]   And those are purely exogenous for the time being at least.
[00:10:29.640 --> 00:10:32.200]   - So I immigrated to this country.
[00:10:32.200 --> 00:10:35.360]   I was born in the Soviet Union in Russia
[00:10:35.360 --> 00:10:37.160]   and-- - Which one?
[00:10:37.160 --> 00:10:38.000]   - Which one?
[00:10:38.000 --> 00:10:39.640]   - Again, it's an important question.
[00:10:39.640 --> 00:10:41.480]   You were born in the Soviet Union, right?
[00:10:41.480 --> 00:10:43.360]   - Yes, I was born in the Soviet Union.
[00:10:43.360 --> 00:10:46.680]   The rest is details, but I grew up in Moscow, Russia.
[00:10:46.680 --> 00:10:47.520]   - Yeah.
[00:10:47.520 --> 00:10:49.920]   - But I came to this country
[00:10:49.920 --> 00:10:52.160]   and this country even back there,
[00:10:52.160 --> 00:10:55.840]   but it's always symbolized to me a place of opportunity
[00:10:55.840 --> 00:11:00.840]   where everybody could build the most incredible things,
[00:11:00.840 --> 00:11:03.400]   especially in the engineering side of things.
[00:11:03.400 --> 00:11:06.600]   Just invent and build and scale
[00:11:06.600 --> 00:11:08.760]   and have a huge impact on the world.
[00:11:08.760 --> 00:11:10.160]   And that's been to me,
[00:11:10.160 --> 00:11:14.120]   that's my version of the American ideal, the American dream.
[00:11:14.120 --> 00:11:19.140]   Do you think the American dream is still there?
[00:11:19.140 --> 00:11:23.560]   Do you think, what do you think of that notion in itself?
[00:11:23.560 --> 00:11:25.240]   Like from an economics perspective,
[00:11:25.240 --> 00:11:28.240]   from a human perspective, is it still alive?
[00:11:28.240 --> 00:11:30.080]   And how do you think about it?
[00:11:30.080 --> 00:11:30.920]   The American dream.
[00:11:30.920 --> 00:11:33.600]   - The American dream is mostly still there.
[00:11:33.600 --> 00:11:37.400]   If you look at which groups are the highest earners,
[00:11:37.400 --> 00:11:41.420]   it is individuals from India and individuals from Iran,
[00:11:41.420 --> 00:11:43.360]   which is a fairly new development.
[00:11:43.360 --> 00:11:46.020]   Great for them, not necessarily easy.
[00:11:46.020 --> 00:11:48.600]   Both you could call persons of color,
[00:11:48.600 --> 00:11:50.360]   may have faced discrimination,
[00:11:50.360 --> 00:11:53.840]   also on the grounds of religion, yet they've done it.
[00:11:53.840 --> 00:11:54.680]   That's amazing.
[00:11:54.680 --> 00:11:56.600]   It says great things about America.
[00:11:56.600 --> 00:11:58.660]   Now, if you look at native born Americans,
[00:11:58.660 --> 00:12:00.660]   the story's trickier.
[00:12:00.660 --> 00:12:03.720]   People think intergenerational mobility
[00:12:03.720 --> 00:12:05.340]   has declined a lot recently,
[00:12:05.340 --> 00:12:08.640]   but it has not for native born Americans.
[00:12:08.640 --> 00:12:11.920]   For about, I think 40 years, it's been fairly constant,
[00:12:11.920 --> 00:12:13.800]   which is sort of good,
[00:12:13.800 --> 00:12:17.000]   but compared to much earlier times,
[00:12:17.000 --> 00:12:19.440]   it was much higher in the past.
[00:12:19.440 --> 00:12:21.040]   I'm not sure we can replicate that,
[00:12:21.040 --> 00:12:23.540]   because look, go to the beginning of the 20th century,
[00:12:23.540 --> 00:12:25.740]   very few Americans finish high school,
[00:12:25.740 --> 00:12:27.920]   or even have much wealth.
[00:12:27.920 --> 00:12:29.280]   There's not much credentialism.
[00:12:29.280 --> 00:12:30.960]   There aren't that many credentials.
[00:12:30.960 --> 00:12:33.120]   So there's more upward mobility
[00:12:33.120 --> 00:12:35.760]   across the generations than today.
[00:12:35.760 --> 00:12:37.400]   And it's a good thing that we had it.
[00:12:37.400 --> 00:12:40.240]   I'm not sure we should blame the modern world
[00:12:40.240 --> 00:12:43.400]   for not being able to reproduce that.
[00:12:43.400 --> 00:12:46.840]   But look, the general issue of who gets into Harvard
[00:12:46.840 --> 00:12:49.440]   or Cornell, is there an injustice?
[00:12:49.440 --> 00:12:50.760]   Should we fix that?
[00:12:50.760 --> 00:12:52.880]   Is there too little opportunity for the bottom,
[00:12:52.880 --> 00:12:54.400]   say half of Americans?
[00:12:54.400 --> 00:12:56.440]   Absolutely, it's a disgrace
[00:12:56.440 --> 00:12:58.800]   how this country has evolved in that way.
[00:12:58.800 --> 00:13:02.040]   And in that sense, the American dream is clearly ailing,
[00:13:02.040 --> 00:13:04.520]   but it has had problems from the beginning
[00:13:04.520 --> 00:13:07.160]   for blacks, for women, for many other groups.
[00:13:07.160 --> 00:13:09.120]   - I mean, isn't that the whole challenge
[00:13:09.120 --> 00:13:12.120]   of opportunity and freedom is that it's hard
[00:13:12.120 --> 00:13:16.880]   and the difficulty of how hard it is to move up in society
[00:13:16.880 --> 00:13:20.340]   is unequal often, and that's the injustice of society.
[00:13:20.340 --> 00:13:23.180]   But the whole point of that freedom
[00:13:23.180 --> 00:13:25.740]   is that over time, it becomes better and better.
[00:13:25.740 --> 00:13:30.740]   You start to fix the leaks, the issues,
[00:13:30.740 --> 00:13:35.240]   and it keeps progressing in that kind of way.
[00:13:35.240 --> 00:13:37.420]   But ultimately, there's always the opportunity,
[00:13:37.420 --> 00:13:39.940]   even if it's harder, there's the opportunity
[00:13:39.940 --> 00:13:42.820]   to create something truly special, to move up,
[00:13:42.820 --> 00:13:47.640]   to be president, to be a leader in whatever the industry
[00:13:47.640 --> 00:13:49.100]   that you're passionate about.
[00:13:49.100 --> 00:13:51.540]   - We each have podcasts, right, in English.
[00:13:51.540 --> 00:13:55.180]   The value of joining that American English language
[00:13:55.180 --> 00:13:58.300]   network is much higher today than it was 30 years ago,
[00:13:58.300 --> 00:14:00.340]   mostly because of the internet.
[00:14:00.340 --> 00:14:03.900]   So that makes immigration returns themselves skewed.
[00:14:03.900 --> 00:14:07.220]   So going to the US, Canada, or the UK,
[00:14:07.220 --> 00:14:09.980]   I think has become much more valuable in relative terms
[00:14:09.980 --> 00:14:11.900]   than say going to France, which is still
[00:14:11.900 --> 00:14:13.900]   a pretty well-off, very nice country.
[00:14:13.900 --> 00:14:18.180]   If you had gone to France, your chance
[00:14:18.180 --> 00:14:21.780]   of having a globally known podcast would be much smaller.
[00:14:21.780 --> 00:14:23.700]   - Yeah, this is the interesting thing
[00:14:23.700 --> 00:14:27.700]   about how much intellectual influence
[00:14:27.700 --> 00:14:28.600]   the United States has.
[00:14:28.600 --> 00:14:31.960]   I don't know if it's connected to what we're discussing here,
[00:14:31.960 --> 00:14:35.020]   the freedom and opportunity of the American dream.
[00:14:35.020 --> 00:14:40.020]   Or does it make any sense to you that we have so much impact
[00:14:40.020 --> 00:14:44.140]   on the rest of the world in terms of ideas?
[00:14:44.140 --> 00:14:47.560]   Is it just simply because English
[00:14:47.560 --> 00:14:49.780]   is the primary language of the world,
[00:14:49.780 --> 00:14:52.140]   or is there something fundamental to the United States
[00:14:52.140 --> 00:14:55.140]   that drives the development of ideas?
[00:14:55.140 --> 00:14:59.820]   So it's almost like what's cool, what's entertaining,
[00:14:59.820 --> 00:15:04.820]   what's meme culture, the internet culture,
[00:15:04.820 --> 00:15:09.260]   the philosophers, the intellectuals, the podcasts,
[00:15:09.260 --> 00:15:13.340]   the movies, music, all that stuff, driving culture.
[00:15:13.340 --> 00:15:15.180]   - There's something above and beyond language
[00:15:15.180 --> 00:15:16.660]   in the United States.
[00:15:16.660 --> 00:15:19.180]   It's a sense of entertainment really mattering,
[00:15:19.180 --> 00:15:20.980]   how to connect with your audience,
[00:15:20.980 --> 00:15:23.740]   being direct and getting to the point,
[00:15:23.740 --> 00:15:27.340]   how humor is integrated even with science.
[00:15:27.340 --> 00:15:30.340]   That is pretty strongly represented here,
[00:15:30.340 --> 00:15:32.740]   much more so than on the European continent.
[00:15:32.740 --> 00:15:36.220]   Britain has its own version of this, which it does very well.
[00:15:36.220 --> 00:15:38.380]   And not surprisingly, they're hugely influential
[00:15:38.380 --> 00:15:41.980]   in music, comedy, most of the other areas you mentioned.
[00:15:41.980 --> 00:15:44.880]   Canada, yes, but their best talent tends to come here.
[00:15:44.880 --> 00:15:48.020]   But you could say it's like a broader North American thing
[00:15:48.020 --> 00:15:50.260]   and give them their fair share of credit.
[00:15:50.260 --> 00:15:52.220]   - What about science?
[00:15:52.220 --> 00:15:56.740]   There's a sense higher education is really strong,
[00:15:56.740 --> 00:15:58.640]   research is really strong in the United States,
[00:15:58.640 --> 00:16:01.940]   but it just feels like culturally speaking,
[00:16:01.940 --> 00:16:06.040]   when we zoom out, scientists aren't very cool here.
[00:16:06.040 --> 00:16:09.460]   Most people wouldn't be able to name
[00:16:09.460 --> 00:16:11.180]   basically a single scientist.
[00:16:11.180 --> 00:16:13.260]   Maybe they would say like, they would say what,
[00:16:13.260 --> 00:16:16.620]   like Einstein and Neil deGrasse Tyson maybe?
[00:16:16.620 --> 00:16:18.780]   And Neil deGrasse Tyson isn't exactly a scientist,
[00:16:18.780 --> 00:16:20.420]   he's a science communicator.
[00:16:20.420 --> 00:16:25.420]   So there's not the same kind of admiration of science
[00:16:25.420 --> 00:16:30.980]   and innovators as there is of like athletes
[00:16:30.980 --> 00:16:35.080]   or actors, actresses, musicians.
[00:16:35.080 --> 00:16:38.140]   - Well, you can become a celebrity scientist if you want to.
[00:16:38.140 --> 00:16:40.260]   It may or may not be best for science.
[00:16:40.260 --> 00:16:44.420]   And we have Spock from Star Trek, who is still a big deal.
[00:16:44.420 --> 00:16:45.860]   But look at it this way.
[00:16:45.860 --> 00:16:47.500]   Which country is most comfortable
[00:16:47.500 --> 00:16:50.980]   with inegalitarian rewards for scientists,
[00:16:50.980 --> 00:16:52.500]   whether it's fame or money?
[00:16:52.500 --> 00:16:53.500]   And I still think it's here.
[00:16:53.500 --> 00:16:55.460]   Some of that's just the tax rate.
[00:16:55.460 --> 00:16:57.820]   Some of it is a lot of America is set up
[00:16:57.820 --> 00:17:00.340]   for rich people to live really well.
[00:17:00.340 --> 00:17:02.820]   And again, that's going to attract a lot of top talent.
[00:17:02.820 --> 00:17:05.300]   And you ask like the two best vaccines.
[00:17:05.300 --> 00:17:08.100]   I know the Pfizer vaccine is sort of from Germany,
[00:17:08.100 --> 00:17:12.200]   sort of from Turkey, but it's nonetheless being distributed
[00:17:12.200 --> 00:17:14.200]   through the United States, Moderna,
[00:17:15.060 --> 00:17:17.980]   and ethnic Armenian immigrant through Lebanon,
[00:17:17.980 --> 00:17:21.460]   first to Canada, then down here to Boston, Cambridge area.
[00:17:21.460 --> 00:17:25.300]   Those are incredible vaccines and US nailed it.
[00:17:25.300 --> 00:17:28.900]   - Yeah, well, that's more almost like the,
[00:17:28.900 --> 00:17:30.060]   I don't know what you would call it,
[00:17:30.060 --> 00:17:32.820]   engineering the sort of scaling.
[00:17:32.820 --> 00:17:34.780]   That's what US is really good at,
[00:17:34.780 --> 00:17:36.220]   not just inventing of ideas,
[00:17:36.220 --> 00:17:38.340]   but taking an idea and actually building the thing
[00:17:38.340 --> 00:17:41.760]   and scaling it and being able to distribute it at scale.
[00:17:42.700 --> 00:17:45.100]   I think some people would attribute that
[00:17:45.100 --> 00:17:49.700]   to the general word of capitalism.
[00:17:49.700 --> 00:17:51.100]   I don't know if you would.
[00:17:51.100 --> 00:17:57.140]   What in your views are the pros and cons of capitalism
[00:17:57.140 --> 00:17:59.700]   as it's implemented in America?
[00:17:59.700 --> 00:18:01.180]   I don't know if you would say capitalism
[00:18:01.180 --> 00:18:04.780]   is really exist in America, but to the extent that it does.
[00:18:04.780 --> 00:18:08.340]   - People use the word capitalism in so many different ways.
[00:18:08.340 --> 00:18:09.780]   - What is capitalism?
[00:18:09.820 --> 00:18:13.500]   - The literal meaning is private ownership of capital goods,
[00:18:13.500 --> 00:18:15.940]   which I favor in most areas,
[00:18:15.940 --> 00:18:17.700]   but no, I don't think the private sector
[00:18:17.700 --> 00:18:20.840]   should own our F-16s or military assets.
[00:18:20.840 --> 00:18:24.240]   Government owned water utilities seem to work as well
[00:18:24.240 --> 00:18:26.820]   as privately owned water utilities.
[00:18:26.820 --> 00:18:29.620]   But with all those qualifications put to the side,
[00:18:29.620 --> 00:18:33.740]   business for the most part,
[00:18:33.740 --> 00:18:35.280]   innovates better than government.
[00:18:35.280 --> 00:18:38.120]   It is oriented toward consumer services.
[00:18:38.120 --> 00:18:41.260]   The biggest businesses tend to pay the highest wages.
[00:18:41.260 --> 00:18:43.660]   Business is great at getting things done.
[00:18:43.660 --> 00:18:46.500]   USA is fundamentally a nation of business
[00:18:46.500 --> 00:18:49.140]   and that makes us a nation of opportunity.
[00:18:49.140 --> 00:18:54.000]   So I am indeed mostly a fan, subject to numerous caveats.
[00:18:54.000 --> 00:18:56.300]   - What's the con?
[00:18:56.300 --> 00:19:01.300]   What are some negative downsides of capitalism in your view?
[00:19:01.300 --> 00:19:04.140]   Or some things that we should be concerned about
[00:19:04.140 --> 00:19:06.300]   maybe for long-term impacts of capitalism?
[00:19:07.740 --> 00:19:10.860]   - Again, capitalism takes a different form in each country.
[00:19:10.860 --> 00:19:12.940]   I would say in the United States,
[00:19:12.940 --> 00:19:16.120]   our weird blend of whatever you want to call it
[00:19:16.120 --> 00:19:20.160]   has had an enduring racial problem from the beginning,
[00:19:20.160 --> 00:19:24.040]   has been a force of taking away land from Native Americans
[00:19:24.040 --> 00:19:27.180]   and oppressing them pretty much from the beginning.
[00:19:27.180 --> 00:19:34.080]   It has done very well by immigrants for the most part.
[00:19:35.020 --> 00:19:38.980]   We revel in tributarian creative destruction more.
[00:19:38.980 --> 00:19:42.260]   So we don't just prop up national champions forever.
[00:19:42.260 --> 00:19:45.440]   And there's a precariousness to life for some people here
[00:19:45.440 --> 00:19:49.000]   that is less so, say in Germany or the Netherlands.
[00:19:49.000 --> 00:19:51.800]   We have weaker communities in some regards
[00:19:51.800 --> 00:19:54.460]   than say Northwestern Europe often would.
[00:19:54.460 --> 00:19:55.860]   That has pluses and minuses.
[00:19:55.860 --> 00:19:57.780]   I think it makes us more creative.
[00:19:57.780 --> 00:20:00.420]   It's a better country in which to be a weirdo
[00:20:00.420 --> 00:20:02.380]   than say Germany or Denmark.
[00:20:02.380 --> 00:20:05.500]   But there is truly, whether from the government
[00:20:05.500 --> 00:20:06.980]   or from your private community,
[00:20:06.980 --> 00:20:10.060]   there is less social security in some fundamental sense.
[00:20:10.060 --> 00:20:11.480]   - On the point of weirdo,
[00:20:11.480 --> 00:20:15.680]   that's kind of a beautiful little statement.
[00:20:15.680 --> 00:20:19.480]   What is that?
[00:20:19.480 --> 00:20:21.500]   I mean, that seems to be,
[00:20:21.500 --> 00:20:23.600]   you could think of a guy like Elon Musk
[00:20:23.600 --> 00:20:25.180]   and say that he's a weirdo.
[00:20:25.180 --> 00:20:27.180]   Is that the sense in which you're using the weirdo
[00:20:27.180 --> 00:20:30.420]   like outside of the norm, like breaking conventions?
[00:20:30.420 --> 00:20:31.260]   - Absolutely.
[00:20:31.260 --> 00:20:35.300]   And here that is either acceptable or even admired
[00:20:35.300 --> 00:20:36.660]   or to be a loner.
[00:20:36.660 --> 00:20:39.340]   And since so many people are outsiders
[00:20:39.340 --> 00:20:41.980]   and that we're all immigrants is selecting for people
[00:20:41.980 --> 00:20:43.220]   who left something behind,
[00:20:43.220 --> 00:20:45.480]   were willing to leave behind their families,
[00:20:45.480 --> 00:20:48.220]   were willing to undergo a certain brutality
[00:20:48.220 --> 00:20:53.060]   of switch in their lives, makes us a nation of weirdos.
[00:20:53.060 --> 00:20:55.020]   And weirdos are creative.
[00:20:55.020 --> 00:20:58.060]   And Denmark is not a nation of weirdos.
[00:20:58.060 --> 00:21:01.100]   It's a wonderful place, great for them.
[00:21:01.100 --> 00:21:02.580]   Ideally, you want part of the world
[00:21:02.580 --> 00:21:04.460]   to be full of weirdos and innovating.
[00:21:04.460 --> 00:21:06.500]   And the other part of the world to be a little
[00:21:06.500 --> 00:21:09.660]   kind of chicken shit, risk averse,
[00:21:09.660 --> 00:21:12.020]   and enjoy the benefits of the innovation.
[00:21:12.020 --> 00:21:13.980]   And to give people these smooth lives
[00:21:13.980 --> 00:21:16.580]   and six weeks off and free ride.
[00:21:16.580 --> 00:21:19.340]   And everyone's like, oh, American way versus European way,
[00:21:19.340 --> 00:21:21.500]   but basically they're compliments.
[00:21:21.500 --> 00:21:22.340]   - Yeah, that's fascinating.
[00:21:22.340 --> 00:21:25.660]   I used to have this conversation with my parents
[00:21:25.660 --> 00:21:27.460]   when I was growing up and just others
[00:21:27.460 --> 00:21:29.820]   from the immigrant kind of flow.
[00:21:29.820 --> 00:21:32.420]   And they used this term, especially in Russian,
[00:21:32.420 --> 00:21:36.500]   is to criticize something I was doing,
[00:21:36.500 --> 00:21:39.420]   they would suggest, normal people don't do this.
[00:21:39.420 --> 00:21:43.520]   And I used to be really offended by that.
[00:21:43.520 --> 00:21:49.500]   But as I got older, I realized that that's a compliment
[00:21:49.500 --> 00:21:55.680]   because in the same kind of, I would say,
[00:21:56.740 --> 00:21:59.860]   way that you're saying that is the American ideal.
[00:21:59.860 --> 00:22:02.860]   Because if you want to do anything special or interesting,
[00:22:02.860 --> 00:22:06.340]   you don't wanna be doing in one particular avenue
[00:22:06.340 --> 00:22:11.340]   what normal people do because that won't be interesting.
[00:22:11.340 --> 00:22:14.860]   - Russians, I think, fit in very well here
[00:22:14.860 --> 00:22:16.780]   because the ones who come are weirdos.
[00:22:16.780 --> 00:22:19.100]   And there's a very different Russian weirdo tradition,
[00:22:19.100 --> 00:22:21.940]   like Alyosha, Bridenbrothers Karamasov,
[00:22:21.940 --> 00:22:23.580]   or Perelman, the mathematician.
[00:22:23.620 --> 00:22:25.100]   They're weirdos.
[00:22:25.100 --> 00:22:27.900]   And they have their own different kind of status
[00:22:27.900 --> 00:22:29.940]   in Soviet Union, Russia, wherever.
[00:22:29.940 --> 00:22:32.420]   And when Russians come to America,
[00:22:32.420 --> 00:22:34.500]   they stay pretty Russian, but it seems to me,
[00:22:34.500 --> 00:22:38.060]   a week later, they've somehow adjusted.
[00:22:38.060 --> 00:22:40.900]   And the ways in which they might wanna be grumpier
[00:22:40.900 --> 00:22:42.300]   than Americans, not smile,
[00:22:42.300 --> 00:22:44.460]   think that people who smile are idiots.
[00:22:44.460 --> 00:22:45.300]   They can do that.
[00:22:45.300 --> 00:22:47.020]   No one takes that away from them.
[00:22:47.020 --> 00:22:50.740]   - What do you, on a tiny tangent,
[00:22:50.740 --> 00:22:52.260]   I'd love to hear if you have thoughts
[00:22:52.260 --> 00:22:57.060]   about Grisha Perelman turning down the Fields Medal.
[00:22:57.060 --> 00:22:59.180]   Is that something you admire?
[00:22:59.180 --> 00:23:01.380]   Does that make sense to you, that somebody,
[00:23:01.380 --> 00:23:05.260]   with the structure of Nobel Prizes, of these huge awards,
[00:23:05.260 --> 00:23:08.580]   of the reputations, the hierarchy of everyone saying,
[00:23:08.580 --> 00:23:10.620]   applauding how special you are,
[00:23:10.620 --> 00:23:13.380]   and here's a person who was doing
[00:23:13.380 --> 00:23:14.820]   one of the greatest accomplishments
[00:23:14.820 --> 00:23:16.140]   in the history of mathematics.
[00:23:16.140 --> 00:23:17.660]   It doesn't want the stupid prize.
[00:23:17.660 --> 00:23:19.380]   It doesn't want recognition.
[00:23:19.380 --> 00:23:20.580]   Doesn't wanna do interviews.
[00:23:20.580 --> 00:23:22.020]   It doesn't wanna be famous.
[00:23:22.020 --> 00:23:24.140]   What do you make of that?
[00:23:24.140 --> 00:23:24.980]   - It's great.
[00:23:24.980 --> 00:23:26.900]   Look, prizes are corrupting.
[00:23:26.900 --> 00:23:28.940]   After scientists win Nobel Prizes,
[00:23:28.940 --> 00:23:31.220]   they tend to become less productive.
[00:23:31.220 --> 00:23:32.980]   Now, statistically, it's hard to sort out
[00:23:32.980 --> 00:23:33.820]   the different effects.
[00:23:33.820 --> 00:23:35.180]   There's aggression toward the mean.
[00:23:35.180 --> 00:23:36.620]   Does the prize make you too busy?
[00:23:36.620 --> 00:23:38.060]   It's a little tricky, but--
[00:23:38.060 --> 00:23:39.900]   - There's not enough Nobel Prizes either
[00:23:39.900 --> 00:23:41.780]   to gather enough data.
[00:23:41.780 --> 00:23:45.260]   - Right, but I've known a lot of Nobel Prize winners,
[00:23:45.260 --> 00:23:47.780]   and it is my sense they become less productive.
[00:23:47.780 --> 00:23:49.580]   They repeat more of their older messages,
[00:23:49.580 --> 00:23:52.220]   which may be highly socially valuable,
[00:23:52.220 --> 00:23:54.940]   but if someone wants to turn their back on that
[00:23:54.940 --> 00:23:58.300]   and keep on working, which I assume is what he's doing,
[00:23:58.300 --> 00:23:59.380]   that's awesome.
[00:23:59.380 --> 00:24:01.300]   I mean, we should respect that.
[00:24:01.300 --> 00:24:02.980]   It's like he wins a bigger prize, right?
[00:24:02.980 --> 00:24:04.460]   Our extreme respect.
[00:24:04.460 --> 00:24:05.300]   - Yeah.
[00:24:05.300 --> 00:24:07.820]   Wow.
[00:24:07.820 --> 00:24:10.700]   Grisha, if you're listening, I need to talk to you soon.
[00:24:10.700 --> 00:24:11.540]   Okay.
[00:24:11.540 --> 00:24:12.460]   (both laughing)
[00:24:12.460 --> 00:24:15.140]   I've been trying to get ahold of him.
[00:24:15.140 --> 00:24:15.980]   Okay.
[00:24:15.980 --> 00:24:18.460]   Back to capitalism.
[00:24:18.460 --> 00:24:20.740]   I gotta ask you, just competition in general
[00:24:20.740 --> 00:24:22.580]   in this world of weirdos,
[00:24:22.580 --> 00:24:24.820]   is competition good for the world?
[00:24:24.820 --> 00:24:29.940]   This kind of seems to be one of the fundamental engines
[00:24:29.940 --> 00:24:30.900]   of capitalism, right?
[00:24:30.900 --> 00:24:32.860]   Do you see it as ultimately constructive
[00:24:32.860 --> 00:24:34.860]   or destructive for the world?
[00:24:34.860 --> 00:24:37.660]   - What really matters is how good your legal framework is.
[00:24:37.660 --> 00:24:40.180]   So competition within nature,
[00:24:40.180 --> 00:24:42.500]   for food leads to bloody conflict all the time.
[00:24:42.500 --> 00:24:46.300]   The animal world is quite unpleasant to say the least.
[00:24:46.300 --> 00:24:49.860]   If you have something like the rule of law
[00:24:49.860 --> 00:24:52.060]   and clearly defined property rights,
[00:24:52.060 --> 00:24:54.780]   which are within reason justly allocated,
[00:24:54.780 --> 00:24:58.940]   competition probably is gonna work very well,
[00:24:58.940 --> 00:25:01.460]   but it's not an unalloyed good thing at all.
[00:25:01.460 --> 00:25:02.900]   It can be highly destructive.
[00:25:02.900 --> 00:25:04.500]   Military competition, right?
[00:25:04.500 --> 00:25:07.740]   Which actually is itself sometimes good,
[00:25:07.740 --> 00:25:09.380]   but it's not good per se.
[00:25:09.380 --> 00:25:12.100]   - What aspects of life do you think we should protect
[00:25:12.100 --> 00:25:13.780]   from competition?
[00:25:13.780 --> 00:25:16.580]   So is there some, you said like the rule of law,
[00:25:16.580 --> 00:25:19.740]   is there some things we should keep away from competition?
[00:25:19.740 --> 00:25:22.140]   - Well, the fight for territory most of all, right?
[00:25:22.140 --> 00:25:24.180]   - So violence, anything that involves
[00:25:24.180 --> 00:25:25.740]   like actual physical violence.
[00:25:25.740 --> 00:25:26.820]   - Right, and it's not that I think
[00:25:26.820 --> 00:25:28.100]   the current borders are just.
[00:25:28.100 --> 00:25:31.820]   I mean, go talk to Hungarians, Romanians,
[00:25:31.820 --> 00:25:34.460]   Serbians, Bosnians, they'll talk your ear off.
[00:25:34.460 --> 00:25:36.700]   And some of them are probably right.
[00:25:36.700 --> 00:25:37.620]   But at the end of the day,
[00:25:37.620 --> 00:25:40.580]   we have some kind of international order
[00:25:40.660 --> 00:25:43.220]   and I would rather we more or less stick with it.
[00:25:43.220 --> 00:25:46.580]   If Catalonians wanna leave, they keep up with it,
[00:25:46.580 --> 00:25:47.500]   you know, let them go.
[00:25:47.500 --> 00:25:48.420]   But.
[00:25:48.420 --> 00:25:50.500]   - What about a space of like healthcare?
[00:25:50.500 --> 00:25:52.660]   This is where you get into a tension of like
[00:25:52.660 --> 00:25:55.420]   between capitalism and kind of more,
[00:25:55.420 --> 00:25:57.900]   I don't wanna use socialism,
[00:25:57.900 --> 00:26:02.380]   but those kinds of policies that are less free market.
[00:26:02.380 --> 00:26:03.420]   - I think in this country,
[00:26:03.420 --> 00:26:06.020]   healthcare should be much more competitive.
[00:26:06.020 --> 00:26:07.580]   So you go to hospitals, doctors,
[00:26:07.580 --> 00:26:10.020]   they don't treat you like a customer.
[00:26:10.020 --> 00:26:11.980]   They treat you like an idiot or like a child
[00:26:11.980 --> 00:26:14.540]   or someone with third party payment.
[00:26:14.540 --> 00:26:17.140]   And it's a pretty humiliating experience often.
[00:26:17.140 --> 00:26:22.860]   - Yeah, do you think a free market in general is possible,
[00:26:22.860 --> 00:26:25.140]   like a pure free market?
[00:26:25.140 --> 00:26:28.260]   And is that a good goal to strive for?
[00:26:28.260 --> 00:26:31.300]   - I don't think the term pure free market's well defined
[00:26:31.300 --> 00:26:33.020]   because you need a legal order.
[00:26:33.020 --> 00:26:35.420]   The legal order has to make decisions on like
[00:26:35.420 --> 00:26:38.540]   what is intellectual property, more important than ever.
[00:26:38.540 --> 00:26:40.540]   There's no benchmark that like represents
[00:26:40.540 --> 00:26:43.820]   the pure free market way of doing things.
[00:26:43.820 --> 00:26:45.460]   What will penalties be?
[00:26:45.460 --> 00:26:47.860]   How much do we put into law enforcement?
[00:26:47.860 --> 00:26:50.820]   No simple answers, but just saying free market
[00:26:50.820 --> 00:26:52.380]   doesn't pin down what you're gonna do
[00:26:52.380 --> 00:26:53.980]   on those all important questions.
[00:26:53.980 --> 00:26:56.980]   - So free market is an economics, I guess, idea.
[00:26:56.980 --> 00:27:00.260]   So there's no, it's not possible for free market
[00:27:00.260 --> 00:27:02.620]   is generate the rules that are like emergent,
[00:27:02.620 --> 00:27:03.820]   like self-governing.
[00:27:03.820 --> 00:27:05.340]   - It generates a lot of them, right?
[00:27:05.340 --> 00:27:08.140]   Through private norms, through trade associations,
[00:27:08.140 --> 00:27:13.140]   international trade is mostly done privately and by norms.
[00:27:13.140 --> 00:27:16.060]   So it's certainly possible, but at the end of the day,
[00:27:16.060 --> 00:27:20.020]   I think you need governments to draw very clear lines
[00:27:20.020 --> 00:27:24.260]   to prevent it from turning into mafia run systems.
[00:27:24.260 --> 00:27:26.460]   - You know, I've been hanging out
[00:27:26.460 --> 00:27:30.820]   with other group of weirdos lately, Michael Malice,
[00:27:30.820 --> 00:27:35.820]   who espouses to be an anarchist, anarchism,
[00:27:36.180 --> 00:27:39.540]   which is like, I think intellectually,
[00:27:39.540 --> 00:27:42.620]   just a fascinating set of ideas,
[00:27:42.620 --> 00:27:47.500]   where the, you know, taking free market to the full extreme
[00:27:47.500 --> 00:27:51.820]   of basically saying there should be no government,
[00:27:51.820 --> 00:27:54.300]   what is it?
[00:27:54.300 --> 00:27:57.780]   Oversight, I guess, and then everything should be fully,
[00:27:57.780 --> 00:28:00.380]   like all the agreements, all the collectives you form
[00:28:00.380 --> 00:28:05.380]   should be voluntary, not based on the geographic land
[00:28:05.700 --> 00:28:07.460]   you were born on and so on.
[00:28:07.460 --> 00:28:10.900]   Do you think that's just a giant mess?
[00:28:10.900 --> 00:28:12.500]   Like, do you think it's possible
[00:28:12.500 --> 00:28:15.180]   for an anarchist society to work where it's,
[00:28:15.180 --> 00:28:18.940]   you know, in a fully distributed way,
[00:28:18.940 --> 00:28:20.500]   people agree with each other,
[00:28:20.500 --> 00:28:22.700]   not just on financial transactions,
[00:28:22.700 --> 00:28:27.700]   but, you know, on their personal security,
[00:28:27.700 --> 00:28:31.940]   on sort of military type of stuff, on healthcare,
[00:28:31.940 --> 00:28:34.100]   on education, all those kinds of things.
[00:28:34.100 --> 00:28:35.700]   And where does it break down?
[00:28:35.700 --> 00:28:37.260]   - Well, I wouldn't press a button to say,
[00:28:37.260 --> 00:28:39.180]   get rid of our current constitution,
[00:28:39.180 --> 00:28:41.900]   which I view as pretty good and quite wise.
[00:28:41.900 --> 00:28:44.940]   But I think the deeper point is that all societies
[00:28:44.940 --> 00:28:47.220]   are in some regards anarchistic,
[00:28:47.220 --> 00:28:49.100]   and we should take the anarchist seriously.
[00:28:49.100 --> 00:28:53.580]   So globally, there's a kind of anarchy across borders,
[00:28:53.580 --> 00:28:57.260]   even within federalistic systems, they're typically complex.
[00:28:57.260 --> 00:29:00.340]   There's not a clear transitivity necessarily
[00:29:00.340 --> 00:29:03.140]   of who has the final say over what.
[00:29:03.140 --> 00:29:05.300]   Just the state vis-a-vis its people.
[00:29:05.300 --> 00:29:09.060]   There's not per se a final arbitrator in that regard.
[00:29:09.060 --> 00:29:13.060]   So you want a good anarchy rather than a bad anarchy.
[00:29:13.060 --> 00:29:15.980]   You want to squish your anarchy into the right corners.
[00:29:15.980 --> 00:29:18.420]   And I don't think there's a theoretical answer
[00:29:18.420 --> 00:29:19.380]   how to do it.
[00:29:19.380 --> 00:29:20.660]   But you start with a country,
[00:29:20.660 --> 00:29:23.540]   like, is it working well enough now?
[00:29:23.540 --> 00:29:25.740]   This country, you'd say mostly.
[00:29:25.740 --> 00:29:28.220]   You'd certainly want to make a lot of improvements.
[00:29:28.220 --> 00:29:29.540]   And that's why I don't want to press
[00:29:29.540 --> 00:29:31.660]   that get rid of the constitution button.
[00:29:31.660 --> 00:29:34.020]   But to just dump on the anarchist system is the point.
[00:29:34.020 --> 00:29:36.540]   Always try to learn from any opinion.
[00:29:36.540 --> 00:29:39.100]   You know, and what in it is true.
[00:29:39.100 --> 00:29:42.900]   - I'm just like marveling at the poetry
[00:29:42.900 --> 00:29:45.580]   of saying that we should squish our anarchy
[00:29:45.580 --> 00:29:46.740]   into the right corners.
[00:29:46.740 --> 00:29:48.500]   Love it.
[00:29:48.500 --> 00:29:52.020]   Okay, I gotta ask, I've been talking with,
[00:29:52.020 --> 00:29:55.940]   since we're doing a whirlwind introduction
[00:29:55.940 --> 00:29:58.100]   to all of economics, I've been talking
[00:29:58.100 --> 00:30:00.100]   to a few objectivists recently,
[00:30:00.100 --> 00:30:04.340]   and just, you know, Ayn Rand comes up as a person,
[00:30:04.340 --> 00:30:07.020]   as a philosopher, throughout many conversations,
[00:30:07.020 --> 00:30:09.180]   a lot of people really despise her.
[00:30:09.180 --> 00:30:11.140]   A lot of people really love her.
[00:30:11.140 --> 00:30:14.940]   It's always weird to me when somebody arouses a philosophy
[00:30:14.940 --> 00:30:16.740]   or a human being arouses that much emotion
[00:30:16.740 --> 00:30:17.740]   in either direction.
[00:30:17.740 --> 00:30:21.260]   Does she make, do you understand, first of all,
[00:30:21.260 --> 00:30:22.780]   that level of emotion?
[00:30:22.780 --> 00:30:25.060]   And what are your thoughts about Ayn Rand
[00:30:25.060 --> 00:30:26.700]   and her philosophy, objectivism?
[00:30:26.700 --> 00:30:30.180]   Is it useful at all to think about this kind
[00:30:30.180 --> 00:30:34.860]   of formulation of rational self-interest,
[00:30:34.860 --> 00:30:36.700]   if I could put it in those words,
[00:30:36.700 --> 00:30:41.700]   or I guess more negatively, the selfishness,
[00:30:41.700 --> 00:30:46.540]   or she would put, I guess, the virtue of selfishness.
[00:30:46.540 --> 00:30:49.300]   - Ayn Rand was a big influence on me growing up.
[00:30:49.300 --> 00:30:50.660]   The book that really mattered for me
[00:30:50.660 --> 00:30:53.340]   was "Capitalism and the Unknown Ideal."
[00:30:53.340 --> 00:30:56.220]   The notion that wealth creates opportunity
[00:30:56.220 --> 00:30:58.900]   and good lives and wealth is something we ought
[00:30:58.900 --> 00:31:01.380]   to valorize and give very high status.
[00:31:01.380 --> 00:31:02.880]   It's one of her key ideas.
[00:31:02.880 --> 00:31:04.500]   I think it's completely correct.
[00:31:04.500 --> 00:31:06.100]   I think she has the most profound
[00:31:06.100 --> 00:31:08.820]   and articulate statement of that idea.
[00:31:08.820 --> 00:31:11.860]   That said, as a philosopher, I disagree with her
[00:31:11.860 --> 00:31:14.500]   on most things, and I did, even as a boy,
[00:31:14.500 --> 00:31:15.700]   when I was reading her.
[00:31:15.700 --> 00:31:19.140]   I read Plato before Ayn Rand, and in a Socratic dialogue,
[00:31:19.140 --> 00:31:20.420]   there's all these different points of view
[00:31:20.420 --> 00:31:22.140]   being thrown around.
[00:31:22.140 --> 00:31:24.140]   And whomever it is you agree with,
[00:31:24.140 --> 00:31:27.060]   you understand the wisdom is in the coming together
[00:31:27.060 --> 00:31:28.740]   at the different points of view.
[00:31:28.740 --> 00:31:29.840]   And she doesn't have that.
[00:31:29.840 --> 00:31:33.040]   So altruism can be wonderful, in my view.
[00:31:33.040 --> 00:31:35.420]   Humans are not actually that rational.
[00:31:35.420 --> 00:31:38.180]   Self-interest is often poorly defined.
[00:31:38.180 --> 00:31:41.200]   To pound the table and say existence exists,
[00:31:41.200 --> 00:31:43.940]   I wouldn't say I disagree, but I'm not sure
[00:31:43.940 --> 00:31:46.500]   that it's a very meaningful statement.
[00:31:46.500 --> 00:31:49.260]   I think the secret to Ayn Rand is that she was Russian.
[00:31:49.260 --> 00:31:52.060]   I'd love to have her on my podcast if she were still alive.
[00:31:52.060 --> 00:31:53.860]   I'd only ask her about Russia.
[00:31:53.860 --> 00:31:56.140]   Which she mostly never talked about
[00:31:56.140 --> 00:31:57.920]   after writing "We the Living."
[00:31:57.920 --> 00:32:01.660]   And she is much more Russian than she seems at first.
[00:32:01.660 --> 00:32:04.420]   Even purging people from the objectivist circles.
[00:32:04.420 --> 00:32:07.420]   It's like how Russians, especially female Russians,
[00:32:07.420 --> 00:32:09.340]   so often purge their friends.
[00:32:09.340 --> 00:32:11.620]   It's weird, all the parallels.
[00:32:11.620 --> 00:32:13.540]   - So you're saying, so yes,
[00:32:13.540 --> 00:32:17.780]   assuming she's still not around,
[00:32:17.780 --> 00:32:20.660]   but if she is and she comes onto your podcast,
[00:32:20.660 --> 00:32:22.020]   can you dig into that a little bit?
[00:32:22.020 --> 00:32:27.020]   Do you mean her personal demons around
[00:32:27.020 --> 00:32:31.980]   the social and economic Russia of the time,
[00:32:31.980 --> 00:32:32.900]   when she escaped?
[00:32:32.900 --> 00:32:34.860]   - The traumas she suffered there.
[00:32:34.860 --> 00:32:37.500]   What she really likes in the music and literature and why.
[00:32:37.500 --> 00:32:38.580]   - Music and literature, huh?
[00:32:38.580 --> 00:32:40.260]   - And getting deeply into that,
[00:32:40.260 --> 00:32:42.900]   her view of relations between the sexes in Russia,
[00:32:42.900 --> 00:32:45.020]   how it differs from America,
[00:32:45.020 --> 00:32:48.300]   why she still carries through the old Russian vision
[00:32:48.300 --> 00:32:51.540]   in her fiction, this extreme sexual dimorphism,
[00:32:51.540 --> 00:32:53.860]   but with also very strong women.
[00:32:53.860 --> 00:32:57.860]   To me is a uniquely, at least Eastern European vision,
[00:32:57.860 --> 00:32:59.420]   mostly Russian, I would say.
[00:32:59.420 --> 00:33:03.180]   And that's in her, that's her actual real philosophy,
[00:33:03.180 --> 00:33:06.100]   not this table-bounding existence exists.
[00:33:06.100 --> 00:33:07.820]   And that's not talked about enough.
[00:33:07.820 --> 00:33:09.340]   She's a Russian philosopher.
[00:33:09.340 --> 00:33:12.420]   Or Soviet, whatever you wanna call it.
[00:33:12.420 --> 00:33:14.740]   - And if she wasn't so certain,
[00:33:14.740 --> 00:33:16.300]   she could have been a Dostoevsky,
[00:33:16.300 --> 00:33:19.100]   where it's not, that certainty is almost the thing
[00:33:19.100 --> 00:33:23.420]   that brings her the adoration of millions,
[00:33:23.420 --> 00:33:25.340]   but also the hatred of millions.
[00:33:25.340 --> 00:33:26.620]   - You became a cult figure
[00:33:26.620 --> 00:33:29.100]   in a somewhat Russian-like manner.
[00:33:29.100 --> 00:33:30.060]   - Yeah.
[00:33:30.060 --> 00:33:30.900]   - Yeah.
[00:33:30.900 --> 00:33:32.260]   - It is what it is.
[00:33:32.260 --> 00:33:34.660]   But I love the idea that, again,
[00:33:34.660 --> 00:33:37.180]   you're just dropping bombs that are poetic,
[00:33:37.180 --> 00:33:41.020]   that the wisdom is in the coming together of ideas.
[00:33:41.020 --> 00:33:42.380]   It's kind of interesting to think
[00:33:42.380 --> 00:33:46.860]   that no one human possesses wisdom.
[00:33:46.860 --> 00:33:49.460]   No one idea is the wisdom,
[00:33:49.460 --> 00:33:52.260]   that the coming together is the wisdom.
[00:33:52.260 --> 00:33:54.700]   - Like in my view, "Boswell's Life of Johnson,"
[00:33:54.700 --> 00:33:56.980]   18th century British biography.
[00:33:56.980 --> 00:33:59.980]   It's in essence a co-authored work, "Boswell and Johnson."
[00:33:59.980 --> 00:34:02.740]   It's one of the greatest philosophy books ever,
[00:34:02.740 --> 00:34:05.220]   though it is commonly regarded as a biography.
[00:34:05.220 --> 00:34:07.260]   John Stuart Mill, who in a sense
[00:34:07.260 --> 00:34:09.220]   was co-authoring with Harriet Taller,
[00:34:09.220 --> 00:34:12.180]   a better philosopher than is realized,
[00:34:12.180 --> 00:34:14.260]   though he's rated very, very highly.
[00:34:14.260 --> 00:34:16.060]   Plato/Socrates,
[00:34:16.060 --> 00:34:19.940]   a lot of the greatest works are in a kind of dialogue form.
[00:34:19.940 --> 00:34:23.140]   Goethe's "Faust" would be another example.
[00:34:23.140 --> 00:34:25.020]   It's very much a dialogue.
[00:34:25.020 --> 00:34:28.700]   And yes, it's drama, but it's also philosophy, Shakespeare.
[00:34:28.700 --> 00:34:30.540]   Maybe the wisest thinker of them all.
[00:34:30.540 --> 00:34:35.380]   - In your book, "Big Business," speaking of Ayn Rand,
[00:34:35.380 --> 00:34:38.940]   "Big Business, a Love Letter to an American Antihero,"
[00:34:38.940 --> 00:34:41.540]   you make the case for the benefit
[00:34:41.540 --> 00:34:43.580]   that large businesses bring to society.
[00:34:43.580 --> 00:34:45.220]   Can you explain?
[00:34:45.220 --> 00:34:46.940]   If you look at, say, the pandemic,
[00:34:46.940 --> 00:34:49.100]   which has been a catastrophic event, right,
[00:34:49.100 --> 00:34:52.660]   for many reasons, but who is it that saved us?
[00:34:52.660 --> 00:34:56.220]   So Amazon has done remarkably well.
[00:34:56.220 --> 00:34:59.460]   They upped their delivery game more or less overnight
[00:34:59.460 --> 00:35:00.980]   with very few hitches.
[00:35:00.980 --> 00:35:03.700]   I've ordered hundreds of Amazon packages,
[00:35:03.700 --> 00:35:07.420]   direct delivery food, whether it's DoorDash or Uber Eats
[00:35:07.420 --> 00:35:10.140]   or using Whole Foods through Amazon shipping.
[00:35:10.140 --> 00:35:12.260]   Again, it's gone remarkably well.
[00:35:12.260 --> 00:35:15.780]   Switching over our entire higher educational system,
[00:35:15.780 --> 00:35:18.140]   basically within two weeks, to Zoom.
[00:35:18.140 --> 00:35:19.100]   Zoom did it.
[00:35:19.100 --> 00:35:21.260]   I mean, I've had a Zoom outage,
[00:35:21.260 --> 00:35:24.820]   but their performance rate has been remarkably high.
[00:35:24.820 --> 00:35:29.420]   So if you just look at resources, competence, incentives,
[00:35:29.420 --> 00:35:31.020]   who's been the star performers?
[00:35:31.020 --> 00:35:33.260]   The NBA, even, just canceling the season
[00:35:33.260 --> 00:35:35.700]   as early as they did, sending a message like,
[00:35:35.700 --> 00:35:37.620]   "Hey, people, this is real,"
[00:35:37.620 --> 00:35:39.580]   and then pulling off the bubble.
[00:35:39.580 --> 00:35:41.700]   It's not a single found case of COVID
[00:35:41.700 --> 00:35:45.060]   and having all the testing set up in advance.
[00:35:45.060 --> 00:35:47.820]   Big business has done very well lately,
[00:35:47.820 --> 00:35:50.700]   and throughout the broader course of American history,
[00:35:50.700 --> 00:35:52.700]   in my view, has mostly been a hero.
[00:35:52.700 --> 00:35:56.220]   - Can we engage in a kind of therapy session?
[00:35:56.220 --> 00:36:03.540]   I'm often troubled by the negativity towards big business,
[00:36:03.540 --> 00:36:07.260]   and I wonder if you could help figure out
[00:36:07.260 --> 00:36:11.380]   how we remove that, or maybe first psychoanalyze it,
[00:36:11.380 --> 00:36:12.940]   and then how we remove it.
[00:36:12.940 --> 00:36:17.940]   It feels like, you know, once we've gotten wifi on flights,
[00:36:17.940 --> 00:36:24.860]   on airplane flights, people started complaining
[00:36:24.860 --> 00:36:27.220]   about how shady the connection is, right?
[00:36:27.220 --> 00:36:29.540]   They take it for granted immediately,
[00:36:29.540 --> 00:36:31.900]   and then start complaining about little details.
[00:36:31.900 --> 00:36:35.620]   Another example that's closer to,
[00:36:35.620 --> 00:36:39.860]   especially as aspiring entrepreneurs,
[00:36:39.860 --> 00:36:41.980]   closer to the things I'm thinking about
[00:36:41.980 --> 00:36:44.020]   is Jack Dorsey with Twitter.
[00:36:44.020 --> 00:36:46.940]   You know, to me, Twitter has enabled
[00:36:46.940 --> 00:36:50.620]   an incredible platform of communication,
[00:36:50.620 --> 00:36:53.420]   and yet the biggest thing that people talk about
[00:36:53.420 --> 00:36:57.380]   is not how incredible this platform is.
[00:36:57.380 --> 00:37:01.380]   They essentially use the platform to complain
[00:37:01.380 --> 00:37:03.800]   about the censorship of a few individuals,
[00:37:03.800 --> 00:37:06.260]   as opposed to how amazing it is.
[00:37:06.260 --> 00:37:08.140]   Now, you should also, you should talk about
[00:37:08.140 --> 00:37:11.040]   how shady the wifi is, and how censorship,
[00:37:11.040 --> 00:37:13.100]   or the removal of Donald Trump from the platform
[00:37:13.100 --> 00:37:16.340]   is a bad thing, but it feels like we don't talk about
[00:37:16.340 --> 00:37:19.660]   the positive impacts at scale of these technologies.
[00:37:19.660 --> 00:37:23.940]   Is there, can you explain why, and is there a way to fix it?
[00:37:23.940 --> 00:37:25.340]   - I don't know if we can fix it.
[00:37:25.340 --> 00:37:28.060]   I think we are beings of high neuroticism,
[00:37:28.060 --> 00:37:30.740]   for the most part, as a personality trait.
[00:37:30.740 --> 00:37:33.460]   Not everyone, but most people.
[00:37:33.460 --> 00:37:34.780]   And as a compliment to that,
[00:37:34.780 --> 00:37:36.660]   if someone says 10 nice things about you,
[00:37:36.660 --> 00:37:39.540]   and one insult, you're more bothered by the insult
[00:37:39.540 --> 00:37:41.180]   than you're pleased by the nice things,
[00:37:41.180 --> 00:37:43.860]   especially if the insult is somewhat true.
[00:37:43.860 --> 00:37:46.620]   So you have these media, these vehicles,
[00:37:46.620 --> 00:37:48.440]   Twitter is one you mentioned,
[00:37:48.440 --> 00:37:50.580]   where it's all kind of messages going back and forth,
[00:37:50.580 --> 00:37:53.580]   and you're really bugged by the messages you don't like.
[00:37:53.580 --> 00:37:56.200]   Most people are neurotic to begin with.
[00:37:56.200 --> 00:37:58.860]   It's not only taken out on big business, to be clear.
[00:37:58.860 --> 00:38:01.580]   So Congress catches a lot of grief,
[00:38:01.580 --> 00:38:03.940]   and some of it they deserve, yes.
[00:38:05.260 --> 00:38:07.380]   Religion is not attacked the same way,
[00:38:07.380 --> 00:38:09.840]   but religiosity is declining.
[00:38:09.840 --> 00:38:14.580]   If you poll people, the military still polls quite well,
[00:38:14.580 --> 00:38:17.040]   but people are very disillusioned with many things,
[00:38:17.040 --> 00:38:19.740]   and the Martin Gury thesis, that because of the internet,
[00:38:19.740 --> 00:38:21.580]   you just see more of things,
[00:38:21.580 --> 00:38:22.860]   and the more you see of something,
[00:38:22.860 --> 00:38:24.940]   whether it's good, bad, or in between,
[00:38:24.940 --> 00:38:26.780]   the more you will find to complain about,
[00:38:26.780 --> 00:38:28.980]   I suspect is the fundamental mechanism here.
[00:38:28.980 --> 00:38:32.660]   I mean, look at Clubhouse, right?
[00:38:32.660 --> 00:38:33.840]   To me, it's a great service,
[00:38:33.840 --> 00:38:35.780]   may or may not be my thing,
[00:38:35.780 --> 00:38:37.380]   but gives people this opportunity.
[00:38:37.380 --> 00:38:39.100]   No one makes you go on it.
[00:38:39.100 --> 00:38:40.380]   And all these media articles,
[00:38:40.380 --> 00:38:42.740]   like, oh, is Clubhouse gonna wreck things?
[00:38:42.740 --> 00:38:44.260]   Are they gonna break things?
[00:38:44.260 --> 00:38:45.740]   New York Times is complaining.
[00:38:45.740 --> 00:38:48.020]   Of course, it's their competitor as well.
[00:38:48.020 --> 00:38:49.780]   I'm like, give these people a chance.
[00:38:49.780 --> 00:38:50.800]   Talk it up.
[00:38:50.800 --> 00:38:52.700]   You may or may not like it.
[00:38:52.700 --> 00:38:55.580]   Let's praise the people who are getting something done.
[00:38:55.580 --> 00:38:57.780]   Very Ayn Randian point.
[00:38:57.780 --> 00:39:01.460]   - As an economic thinker, as a writer, as a podcaster,
[00:39:01.460 --> 00:39:03.700]   what do you think about Clubhouse
[00:39:03.700 --> 00:39:05.200]   as what do you think about...
[00:39:05.200 --> 00:39:09.780]   Okay, let me just throw my feeling about it.
[00:39:09.780 --> 00:39:11.260]   I used to use Discord,
[00:39:11.260 --> 00:39:13.640]   which is another service where people use voice.
[00:39:13.640 --> 00:39:16.420]   So the only thing you do is just hear each other.
[00:39:16.420 --> 00:39:17.280]   There's no face.
[00:39:17.280 --> 00:39:19.060]   You just see a little icon.
[00:39:19.060 --> 00:39:23.580]   That's the essential element of Clubhouse.
[00:39:23.580 --> 00:39:26.540]   And there's an intimacy to voice-only communication
[00:39:26.540 --> 00:39:27.660]   that's hard.
[00:39:27.660 --> 00:39:29.060]   That didn't make sense to me,
[00:39:29.060 --> 00:39:30.860]   but it was just what it is,
[00:39:30.860 --> 00:39:33.940]   which feels like something that won't last for some reason.
[00:39:33.940 --> 00:39:36.220]   Maybe it's the cynical view,
[00:39:36.220 --> 00:39:40.860]   but what's your sense about the intimacy
[00:39:40.860 --> 00:39:43.400]   of what's happening right now with Clubhouse?
[00:39:43.400 --> 00:39:45.540]   - I've greatly enjoyed what I've done,
[00:39:45.540 --> 00:39:47.340]   but I'm not sure it's for me in the long run
[00:39:47.340 --> 00:39:48.700]   for two reasons.
[00:39:48.700 --> 00:39:52.480]   First, if you compare it to doing a podcast,
[00:39:52.480 --> 00:39:55.180]   podcasting has greater reach than Clubhouse.
[00:39:55.180 --> 00:39:58.320]   So I would rather put time into my podcast.
[00:39:58.320 --> 00:40:02.800]   But then also my core asset, so to speak,
[00:40:02.800 --> 00:40:04.940]   is I'm a very fast reader.
[00:40:04.940 --> 00:40:08.200]   So audio per se is not necessarily to my advantage.
[00:40:08.200 --> 00:40:10.800]   I don't speak or listen faster than other people.
[00:40:10.800 --> 00:40:13.560]   In fact, I'm a slower listener 'cause I like 1.0,
[00:40:13.560 --> 00:40:14.400]   not 1.5x.
[00:40:14.400 --> 00:40:17.240]   So I should spend less time on audio
[00:40:17.240 --> 00:40:18.760]   and more time reading and writing.
[00:40:18.760 --> 00:40:21.800]   - Yeah, it's interesting because you mentioned podcasts
[00:40:21.800 --> 00:40:22.820]   and audio books.
[00:40:25.480 --> 00:40:30.040]   The podcasts are recorded and so I can skip things.
[00:40:30.040 --> 00:40:33.560]   Like I can skip commercials or I can skip parts
[00:40:33.560 --> 00:40:36.240]   where it's like, "Ugh, this part is boring."
[00:40:36.240 --> 00:40:40.120]   With live conversations, especially when,
[00:40:40.120 --> 00:40:42.560]   there's a magic to the fact when you have a lot of people
[00:40:42.560 --> 00:40:44.800]   participating in that conversation.
[00:40:44.800 --> 00:40:47.920]   But some people are like, "Ugh, this topic.
[00:40:47.920 --> 00:40:49.240]   "They're going into this thing."
[00:40:49.240 --> 00:40:51.040]   And you can't skip it or you can't fast forward.
[00:40:51.040 --> 00:40:54.000]   You can't go 1.5x or 2x.
[00:40:54.000 --> 00:40:55.200]   You can't speed it up.
[00:40:55.200 --> 00:40:58.520]   Nevertheless, there's a tension between that,
[00:40:58.520 --> 00:41:00.560]   so that's the productivity aspect,
[00:41:00.560 --> 00:41:04.600]   with the actual magic of live communication
[00:41:04.600 --> 00:41:05.580]   where anything can happen,
[00:41:05.580 --> 00:41:10.200]   where Elon Musk can ask the CEO of Robinhood, Vlad,
[00:41:10.200 --> 00:41:13.220]   about like, "Hey, somebody holding a gun to your head.
[00:41:13.220 --> 00:41:15.040]   "There's something shady going on."
[00:41:15.040 --> 00:41:16.080]   The magic of that.
[00:41:16.080 --> 00:41:18.280]   That's also my criticism of like,
[00:41:18.280 --> 00:41:20.900]   there's been a recent conversation with Bill Gates
[00:41:20.900 --> 00:41:22.940]   that he went on a platform,
[00:41:23.940 --> 00:41:27.900]   and had basically a regular interview on the platform
[00:41:27.900 --> 00:41:31.380]   without allowing the possibility of the magic of the chaos.
[00:41:31.380 --> 00:41:35.060]   So I'm not exactly sure.
[00:41:35.060 --> 00:41:36.980]   It's probably not the right platform for you
[00:41:36.980 --> 00:41:39.580]   and for many other people who are exceptionally productive
[00:41:39.580 --> 00:41:42.660]   in other places, but there's still nevertheless a magic
[00:41:42.660 --> 00:41:45.540]   to the chaos that can be created with live conversation
[00:41:45.540 --> 00:41:47.860]   that gives me pause.
[00:41:47.860 --> 00:41:50.140]   - Maybe what it's perfect for is the tribute.
[00:41:50.140 --> 00:41:52.520]   So they had an episode recently that I didn't hear,
[00:41:52.520 --> 00:41:54.040]   but I heard it was wonderful.
[00:41:54.040 --> 00:41:56.380]   It was anecdotes about Steve Jobs.
[00:41:56.380 --> 00:41:58.040]   That you can't do one-to-one, right?
[00:41:58.040 --> 00:41:59.460]   And you don't want control.
[00:41:59.460 --> 00:42:02.020]   You want different people appearing and stepping up
[00:42:02.020 --> 00:42:03.780]   and saying their bit.
[00:42:03.780 --> 00:42:08.780]   And Clubhouse is 110% perfect for that, the tribute.
[00:42:08.780 --> 00:42:10.500]   - I love that, the tribute.
[00:42:10.500 --> 00:42:12.580]   But there's also the possibility,
[00:42:12.580 --> 00:42:15.740]   I think there was a time when somebody arranged
[00:42:15.740 --> 00:42:18.900]   a conversation with Steve Jobs and Bill Gates on stage.
[00:42:18.900 --> 00:42:20.940]   I remember that happened a long time ago.
[00:42:21.880 --> 00:42:24.580]   And it was very formal.
[00:42:24.580 --> 00:42:27.920]   It could have probably gone better,
[00:42:27.920 --> 00:42:30.080]   but it was still magical to have these people
[00:42:30.080 --> 00:42:32.220]   that obviously had a bunch of tension
[00:42:32.220 --> 00:42:33.420]   throughout their history.
[00:42:33.420 --> 00:42:39.480]   It's so frictionless to have two major figures
[00:42:39.480 --> 00:42:42.600]   in world history just jump on a Clubhouse stage.
[00:42:42.600 --> 00:42:43.760]   - Putin and Elon Musk.
[00:42:43.760 --> 00:42:45.680]   - Putin and Elon Musk. - See what happens.
[00:42:45.680 --> 00:42:47.200]   - And that's exactly it.
[00:42:47.200 --> 00:42:48.800]   So there's a language barrier there.
[00:42:48.800 --> 00:42:52.880]   But there's also the problem that in particular,
[00:42:52.880 --> 00:42:55.920]   it's like Biden would have a similar problem.
[00:42:55.920 --> 00:42:58.360]   It's like, they're just not into new technology.
[00:42:58.360 --> 00:43:01.620]   So it's very hard to catch the Kremlin up to,
[00:43:01.620 --> 00:43:02.900]   first of all, Twitter.
[00:43:02.900 --> 00:43:07.300]   But to catch them up to Clubhouse, you have to have the,
[00:43:07.300 --> 00:43:09.260]   Elon Musk has a sense of the internet, the humor,
[00:43:09.260 --> 00:43:10.900]   the memes and all that kind of stuff
[00:43:10.900 --> 00:43:14.920]   that you have to have in order to use a new app
[00:43:14.920 --> 00:43:16.940]   and figure out the timing, the beat,
[00:43:16.940 --> 00:43:18.760]   what is this thing about?
[00:43:18.760 --> 00:43:20.580]   So that's the challenge there.
[00:43:20.580 --> 00:43:21.860]   But that's exactly it.
[00:43:21.860 --> 00:43:26.860]   That magic of have two big personalities just show up.
[00:43:26.860 --> 00:43:30.660]   And I wonder if it's just a temporary thing
[00:43:30.660 --> 00:43:32.340]   that we're going through with the pandemic
[00:43:32.340 --> 00:43:34.660]   where people are just lonely
[00:43:34.660 --> 00:43:37.500]   and they're seeking for that human connection
[00:43:37.500 --> 00:43:40.540]   that we usually get elsewhere through our work.
[00:43:40.540 --> 00:43:42.620]   - But they'll stay lonely, in my opinion.
[00:43:42.620 --> 00:43:44.560]   - You think so? - I do.
[00:43:44.560 --> 00:43:48.140]   So it is a pandemic thing, but I think it will persist.
[00:43:48.140 --> 00:43:49.880]   And the idea of wanting to be connected
[00:43:49.880 --> 00:43:53.480]   to more of the world, Clubhouse will still offer that.
[00:43:53.480 --> 00:43:56.120]   And all the mental health issues out there,
[00:43:56.120 --> 00:43:58.680]   a lot of people have broken ties
[00:43:58.680 --> 00:44:00.880]   and they will still be lonely post-vaccines.
[00:44:00.880 --> 00:44:05.840]   - Yeah, from an artificial intelligence perspective,
[00:44:05.840 --> 00:44:11.460]   have a sense that there is a deep loneliness in the world,
[00:44:11.460 --> 00:44:13.120]   that all of us are really lonely.
[00:44:13.120 --> 00:44:14.420]   Like we don't even acknowledge it.
[00:44:14.420 --> 00:44:16.640]   Even people in happy relationships.
[00:44:16.640 --> 00:44:19.100]   It feels like there's like an iceberg of loneliness
[00:44:19.100 --> 00:44:22.660]   in all of us, like seeking to be understood,
[00:44:22.660 --> 00:44:24.120]   like deeply understood.
[00:44:24.120 --> 00:44:26.900]   Understanding us, like having somebody
[00:44:26.900 --> 00:44:29.500]   with whom you can have a deep interaction enough
[00:44:29.500 --> 00:44:34.060]   to where they can help you to understand yourself
[00:44:34.060 --> 00:44:36.440]   and they also understand you.
[00:44:36.440 --> 00:44:38.780]   Like I have a sense that artificial intelligence systems
[00:44:38.780 --> 00:44:40.660]   can provide that as well,
[00:44:40.660 --> 00:44:44.400]   but humans, I think, crave that from other humans
[00:44:44.400 --> 00:44:46.420]   in ways that we perhaps don't acknowledge.
[00:44:46.420 --> 00:44:48.220]   And I have a hope that technology
[00:44:48.220 --> 00:44:49.460]   will enable that more and more.
[00:44:49.460 --> 00:44:52.220]   Like Clubhouse is an example that allows that.
[00:44:52.220 --> 00:44:54.900]   - Are touring bots gonna out-compete Clubhouse?
[00:44:54.900 --> 00:44:57.540]   Like why not sort of program your own session?
[00:44:57.540 --> 00:44:59.860]   You'll just talk into your device
[00:44:59.860 --> 00:45:02.260]   and say, "Here's the kind of conversation I want."
[00:45:02.260 --> 00:45:04.540]   And it will create the characters for you.
[00:45:04.540 --> 00:45:07.060]   And it may not be as good as Elon and Vladimir Putin,
[00:45:07.060 --> 00:45:09.580]   but it'll be better than ordinary Clubhouse.
[00:45:09.580 --> 00:45:11.620]   - Yeah, and one of the things that's missing,
[00:45:11.620 --> 00:45:14.900]   it's not just conversation, it's memory.
[00:45:14.900 --> 00:45:17.860]   So long-term memory is what current AI systems don't have,
[00:45:17.860 --> 00:45:21.180]   is sharing an experience together.
[00:45:21.180 --> 00:45:23.980]   Forget the words, it's like sharing the highs
[00:45:23.980 --> 00:45:26.260]   and the lows of life together
[00:45:26.260 --> 00:45:29.420]   and the systems around us remembering that,
[00:45:29.420 --> 00:45:31.260]   remembering we've been through that.
[00:45:31.260 --> 00:45:32.860]   Like that's the thing that creates
[00:45:32.860 --> 00:45:35.420]   really close relationships, is going through some shit.
[00:45:35.420 --> 00:45:37.620]   Like go struggle.
[00:45:37.620 --> 00:45:39.420]   If you've survived together,
[00:45:39.420 --> 00:45:40.980]   there's something really difficult
[00:45:40.980 --> 00:45:42.780]   that bonds you with other humans.
[00:45:42.780 --> 00:45:44.340]   - And this is related to immigration
[00:45:44.340 --> 00:45:45.500]   and the American dream.
[00:45:45.500 --> 00:45:47.340]   - In what way?
[00:45:47.340 --> 00:45:48.940]   - The people who have come to this country,
[00:45:48.940 --> 00:45:51.860]   however weird and different they may be,
[00:45:51.860 --> 00:45:53.780]   they or their ancestors at some point
[00:45:53.780 --> 00:45:55.380]   probably have shared this thing.
[00:45:55.380 --> 00:46:00.100]   Right, US is not gonna split up.
[00:46:00.100 --> 00:46:02.380]   It may get more screwed up as a country,
[00:46:02.380 --> 00:46:05.500]   but Texas and California are not gonna break off.
[00:46:05.500 --> 00:46:07.260]   I mean, they're big enough where they could do it,
[00:46:07.260 --> 00:46:08.900]   but it's just never gonna happen.
[00:46:08.900 --> 00:46:10.380]   - We've been through too much together.
[00:46:10.380 --> 00:46:11.220]   - Yeah.
[00:46:11.220 --> 00:46:12.620]   (both laughing)
[00:46:12.620 --> 00:46:14.100]   - Ah, that's a hopeful message.
[00:46:14.100 --> 00:46:17.060]   Do you think, some people have talked to Eric Weinstein,
[00:46:17.060 --> 00:46:18.660]   you've talked to Eric Weinstein.
[00:46:18.660 --> 00:46:21.980]   He has a sense that growth,
[00:46:21.980 --> 00:46:26.260]   the entirety of the American system
[00:46:26.260 --> 00:46:29.260]   is based on the assumption that we're gonna grow forever,
[00:46:29.260 --> 00:46:31.020]   the economy's gonna grow forever.
[00:46:31.020 --> 00:46:35.860]   Do you think economic growth will continue indefinitely,
[00:46:35.860 --> 00:46:37.220]   or will we stagnate?
[00:46:37.220 --> 00:46:42.020]   - I've long been in agreement with Eric, Peter Thiel,
[00:46:42.020 --> 00:46:46.300]   Robert Gordon and others that growth has slowed down.
[00:46:46.300 --> 00:46:47.460]   I argue that in my book,
[00:46:47.460 --> 00:46:50.660]   "The Great Stagnation," appropriately titled.
[00:46:50.660 --> 00:46:53.140]   But the last two years, I've become much more optimistic.
[00:46:53.140 --> 00:46:54.900]   I've seen a lot of breakthroughs
[00:46:54.900 --> 00:46:57.780]   in green energy and battery technology.
[00:46:57.780 --> 00:47:01.860]   mRNA vaccines and medicine is a big deal already.
[00:47:01.860 --> 00:47:03.340]   It will repair our GDP
[00:47:03.340 --> 00:47:06.140]   and save millions of lives around the world.
[00:47:06.140 --> 00:47:09.060]   There's an anti-malaria vaccine
[00:47:09.060 --> 00:47:12.100]   that's now in stage three trial, it probably works.
[00:47:12.100 --> 00:47:14.380]   CRISPR to defeat sickle cell anemia.
[00:47:14.380 --> 00:47:18.340]   Just space, area after area after area,
[00:47:18.340 --> 00:47:20.660]   there's suddenly this surge of breakthroughs.
[00:47:20.660 --> 00:47:23.860]   I would say many of them rooted in superior computation
[00:47:23.860 --> 00:47:25.420]   and ultimately, Moore's law
[00:47:25.420 --> 00:47:28.380]   and access to those computational abilities.
[00:47:28.380 --> 00:47:30.380]   So I'm much more optimistic than say,
[00:47:30.380 --> 00:47:32.060]   the last time I spoke to Eric.
[00:47:32.060 --> 00:47:32.900]   (Lex laughs)
[00:47:32.900 --> 00:47:34.700]   He moves all the time in his views.
[00:47:34.700 --> 00:47:35.980]   I don't know where he's at now.
[00:47:35.980 --> 00:47:38.940]   - He's not at, he hasn't gained, that's really interesting.
[00:47:38.940 --> 00:47:43.780]   So your little drop of optimism comes from,
[00:47:43.780 --> 00:47:46.180]   like there might be a fundamental shift
[00:47:46.180 --> 00:47:49.620]   in the kind of things that computation has unlocked for us
[00:47:49.620 --> 00:47:52.900]   in terms of, like it could be a wellspring of innovation
[00:47:52.900 --> 00:47:55.540]   that enables growth for a long time to come.
[00:47:55.540 --> 00:47:59.420]   Like Eric has not quite connected
[00:47:59.420 --> 00:48:01.620]   to the computation aspect yet
[00:48:01.620 --> 00:48:04.340]   to where it could be a wellspring of innovation.
[00:48:04.340 --> 00:48:06.180]   - But you're very close to it in your own work.
[00:48:06.180 --> 00:48:07.760]   I don't have to tell you that.
[00:48:07.760 --> 00:48:10.020]   The work you're doing would not have been possible
[00:48:10.020 --> 00:48:11.300]   not very long ago.
[00:48:11.300 --> 00:48:13.860]   - But the question is, how much does that work enable
[00:48:13.860 --> 00:48:16.180]   continued growth for decades to come?
[00:48:16.180 --> 00:48:17.640]   - For all their problems,
[00:48:17.640 --> 00:48:20.700]   some version of driverless vehicles will be a thing.
[00:48:20.700 --> 00:48:23.380]   I'm not sure when, you know much better than I do.
[00:48:23.380 --> 00:48:26.700]   Maybe only partially, but that too will be a big deal.
[00:48:26.700 --> 00:48:28.220]   - Well, one of the open questions
[00:48:28.220 --> 00:48:32.940]   that sort of the Peter Thiel School area of ideas
[00:48:32.940 --> 00:48:36.180]   is how much can be converted to technology?
[00:48:36.180 --> 00:48:39.800]   How many parts of our lives can technology integrate
[00:48:39.800 --> 00:48:40.720]   and then innovate?
[00:48:40.720 --> 00:48:43.460]   Like, can it replace healthcare?
[00:48:43.460 --> 00:48:46.440]   Can it replace the legal system?
[00:48:46.440 --> 00:48:47.740]   Can it replace government?
[00:48:47.740 --> 00:48:52.740]   Not replace, but like, you know, make it digital
[00:48:52.740 --> 00:48:57.480]   and thereby enable computation to improve it, right?
[00:48:57.480 --> 00:48:58.820]   That's the open question
[00:48:58.820 --> 00:49:00.740]   because many aspects of our lives
[00:49:00.740 --> 00:49:05.000]   are still not really that digitized.
[00:49:06.020 --> 00:49:08.580]   - There was a New York Times symposium in April,
[00:49:08.580 --> 00:49:09.740]   which is not long ago.
[00:49:09.740 --> 00:49:11.940]   And they asked the so-called experts,
[00:49:11.940 --> 00:49:13.620]   when are we gonna get vaccines?
[00:49:13.620 --> 00:49:17.680]   And the most optimistic answer was in four years.
[00:49:17.680 --> 00:49:21.260]   And obviously we beat that by a long mile.
[00:49:21.260 --> 00:49:23.140]   So I think people still haven't woken up.
[00:49:23.140 --> 00:49:25.000]   You mentioned my tiny drop of optimism,
[00:49:25.000 --> 00:49:27.860]   but it's a big drop of optimism.
[00:49:27.860 --> 00:49:28.900]   - Is it a waterfall yet?
[00:49:28.900 --> 00:49:30.700]   I mean, is it just-
[00:49:30.700 --> 00:49:32.340]   - Well, here's my pessimism.
[00:49:32.340 --> 00:49:34.420]   Whenever there are major new technologies,
[00:49:34.420 --> 00:49:36.300]   they also tend to be used for violence,
[00:49:36.300 --> 00:49:37.660]   directly or indirectly.
[00:49:37.660 --> 00:49:39.940]   Radio, Hitler, not that he hit people
[00:49:39.940 --> 00:49:41.060]   over the head with radios,
[00:49:41.060 --> 00:49:44.660]   but it enabled the rise of various dictators.
[00:49:44.660 --> 00:49:48.580]   So the new technologies now, whatever exactly they may be,
[00:49:48.580 --> 00:49:50.460]   they're gonna cause a lot of trouble.
[00:49:50.460 --> 00:49:51.620]   And that's my pessimism,
[00:49:51.620 --> 00:49:54.700]   not that I think they're all gonna slow to a trickle.
[00:49:54.700 --> 00:49:56.900]   - When was the stagnation book?
[00:49:56.900 --> 00:49:57.820]   - 2011.
[00:49:57.820 --> 00:49:58.700]   - 2011.
[00:49:58.700 --> 00:49:59.860]   - Yes.
[00:49:59.860 --> 00:50:03.020]   It was the first of the stagnation books, in fact.
[00:50:03.020 --> 00:50:05.140]   - It's very interesting.
[00:50:05.140 --> 00:50:07.660]   - But even then I said, this is temporary.
[00:50:07.660 --> 00:50:09.580]   And I was predicting it would be gone
[00:50:09.580 --> 00:50:11.220]   in about 20 years time.
[00:50:11.220 --> 00:50:14.500]   I'm not sure that's exactly the right prediction,
[00:50:14.500 --> 00:50:17.800]   like 2030, but I think we're actually gonna beat that.
[00:50:17.800 --> 00:50:20.980]   - So you think United States might still be
[00:50:20.980 --> 00:50:22.980]   on top of the world for the rest of the century,
[00:50:22.980 --> 00:50:27.400]   in terms of its economic growth, impact on the world,
[00:50:27.400 --> 00:50:29.940]   scientific innovation, all those kinds of things.
[00:50:29.940 --> 00:50:31.460]   - That's too long to predict,
[00:50:31.460 --> 00:50:33.820]   but I'm bullish on America in general.
[00:50:33.820 --> 00:50:35.060]   (Lex laughing)
[00:50:35.060 --> 00:50:35.900]   - Got it.
[00:50:35.900 --> 00:50:38.900]   Speaking of being bullish on America,
[00:50:38.900 --> 00:50:40.820]   the opposite of that is,
[00:50:40.820 --> 00:50:44.460]   we talked about capitalism,
[00:50:44.460 --> 00:50:47.060]   we talked about Ayn Rand and her Russian roots.
[00:50:47.060 --> 00:50:51.220]   What do you think about communism?
[00:50:51.220 --> 00:50:52.760]   Why doesn't it work?
[00:50:52.760 --> 00:50:58.220]   What is the implementation?
[00:50:58.220 --> 00:51:01.580]   Is there anything about its ideas that you find compelling,
[00:51:01.580 --> 00:51:06.260]   or is it just a fundamentally flawed system?
[00:51:06.260 --> 00:51:08.060]   - Well, communism is like capitalism.
[00:51:08.060 --> 00:51:10.780]   The words mean many things to different people.
[00:51:10.780 --> 00:51:12.880]   You could argue my life as a tenured professor
[00:51:12.880 --> 00:51:15.340]   comes closer to communism than anything
[00:51:15.340 --> 00:51:16.500]   the human race has seen,
[00:51:16.500 --> 00:51:19.180]   and I would argue it works pretty well.
[00:51:19.180 --> 00:51:21.700]   But look, if you mean the Soviet Union,
[00:51:21.700 --> 00:51:24.900]   it devolved pretty quickly to a kind of
[00:51:24.900 --> 00:51:29.020]   decentralized set of incentives that were destructive
[00:51:29.020 --> 00:51:30.600]   rather than value maximizing.
[00:51:30.600 --> 00:51:34.260]   It wasn't even central planning, much less communism.
[00:51:34.260 --> 00:51:37.100]   So Paul Craig Roberts and Polanyi were correct
[00:51:37.100 --> 00:51:39.640]   in their descriptions of the Soviet system.
[00:51:39.640 --> 00:51:41.560]   Think of it as weird mixes of barter
[00:51:41.560 --> 00:51:44.060]   and malfunctioning incentives,
[00:51:44.060 --> 00:51:47.060]   and being very good at a whole bunch of things,
[00:51:47.060 --> 00:51:50.180]   but in terms of progress, innovation, and consumer goods,
[00:51:50.180 --> 00:51:52.220]   it really being quite a failure.
[00:51:54.460 --> 00:51:56.380]   And now, I wouldn't call that communism,
[00:51:56.380 --> 00:52:00.420]   but that's what I think of the system the Soviets had,
[00:52:00.420 --> 00:52:04.840]   and it required an ever-increasing pile of lies
[00:52:04.840 --> 00:52:07.740]   that both alienated people but created an elite
[00:52:07.740 --> 00:52:08.940]   that by the end of the thing
[00:52:08.940 --> 00:52:12.300]   no longer believed in the system itself,
[00:52:12.300 --> 00:52:15.400]   or even thought they were doing better by being crooks
[00:52:15.400 --> 00:52:16.980]   than by just, say, moving to Switzerland
[00:52:16.980 --> 00:52:18.740]   and being an upper-middle-class individual.
[00:52:18.740 --> 00:52:20.740]   Like, you would have a higher standard of living
[00:52:20.740 --> 00:52:22.260]   by Gorbachev's time.
[00:52:22.260 --> 00:52:24.020]   Not Gorbachev, but if you're number 30
[00:52:24.020 --> 00:52:25.980]   in the hierarchy, you're better off
[00:52:25.980 --> 00:52:27.900]   as a middle-class person in Switzerland.
[00:52:27.900 --> 00:52:31.180]   And that, of course, did not prove sustainable.
[00:52:31.180 --> 00:52:33.580]   - And so it's, what is it, a momentum, a bureaucracy,
[00:52:33.580 --> 00:52:34.940]   or something like that, it just builds up
[00:52:34.940 --> 00:52:37.700]   where you lose control of the original vision,
[00:52:37.700 --> 00:52:39.160]   and that naturally happens.
[00:52:39.160 --> 00:52:40.180]   It's just people--
[00:52:40.180 --> 00:52:41.980]   - And you can't use normal profit and loss
[00:52:41.980 --> 00:52:44.420]   in price incentives, so you get all prices,
[00:52:44.420 --> 00:52:46.760]   or most prices, set too low, right?
[00:52:46.760 --> 00:52:49.580]   Shortages everywhere, people trade favors.
[00:52:49.580 --> 00:52:52.080]   You have this culture of bartered bribes,
[00:52:52.080 --> 00:52:55.380]   sexual favors, or family friends,
[00:52:55.380 --> 00:52:57.060]   and you get more and more of that,
[00:52:57.060 --> 00:52:58.900]   and you, over time, lose more and more
[00:52:58.900 --> 00:53:02.380]   of the information and the prices and quantities
[00:53:02.380 --> 00:53:04.100]   and practices and norms you had,
[00:53:04.100 --> 00:53:06.900]   and that slowly decays, and then by the end,
[00:53:06.900 --> 00:53:08.200]   no one is believing in it.
[00:53:08.200 --> 00:53:10.620]   That would be my take, but again,
[00:53:10.620 --> 00:53:12.080]   you're the expert here.
[00:53:12.080 --> 00:53:14.400]   - The Russian scholar?
[00:53:14.400 --> 00:53:19.580]   Well, I'm perhaps no more an expert than Ayn Rand.
[00:53:19.580 --> 00:53:24.580]   It's more personal than it is scholarly or historic.
[00:53:24.580 --> 00:53:27.640]   So Stalin held power for 30 years.
[00:53:27.640 --> 00:53:34.040]   Vladimir Putin has held power for 21 years,
[00:53:34.040 --> 00:53:36.840]   where you could argue he took a little break.
[00:53:36.840 --> 00:53:38.120]   - But not much.
[00:53:38.120 --> 00:53:39.960]   He was still holding power, I think.
[00:53:39.960 --> 00:53:44.920]   - And it's still possible now with the new constitution
[00:53:44.920 --> 00:53:47.480]   that he could hold power for longer than Stalin,
[00:53:47.480 --> 00:53:48.880]   longer than 30 years.
[00:53:48.880 --> 00:53:51.380]   What do you think about the man,
[00:53:51.380 --> 00:53:53.500]   the state of affairs in Russia,
[00:53:53.500 --> 00:53:57.900]   in general, the system they have there?
[00:53:57.900 --> 00:53:59.260]   Is there something interesting to you
[00:53:59.260 --> 00:54:02.260]   as an economist, as a human being, about Russia?
[00:54:02.260 --> 00:54:03.400]   - Everything is interesting.
[00:54:03.400 --> 00:54:05.980]   I mean, here would be part of my take.
[00:54:05.980 --> 00:54:09.160]   As you know, the Russian economy, starting, what,
[00:54:09.160 --> 00:54:12.620]   1999, 2000, has really quite a few years
[00:54:12.620 --> 00:54:14.180]   of super excellent growth,
[00:54:14.180 --> 00:54:17.140]   and Putin is still riding on that.
[00:54:17.140 --> 00:54:20.600]   It more or less coincides with his rise
[00:54:20.600 --> 00:54:24.240]   as the truly focal figure on the scene.
[00:54:24.240 --> 00:54:26.460]   Since then, pretty recently, they've had a bunch of years
[00:54:26.460 --> 00:54:31.460]   of negative four to 5% growth in a row, which is terrible.
[00:54:31.460 --> 00:54:35.180]   The economy is way too dependent on fossil fuels,
[00:54:35.180 --> 00:54:37.660]   but the structural problem is this.
[00:54:37.660 --> 00:54:40.860]   You need a concordance across economic power,
[00:54:40.860 --> 00:54:43.140]   social power, political power.
[00:54:43.140 --> 00:54:45.500]   They don't have to be allocated identically,
[00:54:45.500 --> 00:54:48.560]   but they have to be allocated consistently,
[00:54:48.560 --> 00:54:51.140]   and the Russian system under Putin,
[00:54:51.140 --> 00:54:55.140]   from almost the beginning, has never been able to have that,
[00:54:55.140 --> 00:54:58.500]   that ultimately, his incentives are to steer the system
[00:54:58.500 --> 00:55:01.700]   where the economic power is in a small number of hands
[00:55:01.700 --> 00:55:03.940]   in a non-diversified way.
[00:55:03.940 --> 00:55:06.580]   The system won't deliver sustainable gains
[00:55:06.580 --> 00:55:11.160]   in living standards anymore, ever, the way it's set up now,
[00:55:11.160 --> 00:55:13.160]   though if fossil fuel prices go up,
[00:55:13.160 --> 00:55:15.060]   they'll have some good years for sure,
[00:55:15.060 --> 00:55:20.380]   and that is really quite structural, what has gone wrong.
[00:55:20.380 --> 00:55:23.420]   And then on top of that, you can have an opinion of Putin,
[00:55:23.420 --> 00:55:25.700]   but you've gotta start with those structural problems,
[00:55:25.700 --> 00:55:28.740]   and that's why it's just not gonna work,
[00:55:28.740 --> 00:55:30.820]   but he had all those good years in the beginning,
[00:55:30.820 --> 00:55:33.420]   so the number of Russians, say, who live here,
[00:55:33.420 --> 00:55:36.580]   or in Russia, who love Putin and it's sincere,
[00:55:36.580 --> 00:55:39.500]   they're not just afraid of being dragged away,
[00:55:39.500 --> 00:55:41.480]   that's a real phenomenon.
[00:55:41.480 --> 00:55:45.260]   Yeah, I'm really torn on, Putin's approval rating,
[00:55:45.260 --> 00:55:49.060]   real approval rating, seems to be very high,
[00:55:49.060 --> 00:55:54.060]   and I'm torn in whether that has to do with the fact
[00:55:54.060 --> 00:55:57.000]   that there is control of the press,
[00:55:57.000 --> 00:56:01.020]   or if it's, which is the people I talk to
[00:56:01.020 --> 00:56:03.200]   who are in Russia, family and so on,
[00:56:03.200 --> 00:56:07.860]   a genuine love of Putin, appreciation of what Putin has done
[00:56:07.860 --> 00:56:09.940]   and is going to do with Russia.
[00:56:09.940 --> 00:56:11.640]   - But a lot of that would go away
[00:56:11.640 --> 00:56:13.180]   if the press were freer, I think.
[00:56:13.180 --> 00:56:14.440]   - Yes.
[00:56:14.440 --> 00:56:15.960]   - Well, Singapore realizes this,
[00:56:15.960 --> 00:56:18.600]   anyone discussed by the press, no matter who they are,
[00:56:18.600 --> 00:56:20.680]   people in Singapore have done a great job.
[00:56:20.680 --> 00:56:21.520]   - Yes.
[00:56:21.520 --> 00:56:24.720]   - If you're discussed by the press, you don't look good.
[00:56:24.720 --> 00:56:27.160]   Tech company executives are learning this, right?
[00:56:27.160 --> 00:56:28.440]   It's just like a rule.
[00:56:28.440 --> 00:56:32.280]   So in that sense, I think the rating is artificially high,
[00:56:32.280 --> 00:56:36.120]   but I don't, by any means, think it's all insincere,
[00:56:36.120 --> 00:56:39.520]   but that high popularity, I view as bearish for Russia.
[00:56:39.520 --> 00:56:40.960]   I would feel better about the country
[00:56:40.960 --> 00:56:43.160]   if people were more pissed off at him.
[00:56:43.160 --> 00:56:44.280]   - Yeah, that's right.
[00:56:44.280 --> 00:56:47.360]   It's nice to see free speech, even if it's full of hate.
[00:56:47.360 --> 00:56:52.920]   I am also troubled on the scientific side
[00:56:52.920 --> 00:56:54.360]   and entrepreneurial side.
[00:56:54.360 --> 00:56:57.500]   It seems difficult to be an entrepreneur in Russia.
[00:56:57.500 --> 00:57:03.120]   Like, it's not even in terms of rules,
[00:57:03.120 --> 00:57:06.280]   it's just culturally, the people I speak to,
[00:57:06.280 --> 00:57:10.080]   it's not easy to build a business.
[00:57:10.080 --> 00:57:13.960]   No, it's not easy to even dream
[00:57:13.960 --> 00:57:15.760]   of building a business in Russia.
[00:57:15.760 --> 00:57:17.160]   That's just not part of the culture,
[00:57:17.160 --> 00:57:19.080]   part of the conversation.
[00:57:19.080 --> 00:57:21.720]   It's almost like the conversation is,
[00:57:21.720 --> 00:57:25.240]   if you wanna be the next Bill Gates or Elon Musk
[00:57:25.240 --> 00:57:28.800]   or Steve Jobs or whatever, you come to America.
[00:57:28.800 --> 00:57:29.960]   That's the sense they have.
[00:57:29.960 --> 00:57:32.460]   - Yeah, history matters.
[00:57:34.120 --> 00:57:36.920]   Is it history, is it structural problems of today?
[00:57:36.920 --> 00:57:38.440]   - It's all the same thing.
[00:57:38.440 --> 00:57:40.620]   So a history of hostility to commerce,
[00:57:40.620 --> 00:57:44.140]   which of course, the old USSR is gone,
[00:57:44.140 --> 00:57:47.240]   but a lot of the attitudes remain,
[00:57:47.240 --> 00:57:49.120]   a lot of the corruption remains.
[00:57:49.120 --> 00:57:51.040]   You have this legacy distribution of wealth
[00:57:51.040 --> 00:57:53.240]   from the auctioning off of the assets,
[00:57:53.240 --> 00:57:54.920]   which is not conducive to some kind
[00:57:54.920 --> 00:57:57.760]   of broadly egalitarian democracy.
[00:57:57.760 --> 00:58:00.760]   And so you have these small number of PowerPoints
[00:58:00.760 --> 00:58:03.120]   to try to control information and wealth,
[00:58:03.120 --> 00:58:06.200]   and not really so keen to encourage the others
[00:58:06.200 --> 00:58:08.920]   who ultimately would pull the balance of political power
[00:58:08.920 --> 00:58:11.640]   away from the very wealthy and from Putin.
[00:58:11.640 --> 00:58:13.400]   And they support that culture.
[00:58:13.400 --> 00:58:16.320]   And the return of interest in Orthodox Church and all that,
[00:58:16.320 --> 00:58:19.300]   it's all part of the same piece, I think,
[00:58:19.300 --> 00:58:20.680]   'cause the old Orthodox Church
[00:58:20.680 --> 00:58:23.160]   is not that pro-commerce, you'd have to say,
[00:58:23.160 --> 00:58:25.320]   but it's traditionalist, it's pro-family,
[00:58:25.320 --> 00:58:27.360]   those are safer ideas.
[00:58:27.360 --> 00:58:29.440]   And then there's such a great safety valve,
[00:58:29.440 --> 00:58:31.440]   the most ambitious, smartest people,
[00:58:31.440 --> 00:58:33.680]   like they probably will learn English.
[00:58:33.680 --> 00:58:35.880]   They sort of can look like they belong
[00:58:35.880 --> 00:58:37.160]   in all sorts of other countries,
[00:58:37.160 --> 00:58:39.720]   they can show up and blend in, super talented,
[00:58:39.720 --> 00:58:42.320]   they've probably had an excellent education,
[00:58:42.320 --> 00:58:44.520]   especially if they're from one of the two major cities,
[00:58:44.520 --> 00:58:47.320]   but even if not so, even from Siberia,
[00:58:47.320 --> 00:58:49.240]   and they go off, they leave,
[00:58:49.240 --> 00:58:51.280]   they're not a source of opposition,
[00:58:51.280 --> 00:58:52.840]   and that keeps the whole thing up and running
[00:58:52.840 --> 00:58:54.800]   for another generation.
[00:58:54.800 --> 00:58:59.800]   - Yeah, what do you make of the other big player, China?
[00:59:00.800 --> 00:59:04.560]   They seem to have a very different,
[00:59:04.560 --> 00:59:09.520]   messed up, but also functioning system.
[00:59:09.520 --> 00:59:13.880]   They seem to be much better at encouraging entrepreneurs.
[00:59:13.880 --> 00:59:15.680]   They're choosing winners,
[00:59:15.680 --> 00:59:18.360]   but what do you make of the entire Chinese system?
[00:59:18.360 --> 00:59:22.500]   Why does it work as well as it does currently?
[00:59:22.500 --> 00:59:24.360]   What are your concerns about it?
[00:59:24.360 --> 00:59:28.200]   And what are its threats to the United States
[00:59:28.200 --> 00:59:32.320]   or possible, what is it you said,
[00:59:32.320 --> 00:59:34.920]   like wisdom is when two ideas come together.
[00:59:34.920 --> 00:59:36.960]   Is there some possible benefits
[00:59:36.960 --> 00:59:40.440]   of these kinds of ideas coming together?
[00:59:40.440 --> 00:59:42.960]   - It's amazing what China has done,
[00:59:42.960 --> 00:59:44.800]   but I would say to put it in perspective,
[00:59:44.800 --> 00:59:47.240]   if you compare them to Japan, South Korea,
[00:59:47.240 --> 00:59:49.480]   Taiwan, Hong Kong, and Singapore,
[00:59:49.480 --> 00:59:53.440]   they've still done much worse, not even close.
[00:59:53.440 --> 00:59:55.120]   And that's both living standards,
[00:59:55.120 --> 00:59:59.840]   or I hesitate to cite democracy as an unalloyed good
[00:59:59.840 --> 01:00:01.360]   in and of itself, but there's more freedom
[01:00:01.360 --> 01:00:04.000]   in all those other places by a lot.
[01:00:04.000 --> 01:00:07.080]   So China has all these problems of history,
[01:00:07.080 --> 01:00:09.560]   but they've managed as actually the Soviets did
[01:00:09.560 --> 01:00:11.340]   in the middle of the 20th century,
[01:00:11.340 --> 01:00:14.000]   one of the two great mass migrations
[01:00:14.000 --> 01:00:15.740]   from the countryside to cities,
[01:00:15.740 --> 01:00:18.180]   which boosts productivity enormously
[01:00:18.180 --> 01:00:21.000]   and will sustain totalitarian systems.
[01:00:21.000 --> 01:00:24.600]   But they moved from a totalitarian system to an oligarchy
[01:00:24.600 --> 01:00:29.120]   where the CCP is actually, at least for a while,
[01:00:29.120 --> 01:00:31.120]   hey, has been really good at governing,
[01:00:31.120 --> 01:00:33.720]   have made a lot of very good decisions.
[01:00:33.720 --> 01:00:35.560]   You have to admit that.
[01:00:35.560 --> 01:00:38.380]   I don't know how long that streak will continue
[01:00:38.380 --> 01:00:43.380]   with one person so much now holding authority
[01:00:43.380 --> 01:00:45.840]   in a more extreme manner.
[01:00:45.840 --> 01:00:48.160]   The selection pressures for the next generation
[01:00:48.160 --> 01:00:52.600]   of high-level CCP members probably become much worse.
[01:00:52.600 --> 01:00:53.940]   You have this general problem
[01:00:53.940 --> 01:00:55.280]   of the state-owned enterprise
[01:00:55.280 --> 01:00:57.120]   is losing relative productivity
[01:00:57.120 --> 01:00:58.800]   compared to the private sector.
[01:00:58.800 --> 01:01:02.640]   Well, we're gonna kind of hold Jack Ma on this island
[01:01:02.640 --> 01:01:05.240]   and he can only issue like weird hello statements.
[01:01:05.240 --> 01:01:08.680]   It kind of smells bad to me.
[01:01:08.680 --> 01:01:10.900]   I don't feel that it's about to crash,
[01:01:10.900 --> 01:01:14.760]   but I don't see them supplanting America
[01:01:14.760 --> 01:01:17.280]   as like the world's number one country.
[01:01:17.280 --> 01:01:19.280]   I think they will muddle through
[01:01:19.280 --> 01:01:21.760]   and have very serious problems,
[01:01:21.760 --> 01:01:22.840]   but there's enough talent there.
[01:01:22.840 --> 01:01:24.000]   They will muddle through.
[01:01:24.000 --> 01:01:26.640]   - Is there ideas from China or from anywhere in general
[01:01:26.640 --> 01:01:29.360]   of large-scale role of government
[01:01:29.360 --> 01:01:30.480]   that you find might be useful?
[01:01:30.480 --> 01:01:33.800]   Like Andrew Yang recently ran on a platform,
[01:01:33.800 --> 01:01:37.140]   UBI, Universal Basic Income.
[01:01:37.140 --> 01:01:42.140]   Is there some interesting ideas of large-scale government,
[01:01:42.140 --> 01:01:47.600]   sort of welfare programs at scale
[01:01:47.600 --> 01:01:50.360]   that you find interesting?
[01:01:51.700 --> 01:01:54.260]   - Well, keep in mind the current version
[01:01:54.260 --> 01:01:56.800]   of the Chinese Communist Party, post-Mao,
[01:01:56.800 --> 01:02:00.040]   dismantled what was called the iron rice ball.
[01:02:00.040 --> 01:02:02.320]   So it took apart the healthcare protections,
[01:02:02.320 --> 01:02:05.880]   a lot of the welfare system, a lot of the guaranteed jobs.
[01:02:05.880 --> 01:02:08.120]   So the economic rise of China coincided
[01:02:08.120 --> 01:02:10.480]   with the weakening of welfare.
[01:02:10.480 --> 01:02:12.960]   Not saying that's causal per se,
[01:02:12.960 --> 01:02:16.840]   but people think of China as having a government
[01:02:16.840 --> 01:02:17.760]   that takes care of everyone.
[01:02:17.760 --> 01:02:19.440]   It's very far from the truth.
[01:02:19.440 --> 01:02:20.880]   And by a lot of metrics,
[01:02:20.880 --> 01:02:23.520]   I don't mean control over people's lives.
[01:02:23.520 --> 01:02:25.800]   I don't mean speech, but by a lot of metrics,
[01:02:25.800 --> 01:02:28.880]   economically, we have a lot more government than they do.
[01:02:28.880 --> 01:02:32.720]   So what one means here by government, private control,
[01:02:32.720 --> 01:02:35.120]   I don't think you can just add up the numbers
[01:02:35.120 --> 01:02:36.760]   and get a simple answer.
[01:02:36.760 --> 01:02:40.800]   They've been fantastic at building infrastructure in cities
[01:02:40.800 --> 01:02:44.040]   in ways that will attract people from the countryside.
[01:02:44.040 --> 01:02:47.540]   And furthermore, they more or less enforce a meritocracy
[01:02:47.540 --> 01:02:49.000]   in this sense.
[01:02:49.000 --> 01:02:51.400]   Like if you're a kid of a rich guy,
[01:02:51.400 --> 01:02:53.200]   you'll get unfair privilege.
[01:02:53.200 --> 01:02:56.000]   That's unfair, but systems can afford that.
[01:02:56.000 --> 01:02:57.800]   If you are smart and from the countryside
[01:02:57.800 --> 01:02:59.800]   and your parents have nothing,
[01:02:59.800 --> 01:03:02.600]   you will be elevated and sent to a very good school,
[01:03:02.600 --> 01:03:05.160]   graduate school, because of the exam system.
[01:03:05.160 --> 01:03:08.520]   And they do that and they mean that very consistently.
[01:03:08.520 --> 01:03:10.240]   It's like the Soviets had a version of that
[01:03:10.240 --> 01:03:12.400]   like for jazz and romantic piano.
[01:03:12.400 --> 01:03:14.960]   Not for everything, but where they had it,
[01:03:14.960 --> 01:03:17.400]   like again, they were tremendous, right?
[01:03:17.400 --> 01:03:18.240]   - Yeah, exactly.
[01:03:18.240 --> 01:03:21.040]   - And Chinese have it in so many areas,
[01:03:21.040 --> 01:03:23.560]   a genuine meritocracy in this one way.
[01:03:23.560 --> 01:03:26.480]   - That moves people from the rural to the big city
[01:03:26.480 --> 01:03:29.040]   and that's a big boost of productivity
[01:03:29.040 --> 01:03:30.480]   for so a lot of time.
[01:03:30.480 --> 01:03:32.280]   - And when they get there, they're taken seriously.
[01:03:32.280 --> 01:03:34.760]   Jack Ma was riding a bicycle teaching English
[01:03:34.760 --> 01:03:35.720]   in his late 20s.
[01:03:35.720 --> 01:03:36.780]   He was a poor guy.
[01:03:36.780 --> 01:03:39.240]   - So--
[01:03:39.240 --> 01:03:41.520]   - Not a society of credentialism.
[01:03:41.520 --> 01:03:42.360]   - Right.
[01:03:42.360 --> 01:03:45.480]   - Or in America, it's way too much a credentialist society.
[01:03:45.480 --> 01:03:47.760]   - As we were talking about, even with the Nobel Prize.
[01:03:47.760 --> 01:03:48.600]   - Yeah.
[01:03:48.600 --> 01:03:50.840]   - But what do you think about these large government
[01:03:50.840 --> 01:03:53.160]   programs like UBI?
[01:03:53.160 --> 01:03:55.840]   - The one version of UBI that makes the most sense to me
[01:03:55.840 --> 01:03:59.280]   is the Mitt Romney version, UBI for kids.
[01:03:59.280 --> 01:04:00.680]   Like kids are vulnerable.
[01:04:00.680 --> 01:04:03.080]   If their parents screw up, you shouldn't blame the kid
[01:04:03.080 --> 01:04:04.980]   or make the kid suffer.
[01:04:04.980 --> 01:04:08.720]   I believe in something like UBI for kids, maybe just cash.
[01:04:08.720 --> 01:04:13.520]   But if you don't have kids, even with AI,
[01:04:13.520 --> 01:04:16.440]   my sense is at least in the world we know,
[01:04:16.440 --> 01:04:19.000]   you should be able to find a way to adjust.
[01:04:19.000 --> 01:04:22.840]   You might have to move to North Dakota to work,
[01:04:22.840 --> 01:04:26.000]   next to fracking, say.
[01:04:26.000 --> 01:04:30.640]   But look, before the pandemic,
[01:04:30.640 --> 01:04:33.080]   the two most robot-intensive societies,
[01:04:33.080 --> 01:04:36.520]   Japan and the US, US at least for manufacturing,
[01:04:36.520 --> 01:04:38.360]   were at full employment.
[01:04:38.360 --> 01:04:40.180]   So maybe there's some far-off day
[01:04:40.180 --> 01:04:41.560]   where there's literally no work.
[01:04:41.560 --> 01:04:45.500]   John Lennon, imagine it's piped everywhere.
[01:04:46.460 --> 01:04:49.500]   And then we might revisit the question.
[01:04:49.500 --> 01:04:53.500]   But for now, we had rising wages in the Trump years
[01:04:53.500 --> 01:04:54.940]   and full employment.
[01:04:54.940 --> 01:04:55.780]   So I don't see--
[01:04:55.780 --> 01:04:57.580]   - You don't see automation as a threat
[01:04:57.580 --> 01:05:00.380]   that fundamentally shakes our society.
[01:05:00.380 --> 01:05:01.980]   - It's a threat in the following sense.
[01:05:01.980 --> 01:05:04.260]   The new technologies are harder to work with
[01:05:04.260 --> 01:05:07.280]   for many people, and that's a social problem.
[01:05:07.280 --> 01:05:10.460]   But I'm not sure a universal basic income
[01:05:10.460 --> 01:05:13.520]   is the right answer to that very real problem.
[01:05:13.520 --> 01:05:16.800]   - Well, that's also, I like the UBI for kids.
[01:05:16.800 --> 01:05:20.300]   It's also your definition or the line,
[01:05:20.300 --> 01:05:21.960]   the threshold for what is vulnerable
[01:05:21.960 --> 01:05:24.760]   and what is basic human nature.
[01:05:24.760 --> 01:05:27.000]   Going back to Russia, life is suffering.
[01:05:27.000 --> 01:05:31.200]   Struggle is a part of life.
[01:05:31.200 --> 01:05:33.680]   And perhaps sort of changing,
[01:05:33.680 --> 01:05:36.440]   maybe what defines the 21st century
[01:05:36.440 --> 01:05:38.080]   is having multiple careers
[01:05:38.080 --> 01:05:41.120]   and adjusting and learning and evolving.
[01:05:41.120 --> 01:05:45.420]   And some of the technology in terms of,
[01:05:45.420 --> 01:05:50.560]   some of the technology we see, like the internet,
[01:05:50.560 --> 01:05:55.560]   allows us to make those pivots easier.
[01:05:55.560 --> 01:05:59.080]   Allows later life education possible.
[01:05:59.080 --> 01:06:00.680]   It makes it possible.
[01:06:00.680 --> 01:06:01.520]   I don't know.
[01:06:01.520 --> 01:06:03.080]   - And your earlier point about loneliness
[01:06:03.080 --> 01:06:04.680]   being this fundamental human problem,
[01:06:04.680 --> 01:06:07.200]   which I would agree with strongly,
[01:06:07.200 --> 01:06:10.240]   UBI, if it's at a high level, will make that worse.
[01:06:10.240 --> 01:06:11.680]   I mean, say UBI were high enough,
[01:06:11.680 --> 01:06:13.040]   you could just sit at home.
[01:06:13.040 --> 01:06:16.440]   People are not gonna be happy.
[01:06:16.440 --> 01:06:18.280]   They don't actually want that.
[01:06:18.280 --> 01:06:20.320]   And we've relearned that in the pandemic.
[01:06:20.320 --> 01:06:24.080]   - Yeah, the flip side, the hope with UBI
[01:06:24.080 --> 01:06:26.320]   is you have a little bit more freedom
[01:06:26.320 --> 01:06:29.260]   to find the thing that alleviates your loneliness.
[01:06:29.260 --> 01:06:30.740]   That's the idea.
[01:06:30.740 --> 01:06:33.320]   So it's kind of an open question.
[01:06:33.320 --> 01:06:38.240]   If I give you a million dollars or a billion dollars,
[01:06:38.240 --> 01:06:41.640]   will you pursue the thing you love?
[01:06:41.640 --> 01:06:45.800]   Will you be more motivated to find the thing you love,
[01:06:45.800 --> 01:06:46.680]   to do the thing you love?
[01:06:46.680 --> 01:06:50.180]   Or will you be lazy and lose yourself
[01:06:50.180 --> 01:06:52.400]   in the sort of daily activities
[01:06:52.400 --> 01:06:54.900]   that don't actually bring you joy,
[01:06:54.900 --> 01:06:57.760]   but pacify you in some kind of way
[01:06:57.760 --> 01:07:01.040]   where you just let the day slip by?
[01:07:01.040 --> 01:07:02.760]   That's the open question.
[01:07:02.760 --> 01:07:05.640]   - But a lot of the great creators did not have huge cushions,
[01:07:05.640 --> 01:07:07.600]   whether it's Mozart or James Brown
[01:07:07.600 --> 01:07:10.320]   or the great painters in history.
[01:07:10.320 --> 01:07:12.400]   They had to work pretty hard.
[01:07:12.400 --> 01:07:15.320]   And if you look at heirs to great fortunes,
[01:07:15.320 --> 01:07:16.840]   maybe I'm forgetting someone,
[01:07:16.840 --> 01:07:18.640]   but it's hard to think of any
[01:07:18.640 --> 01:07:22.000]   who have creatively been important as novelists.
[01:07:22.000 --> 01:07:25.980]   They might have continued to run the family business,
[01:07:25.980 --> 01:07:30.760]   but Van Gogh was not heir to a great family fortune.
[01:07:30.760 --> 01:07:34.520]   - It's sad that cushions get in the way of progress.
[01:07:34.640 --> 01:07:36.800]   It's just, yeah.
[01:07:36.800 --> 01:07:38.920]   - So it's the same point about prizes, right?
[01:07:38.920 --> 01:07:39.760]   - Yeah.
[01:07:39.760 --> 01:07:42.360]   - Inheriting too much money is like winning a prize.
[01:07:42.360 --> 01:07:45.800]   - We mentioned Eric, Eric Weinstein.
[01:07:45.800 --> 01:07:47.160]   I know you agree on a bunch of things.
[01:07:47.160 --> 01:07:49.260]   Is there some beautiful, fascinating,
[01:07:49.260 --> 01:07:51.680]   insightful disagreement that you have
[01:07:51.680 --> 01:07:54.120]   that has yet to be resolved with him?
[01:07:54.120 --> 01:07:58.120]   Is there some ideas that you guys battle it out on?
[01:07:58.120 --> 01:08:00.560]   Is it the stagnation question that you mentioned?
[01:08:00.560 --> 01:08:03.800]   - That's one of them, but here's at least two others.
[01:08:03.800 --> 01:08:05.200]   (laughing)
[01:08:05.200 --> 01:08:08.040]   But I would stress Eric is always evolving.
[01:08:08.040 --> 01:08:10.720]   So I'm just talking about a time slice, Eric, right?
[01:08:10.720 --> 01:08:13.000]   I don't know where he's at right now.
[01:08:13.000 --> 01:08:15.040]   Like I heard him on Clubhouse three nights ago,
[01:08:15.040 --> 01:08:16.620]   but that was three nights ago.
[01:08:16.620 --> 01:08:20.080]   But I think he's far too pessimistic
[01:08:20.080 --> 01:08:23.520]   about the impact of immigration on US science.
[01:08:23.520 --> 01:08:26.640]   He thinks it has displaced US scientists,
[01:08:26.640 --> 01:08:28.720]   which I think that is partly true.
[01:08:28.720 --> 01:08:30.360]   I just think we've gotten better talent.
[01:08:30.360 --> 01:08:33.200]   I'm like, bring it on, double down.
[01:08:33.200 --> 01:08:35.860]   And look at Currico, who basically came up
[01:08:35.860 --> 01:08:37.040]   with mRNA vaccines.
[01:08:37.040 --> 01:08:41.180]   She was from Hungary and was ridiculed and mocked.
[01:08:41.180 --> 01:08:42.800]   She couldn't get her papers published.
[01:08:42.800 --> 01:08:43.860]   She stuck at it.
[01:08:43.860 --> 01:08:47.960]   An American might not have been so stubborn
[01:08:47.960 --> 01:08:49.800]   'cause we have these cushions.
[01:08:49.800 --> 01:08:52.880]   So Eric is all worried, like mathematicians coming in,
[01:08:52.880 --> 01:08:57.000]   they're discouraging native US citizens from doing math.
[01:08:57.000 --> 01:08:59.520]   I'm like, bring in the best people.
[01:08:59.520 --> 01:09:02.760]   If we all end up in other avocations,
[01:09:02.760 --> 01:09:04.120]   absolutely fine by me.
[01:09:04.120 --> 01:09:06.840]   - Does it trouble you that we kick them out
[01:09:06.840 --> 01:09:08.680]   after they get a degree often?
[01:09:08.680 --> 01:09:11.440]   - I would give anyone with a plausible graduate degree
[01:09:11.440 --> 01:09:13.440]   a green card, universally.
[01:09:13.440 --> 01:09:15.960]   - Yeah, I agree with that.
[01:09:15.960 --> 01:09:17.120]   It makes no sense.
[01:09:17.120 --> 01:09:19.920]   It makes so strange that the best people that come here
[01:09:19.920 --> 01:09:22.640]   suffer here, create awesome stuff here,
[01:09:22.640 --> 01:09:24.440]   then when we kick them out, it doesn't make any sense.
[01:09:24.440 --> 01:09:25.560]   - Here's another view I have.
[01:09:25.560 --> 01:09:27.720]   I call it open borders for Belarus.
[01:09:27.720 --> 01:09:30.340]   Now, Russia's a big country.
[01:09:30.340 --> 01:09:33.440]   I would gladly increase the Russian quota
[01:09:33.440 --> 01:09:36.480]   by three X, four X, five X.
[01:09:36.480 --> 01:09:39.360]   Not 20%, but a big boost.
[01:09:39.360 --> 01:09:43.120]   But Belarus, small country, why can't?
[01:09:43.120 --> 01:09:46.580]   And they're poor, and they have decent education,
[01:09:46.580 --> 01:09:47.800]   and a lot of talent there.
[01:09:47.800 --> 01:09:50.120]   Why can't we just open the door
[01:09:50.120 --> 01:09:53.520]   and convert a Belarus passport to a green card?
[01:09:53.520 --> 01:09:56.640]   Open borders for Belarus, it's my new campaign slogan.
[01:09:56.640 --> 01:09:58.720]   - Are you running for president in 2024?
[01:09:58.720 --> 01:10:00.400]   - Well, write-ins are welcome.
[01:10:00.400 --> 01:10:02.000]   - Okay. (laughs)
[01:10:02.000 --> 01:10:04.440]   What's the second thing you disagree with, Eric?
[01:10:04.440 --> 01:10:07.360]   - Trade.
[01:10:07.360 --> 01:10:10.240]   Again, I'm not sure where he's at now,
[01:10:10.240 --> 01:10:14.840]   but he is suspicious of trade in a way that I am not.
[01:10:14.840 --> 01:10:17.320]   I do understand what's called the China shock
[01:10:17.320 --> 01:10:19.880]   has been a big problem for the US middle class.
[01:10:19.880 --> 01:10:21.400]   I fully accept that.
[01:10:21.400 --> 01:10:24.040]   I think most of that is behind us.
[01:10:24.040 --> 01:10:25.960]   National security issues aside,
[01:10:25.960 --> 01:10:29.200]   I think free trade is very much a good thing.
[01:10:29.200 --> 01:10:33.060]   Eric, I'm not sure he'll say it's not a good thing,
[01:10:33.060 --> 01:10:34.840]   but he won't say it is a good thing.
[01:10:34.840 --> 01:10:36.680]   And I know he's kind of, "Eh."
[01:10:36.680 --> 01:10:38.280]   It's like, "Eric, free trade."
[01:10:38.280 --> 01:10:40.920]   But look, on things like vaccines,
[01:10:40.920 --> 01:10:42.880]   I don't believe in free trade.
[01:10:42.880 --> 01:10:45.540]   You want vaccine production in your own country,
[01:10:45.540 --> 01:10:46.960]   look at the EU.
[01:10:46.960 --> 01:10:50.160]   They have enough money, no one will send them vaccines.
[01:10:50.160 --> 01:10:52.600]   - What's different about vaccines?
[01:10:52.600 --> 01:10:54.520]   There's some things you want to prioritize
[01:10:54.520 --> 01:10:56.400]   the citizenry on.
[01:10:56.400 --> 01:10:58.080]   - You could argue it would be cheaper
[01:10:58.080 --> 01:11:02.000]   to produce all US manufactured vaccines in India.
[01:11:02.000 --> 01:11:05.920]   They have the technologies, obviously lower wages.
[01:11:05.920 --> 01:11:07.560]   But look, there's talk in India right now
[01:11:07.560 --> 01:11:09.840]   of cutting off the export of vaccines.
[01:11:09.840 --> 01:11:11.980]   If you outsource your vaccine production,
[01:11:11.980 --> 01:11:13.360]   you're not sure the other country
[01:11:13.360 --> 01:11:15.820]   will respect the norm of free trade.
[01:11:15.820 --> 01:11:20.120]   So you need to keep some vaccine production in your country.
[01:11:20.120 --> 01:11:22.620]   It's an exception to free trade, not to the logic.
[01:11:23.620 --> 01:11:26.460]   A bunch of things the Navy uses,
[01:11:26.460 --> 01:11:28.540]   you can't buy those components from China.
[01:11:28.540 --> 01:11:29.380]   That's insane.
[01:11:29.380 --> 01:11:34.140]   But look, it would be cheaper to do so, right?
[01:11:34.140 --> 01:11:35.340]   - Yeah.
[01:11:35.340 --> 01:11:37.380]   Let me completely shift topics
[01:11:37.380 --> 01:11:38.700]   on something that's fascinating.
[01:11:38.700 --> 01:11:40.940]   - It's all the same topic, but great.
[01:11:40.940 --> 01:11:42.880]   - Everything is interesting.
[01:11:42.880 --> 01:11:48.820]   What do you think about, what the hell is money?
[01:11:48.820 --> 01:11:53.820]   And the recent excitement around cryptocurrency
[01:11:53.820 --> 01:12:01.100]   that brings to the forefront
[01:12:01.100 --> 01:12:04.860]   the philosophical discussion of the nature of money.
[01:12:04.860 --> 01:12:06.820]   Are you bullish on cryptocurrency?
[01:12:06.820 --> 01:12:07.900]   Are you excited about it?
[01:12:07.900 --> 01:12:09.580]   What does it make you think about
[01:12:09.580 --> 01:12:11.860]   how the nature of money is changing?
[01:12:11.860 --> 01:12:13.740]   - No one knows what money is.
[01:12:13.740 --> 01:12:15.540]   Probably no one ever knew.
[01:12:15.540 --> 01:12:17.900]   Go back to medieval times, bills of exchange.
[01:12:17.900 --> 01:12:19.460]   Were they money?
[01:12:19.460 --> 01:12:21.500]   Maybe it's just a semantic debate.
[01:12:21.500 --> 01:12:23.360]   Gold, silver, what about copper coins?
[01:12:23.360 --> 01:12:26.380]   What about metals that were considered legal tender
[01:12:26.380 --> 01:12:28.300]   but not always circulating?
[01:12:28.300 --> 01:12:29.700]   What about credit?
[01:12:29.700 --> 01:12:32.480]   So being confused about moneyness
[01:12:32.480 --> 01:12:35.240]   is the natural state of affairs for human beings.
[01:12:35.240 --> 01:12:36.260]   And if there's more of that,
[01:12:36.260 --> 01:12:38.860]   I'd say that's probably a good thing.
[01:12:38.860 --> 01:12:42.900]   Now crypto per se, I think Bitcoin has taken over
[01:12:42.900 --> 01:12:45.580]   a lot of the space held by gold.
[01:12:45.580 --> 01:12:48.180]   That to me seems sustainable.
[01:12:48.180 --> 01:12:50.380]   I'm not short Bitcoin.
[01:12:50.380 --> 01:12:53.460]   I don't have some view that the price
[01:12:53.460 --> 01:12:55.100]   has to be different than the current price,
[01:12:55.100 --> 01:12:56.860]   but I know it changes every moment.
[01:12:56.860 --> 01:13:01.180]   I am deeply uncertain about the less of crypto,
[01:13:01.180 --> 01:13:04.500]   which seems connected to ultimate visions
[01:13:04.500 --> 01:13:09.340]   of using it for transactions in ways where I'm not sure
[01:13:09.340 --> 01:13:12.460]   whether it be prediction markets or DeFi.
[01:13:13.340 --> 01:13:16.220]   I'm not sure the retail demand really is there
[01:13:16.220 --> 01:13:18.520]   once it is regulated like everything else is.
[01:13:18.520 --> 01:13:24.340]   I would say I'm 40/60 optimistic on those forms of crypto.
[01:13:24.340 --> 01:13:26.020]   That is, I think it's somewhat more likely
[01:13:26.020 --> 01:13:29.140]   they fail than succeed, but I take them very seriously.
[01:13:29.140 --> 01:13:31.140]   - So we're talking about it becoming
[01:13:31.140 --> 01:13:32.940]   one of the main currencies in the world.
[01:13:32.940 --> 01:13:33.980]   That's what we're discussing.
[01:13:33.980 --> 01:13:35.580]   - That I don't think will happen.
[01:13:35.580 --> 01:13:40.420]   - But the reality is that Bitcoin used to be
[01:13:40.420 --> 01:13:42.300]   in the single digits of a dollar
[01:13:42.300 --> 01:13:46.740]   and now has crossed $50,000 for a single Bitcoin.
[01:13:46.740 --> 01:13:48.620]   Do you think it's possible it reaches
[01:13:48.620 --> 01:13:50.260]   something like a million dollars?
[01:13:50.260 --> 01:13:52.780]   - I don't think we have a good theory
[01:13:52.780 --> 01:13:54.260]   of the value of Bitcoin.
[01:13:54.260 --> 01:13:56.140]   If people decide it's worth a million dollars,
[01:13:56.140 --> 01:13:57.740]   it's worth a million dollars.
[01:13:57.740 --> 01:13:58.580]   - But isn't that money?
[01:13:58.580 --> 01:14:01.660]   Like you said, isn't the ultimate state of money confusion,
[01:14:01.660 --> 01:14:03.180]   however beautifully you put it?
[01:14:03.180 --> 01:14:05.040]   - It's like valuing an Andy Warhol painting.
[01:14:05.040 --> 01:14:06.940]   So when Warhol started off,
[01:14:06.940 --> 01:14:08.900]   probably those things had no value,
[01:14:08.900 --> 01:14:11.540]   sketches, early sketches of shoes.
[01:14:11.540 --> 01:14:14.980]   Now a good Warhol could be worth over 50 million.
[01:14:14.980 --> 01:14:17.620]   That's an incredible rate of price appreciation.
[01:14:17.620 --> 01:14:20.700]   Bitcoin is seeing a similar trajectory.
[01:14:20.700 --> 01:14:22.960]   I don't pretend to know where it will stop,
[01:14:22.960 --> 01:14:25.260]   but it's about trying to figure out,
[01:14:25.260 --> 01:14:26.780]   well, what do people think of Andy Warhol?
[01:14:26.780 --> 01:14:29.780]   He could be out of fashion in a century.
[01:14:29.780 --> 01:14:31.260]   Maybe yes, maybe no.
[01:14:31.260 --> 01:14:35.320]   But you don't think about Warhols as money.
[01:14:35.320 --> 01:14:38.660]   They perform some money-like functions.
[01:14:38.660 --> 01:14:40.580]   You can even use them as collateral
[01:14:40.580 --> 01:14:42.380]   for like deals between gangs.
[01:14:42.380 --> 01:14:46.860]   But they're not basically money, nor is Bitcoin.
[01:14:46.860 --> 01:14:48.900]   And the transactions velocity of Bitcoin,
[01:14:48.900 --> 01:14:51.300]   I would think is likely to fall, if anything.
[01:14:51.300 --> 01:14:53.260]   - So you don't think there'll be some kind of phase shift?
[01:14:53.260 --> 01:14:55.420]   Will it become adopted, become mainstream
[01:14:55.420 --> 01:15:00.180]   for one of the main mechanisms of transactions?
[01:15:00.180 --> 01:15:01.000]   - Bitcoin, no.
[01:15:01.000 --> 01:15:03.340]   Now, Ether has some chance at that.
[01:15:03.340 --> 01:15:04.780]   I would bet against it,
[01:15:04.780 --> 01:15:06.820]   but I wouldn't give you a definitive no.
[01:15:06.820 --> 01:15:07.660]   - And you wouldn't put it in Sierra?
[01:15:07.660 --> 01:15:08.980]   - Bitcoin is too costly.
[01:15:10.100 --> 01:15:12.340]   It may be fine to hold it like gold,
[01:15:12.340 --> 01:15:13.840]   but gold is also costly.
[01:15:13.840 --> 01:15:18.300]   You have smart people trying to make, say, Ether
[01:15:18.300 --> 01:15:21.540]   much more effective as a currency than Bitcoin,
[01:15:21.540 --> 01:15:25.540]   and there's certainly a decent chance they will succeed.
[01:15:25.540 --> 01:15:26.660]   - Yeah, there's a lot of innovation.
[01:15:26.660 --> 01:15:30.060]   I mean, with smart contracts, with NFTs as well,
[01:15:30.060 --> 01:15:33.380]   there's a lot of interesting innovations
[01:15:33.380 --> 01:15:36.060]   that are plugging into the human psyche somehow,
[01:15:36.060 --> 01:15:37.380]   just like money does.
[01:15:38.260 --> 01:15:43.060]   Money seems to be this viral thing, our ideas of money,
[01:15:43.060 --> 01:15:45.340]   and if the idea is strong enough,
[01:15:45.340 --> 01:15:47.700]   it seems to be able to take hold.
[01:15:47.700 --> 01:15:50.860]   Like there's network effects that just take over.
[01:15:50.860 --> 01:15:54.020]   And I particularly see that with,
[01:15:54.020 --> 01:15:57.380]   I'd love to get your comment on Dogecoin,
[01:15:57.380 --> 01:15:59.740]   which is basically by a single human being,
[01:15:59.740 --> 01:16:01.860]   Elon Musk has been created.
[01:16:01.860 --> 01:16:05.620]   It's like these celebrities can have a huge ripple effect
[01:16:05.620 --> 01:16:07.140]   on the impact of money.
[01:16:07.140 --> 01:16:11.580]   Is it possible that in the 21st century,
[01:16:11.580 --> 01:16:13.820]   people like Elon Musk and celebrities,
[01:16:13.820 --> 01:16:17.580]   I don't know, Donald Trump, The Rock, whoever else,
[01:16:17.580 --> 01:16:22.340]   can actually define the currencies that we use?
[01:16:22.340 --> 01:16:24.460]   Maybe, can Dogecoin become
[01:16:24.460 --> 01:16:26.160]   the primary currency of the world?
[01:16:26.160 --> 01:16:29.260]   - I think of it as like baseball cards.
[01:16:29.260 --> 01:16:32.820]   So right now, every baseball player has a baseball card.
[01:16:32.820 --> 01:16:34.120]   And the players who are stars,
[01:16:34.120 --> 01:16:36.700]   their cards can end up worth a fair amount of money.
[01:16:36.700 --> 01:16:37.540]   - Yeah.
[01:16:37.540 --> 01:16:39.940]   - And that's stable, we've had it for many decades.
[01:16:39.940 --> 01:16:42.860]   Sort of the player defines the card,
[01:16:42.860 --> 01:16:46.180]   they sign a contract with Topps or whatever company.
[01:16:46.180 --> 01:16:49.060]   Now, could you imagine celebrities, baseball players,
[01:16:49.060 --> 01:16:51.440]   LeBron James, having their own currencies
[01:16:51.440 --> 01:16:53.000]   instead of cards?
[01:16:53.000 --> 01:16:55.960]   Absolutely, and you're somewhat seeing that right now,
[01:16:55.960 --> 01:16:58.320]   as you mentioned, artists with these unique works
[01:16:58.320 --> 01:16:59.340]   on the blockchain.
[01:16:59.340 --> 01:17:03.340]   But I'm not sure those are macroeconomically important.
[01:17:03.340 --> 01:17:05.180]   If it's just a new class of collectibles
[01:17:05.180 --> 01:17:08.340]   that people have fun with, again, I say bring it on.
[01:17:08.340 --> 01:17:12.100]   But whether there are use cases beyond that,
[01:17:12.100 --> 01:17:16.300]   that challenge fiat monies, which actually work very well.
[01:17:16.300 --> 01:17:20.300]   Yesterday, I sent money to a family in Ethiopia
[01:17:20.300 --> 01:17:22.060]   that I helped support.
[01:17:22.060 --> 01:17:24.980]   In less than 24 hours, they got that money.
[01:17:24.980 --> 01:17:27.300]   - Digitally, yes.
[01:17:27.300 --> 01:17:29.420]   - No, not digitally, through my bank,
[01:17:29.420 --> 01:17:33.220]   my primitive dinosaur bank, BB&T, Mid-Atlantic Bank,
[01:17:33.220 --> 01:17:35.380]   headquartered in North Carolina,
[01:17:35.380 --> 01:17:39.260]   charted by the Fed, regulated by the FDAC and the OCC.
[01:17:39.260 --> 01:17:41.420]   Now, you could say, well, the exchange rate
[01:17:41.420 --> 01:17:42.620]   was not so great.
[01:17:42.620 --> 01:17:47.420]   I don't see crypto as close to beating that
[01:17:47.420 --> 01:17:50.380]   once you take into account all of the last mile problems.
[01:17:50.380 --> 01:17:53.460]   Fiat currency works really well.
[01:17:53.460 --> 01:17:55.620]   People are not sitting around bitching about it.
[01:17:55.620 --> 01:17:57.060]   And when you talk to crypto people,
[01:17:57.060 --> 01:17:58.460]   the number who have to postulate
[01:17:58.460 --> 01:18:00.700]   some out of the blue hyperinflation,
[01:18:00.700 --> 01:18:03.300]   where there's no evidence for that whatsoever,
[01:18:03.300 --> 01:18:06.060]   that, to me, is a sign they're not thinking clearly
[01:18:06.060 --> 01:18:07.620]   about how hard they have to work
[01:18:07.620 --> 01:18:09.220]   to out-compete fiat currency.
[01:18:09.220 --> 01:18:11.860]   - There's a bunch of different technologies
[01:18:11.860 --> 01:18:15.860]   that are really exciting that don't want to address
[01:18:15.860 --> 01:18:17.780]   how difficult it is to out-compete
[01:18:17.780 --> 01:18:19.220]   the current accepted alternative.
[01:18:19.220 --> 01:18:21.780]   So, for example, autonomous vehicles.
[01:18:21.780 --> 01:18:24.100]   A lot of people are really excited.
[01:18:24.100 --> 01:18:28.100]   But it's not trivial to out-compete Uber
[01:18:28.100 --> 01:18:30.860]   on the cost and the effectiveness
[01:18:30.860 --> 01:18:32.860]   and the user experience and all those kinds of--
[01:18:32.860 --> 01:18:34.260]   - Correct. - Sorry, Uber driven
[01:18:34.260 --> 01:18:36.060]   by humans. - Yes.
[01:18:36.060 --> 01:18:40.260]   - And it's not, that's taken for granted, I think.
[01:18:40.260 --> 01:18:41.980]   That, look, wouldn't it be amazing,
[01:18:41.980 --> 01:18:43.260]   how amazing would the world look
[01:18:43.260 --> 01:18:45.860]   when the cars are driving themselves fully?
[01:18:45.860 --> 01:18:47.220]   It's going to drive the cost down.
[01:18:47.220 --> 01:18:48.620]   You can remove the cost of drivers,
[01:18:48.620 --> 01:18:50.180]   all those kinds of things.
[01:18:50.180 --> 01:18:52.180]   But when you actually get down to it
[01:18:52.180 --> 01:18:53.620]   and have to build a business around it,
[01:18:53.620 --> 01:18:55.500]   it's actually very difficult to do.
[01:18:55.500 --> 01:18:56.860]   And I guess you're saying your sense
[01:18:56.860 --> 01:19:00.340]   is similar competition is facing cryptocurrency.
[01:19:00.340 --> 01:19:05.340]   Like you have to actually present a killer app reason
[01:19:05.340 --> 01:19:11.780]   to switch from fiat currency to Ethereum or to whatever.
[01:19:11.780 --> 01:19:15.540]   - And the Biden people are going to regulate crypto.
[01:19:15.540 --> 01:19:17.020]   And they're going to do it soon.
[01:19:17.020 --> 01:19:20.740]   So something like DeFi, I fully get why that is cheaper,
[01:19:20.740 --> 01:19:23.340]   or for some, can be cheaper than other ways
[01:19:23.340 --> 01:19:25.660]   of conducting financial intermediation.
[01:19:25.660 --> 01:19:28.540]   But some of that is regulatory arbitrage.
[01:19:28.540 --> 01:19:30.780]   It will not be allowed to go on forever,
[01:19:30.780 --> 01:19:32.340]   for better or worse.
[01:19:32.340 --> 01:19:35.180]   I would rather see it given greater tolerance.
[01:19:35.180 --> 01:19:37.580]   But the point is, banking lobby is strong.
[01:19:37.580 --> 01:19:40.020]   The government will only let it run so far.
[01:19:40.020 --> 01:19:41.420]   There'll be capital requirements,
[01:19:41.420 --> 01:19:43.460]   reporting requirements imposed,
[01:19:43.460 --> 01:19:45.620]   and it will lose a lot of those advantages.
[01:19:45.620 --> 01:19:48.860]   - What do you make of Wall Street bets?
[01:19:48.860 --> 01:19:53.220]   Another thing that recently happened that shook the world,
[01:19:53.220 --> 01:19:56.900]   and at least me from the outsider perspective,
[01:19:56.900 --> 01:19:59.380]   make me question what I do and don't understand
[01:19:59.380 --> 01:20:01.380]   about our economic system,
[01:20:01.380 --> 01:20:03.540]   which is a bunch of different,
[01:20:03.540 --> 01:20:05.740]   a large number of individuals getting together
[01:20:05.740 --> 01:20:08.620]   on the internet and having a large-scale impact
[01:20:08.620 --> 01:20:10.300]   on the markets.
[01:20:10.300 --> 01:20:11.780]   - If you tell a group of people
[01:20:11.780 --> 01:20:13.460]   and coordinate them through the internet,
[01:20:13.460 --> 01:20:16.100]   we're going to play a fun game, it might cost you money,
[01:20:16.100 --> 01:20:17.500]   but you're going to make the headlines,
[01:20:17.500 --> 01:20:18.860]   and there's a chance you'll screw over
[01:20:18.860 --> 01:20:20.860]   some billionaires and hedge funds.
[01:20:20.860 --> 01:20:22.540]   Enough people will play that game.
[01:20:22.540 --> 01:20:23.380]   - Yes.
[01:20:23.380 --> 01:20:24.260]   - So that game might continue,
[01:20:24.260 --> 01:20:26.860]   but I don't think it's of macroeconomic importance.
[01:20:26.860 --> 01:20:30.660]   And the price of those stocks in the medium term
[01:20:30.660 --> 01:20:32.980]   will end up wherever it ought to be.
[01:20:32.980 --> 01:20:34.980]   - So these are little outliers
[01:20:34.980 --> 01:20:36.780]   from a macroeconomics perspective.
[01:20:36.780 --> 01:20:38.300]   They're not going to,
[01:20:38.300 --> 01:20:43.620]   these are not signals of a shifting power,
[01:20:43.620 --> 01:20:46.300]   like from centralized power to distributed power.
[01:20:46.300 --> 01:20:48.700]   These aren't some fundamental changes
[01:20:48.700 --> 01:20:50.260]   in the way our economy works.
[01:20:50.260 --> 01:20:52.500]   - I think of it as a new brand of esports,
[01:20:52.500 --> 01:20:54.220]   maybe more fun than the old brand,
[01:20:54.220 --> 01:20:56.340]   which is fine, right?
[01:20:56.340 --> 01:20:58.540]   It's like, push the anarchy into the corners
[01:20:58.540 --> 01:20:59.660]   where you want it.
[01:20:59.660 --> 01:21:02.900]   It doesn't bother me,
[01:21:02.900 --> 01:21:04.100]   but I think people are seeing it
[01:21:04.100 --> 01:21:05.260]   as more fundamental than it is.
[01:21:05.260 --> 01:21:07.620]   It's a new esport, more fun for many,
[01:21:07.620 --> 01:21:09.940]   but more expensive than the old esports.
[01:21:09.940 --> 01:21:13.660]   Like chess is a new esport, super cheap,
[01:21:13.660 --> 01:21:15.500]   not as fun as like, you know,
[01:21:15.500 --> 01:21:16.900]   sending hedge funds to their doom,
[01:21:16.900 --> 01:21:19.900]   but like, what would you expect?
[01:21:19.900 --> 01:21:21.220]   The poetry of that, I love it.
[01:21:21.220 --> 01:21:24.900]   Okay, but macroeconomically, it's not fundamental.
[01:21:24.900 --> 01:21:27.380]   Okay, I was going to say, I hope you're right,
[01:21:27.380 --> 01:21:29.820]   'cause I'm uncomfortable with the chaos
[01:21:29.820 --> 01:21:33.740]   of the masses that's creates, but I also--
[01:21:33.740 --> 01:21:36.220]   - I think that chaos is somewhat real, to be clear.
[01:21:36.220 --> 01:21:37.060]   - Yes.
[01:21:37.060 --> 01:21:40.620]   - But it will matter through other channels,
[01:21:40.620 --> 01:21:45.620]   not through manipulating, you know, GameStop or AMC.
[01:21:45.620 --> 01:21:48.540]   So you're seeing the real macro phenomenon.
[01:21:48.540 --> 01:21:50.740]   When people see a real macro phenomenon,
[01:21:50.740 --> 01:21:54.020]   they tend to make every micro story fit the narrative.
[01:21:54.020 --> 01:21:56.220]   And this micro story, like it fits the narrative,
[01:21:56.220 --> 01:21:58.980]   but it doesn't mean its importance fits the narrative.
[01:21:58.980 --> 01:22:01.780]   That's how I would kind of dissect the mistake
[01:22:01.780 --> 01:22:03.080]   I think people are making.
[01:22:03.080 --> 01:22:06.140]   - Do you, within the macro phenomenon,
[01:22:06.140 --> 01:22:07.820]   there are there, do you mean--
[01:22:07.820 --> 01:22:09.140]   - Everyone's weird now.
[01:22:09.140 --> 01:22:12.100]   The internet either allows us to be weirder
[01:22:12.100 --> 01:22:12.980]   or makes us weirder.
[01:22:12.980 --> 01:22:14.540]   I'm not sure what's the right way to put it.
[01:22:14.540 --> 01:22:16.420]   Maybe a mix of both.
[01:22:16.420 --> 01:22:18.780]   - You're probably right that it allows us to be weirder
[01:22:18.780 --> 01:22:21.340]   because, well, this is the other, okay.
[01:22:21.340 --> 01:22:23.980]   So this connects our previous conversation.
[01:22:23.980 --> 01:22:26.660]   Does America allow us to be weirder
[01:22:26.660 --> 01:22:29.220]   or does it make us weirder?
[01:22:29.220 --> 01:22:32.300]   - Like say we're weird and somewhat neurotic to begin with,
[01:22:32.300 --> 01:22:34.940]   but the only messages we get are Dwight D. Eisenhower
[01:22:34.940 --> 01:22:37.580]   and I Love Lucy and Network TV.
[01:22:37.580 --> 01:22:40.060]   Like that's gonna keep us within certain bounds
[01:22:40.060 --> 01:22:41.660]   in good and bad ways.
[01:22:41.660 --> 01:22:43.860]   That's obviously totally gone.
[01:22:43.860 --> 01:22:47.020]   And the internet you can connect to not just QAnon,
[01:22:47.020 --> 01:22:47.980]   but all sorts of things.
[01:22:47.980 --> 01:22:49.740]   Many of them just fantastic, right?
[01:22:49.740 --> 01:22:54.260]   But in good and bad ways, it makes us weirder.
[01:22:54.260 --> 01:22:56.700]   So that maybe is troubling, right?
[01:22:56.700 --> 01:22:58.020]   Like if someone's worried about that,
[01:22:58.020 --> 01:22:59.860]   I would at least say they should give it
[01:22:59.860 --> 01:23:01.500]   deep serious thought.
[01:23:01.500 --> 01:23:04.620]   And then it has a whole lot of ebbs and flows,
[01:23:04.620 --> 01:23:07.580]   micro realizations of the weirdness
[01:23:07.580 --> 01:23:09.580]   that don't actually matter.
[01:23:09.580 --> 01:23:10.940]   So like chess players today,
[01:23:10.940 --> 01:23:12.580]   they play a lot more weird openings
[01:23:12.580 --> 01:23:14.580]   than they did 20 years ago.
[01:23:14.580 --> 01:23:16.740]   Like it reflects the same thing
[01:23:16.740 --> 01:23:19.140]   'cause you can research any weird opening on the internet,
[01:23:19.140 --> 01:23:21.100]   but like, does that matter?
[01:23:21.100 --> 01:23:22.460]   Probably not.
[01:23:22.460 --> 01:23:23.780]   So a lot of the things we see
[01:23:23.780 --> 01:23:26.020]   are just like the weird chess openings.
[01:23:26.020 --> 01:23:28.340]   And to figure out which are like the weird chess openings
[01:23:28.340 --> 01:23:31.540]   and which are fundamental to the new and growing weirdness.
[01:23:31.540 --> 01:23:33.500]   Like that's what a hedge fund investor type
[01:23:33.500 --> 01:23:35.020]   should be trying to do.
[01:23:35.020 --> 01:23:36.900]   I just think no one knows yet.
[01:23:36.900 --> 01:23:40.020]   It's like this itself, this fun, weird guessing game,
[01:23:40.020 --> 01:23:41.940]   which we're partly engaging in right now.
[01:23:41.940 --> 01:23:43.260]   - Well, exactly.
[01:23:43.260 --> 01:23:45.220]   And I mean, as Eric talks about
[01:23:45.220 --> 01:23:47.980]   on the science side of things,
[01:23:47.980 --> 01:23:50.100]   I mean, I said like at MIT,
[01:23:50.100 --> 01:23:52.460]   especially in the machine learning field,
[01:23:52.460 --> 01:23:56.180]   there's a natural institutional resistance to the weird.
[01:23:56.180 --> 01:23:58.220]   It's very, as they talk about,
[01:23:58.220 --> 01:24:00.980]   it's difficult to hire weird faculty, for example.
[01:24:00.980 --> 01:24:01.820]   - Correct.
[01:24:01.820 --> 01:24:05.860]   - You want to hire and give tenure to people that are safe,
[01:24:05.860 --> 01:24:07.660]   not weird.
[01:24:07.660 --> 01:24:09.740]   And that's one of the concerns is like,
[01:24:09.740 --> 01:24:11.020]   it seems like the weird people
[01:24:11.020 --> 01:24:13.100]   are the ones that push the science forward usually.
[01:24:13.100 --> 01:24:14.100]   - Right.
[01:24:14.100 --> 01:24:16.620]   - And so like, how do you balance the two?
[01:24:16.620 --> 01:24:17.860]   It's not obvious.
[01:24:17.860 --> 01:24:20.540]   - It's another area where Eric and I disagree.
[01:24:20.540 --> 01:24:24.460]   As I interpret him, he thinks academia is totally bankrupt.
[01:24:24.460 --> 01:24:26.500]   And I think it's only partially bankrupt.
[01:24:26.500 --> 01:24:28.740]   - How do we fix it?
[01:24:28.740 --> 01:24:29.740]   'Cause I'm with you.
[01:24:29.740 --> 01:24:32.020]   I'm bullish on academia.
[01:24:32.020 --> 01:24:34.220]   - You need up and coming schools
[01:24:34.220 --> 01:24:36.340]   that end up better than where they started off.
[01:24:36.340 --> 01:24:38.780]   And MIT was once one of them.
[01:24:38.780 --> 01:24:40.540]   Now they're not in every area.
[01:24:40.540 --> 01:24:43.180]   In some areas, they have become the problem.
[01:24:43.180 --> 01:24:45.300]   UChicago, you wouldn't call it up and coming,
[01:24:45.300 --> 01:24:46.380]   but it's still different.
[01:24:46.380 --> 01:24:47.220]   And that's great.
[01:24:47.220 --> 01:24:49.900]   Let's hope they manage to keep it that way.
[01:24:49.900 --> 01:24:55.340]   The biggest problem to me is the rank, absurd conformism
[01:24:55.340 --> 01:24:57.140]   at kind of second tier schools,
[01:24:57.140 --> 01:24:59.980]   maybe in the top 40, but not in the top dozen,
[01:24:59.980 --> 01:25:02.700]   that are just trying to be like a junior MIT,
[01:25:02.700 --> 01:25:04.900]   but it's mediocre and copycat.
[01:25:04.900 --> 01:25:07.420]   And they're the most dogmatic enforcers of weirdness
[01:25:07.420 --> 01:25:09.660]   that like Harvard is more open
[01:25:09.660 --> 01:25:11.420]   than those second tier schools.
[01:25:11.420 --> 01:25:12.660]   And those second tier schools
[01:25:12.660 --> 01:25:14.900]   are pretty good typically, right?
[01:25:14.900 --> 01:25:17.420]   - But the mediocrity is enforced there.
[01:25:17.420 --> 01:25:19.540]   - Correct, very strictly.
[01:25:19.540 --> 01:25:21.300]   And the homogenization pressures.
[01:25:21.300 --> 01:25:24.540]   Climb the rankings by another three places
[01:25:24.540 --> 01:25:25.940]   and be a little closer to MIT,
[01:25:25.940 --> 01:25:27.340]   though you'll never touch them.
[01:25:27.340 --> 01:25:28.820]   That to me is very harmful.
[01:25:28.820 --> 01:25:30.980]   And you'd rather they be more like Chicago,
[01:25:30.980 --> 01:25:33.980]   more like Caltech, or the older Caltech all the more.
[01:25:33.980 --> 01:25:37.420]   Like pick some model, be weird in it.
[01:25:37.420 --> 01:25:40.640]   You might fail, that's socially better.
[01:25:40.640 --> 01:25:43.700]   - Yeah, but so the problem with MIT, for example,
[01:25:43.700 --> 01:25:48.700]   is the mediocrity is really enforced on the junior faculty.
[01:25:48.700 --> 01:25:52.660]   So like the people that are allowed to be weird,
[01:25:52.660 --> 01:25:54.940]   or actually they just don't even ask for permissions anymore
[01:25:54.940 --> 01:25:56.380]   are more senior faculty.
[01:25:56.380 --> 01:25:57.820]   And that's good, of course,
[01:25:57.820 --> 01:26:00.360]   but you want the weird young people.
[01:26:00.360 --> 01:26:06.220]   I find this podcast, I like talking to tech people,
[01:26:06.220 --> 01:26:08.740]   and I find the young faculty to be really boring.
[01:26:08.740 --> 01:26:11.140]   - They are, they're the most boring of faculty.
[01:26:11.140 --> 01:26:15.020]   - Their work is interesting technically, technically,
[01:26:15.020 --> 01:26:18.100]   but just the passion.
[01:26:18.100 --> 01:26:19.100]   - They are drudges.
[01:26:19.100 --> 01:26:23.420]   - And some of them sneak by.
[01:26:23.420 --> 01:26:24.980]   Like you have like the Max Stegmark,
[01:26:24.980 --> 01:26:26.420]   young version of Max Stegmark,
[01:26:26.420 --> 01:26:31.420]   who knows how to play the role of boring and fitting in.
[01:26:31.420 --> 01:26:34.420]   And then on the side, he does the weird shit.
[01:26:35.380 --> 01:26:37.380]   But they're not, they're far and few in between,
[01:26:37.380 --> 01:26:41.460]   which I'd love to figure out a way to shake up that system
[01:26:41.460 --> 01:26:42.300]   because--
[01:26:42.300 --> 01:26:45.140]   - You look at MIT's Broad Institute, right,
[01:26:45.140 --> 01:26:47.300]   in biomedical, it's been a huge hit.
[01:26:47.300 --> 01:26:49.420]   I'm not privy to their internal doings,
[01:26:49.420 --> 01:26:52.460]   but I suspect they support weird
[01:26:52.460 --> 01:26:55.340]   more than the formal departments do at the junior level.
[01:26:55.340 --> 01:26:56.500]   - Yes, that's probably true.
[01:26:56.500 --> 01:26:57.800]   Yeah, I don't know what,
[01:26:57.800 --> 01:26:59.640]   whatever they're doing is working,
[01:26:59.640 --> 01:27:03.580]   but we need to figure it out,
[01:27:03.580 --> 01:27:06.700]   'cause I think the best ideas still do come from the,
[01:27:06.700 --> 01:27:10.140]   so forget my apologies,
[01:27:10.140 --> 01:27:11.660]   but for the humanities side of things,
[01:27:11.660 --> 01:27:12.820]   I don't know anything about,
[01:27:12.820 --> 01:27:15.660]   but the engineering and the science side,
[01:27:15.660 --> 01:27:17.740]   I think there's so many amazing ideas
[01:27:17.740 --> 01:27:19.780]   that are still coming from universities.
[01:27:19.780 --> 01:27:21.220]   - It's not true that you don't know anything
[01:27:21.220 --> 01:27:22.060]   about the humanities.
[01:27:22.060 --> 01:27:24.260]   You're doing the humanities right now.
[01:27:24.260 --> 01:27:25.720]   We're talking about people.
[01:27:25.720 --> 01:27:28.300]   There are no numbers put on a blackboard, right?
[01:27:28.300 --> 01:27:30.220]   There's no hypothesis testing per se.
[01:27:30.220 --> 01:27:31.060]   - No, yeah, that's not--
[01:27:31.060 --> 01:27:34.580]   - You have however many subscribers to your podcast
[01:27:34.580 --> 01:27:37.060]   all listening to you on the humanities.
[01:27:37.060 --> 01:27:38.820]   Every, whatever your frequency is--
[01:27:38.820 --> 01:27:40.940]   - But I'm not in the department of the humanities.
[01:27:40.940 --> 01:27:42.420]   - That's why it's innovative.
[01:27:42.420 --> 01:27:44.900]   - They have very different conversations.
[01:27:44.900 --> 01:27:48.180]   There's the number of emails I get about,
[01:27:48.180 --> 01:27:51.900]   listen, I really deeply respect diversity
[01:27:51.900 --> 01:27:56.260]   and the full scope of what diversity means,
[01:27:56.260 --> 01:27:57.540]   and also the more narrow scope
[01:27:57.540 --> 01:27:59.420]   of different races and genders and so on.
[01:27:59.420 --> 01:28:01.060]   It's a really important topic,
[01:28:01.060 --> 01:28:03.820]   but there's a disproportionate number of emails I'm getting
[01:28:03.820 --> 01:28:06.020]   about meetings and discussions,
[01:28:06.020 --> 01:28:08.540]   and that just kind of is overwhelming.
[01:28:08.540 --> 01:28:10.880]   I don't get enough emails from people,
[01:28:10.880 --> 01:28:15.880]   like a meeting about why are all your ideas bad?
[01:28:15.880 --> 01:28:18.860]   Let's, for example, let me call out MIT.
[01:28:18.860 --> 01:28:20.400]   Why don't we do more?
[01:28:20.400 --> 01:28:24.320]   Why don't we kick Stanford's ass or Google's ass,
[01:28:24.320 --> 01:28:26.660]   more importantly, in deep learning and machine learning
[01:28:26.660 --> 01:28:28.280]   and AI research?
[01:28:28.280 --> 01:28:31.620]   What CSAIL, for example, used to be a laboratory
[01:28:31.620 --> 01:28:35.140]   is a laboratory for artificial intelligence research,
[01:28:35.140 --> 01:28:38.860]   and why is that not the beacon
[01:28:38.860 --> 01:28:43.800]   of greatness in artificial intelligence?
[01:28:43.800 --> 01:28:45.780]   Let's have those meetings as well.
[01:28:45.780 --> 01:28:48.940]   - Diversity talk has oddly become this new mechanism
[01:28:48.940 --> 01:28:50.500]   for enforcing conformity.
[01:28:50.500 --> 01:28:52.340]   - Yes, exactly, and right.
[01:28:52.340 --> 01:28:54.340]   So it's almost like this conformity mechanism
[01:28:54.340 --> 01:28:56.860]   finds the hot new topic to use
[01:28:56.860 --> 01:28:58.220]   to enforce further conformity.
[01:28:58.220 --> 01:28:59.720]   - Exactly.
[01:28:59.720 --> 01:29:01.800]   - Oh boy, I still have hope.
[01:29:01.800 --> 01:29:03.280]   I remain optimistic.
[01:29:03.280 --> 01:29:05.640]   - But the humanities have innovated through podcasts,
[01:29:05.640 --> 01:29:08.700]   including yours and mine, and they're alive and well.
[01:29:08.700 --> 01:29:12.560]   All the bad talk you hear about the humanities
[01:29:12.560 --> 01:29:15.160]   in universities, there's been this huge end run
[01:29:15.160 --> 01:29:17.680]   of innovation on the internet, and it's amazing.
[01:29:17.680 --> 01:29:18.520]   - You're right.
[01:29:18.520 --> 01:29:20.820]   I never thought of, I mean, this is humanities.
[01:29:20.820 --> 01:29:22.560]   (both laughing)
[01:29:22.560 --> 01:29:23.400]   This podcast, right?
[01:29:23.400 --> 01:29:25.120]   - It's like you've been speaking prose all one's life
[01:29:25.120 --> 01:29:26.420]   and didn't know it, right?
[01:29:28.160 --> 01:29:31.040]   - Yeah, I am actually part of the humanities department
[01:29:31.040 --> 01:29:31.880]   at MIT now.
[01:29:31.880 --> 01:29:35.040]   I did not realize this, and I will fully embrace it
[01:29:35.040 --> 01:29:36.120]   from this moment on.
[01:29:36.120 --> 01:29:37.600]   - Look, you have this thing, the Media Lab.
[01:29:37.600 --> 01:29:39.080]   I'm sure you know about it.
[01:29:39.080 --> 01:29:42.440]   Done some excellent things, done a lot of very bogus things,
[01:29:42.440 --> 01:29:43.720]   but you're out-competing them.
[01:29:43.720 --> 01:29:45.000]   You're blowing them out of the water.
[01:29:45.000 --> 01:29:45.840]   - Yeah.
[01:29:45.840 --> 01:29:46.920]   - Like you are them.
[01:29:46.920 --> 01:29:48.520]   - Yeah, and I'm talking to those folks,
[01:29:48.520 --> 01:29:51.520]   and they're trying to figure it out.
[01:29:51.520 --> 01:29:53.280]   I mean, they had their issues with Jeffrey Epstein
[01:29:53.280 --> 01:29:56.600]   and so on, but outside of that,
[01:29:57.000 --> 01:30:00.000]   there's a, I've actually gone through a shift
[01:30:00.000 --> 01:30:02.260]   with this particular podcast, for example,
[01:30:02.260 --> 01:30:06.320]   where at first, it was seen as a,
[01:30:06.320 --> 01:30:09.600]   one, at the very first, it was seen as a distraction.
[01:30:09.600 --> 01:30:13.740]   Second, it was a source of almost a kind of jealousy,
[01:30:13.740 --> 01:30:15.320]   like the same kind of jealousy you feel
[01:30:15.320 --> 01:30:18.120]   when junior faculty outshines the senior faculty.
[01:30:18.120 --> 01:30:19.000]   - Of course.
[01:30:19.000 --> 01:30:21.720]   - And now it's more like, oh, okay, this is a thing.
[01:30:21.720 --> 01:30:23.840]   We should do more of that.
[01:30:23.840 --> 01:30:25.240]   We should embrace this guy.
[01:30:25.240 --> 01:30:26.920]   We should embrace this thing.
[01:30:26.920 --> 01:30:29.560]   So there's a sense that podcasting,
[01:30:29.560 --> 01:30:32.800]   and whatever this is, it doesn't have to be podcasting,
[01:30:32.800 --> 01:30:35.480]   will drive some innovation within MIT,
[01:30:35.480 --> 01:30:37.600]   within different universities.
[01:30:37.600 --> 01:30:39.200]   There's a sense that things are changing.
[01:30:39.200 --> 01:30:41.840]   It's just that universities lag behind,
[01:30:41.840 --> 01:30:45.800]   and my hope is that they catch up quickly.
[01:30:45.800 --> 01:30:49.560]   They innovate in some way that goes along
[01:30:49.560 --> 01:30:51.880]   with the innovations of the internet.
[01:30:51.880 --> 01:30:53.460]   - I think the internet will outrace them
[01:30:53.460 --> 01:30:55.760]   for a long time, maybe forever.
[01:30:55.760 --> 01:30:57.680]   - Well, I mean, but it's okay if they're,
[01:30:57.680 --> 01:30:58.640]   as long as they're keeping--
[01:30:58.640 --> 01:31:00.200]   - Yeah, and we're both in universities,
[01:31:00.200 --> 01:31:02.720]   so we have multiple hats on here as we're speaking.
[01:31:02.720 --> 01:31:05.800]   So we can complain about the universities,
[01:31:05.800 --> 01:31:08.440]   but that's like complaining about the podcasters, right?
[01:31:08.440 --> 01:31:09.280]   - Yeah.
[01:31:09.280 --> 01:31:10.100]   - We be them.
[01:31:10.100 --> 01:31:12.960]   - But speaking on the weird, you've,
[01:31:12.960 --> 01:31:16.400]   in the best sense of the word, weird,
[01:31:16.400 --> 01:31:18.520]   you've written about and made the case
[01:31:18.520 --> 01:31:22.280]   that we should take UFO sightings more seriously.
[01:31:22.280 --> 01:31:27.280]   - So that's one of the things that I've been inundated with,
[01:31:27.280 --> 01:31:34.300]   sort of the excitement and the passion
[01:31:34.300 --> 01:31:37.740]   that people have for the possibility
[01:31:37.740 --> 01:31:40.740]   of extraterrestrial life, of life out there in the universe.
[01:31:40.740 --> 01:31:42.460]   I've always felt this excitement.
[01:31:42.460 --> 01:31:43.740]   I was just looking up at the stars
[01:31:43.740 --> 01:31:46.140]   and wondering what the hell's out there.
[01:31:46.140 --> 01:31:48.820]   But there's people that have more like,
[01:31:48.820 --> 01:31:52.160]   more grounded excitement and passion
[01:31:52.160 --> 01:31:56.000]   of actually interacting with aliens on this,
[01:31:56.000 --> 01:31:57.880]   here, our planet.
[01:31:57.880 --> 01:32:01.240]   What's the case, from your perspective,
[01:32:01.240 --> 01:32:05.020]   for taking these sightings more seriously?
[01:32:05.020 --> 01:32:11.920]   - The data from the Navy, to me, seem quite serious.
[01:32:11.920 --> 01:32:14.460]   I don't pretend that I have the technical abilities
[01:32:14.460 --> 01:32:16.820]   to judge it as data,
[01:32:16.820 --> 01:32:21.120]   but there are numerous senators at the very highest of levels
[01:32:21.120 --> 01:32:24.080]   former heads of CIA, Brennan, I talked to him,
[01:32:24.080 --> 01:32:25.440]   did an interview with him.
[01:32:25.440 --> 01:32:27.480]   I asked him, "What's up with these?
[01:32:27.480 --> 01:32:28.440]   "What do you think it is?"
[01:32:28.440 --> 01:32:31.680]   He basically said that was the single most likely explanation
[01:32:31.680 --> 01:32:32.880]   was of alien origin.
[01:32:32.880 --> 01:32:35.440]   Now, you don't have to agree with him,
[01:32:35.440 --> 01:32:37.280]   but look, if you know how government works,
[01:32:37.280 --> 01:32:39.740]   these senators, or Hillary Clinton for that matter,
[01:32:39.740 --> 01:32:41.640]   or Brennan, they sat down,
[01:32:41.640 --> 01:32:43.480]   they were briefed by their smartest people,
[01:32:43.480 --> 01:32:46.160]   and they said, "Hey, what's going on here?
[01:32:46.880 --> 01:32:48.600]   "And everyone around the table, I believe,
[01:32:48.600 --> 01:32:51.260]   "is telling them, 'We don't know.'"
[01:32:51.260 --> 01:32:55.800]   And that is sociological data I take very seriously.
[01:32:55.800 --> 01:32:59.660]   I have not seen a debunking of the technical data,
[01:32:59.660 --> 01:33:02.260]   which is eyewitness reports, and images, and radar.
[01:33:02.260 --> 01:33:03.880]   Again, at a technical level,
[01:33:03.880 --> 01:33:06.680]   I feel quite uncertain on that turf.
[01:33:06.680 --> 01:33:10.040]   But evaluating the testimony of witnesses,
[01:33:10.040 --> 01:33:12.660]   it seems to me it's now at a threshold
[01:33:12.660 --> 01:33:15.140]   where one ought to take it seriously.
[01:33:15.140 --> 01:33:18.520]   - Yeah, there's one of the problems with UFO sightings
[01:33:18.520 --> 01:33:21.100]   is that because of people with good equipment,
[01:33:21.100 --> 01:33:23.200]   don't take it seriously.
[01:33:23.200 --> 01:33:26.640]   It's such a taboo topic that you have
[01:33:26.640 --> 01:33:28.940]   just really shitty equipment collecting data.
[01:33:28.940 --> 01:33:32.520]   And so you have the blurry Bigfoot kind of situation
[01:33:32.520 --> 01:33:33.960]   where you have just bad video,
[01:33:33.960 --> 01:33:35.720]   and all those kinds of things,
[01:33:35.720 --> 01:33:40.200]   as opposed to, I mean, there's a bunch of people,
[01:33:40.200 --> 01:33:44.840]   Avi Loeb from Harvard talking about Amua Amua,
[01:33:44.840 --> 01:33:49.640]   it's just like people with the equipment
[01:33:49.640 --> 01:33:54.200]   to do the data collection don't want to help out.
[01:33:54.200 --> 01:33:57.800]   And that creates a kind of divide
[01:33:57.800 --> 01:34:00.720]   where the scientists ignore that this is happening,
[01:34:00.720 --> 01:34:04.080]   and there's the masses of people who are curious about it.
[01:34:04.080 --> 01:34:06.980]   And then there's the government that's full of secrets
[01:34:06.980 --> 01:34:10.720]   that's leaking some confusion,
[01:34:10.720 --> 01:34:12.800]   and it creates distrust in the government,
[01:34:12.800 --> 01:34:15.240]   it creates distrust in science,
[01:34:15.240 --> 01:34:16.960]   and it prevents the scientists
[01:34:16.960 --> 01:34:19.800]   from being able to explore some cool topics,
[01:34:19.800 --> 01:34:22.760]   some exciting possibilities that they should be,
[01:34:22.760 --> 01:34:25.300]   be curious kids like Avi talks about.
[01:34:25.300 --> 01:34:28.520]   - Even if it has nothing to do with aliens,
[01:34:28.520 --> 01:34:29.560]   whatever the answer is,
[01:34:29.560 --> 01:34:31.200]   it has to be something fascinating.
[01:34:31.200 --> 01:34:32.600]   We already know everything's interesting,
[01:34:32.600 --> 01:34:33.840]   but this is fascinating.
[01:34:33.840 --> 01:34:37.000]   But look, that all said,
[01:34:37.000 --> 01:34:39.080]   I suspect they're not of alien origin,
[01:34:39.080 --> 01:34:40.360]   and let me tell you my reason.
[01:34:40.360 --> 01:34:43.200]   The people who are all gung-ho,
[01:34:43.200 --> 01:34:45.200]   they do a kind of reasoning in reverse,
[01:34:45.200 --> 01:34:46.760]   or argument from elimination.
[01:34:46.760 --> 01:34:49.900]   They figure out a bunch of things that can't be,
[01:34:49.900 --> 01:34:52.340]   like is it a Russian advanced vehicle?
[01:34:52.340 --> 01:34:54.560]   No, probably pretty good arguments there.
[01:34:54.560 --> 01:34:56.320]   Is it a Chinese advanced vehicle?
[01:34:56.320 --> 01:34:57.280]   No.
[01:34:57.280 --> 01:35:01.480]   Is it people from the Earth's future coming back in time?
[01:35:01.480 --> 01:35:02.320]   No.
[01:35:02.320 --> 01:35:03.600]   And they go through a few others,
[01:35:03.600 --> 01:35:05.000]   they have some really good no arguments,
[01:35:05.000 --> 01:35:07.960]   then they're like, well, what we've got left is aliens.
[01:35:07.960 --> 01:35:09.680]   This argument from elimination,
[01:35:09.680 --> 01:35:12.480]   I don't actually find that persuasive.
[01:35:12.480 --> 01:35:16.040]   You can talk yourself into a lot of mistaken ideas that way.
[01:35:16.040 --> 01:35:20.480]   The positive evidence that it's aliens is still quite weak.
[01:35:20.480 --> 01:35:24.080]   The positive evidence that it's a puzzle is quite huge.
[01:35:24.080 --> 01:35:27.200]   - And whatever the solution to the puzzle is,
[01:35:27.200 --> 01:35:28.520]   it might be fascinating.
[01:35:28.520 --> 01:35:30.580]   - And it's gonna be so weird or fascinating,
[01:35:30.580 --> 01:35:33.560]   or maybe even trivial, but that's weird in its own way,
[01:35:33.560 --> 01:35:37.080]   that we can't set up by elimination
[01:35:37.080 --> 01:35:38.720]   all the things that might be able to be.
[01:35:38.720 --> 01:35:40.000]   - Yeah, and just like you said,
[01:35:40.000 --> 01:35:43.840]   the debunking that I've seen of these kinds of things
[01:35:43.840 --> 01:35:48.840]   are less explorations and solutions to the puzzle
[01:35:48.840 --> 01:35:52.840]   and more a kind of half-hearted dismissal.
[01:35:52.840 --> 01:35:56.600]   - And Avi, as you mentioned to him on your podcast with him,
[01:35:56.600 --> 01:35:58.800]   he's been attacked an awful lot.
[01:35:58.800 --> 01:36:01.280]   And when I hear the idea carrier attacked,
[01:36:01.280 --> 01:36:03.180]   I get very suspicious of the critics.
[01:36:03.180 --> 01:36:07.440]   If he's wrong, just tell me why.
[01:36:07.440 --> 01:36:08.960]   My ears are open.
[01:36:08.960 --> 01:36:11.320]   I don't have a set view on Oumuamua.
[01:36:11.320 --> 01:36:14.480]   You know, I know I can't judge Avi's arguments.
[01:36:14.480 --> 01:36:16.040]   He can't convince me in that sense.
[01:36:16.040 --> 01:36:19.080]   I'm too stupid to understand how good his argument
[01:36:19.080 --> 01:36:20.760]   may or may not be.
[01:36:20.760 --> 01:36:24.680]   - And like you said, ultimately, in the argument,
[01:36:24.680 --> 01:36:30.600]   in the meeting of that debate is where we find the wisdom.
[01:36:30.600 --> 01:36:32.240]   Like dismissing it, that's one of the things
[01:36:32.240 --> 01:36:33.080]   that troubles me.
[01:36:33.080 --> 01:36:33.900]   There's a bunch of people,
[01:36:33.900 --> 01:36:35.520]   like Nietzsche sometimes dismissed this way,
[01:36:35.520 --> 01:36:38.120]   Ayn Rand is sometimes dismissed this way.
[01:36:38.120 --> 01:36:38.960]   Oh, here we go.
[01:36:38.960 --> 01:36:43.120]   Like there's a, as opposed to arguing against her ideas,
[01:36:43.120 --> 01:36:44.920]   dismissing it outright.
[01:36:44.920 --> 01:36:48.340]   And that's not productive at all.
[01:36:48.340 --> 01:36:49.800]   She may be wrong in a lot of things,
[01:36:49.800 --> 01:36:52.920]   but like laying out some arguments,
[01:36:52.920 --> 01:36:54.840]   even if they're basic human arguments,
[01:36:54.840 --> 01:36:58.020]   that's where we arrive at the wisdom.
[01:36:58.020 --> 01:36:58.860]   I love that.
[01:36:58.860 --> 01:37:03.340]   Is there something deeper to be said
[01:37:03.340 --> 01:37:06.800]   about our trust in institutions and governments and so on
[01:37:06.800 --> 01:37:09.000]   that has to do with UFOs?
[01:37:09.000 --> 01:37:12.680]   That there's a kind of suspicion that the US government
[01:37:12.680 --> 01:37:15.680]   and governments in general are hiding stuff from us
[01:37:15.680 --> 01:37:17.360]   when you talk about UFOs.
[01:37:17.360 --> 01:37:19.380]   - This is my view on that.
[01:37:19.380 --> 01:37:21.440]   If we declassified everything,
[01:37:21.440 --> 01:37:23.360]   I think we would find a lot more evidence
[01:37:23.360 --> 01:37:25.760]   all pointing toward the same puzzle.
[01:37:25.760 --> 01:37:28.660]   There aren't some alien men being held underground.
[01:37:28.660 --> 01:37:30.600]   There's not some secret file that lays out
[01:37:30.600 --> 01:37:31.800]   whatever is happening.
[01:37:32.720 --> 01:37:34.940]   I think the real lesson about government
[01:37:34.940 --> 01:37:38.980]   is government cannot bring itself to any new belief
[01:37:38.980 --> 01:37:41.100]   on this matter of any kind.
[01:37:41.100 --> 01:37:42.820]   And it's a kind of funny inertia.
[01:37:42.820 --> 01:37:44.820]   Like government is deeply puzzled.
[01:37:44.820 --> 01:37:47.060]   They're more puzzled than they want to admit to us,
[01:37:47.060 --> 01:37:50.260]   which like, I'm okay with that actually.
[01:37:50.260 --> 01:37:53.660]   They shouldn't just be out panicking people in the streets.
[01:37:53.660 --> 01:37:54.700]   But at the end of the day,
[01:37:54.700 --> 01:37:57.620]   it's a bit like approving the AstraZeneca vaccine.
[01:37:57.620 --> 01:37:59.460]   Like which does work and they haven't approved it.
[01:37:59.460 --> 01:38:00.820]   Like when are they gonna do it?
[01:38:00.960 --> 01:38:05.960]   When is our government actually, if only internally,
[01:38:05.960 --> 01:38:08.720]   gonna take this more than just seriously,
[01:38:08.720 --> 01:38:10.880]   but like take it truly seriously?
[01:38:10.880 --> 01:38:13.560]   And I just don't know if we have that capability
[01:38:13.560 --> 01:38:16.320]   kind of mentally to sound like Eric Weinstein
[01:38:16.320 --> 01:38:17.780]   for another moment.
[01:38:17.780 --> 01:38:19.840]   (laughing)
[01:38:19.840 --> 01:38:21.640]   - To stay on the same topic,
[01:38:21.640 --> 01:38:23.840]   although on the surface shifting completely,
[01:38:23.840 --> 01:38:27.000]   because it is all the same topic,
[01:38:27.000 --> 01:38:29.040]   you have written and studied art.
[01:38:30.000 --> 01:38:34.800]   Why do you think we humans long to create art?
[01:38:34.800 --> 01:38:38.080]   Human society in general and just the human mind?
[01:38:38.080 --> 01:38:41.200]   - Well, most of us don't really long to create art, right?
[01:38:41.200 --> 01:38:42.960]   I would start with that point.
[01:38:42.960 --> 01:38:45.140]   - You think so?
[01:38:45.140 --> 01:38:47.760]   You think that's a unique weirdness
[01:38:47.760 --> 01:38:49.960]   of some particular humans?
[01:38:49.960 --> 01:38:52.920]   - I think, I don't know, 10% of humans roughly,
[01:38:52.920 --> 01:38:55.880]   which is a lot, but it is somewhat weird.
[01:38:55.880 --> 01:38:58.880]   I don't aspire to create art.
[01:38:58.880 --> 01:39:01.720]   You could say, like writing nonfiction,
[01:39:01.720 --> 01:39:03.520]   there's something art-like about it,
[01:39:03.520 --> 01:39:05.720]   but it's a different urge, I would say.
[01:39:05.720 --> 01:39:10.800]   So why do some people have it?
[01:39:10.800 --> 01:39:13.220]   I think human brains are very different.
[01:39:13.220 --> 01:39:17.100]   It's a different notion of working through a problem.
[01:39:17.100 --> 01:39:21.000]   Like you and I enjoy working through analytic problems.
[01:39:21.000 --> 01:39:23.440]   For me, economics, for you, AI and other areas,
[01:39:23.440 --> 01:39:26.880]   or your humanities podcast, but that's fun.
[01:39:27.720 --> 01:39:30.000]   For that problem to be visual
[01:39:30.000 --> 01:39:31.960]   and linked to physical materials
[01:39:31.960 --> 01:39:33.900]   and putting those on a canvas,
[01:39:33.900 --> 01:39:37.040]   to me, it's not a huge leap,
[01:39:37.040 --> 01:39:39.280]   but I really don't wanna do it.
[01:39:39.280 --> 01:39:45.800]   If you paid me like 500 bucks to spend an hour painting,
[01:39:45.800 --> 01:39:48.720]   I don't know, is that worth it?
[01:39:48.720 --> 01:39:51.920]   Maybe, but I'm happy when that hour's over.
[01:39:51.920 --> 01:39:56.080]   - And would not be proud or happy with the result.
[01:39:56.080 --> 01:39:57.680]   - It would suck.
[01:39:57.680 --> 01:40:00.240]   I don't think I would do it, actually.
[01:40:00.240 --> 01:40:03.480]   - Do you think you're suppressing some deep, I mean--
[01:40:03.480 --> 01:40:04.400]   - Absolutely not.
[01:40:04.400 --> 01:40:07.320]   Now, when I was young, I played the guitar,
[01:40:07.320 --> 01:40:09.560]   as you played the guitar, and that I greatly enjoyed,
[01:40:09.560 --> 01:40:11.560]   although I was never good.
[01:40:11.560 --> 01:40:14.880]   But it helped me appreciate music much, much more.
[01:40:14.880 --> 01:40:15.840]   - Well, this is the question, okay.
[01:40:15.840 --> 01:40:17.520]   So from the perspective of the observer
[01:40:17.520 --> 01:40:20.360]   and appreciator of art, you said good.
[01:40:20.360 --> 01:40:24.000]   Is there such a concept as good in art?
[01:40:24.000 --> 01:40:26.320]   - There's clearly a concept of bad.
[01:40:26.320 --> 01:40:28.480]   My guitar playing fit that concept.
[01:40:28.480 --> 01:40:30.440]   - Okay.
[01:40:30.440 --> 01:40:31.280]   - But I wasn't trying to be good.
[01:40:31.280 --> 01:40:33.600]   I wanted to learn, like, how do chords work?
[01:40:33.600 --> 01:40:34.440]   - Okay, analytical.
[01:40:34.440 --> 01:40:36.120]   - How does a jazz improvisation work?
[01:40:36.120 --> 01:40:37.840]   How is blues different?
[01:40:37.840 --> 01:40:39.440]   Classical guitar, sort of physically,
[01:40:39.440 --> 01:40:41.160]   how do you make those sounds?
[01:40:41.160 --> 01:40:42.400]   And I did learn those things,
[01:40:42.400 --> 01:40:44.840]   and you can't learn everything about them,
[01:40:44.840 --> 01:40:47.120]   but you can learn a lot about them without ever being good,
[01:40:47.120 --> 01:40:48.760]   or even trying to be that good.
[01:40:48.760 --> 01:40:51.080]   But I could play all the notes.
[01:40:51.080 --> 01:40:53.200]   - So from the observer perspective,
[01:40:53.200 --> 01:40:57.000]   what do you, I apologize for the absurd question,
[01:40:57.000 --> 01:40:58.760]   but what do you use the most beautiful
[01:40:58.760 --> 01:41:02.160]   and maybe moving piece of art you've encountered
[01:41:02.160 --> 01:41:03.160]   in your life?
[01:41:03.160 --> 01:41:05.800]   - It's not an absurd question at all.
[01:41:05.800 --> 01:41:07.560]   And I think about this quite a bit.
[01:41:07.560 --> 01:41:13.080]   I would say the two winners by a clear margin
[01:41:13.080 --> 01:41:15.160]   are both by Michelangelo.
[01:41:15.160 --> 01:41:17.640]   It's the Pieta in the Vatican
[01:41:17.640 --> 01:41:20.800]   and the David statue in Florence.
[01:41:20.800 --> 01:41:21.640]   - Why?
[01:41:21.640 --> 01:41:25.360]   Historical context or just purity, the creation itself?
[01:41:25.360 --> 01:41:26.800]   - I don't think you can view it apart
[01:41:26.800 --> 01:41:28.080]   from historical context.
[01:41:28.080 --> 01:41:30.400]   And being in Florence or in the Vatican,
[01:41:30.400 --> 01:41:32.800]   it's that you're already primed for a lot, right?
[01:41:32.800 --> 01:41:35.520]   You can't pull that out.
[01:41:35.520 --> 01:41:37.800]   But just technically how they express
[01:41:37.800 --> 01:41:40.120]   the emotion of human form,
[01:41:40.120 --> 01:41:42.200]   I do honestly intellectually think
[01:41:42.200 --> 01:41:45.240]   they're the two greatest artworks for doing that.
[01:41:45.240 --> 01:41:46.520]   That's not all that art does.
[01:41:46.520 --> 01:41:48.640]   Not all art is about the human form.
[01:41:48.640 --> 01:41:50.040]   But they are phenomenal.
[01:41:51.160 --> 01:41:52.880]   And I think critical opinion,
[01:41:52.880 --> 01:41:54.320]   not that everyone agrees,
[01:41:54.320 --> 01:41:56.480]   but my view is not considered a crazy one
[01:41:56.480 --> 01:41:58.960]   within the broader court of critical opinion.
[01:41:58.960 --> 01:42:02.880]   Now in painting, I think the most I was ever blown away
[01:42:02.880 --> 01:42:05.440]   was to see Vermeer's artwork.
[01:42:05.440 --> 01:42:07.160]   It's called the Art of Painting.
[01:42:07.160 --> 01:42:11.280]   And it's in Vienna in the Kunsthistorisches Museum.
[01:42:11.280 --> 01:42:13.320]   And I saw that, I think I was 23.
[01:42:13.320 --> 01:42:17.920]   It just stunned me because I'd seen reproductions
[01:42:17.920 --> 01:42:19.520]   but live in front of you in huge,
[01:42:19.520 --> 01:42:21.000]   a completely different artwork.
[01:42:21.000 --> 01:42:23.440]   - And again, Vienna, primed.
[01:42:23.440 --> 01:42:24.640]   - Yes.
[01:42:24.640 --> 01:42:26.800]   And I was living abroad for the first time
[01:42:26.800 --> 01:42:28.720]   and Vienna itself, the city and so on.
[01:42:28.720 --> 01:42:30.240]   Now, unlike the Michelangelo's,
[01:42:30.240 --> 01:42:32.320]   that is not my current favorite painting.
[01:42:32.320 --> 01:42:35.480]   But that would be like historically the one I would pick.
[01:42:35.480 --> 01:42:38.040]   - What do you make in the context of those choices?
[01:42:38.040 --> 01:42:39.680]   What do you make of modern art?
[01:42:39.680 --> 01:42:44.680]   And I apologize if I'm not using the correct terminology
[01:42:44.680 --> 01:42:49.680]   but art that maybe goes another level of weird
[01:42:50.520 --> 01:42:52.920]   outside of the art that you've kind of mentioned
[01:42:52.920 --> 01:42:55.680]   and breaks all the conventions and rules and so on
[01:42:55.680 --> 01:42:59.400]   and becomes something else entirely
[01:42:59.400 --> 01:43:03.880]   that doesn't make sense in the same way that David might.
[01:43:03.880 --> 01:43:05.680]   - I think a lot of it is phenomenal.
[01:43:05.680 --> 01:43:07.760]   And I would say the single biggest mistake
[01:43:07.760 --> 01:43:09.200]   that really smart people make
[01:43:09.200 --> 01:43:13.760]   is to think contemporary art or music for that matter
[01:43:13.760 --> 01:43:16.040]   is just a load of junk or rubbish.
[01:43:16.040 --> 01:43:17.600]   It's just like a kind of mathematics
[01:43:17.600 --> 01:43:18.800]   they haven't learned yet.
[01:43:18.800 --> 01:43:20.640]   It's really hard to learn.
[01:43:20.640 --> 01:43:23.280]   Maybe some people can never learn it.
[01:43:23.280 --> 01:43:26.200]   But there's a very large community of super smart,
[01:43:26.200 --> 01:43:28.800]   well-educated people who spend their lives with it,
[01:43:28.800 --> 01:43:29.720]   who love it.
[01:43:29.720 --> 01:43:30.920]   Those are genuine pleasures.
[01:43:30.920 --> 01:43:31.760]   They understand it.
[01:43:31.760 --> 01:43:34.240]   They talk about it with a common language.
[01:43:34.240 --> 01:43:36.120]   And to think that somehow they're all frauds,
[01:43:36.120 --> 01:43:37.480]   it just isn't true.
[01:43:37.480 --> 01:43:40.080]   Like one doesn't have to like it oneself,
[01:43:40.080 --> 01:43:42.320]   just like Love House may or may not be your thing,
[01:43:42.320 --> 01:43:43.560]   but it is amazing.
[01:43:43.560 --> 01:43:45.840]   And for me personally, highly rewarding.
[01:43:45.840 --> 01:43:47.960]   And if someone doesn't get it,
[01:43:47.960 --> 01:43:50.600]   I do kind of have the conceited response of thinking like,
[01:43:50.600 --> 01:43:54.080]   in that area, I'm just smarter than you are.
[01:43:54.080 --> 01:43:56.840]   - Yeah, so the interesting thing is, as with most-
[01:43:56.840 --> 01:43:58.560]   - We get back to Eric Weinstein again.
[01:43:58.560 --> 01:43:59.400]   - Yes.
[01:43:59.400 --> 01:44:01.040]   - Who is in general smarter than I am.
[01:44:01.040 --> 01:44:02.240]   This I get.
[01:44:02.240 --> 01:44:05.120]   But when it comes to contemporary artistic creations,
[01:44:05.120 --> 01:44:06.760]   I'm smarter than he is.
[01:44:06.760 --> 01:44:08.840]   - So he's not a fan of contemporary art?
[01:44:08.840 --> 01:44:10.240]   - I don't want to speak for him.
[01:44:10.240 --> 01:44:11.080]   I've heard him say-
[01:44:11.080 --> 01:44:12.720]   - He's evolving always.
[01:44:12.720 --> 01:44:13.840]   - He's evolving always.
[01:44:13.840 --> 01:44:16.200]   I've heard him say derogatory things about some of it.
[01:44:16.200 --> 01:44:18.760]   Doesn't mean he doesn't love some other parts of it.
[01:44:18.760 --> 01:44:22.360]   - So I wonder if there's just a higher learning curve,
[01:44:22.360 --> 01:44:24.840]   a steeper learning curve for contemporary art.
[01:44:24.840 --> 01:44:28.680]   Meaning like it takes more work to appreciate the stories,
[01:44:28.680 --> 01:44:32.480]   the context from which they're like thinking about this work.
[01:44:32.480 --> 01:44:35.520]   It feels like in order to appreciate the art,
[01:44:35.520 --> 01:44:37.720]   contemporary, certain pieces of contemporary art,
[01:44:37.720 --> 01:44:41.280]   you have to know the story better behind the art.
[01:44:41.280 --> 01:44:42.920]   - I think that's true for many people,
[01:44:42.920 --> 01:44:45.480]   but I think it's a funny shape distribution
[01:44:45.480 --> 01:44:47.800]   because there's a whole other set of people,
[01:44:47.800 --> 01:44:49.520]   sometimes just small children,
[01:44:49.520 --> 01:44:51.760]   and they get abstract art more easily.
[01:44:51.760 --> 01:44:56.400]   You show them Vermeer or Rembrandt, they don't get it.
[01:44:56.400 --> 01:45:00.400]   But just like a wall of color, they're in love with it.
[01:45:00.400 --> 01:45:03.120]   So I don't think I know the full story.
[01:45:03.120 --> 01:45:04.760]   Again, some strange kind of distribution.
[01:45:04.760 --> 01:45:07.880]   The entry barriers are super high or super low,
[01:45:07.880 --> 01:45:09.360]   but not that often in between.
[01:45:09.360 --> 01:45:13.880]   - But you would challenge saying that there's a lot
[01:45:13.880 --> 01:45:15.480]   to be explored in contemporary art.
[01:45:15.480 --> 01:45:20.480]   It's just you need to learn.
[01:45:20.480 --> 01:45:22.400]   - Yeah, it's one of the most profound bodies
[01:45:22.400 --> 01:45:25.600]   of human thought out there, and it's part of the humanities.
[01:45:25.600 --> 01:45:28.560]   And yes, there are people who also don't like podcasts,
[01:45:28.560 --> 01:45:29.400]   right?
[01:45:29.400 --> 01:45:31.360]   And that's fine.
[01:45:31.360 --> 01:45:32.560]   - Yeah.
[01:45:32.560 --> 01:45:35.520]   You've also been a scholar of food.
[01:45:35.520 --> 01:45:37.040]   We're just going through the entirety
[01:45:37.040 --> 01:45:40.040]   of the human experience today on this humanities podcast.
[01:45:42.600 --> 01:45:45.480]   Another absurd question, say this conversation
[01:45:45.480 --> 01:45:47.120]   is the last thing you ever do in your life.
[01:45:47.120 --> 01:45:49.480]   I, wearing the suit, would murder you
[01:45:49.480 --> 01:45:50.720]   at the end of the conversation.
[01:45:50.720 --> 01:45:52.680]   So this is your last day on Earth,
[01:45:52.680 --> 01:45:54.560]   but I would offer you a last meal.
[01:45:54.560 --> 01:45:57.240]   What would that meal contain?
[01:45:57.240 --> 01:45:59.360]   We can also travel to other parts of the world.
[01:45:59.360 --> 01:46:02.920]   - Well, we have to travel because my preferred last meal
[01:46:02.920 --> 01:46:05.840]   here I probably had like two nights ago.
[01:46:05.840 --> 01:46:06.920]   - Which is what?
[01:46:06.920 --> 01:46:08.600]   Can you describe or no?
[01:46:08.600 --> 01:46:11.320]   - The best restaurant around here is called Mama Chang's,
[01:46:11.320 --> 01:46:16.120]   and it's in Fairfax, and it's food from Wuhan, actually.
[01:46:16.120 --> 01:46:18.440]   And they take pandemic safety seriously
[01:46:18.440 --> 01:46:20.440]   in addition to the food being very good.
[01:46:20.440 --> 01:46:23.080]   But this is what I would do.
[01:46:23.080 --> 01:46:27.480]   I would fly to Hermosillo in Northern Mexico,
[01:46:27.480 --> 01:46:29.240]   which has some of the best food in Mexico,
[01:46:29.240 --> 01:46:32.000]   but I sadly only had two days there.
[01:46:32.000 --> 01:46:33.840]   So somewhere like Oaxaca, Puebla,
[01:46:33.840 --> 01:46:37.520]   I think they have food just as good,
[01:46:37.520 --> 01:46:38.840]   or some people would say better,
[01:46:38.840 --> 01:46:41.240]   but I've spent a lot of time in those places.
[01:46:41.240 --> 01:46:43.440]   - So the scarce, wait, is it possible
[01:46:43.440 --> 01:46:44.920]   the scarcity of time contributed
[01:46:44.920 --> 01:46:46.720]   to the richness of the experience?
[01:46:46.720 --> 01:46:50.040]   - Of course, but the point is that scarcity still holds.
[01:46:50.040 --> 01:46:53.400]   So I want one more dose of the food from Hermosillo.
[01:46:53.400 --> 01:46:55.640]   - Can you describe what the food is?
[01:46:55.640 --> 01:46:57.240]   - It's the one kind of Mexican food
[01:46:57.240 --> 01:46:59.480]   that at least nominally is just like the Mexican food
[01:46:59.480 --> 01:47:00.760]   you get in the US.
[01:47:00.760 --> 01:47:02.880]   So there are burritos, there's fajitas.
[01:47:02.880 --> 01:47:05.440]   It doesn't taste at all like our stuff.
[01:47:05.440 --> 01:47:08.200]   But again, nominally, it's the part of Mexican food
[01:47:08.200 --> 01:47:11.000]   that made it into the US, was then transformed.
[01:47:11.000 --> 01:47:11.840]   - Yes.
[01:47:11.840 --> 01:47:13.360]   - But it's in a way the most familiar,
[01:47:13.360 --> 01:47:16.240]   but for that reason, it's the most radical
[01:47:16.240 --> 01:47:18.680]   because you have to rethink all these things you know,
[01:47:18.680 --> 01:47:21.080]   and they're way better in Hermosillo.
[01:47:21.080 --> 01:47:22.560]   Hardly any tourists go there.
[01:47:22.560 --> 01:47:24.800]   Like there's nothing to see in Hermosillo.
[01:47:24.800 --> 01:47:26.800]   Nothing you do other than eat.
[01:47:26.800 --> 01:47:29.280]   It's not ruined by any outsiders.
[01:47:29.280 --> 01:47:33.000]   It's this longstanding tradition, dirt cheap.
[01:47:33.000 --> 01:47:36.280]   And the thing to do there is just sweet talk a taxi driver
[01:47:36.280 --> 01:47:38.520]   into first taking you seriously,
[01:47:38.520 --> 01:47:40.280]   and then trusting you enough
[01:47:40.280 --> 01:47:42.320]   to know that you trust him to bring you
[01:47:42.320 --> 01:47:44.760]   to the very best like food stands.
[01:47:44.760 --> 01:47:48.920]   - So where's the magic of that
[01:47:48.920 --> 01:47:53.440]   nominally similar entity of the burrito?
[01:47:53.440 --> 01:47:54.840]   Where's the magic come from?
[01:47:54.840 --> 01:47:55.680]   What is it?
[01:47:55.680 --> 01:47:56.720]   Is it the taxi ride?
[01:47:56.720 --> 01:47:57.920]   Is it the whole experience?
[01:47:57.920 --> 01:47:59.920]   Or is there something actually in the food?
[01:47:59.920 --> 01:48:02.280]   - So well, you can break the food down part by part.
[01:48:02.280 --> 01:48:03.960]   So if you think of the beef,
[01:48:03.960 --> 01:48:07.480]   the beef there will be dry aged, just out in the air.
[01:48:07.480 --> 01:48:10.640]   In a way the FDA here would never permit.
[01:48:10.640 --> 01:48:12.520]   Like they dry age it till it turns green,
[01:48:12.520 --> 01:48:14.400]   but it is phenomenal.
[01:48:14.400 --> 01:48:16.320]   The quality of the chilies.
[01:48:16.320 --> 01:48:17.760]   So here there's only a small number
[01:48:17.760 --> 01:48:19.680]   of kinds of chilies you can get.
[01:48:19.680 --> 01:48:21.640]   In most parts of Mexico,
[01:48:21.640 --> 01:48:24.160]   there's quite a large number of chilies you can get.
[01:48:24.160 --> 01:48:26.080]   They're different, they're fresher,
[01:48:26.080 --> 01:48:28.720]   but it's just like a different thing, the chilies.
[01:48:28.720 --> 01:48:33.720]   The wheat used, so this is wheat territory,
[01:48:33.720 --> 01:48:36.120]   not corn territory, which is itself interesting.
[01:48:37.440 --> 01:48:39.720]   The wheat is more diverse and more complex.
[01:48:39.720 --> 01:48:41.120]   Here it's more homogenized,
[01:48:41.120 --> 01:48:43.280]   obviously cheaper, more efficient,
[01:48:43.280 --> 01:48:44.760]   but there it is better.
[01:48:44.760 --> 01:48:50.240]   Non-pasteurized cheeses are legal in all parts of Mexico,
[01:48:50.240 --> 01:48:52.920]   and they can be white and gooey and amazing
[01:48:52.920 --> 01:48:55.680]   in a way that here, again, it's just against the law.
[01:48:55.680 --> 01:48:56.640]   You could legalize them.
[01:48:56.640 --> 01:48:57.800]   The demand wouldn't be that great.
[01:48:57.800 --> 01:48:59.320]   There's a black market in these cheeses,
[01:48:59.320 --> 01:49:01.640]   that Latino groceries around here,
[01:49:01.640 --> 01:49:03.440]   but you just can't get that much of it.
[01:49:03.440 --> 01:49:05.680]   So the cheese, the meat, the wheat,
[01:49:05.680 --> 01:49:09.320]   all different in significant ways.
[01:49:09.320 --> 01:49:13.120]   The chilies, I don't think the onions really matter much.
[01:49:13.120 --> 01:49:14.120]   Garlic, I don't know.
[01:49:14.120 --> 01:49:15.880]   I wouldn't put much stock in that,
[01:49:15.880 --> 01:49:18.720]   but that's a lot of the core food,
[01:49:18.720 --> 01:49:20.080]   and then it's cooked much better,
[01:49:20.080 --> 01:49:22.360]   and everything's super fresh.
[01:49:22.360 --> 01:49:25.320]   The food chain is not relying on refrigeration,
[01:49:25.320 --> 01:49:28.680]   and this is one thing Russia and US have in common.
[01:49:28.680 --> 01:49:31.600]   We were early pioneers in food refrigeration,
[01:49:31.600 --> 01:49:34.280]   and that made a lot of our foods worse quite early,
[01:49:34.280 --> 01:49:37.280]   and it took us a long time to dig out of that
[01:49:37.280 --> 01:49:38.880]   'cause big countries, right?
[01:49:38.880 --> 01:49:42.480]   You've had an extensive rail system in Russia,
[01:49:42.480 --> 01:49:44.320]   USSR, a long time,
[01:49:44.320 --> 01:49:46.560]   which makes it easier to freeze and then ship.
[01:49:46.560 --> 01:49:50.640]   >> What about the actual cooking, the chef?
[01:49:50.640 --> 01:49:53.840]   Is there an artistry to the simple?
[01:49:53.840 --> 01:49:57.120]   I hesitate to call the burrito simple, but--
[01:49:57.120 --> 01:49:58.760]   >> And there's no brain drain out of cooking,
[01:49:58.760 --> 01:50:02.280]   so if you're in the United States,
[01:50:02.280 --> 01:50:03.520]   and you're very talented,
[01:50:03.520 --> 01:50:05.280]   I'm not saying there aren't talented chefs.
[01:50:05.280 --> 01:50:06.800]   Of course there are,
[01:50:06.800 --> 01:50:09.640]   but there's so many other things to pull people away,
[01:50:09.640 --> 01:50:12.320]   but in Mexico, there's so much talent going into food,
[01:50:12.320 --> 01:50:14.120]   as there is in China,
[01:50:14.120 --> 01:50:17.440]   which would be another candidate for last meal questions,
[01:50:17.440 --> 01:50:18.280]   or India.
[01:50:18.280 --> 01:50:21.360]   >> Oh, India, let's not even get started on India.
[01:50:21.360 --> 01:50:22.200]   >> Unbelievable.
[01:50:22.200 --> 01:50:25.440]   >> There's a million things we could talk about here,
[01:50:25.440 --> 01:50:27.720]   but you've written about your dreams of sushi.
[01:50:28.700 --> 01:50:30.640]   This is just a really clean, good example
[01:50:30.640 --> 01:50:31.760]   that people are aware of,
[01:50:31.760 --> 01:50:36.760]   of mastery in the art of the simple in food.
[01:50:36.760 --> 01:50:41.480]   What do you make of that kind of obsessive pursuit
[01:50:41.480 --> 01:50:45.040]   of perfection in creating simple food?
[01:50:45.040 --> 01:50:46.800]   >> Sushi is about perfection,
[01:50:46.800 --> 01:50:48.440]   but it's a bit like the Beatles' "White Album,"
[01:50:48.440 --> 01:50:51.520]   which people think is simple and not overproduced.
[01:50:51.520 --> 01:50:54.320]   It's in a funny way their most overproduced album,
[01:50:54.320 --> 01:50:56.000]   but it's produced just perfectly.
[01:50:56.000 --> 01:50:57.280]   It sounds simple.
[01:50:57.280 --> 01:50:59.860]   It's really hard to produce music to the point
[01:50:59.860 --> 01:51:01.580]   where it's going to sound so simple
[01:51:01.580 --> 01:51:03.140]   and not sound like sludge.
[01:51:03.140 --> 01:51:06.140]   Like "Let It Be" album, has some great songs,
[01:51:06.140 --> 01:51:07.620]   but a lot of it sounds like sludge.
[01:51:07.620 --> 01:51:09.700]   "One After 909," that's sludge.
[01:51:09.700 --> 01:51:11.580]   "I Dig a Pony," it's sludge.
[01:51:11.580 --> 01:51:13.020]   Like it's a bit interesting.
[01:51:13.020 --> 01:51:15.660]   It's not that good, it doesn't sound that good.
[01:51:15.660 --> 01:51:18.140]   "White Album," like the best half, like "Dear Prudence,"
[01:51:18.140 --> 01:51:21.180]   sounds perfect, sounds simple, cry baby cry.
[01:51:21.180 --> 01:51:25.180]   It's not simple, back in the USSR, super complex.
[01:51:25.180 --> 01:51:26.920]   So sushi is like that.
[01:51:26.920 --> 01:51:29.600]   It's because it's so incredibly not simple,
[01:51:29.600 --> 01:51:31.520]   starting with the rice.
[01:51:31.520 --> 01:51:34.440]   You try to refine it to make it appear super simple,
[01:51:34.440 --> 01:51:37.020]   and that's the most complex thing of all.
[01:51:37.020 --> 01:51:40.080]   - So do you admire, I mean,
[01:51:40.080 --> 01:51:43.920]   we're not talking about days, weeks, months.
[01:51:43.920 --> 01:51:46.540]   We're talking about years, generations
[01:51:46.540 --> 01:51:49.240]   of doing the same thing over and over and over again.
[01:51:49.240 --> 01:51:51.480]   Do you admire that kind of sticking to the,
[01:51:53.040 --> 01:51:56.080]   we talked about our admiration of the weird.
[01:51:56.080 --> 01:51:57.540]   That doesn't feel weird.
[01:51:57.540 --> 01:52:01.440]   That seems like discipline and dedication
[01:52:01.440 --> 01:52:05.320]   to like a stoic minimalism or something like that.
[01:52:05.320 --> 01:52:07.960]   - I'm happy they do it, but I actually feel bad about it.
[01:52:07.960 --> 01:52:10.880]   I feel they're sacrificial victims to me,
[01:52:10.880 --> 01:52:12.640]   which I benefit from.
[01:52:12.640 --> 01:52:13.660]   But don't you ever think like,
[01:52:13.660 --> 01:52:16.840]   "Gee, you're a great master sushi chef.
[01:52:16.840 --> 01:52:20.320]   "Wouldn't you be happier if you did something else?"
[01:52:21.780 --> 01:52:23.500]   It doesn't seem to happen.
[01:52:23.500 --> 01:52:25.700]   - That might be something that a weird mind would think.
[01:52:25.700 --> 01:52:27.260]   - Maybe it is weird people,
[01:52:27.260 --> 01:52:29.840]   and maybe they're really enjoying it.
[01:52:29.840 --> 01:52:32.880]   But to learn how to pack rice for 10 years
[01:52:32.880 --> 01:52:35.220]   before they let you do anything else,
[01:52:35.220 --> 01:52:37.880]   it's like these Indian sarod players.
[01:52:37.880 --> 01:52:40.440]   They just spent five years tapping at rhythms
[01:52:40.440 --> 01:52:43.480]   before they're allowed to touch their instruments.
[01:52:43.480 --> 01:52:46.280]   - Well, actually, to defend that--
[01:52:46.280 --> 01:52:48.520]   - It's kind of like graduate school, right?
[01:52:48.520 --> 01:52:50.640]   - Well, I think graduate school,
[01:52:50.640 --> 01:52:54.920]   perhaps, graduate school is full of,
[01:52:54.920 --> 01:52:58.500]   like every single day is full of surprises, I would say.
[01:52:58.500 --> 01:53:03.320]   I did martial arts for a long time, I do martial arts.
[01:53:03.320 --> 01:53:06.960]   And I've always loved kind of the Russian way of drilling
[01:53:06.960 --> 01:53:09.200]   is doing the same technique.
[01:53:09.200 --> 01:53:10.520]   I don't know if this applies
[01:53:10.520 --> 01:53:13.280]   in intellectual or academic disciplines,
[01:53:13.280 --> 01:53:15.040]   where you can do the same thing
[01:53:15.040 --> 01:53:16.960]   over and over and over again,
[01:53:16.960 --> 01:53:19.960]   thousands and thousands and thousands of times.
[01:53:19.960 --> 01:53:23.640]   What I've discovered through that process
[01:53:23.640 --> 01:53:27.280]   is you get to start to appreciate the tiniest of details
[01:53:27.280 --> 01:53:29.440]   and find the beauty in them.
[01:53:29.440 --> 01:53:33.280]   People who go to monasteries to meditate talk about this,
[01:53:33.280 --> 01:53:37.420]   is when you just sit in silence and don't do anything,
[01:53:37.420 --> 01:53:41.360]   you start to appreciate how much complexity and beauty
[01:53:41.360 --> 01:53:43.080]   there is in just the movement of a finger.
[01:53:43.080 --> 01:53:46.060]   Like you can spend the whole day joyously thinking
[01:53:46.060 --> 01:53:49.520]   about how fun it is to move a finger.
[01:53:49.520 --> 01:53:50.360]   - Yeah.
[01:53:50.360 --> 01:53:53.160]   - And so, and then you can almost become
[01:53:53.160 --> 01:53:56.960]   your full weird self about the tiniest details of life.
[01:53:56.960 --> 01:53:58.000]   - The thing you've got to wonder,
[01:53:58.000 --> 01:53:59.800]   like is there a free lunch in there?
[01:53:59.800 --> 01:54:01.920]   Are the rest of us moving around too much?
[01:54:01.920 --> 01:54:02.840]   - Yeah, exactly.
[01:54:02.840 --> 01:54:06.640]   They sure feel like they found a free lunch.
[01:54:06.640 --> 01:54:09.160]   The people meditate, they're onto something.
[01:54:09.160 --> 01:54:11.240]   - I tend to think it's like artists,
[01:54:11.240 --> 01:54:14.760]   that some percent of people are like that, but most are not.
[01:54:14.760 --> 01:54:16.800]   And for most of us, there's no free lunch.
[01:54:16.800 --> 01:54:19.100]   Like my free lunch is to move around a lot
[01:54:19.100 --> 01:54:20.720]   in search of lunch, in fact.
[01:54:20.720 --> 01:54:22.080]   (both laughing)
[01:54:22.080 --> 01:54:24.480]   - Well, with all the food talk, you made me hungry.
[01:54:24.480 --> 01:54:29.480]   What books, three or so books, if any come to mind,
[01:54:29.480 --> 01:54:33.520]   technical fiction, philosophical,
[01:54:33.520 --> 01:54:37.360]   would you recommend had a big impact on you,
[01:54:37.360 --> 01:54:40.400]   or you just drew some insights from throughout your life?
[01:54:40.400 --> 01:54:42.360]   - Well, two of them we've already discussed.
[01:54:42.360 --> 01:54:44.760]   One is Plato's Dialogues,
[01:54:44.760 --> 01:54:47.600]   which I started reading when I was like 13.
[01:54:47.600 --> 01:54:51.100]   Another is Ayn Rand, Capitalism, the Unknown Ideal.
[01:54:51.100 --> 01:54:53.880]   But I would say the Friedrich Hayek essay,
[01:54:53.880 --> 01:54:56.260]   The Use of Knowledge in Society,
[01:54:56.260 --> 01:54:58.940]   which is about how decentralized mechanisms can work,
[01:54:58.940 --> 01:55:01.060]   also why they might go wrong.
[01:55:01.060 --> 01:55:03.300]   And that's where you start to understand
[01:55:03.300 --> 01:55:05.220]   the price system, capitalism.
[01:55:05.220 --> 01:55:06.260]   And that was in a book called
[01:55:06.260 --> 01:55:08.020]   Individualism and Economic Order,
[01:55:08.020 --> 01:55:10.580]   but it was just a few essays in that book.
[01:55:10.580 --> 01:55:12.700]   Those are maybe the three I would say.
[01:55:12.700 --> 01:55:14.580]   - Can you elaborate a little bit on the--
[01:55:14.580 --> 01:55:16.380]   - Say the price of copper goes up, right?
[01:55:16.380 --> 01:55:18.560]   Because there's a problem with a copper mine
[01:55:18.560 --> 01:55:20.680]   in Chile or Bolivia.
[01:55:20.680 --> 01:55:22.780]   So the price of copper goes up all around the world.
[01:55:22.780 --> 01:55:24.900]   People are led to economize copper,
[01:55:24.900 --> 01:55:26.780]   to look for substitutes for copper,
[01:55:26.780 --> 01:55:28.880]   to change their production processes,
[01:55:28.880 --> 01:55:31.140]   to change the goods and services they buy,
[01:55:31.140 --> 01:55:32.700]   to build homes a different way.
[01:55:32.700 --> 01:55:37.160]   And this one event creates this one tiny change
[01:55:37.160 --> 01:55:41.280]   in information, it gets into your AI work very directly.
[01:55:41.280 --> 01:55:44.840]   And how much complexity that one change engenders
[01:55:44.840 --> 01:55:47.200]   in a meaningful, coherent way,
[01:55:47.200 --> 01:55:50.840]   how the different pieces of the price system fit together.
[01:55:50.840 --> 01:55:54.000]   Hayek really laid out very clearly.
[01:55:54.000 --> 01:55:56.440]   And it's like an AI problem.
[01:55:56.440 --> 01:55:58.060]   And how well, not for everything,
[01:55:58.060 --> 01:56:00.960]   but for many things, we solve that AI problem.
[01:56:00.960 --> 01:56:04.840]   I learned, I was I think 13, maybe 14 when I read Hayek.
[01:56:04.840 --> 01:56:07.400]   - Yeah, the distributed nature of things there.
[01:56:07.400 --> 01:56:09.160]   - And it's like your work on human attention,
[01:56:09.160 --> 01:56:10.640]   like how much can we take in?
[01:56:10.640 --> 01:56:11.720]   - Yes.
[01:56:11.720 --> 01:56:13.880]   - Very often not that much.
[01:56:13.880 --> 01:56:16.560]   - And how many of the advances of modern civilization
[01:56:16.560 --> 01:56:19.600]   you need to understand as a response to that constraint.
[01:56:19.600 --> 01:56:21.360]   I got that also from Hayek.
[01:56:21.360 --> 01:56:23.160]   - What's the title of the book again?
[01:56:23.160 --> 01:56:26.120]   - It's reprinted in a lot of books at this point.
[01:56:26.120 --> 01:56:27.520]   But back then the book was called
[01:56:27.520 --> 01:56:30.560]   "Individualism and Economic Order."
[01:56:30.560 --> 01:56:31.760]   But the essay's online,
[01:56:31.760 --> 01:56:34.580]   Hayek, Use of Knowledge in Society.
[01:56:34.580 --> 01:56:37.400]   There are open access versions of it through Google.
[01:56:37.400 --> 01:56:39.120]   And you don't need the whole book.
[01:56:39.120 --> 01:56:40.360]   So it's a very good book.
[01:56:42.200 --> 01:56:46.160]   - Again, one of those profound looking over the ocean,
[01:56:46.160 --> 01:56:49.700]   maybe sitting on a porch, maybe with a drink of some kind,
[01:56:49.700 --> 01:56:55.040]   and a young kid comes by and asks you for advice.
[01:56:55.040 --> 01:56:56.640]   What advice would you give to--
[01:56:56.640 --> 01:56:58.600]   - A drink, that's my advice.
[01:56:58.600 --> 01:56:59.440]   I'm serious.
[01:56:59.440 --> 01:57:03.120]   - So, okay, after that,
[01:57:03.120 --> 01:57:09.420]   what advice would you give to a young person today
[01:57:09.420 --> 01:57:10.880]   as they take on life?
[01:57:10.880 --> 01:57:14.760]   Whether career and academia in general,
[01:57:14.760 --> 01:57:19.340]   or just a life, which is probably more important than career.
[01:57:19.340 --> 01:57:22.040]   - Most good advice is context specific,
[01:57:22.040 --> 01:57:24.560]   but here are my two generic pieces of advice.
[01:57:24.560 --> 01:57:25.400]   - Good.
[01:57:25.400 --> 01:57:27.200]   - First, get a mentor.
[01:57:27.200 --> 01:57:29.080]   Both career, but anything you want to learn.
[01:57:29.080 --> 01:57:31.560]   Like say you want to learn about contemporary art.
[01:57:31.560 --> 01:57:33.040]   People write me this.
[01:57:33.040 --> 01:57:34.360]   What book should I read?
[01:57:34.360 --> 01:57:36.400]   It's probably not going to work that way.
[01:57:36.400 --> 01:57:37.220]   You need a mentor.
[01:57:37.220 --> 01:57:39.120]   Yes, you should read some books on it,
[01:57:39.120 --> 01:57:40.800]   but you want a mentor to help you frame them,
[01:57:40.800 --> 01:57:43.880]   take you around to some art, talk about it with you.
[01:57:43.880 --> 01:57:45.560]   So get as many mentors as you can
[01:57:45.560 --> 01:57:47.640]   in the things you want to learn.
[01:57:47.640 --> 01:57:48.480]   And then--
[01:57:48.480 --> 01:57:51.140]   - Can I ask you a quick tangent on that?
[01:57:51.140 --> 01:57:54.360]   Presumably a good mentor.
[01:57:54.360 --> 01:57:55.200]   - Of course.
[01:57:55.200 --> 01:57:56.800]   I'm begging the question in there.
[01:57:56.800 --> 01:57:59.280]   It's complicated, right?
[01:57:59.280 --> 01:58:00.280]   - Well, it is complicated.
[01:58:00.280 --> 01:58:03.360]   Is there a lot of damage to be done from a bad mentor?
[01:58:03.360 --> 01:58:04.280]   - I don't think that much,
[01:58:04.280 --> 01:58:06.000]   because it's very easy to drop mentors,
[01:58:06.000 --> 01:58:07.940]   and in fact, it's quite hard to maintain them.
[01:58:07.940 --> 01:58:09.600]   Good mentors tend to be busy.
[01:58:09.600 --> 01:58:11.200]   Bad mentors tend to be busy.
[01:58:11.200 --> 01:58:14.200]   And you can try on mentors,
[01:58:14.200 --> 01:58:15.440]   and maybe they're not good for you,
[01:58:15.440 --> 01:58:17.940]   but there's a good chance you'll learn something.
[01:58:17.940 --> 01:58:21.040]   Like I had a mentor, I was an undergrad.
[01:58:21.040 --> 01:58:22.600]   He was a Stalinist.
[01:58:22.600 --> 01:58:24.680]   He edited the book called "The Essential Stalin."
[01:58:24.680 --> 01:58:25.720]   Brilliant guy.
[01:58:25.720 --> 01:58:28.200]   I learned a tremendous amount from him.
[01:58:28.200 --> 01:58:30.640]   Was he like as a Stalinist, a good mentor for me,
[01:58:30.640 --> 01:58:31.480]   fan of Hayek?
[01:58:31.480 --> 01:58:34.140]   Well, no, but for a year, it was tremendous.
[01:58:34.140 --> 01:58:38.240]   - Yeah.
[01:58:38.240 --> 01:58:40.200]   - He introduced me to Soviet
[01:58:40.200 --> 01:58:41.800]   and Eastern European science fiction,
[01:58:41.800 --> 01:58:43.760]   'cause he was a Marxist.
[01:58:43.760 --> 01:58:45.480]   That's what I took from him, among other things.
[01:58:45.480 --> 01:58:47.720]   - Any advice on finding a good mentor?
[01:58:47.720 --> 01:58:52.720]   Daniel Kahneman, as somebody just popped into mind,
[01:58:52.720 --> 01:58:54.320]   as somebody who was able to find
[01:58:54.320 --> 01:58:57.120]   exceptionally good collaborators throughout his life,
[01:58:57.120 --> 01:59:00.520]   there's not many bright minds that find collaborators.
[01:59:00.520 --> 01:59:06.480]   Which I ultimately see what a mentor is.
[01:59:06.480 --> 01:59:07.300]   - Yeah.
[01:59:07.300 --> 01:59:08.920]   - Be interesting, be direct, and try.
[01:59:08.920 --> 01:59:10.200]   (laughs)
[01:59:10.200 --> 01:59:11.640]   It's not like a perfect formula,
[01:59:11.640 --> 01:59:12.920]   but it's amazing how many people
[01:59:12.920 --> 01:59:14.240]   don't even do those things.
[01:59:14.240 --> 01:59:17.540]   - Be interesting, be direct, and try.
[01:59:17.540 --> 01:59:20.120]   - Like what you want from a better known person,
[01:59:20.120 --> 01:59:22.200]   I would just say be very direct with them.
[01:59:22.200 --> 01:59:23.040]   - Yeah.
[01:59:23.040 --> 01:59:24.760]   Beautiful.
[01:59:24.760 --> 01:59:26.520]   What's the second piece of advice?
[01:59:26.520 --> 01:59:29.300]   - Build small groups of peers.
[01:59:29.300 --> 01:59:30.760]   They don't have to be your age,
[01:59:30.760 --> 01:59:32.360]   but very often they'll be your age,
[01:59:32.360 --> 01:59:33.840]   especially if you're younger,
[01:59:33.840 --> 01:59:35.360]   with broadly similar interests,
[01:59:35.360 --> 01:59:37.280]   but there can be different points of view.
[01:59:37.280 --> 01:59:39.000]   People you hang out with,
[01:59:39.000 --> 01:59:41.800]   which can include in a WhatsApp group online,
[01:59:41.800 --> 01:59:43.880]   and like every day or almost every day,
[01:59:43.880 --> 01:59:46.200]   they're talking about the thing you care about,
[01:59:46.200 --> 01:59:48.800]   trying to solve problems in that thing,
[01:59:48.800 --> 01:59:49.960]   and that's your small group,
[01:59:49.960 --> 01:59:51.780]   and you really like them, and they like you,
[01:59:51.780 --> 01:59:53.960]   and you care what you think about each other,
[01:59:53.960 --> 01:59:55.920]   and you have this common interest.
[01:59:55.920 --> 01:59:57.260]   - That's for human connection,
[01:59:57.260 --> 01:59:58.880]   or that's for development of ideas?
[01:59:58.880 --> 02:00:01.420]   - It's both, they're not that different.
[02:00:01.420 --> 02:00:03.880]   Like Beatles, classic small group, right?
[02:00:03.880 --> 02:00:06.600]   - But there's so much drama.
[02:00:06.600 --> 02:00:08.560]   - The Florentine artists, of course there's drama,
[02:00:08.560 --> 02:00:10.920]   and small groups tend to split up, which is fine,
[02:00:10.920 --> 02:00:13.340]   just like entering relationships often end,
[02:00:13.340 --> 02:00:16.880]   but it's remarkable how little has been done
[02:00:16.880 --> 02:00:19.920]   that was not done in small groups in some way.
[02:00:19.920 --> 02:00:24.920]   - So speaking of loss of beautiful relationships,
[02:00:24.920 --> 02:00:28.280]   why do you make this whole love thing?
[02:00:28.280 --> 02:00:31.680]   Why do humans fall in love?
[02:00:31.680 --> 02:00:36.000]   What's the role of love, friendship, family in life?
[02:00:37.000 --> 02:00:40.120]   In a successful life, or just life in general?
[02:00:40.120 --> 02:00:42.200]   Why the hell are we so into this thing?
[02:00:42.200 --> 02:00:44.920]   - There are multiple layers of understanding that question.
[02:00:44.920 --> 02:00:48.720]   So kind of the lowest layer is the Darwinian answer.
[02:00:48.720 --> 02:00:51.800]   If we weren't this way, we wouldn't have been successful
[02:00:51.800 --> 02:00:54.520]   in reproducing and building alliances.
[02:00:54.520 --> 02:00:57.680]   It's important to realize that's far from complete.
[02:00:57.680 --> 02:01:00.040]   Sort of the highest understanding would be poetic,
[02:01:00.040 --> 02:01:03.720]   like read John Keats, or many other love poets.
[02:01:03.720 --> 02:01:08.080]   - So who do I go to to learn about love in terms of poets?
[02:01:08.080 --> 02:01:10.120]   - I would say start with John Keats,
[02:01:10.120 --> 02:01:12.600]   but given that you're fluent in Russian.
[02:01:12.600 --> 02:01:16.320]   - Yeah, let's go Russian literature for a second.
[02:01:16.320 --> 02:01:18.640]   You keep mentioning Russia.
[02:01:18.640 --> 02:01:23.640]   What's your connection, what's your love in Russia?
[02:01:23.640 --> 02:01:26.440]   - Well, first, it's all interesting,
[02:01:26.440 --> 02:01:29.400]   but more concretely, my wife was born in Moscow.
[02:01:29.400 --> 02:01:31.520]   Sokolniki was her neighborhood.
[02:01:31.520 --> 02:01:32.360]   - Oh, wow. - Yeah.
[02:01:32.360 --> 02:01:34.200]   - Oh, okay. - And she grew up there.
[02:01:34.200 --> 02:01:35.560]   I married her here.
[02:01:35.560 --> 02:01:38.360]   My daughter, I adopted her.
[02:01:38.360 --> 02:01:41.280]   I'm not her biological father, but I genuinely raised her.
[02:01:41.280 --> 02:01:42.560]   She was born in Russia,
[02:01:42.560 --> 02:01:44.440]   though she came here when she was one.
[02:01:44.440 --> 02:01:45.520]   - Wow.
[02:01:45.520 --> 02:01:47.320]   - My father-in-law-- - So you're basically Russian.
[02:01:47.320 --> 02:01:49.220]   - No, no, no, I'm a New Jersey boy.
[02:01:49.220 --> 02:01:51.200]   - That's the same thing.
[02:01:51.200 --> 02:01:52.280]   - I'm very sorry to report,
[02:01:52.280 --> 02:01:54.840]   my father-in-law passed away a week ago.
[02:01:54.840 --> 02:01:57.360]   He lived with us for six years.
[02:01:57.360 --> 02:02:01.520]   He lived in Russia till he was, oh, 70?
[02:02:01.520 --> 02:02:04.240]   Saw the Stalinist era.
[02:02:04.240 --> 02:02:05.840]   His father was brought to a camp,
[02:02:05.840 --> 02:02:09.640]   lived through World War II, much, much more,
[02:02:09.640 --> 02:02:11.880]   had an incredible life,
[02:02:11.880 --> 02:02:14.540]   never really learned how to speak English.
[02:02:14.540 --> 02:02:17.960]   So I absorbed something Russian from him as well.
[02:02:17.960 --> 02:02:19.300]   He was part Armenian.
[02:02:19.300 --> 02:02:22.040]   So that's my connection to Russia.
[02:02:22.040 --> 02:02:24.120]   - A bit of the Russian soul, too.
[02:02:24.120 --> 02:02:25.560]   Do you-- - I don't think I have it.
[02:02:25.560 --> 02:02:27.600]   I think I appreciate it.
[02:02:27.600 --> 02:02:29.060]   But there's division of labor, right?
[02:02:29.060 --> 02:02:30.060]   Others in the family.
[02:02:30.060 --> 02:02:31.400]   (laughing)
[02:02:31.400 --> 02:02:32.240]   Take care of that.
[02:02:32.240 --> 02:02:34.200]   I'm more superficial.
[02:02:34.200 --> 02:02:37.000]   - You mentioned Keats and that higher version,
[02:02:37.000 --> 02:02:38.480]   that non-Darwinian love.
[02:02:38.480 --> 02:02:39.760]   What's that about?
[02:02:39.760 --> 02:02:42.400]   - That it's the highest form of human connection,
[02:02:42.400 --> 02:02:46.920]   and it's intoxicating, and it's part of building a life,
[02:02:46.920 --> 02:02:50.400]   and most of us are very, very strongly drawn to it.
[02:02:50.400 --> 02:02:52.600]   And it's part of the highest realization
[02:02:52.600 --> 02:02:54.720]   of you being what you can be.
[02:02:54.720 --> 02:02:55.560]   - Yeah.
[02:02:55.560 --> 02:02:58.400]   You mentioned you lost-- - But ask a Russian.
[02:02:58.720 --> 02:03:01.520]   This is superficial New Jersey boy
[02:03:01.520 --> 02:03:03.880]   who grew up listening to Bruce Springsteen,
[02:03:03.880 --> 02:03:05.280]   and that was his romanticism.
[02:03:05.280 --> 02:03:09.680]   - What's your favorite Bruce Springsteen song?
[02:03:09.680 --> 02:03:12.320]   - I think the album "Born to Run"
[02:03:12.320 --> 02:03:14.160]   has actually held up the best.
[02:03:14.160 --> 02:03:16.600]   Though it's very fashionable to think the earlier
[02:03:16.600 --> 02:03:18.440]   or later works are actually better,
[02:03:18.440 --> 02:03:21.160]   and that's the overproduced super pop album.
[02:03:21.160 --> 02:03:23.320]   But the quality of the songs, to me, "Born to Run"
[02:03:23.320 --> 02:03:25.460]   is just far and away the best.
[02:03:25.460 --> 02:03:27.620]   Then "Darkness on the Edge of Town."
[02:03:27.620 --> 02:03:29.800]   And those are still my favorites.
[02:03:29.800 --> 02:03:32.080]   "Born to Run" is an incredible song.
[02:03:32.080 --> 02:03:33.120]   - Yeah.
[02:03:33.120 --> 02:03:36.520]   - And perfectly produced in a Phil Spector kind of way.
[02:03:36.520 --> 02:03:38.960]   Every detail is right, every lyric.
[02:03:38.960 --> 02:03:40.600]   - What else is on the album?
[02:03:40.600 --> 02:03:43.560]   - "Thunder Road," "Jungle Land," "10th Avenue Freezeout."
[02:03:43.560 --> 02:03:45.360]   She's the one, unbelievable.
[02:03:45.360 --> 02:03:48.920]   - Yeah, Bruce is-- - "Meeting Across the River."
[02:03:48.920 --> 02:03:53.920]   - I really like, I like when he goes into love, personally.
[02:03:55.880 --> 02:03:57.820]   Like, "I'm on fire."
[02:03:57.820 --> 02:04:00.380]   - That's a very good song, "Dancing in the Dark."
[02:04:00.380 --> 02:04:02.460]   A lot of the later work, I find the percussion
[02:04:02.460 --> 02:04:05.940]   becomes too simple and kind of too white, somehow.
[02:04:05.940 --> 02:04:08.140]   And a little clunky.
[02:04:08.140 --> 02:04:10.680]   And it's still good work, he's super talented,
[02:04:10.680 --> 02:04:13.100]   but it doesn't speak to me.
[02:04:13.100 --> 02:04:16.060]   But when it all bursts open into the open road,
[02:04:16.060 --> 02:04:18.420]   like it does on "Born to Run," that's magic.
[02:04:18.420 --> 02:04:20.540]   - Yeah.
[02:04:20.540 --> 02:04:21.380]   - Or Rosalita.
[02:04:21.380 --> 02:04:22.620]   - Have you ever seen him live?
[02:04:22.620 --> 02:04:24.940]   - Is it, yes, twice.
[02:04:25.880 --> 02:04:28.680]   - I wonder what he's like live when he was young, right?
[02:04:28.680 --> 02:04:29.520]   Those years.
[02:04:29.520 --> 02:04:31.280]   - I saw him live when he was young.
[02:04:31.280 --> 02:04:32.120]   I was young.
[02:04:32.120 --> 02:04:34.500]   New Jersey.
[02:04:34.500 --> 02:04:36.560]   I was a little disappointed, actually.
[02:04:36.560 --> 02:04:37.400]   - Yeah?
[02:04:37.400 --> 02:04:41.240]   - I think what I like best from him is quite studio.
[02:04:41.240 --> 02:04:44.080]   He certainly played well, I don't fault his performance.
[02:04:44.080 --> 02:04:46.040]   But it's like when I saw "Plant and Page,"
[02:04:46.040 --> 02:04:47.600]   you know, of Led Zeppelin.
[02:04:47.600 --> 02:04:49.000]   Tremendous creators.
[02:04:49.000 --> 02:04:50.440]   And they showed up, they were not drunk,
[02:04:50.440 --> 02:04:52.160]   like they were paying attention.
[02:04:52.160 --> 02:04:55.220]   But I was underwhelmed, because Led Zeppelin,
[02:04:55.220 --> 02:04:56.700]   like the Beatles' "White Album,"
[02:04:56.700 --> 02:04:59.780]   is much more of a studio band than you think at first.
[02:04:59.780 --> 02:05:01.260]   - And in the case of Bruce Springsteen,
[02:05:01.260 --> 02:05:03.260]   I don't know about you, but for me,
[02:05:03.260 --> 02:05:05.980]   he's somebody that I connect with the most
[02:05:05.980 --> 02:05:10.140]   when I'm alone and there's like a melancholy feeling.
[02:05:10.140 --> 02:05:12.420]   And actually, my folks live in Philly.
[02:05:12.420 --> 02:05:14.900]   I went to school in Philly.
[02:05:14.900 --> 02:05:16.460]   And so, you know, I've...
[02:05:16.460 --> 02:05:20.060]   - You're almost worthy of New Jersey, then.
[02:05:20.060 --> 02:05:24.040]   - Yeah, well, you're almost worthy of Russia.
[02:05:24.040 --> 02:05:27.880]   So we're, we can connect in that aspect.
[02:05:27.880 --> 02:05:28.720]   I mean, I love Jersey.
[02:05:28.720 --> 02:05:31.800]   It's something I feel like,
[02:05:31.800 --> 02:05:35.960]   I feel like, I don't know, it always,
[02:05:35.960 --> 02:05:38.240]   there's this beautiful, like there's a old girls' diner
[02:05:38.240 --> 02:05:39.080]   that closed down.
[02:05:39.080 --> 02:05:41.560]   I used to go there.
[02:05:41.560 --> 02:05:43.880]   There's a melancholy feeling to me.
[02:05:43.880 --> 02:05:44.720]   I mean, of course--
[02:05:44.720 --> 02:05:47.800]   - A thickness to culture in that part of the world.
[02:05:47.800 --> 02:05:49.980]   - Which is oddly similar to some elements
[02:05:49.980 --> 02:05:51.960]   of the thickness of Russian culture.
[02:05:51.960 --> 02:05:55.620]   And when you see like Russian characters on "The Sopranos,"
[02:05:55.620 --> 02:05:56.720]   it totally makes sense,
[02:05:56.720 --> 02:05:59.480]   even though there are these complete outlines.
[02:05:59.480 --> 02:06:01.880]   - Exactly, it totally makes sense.
[02:06:01.880 --> 02:06:06.220]   You've, you mentioned you lost your father-in-law last week.
[02:06:06.220 --> 02:06:09.660]   Do you think about mortality?
[02:06:09.660 --> 02:06:12.520]   Do you think about your own mortality?
[02:06:12.520 --> 02:06:14.900]   Are you afraid of death?
[02:06:14.900 --> 02:06:17.420]   - I don't think about my own mortality that much,
[02:06:17.420 --> 02:06:19.120]   which is probably a good thing.
[02:06:19.120 --> 02:06:22.440]   I think death will be bad.
[02:06:22.440 --> 02:06:24.000]   I wouldn't say I'm afraid of it.
[02:06:24.000 --> 02:06:25.520]   For me, the worst thing about death
[02:06:25.520 --> 02:06:28.760]   is not knowing how the human story turns out.
[02:06:28.760 --> 02:06:29.880]   - The full human story.
[02:06:29.880 --> 02:06:30.760]   - The full human story.
[02:06:30.760 --> 02:06:32.880]   So if I could, right before I die,
[02:06:32.880 --> 02:06:36.560]   read like a Wikipedia page called "The Rest of Human History"
[02:06:36.560 --> 02:06:38.280]   and have enough time, just like a few days,
[02:06:38.280 --> 02:06:40.280]   to absorb it, think about it,
[02:06:40.280 --> 02:06:43.300]   and know like, oh, well, 643 years from now,
[02:06:43.300 --> 02:06:45.120]   that's when all the atomic weapons went off.
[02:06:45.120 --> 02:06:47.980]   And here's what happened between now and then.
[02:06:47.980 --> 02:06:49.620]   I would feel much better dying.
[02:06:49.620 --> 02:06:53.960]   But that's not how it's gonna be, right?
[02:06:53.960 --> 02:06:55.080]   It's unlikely.
[02:06:55.080 --> 02:06:57.080]   - It's almost like the "Hitchhiker's Guide."
[02:06:57.080 --> 02:06:58.840]   They kind of have, what is it?
[02:06:58.840 --> 02:07:02.000]   They have a one or two sentence description of the human,
[02:07:02.000 --> 02:07:03.640]   of what goes on on Earth.
[02:07:03.640 --> 02:07:05.080]   It's kind of interesting to think
[02:07:05.080 --> 02:07:08.080]   if there's a lot of intelligent civilizations out there
[02:07:08.080 --> 02:07:11.040]   that in the big encyclopedia that describes the universe,
[02:07:11.040 --> 02:07:13.240]   humans will only have one sentence, maybe two.
[02:07:13.240 --> 02:07:14.180]   - Probably true.
[02:07:14.180 --> 02:07:16.000]   - Yeah.
[02:07:16.000 --> 02:07:18.640]   - But it's the only one I can read and understand, right?
[02:07:18.640 --> 02:07:20.800]   And it may be hard to understand the human one
[02:07:20.800 --> 02:07:22.880]   past a number of centuries.
[02:07:22.880 --> 02:07:24.600]   - Yeah, with AI, yes.
[02:07:24.600 --> 02:07:29.100]   - Like how many years from now will reading Wikipedia
[02:07:29.100 --> 02:07:31.160]   be like trying to read Chaucer?
[02:07:31.160 --> 02:07:33.400]   Which I almost can do, but I actually can't.
[02:07:33.400 --> 02:07:34.880]   I need a translation.
[02:07:34.880 --> 02:07:36.400]   Probably you can't do it at all.
[02:07:36.400 --> 02:07:37.600]   - Yeah.
[02:07:37.600 --> 02:07:39.400]   I mean, maybe reading will be outdated.
[02:07:39.400 --> 02:07:41.680]   It might be a very silly notion.
[02:07:41.680 --> 02:07:43.560]   Maybe we're fundamentally,
[02:07:43.560 --> 02:07:46.080]   like we think language is fundamental to cognition,
[02:07:46.080 --> 02:07:47.400]   but it could be something visual
[02:07:47.400 --> 02:07:50.280]   or something totally different that we'll plug in.
[02:07:50.280 --> 02:07:51.700]   - Neural anchor, yeah.
[02:07:51.700 --> 02:07:55.840]   - But in that story, that Wikipedia article,
[02:07:55.840 --> 02:08:00.840]   do you think there'll be a section on the meaning of it?
[02:08:00.840 --> 02:08:01.840]   - I hope not.
[02:08:01.840 --> 02:08:04.820]   'Cause that section we could write now,
[02:08:04.820 --> 02:08:07.280]   and it's just not gonna be very good, right?
[02:08:07.280 --> 02:08:08.640]   - What would you put in the section
[02:08:08.640 --> 02:08:11.640]   on the meaning of human existence?
[02:08:11.640 --> 02:08:13.840]   - I don't know, links to a lot of other sections?
[02:08:13.840 --> 02:08:15.960]   (both laughing)
[02:08:15.960 --> 02:08:17.440]   I don't think there are general statements
[02:08:17.440 --> 02:08:19.880]   about the meaning of life that have that much meaning.
[02:08:19.880 --> 02:08:22.040]   I think if you study different cultures,
[02:08:22.040 --> 02:08:23.840]   the arts, travel, mathematics,
[02:08:23.840 --> 02:08:25.440]   like whatever your thing is,
[02:08:25.440 --> 02:08:27.240]   you'll get a lot about the meaning of life.
[02:08:27.240 --> 02:08:30.680]   So like it's there in Wikipedia in some bigger sense.
[02:08:30.680 --> 02:08:32.200]   But I don't wanna read the page on the meaning.
[02:08:32.200 --> 02:08:34.320]   I bet they have such a page, in fact.
[02:08:34.320 --> 02:08:36.040]   The fact that I've never visited it,
[02:08:36.040 --> 02:08:37.740]   none of my friends, oh, here, Tyler,
[02:08:37.740 --> 02:08:39.120]   here's the page on the meaning of life.
[02:08:39.120 --> 02:08:40.360]   I know you've been wondering about this.
[02:08:40.360 --> 02:08:41.600]   You gotta read this one.
[02:08:41.600 --> 02:08:43.320]   No one's ever done that to you, have they?
[02:08:43.320 --> 02:08:46.040]   - It probably has, well, I actually gone to that page.
[02:08:46.040 --> 02:08:48.040]   It does in fact have a lot of links to other pages.
[02:08:48.040 --> 02:08:51.020]   - Okay. (laughing)
[02:08:51.020 --> 02:08:52.520]   - So that's it.
[02:08:52.520 --> 02:08:57.200]   The meaning of life is just a bunch of self-referential
[02:08:57.200 --> 02:09:02.200]   or citation needed type of statements.
[02:09:02.200 --> 02:09:03.880]   I think there's no better way to end it.
[02:09:03.880 --> 02:09:05.120]   Tyler, this is a huge honor.
[02:09:05.120 --> 02:09:06.300]   I'm a huge fan.
[02:09:07.520 --> 02:09:10.080]   Thank you so much for wasting all of this time with me.
[02:09:10.080 --> 02:09:12.000]   It was one of the greatest conversations I've ever had.
[02:09:12.000 --> 02:09:12.840]   Thank you so much.
[02:09:12.840 --> 02:09:15.240]   - My pleasure and delighted to finally have met you
[02:09:15.240 --> 02:09:16.860]   and that we can do this.
[02:09:16.860 --> 02:09:20.280]   - Thanks for listening to this conversation with Tyler Cohen
[02:09:20.280 --> 02:09:23.560]   and thank you to Linode, ExpressVPN,
[02:09:23.560 --> 02:09:25.960]   SimplySafe and Public Goods.
[02:09:25.960 --> 02:09:29.600]   Check them out in the description to support this podcast.
[02:09:29.600 --> 02:09:33.680]   And now let me leave you with some words from Adam Smith.
[02:09:33.680 --> 02:09:36.520]   "Little else is requisite to carry a state
[02:09:36.520 --> 02:09:38.520]   "to the highest degree of opulence
[02:09:38.520 --> 02:09:42.600]   "from the lowest barbarism, but peace, easy taxes
[02:09:42.600 --> 02:09:45.920]   "and a tolerable administration of justice."
[02:09:45.920 --> 02:09:48.920]   Thank you for listening and hope to see you next time.
[02:09:48.920 --> 02:09:51.500]   (upbeat music)
[02:09:51.500 --> 02:09:54.080]   (upbeat music)
[02:09:54.080 --> 02:10:04.080]   [BLANK_AUDIO]

