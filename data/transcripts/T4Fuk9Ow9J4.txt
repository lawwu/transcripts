
[00:00:00.000 --> 00:00:07.120]   One of the challenges in the healthcare space is often you don't get the answer to, "Did
[00:00:07.120 --> 00:00:09.240]   this treatment solve the problem?"
[00:00:09.240 --> 00:00:13.720]   You either get, "Nothing happened after that," or, "Maybe I went to a different doctor somewhere
[00:00:13.720 --> 00:00:18.360]   and you just don't have the data," or, "Maybe I didn't take my meds because I didn't pick
[00:00:18.360 --> 00:00:19.800]   them up," or whatever else.
[00:00:19.800 --> 00:00:24.040]   But there's a lot of challenges in the healthcare space of actually getting good data sets in
[00:00:24.040 --> 00:00:25.880]   order to do machine learning.
[00:00:25.880 --> 00:00:30.240]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:30.240 --> 00:00:32.440]   and I'm your host, Lukas Biewald.
[00:00:32.440 --> 00:00:37.960]   Alyssa Simpson-Rockwerger is an old friend and colleague and expert on real-world AI.
[00:00:37.960 --> 00:00:42.880]   She's currently the director of product at Blue Shield California, and before that, she
[00:00:42.880 --> 00:00:49.720]   was VP of AI at Appen and Figurate, the company that I founded and ran for a decade and sold
[00:00:49.720 --> 00:00:51.080]   to Appen.
[00:00:51.080 --> 00:00:54.460]   Before that, she was director of product at IBM Watson, where she was an important partner
[00:00:54.460 --> 00:00:56.420]   for Figurate.
[00:00:56.420 --> 00:01:00.600]   She has over a decade of experience in machine learning, and she's the author of the book
[00:01:00.600 --> 00:01:05.760]   Real-World AI, a Practical Guide for Responsible Machine Learning, which covers basically everything
[00:01:05.760 --> 00:01:08.520]   we talk about here in this podcast every week.
[00:01:08.520 --> 00:01:12.080]   I'm super excited to catch up with her and talk to her today.
[00:01:12.080 --> 00:01:13.840]   Tell me about your work on the vaccine.
[00:01:13.840 --> 00:01:16.720]   I'm dying to hear about it.
[00:01:16.720 --> 00:01:17.720]   Sure.
[00:01:17.720 --> 00:01:21.880]   At the end of January, you may have heard, Blue Shield got asked to help with the vaccine
[00:01:21.880 --> 00:01:26.560]   rollout in California, and I was privileged enough to get a phone call, I think, the following
[00:01:26.560 --> 00:01:31.800]   Saturday from one of our senior executives saying, "Hey, Alyssa, can you come help?
[00:01:31.800 --> 00:01:32.800]   What are you doing right now?
[00:01:32.800 --> 00:01:35.960]   Can you join a meeting with the state either today or tomorrow?"
[00:01:35.960 --> 00:01:38.740]   I said, "Sure, Jeff.
[00:01:38.740 --> 00:01:39.740]   You bet."
[00:01:39.740 --> 00:01:45.400]   It was supposed to be a two, three-week thing, and I think this is week 12, where I've completely
[00:01:45.400 --> 00:01:49.280]   dropped my day job on the floor and just helping out the state.
[00:01:49.280 --> 00:01:53.880]   There's a team of us that has been deployed full-time, and it's been an absolute whirlwind
[00:01:53.880 --> 00:01:57.480]   and privilege and I'm really excited.
[00:01:57.480 --> 00:01:59.120]   What are you doing practically?
[00:01:59.120 --> 00:02:00.120]   What's the-
[00:02:00.120 --> 00:02:01.120]   Yeah.
[00:02:01.120 --> 00:02:03.420]   Have you heard of myturn.ca.gov?
[00:02:03.420 --> 00:02:06.440]   Which if you haven't, go get your vaccine, schedule it.
[00:02:06.440 --> 00:02:11.280]   There's a website where everyone in California can get vaccinated through and schedule appointments.
[00:02:11.280 --> 00:02:16.700]   We've been coordinating enhancements to that, working with the 61 different local health
[00:02:16.700 --> 00:02:18.600]   jurisdictions in California.
[00:02:18.600 --> 00:02:24.080]   Each one has a slightly different set of challenges and opportunities.
[00:02:24.080 --> 00:02:29.400]   For example, the Bay Area, we have really low hesitancy rates and a lot of really eager
[00:02:29.400 --> 00:02:33.040]   people who are willing to drive three hours to get their vaccine.
[00:02:33.040 --> 00:02:38.900]   Whereas down in Southern California at the moment, we have more supply and we're starting
[00:02:38.900 --> 00:02:40.880]   to experience more hesitancy.
[00:02:40.880 --> 00:02:46.280]   There's appointments availability pretty easily, hard to reach communities that are not interested
[00:02:46.280 --> 00:02:48.720]   or not able to access the vaccine.
[00:02:48.720 --> 00:02:53.160]   We put a really heavy focus on equity and making sure the people who need the vaccine
[00:02:53.160 --> 00:02:56.280]   most get it first and are able to access it.
[00:02:56.280 --> 00:03:01.640]   This week, it's all about homebound populations, people who can't leave their houses, how do
[00:03:01.640 --> 00:03:03.360]   you get vaccine to them?
[00:03:03.360 --> 00:03:10.280]   When these things come in thousand plus dose things, you got to thaw them out.
[00:03:10.280 --> 00:03:12.560]   Pfizer is a deep cold freezer situation.
[00:03:12.560 --> 00:03:14.840]   It could only be at room temperature for so many hours.
[00:03:14.840 --> 00:03:20.680]   If you are an ambulance worker going to a homebound person's house, you need special
[00:03:20.680 --> 00:03:26.280]   training to understand exactly how to administer this and how many houses can you go to before
[00:03:26.280 --> 00:03:28.440]   the vaccine expires.
[00:03:28.440 --> 00:03:32.000]   Logistically, super complicated.
[00:03:32.000 --> 00:03:35.280]   I'm helping on the operations and tech team.
[00:03:35.280 --> 00:03:41.480]   Everything from doing data analysis to understand where should we ship vaccine and who do we
[00:03:41.480 --> 00:03:45.200]   get it to, to helping onboard providers.
[00:03:45.200 --> 00:03:50.920]   I think we've contracted with over 3,000 plus or more providers in the state of California.
[00:03:50.920 --> 00:03:57.720]   Kaiser is a massive one, Sutter Health, Dignity, but there's a long tail of much smaller clinics
[00:03:57.720 --> 00:03:58.720]   and providers.
[00:03:58.720 --> 00:04:04.600]   I think there's over 1,500 clinics on my turn that are giving out vaccine across the state.
[00:04:04.600 --> 00:04:09.720]   Different challenges in Tulare County versus Alpine County versus the Moscone Center in
[00:04:09.720 --> 00:04:14.280]   San Francisco and the logistics of making sure everyone in California gets vaccinated.
[00:04:14.280 --> 00:04:17.680]   So there's a lot to do.
[00:04:17.680 --> 00:04:21.760]   At this point, is it mostly the logistical problem, just getting the vaccine to the person
[00:04:21.760 --> 00:04:24.720]   that wants the vaccine or are there other-
[00:04:24.720 --> 00:04:27.360]   So there's a lot of challenges.
[00:04:27.360 --> 00:04:32.940]   The three big things that are sort of limits to getting shots in arms are supply.
[00:04:32.940 --> 00:04:35.920]   So the first several months of this have been supply constrained.
[00:04:35.920 --> 00:04:38.660]   We only get so much supply for the federal government.
[00:04:38.660 --> 00:04:43.400]   The other potential constraints are ability to administer vaccine.
[00:04:43.400 --> 00:04:48.400]   So that was what we focused on really heavily for the first month and a half or so.
[00:04:48.400 --> 00:04:53.280]   The third party administrator for California is making sure that we could build up a network
[00:04:53.280 --> 00:05:00.240]   of providers who had the logistical capability to receive supply of vaccine and administer
[00:05:00.240 --> 00:05:01.240]   it, right?
[00:05:01.240 --> 00:05:05.880]   So you need nurses, you need security guards, you need freezers, you need ability to mass
[00:05:05.880 --> 00:05:11.000]   vax or whatever it is, but some of these are mobile clinics going into agricultural communities.
[00:05:11.000 --> 00:05:14.000]   Some of these are how do you get the word out to people?
[00:05:14.000 --> 00:05:16.560]   So all that sort of capacity.
[00:05:16.560 --> 00:05:19.360]   And then the last problem is willingness, right?
[00:05:19.360 --> 00:05:21.960]   You need people arms to put shots into.
[00:05:21.960 --> 00:05:24.560]   And so some of that is a hesitancy problem.
[00:05:24.560 --> 00:05:27.280]   Some of that is ability to schedule an appointment, right?
[00:05:27.280 --> 00:05:29.780]   So 40% of California speaks Spanish.
[00:05:29.780 --> 00:05:35.520]   And then there's a long tail of other languages, Vietnamese, Chinese, Hmong, and how do you
[00:05:35.520 --> 00:05:40.600]   address and reach all of those communities, not just logistically support them with making
[00:05:40.600 --> 00:05:45.600]   an appointment if they want to, but also helping them understand that the vaccine is good and
[00:05:45.600 --> 00:05:48.720]   safe and they should show up and get an appointment.
[00:05:48.720 --> 00:05:54.280]   So curve balls get thrown like J&J, no longer being administrated.
[00:05:54.280 --> 00:05:57.560]   So that was last week, I think we found out at like 6am or something.
[00:05:57.560 --> 00:06:02.920]   And by three hours later, we were able to switch the supply to, I think there were like
[00:06:02.920 --> 00:06:05.520]   8,500 appointments, the 48 hours.
[00:06:05.520 --> 00:06:10.400]   And we had to switch to either Moderna or Pfizer for the vast majority and then reschedule
[00:06:10.400 --> 00:06:13.080]   a handful of those appointments.
[00:06:13.080 --> 00:06:17.200]   And I guess as a data person, did you have feelings about the J&J decision?
[00:06:17.200 --> 00:06:19.240]   Are you even allowed to talk about it?
[00:06:19.240 --> 00:06:21.040]   Oh, I have no insider information.
[00:06:21.040 --> 00:06:22.720]   I read the news just like you do.
[00:06:22.720 --> 00:06:29.040]   I assume that the really incredible scientists and doctors who have been making this vaccine
[00:06:29.040 --> 00:06:33.600]   and diligently testing it and following the quality control protocols, it's a good thing
[00:06:33.600 --> 00:06:38.840]   that they're pausing and reviewing it and looking thoroughly.
[00:06:38.840 --> 00:06:43.800]   I have plenty of loved ones who've received the J&J vaccine and so far they've all been
[00:06:43.800 --> 00:06:47.600]   good and haven't had any problems, knock on wood, but I'm really glad that everyone's
[00:06:47.600 --> 00:06:48.600]   taking it super seriously.
[00:06:48.600 --> 00:06:49.600]   That's good.
[00:06:49.600 --> 00:06:54.120]   I guess the main thing that I was planning to talk to you about was the book that you
[00:06:54.120 --> 00:06:55.120]   wrote.
[00:06:55.120 --> 00:06:56.120]   I wrote a book.
[00:06:56.120 --> 00:06:57.120]   Yeah, you wrote a book.
[00:06:57.120 --> 00:06:58.120]   Congratulations.
[00:06:58.200 --> 00:07:03.160]   It feels like real world AI is really what, as long as I've known your career, what you've
[00:07:03.160 --> 00:07:04.160]   been working on.
[00:07:04.160 --> 00:07:07.560]   So it does seem like you would be the person to write this book.
[00:07:07.560 --> 00:07:12.000]   I'll say one thing as an aside, I'm always impressed by people that are able to write
[00:07:12.000 --> 00:07:13.000]   a book without feedback.
[00:07:13.000 --> 00:07:14.000]   Was it a challenging process?
[00:07:14.000 --> 00:07:15.720]   I mean, how did that go?
[00:07:15.720 --> 00:07:19.640]   I was voluntold to write a book, which I think I've said before.
[00:07:19.640 --> 00:07:22.040]   So it was a fascinating process.
[00:07:22.040 --> 00:07:24.080]   I had a lot of help.
[00:07:24.080 --> 00:07:25.080]   Great team.
[00:07:25.080 --> 00:07:26.080]   I'm dyslexic.
[00:07:26.080 --> 00:07:27.520]   Could certainly not have written a book by myself.
[00:07:27.520 --> 00:07:28.520]   Are you actually dyslexic?
[00:07:28.520 --> 00:07:29.520]   Yeah.
[00:07:29.520 --> 00:07:30.520]   Oh, yeah.
[00:07:30.520 --> 00:07:31.520]   Interesting.
[00:07:31.520 --> 00:07:32.520]   Yeah.
[00:07:32.520 --> 00:07:33.520]   How does that?
[00:07:33.520 --> 00:07:34.520]   Extra time in school, the whole thing.
[00:07:34.520 --> 00:07:35.520]   Really?
[00:07:35.520 --> 00:07:36.520]   Yeah.
[00:07:36.520 --> 00:07:37.520]   Wow.
[00:07:37.520 --> 00:07:38.520]   Man, working with you, I never noticed anything like that.
[00:07:38.520 --> 00:07:43.680]   Dyslexia is an umbrella term.
[00:07:43.680 --> 00:07:45.400]   It means a lot of different things for a lot of different people.
[00:07:45.400 --> 00:07:49.120]   So me and my sister, both dyslexic, totally different manifestations.
[00:07:49.120 --> 00:07:52.200]   I cannot spell to save my life, as an example.
[00:07:52.200 --> 00:07:55.000]   There are always typos and issues in everything.
[00:07:55.000 --> 00:07:57.480]   Every email I send and I don't even see it.
[00:07:57.480 --> 00:08:01.120]   My sister is an outstanding speller, math, not her strength.
[00:08:01.120 --> 00:08:02.720]   So our issues are different.
[00:08:02.720 --> 00:08:07.160]   To be diagnosed with dyslexic, you have to score above average or pretty high in certain
[00:08:07.160 --> 00:08:11.600]   categories and then average or below average in other categories.
[00:08:11.600 --> 00:08:16.600]   And the delta between those in enough categories is what classifies you as dyslexic.
[00:08:16.600 --> 00:08:21.560]   So person to person, you could score high or low in totally different areas.
[00:08:21.560 --> 00:08:23.760]   So I would imagine that would make it even harder to write a book.
[00:08:23.760 --> 00:08:26.320]   Sorry, it already seems very hard to me.
[00:08:26.320 --> 00:08:27.320]   Yes.
[00:08:27.320 --> 00:08:30.120]   So writing the book, it was a great, really interesting process.
[00:08:30.120 --> 00:08:31.400]   It took a long time.
[00:08:31.400 --> 00:08:38.040]   So we started out with an idea of what we wanted to do and organized that into an outline
[00:08:38.040 --> 00:08:41.560]   and then started fleshing out those outlines and interviewed you.
[00:08:41.560 --> 00:08:44.240]   Thank you so much for your interview and contributing to it.
[00:08:44.240 --> 00:08:47.520]   And then lots of other folks who were willing to share their stories about what it's like
[00:08:47.520 --> 00:08:54.360]   to actually build and deploy machine learning based technology in the real world for real
[00:08:54.360 --> 00:08:58.080]   actual use cases and not kind of BS hype.
[00:08:58.080 --> 00:09:02.200]   And so what's great about the machine learning community is that people are really nice and
[00:09:02.200 --> 00:09:05.400]   they want to share their stories and they want to help others is kind of what I found
[00:09:05.400 --> 00:09:06.400]   really consistently.
[00:09:06.400 --> 00:09:13.120]   And not every story were we able or authorized to use publicly and put in a book.
[00:09:13.120 --> 00:09:16.400]   There's a lot of lessons learned and we had to anonymize quite a few.
[00:09:16.400 --> 00:09:19.200]   But a bunch we could, so it was awesome.
[00:09:19.200 --> 00:09:23.160]   But the process is you do an outline and then you talk through each story, each chapter
[00:09:23.160 --> 00:09:28.400]   kind of one by one, and then you go back and reorganize information or content that sort
[00:09:28.400 --> 00:09:30.920]   of makes sense in perhaps multiple places.
[00:09:30.920 --> 00:09:35.560]   And our editing team knows how to write books and they do this all day long for a living.
[00:09:35.560 --> 00:09:43.360]   So turn sort of word vomit from Alyssa and Wilson into actual paragraphs and sentences.
[00:09:43.360 --> 00:09:47.480]   It seemed like you, I feel like you had a kind of a focus as you do in your career on
[00:09:47.480 --> 00:09:50.840]   kind of ethics and responsible AI.
[00:09:50.840 --> 00:09:52.920]   Was it kind of hard to get people to talk about that?
[00:09:52.920 --> 00:09:57.160]   It is in the Zeitgeist, but I wonder if it's hard to get real world stories of like tricky
[00:09:57.160 --> 00:09:58.160]   issues.
[00:09:58.160 --> 00:09:59.160]   Yeah.
[00:09:59.160 --> 00:10:05.760]   It's easy to talk about at an abstract level, easy to talk off the record with people around
[00:10:05.760 --> 00:10:12.160]   lessons learned and challenges, harder to get them to go on record about failures and
[00:10:12.160 --> 00:10:17.120]   specifics of those failures in large public companies.
[00:10:17.120 --> 00:10:20.800]   Can you talk about some of the failures and what happened, like some of the anecdotes
[00:10:20.800 --> 00:10:22.000]   that you have in your book?
[00:10:22.000 --> 00:10:23.000]   Yeah.
[00:10:23.000 --> 00:10:25.720]   I'll start with a personal one that I think we've talked about before.
[00:10:25.720 --> 00:10:30.580]   But when I was at IBM, I was new to machine learning and we were launching a visual recognition
[00:10:30.580 --> 00:10:36.240]   system and the API did a very sort of general thing, but we were improving the accuracy
[00:10:36.240 --> 00:10:37.240]   of it.
[00:10:37.240 --> 00:10:39.160]   And I was new and I was like, "Well, how do you know it's better?
[00:10:39.160 --> 00:10:40.160]   How is the accuracy better?"
[00:10:40.160 --> 00:10:45.800]   And we sort of, the team settled on the F1 score as a fairly good measure of that.
[00:10:45.800 --> 00:10:50.600]   And our F1 score improved and there was a big Delta and we were excited to launch the
[00:10:50.600 --> 00:10:51.600]   next version.
[00:10:51.600 --> 00:10:56.360]   And a couple of days before launch, one of the team members reached out to me and said,
[00:10:56.360 --> 00:10:57.360]   "Alyssa, we can't launch this."
[00:10:57.360 --> 00:10:59.040]   And I was like, "What are you talking about?
[00:10:59.040 --> 00:11:00.040]   It's better.
[00:11:00.040 --> 00:11:01.760]   We've all agreed.
[00:11:01.760 --> 00:11:03.080]   There's a lot of energy behind this."
[00:11:03.080 --> 00:11:10.080]   And he sent me an image that I tested against the algorithm and the tag that came back for
[00:11:10.080 --> 00:11:12.320]   that image was the word loser.
[00:11:12.320 --> 00:11:15.600]   And the image itself was a picture of someone in a wheelchair.
[00:11:15.600 --> 00:11:21.520]   And I was horrified and I thought that that was terrible bias that we didn't want to encode
[00:11:21.520 --> 00:11:23.200]   and we certainly didn't want to launch.
[00:11:23.200 --> 00:11:27.120]   And it really sort of gave myself and the team a wake up call to like, "Hey, how could
[00:11:27.120 --> 00:11:30.720]   this have gotten into our data when the accuracy is supposed to be better?"
[00:11:30.720 --> 00:11:36.280]   And the aha moment for me as a newbie was, well, stupid, of course, it's the data.
[00:11:36.280 --> 00:11:40.360]   The data and the training data and the tags that you've associated with that are the problem.
[00:11:40.360 --> 00:11:46.720]   And so, we had a great team where we went back and reviewed every single tag, which
[00:11:46.720 --> 00:11:51.200]   was thousands and thousands and thousands of tags and millions of images.
[00:11:51.200 --> 00:11:53.620]   And we reviewed it by hand as a team.
[00:11:53.620 --> 00:11:58.000]   We divided and conquered and we pulled out quite a few objectionable things that we didn't
[00:11:58.000 --> 00:12:00.280]   want to be the public face of IBM.
[00:12:00.280 --> 00:12:02.520]   That took time and money and pain.
[00:12:02.520 --> 00:12:08.280]   And we were able to relaunch something that contained less sort of what I would call unwanted
[00:12:08.280 --> 00:12:10.640]   bias in that particular system.
[00:12:10.640 --> 00:12:16.860]   But IBM is certainly not free from that and many others have had challenges with visual
[00:12:16.860 --> 00:12:18.760]   recognition systems.
[00:12:18.760 --> 00:12:23.960]   Particularly there's been a lot of talk recently about bias in facial recognition systems.
[00:12:23.960 --> 00:12:25.960]   So they're tricky to get right.
[00:12:25.960 --> 00:12:30.840]   And it seems hard to fix, but even maybe more concerning is that it was just sort of caught
[00:12:30.840 --> 00:12:33.000]   by someone who happened to be trying something.
[00:12:33.000 --> 00:12:37.880]   I mean, do you have recommendations on diagnosing these kinds of problems?
[00:12:37.880 --> 00:12:38.880]   Yeah.
[00:12:38.880 --> 00:12:39.880]   So that was a long time ago.
[00:12:39.880 --> 00:12:42.240]   And since then, there's a ton more.
[00:12:42.240 --> 00:12:47.320]   But what I always say is be proactive around the biases that you're looking for.
[00:12:47.320 --> 00:12:51.720]   So there's a handful of biases that you are regulated.
[00:12:51.720 --> 00:12:53.280]   So you don't want to have gender bias.
[00:12:53.280 --> 00:12:55.800]   You don't want to have racial bias or other ones.
[00:12:55.800 --> 00:13:00.400]   So depending on what your system is deployed for, you're going to want to set up sort of
[00:13:00.400 --> 00:13:06.960]   a proactive monitoring system to take a percentage of your real-time public data.
[00:13:06.960 --> 00:13:10.600]   Do it also in tests, but also once you've launched sort of the real-time data and siphon
[00:13:10.600 --> 00:13:12.080]   it off and review it.
[00:13:12.080 --> 00:13:16.280]   Typically with humans manually, or at least sort of set up some alerting if things fall
[00:13:16.280 --> 00:13:20.560]   or skew outside of what your normal expectations would be.
[00:13:20.560 --> 00:13:26.560]   That involves usually dashboards and a lot of data and looking through tags, but also
[00:13:26.560 --> 00:13:30.200]   sort of proactively setting up a feedback mechanism.
[00:13:30.200 --> 00:13:34.400]   So that people can report things that you didn't hear about or you didn't think of and
[00:13:34.400 --> 00:13:39.560]   being able to escalate those quickly and react quickly and adjust, and hopefully be able
[00:13:39.560 --> 00:13:45.560]   to retrain your model or remove it or have a backup plan that does not include your model
[00:13:45.560 --> 00:13:50.800]   if you need to take it down for some reason for an extended period of time to mitigate
[00:13:50.800 --> 00:13:53.880]   things that you didn't anticipate.
[00:13:53.880 --> 00:13:58.920]   But then I guess fixing it, I mean, it doesn't seem realistic these days to go through all
[00:13:58.920 --> 00:14:03.920]   of a training data set and kind of take out everything objectionable, or maybe it is,
[00:14:03.920 --> 00:14:04.920]   I guess.
[00:14:04.920 --> 00:14:09.240]   I mean, on a model that's trained on kind of millions or billions of records, do you
[00:14:09.240 --> 00:14:14.000]   have recommendations there for how to improve the quality of the training data in a maybe
[00:14:14.000 --> 00:14:16.280]   more cost-effective way?
[00:14:16.280 --> 00:14:18.500]   So to me, that problem is...
[00:14:18.500 --> 00:14:22.520]   So even if it's millions or billions, it's where are you getting those millions or billions,
[00:14:22.520 --> 00:14:23.520]   right?
[00:14:23.520 --> 00:14:27.620]   And is there selection bias of where you're getting that data from?
[00:14:27.620 --> 00:14:30.640]   So take a speech recognition example.
[00:14:30.640 --> 00:14:36.440]   The speech recognition systems today available in the US are better at understanding men's
[00:14:36.440 --> 00:14:41.120]   voices than they are women's voices, or they are better at understanding people who speak
[00:14:41.120 --> 00:14:46.040]   English as their first language versus people who speak English as a secondary language.
[00:14:46.040 --> 00:14:50.860]   And that's largely due to the data that is collected, and it's thousands of hours of
[00:14:50.860 --> 00:14:51.860]   data collected.
[00:14:51.860 --> 00:14:57.840]   So if you're not actively collecting data from the populations that you want to serve,
[00:14:57.840 --> 00:14:59.280]   you're going to have a challenge there.
[00:14:59.280 --> 00:15:05.920]   So I think even in aggregate, it's appropriate and quite feasible to think critically around
[00:15:05.920 --> 00:15:10.360]   where you're getting your data and does it reflect the community or the people that you
[00:15:10.360 --> 00:15:14.480]   are going to be serving with the model.
[00:15:14.480 --> 00:15:18.540]   And I guess it's a little bit of a different issue, maybe men and women versus English
[00:15:18.540 --> 00:15:21.720]   as a first language, English as a second language, at least if you think about...
[00:15:21.720 --> 00:15:22.720]   Well, I don't even know.
[00:15:22.720 --> 00:15:25.680]   I guess there's more people with English as a second language, but you could imagine a
[00:15:25.680 --> 00:15:30.720]   case where there's a smaller group of speech patterns, for example.
[00:15:30.720 --> 00:15:36.160]   Do you think that you should collect at the ratios that you have, or should you try to
[00:15:36.160 --> 00:15:38.720]   over collect the more rare cases?
[00:15:38.720 --> 00:15:41.960]   Do you have thoughts on that?
[00:15:41.960 --> 00:15:44.040]   I think that's where a team comes in.
[00:15:44.040 --> 00:15:50.360]   I'd ask you, I'd certainly skew towards over collecting rare cases, but definitely monitoring
[00:15:50.360 --> 00:15:56.040]   for those cases to understand how those are performing.
[00:15:56.040 --> 00:16:01.040]   And I think as a team, you need to understand and balance the business priorities because
[00:16:01.040 --> 00:16:02.640]   it's not always feasible to collect.
[00:16:02.640 --> 00:16:07.200]   So let's say you're trying to deploy audio recognition in a call center, and let's pretend
[00:16:07.200 --> 00:16:09.800]   you're Walmart, right?
[00:16:09.800 --> 00:16:15.440]   And you serve most of the United States, but look at your customers.
[00:16:15.440 --> 00:16:20.640]   Do they skew to people who speak English as a first language or people who don't speak
[00:16:20.640 --> 00:16:21.840]   English as a first language?
[00:16:21.840 --> 00:16:23.960]   Are you going to deploy this in your entire call center?
[00:16:23.960 --> 00:16:27.920]   Are you going to start with just California or just Texas?
[00:16:27.920 --> 00:16:34.560]   And start to look and deploy models in a small way is what I find often works best to find
[00:16:34.560 --> 00:16:40.600]   sort of a narrow place to apply and then scale up as you can prove success and also collect
[00:16:40.600 --> 00:16:42.240]   more data typically.
[00:16:42.240 --> 00:16:45.200]   Because let's say you deploy it in Texas.
[00:16:45.200 --> 00:16:49.760]   So Texas has a heavy Spanish speaking population and you get a model that works well, and let's
[00:16:49.760 --> 00:16:52.160]   say it's only for handling returns.
[00:16:52.160 --> 00:16:57.360]   But if you then want to expand, say to Georgia, well, a Southern drawl is going to come into
[00:16:57.360 --> 00:17:00.840]   place and that model that you built for Texas is probably not going to work that well for
[00:17:00.840 --> 00:17:05.800]   the population in Atlanta, which skews more African-American versus Latino, and that's
[00:17:05.800 --> 00:17:07.120]   a different sort of speech pattern.
[00:17:07.120 --> 00:17:11.240]   And so you can deploy the same model, but you're probably not going to get the same
[00:17:11.240 --> 00:17:12.240]   results.
[00:17:12.240 --> 00:17:14.520]   And so needing to collect more data, mature and tweak it.
[00:17:14.520 --> 00:17:19.320]   So I think less around like, okay, how do you start right from the very beginning to
[00:17:19.320 --> 00:17:20.520]   try to do everything well?
[00:17:20.520 --> 00:17:25.720]   And it's more like, hey, start small with a specific and narrow business problem and
[00:17:25.720 --> 00:17:31.920]   do that well, and then gradually grow and use perhaps different or related data as you
[00:17:31.920 --> 00:17:34.320]   grow in order to address those additional needs.
[00:17:34.320 --> 00:17:37.040]   I mean, that makes sense.
[00:17:37.040 --> 00:17:42.640]   And that just seems like kind of best practice for any case, even sort of setting aside ethical
[00:17:42.640 --> 00:17:43.640]   concerns.
[00:17:43.640 --> 00:17:44.640]   Yeah.
[00:17:44.640 --> 00:17:48.880]   I think one of the big thing, not mistakes perhaps, but challenges coming into machine
[00:17:48.880 --> 00:17:52.720]   learning is that there's a lot of hype and everyone thinks you can solve a really big
[00:17:52.720 --> 00:17:56.480]   problem with machine learning with magic, and that's just never the case.
[00:17:56.480 --> 00:18:00.660]   It's much, much more successful to start narrow and start small and build out.
[00:18:00.660 --> 00:18:06.280]   And that's also a good way to address unintended biases by narrowing kind of what you're trying
[00:18:06.280 --> 00:18:10.120]   to do because it narrows the data set that you need, it makes it less expensive, and
[00:18:10.120 --> 00:18:12.720]   it allows your pilot to be more successful.
[00:18:12.720 --> 00:18:18.080]   I guess as you're researching the book, what other sorts of anti-patterns or failures did
[00:18:18.080 --> 00:18:20.160]   you see besides those types?
[00:18:20.160 --> 00:18:22.320]   What other things did people run into?
[00:18:22.320 --> 00:18:26.840]   I think, so one, we talked a little bit about the Goldilocks problem, which is trying to
[00:18:26.840 --> 00:18:31.360]   pick the right problem to solve and pick the right size and narrow problem to solve that's
[00:18:31.360 --> 00:18:32.600]   well suited for AI.
[00:18:32.600 --> 00:18:38.640]   I think another challenge is around team and getting a successful team in place that has
[00:18:38.640 --> 00:18:44.120]   the right mix of skills in order to successfully deploy something to production.
[00:18:44.120 --> 00:18:50.420]   This is not a case of a lone data scientist or even a team of data scientists building
[00:18:50.420 --> 00:18:53.400]   something in order to actually get something into production.
[00:18:53.400 --> 00:18:58.520]   You need DevOps, you need data engineers, you need a UX designer, typically, you need
[00:18:58.520 --> 00:19:03.180]   a product manager, you need regular front end software engineers and backend engineers.
[00:19:03.180 --> 00:19:09.080]   You need a whole team of people that is responsible for actually deploying something into a production
[00:19:09.080 --> 00:19:10.080]   environment.
[00:19:10.080 --> 00:19:19.000]   And that can often be at many companies harder than developing the model itself is actually
[00:19:19.000 --> 00:19:23.800]   getting to production because you start to run into things like legal and security and
[00:19:23.800 --> 00:19:25.320]   risk tolerance.
[00:19:25.320 --> 00:19:30.160]   And all of those things mean you have to have a backup plan and you have to understand how
[00:19:30.160 --> 00:19:35.640]   you are going to handle unknown and you're going to need escalation paths and putting
[00:19:35.640 --> 00:19:39.740]   those sort of business and technical processes in place.
[00:19:39.740 --> 00:19:45.120]   Often it's the business sort of thinking through that implications of the model or what happens
[00:19:45.120 --> 00:19:49.600]   when a decision is made and what happens if you need to explain why that decision was
[00:19:49.600 --> 00:19:54.540]   made to auditors or whoever is going to be scrutinizing this.
[00:19:54.540 --> 00:20:00.360]   That conversation, if you don't start early and don't involve those people early in the
[00:20:00.360 --> 00:20:03.720]   process can be big blockers to launching something to production.
[00:20:03.720 --> 00:20:09.100]   So what I encourage folks to do when they're starting out is think broadly around the cross-functional
[00:20:09.100 --> 00:20:14.760]   team of people that you want to have on your bench and think they need to be diverse.
[00:20:14.760 --> 00:20:18.240]   These are the finance people should certainly get involved.
[00:20:18.240 --> 00:20:22.880]   Sometimes HR needs to get involved and it's not just the engineers.
[00:20:22.880 --> 00:20:27.540]   Is there any specific stories you can share where legal came in at the end and blocked
[00:20:27.540 --> 00:20:32.240]   something or HR or finance wasn't involved and then the project couldn't launch even
[00:20:32.240 --> 00:20:34.120]   though the MO model was working well?
[00:20:34.120 --> 00:20:37.080]   So I'll use the Amazon one for HR.
[00:20:37.080 --> 00:20:43.520]   So there's a very public story about how Amazon was trying to use machine learning to predict
[00:20:43.520 --> 00:20:48.120]   who was going to get hired at Amazon or who would be really strong candidates for jobs
[00:20:48.120 --> 00:20:49.120]   there.
[00:20:49.120 --> 00:20:52.600]   And I don't know if it was HR that blocked it at the 11th hour, but they found it to
[00:20:52.600 --> 00:20:57.080]   be not serving the HR professionals and the goals of the HR professionals because it was
[00:20:57.080 --> 00:21:00.960]   super biased and it was biasing against women pretty heavily.
[00:21:00.960 --> 00:21:06.680]   And so that's a scenario where the model was working very well, I think at the beginning,
[00:21:06.680 --> 00:21:11.600]   which is predicting who would be strong candidates, but they weren't considering some other goals
[00:21:11.600 --> 00:21:15.640]   that were really important to Amazon, which is hiring a diverse employee base.
[00:21:15.640 --> 00:21:20.960]   And so that's potentially a case where the training data wasn't appropriate or I'm not
[00:21:20.960 --> 00:21:23.840]   sure exactly what went wrong behind the scenes there.
[00:21:23.840 --> 00:21:25.480]   Perhaps you know those people, I don't.
[00:21:25.480 --> 00:21:30.680]   But those are the types of things where legal or HR can say, "Hey, you know what?
[00:21:30.680 --> 00:21:31.680]   Can't do this."
[00:21:31.680 --> 00:21:37.200]   I know Uber has also had challenges in terms of making sure that their escalation path
[00:21:37.200 --> 00:21:42.120]   for support tickets that they use machine learning to classify, appropriately classify
[00:21:42.120 --> 00:21:46.240]   the right tickets in the right way to the right level of severity and scrutinizing that
[00:21:46.240 --> 00:21:51.400]   process because if you miscategorize something that's really urgent, that's a potentially
[00:21:51.400 --> 00:21:56.680]   sort of legal challenge for the company.
[00:21:56.680 --> 00:22:01.440]   I guess kind of channeling what our audience kind of asked me about all the time, I'm kind
[00:22:01.440 --> 00:22:06.960]   of curious if you have suggestions for an ML practitioner who kind of wants to work
[00:22:06.960 --> 00:22:11.760]   on something meaningful or wants to work for a company that really sort of embodies responsible
[00:22:11.760 --> 00:22:12.760]   or ethical AI.
[00:22:12.760 --> 00:22:17.560]   I mean, do you have any suggestions in what they might kind of look for in the interview
[00:22:17.560 --> 00:22:22.720]   process or before that, or maybe even companies that you think do this really well?
[00:22:22.720 --> 00:22:23.720]   Sure.
[00:22:23.720 --> 00:22:30.080]   So I recently got out of the AI business and I got into healthcare, which a lot of well-meaning
[00:22:30.080 --> 00:22:35.480]   mentors and people I admire are sort of scratching their heads being like, "You left all these
[00:22:35.480 --> 00:22:38.920]   lucrative job opportunities on the floor to go into an insurance company?
[00:22:38.920 --> 00:22:40.600]   Alyssa, are you out of your mind?"
[00:22:40.600 --> 00:22:45.560]   And maybe I am, but what I looked for, I followed the money when I was making that decision.
[00:22:45.560 --> 00:22:51.320]   And I don't mean personally, I mean follow how the money goes in the business or the
[00:22:51.320 --> 00:22:52.320]   organization.
[00:22:52.320 --> 00:22:55.160]   So I chose to work at Blue Shield, which is a nonprofit organization.
[00:22:55.160 --> 00:23:00.680]   And the incentives for the company are to cover more people in California with health
[00:23:00.680 --> 00:23:02.440]   insurance at a lower cost.
[00:23:02.440 --> 00:23:05.160]   By law, we cannot charge more for premiums.
[00:23:05.160 --> 00:23:09.320]   If we accidentally take in more money than we pay out in healthcare, we have to give
[00:23:09.320 --> 00:23:13.560]   it back to the people of California, which this year, because of the pandemic, the models
[00:23:13.560 --> 00:23:15.960]   were all over the place and wrong.
[00:23:15.960 --> 00:23:18.900]   And we ended up giving a lot of money back to our subscribers.
[00:23:18.900 --> 00:23:25.680]   And so for me, understanding how a company makes money and what drives the business will
[00:23:25.680 --> 00:23:28.800]   ultimately drive some of the models that take place.
[00:23:28.800 --> 00:23:33.160]   So if you look at Facebook or you look at Google, you look at Amazon, these are for-profit
[00:23:33.160 --> 00:23:34.160]   companies.
[00:23:34.160 --> 00:23:36.280]   Facebook makes its money on advertising.
[00:23:36.280 --> 00:23:41.720]   And so they have some of the most sophisticated advertising models in the world around encouraging
[00:23:41.720 --> 00:23:43.880]   the right content in front of the right person.
[00:23:43.880 --> 00:23:49.960]   And for me, that wasn't something that I wanted to spend my time doing.
[00:23:49.960 --> 00:23:53.140]   There's a lot of awesome people that work there, but it's not for me.
[00:23:53.140 --> 00:23:56.560]   And I decided to sort of go in a different direction.
[00:23:56.560 --> 00:24:02.000]   And I think it can be really hard for people to take a really hard look at where they want
[00:24:02.000 --> 00:24:06.960]   to spend their time and their day-to-day and what problems they want to think about.
[00:24:06.960 --> 00:24:12.520]   And I'm feeling really fortunate to be thinking about how to get more vaccines in arms.
[00:24:12.520 --> 00:24:15.560]   And there's not much machine learning that's going into that, frankly.
[00:24:15.560 --> 00:24:22.040]   It's spreadsheets and pretty basic data analysis, but I'm thrilled to be spending my time doing
[00:24:22.040 --> 00:24:23.040]   it.
[00:24:23.040 --> 00:24:27.120]   And I hope that Blue Shield can work on some cool, interesting machine learning problems
[00:24:27.120 --> 00:24:28.120]   in other areas.
[00:24:28.120 --> 00:24:31.080]   I guess I wanted to ask you about that.
[00:24:31.080 --> 00:24:35.720]   I mean, it does seem to me like there are a lot of really interesting ML applications
[00:24:35.720 --> 00:24:36.720]   in this field.
[00:24:36.720 --> 00:24:40.440]   And one of the Blue Cross Blue Shields, I think they separate by state, but I think
[00:24:40.440 --> 00:24:42.800]   one of them is actually a Wasted Biases customer.
[00:24:42.800 --> 00:24:46.000]   And I think Figure Eight had some customers in that realm.
[00:24:46.000 --> 00:24:50.000]   Yeah, there's tons of interesting use cases in health insurance.
[00:24:50.000 --> 00:24:52.880]   Can you tell me about some of the use cases in health insurance?
[00:24:52.880 --> 00:24:53.880]   Sure.
[00:24:53.880 --> 00:25:00.100]   I mean, a simple one that you know super well, Lucas, is around looking at healthcare data.
[00:25:00.100 --> 00:25:04.680]   So if you're looking at it in aggregate for thousands or millions of people, you're trying
[00:25:04.680 --> 00:25:11.680]   to understand what are patterns in terms of a patient's record over their lifetime that
[00:25:11.680 --> 00:25:15.680]   can be indicative of good outcomes, right?
[00:25:15.680 --> 00:25:20.840]   So if I, for example, I've been having carpal tunnel issues, challenges from working at
[00:25:20.840 --> 00:25:23.240]   home and not moving nearly enough.
[00:25:23.240 --> 00:25:28.600]   And I went to the doctor and they prescribed some steroids and some physical therapy and
[00:25:28.600 --> 00:25:29.600]   whatever else.
[00:25:29.600 --> 00:25:33.660]   But if you look sort of a few months later, like my hand is still bothering me.
[00:25:33.660 --> 00:25:35.580]   That didn't really work that well.
[00:25:35.580 --> 00:25:40.880]   And so are there patterns that you can look at at a population level to recommend particular
[00:25:40.880 --> 00:25:43.880]   treatment courses of treatment that work, right?
[00:25:43.880 --> 00:25:49.440]   So from a machine learning perspective, if, and this is a big if in healthcare, if you
[00:25:49.440 --> 00:25:54.120]   have a good data training set that's clean and well organized and you're able to access,
[00:25:54.120 --> 00:26:00.140]   you can look at large kind of outcomes like that and say, "Hey, did Alyssa need follow
[00:26:00.140 --> 00:26:01.140]   up after that?
[00:26:01.140 --> 00:26:03.400]   Did we have to spend more money on healthcare?
[00:26:03.400 --> 00:26:05.840]   And was her problem solved or not?"
[00:26:05.840 --> 00:26:09.000]   That's actually kind of one of the challenges in the healthcare space is often like you
[00:26:09.000 --> 00:26:13.400]   don't get the answer to like, did this treatment solve the problem?
[00:26:13.400 --> 00:26:15.840]   You either get like nothing happened after that, right?
[00:26:15.840 --> 00:26:19.520]   Or maybe I went to a different doctor somewhere and you just don't have the data.
[00:26:19.520 --> 00:26:23.960]   Or maybe I didn't take my meds because I didn't pick them up or whatever else.
[00:26:23.960 --> 00:26:28.160]   But there's a lot of challenges in the healthcare space of actually getting good data sets in
[00:26:28.160 --> 00:26:29.860]   order to do machine learning.
[00:26:29.860 --> 00:26:31.360]   So that's one use case.
[00:26:31.360 --> 00:26:35.840]   There's other use cases that are, I would call simpler like chat, right?
[00:26:35.840 --> 00:26:42.680]   Or people are trying to file claims or have billing issues and being able to respond faster
[00:26:42.680 --> 00:26:48.440]   to people and make our call center agents more efficient with their time by automatically
[00:26:48.440 --> 00:26:53.640]   answering sort of tier one support issues like, "I lost my password," or whatever else.
[00:26:53.640 --> 00:26:56.960]   And being able to handle that in a lot of different languages, for example.
[00:26:56.960 --> 00:27:01.880]   So machine learning can support those types of use cases.
[00:27:01.880 --> 00:27:07.560]   And I guess, how real is this?
[00:27:07.560 --> 00:27:09.240]   Is ML chat used today?
[00:27:09.240 --> 00:27:13.840]   If I went to the Blue Shield website, would I interact with the chat?
[00:27:13.840 --> 00:27:17.960]   Yeah, we're rolling out chat.
[00:27:17.960 --> 00:27:20.720]   I don't know if you went today.
[00:27:20.720 --> 00:27:22.080]   I'd have to get back to you.
[00:27:22.080 --> 00:27:25.800]   It's not my particular area of ownership, but we certainly have chat.
[00:27:25.800 --> 00:27:28.120]   I think also for the providers, right?
[00:27:28.120 --> 00:27:31.360]   So insurance, I've learned a ton about insurance.
[00:27:31.360 --> 00:27:36.560]   It's an interesting space because you have customers that are members, like you buy health
[00:27:36.560 --> 00:27:38.760]   insurance from us or you get it through your employer.
[00:27:38.760 --> 00:27:42.520]   But we also have doctors who interact with the insurance company for lots of different
[00:27:42.520 --> 00:27:43.520]   reasons.
[00:27:43.520 --> 00:27:44.520]   So that's the providers.
[00:27:44.520 --> 00:27:48.020]   And then also the employers or brokers or HR people, all those people need help.
[00:27:48.020 --> 00:27:53.440]   So I'm pretty sure our chat is rolled out for employers and brokers and providers.
[00:27:53.440 --> 00:27:55.120]   I'm not sure if it's for members.
[00:27:55.120 --> 00:27:57.960]   And then also we certainly have internally as well.
[00:27:57.960 --> 00:28:05.000]   So if I need something as a employee, I can use our virtual assistant internally to order
[00:28:05.000 --> 00:28:10.780]   a new mouse or get provisioned access to a system or whatever else for IT support.
[00:28:10.780 --> 00:28:14.360]   So that's actually been a really successful use case for us.
[00:28:14.360 --> 00:28:18.960]   And I mean, the health record stuff seems so evocative, right?
[00:28:18.960 --> 00:28:24.840]   I would sort of love to be able to do a deep data analysis on my own health record.
[00:28:24.840 --> 00:28:25.840]   Yeah, if you could get it.
[00:28:25.840 --> 00:28:26.840]   Ask for Will.
[00:28:26.840 --> 00:28:29.560]   Yeah, if I could get it.
[00:28:29.560 --> 00:28:30.560]   And look out into the future.
[00:28:30.560 --> 00:28:35.160]   I guess, why do you think these, or maybe these do exist.
[00:28:35.160 --> 00:28:42.080]   Would you say that your employer is currently doing analysis of health records to forecast
[00:28:42.080 --> 00:28:44.400]   what might help happen to people?
[00:28:44.400 --> 00:28:45.400]   Yeah.
[00:28:45.400 --> 00:28:46.400]   So absolutely.
[00:28:46.400 --> 00:28:50.280]   So we work with population health and not just us, but we work with other companies
[00:28:50.280 --> 00:28:52.720]   who perhaps do some of this analysis.
[00:28:52.720 --> 00:28:56.600]   Then we actually consume the insights from those analysis.
[00:28:56.600 --> 00:28:58.440]   And we work with a lot of different partners.
[00:28:58.440 --> 00:28:59.920]   There's a platform we call Webolution.
[00:28:59.920 --> 00:29:01.280]   So I'll give you an example.
[00:29:01.280 --> 00:29:06.480]   We work with one company that has done a lot of analysis around kidney disease.
[00:29:06.480 --> 00:29:12.520]   So people who are on dialysis and getting good outcomes there, they figured out, "Hey,
[00:29:12.520 --> 00:29:18.120]   here is the right way to treat kidney dialysis patients that has better outcomes."
[00:29:18.120 --> 00:29:24.680]   And so we encourage and steer our patients towards this particular program because it's
[00:29:24.680 --> 00:29:28.520]   proven that it has better outcomes than perhaps treating it without this program.
[00:29:28.520 --> 00:29:34.440]   And so that's an example where we try to recognize patients who have a particular diagnosis or
[00:29:34.440 --> 00:29:39.040]   condition and then encourage them to use the programs that have the best outcome.
[00:29:39.040 --> 00:29:40.040]   I see.
[00:29:40.040 --> 00:29:46.080]   So it is sort of like take one hypothesis and just test it based on-
[00:29:46.080 --> 00:29:47.080]   Not that I'm aware of.
[00:29:47.080 --> 00:29:51.800]   Maybe there are other people that I don't know about, but it's more around, if you look
[00:29:51.800 --> 00:29:56.160]   at population, the big things are the same things, right?
[00:29:56.160 --> 00:29:58.800]   It's diabetes, it's hypertension.
[00:29:58.800 --> 00:30:01.080]   These are sort of the big things that impact our population.
[00:30:01.080 --> 00:30:08.000]   And so if you can sort of encourage people to shift their often lifestyle habits to things
[00:30:08.000 --> 00:30:11.040]   that are going to be more successful, you can have better outcomes.
[00:30:11.040 --> 00:30:18.640]   But as it turns out, it's a lot easier said than done to get people to take their health
[00:30:18.640 --> 00:30:19.640]   seriously.
[00:30:19.640 --> 00:30:20.640]   And some people don't, right?
[00:30:20.640 --> 00:30:25.360]   Some people are like, "It's just not a priority for me to change my lifestyle to be healthier."
[00:30:25.360 --> 00:30:27.840]   And other people are super, super eager to do it.
[00:30:27.840 --> 00:30:31.480]   And then there's a bunch of us probably that fall somewhere in between on that spectrum
[00:30:31.480 --> 00:30:36.720]   and we're willing to make certain accommodations or changes in our lives and others we're not.
[00:30:36.720 --> 00:30:42.360]   And so how do you use different tools or different approaches for different populations to sort
[00:30:42.360 --> 00:30:44.960]   of move them into healthier lifestyles?
[00:30:44.960 --> 00:30:49.640]   Because if you kind of take a step back, it's not, we don't want, at least Blue Shield,
[00:30:49.640 --> 00:30:52.320]   it's not that we want to pay less money in healthcare costs.
[00:30:52.320 --> 00:30:55.200]   We want to get everyone healthier, right?
[00:30:55.200 --> 00:31:04.080]   Because healthcare as a force in the macro sort of US economy is an incredibly inefficient
[00:31:04.080 --> 00:31:06.200]   expense that we simply can't afford.
[00:31:06.200 --> 00:31:10.920]   It represents a huge percentage of our spending as a country and it's not sustainable.
[00:31:10.920 --> 00:31:18.000]   And so we need to find ways to get our healthcare costs down as an industry overall, because
[00:31:18.000 --> 00:31:21.440]   it's just not something our economy can support.
[00:31:21.440 --> 00:31:25.880]   What have you been working on before the vaccine and post leaving happened?
[00:31:25.880 --> 00:31:28.680]   I mean, what projects have you been-
[00:31:28.680 --> 00:31:32.600]   I was working on this longitudinal healthcare record problem.
[00:31:32.600 --> 00:31:35.860]   And so we actually launched a pilot, which I'm super excited about.
[00:31:35.860 --> 00:31:40.840]   And so for a certain percentage of our members, they actually can get their longitudinal patient
[00:31:40.840 --> 00:31:44.960]   record with every provider data, if we have access to it.
[00:31:44.960 --> 00:31:45.960]   So that's a big if.
[00:31:45.960 --> 00:31:50.760]   But if you've submitted a claim to Blue Shield or your provider participates in one of the
[00:31:50.760 --> 00:31:55.200]   statewide networks in California, it's called Manifest Medics, and they have thousands of
[00:31:55.200 --> 00:31:59.240]   providers that send data and by provider, I mean doctor.
[00:31:59.240 --> 00:32:04.920]   So if you're a large healthcare institution, you may participate in this.
[00:32:04.920 --> 00:32:07.720]   And then we show it to you as a member and you can look at your record.
[00:32:07.720 --> 00:32:11.360]   And then we also recommend things that perhaps you haven't done.
[00:32:11.360 --> 00:32:14.760]   So if you haven't gotten your flu shot, or if you haven't been to your annual checkup,
[00:32:14.760 --> 00:32:20.000]   or you're overdue for a cancer screening or something like that, we'll say, "Hey, Alyssa,
[00:32:20.000 --> 00:32:23.160]   you haven't gotten your pap smear this year, go get it done."
[00:32:23.160 --> 00:32:26.800]   And you can interact with us and you can say, "Oh, actually, I already did it and you just
[00:32:26.800 --> 00:32:30.800]   don't have the data," or "Thanks, let me set a reminder to go get that done."
[00:32:30.800 --> 00:32:37.640]   So that's my baby, my project that I was working on before I got pulled into the vaccine work.
[00:32:37.640 --> 00:32:38.640]   That's so great.
[00:32:38.640 --> 00:32:42.480]   And if I'm a member of Blue Shield, can I use it?
[00:32:42.480 --> 00:32:43.480]   Yeah.
[00:32:43.480 --> 00:32:44.480]   So there's a...
[00:32:44.480 --> 00:32:48.440]   Yeah, it's rolled out to, I think about 50,000 people right now and we are working on it
[00:32:48.440 --> 00:32:52.840]   and hopefully going to ramp it up to more Blue Shield members in the future.
[00:32:52.840 --> 00:32:53.840]   That's so cool.
[00:32:53.840 --> 00:32:55.560]   And I guess probably there's...
[00:32:55.560 --> 00:33:00.040]   I would think for myself, there's things that no ML algorithm would be needed to tell me
[00:33:00.040 --> 00:33:01.040]   that would make me healthier.
[00:33:01.040 --> 00:33:02.040]   Yeah, a lot of it's reasonable.
[00:33:02.040 --> 00:33:05.640]   Like, "Hey, you did or you didn't do this."
[00:33:05.640 --> 00:33:07.320]   It doesn't require machine learning.
[00:33:07.320 --> 00:33:16.040]   But what has required machine learning type of thinking in this project is around, frankly,
[00:33:16.040 --> 00:33:17.200]   data cleaning.
[00:33:17.200 --> 00:33:21.400]   So we may have multiple records of the same medication for the same member.
[00:33:21.400 --> 00:33:26.960]   I get birth control every single month and I have multiple prescriptions, I sign an overlap
[00:33:26.960 --> 00:33:27.960]   with each other.
[00:33:27.960 --> 00:33:33.400]   And if you look at the last 10 years, if I'm displaying that to me as a list of symptoms,
[00:33:33.400 --> 00:33:37.360]   I don't want a list of 10 years of data worth of every medication I've ever prescribed.
[00:33:37.360 --> 00:33:42.440]   I want you to group it logically by the brand name or the medication type.
[00:33:42.440 --> 00:33:49.560]   And so that is a data cleaning machine learning exercise around grouping medications together,
[00:33:49.560 --> 00:33:56.000]   because one pharmacy may have reported it with slightly different wording or dosage
[00:33:56.000 --> 00:33:58.960]   or something versus another pharmacy or another doctor.
[00:33:58.960 --> 00:34:02.720]   And so organizing that information, machine learning can be a natural language processing
[00:34:02.720 --> 00:34:04.040]   can be super useful there.
[00:34:04.040 --> 00:34:08.880]   And I guess I should say, we were joking about this, but my wife runs a company called Picnic
[00:34:08.880 --> 00:34:09.880]   Health that does a lot of-
[00:34:09.880 --> 00:34:11.560]   Which does a bang up job.
[00:34:11.560 --> 00:34:12.560]   I like that.
[00:34:12.560 --> 00:34:13.560]   Yeah, it does.
[00:34:13.560 --> 00:34:15.160]   And my unbiased opinion is fantastic at-
[00:34:15.160 --> 00:34:20.400]   I've heard they're really good at that.
[00:34:20.400 --> 00:34:31.400]   I guess why do you think that these health records end up so hard to structure?
[00:34:31.400 --> 00:34:32.920]   I ask your wife.
[00:34:32.920 --> 00:34:34.960]   She knows way better than I do.
[00:34:34.960 --> 00:34:41.720]   From my limited understanding, I think it's because the healthcare system in the United
[00:34:41.720 --> 00:34:43.820]   States is just really, really fragmented.
[00:34:43.820 --> 00:34:49.320]   And there's so many different entities in the data chain.
[00:34:49.320 --> 00:34:51.400]   So I'll use a personal example.
[00:34:51.400 --> 00:34:55.800]   This week, I get headaches and a doctor prescribed me a new medication.
[00:34:55.800 --> 00:34:57.960]   And I have had headaches for a long time.
[00:34:57.960 --> 00:35:00.160]   So I've cycled through all the normal ones that someone would use.
[00:35:00.160 --> 00:35:05.420]   And this one is an expensive medication and out of the bounds of normal.
[00:35:05.420 --> 00:35:08.720]   And she prescribed it a week ago Monday to me.
[00:35:08.720 --> 00:35:13.080]   And my pharmacy followed up with me that same day saying, "Hey, we're working with your
[00:35:13.080 --> 00:35:16.080]   doctor in insurance to get this covered and we'll get out to you."
[00:35:16.080 --> 00:35:18.360]   A couple of days go by, I still don't have my medication.
[00:35:18.360 --> 00:35:22.520]   I followed up and they say they're working on it and blah, blah, blah.
[00:35:22.520 --> 00:35:28.680]   The number of different entities that have to touch or approve this end to end from my
[00:35:28.680 --> 00:35:34.600]   doctor and me having a conversation and her prescribing it to me getting it is like, I'm
[00:35:34.600 --> 00:35:38.600]   not joking, probably 10 different systems.
[00:35:38.600 --> 00:35:44.000]   So it has to go from the electronic medical record that my doctor is using.
[00:35:44.000 --> 00:35:51.800]   And that has to go into an intermediary system that goes in between the doctor's office or
[00:35:51.800 --> 00:35:55.080]   the hospital and the insurance company.
[00:35:55.080 --> 00:35:59.840]   And so there's a third party in between that processes what's called prior authorizations.
[00:35:59.840 --> 00:36:04.200]   And then the insurance company has to...
[00:36:04.200 --> 00:36:07.560]   We don't directly integrate with that particular third party.
[00:36:07.560 --> 00:36:11.700]   And so we have to do some data moving around in order to get it to the right person in
[00:36:11.700 --> 00:36:14.080]   our system to approve that.
[00:36:14.080 --> 00:36:16.960]   And then it has to go back to the doctor's office.
[00:36:16.960 --> 00:36:21.360]   But then there's this pharmacy over here that hasn't been involved in any of this so far.
[00:36:21.360 --> 00:36:23.640]   There's a bunch of systems that they use in between.
[00:36:23.640 --> 00:36:28.360]   The short answer is there's a lot of different systems involved and they don't all talk to
[00:36:28.360 --> 00:36:29.640]   each other very successfully.
[00:36:29.640 --> 00:36:34.040]   And the data gets manipulated and changed and there's different standards and different
[00:36:34.040 --> 00:36:35.040]   data systems.
[00:36:35.040 --> 00:36:39.480]   And even though there are sort of standards around healthcare data, I think July they
[00:36:39.480 --> 00:36:44.280]   go into effect in California in terms of being mandated to follow certain types of standards
[00:36:44.280 --> 00:36:46.720]   for certain narrow use cases.
[00:36:46.720 --> 00:36:51.160]   But there's just not a ton of structure for these different data types.
[00:36:51.160 --> 00:36:53.880]   And so they've evolved in different ways.
[00:36:53.880 --> 00:37:00.200]   And even the electronic medical records, we're dealing with this in the vaccine world.
[00:37:00.200 --> 00:37:04.840]   How does your doctor know that you've been given the vaccine?
[00:37:04.840 --> 00:37:09.340]   Well that's kind of a challenge because let's say you got it at Walgreens.
[00:37:09.340 --> 00:37:13.920]   They may have taken your insurance and then maybe they submitted a claim to your insurance.
[00:37:13.920 --> 00:37:17.720]   Maybe they didn't do that if you didn't have insurance, but they're not reporting it back
[00:37:17.720 --> 00:37:18.720]   to your doctor's system.
[00:37:18.720 --> 00:37:23.720]   And anyway, there's a lot of different software systems that are being used and there's not
[00:37:23.720 --> 00:37:24.720]   standards.
[00:37:24.720 --> 00:37:30.000]   Whereas if you look at different countries that have more nationalized healthcare systems,
[00:37:30.000 --> 00:37:32.920]   there's one, two systems.
[00:37:32.920 --> 00:37:35.680]   And so there's just a lot less fragmentation.
[00:37:35.680 --> 00:37:44.280]   And California has 8,000 different providers and there's 10 major electronic medical records
[00:37:44.280 --> 00:37:50.760]   systems, three of which are really big, but there's a long tail for the rest of them.
[00:37:50.760 --> 00:37:57.840]   And a place like Walgreens doesn't use electronic health records system or Safeway or pharmacy.
[00:37:57.840 --> 00:38:00.480]   They use pharmacy systems, which are different than the hospital system.
[00:38:00.480 --> 00:38:05.440]   So that's a long way to answer your question, but it's amazing data.
[00:38:05.440 --> 00:38:06.440]   Yeah.
[00:38:06.440 --> 00:38:10.520]   I mean, it's funny how I could see how years of working in ML would prepare you well for
[00:38:10.520 --> 00:38:12.160]   the American healthcare system.
[00:38:12.160 --> 00:38:15.000]   But it's basic data problems.
[00:38:15.000 --> 00:38:17.680]   It's not particularly sophisticated machine learning problems.
[00:38:17.680 --> 00:38:18.680]   It's data hygiene.
[00:38:18.680 --> 00:38:19.680]   Right, right.
[00:38:19.680 --> 00:38:21.400]   Although it seems like that's the problem everywhere.
[00:38:21.400 --> 00:38:22.400]   Yeah.
[00:38:22.400 --> 00:38:23.400]   I mean, that's a problem everywhere.
[00:38:23.400 --> 00:38:24.400]   Yeah, exactly.
[00:38:24.400 --> 00:38:31.960]   Were there other sort of surprises kind of going from, I guess, a startup to an insurance
[00:38:31.960 --> 00:38:32.960]   company?
[00:38:32.960 --> 00:38:37.280]   How similar is your job doing product there?
[00:38:37.280 --> 00:38:41.520]   I think product management is similar no matter where you do it.
[00:38:41.520 --> 00:38:44.000]   It's always balancing stakeholders and priorities.
[00:38:44.000 --> 00:38:47.400]   And the day-to-day is certainly different in different types of companies, but I think
[00:38:47.400 --> 00:38:52.080]   fundamentally my skill sets are the same and the job I do is roughly the same.
[00:38:52.080 --> 00:38:56.280]   I think the problem space is really different and the excitement I get around it is really
[00:38:56.280 --> 00:38:57.280]   different.
[00:38:57.280 --> 00:39:04.400]   So to answer your question earlier around how do people navigate into working on problems
[00:39:04.400 --> 00:39:09.000]   that they really want to work on and really love and how do you do that is follow where
[00:39:09.000 --> 00:39:10.600]   your interests are.
[00:39:10.600 --> 00:39:18.060]   I'm thrilled to get into the weeds of doing data munging.
[00:39:18.060 --> 00:39:25.000]   And I personally wrote, I think, 500 different data validation rules for prescription organizing,
[00:39:25.000 --> 00:39:30.880]   looking through hundreds of records of different types of prescriptions and how to organize
[00:39:30.880 --> 00:39:32.680]   some basic data hygiene rules.
[00:39:32.680 --> 00:39:35.680]   That was super fun and I was thrilled to do it.
[00:39:35.680 --> 00:39:41.160]   It was painful work and certainly I have other skills, but I was really excited about the
[00:39:41.160 --> 00:39:46.700]   problem that we were solving, which was launching a cogent experience to my friends and family
[00:39:46.700 --> 00:39:51.120]   who are members of Blue Shield around being able to look at their longitudinal patient
[00:39:51.120 --> 00:39:55.840]   record and not have all this messy duplication of data that they're showing.
[00:39:55.840 --> 00:39:58.440]   And so, I'm sorry, I got a little off track, but-
[00:39:58.440 --> 00:40:02.640]   No, no, I totally relate to what you're saying and I think it's incredibly good advice.
[00:40:02.640 --> 00:40:08.760]   When I was at Appen and Figure Eight, some of the problems we worked on were super interesting
[00:40:08.760 --> 00:40:14.680]   and awesome and others weren't as close to my heart and we were optimizing advertising
[00:40:14.680 --> 00:40:17.320]   dollars or whatever else.
[00:40:17.320 --> 00:40:20.400]   And those are things that I just get less excited about personally.
[00:40:20.400 --> 00:40:21.400]   Totally.
[00:40:21.400 --> 00:40:26.440]   Well, I guess we always end with these questions, but it's funny because they're so relevant
[00:40:26.440 --> 00:40:31.640]   to your book because what we want to talk about on this podcast is really just making
[00:40:31.640 --> 00:40:33.200]   ML work in the real world.
[00:40:33.200 --> 00:40:36.520]   But I want to ask them to you and sort of get your take from all the research that you've
[00:40:36.520 --> 00:40:40.240]   really done and maybe get as specific as you can.
[00:40:40.240 --> 00:40:44.960]   But what do you think is an underrated aspect of machine learning that you think people
[00:40:44.960 --> 00:40:49.740]   should pay more attention to than they currently are?
[00:40:49.740 --> 00:40:51.500]   I think teamwork.
[00:40:51.500 --> 00:40:56.000]   And we talked a little bit about this, but it's really teamwork.
[00:40:56.000 --> 00:41:02.320]   I think there's a misconception that machine learning work is pretty solitary and you can
[00:41:02.320 --> 00:41:06.640]   teach yourself to do it or you can do it by yourself on a laptop or whatever, but it's
[00:41:06.640 --> 00:41:11.760]   a team in order to deploy anything functional that matters.
[00:41:11.760 --> 00:41:13.880]   And it takes a lot of different skill sets.
[00:41:13.880 --> 00:41:20.980]   And for the team to work together successfully, it's really around best practices of any team
[00:41:20.980 --> 00:41:23.460]   functioning successfully and has less to do with machine learning.
[00:41:23.460 --> 00:41:28.300]   But I think that often gets overlooked because there's a lot of focus on the technology and
[00:41:28.300 --> 00:41:31.940]   the right hard skills and the right technical systems.
[00:41:31.940 --> 00:41:36.580]   And I think it's really easy to overlook the team dynamics of getting people to work together
[00:41:36.580 --> 00:41:44.160]   well, whether that's quality engineers or data folks or project managers or designers
[00:41:44.160 --> 00:41:46.220]   or scrum masters.
[00:41:46.220 --> 00:41:48.700]   You need a team of people who trust each other.
[00:41:48.700 --> 00:41:52.420]   We certainly have plenty of those problems at Blue Shield or any team that I've ever
[00:41:52.420 --> 00:41:55.640]   worked on where people don't necessarily trust each other.
[00:41:55.640 --> 00:42:01.420]   And they may be critical of others' work or they may have communication challenges or
[00:42:01.420 --> 00:42:02.420]   whatever.
[00:42:02.420 --> 00:42:03.480]   And they're really remote.
[00:42:03.480 --> 00:42:06.680]   Some of those things are harder to smooth over.
[00:42:06.680 --> 00:42:11.300]   But for successful machine learning teams that I've worked with, they have high trust,
[00:42:11.300 --> 00:42:16.000]   they have high collaboration and cooperation with a diverse group of people.
[00:42:16.000 --> 00:42:20.840]   They welcome outside ideas and people who are willing to roll up their sleeves and get
[00:42:20.840 --> 00:42:21.840]   dirty.
[00:42:21.840 --> 00:42:26.320]   Lukas: If you think about practitioners that you've worked with or someone who's listening
[00:42:26.320 --> 00:42:32.760]   to this, is there any resources that you'd point them to to become a better team member?
[00:42:32.760 --> 00:42:38.080]   Has there been a book that you've read or an article that's helped you with this?
[00:42:38.080 --> 00:42:41.760]   One of the books that was recommended to me that I really like around teamwork is called
[00:42:41.760 --> 00:42:43.220]   Turn the Ship Around.
[00:42:43.220 --> 00:42:48.280]   And it's a book that goes behind the scenes of a nuclear warship that was being deployed
[00:42:48.280 --> 00:42:50.120]   and it was written by the captain of that ship.
[00:42:50.120 --> 00:42:54.920]   And he came in and he took over the ship and it was a low performing team.
[00:42:54.920 --> 00:43:00.520]   But at the end of the day, it was a nuclear ship and I'm going to totally botch all of
[00:43:00.520 --> 00:43:03.120]   the military stuff and get it completely wrong.
[00:43:03.120 --> 00:43:05.440]   But really important to do it well.
[00:43:05.440 --> 00:43:07.600]   Can't screw it up.
[00:43:07.600 --> 00:43:10.520]   The team hadn't been collaborating well.
[00:43:10.520 --> 00:43:14.640]   And he goes behind the scenes and talks about his time literally turning the ship around
[00:43:14.640 --> 00:43:19.280]   to get it ready for deployment, to go back out into doing whatever it's supposed to be
[00:43:19.280 --> 00:43:20.280]   doing.
[00:43:20.280 --> 00:43:23.120]   But it couldn't leave the harbor until it passed all its safety checks and the team
[00:43:23.120 --> 00:43:25.160]   sort of was functioning better.
[00:43:25.160 --> 00:43:30.280]   And they were sort of working on this top down approach and everyone's sort of covering
[00:43:30.280 --> 00:43:34.760]   their own butt and not necessarily really thinking critically about what they were being
[00:43:34.760 --> 00:43:37.360]   asked to do and how to do it better for the right outcomes.
[00:43:37.360 --> 00:43:41.160]   And anyway, I love this book and I think it applies in business and all sorts of different
[00:43:41.160 --> 00:43:45.720]   settings, particularly machine learning, because it's high stakes often what machine learning
[00:43:45.720 --> 00:43:47.120]   projects are being asked to do.
[00:43:47.120 --> 00:43:51.640]   And the problems are big and they're important and they're worthy of solving, but they can
[00:43:51.640 --> 00:43:56.480]   also have pretty dangerous or negative consequences if they're not done well.
[00:43:56.480 --> 00:44:01.400]   And so this is a book with an analogy that I like to a nuclear warship, because it's
[00:44:01.400 --> 00:44:08.320]   an important problem and it requires a huge team of people collaborating towards the right
[00:44:08.320 --> 00:44:09.320]   outcomes.
[00:44:09.320 --> 00:44:10.320]   I love it.
[00:44:10.320 --> 00:44:11.320]   I'm going to read that book.
[00:44:11.320 --> 00:44:12.320]   I'll send it to you.
[00:44:12.320 --> 00:44:13.320]   Awesome.
[00:44:13.320 --> 00:44:20.040]   I guess the question that we always end with is, and this is kind of what you spent, I
[00:44:20.040 --> 00:44:23.640]   think, most of your career on, so I'm curious what you think is the biggest thing here,
[00:44:23.640 --> 00:44:27.920]   but we always ask, what's the biggest challenge of making machine learning work in the real
[00:44:27.920 --> 00:44:34.240]   world or maybe where there's specific pitfalls where you see machine learning projects fail?
[00:44:34.240 --> 00:44:36.560]   Yeah.
[00:44:36.560 --> 00:44:38.640]   We certainly talk a lot about this in our book.
[00:44:38.640 --> 00:44:45.040]   I think a few major areas are not having the right team, not having the right problem,
[00:44:45.040 --> 00:44:46.040]   not having the right data.
[00:44:46.040 --> 00:44:47.040]   I don't know.
[00:44:47.040 --> 00:44:48.040]   I could go on.
[00:44:48.040 --> 00:44:52.960]   I think those three are probably the big ones.
[00:44:52.960 --> 00:44:56.320]   The data to me is often the long pole.
[00:44:56.320 --> 00:44:58.320]   And I guess for more, read the book.
[00:44:58.320 --> 00:44:59.320]   For more, read the book.
[00:44:59.320 --> 00:45:00.320]   Yeah.
[00:45:00.320 --> 00:45:01.320]   We'll put a link to it and yeah, you should read it.
[00:45:01.320 --> 00:45:02.320]   Thank you so much.
[00:45:02.320 --> 00:45:03.320]   I really appreciate it.
[00:45:03.320 --> 00:45:04.320]   Thanks, Lucas.
[00:45:04.320 --> 00:45:07.240]   It's a pleasure to be here, as always, with you.
[00:45:07.240 --> 00:45:10.360]   Thanks for listening to another episode of Grady Dissent.
[00:45:10.360 --> 00:45:14.620]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:45:14.620 --> 00:45:17.380]   hear from the people that are listening to these episodes.
[00:45:17.380 --> 00:45:21.440]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:45:21.440 --> 00:45:25.360]   a conversation, that would make me inspired to do more of these episodes.
[00:45:25.360 --> 00:45:28.920]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

