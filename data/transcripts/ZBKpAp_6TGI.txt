
[00:00:00.000 --> 00:00:05.360]   Hello guys! Welcome to my new video on how to code stable diffusion from scratch.
[00:00:05.360 --> 00:00:09.480]   And stable diffusion is a model that was introduced last year.
[00:00:09.480 --> 00:00:12.360]   I think most of you are already familiar with it.
[00:00:12.360 --> 00:00:16.640]   And we will be coding it from scratch using PyTorch only.
[00:00:16.640 --> 00:00:19.880]   And as usual my video is going to be quite long,
[00:00:19.880 --> 00:00:27.480]   because we will be coding from scratch and at the same time I will be explaining each part that makes up stable diffusion.
[00:00:27.480 --> 00:00:35.520]   So as usual let me introduce you what are the topics that we will discuss and what are the prerequisites for watching this video.
[00:00:35.520 --> 00:00:41.120]   So of course we will discuss stable diffusion because we are going to build it from scratch using only PyTorch.
[00:00:41.120 --> 00:00:45.720]   So no other libraries will be used except for the tokenizer.
[00:00:45.720 --> 00:00:54.240]   I will describe the maths of the diffusion models as defined in the DDPM paper, but I will simplify it as much as possible.
[00:00:54.240 --> 00:00:59.200]   I will show you how classifier-free guidance works and of course we will also implement it,
[00:00:59.200 --> 00:01:03.360]   how the text-to-image works, image-to-image and in-painting.
[00:01:03.360 --> 00:01:08.920]   Of course to have a very complete view of diffusion models actually we should also introduce the score-based models
[00:01:08.920 --> 00:01:12.240]   and all the ODE and SDF theoretical framework.
[00:01:12.240 --> 00:01:18.720]   But most people are not familiar with ordinary differential equations or even stochastic differential equations.
[00:01:18.720 --> 00:01:23.960]   So I will not discuss these topics in this video and I'll leave it for future videos.
[00:01:23.960 --> 00:01:32.280]   So anyway we will have a complete copy of a stable diffusion, we will be able to generate images using the prompt,
[00:01:32.280 --> 00:01:35.680]   also condition on existing images etc.
[00:01:35.680 --> 00:01:42.560]   But for example the samplers based on the Euler method or Runge-Kutta method will not be built in this video.
[00:01:42.560 --> 00:01:46.560]   I will make a future video in which I describe these ones.
[00:01:46.560 --> 00:01:50.560]   What do I expect you to have as a prerequisite for watching this video?
[00:01:50.560 --> 00:01:55.240]   Well first of all it's good that if you have some notion of probability and statistics,
[00:01:55.240 --> 00:01:58.160]   so at least you know what is a Gaussian distribution,
[00:01:58.160 --> 00:02:02.520]   what is the conditional probability, the marginal probability, the likelihood etc.
[00:02:02.520 --> 00:02:08.520]   Now I don't expect you to have the mathematical formulation in your mind about these concepts,
[00:02:08.520 --> 00:02:16.040]   but at least the concepts behind them, so at least what do we mean by conditional probability or what do we mean by marginal probability.
[00:02:16.040 --> 00:02:22.280]   Anyway even if you're not very strong with mathematics I will always give a non-mathematics intuition for most concepts.
[00:02:22.280 --> 00:02:31.000]   So even if you don't have this background you will at least understand the concept behind this, some intuition behind this.
[00:02:31.000 --> 00:02:39.840]   And of course I expect you to know Python and PyTorch, at least basic level, because we will be coding using Python and PyTorch.
[00:02:39.840 --> 00:02:43.040]   And then we will be using a lot the attention mechanism,
[00:02:43.040 --> 00:02:49.800]   so if you're not familiar with the transformer model please watch my previous video on the attention and transformer.
[00:02:49.800 --> 00:02:53.320]   And we will also be using a lot of convolutions.
[00:02:53.320 --> 00:02:57.400]   So I don't expect you to know how mathematically the convolution layers work,
[00:02:57.400 --> 00:03:02.720]   but at least what they do on a practical level in a neural network.
[00:03:02.720 --> 00:03:06.600]   Anyway I will also review this while coding.
[00:03:06.600 --> 00:03:16.840]   And because this is going to be a long video I will first, because the stable diffusion and the diffusion models in general are quite complex from a mathematical point of view,
[00:03:16.840 --> 00:03:23.120]   so we cannot jump directly to the code without explaining what we are going to code and how it works.
[00:03:23.120 --> 00:03:28.600]   The first thing I will do is to give you some background knowledge from a mathematical point of view,
[00:03:28.600 --> 00:03:36.880]   but also from a conceptual point of view of how the diffusion models work and how stable diffusion works.
[00:03:36.880 --> 00:03:41.600]   And then we will build each part one by one.
[00:03:41.600 --> 00:03:49.800]   Of course at the beginning you will have a lot of ideas that are kind of confused because I will give you a lot of new concepts to grasp.
[00:03:49.800 --> 00:03:52.920]   And it's normal that you don't understand everything at the beginning.
[00:03:52.920 --> 00:03:57.840]   But don't worry because while coding I will repeat each concept more than once.
[00:03:57.840 --> 00:04:05.360]   So while coding you will also get a practical knowledge of what each part is doing and how they interact with each other.
[00:04:05.360 --> 00:04:10.560]   So please don't be scared if you don't understand everything in the beginning part of this video.
[00:04:10.560 --> 00:04:14.440]   Later when we start coding it everything will make sense to you.
[00:04:14.440 --> 00:04:22.480]   But we need this initial part because otherwise we cannot just jump in the dark and start coding without knowing what we are going to code.
[00:04:22.480 --> 00:04:26.800]   So let's start our journey.
[00:04:26.800 --> 00:04:28.720]   So what is stable diffusion?
[00:04:28.720 --> 00:04:36.080]   Stable diffusion is a model that was introduced in 2022, so last year, at the end of last year I remember,
[00:04:36.080 --> 00:04:40.680]   by Confit's group at the Ludwig Maximilian University in Munich, Germany.
[00:04:40.680 --> 00:04:45.560]   And it's open source, the weights, the pre-trained weights can be found on the Internet.
[00:04:45.560 --> 00:04:53.240]   And it became very famous because people started doing a lot of stuff and building projects with them and products with them with the stable diffusion.
[00:04:53.240 --> 00:04:58.320]   And one of the most simple use of stable diffusion is to do text to image.
[00:04:58.320 --> 00:05:01.040]   So given a prompt we want to generate an image.
[00:05:01.040 --> 00:05:05.840]   We will also see how image to image works and also how in-painting works.
[00:05:05.840 --> 00:05:12.280]   Image to image means that you already have a picture, for example, of a dog and you want to change it a little bit by using a prompt.
[00:05:12.280 --> 00:05:17.880]   For example, you want to ask the model to add the wings to the dog so that it looks like a flying dog.
[00:05:17.880 --> 00:05:21.920]   Or in-painting means that you remove some part of the image.
[00:05:21.920 --> 00:05:30.000]   For example, you can remove, I don't know, this part here and you ask the model to replace it with some other part that makes sense,
[00:05:30.000 --> 00:05:35.800]   that is coherent with the image. And we will see also how this works.
[00:05:35.800 --> 00:05:41.840]   Let's jump into generative models because diffusion models are generative models.
[00:05:41.840 --> 00:05:44.120]   But what is a generative model?
[00:05:44.120 --> 00:05:54.640]   Well, a generative model learns a probability distribution of the data such that we can then sample from the distribution to create new instances of the data.
[00:05:54.640 --> 00:05:59.640]   For example, if we have many pictures of cats or dogs or whatever we have,
[00:05:59.640 --> 00:06:08.560]   we can train a generative model on it and then we can sample from this distribution to create new images of cats or dogs or whatever.
[00:06:08.560 --> 00:06:11.080]   And this is exactly what we do with stable diffusion.
[00:06:11.080 --> 00:06:15.720]   We actually have a lot of images, we train it on a massive amount of images,
[00:06:15.720 --> 00:06:22.320]   and then we sample from this distribution to generate new images that don't exist in our training set.
[00:06:22.320 --> 00:06:30.240]   But the question may arise in your mind is why do we model data as distributions, as probability distributions?
[00:06:30.240 --> 00:06:38.200]   Well, let me give you an example. Imagine you are a criminal and you want to generate thousands of fake identities.
[00:06:38.200 --> 00:06:45.520]   Imagine you also live in a very simple world and each fake identity is made up of variables representing the characteristic of a person.
[00:06:45.520 --> 00:06:49.600]   So age and height. Suppose we only have two variables that make up a person.
[00:06:49.600 --> 00:06:53.520]   So it's the age of the person and the height of the person.
[00:06:53.520 --> 00:06:59.800]   In my case, I will be using the centimetre for the height. I think the Americans can convert it to feet.
[00:06:59.800 --> 00:07:04.120]   And so how do we proceed if we are a criminal with this goal?
[00:07:04.120 --> 00:07:11.320]   Well, we can ask the statistics department of the government to give us some statistics about the age and the height of the population.
[00:07:11.320 --> 00:07:17.800]   This information you can easily find online, for example. And then we can sample from this distribution.
[00:07:17.800 --> 00:07:25.840]   For example, if we model the age of the population like a Gaussian with the mean of 40 and the variance of 30.
[00:07:25.840 --> 00:07:29.640]   OK, these numbers are made up. I don't know if they reflect the reality.
[00:07:29.640 --> 00:07:38.120]   And the height in centimetres is 120 as mean and the variance is 100.
[00:07:38.120 --> 00:07:45.240]   We get these two distributions. Then we can sample from these two distributions to generate a fake identity.
[00:07:45.240 --> 00:07:52.280]   What does it mean to sample from a distribution? To sample from this kind of distribution means to throw a coin,
[00:07:52.280 --> 00:08:00.200]   a very special coin that has a very high chance of falling in this area, a lower chance of falling in this area,
[00:08:00.200 --> 00:08:07.240]   an even lower chance of falling in this area and a very nearly zero chance of falling in this area.
[00:08:07.240 --> 00:08:12.880]   So imagine we flip this coin once for the age, for example, and it falls here.
[00:08:12.880 --> 00:08:16.760]   So it's quite probable, not very probable, but quite probable.
[00:08:16.760 --> 00:08:27.960]   So suppose the age is three and let me write. So the age, let's say, is. Three.
[00:08:27.960 --> 00:08:35.320]   And then we toss again this coin and we and the coin falls, let's say here.
[00:08:35.320 --> 00:08:43.560]   So one hundred, let's say, thirty height. One hundred thirty centimetres.
[00:08:43.560 --> 00:08:49.960]   So as you can see, the combination of age and height is quite improbable in reality.
[00:08:49.960 --> 00:08:55.160]   I mean, no three years old is one metre and thirty centimetres high.
[00:08:55.160 --> 00:09:02.600]   I mean, at least not the ones I know. So this combination of age and height is very not plausible.
[00:09:02.600 --> 00:09:07.320]   So to produce plausible pairs, we actually need to model these two variables.
[00:09:07.320 --> 00:09:12.840]   So the age and height, not as independent variables and sample from each of them independently,
[00:09:12.840 --> 00:09:19.080]   but as a joint distribution. And usually we represent the joint distribution like this,
[00:09:19.080 --> 00:09:25.000]   where each combination of age and height has a probability score associated with it.
[00:09:25.000 --> 00:09:29.240]   And from this distribution, we only sample using one coin.
[00:09:29.240 --> 00:09:35.160]   And for example, this coin will have a very high probability with very high chance will fall in this area,
[00:09:35.160 --> 00:09:40.440]   with less chance will fall in this area and very close to zero chance of falling in this area.
[00:09:40.440 --> 00:09:44.680]   Suppose we throw the coin and it ends up in this area to get to the corresponding.
[00:09:44.680 --> 00:09:50.280]   Suppose this is the age and this is the height to get to the corresponding age and height.
[00:09:50.280 --> 00:09:55.080]   We just need to do like this. And suppose these are actually the real height and the real height.
[00:09:55.080 --> 00:10:01.080]   Now the numbers here are actually do not match, but you got the idea that to model something,
[00:10:01.080 --> 00:10:03.960]   we need a joint distribution over all the variables.
[00:10:03.960 --> 00:10:06.680]   And this is actually what we do also with our images.
[00:10:06.680 --> 00:10:12.680]   With our images, we create a very complex distribution in which, for example,
[00:10:12.680 --> 00:10:20.120]   each pixel is a distribution and the entirety of all the pixels are one big joint distribution.
[00:10:20.120 --> 00:10:24.520]   And once we have a joint distribution, we can do a lot of interesting things.
[00:10:24.520 --> 00:10:26.440]   For example, we can marginalize.
[00:10:26.440 --> 00:10:30.840]   So, for example, imagine we have a joint distribution over the age and the height.
[00:10:30.840 --> 00:10:36.200]   So let's call the age X and let's call the height, let's say Y.
[00:10:36.200 --> 00:10:42.440]   So if we have a joint distribution, which means having P of X and Y,
[00:10:42.440 --> 00:10:48.680]   which is defined for each combination of X and Y, we can always calculate P of X.
[00:10:48.680 --> 00:10:53.560]   So the probability of over the single variable by marginalizing over the other.
[00:10:53.560 --> 00:10:59.320]   So as the integral of P of X and Y and the Y.
[00:10:59.320 --> 00:11:02.280]   And this is how we marginalize,
[00:11:02.280 --> 00:11:06.040]   which means marginalizing over all the possible Y that we can have.
[00:11:06.040 --> 00:11:09.960]   And then we can also calculate the probability, the conditional probability.
[00:11:09.960 --> 00:11:12.360]   For example, we can say that the probability,
[00:11:12.360 --> 00:11:19.080]   what is the probability of the age being, let's say, from 0 to 3,
[00:11:19.080 --> 00:11:23.320]   given that the height is more than 1 meter.
[00:11:23.320 --> 00:11:24.920]   So something like this.
[00:11:24.920 --> 00:11:28.760]   We can do this kind of queries by using the conditional probability.
[00:11:28.760 --> 00:11:31.400]   So this is actually what we do with the generative model.
[00:11:31.400 --> 00:11:34.920]   We model our data as a very big joint distribution.
[00:11:34.920 --> 00:11:39.800]   And then we learn the parameters of this distribution,
[00:11:39.800 --> 00:11:41.480]   because it's a very complex distribution.
[00:11:41.480 --> 00:11:45.160]   So we let the neural network learn the parameters of this distribution.
[00:11:45.160 --> 00:11:51.000]   And our goal, of course, is to learn this very complex distribution
[00:11:51.000 --> 00:11:53.720]   and then sample from it to generate new data,
[00:11:53.720 --> 00:11:58.360]   just like the criminal before wanted to generate new fake identities
[00:11:58.360 --> 00:12:00.680]   by modeling the very complex distribution
[00:12:00.680 --> 00:12:03.000]   that represents the identity of a person.
[00:12:03.000 --> 00:12:08.680]   In our case, we will model our system as a joint distribution
[00:12:08.680 --> 00:12:10.920]   by including also some latent variables.
[00:12:10.920 --> 00:12:11.880]   So let me describe.
[00:12:11.880 --> 00:12:16.200]   As you probably are familiar with the diffusion models,
[00:12:16.200 --> 00:12:17.560]   we have two processes.
[00:12:17.560 --> 00:12:19.480]   One is called the forward process
[00:12:19.480 --> 00:12:21.960]   and one is called the reverse process.
[00:12:21.960 --> 00:12:25.000]   The forward process means that we have our initial image
[00:12:25.000 --> 00:12:28.760]   that we will call X0, so this here,
[00:12:28.760 --> 00:12:32.120]   and we add noise to it to get another image
[00:12:32.120 --> 00:12:35.240]   that is the same as the previous one,
[00:12:35.240 --> 00:12:37.240]   but with some noise on top of it.
[00:12:37.240 --> 00:12:40.920]   Then we take this image, which has a little noise,
[00:12:40.920 --> 00:12:44.440]   and we generate a new image that is same as the previous one,
[00:12:44.440 --> 00:12:45.960]   but with even more noise.
[00:12:45.960 --> 00:12:48.520]   So as you can see, this one has even more noise,
[00:12:48.520 --> 00:12:49.880]   and so on, so on, so on,
[00:12:49.880 --> 00:12:53.960]   until we arrive to the last latent variable called Zt,
[00:12:53.960 --> 00:12:57.480]   where t is equal to 1000,
[00:12:57.480 --> 00:13:01.560]   when it becomes completely noise, pure noise,
[00:13:01.560 --> 00:13:05.960]   like N0,1, actually N0i,
[00:13:05.960 --> 00:13:09.080]   because we are in the multivariate world.
[00:13:09.080 --> 00:13:13.160]   And our goal, actually, is to...
[00:13:13.160 --> 00:13:16.520]   this process, this forward process is fixed.
[00:13:16.520 --> 00:13:20.280]   So we define how to build the noisified version
[00:13:20.280 --> 00:13:22.600]   of each image given the previous one,
[00:13:22.600 --> 00:13:24.520]   so we know how to add noise,
[00:13:24.520 --> 00:13:27.720]   and we have a specific formula, an analytical formula,
[00:13:27.720 --> 00:13:30.600]   on how to add noise to an image.
[00:13:30.600 --> 00:13:34.200]   The problem is, we don't have the analytical formula
[00:13:34.200 --> 00:13:35.720]   to reverse this process,
[00:13:35.720 --> 00:13:38.280]   so we don't know how to take this one
[00:13:38.280 --> 00:13:40.200]   and just remove noise.
[00:13:40.200 --> 00:13:43.640]   There is no closed formula on how to do it,
[00:13:43.640 --> 00:13:47.720]   so we learn, we train a neural network
[00:13:47.720 --> 00:13:49.800]   to do this inverse process,
[00:13:49.800 --> 00:13:52.840]   to remove noise from something that has noise.
[00:13:52.840 --> 00:13:54.200]   And if you think about it,
[00:13:54.200 --> 00:13:56.840]   it is quite easy to add noise to something
[00:13:56.840 --> 00:13:59.720]   than it is to remove noise from something.
[00:13:59.720 --> 00:14:02.680]   That's why we are using a neural network for this purpose.
[00:14:02.680 --> 00:14:06.840]   Now we need to go inside, of course, of the math,
[00:14:06.840 --> 00:14:09.720]   because we will be using it not only to write the code,
[00:14:09.720 --> 00:14:11.080]   but also to write the sampler.
[00:14:11.080 --> 00:14:13.800]   And in the sampler, it's all about mathematics.
[00:14:13.800 --> 00:14:16.840]   And I will try to simplify it as much as possible,
[00:14:16.840 --> 00:14:18.360]   so don't be scared.
[00:14:18.360 --> 00:14:19.160]   So let's start.
[00:14:19.160 --> 00:14:22.280]   Okay, this is from the DDPM paper,
[00:14:22.280 --> 00:14:24.600]   so the Noising Diffusion Probabilistic Models,
[00:14:24.600 --> 00:14:27.480]   from Ho in 2020.
[00:14:27.480 --> 00:14:29.240]   And here we have two processes.
[00:14:29.240 --> 00:14:31.720]   The first is the forward process,
[00:14:31.720 --> 00:14:36.440]   which means that given the original image,
[00:14:36.440 --> 00:14:40.680]   how can I generate the noisified version of this image
[00:14:40.680 --> 00:14:42.360]   at time step t?
[00:14:42.360 --> 00:14:45.000]   In this case, actually, this is the joint distribution.
[00:14:45.000 --> 00:14:49.000]   Let's look at this one here.
[00:14:49.000 --> 00:14:53.560]   This means if I have the image at time step t minus one,
[00:14:53.560 --> 00:14:55.400]   how can I get the next time step,
[00:14:55.400 --> 00:14:58.920]   so the more noisified version of this image?
[00:14:58.920 --> 00:15:04.200]   Well, we define it as a Gaussian distribution centered,
[00:15:04.200 --> 00:15:06.600]   so the mean centered on the previous one,
[00:15:06.600 --> 00:15:10.520]   and the variance defined by this beta parameter here.
[00:15:10.520 --> 00:15:13.960]   This beta parameter here is decided by us,
[00:15:13.960 --> 00:15:16.920]   and it means how much noise we want to add
[00:15:16.920 --> 00:15:19.560]   at every step of this noisification process.
[00:15:19.560 --> 00:15:23.320]   This is also known as the Markov chain of noisification,
[00:15:23.320 --> 00:15:26.840]   because each variable is conditioned on the previous one.
[00:15:26.840 --> 00:15:30.600]   So to get xt, we need to have xt minus one.
[00:15:30.600 --> 00:15:32.520]   And as you can see from here,
[00:15:32.520 --> 00:15:36.120]   we start from x0, we go to x1.
[00:15:36.120 --> 00:15:38.360]   Here I call it z1 to differentiate it,
[00:15:38.360 --> 00:15:40.920]   but x1 actually is equal to z1.
[00:15:40.920 --> 00:15:43.000]   So x0 is the original image,
[00:15:43.000 --> 00:15:46.360]   and all the next x are noisy versions,
[00:15:46.360 --> 00:15:48.760]   with xt being the most noisy.
[00:15:48.760 --> 00:15:53.240]   So this is called the Markov chain of noisification,
[00:15:53.240 --> 00:15:56.840]   and we can do it like this.
[00:15:56.840 --> 00:15:59.800]   So it's defined by us as a process,
[00:15:59.800 --> 00:16:02.680]   which is a series of Gaussians that add noise.
[00:16:02.680 --> 00:16:05.880]   There is an interesting formula here.
[00:16:05.880 --> 00:16:08.440]   This is a closed loop, closed formula,
[00:16:08.440 --> 00:16:13.320]   to go from the original image to any image at time step t,
[00:16:13.320 --> 00:16:16.120]   without calculating all the intermediate images,
[00:16:16.120 --> 00:16:19.480]   using this particular parametrization.
[00:16:19.480 --> 00:16:23.400]   So we can go from the image, original image,
[00:16:23.400 --> 00:16:25.000]   to the image at time step t,
[00:16:25.000 --> 00:16:29.480]   by sampling from this distribution,
[00:16:29.480 --> 00:16:31.160]   by defining the distribution like this.
[00:16:31.160 --> 00:16:33.800]   So with this mean and with this variance.
[00:16:33.800 --> 00:16:38.440]   This mean here depends on a parameter, alpha, alpha bar,
[00:16:38.440 --> 00:16:40.440]   which is actually depending on beta.
[00:16:40.440 --> 00:16:42.040]   So it's something that we know,
[00:16:42.040 --> 00:16:43.720]   there is nothing we have to learn.
[00:16:43.720 --> 00:16:46.360]   And also the variance actually depends on alpha,
[00:16:46.360 --> 00:16:49.400]   which is defined as in function of beta.
[00:16:49.400 --> 00:16:51.240]   So beta is also something we know,
[00:16:51.240 --> 00:16:53.080]   so there is no parameters to learn here.
[00:16:53.080 --> 00:16:55.880]   Now let's look at the reverse process.
[00:16:55.880 --> 00:16:59.240]   The reverse process means that we have something noisy,
[00:16:59.240 --> 00:17:01.400]   and we want to get something less noisy.
[00:17:01.400 --> 00:17:03.480]   So we want to remove noise.
[00:17:03.480 --> 00:17:05.960]   And we also define it as a Gaussian,
[00:17:05.960 --> 00:17:10.600]   with a mean, mu theta, and a variance, sigma theta.
[00:17:10.600 --> 00:17:16.920]   Now, this mean and this variance are not known to us.
[00:17:16.920 --> 00:17:18.520]   We have to learn them.
[00:17:18.520 --> 00:17:23.000]   And we will use a neural network to learn these two parameters.
[00:17:23.000 --> 00:17:26.920]   Actually, the variance, we will also set this at fixed.
[00:17:27.800 --> 00:17:32.360]   We will parameterize it in such a way that this variance actually is fixed.
[00:17:32.360 --> 00:17:34.920]   So we hypothesize, we already know the variance.
[00:17:34.920 --> 00:17:38.680]   And we let the network learn only the mean of this distribution.
[00:17:38.680 --> 00:17:42.680]   So to rehearse, we have a forward process that adds noise.
[00:17:42.680 --> 00:17:44.280]   And we know everything about this process.
[00:17:44.280 --> 00:17:45.800]   We know how to add noise.
[00:17:45.800 --> 00:17:49.640]   We have a reverse process that we don't know how to denoise.
[00:17:49.640 --> 00:17:53.080]   So we let a network learn the parameters on how to denoise it.
[00:17:54.680 --> 00:17:58.360]   And OK, now that we have defined these two processes,
[00:17:58.360 --> 00:18:01.560]   how do we actually train a model to do it?
[00:18:01.560 --> 00:18:05.000]   Because as you remember, our initial goal is actually to learn
[00:18:05.000 --> 00:18:09.000]   a probability distribution over our data set.
[00:18:09.000 --> 00:18:12.040]   And so this quantity here.
[00:18:12.040 --> 00:18:15.880]   But unlike before, when we could marginalize, for example,
[00:18:15.880 --> 00:18:20.120]   in the case of the criminal who want to generate identities,
[00:18:20.120 --> 00:18:22.680]   we could marginalize over all the variables.
[00:18:23.320 --> 00:18:24.600]   Here we cannot marginalize.
[00:18:24.600 --> 00:18:28.920]   Because we need to marginalize over x1, x2, xt, x4, up to xt.
[00:18:28.920 --> 00:18:30.520]   So over a lot of variables.
[00:18:30.520 --> 00:18:36.600]   And to calculate this integral means to calculate it over all the possible x1.
[00:18:36.600 --> 00:18:38.760]   And over all the possible x2, et cetera.
[00:18:38.760 --> 00:18:44.040]   So it's a very complex calculation that is computationally intractable, we say.
[00:18:44.040 --> 00:18:46.440]   It means that it's theoretically possible.
[00:18:46.440 --> 00:18:48.600]   But practically, it will take forever.
[00:18:48.600 --> 00:18:50.840]   So we cannot use this route here.
[00:18:52.040 --> 00:18:53.320]   So what can we do?
[00:18:53.320 --> 00:18:55.080]   We want to learn this quantity here.
[00:18:55.080 --> 00:19:00.520]   So we want to learn the parameter theta of this to maximize the likelihood we can see here.
[00:19:00.520 --> 00:19:06.520]   What we did is we found a lower bound for this quantity here.
[00:19:06.520 --> 00:19:08.520]   So the quantity, the likelihood.
[00:19:08.520 --> 00:19:11.160]   And this lower bound is called the elbow.
[00:19:11.160 --> 00:19:16.040]   And if we maximize the lower bound, it will also maximize the likelihood.
[00:19:16.040 --> 00:19:21.800]   So let me give you a parallel example on what it means to maximize the lower bound.
[00:19:21.800 --> 00:19:24.920]   For example, imagine you have a company.
[00:19:24.920 --> 00:19:26.840]   And your company has some revenue.
[00:19:26.840 --> 00:19:33.560]   And usually, the revenue is more than or equal to the sales of your company.
[00:19:33.560 --> 00:19:35.640]   So you have some revenue coming from sales.
[00:19:35.640 --> 00:19:41.000]   Maybe you also have some revenue coming from interest that you get from your bank, et cetera.
[00:19:41.000 --> 00:19:47.240]   But we can for sure say that the revenue of your company is more than or equal to the sales of your company.
[00:19:47.240 --> 00:19:52.280]   So if you want to maximize your revenue, you can maximize your sales, for example,
[00:19:52.280 --> 00:19:55.480]   which is a lower bound over your revenue.
[00:19:55.480 --> 00:19:58.200]   So if we maximize the sales, we will also maximize the revenue.
[00:19:58.200 --> 00:19:59.400]   And this is the idea here.
[00:19:59.400 --> 00:20:03.880]   But how do we do it on a practical level?
[00:20:03.880 --> 00:20:13.240]   Well, this is the training code for the DDPM diffusion models as defined by the DDPM paper.
[00:20:13.240 --> 00:20:17.000]   And basically, the idea is after we get the elbow,
[00:20:17.000 --> 00:20:20.360]   we can parameterize the loss function as this.
[00:20:20.360 --> 00:20:27.800]   Which says that we need to learn-- we need to train a network called epsilon theta.
[00:20:27.800 --> 00:20:31.480]   That given a noisy image-- so this formula here
[00:20:31.480 --> 00:20:37.800]   means the noisy image at time step t and the time step at which the noise was added,
[00:20:37.800 --> 00:20:43.240]   the network has to predict how much noise is in the image, the noisified image.
[00:20:43.240 --> 00:20:50.600]   And if we do gradient descent over this loss function here,
[00:20:50.600 --> 00:20:53.400]   we will maximize the elbow.
[00:20:53.400 --> 00:20:59.080]   And at the same time, we will also maximize the log likelihood of our data.
[00:20:59.880 --> 00:21:03.000]   And this is how we train these kind of networks.
[00:21:03.000 --> 00:21:06.920]   Now, I know that this is a lot of concept that you have to grasp.
[00:21:06.920 --> 00:21:07.960]   So don't worry.
[00:21:07.960 --> 00:21:11.400]   For now, just remember that there is a forward process and there is a reverse process.
[00:21:11.400 --> 00:21:15.000]   And to train this network to do the reverse process,
[00:21:15.000 --> 00:21:18.280]   we need to train a network to detect how much noise
[00:21:18.280 --> 00:21:21.560]   is in a noisified version of the image at time step t.
[00:21:21.560 --> 00:21:29.800]   Let me show you how do we-- once we have this network that has already been trained,
[00:21:29.800 --> 00:21:32.280]   how do we actually sample to generate new data?
[00:21:32.280 --> 00:21:35.560]   So let's go here.
[00:21:35.560 --> 00:21:36.920]   Let's go here.
[00:21:36.920 --> 00:21:38.600]   So how do we generate new data?
[00:21:38.600 --> 00:21:44.680]   Suppose we already have a network that was trained for detecting how much noise is in there.
[00:21:44.680 --> 00:21:47.720]   And what we do is we start from complete noise.
[00:21:47.720 --> 00:21:52.040]   And then we ask the network to detect how much noise is in there.
[00:21:52.040 --> 00:21:53.400]   We remove this noise.
[00:21:53.400 --> 00:21:56.440]   And then we ask the network again how much noise is in there.
[00:21:56.440 --> 00:21:57.640]   And we remove it.
[00:21:57.640 --> 00:22:00.280]   And then we ask the network how much noise is there.
[00:22:00.280 --> 00:22:02.040]   OK, remove it.
[00:22:02.040 --> 00:22:03.480]   Then how much noise is here?
[00:22:03.480 --> 00:22:05.160]   OK, remove it, et cetera, et cetera.
[00:22:05.160 --> 00:22:09.560]   Until we reach this step, then here we will have something new.
[00:22:09.560 --> 00:22:13.720]   So if we start from pure noise and we do this reverse process many times,
[00:22:13.720 --> 00:22:15.480]   we will end up with something new.
[00:22:15.480 --> 00:22:19.160]   And this is the idea behind this generative model.
[00:22:19.160 --> 00:22:23.720]   Now that we know how to generate new data starting from pure noise,
[00:22:24.520 --> 00:22:29.160]   we also want to be able to control this denoisification process
[00:22:29.160 --> 00:22:32.280]   so we can generate images of something that we want.
[00:22:32.280 --> 00:22:36.120]   I mean, how can we tell the model to generate a picture of a cat
[00:22:36.120 --> 00:22:40.840]   or a picture of a dog or a picture of a house by starting from pure noise?
[00:22:40.840 --> 00:22:46.360]   Because as of now, by starting from pure noise and keep denoising,
[00:22:46.360 --> 00:22:48.680]   we will generate a new image, of course.
[00:22:48.680 --> 00:22:52.440]   But it's not like we can control which new image will be generated.
[00:22:52.440 --> 00:22:57.160]   So we need to find a way to tell the model what we want in this generational process.
[00:22:57.160 --> 00:23:01.800]   And the idea is that we start from pure noise.
[00:23:01.800 --> 00:23:07.640]   And during this chain of removing noise, so denoisification,
[00:23:07.640 --> 00:23:09.400]   we introduce a signal.
[00:23:09.400 --> 00:23:11.400]   Let's call it prompt.
[00:23:11.400 --> 00:23:12.620]   Prompt.
[00:23:12.620 --> 00:23:16.440]   Or it can also be called the conditioning signal.
[00:23:16.440 --> 00:23:18.920]   Or it can also be called the context.
[00:23:18.920 --> 00:23:20.760]   Anyway, they are the same concept.
[00:23:21.720 --> 00:23:26.280]   In which we influence the model into how to remove the noise
[00:23:26.280 --> 00:23:29.640]   so that the output will move towards what we want.
[00:23:29.640 --> 00:23:33.560]   To understand how this works, let's review again
[00:23:33.560 --> 00:23:36.200]   how the training of this kind of networks works.
[00:23:36.200 --> 00:23:37.880]   Because this is very important for us.
[00:23:37.880 --> 00:23:40.920]   To learn how the training of this kind of network goes,
[00:23:40.920 --> 00:23:42.840]   so that we can introduce the prompt.
[00:23:42.840 --> 00:23:43.480]   Let's go back.
[00:23:46.760 --> 00:23:53.560]   Okay, as I told you before, our final goal is to model a distribution,
[00:23:53.560 --> 00:23:58.520]   theta, p of theta, such that we maximize the likelihood of our data.
[00:23:58.520 --> 00:24:07.000]   And to learn this distribution, we maximize the ELBO, so the lower bound.
[00:24:07.000 --> 00:24:09.240]   But how do we maximize the ELBO?
[00:24:09.240 --> 00:24:14.840]   We minimize this loss, minimize this loss here.
[00:24:14.840 --> 00:24:17.960]   So by minimizing this loss, we maximize the ELBO,
[00:24:17.960 --> 00:24:21.400]   which in turn learns this distribution here.
[00:24:21.400 --> 00:24:25.800]   Because this ELBO here is the lower bound
[00:24:25.800 --> 00:24:29.000]   for the likelihood of our data distribution here.
[00:24:29.000 --> 00:24:32.440]   And what is this loss function?
[00:24:32.440 --> 00:24:37.160]   Loss function here indicates that we need to create a model, epsilon theta,
[00:24:37.160 --> 00:24:44.200]   such that if we give this model a noisified image at a particular noise level,
[00:24:44.200 --> 00:24:47.880]   and we also tell him what noise level we included in this image,
[00:24:47.880 --> 00:24:51.880]   the network has to predict how much noise is there.
[00:24:51.880 --> 00:24:54.520]   So this epsilon is how much noise we have added.
[00:24:54.520 --> 00:25:00.280]   And we can do a gradient descent on this training loop.
[00:25:00.280 --> 00:25:06.040]   This way we will learn a distribution of our data.
[00:25:06.040 --> 00:25:09.560]   But as you can see, this distribution doesn't include anything
[00:25:09.560 --> 00:25:14.120]   that tells the model what is a cat, or what is a dog, or what is a house.
[00:25:14.120 --> 00:25:18.280]   The model is just learning how to generate pictures that make sense,
[00:25:18.280 --> 00:25:21.000]   that are similar to our initial training data.
[00:25:21.000 --> 00:25:24.680]   But they don't know what is the relationship between that picture and the prompt.
[00:25:24.680 --> 00:25:33.400]   So one idea could be, OK, can we learn a joint distribution of our initial data,
[00:25:33.400 --> 00:25:37.480]   so all the images, and the conditioning signal, so the prompt?
[00:25:38.120 --> 00:25:40.120]   Well, this is also something that we don't want,
[00:25:40.120 --> 00:25:42.680]   because we want to actually learn this distribution,
[00:25:42.680 --> 00:25:45.240]   so that we can sample and generate new data.
[00:25:45.240 --> 00:25:47.000]   We don't want to learn the joint distribution
[00:25:47.000 --> 00:25:50.040]   that will be too much influenced by the context,
[00:25:50.040 --> 00:25:53.320]   and the model may not learn the generative process of the data.
[00:25:53.320 --> 00:25:55.560]   So our final goal is always this one.
[00:25:55.560 --> 00:25:59.240]   But we also want to find some how to condition this model
[00:25:59.240 --> 00:26:01.160]   into building something that we want.
[00:26:01.160 --> 00:26:07.000]   And the idea is that we modify this unit,
[00:26:07.000 --> 00:26:11.400]   so this model here, epsilon theta,
[00:26:11.400 --> 00:26:16.280]   will be built using, let me show you, this unit model here.
[00:26:16.280 --> 00:26:21.240]   This unit will receive as input an image that is noisified,
[00:26:21.240 --> 00:26:27.400]   so for example, a cat, with a particular noise level,
[00:26:27.400 --> 00:26:31.240]   and we also tell him what is the noise level that we added to this cat,
[00:26:31.240 --> 00:26:33.560]   and we give them both to the input of the unit,
[00:26:33.560 --> 00:26:37.240]   and the unit has to predict how much noise is there.
[00:26:37.240 --> 00:26:38.600]   This is the job of the unit.
[00:26:38.600 --> 00:26:43.400]   What if we introduce also the prompt signal here,
[00:26:43.400 --> 00:26:46.120]   so the conditioning signal here, so the prompt?
[00:26:46.120 --> 00:26:49.400]   This way, if we tell the model,
[00:26:49.400 --> 00:26:53.560]   can you remove noise from this image,
[00:26:53.560 --> 00:26:56.760]   which has this quantity of noise,
[00:26:56.760 --> 00:26:59.080]   and I am also telling you that it's a cat,
[00:26:59.080 --> 00:27:01.880]   so the model has more information on how to remove the noise.
[00:27:03.080 --> 00:27:05.080]   Yes, the model can learn this way,
[00:27:05.080 --> 00:27:10.360]   how to remove noise into building something that is more closer to the prompt.
[00:27:10.360 --> 00:27:14.280]   This will make the model conditioned,
[00:27:14.280 --> 00:27:16.760]   it means that it will act like a conditioned model,
[00:27:16.760 --> 00:27:19.560]   so we need to tell the model what is the condition that we want,
[00:27:19.560 --> 00:27:23.720]   so that the model can remove the noise in that particular way,
[00:27:23.720 --> 00:27:27.480]   moving the output towards that particular prompt.
[00:27:27.480 --> 00:27:31.400]   But at the same time, when we train the model,
[00:27:31.400 --> 00:27:35.960]   instead of only giving images along with the prompt,
[00:27:35.960 --> 00:27:38.840]   we can also sometimes, with a probability, let's say 50%,
[00:27:38.840 --> 00:27:43.720]   not give any prompt and let the model remove the noise
[00:27:43.720 --> 00:27:46.040]   without telling him anything about the prompt.
[00:27:46.040 --> 00:27:50.680]   So we just give him a bunch of zero when we give him the input.
[00:27:50.680 --> 00:27:56.600]   This way, the model will learn to act both as a conditioned model
[00:27:56.600 --> 00:27:58.520]   and also as a conditioned model,
[00:27:58.520 --> 00:28:01.080]   so the model will learn to pay attention to the prompt
[00:28:01.080 --> 00:28:03.240]   and also to not pay attention to the prompt.
[00:28:03.240 --> 00:28:05.880]   And what is the advantage of this?
[00:28:05.880 --> 00:28:11.320]   Is that we can, once when we want to generate a new picture,
[00:28:11.320 --> 00:28:12.920]   we can do two steps.
[00:28:12.920 --> 00:28:16.680]   In the first one, suppose you want to generate a picture of a cat,
[00:28:16.680 --> 00:28:17.560]   we can do like this.
[00:28:17.560 --> 00:28:19.560]   Let me delete first of all.
[00:28:19.560 --> 00:28:23.000]   Okay, we can do the first step.
[00:28:23.000 --> 00:28:24.600]   So let's call it step one.
[00:28:27.160 --> 00:28:30.440]   And we can start with pure noise,
[00:28:30.440 --> 00:28:31.720]   because as I told you before,
[00:28:31.720 --> 00:28:34.360]   to generate a new image, we start from pure noise.
[00:28:34.360 --> 00:28:37.240]   We indicate the model what is the noise level.
[00:28:37.240 --> 00:28:40.600]   So at the beginning, it will be t equal to 1000,
[00:28:40.600 --> 00:28:42.120]   so maximum noise level.
[00:28:42.120 --> 00:28:44.920]   And we tell the model that we want a cat.
[00:28:44.920 --> 00:28:49.320]   We give this as input to the unit.
[00:28:49.320 --> 00:28:53.800]   The unit will predict some noise that we need to remove
[00:28:53.800 --> 00:28:57.880]   in order to move the image towards what we want as output.
[00:28:57.880 --> 00:28:58.520]   So a cat.
[00:28:58.520 --> 00:29:01.160]   And this is our output one.
[00:29:01.160 --> 00:29:03.720]   Let's call it output one.
[00:29:03.720 --> 00:29:05.320]   Then we do another step.
[00:29:05.320 --> 00:29:06.920]   So let me delete this one.
[00:29:06.920 --> 00:29:09.560]   Then we do another step.
[00:29:09.560 --> 00:29:11.240]   Let's call it step two.
[00:29:11.240 --> 00:29:17.400]   And again, we give the same input noise as before,
[00:29:17.400 --> 00:29:21.720]   the same time step as the noise level.
[00:29:21.720 --> 00:29:24.040]   So it's the same noise with the same noise level,
[00:29:24.040 --> 00:29:25.480]   but we don't give any prompt.
[00:29:25.480 --> 00:29:28.760]   This way, the model will build some output.
[00:29:28.760 --> 00:29:30.120]   Let's call it out two,
[00:29:30.120 --> 00:29:34.040]   which is how to remove the noise to generate something.
[00:29:34.040 --> 00:29:36.280]   We don't know what, but to generate something
[00:29:36.280 --> 00:29:38.040]   that belongs to our data distribution.
[00:29:38.040 --> 00:29:42.360]   And then we combine these two output in such a way
[00:29:42.360 --> 00:29:45.960]   that we can decide how much we want the output
[00:29:45.960 --> 00:29:49.000]   to be closer to the prompt or not.
[00:29:49.720 --> 00:29:52.760]   This is called classifier-free guidance.
[00:29:52.760 --> 00:29:57.320]   So this approach here is called classifier-free guidance.
[00:29:57.320 --> 00:29:59.640]   I will not tell you why it's called classifier-free guidance,
[00:29:59.640 --> 00:30:02.200]   because otherwise I need to introduce the classifier guidance.
[00:30:02.200 --> 00:30:03.960]   And to talk about the classifier guidance,
[00:30:03.960 --> 00:30:06.120]   I need to introduce the score-based models
[00:30:06.120 --> 00:30:07.960]   to understand why it's called like this.
[00:30:07.960 --> 00:30:11.320]   But the idea is this, that we train a model that,
[00:30:11.320 --> 00:30:14.760]   when we train it, sometimes we give it the prompt
[00:30:14.760 --> 00:30:16.440]   and sometimes we don't give it the prompt,
[00:30:16.440 --> 00:30:19.080]   so that the model learns to ignore the prompt,
[00:30:19.080 --> 00:30:21.240]   but also to pay attention to the prompt.
[00:30:21.240 --> 00:30:25.480]   And when we sample from this model, we do two steps.
[00:30:25.480 --> 00:30:27.960]   First time, we give him the prompt of what we want.
[00:30:27.960 --> 00:30:29.800]   And the second time, we give the same noise,
[00:30:29.800 --> 00:30:31.800]   but without the prompt of what we want.
[00:30:31.800 --> 00:30:35.080]   And then we combine the two output,
[00:30:35.080 --> 00:30:37.960]   conditioned and unconditioned,
[00:30:37.960 --> 00:30:41.560]   linearly with a weight that indicates
[00:30:41.560 --> 00:30:44.600]   how much we want the output to be closer
[00:30:44.600 --> 00:30:46.440]   to our condition, to our prompt.
[00:30:47.080 --> 00:30:49.960]   The higher this value, the more the output
[00:30:49.960 --> 00:30:51.640]   will resemble our prompt.
[00:30:51.640 --> 00:30:55.640]   The lower this value, the less it will resemble our prompt.
[00:30:55.640 --> 00:30:58.760]   And this is the idea behind classifier-free guidance.
[00:30:58.760 --> 00:31:01.400]   To give the prompt, actually we will give,
[00:31:01.400 --> 00:31:04.360]   we need to give some kind of embedding to the,
[00:31:04.360 --> 00:31:07.560]   so the model needs to understand this prompt.
[00:31:07.560 --> 00:31:08.840]   To understand the prompt,
[00:31:08.840 --> 00:31:11.560]   the model needs some kind of embedding.
[00:31:11.560 --> 00:31:14.920]   Embedding means that we need some vectors
[00:31:14.920 --> 00:31:17.640]   that represent the meaning of the prompt.
[00:31:17.640 --> 00:31:19.880]   And this embedding are extracted
[00:31:19.880 --> 00:31:21.640]   using the CLIP text encoder.
[00:31:21.640 --> 00:31:24.520]   So before talking about the text encoder,
[00:31:24.520 --> 00:31:25.640]   let's talk about CLIP.
[00:31:25.640 --> 00:31:28.760]   So CLIP was a model built by OpenAI
[00:31:28.760 --> 00:31:33.480]   that allowed to connect text with images.
[00:31:33.480 --> 00:31:38.360]   And the text, basically, they took a bunch of images.
[00:31:38.360 --> 00:31:41.720]   So for example, this picture and its description.
[00:31:41.720 --> 00:31:44.280]   Then they took another image along with its description.
[00:31:44.280 --> 00:31:48.120]   So the image one is associated with the text number one,
[00:31:48.120 --> 00:31:50.760]   which is the description of the image one.
[00:31:50.760 --> 00:31:53.720]   Then the image two has the description number two.
[00:31:53.720 --> 00:31:56.520]   The image three has the text number three,
[00:31:56.520 --> 00:31:58.520]   which is the description of the image three,
[00:31:58.520 --> 00:31:59.800]   et cetera, et cetera.
[00:31:59.800 --> 00:32:02.280]   They built this matrix, you can see here,
[00:32:02.280 --> 00:32:04.360]   which is made up of the dot products
[00:32:04.360 --> 00:32:06.760]   of the embedding of the first image
[00:32:06.760 --> 00:32:09.880]   multiplied with all the possible captions here.
[00:32:09.880 --> 00:32:12.280]   So the image one with the text one.
[00:32:12.280 --> 00:32:13.560]   Image one with the text two.
[00:32:13.560 --> 00:32:15.080]   Image one with the text three, et cetera.
[00:32:15.080 --> 00:32:17.080]   Then image two with the text one.
[00:32:17.080 --> 00:32:18.520]   Image two with the text two, et cetera.
[00:32:18.520 --> 00:32:20.760]   How they train it?
[00:32:20.760 --> 00:32:23.320]   Basically, we know that the correspondence
[00:32:23.320 --> 00:32:26.280]   between image and the text is on the diagonal
[00:32:26.280 --> 00:32:30.040]   because the image one is associated with the text one.
[00:32:30.040 --> 00:32:31.880]   Image two is associated with the text two.
[00:32:31.880 --> 00:32:33.880]   Image three is associated with the text three.
[00:32:33.880 --> 00:32:35.480]   So how they train it?
[00:32:35.480 --> 00:32:37.960]   Basically, they said they built a loss function
[00:32:37.960 --> 00:32:43.400]   that they want this diagonal to have the maximum value
[00:32:43.400 --> 00:32:46.040]   and all the other numbers here to be zero
[00:32:46.040 --> 00:32:47.160]   because they are not matching.
[00:32:47.160 --> 00:32:50.680]   They are not the corresponding description of these images.
[00:32:50.680 --> 00:32:54.360]   In this way, the model learned how to combine
[00:32:54.360 --> 00:32:57.880]   the description of an image with the image itself.
[00:32:57.880 --> 00:32:59.960]   And what we do in stable diffusion
[00:32:59.960 --> 00:33:02.600]   is that we take this text encoder here,
[00:33:02.600 --> 00:33:04.520]   so only this part of this clip,
[00:33:05.480 --> 00:33:09.160]   to encode our prompt to get some embeddings.
[00:33:09.160 --> 00:33:13.480]   And these embeddings are then used as conditioning signal
[00:33:13.480 --> 00:33:17.880]   for our unit to denoise the image into what we want.
[00:33:17.880 --> 00:33:24.280]   Okay, there is another thing that we need to understand.
[00:33:24.280 --> 00:33:25.560]   So as I said before,
[00:33:25.560 --> 00:33:29.560]   we have a forward process that adds noise to the image.
[00:33:29.560 --> 00:33:31.160]   Then we have a reverse process
[00:33:31.160 --> 00:33:33.480]   that removes noise from the image.
[00:33:33.480 --> 00:33:36.120]   And this reverse process can be conditioned
[00:33:36.120 --> 00:33:39.800]   by using the classifier-free guidance.
[00:33:39.800 --> 00:33:43.480]   And this reverse process means
[00:33:43.480 --> 00:33:46.840]   that we need to do many steps of denoisification
[00:33:46.840 --> 00:33:49.480]   to arrive to the image, to the new image.
[00:33:49.480 --> 00:33:53.800]   And this also means that each of these steps
[00:33:53.800 --> 00:33:57.720]   involves going through the unit with a noisified image
[00:33:57.720 --> 00:34:01.400]   and getting as output the amount of noise present in this image.
[00:34:02.200 --> 00:34:04.280]   But if the image is very big,
[00:34:04.280 --> 00:34:10.120]   so suppose this image here is 512 multiplied by 512,
[00:34:10.120 --> 00:34:12.680]   it means every time on the unit,
[00:34:12.680 --> 00:34:14.200]   we will have a very big matrix
[00:34:14.200 --> 00:34:16.600]   that needs to go through this unit.
[00:34:16.600 --> 00:34:19.000]   And this may be very slow,
[00:34:19.000 --> 00:34:22.120]   because it's a very big matrix of data
[00:34:22.120 --> 00:34:24.120]   that the unit has to work on.
[00:34:24.120 --> 00:34:27.560]   What if we could somehow compress this image
[00:34:27.560 --> 00:34:29.160]   into something smaller,
[00:34:29.160 --> 00:34:33.240]   so that each step through the unit takes less time?
[00:34:33.240 --> 00:34:35.080]   Well, the idea is that yes,
[00:34:35.080 --> 00:34:37.000]   we can compress this image
[00:34:37.000 --> 00:34:41.000]   with something that is called the variational autoencoder.
[00:34:41.000 --> 00:34:43.240]   Let's see how the variational autoencoder works.
[00:34:43.240 --> 00:34:48.920]   Okay, the stable diffusion is actually known
[00:34:48.920 --> 00:34:53.320]   as a latent diffusion model,
[00:34:53.320 --> 00:34:56.680]   because what we learn is not the data
[00:34:56.680 --> 00:35:00.440]   probability distribution Px of our data,
[00:35:00.440 --> 00:35:04.680]   but we learn the latent representation of the data
[00:35:04.680 --> 00:35:06.920]   using a variational autoencoder.
[00:35:06.920 --> 00:35:09.000]   So basically we compress our data,
[00:35:09.000 --> 00:35:10.040]   so let's go back,
[00:35:10.040 --> 00:35:12.760]   we compress our data into something smaller,
[00:35:12.760 --> 00:35:16.040]   and then we learn the noisification process
[00:35:16.040 --> 00:35:18.840]   using this compressed version of the data,
[00:35:18.840 --> 00:35:20.200]   not the original data.
[00:35:20.200 --> 00:35:23.400]   And then we can decompress it to build the original data.
[00:35:24.680 --> 00:35:28.280]   Let me show you actually how it works on a practical level.
[00:35:28.280 --> 00:35:31.560]   So imagine you have some data
[00:35:31.560 --> 00:35:34.200]   and you want to send it to your friend over the internet.
[00:35:34.200 --> 00:35:35.240]   What do you do?
[00:35:35.240 --> 00:35:36.840]   You can send the original file
[00:35:36.840 --> 00:35:38.600]   or you can send the zipped file.
[00:35:38.600 --> 00:35:40.200]   So you can zip the file,
[00:35:40.200 --> 00:35:41.960]   maybe with WinZip, for example,
[00:35:41.960 --> 00:35:43.800]   and then you send the file to your friend
[00:35:43.800 --> 00:35:47.960]   and the friend can unzip it after receiving
[00:35:47.960 --> 00:35:49.400]   and rebuild the original data.
[00:35:49.400 --> 00:35:51.880]   This is exactly the job of the autoencoder.
[00:35:51.880 --> 00:35:53.720]   The autoencoder is a network
[00:35:53.720 --> 00:35:55.480]   that given an image, for example,
[00:35:55.480 --> 00:35:58.120]   will, after passing through the encoder,
[00:35:58.120 --> 00:35:59.800]   will transform into a vector
[00:35:59.800 --> 00:36:03.160]   which has a dimension that is much smaller
[00:36:03.160 --> 00:36:04.760]   than the original image.
[00:36:04.760 --> 00:36:06.600]   And if we use this vector
[00:36:06.600 --> 00:36:08.360]   and run it through the decoder,
[00:36:08.360 --> 00:36:10.920]   it will build the original image back.
[00:36:10.920 --> 00:36:15.000]   And we can do it for many images
[00:36:15.000 --> 00:36:17.560]   and each of them will have a representation in this.
[00:36:17.560 --> 00:36:21.400]   This is called a code corresponding to each image.
[00:36:22.280 --> 00:36:23.960]   Now, the problem with autoencoder
[00:36:23.960 --> 00:36:27.480]   is that the code learned by this model
[00:36:27.480 --> 00:36:30.680]   doesn't make any sense from a semantic point of view.
[00:36:30.680 --> 00:36:33.480]   So the code associated with the cat, for example,
[00:36:33.480 --> 00:36:36.040]   may be very similar to the code
[00:36:36.040 --> 00:36:38.120]   associated with pizza, for example,
[00:36:38.120 --> 00:36:40.360]   or the code associated with a building.
[00:36:40.360 --> 00:36:43.320]   So there is no semantic relationship
[00:36:43.320 --> 00:36:44.600]   between these codes.
[00:36:44.600 --> 00:36:47.320]   And to overcome this limitation of the autoencoder,
[00:36:47.320 --> 00:36:49.640]   we introduce the variational autoencoder,
[00:36:49.640 --> 00:36:54.600]   in which we learn to kind of compress the data,
[00:36:54.600 --> 00:36:56.280]   but at the same time,
[00:36:56.280 --> 00:36:58.040]   this data is distributed
[00:36:58.040 --> 00:37:00.120]   according to a multivariate distribution,
[00:37:00.120 --> 00:37:02.440]   which most of the times is a Gaussian.
[00:37:02.440 --> 00:37:08.440]   And we learn the mean and the sigma of this distribution,
[00:37:08.440 --> 00:37:11.240]   this very complex distribution here.
[00:37:11.240 --> 00:37:13.480]   And given the latent representation,
[00:37:13.480 --> 00:37:15.640]   we can always pass it through the decoder
[00:37:15.640 --> 00:37:17.240]   to rebuild the original data.
[00:37:17.240 --> 00:37:20.360]   And this is the idea that we use also in stable diffusion.
[00:37:20.360 --> 00:37:24.440]   Now we can finally combine all these things
[00:37:24.440 --> 00:37:25.960]   that we have seen together
[00:37:25.960 --> 00:37:29.960]   to see what is the architecture of the stable diffusion.
[00:37:29.960 --> 00:37:34.760]   So let's start with how the text-to-image works.
[00:37:34.760 --> 00:37:39.160]   Now, imagine text-to-image basically works like this.
[00:37:39.160 --> 00:37:42.280]   Imagine you want to generate a picture
[00:37:42.280 --> 00:37:44.200]   of a dog with glasses.
[00:37:44.200 --> 00:37:46.120]   So you start, of course, with a prompt,
[00:37:46.120 --> 00:37:47.640]   a dog with glasses.
[00:37:47.640 --> 00:37:49.640]   And then what do we do?
[00:37:49.640 --> 00:37:52.440]   We sample some noise here,
[00:37:52.440 --> 00:37:55.080]   some noise from the N01.
[00:37:55.080 --> 00:37:58.760]   We encode it with our variational autoencoder.
[00:37:58.760 --> 00:38:03.560]   This will give us a latent representation of this noise.
[00:38:03.560 --> 00:38:04.600]   Let's call it Z.
[00:38:04.600 --> 00:38:07.160]   This is, of course, a pure noise,
[00:38:07.160 --> 00:38:09.880]   but has been compressed by the encoder.
[00:38:09.880 --> 00:38:13.960]   And then we send it to the unit.
[00:38:13.960 --> 00:38:17.480]   The goal of the unit is to detect how much noise is there.
[00:38:17.480 --> 00:38:19.320]   And also, because to the unit,
[00:38:19.320 --> 00:38:21.480]   we also give the conditioning signal,
[00:38:21.480 --> 00:38:24.440]   the unit has to detect the noise,
[00:38:24.440 --> 00:38:26.920]   what noise we need to remove
[00:38:26.920 --> 00:38:30.360]   to make it into a picture that follows the prompt,
[00:38:30.360 --> 00:38:32.200]   so into a picture of a dog.
[00:38:32.200 --> 00:38:34.280]   So the unit, we pass it through the unit
[00:38:34.280 --> 00:38:36.920]   along with the time step, initial time step,
[00:38:36.920 --> 00:38:37.720]   so 1,000.
[00:38:37.720 --> 00:38:41.800]   And the unit will detect at the output here
[00:38:41.800 --> 00:38:43.480]   how much noise is there.
[00:38:43.480 --> 00:38:46.280]   Our scheduler, we will see later what is the scheduler,
[00:38:46.280 --> 00:38:48.200]   will remove this noise
[00:38:48.200 --> 00:38:50.120]   and then send it again to the unit
[00:38:50.120 --> 00:38:52.280]   for the second step of denoisification.
[00:38:52.280 --> 00:38:54.840]   And again, we send the time step,
[00:38:54.840 --> 00:38:57.400]   which is in this case not 1,000,
[00:38:57.400 --> 00:38:59.560]   but 980, for example,
[00:38:59.560 --> 00:39:01.080]   because we skipped some steps.
[00:39:01.080 --> 00:39:03.560]   And then we again, with the noise,
[00:39:03.560 --> 00:39:05.240]   we detect how much noise is there.
[00:39:05.240 --> 00:39:07.480]   The scheduler will remove this noise
[00:39:07.480 --> 00:39:09.240]   and again send it back.
[00:39:09.240 --> 00:39:11.560]   And we do many times this.
[00:39:11.560 --> 00:39:15.000]   We keep doing this denoisification for many steps
[00:39:15.000 --> 00:39:18.920]   until there is no more noise present in the image.
[00:39:18.920 --> 00:39:22.520]   And after we have finished this loop of steps,
[00:39:22.520 --> 00:39:26.040]   we get the output Z prime,
[00:39:26.040 --> 00:39:27.480]   which is still a latent
[00:39:27.480 --> 00:39:29.240]   because this unit only works
[00:39:29.240 --> 00:39:31.000]   with the latent representation of the data,
[00:39:31.000 --> 00:39:32.200]   not with the original data.
[00:39:32.200 --> 00:39:35.000]   We pass it through the decoder
[00:39:35.000 --> 00:39:37.000]   to obtain the output image.
[00:39:38.440 --> 00:39:41.880]   And this is why this is called a latent diffusion model
[00:39:41.880 --> 00:39:43.080]   because the unit,
[00:39:43.080 --> 00:39:44.840]   so the denoisification process,
[00:39:44.840 --> 00:39:47.720]   always works with the latent representation of the data.
[00:39:47.720 --> 00:39:50.600]   And this is how we generate text to image.
[00:39:50.600 --> 00:39:53.400]   We can do the same thing for image to image.
[00:39:53.400 --> 00:39:55.480]   Image to image means that I have,
[00:39:55.480 --> 00:39:57.320]   for example, the picture of a dog
[00:39:57.320 --> 00:40:00.040]   and I want to modify this image
[00:40:00.040 --> 00:40:02.440]   into something else by using a prompt.
[00:40:02.440 --> 00:40:05.480]   For example, I want the model to add glasses to this dog
[00:40:05.480 --> 00:40:07.720]   so I can give the input image here.
[00:40:07.720 --> 00:40:09.800]   And then I say a dog with glasses
[00:40:09.800 --> 00:40:13.000]   and hopefully the model will add glasses to this dog.
[00:40:13.000 --> 00:40:13.800]   How does it work?
[00:40:13.800 --> 00:40:16.520]   We encode the image
[00:40:16.520 --> 00:40:18.920]   with the encoder of the variational autoencoder
[00:40:18.920 --> 00:40:22.360]   and we get the latent representation of our image.
[00:40:22.360 --> 00:40:24.760]   Then we add noise to this latent
[00:40:24.760 --> 00:40:26.920]   because the unit, as we saw before,
[00:40:26.920 --> 00:40:29.000]   his job is to denoise an image.
[00:40:29.000 --> 00:40:31.640]   But of course, we need to have some noise to denoise.
[00:40:31.640 --> 00:40:34.920]   So we add noise to this image
[00:40:34.920 --> 00:40:38.360]   and the amount of noise that we add to this image,
[00:40:38.360 --> 00:40:40.120]   so this starting image here,
[00:40:40.120 --> 00:40:43.800]   indicates how much freedom the unit has
[00:40:43.800 --> 00:40:45.400]   into building the output image.
[00:40:45.400 --> 00:40:47.400]   Because the more noise we add,
[00:40:47.400 --> 00:40:50.360]   the more the unit has freedom to alter the image.
[00:40:50.360 --> 00:40:51.720]   But the less noise we add,
[00:40:51.720 --> 00:40:54.520]   the less freedom the model has to alter the image
[00:40:54.520 --> 00:40:57.320]   because it cannot change radically.
[00:40:57.320 --> 00:41:00.680]   If we start from pure noise,
[00:41:00.680 --> 00:41:03.240]   the unit can do anything it wants.
[00:41:03.240 --> 00:41:05.000]   But if we start with less noise,
[00:41:05.000 --> 00:41:09.720]   the unit is forced to modify just a little bit the output image.
[00:41:09.720 --> 00:41:12.920]   So the amount of noise that we start from
[00:41:12.920 --> 00:41:15.880]   indicates how much we want the model to pay attention
[00:41:15.880 --> 00:41:17.560]   to the initial image here.
[00:41:17.560 --> 00:41:20.520]   And then we give the prompt.
[00:41:20.520 --> 00:41:24.040]   For many steps, we keep denoising, denoising, denoising, denoising.
[00:41:24.040 --> 00:41:26.600]   And after there is no more noise,
[00:41:26.600 --> 00:41:28.760]   we take this latent representation,
[00:41:28.760 --> 00:41:30.520]   we pass it through the decoder
[00:41:30.520 --> 00:41:32.760]   and we get the output image here.
[00:41:33.560 --> 00:41:35.560]   And this is how image-to-image works.
[00:41:35.560 --> 00:41:38.120]   Now let's go to the last part,
[00:41:38.120 --> 00:41:40.280]   which is how in-painting works.
[00:41:40.280 --> 00:41:44.600]   In-painting works similar way to the image-to-image,
[00:41:44.600 --> 00:41:45.640]   but with a mask.
[00:41:45.640 --> 00:41:47.960]   So in-painting means, first of all,
[00:41:47.960 --> 00:41:49.720]   that we have an image
[00:41:49.720 --> 00:41:51.960]   and we want to cut some part of this image,
[00:41:51.960 --> 00:41:53.880]   for example, the legs of this dog
[00:41:53.880 --> 00:41:57.240]   and we want the model to generate new legs for this dog
[00:41:57.240 --> 00:41:58.760]   that are maybe a little different.
[00:41:58.760 --> 00:41:59.400]   So as you can see,
[00:42:00.360 --> 00:42:03.000]   these feet here are a little different
[00:42:03.000 --> 00:42:05.480]   from the legs of the dog here.
[00:42:05.480 --> 00:42:08.120]   So what we do is,
[00:42:08.120 --> 00:42:10.440]   we start from our initial image of the dog.
[00:42:10.440 --> 00:42:12.680]   We pass it through the encoder.
[00:42:12.680 --> 00:42:14.520]   It becomes a latent representation.
[00:42:14.520 --> 00:42:17.480]   We add some noise to this latent representation.
[00:42:17.480 --> 00:42:20.280]   We give some prompt to tell the model
[00:42:20.280 --> 00:42:22.440]   what we want the model to generate.
[00:42:22.440 --> 00:42:23.800]   So I just say a dog running
[00:42:23.800 --> 00:42:26.040]   because I want to generate new legs for this dog.
[00:42:26.920 --> 00:42:32.600]   And then we pass the noisified input to the unit.
[00:42:32.600 --> 00:42:34.680]   The unit will produce an output here
[00:42:34.680 --> 00:42:35.880]   for the first time step.
[00:42:35.880 --> 00:42:39.240]   But then, of course, nobody told the model
[00:42:39.240 --> 00:42:41.640]   to only predict this area.
[00:42:41.640 --> 00:42:42.920]   The model, of course,
[00:42:42.920 --> 00:42:44.680]   here at the output predicted
[00:42:44.680 --> 00:42:47.800]   and modified the noise all the image.
[00:42:47.800 --> 00:42:50.920]   But we take this output here
[00:42:50.920 --> 00:42:56.680]   and we don't care what the noise predicted
[00:42:56.680 --> 00:42:58.200]   for this area of the image.
[00:42:58.200 --> 00:43:00.120]   The area that we already know.
[00:43:00.120 --> 00:43:02.680]   We replace it with the image
[00:43:02.680 --> 00:43:03.640]   that we already know.
[00:43:03.640 --> 00:43:07.560]   And we pass it again through the unit.
[00:43:07.560 --> 00:43:08.920]   Basically, what we do is,
[00:43:08.920 --> 00:43:10.040]   at every step,
[00:43:10.040 --> 00:43:11.720]   at every output of the unit,
[00:43:11.720 --> 00:43:14.600]   we replace the areas that are already known
[00:43:14.600 --> 00:43:18.120]   with the areas of the original image.
[00:43:18.120 --> 00:43:20.760]   So, basically, to fool the model
[00:43:20.760 --> 00:43:23.080]   into believing that it was the model itself
[00:43:23.080 --> 00:43:26.120]   that came up with these details of the image,
[00:43:26.120 --> 00:43:27.160]   not us.
[00:43:27.160 --> 00:43:30.760]   So every time here in this area,
[00:43:30.760 --> 00:43:33.160]   before we send it back to the unit here,
[00:43:33.160 --> 00:43:36.600]   here we combine the output of the unit
[00:43:36.600 --> 00:43:38.440]   with the existing image
[00:43:38.440 --> 00:43:40.600]   by replacing whatever output
[00:43:40.600 --> 00:43:43.880]   the unit gave us for this area here
[00:43:43.880 --> 00:43:45.800]   with what is the original image.
[00:43:45.800 --> 00:43:47.880]   And then we give it back to the unit
[00:43:47.880 --> 00:43:49.400]   and we keep doing it.
[00:43:49.400 --> 00:43:51.720]   This way the model will only be able
[00:43:51.720 --> 00:43:53.560]   to work on this area here
[00:43:53.560 --> 00:43:55.560]   because this is the one we never replace
[00:43:55.560 --> 00:43:57.080]   in the output of the unit.
[00:43:57.080 --> 00:43:58.760]   And then after there is no more noise,
[00:43:58.760 --> 00:44:00.120]   we take the output,
[00:44:00.120 --> 00:44:02.440]   we send it to the decoder
[00:44:02.440 --> 00:44:05.800]   and then it will build the image
[00:44:05.800 --> 00:44:06.920]   we can see here.
[00:44:06.920 --> 00:44:11.320]   Okay, this is how the stable diffusion works
[00:44:11.320 --> 00:44:13.080]   from an architecture point of view.
[00:44:13.080 --> 00:44:14.600]   I know it has been a long journey.
[00:44:14.600 --> 00:44:16.280]   I had to introduce many concepts
[00:44:16.920 --> 00:44:18.120]   but it's very important
[00:44:18.120 --> 00:44:19.640]   that we know these concepts
[00:44:19.640 --> 00:44:21.560]   before we start building the unit
[00:44:21.560 --> 00:44:26.200]   because otherwise we don't even know
[00:44:26.200 --> 00:44:28.600]   how to start building the stable diffusion.
[00:44:28.600 --> 00:44:32.520]   Here we are finally coding our stable diffusion.
[00:44:32.520 --> 00:44:35.000]   And the first thing that we will code
[00:44:35.000 --> 00:44:36.680]   is the variational autoencoder
[00:44:36.680 --> 00:44:38.520]   because it's external to the unit,
[00:44:38.520 --> 00:44:40.680]   so it's external to the diffusion model,
[00:44:40.680 --> 00:44:42.360]   so the one that will detect,
[00:44:42.360 --> 00:44:43.880]   will predict how much noise
[00:44:43.880 --> 00:44:45.000]   is present in the image.
[00:44:45.640 --> 00:44:47.320]   And let's review it actually.
[00:44:47.320 --> 00:44:48.600]   Let's review the architecture
[00:44:48.600 --> 00:44:52.520]   and let me go to this slide here.
[00:44:52.520 --> 00:44:55.080]   Okay, oops.
[00:44:55.080 --> 00:44:59.000]   This one here.
[00:44:59.000 --> 00:45:01.240]   Okay, the first thing that we will build
[00:45:01.240 --> 00:45:02.360]   is this part here.
[00:45:02.360 --> 00:45:07.240]   The encoder and the decoder
[00:45:07.240 --> 00:45:09.240]   of our variational autoencoder.
[00:45:09.240 --> 00:45:10.760]   The job of the encoder
[00:45:10.760 --> 00:45:12.440]   and the decoder of the variational autoencoder
[00:45:12.440 --> 00:45:15.960]   is to encode an image or noise
[00:45:15.960 --> 00:45:18.520]   into a compressed version of the image
[00:45:18.520 --> 00:45:19.640]   or the noise itself
[00:45:19.640 --> 00:45:23.560]   such that then we can take this latent
[00:45:23.560 --> 00:45:25.480]   and run it through the unit.
[00:45:25.480 --> 00:45:28.120]   And then after the last step
[00:45:28.120 --> 00:45:29.320]   of the noisification
[00:45:29.320 --> 00:45:32.360]   we take this compressed version or latent
[00:45:32.360 --> 00:45:34.440]   and we pass it through the decoder
[00:45:34.440 --> 00:45:35.720]   and to get the original,
[00:45:35.720 --> 00:45:37.480]   the output image, not the original.
[00:45:37.480 --> 00:45:41.080]   And so the encoder,
[00:45:41.080 --> 00:45:43.640]   actually his job is to reduce
[00:45:43.640 --> 00:45:45.320]   the dimension of the data
[00:45:45.320 --> 00:45:46.760]   into a smaller data,
[00:45:46.760 --> 00:45:49.480]   into the data with the smaller dimension.
[00:45:49.480 --> 00:45:51.240]   And the idea is very similar
[00:45:51.240 --> 00:45:52.280]   to the one of the unit.
[00:45:52.280 --> 00:45:54.760]   So we start with a picture that is very big
[00:45:54.760 --> 00:45:57.720]   and at each step there are multiple levels.
[00:45:57.720 --> 00:46:01.080]   We keep reducing the size of the image
[00:46:01.080 --> 00:46:02.440]   but at the same time
[00:46:02.440 --> 00:46:05.400]   we keep increasing the features of the image.
[00:46:05.400 --> 00:46:06.440]   What does it mean?
[00:46:06.440 --> 00:46:08.920]   That initially each pixel of the image
[00:46:08.920 --> 00:46:11.560]   will be represented by three channels.
[00:46:11.560 --> 00:46:13.800]   So red, green and blue RGB.
[00:46:13.800 --> 00:46:17.720]   At each step by using convolutions
[00:46:17.720 --> 00:46:19.880]   we will reduce the size of the image
[00:46:19.880 --> 00:46:20.840]   but at the same time
[00:46:20.840 --> 00:46:23.720]   we will increase the number of features
[00:46:23.720 --> 00:46:26.280]   that each pixel represents.
[00:46:26.280 --> 00:46:27.800]   So each pixel will be represented
[00:46:27.800 --> 00:46:28.760]   not by three channels
[00:46:28.760 --> 00:46:30.840]   but maybe by more channels.
[00:46:30.840 --> 00:46:32.840]   This means that each pixel
[00:46:32.840 --> 00:46:34.920]   will actually capture more data.
[00:46:34.920 --> 00:46:37.640]   More data of the area
[00:46:37.640 --> 00:46:39.560]   to which that pixel belongs.
[00:46:39.560 --> 00:46:42.040]   And this is thanks to the convolutions.
[00:46:42.040 --> 00:46:43.720]   But I will show you later
[00:46:43.720 --> 00:46:45.400]   with an animation.
[00:46:45.400 --> 00:46:47.720]   So let's start building.
[00:46:47.720 --> 00:46:51.320]   The first thing we do is open Visual Studio.
[00:46:51.320 --> 00:46:54.600]   And we create three folders.
[00:46:54.600 --> 00:46:56.440]   The first is called data.
[00:46:56.440 --> 00:46:59.160]   And later we download the pre-trained weights
[00:46:59.160 --> 00:47:01.000]   that you can also find on my GitHub.
[00:47:01.000 --> 00:47:03.560]   Another folder called images
[00:47:03.560 --> 00:47:06.920]   in which we put images as input and output.
[00:47:06.920 --> 00:47:08.520]   And then another folder called SD
[00:47:08.520 --> 00:47:09.560]   which is our module.
[00:47:09.560 --> 00:47:11.480]   Let's create two files.
[00:47:11.480 --> 00:47:13.320]   One called encoder.py
[00:47:13.320 --> 00:47:15.560]   and one called decoder.py.
[00:47:15.560 --> 00:47:18.520]   These are the encoder
[00:47:18.520 --> 00:47:22.600]   and the decoder of our variational autoencoder.
[00:47:22.600 --> 00:47:24.600]   Let's start with the encoder.
[00:47:24.600 --> 00:47:28.200]   And the encoder is quite simple.
[00:47:28.200 --> 00:47:30.040]   So let's start by importing Torch
[00:47:30.040 --> 00:47:34.600]   and all the other stuff.
[00:47:35.560 --> 00:47:38.360]   Let me also select the interpreter.
[00:47:38.360 --> 00:47:42.140]   Okay.
[00:47:42.140 --> 00:47:45.880]   Then we need to import two other blocks
[00:47:45.880 --> 00:47:47.800]   that we will define later in the decoder.
[00:47:47.800 --> 00:47:49.080]   Let's call them for now
[00:47:49.080 --> 00:47:54.600]   port VAE attention block.
[00:47:54.600 --> 00:47:58.280]   And this is the port VAE attention block.
[00:47:58.280 --> 00:48:00.680]   And this is the port VAE attention block.
[00:48:00.680 --> 00:48:02.680]   And this is the port VAE attention block.
[00:48:02.680 --> 00:48:04.280]   And this is the port VAE attention block.
[00:48:04.280 --> 00:48:06.840]   And VAE residual block.
[00:48:06.840 --> 00:48:09.880]   For those who are familiar with computer vision models,
[00:48:09.880 --> 00:48:11.320]   the residual block is very similar
[00:48:11.320 --> 00:48:13.800]   to the residual block that is used in the ResNet.
[00:48:13.800 --> 00:48:15.160]   So later you will see the structure.
[00:48:15.160 --> 00:48:15.880]   It's very similar.
[00:48:15.880 --> 00:48:17.400]   But if those who are not familiar,
[00:48:17.400 --> 00:48:19.320]   don't worry, I will explain it later.
[00:48:19.320 --> 00:48:21.000]   So let's start building this encoder.
[00:48:21.000 --> 00:48:27.720]   And this will inherit from the sequential module
[00:48:27.720 --> 00:48:29.480]   which means basically our encoder
[00:48:29.480 --> 00:48:31.800]   is a sequence of modules, submodules.
[00:48:32.040 --> 00:48:32.540]   Okay.
[00:48:32.540 --> 00:48:50.760]   It's a sequence of submodules
[00:48:50.760 --> 00:48:53.960]   in which each module is something
[00:48:53.960 --> 00:48:56.520]   that reduces the dimension of the data,
[00:48:56.520 --> 00:48:59.160]   but at the same time increases its number of features.
[00:49:00.760 --> 00:49:07.160]   I will write the blocks one by one.
[00:49:07.160 --> 00:49:09.720]   And as soon as we encounter a block
[00:49:09.720 --> 00:49:11.640]   that we didn't define, we go to define it.
[00:49:11.640 --> 00:49:13.480]   And then we define also the shapes.
[00:49:13.480 --> 00:49:16.200]   So the first thing we do, just like in the unit,
[00:49:16.200 --> 00:49:18.520]   is we define a convolution.
[00:49:18.520 --> 00:49:20.840]   Convolution 2D.
[00:49:20.840 --> 00:49:25.160]   Initially, our image will have three channels.
[00:49:25.160 --> 00:49:27.480]   And we convert it to 128 channels
[00:49:27.480 --> 00:49:32.120]   with a kernel size of 3 and a padding of 1.
[00:49:32.120 --> 00:49:35.640]   For those who are not familiar with convolutions,
[00:49:35.640 --> 00:49:38.360]   let's go have a look at how convolutions work.
[00:49:38.360 --> 00:49:55.260]   Here.
[00:49:55.500 --> 00:50:00.940]   Here we can see that a convolution,
[00:50:00.940 --> 00:50:02.300]   basically, it's a kernel.
[00:50:02.300 --> 00:50:06.940]   So it's made of a matrix of a size that we can decide,
[00:50:06.940 --> 00:50:09.420]   which is defined by the parameter kernel size,
[00:50:09.420 --> 00:50:12.300]   which is run through the image
[00:50:12.300 --> 00:50:15.180]   as in the following animation.
[00:50:15.180 --> 00:50:17.980]   So block by block, as you can see.
[00:50:17.980 --> 00:50:23.500]   And at each block, each of the pixel below the kernel
[00:50:23.500 --> 00:50:26.700]   is multiplied by the value of the kernel in that position.
[00:50:26.700 --> 00:50:28.780]   So in this, for example, this pixel here,
[00:50:28.780 --> 00:50:31.260]   which is in position, let's call the,
[00:50:31.260 --> 00:50:32.780]   let's say this one here.
[00:50:32.780 --> 00:50:34.860]   So the first row and the first column
[00:50:34.860 --> 00:50:39.740]   is multiplied by this red value of the kernel.
[00:50:39.740 --> 00:50:42.380]   The second column, first row,
[00:50:42.380 --> 00:50:45.420]   is multiplied by the green value of the kernel.
[00:50:45.420 --> 00:50:46.940]   And then all of these multiplications
[00:50:46.940 --> 00:50:49.260]   are summed up to produce one output.
[00:50:49.260 --> 00:50:51.980]   So this output here comes from four multiplications
[00:50:51.980 --> 00:50:53.500]   that we do in this area,
[00:50:53.500 --> 00:50:57.420]   each one with the corresponding number of the kernel.
[00:50:57.420 --> 00:51:00.540]   This way, basically, by running this kernel through the image,
[00:51:00.540 --> 00:51:03.980]   we capture local information about the image.
[00:51:03.980 --> 00:51:07.500]   And this pixel here combines somehow
[00:51:07.500 --> 00:51:10.380]   the information of four pixels, not only one.
[00:51:10.380 --> 00:51:12.540]   And that's it.
[00:51:12.540 --> 00:51:16.380]   Then we can also increase the kernel size, for example.
[00:51:16.380 --> 00:51:19.180]   And the kernel size, increasing the kernel
[00:51:19.180 --> 00:51:21.900]   means that we capture more global information.
[00:51:21.900 --> 00:51:24.140]   So each pixel represents the information
[00:51:24.140 --> 00:51:27.100]   of more pixel from the original picture.
[00:51:27.100 --> 00:51:28.700]   So the output is smaller.
[00:51:28.700 --> 00:51:32.620]   And then we can introduce, for example, the stride,
[00:51:32.620 --> 00:51:36.460]   which means that we don't do it every successive pixel,
[00:51:36.460 --> 00:51:38.700]   but we skip some pixels, as you can see here.
[00:51:38.700 --> 00:51:41.340]   So we skip every second pixel here.
[00:51:41.340 --> 00:51:46.780]   And if the number is, the kernel size is even
[00:51:46.780 --> 00:51:49.020]   and the input size is odd,
[00:51:49.020 --> 00:51:51.020]   we will also never touch, for example,
[00:51:51.020 --> 00:51:52.380]   here, the border, as you can see.
[00:51:52.380 --> 00:51:54.780]   We can also implement a dilation,
[00:51:54.780 --> 00:51:59.100]   which means that it becomes, with the same kernel size,
[00:51:59.100 --> 00:52:01.820]   the information becomes even more global
[00:52:01.820 --> 00:52:04.700]   because we don't watch consecutive pixel,
[00:52:04.700 --> 00:52:07.580]   but we skip some pixels, et cetera, et cetera.
[00:52:07.580 --> 00:52:09.580]   So the kernels, basically, the convolutions
[00:52:09.580 --> 00:52:12.060]   allow us to capture information
[00:52:12.060 --> 00:52:15.180]   from a local area of the picture, of the image,
[00:52:15.180 --> 00:52:18.300]   and combine it using a kernel.
[00:52:18.300 --> 00:52:20.140]   And this is the idea behind convolutions.
[00:52:20.140 --> 00:52:23.660]   So this convolution here, for example,
[00:52:23.660 --> 00:52:27.020]   will start with our, okay, let's define some shapes.
[00:52:27.020 --> 00:52:29.580]   Our variational autoencoder,
[00:52:29.580 --> 00:52:32.060]   so the encoder of the variational autoencoder
[00:52:32.060 --> 00:52:37.580]   will start with batch size and three channels.
[00:52:37.580 --> 00:52:39.660]   Let's define it as channel.
[00:52:39.660 --> 00:52:43.580]   Then this image will have a height and the width,
[00:52:43.580 --> 00:52:47.100]   which will be 512 by 512, as we will see later.
[00:52:47.900 --> 00:52:54.140]   And this convolution will convert it into batch size 128 features
[00:52:54.140 --> 00:52:56.780]   with the same height and the same width.
[00:52:56.780 --> 00:53:02.540]   Why, in this case, the height and the width doesn't change?
[00:53:02.540 --> 00:53:05.740]   Because even if we have a kernel size of size three,
[00:53:05.740 --> 00:53:07.500]   because we add padding, basically,
[00:53:07.500 --> 00:53:09.820]   we add something to the right side,
[00:53:09.820 --> 00:53:11.180]   something to the top side,
[00:53:11.180 --> 00:53:12.940]   something to the bottom and the left of the image.
[00:53:12.940 --> 00:53:15.580]   So the image with the padding becomes bigger,
[00:53:15.580 --> 00:53:18.940]   but then the output of the convolution makes it smaller
[00:53:18.940 --> 00:53:21.420]   and matches the original size of the image.
[00:53:21.420 --> 00:53:24.780]   This is the reason we have the padding here.
[00:53:24.780 --> 00:53:28.220]   But we will see later that with the next blocks,
[00:53:28.220 --> 00:53:30.940]   the image size will start becoming smaller.
[00:53:30.940 --> 00:53:35.420]   The next block is called the residual block.
[00:53:35.420 --> 00:53:38.220]   And VAE residual block,
[00:53:38.220 --> 00:53:43.820]   which is from 128 channels to 128 channels.
[00:53:44.380 --> 00:53:45.580]   This is a combination,
[00:53:45.580 --> 00:53:52.540]   this residual block is a combination of convolutions and normalization.
[00:53:52.540 --> 00:53:55.980]   So it's just a bunch of convolutions that we will define later.
[00:53:55.980 --> 00:54:00.300]   And this one indicates how many input channels we have
[00:54:00.300 --> 00:54:02.060]   and how many output channels we have.
[00:54:02.060 --> 00:54:05.100]   And the residual block will not change the size of the image.
[00:54:05.100 --> 00:54:07.580]   So we define it.
[00:54:07.580 --> 00:54:11.820]   So our input image is 128.
[00:54:11.820 --> 00:54:17.900]   So batch size 128 height and width.
[00:54:17.900 --> 00:54:22.140]   And it becomes, it remains the same basically.
[00:54:22.140 --> 00:54:22.640]   Oops!
[00:54:22.640 --> 00:54:27.580]   Okay, we have another one.
[00:54:27.580 --> 00:54:33.100]   Another residual block with the same transformation.
[00:54:33.100 --> 00:54:35.820]   Then we have another convolution.
[00:54:35.820 --> 00:54:38.780]   And this time the convolution will change the size of the image.
[00:54:38.780 --> 00:54:39.660]   And we will see why.
[00:54:39.660 --> 00:54:41.660]   So we have a convolution.
[00:54:41.740 --> 00:54:47.100]   To the 128 to 128.
[00:54:47.100 --> 00:54:51.660]   Because the output channels of the last block is 128.
[00:54:51.660 --> 00:54:53.340]   So the input channel is 128.
[00:54:53.340 --> 00:54:54.700]   The output is 128.
[00:54:54.700 --> 00:54:57.820]   The kernel size is 3.
[00:54:57.820 --> 00:55:00.140]   The stride is 2.
[00:55:00.140 --> 00:55:02.860]   And the padding is 0.
[00:55:02.860 --> 00:55:06.700]   This will basically introduce kernel size 3, stride 2.
[00:55:06.700 --> 00:55:07.580]   Let's watch.
[00:55:07.580 --> 00:55:10.460]   So imagine the batch size is 6 by 6.
[00:55:10.460 --> 00:55:13.020]   Kernel size is 3.
[00:55:13.020 --> 00:55:15.020]   Stride is 2 without the deletion.
[00:55:15.020 --> 00:55:16.940]   And this is the output.
[00:55:16.940 --> 00:55:20.140]   Let me make it bigger.
[00:55:20.140 --> 00:55:25.020]   Okay, something.
[00:55:25.020 --> 00:55:27.040]   Yeah.
[00:55:27.040 --> 00:55:30.220]   So as you can see, with the stride of 2...
[00:55:30.220 --> 00:55:33.340]   Need to make it...
[00:55:33.340 --> 00:55:37.660]   Okay, with the stride of 2 and the kernel size of 3.
[00:55:37.660 --> 00:55:38.620]   This is the behavior.
[00:55:38.620 --> 00:55:43.660]   So we skip every 2 pixels before calculating the output.
[00:55:43.660 --> 00:55:46.700]   And this makes the output smaller than the input.
[00:55:46.700 --> 00:55:48.300]   Because of this stride.
[00:55:48.300 --> 00:55:49.900]   And also because of the kernel size.
[00:55:49.900 --> 00:55:51.260]   And we don't have any padding.
[00:55:51.260 --> 00:55:57.820]   So this transformation here will have the following shapes.
[00:55:57.820 --> 00:55:59.900]   So we are starting from batch size.
[00:55:59.900 --> 00:56:03.260]   128.
[00:56:03.260 --> 00:56:05.420]   Height, width.
[00:56:05.420 --> 00:56:07.900]   So the original height and the width of the input image.
[00:56:08.540 --> 00:56:11.980]   But this time it will become batch size.
[00:56:11.980 --> 00:56:16.220]   128.
[00:56:16.220 --> 00:56:18.540]   The height will become half.
[00:56:18.540 --> 00:56:20.780]   And the width will become half.
[00:56:20.780 --> 00:56:25.580]   Etc.
[00:56:25.580 --> 00:56:28.460]   Then we have two more residual blocks.
[00:56:28.460 --> 00:56:30.860]   With the same...
[00:56:30.860 --> 00:56:34.540]   Same as before.
[00:56:34.540 --> 00:56:36.860]   But this time by increasing the number of features.
[00:56:36.940 --> 00:56:43.420]   And also here we don't increase any.
[00:56:43.420 --> 00:56:48.940]   Here by increasing the feature means that we don't increase the size of the image.
[00:56:48.940 --> 00:56:50.380]   Or we reduce the size of the image.
[00:56:50.380 --> 00:56:52.620]   We just increase the number of features.
[00:56:52.620 --> 00:56:55.500]   So this one becomes 256.
[00:56:55.500 --> 00:56:58.940]   And here we start from...
[00:57:03.980 --> 00:57:07.260]   Oops 256 and we remain 256.
[00:57:07.260 --> 00:57:10.140]   Now you may be confused of why we are doing all of this.
[00:57:10.140 --> 00:57:12.460]   Okay the idea is we start with the initial image.
[00:57:12.460 --> 00:57:15.180]   And we keep decreasing the size of the image.
[00:57:15.180 --> 00:57:19.180]   So later you will see that the image will become divided by 4, divided by 8.
[00:57:19.180 --> 00:57:22.220]   But at the same time we keep increasing the features.
[00:57:22.220 --> 00:57:25.980]   So each pixel represents more information.
[00:57:25.980 --> 00:57:28.780]   But the number of pixels is diminishing.
[00:57:28.780 --> 00:57:30.860]   Is reducing at every step.
[00:57:33.180 --> 00:57:34.700]   So let's go forward.
[00:57:34.700 --> 00:57:36.780]   Then we have another convolution.
[00:57:36.780 --> 00:57:41.500]   And this time the size will become divided by 4.
[00:57:41.500 --> 00:57:43.900]   And the convolution is...
[00:57:43.900 --> 00:57:44.860]   Let me copy this one.
[00:57:44.860 --> 00:57:52.300]   256 by 256.
[00:57:52.300 --> 00:57:54.940]   Because the previous output is 256.
[00:57:54.940 --> 00:57:56.460]   The kernel size is 3.
[00:57:56.460 --> 00:57:58.700]   The stride is 2 and the padding is 0.
[00:57:58.700 --> 00:57:59.740]   So just like before.
[00:57:59.740 --> 00:58:04.060]   Also in this case the size of the image will become half of what is it now.
[00:58:04.060 --> 00:58:05.820]   So the image is already divided by 2.
[00:58:05.820 --> 00:58:07.580]   So it will become divided by 4 now.
[00:58:07.580 --> 00:58:18.140]   Then we have another residual block.
[00:58:18.140 --> 00:58:21.420]   In which we increase the number of features.
[00:58:21.420 --> 00:58:26.860]   This time from 256 to 512.
[00:58:26.860 --> 00:58:31.340]   So we start from 256 and the image is divided by 4.
[00:58:31.340 --> 00:58:34.780]   And we go to 512.
[00:58:34.780 --> 00:58:36.620]   And the image size doesn't change.
[00:58:36.620 --> 00:58:41.180]   Then we have another one.
[00:58:41.180 --> 00:58:45.420]   From 512 to 512.
[00:58:45.420 --> 00:58:48.060]   In this case...
[00:58:48.060 --> 00:58:49.200]   Oops.
[00:58:49.200 --> 00:58:52.780]   We will see later what is the residual block.
[00:58:52.780 --> 00:58:56.940]   But the residual block you have to think of it as just a convolution with a normalization.
[00:58:56.940 --> 00:58:57.820]   We will see later.
[00:58:57.820 --> 00:59:01.100]   And this one is 512.
[00:59:01.100 --> 00:59:02.860]   And that goes into 512.
[00:59:02.860 --> 00:59:10.140]   And then we have another convolution that will make it even smaller.
[00:59:10.140 --> 00:59:12.060]   So let's copy this convolution here.
[00:59:12.060 --> 00:59:18.460]   This one will go from 512 to 512.
[00:59:19.020 --> 00:59:23.180]   The same kernel size and the same stride and the same padding as before.
[00:59:23.180 --> 00:59:25.340]   So the image will become even smaller.
[00:59:25.340 --> 00:59:28.540]   So our last dimension was this.
[00:59:28.540 --> 00:59:29.500]   Let me copy it.
[00:59:29.500 --> 00:59:32.460]   So we start with an image that is 512.
[00:59:32.460 --> 00:59:36.300]   4 times smaller than the original image.
[00:59:36.300 --> 00:59:41.420]   And with the 4 times smaller width it will become 8 times smaller.
[00:59:41.420 --> 00:59:44.860]   And that's it.
[00:59:46.220 --> 00:59:49.740]   And then we have residual blocks also here.
[00:59:49.740 --> 00:59:51.180]   We have three of them in this case.
[00:59:51.180 --> 00:59:54.300]   Let me copy.
[00:59:54.300 --> 00:59:59.660]   One, two, three.
[00:59:59.660 --> 01:00:01.900]   I just write the one for the last one.
[01:00:01.900 --> 01:00:06.860]   So anyway the size, the shape changes here.
[01:00:06.860 --> 01:00:10.380]   It doesn't change the shape of the image or the number of features.
[01:00:11.020 --> 01:00:16.460]   So here we are going from divide by 8 and 512 here.
[01:00:16.460 --> 01:00:18.460]   512.
[01:00:18.460 --> 01:00:24.060]   And we go to same dimension.
[01:00:24.060 --> 01:00:24.860]   512.
[01:00:24.860 --> 01:00:29.020]   Divide by 8 and divide by 8.
[01:00:29.020 --> 01:00:31.340]   Then we have an attention block.
[01:00:31.340 --> 01:00:34.220]   And later we will see what is the attention block.
[01:00:34.220 --> 01:00:38.700]   Basically it will run a self-attention over each pixel.
[01:00:38.700 --> 01:00:40.780]   So each pixel will become kind of,
[01:00:40.780 --> 01:00:45.020]   as you remember, the attention is a way to relate tokens to each other in a sentence.
[01:00:45.020 --> 01:00:48.620]   So if we have an image made of pixels,
[01:00:48.620 --> 01:00:52.300]   the attention can be thought of as a sequence of pixels
[01:00:52.300 --> 01:00:55.180]   and the attention as a way to relate the pixel to each other.
[01:00:55.180 --> 01:00:57.820]   So this is the goal of the attention block.
[01:00:57.820 --> 01:01:04.620]   And because this way each pixel is related to each other,
[01:01:04.620 --> 01:01:06.620]   is not independent from each other.
[01:01:06.620 --> 01:01:12.380]   Even if the convolution already actually relates close pixels to each other,
[01:01:12.380 --> 01:01:14.220]   but the attention will be global.
[01:01:14.220 --> 01:01:17.180]   So even the last pixel can be related to the first pixel.
[01:01:17.180 --> 01:01:19.020]   This is the goal of the attention block.
[01:01:19.020 --> 01:01:23.020]   And also in this case we don't reduce the size
[01:01:23.020 --> 01:01:26.380]   because the attention is, the transformer's attention,
[01:01:26.380 --> 01:01:27.660]   is a sequence-to-sequence model.
[01:01:27.660 --> 01:01:30.780]   So we don't reduce the size of the sequence.
[01:01:30.780 --> 01:01:33.420]   And the image remains the same.
[01:01:35.100 --> 01:01:36.860]   Finally, we have another residual block.
[01:01:36.860 --> 01:01:40.800]   Let's...
[01:01:40.800 --> 01:01:44.300]   Let me copy here.
[01:01:44.300 --> 01:01:47.740]   Also no change in shape or size of the image.
[01:01:47.740 --> 01:01:49.420]   Then we have a normalization.
[01:01:49.420 --> 01:01:51.660]   And we will see what is this normalization.
[01:01:51.660 --> 01:01:53.420]   It's the group normalization,
[01:01:53.420 --> 01:01:55.180]   which also doesn't change the size.
[01:01:55.180 --> 01:01:58.060]   Just like any normalization, by the way.
[01:01:58.060 --> 01:02:03.980]   With the number of groups being 32
[01:02:03.980 --> 01:02:06.460]   and the number of channels being 512,
[01:02:06.460 --> 01:02:07.820]   because it's the number of features.
[01:02:07.820 --> 01:02:11.500]   Finally, we have an activation function called the CELU.
[01:02:11.500 --> 01:02:13.820]   The CELU is a function...
[01:02:13.820 --> 01:02:17.340]   Okay, it's derived from the sigmoid linear unit.
[01:02:17.340 --> 01:02:20.220]   And it's a function just like the RELU.
[01:02:20.220 --> 01:02:21.420]   There is nothing special.
[01:02:21.420 --> 01:02:25.580]   They just saw that this one works better for this kind of application.
[01:02:25.580 --> 01:02:31.820]   But there is no particular reason to choose one over another,
[01:02:31.820 --> 01:02:36.460]   except that they thought that practically this one works fine for this kind of models.
[01:02:36.460 --> 01:02:40.860]   And if you watch my previous video about LAMA, for example,
[01:02:40.860 --> 01:02:43.740]   in which we analyzed why they chose the ZWIGLU function.
[01:02:43.740 --> 01:02:45.580]   If you read the paper, at the end of the paper,
[01:02:45.580 --> 01:02:48.700]   they say that there is no particular reason they chose the ZWIGLU.
[01:02:48.700 --> 01:02:50.620]   They just saw that practically it works better.
[01:02:50.620 --> 01:02:52.460]   I mean, it's very difficult to describe why
[01:02:52.460 --> 01:02:56.060]   activation function works better than the others.
[01:02:56.060 --> 01:02:57.660]   So this is why they use the CELU here,
[01:02:57.660 --> 01:02:59.100]   because practically it works well.
[01:03:01.100 --> 01:03:03.340]   Now, we have another two convolutions.
[01:03:03.340 --> 01:03:05.500]   And then we are done with the encoder.
[01:03:05.500 --> 01:03:16.460]   Convolution, 512, 8, kernel size, and then padding.
[01:03:16.460 --> 01:03:22.860]   This will not change the size of the model.
[01:03:22.860 --> 01:03:25.580]   Because just like before, we have the kernel size as 3.
[01:03:25.580 --> 01:03:30.220]   But we have the padding that compensates for the reduction given by the kernel size.
[01:03:30.220 --> 01:03:32.300]   But we are decreasing the number of features.
[01:03:32.300 --> 01:03:36.060]   And this is the bottleneck of the encoder.
[01:03:36.060 --> 01:03:38.780]   And I will show you later on the architecture what is the bottleneck.
[01:03:38.780 --> 01:03:47.260]   And finally, we have another convolution.
[01:03:47.260 --> 01:03:55.980]   Which is 8 by 8 with kernel size equal to 1.
[01:03:55.980 --> 01:03:57.980]   And the padding is equal to 0.
[01:03:57.980 --> 01:04:02.540]   Which also doesn't change the size of the image.
[01:04:02.540 --> 01:04:05.740]   Because if you watch here, if you have a kernel size of 1,
[01:04:05.740 --> 01:04:08.700]   it means that each, without stride,
[01:04:08.700 --> 01:04:13.020]   each kernel basically is running over each pixel.
[01:04:13.020 --> 01:04:16.620]   So each output actually captures the information of only one pixel.
[01:04:16.620 --> 01:04:18.860]   So the output has the same dimension as the input.
[01:04:18.860 --> 01:04:23.260]   And this is why here also we don't change the...
[01:04:24.460 --> 01:04:27.500]   But here we need to change the number of...
[01:04:27.500 --> 01:04:32.540]   It becomes 8.
[01:04:32.540 --> 01:04:37.820]   And here from 8 to 8.
[01:04:37.820 --> 01:04:44.540]   And this is the list of modules that will make up our encoder.
[01:04:44.540 --> 01:04:49.660]   Before building the residual block and the attention block,
[01:04:49.660 --> 01:04:50.860]   so this attention block,
[01:04:50.860 --> 01:04:55.020]   let's write the forward method and then we build the residual block.
[01:04:55.020 --> 01:04:57.500]   So this is the init.
[01:04:57.500 --> 01:05:00.060]   Define it like this.
[01:05:00.060 --> 01:05:04.380]   Let me review it if it's correct.
[01:05:04.380 --> 01:05:05.180]   Okay, yeah.
[01:05:05.180 --> 01:05:12.700]   Now let's define the forward method.
[01:05:12.700 --> 01:05:19.660]   x is the image for which we want to encode.
[01:05:19.660 --> 01:05:20.620]   So it's a tensor.
[01:05:21.420 --> 01:05:23.020]   Torch.tensor.
[01:05:23.020 --> 01:05:25.740]   And the noise, we need some noise.
[01:05:25.740 --> 01:05:27.740]   And later I will show you why we need some noise.
[01:05:27.740 --> 01:05:31.260]   That has the same size as the output of the encoder.
[01:05:31.260 --> 01:05:35.500]   This returns a tensor.
[01:05:35.500 --> 01:05:45.820]   Okay, our input x will be of size patch size with some channels.
[01:05:45.820 --> 01:05:47.900]   Initially it will be 3 because it's an image.
[01:05:48.860 --> 01:05:52.540]   Height and width which will be 512 by 512.
[01:05:52.540 --> 01:05:55.100]   And then some noise.
[01:05:55.100 --> 01:05:59.660]   This noise has the same size as the output of the encoder.
[01:05:59.660 --> 01:06:05.820]   And we will see that it's jelly patch size.
[01:06:05.820 --> 01:06:08.540]   Then output channels.
[01:06:08.540 --> 01:06:15.500]   Height divided by 8 and width divided by 8.
[01:06:16.940 --> 01:06:21.340]   Then we just run sequentially all of these modules.
[01:06:21.340 --> 01:06:35.100]   And then there is one little thing here that in the convolutions that have the stride,
[01:06:35.100 --> 01:06:38.460]   we need to apply a special embedding.
[01:06:38.460 --> 01:06:41.100]   And I will show you why and how it works.
[01:06:46.460 --> 01:06:52.460]   So if the module has a stride attribute and it's equal to 2 2,
[01:06:52.460 --> 01:06:57.180]   which basically means this convolution here,
[01:06:57.180 --> 01:06:59.980]   this convolution here and this convolution here,
[01:06:59.980 --> 01:07:05.180]   we don't apply the padding here because the padding here is applied to the top of the image,
[01:07:05.180 --> 01:07:07.180]   bottom, left and right.
[01:07:07.180 --> 01:07:10.460]   But we want to do an asymmetrical padding so we do it manually.
[01:07:10.460 --> 01:07:13.420]   And this is applied like this.
[01:07:13.420 --> 01:07:14.380]   F.padding.
[01:07:14.380 --> 01:07:27.740]   Basically this says can you add a layer of pixels on the right side of the image
[01:07:27.740 --> 01:07:30.380]   and on the bottom side of the image only?
[01:07:30.380 --> 01:07:38.220]   Because when you apply the padding, it's padding left, padding right, padding top,
[01:07:40.940 --> 01:07:42.220]   padding bottom.
[01:07:42.220 --> 01:07:47.340]   This means add a layer of pixels in the right side of the image
[01:07:47.340 --> 01:07:48.860]   and on the top side of the image.
[01:07:48.860 --> 01:07:54.380]   And this is asymmetrical padding.
[01:07:54.380 --> 01:08:00.140]   And then if we apply it only for these convolutions that have the stride equal to 2.
[01:08:00.140 --> 01:08:05.660]   And then x is equal to module of x.
[01:08:05.660 --> 01:08:10.060]   OK, now you may be wondering why are we building this kind of structure?
[01:08:10.060 --> 01:08:11.500]   Why it's made like this?
[01:08:11.500 --> 01:08:16.620]   OK, usually in deep learning communities, especially during research,
[01:08:16.620 --> 01:08:18.460]   we don't reinvent the wheel every time.
[01:08:18.460 --> 01:08:23.180]   So the people who made the stable diffusion, but also the people before them,
[01:08:23.180 --> 01:08:25.100]   every time we want to use a model,
[01:08:25.100 --> 01:08:29.260]   we check what models similar to the one we want to build
[01:08:29.260 --> 01:08:32.140]   are already out there and they are working fine.
[01:08:32.140 --> 01:08:35.900]   So very probably the people who built stable diffusion,
[01:08:35.900 --> 01:08:39.100]   they saw that a model like this is working very well
[01:08:39.100 --> 01:08:42.700]   for some previous project as a variational autoencoder.
[01:08:42.700 --> 01:08:46.780]   They just modified it a little bit and kept it like it.
[01:08:46.780 --> 01:08:49.740]   So for most choices, actually, there is no reason.
[01:08:49.740 --> 01:08:53.340]   There is a historical reason, because it worked well in practice.
[01:08:53.340 --> 01:08:57.420]   And we know that convolutions work well in practice
[01:08:57.420 --> 01:09:01.340]   for image segmentation, for example, or anything related to computer vision.
[01:09:01.340 --> 01:09:04.940]   And this is why they made the model like this.
[01:09:04.940 --> 01:09:09.500]   So most encoders actually work like this, that we reduce the size of the image,
[01:09:09.500 --> 01:09:12.860]   but each we keep increasing the features of the image,
[01:09:12.860 --> 01:09:14.940]   the channels, the number of channels of the image.
[01:09:14.940 --> 01:09:18.620]   So the number of pixels becomes smaller,
[01:09:18.620 --> 01:09:22.460]   but each pixel is represented by more than three channels.
[01:09:22.460 --> 01:09:23.980]   So more channels at every step.
[01:09:23.980 --> 01:09:30.220]   Now, what we do is here we are running our image into sequentially,
[01:09:30.940 --> 01:09:37.900]   in one by one, through all of these modules here.
[01:09:37.900 --> 01:09:41.020]   So first through this convolution, then through this residual block,
[01:09:41.020 --> 01:09:44.860]   which is also some convolutions, then this residual block,
[01:09:44.860 --> 01:09:46.940]   then again convolution, convolution, convolution,
[01:09:46.940 --> 01:09:50.460]   until we run it through this attention block and et cetera.
[01:09:50.460 --> 01:09:55.260]   This will transform the image into something smaller,
[01:09:55.260 --> 01:09:57.500]   so a compressed version of the image.
[01:09:57.500 --> 01:10:00.540]   But as I showed you before, this is not an autoencoder.
[01:10:00.540 --> 01:10:02.940]   This is a variational autoencoder.
[01:10:02.940 --> 01:10:09.180]   So the variational autoencoder, let me show you again the picture here.
[01:10:09.180 --> 01:10:11.500]   We are not learning how to compress data.
[01:10:11.500 --> 01:10:13.260]   We are learning a latent space.
[01:10:13.260 --> 01:10:19.340]   And this latent space are the parameters of a multivariate Gaussian distribution.
[01:10:19.340 --> 01:10:25.260]   So actually, the variational autoencoder is trained to learn the mu and the sigma,
[01:10:25.260 --> 01:10:29.820]   so the mean and the variance of this distribution.
[01:10:30.460 --> 01:10:35.900]   And this is actually what we will get from the output of this variational autoencoder,
[01:10:35.900 --> 01:10:39.020]   not directly the compressed image.
[01:10:39.020 --> 01:10:42.140]   And if this is not clear, guys,
[01:10:42.140 --> 01:10:45.100]   I made a previous video about the variational autoencoder,
[01:10:45.100 --> 01:10:48.460]   in which I show you also why the history of why we do it like this,
[01:10:48.460 --> 01:10:51.820]   all the reparameterization trick, et cetera.
[01:10:51.820 --> 01:10:56.700]   But for now, just remember that this is not just a compressed version of the image,
[01:10:56.700 --> 01:10:58.300]   it's actually a distribution.
[01:10:58.300 --> 01:11:01.100]   And then we can sample from this distribution.
[01:11:01.100 --> 01:11:02.380]   And I will show you how.
[01:11:02.380 --> 01:11:08.220]   So the output of the variational autoencoder is actually the mean and the variance.
[01:11:08.220 --> 01:11:11.420]   And actually, it's actually not the variance, but the log variance.
[01:11:11.420 --> 01:11:22.380]   So the mean and the log variance is equal to torch.chunk(x2, dimension equal 1).
[01:11:22.380 --> 01:11:25.580]   We will see also what is the chunk function.
[01:11:25.580 --> 01:11:27.500]   So I will show you.
[01:11:27.500 --> 01:11:32.700]   So this basically converts batch size, 8 channels, height,
[01:11:32.700 --> 01:11:36.940]   height divided by 8, width divided by 8,
[01:11:36.940 --> 01:11:40.140]   which is the output of the last layer of this encoder.
[01:11:40.140 --> 01:11:41.100]   So this one.
[01:11:41.100 --> 01:11:44.380]   And we divide it into two tensors.
[01:11:44.380 --> 01:11:49.500]   So this chunk basically means divide it into two tensors along this dimension.
[01:11:49.500 --> 01:11:52.860]   So along this dimension, it will become two tensors of size,
[01:11:52.860 --> 01:11:55.260]   along this dimension of size 4.
[01:11:55.980 --> 01:12:12.540]   So two tensors of shape, batch size 4, then height divided by 8, and width divided by 8.
[01:12:12.540 --> 01:12:20.700]   And this basically, the output of this actually represents the mean and the variance.
[01:12:21.660 --> 01:12:27.340]   And what we do, we don't want the log variance, we want the variance actually.
[01:12:27.340 --> 01:12:32.540]   So to transform the log variance into variance, we do the exponentiation.
[01:12:32.540 --> 01:12:37.100]   So the first thing actually we also need to do is to clamp this variance,
[01:12:37.100 --> 01:12:39.100]   because otherwise it will become very small.
[01:12:39.100 --> 01:12:42.460]   So clamping means that if the variance is too small or too big,
[01:12:42.460 --> 01:12:46.380]   we want it to become within some ranges that are acceptable for us.
[01:12:47.820 --> 01:12:50.700]   So this clamping function, log variance,
[01:12:50.700 --> 01:12:55.900]   tells the PyTorch that if the value is too small or too big, make it within this range.
[01:12:55.900 --> 01:13:01.020]   And this doesn't change the shape of the tensors.
[01:13:01.020 --> 01:13:04.300]   So this still remains this tensor here.
[01:13:04.300 --> 01:13:09.660]   And then we transform the log variance into variance.
[01:13:09.660 --> 01:13:13.820]   So the variance is equal to the log variance dot exp,
[01:13:13.820 --> 01:13:16.140]   which means make the exponential of this.
[01:13:16.140 --> 01:13:18.620]   So you delete the log and it becomes the variance.
[01:13:18.620 --> 01:13:25.100]   And this also doesn't change the size of the shape of the tensor.
[01:13:25.100 --> 01:13:28.140]   And then to calculate the standard deviation from the variance,
[01:13:28.140 --> 01:13:31.180]   as you know, the standard deviation is the square root of the variance.
[01:13:31.180 --> 01:13:40.300]   So standard deviation is the variance dot sqrt.
[01:13:40.300 --> 01:13:44.540]   And also this doesn't change the size of the tensor.
[01:13:44.540 --> 01:13:51.500]   OK, now what we want, as I told you before, this is a latent space.
[01:13:51.500 --> 01:13:56.060]   It's a multivariate Gaussian, which has its own mean and its own variance.
[01:13:56.060 --> 01:13:59.500]   And we know the mean and the variance, this mean and this variance.
[01:13:59.500 --> 01:14:03.740]   How do we convert? How do we sample from it?
[01:14:03.740 --> 01:14:09.420]   Well, what we can sample from is, basically, we can sample from n_01.
[01:14:09.420 --> 01:14:12.620]   This is, if we have a sample from n_01,
[01:14:12.620 --> 01:14:19.100]   how do we convert it into a sample of a given mean and the given variance?
[01:14:19.100 --> 01:14:23.260]   This, as if you remember from probability and statistics,
[01:14:23.260 --> 01:14:25.340]   if you have a sample from n_01,
[01:14:25.340 --> 01:14:28.860]   you can convert it into any other sample of a Gaussian
[01:14:28.860 --> 01:14:32.140]   with a given mean and a variance through this transformation.
[01:14:32.140 --> 01:14:37.180]   So if z, let's call it this one, z is equal to n_01,
[01:14:37.180 --> 01:14:40.940]   we can transform into another n, let's call it x,
[01:14:40.940 --> 01:14:43.820]   through this transformation x is equal to z.
[01:14:43.820 --> 01:14:49.980]   Well, the mean of the new distribution plus
[01:14:49.980 --> 01:14:54.780]   the standard deviation of the new distribution multiplied by z.
[01:14:54.780 --> 01:14:58.860]   This is the transformation, this is the formula from probability and statistics.
[01:14:58.860 --> 01:15:01.420]   Basically means transform this distribution into this one,
[01:15:01.420 --> 01:15:03.020]   that has this mean and this variance,
[01:15:03.020 --> 01:15:05.980]   which basically means sample from this distribution.
[01:15:05.980 --> 01:15:09.100]   This is why we are given also the noise as input,
[01:15:09.100 --> 01:15:14.700]   because the noise we want it to come from with a particular seed of the noise generator.
[01:15:14.700 --> 01:15:19.180]   So we ask is as input and we sample from this distribution like this,
[01:15:19.180 --> 01:15:25.180]   x is equal to mean plus standard deviation multiplied by noise.
[01:15:25.180 --> 01:15:33.740]   Finally, there is also another step that we need to scale the output by a constant.
[01:15:34.460 --> 01:15:37.740]   This constant, I found it in the original repository.
[01:15:37.740 --> 01:15:41.180]   So I'm just writing it here without any explanation on why,
[01:15:41.180 --> 01:15:43.260]   because I actually, I also don't know.
[01:15:43.260 --> 01:15:46.460]   It's just a scaling constant that they use at the end.
[01:15:46.460 --> 01:15:49.020]   I don't know if it's there for historical reason,
[01:15:49.020 --> 01:15:51.260]   because they use some previous model that had this constant,
[01:15:51.260 --> 01:15:54.060]   or they introduced it for some particular reason.
[01:15:54.060 --> 01:15:57.100]   But it's a constant that I saw it in the original repository.
[01:15:57.100 --> 01:16:01.420]   And actually, if you check the original parameters of the stable diffusion model,
[01:16:01.420 --> 01:16:02.460]   there is also this constant.
[01:16:02.460 --> 01:16:05.020]   So I am also scaling the output by this constant.
[01:16:05.020 --> 01:16:08.140]   And then we return x.
[01:16:08.140 --> 01:16:10.620]   So now what we built so far,
[01:16:10.620 --> 01:16:14.940]   except that we didn't build the residual block and the attention block here,
[01:16:14.940 --> 01:16:20.140]   we built the encoder part of the variational autoencoder and also the sampling part.
[01:16:20.140 --> 01:16:24.140]   So we take the image, we run it through the encoder, it becomes very small.
[01:16:24.140 --> 01:16:26.940]   It will tell us the mean and the variance.
[01:16:26.940 --> 01:16:31.340]   And then we sample from that distribution given the mean and the variance.
[01:16:31.340 --> 01:16:36.700]   Now we need to build the decoder along with the residual block and the attention block.
[01:16:36.700 --> 01:16:38.780]   And what we will see is that in the decoder,
[01:16:38.780 --> 01:16:42.220]   we do the opposite of what we did in the encoder.
[01:16:42.220 --> 01:16:46.700]   So we will reduce the number of channels and at the same time,
[01:16:46.700 --> 01:16:48.620]   we will increase the size of the image.
[01:16:48.620 --> 01:16:52.620]   So let's go to the decoder.
[01:16:52.620 --> 01:16:55.820]   Let me review if everything is fine.
[01:16:55.820 --> 01:17:00.380]   Looks like it is.
[01:17:00.380 --> 01:17:01.820]   So let's go to the decoder.
[01:17:01.820 --> 01:17:08.220]   Again, import torch.
[01:17:08.220 --> 01:17:26.700]   We also need to define the attention.
[01:17:29.260 --> 01:17:30.860]   We need to define the self-attention.
[01:17:30.860 --> 01:17:31.820]   Later we define it.
[01:17:31.820 --> 01:17:39.900]   Let's define first the residual block, the one we defined before,
[01:17:39.900 --> 01:17:42.860]   so that you understand what is this residual block.
[01:17:42.860 --> 01:17:48.220]   And then we define the attention block that we defined before.
[01:17:48.220 --> 01:17:49.660]   And finally, we build the attention.
[01:17:50.860 --> 01:18:00.860]   So...
[01:18:00.860 --> 01:18:09.420]   Okay, this is made up of normalization and convolutions, like I said before.
[01:18:09.420 --> 01:18:14.860]   There is a two normalization, which is the group norm one.
[01:18:14.860 --> 01:18:24.860]   So...
[01:18:24.860 --> 01:18:40.860]   And then there is another group normalization.
[01:18:40.860 --> 01:18:58.860]   With remote channels to out channels.
[01:18:58.860 --> 01:19:05.900]   And then we have a skip connection.
[01:19:05.900 --> 01:19:09.660]   Skip connection basically means that you take the input,
[01:19:09.660 --> 01:19:13.580]   you skip some layers, and then you connect it there with the output of the last layer.
[01:19:13.580 --> 01:19:17.820]   And we also need this residual connection.
[01:19:17.820 --> 01:19:21.740]   If the two channels are different, we need to create another intermediate layer.
[01:19:21.740 --> 01:19:34.860]   Now I create it, later I explain it.
[01:19:34.860 --> 01:19:51.100]   Okay, let's create the forward method.
[01:19:51.100 --> 01:19:59.100]   Which is a torch.tensor.
[01:20:02.620 --> 01:20:05.260]   And returns a torch.tensor.
[01:20:05.260 --> 01:20:10.780]   Okay, the input of this residual layer, as you saw before,
[01:20:10.780 --> 01:20:15.020]   is something that has a batch with some channels,
[01:20:15.020 --> 01:20:18.380]   and then height and width, which can be different.
[01:20:18.380 --> 01:20:19.500]   It's not always the same.
[01:20:19.500 --> 01:20:23.420]   Sometimes it's 512 by 512, sometimes it's half of that,
[01:20:23.420 --> 01:20:25.180]   sometimes it's one fourth of that, etc.
[01:20:25.980 --> 01:20:36.220]   So suppose it's x is batch size in channels height width.
[01:20:36.220 --> 01:20:39.740]   What we do is we create the skip connection.
[01:20:39.740 --> 01:20:41.420]   So we save the initial input.
[01:20:41.420 --> 01:20:45.500]   We call it the residual or residue is equal to x.
[01:20:45.500 --> 01:20:48.060]   We apply the normalization.
[01:20:48.060 --> 01:20:53.660]   The first one.
[01:20:53.660 --> 01:20:56.940]   And this doesn't change the shape of the tensor.
[01:20:56.940 --> 01:20:59.340]   The normalization doesn't change.
[01:20:59.340 --> 01:21:00.780]   Then we apply the silo function.
[01:21:00.780 --> 01:21:07.820]   And this also doesn't change the size of the tensor.
[01:21:07.820 --> 01:21:10.300]   Then we apply the first convolution.
[01:21:10.300 --> 01:21:18.220]   This also doesn't change the size of the tensor,
[01:21:18.220 --> 01:21:21.260]   because as you can see here, we have kernel size 3, yes,
[01:21:21.260 --> 01:21:22.860]   but with the padding of 1.
[01:21:22.860 --> 01:21:26.540]   With the padding of 1, actually, it will not change the size of the tensor.
[01:21:26.540 --> 01:21:28.140]   So it will still remain this one.
[01:21:28.140 --> 01:21:32.540]   Then we apply again the group normalization 2.
[01:21:32.540 --> 01:21:38.060]   This again doesn't change the size of the tensor.
[01:21:38.060 --> 01:21:40.540]   Then we apply the silo again.
[01:21:40.540 --> 01:21:45.740]   Then we apply the convolution number 2.
[01:21:46.700 --> 01:21:54.060]   And finally, we apply the residual connection,
[01:21:54.060 --> 01:21:58.940]   which basically means that we take x plus the residual.
[01:21:58.940 --> 01:22:05.740]   But if the number of output channels is not equal to the input channels,
[01:22:05.740 --> 01:22:07.420]   you cannot add this one with this one,
[01:22:07.420 --> 01:22:09.740]   because this dimension will not match between the two.
[01:22:09.740 --> 01:22:13.660]   So what we do, we create this layer here
[01:22:13.660 --> 01:22:17.420]   to convert the input channels to the output channels of x,
[01:22:17.420 --> 01:22:19.180]   such that this sum can be done.
[01:22:19.180 --> 01:22:22.380]   So what we do is, we apply this residual layer.
[01:22:22.380 --> 01:22:27.100]   Residual layer of residual, like this.
[01:22:27.100 --> 01:22:29.660]   And this is our residual block.
[01:22:29.660 --> 01:22:33.580]   So as I told you, it's just a bunch of convolutions and group normalization.
[01:22:33.580 --> 01:22:36.140]   And for those who are familiar with the computer vision models,
[01:22:36.140 --> 01:22:38.220]   especially in ResNet, we use a lot of it.
[01:22:38.220 --> 01:22:41.980]   It's a very common block.
[01:22:43.180 --> 01:22:47.340]   Let's go build the attention block that we used also before in the encoder.
[01:22:47.340 --> 01:22:49.100]   This one here.
[01:22:49.100 --> 01:22:53.500]   And to define the attention, we also need to define the self-attention.
[01:22:53.500 --> 01:22:55.660]   So let's first build the attention block,
[01:22:55.660 --> 01:22:57.340]   which is used in the variational autoencoder.
[01:22:57.340 --> 01:23:11.020]   And then we define what is this self-attention.
[01:23:11.500 --> 01:23:28.140]   So it has a group normalization.
[01:23:28.140 --> 01:23:34.060]   Again, the channel is always 32 here in stable diffusion.
[01:23:34.060 --> 01:23:37.420]   But you also may be wondering, what is group normalization, right?
[01:23:37.420 --> 01:23:39.820]   So let's go to review it, actually, since we are here.
[01:23:40.780 --> 01:23:47.500]   And, okay, if you remember from my previous slides on Lama,
[01:23:47.500 --> 01:23:52.620]   let's go here, where we use a layer normalization.
[01:23:52.620 --> 01:23:58.140]   And also in the vanilla transformer, actually, we use layer normalization.
[01:23:58.140 --> 01:24:00.220]   So first of all, what is normalization?
[01:24:00.220 --> 01:24:03.660]   Normalization is basically when we have a deep neural network,
[01:24:03.660 --> 01:24:08.700]   each layer of the network produces some output that is fed to the next layer.
[01:24:08.700 --> 01:24:14.700]   Now, what happens is that if the output of a layer is varying in distribution,
[01:24:14.700 --> 01:24:18.380]   so sometimes, for example, the output of a layer is between 0 and 1,
[01:24:18.380 --> 01:24:22.140]   but the next step, maybe it's between 3 and 5,
[01:24:22.140 --> 01:24:25.740]   and the next step, maybe it's between 10 and 15, etc.
[01:24:25.740 --> 01:24:29.180]   So the distribution of the output of a layer changes,
[01:24:29.180 --> 01:24:32.060]   then the next layer also will see some input
[01:24:32.060 --> 01:24:36.300]   that is very different from what the layer is used to see.
[01:24:36.860 --> 01:24:42.620]   This will basically push the output of the next layer into a new distribution itself,
[01:24:42.620 --> 01:24:45.900]   which, in turn, will push the loss function into,
[01:24:45.900 --> 01:24:53.740]   basically, the output of the model to change very frequently in distribution.
[01:24:53.740 --> 01:24:55.500]   So sometimes it will be a very big number,
[01:24:55.500 --> 01:24:56.940]   sometimes it will be a very small number,
[01:24:56.940 --> 01:24:59.740]   sometimes it will be negative, sometimes it will be positive, etc.
[01:24:59.740 --> 01:25:04.060]   And this basically makes the loss function oscillate too much,
[01:25:04.060 --> 01:25:05.980]   and it makes the training slower.
[01:25:05.980 --> 01:25:09.740]   So what we do is we normalize the values before feeding them into layers,
[01:25:09.740 --> 01:25:13.820]   such that each layer always sees the same distribution of the data.
[01:25:13.820 --> 01:25:19.260]   So it will always see numbers that are distributed around 0 with a variance of 1.
[01:25:19.260 --> 01:25:21.580]   And this is the job of the layer normalization.
[01:25:21.580 --> 01:25:25.180]   So imagine you are a layer, and you have some input,
[01:25:25.180 --> 01:25:27.260]   which is a batch of 10 items.
[01:25:27.260 --> 01:25:31.180]   Each item has some features, so feature 1, feature 2, feature 3.
[01:25:31.180 --> 01:25:36.380]   Layer normalization calculates a mean and the variance over these features here,
[01:25:36.380 --> 01:25:38.380]   so over this distribution here,
[01:25:38.380 --> 01:25:42.140]   and then normalizes this value according to this formula.
[01:25:42.140 --> 01:25:46.460]   So each value basically becomes distributed between 0 and 1.
[01:25:46.460 --> 01:25:50.700]   With batch normalization, we normalize by columns,
[01:25:50.700 --> 01:25:54.380]   so the statistics mean and the sigma is calculated by columns.
[01:25:54.380 --> 01:25:57.420]   With layer normalization, it is calculated by rows,
[01:25:57.420 --> 01:26:00.620]   so each item independently from the others.
[01:26:00.620 --> 01:26:03.100]   With group normalization, on the other hand,
[01:26:03.100 --> 01:26:10.860]   it is like layer normalization, but not all of the features of the item, but grouped.
[01:26:10.860 --> 01:26:13.500]   So for example, imagine you have four features here.
[01:26:13.500 --> 01:26:18.620]   So here you have F1, F2, F3, F4, and you have two groups.
[01:26:18.620 --> 01:26:23.740]   Then the first group will be F1 and F2, and the second group will be F3 and F4.
[01:26:23.740 --> 01:26:26.860]   So you will have two means and two variance,
[01:26:26.860 --> 01:26:29.260]   one for the first group, one for the second group.
[01:26:29.900 --> 01:26:32.540]   But why do we use it like this?
[01:26:32.540 --> 01:26:35.740]   Why do we want to group this kind of features?
[01:26:35.740 --> 01:26:39.180]   Because these features actually, they come from convolutions.
[01:26:39.180 --> 01:26:42.460]   And as we saw before, let's go back to the website.
[01:26:42.460 --> 01:26:45.740]   Imagine you have a kernel of five here.
[01:26:45.740 --> 01:26:51.900]   Each output here actually comes from local area of the image.
[01:26:51.900 --> 01:26:55.260]   So the two close features, for example,
[01:26:55.260 --> 01:26:59.340]   two things that are close to each other, may be related to each other.
[01:26:59.340 --> 01:27:02.540]   So two things that are far from each other are not related to each other.
[01:27:02.540 --> 01:27:07.260]   This is why we can group, we can use group normalization in this case.
[01:27:07.260 --> 01:27:12.860]   Because closer features to each other will have kind of the same distribution,
[01:27:12.860 --> 01:27:15.020]   or we make them have the same distribution,
[01:27:15.020 --> 01:27:17.580]   and things that are far from each other may not.
[01:27:17.580 --> 01:27:20.300]   This is the basic idea behind group normalization.
[01:27:20.300 --> 01:27:22.700]   But the whole idea behind the normalization is that
[01:27:22.700 --> 01:27:25.260]   we don't want these things to oscillate too much.
[01:27:25.260 --> 01:27:27.420]   Otherwise, the loss of function will oscillate
[01:27:27.420 --> 01:27:28.860]   and will make the training slower.
[01:27:28.860 --> 01:27:30.940]   With normalization, we make the training faster.
[01:27:30.940 --> 01:27:33.100]   So let's go back to coding.
[01:27:33.100 --> 01:27:36.300]   So we were coding the attention block.
[01:27:36.300 --> 01:27:40.540]   So now the attention block has this group normalization and also an attention,
[01:27:40.540 --> 01:27:43.020]   which is a self-attention.
[01:27:43.020 --> 01:27:44.140]   And later we define it.
[01:27:44.140 --> 01:27:48.140]   And channels, okay.
[01:27:48.140 --> 01:27:51.180]   This one have a forward method.
[01:27:54.300 --> 01:27:59.020]   Torch.tensor, returns, of course, torch.tensor.
[01:27:59.020 --> 01:28:02.060]   Okay, what is the input of this block?
[01:28:02.060 --> 01:28:06.700]   The input of this block is something, where is it?
[01:28:06.700 --> 01:28:07.500]   Here.
[01:28:07.500 --> 01:28:11.980]   It's something in the form of batch size, number of channels, height and width.
[01:28:11.980 --> 01:28:15.180]   But because it will be used in many positions, this attention block,
[01:28:15.180 --> 01:28:17.500]   we don't define a specific size.
[01:28:17.500 --> 01:28:20.860]   So we just say that x is something that is a batch size,
[01:28:21.900 --> 01:28:25.900]   features or channels, if you want, height and width.
[01:28:25.900 --> 01:28:28.940]   Again, we create a residual connection.
[01:28:28.940 --> 01:28:34.860]   And the first thing we do is we extract the shape.
[01:28:34.860 --> 01:28:38.220]   So n is the batch size, the number of channels,
[01:28:38.220 --> 01:28:41.420]   the height and the width is equal to x.shape.
[01:28:41.420 --> 01:28:45.340]   Then, as I told you before,
[01:28:45.340 --> 01:28:49.580]   we do the self-attention between all the pixels of this image.
[01:28:49.580 --> 01:28:50.700]   And I will show you how.
[01:28:50.700 --> 01:29:03.660]   This will transform this tensor here into this tensor here.
[01:29:03.660 --> 01:29:06.620]   Height multiplied by width.
[01:29:06.620 --> 01:29:11.340]   So now we have a sequence where each item represents a pixel
[01:29:11.340 --> 01:29:13.260]   because we multiplied height by width.
[01:29:13.260 --> 01:29:15.180]   And then we transpose it.
[01:29:15.180 --> 01:29:17.420]   So put it back a little before.
[01:29:18.220 --> 01:29:20.620]   Transpose the -1 with -2.
[01:29:20.620 --> 01:29:26.940]   This will transform this shape into this shape.
[01:29:26.940 --> 01:29:31.180]   So we put back this one.
[01:29:31.180 --> 01:29:34.700]   So this one comes before and features becomes the last one.
[01:29:34.700 --> 01:29:37.420]   Something like this.
[01:29:37.420 --> 01:29:40.140]   And okay.
[01:29:40.140 --> 01:29:43.660]   So as you can see from this tensor here,
[01:29:43.660 --> 01:29:47.500]   this is like when we do the attention in the transformer model.
[01:29:47.500 --> 01:29:50.220]   So in the transformer model, we have a sequence of tokens.
[01:29:50.220 --> 01:29:52.700]   Each token is representing, for example, a word.
[01:29:52.700 --> 01:29:57.180]   And the attention basically calculates the attention between each token.
[01:29:57.180 --> 01:30:00.220]   So how do two tokens are related to each other?
[01:30:00.220 --> 01:30:03.420]   In this case, we can think of it as a sequence of pixels.
[01:30:03.420 --> 01:30:07.820]   Each pixel with its own embedding, which is the features of that pixel.
[01:30:07.820 --> 01:30:10.060]   And we relate pixels to each other.
[01:30:10.060 --> 01:30:12.780]   And then we do the attention.
[01:30:12.780 --> 01:30:17.260]   Which is a self-attention.
[01:30:17.260 --> 01:30:20.940]   In which self-attention means that the query key and values are the same input.
[01:30:20.940 --> 01:30:25.500]   And this doesn't change the shape.
[01:30:25.500 --> 01:30:28.540]   So this one remains the same.
[01:30:28.540 --> 01:30:30.620]   Then we transpose back.
[01:30:30.620 --> 01:30:36.060]   And we do the inverse transformation.
[01:30:36.060 --> 01:30:39.260]   So because we put it in this form only to do attention.
[01:30:39.260 --> 01:30:40.940]   So now we transpose.
[01:30:40.940 --> 01:30:42.460]   So we take this one.
[01:30:46.140 --> 01:30:48.860]   And we convert it into features.
[01:30:48.860 --> 01:30:51.900]   And then height and width.
[01:30:51.900 --> 01:30:57.500]   And then again, we remove this multiplication by viewing again the tensor.
[01:30:57.500 --> 01:31:01.420]   So n, c, h, w.
[01:31:01.420 --> 01:31:05.980]   So we go from here.
[01:31:05.980 --> 01:31:11.020]   To here.
[01:31:11.020 --> 01:31:13.660]   Then we add the residual connection.
[01:31:15.580 --> 01:31:16.460]   And we return x.
[01:31:16.460 --> 01:31:17.660]   That's it.
[01:31:17.660 --> 01:31:22.380]   The residual connection will not change the size of the input.
[01:31:22.380 --> 01:31:25.900]   And we return a tensor of this shape here.
[01:31:25.900 --> 01:31:27.660]   Let me check also the residual connection here.
[01:31:27.660 --> 01:31:28.460]   It's correct.
[01:31:28.460 --> 01:31:29.260]   Okay.
[01:31:29.260 --> 01:31:32.700]   Now that we have also built the attention block, let's build also the self-attention.
[01:31:32.700 --> 01:31:35.180]   Since we are building the attentions.
[01:31:35.180 --> 01:31:39.820]   And the attentions, because we have two kinds of attention in the stable diffusion.
[01:31:39.820 --> 01:31:41.820]   One is called the self-attention.
[01:31:41.820 --> 01:31:43.820]   And one is the cross-attention.
[01:31:43.820 --> 01:31:44.940]   And we need to build both.
[01:31:44.940 --> 01:31:48.300]   So let's go build it in a separate class called "Attention".
[01:31:48.300 --> 01:31:55.100]   And okay.
[01:31:55.100 --> 01:32:08.780]   So again, import torch.
[01:32:08.940 --> 01:32:18.940]   Okay.
[01:32:18.940 --> 01:32:27.580]   I think you guys maybe want to review the attention before building it.
[01:32:27.580 --> 01:32:28.860]   So let's go review it.
[01:32:28.860 --> 01:32:34.780]   I have here opened my slides from my video about the attention model for the transformer model.
[01:32:35.740 --> 01:32:42.140]   So the self-attention, basically, it's a way for, especially in a language model,
[01:32:42.140 --> 01:32:45.180]   is a way for us to relate tokens to each other.
[01:32:45.180 --> 01:32:47.420]   So we start with a sequence of tokens.
[01:32:47.420 --> 01:32:50.540]   Each one of them having an embedding of size d model.
[01:32:50.540 --> 01:32:53.260]   And we transform it into queries, key, and values.
[01:32:53.260 --> 01:32:57.660]   In which query, key, and values in the self-attention are the same matrix, same sequence.
[01:32:57.660 --> 01:33:01.180]   We multiply them by wq matrix.
[01:33:01.180 --> 01:33:04.620]   So wq, wk, and wv, which are parameter matrices.
[01:33:05.340 --> 01:33:10.540]   Then we split them along the d model dimension into number of heads.
[01:33:10.540 --> 01:33:12.700]   So we can specify how many heads we want.
[01:33:12.700 --> 01:33:18.940]   In our case, the one attention that we will do here is actually only one head.
[01:33:18.940 --> 01:33:19.740]   I will show you later.
[01:33:19.740 --> 01:33:23.580]   And then we calculate the attention for each of this head.
[01:33:23.580 --> 01:33:28.060]   Then we combine back by concatenating this head together.
[01:33:28.780 --> 01:33:35.260]   We multiply this output matrix of the concatenation with another matrix called wo,
[01:33:35.260 --> 01:33:36.620]   which is the output matrix.
[01:33:36.620 --> 01:33:40.700]   And then this is the output of the multi-head attention.
[01:33:40.700 --> 01:33:44.540]   If we have only one head, instead of being a multi-head,
[01:33:44.540 --> 01:33:46.940]   then we will not do this splitting operation.
[01:33:46.940 --> 01:33:51.260]   We will just do this multiplication with the w and with the wo.
[01:33:51.260 --> 01:33:54.940]   And OK, this is how the self-attention works.
[01:33:54.940 --> 01:33:59.100]   So in a self-attention, we have this query key and values coming from the same matrix input.
[01:33:59.100 --> 01:34:01.500]   And this is what we are going to build.
[01:34:01.500 --> 01:34:12.300]   So we have the number of heads.
[01:34:12.300 --> 01:34:14.380]   Then we have the embedding.
[01:34:14.380 --> 01:34:16.700]   So what is the embedding of each token?
[01:34:16.700 --> 01:34:19.980]   But in our case, we are not talking about tokens.
[01:34:19.980 --> 01:34:21.340]   We will talk about pixels.
[01:34:21.340 --> 01:34:26.700]   And we can think that the number of channels of each pixel is the embedding of the pixel.
[01:34:26.700 --> 01:34:30.380]   So the embedding, just like in the original transformer,
[01:34:30.380 --> 01:34:35.020]   the embeddings are the kind of vectors that capture the meaning of the word.
[01:34:35.020 --> 01:34:36.540]   In this case, we have the channels.
[01:34:36.540 --> 01:34:39.900]   Each channel, each pixel represented by many channels
[01:34:39.900 --> 01:34:41.980]   that capture the information about that pixel.
[01:34:41.980 --> 01:34:49.420]   Here we have also the bias for the w matrices,
[01:34:49.420 --> 01:34:51.500]   which we don't have in the original transformer.
[01:34:51.500 --> 01:35:07.900]   OK, now let's define the w matrices.
[01:35:07.900 --> 01:35:09.580]   So wqwq and wv.
[01:35:09.580 --> 01:35:11.980]   We will represent it as one big linear layer.
[01:35:11.980 --> 01:35:16.540]   Instead of representing it as three different matrices, it's possible.
[01:35:17.500 --> 01:35:21.340]   We just say that it's a big matrix, three by the embedding.
[01:35:21.340 --> 01:35:25.260]   And the bias is if we want it.
[01:35:25.260 --> 01:35:28.620]   So in projection, in projection bias.
[01:35:28.620 --> 01:35:30.620]   So this means stands for in projection,
[01:35:30.620 --> 01:35:34.300]   because it's a projection of the input before we apply the attention.
[01:35:34.300 --> 01:35:37.420]   And then there is an auto projection, which is after we apply the attention.
[01:35:37.420 --> 01:35:38.700]   So the wo matrix.
[01:35:47.100 --> 01:35:51.100]   So as you remember here, the wo matrix is actually the model by the model.
[01:35:51.100 --> 01:35:53.420]   The input is also the model by the model.
[01:35:53.420 --> 01:35:54.780]   And this is exactly what we did.
[01:35:54.780 --> 01:35:56.860]   But we have three of them here.
[01:35:56.860 --> 01:35:58.220]   So it's three by the model.
[01:35:58.220 --> 01:36:08.060]   And then we save the number of heads.
[01:36:08.060 --> 01:36:14.300]   And then we saved the dimension of each head.
[01:36:15.500 --> 01:36:18.780]   The dimension of each head basically means that if we have multi head,
[01:36:18.780 --> 01:36:21.980]   each head will watch a part of the embedding of each token.
[01:36:21.980 --> 01:36:25.820]   So we need to save how much is this size.
[01:36:25.820 --> 01:36:28.060]   So the model divided by the number of heads.
[01:36:28.060 --> 01:36:32.700]   But divide by the number of heads.
[01:36:32.700 --> 01:36:35.420]   Let's implement the forward.
[01:36:35.420 --> 01:36:41.660]   We can also apply a mask.
[01:36:43.660 --> 01:36:47.580]   As you remember, the mask is a way to avoid relating tokens,
[01:36:47.580 --> 01:36:52.540]   one particular token with the tokens that come after it,
[01:36:52.540 --> 01:36:54.780]   but only with the token that come before it.
[01:36:54.780 --> 01:36:58.380]   And this is called the causal mask.
[01:36:58.380 --> 01:37:05.500]   If you really are not understanding what is happening here in the attention,
[01:37:05.500 --> 01:37:07.420]   I highly recommend you watch my previous video,
[01:37:07.420 --> 01:37:09.980]   because it's explained very well.
[01:37:09.980 --> 01:37:14.380]   And if you watch it, it will take not so much time.
[01:37:14.380 --> 01:37:16.300]   And I think you will learn a lot.
[01:37:16.300 --> 01:37:20.780]   So the first thing we do is extract the shape.
[01:37:20.780 --> 01:37:31.420]   Then we extract the size, the sequence,
[01:37:31.420 --> 01:37:38.460]   length and the embedding is equal to input shape.
[01:37:38.780 --> 01:37:47.020]   And then we say that we will convert it into another shape
[01:37:47.020 --> 01:37:48.460]   that I will show you later why.
[01:37:48.460 --> 01:37:58.140]   This is called the interim shape, intermediate shape.
[01:37:58.140 --> 01:38:06.700]   Then we apply the query key and value.
[01:38:06.700 --> 01:38:12.540]   We apply the in projection, so the wq, wq and wv matrix to the input,
[01:38:12.540 --> 01:38:14.620]   and we convert it into query key and values.
[01:38:14.620 --> 01:38:16.620]   So query key and values are equal to...
[01:38:16.620 --> 01:38:22.220]   We multiply it, but then we divide it with chunk.
[01:38:22.220 --> 01:38:23.900]   As I showed you before, what is chunk?
[01:38:23.900 --> 01:38:29.260]   Basically, we will multiply the input with the big matrix
[01:38:29.260 --> 01:38:31.420]   that represents wq, wq and wq,
[01:38:31.420 --> 01:38:34.700]   but then we split it back into three smaller matrices.
[01:38:34.700 --> 01:38:37.740]   This is the same as applying three different projections.
[01:38:37.740 --> 01:38:38.960]   Instead of...
[01:38:38.960 --> 01:38:43.500]   It's the same as applying three separate in projections,
[01:38:43.500 --> 01:38:46.860]   but it's also possible to combine it in one big matrix.
[01:38:46.860 --> 01:38:53.420]   This, what we will do, basically it will convert batch size,
[01:38:56.060 --> 01:39:05.740]   sequence length, dimension into batch size, sequence length, dimension multiplied by three.
[01:39:05.740 --> 01:39:09.820]   And then by using chunk, we split it along the last dimension
[01:39:09.820 --> 01:39:19.100]   into three different tensors of shape, batch size, sequence length and dimension.
[01:39:23.900 --> 01:39:31.180]   Okay, now we can split the query key and values in the number of heads.
[01:39:31.180 --> 01:39:34.220]   According to the number of heads, this is why we built this shape,
[01:39:34.220 --> 01:39:42.940]   which means split the dimension, the last dimension into n heads.
[01:39:42.940 --> 01:40:02.540]   And the values v.view, wonderful.
[01:40:02.540 --> 01:40:07.580]   This will convert, okay, let's write it,
[01:40:08.620 --> 01:40:17.180]   batch size, sequence length, dimension into batch size, sequence length,
[01:40:17.180 --> 01:40:23.260]   then h, so the number of heads and each dimension divided by the number of heads.
[01:40:23.260 --> 01:40:26.060]   So each head will watch the full sequence,
[01:40:26.060 --> 01:40:31.260]   but only a part of the embedding of each token, in this case, pixel.
[01:40:31.260 --> 01:40:35.180]   And we'll watch this part of the head.
[01:40:35.180 --> 01:40:39.500]   So the full dimension, the embedding divided by the number of heads.
[01:40:39.500 --> 01:40:46.380]   And then this will convert it, because we are also transposing,
[01:40:46.380 --> 01:40:49.020]   this will convert it into batch size,
[01:40:49.020 --> 01:40:55.900]   h, sequence length, and then dimension h.
[01:40:55.900 --> 01:41:00.220]   So each head will watch all the sequence, but only a part of the embedding.
[01:41:03.180 --> 01:41:07.100]   We then calculate the attention, just like the formula.
[01:41:07.100 --> 01:41:09.900]   So query multiplied by the transpose of the keys.
[01:41:09.900 --> 01:41:15.100]   So is the query, matrix multiplication with the transpose of the keys.
[01:41:15.100 --> 01:41:19.740]   This will return a matrix of size,
[01:41:19.740 --> 01:41:25.420]   batch size, h, sequence length by sequence length.
[01:41:25.420 --> 01:41:28.300]   We can then apply the mask.
[01:41:30.860 --> 01:41:35.980]   As you remember, the mask is something that we apply when we calculate the attention,
[01:41:35.980 --> 01:41:38.780]   if we don't want two tokens to relate to each other.
[01:41:38.780 --> 01:41:41.820]   We basically substitute their value.
[01:41:41.820 --> 01:41:47.500]   In this matrix, we substitute the interaction with minus infinity before applying the softmax,
[01:41:47.500 --> 01:41:49.820]   so that the softmax will make it zero.
[01:41:49.820 --> 01:41:51.260]   So this is what we are doing here.
[01:41:51.260 --> 01:41:54.140]   We first build the mask.
[01:41:54.540 --> 01:42:00.700]   This will create a causal mask, basically a mask where the upper triangle,
[01:42:00.700 --> 01:42:13.740]   so above the principal diagonal, is made up of one ones, a lot of ones.
[01:42:13.740 --> 01:42:17.500]   And then we can then apply the softmax.
[01:42:17.500 --> 01:42:22.540]   One ones, a lot of ones.
[01:42:22.540 --> 01:42:25.900]   And then we fill it up with minus infinity.
[01:42:25.900 --> 01:42:32.940]   Masked, oops, not mask, but wait.
[01:42:32.940 --> 01:42:44.860]   Masked fill, but with mask, and we put minus infinity, like this.
[01:42:46.220 --> 01:42:48.860]   As you remember, the formula of the transformer is a
[01:42:48.860 --> 01:42:54.220]   query multiplied by the transpose of the keys, and then divided by the square root of the model.
[01:42:54.220 --> 01:42:55.580]   So this is what we will do now.
[01:42:55.580 --> 01:43:02.140]   So divided by the square root of the model, set of the head.
[01:43:02.140 --> 01:43:06.140]   And then we apply the softmax.
[01:43:15.020 --> 01:43:17.180]   We multiply it by the WO matrix.
[01:43:17.180 --> 01:43:24.540]   We transpose back.
[01:43:24.540 --> 01:43:29.420]   So we want to remove, now we want to remove the head dimension.
[01:43:29.420 --> 01:43:35.820]   So output is equal to, let me write some shapes.
[01:43:35.820 --> 01:43:37.020]   So what is this?
[01:43:37.020 --> 01:43:45.580]   This is equal to patch size, sequence by sequence,
[01:43:45.580 --> 01:43:50.620]   multiplied, so matrix multiplication with patch size.
[01:43:56.620 --> 01:44:09.820]   This will result into patch size, H, sequence length, and dimension divided by H.
[01:44:09.820 --> 01:44:15.660]   This we then multiplied by the, we then transpose.
[01:44:15.660 --> 01:44:23.580]   And this will result into, so we start with this one.
[01:44:23.580 --> 01:44:38.860]   And it becomes, wait I put too many parentheses here, patch size, sequence length,
[01:44:38.860 --> 01:44:44.620]   H, and dimensions, okay.
[01:44:44.620 --> 01:44:57.980]   Then we can reshape as the input, like the initial shape, so this one.
[01:44:57.980 --> 01:45:02.540]   And then we apply the output projection.
[01:45:02.540 --> 01:45:13.900]   So we multiply it by the WO matrix.
[01:45:13.900 --> 01:45:22.060]   Okay.
[01:45:22.060 --> 01:45:26.220]   This is the self-attention.
[01:45:26.220 --> 01:45:29.100]   Now let's go back to continue building the decoder.
[01:45:29.100 --> 01:45:31.900]   For now we have built the attention block and the residual block.
[01:45:31.900 --> 01:45:45.740]   But we need to build the decoder.
[01:45:45.740 --> 01:46:00.860]   And also this one is a sequence of modules that we will apply one after another.
[01:46:00.860 --> 01:46:03.180]   We start with the convolution just like before.
[01:46:03.180 --> 01:46:07.100]   Now I will not write again the shapes change, but you got the idea.
[01:46:07.100 --> 01:46:12.860]   In the encoder we, in the encoder, let me show you here.
[01:46:12.860 --> 01:46:17.900]   Here.
[01:46:17.900 --> 01:46:22.540]   In the encoder we keep reducing the size of the image until it becomes small.
[01:46:22.540 --> 01:46:27.180]   In the decoder we need to return to the original size of the image.
[01:46:27.180 --> 01:46:34.940]   So we start with the latent dimension and we return to the original dimension of the image.
[01:46:34.940 --> 01:46:39.100]   Convolution.
[01:46:39.100 --> 01:46:52.460]   So we start with four channels and we output four channels.
[01:46:54.460 --> 01:46:55.820]   Then we have another convolution.
[01:46:55.820 --> 01:47:03.100]   We go to 500.
[01:47:03.100 --> 01:47:12.220]   Then we have a residual block just like before.
[01:47:12.220 --> 01:47:22.540]   Then we have an attention block.
[01:47:23.180 --> 01:47:31.980]   Then we have a bunch of residual blocks and we have four of them.
[01:47:31.980 --> 01:47:42.540]   Let me copy.
[01:47:42.540 --> 01:47:45.360]   Okay.
[01:47:45.360 --> 01:47:49.100]   Now the residual blocks, let me write some shapes here.
[01:47:49.820 --> 01:47:53.660]   Here we arrived to a situation in which we have batch size.
[01:47:53.660 --> 01:47:58.700]   We have 512 features and the size of the image still didn't grow
[01:47:58.700 --> 01:48:02.140]   because we didn't have any convolution that will make it grow.
[01:48:02.140 --> 01:48:12.700]   This one of course will remain the same because it's a residual block and etc.
[01:48:12.700 --> 01:48:15.580]   Now to increase the size of the image.
[01:48:15.580 --> 01:48:21.740]   So now the image is actually height divided by 8 which height as you remember is 512,
[01:48:21.740 --> 01:48:24.220]   the size of the image that we are working with.
[01:48:24.220 --> 01:48:27.980]   So this dimension here is 64 by 64.
[01:48:27.980 --> 01:48:29.260]   How can we increase it?
[01:48:29.260 --> 01:48:31.260]   We use one module called upsample.
[01:48:31.260 --> 01:48:42.220]   The upsample, we have to think of it like when we resize an image.
[01:48:42.220 --> 01:48:45.340]   So imagine you have an image that is 64 by 64
[01:48:45.340 --> 01:48:48.860]   and you want to transform it to 128 by 128.
[01:48:48.860 --> 01:48:52.620]   The upsample will do it just like when we resize an image.
[01:48:52.620 --> 01:48:57.580]   So it will replicate the pixels twice.
[01:48:57.580 --> 01:49:02.220]   So along the dimensions right and down for example twice.
[01:49:02.220 --> 01:49:07.180]   So that the total amount of pixels, the height and the width actually doubles.
[01:49:07.180 --> 01:49:10.860]   This is the upsample basically.
[01:49:10.860 --> 01:49:16.940]   It will just replicate each pixel so that by this scale factor along each dimension.
[01:49:16.940 --> 01:49:24.060]   So this one becomes batch size
[01:49:24.060 --> 01:49:32.860]   divided by 8, width divided by 8 becomes as we see here
[01:49:38.460 --> 01:49:41.260]   8 divided by 4 and width divided by 4.
[01:49:41.260 --> 01:49:47.420]   Then we have a convolution, residual blocks.
[01:49:47.420 --> 01:50:00.220]   So we have convolutions of 2D, 512 to 512.
[01:50:05.180 --> 01:50:09.340]   Then we have residual blocks of 512 by 500.
[01:50:09.340 --> 01:50:11.660]   But in this case we have three of them, 2, 3.
[01:50:11.660 --> 01:50:14.060]   Then we have another upsample.
[01:50:14.060 --> 01:50:17.660]   This will again double the size of the image.
[01:50:17.660 --> 01:50:20.860]   So we have another one that will double the size of the image.
[01:50:20.860 --> 01:50:23.340]   And by a scale factor of 2.
[01:50:23.340 --> 01:50:29.340]   So now our image which was divided by 4 with 512 channels.
[01:50:29.340 --> 01:50:30.780]   So let's write it like this.
[01:50:30.780 --> 01:50:33.340]   Will become divided by 2 now.
[01:50:34.300 --> 01:50:37.100]   So it will double the size of the image.
[01:50:37.100 --> 01:50:41.660]   So now our image is 256 by 256.
[01:50:41.660 --> 01:50:45.980]   Then again we have a convolution.
[01:50:45.980 --> 01:50:54.940]   And then we have three residual blocks again.
[01:50:54.940 --> 01:50:58.860]   But this time we reduce the number of features.
[01:50:58.940 --> 01:51:06.780]   So 256 and then it's 256 to 256.
[01:51:06.780 --> 01:51:14.300]   Okay, then we have another upsampling which will again double the size of the image.
[01:51:14.300 --> 01:51:23.020]   And this time we will go from divide by 2 to divide by 2 up to the original size.
[01:51:26.220 --> 01:51:31.100]   And because the number of channels has changed, we are not 512 anymore.
[01:51:31.100 --> 01:51:32.480]   Okay.
[01:51:32.480 --> 01:51:35.820]   And then we have another convolution.
[01:51:35.820 --> 01:51:40.540]   This case with 256 because it's the new number of features.
[01:51:40.540 --> 01:51:49.260]   Then we have another bunch of residual blocks that will decrease the number of features.
[01:51:52.380 --> 01:52:00.060]   So we go to 256 to 128.
[01:52:00.060 --> 01:52:07.980]   We have finally a group norm.
[01:52:07.980 --> 01:52:12.700]   32 is the group size.
[01:52:12.700 --> 01:52:19.660]   So we group features in groups of 32 before calculating the mu and the sigma before normalizing.
[01:52:20.540 --> 01:52:25.180]   And we define the number of channels as 128 which is the number of features that we have.
[01:52:25.180 --> 01:52:32.380]   So this group normalization will divide these 128 features into groups of 32.
[01:52:32.380 --> 01:52:36.220]   Then we apply the silu.
[01:52:36.220 --> 01:52:41.340]   And then we have a convolution.
[01:52:41.340 --> 01:52:47.660]   The final convolution that will transform into an image with the three channels.
[01:52:47.660 --> 01:52:54.380]   So RGB by applying these convolutions here which doesn't change the size of the output.
[01:52:54.380 --> 01:53:01.660]   So we'll go from an image that is batch size 128 height width.
[01:53:01.660 --> 01:53:02.380]   Why height width?
[01:53:02.380 --> 01:53:10.860]   Because after the last upsampling we become of the original size into an image with only three channels.
[01:53:10.860 --> 01:53:20.860]   And this is our decoder.
[01:53:20.860 --> 01:53:29.260]   Now we can write the forward method.
[01:53:36.780 --> 01:53:39.580]   I'm sorry if I'm putting a lot of spaces between here.
[01:53:39.580 --> 01:53:43.580]   But otherwise it's easy to get lost and not understand where we are.
[01:53:43.580 --> 01:53:50.620]   So here the input of the decoder is our latent.
[01:53:50.620 --> 01:53:57.500]   So it's batch size 4 height divided by 8 width divided by 8.
[01:53:57.500 --> 01:54:02.300]   As you remember here in the encoder the last thing we do is be scaled by this constant.
[01:54:02.300 --> 01:54:05.180]   So we nullify this scaling.
[01:54:05.180 --> 01:54:07.020]   So we reverse this scaling.
[01:54:07.020 --> 01:54:13.980]   215 and then we run it through the decoder.
[01:54:13.980 --> 01:54:30.380]   And then return x which is batch size 3 height and width.
[01:54:31.260 --> 01:54:35.500]   Let me also write the input of this decoder which is this one.
[01:54:35.500 --> 01:54:37.820]   We already have it.
[01:54:37.820 --> 01:54:41.180]   Okay this is our variational auto encoder.
[01:54:41.180 --> 01:54:43.740]   So far let's go review.
[01:54:43.740 --> 01:54:50.620]   We are building our architecture of the stable diffusion.
[01:54:50.620 --> 01:54:53.980]   So far we have built the encoder and the decoder.
[01:54:53.980 --> 01:55:00.140]   But now we have to build the unit and then we have to build the clip text encoder.
[01:55:01.020 --> 01:55:05.740]   And finally we have to build the pipeline that will connect all of these things.
[01:55:05.740 --> 01:55:10.220]   So it's going to be a long journey but it's fun actually to build things.
[01:55:10.220 --> 01:55:13.180]   Because you learn every detail of how they work.
[01:55:13.180 --> 01:55:16.780]   So the next thing that we are going to build is the text encoder.
[01:55:16.780 --> 01:55:22.700]   So this clip encoder here that will allow us to encode the prompt into embeddings
[01:55:22.700 --> 01:55:25.660]   that we can then feed to this unit model here.
[01:55:25.660 --> 01:55:28.620]   So let's build this clip encoder.
[01:55:28.620 --> 01:55:31.180]   And we will of course use a pre-trained version.
[01:55:31.180 --> 01:55:35.260]   So by downloading the vocabulary and I will show you how it works.
[01:55:35.260 --> 01:55:36.700]   So let's start.
[01:55:36.700 --> 01:55:39.340]   We go to Visual Studio Code.
[01:55:39.340 --> 01:55:43.260]   We create a new file in st folder called clip.py.
[01:55:43.260 --> 01:55:45.820]   And here.
[01:55:45.820 --> 01:55:49.020]   And we start importing the usual stuff.
[01:55:49.020 --> 01:56:08.060]   [typing]
[01:56:08.060 --> 01:56:10.620]   And we also import self-attention because we will be using it.
[01:56:10.620 --> 01:56:15.100]   So basically clip is a layer.
[01:56:15.100 --> 01:56:18.620]   It's very similar to the encoder layer of the transformer.
[01:56:18.620 --> 01:56:20.300]   So as you remember the transformer.
[01:56:20.300 --> 01:56:21.580]   Let me show you here.
[01:56:21.580 --> 01:56:24.140]   The transformer.
[01:56:24.140 --> 01:56:26.460]   This is the encoder layer of the transformer.
[01:56:26.460 --> 01:56:30.060]   It's made of attention and then feed forwards.
[01:56:30.060 --> 01:56:34.140]   And there are many blocks like this one after another that are applied one after another.
[01:56:34.140 --> 01:56:38.700]   We also have something that tells the position of each token inside of the sentence.
[01:56:38.700 --> 01:56:41.020]   And we will also have something similar in clip.
[01:56:41.020 --> 01:56:44.460]   So we need to build something very similar to this one.
[01:56:44.460 --> 01:56:48.140]   And actually this is why I mean the transformer model was very successful.
[01:56:48.140 --> 01:56:51.420]   So that's why they use the same structure of course also for this purpose.
[01:56:51.420 --> 01:56:54.140]   And so let's go to build it.
[01:56:54.140 --> 01:56:57.500]   The first thing we will build.
[01:56:57.500 --> 01:57:00.700]   I will build first the skeleton of the model and then we will build each block.
[01:57:00.700 --> 01:57:02.860]   So let's build clip.
[01:57:02.860 --> 01:57:13.100]   [typing]
[01:57:13.100 --> 01:57:15.820]   And this has some embeddings.
[01:57:15.820 --> 01:57:19.660]   The embeddings allow us to convert the tokens.
[01:57:19.660 --> 01:57:23.180]   So as you remember in when you have a sentence made up of text.
[01:57:23.180 --> 01:57:25.260]   First you convert it into numbers.
[01:57:25.260 --> 01:57:28.940]   Where each number indicates the position of the token inside of the vocabulary.
[01:57:28.940 --> 01:57:31.180]   And then you convert it into embeddings.
[01:57:31.180 --> 01:57:36.220]   Where each embedding represents a vector of size 512 in the original transformer.
[01:57:36.220 --> 01:57:39.740]   But here in clip the size is 768.
[01:57:40.300 --> 01:57:45.660]   And each vector represents kind of the meaning of the word or the token captures.
[01:57:45.660 --> 01:57:48.380]   So this is an embedding.
[01:57:48.380 --> 01:57:51.260]   And later we define it.
[01:57:51.260 --> 01:57:52.620]   We need the vocabulary size.
[01:57:52.620 --> 01:57:55.420]   The vocabulary size is 49408.
[01:57:55.420 --> 01:57:56.940]   I took it directly from the file.
[01:57:56.940 --> 01:57:59.100]   This is the embedding size.
[01:57:59.100 --> 01:58:00.140]   And the sequence length.
[01:58:00.140 --> 01:58:02.060]   The maximum sequence length that we can have.
[01:58:02.060 --> 01:58:05.260]   Because we need to use the padding is 77.
[01:58:05.260 --> 01:58:09.820]   Because we should actually use some configuration file to save.
[01:58:09.820 --> 01:58:13.820]   But because we will be using with the pre-trained stable diffusion model.
[01:58:13.820 --> 01:58:16.060]   The size are already fixed for us.
[01:58:16.060 --> 01:58:20.300]   But in the future I will refactor the code to add some configuration actually.
[01:58:20.300 --> 01:58:23.900]   To make it more extensible.
[01:58:23.900 --> 01:58:31.180]   This is a list of layers.
[01:58:31.180 --> 01:58:33.900]   Each we call it the clip layer.
[01:58:36.060 --> 01:58:37.340]   We have this 12.
[01:58:37.340 --> 01:58:43.020]   Which indicates the number of head of the multihead attention.
[01:58:43.020 --> 01:58:47.020]   And then the embedding size which is 768.
[01:58:47.020 --> 01:58:51.020]   And we have 12 of these layers.
[01:58:51.020 --> 01:58:54.940]   Then we have the layer normalization.
[01:58:54.940 --> 01:58:58.460]   Layer norm.
[01:58:58.460 --> 01:59:03.100]   And we tell him how many features.
[01:59:03.100 --> 01:59:04.380]   So 768.
[01:59:05.180 --> 01:59:07.580]   And then we define the forward method.
[01:59:07.580 --> 01:59:14.940]   This is tensor.
[01:59:14.940 --> 01:59:18.460]   And this one returns float tensor.
[01:59:18.460 --> 01:59:20.940]   Why long tensor?
[01:59:20.940 --> 01:59:25.340]   Because the input IDs are usually numbers.
[01:59:25.340 --> 01:59:28.460]   That indicate the position of each token inside of the vocabulary.
[01:59:28.460 --> 01:59:29.420]   Also this concept.
[01:59:29.420 --> 01:59:30.460]   Please if it's not clear.
[01:59:30.460 --> 01:59:31.980]   Go watch my previous video.
[01:59:31.980 --> 01:59:32.540]   About the transformer.
[01:59:32.540 --> 01:59:34.460]   Because it's very clear there.
[01:59:34.460 --> 01:59:36.540]   When we work with the textual models.
[01:59:36.540 --> 01:59:44.140]   Okay.
[01:59:44.140 --> 01:59:46.140]   First we convert each token into embeddings.
[01:59:46.140 --> 01:59:52.380]   And then.
[01:59:52.380 --> 01:59:54.300]   So what is the size here?
[01:59:54.300 --> 01:59:55.740]   We are going from batch size.
[01:59:55.740 --> 01:59:58.620]   Sequence length into.
[02:00:00.620 --> 02:00:03.260]   Batch size.
[02:00:03.260 --> 02:00:04.460]   Sequence length.
[02:00:04.460 --> 02:00:05.100]   And dimension.
[02:00:05.100 --> 02:00:07.260]   Where the dimension is 768.
[02:00:07.260 --> 02:00:09.580]   Then we apply one after.
[02:00:09.580 --> 02:00:12.220]   One after another.
[02:00:12.220 --> 02:00:13.740]   All the layers of this encoder.
[02:00:13.740 --> 02:00:18.060]   Just like in the transformer model.
[02:00:18.060 --> 02:00:24.540]   And the last one we apply the layer normalization.
[02:00:29.820 --> 02:00:30.320]   Oh.
[02:00:30.320 --> 02:00:36.140]   And finally we return the output.
[02:00:36.140 --> 02:00:37.100]   Where the output is.
[02:00:37.100 --> 02:00:41.660]   Of course it's a sequence to sequence model.
[02:00:41.660 --> 02:00:42.620]   Just like the transformer.
[02:00:42.620 --> 02:00:44.140]   So the input should match the.
[02:00:44.140 --> 02:00:47.740]   The shape of the input should match the shape of the output.
[02:00:47.740 --> 02:00:52.300]   So we always obtain sequence length by the model.
[02:00:52.300 --> 02:00:53.980]   Okay.
[02:00:53.980 --> 02:00:55.580]   Now let's define these two blocks.
[02:00:55.580 --> 02:00:57.340]   The first one is the clip embedding.
[02:00:57.340 --> 02:00:59.020]   So let's go.
[02:00:59.660 --> 02:01:06.300]   Clip embedding.
[02:01:06.300 --> 02:01:10.220]   How much is the vocabulary size?
[02:01:10.220 --> 02:01:12.860]   What is the embedding size?
[02:01:12.860 --> 02:01:20.060]   And number of token.
[02:01:20.060 --> 02:01:21.660]   Okay.
[02:01:21.660 --> 02:01:23.100]   So the sequence length basically.
[02:01:28.060 --> 02:01:28.560]   And.
[02:01:28.560 --> 02:01:36.060]   Okay.
[02:01:36.060 --> 02:01:38.380]   We define the embedding itself.
[02:01:38.380 --> 02:01:39.740]   Using nn.embedding.
[02:01:39.740 --> 02:01:40.540]   Just like always.
[02:01:40.540 --> 02:01:49.740]   We need to tell him what is the number of embeddings.
[02:01:49.740 --> 02:01:51.260]   So the vocabulary size.
[02:01:51.260 --> 02:01:54.540]   And what is the dimension of each vector of the embedding token.
[02:01:54.540 --> 02:01:57.420]   Then we define some positional encoding.
[02:01:57.420 --> 02:01:59.580]   So now as you remember.
[02:01:59.580 --> 02:02:01.580]   The positional encoding in the original transformer.
[02:02:01.580 --> 02:02:04.300]   Are given by sinusoidal functions.
[02:02:04.300 --> 02:02:06.140]   But here in clip.
[02:02:06.140 --> 02:02:07.500]   They actually don't use them.
[02:02:07.500 --> 02:02:10.380]   They use some learned parameters.
[02:02:10.380 --> 02:02:13.260]   So they have these parameters.
[02:02:13.260 --> 02:02:15.980]   That are learned by the model during training.
[02:02:15.980 --> 02:02:18.700]   That tell the position of the token to the model.
[02:02:23.740 --> 02:02:27.740]   Tokens and embeddings.
[02:02:27.740 --> 02:02:29.260]   Like this.
[02:02:29.260 --> 02:02:39.180]   We apply them.
[02:02:39.180 --> 02:02:40.620]   So first we apply the embedding.
[02:02:40.620 --> 02:02:41.500]   So we go from.
[02:02:41.500 --> 02:02:43.340]   Patch size.
[02:02:43.340 --> 02:02:46.460]   Sequence length.
[02:02:46.460 --> 02:02:46.960]   To.
[02:02:46.960 --> 02:02:49.900]   Patch size.
[02:02:49.900 --> 02:02:51.180]   Sequence length.
[02:02:51.180 --> 02:02:51.820]   Dimension.
[02:02:51.980 --> 02:03:02.460]   And then just like in the original transformer.
[02:03:02.460 --> 02:03:07.820]   We add the positional encodings to each token.
[02:03:07.820 --> 02:03:09.180]   But in this case as I told you.
[02:03:09.180 --> 02:03:12.620]   The positional embeddings are not fixed.
[02:03:12.620 --> 02:03:14.380]   Like not sinusoidal functions.
[02:03:14.380 --> 02:03:16.460]   But they are learned by the model.
[02:03:16.460 --> 02:03:17.260]   So they are learned.
[02:03:17.260 --> 02:03:19.740]   And then later we will load these parameters.
[02:03:19.740 --> 02:03:20.780]   When we load the model.
[02:03:21.020 --> 02:03:23.420]   And then we return this x.
[02:03:23.420 --> 02:03:26.220]   Then we have the clip layer.
[02:03:26.220 --> 02:03:29.100]   Which is just like the layer of the transformer model.
[02:03:29.100 --> 02:03:43.420]   The encoder of the transformer model.
[02:03:43.420 --> 02:03:47.420]   So it returns nothing actually.
[02:03:47.420 --> 02:04:01.100]   And this one is wrong in it.
[02:04:01.100 --> 02:04:03.600]   Okay.
[02:04:03.600 --> 02:04:06.620]   We have just like in the transformer block.
[02:04:06.620 --> 02:04:08.860]   We have the pre norm.
[02:04:08.860 --> 02:04:09.820]   Then we have the attention.
[02:04:09.820 --> 02:04:11.020]   Then we have a post norm.
[02:04:11.020 --> 02:04:12.300]   And then we have the feed forward.
[02:04:12.300 --> 02:04:16.380]   So layer normalization.
[02:04:16.860 --> 02:04:27.900]   Then we have the attention.
[02:04:27.900 --> 02:04:31.980]   Which is a self attention.
[02:04:31.980 --> 02:04:37.580]   Later we will build the cross attention.
[02:04:37.580 --> 02:04:38.780]   And I will show you what is it.
[02:04:38.780 --> 02:04:43.740]   Then we have another layer normalization.
[02:04:43.820 --> 02:05:03.260]   Then we have two feed forward layers.
[02:05:03.260 --> 02:05:16.780]   And finally we have the forward method.
[02:05:16.780 --> 02:05:17.260]   Finally.
[02:05:17.260 --> 02:05:23.500]   So this one takes tensor.
[02:05:23.500 --> 02:05:24.700]   And returns a tensor.
[02:05:24.700 --> 02:05:25.660]   So let me write it.
[02:05:25.660 --> 02:05:27.500]   Tensor.
[02:05:27.500 --> 02:05:30.960]   Okay.
[02:05:32.300 --> 02:05:33.740]   Just like the transformer model.
[02:05:33.740 --> 02:05:35.020]   Okay let's go have a look.
[02:05:35.020 --> 02:05:37.740]   We have a bunch of residual connections.
[02:05:37.740 --> 02:05:38.460]   As you can see here.
[02:05:38.460 --> 02:05:39.580]   One residual connection here.
[02:05:39.580 --> 02:05:40.780]   One residual connection here.
[02:05:40.780 --> 02:05:41.980]   We have two normalizations.
[02:05:41.980 --> 02:05:42.460]   One here.
[02:05:42.460 --> 02:05:42.960]   One here.
[02:05:42.960 --> 02:05:44.460]   The feed forward.
[02:05:44.460 --> 02:05:46.220]   As just like in the original transformer.
[02:05:46.220 --> 02:05:48.780]   We have two linear layers.
[02:05:48.780 --> 02:05:50.700]   And then we have this multi head attention.
[02:05:50.700 --> 02:05:51.980]   Which is actually a self attention.
[02:05:51.980 --> 02:05:55.100]   Because it's the same input that becomes query key and values.
[02:05:55.100 --> 02:05:57.580]   So let's do it.
[02:05:57.580 --> 02:06:00.940]   The first residual connection x.
[02:06:00.940 --> 02:06:03.260]   So what is the input of this forward method?
[02:06:03.260 --> 02:06:04.300]   It's a batch size.
[02:06:04.300 --> 02:06:07.580]   Sequence length d mod.
[02:06:07.580 --> 02:06:11.980]   And the dimension of the embedding which is 768.
[02:06:11.980 --> 02:06:15.260]   The first thing we do is we apply the self attention.
[02:06:15.260 --> 02:06:18.220]   But before applying the self attention.
[02:06:18.220 --> 02:06:20.300]   We apply the layer normalization.
[02:06:20.300 --> 02:06:23.100]   So layer normal 1.
[02:06:23.100 --> 02:06:25.820]   Then we apply the attention.
[02:06:28.940 --> 02:06:30.220]   But with the causal mask.
[02:06:30.220 --> 02:06:36.780]   As you remember here.
[02:06:36.780 --> 02:06:37.580]   Self attention.
[02:06:37.580 --> 02:06:39.180]   We have the causal mask.
[02:06:39.180 --> 02:06:42.940]   Which basically means that every token cannot watch the next tokens.
[02:06:42.940 --> 02:06:44.780]   So cannot be related to future tokens.
[02:06:44.780 --> 02:06:46.860]   But only the one on the left of it.
[02:06:46.860 --> 02:06:49.900]   And this is what we want from a text model actually.
[02:06:49.900 --> 02:06:53.900]   We don't want the one word to watch the words that come after it.
[02:06:53.900 --> 02:06:55.820]   But only the words that come before it.
[02:06:55.820 --> 02:06:58.780]   Then we do this residual connection.
[02:06:59.020 --> 02:06:59.820]   So now we are.
[02:06:59.820 --> 02:07:04.380]   Now we are doing this connection here.
[02:07:04.380 --> 02:07:10.540]   Then we do the feed forward layer.
[02:07:10.540 --> 02:07:14.460]   Again we have a residual connection.
[02:07:14.460 --> 02:07:18.300]   We apply the normalization.
[02:07:18.300 --> 02:07:24.220]   I'm not writing all the shapes.
[02:07:24.220 --> 02:07:25.660]   If you watch my code online.
[02:07:25.660 --> 02:07:27.340]   I have written all of them.
[02:07:27.340 --> 02:07:28.780]   But mostly to save time.
[02:07:28.780 --> 02:07:32.940]   Because here we are already familiar with the structure of the transformer.
[02:07:32.940 --> 02:07:33.740]   Hopefully.
[02:07:33.740 --> 02:07:35.900]   So I am not repeating all the shapes here.
[02:07:35.900 --> 02:07:43.900]   We apply the first linear of the feed forward.
[02:07:43.900 --> 02:07:47.980]   Then as activation function.
[02:07:47.980 --> 02:07:49.740]   We use the GLUE function.
[02:07:49.740 --> 02:07:53.100]   And actually we call the quick GLUE function.
[02:07:53.100 --> 02:07:54.380]   Which is defined like this.
[02:07:56.540 --> 02:07:59.980]   X multiplied by torch dot sigmoid.
[02:07:59.980 --> 02:08:05.100]   Of 1.702 multiplied by x.
[02:08:05.100 --> 02:08:09.260]   And that's it.
[02:08:09.260 --> 02:08:10.140]   Should be like this.
[02:08:10.140 --> 02:08:16.940]   So this is called the quick GLUE activation function.
[02:08:16.940 --> 02:08:17.500]   Also here.
[02:08:17.500 --> 02:08:23.420]   There is no justification on why we should use this one and not another one.
[02:08:23.420 --> 02:08:27.100]   They just saw that in practice this one works better for this kind of application.
[02:08:27.100 --> 02:08:29.340]   So that's why we are using this function here.
[02:08:29.340 --> 02:08:31.100]   So now.
[02:08:31.100 --> 02:08:37.900]   And then we apply the residual connection.
[02:08:37.900 --> 02:08:40.380]   And finally we return x.
[02:08:40.380 --> 02:08:44.940]   This is exactly like the feed forward layer of the transformer.
[02:08:44.940 --> 02:08:47.580]   Except that in the transformer we don't have this activation function.
[02:08:47.580 --> 02:08:48.780]   But we have the RELU function.
[02:08:48.780 --> 02:08:52.460]   And if you remember in LLAMA we don't have the RELU function.
[02:08:52.460 --> 02:08:54.300]   We have the ZWIGLUE function.
[02:08:54.300 --> 02:08:56.940]   But here we are using the quick GLUE function.
[02:08:56.940 --> 02:08:58.780]   Which I actually am not so familiar with.
[02:08:58.780 --> 02:09:02.940]   But I think that it works good for this model.
[02:09:02.940 --> 02:09:03.820]   And they just kept it.
[02:09:03.820 --> 02:09:08.060]   So now we have built our text encoder here.
[02:09:08.060 --> 02:09:08.460]   CLIP.
[02:09:08.460 --> 02:09:09.980]   Which is very small as you can see.
[02:09:09.980 --> 02:09:15.900]   And our next thing to build is our unit.
[02:09:15.900 --> 02:09:18.780]   So we have built the variational autoencoder.
[02:09:18.780 --> 02:09:19.900]   The encoder part.
[02:09:19.900 --> 02:09:21.580]   And the decoder part.
[02:09:21.580 --> 02:09:25.340]   Now the next thing we have to build is this unit.
[02:09:25.340 --> 02:09:31.900]   As you remember the unit is the network that will give some noisified image.
[02:09:31.900 --> 02:09:33.420]   And the amount.
[02:09:33.420 --> 02:09:38.940]   And we also indicated to the network what is the amount of noise that we added to this image.
[02:09:38.940 --> 02:09:43.740]   The model has to predict how much noise is there.
[02:09:43.740 --> 02:09:44.700]   And how to remove it.
[02:09:44.700 --> 02:09:49.660]   And this unit is a bunch of convolutions.
[02:09:49.660 --> 02:09:52.060]   That will reduce the size of the image.
[02:09:52.060 --> 02:09:52.700]   As you can see.
[02:09:52.700 --> 02:09:54.620]   With each step.
[02:09:54.620 --> 02:09:58.540]   But by increasing the number of features.
[02:09:58.540 --> 02:09:59.580]   So we reduce the size.
[02:09:59.580 --> 02:10:04.700]   But we increase exactly what we did in the encoder of the variational autoencoder.
[02:10:04.700 --> 02:10:07.740]   And then we do the reverse steps.
[02:10:07.740 --> 02:10:10.540]   Just like we did with the decoder of the variational autoencoder.
[02:10:10.540 --> 02:10:13.900]   So now again we will work with some convolutions.
[02:10:13.900 --> 02:10:15.100]   With the residual blocks.
[02:10:15.100 --> 02:10:16.060]   With attentions.
[02:10:16.060 --> 02:10:16.560]   Etc.
[02:10:17.260 --> 02:10:21.100]   The one big difference is that we need to tell our unit.
[02:10:21.100 --> 02:10:24.060]   Not only the image that is already.
[02:10:24.060 --> 02:10:26.620]   So what is the image with noise.
[02:10:26.620 --> 02:10:29.500]   Not only the amount of noise.
[02:10:29.500 --> 02:10:32.540]   So the time step at which this noise was added.
[02:10:32.540 --> 02:10:34.540]   But also the prompt.
[02:10:34.540 --> 02:10:40.060]   Because as you remember we need to also tell this unit what is our prompt.
[02:10:40.060 --> 02:10:44.140]   Because we need to tell him how we want our output image to be.
[02:10:44.940 --> 02:10:47.660]   Because there are many ways to deny the initial noise.
[02:10:47.660 --> 02:10:50.460]   So if we want the initial noise to become a dog.
[02:10:50.460 --> 02:10:52.140]   We need to tell him we want a dog.
[02:10:52.140 --> 02:10:53.900]   If we want the initial noise to become a cat.
[02:10:53.900 --> 02:10:56.140]   We need to tell him we want a cat.
[02:10:56.140 --> 02:10:58.620]   So the unit has to know what is the prompt.
[02:10:58.620 --> 02:11:03.180]   And also he has to relate this prompt with the rest of the information.
[02:11:03.180 --> 02:11:07.580]   And what is the best way to combine two different stuff.
[02:11:07.580 --> 02:11:10.220]   So for example an image with text.
[02:11:10.220 --> 02:11:13.660]   We will use what is called the cross attention.
[02:11:13.660 --> 02:11:18.460]   Cross attention basically allows us to calculate the attention between two sequences.
[02:11:18.460 --> 02:11:21.980]   In which the query is the first sequence.
[02:11:21.980 --> 02:11:25.340]   And the keys and the values are coming from another sequence.
[02:11:25.340 --> 02:11:28.060]   So let's go build it and let's see how this works.
[02:11:28.060 --> 02:11:33.740]   Now the first thing we will do is create a new class.
[02:11:33.740 --> 02:11:36.140]   New file here called diffusion.
[02:11:36.140 --> 02:11:38.780]   Because this will be our diffusion model.
[02:11:39.900 --> 02:11:44.860]   And I think also here I will build from top down.
[02:11:44.860 --> 02:11:47.820]   So we first define the diffusion class.
[02:11:47.820 --> 02:11:49.740]   And then we build each block one by one.
[02:11:49.740 --> 02:11:53.900]   Let's start by importing the usual libraries.
[02:11:53.900 --> 02:11:55.180]   So import torch.
[02:12:03.100 --> 02:12:10.380]   From torch.
[02:12:10.380 --> 02:12:15.420]   And then we import the attention.
[02:12:15.420 --> 02:12:19.340]   The self attention.
[02:12:19.340 --> 02:12:22.460]   But also we will need the cross attention.
[02:12:22.460 --> 02:12:24.620]   Attention.
[02:12:24.620 --> 02:12:25.740]   And later we will build it.
[02:12:25.740 --> 02:12:30.140]   Then let's create the class diffusion.
[02:12:32.940 --> 02:12:34.860]   The class diffusion is basically our unit.
[02:12:34.860 --> 02:12:46.140]   This is made of time embedding.
[02:12:46.140 --> 02:12:50.780]   So something that we will define it later.
[02:12:50.780 --> 02:12:52.220]   Time embedding.
[02:12:52.220 --> 02:12:56.140]   320 which is the size of the time embedding.
[02:12:56.140 --> 02:13:00.700]   So because we need to give the unit not only the noisified image.
[02:13:00.700 --> 02:13:03.500]   But also the time step at which it was noisified.
[02:13:03.500 --> 02:13:10.300]   So the image, the unit needs some way to understand this time step.
[02:13:10.300 --> 02:13:12.540]   So this is why this time step which is a number.
[02:13:12.540 --> 02:13:14.620]   Will be converted into an embedding.
[02:13:14.620 --> 02:13:17.260]   By using this particular module called time embedding.
[02:13:17.260 --> 02:13:18.220]   And later we will see it.
[02:13:18.220 --> 02:13:21.340]   Then we build the unit.
[02:13:21.340 --> 02:13:27.180]   And then the output layer of the unit.
[02:13:27.180 --> 02:13:29.660]   And later we will see what is it.
[02:13:29.660 --> 02:13:30.620]   This output layer.
[02:13:30.620 --> 02:13:33.500]   Put layer.
[02:13:33.500 --> 02:13:37.100]   Later we will see how to build it.
[02:13:37.100 --> 02:13:39.020]   Let's do the forward.
[02:13:39.020 --> 02:13:45.740]   As you remember the unit will receive the latent.
[02:13:45.740 --> 02:13:48.220]   So this Z which is a latent.
[02:13:48.220 --> 02:13:50.460]   Is the output of the variational autoencoder.
[02:13:50.460 --> 02:13:53.020]   So this latent which is a torch dot tensor.
[02:13:53.020 --> 02:13:55.260]   It will receive the context.
[02:13:55.260 --> 02:13:56.380]   What is the context?
[02:13:56.380 --> 02:13:57.340]   Is our prompt.
[02:13:57.340 --> 02:14:00.620]   Which is also a torch dot tensor.
[02:14:00.620 --> 02:14:02.460]   And it will receive the time.
[02:14:02.460 --> 02:14:04.700]   At which this latent was noisified.
[02:14:04.700 --> 02:14:07.020]   Which is also.
[02:14:07.020 --> 02:14:07.740]   I don't remember.
[02:14:07.740 --> 02:14:09.660]   I think it's a tensor also.
[02:14:09.660 --> 02:14:12.860]   Later I define it.
[02:14:12.860 --> 02:14:15.580]   Okay yeah it's tensor.
[02:14:15.580 --> 02:14:21.420]   Okay let's define the sizes.
[02:14:21.420 --> 02:14:24.860]   So the latent here is batch size.
[02:14:25.500 --> 02:14:28.380]   4 because 4 is the output of the encoder.
[02:14:28.380 --> 02:14:29.740]   If you remember correctly here.
[02:14:29.740 --> 02:14:30.780]   4.
[02:14:30.780 --> 02:14:33.500]   Closing.
[02:14:33.500 --> 02:14:33.740]   Okay.
[02:14:33.740 --> 02:14:38.620]   Grid and width divided by 8.
[02:14:38.620 --> 02:14:41.180]   Then we have the context.
[02:14:41.180 --> 02:14:42.300]   Which is our prompt.
[02:14:42.300 --> 02:14:46.540]   Which we already converted with the clip encoder here.
[02:14:46.540 --> 02:14:48.940]   Which will be batch size.
[02:14:48.940 --> 02:14:50.140]   By sequence length.
[02:14:50.140 --> 02:14:50.860]   By dimension.
[02:14:50.860 --> 02:14:53.020]   Where the dimension is 768.
[02:14:53.020 --> 02:14:54.540]   Like we defined before.
[02:14:54.540 --> 02:14:55.820]   And the time will be another.
[02:14:55.820 --> 02:14:58.460]   We will define it later.
[02:14:58.460 --> 02:14:59.580]   How it's defined.
[02:14:59.580 --> 02:15:00.300]   How it's built.
[02:15:00.300 --> 02:15:03.020]   But it's each embedding.
[02:15:03.020 --> 02:15:05.180]   It's a number with an embedding of size.
[02:15:05.180 --> 02:15:07.820]   It's a vector of a size of 320.
[02:15:07.820 --> 02:15:11.260]   The first thing we do is.
[02:15:11.260 --> 02:15:13.180]   We convert this time into an embedding.
[02:15:13.180 --> 02:15:17.020]   And actually this time.
[02:15:17.020 --> 02:15:17.820]   We will see later.
[02:15:17.820 --> 02:15:18.700]   That it's actually.
[02:15:18.700 --> 02:15:20.940]   Just like the positional encoding.
[02:15:20.940 --> 02:15:23.180]   Of the transformer model.
[02:15:23.180 --> 02:15:26.540]   It's actually a number that is multiplied by.
[02:15:26.540 --> 02:15:28.860]   Sines and cosines.
[02:15:28.860 --> 02:15:30.940]   Just like in the transformer.
[02:15:30.940 --> 02:15:33.500]   Because they saw that it works for the transformer.
[02:15:33.500 --> 02:15:35.820]   So we can also use the same positional encoding.
[02:15:35.820 --> 02:15:37.980]   To convey the information of the time.
[02:15:37.980 --> 02:15:39.980]   Which is actually kind of an information.
[02:15:39.980 --> 02:15:40.780]   About position.
[02:15:40.780 --> 02:15:42.300]   So it tells the model.
[02:15:42.300 --> 02:15:45.580]   At which step we arrived in the denoisification.
[02:15:48.460 --> 02:15:53.260]   So this one will convert tensor of one 320.
[02:15:53.260 --> 02:15:57.740]   Into a tensor of one one two eight zero one thousand.
[02:15:57.740 --> 02:16:03.180]   The unit will convert our latent.
[02:16:03.180 --> 02:16:04.620]   Into another latent.
[02:16:04.620 --> 02:16:07.820]   So it will not change the size.
[02:16:11.500 --> 02:16:18.780]   Batch for height this is the output.
[02:16:18.780 --> 02:16:20.140]   Of the variation of the encoder.
[02:16:20.140 --> 02:16:25.340]   Which first becomes batch 320 features.
[02:16:30.060 --> 02:16:39.900]   Through this the unit so.
[02:16:39.900 --> 02:16:45.020]   So why here we have more features.
[02:16:45.020 --> 02:16:45.900]   Than the starting.
[02:16:45.900 --> 02:16:47.420]   Because let's review here.
[02:16:47.420 --> 02:16:48.140]   As you can see.
[02:16:48.140 --> 02:16:50.940]   The last layer of the unit.
[02:16:50.940 --> 02:16:53.660]   Actually we need to go back.
[02:16:53.660 --> 02:16:57.340]   To the same number of the features.
[02:16:57.340 --> 02:16:58.060]   You can see here.
[02:16:58.060 --> 02:16:58.860]   So here we start.
[02:16:59.820 --> 02:17:01.260]   Actually the dimensions here.
[02:17:01.260 --> 02:17:02.620]   Don't match what we will be using.
[02:17:02.620 --> 02:17:04.300]   So this is the original unit.
[02:17:04.300 --> 02:17:05.180]   But the one used.
[02:17:05.180 --> 02:17:08.380]   By stable diffusion is a modified unit.
[02:17:08.380 --> 02:17:09.580]   So in the last.
[02:17:09.580 --> 02:17:11.260]   When we build the decoder.
[02:17:11.260 --> 02:17:12.700]   The decoder will not build.
[02:17:12.700 --> 02:17:15.100]   The final number of features that we need.
[02:17:15.100 --> 02:17:16.140]   Which is four.
[02:17:16.140 --> 02:17:18.060]   But we need an additional output layer.
[02:17:18.060 --> 02:17:20.700]   To go back to the original size of features.
[02:17:20.700 --> 02:17:23.100]   And this is the job of this output layer.
[02:17:23.100 --> 02:17:25.580]   So later we will see.
[02:17:25.580 --> 02:17:28.140]   When we build this this layer.
[02:17:28.140 --> 02:17:30.860]   So output is equal to self dot final.
[02:17:30.860 --> 02:17:35.580]   This one will go from this size here.
[02:17:35.580 --> 02:17:42.540]   To back to the original size of the unit.
[02:17:42.540 --> 02:17:44.940]   Because the unit.
[02:17:44.940 --> 02:17:47.900]   His job is to take in latents.
[02:17:47.900 --> 02:17:49.900]   Predict how much noise is it.
[02:17:49.900 --> 02:17:51.900]   Then take again the same latent.
[02:17:51.900 --> 02:17:53.020]   Predict how much noise.
[02:17:53.020 --> 02:17:54.140]   We remove it.
[02:17:54.140 --> 02:17:55.020]   We remove the noise.
[02:17:55.020 --> 02:17:57.020]   Then again we give another latent.
[02:17:57.020 --> 02:17:58.220]   We predict how much noise.
[02:17:58.220 --> 02:17:59.180]   We remove the noise.
[02:17:59.180 --> 02:18:01.100]   We give another latent.
[02:18:01.100 --> 02:18:01.900]   We predict the noise.
[02:18:01.900 --> 02:18:02.620]   We remove the noise.
[02:18:02.620 --> 02:18:04.060]   Etc, etc, etc.
[02:18:04.060 --> 02:18:06.460]   So the output dimension must match the input dimension.
[02:18:06.460 --> 02:18:09.660]   And then we return the output.
[02:18:09.660 --> 02:18:13.100]   Which is the latent.
[02:18:13.100 --> 02:18:15.360]   Like this.
[02:18:15.360 --> 02:18:18.540]   Let's build first the time embedding.
[02:18:18.540 --> 02:18:19.740]   I think it's easy to build.
[02:18:19.740 --> 02:18:23.580]   So something that encodes information.
[02:18:23.580 --> 02:18:25.340]   About the time step in which we are.
[02:18:26.060 --> 02:18:28.940]   [TYPING SOUNDS]
[02:18:28.940 --> 02:18:40.320]   Okay.
[02:18:40.320 --> 02:18:45.740]   It is made of two linear layers.
[02:18:45.740 --> 02:18:46.860]   Nothing fancy here.
[02:18:46.860 --> 02:18:48.940]   Linear one.
[02:18:54.460 --> 02:18:58.060]   Which will map it to 4 by n embedding.
[02:18:58.060 --> 02:18:59.900]   And then linear two.
[02:18:59.900 --> 02:19:11.180]   4 by n embedding into 4 by n embedding.
[02:19:11.180 --> 02:19:15.180]   And now you understand why it becomes 1280.
[02:19:15.180 --> 02:19:17.580]   Which is 4 times 320.
[02:19:18.300 --> 02:19:21.340]   [TYPING SOUNDS]
[02:19:21.340 --> 02:19:27.580]   This one returns to a tensor.
[02:19:27.580 --> 02:19:32.300]   So the input size is 1320.
[02:19:32.300 --> 02:19:36.380]   What we do is first we apply this first layer.
[02:19:36.380 --> 02:19:38.300]   Linear one.
[02:19:38.300 --> 02:19:40.700]   Then we apply the silo function.
[02:19:40.700 --> 02:19:45.900]   Then we apply again the second linear layer.
[02:19:45.900 --> 02:19:49.900]   [TYPING SOUNDS]
[02:19:49.900 --> 02:19:51.100]   And then we return it.
[02:19:51.100 --> 02:19:52.540]   Nothing special here.
[02:19:52.540 --> 02:19:58.220]   The output dimension is 1 by 1280.
[02:19:58.220 --> 02:20:01.020]   [TYPING SOUNDS]
[02:20:01.020 --> 02:20:02.220]   280.
[02:20:02.220 --> 02:20:04.560]   Okay.
[02:20:04.560 --> 02:20:06.860]   The next thing we need to build is the unit.
[02:20:06.860 --> 02:20:10.860]   The unit will require many blocks.
[02:20:10.860 --> 02:20:13.660]   So let's first build the unit itself.
[02:20:13.660 --> 02:20:16.220]   And then we build each of the blocks that it will require.
[02:20:16.220 --> 02:20:19.500]   So class unit.
[02:20:19.500 --> 02:20:36.380]   [TYPING SOUNDS]
[02:20:36.380 --> 02:20:40.060]   As you can see, the unit is made up of one encoder branch.
[02:20:40.060 --> 02:20:43.580]   So this is like the encoder of the variational autoencoder.
[02:20:44.140 --> 02:20:45.340]   Things go down.
[02:20:45.340 --> 02:20:47.500]   So the image becomes smaller, smaller, smaller.
[02:20:47.500 --> 02:20:49.420]   But the channels keep increasing.
[02:20:49.420 --> 02:20:51.020]   The features keep increasing.
[02:20:51.020 --> 02:20:53.500]   Then we have this bottleneck layer here.
[02:20:53.500 --> 02:20:54.860]   It's called bottleneck.
[02:20:54.860 --> 02:20:57.180]   And then we have a decoder part here.
[02:20:57.180 --> 02:20:58.860]   So it becomes original size.
[02:20:58.860 --> 02:21:02.700]   The image from the very small size becomes the original size.
[02:21:02.700 --> 02:21:07.020]   And then we have these skip connections between the encoder and the decoder.
[02:21:07.020 --> 02:21:12.460]   So the output of each layer of each step of the encoder
[02:21:12.460 --> 02:21:15.820]   is connected to the same step of the decoder on the other side.
[02:21:15.820 --> 02:21:19.500]   And you will see this one here.
[02:21:19.500 --> 02:21:22.380]   So we start building the left side, which is the encoders.
[02:21:22.380 --> 02:21:25.100]   Which is a list of modules.
[02:21:25.100 --> 02:21:37.980]   And to build these encoders, we need to define a special layer, basically, that will apply...
[02:21:37.980 --> 02:21:40.940]   Okay, let's build it and then I will describe it.
[02:21:40.940 --> 02:21:48.060]   SwitchSequential.
[02:21:48.060 --> 02:22:05.340]   And basically, this switchSequential, given a list of layers, will apply them one by one.
[02:22:05.340 --> 02:22:07.580]   So we can think of it as a sequential.
[02:22:07.580 --> 02:22:14.860]   But it can recognize what are the parameters of each of them and will apply accordingly.
[02:22:14.860 --> 02:22:17.180]   So after I define it, it will be more clear.
[02:22:17.180 --> 02:22:20.300]   So first we have, just like before, a convolution.
[02:22:20.300 --> 02:22:23.100]   Because we want to increase the number of channels.
[02:22:23.100 --> 02:22:26.860]   So as you can see, at the beginning, we increase the number of channels of the image.
[02:22:26.860 --> 02:22:29.900]   Here it's 64, but we go directly to 320.
[02:22:29.900 --> 02:22:36.460]   And then we have another one of this switchSequential.
[02:22:36.540 --> 02:22:41.660]   Which is a unit residual block.
[02:22:41.660 --> 02:22:44.700]   We define it later.
[02:22:44.700 --> 02:22:50.460]   But it's very similar to the residual block that we built already for the variational autoencoder.
[02:22:50.460 --> 02:22:54.300]   And then we have an attention block, which is also very similar to the attention block
[02:22:54.300 --> 02:22:56.140]   that we built for the variational autoencoder.
[02:23:03.900 --> 02:23:07.740]   Then we have-- OK, I think it's better to build this switchSequential.
[02:23:07.740 --> 02:23:09.500]   Otherwise, we have too many-- yeah.
[02:23:09.500 --> 02:23:11.980]   Let's build it.
[02:23:11.980 --> 02:23:12.700]   It's very simple.
[02:23:12.700 --> 02:23:21.260]   As you can see, it's a sequence.
[02:23:21.260 --> 02:23:32.620]   But given x, which is our latent, which is a torch.tensor, our context, so our prompt.
[02:23:33.580 --> 02:23:36.540]   And the time, which is also a tensor.
[02:23:36.540 --> 02:23:42.380]   We'll apply them one by one.
[02:23:42.380 --> 02:23:49.340]   But based on what they are.
[02:23:49.340 --> 02:23:52.460]   So if the layer is a unit attention block, for example.
[02:23:52.460 --> 02:23:57.180]   It will apply it like this.
[02:23:57.180 --> 02:23:59.500]   So layer of x and context.
[02:23:59.500 --> 02:24:00.620]   Why?
[02:24:00.620 --> 02:24:04.060]   Because this attention block basically will compute the cross-attention between
[02:24:04.060 --> 02:24:05.980]   our latent and the prompt.
[02:24:05.980 --> 02:24:06.620]   This is why.
[02:24:06.620 --> 02:24:22.860]   This residual block will compute-- will match our latent with its time step.
[02:24:22.860 --> 02:24:26.620]   And then if it's any other layer, we just apply it.
[02:24:27.100 --> 02:24:34.300]   And then we return, but after the for a while.
[02:24:34.300 --> 02:24:34.540]   Yeah.
[02:24:34.540 --> 02:24:37.020]   So this is-- now we understood this.
[02:24:37.020 --> 02:24:40.220]   We just need to define this residual block and this attention block.
[02:24:40.220 --> 02:24:48.300]   Then we have another sequence-- sequential switch.
[02:24:48.300 --> 02:24:49.020]   This one here.
[02:24:49.020 --> 02:24:54.220]   So the code I'm writing actually is based on a repository.
[02:24:55.580 --> 02:24:59.100]   Upon which actually most of the code I wrote is based on.
[02:24:59.100 --> 02:25:01.180]   Which is in turn based on another repository,
[02:25:01.180 --> 02:25:04.460]   which was originally written for TensorFlow, if I remember correctly.
[02:25:04.460 --> 02:25:11.500]   So actually, the code for stable diffusion-- because it's a model that is built by
[02:25:11.500 --> 02:25:15.740]   Comfit's group at the LMU University, of course, it cannot be different from that code.
[02:25:15.740 --> 02:25:19.020]   So most of the code are actually similar to each other.
[02:25:19.020 --> 02:25:23.180]   I mean, you cannot create the same model and change the code.
[02:25:23.180 --> 02:25:24.620]   Of course, the code will be similar.
[02:25:25.500 --> 02:25:31.740]   So we again use this one-- switch sequential.
[02:25:31.740 --> 02:25:34.140]   So here we are building the encoder side.
[02:25:34.140 --> 02:25:46.940]   So we are reducing the size of the image.
[02:25:54.220 --> 02:25:56.220]   Let me check where we are.
[02:25:56.220 --> 02:26:00.940]   So we have the residual block of 320 to 64.
[02:26:00.940 --> 02:26:05.340]   And then we have an attention block of 8 to 80.
[02:26:05.340 --> 02:26:10.460]   And this attention block takes the number of head.
[02:26:10.460 --> 02:26:12.140]   This 8 indicates the number of head.
[02:26:12.140 --> 02:26:14.780]   And this indicates the embedding size.
[02:26:14.780 --> 02:26:19.260]   We will see later how we transform this, the output of this,
[02:26:19.260 --> 02:26:21.900]   into a sequence so that we can run attention on it.
[02:26:22.620 --> 02:26:26.460]   OK, we have this sequential.
[02:26:26.460 --> 02:26:29.500]   And then we have another one.
[02:26:29.500 --> 02:26:38.380]   Then we have another convolution.
[02:26:38.380 --> 02:26:42.140]   Let me just copy.
[02:26:42.140 --> 02:26:49.660]   Convolution of size from 640 to 640 channels.
[02:26:49.660 --> 02:26:51.900]   Kernel size 3, stride 2, padding 1.
[02:26:51.900 --> 02:26:59.260]   Then we have another residual block that will again increase the features.
[02:26:59.260 --> 02:27:05.020]   So from 640 to 1280.
[02:27:05.020 --> 02:27:14.700]   And then we have an attention block of 8 heads and 160 is the embedding size.
[02:27:15.820 --> 02:27:28.860]   Then we have another residual block of 1280 and 8 and 160.
[02:27:28.860 --> 02:27:31.980]   So as you can see, just like in the encoder of the variational autoencoder,
[02:27:31.980 --> 02:27:38.940]   we, with these convolutions, we keep decreasing the size of the image.
[02:27:38.940 --> 02:27:42.620]   So actually here we started with the latent representation,
[02:27:42.620 --> 02:27:47.180]   which was height divided by 8 and height divided by 8.
[02:27:47.180 --> 02:27:49.100]   So let me write some shapes here.
[02:27:49.100 --> 02:27:52.700]   At least you need to understand the size changes.
[02:27:52.700 --> 02:27:58.940]   So batch size for height divided by 8 and width divided by 8.
[02:27:58.940 --> 02:28:04.460]   When we apply this convolution, it will become divided by 16.
[02:28:04.460 --> 02:28:13.100]   So it will become divided by 16.
[02:28:13.100 --> 02:28:17.020]   So it will become a very small image.
[02:28:17.020 --> 02:28:22.380]   And after we apply the second one, it will become divided by 32.
[02:28:22.380 --> 02:28:27.100]   So here we start from 16.
[02:28:27.100 --> 02:28:31.660]   Here it will become divided by 32.
[02:28:32.380 --> 02:28:34.060]   So what does it mean divided by 32?
[02:28:34.060 --> 02:28:40.220]   That if the initial image was of size 512, the latent is of size 64 by 64.
[02:28:40.220 --> 02:28:43.340]   Then it becomes 32 by 32.
[02:28:43.340 --> 02:28:45.580]   Now it has become 16 by 16.
[02:28:45.580 --> 02:28:51.020]   And then we apply these residual connections.
[02:28:51.020 --> 02:28:57.820]   And then we apply another convolutional layer,
[02:28:57.820 --> 02:29:01.420]   which will reduce the size of the image further.
[02:29:01.660 --> 02:29:12.460]   So from 32 here, divide by 32 and divide by 32 to divide by 64.
[02:29:12.460 --> 02:29:18.140]   Every time we divided the size of the image by 2.
[02:29:18.140 --> 02:29:25.500]   And the number of features is 12801280.
[02:29:25.500 --> 02:29:34.940]   And then we have a unit residual block.
[02:29:34.940 --> 02:29:36.460]   So let me copy also this one.
[02:29:36.460 --> 02:29:46.060]   Of 1280 and 1280.
[02:29:46.060 --> 02:29:49.740]   And then we have a last one, which is another one of the same size.
[02:29:51.740 --> 02:29:57.340]   So now we have an image that is 64 divided by 64 and divided by 64,
[02:29:57.340 --> 02:29:59.580]   but with much more channels.
[02:29:59.580 --> 02:30:02.620]   I forgot to change the channel numbers here.
[02:30:02.620 --> 02:30:07.020]   So here is 1280 channels and divided by 64 divided by 64.
[02:30:07.020 --> 02:30:09.420]   And this one remains the same.
[02:30:09.420 --> 02:30:12.940]   Because the residual connections don't change the size.
[02:30:12.940 --> 02:30:19.660]   Here should be 1280 to 1280.
[02:30:20.860 --> 02:30:25.020]   Here should be 640 to 640.
[02:30:25.020 --> 02:30:31.020]   And here it should be 320 to 320.
[02:30:31.020 --> 02:30:35.900]   So as I said before, we keep reducing the size of the image,
[02:30:35.900 --> 02:30:40.620]   but we keep increasing this number of features of each pixel basically.
[02:30:40.620 --> 02:30:42.780]   Then we build the bottleneck,
[02:30:42.780 --> 02:30:49.260]   which is this part here of the unit.
[02:30:49.420 --> 02:30:57.420]   This is a sequence of a residual block.
[02:30:57.420 --> 02:31:06.940]   Then we have the attention block,
[02:31:06.940 --> 02:31:11.100]   which will make a self-attention.
[02:31:11.100 --> 02:31:14.380]   Sorry, not self-attention, cross-attention.
[02:31:14.380 --> 02:31:16.540]   And then we have another residual block.
[02:31:16.540 --> 02:31:24.140]   And then we have the decoder.
[02:31:24.140 --> 02:31:27.660]   So in the decoder, we will do the opposite of what we did in the encoder.
[02:31:27.660 --> 02:31:36.700]   So we will reduce the number of features, but increase the image size.
[02:31:36.700 --> 02:31:43.900]   Again, let's start with our beautiful switch sequential.
[02:31:44.860 --> 02:31:54.620]   So we have 2560 to 1280.
[02:31:54.620 --> 02:32:02.940]   Why here is 2560 even if after the bottleneck we have 1280?
[02:32:02.940 --> 02:32:06.460]   So we are talking about this part here.
[02:32:06.460 --> 02:32:11.420]   So after the input of the decoder,
[02:32:11.420 --> 02:32:16.060]   so this side here of the unit is the output of the bottleneck.
[02:32:16.060 --> 02:32:20.140]   But the bottleneck is outputting 1280 features,
[02:32:20.140 --> 02:32:25.180]   while the encoder is expecting 2560, so double the amount.
[02:32:25.180 --> 02:32:29.020]   Why? Because we need to consider that we have this skip connection here.
[02:32:29.020 --> 02:32:33.020]   So this skip connection will double the amount at each layer here.
[02:32:33.020 --> 02:32:36.540]   And this is why the input we expect here is double the size
[02:32:36.540 --> 02:32:38.460]   of what is the output of the previous layer.
[02:32:39.420 --> 02:32:40.940]   Let me write some shapes also here.
[02:32:40.940 --> 02:32:44.140]   So batch size 2560.
[02:32:44.140 --> 02:32:49.660]   The image is very small, so height by end width divided by 64.
[02:32:49.660 --> 02:32:59.260]   And it will become 1280.
[02:32:59.260 --> 02:33:08.300]   Then we apply another switch sequential of the same size.
[02:33:09.260 --> 02:33:12.060]   Then we apply another one with an upsample,
[02:33:12.060 --> 02:33:14.140]   just like we did in the variational autoencoder.
[02:33:14.140 --> 02:33:16.300]   So if you remember in the variational autoencoder,
[02:33:16.300 --> 02:33:19.660]   to increase the size of the image we do upsampling.
[02:33:19.660 --> 02:33:21.260]   And this is what we do exactly here.
[02:33:21.260 --> 02:33:23.420]   We do upsample.
[02:33:23.420 --> 02:33:28.780]   But this is not the upsample that we did exactly the same,
[02:33:28.780 --> 02:33:30.620]   but the concept is similar.
[02:33:30.620 --> 02:33:36.380]   And we will define it later also, this one.
[02:33:36.620 --> 02:33:39.100]   So we have another residual with attention.
[02:33:39.100 --> 02:33:42.140]   So we have a residual of 2000.
[02:33:42.140 --> 02:33:45.260]   And then we have an attention block.
[02:33:45.260 --> 02:33:52.780]   8 by 160.
[02:33:52.780 --> 02:33:57.020]   Then we have again this one.
[02:33:57.020 --> 02:34:02.860]   Then we have another one with an attention block.
[02:34:03.900 --> 02:34:08.220]   Then we have another one with upsampling.
[02:34:08.220 --> 02:34:12.300]   So we have 9020.
[02:34:12.300 --> 02:34:19.740]   And then we have an upsample.
[02:34:19.740 --> 02:34:28.700]   This one is small.
[02:34:28.700 --> 02:34:32.380]   So I know that I'm not writing all the shapes,
[02:34:32.380 --> 02:34:37.180]   but otherwise it's a really tiring job and very long.
[02:34:37.180 --> 02:34:41.100]   So just remember that we are keep increasing the size of the image,
[02:34:41.100 --> 02:34:44.380]   but we will decrease the number of features.
[02:34:44.380 --> 02:34:47.580]   Later we will see that this number here will become very small,
[02:34:47.580 --> 02:34:50.700]   and the size of the image will become nearly to the normal.
[02:34:50.700 --> 02:34:57.580]   Then we have another one with attention.
[02:34:57.580 --> 02:35:05.820]   So as you can see, we are decreasing the features here.
[02:35:05.820 --> 02:35:12.380]   Then we have 8 by 80, and we are increasing also here the size.
[02:35:12.380 --> 02:35:15.420]   Then we have another one.
[02:35:15.420 --> 02:35:23.180]   And 880.
[02:35:23.180 --> 02:35:26.060]   Then we have another one with upsampling.
[02:35:26.060 --> 02:35:28.140]   So we increase the size of the image.
[02:35:28.140 --> 02:35:34.540]   So 960 to 640.
[02:35:34.540 --> 02:35:39.100]   8 heads with the dimensions embedding size of 80,
[02:35:39.100 --> 02:35:43.100]   and the upsampling with 640 features.
[02:35:43.100 --> 02:35:47.580]   And then we have another residual block with attention.
[02:35:47.580 --> 02:35:58.620]   40.
[02:35:58.620 --> 02:36:09.580]   Then we have another one, which is a 640, 320, 840.
[02:36:09.580 --> 02:36:17.100]   And finally, the last one, we have 640 by 320.
[02:36:17.900 --> 02:36:19.980]   And 8 and 40.
[02:36:19.980 --> 02:36:28.540]   This dimension here is the same that will be applied by the output of the unit,
[02:36:28.540 --> 02:36:29.340]   as you can see here.
[02:36:29.340 --> 02:36:31.180]   This one here.
[02:36:31.180 --> 02:36:34.620]   And then we will give it to the final layer to build the original latent size.
[02:36:34.620 --> 02:36:42.380]   Okay, let's build all these blocks that we didn't build before.
[02:36:42.380 --> 02:36:44.060]   So first, let's build the upsample.
[02:36:45.900 --> 02:36:54.300]   Let's build it here, which is exactly the same as the two.
[02:36:54.300 --> 02:36:56.300]   Okay.
[02:36:56.300 --> 02:37:10.300]   We have this convolution.
[02:37:10.300 --> 02:37:18.300]   Without changing the number of features.
[02:37:18.300 --> 02:37:28.700]   And this is also doesn't change the size of the image, actually.
[02:37:28.700 --> 02:37:36.700]   So we will go from batch channels or features.
[02:37:36.700 --> 02:37:46.380]   Let's call it features height width to batch size features.
[02:37:46.380 --> 02:37:51.020]   Height multiplied by 2 and width multiplied by 2.
[02:37:51.020 --> 02:37:51.660]   Why?
[02:37:51.660 --> 02:37:54.300]   Because we are going to use the upsampling.
[02:37:54.300 --> 02:37:56.940]   This interpolation that we will do now.
[02:37:58.380 --> 02:38:08.860]   Interpolate x scale factor equal to mode is equal to nearest is the same operation that we did here.
[02:38:08.860 --> 02:38:10.940]   The same operation here.
[02:38:10.940 --> 02:38:14.540]   It will double the size, basically.
[02:38:14.540 --> 02:38:17.900]   And then we apply a convolution.
[02:38:17.900 --> 02:38:25.260]   Now, we have to define the final block.
[02:38:26.700 --> 02:38:30.380]   And we also have to define for the output layer.
[02:38:30.380 --> 02:38:33.260]   And we also have to define the attention block and the residual block.
[02:38:33.260 --> 02:38:36.380]   So let's build first this output layer.
[02:38:36.380 --> 02:38:47.580]   It's easier to build.
[02:38:47.580 --> 02:39:05.260]   So let's... this one also has a group normalization.
[02:39:05.260 --> 02:39:08.220]   Again, with the 32 size of the group 32.
[02:39:08.220 --> 02:39:12.460]   Also has a convolution.
[02:39:12.620 --> 02:39:20.780]   And the padding of 1.
[02:39:20.780 --> 02:39:25.840]   Okay.
[02:39:25.840 --> 02:39:31.500]   The final layer needs to convert this shape into this shape.
[02:39:31.500 --> 02:39:33.740]   So 320 features into 4.
[02:39:33.740 --> 02:39:42.060]   We have... so we have an input which is batch size of 320 features.
[02:39:42.060 --> 02:39:44.060]   The height is divided by 8.
[02:39:44.060 --> 02:39:45.820]   And the width is divided by 8.
[02:39:45.820 --> 02:39:48.940]   We first apply a group normalization.
[02:39:48.940 --> 02:39:54.860]   Then we apply the SILU.
[02:39:54.860 --> 02:40:00.060]   Then we apply the convolution.
[02:40:00.060 --> 02:40:02.540]   And then we return.
[02:40:02.540 --> 02:40:09.660]   This will basically... the convolution... let me write also why we are reducing the size.
[02:40:09.660 --> 02:40:13.180]   This convolution will change the number of channels from in to out.
[02:40:13.180 --> 02:40:18.700]   And when we will declare it, we say that we want to convert from 320 to 4 here.
[02:40:18.700 --> 02:40:23.020]   So this one will be of shape batch size 4.
[02:40:23.020 --> 02:40:25.100]   Height divided by 4.
[02:40:25.100 --> 02:40:28.060]   Height divided by 4.
[02:40:28.060 --> 02:40:30.620]   And width divided by 8.
[02:40:30.620 --> 02:40:39.500]   Then we need to go build this residual block and this attention block here.
[02:40:39.660 --> 02:40:40.860]   So let's build it here.
[02:40:40.860 --> 02:40:44.060]   Let's start with the residual block,
[02:40:44.060 --> 02:40:47.740]   which is very similar to the residual block that we built for the variational autoencoder.
[02:40:47.740 --> 02:41:00.940]   So unit lock.
[02:41:01.580 --> 02:41:16.220]   So this is the embedding of the time step.
[02:41:16.220 --> 02:41:22.940]   As you remember, with the time embedding, we transform into an embedding of size 1280.
[02:41:30.460 --> 02:41:32.460]   We have this group normalization.
[02:41:32.460 --> 02:41:33.660]   It's always this group norm.
[02:41:33.660 --> 02:41:51.980]   Then we have a convolution.
[02:41:57.500 --> 02:42:01.020]   And we have a linear for the time embedding.
[02:42:01.020 --> 02:42:11.020]   Then we have another group normalization.
[02:42:11.020 --> 02:42:15.020]   We will see later what is this merged.
[02:42:24.860 --> 02:42:30.780]   And another convolution.
[02:42:30.780 --> 02:42:37.500]   Oops, kernel size 3, embedding 1.
[02:42:37.500 --> 02:42:45.260]   Again, just like before, we have if the in channels is equal to the out channels,
[02:42:45.260 --> 02:42:47.740]   we can connect them directly with the residual connection.
[02:42:54.220 --> 02:42:57.180]   Otherwise, we create a convolution to connect them,
[02:42:57.180 --> 02:42:59.740]   to convert the size of the input into the output.
[02:42:59.740 --> 02:43:15.180]   Otherwise, we cannot add the two tensors.
[02:43:18.540 --> 02:43:27.500]   Zero, okay.
[02:43:27.500 --> 02:43:37.500]   So it takes in as input this feature tensor, which is actually the latent
[02:43:37.500 --> 02:43:42.300]   batch size in channels.
[02:43:42.300 --> 02:43:44.140]   Then we have height and width.
[02:43:45.100 --> 02:43:50.460]   And then also the time embedding, which is 1 by 1280, just like here.
[02:43:50.460 --> 02:43:55.180]   And we build, first of all, a residual connection.
[02:43:55.180 --> 02:44:01.580]   Then we do apply the group normalization.
[02:44:01.580 --> 02:44:05.500]   So usually the residual connection, the residual blocks are more or less always the same.
[02:44:05.500 --> 02:44:08.780]   So there is a normalization and activation function.
[02:44:08.780 --> 02:44:11.900]   Then we can have some skip connection, etc, etc.
[02:44:12.620 --> 02:44:14.620]   So
[02:44:14.620 --> 02:44:38.620]   then we have the time.
[02:44:39.420 --> 02:44:41.420]   So
[02:44:41.420 --> 02:45:03.180]   here we are merging the latency with the time embedding,
[02:45:03.180 --> 02:45:07.500]   but the time embedding doesn't have the batch and the channels dimension.
[02:45:07.500 --> 02:45:09.500]   So we add it here with unsqueeze.
[02:45:09.500 --> 02:45:11.900]   And we merge them.
[02:45:11.900 --> 02:45:15.180]   Then we normalize this merged connection.
[02:45:15.180 --> 02:45:19.260]   This is why it's called merged.
[02:45:19.260 --> 02:45:22.220]   We apply the activation function.
[02:45:34.700 --> 02:45:37.500]   Then we apply this convolution.
[02:45:37.500 --> 02:45:39.820]   And finally, we apply the residual connection.
[02:45:39.820 --> 02:45:49.900]   So why are we doing this?
[02:45:49.900 --> 02:45:52.780]   Well, the idea is that here we have three inputs.
[02:45:52.780 --> 02:45:56.300]   We have the time embedding, we have the latent, we have the prompt.
[02:45:56.300 --> 02:46:00.140]   We need to find a way to combine the three information together.
[02:46:00.140 --> 02:46:05.100]   So the unit needs to learn to detect the noise present in a noisified image
[02:46:05.100 --> 02:46:09.100]   at a particular time step using a particular prompt as a condition.
[02:46:09.100 --> 02:46:14.860]   Which means that the model needs to recognize this time embedding
[02:46:14.860 --> 02:46:17.980]   and needs to relate this time embedding with the latency.
[02:46:17.980 --> 02:46:21.100]   And this is exactly what we are doing in this residual block here.
[02:46:21.100 --> 02:46:24.460]   We are relating the latent with the time embedding,
[02:46:24.460 --> 02:46:29.020]   so that the output will depend on the combination of both,
[02:46:29.020 --> 02:46:31.820]   not on the single noise or in the single time step.
[02:46:31.820 --> 02:46:35.980]   And this will also be done with the context using cross-attention
[02:46:35.980 --> 02:46:37.900]   in the attention block that we will build now.
[02:46:37.900 --> 02:46:53.900]   So, unit attention block.
[02:46:58.460 --> 02:47:09.340]   768, okay.
[02:47:09.340 --> 02:47:18.140]   Okay, I will define some layers that for now will not make much sense,
[02:47:18.140 --> 02:47:23.340]   but later they will make sense when we make the forward method.
[02:47:23.340 --> 02:47:34.220]   So,
[02:47:34.220 --> 02:47:43.100]   okay.
[02:47:43.100 --> 02:47:59.420]   So, my cat is asking for food.
[02:48:00.700 --> 02:48:06.540]   I think he already has food, but maybe he wants to eat something special today.
[02:48:06.540 --> 02:48:12.220]   So, let me finish this attention block and the unit and then I'm all his.
[02:48:12.220 --> 02:48:16.140]   Why everyone wants attention?
[02:48:16.140 --> 02:48:22.300]   Self-attention and head channels.
[02:48:22.300 --> 02:48:24.940]   Here we don't have any bias.
[02:48:24.940 --> 02:48:29.020]   As you remember, the self-attention we can have the bias for the W matrices.
[02:48:30.060 --> 02:48:33.020]   Here we don't have any bias, just like in the vanilla transformer.
[02:48:33.020 --> 02:48:35.660]   So, we have this attention.
[02:48:35.660 --> 02:48:39.980]   Then we have a layer normalization, self.layernorm 2,
[02:48:39.980 --> 02:48:45.740]   which is along the same number of features.
[02:48:45.740 --> 02:48:47.020]   Then we have another attention.
[02:48:47.020 --> 02:48:49.980]   We will see later why we need all this attention,
[02:48:49.980 --> 02:48:51.260]   but this is not a self-attention.
[02:48:51.260 --> 02:48:54.780]   It's a cross-attention and we will see later how it works.
[02:48:54.780 --> 02:49:00.300]   So,
[02:49:00.300 --> 02:49:14.300]   then we have the layer norm 3.
[02:49:14.540 --> 02:49:23.820]   So,
[02:49:23.820 --> 02:49:32.380]   this is because we are using a function that is called the
[02:49:32.380 --> 02:49:34.300]   JGLU activation function.
[02:49:34.300 --> 02:49:37.820]   So, we need these matrices here.
[02:49:38.700 --> 02:49:47.340]   So,
[02:49:47.340 --> 02:49:59.100]   okay.
[02:49:59.100 --> 02:50:00.620]   Now we can build the forward method.
[02:50:00.620 --> 02:50:03.260]   So, our X is our latency.
[02:50:03.260 --> 02:50:04.700]   So, we have a batch size.
[02:50:04.700 --> 02:50:06.940]   We have features.
[02:50:06.940 --> 02:50:07.900]   We have height.
[02:50:07.900 --> 02:50:08.700]   We have width.
[02:50:08.700 --> 02:50:11.660]   Then we have our context, which is our prompt,
[02:50:11.660 --> 02:50:15.980]   which is a batch size, sequence length, dimension.
[02:50:15.980 --> 02:50:19.260]   The dimension is size 768, as we saw before.
[02:50:19.260 --> 02:50:25.100]   So, the first thing we will do is we will do the normalization.
[02:50:25.100 --> 02:50:28.540]   So, just like in the transformer, we will take the input,
[02:50:28.540 --> 02:50:32.700]   so our latency, and we apply the normalization and the convolution.
[02:50:32.700 --> 02:50:34.700]   Actually, in the transformer, there is no convolution,
[02:50:34.700 --> 02:50:36.220]   but only the normalization.
[02:50:36.380 --> 02:50:41.740]   So, this is called the long residual,
[02:50:41.740 --> 02:50:43.420]   because it will be applied at the end.
[02:50:43.420 --> 02:50:50.860]   Okay, so we have this here.
[02:50:50.860 --> 02:50:52.140]   We are applying the normalization,
[02:50:52.140 --> 02:50:54.460]   which doesn't change the size of the tensor.
[02:50:54.460 --> 02:50:56.540]   Then we have a convolution.
[02:50:56.540 --> 02:51:01.900]   X is equal to self.com input of X,
[02:51:01.900 --> 02:51:04.700]   which also doesn't change the size of the tensor.
[02:51:05.580 --> 02:51:08.460]   Then we take the shape,
[02:51:08.460 --> 02:51:17.340]   which is the batch size, the number of features, the height, and the width.
[02:51:17.340 --> 02:51:22.060]   We transpose because we want to apply cross-attention.
[02:51:22.060 --> 02:51:26.300]   First, we apply self-attention, then we apply cross-attention.
[02:51:28.460 --> 02:51:40.220]   So, we do normalization plus self-attention with skip connection.
[02:51:40.220 --> 02:51:46.700]   So, X is X dot transpose of minus one, minus two.
[02:51:46.700 --> 02:51:49.820]   So, we are going from this.
[02:51:49.820 --> 02:51:54.140]   Wait, I forgot something.
[02:51:54.140 --> 02:51:57.980]   Here, first of all, we need to do X is equal to X dot view.
[02:51:58.860 --> 02:52:02.780]   Then C, H multiplied by W.
[02:52:02.780 --> 02:52:11.980]   So, we are going from this to batch size features,
[02:52:11.980 --> 02:52:13.500]   and then H multiplied by W.
[02:52:13.500 --> 02:52:14.940]   So, this one multiplied by this.
[02:52:14.940 --> 02:52:17.500]   Then we transpose these two dimensions.
[02:52:17.500 --> 02:52:23.420]   So, now we get from here to here.
[02:52:23.420 --> 02:52:25.100]   So, the features become the last one.
[02:52:26.940 --> 02:52:29.500]   Now, we apply this normalization plus self-attention.
[02:52:29.500 --> 02:52:35.260]   So, we have a first short residual connection
[02:52:35.260 --> 02:52:37.180]   that we'll apply right after the attention.
[02:52:37.180 --> 02:52:42.380]   So, we say that X is equal to layer norm one.
[02:52:42.380 --> 02:52:43.020]   So, X.
[02:52:43.020 --> 02:52:44.940]   Then we apply the attention.
[02:52:44.940 --> 02:52:46.700]   So, self dot attention one.
[02:52:46.700 --> 02:52:49.740]   And then we apply the residual connection.
[02:52:49.740 --> 02:52:55.100]   So, X is plus equal to residual short, the first residual connection.
[02:52:56.060 --> 02:52:58.780]   Then we say that the residual short is again equal to six,
[02:52:58.780 --> 02:53:01.740]   because we are going to apply now the cross attention.
[02:53:01.740 --> 02:53:11.100]   So, now we apply the normalization plus the cross attention with skip connection.
[02:53:11.100 --> 02:53:20.060]   So, what we did here is what we do in any transformer.
[02:53:20.060 --> 02:53:23.020]   So, let me show you here what we do in any transformer.
[02:53:23.020 --> 02:53:24.460]   So, we apply some normalization.
[02:53:24.460 --> 02:53:26.140]   We calculate the attention.
[02:53:26.140 --> 02:53:28.460]   And then we combine it with a skip connection here.
[02:53:28.460 --> 02:53:31.660]   And now we will, instead of calculating a self-attention,
[02:53:31.660 --> 02:53:35.020]   we will do a cross attention, which we still didn't define.
[02:53:35.020 --> 02:53:36.460]   We will define it later.
[02:53:36.460 --> 02:53:39.340]   So, short.
[02:53:39.340 --> 02:53:43.020]   And then first we calculate, we apply the normalization.
[02:53:43.020 --> 02:53:52.700]   Then the cross attention between the latency and the prompt.
[02:53:53.420 --> 02:53:54.860]   This is cross attention.
[02:53:54.860 --> 02:53:57.420]   So, this is cross attention.
[02:53:57.420 --> 02:53:59.660]   And we will see how.
[02:53:59.660 --> 02:54:07.260]   And X plus or equal to residual short.
[02:54:07.260 --> 02:54:09.420]   Okay.
[02:54:09.420 --> 02:54:12.300]   And then again, equal to X.
[02:54:12.300 --> 02:54:16.140]   Finally, just like with the attention transformer,
[02:54:16.140 --> 02:54:20.620]   we have a feedforward layer with the JGLU activation function.
[02:54:20.780 --> 02:54:26.780]   Okay.
[02:54:26.780 --> 02:54:45.180]   And this is actually, if you watch the original implementation of the transformer,
[02:54:45.180 --> 02:54:47.900]   of the stable diffusion, it's implemented exactly like this.
[02:54:48.620 --> 02:54:55.020]   So, basically later we do element-wise multiplication.
[02:54:55.020 --> 02:55:03.420]   So, these are special activation functions that involve a lot of parameters.
[02:55:03.420 --> 02:55:09.420]   But why we use one and not the other?
[02:55:09.420 --> 02:55:11.500]   I told you, just like before,
[02:55:11.500 --> 02:55:14.940]   they just saw that this one works better for this kind of application.
[02:55:14.940 --> 02:55:16.460]   There is no other.
[02:55:16.940 --> 02:55:22.940]   Then we apply the skip connection.
[02:55:22.940 --> 02:55:24.620]   So, we apply the cross attention.
[02:55:24.620 --> 02:55:26.940]   Then we define another one here.
[02:55:26.940 --> 02:55:38.620]   So, this one is basically normalization plus feedforward layer with JGLU and skip connection.
[02:55:38.620 --> 02:55:41.660]   In which the skip connection is defined here.
[02:55:41.660 --> 02:55:43.980]   So, at the end, we always apply the skip connection.
[02:55:44.780 --> 02:55:50.140]   Finally, we change back to our tensor to not be a sequence of pixels anymore.
[02:55:50.140 --> 02:55:54.700]   So, we reverse the previous transposition.
[02:55:54.700 --> 02:55:57.820]   Transpose.
[02:55:57.820 --> 02:56:12.540]   So, basically, we go from batch size with width multiplied by height multiplied by width.
[02:56:13.980 --> 02:56:25.260]   And features into batch size features height multiplied by width.
[02:56:25.260 --> 02:56:30.460]   Then we remove this multiplication.
[02:56:30.460 --> 02:56:32.700]   So, we reverse this multiplication.
[02:56:32.700 --> 02:56:36.940]   And CHW.
[02:56:36.940 --> 02:56:43.820]   Finally, we apply the long skip connection that we defined here at the beginning.
[02:56:43.820 --> 02:56:46.540]   So, only if the size match.
[02:56:46.540 --> 02:56:49.580]   If the sizes don't match, we apply the here.
[02:56:49.580 --> 02:56:50.620]   This one we have here.
[02:56:50.620 --> 02:56:55.420]   Return self.com output.
[02:56:55.420 --> 02:57:04.300]   And this is all of our unit.
[02:57:04.300 --> 02:57:09.340]   We have defined everything, I think, except for the cross attention, which is very fast.
[02:57:09.340 --> 02:57:13.100]   So, we go to the attention that we defined before.
[02:57:13.100 --> 02:57:15.180]   And I put it in the wrong folder.
[02:57:15.180 --> 02:57:17.660]   It should be skip changes.
[02:57:17.660 --> 02:57:20.700]   Let me check if I put it correctly.
[02:57:20.700 --> 02:57:28.460]   Yeah, we only need to define this cross attention here.
[02:57:28.460 --> 02:57:31.660]   Okay, attention.
[02:57:31.660 --> 02:57:33.100]   So, let's go.
[02:57:33.100 --> 02:57:38.060]   And let's define this cross attention.
[02:57:38.060 --> 02:57:44.780]   So, class, it will be very similar to the, not very similar, actually, same as the self
[02:57:44.780 --> 02:57:51.420]   attention, except that the keys come from one side and the query and, sorry, the query
[02:57:51.420 --> 02:58:05.420]   come from one side and the key and the values from another side.
[02:58:06.300 --> 02:58:15.100]   So, this is the dimension of the embedding of the keys and the values.
[02:58:15.100 --> 02:58:28.780]   This is the one of the queries.
[02:58:28.780 --> 02:58:37.420]   This is the WQ matrix.
[02:58:37.420 --> 02:58:42.940]   In this case, we will define, instead of one big matrix made of three, WQ, WK and WV, we
[02:58:42.940 --> 02:58:44.540]   will define three different matrices.
[02:58:44.540 --> 02:58:46.860]   Both systems are fine.
[02:58:46.860 --> 02:58:50.220]   You can define it as one big matrix or three separately.
[02:58:50.220 --> 02:59:02.860]   It doesn't change anything, actually.
[02:59:02.860 --> 02:59:16.860]   So, the cross is from the keys and the values.
[02:59:16.860 --> 02:59:31.500]   Oops, linear.
[02:59:32.220 --> 03:00:00.460]   Then, we save the number of heads of this cross attention and also the dimension of
[03:00:00.460 --> 03:00:04.140]   each, how much information each head will see.
[03:00:04.140 --> 03:00:13.660]   And the head is equal to the embed divided by the number of heads.
[03:00:13.660 --> 03:00:17.100]   Let's define the forward method.
[03:00:17.100 --> 03:00:23.660]   X is our query and Y is our keys and values.
[03:00:27.740 --> 03:00:36.060]   So, we are relating X, which is our latency, which is of size batch size.
[03:00:36.060 --> 03:00:43.580]   It will have a sequence length, its own sequence length, Q, let's call it Q, and its own dimension.
[03:00:43.580 --> 03:00:50.780]   And the Y, which is the context or the prompt, which will be batch size.
[03:00:53.580 --> 03:00:58.860]   Sequence length of the key, because the prompt will become the key and the values.
[03:00:58.860 --> 03:01:03.100]   And each of them will have its own embedding size, the dimension of KV.
[03:01:03.100 --> 03:01:09.580]   We can already say that this will be a batch size of 77, because our sequence length of
[03:01:09.580 --> 03:01:14.140]   the prompt is 77 and its embedding is of size 768.
[03:01:14.140 --> 03:01:18.140]   So, let's build this one.
[03:01:19.580 --> 03:01:23.740]   This is input shape is equal to x dot shape.
[03:01:23.740 --> 03:01:40.860]   Okay, then we have the interim shape, like the same as before.
[03:01:40.860 --> 03:01:49.020]   So, this is the sequence length, then the n number of heads.
[03:01:49.980 --> 03:01:52.300]   And how much information each head will see.
[03:01:52.300 --> 03:01:55.260]   The head.
[03:01:55.260 --> 03:02:05.660]   The first thing we do is multiply queries by WQ matrix.
[03:02:05.660 --> 03:02:07.580]   So, query is equal to.
[03:02:07.580 --> 03:02:15.660]   Then we do the same for the keys and the values, but by using the other matrices.
[03:02:17.900 --> 03:02:22.140]   And as I told you before, the key and the values are the Y and not the X.
[03:02:22.140 --> 03:02:32.140]   Again, we split them into H heads, so H number of heads.
[03:02:32.140 --> 03:02:38.860]   Then we transpose.
[03:02:38.860 --> 03:02:46.300]   I will not write the shapes because they match the same transformation that we do here.
[03:02:46.700 --> 03:03:07.580]   Okay, again, we calculate the weight, which is the attention, as a query multiplied by
[03:03:07.580 --> 03:03:08.860]   the transpose of the keys.
[03:03:12.300 --> 03:03:21.820]   And then we divide it by the dimension of each head by the square root.
[03:03:21.820 --> 03:03:29.420]   Then we do the softmax.
[03:03:29.420 --> 03:03:36.300]   In this case, we don't have any causal mask, so we don't need to apply the mask like before,
[03:03:36.300 --> 03:03:41.980]   because here we are trying to relate the tokens, so the prompt with the pixels.
[03:03:41.980 --> 03:03:49.100]   So, each pixel can watch any word of the token, and any token can watch any pixel, basically.
[03:03:49.100 --> 03:03:56.700]   So, we don't need any mask.
[03:04:00.940 --> 03:04:04.140]   We are to obtain the output, we multiply it by the bit matrix.
[03:04:04.140 --> 03:04:10.140]   And then the output, again, is transposed, just like before.
[03:04:10.140 --> 03:04:13.900]   So, now we are doing exactly the same things that we did here.
[03:04:13.900 --> 03:04:24.460]   So, transpose, reshape, etc.
[03:04:24.620 --> 03:04:38.780]   And then return output.
[03:04:38.780 --> 03:04:43.020]   And this ends our building of the... let me show you.
[03:04:43.020 --> 03:04:49.420]   Now we have built all the building blocks for the stable diffusion.
[03:04:49.420 --> 03:04:53.420]   So, now we can finally combine them together.
[03:04:53.420 --> 03:04:57.500]   So, the next thing that we are going to do is to create the system that,
[03:04:57.500 --> 03:05:03.020]   taking the noise, taking the text, taking the time embedding, will run,
[03:05:03.020 --> 03:05:07.900]   for example, if we want to do text to image, will run this noise many times through the unit,
[03:05:07.900 --> 03:05:10.540]   according to a schedule.
[03:05:10.540 --> 03:05:13.340]   So, we will build the scheduler, which means that,
[03:05:13.340 --> 03:05:17.820]   because the unit is trained to predict how much noise is there,
[03:05:17.820 --> 03:05:19.740]   but we then need to remove this noise.
[03:05:20.460 --> 03:05:25.180]   So, to go from a noisy version to obtain a less noisy version,
[03:05:25.180 --> 03:05:28.140]   we need to remove the noise that is predicted by the unit.
[03:05:28.140 --> 03:05:30.460]   And this job is done by the scheduler.
[03:05:30.460 --> 03:05:32.220]   And now we will build the scheduler.
[03:05:32.220 --> 03:05:35.980]   We will build the code to load the weights of the pre-trained model.
[03:05:35.980 --> 03:05:39.580]   And then we combine all these things together.
[03:05:39.580 --> 03:05:42.060]   And we actually build what is called the pipeline.
[03:05:42.060 --> 03:05:45.180]   So, the pipeline of text to image, image to image, etc.
[03:05:45.180 --> 03:05:48.300]   And let's go.
[03:05:48.300 --> 03:05:52.060]   Now that we have built all the structure of the unit,
[03:05:52.060 --> 03:05:55.500]   or we have built the variational autoencoder, we have built a clip,
[03:05:55.500 --> 03:05:58.700]   we have built the attention blocks, etc.
[03:05:58.700 --> 03:06:01.980]   Now it's time to combine it all together.
[03:06:01.980 --> 03:06:05.500]   So, the first thing I kindly ask you to do is to actually download
[03:06:05.500 --> 03:06:09.340]   the pre-trained weights of the stable diffusion, because we need to inference it later.
[03:06:09.340 --> 03:06:13.660]   So, if you go to the repository I shared, this one, PyTorch Stable Diffusion,
[03:06:14.220 --> 03:06:18.380]   you can download the pre-trained weights of the stable diffusion 1.5
[03:06:18.380 --> 03:06:20.940]   directly from the website of Hugging Face.
[03:06:20.940 --> 03:06:24.700]   So, you download this file here, which is the EMA,
[03:06:24.700 --> 03:06:27.020]   which means Exponentially Moving Average,
[03:06:27.020 --> 03:06:30.460]   which means that it's a model that has been trained,
[03:06:30.460 --> 03:06:32.860]   but they didn't change the weights at each iteration,
[03:06:32.860 --> 03:06:35.180]   but with an Exponentially Moving Average schedule.
[03:06:35.180 --> 03:06:37.500]   So, this is good for inferencing.
[03:06:37.500 --> 03:06:39.580]   It means that the weights are more stable.
[03:06:39.580 --> 03:06:42.700]   But if you want to fine-tune later the model, you need to download this one.
[03:06:43.900 --> 03:06:48.380]   And we also need to download the files of the tokenizer,
[03:06:48.380 --> 03:06:53.660]   because, of course, we will give some prompt to the model to generate an image.
[03:06:53.660 --> 03:06:56.700]   And the prompt needs to be tokenized by a tokenizer,
[03:06:56.700 --> 03:07:00.620]   which will convert the words into tokens and the tokens into numbers.
[03:07:00.620 --> 03:07:05.420]   The numbers will then be mapped into embeddings by our clip embedding here.
[03:07:05.420 --> 03:07:08.780]   So, we need to download two files for the tokenizer.
[03:07:08.780 --> 03:07:12.380]   So, first of all, the weights of this one file here,
[03:07:12.380 --> 03:07:17.340]   then on the tokenizer folder, we find the merges.txt and the vocab.json.
[03:07:17.340 --> 03:07:21.420]   If we look at the vocab.json file, which I already downloaded,
[03:07:21.420 --> 03:07:23.420]   it's basically vocabulary.
[03:07:23.420 --> 03:07:25.260]   So, each token mapped to a number.
[03:07:25.260 --> 03:07:27.580]   That's it, just like what the tokenizer does.
[03:07:27.580 --> 03:07:32.140]   And then I also prepared the picture of a dog that I will be using for image-to-image,
[03:07:32.140 --> 03:07:33.420]   but you can use any image.
[03:07:33.420 --> 03:07:36.700]   You don't have to use the one I am using, of course.
[03:07:36.700 --> 03:07:41.180]   So, now, let's first build the pipeline.
[03:07:41.180 --> 03:07:44.540]   So, how we will inference this stable diffusion model.
[03:07:44.540 --> 03:07:47.900]   And then, while building the pipeline,
[03:07:47.900 --> 03:07:51.420]   I will also explain you how the scheduler will work.
[03:07:51.420 --> 03:07:54.540]   And we will build the scheduler later.
[03:07:54.540 --> 03:07:57.660]   I will explain all the formulas, all the mathematics behind it.
[03:07:57.660 --> 03:07:59.020]   So, let's start.
[03:07:59.020 --> 03:08:01.340]   Let's create a new file.
[03:08:01.340 --> 03:08:04.300]   Let's call it pipeline.py.
[03:08:05.980 --> 03:08:07.900]   And we import the usual stuff.
[03:08:07.900 --> 03:08:12.380]   NumPy.
[03:08:12.380 --> 03:08:15.580]   Oops, stop, stop, stop.
[03:08:15.580 --> 03:08:16.940]   NumPy as empty.
[03:08:16.940 --> 03:08:22.540]   We will also use a tqdm to show the progress bar.
[03:08:22.540 --> 03:08:28.700]   And later, we will build this sampler, the DPM sampler.
[03:08:28.700 --> 03:08:30.220]   And we will build it later.
[03:08:30.220 --> 03:08:35.660]   And I will also explain what is this sampler doing and how it works, etc, etc.
[03:08:35.980 --> 03:08:38.060]   So, first of all, let's define some constants.
[03:08:38.060 --> 03:08:43.980]   The stable diffusion can only produce images of size 512 by 512.
[03:08:43.980 --> 03:08:46.620]   So, height is 512 by 512.
[03:08:46.620 --> 03:08:55.420]   The latent dimension is the size of the latent tensor of the variational autoencoder.
[03:08:55.420 --> 03:08:59.500]   And as we saw before, if we go check the size,
[03:09:00.300 --> 03:09:05.900]   the encoder of the variational autoencoder will convert something that is 512 by 512
[03:09:05.900 --> 03:09:09.260]   into something that is 512 divided by 8.
[03:09:09.260 --> 03:09:12.780]   So, the latent dimension is 512 divided by 8.
[03:09:12.780 --> 03:09:16.060]   And the same goes on for the height.
[03:09:16.060 --> 03:09:19.420]   512 divided by 8.
[03:09:19.420 --> 03:09:23.420]   We can also call it width divided by 8 and height divided by 8.
[03:09:23.420 --> 03:09:27.660]   Then, we create a function called the generator.
[03:09:27.660 --> 03:09:33.900]   This will be the main function that will allow us to do text to image and also image to image,
[03:09:33.900 --> 03:09:36.460]   which accepts a prompt, which is a string.
[03:09:36.460 --> 03:09:40.300]   An unconditional prompt.
[03:09:40.300 --> 03:09:41.820]   So, unconditional prompt.
[03:09:41.820 --> 03:09:44.620]   This is also called the negative prompt.
[03:09:44.620 --> 03:09:48.380]   If you ever used stable diffusion, for example, with the HuggingFace library,
[03:09:48.380 --> 03:09:51.820]   you will know that you can also specify a negative prompt,
[03:09:51.820 --> 03:09:56.620]   which tells that you want, for example, you want a picture of a cat,
[03:09:56.620 --> 03:09:59.900]   but you don't want the cat to be on the sofa.
[03:09:59.900 --> 03:10:04.060]   So, for example, you can put the word sofa in the negative prompt.
[03:10:04.060 --> 03:10:08.620]   So, it will try to go away from the concept of sofa when generating the image.
[03:10:08.620 --> 03:10:09.260]   Something like this.
[03:10:09.260 --> 03:10:13.980]   And this is connected with the classifier free guidance that we saw before.
[03:10:13.980 --> 03:10:17.020]   So, but don't worry, I will repeat all the concepts while we are building it.
[03:10:17.020 --> 03:10:18.540]   So, this is also a string.
[03:10:18.540 --> 03:10:23.660]   We can have an input image in case we are building an image to image.
[03:10:25.580 --> 03:10:27.660]   And then we have the strength.
[03:10:27.660 --> 03:10:33.340]   Strength, I will show you later what is it, but it's related to if we have an input image
[03:10:33.340 --> 03:10:37.180]   and how much, if we start from an image to generate another image,
[03:10:37.180 --> 03:10:40.780]   how much attention we want to pay to the initial starting image.
[03:10:40.780 --> 03:10:46.300]   And we can also have a parameter called doCFG,
[03:10:46.300 --> 03:10:49.260]   which means do classifier free guidance.
[03:10:49.260 --> 03:10:50.220]   We set it to yes.
[03:10:51.020 --> 03:10:56.460]   CFG scale, which is the weight of how much we want the model to pay attention to our prompt.
[03:10:56.460 --> 03:10:58.700]   It's a value that goes from 1 to 14.
[03:10:58.700 --> 03:11:00.380]   We start with 7.5.
[03:11:00.380 --> 03:11:03.980]   The sampler name, we will only implement one.
[03:11:03.980 --> 03:11:05.500]   So, it's called edpm.
[03:11:05.500 --> 03:11:09.100]   How many inference steps we want to do.
[03:11:09.100 --> 03:11:12.060]   And we will do 50.
[03:11:12.060 --> 03:11:17.180]   I think it's quite common to do 50 steps, which produces actually not bad results.
[03:11:17.180 --> 03:11:20.140]   The models are the pre-trained models.
[03:11:20.140 --> 03:11:23.980]   The seed is how we want to initialize our random number generator.
[03:11:23.980 --> 03:11:27.820]   Let me put a new line, otherwise we become crazy reading this.
[03:11:27.820 --> 03:11:31.920]   Okay.
[03:11:31.920 --> 03:11:34.300]   New line.
[03:11:34.300 --> 03:11:36.540]   So, seed.
[03:11:36.540 --> 03:11:39.820]   Then we have the device where we want to create our tensor.
[03:11:39.820 --> 03:11:45.020]   We have an idle device, which means basically if we load some model on CUDA
[03:11:45.020 --> 03:11:47.340]   and then we don't need the model, we move it to the CPU.
[03:11:48.060 --> 03:11:50.300]   And then the tokenizer that we will load later.
[03:11:50.300 --> 03:11:52.540]   Tokenizer is none.
[03:11:52.540 --> 03:11:53.040]   Okay.
[03:11:53.040 --> 03:11:54.860]   This is our method.
[03:11:54.860 --> 03:11:59.340]   This is our main pipeline that, given all this information, will generate one picture.
[03:11:59.340 --> 03:12:01.340]   So, it will pay attention to the prompt.
[03:12:01.340 --> 03:12:03.980]   It will pay attention to the input image, if there is,
[03:12:03.980 --> 03:12:06.220]   according to the weights that we have specified.
[03:12:06.220 --> 03:12:08.460]   So, the strength and the CFG scale.
[03:12:08.460 --> 03:12:10.540]   I will repeat all this concept.
[03:12:10.540 --> 03:12:14.940]   Don't worry, later I will explain them actually how they work also on the code level.
[03:12:14.940 --> 03:12:16.860]   So, let's start.
[03:12:16.860 --> 03:12:21.980]   So, the first thing we do is we disable.
[03:12:21.980 --> 03:12:22.480]   Okay.
[03:12:22.480 --> 03:12:28.780]   Torch.log(red) because we are inferencing the model.
[03:12:28.780 --> 03:12:34.220]   The first thing we make sure is the strength should be between 0 and 1.
[03:12:34.220 --> 03:12:34.860]   So, if...
[03:12:34.860 --> 03:12:44.220]   Then we raise an error.
[03:12:45.020 --> 03:12:47.660]   Raise value error.
[03:12:47.660 --> 03:12:55.020]   Must be between 0 and 1.
[03:12:55.020 --> 03:12:59.340]   If idle device.
[03:12:59.340 --> 03:13:05.260]   If we want to move things to the CPU, we create this lambda function.
[03:13:06.140 --> 03:13:14.140]   Otherwise.
[03:13:14.140 --> 03:13:17.440]   Okay.
[03:13:17.440 --> 03:13:20.940]   Then we create the... oops.
[03:13:20.940 --> 03:13:24.940]   I think I... okay.
[03:13:24.940 --> 03:13:28.940]   Then we create the... oops.
[03:13:28.940 --> 03:13:30.940]   I think I... okay.
[03:13:33.100 --> 03:13:33.600]   Okay.
[03:13:33.600 --> 03:13:38.060]   Then we create the random number generator that we will use.
[03:13:38.060 --> 03:13:43.260]   I think I made some mess with this.
[03:13:43.260 --> 03:13:47.420]   So, this one should be like here.
[03:13:47.420 --> 03:13:51.040]   Okay.
[03:13:51.040 --> 03:13:59.420]   And the generator is a random number generator that we will use to generate the noise.
[03:14:01.260 --> 03:14:04.540]   And if we want to start it with the seed.
[03:14:04.540 --> 03:14:05.580]   So, if seed.
[03:14:05.580 --> 03:14:11.580]   Then we generate with the random seed.
[03:14:11.580 --> 03:14:14.140]   Otherwise, we specify one manually.
[03:14:14.140 --> 03:14:27.260]   Let me fix this formatting because I don't know format document.
[03:14:27.260 --> 03:14:27.760]   Okay.
[03:14:28.700 --> 03:14:29.660]   Now, at least the...
[03:14:29.660 --> 03:14:33.900]   Then we define clip.
[03:14:33.900 --> 03:14:37.900]   The clip is a model that we take from the pre-trained models.
[03:14:37.900 --> 03:14:41.180]   So, it will have the clip model inside.
[03:14:41.180 --> 03:14:43.580]   So, this model here, basically.
[03:14:43.580 --> 03:14:45.420]   This one here.
[03:14:45.420 --> 03:14:49.100]   We move it to our device.
[03:14:49.100 --> 03:14:57.200]   Okay.
[03:14:58.160 --> 03:15:01.520]   As you remember with the classifier-free guidance.
[03:15:01.520 --> 03:15:03.280]   So, let me go back to my slides.
[03:15:03.280 --> 03:15:11.680]   When we do classifier-free guidance, we inference the model twice.
[03:15:11.680 --> 03:15:15.680]   First, by specifying the condition.
[03:15:15.680 --> 03:15:16.560]   So, the prompt.
[03:15:16.560 --> 03:15:19.200]   And another time by not specifying the condition.
[03:15:19.200 --> 03:15:20.720]   So, without the prompt.
[03:15:20.720 --> 03:15:25.760]   And then we combine the output of the model linearly with a weight.
[03:15:26.320 --> 03:15:28.000]   This weight, W, is our...
[03:15:28.000 --> 03:15:31.440]   This weight here, CFG scale.
[03:15:31.440 --> 03:15:36.320]   It indicates how much we want to pay attention to the conditioned output
[03:15:36.320 --> 03:15:38.320]   with respect to the unconditioned output.
[03:15:38.320 --> 03:15:43.280]   Which also means that how much we want the model to pay attention to the condition
[03:15:43.280 --> 03:15:44.400]   that we have specified.
[03:15:44.400 --> 03:15:45.200]   What is the condition?
[03:15:45.200 --> 03:15:46.000]   The prompt.
[03:15:46.000 --> 03:15:47.680]   The textual prompt that we have written.
[03:15:47.680 --> 03:15:53.360]   And the unconditioned actually is also...
[03:15:53.360 --> 03:15:54.880]   Will use the negative prompt.
[03:15:54.880 --> 03:15:57.920]   So, the negative prompt that you use in stable diffusion.
[03:15:57.920 --> 03:16:00.160]   Which is this parameter here.
[03:16:00.160 --> 03:16:02.000]   So, unconditioned prompt.
[03:16:02.000 --> 03:16:03.680]   This is the unconditional output.
[03:16:03.680 --> 03:16:05.520]   So, we will sample the...
[03:16:05.520 --> 03:16:07.600]   We will inference from the model twice.
[03:16:07.600 --> 03:16:09.200]   One with the prompt.
[03:16:09.200 --> 03:16:10.000]   One without.
[03:16:10.000 --> 03:16:10.480]   With the...
[03:16:10.480 --> 03:16:11.440]   One with the prompt.
[03:16:11.440 --> 03:16:13.040]   One with the unconditioned prompt.
[03:16:13.040 --> 03:16:14.880]   Which is usually an empty text.
[03:16:14.880 --> 03:16:16.320]   An empty string.
[03:16:16.320 --> 03:16:19.600]   And then we combine the two by this.
[03:16:19.600 --> 03:16:21.760]   And this will tell the model by using this weight.
[03:16:21.760 --> 03:16:24.880]   We will combine the output in such a way that we can decide
[03:16:24.880 --> 03:16:27.360]   how much we want the model to pay attention to the prompt.
[03:16:27.360 --> 03:16:29.920]   So, let's do it.
[03:16:29.920 --> 03:16:32.960]   If we want to do classifier-free guidance.
[03:16:32.960 --> 03:16:38.080]   First, convert the prompt into tokens.
[03:16:38.080 --> 03:16:42.320]   Using the tokenizer.
[03:16:42.320 --> 03:16:48.480]   We didn't specify what is the tokenizer yet.
[03:16:48.480 --> 03:16:49.760]   But later we will define it.
[03:16:50.480 --> 03:16:51.920]   So, the conditional tokens.
[03:16:51.920 --> 03:16:54.560]   Tokenizer.
[03:16:54.560 --> 03:16:55.840]   Patch.
[03:16:55.840 --> 03:16:58.480]   Encode plus.
[03:16:58.480 --> 03:17:02.720]   We want to encode the prompt.
[03:17:02.720 --> 03:17:06.880]   We want to append the padding up to the maximum length.
[03:17:06.880 --> 03:17:10.000]   Which means that the prompt, if it's too short,
[03:17:10.000 --> 03:17:11.680]   it will fill up it with paddings.
[03:17:11.680 --> 03:17:15.280]   And the max length, as you remember, is 77.
[03:17:15.280 --> 03:17:17.520]   Because we have also defined it here.
[03:17:17.520 --> 03:17:19.200]   The sequence length is 77.
[03:17:19.360 --> 03:17:22.960]   And we take the input IDs of this tokenizer.
[03:17:22.960 --> 03:17:30.960]   Then we convert these tokens, which are input IDs, into a tensor.
[03:17:30.960 --> 03:17:35.920]   Which will be of size batch size and sequence length.
[03:17:35.920 --> 03:17:38.160]   So, conditional tokens.
[03:17:38.560 --> 03:17:41.920]   So, conditional tokens.
[03:17:41.920 --> 03:17:54.160]   And we put it in the right device.
[03:17:54.160 --> 03:17:56.880]   Now, we run it through clip.
[03:17:56.880 --> 03:17:59.840]   So, it will convert batch size sequence length.
[03:17:59.840 --> 03:18:03.040]   So, these input IDs will be converted into embeddings.
[03:18:05.680 --> 03:18:08.480]   Of size 768.
[03:18:08.480 --> 03:18:10.720]   Each vector of size 768.
[03:18:10.720 --> 03:18:12.400]   So, let's call it dim.
[03:18:12.400 --> 03:18:22.800]   And what we do is conditional context is equal to clip of conditional tokens.
[03:18:22.800 --> 03:18:25.840]   So, we are taking these tokens and we are running them through clips.
[03:18:25.840 --> 03:18:27.360]   So, this forward method here.
[03:18:27.360 --> 03:18:31.040]   Which will return batch size sequence length dimension.
[03:18:31.040 --> 03:18:33.040]   And this is exactly what I have written here.
[03:18:34.080 --> 03:18:36.480]   We do the same for the unconditioned tokens.
[03:18:36.480 --> 03:18:38.080]   So, the negative prompt.
[03:18:38.080 --> 03:18:41.440]   Which, if you don't want to specify, we will use the empty string.
[03:18:41.440 --> 03:18:44.240]   Which means the unconditional output of the model.
[03:18:44.240 --> 03:18:51.440]   So, the model, what would the model produce without any condition?
[03:18:51.440 --> 03:18:54.480]   So, if we start with random noise and we ask the model to produce an image.
[03:18:54.480 --> 03:18:55.600]   It will produce an image.
[03:18:55.600 --> 03:18:56.880]   But without any condition.
[03:18:56.880 --> 03:19:01.200]   So, the model will output anything that it wants based on the initial noise.
[03:19:01.920 --> 03:19:03.920]   So,
[03:19:03.920 --> 03:19:29.600]   we convert it into tensor.
[03:19:29.600 --> 03:19:31.360]   Then we pass it through clips.
[03:19:31.360 --> 03:19:32.800]   Just like the conditional tokens.
[03:19:32.800 --> 03:19:38.320]   So, it will become tokens.
[03:19:38.320 --> 03:19:38.820]   Yes.
[03:19:38.820 --> 03:19:47.600]   So, it will also become a tensor of batch size sequence length dimension.
[03:19:47.600 --> 03:19:50.560]   Where the sequence length is actually always 77.
[03:19:50.560 --> 03:19:52.480]   And also, in this case, it was always 77.
[03:19:52.480 --> 03:19:54.000]   Because it's the max length here.
[03:19:55.520 --> 03:19:59.040]   But I forgot to write the code to convert it into.
[03:19:59.040 --> 03:20:07.680]   So, unconditional tokens is equal to tokenizer batch plus.
[03:20:07.680 --> 03:20:13.680]   So, the unconditional prompt.
[03:20:13.680 --> 03:20:16.240]   So, also the negative prompt.
[03:20:16.240 --> 03:20:19.120]   The padding is the same as before.
[03:20:19.120 --> 03:20:20.000]   So, max length.
[03:20:20.000 --> 03:20:24.560]   And the max length is defined as 77.
[03:20:25.280 --> 03:20:27.280]   And we take the input IDs from here.
[03:20:27.280 --> 03:20:29.920]   So, now we have these two prompts.
[03:20:29.920 --> 03:20:31.680]   What we do is we concatenate them.
[03:20:31.680 --> 03:20:43.280]   They will become the batch of our input to the unit.
[03:20:43.280 --> 03:20:49.140]   Okay.
[03:20:49.140 --> 03:20:51.520]   So, basically what we are doing is.
[03:20:51.520 --> 03:20:54.240]   We are taking the conditional and unconditional input.
[03:20:54.240 --> 03:20:56.560]   And we are combining them into one single tensor.
[03:20:56.560 --> 03:21:00.640]   So, they will become a tensor of batch size 2.
[03:21:00.640 --> 03:21:05.200]   So, 2 sequence length and dimension.
[03:21:05.200 --> 03:21:07.760]   Where sequence length is actually.
[03:21:07.760 --> 03:21:09.040]   We can already write it.
[03:21:09.040 --> 03:21:12.800]   It will become 2 by 77 by 768.
[03:21:12.800 --> 03:21:14.400]   Because 77 is the sequence length.
[03:21:14.400 --> 03:21:16.800]   And the dimension is 768.
[03:21:17.840 --> 03:21:24.880]   If we don't want to do conditional classifier free guidance.
[03:21:24.880 --> 03:21:28.080]   We only need to use the prompt and that's it.
[03:21:28.080 --> 03:21:31.520]   So, we do only one step through the unit.
[03:21:31.520 --> 03:21:33.920]   And only with the prompt.
[03:21:33.920 --> 03:21:38.240]   Without combining the unconditional input with the conditional input.
[03:21:38.240 --> 03:21:38.880]   But in this case.
[03:21:38.880 --> 03:21:44.880]   We cannot decide how much the model pays attention to the prompt.
[03:21:44.880 --> 03:21:47.360]   Because we don't have anything to combine it with.
[03:21:48.240 --> 03:21:57.920]   So, again we take the just the prompt.
[03:21:57.920 --> 03:21:58.800]   Just like before.
[03:21:58.800 --> 03:22:02.560]   You can take it.
[03:22:02.560 --> 03:22:04.480]   Let's call it just tokens.
[03:22:04.480 --> 03:22:09.280]   And then we transform this into a tensor.
[03:22:09.280 --> 03:22:14.720]   Tensor long.
[03:22:16.640 --> 03:22:18.400]   We put it in the right device.
[03:22:18.400 --> 03:22:22.560]   We calculated the context.
[03:22:22.560 --> 03:22:24.480]   Which is a one big tensor.
[03:22:24.480 --> 03:22:26.720]   We pass it through clip.
[03:22:26.720 --> 03:22:30.720]   But this case it will be only one.
[03:22:30.720 --> 03:22:32.800]   Only one.
[03:22:32.800 --> 03:22:34.160]   So, the batch size will be one.
[03:22:34.160 --> 03:22:35.440]   So, the batch dimension.
[03:22:35.440 --> 03:22:37.520]   The sequence is again 77.
[03:22:37.520 --> 03:22:39.520]   And the dimension is 768.
[03:22:39.520 --> 03:22:41.360]   So, here we are combining two prompts.
[03:22:41.360 --> 03:22:42.960]   Here we are combining one.
[03:22:42.960 --> 03:22:43.200]   Why?
[03:22:43.200 --> 03:22:45.040]   Because we will run through the model.
[03:22:45.040 --> 03:22:45.840]   Two prompts.
[03:22:45.840 --> 03:22:47.040]   One unconditioned.
[03:22:47.040 --> 03:22:48.080]   One conditioned.
[03:22:48.080 --> 03:22:50.160]   So, one with the prompt that we want.
[03:22:50.160 --> 03:22:51.520]   One with the empty string.
[03:22:51.520 --> 03:22:53.840]   And the model will produce two outputs.
[03:22:53.840 --> 03:22:56.400]   Because the model takes care of the batch size.
[03:22:56.400 --> 03:22:57.840]   That's why we have the batch size.
[03:22:57.840 --> 03:23:02.480]   Since we have finished using the clip.
[03:23:02.480 --> 03:23:04.480]   We can move it to the idle device.
[03:23:04.480 --> 03:23:05.520]   This is very useful.
[03:23:05.520 --> 03:23:08.640]   Actually, if you have a very limited GPU.
[03:23:08.640 --> 03:23:11.760]   And you want to offload the models after using them.
[03:23:11.760 --> 03:23:13.440]   You can offload them back to the CPU.
[03:23:13.440 --> 03:23:14.960]   By moving them to the CPU again.
[03:23:15.840 --> 03:23:21.920]   And then we load the sampler.
[03:23:21.920 --> 03:23:23.680]   For now, we didn't define the sampler.
[03:23:23.680 --> 03:23:26.480]   But we use it and later we build it.
[03:23:26.480 --> 03:23:30.720]   Because it's better to build it after you know how it is used.
[03:23:30.720 --> 03:23:32.560]   If we build it before.
[03:23:32.560 --> 03:23:35.200]   I think it's easy to get lost in what is happening.
[03:23:35.200 --> 03:23:36.160]   What's happening actually.
[03:23:36.720 --> 03:23:40.960]   So, if the sampler name is ddpm.
[03:23:40.960 --> 03:23:44.960]   ddpm.
[03:23:44.960 --> 03:23:46.960]   Then we build the sampler.
[03:23:46.960 --> 03:23:49.520]   ddpm sampler.
[03:23:49.520 --> 03:23:51.440]   We pass it to the noise generator.
[03:23:51.440 --> 03:23:55.920]   And we tell the sampler how many steps we want to do for the inferencing.
[03:23:55.920 --> 03:24:00.480]   And I will show you later why.
[03:24:00.480 --> 03:24:03.920]   If the sampler is not ddpm.
[03:24:03.920 --> 03:24:04.960]   Then we raise an error.
[03:24:04.960 --> 03:24:08.720]   Because we didn't implement any other sampler.
[03:24:08.720 --> 03:24:21.600]   Why we need to tell him how many steps?
[03:24:21.600 --> 03:24:22.800]   Because as you remember.
[03:24:22.800 --> 03:24:23.920]   Let's go here.
[03:24:23.920 --> 03:24:26.820]   Here.
[03:24:26.820 --> 03:24:30.240]   This scheduler needs to do many steps.
[03:24:30.240 --> 03:24:30.800]   How many?
[03:24:30.800 --> 03:24:33.120]   We tell him exactly how many we want to do.
[03:24:33.120 --> 03:24:36.960]   In this case the denoisification steps will be 50.
[03:24:36.960 --> 03:24:41.120]   Even if during the training we have maximum 1000 steps.
[03:24:41.120 --> 03:24:43.600]   During inferencing we don't need to do 1000 steps.
[03:24:43.600 --> 03:24:44.720]   We can do less.
[03:24:44.720 --> 03:24:47.520]   Of course usually the more steps you do the better the quality.
[03:24:47.520 --> 03:24:50.880]   Because the more noise you can remove.
[03:24:50.880 --> 03:24:55.920]   But with different samplers they work in different way.
[03:24:55.920 --> 03:25:00.000]   And with ddpm usually 50 is good enough to get a nice result.
[03:25:00.000 --> 03:25:00.880]   For some other sampler.
[03:25:00.880 --> 03:25:03.120]   For example ddim you can do less steps.
[03:25:03.120 --> 03:25:07.600]   For some other samplers that work on with differential equations.
[03:25:07.600 --> 03:25:09.120]   You can do even less.
[03:25:09.120 --> 03:25:10.640]   Depends on which sampler you use.
[03:25:10.640 --> 03:25:15.920]   And how lucky you are with the particular prompt actually also.
[03:25:15.920 --> 03:25:26.400]   This is the latency that will run through the unit.
[03:25:26.400 --> 03:25:32.480]   And as you know it's of size "lat_height" and "lat_width".
[03:25:32.480 --> 03:25:33.600]   Which we defined before.
[03:25:33.600 --> 03:25:37.840]   So it's 512 divided by 8 by 512 divided by 8.
[03:25:37.840 --> 03:25:39.360]   So 64 by 64.
[03:25:39.360 --> 03:25:43.280]   And now let's do.
[03:25:43.280 --> 03:25:47.280]   What happens if the user specifies an input image?
[03:25:47.280 --> 03:25:49.600]   So if we have a prompt.
[03:25:49.600 --> 03:25:56.000]   We can take care of the prompt by either running a classifier free guidance.
[03:25:56.000 --> 03:26:03.520]   Which means combining the output of the model with the prompt and without the prompt.
[03:26:03.520 --> 03:26:05.920]   According to this scale here.
[03:26:05.920 --> 03:26:12.480]   Or we can directly just ask the model to output only one image.
[03:26:12.480 --> 03:26:13.600]   Only using the prompt.
[03:26:13.600 --> 03:26:17.360]   But then we cannot combine the two output with this scale.
[03:26:17.360 --> 03:26:20.960]   What happens however if we don't want to do text to image.
[03:26:20.960 --> 03:26:22.800]   But we want to do image to image.
[03:26:22.800 --> 03:26:24.880]   If we do image to image as we saw before.
[03:26:25.680 --> 03:26:27.120]   We start with an image.
[03:26:27.120 --> 03:26:29.120]   We encode it with the encoder.
[03:26:29.120 --> 03:26:30.560]   And then we add noise to it.
[03:26:30.560 --> 03:26:32.720]   And then we ask the scheduler to remove noise.
[03:26:32.720 --> 03:26:33.440]   Noise, noise.
[03:26:33.440 --> 03:26:38.400]   But since the unit will also be conditioned by the text prompt.
[03:26:38.400 --> 03:26:43.520]   We hope that while the unit will denoise this image.
[03:26:43.520 --> 03:26:46.960]   It will move it towards this prompt.
[03:26:46.960 --> 03:26:48.080]   So this is what we will do.
[03:26:48.080 --> 03:26:50.560]   First of things we load the image.
[03:26:50.560 --> 03:26:51.760]   And we encode it.
[03:26:51.760 --> 03:26:52.960]   And we add noise to it.
[03:26:55.120 --> 03:26:57.760]   So if an input image is specified.
[03:26:57.760 --> 03:27:00.960]   We load the encoder.
[03:27:00.960 --> 03:27:08.080]   We move it to the device.
[03:27:08.080 --> 03:27:10.240]   In case we are using CUDA for example.
[03:27:10.240 --> 03:27:14.400]   Then we load the tensor of the image.
[03:27:14.400 --> 03:27:18.640]   We resize it.
[03:27:18.640 --> 03:27:21.280]   We make sure that it's 512 by 512.
[03:27:21.280 --> 03:27:27.280]   With 8.
[03:27:27.280 --> 03:27:31.120]   And then we transform it into a NumPy array.
[03:27:31.120 --> 03:27:34.240]   And then into a tensor.
[03:27:35.040 --> 03:27:56.400]   So what will be the size here?
[03:27:56.400 --> 03:28:00.800]   It will be height by width by channel.
[03:28:00.800 --> 03:28:02.000]   And the channel will be 3.
[03:28:03.120 --> 03:28:06.080]   The next thing we do is we rescale this image.
[03:28:06.080 --> 03:28:06.960]   What does it mean?
[03:28:06.960 --> 03:28:12.480]   That the input of this unit should be normalized between.
[03:28:12.480 --> 03:28:16.240]   Should be, sorry, rescaled between -1 and +1.
[03:28:16.240 --> 03:28:17.680]   Because if we load the image.
[03:28:17.680 --> 03:28:19.360]   It will have three channels.
[03:28:19.360 --> 03:28:23.200]   Each channel will be between 0 and 255.
[03:28:23.200 --> 03:28:25.920]   So each pixel have three channels RGB.
[03:28:25.920 --> 03:28:28.480]   And each number is between 0 and 255.
[03:28:28.480 --> 03:28:31.200]   But this is not what the unit wants as input.
[03:28:31.200 --> 03:28:33.760]   The unit wants every channel, every pixel.
[03:28:33.760 --> 03:28:35.680]   To be between -1 and +1.
[03:28:35.680 --> 03:28:37.440]   So we will do this.
[03:28:37.440 --> 03:28:43.440]   We will build it later this function.
[03:28:43.440 --> 03:28:44.640]   It's called the rescale.
[03:28:44.640 --> 03:28:52.240]   To transform anything from that is from between 0 and 255.
[03:28:52.240 --> 03:28:55.360]   Into something that is between -1 and +1.
[03:28:57.680 --> 03:29:02.640]   And this will not change the size of the tensor.
[03:29:02.640 --> 03:29:04.000]   We add the batch dimension.
[03:29:04.000 --> 03:29:07.840]   Unsqueeze.
[03:29:07.840 --> 03:29:11.600]   This adds the batch dimension.
[03:29:11.600 --> 03:29:17.040]   Batch size.
[03:29:17.040 --> 03:29:23.760]   Okay, and then we change the order of the dimensions.
[03:29:24.640 --> 03:29:34.960]   Which is 0, 3, 1, 2.
[03:29:34.960 --> 03:29:36.880]   Why?
[03:29:36.880 --> 03:29:41.600]   Because as you know the encoder of the variation autoencoder.
[03:29:41.600 --> 03:29:45.520]   Wants batch size, channel, height and width.
[03:29:45.520 --> 03:29:48.480]   While we have batch size, height, width, channel.
[03:29:48.480 --> 03:29:49.520]   So we permute them.
[03:29:49.520 --> 03:29:52.880]   So to obtain the correct input for the encoder.
[03:29:53.120 --> 03:29:55.680]   We have this one.
[03:29:55.680 --> 03:29:58.960]   Go into channel.
[03:29:58.960 --> 03:30:04.960]   And height and width.
[03:30:04.960 --> 03:30:07.360]   And then this part we can delete.
[03:30:07.360 --> 03:30:09.520]   Okay, this is the input.
[03:30:09.520 --> 03:30:13.840]   Then what we do is we sample some noise.
[03:30:13.840 --> 03:30:15.920]   Because as you remember the encoder.
[03:30:15.920 --> 03:30:18.640]   To run the encoder we need some noise.
[03:30:18.640 --> 03:30:21.280]   And then he will sample from this particular Gaussian.
[03:30:21.280 --> 03:30:22.560]   That we have defined before.
[03:30:22.560 --> 03:30:25.920]   So encoder noise.
[03:30:25.920 --> 03:30:28.640]   We sample it from our generator.
[03:30:28.640 --> 03:30:34.400]   So as you we have defined this generator.
[03:30:34.400 --> 03:30:36.480]   So that we can define only one seed.
[03:30:36.480 --> 03:30:40.320]   And we can also make the output deterministic.
[03:30:40.320 --> 03:30:41.600]   If we never change the seed.
[03:30:41.600 --> 03:30:45.680]   And this is why we use the generator.
[03:30:45.680 --> 03:30:46.560]   Latent shape.
[03:30:47.040 --> 03:30:49.760]   Okay, and now let's run it through the decoder.
[03:30:49.760 --> 03:31:00.160]   Run the image through the of the VAE.
[03:31:00.160 --> 03:31:03.280]   This will produce latency.
[03:31:03.280 --> 03:31:07.120]   So input image tensor.
[03:31:07.120 --> 03:31:10.000]   And then we give it some noise.
[03:31:10.000 --> 03:31:13.120]   Now we have to run it through the decoder.
[03:31:14.000 --> 03:31:15.280]   And then we give it some noise.
[03:31:15.280 --> 03:31:18.240]   Now we are exactly here.
[03:31:18.240 --> 03:31:19.520]   We produced this.
[03:31:19.520 --> 03:31:20.640]   This is our latency.
[03:31:20.640 --> 03:31:22.560]   So we give the image to the encoder.
[03:31:22.560 --> 03:31:23.920]   Along with some noise.
[03:31:23.920 --> 03:31:27.280]   It will produce a latent representation of this image.
[03:31:27.280 --> 03:31:29.920]   Now we need to tell our...
[03:31:29.920 --> 03:31:32.080]   As you can see here.
[03:31:32.080 --> 03:31:34.640]   We need to add some noise to this latent.
[03:31:34.640 --> 03:31:35.600]   How can we add noise?
[03:31:35.600 --> 03:31:36.800]   We use our scheduler.
[03:31:36.800 --> 03:31:39.760]   The strength basically tells us.
[03:31:40.960 --> 03:31:43.760]   The strength parameter that we defined here.
[03:31:43.760 --> 03:31:46.800]   Tells us how much we want the model.
[03:31:46.800 --> 03:31:49.120]   To pay attention to the input image.
[03:31:49.120 --> 03:31:50.960]   When generating the output image.
[03:31:50.960 --> 03:31:53.360]   The more the strength.
[03:31:53.360 --> 03:31:55.360]   The more the noise we add.
[03:31:55.360 --> 03:31:57.600]   So the more the strength.
[03:31:57.600 --> 03:31:59.200]   The more the strong the noise.
[03:31:59.200 --> 03:32:02.480]   So the model will be more creative.
[03:32:02.480 --> 03:32:05.440]   Because the model will have more noise to remove.
[03:32:05.440 --> 03:32:08.000]   And can create a different image.
[03:32:08.000 --> 03:32:11.200]   But if we add less noise to this initial image.
[03:32:11.200 --> 03:32:13.120]   The model cannot be very creative.
[03:32:13.120 --> 03:32:15.920]   Because most of the image is already defined.
[03:32:15.920 --> 03:32:17.680]   So there is not much noise to remove.
[03:32:17.680 --> 03:32:21.840]   So we expect that the output will resemble more or less the input.
[03:32:21.840 --> 03:32:26.240]   So this strength here basically means.
[03:32:26.240 --> 03:32:27.440]   The more noise.
[03:32:27.440 --> 03:32:28.960]   How much noise to add.
[03:32:28.960 --> 03:32:30.480]   The more noise we add.
[03:32:30.480 --> 03:32:33.120]   The less the output will resemble the input.
[03:32:33.120 --> 03:32:34.400]   The less noise we add.
[03:32:34.400 --> 03:32:37.520]   The more the output will resemble the input.
[03:32:37.520 --> 03:32:40.880]   Because the scheduler, the unit sorry.
[03:32:40.880 --> 03:32:44.640]   Has less possibility of changing the image.
[03:32:44.640 --> 03:32:45.600]   Because there is less noise.
[03:32:45.600 --> 03:32:49.680]   So let's do it.
[03:32:49.680 --> 03:32:52.080]   First we tell the sampler.
[03:32:52.080 --> 03:32:54.320]   What is the strength that we have defined.
[03:32:54.320 --> 03:32:57.200]   And later we will see what is this method doing.
[03:32:57.200 --> 03:32:59.760]   But for now we just write it.
[03:32:59.760 --> 03:33:01.040]   And then we ask the sampler.
[03:33:01.040 --> 03:33:03.200]   To add noise to our latency here.
[03:33:03.200 --> 03:33:06.800]   According to the strength that we have defined.
[03:33:06.800 --> 03:33:13.280]   Add noise.
[03:33:13.280 --> 03:33:26.240]   Basically the sampler will create.
[03:33:26.240 --> 03:33:27.280]   By setting the strength.
[03:33:27.280 --> 03:33:29.440]   Will create a time step schedule.
[03:33:29.440 --> 03:33:30.800]   Later we will see it.
[03:33:30.800 --> 03:33:33.600]   And by defining this time step schedule.
[03:33:33.600 --> 03:33:34.320]   It will.
[03:33:34.320 --> 03:33:35.200]   We will start.
[03:33:35.200 --> 03:33:37.920]   What is the initial noise level we will start with.
[03:33:37.920 --> 03:33:39.680]   Because if we set the noise level to be.
[03:33:39.680 --> 03:33:41.280]   For example the strength to be one.
[03:33:41.280 --> 03:33:43.440]   We will start with the maximum noise level.
[03:33:43.440 --> 03:33:46.480]   But if we set the strength to be 0.5.
[03:33:46.480 --> 03:33:48.800]   We will start with half noise.
[03:33:48.800 --> 03:33:50.240]   Not all completely noise.
[03:33:50.240 --> 03:33:53.600]   And later this will be more clear.
[03:33:53.600 --> 03:33:55.360]   When we actually build the sampler.
[03:33:55.360 --> 03:33:56.800]   So now just remember that.
[03:33:56.800 --> 03:33:57.920]   We are exactly here.
[03:33:57.920 --> 03:33:59.120]   So we have the image.
[03:33:59.120 --> 03:34:00.080]   We transform.
[03:34:00.080 --> 03:34:02.240]   We compress it with the encoder.
[03:34:02.240 --> 03:34:03.120]   Became a latent.
[03:34:03.120 --> 03:34:04.640]   We added some noise to it.
[03:34:04.640 --> 03:34:06.080]   According to the strength level.
[03:34:06.080 --> 03:34:09.680]   And then we need to pass it to the model.
[03:34:09.680 --> 03:34:10.720]   To the diffusion model.
[03:34:10.720 --> 03:34:12.400]   So now we don't need the encoder anymore.
[03:34:12.400 --> 03:34:15.440]   We can set it to the idle device.
[03:34:15.440 --> 03:34:19.760]   If the user didn't specify any image.
[03:34:19.760 --> 03:34:22.480]   Then how can we start the denoising?
[03:34:22.480 --> 03:34:24.880]   It means that we want to do text to image.
[03:34:24.880 --> 03:34:27.280]   So we start with random noise.
[03:34:27.280 --> 03:34:28.800]   So we start with random noise.
[03:34:28.800 --> 03:34:31.040]   Let's sample some random noise then.
[03:34:31.520 --> 03:34:42.400]   Generator and device is device.
[03:34:42.400 --> 03:34:45.440]   So let me write some comments.
[03:34:45.440 --> 03:34:49.680]   If we are doing text to image.
[03:34:49.680 --> 03:34:52.160]   Start with random noise.
[03:34:52.160 --> 03:34:56.480]   Random noise defined as N01.
[03:34:56.480 --> 03:34:58.560]   Or N0i actually.
[03:34:58.880 --> 03:35:05.920]   And we then finally load the diffusion model.
[03:35:05.920 --> 03:35:06.880]   Which is our unit.
[03:35:06.880 --> 03:35:09.200]   Diffusion.
[03:35:09.200 --> 03:35:10.080]   It's models.
[03:35:10.080 --> 03:35:12.160]   Diffusion.
[03:35:12.160 --> 03:35:14.640]   Later we see what is this model and how to load it.
[03:35:14.640 --> 03:35:18.640]   We take it to our device where we are working.
[03:35:18.640 --> 03:35:19.840]   So for example CUDA.
[03:35:19.840 --> 03:35:24.880]   And then our sampler will define some time steps.
[03:35:24.880 --> 03:35:27.200]   Time steps basically means that.
[03:35:27.840 --> 03:35:31.680]   As you remember to train the model we have maximum of 1000 time steps.
[03:35:31.680 --> 03:35:34.640]   But when we inference we don't need to do 1001 steps.
[03:35:34.640 --> 03:35:38.400]   In our case we will be doing for example 50 steps of inferencing.
[03:35:38.400 --> 03:35:41.840]   If the maximum strength level is 1000.
[03:35:41.840 --> 03:35:46.400]   For example if the maximum level is 1000.
[03:35:46.400 --> 03:35:48.720]   The minimum level will be 1.
[03:35:48.720 --> 03:35:50.720]   Or if the maximum level is 999.
[03:35:50.720 --> 03:35:52.320]   The minimum will be 0.
[03:35:52.320 --> 03:35:54.960]   And this is a linear time steps.
[03:35:54.960 --> 03:35:57.760]   If we do only 50 it means that we need to do.
[03:35:57.760 --> 03:35:59.920]   For example we start with 1000.
[03:35:59.920 --> 03:36:01.760]   And then we do every 20.
[03:36:01.760 --> 03:36:03.280]   So 980.
[03:36:03.280 --> 03:36:04.480]   Then 960.
[03:36:04.480 --> 03:36:05.680]   940.
[03:36:05.680 --> 03:36:07.040]   920.
[03:36:07.040 --> 03:36:07.680]   900.
[03:36:07.680 --> 03:36:09.280]   Then 800.
[03:36:09.280 --> 03:36:11.920]   What?
[03:36:11.920 --> 03:36:12.880]   880.
[03:36:12.880 --> 03:36:14.800]   860.
[03:36:14.800 --> 03:36:15.840]   840.
[03:36:15.840 --> 03:36:17.680]   820 etc etc.
[03:36:17.680 --> 03:36:19.280]   Until we arrive to the 0th level.
[03:36:19.280 --> 03:36:24.240]   Basically each of these time steps indicates a noise level.
[03:36:24.240 --> 03:36:29.520]   So when we denoise the image.
[03:36:29.520 --> 03:36:32.400]   Or the initial noise in case we are doing the text to image.
[03:36:32.400 --> 03:36:36.000]   We can tell the scheduler to remove noise.
[03:36:36.000 --> 03:36:38.480]   According to particular time steps.
[03:36:38.480 --> 03:36:41.440]   Which are defined by how many inference steps we want.
[03:36:41.440 --> 03:36:44.320]   And this is exactly what we are going to do now.
[03:36:44.320 --> 03:36:47.200]   When we initialize the sampler.
[03:36:47.200 --> 03:36:49.520]   We tell him how many steps we want to do.
[03:36:49.520 --> 03:36:52.720]   And he will create this time step schedule.
[03:36:52.720 --> 03:36:54.640]   So according to how many we want.
[03:36:54.640 --> 03:36:56.640]   And now we just go through it.
[03:36:56.640 --> 03:36:58.480]   So we tell the time steps.
[03:36:58.480 --> 03:37:02.960]   We create tqdm which is a progress bar.
[03:37:02.960 --> 03:37:04.080]   We take the time steps.
[03:37:04.080 --> 03:37:10.000]   And for each of these time steps we denoise the image.
[03:37:10.000 --> 03:37:18.560]   So we have 1300.
[03:37:18.560 --> 03:37:19.680]   This is our...
[03:37:19.680 --> 03:37:22.560]   We need to tell the unit as you remember diffusion.
[03:37:23.200 --> 03:37:26.560]   The unit has as input the time embedding.
[03:37:26.560 --> 03:37:29.520]   So what is the time step we want to denoise.
[03:37:29.520 --> 03:37:32.000]   The context which is the prompt.
[03:37:32.000 --> 03:37:35.040]   Or in case we are doing a classifier free guidance.
[03:37:35.040 --> 03:37:36.560]   Also the unconditional prompt.
[03:37:36.560 --> 03:37:38.400]   And the latent.
[03:37:38.400 --> 03:37:40.400]   The current state of the latent.
[03:37:40.400 --> 03:37:42.320]   Because we will start with some latent.
[03:37:42.320 --> 03:37:43.600]   And then keep denoising it.
[03:37:43.600 --> 03:37:44.800]   And keep denoising it.
[03:37:44.800 --> 03:37:47.360]   Keep denoising it according to the time embedding.
[03:37:47.360 --> 03:37:48.160]   To the time step.
[03:37:48.160 --> 03:37:51.280]   So we calculate first the time embedding.
[03:37:51.280 --> 03:37:53.680]   Which is an embedding of the current time step.
[03:37:53.680 --> 03:37:58.160]   And we will obtain it from this function.
[03:37:58.160 --> 03:38:02.240]   Later we define it.
[03:38:02.240 --> 03:38:04.480]   This function basically will convert a number.
[03:38:04.480 --> 03:38:06.400]   So the time step into a vector.
[03:38:06.400 --> 03:38:10.000]   One of size 320.
[03:38:10.000 --> 03:38:12.320]   That describes this particular time step.
[03:38:12.320 --> 03:38:15.200]   And as you will see later.
[03:38:15.200 --> 03:38:18.000]   It's basically just equal to the positional encoding.
[03:38:18.000 --> 03:38:20.400]   That we did for the transformer model.
[03:38:20.400 --> 03:38:21.520]   So in the transformer model.
[03:38:21.520 --> 03:38:22.960]   We use the sines and the cosines.
[03:38:22.960 --> 03:38:24.400]   To define the position.
[03:38:24.400 --> 03:38:25.760]   Here we use the sines and cosines.
[03:38:25.760 --> 03:38:26.800]   To define the time step.
[03:38:26.800 --> 03:38:30.480]   And let's build the model input.
[03:38:30.480 --> 03:38:31.440]   Which is the latency.
[03:38:31.440 --> 03:38:36.880]   Which is of shape patch size 4.
[03:38:36.880 --> 03:38:40.800]   Because it's the input of the encoder.
[03:38:40.800 --> 03:38:42.080]   Of the variational autoencoder.
[03:38:42.080 --> 03:38:43.760]   Which is of size 4.
[03:38:43.760 --> 03:38:47.520]   Sorry, which has four channels.
[03:38:48.160 --> 03:38:50.320]   And then has latency height.
[03:38:50.320 --> 03:38:54.240]   Height and the latency width.
[03:38:54.240 --> 03:38:56.000]   Which is 64 by 64.
[03:38:56.000 --> 03:39:01.760]   Now if we do this one.
[03:39:01.760 --> 03:39:04.080]   We need to send.
[03:39:04.080 --> 03:39:06.640]   Basically we are sending the conditioned.
[03:39:06.640 --> 03:39:08.240]   Where is it?
[03:39:08.240 --> 03:39:09.380]   Here.
[03:39:09.380 --> 03:39:12.480]   We send the conditional input.
[03:39:12.480 --> 03:39:14.080]   But also the unconditional input.
[03:39:14.080 --> 03:39:16.000]   If we do the classifier free guidance.
[03:39:16.000 --> 03:39:17.760]   Which means that we need to send.
[03:39:17.760 --> 03:39:20.320]   The same latent with the prompt.
[03:39:20.320 --> 03:39:21.840]   And without the prompt.
[03:39:21.840 --> 03:39:23.680]   And so what we can do is.
[03:39:23.680 --> 03:39:25.760]   We can repeat this latent twice.
[03:39:25.760 --> 03:39:27.600]   If we are doing the classifier free guidance.
[03:39:27.600 --> 03:39:32.080]   It will become model input.
[03:39:32.080 --> 03:39:33.760]   By repeat.
[03:39:33.760 --> 03:39:37.120]   On one.
[03:39:37.120 --> 03:39:38.640]   This will basically transform.
[03:39:38.640 --> 03:39:42.400]   Batch size 4.
[03:39:43.120 --> 03:39:57.120]   So this is going to be twice.
[03:39:57.120 --> 03:39:58.960]   The size of the initial batch size.
[03:39:58.960 --> 03:40:00.000]   Which is one actually.
[03:40:00.000 --> 03:40:02.720]   And four channels.
[03:40:02.720 --> 03:40:04.960]   And latency height and latency width.
[03:40:04.960 --> 03:40:07.280]   So basically we are repeating this dimension twice.
[03:40:07.280 --> 03:40:10.720]   We are making two copies of the latency.
[03:40:10.720 --> 03:40:12.320]   One will be used with the prompt.
[03:40:12.320 --> 03:40:13.520]   One without the prompt.
[03:40:13.520 --> 03:40:16.160]   So now we do.
[03:40:16.160 --> 03:40:18.000]   We check the model output.
[03:40:18.000 --> 03:40:19.120]   What is the model output?
[03:40:19.120 --> 03:40:22.000]   It is the predicted noise by the unit.
[03:40:22.000 --> 03:40:24.080]   So the model output is.
[03:40:24.080 --> 03:40:28.400]   The predicted noise by the unit.
[03:40:28.400 --> 03:40:33.120]   We do diffusion.
[03:40:33.120 --> 03:40:36.160]   Model input.
[03:40:38.720 --> 03:40:40.720]   Context and time embedding.
[03:40:40.720 --> 03:40:45.920]   And if we do classifier free guidance.
[03:40:45.920 --> 03:40:49.200]   We need to combine the conditional output.
[03:40:49.200 --> 03:40:50.720]   And the unconditional output.
[03:40:50.720 --> 03:40:54.640]   Because we are passing the input of the model.
[03:40:54.640 --> 03:40:56.080]   If we are doing classifier free guidance.
[03:40:56.080 --> 03:40:57.760]   We are giving a batch size of two.
[03:40:57.760 --> 03:40:59.440]   The model will produce an output.
[03:40:59.440 --> 03:41:01.040]   That has batch size of two.
[03:41:01.040 --> 03:41:06.160]   So we can then split it into two different tensor.
[03:41:06.160 --> 03:41:08.000]   One will be the conditional.
[03:41:08.000 --> 03:41:10.000]   And one will be the unconditional.
[03:41:10.000 --> 03:41:12.400]   So the output conditional.
[03:41:12.400 --> 03:41:14.160]   And the output unconditional.
[03:41:14.160 --> 03:41:16.560]   Are split in this way.
[03:41:16.560 --> 03:41:17.360]   Using chunk.
[03:41:17.360 --> 03:41:22.480]   The dimension is along the 0th dimension.
[03:41:22.480 --> 03:41:24.400]   So by default it's the 0th dimension.
[03:41:24.400 --> 03:41:29.600]   And then we combine them according to this formula here.
[03:41:29.600 --> 03:41:30.960]   Where is the.
[03:41:30.960 --> 03:41:34.000]   I missed out.
[03:41:34.000 --> 03:41:36.000]   According to.
[03:41:37.600 --> 03:41:38.800]   This formula here.
[03:41:38.800 --> 03:41:40.720]   So unconditional output.
[03:41:40.720 --> 03:41:41.840]   Minus the sorry.
[03:41:41.840 --> 03:41:42.880]   The conditioned output.
[03:41:42.880 --> 03:41:44.400]   Minus the unconditioned output.
[03:41:44.400 --> 03:41:46.960]   Multiplied by the scale that we defined.
[03:41:46.960 --> 03:41:49.600]   Plus the unconditioned output.
[03:41:49.600 --> 03:41:51.920]   So the model output.
[03:41:51.920 --> 03:41:55.280]   Will be conditioned scale.
[03:41:55.280 --> 03:41:58.160]   Multiplied by the output.
[03:41:58.160 --> 03:42:00.880]   Conditioned minus the output.
[03:42:00.880 --> 03:42:03.360]   Unconditioned plus the output.
[03:42:03.360 --> 03:42:04.880]   Unconditioned.
[03:42:06.000 --> 03:42:10.000]   And then what we do is basically.
[03:42:10.000 --> 03:42:13.520]   Okay now comes the let's say the clue part.
[03:42:13.520 --> 03:42:16.000]   So we have a model.
[03:42:16.000 --> 03:42:20.560]   That is able to predict the noise in the current latency.
[03:42:20.560 --> 03:42:21.680]   So we start for example.
[03:42:21.680 --> 03:42:23.600]   Imagine we are doing text to image.
[03:42:23.600 --> 03:42:25.760]   So let me go back here.
[03:42:25.760 --> 03:42:32.080]   We are going text to images.
[03:42:32.080 --> 03:42:35.120]   Here.
[03:42:35.120 --> 03:42:36.960]   So we start with some random noise.
[03:42:36.960 --> 03:42:40.240]   And we transform into latency.
[03:42:40.240 --> 03:42:43.200]   Then according to some scheduler.
[03:42:43.200 --> 03:42:44.800]   According to some time step.
[03:42:44.800 --> 03:42:46.240]   We keep denoising it.
[03:42:46.240 --> 03:42:47.200]   Now our unit.
[03:42:47.200 --> 03:42:52.640]   Will predict the noise in the latency.
[03:42:52.640 --> 03:42:56.480]   But how can we remove this noise.
[03:42:56.480 --> 03:42:59.440]   From the image to obtain a less noisy image.
[03:42:59.440 --> 03:43:02.080]   This is done by the scheduler.
[03:43:02.720 --> 03:43:05.280]   So at each step we ask the unit.
[03:43:05.280 --> 03:43:07.360]   How much noise is in the image.
[03:43:07.360 --> 03:43:08.320]   We remove it.
[03:43:08.320 --> 03:43:10.160]   And then we give it again to the unit.
[03:43:10.160 --> 03:43:11.840]   And ask how much noise is there.
[03:43:11.840 --> 03:43:12.720]   And we remove it.
[03:43:12.720 --> 03:43:14.800]   And then ask again how much noise is there.
[03:43:14.800 --> 03:43:15.680]   And then we remove it.
[03:43:15.680 --> 03:43:17.040]   And then how much noise is there.
[03:43:17.040 --> 03:43:18.000]   And then we remove it.
[03:43:18.000 --> 03:43:20.320]   Until we finish all these time steps.
[03:43:20.320 --> 03:43:23.280]   After we have finished these time steps.
[03:43:23.280 --> 03:43:25.200]   We take the latent.
[03:43:25.200 --> 03:43:26.320]   Give it to the decoder.
[03:43:26.320 --> 03:43:28.080]   Which will build our image.
[03:43:28.080 --> 03:43:30.320]   And this is exactly what we are doing here.
[03:43:30.320 --> 03:43:32.240]   So imagine we don't have any input image.
[03:43:32.240 --> 03:43:34.000]   So we have some random noise.
[03:43:34.000 --> 03:43:37.520]   We define some time steps on this sampler.
[03:43:37.520 --> 03:43:40.800]   Based on how many inference steps we want to do.
[03:43:40.800 --> 03:43:43.200]   We do all this time step.
[03:43:43.200 --> 03:43:46.240]   We give the latency to the unit.
[03:43:46.240 --> 03:43:48.960]   The unit will tell us how much is the predicted noise.
[03:43:48.960 --> 03:43:50.960]   But then we need to remove this noise.
[03:43:50.960 --> 03:43:51.920]   So let's do it.
[03:43:51.920 --> 03:43:53.520]   So let's remove this noise.
[03:43:53.520 --> 03:43:57.680]   So the latency are equal to sampler dot step.
[03:43:58.320 --> 03:44:03.040]   Time step, latency, model, output.
[03:44:03.040 --> 03:44:11.120]   This basically means take the image from a more noisy version.
[03:44:11.120 --> 03:44:14.000]   Okay, let me write it better.
[03:44:14.000 --> 03:44:18.240]   Remove noise predicted by the unit.
[03:44:18.240 --> 03:44:21.520]   Unit, okay.
[03:44:21.520 --> 03:44:26.400]   And this is our loop of denoising.
[03:44:26.400 --> 03:44:29.040]   Then we can do to idle, diffusion.
[03:44:29.040 --> 03:44:33.760]   Now we have our denoised image.
[03:44:33.760 --> 03:44:35.520]   Because we have done it for many steps.
[03:44:35.520 --> 03:44:39.600]   Now what we do is we load the decoder.
[03:44:39.600 --> 03:44:42.960]   Which is models decoder.
[03:44:42.960 --> 03:44:53.840]   And then our image is run through the decoder.
[03:44:55.760 --> 03:44:57.920]   So we run the latency through the decoder.
[03:44:57.920 --> 03:44:59.120]   So we do this step here.
[03:44:59.120 --> 03:45:01.040]   So we run this latency through the decoder.
[03:45:01.040 --> 03:45:02.080]   This will give the image.
[03:45:02.080 --> 03:45:04.880]   It actually will be only one image.
[03:45:04.880 --> 03:45:06.640]   Because we only specify one image.
[03:45:06.640 --> 03:45:13.360]   Then we do images is equal to.
[03:45:13.360 --> 03:45:16.960]   Because the image was initially, as you remember here.
[03:45:16.960 --> 03:45:18.000]   It was rescaled.
[03:45:18.000 --> 03:45:22.960]   So from 0 to 255 in a new scale.
[03:45:23.520 --> 03:45:25.840]   That is between -1 and +1.
[03:45:25.840 --> 03:45:27.120]   Now we do the opposite step.
[03:45:27.120 --> 03:45:29.840]   So rescale again.
[03:45:29.840 --> 03:45:32.800]   From -1 to 1.
[03:45:32.800 --> 03:45:36.320]   Into 0 to 255.
[03:45:36.320 --> 03:45:37.920]   With clamp equal true.
[03:45:37.920 --> 03:45:40.800]   Later we will see this function.
[03:45:40.800 --> 03:45:41.440]   It's very easy.
[03:45:41.440 --> 03:45:42.880]   It's just a rescaling function.
[03:45:42.880 --> 03:45:45.120]   We permute.
[03:45:45.120 --> 03:45:47.600]   Because to save the image on the CPU.
[03:45:47.600 --> 03:45:49.840]   We want the channel dimension to be the last one.
[03:45:49.840 --> 03:45:52.080]   Permute.
[03:45:52.400 --> 03:45:56.320]   From 0 to 3.1.
[03:45:56.320 --> 03:45:59.040]   So this one basically will take the batch size.
[03:45:59.040 --> 03:46:04.320]   Channel height width.
[03:46:04.320 --> 03:46:09.680]   Into batch size.
[03:46:09.680 --> 03:46:15.920]   Height width channel.
[03:46:15.920 --> 03:46:21.760]   And then we move the image to the CPU.
[03:46:21.840 --> 03:46:34.400]   And then we need to convert it into a NumPy array.
[03:46:34.400 --> 03:46:35.680]   And then we return the image.
[03:46:35.680 --> 03:46:38.320]   Voila!
[03:46:38.320 --> 03:46:40.080]   Let's build this rescale method.
[03:46:40.080 --> 03:46:47.760]   So what is the old scale?
[03:46:47.760 --> 03:46:48.320]   Old range.
[03:46:48.320 --> 03:46:51.520]   What is the new range?
[03:46:51.920 --> 03:46:52.640]   And the clamp.
[03:46:52.640 --> 03:46:58.640]   So let's define the old minimum.
[03:46:58.640 --> 03:47:00.880]   Old maximum is the old range.
[03:47:00.880 --> 03:47:04.160]   New minimum and new maximum.
[03:47:04.160 --> 03:47:07.680]   New range.
[03:47:07.680 --> 03:47:13.360]   Minus equal to old min.
[03:47:14.720 --> 03:47:20.800]   x multiply equal to new max minus new min.
[03:47:20.800 --> 03:47:28.000]   Divided by old max minus old min.
[03:47:28.000 --> 03:47:31.520]   x plus equal to new min.
[03:47:31.520 --> 03:47:33.600]   We are just rescaling.
[03:47:33.600 --> 03:47:37.040]   So convert something that is within this range into this range.
[03:47:37.040 --> 03:47:40.880]   And if it's clamp.
[03:47:42.240 --> 03:47:44.240]   Then x is equal to x.clamp.
[03:47:44.240 --> 03:47:46.880]   New min.
[03:47:46.880 --> 03:47:49.280]   New max.
[03:47:49.280 --> 03:47:51.680]   And then we return x.
[03:47:51.680 --> 03:47:55.520]   Then we have the time embedding.
[03:47:55.520 --> 03:47:58.320]   The method that we didn't define here.
[03:47:58.320 --> 03:47:59.760]   This getTimeEmbedding.
[03:47:59.760 --> 03:48:02.640]   This means basically take the time step which is a number.
[03:48:02.640 --> 03:48:04.640]   So which is an integer.
[03:48:04.640 --> 03:48:08.320]   And convert it into a vector of size 320.
[03:48:08.320 --> 03:48:12.320]   And this will be done exactly using the same system that we use for the transformer.
[03:48:12.320 --> 03:48:13.520]   For the positional embeddings.
[03:48:13.520 --> 03:48:23.440]   So we first define the frequencies of our cosines and the sines.
[03:48:23.440 --> 03:48:26.240]   Exactly using the same formula of the transformer.
[03:48:26.240 --> 03:48:30.560]   So if you remember the formula is equal to the 10,000.
[03:48:30.560 --> 03:48:33.840]   1 over 10,000 to the power of something.
[03:48:33.840 --> 03:48:34.240]   Of i.
[03:48:34.240 --> 03:48:35.200]   I remember correctly.
[03:48:37.840 --> 03:48:45.040]   So it's power of 10,000 and minus torch.range.
[03:48:45.040 --> 03:48:48.000]   So I am referring to this formula just in case you forgot.
[03:48:48.000 --> 03:48:51.440]   Let me find it using the slides.
[03:48:51.440 --> 03:48:56.480]   I am talking about this formula here.
[03:48:56.480 --> 03:48:59.360]   So the formula that defines the positional encodings here.
[03:48:59.920 --> 03:49:02.960]   Here we just use a different dimension of the embedding.
[03:49:02.960 --> 03:49:18.480]   This one will produce something that is 160 numbers.
[03:49:18.480 --> 03:49:25.760]   And this one will produce something that is 200 numbers.
[03:49:25.760 --> 03:49:27.280]   And 160 numbers.
[03:49:27.280 --> 03:49:31.680]   Then we multiply it.
[03:49:31.680 --> 03:49:35.840]   We multiply it with the time step.
[03:49:35.840 --> 03:49:39.040]   So we create a shape of size 1.
[03:49:39.040 --> 03:49:44.480]   So x is equal to torch dot tensor.
[03:49:44.480 --> 03:49:46.640]   Which is a single time step.
[03:49:46.640 --> 03:49:49.200]   Of t type.
[03:49:49.200 --> 03:49:57.520]   Take everything.
[03:49:57.520 --> 03:49:59.520]   We add one dimension.
[03:49:59.520 --> 03:50:02.160]   So we add one dimension here.
[03:50:02.160 --> 03:50:03.920]   This is like doing an unsqueeze.
[03:50:03.920 --> 03:50:06.720]   Multiply by the frequencies.
[03:50:06.720 --> 03:50:13.920]   And then we multiply this by the sines and the cosine.
[03:50:13.920 --> 03:50:16.160]   Just like we did in the original transformer.
[03:50:16.880 --> 03:50:20.960]   This one will return a tensor of size 100 by 62.
[03:50:20.960 --> 03:50:22.480]   So which is 320.
[03:50:22.480 --> 03:50:36.480]   Because we are concatenating two tensors.
[03:50:36.480 --> 03:50:38.080]   Not cosine, but sine of x.
[03:50:38.080 --> 03:50:42.000]   And then I concatenated along the dimension.
[03:50:42.000 --> 03:50:47.040]   The last dimension.
[03:50:47.040 --> 03:50:49.680]   And this is our time embedding.
[03:50:49.680 --> 03:50:52.000]   So now let's review what we have built here.
[03:50:52.000 --> 03:50:55.360]   We built basically a system.
[03:50:55.360 --> 03:50:58.000]   A method that takes the prompt.
[03:50:58.000 --> 03:50:59.280]   The unconditional prompt.
[03:50:59.280 --> 03:51:01.360]   Also called the negative prompt.
[03:51:01.360 --> 03:51:04.640]   The prompt or empty string.
[03:51:04.640 --> 03:51:07.840]   Because if we don't want to use any negative prompt.
[03:51:07.840 --> 03:51:08.800]   The input image.
[03:51:08.800 --> 03:51:10.960]   So what is the image we want to start from.
[03:51:10.960 --> 03:51:13.280]   In case we want to do an image to image.
[03:51:13.280 --> 03:51:17.600]   The strength is how much attention we want to pay to this input image.
[03:51:17.600 --> 03:51:18.800]   When we denoise the image.
[03:51:18.800 --> 03:51:22.160]   Or how much noise we want to add to it basically.
[03:51:22.160 --> 03:51:24.800]   And the more noise we add.
[03:51:24.800 --> 03:51:27.760]   The less the output will resemble the input image.
[03:51:27.760 --> 03:51:31.120]   If we want to do classifier free guidance.
[03:51:31.120 --> 03:51:34.640]   Which means that if we want the model to output to output.
[03:51:34.640 --> 03:51:36.960]   One is the output with the prompt.
[03:51:36.960 --> 03:51:38.320]   And one without the prompt.
[03:51:38.320 --> 03:51:42.880]   And then we can adjust how much we want to pay attention to the prompt.
[03:51:42.880 --> 03:51:44.160]   According to this scale.
[03:51:44.160 --> 03:51:48.240]   And then we defined the scheduler.
[03:51:48.240 --> 03:51:48.880]   Which is only one.
[03:51:48.880 --> 03:51:49.440]   The DPM.
[03:51:49.440 --> 03:51:50.560]   And we will define it now.
[03:51:50.560 --> 03:51:52.880]   And how many steps we want to do.
[03:51:52.880 --> 03:51:55.280]   The first thing we do is we create a generator.
[03:51:55.280 --> 03:51:57.120]   Which is just a random number generator.
[03:51:57.120 --> 03:51:59.680]   Then the second thing we do is.
[03:51:59.680 --> 03:52:01.520]   If we want to do classifier free guidance.
[03:52:01.520 --> 03:52:02.960]   As we need to do the.
[03:52:02.960 --> 03:52:05.600]   Basically we need to go through the units twice.
[03:52:05.600 --> 03:52:06.560]   One with the prompt.
[03:52:06.560 --> 03:52:07.760]   One without the prompt.
[03:52:07.760 --> 03:52:09.040]   The thing we do is that actually.
[03:52:09.040 --> 03:52:11.120]   We create a batch size of two.
[03:52:11.120 --> 03:52:12.400]   One with the prompt.
[03:52:12.400 --> 03:52:13.840]   And one without the prompt.
[03:52:13.840 --> 03:52:15.760]   Or using the unconditioned prompt.
[03:52:15.760 --> 03:52:16.800]   Or the negative prompt.
[03:52:16.800 --> 03:52:20.880]   In case we don't do the classifier free guidance.
[03:52:20.880 --> 03:52:22.480]   We only build one tensor.
[03:52:22.480 --> 03:52:23.840]   That only includes the prompt.
[03:52:23.840 --> 03:52:26.960]   The second thing we do is we load.
[03:52:26.960 --> 03:52:28.240]   If there is an input image.
[03:52:28.240 --> 03:52:29.200]   We load it.
[03:52:29.200 --> 03:52:31.120]   So instead of starting from random noise.
[03:52:31.120 --> 03:52:32.400]   We start from an image.
[03:52:32.400 --> 03:52:34.560]   Which is to which we add the noise.
[03:52:34.560 --> 03:52:36.320]   According to the strength we have defined.
[03:52:37.200 --> 03:52:39.120]   Then for the number of steps.
[03:52:39.120 --> 03:52:40.400]   Defined by the sampler.
[03:52:40.400 --> 03:52:41.520]   Which are actually defined.
[03:52:41.520 --> 03:52:42.880]   By the number of inference steps.
[03:52:42.880 --> 03:52:43.840]   We have defined here.
[03:52:43.840 --> 03:52:45.360]   We do a loop.
[03:52:45.360 --> 03:52:46.000]   A for loop.
[03:52:46.000 --> 03:52:48.240]   That for each for loop.
[03:52:48.240 --> 03:52:50.720]   Let me go here.
[03:52:50.720 --> 03:52:53.360]   The unit will predict some noise.
[03:52:53.360 --> 03:52:56.240]   And the scheduler will remove this noise.
[03:52:56.240 --> 03:52:57.760]   And give a new latent.
[03:52:57.760 --> 03:53:00.080]   Then this new latent is fed again to the unit.
[03:53:00.080 --> 03:53:01.520]   Which will predict some noise.
[03:53:01.520 --> 03:53:02.880]   And we remove this noise.
[03:53:02.880 --> 03:53:04.320]   According to the scheduler.
[03:53:04.320 --> 03:53:06.080]   Then we again predict some noise.
[03:53:06.080 --> 03:53:07.680]   And we remove some noise.
[03:53:07.680 --> 03:53:09.280]   The only thing we need to understand.
[03:53:09.280 --> 03:53:11.920]   Is how we remove the noise from the image now.
[03:53:11.920 --> 03:53:14.480]   Because we know that the unit is trained.
[03:53:14.480 --> 03:53:15.440]   To predict the noise.
[03:53:15.440 --> 03:53:16.960]   But how do we actually remove it?
[03:53:16.960 --> 03:53:20.320]   And this is the job of the scheduler.
[03:53:20.320 --> 03:53:23.200]   So now we need to go build this scheduler here.
[03:53:23.200 --> 03:53:24.960]   So let's go build it.
[03:53:24.960 --> 03:53:28.400]   Let's start building our ddpm scheduler.
[03:53:28.400 --> 03:53:30.240]   So ddpm.py
[03:53:30.240 --> 03:53:33.840]   Oops I forgot to put it inside the folder.
[03:53:35.680 --> 03:53:37.920]   And let me review one thing.
[03:53:37.920 --> 03:53:39.860]   Yeah.
[03:53:39.860 --> 03:53:43.280]   This is wrong.
[03:53:43.280 --> 03:53:43.780]   Okay.
[03:53:43.780 --> 03:53:46.800]   So import torch.
[03:53:46.800 --> 03:53:49.840]   Import roompy.
[03:53:49.840 --> 03:53:55.120]   And let's create the class ddpm sampler.
[03:53:55.120 --> 03:53:56.880]   Okay I didn't call it scheduler.
[03:53:56.880 --> 03:54:00.320]   Because I don't want you to be confused with the beta schedule.
[03:54:00.320 --> 03:54:01.760]   Which we will define later.
[03:54:01.760 --> 03:54:04.560]   So I call it scheduler here.
[03:54:05.520 --> 03:54:06.960]   Oops why I open this one.
[03:54:06.960 --> 03:54:09.600]   I call it scheduler here.
[03:54:09.600 --> 03:54:12.240]   But actually I mean the sampler.
[03:54:12.240 --> 03:54:15.360]   Because there is the beta schedule that we will define now.
[03:54:15.360 --> 03:54:16.400]   What is the beta schedule?
[03:54:16.400 --> 03:54:19.840]   Which indicates the amount of noise at each time step.
[03:54:19.840 --> 03:54:23.040]   And then there is what is known as the scheduler or the sampler.
[03:54:23.040 --> 03:54:26.400]   From now on I will refer it to as sampler.
[03:54:26.400 --> 03:54:29.200]   So this scheduler here actually means a sampler.
[03:54:29.200 --> 03:54:30.480]   I'm sorry for the confusion.
[03:54:30.480 --> 03:54:33.040]   I will update the slides when the video is out.
[03:54:33.520 --> 03:54:36.320]   So how much were the training steps?
[03:54:36.320 --> 03:54:39.520]   Which is 1000.
[03:54:39.520 --> 03:54:44.560]   The beta is okay now I define two constants.
[03:54:44.560 --> 03:54:46.560]   And later I define them.
[03:54:46.560 --> 03:54:48.800]   Where what are they and where they come from?
[03:54:48.800 --> 03:54:53.920]   0, 85 and beta end.
[03:54:53.920 --> 03:54:55.920]   And I define the sampler.
[03:54:55.920 --> 03:54:57.600]   So this is the sampler.
[03:54:57.600 --> 03:54:58.720]   And this is the scheduler.
[03:54:58.720 --> 03:55:00.080]   And this is the sampler.
[03:55:00.080 --> 03:55:01.120]   And this is the scheduler.
[03:55:01.120 --> 03:55:06.640]   And beta end is a starting point of 0.0120.
[03:55:06.640 --> 03:55:09.060]   Okay.
[03:55:09.060 --> 03:55:11.840]   The parameter beta start and beta end.
[03:55:11.840 --> 03:55:13.200]   Basically if you go to the paper.
[03:55:13.200 --> 03:55:16.160]   If you look at the forward process.
[03:55:16.160 --> 03:55:22.880]   We can see that the forward process is the process that makes the image more noisy.
[03:55:22.880 --> 03:55:24.800]   We add noise to the image.
[03:55:24.800 --> 03:55:28.640]   So given an image that don't have less noise.
[03:55:28.640 --> 03:55:30.400]   How to get a more noisy image?
[03:55:31.120 --> 03:55:33.840]   According to this Gaussian distribution.
[03:55:33.840 --> 03:55:36.720]   Which is actually a chain of Gaussian distribution.
[03:55:36.720 --> 03:55:39.520]   Which is called a Markov chain of Gaussian distribution.
[03:55:39.520 --> 03:55:46.720]   And the noise that we add varies according to a variance schedule.
[03:55:46.720 --> 03:55:49.840]   Beta 1, beta 2, beta 3, beta 4, beta t.
[03:55:49.840 --> 03:55:52.720]   So beta basically it's a series of numbers.
[03:55:52.720 --> 03:55:57.840]   That indicates the variance of the noise that we add with each of these steps.
[03:55:59.040 --> 03:56:03.440]   And as in the latent in the stable diffusion.
[03:56:03.440 --> 03:56:05.040]   They use a beta start.
[03:56:05.040 --> 03:56:09.120]   So the first value of beta is 0.0085.
[03:56:09.120 --> 03:56:10.560]   And the last variance.
[03:56:10.560 --> 03:56:14.960]   So this the beta that will turn the image into complete noise.
[03:56:14.960 --> 03:56:17.680]   Is equal to 0.0120.
[03:56:17.680 --> 03:56:19.920]   It's a choice made by the authors.
[03:56:19.920 --> 03:56:25.280]   And we will use a linear schedule.
[03:56:25.280 --> 03:56:26.880]   Actually there are other schedules.
[03:56:26.880 --> 03:56:29.200]   Which are for example the cosine schedule etc.
[03:56:29.200 --> 03:56:30.880]   But we will be using the linear one.
[03:56:30.880 --> 03:56:35.360]   And we need to define this beta schedule.
[03:56:35.360 --> 03:56:39.600]   Which is actually 1000 numbers between beta start and beta end.
[03:56:39.600 --> 03:56:42.320]   So let's do it.
[03:56:42.320 --> 03:56:48.240]   So this is defined using the linear space.
[03:56:48.240 --> 03:56:51.440]   Where the starting number is beta start.
[03:56:51.440 --> 03:56:54.560]   Actually to the square root of beta start.
[03:56:55.200 --> 03:56:57.680]   So square root of beta start.
[03:56:57.680 --> 03:57:01.600]   Because this is how they define it in the stable diffusion.
[03:57:01.600 --> 03:57:03.600]   If you check the official repository.
[03:57:03.600 --> 03:57:04.960]   They will also have these numbers.
[03:57:04.960 --> 03:57:06.640]   And define in exactly the same way.
[03:57:06.640 --> 03:57:11.920]   0.5 then the number of training steps.
[03:57:11.920 --> 03:57:16.000]   So in how many pieces we want to divide this linear space.
[03:57:16.000 --> 03:57:17.760]   Beta end.
[03:57:21.280 --> 03:57:26.720]   And then the type is torch dot float 32 I think.
[03:57:26.720 --> 03:57:28.560]   And then to the power of 2.
[03:57:28.560 --> 03:57:32.320]   Because they divide it into 1000.
[03:57:32.320 --> 03:57:33.360]   Then to the power of 2.
[03:57:33.360 --> 03:57:38.480]   This is in the diffusers libraries from Hugging Face.
[03:57:38.480 --> 03:57:40.960]   I think this is called the scaled linear schedule.
[03:57:40.960 --> 03:57:44.480]   Now we need to define other constants.
[03:57:44.480 --> 03:57:47.600]   That are needed for our forward and our backward process.
[03:57:47.600 --> 03:57:50.720]   So our forward process depends on this beta schedule.
[03:57:50.720 --> 03:57:53.120]   But actually this is only for the single step.
[03:57:53.120 --> 03:57:56.320]   So if we want to go from for example the original image.
[03:57:56.320 --> 03:57:58.800]   By one step forward of more noise.
[03:57:58.800 --> 03:58:00.480]   We need to apply this formula here.
[03:58:00.480 --> 03:58:03.920]   But there is a closed formula here.
[03:58:03.920 --> 03:58:05.920]   Called this one here.
[03:58:05.920 --> 03:58:08.320]   That allows you to go from the original image.
[03:58:08.320 --> 03:58:10.800]   To any noisified version of the image.
[03:58:10.800 --> 03:58:12.240]   At any time step.
[03:58:12.240 --> 03:58:13.920]   Between 0 and 1000.
[03:58:13.920 --> 03:58:15.920]   Using this one here.
[03:58:15.920 --> 03:58:18.000]   Which depends on alpha bar.
[03:58:18.000 --> 03:58:18.880]   That you can see here.
[03:58:18.880 --> 03:58:21.040]   So the square root of this alpha bar.
[03:58:21.040 --> 03:58:23.520]   And the variance also depends on this alpha bar.
[03:58:23.520 --> 03:58:24.720]   What is alpha bar?
[03:58:24.720 --> 03:58:27.600]   Alpha bar is the product of alpha.
[03:58:27.600 --> 03:58:29.920]   Going from 1 up to t.
[03:58:29.920 --> 03:58:31.120]   So if we are for example.
[03:58:31.120 --> 03:58:33.440]   We want to go from the time step 0.
[03:58:33.440 --> 03:58:35.440]   Which is the image without any noise.
[03:58:35.440 --> 03:58:37.040]   To the time step 10.
[03:58:37.040 --> 03:58:38.800]   Which is the image with some noise.
[03:58:38.800 --> 03:58:41.840]   And remember that time step 1000.
[03:58:41.840 --> 03:58:43.680]   Means that it's only noise.
[03:58:43.680 --> 03:58:46.240]   So we want to go to time step 10.
[03:58:46.240 --> 03:58:48.160]   Which means that we need to calculate.
[03:58:48.160 --> 03:58:50.880]   This As of 1.
[03:58:50.880 --> 03:58:51.600]   As 1.
[03:58:51.600 --> 03:58:52.160]   As 2.
[03:58:52.160 --> 03:58:52.560]   As 3.
[03:58:52.560 --> 03:58:53.040]   As 2.
[03:58:53.040 --> 03:58:54.720]   And up until As 10.
[03:58:54.720 --> 03:58:56.720]   And we multiply them together.
[03:58:56.720 --> 03:58:58.240]   This is the productory.
[03:58:58.240 --> 03:58:59.440]   And this A.
[03:58:59.440 --> 03:59:00.880]   What is this alpha?
[03:59:00.880 --> 03:59:03.200]   This alpha actually is 1 minus beta.
[03:59:03.200 --> 03:59:05.360]   So let's calculate this alphas first.
[03:59:05.360 --> 03:59:07.520]   So alpha is actually 1 minus beta.
[03:59:07.520 --> 03:59:12.160]   Beta self dot betas.
[03:59:12.160 --> 03:59:15.120]   So it becomes floating.
[03:59:16.080 --> 03:59:18.080]   And then we need to calculate.
[03:59:18.080 --> 03:59:19.760]   The product of this alphas.
[03:59:19.760 --> 03:59:21.120]   From 1 to t.
[03:59:21.120 --> 03:59:23.360]   And this is easily done with the PyTorch.
[03:59:23.360 --> 03:59:26.640]   We pre-compute them basically.
[03:59:26.640 --> 03:59:33.440]   This is also comprod self dot alphas.
[03:59:33.440 --> 03:59:37.840]   This will create basically an array.
[03:59:37.840 --> 03:59:40.080]   Where the first element is the first alpha.
[03:59:40.080 --> 03:59:42.480]   So alpha for example 0.
[03:59:42.480 --> 03:59:47.360]   The second element is alpha 0 multiplied by alpha 1.
[03:59:47.360 --> 03:59:49.840]   The third element is alpha 0.
[03:59:49.840 --> 03:59:52.240]   Multiplied by alpha 1.
[03:59:52.240 --> 03:59:54.320]   Multiplied by alpha 2 etc.
[03:59:54.320 --> 03:59:55.840]   So it's a cumulative product.
[03:59:55.840 --> 03:59:56.480]   It's we say.
[03:59:56.480 --> 03:59:59.760]   Then we create one tensor.
[03:59:59.760 --> 04:00:01.040]   That represents the number 1.
[04:00:01.040 --> 04:00:02.080]   And later we will use it.
[04:00:02.080 --> 04:00:07.280]   Tensor 1.0.
[04:00:07.280 --> 04:00:08.640]   Okay we save the generator.
[04:00:12.000 --> 04:00:13.760]   We save the number of training steps.
[04:00:13.760 --> 04:00:21.600]   And then we create the time step schedule.
[04:00:21.600 --> 04:00:23.840]   The time step basically.
[04:00:23.840 --> 04:00:27.680]   Because we want to reverse the noise.
[04:00:27.680 --> 04:00:28.720]   We want to remove noise.
[04:00:28.720 --> 04:00:31.600]   We will start from the more noisy to less noise.
[04:00:31.600 --> 04:00:34.000]   So we will go from 1000 to 0.
[04:00:34.000 --> 04:00:36.240]   Initially.
[04:00:36.240 --> 04:00:40.800]   So let's say time steps is equal to torch from.
[04:00:41.200 --> 04:00:52.560]   We reverse this.
[04:00:52.560 --> 04:00:54.320]   So this is from 0 to 1000.
[04:00:54.320 --> 04:00:56.240]   But actually we want 1000 to 0.
[04:00:56.240 --> 04:01:02.400]   And this is our initial schedule.
[04:01:02.400 --> 04:01:04.080]   In case we want to do 1000 steps.
[04:01:04.080 --> 04:01:07.040]   But later because here we actually specify.
[04:01:07.040 --> 04:01:10.080]   How many inference steps we want to do.
[04:01:10.080 --> 04:01:12.720]   We will change these time steps here.
[04:01:12.720 --> 04:01:15.200]   So if the user later specifies less than 1000.
[04:01:15.200 --> 04:01:15.920]   We will change it.
[04:01:15.920 --> 04:01:18.720]   So let's do it.
[04:01:18.720 --> 04:01:19.840]   We let's create the method.
[04:01:19.840 --> 04:01:22.960]   That will change this time steps.
[04:01:22.960 --> 04:01:25.840]   Based on how many actual steps we want to make.
[04:01:25.840 --> 04:01:29.680]   So set inference step.
[04:01:29.680 --> 04:01:32.720]   Time steps.
[04:01:32.720 --> 04:01:38.480]   As I said before.
[04:01:38.480 --> 04:01:39.920]   We usually perform 50.
[04:01:39.920 --> 04:01:42.480]   Which is also actually the one they use normally.
[04:01:42.480 --> 04:01:45.840]   For example in hugging face library.
[04:01:45.840 --> 04:01:50.160]   Let's save this value.
[04:01:50.160 --> 04:01:51.680]   Because we will need it later.
[04:01:51.680 --> 04:01:54.800]   Now if we have a number.
[04:01:54.800 --> 04:01:57.200]   For example we go from 1000.
[04:01:57.200 --> 04:01:58.320]   Actually it's not from.
[04:01:58.320 --> 04:02:00.560]   This is not from 0 to 1000.
[04:02:00.560 --> 04:02:02.720]   But it's from 0 to 1000 minus 1.
[04:02:02.720 --> 04:02:04.000]   Because this is excluded.
[04:02:04.000 --> 04:02:05.520]   So it will be from 99.
[04:02:06.080 --> 04:02:12.640]   999, 998, 997, 996 etc up to 0.
[04:02:12.640 --> 04:02:15.440]   So we have 1000 numbers.
[04:02:15.440 --> 04:02:17.040]   But we don't want 1000 numbers.
[04:02:17.040 --> 04:02:18.080]   We want less.
[04:02:18.080 --> 04:02:19.680]   We want 50 of them.
[04:02:19.680 --> 04:02:20.960]   So what we do is basically.
[04:02:20.960 --> 04:02:22.880]   We space them every 20.
[04:02:22.880 --> 04:02:25.280]   So we start with 999.
[04:02:25.280 --> 04:02:27.600]   Then 999 minus 20.
[04:02:27.600 --> 04:02:31.120]   Then 999 minus 40 etc etc.
[04:02:31.120 --> 04:02:32.320]   Until we arrive to 0.
[04:02:32.320 --> 04:02:37.120]   But in total here will be 1000 steps.
[04:02:37.120 --> 04:02:39.440]   And here will be 50 steps.
[04:02:39.440 --> 04:02:41.600]   Why minus 20?
[04:02:41.600 --> 04:02:44.800]   Because 20 is 1000 divided by 50.
[04:02:44.800 --> 04:02:45.920]   If i'm not mistaken.
[04:02:45.920 --> 04:02:51.120]   So this is exactly what we are going to do.
[04:02:51.120 --> 04:02:52.880]   So we calculate the step ratio.
[04:02:52.880 --> 04:02:56.160]   Which is self dot num training step.
[04:02:56.160 --> 04:02:57.920]   Divide by how many we actually want.
[04:02:58.400 --> 04:03:00.720]   And we redefine the time steps.
[04:03:00.720 --> 04:03:03.520]   According to how many we actually want to make.
[04:03:03.520 --> 04:03:15.440]   0 num inference steps.
[04:03:15.440 --> 04:03:17.200]   Multiply it by this step ratio.
[04:03:17.200 --> 04:03:20.720]   And round it.
[04:03:24.160 --> 04:03:27.680]   We reverse it just like before.
[04:03:27.680 --> 04:03:29.120]   Because this is from 0.
[04:03:29.120 --> 04:03:31.360]   So this is actually means 0.
[04:03:31.360 --> 04:03:32.160]   Then 20.
[04:03:32.160 --> 04:03:32.880]   Then 40.
[04:03:32.880 --> 04:03:34.480]   Then 60 etc.
[04:03:34.480 --> 04:03:36.080]   Until we reach 999.
[04:03:36.080 --> 04:03:37.200]   Then we reverse it.
[04:03:37.200 --> 04:03:40.720]   Then copy.
[04:03:40.720 --> 04:03:43.760]   S type.
[04:03:43.760 --> 04:03:48.560]   np dot int 64.
[04:03:48.560 --> 04:03:49.280]   So a long one.
[04:03:50.640 --> 04:03:54.240]   And then we define as tensor.
[04:03:54.240 --> 04:04:01.040]   Now the code looks very different from each other.
[04:04:01.040 --> 04:04:04.560]   Because actually I have been copying the code from multiple sources.
[04:04:04.560 --> 04:04:08.720]   Maybe one of them I think I copied from the HuggingFace library.
[04:04:08.720 --> 04:04:10.160]   So I didn't change it.
[04:04:10.160 --> 04:04:12.080]   I kept it to the original one.
[04:04:12.080 --> 04:04:13.760]   Okay.
[04:04:13.760 --> 04:04:16.160]   But the idea is the one I showed you before.
[04:04:16.160 --> 04:04:19.520]   So we copy the code from the HuggingFace library.
[04:04:19.520 --> 04:04:20.480]   I showed you before.
[04:04:20.480 --> 04:04:23.760]   So now we set the exact number of time steps we want.
[04:04:23.760 --> 04:04:26.800]   And we redefine this time steps array like this.
[04:04:26.800 --> 04:04:31.280]   Let's define the next method.
[04:04:31.280 --> 04:04:33.520]   Which basically tells us.
[04:04:33.520 --> 04:04:36.720]   Let's define the method on how to add noise to something.
[04:04:36.720 --> 04:04:38.400]   So imagine we have the image.
[04:04:38.400 --> 04:04:41.520]   As you remember to do image to image.
[04:04:41.520 --> 04:04:43.520]   We need to add noise to this latent.
[04:04:43.520 --> 04:04:45.840]   How do we add noise to something?
[04:04:45.840 --> 04:04:49.280]   Well we need to apply the formula as defined in the paper.
[04:04:49.280 --> 04:04:51.280]   Let's go in the paper here.
[04:04:51.280 --> 04:04:54.080]   We need to apply this formula here.
[04:04:54.080 --> 04:04:55.520]   And that's it.
[04:04:55.520 --> 04:04:57.680]   This means that given this image.
[04:04:57.680 --> 04:05:03.360]   I want to go to the noisified version of this image at time step t.
[04:05:03.360 --> 04:05:06.800]   Which means that I need to take.
[04:05:06.800 --> 04:05:11.600]   We need to have a sample from this Gaussian.
[04:05:11.600 --> 04:05:12.320]   But we don't.
[04:05:12.320 --> 04:05:13.680]   Okay.
[04:05:13.680 --> 04:05:14.320]   Let's build it.
[04:05:14.320 --> 04:05:17.840]   And we will apply the same trick that we did for the variational autoencoder.
[04:05:17.840 --> 04:05:19.920]   As you remember in the variational autoencoder.
[04:05:19.920 --> 04:05:23.040]   I actually already showed how we sample from a distribution.
[04:05:23.040 --> 04:05:25.440]   Of which we know the mean and the variance here.
[04:05:25.440 --> 04:05:26.640]   We will do the same here.
[04:05:26.640 --> 04:05:29.680]   But we of course we need to build the mean and the variance.
[04:05:29.680 --> 04:05:31.520]   What is the mean of this distribution?
[04:05:31.520 --> 04:05:33.120]   It's this one.
[04:05:33.120 --> 04:05:35.040]   And what is the variance?
[04:05:35.040 --> 04:05:35.920]   It's this one.
[04:05:35.920 --> 04:05:37.520]   So we need to build the mean and the variance.
[04:05:37.520 --> 04:05:38.640]   And then we sample from this.
[04:05:38.640 --> 04:05:40.480]   So let's do it.
[04:05:40.480 --> 04:05:42.960]   DDPM.
[04:05:43.360 --> 04:05:45.360]   So we take the original samples.
[04:05:45.360 --> 04:05:48.880]   Which is the float tensor.
[04:05:48.880 --> 04:05:50.080]   And then the time steps.
[04:05:50.080 --> 04:05:57.040]   So this is actually time step, not time steps.
[04:05:57.040 --> 04:06:00.000]   It indicates at what time step we want to add the noise.
[04:06:00.000 --> 04:06:04.240]   Because you can add the time step at the noise at time step 1, 2, 3, 4.
[04:06:04.240 --> 04:06:05.520]   Up to 1000.
[04:06:05.520 --> 04:06:10.160]   And we need to add the noise at the noise at time step 1, 2, 3, 4.
[04:06:10.160 --> 04:06:11.120]   1, 2, 3, 4.
[04:06:11.120 --> 04:06:12.240]   Up to 1000.
[04:06:12.240 --> 04:06:15.200]   And with each level the noise increases.
[04:06:15.200 --> 04:06:19.280]   So the noisified version at the time step 1 will be not so noisy.
[04:06:19.280 --> 04:06:22.800]   But at the time step 1000 will be complete noise.
[04:06:22.800 --> 04:06:27.680]   This returns a float tensor.
[04:06:27.680 --> 04:06:31.040]   Okay.
[04:06:31.040 --> 04:06:35.120]   Let's calculate first.
[04:06:35.120 --> 04:06:38.640]   Let me check what we need to calculate first.
[04:06:38.640 --> 04:06:40.400]   We can calculate first the mean.
[04:06:40.400 --> 04:06:41.360]   And then the variance.
[04:06:41.360 --> 04:06:46.000]   So to calculate the mean we need this alpha cum prod.
[04:06:46.000 --> 04:06:48.240]   So the cumulative product of the alpha.
[04:06:48.240 --> 04:06:50.160]   Which stands for alpha bar.
[04:06:50.160 --> 04:06:54.000]   So the alpha bar as you can see is the cumulative product of all the alphas.
[04:06:54.000 --> 04:06:56.240]   Which is each alpha is 1 minus beta.
[04:06:56.240 --> 04:06:59.360]   So we take this alpha bar.
[04:06:59.360 --> 04:07:01.280]   Which we will call alpha cum prod.
[04:07:01.280 --> 04:07:02.720]   So it's already defined here.
[04:07:04.560 --> 04:07:10.160]   Alpha cum prod is self dot 2 device.
[04:07:10.160 --> 04:07:14.080]   We move it to the same device.
[04:07:14.080 --> 04:07:16.080]   Because we need to later combine it with it.
[04:07:16.080 --> 04:07:21.840]   And of the same type.
[04:07:21.840 --> 04:07:27.760]   This is a tensor.
[04:07:30.400 --> 04:07:34.320]   That we also move to the same device of the other tensor.
[04:07:34.320 --> 04:07:39.280]   Now we need to calculate the square root of alpha bar.
[04:07:39.280 --> 04:07:40.960]   So let's do it.
[04:07:40.960 --> 04:07:47.040]   Square root of alpha cum prod.
[04:07:47.040 --> 04:07:53.280]   Or alpha prod is alpha cum prod at the time step t.
[04:07:53.280 --> 04:07:56.160]   To the power of 0.5.
[04:07:56.160 --> 04:07:57.760]   Why to the power of 0.5?
[04:07:57.760 --> 04:08:04.240]   Because having a number to the power of 0.5 means doing it's the square root of the number.
[04:08:04.240 --> 04:08:07.840]   Because the square root of 1/2 which becomes the square.
[04:08:07.840 --> 04:08:11.040]   Sorry to the power of 1/2 which becomes the square root.
[04:08:11.040 --> 04:08:14.800]   And then we flatten this array.
[04:08:14.800 --> 04:08:21.360]   And then basically because we need to combine this alpha cum prod.
[04:08:21.360 --> 04:08:23.440]   Which doesn't have dimensions.
[04:08:23.440 --> 04:08:24.640]   It only has one dimension.
[04:08:24.640 --> 04:08:25.680]   Which is the number itself.
[04:08:25.680 --> 04:08:27.760]   But we need to combine it with the latency.
[04:08:27.760 --> 04:08:29.040]   We need to add some dimensions.
[04:08:29.040 --> 04:08:32.640]   So one trick is to just keep adding dimensions with unsqueeze.
[04:08:32.640 --> 04:08:34.480]   Until you have the same number of dimensions.
[04:08:34.480 --> 04:08:41.440]   So until the n of the square of the shape is less than.
[04:08:41.440 --> 04:08:52.080]   Most of this code I have taken from the Hugging Face libraries samplers.
[04:08:52.080 --> 04:09:07.440]   So we keep the dimension until this one and this tensor and this tensor have the same dimensions.
[04:09:07.440 --> 04:09:11.120]   This is because otherwise we cannot do broadcasting when we multiply them together.
[04:09:11.120 --> 04:09:15.600]   The other thing that we need to calculate this formula is this part here.
[04:09:15.600 --> 04:09:16.880]   1 minus alpha bar.
[04:09:16.880 --> 04:09:18.800]   So let's do it.
[04:09:18.800 --> 04:09:24.240]   So sqrt of 1 minus alpha prod.
[04:09:24.240 --> 04:09:30.640]   As the name implies is 1 minus alpha cum prod at the time step t.
[04:09:30.640 --> 04:09:34.480]   To the power of 0.5.
[04:09:34.480 --> 04:09:35.680]   Why 0.5?
[04:09:35.680 --> 04:09:37.520]   Because we don't want the variance.
[04:09:37.520 --> 04:09:39.440]   We want the standard deviation.
[04:09:39.440 --> 04:09:43.920]   Just like we did with the encoder of the variational autoencoder.
[04:09:43.920 --> 04:09:45.680]   We want the standard deviation.
[04:09:45.680 --> 04:09:52.720]   Because as you remember if you have an n01 and you want to transform into an n with the given mean and the variance.
[04:09:52.720 --> 04:09:58.240]   The formula is x is equal to mean plus the standard deviation multiplied by the n01.
[04:09:58.240 --> 04:10:00.880]   Let's go back.
[04:10:00.880 --> 04:10:03.600]   So this is the standard deviation.
[04:10:03.600 --> 04:10:10.640]   And we also flatten this one.
[04:10:13.920 --> 04:10:19.680]   Flatten and then again we keep adding the dimensions until they have the same dimension.
[04:10:19.680 --> 04:10:27.760]   Otherwise we cannot multiply them together or sum them together.
[04:10:27.760 --> 04:10:36.800]   Unsqueeze so we keep adding dimensions.
[04:10:37.360 --> 04:10:44.080]   Now as you remember our method should add noise to an image.
[04:10:44.080 --> 04:10:47.280]   So we need to add noise means we need to sample some noise.
[04:10:47.280 --> 04:10:51.040]   So we need to sample some noise from the n01.
[04:11:00.560 --> 04:11:07.280]   Using this generator that we have.
[04:11:07.280 --> 04:11:14.000]   I think my cat is very angry today with me because I didn't play with him enough.
[04:11:14.000 --> 04:11:20.160]   So later if you guys excuse me I need to later play with him.
[04:11:20.160 --> 04:11:23.040]   I think we will be done very soon.
[04:11:24.240 --> 04:11:30.720]   So let's get the noisy samples using the noise and the mean and the variance that we have calculated.
[04:11:30.720 --> 04:11:32.880]   According exactly to this formula here.
[04:11:32.880 --> 04:11:35.520]   So we do the mean.
[04:11:35.520 --> 04:11:41.760]   Actually no the mean is this one multiplied by x0.
[04:11:41.760 --> 04:11:45.520]   So the mean is this one multiplied by x0 is the mean.
[04:11:45.520 --> 04:11:50.880]   So we need to take this square root of alpha comprod multiplied by x0 and this will be the mean.
[04:11:50.880 --> 04:11:56.320]   So the mean is square root of alpha prod multiplied by the original latency.
[04:11:56.320 --> 04:12:00.240]   So x0 so the input image or whatever we want to noisify.
[04:12:00.240 --> 04:12:09.200]   Plus the standard deviation which is a square root of this one multiplied by a sample of the
[04:12:09.200 --> 04:12:11.600]   from the n01 so the noise.
[04:12:11.600 --> 04:12:14.720]   And this is how we noisify an image.
[04:12:14.720 --> 04:12:18.960]   This is how we add noise to an image.
[04:12:19.920 --> 04:12:22.160]   So this one let me write it down.
[04:12:22.160 --> 04:12:36.480]   So all of this is according to the equation 4 of the DDM paper and also according to this.
[04:12:36.480 --> 04:12:46.400]   Okay now that we know how to add noise we need to understand how to remove noise.
[04:12:46.400 --> 04:12:49.920]   So as you remember let's review again here.
[04:12:49.920 --> 04:12:57.200]   Imagine we are doing text to text or text to image or image to image it doesn't matter.
[04:12:57.200 --> 04:13:03.600]   The point is our unit as you remember is trained to only predict the amount of noise given the
[04:13:03.600 --> 04:13:09.520]   latent with noise given the prompt and the time step at which this noise was added.
[04:13:12.400 --> 04:13:17.280]   So what we do is we have this predicted noise from the unit.
[04:13:17.280 --> 04:13:22.480]   We need to remove this noise so the unit will predict the noise but we need some way of
[04:13:22.480 --> 04:13:25.760]   removing the noise to get the next latent.
[04:13:25.760 --> 04:13:32.960]   What I mean by this is you can see this reverse process here.
[04:13:32.960 --> 04:13:35.680]   So the reverse process is defined here.
[04:13:36.720 --> 04:13:49.040]   We want to go from Xt so something more noisy to something less noisy based on the noise
[04:13:49.040 --> 04:13:51.440]   that was predicted by the unit.
[04:13:51.440 --> 04:13:56.800]   But here in this formula you don't see any relationship to the noise predicted by the
[04:13:56.800 --> 04:13:57.680]   unit.
[04:13:57.680 --> 04:14:06.640]   Actually here it just says if you have a network that can evaluate this mean and this variance
[04:14:06.640 --> 04:14:13.120]   you know how to remove the noise to how to go from Xt to Xt-1 but we don't have a method
[04:14:13.120 --> 04:14:15.120]   that actually predicts the mean and the variance.
[04:14:15.120 --> 04:14:18.000]   We have a method that tells us how much noise is there.
[04:14:18.000 --> 04:14:22.800]   So the formula we should be looking at is actually here.
[04:14:22.800 --> 04:14:33.600]   So here here because we have we trained our network our unit as a epsilon theta as you
[04:14:33.600 --> 04:14:40.000]   remember our training method was this we do gradient descent on this loss in which we
[04:14:40.000 --> 04:14:45.040]   train a network to predict the noise in a noisy image.
[04:14:45.040 --> 04:14:51.120]   So we need to use this epsilon theta now to remove the noise so this predicted noise to
[04:14:51.120 --> 04:14:58.000]   remove the noise and if we read the paper it's written here that to sample Xt-1 given
[04:14:58.000 --> 04:15:04.640]   Xt is to compute Xt-1 is equal to this formula here.
[04:15:04.640 --> 04:15:12.720]   This tells us how to go from Xt to Xt-1 and this is the so basically we sample some noise
[04:15:12.720 --> 04:15:21.440]   we multiply it by d sigma and this basically reminds us on how to move go from the N01
[04:15:21.440 --> 04:15:24.880]   to any distribution with a particular mean and a particular variance.
[04:15:26.400 --> 04:15:31.360]   So we will be working according to this formula here actually because we have a model that
[04:15:31.360 --> 04:15:36.080]   predicts noise here this epsilon theta and this is our unit.
[04:15:36.080 --> 04:15:38.000]   The unit is trained to predict noise.
[04:15:38.000 --> 04:15:44.160]   So let's build this part now and I will while building it I will also tell you which formula
[04:15:44.160 --> 04:15:47.680]   I'm referring to at each step so you can also follow the paper.
[04:15:47.680 --> 04:15:54.160]   So now let's build the method let's call step method that given the time step at which the
[04:15:54.160 --> 04:16:00.400]   noise was added or we think it was added because when we do the reverse process we can also
[04:16:00.400 --> 04:16:06.000]   skip it's not we think it was other but we can skip some time steps so we need to tell
[04:16:06.000 --> 04:16:12.640]   him what is the time step at which it should remove the noise the latency so as you know
[04:16:12.640 --> 04:16:19.040]   the unit works with the latency so with this z's here so this is z and it keeps denoising
[04:16:19.040 --> 04:16:27.920]   so the latency and then what is the model output so the predicted noise of the unit
[04:16:27.920 --> 04:16:33.520]   so the model output is the predicted noise torch dot tensor
[04:16:33.520 --> 04:16:43.680]   this model output corresponds to this epsilon theta of xtt so this is the predicted noise
[04:16:43.680 --> 04:16:52.320]   at time step t this latency is our xt and what else we need the alpha we have the beta
[04:16:52.320 --> 04:17:02.160]   we have we have everything okay let's go so t is equal to time step the previous t is
[04:17:02.160 --> 04:17:11.040]   equal to self dot get previous time step t this is a function that given this time step
[04:17:11.040 --> 04:17:16.160]   calculates the previous one later we will build it actually we can build it now it's very simple
[04:17:16.160 --> 04:17:24.100]   okay
[04:17:31.680 --> 04:17:38.960]   get previous time step self time step which is an integer we return another integer
[04:17:38.960 --> 04:17:43.680]   previous time step is equal to the time step
[04:17:43.680 --> 04:17:54.000]   minus self minus basically this quantity here step ratio so self dot num training steps
[04:17:56.720 --> 04:18:06.560]   divided by self dot num inference steps return previous t this one will return basically
[04:18:06.560 --> 04:18:16.640]   given for example the number 999 it will return number 999 minus 20 because the time steps
[04:18:16.640 --> 04:18:24.080]   for example the initial time step will be suppose it's 1000 the training steps we are doing is 1000
[04:18:24.080 --> 04:18:30.000]   divided by the number of inference step which is we will be doing is 50 so this is means 1000 minus
[04:18:30.000 --> 04:18:37.760]   20 because 1000 divided by 50 is 20 so it will return 980 when we give him 980 as input he will
[04:18:37.760 --> 04:18:45.200]   return 960 so what is the next step that we will be doing in our for loop or what is the previous
[04:18:45.200 --> 04:18:52.320]   step of the denoising so we are going from the image noise at the time step 1000 to an image
[04:18:52.320 --> 04:19:01.280]   noise that time step 980 for example this is the meaning of previous stem then we retrieve some
[04:19:01.280 --> 04:19:08.240]   data later we will use it so alpha pod t is equal to self dot alpha for now if you don't understand
[04:19:08.240 --> 04:19:12.400]   don't worry because later i will write i will just collect some data that we need to calculate
[04:19:12.400 --> 04:19:16.560]   a formula and then i will tell you exactly which formula we are going to calculate
[04:19:20.240 --> 04:19:22.640]   alpha prod t
[04:19:22.640 --> 04:19:39.200]   if we don't have any previous step then we don't know which alpha to return so we just return one
[04:19:45.440 --> 04:19:50.960]   and actually there is a paper that came out i think from by dance that was complaining that
[04:19:50.960 --> 04:19:58.720]   this method of doing is not correct because the the last time step doesn't have this is not doesn't
[04:19:58.720 --> 04:20:03.760]   have the signal to noise ratio about equal to zero but okay this is something we don't need
[04:20:03.760 --> 04:20:16.320]   to care about now actually if you're interested i will link the paper in the comments
[04:20:16.320 --> 04:20:23.760]   rev current alpha t
[04:20:26.320 --> 04:20:33.600]   prod t divided by alpha prod current also this code i took it from
[04:20:33.600 --> 04:20:43.520]   hugging face diffusers library because i mean we are applying formulas so even if i wrote it by
[04:20:43.520 --> 04:20:48.240]   myself it wouldn't be any different because we are just applying formulas from the paper so
[04:20:48.240 --> 04:20:55.040]   so the first thing we need to do is to compute the original sample according to the formula 15
[04:20:55.040 --> 04:21:05.120]   of the paper what do i mean by this as you can see where is it this one where is it
[04:21:05.120 --> 04:21:12.160]   here so actually let me show you another formula here
[04:21:12.160 --> 04:21:21.120]   as you can see we can calculate the previous step so the less noise is the the forward process
[04:21:22.080 --> 04:21:26.960]   sorry the reverse process we can calculate the less noisy image given a more noisy image
[04:21:26.960 --> 04:21:34.560]   and the predicted image at time step zero according to this formula here where the mean
[04:21:34.560 --> 04:21:42.480]   is defined in this way and the variance is defined in this way but what is the predicted
[04:21:44.400 --> 04:21:53.920]   x0 so given an image given a noisy image at time step t how can we predict what is the x0
[04:21:53.920 --> 04:22:00.640]   of course this is the predicted x0 not what will be the x0 so this predicted x0 we can also retrieve
[04:22:00.640 --> 04:22:10.800]   it using the formula number 15 if i remember correctly it's here so this x0 is given as xt
[04:22:10.800 --> 04:22:17.200]   minus 1 minus alpha multiplied by the predicted noise at time step t divided by the square root
[04:22:17.200 --> 04:22:22.800]   of alpha all these quantities we have so actually there are two ways which are equivalent to each
[04:22:22.800 --> 04:22:28.960]   other actually numerically of going from more noisy to less noisy one way is this one this
[04:22:28.960 --> 04:22:36.560]   one here which is the algorithm 2 of the sampling and one is this one here so the equation number
[04:22:36.560 --> 04:22:42.400]   7 that allows you to go from more noisy to less noisy but the two are numerically equivalent they
[04:22:42.400 --> 04:22:48.000]   just in the in the effect they are equivalent it's just they have different parameterization
[04:22:48.000 --> 04:22:54.720]   so they have different formulas so as a matter of fact for example here in the code they say
[04:22:54.720 --> 04:23:02.480]   to go from xt to xt minus 1 you need to do this calculation here but as you can see for example
[04:23:02.480 --> 04:23:11.120]   this is this numerator of this multiplied by this epsilon theta is different from the one
[04:23:11.120 --> 04:23:16.720]   in the algorithm here but actually they are the same thing because bt is equal to 1 minus alpha t
[04:23:16.720 --> 04:23:24.080]   as beta alpha is defined as 1 minus beta as you remember so there are multiple ways of obtaining
[04:23:24.080 --> 04:23:29.760]   the same thing so what we will do is we actually we will apply this formula here in which we need
[04:23:29.760 --> 04:23:34.720]   to calculate the mean and we need to calculate the variance according to these formulas here
[04:23:34.720 --> 04:23:40.320]   in which we know alpha we know beta we know alpha bar we know all the other alphas we know
[04:23:40.320 --> 04:23:46.240]   because there are parameters that depend on beta what we don't know is x0 but x0 can be calculated
[04:23:46.240 --> 04:23:54.400]   as in the formula 15 here so first we will calculate this x0 predicted x0
[04:23:54.400 --> 04:24:10.880]   so first compute the predicted original sample using formula 15 of the DDTM paper
[04:24:13.360 --> 04:24:16.960]   predicted original sample
[04:24:16.960 --> 04:24:28.000]   latency minus while so we do latency minus the square root of 1 minus alpha t what is the square
[04:24:28.000 --> 04:24:36.160]   root of 1 minus alpha t is equal to beta so i have here beta t which is already 1 minus alpha t as
[04:24:36.160 --> 04:24:43.920]   you can see alpha bar 1 minus alpha bar at the time step t because i already retrieve it from here
[04:24:43.920 --> 04:24:52.000]   so 1 minus sorry beta to the power to to the power of one half or the square root of beta
[04:24:52.000 --> 04:25:01.120]   so we do latency minus beta rod at time step t to the power of 0.5 which it means basically square
[04:25:01.120 --> 04:25:08.640]   root of beta and then we multiply this by the predicted noise of the image of the latent at
[04:25:08.640 --> 04:25:15.120]   time step t so what is the predicted noise it's the model output because our unit predicts the
[04:25:15.120 --> 04:25:21.120]   noise model output and then we need to divide this by let me check
[04:25:23.360 --> 04:25:31.360]   square root of alpha t which we have i think here alpha t here so the square root of
[04:25:31.360 --> 04:25:36.240]   alpha t alpha prod t to the power of 0.5
[04:25:36.240 --> 04:25:44.560]   here i have something on this one i don't need this one i don't need okay
[04:25:44.560 --> 04:25:50.080]   because otherwise it's wrong right yeah before first there is a product between
[04:25:50.080 --> 04:25:56.160]   these two terms and then there is the difference yeah okay this is how we compute the prediction
[04:25:56.160 --> 04:25:59.440]   the x0 now let's go back to the formula number seven
[04:25:59.440 --> 04:26:06.960]   seven seven okay now we have this x0 so we can compute this term and we can compute this term
[04:26:06.960 --> 04:26:11.120]   and this we can compute this term and all the other terms we also can compute so we calculate
[04:26:11.120 --> 04:26:20.640]   this mean and this variance and then we sample from this distribution so compute the coefficients
[04:26:20.640 --> 04:26:31.280]   for bred original sample and the current sample xt this is the same comment that you can find on
[04:26:31.280 --> 04:26:36.640]   the diffusers library which basically means we need to compute this one this is the coefficient
[04:26:36.640 --> 04:26:43.520]   for the predicted sample and this is the coefficient for xt this one here so predicted
[04:26:43.520 --> 04:26:54.320]   original sample coefficient which is equal to what alpha prod t minus one so the previous alpha
[04:26:54.320 --> 04:27:02.320]   prod t which is alpha prod t previous which means the alpha prod t but at the previous time step
[04:27:04.720 --> 04:27:10.720]   under the square root so to the power of 0.5 multiplied by the current beta t so
[04:27:10.720 --> 04:27:18.640]   the beta at the time step t so current beta t which is we define it here
[04:27:18.640 --> 04:27:26.960]   current beta t we retrieve it from alpha we could have a okay and then we divide it by
[04:27:28.720 --> 04:27:33.120]   beta product t because one minus alpha bar is actually equal to beta bar
[04:27:33.120 --> 04:27:42.320]   beta product t then we have the this coefficient here so this one here
[04:27:42.320 --> 04:27:50.880]   so this is current sample coefficient is equal to current alpha t to the power of 0.5
[04:27:50.880 --> 04:27:57.120]   which means the square root of this time this this thing here so the square root of alpha t
[04:27:58.080 --> 04:28:03.200]   and then we multiply it by beta at the previous time step because it's one minus alpha at the
[04:28:03.200 --> 04:28:09.280]   previous time step corresponds to beta as the previous time steps time step multiplied by beta
[04:28:09.280 --> 04:28:17.840]   prod t prev divide by beta at the time step t so beta prod t
[04:28:17.840 --> 04:28:24.160]   now we can compute the mean so the mean is the sum of these two terms
[04:28:26.160 --> 04:28:33.920]   pred prev sample so let me write some here compute the predicted
[04:28:33.920 --> 04:28:39.840]   previous sample mean mod t
[04:28:39.840 --> 04:28:50.000]   is equal to predicted original sample coefficient multiplied by what by x0 what is x0 is this one
[04:28:50.000 --> 04:28:56.000]   that we obtained by the formula number 15 so the prediction predicted original sample so x0
[04:28:56.000 --> 04:29:02.560]   plus this term here what is this term is this one here so the current sample coefficient
[04:29:02.560 --> 04:29:08.480]   multiplied by xt what is xt is the latency at the time step t
[04:29:08.480 --> 04:29:16.320]   now this we have computed the mean for now we need to compute also the variance
[04:29:17.120 --> 04:29:19.760]   let's create another method to compute the variance
[04:29:19.760 --> 04:29:28.480]   test get variance self time step int
[04:29:28.480 --> 04:29:37.440]   okay we obtained the previous time test t because we need to do for later calculations
[04:29:37.440 --> 04:29:45.280]   again we calculate the alpha prod t so all the terms that we need to calculate
[04:29:45.280 --> 04:29:51.440]   these particular terms here
[04:30:15.040 --> 04:30:24.240]   and the current beta t is equal to one minus alpha prod t divided by alpha prod this one
[04:30:24.240 --> 04:30:34.560]   what is current beta t is equal to one minus alpha prod t yeah one minus alpha prod t
[04:30:36.080 --> 04:30:48.800]   divided by alpha prod t0 okay so the variance according to the formula number six and seven
[04:30:48.800 --> 04:31:01.280]   so this formula here is given as one minus alpha prod tprev so one minus alpha prod tprev
[04:31:04.000 --> 04:31:11.760]   divided by one minus alpha prod which is one minus alpha prod why prod because
[04:31:11.760 --> 04:31:15.920]   this is the alpha bar and multiplied by the current beta
[04:31:15.920 --> 04:31:24.160]   beta t and beta t is defined i don't remember where it's one minus alpha
[04:31:26.320 --> 04:31:29.840]   and this is our variance we clamp it
[04:31:29.840 --> 04:31:41.600]   oops torch dot clamp the variance and the minimum that we want is one
[04:31:41.600 --> 04:31:49.280]   equal to minus 20 to make sure that with it doesn't reach zero and then we return the variance
[04:31:52.240 --> 04:31:58.960]   and now that we have the mean and the variance so this variance has also been computed using
[04:31:58.960 --> 04:32:07.840]   let me write here computed using formula seven of the ddpm paper
[04:32:07.840 --> 04:32:15.680]   and now we go back to our step function so what we do is
[04:32:17.760 --> 04:32:24.480]   it's equal to zero so because we only need to add the variance if we are not at the last
[04:32:24.480 --> 04:32:28.320]   last time step if you are at the last time step we have no noise so we don't add any
[04:32:28.320 --> 04:32:36.880]   we don't add we don't need to add any noise actually because the point is we are going
[04:32:36.880 --> 04:32:42.240]   to sample from this distribution and just like we did before we actually sample from the n01 and
[04:32:42.240 --> 04:32:52.160]   then we shift it according to the formula so the n gaussian as with the particular mean and the
[04:32:52.160 --> 04:32:58.960]   particular variance is equal to the the gaussian at zero one multiplied by the standard deviation
[04:32:58.960 --> 04:33:26.080]   plus the um the plus the mean so we sample the noise
[04:33:26.080 --> 04:33:38.000]   okay we sample some noise compute the variance
[04:33:38.000 --> 04:33:42.640]   actually this is the
[04:33:42.640 --> 04:33:48.800]   variance already multiplied by the noise so it's actually the standard deviation
[04:33:48.800 --> 04:33:58.240]   because we will see self dot get variance after the time step t to the power of 0.5 so this
[04:33:58.240 --> 04:34:04.720]   0.5 so this one becomes the standard deviation we multiply it by the n01
[04:34:04.720 --> 04:34:09.520]   so what we are doing is basically we are going from n01
[04:34:09.520 --> 04:34:18.080]   to nn with a particular mu and a particular sigma using the usual trick of going from
[04:34:18.080 --> 04:34:25.520]   x is equal to the mu plus the sigma actually not yeah this is the sigma squared then
[04:34:25.520 --> 04:34:33.360]   because this is the variance sigma multiplied by the z where z where z is distributed according
[04:34:33.360 --> 04:34:39.520]   to the n01 this is the same thing that we always done also for the variation of the
[04:34:39.520 --> 04:34:45.840]   encoder also for adding the noise the same thing that we did before this is how you sample from a
[04:34:45.840 --> 04:34:49.280]   distribution how you actually shift the parameter of the gaussian distribution
[04:34:49.280 --> 04:34:59.600]   so predicted prev sample is equal to the predicted prev sample plus the variance
[04:34:59.600 --> 04:35:03.600]   this variance term here already includes the sigma multiplied by z
[04:35:03.600 --> 04:35:12.080]   and then we return predicted prev sample oh okay now we have also built the
[04:35:14.000 --> 04:35:19.360]   the sampler let me check if we have everything no we missed still still something which is the
[04:35:19.360 --> 04:35:26.320]   set strength method as you remember once we want when we want to do image to image so let's go
[04:35:26.320 --> 04:35:32.560]   back to check our slides if we want to do image to image we convert the image using the vae to a
[04:35:32.560 --> 04:35:38.640]   latent then we need to add noise to this latent but how much noise we can decide the more noise
[04:35:38.640 --> 04:35:43.360]   we add the more freedom the unit will have to change this image the less noise we add the
[04:35:43.360 --> 04:35:48.640]   less freedom it will have to change the image so what we do is basically by setting the strength
[04:35:48.640 --> 04:35:55.600]   we make our sampler start from a particular noise level and this is exactly what the method we want
[04:35:55.600 --> 04:36:02.640]   to implement so i made some mess okay so for example as soon as we load the image we set
[04:36:02.640 --> 04:36:08.240]   the strength which will shift the noise level from which we start from and then we add noise
[04:36:08.240 --> 04:36:16.480]   to our latent to create the image to image here so let's go here and we create this method called
[04:36:16.480 --> 04:36:20.720]   set strength
[04:36:20.720 --> 04:36:28.480]   okay in the start step because we will skip some steps
[04:36:29.840 --> 04:36:37.920]   is equal to self.num inference steps minus int of self.num inference
[04:36:37.920 --> 04:36:48.000]   this basically means that if we have 50 inference steps and then we set the strength to let's say
[04:36:48.000 --> 04:36:55.200]   0.8 it means that we will skip 20% of the steps so when we will add we will start from image to
[04:36:55.200 --> 04:37:01.760]   image for example we will not start from a pure noise image but we will start from 80% of noise
[04:37:01.760 --> 04:37:08.160]   in this image so the unit will still have freedom to change this image but not as much as with 100%
[04:37:08.160 --> 04:37:17.920]   noise we redefine the time steps because we are altering the schedule so basically we skip some
[04:37:17.920 --> 04:37:29.840]   time steps and self.start step is equal to start step so actually what we do here is suppose we
[04:37:29.840 --> 04:37:35.440]   have the strength of 80% we are actually fooling the method the the unit into believing that he
[04:37:35.440 --> 04:37:41.280]   came up with this image which is now with this level of strength and now he needs to keep denoising
[04:37:41.280 --> 04:37:47.280]   it this is how we do image to image so we start with an image we noise it and then we make the
[04:37:47.280 --> 04:37:53.760]   unit believe that he came up with this image with this particular noise level and now he has to keep
[04:37:53.760 --> 04:38:01.120]   denoising it until according of course also to the prompt until we reach the clean image without any
[04:38:01.120 --> 04:38:12.080]   noise now we have the pipeline that we can call we have the ddpm sampler we have the model built
[04:38:12.080 --> 04:38:18.000]   of course we need to create the function to load the weights of this model so let's create another
[04:38:18.000 --> 04:38:27.760]   file we will call it the model loader here model loader because now we are nearly close to sampling
[04:38:27.760 --> 04:38:32.160]   from this finally from this table diffusion so now we need to create the method to load the
[04:38:32.160 --> 04:38:38.240]   pre-trained the pre-trained weights that we have downloaded before so let's create it
[04:38:40.800 --> 04:38:42.240]   import clip
[04:38:42.240 --> 04:38:55.600]   decoder va encoder then from decoder import va decoder
[04:38:55.600 --> 04:39:04.160]   fusion import diffusion our diffusion model which is our unit
[04:39:05.040 --> 04:39:14.800]   now let me first define it then i tell you what we need to do so preload preload models from
[04:39:14.800 --> 04:39:17.280]   standard weights
[04:39:17.280 --> 04:39:32.720]   okay as usual we load the weights using torch but we use we will create another function
[04:39:32.720 --> 04:39:37.280]   model converter dot load from standard weights
[04:39:37.280 --> 04:39:49.360]   this is a method that we will create later to to load the weights
[04:39:49.360 --> 04:39:54.320]   the pre-trained weights and i will show you why we need this method then we create our encoder
[04:39:58.880 --> 04:40:04.160]   and we load the state addict load state addict from our state addict
[04:40:04.160 --> 04:40:12.400]   and we also set strict to two oops don't strict
[04:40:12.400 --> 04:40:22.480]   strict
[04:40:26.960 --> 04:40:37.040]   true then we have the decoder
[04:40:37.040 --> 04:40:52.000]   and strict also so this strict parameter here basically tells that when you load a model from
[04:40:52.000 --> 04:40:59.680]   pytorch this for example this ckp ckpt file here it is a dictionary that contains many keys
[04:40:59.680 --> 04:41:06.320]   and each key corresponds to one matrix of our model so for example this uh self this group
[04:41:06.320 --> 04:41:12.720]   normalization has some parameters and the the how can torch load this parameter is exactly in this
[04:41:12.720 --> 04:41:20.480]   group norm by using the name of the variables that we have defined here and he will when we load a
[04:41:20.480 --> 04:41:25.680]   model from pytorch he will actually load the dictionary and then we load this dictionary
[04:41:25.680 --> 04:41:30.960]   into our models and he will match by names now the problem is the pre-trained model
[04:41:30.960 --> 04:41:36.720]   actually they don't use the same name that i have used and actually this code is based on another
[04:41:36.720 --> 04:41:42.560]   code that i have seen so actually the the names that we use are not the same as the pre-trained
[04:41:42.560 --> 04:41:49.120]   model also because the names in the pre-trained model not always uh very friendly for learning
[04:41:49.120 --> 04:41:55.200]   this is why i changed the names and also other people changed the names of the methods but this
[04:41:55.200 --> 04:42:01.440]   also means that the automatic mapping between the names of the pre-trained model and the names
[04:42:01.440 --> 04:42:06.400]   defined in our classes here cannot happen because it cannot happen automatically because the names
[04:42:06.400 --> 04:42:13.200]   do not match for this reason there is a script that i have created in my github library here
[04:42:14.320 --> 04:42:20.160]   that you need to download to convert these names it's just a script that maps one name into another
[04:42:20.160 --> 04:42:24.560]   so if the name is this one map it into this if the name is this one mapping into this
[04:42:24.560 --> 04:42:30.560]   there is nothing special about this script it's just a very big mapping of the names and this is
[04:42:30.560 --> 04:42:36.960]   actually done by most models because if you want to change the name of the classes and or the
[04:42:36.960 --> 04:42:43.920]   variables then you need to do this kind of mapping so i will also i will basically copy it i don't
[04:42:43.920 --> 04:42:52.320]   need to download the file so this will call the model converter.py model converter.py
[04:42:52.320 --> 04:43:00.480]   and that's it it's just a very big mapping of names and i take it from this comment here on
[04:43:00.480 --> 04:43:12.000]   github so this is model converter so we need to import this model converter import model converter
[04:43:12.560 --> 04:43:17.840]   import this model converter basically will convert the names and then we can use the
[04:43:17.840 --> 04:43:22.720]   load state dict and this will actually map all the names it's now now the names will map with
[04:43:22.720 --> 04:43:26.880]   each other and this trick makes sure that if there is even one name that doesn't map
[04:43:26.880 --> 04:43:31.200]   then throw an exception which is what i want because i want to make sure that all the names map
[04:43:38.240 --> 04:43:40.480]   so we define the diffusion and we load it's
[04:43:40.480 --> 04:43:43.280]   state dict
[04:43:43.280 --> 04:43:52.480]   diffusion and strict equal to true
[04:43:52.480 --> 04:43:59.280]   and let me check
[04:44:07.440 --> 04:44:13.360]   then we do clip is equal to clip dot to device so we move it to device where we
[04:44:13.360 --> 04:44:17.840]   want to work and then we load also his state dict so the parameters of the weights
[04:44:17.840 --> 04:44:33.280]   and then we return a dictionary clip
[04:44:35.760 --> 04:44:42.960]   clip and then we have the encoder is the encoder we have the decoder
[04:44:42.960 --> 04:44:51.600]   is the decoder and then we have the diffusion we have the diffusion etc
[04:44:51.600 --> 04:44:58.800]   now we have all the ingredients to run finally the inference guys so thank you for being patient so
[04:44:58.800 --> 04:45:07.680]   much and it's really finally we have we can see the light coming so let's build our notebook so we
[04:45:07.680 --> 04:45:16.640]   can visualize the image that we will build okay let's select the kernel stable diffusion i already
[04:45:16.640 --> 04:45:23.360]   created it in my repository you will also find the requirements that you need to install in order to
[04:45:23.360 --> 04:45:30.000]   run this so let's import everything we need so the model loader the pipeline
[04:45:30.000 --> 04:45:41.040]   peel import image this is how to load the image from python so patlib import actually this one
[04:45:41.040 --> 04:45:48.000]   we don't need transformers this is the only library that we will be using because there
[04:45:48.000 --> 04:45:54.080]   is the tokenizer of the clip so how to tokenize the the text into tokens before sending it to
[04:45:54.080 --> 04:45:59.120]   the clip embeddings otherwise we also need to build the tokenizer and it's really a lot of
[04:45:59.120 --> 04:46:15.520]   job i don't allow cuda and i also don't allow mps but you can activate these two
[04:46:17.680 --> 04:46:19.920]   variables if you want to use cuda or mps
[04:46:19.920 --> 04:46:43.200]   valuable and low cuda then the device becomes cuda of course
[04:46:43.200 --> 04:46:54.720]   so
[04:46:54.720 --> 04:47:08.720]   and then we printed the device we are using
[04:47:11.680 --> 04:47:17.760]   okay let's load the tokenizer tokenizer is the clip tokenizer we need to tell him what is the
[04:47:17.760 --> 04:47:25.440]   vocabulary file so which is already saved here in the data data vocabulary.json and then also the
[04:47:25.440 --> 04:47:32.320]   merges file maybe one day i will make a video on how the tokenizer works so we can build also the
[04:47:32.320 --> 04:47:38.880]   tokenizer but this is something that requires a lot of time i mean and it's not really related
[04:47:38.880 --> 04:47:45.920]   to the diffusion model so that's why i didn't want to build it the model file is i will use the data
[04:47:45.920 --> 04:47:54.480]   and then this file here then we load the model so the models are model loader dot preload model from
[04:47:54.480 --> 04:48:02.640]   the model file into this device that we have selected okay let's build from text to image
[04:48:02.640 --> 04:48:08.160]   what we need to define the prompt for example i want a cat
[04:48:08.160 --> 04:48:18.560]   sitting or stretching let's say stretching on the floor highly detailed we need to create a
[04:48:18.560 --> 04:48:25.360]   prompt that will create a good image so we need to add some a lot of details ultra sharp cinematic
[04:48:25.360 --> 04:48:32.720]   etc etc 8k resolution the unconditioned prompt
[04:48:32.720 --> 04:48:42.720]   i keep it blank this you can also use it as a negative number you can use it as a negative
[04:48:42.720 --> 04:48:50.800]   prompt so if you don't want the sum you don't want the output to have some how to say some
[04:48:50.800 --> 04:48:57.120]   characteristics you can define it in the negative prompt of course i like to do cfg so the
[04:48:57.120 --> 04:49:05.760]   classifier free guidance which we set to true cfg scale is a number between 1 and 14 which
[04:49:05.760 --> 04:49:10.560]   indicates how much attention we want the model to pay to this prompt 14 means pay
[04:49:10.560 --> 04:49:14.880]   very much attention or 1 means we pay very little attention i use 7
[04:49:17.920 --> 04:49:20.480]   then we can define also the parameters for image to image
[04:49:20.480 --> 04:49:31.440]   so input image is equal to none image path is equal to i will define it with my
[04:49:31.440 --> 04:49:38.960]   image of the dog which i already have here and um but for now i don't want to load it
[04:49:38.960 --> 04:49:44.560]   so if we want to load it we need to do input image is equal to image.open
[04:49:46.320 --> 04:49:48.480]   image path but for now i will
[04:49:48.480 --> 04:49:56.960]   i will not use it so now let's comment it and if we use it we need to define the strength
[04:49:56.960 --> 04:50:00.080]   so how much noise we want to add to this image but for now let's not use it
[04:50:00.080 --> 04:50:04.640]   the sampler we will be using of course is the only one we have is the ddpm
[04:50:04.640 --> 04:50:08.640]   the number of inference steps is equal to
[04:50:10.880 --> 04:50:19.280]   50 and the seed is equal to 42 because it's a lucky number at least according to some books
[04:50:19.280 --> 04:50:31.280]   output image is equal to pipeline generate okay the prompt is the prompt that we have defined
[04:50:31.280 --> 04:50:35.280]   the unconditioned prompt is the unconditioned prompt that we have defined
[04:50:36.240 --> 04:50:42.000]   input image is the input image that we have defined if it's not commented of course
[04:50:42.000 --> 04:50:44.000]   the strength for the image
[04:50:44.000 --> 04:50:53.200]   and the cfg scale is the one we have defined
[04:50:53.200 --> 04:50:59.760]   the sampler name is the sampler name we have defined
[04:51:01.760 --> 04:51:07.200]   the number of inference steps is the number of inference steps the seed
[04:51:07.200 --> 04:51:10.000]   models
[04:51:10.000 --> 04:51:14.720]   device
[04:51:14.720 --> 04:51:22.720]   idle device is our cpu so when we don't want to use something we move it to the cpu
[04:51:22.720 --> 04:51:24.800]   and the tokenizer is the tokenizer
[04:51:27.200 --> 04:51:37.680]   and then image dot from array output image if everything is done well if all the code has
[04:51:37.680 --> 04:51:43.280]   been written correctly you can always go back to my repository and download the code if you
[04:51:43.280 --> 04:51:48.880]   don't want to write it by yourself let's run the code and let's see what is the result my
[04:51:48.880 --> 04:51:55.280]   computer will take a while so it will take some time so let's run it so if we run the code it will
[04:51:56.800 --> 04:52:01.520]   generate an image according to our prompt in my computer it took really a long time so i cut the
[04:52:01.520 --> 04:52:08.240]   video and i actually already replaced the code with the one from my github because now i want
[04:52:08.240 --> 04:52:14.320]   to actually explain you the code without while showing you all the code together how does it
[04:52:14.320 --> 04:52:20.400]   work so now we we generated an image using only the prompt i use the cpu that's why it's very slow
[04:52:20.400 --> 04:52:25.840]   because my gpu is not powerful enough and we set a unconditioned prompt to zero we are using the
[04:52:25.840 --> 04:52:30.880]   classifier free guidance and with a scale of seven so let's go in the pipeline and let's see
[04:52:30.880 --> 04:52:36.400]   what happens so basically because we are doing the classifier free guidance we will generate
[04:52:36.400 --> 04:52:43.280]   two conditioning signals one with the prompt and one with empty text which is the unconditioned
[04:52:43.280 --> 04:52:50.560]   prompt which is also called the negative prompt this will result in a batch size of two that will
[04:52:50.560 --> 04:52:56.240]   run through the unit so let's go back to here suppose we are doing text to image so now our
[04:52:56.240 --> 04:53:01.680]   unit has two latents that he's doing at the same time because we have the batch size equal to two
[04:53:01.680 --> 04:53:10.160]   and for each of them it is predicting the noise level but how can we move remove this noise from
[04:53:10.160 --> 04:53:17.680]   the predicted noise from the initial noise so because to generate an image we start from random
[04:53:17.680 --> 04:53:24.960]   noise and the prompt initially we encode it with our vae so it becomes a latent which is still
[04:53:24.960 --> 04:53:31.040]   noise and with the unit we predict we predict how much noise is it according to a schedule so
[04:53:31.040 --> 04:53:37.360]   according to 50 steps that of inferencing that we will be doing at the beginning the first step will
[04:53:37.360 --> 04:53:44.720]   be 1000 the next step will be 980 the next step will be 960 etc so this time will change according
[04:53:44.720 --> 04:53:55.440]   to this schedule so that at the 50th step we are at the time step 0 and how can we then with the
[04:53:55.440 --> 04:54:01.840]   predicted noise go to the next latent so we remove this noise that was predicted by the unit well we
[04:54:01.840 --> 04:54:09.040]   do it with the sampler and in particular we do it with the sample method of the sampler step method
[04:54:09.040 --> 04:54:14.640]   sorry of the sampler which basically will calculate the previous sample given the current sample
[04:54:14.640 --> 04:54:21.120]   according to the formula number 7 here so which basically calculates the previous sample
[04:54:21.120 --> 04:54:27.840]   given the current one so the less noisy one given the current one and the predicted x0 so this is
[04:54:27.840 --> 04:54:34.880]   not x0 because we don't have x0 so we don't have the noise the sample without any noise so but we
[04:54:34.880 --> 04:54:42.480]   can predict it given the values of the current noise and the beta schedule another way of denoising
[04:54:42.480 --> 04:54:48.000]   is to do the sampling like this if you watch my other repository about the ddbm paper i actually
[04:54:48.000 --> 04:54:54.160]   implemented it like this if you want to see this version here and this is how we remove the noise
[04:54:54.160 --> 04:54:59.920]   to get a less noisy version so once we get the less noisy version we keep doing this process
[04:54:59.920 --> 04:55:04.800]   until there is no more noise so we are at the time step zero in which we have no more noise
[04:55:04.800 --> 04:55:09.920]   we give this latent to the decoder which will turn it into an image this is how the text to image
[04:55:09.920 --> 04:55:15.200]   works the image to image on the other side so let's try to do the image to image so to do the
[04:55:15.200 --> 04:55:25.680]   image to image we need to go here and we uncomment this code here this allows us to start with the
[04:55:25.680 --> 04:55:31.840]   dog and then give for example some prompt for example we want this dog here we want to say
[04:55:31.840 --> 04:55:38.560]   okay we want a dog stretching on the floor highly detailed etc we can run it i will not run it
[04:55:38.560 --> 04:55:45.760]   because it will take another five minutes and if we do this we can set a strength of let's say 0.6
[04:55:45.760 --> 04:55:54.000]   which means that let's go here so we set a strength of 0.6 so we have this input image
[04:55:54.000 --> 04:55:59.680]   strength of 0.6 means that we will add we will encode it with the variation auto encoder will
[04:55:59.680 --> 04:56:06.560]   become a latent will add some noise but how much noise not all the noise so that it becomes
[04:56:06.560 --> 04:56:14.080]   completely noise but less noise than that so at let's let's say 60 percent noise is not really
[04:56:14.080 --> 04:56:21.040]   true because because it depends on the schedule in our case it's linear so it can be considered
[04:56:21.040 --> 04:56:28.240]   60 percent of noise we then give this image to the scheduler which will start not from the 1000
[04:56:28.240 --> 04:56:34.800]   step it will start before so if we set the strength to 0.6 it will start from the 600
[04:56:34.800 --> 04:56:46.640]   step and then move by 20 we'll keep going 600 then 580 then 560 then 540 etc until it reaches 20
[04:56:46.640 --> 04:56:52.560]   so in total it will do less steps because we start from a less noisy example but at the same
[04:56:52.560 --> 04:57:00.720]   time because we start with less noise the the unit also has less freedom to change the to alter the
[04:57:00.720 --> 04:57:07.040]   image because he already have the image so he cannot change it too much so how do you adjust
[04:57:07.040 --> 04:57:13.440]   the noise the the noise level depends if you want the unit to pay very much attention to the input
[04:57:13.440 --> 04:57:20.800]   image and not change it too much then you add less noise if you want to change completely the
[04:57:20.800 --> 04:57:25.280]   original image then you can add all the possible noise so you set the strength to one and this is
[04:57:25.280 --> 04:57:32.880]   how the image to image works i didn't implement the inpainting because the reason is that the
[04:57:32.880 --> 04:57:38.080]   pre-trained model here so the model that we are using is not fine-tuned for inpainting so if you
[04:57:38.080 --> 04:57:45.280]   go on the website and you look at the model card they have another model for inpainting which has
[04:57:45.280 --> 04:57:53.120]   different weights here the this one here but this the structure of this model is also a little
[04:57:53.120 --> 04:57:58.800]   different because they have in the unit they have five additional input channels for the mask
[04:57:58.800 --> 04:58:06.800]   i will of course implement it in my repository directly so i will modify the code and
[04:58:07.440 --> 04:58:12.960]   also implement the code for inpainting so that we can support this model but unfortunately i don't
[04:58:12.960 --> 04:58:19.760]   have the time now because in china here is guoqing and i'm going to laojia with my my wife so we are
[04:58:19.760 --> 04:58:26.240]   a little short of time but i hope that with my video guys you you got really into stable diffusion
[04:58:26.240 --> 04:58:31.440]   and you understood what is happening under the hood instead of just using the hugging face library
[04:58:31.440 --> 04:58:39.520]   and also notice that the model itself is not so particularly sophisticated if you check the
[04:58:39.520 --> 04:58:47.200]   decoder and the encoder they are just a bunch of convolutions and upsampling and the normalizations
[04:58:47.200 --> 04:58:53.280]   just like any other computer vision model and the same goes on for the unit of course there are very
[04:58:53.280 --> 04:58:59.440]   smart choices in how they do it okay but that's not the important thing of the diffusion and
[04:58:59.440 --> 04:59:03.680]   actually if we study the diffusion models like score models you will see that it doesn't even
[04:59:03.680 --> 04:59:09.040]   matter the structure of the model as long as the model is expressive it will actually learn the
[04:59:09.040 --> 04:59:14.000]   score function in the same way but this is not our case in this video i will talk about score model
[04:59:14.000 --> 04:59:20.400]   in future videos what i want you to understand is that how this all mechanism works together
[04:59:20.400 --> 04:59:27.200]   so how can we just learn a model that predicts the noise and then we come up with images and
[04:59:28.160 --> 04:59:35.760]   let me rehearse again the idea so we started by training a model that needs to learn a probability
[04:59:35.760 --> 04:59:43.120]   distribution as you remember p of theta here we we cannot learn this one directly because we don't
[04:59:43.120 --> 04:59:49.120]   know how to marginalize here so what we did is we find some lower bound for this quantity here and
[04:59:49.120 --> 04:59:54.800]   we maximize this lower bound how do we maximize this lower bound by training a model by running
[04:59:54.800 --> 05:00:03.760]   the gradient descent on this loss this loss produces a model that allow us to predict the
[05:00:03.760 --> 05:00:10.880]   noise then how do we actually use this model with the predicted noise to go back in time with the
[05:00:10.880 --> 05:00:15.680]   noise because the forward process we know how to go it's defined by us how to add noise but in back
[05:00:15.680 --> 05:00:20.880]   in time so how to remove noise we don't know and we do it according to the formulas that i have
[05:00:20.880 --> 05:00:27.360]   described in the sampler so the formula number seven and the formula number also this one actually
[05:00:27.360 --> 05:00:33.760]   we can use actually i will show you in my other um here i have another repository i think it's
[05:00:33.760 --> 05:00:39.840]   called python ddpm in which i implemented the ddpm paper but by using this algorithm here so
[05:00:39.840 --> 05:00:44.720]   if you are interested in this version of the denoising you can check my other uh repository
[05:00:44.720 --> 05:00:52.720]   here this one ddpm and i also wanted to show you how the inpainting works how the how the
[05:00:52.720 --> 05:00:59.600]   image to image and how the text to image works of course the possibilities are limitless it all
[05:00:59.600 --> 05:01:07.200]   depends on the powerfulness of the model and how you use it and i hope you use it in a clever way
[05:01:07.200 --> 05:01:13.600]   to build amazing products i also want to thank very much many repositories that i have used
[05:01:13.600 --> 05:01:18.720]   as a self-studying material so because of course i didn't make up all this by myself i studied a
[05:01:18.720 --> 05:01:23.600]   lot of papers i read i think to study this diffusion models i read more than 30 papers
[05:01:23.600 --> 05:01:30.240]   in the last few weeks so it took me a lot of time but i was really passionate about this kind of
[05:01:30.240 --> 05:01:35.600]   models because they're complicated and i really like to study things that can generate new stuff
[05:01:35.600 --> 05:01:42.880]   so i want to really thank in particularly some resources that i have used let me see
[05:01:42.880 --> 05:01:49.200]   this one's here so the official code the this guy divam gupta this other repository from
[05:01:49.200 --> 05:01:56.640]   this person here which i used very much actually as a base and the diffusers library from this
[05:01:56.640 --> 05:02:03.040]   hugging face upon which i based most of the code of my sampler because i think it's better to use
[05:02:03.040 --> 05:02:06.960]   because we are actually just applying some formulas there is no point in writing it from
[05:02:06.960 --> 05:02:11.200]   zero the point is actually understanding what is happening with these formulas and why we are doing
[05:02:11.200 --> 05:02:16.480]   it the things we are doing and as usual the full code is available i will also make all the slides
[05:02:16.480 --> 05:02:21.920]   available for you guys and i hope if you are in china you also have a great holiday with me and
[05:02:21.920 --> 05:02:25.840]   if you're not in china i hope you have a great time with your family and friends and everyone
[05:02:25.840 --> 05:02:31.840]   else so welcome back to my channel anytime and please feel free to comment on send me a comment
[05:02:31.840 --> 05:02:37.520]   or if you didn't understand something or if you want me to explain something better because i'm
[05:02:37.520 --> 05:02:43.920]   always available for explanation and guys i do this not as my full-time job of course i do it
[05:02:43.920 --> 05:02:51.040]   as a part-time and lately i'm doing consulting so i'm very busy but sometime i take time to record
[05:02:51.040 --> 05:02:57.680]   videos and so please share my channel share my video with people if you like it and so that my
[05:02:57.680 --> 05:03:02.800]   channel can grow and i have more motivation to keep doing this kind of videos which take really
[05:03:02.800 --> 05:03:08.560]   a lot of time because to prepare a video like this i spend around many weeks of research but
[05:03:08.560 --> 05:03:14.640]   this is okay i do it for as a passion i don't do it as a job and i spend really a lot of time
[05:03:14.640 --> 05:03:20.080]   preparing all the slides and preparing all the speeches and preparing the code and cleaning it
[05:03:20.080 --> 05:03:26.400]   and commenting it etc etc i always do it for free so if you would like to support me the best way is
[05:03:26.400 --> 05:03:30.960]   to subscribe like my video and share it with other people thank you guys and have a nice day

