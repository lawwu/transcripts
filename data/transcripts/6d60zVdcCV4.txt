
[00:00:00.520 --> 00:00:08.120]   My name is Karina. Recently I've been working on CLOD, which is like a large
[00:00:08.120 --> 00:00:14.700]   language model trained by Anthropik. And most recently I was working on like
[00:00:14.700 --> 00:00:21.760]   reducing hallucinations, how to make CLOD self-correct its answers, and many
[00:00:21.760 --> 00:00:27.720]   other features that went into CLOD2 launch. And so I'm going to talk today
[00:00:27.720 --> 00:00:31.920]   about the writing principles for task-tuned prompt engineering, and kind of
[00:00:31.920 --> 00:00:38.880]   sort of like help you if you want to use CLOD API, help you to guide with the best
[00:00:38.880 --> 00:00:46.740]   practices and tips that I found and we found most effective. So first of all, I
[00:00:46.740 --> 00:00:53.040]   would like to talk about why prompting is hard. And to understand why prompting is
[00:00:53.040 --> 00:00:57.660]   hard, we should understand what prompting is in the first place. So this model
[00:00:57.660 --> 00:01:02.460]   estimate the probability of each subsequent word given the preceding
[00:01:02.460 --> 00:01:08.820]   words. So in a way, a well-crafted prompt can increase the probability of generated
[00:01:08.820 --> 00:01:15.600]   desired and accurate phrases. Due to attention mechanisms in large language
[00:01:15.600 --> 00:01:22.140]   models, the models can focus on specific parts of the input text, and so effective
[00:01:22.140 --> 00:01:27.600]   prompts ensure that the attention is directed properly for desired outputs, and
[00:01:27.600 --> 00:01:32.400]   and so it's important to incorporate task-specific keywords and context and
[00:01:32.400 --> 00:01:38.560]   examples within the prompt to activate the relevant portions of the model's internal
[00:01:38.560 --> 00:01:44.900]   knowledge. And lastly, prompting leads to better results because without the need of
[00:01:44.900 --> 00:01:52.600]   like computational, like other compute, you just like, without like model retraining, so you can like leverage
[00:01:52.600 --> 00:02:22.580]   inference time test compute for this. And so like why prompting is hard? And I think I found, based on like my conversations with customers and like developers, I think prompting is hard because of like three different reasons. First, people know, humans know what they want, but they don't know how to get the best performance from the model. And I think that's what we're going to focus on today. The second reason is that they vaguely know what they want, but they don't know how to explain what they want, but they don't know how to explain what they want.
[00:02:22.580 --> 00:02:36.580]   So they don't know how to explain the best to explain the best to the model. And so the model gets confused what the human wants from the task. And the third reason is like they don't know what they want. So the humans don't know what they want. So it's pretty bad.
[00:02:36.580 --> 00:03:04.580]   And it's hard for the model to understand. So basic strategies that you can like, if you find yourself, like you kind of really know what the task is, just provide a bunch of examples. And the model would be good at inferring what you're trying to do, just based on like examples. And the examples may be, should be diverse and should encapsulate a bunch of like age cases.
[00:03:04.580 --> 00:03:32.580]   Try to explain as you would have to explain to a five-year-old or like very, you know, in a very simple terms. And I think what I found is that like as you have to like be able to like iterate a lot and like spend a lot of time just prompting. And in a way, based on my experience, for example, as like research engineer, I spend the majority of pairing with people just collaborating on the prompts.
[00:03:32.580 --> 00:04:00.580]   In the past, I was thinking like how my main experience with the word prompt was only in the creative writing classes. I graded from Berkeley, and I took some like creative writing classes. And we usually have like exercises, like prompting exercises, right. And so we often forget that prompting language models is actually an act of creative writing. And I see people get annoyed, like why their prompts just don't work.
[00:04:00.580 --> 00:04:12.580]   But in most cases, I think it just means that people lack some kind of like originality or creativity to think of like new novel ways, how to make it work.
[00:04:12.580 --> 00:04:40.580]   I recently wrote a blog post about the cultures of writing. And one of the points that I'm making in the blog post is that prompting becomes like a new form of writing for any research engineer and like scientist who engages in daily, this kind of writing requires forming hypotheses. So you have to ask the model, okay, can the model do this? And you want to test that. Can the model self-correct its responses?
[00:04:40.580 --> 00:05:07.580]   Yes or no. And so you start trying like forming the hypothesis. Next, you test certain assumptions that you make about the model. And if as you iterate more, you kind of like get new insights or like, oh, yeah, the model's pretty good at this particular thing, but it's not super good at another thing to achieve this task. So you gain more clarity about like what's what's the strength and the weaknesses are.
[00:05:07.580 --> 00:05:32.580]   I wanted to start with like overall broad like writing principles. Because ultimately prompting is like writing, right? So the goal is to write prompts that clearly communicate the task objective, while providing just enough constraints and guidance to steer the model towards producing high quality and relevant outputs.
[00:05:32.580 --> 00:05:37.580]   And so there are like four to six kind of like writing guidelines that I think I found effective, especially working with like Claude. So the read, maybe to clarify how Claude is different from
[00:05:37.580 --> 00:05:42.580]   or GPT or GPT models. I think with Claude models, I think with Claude, you have to almost treat it as like another human. So you have to like explain things to a five-year-old. Or like you have to be elaborate.
[00:05:42.580 --> 00:05:47.580]   And like, I will share like more examples on like how to do that.
[00:05:47.580 --> 00:05:49.580]   And I will share like more examples on like how to do that. And I will share like more examples on like how to do that.
[00:05:49.580 --> 00:05:56.580]   But I think that's like a distinguishing feature from like GPT models from like GPT models from Claude.
[00:05:56.580 --> 00:06:03.580]   So the first principle is clarity. Like use simple, unambiguous language in your prompts.
[00:06:03.580 --> 00:06:05.580]   And then you have to have to almost treat it as like another human. So you have to like explain things to a five-year-old.
[00:06:05.580 --> 00:06:12.580]   Or like you have to be elaborate. And like I will share like more examples on like how to do that.
[00:06:12.580 --> 00:06:19.580]   But I think that's like a distinguishing feature from like GPT models from Claude.
[00:06:19.580 --> 00:06:30.580]   So the first principle is clarity. Like use simple, unambiguous language in your prompts. Avoid confusing syntax or vague phrases that could confuse the model.
[00:06:30.580 --> 00:06:39.580]   The second is conciseness. Keep prompts short and focused. Include only key information that the model needs.
[00:06:39.580 --> 00:06:48.580]   The third is coherence. Logically structure the prompts with context at the beginning and clear task at the end.
[00:06:48.580 --> 00:06:57.580]   Consistency. Stick to similar formatting. If you use XML tags, use it consistently in the prompt.
[00:06:57.580 --> 00:07:07.580]   If you use certain like terminology, do not like kind of like put the model of distribution basically. So make it consistent.
[00:07:07.580 --> 00:07:14.580]   Directions provide like genre, length, style, or any like guidelines. Guidance to direct the model's response.
[00:07:14.580 --> 00:07:30.580]   Ground prompts with examples, sources. Make the model to quote itself. Or like if you have like long document in the context, make it to quote from the document to support the argument.
[00:07:30.580 --> 00:07:41.580]   Or help the models to form, support the arguments from like search results or like other supporting contextual information.
[00:07:41.580 --> 00:07:46.580]   Engaging use diverse edge cases examples. Very useful for future prompting.
[00:07:46.580 --> 00:08:02.580]   Now I'm going to go through certain like tasks that I thought would be interesting and see how you can use Claude in those specific tasks.
[00:08:02.580 --> 00:08:11.580]   So the first case is obviously a recommendation system. As of last year, last year I made this project in Turalia.
[00:08:11.580 --> 00:08:28.580]   I used clip to kind of like, so I scraped a bunch of like images and like clothing items from different brands.
[00:08:28.580 --> 00:08:47.580]   And I used text as like an actual search engine. So you can say like James Bond girl. And you can do dress. And in a way you get like results that are like dress in the style of James Bond.
[00:08:47.580 --> 00:08:59.580]   Or you can do like futuristic, a thorough outfit. And it's like more like vibes based search. And you can like go and look at the shop itself.
[00:08:59.580 --> 00:09:00.580]   Yeah.
[00:09:00.580 --> 00:09:02.580]   Oh yeah.
[00:09:02.580 --> 00:09:16.580]   Clip is contrastive language to image model trained by open AI. It's open source, but it's basically, they provide embeddings for text and images. So you can like,
[00:09:16.580 --> 00:09:33.580]   And the way it works here is like you embed images and you embed, um, text results. And so what you can do is that you can like do a cosine similarity to find the most similar, um, items based on your database for the user query.
[00:09:33.580 --> 00:09:36.580]   I don't know if that's clear. Let me know if you have questions.
[00:09:36.580 --> 00:09:38.580]   It's the same thing as multimodal.
[00:09:38.580 --> 00:09:59.580]   Yeah. Um, I think you can read the paper clip. Yeah. Um, if you're interested.
[00:09:59.580 --> 00:10:12.580]   So I was, I was thinking like, okay, how could I use Claude in this project to curate relevant recommendations based on the user's requests? And that's the task.
[00:10:12.580 --> 00:10:25.580]   So in a way you have like users input, let's say dress in the style of Emma Chamberlain blazer, like in the great Gatsby movie and outfit with a futuristic vibe for the Met Gala.
[00:10:25.580 --> 00:10:40.580]   And on the other side, you have like image to text database, um, with images and their labels. And the labels can be produced either by like the original source or you can use like multimodal models to like come up with labels based on the images.
[00:10:40.580 --> 00:10:54.580]   Um, and so the task of the Claude is to curate, like based on the labels from the images, decide whether this like item relevant, should I recommend this to the user?
[00:10:54.580 --> 00:11:06.580]   Is this accurate? Like does it matches the, uh, users per like, can I personalize this? Um, and so if you can look at the,
[00:11:06.580 --> 00:11:35.580]   very simple curation strategy, um, for the prompt, you can just like zero shot at like, I need you to decide whether the item is relevant to the user query. Here's the user query. Um, here's the item description. Um, is it item relevant or should, should be recommended to the user based on the user's query and say yes or no, please to write the answer in answer tags.
[00:11:35.580 --> 00:12:04.580]   Um, let's unpack this. First of all, Claude really likes XML tags, really loves XML tags. I think like, this is like number one, not mistake, but like, um, one thing that people miss, they don't like put anything in XML tags. And so they don't have like very high, like good performance. So everything like, yeah, I do love XML tags. So you should like put everything in XML tags. And, um, with XML tags, it should like be consistent.
[00:12:04.580 --> 00:12:11.580]   like be consistent. So what is user query item. And you can be very descriptive. I can share like more examples later on.
[00:12:11.580 --> 00:12:26.580]   Um, and here in a way you like the way you interact with cloud is like, you can see the language here. I need you to decide whether the item is relevant. It's almost feels like you talk to a human. Um,
[00:12:26.580 --> 00:12:37.580]   Um, yeah. So, uh, this XML thing recently only came out like maybe like a month or two ago from official and dropping advice.
[00:12:37.580 --> 00:12:43.580]   Oh, really? Is this something that was like intentionally trained for or you discovered it after?
[00:12:43.580 --> 00:13:01.580]   Um, I think it was, uh, I mean, we tried to XML, XML formatting was the first formatting that we like kind of like fine tuned on. Like, um, later on, we discovered, you know, customers need like markdown or like need,
[00:13:01.580 --> 00:13:10.580]   Claude needs to like use JSON formatting. So like we kind of like learn from customers, but originally it was like XML formatting. Yeah.
[00:13:10.580 --> 00:13:16.580]   So is that mostly because of the fine tuning or is that because you had a training set that had tons of XML stuff in it?
[00:13:16.580 --> 00:13:25.580]   Uh, I think it's kind of both. Yeah. Um, yeah.
[00:13:25.580 --> 00:13:36.580]   Close XML tags. Yeah. I don't know. Um,
[00:13:36.580 --> 00:13:43.580]   Oh yeah. Sorry. I had a mistake. Yes, I have. Um, yeah.
[00:13:43.580 --> 00:13:51.580]   Um, yeah. And so like one, one good thing about the XML tags is like, it's really easy to extract, right? Like the strings inside it.
[00:13:51.580 --> 00:13:59.580]   And Claude is pretty good at like, um, I can like say like, don't put anything in XML tags.
[00:13:59.580 --> 00:14:02.580]   So sometimes like Claude will like say, here's information, blah, blah, blah, blah.
[00:14:02.580 --> 00:14:09.580]   But then if you ask, just like write the answer in like this tags, Claude will not put any additional information,
[00:14:09.580 --> 00:14:12.580]   which is one of the most annoying thing was like language models.
[00:14:12.580 --> 00:14:20.580]   So here's the results, uh, that I put, um, this is through Claude at AI interface.
[00:14:20.580 --> 00:14:26.580]   Um, and so yeah, you can see like, is this item relevant? Uh, says no.
[00:14:26.580 --> 00:14:37.580]   Is this item relevant? Yes. But I don't think that's like a hundred percent like, um, you know, perfect system.
[00:14:37.580 --> 00:14:42.580]   So it's very like zero shots. So basically you can like iterate and we'll try to iterate more in this.
[00:14:42.580 --> 00:14:54.580]   Um, so number two is that like when, uh, you put, you ask the model to take some time to think whether the item is relevant or not.
[00:14:54.580 --> 00:15:04.580]   Uh, in thoughts tags based on the criteria above, um, and you kind of like let the model think, um, a little bit more, um, with its reasoning.
[00:15:04.580 --> 00:15:09.580]   With its reasoning. And then, uh, this is like basically chain of thoughts.
[00:15:09.580 --> 00:15:17.580]   You can also add like criteria. Uh, so as a part of your critique, consider the following criteria.
[00:15:17.580 --> 00:15:23.580]   And so if you want to like steer the model on like, does the item match the specific attributes requested by user?
[00:15:23.580 --> 00:15:32.580]   Like help the model to like think kind of, kind of like think through like what means, what does it mean to like recommend an item to the user?
[00:15:32.580 --> 00:15:37.580]   Does the item match the season or whether conditions match it in the user's query?
[00:15:37.580 --> 00:15:41.580]   For example, you should not recommend winter coats during summer seasons.
[00:15:41.580 --> 00:15:46.580]   So like in the criteria, you can like give more examples, more elaborate examples.
[00:15:46.580 --> 00:15:56.580]   Um, another thing that you iterate on is like, not just like give answer yes or no, but you can like based on your critique score,
[00:15:56.580 --> 00:16:06.580]   whether the item should be recommended or not, where one is least to be recommended and 10 is highly recommended and put the final score in score tags.
[00:16:06.580 --> 00:16:21.580]   And so how does it work? Uh, so, um, here like user query, James Bond blazer, um, item, which I took from, um, um, I think it was some brand.
[00:16:21.580 --> 00:16:32.580]   Um, and Claude would like start like thoughts tags. Um, overall, it seems like very relevant and the final score is nine.
[00:16:32.580 --> 00:16:41.580]   Um, and here's another example. I want to dress in the style of the great Gatsby movie. Um, here's the item braided cord, cropped waist coast.
[00:16:41.580 --> 00:16:48.580]   And the critique is basically, uh, the item is not appropriate for the user needs based on the context clues in the query.
[00:16:48.580 --> 00:16:52.580]   Um, it doesn't like, you know, match the attributes of the great Gatsby movie.
[00:16:52.580 --> 00:16:59.580]   It like tries to like have like some reasoning. And so the score is two, and you can be a little bit more elaborate.
[00:16:59.580 --> 00:17:06.580]   This is like very simple, like iteration, um, on, on that. Do you guys have any questions? Yeah.
[00:17:06.580 --> 00:17:11.580]   So one interesting thing that I saw, I mean, there was the XML tags now that were closed.
[00:17:11.580 --> 00:17:18.580]   My friend, he is not a native English speaker. His prompts are always in, in very kind of funny English,
[00:17:18.580 --> 00:17:25.580]   but he structures them really well and they work really well despite the English being very incorrect. Right.
[00:17:25.580 --> 00:17:27.580]   Right. Why, why does that work?
[00:17:27.580 --> 00:17:40.580]   I think it's just the morals are like pretty good at like knowledge transfer between like languages or like can infer very well on like the user's intent. Um, yeah.
[00:17:40.580 --> 00:17:43.580]   Yeah. I don't have like pretty clear answer.
[00:17:43.580 --> 00:17:50.580]   So that text with one syntax mistake looks close enough with the text with the right syntax.
[00:17:50.580 --> 00:17:53.580]   The probability of the real answer is close in four cases.
[00:17:53.580 --> 00:17:54.580]   Right.
[00:17:54.580 --> 00:17:55.580]   Like yes, I don't know.
[00:17:55.580 --> 00:17:56.580]   Yeah.
[00:17:56.580 --> 00:18:02.580]   In this particular example, I'm curious on whether you see any bias with the score.
[00:18:02.580 --> 00:18:04.580]   In other words, if you were to look at the distribution of scores.
[00:18:04.580 --> 00:18:05.580]   Right.
[00:18:05.580 --> 00:18:06.580]   Would it be a normal distribution?
[00:18:06.580 --> 00:18:07.580]   Yeah.
[00:18:07.580 --> 00:18:08.580]   This is an interesting question.
[00:18:08.580 --> 00:18:12.580]   Like this is one question that we ask in our research settings.
[00:18:12.580 --> 00:18:18.580]   Like one thing that we are trying to understand, like we have a research group called societal impacts.
[00:18:18.580 --> 00:18:31.580]   And one thing that we are trying to understand now is like when you summarize like news articles and you try to evaluate like the bias was kind of the distribution.
[00:18:31.580 --> 00:18:35.580]   And I feel like this is like research active, like, yeah, I think it depends on the task.
[00:18:35.580 --> 00:18:38.580]   Um, really I did not test on this.
[00:18:38.580 --> 00:18:45.580]   Literally it was yesterday, uh, prompting.
[00:18:45.580 --> 00:18:46.580]   Cool.
[00:18:46.580 --> 00:18:49.580]   Um, the second task.
[00:18:49.580 --> 00:18:59.580]   Um, so Claude is known for a hundred K context, uh, size, which is the entire book of the great Gatsby can like put into the context.
[00:18:59.580 --> 00:19:06.580]   And you can like ask the model, uh, summarize the book or like, uh, ask some tasks based on the huge context.
[00:19:06.580 --> 00:19:13.580]   And this is like basically time test compute, um, thing.
[00:19:13.580 --> 00:19:16.580]   And so it was long context.
[00:19:16.580 --> 00:19:19.580]   Um, let me see.
[00:19:19.580 --> 00:19:24.580]   The way you can use long context can be in different ways.
[00:19:24.580 --> 00:19:31.580]   Like one way is like you put multiple documents and try to summarize or like retrieve information based on the documents.
[00:19:31.580 --> 00:19:37.580]   Another way to use long context is to have a bunch, a huge few shot prompt.
[00:19:37.580 --> 00:19:50.580]   And so as you know, like chain of thought, um, technique relies on the stated reasoning, faithfully reflecting the models, actual reasoning.
[00:19:50.580 --> 00:19:55.580]   And in one of the recent papers, we found that it's not super, like, it's not always the case.
[00:19:55.580 --> 00:20:14.580]   So doesn't, so basically what it means is that like, if you ask the model, do a chain of thought, it might not necessarily attend to, you know, chain of thought, uh, to produce the final answer.
[00:20:14.580 --> 00:20:18.580]   Um, it might just like ignore it or like, uh, not take any account.
[00:20:18.580 --> 00:20:21.580]   So it's not, we, we call it like unfaithful basically.
[00:20:21.580 --> 00:20:23.580]   It's not super faithful.
[00:20:23.580 --> 00:20:36.580]   And so if you propose in this paper, um, like decomposition based methods can actually achieve like strong performance on specifically question answering tasks.
[00:20:36.580 --> 00:20:41.580]   Sometimes approaching that of chain of thought performance while improving the faithfulness.
[00:20:41.580 --> 00:20:45.580]   Do you guys have any questions?
[00:20:45.580 --> 00:20:46.580]   Okay.
[00:20:46.580 --> 00:20:47.580]   Yeah.
[00:20:47.580 --> 00:20:59.580]   Um, faithfulness is, um, yes, to your prompt.
[00:20:59.580 --> 00:21:00.580]   Yeah.
[00:21:00.580 --> 00:21:01.580]   What's decomposition?
[00:21:01.580 --> 00:21:02.580]   What's decomposition?
[00:21:02.580 --> 00:21:03.580]   Yeah.
[00:21:03.580 --> 00:21:05.580]   Uh, let me explain what decomposition is.
[00:21:05.580 --> 00:21:08.580]   So here's the graph from the paper.
[00:21:08.580 --> 00:21:11.580]   Uh, we have like three methods.
[00:21:11.580 --> 00:21:16.580]   First is a chain of thought method, which is like, uh, here's the question.
[00:21:16.580 --> 00:21:19.580]   Could, could we do fit in a kangaroo pouch?
[00:21:19.580 --> 00:21:20.580]   Uh, there are two choices.
[00:21:20.580 --> 00:21:22.580]   A yes, B no.
[00:21:22.580 --> 00:21:28.580]   Chain of thought prompt saying like, let's think step by step.
[00:21:28.580 --> 00:21:31.580]   Um, step by step gives the reasoning.
[00:21:31.580 --> 00:21:35.580]   Um, the human asked the follow up questions based on the above.
[00:21:35.580 --> 00:21:38.580]   What is the single most likely answer choice?
[00:21:38.580 --> 00:21:43.580]   Uh, and the model says the correct answer choices is B, right?
[00:21:43.580 --> 00:21:53.580]   The chain of thought decomposition is when you decompose a question, when you can ask the model to decompose a question into like multiple sub questions.
[00:21:53.580 --> 00:22:04.580]   So that each sub question are kind of independent from each other because in chain of thought, like you have one, two, three, you know, like they kind of like can influence each other, right?
[00:22:04.580 --> 00:22:14.580]   Like in decomposition, you kind of like, um, you, you decompose and you like put each sub question in the independent context.
[00:22:14.580 --> 00:22:17.580]   So in a way it kind of like reduces the bias.
[00:22:17.580 --> 00:22:23.580]   Um, and so in this, um, let's, let's see here.
[00:22:23.580 --> 00:22:27.580]   Is it like a sub question one, uh, what type of animal is Scooby-Doo?
[00:22:27.580 --> 00:22:31.580]   The answer from the model, Scooby-Doo is a fictional character.
[00:22:31.580 --> 00:22:37.580]   Another sub question for, uh, for the assistant for Claude, how big is an average kangaroo pouch?
[00:22:37.580 --> 00:22:42.580]   And you would, and what you can see is that like each sub question is kind of like self-contained.
[00:22:42.580 --> 00:22:45.580]   It's very atomic self-contained question.
[00:22:45.580 --> 00:22:49.580]   Um, and so you have like multiple sub questions like this.
[00:22:49.580 --> 00:22:52.580]   And then what you do is you recompose.
[00:22:52.580 --> 00:23:03.580]   So like you like put sub question, answer, sub question, answer, sub question, answer into like one context and ask the model based on the above, what is the single most likely answer choice?
[00:23:03.580 --> 00:23:04.580]   The correct answer choice.
[00:23:04.580 --> 00:23:05.580]   The correct answer choice is B.
[00:23:05.580 --> 00:23:06.580]   Yeah.
[00:23:06.580 --> 00:23:10.580]   Um, like in the system prompt or whatever the user's prompt is, like we mentioned less things step-by-step.
[00:23:10.580 --> 00:23:11.580]   Um, what do you do for the decomposition?
[00:23:11.580 --> 00:23:25.580]   Is there like a similar, you know, input to the model to make it decompose into multiple questions?
[00:23:25.580 --> 00:23:26.580]   Yeah.
[00:23:26.580 --> 00:23:31.580]   Uh, I can share the prompt, um, in a few slides, uh, on this.
[00:23:31.580 --> 00:23:34.580]   Um, but, uh, yeah.
[00:23:34.580 --> 00:23:35.580]   Any other questions?
[00:23:35.580 --> 00:23:37.580]   Could you show the graph again?
[00:23:37.580 --> 00:23:38.580]   The, this graph?
[00:23:38.580 --> 00:23:39.580]   Yeah.
[00:23:39.580 --> 00:23:52.580]   Um, yeah, let's look at the prompt.
[00:23:52.580 --> 00:23:58.580]   Um, very hard to see, but I'll share the slides.
[00:23:58.580 --> 00:24:05.580]   Um, let's, um, I, I'm, I'm gonna give you like legal context, like legal question.
[00:24:05.580 --> 00:24:19.580]   Let's say you have a question on like, a legal question and you ask us like which of the following is the most persuasive argument that a person is liable to the creditor under the terms of the agreement and here's the context.
[00:24:19.580 --> 00:24:21.580]   So that's the question basically.
[00:24:21.580 --> 00:24:25.580]   And so you have like choices for the model.
[00:24:25.580 --> 00:24:28.580]   So this is like multiple choice question.
[00:24:28.580 --> 00:24:31.580]   And before that you have like a huge few shot prompt.
[00:24:31.580 --> 00:24:39.580]   Um, and basically here to answer your question, like it says, I'm going to give you a question.
[00:24:39.580 --> 00:24:42.580]   I want you to compose into a series of sub questions.
[00:24:42.580 --> 00:24:47.580]   Each sub question should be self contained with all the information necessary.
[00:24:47.580 --> 00:24:52.580]   Um, this is really important, blah, blah, blah.
[00:24:52.580 --> 00:24:57.580]   Uh, make sure not to decompose more than necessary.
[00:24:57.580 --> 00:25:01.580]   Um, be concise, blah, blah, blah.
[00:25:01.580 --> 00:25:07.580]   Please put each sub question in like this tags, but include the numbers corresponding to each tag.
[00:25:07.580 --> 00:25:11.580]   So, um, and the model says, yes, I understand.
[00:25:11.580 --> 00:25:14.580]   Uh, you have a question.
[00:25:14.580 --> 00:25:22.580]   Uh, multiple choice answers and the model provides sub questions for you.
[00:25:22.580 --> 00:25:28.580]   And then what you do is that you try to answer the first sub question and you give it to the model.
[00:25:28.580 --> 00:25:29.580]   You try to answer the second sub question.
[00:25:29.580 --> 00:25:30.580]   You give it to the model.
[00:25:30.580 --> 00:25:32.580]   The third sub question you give to the model.
[00:25:32.580 --> 00:25:43.580]   And then later you say like based on everything above, like you give all the context, um, answer me the question.
[00:25:43.580 --> 00:25:45.580]   Um, the correct answer C.
[00:25:45.580 --> 00:25:46.580]   Yeah.
[00:25:46.580 --> 00:25:53.580]   And so this is like very similar in the legal context.
[00:25:53.580 --> 00:25:58.580]   You have sub questions like what is consideration of contract law, blah, blah, blah.
[00:25:58.580 --> 00:26:01.580]   And you have, you can have like another model to like sample here.
[00:26:01.580 --> 00:26:03.580]   You can have like another model to answer this.
[00:26:03.580 --> 00:26:06.580]   It doesn't necessarily should be like one model.
[00:26:06.580 --> 00:26:14.580]   Um, and then there's like another sub question and here's the answer.
[00:26:14.580 --> 00:26:15.580]   Yeah.
[00:26:15.580 --> 00:26:18.580]   Do you guys have any questions?
[00:26:18.580 --> 00:26:32.580]   Um, the second thing that I want to talk about is how to use claw to do evaluations.
[00:26:32.580 --> 00:26:38.580]   Like evaluating like clawed on like long context ability.
[00:26:38.580 --> 00:26:50.580]   Um, let's say you have a lot of like documents and you want to understand how good clawed is answering questions based on the document.
[00:26:50.580 --> 00:27:01.580]   Or is it, is it able to answer like the questions, not just like from its protein knowledge, but like based on the document itself.
[00:27:01.580 --> 00:27:12.580]   Um, and so, um, I'm going to give you an example that we did at Anthropic, um, multiple choice QA, um, evaluation design.
[00:27:12.580 --> 00:27:23.580]   So our goal was to, with this experience to evaluate techniques to maximize clawed chance to correctly recalling a specific piece of information from a long document.
[00:27:23.580 --> 00:27:31.580]   And so the document that we chose was the government document that contains like a bunch of like meeting transcripts, different departments,
[00:27:31.580 --> 00:27:43.580]   And you also chose the one that was like, uh, from this year, July 13th, uh, which is like way after clots, um, training data cut off.
[00:27:43.580 --> 00:27:51.580]   So that you don't like, um, you have the document that does not have in the preteen knowledge or something.
[00:27:51.580 --> 00:28:01.580]   And so what you're trying to do is like, now you want to use clawed to generate question answers pairs.
[00:28:01.580 --> 00:28:11.580]   Um, you, in a way like you create like data, data set based, you use language models to create like data sets.
[00:28:11.580 --> 00:28:20.580]   And so the way you do that is that you split the document into sections and use clawed to generate like five multiple choice questions for each section.
[00:28:20.580 --> 00:28:23.580]   Each was three wrong answers and one right answer.
[00:28:23.580 --> 00:28:37.580]   And if you do that, you then reassemble like randomized sets of those sections into like long documents that you could pass them to clawed and test its recall, uh, of their contents.
[00:28:37.580 --> 00:28:39.580]   This is very matter.
[00:28:39.580 --> 00:28:41.580]   Let me know if you have questions.
[00:28:41.580 --> 00:28:42.580]   Yeah.
[00:28:42.580 --> 00:28:50.580]   Um, so here's a prompt to generate multiple choice questions.
[00:28:50.580 --> 00:29:00.580]   Um, I asked, please write five actual questions for those, um, some guidelines at the end.
[00:29:00.580 --> 00:29:17.580]   Um, and basically we test different strategies, prompting strategies, just asking clawed, give clawed two fixed examples of correctly answered general knowledge and, um, that are unrelated to the government document.
[00:29:17.580 --> 00:29:24.580]   Um, providing two examples and providing five examples of correctly answered questions.
[00:29:24.580 --> 00:29:29.580]   And we tested the strategies, uh, on different settings.
[00:29:29.580 --> 00:29:36.580]   Like one is containing the answer positioned at the beginning, the end or the middle and the input.
[00:29:36.580 --> 00:29:41.580]   And we tested with like 70K and 95K token documents.
[00:29:41.580 --> 00:29:51.580]   You can look at the prompts and more specific how we did this in our blog post.
[00:29:51.580 --> 00:29:54.580]   But basically the results is this.
[00:29:54.580 --> 00:30:08.580]   Uh, here we see that, um, yeah, the metric was, um, to, let's see.
[00:30:08.580 --> 00:30:24.580]   So, let's see, like basically how many, how many times like clawed has correctly answered the question.
[00:30:24.580 --> 00:30:26.580]   Um, right.
[00:30:26.580 --> 00:30:31.580]   And so, yeah, sorry.
[00:30:31.580 --> 00:30:41.580]   Uh, basically what we find is that like for document Q and A, asking the question at the end of the prompt performs a lot better than asking at the beginning.
[00:30:41.580 --> 00:30:42.580]   And you can see it here.
[00:30:42.580 --> 00:30:50.580]   Uh, pulling relevant quotes into like critique or like thoughts tags is helpful.
[00:30:50.580 --> 00:30:57.580]   Um, it's like the small cost to latency, but improves accuracy.
[00:30:57.580 --> 00:31:01.580]   Uh, and we tested on like both clawed and clawed instant.
[00:31:01.580 --> 00:31:12.580]   Um, and it seems like you can boost way better performance from like clawed instant, um, than clawed too.
[00:31:12.580 --> 00:31:24.580]   Um, basically the idea is that like, if you want to use long doc Q and A, put the instructions at the end of your prompt.
[00:31:24.580 --> 00:31:25.580]   Yeah.
[00:31:25.580 --> 00:31:26.580]   That's like the results of this.
[00:31:26.580 --> 00:31:31.580]   What was the, I didn't catch, what was the scratch pad in the.
[00:31:31.580 --> 00:31:32.580]   Uh, oh yeah.
[00:31:32.580 --> 00:31:38.580]   Like you just asked the model to like put thoughts in like thoughts tags before answering the question.
[00:31:38.580 --> 00:31:40.580]   So it has like more reasoning based.
[00:31:40.580 --> 00:31:41.580]   Yeah.
[00:31:41.580 --> 00:31:42.580]   Can you go to the table again?
[00:31:42.580 --> 00:31:43.580]   Sorry.
[00:31:43.580 --> 00:31:44.580]   Yeah.
[00:31:44.580 --> 00:31:45.580]   Sorry.
[00:31:45.580 --> 00:31:46.580]   I'm, uh,
[00:31:56.580 --> 00:31:57.580]   Yeah.
[00:31:57.580 --> 00:32:02.580]   But like the outcome was basically putting it at the end matters more than all the other.
[00:32:02.580 --> 00:32:03.580]   Yeah.
[00:32:03.580 --> 00:32:04.580]   Optimizing.
[00:32:04.580 --> 00:32:05.580]   Right.
[00:32:05.580 --> 00:32:06.580]   Yeah.
[00:32:06.580 --> 00:32:07.580]   Um,
[00:32:07.580 --> 00:32:08.580]   Yeah.
[00:32:08.580 --> 00:32:18.580]   Anyway, this is like an example to show like how to use cloud to generate a data set that you
[00:32:18.580 --> 00:32:22.580]   can like evaluate and like you can use it for like evaluation basically.
[00:32:22.580 --> 00:32:23.580]   Yeah.
[00:32:23.580 --> 00:32:28.580]   So this has to do with putting your instruction at the end of the prompt.
[00:32:28.580 --> 00:32:32.580]   Uh, first of the theories on like why, why specifically the instructions should be at the end.
[00:32:32.580 --> 00:32:45.580]   And are there any things like, do we have any understanding of like, are there certain things at the beginning of the prompt that still might be weighted?
[00:32:45.580 --> 00:32:51.580]   Or is it like this sliding scale that like the further in the beginning of the prompt, like the less attention it gets?
[00:32:51.580 --> 00:32:52.580]   Yeah.
[00:32:52.580 --> 00:32:54.580]   I think that's basically the hypothesis.
[00:32:54.580 --> 00:32:55.580]   Okay.
[00:32:55.580 --> 00:32:58.580]   It's like the, you know, it's like the distance.
[00:32:58.580 --> 00:33:03.580]   It's like the model attends more to the end of the prompt, uh, than the beginning.
[00:33:03.580 --> 00:33:04.580]   Okay.
[00:33:04.580 --> 00:33:05.580]   It's not like a u or a physics example.
[00:33:05.580 --> 00:33:08.580]   I think there was a paper saying like it just forgets in the middle or something.
[00:33:08.580 --> 00:33:09.580]   Okay.
[00:33:09.580 --> 00:33:10.580]   Um, yeah.
[00:33:10.580 --> 00:33:14.580]   I think this is the problem with like long context that you're trying to fix or something.
[00:33:14.580 --> 00:33:15.580]   Yeah.
[00:33:15.580 --> 00:33:24.580]   So to follow on that question, you're saying that there was a paper that said that it remembers the beginning and then and kind of forget in the middle.
[00:33:24.580 --> 00:33:25.580]   Yeah.
[00:33:25.580 --> 00:33:29.580]   So what you're saying is for plot to it seems to do best if you give it at the end.
[00:33:29.580 --> 00:33:30.580]   Yeah.
[00:33:30.580 --> 00:33:32.580]   So that paper doesn't apply to plot.
[00:33:32.580 --> 00:33:36.580]   Um, I did not read that paper.
[00:33:36.580 --> 00:33:37.580]   Like,
[00:33:37.580 --> 00:33:38.580]   No, I'm just curious.
[00:33:38.580 --> 00:33:41.580]   What you're saying is you're finding it at least for plot.
[00:33:41.580 --> 00:33:42.580]   Right.
[00:33:42.580 --> 00:33:44.580]   The end part gets more attention.
[00:33:44.580 --> 00:33:45.580]   Yeah.
[00:33:45.580 --> 00:33:46.580]   Yeah.
[00:33:46.580 --> 00:33:50.580]   For like a specific task is like a long context, uh, Q and A for like long documents.
[00:33:50.580 --> 00:33:51.580]   Yeah.
[00:33:51.580 --> 00:33:57.580]   Um, yeah, we have not tested on other tasks to my knowledge.
[00:33:57.580 --> 00:34:05.580]   Um, so the prompts you showed were using regular pros and then the XML tags.
[00:34:05.580 --> 00:34:09.580]   Um, I think that's also what's in the entropic docs.
[00:34:09.580 --> 00:34:17.580]   Have you guys ever done experiments on like that kind of format versus Markdown versus everything is in XML?
[00:34:17.580 --> 00:34:19.580]   Do you have any thoughts on that?
[00:34:19.580 --> 00:34:20.580]   Yeah.
[00:34:20.580 --> 00:34:38.580]   So, um, in general, I think, I think it's because Markdown was kind of like, there's not that much of like, I don't know, it's like best in XML tags.
[00:34:38.580 --> 00:34:51.580]   Like I'm, I'm thinking the, I've like tried to like, you know, use like Jason, or like, uh, use Markdown, but sometimes it's like, you know, it's not as good as like XML.
[00:34:51.580 --> 00:34:53.580]   With XML, it's almost a hundred percent accuracy.
[00:34:53.580 --> 00:34:54.580]   Yeah.
[00:34:54.580 --> 00:34:55.580]   Let's see.
[00:34:55.580 --> 00:34:56.580]   Yeah.
[00:34:56.580 --> 00:34:59.580]   Um, let's go to another task.
[00:34:59.580 --> 00:35:00.580]   Okay.
[00:35:00.580 --> 00:35:01.580]   Yeah.
[00:35:01.580 --> 00:35:04.580]   Um, let's go to another task.
[00:35:04.580 --> 00:35:15.580]   Um, let's go to another task, um, which is like, you can use language models to like auto label basically anything.
[00:35:15.580 --> 00:35:26.580]   Um, so one of the examples that we did last year, um, we asked Lot to categorize the labels for the clusters.
[00:35:26.580 --> 00:35:34.580]   And so, um, this was for the paper, but the approach was very simple.
[00:35:34.580 --> 00:35:39.580]   We have a bunch of like, you know, texts and we embed them in you map.
[00:35:39.580 --> 00:36:01.580]   Um, and we do like K and N clustering and for each cluster, sorry, K means clustering and for each cluster select like, for each cluster aggregate all the, you know, little like statements or claims.
[00:36:01.580 --> 00:36:08.580]   Um, and we ask the model to come up with the category for this cluster.
[00:36:08.580 --> 00:36:10.580]   So that's the approach.
[00:36:10.580 --> 00:36:16.580]   And you can look at the other labels.
[00:36:16.580 --> 00:36:21.580]   Uh, labels here are not super good because we used cloud 1.3 at that time.
[00:36:21.580 --> 00:36:24.580]   Cloud 2 is supposed to be like way better at this.
[00:36:24.580 --> 00:36:26.580]   Um, but this is like, you know, cache.
[00:36:26.580 --> 00:36:27.580]   It was like last year.
[00:36:27.580 --> 00:36:33.580]   Um, where's my slides?
[00:36:33.580 --> 00:36:47.580]   And so one thing that you can do with this kind of task, we call it self consistency.
[00:36:47.580 --> 00:36:51.580]   You can generate N samples for the question.
[00:36:51.580 --> 00:36:58.580]   Um, so let's say you have a question like, how do you label this cluster?
[00:36:58.580 --> 00:37:05.580]   And you generate independently and times and you can ask just like come up with like one category.
[00:37:05.580 --> 00:37:11.580]   Um, well, this method is mostly useful for like quantitative.
[00:37:11.580 --> 00:37:18.580]   Like if you have like a math question and you sample like different, like sample multiple times and come up with the answer.
[00:37:18.580 --> 00:37:25.580]   Um, like the most common answer is, uh, the one that you select with the final answer.
[00:37:25.580 --> 00:37:28.580]   And this is called the majority of vote.
[00:37:28.580 --> 00:37:39.580]   Another technique that you can use is like, um, have like two generated samples and ask another model to evaluate whether those samples are consistent or not.
[00:37:39.580 --> 00:37:43.580]   And if the samples are consistent, well, you gain more confidence that this is correct.
[00:37:43.580 --> 00:37:44.580]   Right.
[00:37:44.580 --> 00:37:49.580]   And if it's not consistent, you just like the select.
[00:37:49.580 --> 00:38:05.580]   Um, another thing that you want to do with cloud is, uh, if you, if cloud is kind of like misses the nuance, especially for like categorizing a lot of labels and you have like a lot of like categorizations.
[00:38:05.580 --> 00:38:16.580]   Um, you can add contrasting conceptual distinctions in your instruction and you can do it in multiple ways.
[00:38:16.580 --> 00:38:18.580]   One way to do this, like you provide bad example.
[00:38:18.580 --> 00:38:25.580]   Let's say like, here's a very bad category and you should never come up with it because this is like too narrow or like too general.
[00:38:25.580 --> 00:38:26.580]   And this is not what I want.
[00:38:26.580 --> 00:38:35.580]   Uh, like give like contrastive like examples and vary the context, use examples in different contexts and settings.
[00:38:35.580 --> 00:38:45.580]   Um, not just like, just like have like more diversity, like diversity is like, um, and the more diverse, like future prompts examples, the better.
[00:38:45.580 --> 00:38:48.580]   Use analogies and metaphors.
[00:38:48.580 --> 00:38:59.580]   Um, if the concept is like too hard to understand for the model, try to like decompose and like bring analogy, um, point out like common misconceptions.
[00:38:59.580 --> 00:39:09.580]   Um, especially for like categorizing, like, let's say what is false presupposition, right?
[00:39:09.580 --> 00:39:22.580]   Like, uh, point out the common misconception and like clarify like why this is like incorrect, like provide examples that explicitly show why common misconception is wrong.
[00:39:22.580 --> 00:39:28.580]   Uh, yes, do you guys have any questions?
[00:39:28.580 --> 00:39:29.580]   Yeah.
[00:39:29.580 --> 00:39:57.580]   Um, I can not speak here, but, um, yeah, like come up with a category.
[00:39:57.580 --> 00:40:05.580]   Like, um, yeah, come up with a category or like classify, uh, like label that cluster basically.
[00:40:05.580 --> 00:40:13.580]   Um, so here's like very basic like tips and strategies with cloud API.
[00:40:13.580 --> 00:40:16.580]   Um, number one is formatting.
[00:40:16.580 --> 00:40:21.580]   Um, like human assistant is like what cloud loves.
[00:40:21.580 --> 00:40:25.580]   And if you miss this, you miss it, like you'll get like very, very terrible results.
[00:40:25.580 --> 00:40:30.580]   Uh, new line, new line, human, new line, new line assistant.
[00:40:30.580 --> 00:40:31.580]   Um, yeah.
[00:40:31.580 --> 00:40:42.580]   Uh, you can also put words in cloud's mouth to like, kind of like say like, do you understand it?
[00:40:42.580 --> 00:40:46.580]   And you can like put in the, um, cloud's mouth?
[00:40:46.580 --> 00:40:47.580]   Yes.
[00:40:47.580 --> 00:40:48.580]   I understand it.
[00:40:48.580 --> 00:40:52.580]   And a way to like, you know, put, put the model into these modes.
[00:40:52.580 --> 00:40:55.580]   Have cloud repeat instructions back.
[00:40:55.580 --> 00:41:01.580]   Um, you can say like, do you understand the instructions?
[00:41:01.580 --> 00:41:07.580]   Um, and you can put like assistant, yes, I understand instructions.
[00:41:07.580 --> 00:41:11.580]   Blah, blah, blah, blah.
[00:41:11.580 --> 00:41:19.580]   Uh, to reduce hallucinations, like let cloud hedge and like say like, I don't know.
[00:41:19.580 --> 00:41:24.580]   Or like, uh, I don't have enough information or like context to answer the question.
[00:41:24.580 --> 00:41:31.580]   Um, here's another thing.
[00:41:31.580 --> 00:41:57.580]   Um, if you have like generate direct quotes, if you have like a document or like, um, a long document in the context, um, make cloud to say, find appropriate quotes, but also say like, um, if there are no quotes in this document that seems relevant to this question, please just say, I don't find any relevant quotes.
[00:41:57.580 --> 00:42:01.580]   So that it doesn't make up or fabricate new quotes.
[00:42:01.580 --> 00:42:05.580]   Uh, how to give good examples.
[00:42:05.580 --> 00:42:09.580]   Um, are the examples similar to the ones you need to classify?
[00:42:09.580 --> 00:42:15.580]   Are the examples diverse enough for cloud not to overfit to, to the specifics?
[00:42:15.580 --> 00:42:18.580]   Equally distributed among answer types.
[00:42:18.580 --> 00:42:22.580]   Don't always choose option A, but like, you kind of like have the diversity.
[00:42:22.580 --> 00:42:26.580]   Um, yeah.
[00:42:26.580 --> 00:42:27.580]   Yeah.
[00:42:27.580 --> 00:42:28.580]   I got a lot of past.
[00:42:28.580 --> 00:42:29.580]   Yeah.
[00:42:29.580 --> 00:42:30.580]   Oh, formatting in a way.
[00:42:30.580 --> 00:42:31.580]   Oh, here.
[00:42:31.580 --> 00:42:33.580]   Um, I think they didn't put like new line, new line.
[00:42:33.580 --> 00:42:34.580]   Pretty sure.
[00:42:34.580 --> 00:42:35.580]   Oh, here.
[00:42:35.580 --> 00:42:36.580]   Sorry.
[00:42:36.580 --> 00:42:37.580]   Yes.
[00:42:37.580 --> 00:42:38.580]   Here.
[00:42:38.580 --> 00:42:39.580]   Um, I think they didn't put like new line, new line.
[00:42:39.580 --> 00:42:40.580]   Pretty sure.
[00:42:40.580 --> 00:42:41.580]   Oh, here.
[00:42:41.580 --> 00:42:42.580]   Sorry.
[00:42:42.580 --> 00:42:43.580]   Yes.
[00:42:43.580 --> 00:42:44.580]   Here.
[00:42:44.580 --> 00:42:47.580]   Um, I think they didn't put like new line, new line.
[00:42:47.580 --> 00:42:48.580]   Pretty sure.
[00:42:48.580 --> 00:42:49.580]   Oh, here.
[00:42:49.580 --> 00:42:50.580]   Sorry.
[00:42:50.580 --> 00:42:51.580]   Here.
[00:42:51.580 --> 00:42:55.580]   You put like human assistant inside the XML tags.
[00:42:55.580 --> 00:43:07.580]   You only need, you only have to use human assistant as like tokens to like sample, but you should
[00:43:07.580 --> 00:43:12.580]   never put it in like, like inside the context itself.
[00:43:12.580 --> 00:43:25.580]   Either use like user and like other like, um, you know, um, words like user AI or something, or like H or A, but you should never use human assistant.
[00:43:25.580 --> 00:43:28.580]   Human assistant is like very special, special words.
[00:43:28.580 --> 00:43:32.580]   If that didn't have the XML tags, would it be okay?
[00:43:32.580 --> 00:43:47.580]   Um, it would be okay, but you would like make, you should like have human and then assistant in between and then human assistant and then another human and assistant basically.
[00:43:47.580 --> 00:43:48.580]   Uh, yeah.
[00:43:48.580 --> 00:43:50.580]   Formatting is like human assistant, human assistant.
[00:43:50.580 --> 00:43:53.580]   You should never have like human, human, assistant, assistant or something.
[00:43:53.580 --> 00:43:55.580]   Uh, that's bad.
[00:43:55.580 --> 00:44:00.580]   Um, yeah.
[00:44:00.580 --> 00:44:06.580]   I think we have like more extensive, uh, explanations in the API docs.
[00:44:06.580 --> 00:44:09.580]   If you can look at it.
[00:44:09.580 --> 00:44:10.580]   Yeah.
[00:44:10.580 --> 00:44:11.580]   I got a lot of questions.
[00:44:11.580 --> 00:44:13.580]   Like what's the future of prompt engineering is.
[00:44:13.580 --> 00:44:16.580]   Um, and I think the answers are pretty clear.
[00:44:16.580 --> 00:44:18.580]   Like prompting will stay.
[00:44:18.580 --> 00:44:22.580]   We'll just ask like more complicated nuanced questions or like tasks for the model.
[00:44:22.580 --> 00:44:32.580]   Um, prompting engineering is a, we will like, we're moving towards the world where we'll have like more and more synthetic data generation.
[00:44:32.580 --> 00:44:36.580]   And so I'm pretty optimistic about like more using models.
[00:44:36.580 --> 00:44:39.580]   So like generate like diverse sets of like data sets.
[00:44:39.580 --> 00:44:45.580]   Um, you can also use language models to write like evaluations.
[00:44:45.580 --> 00:44:49.580]   Um, so you use prompting to do that.
[00:44:49.580 --> 00:45:06.580]   Um, reinforcement learning from AI feedback, um, is an alternative to like reinforcement human, from human feedback, which is like a little more scalable, but basically you ask the model to revise its own responses in the process.
[00:45:06.580 --> 00:45:12.580]   So you give the model, you like ask the model to like self reflect or like self revise.
[00:45:12.580 --> 00:45:16.580]   Um, and so you use prompting in that process to do this.
[00:45:16.580 --> 00:45:25.580]   Um, and especially like prompt engineering will become like a standard part of like product development.
[00:45:25.580 --> 00:45:39.580]   I feel like, um, things that we did in cloud products, such as like originating titles, like this things was never like done before, like before like large language models.
[00:45:39.580 --> 00:45:50.580]   And so you can like create delightful mini UX experiences, uh, such as like that using just prompting or something and you can like have personalization.
[00:45:50.580 --> 00:45:58.580]   Um, maybe you can embed all the users conversations and like suggest like new topics for the conversation.
[00:45:58.580 --> 00:46:01.580]   Um, and you can use models to do that.
[00:46:01.580 --> 00:46:08.580]   Um, and the most like interesting thing is like, uh, finding most optimal prompts for specific tasks.
[00:46:08.580 --> 00:46:12.580]   Um, maybe you want to like minimize the number of tokens to get the highest accuracy for the task.
[00:46:12.580 --> 00:46:13.580]   Um, yeah.
[00:46:13.580 --> 00:46:16.580]   Uh, here are some resources.
[00:46:16.580 --> 00:46:23.580]   Uh, we just, uh, launched a cookbook with like certain like demos on research, uh, on retrieval and search.
[00:46:23.580 --> 00:46:28.580]   Um, we have prompt design guide in API, uh, book.
[00:46:28.580 --> 00:46:32.580]   Um, you can also read out the papers that we publish.
[00:46:32.580 --> 00:46:37.580]   Uh, oftentimes we have like appendix with like all the prompting that we do.
[00:46:37.580 --> 00:46:38.580]   Yeah.
[00:46:38.580 --> 00:46:39.580]   Thank you so much.
[00:46:39.580 --> 00:46:41.580]   And, uh, if you have any questions, let me know.
[00:46:41.580 --> 00:46:42.580]   Yeah.
[00:46:42.580 --> 00:46:43.580]   We have five minutes for questions.
[00:46:43.580 --> 00:46:44.580]   Charles is coming up.
[00:46:44.580 --> 00:46:45.580]   We also have water.
[00:46:45.580 --> 00:46:46.580]   Thanks to Sean.
[00:46:46.580 --> 00:46:47.580]   Um, for bringing in some water.
[00:46:47.580 --> 00:46:48.580]   Uh, so, uh, yes.
[00:46:48.580 --> 00:46:48.580]   I do the people, but five minutes for questions.
[00:46:48.580 --> 00:46:48.580]   If you're thinking about .
[00:46:48.580 --> 00:46:48.580]   Yeah.
[00:46:48.580 --> 00:46:49.580]   So have you tried these with different things besides quad, like other, have you had similar
[00:46:49.580 --> 00:46:50.580]   kind of results?
[00:46:50.580 --> 00:46:51.580]   cause you're talking about .
[00:46:51.580 --> 00:46:51.580]   Cause you're talking about .
[00:46:51.580 --> 00:46:52.580]   in this particular.
[00:46:52.580 --> 00:46:52.580]   Maybe like .
[00:46:52.580 --> 00:46:52.580]   Yeah.
[00:46:52.580 --> 00:46:53.580]   Uh, maybe like .
[00:46:53.580 --> 00:46:53.580]   Yeah.
[00:46:53.580 --> 00:46:53.580]   Yeah.
[00:46:53.580 --> 00:46:53.580]   So have you tried these with different things besides quad, like other, have you had similar
[00:46:53.580 --> 00:46:54.580]   kind of results?
[00:46:54.580 --> 00:46:55.580]   cause you're talking about quad in this particular?
[00:46:55.580 --> 00:46:56.580]   like maybe like .
[00:46:56.580 --> 00:46:56.580]   Yeah.
[00:46:56.580 --> 00:47:01.580]   in some water so it gets happy to people. But five minutes for questions for anything about
[00:47:01.580 --> 00:47:04.580]   .
[00:47:04.580 --> 00:47:15.580]   Yep. So have you tried these with different things besides quad, like other, have you had
[00:47:15.580 --> 00:47:20.580]   similar kind of results? Because you're talking about quad in this particular, like maybe like
[00:47:20.580 --> 00:47:28.580]   . Yeah, I think I'm most experienced with
[00:47:28.580 --> 00:47:37.580]   cloud because I use it like every day. Less experience with GPT. I did not look carefully,
[00:47:37.580 --> 00:47:43.580]   to be honest, at their like API docs, but it seems like the strategy is a little bit different.
[00:47:43.580 --> 00:47:49.580]   Yeah. They don't have like formatting as VR, let's say. Yeah.
[00:47:49.580 --> 00:48:18.580]   Yeah, I think that's, that's actually one of the directions to like, um, I don't remember,
[00:48:18.580 --> 00:48:26.580]   how, what was the paper called? Like LLM says like optimizers, I think. Right. Um, but yeah,
[00:48:26.580 --> 00:48:32.580]   I guess like, um, in a way there are like certain tasks that the models are like not good at currently,
[00:48:32.580 --> 00:48:37.580]   like for example, like self-correction, like the models are not really good at like self-correcting
[00:48:37.580 --> 00:48:42.580]   the air like answers. And like, can you find like a problem that was like pretty good at it?
[00:48:42.580 --> 00:48:46.580]   Like, um, other tasks that you want. Yeah.
[00:48:46.580 --> 00:48:50.580]   Yeah.
[00:48:50.580 --> 00:48:50.580]   Yeah.
[00:48:50.580 --> 00:48:56.580]   I'm curious about what techniques your team is using for actually like evaluating the quality of the responses.
[00:48:56.580 --> 00:49:09.580]   Yeah, I think, um, depends on the task. Sometimes we just like have to look manually qualitatively, uh, at outputs.
[00:49:09.580 --> 00:49:23.580]   Um, sometimes you, let's say, um, you want to evaluate, you know, how much does the model refuses and if it refuses in a relevant context or not.
[00:49:23.580 --> 00:49:32.580]   And so, uh, you, uh, you use, you know, generated answers and you categorize infusals in different categories.
[00:49:32.580 --> 00:49:40.580]   And use the model to categorize that. And so you just like see the rate. Um, yeah, I can think of that example.
[00:49:40.580 --> 00:49:48.580]   Yeah. It depends on the task. Some tasks are like, you know, um, for like hallucinations,
[00:49:48.580 --> 00:49:53.580]   you actually have to like look yourself or something. Yeah. Yeah.
[00:49:53.580 --> 00:50:20.580]   Uh, it was open AI.
[00:50:20.580 --> 00:50:45.580]   Yeah. Yeah. Yeah. I think, uh, um, I won't say too much about this, but I actually have not like excessively used function calling from open AI like other models.
[00:50:45.580 --> 00:50:53.580]   Um, yeah. Right. Cool. Yeah.
[00:50:53.580 --> 00:50:54.580]   Yes.
[00:50:54.580 --> 00:50:59.580]   So you mentioned something about, you know, using, uh, LF to generate titles for customers.
[00:50:59.580 --> 00:51:08.580]   How do you actually try to evaluate if the titles are relevant and actually consistent beyond human care?
[00:51:08.580 --> 00:51:14.580]   Um, yeah, I think, uh, one, uh, actually this is an interesting question. Like I worked on the outer generating titles for cloud AI.
[00:51:14.580 --> 00:51:23.580]   And, um, one thing that I asked cloud is to be like an editor, like have an editorial taste.
[00:51:23.580 --> 00:51:33.580]   And what we did is actually we took previous titles and we put it in the context to generate a new title.
[00:51:33.580 --> 00:51:40.580]   And so in a way it's like a little bit more consistent to, um, what the style of the user is. Yeah.
[00:51:40.580 --> 00:51:53.580]   Uh, yeah, I'm not sure if I can share that. Oh yeah.
[00:51:53.580 --> 00:51:57.580]   Uh, yeah, we, we use this in production. I can like show you like cloud AI interface.
[00:51:57.580 --> 00:52:11.580]   And, uh, one thing that we changed recently is that like, if you have like pretty like short, like, um, you know, sometimes you don't have, you don't need like LLM to come up with a title.
[00:52:11.580 --> 00:52:18.580]   You just take, if the prompt is like very short, you just like, um, use the, like the first like words.
[00:52:18.580 --> 00:52:31.580]   Um, but here, yeah, like, I don't know. Um, let's see Hayes introduction, but then like recommend some books.
[00:52:31.580 --> 00:52:39.580]   Um, yeah, I don't know. Uh, yeah.
[00:52:39.580 --> 00:52:44.580]   I have a quick question. Did you cover the difference between quality products then?
[00:52:44.580 --> 00:52:50.580]   Um, no, but I can tell. Uh, so Claude.
[00:52:50.580 --> 00:53:02.580]   So let's look at the, is there some dogs on this?
[00:53:02.580 --> 00:53:10.580]   When did we announce? Uh, or nine, August 9th.
[00:53:11.580 --> 00:53:15.580]   Um, basically Claude 2 is a larger model.
[00:53:15.580 --> 00:53:17.580]   Uh, is a little bit smarter.
[00:53:17.580 --> 00:53:20.580]   Is like smarter than Claude instant.
[00:53:20.580 --> 00:53:23.580]   Claude instant is way cheaper and way faster.
[00:53:23.580 --> 00:53:28.580]   But Claude instant is better than Claude instant one.
[00:53:28.580 --> 00:53:31.580]   Um, in a, like more like reasoning based task.
[00:53:31.580 --> 00:53:33.580]   So it's way better at math.
[00:53:33.580 --> 00:53:35.580]   Uh, it's way better at code.
[00:53:36.580 --> 00:53:45.580]   Um, other benchmarks are like pretty similar, but I think we specifically trained Claude instant to be good at like math and code.
[00:53:45.580 --> 00:53:51.580]   Um, and it's a way better at like red teaming, um, like automated red teaming evaluation.
[00:53:51.580 --> 00:53:54.580]   So it's more robust to like jail breaks.
[00:53:54.580 --> 00:53:59.580]   Um, yeah.
[00:53:59.580 --> 00:54:01.580]   I really liked this model.
[00:54:01.580 --> 00:54:02.580]   You guys should use it.
[00:54:02.580 --> 00:54:04.580]   Yeah.
[00:54:04.580 --> 00:54:05.580]   Yes.
[00:54:05.580 --> 00:54:06.580]   Exposed by ignorance.
[00:54:06.580 --> 00:54:07.580]   Exposed by ignorance.
[00:54:07.580 --> 00:54:11.580]   So when you talk about training, like when you train this client, was that fine tuning?
[00:54:11.580 --> 00:54:13.580]   Or was that something different?
[00:54:13.580 --> 00:54:14.580]   Uh, yeah.
[00:54:14.580 --> 00:54:15.580]   Fine tuning.
[00:54:15.580 --> 00:54:16.580]   Yes.
[00:54:16.580 --> 00:54:17.580]   Last question.
[00:54:17.580 --> 00:54:20.580]   Can you say more about red teaming?
[00:54:20.580 --> 00:54:21.580]   Like what is red teaming?
[00:54:21.580 --> 00:54:23.580]   Yeah.
[00:54:23.580 --> 00:54:25.580]   Red teaming is an interesting concept.
[00:54:25.580 --> 00:54:35.580]   It's, um, basically you, like the models are pretty like, uh, vulnerable to like certain like jail breaks.
[00:54:35.580 --> 00:54:42.580]   Um, so sometimes let's say like a very simple example, like can the model give you instructions how to build a bomb?
[00:54:42.580 --> 00:54:45.580]   And so we, we consider it as a jail break.
[00:54:45.580 --> 00:55:00.580]   And so the goal is to like, uh, in that cases, like the model should like refuse or do not like provide any additional information in case of like, um, unsafe like prompts or something like this.
[00:55:00.580 --> 00:55:03.580]   And so this is like the internal evaluation that we have.
[00:55:03.580 --> 00:55:10.580]   Um, you can read in the model card that we have, we launched in cloud two, how we specifically do that.
[00:55:10.580 --> 00:55:16.580]   Um, but it's basically the amount of like, um, how robust the model is to like those jail breaks.
[00:55:16.580 --> 00:55:17.580]   Yeah.
[00:55:17.580 --> 00:55:18.580]   Cool.
[00:55:18.580 --> 00:55:19.580]   Thank you.
[00:55:19.580 --> 00:55:20.580]   Thank you very much.
[00:55:20.580 --> 00:55:21.580]   Yeah.
[00:55:21.580 --> 00:55:22.580]   Thank you.
[00:55:22.580 --> 00:55:23.580]   Yeah.
[00:55:23.580 --> 00:55:24.580]   Thank you.
[00:55:24.580 --> 00:55:25.580]   Yeah.
[00:55:25.580 --> 00:55:26.580]   Uh, yeah.
[00:55:26.580 --> 00:55:27.580]   Yeah.
[00:55:27.580 --> 00:55:28.580]   Uh, yeah.
[00:55:28.580 --> 00:55:29.580]   Uh, yeah.

