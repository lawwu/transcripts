
[00:00:00.000 --> 00:00:05.000]   Scheduling on a supercomputer typically is by post-it, right?
[00:00:05.000 --> 00:00:09.000]   It's, "Joe, it's your cluster this week, right?
[00:00:09.000 --> 00:00:11.000]   But I need it next week," right?
[00:00:11.000 --> 00:00:14.000]   And it's not, it doesn't work that way at scale anymore, right?
[00:00:14.000 --> 00:00:19.000]   You want to interact with something that is actually
[00:00:19.000 --> 00:00:23.000]   understanding the use of the cluster, optimizing its use
[00:00:23.000 --> 00:00:26.000]   so that the overall output across all of the users
[00:00:26.000 --> 00:00:30.000]   is guaranteed at any given point in time.
[00:00:30.000 --> 00:00:32.000]   You're listening to Gradient Dissent,
[00:00:32.000 --> 00:00:34.000]   a show about machine learning in the real world,
[00:00:34.000 --> 00:00:36.000]   and I'm your host, Lukas Biewald.
[00:00:36.000 --> 00:00:39.000]   This is a conversation I had with Stefan Fabel,
[00:00:39.000 --> 00:00:43.000]   who is a senior director of product management at NVIDIA,
[00:00:43.000 --> 00:00:46.000]   where he works on the Base Command Platform software
[00:00:46.000 --> 00:00:50.000]   that runs on top of NVIDIA's DGX machines,
[00:00:50.000 --> 00:00:53.000]   which are basically the most powerful computers
[00:00:53.000 --> 00:00:57.000]   that you can buy to train your machine learning models on top of.
[00:00:57.000 --> 00:01:01.000]   And it's fun to talk about the challenges that customers face
[00:01:01.000 --> 00:01:05.000]   when they have access to basically unlimited compute power.
[00:01:05.000 --> 00:01:08.000]   This is a super fun conversation, and I hope you enjoy it.
[00:01:08.000 --> 00:01:13.000]   My first question, for those who haven't heard of NVIDIA Base Command,
[00:01:13.000 --> 00:01:15.000]   since you are the senior product manager on it,
[00:01:15.000 --> 00:01:19.000]   can you tell me what Base Command aspires to do?
[00:01:19.000 --> 00:01:24.000]   Yeah, so in a way, think of NVIDIA Base Command
[00:01:24.000 --> 00:01:28.000]   as a one-stop shop for all of your AI development.
[00:01:28.000 --> 00:01:32.000]   So it's a SaaS offering from NVIDIA where you log on directly
[00:01:32.000 --> 00:01:36.000]   or you log on via an integration partner.
[00:01:36.000 --> 00:01:40.000]   And you leverage the capabilities of Base Command
[00:01:40.000 --> 00:01:43.000]   to schedule jobs across a variety of infrastructures.
[00:01:43.000 --> 00:01:45.000]   And you do that in a secure manner.
[00:01:45.000 --> 00:01:48.000]   You gain access to your data and retain access to your data,
[00:01:48.000 --> 00:01:53.000]   and data sovereignty across the infrastructure
[00:01:53.000 --> 00:01:56.000]   that you're scheduling the jobs on.
[00:01:56.000 --> 00:02:00.000]   And then it's really just a matter of optimizing
[00:02:00.000 --> 00:02:02.000]   that job run on NVIDIA infrastructure.
[00:02:02.000 --> 00:02:05.000]   So that's really what Base Command aims to do.
[00:02:05.000 --> 00:02:09.000]   And so these jobs, are they model training jobs exclusively,
[00:02:09.000 --> 00:02:11.000]   or is it broader than that?
[00:02:11.000 --> 00:02:15.000]   Yeah, model training jobs are generally the ones that we focus on,
[00:02:15.000 --> 00:02:18.000]   but we also do model validation, for example.
[00:02:18.000 --> 00:02:21.000]   So you could have single-shot inference runs as well.
[00:02:21.000 --> 00:02:24.000]   And are there other pain points, I guess, of model development
[00:02:24.000 --> 00:02:28.000]   that Base Command aspires to solve or tries to solve?
[00:02:28.000 --> 00:02:35.000]   Yeah, so I think that a lot of the issues that you have
[00:02:35.000 --> 00:02:39.000]   with AI infrastructure, it's really that's where it starts, right?
[00:02:39.000 --> 00:02:43.000]   It's sort of the question is, where do you train your models?
[00:02:43.000 --> 00:02:45.000]   And how do you go about it?
[00:02:45.000 --> 00:02:49.000]   And so most people start in the cloud to train their models, right?
[00:02:49.000 --> 00:02:52.000]   And that's reasonable because just any development effort
[00:02:52.000 --> 00:02:54.000]   would start in the cloud today.
[00:02:54.000 --> 00:02:57.000]   And then at some point, you'll reach a certain amount of scale
[00:02:57.000 --> 00:03:02.000]   where you say, well, it may not deliver the performance I need
[00:03:02.000 --> 00:03:06.000]   or it may not deliver the scale I need,
[00:03:06.000 --> 00:03:10.000]   the economics I'm comfortable with, et cetera.
[00:03:10.000 --> 00:03:13.000]   And so for those high-end runs,
[00:03:13.000 --> 00:03:17.000]   typically you look at infrastructure alternatives, right?
[00:03:17.000 --> 00:03:20.000]   So then the question becomes, okay, I already am used
[00:03:20.000 --> 00:03:24.000]   to this whole SaaS interaction model with my AI development.
[00:03:24.000 --> 00:03:27.000]   So how do I maintain that developer emotion going forward
[00:03:27.000 --> 00:03:31.000]   where I don't have to teach them something new
[00:03:31.000 --> 00:03:33.000]   just because the infrastructure is different?
[00:03:33.000 --> 00:03:37.000]   And so what we have at NVIDIA is this DGX super pod.
[00:03:37.000 --> 00:03:41.000]   And the idea is to say, well, how about we try this
[00:03:41.000 --> 00:03:44.000]   and develop base command as a way to access a super pod
[00:03:44.000 --> 00:03:48.000]   just as a cloud API would behave.
[00:03:48.000 --> 00:03:51.000]   And so a DGX super pod, is that something that I could put
[00:03:51.000 --> 00:03:54.000]   in my own infrastructure or is that something that I could access
[00:03:54.000 --> 00:03:58.000]   in the cloud or both? How does that work?
[00:03:58.000 --> 00:04:01.000]   Yeah, so typically our customers for super pods,
[00:04:01.000 --> 00:04:05.000]   I mean, maybe we should take a step back and understand what it is, right?
[00:04:05.000 --> 00:04:10.000]   So, you know, when you, the easiest way to think about,
[00:04:10.000 --> 00:04:12.000]   or the most straightforward way to think about a DGX super pod
[00:04:12.000 --> 00:04:14.000]   is to think of it as a supercomputer in a box.
[00:04:14.000 --> 00:04:18.000]   And it's a packaged up infrastructure solution from NVIDIA
[00:04:18.000 --> 00:04:21.000]   that you can purchase and it'll be deployed on-premises
[00:04:21.000 --> 00:04:24.000]   for you in your own data center or in a colo facility.
[00:04:24.000 --> 00:04:28.000]   And actually we found that a colo facility is the most likely place
[00:04:28.000 --> 00:04:33.000]   for you to put that because it is a pretty intensive investment.
[00:04:33.000 --> 00:04:37.000]   Number one, not just in terms of the, just the number of DGXs
[00:04:37.000 --> 00:04:40.000]   that are involved, for example, but also of course,
[00:04:40.000 --> 00:04:43.000]   in the terms of the power draw and cooling and just the requirements
[00:04:43.000 --> 00:04:49.000]   that you need to bring to even run this beast essentially.
[00:04:49.000 --> 00:04:54.000]   So, yeah, so I mean, that's really what then dictates
[00:04:54.000 --> 00:04:56.000]   kind of where this thing usually is.
[00:04:56.000 --> 00:05:00.000]   So what we did is we put it in a colo facility
[00:05:00.000 --> 00:05:05.000]   and make it available right now in kind of directed availability fashion.
[00:05:05.000 --> 00:05:08.000]   So we have a couple of golden tickets for some customers
[00:05:08.000 --> 00:05:10.000]   who want to be on this thing.
[00:05:10.000 --> 00:05:14.000]   And then they get to select the size of the slice they want
[00:05:14.000 --> 00:05:17.000]   and access that through base command.
[00:05:17.000 --> 00:05:22.000]   I see. So when you use base command, you're using DGX,
[00:05:22.000 --> 00:05:26.000]   but it's in NVIDIA's cloud and you get kind of a slice of it.
[00:05:26.000 --> 00:05:28.000]   Is that right?
[00:05:28.000 --> 00:05:32.000]   Yeah, that's right. Although I know we call it NVIDIA GPU cloud,
[00:05:32.000 --> 00:05:36.000]   but really think of the whole base command proposition today
[00:05:36.000 --> 00:05:40.000]   as a SaaS portal that you access that is currently coupled
[00:05:40.000 --> 00:05:42.000]   to more like a rental program.
[00:05:42.000 --> 00:05:47.000]   So it's less like cloud bursty elastic.
[00:05:47.000 --> 00:05:51.000]   Think of it more like, okay, I have three DGX A100s today
[00:05:51.000 --> 00:05:55.000]   and then maybe in the next couple of months I know I need three more.
[00:05:55.000 --> 00:06:00.000]   So I'll call NVIDIA and say, "Hey, I need three more for the next month."
[00:06:00.000 --> 00:06:03.000]   And then that's kind of how that works.
[00:06:03.000 --> 00:06:06.000]   So maybe let's start with the DGX box.
[00:06:06.000 --> 00:06:09.000]   I guess what would a standard box look like?
[00:06:09.000 --> 00:06:12.000]   What's its power draw? How big is it?
[00:06:12.000 --> 00:06:15.000]   How much does it cost? Can you answer these questions?
[00:06:15.000 --> 00:06:17.000]   Just the order of magnitude.
[00:06:17.000 --> 00:06:23.000]   Yeah. So you're looking at about $300,000 for a single DGX A100.
[00:06:23.000 --> 00:06:29.000]   We have eight GPUs and 640 gigabytes of memory to come along with that.
[00:06:29.000 --> 00:06:34.000]   Those are the A100 GPUs, so the latest and greatest that we have.
[00:06:34.000 --> 00:06:40.000]   And you're going to look at about 13 kilowatts per rack of standard deployment.
[00:06:40.000 --> 00:06:42.000]   13 kilowatts?
[00:06:42.000 --> 00:06:44.000]   Yeah. And you're going to have-
[00:06:44.000 --> 00:06:46.000]   Constant or just when it's training?
[00:06:46.000 --> 00:06:53.000]   No, no. When you fire these things up, these puppies, they heat up quite a lot.
[00:06:53.000 --> 00:06:56.000]   So yeah, I mean, they're pretty powerful.
[00:06:56.000 --> 00:07:01.000]   And so the DGX SuperPOD consists of a minimum 20 of those.
[00:07:01.000 --> 00:07:06.000]   And if you think about that, that's what we call one scale unit.
[00:07:06.000 --> 00:07:11.000]   And we have customers that build 140 of those.
[00:07:11.000 --> 00:07:15.000]   Wow. And what kinds of things do they do with that?
[00:07:15.000 --> 00:07:20.000]   Well, just all the largest multi-node jobs that you could possibly imagine, right?
[00:07:20.000 --> 00:07:31.000]   Starting from climate change analysis, large, huge data sets that need to be worked on there.
[00:07:31.000 --> 00:07:37.000]   NLP is a big draw for some of these customers, right?
[00:07:37.000 --> 00:07:39.000]   Just natural language processing.
[00:07:39.000 --> 00:07:45.000]   And the analytics that comes with those models is pretty intensive, data intensive and transfer intensive.
[00:07:45.000 --> 00:07:51.000]   And I think I should mention that we keep talking about the DGXs, right?
[00:07:51.000 --> 00:07:54.000]   And of course, we're very proud of them and all of that.
[00:07:54.000 --> 00:07:59.000]   But we also acquired a company called Mellanox a year ago.
[00:07:59.000 --> 00:08:06.000]   And so, of course, the networking plays a huge role in the infrastructure layout of such a SuperPOD.
[00:08:06.000 --> 00:08:12.000]   So if you have multi-rail InfiniBand connections between all of those boxes and the storage,
[00:08:12.000 --> 00:08:15.000]   which typically uses a parallel file system in a SuperPOD,
[00:08:15.000 --> 00:08:22.000]   then what you'll get is essentially extreme performance even for multi-node jobs.
[00:08:22.000 --> 00:08:28.000]   So any job that even has to go above and beyond multiple GPUs,
[00:08:28.000 --> 00:08:32.000]   a DGX SuperPOD architecture will get you there,
[00:08:32.000 --> 00:08:41.000]   essentially at, I would say, probably one of the best speed performance characteristics that you could possibly have.
[00:08:41.000 --> 00:08:44.000]   I mean, the SuperPOD scored number five on the top 500.
[00:08:44.000 --> 00:08:46.000]   So it's nothing to sneeze at.
[00:08:46.000 --> 00:08:53.000]   Yeah. I guess, how does the experience of training on that compare to something that listeners would be more familiar with,
[00:08:53.000 --> 00:08:57.000]   like a 2080 or 3080, which feels pretty fast already?
[00:08:57.000 --> 00:09:07.000]   How much faster is this? And do you need to use a special version of TensorFlow or PyTorch or something like this to even take advantage of the parallelism?
[00:09:07.000 --> 00:09:12.000]   So I'd have to check exactly how to quantify an A30 to an A100.
[00:09:12.000 --> 00:09:17.000]   But think of it as this, right?
[00:09:17.000 --> 00:09:23.000]   So any other GPU that you might want to use for training in a traditional server,
[00:09:23.000 --> 00:09:27.000]   think of it as a subset of the capabilities of an A100.
[00:09:27.000 --> 00:09:39.000]   And so if you use, for example, our MIG capability, you can really slice that GPU down to a T4 type performance profile and say,
[00:09:39.000 --> 00:09:46.000]   "Well, I'm testing stuff out on a really small performance profile without having to occupy the entire GPU."
[00:09:46.000 --> 00:09:52.000]   And then once you have the same approach from a software perspective,
[00:09:52.000 --> 00:09:57.000]   if you do your sweeps, then you do essentially the same thing.
[00:09:57.000 --> 00:10:00.000]   You could do those on MIG instances, right?
[00:10:00.000 --> 00:10:05.000]   And then thereby, you don't need that many DGXs when you do it.
[00:10:05.000 --> 00:10:17.000]   But I guess I should say that that's the beauty of CUDA, that if you write this once, it'll run on an A30, it'll run on an A100, it'll run on a T4.
[00:10:17.000 --> 00:10:28.000]   And in fact, we provide a whole lot of base images that are free for people to use and to start with, and then lift the tide for everybody.
[00:10:28.000 --> 00:10:33.000]   So these are pre-optimized container images that people can build on.
[00:10:33.000 --> 00:10:41.000]   I would think there'd be a lot of networking issues and parallelization issues that would come up maybe uniquely at this scale.
[00:10:41.000 --> 00:10:44.000]   Is that something that NVIDIA tries to help with?
[00:10:44.000 --> 00:10:45.000]   Does CUDA actually help with that?
[00:10:45.000 --> 00:10:50.000]   I sort of think of CUDA as like compiling something to run on a single GPU.
[00:10:50.000 --> 00:10:52.000]   Yeah, absolutely.
[00:10:52.000 --> 00:11:04.000]   So if you think of CUDA as sort of a very horizontal platform piece in the software stack of your AI training stack,
[00:11:04.000 --> 00:11:13.000]   then components like Nickel, for example, provide you with pretty optimized communication paths for multi-GPU jobs, for example.
[00:11:13.000 --> 00:11:16.000]   But they also span multi-nodes, right?
[00:11:16.000 --> 00:11:21.000]   And this starts from selecting the right NIC to exit the signal, right?
[00:11:21.000 --> 00:11:25.000]   Because that means you're going to the right port and the top of the rack switch.
[00:11:25.000 --> 00:11:33.000]   And that means you minimize the latency that your signal takes from point A to point B inside your data center.
[00:11:33.000 --> 00:11:40.000]   So when you look at CUDA and especially at components like Nickel and Magnum I/O as a whole,
[00:11:40.000 --> 00:11:47.000]   which is sort of our portfolio of communication libraries and storage acceleration libraries,
[00:11:47.000 --> 00:11:54.000]   it starts from the integration of the hardware and the understanding of the actual chip itself, right?
[00:11:54.000 --> 00:11:57.000]   And then it builds outward from there.
[00:11:57.000 --> 00:12:07.000]   And the big shift that NVIDIA that we're looking at sort of accelerating with use of base command is this understanding that,
[00:12:07.000 --> 00:12:10.000]   "Hey, NVIDIA is now thinking about the entire data center.
[00:12:10.000 --> 00:12:16.000]   It's not just about, I got the newest GPU and now my game runs faster, right?"
[00:12:16.000 --> 00:12:20.000]   Certainly that's a focus area of us as well, right?
[00:12:20.000 --> 00:12:29.000]   But really, if you take the entire stack and work inside out, essentially,
[00:12:29.000 --> 00:12:35.000]   then the value proposition just multiplies the further out you go, right?
[00:12:35.000 --> 00:12:43.000]   And so with base command, this is sort of the last step in this whole journey to turn it into kind of a hybrid proposition.
[00:12:43.000 --> 00:12:55.000]   So anyway, I know it's very high level right now and abstract, but it's a super interesting problem to solve
[00:12:55.000 --> 00:13:04.000]   because if you think about how data center infrastructure evolved over the last, let's say, 10 years or so, right?
[00:13:04.000 --> 00:13:09.000]   Then it was about introducing more homogeneity into the actual layout of the data center.
[00:13:09.000 --> 00:13:16.000]   So, certain type of server, certain type of CPU, certain type of Tupper rack switch, and then a certain layout, right?
[00:13:16.000 --> 00:13:23.000]   So you have all these non-blocking fabric reference architectures that are out there and et cetera, et cetera, right?
[00:13:23.000 --> 00:13:30.000]   And ultimately, now that everything is homogeneous, you can now make it addressable using an API
[00:13:30.000 --> 00:13:36.000]   because everything is at least intended to behave in this very standard and predictable way.
[00:13:36.000 --> 00:13:38.000]   And so we've worked our way up there.
[00:13:38.000 --> 00:13:41.000]   This has never been the case for something like a supercomputer.
[00:13:41.000 --> 00:13:50.000]   A supercomputer was a two-year research project with a lot of finagling and parameters here
[00:13:50.000 --> 00:13:56.000]   and then set this thing to a magic value and that thing to a magic value and then run it on five minutes after midnight,
[00:13:56.000 --> 00:13:59.000]   but not on Tuesdays, and then you get the performance, right?
[00:13:59.000 --> 00:14:10.000]   And so this whole contribution that we're really making here is that we're raising that bar to a predictable performance profile
[00:14:10.000 --> 00:14:17.000]   that is repeatable, not just inside an NVIDIA data center where we know five minutes after midnight and so on, right?
[00:14:17.000 --> 00:14:24.000]   But also in your data center or in an actual random data center, provided you can afford the cooling and power, of course.
[00:14:24.000 --> 00:14:27.000]   But then once we got that out of the way, we're pretty good, right?
[00:14:27.000 --> 00:14:42.000]   So that's a real shift forward towards enabling enterprises, real bonafide true blue chip companies to actually adopt AI at a larger scale.
[00:14:42.000 --> 00:14:44.000]   It's interesting.
[00:14:44.000 --> 00:14:53.000]   One thing I was thinking of as you were talking is most of the customers that we work with, we don't always know,
[00:14:53.000 --> 00:15:04.000]   but I think what we typically see with our customers that are training a lot of machine learning models is they use a lot of NVIDIA hardware,
[00:15:04.000 --> 00:15:08.000]   but it's less powerful hardware than the DGX.
[00:15:08.000 --> 00:15:19.000]   It might be like P100 or basically whatever's available to them through Amazon or Azure or Google Cloud.
[00:15:19.000 --> 00:15:23.000]   And I think they do that for convenience.
[00:15:23.000 --> 00:15:28.000]   I think people come out of school knowing how to train on those types of infrastructure.
[00:15:28.000 --> 00:15:30.000]   And then their compute costs do get high enough.
[00:15:30.000 --> 00:15:35.000]   I mean, we do see compute costs certainly well into the seven, eight figures.
[00:15:35.000 --> 00:15:40.000]   And so do you think that they're making a mistake by doing it that way?
[00:15:40.000 --> 00:15:48.000]   Should they be buying custom DGX hardware and putting that in a collab?
[00:15:48.000 --> 00:15:52.000]   Would they actually save money or make their teams more productive if they did it that way?
[00:15:52.000 --> 00:15:54.000]   Oh, God, no.
[00:15:54.000 --> 00:15:59.000]   So just to be really clear, just like I said, Base Command is not a cloud.
[00:15:59.000 --> 00:16:06.000]   We're not intending to go out there and say, "Go here instead of, let's say, Amazon or something like that."
[00:16:06.000 --> 00:16:08.000]   That's not what we're saying.
[00:16:08.000 --> 00:16:12.000]   I mean, first of all, you can get A100 instances in all the major public clouds as well.
[00:16:12.000 --> 00:16:24.000]   So you could have access to those instances in just the same way that you're used to consuming the P100s or V100s or anything like that.
[00:16:24.000 --> 00:16:31.000]   So whether it's Pascal or Volta or Ampere architecture, all of it is available in the public cloud.
[00:16:31.000 --> 00:16:36.000]   And like I said in the beginning, it's just a perfectly acceptable way to start.
[00:16:36.000 --> 00:16:44.000]   In fact, it's the recommended path to start in the cloud because it requires the least upfront investment, I mean, zero.
[00:16:44.000 --> 00:16:49.000]   And you get to see how far you can push something, an idea.
[00:16:49.000 --> 00:16:55.000]   And then once you arrived at a certain point, I think then it's a question of economics.
[00:16:55.000 --> 00:16:59.000]   And then just everything will start falling into place.
[00:16:59.000 --> 00:17:05.000]   What we found is that enterprises typically arrive at a base load of GPUs.
[00:17:05.000 --> 00:17:13.000]   So in other words, at any given moment in time, for whatever reason, there is a certain number of GPUs working.
[00:17:13.000 --> 00:17:26.000]   And once you identify that, "Hey, every day I keep at least 500 GPUs busy," well, then typically the economics are better if you purchase.
[00:17:26.000 --> 00:17:31.000]   Typically, a kind of a CapEx approach works out better.
[00:17:31.000 --> 00:17:34.000]   It's not always the case, but typically that might be the case.
[00:17:34.000 --> 00:17:40.000]   And so to meet that need in the market is where we sort of come in.
[00:17:40.000 --> 00:17:46.000]   So what Base Command right now offers is this, it's not the all the way purchase it.
[00:17:46.000 --> 00:17:52.000]   You don't have to now have that big CapEx investment up front, but it is something in between.
[00:17:52.000 --> 00:18:01.000]   You do get to rent something. It's not entirely cloud, but you're moving from the Uber model to the national car rental type model.
[00:18:01.000 --> 00:18:05.000]   And then once you're done renting, then maybe you want to buy a car.
[00:18:05.000 --> 00:18:10.000]   But the point is that there is room here on that spectrum.
[00:18:10.000 --> 00:18:13.000]   And so currently we're right smack in the middle of that one.
[00:18:13.000 --> 00:18:19.000]   So that's typically what we say to customers.
[00:18:19.000 --> 00:18:24.000]   Actually, yesterday somebody said, "Well, how do you support bursting and how elastic are you?"
[00:18:24.000 --> 00:18:26.000]   And I said, "That's not the point here."
[00:18:26.000 --> 00:18:35.000]   You want to be in cloud when you want to be elastic and bursty, but typically that baseload is done better in different ways.
[00:18:35.000 --> 00:18:38.000]   So now what breaks if I don't use Base Command?
[00:18:38.000 --> 00:18:49.000]   If I just purchased one of these machines and I'm just kind of shelling into the machine and kicking off my jobs the way I'm typically used to or running something in a notebook,
[00:18:49.000 --> 00:18:54.000]   what starts to break where you know that you need something more sophisticated?
[00:18:54.000 --> 00:18:59.000]   So, well, on the face of it, nothing really breaks.
[00:18:59.000 --> 00:19:02.000]   It just takes a lot of expertise to put these things together.
[00:19:02.000 --> 00:19:10.000]   So if you buy a single box, then there's probably very little value in adding that to a SaaS platform per se.
[00:19:10.000 --> 00:19:21.000]   But as soon as you start thinking about a cluster of machines, and like I said, more and more of our enterprise customers are actually thinking about deploying many of those, not just a single machine.
[00:19:21.000 --> 00:19:34.000]   And then as soon as that comes into play, then you're faced with all the traditional skill challenges in your enterprise that you'd be used to from just rolling out private cloud infrastructure.
[00:19:34.000 --> 00:19:37.000]   It's the same exact journey. It's the same exact challenge.
[00:19:37.000 --> 00:19:46.000]   You need to have somebody who understands these machines, somebody who understands networking, somebody who understands storage, Kubernetes, and so on and so forth.
[00:19:46.000 --> 00:19:57.000]   And as soon as you build up the skill profile that you need to actually run this infrastructure at scale and at capacity, then you're good to go.
[00:19:57.000 --> 00:19:59.000]   You can build your own solution.
[00:19:59.000 --> 00:20:05.000]   But typically, what you'd be lacking are things that then help you make the most of it.
[00:20:05.000 --> 00:20:15.000]   So all the kinds of efficiency gains that you'd have by just having visibility into the use of the GPU.
[00:20:15.000 --> 00:20:21.000]   So all the telemetry and the aggregates by job and by user and by team.
[00:20:21.000 --> 00:20:28.000]   So this entire concept of chargeback, et cetera, is a whole other hurdle that you then have to climb.
[00:20:28.000 --> 00:20:37.000]   And so what we're looking at is people who want to build a cluster, typically they want to do that because they want to share that cluster.
[00:20:37.000 --> 00:20:42.000]   Like I said, it's a pretty big beast.
[00:20:42.000 --> 00:20:49.000]   So if you build a big cluster, might as well, because you want to be more efficient and you want to make the most of it.
[00:20:49.000 --> 00:20:52.000]   And so now you need to have a broker.
[00:20:52.000 --> 00:20:55.000]   Who brokers access to the system?
[00:20:55.000 --> 00:21:05.000]   Supercomputers, as ridiculous as it sounds, they work scheduling on a supercomputer typically is by post-it.
[00:21:05.000 --> 00:21:10.000]   It's, "Joe, it's your cluster this week, but I need it next week."
[00:21:10.000 --> 00:21:14.000]   And it doesn't work that way at scale anymore.
[00:21:14.000 --> 00:21:30.000]   You want to interact with something that is actually understanding the use of the cluster, optimizing its use so that the overall output across all of the users is guaranteed at any given point in time.
[00:21:30.000 --> 00:21:44.000]   I had the sense that many years ago, decades ago, when I was a kid or maybe even before that, supercomputers felt like this really important resource that we use for lots of applications.
[00:21:44.000 --> 00:21:49.000]   And then maybe in the '90s or the aughts, they became less popular.
[00:21:49.000 --> 00:21:54.000]   People started moving their compute jobs to distributed commodity hardware.
[00:21:54.000 --> 00:21:57.000]   And maybe they're making a comeback again.
[00:21:57.000 --> 00:22:00.000]   Do you think that's an accurate impression?
[00:22:00.000 --> 00:22:13.000]   And do you have a sense of what the forces are that makes supercomputers more or less interesting compared to just making a huge stack of chips that you could buy in the store?
[00:22:13.000 --> 00:22:16.000]   Yeah, it is interesting, right?
[00:22:16.000 --> 00:22:23.000]   Because if you think about it, we've actually oscillated back and forth between this concept a little bit for years.
[00:22:23.000 --> 00:22:25.000]   I mean, you're exactly right.
[00:22:25.000 --> 00:22:38.000]   You had the first wave of standardization was, let's just use 19-inch rack units and start from there and then see maybe that's a little bit better.
[00:22:38.000 --> 00:22:47.000]   And then the same thing happened when we decided to use containers as an artifact to deliver software from point A to point B.
[00:22:47.000 --> 00:22:53.000]   And just standardization and form factor really is what drove us there.
[00:22:53.000 --> 00:22:59.000]   And certainly there's value in that, right?
[00:22:59.000 --> 00:23:15.000]   The interesting moment happens when all of that together becomes-- when the complexity of running all of that together and lining it all just up, right?
[00:23:15.000 --> 00:23:20.000]   Because in the beginning, you had one IBM S390, right?
[00:23:20.000 --> 00:23:23.000]   And that's the one thing you have to line up, right?
[00:23:23.000 --> 00:23:28.000]   And now you have 200 OEM servers across X racks.
[00:23:28.000 --> 00:23:31.000]   And that's a lot of ducks to line up, right?
[00:23:31.000 --> 00:23:38.000]   So the complexity and management of independent systems that you're sort of adding together, that sounds good on paper.
[00:23:38.000 --> 00:23:45.000]   But at some point, you're sort of crossing that complexity line where it's just more complex to even manage the hardware.
[00:23:45.000 --> 00:23:48.000]   And this is not just from an effort perspective.
[00:23:48.000 --> 00:23:50.000]   This is also from a CPU load perspective.
[00:23:50.000 --> 00:24:02.000]   If more than 50% of your cores go towards just staying in sync with everybody else, well, how much are you really getting out of each individual component that makes up this cluster, right?
[00:24:02.000 --> 00:24:06.000]   So now, of course, you're saying, "Well, how do I disrupt it?"
[00:24:06.000 --> 00:24:14.000]   Well, you disrupt it by making assumption about how this infrastructure actually looks like rather than saying, "Well, you're dropping the ocean.
[00:24:14.000 --> 00:24:17.000]   You first have to figure out where you're even at."
[00:24:17.000 --> 00:24:32.000]   And so if you eliminate that complexity, then fundamentally, you can go straight into focusing more on kind of a data plane type of focus rather than figuring out how the control plane looks like and how busy that one is, right?
[00:24:32.000 --> 00:24:34.000]   So it's got a little bit of that.
[00:24:34.000 --> 00:24:48.000]   And I think the DGX represents an optimization that shows rather than purchasing eight separate servers that have potentially similar GPUs in them, right?
[00:24:48.000 --> 00:25:04.000]   Here's a way that not only has those eight GPUs in them, but it also is interconnected in a way that just makes optimal assumptions about what's going on between those two GPUs and what could possibly run on them.
[00:25:04.000 --> 00:25:12.000]   And that combined with a software stack that's optimized for this layout just brings the value home, right?
[00:25:12.000 --> 00:25:14.000]   So that's really where we're coming from.
[00:25:14.000 --> 00:25:15.000]   It's interesting.
[00:25:15.000 --> 00:25:22.000]   When I started doing machine learning, the hardware was pretty abstracted away.
[00:25:22.000 --> 00:25:25.000]   We would kind of compete for computing resources.
[00:25:25.000 --> 00:25:35.000]   And so I got a little bit handy with Unix and Nicing processes and just sort of coordinating with other people in grad school.
[00:25:35.000 --> 00:25:39.000]   But I really had no sense of the underlying hardware.
[00:25:39.000 --> 00:25:44.000]   I don't even think I took any classes on networking or chip architecture.
[00:25:44.000 --> 00:25:46.000]   And now I really regret it.
[00:25:46.000 --> 00:25:48.000]   I feel like I'm actually learning more and more about it.
[00:25:48.000 --> 00:25:53.000]   And the hardware is becoming less and less abstracted away every year.
[00:25:53.000 --> 00:25:57.000]   And I think NVIDIA has a real role to play there.
[00:25:57.000 --> 00:26:08.000]   Do you think that over time, we'll go back to a more abstracted away hardware model and we'll kind of figure out the right APIs to this?
[00:26:08.000 --> 00:26:16.000]   Or do you think that we're going to make more and more specialized hardware for the different things that people are likely going to want to do?
[00:26:16.000 --> 00:26:24.000]   And a core skill of an ML practitioner is going to need to be understanding how the underlying hardware works.
[00:26:24.000 --> 00:26:26.000]   Yeah.
[00:26:26.000 --> 00:26:30.000]   I think what you said there is...
[00:26:30.000 --> 00:26:38.000]   I'm reminded of like 10 years ago, we used to say, well, if you're a web front-end developer and you don't know TCP/IP, you're not really a web front-end developer.
[00:26:38.000 --> 00:26:42.000]   But most web front-end developers will never think about TCP/IP.
[00:26:42.000 --> 00:26:45.000]   And I think this is very true here, too.
[00:26:45.000 --> 00:26:47.000]   You have an ML Ops practitioner.
[00:26:47.000 --> 00:26:54.000]   And today, well, yeah, you get to think about your models and tensors, hyperparameter searches and all of that kind of stuff.
[00:26:54.000 --> 00:26:56.000]   And yes, that's important.
[00:26:56.000 --> 00:26:58.000]   Well, not important. It's crucial.
[00:26:58.000 --> 00:27:00.000]   You have to know what you can do, what you couldn't do, your work.
[00:27:00.000 --> 00:27:07.000]   But increasingly, you also have to know where you're actually running in order to get the performance that you need.
[00:27:07.000 --> 00:27:17.000]   So today, it's a real competitive advantage for the companies out there to increase the training speed.
[00:27:17.000 --> 00:27:21.000]   Not just being able to...
[00:27:21.000 --> 00:27:24.000]   I mean, obviously, what we're solving is just getting started.
[00:27:24.000 --> 00:27:26.000]   I mean, we take all that pain away.
[00:27:26.000 --> 00:27:28.000]   You go to the basement, off you go.
[00:27:28.000 --> 00:27:36.000]   But increasingly, it's a true competitive advantage not to be in the cloud, but to be training faster than anybody else.
[00:27:36.000 --> 00:27:45.000]   Like 2012, 2013, if you weren't working on a cloud initiative as a CIO, that was a problem.
[00:27:45.000 --> 00:27:53.000]   Now, increasingly, if you're not focusing on how to accelerate AI training, now you're putting your company at a disadvantage.
[00:27:53.000 --> 00:28:05.000]   That means that the necessity for each individual practitioner who interacts with the hardware to actually understand what they run on and how to optimize for this is going to increase.
[00:28:05.000 --> 00:28:17.000]   Now, having said that, though, part of our job at NVIDIA, I think, is to make optimal choices on behalf of the practitioner out of the gate.
[00:28:17.000 --> 00:28:27.000]   So rather than requiring people to really understand, let's say, the clock rates of each individual bus or something like that, we'll abstract it away.
[00:28:27.000 --> 00:28:32.000]   And people will argue that CUDA is already still pretty low level.
[00:28:32.000 --> 00:28:38.000]   But we're actually abstracting a whole lot to even get to that point.
[00:28:38.000 --> 00:28:47.000]   So I would say, while that's true, we're trying to shield the practitioner as much as possible.
[00:28:47.000 --> 00:28:56.000]   And we have a leg up because we can work with both the knowledge of how the GPU looks like and, most importantly, how the next GPU will look like.
[00:28:56.000 --> 00:29:09.000]   But also how to expose that optimally at the application layer and then interact with the MLOps providers in a meaningful way that just is optimal throughout.
[00:29:09.000 --> 00:29:33.000]   Have there been any kind of cultural changes needed to build a SaaS customer facing product like Base Command at a company that kind of comes up through making really great semiconductors and kind of very, well, I would call it, I would call it CUDA low level from my vantage point.
[00:29:33.000 --> 00:29:39.000]   And obviously, it's an amazing piece of software, but it's very, very low level software.
[00:29:39.000 --> 00:29:49.000]   Has NVIDIA needed to make adjustments in the product development process to make Base Command work for customers?
[00:29:49.000 --> 00:29:55.000]   Yeah, it's interesting because Base Command is actually not a new product.
[00:29:55.000 --> 00:30:00.000]   We've been using this thing internally for over five years.
[00:30:00.000 --> 00:30:11.000]   And it was kind of a natural situation for us because five years ago, we launched the first DGX.
[00:30:11.000 --> 00:30:28.000]   And then, of course, if you launch something like the DGX, and you say that's the best thing you could possibly purchase for the purposes of training, and you have 2,600 AI researchers in-house,
[00:30:28.000 --> 00:30:38.000]   then you can imagine the next sort of the obvious next question is like, okay, well, how do we use this thing to accelerate our own AI research?
[00:30:38.000 --> 00:30:50.000]   And so this need for creating large scale AI infrastructure on the basis of the DGXs was born out of this situation.
[00:30:50.000 --> 00:30:53.000]   And so with that came all these issues.
[00:30:53.000 --> 00:31:03.000]   And as we solved them, we just kept adding to this portal or to this, it's not just as much as a portal, right?
[00:31:03.000 --> 00:31:12.000]   I mean, it's the entire stack, it's the infrastructure provisioning, and then the exposure, the middleware, the scheduler, the entire thing, right?
[00:31:12.000 --> 00:31:18.000]   So it became more and more obvious to us what should be done.
[00:31:18.000 --> 00:31:24.000]   And so these 2,600 researchers that I just mentioned, I mean, bless their heart, right?
[00:31:24.000 --> 00:31:35.000]   They really had to go through a lot of iteration with us and be very patient with us until we got it to the point where they would, well, let's say not complain as much.
[00:31:35.000 --> 00:31:39.000]   But the point is that we really tried to get it right.
[00:31:39.000 --> 00:31:48.000]   And we acted in a very transparent manner with a pretty large community of AI researchers and developers.
[00:31:48.000 --> 00:31:51.000]   And they told us what they needed and what they wanted and what their pain points were.
[00:31:51.000 --> 00:32:04.000]   And so really this going to market now with Base Command as an externally facing product was simply turning that to the outside.
[00:32:04.000 --> 00:32:11.000]   Have there been any surprises in taking it to market?
[00:32:11.000 --> 00:32:24.000]   Because I know that sometimes when companies have an internal tool, like I think the TensorFlow team has talked about this, that it's kind of made for, especially for a really, really advanced large team.
[00:32:24.000 --> 00:32:34.000]   And then you want to take it to someone who's newer or a smaller team, they kind of have new needs that are a little bit surprising to people that have been doing this for a long time.
[00:32:34.000 --> 00:32:38.000]   Have you encountered anything like that as you bring it to market?
[00:32:38.000 --> 00:32:42.000]   Yeah, it's funny you ask.
[00:32:42.000 --> 00:32:47.000]   So we encounter this in just in many different aspects.
[00:32:47.000 --> 00:32:54.000]   So one example is that most customers, so like I said, I mean, we make this available.
[00:32:54.000 --> 00:33:02.000]   It's the sort of the internal example that we use is, oh, you get to drive the Lamborghini for a while.
[00:33:02.000 --> 00:33:05.000]   And so the idea is this is a short term rental.
[00:33:05.000 --> 00:33:07.000]   I mean, how long are you renting a Lamborghini?
[00:33:07.000 --> 00:33:09.000]   Maybe a day or two, right.
[00:33:09.000 --> 00:33:11.000]   Or a weekend.
[00:33:11.000 --> 00:33:19.000]   And so here we're saying, well, you know, short term rental, they're probably going to rent this for three months or, you know, something like that.
[00:33:19.000 --> 00:33:25.000]   Well, it turns out most customers want to rent this for two years, three years.
[00:33:25.000 --> 00:33:37.000]   And so what surprised us was that there's a real need for, well, not only for a long term rental, but especially the immediacy of access to this.
[00:33:37.000 --> 00:33:45.000]   I think we had underestimated a little bit how desperate the market was to get started right away.
[00:33:45.000 --> 00:33:48.000]   I mean, we knew that people would want to get started.
[00:33:48.000 --> 00:33:52.000]   But we always figured, OK, well, you know, the cloud is there to get started right away.
[00:33:52.000 --> 00:33:55.000]   I mean, just sign up and swipe your credit card and off you go.
[00:33:55.000 --> 00:34:04.000]   But no, but it's also the need for large scale training and just the immediacy of that need that personally was a surprise to me.
[00:34:04.000 --> 00:34:06.000]   I hadn't expected that.
[00:34:06.000 --> 00:34:10.000]   I thought that would be much more of a slow ramp than it was.
[00:34:10.000 --> 00:34:12.000]   So, yeah.
[00:34:12.000 --> 00:34:18.000]   So anyway, I mean, I thought I was going to be in different sales conversations than I actually found myself in.
[00:34:18.000 --> 00:34:21.000]   And so that was a surprise.
[00:34:21.000 --> 00:34:28.000]   Surprises are just, you know, understanding just how much people still have to go.
[00:34:28.000 --> 00:34:39.000]   You know, they typically, we encounter folks who say, well, you know, I really, my way to scale and accelerate my training is just to pick a larger GPU.
[00:34:39.000 --> 00:34:46.000]   And, you know, and there's a big, big portion of the market that certainly has been operating that way.
[00:34:46.000 --> 00:34:56.000]   Right. But really helping them see that, you know, sometimes it's not the, you know, sort of the scale up model, but the scale out model that might be appropriate as kind of the next step.
[00:34:56.000 --> 00:35:09.000]   I think that was, it wasn't exactly surprising, but it was interesting to see just how widespread that scale up thinking was rather than the scale out thinking.
[00:35:09.000 --> 00:35:11.000]   Can you say more about scale up versus scale out?
[00:35:11.000 --> 00:35:12.000]   What do you mean?
[00:35:12.000 --> 00:35:13.000]   What's the difference?
[00:35:13.000 --> 00:35:14.000]   So, yeah.
[00:35:14.000 --> 00:35:23.000]   I mean, if you think about cloud infrastructure, then a scale up approach would be, you know, you started with a medium instance and you go to an X large or something like that.
[00:35:23.000 --> 00:35:34.000]   So you just choose more powerful resources to power the same exact hardware, but you don't really think about adding a second server, for example, and now spread the load across multiple instances.
[00:35:34.000 --> 00:35:37.000]   So here it would be something similar. Right.
[00:35:37.000 --> 00:35:48.000]   So, like I said, if you always think about saying, well, OK, I choose to run this on a Volta based system. Right.
[00:35:48.000 --> 00:35:55.000]   And now I have a Volta based GPU. And now my way to make this faster is to go to an Ampere based architecture GPU. Right.
[00:35:55.000 --> 00:36:00.000]   So that would be scaling up. And certainly, you know, that's something that you want to do.
[00:36:00.000 --> 00:36:15.000]   But at some point you're saying your pace of and your need for accelerated training actually exceeds the sort of the cadence at which we can provide you the next fastest GPU. Right.
[00:36:15.000 --> 00:36:25.000]   So if you need to scale faster than that, and if that curve exceeds the other, then you're essentially in a situation where you have to say, well, how about I take a second A100. Right.
[00:36:25.000 --> 00:36:30.000]   And then I have a multi GPU scenario and I just deal with that. Right. And so on and so forth.
[00:36:30.000 --> 00:36:41.000]   And so then the actual conclusion of that is, well, how about multi node jobs where, you know, they're smack full of the latest and greatest GPUs.
[00:36:41.000 --> 00:36:51.000]   And, you know, how many nodes can I spread my job across? And if you do, you know, I don't know, five billion parameters, then, yeah, you know, you're going to have to do that.
[00:36:51.000 --> 00:37:00.000]   And then you're going to be pretty busy. Right. Trying to organize, again, a job across a multiple sets of nodes.
[00:37:00.000 --> 00:37:14.000]   Do you have any sense on how your customers sort of view the tradeoff of, you know, buying more GPUs, buying more hardware to make their models perform better?
[00:37:14.000 --> 00:37:34.000]   Are they really doing like a clear ROI calculation? One of the things that we see at Weights & Biases is that it seems like our customers sort of use of GPUs just expands to fit whatever capacity they actually have,
[00:37:34.000 --> 00:37:51.000]   which I'm sure is like wonderful for Nvidia, but, you know, you wonder if the day will come where people start to scrutinize that cost more carefully or even, you know, some people have pointed out that there's possibly even environmental impact from just monstrous
[00:37:51.000 --> 00:38:05.000]   training runs or even a kind of a sad effect where, you know, no one can replicate the, you know, the latest academic research if it's only going to be done at like, you know, multi-million dollar scale.
[00:38:05.000 --> 00:38:08.000]   Yeah. Compute. How do you think about that?
[00:38:08.000 --> 00:38:25.000]   In the end, I think it's a pretty simple concept. I think that if the competitive advantage for companies today is derived from being able to train faster and larger and better models, you're not speaking to the CFO anymore, right?
[00:38:25.000 --> 00:38:47.000]   You're speaking to the product teams. And so at that point, it just becomes a completely different conversation, right? I mean, the only interesting piece here is that traditionally, of course, data center infrastructure is a cost center, whereas now we're talking about it, turning it into a value center.
[00:38:47.000 --> 00:39:03.000]   And so if you turn it into a value center, then you really don't have this problem. Now, yes, of course, we have extensive ROI conversations with our customers or, you know, we have TCO calculators and all that good stuff is definitely there.
[00:39:03.000 --> 00:39:17.000]   And it's really about helping customers choose, you know, should we do more cloud for, you know, for where we're at? And then, you know, from a GPU standpoint, we're happy either with either outcome, right?
[00:39:17.000 --> 00:39:29.000]   So we're maintaining neutrality in that aspect that we're saying, well, if more cloud usage turns out to be better for you, then you should absolutely go and do that, right?
[00:39:29.000 --> 00:39:42.000]   And then if we figure out that the economic shifted in such a way that a mix of cloud and on-prem or a cloud and hosted resources make sense, then, you know, we'll propose that, right?
[00:39:42.000 --> 00:39:53.000]   So it's really about finding the best solution there. And definitely our customers are asking these questions and making pretty hard calculations on that, right?
[00:39:53.000 --> 00:40:14.000]   But I mean, it's pretty obvious, right? I mean, if you think about it, what was it a couple of years ago, we, you know, talked to an autonomous driving lab team and they said, well, you know, company A put 300,000 miles autonomously on the road last year, and we put 70,000 miles on the road last year autonomously, right?
[00:40:14.000 --> 00:40:35.000]   We got to change that, right? How do I at least match the 300,000 miles a year that I can put autonomously on the road, right? And so that's a direct function of how well does your model work, right? And so on and so forth, right? So it's a pretty clear tie-in right now.
[00:40:35.000 --> 00:40:52.000]   What about inference? A lot of the customers that we talk to, inference is really the dominant compute cost that they have. So the training is actually much smaller than the spend on inference.
[00:40:52.000 --> 00:41:10.000]   Do you offer solutions for inference too? Like, could I use base command at inference time or is it entirely training? And do people ever use these DGX machines for inference or would that just be a crazy waste of an incredibly expensive resource?
[00:41:10.000 --> 00:41:35.000]   Well, I mean, yes and no. I mean, it depends on how you use it. So first of all, you can use base command for model validation purposes, right? So you can have sort of single shot runs. But some customers want to set up a server that is dedicated to inference and then just take mix slices and say, well, you know, I'll do my model validation at scale, basically.
[00:41:35.000 --> 00:41:48.000]   I'll do my scoring there. And so now if you share that infrastructure across a large number of data scientists, you put your DGX to a good use. I mean, there's no issue with that, right?
[00:41:48.000 --> 00:42:12.000]   We do have a sort of a sister SaaS offering to base command called Fleet Command. And that is meant to take the output of base command in form of a container, of course, and then deploy that at scale and orchestrate it at scale and really manage the inference workloads at the edge for our customers. So it's an end to end coverage there from a SaaS perspective.
[00:42:12.000 --> 00:42:29.000]   Cool. In your view, based on the problems that you're seeing in the market, what functionality are our customers asking for in their software layer for machine learning training that you're interested in providing?
[00:42:29.000 --> 00:42:50.000]   That's a really good question, because it sort of goes to the heart of the question, what role, what space is base command seeking to occupy, you know, in a theoretical stack of where, you know, the infrastructure is at the bottom and something like weights and biases at the top, right?
[00:42:50.000 --> 00:43:09.000]   I would see base commands role as a, an arbiter and a broker, and almost like a bridge between, you know, a pure developer focused, almost like an IDE perspective and bridge that into enterprise ready architecture.
[00:43:09.000 --> 00:43:27.000]   So let me give you a simple example. If you do dataset versioning, right, and let's say that's, you know, what you want to do with your MLOps platform, then, you know, then there's many ways to version data, right?
[00:43:27.000 --> 00:43:47.000]   You can try and be smart about this, but at the end of the day, right, it's a question of what infrastructure is available to you. If I have an optimized storage filer underneath, my dataset versioning strategy might be looking entirely different than if I just have kind of a scale out open source storage backend, right?
[00:43:47.000 --> 00:44:04.000]   If I work with S3 buckets, then my versioning looks different, you know, then I do that with NFS shares, right? And so the value that base command provides is that it extracts it away. If you do dataset versioning with base command, then, you know, it'll do snapshots.
[00:44:04.000 --> 00:44:18.000]   If you do it on an ADAPT filer, you know, it'll do other things if you do it with a different storage. And so, but those are exactly the questions that an enterprise, you know, architect will be interested in. How do you deal with that?
[00:44:18.000 --> 00:44:37.000]   Am I going to have to, like, just because you figure you need 50 versions of your dataset, that's three terabytes, you know, large, does that mean I need to plan for like almost infinite storage? No, it doesn't, right? We can help you translate that and make that consumable in the enterprise.
[00:44:37.000 --> 00:44:52.000]   And I think that's a big piece that I think that base command can provide as this arbiter between the infrastructure and sort of the API, if you will.
[00:44:52.000 --> 00:45:13.000]   The second thing is, is increasingly I've seen people being very concerned about data security and governance around this. So if you have a, you know, sufficiently large infrastructure to deal with, then almost always you have multiple geos to deal with.
[00:45:13.000 --> 00:45:29.000]   And you have different laws about the data that's being allowed at any given point in time. And so just the ability to say, well, this dataset can never leave France, right? Or that dataset has to only be visible to these three people and nobody else, right?
[00:45:29.000 --> 00:45:39.000]   Is of extreme value to enterprises. So all those things come into play. And I think that's where base command can help.
[00:45:39.000 --> 00:45:55.000]   Are there other parts of base command that you've put a lot of work into that people might not realize the amount of effort that it took that might be invisible just to a customer, even just me sort of even imagining what base command does?
[00:45:55.000 --> 00:46:22.000]   Yeah, you know, I think that we invested a lot in our scheduler. And I think if you look at the layout of DGXs in a super pod arrangement, and the nature of the jobs that go into this, I think people underestimate just how optimized the scheduler is across, you know, not just multiple nodes, but also within the node, right?
[00:46:22.000 --> 00:46:36.000]   So for you to be able to say, I'm running a job in one GPU configuration, and then it's a slider. And I say, well, I'm turning this into an eight GPU job now. And that's literally a selection.
[00:46:36.000 --> 00:46:58.000]   What goes on in the background is just a lot more intricate than people typically realize. But it goes on automatically. And you do have to be ready for it. You have to program for it. And people know that, right? But as soon as you do that at your layer, all the optimization underneath is just incredible.
[00:46:58.000 --> 00:47:06.000]   And what's tricky? Is it like you need to find eight GPUs that are close to each other and not being used and all that? Is that the challenge?
[00:47:06.000 --> 00:47:15.000]   Yeah, exactly. Data locality, caching strategies, all that kind of stuff is going straight into that selection.
[00:47:15.000 --> 00:47:37.000]   Cool. All right. Well, you know, we always end with two questions, both on ML. So let's see how you answer them. So one thing we always ask is, what's an underrated aspect of machine learning that you think people should pay more attention to, or you would love to spend time on if you had more free time?
[00:47:37.000 --> 00:48:00.000]   I think what's underrated is this aspect of crowdsourcing. I don't think anybody is looking at machine learning and the potential that just many small devices that are contributory to the creation of a model would bring. I think that we're at the cusp of that. But we're not really doing that right now.
[00:48:00.000 --> 00:48:16.000]   I think to the degree that it already happens, it's very hidden from us. We all know Google will run some algorithms across data that was collected through the phones. We understand that on a conceptual level.
[00:48:16.000 --> 00:48:35.000]   But just the ability to bring that together in a more natural sense that we might want to find recommendations, not on the basis of a single parameter, but find recommendations of more meaningful parameters. I find five star reviews very meaningless, for example.
[00:48:35.000 --> 00:48:58.000]   I think that is a very simplified view of the world. And I find consequently also one star reviews very meaningless. But if you could actually have a more natural understanding based on machine learning, I think that would be an interesting topic to explore because it would have to be based on just all kinds of inputs that would have to be taken into account.
[00:48:58.000 --> 00:49:17.000]   So I would like to see that. And I think that would be an interesting field of research, an interesting field of development. I think people still assume that it's only a prerogative of the big companies to be able to do that. But I think there's an open source project in there somewhere.
[00:49:17.000 --> 00:49:44.000]   Cool. I hope somebody starts that and they should send it to us when they do. And our final question is, when you look at your customers and their effort to take business problems and turn them into machine learning problems and deploy them and solve those problems, where do you see the biggest bottleneck? Where are they struggling the most right now?
[00:49:44.000 --> 00:50:11.000]   The biggest issue they have, at least as far as I can tell, is that they have just a getting started issue in the sense of how do I scale this beyond my initial POC? So I think that the prefab solutions that are out there are pretty good at walking you through a getting started tutorial.
[00:50:11.000 --> 00:50:32.000]   And then that probably gets you really far if you're a serious practitioner and you devote some time to it. But I think that at some point, you hit problems that may not even have anything to do with ML. They may just have something to do with infrastructure that's available to you and things like that.
[00:50:32.000 --> 00:50:55.000]   So I think that anybody trying to use this for commercial and a business strategic purpose is going to run into an issue of sooner or later, how do I go from point A to point B here? People call it like, what was it? It was something like AI DevOps or something like that, that floated around.
[00:50:55.000 --> 00:51:09.000]   And I think as an industry, we should be aiming to make sure that that job never comes and sees the light of day.
[00:51:09.000 --> 00:51:11.000]   Too late, I think.
[00:51:11.000 --> 00:51:39.000]   Yeah, I feel like we lost on that one already. But I really think we should do better. We shouldn't have to require super special skills to create this whole DevOps approach around AI training. We should really know better by now how that whole approach works and then build products that drive that.
[00:51:39.000 --> 00:51:43.000]   Awesome. Well, thanks so much for your time. I really appreciate it. That was fun.
[00:51:43.000 --> 00:51:44.000]   Thank you.
[00:51:44.000 --> 00:51:59.000]   If you're enjoying these interviews and you want to learn more, please click on the link to the show notes in the description where you can find links to all the papers that are mentioned, supplemental material, and a transcription that we worked really hard to produce. So check it out.
[00:51:59.000 --> 00:51:59.840]   Check it out.
[00:51:59.840 --> 00:52:01.140]   (gentle upbeat music)

