
[00:00:00.000 --> 00:00:06.000]   Today we're going to talk about using convolutional neural nets and LSTMs and deep learning techniques
[00:00:06.000 --> 00:00:07.520]   to video.
[00:00:07.520 --> 00:00:10.940]   And in particular, we're going to do frame prediction.
[00:00:10.940 --> 00:00:15.760]   So this is taking some set of frames from a video and predicting what the next frame
[00:00:15.760 --> 00:00:16.760]   is going to be.
[00:00:16.760 --> 00:00:18.520]   These techniques are generally applicable.
[00:00:18.520 --> 00:00:22.280]   This is our first video on video, I guess, and we can do some more.
[00:00:22.280 --> 00:00:27.760]   But I really like this application, even though it's a really, really difficult application,
[00:00:27.760 --> 00:00:31.960]   because I sort of imagine that maybe one day we could use this to automatically generate
[00:00:31.960 --> 00:00:32.960]   these videos.
[00:00:32.960 --> 00:00:37.280]   Or maybe you could imagine putting a video camera on me and predicting where I might
[00:00:37.280 --> 00:00:38.280]   go.
[00:00:38.280 --> 00:00:39.280]   Now, we're really far from that.
[00:00:39.280 --> 00:00:44.040]   Actually, in fact, the useful applications that we have today are really around compression
[00:00:44.040 --> 00:00:45.040]   with this video frame prediction.
[00:00:45.040 --> 00:00:47.360]   So people really do use it for that.
[00:00:47.360 --> 00:00:53.520]   And people are thinking about it a lot, because drones and autonomous vehicles, they need
[00:00:53.520 --> 00:00:58.120]   a way to actually look at the world through a camera and predict where things are going.
[00:00:58.120 --> 00:01:02.160]   Now, one issue with video and why we kind of waited to do this until later in the series
[00:01:02.160 --> 00:01:04.560]   is that it requires a ton of compute.
[00:01:04.560 --> 00:01:06.360]   And so that's not really fun for a beginner.
[00:01:06.360 --> 00:01:09.600]   And a lot of people watching these videos, I know that you don't have access to tons
[00:01:09.600 --> 00:01:11.080]   of compute.
[00:01:11.080 --> 00:01:17.660]   And so we tried to make a problem for today that would be fun to do even without tons
[00:01:17.660 --> 00:01:18.660]   of compute.
[00:01:18.660 --> 00:01:23.120]   So we actually talked to our friends at Giphy, the company, and they gave us a whole set
[00:01:23.120 --> 00:01:25.280]   of animated GIFs of cats.
[00:01:25.280 --> 00:01:27.480]   Now, actually, not all of them really are of cats.
[00:01:27.480 --> 00:01:29.920]   These are just videos tagged with cat.
[00:01:29.920 --> 00:01:31.840]   But that makes it even more interesting.
[00:01:31.840 --> 00:01:37.620]   And the goal is to take a few frames of these animated GIFs and predict the next frame.
[00:01:37.620 --> 00:01:39.280]   So even that, that might seem simple.
[00:01:39.280 --> 00:01:43.560]   In fact, you could even just use the last frame to predict the next frame by just guessing
[00:01:43.560 --> 00:01:44.600]   it and it works OK.
[00:01:44.600 --> 00:01:49.880]   But even doing this kind of simple task that a human could probably do without too much
[00:01:49.880 --> 00:01:53.280]   work, we're going to see is pretty tough for the methods today.
[00:01:53.280 --> 00:01:54.680]   So let's jump right in.
[00:01:54.680 --> 00:01:59.380]   So let's walk through this notebook on how we're actually going to do video frame prediction.
[00:01:59.380 --> 00:02:03.760]   And as always, we make all this code publicly available for you to use and modify.
[00:02:03.760 --> 00:02:07.600]   And if you do something cool with it, please let us know.
[00:02:07.600 --> 00:02:12.480]   So we start off by going through and importing a whole bunch of files, which we're going
[00:02:12.480 --> 00:02:16.280]   to need as we build this model.
[00:02:16.280 --> 00:02:23.520]   And we set some hyperparameters and we load in a data set of cats.
[00:02:23.520 --> 00:02:28.880]   So here we're going to use a generator to feed into our fit function.
[00:02:28.880 --> 00:02:32.480]   So we go over generators in detail in our data augmentation video.
[00:02:32.480 --> 00:02:36.240]   But what this is going to do is instead of loading all of the data into memory, it's
[00:02:36.240 --> 00:02:39.720]   going to load the data into memory in batches.
[00:02:39.720 --> 00:02:44.180]   So in my generator, we're going to pass in batch size and then an image directory.
[00:02:44.180 --> 00:02:47.160]   And the image directory is going to tell us whether we're building a generator on the
[00:02:47.160 --> 00:02:49.320]   test data or the train data.
[00:02:49.320 --> 00:02:56.280]   And the batch size is going to tell us how many images or how many videos to load.
[00:02:56.280 --> 00:02:59.600]   So this is a Python generator style.
[00:02:59.600 --> 00:03:03.400]   And what it does is it sets up input images and output images.
[00:03:03.400 --> 00:03:09.880]   And in this case, it actually flattens all the input images into a 96 by 96 by 15 array,
[00:03:09.880 --> 00:03:12.920]   where 96 is the image width, 96 is the image height.
[00:03:12.920 --> 00:03:17.920]   And then 15 is actually because there's five RGB image frames.
[00:03:17.920 --> 00:03:21.200]   So there's 15 frames all in a row.
[00:03:21.200 --> 00:03:26.880]   And then the output images is going to be three frames in a sense, or one frame of video
[00:03:26.880 --> 00:03:28.800]   where it's RGB values.
[00:03:28.800 --> 00:03:35.800]   We actually divide the input images and output images by 255 to normalize all the data between
[00:03:35.800 --> 00:03:37.040]   zero and one.
[00:03:37.040 --> 00:03:38.960]   And then we call this yield function.
[00:03:38.960 --> 00:03:45.920]   And so this yield is going to return input images, which again is this batch size by
[00:03:45.920 --> 00:03:49.920]   96 by 96 by 15 data structure.
[00:03:49.920 --> 00:03:53.480]   And then output images, which is going to be the single frame it wants next.
[00:03:53.480 --> 00:03:55.980]   But there's going to be batch size number of them.
[00:03:55.980 --> 00:04:00.660]   So there's a matching number of input images and output images.
[00:04:00.660 --> 00:04:05.800]   We're also going to set steps per epoch, which is going to be how many steps we take before
[00:04:05.800 --> 00:04:09.640]   we check our validation accuracy and validation metrics.
[00:04:09.640 --> 00:04:12.840]   And then validation steps, which is how much of the validation data we're going to look
[00:04:12.840 --> 00:04:17.760]   at each time that we want to measure our validation metrics.
[00:04:17.760 --> 00:04:21.800]   We're going to set a callback here to log the images.
[00:04:21.800 --> 00:04:24.460]   And then here I have a little test for our generator.
[00:04:24.460 --> 00:04:28.320]   We don't need to necessarily run this, but this might give you a better understanding
[00:04:28.320 --> 00:04:30.040]   of how the generator works.
[00:04:30.040 --> 00:04:34.120]   So in this case, we set up a generator to have a batch size of two.
[00:04:34.120 --> 00:04:36.280]   And we're going to use the train directory.
[00:04:36.280 --> 00:04:39.200]   And then the next line here, we actually call next on the generator.
[00:04:39.200 --> 00:04:42.960]   So that's going to return two videos and two next frames.
[00:04:42.960 --> 00:04:49.040]   So videos, as I mentioned, is actually five frames of video with RGB values for each frame.
[00:04:49.040 --> 00:04:52.000]   And so the shape here is going to be 96 by 96 by 15.
[00:04:52.000 --> 00:04:55.880]   And there's actually two of these videos and then two of the next frames.
[00:04:55.880 --> 00:04:58.340]   So we could try calling next frame dot shape.
[00:04:58.340 --> 00:05:00.260]   You might think about what that shape's going to be.
[00:05:00.260 --> 00:05:03.880]   But if we did it, it would be 96 by 96 by 3.
[00:05:03.880 --> 00:05:06.040]   It'd be called next frame 0 dot shape.
[00:05:06.040 --> 00:05:07.040]   Maybe we'll just do that.
[00:05:07.040 --> 00:05:13.200]   All right, so let's take a look at the data.
[00:05:13.200 --> 00:05:15.400]   So we can use the incredibly useful imshow command.
[00:05:15.400 --> 00:05:18.820]   And we can look at each frame of video that's our input.
[00:05:18.820 --> 00:05:21.640]   And then finally, we can take a look at the next frame, which is the video
[00:05:21.640 --> 00:05:23.640]   that we're trying to predict.
[00:05:23.640 --> 00:05:26.000]   And you can see that in this example, at least,
[00:05:26.000 --> 00:05:28.320]   that each frame of video is very similar.
[00:05:28.320 --> 00:05:30.660]   These are successive frames.
[00:05:30.660 --> 00:05:33.160]   And so there's not much motion happening.
[00:05:33.160 --> 00:05:35.640]   And this is the final frame that we're trying to predict.
[00:05:35.640 --> 00:05:37.340]   So other videos might look differently.
[00:05:37.340 --> 00:05:40.080]   And I strongly recommend exploring this a little bit
[00:05:40.080 --> 00:05:43.640]   before trying to do a real prediction.
[00:05:43.640 --> 00:05:46.760]   Now, one more detail is that we're going to set
[00:05:46.760 --> 00:05:49.000]   a function called perceptual distance.
[00:05:49.000 --> 00:05:52.360]   And this is because if we just actually looked at the default loss function,
[00:05:52.360 --> 00:05:55.480]   which would be the difference between the red values in the pixel
[00:05:55.480 --> 00:05:58.480]   and the green pixels and the blue pixels,
[00:05:58.480 --> 00:06:00.400]   it actually isn't exactly how the eye works.
[00:06:00.400 --> 00:06:03.160]   So for example, if you get all the blue pixels right,
[00:06:03.160 --> 00:06:04.960]   but you mess up the red pixels, that's going
[00:06:04.960 --> 00:06:06.400]   to look pretty bad to the human eye.
[00:06:06.400 --> 00:06:09.280]   In fact, that's going to look worse than if you got the red pixels right
[00:06:09.280 --> 00:06:11.680]   and didn't get the blue pixels right.
[00:06:11.680 --> 00:06:14.520]   There's many different ways to define perceptual distance.
[00:06:14.520 --> 00:06:16.960]   But we're going to use a reasonably simple one here,
[00:06:16.960 --> 00:06:19.280]   which is kind of based on how the human eye works,
[00:06:19.280 --> 00:06:23.160]   but also pretty quick to calculate.
[00:06:23.160 --> 00:06:26.720]   So now here, we set up our first model.
[00:06:26.720 --> 00:06:31.200]   And so what we're going to do is a 2D convolution on our input.
[00:06:31.200 --> 00:06:34.640]   So our input is 96 by 96 by 15.
[00:06:34.640 --> 00:06:37.120]   And we're actually just going to take each of those frames,
[00:06:37.120 --> 00:06:40.720]   and we're going to do a 2D convolution across all 15 frames
[00:06:40.720 --> 00:06:42.600]   with three frame output.
[00:06:42.600 --> 00:06:45.920]   And we're going to interpret that three frame output as red, green,
[00:06:45.920 --> 00:06:47.200]   and blue.
[00:06:47.200 --> 00:06:49.440]   We're going to do a 3 by 3 kernel.
[00:06:49.440 --> 00:06:52.320]   And we're going to set padding equal to same here, which,
[00:06:52.320 --> 00:06:55.400]   as mentioned in earlier videos, means that the output size is
[00:06:55.400 --> 00:06:58.920]   going to be the same as the input size, so it zero pads the data.
[00:06:58.920 --> 00:07:02.480]   And our input shape is going to be config height by config width,
[00:07:02.480 --> 00:07:05.920]   which is 96 by 96 by 5 times 3, which is the five
[00:07:05.920 --> 00:07:09.600]   frames of three RGB values.
[00:07:09.600 --> 00:07:12.280]   We're going to compile it and set the optimizer to atom,
[00:07:12.280 --> 00:07:14.120]   the loss to mean squared error.
[00:07:14.120 --> 00:07:16.560]   And we're going to set this perceptual distance metric
[00:07:16.560 --> 00:07:20.600]   as something for the model to return to us as it trains.
[00:07:20.600 --> 00:07:22.760]   Then we're going to call fit generator.
[00:07:22.760 --> 00:07:25.480]   So fit generator is a lot like model.fit,
[00:07:25.480 --> 00:07:28.760]   except that instead of taking in tensors as input,
[00:07:28.760 --> 00:07:31.160]   it takes in generators that return tensors.
[00:07:31.160 --> 00:07:34.920]   So in this case, we're going to pass in my generator, which
[00:07:34.920 --> 00:07:38.000]   is initialized with config.batchsize, and then
[00:07:38.000 --> 00:07:40.520]   our training directory, so it returns the training input
[00:07:40.520 --> 00:07:42.800]   frames and output frames.
[00:07:42.800 --> 00:07:44.480]   Then we're going to set steps for epic.
[00:07:44.480 --> 00:07:46.100]   And we're going to divide by four here,
[00:07:46.100 --> 00:07:48.380]   just so that we get more data as the model runs.
[00:07:48.380 --> 00:07:50.080]   We could remove this later if we wanted
[00:07:50.080 --> 00:07:53.860]   to go longer between checking the validation accuracy.
[00:07:53.860 --> 00:07:56.180]   We're going to set some callbacks, image callback
[00:07:56.180 --> 00:07:58.900]   and W and B callback, so we can kind of see what's going on.
[00:07:58.900 --> 00:08:01.520]   And then we're going to also set the number of validation steps
[00:08:01.520 --> 00:08:04.940]   to divide by four, just so the model runs a little bit faster.
[00:08:04.940 --> 00:08:07.760]   We set the validation data to be a generator,
[00:08:07.760 --> 00:08:10.580]   this time initialized with the same config.batchsize,
[00:08:10.580 --> 00:08:12.340]   but in this case, the validation directory,
[00:08:12.340 --> 00:08:16.940]   so that we get the validation images for the validation data.
[00:08:16.940 --> 00:08:19.280]   So let's run this and see how it does.
[00:08:19.280 --> 00:08:22.420]   So you can follow this link here to see
[00:08:22.420 --> 00:08:25.820]   how the model is training.
[00:08:25.820 --> 00:08:28.220]   And you can see that the model is getting better,
[00:08:28.220 --> 00:08:30.220]   but actually perceptual distance is pretty high.
[00:08:30.220 --> 00:08:34.380]   So right now, the perceptual distance is at 43.
[00:08:34.380 --> 00:08:36.680]   And I actually have no idea what that means,
[00:08:36.680 --> 00:08:38.620]   if that's really good or bad.
[00:08:38.620 --> 00:08:41.740]   But one thing to look at is the actual output.
[00:08:41.740 --> 00:08:45.420]   So here on the left, we have the frame
[00:08:45.420 --> 00:08:46.980]   that we were hoping for, and on the right,
[00:08:46.980 --> 00:08:47.940]   we have our prediction.
[00:08:47.940 --> 00:08:50.100]   And you can see this is kind of obviously wrong.
[00:08:50.100 --> 00:08:52.700]   It's kind of close, but there's clearly still
[00:08:52.700 --> 00:08:53.700]   some weird artifacts.
[00:08:53.700 --> 00:08:55.220]   It's more blurry.
[00:08:55.220 --> 00:08:57.020]   The colors are different.
[00:08:57.020 --> 00:08:59.340]   So it doesn't seem like this model is actually
[00:08:59.340 --> 00:09:01.580]   performing very well.
[00:09:01.580 --> 00:09:02.620]   So how do we improve it?
[00:09:02.620 --> 00:09:06.740]   So one thing that we noticed looking at our input data
[00:09:06.740 --> 00:09:08.980]   is that the final frame is actually
[00:09:08.980 --> 00:09:13.100]   a pretty good prediction of the output frame.
[00:09:13.100 --> 00:09:14.940]   So let's build a model that just takes
[00:09:14.940 --> 00:09:18.940]   the last frame in our input and predicts it as the next frame.
[00:09:18.940 --> 00:09:21.140]   So here's our Keras model.
[00:09:21.140 --> 00:09:23.180]   First, we actually use a reshape layer
[00:09:23.180 --> 00:09:26.620]   to change our input, which was 96 by 96 by 15,
[00:09:26.620 --> 00:09:29.840]   into 96 by 96 by 5 by 3.
[00:09:29.840 --> 00:09:33.580]   So what that does is it actually pulls out each RGB value
[00:09:33.580 --> 00:09:35.660]   and kind of puts it in its own slot.
[00:09:35.660 --> 00:09:38.180]   So instead of all the RGB values mashed together,
[00:09:38.180 --> 00:09:39.860]   we have a little more structure.
[00:09:39.860 --> 00:09:42.860]   And what that lets us do is call this permute layer.
[00:09:42.860 --> 00:09:44.780]   And so what permute in this case does
[00:09:44.780 --> 00:09:48.580]   is it swaps the last two dimensions of our data.
[00:09:48.580 --> 00:09:52.260]   So the input to this is 96 by 96 by 5 by 3.
[00:09:52.260 --> 00:09:56.660]   The output is going to be 96 by 96 by 3 by 5.
[00:09:56.660 --> 00:10:01.260]   So now we have five sets of RGB values.
[00:10:01.260 --> 00:10:03.860]   And we can call this layer, which is a lambda layer, which
[00:10:03.860 --> 00:10:05.340]   you may not have seen before.
[00:10:05.340 --> 00:10:08.060]   And this is a Keras layer that can do basically anything,
[00:10:08.060 --> 00:10:09.580]   as long as it operates on tensors.
[00:10:09.580 --> 00:10:12.520]   So it actually calls our function slice.
[00:10:12.520 --> 00:10:14.820]   In this case, we've defined a little function up here.
[00:10:14.820 --> 00:10:17.540]   And what slice does is it slices off
[00:10:17.540 --> 00:10:21.460]   the last frame of our input video.
[00:10:21.460 --> 00:10:24.900]   So this lambda layer is going to take those five frames,
[00:10:24.900 --> 00:10:26.660]   and it's going to return the final one.
[00:10:26.660 --> 00:10:30.340]   So the output shape is going to be 96 by 96 by 3.
[00:10:30.340 --> 00:10:31.740]   So this is kind of a funny model.
[00:10:31.740 --> 00:10:34.260]   We wouldn't normally think of this as a deep learning model.
[00:10:34.260 --> 00:10:36.180]   But we've defined it as a Keras model.
[00:10:36.180 --> 00:10:38.740]   So we can actually call model.compile on it.
[00:10:38.740 --> 00:10:40.180]   We can give it the same optimizer.
[00:10:40.180 --> 00:10:41.980]   We can give it the same loss function.
[00:10:41.980 --> 00:10:42.900]   Although it's not going to matter,
[00:10:42.900 --> 00:10:44.940]   because there's nothing here really to optimize.
[00:10:44.940 --> 00:10:47.740]   And we can even call fit generator and see how it does.
[00:10:47.740 --> 00:11:00.380]   So you can see that in this graph, right from the start,
[00:11:00.380 --> 00:11:05.300]   predict last frame does much better than our 2D CNN model.
[00:11:05.300 --> 00:11:07.800]   And we can even go in and we can look at its predictions.
[00:11:07.800 --> 00:11:10.060]   And we can see that actually predicting the last frame,
[00:11:10.060 --> 00:11:12.060]   in many cases, is quite good.
[00:11:12.060 --> 00:11:14.100]   It's really close to the output frame,
[00:11:14.100 --> 00:11:17.620]   just because all these frames are in such rapid succession.
[00:11:17.620 --> 00:11:19.300]   But now, that doesn't feel that exciting.
[00:11:19.300 --> 00:11:21.980]   It sort of feels like a CNN model
[00:11:21.980 --> 00:11:23.860]   should be able to learn a little bit more.
[00:11:23.860 --> 00:11:27.900]   In fact, a CNN model could predict just the last frame
[00:11:27.900 --> 00:11:29.380]   with the right kernel.
[00:11:29.380 --> 00:11:32.180]   So it's kind of interesting that it's so hard for a 2D CNN
[00:11:32.180 --> 00:11:34.740]   model to learn to do this predict last frame.
[00:11:34.740 --> 00:11:36.740]   You would think that it would do strictly better.
[00:11:36.740 --> 00:11:39.260]   And maybe over time, with enough training,
[00:11:39.260 --> 00:11:41.460]   it would learn to do this.
[00:11:41.460 --> 00:11:43.980]   But we don't have an infinite amount of time.
[00:11:43.980 --> 00:11:45.500]   So we want to help our model.
[00:11:45.500 --> 00:11:48.420]   So our idea is that we're going to take these two models
[00:11:48.420 --> 00:11:49.900]   and we're going to combine them.
[00:11:49.900 --> 00:11:51.940]   So we're going to make the baseline be predicting
[00:11:51.940 --> 00:11:52.620]   this last frame.
[00:11:52.620 --> 00:11:54.060]   But then on top of that, it's going
[00:11:54.060 --> 00:11:56.420]   to be able to use a convolution model,
[00:11:56.420 --> 00:12:00.420]   potentially modify it to get higher performance.
[00:12:00.420 --> 00:12:02.860]   So now, in order to combine two models,
[00:12:02.860 --> 00:12:04.820]   we're going to have to get away from the simpler
[00:12:04.820 --> 00:12:08.900]   Keras sequential style and move to a more functional style.
[00:12:08.900 --> 00:12:10.820]   And actually, we introduced the functional style
[00:12:10.820 --> 00:12:13.100]   in an earlier video on one-shot learning.
[00:12:13.100 --> 00:12:15.220]   But here, it's going to be really useful to us,
[00:12:15.220 --> 00:12:16.680]   because we're actually going to try
[00:12:16.680 --> 00:12:19.180]   to do in parallel predicting the last frame
[00:12:19.180 --> 00:12:20.900]   and a convolutional model.
[00:12:20.900 --> 00:12:23.060]   And this is going to make our model work significantly
[00:12:23.060 --> 00:12:23.560]   better.
[00:12:23.560 --> 00:12:27.060]   So before we add the convolutional model in,
[00:12:27.060 --> 00:12:29.980]   I wanted to show how we would take the same predict last
[00:12:29.980 --> 00:12:33.100]   frame model and just switch it into a functional style.
[00:12:33.100 --> 00:12:36.540]   So this cell here is actually identical to the previous cell
[00:12:36.540 --> 00:12:38.900]   we looked at, except it's in the Keras functional style.
[00:12:38.900 --> 00:12:43.860]   So instead of calling model.add, we set input to be a tensor.
[00:12:43.860 --> 00:12:45.940]   And then we call the same reshape layer
[00:12:45.940 --> 00:12:47.420]   on that input tensor.
[00:12:47.420 --> 00:12:49.020]   Then we call the same permute layer
[00:12:49.020 --> 00:12:50.740]   on the output of the reshape layer.
[00:12:50.740 --> 00:12:52.160]   And then we call the lambda layer
[00:12:52.160 --> 00:12:54.180]   on the output of the permute layer.
[00:12:54.180 --> 00:12:56.580]   And then finally, we set our model in the functional style
[00:12:56.580 --> 00:12:59.600]   where we set the input and we set the output as tensors
[00:12:59.600 --> 00:13:01.660]   that we've already defined.
[00:13:01.660 --> 00:13:04.020]   So this should behave identically
[00:13:04.020 --> 00:13:06.420]   to the previous model that we built.
[00:13:06.420 --> 00:13:08.500]   And it's good to just check that it does.
[00:13:08.500 --> 00:13:09.860]   And you can see that it seems to.
[00:13:09.860 --> 00:13:11.620]   The perceptual distance in this case
[00:13:11.620 --> 00:13:13.420]   bounces around a little bit because there's
[00:13:13.420 --> 00:13:15.900]   differences in order of the images that get shown to it.
[00:13:15.900 --> 00:13:18.220]   But it does seem to be performing identically
[00:13:18.220 --> 00:13:20.140]   to the previous model that we built.
[00:13:20.140 --> 00:13:23.100]   So now we can make something more interesting.
[00:13:23.100 --> 00:13:25.480]   We can actually combine these two.
[00:13:25.480 --> 00:13:27.140]   So now we're going to try to take
[00:13:27.140 --> 00:13:29.660]   the convolutional network.
[00:13:29.660 --> 00:13:31.660]   And we're going to try to take the network that
[00:13:31.660 --> 00:13:34.500]   outputted just the last layer and combine them
[00:13:34.500 --> 00:13:38.340]   into essentially an ensemble where both methods can
[00:13:38.340 --> 00:13:39.300]   make their prediction.
[00:13:39.300 --> 00:13:41.340]   And we can merge them into a single prediction.
[00:13:41.340 --> 00:13:44.060]   And so we do the same reshape.
[00:13:44.060 --> 00:13:46.480]   And we do the same permute.
[00:13:46.480 --> 00:13:49.700]   And then we do the same lambda on the output
[00:13:49.700 --> 00:13:50.740]   of the permute layer.
[00:13:50.740 --> 00:13:53.900]   But then we also call a 3D convolution, in this case,
[00:13:53.900 --> 00:13:56.300]   on the output of the permuted layer.
[00:13:56.300 --> 00:13:59.020]   So it might make more sense to do a 3D convolution
[00:13:59.020 --> 00:14:03.020]   than a 2D convolution because we're dealing with image data.
[00:14:03.020 --> 00:14:05.420]   So a 3D convolution is just like a 2D convolution.
[00:14:05.420 --> 00:14:07.580]   But it's going to take in a 3D data structure.
[00:14:07.580 --> 00:14:10.660]   In this case, it's going to be the 96 width by 96 height
[00:14:10.660 --> 00:14:13.340]   by the five frames.
[00:14:13.340 --> 00:14:16.380]   So actually, all we need here is a single 3D convolution
[00:14:16.380 --> 00:14:18.840]   because it's going to operate on the red plane and the green
[00:14:18.840 --> 00:14:20.740]   plane and the blue plane.
[00:14:20.740 --> 00:14:26.440]   And then it's going to output a 96 by 96 by 3 by 1 data
[00:14:26.440 --> 00:14:28.860]   structure, which we're then going
[00:14:28.860 --> 00:14:33.060]   to reshape into 96 by 96 by 3 so we can use it.
[00:14:33.060 --> 00:14:35.580]   And then we're going to take that convolution output
[00:14:35.580 --> 00:14:39.180]   and we're going to add it to the last layer output.
[00:14:39.180 --> 00:14:42.460]   And so what this does, this just sums the last layer
[00:14:42.460 --> 00:14:44.020]   and the convolution output reshaped.
[00:14:44.020 --> 00:14:46.860]   So potentially, if the convolution output reshaped
[00:14:46.860 --> 00:14:49.540]   is all zeros, it's just going to use the last layer.
[00:14:49.540 --> 00:14:52.700]   But if there's information in that conv output reshape,
[00:14:52.700 --> 00:14:54.780]   then it's going to use that information.
[00:14:54.780 --> 00:14:57.380]   So hopefully, this combined model
[00:14:57.380 --> 00:14:59.580]   will do better than either of the models alone.
[00:14:59.580 --> 00:15:04.140]   So one thing that you'll notice when you run this model
[00:15:04.140 --> 00:15:06.300]   is that it's a lot slower than just running
[00:15:06.300 --> 00:15:09.460]   the deterministic model.
[00:15:09.460 --> 00:15:27.980]  , So one thing that's clear with our hybrid model,
[00:15:27.980 --> 00:15:31.460]   though, is that the validation loss is not
[00:15:31.460 --> 00:15:33.060]   as good as the training loss.
[00:15:33.060 --> 00:15:35.260]   And so clearly, there's some overfitting happening.
[00:15:35.260 --> 00:15:37.860]   And we talked a lot about this in previous videos.
[00:15:37.860 --> 00:15:40.220]   And we talked about using dropout, which is more common.
[00:15:40.220 --> 00:15:41.940]   But actually, with video data, it's
[00:15:41.940 --> 00:15:44.180]   typical to add something called Gaussian noise.
[00:15:44.180 --> 00:15:46.940]   So instead of dropout, which would take some of the pixels
[00:15:46.940 --> 00:15:50.300]   and just set them to zero, which you could try, in this case,
[00:15:50.300 --> 00:15:52.220]   we're going to add Gaussian noise, which is just
[00:15:52.220 --> 00:15:54.580]   going to add a random number that's
[00:15:54.580 --> 00:15:56.900]   Gaussian distributed with a standard deviation of,
[00:15:56.900 --> 00:15:59.460]   in this case, 0.1.
[00:15:59.460 --> 00:16:01.220]   And it's going to add that to every pixel.
[00:16:01.220 --> 00:16:03.420]   And so in each batch, the noise that it adds
[00:16:03.420 --> 00:16:04.820]   is going to be different.
[00:16:04.820 --> 00:16:06.420]   And this, hopefully, is going to help our model
[00:16:06.420 --> 00:16:07.540]   learn to be more robust.
[00:16:07.540 --> 00:16:10.140]   So this is just the only line that we added.
[00:16:10.140 --> 00:16:13.100]   And now, instead of last layer being a lambda function
[00:16:13.100 --> 00:16:15.460]   on permuted and convolution output
[00:16:15.460 --> 00:16:17.580]   being a conf3d on permuted, both of these
[00:16:17.580 --> 00:16:19.620]   are called on the noise layer.
[00:16:19.620 --> 00:16:27.900]   So you can see that, as expected,
[00:16:27.900 --> 00:16:29.420]   the validation perceptual distance
[00:16:29.420 --> 00:16:33.540]   is much better than the training perceptual distance.
[00:16:33.540 --> 00:16:35.700]   Because in this case, in the validation step,
[00:16:35.700 --> 00:16:37.660]   we don't actually add the Gaussian noise, which
[00:16:37.660 --> 00:16:38.860]   makes the task much harder.
[00:16:38.860 --> 00:16:48.340]   But you can see, though, that immediately adding
[00:16:48.340 --> 00:16:50.420]   this Gaussian noise actually helps our model.
[00:16:50.420 --> 00:16:53.460]   The hybrid model with Gaussian noise is the blue line here.
[00:16:53.460 --> 00:16:57.020]   And the model without is the orange line.
[00:16:57.020 --> 00:16:59.780]   The 3D convolution, though, can make sense
[00:16:59.780 --> 00:17:02.860]   if you have a specific number of video frames.
[00:17:02.860 --> 00:17:05.060]   But typically, you have a variable number
[00:17:05.060 --> 00:17:06.140]   of video frames.
[00:17:06.140 --> 00:17:09.900]   And in that case, it probably makes more sense to use an LSTM.
[00:17:09.900 --> 00:17:11.620]   But now, a standard LSTM wouldn't really
[00:17:11.620 --> 00:17:13.860]   make sense on a video, because the videos actually
[00:17:13.860 --> 00:17:16.260]   have a two-dimensional shape.
[00:17:16.260 --> 00:17:17.820]   And so there's a layer we can use
[00:17:17.820 --> 00:17:19.900]   that's perfect for this situation, which is called
[00:17:19.900 --> 00:17:22.340]   a convolutional LSTM2D.
[00:17:22.340 --> 00:17:24.940]   So this is just like the LSTMs that we talked
[00:17:24.940 --> 00:17:26.500]   about in a previous video.
[00:17:26.500 --> 00:17:29.420]   But at each step, instead of doing, essentially,
[00:17:29.420 --> 00:17:33.140]   a dense layer, it's doing a convolution.
[00:17:33.140 --> 00:17:34.980]   So it's perfect for this video case.
[00:17:34.980 --> 00:17:37.100]   And you can actually add it with just one line here.
[00:17:37.100 --> 00:17:40.380]   So instead of that conv3D, we're going
[00:17:40.380 --> 00:17:43.500]   to use a convLSTM2D layer.
[00:17:43.500 --> 00:17:46.220]   And it has all the same options as an LSTM.
[00:17:46.220 --> 00:17:48.180]   So if we're going to stack these,
[00:17:48.180 --> 00:17:50.220]   we'd have to call returnSequences equal true.
[00:17:50.220 --> 00:17:51.800]   And if that doesn't make sense to you,
[00:17:51.800 --> 00:17:53.460]   go back to the LSTM video, which is now
[00:17:53.460 --> 00:17:55.580]   super relevant for this case.
[00:17:55.580 --> 00:17:56.980]   It's only actually, in this case,
[00:17:56.980 --> 00:17:59.060]   because we didn't call returnSequences equal true,
[00:17:59.060 --> 00:18:02.180]   it's only going to return the last output.
[00:18:02.180 --> 00:18:04.900]   And then we're going to run that through a conv2D
[00:18:04.900 --> 00:18:10.140]   to get it in the shape that we want, which is 96 by 96 by 3.
[00:18:10.140 --> 00:18:12.420]   And then we're going to actually combine
[00:18:12.420 --> 00:18:16.020]   the convolutional output and the output of just taking
[00:18:16.020 --> 00:18:18.100]   the last layer.
[00:18:18.100 --> 00:18:20.220]   So this is a very small change.
[00:18:20.220 --> 00:18:22.260]   This is a two-line change here.
[00:18:22.260 --> 00:18:25.260]   But it actually makes our model significantly more complicated
[00:18:25.260 --> 00:18:26.620]   and powerful.
[00:18:26.620 --> 00:18:29.220]   And it's going to make it take significantly longer to train.
[00:18:29.220 --> 00:18:30.220]   So let's get it started.
[00:18:30.220 --> 00:18:38.140]   Yeah, we should get a shot of this cool model.
[00:18:38.140 --> 00:18:38.820]   Look at that.
[00:18:38.820 --> 00:18:43.100]   Got a convLSTM2D followed by a conv2D.
[00:18:43.100 --> 00:18:45.220]   Just adding the outputs.
[00:18:45.220 --> 00:19:09.700]   [AUDIO OUT]
[00:19:09.700 --> 00:19:10.660]   So there's a lot of different places
[00:19:10.660 --> 00:19:11.860]   that we could go with this.
[00:19:11.860 --> 00:19:14.020]   And one direction we can go is essentially
[00:19:14.020 --> 00:19:16.060]   borrowing from autoencoders.
[00:19:16.060 --> 00:19:17.840]   So there's a whole bunch of papers
[00:19:17.840 --> 00:19:19.580]   out there on video processing.
[00:19:19.580 --> 00:19:21.660]   And they use an architecture where they essentially
[00:19:21.660 --> 00:19:24.940]   chain together convLSTM2D models,
[00:19:24.940 --> 00:19:28.940]   at first shrinking the output and then expanding the output.
[00:19:28.940 --> 00:19:30.900]   So this kind of combines some of the stuff
[00:19:30.900 --> 00:19:32.580]   from many different videos we've done.
[00:19:32.580 --> 00:19:35.980]   So first, we are using this convLSTM2D operation,
[00:19:35.980 --> 00:19:38.500]   which we explained earlier, which is kind of like an LSTM
[00:19:38.500 --> 00:19:41.140]   but operates on two-dimensional data.
[00:19:41.140 --> 00:19:43.940]   Then we use time distributed, which is actually
[00:19:43.940 --> 00:19:47.540]   something we introduced in the seek2seek video that we did.
[00:19:47.540 --> 00:19:49.300]   And we're time distributing, in this case,
[00:19:49.300 --> 00:19:51.780]   max pooling 2D, which is going to shrink down
[00:19:51.780 --> 00:19:55.740]   the output of these convolutional LSTM layers.
[00:19:55.740 --> 00:19:58.020]   Then we're going to do another convLSTM2D.
[00:19:58.020 --> 00:20:00.100]   And then we're going to do another shrinking down.
[00:20:00.100 --> 00:20:01.900]   But then we're going to actually upsample.
[00:20:01.900 --> 00:20:06.100]   So we're going to expand out the output of these LSTM2Ds.
[00:20:06.100 --> 00:20:08.500]   So we're going to build this pretty complicated model, which
[00:20:08.500 --> 00:20:12.500]   has a whole chain of LSTMs, at first shrinking down
[00:20:12.500 --> 00:20:14.940]   the outputs and then expanding the outputs.
[00:20:14.940 --> 00:20:16.740]   And then finally, we take the output
[00:20:16.740 --> 00:20:19.380]   of this complicated thing and we concatenate it
[00:20:19.380 --> 00:20:20.860]   with the last layer output, which
[00:20:20.860 --> 00:20:23.220]   is a very simple thing of just returning the last layer.
[00:20:23.220 --> 00:20:26.340]   And then we use this 2D convolution
[00:20:26.340 --> 00:20:27.380]   with a one-by-one kernel.
[00:20:27.380 --> 00:20:29.660]   So what does that do?
[00:20:29.660 --> 00:20:34.700]   It actually optionally takes the output of just the last layer.
[00:20:34.700 --> 00:20:36.340]   Also optionally, it takes the output
[00:20:36.340 --> 00:20:39.140]   of our much more complicated model.
[00:20:39.140 --> 00:20:42.860]   So this is a good way to fit this particular data.
[00:20:42.860 --> 00:20:45.100]   But this video is being filmed in 2019,
[00:20:45.100 --> 00:20:47.260]   and things are going to change over time.
[00:20:47.260 --> 00:20:49.660]   So one of the things that we did was we actually
[00:20:49.660 --> 00:20:51.660]   built what we call a benchmark.
[00:20:51.660 --> 00:20:53.500]   And we'll put a link to this in the comments.
[00:20:53.500 --> 00:20:56.300]   And we let anyone who wanted to modify this code
[00:20:56.300 --> 00:20:58.500]   and model this data.
[00:20:58.500 --> 00:21:01.380]   And you can actually look at all the different people that
[00:21:01.380 --> 00:21:03.840]   have edited this benchmark and see how well they did.
[00:21:03.840 --> 00:21:06.500]   So you can sort it by a vowel perceptual distance.
[00:21:06.500 --> 00:21:09.140]   And you can see that the different models
[00:21:09.140 --> 00:21:11.580]   of the people that actually were able to model this particular
[00:21:11.580 --> 00:21:12.860]   cat data the best.
[00:21:12.860 --> 00:21:15.660]   And we can zoom in, and we can look at the models
[00:21:15.660 --> 00:21:17.220]   that they built. For example, this
[00:21:17.220 --> 00:21:19.780]   is one of the good ones that we see right now.
[00:21:19.780 --> 00:21:21.940]   And we can zoom in, and we can see
[00:21:21.940 --> 00:21:24.300]   that this person is actually using
[00:21:24.300 --> 00:21:26.060]   three different strategies in parallel
[00:21:26.060 --> 00:21:28.520]   and then concatenating them and then combining them
[00:21:28.520 --> 00:21:29.920]   with the conv 2D layer.
[00:21:29.920 --> 00:21:31.960]   So one of the awesome things about this benchmark
[00:21:31.960 --> 00:21:33.620]   is that you can actually contribute too.
[00:21:33.620 --> 00:21:36.140]   So I would love it if you took this notebook,
[00:21:36.140 --> 00:21:37.940]   modified the structure of the model,
[00:21:37.940 --> 00:21:40.020]   maybe find something that works a little bit better
[00:21:40.020 --> 00:21:43.220]   than what I have here, and then submit it back to the benchmark
[00:21:43.220 --> 00:21:46.100]   so other people can see what you did and expand on it.
[00:21:46.100 --> 00:21:48.020]   Today we're going to talk about applying
[00:21:48.020 --> 00:21:51.820]   convolutional neural networks and LSTMs to videos.
[00:21:51.820 --> 00:21:53.460]   And we're going to use this technique
[00:21:53.460 --> 00:21:58.060]   to generate all the rest of the videos going forward.
[00:21:58.060 --> 00:22:01.100]   Sorry, that's so dumb.
[00:22:01.100 --> 00:22:02.940]   Not at all.

