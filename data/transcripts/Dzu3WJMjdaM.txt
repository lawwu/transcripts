
[00:00:00.000 --> 00:00:06.520]   We had presented to OpenAI to a group of researchers.
[00:00:06.520 --> 00:00:13.400]   I remember I was presenting and half the audience was just looking at their laptops and left
[00:00:13.400 --> 00:00:16.840]   that meeting feeling like, "Oh, no one cares."
[00:00:16.840 --> 00:00:24.640]   Then a week later, Raj calls us up and he says, "Hey, we've got this problem on the
[00:00:24.640 --> 00:00:26.240]   robotics team.
[00:00:26.240 --> 00:00:27.240]   Come check this out.
[00:00:27.240 --> 00:00:28.880]   Can you help us out?"
[00:00:28.880 --> 00:00:33.400]   We go look and I remember going with Sean and both Sean and I are looking at it and we're
[00:00:33.400 --> 00:00:37.680]   excited because this is like, we can solve the problem and they're telling us they have
[00:00:37.680 --> 00:00:39.760]   a problem and they want us to fix it.
[00:00:39.760 --> 00:00:44.140]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:44.140 --> 00:00:46.880]   and I'm your host, Lukas Biewald.
[00:00:46.880 --> 00:00:54.520]   This episode is a fun departure from our normal format where I interview two people in the
[00:00:54.520 --> 00:01:00.040]   machine learning space about their work, but these two people happen to be my co-founders
[00:01:00.040 --> 00:01:02.320]   of Weights & Biases.
[00:01:02.320 --> 00:01:09.280]   Sean Lewis, our CTO, and Chris Van Pelt, our corporate vice president.
[00:01:09.280 --> 00:01:14.920]   I talked to them about questions that have been stewing inside of me for years, like,
[00:01:14.920 --> 00:01:18.040]   why did we start this company and where is it going?
[00:01:18.040 --> 00:01:22.120]   Honestly, I was surprised and educated by their answers.
[00:01:22.120 --> 00:01:27.760]   I hope that you enjoy listening to this episode as much as I enjoyed asking them these hard
[00:01:27.760 --> 00:01:28.760]   questions.
[00:01:28.760 --> 00:01:30.760]   All right, here we go.
[00:01:30.760 --> 00:01:33.280]   Should we just jump right in?
[00:01:33.280 --> 00:01:35.000]   How did this company start?
[00:01:35.000 --> 00:01:39.380]   And I ask this because it's the most common question I get asked if I go on any other
[00:01:39.380 --> 00:01:41.800]   podcast or with any candidate.
[00:01:41.800 --> 00:01:43.520]   Everyone wants to know how did you start the company?
[00:01:43.520 --> 00:01:49.360]   So I was realizing you two both have founding stories, it's telling the same story, but
[00:01:49.360 --> 00:01:53.200]   it may have diverged in its evolution.
[00:01:53.200 --> 00:01:58.320]   So I would be curious to hear you two's version of the story.
[00:01:58.320 --> 00:02:00.400]   I guess which of you wants to go first?
[00:02:00.400 --> 00:02:02.720]   Sean, I think you should go first.
[00:02:02.720 --> 00:02:04.280]   All right.
[00:02:04.280 --> 00:02:09.400]   When I started my career as a software engineer, I was at Google and I was in the platforms
[00:02:09.400 --> 00:02:10.400]   team at Google.
[00:02:10.400 --> 00:02:16.400]   So I joined, this is back in 2006, and I worked on all kinds of stuff in the platforms team.
[00:02:16.400 --> 00:02:21.160]   The platforms team was responsible for building the data centers and all the machines inside
[00:02:21.160 --> 00:02:23.880]   the data centers that Google uses.
[00:02:23.880 --> 00:02:27.840]   And I wrote a lot of the software that ran in those environments.
[00:02:27.840 --> 00:02:31.800]   And when you write tests like that, they generate a ton of data.
[00:02:31.800 --> 00:02:36.600]   So at first I was writing tests, but then I ended up with just all of this data.
[00:02:36.600 --> 00:02:41.800]   And really where I ended up spending my time was on making tools and data pipelines that
[00:02:41.800 --> 00:02:43.960]   would help us understand that data.
[00:02:43.960 --> 00:02:47.620]   And that's what I realized that I loved doing.
[00:02:47.620 --> 00:02:51.720]   So I ended up writing all kinds of different tools and data pipelines, and the data's in
[00:02:51.720 --> 00:02:54.360]   all these different databases all over Google.
[00:02:54.360 --> 00:02:58.440]   And I merged it into one place and defined the metrics that we use to understand hard
[00:02:58.440 --> 00:03:01.440]   drives and then made these user-facing tools.
[00:03:01.440 --> 00:03:06.760]   And when I say user, I mean other people at Google that they could use to dig in and understand
[00:03:06.760 --> 00:03:07.760]   this data.
[00:03:07.760 --> 00:03:09.640]   So I worked on a lot of stuff like that at Google.
[00:03:09.640 --> 00:03:11.360]   It was super fun.
[00:03:11.360 --> 00:03:14.200]   And then I eventually left, and then I started this company called Beep.
[00:03:14.200 --> 00:03:16.280]   We built hardware.
[00:03:16.280 --> 00:03:19.560]   We built a lot of things, and it's kind of tangential to the story, so I won't go into
[00:03:19.560 --> 00:03:20.560]   it.
[00:03:20.560 --> 00:03:21.720]   We were also in Y Combinator.
[00:03:21.720 --> 00:03:27.600]   And in that Y Combinator batch was actually Lucas's wife, Noga, who I'm also not sure
[00:03:27.600 --> 00:03:30.920]   if folks know this, but she's the founder of a company called Picnic Health.
[00:03:30.920 --> 00:03:32.920]   And so we were in a Y Combinator batch.
[00:03:32.920 --> 00:03:36.900]   We ended up sharing an office together in the Mission in San Francisco for a couple
[00:03:36.900 --> 00:03:38.040]   of years.
[00:03:38.040 --> 00:03:39.760]   And I got to know Lucas that way.
[00:03:39.760 --> 00:03:41.960]   So we had a hardware component at Beep.
[00:03:41.960 --> 00:03:45.520]   We had this hardware lab, and Lucas loves robots, and he was kind of always tinkering
[00:03:45.520 --> 00:03:47.360]   in his garage building robots.
[00:03:47.360 --> 00:03:51.680]   And he would come up to my hardware lab and poke his head through the door and go, "What
[00:03:51.680 --> 00:03:53.320]   are you guys doing in here?"
[00:03:53.320 --> 00:03:55.640]   And also, "My robot's broken."
[00:03:55.640 --> 00:03:59.400]   And we'd just start to get to know each other and work on the robot and help him fix it.
[00:03:59.400 --> 00:04:02.480]   So we became good friends that way.
[00:04:02.480 --> 00:04:08.960]   And I guess as Beep won down, I think, I don't know, that was like 2016, I think this is
[00:04:08.960 --> 00:04:09.960]   my version of the story.
[00:04:09.960 --> 00:04:14.520]   Chris and Lucas were sort of starting to step out of figure eight around the same time.
[00:04:14.520 --> 00:04:16.280]   And the circumstances were really good.
[00:04:16.280 --> 00:04:17.680]   Deep learning was starting to take off.
[00:04:17.680 --> 00:04:20.160]   We were all really excited about it.
[00:04:20.160 --> 00:04:22.360]   I had been thinking about these problems a lot.
[00:04:22.360 --> 00:04:25.760]   I really would love the chance to work with Lucas and Chris.
[00:04:25.760 --> 00:04:30.840]   And we just sort of started hanging out and talking about it and jumped on that opportunity
[00:04:30.840 --> 00:04:34.000]   and started building the stuff that I needed to organize that data.
[00:04:34.000 --> 00:04:36.800]   Yeah, it was an exciting time and it still is.
[00:04:36.800 --> 00:04:40.680]   All right, Chris, I think you have a different version of the story.
[00:04:40.680 --> 00:04:41.680]   Let's hear it.
[00:04:41.680 --> 00:04:42.680]   All right.
[00:04:42.680 --> 00:04:43.680]   So we got to go back.
[00:04:43.680 --> 00:04:51.800]   We got to go back to 2006, like Sean, except in my 2006, I'm coming to San Francisco for
[00:04:51.800 --> 00:04:55.860]   the first time and it's to work at an exciting startup.
[00:04:55.860 --> 00:04:57.600]   I feel like I've made it.
[00:04:57.600 --> 00:05:03.120]   I've been doing web development and trying to kind of advance my career and happened
[00:05:03.120 --> 00:05:07.800]   to be into Ruby on Rails, which was really hot and exciting at the time.
[00:05:07.800 --> 00:05:15.240]   And there's this startup in San Francisco that was using Ruby on Rails and using machine
[00:05:15.240 --> 00:05:21.680]   learning to create a smarter, hopefully more relevant search engine.
[00:05:21.680 --> 00:05:23.400]   So that startup was PowerSet.
[00:05:23.400 --> 00:05:27.300]   And I came up to San Francisco, it was like the beginning of 2007.
[00:05:27.300 --> 00:05:29.600]   And that's actually where I met Lucas.
[00:05:29.600 --> 00:05:33.600]   So Lucas and I joined PowerSet at roughly the same time.
[00:05:33.600 --> 00:05:38.700]   And I was in the product team interfacing with a whole bunch of the other backend teams
[00:05:38.700 --> 00:05:42.800]   to try to create an interface to this exciting new tool.
[00:05:42.800 --> 00:05:48.960]   Fast forward about a year or so and Luke and I decided, PowerSet, it's been fun, but I
[00:05:48.960 --> 00:05:53.760]   think it's time for us to give our own go at creating a PowerSet or a startup.
[00:05:53.760 --> 00:06:00.360]   So we set out to make crowdsourcing more accessible to the enterprise and people that wanted to
[00:06:00.360 --> 00:06:06.280]   do it, to collect training data, to train machine learning models way before machine
[00:06:06.280 --> 00:06:08.920]   learning was as cool as it is today.
[00:06:08.920 --> 00:06:14.840]   Fast forward about 10 years through kind of creating a startup with Lucas, learning a
[00:06:14.840 --> 00:06:17.160]   lot along the way.
[00:06:17.160 --> 00:06:23.680]   And at the time that we started Weights & Biases, kind of Luke and I's day-to-day responsibilities
[00:06:23.680 --> 00:06:26.840]   at the company were winding down.
[00:06:26.840 --> 00:06:29.720]   So we were asking ourselves, what is next?
[00:06:29.720 --> 00:06:37.520]   I remember going to Lucas's workshop where most of his podcasts are recorded and playing
[00:06:37.520 --> 00:06:38.520]   with robots.
[00:06:38.520 --> 00:06:44.960]   And I was really into Lua for a little while there, thinking we can make a cool Lua toolkit
[00:06:44.960 --> 00:06:47.560]   for robots.
[00:06:47.560 --> 00:06:56.600]   But then it was really Lucas having an internship at OpenAI and actually building models with
[00:06:56.600 --> 00:07:01.960]   some of the world's most renowned machine learning researchers and needing tools to
[00:07:01.960 --> 00:07:04.560]   help him get his job done.
[00:07:04.560 --> 00:07:07.600]   That was the initial kind of itch that we were scratching.
[00:07:07.600 --> 00:07:09.760]   So I love building tools.
[00:07:09.760 --> 00:07:12.640]   Luke's like, "Hey, I need a tool to help me build models at OpenAI."
[00:07:12.640 --> 00:07:16.560]   I said, "Great, let me try to whip something up."
[00:07:16.560 --> 00:07:20.080]   And just really poured myself into making a very early prototype.
[00:07:20.080 --> 00:07:26.320]   And then shortly thereafter, Sean came into the picture and I am forever grateful.
[00:07:26.320 --> 00:07:27.320]   Awesome, man.
[00:07:27.320 --> 00:07:32.520]   Those were, you know, I was getting some messages from Lavanya to pull you both back on the
[00:07:32.520 --> 00:07:33.800]   rails while you're talking.
[00:07:33.800 --> 00:07:38.920]   But you know, I love the extended cut founding stories.
[00:07:38.920 --> 00:07:40.840]   I think mine is like a sentence or two.
[00:07:40.840 --> 00:07:42.920]   Sorry, Lavanya, now she's texting me, what the ****.
[00:07:42.920 --> 00:07:45.360]   But you know, the secrets here.
[00:07:45.360 --> 00:07:47.920]   Actually, that's a good segue into here's another question that I have for you.
[00:07:47.920 --> 00:07:49.960]   So Sean, you've been out on paternity leave.
[00:07:49.960 --> 00:07:54.880]   Chris, you've been talking to lots of customers independently from me.
[00:07:54.880 --> 00:07:56.200]   And so people keep asking me this.
[00:07:56.200 --> 00:08:00.840]   They're like, "What is the architecture of the Weights & Biases server?"
[00:08:00.840 --> 00:08:03.120]   And I try to describe it.
[00:08:03.120 --> 00:08:04.960]   And I realize I honestly have no idea.
[00:08:04.960 --> 00:08:08.520]   I know there's MySQL involved and React.
[00:08:08.520 --> 00:08:11.960]   Can you give me the like several sentences, like lay it out.
[00:08:11.960 --> 00:08:15.200]   Say I'm like an engineering candidate and I just want to know, like, what are we using?
[00:08:15.200 --> 00:08:16.200]   How does it all fit together?
[00:08:16.200 --> 00:08:17.200]   Okay.
[00:08:17.200 --> 00:08:18.200]   All right.
[00:08:18.200 --> 00:08:22.000]   We've got a single page React application that is our front end.
[00:08:22.000 --> 00:08:23.940]   It's just a lot of JavaScript.
[00:08:23.940 --> 00:08:29.600]   We load that up into the browser and then it makes requests against our GraphQL backend,
[00:08:29.600 --> 00:08:31.800]   which happens to be written in Golang.
[00:08:31.800 --> 00:08:37.960]   When a customer wants to run Weights & Biases themselves, we actually deliver all of this,
[00:08:37.960 --> 00:08:43.040]   the single page React app and the GraphQL backend API in a single Docker container that
[00:08:43.040 --> 00:08:49.280]   they could run within their Kubernetes cluster or in a managed kind of Terraform-based deployment
[00:08:49.280 --> 00:08:50.640]   that we support.
[00:08:50.640 --> 00:08:53.640]   And then the backing persistence stores are super simple.
[00:08:53.640 --> 00:09:01.000]   We've got MySQL and an S3 compatible object store or a Zurblob storage or Google Cloud
[00:09:01.000 --> 00:09:02.000]   Storage.
[00:09:02.000 --> 00:09:05.680]   There's a little reticence in there, but customers generally don't have to worry about that.
[00:09:05.680 --> 00:09:11.320]   It's nice because from early on, we knew that having on-prem, the potential to go on-prem
[00:09:11.320 --> 00:09:15.440]   was really important for our customers because of data privacy concerns, because these datasets
[00:09:15.440 --> 00:09:19.880]   are so valuable and have all these sort of other privacy concerns.
[00:09:19.880 --> 00:09:24.000]   So we really just kept it simple to make that possible.
[00:09:24.000 --> 00:09:27.640]   And yeah, those were good early choices.
[00:09:27.640 --> 00:09:31.960]   Sean, you wrote a document, I think at one point, that was like what it would take to
[00:09:31.960 --> 00:09:37.800]   be a billion dollar business for Weights and Biases.
[00:09:37.800 --> 00:09:43.440]   And I thought maybe we could pull it up and compare it to what it actually took to be
[00:09:43.440 --> 00:09:45.720]   a billion dollar market cap business.
[00:09:45.720 --> 00:09:51.120]   Looking at this document, how do you feel that things have played out?
[00:09:51.120 --> 00:09:53.560]   Is anything played out differently than you expected?
[00:09:53.560 --> 00:09:54.560]   Yeah.
[00:09:54.560 --> 00:09:59.600]   So, well, I guess the core of the argument was like, we're not sure if we can build better
[00:09:59.600 --> 00:10:04.440]   products than everybody else in this space, but we can raise a lot of money.
[00:10:04.440 --> 00:10:05.440]   We know we can do that.
[00:10:05.440 --> 00:10:11.560]   And we're well connected to lots of customers because of Lucas and Chris's background with
[00:10:11.560 --> 00:10:12.560]   Figure 8.
[00:10:12.560 --> 00:10:17.960]   So it's very easy for us in the early days to kind of go into a customer with very little
[00:10:17.960 --> 00:10:19.360]   to show.
[00:10:19.360 --> 00:10:23.480]   We had a demo of the early parts of the product and have good conversations.
[00:10:23.480 --> 00:10:27.400]   And that's really what it takes to develop good products, is to actually interact with
[00:10:27.400 --> 00:10:33.760]   customers who look at the product and give you feedback on either a demo or by actually
[00:10:33.760 --> 00:10:35.040]   using the product.
[00:10:35.040 --> 00:10:42.960]   So the argument was we should rapidly expand into sort of the different parts of the ML
[00:10:42.960 --> 00:10:48.680]   pipeline in parallel and leverage those connections and the ability to raise money.
[00:10:48.680 --> 00:10:51.640]   We could build a team that could sort of build products in each of these spaces.
[00:10:51.640 --> 00:10:53.760]   I would say we didn't quite do that.
[00:10:53.760 --> 00:10:58.840]   I think we still have this goal of expanding across the ML pipeline.
[00:10:58.840 --> 00:11:04.080]   And this early sort of theory that maybe we're not better, we may not be great at building
[00:11:04.080 --> 00:11:08.720]   products, I would say maybe this is maybe not so humble of a statement, but I think
[00:11:08.720 --> 00:11:10.720]   we built something that users really love.
[00:11:10.720 --> 00:11:15.680]   And we definitely did it by hiring great engineers and great product people and by talking to
[00:11:15.680 --> 00:11:19.520]   these customers a ton and spending lots and lots of time and just having this relentless
[00:11:19.520 --> 00:11:20.520]   customer focus.
[00:11:20.520 --> 00:11:25.680]   But I also think that somehow at core, there's kind of this magic somewhere in what we're
[00:11:25.680 --> 00:11:31.640]   doing at WMB and that we do understand the space and the customers and we turn that money
[00:11:31.640 --> 00:11:34.040]   and customer connection into great products.
[00:11:34.040 --> 00:11:38.160]   And so back then I was thinking, well, we have great products and looking back now,
[00:11:38.160 --> 00:11:39.240]   I feel like we really do.
[00:11:39.240 --> 00:11:41.440]   And that's really been a cool journey.
[00:11:41.440 --> 00:11:42.440]   All right.
[00:11:42.440 --> 00:11:44.440]   I have a question for both of you.
[00:11:44.440 --> 00:11:48.800]   I was kind of wondering about, was there a moment where you felt like the building, the
[00:11:48.800 --> 00:11:51.640]   business was really working or the company is really working?
[00:11:51.640 --> 00:11:58.120]   Like, can you think of a time when you really suddenly felt like that or not?
[00:11:58.120 --> 00:11:59.840]   And if so, what time was it?
[00:11:59.840 --> 00:12:06.540]   I think there's a couple of times, but one that really stands out for me is driving down
[00:12:06.540 --> 00:12:11.040]   to like Palo Alto or Mountain View or wherever we were meeting.
[00:12:11.040 --> 00:12:15.640]   And one of our first deals was with Toyota Research Institute.
[00:12:15.640 --> 00:12:22.080]   And I remember kind of sitting out, I grabbed lunch with Ari, our kind of first account
[00:12:22.080 --> 00:12:23.800]   executive, grabbed lunch.
[00:12:23.800 --> 00:12:28.500]   We knew we were going to go into this meeting and present the number that we were going
[00:12:28.500 --> 00:12:30.960]   to sell our software for.
[00:12:30.960 --> 00:12:36.520]   And I had this thought of like, this moment is really important.
[00:12:36.520 --> 00:12:41.480]   Like, it's scary when you're brand new, you don't have many other customers and you go
[00:12:41.480 --> 00:12:45.040]   and you say, "Hey, we want to charge this much for our software."
[00:12:45.040 --> 00:12:48.240]   I felt like this is make or break.
[00:12:48.240 --> 00:12:53.880]   And we went in, the meeting went really well, and we ended up closing TRI as one of our
[00:12:53.880 --> 00:12:55.680]   first customers, which is great.
[00:12:55.680 --> 00:12:59.240]   But after that, I mean, I wasn't like, "Okay, now we're set."
[00:12:59.240 --> 00:13:03.280]   It's like just, you know, next week we can get to a billion dollar valuation.
[00:13:03.280 --> 00:13:07.880]   But it was that first customer was really big.
[00:13:07.880 --> 00:13:09.080]   Yeah, that was a big one.
[00:13:09.080 --> 00:13:14.000]   I mean, that's the thing as a founder, or probably anybody at an early stage startup,
[00:13:14.000 --> 00:13:17.000]   you're kind of, there's always, you do the thing, you say, "Oh, if we can just do this
[00:13:17.000 --> 00:13:19.240]   one thing, then we'll be sure that we made it."
[00:13:19.240 --> 00:13:24.640]   And then like the next week, you're back to work and you have to grow some more, and you're
[00:13:24.640 --> 00:13:27.040]   always looking at the next thing.
[00:13:27.040 --> 00:13:30.800]   But that first customer was a great one.
[00:13:30.800 --> 00:13:36.320]   I think for me, what comes to mind is, I think this was maybe like around the holidays last
[00:13:36.320 --> 00:13:37.960]   two years ago.
[00:13:37.960 --> 00:13:43.640]   And so, you know, in the earlier stages, even up to say like 15, 20 people in the company,
[00:13:43.640 --> 00:13:46.980]   you kind of have a pretty good sense of everything that's happening.
[00:13:46.980 --> 00:13:50.300]   Every deal I remember, you know, being a part of in some form.
[00:13:50.300 --> 00:13:53.760]   But there was a moment around the holidays a couple of years ago where we have these
[00:13:53.760 --> 00:13:57.320]   Friday meetings where we get together and sort of talk about how the week went and so
[00:13:57.320 --> 00:14:01.000]   everybody at the company and somebody says something great that they did.
[00:14:01.000 --> 00:14:05.000]   In that particular meeting, I just remember, you know, there was like somebody on the sales
[00:14:05.000 --> 00:14:08.200]   team who had made another sale to a customer I'd never talked to.
[00:14:08.200 --> 00:14:12.440]   And somebody in the growth team that had like found a new growth experiment to do and executed
[00:14:12.440 --> 00:14:14.840]   it and actually like made numbers change.
[00:14:14.840 --> 00:14:18.240]   And somebody in the product team that had done something that I didn't even know they
[00:14:18.240 --> 00:14:19.340]   were doing.
[00:14:19.340 --> 00:14:22.360]   And so all of those things came together in one meeting for me.
[00:14:22.360 --> 00:14:26.640]   And that's when I felt like, wow, this company is a lot more than like sort of I can't wrap
[00:14:26.640 --> 00:14:30.320]   my arms around it and push everything forward anymore.
[00:14:30.320 --> 00:14:34.680]   There's like all these great people around me that are doing that.
[00:14:34.680 --> 00:14:41.040]   And that is an amazing feeling because like, you know, from there, you sort of add more
[00:14:41.040 --> 00:14:44.600]   and more great people and the company continues to go in a good direction.
[00:14:44.600 --> 00:14:50.160]   And yeah, it's like bigger than yourself.
[00:14:50.160 --> 00:14:55.560]   Another question I had for both of you is, is there like a favorite feature in the product
[00:14:55.560 --> 00:15:01.560]   that you feel really proud of or something, some way that the product works that you think
[00:15:01.560 --> 00:15:03.320]   you feel like is uniquely great?
[00:15:03.320 --> 00:15:05.200]   For me, it's got to be the command line interface.
[00:15:05.200 --> 00:15:09.600]   I think it's a very underappreciated interface to our product.
[00:15:09.600 --> 00:15:15.560]   In the early days, I spent an obsessive amount of time on making a whole bunch of command
[00:15:15.560 --> 00:15:18.800]   line commands and making it work nicely in Unix.
[00:15:18.800 --> 00:15:22.640]   I was like piping stuff at one point.
[00:15:22.640 --> 00:15:26.080]   I think we've since decided that's not the way most people want to interact with our
[00:15:26.080 --> 00:15:30.920]   product, but it's as a nerd, as a Unix nerd, that's my favorite part for sure.
[00:15:30.920 --> 00:15:36.280]   I mean, there's a lot of great parts of the product that I'm proud of that everybody at
[00:15:36.280 --> 00:15:37.280]   the company have built.
[00:15:37.280 --> 00:15:41.960]   I guess maybe something that people don't really see is there's this layer in the front
[00:15:41.960 --> 00:15:43.840]   end that we made.
[00:15:43.840 --> 00:15:47.720]   And so in WMB, you've got all these different charts on the screen.
[00:15:47.720 --> 00:15:51.880]   And the architecture of the front end is that each chart can individually make its own network
[00:15:51.880 --> 00:15:54.480]   request to get the data that it needs to show.
[00:15:54.480 --> 00:15:57.240]   And those are actually kind of heavy requests because there's like millions of data points
[00:15:57.240 --> 00:15:59.760]   sometimes if you've logged a lot of data.
[00:15:59.760 --> 00:16:03.120]   So you've got this cool layer in the front end that kind of, it's like a middleware that
[00:16:03.120 --> 00:16:07.120]   watches all of the requests going out and it aggregates them all together.
[00:16:07.120 --> 00:16:09.800]   So it has a little time delay and it says, give me all the requests that happened in
[00:16:09.800 --> 00:16:11.760]   the last hundred milliseconds.
[00:16:11.760 --> 00:16:16.760]   It does lots of cool like handwritten optimizations to figure out how to merge certain kinds of
[00:16:16.760 --> 00:16:21.120]   queries together and get all the results at once in a single request, and then give that
[00:16:21.120 --> 00:16:22.300]   back to the user.
[00:16:22.300 --> 00:16:26.000]   And so a lot of users, you have no idea that's going on, but this is, you know, you have
[00:16:26.000 --> 00:16:29.080]   to, when you're building UIs, you really want it to be snappy and fast.
[00:16:29.080 --> 00:16:31.400]   And I hope that I'm not like shooting myself in the foot.
[00:16:31.400 --> 00:16:34.960]   I'm sure somebody will have a story of Weights and Biases not being snappy and fast.
[00:16:34.960 --> 00:16:38.680]   And if you do, send it my way and we'll get it fixed.
[00:16:38.680 --> 00:16:41.840]   But you know, there's like this massive amount of engineering effort that went into that
[00:16:41.840 --> 00:16:46.040]   chunk of code to make sure that charts with like millions of data points all on the screen
[00:16:46.040 --> 00:16:49.200]   at once can be updated really quickly.
[00:16:49.200 --> 00:16:52.720]   I guess just so people don't think I'm only asking like kind of softball questions here,
[00:16:52.720 --> 00:16:55.920]   there's something that a couple of candidates have asked me about recently.
[00:16:55.920 --> 00:17:01.680]   Have there been like product or engineering efforts that we would do a lot differently
[00:17:01.680 --> 00:17:02.680]   in hindsight?
[00:17:02.680 --> 00:17:07.280]   I mean, I've got one that isn't bad, so it's not fair.
[00:17:07.280 --> 00:17:13.160]   When we first started the company, I wrote the backend in Python and it was Python 2
[00:17:13.160 --> 00:17:18.160]   because I wanted to use Google App Engine to serve things up so I didn't have to do
[00:17:18.160 --> 00:17:21.040]   a bunch of DevOps stuff.
[00:17:21.040 --> 00:17:24.280]   That quickly stopped scaling, especially because we have a GraphQL backend where things need
[00:17:24.280 --> 00:17:26.960]   to happen in parallel.
[00:17:26.960 --> 00:17:33.400]   So our first engineer, Tom, actually rewrote that entire backend in Golang.
[00:17:33.400 --> 00:17:38.080]   And when you do a big rewrite like that, like we already had TRI as a customer, OpenAI was
[00:17:38.080 --> 00:17:39.280]   a heavy user.
[00:17:39.280 --> 00:17:41.880]   We had to like keep the site up while this was happening.
[00:17:41.880 --> 00:17:45.820]   And usually those kinds of exercises can go sideways.
[00:17:45.820 --> 00:17:49.960]   You can start to take way longer than you would have anticipated or the ultimate project
[00:17:49.960 --> 00:17:50.960]   wouldn't have worked out.
[00:17:50.960 --> 00:17:56.260]   And that was an example where Tom actually got the project done ahead of time and we're
[00:17:56.260 --> 00:18:01.600]   still running pretty much the same backend code to scale up to the tens of thousands
[00:18:01.600 --> 00:18:04.280]   of users that we have every day now.
[00:18:04.280 --> 00:18:09.280]   So it's, I guess, not a decision I would go back and say not do.
[00:18:09.280 --> 00:18:13.880]   I also, now that I had a good experience doing a rewrite, I certainly wouldn't say we always
[00:18:13.880 --> 00:18:17.600]   do a rewrite, but you don't hear it often.
[00:18:17.600 --> 00:18:20.440]   Our choice to do the rewrite was definitely the right choice and it worked out really
[00:18:20.440 --> 00:18:21.640]   well.
[00:18:21.640 --> 00:18:25.400]   What customer feedback has surprised you the most?
[00:18:25.400 --> 00:18:31.660]   I guess I have an example like from early on when we started the company, which was
[00:18:31.660 --> 00:18:36.920]   the very first version of Weights and Biases was more of a command line tool around saving
[00:18:36.920 --> 00:18:38.220]   data.
[00:18:38.220 --> 00:18:43.140]   But there was a little bit of UI and Chris had built this Python library and this UI.
[00:18:43.140 --> 00:18:48.500]   And the UI was essentially what it did was it'll let you log in and give, like set up
[00:18:48.500 --> 00:18:51.380]   a place to store the data that the command line tool was saving.
[00:18:51.380 --> 00:18:54.180]   But it didn't do a whole lot more than that.
[00:18:54.180 --> 00:18:58.460]   And I remember Chris added a feature that would just kind of, it also collected the
[00:18:58.460 --> 00:19:01.300]   outputs of your training runs, like the standard out, the logs.
[00:19:01.300 --> 00:19:06.720]   And Chris added a feature that would look for the Keras, the specific Keras metrics
[00:19:06.720 --> 00:19:07.860]   that it printed over time.
[00:19:07.860 --> 00:19:10.220]   And he just made a line chart of that.
[00:19:10.220 --> 00:19:14.020]   And of course, early on in the startup, what you do is you have a demo or a thing that
[00:19:14.020 --> 00:19:16.420]   kind of works, you go to customers and you show it to them.
[00:19:16.420 --> 00:19:19.540]   And all the customers were like, yeah, command line tool, okay, okay.
[00:19:19.540 --> 00:19:21.460]   But what's that chart?
[00:19:21.460 --> 00:19:23.140]   And they would really focus on that chart.
[00:19:23.140 --> 00:19:27.020]   And the reason it's surprising to me is because these are programmers and data scientists
[00:19:27.020 --> 00:19:31.340]   and people who are really comfortable in like Matplotlib and Jupyter notebooks.
[00:19:31.340 --> 00:19:36.260]   And so, you know, sort of a thesis you might have is, well, data scientists don't need
[00:19:36.260 --> 00:19:39.300]   like a tool to create a bunch of charts in their browser because their use cases are
[00:19:39.300 --> 00:19:42.180]   going to be so different and they're just comfortable doing it themselves.
[00:19:42.180 --> 00:19:46.860]   So it was really a big surprise to me that that was the main thing people focused on.
[00:19:46.860 --> 00:19:50.540]   And so we saw that and we said, well, let's follow what users want.
[00:19:50.540 --> 00:19:51.660]   We did that.
[00:19:51.660 --> 00:19:53.940]   And we kept building the UI and making it better and better.
[00:19:53.940 --> 00:19:58.860]   And we were able now to have a sort of generic UI that solves lots of different kinds of
[00:19:58.860 --> 00:19:59.860]   use cases.
[00:19:59.860 --> 00:20:03.340]   But it was surprising that that would be possible for this kind of user.
[00:20:03.340 --> 00:20:04.860]   That's a good founding story right there.
[00:20:04.860 --> 00:20:07.500]   Chris, what about you?
[00:20:07.500 --> 00:20:16.700]   The most notable user feedback that is top of mind is an early user, Hamo at GitHub,
[00:20:16.700 --> 00:20:18.140]   was like a heavy user of the tool.
[00:20:18.500 --> 00:20:23.860]   I remember one night Hamo wrote in and said, hey, we really want to log HTML.
[00:20:23.860 --> 00:20:32.700]   And we were actually able to ship that feature in that same night, which was delighting to
[00:20:32.700 --> 00:20:33.980]   Hamo.
[00:20:33.980 --> 00:20:41.220]   But Hamo was also like, he did not hold back in telling us where we were not being excellent
[00:20:41.220 --> 00:20:48.220]   in the UI and was very honest about some pretty serious issues with the system at the time.
[00:20:48.220 --> 00:20:50.660]   I remember breaking my heart as a founder.
[00:20:50.660 --> 00:20:56.540]   Here, I've got someone who's engaged and excited about us, but he's getting frustrated by using
[00:20:56.540 --> 00:20:57.540]   our tool.
[00:20:57.540 --> 00:21:01.780]   It's the worst possible thing I could imagine.
[00:21:01.780 --> 00:21:10.220]   So the team really focused and did a lot of hard work to kind of redesign and re-engineer
[00:21:10.220 --> 00:21:13.980]   a lot of the problem interfaces that Hamo was running into.
[00:21:13.980 --> 00:21:17.860]   And ultimately, I think it really helped us make a better product.
[00:21:17.860 --> 00:21:23.820]   So how do you think the last four and a half years of running this kind of hyper growth
[00:21:23.820 --> 00:21:27.700]   startup has changed you as a person or changed your perspective on the world?
[00:21:27.700 --> 00:21:32.820]   I remember when we first started the company, after Luke and I had been working at Crowdflower
[00:21:32.820 --> 00:21:39.180]   for 10 plus years, and I was just so excited to have a blank canvas.
[00:21:39.180 --> 00:21:41.340]   It was like, "Oh, we can start fresh."
[00:21:41.340 --> 00:21:44.740]   There's nothing legacy we need to support.
[00:21:44.740 --> 00:21:48.500]   It's just green fields.
[00:21:48.500 --> 00:21:56.540]   I tend to think of all the company stuff, the process and the management and all of
[00:21:56.540 --> 00:22:00.580]   the things that you need to do to make a company work.
[00:22:00.580 --> 00:22:03.940]   Historically, they didn't interest me that much.
[00:22:03.940 --> 00:22:08.620]   And I think something has changed, especially with this company, is that now I find those
[00:22:08.620 --> 00:22:12.860]   things more interesting.
[00:22:12.860 --> 00:22:16.980]   Being able to kind of step away from just hacking all the time and actually think about,
[00:22:16.980 --> 00:22:24.320]   "Okay, how do we build a culture and how do we kind of mentor and work with the team to
[00:22:24.320 --> 00:22:27.500]   ultimately build a better product?"
[00:22:27.500 --> 00:22:32.580]   I think those problems are much more interesting for me this time around than they were when
[00:22:32.580 --> 00:22:35.140]   we were running Crowdflower.
[00:22:35.140 --> 00:22:40.220]   I guess another question I had is, what do you think has changed around us as we've been
[00:22:40.220 --> 00:22:42.660]   running this company?
[00:22:42.660 --> 00:22:45.900]   Do you feel like customers are different now?
[00:22:45.900 --> 00:22:49.280]   Do you feel like the industry is different at all?
[00:22:49.280 --> 00:22:59.860]   We started the company a year or two within when AlexNet was first trained or when AlexNet
[00:22:59.860 --> 00:23:02.980]   actually showed real results.
[00:23:02.980 --> 00:23:04.700]   We really focused on deep learning.
[00:23:04.700 --> 00:23:06.900]   One of our first customers was OpenAI.
[00:23:06.900 --> 00:23:07.900]   That's well known.
[00:23:07.900 --> 00:23:09.540]   They're still a great customer of ours.
[00:23:09.540 --> 00:23:17.900]   We spent a lot of time building things that were sort of tailored to OpenAI use cases.
[00:23:17.900 --> 00:23:22.140]   When you start a company, it's good to make a bet on what you think a growing market will
[00:23:22.140 --> 00:23:26.640]   be because you can do this, but you don't necessarily want to go into a big established
[00:23:26.640 --> 00:23:30.100]   market and just fight with Google and Amazon.
[00:23:30.100 --> 00:23:34.180]   It's better to focus maybe on something that's smaller and they won't spend all their resources
[00:23:34.180 --> 00:23:35.180]   fighting you on.
[00:23:35.180 --> 00:23:39.340]   Then the market sort of grows to the point along with you where all of a sudden you're
[00:23:39.340 --> 00:23:42.220]   this billion-dollar company.
[00:23:42.220 --> 00:23:44.940]   You can't just do that.
[00:23:44.940 --> 00:23:46.500]   There's some amount of luck for sure.
[00:23:46.500 --> 00:23:51.580]   We had very good timing in starting Weights & Biases.
[00:23:51.580 --> 00:23:55.020]   That's a really cool feeling and it's really cool to ride that trend.
[00:23:55.020 --> 00:23:59.640]   In doing that, what we've seen is deep learning really took off.
[00:23:59.640 --> 00:24:01.380]   It's applied in every vertical now.
[00:24:01.380 --> 00:24:06.980]   Every company has at least a few people who are building deep learning models now.
[00:24:06.980 --> 00:24:11.320]   Those teams are constantly growing and we see that in the way that our contracts grow
[00:24:11.320 --> 00:24:14.520]   with our customers.
[00:24:14.520 --> 00:24:18.480]   I guess we sort of bet that that would happen, but to see it actually happen and to be able
[00:24:18.480 --> 00:24:22.400]   to ride that trend, there's no way to really feel what exponential growth is until you're
[00:24:22.400 --> 00:24:25.240]   in the middle of it and that's what it feels like.
[00:24:25.240 --> 00:24:31.240]   I remember one moment early in the company that stands out, which was we had presented
[00:24:31.240 --> 00:24:35.280]   to OpenAI to a group of researchers.
[00:24:35.280 --> 00:24:41.800]   I remember I was presenting and half the audience was just looking at their laptops and left
[00:24:41.800 --> 00:24:45.200]   that meeting feeling like, "Oh, no one cares."
[00:24:45.200 --> 00:24:53.040]   Then a week later, Waj calls us up and he says, "Hey, we've got this problem on the
[00:24:53.040 --> 00:24:54.600]   robotics team.
[00:24:54.600 --> 00:24:55.600]   Come check this out.
[00:24:55.600 --> 00:24:57.240]   Can you help us out?"
[00:24:57.240 --> 00:25:01.720]   We go look and I remember going with Sean and both Sean and I are looking at it and
[00:25:01.720 --> 00:25:06.400]   we're excited because we can solve the problem and they're telling us they have a problem
[00:25:06.400 --> 00:25:08.320]   and they want us to fix it.
[00:25:08.320 --> 00:25:15.300]   After that, Sean and I, we pulled an all-nighter basically, just cranking out the interface
[00:25:15.300 --> 00:25:20.280]   that they wanted and got it to them within a couple of days.
[00:25:20.280 --> 00:25:24.560]   I remember thinking, "How precious is this relationship with OpenAI, this institution
[00:25:24.560 --> 00:25:27.440]   that I really, really admire?"
[00:25:27.440 --> 00:25:32.000]   Also that same feeling Sean described of some users saying, "Hey, I have this problem,"
[00:25:32.000 --> 00:25:36.440]   and we had the power to go back and actually fix that problem for them.
[00:25:36.440 --> 00:25:40.080]   Do you remember the afternoon when they turned on Wait's Advices?
[00:25:40.080 --> 00:25:41.720]   That was another all-nighter.
[00:25:41.720 --> 00:25:46.960]   Yeah, it turns out there was some performance problems with my Python back-end, if I'm recalling.
[00:25:46.960 --> 00:25:50.040]   Well, there were a couple of things.
[00:25:50.040 --> 00:25:54.400]   We did not anticipate OpenAI's scale because we're doing the thing that you do as a startup,
[00:25:54.400 --> 00:25:56.120]   which is you make an MVP.
[00:25:56.120 --> 00:26:00.560]   It doesn't really need to scale, but it turns out our very first customer was one of the
[00:26:00.560 --> 00:26:06.040]   largest scale customers we could have.
[00:26:06.040 --> 00:26:10.600]   We finally convinced they were the first person who integrated Weights and Biases into this
[00:26:10.600 --> 00:26:14.680]   library that they had that everybody on their robotics team was using to run training code.
[00:26:14.680 --> 00:26:19.720]   As soon as they committed that to production, it started sending us a ton of traffic.
[00:26:19.720 --> 00:26:24.720]   The site just immediately went down because it's this cobbled-together startup website.
[00:26:24.720 --> 00:26:29.760]   Actually, the first problem was there was some API limit that we hit on Google because
[00:26:29.760 --> 00:26:32.680]   of the way we were making a specific request.
[00:26:32.680 --> 00:26:35.040]   Chris might remember what it was.
[00:26:35.040 --> 00:26:36.440]   There was no resolution.
[00:26:36.440 --> 00:26:39.720]   You can't just call Google and get them to immediately change a limit for you.
[00:26:39.720 --> 00:26:43.400]   You actually have to wait for a support case to go through for a number of hours.
[00:26:43.400 --> 00:26:48.240]   Of course, now maybe we're a big enough customer of Google that we could have some influence.
[00:26:48.240 --> 00:26:50.000]   But back then, we were just this little startup.
[00:26:50.000 --> 00:26:52.080]   We can't go, "But it's our first customer."
[00:26:52.080 --> 00:26:55.680]   That doesn't really sway anyone over there.
[00:26:55.680 --> 00:26:57.360]   I don't remember how we worked around that.
[00:26:57.360 --> 00:26:58.360]   Do you remember?
[00:26:58.360 --> 00:26:59.720]   Did we just wait?
[00:26:59.720 --> 00:27:09.560]   We very smartly designed the Python library to back off when things started failing.
[00:27:09.560 --> 00:27:15.920]   I think the quota resolution got resolved within the retry timeout.
[00:27:15.920 --> 00:27:18.280]   That was one problem.
[00:27:18.280 --> 00:27:21.520]   Later in that evening, of course, we're pumped because we have all this data coming.
[00:27:21.520 --> 00:27:22.520]   It's opening.
[00:27:22.520 --> 00:27:24.840]   It's our first customer.
[00:27:24.840 --> 00:27:26.440]   Some other problem cropped up.
[00:27:27.000 --> 00:27:31.040]   You might remember this, but it was something where Chris and I were up until 5 a.m. that
[00:27:31.040 --> 00:27:32.040]   night.
[00:27:32.040 --> 00:27:35.120]   Chris was live-patching our App Engine code.
[00:27:35.120 --> 00:27:39.480]   It was Google App Engine at the time, which is this Python auto-scaling platform that's
[00:27:39.480 --> 00:27:42.480]   not used as much now.
[00:27:42.480 --> 00:27:44.960]   I remember we came up with a plan, and we live-patched the thing.
[00:27:44.960 --> 00:27:47.560]   I was like, "Is this going to work?"
[00:27:47.560 --> 00:27:49.360]   It did, and the traffic started coming in clean.
[00:27:49.360 --> 00:27:50.360]   We could see all the data.
[00:27:50.360 --> 00:27:52.640]   We were so proud to have our first customer.
[00:27:52.640 --> 00:27:56.760]   Then, of course, the next morning, we went to talk to OpenAI, and they didn't even notice
[00:27:56.760 --> 00:27:57.760]   the hiccup.
[00:27:57.760 --> 00:27:58.760]   They were like, "Oh, yeah.
[00:27:58.760 --> 00:27:59.760]   Cool.
[00:27:59.760 --> 00:28:00.760]   Thanks."
[00:28:00.760 --> 00:28:03.760]   But really, it was working when we went to have that conversation.
[00:28:03.760 --> 00:28:07.920]   They started looking at the charts that we had and started giving us that feedback.
[00:28:07.920 --> 00:28:10.960]   That's when we got into the feedback cycle.
[00:28:10.960 --> 00:28:11.960]   It's important.
[00:28:11.960 --> 00:28:16.120]   In the early days, if you have any customer at all, and they have a problem, stay up all
[00:28:16.120 --> 00:28:18.080]   night and solve their problem.
[00:28:18.080 --> 00:28:22.720]   Even if they don't notice it, it's worth it for you to start getting that great customer
[00:28:22.720 --> 00:28:24.720]   feedback.
[00:28:24.720 --> 00:28:26.920]   What were your darkest moments, specifically?
[00:28:26.920 --> 00:28:33.140]   I think early on, it was me and Ari.
[00:28:33.140 --> 00:28:37.560]   That was the sales team.
[00:28:37.560 --> 00:28:44.560]   This was before the pandemic, so we would fly wherever we needed to fly to.
[00:28:44.560 --> 00:28:49.680]   Some might think, "Oh, you get to fly to Toronto?
[00:28:49.680 --> 00:28:51.040]   It's got to be great."
[00:28:51.040 --> 00:28:52.400]   It's not great.
[00:28:52.400 --> 00:28:57.120]   You fly, and then you go to some hotel, and then you go to a meeting where you're just
[00:28:57.120 --> 00:29:02.480]   trying to get people to engage with you and learn about the product.
[00:29:02.480 --> 00:29:07.600]   There were a couple of months there, very early on, where I felt like we can't charge
[00:29:07.600 --> 00:29:08.920]   enough for the product.
[00:29:08.920 --> 00:29:12.160]   People don't see it as being valuable enough.
[00:29:12.160 --> 00:29:18.880]   It can be very demoralizing, especially when you're out there on the front lines of sales
[00:29:18.880 --> 00:29:25.920]   and trying to educate and teach people about these concepts that are literally being created
[00:29:25.920 --> 00:29:31.200]   as we're iterating on the product.
[00:29:31.200 --> 00:29:35.960]   A dark moment for me is, we already talked about this, but when we were first trying
[00:29:35.960 --> 00:29:39.840]   to sell to GitHub, we had this user, Hamal, who we talked about earlier.
[00:29:39.840 --> 00:29:44.840]   He gave us very direct feedback about how our product sucked.
[00:29:44.840 --> 00:29:47.880]   He was totally right.
[00:29:47.880 --> 00:29:50.200]   I love building this product.
[00:29:50.200 --> 00:29:51.560]   Even now, it's hard.
[00:29:51.560 --> 00:29:54.240]   I'll take it personally, for sure.
[00:29:54.240 --> 00:29:57.880]   Even though I know that some of the decisions are bad, or there's lots of things that could
[00:29:57.880 --> 00:30:01.040]   be improved, when somebody calls it out, it definitely hurts.
[00:30:01.040 --> 00:30:03.040]   We really want that feedback.
[00:30:03.040 --> 00:30:08.640]   I'm happy to go through that rollercoaster of emotions to make a better product.
[00:30:08.640 --> 00:30:16.080]   Really, that feedback led Weights & Biases to the place it is today.
[00:30:16.080 --> 00:30:19.040]   You have to be willing to accept there's lots of bad things.
[00:30:19.040 --> 00:30:20.440]   We want to know what they are.
[00:30:20.440 --> 00:30:21.840]   We made those decisions.
[00:30:21.840 --> 00:30:22.840]   It's my fault.
[00:30:22.840 --> 00:30:23.840]   It's Chris's fault.
[00:30:23.840 --> 00:30:27.400]   It's probably Lucas's fault and Lavanya's fault to some extent.
[00:30:27.400 --> 00:30:30.240]   We can always improve stuff.
[00:30:30.240 --> 00:30:34.640]   You take those gut punches in stride and keep making it better.
[00:30:34.640 --> 00:30:35.640]   I have this memory.
[00:30:35.640 --> 00:30:39.320]   I wonder if this is an accurate memory or you see it the same way.
[00:30:39.320 --> 00:30:45.480]   I remember thinking of making experiment tracking and doing an offsite where we really built
[00:30:45.480 --> 00:30:54.240]   something super custom for TRI, and then going there and showing them a beta of the experiment
[00:30:54.240 --> 00:30:56.160]   tracking stuff that we've built.
[00:30:56.160 --> 00:31:00.680]   Having them basically tell us this isn't that interesting and that feeling bad.
[00:31:00.680 --> 00:31:03.840]   Then I have this memory of talking to you, Sean.
[00:31:03.840 --> 00:31:06.800]   I think you were like, "I don't think anyone will pay for experiment tracking."
[00:31:06.800 --> 00:31:09.160]   I was thinking, "You're probably right."
[00:31:09.160 --> 00:31:10.160]   Then I remember talking to you.
[00:31:10.160 --> 00:31:14.120]   I was like, "I just need to tell you that I need you to be more positive," which is
[00:31:14.120 --> 00:31:17.360]   so funny because I feel like actually you're almost always the optimist.
[00:31:17.360 --> 00:31:18.640]   It was funny.
[00:31:18.640 --> 00:31:21.880]   I remember at least thinking to myself, "What I need to communicate is I just need you to
[00:31:21.880 --> 00:31:26.600]   be positive, even if it's not rational to be positive here because I'm feeling a lot
[00:31:26.600 --> 00:31:28.200]   of doubt myself."
[00:31:28.200 --> 00:31:29.200]   Is that an accurate memory?
[00:31:29.200 --> 00:31:30.200]   Yeah, yeah.
[00:31:30.200 --> 00:31:31.200]   I remember that.
[00:31:31.560 --> 00:31:36.360]   We had that user, that first user who was actually using the thing and we felt like
[00:31:36.360 --> 00:31:37.920]   they were getting value.
[00:31:37.920 --> 00:31:39.840]   We kept saying, "What if we build this other thing?
[00:31:39.840 --> 00:31:40.840]   Will that get us a user?
[00:31:40.840 --> 00:31:41.840]   What if we build this other thing?"
[00:31:41.840 --> 00:31:44.840]   We did that for a number of months.
[00:31:44.840 --> 00:31:48.280]   It was the early stages of the startup.
[00:31:48.280 --> 00:31:51.760]   That was disheartening because it's like, "Okay, if we could build this other feature
[00:31:51.760 --> 00:31:53.480]   tomorrow, but nobody's going to care."
[00:31:53.480 --> 00:31:56.080]   That was the mindset I was getting into.
[00:31:56.080 --> 00:32:01.080]   You were rightfully calling me out on, "Well, this is the early stage of a startup, so let's
[00:32:01.080 --> 00:32:04.160]   make the next thing until we have that user."
[00:32:04.160 --> 00:32:09.640]   I think it was like, again, it was getting that first user who broke that off.
[00:32:09.640 --> 00:32:13.080]   From there, it was like, "Hey, I just need this one little tweak.
[00:32:13.080 --> 00:32:14.080]   Great, we'll do it."
[00:32:14.080 --> 00:32:16.120]   It was all positive.
[00:32:16.120 --> 00:32:19.080]   Maybe not all positive, but more positive from there.
[00:32:19.080 --> 00:32:21.200]   All right, Luke.
[00:32:21.200 --> 00:32:32.040]   If there are other entrepreneurs listening to the podcast and wanting to build a startup
[00:32:32.040 --> 00:32:40.200]   that achieves a billion-dollar valuation, what advice as a startup CEO would you give
[00:32:40.200 --> 00:32:41.200]   them?
[00:32:41.200 --> 00:32:44.600]   I feel like the advice probably depends on who that person is.
[00:32:44.600 --> 00:32:45.600]   Let's picture someone.
[00:32:45.600 --> 00:32:46.920]   Who are you thinking of here?
[00:32:46.920 --> 00:32:47.920]   All right.
[00:32:47.920 --> 00:32:50.720]   It's someone that looks a little like us.
[00:32:50.720 --> 00:32:52.720]   They're programmers.
[00:32:52.720 --> 00:32:58.240]   They're interested in starting a company, but maybe don't have a ton of experience on
[00:32:58.240 --> 00:33:03.600]   the business side of things, but they're passionate about the product they're creating.
[00:33:03.600 --> 00:33:06.720]   There's so much advice out there that I think is really good these days.
[00:33:06.720 --> 00:33:11.800]   I feel like when we were all starting our companies the first time, being an entrepreneur
[00:33:11.800 --> 00:33:14.440]   wasn't a thing.
[00:33:14.440 --> 00:33:16.880]   What Combinator has put out so much good stuff.
[00:33:16.880 --> 00:33:20.440]   You forget how not obvious it is to people that you need to make something that people
[00:33:20.440 --> 00:33:21.440]   want.
[00:33:21.440 --> 00:33:24.160]   You can't emphasize that enough.
[00:33:24.160 --> 00:33:27.840]   But I feel like now people know that, which is fantastic.
[00:33:27.840 --> 00:33:33.560]   It definitely wasn't obvious to everyone when we were starting, or maybe as obvious to us
[00:33:33.560 --> 00:33:35.360]   as it should have been.
[00:33:35.360 --> 00:33:39.920]   But I think the thing that people don't talk about as much as I think they should, or the
[00:33:39.920 --> 00:33:46.120]   advice that I feel like I can uniquely offer because it's worked so well for me, is to
[00:33:46.120 --> 00:33:50.240]   pick a customer that you really love spending time with.
[00:33:50.240 --> 00:33:56.480]   I feel like a lot of these ML startups, especially, they totally start from a technology and what's
[00:33:56.480 --> 00:33:58.680]   interesting to do with it.
[00:33:58.680 --> 00:34:00.000]   I think that's a bad idea.
[00:34:00.000 --> 00:34:04.920]   Everyone knows that that's a bad idea, but then they work backwards from a use case that
[00:34:04.920 --> 00:34:06.720]   they find interesting.
[00:34:06.720 --> 00:34:13.840]   That's maybe an okay idea, but I think the thing that gets lost is that, at least for
[00:34:13.840 --> 00:34:20.560]   me, the thing I have to do all day long is spending time with customers, spending time
[00:34:20.560 --> 00:34:25.400]   empathizing with customers, thinking about customers and bringing the customer voice
[00:34:25.400 --> 00:34:26.400]   into the company.
[00:34:26.400 --> 00:34:34.400]   And so given that that's, I think, maybe the most important job as CEO, you should pick
[00:34:34.400 --> 00:34:36.080]   a customer that you really like, right?
[00:34:36.080 --> 00:34:41.560]   Because you're going to spend so much time with them over the entire arc of your company.
[00:34:41.560 --> 00:34:46.560]   And so I think having a specific idea of who that is and making sure you like them, I think,
[00:34:46.560 --> 00:34:48.440]   is a really key thing.
[00:34:48.440 --> 00:34:54.080]   I remember at CrowdFlower, we tried to sell into different types of customers.
[00:34:54.080 --> 00:34:55.200]   And so I really felt this.
[00:34:55.200 --> 00:35:03.200]   I went to CMO conferences and I contrast that for myself, going to NeurIPS and just really
[00:35:03.200 --> 00:35:07.400]   enjoying making small talk, enjoying all the details of things that people say.
[00:35:07.400 --> 00:35:11.720]   I mean, I also believe that's really powerful for the world and good for the world, but
[00:35:11.720 --> 00:35:17.880]   I think even more than that, just on a day-to-day motivation, the impact will sustain you over
[00:35:17.880 --> 00:35:23.160]   the long-term, but over the short-term, I think I really appreciate that I'm working
[00:35:23.160 --> 00:35:28.600]   with a user base that I really care about and enjoy talking to.
[00:35:28.600 --> 00:35:31.840]   LaVanya's in the chat here.
[00:35:31.840 --> 00:35:34.160]   We're taking questions from the audience.
[00:35:34.160 --> 00:35:37.280]   What's the hope for Weights & Biases in the next five years?
[00:35:37.280 --> 00:35:42.080]   I feel like almost like it's a jinx to say that question.
[00:35:42.080 --> 00:35:43.720]   I don't even know if I have a good answer.
[00:35:43.720 --> 00:35:45.200]   Maybe you guys want to try first?
[00:35:45.200 --> 00:35:49.320]   Of course, I take it from a product and tools standpoint.
[00:35:49.320 --> 00:35:56.760]   I hope that we can build interconnected tools across the ML pipeline that really work well
[00:35:56.760 --> 00:36:03.320]   together because they share these common underlying threads or infrastructural pieces or dare
[00:36:03.320 --> 00:36:08.040]   I say bones, which is what I like to call them internally and everybody sort of makes
[00:36:08.040 --> 00:36:10.000]   fun of me for.
[00:36:10.000 --> 00:36:14.760]   But to me, it's really important that the data that you collect about your model in
[00:36:14.760 --> 00:36:18.360]   production can be used to inform decisions that you make back in the data collection
[00:36:18.360 --> 00:36:20.160]   process and the training process.
[00:36:20.160 --> 00:36:24.240]   So many parts of the ML pipeline, it's hard to build all this stuff.
[00:36:24.240 --> 00:36:29.240]   But if you think of the best companies, somebody like Google who's building ML, they've built
[00:36:29.240 --> 00:36:32.760]   all this, they've verticalized all of it internally and built it themselves.
[00:36:32.760 --> 00:36:35.560]   And they've built tools out of other tools.
[00:36:35.560 --> 00:36:41.320]   And I want to be able to make a platform like that outside of a giant company and give it
[00:36:41.320 --> 00:36:44.760]   to the rest of the world and sort of use all the use cases that we encounter to make it
[00:36:44.760 --> 00:36:47.520]   better and better and more general.
[00:36:47.520 --> 00:36:56.280]   I think it'd be really satisfying if Weights & Biases becomes a core part of every ML team's
[00:36:56.280 --> 00:37:02.920]   infrastructure and we're really known for making really high quality stuff, really useful
[00:37:02.920 --> 00:37:05.720]   stuff, really powerful stuff.
[00:37:05.720 --> 00:37:11.840]   I think we're on that trajectory, but I think ML is growing so much that that becomes every
[00:37:11.840 --> 00:37:16.000]   company when every company has an ML team, which it seems like we're headed to in the
[00:37:16.000 --> 00:37:17.600]   next five years.
[00:37:17.600 --> 00:37:19.600]   So I think that's the biggest thing.
[00:37:19.600 --> 00:37:25.400]   And it's like, if you imagine the company that is in that position, what does it look
[00:37:25.400 --> 00:37:27.000]   like internally?
[00:37:27.000 --> 00:37:31.440]   I think we kind of have this today, but I mean, there's folks building data tools and
[00:37:31.440 --> 00:37:35.080]   ML tools at lots of companies in the world and doing a great job.
[00:37:35.080 --> 00:37:39.560]   And I just want to get all those people in the same place, people who love building these
[00:37:39.560 --> 00:37:40.720]   tools.
[00:37:40.720 --> 00:37:44.040]   And maybe there's folks out there who work on these tools and don't really love it and
[00:37:44.040 --> 00:37:45.720]   want to switch to something else.
[00:37:45.720 --> 00:37:46.720]   That's great too.
[00:37:46.720 --> 00:37:49.680]   I mean, you should move through things in your career, but I would love to be surrounded
[00:37:49.680 --> 00:37:53.840]   by people who really love this problem and really love the people who work on this problem
[00:37:53.840 --> 00:37:59.720]   as customers, just all together, all of us, maybe not in an office in the modern world
[00:37:59.720 --> 00:38:03.600]   anymore, but kind of distributed around working on these problems together and just building
[00:38:03.600 --> 00:38:09.440]   great stuff that the users love so they can build ML models that make the world better.
[00:38:09.440 --> 00:38:14.400]   Here at Gradient Dissent, since you guys are both mega fans, you know this, but for new
[00:38:14.400 --> 00:38:23.800]   listeners, we always end with two questions and the penultimate question is, what is the
[00:38:23.800 --> 00:38:27.160]   most or an underrated topic in machine learning?
[00:38:27.160 --> 00:38:32.320]   Something that you would love to work on if you weren't working on Weights and Biases.
[00:38:32.320 --> 00:38:41.080]   I want to make a painting robot, but it uses an actual brush, okay?
[00:38:41.080 --> 00:38:42.920]   Not just a plotter or something.
[00:38:42.920 --> 00:38:47.320]   It's going to be very complicated.
[00:38:47.320 --> 00:38:53.720]   That's not the big world-changing answer maybe you were looking for, but that's what I would
[00:38:53.720 --> 00:38:57.960]   be really stoked to pour a year of my life into, I think.
[00:38:57.960 --> 00:38:58.960]   Like a paint bot?
[00:38:58.960 --> 00:38:59.960]   Yeah, a paint bot.
[00:38:59.960 --> 00:39:00.960]   That's right.
[00:39:00.960 --> 00:39:01.960]   Cool, cool.
[00:39:01.960 --> 00:39:02.960]   All right.
[00:39:02.960 --> 00:39:03.960]   Underrated topic.
[00:39:03.960 --> 00:39:07.120]   Maybe we should have started that robot company.
[00:39:07.120 --> 00:39:09.880]   It could have been kind of a myth.
[00:39:09.880 --> 00:39:14.040]   This question's funny because we touch so many of the problems in machine learning and
[00:39:14.040 --> 00:39:15.920]   Weights and Biases.
[00:39:15.920 --> 00:39:20.120]   Not all of them, but we're building tools that are used for all the verticals, right?
[00:39:20.120 --> 00:39:21.120]   So of course we're going to touch them.
[00:39:21.120 --> 00:39:25.360]   So maybe I'll sort of say something that we're not explicitly doing that I think is really
[00:39:25.360 --> 00:39:30.160]   important and that I feel like I am actually doing, which is I think model understanding
[00:39:30.160 --> 00:39:33.880]   is critical in the future.
[00:39:33.880 --> 00:39:36.240]   Deep learning models are really tricky to understand.
[00:39:36.240 --> 00:39:38.120]   It's like a research area.
[00:39:38.120 --> 00:39:41.600]   It's really dependent on what kind of model you're building, what techniques you might
[00:39:41.600 --> 00:39:47.160]   use to figure out why did my car make the decision that it did at a specific point in
[00:39:47.160 --> 00:39:48.160]   time.
[00:39:48.160 --> 00:39:53.080]   And so I think as these models get more and more complex, it's more and more important.
[00:39:53.080 --> 00:39:55.560]   And I want to understand the models.
[00:39:55.560 --> 00:39:58.920]   I want to understand the models for the world to be good, and I want to understand the models
[00:39:58.920 --> 00:40:05.160]   because I think that gives us some understanding into the nature of intelligence and our own
[00:40:05.160 --> 00:40:07.600]   sort of decision-making processes.
[00:40:07.600 --> 00:40:12.920]   So we're not explicitly doing model understanding at Weights & Biases, but we're trying to build
[00:40:12.920 --> 00:40:19.240]   tools and we'll talk more about this over the next six months, I guess, that head in
[00:40:19.240 --> 00:40:20.240]   that direction.
[00:40:20.240 --> 00:40:23.360]   I'm kind of tempted to answer this question, but I feel like maybe it's better as a host
[00:40:23.360 --> 00:40:27.280]   if I remain mysterious in terms of what I think is the most underrated topic.
[00:40:27.280 --> 00:40:34.760]   But I will say I love the company that we started and I would not change it.
[00:40:34.760 --> 00:40:36.360]   Obviously it was a really good choice to do it.
[00:40:36.360 --> 00:40:40.960]   But one thing that we kicked around in the early days, well, we kicked around two ideas.
[00:40:40.960 --> 00:40:46.440]   One I think is a terrible idea, which is a painting drone, which I think will be fun,
[00:40:46.440 --> 00:40:50.360]   but probably just a terrible idea, but still it would be really fun.
[00:40:50.360 --> 00:40:54.760]   The second idea, which I think I would still love to do it, and I think we thought we were
[00:40:54.760 --> 00:40:58.360]   kind of the wrong team to do it, but actually I think we might've been a good team to do
[00:40:58.360 --> 00:41:02.720]   it, but the timing seems like it might've been bad, which is to build a better simulator
[00:41:02.720 --> 00:41:08.600]   to help robot companies simulate what they're doing and then deploy into the real world.
[00:41:08.600 --> 00:41:13.080]   I just think that that still kind of needs to exist and there's different takes on it,
[00:41:13.080 --> 00:41:16.560]   but it doesn't seem like it's been nailed at all, especially with the kind of physical
[00:41:16.560 --> 00:41:19.160]   part of it.
[00:41:19.160 --> 00:41:25.780]   I also think that'd be a really fun company to do, although a much slower ride.
[00:41:25.780 --> 00:41:30.860]   But my answer to the most underrated topic in ML is still a secret.
[00:41:30.860 --> 00:41:38.320]   The final question is, when you look at actually all of our customers trying to deploy a model
[00:41:38.320 --> 00:41:42.360]   successfully, what do you think their biggest pain point is?
[00:41:42.360 --> 00:41:47.400]   We always ask guests this, who are mostly at these companies trying to do it, but looking
[00:41:47.400 --> 00:41:52.120]   out at everyone, maybe we should start with those people already using weights and biases,
[00:41:52.120 --> 00:41:55.260]   because the people not using weights and biases, obviously maybe that's their biggest problem,
[00:41:55.260 --> 00:41:59.560]   but the people that are already using us, what is the big thing that they run into?
[00:41:59.560 --> 00:42:03.960]   I mean, there's a cop-out answer, which is probably it's hiring.
[00:42:03.960 --> 00:42:07.320]   That might be accurate.
[00:42:07.320 --> 00:42:08.320]   Hiring, yeah.
[00:42:08.320 --> 00:42:12.760]   At some point, we should collect some stats on what people say and maybe that'll just
[00:42:12.760 --> 00:42:16.040]   sort of answer it based on at least people we interview.
[00:42:16.040 --> 00:42:17.560]   But yeah, what are you guys saying?
[00:42:17.560 --> 00:42:22.760]   It really varies in our customers, because somebody who's building a self-driving car,
[00:42:22.760 --> 00:42:27.640]   for example, they're building like a hundred models in parallel and with no proof that
[00:42:27.640 --> 00:42:31.120]   self-driving cars could still actually exist and work on public streets.
[00:42:31.120 --> 00:42:36.720]   I guess we're getting very close now, as opposed to somebody maybe who's got a bunch of financial
[00:42:36.720 --> 00:42:40.920]   data and needs to predict credit scores.
[00:42:40.920 --> 00:42:43.960]   In the credit score problem, you actually model understanding what I was just talking
[00:42:43.960 --> 00:42:45.240]   about really, really important.
[00:42:45.240 --> 00:42:48.140]   You probably need to use something dumber than a deep learning model so that you can
[00:42:48.140 --> 00:42:52.680]   actually say why you made a particular credit prediction.
[00:42:52.680 --> 00:42:55.920]   Our customers are extremely varied.
[00:42:55.920 --> 00:42:59.880]   I hope we're solving a lot of the problems on the model creation side of things.
[00:42:59.880 --> 00:43:04.400]   I think that there's a really hard problem of figuring out what models are doing in production
[00:43:04.400 --> 00:43:09.120]   and then taking the data from production and integrating it back into the model training
[00:43:09.120 --> 00:43:10.120]   process.
[00:43:10.120 --> 00:43:15.560]   Yeah, I hope that we get to work on that problem too, but I bet that a lot of our customers
[00:43:15.560 --> 00:43:18.080]   would express they have challenges there.
[00:43:19.080 --> 00:43:29.200]   Yeah, I'll piggyback on Sean's response and say specifically CI and CD when it comes to
[00:43:29.200 --> 00:43:34.400]   these ML pipelines, this is nowhere close to what we have in the regular software development
[00:43:34.400 --> 00:43:35.400]   world.
[00:43:35.400 --> 00:43:42.120]   I know we have a lot of exciting things in our roadmap to help with automating all of
[00:43:42.120 --> 00:43:48.560]   these steps as a model moves through the pipeline and then comes back to get retrained and understanding
[00:43:48.560 --> 00:43:51.480]   how it's performing in production.
[00:43:51.480 --> 00:43:59.680]   I personally look forward to the day that all of this can be automated in a way that
[00:43:59.680 --> 00:44:05.240]   doesn't involve people manually running shell scripts, which is often the case today and
[00:44:05.240 --> 00:44:06.240]   really unfortunate.
[00:44:06.240 --> 00:44:07.240]   Awesome.
[00:44:07.240 --> 00:44:08.240]   Well, thanks so much, guys.
[00:44:08.240 --> 00:44:14.320]   It's been a real pleasure working with you and can't wait for many more years.
[00:44:14.320 --> 00:44:17.840]   Can't wait for more podcasts, Luke.
[00:44:17.840 --> 00:44:18.840]   Yeah, we got to bring you back.
[00:44:18.840 --> 00:44:19.840]   You're a fantastic podcaster.
[00:44:19.840 --> 00:44:23.240]   A year from now, see where we're at.
[00:44:23.240 --> 00:44:27.640]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:44:27.640 --> 00:44:32.360]   to the show notes in the description where you can find links to all the papers that
[00:44:32.360 --> 00:44:36.600]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:44:36.600 --> 00:44:38.480]   produce, so check it out.
[00:44:38.480 --> 00:44:46.640]   I thought it'd be fun, since it's a friendly guest, obviously, to add some zany new features
[00:44:46.640 --> 00:44:47.640]   to this podcast.
[00:44:47.640 --> 00:44:51.940]   So I thought one section would be, I'm going to name a technology and then you guys immediately
[00:44:51.940 --> 00:44:53.580]   say underrated or overrated.
[00:44:53.580 --> 00:44:57.560]   And then if you disagree, we can fight it out.
[00:44:57.560 --> 00:44:58.560]   What do you think?
[00:44:58.560 --> 00:44:59.560]   Let's go.
[00:44:59.560 --> 00:45:00.560]   Are you ready?
[00:45:00.560 --> 00:45:01.560]   Okay.
[00:45:01.560 --> 00:45:03.120]   Reinforcement learning.
[00:45:03.120 --> 00:45:04.120]   Is there a middle ground?
[00:45:04.120 --> 00:45:06.120]   Wait, is it only under or over?
[00:45:06.120 --> 00:45:08.240]   Yeah, you got to decide.
[00:45:08.240 --> 00:45:09.880]   I love reinforcement learning.
[00:45:09.880 --> 00:45:10.880]   Underrated.
[00:45:10.880 --> 00:45:12.740]   Yeah, I'll go with Sean, underrated.
[00:45:12.740 --> 00:45:13.740]   All right, all right.
[00:45:13.740 --> 00:45:14.740]   Auto ML.
[00:45:14.740 --> 00:45:15.740]   Overrated.
[00:45:15.740 --> 00:45:16.740]   Come on, Sean, it's bad for business.
[00:45:16.740 --> 00:45:17.740]   Underrated.
[00:45:17.740 --> 00:45:18.740]   Wait, I already forgot what you said, Chris.
[00:45:18.740 --> 00:45:19.740]   You think it's overrated?
[00:45:19.740 --> 00:45:20.740]   Yeah.
[00:45:20.740 --> 00:45:21.740]   Yeah.
[00:45:21.740 --> 00:45:22.740]   Wait, why?
[00:45:22.740 --> 00:45:23.740]   Because I think it's really important that the EML practitioners understand that this
[00:45:23.740 --> 00:45:24.740]   is a tool that's going to help them get better at their job.
[00:45:24.740 --> 00:45:25.740]   So if you have a problem with that, you should go to the EML practice.
[00:45:25.740 --> 00:45:26.740]   If you don't, you should go to the EML practice.
[00:45:26.740 --> 00:45:27.740]   You should go to the EML practice.
[00:45:27.740 --> 00:45:28.740]   You should go to the EML practice.
[00:45:28.740 --> 00:45:29.740]   You should go to the EML practice.
[00:45:29.740 --> 00:45:30.740]   You should go to the EML practice.
[00:45:30.740 --> 00:45:31.740]   You should go to the EML practice.
[00:45:31.740 --> 00:45:32.740]   You should go to the EML practice.
[00:45:32.740 --> 00:45:33.740]   You should go to the EML practice.
[00:45:33.740 --> 00:45:34.740]   You should go to the EML practice.
[00:45:34.740 --> 00:45:35.740]   You should go to the EML practice.
[00:45:35.740 --> 00:45:36.740]   You should go to the EML practice.
[00:45:36.740 --> 00:45:37.740]   You should go to the EML practice.
[00:45:37.740 --> 00:45:38.740]   You should go to the EML practice.
[00:45:38.740 --> 00:45:39.740]   You should go to the EML practice.
[00:45:39.740 --> 00:45:40.740]   You should go to the EML practice.
[00:45:40.740 --> 00:45:41.740]   You should go to the EML practice.
[00:45:41.740 --> 00:45:42.740]   You should go to the EML practice.
[00:45:42.740 --> 00:45:43.740]   You should go to the EML practice.
[00:45:43.740 --> 00:45:44.740]   You should go to the EML practice.
[00:45:44.740 --> 00:45:45.740]   You should go to the EML practice.
[00:45:45.740 --> 00:45:46.740]   You should go to the EML practice.
[00:45:46.740 --> 00:45:47.740]   You should go to the EML practice.
[00:45:47.740 --> 00:45:48.740]   You should go to the EML practice.
[00:45:48.740 --> 00:45:49.740]   You should go to the EML practice.
[00:45:49.740 --> 00:45:50.740]   You should go to the EML practice.
[00:45:50.740 --> 00:45:51.740]   You should go to the EML practice.
[00:45:51.740 --> 00:45:52.740]   You should go to the EML practice.
[00:45:52.740 --> 00:45:53.740]   You should go to the EML practice.
[00:45:53.740 --> 00:45:54.740]   You should go to the EML practice.
[00:45:54.740 --> 00:45:55.740]   You should go to the EML practice.
[00:45:55.740 --> 00:45:56.740]   You should go to the EML practice.
[00:45:56.740 --> 00:45:57.740]   You should go to the EML practice.
[00:45:57.740 --> 00:45:58.740]   You should go to the EML practice.
[00:45:58.740 --> 00:45:59.740]   You should go to the EML practice.
[00:45:59.740 --> 00:46:00.740]   You should go to the EML practice.
[00:46:00.740 --> 00:46:01.740]   You should go to the EML practice.
[00:46:01.740 --> 00:46:02.740]   You should go to the EML practice.
[00:46:02.740 --> 00:46:03.740]   You should go to the EML practice.
[00:46:03.740 --> 00:46:04.740]   You should go to the EML practice.
[00:46:04.740 --> 00:46:05.740]   You should go to the EML practice.
[00:46:05.740 --> 00:46:06.740]   You should go to the EML practice.
[00:46:06.740 --> 00:46:07.740]   You should go to the EML practice.
[00:46:07.740 --> 00:46:08.740]   You should go to the EML practice.
[00:46:08.740 --> 00:46:09.740]   You should go to the EML practice.
[00:46:09.740 --> 00:46:14.980]   As a company, what we need to do is, as those tools get better and better, it's automatically
[00:46:14.980 --> 00:46:15.980]   building models.
[00:46:15.980 --> 00:46:19.660]   There's all kinds of other problems around model building, like getting the right data
[00:46:19.660 --> 00:46:23.500]   in the first place, and we need to build those.
[00:46:23.500 --> 00:46:25.940]   This space is moving so quickly.
[00:46:25.940 --> 00:46:30.060]   Of course, AutoML will probably continue to improve, but there's lots of other problems
[00:46:30.060 --> 00:46:32.740]   around it to be solved for the practitioners.
[00:46:32.740 --> 00:46:33.740]   All right.
[00:46:33.740 --> 00:46:37.340]   Well, I think people should leave comments for which they found more convincing, but
[00:46:37.340 --> 00:46:39.900]   definitely bonus points for the weights and biases plug in there.
[00:46:39.900 --> 00:46:40.900]   I like it.
[00:46:40.900 --> 00:46:41.900]   Okay.
[00:46:41.900 --> 00:46:42.900]   Next one.
[00:46:42.900 --> 00:46:43.900]   The singularity.
[00:46:43.900 --> 00:46:44.900]   Underrated?
[00:46:44.900 --> 00:46:45.900]   Yep.
[00:46:45.900 --> 00:46:46.900]   I'm with Chris.
[00:46:46.900 --> 00:46:47.900]   Underrated.
[00:46:47.900 --> 00:46:48.900]   Whoa.
[00:46:48.900 --> 00:46:49.900]   Whoa.
[00:46:49.900 --> 00:46:50.900]   Bold.
[00:46:50.900 --> 00:46:51.900]   All right.
[00:46:51.900 --> 00:46:52.900]   We'll move on.
[00:46:52.900 --> 00:46:53.900]   Okay.
[00:46:53.900 --> 00:46:54.900]   Ready?
[00:46:54.900 --> 00:46:55.900]   Bigtable.
[00:46:55.900 --> 00:46:56.900]   Overrated.
[00:46:56.900 --> 00:46:57.900]   I'll go with underrated.
[00:46:57.900 --> 00:46:58.900]   Sure.
[00:46:58.900 --> 00:46:59.900]   Whoa.
[00:46:59.900 --> 00:47:00.900]   All right.
[00:47:00.900 --> 00:47:01.900]   Let's...
[00:47:01.900 --> 00:47:02.900]   All right.
[00:47:02.900 --> 00:47:03.900]   Sean, maybe you go first.
[00:47:03.900 --> 00:47:04.900]   I think Bigtable...
[00:47:04.900 --> 00:47:08.340]   I think Google, I guess, Bigtable was starting to be used.
[00:47:08.340 --> 00:47:11.900]   At the time, this was, again, it's like 2006.
[00:47:11.900 --> 00:47:13.420]   Bigtable was really big inside of Google.
[00:47:13.420 --> 00:47:17.380]   It kind of enabled all of the technology that we had, the search and everything else that
[00:47:17.380 --> 00:47:19.500]   people were building, Gmail.
[00:47:19.500 --> 00:47:22.420]   It didn't exist elsewhere in the world.
[00:47:22.420 --> 00:47:25.820]   When I left Google, I saw the world starting to sort of copy Bigtable, and it became things
[00:47:25.820 --> 00:47:26.820]   like NoSQL.
[00:47:26.820 --> 00:47:29.380]   I mean, it had a huge impact on the world.
[00:47:29.380 --> 00:47:33.420]   I think maybe today, Bigtable, raw Bigtable itself, maybe this is what Chris is probably
[00:47:33.420 --> 00:47:36.940]   alluding to, is a bit of a challenge.
[00:47:36.940 --> 00:47:40.260]   Maybe I'll let Chris take it from there.
[00:47:40.260 --> 00:47:44.100]   The reason I said it, because I remember a day early in the founding of this company
[00:47:44.100 --> 00:47:47.620]   when we were trying to figure out where to store our metrics, and you were like, "Bigtable
[00:47:47.620 --> 00:47:49.680]   will solve all of our problems.
[00:47:49.680 --> 00:47:51.060]   It's perfect for this."
[00:47:51.060 --> 00:47:53.700]   It's kind of been a thorn in our side a little bit.
[00:47:53.700 --> 00:47:58.700]   I mean, it's done its job well, but you could ask a handful of engineers at Winsome Biases,
[00:47:58.700 --> 00:48:02.940]   and I bet you most of them would gripe about Bigtable.
[00:48:02.940 --> 00:48:06.380]   I think our use case is a little funky for the way we're using it.
[00:48:06.380 --> 00:48:12.020]   For large time series where you want to fetch a few million contiguous points at a time,
[00:48:12.020 --> 00:48:16.180]   especially in the shared Bigtable cluster that you get from Google, there's some challenges
[00:48:16.180 --> 00:48:17.180]   there.
[00:48:17.180 --> 00:48:18.180]   Okay.
[00:48:18.180 --> 00:48:19.180]   Python.
[00:48:19.180 --> 00:48:20.180]   Underrated.
[00:48:20.180 --> 00:48:21.180]   Yeah, underrated.
[00:48:21.180 --> 00:48:22.180]   Okay.
[00:48:22.180 --> 00:48:23.180]   Jupyter.
[00:48:23.180 --> 00:48:24.180]   Underrated.
[00:48:24.180 --> 00:48:25.180]   Underrated.
[00:48:25.180 --> 00:48:26.180]   JupyterHub.
[00:48:26.180 --> 00:48:27.180]   Underrated.
[00:48:27.180 --> 00:48:28.180]   Underrated.
[00:48:28.180 --> 00:48:29.180]   Great.
[00:48:29.180 --> 00:48:30.180]   Kubeflow.
[00:48:30.180 --> 00:48:31.180]   Underrated.
[00:48:31.180 --> 00:48:32.180]   Underrated.
[00:48:32.180 --> 00:48:33.180]   All right.
[00:48:33.180 --> 00:48:34.180]   SageMaker.
[00:48:34.180 --> 00:48:35.180]   Overrated.
[00:48:35.180 --> 00:48:36.180]   Interesting.
[00:48:36.180 --> 00:48:37.180]   All right.
[00:48:37.180 --> 00:48:38.180]   TensorFlow.
[00:48:38.180 --> 00:48:39.180]   Underrated.
[00:48:39.180 --> 00:48:40.180]   Underrated.
[00:48:40.180 --> 00:48:41.180]   Incredible.
[00:48:41.180 --> 00:48:42.180]   You guys are super aligned.
[00:48:42.180 --> 00:48:43.180]   Hard to get some descent here.
[00:48:43.180 --> 00:48:44.180]   Yeah, we've got to live up to your podcast name.
[00:48:44.180 --> 00:48:45.180]   Yeah, yeah.
[00:48:45.180 --> 00:48:46.180]   I guess we'll get some other pairs of guests, or maybe we could ask other ones who are not
[00:48:46.180 --> 00:48:47.180]   here.
[00:48:47.180 --> 00:48:48.180]   I'm not sure who's here.
[00:48:48.180 --> 00:48:49.180]   I'm not sure who's here.
[00:48:49.180 --> 00:48:50.180]   I'm not sure.
[00:48:50.180 --> 00:48:51.180]   Let's do it.
[00:48:51.180 --> 00:48:52.180]   Let's do it.
[00:48:52.180 --> 00:48:53.180]   Let's do it.
[00:48:53.180 --> 00:48:54.180]   Let's do it.
[00:48:54.180 --> 00:48:55.180]   Let's do it.
[00:48:55.180 --> 00:48:56.180]   Let's do it.
[00:48:56.180 --> 00:48:57.180]   Let's do it.
[00:48:57.180 --> 00:48:58.180]   I'm not sure who's here.
[00:48:58.180 --> 00:48:59.180]   I'm not sure who's here.
[00:48:59.180 --> 00:49:02.060]   Let's get some other pairs of guests, or maybe we could ask other ones the same questions
[00:49:02.060 --> 00:49:03.060]   and then see what they say.
[00:49:03.060 --> 00:49:05.460]   I mean, do you differ from us in any of those, Luke?
[00:49:05.460 --> 00:49:08.220]   I mean, some of these technologies, I'm even kind of hazy on what they are.
[00:49:08.220 --> 00:49:10.540]   Like I was thinking BigQuery, Bigtable.
[00:49:10.540 --> 00:49:20.100]   I was hoping I would learn what the difference is, to be honest.
[00:49:20.100 --> 00:49:30.940]   I don't know.

