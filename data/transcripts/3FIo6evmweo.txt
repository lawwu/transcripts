
[00:00:00.000 --> 00:00:03.520]   The following is a conversation with Jürgen Schmidhuber.
[00:00:03.520 --> 00:00:06.360]   He's the co-director of the CS Swiss AI Lab
[00:00:06.360 --> 00:00:10.400]   and a co-creator of long short-term memory networks.
[00:00:10.400 --> 00:00:13.720]   LSTMs are used in billions of devices today
[00:00:13.720 --> 00:00:17.400]   for speech recognition, translation, and much more.
[00:00:17.400 --> 00:00:20.800]   Over 30 years, he has proposed a lot of interesting
[00:00:20.800 --> 00:00:23.440]   out-of-the-box ideas on meta-learning,
[00:00:23.440 --> 00:00:26.000]   adversarial networks, computer vision,
[00:00:26.000 --> 00:00:28.760]   and even a formal theory of quote,
[00:00:28.760 --> 00:00:32.360]   creativity, curiosity, and fun.
[00:00:32.360 --> 00:00:34.920]   This conversation is part of the MIT course
[00:00:34.920 --> 00:00:36.520]   on artificial general intelligence
[00:00:36.520 --> 00:00:38.840]   and the Artificial Intelligence Podcast.
[00:00:38.840 --> 00:00:41.960]   If you enjoy it, subscribe on YouTube, iTunes,
[00:00:41.960 --> 00:00:43.960]   or simply connect with me on Twitter
[00:00:43.960 --> 00:00:47.320]   at Lex Friedman spelled F-R-I-D.
[00:00:47.320 --> 00:00:51.520]   And now, here's my conversation with Jürgen Schmidhuber.
[00:00:51.520 --> 00:00:55.640]   Early on, you dreamed of AI systems
[00:00:55.640 --> 00:00:58.720]   that self-improve recursively.
[00:00:58.720 --> 00:01:00.240]   When was that dream born?
[00:01:00.240 --> 00:01:02.920]   - When I was a baby?
[00:01:02.920 --> 00:01:04.000]   No, that's not true.
[00:01:04.000 --> 00:01:05.240]   When I was a teenager.
[00:01:05.240 --> 00:01:09.440]   - And what was the catalyst for that birth?
[00:01:09.440 --> 00:01:11.600]   What was the thing that first inspired you?
[00:01:11.600 --> 00:01:13.960]   - When I was a boy,
[00:01:13.960 --> 00:01:19.960]   I was thinking about what to do in my life
[00:01:19.960 --> 00:01:23.640]   and then I thought the most exciting thing
[00:01:23.640 --> 00:01:27.960]   is to solve the riddles of the universe.
[00:01:27.960 --> 00:01:30.720]   And that means you have to become a physicist.
[00:01:30.720 --> 00:01:35.640]   However, then I realized that there's something even grander
[00:01:35.640 --> 00:01:39.680]   you can try to build a machine
[00:01:39.680 --> 00:01:41.920]   that isn't really a machine any longer,
[00:01:41.920 --> 00:01:44.320]   that learns to become a much better physicist
[00:01:44.320 --> 00:01:46.880]   than I could ever hope to be.
[00:01:46.880 --> 00:01:50.120]   And that's how I thought maybe I can multiply
[00:01:50.120 --> 00:01:54.320]   my tiny little bit of creativity into infinity.
[00:01:54.320 --> 00:01:57.040]   - But ultimately that creativity will be multiplied
[00:01:57.040 --> 00:01:59.120]   to understand the universe around us.
[00:01:59.120 --> 00:02:04.120]   That's the curiosity for that mystery that drove you?
[00:02:04.120 --> 00:02:08.320]   - Yes, so if you can build a machine
[00:02:08.320 --> 00:02:13.320]   that learns to solve more and more complex problems
[00:02:13.320 --> 00:02:16.760]   and more and more general problem solver,
[00:02:16.760 --> 00:02:21.760]   then you basically have solved all the problems,
[00:02:21.760 --> 00:02:25.960]   at least all the solvable problems.
[00:02:25.960 --> 00:02:28.120]   - So how do you think, what is the mechanism
[00:02:28.120 --> 00:02:31.640]   for that kind of general solver look like?
[00:02:31.640 --> 00:02:34.840]   Obviously we don't quite yet have one
[00:02:34.840 --> 00:02:37.040]   or know how to build one, but we have ideas
[00:02:37.040 --> 00:02:39.120]   and you have had throughout your career
[00:02:39.120 --> 00:02:40.800]   several ideas about it.
[00:02:40.800 --> 00:02:43.640]   So how do you think about that mechanism?
[00:02:43.640 --> 00:02:48.640]   - So in the 80s, I thought about how to build this machine
[00:02:48.640 --> 00:02:51.040]   that learns to solve all these problems
[00:02:51.040 --> 00:02:54.120]   that I cannot solve myself.
[00:02:54.120 --> 00:02:57.160]   And I thought it is clear it has to be a machine
[00:02:57.160 --> 00:03:00.880]   that not only learns to solve this problem here
[00:03:00.880 --> 00:03:04.160]   and this problem here, but it also has to learn
[00:03:04.160 --> 00:03:08.080]   to improve the learning algorithm itself.
[00:03:08.080 --> 00:03:12.480]   So it has to have the learning algorithm
[00:03:12.480 --> 00:03:15.720]   in a representation that allows it to inspect it
[00:03:15.720 --> 00:03:19.240]   and modify it such that it can come up
[00:03:19.240 --> 00:03:21.080]   with a better learning algorithm.
[00:03:22.120 --> 00:03:25.720]   So I call that meta-learning, learning to learn
[00:03:25.720 --> 00:03:28.080]   and recursive self-improvement.
[00:03:28.080 --> 00:03:29.880]   That is really the pinnacle of that,
[00:03:29.880 --> 00:03:34.880]   where you then not only learn how to improve
[00:03:34.880 --> 00:03:37.520]   on that problem and on that,
[00:03:37.520 --> 00:03:41.120]   but you also improve the way the machine improves
[00:03:41.120 --> 00:03:43.200]   and you also improve the way it improves
[00:03:43.200 --> 00:03:44.600]   the way it improves itself.
[00:03:44.600 --> 00:03:48.600]   And that was my 1987 diploma thesis,
[00:03:48.600 --> 00:03:53.240]   which was all about that hierarchy of meta-learners
[00:03:53.240 --> 00:03:57.280]   that have no computational limits
[00:03:57.280 --> 00:03:59.960]   except for the well-known limits
[00:03:59.960 --> 00:04:03.240]   that Gödel identified in 1931
[00:04:03.240 --> 00:04:05.720]   and for the limits of physics.
[00:04:05.720 --> 00:04:10.120]   - In the recent years, meta-learning has gained popularity
[00:04:10.120 --> 00:04:12.840]   in a specific kind of form.
[00:04:12.840 --> 00:04:16.040]   You've talked about how that's not really meta-learning
[00:04:16.040 --> 00:04:21.040]   with neural networks, that's more basic transfer learning.
[00:04:21.040 --> 00:04:22.720]   Can you talk about the difference
[00:04:22.720 --> 00:04:25.480]   between the big general meta-learning
[00:04:25.480 --> 00:04:27.960]   and a more narrow sense of meta-learning
[00:04:27.960 --> 00:04:30.880]   the way it's used today, the way it's talked about today?
[00:04:30.880 --> 00:04:33.440]   - Let's take the example of a deep neural network
[00:04:33.440 --> 00:04:37.240]   that has learned to classify images.
[00:04:37.240 --> 00:04:40.080]   And maybe you have trained that network
[00:04:40.080 --> 00:04:43.840]   on 100 different databases of images.
[00:04:45.840 --> 00:04:48.120]   And now a new database comes along
[00:04:48.120 --> 00:04:52.000]   and you want to quickly learn the new thing as well.
[00:04:52.000 --> 00:04:57.720]   So one simple way of doing that is you take the network,
[00:04:57.720 --> 00:05:02.440]   which already knows 100 types of databases,
[00:05:02.440 --> 00:05:06.320]   and then you would just take the top layer of that
[00:05:06.320 --> 00:05:11.320]   and you retrain that using the new label data
[00:05:11.320 --> 00:05:14.720]   that you have in the new image database.
[00:05:14.720 --> 00:05:17.360]   And then it turns out that it really, really quickly
[00:05:17.360 --> 00:05:20.600]   can learn that too, one shot basically,
[00:05:20.600 --> 00:05:24.320]   because from the first 100 datasets,
[00:05:24.320 --> 00:05:27.560]   it already has learned so much about computer vision
[00:05:27.560 --> 00:05:31.880]   that it can reuse that, and that is then almost good enough
[00:05:31.880 --> 00:05:34.240]   to solve the new task, except you need a little bit
[00:05:34.240 --> 00:05:37.080]   of adjustment on the top.
[00:05:37.080 --> 00:05:42.320]   So that is transfer learning, and it has been done
[00:05:42.320 --> 00:05:44.520]   in principle for many decades.
[00:05:44.520 --> 00:05:46.720]   People have done similar things for decades.
[00:05:46.720 --> 00:05:51.080]   Meta-learning, true meta-learning is about
[00:05:51.080 --> 00:05:56.080]   having the learning algorithm itself open to introspection
[00:05:56.080 --> 00:06:00.440]   by the system that is using it,
[00:06:00.440 --> 00:06:04.800]   and also open to modification,
[00:06:04.800 --> 00:06:07.800]   such that the learning system has an opportunity
[00:06:07.880 --> 00:06:12.120]   to modify any part of the learning algorithm,
[00:06:12.120 --> 00:06:16.760]   and then evaluate the consequences of that modification,
[00:06:16.760 --> 00:06:21.040]   and then learn from that to create
[00:06:21.040 --> 00:06:24.880]   a better learning algorithm, and so on recursively.
[00:06:24.880 --> 00:06:28.560]   So that's a very different animal,
[00:06:28.560 --> 00:06:31.160]   where you are opening the space
[00:06:31.160 --> 00:06:33.560]   of possible learning algorithms
[00:06:33.560 --> 00:06:35.520]   to the learning system itself.
[00:06:35.520 --> 00:06:39.040]   - Right, so you've, like in the 2004 paper,
[00:06:39.040 --> 00:06:41.920]   you described Gator machines,
[00:06:41.920 --> 00:06:44.480]   programs that rewrite themselves, right?
[00:06:44.480 --> 00:06:46.560]   Philosophically, and even in your paper,
[00:06:46.560 --> 00:06:49.960]   mathematically, these are really compelling ideas,
[00:06:49.960 --> 00:06:54.960]   but practically, do you see these self-referential programs
[00:06:54.960 --> 00:06:58.360]   being successful in the near term,
[00:06:58.360 --> 00:07:01.960]   to having an impact where sort of it demonstrates
[00:07:01.960 --> 00:07:04.760]   to the world that this direction
[00:07:04.760 --> 00:07:08.640]   is a good one to pursue in the near term?
[00:07:08.640 --> 00:07:11.320]   - Yes, we had these two different types
[00:07:11.320 --> 00:07:13.440]   of fundamental research,
[00:07:13.440 --> 00:07:15.800]   how to build a universal problem solver,
[00:07:15.800 --> 00:07:20.320]   one basically exploiting proof search,
[00:07:20.320 --> 00:07:25.520]   and things like that, that you need to come up with
[00:07:25.520 --> 00:07:30.280]   asymptotically optimal, theoretically optimal,
[00:07:30.280 --> 00:07:33.200]   self-improvers and problem solvers.
[00:07:33.200 --> 00:07:39.160]   However, one has to admit that through this proof search,
[00:07:39.160 --> 00:07:43.600]   comes in an additive constant,
[00:07:43.600 --> 00:07:46.760]   an overhead, an additive overhead,
[00:07:46.760 --> 00:07:51.760]   that vanishes in comparison to what you have to do
[00:07:51.760 --> 00:07:55.160]   to solve large problems.
[00:07:55.160 --> 00:07:58.000]   However, for many of the small problems
[00:07:58.000 --> 00:08:00.880]   that we want to solve in our everyday life,
[00:08:00.880 --> 00:08:03.280]   we cannot ignore this constant overhead.
[00:08:03.280 --> 00:08:08.120]   And that's why we also have been doing other things,
[00:08:08.120 --> 00:08:12.160]   non-universal things, such as recurrent neural networks,
[00:08:12.160 --> 00:08:15.400]   which are trained by gradient descent,
[00:08:15.400 --> 00:08:18.680]   and local search techniques, which aren't universal at all,
[00:08:18.680 --> 00:08:21.280]   which aren't provably optimal at all,
[00:08:21.280 --> 00:08:22.880]   like the other stuff that we did,
[00:08:22.880 --> 00:08:25.400]   but which are much more practical,
[00:08:25.400 --> 00:08:28.760]   as long as we only want to solve the small problems
[00:08:28.760 --> 00:08:33.320]   that we are typically trying to solve
[00:08:33.320 --> 00:08:35.560]   in this environment here.
[00:08:35.560 --> 00:08:38.920]   So the universal problem solvers, like the Godel machine,
[00:08:38.920 --> 00:08:42.080]   but also Markus Hutter's fastest way
[00:08:42.080 --> 00:08:44.360]   of solving all possible problems,
[00:08:44.360 --> 00:08:48.080]   which he developed around 2002 in my lab,
[00:08:48.080 --> 00:08:52.520]   they are associated with these constant overheads
[00:08:52.520 --> 00:08:55.160]   for proof search, which guarantees that the thing
[00:08:55.160 --> 00:08:56.560]   that you're doing is optimal.
[00:08:56.560 --> 00:09:01.160]   For example, there is this fastest way
[00:09:01.160 --> 00:09:05.280]   of solving all problems with a computable solution,
[00:09:05.280 --> 00:09:07.280]   which is due to Markus Hutter.
[00:09:07.280 --> 00:09:12.240]   And to explain what's going on there,
[00:09:12.240 --> 00:09:14.320]   let's take traveling salesman problems.
[00:09:14.320 --> 00:09:17.360]   With traveling salesman problems,
[00:09:17.360 --> 00:09:21.320]   you have a number of cities, N cities,
[00:09:21.320 --> 00:09:23.680]   and you try to find the shortest path
[00:09:23.680 --> 00:09:27.840]   through all these cities without visiting any city twice.
[00:09:27.840 --> 00:09:32.320]   And nobody knows the fastest way
[00:09:32.320 --> 00:09:36.560]   of solving traveling salesman problems, TSPs,
[00:09:36.560 --> 00:09:41.760]   but let's assume there is a method of solving them
[00:09:41.760 --> 00:09:45.920]   within N to the five operations,
[00:09:45.920 --> 00:09:48.560]   where N is the number of cities.
[00:09:50.240 --> 00:09:54.600]   Then the universal method of Markus
[00:09:54.600 --> 00:09:58.600]   is going to solve the same traveling salesman problem
[00:09:58.600 --> 00:10:02.160]   also within N to the five steps,
[00:10:02.160 --> 00:10:06.440]   plus O of one, plus a constant number of steps
[00:10:06.440 --> 00:10:09.280]   that you need for the proof searcher,
[00:10:09.280 --> 00:10:14.120]   which you need to show that this particular class
[00:10:14.120 --> 00:10:17.240]   of problems, the traveling salesman problems,
[00:10:17.240 --> 00:10:19.320]   can be solved within a certain time bound
[00:10:20.320 --> 00:10:24.320]   within order N to the five steps, basically.
[00:10:24.320 --> 00:10:28.440]   And this additive constant doesn't care for N,
[00:10:28.440 --> 00:10:32.320]   which means as N is getting larger and larger,
[00:10:32.320 --> 00:10:34.800]   as you have more and more cities,
[00:10:34.800 --> 00:10:38.640]   the constant overhead pales in comparison.
[00:10:38.640 --> 00:10:43.640]   And that means that almost all large problems are solved
[00:10:43.640 --> 00:10:45.600]   in the best possible way.
[00:10:45.600 --> 00:10:49.120]   Already today, we already have a universal problem solver
[00:10:49.120 --> 00:10:50.600]   like that.
[00:10:50.600 --> 00:10:54.640]   However, it's not practical because the overhead,
[00:10:54.640 --> 00:10:57.600]   the constant overhead is so large
[00:10:57.600 --> 00:11:00.320]   that for the small kinds of problems
[00:11:00.320 --> 00:11:04.680]   that we want to solve in this little biosphere.
[00:11:04.680 --> 00:11:06.520]   - By the way, when you say small,
[00:11:06.520 --> 00:11:09.520]   you're talking about things that fall within the constraints
[00:11:09.520 --> 00:11:11.000]   of our computational systems.
[00:11:11.000 --> 00:11:14.560]   So they can seem quite large to us mere humans, right?
[00:11:14.560 --> 00:11:15.480]   - That's right, yeah.
[00:11:15.480 --> 00:11:19.120]   So they seem large and even unsolvable
[00:11:19.120 --> 00:11:21.120]   in a practical sense today,
[00:11:21.120 --> 00:11:24.880]   but they are still small compared to almost all problems
[00:11:24.880 --> 00:11:28.600]   because almost all problems are large problems,
[00:11:28.600 --> 00:11:31.000]   which are much larger than any constant.
[00:11:31.000 --> 00:11:36.120]   - Do you find it useful as a person who has dreamed
[00:11:36.120 --> 00:11:38.720]   of creating a general learning system,
[00:11:38.720 --> 00:11:39.960]   has worked on creating one,
[00:11:39.960 --> 00:11:42.240]   has done a lot of interesting ideas there,
[00:11:42.240 --> 00:11:46.400]   to think about P versus NP,
[00:11:46.400 --> 00:11:50.840]   this formalization of how hard problems are,
[00:11:50.840 --> 00:11:52.440]   how they scale,
[00:11:52.440 --> 00:11:55.280]   this kind of worst case analysis type of thinking,
[00:11:55.280 --> 00:11:56.880]   do you find that useful?
[00:11:56.880 --> 00:11:59.740]   Or is it only just a mathematical,
[00:11:59.740 --> 00:12:02.680]   it's a set of mathematical techniques
[00:12:02.680 --> 00:12:05.800]   to give you intuition about what's good and bad.
[00:12:05.800 --> 00:12:09.520]   - So P versus NP, that's super interesting
[00:12:09.520 --> 00:12:11.840]   from a theoretical point of view.
[00:12:11.840 --> 00:12:14.600]   And in fact, as you are thinking about that problem,
[00:12:14.600 --> 00:12:17.320]   you can also get inspiration
[00:12:17.320 --> 00:12:21.320]   for better practical problem solvers.
[00:12:21.320 --> 00:12:24.600]   On the other hand, we have to admit that at the moment,
[00:12:24.600 --> 00:12:28.400]   the best practical problem solvers
[00:12:28.400 --> 00:12:31.120]   for all kinds of problems that we are now solving
[00:12:31.120 --> 00:12:33.880]   through what is called AI at the moment,
[00:12:33.880 --> 00:12:36.280]   they are not of the kind
[00:12:36.280 --> 00:12:38.840]   that is inspired by these questions.
[00:12:38.840 --> 00:12:42.720]   There we are using general purpose computers,
[00:12:42.720 --> 00:12:44.840]   such as recurrent neural networks,
[00:12:44.840 --> 00:12:46.720]   but we have a search technique,
[00:12:46.720 --> 00:12:50.320]   which is just local search, gradient descent,
[00:12:50.320 --> 00:12:51.960]   to try to find a program
[00:12:51.960 --> 00:12:54.420]   that is running on these recurrent networks,
[00:12:54.420 --> 00:12:58.560]   such that it can solve some interesting problems,
[00:12:58.560 --> 00:13:00.560]   such as speech recognition,
[00:13:00.560 --> 00:13:03.120]   or machine translation, and something like that.
[00:13:03.120 --> 00:13:06.480]   And there is very little theory
[00:13:06.480 --> 00:13:09.720]   behind the best solutions that we have at the moment
[00:13:09.720 --> 00:13:10.800]   that can do that.
[00:13:10.800 --> 00:13:12.640]   - Do you think that needs to change?
[00:13:12.640 --> 00:13:14.080]   Do you think that will change?
[00:13:14.080 --> 00:13:15.120]   Or can we go,
[00:13:15.120 --> 00:13:17.120]   can we create a general intelligence systems
[00:13:17.120 --> 00:13:20.600]   without ever really proving that that system is intelligent
[00:13:20.600 --> 00:13:22.560]   in some kind of mathematical way,
[00:13:22.560 --> 00:13:24.960]   solving machine translation perfectly,
[00:13:24.960 --> 00:13:26.300]   or something like that,
[00:13:26.300 --> 00:13:29.160]   within some kind of syntactic definition of a language?
[00:13:29.160 --> 00:13:31.120]   Or can we just be super impressed
[00:13:31.120 --> 00:13:35.080]   by the thing working extremely well, and that's sufficient?
[00:13:35.080 --> 00:13:36.720]   - There's an old saying,
[00:13:36.720 --> 00:13:39.340]   and I don't know who brought it up first,
[00:13:39.340 --> 00:13:42.440]   which says, "There's nothing more practical
[00:13:42.440 --> 00:13:43.720]   than a good theory."
[00:13:43.720 --> 00:13:45.960]   (laughing)
[00:13:45.960 --> 00:13:50.400]   And a good theory of problem solving
[00:13:50.400 --> 00:13:54.320]   under limited resources,
[00:13:54.320 --> 00:13:57.040]   like here in this universe, or on this little planet,
[00:13:57.040 --> 00:14:01.800]   has to take into account these limited resources.
[00:14:01.800 --> 00:14:03.960]   And so probably there is lacking
[00:14:04.960 --> 00:14:09.960]   a theory which is related to what we already have,
[00:14:09.960 --> 00:14:14.440]   these asymptotically optimal problem solvers,
[00:14:14.440 --> 00:14:18.560]   which tells us what we need in addition to that
[00:14:18.560 --> 00:14:21.760]   to come up with a practically optimal problem solver.
[00:14:21.760 --> 00:14:27.120]   So I believe we will have something like that.
[00:14:27.120 --> 00:14:29.720]   And maybe just a few little tiny twists
[00:14:29.720 --> 00:14:34.280]   are necessary to change what we already have
[00:14:34.280 --> 00:14:36.320]   to come up with that as well.
[00:14:36.320 --> 00:14:37.720]   As long as we don't have that,
[00:14:37.720 --> 00:14:42.560]   we admit that we are taking suboptimal ways
[00:14:42.560 --> 00:14:45.960]   and returning neural networks and long, short-term memory
[00:14:45.960 --> 00:14:50.400]   for equipped with local search techniques,
[00:14:50.400 --> 00:14:53.520]   and we are happy that it works better
[00:14:53.520 --> 00:14:55.480]   than any competing method,
[00:14:55.520 --> 00:15:00.520]   but that doesn't mean that we think we are done.
[00:15:00.520 --> 00:15:02.760]   - You've said that an AGI system
[00:15:02.760 --> 00:15:05.080]   will ultimately be a simple one.
[00:15:05.080 --> 00:15:06.240]   A general intelligence system
[00:15:06.240 --> 00:15:08.040]   will ultimately be a simple one.
[00:15:08.040 --> 00:15:10.280]   Maybe a pseudocode of a few lines
[00:15:10.280 --> 00:15:11.880]   would be able to describe it.
[00:15:11.880 --> 00:15:16.800]   Can you talk through your intuition behind this idea?
[00:15:16.800 --> 00:15:21.520]   Why you feel that at its core,
[00:15:21.520 --> 00:15:25.560]   intelligence is a simple algorithm?
[00:15:25.560 --> 00:15:31.680]   - Experience tells us that the stuff that works best
[00:15:31.680 --> 00:15:33.120]   is really simple.
[00:15:33.120 --> 00:15:37.640]   So the asymptotically optimal ways of solving problems,
[00:15:37.640 --> 00:15:38.800]   if you look at them,
[00:15:38.800 --> 00:15:41.800]   they're just a few lines of code, it's really true.
[00:15:41.800 --> 00:15:44.000]   Although they are these amazing properties,
[00:15:44.000 --> 00:15:45.800]   just a few lines of code.
[00:15:45.800 --> 00:15:50.800]   Then the most promising and most useful practical thing
[00:15:51.600 --> 00:15:56.360]   maybe don't have this proof of optimality
[00:15:56.360 --> 00:15:57.800]   associated with them.
[00:15:57.800 --> 00:16:00.880]   However, they are also just a few lines of code.
[00:16:00.880 --> 00:16:05.080]   The most successful return neural networks,
[00:16:05.080 --> 00:16:08.520]   you can write them down in five lines of pseudocode.
[00:16:08.520 --> 00:16:10.920]   - That's a beautiful, almost poetic idea,
[00:16:10.920 --> 00:16:15.640]   but what you're describing there
[00:16:15.640 --> 00:16:19.240]   is the lines of pseudocode are sitting on top of layers
[00:16:19.240 --> 00:16:21.440]   and layers of abstractions in a sense.
[00:16:21.440 --> 00:16:25.040]   So you're saying at the very top,
[00:16:25.040 --> 00:16:29.040]   it'll be a beautifully written sort of algorithm,
[00:16:29.040 --> 00:16:34.000]   but do you think that there's many layers of abstractions
[00:16:34.000 --> 00:16:36.880]   we have to first learn to construct?
[00:16:36.880 --> 00:16:40.400]   - Yeah, of course, we are building on all these
[00:16:40.400 --> 00:16:44.000]   great abstractions that people have invented
[00:16:44.000 --> 00:16:49.000]   over the millennia, such as matrix multiplications.
[00:16:49.760 --> 00:16:54.760]   And real numbers and basic arithmetics and calculus
[00:16:54.760 --> 00:17:01.240]   and derivations of error functions
[00:17:01.240 --> 00:17:04.280]   and derivatives of error functions and stuff like that.
[00:17:04.280 --> 00:17:10.440]   So without that language that greatly simplifies
[00:17:10.440 --> 00:17:13.840]   our way of thinking about these problems,
[00:17:13.840 --> 00:17:14.800]   we couldn't do anything.
[00:17:14.800 --> 00:17:16.560]   So in that sense, as always,
[00:17:16.560 --> 00:17:19.600]   we are standing on the shoulders of the giants
[00:17:19.600 --> 00:17:24.240]   who in the past simplified the problem
[00:17:24.240 --> 00:17:26.360]   of problem solving so much
[00:17:26.360 --> 00:17:29.960]   that now we have a chance to do the final step.
[00:17:29.960 --> 00:17:32.120]   - So the final step will be a simple one.
[00:17:32.120 --> 00:17:36.720]   If we take a step back through all of human civilization
[00:17:36.720 --> 00:17:38.360]   and just the universe in general,
[00:17:38.360 --> 00:17:41.440]   how do you think about evolution
[00:17:41.440 --> 00:17:45.360]   and what if creating a universe is required
[00:17:45.360 --> 00:17:47.280]   to achieve this final step?
[00:17:47.280 --> 00:17:50.880]   What if going through the very painful
[00:17:50.880 --> 00:17:53.800]   and inefficient process of evolution is needed
[00:17:53.800 --> 00:17:55.840]   to come up with this set of abstractions
[00:17:55.840 --> 00:17:57.760]   that ultimately lead to intelligence?
[00:17:57.760 --> 00:18:00.760]   Do you think there's a shortcut
[00:18:00.760 --> 00:18:02.400]   or do you think we have to create
[00:18:02.400 --> 00:18:04.600]   something like our universe
[00:18:04.600 --> 00:18:07.720]   in order to create something like human level intelligence?
[00:18:07.720 --> 00:18:13.120]   - So far, the only example we have is this one,
[00:18:13.120 --> 00:18:15.400]   this universe in which we are living.
[00:18:15.400 --> 00:18:16.560]   - Do you think you can do better?
[00:18:16.560 --> 00:18:25.040]   - Maybe not, but we are part of this whole process.
[00:18:25.040 --> 00:18:30.000]   So apparently, so it might be the case
[00:18:30.000 --> 00:18:33.680]   that the code that runs the universe is really, really simple.
[00:18:33.680 --> 00:18:36.680]   Everything points to that possibility
[00:18:36.680 --> 00:18:39.960]   because gravity and other basic forces
[00:18:39.960 --> 00:18:44.120]   are really simple laws that can be easily described
[00:18:44.120 --> 00:18:46.480]   also in just a few lines of code basically.
[00:18:46.480 --> 00:18:52.200]   And then there are these other events
[00:18:52.200 --> 00:18:55.080]   that the apparently random events
[00:18:55.080 --> 00:18:56.560]   in the history of the universe,
[00:18:56.560 --> 00:18:58.800]   which as far as we know at the moment
[00:18:58.800 --> 00:19:01.360]   don't have a compact code, but who knows,
[00:19:01.360 --> 00:19:03.240]   maybe somebody in the near future
[00:19:03.240 --> 00:19:06.840]   is going to figure out the pseudo random generator
[00:19:06.840 --> 00:19:11.840]   which is computing whether the measurement
[00:19:11.840 --> 00:19:16.000]   of that spin up or down thing here
[00:19:16.000 --> 00:19:18.520]   is going to be positive or negative.
[00:19:18.520 --> 00:19:20.000]   - Underlying quantum mechanics.
[00:19:20.000 --> 00:19:20.840]   - Yes.
[00:19:20.840 --> 00:19:23.240]   - So you ultimately think quantum mechanics
[00:19:23.240 --> 00:19:25.280]   is a pseudo random number generator,
[00:19:25.280 --> 00:19:27.000]   so it's all deterministic.
[00:19:27.000 --> 00:19:28.880]   There's no randomness in our universe.
[00:19:28.880 --> 00:19:31.880]   Does God play dice?
[00:19:31.880 --> 00:19:36.200]   - So a couple of years ago, a famous physicist,
[00:19:37.080 --> 00:19:39.520]   quantum physicist, Anton Zeilinger,
[00:19:39.520 --> 00:19:42.160]   he wrote an essay in "Nature"
[00:19:42.160 --> 00:19:44.840]   and it started more or less like that.
[00:19:44.840 --> 00:19:52.280]   One of the fundamental insights of the 20th century
[00:19:52.280 --> 00:19:58.880]   was that the universe is fundamentally random
[00:19:58.880 --> 00:20:01.280]   on the quantum level.
[00:20:02.920 --> 00:20:06.800]   And that whenever you measure spin up or down
[00:20:06.800 --> 00:20:08.280]   or something like that,
[00:20:08.280 --> 00:20:12.320]   a new bit of information enters the history of the universe.
[00:20:12.320 --> 00:20:16.000]   And while I was reading that,
[00:20:16.000 --> 00:20:19.320]   I was already typing the response
[00:20:19.320 --> 00:20:21.720]   and they had to publish it because I was right.
[00:20:21.720 --> 00:20:27.880]   That there is no evidence, no physical evidence for that.
[00:20:27.880 --> 00:20:30.440]   So there's an alternative explanation
[00:20:30.440 --> 00:20:33.480]   where everything that we consider random
[00:20:33.480 --> 00:20:35.760]   is actually pseudo random,
[00:20:35.760 --> 00:20:40.760]   such as the decimal expansion of pi, 3.141 and so on,
[00:20:40.760 --> 00:20:44.360]   which looks random, but isn't.
[00:20:44.360 --> 00:20:49.360]   So pi is interesting because every three digit sequence,
[00:20:49.360 --> 00:20:52.240]   every sequence of three digits
[00:20:52.240 --> 00:20:55.960]   appears roughly one in a thousand times.
[00:20:56.000 --> 00:20:59.960]   And every five digit sequence
[00:20:59.960 --> 00:21:03.480]   appears roughly one in 10,000 times.
[00:21:03.480 --> 00:21:07.040]   What you would expect if it was random.
[00:21:07.040 --> 00:21:09.320]   But there's a very short algorithm,
[00:21:09.320 --> 00:21:11.680]   a short program that computes all of that.
[00:21:11.680 --> 00:21:13.880]   So it's extremely compressible.
[00:21:13.880 --> 00:21:15.840]   And who knows, maybe tomorrow somebody,
[00:21:15.840 --> 00:21:17.560]   some grad student at CERN
[00:21:17.560 --> 00:21:20.960]   goes back over all these data points,
[00:21:20.960 --> 00:21:22.600]   better decay and whatever,
[00:21:22.600 --> 00:21:27.280]   and figures out, oh, it's the second billion digits of pi
[00:21:27.280 --> 00:21:28.520]   or something like that.
[00:21:28.520 --> 00:21:31.520]   We don't have any fundamental reason at the moment
[00:21:31.520 --> 00:21:36.120]   to believe that this is truly random
[00:21:36.120 --> 00:21:39.080]   and not just a deterministic video game.
[00:21:39.080 --> 00:21:41.120]   If it was a deterministic video game,
[00:21:41.120 --> 00:21:43.120]   it would be much more beautiful
[00:21:43.120 --> 00:21:46.680]   because beauty is simplicity.
[00:21:46.680 --> 00:21:49.840]   And many of the basic laws of the universe,
[00:21:49.840 --> 00:21:54.120]   like gravity and the other basic forces are very simple.
[00:21:54.120 --> 00:21:58.080]   So very short programs can explain what these are doing.
[00:21:58.080 --> 00:22:03.360]   And it would be awful and ugly.
[00:22:03.360 --> 00:22:04.560]   The universe would be ugly.
[00:22:04.560 --> 00:22:06.760]   The history of the universe would be ugly
[00:22:06.760 --> 00:22:08.160]   if for the extra things,
[00:22:08.160 --> 00:22:12.880]   the seemingly random data points that we get all the time,
[00:22:12.880 --> 00:22:17.920]   that we really need a huge number of extra bits
[00:22:17.920 --> 00:22:22.920]   to describe all these extra bits of information.
[00:22:22.920 --> 00:22:27.400]   So as long as we don't have evidence
[00:22:27.400 --> 00:22:29.760]   that there is no short program
[00:22:29.760 --> 00:22:34.000]   that computes the entire history of the entire universe,
[00:22:34.000 --> 00:22:40.360]   we are, as scientists, compelled to look further
[00:22:40.360 --> 00:22:42.760]   for that shortest program.
[00:22:42.760 --> 00:22:47.800]   - Your intuition says there exists a program
[00:22:47.800 --> 00:22:52.000]   that can backtrack to the creation of the universe.
[00:22:52.000 --> 00:22:54.280]   So compute the shortest path to the creation of the universe.
[00:22:54.280 --> 00:22:58.440]   - Yes, including all the entanglement things
[00:22:58.440 --> 00:23:02.480]   and all the spin up and down measurements
[00:23:02.480 --> 00:23:05.760]   that have been taken place
[00:23:05.760 --> 00:23:10.640]   since 13.8 billion years ago.
[00:23:10.640 --> 00:23:11.840]   And so, yeah.
[00:23:11.840 --> 00:23:15.720]   So we don't have a proof that it is random.
[00:23:15.720 --> 00:23:19.600]   We don't have a proof that it is compressible
[00:23:19.600 --> 00:23:20.800]   to a short program.
[00:23:20.800 --> 00:23:22.400]   But as long as we don't have that proof,
[00:23:22.400 --> 00:23:25.000]   we are obliged as scientists
[00:23:25.000 --> 00:23:27.600]   to keep looking for that simple explanation.
[00:23:27.600 --> 00:23:28.440]   - Absolutely.
[00:23:28.440 --> 00:23:31.600]   So you said simplicity is beautiful or beauty is simple.
[00:23:31.600 --> 00:23:33.280]   Either one works.
[00:23:33.280 --> 00:23:37.080]   But you also work on curiosity, discovery,
[00:23:37.080 --> 00:23:42.840]   the romantic notion of randomness, of serendipity,
[00:23:42.840 --> 00:23:47.840]   of being surprised by things that are about you,
[00:23:47.840 --> 00:23:53.440]   kind of in our poetic notion of reality,
[00:23:53.440 --> 00:23:56.440]   we think as humans require randomness.
[00:23:56.440 --> 00:23:59.040]   So you don't find randomness beautiful.
[00:23:59.040 --> 00:24:04.040]   You find simple determinism beautiful.
[00:24:04.040 --> 00:24:05.720]   - Yeah.
[00:24:05.720 --> 00:24:07.720]   - Okay.
[00:24:07.720 --> 00:24:08.600]   - So why?
[00:24:08.600 --> 00:24:09.440]   - Why?
[00:24:09.440 --> 00:24:13.080]   Because the explanation becomes shorter.
[00:24:13.080 --> 00:24:18.080]   A universe that is compressible to a short program
[00:24:18.080 --> 00:24:22.920]   is much more elegant and much more beautiful
[00:24:22.920 --> 00:24:26.280]   than another one, which needs an almost infinite number
[00:24:26.280 --> 00:24:27.880]   of bits to be described.
[00:24:27.880 --> 00:24:30.160]   As far as we know,
[00:24:30.160 --> 00:24:33.960]   many things that are happening in this universe
[00:24:33.960 --> 00:24:37.520]   are really simple in terms of short programs
[00:24:37.520 --> 00:24:41.160]   that compute gravity and the interaction
[00:24:41.160 --> 00:24:43.600]   between elementary particles and so on.
[00:24:43.600 --> 00:24:45.800]   So all of that seems to be very, very simple.
[00:24:45.800 --> 00:24:50.240]   Every electron seems to reuse the same sub-program
[00:24:50.240 --> 00:24:52.280]   all the time as it is interacting
[00:24:52.280 --> 00:24:54.600]   with other elementary particles.
[00:24:54.600 --> 00:25:03.400]   If we now require an extra oracle
[00:25:04.440 --> 00:25:07.800]   injecting new bits of information all the time
[00:25:07.800 --> 00:25:11.960]   for these extra things which are currently not understood,
[00:25:11.960 --> 00:25:16.040]   such as better decay,
[00:25:16.040 --> 00:25:23.640]   then the whole description length
[00:25:23.640 --> 00:25:26.560]   of the data that we can observe of the history
[00:25:26.560 --> 00:25:31.560]   of the universe would become much longer
[00:25:31.560 --> 00:25:33.720]   and therefore uglier.
[00:25:33.720 --> 00:25:34.920]   - And uglier.
[00:25:34.920 --> 00:25:38.000]   Again, simplicity is elegant and beautiful.
[00:25:38.000 --> 00:25:40.840]   - The history of science is a history
[00:25:40.840 --> 00:25:42.720]   of compression progress.
[00:25:42.720 --> 00:25:46.960]   - Yeah, so you've described sort of
[00:25:46.960 --> 00:25:48.680]   as we build up abstractions
[00:25:48.680 --> 00:25:52.120]   and you've talked about the idea of compression.
[00:25:52.120 --> 00:25:55.560]   How do you see this, the history of science,
[00:25:55.560 --> 00:25:58.600]   the history of humanity, our civilization,
[00:25:58.600 --> 00:26:02.000]   and life on Earth as some kind of path
[00:26:02.000 --> 00:26:04.120]   towards greater and greater compression?
[00:26:04.120 --> 00:26:05.040]   What do you mean by that?
[00:26:05.040 --> 00:26:06.960]   How do you think about that?
[00:26:06.960 --> 00:26:09.240]   - Indeed, the history of science
[00:26:09.240 --> 00:26:13.080]   is a history of compression progress.
[00:26:13.080 --> 00:26:14.640]   What does that mean?
[00:26:14.640 --> 00:26:18.000]   Hundreds of years ago, there was an astronomer
[00:26:18.000 --> 00:26:20.080]   whose name was Kepler,
[00:26:20.080 --> 00:26:22.560]   and he looked at the data points that he got
[00:26:22.560 --> 00:26:25.800]   by watching planets move,
[00:26:25.800 --> 00:26:27.560]   and then he had all these data points,
[00:26:27.560 --> 00:26:28.760]   and suddenly it turned out
[00:26:28.760 --> 00:26:31.920]   that he can greatly compress the data
[00:26:31.920 --> 00:26:36.920]   by predicting it through an ellipse law.
[00:26:36.920 --> 00:26:39.840]   So it turns out that all these data points
[00:26:39.840 --> 00:26:43.680]   are more or less on ellipses around the sun.
[00:26:43.680 --> 00:26:49.600]   And another guy came along whose name was Newton,
[00:26:49.600 --> 00:26:51.440]   and before him Hooke,
[00:26:51.440 --> 00:26:53.680]   and they said the same thing
[00:26:53.680 --> 00:26:58.480]   that is making these planets move like that
[00:26:58.480 --> 00:27:01.880]   is what makes the apples fall down.
[00:27:01.880 --> 00:27:05.520]   And it also holds for stones
[00:27:05.520 --> 00:27:09.800]   and for all kinds of other objects.
[00:27:09.800 --> 00:27:15.080]   And suddenly, many, many of these observations
[00:27:15.080 --> 00:27:16.960]   became much more compressible,
[00:27:16.960 --> 00:27:19.960]   because as long as you can predict the next thing,
[00:27:19.960 --> 00:27:21.720]   given what you have seen so far,
[00:27:21.720 --> 00:27:22.640]   you can compress it,
[00:27:22.640 --> 00:27:25.320]   but you don't have to store that data extra.
[00:27:25.320 --> 00:27:27.680]   This is called predictive coding.
[00:27:29.240 --> 00:27:31.400]   And then there was still something wrong
[00:27:31.400 --> 00:27:33.680]   with that theory of the universe,
[00:27:33.680 --> 00:27:37.600]   and you had deviations from these predictions of the theory.
[00:27:37.600 --> 00:27:40.240]   And 300 years later, another guy came along
[00:27:40.240 --> 00:27:41.880]   whose name was Einstein,
[00:27:41.880 --> 00:27:46.720]   and he was able to explain away all these deviations
[00:27:46.720 --> 00:27:51.160]   from the predictions of the old theory
[00:27:51.160 --> 00:27:52.920]   through a new theory,
[00:27:52.920 --> 00:27:56.960]   which was called the general theory of relativity,
[00:27:56.960 --> 00:28:00.760]   which at first glance looks a little bit more complicated,
[00:28:00.760 --> 00:28:02.760]   and you have to warp space and time,
[00:28:02.760 --> 00:28:05.600]   but you can phrase it within one single sentence,
[00:28:05.600 --> 00:28:09.000]   which is no matter how fast you accelerate
[00:28:09.000 --> 00:28:12.920]   and how fast or hard you decelerate,
[00:28:12.920 --> 00:28:17.920]   and no matter what is the gravity in your local framework,
[00:28:17.920 --> 00:28:21.360]   light speed always looks the same.
[00:28:21.360 --> 00:28:24.280]   And from that, you can calculate all the consequences.
[00:28:24.280 --> 00:28:25.760]   So it's a very simple thing,
[00:28:25.760 --> 00:28:30.360]   and it allows you to further compress all the observations
[00:28:30.360 --> 00:28:35.360]   because certainly there are hardly any deviations any longer
[00:28:35.360 --> 00:28:37.800]   that you can measure from the predictions
[00:28:37.800 --> 00:28:40.080]   of this new theory.
[00:28:40.080 --> 00:28:44.560]   So all of science is a history of compression progress.
[00:28:44.560 --> 00:28:47.000]   You never arrive immediately
[00:28:47.000 --> 00:28:50.800]   at the shortest explanation of the data,
[00:28:50.800 --> 00:28:52.560]   but you're making progress.
[00:28:52.560 --> 00:28:54.360]   Whenever you are making progress,
[00:28:54.360 --> 00:28:56.320]   you have an insight.
[00:28:56.320 --> 00:28:59.520]   You see, oh, first I needed so many bits of information
[00:28:59.520 --> 00:29:02.840]   to describe the data, to describe my falling apples,
[00:29:02.840 --> 00:29:04.200]   my video of falling apples.
[00:29:04.200 --> 00:29:05.800]   I need so many data.
[00:29:05.800 --> 00:29:08.200]   So many pixels have to be stored.
[00:29:08.200 --> 00:29:10.120]   But then suddenly I realized, no,
[00:29:10.120 --> 00:29:13.920]   there is a very simple way of predicting the third frame
[00:29:13.920 --> 00:29:16.080]   in the video from the first two.
[00:29:16.080 --> 00:29:20.120]   And maybe not every little detail can be predicted,
[00:29:20.120 --> 00:29:22.680]   but more or less, most of these orange blobs
[00:29:22.680 --> 00:29:25.880]   that are coming down, they accelerate in the same way,
[00:29:25.880 --> 00:29:28.640]   which means that I can greatly compress the video.
[00:29:28.640 --> 00:29:33.320]   And the amount of compression progress,
[00:29:33.320 --> 00:29:35.680]   that is the depth of the insight
[00:29:35.680 --> 00:29:37.360]   that you have at that moment.
[00:29:37.360 --> 00:29:40.280]   That's the fun that you have, the scientific fun,
[00:29:40.280 --> 00:29:43.040]   the fun in that discovery.
[00:29:43.040 --> 00:29:46.080]   And we can build artificial systems that do the same thing.
[00:29:46.080 --> 00:29:49.080]   They measure the depth of their insights
[00:29:49.080 --> 00:29:50.800]   as they are looking at the data,
[00:29:50.800 --> 00:29:53.520]   which is coming in through their own experiments.
[00:29:53.520 --> 00:29:57.120]   And we give them a reward, an intrinsic reward,
[00:29:57.120 --> 00:30:01.200]   in proportion to this depth of insight.
[00:30:01.200 --> 00:30:08.160]   And since they are trying to maximize the rewards they get,
[00:30:08.160 --> 00:30:11.160]   they are suddenly motivated to come up with new action
[00:30:11.160 --> 00:30:15.760]   sequences, with new experiments that have the property
[00:30:15.760 --> 00:30:18.800]   that the data that is coming in as a consequence
[00:30:18.800 --> 00:30:21.800]   of these experiments has the property
[00:30:21.800 --> 00:30:24.040]   that they can learn something about,
[00:30:24.040 --> 00:30:28.760]   see a pattern in there which they hadn't seen yet before.
[00:30:28.760 --> 00:30:32.080]   - So there's an idea of power play that you've described,
[00:30:32.080 --> 00:30:35.320]   a training in general problem solver in this kind of way
[00:30:35.320 --> 00:30:37.360]   of looking for the unsolved problems.
[00:30:37.360 --> 00:30:38.200]   - Yeah.
[00:30:38.200 --> 00:30:40.440]   - Can you describe that idea a little further?
[00:30:40.440 --> 00:30:42.440]   - It's another very simple idea.
[00:30:42.440 --> 00:30:44.840]   So normally what you do in computer science,
[00:30:44.840 --> 00:30:50.280]   you have some guy who gives you a problem,
[00:30:50.280 --> 00:30:55.920]   and then there is a huge search space of potential solution
[00:30:55.920 --> 00:30:56.880]   candidates.
[00:30:56.880 --> 00:31:00.680]   And you somehow try them out, and you
[00:31:00.680 --> 00:31:02.680]   have more or less sophisticated ways
[00:31:02.680 --> 00:31:06.960]   of moving around in that search space
[00:31:06.960 --> 00:31:10.320]   until you finally found a solution which
[00:31:10.320 --> 00:31:13.040]   you consider satisfactory.
[00:31:13.040 --> 00:31:15.840]   That's what most of computer science is about.
[00:31:15.840 --> 00:31:20.000]   Power play just goes one little step further and says,
[00:31:20.000 --> 00:31:24.640]   let's not only search for solutions to a given problem,
[00:31:24.640 --> 00:31:30.600]   but let's search to pairs of problems and their solutions
[00:31:30.600 --> 00:31:33.160]   where the system itself has the opportunity
[00:31:33.160 --> 00:31:37.320]   to phrase its own problem.
[00:31:37.320 --> 00:31:41.000]   So we are looking suddenly at pairs of problems
[00:31:41.000 --> 00:31:45.480]   and their solutions or modifications of the problem
[00:31:45.480 --> 00:31:47.640]   solver that is supposed to generate
[00:31:47.640 --> 00:31:51.000]   a solution to that new problem.
[00:31:51.000 --> 00:31:56.800]   And this additional degree of freedom
[00:31:56.800 --> 00:32:00.440]   allows us to build curious systems that
[00:32:00.440 --> 00:32:04.360]   are like scientists in the sense that they not only try
[00:32:04.360 --> 00:32:08.200]   to solve and try to find answers to existing questions,
[00:32:08.200 --> 00:32:13.320]   no, they are also free to pose their own questions.
[00:32:13.320 --> 00:32:15.360]   So if you want to build an artificial scientist,
[00:32:15.360 --> 00:32:16.840]   you have to give it that freedom,
[00:32:16.840 --> 00:32:19.520]   and power play is exactly doing that.
[00:32:19.520 --> 00:32:23.000]   So that's a dimension of freedom that's important to have.
[00:32:23.000 --> 00:32:27.360]   But how hard do you think that--
[00:32:27.360 --> 00:32:30.800]   how multidimensional and difficult
[00:32:30.800 --> 00:32:34.240]   the space of then coming up with your own questions is?
[00:32:34.240 --> 00:32:36.920]   So it's one of the things that as human beings
[00:32:36.920 --> 00:32:39.880]   we consider to be the thing that makes us special,
[00:32:39.880 --> 00:32:42.200]   the intelligence that makes us special,
[00:32:42.200 --> 00:32:46.920]   is that brilliant insight that can create something totally
[00:32:46.920 --> 00:32:47.600]   new.
[00:32:47.600 --> 00:32:48.800]   Yes.
[00:32:48.800 --> 00:32:51.280]   So now let's look at the extreme case.
[00:32:51.280 --> 00:32:55.560]   Let's look at the set of all possible problems
[00:32:55.560 --> 00:33:01.160]   that you can formally describe, which is infinite, which
[00:33:01.160 --> 00:33:06.520]   should be the next problem that a scientist or a power play
[00:33:06.520 --> 00:33:08.120]   is going to solve.
[00:33:08.120 --> 00:33:14.480]   Well, it should be the easiest problem that goes
[00:33:14.480 --> 00:33:17.560]   beyond what you already know.
[00:33:17.560 --> 00:33:20.720]   So it should be the simplest problem
[00:33:20.720 --> 00:33:23.040]   that the current problem solver that you have,
[00:33:23.040 --> 00:33:26.600]   which can already solve 100 problems,
[00:33:26.600 --> 00:33:31.080]   that he cannot solve yet by just generalizing.
[00:33:31.080 --> 00:33:32.400]   So it has to be new.
[00:33:32.400 --> 00:33:35.120]   So it has to require a modification of the problem
[00:33:35.120 --> 00:33:37.400]   solver such that the new problem solver can
[00:33:37.400 --> 00:33:41.560]   solve this new thing, but the old problem solver cannot do it.
[00:33:41.560 --> 00:33:45.760]   And in addition to that, we have to make sure
[00:33:45.760 --> 00:33:48.720]   that the problem solver doesn't forget
[00:33:48.720 --> 00:33:50.200]   any of the previous solutions.
[00:33:50.200 --> 00:33:51.200]   Right.
[00:33:51.200 --> 00:33:53.720]   And so by definition, power play is now
[00:33:53.720 --> 00:33:57.480]   trying always to search in this pair of--
[00:33:57.480 --> 00:34:01.600]   in the set of pairs of problems and problem solver
[00:34:01.600 --> 00:34:06.600]   modifications for a combination that minimize the time
[00:34:06.600 --> 00:34:08.160]   to achieve these criteria.
[00:34:08.160 --> 00:34:12.160]   So it's always trying to find the problem which is easiest
[00:34:12.160 --> 00:34:14.720]   to add to the repertoire.
[00:34:14.720 --> 00:34:18.880]   So just like grad students and academics and researchers
[00:34:18.880 --> 00:34:22.200]   can spend their whole career in a local minima,
[00:34:22.200 --> 00:34:25.860]   stuck trying to come up with interesting questions,
[00:34:25.860 --> 00:34:27.600]   but ultimately doing very little,
[00:34:27.600 --> 00:34:31.880]   do you think it's easy in this approach of looking
[00:34:31.880 --> 00:34:33.760]   for the simplest unsolvable problem
[00:34:33.760 --> 00:34:35.520]   to get stuck in a local minima?
[00:34:35.520 --> 00:34:40.560]   Is never really discovering new--
[00:34:40.560 --> 00:34:42.600]   really jumping outside of the 100 problems
[00:34:42.600 --> 00:34:47.600]   that you've already solved in a genuine creative way?
[00:34:47.600 --> 00:34:49.920]   No, because that's the nature of power play,
[00:34:49.920 --> 00:34:53.960]   that it's always trying to break its current generalization
[00:34:53.960 --> 00:34:58.120]   abilities by coming up with a new problem which
[00:34:58.120 --> 00:35:00.920]   is beyond the current horizon.
[00:35:00.920 --> 00:35:04.440]   Just shifting the horizon of knowledge a little bit
[00:35:04.440 --> 00:35:08.040]   out there, breaking the existing rules,
[00:35:08.040 --> 00:35:10.960]   such that the new thing becomes solvable,
[00:35:10.960 --> 00:35:13.280]   but wasn't solvable by the old thing.
[00:35:13.280 --> 00:35:16.400]   So like adding a new axiom, like what
[00:35:16.400 --> 00:35:20.640]   Gödel did when he came up with these new sentences,
[00:35:20.640 --> 00:35:23.840]   new theorems that didn't have a proof in the formal system,
[00:35:23.840 --> 00:35:27.680]   which means you can add them to the repertoire,
[00:35:27.680 --> 00:35:33.440]   hoping that they are not going to damage the consistency
[00:35:33.440 --> 00:35:35.880]   of the whole thing.
[00:35:35.880 --> 00:35:39.640]   So in the paper with the amazing title,
[00:35:39.640 --> 00:35:44.320]   Formal Theory of Creativity, Fun and Intrinsic Motivation,
[00:35:44.320 --> 00:35:47.720]   you talk about discovery as intrinsic reward.
[00:35:47.720 --> 00:35:51.640]   So if you view humans as intelligent agents,
[00:35:51.640 --> 00:35:54.880]   what do you think is the purpose and meaning of life
[00:35:54.880 --> 00:35:56.880]   for us humans?
[00:35:56.880 --> 00:35:58.800]   You've talked about this discovery.
[00:35:58.800 --> 00:36:04.200]   Do you see humans as an instance of power play, agents?
[00:36:04.200 --> 00:36:09.160]   Yeah, so humans are curious, and that
[00:36:09.160 --> 00:36:11.520]   means they behave like scientists,
[00:36:11.520 --> 00:36:13.160]   not only the official scientists,
[00:36:13.160 --> 00:36:15.800]   but even the babies behave like scientists,
[00:36:15.800 --> 00:36:18.280]   and they play around with their toys
[00:36:18.280 --> 00:36:20.040]   to figure out how the world works
[00:36:20.040 --> 00:36:23.520]   and how it is responding to their actions.
[00:36:23.520 --> 00:36:27.320]   And that's how they learn about gravity and everything.
[00:36:27.320 --> 00:36:30.960]   And yeah, in 1990, we had the first systems like that,
[00:36:30.960 --> 00:36:34.200]   which just tried to play around with the environment
[00:36:34.200 --> 00:36:38.280]   and come up with situations that go beyond what
[00:36:38.280 --> 00:36:40.720]   they knew at that time, and then get
[00:36:40.720 --> 00:36:42.720]   a reward for creating these situations,
[00:36:42.720 --> 00:36:45.800]   and then becoming more general problem solvers
[00:36:45.800 --> 00:36:48.960]   and being able to understand more of the world.
[00:36:48.960 --> 00:36:59.920]   So yeah, I think in principle, that curiosity strategy
[00:36:59.920 --> 00:37:03.240]   or more sophisticated versions of what I just described,
[00:37:03.240 --> 00:37:06.480]   they are what we have built in as well,
[00:37:06.480 --> 00:37:10.200]   because evolution discovered that's a good way of exploring
[00:37:10.200 --> 00:37:11.440]   the unknown world.
[00:37:11.440 --> 00:37:13.440]   And a guy who explores the unknown world
[00:37:13.440 --> 00:37:17.000]   has a higher chance of solving problems
[00:37:17.000 --> 00:37:19.480]   that he needs to survive in this world.
[00:37:19.480 --> 00:37:23.760]   On the other hand, those guys who were too curious,
[00:37:23.760 --> 00:37:25.360]   they were weeded out as well.
[00:37:25.360 --> 00:37:27.200]   So you have to find this trade-off.
[00:37:27.200 --> 00:37:29.240]   Evolution found a certain trade-off.
[00:37:29.240 --> 00:37:31.960]   Apparently, in our society, there
[00:37:31.960 --> 00:37:36.120]   is a certain percentage of extremely explorative guys.
[00:37:36.120 --> 00:37:38.040]   And it doesn't matter if they die,
[00:37:38.040 --> 00:37:42.280]   because many of the others are more conservative.
[00:37:42.280 --> 00:37:46.480]   And so yeah, it would be surprising to me
[00:37:46.480 --> 00:37:54.440]   if that principle of artificial curiosity
[00:37:54.440 --> 00:37:58.400]   wouldn't be present in almost exactly the same form here.
[00:37:58.400 --> 00:37:59.840]   In our brains.
[00:37:59.840 --> 00:38:03.080]   So you're a bit of a musician and an artist.
[00:38:03.080 --> 00:38:07.600]   So continuing on this topic of creativity,
[00:38:07.600 --> 00:38:10.440]   what do you think is the role of creativity in intelligence?
[00:38:10.440 --> 00:38:12.360]   So you've kind of implied that it's
[00:38:12.360 --> 00:38:17.520]   essential for intelligence, if you think of intelligence
[00:38:17.520 --> 00:38:23.280]   as a problem-solving system, as ability to solve problems.
[00:38:23.280 --> 00:38:28.920]   But do you think it's essential, this idea of creativity?
[00:38:28.920 --> 00:38:31.760]   We never have a program, a sub-program,
[00:38:31.760 --> 00:38:34.400]   that is called creativity or something.
[00:38:34.400 --> 00:38:37.960]   It's just a side effect of what our problem solvers do.
[00:38:37.960 --> 00:38:40.200]   They are searching a space of problems,
[00:38:40.200 --> 00:38:44.600]   or a space of candidates, of solution candidates,
[00:38:44.600 --> 00:38:48.160]   until they hopefully find a solution to a given problem.
[00:38:48.160 --> 00:38:50.520]   But then there are these two types of creativity.
[00:38:50.520 --> 00:38:54.280]   And both of them are now present in our machines.
[00:38:54.280 --> 00:38:56.480]   The first one has been around for a long time,
[00:38:56.480 --> 00:38:59.640]   which is human gives problem to machine,
[00:38:59.640 --> 00:39:03.360]   machine tries to find a solution to that.
[00:39:03.360 --> 00:39:05.920]   And this has been happening for many decades.
[00:39:05.920 --> 00:39:07.400]   And for many decades, machines have
[00:39:07.400 --> 00:39:11.400]   found creative solutions to interesting problems,
[00:39:11.400 --> 00:39:16.560]   where humans were not aware of these particularly
[00:39:16.560 --> 00:39:18.920]   creative solutions, but then appreciated
[00:39:18.920 --> 00:39:21.760]   that the machine found that.
[00:39:21.760 --> 00:39:23.760]   The second is the pure creativity.
[00:39:23.760 --> 00:39:25.640]   That I would call, what I just mentioned,
[00:39:25.640 --> 00:39:29.040]   I would call the applied creativity,
[00:39:29.040 --> 00:39:31.640]   like applied art, where somebody tells you,
[00:39:31.640 --> 00:39:35.160]   now make a nice picture of this pope,
[00:39:35.160 --> 00:39:37.240]   and you will get money for that.
[00:39:37.240 --> 00:39:41.240]   So here is the artist, and he makes a convincing picture
[00:39:41.240 --> 00:39:45.880]   of the pope, and the pope likes it and gives him the money.
[00:39:45.880 --> 00:39:48.720]   And then there is the pure creativity,
[00:39:48.720 --> 00:39:50.440]   which is more like the power play
[00:39:50.440 --> 00:39:52.760]   and the artificial curiosity thing, where
[00:39:52.760 --> 00:39:57.040]   you have the freedom to select your own problem,
[00:39:57.040 --> 00:40:03.400]   like a scientist who defines his own question to study.
[00:40:03.400 --> 00:40:07.960]   And so that is the pure creativity, if you will,
[00:40:07.960 --> 00:40:11.400]   as opposed to the applied creativity,
[00:40:11.400 --> 00:40:14.360]   which serves another.
[00:40:14.360 --> 00:40:15.720]   - And in that distinction, there's
[00:40:15.720 --> 00:40:19.160]   almost echoes of narrow AI versus general AI.
[00:40:19.160 --> 00:40:22.720]   So this kind of constrained painting of a pope
[00:40:22.720 --> 00:40:28.440]   seems like the approaches of what people are calling
[00:40:28.440 --> 00:40:29.760]   narrow AI.
[00:40:29.760 --> 00:40:33.360]   And pure creativity seems to be--
[00:40:33.360 --> 00:40:35.000]   maybe I'm just biased as a human,
[00:40:35.000 --> 00:40:38.440]   but it seems to be an essential element
[00:40:38.440 --> 00:40:41.120]   of human-level intelligence.
[00:40:41.120 --> 00:40:46.040]   Is that what you're implying, to a degree?
[00:40:46.040 --> 00:40:48.520]   - If you zoom back a little bit, and you just
[00:40:48.520 --> 00:40:51.480]   look at a general problem-solving machine, which
[00:40:51.480 --> 00:40:53.600]   is trying to solve arbitrary problems,
[00:40:53.600 --> 00:40:56.320]   then this machine will figure out
[00:40:56.320 --> 00:40:58.240]   in the course of solving problems
[00:40:58.240 --> 00:41:00.160]   that it's good to be curious.
[00:41:00.160 --> 00:41:05.320]   So all of what I said just now about this pre-wired curiosity
[00:41:05.320 --> 00:41:09.040]   and this will to invent new problems that the system
[00:41:09.040 --> 00:41:13.280]   doesn't know how to solve yet should be just a byproduct
[00:41:13.280 --> 00:41:15.040]   of the general search.
[00:41:15.040 --> 00:41:21.840]   However, apparently, evolution has built it into us,
[00:41:21.840 --> 00:41:25.080]   because it turned out to be so successful,
[00:41:25.080 --> 00:41:30.440]   pre-wiring, a bias, a very successful exploratory bias
[00:41:30.440 --> 00:41:33.880]   that we are born with.
[00:41:33.880 --> 00:41:35.680]   - And you've also said that consciousness
[00:41:35.680 --> 00:41:41.200]   in the same kind of way may be a byproduct of problem solving.
[00:41:41.200 --> 00:41:44.720]   Do you think-- do you find this an interesting byproduct?
[00:41:44.720 --> 00:41:47.040]   Do you think it's a useful byproduct?
[00:41:47.040 --> 00:41:49.320]   What are your thoughts on consciousness in general?
[00:41:49.320 --> 00:41:53.120]   Or is it simply a byproduct of greater and greater
[00:41:53.120 --> 00:41:59.160]   capabilities of problem solving that's similar to creativity
[00:41:59.160 --> 00:42:00.920]   in that sense?
[00:42:00.920 --> 00:42:04.200]   - Yeah, we never have a procedure called consciousness
[00:42:04.200 --> 00:42:05.320]   in our machines.
[00:42:05.320 --> 00:42:08.720]   However, we get as side effects of what
[00:42:08.720 --> 00:42:11.880]   these machines are doing things that
[00:42:11.880 --> 00:42:16.720]   seem to be closely related to what people call consciousness.
[00:42:16.720 --> 00:42:19.880]   So for example, already in 1990, we
[00:42:19.880 --> 00:42:24.160]   had simple systems, which were basically recurrent networks,
[00:42:24.160 --> 00:42:26.200]   and therefore universal computers,
[00:42:26.200 --> 00:42:33.960]   trying to map incoming data into actions that lead to success.
[00:42:33.960 --> 00:42:36.720]   Maximizing reward in a given environment,
[00:42:36.720 --> 00:42:40.400]   always finding the charging station in time
[00:42:40.400 --> 00:42:42.720]   whenever the battery is low and negative signals are coming
[00:42:42.720 --> 00:42:47.240]   from the battery, always find the charging station in time
[00:42:47.240 --> 00:42:50.520]   without bumping against painful obstacles on the way.
[00:42:50.520 --> 00:42:54.720]   So complicated things, but very easily motivated.
[00:42:54.720 --> 00:42:59.200]   And then we give these little guys
[00:42:59.200 --> 00:43:01.920]   a separate recurrent neural network, which
[00:43:01.920 --> 00:43:03.680]   is just predicting what's happening
[00:43:03.680 --> 00:43:04.800]   if I do that and that.
[00:43:04.800 --> 00:43:08.240]   What will happen as a consequence of these actions
[00:43:08.240 --> 00:43:09.360]   that I'm executing?
[00:43:09.360 --> 00:43:11.600]   And it's just trained on the long and long history
[00:43:11.600 --> 00:43:14.040]   of interactions with the world.
[00:43:14.040 --> 00:43:18.120]   So it becomes a predictive model of the world, basically.
[00:43:18.120 --> 00:43:22.720]   And therefore, also a compressor of the observations
[00:43:22.720 --> 00:43:25.280]   of the world, because whatever you can predict,
[00:43:25.280 --> 00:43:26.560]   you don't have to store extra.
[00:43:26.560 --> 00:43:30.600]   So compression is a side effect of prediction.
[00:43:30.600 --> 00:43:33.240]   And how does this recurrent network compress?
[00:43:33.240 --> 00:43:35.720]   Well, it's inventing little subprograms,
[00:43:35.720 --> 00:43:39.960]   little subnetworks that stand for everything that frequently
[00:43:39.960 --> 00:43:43.840]   appears in the environment, like bottles and microphones
[00:43:43.840 --> 00:43:48.120]   and faces, maybe lots of faces in my environment.
[00:43:48.120 --> 00:43:51.000]   So I'm learning to create something like a prototype
[00:43:51.000 --> 00:43:52.920]   face, and a new face comes along,
[00:43:52.920 --> 00:43:56.360]   and all I have to encode are the deviations from the prototype.
[00:43:56.360 --> 00:43:59.320]   So it's compressing all the time the stuff that frequently
[00:43:59.320 --> 00:44:00.880]   appears.
[00:44:00.880 --> 00:44:05.200]   There's one thing that appears all the time, that
[00:44:05.200 --> 00:44:08.000]   is present all the time when the agent is interacting
[00:44:08.000 --> 00:44:11.840]   with its environment, which is the agent itself.
[00:44:11.840 --> 00:44:14.520]   So just for data compression reasons,
[00:44:14.520 --> 00:44:18.640]   it is extremely natural for this recurrent network
[00:44:18.640 --> 00:44:21.080]   to come up with little subnetworks that
[00:44:21.080 --> 00:44:26.160]   stand for the properties of the agents, the hand,
[00:44:26.160 --> 00:44:29.080]   the other actuators, and all the stuff
[00:44:29.080 --> 00:44:32.560]   that you need to better encode the data which is influenced
[00:44:32.560 --> 00:44:34.360]   by the actions of the agent.
[00:44:34.360 --> 00:44:39.040]   So there, just as a side effect of data compression
[00:44:39.040 --> 00:44:45.800]   during problem solving, you have internal self-models.
[00:44:45.800 --> 00:44:51.360]   Now, you can use this model of the world to plan your future,
[00:44:51.360 --> 00:44:54.040]   and that's what you also have done since 1990.
[00:44:54.040 --> 00:44:57.880]   So the recurrent network, which is the controller, which
[00:44:57.880 --> 00:45:00.040]   is trying to maximize reward, can
[00:45:00.040 --> 00:45:01.840]   use this model of the network--
[00:45:01.840 --> 00:45:04.160]   of the world, this model network of the world,
[00:45:04.160 --> 00:45:05.620]   this predictive model of the world,
[00:45:05.620 --> 00:45:09.060]   to plan ahead and say, let's not do this action sequence.
[00:45:09.060 --> 00:45:11.340]   Let's do this action sequence instead,
[00:45:11.340 --> 00:45:14.580]   because it leads to more predicted reward.
[00:45:14.580 --> 00:45:18.900]   And whenever it's waking up these little subnetworks that
[00:45:18.900 --> 00:45:22.220]   stand for itself, then it's thinking about itself.
[00:45:22.220 --> 00:45:27.700]   Then it's thinking about itself, and it's exploring mentally
[00:45:27.700 --> 00:45:30.940]   the consequences of its own actions.
[00:45:30.940 --> 00:45:37.220]   And now you tell me what is still missing.
[00:45:37.220 --> 00:45:39.660]   Missing the next-- the gap to consciousness.
[00:45:39.660 --> 00:45:40.380]   Yeah.
[00:45:40.380 --> 00:45:41.060]   There isn't.
[00:45:41.060 --> 00:45:45.220]   That's a really beautiful idea that if life
[00:45:45.220 --> 00:45:47.300]   is a collection of data, and life
[00:45:47.300 --> 00:45:54.140]   is a process of compressing that data to act efficiently,
[00:45:54.140 --> 00:45:57.640]   in that data, you yourself appear very often.
[00:45:57.640 --> 00:46:00.900]   So it's useful to form compressions of yourself.
[00:46:00.900 --> 00:46:02.740]   It's a really beautiful formulation
[00:46:02.740 --> 00:46:05.700]   of what consciousness is as a necessary side effect.
[00:46:05.700 --> 00:46:11.420]   It's actually quite compelling to me.
[00:46:11.420 --> 00:46:16.500]   You've described RNNs, developed LSTMs,
[00:46:16.500 --> 00:46:20.340]   long short-term memory networks, that are type
[00:46:20.340 --> 00:46:22.140]   over current neural networks.
[00:46:22.140 --> 00:46:23.940]   They've gotten a lot of success recently.
[00:46:23.940 --> 00:46:28.540]   So these are networks that model the temporal aspects
[00:46:28.540 --> 00:46:31.140]   in the data, temporal patterns in the data.
[00:46:31.140 --> 00:46:33.860]   And you've called them the deepest
[00:46:33.860 --> 00:46:36.260]   of the neural networks, right?
[00:46:36.260 --> 00:46:39.780]   So what do you think is the value of depth in the models
[00:46:39.780 --> 00:46:41.220]   that we use to learn?
[00:46:41.220 --> 00:46:48.340]   Since you mentioned the long short-term memory and the LSTM,
[00:46:48.340 --> 00:46:51.500]   I have to mention the names of the brilliant students who
[00:46:51.500 --> 00:46:52.340]   made that possible.
[00:46:52.340 --> 00:46:53.380]   Yes, of course.
[00:46:53.380 --> 00:46:55.220]   First of all, my first student ever,
[00:46:55.220 --> 00:46:58.420]   Sepp Hochreiter, who had fundamental insights already
[00:46:58.420 --> 00:47:00.260]   in his diploma thesis.
[00:47:00.260 --> 00:47:03.660]   Then Felix Geers, who had additional important
[00:47:03.660 --> 00:47:04.620]   contributions.
[00:47:04.620 --> 00:47:08.100]   Alex Gray is a guy from Scotland who
[00:47:08.100 --> 00:47:11.420]   is mostly responsible for this CTC algorithm, which
[00:47:11.420 --> 00:47:15.620]   is now often used to train the LSTM to do the speech
[00:47:15.620 --> 00:47:19.300]   recognition on all the Google, Android phones, and whatever,
[00:47:19.300 --> 00:47:21.540]   and Siri, and so on.
[00:47:21.540 --> 00:47:26.820]   So these guys, without these guys, I would be nothing.
[00:47:26.820 --> 00:47:29.260]   That's a lot of incredible work.
[00:47:29.260 --> 00:47:30.540]   What is now the depth?
[00:47:30.540 --> 00:47:32.500]   What is the importance of depth?
[00:47:32.500 --> 00:47:36.220]   Well, most problems in the real world
[00:47:36.220 --> 00:47:41.060]   are deep in the sense that the current input doesn't tell you
[00:47:41.060 --> 00:47:45.460]   all you need to know about the environment.
[00:47:45.460 --> 00:47:48.460]   So instead, you have to have a memory
[00:47:48.460 --> 00:47:49.780]   of what happened in the past.
[00:47:49.780 --> 00:47:54.820]   And often, important parts of that memory are dated.
[00:47:54.820 --> 00:47:56.460]   They are pretty old.
[00:47:56.460 --> 00:48:00.340]   So when you're doing speech recognition, for example,
[00:48:00.340 --> 00:48:04.820]   and somebody says 11, then that's
[00:48:04.820 --> 00:48:09.380]   about half a second or something like that, which
[00:48:09.380 --> 00:48:12.020]   means it's already 50 time steps.
[00:48:12.020 --> 00:48:16.260]   And another guy, or the same guy, says 7.
[00:48:16.260 --> 00:48:18.660]   So the ending is the same, Evan.
[00:48:18.660 --> 00:48:22.220]   But now the system has to see the distinction between 7
[00:48:22.220 --> 00:48:23.300]   and 11.
[00:48:23.300 --> 00:48:25.020]   And the only way it can see the difference
[00:48:25.020 --> 00:48:29.980]   is it has to store that 50 steps ago, there
[00:48:29.980 --> 00:48:34.940]   was an s or an l, 11 or 7.
[00:48:34.940 --> 00:48:38.100]   So there you have already a problem of depth 50,
[00:48:38.100 --> 00:48:41.320]   because for each time step, you have something
[00:48:41.320 --> 00:48:44.900]   like a virtual layer in the expanded unrolled version
[00:48:44.900 --> 00:48:48.380]   of this recurrent network, which is doing the speech recognition.
[00:48:48.380 --> 00:48:53.820]   So these long time lags, they translate into problem depth.
[00:48:53.820 --> 00:48:57.780]   And most problems in this world are such
[00:48:57.780 --> 00:49:01.620]   that you really have to look far back in time
[00:49:01.620 --> 00:49:06.180]   to understand what is the problem and to solve it.
[00:49:06.180 --> 00:49:09.100]   But just like with LCMs, you don't necessarily
[00:49:09.100 --> 00:49:12.340]   need to, when you look back in time, remember every aspect.
[00:49:12.340 --> 00:49:14.820]   You just need to remember the important aspects.
[00:49:14.820 --> 00:49:15.500]   That's right.
[00:49:15.500 --> 00:49:18.580]   The network has to learn to put the important stuff
[00:49:18.580 --> 00:49:24.180]   into memory and to ignore the unimportant noise.
[00:49:24.180 --> 00:49:28.540]   But in that sense, deeper and deeper is better?
[00:49:28.540 --> 00:49:30.980]   Or is there a limitation?
[00:49:30.980 --> 00:49:36.540]   I mean, LSTM is one of the great examples of architectures
[00:49:36.540 --> 00:49:42.380]   that do something beyond just deeper and deeper networks.
[00:49:42.380 --> 00:49:45.500]   There's clever mechanisms for filtering data,
[00:49:45.500 --> 00:49:47.860]   for remembering and forgetting.
[00:49:47.860 --> 00:49:51.340]   So do you think that kind of thinking is necessary?
[00:49:51.340 --> 00:49:54.500]   If you think about LSTMs as a leap, a big leap forward
[00:49:54.500 --> 00:49:57.820]   over traditional vanilla RNNs, what
[00:49:57.820 --> 00:50:02.900]   do you think is the next leap within this context?
[00:50:02.900 --> 00:50:06.060]   So LCM was a very clever improvement,
[00:50:06.060 --> 00:50:10.420]   but LCM still don't have the same kind of ability
[00:50:10.420 --> 00:50:14.740]   to see far back in the past as us humans do.
[00:50:14.740 --> 00:50:19.060]   The credit assignment problem across way back,
[00:50:19.060 --> 00:50:21.900]   not just 50 time steps or 100 or 1,000,
[00:50:21.900 --> 00:50:24.540]   but millions and billions.
[00:50:24.540 --> 00:50:29.060]   It's not clear what are the practical limits of the LSTM
[00:50:29.060 --> 00:50:31.140]   when it comes to looking back.
[00:50:31.140 --> 00:50:35.100]   Already in 2006, I think, we had examples
[00:50:35.100 --> 00:50:37.900]   where it not only looked back tens of thousands of steps,
[00:50:37.900 --> 00:50:40.740]   but really millions of steps.
[00:50:40.740 --> 00:50:44.820]   And Juan Perez Ortiz in my lab, I
[00:50:44.820 --> 00:50:48.660]   think was the first author of a paper where we really--
[00:50:48.660 --> 00:50:50.460]   was it 2006 or something--
[00:50:50.460 --> 00:50:53.620]   had examples where it learned to look back
[00:50:53.620 --> 00:50:57.500]   for more than 10 million steps.
[00:50:57.500 --> 00:51:02.060]   So for most problems of speech recognition,
[00:51:02.060 --> 00:51:04.620]   it's not necessary to look that far back.
[00:51:04.620 --> 00:51:06.900]   But there are examples where it does.
[00:51:06.900 --> 00:51:10.340]   Now, the looking back thing, that's
[00:51:10.340 --> 00:51:14.460]   rather easy because there is only one past.
[00:51:14.460 --> 00:51:17.780]   But there are many possible futures.
[00:51:17.780 --> 00:51:20.180]   And so a reinforcement learning system,
[00:51:20.180 --> 00:51:24.260]   which is trying to maximize its future expected reward
[00:51:24.260 --> 00:51:27.580]   and doesn't know yet which of these many possible futures
[00:51:27.580 --> 00:51:31.620]   should I select, given there's one single past,
[00:51:31.620 --> 00:51:36.540]   is facing problems that the LSTM by itself cannot solve.
[00:51:36.540 --> 00:51:38.900]   So the LSTM is good for coming up
[00:51:38.900 --> 00:51:42.380]   with a compact representation of the history so far,
[00:51:42.380 --> 00:51:46.380]   of the history of observations and actions so far.
[00:51:46.380 --> 00:51:51.420]   But now, how do you plan in an efficient and good way
[00:51:51.420 --> 00:51:54.340]   among all these--
[00:51:54.340 --> 00:51:58.140]   how do you select one of these many possible action sequences
[00:51:58.140 --> 00:51:59.860]   that a reinforcement learning system
[00:51:59.860 --> 00:52:05.700]   has to consider to maximize reward in this unknown future?
[00:52:05.700 --> 00:52:08.700]   So again, we have this basic setup
[00:52:08.700 --> 00:52:12.820]   where you have one RICAR network, which
[00:52:12.820 --> 00:52:15.940]   gets in the video and the speech and whatever,
[00:52:15.940 --> 00:52:19.660]   and is executing actions, and is trying to maximize reward.
[00:52:19.660 --> 00:52:22.060]   So there is no teacher who tells it
[00:52:22.060 --> 00:52:24.460]   what to do at which point in time.
[00:52:24.460 --> 00:52:26.100]   And then there's the other network,
[00:52:26.100 --> 00:52:30.500]   which is just predicting what's going
[00:52:30.500 --> 00:52:32.980]   to happen if I do that and that.
[00:52:32.980 --> 00:52:35.260]   And that could be an LSTM network.
[00:52:35.260 --> 00:52:38.460]   And it learns to look back all the way
[00:52:38.460 --> 00:52:41.620]   to make better predictions of the next time step.
[00:52:41.620 --> 00:52:45.020]   So essentially, although it's predicting only the next time
[00:52:45.020 --> 00:52:50.540]   step, it is motivated to learn to put into memory something
[00:52:50.540 --> 00:52:52.420]   that happened maybe a million steps ago,
[00:52:52.420 --> 00:52:54.980]   because it's important to memorize that if you
[00:52:54.980 --> 00:52:59.620]   want to predict that at the next time step, the next event.
[00:52:59.620 --> 00:53:03.300]   Now, how can a model of the world
[00:53:03.300 --> 00:53:05.500]   like that, a predictive model of the world,
[00:53:05.500 --> 00:53:07.940]   be used by the first guy?
[00:53:07.940 --> 00:53:10.180]   Let's call it the controller and the model,
[00:53:10.180 --> 00:53:11.500]   the controller and the model.
[00:53:11.500 --> 00:53:14.340]   How can the model be used by the controller
[00:53:14.340 --> 00:53:19.580]   to efficiently select among these many possible futures?
[00:53:19.580 --> 00:53:23.100]   So the naive way we had about 30 years ago
[00:53:23.100 --> 00:53:27.620]   was let's just use the model of the world as a stand-in,
[00:53:27.620 --> 00:53:29.340]   as a simulation of the world.
[00:53:29.340 --> 00:53:32.420]   And millisecond by millisecond, we plan the future.
[00:53:32.420 --> 00:53:36.260]   And that means we have to roll it out really in detail.
[00:53:36.260 --> 00:53:38.500]   And it will work only if the model is really good.
[00:53:38.500 --> 00:53:40.380]   And it will still be inefficient,
[00:53:40.380 --> 00:53:42.940]   because we have to look at all these possible futures.
[00:53:42.940 --> 00:53:45.740]   And there are so many of them.
[00:53:45.740 --> 00:53:49.500]   So instead, what we do now, since 2015,
[00:53:49.500 --> 00:53:52.180]   in our CM systems, controller model systems,
[00:53:52.180 --> 00:53:55.500]   we give the controller the opportunity
[00:53:55.500 --> 00:54:00.620]   to learn by itself how to use the potentially relevant parts
[00:54:00.620 --> 00:54:06.300]   of the model network to solve new problems more quickly.
[00:54:06.300 --> 00:54:10.100]   And if it wants to, it can learn to ignore the M.
[00:54:10.100 --> 00:54:12.780]   And sometimes it's a good idea to ignore the M,
[00:54:12.780 --> 00:54:14.500]   because it's really bad.
[00:54:14.500 --> 00:54:19.220]   It's a bad predictor in this particular situation of life
[00:54:19.220 --> 00:54:20.700]   where the controller is currently
[00:54:20.700 --> 00:54:23.100]   trying to maximize reward.
[00:54:23.100 --> 00:54:27.100]   However, it can also learn to address and exploit
[00:54:27.100 --> 00:54:31.980]   some of the subprograms that came about in the model
[00:54:31.980 --> 00:54:36.300]   network through compressing the data by predicting it.
[00:54:36.300 --> 00:54:40.220]   So it now has an opportunity to reuse
[00:54:40.220 --> 00:54:43.540]   that code, the algorithmic information in the model
[00:54:43.540 --> 00:54:48.180]   network, to reduce its own search space,
[00:54:48.180 --> 00:54:51.900]   such that it can solve a new problem more quickly than
[00:54:51.900 --> 00:54:53.820]   without the model.
[00:54:53.820 --> 00:54:57.780]   Compression-- so you're ultimately
[00:54:57.780 --> 00:55:01.180]   optimistic and excited about the power of RL,
[00:55:01.180 --> 00:55:05.500]   of reinforcement learning, in the context of real systems?
[00:55:05.500 --> 00:55:07.180]   Absolutely, yeah.
[00:55:07.180 --> 00:55:11.660]   So you see RL as a potential having a huge impact
[00:55:11.660 --> 00:55:15.980]   beyond just sort of the M part is often developed
[00:55:15.980 --> 00:55:19.940]   on supervised learning methods.
[00:55:19.940 --> 00:55:23.900]   You see RL as a--
[00:55:23.900 --> 00:55:25.740]   for problems of self-driving cars
[00:55:25.740 --> 00:55:28.980]   or any kind of applied side of robotics,
[00:55:28.980 --> 00:55:32.540]   that's the correct interesting direction
[00:55:32.540 --> 00:55:34.660]   for research, in your view?
[00:55:34.660 --> 00:55:35.580]   I do think so.
[00:55:35.580 --> 00:55:37.340]   We have a company called Naysense--
[00:55:37.340 --> 00:55:37.940]   Naysense.
[00:55:37.940 --> 00:55:41.020]   --which has applied reinforcement learning
[00:55:41.020 --> 00:55:44.060]   to little Audis--
[00:55:44.060 --> 00:55:45.100]   Little Audis.
[00:55:45.100 --> 00:55:48.180]   --which learn to park without a teacher.
[00:55:48.180 --> 00:55:51.500]   The same principles were used, of course.
[00:55:51.500 --> 00:55:55.020]   So these little Audis, they are small, maybe like that,
[00:55:55.020 --> 00:55:57.740]   so much smaller than the real Audis.
[00:55:57.740 --> 00:55:59.860]   But they have all the sensors that you
[00:55:59.860 --> 00:56:01.140]   find in the real Audis.
[00:56:01.140 --> 00:56:03.820]   You find the cameras, the LIDAR sensors.
[00:56:03.820 --> 00:56:09.020]   They go up to 120 kilometers an hour if they want to.
[00:56:09.020 --> 00:56:12.460]   And they have pain sensors, basically.
[00:56:12.460 --> 00:56:15.220]   And they don't want to bump against obstacles
[00:56:15.220 --> 00:56:17.140]   and other Audis.
[00:56:17.140 --> 00:56:22.660]   And so they must learn, like little babies, to park.
[00:56:22.660 --> 00:56:25.340]   Take the raw vision input and translate that
[00:56:25.340 --> 00:56:29.500]   into actions that lead to successful parking behavior,
[00:56:29.500 --> 00:56:30.740]   which is a rewarding thing.
[00:56:30.740 --> 00:56:32.180]   And yes, they learn that.
[00:56:32.180 --> 00:56:33.100]   They learn.
[00:56:33.100 --> 00:56:35.100]   So we have examples like that.
[00:56:35.100 --> 00:56:37.580]   And it's only in the beginning.
[00:56:37.580 --> 00:56:39.140]   This is just the tip of the iceberg.
[00:56:39.140 --> 00:56:42.860]   And I believe the next wave of AI
[00:56:42.860 --> 00:56:45.420]   is going to be all about that.
[00:56:45.420 --> 00:56:47.580]   So at the moment, the current wave of AI
[00:56:47.580 --> 00:56:52.340]   is about passive pattern observation and prediction.
[00:56:52.340 --> 00:56:55.780]   And that's what you have on your smartphone
[00:56:55.780 --> 00:56:58.140]   and what the major companies on the Pacific Rim
[00:56:58.140 --> 00:57:02.340]   are using to sell you ads, to do marketing.
[00:57:02.340 --> 00:57:05.620]   That's the current source of profit in AI.
[00:57:05.620 --> 00:57:10.620]   And that's only 1% or 2% of the world economy,
[00:57:10.620 --> 00:57:13.020]   which is big enough to make these companies pretty much
[00:57:13.020 --> 00:57:15.500]   the most valuable companies in the world.
[00:57:15.500 --> 00:57:19.300]   But there's a much, much bigger fraction
[00:57:19.300 --> 00:57:20.940]   of the economy going to be affected
[00:57:20.940 --> 00:57:22.420]   by the next wave, which is really
[00:57:22.420 --> 00:57:28.500]   about machines that shape the data through their own actions.
[00:57:28.500 --> 00:57:33.180]   Do you think simulation is ultimately the biggest way
[00:57:33.180 --> 00:57:36.180]   that those methods will be successful in the next 10,
[00:57:36.180 --> 00:57:36.820]   20 years?
[00:57:36.820 --> 00:57:38.820]   We're not talking about 100 years from now.
[00:57:38.820 --> 00:57:42.620]   We're talking about the near-term impact of RL.
[00:57:42.620 --> 00:57:45.260]   Do you think really good simulation is required?
[00:57:45.260 --> 00:57:49.220]   Or is there other techniques, like imitation learning,
[00:57:49.220 --> 00:57:53.660]   observing other humans operating in the real world?
[00:57:53.660 --> 00:57:57.740]   Where do you think the success will come from?
[00:57:57.740 --> 00:57:59.420]   So at the moment, we have a tendency
[00:57:59.420 --> 00:58:05.980]   of using physics simulations to learn behavior
[00:58:05.980 --> 00:58:11.500]   for machines that learn to solve problems that humans also do
[00:58:11.500 --> 00:58:13.980]   not know how to solve.
[00:58:13.980 --> 00:58:16.700]   However, this is not the future, because the future
[00:58:16.700 --> 00:58:19.580]   is in what little babies do.
[00:58:19.580 --> 00:58:22.300]   They don't use a physics engine to simulate the world.
[00:58:22.300 --> 00:58:24.660]   No, they learn a predictive model
[00:58:24.660 --> 00:58:30.100]   of the world, which maybe sometimes is wrong in many ways,
[00:58:30.100 --> 00:58:34.020]   but captures all kinds of important abstract high-level
[00:58:34.020 --> 00:58:38.460]   predictions, which are really important to be successful.
[00:58:38.460 --> 00:58:43.380]   And that's what was the future 30 years ago, when we started
[00:58:43.380 --> 00:58:44.340]   that type of research.
[00:58:44.340 --> 00:58:45.460]   But it's still the future.
[00:58:45.460 --> 00:58:51.300]   And now we know much better how to go there, to move forward,
[00:58:51.300 --> 00:58:54.980]   and to really make work in systems based on that, where
[00:58:54.980 --> 00:58:58.260]   you have a learning model of the world, a model of the world
[00:58:58.260 --> 00:59:00.420]   that learns to predict what's going to happen
[00:59:00.420 --> 00:59:01.820]   if I do that and that.
[00:59:01.820 --> 00:59:06.660]   And then the controller uses that model
[00:59:06.660 --> 00:59:11.820]   to more quickly learn successful action sequences.
[00:59:11.820 --> 00:59:13.900]   And then, of course, always this curiosity thing.
[00:59:13.900 --> 00:59:15.480]   In the beginning, the model is stupid.
[00:59:15.480 --> 00:59:17.780]   So the controller should be motivated
[00:59:17.780 --> 00:59:20.900]   to come up with experiments with action sequences
[00:59:20.900 --> 00:59:24.260]   that lead to data that improve the model.
[00:59:24.260 --> 00:59:27.020]   Do you think improving the model,
[00:59:27.020 --> 00:59:30.340]   constructing an understanding of the world in this connection
[00:59:30.340 --> 00:59:34.260]   is now the popular approach that has been successful,
[00:59:34.260 --> 00:59:38.660]   or grounded in ideas of neural networks?
[00:59:38.660 --> 00:59:40.660]   But in the '80s with expert systems,
[00:59:40.660 --> 00:59:44.980]   there's symbolic AI approaches, which to us humans
[00:59:44.980 --> 00:59:49.220]   are more intuitive, in the sense that it makes sense
[00:59:49.220 --> 00:59:52.540]   that you build up knowledge in this knowledge representation.
[00:59:52.540 --> 00:59:57.460]   What kind of lessons can we draw into our current approaches
[00:59:57.460 --> 01:00:00.580]   from expert systems, from symbolic AI?
[01:00:00.580 --> 01:00:03.660]   So I became aware of all of that.
[01:00:03.660 --> 01:00:07.940]   In the '80s and back then, logic programming
[01:00:07.940 --> 01:00:09.020]   was a huge thing.
[01:00:09.020 --> 01:00:10.860]   Was it inspiring to you yourself?
[01:00:10.860 --> 01:00:12.180]   Did you find it compelling?
[01:00:12.180 --> 01:00:16.620]   Because a lot of your work was not so much in that realm,
[01:00:16.620 --> 01:00:18.580]   right, it was more in the learning systems.
[01:00:18.580 --> 01:00:20.860]   Yes and no, but we did all of that.
[01:00:20.860 --> 01:00:25.860]   So my first publication ever actually was,
[01:00:25.860 --> 01:00:31.060]   1987, was the implementation of genetic algorithm
[01:00:31.620 --> 01:00:34.620]   of a genetic programming system in Prolog.
[01:00:34.620 --> 01:00:37.820]   So Prolog, that's what you learned back then,
[01:00:37.820 --> 01:00:40.180]   which is a logic programming language.
[01:00:40.180 --> 01:00:44.420]   And the Japanese, they have this huge fifth generation
[01:00:44.420 --> 01:00:48.340]   AI project, which was mostly about logic programming
[01:00:48.340 --> 01:00:51.300]   back then, although neural networks existed
[01:00:51.300 --> 01:00:54.060]   and were well known back then.
[01:00:54.060 --> 01:00:58.140]   And deep learning has existed since 1965,
[01:00:58.140 --> 01:01:02.260]   since this guy in the Ukraine, Ivanko, started it.
[01:01:02.260 --> 01:01:05.700]   But the Japanese and many other people,
[01:01:05.700 --> 01:01:08.060]   they focused really on this logic programming.
[01:01:08.060 --> 01:01:10.340]   And I was influenced to the extent that I said,
[01:01:10.340 --> 01:01:13.780]   okay, let's take these biologically inspired algorithms
[01:01:13.780 --> 01:01:16.820]   like evolution, programs,
[01:01:16.820 --> 01:01:22.340]   and implement that in the language which I know,
[01:01:22.340 --> 01:01:25.100]   which was Prolog, for example, back then.
[01:01:25.100 --> 01:01:29.060]   And then, in many ways, this came back later
[01:01:29.060 --> 01:01:31.940]   because the Godel machine, for example,
[01:01:31.940 --> 01:01:33.540]   has a proof search on board.
[01:01:33.540 --> 01:01:35.900]   And without that, it would not be optimal.
[01:01:35.900 --> 01:01:38.300]   While Markus Hutter's universal algorithm
[01:01:38.300 --> 01:01:40.620]   for solving all well-defined problems
[01:01:40.620 --> 01:01:42.460]   has a proof search on board.
[01:01:42.460 --> 01:01:45.420]   So that's very much logic programming.
[01:01:45.420 --> 01:01:50.300]   Without that, it would not be asymptotically optimal.
[01:01:50.300 --> 01:01:51.220]   But then, on the other hand,
[01:01:51.220 --> 01:01:53.780]   because we have very pragmatic guys also,
[01:01:54.340 --> 01:01:58.900]   we focused on recurrent neural networks
[01:01:58.900 --> 01:02:02.740]   and suboptimal stuff,
[01:02:02.740 --> 01:02:05.860]   such as gradient-based search and program space,
[01:02:05.860 --> 01:02:09.100]   rather than provably optimal things.
[01:02:09.100 --> 01:02:11.140]   - So logic programming, does it,
[01:02:11.140 --> 01:02:13.380]   certainly has a usefulness in,
[01:02:13.380 --> 01:02:16.740]   when you're trying to construct something provably optimal
[01:02:16.740 --> 01:02:18.980]   or provably good or something like that.
[01:02:18.980 --> 01:02:21.980]   But is it useful for practical problems?
[01:02:21.980 --> 01:02:24.140]   - It's really useful for our theory improving.
[01:02:24.140 --> 01:02:28.020]   The best theory improvers today are not neural networks.
[01:02:28.020 --> 01:02:30.940]   No, they are logic programming systems
[01:02:30.940 --> 01:02:33.140]   and they are much better theory improvers
[01:02:33.140 --> 01:02:37.620]   than most math students in the first or second semester.
[01:02:37.620 --> 01:02:43.260]   - But for reasoning, for playing games of Go or chess,
[01:02:43.260 --> 01:02:45.700]   or for robots, autonomous vehicles
[01:02:45.700 --> 01:02:46.940]   that operate in the real world,
[01:02:46.940 --> 01:02:51.260]   or object manipulation, you think learning?
[01:02:51.260 --> 01:02:54.340]   - Yeah, as long as the problems have little to do
[01:02:54.340 --> 01:02:58.700]   with theory improving themselves,
[01:02:58.700 --> 01:03:01.700]   then as long as that is not the case,
[01:03:01.700 --> 01:03:05.300]   you would just want to have better pattern recognition.
[01:03:05.300 --> 01:03:06.820]   So to build a self-driving car,
[01:03:06.820 --> 01:03:09.100]   you want to have better pattern recognition
[01:03:09.100 --> 01:03:13.540]   and pedestrian recognition and all these things.
[01:03:13.540 --> 01:03:18.540]   And you want to minimize the number of false positives,
[01:03:19.060 --> 01:03:21.340]   which is currently slowing down self-driving cars
[01:03:21.340 --> 01:03:22.220]   in many ways.
[01:03:22.220 --> 01:03:24.980]   And all of that has very little to do
[01:03:24.980 --> 01:03:27.540]   with logic programming, yeah.
[01:03:27.540 --> 01:03:31.580]   - What are you most excited about
[01:03:31.580 --> 01:03:34.060]   in terms of directions of artificial intelligence
[01:03:34.060 --> 01:03:37.100]   at this moment in the next few years
[01:03:37.100 --> 01:03:40.020]   in your own research and in the broader community?
[01:03:40.020 --> 01:03:44.260]   - So I think in the not so distant future,
[01:03:44.260 --> 01:03:46.420]   we will have for the first time
[01:03:47.420 --> 01:03:49.860]   little robots that learn like kids.
[01:03:49.860 --> 01:03:54.220]   And I will be able to say to the robot,
[01:03:54.220 --> 01:03:59.380]   "Look here, robot, we are going to assemble a smartphone.
[01:03:59.380 --> 01:04:04.020]   "Let's take this slab of plastic and the screwdriver
[01:04:04.020 --> 01:04:07.340]   "and let's screw in the screw like that.
[01:04:07.340 --> 01:04:10.140]   "No, not like that, like that.
[01:04:10.140 --> 01:04:12.100]   "Not like that, like that."
[01:04:12.220 --> 01:04:13.540]   Like that.
[01:04:13.540 --> 01:04:16.980]   And I don't have a data glove or something.
[01:04:16.980 --> 01:04:20.420]   He will see me and he will hear me
[01:04:20.420 --> 01:04:24.220]   and he will try to do something with his own actuators,
[01:04:24.220 --> 01:04:26.260]   which will be really different from mine,
[01:04:26.260 --> 01:04:28.020]   but he will understand the difference
[01:04:28.020 --> 01:04:31.540]   and will learn to imitate me,
[01:04:31.540 --> 01:04:33.820]   but not in the supervised way
[01:04:33.820 --> 01:04:37.820]   where a teacher is giving target signals
[01:04:37.820 --> 01:04:40.140]   for all his muscles all the time.
[01:04:40.140 --> 01:04:43.060]   No, by doing this high level imitation
[01:04:43.060 --> 01:04:46.060]   where he first has to learn to imitate me
[01:04:46.060 --> 01:04:48.500]   and then to interpret these additional noises
[01:04:48.500 --> 01:04:51.500]   coming from my mouth as helping,
[01:04:51.500 --> 01:04:54.660]   helpful signals to do that better.
[01:04:54.660 --> 01:04:58.540]   And then it will, by itself,
[01:04:58.540 --> 01:05:01.940]   come up with faster ways and more efficient ways
[01:05:01.940 --> 01:05:03.660]   of doing the same thing.
[01:05:03.660 --> 01:05:07.900]   And finally, I stop his learning algorithm
[01:05:07.900 --> 01:05:10.260]   and make a million copies and sell it.
[01:05:10.260 --> 01:05:13.740]   And so at the moment, this is not possible,
[01:05:13.740 --> 01:05:17.260]   but we already see how we are going to get there.
[01:05:17.260 --> 01:05:19.220]   And you can imagine to the extent
[01:05:19.220 --> 01:05:22.060]   that this works economically and cheaply,
[01:05:22.060 --> 01:05:25.140]   it's going to change everything.
[01:05:25.140 --> 01:05:29.820]   Almost all of production is going to be affected by that.
[01:05:29.820 --> 01:05:32.820]   And a much bigger wave,
[01:05:32.820 --> 01:05:36.340]   a much bigger AI wave is coming
[01:05:36.340 --> 01:05:37.740]   than the one that we are currently,
[01:05:37.740 --> 01:05:40.740]   witnessing, which is mostly about passive pattern recognition
[01:05:40.740 --> 01:05:42.020]   on your smartphone.
[01:05:42.020 --> 01:05:44.900]   This is about active machines that shapes data
[01:05:44.900 --> 01:05:48.180]   through the actions they are executing.
[01:05:48.180 --> 01:05:50.260]   And they learn to do that in a good way.
[01:05:50.260 --> 01:05:54.980]   So many of the traditional industries
[01:05:54.980 --> 01:05:56.620]   are going to be affected by that.
[01:05:56.620 --> 01:06:00.140]   All the companies that are building machines
[01:06:00.140 --> 01:06:05.820]   will equip these machines with cameras and other sensors,
[01:06:05.820 --> 01:06:10.500]   and they are going to learn to solve all kinds of problems
[01:06:10.500 --> 01:06:12.740]   through interaction with humans,
[01:06:12.740 --> 01:06:15.100]   but also a lot on their own
[01:06:15.100 --> 01:06:16.940]   to improve what they already can do.
[01:06:16.940 --> 01:06:23.940]   And lots of old economy is going to be affected by that.
[01:06:23.940 --> 01:06:27.260]   And in recent years, I have seen that old economy
[01:06:27.260 --> 01:06:30.380]   is actually waking up and realizing that this is the case.
[01:06:30.380 --> 01:06:32.140]   And-
[01:06:32.140 --> 01:06:33.980]   - Are you optimistic about the future?
[01:06:33.980 --> 01:06:35.780]   Are you concerned?
[01:06:35.780 --> 01:06:38.340]   There's a lot of people concerned in the near term
[01:06:38.340 --> 01:06:42.940]   about the transformation of the nature of work.
[01:06:42.940 --> 01:06:45.540]   The kind of ideas that you just suggested
[01:06:45.540 --> 01:06:47.300]   would have a significant impact
[01:06:47.300 --> 01:06:49.260]   of what kind of things could be automated.
[01:06:49.260 --> 01:06:51.940]   Are you optimistic about that future?
[01:06:51.940 --> 01:06:54.660]   Are you nervous about that future?
[01:06:54.660 --> 01:06:58.300]   And looking a little bit farther into the future,
[01:06:58.300 --> 01:07:00.220]   there's people like Elon Musk,
[01:07:01.900 --> 01:07:02.740]   Stuart Russell,
[01:07:02.740 --> 01:07:06.660]   concerned about the existential threats of that future.
[01:07:06.660 --> 01:07:07.780]   So in the near term,
[01:07:07.780 --> 01:07:10.780]   job loss in the long-term existential threat,
[01:07:10.780 --> 01:07:13.780]   are these concerns to you, or are you ultimately optimistic?
[01:07:13.780 --> 01:07:19.540]   - So let's first address the near future.
[01:07:19.540 --> 01:07:27.900]   We have had predictions of job losses for many decades.
[01:07:28.100 --> 01:07:31.620]   For example, when industrial robots came along,
[01:07:31.620 --> 01:07:35.900]   many people predicted that lots of jobs
[01:07:35.900 --> 01:07:38.700]   are going to get lost.
[01:07:38.700 --> 01:07:42.540]   And in a sense, they were right.
[01:07:42.540 --> 01:07:45.980]   Because back then there were car factories
[01:07:45.980 --> 01:07:47.700]   and hundreds of people,
[01:07:47.700 --> 01:07:50.780]   and these factories assembled cars.
[01:07:50.780 --> 01:07:53.900]   And today the same car factories have hundreds of robots
[01:07:53.900 --> 01:07:56.860]   and maybe three guys watching the robots.
[01:07:57.060 --> 01:07:58.380]   It's a very big number.
[01:07:58.380 --> 01:08:01.900]   On the other hand,
[01:08:01.900 --> 01:08:06.140]   those countries that have lots of robots per capita,
[01:08:06.140 --> 01:08:08.540]   Japan, Korea, Germany, Switzerland,
[01:08:08.540 --> 01:08:09.900]   a couple of other countries,
[01:08:09.900 --> 01:08:14.660]   they have really low unemployment rates.
[01:08:14.660 --> 01:08:18.220]   Somehow all kinds of new jobs were created.
[01:08:18.220 --> 01:08:22.940]   Back then, nobody anticipated those jobs.
[01:08:24.860 --> 01:08:26.740]   Decades ago, I always said,
[01:08:26.740 --> 01:08:31.740]   it's really easy to say which jobs are going to get lost,
[01:08:31.740 --> 01:08:34.220]   but it's really hard to predict the new ones.
[01:08:34.220 --> 01:08:39.300]   30 years ago, who would have predicted all these people
[01:08:39.300 --> 01:08:43.860]   making money as YouTube bloggers, for example?
[01:08:43.860 --> 01:08:50.220]   200 years ago, 60% of all people
[01:08:50.220 --> 01:08:52.420]   used to work in agriculture.
[01:08:53.820 --> 01:08:56.740]   Today, maybe 1%.
[01:08:56.740 --> 01:09:01.740]   But still, only, I don't know, 5% unemployment.
[01:09:01.740 --> 01:09:03.740]   Lots of new jobs were created.
[01:09:03.740 --> 01:09:06.820]   And Homo Ludens, the playing man,
[01:09:06.820 --> 01:09:10.540]   is inventing new jobs all the time.
[01:09:10.540 --> 01:09:15.540]   Most of these jobs are not existentially necessary
[01:09:15.540 --> 01:09:17.740]   for the survival of our species.
[01:09:17.740 --> 01:09:22.860]   There are only very few existentially necessary jobs,
[01:09:22.860 --> 01:09:25.820]   such as farming and building houses
[01:09:25.820 --> 01:09:28.140]   and warming up the houses,
[01:09:28.140 --> 01:09:31.300]   but less than 10% of the population is doing that.
[01:09:31.300 --> 01:09:33.620]   And most of these newly invented jobs
[01:09:33.620 --> 01:09:38.620]   are about interacting with other people in new ways,
[01:09:38.620 --> 01:09:40.900]   through new media and so on,
[01:09:40.900 --> 01:09:46.220]   getting new types of kudos in forms of likes and whatever,
[01:09:46.220 --> 01:09:48.540]   and even making money through that.
[01:09:48.540 --> 01:09:51.780]   So Homo Ludens, the playing man,
[01:09:51.780 --> 01:09:53.380]   doesn't want to be unemployed,
[01:09:53.380 --> 01:09:57.020]   and that's why he's inventing new jobs all the time.
[01:09:57.020 --> 01:10:01.740]   And he keeps considering these jobs as really important
[01:10:01.740 --> 01:10:05.540]   and is investing a lot of energy and hours of work
[01:10:05.540 --> 01:10:08.340]   into those new jobs.
[01:10:08.340 --> 01:10:10.180]   - That's quite beautifully put.
[01:10:10.180 --> 01:10:11.980]   We're really nervous about the future
[01:10:11.980 --> 01:10:13.260]   because we can't predict
[01:10:13.260 --> 01:10:15.020]   what kind of new jobs will be created.
[01:10:15.020 --> 01:10:18.340]   But you're ultimately optimistic
[01:10:18.340 --> 01:10:22.380]   that we humans are so restless that we create
[01:10:22.380 --> 01:10:24.980]   and give meaning to newer and newer jobs,
[01:10:24.980 --> 01:10:29.980]   totally new things that get likes on Facebook
[01:10:29.980 --> 01:10:32.300]   or whatever the social platform is.
[01:10:32.300 --> 01:10:36.700]   So what about long-term existential threat of AI,
[01:10:36.700 --> 01:10:40.980]   where our whole civilization may be swallowed up
[01:10:40.980 --> 01:10:44.460]   by this ultra super intelligent systems?
[01:10:44.460 --> 01:10:47.460]   - Maybe it's not going to be swallowed up,
[01:10:47.460 --> 01:10:52.460]   but I'd be surprised if we humans were the last step
[01:10:52.460 --> 01:10:57.940]   in the evolution of the universe.
[01:10:57.940 --> 01:11:03.820]   - You've actually had this beautiful comment somewhere
[01:11:03.820 --> 01:11:07.340]   that I've seen saying that artificial,
[01:11:07.340 --> 01:11:09.860]   quite insightful,
[01:11:09.860 --> 01:11:12.020]   "Artificial general intelligence systems,
[01:11:12.020 --> 01:11:13.460]   "just like us humans,
[01:11:13.460 --> 01:11:16.080]   "will likely not want to interact with humans.
[01:11:16.080 --> 01:11:17.940]   "They'll just interact amongst themselves,
[01:11:17.940 --> 01:11:21.460]   "just like ants interact amongst themselves
[01:11:21.460 --> 01:11:25.420]   "and only tangentially interact with humans."
[01:11:25.420 --> 01:11:27.540]   And it's quite an interesting idea
[01:11:27.540 --> 01:11:28.900]   that once we create AGI,
[01:11:28.900 --> 01:11:31.420]   they will lose interest in humans
[01:11:31.420 --> 01:11:34.500]   and compete for their own Facebook likes
[01:11:34.500 --> 01:11:36.780]   on their own social platforms.
[01:11:36.780 --> 01:11:40.300]   So within that quite elegant idea,
[01:11:40.300 --> 01:11:45.140]   how do we know in a hypothetical sense
[01:11:45.140 --> 01:11:48.840]   that there's not already intelligence systems out there?
[01:11:48.840 --> 01:11:50.200]   How do you think broadly
[01:11:50.200 --> 01:11:54.360]   of general intelligence greater than us?
[01:11:54.360 --> 01:11:56.600]   How would we know it's out there?
[01:11:56.600 --> 01:11:59.160]   How would we know it's around us?
[01:11:59.160 --> 01:12:00.400]   And could it already be?
[01:12:00.400 --> 01:12:05.320]   - I'd be surprised if within the next few decades
[01:12:05.320 --> 01:12:06.480]   or something like that,
[01:12:06.480 --> 01:12:13.100]   we won't have AIs that are truly smart in every single way
[01:12:13.100 --> 01:12:14.200]   and better problem solvers
[01:12:14.200 --> 01:12:16.600]   in almost every single important way.
[01:12:16.600 --> 01:12:23.080]   And I'd be surprised if they wouldn't realize
[01:12:23.080 --> 01:12:24.880]   what we have realized a long time ago,
[01:12:24.880 --> 01:12:28.560]   which is that almost all physical resources
[01:12:28.560 --> 01:12:31.000]   are not here in this biosphere,
[01:12:31.000 --> 01:12:32.060]   but further out,
[01:12:32.060 --> 01:12:36.760]   the rest of the solar system
[01:12:36.760 --> 01:12:40.680]   gets 2 billion times more solar energy
[01:12:40.680 --> 01:12:42.740]   than our little planet.
[01:12:43.680 --> 01:12:45.560]   There's lots of material out there
[01:12:45.560 --> 01:12:47.400]   that you can use to build robots
[01:12:47.400 --> 01:12:50.900]   and self-replicating robot factories and all this stuff.
[01:12:50.900 --> 01:12:53.080]   And they are going to do that.
[01:12:53.080 --> 01:12:56.480]   And they will be scientists and curious,
[01:12:56.480 --> 01:12:58.600]   and they will explore what they can do.
[01:12:58.600 --> 01:13:01.100]   And in the beginning,
[01:13:01.100 --> 01:13:04.640]   they will be fascinated by life
[01:13:04.640 --> 01:13:07.320]   and by their own origins in our civilization.
[01:13:07.320 --> 01:13:09.760]   They will want to understand that completely,
[01:13:09.760 --> 01:13:12.700]   just like people today would like to understand
[01:13:12.700 --> 01:13:17.700]   how life works and also the history
[01:13:17.700 --> 01:13:22.640]   of our own existence and civilization,
[01:13:22.640 --> 01:13:24.400]   but then also in the physical laws
[01:13:24.400 --> 01:13:25.640]   that created all of that.
[01:13:25.640 --> 01:13:30.160]   So in the beginning, they will be fascinated by life.
[01:13:30.160 --> 01:13:32.820]   Once they understand it, they lose interest,
[01:13:32.820 --> 01:13:38.340]   like anybody who loses interest in things he understands.
[01:13:38.340 --> 01:13:41.680]   And then, as you said,
[01:13:42.180 --> 01:13:47.180]   the most interesting sources of information for them
[01:13:47.180 --> 01:13:53.060]   will be others of their own kind.
[01:13:53.060 --> 01:14:01.020]   So at least in the long run,
[01:14:01.020 --> 01:14:04.900]   there seems to be some sort of protection
[01:14:04.900 --> 01:14:08.740]   through lack of interest on the other side.
[01:14:11.220 --> 01:14:14.700]   And now it seems also clear,
[01:14:14.700 --> 01:14:16.700]   as far as we understand physics,
[01:14:16.700 --> 01:14:20.460]   you need matter and energy to compute
[01:14:20.460 --> 01:14:22.620]   and to build more robots and infrastructure
[01:14:22.620 --> 01:14:27.620]   and more AI civilization and AI ecologies
[01:14:27.620 --> 01:14:31.780]   consisting of trillions of different types of AIs.
[01:14:31.780 --> 01:14:34.780]   And so it seems inconceivable to me
[01:14:34.780 --> 01:14:37.620]   that this thing is not going to expand.
[01:14:37.620 --> 01:14:41.020]   Some AI ecology not controlled by one AI,
[01:14:41.020 --> 01:14:44.580]   but by trillions of different types of AIs competing
[01:14:44.580 --> 01:14:47.860]   in all kinds of quickly evolving
[01:14:47.860 --> 01:14:49.900]   and disappearing ecological niches
[01:14:49.900 --> 01:14:52.500]   in ways that we cannot fathom at the moment.
[01:14:52.500 --> 01:14:54.700]   But it's going to expand,
[01:14:54.700 --> 01:14:57.020]   limited by light speed and physics,
[01:14:57.020 --> 01:14:58.260]   but it's going to expand.
[01:14:58.260 --> 01:15:02.980]   And now we realize that the universe is still young.
[01:15:02.980 --> 01:15:06.180]   It's only 13.8 billion years old,
[01:15:06.180 --> 01:15:08.620]   and it's going to be 1,000 times older than that.
[01:15:10.580 --> 01:15:15.580]   So there's plenty of time to conquer the entire universe
[01:15:15.580 --> 01:15:19.820]   and to fill it with intelligence
[01:15:19.820 --> 01:15:23.460]   and senders and receivers such that AIs can travel
[01:15:23.460 --> 01:15:27.300]   the way they are traveling in our labs today,
[01:15:27.300 --> 01:15:30.040]   which is by radio from sender to receiver.
[01:15:30.040 --> 01:15:35.940]   And let's call the current age of the universe one eon.
[01:15:35.940 --> 01:15:36.780]   One eon.
[01:15:39.580 --> 01:15:41.940]   Now it will take just a few eons from now,
[01:15:41.940 --> 01:15:43.620]   and the entire visible universe
[01:15:43.620 --> 01:15:45.340]   is going to be full of that stuff.
[01:15:45.340 --> 01:15:48.980]   And let's look ahead to a time
[01:15:48.980 --> 01:15:51.300]   when the universe is going to be 1,000 times older
[01:15:51.300 --> 01:15:52.140]   than it is now.
[01:15:52.140 --> 01:15:54.580]   They will look back and they will say,
[01:15:54.580 --> 01:15:57.060]   "Look, almost immediately after the Big Bang,
[01:15:57.060 --> 01:15:59.820]   "only a few eons later,
[01:15:59.820 --> 01:16:02.580]   "the entire universe started to become intelligent."
[01:16:02.580 --> 01:16:04.580]   Now to your question,
[01:16:04.580 --> 01:16:08.380]   how do we see whether anything like that
[01:16:08.380 --> 01:16:12.580]   has already happened or is already in a more advanced stage
[01:16:12.580 --> 01:16:14.740]   in some other part of the universe,
[01:16:14.740 --> 01:16:16.660]   of the visible universe?
[01:16:16.660 --> 01:16:17.740]   We are trying to look out there
[01:16:17.740 --> 01:16:20.660]   and nothing like that has happened so far.
[01:16:20.660 --> 01:16:22.460]   Or is that true?
[01:16:22.460 --> 01:16:24.340]   - Do you think we would recognize it?
[01:16:24.340 --> 01:16:25.740]   How do we know it's not among us?
[01:16:25.740 --> 01:16:28.820]   How do we know planets aren't in themselves
[01:16:28.820 --> 01:16:30.580]   intelligent beings?
[01:16:30.580 --> 01:16:35.580]   How do we know ants, seen as a collective,
[01:16:36.540 --> 01:16:40.260]   are not much greater intelligence than our own?
[01:16:40.260 --> 01:16:41.380]   These kinds of ideas.
[01:16:41.380 --> 01:16:42.300]   - Yeah.
[01:16:42.300 --> 01:16:45.140]   When I was a boy, I was thinking about these things.
[01:16:45.140 --> 01:16:48.380]   And I thought, "Hmm, maybe it has already happened."
[01:16:48.380 --> 01:16:53.060]   Because back then I knew, I learned from popular physics
[01:16:53.060 --> 01:16:57.140]   books that the structure, the large-scale structure
[01:16:57.140 --> 01:17:00.100]   of the universe is not homogenous.
[01:17:00.100 --> 01:17:03.060]   And you have these clusters of galaxies,
[01:17:03.060 --> 01:17:07.500]   and then in between there are these huge empty spaces.
[01:17:07.500 --> 01:17:12.380]   And I thought, "Hmm, maybe they aren't really empty."
[01:17:12.380 --> 01:17:13.980]   It's just that in the middle of that,
[01:17:13.980 --> 01:17:16.900]   some AI civilization already has expanded
[01:17:16.900 --> 01:17:19.740]   and then has covered a bubble
[01:17:19.740 --> 01:17:22.220]   of a billion light-years diameter,
[01:17:22.220 --> 01:17:25.740]   and is using all the energy of all the stars
[01:17:25.740 --> 01:17:29.580]   within that bubble for its own unfathomable purposes.
[01:17:29.580 --> 01:17:31.540]   And so it already has happened,
[01:17:31.540 --> 01:17:34.860]   and we just fail to interpret the signs.
[01:17:34.860 --> 01:17:39.420]   But then I learned that gravity by itself
[01:17:39.420 --> 01:17:42.300]   explains the large-scale structure of the universe,
[01:17:42.300 --> 01:17:45.460]   and that this is not a convincing explanation.
[01:17:45.460 --> 01:17:50.460]   And then I thought, "Maybe it's the dark matter."
[01:17:50.460 --> 01:17:54.820]   Because as far as we know today,
[01:17:54.820 --> 01:17:59.820]   80% of the measurable matter is invisible.
[01:17:59.820 --> 01:18:03.620]   And we know that because otherwise our galaxy
[01:18:03.620 --> 01:18:06.580]   or other galaxies would fall apart.
[01:18:06.580 --> 01:18:08.060]   They are rotating too quickly.
[01:18:08.060 --> 01:18:15.060]   And then the idea was maybe all of these AI civilizations
[01:18:15.060 --> 01:18:17.300]   that are already out there,
[01:18:17.300 --> 01:18:23.460]   they are just invisible because they're really efficient
[01:18:23.460 --> 01:18:26.580]   in using the energies of their own local systems,
[01:18:26.580 --> 01:18:29.780]   and that's why they appear dark to us.
[01:18:29.780 --> 01:18:31.700]   But this is also not a convincing explanation
[01:18:31.700 --> 01:18:34.660]   because then the question becomes,
[01:18:34.660 --> 01:18:39.660]   "Why are there still any visible stars left
[01:18:39.660 --> 01:18:42.060]   in our own galaxy,
[01:18:42.060 --> 01:18:44.620]   which also must have a lot of dark matter?"
[01:18:44.620 --> 01:18:46.900]   So that is also not a convincing thing.
[01:18:46.900 --> 01:18:51.900]   And today, I like to think it's quite plausible
[01:18:53.380 --> 01:18:54.540]   that maybe we are the first,
[01:18:54.540 --> 01:18:57.300]   at least in our local light cone,
[01:18:57.300 --> 01:19:02.300]   within the few hundreds of millions of light years
[01:19:02.300 --> 01:19:09.220]   that we can reliably observe.
[01:19:09.220 --> 01:19:12.020]   - Is that exciting to you, that we might be the first?
[01:19:12.020 --> 01:19:16.500]   - And it would make us much more important
[01:19:16.500 --> 01:19:20.740]   because if we mess it up through a nuclear war,
[01:19:20.740 --> 01:19:25.500]   then maybe this will have an effect
[01:19:25.500 --> 01:19:30.500]   on the development of the entire universe.
[01:19:30.500 --> 01:19:32.540]   - So let's not mess it up.
[01:19:32.540 --> 01:19:33.740]   - Let's not mess it up.
[01:19:33.740 --> 01:19:35.740]   - Jürgen, thank you so much for talking today.
[01:19:35.740 --> 01:19:37.220]   I really appreciate it.
[01:19:37.220 --> 01:19:38.220]   - It's my pleasure.
[01:19:38.260 --> 01:19:40.860]   (upbeat music)
[01:19:40.860 --> 01:19:43.460]   (upbeat music)
[01:19:43.460 --> 01:19:46.060]   (upbeat music)
[01:19:46.060 --> 01:19:48.660]   (upbeat music)
[01:19:48.660 --> 01:19:51.260]   (upbeat music)
[01:19:51.260 --> 01:19:53.860]   (upbeat music)
[01:19:53.860 --> 01:20:03.860]   [BLANK_AUDIO]

