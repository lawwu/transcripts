
[00:00:00.000 --> 00:00:05.200]   Machine learning models break.
[00:00:05.200 --> 00:00:06.740]   They usually don't throw errors.
[00:00:06.740 --> 00:00:09.240]   They fail silently, extremely silently.
[00:00:09.240 --> 00:00:13.260]   It's almost impossible to know when your machine learning model is not doing what
[00:00:13.260 --> 00:00:16.020]   you were hoping it would do on your production data.
[00:00:16.020 --> 00:00:18.980]   There's been a lot of very high profile examples of machine
[00:00:18.980 --> 00:00:20.840]   learning models behaving badly.
[00:00:20.840 --> 00:00:25.200]   The only way to really know what your machine learning models are doing is to
[00:00:25.200 --> 00:00:28.080]   log, okay, what went in and what came out.
[00:00:28.140 --> 00:00:31.600]   Without an experiment tracking library, this would require writing a ton of
[00:00:31.600 --> 00:00:36.000]   extra code to log the inputs, log the outputs, and then visualize them.
[00:00:36.000 --> 00:00:37.980]   You're going to want a nice way to explore them.
[00:00:37.980 --> 00:00:42.640]   If you log at 10,000 PNGs from CIFAR-10, how do I look through those?
[00:00:42.640 --> 00:00:45.360]   This is time that you're not spending doing what you want to do, which
[00:00:45.360 --> 00:00:47.120]   is train machines to do cool stuff.
[00:00:47.120 --> 00:00:49.800]   We're going to demonstrate how to do this with WANB callback.
[00:00:49.800 --> 00:00:54.040]   It looks really similar to normal Keras code without WNB instrumented.
[00:00:54.040 --> 00:00:57.660]   And then here, we're going to pull out a small subset of images.
[00:00:57.680 --> 00:01:01.300]   We're going to take this and we're going to pass this to WNB callback.
[00:01:01.300 --> 00:01:03.600]   This is a call signature for the WANB callback.
[00:01:03.600 --> 00:01:05.520]   There's lots of ways to configure it.
[00:01:05.520 --> 00:01:09.420]   We're going to focus on just one, which is solving this problem of how do I know
[00:01:09.420 --> 00:01:11.140]   what went into my model and what came out?
[00:01:11.140 --> 00:01:13.460]   Check out this documentation, look at the source code.
[00:01:13.460 --> 00:01:15.880]   You can see how to use all of these different things.
[00:01:15.880 --> 00:01:21.540]   We're going to do is we're going to pass in some data using this training data
[00:01:21.540 --> 00:01:27.040]   keyword argument and the labels for the like class names that correspond to each
[00:01:27.040 --> 00:01:29.860]   index, also, we need to tell it that this is image data, so it
[00:01:29.860 --> 00:01:30.980]   knows to log it as an image.
[00:01:30.980 --> 00:01:34.660]   Callback is smart enough to know what you, what you want out of this is you
[00:01:34.660 --> 00:01:39.900]   want to run the model on these images and check it against these labels.
[00:01:39.900 --> 00:01:41.680]   Log that at the end of each epoch.
[00:01:41.680 --> 00:01:44.180]   Let's take a look at the results.
[00:01:44.180 --> 00:01:46.220]   This'll be on the project page.
[00:01:46.220 --> 00:01:48.980]   That's the link that comes out when you run WANB.init.
[00:01:48.980 --> 00:01:49.860]   Let's go to that.
[00:01:49.860 --> 00:01:52.140]   You can always go to your profile and find it from there.
[00:01:52.140 --> 00:01:53.320]   Let's take a look here.
[00:01:53.320 --> 00:01:56.900]   We still got the stuff that we had on the previous run here, but now we've
[00:01:56.900 --> 00:02:02.540]   also got these examples, so our model is not doing super great right now.
[00:02:02.540 --> 00:02:06.980]   We got a 40% accuracy, which is not so bad for your one out of 10, but I'm
[00:02:06.980 --> 00:02:09.300]   pretty sure that is an ostrich, not a horse.
[00:02:09.300 --> 00:02:11.660]   It's certainly a bird of some kind.
[00:02:11.660 --> 00:02:12.880]   And that is a kitty.
[00:02:12.880 --> 00:02:14.840]   That is for sure a kitty and not a frog.
[00:02:14.840 --> 00:02:16.220]   We're not doing great right now.
[00:02:16.220 --> 00:02:20.580]   I will say this bird here in the middle, it's green in the background, brown in
[00:02:20.580 --> 00:02:23.520]   the foreground, that is classic deer territory.
[00:02:23.520 --> 00:02:25.640]   This has got sky in the background.
[00:02:25.700 --> 00:02:29.180]   Obviously birds appear in front of sky, whereas deer appear in front of trees.
[00:02:29.180 --> 00:02:30.920]   So our model has been learning something.
[00:02:30.920 --> 00:02:32.360]   Probably should run it for more epochs.
[00:02:32.360 --> 00:02:33.800]   Maybe we should give it more capacity.
[00:02:33.800 --> 00:02:37.260]   With this rich media output, we can see more than just
[00:02:37.260 --> 00:02:39.260]   accuracies going up, losses going down.
[00:02:39.260 --> 00:02:41.660]   We can see how the model is learning.
[00:02:41.660 --> 00:02:43.420]   I love the media logging stuff.
[00:02:43.420 --> 00:02:47.620]   I was always writing to matplotlib code to pull out predictions from my model
[00:02:47.620 --> 00:02:51.120]   and like plot images and try and arrange them, it was nice to learn some
[00:02:51.120 --> 00:02:53.340]   matplotlib and get practice with it.
[00:02:53.340 --> 00:02:55.340]   That's not what I was getting my PhD in.
[00:02:55.340 --> 00:02:58.140]   You know, I was getting my PhD in neural networks, not matplotlib.
[00:02:58.140 --> 00:03:00.720]   (upbeat music)
[00:03:00.720 --> 00:03:03.300]   (upbeat music)
[00:03:03.300 --> 00:03:05.880]   (upbeat music)
[00:03:05.880 --> 00:03:08.460]   (upbeat music)
[00:03:08.460 --> 00:03:11.040]   (upbeat music)
[00:03:11.040 --> 00:03:13.620]   (upbeat music)
[00:03:14.460 --> 00:03:17.040]   (upbeat music)
[00:03:17.040 --> 00:03:18.840]   *Disgustingly loud chewing noises*

