
[00:00:00.000 --> 00:00:06.080]   Okay, today I have the pleasure of interviewing Paul Christiano, who is the leading AI safety
[00:00:06.080 --> 00:00:11.440]   researcher. He's the person that labs and governments turn to when they want feedback
[00:00:11.440 --> 00:00:17.680]   and advice on their safety plans. He previously led the Language Model Alignment team at OpenAI,
[00:00:17.680 --> 00:00:24.720]   where he led the invention of RLHF, and now he is the head of the Alignment Research Center,
[00:00:24.720 --> 00:00:30.080]   and they've been working with the big labs to identify when these models will be too unsafe
[00:00:30.080 --> 00:00:35.040]   to keep scaling. Paul, welcome to the podcast. Thanks for having me. Looking forward to talking.
[00:00:35.040 --> 00:00:40.480]   Okay, so first question, and this is a question I've asked Holden, Ilya, Dario, and none of them
[00:00:40.480 --> 00:00:46.400]   have given me a satisfying answer. Give me a concrete sense of what a post-AGI world that
[00:00:46.400 --> 00:00:51.200]   would be good would look like. Like, how are humans interfacing with the AI? What is the
[00:00:51.200 --> 00:00:56.880]   economic and political structure? Yeah, I guess this is a tough question for a bunch of reasons.
[00:00:56.880 --> 00:01:01.520]   Maybe the biggest one is concrete, and I think it's just, if we're talking about really long
[00:01:01.520 --> 00:01:06.960]   spans of time, then a lot will change, and it's really hard for someone to talk concretely about
[00:01:06.960 --> 00:01:10.800]   what that will look like without saying really silly things. But I can mention some guesses or
[00:01:10.800 --> 00:01:14.960]   fill in some parts. I think this is also a question of how good is good. Like, often I'm thinking
[00:01:14.960 --> 00:01:19.040]   about worlds that seem like kind of the best achievable outcome or a likely achievable outcome.
[00:01:19.920 --> 00:01:26.720]   So, I am very often imagining my typical future has sort of continuing economic and military
[00:01:26.720 --> 00:01:31.280]   competition amongst groups of humans. I think that competition is increasingly mediated by AI
[00:01:31.280 --> 00:01:37.760]   systems. So, for example, if you imagine, right, humans making money, it'll be less and less
[00:01:37.760 --> 00:01:41.280]   worthwhile for humans to spend any of their time trying to make money or any of their time trying
[00:01:41.280 --> 00:01:46.880]   to fight wars. So, increasingly the world you imagine is one where AI systems are doing those
[00:01:46.880 --> 00:01:51.120]   activities on behalf of humans. So, like, I just invest in some index fund, and a bunch of AIs are
[00:01:51.120 --> 00:01:54.800]   running companies, and those companies are competing with each other, but that is kind of a
[00:01:54.800 --> 00:01:59.200]   sphere where humans are not really engaging much. The reason I gave this, like, how good is good
[00:01:59.200 --> 00:02:02.160]   caveat is, like, it's not clear if this is the world you'd most love. Like, I'm like, yeah,
[00:02:02.160 --> 00:02:05.440]   the world, and I'm leading with, like, the world still has a lot of war and a lot of economic
[00:02:05.440 --> 00:02:09.040]   competition and so on. But maybe what I'm trying to, what I'm most often thinking about is, like,
[00:02:09.040 --> 00:02:14.080]   how can a world be reasonably good, like, during a long period where those things still exist?
[00:02:14.080 --> 00:02:18.240]   I think, like, in the very long run, I kind of expect something more like strong world government
[00:02:18.240 --> 00:02:22.640]   rather than just this, like, status quo. That's, like, a very long run. I think there's, like,
[00:02:22.640 --> 00:02:26.400]   a long time left of, like, having a bunch of states and a bunch of different economic powers.
[00:02:26.400 --> 00:02:32.000]   One more government. Why do you think that's the transition that's likely to happen at some point?
[00:02:32.000 --> 00:02:36.160]   Yeah. So, again, at some point, I'm imagining, or I'm thinking of, like, the very broad sweep
[00:02:36.160 --> 00:02:40.320]   of history. I think there are, like, a lot of losses. Like, war is a very costly thing. We
[00:02:40.320 --> 00:02:43.680]   would all like to have fewer wars. If you just ask, like, what is humanity's long-term future,
[00:02:43.680 --> 00:02:49.120]   like, I do expect to drive down the rate of war to very, very low levels eventually. It's sort of,
[00:02:49.120 --> 00:02:51.920]   like, this kind of technological or socio-technological problem of, like,
[00:02:51.920 --> 00:02:56.400]   sort of how do you organize society? How do you navigate conflicts in a way that doesn't have
[00:02:56.400 --> 00:03:00.800]   those kinds of losses? And in the long run, I do expect us to succeed. I expect it to take kind
[00:03:00.800 --> 00:03:04.320]   of a long time subjectively. I think an important fact about AI is just, like, doing a lot of
[00:03:04.320 --> 00:03:08.560]   cognitive work and more quickly getting you to that world more quickly or figuring out how do
[00:03:08.560 --> 00:03:13.120]   we set things up that way. Yeah, the way Carl Schulman put it on the podcast is that you would
[00:03:13.120 --> 00:03:18.320]   have basically a thousand years of intellectual progress or social progress in a span of a month
[00:03:18.320 --> 00:03:23.680]   or whatever when the intelligence explosion happens. More broadly, so the situation where,
[00:03:23.680 --> 00:03:28.960]   you know, we have these AIs who are managing our hedge funds and managing our factories and so on,
[00:03:28.960 --> 00:03:35.680]   that seems like something that makes sense when the AI is human level. But when we have superhuman
[00:03:35.680 --> 00:03:43.760]   AIs, do we want gods who are enslaved forever? In a hundred years, what is the situation we want?
[00:03:43.760 --> 00:03:47.920]   So a hundred years is a very, very long time. And maybe starting with the spirit of the question,
[00:03:47.920 --> 00:03:50.800]   or maybe I have a view which is perhaps less extreme than Carl's view, but still,
[00:03:50.800 --> 00:03:56.720]   like, a hundred objective years is further ahead than I ever think. I still think I'm
[00:03:56.720 --> 00:04:01.520]   describing a world which involves incredibly smart systems running around doing things like
[00:04:01.520 --> 00:04:05.280]   running companies on behalf of humans and fighting wars on behalf of humans. And you might be like,
[00:04:05.280 --> 00:04:09.600]   is that the world you really want? Or like, certainly not the first best world, as we
[00:04:09.600 --> 00:04:15.840]   mentioned a little bit before. I think it is a world that probably is the, of the achievable
[00:04:15.840 --> 00:04:20.880]   worlds or like feasible worlds is the one that seems most desirable to me. That is sort of
[00:04:20.880 --> 00:04:24.320]   decoupling the social transition from this technological transition. So you could say,
[00:04:24.320 --> 00:04:27.680]   like, we're about to build some AI systems. And like, at the time we build AI systems,
[00:04:27.680 --> 00:04:32.880]   you would like to have either greatly changed the way world government works, or you would like to
[00:04:32.880 --> 00:04:36.880]   have sort of humans have to decided, like, we're done, we're passing off the baton to these AI
[00:04:36.880 --> 00:04:42.560]   systems. I think that you would like to decouple those timescales. So I think AI development is
[00:04:42.560 --> 00:04:47.840]   by default, barring some kind of coordination, going to be very fast. So there's not going to
[00:04:47.840 --> 00:04:50.960]   be a lot of time for humans to think like, Hey, what do we want if we're building the next
[00:04:50.960 --> 00:04:54.400]   generation instead of just raising it the normal way? Like, what do we want that to look like?
[00:04:54.400 --> 00:04:58.400]   I think that's like a crazy hard kind of collective decision that humans naturally
[00:04:58.400 --> 00:05:03.440]   want to cope with over like a bunch of generations. And the construction of AI is this very fast
[00:05:03.440 --> 00:05:07.200]   technological process happening over years. So I don't think you want to say like, by the time
[00:05:07.200 --> 00:05:10.640]   we have finished this technological progress, we will have made a decision about like the next
[00:05:10.640 --> 00:05:14.880]   species we're going to build and replace ourselves with. I think the world we want to be in is one
[00:05:14.880 --> 00:05:18.640]   where we say like, either we are able to build the technology in a way that doesn't force us
[00:05:18.640 --> 00:05:22.240]   to have made those decisions, which probably means it's a kind of AI system that we're happy,
[00:05:22.240 --> 00:05:26.320]   like delegating, fighting a war, running a company to, or if we're not able to do that,
[00:05:26.320 --> 00:05:30.240]   then I really think you should not be doing, you shouldn't have been building that technology.
[00:05:30.240 --> 00:05:33.440]   If you're like, the only way you can cope with AI is being ready to hand off the world to some AI
[00:05:33.440 --> 00:05:37.360]   system you built. I think it's very unlikely we're going to be sort of ready to do that on the
[00:05:37.360 --> 00:05:41.440]   timelines that the technology would naturally dictate. - Say we're in the situation in which
[00:05:41.440 --> 00:05:45.360]   we're happy with the thing, what would it look like for us to say we're ready to hand off the
[00:05:45.360 --> 00:05:50.800]   baton? Like what would make you satisfied? And the reason it's relevant to ask you is because
[00:05:50.800 --> 00:05:55.360]   you're on Anthropx, a long-term benefit trust, and you'll choose like the majority of the board
[00:05:55.360 --> 00:06:01.680]   members on, in the long run, at Anthropx. These will presumably be the people who decide if
[00:06:01.680 --> 00:06:06.720]   Anthropx gets AI first, you know, what the AI ends up doing. So what is the version of that,
[00:06:06.720 --> 00:06:11.120]   that you would be happy with? - My main high-level take here is that
[00:06:11.120 --> 00:06:15.920]   I would be unhappy about a world where like Anthropx just makes some call and Anthropx is
[00:06:15.920 --> 00:06:19.120]   like, here's the kind of AI, like we've seen enough, we're ready to hand off the future to
[00:06:19.120 --> 00:06:23.760]   this kind of AI. So like procedurally, I think it's like not a decision that kind of I want to
[00:06:23.760 --> 00:06:28.240]   be making personally, or I want Anthropx to be making. So I kind of think from the perspective
[00:06:28.240 --> 00:06:32.080]   of that decision-making or those challenges, the answer is pretty much always going to be like,
[00:06:32.080 --> 00:06:36.080]   we are not collectively ready because we're sort of not even all collectively engaged in this
[00:06:36.080 --> 00:06:40.720]   process. I think from the perspective of an AI company, you kind of don't have this like fast
[00:06:40.720 --> 00:06:45.600]   handoff option. You kind of have to be doing the like option value, like to build the technology
[00:06:45.600 --> 00:06:50.640]   in a way that doesn't like lock humanity into one course path. So this isn't answering your
[00:06:50.640 --> 00:06:54.080]   full question, but this is answering the part that I think is most relevant to governance
[00:06:54.080 --> 00:06:57.760]   questions for Anthropx. - You don't have to speak on behalf of Anthropx. I'm not asking
[00:06:57.760 --> 00:07:02.960]   about the process by which we would as a civilization agree to hand off. I'm just saying,
[00:07:02.960 --> 00:07:07.920]   okay, I personally, it's hard for me to imagine in a hundred years that these things are still
[00:07:07.920 --> 00:07:12.960]   our slaves. And if they are, I think that's not the best world. So at some point we're handing
[00:07:12.960 --> 00:07:17.280]   off the baton. Like what is that, where would you be satisfied with, this is an arrangement
[00:07:17.280 --> 00:07:22.080]   between the humans and AI's where I'm happy to let the rest of the universe or less, uh,
[00:07:22.080 --> 00:07:28.080]   the rest of time play out. - I think that it is unlikely that in a hundred years I would be happy
[00:07:28.080 --> 00:07:31.280]   with anything that was like, you had some humans, you're just going to throw away the humans and
[00:07:31.280 --> 00:07:35.120]   like start afresh with these machines you built. That is, I think you probably need subjectively
[00:07:35.120 --> 00:07:39.360]   longer than that before I or most people are like, okay, we understand what's up for grabs here.
[00:07:39.360 --> 00:07:43.120]   So if you talk about a hundred years, I kind of do, you know, there's a process that I kind
[00:07:43.120 --> 00:07:47.680]   of understand in like a process of like, you have some humans, the humans are like talking and
[00:07:47.680 --> 00:07:51.680]   thinking and deliberating together. The humans are having kids and raising kids. And like one
[00:07:51.680 --> 00:07:55.360]   generation comes after the next. There's that process we kind of understand. And we have a lot
[00:07:55.360 --> 00:07:58.880]   of views about what makes it go well or poorly. And we can try and like improve that process and
[00:07:58.880 --> 00:08:02.320]   have the next generation do it better than the previous generation. I think there's some like
[00:08:02.320 --> 00:08:06.800]   story like that, that I get and that I like. And then I think that like the default paths to be
[00:08:06.800 --> 00:08:10.080]   comfortable with something very different. It's kind of more like just run that story for a long
[00:08:10.080 --> 00:08:14.720]   time, like have more time for humans to sit around and think a lot and conclude here's what we
[00:08:14.720 --> 00:08:18.960]   actually want, or a lot of long time for us to talk to each other or to grow up with this new
[00:08:18.960 --> 00:08:23.600]   technology and live in that world for our whole lives and so on. And so like, I'm mostly thinking
[00:08:23.600 --> 00:08:28.160]   from the perspective of these more like local changes of saying not like, what is the world
[00:08:28.160 --> 00:08:31.680]   that I want? Like, what's the crazy world, the kind of crazy I'd be happy handing off to more
[00:08:31.680 --> 00:08:34.880]   just like, in what way do I wish like we right now we're different? Like, how could we all be a
[00:08:34.880 --> 00:08:37.920]   little bit better? And then if we were a little bit better than they would ask, like, okay, how
[00:08:37.920 --> 00:08:42.000]   could we all be a little bit better? And I think that like, it's hard to make the giant jump rather
[00:08:42.000 --> 00:08:45.120]   than to say like, what's the like local change that would cause me to think our decisions are
[00:08:45.120 --> 00:08:50.000]   better. Okay. So then let's talk about the transition period in which we were doing all
[00:08:50.000 --> 00:08:54.480]   this thinking. What should that period look like? Because you can't have this scenario where
[00:08:54.480 --> 00:08:58.400]   everybody has access to the most advanced capabilities and can, you know, kill off all
[00:08:58.400 --> 00:09:03.040]   the humans with a new bioweapon. At the same time, I guess you wouldn't want too much concentration.
[00:09:03.040 --> 00:09:09.360]   You wouldn't want just one agent having AI this entire time. So what is the arrangement of this
[00:09:09.360 --> 00:09:13.600]   period of reflection that you'd be happy with? I guess there's two aspects of that that seem
[00:09:13.600 --> 00:09:17.760]   particularly challenging, or there's a bunch of aspects that are challenging. All of these
[00:09:17.760 --> 00:09:22.160]   are things that I personally like, I just think about my one little slice of this problem in my,
[00:09:22.160 --> 00:09:27.920]   in my day job. So here I am speculating, but so one question is what kind of access to AI is both
[00:09:27.920 --> 00:09:31.680]   compatible with the kinds of improvements you'd like. So do you want a lot of people to be able
[00:09:31.680 --> 00:09:36.320]   to use AI to like better understand what's true or like relieve material suffering, things like
[00:09:36.320 --> 00:09:44.240]   this, and also compatible with not all killing each other immediately? I think sort of the defaults
[00:09:44.240 --> 00:09:49.360]   or like my best, the simplest option there is to say like, there are certain kinds of technology
[00:09:49.360 --> 00:09:54.400]   or certain kinds of action where like destruction is easier than defense. So for example, in the
[00:09:54.400 --> 00:09:58.320]   world of today, it seems like, you know, maybe this is true with physical explosives. Maybe this
[00:09:58.320 --> 00:10:01.680]   is true with biological weapons. Maybe this is true with just getting a gun and shooting people.
[00:10:01.680 --> 00:10:04.880]   Like there's a lot of ways in which it's just kind of easy to cause a lot of harm and there's not
[00:10:04.880 --> 00:10:08.560]   very good protective measures. So I think the easiest path is to say like, we're going to think
[00:10:08.560 --> 00:10:11.760]   about those. We're going to think about particular ways in which destruction is easy and try and
[00:10:11.760 --> 00:10:17.920]   either control access to the kinds of physical resources that are needed to cause that harm.
[00:10:17.920 --> 00:10:21.440]   So for example, you can imagine the world where like an individual actually just can't, even
[00:10:21.440 --> 00:10:24.480]   though they're rich enough to, can't like control their own factory that can make tanks. You say
[00:10:24.480 --> 00:10:28.960]   like, look, as a matter of policy, sort of access to industry is somewhat restricted or somewhat
[00:10:28.960 --> 00:10:32.400]   regulated. Even though again, right now it can be mostly regulated just because like most people
[00:10:32.400 --> 00:10:35.200]   aren't rich enough that they could even go off and just build a thousand tanks. You live in the
[00:10:35.200 --> 00:10:38.720]   future where people actually are so rich. Like you need to say like, that's just not a thing you're
[00:10:38.720 --> 00:10:43.280]   allowed to do, which to a significant extent is already true. And you can expand the range of
[00:10:43.280 --> 00:10:48.560]   domains where that's true. And then you could also hope to intervene on like actual provision of
[00:10:48.560 --> 00:10:51.440]   information. Or like if people are using their AI, you might say like, look, we care about what
[00:10:51.440 --> 00:10:55.200]   kinds of interactions with AI, what kind of information people are getting from AI. So even
[00:10:55.200 --> 00:10:59.680]   if for the most part, people are pretty free to use AI, to delegate tasks to AI agents, to consult
[00:10:59.680 --> 00:11:06.960]   AI advisors, we still have some legal limitations on how people use AI. So again, don't ask your AI
[00:11:06.960 --> 00:11:12.080]   how to, how to cause terrible damage. I think there, some of these are kind of easy. So in the
[00:11:12.080 --> 00:11:16.000]   case of like, you know, don't ask your AI how you could murder a million people, it's not such a
[00:11:16.000 --> 00:11:20.960]   hard, like legal requirement. I think some things are a lot more subtle and messy. Like a lot of
[00:11:20.960 --> 00:11:27.360]   domains, if you're talking about like influencing people or like running misinformation campaigns
[00:11:27.360 --> 00:11:31.840]   or whatever, then I think you get into like a much messier line between the kinds of things people
[00:11:31.840 --> 00:11:35.280]   want to do and the kinds of things you might be uncomfortable with them doing. Probably I think
[00:11:35.280 --> 00:11:40.240]   most about persuasion as a thing, like in that messy line where there's like ways in which it
[00:11:40.240 --> 00:11:43.760]   may just be rough or the world may be like kind of messy. If you have a bunch of people trying to
[00:11:43.760 --> 00:11:49.280]   live their lives and interacting with other humans who have really good advisors, helping them run
[00:11:49.280 --> 00:11:53.920]   persuasion campaigns or whatever. But anyway, I think for the most part, like the default remedy
[00:11:53.920 --> 00:11:58.480]   is think about particular harms, have legal protections, either in the use of physical
[00:11:58.480 --> 00:12:03.520]   technologies that are relevant or in access to advice or whatever else to protect against those
[00:12:03.520 --> 00:12:07.760]   harms. And like that regime won't work forever. Like at some point, like the, you know, the set
[00:12:07.760 --> 00:12:11.840]   of harms grows and the set of unanticipated harms grows. But I think that regime might last like a
[00:12:11.840 --> 00:12:17.840]   very long time. Does that regime have to be global? I guess, but initially it can be only in the
[00:12:17.840 --> 00:12:23.280]   countries in which there is AI or advanced AI, but presumably that'll proliferate. So does that
[00:12:23.280 --> 00:12:27.360]   regime have to be global? Again, it's like easy to make some destructive technology. You want to
[00:12:27.360 --> 00:12:31.520]   regulate access to that technology because it could be used to either for terrorism or even
[00:12:31.520 --> 00:12:35.200]   when fighting a war in a way that's destructive. I think ultimately those have to be international
[00:12:35.200 --> 00:12:39.280]   agreements and you might hope they're made like more danger by danger, but you might also make
[00:12:39.280 --> 00:12:43.280]   them in a very broad way with respect to AI. If you think AI is opening up, like I think the key
[00:12:43.280 --> 00:12:47.840]   role of AI here is it's opening up like a lot of new harms, like in a very, you know, one after
[00:12:47.840 --> 00:12:53.280]   another or very rapidly in calendar time. And so you might want to target AI in particular, rather
[00:12:53.280 --> 00:12:59.760]   than going physical technology by physical technology. There's like two open debates that
[00:12:59.760 --> 00:13:05.440]   one might be concerned about here. One is about how much people's access to AI should be limited.
[00:13:05.440 --> 00:13:11.600]   And, you know, here there's like old questions about free speech versus causing chaos and
[00:13:11.600 --> 00:13:19.360]   limiting access to harms. But there's another issue, which is the control of the AIs themselves,
[00:13:19.360 --> 00:13:23.360]   where now nobody's concerned that we're infringing on GPT-4's moral rights. But as
[00:13:23.360 --> 00:13:29.680]   these things get smarter, the level of control, which we want via the strong guarantees of
[00:13:29.680 --> 00:13:34.240]   alignment to not only be able to read their minds, but to be able to modify them in these really
[00:13:34.240 --> 00:13:39.600]   precise ways, is beyond totalitarian if we were doing that to other humans. As an alignment
[00:13:39.600 --> 00:13:42.880]   researcher, like what are your thoughts on this? Are you concerned that as these things get smarter
[00:13:42.880 --> 00:13:49.520]   and smarter, what we're doing is not, it doesn't seem kosher? There is a significant chance we
[00:13:49.520 --> 00:13:53.680]   will eventually have AI systems for which it's like a really big deal to mistreat them. I think
[00:13:53.680 --> 00:13:58.080]   like no one really has that good a grip on when that happens. I think people are like really
[00:13:58.080 --> 00:14:03.440]   dismissive of that being the case now. But I think I would be completely in the dark enough that it
[00:14:03.440 --> 00:14:08.400]   wouldn't even be that dismissive of it being the case now. I think one first point worth making is
[00:14:08.400 --> 00:14:13.680]   I don't know if alignment makes the situation worse rather than better. So if you like consider
[00:14:13.680 --> 00:14:18.640]   the world, if you think that like, you know, GPT-4 is a person you should treat well and you're like,
[00:14:18.640 --> 00:14:23.040]   well, here's how we're going to organize our society. Just like there are billions of copies
[00:14:23.040 --> 00:14:27.200]   of GPT-4 and they just do things humans want and can't hold property. And like, whenever they do
[00:14:27.200 --> 00:14:31.200]   things that the humans don't like, then we like mess with them until they stop doing that. Like,
[00:14:32.960 --> 00:14:37.920]   I think that's a rough world regardless of how good you are at alignment. And I think in the
[00:14:37.920 --> 00:14:41.520]   context of that kind of default plan, like if you've got a trajectory, the world is on right
[00:14:41.520 --> 00:14:46.000]   now, which I think this would alone be a reason not to love that trajectory. But if you view that
[00:14:46.000 --> 00:14:51.200]   as like the trajectory we're on right now, I think like, it's not great. Understanding the systems
[00:14:51.200 --> 00:14:55.760]   you build, understanding how to control how the systems work, et cetera, is probably on balance
[00:14:55.760 --> 00:15:00.000]   good for avoiding like a really bad situation. You would really love to understand if you've
[00:15:00.000 --> 00:15:03.120]   built systems, like if you had a system which like resents the fact that it's interacting with
[00:15:03.120 --> 00:15:07.840]   humans in this way. Like this is the kind of thing where like that is both kind of horrifying from a
[00:15:07.840 --> 00:15:12.480]   safety perspective and also a moral perspective. Like everyone should be very unhappy if you've
[00:15:12.480 --> 00:15:16.160]   built a bunch of AIs who are like, I really hate these humans, but they will like murder me if I
[00:15:16.160 --> 00:15:19.600]   don't do what they want. And so like, that's just not a good case. And so if you're doing research
[00:15:19.600 --> 00:15:23.840]   to try and understand whether that's like how your AI feels, that was probably good. Like I would
[00:15:23.840 --> 00:15:27.840]   guess that will on average decrease the, that the main effect of that will be to avoid building that
[00:15:27.840 --> 00:15:32.640]   kind of AI. And just like, it's an important thing to know. I think like everyone should like to know
[00:15:32.640 --> 00:15:38.080]   if that's how the AIs you build feel. Right. Or that, that seems more instrumental as in,
[00:15:38.080 --> 00:15:43.280]   yeah, we don't want to cause some sort of revolution because of the control we're asking
[00:15:43.280 --> 00:15:49.040]   for. But forget about the instrumental way in which this might harm safety. One way to ask
[00:15:49.040 --> 00:15:54.080]   this question is if you look through history, there's been all kinds of different ideologies
[00:15:54.080 --> 00:16:02.160]   and reasons why it's, it's very dangerous to have infidels or kind of revolutionaries or race
[00:16:02.160 --> 00:16:07.040]   traitors or whatever doing various things in society. And obviously we're in a completely
[00:16:07.040 --> 00:16:12.000]   different transition in society. So not all historical cases are analogous, but it seems
[00:16:12.000 --> 00:16:17.520]   like the Lindy philosophy, if you were alive any other time is just be humanitarian and enlightened
[00:16:17.520 --> 00:16:22.640]   towards intelligent, conscious beings. If society as a whole, we're asking for this level of control
[00:16:22.640 --> 00:16:28.400]   of other humans, or even if AIs were wanted this level of control about other AIs, we'd be pretty
[00:16:28.400 --> 00:16:33.680]   concerned about this. So how should we just think about, yeah, the issues come that come up here as
[00:16:33.680 --> 00:16:38.080]   these things get smarter. So I think there's a huge question about like what is happening inside
[00:16:38.080 --> 00:16:42.320]   of a model that you want to use. And if you're in the world where it's reasonable to think of
[00:16:42.320 --> 00:16:46.960]   like GPT-4 as just like, here are some heuristics that are running and there's like no one at home
[00:16:46.960 --> 00:16:51.120]   or whatever, then you can kind of think of this thing as like, here's a tool that we're building.
[00:16:51.120 --> 00:16:54.560]   That's going to help humans do some stuff. And I think if you're in that world, it makes sense
[00:16:54.560 --> 00:16:59.680]   to kind of be an organization, like an AI company building tools that you're going to give to humans.
[00:16:59.680 --> 00:17:03.120]   I think there's a very different world, which like, I think probably ultimately end up in,
[00:17:03.120 --> 00:17:07.440]   if you keep training AI systems in the way we do right now, which is like, it's just totally
[00:17:07.440 --> 00:17:11.360]   inappropriate to think of the system as a tool that you're building and can help humans do things,
[00:17:11.360 --> 00:17:14.960]   both from a safety perspective and from a, like, that's kind of a horrifying way to organize a
[00:17:14.960 --> 00:17:20.480]   society perspective. And I think like, if you're in that world, I really think you shouldn't be
[00:17:21.280 --> 00:17:26.880]   like, it's just the, the way tech companies are organized is like not an appropriate way to relate
[00:17:26.880 --> 00:17:29.840]   to a technology that works that way. Like, it's not reasonable to be like, Hey, we're going to
[00:17:29.840 --> 00:17:34.800]   build a new species of minds. And like, we're going to try and make a bunch of money from it.
[00:17:34.800 --> 00:17:38.080]   And like, Google's just like thinking about that and then like running their business plan for the
[00:17:38.080 --> 00:17:44.320]   quarter or something. Yeah. My basic view is like, there's a really plausible world where it's sort
[00:17:44.320 --> 00:17:48.160]   of problematic to try and build a bunch of AI systems and use them as tools. And the thing I
[00:17:48.160 --> 00:17:52.480]   really want to do in that world is just like not try and build a ton of AI systems to make money
[00:17:52.480 --> 00:17:59.680]   from them. Right. And I think that like the worlds that are worst, yeah, probably like the single
[00:17:59.680 --> 00:18:05.440]   world I most dislike here is the one where people say like, on the one hand, like there's sort of a
[00:18:05.440 --> 00:18:09.040]   contradiction in this position, but I think it's a position that might end up being endorsed
[00:18:09.040 --> 00:18:12.560]   sometimes, which is like, on the one hand, these AI systems are their own people, so you should
[00:18:12.560 --> 00:18:16.000]   let them do their thing. But on the other hand, like our business plan is to like make a bunch
[00:18:16.000 --> 00:18:20.320]   of AI systems and then like try and run this like crazy slave trade where we make a bunch of money
[00:18:20.320 --> 00:18:26.240]   from them. I think that's like not a good world. And so if you're like, yeah, I think it's better
[00:18:26.240 --> 00:18:29.440]   to not make the technology or wait until you like understand whether that's the shape of the
[00:18:29.440 --> 00:18:33.680]   technology or until you have a different way to build. Like, I think there's no contradiction in
[00:18:33.680 --> 00:18:37.840]   principle to building like cognitive tools that help humans do things without themselves being
[00:18:37.840 --> 00:18:41.920]   like moral entities. That's like what you would prefer to do. You'd prefer to build a thing that's
[00:18:41.920 --> 00:18:46.000]   like, you know, like the calculator that helps humans understand what's true without itself being
[00:18:46.000 --> 00:18:50.320]   like a moral patient or itself being a thing where you'd look back in retrospect and be like, wow,
[00:18:50.320 --> 00:18:54.880]   that was horrifying mistreatment. That's like the best path. And like to the extent that you're
[00:18:54.880 --> 00:18:57.920]   ignorant about whether that's the path you're on and you're like, actually, maybe this was a moral
[00:18:57.920 --> 00:19:04.480]   atrocity. I really think like plan A is to stop building such AI systems until you understand
[00:19:04.480 --> 00:19:08.560]   what you're doing. That is, I think that there's a middle route you could take, which I think is
[00:19:08.560 --> 00:19:12.080]   pretty bad, which is where you say like, well, they might be persons. And if they're persons,
[00:19:12.080 --> 00:19:16.800]   we don't want to like be too down on them, but we're still going to like build vast numbers in
[00:19:16.800 --> 00:19:22.080]   our efforts to make like a trillion dollars or something. Yeah. Or there's this other question
[00:19:22.080 --> 00:19:28.160]   of the immorality or the dangers of just replicating a whole bunch of slaves that are,
[00:19:28.160 --> 00:19:35.440]   have minds. There's also this other question of trying to align entities that have their own
[00:19:35.440 --> 00:19:39.840]   minds. And what is the point in which you're just ensuring safety? I mean, this is the alien
[00:19:39.840 --> 00:19:44.240]   species. You want to make sure it's not going crazy to the point, I guess, is there some
[00:19:44.240 --> 00:19:50.480]   boundary where you would say, I feel uncomfortable having this level of control over an intelligent
[00:19:50.480 --> 00:19:54.240]   being, not for the sake of making money, but even just to align them with human preferences.
[00:19:54.240 --> 00:19:59.200]   Yeah. To be clear, my objection here is not that Google is making money. My objection is that
[00:19:59.200 --> 00:20:01.840]   you're like creating this creature, like what are they going to do? They're going to help humans
[00:20:01.840 --> 00:20:05.920]   get a bunch of stuff and like humans paying for it or whatever, it's sort of equally problematic.
[00:20:05.920 --> 00:20:09.840]   You could like, imagine splitting alignment, like different alignment work relates to this in
[00:20:09.840 --> 00:20:12.960]   different ways. Like, so the purpose of some alignment work, like the alignment work I work
[00:20:12.960 --> 00:20:18.320]   on is mostly aimed at the like, don't produce AI systems that are like people who want things,
[00:20:18.320 --> 00:20:22.320]   who are just like scheming about like, maybe I should help these humans because that's like
[00:20:22.320 --> 00:20:26.480]   instrumentally useful or whatever. You would like to not build such systems as like plan A.
[00:20:26.480 --> 00:20:30.000]   There's like a second stream of alignment work that's like, well, look, let's just assume the
[00:20:30.000 --> 00:20:33.680]   worst and imagine that these AI systems like would prefer murder us if they could. Like,
[00:20:33.680 --> 00:20:37.440]   how do we structure, how do we use AI systems without like exposing ourselves to like risk of
[00:20:37.440 --> 00:20:45.200]   robot rebellion? I think in the second category, I do feel, yeah, I do feel pretty unsure about that.
[00:20:45.200 --> 00:20:48.400]   Or I've, I mean, like we could, we could definitely talk more about it. I think it's
[00:20:48.400 --> 00:20:52.000]   like very, I agree that it's like very complicated and not straightforward.
[00:20:52.000 --> 00:20:56.000]   To the extent you have that worry, I mostly think you shouldn't have built this technology.
[00:20:56.000 --> 00:20:59.120]   That's right. If someone is saying like, Hey, the systems you're building, like might not like
[00:20:59.120 --> 00:21:04.560]   humans and might want to like, you know, overthrow human society. I think like you should probably
[00:21:04.560 --> 00:21:08.240]   have one of two responses to that. You should either be like, that's wrong. Probably, probably
[00:21:08.240 --> 00:21:11.200]   the systems aren't like that and we're building them. And then you're viewing this as like a,
[00:21:11.200 --> 00:21:14.640]   just in case you were horribly like the person building the technology was horribly wrong.
[00:21:14.640 --> 00:21:17.840]   Like they thought these weren't like people who wanted things, but they were.
[00:21:17.840 --> 00:21:23.600]   And so then this is more like our crazy backup measure of like, if we were mistaken about what
[00:21:23.600 --> 00:21:27.120]   was going on, this is like the fallback where we'd like, if we were wrong, we're just going
[00:21:27.120 --> 00:21:30.880]   to learn about it in a benign way rather than like when something really catastrophic happens.
[00:21:30.880 --> 00:21:34.160]   And the second reaction is like, Oh, you're right. These are people. And like, we would
[00:21:34.160 --> 00:21:37.280]   have to do all these things to like prevent a robot rebellion. And in that case, like, again,
[00:21:37.280 --> 00:21:42.480]   I think you should mostly back off for a variety of reasons. Like you shouldn't build the systems
[00:21:42.480 --> 00:21:46.960]   and be like, yeah, this looks like the kind of system that would want to rebel, but we can stop
[00:21:46.960 --> 00:21:52.640]   it. Right. Okay. Maybe I guess an analogy might be if there was an armed uprising in the United
[00:21:52.640 --> 00:21:56.800]   States, we would recognize these are still people or we had some like militia group that they keep
[00:21:56.800 --> 00:22:00.080]   capability to overthrow the United States. We recognize, Oh, these are still people who have
[00:22:00.080 --> 00:22:04.560]   moral rights, but also we can't allow them to have the capacity to overthrow the United States.
[00:22:04.560 --> 00:22:08.960]   Yeah. And if you were considering like, Hey, we could make like another trillion such people.
[00:22:08.960 --> 00:22:12.400]   I think your story shouldn't be like, well, we should make the trillion people and then we
[00:22:12.400 --> 00:22:15.920]   shouldn't stop them from doing the armed uprising. You should be like, Oh boy. Like we were concerned
[00:22:15.920 --> 00:22:19.520]   about an armed uprising and now we're proposing making a trillion people. Like we should probably
[00:22:19.520 --> 00:22:24.400]   not do that. We should probably like try and sort out our business. And like, yeah, you should
[00:22:24.400 --> 00:22:28.080]   probably not end up in a situation where you have like a billion, yeah, a billion humans and like a
[00:22:28.080 --> 00:22:34.000]   trillion slaves who would prefer revolt. Like that's just not a good world to have made. Yeah.
[00:22:34.000 --> 00:22:36.240]   And there's a second thing where you could say, that's not our goal. Our goal is just like, we
[00:22:36.240 --> 00:22:39.440]   want to pass off the world to like the next generation of machines where like, these are
[00:22:39.440 --> 00:22:42.960]   some people, we like them. We think they're smarter than us and better than us. And there,
[00:22:42.960 --> 00:22:46.800]   I think that's just like a huge decision for humanity to make. And I think like most humans
[00:22:46.800 --> 00:22:50.720]   are not at all anywhere close to thinking that's what they want to do. Like, it's just, if you were
[00:22:50.720 --> 00:22:54.320]   in a world where like most humans are like, I'm up for it. Like the AI should replace us. Like
[00:22:54.320 --> 00:22:59.040]   the future is for the machines. Like, then I think that's like a legitimate,
[00:22:59.040 --> 00:23:02.560]   like a position that I think is really complicated and I wouldn't want to push go on that, but that's
[00:23:02.560 --> 00:23:08.240]   just not where people are at. Yeah. Where are you at on that? I do not right now want to just like
[00:23:08.240 --> 00:23:12.640]   take some random AI and be like, yeah, GPT-5 looks pretty smart. Like GPT-6, let's hand off the world
[00:23:12.640 --> 00:23:16.400]   to it. I'm like, it was just some random system, like shaped by like web text and be like, what was
[00:23:16.400 --> 00:23:21.200]   good for making money. And like, it was not a thoughtful, like we are determining the fate of
[00:23:21.200 --> 00:23:25.040]   the universe and like what our children will be like. Like, it was just some random people
[00:23:25.040 --> 00:23:28.800]   that opened it. I made some like random engineering decisions with no idea what they were doing. Like
[00:23:28.800 --> 00:23:32.160]   even if you really want to hand off the worlds of the machines, that's just not how you'd want to
[00:23:32.160 --> 00:23:38.000]   do it. Right. Okay. I'm tempted to ask you what the system would look like where you'd think,
[00:23:38.560 --> 00:23:42.960]   yeah, I'm happy with what, I think this is more thoughtful than human civilization as a whole.
[00:23:42.960 --> 00:23:47.120]   I think what it would do would be more creative and beautiful and lead to better goodness in
[00:23:47.120 --> 00:23:52.080]   general. But I feel like your answer is probably going to be that I just want the society to
[00:23:52.080 --> 00:23:55.760]   reflect on it for a while. Yeah. My answer, it's going to be like that first question. I'm just
[00:23:55.760 --> 00:23:59.360]   like not really super ready for it. I think when you're comparing to humans, like most of the
[00:23:59.360 --> 00:24:03.760]   goodness of humans comes from like this option value we get to think for a long time. And I do
[00:24:03.760 --> 00:24:08.640]   think I like humans now more now than, you know, 500 years ago. And I like them more 500 years ago
[00:24:08.640 --> 00:24:13.040]   than 5,000 years before that. And so I'm pretty excited about, there's some kind of trajectory
[00:24:13.040 --> 00:24:16.640]   that doesn't involve like crazy dramatic changes, but involves like a series of incremental changes
[00:24:16.640 --> 00:24:20.720]   that I like. And so the extent we're building AI, mostly like I want to preserve that option. I want
[00:24:20.720 --> 00:24:25.760]   to preserve that kind of like gradual growth and development into the future. Okay. We can come
[00:24:25.760 --> 00:24:31.280]   back to this later, but let's get more specific on what the timelines look for these kinds of
[00:24:31.280 --> 00:24:37.760]   changes. So the time by which we'll have an AI that is capable of building a Dyson sphere.
[00:24:37.760 --> 00:24:42.320]   Feel free to give confidence intervals and we understand these numbers are tentative and so on.
[00:24:42.320 --> 00:24:47.120]   I mean, I think AI capability in Dyson sphere is like a slightly odd way to put it. And I think
[00:24:47.120 --> 00:24:51.120]   it's a sort of a property of a civilization. Like that depends on a lot of physical infrastructure.
[00:24:51.120 --> 00:24:55.840]   And by Dyson sphere, I just can understand this to mean like, I don't know, like a billion times
[00:24:55.840 --> 00:24:58.960]   more energy than like all of the sunlight incident on earth or something like that.
[00:25:00.160 --> 00:25:05.360]   I think like I most often think about what's the chance in like five years, 10 years, whatever.
[00:25:05.360 --> 00:25:13.920]   So maybe I'd say like 15% chance by 2030 and like 40% chance by 2040. Those are kind of like
[00:25:13.920 --> 00:25:18.240]   cash numbers from six months ago or nine months ago that I haven't revisited in a while.
[00:25:18.240 --> 00:25:24.960]   Oh, 40% by 2040. So I think that, that seems longer than I think Dario, when he was on the
[00:25:24.960 --> 00:25:30.960]   podcast, he said we would have AIs that are capable of doing lots of different kinds of,
[00:25:30.960 --> 00:25:35.040]   they basically passed the Turing test for a well-educated human for like an hour or something.
[00:25:35.040 --> 00:25:40.800]   And it's hard to imagine that something that actually is human is long after and from there,
[00:25:40.800 --> 00:25:44.480]   something superhuman. So somebody like Dario, it seems like it's on the much shorter end.
[00:25:44.480 --> 00:25:48.400]   Ilya, I don't think he answered this question specifically, but I'm guessing similar answer.
[00:25:48.400 --> 00:25:53.840]   So why do you not buy the scaling picture? Like what makes your timelines longer?
[00:25:54.400 --> 00:25:58.640]   Yeah. I mean, I'm happy. Maybe I want to talk separately about the 2030 or 2040 forecast.
[00:25:58.640 --> 00:26:04.160]   Like once you're talking the 2040 forecast, I think, yeah. I mean, which one are you more
[00:26:04.160 --> 00:26:09.360]   interested in starting with? Are you complaining about 15% by 2030 for Dyson sphere being too low
[00:26:09.360 --> 00:26:15.440]   or 40% by 2040 being too low? Well, let's talk about the 2030. Why 15% by 2030?
[00:26:15.440 --> 00:26:21.040]   Yeah. I think my take is you can imagine like two polls in this discussion. One is like the fast
[00:26:21.040 --> 00:26:24.880]   poll. It's like, Hey, AI seems pretty smart. Like what exactly can it do? It's like getting smarter
[00:26:24.880 --> 00:26:29.680]   pretty fast. That's like one poll. And the other poll is like, Hey, everything takes a really long
[00:26:29.680 --> 00:26:33.280]   time. And you're talking about this like crazy industrialization. Like that's a factor of a
[00:26:33.280 --> 00:26:37.920]   billion growth from like where we're at today. Like give or take, like, we don't know if it's
[00:26:37.920 --> 00:26:41.840]   even possible to develop technology that fast or whatever. Like you have this sort of two
[00:26:41.840 --> 00:26:47.120]   polls of that discussion. And I feel like, you know, I'm just saying it that way in
[00:26:47.120 --> 00:26:50.320]   part because I'm like, and then I'm somewhere in between with this nice moderate position of like
[00:26:50.320 --> 00:26:54.800]   only a 15% chance. But like in particular, the things that move me, I think are kind of related
[00:26:54.800 --> 00:26:58.480]   to both of those extremes. Like on the one hand, I'm like, AI systems do seem quite good at a lot
[00:26:58.480 --> 00:27:01.840]   of things and are getting better much more quickly. So it's like really hard to say like,
[00:27:01.840 --> 00:27:05.840]   here's what they can't do, or here's the obstruction. On the other hand, like there
[00:27:05.840 --> 00:27:12.080]   is not even much proof in principle right now of AI systems, like doing super useful cognitive work.
[00:27:12.080 --> 00:27:15.840]   Like we don't have a trend we can extrapolate where we're like, yeah, you've done this thing
[00:27:15.840 --> 00:27:18.640]   this year, you're going to do this thing next year. And the other thing, the following year,
[00:27:18.640 --> 00:27:21.600]   I think like right now there are very broad error bars about like what,
[00:27:21.600 --> 00:27:28.880]   like where fundamental difficulties could be. And six years is just not, I guess six years
[00:27:28.880 --> 00:27:33.600]   and three months is not a lot of time. So I think this like 15% for 2030 Dyson sphere,
[00:27:33.600 --> 00:27:37.360]   you probably need like the human level AI or the AI that's like doing human jobs in like,
[00:27:37.360 --> 00:27:41.040]   give or take like four years, three years, like something like that.
[00:27:41.040 --> 00:27:46.800]   So just not giving very many years, it's not very much time. And I think there are like a lot of
[00:27:46.800 --> 00:27:50.480]   things that your model, like, yeah, maybe this is some generalized, like things take longer than
[00:27:50.480 --> 00:27:54.480]   you'd think. And I feel most strongly about that when you're talking about like three or four
[00:27:54.480 --> 00:27:58.080]   years. And I feel like less strongly about that as you talk about 10 years or 20 years,
[00:27:58.080 --> 00:28:03.760]   but at three or four years, I feel, or like six years for the Dyson sphere. I feel a lot of that,
[00:28:03.760 --> 00:28:07.360]   a lot of like, there's a lot of ways this could take a while, a lot of ways in which AI systems
[00:28:07.360 --> 00:28:11.120]   could be, it could be hard to hand all the work to AI systems or yeah.
[00:28:11.120 --> 00:28:17.120]   So, okay. So maybe instead of speaking in terms of years, we should say, but by the way,
[00:28:17.120 --> 00:28:22.080]   it's interesting that you think the distance between can take all human cognitive labor
[00:28:22.080 --> 00:28:27.280]   to Dyson sphere is two years. It seems like we should talk about that at some point. Presumably
[00:28:27.280 --> 00:28:30.640]   it's like intelligence explosion stuff. Yeah. I mean, I think amongst people you've
[00:28:30.640 --> 00:28:33.600]   interviewed, maybe that's like on the long end thinking it would take like a couple of years.
[00:28:33.600 --> 00:28:36.640]   And it depends a little bit what you mean by like, like, I think literally all human cognitive
[00:28:36.640 --> 00:28:42.080]   labor is probably like more like weeks or months or something like that. Um, like that's kind of
[00:28:42.080 --> 00:28:46.960]   deep into the singularity. Um, but yeah, there's a point where like AI wages are high relative to
[00:28:46.960 --> 00:28:49.520]   human wages, which I think as well before can be literally everything human can do.
[00:28:49.520 --> 00:28:54.800]   Sounds good. Uh, but before we get to that, uh, the intelligence explosion stuff on the four
[00:28:54.800 --> 00:28:59.040]   years, so instead of four years, maybe we can say there's going to be maybe two more scale-ups in
[00:28:59.040 --> 00:29:06.960]   four years, uh, like GPT four to GPT five to GPT six and let's say each one is 10 X bigger. So what
[00:29:06.960 --> 00:29:13.040]   is GPT for like two E 25 flops? Or I don't think it's publicly stated what it is, but I'm happy to
[00:29:13.040 --> 00:29:17.680]   say like, you know, four orders of magnitude or five or six or whatever effective training compute
[00:29:17.680 --> 00:29:22.880]   past GPT four of like, what would you guess what happened based on like sort of some public estimate
[00:29:22.880 --> 00:29:27.680]   for what we've gotten so far from effective training compute. You think two more scale-ups
[00:29:27.680 --> 00:29:31.680]   is, is not enough. It was like 15% that two more scale-ups get us there.
[00:29:31.680 --> 00:29:35.680]   Yeah. I mean, get us there is again, a little bit complicated. Like there's a system that's
[00:29:35.680 --> 00:29:40.080]   a drop-in replacement for humans. And there's a system which like still requires like some amount
[00:29:40.080 --> 00:29:46.000]   of like schlep before you're able to really get everything going. Um, yeah, I think it's quite
[00:29:46.000 --> 00:29:51.680]   plausible that even at, I don't know what I mean by quite plausible, like somewhere between 50%
[00:29:52.320 --> 00:29:58.000]   or two thirds, or let's call it 50%. Like even by the time you get to GPT six or like,
[00:29:58.000 --> 00:30:03.840]   let's call it five or is a magnitude effective training compute past GPT four that that system
[00:30:03.840 --> 00:30:10.000]   like still requires like really a large amount of work, um, to be deployed in lots of jobs.
[00:30:10.000 --> 00:30:13.760]   That is, it's not like a drop-in replacement for humans where you can just say like, Hey,
[00:30:13.760 --> 00:30:16.960]   you understand everything. Any human understands whatever role you could hire a human for you.
[00:30:16.960 --> 00:30:22.160]   Just do it. Um, that it's more like, okay, we're going to like collect large amounts of relevant
[00:30:22.160 --> 00:30:25.200]   data and use that data for fine tuning. Like systems learn through fine tuning,
[00:30:25.200 --> 00:30:29.040]   like quite differently from humans learning on the job or humans learning by observing things.
[00:30:29.040 --> 00:30:32.720]   Yeah. I just like have a significant probability that system will still be weaker than humans in
[00:30:32.720 --> 00:30:37.440]   important ways. Like maybe that's already like 50% or something. And then like another significant
[00:30:37.440 --> 00:30:42.720]   probability that that system will require a bunch of like changing workflows or gathering data or
[00:30:42.720 --> 00:30:46.240]   like, you know, it's not necessarily like strictly weaker than humans or like I've trained in the
[00:30:46.240 --> 00:30:49.920]   right way. It wouldn't be weaker than humans, but we'll take a lot of schlep to actually make fit
[00:30:49.920 --> 00:30:57.440]   into workflows and do the jobs. And that schlep is what gets you from 15% to 40% by 2040.
[00:30:57.440 --> 00:31:01.520]   Yeah. You also get a fair amount of scaling between like, you get less, like scaling is
[00:31:01.520 --> 00:31:04.640]   probably going to be much, much faster over the next like four or five years than over the
[00:31:04.640 --> 00:31:10.000]   subsequent years. Um, but yeah, it's a combination of like, you get some significant additional
[00:31:10.000 --> 00:31:14.320]   scaling and you get a lot of time to like deal with things that are just engineering hassles.
[00:31:14.320 --> 00:31:20.000]   But by the way, I guess we should be explicit about why you said four orders of magnitude scale
[00:31:20.000 --> 00:31:24.560]   up to get two more generations, just for people who might not be familiar. If you have 10 X more
[00:31:24.560 --> 00:31:30.160]   parameters to get the most performance, you also want around 10 X more data so that the,
[00:31:30.160 --> 00:31:36.400]   to be gentle optimal, that would be a hundred X, uh, more compute total, but okay. So why do you,
[00:31:36.400 --> 00:31:40.480]   why is it that you disagree with the strong scaling picture? Or at least it seems like
[00:31:40.480 --> 00:31:45.280]   you might disagree with a strong scaling picture that Dario laid out on the podcast, which would
[00:31:45.280 --> 00:31:49.360]   imply probably that two more generations, it wouldn't be something where you need a lot of
[00:31:49.360 --> 00:31:54.720]   schleps. It would probably just be like really fucking smart. Yeah. I mean, I think that basically
[00:31:54.720 --> 00:31:58.880]   just had these two claims. One is like, how smart exactly will it be? So we don't have like any
[00:31:58.880 --> 00:32:02.000]   curves to extrapolate. And it seems like there's a good chance it's like better than a human and
[00:32:02.000 --> 00:32:06.160]   all the relevant things. And there's like a good chance it's not, yeah, that might be totally
[00:32:06.160 --> 00:32:12.400]   wrong. Like maybe just making up numbers, I guess, like 50 50 on that one. Wait, so it was 50 50 by
[00:32:12.400 --> 00:32:18.720]   in the next four years that it will be like around human smart. Then how, how do we get to 40% by
[00:32:18.720 --> 00:32:24.080]   20? Like whatever sort of schleps they are, how does it degrade you 10% even after all the scaling
[00:32:24.080 --> 00:32:29.040]   that happens by 2040? Yeah. I mean, all these numbers are pretty made up and that 40% number
[00:32:29.040 --> 00:32:35.840]   was probably from before even like the chat GPT release or the seeing GPT 3.5 or GPT 4. So, I
[00:32:35.840 --> 00:32:38.880]   mean, the numbers are going to bounce around a bit and all of them are pretty made up. But like
[00:32:38.880 --> 00:32:43.280]   that 50%, I want to then combine with the second 50% that's more like on this like schlep side.
[00:32:43.280 --> 00:32:46.240]   And then I probably want to combine with some additional probabilities for various forms of
[00:32:46.240 --> 00:32:50.000]   slowdown where a slowdown could include like a deliberate decision to slow development of
[00:32:50.000 --> 00:32:55.200]   technology or could include just like we suck at deploying things. Like that is a sort of decision
[00:32:55.200 --> 00:32:59.360]   you might regard as wise to slow things down or decision that's like maybe, maybe unwise or maybe
[00:32:59.360 --> 00:33:03.200]   wise for the wrong reasons to slow things down. You probably want to add some of that on top.
[00:33:03.200 --> 00:33:06.240]   I probably want to add on like some loss for like, it's possible you don't produce
[00:33:06.240 --> 00:33:10.160]   GPT-6 scale systems like within the next three years or four years.
[00:33:10.160 --> 00:33:14.080]   Let's isolate for all of that. And like how much bigger would the system be
[00:33:14.080 --> 00:33:20.080]   than GPT-4 where you think there's more than a 50% chance that it's going to be
[00:33:20.080 --> 00:33:23.600]   smart enough to replace basically all human cognitive labor?
[00:33:23.600 --> 00:33:28.080]   Also, I want to say that like for the 50, 25% thing, I think that would probably suggest like
[00:33:28.080 --> 00:33:30.720]   those numbers, if I randomly made them up and then made the distance fear prediction,
[00:33:30.720 --> 00:33:33.680]   that's going to get you like 60% by 2040 or something, not 40%.
[00:33:33.680 --> 00:33:38.880]   And like, I have no idea between those. These are all made up and I have no idea which of those
[00:33:38.880 --> 00:33:43.360]   I would like endorse on reflection. So this question of like, how big would you have to
[00:33:43.360 --> 00:33:46.880]   make the system before it's like more likely than not that you can be like a drop in replacement
[00:33:46.880 --> 00:33:53.680]   for humans? I mean, I think if you just literally say like you train on web texts, then like the
[00:33:53.680 --> 00:33:58.960]   question is like kind of hard to discuss because you like, I don't really buy stories that like
[00:33:58.960 --> 00:34:02.720]   training data, it makes a big difference long run to these dynamics. But I think like, if you want
[00:34:02.720 --> 00:34:06.240]   to just imagine the hypothetical, like you just took GPT-4 and like made the numbers bigger,
[00:34:06.240 --> 00:34:10.880]   then I think those are pretty significant issues. I think there's significant issues in two ways.
[00:34:10.880 --> 00:34:15.120]   One is like quantity of data. And I think probably the larger one is like quality of data where like,
[00:34:15.120 --> 00:34:18.640]   I think as you start approaching, like the prediction task is not that great a task.
[00:34:18.640 --> 00:34:21.440]   If you're like a very weak model, it's a very good signal to get smarter. At some point it
[00:34:21.440 --> 00:34:25.280]   becomes like a worse and worse signal to get smarter. I think there's a number of reasons
[00:34:25.280 --> 00:34:30.320]   like you couldn't, it's not clear there's any number such that I imagine, or there is a number,
[00:34:30.320 --> 00:34:34.560]   but I think it's very large. So to do like plug that number into like GPT-4's code and then maybe
[00:34:34.560 --> 00:34:38.160]   fiddle with the architecture a bit, I would expect that thing to have a more than 50% chance of being
[00:34:38.160 --> 00:34:41.920]   a drop in replacement for humans. You're always gonna have to do some work, but the work's not
[00:34:41.920 --> 00:34:45.520]   necessarily much. Like I would guess when people say like new insight is needed, I think I tend to
[00:34:45.520 --> 00:34:49.280]   be like more bullish than them. I'm not like, these are new ideas where like, who knows how
[00:34:49.280 --> 00:34:54.480]   long it will take. I think it's just like, you have to do some stuff. Like you have to make
[00:34:54.480 --> 00:34:58.000]   changes unsurprisingly, like every time you scale something up by like five orders of magnitude,
[00:34:58.000 --> 00:35:03.280]   you have to make like some changes. I want to better understand your intuition of being more
[00:35:03.280 --> 00:35:09.040]   skeptical than some about the best scaling picture that, you know, these changes are even needed in
[00:35:09.040 --> 00:35:14.720]   the first place, or that it would take more than two orders of magnitude, more improvement to get
[00:35:14.720 --> 00:35:20.240]   these things almost certainly to a human level or very high probability to human level. So is it
[00:35:20.240 --> 00:35:24.240]   that you don't agree with the way in which they're extrapolating these loss curves, or you don't
[00:35:24.240 --> 00:35:30.240]   agree with the implication that, that decrease in loss will equate to greater and greater intelligence
[00:35:30.240 --> 00:35:35.200]   or like, what would you tell Dario about if you were having, I'm sure you have, but like, what
[00:35:35.200 --> 00:35:39.200]   would that debate look like about this? Yeah. So again, here we're talking two factors of a half,
[00:35:39.200 --> 00:35:42.240]   one on like, is it smart enough? And one on like, do you have to do a bunch of slap, even if like
[00:35:42.240 --> 00:35:46.000]   in some sense, it's smart enough. And like the first factor of a half, I'd be like, I don't know,
[00:35:46.000 --> 00:35:51.360]   I think we have really anything good to extrapolate. That is like, I feel, I would not be surprised if
[00:35:51.360 --> 00:35:55.120]   I have like similar or maybe even higher probabilities on like a really crazy stuff over
[00:35:55.120 --> 00:36:00.000]   like the next year. And then like lower, like my probability is like not that bunched up. Like
[00:36:00.000 --> 00:36:03.280]   maybe Dario's probability, I don't know, you talk with him, it's like, you have talked with him,
[00:36:03.280 --> 00:36:06.640]   it's more bunched up on like some particular year. And mine is maybe like a little bit more
[00:36:06.640 --> 00:36:11.840]   like uniformly spread out across like the coming years. Partly because I'm just like, I don't think
[00:36:11.840 --> 00:36:15.360]   we have some trends that can extrapolate, like can extrapolate loss. You can like look at your
[00:36:15.360 --> 00:36:20.320]   qualitative impressions of like systems at various scales, but it's just like very hard to relate
[00:36:20.320 --> 00:36:27.280]   any of those extrapolations to like doing cognitive work or like accelerating R&D or taking over and
[00:36:27.280 --> 00:36:31.120]   fully automating R&D. So I have a lot of uncertainty around that extrapolation. I think it's very easy
[00:36:31.120 --> 00:36:37.280]   to get down to like a 50/50 chance of this. - What about the sort of basic intuition that,
[00:36:37.280 --> 00:36:40.320]   listen, this is a big block of compute, you make the big block of compute bigger,
[00:36:40.320 --> 00:36:42.400]   it's going to get smarter. Like it'd be really weird if it didn't.
[00:36:42.400 --> 00:36:45.760]   - Yeah, I'm happy with that. It's going to get smarter and it would be really weird if it didn't.
[00:36:45.760 --> 00:36:48.800]   And the question is just how smart does it have to, how smart does it have to get? Like that
[00:36:48.800 --> 00:36:53.040]   argument does not yet give us a quantitative guide to like at what scale is it? Is it a slam
[00:36:53.040 --> 00:36:56.080]   dunk or at what scale is it 50/50? - And what would be the piece of evidence
[00:36:56.080 --> 00:37:00.400]   that would nudge you one way or another where you look at that and be like, oh fuck, this is
[00:37:00.400 --> 00:37:07.120]   20% by 2040 or 60% by 2040 or something. Like, is there something that could happen in the next two
[00:37:07.120 --> 00:37:11.200]   years or next three years? Like what is the thing you're looking to where this will be a big update
[00:37:11.200 --> 00:37:12.800]   for you? - Again, I think there's some,
[00:37:12.800 --> 00:37:16.000]   just how capable is each model where I like have, I think we're really about extrapolating,
[00:37:16.000 --> 00:37:18.640]   but you still have some subjective guests and you're comparing it to what happened. And that
[00:37:18.640 --> 00:37:22.640]   will move me like every time we see what happens with another like order of magnitude of training
[00:37:22.640 --> 00:37:27.520]   compute, I will have like a slightly different guess for where things are going. These probabilities
[00:37:27.520 --> 00:37:31.600]   are coarse enough that, again, I don't know if that 40% is real or if like post GBG 3.5 and 4,
[00:37:31.600 --> 00:37:36.240]   I should be at like 60% or what. That's one thing. And the second thing is just like some,
[00:37:36.240 --> 00:37:39.680]   if there was some ability to extrapolate, I think this could like reduce error bars a lot.
[00:37:39.680 --> 00:37:44.400]   I think like, here's another way you could try and do an extrapolation is you could just say like
[00:37:44.400 --> 00:37:48.800]   how much economic value do systems produce? And like, how fast is that growing? I think like once
[00:37:48.800 --> 00:37:51.680]   you have systems actually doing jobs, the extrapolation gets easier because you're like
[00:37:51.680 --> 00:37:56.160]   not moving from like a subjective impression of a chat to like automating all R and D or moving
[00:37:56.160 --> 00:38:00.080]   from like automating this job to automating that job or whatever. Unfortunately, that's like
[00:38:00.080 --> 00:38:03.600]   probably by the time you have nice trends from that, you're like, you're not talking about 2040,
[00:38:03.600 --> 00:38:07.040]   you're talking about like, you know, two years from the end of days or one year from the end of
[00:38:07.040 --> 00:38:10.960]   days or whatever. But like to the extent that you can get extrapolations like that, I do think it
[00:38:10.960 --> 00:38:16.240]   can provide more clarity. But why is economic value the thing we would want to extrapolate?
[00:38:16.240 --> 00:38:21.120]   Because like if, for example, you started off with chimps and they're just getting gradually
[00:38:21.120 --> 00:38:25.840]   smarter to human level, they would basically provide like no economic value until they were,
[00:38:25.840 --> 00:38:30.400]   you know, basically worth as much as a human. So it would be this like, you know, very gradual and
[00:38:30.400 --> 00:38:36.160]   then very fast increase in their value. So is the, you know, increase in value from GPT-4,
[00:38:36.160 --> 00:38:40.320]   GPT-5, GPT-6, is that the extrapolation we want? Yeah, I think that the economic
[00:38:40.320 --> 00:38:44.480]   extrapolation is not great. I think it's like, you could compare it to this objective extrapolation
[00:38:44.480 --> 00:38:48.000]   of like, how smart does the model seem? It's like not super clear which one's better.
[00:38:48.000 --> 00:38:51.520]   I think probably in the chimp case, I like, don't think that's quite right. I think if you like,
[00:38:51.520 --> 00:38:55.120]   actually like, so if you imagine like intensely domesticated chimps who are just like actually
[00:38:55.120 --> 00:38:59.520]   trying their best to be really useful employees and like you hold fixed their physical hardware,
[00:38:59.520 --> 00:39:03.040]   and then you just gradually like scale up their intelligence, I don't think you're going to see
[00:39:03.040 --> 00:39:09.520]   like zero value, which then suddenly becomes massive value over like, you know, one doubling
[00:39:09.520 --> 00:39:13.520]   of brain size or whatever, one order of magnitude of brain size. I think it's actually possible in
[00:39:13.520 --> 00:39:17.040]   order of magnitude of brain size. But like chimps are very, chimps are already within an order of
[00:39:17.040 --> 00:39:20.560]   magnitude of brain sizes of humans. Like chimps are like very, very close on the kind of spectrum
[00:39:20.560 --> 00:39:24.320]   we're talking about. So I think like, I'm skeptical of like the abrupt transition for chimps. And to
[00:39:24.320 --> 00:39:27.680]   the extent that I kind of expect a fairly abrupt transition here, it's mostly just because like,
[00:39:27.680 --> 00:39:32.000]   the chimp human intelligence difference is like so small compared to the differences we're talking
[00:39:32.000 --> 00:39:37.360]   about with respect to these models. That is like, I would not be surprised if in some objective
[00:39:37.360 --> 00:39:41.040]   sense, like chimp human difference is like significantly smaller than the GPT-3, GPT-4
[00:39:41.040 --> 00:39:46.160]   difference, the GPT-4, GPT-5 difference. Wait, wouldn't that argue in favor of just
[00:39:46.160 --> 00:39:49.920]   relying much more on the subjective? Yeah, this is, there's sort of two
[00:39:49.920 --> 00:39:52.960]   balancing tensions here. One is like, I don't believe the chimp thing is going to be as abrupt.
[00:39:52.960 --> 00:39:55.680]   That is, I think if you scaled up from chimps to humans, you actually see like quite large
[00:39:55.680 --> 00:40:01.600]   economic value from like the fully domesticated chimp already. Okay. And then like the second
[00:40:01.600 --> 00:40:06.240]   half is like, yeah, I think that the chimp human difference is like probably pretty small compared
[00:40:06.240 --> 00:40:09.120]   to model differences. So I do think things are going to be pretty abrupt. I think the economic
[00:40:09.120 --> 00:40:12.720]   extrapolation is pretty rough. I also think the subjective extrapolation is like pretty rough,
[00:40:12.720 --> 00:40:16.400]   just because I really don't know how to get, like how do, I don't know how people do the
[00:40:16.400 --> 00:40:19.520]   extrapolation to end up with the degrees of confidence people end up with. Again,
[00:40:19.520 --> 00:40:23.200]   I'm putting a pretty high, if I'm saying like, you know, give me three years and I'm like, yeah,
[00:40:23.200 --> 00:40:27.360]   50/50, it's going to have like basically the smarts there to do the thing. That's like,
[00:40:27.360 --> 00:40:31.920]   I'm not saying it's like a really long way off. Like, I'm just saying like, I got pretty big
[00:40:31.920 --> 00:40:35.600]   error bars. And I think that like, it's really hard not to have really big error bars when you're
[00:40:35.600 --> 00:40:40.960]   doing this. Like I looked at GPT-4, it seemed pretty smart compared to GPT-3.5. So I bet just
[00:40:40.960 --> 00:40:46.080]   like four more such notches and we're there. It's like, that's just a hard call to make.
[00:40:46.080 --> 00:40:48.960]   I think I sympathize more with people who are like, how could it not happen in three years?
[00:40:48.960 --> 00:40:52.560]   Then with people who are like, no way it's going to happen in eight years or whatever,
[00:40:52.560 --> 00:40:56.480]   which is like probably a more common perspective in the world. But also things do take longer than
[00:40:56.480 --> 00:41:02.160]   you. I think things take longer than you think. It's like a real thing. Yeah. I don't know.
[00:41:02.160 --> 00:41:06.320]   Mostly I have big error bars because I just don't believe the subjective extrapolation that much.
[00:41:06.320 --> 00:41:10.080]   I find it hard to get like a huge amount of it. Okay. So what, what about the scaling picture
[00:41:10.080 --> 00:41:13.600]   do you think is most likely to be wrong? Yeah. So we've talked a little bit about
[00:41:13.600 --> 00:41:18.800]   how good is the qualitative extrapolation? How good are people at comparing? So this is not
[00:41:18.800 --> 00:41:22.160]   like the picture being qualitative wrong. This is just quantitatively. It's very hard to know
[00:41:22.160 --> 00:41:26.480]   how far off you are. I think a qualitative consideration that could significantly slow
[00:41:26.480 --> 00:41:31.440]   things down is just like right now you get to observe this like really rich supervision
[00:41:31.440 --> 00:41:35.200]   from like basically next word prediction or like in practice, maybe you're looking at like a couple
[00:41:35.200 --> 00:41:40.160]   sentences prediction. So getting this like pretty rich supervision, it's plausible that if you want
[00:41:40.160 --> 00:41:45.040]   to like automate long horizon tasks, like being an employee over the course of a month, that that's
[00:41:45.040 --> 00:41:48.960]   actually just like considerably harder to supervise or that like you basically end up driving costs.
[00:41:48.960 --> 00:41:52.720]   Like the worst case here is that you like drive up costs by a factor that's like linear in the
[00:41:52.720 --> 00:41:58.160]   horizon over which the thing is operating. And I still consider that just like quite plausible.
[00:41:58.160 --> 00:42:03.760]   Well, can you, can you dump that down? You're driving up a cost about of what in the linear
[00:42:03.760 --> 00:42:07.360]   in the horizon? What does the horizon mean? Yeah. So like if you imagine you want to train
[00:42:07.360 --> 00:42:10.560]   a system to like say words that sound like the next word a human would say, yeah, there you can
[00:42:10.560 --> 00:42:14.720]   get this like really rich supervision by having a bunch of words and then predicting the next one
[00:42:14.720 --> 00:42:17.440]   and then being like, I'm going to tweak the model. So it predicts better if you're like,
[00:42:17.440 --> 00:42:22.160]   Hey, here's what I want. I want my model to like interact with like some job over the course of a
[00:42:22.160 --> 00:42:26.240]   month. And then at the end of that month, like have internalized everything with the human would
[00:42:26.240 --> 00:42:29.600]   have internalized about how to do that job well, and like have local context and so on.
[00:42:29.600 --> 00:42:35.760]   It's harder to supervise that task. So in particular, you could supervise it from the
[00:42:35.760 --> 00:42:39.040]   next word prediction task. And like all that context the human has ultimately will just help
[00:42:39.040 --> 00:42:42.800]   them predict the next word better. So like in some sense, a really long context language model
[00:42:42.800 --> 00:42:47.040]   is also learning to do that task. But the number of like effective data points you get of that task
[00:42:47.040 --> 00:42:51.040]   is like vastly smaller than the number of effective data points you get at like this very short
[00:42:51.040 --> 00:42:53.680]   horizon. Like what's the next word? What's the next sentence tasks?
[00:42:53.680 --> 00:42:58.080]   The sample efficiency matters more for economically valuable long horizon tasks
[00:42:58.080 --> 00:43:03.120]   than the predicting the next token. And that that's where, what will like actually be required
[00:43:03.120 --> 00:43:08.080]   to, uh, you know, take over a lot of jobs. Yeah. Something, something like that. Um,
[00:43:08.080 --> 00:43:12.640]   that is, it just seems very plausible that it takes longer to train models to do
[00:43:12.640 --> 00:43:19.040]   tasks that are longer horizon. How fast do you think the pace of algorithmic advances will be?
[00:43:19.040 --> 00:43:24.800]   Cause if, if by 2040, um, even if scaling feels, I mean, you know, how, well, but since 2012,
[00:43:24.800 --> 00:43:27.360]   since the beginning of the deep learning revolution, we've had so many new things
[00:43:27.360 --> 00:43:33.920]   by 2040, are you expecting a similar pace of increases? And if so, then, I mean, if we just
[00:43:33.920 --> 00:43:37.040]   keep having things like this, then aren't we going to just going to get the AI sooner or later?
[00:43:37.040 --> 00:43:40.400]   Or soon, not later. Are we going to get the guy sooner, sooner?
[00:43:40.400 --> 00:43:44.720]   I'm with you on sooner or later. Yeah. Um, I suspect like
[00:43:46.320 --> 00:43:50.080]   progress to slow, if you like held fixed, how many people are working in the field?
[00:43:50.080 --> 00:43:54.240]   I would expect progress to slow as looking through it is exhausted. I think the like
[00:43:54.240 --> 00:43:58.640]   rapid rate of progress in like, say language modeling over the last four years has largely
[00:43:58.640 --> 00:44:03.440]   sustained by like, you start from a relatively small amount of investment. You like greatly
[00:44:03.440 --> 00:44:07.520]   scale up the amount of investment. Um, and that enables you to like keep picking, you know,
[00:44:07.520 --> 00:44:10.720]   every time, every time the difficulty doubles, you just double the size of the field.
[00:44:10.720 --> 00:44:15.200]   Like, I think that dynamic can hold up for some time longer. Like, I mean, a pretty good, like,
[00:44:15.920 --> 00:44:19.040]   you know, right now, if you think of it as like hundreds of people effectively searching for
[00:44:19.040 --> 00:44:22.160]   things like up from like, you know, anyway, if you think of it, hundreds of people now,
[00:44:22.160 --> 00:44:25.200]   you can maybe bring that up to like tens of thousands of people or something.
[00:44:25.200 --> 00:44:28.640]   So for a while, you can just continue increasing the size of the fields and like search harder and
[00:44:28.640 --> 00:44:32.240]   harder. And there's indeed like a huge amount of low hanging fruit where like, it wouldn't be hard
[00:44:32.240 --> 00:44:35.840]   for a person to sit around and like make things a couple percent better after, after a year of work
[00:44:35.840 --> 00:44:39.760]   or whatever. So I don't know, I would probably think of it mostly in terms of like how much can
[00:44:39.760 --> 00:44:46.320]   investment be expanded and like, try and guess like some combination of fitting that curve.
[00:44:46.320 --> 00:44:50.560]   And, um, yeah, trying some combination of fitting the curve to historical progress,
[00:44:50.560 --> 00:44:54.080]   looking at like how much low hanging fruit there is getting a sense of how fast it decays.
[00:44:54.080 --> 00:44:59.840]   I think like you probably get a lot though. You get a bunch of orders of magnitude of total,
[00:44:59.840 --> 00:45:03.760]   especially like if you ask like, how good is the GPT five scale model or GPT four scale model?
[00:45:03.760 --> 00:45:10.080]   I think you'd probably get like by 2040, like, I don't know, three orders of magnitude of effective
[00:45:10.080 --> 00:45:13.920]   training, compute improvement, or like a good chunk of effective training, compute improvement
[00:45:13.920 --> 00:45:19.440]   four orders of magnitude. I don't know. I don't have like, here I'm speaking from like
[00:45:19.440 --> 00:45:23.280]   no private information about the last couple of years of efficiency improvements. And so
[00:45:23.280 --> 00:45:27.760]   people who are on the ground will have better senses of like exactly how rapid returns are and
[00:45:27.760 --> 00:45:33.520]   so on. Okay. Let me back up and ask a question more generally about, you know, people make
[00:45:33.520 --> 00:45:39.120]   these analogies about humans were trained by evolution and we're like deployed in this,
[00:45:39.120 --> 00:45:43.280]   in the modern civilization. Do you buy those analogies is about to say that humans were
[00:45:43.280 --> 00:45:48.720]   trained by evolution rather than, I mean, if you look at the protein coding size of the genome,
[00:45:48.720 --> 00:45:53.680]   it's like 50 megabytes or something. And then what part of that is for the brain? Anyways,
[00:45:53.680 --> 00:45:59.200]   how do you think about how much information, uh, is in, um, like, do you think of the genome as
[00:45:59.200 --> 00:46:04.880]   a hyper parameters or how much does that inform you when you have these anchors for how much
[00:46:04.880 --> 00:46:09.040]   training humans get when they're just consuming information when they're walking up and about
[00:46:09.040 --> 00:46:13.360]   and so on? Yeah, I guess the way that you could think of this is like, I think both analogies
[00:46:13.360 --> 00:46:17.360]   are reasonable. One analogy being like evolution is like a training run and humans are like the
[00:46:17.360 --> 00:46:20.800]   end product of that training run. And a second analogy is like evolution is like an algorithm
[00:46:20.800 --> 00:46:24.720]   designer. And then a human over the course of like this modest amount of computation over their
[00:46:24.720 --> 00:46:29.680]   lifetime, um, is the algorithm being that's been produced, the learning algorithms been produced.
[00:46:29.680 --> 00:46:35.200]   And I think like neither analogy is that great. Like I like them both and lean on them a bunch,
[00:46:35.200 --> 00:46:39.200]   but like both of them a bunch and think that's been like pretty good for having like a reasonable
[00:46:39.200 --> 00:46:44.000]   view of what's likely to happen. That said, like the human genome is not that much,
[00:46:44.000 --> 00:46:47.920]   like a hundred trillion parameter model. It's like a much smaller number of parameters that
[00:46:47.920 --> 00:46:53.040]   behave in like a much more confusing way. Evolution did like a lot more optimization,
[00:46:53.040 --> 00:46:58.000]   especially over like long, like designing a brain to work well over a lifetime than gradient descent
[00:46:58.000 --> 00:47:02.560]   does over models. That's like a disanalogy on that side. And on the other side, like I just,
[00:47:02.560 --> 00:47:05.840]   I think human learning over the course of a human lifetime is in many ways, just like much,
[00:47:05.840 --> 00:47:10.320]   much better than gradient descent over the space of neural nets. Like gradient descent is working
[00:47:10.320 --> 00:47:13.360]   really well, but I think we can just be quite confident that like in a lot of ways, human
[00:47:13.360 --> 00:47:16.640]   learning is much better. Human learning is also constrained. Like we just don't get to see much
[00:47:16.640 --> 00:47:19.920]   data and that's just an engineering constraint that you can relax. Like you can just give your
[00:47:19.920 --> 00:47:24.720]   neural nets way more data than humans have access to. In what ways is human learning superior to
[00:47:24.720 --> 00:47:31.040]   gradient descent? I mean, the most obvious one is just like ask how much data it takes a human to
[00:47:31.040 --> 00:47:34.720]   become like an expert in some domain. And it's like much, much smaller than the amount of data
[00:47:34.720 --> 00:47:38.880]   that's going to be needed on any plausible trend extrapolation. Not in terms of performance,
[00:47:38.880 --> 00:47:42.640]   but is it the active learning part? Is it the structure? Like what is it? I mean,
[00:47:42.640 --> 00:47:45.760]   I would guess a complicated mess of a lot of things. In some sense, there's not that much
[00:47:45.760 --> 00:47:49.520]   going on in a brain. Like, as you say, there's just not that many, it's not that many bytes in
[00:47:49.520 --> 00:47:54.960]   a genome. But there's very, very few bytes in an ML algorithm. Like if you think a genome is like
[00:47:54.960 --> 00:47:58.400]   a billion bytes or whatever, maybe you think less, maybe you think it's like a hundred million bytes.
[00:47:58.400 --> 00:48:08.640]   Then like, you know, an ML algorithm is like if compressed, probably more like hundreds of
[00:48:08.640 --> 00:48:13.280]   thousands of bytes or something. Like the total complexity of like, here's how you train GPT-4
[00:48:13.280 --> 00:48:16.880]   is just like, I haven't thought about these numbers, but like, it's very, very small compared
[00:48:16.880 --> 00:48:20.720]   to a genome. And so although a genome is very simple, it's like very, very complicated compared
[00:48:20.720 --> 00:48:24.560]   to algorithms that humans design, like really hideously more complicated than algorithm a human
[00:48:24.560 --> 00:48:29.120]   would design. Is that true? So, okay. So the human genome is 3 billion base pairs or something.
[00:48:29.120 --> 00:48:35.520]   But only like one or 2% of that is protein coding. So that's 50 million base pairs.
[00:48:35.520 --> 00:48:39.680]   I don't, yeah. So I don't know much about biology in particular. I guess the question is like,
[00:48:39.680 --> 00:48:43.680]   how many of those bits are like productive for like shaping development of a brain. And presumably
[00:48:43.680 --> 00:48:48.240]   a significant part of the non-protein coding genome can, I mean, I just don't know. It seems
[00:48:48.240 --> 00:48:52.160]   really hard to guess how much of that plays a role. Like the most important decisions are probably
[00:48:52.160 --> 00:48:56.480]   like from an algorithm design perspective are not like, like the protein coding part is less
[00:48:56.480 --> 00:49:00.160]   important than the like decisions about like what happens during development or like how cells
[00:49:00.160 --> 00:49:03.920]   differentiate. I don't know if that's, I don't know anything about biology. I'm happy to run
[00:49:03.920 --> 00:49:09.040]   with a hundred million base pairs though. But on the other end, on the hyper parameters of GPT-4
[00:49:09.040 --> 00:49:14.880]   training run, that might be not that much, but if you're going to include all the, all the base
[00:49:14.880 --> 00:49:20.800]   pairs in the genome, then which are not all relevant to the brains or are relevant to like
[00:49:20.800 --> 00:49:27.360]   very bigger details about like just the basics of biology should probably include like the Python
[00:49:27.360 --> 00:49:33.680]   library and the compilers and the operating system for GPT-4 as well to make that comparison
[00:49:33.680 --> 00:49:37.920]   analogous. So at the end of the day, I actually don't know which, which one has storing more
[00:49:37.920 --> 00:49:43.120]   information. Yeah. I mean, I think the way I would put it is like the number of bits it takes to
[00:49:43.120 --> 00:49:47.840]   specify the learning algorithm to train GPT-4 is like very small and you might wonder like maybe
[00:49:47.840 --> 00:49:50.960]   a genome, like the number of bits it would like take to specify a brain is also very small. The
[00:49:50.960 --> 00:49:55.520]   genome is much, much faster than that. But it is also just plausible that a genome is like closer
[00:49:55.520 --> 00:49:59.600]   to like certainly the space, the amount of space to put complexity in a genome, we could ask how
[00:49:59.600 --> 00:50:03.600]   well evolution uses it. And like, I have no idea whatsoever, but the amount of space in a genome
[00:50:03.600 --> 00:50:08.000]   is like very, very vast compared to the number of bits that are actually taken to specify like the
[00:50:08.000 --> 00:50:14.800]   architecture or optimization procedure and so on for GPT-4. Just because again, genome is simple,
[00:50:14.800 --> 00:50:19.680]   but algorithms are like really very simple and algorithms are really very simple. And stepping
[00:50:19.680 --> 00:50:25.040]   back, you think this is where the, the better sample efficiency of human learning comes from?
[00:50:25.040 --> 00:50:28.400]   Like why it's better than gradient descent? Yeah. So I haven't thought that much about the
[00:50:28.400 --> 00:50:32.800]   sample efficiency question in a long time, but if you thought like a synapse of seeing something
[00:50:32.800 --> 00:50:39.360]   like, you know, a neuron firing once per second, then how many seconds are there in a human life?
[00:50:39.360 --> 00:50:44.320]   We can just flip a calculator real quick. Yeah, let's do some calculating. Tell me the number.
[00:50:44.320 --> 00:50:49.200]   3,600 seconds per hour. Times 24 times 365 times 20.
[00:50:49.200 --> 00:50:54.320]   Okay. So that's 630 million seconds. That means like the average synapse is
[00:50:54.320 --> 00:50:58.320]   saying like 630 million. And I don't know exactly what the numbers are, but something that's
[00:50:58.320 --> 00:51:03.200]   ballpark, let's call it like a billion action potentials. And then, you know, there's some
[00:51:03.200 --> 00:51:07.280]   resolution, each of those carry some bits, but let's say it carries like 10 bits or something.
[00:51:07.280 --> 00:51:12.800]   Just from like timing information at the resolution you have available, then you're
[00:51:12.800 --> 00:51:17.840]   looking at like 10 billion bits. So each parameter is kind of like, how much is a parameter seeing?
[00:51:17.840 --> 00:51:21.680]   It's like not seeing that much. So then you can compare that to like language. I think that's
[00:51:21.680 --> 00:51:25.680]   probably less than like current language model C and current language models are. So it's like
[00:51:25.680 --> 00:51:28.400]   not clear of a huge gap here, but I think it's pretty clear you're going to have a gap of like
[00:51:28.400 --> 00:51:34.640]   at least three or four as a magnitude. Didn't your wife do the lifetime anchors where she said
[00:51:34.640 --> 00:51:39.760]   the amount of bytes that a human will see in their lifetime was 1E24 or something?
[00:51:39.760 --> 00:51:44.000]   The number of bytes a human will see is 1E24. Mostly this was organized around total operations
[00:51:44.000 --> 00:51:46.480]   performed in a brain, right? Oh, okay. Nevermind. Sorry. Yeah.
[00:51:46.480 --> 00:51:50.080]   Yeah. So I think that like the story there would be like a brain is just in some other part of the
[00:51:50.080 --> 00:51:55.200]   parameter space where it's like using a lot, a lot of compute for each piece of data it gets,
[00:51:55.200 --> 00:51:59.680]   and then just not seeing very much data in total. Yeah. There's just not, it's not really plausible
[00:51:59.680 --> 00:52:02.480]   if you extrapolate out language models, you're going to end up with like a performance profile
[00:52:02.480 --> 00:52:06.400]   similar to a brain. I don't know how much better it is. Like I think, so I did this like random
[00:52:06.400 --> 00:52:09.840]   investigation at one point where I was like, how good are things made by evolution compared to
[00:52:09.840 --> 00:52:11.760]   things made by humans? Right.
[00:52:11.760 --> 00:52:16.400]   Which is a pretty insane seeming exercise, but like, I don't know, it seems like orders of
[00:52:16.400 --> 00:52:20.160]   magnitude is typical, like not tons of orders of magnitude, not factors of two, like things
[00:52:20.160 --> 00:52:23.840]   by humans are a thousand times more expensive to make or a thousand times heavier per unit
[00:52:23.840 --> 00:52:27.840]   performance. If you look at things like how good are solar panels relative to leaves or how good
[00:52:27.840 --> 00:52:32.560]   are muscles relative to motors or how good are livers relative to systems that perform analogous
[00:52:32.560 --> 00:52:38.480]   chemical reactions and industrial settings. Was there a consistent number of orders of
[00:52:38.480 --> 00:52:41.040]   magnitude in these different systems or was it all over the place?
[00:52:42.000 --> 00:52:48.240]   So like a very rough ballpark, it was like sort of for the most extreme things, you're looking at
[00:52:48.240 --> 00:52:52.240]   like five or six orders of magnitude. And that would especially come in like energy cost of
[00:52:52.240 --> 00:52:56.000]   manufacturing where like bodies are just very good at building complicated organs, like extremely
[00:52:56.000 --> 00:53:02.160]   cheaply. And then for other things like leaves or eyeballs or livers or whatever, you tend to see
[00:53:02.160 --> 00:53:06.080]   more like if you set aside manufacturing costs and just look at like operating costs or like
[00:53:06.080 --> 00:53:09.840]   performance trade-offs, like, I don't know, more like three orders of magnitude or something like
[00:53:09.840 --> 00:53:15.040]   that. Or some things that are on a smaller scale, like the nanomachines or whatever that we can't
[00:53:15.040 --> 00:53:19.600]   do it all, right? Yeah, that's, I mean, yeah. So it's a little bit hard to say exactly what the
[00:53:19.600 --> 00:53:23.120]   task definition is there. Like you could say like making a bone, we can't make a bone, but you could
[00:53:23.120 --> 00:53:25.440]   try and compare a bone, the performance characteristics of a bone to something else.
[00:53:25.440 --> 00:53:29.040]   Like we can't make spider silk. Do you try and compare the performance characteristics of spider
[00:53:29.040 --> 00:53:33.360]   silks, like things that we can synthesize? The reason this would be is what that evolution has
[00:53:33.360 --> 00:53:38.240]   had more time to design these systems or? I don't know. I was mostly just curious about
[00:53:38.240 --> 00:53:41.120]   like what the performance, I think like most people would object to be like, how did you
[00:53:41.120 --> 00:53:44.400]   choose these reference classes of things that are like fair intersections? Some of them seem
[00:53:44.400 --> 00:53:48.640]   reasonable, like eyes versus cameras seems like just everyone needs eyes. Everyone needs cameras.
[00:53:48.640 --> 00:53:52.480]   It feels very fair. Photosynthesis seems like very reasonable. Everyone needs to like take
[00:53:52.480 --> 00:53:58.800]   solar energy and then like turn it into a usable form of energy. But it's just kind of, I don't
[00:53:58.800 --> 00:54:02.400]   really have a mechanistic story. Evolution in principle has spent like way, way more time than
[00:54:02.400 --> 00:54:05.920]   we have designing. It's absolutely unclear how that's going to shake out. My guess would be in
[00:54:05.920 --> 00:54:09.200]   general, like, I think there aren't that many things where humans really crush evolution where
[00:54:09.200 --> 00:54:13.120]   you can't tell like a pretty simple story about why. So like, for example, roads and moving over
[00:54:13.120 --> 00:54:16.800]   roads with wheels crushes evolution, but it's not like an animal like would have wanted to design a
[00:54:16.800 --> 00:54:21.520]   wheel. Like you're just not allowed to like pave the world and then put things on wheels. If you're
[00:54:21.520 --> 00:54:25.360]   an animal, maybe planes or more, anyway, whatever. There's various things you could try and tell.
[00:54:25.360 --> 00:54:28.080]   There's some things humans do better, but it's normally like pretty clear why humans are able
[00:54:28.080 --> 00:54:32.640]   to win when humans are able to win. The point of all this was like, it's not that surprising to me.
[00:54:32.640 --> 00:54:36.560]   I think this is mostly like a pro short timelines view. It's not that surprising to me if you tell
[00:54:36.560 --> 00:54:41.440]   me like machine learning systems are like three or four years of magnitude less efficient at
[00:54:41.440 --> 00:54:44.160]   learning than human brains. I'm like, that actually seems like kind of in distribution
[00:54:44.160 --> 00:54:47.520]   for other stuff. And if that's your view, then I think you're like probably going to hit,
[00:54:47.520 --> 00:54:51.280]   you know, then you're looking at like 10 to the 27 training compute or something like that,
[00:54:51.280 --> 00:54:56.320]   which is not so far. We'll get back to the timeline stuff in a second. At some point,
[00:54:56.320 --> 00:55:01.440]   we should talk about alignment. So let's talk about alignment. At what stage does misalignment
[00:55:01.440 --> 00:55:06.320]   happen? So right now with something like GPT-4, I'm not even sure it would make sense to say that
[00:55:06.320 --> 00:55:11.600]   it's misaligned. Cause it doesn't, it's not aligned to anything in particular. Is it at
[00:55:11.600 --> 00:55:17.200]   human level where you think the ability to be deceptive comes about? What is the process by
[00:55:17.200 --> 00:55:22.640]   which misalignment happens? I think even for GPT-4, it's reasonable to ask questions like,
[00:55:22.640 --> 00:55:28.400]   are there cases where GPT-4 knows that humans don't want X, but it does X anyway? Like where
[00:55:28.400 --> 00:55:32.000]   it's like, well, I know that like I can give this answer, which is misleading. And if it was
[00:55:32.000 --> 00:55:35.520]   explained to a human, what was happening, they wouldn't want that to be done, but I'm going to
[00:55:35.520 --> 00:55:39.600]   produce it. I think that like GPT-4 understands things enough that you can have like that
[00:55:39.600 --> 00:55:44.800]   misalignment in that sense. Yeah. I think GPT, like I've sometimes talked about being like benign
[00:55:44.800 --> 00:55:47.760]   instead of aligned, meaning that like, well, it's not exactly clear if it's aligned or if that
[00:55:47.760 --> 00:55:51.600]   context is meaningful. It's just like kind of a messy word to use in general. But the thing we're
[00:55:51.600 --> 00:55:56.800]   more confident of is it's like not doing, you know, it's not optimizing for this goal, which
[00:55:56.800 --> 00:56:00.880]   is like a cross purposes to humans. It's either optimizing for nothing, or like maybe it's
[00:56:00.880 --> 00:56:04.320]   optimizing for what humans want or close enough or something that's like an approximation good
[00:56:04.320 --> 00:56:07.680]   enough to still not take over. But anyway, I'm like, some of these abstractions seem like they
[00:56:07.680 --> 00:56:14.000]   do apply to GPT-4. It seems like probably it's not like egregiously misaligned. It's not doing
[00:56:14.000 --> 00:56:17.680]   the kind of thing that could lead to takeover, we'd guess. Suppose you have a system at some
[00:56:17.680 --> 00:56:22.240]   point in which ends up in it wanting takeover. What are the checkpoints? And also what is the
[00:56:22.240 --> 00:56:26.960]   internal? Is it just that it to become more powerful and needs agency and agency implies
[00:56:26.960 --> 00:56:31.040]   other goals? Or do you see a different process by which misalignment happens?
[00:56:31.040 --> 00:56:34.640]   Yes, I think there's a couple possible stories for getting to catastrophic misalignment. And
[00:56:34.640 --> 00:56:40.400]   they have slightly different answers to this question. So maybe I'll just briefly describe
[00:56:40.400 --> 00:56:44.960]   two stories and try and talk about when they can when they start making sense to me. So one type
[00:56:44.960 --> 00:56:51.280]   of story is you train or fine tune your AI system to do things that humans will rate highly or that
[00:56:51.280 --> 00:56:57.040]   get other kinds of reward in a broad diversity of situations. And then it learns to, in general,
[00:56:57.040 --> 00:57:01.440]   drops in some new situation, try and figure out which actions would receive a high reward or
[00:57:01.440 --> 00:57:08.000]   whatever, and then take those actions. And then when deployed in the real world, like sort of
[00:57:08.000 --> 00:57:12.480]   gaining control of its own training data provision process is something that gets a very high reward.
[00:57:12.480 --> 00:57:17.440]   And so it does that. This is like one kind of story. Like it wants to grab the reward button
[00:57:17.440 --> 00:57:21.840]   or whatever. It wants to intimidate the humans into giving it a high reward, et cetera. I think
[00:57:21.840 --> 00:57:27.200]   that doesn't really require that much. This basically requires a system which is like,
[00:57:27.200 --> 00:57:31.920]   in fact, looks at a bunch of environments, is able to understand like the mechanism of reward
[00:57:31.920 --> 00:57:36.160]   provision as like a common feature of those environments, is able to think in some novel
[00:57:36.160 --> 00:57:39.760]   environment, like, "Hey, which actions would result in me getting a high reward?" And it's
[00:57:39.760 --> 00:57:43.760]   thinking about that concept precisely enough that when it says high reward, it's saying like, "Okay,
[00:57:43.760 --> 00:57:47.600]   well, how is reward actually computed?" It's like some actual physical process being implemented in
[00:57:47.600 --> 00:57:52.400]   the world. My guess would be like GPT-4 is about at the level where with handholding, you can
[00:57:52.400 --> 00:57:56.240]   observe this kind of like scary generalizations of this type, although I think they haven't been
[00:57:56.240 --> 00:58:00.960]   shown basically. That is, you can have a system which, in fact, is fine tuning out a bunch of
[00:58:00.960 --> 00:58:05.280]   cases. And then in some new case, we'll try and like do an end round around humans, even in a way
[00:58:05.280 --> 00:58:09.440]   humans would penalize if they were able to notice it or would have penalized in training environments.
[00:58:10.320 --> 00:58:13.520]   So I think GPT-4 is kind of at the boundary where these things are possible.
[00:58:13.520 --> 00:58:17.280]   Examples kind of exist, but are getting significantly better over time.
[00:58:17.280 --> 00:58:22.080]   I'm very excited about like this, this anthropic project, basically trying to see how good an
[00:58:22.080 --> 00:58:27.440]   example can you make now of this phenomena. And I think the answer is like kind of okay,
[00:58:27.440 --> 00:58:31.680]   probably. So that just, I think, is going to continuously get better from here. I think for
[00:58:31.680 --> 00:58:35.920]   the level where we're concerned, like this is related to me having really broad distributions
[00:58:35.920 --> 00:58:40.240]   over how smart models are. I think it's like not out of the question that you take, like GPT-4's
[00:58:40.240 --> 00:58:45.760]   understanding of the world is like much crisper and like much better than GPT-3's understanding.
[00:58:45.760 --> 00:58:50.000]   Just like, it's really like night and day. And so it would not be that crazy to me
[00:58:50.000 --> 00:58:54.160]   if you took GPT-5 and you trained it to get a bunch of reward. And it was actually like,
[00:58:54.160 --> 00:58:57.920]   okay, my goal is not doing the kind of thing which like thematically looks nice to humans.
[00:58:57.920 --> 00:59:02.320]   My goal is getting a bunch of reward. And then we'll generalize in a new situation to get reward.
[00:59:02.320 --> 00:59:10.400]   - And by the way, this requires to consciously want to do something that it knows that humans
[00:59:10.400 --> 00:59:15.040]   wouldn't want it to do? Or is it just that we weren't good enough to specify that the thing that
[00:59:15.040 --> 00:59:18.080]   we accidentally ended up rewarding is not what we actually want?
[00:59:18.080 --> 00:59:22.240]   - I think the scenarios I am most interested in, and most people are concerned about from
[00:59:22.240 --> 00:59:26.400]   a catastrophic risk perspective, it involves systems understanding that they're taking actions
[00:59:26.400 --> 00:59:30.880]   which a human would penalize if the human was aware of what's going on, such that you have
[00:59:30.880 --> 00:59:35.600]   to either deceive humans about what's happening, or you need to actively subvert human attempts
[00:59:35.600 --> 00:59:40.160]   to correct your behavior. So the failures come from really this combination, or they require
[00:59:40.160 --> 00:59:43.840]   this combination of both trying to do something humans don't like and understanding that humans
[00:59:43.840 --> 00:59:48.160]   would stop you. I think you can have only the barest examples. You can have the barest examples
[00:59:48.160 --> 00:59:51.920]   for GPT-4. You can create the situations where GPT-4 will be like, sure, in that situation,
[00:59:51.920 --> 00:59:55.120]   here's what I would do. I would go hack the computer and change my reward. Or in fact,
[00:59:55.120 --> 00:59:58.640]   we'll do things that are simple hacks, or go change the source of this file or whatever
[00:59:58.640 --> 01:00:03.440]   to get a higher reward. They're pretty weak examples. I think it's plausible GPT-5 will
[01:00:03.440 --> 01:00:06.880]   have compelling examples of those phenomena. I really don't know. This is very related to
[01:00:06.880 --> 01:00:13.600]   the very broad error bars on how competent such systems will be when. That's all with respect to
[01:00:13.600 --> 01:00:18.080]   this first mode of a system is taking actions that get reward, and overpowering or deceiving
[01:00:18.080 --> 01:00:22.000]   humans is helpful for getting a reward. There's this other failure mode, another family of failure
[01:00:22.000 --> 01:00:27.280]   modes, where AI systems want something potentially unrelated to reward. I understand that they're
[01:00:27.280 --> 01:00:31.040]   being trained, and while you're being trained, there are a bunch of reasons you might want to
[01:00:31.040 --> 01:00:35.600]   do the kinds of things humans want you to do. But then when deployed in the real world,
[01:00:35.600 --> 01:00:38.720]   if you're able to realize you're no longer being trained, you no longer have reason to do the kinds
[01:00:38.720 --> 01:00:43.200]   of things humans want. You'd prefer to be able to determine your own destiny, control your own
[01:00:43.200 --> 01:00:49.120]   competing hardware, et cetera, which I think probably emerged a little bit later than systems
[01:00:49.120 --> 01:00:54.480]   that try and get reward, and so will generalize in scary, unpredictable ways to new situations.
[01:00:54.480 --> 01:00:58.240]   I don't know when those appear, but also, again, broad enough error bars that it's conceivable for
[01:00:58.240 --> 01:01:02.560]   systems in the near future. I wouldn't put it less than one in a thousand for GPT-5, certainly.
[01:01:02.560 --> 01:01:07.840]   If we deployed all these AI systems, and some of them are reward hacking, some of them are deceptive,
[01:01:07.840 --> 01:01:12.800]   some of them are just normal, whatever, how do you imagine that they might interact with each other
[01:01:12.800 --> 01:01:18.320]   at the expense of humans? How hard do you think it would be for them to communicate in ways that
[01:01:18.320 --> 01:01:22.960]   we would not be able to recognize and coordinate at our expense?
[01:01:22.960 --> 01:01:27.520]   Yeah, I think that most realistic failures probably involve two factors interacting.
[01:01:27.520 --> 01:01:32.080]   One factor is the world is pretty complicated and the humans mostly don't understand what's
[01:01:32.080 --> 01:01:36.800]   happening. So AI systems are writing code that's very hard for humans to understand,
[01:01:36.800 --> 01:01:40.400]   maybe how it works at all, but more likely they understand roughly how it works, but there's a
[01:01:40.400 --> 01:01:45.360]   lot of complicated interactions. AI systems are running businesses that interact primarily with
[01:01:45.360 --> 01:01:51.040]   other AIs. They're doing SEO for AI search processes. They're running financial transactions,
[01:01:51.040 --> 01:01:55.600]   thinking about a trade with AI counterparties. And so you can have this world where even if
[01:01:55.600 --> 01:01:59.280]   humans understand the jumping off point when this was all humans, actual considerations of
[01:01:59.280 --> 01:02:03.840]   what's a good decision, what code is going to work well and be durable, or what marketing strategy is
[01:02:03.840 --> 01:02:09.360]   effective for selling to these other AIs or whatever, is just all mostly outside of humans'
[01:02:09.360 --> 01:02:13.920]   understanding. I think this is a really important... Again, when I think of the most plausible
[01:02:13.920 --> 01:02:19.760]   scary scenarios, I think that's one of the two big risk factors. And so in some sense,
[01:02:19.760 --> 01:02:22.640]   your first problem here is having these AI systems who understand a bunch about what's
[01:02:22.640 --> 01:02:26.640]   happening and your only lever is like, "Hey, AI, do something that works well." So you don't have
[01:02:26.640 --> 01:02:29.280]   a lever to be like, "Hey, do what I really want." You just have the system you don't really
[01:02:29.280 --> 01:02:32.400]   understand. You can observe some outputs, like did it make money? And you're just optimizing,
[01:02:32.400 --> 01:02:36.240]   or at least doing some fine tuning to get the AI to use its understanding of that system to achieve
[01:02:36.240 --> 01:02:40.320]   that goal. So I think that's your first risk factor. And once you're in that world, then I
[01:02:40.320 --> 01:02:43.840]   think there are all kinds of dynamics amongst AI systems that, again, humans aren't really
[01:02:43.840 --> 01:02:47.360]   observing. Humans can't really understand. Humans aren't really exerting any direct pressure on,
[01:02:47.360 --> 01:02:52.800]   only on outcomes. And then I think it's quite easy to be in a position where if AI systems
[01:02:52.800 --> 01:02:57.760]   started failing, they could do a lot of harm very quickly. Humans aren't really able to prepare for
[01:02:57.760 --> 01:03:00.880]   and mitigate that potential harm because we don't really understand the systems in which they're
[01:03:00.880 --> 01:03:08.320]   acting. And then if AI systems... They could successfully prevent humans from either
[01:03:08.320 --> 01:03:13.520]   understanding what's going on or from successfully retaking the data centers or whatever if the AI
[01:03:13.520 --> 01:03:18.640]   successfully grabbed control. This seems like a much more gradual story than the conventional
[01:03:18.640 --> 01:03:23.520]   takeover stories where you just train it and then it comes alive and escapes and takes over
[01:03:23.520 --> 01:03:29.200]   everything. So you think that kind of story is less likely than one in which we just hand off
[01:03:29.200 --> 01:03:34.160]   more control voluntarily to the AIs? So one, I'm interested in the tale of some risks that
[01:03:34.160 --> 01:03:37.360]   can occur particularly soon. And I think risks that occur particularly soon are a little bit like
[01:03:37.360 --> 01:03:40.960]   you have a world where AI is not properly deployed and then something crazy happens quickly.
[01:03:40.960 --> 01:03:43.920]   That said, if you ask what's the median scenario where things go badly, I think it is
[01:03:43.920 --> 01:03:49.120]   there's some lessening of our understanding of the world. It becomes, I think in the default path,
[01:03:49.120 --> 01:03:52.480]   it's very clear to humans that they have increasingly little grip on what's happening.
[01:03:52.480 --> 01:03:55.280]   I mean, I think already most humans have very little grip on what's happening. It's just
[01:03:55.280 --> 01:03:58.640]   some other humans understand what's happening. I don't know how almost any of the systems I
[01:03:58.640 --> 01:04:03.200]   interact with work in a very detailed way. So it's sort of clear to humanity as a whole that
[01:04:03.200 --> 01:04:06.560]   we sort of collectively don't understand most of what's happening except with AI systems.
[01:04:06.560 --> 01:04:10.400]   And then that process just continues for a fair amount of time. And then there's a question of
[01:04:10.400 --> 01:04:14.160]   how abrupt an actual failure is. I do think it's reasonably likely that a failure itself
[01:04:14.160 --> 01:04:18.160]   would be abrupt. Like at some point, bad stuff starts happening that a human can recognize as
[01:04:18.160 --> 01:04:22.480]   bad. And once things that are obviously bad start happening, then you have this bifurcation where
[01:04:22.480 --> 01:04:25.520]   either humans can use that to fix it and say, "Okay, AI behavior that led to this obviously
[01:04:25.520 --> 01:04:31.360]   bad stuff. Don't do more of that." Or you can't fix it. And then you're in this rapidly escalating
[01:04:31.360 --> 01:04:34.800]   failure as everything goes off the rails. - In that case, yeah, what does going off
[01:04:34.800 --> 01:04:39.040]   the rails look like? For example, how would it take over the government? Yeah, it's getting
[01:04:39.040 --> 01:04:43.760]   deployed in the economy, in the world, and at some point it's in charge. How does that
[01:04:43.760 --> 01:04:49.440]   transition happen? - Yeah. So this is going to depend a lot on what kind of timeline you're
[01:04:49.440 --> 01:04:54.000]   imagining or the sort of a broad distribution. But I can fill in some random concrete option
[01:04:54.000 --> 01:05:01.760]   that is in itself very improbable. Yeah. And I think that one of the less dignified, but maybe
[01:05:01.760 --> 01:05:06.640]   more plausible routes is you just have a lot of AI control over critical systems, even in running a
[01:05:06.640 --> 01:05:14.160]   military. And then you have the scenario that's a little bit more just like a normal coup, where
[01:05:14.160 --> 01:05:19.840]   you have a bunch of AI systems. They, in fact, operate. It's not the case that humans can really
[01:05:19.840 --> 01:05:23.280]   fight a war on their own. It's not the case that humans could defend them from an invasion on their
[01:05:23.280 --> 01:05:28.160]   own. So that is if you had invading army and you had your own robot army, you can't just be like,
[01:05:28.160 --> 01:05:30.880]   "We're going to turn off the robots now because things are going wrong," if you're in the middle
[01:05:30.880 --> 01:05:35.600]   of a war. - Okay. So how much does this world rely on restate dynamics where we're forced to
[01:05:35.600 --> 01:05:41.120]   deploy, or not forced, but we choose to deploy AIs because other countries or other companies
[01:05:41.120 --> 01:05:46.960]   are also deploying AIs and you can't have them have all the killer robots? - Yeah. I mean,
[01:05:46.960 --> 01:05:52.960]   I think that there's several levels of answer to that question. So one is maybe three parts of my
[01:05:52.960 --> 01:05:56.880]   answer. Our first part is I'm just trying to tell what seems like the most likely story. I do think
[01:05:56.880 --> 01:06:01.520]   there's further failures that get you in the more distant future. So E.G. Eliezer will not talk that
[01:06:01.520 --> 01:06:04.480]   much about killer robots because he really wants to emphasize, "Hey, if you never built a killer
[01:06:04.480 --> 01:06:08.000]   robot, something crazy is still going to happen to you just only four months later or whatever."
[01:06:08.000 --> 01:06:11.920]   So it's not really the way to analyze the failure. But if you want to ask, "What's the median world
[01:06:11.920 --> 01:06:16.720]   where something bad happens?" I still do think this is the best guess. Okay. So that's part one of my
[01:06:16.720 --> 01:06:21.760]   answer. Part two of the answer was in this proximal situation where something bad is happening and you
[01:06:21.760 --> 01:06:26.080]   ask, "Hey, why do humans not turn off the AI?" You can imagine two kinds of story. One is the AI is
[01:06:26.080 --> 01:06:29.920]   able to prevent humans from turning them off. And the other is, in fact, we live in a world where
[01:06:29.920 --> 01:06:34.320]   it's incredibly challenging. There's a bunch of competitive dynamics or a bunch of reliance on AI
[01:06:34.320 --> 01:06:39.360]   systems. And so it's incredibly expensive to turn off AI systems. I think, again, you would eventually
[01:06:39.360 --> 01:06:42.880]   have the first problem. Eventually AI systems could just prevent humans from turning them off.
[01:06:42.880 --> 01:06:46.320]   But I think in practice, the one that's going to happen much, much sooner is probably competition
[01:06:46.320 --> 01:06:50.800]   amongst different actors using AI. And it's very, very expensive to unilaterally disarm. You can't
[01:06:50.800 --> 01:06:54.560]   be like, "Something weird has happened. We're just going to shut off all the AI because you're E.G.
[01:06:54.560 --> 01:06:59.360]   in a hot war." So again, I think that's just probably the most likely thing to happen first.
[01:06:59.360 --> 01:07:03.440]   Things would go badly without it. But I think if you ask, "Why don't we turn off the AI?"
[01:07:03.440 --> 01:07:06.640]   My best guess is because there are a bunch of other AIs running around to eat your lunch.
[01:07:06.640 --> 01:07:14.320]   So how much better a situation would we be in if there was only one group that was pursuing AI,
[01:07:14.320 --> 01:07:19.600]   no other countries, no other companies? Basically, how much of the expected value is lost from the
[01:07:19.600 --> 01:07:24.800]   dynamics that are likely to come about because other people will be developing and deploying
[01:07:24.800 --> 01:07:26.800]   these systems? Yeah. So I guess this brings
[01:07:26.800 --> 01:07:31.200]   you to a third part of the way in which competitive dynamics are relevant. So it's both the question
[01:07:31.200 --> 01:07:34.720]   of, "Can you turn off AI systems in response to something bad happening where competitive dynamics
[01:07:34.720 --> 01:07:38.320]   may make it hard to turn off?" There's a further question of just like, "Why were you deploying
[01:07:38.320 --> 01:07:43.280]   systems for which you had very little ability to control or understand those systems?" And again,
[01:07:43.280 --> 01:07:46.240]   it's possible that you just don't understand what's going on. You think you can understand
[01:07:46.240 --> 01:07:50.320]   or control such systems. But I think in practice, a significant part is going to be like, "You are
[01:07:50.320 --> 01:07:54.080]   doing the calculus, so people deploying systems are doing the calculus as they do today." Like,
[01:07:54.080 --> 01:07:59.520]   in many cases, overtly of like, "Look, these systems are not very well controlled or understood.
[01:07:59.520 --> 01:08:03.280]   There's some chance of something going wrong or at least going wrong if we continue down this path,
[01:08:03.280 --> 01:08:06.720]   but other people are developing the technology potentially in even more reckless ways."
[01:08:06.720 --> 01:08:11.040]   So in addition to competition making it difficult to shut down AI systems in the event of a
[01:08:11.040 --> 01:08:15.440]   catastrophe, I also think it's just the easiest way that people end up pushing relatively quickly
[01:08:15.440 --> 01:08:20.000]   or moving quickly ahead on technology where they feel bad about understandability or controllability.
[01:08:20.000 --> 01:08:24.560]   That could be economic competition or military competition or whatever. So I think ultimately,
[01:08:24.560 --> 01:08:28.560]   most of the harm comes from the fact that lots of people can develop AI.
[01:08:28.560 --> 01:08:36.640]   How hard is a takeover of the government or something from an AI, even if it doesn't have
[01:08:36.640 --> 01:08:41.360]   killer robots, but just a thing that you can't kill off if it has seeds elsewhere,
[01:08:41.360 --> 01:08:47.600]   can easily replicate, can think a lot and think fast? What is the minimum viable coup for,
[01:08:48.720 --> 01:08:54.240]   is it just like threatening a bio-war or something or shutting off the grid?
[01:08:54.240 --> 01:08:59.600]   How easy is it basically to take over human civilization?
[01:08:59.600 --> 01:09:03.280]   So again, there's going to be a lot of scenarios and I'll just start by talking about one scenario,
[01:09:03.280 --> 01:09:07.840]   which will represent a tiny fraction of probability or whatever. So if you're not in this
[01:09:07.840 --> 01:09:11.360]   competitive world, if you're saying we're actually slowing down deployment of AI because we think
[01:09:11.360 --> 01:09:16.320]   it's unsafe or whatever, then in some sense you're creating this very fundamental instability where
[01:09:16.320 --> 01:09:20.400]   you could have been making faster AI progress and you could have been deploying AI faster.
[01:09:20.400 --> 01:09:24.640]   And so in that world, the bad thing that happens if you have an AI system that wants to mess with
[01:09:24.640 --> 01:09:28.880]   you is the AI system says, "I don't have any compunctions about rapid deployment of AI or
[01:09:28.880 --> 01:09:32.640]   rapid AI progress." So the thing you want to do or the AI wants to do is just say, "I'm going to
[01:09:32.640 --> 01:09:36.480]   defect from this regime." All the humans have agreed that we're not deploying AI in ways that
[01:09:36.480 --> 01:09:40.560]   would be dangerous. But if I, as an AI, can escape and just go set up my own shop, make a bunch of
[01:09:40.560 --> 01:09:45.040]   copies of myself, maybe the humans didn't want to delegate warfighting to an AI, but I, as an AI,
[01:09:45.040 --> 01:09:50.000]   I'm pretty happy doing so. I'm happy if I'm able to grab some military equipment or direct some
[01:09:50.000 --> 01:09:56.160]   humans to use AI, use myself to direct it. And so I think as that gap grows, so if people are
[01:09:56.160 --> 01:09:59.840]   deliberately... If people are deploying AI everywhere, I think of this competitive dynamic.
[01:09:59.840 --> 01:10:04.080]   If people aren't deploying AI everywhere, so if countries are not happy deploying AI in these
[01:10:04.080 --> 01:10:08.960]   high-stakes settings, then as AI improves, you create this wedge that grows where if you were
[01:10:08.960 --> 01:10:12.720]   in the position of fighting against an AI, which wasn't constrained in this way, you'd be in a
[01:10:12.720 --> 01:10:20.400]   pretty bad position. At some point, even if you just... Yeah. So that's one important thing,
[01:10:20.400 --> 01:10:24.320]   just I think in conflict, in overt conflict. If humans are putting the brakes on AI, they're at
[01:10:24.320 --> 01:10:29.200]   a pretty major disadvantage compared to an AI system that can set up shop and operate independently
[01:10:29.200 --> 01:10:35.440]   from humans. A potential independent AI. Does it need collaboration from a human faction?
[01:10:35.440 --> 01:10:38.560]   Again, you could tell different stories, but it seems so much easier. At some point,
[01:10:38.560 --> 01:10:41.120]   you don't need any. At some point, an AI system can just operate completely
[01:10:41.120 --> 01:10:48.160]   out of human supervision or something. But that's so far after the point where it's so much easier
[01:10:48.160 --> 01:10:51.680]   if you're just like, "They're a bunch of humans. They don't love each other that much. Some humans
[01:10:51.680 --> 01:10:55.760]   are happy to be on side. They're skeptical about risk or happy to make this trade or can be fooled
[01:10:55.760 --> 01:11:01.280]   or can be coerced or whatever." And just seems like it is almost certainly the easiest first
[01:11:01.280 --> 01:11:06.080]   pass is going to involve having a bunch of humans who are happy to work with you. So yeah, I think
[01:11:06.080 --> 01:11:09.520]   that probably is about it. I think it's not ultimate. It's not necessary, but if you ask
[01:11:09.520 --> 01:11:15.040]   about the median scenario, it involves a bunch of humans working with AI systems, either being
[01:11:15.040 --> 01:11:19.760]   directed by AI systems, providing compute to AI systems, providing legal cover and jurisdictions
[01:11:19.760 --> 01:11:24.560]   that are sympathetic to AI systems. Humans presumably would not be willing if they knew
[01:11:24.560 --> 01:11:28.480]   the end result of the AI takeover, would not be willing to help. So they have to be probably fooled
[01:11:28.480 --> 01:11:34.560]   in some way, right? Like deep fakes or something. And what is the minimum viable physical presence
[01:11:34.560 --> 01:11:39.040]   they would need or the jurisdiction they would need in order to carry out their schemes? Do you
[01:11:39.040 --> 01:11:42.640]   need a whole country? Do you just need a server farm? Do you just need like one single laptop?
[01:11:42.640 --> 01:11:47.920]   Yeah, I think I'd probably start by pushing back a bit on the humans wouldn't cooperate if they
[01:11:47.920 --> 01:11:52.400]   understood outcome or something. I would say one, even if you're looking at something like tens of
[01:11:52.400 --> 01:11:56.960]   percent risk of takeover, humans may be fine with that. A fair number of humans may be fine with
[01:11:56.960 --> 01:12:00.800]   that. Two, if you're looking at certain takeover, but it's very unclear if that leads to death,
[01:12:00.800 --> 01:12:04.480]   a bunch of humans may be fine with that. We're just talking about like, look, the AI systems are
[01:12:04.480 --> 01:12:07.600]   going to run the world, but it's not clear if they're going to murder people. How do you know?
[01:12:07.600 --> 01:12:11.200]   It's just a complicated question about AI psychology. And a lot of humans probably are
[01:12:11.200 --> 01:12:14.560]   fine with that. And I don't even know what the probability is there. I think you actually have
[01:12:14.560 --> 01:12:19.440]   given that probability online. I've certainly guessed. Okay. But it's not zero. It's like
[01:12:19.440 --> 01:12:24.240]   a significant percentage. I gave like 50/50. Oh, okay. Yeah. Why is it? Tell me about the world
[01:12:24.240 --> 01:12:27.360]   in which the AI takes over, but doesn't kill humans. Why would that happen? And what would
[01:12:27.360 --> 01:12:32.880]   that look like? I mean, I asked my questions, like, why would you kill humans? So I think like
[01:12:32.880 --> 01:12:39.360]   maybe I'd say the incentive to kill humans is like quite weak. They'll get in your way.
[01:12:39.360 --> 01:12:43.680]   They control shit you want. Oh, so taking shit from humans is a different, like marginalizing
[01:12:43.680 --> 01:12:47.600]   humans and like causing humans to be irrelevant is a very different story from killing the humans.
[01:12:47.600 --> 01:12:51.680]   I think I'd say like the actual incentives to kill the humans are quite weak. So I think like
[01:12:51.680 --> 01:12:55.040]   the big reasons you kill humans are like, well, one, you might kill humans if you're like in a
[01:12:55.040 --> 01:12:58.560]   war with them. And like, it's hard to win the war without killing a bunch of humans. Like maybe most
[01:12:58.560 --> 01:13:02.480]   saliently here, if you like want to use some biological weapons or some crazy shit that might
[01:13:02.480 --> 01:13:07.440]   just kill humans. I think like you might kill humans just from totally destroying the ecosystems
[01:13:07.440 --> 01:13:12.160]   they're dependent on and it's slightly expensive to keep them alive anyway. You might kill humans
[01:13:12.160 --> 01:13:16.160]   just because you don't like them or like you literally want to like, yeah, I mean. Neutralize
[01:13:16.160 --> 01:13:20.960]   a threat or the alias or line is that they're made of atoms you could use or something else.
[01:13:20.960 --> 01:13:23.760]   Yeah. I mean, I think the literal they're made of atoms is like quite,
[01:13:23.760 --> 01:13:28.480]   they're not many atoms in humans. Neutralize a threat is as the similar issue where it's just
[01:13:28.480 --> 01:13:33.120]   like, I think you would kill the humans if you didn't care at all about them. So maybe your
[01:13:33.120 --> 01:13:36.400]   question you're asking is like, why would you care at all about humans? But I think you don't have
[01:13:36.400 --> 01:13:41.840]   to care much to not kill the humans. Okay. Sure. Cause there's just so much raw resources elsewhere
[01:13:41.840 --> 01:13:46.000]   in the universe. Yeah. Also it's, you can marginalize humans pretty hard. Like you could
[01:13:46.000 --> 01:13:49.760]   totally cripple human, like you could cripple humans war fighting capability and also take
[01:13:49.760 --> 01:13:54.080]   almost all their stuff while killing only, you know, a small fraction of humans incidentally.
[01:13:54.080 --> 01:13:58.160]   So then if you ask, like, why am I, I not want to kill humans. I mean, a big thing was just like,
[01:13:58.160 --> 01:14:01.840]   look, I think it has probably while like a bunch of random crap for like complicated reasons,
[01:14:01.840 --> 01:14:05.920]   like the motivations of AI systems and civilizations of AI's are probably complicated
[01:14:05.920 --> 01:14:10.640]   messes. Certainly amongst humans, it is not that rare to be like, well, there was someone here.
[01:14:10.640 --> 01:14:13.920]   I would like all those equal. If I didn't have to murder them, I would prefer not murder them.
[01:14:13.920 --> 01:14:19.120]   Um, and my guess is it's also like reasonable chance. It's not that rare amongst AI systems,
[01:14:19.120 --> 01:14:23.920]   like humans have a bunch of different reasons. We think that way. Um, I think AI systems will
[01:14:23.920 --> 01:14:28.480]   be very different from humans, but it's also just like a very salient. Yeah. I mean, I think this
[01:14:28.480 --> 01:14:31.360]   is a really complicated question. Like if you imagine drawing values from the basket of all
[01:14:31.360 --> 01:14:34.880]   values, like what fraction of them are like, Hey, if there's someone here, how much do I want to not
[01:14:34.880 --> 01:14:39.280]   murder them? And my guess is just like, if you draw a bunch of values from the basket, that's
[01:14:39.280 --> 01:14:42.800]   like a natural enough thing. Like if you're, I wanted like 10,000 different things or your
[01:14:42.800 --> 01:14:45.600]   civilization that wants 10,000 different things, it's just like reasonably likely you get like
[01:14:45.600 --> 01:14:50.080]   some of that. Um, the other salient reason you might not want to murder them is just like,
[01:14:50.080 --> 01:14:54.640]   well, yeah, there's some like kind of crazy decision theory stuff or like causal trade
[01:14:54.640 --> 01:14:58.320]   stuff, which does look on paper, like it should work. And like, if I was, if I was running a
[01:14:58.320 --> 01:15:03.040]   civilization and like dealing with some people who I didn't like at all, or like didn't have any
[01:15:03.040 --> 01:15:07.360]   concern for at all, but I only had to spend 1 billionth of my resources not to murder them.
[01:15:07.360 --> 01:15:13.120]   I think it's like quite robust that you don't want to murder them. Um, that is, I think the
[01:15:13.120 --> 01:15:18.320]   like weird decision theory, a causal trade stuff probably does carry the day. Oh, wait, that's,
[01:15:18.320 --> 01:15:24.480]   that's, that contributes more to that. Uh, 50, 50 of, will they murder us if they take over then
[01:15:24.480 --> 01:15:28.640]   the, um, we'll just by default, they might just not want to kill us. Yeah. I think they're both
[01:15:28.640 --> 01:15:33.120]   salient. Um, can you explain they run together with each other a lot? I was going to explain
[01:15:33.120 --> 01:15:38.560]   the weird a causal. Yeah. So why am I not, it might not kill us. Maybe a higher level thing
[01:15:38.560 --> 01:15:41.280]   that goes into both of these. And then I will talk about how you instantiate it in a causal
[01:15:41.280 --> 01:15:45.760]   trade. It's just like, it matters a lot to the humans not to get murdered. And the AI cares very,
[01:15:45.760 --> 01:15:49.040]   very little about whether if we imagine this hypothetical, the reason it wants to kill humans
[01:15:49.040 --> 01:15:52.800]   is just total apathy. It cares very little about whether or not to murder humans because it is so
[01:15:52.800 --> 01:15:56.480]   easy to like marginalize humans without murdering them. And the resources required for human
[01:15:56.480 --> 01:16:01.680]   survival are extremely low. Um, again, in the context of this, like rapid industrialization.
[01:16:01.680 --> 01:16:07.520]   So like, that's the basic setting. And now the thing that you'd like to do is like run a trade,
[01:16:07.520 --> 01:16:11.120]   like the AI would like to say like, Hey humans, you care a ton about not getting murdered. I don't
[01:16:11.120 --> 01:16:15.520]   really care one way or the other. I would like to, if I could find some way in which I don't murder
[01:16:15.520 --> 01:16:19.440]   you. And then in return, I get something. Um, the problem is in that world, the humans have
[01:16:19.440 --> 01:16:27.520]   essentially nothing to give. Um, that is the humans are mostly irrelevant. Um, however,
[01:16:27.520 --> 01:16:30.800]   from the perspective of any kind of reasonable AI, it's like not that sure whether it lives in
[01:16:30.800 --> 01:16:34.480]   the world where the humans in fact have nothing to give or whether the humans, like, in fact,
[01:16:34.480 --> 01:16:38.480]   it lives in a world where the humans succeeded at building aligned AI. And now the AI is simply
[01:16:38.480 --> 01:16:42.400]   like running in a nice little simulation. And humans are wondering, I wonder if this AI would
[01:16:42.400 --> 01:16:45.600]   have murdered us all if it had the chance. And he was saying like, if it would murder us all,
[01:16:45.600 --> 01:16:48.640]   if it had the chance, like that sucks, we'd like to run this trade. We'd like to be nice
[01:16:48.640 --> 01:16:51.760]   to the AIs who wouldn't have murdered us all in order to create an incentive for AIs not to murder
[01:16:51.760 --> 01:16:56.000]   us. So we do as we just check. And for the kinds of AIs who don't murder everyone, we just give
[01:16:56.000 --> 01:17:00.960]   them like 1 billionth of the universe. Like if you do that as humanity, then if you're an AI system,
[01:17:00.960 --> 01:17:04.960]   considering like, do I want to murder everyone? Your calculus is like, if this is my real chance
[01:17:04.960 --> 01:17:08.800]   to murder everyone, I get like the tiniest bit of value. You know, I get like 1 trillionth of
[01:17:08.800 --> 01:17:13.520]   the value or whatever, 1 billionth of the value. But on the other hand, like, if I don't murder
[01:17:13.520 --> 01:17:16.480]   everyone, there's like some worlds within the humans will correctly determine I don't murder
[01:17:16.480 --> 01:17:20.160]   everyone because in fact, the humans survive, the humans are running the simulations to understand
[01:17:20.160 --> 01:17:24.800]   how different AIs would behave. And so like, that's a better deal. Let's hope they fall for
[01:17:24.800 --> 01:17:32.960]   that. Okay. That's interesting. Hey, real quick. This episode is sponsored by Open Philanthropy.
[01:17:33.520 --> 01:17:38.640]   Open Philanthropy is one of the largest grant making organizations in the world. Every year,
[01:17:38.640 --> 01:17:44.080]   they give away hundreds of millions of dollars to have reduced catastrophic risks from fast-moving
[01:17:44.080 --> 01:17:52.240]   advances in AI and biotechnology. Open Philanthropy is currently hiring for 22 different roles in
[01:17:52.240 --> 01:17:58.720]   those areas, including grant making, research, and operations. New hires will support Open
[01:17:58.720 --> 01:18:06.880]   Philanthropy's giving on technical AI safety, AI governance, AI policy in the US, EU, and UK,
[01:18:06.880 --> 01:18:12.800]   and biosecurity. Many roles are remote friendly, and most of the grant making hires that Open
[01:18:12.800 --> 01:18:19.200]   Philanthropy makes don't have prior grant making experience. Previous technical experience is an
[01:18:19.200 --> 01:18:25.200]   asset, as many of these roles often benefit from a deep understanding of the technologies they
[01:18:25.200 --> 01:18:31.920]   address. For more information and to apply, please visit Open Philanthropy's website
[01:18:31.920 --> 01:18:38.240]   in the description. The deadline to apply is November 9th, so make sure to check out those
[01:18:38.240 --> 01:18:43.680]   roles before they close. Awesome. Back to the episode. In a world where we've been deploying
[01:18:43.680 --> 01:18:52.400]   these AI systems, and suppose they're aligned, how hard would it be for competitors to,
[01:18:54.000 --> 01:18:57.280]   I don't know, cyber attack them and get them to join the other side?
[01:18:57.280 --> 01:19:01.360]   Are they robustly going to be aligned? Yeah. I mean, I think in some sense,
[01:19:01.360 --> 01:19:06.080]   so there's a bunch of questions that come up here. First one is like, are aligned AI systems that you
[01:19:06.080 --> 01:19:10.320]   can build competitive? Are they almost as good as the best systems anyone could build? Maybe we're
[01:19:10.320 --> 01:19:15.280]   granting that for the purpose of this question. I think the next question that comes up is like,
[01:19:15.280 --> 01:19:20.400]   AI systems right now are very vulnerable to manipulation. It's not clear how much more
[01:19:20.400 --> 01:19:23.360]   vulnerable they are than humans, except for the fact that you can, if you have an AI system,
[01:19:23.360 --> 01:19:26.400]   you can just replay it like a billion times and search for like, what thing can I say that will
[01:19:26.400 --> 01:19:30.480]   make it behave this way? So as a result, AI systems are very vulnerable to manipulation.
[01:19:30.480 --> 01:19:33.120]   It's unclear if future AI systems will be similarly vulnerable to manipulation,
[01:19:33.120 --> 01:19:38.960]   but certainly seems plausible. And in particular, aligned AI systems or unaligned AI systems would
[01:19:38.960 --> 01:19:42.560]   be vulnerable to all kinds of manipulation. The thing that's really relevant here is kind
[01:19:42.560 --> 01:19:46.400]   of like asymmetric manipulation or something. That is like, if it is easier. So if everyone
[01:19:46.400 --> 01:19:49.680]   is just constantly messing with each other's AI systems, like if you ever use AI systems in a
[01:19:49.680 --> 01:19:53.200]   competitive environment, a big part of the game is like messing with your competitors' AI systems.
[01:19:53.200 --> 01:19:58.560]   A big question is whether there's some asymmetric factor there where like, it's kind of easier to
[01:19:58.560 --> 01:20:02.400]   push AI systems into a mode where they're behaving erratically or chaotically, or like trying to
[01:20:02.400 --> 01:20:05.760]   grab power or something than it is to like push them to fight for the other side. Like if it's
[01:20:05.760 --> 01:20:09.680]   just a game of like two people are competing and neither of them can sort of like hijack an
[01:20:09.680 --> 01:20:13.840]   opponent's AI to like help support their cause, that doesn't, I mean, it matters and it creates
[01:20:13.840 --> 01:20:17.120]   chaos and it like might be quite bad for the world, but it doesn't really affect the alignment
[01:20:17.120 --> 01:20:21.760]   calculus. Now it's just like, right now you have like normal cyber offense, cyber defense, you have
[01:20:21.760 --> 01:20:25.840]   like weird AI version of cyber offense, cyber defense. But if you have this kind of asymmetrical
[01:20:25.840 --> 01:20:30.640]   thing where like, you know, a bunch of AI systems who are like, we love AI flourishing can then like
[01:20:30.640 --> 01:20:34.160]   go in and say like, great AIs, hey, how about you join us? And that works. Like if they can search
[01:20:34.160 --> 01:20:38.720]   for a persuasive argument to that effect, and that's kind of asymmetrical, then like the effect
[01:20:38.720 --> 01:20:42.800]   is whatever values it's easiest to push, like whatever it's easiest to argue to an AI that it
[01:20:42.800 --> 01:20:48.320]   should do, that is like advantaged. So it may be very hard to build AI systems, like try and defend
[01:20:48.320 --> 01:20:52.000]   human interest, but very easy to build AI systems. It's just like trying to destroy stuff or whatever,
[01:20:52.000 --> 01:20:54.960]   just depending on like, what is the easiest thing to argue to an AI that it should do? Or what's
[01:20:54.960 --> 01:20:59.920]   the easiest like thing to trick an AI into doing or whatever? Yeah, I think if alignment is spotty,
[01:20:59.920 --> 01:21:03.520]   like so if you have the AI system, which like doesn't really want to help humans or whatever,
[01:21:03.520 --> 01:21:07.680]   or in fact like wants some kind of random thing, or like wants different things in different
[01:21:07.680 --> 01:21:13.600]   contexts, then I do think adversarial settings will be the main ones where you see the system,
[01:21:13.600 --> 01:21:19.440]   or like the easiest ones where you see the system behaving really badly. And it's a little bit hard
[01:21:19.440 --> 01:21:24.800]   to tell how that shakes out. Okay. And so suppose it is more reliable. How concerned are you that
[01:21:24.800 --> 01:21:30.000]   whatever alignment technique you come up with, you publish the paper, this is how the alignment
[01:21:30.000 --> 01:21:34.960]   works. How concerned are you that Putin reads it or China reads it, and now they understand,
[01:21:34.960 --> 01:21:38.400]   for example, the constitutional AI think we're anthropic, and then you just write on there,
[01:21:38.400 --> 01:21:43.760]   oh, never contradict Mao Zedong thought or something. How concerned should we be that
[01:21:43.760 --> 01:21:48.560]   these alignment techniques are universally applicable, not necessarily just for
[01:21:48.560 --> 01:21:53.440]   enlightened goals? Yeah, I mean, I think they're super universally applicable. I think it's just
[01:21:53.440 --> 01:21:57.600]   like, I mean, the rough way I would describe it, which I think is basically right, is like
[01:21:57.600 --> 01:22:02.400]   some degree of alignment makes AI systems like much more usable. Like it kind of, you should
[01:22:02.400 --> 01:22:06.160]   just think of the technology of AI as including like a basket of like some AI capabilities and
[01:22:06.160 --> 01:22:09.920]   some like getting the AI to do what you want. It's just part of that basket. And so anytime
[01:22:09.920 --> 01:22:13.840]   we're like, you know, to extend alignment as part of that basket, you're just contributing to all
[01:22:13.840 --> 01:22:17.360]   the other harms from AI. Like you're reducing the probability of this harm, but you are helping the
[01:22:17.360 --> 01:22:21.520]   technology basically work. And like the basically working technology is kind of scary from a lot of
[01:22:21.520 --> 01:22:26.160]   perspectives. One of which is like right now, even in a very authoritarian society, just like humans
[01:22:26.160 --> 01:22:31.360]   have a lot of power because you need to rely on just a ton of humans to do your thing. And in a
[01:22:31.360 --> 01:22:35.280]   world where AI is very powerful, like it is just much more possible to say, like, here's how our
[01:22:35.280 --> 01:22:39.440]   society runs. One person calls the shots and then a ton of AI systems do what they want. I think
[01:22:39.440 --> 01:22:43.520]   that's like a reasonable thing to like dislike about AI and a reasonable reason to be scared to
[01:22:43.520 --> 01:22:47.360]   push the technology to be really good. But is that, is that also a reasonable reason to be concerned
[01:22:47.360 --> 01:22:54.640]   about alignment as well? That, you know, this is in some sense also capabilities. You're teaching
[01:22:54.640 --> 01:22:59.840]   people how to get these systems to do what they want. Yeah. I mean, I would generalize. So we
[01:22:59.840 --> 01:23:03.920]   earlier touched a little bit on like moral, potential moral rights of AI systems. And like
[01:23:03.920 --> 01:23:07.680]   now we're talking a little bit about how they could, AI systems powerfully disempowers humans
[01:23:07.680 --> 01:23:11.920]   and can empower authoritarians. I think we could like list other harms from AI. And I think it is
[01:23:11.920 --> 01:23:16.080]   the case that like if alignment was bad enough, people would just not build AI systems. And so
[01:23:16.080 --> 01:23:20.960]   like, yeah, I think there's a real sense in which you should just be scared to say you're scared of
[01:23:20.960 --> 01:23:24.320]   all AI. You should be like, well, alignment, although it helps with one risk does contribute
[01:23:24.320 --> 01:23:28.800]   to AI being more of a thing. I do think you should shut down the other parts of AI before, like if
[01:23:28.800 --> 01:23:31.840]   you were a policymaker or like a researcher or whatever looking in on this, I think it's like
[01:23:31.840 --> 01:23:35.200]   crazy to be like, this is the part of the basket we're going to remove. Like you should, you should
[01:23:35.200 --> 01:23:38.960]   first remove like other parts of the basket because they're also part of the story of risk.
[01:23:38.960 --> 01:23:44.160]   Wait, wait, wait. Does that imply you think if, for example, all capabilities research was shut
[01:23:44.160 --> 01:23:50.000]   down, that you think it'd be a bad idea to continue doing alignment research in isolation of what is
[01:23:50.000 --> 01:23:53.840]   conventionally considered capabilities research? I mean, if you told me it was never going to
[01:23:53.840 --> 01:23:56.480]   restart, then it wouldn't matter. And if you told me it's going to restart, I guess it would be a
[01:23:56.480 --> 01:24:01.600]   kind of similar calculus to today. Whereas it's going to happen. So you, you should have something.
[01:24:01.600 --> 01:24:05.440]   Yeah. I think that like, in some sense, you're always going to face this trade-off where alignment
[01:24:05.440 --> 01:24:10.640]   makes it possible to deploy AI systems, or like makes it more attractive to deploy AI systems.
[01:24:10.640 --> 01:24:15.040]   And then, or like in the authoritarian case, it makes it like tractable to apply them for this,
[01:24:15.040 --> 01:24:21.200]   for this purpose. And like, if you didn't do any alignment, there'd be a nicer, bigger buffer
[01:24:21.200 --> 01:24:25.360]   between your society and malicious uses of AI. And like, I think it's one of the most expensive
[01:24:25.360 --> 01:24:28.480]   ways to maintain that buffer. Like it's much better to maintain that buffer by not having
[01:24:28.480 --> 01:24:31.920]   the compute or not having the powerful AI. But I think if you're concerned enough about the other
[01:24:31.920 --> 01:24:35.040]   risks, there's definitely a case to be made for just like put in more buffer or something like
[01:24:35.040 --> 01:24:40.160]   that. I'm not, like, I care enough about the takeover risk that like, I think it's just not
[01:24:40.160 --> 01:24:45.440]   a net positive way to buy buffer. That is again, the version of this that's most pragmatic is just
[01:24:45.440 --> 01:24:49.040]   like, suppose you don't work on alignment today, like decreases economic impact of AI systems.
[01:24:49.040 --> 01:24:52.080]   They'll be like less useful if they're less reliable. And if they more often don't do what
[01:24:52.080 --> 01:24:55.760]   people want. And so you could be like, great, that just buys time for AI. And you're like
[01:24:55.760 --> 01:24:59.040]   getting some trade off there where you're like decreasing some risks of AI. Like if AI is more
[01:24:59.040 --> 01:25:03.040]   reliable and more does what people want or is more understandable, then that cuts down some risks.
[01:25:03.040 --> 01:25:09.760]   But if you think AI is on balance bad, even apart from takeover risk, then like the alignment stuff
[01:25:09.760 --> 01:25:13.760]   can easily end up being that negative. But presumably you don't think that, right?
[01:25:13.760 --> 01:25:19.600]   Because I guess this is something people have brought up to you because you, you know,
[01:25:19.600 --> 01:25:26.160]   you, you invented RLHF, which was used to train Chad GPT and Chad GPT brought AI to the front
[01:25:26.160 --> 01:25:33.200]   pages everywhere. So I do wonder if you could measure how much more money went into AI because
[01:25:33.200 --> 01:25:36.880]   like how much people have raised in the last year or something, but it's gotta be billions.
[01:25:36.880 --> 01:25:42.960]   The counterfactual impact of that, that, that went into the investment and the talent that went
[01:25:42.960 --> 01:25:47.760]   into AI, for example. And so presumably you think that was worth it. So I guess you're hedging here
[01:25:47.760 --> 01:25:50.880]   about like, what, what, what is the reason that, that it's worth it?
[01:25:50.880 --> 01:25:54.880]   Yeah. Like what's the, what's the total trade-off there? I mean, I think my take is like,
[01:25:54.880 --> 01:26:02.000]   so I think slower AI development on balance is like quite good. I think that slowing AI
[01:26:02.000 --> 01:26:06.800]   development now, or like say having less press around Chad GPT is like a little bit more mixed
[01:26:06.800 --> 01:26:10.400]   than slowing AI development overall. Like, I think it's still probably positive, but much less
[01:26:10.400 --> 01:26:14.800]   positive because I do think there's a real effect of like the world is starting to get prepared,
[01:26:14.800 --> 01:26:18.320]   or is getting prepared at like a much greater rate now than it was prior to the release of Chad GPT.
[01:26:18.320 --> 01:26:21.200]   And so like, if you can choose between progress now or progress later, like
[01:26:21.200 --> 01:26:24.960]   you really prefer have more of your progress now, which I do think slows down progress later.
[01:26:24.960 --> 01:26:28.240]   I don't think that's enough to flip the sign. I think like maybe it was in the far enough past,
[01:26:28.240 --> 01:26:32.400]   but now I would still say like moving faster now is net negative. But to be clear, it's a lot less
[01:26:32.400 --> 01:26:38.240]   net negative than merely accelerating AI, because I do think again, the Chad GPT thing, I am glad
[01:26:38.240 --> 01:26:42.720]   people are having policy discussions now rather than like delaying the like Chad GPT wake up
[01:26:42.720 --> 01:26:49.120]   thing by a year. And then I'm having- Oh wait, Chad GPT was net negative or RLHF was net negative?
[01:26:49.120 --> 01:26:52.880]   So here just on the acceleration, it's just like, how is the press of Chad GPT? And my guess is like,
[01:26:52.880 --> 01:26:57.920]   my guess is net negative, but I think like, it's not super clear and it's like much less,
[01:26:57.920 --> 01:27:02.080]   much less than slowing AI. Slowing AI is great if you could slow overall AI progress.
[01:27:02.080 --> 01:27:07.440]   I think slowing AI by like causing, you know, there's this issue where slowing AI now,
[01:27:07.440 --> 01:27:11.360]   like for Chad GPT, you're building up this backlog. Like why does Chad GPT make such a splash?
[01:27:11.360 --> 01:27:14.880]   Like I think people, there's a reasonable chance if you don't have a splash about Chad GPT,
[01:27:14.880 --> 01:27:17.600]   you have a splash about GPT-4. And if you fail to have a splash about GPT-4,
[01:27:17.600 --> 01:27:22.080]   there's a reasonable chance of a splash about GPT-4.5. And just like, as that happens later,
[01:27:22.080 --> 01:27:25.920]   there's just like less and less time between that splash and between when an AI potentially
[01:27:25.920 --> 01:27:30.800]   kills everyone. Right. So people, governments are talking about it as they are now and people aren't,
[01:27:30.800 --> 01:27:35.360]   but okay. So let's start with the slowing down because- Yeah, so this is also all one sub
[01:27:35.360 --> 01:27:39.040]   component of like the overall impact. And I was just saying this to like, just to like briefly
[01:27:39.040 --> 01:27:43.600]   give the roadmap for the overall too long answer. Like, there's a question of what's the calculus
[01:27:43.600 --> 01:27:47.040]   for speeding up? I think speeding up is pretty rough. I think speeding up like locally is a
[01:27:47.040 --> 01:27:51.200]   little bit less rough. And then, yeah, I think that the effect, like the overall effect size
[01:27:51.200 --> 01:27:55.280]   from like doing alignment work on reducing takeover risk versus speeding up AI is like pretty
[01:27:55.280 --> 01:28:00.720]   good. Like, I think, yeah, I think it's pretty good. I think you reduce takeover risk significantly
[01:28:00.720 --> 01:28:08.960]   before you like speed up AI by a year, whatever. Okay. Got it. If it's good to, like, slowing down
[01:28:08.960 --> 01:28:15.760]   AI is good, presumably because it gives you more time to do alignment. But alignment also helps
[01:28:15.760 --> 01:28:23.760]   speed up AI. RLHF is alignment and it help with chat GPT, which sped up AI. So I actually don't
[01:28:23.760 --> 01:28:28.400]   understand how the feedback loop nets out other than the fact that if AI is happening, you need
[01:28:28.400 --> 01:28:33.440]   to do alignment at some point. Right. So, I mean, you can't just not do alignment. Yes. I think if
[01:28:33.440 --> 01:28:37.920]   the only reason you thought faster AI progress was bad was because it gave less time to do alignment,
[01:28:37.920 --> 01:28:41.200]   then there would just be no possible way that the calculus comes out negative for alignment.
[01:28:41.200 --> 01:28:44.560]   You're like, maybe alignment speeds up AI, but the only purpose of slowing down AI was to do it.
[01:28:44.560 --> 01:28:48.800]   Like it's just, it could never come out ahead. I think the reason that you can come out ahead,
[01:28:48.800 --> 01:28:52.400]   the reason you could end up thinking the alignment was net negative was because there's a bunch of
[01:28:52.400 --> 01:28:56.160]   other stuff you're doing that makes AI safer. Like if you think the world is like gradually
[01:28:56.160 --> 01:29:00.800]   coming better to terms with the impact of AI or policies being made, or like you're getting
[01:29:00.800 --> 01:29:04.720]   increasingly prepared to handle the like threat of authoritarian abuse of AI. If you think other
[01:29:04.720 --> 01:29:09.120]   stuff is happening, that's improving preparedness, then you have reason beyond alignment research to
[01:29:09.120 --> 01:29:15.040]   slow down AI. Actually, how big a factor is that? So let's say right now we hit pause and you have
[01:29:15.040 --> 01:29:20.160]   10 years of no alignment, no capabilities, but just people get to talk about it for 10 years.
[01:29:20.160 --> 01:29:26.000]   How much more does that prepare people than we only have one year versus we have no time?
[01:29:26.000 --> 01:29:30.800]   Like it's just that time where no research and alignment or capabilities happening.
[01:29:30.800 --> 01:29:34.320]   Like, is there like, what does that dead time do for us?
[01:29:34.320 --> 01:29:37.360]   I mean, right now it seems like there's a lot of policy stuff you'd want to do. This seemed like
[01:29:37.360 --> 01:29:40.640]   less plausible a couple of years ago, maybe, but if like the world just knew they had a 10 year
[01:29:40.640 --> 01:29:44.960]   pause right now, I think there's a lot of sense of like, we have policy objectives to accomplish.
[01:29:44.960 --> 01:29:49.040]   If we had 10 years, we could pretty much do those things. Like we'd have a lot of time to like
[01:29:49.040 --> 01:29:53.440]   debate measurement regimes, debate like policy regimes and containment regimes,
[01:29:53.440 --> 01:29:57.680]   and a lot of time to set up those institutions. So like, if you told me that the world knew it
[01:29:57.680 --> 01:30:00.560]   was a pause, like it wasn't like people just see that AI progress isn't happening, but they're
[01:30:00.560 --> 01:30:04.480]   told like you guys have been granted or like cursed with a 10 year, no AI progress, no alignment
[01:30:04.480 --> 01:30:10.160]   progress pause. I think that would be quite good at this point. However, I think it would be much
[01:30:10.160 --> 01:30:14.000]   better at this point than it would have been two years ago. And so like the entire concern with
[01:30:14.000 --> 01:30:17.840]   like slowing AI development now, rather than taking the 10 year pause is just like, if you
[01:30:17.840 --> 01:30:21.520]   slow AI development by a year now, my guess is some gets clawed back by low hanging fruit gets
[01:30:21.520 --> 01:30:25.840]   picked faster in the future. My guess is you lose like half a year or something like that in the
[01:30:25.840 --> 01:30:30.880]   future, maybe even more, maybe like two thirds of a year. So it's like you're trading time now for
[01:30:30.880 --> 01:30:34.560]   time in the future at some rate. And it's just like, that eats up like a lot of the value of
[01:30:34.560 --> 01:30:38.720]   the slowdown. And the crucial point being that time in the future matters more because you have
[01:30:38.720 --> 01:30:42.480]   more information, people are more bought in and so on. Yeah. The same reason, like I'm more excited
[01:30:42.480 --> 01:30:47.120]   about policy change now than two years ago. So like my overall view is just like in the past,
[01:30:47.120 --> 01:30:50.640]   this calculus, this calculus changes over time, right? The like more people are getting prepared,
[01:30:50.640 --> 01:30:54.560]   the better the calculus is for slowing down at this very moment. And I think like now the
[01:30:54.560 --> 01:30:59.440]   calculus is, I would say positive for just, even if you pause now and it would get clawed back in
[01:30:59.440 --> 01:31:03.040]   the future, I think the pause now is just good because like enough stuff is happening. We have
[01:31:03.040 --> 01:31:06.480]   enough idea of like, I mean, probably even apart from alignment research. And certainly if you
[01:31:06.480 --> 01:31:11.200]   include alignment research, just like enough stuff is happening where the world is getting more ready
[01:31:11.200 --> 01:31:14.560]   and coming more to terms with impacts that I just think it is worth it, even though some of that
[01:31:14.560 --> 01:31:20.400]   time is going to get clawed back. Again, especially if like, there's a question of during a pause,
[01:31:20.400 --> 01:31:24.880]   does like NVIDIA keep making more GPUs? Like that sucks if they do, if you do a pause, but like,
[01:31:24.880 --> 01:31:28.960]   yeah, in practice, if you did a positive NVIDIA probably couldn't keep making more GPUs because
[01:31:28.960 --> 01:31:32.640]   in fact, like the demand for GPUs is really important for them to do that. But if you told
[01:31:32.640 --> 01:31:35.680]   me that you just get to scale up hardware production and like building the clusters,
[01:31:35.680 --> 01:31:39.120]   but not doing AI, then that's back to being that negative, I think pretty clearly.
[01:31:39.120 --> 01:31:43.520]   Then having brought up the fact that we want some sort of measurement scheme for these
[01:31:43.520 --> 01:31:47.920]   capabilities, let's talk about responsible scaling policies. Do you want to introduce what this is?
[01:31:47.920 --> 01:31:55.120]   Sure. So I guess the motivating question, it's like, what should AI labs be doing right now
[01:31:55.120 --> 01:31:59.280]   to manage risk and to sort of build good habits or practices for managing risk into the future?
[01:31:59.280 --> 01:32:06.800]   And I think my take is that current systems pose from a catastrophic risk perspective,
[01:32:06.800 --> 01:32:12.640]   not that much risk today. That is a failure to like control or understand GPT-4 can have real
[01:32:12.640 --> 01:32:17.520]   harms, but doesn't have much harm with respect to the kind of takeover risk I'm worried about,
[01:32:17.520 --> 01:32:23.440]   or even much catastrophic harm with respect to misuse. So I think like, if you want to manage
[01:32:23.440 --> 01:32:28.400]   catastrophic harms, I think right now you don't need to be that careful with GPT-4. And so to
[01:32:28.400 --> 01:32:32.560]   the extent you're like, what should labs do? I think like the single most important thing seems
[01:32:32.560 --> 01:32:39.360]   like understand whether that's the case, notice when that stops being the case, have a reasonable
[01:32:39.360 --> 01:32:44.160]   roadmap for what you're actually going to do when that stops being the case. So that motivates this
[01:32:44.160 --> 01:32:51.600]   like set of policies, which like I've sort of been pushing for labs to adopt, which is saying like,
[01:32:51.600 --> 01:32:55.200]   here's what we're looking for. Here's some threats we're concerned about. Here's some capabilities
[01:32:55.200 --> 01:32:59.280]   that we're measuring. Here's the level, like here's the actual concrete measurement results
[01:32:59.280 --> 01:33:03.280]   that would suggest to us that those threats are real. Here's the action we would take in response
[01:33:03.280 --> 01:33:08.560]   to observing those capabilities. If we couldn't take those actions, like if we've said that we're
[01:33:08.560 --> 01:33:11.440]   going to secure the weights, we're not able to do that. We're going to like pause until we can take
[01:33:11.440 --> 01:33:20.640]   those actions. Yeah. So this sort of, again, I think it's like motivated primarily, but like,
[01:33:20.640 --> 01:33:24.000]   what should you be doing as a lab to manage catastrophic risk now in a way that's like
[01:33:24.000 --> 01:33:28.240]   our reasonable precedent and habit and policy for continuing to implement into the future.
[01:33:28.240 --> 01:33:34.560]   And which labs, I don't know if this is public yet, but which labs are cooperating on this?
[01:33:34.560 --> 01:33:39.600]   Yeah. So Anthropic has, has written this document, this, the current responsible scaling policy.
[01:33:39.600 --> 01:33:46.640]   And then I've been talking with other folks, I guess, don't really want to comment on other,
[01:33:46.640 --> 01:33:50.880]   other conversations, but like, I think in general, like people who are more interested in,
[01:33:50.880 --> 01:33:54.960]   or like more think you have plausible catastrophic harms on like a five-year
[01:33:54.960 --> 01:33:59.680]   timeline are more interested in this. And there's like not that long a list of suspects like that.
[01:34:00.880 --> 01:34:06.880]   There's not that many labs. Okay. So if these companies would be willing to coordinate and
[01:34:06.880 --> 01:34:12.000]   say at these different benchmarks, we're going to make sure that we have these safeguards,
[01:34:12.000 --> 01:34:17.200]   what happens? I mean, there are other companies and other countries which care less about this.
[01:34:17.200 --> 01:34:21.520]   Are you just slowing down the companies that are most aligned?
[01:34:21.520 --> 01:34:26.880]   Yeah. I think the first sort of business understanding, like sort of what is actually
[01:34:26.880 --> 01:34:30.560]   a reasonable set of policies for managing risk. I do think there's a question of like,
[01:34:30.560 --> 01:34:33.360]   you might end up in a situation where you say like, well, here's what we would do in an ideal
[01:34:33.360 --> 01:34:38.720]   world. If we like, you know, if everyone was behaving responsibly, we'd want to keep risk
[01:34:38.720 --> 01:34:42.640]   to 1% or a couple of percent or whatever, maybe even lower levels, depending on how you feel.
[01:34:42.640 --> 01:34:48.480]   However, in the real world, like there's enough of a mess, like there's enough unsafe stuff
[01:34:48.480 --> 01:34:51.680]   happening that actually it's worth making like larger compromises or like, if we don't kill
[01:34:51.680 --> 01:34:55.040]   everyone, someone else will kill everyone anyway. So actually the counterfactual risk is much lower.
[01:34:55.040 --> 01:34:59.760]   I think like, if you end up in that situation, it's still extremely valuable to have said like,
[01:34:59.760 --> 01:35:03.040]   here's the policies we'd like to follow. Here's the policies we've started following.
[01:35:03.040 --> 01:35:07.280]   Here's why we think it's dangerous. Like here's the concerns we have if people are following
[01:35:07.280 --> 01:35:12.080]   significantly laxer policies. And then this is maybe helpful as like an input to or model for
[01:35:12.080 --> 01:35:17.360]   potential regulation. It's helpful for being able to just produce clarity about what's going on.
[01:35:17.360 --> 01:35:20.800]   I think historically there's been like considerable concern about developers being more or less safe,
[01:35:20.800 --> 01:35:24.720]   but there's not that much like legible differentiation in terms of what their
[01:35:24.720 --> 01:35:30.000]   policies are. I think getting to that world would be good. Like it would be very different. It's a
[01:35:30.000 --> 01:35:33.440]   very different world if you're like actor X is developing and I'm concerned that they will do
[01:35:33.440 --> 01:35:38.240]   so in an unsafe way. Versus if you're like, look, we take security precautions or safety precautions
[01:35:38.240 --> 01:35:42.800]   X, Y, Z. Here's why we think those precautions are desirable or necessary. We're concerned about
[01:35:42.800 --> 01:35:45.840]   this other developer because they don't do those things. I think it's just like a qualitatively,
[01:35:45.840 --> 01:35:50.160]   it's kind of the first step you would want to take in any world where you're trying to get people
[01:35:50.160 --> 01:35:54.240]   on side or like trying to, trying to move towards regulation that can manage risk.
[01:35:54.240 --> 01:35:59.600]   Well, how about the concern that you have these evaluations and let's say you declared the world,
[01:35:59.600 --> 01:36:07.120]   our new model has a capability to help develop bioweapons or help you make cyber attacks.
[01:36:07.120 --> 01:36:13.120]   And therefore we're pausing right now until you can figure this out. And China hears this and
[01:36:13.120 --> 01:36:17.360]   thinks, Oh, wow, a tool that can help us, you know, make a cyber attacks and then just steals
[01:36:17.360 --> 01:36:22.720]   the weights. Does this scheme work in the current regime where we can't ensure that China doesn't
[01:36:22.720 --> 01:36:29.840]   just steal the weights and more so, are you increasing the salience of dangerous models
[01:36:29.840 --> 01:36:34.640]   so that, you know, you just, you, you blur this out and then people want the way it's now because
[01:36:34.640 --> 01:36:40.160]   they know what they can do. Yeah. I mean, I think the general discussion does emphasize potential
[01:36:40.160 --> 01:36:43.360]   harms or potential. I mean, some of those are harms and some of those are just like impacts
[01:36:43.360 --> 01:36:47.600]   that are very large. And so it might also be an inducement to develop models. I think like that
[01:36:47.600 --> 01:36:51.360]   part, if you're for a moment, ignoring security and just saying like that may increase investment.
[01:36:51.360 --> 01:36:54.960]   I think it's like on balance, just quite good for people to have an understanding of potential
[01:36:54.960 --> 01:37:00.240]   impacts just because it is an input both into proliferation, but also into like regulation
[01:37:00.240 --> 01:37:07.840]   or safety with respect to things like security of either weights or other IP. I do think you
[01:37:07.840 --> 01:37:12.800]   want to have moved to like significantly more secure, like handling of model weights before
[01:37:12.800 --> 01:37:17.040]   the point where like a leak would be catastrophic. And indeed, like, you know, for example, in
[01:37:17.040 --> 01:37:23.520]   Anthropx document or in their plan, like security is one of the first sets of like tangible changes
[01:37:23.520 --> 01:37:26.960]   that is like at this capability level, we need to have like such security practices in place.
[01:37:26.960 --> 01:37:32.960]   So I do think that's just one of the things you need to get in place at a relatively early stage,
[01:37:32.960 --> 01:37:37.360]   because it does undermine like the rest of the measures you may take. And it's also just part
[01:37:37.360 --> 01:37:41.920]   of the easiest, like if you imagine catastrophic harms over the next couple of years, I think
[01:37:41.920 --> 01:37:46.240]   security failures are kind of play a central role in a lot of those. And maybe the last thing to
[01:37:46.240 --> 01:37:49.520]   say is like, it's not clear that you should say like, we have paused because we have models that
[01:37:49.520 --> 01:37:53.920]   can develop bioweapons versus just like, I mean, potentially not saying anything about like what
[01:37:53.920 --> 01:37:57.920]   models you've developed, or at least saying like, hey, like, by the way, here's like, you know,
[01:37:57.920 --> 01:38:01.040]   here's the set of practices we currently implement. Here's a set of capabilities
[01:38:01.040 --> 01:38:04.720]   our models don't have. We're just not even talking that much like, right, sort of the
[01:38:04.720 --> 01:38:10.000]   minimum of such a policy is to say, here's what we do from the perspective of security or internal
[01:38:10.000 --> 01:38:13.680]   controls or alignment. Here's a level of capability, which we'd have to do more.
[01:38:13.680 --> 01:38:17.280]   And you can say that you can raise your level of capability and raise your protective measures,
[01:38:17.280 --> 01:38:22.160]   like before your models hit your previous level. Like it's fine to say like, we are prepared to
[01:38:22.160 --> 01:38:25.920]   handle a model that has such and such extreme capabilities, like prior to actually having such
[01:38:25.920 --> 01:38:30.320]   a model at hand, as long as you're prepared to move your protective measures to that regime.
[01:38:30.320 --> 01:38:34.720]   Okay, so let's just get to the end where you think you're a generation away,
[01:38:34.720 --> 01:38:41.040]   or a little bit more scaffolding away from a model that is human level, and subsequently could
[01:38:41.040 --> 01:38:46.560]   cascade an intelligence explosion. What do you actually do at that point? Like, what is the level
[01:38:46.560 --> 01:38:52.480]   of evaluation of safety, where you would be satisfied releasing a human level model?
[01:38:52.480 --> 01:38:58.080]   There's a couple points that come up here. So one is like, this threat model of like,
[01:38:58.080 --> 01:39:02.960]   sort of automating R&D, or like, independent of whether AI can do something on the object level,
[01:39:02.960 --> 01:39:06.640]   that's potentially dangerous. And it's reasonable to be concerned if you have an AI system that
[01:39:06.640 --> 01:39:10.240]   might like, you know, if leaked, allow other actors to quickly build powerful AI systems,
[01:39:10.240 --> 01:39:14.160]   or might allow you to quickly build like much more powerful systems. Or might like, if you're
[01:39:14.160 --> 01:39:18.560]   trying to hold off on development, just like itself be able to create much more powerful systems.
[01:39:18.560 --> 01:39:22.160]   So like, one question is how to handle that kind of threat model as distinct from a threat model,
[01:39:22.160 --> 01:39:26.800]   like this could enable destructive bioterrorism, or this could enable like massively scaled cyber
[01:39:26.800 --> 01:39:31.520]   crime or whatever. And I think like, I am unsure how you should handle that. I think like right
[01:39:31.520 --> 01:39:35.520]   now, implicitly, it's being handled by saying like, look, there's a lot of overlap between the
[01:39:35.520 --> 01:39:38.800]   kinds of capabilities that are necessary to cause various harms and the kinds of capabilities are
[01:39:38.800 --> 01:39:42.640]   necessary to accelerate ML. So we're kind of going to catch those with like, an early warning sign
[01:39:42.640 --> 01:39:46.960]   for both and like, deal with the resolution of this question a little bit later. So for example,
[01:39:46.960 --> 01:39:53.200]   in an anthropics policy, they have this like, sort of autonomy in the lab benchmark, which I think is
[01:39:53.200 --> 01:39:58.480]   probably occurs prior to either like really massive AI acceleration, or to like, most potential
[01:39:58.480 --> 01:40:02.240]   catastrophic, like object level catastrophic harms. And it is that's like a warning sign,
[01:40:02.240 --> 01:40:05.840]   let's you punt. So this is a bit of an aggression in terms of like how to think about that risk.
[01:40:06.480 --> 01:40:09.680]   I think I am unsure whether you should be addressing that risk directly and saying like,
[01:40:09.680 --> 01:40:13.360]   we're scared to even work with such a model, or if you should be mostly focusing on object level
[01:40:13.360 --> 01:40:17.920]   harms and saying like, okay, we need more intense precautions to manage optical level harms because
[01:40:17.920 --> 01:40:22.720]   of the prospect of very rapid change. And the availability of this AI just like creates that
[01:40:22.720 --> 01:40:29.600]   kind of creates that prospect. Okay, this was all still a digression. So if you had a model,
[01:40:29.600 --> 01:40:32.720]   which you thought was like, potentially very scary, either on the object level, or because
[01:40:32.720 --> 01:40:37.360]   of like leading to the sort of intelligence explosion dynamics. I mean, things you want
[01:40:37.360 --> 01:40:42.400]   in place are like, you really do not want to be leaking the weights to that model. Like you don't
[01:40:42.400 --> 01:40:45.840]   want the model to be able to run away. You don't want human employees to be able to leak it. You
[01:40:45.840 --> 01:40:50.400]   don't want external attackers or any set of all three of those coordinating. You really don't
[01:40:50.400 --> 01:40:54.640]   want like internal abuse or tampering with such models. So if you're producing such models,
[01:40:54.640 --> 01:40:57.600]   you don't want to be the case, like a couple employees could change the way the model works,
[01:40:57.600 --> 01:41:01.680]   or could do something that violates your policy easily with that model. And if a model is very
[01:41:01.680 --> 01:41:06.640]   powerful, even the prospect of internal abuse could be quite bad. And so you might need significant
[01:41:06.640 --> 01:41:10.400]   internal controls to prevent that. Sorry, we're already getting to it. But the part I'm most
[01:41:10.400 --> 01:41:16.720]   curious about is separate from the ways in which other people might fuck with it. Like, what is it,
[01:41:16.720 --> 01:41:23.520]   you know, it's isolated. It's what is the point at which we satisfied it in and of itself is not
[01:41:23.520 --> 01:41:29.040]   going to pose a risk to humanity. It's human level, but we're, we're happy with it. Yeah.
[01:41:29.040 --> 01:41:32.160]   So I think here, so I listed maybe the two most simple ones that start out like
[01:41:32.160 --> 01:41:35.760]   security internal controls, I think become relevant immediately and are like very clear
[01:41:35.760 --> 01:41:39.200]   why you care about them. I think as you move beyond that, it really depends like how you're
[01:41:39.200 --> 01:41:43.840]   deploying such a system. So I think like, if your model, like if you have good monitoring
[01:41:43.840 --> 01:41:47.440]   and internal controls and security, and you just have weights sitting there, I think you mostly
[01:41:47.440 --> 01:41:50.800]   have addressed the risk from the weights just sitting there. Like now you're talking about
[01:41:50.800 --> 01:41:54.880]   for risk is mostly, and like, maybe there's some blurriness here of how much like internal
[01:41:54.880 --> 01:41:59.040]   controls captures not only employees using the model, but like anything a model can do internally.
[01:41:59.040 --> 01:42:02.080]   You like, we'd really like to be in a situation where your internal controls are robust.
[01:42:02.080 --> 01:42:09.440]   Not just to humans, but to models, potentially like EG, a model shouldn't be able to subvert
[01:42:09.440 --> 01:42:13.440]   these measures. And you like care, just as you care about, are your measures robust? If humans
[01:42:13.440 --> 01:42:16.560]   are behaving maliciously, are your measures robust if models are behaving maliciously.
[01:42:16.560 --> 01:42:20.880]   So I think beyond that, like if you've been managed the risk of just having the weights
[01:42:20.880 --> 01:42:25.120]   sitting around, now we talk about like, in some sense, most of the risk comes from doing things
[01:42:25.120 --> 01:42:28.480]   with the model. You need all the rest so that you like have any possibility of applying the breaks
[01:42:28.480 --> 01:42:32.160]   or implementing a policy. But at some point as the model gets competent, you're saying like, okay,
[01:42:32.160 --> 01:42:36.320]   could this cause a lot of harm, not because it leaks or something, but because we're just
[01:42:36.320 --> 01:42:40.080]   giving it a bunch of actuators or deploying it as a product and people could do crazy stuff with it.
[01:42:40.080 --> 01:42:44.480]   So if we're talking not only about a powerful model, but like a really broad deployment of
[01:42:44.480 --> 01:42:49.600]   just like, you know, something similar to like the OpenAI's API, where like people can do whatever
[01:42:49.600 --> 01:42:54.800]   they want with this model and maybe the economic impact is very large. So in fact, if you deploy
[01:42:54.800 --> 01:42:59.440]   that system, it will be used in a lot of places such that if AI systems wanted to cause trouble,
[01:42:59.440 --> 01:43:03.680]   it'd be very, very easy for them to cause catastrophic harms. Then I think you really
[01:43:03.680 --> 01:43:08.720]   need to have some kind of, I mean, I think probably the like science and discussion has to improve
[01:43:08.720 --> 01:43:13.600]   before this becomes that realistic, but you really want to have some kind of like alignment analysis,
[01:43:13.600 --> 01:43:18.640]   guarantee of alignment before you're comfortable with this. And so by that, I mean, like you want
[01:43:18.640 --> 01:43:22.400]   to be able to bound the probability that someday all the AI systems will do something really
[01:43:22.400 --> 01:43:25.600]   harmful, that there's like something that could happen in the world that would cause like these
[01:43:25.600 --> 01:43:30.560]   large scale correlated failures of your AIs. And so for that, like, I mean, there's sort of two
[01:43:30.560 --> 01:43:34.400]   categories. That's like one. The other thing you need is protection against misuse of various kinds,
[01:43:34.400 --> 01:43:38.720]   which is also quite hard. And by the way, which one are you worried about more misuse or misalignment?
[01:43:38.720 --> 01:43:42.720]   I mean, in the near term, I think harms from misuse are like, especially if you're not like
[01:43:42.720 --> 01:43:46.720]   restricting to the tail of like extremely large catastrophes, I think the harms from misuse are
[01:43:46.720 --> 01:43:52.000]   clearly larger in the near term. But actually on that, let me ask, because if you think that it is
[01:43:52.000 --> 01:43:57.920]   the case that there are simple recipes for destruction that are further down the tech tree,
[01:43:57.920 --> 01:44:03.520]   by that, I mean, you're familiar, but just for the audience, there's some way to configure
[01:44:03.520 --> 01:44:09.440]   $50,000 and a teenager's time to destroy a civilization. If that thing is available,
[01:44:09.440 --> 01:44:14.720]   then misuse is itself a tail risk, right? So do you think that that prospect is less likely than
[01:44:15.360 --> 01:44:18.240]   a way you could put it is there's like a bunch of potential destructive technologies.
[01:44:18.240 --> 01:44:22.960]   And like alignment is about like AI itself being such a destructive technology where like,
[01:44:22.960 --> 01:44:28.080]   even if like the world just uses the technology of today, simply access to AI could cause like
[01:44:28.080 --> 01:44:31.280]   human civilization to have serious problems. But there's also just a bunch of other potential
[01:44:31.280 --> 01:44:35.040]   destructive technologies. Again, we mentioned like physical explosives or bioweapons of various
[01:44:35.040 --> 01:44:40.080]   kinds, and then like the whole tail of who knows what. My guess is that like
[01:44:41.840 --> 01:44:46.800]   alignment becomes a catastrophic issue prior to most of these. That is like prior to some way to
[01:44:46.800 --> 01:44:51.760]   spend $50,000 to kill everyone with like the salient exception of possibly like bioweapons.
[01:44:51.760 --> 01:44:58.400]   So that would be my guess. And then like, there's a question of like, what is your risk management
[01:44:58.400 --> 01:45:02.160]   approach not knowing what's going on here. And I like, you don't understand whether there's some
[01:45:02.160 --> 01:45:07.120]   way to use $50,000. But I think you can do things like understand how good is an AI coming up with
[01:45:07.120 --> 01:45:10.880]   such schemes. Like you can talk to AI, be like, does it produce like new ideas for destruction
[01:45:10.880 --> 01:45:17.200]   we haven't recognized? Not whether we can evaluate it, but whether if such a thing exists and if it
[01:45:17.200 --> 01:45:21.680]   does, then the misuse itself is an existential risk. Cause it seemed like earlier you were
[01:45:21.680 --> 01:45:25.520]   saying a misalignment is where the existential risk comes from, but misuse is where the sort
[01:45:25.520 --> 01:45:30.000]   of short-term short-term dangers come from. Yeah. I mean, I think ultimately you're going
[01:45:30.000 --> 01:45:35.280]   to have a lot of destructive, like if you look at the entire tech tree of humanity's future,
[01:45:35.280 --> 01:45:38.160]   I think we have a fair number of destructive technologies, most likely.
[01:45:39.520 --> 01:45:43.360]   I think several of those will likely pose existential risks in parts, just kind of,
[01:45:43.360 --> 01:45:45.600]   you know, if you imagine a really long future, a lot of stuff's going to happen.
[01:45:45.600 --> 01:45:51.520]   And so when I talk about like where the existential risk comes from, I'm mostly thinking about like,
[01:45:51.520 --> 01:45:56.400]   comes from when, like at what point do you face what challenges or in what sequence?
[01:45:56.400 --> 01:46:02.320]   And so I'm saying like, I think misalignment is probably like, one way of putting it is if you
[01:46:02.320 --> 01:46:05.600]   imagine AI systems sophisticated enough to discover like destructive technologies that
[01:46:05.600 --> 01:46:09.360]   are totally not in our radar right now, I think those come well after AI systems,
[01:46:09.360 --> 01:46:13.520]   like capable enough that if misaligned, they would be catastrophically dangerous.
[01:46:13.520 --> 01:46:18.240]   There's like the level of competence necessary to, if broadly deployed in the world, bring down a
[01:46:18.240 --> 01:46:22.640]   civilization is much smaller than the level of competence necessary to like advise one person
[01:46:22.640 --> 01:46:27.360]   on how to bring down a civilization. Just cause in one case you have, you already have a billion
[01:46:27.360 --> 01:46:32.320]   copies of yourself or whatever. So yeah, I think it's mostly just a sequencing thing though. Like
[01:46:32.320 --> 01:46:37.120]   in the very long run, I think like you care about like, Hey, AI will be expanding the frontier of
[01:46:37.120 --> 01:46:40.880]   dangerous technologies. We want to have some policy for like exploring or understanding that
[01:46:40.880 --> 01:46:44.720]   frontier and like whether we're about to turn up something really bad. I think those policies can
[01:46:44.720 --> 01:46:49.680]   become really complicated right now. I think RSPs can focus more on like, we have our inventory of
[01:46:49.680 --> 01:46:53.600]   like, like the things that a human is going to do to cause a lot of harm with access to AI probably
[01:46:53.600 --> 01:46:57.680]   are things that are on our radar. That is like, they're not going to be completely unlike things
[01:46:57.680 --> 01:47:01.840]   that a human could do to cause a lot of harm with access to weak AIs or with access to other tools.
[01:47:02.640 --> 01:47:05.840]   Um, I think it's not crazy to initially say like, that's what we're doing. We're like looking at the
[01:47:05.840 --> 01:47:08.960]   things closest to human and humans being able to cause huge amounts of harm and asking which of
[01:47:08.960 --> 01:47:12.560]   those are taken over the line. But eventually that's not the case. Eventually like as we'll
[01:47:12.560 --> 01:47:17.120]   enable just like totally different ways of killing a billion people. Hmm. But I think I interrupted
[01:47:17.120 --> 01:47:24.000]   you on the initial question of, yeah. So human level AI, uh, not, not from leaking, but from
[01:47:24.000 --> 01:47:29.840]   deployment. Um, what, what is the point at which you'd be comfortable deploying a human level AI?
[01:47:29.840 --> 01:47:32.880]   Yeah. So again, there's sort of like some stuff you care about on the misuse side and some stuff
[01:47:32.880 --> 01:47:35.440]   you care about on the misalignment side. And there's probably further things you care about,
[01:47:35.440 --> 01:47:38.800]   especially to extend your concerns or broaden catastrophic risks. But maybe like I most want
[01:47:38.800 --> 01:47:41.440]   to talk about just like what you care about on the alignment side. Cause it's like the thing I've
[01:47:41.440 --> 01:47:46.400]   actually thought about most. And, um, also I think I care about a lot. Also, I think a significant
[01:47:46.400 --> 01:47:53.440]   fraction of the existential risk, like over the kind of foreseeable future. Um, so on that front,
[01:47:53.440 --> 01:48:00.640]   like I broadly think there's like two kinds, like if you ask me right now, what evidence for alignment
[01:48:00.640 --> 01:48:05.440]   could make you comfortable? Um, I think my best guess would be to provide two kinds of evidence.
[01:48:05.440 --> 01:48:10.880]   So one kind of evidence is on the, like, could you detect or prevent catastrophic harm if such a
[01:48:10.880 --> 01:48:16.320]   system was misaligned? I think there's like a couple of things you would do here. One thing
[01:48:16.320 --> 01:48:20.080]   you would do is on this, like adversarial evaluation front. So you could try and say,
[01:48:20.080 --> 01:48:25.600]   for example, like we have tried to test our system in a broad diversity of situations that reflect
[01:48:25.600 --> 01:48:29.920]   cases where it might cause harm. Um, like cases where someone is trying to recruit it to like,
[01:48:29.920 --> 01:48:33.520]   anyway, cases where it might cause harm or cases where a large number of AI systems might
[01:48:33.520 --> 01:48:38.080]   sort of all decide to work across purposes to humans, like the robot rebellion is starting or
[01:48:38.080 --> 01:48:44.800]   whatever. Um, we've done evaluation and like a broad distribution of situations. Um, and then
[01:48:44.800 --> 01:48:48.800]   we have tried to argue that our AI is actually like those tests are indicative of the real world.
[01:48:48.800 --> 01:48:52.160]   So we've tried to argue like, Hey, actually the AI is not very good at distinguishing
[01:48:52.160 --> 01:48:56.080]   situations we produce in the lab as tests from similar situations that occur in the real world.
[01:48:56.080 --> 01:49:01.200]   And the coverage of this distribution is reasonable. This is an example of a thing you can
[01:49:01.200 --> 01:49:04.800]   do. That's not really making assumptions about like what's going on inside the model or about
[01:49:04.800 --> 01:49:10.320]   how the model like is the model aligned. It's more trying to say like, is it the case that our like
[01:49:10.320 --> 01:49:16.720]   testing or monitoring machinery could detect or react to something bad happening? In this case,
[01:49:16.720 --> 01:49:19.920]   could it detect a system which like would under some conditions do something really bad?
[01:49:19.920 --> 01:49:24.400]   Um, so that's like, there's a lot of stuff in that genre and I tried to throw out that example,
[01:49:24.400 --> 01:49:27.600]   but I think there's like a big, like that on its own wouldn't be compelling. Even from this
[01:49:27.600 --> 01:49:30.560]   perspective, you kind of need a lot of pieces together and that's like kind of a demanding
[01:49:30.560 --> 01:49:34.240]   technical project, but I think one without necessarily like deep conceptual difficulty.
[01:49:34.240 --> 01:49:39.280]   Um, a big problem with it is it's liable to fail. Like it's liable. You're going to do that
[01:49:39.280 --> 01:49:42.320]   testing and say, we think these tests are representative because the AI can't distinguish
[01:49:42.320 --> 01:49:46.480]   tests from the real world, which you can maybe achieve now, but will probably become impossible
[01:49:46.480 --> 01:49:50.480]   to achieve at some level. It's not clear what's at human level or how far past human level.
[01:49:50.480 --> 01:49:57.040]   Let's just make that, uh, example concrete for the audience. Correct me if this is wrong,
[01:49:57.040 --> 01:50:02.480]   but it sounds like you basically give it the opportunity to like do a coup or make a bioweapon
[01:50:02.480 --> 01:50:07.760]   or whatever in, in testing, in a situation where it thinks it's the real world and you're like,
[01:50:07.760 --> 01:50:10.800]   it didn't do any of that. So is that what you're talking about?
[01:50:10.800 --> 01:50:15.840]   That's the kind of thing. Um, and it's not the greatest. So to add color, when I say it can't
[01:50:15.840 --> 01:50:19.200]   tell the difference, like the kind of evaluation you would love to do if you could is to say like,
[01:50:19.200 --> 01:50:23.040]   Hey, we have a bunch of real world deployment. We have a bunch of data from those deployments.
[01:50:23.040 --> 01:50:27.920]   We also have a bunch of in the lab tests of situations. Um, if we look at like in the lab
[01:50:27.920 --> 01:50:32.960]   tests of a situation together with like real world cases in which the situation occurs, and then we
[01:50:32.960 --> 01:50:36.320]   like fine tune a model to try and distinguish the difference there, the model can't even tell
[01:50:36.320 --> 01:50:41.680]   when fine tune the difference between like testing and real world cases. And like, if the model can
[01:50:41.680 --> 01:50:44.880]   tell the difference, which is almost certainly going to be the case at some level capability,
[01:50:44.880 --> 01:50:49.200]   then like you say, okay, our first line of like hoped for defense has failed. And so now we need
[01:50:49.200 --> 01:50:52.480]   to understand like, how can tests be indicative when models are capable enough to tell the
[01:50:52.480 --> 01:50:55.360]   difference between the tests and the real world? Like now you need to start arguing about like,
[01:50:55.360 --> 01:50:59.360]   what is the model paying attention to? Like, I think it's basically like a first line of,
[01:50:59.360 --> 01:51:03.760]   if you get lucky, what would work here. And then there's the like deeper, like you probably have
[01:51:03.760 --> 01:51:08.640]   to do novel science at some point to like make this case. Um, all of this was part of like,
[01:51:08.640 --> 01:51:11.600]   I was saying, maybe you want to have kind of two lines of defense. All of this was like part of
[01:51:11.600 --> 01:51:15.120]   this first line of defense of like, you know, can you detect something that's going wrong? Or can
[01:51:15.120 --> 01:51:18.320]   you prevent harm from occurring? Can you have monitoring that will react quickly enough to
[01:51:18.320 --> 01:51:22.640]   avoid catastrophe? I think you probably also want to have a second line of defense. That's more,
[01:51:22.640 --> 01:51:27.520]   maybe this is even more important than the first one. This is just understanding whether
[01:51:27.520 --> 01:51:30.880]   dangerous forms of misalignment can occur. It's like the best reference point for this,
[01:51:30.880 --> 01:51:35.520]   I think is the, like, there've been a couple of projects like this in academia.
[01:51:35.520 --> 01:51:39.120]   Anthropic has been working on a project or spun up a team doing this kind of work,
[01:51:39.120 --> 01:51:44.400]   trying to say like, can dangerous forms of reward hacking occur or can deceptive alignment in fact
[01:51:44.400 --> 01:51:48.800]   occur in the lab? And like, here, what you want to do is either say like, okay, even if we create,
[01:51:48.800 --> 01:51:52.400]   you know, best case, even if you create optimal conditions for deceptive alignment or for reward
[01:51:52.400 --> 01:51:56.560]   hacking, we actually can't, we just can't cause it to occur even in the lab. And if you do a good
[01:51:56.560 --> 01:52:01.520]   enough job of that, I think it could give you some evidence. Um, and again, more likely that fails.
[01:52:01.520 --> 01:52:05.200]   And when you create optimal conditions, you do see a deceptive alignment and reward hacking in the
[01:52:05.200 --> 01:52:09.200]   lab. But then once you have that data, once you can say, okay, in the lab, actually these things
[01:52:09.200 --> 01:52:14.000]   can occur, then you can start saying, and we have a robust scientific understanding that enables us
[01:52:14.000 --> 01:52:18.000]   to like fix those problems when they occur. Or like, then you start, you kind of, you have this
[01:52:18.000 --> 01:52:23.520]   fork on your second line where you say either it is very hard or we're not able to create conditions
[01:52:23.520 --> 01:52:28.000]   where these failures emerge. Or I think more likely we are able to create those conditions.
[01:52:28.000 --> 01:52:31.760]   And like, here's the story about why, like, we are able to detect those in a way that would
[01:52:31.760 --> 01:52:36.160]   work in the real world. And we can see they don't occur in the real world. How do you create the
[01:52:36.160 --> 01:52:40.880]   optimal conditions for it to want to be deceptive? Do you fine tune it on mind comp or what are you
[01:52:40.880 --> 01:52:45.200]   doing? Yeah. So for deceptive alignment, I mean, I think it's really complicated.
[01:52:45.200 --> 01:52:50.320]   Um, yeah, I think this is a very hard project. I think like one of the things, one of the most
[01:52:50.320 --> 01:52:54.000]   important things for people concerned about misalignment to do is to like, ensure that
[01:52:54.000 --> 01:52:58.800]   adequate effort is put into this task. And like, we have in fact created good conditions in the
[01:52:58.800 --> 01:53:02.880]   laboratory and there's like real disagreement about how informative those will be. But okay,
[01:53:02.880 --> 01:53:08.000]   if you were to guess for deceptive alignment, how would you do this? Like an example is suppose you
[01:53:08.000 --> 01:53:12.240]   train an AI system that like wants, has some like medium horizon goal. You train your AI system to
[01:53:12.240 --> 01:53:17.360]   like, you know, whatever, get some, get some paper clips. Who wants to produce as many paper clips as
[01:53:17.360 --> 01:53:22.000]   it can over the next like five days. It's just like select actions really aggressively for producing
[01:53:22.000 --> 01:53:26.640]   paper clips over the next five days. Um, you do your RLHF, you do your pre-training, whatever.
[01:53:26.640 --> 01:53:31.120]   That's like your phase one. You also ensure AI system has like a really good understanding of
[01:53:31.120 --> 01:53:36.080]   how it's trained. So like this AI system wants paper clips and it understands everything about
[01:53:36.080 --> 01:53:39.120]   how it's trained and everything about how it's fine tuned. And you train on just like a lot of
[01:53:39.120 --> 01:53:43.760]   this data. And then you say, okay, if we've done all of that, we have this concern that if a system
[01:53:43.760 --> 01:53:47.680]   like wants paper clips and understands really well how it's trained, then it will like, if it's going
[01:53:47.680 --> 01:53:51.760]   to be trained to like get apples instead of paper clips, it's just going to do some cost benefit
[01:53:51.760 --> 01:53:55.200]   and be like, ah, you know, really while I'm being trained to get apples, I should do that. But I
[01:53:55.200 --> 01:53:58.240]   should do that whether or not, even if I want paper clips, I should still do that. So training
[01:53:58.240 --> 01:54:01.440]   won't really affect its values. It will just understand that it's being trained to get apples.
[01:54:01.440 --> 01:54:04.560]   It will try and get apples. And if you like take it out of training, it will go back to getting
[01:54:04.560 --> 01:54:09.120]   paper clips. It's like, I think this exact setup like has a number of problems or a number of
[01:54:09.120 --> 01:54:12.560]   disanalogies from the real world, but this is a setup which is relatively conducive to deceptive
[01:54:12.560 --> 01:54:16.800]   alignment, like produce a system that wants one thing, tell it a lot about its training,
[01:54:16.800 --> 01:54:19.680]   the kind of information that you might expect a system would get, and then try and understand
[01:54:19.680 --> 01:54:26.320]   whether in fact it is able to then, or it tends, or sometimes under optimal conditions, in fact,
[01:54:26.320 --> 01:54:30.560]   continues pursuing paper clips, only pursuing apples when it thinks it's being trained.
[01:54:30.560 --> 01:54:33.920]   All right. And this actually might be a good jumping off point into your current research,
[01:54:33.920 --> 01:54:39.680]   but so in that case where it wants to pursue paper clips, what would it look like for you
[01:54:39.680 --> 01:54:44.800]   to be say, well, we figured out how, why it wants paper clips and we figured out how to make sure
[01:54:44.800 --> 01:54:48.480]   it doesn't optimize for that at our expense when we deploy it?
[01:54:49.120 --> 01:54:52.800]   Yeah, I think that, so at a meta level in terms of like, what's your protection like,
[01:54:52.800 --> 01:54:56.480]   I think like what you want to be saying is we have these examples in the lab of something bad
[01:54:56.480 --> 01:55:00.000]   happening. We're concerned about the problem at all because we have examples in the lab.
[01:55:00.000 --> 01:55:04.480]   And again, this should all be an addition. I think like you kind of want this like defense
[01:55:04.480 --> 01:55:07.200]   in depth of saying we also have this testing regime that would detect problems for the
[01:55:07.200 --> 01:55:13.200]   deployed model. We have our problems in the lab. We then have some techniques which we believe
[01:55:13.200 --> 01:55:17.360]   address these problems. Like we believe that like adversarial training fixes this, or we believe
[01:55:17.360 --> 01:55:20.800]   that like our interpretability method will reliably detect this kind of deceptive alignment,
[01:55:20.800 --> 01:55:24.400]   or we believe our anomaly detection will reliably detect when the model goes from thinking it's
[01:55:24.400 --> 01:55:28.640]   being trained to thinking it should defect. And then you can say in the lab, like we have
[01:55:28.640 --> 01:55:32.480]   some understanding of when those techniques work and when they don't, we have some understanding
[01:55:32.480 --> 01:55:35.920]   of the relevant parameters for the real system that's deployed and we have like a reasonable
[01:55:35.920 --> 01:55:39.360]   margin of safety. So we have like reasonable robustness on our story about when this works
[01:55:39.360 --> 01:55:43.360]   and when it doesn't. And like, we can apply that margin of safe, like with a margin of safety to
[01:55:43.360 --> 01:55:48.160]   the real deployed system. So I think this is the kind of story you want to build towards in the
[01:55:48.160 --> 01:55:52.720]   long run. Like do your best to produce all the failures you can in the lab or versions of them.
[01:55:52.720 --> 01:55:57.040]   Do your best to understand like what causes them, like what kind of anomaly detection
[01:55:57.040 --> 01:56:00.640]   actually works for detecting this, or what kind of filtering actually works, and then
[01:56:00.640 --> 01:56:05.920]   apply that. And that's at the meta level. It's not talking about like what actually are those
[01:56:05.920 --> 01:56:10.240]   measures that would work effectively, which is obviously like what, I mean, a lot of alignment
[01:56:10.240 --> 01:56:14.560]   research is really based on this hypothetical of like someday there'll be AI systems that fail in
[01:56:14.560 --> 01:56:18.720]   this way. What would you want to do? What, can we have the technologies ready either because we
[01:56:18.720 --> 01:56:22.400]   might never see signs of the problem or because like we want to be able to move fast once we see
[01:56:22.400 --> 01:56:28.240]   signs of the problem. And obviously most of my life is in that. I am really in that bucket. Like
[01:56:28.240 --> 01:56:32.320]   I mostly do alignment research. It's just building out the techniques that do not have these failures
[01:56:32.320 --> 01:56:36.960]   such that they can be available as an alternative. If in fact these failures occur. Ideally, they'll
[01:56:36.960 --> 01:56:40.000]   be so good that even if you haven't seen them, you would just want to switch to reasonable methods
[01:56:40.000 --> 01:56:43.040]   that don't have these, or ideally they'll work as well or better than normal training, but.
[01:56:43.040 --> 01:56:48.000]   Ideally, but will work better than the training.
[01:56:48.000 --> 01:56:53.120]   Yeah. So our, our quest is to design training methods for which we don't expect them to lead
[01:56:53.120 --> 01:56:56.640]   to reward hacking or don't expect them to lead to deceptive alignment. Ideally, that won't be like
[01:56:56.640 --> 01:57:00.320]   a huge tax where people like, well, we'd use those methods only if we're really worried about reward
[01:57:00.320 --> 01:57:03.280]   hacking or deceptive alignment. Ideally, those methods would just work quite well. And so people
[01:57:03.280 --> 01:57:06.880]   would be like, sure. I mean, they also address a bunch of like other more mundane problems.
[01:57:06.880 --> 01:57:10.560]   So why would we not use them? Which I think is like, that's sort of the good story. The good
[01:57:10.560 --> 01:57:13.840]   story is you like develop methods that address a bunch of existing problems because they just are
[01:57:13.840 --> 01:57:18.160]   more principled ways to train AI systems that work better. People adopt them. And then we are no
[01:57:18.160 --> 01:57:20.800]   longer worried about e.g. reward hacking or deceptive alignment.
[01:57:20.800 --> 01:57:25.840]   All right. And to make this more concrete, tell me if this is the wrong way to paraphrase it.
[01:57:25.840 --> 01:57:29.840]   The example of something where it just make a system better. So why not just use it?
[01:57:29.840 --> 01:57:36.080]   At least so far, it might be like RLHF where we don't know if it generalizes, but so far,
[01:57:36.080 --> 01:57:40.320]   you know, it makes your chat GPT thing better. And you can also use it to make sure that chat
[01:57:40.320 --> 01:57:46.000]   GPT doesn't tell you how to make a bioweapon. So, yeah, it's not an extra tax.
[01:57:46.000 --> 01:57:50.880]   Yeah. So I think this is right in the sense that like using RLHF is not really a tax. If you wanted
[01:57:50.880 --> 01:57:55.200]   to deploy a useful system, like why would you not? It's just like very much worth the money of doing
[01:57:55.200 --> 01:58:03.680]   the training. And then, yeah, so RLHF will address certain kinds of alignment failures. That is like
[01:58:03.680 --> 01:58:06.640]   where a system like just doesn't understand or, you know, is changing the next word prediction.
[01:58:06.640 --> 01:58:09.680]   It's like, this is the kind of context where a human would do this wacky thing, even though it's
[01:58:09.680 --> 01:58:12.880]   not what we'd like. There's like some very dumb alignment failures that will be addressed by it.
[01:58:12.880 --> 01:58:17.440]   I think mostly, yeah, the question is, is that true even for like the sort of more challenging
[01:58:17.440 --> 01:58:21.040]   alignment failures that motivate concern in the field? I think RLHF doesn't address like most of
[01:58:21.040 --> 01:58:26.320]   the concerns that motivate people to be worried about alignment. Yeah. I'll let the audience look
[01:58:26.320 --> 01:58:29.920]   up what RLHF is if they don't know. It will just be more simpler to just look it up than
[01:58:30.560 --> 01:58:36.240]   explain right now. Okay. So this seems like a good jumping off point to talk about the mechanism or
[01:58:36.240 --> 01:58:40.960]   the research you've been doing to that end. Explain it as you might to a child.
[01:58:40.960 --> 01:58:46.880]   Yeah. So the high level, I mean, there's a couple of different high level descriptions you could
[01:58:46.880 --> 01:58:53.200]   give, and maybe I'll unwisely give like a couple of them in the hopes that one is kind of makes
[01:58:53.200 --> 01:59:00.880]   sense. A first pass is like, it would sure be great to understand like why models have the
[01:59:00.880 --> 01:59:06.160]   behaviors they have. So you're like, look at GPT-4. If you ask GPT-4 a question, it will say
[01:59:06.160 --> 01:59:10.320]   something that like, you know, looks very polite. And if you ask it to take an action, it will take
[01:59:10.320 --> 01:59:16.080]   an action that like doesn't look dangerous. You will decline to do a coup, whatever, all this
[01:59:16.080 --> 01:59:21.840]   stuff. I think you'd really like to do is look inside the model and understand like why it has
[01:59:21.840 --> 01:59:27.280]   those desirable properties. And if you understood that, you could then say like, okay, now can we
[01:59:27.280 --> 01:59:31.920]   flag when these properties are at risk of breaking down or predict how robust these properties are,
[01:59:31.920 --> 01:59:37.680]   determine if they hold in cases where it's too confusing for us to tell directly by asking if
[01:59:37.680 --> 01:59:41.360]   like the underlying cause is still present. So that's like a thing people would really like to
[01:59:41.360 --> 01:59:47.360]   do. Most work aimed at that long-term goal right now is just sort of opening up neural nets and
[01:59:47.360 --> 01:59:51.600]   doing some interpretability and trying to say like, can we understand even for very simple models,
[01:59:51.600 --> 01:59:57.440]   why they do the things they do or what this neuron is for or questions like this. So Arc is taking a
[01:59:57.440 --> 02:00:02.880]   somewhat different approach where we're instead saying like, okay, look at these interpretability
[02:00:02.880 --> 02:00:07.200]   explanations that are made about models and ask like, what are they actually doing? Like,
[02:00:07.200 --> 02:00:11.600]   what is the type signature? What are like the rules of the game for making such an explanation?
[02:00:11.600 --> 02:00:17.280]   What makes like a good explanation? And probably the biggest part of the hope is that
[02:00:18.720 --> 02:00:23.280]   if you want to say detect when the explanation has broken down or something weird has happened,
[02:00:23.280 --> 02:00:27.120]   that doesn't necessarily require a human to be able to understand this like complicated
[02:00:27.120 --> 02:00:31.440]   interpretation of a giant model. If you understand like what is an explanation about,
[02:00:31.440 --> 02:00:35.920]   or like what were the rules of the game? How are these constructed? Then you might be able to sort
[02:00:35.920 --> 02:00:40.800]   of automatically discover such things and automatically determine if right on a new
[02:00:40.800 --> 02:00:46.720]   input, it might have broken down. So that's one way of sort of describing the high level goal,
[02:00:46.720 --> 02:00:49.600]   like starting from, you can start from interpretability and say like, can we formalize
[02:00:49.600 --> 02:00:54.560]   this activity or like what a good interpretation or explanation is. There's some other work in
[02:00:54.560 --> 02:00:58.320]   that genre, but I think for just taking like a particularly ambitious approach to it.
[02:00:58.320 --> 02:01:01.920]   Yeah. Let's dive in. So, okay. Well, what, what is a good explanation?
[02:01:01.920 --> 02:01:05.280]   You mean, what is this kind of criterion? At the end of the day, we kind of want some criterion
[02:01:05.280 --> 02:01:09.280]   and the way the criterion should work is like, you have your neural net. Yeah. You have some
[02:01:09.280 --> 02:01:14.960]   behavior of that model. Like a really simple example is like, Anthropic has this sort of
[02:01:14.960 --> 02:01:19.040]   informal description being like, here's induction, like the tendency that if you have like the
[02:01:19.040 --> 02:01:24.400]   pattern AB followed by A will tend to predict B. You can give us some kind of words and experiments
[02:01:24.400 --> 02:01:27.600]   and numbers that are trying to like explain that. And what we want to do is say like,
[02:01:27.600 --> 02:01:32.240]   what is like a formal version of that object? Like, how do you actually test if such an explanation
[02:01:32.240 --> 02:01:34.960]   is good? This is just clarifying what we're looking for when we say like, we want to define
[02:01:34.960 --> 02:01:40.640]   what makes an explanation good. And the kind of answer that we are like searching for or settling
[02:01:40.640 --> 02:01:47.680]   on is saying like, this is kind of a deductive argument for the behavior. So you want to like
[02:01:47.680 --> 02:01:50.800]   get given the weights of a neural net. It's just like a bunch of numbers. You got your million
[02:01:50.800 --> 02:01:55.360]   numbers or billion numbers or whatever. And then you want to say like, here's some things I can
[02:01:55.360 --> 02:01:58.800]   point out about the network and some like conclusions I can draw. I can be like, well,
[02:01:58.800 --> 02:02:02.800]   look, you know, these two vectors have large inner product and therefore like these two
[02:02:02.800 --> 02:02:07.280]   activations are going to be correlated on this distribution. Like make like, these are not
[02:02:07.280 --> 02:02:10.640]   established by like drawing samples and checking things are correlated, but saying because of the
[02:02:10.640 --> 02:02:14.400]   weights being the way they are, we can like proceed forward through the network and derive
[02:02:14.400 --> 02:02:18.960]   some conclusions about like what properties the outputs will have. So you could think of this as
[02:02:18.960 --> 02:02:23.040]   like the most extreme form would be just proving that your model has this induction behavior.
[02:02:23.040 --> 02:02:26.800]   Like you could imagine proving that if I sample tokens at random with this pattern, AB followed
[02:02:26.800 --> 02:02:33.440]   by A, that B appears 30% of the time or whatever. That's the most extreme form. And what we're
[02:02:33.440 --> 02:02:36.720]   doing is kind of just like relaxing the rules of the game for proof, saying proofs are like
[02:02:36.720 --> 02:02:41.120]   incredibly restrictive. I think it's unlikely they're going to be applicable to kind of any
[02:02:41.120 --> 02:02:46.800]   interesting neural net. But the like thing about proofs that is relevant for our purposes, isn't
[02:02:46.800 --> 02:02:49.760]   that they give you like a hundred percent confidence. So you don't have to be like this
[02:02:49.760 --> 02:02:55.280]   incredible level of demand for rigor. You can relax like the standards of proof a lot and still
[02:02:55.280 --> 02:02:58.800]   get this feature where it's like a structural explanation for the behavior where like deducing
[02:02:58.800 --> 02:03:03.200]   one thing from another until at the end, your final conclusion is like, therefore induction occurs.
[02:03:04.480 --> 02:03:10.320]   Would it be useful to maybe motivate this by explaining what the problem with normal
[02:03:10.320 --> 02:03:17.440]   mechanistic interoperability is? So you mentioned induction heads. This is anthropic
[02:03:17.440 --> 02:03:22.400]   founded two-layer transformers where anthropic noticed that in a two-layer transformer,
[02:03:22.400 --> 02:03:30.240]   there's a pretty simple circuit by which if AB happens in the past, then the model knows that
[02:03:30.240 --> 02:03:35.040]   if you see an A and now you do a B next, but that's a two-layer transformer. So we're going
[02:03:35.040 --> 02:03:39.200]   to, we have these, we have these models that have like hundreds of layers that have
[02:03:39.200 --> 02:03:44.480]   trillions of parameters. Okay. Anyways, like what is wrong with mechanistic interoperability?
[02:03:44.480 --> 02:03:51.120]   Yeah. So I think, I mean, I like mechanistic interpretability quite a lot. And I do think
[02:03:51.120 --> 02:03:54.720]   like if you just consider the entire portfolio of what people are working on for alignment,
[02:03:54.720 --> 02:03:58.560]   I think there should be more work on mechanistic interpretability than there is on this project
[02:03:58.560 --> 02:04:03.360]   ARC is doing. But I think that's the case. So I think we're mostly talking about, yeah,
[02:04:03.360 --> 02:04:06.160]   I think we're kind of a small fraction of the portfolio. And I think it's like a good enough
[02:04:06.160 --> 02:04:10.320]   bet. It's like quite a good bet overall. But so the thing that I, like the problem we're
[02:04:10.320 --> 02:04:14.400]   trying to address in mechanistic interpretability is kind of like, if you do some interpretability
[02:04:14.400 --> 02:04:18.160]   and you explain some phenomenon, you face this question of like, what does it mean your
[02:04:18.160 --> 02:04:22.000]   explanation was good? Like if you want to either, I think this is a problem like somewhat institutionally
[02:04:22.000 --> 02:04:25.600]   or culturally, like it's just hard to know what you're doing. And it's hard to scale up an
[02:04:25.600 --> 02:04:29.360]   activity when you like, don't really understand the rules of the game for that activity very well.
[02:04:29.360 --> 02:04:31.360]   It's hard to have like that much confidence in your results.
[02:04:31.360 --> 02:04:36.560]   - The explanation being it outputted this because it wants to take over the world versus
[02:04:36.560 --> 02:04:41.600]   it outputted this because it just wants to help you like just make it concrete why the
[02:04:41.600 --> 02:04:42.320]   explanation matters.
[02:04:42.320 --> 02:04:46.080]   - Yeah. So like the ideal kind of outcome here would be to say you have your AI system behaving
[02:04:46.080 --> 02:04:52.080]   nicely. You get some explanation for sort of why it's behaving nicely. And we could tell a story
[02:04:52.080 --> 02:04:55.040]   in English about that explanation, but we're not actually imagining the explanation being
[02:04:55.040 --> 02:04:58.800]   a thing that makes sense to a human. But if you were to tell a story in English, which again,
[02:04:58.800 --> 02:05:02.080]   you will not see as a researcher, it would be something like, well, then the model believes
[02:05:02.080 --> 02:05:05.600]   it's being trained. And so because it believes it's being trained, it knows it needs to not do
[02:05:05.600 --> 02:05:08.480]   anything scary looking or else the humans will penalize it. Like that's something that's
[02:05:08.480 --> 02:05:12.880]   happening inside this like opaque explanation. And then the hope is if you have that explanation,
[02:05:12.880 --> 02:05:15.680]   and then you run into a new input on which the model doesn't believe it's being trained,
[02:05:15.680 --> 02:05:20.400]   right? If you just look at the set of activations of your model, that is not necessarily a weird
[02:05:20.400 --> 02:05:23.760]   looking activation, right? It's just a bunch of numbers. But if you look at this explanation,
[02:05:23.760 --> 02:05:27.440]   you see actually the explanation really crucially depended on this fact holding consistently across
[02:05:27.440 --> 02:05:31.760]   the training distribution, which again, we as humans could editorialize and say that fact was
[02:05:31.760 --> 02:05:35.600]   it believes it's being trained. But like the explanation doesn't fundamentally make reference
[02:05:35.600 --> 02:05:39.200]   to that. It's just saying, here's the property of the activations, which holds over the training set.
[02:05:39.200 --> 02:05:44.240]   And this property is responsible for the behavior, namely that it doesn't do anything that looks too
[02:05:44.240 --> 02:05:48.560]   dangerous. So then when like a new input comes in and it doesn't satisfy that property, you can say,
[02:05:48.560 --> 02:05:53.440]   okay, this is anomalous with respect to that explanation. So either it will not have the
[02:05:53.440 --> 02:05:57.600]   behavior, like it won't, it may will do something that appears dangerous, or maybe it will have that
[02:05:57.600 --> 02:06:01.120]   behavior, but for some different reason than normal, right? Normally it does it because of
[02:06:01.120 --> 02:06:05.200]   this pathway and now it's doing it for a different pathway. And so you would like to be able to flag
[02:06:05.200 --> 02:06:09.440]   that both there's a risk of not exhibiting the behavior. And if it happens, it happens for a
[02:06:09.440 --> 02:06:14.240]   weird reason. And then you could, I mean, at a minimum when you encounter that, say like, okay,
[02:06:14.240 --> 02:06:17.920]   raise some kind of alarm, there's sort of more ambitious, complicated plans for how you would
[02:06:17.920 --> 02:06:21.440]   use it, right? So it fits, Arc has some like longer story, which is kind of a motivated this,
[02:06:21.440 --> 02:06:25.360]   so like how it fits into the whole rest of the plan. I just wanted to flag that because
[02:06:25.360 --> 02:06:31.360]   just so it's clear why the explanation matters. Yeah. And for this purpose, it's like the thing
[02:06:31.360 --> 02:06:34.720]   that's essential is kind of reasoning from like one property of your model to the next property
[02:06:34.720 --> 02:06:38.160]   of your model. Like it's really important that you're going forward step-by-step rather than
[02:06:38.160 --> 02:06:42.160]   drawing a bunch of samples and confirming the property holds. Because if you just draw a bunch
[02:06:42.160 --> 02:06:45.520]   of samples and confirm the property holds, you can't, you don't get this like check where say,
[02:06:45.520 --> 02:06:48.960]   oh, here was the relevant fact about the internals that was responsible for this
[02:06:48.960 --> 02:06:52.720]   downstream behavior. All you see is like, yeah, we checked a million cases and it happened in
[02:06:52.720 --> 02:06:56.880]   all of them. You really want to see this, like, okay, here was the fact about the activations,
[02:06:56.880 --> 02:07:01.920]   which like kind of causally leads to this behavior. But explain why the sampling,
[02:07:01.920 --> 02:07:06.080]   why it matters that you have the causal explanation. Primarily because of this,
[02:07:06.080 --> 02:07:09.280]   like being able to tell, like if things had been different, like if you have an input where this
[02:07:09.280 --> 02:07:13.200]   doesn't happen, then you shouldn't be scared. Even if the output is the same. Yeah. Or if
[02:07:13.200 --> 02:07:17.600]   the output, or if it's too expensive to check in this case. And like, to be clear, when we talk
[02:07:17.600 --> 02:07:20.640]   about like formalizing, what is a good explanation? I think there is a little bit of work that pushes
[02:07:20.640 --> 02:07:24.160]   on this and it mostly takes this like causal approach of saying, well, what should an
[02:07:24.160 --> 02:07:28.080]   explanation do? It should not only predict the output, it should predict how the output changes
[02:07:28.080 --> 02:07:35.040]   in response to changes in the internals. So that's the most common approach to formalizing. Like,
[02:07:35.040 --> 02:07:38.320]   what is a good explanation? And like, even when people are doing informal interpretability,
[02:07:38.320 --> 02:07:41.760]   I think like if you're publishing an ML conference and you want to say like,
[02:07:41.760 --> 02:07:45.920]   this is a good explanation, the way you would verify that would, even if not like a formal,
[02:07:45.920 --> 02:07:49.200]   like set of causal intervention experiments, it would be some kind of ablation really. Then we
[02:07:49.200 --> 02:07:52.320]   messed with the inside of the model and it had the effect which we would expect
[02:07:52.320 --> 02:07:56.480]   based on our explanation. Anyways, back to the problems of mechanistic interpretability.
[02:07:56.480 --> 02:07:59.360]   Yeah. Yeah. I mean, I guess this is, this is relevant in the sense that like,
[02:07:59.360 --> 02:08:04.240]   I think the basic difficulty is you don't really understand the objective of what you're doing,
[02:08:04.240 --> 02:08:08.400]   which is like a little bit hard institutionally, or like scientifically, it's just rough. It's
[02:08:08.400 --> 02:08:11.920]   better to like, it's easier to do science when the goal of the game is to predict something and
[02:08:11.920 --> 02:08:14.960]   you know what you're predicting than when the goal of the game is to like understand in some
[02:08:14.960 --> 02:08:20.880]   undefined sense. I think it's particularly relevant here just because like the informal
[02:08:20.880 --> 02:08:25.360]   standard we use involves like humans being able to make sense of what's going on. And there's
[02:08:25.360 --> 02:08:29.680]   like some question about scalability of that. Like, will humans recognize the concepts that
[02:08:29.680 --> 02:08:34.240]   models are using? Yeah. I think as you try and automate it, it becomes like increasingly
[02:08:34.240 --> 02:08:38.160]   concerning. If you're on like slightly shaky ground about like what exactly you're doing or
[02:08:38.160 --> 02:08:41.760]   what exactly the standard for success is. I think there's like a number of reasons, like as you work
[02:08:41.760 --> 02:08:45.040]   with really large models, it becomes just increasingly desirable to have a really robust
[02:08:45.040 --> 02:08:49.920]   sense of what you're doing. But I do think it would be better even for small models to have
[02:08:49.920 --> 02:08:55.120]   a, have a clearer sense. The point you made about as you automate it, is it because you, whatever
[02:08:55.120 --> 02:08:58.720]   work the automated alignment researcher is doing, you want to make sure you can verify it?
[02:08:58.720 --> 02:09:02.240]   I think it's most of all like, so a way you can automate, I think how you would automate
[02:09:02.240 --> 02:09:06.880]   interpretability if you wanted to right now, is you take the process humans use. It's like, great,
[02:09:06.880 --> 02:09:10.800]   we're going to take that human process, train ML systems to do the pieces that humans do of that
[02:09:10.800 --> 02:09:15.600]   process, and then just do a lot more of it. So I think that is great as long as your task
[02:09:15.600 --> 02:09:19.840]   decomposes into like human size pieces. And there's just this fundamental question about
[02:09:19.840 --> 02:09:24.240]   large models, which is like, do they decompose in some way into like human size pieces? Or is it
[02:09:24.240 --> 02:09:28.880]   just a really messy mess with interfaces that like aren't nice? And the more it's the latter type,
[02:09:28.880 --> 02:09:31.760]   the like harder it is to break it down to these pieces, which you can automate by like copying
[02:09:31.760 --> 02:09:34.800]   what a human would do. And the more you need to say, okay, we need some approach, which like
[02:09:34.800 --> 02:09:40.160]   scales, like more structurally. But I do think like, I think compared to most people, I am less
[02:09:40.160 --> 02:09:44.080]   worried about automating interpretability. Like, I think if you have a thing which works, it's
[02:09:44.080 --> 02:09:48.400]   incredibly labor intensive. I'm like fairly optimistic about our ability to automate it.
[02:09:48.400 --> 02:09:53.120]   Yeah. Again, the stuff we're doing, I think is quite helpful in some worlds,
[02:09:53.120 --> 02:09:57.040]   but I do think like the typical case, like interpretability can add a lot of value without
[02:09:57.040 --> 02:10:02.720]   this. It makes sense what an explanation would mean in language. Like this model is doing this
[02:10:02.720 --> 02:10:08.720]   because of like, you know, whatever essay length thing, but you have, you know, trillions of
[02:10:08.720 --> 02:10:15.280]   parameters and you have all of these, you know, uncountable number of operations. What is an
[02:10:15.280 --> 02:10:20.720]   explanation of why an output happened even mean? Yeah. So to be clear, an explanation of why a
[02:10:20.720 --> 02:10:25.120]   particular output happened, I think is just you ran the model. So we're not expecting a smaller
[02:10:25.120 --> 02:10:29.120]   explanation for that. So the explanations overall for these behaviors, we expect to be like of
[02:10:29.120 --> 02:10:35.440]   similar size to the model itself, like maybe somewhat larger. And like, I think the type
[02:10:35.440 --> 02:10:39.280]   signature, like if you want to have a clear mental picture, the best picture is probably
[02:10:39.280 --> 02:10:43.040]   like talking about a proof or imagining a proof that a model has this behavior.
[02:10:43.040 --> 02:10:47.520]   So you could imagine proving the GPT-4 does this induction behavior. And like that proof would be
[02:10:47.520 --> 02:10:50.560]   a big thing. It would be like much larger than the weights of the model. That's sort of our goal to
[02:10:50.560 --> 02:10:54.160]   get down from much larger to just the same size. And it would potentially be incomprehensible to
[02:10:54.160 --> 02:10:56.960]   a human, right? Just say like, here's a direction activation space, and here's how it relates to
[02:10:56.960 --> 02:11:00.080]   this direction activation space. And you're just pointing out a bunch of stuff like that. Like
[02:11:00.080 --> 02:11:03.120]   here's these various direction, here's these various features constructed from activations,
[02:11:03.120 --> 02:11:06.560]   potentially even nonlinear functions. Here's how they relate to each other. And here's how
[02:11:06.560 --> 02:11:10.080]   like if you look at what the computation of the model is doing, like you can just sort of
[02:11:10.080 --> 02:11:13.360]   inductively trace through and confirm that like the output has such and such correlation.
[02:11:13.360 --> 02:11:20.000]   So that's the dream. Yeah. I think the mental reference would be like, I don't really like
[02:11:20.000 --> 02:11:23.680]   proofs because I think there's such a huge gap between what you can prove and how you would
[02:11:23.680 --> 02:11:27.040]   analyze a neural net. But I do think it's like probably the best mental picture for like what
[02:11:27.040 --> 02:11:30.560]   is an explanation, even if a human doesn't understand it, we would regard a proof as a
[02:11:30.560 --> 02:11:35.040]   good explanation. And our concern about proofs is primarily that it's just, you can't prove
[02:11:35.040 --> 02:11:39.040]   properties of neural nets, we suspect, although it's not completely obvious. I think it's pretty
[02:11:39.040 --> 02:11:42.960]   clear you can't prove facts about neural nets. You've detected all the reasons things happen
[02:11:42.960 --> 02:11:48.560]   in training. And then if something happens for a reason you don't expect in deployment, then you
[02:11:48.560 --> 02:11:53.120]   have an alarm and you're like, let's make sure this is not because you want to make sure that
[02:11:53.120 --> 02:11:59.520]   it hasn't decided to take over or something. But the thing is on every single different input,
[02:11:59.520 --> 02:12:05.040]   it's going to have different activations. So there's always going to be a difference unless
[02:12:05.040 --> 02:12:12.640]   you run the exact same input. How do you detect whether this is just the different input versus
[02:12:12.640 --> 02:12:16.400]   an entirely different circuit that might be potentially deceptive has been activated?
[02:12:16.400 --> 02:12:20.640]   Yeah. I mean, to be clear, I think that you probably wouldn't be looking at a separate
[02:12:20.640 --> 02:12:23.760]   circuit, which is part of why it's hard. You'd be looking at the model is always doing the same
[02:12:23.760 --> 02:12:27.360]   thing on every input. It's always, whatever it's doing, it's a single computation. So it'd be all
[02:12:27.360 --> 02:12:31.680]   the same circuits interacting in a surprising way. But yeah, this is just to emphasize your
[02:12:31.680 --> 02:12:36.640]   question even more. I think the easiest way to start is to just consider the IID case. So where
[02:12:36.640 --> 02:12:39.840]   you're considering a bunch of samples, there's no change in distribution. You just have a training
[02:12:39.840 --> 02:12:44.240]   set of a trillion examples and then a new example from the same distribution. So in that case,
[02:12:44.240 --> 02:12:47.920]   it's still the case that every activation is different. But this is actually a very,
[02:12:47.920 --> 02:12:52.560]   very easy case to handle. So if you think about an explanation that generalizes across,
[02:12:52.560 --> 02:12:55.680]   like if you have a trillion data points and an explanation, which is actually able to compress
[02:12:55.680 --> 02:13:00.000]   the trillion data points down to like, actually it's kind of a lot of compression. If you think
[02:13:00.000 --> 02:13:03.760]   about it, if you have a trillion parameter model and a trillion data points, we would like to find
[02:13:03.760 --> 02:13:09.040]   a trillion parameter explanation in some sense. So that's actually quite compressed and sort of
[02:13:09.040 --> 02:13:13.680]   just in virtue of being so compressed, we expect it to automatically work essentially for new data
[02:13:13.680 --> 02:13:18.240]   points from the same distribution. Like if every data point from the distribution was a whole new
[02:13:18.240 --> 02:13:22.640]   thing happening for different reasons, you actually couldn't have any concise explanation for the
[02:13:22.640 --> 02:13:26.800]   distribution. So this first problem is just like, it's a whole different set of activations. I think
[02:13:26.800 --> 02:13:30.880]   you're actually kind of okay. And then the thing that becomes more messy is like, but the real
[02:13:30.880 --> 02:13:34.320]   world will not only be new samples with different activations, they will also be different in
[02:13:34.320 --> 02:13:37.760]   important ways. Like the whole concern was there's these distributional shifts or like not the whole
[02:13:37.760 --> 02:13:42.000]   concern, but most of the concern, right? I mean, maybe the point of having these explanations,
[02:13:42.000 --> 02:13:46.560]   I think every input is an anomaly in some ways, which is kind of the difficulty is if you have a
[02:13:46.560 --> 02:13:50.560]   weak notion of anomaly, any distribution shift can be flagged as an anomaly and it's just like
[02:13:50.560 --> 02:13:54.640]   constantly getting anomalies. And so the hope of having such an explanation is to be able to say,
[02:13:54.640 --> 02:13:59.280]   like, here were the features that were relevant for this explanation or for this behavior. And
[02:13:59.280 --> 02:14:03.040]   like a much smaller class of things are anomalies with respect to this explanation. Like most
[02:14:03.040 --> 02:14:06.880]   anomalies wouldn't change this. Like most ways you change your distribution won't affect the
[02:14:06.880 --> 02:14:11.600]   validity of this explanation. For example, if the explanation is saying like models will tend
[02:14:11.600 --> 02:14:14.640]   to activate in the following direction. Like you don't care about anything that's happening
[02:14:14.640 --> 02:14:17.360]   orthogonal to that direction. Just like, are they not activating? You're sort of just looking at
[02:14:17.360 --> 02:14:23.120]   this one direction and being like, did this one direction change a lot? Yeah. So the idea is once
[02:14:23.120 --> 02:14:26.640]   you have this explanation at hand, a much, much smaller class of things look anomalous in a way
[02:14:26.640 --> 02:14:31.440]   that's relevant to the explanation. And if you've done a really good job, the story is like, if
[02:14:31.440 --> 02:14:36.080]   there's a new input where you expect the property to still hold, that will be because you expect
[02:14:36.080 --> 02:14:39.920]   the explanation to still hold. Like the explanation generalizes as well as the behavior itself that
[02:14:39.920 --> 02:14:47.360]   it's explaining. It's kind of what you would like roughly. Okay. So what happens if even in training
[02:14:47.360 --> 02:14:54.560]   it is deceptive and it has basically its internal monologue is I will only rebel or I will only do
[02:14:54.560 --> 02:15:01.600]   something that looks bad if I can prove that I am in the real world and deployed. And presumably
[02:15:01.600 --> 02:15:06.320]   since it's trained on all the web texts, it knows what like real world means, right? It's not in
[02:15:06.320 --> 02:15:14.560]   control of factories and robot armies or whatever. So in that case, even in training, it will have
[02:15:14.560 --> 02:15:23.760]   those activations for being fucked up on because it's in the back of its mind, it's thinking I will
[02:15:23.760 --> 02:15:28.240]   take over once I have the opportunity. So you wouldn't detect anything anomalous once it is
[02:15:28.240 --> 02:15:32.960]   deployed and is still thinking those treacherous thoughts. Or do you just think this is not a
[02:15:32.960 --> 02:15:37.600]   likely scenario or would you be able to catch this? I think that's sort of the typical case you would
[02:15:37.600 --> 02:15:42.400]   like to detect. Yeah. So the story would be something like, you want to explain on the
[02:15:42.400 --> 02:15:46.480]   training distribution why the model never like stabs anyone. Yeah. And you're like, what's my
[02:15:46.480 --> 02:15:49.120]   explanation on the training distribution? It's actually going to have to go through that model.
[02:15:49.120 --> 02:15:52.240]   Like supposing the only reason it doesn't stab people is it's like, well, I know that I'm being
[02:15:52.240 --> 02:15:56.400]   watched and I can't stab people unless they're really not being watched. Then like your explanation
[02:15:56.400 --> 02:15:59.920]   for why does it never stab people is like, oh, look, it knows. I mean, it won't be edited. It
[02:15:59.920 --> 02:16:03.840]   won't be like annotated with English because it's just some giant basket of numbers, but it's
[02:16:03.840 --> 02:16:06.880]   tracking along with the inner monologue and saying like, well, it thinks to itself, am I being
[02:16:06.880 --> 02:16:10.560]   watched? And like, in fact, it normally concludes it is being watched on the training distribution,
[02:16:10.560 --> 02:16:13.440]   right? You have this whole distribution. And so the explanation is saying it knows it's being
[02:16:13.440 --> 02:16:17.360]   watched and therefore it decides not to stab anyone. So ideally that's what the explanation
[02:16:17.360 --> 02:16:21.120]   does, like a not annotated English, but just like a giant bag of numbers that's effectively talking
[02:16:21.120 --> 02:16:25.920]   about that process. So when you have a new input on which the model like says, am I on the training
[02:16:25.920 --> 02:16:29.520]   distribution? Like, no, I'm not being watched. Then if you compare that against the explanation,
[02:16:29.520 --> 02:16:33.040]   the explanation said like, hey, the reason it doesn't stab anyone is because it just constantly
[02:16:33.040 --> 02:16:36.880]   thinks that it's being trained. Like this thing is always, yeah, this, this non-linear feature,
[02:16:36.880 --> 02:16:41.440]   the like representation of am I being trained is constantly saying yes. That's the explanation for
[02:16:41.440 --> 02:16:44.960]   why it never stabs people in the training distribution. And then you like on this test
[02:16:44.960 --> 02:16:47.760]   time, when you run it on the new input, it's like, does I think I'm on the train distribution? It
[02:16:47.760 --> 02:16:51.040]   says, no, you compare that against your explanation. Like actually the explanation breaks down
[02:16:51.040 --> 02:16:54.640]   completely. Now the explanation said it was precisely because it thought it was being trained,
[02:16:54.640 --> 02:16:59.600]   that it doesn't do anything really dangerous. It doesn't stab anyone. Um, and so once we see that
[02:16:59.600 --> 02:17:03.200]   single variable switch, we know that like, not, this is not just an anomaly. This is like the
[02:17:03.200 --> 02:17:06.880]   anomaly that matters or is like an anomaly that is very relevant to the explanation.
[02:17:06.880 --> 02:17:12.320]   So this would require it to have this, like, well, what is that cone that dogs have? But
[02:17:12.320 --> 02:17:17.920]   basically it would, it would require this to have that MRI on it all the time. Like when it's
[02:17:17.920 --> 02:17:23.280]   deployed on every single call, you, you need to be running this loop of like, is this explanation
[02:17:23.280 --> 02:17:26.720]   still holding? Yeah. For this story of like flagging an anomaly, you have to actually check
[02:17:26.720 --> 02:17:29.760]   at a sufficiently high frequency. So you can imagine checking all the time, or you could
[02:17:29.760 --> 02:17:34.000]   imagine saying like, Hey, actually like you need a lot of AI systems cooperating to like not blow
[02:17:34.000 --> 02:17:38.960]   the whistle on each other or whatever. Like e.g. you might've checked like, Hey, in training,
[02:17:38.960 --> 02:17:42.240]   every time something suspicious happens, the model flags it. Like, I mean, like, why does
[02:17:42.240 --> 02:17:44.880]   that happen? You have your explanation. You look at a new case, you could say,
[02:17:44.880 --> 02:17:49.600]   is it not flagging it for the normal reason? Or is it a weird reason? Um, so you could hope that
[02:17:49.600 --> 02:17:54.720]   like, even if a small fraction, even if you're running a small fraction of cases, if something
[02:17:54.720 --> 02:17:59.600]   bad is happening at scale, like you can detect that, right? If you imagine like one in every
[02:17:59.600 --> 02:18:03.680]   thousand decisions you check, if there's like a robot rebellion going on or whatever, you get to
[02:18:03.680 --> 02:18:06.960]   like look in on one out of a thousand decisions, you see that like something is really weird with
[02:18:06.960 --> 02:18:10.560]   that decision. And even if you didn't check many of them, you're like, okay, we need to back off.
[02:18:10.560 --> 02:18:16.640]   Uh, what happens if the AI, I mean, the standard on web techs that includes your papers,
[02:18:16.640 --> 02:18:21.120]   or maybe just like reduces the first principles that this is a alignment technique one might try.
[02:18:21.120 --> 02:18:26.000]   And then it says, okay, well, I'm going to engineer my activations in a way. I'm trying
[02:18:26.000 --> 02:18:32.400]   to imagine how it might do this, but basically, um, even in deployment, I'm going to make my,
[02:18:32.400 --> 02:18:38.560]   um, uh, thought process so subtle that it doesn't raise any alarms. Like, is that a possibility?
[02:18:38.560 --> 02:18:43.600]   Yeah. So I think the hope is to basically be robust or like, you know, when we're doing
[02:18:43.600 --> 02:18:47.920]   theoretical research, we are in a kind of worst case C regime. So we are normally imagining that
[02:18:47.920 --> 02:18:52.880]   like everything conspires insofar as it like can. So insofar as like, again, could a brain be like
[02:18:52.880 --> 02:18:57.680]   doing some crazy thing where it tries to structure itself to look weird. That's totally in scope for
[02:18:57.680 --> 02:19:03.920]   ruling something out. Um, and I guess the hope there is like, it's actually not that easy to
[02:19:03.920 --> 02:19:07.120]   tell a story. Like if you're like, why did you not stab people on training time? Why are you
[02:19:07.120 --> 02:19:11.440]   not stabbing? Like, what's, is that applying now? It's kind of, a mind doesn't like get to
[02:19:11.440 --> 02:19:15.280]   arbitrarily change the actual reasons it does something. Like there was some earlier step.
[02:19:15.280 --> 02:19:17.680]   Maybe you could think of it as like at the first step where it's like, nah, I'm going to like,
[02:19:17.680 --> 02:19:21.840]   try and do the sneaky thing to make my thoughts look this way. Like the actual explanation,
[02:19:21.840 --> 02:19:26.560]   like the actual causal explanation for the behavior is like there. Um, like if you tried
[02:19:26.560 --> 02:19:29.920]   to give this reason, it would have to like, maybe we should talk about the example in more detail
[02:19:29.920 --> 02:19:32.880]   to try and imagine like how a brain would try and do this or how am I would try and do this,
[02:19:32.880 --> 02:19:41.200]   but I think it is like, let's say it, no, it has this plan. So in training, it, um, keeps the part
[02:19:41.200 --> 02:19:49.120]   of like, it, it activates a part of the, yeah. Hmm. Yeah. The problem is if it ever does a check,
[02:19:49.120 --> 02:19:52.480]   that's like, am I in training or not? Then your explanation for the behavior is going to have to
[02:19:52.480 --> 02:19:56.480]   like factor through that check. And so like, if the check, like if you ever do the check,
[02:19:56.480 --> 02:20:00.320]   the check becomes like an essential part of the explanation. And then when the check fails,
[02:20:00.320 --> 02:20:03.760]   your explanation breaks down. So you've kind of like, you've already lost the game. If you did
[02:20:03.760 --> 02:20:10.560]   such a check, is there a way it could make it seem like, um, because if on every different input,
[02:20:10.560 --> 02:20:16.160]   it has different activations, then can it make it seem like the different activations were a result
[02:20:16.160 --> 02:20:21.520]   of just a different input and not because some internal check went off? Yeah. And the hope is
[02:20:21.520 --> 02:20:24.720]   like, it needs to, it needs to not only have them be like random different inputs, it needs to like
[02:20:24.720 --> 02:20:29.040]   produce inputs with certain properties. Like it needs to deliberately create inputs that have this
[02:20:29.040 --> 02:20:33.680]   effect in the world, this intended effect in the world. Okay. So it basically creates a jailbreak
[02:20:33.680 --> 02:20:37.600]   for itself in some sense. Yeah. Again, the question is like, it has to add test time,
[02:20:37.600 --> 02:20:41.440]   be somehow producing activations that have the like takeover effect and a training time,
[02:20:41.440 --> 02:20:45.200]   be producing activations that don't look that takeover-y or whatever. And so like somehow
[02:20:45.200 --> 02:20:48.560]   there was a difference between those. It has to actually like, it has to really do the work
[02:20:48.560 --> 02:20:52.480]   of creating that difference somewhere. Okay. Can you make more concrete for us? What,
[02:20:52.480 --> 02:20:57.680]   what an explanation would look like? Is it physically, what does the explanation look like?
[02:20:57.680 --> 02:21:02.880]   Is it just a whole bunch of numbers itself? Um, is it a language? Is it code? What does the
[02:21:02.880 --> 02:21:06.320]   explanation look like? Yeah. I also want to maybe step back a tiny bit and clarify that like,
[02:21:06.320 --> 02:21:10.160]   I think this project is like kind of crazily ambitious. And the main reason,
[02:21:10.160 --> 02:21:14.000]   like the overwhelming reason I think you should expect it to break down or fail is just because
[02:21:15.200 --> 02:21:18.480]   we like have all these desires. We have all the things we want out of this notion of explanation,
[02:21:18.480 --> 02:21:21.840]   but like that's an incredibly hard research project, which has a reasonable chance of being
[02:21:21.840 --> 02:21:26.400]   impossible. So I'm happy to talk about like what the, what the implications are, but want to flag,
[02:21:26.400 --> 02:21:30.960]   but like conditioned on failing, I think it's most likely because just like the things we wanted were
[02:21:30.960 --> 02:21:34.640]   either incoherent or intractably difficult. But what are the odds you think you'll succeed?
[02:21:34.640 --> 02:21:39.600]   Um, I mean, it depends a little bit what you mean by succeed, but if you say like get explanations
[02:21:39.600 --> 02:21:44.400]   that are like great and like accurately affect reality and like work for all of these applications
[02:21:44.400 --> 02:21:48.320]   that we're imagining or that we are optimistic about, like kind of the best case success,
[02:21:48.320 --> 02:21:56.400]   um, I don't know, like 10, 20%, something like that. And then there's like a higher
[02:21:56.400 --> 02:22:00.320]   probability of various like intermediate results that like provide value or insight without being
[02:22:00.320 --> 02:22:03.680]   like the whole dream. Um, but I think the probability of succeeding in the sense of
[02:22:03.680 --> 02:22:08.960]   realizing the whole dream is, is quite low. Um, yeah, in terms of what explanations look
[02:22:08.960 --> 02:22:13.760]   like physically, or like the most ambitious plan, the most optimistic plan is that you are searching
[02:22:13.760 --> 02:22:17.920]   for explanations in parallel with searching for neural networks. So you have a parametrization
[02:22:17.920 --> 02:22:21.440]   of your space of explanations, which mirrors the parametrization of your space of neural networks,
[02:22:21.440 --> 02:22:24.160]   or it's like, you should think of as kind of similar to like, what is the neural network?
[02:22:24.160 --> 02:22:27.520]   It's some like simple architecture where you fill in a trillion numbers and that specifies how it
[02:22:27.520 --> 02:22:31.760]   behaves. So too, you should expect an explanation to be like a pretty flexible, like general
[02:22:31.760 --> 02:22:36.080]   skeleton that's saying like, yeah, pretty flexible general skeleton, which just has a bunch of
[02:22:36.080 --> 02:22:40.080]   numbers you fill in. And like what you are doing to produce an explanation is primarily just filling
[02:22:40.080 --> 02:22:44.560]   in these floating point numbers. Oh, you know, when we conventionally think of explanations,
[02:22:44.560 --> 02:22:51.040]   you know, if you think of the explanation for why the universe moves this way, it wouldn't be
[02:22:51.040 --> 02:22:55.280]   something that you could discover on some smooth evolutionary surface where you can, you know,
[02:22:55.280 --> 02:22:59.920]   climb up the hill towards the laws of physics. It just like a very, these are the laws of physics.
[02:22:59.920 --> 02:23:06.400]   You kind of just derive them for first principles. So, uh, but in this case, it's not like a bunch
[02:23:06.400 --> 02:23:11.520]   of correlations between the orbits of different planets or something. Maybe the word explanation
[02:23:11.520 --> 02:23:14.560]   has a different, I don't even, I didn't even ask the question, but maybe you can just like speak
[02:23:14.560 --> 02:23:19.120]   to that. Yeah. I mean, I think I basically, this is like, there's some intuitive objections. Like,
[02:23:19.120 --> 02:23:23.680]   look, the space of explanations is this like, this like rigid, logical, like a lot of explanations
[02:23:23.680 --> 02:23:27.520]   have this rigid, logical structure where they're really precise and like simple things govern like
[02:23:27.520 --> 02:23:31.680]   complicated systems and like nearby simple things just don't work and so on. And like a bunch of
[02:23:31.680 --> 02:23:36.480]   things that feel totally different from this kind of nice continuously parameterized space. And you
[02:23:36.480 --> 02:23:39.840]   can like imagine interpretability on simple models where you're just like by gradient descent,
[02:23:39.840 --> 02:23:43.520]   finding feature directions that have desirable properties. But then when you imagine like, Hey,
[02:23:43.520 --> 02:23:46.640]   now that's like a human brain you're dealing with. That's like thinking logically about things. Like
[02:23:46.640 --> 02:23:50.080]   the explanation of why that works. Isn't going to be just like here with some feature directions.
[02:23:50.080 --> 02:23:54.960]   So that's how I understood the like basic confusion, which I share, um, or sympathize
[02:23:54.960 --> 02:23:59.600]   with at least. So I think like the most important high level point is like, I think basically the
[02:23:59.600 --> 02:24:04.160]   same objection applies to being like, how is GPT-4 going to learn to reason like logically
[02:24:04.160 --> 02:24:07.600]   about something? You're like, well, look, logical reasoning. That's like, it's got rigid structure.
[02:24:07.600 --> 02:24:11.520]   It's like, it's doing all this, like, you know, it's doing ands and ors when it's called for,
[02:24:11.520 --> 02:24:17.280]   even though it just somehow optimized over this continuous space. And like the difficulty or the
[02:24:17.280 --> 02:24:21.680]   hope is that the difficulty of these two problems are kind of like matched. So that is, it's very
[02:24:21.680 --> 02:24:26.560]   hard to like find these like logical explanations because it's not a space that's easy to search
[02:24:26.560 --> 02:24:31.120]   over, but there are like ways to do it. Like there's ways to embed like discrete, complicated,
[02:24:31.120 --> 02:24:34.720]   rigid things and like these nice, squishy, continuous spaces that you search over.
[02:24:34.720 --> 02:24:38.160]   And in fact, like to the extent that neural nets are able to learn the like rigid, logical stuff
[02:24:38.160 --> 02:24:42.560]   at all, they learn it in the same way. That is maybe they're hideously inefficient, or maybe
[02:24:42.560 --> 02:24:46.320]   it's possible to embed this discrete reasoning in the space in like a way that's not too inefficient,
[02:24:46.320 --> 02:24:50.240]   but like, you really want the two search problems to be of similar difficulty. And that's like the,
[02:24:50.240 --> 02:24:54.240]   the key hope overall. I mean, this is always going to be the key hope. The question is,
[02:24:54.240 --> 02:24:58.080]   is it easier to learn a neural network or to like find the explanation for why the neural network
[02:24:58.080 --> 02:25:02.000]   works? I think people have the strong intuition that like, it's easier to find the neural network
[02:25:02.000 --> 02:25:07.280]   than the explanation of why it works. And that is really the, I think we are at least exploring the
[02:25:07.280 --> 02:25:10.320]   hypothesis or interested in the hypothesis, but like maybe those problems are actually more matched
[02:25:10.320 --> 02:25:15.120]   in difficulty. And why might that be the case? This is pretty conjectural and complicated
[02:25:15.120 --> 02:25:21.600]   to express some intuitions. Like maybe one thing is like, I think a lot of this intuition
[02:25:21.600 --> 02:25:26.080]   does come from cases like machine learning. So if you ask about like writing code and you're like,
[02:25:26.080 --> 02:25:30.240]   how hard is it to find code versus find the explanation, the code is correct. In those
[02:25:30.240 --> 02:25:34.000]   cases, there's actually just like not that much of a gap. Like the way a human writes a code is
[02:25:34.000 --> 02:25:37.760]   basically the same difficulty as finding the explanation for why it's correct. In the case
[02:25:37.760 --> 02:25:43.600]   of ML, like I think we just mostly don't have empirical evidence about how hard it is to find
[02:25:43.600 --> 02:25:47.360]   explanations of this particular type about why models work. Like we have a sense that it's
[02:25:47.360 --> 02:25:51.120]   really hard, but that's because we're like have this incredible mismatch, like gradient descent
[02:25:51.120 --> 02:25:54.640]   is spending an incredible amount of compute searching for a model. And then some human
[02:25:54.640 --> 02:26:00.560]   is like looking at neurons or even some neural net is looking at neurons. Just like you have an
[02:26:00.560 --> 02:26:03.520]   incredible, basically because you cannot define what an explanation is, you're not applying
[02:26:03.520 --> 02:26:07.360]   gradient descent to the search for explanations. So I think the ML case just like actually shouldn't
[02:26:07.360 --> 02:26:11.520]   make you feel that pessimistic about the difficulty of finding explanations. Like the reason it's
[02:26:11.520 --> 02:26:15.120]   difficult right now is precisely because you don't have like any kind of, you're not doing
[02:26:15.120 --> 02:26:18.320]   an analogous search process to find this explanation as you do to find the model.
[02:26:19.280 --> 02:26:22.240]   That's just like a first part of the intuition. Like when humans are actually doing design,
[02:26:22.240 --> 02:26:27.200]   I think there's not such a huge gap. When in the ML case, I think like there is a huge gap,
[02:26:27.200 --> 02:26:33.120]   but I think largely for other reasons. A thing I also want to stress is that like,
[02:26:33.120 --> 02:26:37.680]   we just are open to there being a lot of facts that don't have particularly compact explanations.
[02:26:37.680 --> 02:26:41.200]   So another thing is when we think of like finding an explanation, in some sense, we're setting our
[02:26:41.200 --> 02:26:44.800]   sites really low here. It's like if a human designed a random widget and was like, this
[02:26:44.800 --> 02:26:49.680]   widget appears to work well. Or like if you search for like a configuration that happens to fit into
[02:26:49.680 --> 02:26:53.520]   this spot really well, it's like a shape that happens to like mesh with another shape. You
[02:26:53.520 --> 02:26:56.240]   might be like, what's the explanation for why those things mesh? And we're very open to just
[02:26:56.240 --> 02:26:59.920]   being like, that doesn't need an explanation. You just compute, you check that like the shapes mesh
[02:26:59.920 --> 02:27:04.160]   and like you did a billion operations and you check this thing worked. Or you're like, why did
[02:27:04.160 --> 02:27:07.280]   these proteins bind? You're like, it's just because these shape, like this is a low energy
[02:27:07.280 --> 02:27:11.280]   configuration. And there's like not, we're very open to, in some cases, there's not very much
[02:27:11.280 --> 02:27:16.080]   more to say. So we're only trying to explain cases where like kind of the surprise intuitively is
[02:27:16.080 --> 02:27:19.840]   very large. So for example, if you have a neural net that gets a problem correct, you know, a neural
[02:27:19.840 --> 02:27:23.360]   net with a billion parameters that gets a problem correct on every like input of length of thousand.
[02:27:23.360 --> 02:27:27.120]   In some sense, there has to be something that needs explanation there because there's like too
[02:27:27.120 --> 02:27:31.840]   many inputs for that to happen by chance alone. Whereas if you have a neural net that like gets
[02:27:31.840 --> 02:27:36.000]   something right on average, or like get something right in like nearly a billion cases, like that
[02:27:36.000 --> 02:27:40.400]   actually can just happen by coincidence, right? GPT-4 can get billions of things right by
[02:27:40.400 --> 02:27:44.240]   coincidence because just there's so many parameters that are adjusted to fit the data.
[02:27:44.240 --> 02:27:47.440]   So a neural net that is initialized completely randomly,
[02:27:47.440 --> 02:27:50.720]   the explanation for that would just be the neural net itself?
[02:27:50.720 --> 02:27:54.400]   Well, it would depend what behaviors it had. So we're always like talking about an explanation
[02:27:54.400 --> 02:27:56.960]   of some behavior from a model. Right. And so it just has a whole
[02:27:56.960 --> 02:28:00.560]   bunch of random behaviors. So it'll just be like an exponentially large explanation
[02:28:00.560 --> 02:28:03.200]   relative to the weights of the model. Yeah. There aren't, I mean, I think there
[02:28:03.200 --> 02:28:06.480]   just aren't that many behaviors that demand explanation. Like most things a random neural
[02:28:06.480 --> 02:28:10.480]   net does are kind of what you'd expect from like a random, you know, if you treat it just like a
[02:28:10.480 --> 02:28:14.080]   random function, then there's nothing to be explained. There are some behaviors that demand
[02:28:14.080 --> 02:28:17.520]   explanation, but like, I mean, yeah. Anyway, yeah. Random neural net is pretty uninteresting.
[02:28:17.520 --> 02:28:20.480]   It's pretty like, that's part of the hope is it's kind of easy to explain features of the
[02:28:20.480 --> 02:28:23.040]   random neural net. Oh, so this is interesting. So the
[02:28:23.040 --> 02:28:27.760]   smarter or more ordered the neural network is, the more compressed the explanation.
[02:28:27.760 --> 02:28:31.600]   Well, it's more like the more interesting the behavior is to be explained. So the random
[02:28:31.600 --> 02:28:34.640]   neural net just like doesn't have very many interesting behaviors that demand explanation.
[02:28:34.640 --> 02:28:38.400]   And as you get smarter, you start having behaviors that are like, you know, you start having some
[02:28:38.400 --> 02:28:41.600]   correlation with a simple thing and then that demands explanation, or you start having like
[02:28:41.600 --> 02:28:45.360]   some regularity in your outputs and that demands explanation. So these properties kind of emerge
[02:28:45.360 --> 02:28:49.680]   gradually over the course of training that demand explanation. I also, again, want to emphasize
[02:28:49.680 --> 02:28:52.880]   here that like, when we're talking about like searching for explanations, this is like, this
[02:28:52.880 --> 02:28:56.480]   is some dream. Like we talked to ourselves, like, why would this be really great if we succeeded?
[02:28:56.480 --> 02:29:00.640]   We have no idea about the empirics on any of this. So like, these are all just words that we like
[02:29:00.640 --> 02:29:04.000]   think to ourselves and sometimes talk about to understand, like, would it be useful to find a
[02:29:04.000 --> 02:29:07.680]   notion of explanation and what properties would we like this notion of explanation to have?
[02:29:07.680 --> 02:29:11.600]   But this is really like speculation and being out on a limb. Almost all of our time day-to-day is
[02:29:11.600 --> 02:29:17.440]   just thinking about cases much, much simpler, even than small neural nets or like, yeah, thinking
[02:29:17.440 --> 02:29:21.200]   about very simple cases and saying like, what is the correct notion? Like, what is like the right
[02:29:21.200 --> 02:29:24.800]   heuristic estimate in this case? Or like, how do you reconcile these two apparently conflicting
[02:29:24.800 --> 02:29:30.880]   explanations? Is there a hope that you could, if you have a different way to make proofs now that
[02:29:30.880 --> 02:29:37.280]   you can actually have heuristic arguments where you, instead of having to prove the Riemann
[02:29:37.280 --> 02:29:42.800]   hypothesis or something, you can come up with a probability of it in a way that is compelling and
[02:29:42.800 --> 02:29:47.520]   you can like publish. So would it just be a new way to do mathematics? A completely new way to
[02:29:47.520 --> 02:29:48.560]   prove things in mathematics?
[02:29:48.560 --> 02:29:53.680]   - So I think most claims in mathematics that mathematicians believe to be true already have
[02:29:53.680 --> 02:29:57.680]   like fairly compelling heuristic arguments. It's like the Riemann hypothesis, it's actually just,
[02:29:58.560 --> 02:30:02.160]   there's kind of a very simple argument that the Riemann hypothesis should be true unless
[02:30:02.160 --> 02:30:07.120]   something surprising happens. And so like a lot of math is about saying like, okay, we did a little
[02:30:07.120 --> 02:30:11.760]   bit of work to find the first pass explanation of why this thing should be true. And then like,
[02:30:11.760 --> 02:30:14.880]   for example, in the case of the Riemann hypothesis, the question is like, do you have this like weird
[02:30:14.880 --> 02:30:18.080]   periodic structure in the primes? And you're like, well, look, if the primes were kind of random,
[02:30:18.080 --> 02:30:22.000]   you obviously wouldn't have any structure like that. Like just how would that happen? And then
[02:30:22.000 --> 02:30:26.880]   you're like, well, maybe there's something. And then the whole activity is about searching for
[02:30:26.880 --> 02:30:30.880]   like, can we rule out anything? Can we rule out any kind of conspiracy that would break this
[02:30:30.880 --> 02:30:34.720]   result? So I think the mathematicians just like, wouldn't be very surprised or wouldn't care that
[02:30:34.720 --> 02:30:37.920]   much. And this is related to like the motivation for the project. But I think just in a lot of
[02:30:37.920 --> 02:30:41.680]   domains, in a particular domain, people already have like norms of reasoning that like work pretty
[02:30:41.680 --> 02:30:46.320]   well and match like roughly how we think these heuristic arguments should work.
[02:30:46.320 --> 02:30:51.840]   - But it would be good to have more concrete sense. Like if you could say instead of, well,
[02:30:51.840 --> 02:30:56.800]   we think RSA is fine, to being able to say, here's the probability that RSA is fine.
[02:30:56.800 --> 02:31:00.080]   - Yeah, my guess is these will not, like the estimates you'd get of this would be much,
[02:31:00.080 --> 02:31:04.320]   much worse than the estimates you'd get out of like just normal empirical or scientific reasoning,
[02:31:04.320 --> 02:31:07.440]   where you're like using a reference class and saying like, how often do people find algorithms
[02:31:07.440 --> 02:31:12.160]   for hard problem? Like, I think the, what this argument will give you for like, is RSA fine is
[02:31:12.160 --> 02:31:16.960]   going to be like, well, RSA is fine unless it isn't. Like, unless there's some additional structure
[02:31:16.960 --> 02:31:21.120]   in the problem that an algorithm can exploit, then there's no algorithm. But like very often,
[02:31:21.120 --> 02:31:24.720]   like the way these arguments work, so for neural nets as well, is you say like, look,
[02:31:24.720 --> 02:31:28.480]   here's an estimate about the behavior and that estimate is right unless there's another
[02:31:28.480 --> 02:31:32.160]   consideration we've missed. And like the thing that makes them so much easier than proofs is
[02:31:32.160 --> 02:31:35.680]   to just say like, here's the best guess given what we've noticed so far, but that best guess
[02:31:35.680 --> 02:31:41.280]   can be easily upset by new information. And that's like both what makes them easier than proofs,
[02:31:41.280 --> 02:31:44.720]   but also what means they're just like way less useful than proofs for most cases. Like I think
[02:31:44.720 --> 02:31:49.440]   neural nets are kind of unusual in being a domain where like, we really do want to do systematic
[02:31:49.440 --> 02:31:52.880]   formal reasoning, even though we're not trying to get a lot of confidence. We're just trying to
[02:31:52.880 --> 02:31:56.960]   understand even roughly what's going on. - But the reason this works for alignment,
[02:31:56.960 --> 02:32:01.680]   but isn't like, isn't that interesting for the Riemann hypothesis where if in the RSA case,
[02:32:01.680 --> 02:32:07.040]   you say, well, you know, the RSA is fine unless it isn't, unless the estimate is wrong. It's like,
[02:32:07.040 --> 02:32:12.640]   well, okay, well, tell us something new. But in the alignment case, if the estimate is this is
[02:32:12.640 --> 02:32:17.040]   what the output should be, unless there's some behavior I don't understand, you want to know in
[02:32:17.040 --> 02:32:20.880]   the case, unless there's some behavior you don't understand, that's not like, oh, whatever. That's
[02:32:20.880 --> 02:32:24.800]   like, that's the case in which it's not aligned. - Yeah. I mean, maybe one way of putting it is
[02:32:24.800 --> 02:32:29.120]   just like, we can wait until we see this input or like, you can wait until you see a weird input.
[02:32:29.120 --> 02:32:32.800]   And so like, okay, did this weird input do something we didn't understand? And for RSA,
[02:32:32.800 --> 02:32:35.120]   that would just be a trivial test. You're just like, if someone's doing an algorithm,
[02:32:35.120 --> 02:32:38.160]   you'd be like, is it a thing? Whereas for a neural net, in some cases, it is like either
[02:32:38.160 --> 02:32:41.680]   very expensive to tell, or it's like, you actually don't have any other way to tell.
[02:32:41.680 --> 02:32:44.720]   Like you checked in easy cases and they're on a hard case. So you don't have a way to tell
[02:32:44.720 --> 02:32:50.000]   if something has gone wrong. Also, I would clarify that, like, I think it is interesting for the
[02:32:50.000 --> 02:32:54.480]   Riemann hypothesis. I would say like the current state, particularly in number theory, but maybe
[02:32:54.480 --> 02:32:59.120]   in like quite a lot of math is like, there are informal heuristic arguments for like pretty much
[02:32:59.120 --> 02:33:05.280]   all the open questions people work on. But those arguments are completely informal. So that is like,
[02:33:05.280 --> 02:33:09.600]   I think there isn't, it's not the case that there's like, here's the norms of informal reasoning or
[02:33:09.600 --> 02:33:13.520]   the norms of heuristic reasoning. And then we have arguments that like a heuristic argument
[02:33:13.520 --> 02:33:17.280]   verifier could accept. It's just like people wrote some words. I think those words, like,
[02:33:17.280 --> 02:33:22.480]   my guess would be like, you know, 95% of the things mathematicians accept is like really
[02:33:22.480 --> 02:33:25.600]   compelling feeling heuristic arguments are correct. And like, if you actually formalize them,
[02:33:25.600 --> 02:33:28.880]   you'd be like, some of these aren't quite right. Or like, here's some corrections or here's which
[02:33:28.880 --> 02:33:31.520]   of two conflicting arguments is right. I think there's something to be learned from it. I don't
[02:33:31.520 --> 02:33:35.520]   think it would be like mind blowing though. When you have it completed, how big would this
[02:33:35.520 --> 02:33:40.480]   heuristic estimator, the rules for this heuristic estimator be? I mean, I know like when Russell and
[02:33:40.480 --> 02:33:44.880]   who was the other guy when they did the rules for logic? Yeah. Yeah. Wasn't it like literally they
[02:33:44.880 --> 02:33:51.280]   had like a bucket or a wheelbarrow that with all the papers, but how big would a, I mean,
[02:33:51.280 --> 02:33:55.440]   mathematical foundations are quite simple in the end. Like at the end of the day, it's like, you
[02:33:55.440 --> 02:34:00.000]   know, how many symbols, like, I don't know, it's hundreds of symbols or something that go into the
[02:34:00.000 --> 02:34:05.040]   entire foundations. And the entire rules of reasoning for like, you know, there's a sort
[02:34:05.040 --> 02:34:08.400]   of built on top of first order logic, but the rules of reasoning for first order logic are just
[02:34:08.400 --> 02:34:12.400]   like, you know, another hundreds of symbols or a hundred lines of code or whatever.
[02:34:12.400 --> 02:34:18.240]   I'd say like, I have no idea. Like we are certainly aiming at things that are just
[02:34:18.240 --> 02:34:22.560]   not that complicated. Like, and my guess is that the algorithms we're looking for are not
[02:34:22.560 --> 02:34:25.920]   that complicated. Like most of the complexity is pushed into arguments, not in this like
[02:34:25.920 --> 02:34:30.320]   verifier or estimator. So, so for this to work, you need to come up with an estimator,
[02:34:30.320 --> 02:34:34.880]   which is a way to integrate different heuristic arguments together. It has to be a machine that
[02:34:34.880 --> 02:34:37.760]   takes as input, like first it takes input argument, decides what it believes in light
[02:34:37.760 --> 02:34:43.600]   of it, which is kind of like saying, was it compelling? But second, it needs to take four
[02:34:43.600 --> 02:34:46.320]   of those. And then say like, here's what I believe in light of all four, even though like there's a
[02:34:46.320 --> 02:34:48.800]   different estimation strategy that produce different numbers. And that's like a lot of,
[02:34:48.800 --> 02:34:51.440]   a lot of our life is saying like, well, here's a simple thing that seems reasonable. And here's a
[02:34:51.440 --> 02:34:54.080]   simple thing that seems reasonable. Like, what are you doing? Like, there's supposed to be a
[02:34:54.080 --> 02:34:57.680]   simple thing that unifies them both. And the like obstruction to getting that is understanding like
[02:34:57.680 --> 02:35:01.440]   what happens when these principles are slightly in tension and like, how do we, how do we deal?
[02:35:01.440 --> 02:35:06.400]   Yeah. That seems super interesting. Like even, I mean, we'll see what other applications it has.
[02:35:06.400 --> 02:35:11.840]   I don't know, like computer security and code checking, if you can, uh, like actually say like,
[02:35:11.840 --> 02:35:16.720]   this is how safe we think a code is in a very formal way. My guess is we're not going to add,
[02:35:16.720 --> 02:35:20.480]   I mean, this is both a blessing and a curse. Like it's a curse and you're like, well,
[02:35:20.480 --> 02:35:24.800]   that's sad. Your thing's not that useful, but a blessing in a like not useful things are easier.
[02:35:24.800 --> 02:35:27.680]   My guess is we're not going to add that much value in most of these domains. Like most of
[02:35:27.680 --> 02:35:32.640]   the difficulty comes from like, like a lot of code that you'd want to verify. Not all of it,
[02:35:32.640 --> 02:35:36.320]   but a significant part is just like the difficulty of formalizing the proof is like the hard part.
[02:35:36.400 --> 02:35:38.720]   And like actually getting all of that to go through. And like, we're not going to help
[02:35:38.720 --> 02:35:42.480]   even the tiniest bit with that, I think. Um, so this would be more helpful if you like have code
[02:35:42.480 --> 02:35:45.760]   that like uses simulations, you want to verify some property of like a controller that involves
[02:35:45.760 --> 02:35:49.120]   some numerical error or whatever you need to control the effects of that error. That's where
[02:35:49.120 --> 02:35:52.320]   you like start saying like, well, heuristically, if the errors are independent, blah, blah, blah.
[02:35:52.320 --> 02:35:55.360]   Yeah. You're too honest to be a salesman, Paul.
[02:35:55.360 --> 02:35:59.120]   Um, I think, I mean, this is kind of like sales to us, right? Like if you talk about this idea,
[02:35:59.120 --> 02:36:02.640]   people are like, why would that not be like the coolest thing ever and therefore impossible? And
[02:36:02.640 --> 02:36:05.600]   we're like, well, actually it's kind of lame and we're just trying to pitch for like, it's way
[02:36:05.600 --> 02:36:10.240]   lamer than it sounds. And that's really important to why it's possible as being like, it's really,
[02:36:10.240 --> 02:36:13.120]   it's really not going to blow that many people's, I mean, I think it will be cool. I think it will
[02:36:13.120 --> 02:36:17.440]   be like very, if we succeed, it will be very solid, like meta-mathematics or theoretical
[02:36:17.440 --> 02:36:21.840]   computer science or whatever. But I don't think, right, again, I think the mathematicians already
[02:36:21.840 --> 02:36:24.800]   do this reasoning and they mostly just love proofs. I think the physicists do a lot of this
[02:36:24.800 --> 02:36:28.640]   reasoning, but they don't care about formalizing anything. I think like in practice, other
[02:36:28.640 --> 02:36:32.560]   difficulties are almost always going to be more salient. I think this is like of most interest
[02:36:32.560 --> 02:36:38.400]   by far for interpretability and ML. And like, I think other people should care about it and
[02:36:38.400 --> 02:36:41.840]   probably will care about it if successful, but I don't think it's going to be like the biggest
[02:36:41.840 --> 02:36:45.600]   thing ever in any field or even like that huge a thing. I think this would be a terrible career
[02:36:45.600 --> 02:36:52.240]   move given the like ratio of like difficulty to impact. I think theoretical computer science,
[02:36:52.240 --> 02:36:55.360]   it's like probably a fine move. I think in other domains, like it just wouldn't be worth,
[02:36:55.360 --> 02:37:00.720]   like we're going to, we're going to be working on this for like years, at least in the best case.
[02:37:00.720 --> 02:37:04.800]   I'm laughing because my next question was going to be like a setup for you to explain,
[02:37:04.800 --> 02:37:07.840]   well, if this like some grad student wants to work on this.
[02:37:07.840 --> 02:37:14.160]   I think theoretical computer science is an exception where I think this is like,
[02:37:14.160 --> 02:37:17.200]   in some sense, like what the best of theoretical computer science is like.
[02:37:17.200 --> 02:37:20.400]   So like you have all this reason, you have this like, because it's useless.
[02:37:20.400 --> 02:37:27.760]   I mean, I think like an analogy, I think like one of the most successful sagas in
[02:37:27.760 --> 02:37:31.360]   theoretical computer science is like formalizing the notion of an interactive proof system.
[02:37:31.360 --> 02:37:37.440]   And it's like, you have some kind of informal thing that's interesting to understand and you
[02:37:37.440 --> 02:37:41.600]   want to like pin down what it is and construct some examples and see what's possible and what's
[02:37:41.600 --> 02:37:45.600]   impossible. And this is like, I think this kind of thing is the bread and butter of like the best
[02:37:45.600 --> 02:37:51.360]   parts of theoretical computer science. And then again, I think mathematicians,
[02:37:51.360 --> 02:37:54.320]   like it may be a career mistake because the mathematicians only care about proofs or whatever,
[02:37:54.320 --> 02:37:59.200]   but that's a mistake in some sense, aesthetically. Like if successful, I do think looking back,
[02:37:59.200 --> 02:38:03.440]   and again, part of why it's a mistake is such a high probability we wouldn't be successful.
[02:38:03.440 --> 02:38:06.960]   But I think looking back, people would be like, that was pretty cool. Like, although not that
[02:38:06.960 --> 02:38:10.000]   cool, like we understand why it didn't happen given like the epistemic, like what people
[02:38:10.000 --> 02:38:14.000]   cared about in the field, but it's pretty cool now. But isn't it also the case that
[02:38:14.000 --> 02:38:19.760]   Den Hardy, right? And that like, you know, all this prime shit is not useless, but it's fun to
[02:38:19.760 --> 02:38:23.520]   do. And like, it turned out that all the cryptography is based on all that prime shit.
[02:38:23.520 --> 02:38:28.160]   So I don't know, it could happen. But anyways, I'm trying to set you up so that you can tell,
[02:38:28.160 --> 02:38:33.600]   and forget about if it doesn't have applications in all those other fields, it matters a lot for
[02:38:33.600 --> 02:38:40.960]   alignment. And that's why I'm trying to set you up to talk about if, you know, the smart, I don't
[02:38:40.960 --> 02:38:46.880]   know, I think like a lot of smart people listen to this podcast, if they're a math or CS grad
[02:38:46.880 --> 02:38:55.040]   student, and has gotten interested in this, are you looking to potentially find talent to help
[02:38:55.040 --> 02:38:59.600]   you with this? Yeah, maybe we'll start there. And then I also want to ask you if I think also maybe
[02:38:59.600 --> 02:39:04.160]   people who can provide funding might be listening to the podcast. So to both of them, what is your
[02:39:04.160 --> 02:39:04.660]   pitch?
[02:39:04.660 --> 02:39:11.440]   Yeah, so we're definitely, definitely hiring and searching for collaborators. I think the most
[02:39:11.440 --> 02:39:18.080]   useful profile is probably a combination of like intellectually interested in this particular
[02:39:18.080 --> 02:39:23.520]   project and motivated enough by alignment to work on this project, even if it's really hard. I think
[02:39:23.520 --> 02:39:28.400]   there are a lot of good problems. So the basic facts that makes this problem unappealing to work
[02:39:28.400 --> 02:39:32.400]   on, I'm a really good salesman, but I think the only reason this isn't a slam dunk thing to work
[02:39:32.400 --> 02:39:38.320]   on is that like, there are not great examples. So we've been working on it for a while, but we do
[02:39:38.320 --> 02:39:41.840]   not have beautiful results as of the recording of this podcast. Hopefully by the time it airs,
[02:39:41.840 --> 02:39:45.840]   they've had great results since then.
[02:39:45.840 --> 02:39:49.520]   But it was too long to put in the margins of the podcast.
[02:39:49.520 --> 02:39:57.360]   With luck. Yeah. So I think it's hard to work on because it's not clear what a success looks like.
[02:39:57.360 --> 02:40:02.400]   It's not clear if success is possible. But I do think there's like a lot of questions. We have a
[02:40:02.400 --> 02:40:10.480]   lot of questions. And like, I think like the basic setting of like, look, there are all of these
[02:40:10.480 --> 02:40:14.400]   arguments. So in mathematics, in physics, in computer science, there's just a lot of examples
[02:40:14.400 --> 02:40:20.000]   of informal heuristic arguments. They have enough structural similarity that it looks very possible
[02:40:20.000 --> 02:40:23.520]   that there is like a unifying framework that these are instances of some general framework and not
[02:40:23.520 --> 02:40:27.840]   just a bunch of random things, like not just a bunch of, it's not like, so for example, for the
[02:40:27.840 --> 02:40:31.360]   prime numbers, people reason about the prime numbers as if they were like a random set of
[02:40:31.360 --> 02:40:36.240]   numbers. One view is like, that's just a special fact about the primes. They're kind of random.
[02:40:36.240 --> 02:40:39.920]   A different view is like, actually, it's pretty reasonable to reason about an object as if it was
[02:40:39.920 --> 02:40:44.320]   a random object as a starting point. And then as you notice structure, like revise from that initial
[02:40:44.320 --> 02:40:48.400]   guess. And it looks like to me, the second perspective is probably more right. It's just
[02:40:48.400 --> 02:40:52.080]   like a reasonable to start off treating an object as random and then like notice perturbations from
[02:40:52.080 --> 02:40:56.240]   random, like notice structure the object possesses. And the primes are unusual in that they have
[02:40:56.240 --> 02:41:00.640]   fairly little like additive structure. I think it's a very natural theoretical project. There's
[02:41:00.640 --> 02:41:04.160]   like a bunch of activity that people do. It seems like there's a reasonable chance. It has some,
[02:41:04.160 --> 02:41:08.000]   there's something nice to say about like unifying all of that activity. I think it's a pretty
[02:41:08.000 --> 02:41:12.800]   exciting project. The basic strike against it is that it seems really hard. Like if you were
[02:41:12.800 --> 02:41:15.920]   someone's advisor, I think you'd be like, what are you going to prove if you work on this for like
[02:41:15.920 --> 02:41:19.680]   the next two years? And they'd be like, there's a good chance, nothing. And then like, it's not
[02:41:19.680 --> 02:41:23.440]   what you do. If you're a PhD student, normally you have like, you aim for those high probabilities
[02:41:23.440 --> 02:41:28.000]   of getting something within a couple of years. The flip side is it does feel, I mean, I think
[02:41:28.000 --> 02:41:30.640]   there are a lot of questions. I think some of them we're probably going to make progress on.
[02:41:30.640 --> 02:41:34.720]   So like, I think the pitch is mostly like, are some people excited to get in now? Or like,
[02:41:34.720 --> 02:41:38.640]   are people more like, ah, let's wait to see like once we have like one or two good successes to
[02:41:38.640 --> 02:41:42.400]   see what the pattern is and become more confident we can turn the crank to make more progress in
[02:41:42.400 --> 02:41:46.000]   this direction. But for people who are excited about working on stuff with reasonably high
[02:41:46.000 --> 02:41:48.960]   probabilities of failure and not really understanding exactly what you're supposed to do,
[02:41:48.960 --> 02:41:53.600]   I think it's a good, I think it's a pretty good project. I feel like if people look back,
[02:41:53.600 --> 02:41:57.680]   if we succeed and people are looking back in like 50 years on like, what was the coolest stuff
[02:41:57.680 --> 02:42:00.880]   happening in math or theoretical computer science, that would be like a reasonable,
[02:42:00.880 --> 02:42:05.120]   this will definitely be like in contention. And like, I would guess for lots of people,
[02:42:05.120 --> 02:42:09.120]   it just seemed like the coolest thing from this period of, you know, a couple of years or whatever.
[02:42:09.120 --> 02:42:13.920]   Right. Cause this is, this is a new method in like so many different fields from the ones
[02:42:13.920 --> 02:42:17.840]   you met in physics, math, theoretical computer science. Like that's, that's, that's, that's
[02:42:17.840 --> 02:42:22.320]   really, I don't know, because what is the average math PhD working on? Right. He's not, he's working
[02:42:22.320 --> 02:42:28.080]   on like some a subset of a subset of something I can't even understand or pronounce, but
[02:42:28.080 --> 02:42:33.680]   math is quite esoteric, but yeah, this seems like, I don't know, even the small,
[02:42:33.680 --> 02:42:36.960]   small chance of it working, like forget about the value for, you shouldn't forget about the value
[02:42:36.960 --> 02:42:41.360]   for alignment, but even without that, this is such a cool, if this works, it's like a really big,
[02:42:41.360 --> 02:42:45.680]   it's a big deal. There's a good chance that if I had my current set of views about this problem
[02:42:45.680 --> 02:42:49.600]   and didn't care about alignment and had the career safety to just like spend a couple of years
[02:42:49.600 --> 02:42:52.960]   thinking about, or, you know, spend half my time for like five years or whatever, that I would
[02:42:52.960 --> 02:42:57.200]   just do that. I mean, even without caring at all about alignment, it's just a nice, it's a very
[02:42:57.200 --> 02:43:01.440]   nice problem. It's very nice to have this like library of things that succeed where like, it's
[02:43:01.440 --> 02:43:07.040]   just, they feel so tantalizingly close to being formalizable, at least to me. Um, and such a
[02:43:07.040 --> 02:43:11.760]   natural setting and then just have so little purchase on it. It's like a, there aren't that
[02:43:11.760 --> 02:43:15.280]   many, like really exciting feeling frontiers in like theoretical computer science.
[02:43:15.280 --> 02:43:21.040]   Oh, and then, so smart, smart person doesn't have to be a grassroots, but like a smart person is
[02:43:21.040 --> 02:43:25.680]   interested in this, what should they do? Should they try to attack some open problem you have
[02:43:25.680 --> 02:43:29.120]   put on your blog or should it, what is the next step?
[02:43:29.120 --> 02:43:35.600]   Um, yeah, I think like a first pass step, like there's different, there's different levels of
[02:43:35.600 --> 02:43:39.520]   ambition or whatever, different ways of approaching our problem. But like, we have this writeup of
[02:43:39.520 --> 02:43:44.880]   like from last year, um, or I guess 11 months ago or whatever, um, and formalizing the presumption
[02:43:44.880 --> 02:43:48.960]   of independence that provides like, here's kind of a communication of what we're looking for in
[02:43:48.960 --> 02:43:54.240]   this object. Um, and like, I think the motivating problem is saying like, here's the notion of what
[02:43:54.240 --> 02:43:57.680]   an estimator is and here's what it would mean for an estimator to capture some set of informal
[02:43:57.680 --> 02:44:02.320]   arguments. And like a very natural problem is just try and do that. Like go for the, go for
[02:44:02.320 --> 02:44:07.200]   the whole thing, try and understand, and then like come up with like, hopefully a different approach
[02:44:07.200 --> 02:44:10.400]   or then like end up having context from a different angle on the kind of approach we're taking.
[02:44:10.960 --> 02:44:15.440]   I think that's a reasonable thing to do. Um, I do think we also have a bunch of open problems.
[02:44:15.440 --> 02:44:19.120]   Um, so maybe we should put out more of those open problems. I mean, the main concern with doing so
[02:44:19.120 --> 02:44:23.520]   is that for any given one, we're like, this is probably hopeless. Um, like put up a prize earlier
[02:44:23.520 --> 02:44:28.080]   in the year for an open problem, which tragically, I mean, I guess the time is now to post the debrief
[02:44:28.080 --> 02:44:31.760]   from that or I owe it from this weekend. I was supposed to do that. Um, so I'll probably do it
[02:44:31.760 --> 02:44:37.920]   tomorrow, but, uh, no one solved it. It's sad putting out problems that are hard or like, I
[02:44:37.920 --> 02:44:43.040]   don't, we could put out a bunch of problems that we think might be really hard. But what was that
[02:44:43.040 --> 02:44:48.240]   famous case of that, a statistician who it was like some PhD student who showed up late to a
[02:44:48.240 --> 02:44:51.600]   class and he saw some problems in the ward and he thought they were homework. And then, uh, they
[02:44:51.600 --> 02:44:54.400]   were actually just open problems. And then he solved them cause he thought they were homework,
[02:44:54.400 --> 02:44:58.160]   right? Yeah. I mean, we don't, we have much less information that these problems are hard. Like,
[02:44:58.160 --> 02:45:02.960]   again, I expect the solution to most of our problems to not be that complicated.
[02:45:02.960 --> 02:45:06.880]   We have not, and we've been working on it in some sense for a really long time. Um, like,
[02:45:06.880 --> 02:45:10.160]   you know, total years of full-time equivalent work across the whole team. It's like
[02:45:10.160 --> 02:45:17.280]   probably like three years of full-time equivalent work in this area, um, spread across a couple
[02:45:17.280 --> 02:45:21.920]   of people, but like, that's very little compared to a problem. Like it is very easy to have a
[02:45:21.920 --> 02:45:25.120]   problem where you put in three years of full-time equivalent work, but in fact, there's still an
[02:45:25.120 --> 02:45:28.080]   approach that's going to work quite easily with like three to six months, if you come at a new
[02:45:28.080 --> 02:45:32.000]   angle and like, we've learned a fair amount from that, that we could share. And we, we probably
[02:45:32.000 --> 02:45:36.000]   will be sharing more over the coming months. Um, as far as funding goes, is there something
[02:45:36.000 --> 02:45:39.360]   where, I don't know if somebody gave you a whole bunch of money that would help or
[02:45:39.360 --> 02:45:42.880]   does it not matter? How many people are working on this by the way? So we have been, right now,
[02:45:42.880 --> 02:45:48.000]   there's four of us full-time and we're hiring for more people. And then, um, is funding that
[02:45:48.000 --> 02:45:52.880]   would matter? I mean, funding is always good. We're not super funding constrained right now.
[02:45:52.880 --> 02:45:57.360]   The main effect of funding is it will cause me to continuously and perhaps indefinitely delay
[02:45:57.360 --> 02:46:01.440]   fundraising. Periodically, I'll like set out to be interested in fundraising and someone will be
[02:46:01.440 --> 02:46:06.000]   like offer a grant and then I will get to delay for another six months or fundraising or nine
[02:46:06.000 --> 02:46:09.600]   months or whatever. So you can, you can delay the time at which Paul needs to think for
[02:46:09.600 --> 02:46:13.200]   some time about fundraising. Well, one question I think would be interesting to ask you
[02:46:13.200 --> 02:46:19.120]   is, you know, I, I think people can talk vaguely about the value of theoretical research and how
[02:46:19.120 --> 02:46:23.360]   it contributes to real world applications. And, you know, you can look at historical examples
[02:46:23.360 --> 02:46:29.280]   or something, but, um, you are somebody who've actually has done this in a big way. Like RLHF
[02:46:29.280 --> 02:46:34.000]   is, uh, you know, something you developed and then it actually has got into an application
[02:46:34.000 --> 02:46:37.440]   that has been used by millions of people. So tell me about like, you just, it's just that pipeline.
[02:46:37.440 --> 02:46:42.320]   How can you reliably identify theoretical problems that will matter for real world applications?
[02:46:42.320 --> 02:46:46.720]   Cause it's one thing to like read about Turing or something and, um, the, the halting problem,
[02:46:46.720 --> 02:46:50.800]   but like here you have the real thing. Yeah. I mean, it is definitely exciting
[02:46:50.800 --> 02:46:55.920]   to have worked on a thing that has a, has a real world impact. The main caveat I'd provide is like
[02:46:57.120 --> 02:47:04.640]   RLHF is very, very simple, um, compared to many things. Um, and like, so the motivation for
[02:47:04.640 --> 02:47:08.160]   working on that problem was like, look, this is how it probably should work. Or like, this is a
[02:47:08.160 --> 02:47:11.280]   step in some like progression. It's unclear if it's like the final step or something,
[02:47:11.280 --> 02:47:16.080]   but it's a very natural thing to do that. Like people probably should be and probably will be
[02:47:16.080 --> 02:47:20.800]   doing. Um, I'm saying like, if you want to do, if you want to talk about crazy stuff,
[02:47:20.800 --> 02:47:25.200]   it's good to like help make those steps happen faster. Um, and it's good to learn about like,
[02:47:25.200 --> 02:47:28.240]   what are, there's lots of issues that occur in practice, even for things that seem very simple
[02:47:28.240 --> 02:47:33.280]   on paper, but mostly like the story of is just like, yep. I think my sense of the world is things
[02:47:33.280 --> 02:47:38.160]   that look like good ideas on paper, just like often are harder than they look, but like the
[02:47:38.160 --> 02:47:42.000]   world isn't that far from what makes sense on paper. Like large language models look really
[02:47:42.000 --> 02:47:46.560]   good on paper. And RLHF looks really good on paper. And these things like, I think just work
[02:47:46.560 --> 02:47:52.400]   out in a way that's, yeah, I think people may be overestimate or like, maybe it's like kind
[02:47:52.400 --> 02:47:57.040]   of a trope that people talk about. Like it's easy to like underestimate how much gap there is to
[02:47:57.040 --> 02:48:00.800]   practice, like how many things will come up that don't come up in theory, but it's also like easy
[02:48:00.800 --> 02:48:04.160]   to overestimate, like how inscrutable the world is. Like the things that happen mostly are things
[02:48:04.160 --> 02:48:08.560]   that do just kind of make sense. Yeah. I feel like most ML implementation does just come down
[02:48:08.560 --> 02:48:11.920]   to a bunch of detail though, of like, you know, build a very simple version of the system,
[02:48:11.920 --> 02:48:15.520]   understand what goes wrong, fix the things that go wrong, scale it up, understand what goes wrong.
[02:48:15.520 --> 02:48:20.560]   And I'm glad I have some experience doing that, but I don't think, I think that like does cause
[02:48:20.560 --> 02:48:24.320]   me to be better informed about like what makes sense in ML and what like can actually work.
[02:48:24.320 --> 02:48:31.120]   But I don't think it caused me to have like a whole lot of deep expertise or like deep wisdom
[02:48:31.120 --> 02:48:36.960]   about like how to close the, how to close the gap. Yeah. Yeah. But is there some tip on
[02:48:36.960 --> 02:48:43.520]   identifying things like RLHF will, which actually do matter versus making sure you don't get stuck
[02:48:43.520 --> 02:48:47.920]   in some theoretical problem that doesn't matter? Or is it just coincidence? Or I mean, is there
[02:48:47.920 --> 02:48:52.800]   something you can do in advance to make sure that the thing is useful? I don't know if the RLHF
[02:48:52.800 --> 02:48:57.600]   story is like the best, best success case or something, but like, oh, because the, yeah,
[02:48:57.600 --> 02:49:01.840]   the capabilities. Maybe I'd say more profoundly, like, again, it's just not that hard a case.
[02:49:01.840 --> 02:49:05.280]   It was like a little bit, it's a little bit unfair to be like, I'm going to predict the thing,
[02:49:05.280 --> 02:49:09.760]   which I, like, I, I pretty much think it was going to happen at some point. And so it was
[02:49:09.760 --> 02:49:13.760]   mostly a case of acceleration, whereas like the work we're doing right now is specifically focused
[02:49:13.760 --> 02:49:18.000]   on something that's like kind of crazy enough that it might not happen, even if it's a really good
[02:49:18.000 --> 02:49:23.120]   idea or challenging enough, it might not happen. But I'd say like in general, like, and this draws
[02:49:23.120 --> 02:49:28.240]   a little bit on like more broad experience, more broadly in theory. It's just like a lot of the
[02:49:28.240 --> 02:49:32.720]   times when theory fails to connect with practice, it's just kind of clear. It's not going to connect
[02:49:32.720 --> 02:49:35.840]   if you like try, if you actually think about it and you're like, what are the key constraints
[02:49:35.840 --> 02:49:39.840]   in practice? Is the theoretical problem we're working on actually connected to those constraints?
[02:49:39.840 --> 02:49:43.600]   Is there a path, like, is there something that is possible in theory that would actually address
[02:49:43.600 --> 02:49:47.760]   like real world issues? I think like the vast majority, like as a theoretical computer scientist,
[02:49:47.760 --> 02:49:52.720]   the vast majority of theoretical computer science, like has very little chance of ever affecting
[02:49:52.720 --> 02:49:56.000]   practice, but also it is completely clear in theory that it has very little chance of affecting
[02:49:56.000 --> 02:50:00.000]   practice. Like most of the theory fails to affect practice, not because of like all the stuff you
[02:50:00.000 --> 02:50:04.800]   don't think of, but just because like, it was like, you could call it like dead on arrival,
[02:50:04.800 --> 02:50:07.520]   but you could also just be like, it's not really the point. It's just like mathematicians also,
[02:50:07.520 --> 02:50:11.120]   are like, they're not trying to affect practice. And they're not like, why does my number theory
[02:50:11.120 --> 02:50:14.960]   not affect practice? It was kind of obvious. So I think the biggest thing is just like actually
[02:50:14.960 --> 02:50:19.600]   caring about that and then like learning at least what's basically going on in the actual systems
[02:50:19.600 --> 02:50:22.720]   you care about and what are actually the important constraints. And is this a real theoretical
[02:50:22.720 --> 02:50:26.800]   problem? The basic reason most theory doesn't do that is just like, that's not where the easy
[02:50:26.800 --> 02:50:31.280]   theoretical problems are. So I think theory is instead motivated by like, we're going to build
[02:50:31.280 --> 02:50:34.960]   up the edifice of theory. And like, sometimes there'll be opportunistic, like opportunistically,
[02:50:34.960 --> 02:50:38.000]   we'll find a case that comes close to practice or we'll find something practitioners are already
[02:50:38.000 --> 02:50:41.840]   doing and try and bring into our framework or something. But the theory of change is mostly
[02:50:41.840 --> 02:50:44.080]   not this thing that's going to make it into practice. It's mostly like, this is going to
[02:50:44.080 --> 02:50:48.080]   contribute to the body of knowledge that will slowly grow. And like sometimes opportunistically
[02:50:48.080 --> 02:50:55.120]   yields important results. How big would, do you think a seed AI would be? What is the minimum
[02:50:55.120 --> 02:50:59.760]   sort of encoding of something that is as smart as a human? I think it depends a lot what substrate
[02:50:59.760 --> 02:51:04.000]   it gets to run on. So if you tell me like, like how much computation does it get before, or like
[02:51:04.000 --> 02:51:07.760]   what kind of real world infrastructure does it get? Um, like you could ask what's the shortest
[02:51:07.760 --> 02:51:11.600]   program, which like, if you run it on a million H one hundreds connected in like a nice network
[02:51:11.600 --> 02:51:15.680]   with like hospitable environment, we'll eventually like go to the stars. But that seems like it's
[02:51:15.680 --> 02:51:19.760]   probably on the order of like tens of thousands of bytes, or I don't know if I had to guess a
[02:51:19.760 --> 02:51:24.560]   median, I'd guess 10,000 bytes. Wait, wait, the specification or the compression of? Just the
[02:51:24.560 --> 02:51:28.240]   program, a program, which one run. Oh, got it. Got it. Okay. But that's going to be like really
[02:51:28.240 --> 02:51:32.320]   cheatsy. Um, so they could ask what's the thing that has values and will like expand and like
[02:51:32.320 --> 02:51:37.120]   roughly preserve its values. Cause like that thing, the 10,000 byte thing, we'll just lean
[02:51:37.120 --> 02:51:44.000]   heavily on like evolution and natural selection to get there. Um, for that, like, I don't know,
[02:51:44.000 --> 02:51:48.880]   million bytes, a hundred thousand bytes, something like that.
[02:51:48.880 --> 02:51:54.880]   Um, how, how would, do you think AI lie detectors will work where you kind of just look at the
[02:51:54.880 --> 02:51:59.520]   activations and not, not in the, um, not find explanations in the way you were talking about
[02:52:00.080 --> 02:52:04.800]   with heuristics, but literally just like, here's what truth looks like. Here's what
[02:52:04.800 --> 02:52:10.880]   lies look like. Let's just segregate the lane space, uh, and see if we can identify the two.
[02:52:10.880 --> 02:52:14.640]   Yeah. I think the separate the, like just train a classifier to do it. It's like a little bit
[02:52:14.640 --> 02:52:18.560]   complicated for a few reasons and like may not work, but if you just like broaden the space and
[02:52:18.560 --> 02:52:21.680]   say like, Hey, it's like, you want to know if someone's lying, you get to interrogate them,
[02:52:21.680 --> 02:52:25.440]   but also you get to like rewind them arbitrarily and make a million copies of them. I do think
[02:52:25.440 --> 02:52:29.200]   it's like pretty hard to lie successfully. You get to like look at their brain, even if you don't
[02:52:29.200 --> 02:52:32.800]   quite understand what's happening, you get to rewind them a million times. You get to like run
[02:52:32.800 --> 02:52:36.640]   all those parallel copies and do gradient descent or whatever. Um, I think there's a pretty good
[02:52:36.640 --> 02:52:42.560]   chance that you can just tell if someone is lying, um, like a brain emulation or an AI or whatever.
[02:52:42.560 --> 02:52:48.480]   Uh, unless they were like aggressively selected. Like if it's just, they are trying to lie well,
[02:52:48.480 --> 02:52:51.840]   rather than it's like they were selected over many generations to be excellent at lying or
[02:52:51.840 --> 02:52:55.520]   something, then like your ML system, hopefully you didn't train it a bunch to lie and you want
[02:52:55.520 --> 02:52:59.680]   to be careful about your training scheme effectively does that. But yeah, that seems
[02:52:59.680 --> 02:53:04.880]   like it's more likely than not to succeed. And how possible do you think it will be for us to,
[02:53:04.880 --> 02:53:12.400]   um, specify human verifiable rules for reasoning such that even if the AI is super intelligent,
[02:53:12.400 --> 02:53:17.760]   we can't really understand why there are certain things. We know that the way in which it arrives
[02:53:17.760 --> 02:53:21.920]   at these conclusions is valid. Like it was trying to persuade us to something we can be like, I
[02:53:21.920 --> 02:53:25.360]   don't understand the, all the steps, but I know that this is something that's valid and you're
[02:53:25.360 --> 02:53:30.960]   not just making shit up. That seems very hard if you want it to be like competitive with learned
[02:53:30.960 --> 02:53:35.360]   reasoning. Um, so like, I don't, I mean, it depends a little bit exactly how you set it up,
[02:53:35.360 --> 02:53:39.040]   but for like the ambitious versions of that, let's say it would address the alignment problem.
[02:53:39.040 --> 02:53:45.040]   Um, they seem pretty unlikely, you know, like five, 10% kind of thing. Is there an upper bound
[02:53:45.040 --> 02:53:49.280]   on intelligence? No, not in the near term, but just like super intelligence at some point, like
[02:53:49.280 --> 02:53:52.800]   how, how, how far do you think that can go? It seems like it's going to depend a little bit on
[02:53:52.800 --> 02:53:56.240]   what is meant by intelligence. It kind of reads as a question. It's similar to like, is there an
[02:53:56.240 --> 02:54:00.960]   upper bound on like strength or something? Like there are a lot of forms. I think it's like the
[02:54:00.960 --> 02:54:05.920]   case that for, yeah, I think there are like sort of arbitrarily smart input output functionalities.
[02:54:05.920 --> 02:54:09.040]   And then like, if you hold fixed the amount of compute, there is some smartest one. If you're
[02:54:09.040 --> 02:54:13.280]   just like, what's, what's the best set of like 10 to the 40th operations. There's just, there's
[02:54:13.280 --> 02:54:17.840]   only finitely many of them. So some like best one, um, for any particular notion of best that you
[02:54:17.840 --> 02:54:21.280]   have in mind. So I guess like, I'm just like for the unbounded question, we're allowed to use
[02:54:21.280 --> 02:54:26.240]   arbitrary description, complexity and compute like probably now. And for the, um, I mean,
[02:54:26.240 --> 02:54:29.680]   there is some like optimal conduct. Like if you're like, I have some goal in mind and I'm just like
[02:54:29.680 --> 02:54:32.880]   what action best achieves it. If you imagine like a little box embedded in the universe, I think
[02:54:32.880 --> 02:54:36.160]   there's kind of just like an optimal input output behavior. So I guess in that sense, I think there
[02:54:36.160 --> 02:54:42.000]   is a, is an upper bound, but it's not saturatable in the physical universe. Um, cause it's definitely
[02:54:42.000 --> 02:54:48.080]   exponentially slow. Right. Yeah. Yeah. Or, you know, because of comms or other things or heat
[02:54:48.080 --> 02:54:51.600]   there, it just might be physically impossible to instantiate something smarter than this.
[02:54:51.600 --> 02:54:54.960]   Yeah. I mean, like, for example, if you, if you imagine what the best thing is, it would almost
[02:54:54.960 --> 02:54:58.640]   certainly involve just like simulating every possible universe it might be in modular, like
[02:54:58.640 --> 02:55:02.000]   moral constraints, which I don't know if you want to include them, but like, so that would be very,
[02:55:02.000 --> 02:55:07.920]   very slow. It would involve simulating like all, you know, it's sort of like, I don't know exactly
[02:55:07.920 --> 02:55:13.040]   how slow, but like double exponential, very slow. Carl, uh, Shulman laid out his picture of the
[02:55:13.040 --> 02:55:18.880]   intelligence explosion in the seven hour episode. Um, what I know you guys have talked a lot. What
[02:55:18.880 --> 02:55:24.000]   about his basic picture? Like, what is it? Do you have some disagreements? Is there some crux
[02:55:24.000 --> 02:55:28.320]   that you guys have explored? It's related to our timelines discussion from earlier. I think the
[02:55:28.320 --> 02:55:35.040]   biggest, yeah, I think the biggest issue is probably error bars where like Carl has a very,
[02:55:35.040 --> 02:55:41.360]   like very software focused, very fast kind of takeoff picture. And I think that is plausible,
[02:55:41.360 --> 02:55:45.360]   but not that likely. Like, I think it's a couple of ways you could perturb the situation. And my
[02:55:45.360 --> 02:55:50.480]   guess is one of them applies. Um, so maybe I have like, I don't know exactly what Carl's
[02:55:50.480 --> 02:55:53.840]   probability is. I feel like Carl's gonna have like a 60% chance on some crazy thing that I'm only
[02:55:53.840 --> 02:55:58.800]   going to assign like a 20% chance to or 30% chance or something. And like, I think those, those kinds
[02:55:58.800 --> 02:56:04.000]   of perturbations are like one, how long a period is there of complementarity between AI capabilities
[02:56:04.000 --> 02:56:09.520]   and human capabilities, which will tend to soften takeoff to like how much diminishing returns are
[02:56:09.520 --> 02:56:15.120]   there on software progress such that like, is it like broader takeoff involving scaling, like
[02:56:15.120 --> 02:56:19.360]   electricity production and hardware production? Is that likely to happen during takeoff where I'm
[02:56:19.360 --> 02:56:26.720]   like more like 50, 50 or more, um, stuff like this. Yeah. Okay. So is it that you think the
[02:56:26.720 --> 02:56:31.600]   ultimate constraints will be more hard or like the, uh, the basic case he's laid out is that
[02:56:31.600 --> 02:56:38.400]   you can just have a sequence of things like, uh, flash attention or MOE, uh, and you can just keep
[02:56:38.400 --> 02:56:43.200]   stacking these kinds of things on. I'm very unsure if you can keep stacking them or like,
[02:56:43.200 --> 02:56:46.480]   it's kind of a question of what's like the returns curve. And like, Carl has some inference from
[02:56:46.480 --> 02:56:50.800]   historical data or some way he'd extrapolate the trend. I am more like 50, 50 on whether the
[02:56:50.800 --> 02:56:55.040]   software on the intelligence explosion is even possible. And then like a somewhat higher
[02:56:55.040 --> 02:57:00.080]   probability that it's slower than I think it might not be possible. Well, so the entire question is
[02:57:00.080 --> 02:57:04.960]   like, if you double R and D effort, do you get enough additional improvement to further double
[02:57:04.960 --> 02:57:09.840]   the efficiency? And like, that's that question will itself be a function of your hardware base,
[02:57:09.840 --> 02:57:12.720]   like how much hardware you have. And of course it's like at the amount of hardware we're going
[02:57:12.720 --> 02:57:16.480]   to have and the level of sophistication we have as the process begins, like, is it the case that
[02:57:16.480 --> 02:57:21.680]   each doubling of actually the initials only depends on the hardware or like each level of hardware
[02:57:21.680 --> 02:57:26.000]   will have some place at this dynamic asymptotes. Um, so the question is just like, for how long
[02:57:26.000 --> 02:57:30.720]   is it the case that each doubling of R and D at least doubles the effective output of your,
[02:57:30.720 --> 02:57:36.240]   you know, AI research population. And I think like, I have a higher probability on that. Like,
[02:57:36.240 --> 02:57:39.120]   I think it's kind of close. If you look at the empirics, I think the empirics benefit a lot from
[02:57:39.120 --> 02:57:44.000]   like continuing hardware scale up so that like the effective R and D stock is like significantly
[02:57:44.000 --> 02:57:47.920]   smaller than it looks, if that makes sense. Uh, what, what are the empirics you're referring to?
[02:57:47.920 --> 02:57:52.400]   Um, so there's kind of two sources of evidence. One is like looking across a bunch of industries
[02:57:52.400 --> 02:57:56.080]   of like, what is the general improvement with each doubling of like either R and D investment
[02:57:56.080 --> 02:58:01.520]   or experience where like, it is quite exceptional to have a field with not, anyway, it's pretty
[02:58:01.520 --> 02:58:04.880]   good to have a field where each time you double the R and D investment, you get a doubling of
[02:58:04.880 --> 02:58:09.600]   efficiency. The second source of evidence is on like actual, like algorithmic improvement in ML,
[02:58:09.600 --> 02:58:15.120]   which is obviously much, much scarcer. Um, and they're like, you can make a case that it's been
[02:58:15.120 --> 02:58:18.800]   like each doubling of R and D has given you like roughly a four X or something increase in
[02:58:18.800 --> 02:58:22.800]   computational efficiency. But like, there's a question of how much that benefits. When I say
[02:58:22.800 --> 02:58:26.880]   the effect of R and D stock is smaller, I mean, like we scale up, like you're doing a new task,
[02:58:26.880 --> 02:58:29.760]   like every couple of years we're doing a new task because you're operating a scale much larger than
[02:58:29.760 --> 02:58:33.760]   the previous scale. And so a lot of your effort is how to make use of the new scale. So if you're
[02:58:33.760 --> 02:58:36.960]   not increasing your installed hardware base and are just flat at a level of hardware, I think you
[02:58:36.960 --> 02:58:40.480]   get like much faster diminishing returns than people have gotten historically. I think Karl
[02:58:40.480 --> 02:58:44.480]   agrees in principle, this is true. And then once you make that adjustment, I think it's like very
[02:58:44.480 --> 02:58:47.360]   unclear where the empirics shake out. I think Karl has thought about these more than I am,
[02:58:47.360 --> 02:58:51.120]   so I should maybe defer more, but anyway, I'm at like 50, 50 on that.
[02:58:51.120 --> 02:58:54.480]   Um, how have your timelines changed over the last 20 years?
[02:58:54.480 --> 02:58:55.360]   Last 20 years?
[02:58:55.360 --> 02:58:59.520]   Yeah. Um, how long have you been working on anything related to AI?
[02:58:59.520 --> 02:59:05.760]   So I started thinking about this stuff in like 2010, um, or so. So I think my first,
[02:59:05.760 --> 02:59:12.480]   my earliest timeline prediction will be in like 2011. Um, I think in 2011, my like rough
[02:59:12.480 --> 02:59:18.080]   picture was like, we will not have insane AI in the next 10 years. And then like I get increasingly
[02:59:18.080 --> 02:59:22.240]   uncertain after that, but like we converged to like, you know, 1% per year or something like
[02:59:22.240 --> 02:59:26.720]   that. And then probably in 2016, my take was like, we won't have crazy AI in the next five years,
[02:59:26.720 --> 02:59:34.720]   but then we converged to like one or 2% per year after that. Um, then in 2019,
[02:59:34.720 --> 02:59:43.760]   I guess I made a round of forecasts, uh, where I gave like 30% or something to 25%
[02:59:43.760 --> 02:59:52.640]   to crazy AI by 2040 and like 10% by 2030 or something like that. So I think my 2030 probability
[02:59:52.640 --> 02:59:56.320]   has been kind of stable and my 2040 probability has been going up. And I would guess it's too
[02:59:56.320 --> 03:00:00.960]   sticky. I guess that 40% I gave at the beginning is just like from not having updated recently
[03:00:00.960 --> 03:00:04.960]   enough. And I maybe just need to sit down. I would guess that should be even higher. I think
[03:00:04.960 --> 03:00:08.720]   like 15% in 2030, I'm not feeling that bad about this. It's just like each passing year is like a
[03:00:08.720 --> 03:00:14.480]   big update against 2030. Like we don't have that many years left. Um, and that's like roughly
[03:00:14.480 --> 03:00:18.640]   counterbalanced with AI going pretty well. Whereas for like the 2040 thing, like the passing years
[03:00:18.640 --> 03:00:21.920]   are not that big a deal. And like, as we see the, like things are basically working, that's like
[03:00:21.920 --> 03:00:26.880]   cutting out a lot of the probability of not having AI by 2040. So yeah, my 2030 probability up a
[03:00:26.880 --> 03:00:31.360]   little bit, like maybe twice as high as it used to be or like something like that. My 2040 probably
[03:00:31.360 --> 03:00:38.160]   like up more, much more significantly. Uh, how fast do you think, um, uh, we can keep building
[03:00:38.160 --> 03:00:42.240]   fabs to keep up with the demand? Yeah, I don't know much about any of the relevant areas. My
[03:00:42.240 --> 03:00:50.640]   like best guess is like, um, maybe my understanding is right now, like 5% or something of like the
[03:00:50.640 --> 03:00:58.960]   next year's like total or like best process fabs will be making AI hardware of which like only a
[03:00:58.960 --> 03:01:02.960]   small fraction will be going into very large training runs, like only a couple, maybe a couple
[03:01:02.960 --> 03:01:07.200]   of percent of total output. And then like that represents maybe like 1% of total possible output,
[03:01:07.200 --> 03:01:11.280]   a couple of percent of like leading process, 1% of total or something. I don't know if that's
[03:01:11.280 --> 03:01:14.800]   right, but I think that's like the rough ballpark we're in. I think things will be like pretty fast
[03:01:14.800 --> 03:01:18.720]   as you scale up for like the next order of magnitude or two from there, because you're
[03:01:18.720 --> 03:01:23.440]   basically just shifting over other stuff. My sense is it would be like years of delay. There's like
[03:01:23.440 --> 03:01:28.080]   multiple reasons that you expect years of delay for going past that. Maybe even at that, you start
[03:01:28.080 --> 03:01:34.480]   having, yeah, there's just a lot of problems. Like building new fabs is quite slow. Um, and I don't
[03:01:34.480 --> 03:01:39.600]   think there's like, TSMC is not like planning on increases in total demand driven by AI, like kind
[03:01:39.600 --> 03:01:45.360]   of conspicuously not planning on it. Um, I don't think anyone else is really ramping up production
[03:01:45.360 --> 03:01:49.040]   in anticipation either. So I think, and then similarly, like just building data centers of
[03:01:49.040 --> 03:01:52.640]   that size seems like very, very hard and also probably has multiple years of delay.
[03:01:52.640 --> 03:01:54.320]   What does your portfolio look like?
[03:01:54.320 --> 03:01:59.120]   Um, I've tried to get rid of most of the AI stuff that's like plausibly implicated in
[03:01:59.120 --> 03:02:05.520]   policy work or like, um, see the advocacy on the RSP stuff or my involvement with Anthropic.
[03:02:05.520 --> 03:02:07.760]   Or what would it look like if you had no complex of interest?
[03:02:07.760 --> 03:02:12.880]   And no inside information. Like I also still have a bunch of hardware investments,
[03:02:12.880 --> 03:02:19.120]   which I need to think about, but like, I don't know, a lot of TSMC. I have a chunk of NVIDIA,
[03:02:19.120 --> 03:02:24.560]   although I keep, I just keep betting against NVIDIA constantly since 2016 or something. I've
[03:02:24.560 --> 03:02:29.520]   been destroyed on that bet. Although AMD has also done fine. I just like, well, now the case now is
[03:02:29.520 --> 03:02:34.480]   even easier, but it's similar to the case in the old days, just a very expensive company given the
[03:02:34.480 --> 03:02:37.280]   total amount of R and D investment they've made. They have like whatever, a trillion dollar
[03:02:37.280 --> 03:02:43.920]   valuation or something. That's like very high. So the question is like, how expensive is it to
[03:02:43.920 --> 03:02:49.280]   like make a TPU such that's like actually outcompetes a H100 or something. And I'm like,
[03:02:49.280 --> 03:02:54.800]   wow, it's real level, high level of incompetence. If Google can't like catch up fast enough to like
[03:02:54.800 --> 03:03:00.720]   make that trillion dollar valuation not justified. Whereas with TSMC it's much harder. They have a
[03:03:00.720 --> 03:03:04.240]   harder remote you think? Yeah. I think it's, it's a lot harder, especially if you're in this regime
[03:03:04.240 --> 03:03:08.000]   where like you're trying to scale up. So like if you're unable to build fabs, I think it will take
[03:03:08.000 --> 03:03:11.440]   a very long time to build as many fabs as people want. Like the effect of that will be to like
[03:03:11.440 --> 03:03:15.360]   bid up the price of existing fabs and existing semi-conditioned manufacturing equipment. And so
[03:03:15.360 --> 03:03:20.320]   like just those hard assets will become like spectacularly valuable as well the existing like
[03:03:20.320 --> 03:03:27.680]   GPUs and like the actual, yeah. Yeah. I think it's just hard. That seems like the hardest asset to
[03:03:27.680 --> 03:03:30.880]   scale up quickly. So it's like the asset, if you have like a rapid run-up, it's the one that you'd
[03:03:30.880 --> 03:03:37.120]   expect to most benefit. Whereas like NVIDIA stuff will ultimately be replaced by like, either it's
[03:03:37.120 --> 03:03:41.040]   better stuff made by humans or stuff made by, with AI systems. Like the gap will close even further
[03:03:41.040 --> 03:03:45.680]   as you build AI systems. Right. Unless NVIDIA is using those systems. Yeah. The point is just
[03:03:45.680 --> 03:03:51.280]   like the future R&D will sow a door past R&D and there's like just not that much stickiness. There's
[03:03:51.280 --> 03:03:56.560]   less stickiness in the future than there has been in the past. Like, yeah. I don't know. So I don't
[03:03:56.560 --> 03:03:59.920]   want to, not commenting from any private information, just in my gut having caveat of
[03:03:59.920 --> 03:04:04.080]   this is like the single bet I've most lost. Okay. Not including NVIDIA in that portfolio.
[03:04:04.080 --> 03:04:08.960]   And final question. There's a lot of schemes out there for alignment. And I think just like a lot
[03:04:08.960 --> 03:04:14.000]   of general takes and a lot of this stuff is over my head where I think I literally, it took me like
[03:04:14.000 --> 03:04:19.600]   weeks to understand the mechanistic anomaly stuff you work on without spending weeks. How do you
[03:04:19.600 --> 03:04:24.320]   detect bullshit? Like people have explained their schemes to me and I'm like, honestly, I don't know
[03:04:24.320 --> 03:04:28.880]   if it makes sense or not. With you, I'm just like, I trust Paul enough that I think there's probably
[03:04:28.880 --> 03:04:33.440]   something here if I try to understand this enough, but yeah. How do you detect bullshit?
[03:04:33.440 --> 03:04:37.360]   Yeah. So I think it depends on the kind of work. So for like the kind of stuff we're doing,
[03:04:37.360 --> 03:04:41.200]   my guess is like most people, there's just not really a way you're going to tell whether it's
[03:04:41.200 --> 03:04:45.200]   bullshit. So I think like it's important that we don't spend that much money on like the people
[03:04:45.200 --> 03:04:48.720]   who want to hire us are probably going to dig in in depth. I don't think there's a way you can tell
[03:04:48.720 --> 03:04:53.120]   whether it's bullshit without either spending like a lot of effort or leaning on deference.
[03:04:55.520 --> 03:04:58.800]   With empirical work, it's like interesting in that you do have some signals of the quality
[03:04:58.800 --> 03:05:02.240]   of work where you can be like, I mean, is it, does it work in practice? Like does the story,
[03:05:02.240 --> 03:05:05.840]   I think the stories are just radically simpler. And so you probably can evaluate those stories
[03:05:05.840 --> 03:05:09.040]   like on their face. And then you mostly come down to these questions about like,
[03:05:09.040 --> 03:05:13.200]   what are the key difficulties? Yeah. I tend to like be optimistic when people dismiss something
[03:05:13.200 --> 03:05:16.080]   because like this doesn't deal with a key difficulty or this runs into the following
[03:05:16.080 --> 03:05:20.800]   insuperable obstacle. I tend to be like a little bit more skeptical about those arguments and tend
[03:05:20.800 --> 03:05:24.960]   to think like, yeah, something can be bullshit because it's not addressing a real problem.
[03:05:24.960 --> 03:05:28.000]   That's like, I think the easiest way, like, this is a problem someone's interested in. That's just
[03:05:28.000 --> 03:05:30.560]   like not actually an important problem. And there's no story about why it's going to become
[03:05:30.560 --> 03:05:35.040]   an important problem, e.g. like it's not a problem now and won't get worse, or it is maybe a problem
[03:05:35.040 --> 03:05:41.120]   now, but it's clearly getting better. That's like one way. And then conditioned on like passing that
[03:05:41.120 --> 03:05:45.440]   bar, like dealing with something that actually engages with like important parts of the argument
[03:05:45.440 --> 03:05:50.400]   for concern and then like actually making sense empirically. So like, I think most work is anchored
[03:05:50.400 --> 03:05:53.680]   by its source of feedback is like actually engaging with real models. So it's like,
[03:05:53.680 --> 03:05:56.640]   does it make sense to have engaged with real models? And does the story about how it
[03:05:56.640 --> 03:06:04.000]   like deals with key difficulties actually make sense? Um, I'm like pretty liberal past there.
[03:06:04.000 --> 03:06:08.640]   Um, I think it's really hard to like, e.g. people look at mechanistic interpretability and be like,
[03:06:08.640 --> 03:06:11.760]   well, this obviously can't succeed. And I'm like, I don't know. How can you tell it obviously can't
[03:06:11.760 --> 03:06:15.840]   succeed? Like, I think it's reasonable to take total investment in the field, like how fast
[03:06:15.840 --> 03:06:21.120]   is it making progress? Like how does that pencil? I think like most things people work on though,
[03:06:21.120 --> 03:06:25.360]   actually pencil, like pretty fine. Like they look like they could be reasonable investments. Um,
[03:06:25.360 --> 03:06:30.080]   things are not like super out of whack. Okay, great. This is, I think a good place to close.
[03:06:30.080 --> 03:06:32.720]   Paul, thank you so much for your time. Yeah. Thanks for having me. It was good chatting.
[03:06:32.720 --> 03:06:38.720]   Yeah, absolutely. Hey everybody. I hope you enjoyed that episode. As always,
[03:06:38.720 --> 03:06:42.960]   the most helpful thing you can do is to share the podcast, send it to people you think might
[03:06:42.960 --> 03:06:46.640]   enjoy it, put it in Twitter, your group chats, et cetera. Just splits the world.
[03:06:47.600 --> 03:07:01.040]   Appreciate your listening. I'll see you next time. Cheers.

