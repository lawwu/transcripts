
[00:00:00.000 --> 00:00:03.120]   to make sure. Just give me one second.
[00:00:03.120 --> 00:00:17.440]   I can hear you, Niko. I always like to double check. I'm so terrified of that.
[00:00:17.440 --> 00:00:23.040]   Hey, everyone. Welcome back to another Jaxx talk. I'm super excited because we have someone
[00:00:23.040 --> 00:00:29.520]   really incredible today. Two incredible people apart from me. Sasha Rush and Christian. Christian
[00:00:29.520 --> 00:00:34.880]   has been the cohost, like always. So I'll skip his introduction. But we're super excited to host
[00:00:34.880 --> 00:00:42.640]   Alexander Rush. He goes as Sasha Rush on the internet. He's an associate professor at Cornell
[00:00:42.640 --> 00:00:49.360]   University. And he aims to build NLP systems that are safe, fast, and controllable. You might have
[00:00:49.360 --> 00:00:56.320]   also seen his work at Hugging Face, where he contributes part time. And today he'll be teaching
[00:00:56.320 --> 00:01:03.280]   us more stuff about Jaxx. We'll be learning how to create extremely long sequences. So thank you so
[00:01:03.280 --> 00:01:07.280]   much, Sasha, for joining us. >> Thanks. Thanks so much.
[00:01:07.280 --> 00:01:13.520]   >> Yeah. >> Great. Cool. I'll get started. I'll have
[00:01:13.520 --> 00:01:19.520]   a chat up if people have questions as we go. So today I'm going to talk about some work I've been
[00:01:19.520 --> 00:01:25.600]   doing in a kind of open source and tutorial realm. It's about generating extremely long sequences.
[00:01:26.160 --> 00:01:31.440]   And for this project, it was a really nice opportunity to dive deep into Jaxx and the
[00:01:31.440 --> 00:01:38.800]   Jaxx neural network landscape. This is joint work with Sid Karamchedi, who was working or still
[00:01:38.800 --> 00:01:45.600]   works with us at Hugging Face. And it's based on a paper known as S4. It was written by Albert Gu,
[00:01:45.600 --> 00:01:52.080]   Karen Goh, and Christopher Wray. Everything I'm going to describe today is located on this GitHub
[00:01:52.080 --> 00:02:01.120]   repo that you can get here. And the slides and code is also available there. So let me first
[00:02:01.120 --> 00:02:07.120]   talk a little bit about kind of high level what I do, what kind of work I'm interested in, and
[00:02:07.120 --> 00:02:15.600]   kind of the context for this project. So first off, let me introduce myself. I'm a professor
[00:02:15.600 --> 00:02:23.120]   at Cornell. I work in New York City at a new part of Cornell known as Cornell Tech. I'm also a
[00:02:23.120 --> 00:02:29.200]   researcher at Hugging Face, where I work with a lot of great folks there on NLP and machine learning
[00:02:29.200 --> 00:02:35.680]   generally. Besides that, I like to contribute open source projects. And I'm particularly interested
[00:02:35.680 --> 00:02:43.040]   as kind of a casual hobbyist in the sort of programming languages that we utilize for deep
[00:02:43.040 --> 00:02:50.320]   learning and kind of safe mathematical applications. So past work along this realm has been
[00:02:50.320 --> 00:02:57.280]   work on named tensors, a post about the annotated transformer. And I have a little course that I
[00:02:57.280 --> 00:03:04.480]   teach based on a library called Minitorch, which is kind of a from scratch kind of teaching library
[00:03:04.480 --> 00:03:10.560]   that goes through the basics of the Torch programming language. I just wanted to say
[00:03:10.560 --> 00:03:15.520]   it's such an awesome framework. I've played around with it a lot. So thanks. Thanks so much.
[00:03:15.520 --> 00:03:20.960]   >> That's awesome. Yeah. Yeah. I really learned a lot putting it together and it's fun watching
[00:03:20.960 --> 00:03:25.840]   students go through it. I really highly recommend anyone to do it. I mean, we use these libraries
[00:03:25.840 --> 00:03:31.680]   so often and on the one hand, like they're kind of magic, but on the other hand, a lot of it is
[00:03:31.680 --> 00:03:40.000]   pretty understandable if you dive into the details. So yeah, it's been fun. Cool. So today's talk,
[00:03:40.560 --> 00:03:44.960]   is not a research talk. It's a kind of about a kind of learning process that I went through.
[00:03:44.960 --> 00:03:49.760]   So there's going to be bugs and there will unfortunately not be as many citations as I
[00:03:49.760 --> 00:03:55.600]   would have liked. So I just want to make that as a quick caveat. And the goals of the talk are going
[00:03:55.600 --> 00:04:00.640]   to be to kind of learn about a new machine learning architecture that I think is a very
[00:04:00.640 --> 00:04:08.640]   promising approach for generating extremely long sequences and to understand how Jack supports this.
[00:04:08.640 --> 00:04:15.200]   And in particular, I want to emphasize that we're going to see a system that I think
[00:04:15.200 --> 00:04:23.040]   kind of goes beyond the kind of boundaries of some deep learning libraries. Like it has some
[00:04:23.040 --> 00:04:28.240]   parts in mathematical operations that I think are just hard to kind of do in a kind of generic
[00:04:28.240 --> 00:04:33.040]   approach. So I feel like a lot of the kind of more powerful aspects of Jack's really do kind
[00:04:33.040 --> 00:04:37.040]   of support this kind of research and this kind of architecture. And I'm a big believer that with
[00:04:37.040 --> 00:04:40.640]   kind of better tools, we'll be able to do kind of better research going forward.
[00:04:40.640 --> 00:04:47.600]   So before I begin, let me talk a little bit about Jack's. So I have strong feelings about the
[00:04:47.600 --> 00:04:53.840]   library. I think it has both pros and cons. I'll start with the cons just to get them out there.
[00:04:53.840 --> 00:05:00.240]   This project was hard and it was particularly hard because debugging is still quite difficult.
[00:05:00.240 --> 00:05:06.240]   I know there's been a lot of kind of advances in the debugging aspect of Jack's, but it really is
[00:05:06.240 --> 00:05:11.120]   just a lot more difficult for me than PyTorch. And I understand why people struggle with that.
[00:05:11.120 --> 00:05:19.040]   I also still kind of feel like there's not really a standard for neural networks. And this talk is
[00:05:19.040 --> 00:05:26.480]   going to utilize Flax. And I'm using Flax for neural networks simply because like Google kind
[00:05:26.480 --> 00:05:33.280]   of told me to. But I'm not convinced Flax is like really like got it totally yet. It has a lot of
[00:05:33.280 --> 00:05:38.000]   really cool things about it, but it has a lot of warts that came up in the part of the project.
[00:05:38.000 --> 00:05:42.800]   And then the last thing is that I still kind of struggle a little bit to reason about some
[00:05:42.800 --> 00:05:50.000]   aspects of Jack's. Like I don't totally get the semantics or particular aspects about the neural
[00:05:50.000 --> 00:05:54.560]   network semantics. And I know there are a lot of tutorials out there, but some of the times I kind
[00:05:54.560 --> 00:06:00.240]   of fall between like really in depth, like here's how XLA works to like a little bit too high level
[00:06:00.240 --> 00:06:05.440]   for me of like, here's how you build a kind of the three models that people use in deep learning.
[00:06:05.440 --> 00:06:11.440]   Okay. But those are the cons. I think the pros are what really excite me and why I dived into
[00:06:11.440 --> 00:06:18.240]   this project. I really, really, really think it's a kind of transformative idea that Jack's actually
[00:06:18.240 --> 00:06:24.080]   allows you to separate the math from the rest of your model. And this facilitates testing. It
[00:06:24.080 --> 00:06:29.040]   facilitates difficult modeling. And we're really going to see that in this project where there's
[00:06:29.040 --> 00:06:34.000]   no plausible way I would have been able to keep track of like seven different dimensions going on.
[00:06:34.000 --> 00:06:39.760]   And the fact that Jack's actually let you abstract that away is really quite transformative.
[00:06:39.760 --> 00:06:47.280]   There's also aspects of the JIT that just continue to really impress me. And at the core of this
[00:06:47.280 --> 00:06:53.120]   talk, there'll be an operation that's quite simple, but in practice, the PyTorch code had to call out
[00:06:53.120 --> 00:06:58.800]   to an external library where the Jack's JIT is able just to make it go away. And it really like
[00:06:58.800 --> 00:07:07.760]   makes the model much more usable. And finally, I said that I'm not totally convinced by Flax yet,
[00:07:07.760 --> 00:07:12.400]   but what I am really convinced by is the fact that they were able to do these kind of lifted
[00:07:12.400 --> 00:07:19.200]   transformations. In particular, in Flax, they're able to apply Vmap and scan to kind of entirely
[00:07:19.200 --> 00:07:24.400]   parameterized models, which I think is really, really awesome. And it saved us a ton of effort
[00:07:24.400 --> 00:07:31.280]   in this project. Cool. So that's kind of to start with. Now let me talk a little bit about the
[00:07:31.280 --> 00:07:40.240]   problem and then we'll dive into the actual model itself. So to give some context, the problem we're
[00:07:40.240 --> 00:07:46.800]   going to talk about today is sequence modeling. And in some sense, sequence modeling is like 100
[00:07:46.800 --> 00:07:51.280]   different problems. It's used in all sorts of different fields and all sorts of different areas
[00:07:51.280 --> 00:07:56.000]   of machine learning. But for our purposes, we're going to take an extremely bird's eye view.
[00:07:56.000 --> 00:08:01.440]   We're going to refer to sequence modeling as basically anything where we learn over a list
[00:08:01.440 --> 00:08:08.080]   of elements. And these lists of elements might be discrete or they might be sampled from a signal.
[00:08:08.080 --> 00:08:13.600]   We're going to look at several different problems, including problems involving text tokens,
[00:08:13.600 --> 00:08:21.040]   but also problems involving waveforms or problems involving images. And we're going to look at two
[00:08:21.040 --> 00:08:27.920]   variants. And both of these are implemented in our library. The first is a classification style task.
[00:08:27.920 --> 00:08:33.920]   So here we're given a sequence. And just for simplicity, I'll represent the sequence as text
[00:08:33.920 --> 00:08:40.160]   because that's my area of research. And so given a sequence like, is the dog a good boy? We want to
[00:08:40.160 --> 00:08:47.040]   classify that as yes or no. And in these settings, it really is just kind of a sequence model
[00:08:47.040 --> 00:08:53.600]   into a classification head. The other sort of problem we'll look at is generation, where we're
[00:08:53.600 --> 00:08:59.440]   given a sequence of tokens and we'd like to produce the next one. So the dog is a good blank,
[00:08:59.440 --> 00:09:05.360]   and our goal is to fill that in. Now, this problem is a bit more interesting because
[00:09:06.480 --> 00:09:11.360]   it really matters that we do it efficiently. If we're trying to generate an extremely long
[00:09:11.360 --> 00:09:16.480]   sequence, think like thousands of tokens, we need to kind of quickly generate the next
[00:09:16.480 --> 00:09:20.240]   element in the sequence and then move on to the next one and so on as we go.
[00:09:20.240 --> 00:09:28.160]   Now, both of these models, at least in NLP, are almost universally solved with a model known as
[00:09:28.160 --> 00:09:33.520]   the transformer. And we're seeing transformer being used in all sorts of other modalities
[00:09:34.080 --> 00:09:39.760]   these days, including images and other sorts of sequences and tables and basically kind of any
[00:09:39.760 --> 00:09:45.840]   problem that I can describe similar to the form in the previous slide. And I'm not going to talk
[00:09:45.840 --> 00:09:52.640]   too much about the transformer besides just noting that it's kind of completely ubiquitous. So any
[00:09:52.640 --> 00:09:56.560]   kind of sequence modeling, you're going to apply this sort of approach to. It's kind of underlying
[00:09:56.560 --> 00:10:03.120]   BERT and underlines GPT-3 and underlies the first dolly and a bunch of different models that we've
[00:10:03.120 --> 00:10:08.960]   seen. And the key thing to note about the transformer is that its internal operation
[00:10:08.960 --> 00:10:15.280]   is a kind of sequence modeling operation that requires every position in the model to look back
[00:10:15.280 --> 00:10:22.240]   at every previous position. So if we want to generate a sequence, like my dog is a good blank
[00:10:22.240 --> 00:10:28.480]   and produce the next word boy, we have to look back upon everything that we've created so far
[00:10:28.480 --> 00:10:35.600]   in this process. So we have this self-attention operation where our memory is everything that
[00:10:35.600 --> 00:10:42.240]   we've seen so far up until this point. And I kind of have a little joke going, which is that
[00:10:42.240 --> 00:10:48.320]   transformer has been kind of so dominant in NLP that it's basically the state of the art in every
[00:10:48.320 --> 00:10:55.120]   problem we're working on. And I had this bet going with Jonathan Frankel about whether this will be
[00:10:55.120 --> 00:11:01.120]   true in five years or not, like whether this operation will still be the most dominant thing
[00:11:01.120 --> 00:11:07.200]   in NLP. And the clock is ticking down. I think I have about four and a half years left. And our
[00:11:07.200 --> 00:11:12.720]   current status is that yes, basically this operation is all you need to calculate.
[00:11:12.720 --> 00:11:21.840]   But there are some weaknesses with the transformer. And one weakness is that this kind of
[00:11:21.840 --> 00:11:29.200]   full memory, this kind of full self-attention is rather expensive. As we add new words,
[00:11:29.200 --> 00:11:34.400]   particularly in a generation context, we have to keep around our memory of everything else
[00:11:34.400 --> 00:11:40.800]   that we've seen so far. So the amount of computation we're going to do is going to scale
[00:11:40.800 --> 00:11:47.120]   quadratically with the length of the sequence. And this isn't anything kind of specific about
[00:11:47.120 --> 00:11:52.480]   the neural network of the transformer. It's just a kind of inherent property of self-attention as
[00:11:52.480 --> 00:11:57.280]   an operation. It just kind of baked into the idea that you're going to need to look at everything
[00:11:57.280 --> 00:12:04.160]   else previously. You're going to kind of scale in this way. And so because of this problem,
[00:12:04.160 --> 00:12:10.640]   there's been kind of a cottage industry of comparing transformers with different architectures.
[00:12:10.640 --> 00:12:15.520]   And one architecture in particular is the one that kind of dominated before
[00:12:16.320 --> 00:12:21.200]   the transformer took over. And this is the recurrent neural network architecture.
[00:12:21.200 --> 00:12:25.920]   And in recurrent neural networks, there's just this inherent assumption that you're not going
[00:12:25.920 --> 00:12:31.120]   to have to remember everything that you've previously computed. You're going to just have
[00:12:31.120 --> 00:12:37.120]   this bottleneck that all of your memory has to be a fixed size, and you're going to utilize and
[00:12:37.120 --> 00:12:43.280]   update that memory over time. And so again, kind of accepting the details of this architecture,
[00:12:43.280 --> 00:12:47.680]   it's just important to note that this is going to scale in a much better way,
[00:12:47.680 --> 00:12:52.400]   but it's going to require you to kind of do this sort of compression as you go.
[00:12:52.400 --> 00:13:00.880]   And I think because of this, we saw a really sharp change in NLP and other fields from RNNs to
[00:13:00.880 --> 00:13:06.400]   transformers, as transformers just seem to scale better with data and perform better kind of across
[00:13:06.400 --> 00:13:10.640]   the board in core benchmarks like machine translation and other forms of classification.
[00:13:12.720 --> 00:13:21.360]   Now, I mentioned that this kind of N squared problem is kind of in the air right now. And
[00:13:21.360 --> 00:13:27.200]   over the last couple of years, there've been a lot of different models that have kind of targeted
[00:13:27.200 --> 00:13:34.000]   methods to get around the quadratic scaling of transformers. So here are about 10 different
[00:13:34.000 --> 00:13:40.400]   methods that people have proposed. And this is from a benchmark known as Long Range Arena
[00:13:40.400 --> 00:13:45.920]   that came out from Google that tries to kind of classify how good these different kind of
[00:13:45.920 --> 00:13:53.360]   scaling transformers do on various tasks. And there are about six tasks here, and all of them
[00:13:53.360 --> 00:14:01.040]   are kind of picked because they have very long range dependencies and very high number of tokens
[00:14:01.040 --> 00:14:06.720]   to work with. So I don't think these are kind of inherently important tasks for the various fields,
[00:14:06.720 --> 00:14:11.520]   but they do kind of demonstrate the kind of difficulty of handling very long sequences.
[00:14:11.520 --> 00:14:16.160]   And the key thing to note from this slide is that for a lot of these tasks,
[00:14:16.160 --> 00:14:24.320]   the models are not doing particularly well. And not only are the fast models not doing well,
[00:14:24.320 --> 00:14:29.840]   the models kind of on this part of the slide, transformer is even failing on some of them.
[00:14:29.840 --> 00:14:36.160]   So kind of a raw transformer without any tricks fails on some of these tasks.
[00:14:36.160 --> 00:14:42.480]   So we'd like to kind of both improve on transformers performance and also improve
[00:14:42.480 --> 00:14:48.000]   on these kind of fast variants of transformers that get used in these other columns.
[00:14:48.000 --> 00:14:53.920]   And let me focus in on one of these tasks, because I think it's relatively interesting.
[00:14:53.920 --> 00:14:58.880]   So I mentioned that I work in natural language processing, but these tasks are really cross
[00:14:58.880 --> 00:15:04.560]   modality. You can see that they have the first one is kind of almost like a programming languages
[00:15:04.560 --> 00:15:09.840]   task. The second one is a text task. The third is information retrieval. And then the fourth is
[00:15:09.840 --> 00:15:17.200]   image. And then these other ones are also images as well. And so how do we kind of turn all these
[00:15:17.200 --> 00:15:23.200]   tasks into a sequence task? The way we do it is kind of doing it in the most naive way you could
[00:15:23.200 --> 00:15:30.720]   imagine. So we're just going to linearize or make sequential some of these data sets. So to take
[00:15:30.720 --> 00:15:37.440]   this image, we literally just kind of flatten it and then read it left to right, pixel by pixel
[00:15:37.440 --> 00:15:43.600]   to do classification or do prediction. And I think this is like pretty counterintuitive. It kind of
[00:15:43.600 --> 00:15:50.560]   breaks a lot of locality in the underlying image. And it kind of requires you to have very, very
[00:15:50.560 --> 00:15:56.640]   long-term memory to even kind of figure out that two pixels were next to each other along the row
[00:15:56.640 --> 00:16:02.480]   axis of the original image. And just give you a sense of what some of these tasks looks like.
[00:16:02.480 --> 00:16:10.320]   This is the path x task that kind of raw transformers were not able to solve. So this is
[00:16:10.320 --> 00:16:16.800]   what you get. You get this image. So you're going to like linearize it. So you start from the first
[00:16:16.800 --> 00:16:23.280]   row, go through, go through each of the columns and feed that to the model. And the result of
[00:16:23.280 --> 00:16:31.520]   the model is that it has to tell us whether or not the two white dots connect. So whether this dot
[00:16:31.520 --> 00:16:37.520]   connects with this dot. So it has to basically realize that all these other paths are basically
[00:16:37.520 --> 00:16:43.280]   spurious and then figure out that there is a line from here to here and that they are connected.
[00:16:44.240 --> 00:16:48.960]   And there are variants or other images that you can see that will kind of not be connected. But
[00:16:48.960 --> 00:16:54.000]   I think as a human, it's pretty easy to see that they're connected in this way. But imagine like
[00:16:54.000 --> 00:16:59.360]   if I gave you a list of pixels kind of starting from the top row, how hard that would be to kind
[00:16:59.360 --> 00:17:07.040]   of figure that out. Cool. So that's the context of the work. I'll stop there if anyone has any
[00:17:07.040 --> 00:17:16.720]   questions to get started. There are no questions from the audience, Edward. Maybe Christian has a
[00:17:16.720 --> 00:17:25.920]   few. I do have some, but it's weird because we haven't explained the S4. Okay. So it doesn't
[00:17:25.920 --> 00:17:32.560]   make sense right now. I'll keep going. Sounds great. Okay, great. So let's dive into the actual
[00:17:32.560 --> 00:17:38.960]   project in this work. So we're going to look at this paper called "Efficiently Modeling Long
[00:17:38.960 --> 00:17:45.440]   Sequences with Structured State Spaces." And the compilable idea of this work is that we're going
[00:17:45.440 --> 00:17:53.200]   to learn a new approach for learning long sequences. This approach is going to allow us
[00:17:53.200 --> 00:18:00.080]   to basically learn very long distance dependencies. And we're going to be able to train our model in
[00:18:00.080 --> 00:18:04.640]   such a way that it's able to actually learn from these long distance dependencies from the data.
[00:18:04.640 --> 00:18:09.760]   But then when we actually use the model in practice, we're going to be able to use it
[00:18:09.760 --> 00:18:15.600]   as a recurrent neural network. So the final thing we're going to get to is going to look like this
[00:18:15.600 --> 00:18:21.040]   model here, where it acts like an RNN. And in fact, it acts like a very simple RNN. It's going
[00:18:21.040 --> 00:18:27.440]   to be just a fully linear recurrent neural network. But because of the way we train it,
[00:18:27.440 --> 00:18:31.920]   we're going to actually be able to train it to learn these very long distance dependencies
[00:18:31.920 --> 00:18:38.480]   and do very well on some of these tasks. So just to give you a sense of the punchline,
[00:18:38.480 --> 00:18:46.800]   so the results in S4 show that this model, when run on the long range arena benchmark,
[00:18:46.800 --> 00:18:52.640]   is able to do extremely well. It's able to do much better than a bunch of different
[00:18:53.440 --> 00:18:59.280]   kind of fast transformer models. And it's even able to outperform the transformer on a lot of
[00:18:59.280 --> 00:19:06.080]   tasks, including getting a relatively high score on the PathX benchmark, which is this benchmark
[00:19:06.080 --> 00:19:14.320]   that these previous models were not able to solve at all. OK, so the kind of high level challenges
[00:19:14.320 --> 00:19:22.400]   for us are that this model is quite mathematically complicated. And so actually understanding all
[00:19:22.400 --> 00:19:28.000]   the details of how it works is definitely beyond me when I first started. But it also, I think,
[00:19:28.000 --> 00:19:31.440]   is intimidating for a lot of folks who want to use these models in practice.
[00:19:31.440 --> 00:19:37.600]   The other aspect that I think is really important is that we want to be able to test this model. So
[00:19:37.600 --> 00:19:41.920]   I want to be able to have kind of unit tests that it respects, kind of the mathematical properties
[00:19:41.920 --> 00:19:47.200]   that are claimed from the paper. And to do that, I don't want a kind of pile of neural network code.
[00:19:47.200 --> 00:19:50.560]   I want actual kind of mathematical functions that we can test directly.
[00:19:50.560 --> 00:19:56.160]   I mentioned before that some of the core operations require an external library to run
[00:19:56.160 --> 00:20:02.000]   in Torch. And we'd like to use Jack's JIT to get around that issue and make it run fast.
[00:20:02.000 --> 00:20:09.040]   And finally, it seems like there are other methods that are coming out that utilize a kind of similar
[00:20:09.040 --> 00:20:14.160]   structure of S4. So we'd like our library to be extensible and kind of separate out the different
[00:20:14.160 --> 00:20:20.160]   mathematical concepts so that we can quickly adapt to new papers. And in fact, I'll conclude the talk
[00:20:20.160 --> 00:20:25.520]   by showing some code for a new work that just came out that we're releasing in the next couple days.
[00:20:25.520 --> 00:20:31.200]   Cool. And then the actual way we're going to do this is we're going to write a concise,
[00:20:31.200 --> 00:20:37.360]   pedagogical, literate code implementation that uses Jack's and Flax.
[00:20:38.960 --> 00:20:44.480]   Great. So the project is called the Annotated S4. You can check it out at this link. And let's
[00:20:44.480 --> 00:20:50.240]   see actually if I can-- Sasha, can I ask you a question from the chat? Yeah. People are
[00:20:50.240 --> 00:21:00.880]   wondering if a convolutional network can solve the x-path task. It's a good question. I actually
[00:21:00.880 --> 00:21:08.080]   don't know offhand. My guess is that a convolutional network, given the 2D structure of the image,
[00:21:08.640 --> 00:21:14.000]   probably could solve this task pretty straightforward. So I think part of the
[00:21:14.000 --> 00:21:18.240]   task is kind of like doing it from the raw data of the task itself.
[00:21:18.240 --> 00:21:26.400]   OK. Yeah. So this is kind of a contrived task that forces the long-range transformer, I guess,
[00:21:26.400 --> 00:21:33.680]   to squeeze all their power, something like that? Yeah. So actually, let me talk through some of
[00:21:33.680 --> 00:21:37.760]   the different tasks we're going to look at just to give people a sense of which ones are contrived
[00:21:37.760 --> 00:21:43.200]   and which ones are not. So we're going to look at several different tasks. We're going to look at
[00:21:43.200 --> 00:21:51.520]   both image classification and image generation. And so in these tasks are a bit contrived because
[00:21:51.520 --> 00:21:59.040]   you are kind of trying to generate an image by its pixels or classify an image by its pixels.
[00:21:59.040 --> 00:22:04.880]   And so in these cases, you really could utilize the kind of full structure of the image itself.
[00:22:04.880 --> 00:22:10.160]   You'd obviously do much better than trying to kind of process it in this sort of long-range
[00:22:10.160 --> 00:22:16.480]   sequence. However, there are a lot of tasks that this model applies to that that's not true for.
[00:22:16.480 --> 00:22:23.840]   So one task we'll talk about is speech generation. So in speech generation, you really want to
[00:22:23.840 --> 00:22:31.840]   produce speech in the kind of format of a wave. So you can do classification by converting it to
[00:22:32.960 --> 00:22:39.120]   a different format, such as MFCC. But for speech generation, you really kind of want to do it on
[00:22:39.120 --> 00:22:45.040]   the waveform itself. And so in this task, it really is just an extremely long sequence that
[00:22:45.040 --> 00:22:50.320]   you get as input. And in this case here, the orange is our input that we're getting, and the
[00:22:50.320 --> 00:22:56.560]   blue is our completion that we actually want to produce from the model itself. So while some of
[00:22:56.560 --> 00:23:00.640]   these image tasks are a little bit contrived, some of these other ones really do kind of have
[00:23:00.640 --> 00:23:10.080]   the property of just being an extremely long 1D sequence. Great. And then just to summarize, so
[00:23:10.080 --> 00:23:16.080]   the tasks we'll look at in the blog post are these kind of image generation, image classification
[00:23:16.080 --> 00:23:21.520]   from sequential sequences, as well as sound classification and sound generation
[00:23:21.520 --> 00:23:29.600]   directly from the waveforms themselves. Cool. Christian, were there any other questions,
[00:23:29.600 --> 00:23:36.880]   or should I keep on going? No, I think people are discussing around the convolution layers,
[00:23:36.880 --> 00:23:44.560]   but I think you covered it. Okay, sounds good. Yeah, so I don't think I really want to overclaim
[00:23:44.560 --> 00:23:50.880]   that this is the way you should do image processing. But there are a lot of kind of
[00:23:50.880 --> 00:23:57.200]   sequential tasks that we don't have as clear of a model from that image processing.
[00:23:57.520 --> 00:24:02.960]   Okay, cool. So that's the background. Let's dive into the actual implementation.
[00:24:02.960 --> 00:24:10.960]   So I'm going to cover the implementation in three parts. This is the part of the talk where things
[00:24:10.960 --> 00:24:18.160]   get a little bit more mathy. But in part one, I think everything should be, in theory, kind of
[00:24:18.160 --> 00:24:23.600]   understandable as we go. Part two gets a little bit more complex, and I'll mostly kind of cover
[00:24:23.600 --> 00:24:29.200]   it at a high level. And then part three, we'll dive into some of the particular implementations
[00:24:29.200 --> 00:24:36.320]   of the model. So part one covers a kind of classical model known as a state-space model,
[00:24:36.320 --> 00:24:42.960]   or an SSM. This is a model that had been explored by various different authors as a machine learning
[00:24:42.960 --> 00:24:49.200]   model. It's actually a relatively straightforward model, and we'll dive into some of its details.
[00:24:49.760 --> 00:24:55.360]   So this part of the talk really does cover the core structure of what we get out of the system,
[00:24:55.360 --> 00:25:00.320]   but we'll see that it's not kind of trainable directly by itself.
[00:25:00.320 --> 00:25:10.400]   So let's talk about what a state-space model is. So a state-space model is going to map
[00:25:10.400 --> 00:25:18.880]   a 1D input signal, which we're going to call ut, to an n-dimensional input signal.
[00:25:19.040 --> 00:25:26.720]   An nd, latent space, which we'll call xt. And then finally, it's going to map it back
[00:25:26.720 --> 00:25:33.680]   to a 1D output signal. So let me make some of the terms clear so we can work with them.
[00:25:33.680 --> 00:25:43.920]   So u is our input, and when I say signal, I mean it's a continuous function from 1D to 1D.
[00:25:43.920 --> 00:25:50.400]   So we're going to have a continuous time as our input, and u will be just a function that
[00:25:50.400 --> 00:25:56.800]   transforms time into some value. That's going to be our input to our system. So you can think
[00:25:56.800 --> 00:26:03.840]   about this as like the true waveform that comes into a speech system, or kind of the true kind
[00:26:03.840 --> 00:26:13.600]   of light in the world. Now, we're going to then map this to an nd latent state. So that is going
[00:26:13.600 --> 00:26:21.600]   to be a function from time to an n-dimensional vector, and we're calling that x. And that's
[00:26:21.600 --> 00:26:27.040]   going to be some hidden or latent state that keeps track of all the different properties we need
[00:26:27.040 --> 00:26:35.280]   in order to transform our signal over time. And finally, we're going to produce this output y,
[00:26:35.280 --> 00:26:42.240]   which again is a kind of continuous function over time that is a projection of this hidden state.
[00:26:42.480 --> 00:26:46.720]   Now, we're going to define this by the following equation. So this equation says that the
[00:26:46.720 --> 00:26:58.480]   derivative of x is equal to our transformation A of the signal, xt, plus B times our input ut.
[00:26:58.480 --> 00:27:04.960]   And then our output, the y signal, is this transformation or projection C of our latent
[00:27:04.960 --> 00:27:11.680]   state x. Now, the key kind of parameters that we need to take away from this system are A,
[00:27:12.400 --> 00:27:20.800]   B, and C. So A is going to be the transformation of the hidden state over time. B is the input,
[00:27:20.800 --> 00:27:26.880]   or what we take from the input signal u. And then C produces our output signal at the end.
[00:27:26.880 --> 00:27:33.440]   And the thing I want to note is that this system, while it looks familiar, is going to be continuous,
[00:27:33.440 --> 00:27:38.080]   and so it's different for folks who've kind of worked with kind of discretized systems
[00:27:38.080 --> 00:27:44.240]   that transform their values over time. And at the bottom, we're just going to have our
[00:27:44.240 --> 00:27:50.240]   parameter initialization. I just wrote this down to give you a sense of the sizes of the
[00:27:50.240 --> 00:27:56.480]   various objects. So A is going to be most of our parameters, it's n by n, whereas B and C
[00:27:56.480 --> 00:28:05.360]   just transform our input and then transform our output. Okay. Now, in practice, we're not going
[00:28:05.360 --> 00:28:13.280]   to actually work with the direct speech wave signal as a signal itself. We're going to discretize it,
[00:28:13.280 --> 00:28:17.920]   and we're going to discretize it based on some sampling rate. So what we're actually going to
[00:28:17.920 --> 00:28:26.880]   take in input is a sequence of these scalar values, u0 through ul minus 1. So in the case of
[00:28:26.880 --> 00:28:33.120]   speech, that will just be the sampled signal. In the case of language, it might be some embeddings
[00:28:33.120 --> 00:28:41.200]   or a dimension of an embedding. For images, it's just going to be each of the pixels in the way
[00:28:41.200 --> 00:28:49.280]   we described before. And the key thing we need here is we need a step size, delta, and that's
[00:28:49.280 --> 00:28:55.520]   going to represent basically our sampling rate for the underlying signal. Now, there are many ways to
[00:28:55.520 --> 00:29:01.520]   discretize these kind of models. One of them is this equation here, which is a bilinear transform.
[00:29:02.400 --> 00:29:09.360]   I won't go into the details too much except to note that this discretization turns our original
[00:29:09.360 --> 00:29:18.240]   matrices A, B, and C into these A bar, B bar, and C bar matrices. And these basically tell us how to
[00:29:18.240 --> 00:29:27.040]   approximate our original states-based model with a discretized states-based model with a kind of
[00:29:27.040 --> 00:29:31.840]   transformation of the parameters. And in fact, in practice, we're going to really be learning
[00:29:31.840 --> 00:29:37.920]   the underlying true parameters and doing this discretization on the fly at each time step.
[00:29:37.920 --> 00:29:42.800]   So at the bottom, I just have the code for how we do this in JAX or in NumPy,
[00:29:42.800 --> 00:29:47.280]   and it's a pretty straightforward application of matrix operations.
[00:29:47.280 --> 00:29:56.480]   Okay, so what does this give us? So the discretization from the states-based model
[00:29:56.480 --> 00:30:03.200]   to A bar, B bar, C bar gives us a new equation where instead of working with differential
[00:30:03.200 --> 00:30:10.080]   equations, we're now working with something that looks much more like an RNN. And in fact,
[00:30:10.080 --> 00:30:15.120]   in the paper, they kind of just view this as being a linear recurrent neural network.
[00:30:15.120 --> 00:30:21.600]   So I'm going to use subscripts to represent the discretized time steps. So now instead of working
[00:30:21.600 --> 00:30:28.960]   with a continuous signal u t, we're working with u sub k, which is the kind of sampled part of our
[00:30:28.960 --> 00:30:37.200]   original signal. We then transform that with B bar and then update our hidden state x sub k minus 1
[00:30:37.200 --> 00:30:43.360]   with our matrix A. We keep on doing this at each time step to produce the next step in our sequence.
[00:30:44.400 --> 00:30:52.560]   And just as a reminder, u and y here are still just scalar values, whereas this x k represents
[00:30:52.560 --> 00:30:57.120]   the hidden state and is it going to be an n-dimensional vector.
[00:30:57.120 --> 00:31:04.800]   So to implement this in JAX, we're just going to apply JAX scan. We have a little step function
[00:31:04.800 --> 00:31:14.160]   that just updates our internal state and returns our x k and y k values. So kind of as you would
[00:31:14.160 --> 00:31:18.800]   implement recurrent neural networks anywhere else in JAX, we have it in this function here.
[00:31:18.800 --> 00:31:28.400]   Okay, so let's do a quick tangent just to get a sense of what these models are and what they do.
[00:31:28.400 --> 00:31:34.480]   So to do this, we're going to look at an example from mechanics. So in this example here, we're
[00:31:34.480 --> 00:31:40.480]   going to have a mass. It's on a spring. And we're going to try to basically understand the position
[00:31:40.480 --> 00:31:49.360]   of the mass as we apply a force to it over time. So in this example, our input signal u is going
[00:31:49.360 --> 00:31:54.640]   to be the force that's applied to the mass, and it's going to be moving in one dimension. So its
[00:31:54.640 --> 00:32:02.800]   position y t along one dimension will change as a force is applied to it. We can write down the
[00:32:02.800 --> 00:32:11.120]   equation for this at the bottom here. This tells us basically how the mass m with spring constant k
[00:32:11.120 --> 00:32:17.520]   and friction constant v changes its position over time. So this equation here is just kind of a
[00:32:17.520 --> 00:32:26.000]   undergraduate mechanics equation relating the velocity acceleration and the position
[00:32:26.000 --> 00:32:33.120]   of an object over time. So normally, I guess if you looked at this equation, you would kind of
[00:32:33.120 --> 00:32:43.520]   convert it to a state-space model just to get intuition about what a state-space model does
[00:32:43.520 --> 00:32:50.480]   and what a hidden state might represent. So here we're going to represent this equation
[00:32:50.480 --> 00:32:59.040]   in matrix form. We write down our A matrix, which tells us how the basically hidden state
[00:32:59.040 --> 00:33:06.160]   in this model changes over time. We have a B matrix that tells us how the force impacts our
[00:33:06.160 --> 00:33:12.160]   velocity and position, and then a C matrix, which converts our hidden state to the final position.
[00:33:13.280 --> 00:33:20.160]   So in this case, again, the input u is one-dimensional. It's a force. The output y
[00:33:20.160 --> 00:33:26.160]   is one-dimensional. It's a position. And the hidden state x needs to encapsulate all the
[00:33:26.160 --> 00:33:32.400]   information we have about the current state of the object. And I believe in this particular example,
[00:33:32.400 --> 00:33:38.560]   x is two dimensions, and the dimensions are the current position and its current velocity. So
[00:33:38.560 --> 00:33:44.480]   that's what it's actually storing in the hidden state. So to implement this in our code, we simply
[00:33:44.480 --> 00:33:52.320]   kind of create these matrices, A, B, and C below. And then to run the model, we have to give it a
[00:33:52.320 --> 00:33:58.480]   signal u, representing the force applied to the object, and then we run our state-space model.
[00:33:58.480 --> 00:34:02.880]   So let's just look at this in a little more detail. So here's a function
[00:34:04.640 --> 00:34:10.640]   u. It produces a continuous output over time. I just made up some little function.
[00:34:10.640 --> 00:34:18.560]   And we transform that time to an output signal. Now, I wrote it as a scalar function,
[00:34:18.560 --> 00:34:29.120]   but in practice, we can use VMAP to actually convert it to a function over time. And yeah,
[00:34:29.120 --> 00:34:34.400]   sometimes in my code, I use np.vectorize, sometimes I use VMAP. I think they're both
[00:34:34.400 --> 00:34:40.800]   kind of nice operations. I like that vectorize actually gives you a kind of explicit signature
[00:34:40.800 --> 00:34:45.680]   for your functions, so you can have a sense of actually what's there or what you're utilizing.
[00:34:45.680 --> 00:34:52.880]   And yeah, it's kind of a nice way to utilize JAX in order to get kind of,
[00:34:53.920 --> 00:34:57.200]   turn simple functions into functions over vectors.
[00:34:57.200 --> 00:35:05.520]   Below, we have the actual code for running this model. So we have a bunch of constants
[00:35:05.520 --> 00:35:10.640]   representing the physical system. We have L, which is how long we run it for.
[00:35:10.640 --> 00:35:18.880]   Our step size here is one over L, so that just means we're sampling it at every one over L
[00:35:19.840 --> 00:35:28.080]   times the original signal. We use a range to actually compute each of the different time
[00:35:28.080 --> 00:35:36.800]   steps. And then we convert our u from a continuous signal to a discrete signal by actually taking the
[00:35:36.800 --> 00:35:44.480]   force values at each of the ks times delta time steps. So all the basic time steps between position
[00:35:44.480 --> 00:35:55.200]   zero and position one in time. Once we've done that, we simply call our scan SSM to run over it
[00:35:55.200 --> 00:36:01.440]   in practice. And actually, I think I dropped, this should be a scan of discretized SSM,
[00:36:01.440 --> 00:36:04.800]   because we're running it with the discretized version of the model.
[00:36:04.800 --> 00:36:11.920]   Cool. So just to put that into context, this is what it actually looks like when we run it
[00:36:11.920 --> 00:36:19.440]   in practice. So at the top, we have our force signal, which we've discretized into a hundred
[00:36:19.440 --> 00:36:25.840]   different positions from time step zero to one. That's the input signal that our model gets.
[00:36:25.840 --> 00:36:34.000]   We then have the position, yk, of the model, which changes over time as we apply force. So as we
[00:36:34.000 --> 00:36:39.040]   apply force, we push it further from the wall towards the center. We stop applying force,
[00:36:39.040 --> 00:36:44.320]   and so it starts getting pulled back to the wall itself. And we can actually see the movement of
[00:36:44.320 --> 00:36:50.800]   the object at the bottom of the slide. So not that exciting as a physics demo,
[00:36:50.800 --> 00:36:57.360]   but it gives you intuition about what the model is doing internally and how it's tracking the signal
[00:36:57.360 --> 00:37:02.560]   and memorizing what it did to push the model, the object forward.
[00:37:05.840 --> 00:37:17.680]   Great. So given this model, our goal is going to be to train a neural network that utilizes
[00:37:17.680 --> 00:37:25.600]   a state space model as its internal time series model. So I don't want to claim that this is
[00:37:25.600 --> 00:37:30.560]   going to just totally get rid of neural networks altogether. The actual model we're going to build
[00:37:30.560 --> 00:37:37.840]   is going to stack a bunch of feedforward layers with SSMs in between. So really what the SSM is
[00:37:37.840 --> 00:37:43.840]   doing is kind of replacing the self-attention layer of these networks. So basically, imagine
[00:37:43.840 --> 00:37:49.520]   a transformer, you pull out the self-attention, and you put some of these SSMs in between.
[00:37:49.520 --> 00:37:58.000]   But the problem is that so far, all I've described is a state space model that looks like a linear
[00:37:58.000 --> 00:38:03.440]   recurrent neural network. So we've seen that this kind of model is very fast for generation,
[00:38:03.440 --> 00:38:10.240]   but unfortunately, it's actually not that fast for training, and it's pretty difficult to train
[00:38:10.240 --> 00:38:18.480]   in practice. So we'd like to kind of get the model that I just described by training a different
[00:38:18.480 --> 00:38:26.160]   model that has the same underlying mathematical properties. So the key insight of this paper
[00:38:26.160 --> 00:38:29.680]   are going to be - or sorry, the key insight that we're going to utilize in the second half of this
[00:38:29.680 --> 00:38:38.320]   paper is that we can write a state space model as a convolutional neural network. So instead of kind
[00:38:38.320 --> 00:38:43.520]   of writing it as a recurrent neural network, we're going to write it as convolutions, which are slow
[00:38:43.520 --> 00:38:48.640]   for generation, but fast and more stable for training. And then the second property we're
[00:38:48.640 --> 00:38:55.280]   going to utilize is a method for initializing the parameters of this model that's just extremely
[00:38:55.280 --> 00:39:02.960]   effective and allows them to learn really long-term sequences. Cool. So let's talk through that.
[00:39:02.960 --> 00:39:09.680]   And I think probably the next three slides, if you're going to kind of focus in on the
[00:39:09.680 --> 00:39:16.320]   mathematical ideas, are probably the most critical thing to understand. This trick is really nice,
[00:39:16.320 --> 00:39:22.080]   and it's utilized in a lot of papers besides this one as well, but it really does kind of
[00:39:22.080 --> 00:39:28.640]   give you intuition about why this is going to be possible. So the key idea is that,
[00:39:28.640 --> 00:39:36.400]   unlike a recurrent neural network, or sorry, unlike a kind of non-linear recurrent neural
[00:39:36.400 --> 00:39:41.600]   network, or like an LSTM, or the kind of models that are common to use in this space,
[00:39:41.600 --> 00:39:50.720]   the SMM model is going to be fully linear. And because of that, we can apply this unrolling trick
[00:39:51.280 --> 00:39:58.160]   where we write out an equation for the hidden state based on all the previous calculations
[00:39:58.160 --> 00:40:05.120]   that we've done to get up to that point. So we look at this equation. This is saying that xk
[00:40:05.120 --> 00:40:13.520]   is equal to a bar times xk minus one plus buk. What that means is that at the first time step,
[00:40:14.160 --> 00:40:21.840]   we don't have a previous x negative one, so this first term is zero. So it's really just
[00:40:21.840 --> 00:40:34.720]   b times the input signal at that time, u sub zero. y zero is then the output matrix c
[00:40:35.520 --> 00:40:46.480]   times b times u zero. At time step x one, we're going to have a bar times x sub zero,
[00:40:46.480 --> 00:40:56.000]   which means if we substitute in x zero for that position, we now have a bar b bar times u sub zero
[00:40:56.720 --> 00:41:04.480]   plus the new u one that we've gotten at that time step. When we output y one,
[00:41:04.480 --> 00:41:10.000]   we multiply this output c bar times both terms here and here.
[00:41:10.000 --> 00:41:20.080]   If we do this step again, we just unroll one more term and we get a squared b u zero plus
[00:41:20.080 --> 00:41:33.280]   a bu one plus bu two. And we get a y output that has c a squared b u zero plus c ab plus cb.
[00:41:33.280 --> 00:41:42.000]   Now the key pattern to notice here is that the calculation of our output y two really doesn't
[00:41:42.000 --> 00:41:49.040]   need to know anything about the hidden state x. In fact, it just needs to know each of these values
[00:41:49.600 --> 00:41:57.360]   times the input u zero, u one, u two. And this value here is just going to be a scalar
[00:41:57.360 --> 00:42:03.040]   that changes only based on the powering up of a that happens at each time step.
[00:42:03.040 --> 00:42:11.920]   However, this does get extremely long. By the time we get to y l, the last value in our output
[00:42:11.920 --> 00:42:18.240]   sequence, we're going to have a term for every one of the previous u's that we have gotten.
[00:42:18.960 --> 00:42:23.600]   So if you have this kind of sliding window, the length of the window is going to be the
[00:42:23.600 --> 00:42:32.240]   length of the entire sequence. So if we have a sequence of length l where l is 16,000,
[00:42:32.240 --> 00:42:37.040]   we're going to have 16,000 different coefficients that we need to multiply
[00:42:37.040 --> 00:42:46.080]   by each of our different input values u zero through u l. Now this can be written as a kernel
[00:42:46.080 --> 00:42:52.400]   of length l, where this kernel, which we'll call k bar, is equal to this term below.
[00:42:52.400 --> 00:42:57.840]   And this kernel is really the kind of key mathematical aspect to take away from this work.
[00:42:57.840 --> 00:43:07.840]   We're going to basically create this kernel k by multiplying our discretized c times our discretized
[00:43:07.840 --> 00:43:15.200]   a to the power of the position times our discretized b. And if we have this, we multiply
[00:43:15.200 --> 00:43:21.360]   this last term by the first term in the sequence and keep on going across the sequence itself.
[00:43:21.360 --> 00:43:29.600]   So this kernel is a convolutional kernel, but it looks very different than kind of the convolutions
[00:43:29.600 --> 00:43:34.560]   that we typically use in areas like computer vision, just because it doesn't have a small
[00:43:34.560 --> 00:43:42.720]   fixed size. Its size is the entire length of the sequence itself. Note that it also kind of
[00:43:42.720 --> 00:43:46.160]   gets applied at every position, so it slides all the way across,
[00:43:46.160 --> 00:43:50.480]   kind of going outside the boundaries of the input itself.
[00:43:50.480 --> 00:43:58.880]   So luckily, this sort of convolution is exactly the kind of thing that we use Fourier transforms
[00:43:58.880 --> 00:44:05.360]   to efficiently compute. Because the convolution is so big, we don't have to compute it by actually
[00:44:05.360 --> 00:44:12.000]   sliding the window and doing this multiplication. We can compute a Fourier transform of the kernel k
[00:44:12.720 --> 00:44:19.600]   a Fourier transform of the input signal u, apply them together to compute our output sequence y.
[00:44:19.600 --> 00:44:24.240]   I'm sorry, I keep on saying signal, I mean to say sequence. So it's a convolution of our
[00:44:24.240 --> 00:44:32.560]   input sequence by our kernel k bar. So we can look at this in two ways. So they're both equivalent.
[00:44:32.560 --> 00:44:38.960]   So one is just applying a standard convolution between our input sequence and this very,
[00:44:38.960 --> 00:44:46.000]   very long kernel that we computed. If you run it this way, probably most of the time it will
[00:44:46.000 --> 00:44:52.640]   actually try to compute the convolution as a sliding window. If you compute it this way,
[00:44:52.640 --> 00:45:01.280]   it instead applies to a fast Fourier transform, then applies the convolution in Fourier space,
[00:45:01.280 --> 00:45:09.600]   and then converts it back to our output y. And so this actual convolution can be computed
[00:45:09.600 --> 00:45:14.240]   much more efficiently and allows us to do efficient training for this model.
[00:45:14.240 --> 00:45:23.280]   Okay, so there's one last trick and then I will stop for questions. So the
[00:45:27.440 --> 00:45:34.480]   final thing that we have to note is that you can apply this model and you'll get basically
[00:45:34.480 --> 00:45:38.560]   what you might expect from running a linear recurrent neural network.
[00:45:38.560 --> 00:45:43.520]   That is, linear recurrent neural networks by themselves are just not very good.
[00:45:43.520 --> 00:45:48.320]   If linear recurrent neural networks just did awesomely on these tasks, we would have known
[00:45:48.320 --> 00:45:56.480]   about it like years ago. And in fact, if you try to apply this model to MNIST classification,
[00:45:56.480 --> 00:46:04.480]   you only get about 50 percent accuracy. However, if you initialize your hidden states in a particular
[00:46:04.480 --> 00:46:11.600]   way, which is known as a HIPPO matrix, you can improve this number to about 98 percent accuracy.
[00:46:11.600 --> 00:46:19.520]   And this is done by the matrix I'm showing here. It's a pretty simple matrix, but it has this kind
[00:46:19.520 --> 00:46:25.280]   of really nice property that it's able to improve the performance of these models.
[00:46:25.760 --> 00:46:31.680]   Now, the derivation of this matrix and why it performs so well is really kind of very much
[00:46:31.680 --> 00:46:36.720]   beyond the scope of this talk and this blog post. But let me try to give you a little bit of a
[00:46:36.720 --> 00:46:45.120]   sense of what's going on. So the intuition that you want to have is that this matrix
[00:46:45.120 --> 00:46:52.320]   is trying to keep track of everything it's seen previously in you. So it's trying to produce
[00:46:52.320 --> 00:47:00.400]   basically a summary of the function u over time. And if we can produce a summary of the function
[00:47:00.400 --> 00:47:07.840]   u, then we don't need to remember everything about u. We can just keep track of the coefficients,
[00:47:07.840 --> 00:47:15.520]   the n different coefficients in our latent state over time. So here we have these n different
[00:47:15.520 --> 00:47:24.720]   coefficients. That's just a matrix of size one by n. And then here is our u. And the argument
[00:47:24.720 --> 00:47:30.480]   put forth in the HIPPO paper is that this matrix, this particular set of equations here,
[00:47:30.480 --> 00:47:39.520]   computes an approximation to this function by storing coefficients of a Legendre polynomial.
[00:47:39.520 --> 00:47:47.680]   So each of these represent basically the set of underlying basis functions that it's using.
[00:47:47.680 --> 00:47:54.000]   And if you multiply this value by this function and this value by this function and sum them up
[00:47:54.000 --> 00:48:01.760]   all together, you'll get something close to this red line. So in that way, we're able to store
[00:48:01.760 --> 00:48:06.800]   enough information in our hidden state that we don't need to save everything about the u we've
[00:48:06.800 --> 00:48:13.200]   seen so far. We just save these coefficients. And if we wanted to, we could reconstruct the
[00:48:13.200 --> 00:48:20.080]   underlying function behind it. So this initialization gives us a kind of way that we
[00:48:20.080 --> 00:48:25.280]   can utilize a recurrent neural network to get the kind of properties that we'd like in a transformer,
[00:48:25.280 --> 00:48:41.040]   the kind of the memory of our signal over time. Cool. So let me - well, actually,
[00:48:41.040 --> 00:48:44.000]   before I get here, let me just pause for a sec if there are any questions.
[00:48:51.200 --> 00:48:57.600]   Let's see. I think there are none. There's a discussion going on in the YouTube chat.
[00:48:57.600 --> 00:49:01.440]   People are answering each other's questions. Okay. Sounds great.
[00:49:01.440 --> 00:49:05.760]   Christian, did you have any that you wanted to highlight from here?
[00:49:05.760 --> 00:49:14.320]   No. I personally, when I first saw the Hippomatrix, I was kind of, what is this?
[00:49:17.920 --> 00:49:24.080]   Like, what I have trouble understanding, it's like a triangular matrix. So that makes sense
[00:49:24.080 --> 00:49:28.880]   because, I mean, the first one should only see probably itself and that kind of stuff.
[00:49:28.880 --> 00:49:36.480]   And the diagonal is n plus one. That's kind of what I get. The other term is like,
[00:49:36.480 --> 00:49:45.920]   it seems like black magic. Yeah. I don't actually have a simple way to understand this matrix.
[00:49:46.080 --> 00:49:54.240]   I think that the Hippo paper does kind of go through the derivation for why it obeys these
[00:49:54.240 --> 00:49:59.200]   properties. And so I think that's a good way to go through it. But I don't think there's going
[00:49:59.200 --> 00:50:04.240]   to be anything I can tell you that's like, makes it super clear why it's like the right starting
[00:50:04.240 --> 00:50:09.680]   point for this system. I do have a question. I guess this is in general,
[00:50:09.680 --> 00:50:13.760]   because even if you have this clever matrix, it's still kind of a linear system.
[00:50:15.200 --> 00:50:23.200]   That's right. Yeah. So linear systems are powerful as long as you get the right number. That's kind
[00:50:23.200 --> 00:50:31.040]   of the idea. Yeah. I think that's what's so startling about this is that this really is
[00:50:31.040 --> 00:50:38.960]   just a kind of linear system. And I don't think there's any inherent reason why a linear system
[00:50:38.960 --> 00:50:50.320]   can't work. It's just surprising that it does. Yeah. I should be clear though, that this is again,
[00:50:50.320 --> 00:50:56.800]   just the time series part of this model. So, and actually maybe that's a good time to talk
[00:50:56.800 --> 00:51:02.400]   about what the model actually looks like. So what we're going to do is we're going to build a layer.
[00:51:02.400 --> 00:51:10.560]   So this is a kind of an S4 layer, and this is what it looks like. So this part is going to have A,
[00:51:10.560 --> 00:51:17.520]   which is our HIPPO matrix. We're going to have N and L, and then we're going to learn three parts
[00:51:17.520 --> 00:51:23.680]   of this. So we're going to learn B, we're going to learn C, and we're going to learn our step size.
[00:51:23.680 --> 00:51:30.400]   So learning the step size allows us to kind of learn different kind of varying sequences of the
[00:51:30.400 --> 00:51:38.160]   underlying model. So kind of like have different SSM layers kind of act at different scales.
[00:51:38.160 --> 00:51:44.560]   Then really this is the code we're going to discretize, and we're going to compute K.
[00:51:44.560 --> 00:51:49.600]   And the one thing I want to note is that this is a little different than what we normally do
[00:51:49.600 --> 00:51:56.160]   in kind of neural network models. This computation all happens during the setup phase.
[00:51:57.200 --> 00:52:04.800]   So none of this actually depends on our input. It's just basically reconstructing the kernel
[00:52:04.800 --> 00:52:09.920]   every time the parameters change. And so at test time, you actually don't need to rerun this at
[00:52:09.920 --> 00:52:16.320]   all. The only thing we actually run at test time is just this convolution. So it really does just
[00:52:16.320 --> 00:52:22.560]   look like calling this convolution function for our input. Now, as I mentioned earlier, though,
[00:52:23.920 --> 00:52:28.560]   we want a lot of these, and we want to put them within another nonlinear system.
[00:52:28.560 --> 00:52:34.320]   So we're going to have a bunch of SSM layers, then we're going to have a big fat feed-forward
[00:52:34.320 --> 00:52:38.800]   network, and then we're going to have a bunch of SSM layers, and then a feed-forward network.
[00:52:38.800 --> 00:52:41.920]   So I'm not saying that you don't need nonlinearities. It's just that you don't
[00:52:41.920 --> 00:52:44.880]   need nonlinearities in the kind of time series part of the model.
[00:52:44.880 --> 00:52:52.640]   Cool. Now, the other thing I want to note about this code, and maybe everyone here kind of knows
[00:52:52.640 --> 00:52:59.600]   this already and thinks it's cool, but this implementation is entirely scalar.
[00:52:59.600 --> 00:53:07.120]   This is a scalar time sequence that gets us input. And so to actually run this thing,
[00:53:07.120 --> 00:53:14.960]   we're going to need a ton of these and a ton of them per batch. So in the PyTorch code,
[00:53:14.960 --> 00:53:19.280]   you have to kind of keep track of a bunch of extra dimensions to keep track of all the different
[00:53:19.280 --> 00:53:24.960]   copies of the parameters and keep track of the batching and all that. But we're just going to do
[00:53:24.960 --> 00:53:34.480]   that by using Flax. So I think this is kind of a really neat aspect that Flax/Linnen has built in,
[00:53:34.480 --> 00:53:39.600]   which are these lifted transformations. So we're going to apply two different lifted
[00:53:39.600 --> 00:53:49.120]   transformations. The first is a VMAP that produces new parameters with their own random initializations.
[00:53:49.920 --> 00:53:55.840]   So in order to work with, say, a time series with many different channels,
[00:53:55.840 --> 00:54:03.440]   we're going to have each different channels that we create by simply lifting the layer I showed
[00:54:03.440 --> 00:54:09.360]   before such that each of them have their own parameters and the parameters are initialized
[00:54:09.360 --> 00:54:16.640]   randomly. Then to get batching, we're going to lift it again, but this time not creating
[00:54:16.640 --> 00:54:22.960]   new parameters and not creating new initializations. So throughout the code, we basically have all the
[00:54:22.960 --> 00:54:27.680]   kind of math part of it separated from these kind of lifting aspects and the neural network part.
[00:54:27.680 --> 00:54:37.600]   Sasha, can you explain? Because I think this is one of the most interesting
[00:54:39.200 --> 00:54:45.920]   aspects, but it seems, I guess, a little bit magical from the outside.
[00:54:45.920 --> 00:54:54.960]   Can you explain what the lifting is doing to the parameters?
[00:54:54.960 --> 00:55:01.840]   Yeah, yeah. I agree. The syntax is a little magical, but let me try to describe. So
[00:55:05.520 --> 00:55:14.880]   this is a FLOX module. It's taking in as input a scalar value and outputting a scalar value.
[00:55:14.880 --> 00:55:22.240]   But let's say I wanted it to take in a multivariate time series of 100 values per
[00:55:22.240 --> 00:55:27.920]   time step and output a multivariate time series of 100 values per time step.
[00:55:27.920 --> 00:55:34.800]   And I wanted each of those to be computed with an SSM with different parameters.
[00:55:35.760 --> 00:55:41.280]   So that would be a kind of typical use case that you'd have in a neural network. You're just going
[00:55:41.280 --> 00:55:52.560]   to have basically 100 of these getting applied as you go. One way to do that would be to add
[00:55:52.560 --> 00:55:59.920]   an extra h dimension to each of these parameters and then keep track of that h dimension throughout.
[00:56:00.640 --> 00:56:07.760]   That would let us learn an h by n by one parameter matrix. And that is how you'd implement it in
[00:56:07.760 --> 00:56:15.680]   PyTorch. This lifting transformation here though allows us to say that we're automatically
[00:56:15.680 --> 00:56:23.200]   going to VMAP this layer such that whatever the h that it got in its input, let's say there are
[00:56:23.200 --> 00:56:29.280]   kind of 100 different dimensions of our time series, you're going to have 100 different
[00:56:29.280 --> 00:56:36.320]   randomly initialized parameters. So 100 copies of this layer, each of them with different
[00:56:36.320 --> 00:56:43.280]   parameters. So that's what this says. It says take the input axis one, which in this case is 100,
[00:56:43.280 --> 00:56:50.240]   and produce an input and output axis one, which is also of size 100. And along that axis have
[00:56:50.240 --> 00:56:56.240]   a split of our parameters and make sure that they're randomly initialized separately.
[00:56:58.160 --> 00:57:04.400]   Now contrast that with batching. So in batching you want to have the same parameters but get
[00:57:04.400 --> 00:57:10.240]   applied to a batch of many examples. So in batching we'd have say a batch of a 100,
[00:57:10.240 --> 00:57:15.200]   say a thousand different time series coming in, but we don't want different parameters here.
[00:57:15.200 --> 00:57:22.640]   So to do that in flex you say the batch axis is dimension zero and it will be dimension zero when
[00:57:22.640 --> 00:57:29.360]   it comes out. Don't use different parameters for each of that axis so they're shared,
[00:57:29.360 --> 00:57:35.280]   and don't randomly initialize them differently. So there are two different uses of the same
[00:57:35.280 --> 00:57:40.400]   function based on how we wanted to actually treat the dimensions that doesn't actually get
[00:57:40.400 --> 00:57:47.680]   passed in to this model. Does that make sense Christian? Yeah, no I just because when I saw it
[00:57:47.680 --> 00:57:54.320]   I was like oh wow this is really neat. And like you have all these implementations like for example
[00:57:54.320 --> 00:58:00.960]   like multi-head attention and that kind of stuff that it's like you could also do this trick.
[00:58:00.960 --> 00:58:09.280]   That's right, yeah. But I don't know people usually don't, but it looks really nice when
[00:58:09.280 --> 00:58:13.680]   you see it like ah he implemented it for a single one and then it's suddenly
[00:58:14.960 --> 00:58:22.960]   like a multi-head and then batched. I found it really cool this trick you did. Yeah, I think
[00:58:22.960 --> 00:58:29.440]   it's really neat. The flex linen documentation does have a multi-headed attention example that
[00:58:29.440 --> 00:58:34.240]   does exactly what you described, but honestly like there was so much going on I had a lot of trouble
[00:58:34.240 --> 00:58:41.360]   kind of parsing out exactly how it works. But it is really cool once you see it. I should also
[00:58:41.360 --> 00:58:46.160]   note that they have a bunch of these transformations so you can also do nn.scan.
[00:58:46.160 --> 00:58:53.920]   It does a similar thing. Yeah, so yeah lifted transformations.
[00:58:53.920 --> 00:59:03.200]   I'm just gonna say it for the if somebody sees the video. So you can do this in like Bear Jacks,
[00:59:04.880 --> 00:59:14.400]   but the Flex team has like made them like know about modules basically. So you can technically
[00:59:14.400 --> 00:59:23.040]   do all this by hand. It's just that it's a lot of work and they constructed these nice APIs that
[00:59:23.040 --> 00:59:30.640]   they sort of transform the whole module. It's really cool and sometimes I've been seeing
[00:59:30.640 --> 00:59:37.840]   discussions around how for example if you have multiple layers that have the same input, outputs,
[00:59:37.840 --> 00:59:44.560]   and parameters shapes. Like for example, yeah transformer stack or something like that.
[00:59:44.560 --> 00:59:56.560]   You can use scan for example to suddenly do like the multiple applications and then that is
[00:59:56.560 --> 01:00:06.160]   also super efficient. Like it's optimized by XLA stuff like that. It's really interesting.
[01:00:06.160 --> 01:00:10.960]   Yeah, I tried a little bit doing that transformation as well. I think the downside
[01:00:10.960 --> 01:00:19.200]   is that it causes your JIT time to get a lot higher. Actually, I don't know. I was reading
[01:00:19.200 --> 01:00:26.400]   a conversation where scan actually decreases it by 10x. Well, I feel like I maybe read the opposite.
[01:00:26.400 --> 01:00:32.400]   Yeah, I read it yesterday. So I was like oh it's actually even better because
[01:00:32.400 --> 01:00:42.960]   since it's not unrolled it can actually say hey this is just a single thing like applied multiple
[01:00:42.960 --> 01:00:50.160]   times I guess with different parameters. So the code internally it's simpler whereas if you have
[01:00:50.160 --> 01:00:55.760]   like a for loop doing it it would try to optimize like a lot of stuff. So yeah.
[01:00:55.760 --> 01:01:00.160]   So we actually do have an instance of this in this paper and I was at...
[01:01:00.160 --> 01:01:04.000]   So let's see.
[01:01:04.000 --> 01:01:07.440]   Oh how do I switch?
[01:01:10.640 --> 01:01:15.600]   Let's see.
[01:01:15.600 --> 01:01:28.320]   Okay, so this is a blog post and this came up actually and I think I filed some issues about
[01:01:28.320 --> 01:01:34.640]   it. So let me... maybe people are interested. So this is an example of that where we have
[01:01:35.440 --> 01:01:41.920]   a bunch of these layers one after another and they're defined in this way and then we have a
[01:01:41.920 --> 01:01:46.960]   for loop that applies them one at a time like this, right? So this is exactly an example where
[01:01:46.960 --> 01:01:55.440]   you'd really like to instead have this be a lifted nn.scan and then apply them each as you go. And
[01:01:55.440 --> 01:02:01.120]   one thing that's really neat about this is that remember I said that actually a lot of the work
[01:02:01.120 --> 01:02:09.680]   for this layer happens during initialization? Well in theory nn.scan could vmap the initialization.
[01:02:09.680 --> 01:02:15.200]   So you have maybe 12 layers. They all get initialized because there's no serial dependency
[01:02:15.200 --> 01:02:20.400]   at initialization. They could all be initialized together and then you only apply scan here.
[01:02:20.400 --> 01:02:24.880]   So a lot of really neat optimizations that are kind of facilitated by kind of
[01:02:25.600 --> 01:02:29.600]   that sort of structure. Yeah. It didn't work or it was slow?
[01:02:29.600 --> 01:02:36.880]   I think it didn't work but I think I filed an issue and then never got back to it.
[01:02:36.880 --> 01:02:45.360]   That's strange. Yeah, no actually yesterday I read somebody that it says, oh no, it's even better.
[01:02:45.360 --> 01:02:51.920]   Yeah, I'll give it a try. Or if someone wants to send a PR who's interested in this, let me know.
[01:02:53.760 --> 01:02:58.960]   You should share maybe. Ah, no, the link of the code is in the blog post. Yeah,
[01:02:58.960 --> 01:03:04.880]   no, it's really interesting. I think it's kind of a different way of thinking about
[01:03:04.880 --> 01:03:16.320]   how you do stuff. This like Jaxx transformations I guess. Yeah, it's a little bit of a learning
[01:03:16.320 --> 01:03:21.040]   curve but then suddenly you get extra powers. I don't know. Yeah, I think that's right.
[01:03:21.520 --> 01:03:32.080]   Cool. Okay, so let me, I have one other interesting thing that came up which was that
[01:03:32.080 --> 01:03:39.920]   the other kind of really neat aspect of this model is that I mentioned it has two forms, right? So
[01:03:39.920 --> 01:03:46.800]   there's the CNN form during training and then there's an RNN form which you want to utilize
[01:03:46.800 --> 01:03:51.840]   during test time. So you can actually have another implementation of the same layer
[01:03:51.840 --> 01:03:58.640]   with all the same parameters. So all this code here, exactly the same. But now we're not going
[01:03:58.640 --> 01:04:06.880]   to actually kind of convert it to be a CNN. We're going to instead discretize it to a discrete
[01:04:06.880 --> 01:04:14.240]   state-space model. And then down here we're going to run our RNN code that actually goes over the
[01:04:14.240 --> 01:04:22.720]   state-space model as an RNN. But when running an RNN during test time, you're going to have to
[01:04:22.720 --> 01:04:30.560]   basically generate and then feed back what you put back into the model. And what that means is you're
[01:04:30.560 --> 01:04:36.880]   going to run the same model L times in a row if you want to generate a sequence. Now in practice,
[01:04:36.880 --> 01:04:41.920]   if you're generating a sequence of length 16,000, you're going to be calling this function again and
[01:04:41.920 --> 01:04:48.160]   again and again. So the way that Flax handles this is it has this thing called self.variable.
[01:04:48.160 --> 01:04:55.600]   And self.variable allows you to save state inside of your model itself.
[01:04:55.600 --> 01:05:02.400]   So this took me a lot of time to figure out, but it is actually pretty cool. So what this does is
[01:05:02.400 --> 01:05:11.440]   it lets us save a state of what our current RNN is at, basically the current XK, so that we can
[01:05:11.440 --> 01:05:18.640]   then utilize that state in the next call. So this saves the state, this utilizes the state as we go.
[01:05:18.640 --> 01:05:24.960]   Now just by itself this wouldn't be that interesting, but because it plays nicely
[01:05:24.960 --> 01:05:30.720]   with all these transformations, this becomes quite simple. It just becomes saving n values,
[01:05:30.720 --> 01:05:38.400]   the current X thing, but when we vmap it, it ends up saving batch times hidden state times n values
[01:05:38.400 --> 01:05:43.760]   throughout the whole model itself. So we get kind of locality and caching that then plays nicely
[01:05:43.760 --> 01:05:52.160]   with the kind of lifted transformation. Okay, cool. So let's see. So I don't want to go too
[01:05:52.160 --> 01:05:58.880]   much longer. I've gone for a while now. I think what I'm going to do is I'm going to skip part two.
[01:05:58.880 --> 01:06:06.880]   So if you're interested in the actual math of the system, note that the convolutional function I
[01:06:06.880 --> 01:06:16.400]   wrote earlier is extremely naive. So this function here, extremely slow. And so everything was based
[01:06:16.400 --> 01:06:22.720]   on this function and all the code before, while technically correct, relied on computing this K
[01:06:22.720 --> 01:06:28.880]   conv. So what's really cool about this paper S4 is it gives you this really neat approach for
[01:06:28.880 --> 01:06:36.240]   calculating K conv exactly efficiently. And the way to do that plays really nicely with a bunch
[01:06:36.240 --> 01:06:41.040]   of JAX functions. So if you're interested in those details, you can come check it out and see
[01:06:41.040 --> 01:06:47.120]   kind of how JAX JIT makes it possible to actually implement this efficiently. But what I do want to
[01:06:47.120 --> 01:06:52.720]   do is dive into results. So I'm just going to skip through this guy and we'll talk a little bit about
[01:06:52.720 --> 01:07:00.880]   results. So the code we have online does a lot of really cool things. So it can classify images
[01:07:00.880 --> 01:07:08.640]   by treating the pixels as a sequence. So thank you, Weights and Biases, for having me here today.
[01:07:08.640 --> 01:07:14.160]   We have your system implemented. You can try it out. So this is it getting roughly
[01:07:14.160 --> 01:07:19.840]   kind of close to state of the art on a sequential version of the CIFAR dataset.
[01:07:19.840 --> 01:07:26.800]   But as people have noted, this kind of classification is better to be done with CNNs.
[01:07:28.640 --> 01:07:33.680]   What's particularly cool is that we can use the RNN form to actually generate sequences.
[01:07:33.680 --> 01:07:41.520]   And so we ran experiments on generating images, generating drawings, and generating speech.
[01:07:41.520 --> 01:07:47.760]   So in each of these examples, we're going to use the RNN form and we're going to basically sample
[01:07:47.760 --> 01:07:54.400]   some output sequence and then treat it as an image or as a waveform. So here's the code.
[01:07:54.400 --> 01:08:01.600]   So here we're going to use the Jax-4i loop to basically generate a bunch of things in a row.
[01:08:01.600 --> 01:08:09.440]   We're going to do that by getting a random value for each pixel, applying our model. And as I
[01:08:09.440 --> 01:08:15.440]   mentioned before, we're applying it with a cache so it remembers what the hidden state was at each
[01:08:15.440 --> 01:08:22.160]   time step. And the cache is mutable, which means that we're going to update the cache based on the
[01:08:22.160 --> 01:08:29.600]   next pixel in our sequence. So we're going to apply this basically 16,000 times, each time
[01:08:29.600 --> 01:08:35.600]   kind of getting our next output while caching the next value in our RNN. Based on our output,
[01:08:35.600 --> 01:08:41.840]   we then sample the next pixel and set it in our matrix. And then we kind of apply this again.
[01:08:41.840 --> 01:08:50.400]   So this is basically how we run a kind of stateful RNN process with a kind of stacked S4 layer
[01:08:50.400 --> 01:08:57.280]   using Jax and Flax. And the output of this is something like this. It started from here. It
[01:08:57.280 --> 01:09:03.280]   just sampled. And you get basically something that looks like an image. Here you actually
[01:09:03.280 --> 01:09:11.760]   get a 4L just by kind of sampling from this RNN process. For these examples, we had it complete
[01:09:11.760 --> 01:09:18.480]   images. So here we give it all the pixels in white and have it sample the rest in red
[01:09:19.120 --> 01:09:24.080]   and compare it to the true image. So it does look different, but most of the time it does
[01:09:24.080 --> 01:09:28.800]   kind of reconstruct the original number. So here are some more examples.
[01:09:28.800 --> 01:09:37.440]   We also applied it to QuickDraw. So this is a version of QuickDraw where you
[01:09:37.440 --> 01:09:41.920]   basically have a picture of a coat and it has to generate the kind of completion below.
[01:09:41.920 --> 01:09:48.080]   So they're not always good. This one kind of misses it, but you get some ideas.
[01:09:48.240 --> 01:09:53.360]   And finally, the last experiment we ran was generating sound. I unfortunately couldn't
[01:09:53.360 --> 01:09:58.000]   figure out how to get sound working in my browser, but you can look at the waveform.
[01:09:58.000 --> 01:10:03.040]   So in these examples, you really are generating a sequence that's like 16,000 steps long.
[01:10:03.040 --> 01:10:09.760]   Each time step here, you're kind of generating a value between 0 and 256, representing a kind of
[01:10:09.760 --> 01:10:16.160]   discretized version of the waveform. And you get - most of these actually look pretty good. And
[01:10:16.160 --> 01:10:19.360]   if you listen to them, it'll actually sound like someone's saying a digit
[01:10:19.360 --> 01:10:22.640]   compared to the original true value.
[01:10:22.640 --> 01:10:31.360]   Okay, so I'll end there. I'll note the following. This was a really kind of good exploration for
[01:10:31.360 --> 01:10:40.320]   JAX. I don't know what I meant to write here, but JAX really shines at modular mathematical code.
[01:10:40.320 --> 01:10:48.640]   So particularly kind of getting things in this form lets us test it. It lets us basically confirm
[01:10:48.640 --> 01:10:53.120]   properties and make sure the mathematical properties are true. And it separates out the
[01:10:53.120 --> 01:10:58.320]   kind of mathematical structure from the neural network. We didn't talk about this so much,
[01:10:58.320 --> 01:11:02.640]   but some of the kind of slow operations become really fast when you apply a JIT.
[01:11:02.640 --> 01:11:08.720]   And then lifting and flax actually lets us turn that mathematical code into a neural network.
[01:11:10.160 --> 01:11:13.680]   So one other thing I'll note is that because we wrote it in this form,
[01:11:13.680 --> 01:11:19.200]   it's allowed us to basically kind of use this same structure as new papers on this topic came out.
[01:11:19.200 --> 01:11:24.400]   So just a couple days ago, this paper called "Diagonal State Spaces" came out,
[01:11:24.400 --> 01:11:29.840]   and they showed that you were able to do basically almost as well as S4,
[01:11:29.840 --> 01:11:35.040]   but basically ignoring part two of this talk and just focusing on part one with a kind of
[01:11:35.040 --> 01:11:41.680]   more structured model. And this is basically the code for this paper. It's these three functions
[01:11:41.680 --> 01:11:45.920]   plus part one of this talk. And we were actually able just to get this up and running
[01:11:45.920 --> 01:11:52.960]   basically in a couple hours and get replicated results from the paper itself. So kind of it's
[01:11:52.960 --> 01:11:57.040]   nice to have this structure and actually basically have it running. And this is all checked in on
[01:11:57.040 --> 01:12:02.320]   the repo if you want to play with it. So thanks very much for listening and a huge thanks to
[01:12:02.880 --> 01:12:07.920]   the authors of this paper, the authors of the DSS paper, and a bunch of our open source
[01:12:07.920 --> 01:12:15.520]   contributors to the project itself. >> Amazing, Sasha. Thanks a lot.
[01:12:15.520 --> 01:12:20.640]   Do we have questions, Amnim? >> Sasha, if you have time,
[01:12:20.640 --> 01:12:23.280]   I'll bring up just one question. >> Sure, yeah.
[01:12:23.280 --> 01:12:30.800]   >> So this question is by Will Z. The S4 paper says something about nonlinearities popping up
[01:12:30.800 --> 01:12:36.560]   between layers. Even if it's building blocks are linear, where does that happen?
[01:12:36.560 --> 01:12:42.400]   >> Yeah, absolutely. So that was roughly what I... And actually, I don't have a slide for this. Let
[01:12:42.400 --> 01:12:53.600]   me stop sharing and then we can share this again. So I do want to mention that... Oops,
[01:12:53.600 --> 01:12:59.120]   that's the wrong tab. What I showed again is just kind of the intermediate, like the part
[01:12:59.680 --> 01:13:07.200]   is replacing a tension. But in practice, you still have kind of nonlinearities at every
[01:13:07.200 --> 01:13:15.360]   intermediate position. So here's what our model actually looks like. So it's a stacked model.
[01:13:15.360 --> 01:13:26.960]   It has a dense encoder, decoder to start, and then it's a block of sequences. So this is our input,
[01:13:27.600 --> 01:13:32.640]   this is our intermediate layers. So we have a self.layers numbers of these,
[01:13:32.640 --> 01:13:37.680]   and then we classify, and then we kind of produce a softmax as our output.
[01:13:37.680 --> 01:13:44.720]   Now, if we look into one of these sequence blocks, we can see that it has several things. It has
[01:13:44.720 --> 01:13:55.600]   dense, it has layer norm, and it has dropout, and it has a GALU. So it has all these things.
[01:13:56.160 --> 01:14:05.600]   And then it also has self.seq. So this is the S4 part followed by basically a little residual
[01:14:05.600 --> 01:14:11.440]   nonlinearity feedforward network here. So we kind of alternate between applying the sequence over
[01:14:11.440 --> 01:14:17.440]   time, then at each position, applying a nonlinearity and a feedforward, then kind of
[01:14:17.440 --> 01:14:23.200]   applying the sequence again. Does that make sense? So this part's all kind of independent of the S4
[01:14:23.200 --> 01:14:27.920]   contribution. >> Yeah. So it's like a
[01:14:27.920 --> 01:14:35.520]   transformer block basically, with removed self-attention. >> That's right. And in fact,
[01:14:35.520 --> 01:14:39.840]   I think in the paper, they literally have some experiments where it's just a transformer block
[01:14:39.840 --> 01:14:47.600]   where they remove self-attention. >> Yeah. Awesome. I had a personal question.
[01:14:49.520 --> 01:14:59.040]   If you've been tracking the field, where is this going, the state-space, I guess, approach?
[01:14:59.040 --> 01:15:06.560]   Like, is it evolving? >> Yeah. So I mentioned this paper,
[01:15:06.560 --> 01:15:10.240]   this diagonal state-space model. Sorry, you can't see my screen.
[01:15:12.800 --> 01:15:21.360]   So this literally just came out a couple of days ago. And I think there's been other papers from
[01:15:21.360 --> 01:15:28.000]   the original author on various applications like speech processing. But this is the paper I
[01:15:28.000 --> 01:15:36.400]   mentioned before, kind of argues that you can get away with a much simpler structure. So that's
[01:15:36.400 --> 01:15:41.840]   pretty interesting. It kind of reduces some of the kind of mental complexity of understanding
[01:15:41.840 --> 01:15:50.720]   these models, which is nice. I think that like, I don't know, I'd like to see people try pre-training
[01:15:50.720 --> 01:15:57.040]   with these approaches. I'd like to see them try video or kind of trajectory planning sort of
[01:15:57.040 --> 01:16:02.560]   approaches. It'd be nice to understand what the limits of these kind of applications are.
[01:16:02.560 --> 01:16:08.560]   And I think one of the reasons why I'm excited to talk about them and implement them is to kind of
[01:16:09.280 --> 01:16:12.560]   reduce the barrier of entry for people to try them out on their own problems.
[01:16:12.560 --> 01:16:21.760]   >> Okay. It's really cool. This is something that I thought while I was reading your blog post.
[01:16:21.760 --> 01:16:27.440]   Are there intuitions onto like what kind of dynamics happen inside this block?
[01:16:27.440 --> 01:16:37.920]   >> Yeah. So one thing I've heard people mention is that the step size seems to matter a lot.
[01:16:37.920 --> 01:16:44.320]   And each of the different blocks end up having quite different step sizes. So unlike with an RNN,
[01:16:44.320 --> 01:16:48.480]   where everything has to happen kind of simultaneously on top of each other,
[01:16:48.480 --> 01:16:53.920]   step size does give you a sense of like, oh, this SSM is managing long range and this one
[01:16:53.920 --> 01:17:02.400]   is managing short term. It's one interesting idea. There are some visualizations in the paper on some
[01:17:02.400 --> 01:17:07.280]   of the tasks, but it's a little hard to kind of figure out exactly what's going on or what they
[01:17:07.280 --> 01:17:13.680]   mean. And particularly when you talk about speech, you can maybe make an argument that like the
[01:17:13.680 --> 01:17:18.560]   state-based dynamics have some sort of meaning, they're capturing some sort of lower frequencies
[01:17:18.560 --> 01:17:25.120]   or high frequencies or about. But when you start working with like linearized images or text data,
[01:17:25.120 --> 01:17:32.640]   it's a little unclear what they could be capturing. >> Yeah. Yeah. It's really cool,
[01:17:32.640 --> 01:17:37.280]   especially since in the blog you started with the actual like real dynamical system. So
[01:17:37.280 --> 01:17:49.120]   I guess that brings up ideas into, oh, is it kind of learning like some behavior that
[01:17:49.120 --> 01:17:56.160]   might be useful? Well, obviously it is, but it's kind of in the end hard to think about it, but it's
[01:17:56.160 --> 01:18:00.640]   a food for thought, I guess. >> Yeah. And then there's also a whole
[01:18:00.640 --> 01:18:07.040]   question of kind of identifiability. Like I think under certain circumstances sometimes
[01:18:07.040 --> 01:18:15.440]   it's impossible to figure out exactly what the like true state is. And so while we're using it
[01:18:15.440 --> 01:18:21.040]   like an RNN, it's not totally clear that this is like the right state or it's learned the true
[01:18:21.040 --> 01:18:27.360]   latent dynamics of an online system. >> Sorry, Sasha, I have one last question.
[01:18:28.240 --> 01:18:33.840]   This paper made me think a lot about Kalman filters. Are they related?
[01:18:33.840 --> 01:18:38.880]   >> I was thinking the same, but I was scared to ask. I thought I was disconnecting.
[01:18:38.880 --> 01:18:45.440]   >> Yeah. So I'm not the best person to ask about this, but yes, I think Kalman filters are a type
[01:18:45.440 --> 01:18:51.920]   of state-based model. Kalman filters kind of assume a probabilistic generative process,
[01:18:51.920 --> 01:18:57.760]   whereas I think in this case it's not really doing that. So I think it's like a Kalman filter,
[01:18:57.760 --> 01:19:03.680]   is a specific case of this property. >> Okay. I just had to ask.
[01:19:03.680 --> 01:19:08.720]   >> Yes. I mean, there's a lot you can do with Kalman filters that we're not doing. Like for
[01:19:08.720 --> 01:19:13.920]   instance, we're not really running in France or I don't know, filtering in a kind of probabilistic
[01:19:13.920 --> 01:19:18.880]   sense here. >> Thanks for asking that question. I was
[01:19:18.880 --> 01:19:24.720]   scared to ask about what you did. But thanks so much again, Christian, for helping make another
[01:19:24.720 --> 01:19:29.120]   awesome Jack's talk happen. And of course, Sasha for the incredible talk. It was really an honor
[01:19:29.120 --> 01:19:33.120]   to host you. And thanks for all your awesome work. >> Thanks, everyone.
[01:19:33.120 --> 01:19:36.560]   >> Okay. It was really fun. Thanks a lot, Sasha. >> See you.

