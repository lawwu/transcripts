
[00:00:00.000 --> 00:00:02.520]   check that's why because I'm scared if it doesn't work
[00:00:02.520 --> 00:00:05.680]   properly. I'll just double check and then start it.
[00:00:05.680 --> 00:00:07.120]   Sanyam Bhutani: Okay.
[00:00:07.120 --> 00:00:15.600]   Sanyam Bhutani: Right now I'm just checking on YouTube if we
[00:00:15.600 --> 00:00:16.320]   live or not.
[00:00:16.320 --> 00:00:29.040]   Awesome. I can hear an echo which means we're live. Sorry,
[00:00:29.040 --> 00:00:34.880]   I had my monitor glitched out. Just one second.
[00:00:34.880 --> 00:00:37.240]   Okay.
[00:00:37.240 --> 00:00:49.400]   Awesome. My monitor got disconnected. I'm sorry about
[00:00:49.400 --> 00:00:53.040]   that. Hey, everybody, welcome back to "Chai Time Kaggle Talks".
[00:00:53.040 --> 00:00:56.000]   I'm super excited and really honored to be talking to
[00:00:56.000 --> 00:00:59.800]   Chishin today. I was told that's the correct pronunciation.
[00:00:59.800 --> 00:01:02.560]   Chishin, thanks. Thanks so much for joining me today.
[00:01:02.560 --> 00:01:04.200]   Chishin Thakur: Hi, everyone.
[00:01:04.200 --> 00:01:07.840]   Sanyam Bhutani: So I want to point out to the world that
[00:01:07.840 --> 00:01:13.080]   there are about 176,000 Kagglers who've dared to sign up for
[00:01:13.080 --> 00:01:19.640]   competition. Chishin is sixth, number six in the top 10 of the
[00:01:19.640 --> 00:01:23.200]   worldwide ranking. So it's an absolute honor to host him.
[00:01:23.560 --> 00:01:27.080]   Today I'm having chai at the actual chai time, because
[00:01:27.080 --> 00:01:31.000]   Chishin is in Japan. So luckily, it's evening for me. And we'll
[00:01:31.000 --> 00:01:36.320]   be learning about his journey and how did he win? How did his
[00:01:36.320 --> 00:01:41.880]   team finish number one on the TensorFlow GPR competition? So
[00:01:41.880 --> 00:01:45.360]   I'll quickly introduce the session and the agenda and then
[00:01:45.360 --> 00:01:49.360]   move on to talking about Chishin's background. So this is
[00:01:49.360 --> 00:01:53.520]   part of "Chai Time Data Science 2" where I try to interview my
[00:01:53.520 --> 00:01:57.520]   heroes and also understand their solutions. So the agenda for
[00:01:57.520 --> 00:02:01.800]   today is to understand how did Chishin become number six
[00:02:01.800 --> 00:02:05.960]   worldwide? How did he get to the top of world rankings? Then we'll
[00:02:05.960 --> 00:02:09.120]   try to understand the TensorFlow competition and then his
[00:02:09.120 --> 00:02:15.040]   solution. So I want to start by asking you, I believe you have a
[00:02:15.040 --> 00:02:18.560]   background in economics. How did you get interested in modeling
[00:02:18.560 --> 00:02:19.560]   and machine learning?
[00:02:20.160 --> 00:02:26.240]   Yeah, actually, um, when I college like the first or second
[00:02:26.240 --> 00:02:31.040]   year, I found that I actually not that interested in economic.
[00:02:31.040 --> 00:02:37.480]   Okay. So, um, at that time, I was just like, learning,
[00:02:37.480 --> 00:02:43.040]   studying Japanese and some programming skill by myself. And
[00:02:43.040 --> 00:02:48.840]   I find that I found that I'm really interested in computer
[00:02:48.840 --> 00:02:57.240]   science. So, but it's really hard to change my major in, in
[00:02:57.240 --> 00:03:03.640]   my college. So I didn't choose to change my major in my
[00:03:03.640 --> 00:03:10.160]   college. But instead, when I graduated from university, I
[00:03:10.680 --> 00:03:20.320]   come to Japan, and try to enter postgraduate school of
[00:03:20.320 --> 00:03:27.680]   information technology. So yes, actually, I learned many things,
[00:03:27.680 --> 00:03:28.440]   many
[00:03:28.440 --> 00:03:39.920]   laboratory that are working on computer science, specifically
[00:03:39.960 --> 00:03:46.440]   people are learning. But I'm kind of like, incident because
[00:03:46.440 --> 00:03:51.600]   I don't, I don't know much about high tech technology things in
[00:03:51.600 --> 00:03:57.600]   computer science, just like writing code by myself, but
[00:03:57.600 --> 00:04:04.880]   don't know much about high tech things at that time. So it was
[00:04:04.880 --> 00:04:09.640]   kind of in incidents that I can join a laboratory that are
[00:04:09.640 --> 00:04:14.320]   working on deep learning. And finally, I find out that I'm
[00:04:14.320 --> 00:04:19.480]   also interested interested in deep learning. That was kind of
[00:04:19.480 --> 00:04:28.880]   good result for me. So yes, I'm really thankful to my, my, my
[00:04:28.880 --> 00:04:29.880]   boss. Yes.
[00:04:29.880 --> 00:04:33.840]   Sanyam Bhutani: How did how did you practice programming when
[00:04:33.840 --> 00:04:37.200]   you started? Because you come from a different background? How
[00:04:37.200 --> 00:04:40.480]   did you practice Python and learn these skills?
[00:04:40.480 --> 00:04:47.440]   Unknown: Yes, at first I buy some book like that are some
[00:04:47.440 --> 00:04:53.520]   basic knowledge like C++ one Java one, or at that time,
[00:04:53.520 --> 00:05:00.440]   Python is not that popular yet. So yes, so I learned C++ and,
[00:05:01.520 --> 00:05:11.840]   and like objects, Java, and then something like database and
[00:05:11.840 --> 00:05:24.760]   network and web and like web applications. I read many books
[00:05:24.800 --> 00:05:32.960]   and look at many videos to learn that but without anyone teaching
[00:05:32.960 --> 00:05:34.160]   Yeah.
[00:05:34.160 --> 00:05:39.400]   Sanyam Bhutani: How did you discover Kaggle? Your first
[00:05:39.400 --> 00:05:42.480]   competition? I went through your competition history. I think it
[00:05:42.480 --> 00:05:46.880]   was the red heart or the salt identification one. How did you
[00:05:46.880 --> 00:05:49.360]   find Kaggle? And how did you get addicted to it?
[00:05:50.640 --> 00:05:55.960]   Yes, actually, if you don't mention that to competition, I
[00:05:55.960 --> 00:06:01.360]   even don't remember that I have ever attempted that to
[00:06:01.360 --> 00:06:07.680]   competition because actually, I just like, like to find things
[00:06:07.680 --> 00:06:19.280]   some public data sets. You know that at that time, before 2016,
[00:06:19.320 --> 00:06:25.000]   or 17. There, they are not that much public, available data set
[00:06:25.000 --> 00:06:31.120]   yet. Specifically in computer vision, you know that labeling
[00:06:31.120 --> 00:06:36.600]   large size large scale image data sets are very expensive. So
[00:06:36.600 --> 00:06:43.320]   we usually use like Cocoa or image net to do our experiment
[00:06:43.320 --> 00:06:49.280]   to figure out whether our chick our new, new idea works or not.
[00:06:49.280 --> 00:06:55.520]   But actually, sometimes, if we only optimize one or two data
[00:06:55.520 --> 00:07:01.680]   sets, we might over optimize is is kind of different from over
[00:07:01.680 --> 00:07:07.400]   fit is we I call it over optimized. It says maybe this
[00:07:07.400 --> 00:07:14.320]   trick is work at image net or Cocoa, but somehow not working
[00:07:14.320 --> 00:07:22.360]   on other data sets. So to prove one day, I would like to find
[00:07:22.360 --> 00:07:29.720]   some new image, public available image data set to figure out
[00:07:29.720 --> 00:07:36.760]   whether our new ideas or chase works or not. So maybe at that
[00:07:36.760 --> 00:07:42.120]   time, I find cargo that there's there are many public data sets
[00:07:42.200 --> 00:07:47.480]   at cargo that I can freely access. So I'm quite happy at
[00:07:47.480 --> 00:07:52.560]   that time. But actually, I don't attempt competition very
[00:07:52.560 --> 00:07:56.560]   seriously, just like downloading their data set and try something.
[00:07:56.560 --> 00:07:58.960]   Yeah. At that time,
[00:07:58.960 --> 00:08:02.880]   Sanyam Bhutani: that sounds like my approach right now. So I just
[00:08:02.880 --> 00:08:06.720]   download four kernels and don't do anything. That's all I do.
[00:08:06.760 --> 00:08:11.360]   Yeah. Actually, that is the juice.
[00:08:11.360 --> 00:08:16.480]   There's someone who saying they're joining from China.
[00:08:16.480 --> 00:08:21.840]   Thank you for joining. I wanted to ask you how did you then once
[00:08:21.840 --> 00:08:26.160]   you once you had found Kaggle and you got interested in
[00:08:26.160 --> 00:08:30.960]   computer vision? Yeah. How did you improve on Kaggle? Because
[00:08:30.960 --> 00:08:33.600]   you've taken competition very seriously. That's what I
[00:08:33.600 --> 00:08:37.400]   believe. It takes a lot of efforts and brain also to get to
[00:08:37.400 --> 00:08:40.600]   the top. How did you improve on Kaggle?
[00:08:40.600 --> 00:08:48.840]   Okay, so like the first competition, really, very
[00:08:48.840 --> 00:08:54.760]   seriously attempts was a was a natural language processing
[00:08:54.760 --> 00:08:59.480]   competition, which is called jigsaw. The second jigsaw
[00:08:59.480 --> 00:09:09.440]   competition. And at that time, maybe a little newer, a little
[00:09:09.440 --> 00:09:21.040]   bit. I think solo gold. Okay. Yeah. Yes, yes. The eight. Yes,
[00:09:21.040 --> 00:09:26.680]   jigsaw unintended bias in toxicity classification one. So
[00:09:27.000 --> 00:09:33.160]   actually, at that time, because I when I was in laboratory, I
[00:09:33.160 --> 00:09:39.880]   was working on computer vision. So actually, many people are
[00:09:39.880 --> 00:09:43.200]   working on natural language processing as well. But
[00:09:43.200 --> 00:09:47.880]   actually, during the two year I was at laboratory, I don't know
[00:09:47.880 --> 00:09:53.800]   anything about NLP. So but but I personally was very interested
[00:09:53.800 --> 00:10:00.920]   in NLP. So I was a very long time for a very long time I
[00:10:00.920 --> 00:10:07.040]   looking forward to learn and to master NLP skills like at that
[00:10:07.040 --> 00:10:12.240]   time is still like LSTM or GRU or something like that.
[00:10:12.240 --> 00:10:16.560]   But no transformers, no transformers or no attention
[00:10:16.560 --> 00:10:17.360]   back then.
[00:10:18.360 --> 00:10:24.360]   Yeah, so it is still a transformer was not not born
[00:10:24.360 --> 00:10:33.440]   yet. So yeah, so I was I was very interested in LP. So at
[00:10:33.440 --> 00:10:37.960]   that time, I found I find that there is a new cargo
[00:10:37.960 --> 00:10:45.640]   competition come coming up. That is a natural language
[00:10:45.640 --> 00:10:52.320]   processing LP. So actually, I was just like, wanting to learn
[00:10:52.320 --> 00:10:58.680]   how to do NLP to attempt that competition, but but not to win
[00:10:58.680 --> 00:11:03.640]   or not not to fight fight fight with with any other guys just to
[00:11:03.640 --> 00:11:10.360]   learn. And at that time, I read very many discussions, many
[00:11:10.400 --> 00:11:15.680]   public notebook and figure figuring out how how people
[00:11:15.680 --> 00:11:20.360]   pre processing tags later, how to like how to modeling that how
[00:11:20.360 --> 00:11:27.880]   to do tokenizer how to how to like, build LSTM models and even
[00:11:27.880 --> 00:11:34.040]   birds models. So at that time, it was just like the first year
[00:11:34.040 --> 00:11:44.280]   of birth was born, if I don't remember wrong. So it was a
[00:11:44.280 --> 00:11:50.920]   very good chance for me to learn many, many new LP technique. And
[00:11:50.920 --> 00:11:59.720]   like, how to say it was another incident that I finally finished
[00:11:59.720 --> 00:12:05.800]   in the solo solo gold. So actually, that encouraged me,
[00:12:05.800 --> 00:12:15.000]   encouraged me very much in competing in cargo. Yes. So like
[00:12:15.000 --> 00:12:19.640]   this is my first time very seriously to attempt a
[00:12:19.640 --> 00:12:25.520]   competition. And I finished in a solo gold, which is the biggest
[00:12:25.520 --> 00:12:29.920]   difficult difficulty for being a grandmasters that was
[00:12:29.920 --> 00:12:34.360]   accomplished by my first competition. So
[00:12:34.360 --> 00:12:36.880]   Sanyam Bhutani: may I interrupt you for one second?
[00:12:36.880 --> 00:12:41.360]   Okay, no problem. I just want to tell the audience. This is like
[00:12:41.360 --> 00:12:45.360]   Chisholm said, this is the most difficult possible thing to do
[00:12:45.360 --> 00:12:49.960]   on Kaggle one of the most difficult things to finish in
[00:12:49.960 --> 00:12:53.680]   the gold medal zone by yourself. So as you can see, there are
[00:12:53.680 --> 00:12:58.280]   many people who have formed teams. But Chisholm finished by
[00:12:58.280 --> 00:13:02.600]   himself in the eighth rank. And this on Kaggle is one of the
[00:13:02.600 --> 00:13:05.960]   most prestigious and difficult things to do. So just just
[00:13:05.960 --> 00:13:08.720]   wanted to emphasize on that it's really hard to do.
[00:13:08.720 --> 00:13:14.360]   Chisholm Nguyen: Yes, yes. To, to be a grandmaster, you have to
[00:13:14.360 --> 00:13:20.000]   have a solo gold. So like, when you have a solo gold, then you
[00:13:20.000 --> 00:13:25.840]   can have you can just team up with other guys and attempt
[00:13:25.840 --> 00:13:29.480]   other competition to get gold medal and then you become
[00:13:29.480 --> 00:13:33.840]   grandmaster. But the most difficult thing is to have a
[00:13:33.840 --> 00:13:43.760]   solo gold. So I mean, it's just an incident that I can get solo
[00:13:43.760 --> 00:13:50.000]   gold, I can get gold for my first competition. So I mean,
[00:13:50.000 --> 00:13:58.200]   that encouraged me very much. So I decided to continue my
[00:13:58.200 --> 00:14:05.120]   continue to join other competitions like, like, maybe
[00:14:05.120 --> 00:14:11.680]   can you show the my competition list? I actually that was two
[00:14:11.680 --> 00:14:17.400]   years ago. So I don't quite remember which competition I
[00:14:17.400 --> 00:14:17.840]   have joined.
[00:14:17.840 --> 00:14:18.200]   Sanyam Bhutani: This one?
[00:14:18.200 --> 00:14:22.720]   Chisholm Nguyen: That's my first team goal, but not that
[00:14:22.720 --> 00:14:29.040]   one. So actually, the second competition where I joined
[00:14:29.040 --> 00:14:31.720]   Aptos. Yes. That
[00:14:31.720 --> 00:14:34.160]   Sanyam Bhutani: You have won so many medals, you're also
[00:14:34.160 --> 00:14:35.360]   forgetting where you had.
[00:14:38.440 --> 00:14:42.120]   Chisholm Nguyen: Sorry for that. Yeah, the second competition I
[00:14:42.120 --> 00:14:47.360]   joined was Aptos and at that time I got my second solo gold.
[00:14:47.360 --> 00:14:53.320]   So that was another very encouraging things for me. Yes.
[00:14:53.320 --> 00:14:59.480]   So actually, I have three solo golds in total. And after that
[00:14:59.480 --> 00:15:06.000]   many invitation was sent to my emails and like many people
[00:15:06.000 --> 00:15:11.480]   wanting to want me to be part of them and competing with them.
[00:15:11.480 --> 00:15:19.200]   And so, like, you know, that's attempting joining a competition
[00:15:19.200 --> 00:15:25.200]   solo. It was like your, you feel very lonely and you have to work
[00:15:25.200 --> 00:15:32.680]   very hard, you have to deal with everything you faced. So it was
[00:15:32.680 --> 00:15:40.320]   quite hard time for me to do so. So it was time to team up. So I
[00:15:40.320 --> 00:15:45.000]   was just thought about that and team up. After that, I haven't
[00:15:45.000 --> 00:15:51.800]   done any solo competition. Yes, from that time. Yeah.
[00:15:51.800 --> 00:15:54.280]   Sanyam Bhutani: Please continue.
[00:15:54.320 --> 00:16:01.440]   Chisholm Nguyen: Sorry. Yeah, so after teaming up with many like
[00:16:01.440 --> 00:16:11.840]   Bo and Gary, maybe you know them. And I found that to team
[00:16:11.840 --> 00:16:18.360]   up with other Grandmaster and I find that it's really, really,
[00:16:18.360 --> 00:16:24.880]   really easy to get to go zone, even the green zone. You know
[00:16:24.880 --> 00:16:30.400]   that. So yeah, after that, I just give up solo in
[00:16:30.400 --> 00:16:31.200]   competition.
[00:16:31.200 --> 00:16:35.640]   Sanyam Bhutani: There's a question and they are asking
[00:16:35.640 --> 00:16:41.680]   about your self learning. So when you started solo, how did
[00:16:41.680 --> 00:16:45.160]   you learn by yourself? Kaggle is so so challenging, so
[00:16:45.160 --> 00:16:49.800]   overwhelming. There's so much information. How did you get
[00:16:49.800 --> 00:16:51.640]   gold by yourself just starting out?
[00:16:51.640 --> 00:16:55.480]   Chisholm Nguyen: Yes, I think I because I don't know anything
[00:16:55.480 --> 00:17:00.800]   about NLP before I joined Jigsaw competition. So maybe I can
[00:17:00.800 --> 00:17:05.560]   give give you guys some advice about that. Because yeah, you
[00:17:05.560 --> 00:17:12.800]   can imagine that I actually know nothing about before I joined
[00:17:12.800 --> 00:17:20.600]   Jigsaw competition. So I think the first step is to read public
[00:17:20.600 --> 00:17:25.480]   notebook from the competition. You can you can see that there
[00:17:25.480 --> 00:17:32.120]   is a like sort by public score button in the like right top
[00:17:32.120 --> 00:17:38.680]   side and you can just see which notebook has best score at
[00:17:38.680 --> 00:17:49.000]   public leaderboard. And yes, yes. Best score. Yes. Like that
[00:17:49.000 --> 00:17:54.000]   you can see which notebook has best score at the public
[00:17:54.000 --> 00:18:01.640]   leaderboard and then you can read it to learn how they deal
[00:18:01.640 --> 00:18:05.640]   with the data sets, how they build model, how they set
[00:18:05.640 --> 00:18:11.280]   hyperparameters. And then once you read many of those things,
[00:18:11.280 --> 00:18:17.120]   you have some ideas, like maybe reduce learning rates is better
[00:18:17.120 --> 00:18:23.880]   or not, you can have try by by yourself. And have many ideas by
[00:18:23.880 --> 00:18:28.600]   yourself. You try them, try an error and do many experiments
[00:18:28.640 --> 00:18:36.560]   and just like if you get stuck and you go back to discussion or
[00:18:36.560 --> 00:18:40.640]   other notebook and find out other and learn other new
[00:18:40.640 --> 00:18:45.360]   things and then you get back to your experiments and just do it
[00:18:45.360 --> 00:18:49.160]   again and again. And then you finally find that you have
[00:18:49.160 --> 00:18:54.320]   learned many things. So you actually don't need to care
[00:18:54.320 --> 00:19:00.040]   about where, where is your ranking finally landed.
[00:19:00.040 --> 00:19:04.360]   Actually, I didn't care about that at all. Because finally, I
[00:19:04.360 --> 00:19:13.400]   didn't just like, I believe I was 32 place or somewhat at
[00:19:13.400 --> 00:19:18.280]   public leaderboards, which is civil zones, but I finally shake
[00:19:18.280 --> 00:19:22.960]   up to go zone. So you don't need to care about your ranking in
[00:19:23.360 --> 00:19:26.840]   public leaderboard when you want to learn something, actually.
[00:19:26.840 --> 00:19:30.200]   Yeah, the key point is to learn.
[00:19:30.200 --> 00:19:36.680]   Sanyam Bhutani: Thanks for answering that. For some reason,
[00:19:36.680 --> 00:19:40.160]   this public leaderboard doesn't show anymore. Maybe Kaggle
[00:19:40.160 --> 00:19:42.120]   removes it after it's been a few years.
[00:19:42.120 --> 00:19:45.240]   I didn't know that.
[00:19:45.240 --> 00:19:50.200]   You're also very active in discussions. I believe you're
[00:19:50.200 --> 00:19:54.800]   really close to becoming a grandmaster there. How do you
[00:19:54.800 --> 00:19:58.640]   suggest beginners engage in discussion? How can they
[00:19:58.640 --> 00:20:03.440]   contribute there? Or some people ask very easy questions that
[00:20:03.440 --> 00:20:07.000]   they can just look up what, what questions should they ask? And
[00:20:07.000 --> 00:20:08.640]   how can they learn best from there?
[00:20:08.640 --> 00:20:15.800]   Yes, yes, actually, like, I don't like to ask question
[00:20:15.800 --> 00:20:19.400]   actually, you know, that's because I learned many things by
[00:20:19.400 --> 00:20:24.000]   myself without teacher, including computer scientists
[00:20:24.000 --> 00:20:28.240]   and Japanese, I don't have a Japanese teacher as well. And,
[00:20:28.240 --> 00:20:37.640]   yes. So actually, I don't like to ask question, like, although
[00:20:37.640 --> 00:20:41.960]   like, when I read notebook or discussion at the Kaggle
[00:20:41.960 --> 00:20:49.600]   competition, I find that maybe the people that the person of
[00:20:49.600 --> 00:20:58.080]   that notebook of the is not making his idea very clearly by
[00:20:58.080 --> 00:21:04.880]   I think we all have the experience that you find that
[00:21:04.880 --> 00:21:10.600]   there is something maybe hidden by the author of the of the
[00:21:10.960 --> 00:21:14.880]   notebook or discussion. So some secret that is hidden in there
[00:21:14.880 --> 00:21:19.960]   that you have to understand yourself. Yes. So actually, it
[00:21:19.960 --> 00:21:24.960]   may be you, although you ask him, he he might not answer
[00:21:24.960 --> 00:21:32.160]   you. For most of the time, actually, specifically before
[00:21:32.160 --> 00:21:36.280]   the end of the competition. Maybe after the end of the
[00:21:36.280 --> 00:21:40.080]   competition, they would like to answer you but probably
[00:21:40.120 --> 00:21:45.160]   probably not before the end of the competition. So when you
[00:21:45.160 --> 00:21:51.080]   find that you have some question about the idea, you can try the
[00:21:51.080 --> 00:21:57.120]   idea by yourself or, or simply just choose not to trust them.
[00:21:57.120 --> 00:22:05.760]   It's very simple. Yes, actually, when I I have attempt, like 20
[00:22:05.760 --> 00:22:10.800]   or 30 competition for for this two years, I have read many,
[00:22:10.800 --> 00:22:16.760]   many public notebook, many, many public discussion. So for my
[00:22:16.760 --> 00:22:25.880]   experience, there are around like 60% of the discussion,
[00:22:25.880 --> 00:22:31.320]   like he said, the this trick is not working, or that trick is
[00:22:31.320 --> 00:22:38.120]   working. And after I tried it by myself, 60% of them is actually
[00:22:38.120 --> 00:22:42.880]   the opposite of what the result is the opposite of what they
[00:22:42.880 --> 00:22:50.720]   said. So you have to try it by yourself to find out if that
[00:22:50.720 --> 00:22:56.000]   trick is really working or not, but not just listen to them.
[00:22:56.400 --> 00:23:03.840]   That is quite important when you learn things, just not trust
[00:23:03.840 --> 00:23:06.200]   them directly. Try by yourself.
[00:23:06.200 --> 00:23:11.760]   Sanyam Bhutani: That is such a beautiful advice that don't like
[00:23:11.760 --> 00:23:15.520]   one of my mistakes or maybe because I do the podcast, I use
[00:23:15.520 --> 00:23:19.880]   an excuse is I just read and I don't implement things. I think
[00:23:19.880 --> 00:23:23.480]   many people do that. It's that's the difference between a
[00:23:23.680 --> 00:23:26.800]   competition expert and grandmaster. Grandmaster
[00:23:26.800 --> 00:23:29.920]   implements everything and tries tries it by themselves.
[00:23:29.920 --> 00:23:37.560]   So maybe they are just
[00:23:37.560 --> 00:23:40.960]   Unknown: Ah, yes, maybe there is.
[00:23:40.960 --> 00:23:45.440]   Sanyam Bhutani: Sorry, I'm sorry about that. I'll repeat. I was
[00:23:45.440 --> 00:23:48.560]   saying that's the difference between a competition expert and
[00:23:48.560 --> 00:23:53.000]   a top ranked grandmaster is that a grandmaster tries everything
[00:23:53.000 --> 00:23:55.600]   and a noob just reads and doesn't do anything.
[00:23:55.600 --> 00:24:03.440]   So there's another question, how how much time were you spending
[00:24:03.440 --> 00:24:06.240]   every day during your first solo gold medals?
[00:24:06.240 --> 00:24:16.680]   Okay, so actually, for the past two years, I spent like 90% of
[00:24:16.680 --> 00:24:23.480]   my personal time, I have a full time job. But besides the full
[00:24:23.480 --> 00:24:32.400]   time job of my time, I spend 90% of it to do cargo competition.
[00:24:32.400 --> 00:24:38.600]   It's quite a lot. Yeah, I actually, in the past two years,
[00:24:38.600 --> 00:24:49.480]   I my memory is full full of cargoes. Yes, it's been quite a
[00:24:49.480 --> 00:24:55.200]   lot of time to to get a gold specifically solo gold. Yes.
[00:24:55.200 --> 00:25:02.800]   Do you ever feel burned out from this? Like, of course, everyone
[00:25:02.800 --> 00:25:08.160]   loves cargo loves experimenting, but do how do you like balance
[00:25:08.160 --> 00:25:11.520]   it? Or how do you take a break? How do you ever take a break?
[00:25:11.520 --> 00:25:12.240]   Or?
[00:25:12.240 --> 00:25:20.040]   Yes, of course. I like video games or animations. That that's
[00:25:20.040 --> 00:25:25.760]   the main reason why why I come to Japan. So So yes, but
[00:25:25.760 --> 00:25:32.840]   actually, most of my hobbies are indoor. I just like sitting in
[00:25:32.840 --> 00:25:39.720]   front of my computer and I can have fun. Nothing I'm arguing
[00:25:39.720 --> 00:25:44.960]   just sitting in front of my computer. And just looking at a
[00:25:44.960 --> 00:25:46.760]   screen and do something.
[00:25:46.760 --> 00:25:53.560]   That's awesome. So what's, what's the most challenging
[00:25:53.560 --> 00:25:56.760]   thing for you today on Kaggle? What what's something you still
[00:25:56.760 --> 00:25:57.760]   find difficult?
[00:25:59.120 --> 00:26:06.240]   Okay, so it's, it was very difficult. If I find that I
[00:26:06.240 --> 00:26:13.920]   just work very hard to try to make my score better and finally
[00:26:13.920 --> 00:26:19.960]   find that the top one of public leaderboard has a very big gap
[00:26:19.960 --> 00:26:26.120]   from our school. And I have no idea why they can get such high
[00:26:26.120 --> 00:26:32.160]   scoring public no public public leaderboard. But finally,
[00:26:32.160 --> 00:26:38.760]   finally, I like 80% of the case, I find that they are overfitting
[00:26:38.760 --> 00:26:47.240]   to the public leaderboards. So I don't know, I should say that
[00:26:47.240 --> 00:26:51.480]   is lucky or not, because I was really looking forward to learn
[00:26:51.480 --> 00:26:58.000]   how they can get to such high score, but it finally turns out
[00:26:58.000 --> 00:27:00.600]   that they are overfitting. It's kind of sad.
[00:27:00.600 --> 00:27:08.480]   I want to point out that Chishen has in his team with Grand
[00:27:08.480 --> 00:27:13.880]   Master Bo and Gary, and also, they're called sheep on Kaggle.
[00:27:13.880 --> 00:27:18.400]   I don't know their real name. He has won three competitions as
[00:27:18.400 --> 00:27:22.480]   well. So he knows how not to overfit and we'll be talking
[00:27:22.480 --> 00:27:29.760]   about that also very soon. What what competitions do you enjoy
[00:27:29.760 --> 00:27:34.680]   today? And what led you to participating in the TensorFlow
[00:27:34.680 --> 00:27:37.280]   one? Why did you decide to spend time on it?
[00:27:37.280 --> 00:27:44.400]   Yeah, um, maybe you know that for past few years there, there
[00:27:44.400 --> 00:27:49.800]   are many image classification task, or image segmentation
[00:27:49.800 --> 00:27:58.520]   task. And these competition are all very, like, well, the, the
[00:27:58.520 --> 00:28:03.960]   solution of those competition are all well developed. So in
[00:28:03.960 --> 00:28:08.960]   past, in the past year, there, there are not too, too many
[00:28:08.960 --> 00:28:12.680]   image classification or image segmentation competition.
[00:28:12.720 --> 00:28:18.200]   Instead, there are many object detection competition. So I
[00:28:18.200 --> 00:28:26.520]   think it's because Kaggle or the organizer of competition, just
[00:28:26.520 --> 00:28:31.680]   think about thinking about that, the object detection is still
[00:28:31.680 --> 00:28:38.280]   like they want more people to focus on object detection, they
[00:28:38.280 --> 00:28:44.520]   maybe they think this area is still under discovery. So I
[00:28:44.520 --> 00:28:51.640]   just, of course, for specifically for the TensorFlow
[00:28:51.640 --> 00:28:55.760]   competition is because it's not only object detection, but it's
[00:28:55.760 --> 00:29:04.480]   also a video one, we don't detect object on single image,
[00:29:04.800 --> 00:29:15.160]   but on on video. So actually, I was like to try some object
[00:29:15.160 --> 00:29:20.680]   checking things, because I am not very familiar with that I
[00:29:20.680 --> 00:29:25.040]   want to learn some technique about checking. So that that's
[00:29:25.040 --> 00:29:27.760]   why I joined this competition.
[00:29:27.760 --> 00:29:33.400]   What's your first object detection competition?
[00:29:34.400 --> 00:29:34.840]   Sorry?
[00:29:34.840 --> 00:29:38.360]   Was this your first object detection competition that you
[00:29:38.360 --> 00:29:39.280]   participated in?
[00:29:39.280 --> 00:29:44.400]   Sorry, I don't understand what you
[00:29:44.400 --> 00:29:48.840]   Sorry, sorry. Was this the first competition where you actually
[00:29:48.840 --> 00:29:51.760]   tried object detection? Or did you participate in similar
[00:29:51.760 --> 00:29:53.160]   competitions before this?
[00:29:53.160 --> 00:29:59.760]   Um, maybe three or four months ago there, there, there is
[00:29:59.760 --> 00:30:07.520]   another object detection competition is SIM one fiber.
[00:30:07.520 --> 00:30:13.440]   What I don't know how to pronounce it. So yes. Um,
[00:30:13.440 --> 00:30:19.120]   actually, I'm not quite familiar with object detection and object
[00:30:19.120 --> 00:30:22.720]   tracking that that's the main purpose I joined this
[00:30:22.720 --> 00:30:23.400]   competition.
[00:30:23.400 --> 00:30:29.080]   Thanks. Thanks for clarifying that. So for the next part, I
[00:30:29.080 --> 00:30:31.600]   would like to explain the problem statement to the
[00:30:31.600 --> 00:30:35.480]   audience. So I'll go through the competition page and just
[00:30:35.480 --> 00:30:38.800]   explain what I understand. If I say anything incorrectly, please
[00:30:38.800 --> 00:30:42.200]   interrupt me and correct me because I didn't participate. I
[00:30:42.200 --> 00:30:44.320]   was just there to read the solutions.
[00:30:44.320 --> 00:30:45.720]   Okay, no problem.
[00:30:45.720 --> 00:30:52.320]   So from what I understand here, so video that everyone can
[00:30:52.320 --> 00:30:55.800]   watch TensorFlow channel made this video and detail about
[00:30:56.480 --> 00:31:01.920]   this, this species of starfish that they want to protect. And
[00:31:01.920 --> 00:31:05.240]   the task is to be able to identify them through the video.
[00:31:05.240 --> 00:31:09.880]   So what makes this challenging is it's not just an object
[00:31:09.880 --> 00:31:15.400]   detection problem. It's inside of videos. So for anyone who's
[00:31:15.400 --> 00:31:19.080]   ever worked with images knows they eat up a lot of GPU memory.
[00:31:19.080 --> 00:31:23.480]   Chishin has two wonderful a 6000 I'm sure they would have
[00:31:23.480 --> 00:31:27.040]   helped him a lot. He's also adding the z by HP cap that
[00:31:27.040 --> 00:31:33.240]   would have helped him. For for anyone who's worked with video
[00:31:33.240 --> 00:31:36.160]   knows it's even more challenging. So that was the
[00:31:36.160 --> 00:31:42.760]   main thing that made this very tough competition. For the data,
[00:31:42.760 --> 00:31:47.480]   as I mentioned, you had videos. So inside of the train images,
[00:31:47.480 --> 00:31:51.440]   it's actually three videos that are there. And then you have to
[00:31:51.440 --> 00:31:57.040]   predict based on that, and you will be scored on the F2 score
[00:31:57.040 --> 00:32:01.280]   of your answers. So I won't explain what F2 score is, I
[00:32:01.280 --> 00:32:04.920]   would assume everyone knows this. And I also want to share
[00:32:04.920 --> 00:32:09.640]   a joke. I think that Chishin had made he had posted this before
[00:32:09.640 --> 00:32:15.160]   the competition ended. And I want to point out that as you
[00:32:15.160 --> 00:32:19.360]   can see his team moved up 120 ranks. So this was one day
[00:32:19.360 --> 00:32:22.240]   before the competition ended. And I think you were making a
[00:32:22.240 --> 00:32:24.920]   joke that you want to learn from everyone and you knew you were
[00:32:24.920 --> 00:32:25.600]   going to win.
[00:32:25.600 --> 00:32:30.920]   I don't mean that.
[00:32:30.920 --> 00:32:34.560]   I think you you knew you were going to win and you were just
[00:32:34.560 --> 00:32:37.600]   making fun of everybody because next day we'll just tell Oh, see
[00:32:37.600 --> 00:32:38.040]   I won't
[00:32:38.040 --> 00:32:40.080]   know absolutely.
[00:32:40.080 --> 00:32:47.040]   But were you surprised by the joke that I think it was a joke
[00:32:47.080 --> 00:32:49.320]   afterwards. But were you surprised yourself?
[00:32:49.320 --> 00:32:56.600]   Yes, yes, sure. Our team, all three people in our team all
[00:32:56.600 --> 00:33:01.920]   surprised that finally turns out that our private score is the
[00:33:01.920 --> 00:33:08.880]   best. It's kind of how to say like, actually, we work very
[00:33:08.880 --> 00:33:14.960]   hard on this competition. And she find that higher resolution
[00:33:16.040 --> 00:33:21.360]   is has better public score on public leaderboards. But
[00:33:21.360 --> 00:33:29.360]   actually, after like 4000 4000 times 4000 resolution that there
[00:33:29.360 --> 00:33:36.120]   are not not that different on public scores. So actually,
[00:33:36.120 --> 00:33:42.240]   until the end of the competition, we have no idea how
[00:33:42.240 --> 00:33:49.920]   to get public LB score to 0.8 or something that's that was
[00:33:49.920 --> 00:33:56.200]   unimaginable for us at that time. Okay, so it turns out that
[00:33:56.200 --> 00:34:01.000]   they have some trick to overfit to the public leaderboards by
[00:34:01.000 --> 00:34:07.680]   shrinking the predicted box that that was actually not good, I
[00:34:07.680 --> 00:34:13.960]   think. Well, I'm not blaming anyone, but maybe people some
[00:34:13.960 --> 00:34:17.400]   people is just like to dig in little secret from public
[00:34:17.400 --> 00:34:20.840]   leaderboard. So that's also okay.
[00:34:20.840 --> 00:34:27.040]   So I wanted to mention this because this competition and
[00:34:27.040 --> 00:34:32.120]   Chisholm's solution also says, trust your cross validation,
[00:34:32.120 --> 00:34:36.120]   trust your CV. So this competition had a really big
[00:34:36.120 --> 00:34:39.240]   shake up, as you can see, so there was a lot of difference
[00:34:39.240 --> 00:34:43.920]   between the public and private leaderboard. May I ask you, when
[00:34:43.920 --> 00:34:48.640]   you first approached the competition, how did you start
[00:34:48.640 --> 00:34:52.600]   applying your ideas? What were the first ideas that you started
[00:34:52.600 --> 00:34:56.680]   and did you team up from the first few days or later on?
[00:34:56.680 --> 00:35:04.040]   Oh, no, I actually not not teaming up to teaming up with
[00:35:04.080 --> 00:35:09.000]   anyone. When I joined this competition, I worked by myself
[00:35:09.000 --> 00:35:14.680]   for maybe two or three weeks. And at that time, I was like,
[00:35:14.680 --> 00:35:19.960]   for the first step is to read public notebook and public
[00:35:19.960 --> 00:35:26.040]   discussion to find out what most most most of those people are
[00:35:26.040 --> 00:35:31.520]   doing. Like, I find that many people are using YOLO X and
[00:35:31.520 --> 00:35:40.080]   YOLO V5. And so I decided to start started from YOLO V5 as
[00:35:40.080 --> 00:35:47.680]   well. And, like, try to turn in some parameter first, like,
[00:35:47.680 --> 00:35:53.160]   turning the learning rate, turning the augmentation, the
[00:35:53.160 --> 00:35:59.120]   number of epoch and find and find out the how how the result
[00:35:59.120 --> 00:36:07.120]   would change. Yes, I was started like that. And after two or
[00:36:07.120 --> 00:36:13.600]   three weeks, I find that sheet published, very high score
[00:36:13.600 --> 00:36:17.160]   notebook, maybe you know that and yes, yes.
[00:36:17.160 --> 00:36:19.880]   May I share the story?
[00:36:19.880 --> 00:36:21.960]   Okay, no problem.
[00:36:21.960 --> 00:36:27.480]   So, I believe I read all of the solutions this time to prepare
[00:36:27.480 --> 00:36:31.560]   for this interview. Steam published a notebook where he
[00:36:31.560 --> 00:36:35.560]   shared a trick. And then he got pushed out of the leaderboard.
[00:36:35.560 --> 00:36:38.360]   I think that was the story, right? Because everyone just
[00:36:38.360 --> 00:36:42.160]   used their trick, and they went above him on the leaderboard.
[00:36:42.160 --> 00:36:49.600]   Yes, yes, yes. Like before before we team up, she is about
[00:36:49.600 --> 00:36:55.560]   to quit this competition. So we decided to publish his notebook.
[00:36:56.360 --> 00:37:04.160]   So after that, we decided to, we decided to team up and, and the
[00:37:04.160 --> 00:37:12.520]   other two guys, me and MVN, just just bring him for the public
[00:37:12.520 --> 00:37:17.640]   the way we high high public score notebook and just like
[00:37:17.640 --> 00:37:21.480]   something like leaderboard destroyer or something.
[00:37:22.480 --> 00:37:26.320]   I want to just quickly mention your teammates. They are also
[00:37:26.320 --> 00:37:30.040]   both Kaggle Grandmasters in competition. And when we say
[00:37:30.040 --> 00:37:35.160]   sheep, I don't know their real name. Kaggle name is sheep. And
[00:37:35.160 --> 00:37:37.960]   same for their other teammates. So we are talking about Kaggle
[00:37:37.960 --> 00:37:40.600]   Grandmaster sheep because we don't know their real name.
[00:37:40.600 --> 00:37:46.840]   So after that, sorry, please.
[00:37:47.840 --> 00:37:52.160]   Oh, please. I was going to ask after you teamed up, how did
[00:37:52.160 --> 00:37:57.680]   you start applying ideas? And I'm just I want to learn how does
[00:37:57.680 --> 00:38:02.600]   a Grandmaster while approaching competition think of ideas? How
[00:38:02.600 --> 00:38:06.360]   do you apply them? And how does that happen behind the scenes?
[00:38:07.760 --> 00:38:16.280]   Okay, so after we at the beginning of the team up, we
[00:38:16.280 --> 00:38:21.560]   have our own ideas, our own training pipelines, our own
[00:38:21.560 --> 00:38:28.280]   methods for pre processing images. So after we team up, the
[00:38:28.280 --> 00:38:35.000]   first thing is to like to summarize our own ideas. So
[00:38:35.000 --> 00:38:39.760]   after we team up, the first thing is to like to summarize
[00:38:39.760 --> 00:38:47.320]   our own ideas and tricks to share with each other. And we
[00:38:47.320 --> 00:38:54.680]   will like try other guys ideas to see whether it will improve
[00:38:54.680 --> 00:39:00.920]   our own score as well. And then we will come up with a single
[00:39:00.920 --> 00:39:07.880]   tricks or ideas from from all members of our team. And then we
[00:39:07.880 --> 00:39:15.960]   will begin to improve the pipeline. And like we like I
[00:39:15.960 --> 00:39:21.920]   when I have a new idea, I will share share on our Slack, we
[00:39:21.920 --> 00:39:28.880]   communicate via Slack, and share it to Slack and maybe the other
[00:39:28.880 --> 00:39:33.680]   guys say just go ahead and I will try it. Or sometimes they
[00:39:33.680 --> 00:39:38.560]   will they will say no, I think the idea is just like a shit
[00:39:38.560 --> 00:39:43.800]   don't try it. Of course they will they will explain why why
[00:39:43.800 --> 00:39:50.840]   they think it like they have tried it on before on some
[00:39:50.840 --> 00:39:54.480]   competition before and it doesn't work. So just please
[00:39:54.480 --> 00:39:59.320]   don't try it because it's a shit. So yes, if I didn't team
[00:39:59.320 --> 00:40:05.280]   up with with them, I may have a I may have to try many ideas
[00:40:05.280 --> 00:40:11.160]   that they are just not working. So yes, teaming up with many
[00:40:11.160 --> 00:40:18.960]   experienced guys is kind of like saving your time not only on not
[00:40:18.960 --> 00:40:25.720]   only on that you can like other guys to working but they have
[00:40:25.720 --> 00:40:33.600]   some experience might help you to like avoid some time
[00:40:33.600 --> 00:40:39.040]   wasting. That's quite important. Yes.
[00:40:39.040 --> 00:40:44.280]   A grandmaster also has the experience of hundreds of
[00:40:44.280 --> 00:40:47.520]   competition, thousands of models, so you you get to learn
[00:40:47.520 --> 00:40:48.640]   from them when you team up.
[00:40:48.640 --> 00:40:55.160]   Yeah, we we always learn from each other. That is why I I'm
[00:40:55.160 --> 00:41:01.600]   very happy to team up with other guys that was very important
[00:41:01.600 --> 00:41:08.720]   for for me. And it's more important than earning prize
[00:41:08.720 --> 00:41:09.560]   from Kaggle.
[00:41:09.560 --> 00:41:15.200]   That's that's great to know. So now I want to summarize your
[00:41:15.200 --> 00:41:19.480]   solution. So I request your help to help the audience understand
[00:41:19.480 --> 00:41:30.720]   how did you design the solution and its overview. Okay. So I
[00:41:30.720 --> 00:41:34.760]   believe you've already shared that you found this trick of
[00:41:34.760 --> 00:41:39.400]   resolutions and that's what is going on here. So you're trained
[00:41:39.400 --> 00:41:43.520]   six, you will have the five models and you're trained them
[00:41:43.520 --> 00:41:49.040]   on different resolutions and you had also cut patches of images.
[00:41:49.040 --> 00:41:53.560]   So you had cut into smaller parts of images, I believe. Yeah.
[00:41:53.560 --> 00:41:59.200]   And from there, the smaller portion. So like, for example,
[00:41:59.200 --> 00:42:03.600]   right now I'm zooming in. And let's say this is the image you
[00:42:03.600 --> 00:42:07.000]   would just train on this bit instead of the complete image.
[00:42:07.000 --> 00:42:12.400]   Yeah. And on there, you had applied a bunch of augmentation.
[00:42:12.400 --> 00:42:16.480]   I didn't see a few names. I didn't see Mosaic and Clavier,
[00:42:16.480 --> 00:42:20.520]   which I think everyone had used. Did you not find them useful for
[00:42:20.520 --> 00:42:21.440]   your experiments?
[00:42:21.440 --> 00:42:27.480]   Yes, there's actually most of the default augmentation methods
[00:42:27.480 --> 00:42:38.480]   in YOLOv5 were very useful. But we find that rotation and
[00:42:39.000 --> 00:42:45.040]   transpose, specifically transpose, that are not in the
[00:42:45.040 --> 00:42:52.480]   default list of augmentation are also useful. So we just add it
[00:42:52.480 --> 00:42:58.800]   to the pipeline. And actually we have to do many, many experiments
[00:42:58.800 --> 00:43:05.080]   on turning augmentation methods for the computation, not only
[00:43:05.080 --> 00:43:15.280]   this one, but other computation. We always have to do like 100
[00:43:15.280 --> 00:43:21.000]   experiments on augmentation to see which augmentation methods
[00:43:21.000 --> 00:43:30.240]   is useful, which is not useful. It's not just about improving
[00:43:30.240 --> 00:43:35.360]   series scores, but also we can figure out some little secrets
[00:43:35.360 --> 00:43:40.960]   from the data sets by turning the augmentation methods. Let me
[00:43:40.960 --> 00:43:48.280]   give you a simple example. If I add a new augmentation method to
[00:43:48.280 --> 00:43:53.240]   the pipeline called motion blur, do you know motion blur? It's
[00:43:53.240 --> 00:44:00.120]   just like when you take a photo and your hand is not really
[00:44:00.120 --> 00:44:03.480]   very stable and you got some motion blur.
[00:44:03.480 --> 00:44:07.480]   Anyone who plays games knows of motion blur.
[00:44:07.480 --> 00:44:15.680]   Yes, when you play playing a card game or like that you get
[00:44:15.680 --> 00:44:21.240]   your screen like many blur and it's called motion blur. So if I
[00:44:21.240 --> 00:44:29.080]   add motion blur and find the local score improved, that mean
[00:44:29.800 --> 00:44:35.800]   in the data set there are many images that are blurred like
[00:44:35.800 --> 00:44:45.120]   motion blur. Yeah, so this is a very like we experience or
[00:44:45.120 --> 00:44:49.240]   experiment to do many experiments on augmentation to
[00:44:49.240 --> 00:44:55.040]   find the property of the data set to take little secret of the
[00:44:55.040 --> 00:45:01.680]   data set and try to deal with them to achieve better score.
[00:45:01.680 --> 00:45:08.480]   So, it sounds like image augmentation is really an
[00:45:08.480 --> 00:45:13.000]   important part of computer vision experiments. How do you
[00:45:13.000 --> 00:45:17.640]   Yes. How do you how do you decide what experiments to run?
[00:45:17.640 --> 00:45:22.840]   Do you just run a grid search? Do you just run all possible
[00:45:22.840 --> 00:45:26.360]   parameters? Or how are you experimenting with augmentation
[00:45:26.360 --> 00:45:28.520]   when you say you run 100?
[00:45:28.520 --> 00:45:35.000]   Yes. The first thing is you should know what what
[00:45:35.000 --> 00:45:41.240]   augmentation methods are usually useful like you can go through
[00:45:41.240 --> 00:45:47.000]   previous competitions to find out what what augmentation
[00:45:47.000 --> 00:45:52.040]   methods are they using. And finally you come up with a
[00:45:52.040 --> 00:45:58.280]   augmentation method list. Once you have the list, you can like
[00:45:58.280 --> 00:46:03.760]   try that one by one. And to figure out which one is useful
[00:46:03.760 --> 00:46:09.440]   and which one is not useful on current data sets. You must know
[00:46:09.440 --> 00:46:14.320]   that for different data sets the optimized augmentation methods
[00:46:14.320 --> 00:46:20.440]   are different, like what I just said maybe motion blur my my
[00:46:20.440 --> 00:46:25.200]   work on some data sets but not on others. It's not always
[00:46:25.200 --> 00:46:29.920]   working. Yes, so you have to when you switch to other data
[00:46:29.920 --> 00:46:35.440]   sets, you have to do it to turn to turn augmentation method from
[00:46:35.440 --> 00:46:40.760]   the beginning from the very beginning. So, like when when I
[00:46:40.760 --> 00:46:48.080]   have a new data set I usually when I build build a baseline on
[00:46:48.080 --> 00:46:54.080]   that I usually use not use any augmentation method to have a
[00:46:54.080 --> 00:47:01.200]   pure score of the data sets and then try to add augmentation
[00:47:01.200 --> 00:47:06.080]   methods one by one, but not many augmentation at the same time.
[00:47:06.080 --> 00:47:11.120]   If you do that you won't know which one are useful, which one
[00:47:11.120 --> 00:47:15.440]   is useful which one is not because they are just like mixed
[00:47:16.200 --> 00:47:21.440]   together and you you you just learned nothing from from the
[00:47:21.440 --> 00:47:25.080]   experiments, you have to try it one by one.
[00:47:25.080 --> 00:47:31.080]   I want to ask another question similar to this you said, when
[00:47:31.080 --> 00:47:35.320]   you started competing, you saw many people say something won't
[00:47:35.320 --> 00:47:39.760]   work. And then you tried and it worked. Many times people just
[00:47:39.760 --> 00:47:43.960]   give up without trying till it works. How do you decide when to
[00:47:43.960 --> 00:47:47.480]   stop because sometimes you can keep trying and it wouldn't work
[00:47:47.480 --> 00:47:49.680]   because it doesn't work on the data set.
[00:47:49.680 --> 00:47:57.640]   Okay, so how to say it's because most of the time I will just
[00:47:57.640 --> 00:48:06.880]   have a gut feel like I think in my work, but, but if some people
[00:48:06.880 --> 00:48:14.560]   is just post post discussion to say that it's just not working,
[00:48:14.560 --> 00:48:21.120]   but I will just first I will trust my gut feel and try it try
[00:48:21.120 --> 00:48:26.800]   it by myself. Like, how to say if you have attempt many
[00:48:26.800 --> 00:48:31.920]   computer vision competition, maybe you have that gut feel as
[00:48:31.920 --> 00:48:37.080]   well, you will you will know that at that time, maybe this
[00:48:37.080 --> 00:48:43.200]   idea will work at or at the other time that idea might work.
[00:48:43.200 --> 00:48:48.480]   Yes, if you have touched many many computer vision data set
[00:48:48.480 --> 00:48:53.320]   you, you will have that feel as well, I think.
[00:48:53.320 --> 00:48:58.320]   For the audience, I want to remind them that this gut
[00:48:58.320 --> 00:49:02.280]   feeling comes through 16 gold medals and 18 silver and two
[00:49:02.280 --> 00:49:06.960]   bronze medals. Gold medal is the most difficult thing to get on
[00:49:06.960 --> 00:49:10.480]   Kaggle and when Chishen says he has a gut feeling it comes
[00:49:10.480 --> 00:49:13.840]   through running 1000s, hundreds of 1000s of experiments in
[00:49:13.840 --> 00:49:15.520]   these competitions, I would guess.
[00:49:15.520 --> 00:49:16.920]   Definitely.
[00:49:16.920 --> 00:49:23.600]   So now coming back to your solution. I'll keep going in
[00:49:23.600 --> 00:49:26.480]   sequential order and please interrupt me if I miss anything.
[00:49:27.480 --> 00:49:32.120]   So now you have tried all of the augmentations and then you had
[00:49:32.120 --> 00:49:36.240]   optimized them based on cross validation. So your cross
[00:49:36.240 --> 00:49:40.920]   validation again was threefold cross validation split by video
[00:49:40.920 --> 00:49:46.200]   ID. So you were splitting across three sets, just based on the
[00:49:46.200 --> 00:49:47.400]   video IDs that
[00:49:47.400 --> 00:49:56.400]   yes, the training data has three only three videos. 011,
[00:49:56.400 --> 00:50:04.000]   01 and two. So, so the best way to split the is kind of like
[00:50:04.000 --> 00:50:09.600]   time series, although it's video so so we can just think it
[00:50:09.600 --> 00:50:16.800]   consider it as somehow time series data. So, random
[00:50:16.800 --> 00:50:21.800]   splitting is just not good, of course. And of course there are
[00:50:22.600 --> 00:50:27.400]   some people are splitting the data set using sequence ID.
[00:50:27.400 --> 00:50:32.800]   Sequence ID means that for for a single video that there are
[00:50:32.800 --> 00:50:41.560]   many sequence in that video. And they are splitting the data via
[00:50:41.560 --> 00:50:48.640]   by by sequence ID and they have higher series score rather than
[00:50:48.680 --> 00:50:59.920]   than us. But actually I think that my, how to say, if we are
[00:50:59.920 --> 00:51:06.640]   optimizing our cross validation score, we have, we have to
[00:51:06.640 --> 00:51:13.600]   consider the hardest case, like when, when the test that is
[00:51:14.080 --> 00:51:20.240]   taken in a very different environment, then it will be
[00:51:20.240 --> 00:51:28.040]   very difficult to predict starfish from that video. So we
[00:51:28.040 --> 00:51:35.480]   decided to split the data by video ID just by imaging that
[00:51:35.480 --> 00:51:40.840]   maybe you have two videos for training and one for validating.
[00:51:41.120 --> 00:51:48.400]   And of course, they have some diversities, they are not the
[00:51:48.400 --> 00:51:57.200]   same. And this is the most difficult case that we can split
[00:51:57.200 --> 00:52:04.040]   the data. Rather than, yes, instead of sequence please I we
[00:52:04.040 --> 00:52:05.520]   use videos, please.
[00:52:07.000 --> 00:52:11.520]   That makes sense. I learned I now now I understand the
[00:52:11.520 --> 00:52:14.040]   intuition. It was split like time series. I was trying to
[00:52:14.040 --> 00:52:18.600]   understand how did how does this make sense intuitively now I
[00:52:18.600 --> 00:52:20.120]   understand the gut feeling of it.
[00:52:20.120 --> 00:52:29.680]   Yeah, so we have to imagine what what the most difficult case,
[00:52:29.680 --> 00:52:33.720]   and we have to try to optimize that case.
[00:52:36.400 --> 00:52:41.200]   Continuing in your solution explanation after you got these
[00:52:41.200 --> 00:52:44.760]   bounding boxes. So for the audience in this you would have
[00:52:44.760 --> 00:52:48.040]   bounding boxes like for example, right now Chisholm's bounding
[00:52:48.040 --> 00:52:52.600]   box would say legend, Maya would say no. So that's how you would
[00:52:52.600 --> 00:52:57.200]   have bounding boxes for all these starfishes. And after
[00:52:57.200 --> 00:53:01.880]   that, I believe you rescored or re-evaluated these predictions.
[00:53:01.880 --> 00:53:05.120]   So all of these predictions that you had gotten from these YOLO
[00:53:05.120 --> 00:53:06.680]   models were then re-evaluated?
[00:53:06.680 --> 00:53:16.840]   Yes. We have like three. First we train three fold models and
[00:53:16.840 --> 00:53:22.320]   we find that for inference, it takes so much time for YOLO
[00:53:22.320 --> 00:53:28.240]   model for inference. So we finally before we submitted we
[00:53:28.240 --> 00:53:33.600]   train another model using the using exactly the same setting
[00:53:33.640 --> 00:53:37.960]   but using all data. And finally, we only use one one model for
[00:53:37.960 --> 00:53:45.720]   inference. And, but when we are training classification models,
[00:53:45.720 --> 00:53:52.080]   we have to use OOF that is generally generated by three
[00:53:52.080 --> 00:54:00.720]   fold models. Because it can avoid from data leak. So after
[00:54:01.000 --> 00:54:09.400]   we have our YOLO model trained and our OOF file generated, we
[00:54:09.400 --> 00:54:15.320]   ensemble those OOF model, OOF files and finally results in a
[00:54:15.320 --> 00:54:22.240]   very precise prediction with predicted boxes. And we crop
[00:54:22.240 --> 00:54:30.000]   them out. And then like, of course, we don't we don't resize
[00:54:30.000 --> 00:54:35.840]   it, we don't change its length and width, we just crop like a
[00:54:35.840 --> 00:54:40.840]   square, we don't like if the if the starfish is like like that,
[00:54:40.840 --> 00:54:46.080]   we don't just say simply resize it to a square, we just crop
[00:54:46.080 --> 00:54:51.760]   more environment information into that. And then we train
[00:54:51.760 --> 00:54:57.400]   class classification model on it. Actually, after the end of
[00:54:57.400 --> 00:55:05.760]   the competition, we found that I tried similar methods like us,
[00:55:05.760 --> 00:55:11.760]   but many people find many people find find it's just not
[00:55:11.760 --> 00:55:17.960]   working. Actually, I don't know why they find classification
[00:55:17.960 --> 00:55:22.720]   model not working. Because from from the very beginning of our
[00:55:23.880 --> 00:55:28.640]   classification model, it, it would it work from from the
[00:55:28.640 --> 00:55:34.040]   beginning. You know that because classification classification
[00:55:34.040 --> 00:55:40.920]   model also, we have to build a baseline model for classification
[00:55:40.920 --> 00:55:47.760]   and we find that our baseline is also working. And after that, we
[00:55:47.760 --> 00:55:52.920]   optimize the classification model and make it works better
[00:55:52.920 --> 00:55:58.000]   and better. And yes, actually, it can improve our cloud class
[00:55:58.000 --> 00:56:05.800]   validation, we improve our CV like 0.0, near 0.03 is quite
[00:56:05.800 --> 00:56:06.560]   quite a bit.
[00:56:06.560 --> 00:56:14.160]   Sanyam Bhutani: I think that's where the difference of your
[00:56:14.160 --> 00:56:17.760]   experience and other people's experience comes in. So as you
[00:56:17.760 --> 00:56:20.600]   were talking about the gut feeling that your experience was
[00:56:20.600 --> 00:56:22.400]   helpful in tying these models.
[00:56:23.000 --> 00:56:27.560]   Sometimes sometimes people may may not hit them something they
[00:56:27.560 --> 00:56:32.640]   may just they may just actually find that it's not working but
[00:56:32.640 --> 00:56:38.720]   not by accident or or something they for some reason they find
[00:56:38.720 --> 00:56:42.200]   find that it's not working. I don't know why.
[00:56:42.200 --> 00:56:50.320]   Okay. So I have two questions for you. May I ask why was very
[00:56:50.320 --> 00:56:53.680]   high dropout helpful because I didn't understand this. And
[00:56:53.680 --> 00:56:57.200]   second is, why did you use PC loss here?
[00:56:57.200 --> 00:57:05.400]   Yes, yes. Because the F2 score for this competition, we have to
[00:57:05.400 --> 00:57:15.040]   compute 0.3, the threshold from 0.3 to 0.8. While the 0.8 is a
[00:57:15.040 --> 00:57:21.200]   very high IOU. If someone is very familiar with object
[00:57:21.200 --> 00:57:26.440]   detection that you will know that many matrix we only compute
[00:57:26.440 --> 00:57:34.080]   IOU at the threshold of 0.5. It's a normal threshold. And
[00:57:34.080 --> 00:57:42.280]   because when we draw a bounding box on object, we might somehow
[00:57:42.400 --> 00:57:48.960]   is it never be a very accurate the box like one pixel left or
[00:57:48.960 --> 00:57:55.520]   one pixel up or down is not that different. So for avoiding that
[00:57:55.520 --> 00:58:02.640]   difference from the ground truth and predictive box, so we
[00:58:02.640 --> 00:58:08.440]   consider the IOU the intersection of union between
[00:58:08.440 --> 00:58:13.720]   the GT box and the predicted box is bigger than 0.5 that that
[00:58:13.720 --> 00:58:20.000]   would be a good box. But at this competition, we need to compute
[00:58:20.000 --> 00:58:27.960]   the score of the IOU bigger than 0.8. This is very, very
[00:58:27.960 --> 00:58:37.680]   difficult. So actually, we have no idea how to optimize those
[00:58:37.720 --> 00:58:46.920]   predicted box at that high IOU because the quality of the GT
[00:58:46.920 --> 00:58:53.240]   box is not not that stable. But yeah, it's quite normal because
[00:58:53.240 --> 00:58:58.600]   you know that different people are joining like like using
[00:58:58.600 --> 00:59:04.760]   different computer to draw the bounding box and they have a
[00:59:04.760 --> 00:59:09.800]   habit to do it. So, you know, so the quality job by different
[00:59:09.800 --> 00:59:18.120]   people from there are, I should say diversity. They are quite
[00:59:18.120 --> 00:59:25.200]   quite a diversity for those GT boxes. So it's really hard to
[00:59:25.200 --> 00:59:32.680]   optimize the score at at the threshold bigger than 0.8. So
[00:59:34.120 --> 00:59:40.000]   for why we use BC is just like when we use regression, like if
[00:59:40.000 --> 00:59:45.920]   we use regression methods to predict the IOU, it will be
[00:59:45.920 --> 00:59:52.200]   introduced many noise, just like I just said, the quality of the
[00:59:52.200 --> 00:59:58.600]   GT box is not stable. So, if you use regression that we just
[00:59:58.760 --> 01:00:05.080]   like introduce many, many noise from the GT box. It actually the
[01:00:05.080 --> 01:00:12.200]   GT box is not necessarily better than predicted box. So in this
[01:00:12.200 --> 01:00:23.000]   case, we choose like spinning the IOU split by 0.1 to make it
[01:00:23.200 --> 01:00:36.480]   a stable things and use BC loss to optimize it. Yes. Yes, like
[01:00:36.480 --> 01:00:42.560]   we consider the first thing as IOU bigger than 0.3. The second
[01:00:42.560 --> 01:00:50.280]   being as IOU bigger than 0.4. And if the IOU is bigger than
[01:00:50.280 --> 01:00:59.320]   0.9, we have all seven things equals to one. Yes, just, just
[01:00:59.320 --> 01:01:04.160]   to explain to the audience, usually they're with object
[01:01:04.160 --> 01:01:08.560]   detection, it's a bit relaxed for the IOU requirements, but
[01:01:08.560 --> 01:01:13.720]   here you wanted to perfectly identify starfishes. So you had,
[01:01:13.720 --> 01:01:17.360]   I think the requirement of having a very close box on
[01:01:17.360 --> 01:01:20.720]   wherever starfish were in the video. That's why the score had
[01:01:20.720 --> 01:01:22.920]   to be very high, if I understand correctly.
[01:01:22.920 --> 01:01:26.320]   Yes.
[01:01:26.320 --> 01:01:31.960]   I was just trying to make sure I understand correctly. So yes.
[01:01:31.960 --> 01:01:37.440]   Thanks. Thanks for explaining that. Now just to cover the
[01:01:37.440 --> 01:01:43.520]   post processing, I want to ask, I saw many people using WBF. I
[01:01:43.520 --> 01:01:45.920]   don't think your team used it. So
[01:01:45.920 --> 01:01:49.120]   Oh, also use WBF as ensemble.
[01:01:49.120 --> 01:01:56.200]   Yeah, actually, we, we actually don't consider WBF as post
[01:01:56.200 --> 01:02:02.480]   processing method. Let's just like we are doing on image
[01:02:02.480 --> 01:02:07.720]   classification or image segmentation, we can use a very
[01:02:07.720 --> 01:02:13.680]   simple method to ensemble like just take, take the take the
[01:02:13.680 --> 01:02:19.080]   average value of the prediction as ensemble value, ensemble
[01:02:19.080 --> 01:02:24.720]   score. But for object detection, it's not the case because you
[01:02:24.720 --> 01:02:29.200]   know that the different model is just predicting the box at
[01:02:29.200 --> 01:02:34.360]   different place and predicting the box at different confidence.
[01:02:34.360 --> 01:02:42.760]   And so we have a very common tool called WBF to ensemble
[01:02:42.760 --> 01:02:53.360]   those object detection model is it is introduced by very famous
[01:02:53.360 --> 01:03:01.160]   cargo and maybe you know, know him. I actually see he he's
[01:03:01.160 --> 01:03:03.560]   called ZF turbo.
[01:03:03.560 --> 01:03:04.480]   I don't know.
[01:03:04.480 --> 01:03:09.160]   ZF turbo. I don't know the real name. But I know they are a
[01:03:09.160 --> 01:03:10.000]   legend and
[01:03:10.320 --> 01:03:17.520]   yes, yes. Yes, he is very famous and the WBF method is
[01:03:17.520 --> 01:03:25.520]   introduced by him. And it's kind of like every object detection
[01:03:25.520 --> 01:03:33.480]   competition we all use WBF to do the ensemble after after he
[01:03:33.480 --> 01:03:35.240]   published the tool.
[01:03:38.280 --> 01:03:44.840]   Just to mention ZF turbo real quick. Once I went to their
[01:03:44.840 --> 01:03:49.640]   GitHub page and I saw that they have CNN implemented in C++ I
[01:03:49.640 --> 01:03:53.280]   got very scared that day. And I knew that he he is one of the
[01:03:53.280 --> 01:03:54.760]   best people just by that.
[01:03:54.760 --> 01:04:02.720]   Yeah, so we we don't we we haven't write that we use WBF
[01:04:02.760 --> 01:04:09.360]   because I we just think that it's kind of like default. If
[01:04:09.360 --> 01:04:14.920]   you attempt object detection competition, you have to use
[01:04:14.920 --> 01:04:16.880]   WBF by default.
[01:04:16.880 --> 01:04:20.480]   Yeah, so so we
[01:04:20.480 --> 01:04:24.080]   didn't know of that. I was just asking the stupid question
[01:04:24.080 --> 01:04:25.720]   because I was curious. I didn't see.
[01:04:26.960 --> 01:04:33.560]   Yes. So yeah, so so if you guys are just just going to attempt
[01:04:33.560 --> 01:04:37.000]   another object detection competition, so please remember
[01:04:37.000 --> 01:04:39.280]   WBF, you have to use it.
[01:04:39.280 --> 01:04:47.200]   Thanks for answering that. I believe we have covered all of
[01:04:47.200 --> 01:04:49.720]   your solution. Any other things you want to mention?
[01:04:51.720 --> 01:04:58.840]   Oh, yes. I think there's another little stories behind our team,
[01:04:58.840 --> 01:05:07.840]   because the F2 score described in the in the web page of the
[01:05:07.840 --> 01:05:15.320]   competition is not very detailed. So so all of us just
[01:05:15.320 --> 01:05:19.720]   implement F2 score algorithm by ourself, by our own
[01:05:19.720 --> 01:05:27.280]   understanding. And we, we find that we have many difference on
[01:05:27.280 --> 01:05:35.280]   the implementation. So yes, so at that time, and we just shared
[01:05:35.280 --> 01:05:43.200]   an old F file. And he claimed that the old F file has the
[01:05:43.240 --> 01:05:49.480]   cross validation score at 0.62. And I download his old F file
[01:05:49.480 --> 01:05:54.560]   and compute by my own F2 implementation and find that for
[01:05:54.560 --> 01:06:02.480]   my side, the old F file should score 0.68. Like, there is quite
[01:06:02.480 --> 01:06:11.360]   different from the implementation of F2 score. That
[01:06:11.360 --> 01:06:18.520]   this due to the cargo or the organizer just not explained
[01:06:18.520 --> 01:06:28.320]   F2 score well, clearly. Yes, yes, if you they have, they have
[01:06:28.320 --> 01:06:33.120]   a equation on on the page of the F2 score, but actually it's just
[01:06:33.160 --> 01:06:42.160]   not useful at all. Okay. Yeah, so okay, they even don't have
[01:06:42.160 --> 01:06:51.240]   any equation on the page. Okay. So like, they explained that
[01:06:51.240 --> 01:06:55.680]   they the predicted false should should be sorted by the score
[01:06:55.680 --> 01:07:00.680]   and the higher score once the higher score is matched by match
[01:07:00.680 --> 01:07:06.360]   to a GT score, the the then the GT score should should not be
[01:07:06.360 --> 01:07:13.720]   matched to either predicted box. But it's really like, we have,
[01:07:13.720 --> 01:07:18.640]   we have our own understanding and have different patient of
[01:07:18.640 --> 01:07:26.400]   that of the F2 score. Yes. So finally, we, we find that that
[01:07:26.400 --> 01:07:32.800]   the lowest score comes from comes from MVN. And I said
[01:07:32.800 --> 01:07:37.560]   before, we have to consider the most difficult case for the data
[01:07:37.560 --> 01:07:43.640]   splitting or the like, or we have to consider the
[01:07:43.640 --> 01:07:50.880]   generalization. So we choose to use the lowest one to as our
[01:07:51.600 --> 01:07:59.680]   cross validation score, which is come from MVN. So finally, our
[01:07:59.680 --> 01:08:13.680]   series score is at 0.74. For our final submission. Yes, I think
[01:08:13.680 --> 01:08:20.160]   if you go through the discussion or the notebook, my data might
[01:08:20.600 --> 01:08:26.640]   be some people claiming that their series score is at 0.8, or
[01:08:26.640 --> 01:08:31.720]   something like that. But actually, different people have
[01:08:31.720 --> 01:08:36.200]   an implementation of F2 score. So it's just comparable.
[01:08:36.200 --> 01:08:41.800]   Sanyam Bhutani: Thanks. Thanks for explaining your solution.
[01:08:41.800 --> 01:08:47.040]   And congratulations again on the first position finish. I think
[01:08:47.040 --> 01:08:50.320]   this was your third time you won the competition. I'm sure
[01:08:50.320 --> 01:08:53.440]   it was awesome. And thank you so much again for sharing your
[01:08:53.440 --> 01:08:58.280]   solution. Now I have these same questions that I ask every time
[01:08:58.280 --> 01:09:01.320]   I would love to finish the interview by asking them.
[01:09:01.320 --> 01:09:04.600]   Someone has already asked this. So I'll use their question,
[01:09:04.600 --> 01:09:09.040]   which says, How do you apply what you learn on Kaggle to
[01:09:09.040 --> 01:09:13.360]   real life? real world? Do you find it useful? And have you
[01:09:13.360 --> 01:09:14.560]   ever found it useful?
[01:09:16.120 --> 01:09:24.680]   Okay, that's absolutely very useful. Well, as for my work,
[01:09:24.680 --> 01:09:28.840]   for my full time work, I mainly work on recommendation system,
[01:09:28.840 --> 01:09:37.800]   but sometimes some computer vision tasks as well. Like, just
[01:09:37.800 --> 01:09:43.720]   I just like it just many tricks. It's they're not only working on
[01:09:43.720 --> 01:09:49.120]   public data sets, but also work on other data sets. We come to
[01:09:49.120 --> 01:09:53.680]   Kaggle to figure out which, which idea which tricks which
[01:09:53.680 --> 01:09:59.880]   paper are not only working on image net and Coco, but also
[01:09:59.880 --> 01:10:05.440]   working on Kaggle data sets. If it's the case, like, we have a
[01:10:05.440 --> 01:10:11.480]   trick, we have a new seeing architecture that are not only
[01:10:11.480 --> 01:10:16.920]   work on works on image net and Coco, but also work on Kaggle
[01:10:16.920 --> 01:10:27.840]   data sets. It must be works on our own data sets to of our own
[01:10:27.840 --> 01:10:35.720]   of of my company as well. It's kind of like, we have made we
[01:10:35.720 --> 01:10:42.280]   have do many experience experiment on Kaggle. And we
[01:10:42.280 --> 01:10:48.880]   don't need to do it again, using our own data sets, actually. So
[01:10:48.880 --> 01:10:55.880]   like, I have a very good setting at a Kaggle data set. And
[01:10:55.880 --> 01:11:01.240]   somehow, sometimes we can just copy the pipeline to our own
[01:11:01.720 --> 01:11:08.280]   task. And it usually just gives a very reasonable results,
[01:11:08.280 --> 01:11:13.040]   although not maybe not the best, but it's kind of good enough.
[01:11:13.040 --> 01:11:20.200]   Like, because because at company we are not doing it. We are not
[01:11:20.200 --> 01:11:24.360]   doing competition, we are just we need to consider both
[01:11:24.360 --> 01:11:30.360]   performance, both accuracy, runtime, the speed of the
[01:11:30.360 --> 01:11:35.040]   model. So sometimes we don't need the accuracy to be the
[01:11:35.040 --> 01:11:41.680]   best, but we can like we, for example, we use efficient math
[01:11:41.680 --> 01:11:46.560]   B seven when we are cuddling, and we just simply switch it to
[01:11:46.560 --> 01:11:55.080]   efficient that like B two or B three. Work. Yes. So definitely
[01:11:55.120 --> 01:12:01.520]   many useful trick, many useful architecture, many useful ideas
[01:12:01.520 --> 01:12:06.320]   on cargo that can be transferred to our own work directly.
[01:12:06.320 --> 01:12:13.400]   Sanyam Bhutani: Okay, thanks. Thanks for sharing that. I, are
[01:12:13.400 --> 01:12:16.800]   you taking part in this competition? Are you looking at
[01:12:16.800 --> 01:12:19.920]   this? Because you mentioned you work on recommender systems?
[01:12:22.280 --> 01:12:29.880]   Yes, actually, we my I and some of my friends at the company,
[01:12:29.880 --> 01:12:35.560]   my day parking that competition in next month.
[01:12:35.560 --> 01:12:41.880]   Okay. So this is one of the most difficult questions some
[01:12:41.880 --> 01:12:45.960]   Kagglers have found. Are you ready? Because I want to share
[01:12:45.960 --> 01:12:53.880]   that. Are yours? I'm not sure if you know him and Jeeba. They
[01:12:53.880 --> 01:12:57.240]   found this very difficult. So the question is, which is your
[01:12:57.240 --> 01:13:00.800]   single favorite game of all time? One game that you like the
[01:13:00.800 --> 01:13:01.200]   most?
[01:13:01.200 --> 01:13:05.080]   Sorry, I don't understand.
[01:13:05.080 --> 01:13:09.440]   Sorry, one computer game. Yeah, one video game that you like the
[01:13:09.440 --> 01:13:11.960]   most. So your favorite game of all time.
[01:13:13.520 --> 01:13:21.840]   Okay. Okay. Okay. So it's like, it's definitely not
[01:13:21.840 --> 01:13:29.880]   advertisement. But it's called Genshin Impact. Maybe maybe you
[01:13:29.880 --> 01:13:36.080]   know that? I know. Okay. Yes, I like I like Genshin pack very
[01:13:36.080 --> 01:13:38.440]   much and I play it every day.
[01:13:39.160 --> 01:13:43.800]   Okay. Thanks for that. My my last question, which I always
[01:13:43.800 --> 01:13:48.880]   ask is, what is your best advice to someone who is just starting
[01:13:48.880 --> 01:13:52.320]   on Kaggle? So to Anup, what is your best advice?
[01:13:52.320 --> 01:14:01.040]   Okay, so do read many, many notebook and discussion, but
[01:14:01.640 --> 01:14:12.160]   don't trust it directly. You can trust it 50 to 70%. But do not
[01:14:12.160 --> 01:14:18.360]   100% trust it. That's my advice. Yes.
[01:14:18.360 --> 01:14:24.160]   Okay, thanks. Thanks for sharing that. To wrap up, I'll mention
[01:14:24.160 --> 01:14:28.040]   your Kaggle profile again to the audience so that they can find
[01:14:28.040 --> 01:14:31.680]   you on Kaggle on there. They can also find your LinkedIn profile
[01:14:31.680 --> 01:14:36.000]   and they can connect with you on LinkedIn any place else where
[01:14:36.000 --> 01:14:37.080]   people can find you.
[01:14:37.080 --> 01:14:40.760]   Yes.
[01:14:40.760 --> 01:14:45.760]   Sorry, are these the two platforms where you're active or
[01:14:45.760 --> 01:14:48.320]   any other platforms you want to mention?
[01:14:48.320 --> 01:14:54.880]   Oh, okay. Not for now. Yes. You can find me at Kaggle or
[01:14:54.920 --> 01:14:55.760]   LinkedIn.
[01:14:55.760 --> 01:15:00.840]   Okay, thanks. Thanks for that. Again, thank you so much for
[01:15:00.840 --> 01:15:04.480]   your time. And it's been an honor learning from you. I also
[01:15:04.480 --> 01:15:08.240]   want to share with the audience. Jishan told me before we started
[01:15:08.240 --> 01:15:11.520]   that he doesn't interact a lot in English. And he was kind
[01:15:11.520 --> 01:15:14.520]   enough to do this interview in English. So we are very lucky
[01:15:14.520 --> 01:15:19.360]   because I know many Kagglers in Japan and China, they don't
[01:15:19.360 --> 01:15:22.520]   interact in English a lot. So we are really lucky to learn from
[01:15:22.520 --> 01:15:23.240]   him today.
[01:15:23.760 --> 01:15:29.520]   Yeah, sorry for my poor English skills, but next time you
[01:15:29.520 --> 01:15:30.680]   practice more.
[01:15:30.680 --> 01:15:35.320]   I didn't mean it like that. I just wanted to really thank you
[01:15:35.320 --> 01:15:41.040]   for. Yeah. So any, any final words you want to mention before
[01:15:41.040 --> 01:15:44.040]   we wrap up? Or should we should we wrap up here?
[01:15:44.040 --> 01:15:48.960]   Yeah, just good luck Kaggling. Yeah.
[01:15:48.960 --> 01:15:53.520]   Thanks again. And thank you everyone who joined us today.
[01:15:53.520 --> 01:15:55.520]   Yeah, thank you as well.

