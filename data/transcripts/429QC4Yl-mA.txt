
[00:00:00.000 --> 00:00:08.900]   We almost think about it as the safety of airplane.
[00:00:08.900 --> 00:00:11.400]   There will be multiple layers into it.
[00:00:11.400 --> 00:00:17.080]   You could imagine that one layer might have to do with appropriate data filtering, or
[00:00:17.080 --> 00:00:23.520]   maybe then another layer has to do with injecting human feedback, and maybe some final, let's
[00:00:23.520 --> 00:00:26.880]   say, discrimination by the model at the very end.
[00:00:26.880 --> 00:00:31.240]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:31.240 --> 00:00:33.400]   and I'm your host, Lukas Biewald.
[00:00:33.400 --> 00:00:39.480]   Today we're talking to Wojciech Zaremba, who's one of the co-founders of OpenAI.
[00:00:39.480 --> 00:00:44.220]   He's worked on the robotics team through most of his time there, where he made the hand
[00:00:44.220 --> 00:00:47.840]   that manipulated and solved a Rubik's cube.
[00:00:47.840 --> 00:00:52.040]   At Weights & Biases, we have been working with him for quite a long time and rooting
[00:00:52.040 --> 00:00:53.800]   for his team.
[00:00:53.800 --> 00:01:00.240]   I'm super excited to get tactical on robotics, but also Wojciech loves to think deeply about
[00:01:00.240 --> 00:01:03.840]   the bigger picture in AI, and so we'll get into that too.
[00:01:03.840 --> 00:01:08.760]   The first question I wanted to ask you about was what it was like starting OpenAI.
[00:01:08.760 --> 00:01:15.680]   The first time I heard the idea when I met with Greg in New York, and actually even when
[00:01:15.680 --> 00:01:22.040]   I was about to meet the first time, I overslept and we were about to meet at 5 PM.
[00:01:22.040 --> 00:01:29.640]   I had this weird working schedule that I used to do research over the night, and I was going
[00:01:29.640 --> 00:01:35.960]   to sleep at 6 AM, 7 AM, so I overslept for our meeting at 5 PM.
[00:01:35.960 --> 00:01:37.800]   But eventually we met.
[00:01:37.800 --> 00:01:43.800]   I would say early on there was some discussion about the mission of the company.
[00:01:43.800 --> 00:01:50.360]   It's also interesting that back then in the community, whenever someone spoke about safety,
[00:01:50.360 --> 00:01:52.520]   they were considered pretty much crazy.
[00:01:52.520 --> 00:01:58.280]   Many people were saying, "Oh, AI is so far away that it actually makes no sense to speak
[00:01:58.280 --> 00:01:59.280]   about it."
[00:01:59.280 --> 00:02:03.480]   There were even these quotes saying that it's like thinking about overpopulation on Mars.
[00:02:03.480 --> 00:02:09.560]   Then at some point that might be a problem, but we shouldn't be concerned about it today.
[00:02:09.560 --> 00:02:11.920]   I was also very excited.
[00:02:11.920 --> 00:02:18.160]   I told Greg that one of the most important people that we have to have is Ilya, and we've
[00:02:18.160 --> 00:02:19.160]   got Ilya.
[00:02:19.160 --> 00:02:25.280]   Then there was a meeting around November 2015 in Napa.
[00:02:25.280 --> 00:02:26.520]   I met there with Sam.
[00:02:26.520 --> 00:02:30.240]   I met there with Greg, Ilya, John Schulman.
[00:02:30.240 --> 00:02:33.200]   There was also Andre, who is now at Tesla.
[00:02:33.200 --> 00:02:39.000]   Of course, we discussed AGI, what are the steps, what do we think is missing.
[00:02:39.000 --> 00:02:49.480]   It was also quite cool to see that these folks are thinking about, even during spare time,
[00:02:49.480 --> 00:02:53.360]   about big fundamental questions.
[00:02:53.360 --> 00:02:59.040]   There was this time that we're sitting at the table, and Sam Altman asked everyone what
[00:02:59.040 --> 00:03:06.080]   they think was the solution to Fermi paradox, why we don't observe aliens.
[00:03:06.080 --> 00:03:09.120]   People had very sophisticated opinion about this topic.
[00:03:09.120 --> 00:03:14.200]   I was thinking, "Oh, that's the group of people with whom I would like to work."
[00:03:14.200 --> 00:03:20.280]   They consider this even like a metaphysical questions, because it's almost like the questions
[00:03:20.280 --> 00:03:23.680]   about AGI are almost like a metaphysical.
[00:03:23.680 --> 00:03:24.680]   That makes sense.
[00:03:24.680 --> 00:03:31.800]   But I guess what's kind of intriguing about the way OpenAI operates is, at its core, its
[00:03:31.800 --> 00:03:35.160]   mission is focused on AI safety, I believe.
[00:03:35.160 --> 00:03:40.240]   But it seems like the sort of remarkable results coming out of OpenAI, including the stuff
[00:03:40.240 --> 00:03:47.200]   that you work on, seems less about safety and more about showing the power of AI or
[00:03:47.200 --> 00:03:48.480]   moving the field forward.
[00:03:48.480 --> 00:03:50.480]   Is that right?
[00:03:50.480 --> 00:03:52.480]   What's the thinking there?
[00:03:52.480 --> 00:03:54.800]   We have plenty of team working on safety.
[00:03:54.800 --> 00:04:02.240]   One of the efforts working on safety was, let's try to foresight what it takes to build
[00:04:02.240 --> 00:04:04.180]   AGI.
[00:04:04.180 --> 00:04:09.040]   People started to look from perspective of resources, how the things actually scale and
[00:04:09.040 --> 00:04:10.040]   connect.
[00:04:10.040 --> 00:04:17.600]   Then, they realized that we are able to make our models significantly better with appropriate
[00:04:17.600 --> 00:04:18.600]   scale up.
[00:04:18.600 --> 00:04:23.800]   In some sense, that was actually a result of safety work.
[00:04:23.800 --> 00:04:29.600]   Another example maybe like that is, there has been work on human feedback.
[00:04:29.600 --> 00:04:34.820]   The idea is, how could we inject human values into the model?
[00:04:34.820 --> 00:04:38.740]   How could we tell model what is good versus what is bad?
[00:04:38.740 --> 00:04:44.940]   Of course, at first, people started to work in the most simplified domain.
[00:04:44.940 --> 00:04:50.420]   The question is, how can we tell model that the given summary of text is good versus bad?
[00:04:50.420 --> 00:04:55.340]   It turns out that actually this development led to capabilities, but motivation was from
[00:04:55.340 --> 00:04:57.020]   safety perspective.
[00:04:57.020 --> 00:05:04.940]   Our stance regarding safety is, we would wish to maximally release the capabilities and
[00:05:04.940 --> 00:05:07.220]   description how to build safe systems.
[00:05:07.220 --> 00:05:13.220]   It's also very likely that safety and capabilities might be the same thing.
[00:05:13.220 --> 00:05:18.900]   In some sense, safety means what to do to make sure that we can control the model.
[00:05:18.900 --> 00:05:20.980]   There's various levels of safety.
[00:05:20.980 --> 00:05:25.600]   One level of safety is what to do to make sure that we can control the model.
[00:05:25.600 --> 00:05:29.820]   This is also very similar from perspective of commercialization or capabilities.
[00:05:29.820 --> 00:05:31.700]   That's also what you want to happen.
[00:05:31.700 --> 00:05:37.140]   You don't want the model such that it go nuts when you're asking some slightly out of distribution
[00:05:37.140 --> 00:05:38.140]   question.
[00:05:38.140 --> 00:05:45.700]   When you think from perspective of our mission, the mission is to serve humanity.
[00:05:45.700 --> 00:05:54.100]   There are actually three different axes how you can distribute what we have developed.
[00:05:54.100 --> 00:05:59.400]   One axis is you can literally just give people money, something like universal basic income.
[00:05:59.400 --> 00:06:05.680]   That still requires actually making a lot of money to make any difference to people.
[00:06:05.680 --> 00:06:08.880]   Second one is you can give away technology.
[00:06:08.880 --> 00:06:13.680]   You are, let's say, building technology and they are actually sharing it maximally.
[00:06:13.680 --> 00:06:15.120]   Third one is governance.
[00:06:15.120 --> 00:06:21.680]   The question is how to make sure that the humanity as a whole can decide on what to
[00:06:21.680 --> 00:06:23.560]   do with this technology.
[00:06:23.560 --> 00:06:31.760]   And OpenAI actually is interested in each of these axes and various stages.
[00:06:31.760 --> 00:06:35.380]   I'm kind of curious, do you fear AGI?
[00:06:35.380 --> 00:06:39.720]   You talked about Fermi's paradox and it seems like one reason that we don't see aliens might
[00:06:39.720 --> 00:06:46.020]   be that they develop AGI or some technology and kill themselves inevitably.
[00:06:46.020 --> 00:06:47.980]   That could be one reason that we don't see them.
[00:06:47.980 --> 00:06:49.500]   Do you worry about that?
[00:06:49.500 --> 00:06:51.540]   Do you put a percent probability on that?
[00:06:51.540 --> 00:06:54.420]   Is that something you imagine might happen in your lifetime?
[00:06:54.420 --> 00:06:59.860]   Yes, I think it's possible, but I think that there will be actually various stages of AI
[00:06:59.860 --> 00:07:00.860]   developments.
[00:07:00.860 --> 00:07:06.200]   The first stage is when AI will become very valuable commercially.
[00:07:06.200 --> 00:07:09.860]   And I believe that might be multi-trillion industry.
[00:07:09.860 --> 00:07:15.700]   Then second stage is actually AI might become a national security threat.
[00:07:15.700 --> 00:07:24.060]   So you could imagine that AI could be used to control a farm of bots or manipulate elderly
[00:07:24.060 --> 00:07:28.220]   or sway public opinion for some election or so.
[00:07:28.220 --> 00:07:33.100]   And in some sense, you can say that it's already happening in some form.
[00:07:33.100 --> 00:07:40.060]   There is a selectively displayed content online that actually biases people in various ways.
[00:07:40.060 --> 00:07:45.660]   The first stage is essentially that the value of technology is just keep on increasing.
[00:07:45.660 --> 00:07:48.500]   Second stage, it's national security.
[00:07:48.500 --> 00:07:53.200]   And then the third stage is existential risk to humanity.
[00:07:53.200 --> 00:07:58.140]   It's almost a question how they are spreading in time and so on.
[00:07:58.140 --> 00:08:03.240]   And usually we should just be worried that the initial parts of the sequence and we should
[00:08:03.240 --> 00:08:05.420]   bear in mind all the pieces.
[00:08:05.420 --> 00:08:07.460]   We shouldn't just focus on the last one.
[00:08:07.460 --> 00:08:08.460]   I see.
[00:08:08.460 --> 00:08:10.580]   So we should focus on all three of those risks then.
[00:08:10.580 --> 00:08:11.580]   Correct.
[00:08:11.580 --> 00:08:12.580]   Or like the first one is not the risk.
[00:08:12.580 --> 00:08:14.420]   It's like an increase of commercial value.
[00:08:14.420 --> 00:08:17.540]   I guess maybe the risk might be job misplacement.
[00:08:17.540 --> 00:08:18.540]   Right.
[00:08:18.540 --> 00:08:19.540]   Right.
[00:08:19.540 --> 00:08:26.020]   I mean, do you have a sense of, for yourself, a probability that you put on existential
[00:08:26.020 --> 00:08:27.020]   risk?
[00:08:27.020 --> 00:08:33.660]   It's actually hard for me to think here in terms of probabilities.
[00:08:33.660 --> 00:08:39.860]   I could tell you some convincing stories and I noticed that these probabilities, they really
[00:08:39.860 --> 00:08:45.100]   change over the time depending on some external factors and so on.
[00:08:45.100 --> 00:08:47.800]   What external factors change your probabilities?
[00:08:47.800 --> 00:08:49.980]   Because we're not really getting new information, right?
[00:08:49.980 --> 00:08:50.980]   Yeah.
[00:08:50.980 --> 00:08:54.460]   So I'm saying external factors like political climate or so.
[00:08:54.460 --> 00:08:55.940]   Ah, I see.
[00:08:55.940 --> 00:08:56.940]   Let's see.
[00:08:56.940 --> 00:09:00.740]   So let me tell you the gloom story and I can tell you, let's say, positive story.
[00:09:00.740 --> 00:09:01.740]   Okay, great.
[00:09:01.740 --> 00:09:04.460]   So let's get the gloom and then do the positive one.
[00:09:04.460 --> 00:09:10.020]   In principle, I can say that it's almost like an inevitable that we'll build superhuman
[00:09:10.020 --> 00:09:11.020]   AI.
[00:09:11.020 --> 00:09:13.100]   It's just a matter of time.
[00:09:13.100 --> 00:09:18.460]   Then it's also very likely that we'll end up actually with multiple organizations building
[00:09:18.460 --> 00:09:21.300]   it because it's so valuable and there will be a competition.
[00:09:21.300 --> 00:09:26.380]   There might be some organization ahead, but it's very likely that we'll end up with multiple
[00:09:26.380 --> 00:09:27.860]   organizations.
[00:09:27.860 --> 00:09:33.540]   Then there will be, then I can say, various people will be tinkering with the code of
[00:09:33.540 --> 00:09:39.460]   AI and AI will be tinkering with its own code and it will have powerful capabilities to
[00:09:39.460 --> 00:09:40.740]   achieve various goals.
[00:09:40.740 --> 00:09:44.940]   And initially, this would be a goal given by a human.
[00:09:44.940 --> 00:09:55.220]   But then I can notice that at least in case of natural organisms that also are derived
[00:09:55.220 --> 00:09:59.940]   from a code that's a DNA code, there is this property that if you slightly mess up the
[00:09:59.940 --> 00:10:04.380]   code, it actually, the organism might misbehave.
[00:10:04.380 --> 00:10:06.900]   It actually might work against the host.
[00:10:06.900 --> 00:10:13.940]   So in case of cells, it's actually possible to get the cancer and cancer is a prevalent
[00:10:13.940 --> 00:10:16.540]   phenomenon in the nature.
[00:10:16.540 --> 00:10:21.060]   So then you could imagine, now in case of AIs, maybe if you have a couple of AIs, then
[00:10:21.060 --> 00:10:24.820]   we actually know what they are optimizing for and who they serve.
[00:10:24.820 --> 00:10:29.420]   But once then there is an increased number of AIs and some sense, there is a process
[00:10:29.420 --> 00:10:33.100]   of mutation, which is AIs are modifying its own code.
[00:10:33.100 --> 00:10:35.140]   Humans are modifying their own code.
[00:10:35.140 --> 00:10:39.420]   Then there is a process of natural selection.
[00:10:39.420 --> 00:10:46.660]   And you can say that the AI that literally wants to maximally spread will be the one
[00:10:46.660 --> 00:10:47.660]   that will exist.
[00:10:47.660 --> 00:10:54.220]   The things in the universe that want to replicate are the things that exist.
[00:10:54.220 --> 00:11:00.020]   Here the main difference is that AI will have just a huge power.
[00:11:00.020 --> 00:11:02.260]   Therefore it's kind of risky.
[00:11:02.260 --> 00:11:09.180]   What are the consequences of AI wanting really just to optimize for replication?
[00:11:09.180 --> 00:11:14.100]   So I guess that's maybe a gloom scenario.
[00:11:14.100 --> 00:11:17.620]   Wait, one question I always have about the gloom scenario.
[00:11:17.620 --> 00:11:24.060]   It makes sense to me, but I feel like the metaphor of natural selection, well, at least
[00:11:24.060 --> 00:11:27.780]   with plants and animals, we reproduce.
[00:11:27.780 --> 00:11:36.240]   So you can't change the whole system at once, but it seems like AI might have a more kind
[00:11:36.240 --> 00:11:39.740]   of complicated system of changing and reproduction.
[00:11:39.740 --> 00:11:43.380]   You could imagine all the AIs changing at once or communicating.
[00:11:43.380 --> 00:11:47.220]   It sort of seems like you might not necessarily...
[00:11:47.220 --> 00:11:53.620]   You could imagine a stable equilibrium where things aren't allowed to consume all the resources,
[00:11:53.620 --> 00:11:54.620]   for example.
[00:11:54.620 --> 00:11:55.620]   Right?
[00:11:55.620 --> 00:11:58.000]   Or am I missing something?
[00:11:58.000 --> 00:12:02.820]   It's possible that we'll have thousands of benign AIs.
[00:12:02.820 --> 00:12:07.860]   And it might be not that simple even to get all the resources, but you could imagine that
[00:12:07.860 --> 00:12:12.160]   randomly happens so that one of the AIs won't be that benign.
[00:12:12.160 --> 00:12:16.260]   And it happened because people are modifying code because it started optimizing different
[00:12:16.260 --> 00:12:18.340]   reward function.
[00:12:18.340 --> 00:12:24.340]   And it still has immense skills and then it can pursue its goal.
[00:12:24.340 --> 00:12:31.380]   Then it might be the case that other AIs are kind of defending the system, or maybe they
[00:12:31.380 --> 00:12:35.100]   were never trained for defending.
[00:12:35.100 --> 00:12:42.080]   And it's very hard to predict the dynamics in multi-agent setup.
[00:12:42.080 --> 00:12:45.560]   With one AI, you can maybe predict what are the possibilities.
[00:12:45.560 --> 00:12:47.520]   It would be still, let's say, extremely hard.
[00:12:47.520 --> 00:12:52.800]   But once you have many of them competing in some sense for resources, very hard to say
[00:12:52.800 --> 00:12:55.200]   actually what might be the consequences.
[00:12:55.200 --> 00:12:56.200]   Okay.
[00:12:56.200 --> 00:12:58.000]   So tell me the positive story.
[00:12:58.000 --> 00:13:04.280]   I can say that even if AI would become so powerful, it wouldn't even care that much
[00:13:04.280 --> 00:13:05.280]   to be here.
[00:13:05.280 --> 00:13:07.760]   It would just go to the stars.
[00:13:07.760 --> 00:13:10.560]   It would build all sorts of technology for us.
[00:13:10.560 --> 00:13:16.080]   It's like the same way as we are not competing with crystals.
[00:13:16.080 --> 00:13:17.080]   Crystals are also replicating.
[00:13:17.080 --> 00:13:19.120]   It's like a self-replicating machinery.
[00:13:19.120 --> 00:13:25.920]   It's kind of in different level of abstraction and it doesn't bother us that they are replicating.
[00:13:25.920 --> 00:13:30.400]   Of course, there's all the advancements that could happen.
[00:13:30.400 --> 00:13:39.520]   So could imagine that AI would cure all the diseases, remove suffering, allow us to go
[00:13:39.520 --> 00:13:42.520]   to the stars, and so on.
[00:13:42.520 --> 00:13:47.720]   It's interesting though, in both the scenarios, it involves steadily consuming resources and
[00:13:47.720 --> 00:13:48.720]   expanding.
[00:13:48.720 --> 00:13:53.420]   It's just sort of in one, the AI leaves us alone, and the other, it doesn't care or maybe
[00:13:53.420 --> 00:13:55.860]   consumes our planet.
[00:13:55.860 --> 00:14:00.460]   But in both cases, wouldn't you think that we would see evidence of this in some other
[00:14:00.460 --> 00:14:06.260]   alien life that created an AI and came to us in some self-replicating way?
[00:14:06.260 --> 00:14:08.580]   What do you think about that?
[00:14:08.580 --> 00:14:11.100]   You are asking the question about Fermi paradox.
[00:14:11.100 --> 00:14:12.100]   Yeah.
[00:14:12.100 --> 00:14:13.100]   Sorry, you brought it up.
[00:14:13.100 --> 00:14:14.100]   Sorry.
[00:14:14.100 --> 00:14:15.100]   It's top of mind.
[00:14:15.100 --> 00:14:22.580]   Isn't there a collapse scenario, I guess?
[00:14:22.580 --> 00:14:27.180]   So let's say you said, "Oh, if aliens would build AGI and then AGI destroyed them, but
[00:14:27.180 --> 00:14:30.700]   then we would see some traces of AGI in the universe.
[00:14:30.700 --> 00:14:34.620]   Like the AGI would consume a lot of resources."
[00:14:34.620 --> 00:14:37.420]   Assuming that actually once you are advancing your...
[00:14:37.420 --> 00:14:38.420]   So there's a few assumptions.
[00:14:38.420 --> 00:14:44.660]   There is assumption that once you are sufficiently technologically advanced, then you are spreading
[00:14:44.660 --> 00:14:47.740]   in every direction in the universe with a speed of light.
[00:14:47.740 --> 00:14:54.300]   And we haven't observed in any parts of the universe, anything like that.
[00:14:54.300 --> 00:14:57.060]   We haven't seen any Dyson spheres or so.
[00:14:57.060 --> 00:15:02.980]   So one simple explanation might be that actually we are alone in the universe.
[00:15:02.980 --> 00:15:09.020]   Maybe it's so unlikely for life to flourish that we are alone.
[00:15:09.020 --> 00:15:14.180]   So that almost like puts maybe more responsibility, but who knows?
[00:15:14.180 --> 00:15:16.780]   Is that what you believe?
[00:15:16.780 --> 00:15:20.300]   I have a probability distribution of beliefs.
[00:15:20.300 --> 00:15:21.300]   What might be the case?
[00:15:21.300 --> 00:15:22.300]   Tell me.
[00:15:22.300 --> 00:15:25.620]   You can't reveal your distribution?
[00:15:25.620 --> 00:15:26.620]   So let's see.
[00:15:26.620 --> 00:15:30.260]   I can tell you a fun one that I heard recently.
[00:15:30.260 --> 00:15:38.880]   So let's say we are having super advanced civilization, then of course it makes sense
[00:15:38.880 --> 00:15:47.840]   to turn the entire planets into computer and to kind of maximally use matter for the purpose
[00:15:47.840 --> 00:15:51.220]   of computation.
[00:15:51.220 --> 00:15:59.140]   And then one thing that is actually interesting is apparently once the universe would be cooler,
[00:15:59.140 --> 00:16:03.380]   then it is possible to do more efficient computation.
[00:16:03.380 --> 00:16:08.060]   So one statement is that maybe aliens are just waiting for the universe to be cooler.
[00:16:08.060 --> 00:16:12.580]   I'm not sure if I believe in this.
[00:16:12.580 --> 00:16:18.300]   That might be a cool description.
[00:16:18.300 --> 00:16:23.020]   So I guess how do these beliefs inform the work that you do?
[00:16:23.020 --> 00:16:30.380]   You talked about sort of two kind of bad AI scenarios that both actually seem very relevant
[00:16:30.380 --> 00:16:31.380]   to me.
[00:16:31.380 --> 00:16:36.680]   I sort of feel like the sort of inequality feels real to me right now at this moment
[00:16:36.680 --> 00:16:40.980]   and the political stuff also feels like it's starting to become real.
[00:16:40.980 --> 00:16:45.620]   And then the existential threat feels like you're telling me a very compelling story,
[00:16:45.620 --> 00:16:52.020]   but somehow it doesn't feel the same visceral fear for me and my child.
[00:16:52.020 --> 00:16:53.020]   But maybe that's irrational.
[00:16:53.020 --> 00:16:57.740]   Like how do you think about, are those three worries what really drives you to do your
[00:16:57.740 --> 00:17:00.220]   work or are they more theoretical for you?
[00:17:00.220 --> 00:17:04.460]   And how do you kind of weight the different AI safety issues?
[00:17:04.460 --> 00:17:11.340]   See let me at first, let me try to even describe where the usually the drive comes from.
[00:17:11.340 --> 00:17:24.420]   So as a kid, I did quite a lot of mathematics and I realized that in mathematics, I've got
[00:17:24.420 --> 00:17:28.060]   a lot of pleasure by solving difficult problems.
[00:17:28.060 --> 00:17:36.580]   But all of a sudden this amazing moment of excitement once I was able to figure out a
[00:17:36.580 --> 00:17:39.700]   solution to some mathematical problem.
[00:17:39.700 --> 00:17:47.660]   And I actually realized that that's the main drive for the majority of scientists, that
[00:17:47.660 --> 00:17:53.460]   there is like a just very complicated puzzle involving mathematics and computers.
[00:17:53.460 --> 00:18:01.260]   And somehow they can put all the pieces together and that actually gives them amazing excitement.
[00:18:01.260 --> 00:18:03.420]   So that's cool.
[00:18:03.420 --> 00:18:13.700]   But simultaneously it would be very sad if due to this excitement, we would actually
[00:18:13.700 --> 00:18:17.300]   destroy a lot of value or destroy how the humans operate and so on.
[00:18:17.300 --> 00:18:26.100]   So now there is a piece of me that is excited about the technology, about solving mathematical
[00:18:26.100 --> 00:18:28.460]   and computer science problems.
[00:18:28.460 --> 00:18:35.460]   And there is also part of me like thinking maybe from perspective of altruism and responsibility.
[00:18:35.460 --> 00:18:44.060]   So like at some point of my life, I realized that ultimately the happiness comes from within
[00:18:44.060 --> 00:18:47.740]   and I actually have already everything that I need.
[00:18:47.740 --> 00:18:53.180]   So then it's almost like my cup is full.
[00:18:53.180 --> 00:18:58.380]   Just want to make sure that actually that it's enough for others.
[00:18:58.380 --> 00:19:07.820]   So then it becomes quite natural to think how can I actually make sure that my work
[00:19:07.820 --> 00:19:10.380]   has the maximally positive impact.
[00:19:10.380 --> 00:19:17.460]   And in case of AI, it is actually quite complicated.
[00:19:17.460 --> 00:19:20.660]   Why did you choose to work on robotics?
[00:19:20.660 --> 00:19:23.100]   So actually here is a reveal.
[00:19:23.100 --> 00:19:28.900]   I was actually working for several years on robotics and as of recently, we changed the
[00:19:28.900 --> 00:19:33.700]   focus at OpenAI and I'm actually, I disbanded the robotics team.
[00:19:33.700 --> 00:19:35.380]   Oh, wow.
[00:19:35.380 --> 00:19:37.260]   Why did you do that?
[00:19:37.260 --> 00:19:44.460]   So the reasoning is that there is like a few pieces.
[00:19:44.460 --> 00:19:53.460]   So it turns out that we can make a gigantic progress whenever we have access to data and
[00:19:53.460 --> 00:19:58.700]   all our machinery and supervise there in reinforcement learning, they work extremely well.
[00:19:58.700 --> 00:20:04.020]   And there is actually plenty of domains that are very, very rich with data.
[00:20:04.020 --> 00:20:09.180]   And ultimately that was holding us back in case of robotics.
[00:20:09.180 --> 00:20:12.580]   And this decision was quite hard for me.
[00:20:12.580 --> 00:20:19.180]   I got the realization some time ago that actually that's the best from perspective of the company.
[00:20:19.180 --> 00:20:27.140]   And the sad thing is I think if it would be a robotics company or if the mission of the
[00:20:27.140 --> 00:20:30.900]   company would be different, then I think we would just continue.
[00:20:30.900 --> 00:20:37.660]   I actually quite strongly believe in the approach that robotics took and the direction.
[00:20:37.660 --> 00:20:43.100]   But from perspective of what we want to achieve, which is to build AGI, I think there was actually
[00:20:43.100 --> 00:20:44.300]   some components missing.
[00:20:44.300 --> 00:20:50.420]   So when we created robotics, we thought that we can go very far and we've self-generated
[00:20:50.420 --> 00:20:52.780]   data and the reinforcement learning.
[00:20:52.780 --> 00:21:02.820]   At the moment, I believe that actually pre-training allows to give model a hundred X cheaper IQ
[00:21:02.820 --> 00:21:03.820]   points.
[00:21:03.820 --> 00:21:07.980]   And then that might be followed with other techniques.
[00:21:07.980 --> 00:21:08.980]   And what is pre-training?
[00:21:08.980 --> 00:21:14.060]   Pre-training, that's I can explain it in case of GPT-3.
[00:21:14.060 --> 00:21:21.060]   So pre-training in case of GPT-3 or in case of like a language models means training them
[00:21:21.060 --> 00:21:25.660]   on some unsupervised task such as next word prediction.
[00:21:25.660 --> 00:21:31.620]   And that builds in all the internal representation that allows model to off the bat to solve
[00:21:31.620 --> 00:21:33.180]   many tasks.
[00:21:33.180 --> 00:21:37.420]   And in case of robotics, we haven't had such a data.
[00:21:37.420 --> 00:21:39.740]   I see.
[00:21:39.740 --> 00:21:42.820]   So do you regret working on robotics?
[00:21:42.820 --> 00:21:43.820]   No.
[00:21:43.820 --> 00:21:49.180]   I think that actually we've got plenty of insights for other projects.
[00:21:49.180 --> 00:21:53.100]   I think that also we build really amazing technology.
[00:21:53.100 --> 00:21:56.140]   I would say I'm actually very proud.
[00:21:56.140 --> 00:22:01.180]   There was like, of course, moments of sadness when I was making this decision, but I'm quite
[00:22:01.180 --> 00:22:04.020]   happy where we've got.
[00:22:04.020 --> 00:22:10.620]   Also I would say even from my own perspective, in the meanwhile, I manage also other things
[00:22:10.620 --> 00:22:14.020]   that made some significant progress in the meanwhile.
[00:22:14.020 --> 00:22:20.460]   And that there will be more information about it sometime.
[00:22:20.460 --> 00:22:21.460]   Cool.
[00:22:21.460 --> 00:22:27.340]   I guess one thing that I always kind of observe is when you look at what computers do versus
[00:22:27.340 --> 00:22:30.540]   what seems easy, robotics seems the most striking.
[00:22:30.540 --> 00:22:34.900]   I feel like the simplest things of picking up an arbitrary object seems like the most
[00:22:34.900 --> 00:22:41.100]   natural thing for my brain and seems so hard, maybe harder than anything else that feels
[00:22:41.100 --> 00:22:45.180]   natural to make a robot do it.
[00:22:45.180 --> 00:22:46.180]   What do you think about that?
[00:22:46.180 --> 00:22:49.820]   Do you think that there's more progress in the short term or will it be sort of the last
[00:22:49.820 --> 00:22:52.900]   thing that we solve on the path to AGI?
[00:22:52.900 --> 00:22:58.380]   So there are two possibilities for me, a few possibilities.
[00:22:58.380 --> 00:23:06.060]   So one is if someone would be able to actually in a natural way to collect a lot of data,
[00:23:06.060 --> 00:23:09.420]   I think that might be the capabilities.
[00:23:09.420 --> 00:23:15.500]   Another possibility is that we just need very powerful video models the same way as the
[00:23:15.500 --> 00:23:21.100]   moment we have very powerful text models, we need very powerful video models to take
[00:23:21.100 --> 00:23:22.100]   it off the ground.
[00:23:22.100 --> 00:23:28.700]   The trickiness at the moment with video models is that they just require way more compute
[00:23:28.700 --> 00:23:29.700]   than text models.
[00:23:29.700 --> 00:23:38.420]   So in case of text, already an individual word conveys a lot of information and just
[00:23:38.420 --> 00:23:41.180]   takes a few bits to represent it.
[00:23:41.180 --> 00:23:49.180]   In case of video, if we would like to process images of a size few hundred by few hundred,
[00:23:49.180 --> 00:23:54.340]   several frames at a time, that requires orders of magnitude more compute.
[00:23:54.340 --> 00:24:00.740]   So I believe that if we would have models that have a pretty powerful understanding
[00:24:00.740 --> 00:24:07.580]   of video, it would be way easier to train them toward manipulation.
[00:24:07.580 --> 00:24:10.980]   There is also one more technical issue here.
[00:24:10.980 --> 00:24:15.860]   These models most likely they would have to be very huge and then the difficulties in
[00:24:15.860 --> 00:24:17.660]   running them real time.
[00:24:17.660 --> 00:24:25.620]   So at the moment I see few issues with robotics simultaneously and it is ideal to be able
[00:24:25.620 --> 00:24:31.700]   to go after domains when the number of issues is, let's say one or two, so I think that
[00:24:31.700 --> 00:24:33.500]   it's very favorable.
[00:24:33.500 --> 00:24:43.740]   It's also when we started, okay, some sense, we started all sorts of projects at the beginning
[00:24:43.740 --> 00:24:49.700]   of OpenAI and we haven't had the clarity how and exactly what we want to build.
[00:24:49.700 --> 00:24:55.540]   And over the time we got way more clarity and the moment we can increase the focus in
[00:24:55.540 --> 00:24:58.300]   different direction.
[00:24:58.300 --> 00:25:04.140]   So that's another question that I've always kind of had is how does OpenAI think about
[00:25:04.140 --> 00:25:05.500]   the projects you pick?
[00:25:05.500 --> 00:25:11.180]   I feel like maybe critics would say that OpenAI has sort of been too good at picking projects
[00:25:11.180 --> 00:25:13.540]   that are very evocative.
[00:25:13.540 --> 00:25:19.980]   You guys put out these GPT-3 and the music stuff that you did, at least to me, it just
[00:25:19.980 --> 00:25:20.980]   seems so cool.
[00:25:20.980 --> 00:25:27.260]   But I think maybe some people feel frustrated that it feels almost targeted towards a media
[00:25:27.260 --> 00:25:29.460]   event or something.
[00:25:29.460 --> 00:25:32.220]   Is that something that you think about at OpenAI?
[00:25:32.220 --> 00:25:34.840]   How does OpenAI pick what to work on next?
[00:25:34.840 --> 00:25:42.260]   We have some internal beliefs, what has to be built for general purpose intelligence.
[00:25:42.260 --> 00:25:45.060]   And people mostly choose projects around.
[00:25:45.060 --> 00:25:52.460]   There is also, let's say, there is some level of freedom to go after crazy high payoff ideas.
[00:25:52.460 --> 00:25:58.860]   I don't think ever that people are like saying, "Let's go after this one because it's a high
[00:25:58.860 --> 00:25:59.860]   PR payoff."
[00:25:59.860 --> 00:26:09.340]   It's more that we have amazing people in, say, conveying our work to public.
[00:26:09.340 --> 00:26:20.460]   Maybe if we would release GPT-3 or Jukebox as .txt file, then people wouldn't say that
[00:26:20.460 --> 00:26:22.580]   they wouldn't say such a thing.
[00:26:22.580 --> 00:26:23.980]   Just did a bad job at the PR.
[00:26:23.980 --> 00:26:28.100]   That people would give you more benefit of the doubt.
[00:26:28.100 --> 00:26:29.100]   But I don't know.
[00:26:29.100 --> 00:26:34.780]   I feel like you chose to win Dota, which weren't other people thinking about this?
[00:26:34.780 --> 00:26:40.300]   It seemed like it was a very clear milestone, I guess, as opposed to putting out a paper
[00:26:40.300 --> 00:26:44.340]   on reinforcement learning at massive scale or something like that.
[00:26:44.340 --> 00:26:52.860]   There is also actually even element of even internal motivation with these significant
[00:26:52.860 --> 00:26:53.860]   goals.
[00:26:53.860 --> 00:27:03.340]   I actually think that Elon suggested us to go after Dota and motivation was, let's pick
[00:27:03.340 --> 00:27:11.900]   very complicated game such that if we would make a progress, it would be undeniable.
[00:27:11.900 --> 00:27:14.140]   There is a lot of toy tasks out there.
[00:27:14.140 --> 00:27:23.340]   For instance, people work on humanoid walking in Mojoko and this one is clearly, I'd say,
[00:27:23.340 --> 00:27:28.740]   disconnected from reality because people can make it walk in a simulation for multiple
[00:27:28.740 --> 00:27:33.500]   years already, but none of it works in reality.
[00:27:33.500 --> 00:27:40.780]   Here in case of Dota, we wanted to ensure that actually what we are after is meaningful.
[00:27:40.780 --> 00:27:42.900]   So how to ensure that it's meaningful?
[00:27:42.900 --> 00:27:49.340]   Some people are really devoting their life to actually play Dota, who are strategizing
[00:27:49.340 --> 00:27:54.480]   about it to play against us.
[00:27:54.480 --> 00:28:01.980]   How much of the work then on Dota was you felt like fundamentally moving ML forward
[00:28:01.980 --> 00:28:06.620]   and how much of it was Dota specific or can you even pull those apart?
[00:28:06.620 --> 00:28:16.300]   I think there was a decent amount of Dota specific work and maybe it was more than optimal,
[00:28:16.300 --> 00:28:19.220]   but also simultaneously.
[00:28:19.220 --> 00:28:27.300]   So I remember at the beginning of Dota project, it was actually unclear how to approach it.
[00:28:27.300 --> 00:28:33.740]   People are saying that contemporary reinforcement learning will have no chance in solving this
[00:28:33.740 --> 00:28:41.020]   problem and people looked into off-policy methods, on-policy methods, evolutionary strategies.
[00:28:41.020 --> 00:28:50.700]   And the thing that became quite surprising is that methods that already exist with appropriate
[00:28:50.700 --> 00:28:53.100]   scale work extremely well.
[00:28:53.100 --> 00:28:55.620]   So that was a big surprise.
[00:28:55.620 --> 00:29:04.820]   And I remember some people even before Dota time at OpenAI saying that maybe reinforcement
[00:29:04.820 --> 00:29:07.340]   learning is at that end.
[00:29:07.340 --> 00:29:10.700]   And all of a sudden it's a very different story now.
[00:29:10.700 --> 00:29:11.700]   For sure.
[00:29:11.700 --> 00:29:15.780]   At OpenAI, do you feel like you're competing with someone?
[00:29:15.780 --> 00:29:20.420]   The way how I would like the competition to be fully perceived is actually a competition
[00:29:20.420 --> 00:29:22.580]   with bad outcome.
[00:29:22.580 --> 00:29:23.580]   With what?
[00:29:23.580 --> 00:29:24.580]   Bad outcome.
[00:29:24.580 --> 00:29:25.580]   Bad outcome.
[00:29:25.580 --> 00:29:26.940]   Oh, I see.
[00:29:26.940 --> 00:29:29.020]   Competing with the bad outcome.
[00:29:29.020 --> 00:29:34.980]   I wouldn't like us to necessarily compete against, let's say, other technical labs and
[00:29:34.980 --> 00:29:36.060]   so on.
[00:29:36.060 --> 00:29:41.780]   But obviously there is some fear of being scooped or so.
[00:29:41.780 --> 00:29:49.140]   It's interesting that in case of large projects, I have seen it way less than in case of work
[00:29:49.140 --> 00:29:51.300]   of individuals on the paper.
[00:29:51.300 --> 00:29:57.340]   So my understanding is that it's very easy to be scooped when you are working alone.
[00:29:57.340 --> 00:30:02.980]   And it's almost impossible to get scooped if you work with, let's say, seven people.
[00:30:02.980 --> 00:30:04.700]   Why is that?
[00:30:04.700 --> 00:30:11.340]   So I think it might have to do with that there is many people working individually, but very
[00:30:11.340 --> 00:30:14.620]   few working as a group.
[00:30:14.620 --> 00:30:19.140]   It does seem like OpenAI is kind of maybe uniquely good at that.
[00:30:19.140 --> 00:30:23.100]   It seems like compared to academia, you have much more authors on your...
[00:30:23.100 --> 00:30:27.300]   Or compared to ML research, typically, you seem to do bigger projects and have more authors
[00:30:27.300 --> 00:30:29.220]   on your papers.
[00:30:29.220 --> 00:30:32.540]   I think that in reality, we need both.
[00:30:32.540 --> 00:30:40.900]   Sometimes we need these insights from secluded individuals who work in their Kermit house
[00:30:40.900 --> 00:30:47.900]   for several months to figure out that there is actually a different way to build transformer
[00:30:47.900 --> 00:30:51.020]   or to train models or so.
[00:30:51.020 --> 00:30:55.500]   And it's almost impossible to work on such a staff as a larger group.
[00:30:55.500 --> 00:30:58.540]   And then eventually we want to build systems.
[00:30:58.540 --> 00:31:06.700]   And the systems allow us to all the same take our work to next level, next level, next level.
[00:31:06.700 --> 00:31:13.620]   I guess what role do you feel like OpenAI plays that maybe the corporate, like DeepMind
[00:31:13.620 --> 00:31:17.380]   isn't doing or Berkeley isn't doing?
[00:31:17.380 --> 00:31:24.940]   I actually think that OpenAI has a fair amount of push on safety, that it became a mainstream
[00:31:24.940 --> 00:31:25.940]   topic.
[00:31:25.940 --> 00:31:28.240]   It wasn't a mainstream topic.
[00:31:28.240 --> 00:31:31.180]   So I think that's extremely important.
[00:31:31.180 --> 00:31:35.480]   Yeah, I actually think that's one of the most important things.
[00:31:35.480 --> 00:31:38.460]   Do you feel like it's sufficiently a mainstream topic now?
[00:31:38.460 --> 00:31:42.140]   I mean, it's certainly much more mainstream than 2015.
[00:31:42.140 --> 00:31:48.940]   In some sense, we would like it to be sufficiently mainstream such that would avoid bad outcomes.
[00:31:48.940 --> 00:31:59.580]   But I also almost think that the small bad outcomes might be a good thing because then
[00:31:59.580 --> 00:32:06.060]   they will inform the public that actually these problems are real rather than imaginary.
[00:32:06.060 --> 00:32:14.180]   At the moment, in case of GPT-3, we see some kind of rudimentary aspects of safety.
[00:32:14.180 --> 00:32:16.900]   It's more like on the side of controllability.
[00:32:16.900 --> 00:32:23.260]   We have a model that can have a conversation with you, but it's unclear how to make sure
[00:32:23.260 --> 00:32:28.780]   that the model won't be offending you or won't go off the track or won't leak some like a
[00:32:28.780 --> 00:32:31.380]   secret information.
[00:32:31.380 --> 00:32:38.380]   And we almost think about it as the safety of airplane.
[00:32:38.380 --> 00:32:40.820]   There will be multiple layers into it.
[00:32:40.820 --> 00:32:46.380]   Like you could imagine that one layer might have to do with appropriate data filtering
[00:32:46.380 --> 00:32:52.020]   or maybe then another layer has to do with injecting human feedback and maybe some like
[00:32:52.020 --> 00:32:56.260]   a final, let's say, discrimination by the model at the very end.
[00:32:56.260 --> 00:33:01.620]   Yeah, so I would say I think that at OpenA there is a lot of discussion about this topic.
[00:33:01.620 --> 00:33:06.580]   And at the moment, some aspects of safety, they became even important from commercial
[00:33:06.580 --> 00:33:10.260]   perspective.
[00:33:10.260 --> 00:33:14.580]   And so it seems like you've made GPT-3 something of a commercial product.
[00:33:14.580 --> 00:33:15.580]   Is that right?
[00:33:15.580 --> 00:33:16.580]   Is that how you think about it?
[00:33:16.580 --> 00:33:17.580]   Yes.
[00:33:17.580 --> 00:33:26.740]   I think our thinking is that if we want actually to deploy AGI one day, then it actually might
[00:33:26.740 --> 00:33:33.380]   be very important to have a lower stake around before.
[00:33:33.380 --> 00:33:36.620]   And GPT is definitely lower stake around.
[00:33:36.620 --> 00:33:42.700]   We can see what are the ways how the systems might be failing.
[00:33:42.700 --> 00:33:50.180]   Do you think there's any intuitions from neuroscience in general that can guide the development
[00:33:50.180 --> 00:33:53.020]   of machine learning models?
[00:33:53.020 --> 00:33:59.380]   There's obviously a question, is consciousness independent of intelligence or how they are
[00:33:59.380 --> 00:34:00.380]   related?
[00:34:00.380 --> 00:34:03.500]   What would it make AI conscious?
[00:34:03.500 --> 00:34:07.140]   And I guess there are a few proposals.
[00:34:07.140 --> 00:34:13.220]   It might be the case that all what is needed to be conscious is to build a model of reality
[00:34:13.220 --> 00:34:14.660]   around.
[00:34:14.660 --> 00:34:19.620]   And at the moment, our models, they implicitly build such a model.
[00:34:19.620 --> 00:34:25.140]   And that would be a claim in direction that actually our models are conscious.
[00:34:25.140 --> 00:34:28.100]   There is maybe that's maybe one axis.
[00:34:28.100 --> 00:34:34.540]   Other axis is other idea behind what consciousness could be.
[00:34:34.540 --> 00:34:42.980]   You can look in mathematics and computer science for some very special mathematical objects.
[00:34:42.980 --> 00:34:50.860]   And you can notice that in mathematics, there is a lot of weird things pop up once you allow
[00:34:50.860 --> 00:34:55.300]   mathematical system to be powerful enough to point on itself.
[00:34:55.300 --> 00:35:00.420]   In computer science, there is similar phenomenon with halting problem.
[00:35:00.420 --> 00:35:06.180]   Once the system points on itself, there is like an indecisiveness.
[00:35:06.180 --> 00:35:12.920]   And I can say that maybe intelligence fundamentally has to do with compression.
[00:35:12.920 --> 00:35:14.580]   And compression and prediction are the same thing.
[00:35:14.580 --> 00:35:18.940]   So for instance, next frame prediction, it's actually compression.
[00:35:18.940 --> 00:35:26.380]   And once the system would become powerful enough that it tries to compress itself, that
[00:35:26.380 --> 00:35:34.180]   might be in some way analogous to halting problem or to get LDRM in mathematics.
[00:35:34.180 --> 00:35:42.660]   And also some people claim that consciousness is not the property of information, but rather
[00:35:42.660 --> 00:35:48.980]   it's a physical property, most likely of electromagnetic field.
[00:35:48.980 --> 00:35:54.060]   And then that would actually mean that our AI wouldn't be conscious.
[00:35:54.060 --> 00:36:00.180]   It could have the same behavior as we do, but it wouldn't be conscious.
[00:36:00.180 --> 00:36:05.140]   So I frankly don't know which of this is true.
[00:36:05.140 --> 00:36:07.940]   And that's something that I actually keep on thinking about.
[00:36:07.940 --> 00:36:15.020]   I would say a fair amount because some sense consciousness is almost, or subjective experience
[00:36:15.020 --> 00:36:19.300]   is almost the only thing that I can be certain about.
[00:36:19.300 --> 00:36:23.060]   When I wake up, that's something that I experience.
[00:36:23.060 --> 00:36:28.260]   I cannot be that certain about mathematical equation or that tomorrow there will be a
[00:36:28.260 --> 00:36:34.340]   new day, but I'm certain that I'm having conscious experience at the moment.
[00:36:34.340 --> 00:36:37.740]   So it is like a incredible mystery.
[00:36:37.740 --> 00:36:42.420]   And I think it should be solvable by science.
[00:36:42.420 --> 00:36:49.620]   And AI allows to, or in case of artificial intelligence systems, we can control every
[00:36:49.620 --> 00:36:52.020]   aspect of the computation.
[00:36:52.020 --> 00:36:57.820]   I guess one difference with consciousness and the halting problem maybe, there's not
[00:36:57.820 --> 00:37:04.780]   a binary consciousness on versus off, but it seems to me like there's different levels
[00:37:04.780 --> 00:37:05.780]   of this.
[00:37:05.780 --> 00:37:09.940]   And I think we intuit that in the sense that we want to be kind towards other humans and
[00:37:09.940 --> 00:37:16.700]   we want to be somewhat kind to say a cat, but we don't put it on the same level.
[00:37:16.700 --> 00:37:23.580]   Do you feel like the models you're building might be sort of approaching the consciousness
[00:37:23.580 --> 00:37:25.300]   of a worm or something?
[00:37:25.300 --> 00:37:29.740]   I mean, certainly they can do things that animals can't do.
[00:37:29.740 --> 00:37:34.260]   So yeah, I frankly, I don't know.
[00:37:34.260 --> 00:37:42.500]   There is a Slack channel at OpenAI about welfare for artificial intelligence because it is
[00:37:42.500 --> 00:37:50.940]   conceivable that through some kinds of trainings, we could generate immense amount of suffering,
[00:37:50.940 --> 00:37:54.620]   like massive genocides.
[00:37:54.620 --> 00:37:56.740]   But frankly, we don't understand it.
[00:37:56.740 --> 00:38:05.340]   We don't know if let's say giving negative reward to model is the same as stabbing someone.
[00:38:05.340 --> 00:38:07.060]   Right.
[00:38:07.060 --> 00:38:12.500]   It seems that at first glance, it seems maybe ridiculous, but then it's kind of hard to
[00:38:12.500 --> 00:38:13.820]   pull it apart.
[00:38:13.820 --> 00:38:16.260]   It's hard to really articulate what the difference is.
[00:38:16.260 --> 00:38:17.260]   Yeah.
[00:38:17.260 --> 00:38:26.660]   I mean, the interesting thing is, so I can see now path from here to AGI.
[00:38:26.660 --> 00:38:32.700]   Of course, it might take a really long time and people are like, I think that there is
[00:38:32.700 --> 00:38:41.180]   a belief maybe that if model would be having human intelligence, then most likely it would
[00:38:41.180 --> 00:38:45.500]   be conscious as a human.
[00:38:45.500 --> 00:38:53.540]   The same time, at the moment, I can speak with GPT, I can ask GPT about consciousness
[00:38:53.540 --> 00:38:58.940]   and it would tell me, yeah, of course, it would explain its conscious state and so on.
[00:38:58.940 --> 00:39:05.300]   Of course, it has to do with GPT being trained and with data speaking about the consciousness.
[00:39:05.300 --> 00:39:12.900]   But the weird thing is, how would I be able to distinguish if indeed GPT would become
[00:39:12.900 --> 00:39:17.060]   conscious versus just knowing about it?
[00:39:17.060 --> 00:39:23.860]   I think there is maybe, there's like a few funny answers that come to my mind.
[00:39:23.860 --> 00:39:32.420]   So one is, we could try to remove all the data that mentions consciousness, train model
[00:39:32.420 --> 00:39:37.220]   on it, and then have a conversation about consciousness.
[00:39:37.220 --> 00:39:41.980]   And if model would say, "Oh, that's something I was thinking about it.
[00:39:41.980 --> 00:39:46.820]   And I noticed this thing and that's so surprising that it's there."
[00:39:46.820 --> 00:39:48.620]   That would be maybe one way.
[00:39:48.620 --> 00:39:54.940]   Another way that comes to my mind has to do with even how to check that some other human
[00:39:54.940 --> 00:39:56.580]   is conscious.
[00:39:56.580 --> 00:40:04.980]   So one idea of verifying that some other human is conscious is literally by connecting brains.
[00:40:04.980 --> 00:40:11.380]   If you can connect brains and then look and feel that their consciousness expanded, then
[00:40:11.380 --> 00:40:13.580]   that might be an indication that someone else is conscious.
[00:40:13.580 --> 00:40:19.500]   There are of course, various counter examples into it, but you could imagine that similarly,
[00:40:19.500 --> 00:40:24.500]   if you would connect your brain to AI, and if you would experience that your consciousness
[00:40:24.500 --> 00:40:26.620]   expanded, that might be evidence.
[00:40:26.620 --> 00:40:32.940]   Well, that might be a nice note to end on, but I do want to pull this back into a little
[00:40:32.940 --> 00:40:39.260]   bit more practical realm for two final questions that we always ask people.
[00:40:39.260 --> 00:40:43.960]   The second to last question we always ask is, what's a topic in machine learning right
[00:40:43.960 --> 00:40:49.980]   now that you think is underrated or doesn't have enough people paying attention to it?
[00:40:49.980 --> 00:40:54.820]   Maybe something that you would study if you were totally free to start anew on some other
[00:40:54.820 --> 00:40:57.540]   topic.
[00:40:57.540 --> 00:41:04.700]   I actually think that the models that can decide on its own compute budget, that they
[00:41:04.700 --> 00:41:12.140]   can keep on spinning inside a Turing complete models, like a universal Turing machines or
[00:41:12.140 --> 00:41:18.100]   universal transformers, or you can think about something like having inner monologue as a
[00:41:18.100 --> 00:41:26.020]   means of just increasing amount of compute that it's the model somehow while solving
[00:41:26.020 --> 00:41:29.740]   problem it speaks inside of its head.
[00:41:29.740 --> 00:41:31.780]   I think that's what I would work on.
[00:41:31.780 --> 00:41:32.780]   Cool.
[00:41:33.340 --> 00:41:34.340]   All right.
[00:41:34.340 --> 00:41:37.660]   The last question that we always ask, and this is for our audience, which I think is
[00:41:37.660 --> 00:41:42.260]   kind of a little more practically minded day to day than the conversation we got into,
[00:41:42.260 --> 00:41:47.740]   but what's the thing that you think is the hardest part today of going from a conceived
[00:41:47.740 --> 00:41:51.100]   model to a deployed model?
[00:41:51.100 --> 00:41:53.900]   Maybe specifically for you, I'm curious in robotics.
[00:41:53.900 --> 00:41:58.980]   If you were building a robotics company or OpenAI was geared towards just making a successful
[00:41:58.980 --> 00:42:04.060]   robotics application, which would be amazing, what do you think are the challenges that
[00:42:04.060 --> 00:42:08.020]   you need to solve today to make that work?
[00:42:08.020 --> 00:42:12.180]   I think that there are actually two stages.
[00:42:12.180 --> 00:42:20.660]   First stage is creating a model that is good enough for any deployment.
[00:42:20.660 --> 00:42:27.420]   Then second one is literally building minimum viable products such that there is a feedback
[00:42:27.420 --> 00:42:34.220]   and actually resources can be focused in the appropriate place.
[00:42:34.220 --> 00:42:35.220]   And what might that look like?
[00:42:35.220 --> 00:42:37.780]   So you need something that's sort of useful enough that you could make a lot of it and
[00:42:37.780 --> 00:42:40.220]   deploy it so it's collecting data.
[00:42:40.220 --> 00:42:41.700]   Am I understanding you right?
[00:42:41.700 --> 00:42:42.700]   Yeah.
[00:42:42.700 --> 00:42:48.500]   So I mean, you could imagine, for instance, for the robotics company, seems to me that
[00:42:48.500 --> 00:42:53.340]   the problem of pick and place is actually completely tractable.
[00:42:53.340 --> 00:42:59.460]   I would also say that I wouldn't shy away from collecting data.
[00:42:59.460 --> 00:43:04.580]   So I think that the path that I would take now, if I would be focused on solving the
[00:43:04.580 --> 00:43:14.300]   problem, I would at first try to find some viable domain where there's a big enough market,
[00:43:14.300 --> 00:43:18.820]   the movement doesn't look complicated enough.
[00:43:18.820 --> 00:43:25.260]   And then I would actually hire plenty of people to do their operation and I would collect
[00:43:25.260 --> 00:43:29.820]   million trajectories and then train a model on it.
[00:43:29.820 --> 00:43:34.940]   And I would say people are very excited about reinforcement learning and I think reinforcement
[00:43:34.940 --> 00:43:38.460]   learning is very, very powerful.
[00:43:38.460 --> 00:43:44.100]   While at the same time, I'll say they shy away almost from supervised learning.
[00:43:44.100 --> 00:43:50.020]   And in my belief, if I would have a company, I would double down on supervised learning.
[00:43:50.020 --> 00:43:54.660]   And it's just keep on surprising me how far it takes.
[00:43:54.660 --> 00:43:55.660]   All right.
[00:43:55.660 --> 00:43:57.340]   Well, thank you so much.
[00:43:57.340 --> 00:43:58.340]   That was a lot of fun.
[00:43:58.340 --> 00:43:59.340]   Thank you.
[00:43:59.340 --> 00:44:02.420]   I really appreciate you getting up so early.
[00:44:02.420 --> 00:44:03.420]   Thank you, Lukasz.
[00:44:03.420 --> 00:44:04.420]   Have a great day.
[00:44:04.420 --> 00:44:08.060]   Thanks for listening to another episode of Graded Descent.
[00:44:08.060 --> 00:44:12.340]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:44:12.340 --> 00:44:15.100]   hear from the people that are listening to the episodes.
[00:44:15.100 --> 00:44:19.180]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:44:19.180 --> 00:44:23.120]   a conversation, that would make me inspired to do more of these episodes.
[00:44:23.120 --> 00:44:26.660]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

