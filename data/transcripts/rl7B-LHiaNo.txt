
[00:00:00.000 --> 00:00:06.400]   general intelligence is not task-specific skills scaled up to many skills, because there is an
[00:00:06.400 --> 00:00:11.600]   infinite space of possible skills. General intelligence is the ability to approach any
[00:00:11.600 --> 00:00:17.680]   problem, any skill, and very quickly master it using valid data. Because this is what makes you
[00:00:17.680 --> 00:00:24.080]   able to face anything you might ever encounter. This is what makes, this is the definition of
[00:00:24.080 --> 00:00:31.200]   generality. Like generality is not specificity scaled up. It is the ability to apply your mind
[00:00:31.200 --> 00:00:36.160]   to anything at all, to arbitrary things. And this requires, fundamentally, this requires the ability
[00:00:36.160 --> 00:00:42.560]   to adapt, to learn on the fly efficiently. The scale maximalist argument, really, it boils down
[00:00:42.560 --> 00:00:48.480]   to these people, they refer to scaling laws, which is this empirical relationship that you can draw
[00:00:48.480 --> 00:00:52.560]   between how much compute you spend on training a model and the performance you're getting on
[00:00:52.560 --> 00:00:58.560]   benchmarks, right? And the key question here, of course, is, well, how do you measure performance?
[00:00:58.560 --> 00:01:04.080]   What it is that you're actually improving by adding more compute and more data? And well,
[00:01:04.080 --> 00:01:09.920]   it's benchmark performance, right? And the thing is, the way you measure performance is not a
[00:01:09.920 --> 00:01:17.040]   technical detail. It's not an afterthought, because it's going to narrow down the set of
[00:01:17.040 --> 00:01:22.160]   questions that you're asking. And so accordingly, it's going to narrow down the set of answers
[00:01:22.160 --> 00:01:28.080]   that you're looking for. If you look at the benchmarks we're using for LLMs, they're all
[00:01:28.080 --> 00:01:32.880]   memorization-based benchmarks. Like sometimes they're literally just knowledge-based, like a
[00:01:32.880 --> 00:01:39.280]   school test. And even if you look at the ones that are, you know, explicitly about reasoning,
[00:01:39.280 --> 00:01:46.880]   you realize, if you look closely, that in order to solve them, it's enough to memorize a finite set
[00:01:46.880 --> 00:01:53.920]   of reasoning patterns. And then you just reapply them. They're like static programs. LLMs are very
[00:01:53.920 --> 00:01:59.440]   good at memorizing static programs, small static programs. And they've got this sort of like bank
[00:01:59.440 --> 00:02:06.000]   of solution programs. And when you give them a new puzzle, they can just fetch the appropriate
[00:02:06.000 --> 00:02:11.360]   program, apply it, and it's looking like it's reasoning, but really it's not doing any sort of
[00:02:11.360 --> 00:02:16.720]   on-the-fly program synthesis. All it's doing is program fetching. So you can actually solve all
[00:02:16.720 --> 00:02:22.960]   these benchmarks with memorization. And so what you're scaling up here, like if you look at the
[00:02:22.960 --> 00:02:29.840]   models, they are big parametric curves fitted to a data distribution, which I can't understand.
[00:02:29.840 --> 00:02:36.160]   So they're basically these big interpolative databases, interpolative memories. And of course,
[00:02:36.160 --> 00:02:42.560]   if you scale up the size of your database and you cram into it more knowledge, more patterns,
[00:02:42.560 --> 00:02:49.200]   and so on, you are going to be increasing its performance as measured by a memorization
[00:02:49.200 --> 00:02:55.440]   benchmark. That's kind of obvious. But as you're doing this, you are not increasing the intelligence
[00:02:55.440 --> 00:03:00.160]   of the system one bit. You are increasing the skill of the system. You are increasing its
[00:03:00.160 --> 00:03:07.040]   usefulness, its scope of applicability, but not its intelligence because skill is not intelligence.
[00:03:07.040 --> 00:03:13.200]   And that's the fundamental confusion that people run into is that they're confusing
[00:03:13.200 --> 00:03:19.280]   skill and intelligence. As far as the interpolation goes, so okay, let's look at one of the benchmarks
[00:03:19.280 --> 00:03:26.800]   here. There's one benchmark that does great school math, and these are problems that like a smart
[00:03:26.800 --> 00:03:32.560]   high schooler would be able to solve. It's called GSM 8K, and these models get 95% on these. Like
[00:03:32.560 --> 00:03:35.680]   basically, they always nail it. That's the memorization benchmark. Okay, let's talk about
[00:03:35.680 --> 00:03:40.720]   what that means. So here's one question from that benchmark. So 30 students are in a class,
[00:03:40.720 --> 00:03:46.880]   1/5 of them are 12-year-olds, 1/3 are 13-year-old, 1/10 are 11-year-olds. How many of them are not
[00:03:46.880 --> 00:03:52.320]   11, 12, or 13 years old? So I agree, this is not rocket science, right? You can write down on paper
[00:03:52.320 --> 00:03:56.320]   how you go through this problem, and a high school kid, at least a smart high school kid, should be
[00:03:56.320 --> 00:04:02.800]   able to solve it. Now, when you say memorization, it still has to reason through how to think about
[00:04:02.800 --> 00:04:07.280]   fractions, and what is the context of the whole problem, and then combining the different
[00:04:07.280 --> 00:04:12.080]   calculations it's doing. It depends how you want to define reasoning, but there are two definitions
[00:04:12.080 --> 00:04:20.080]   you can use. So one is, I have available a set of program templates. It's like the structure
[00:04:20.080 --> 00:04:25.360]   of the puzzle, which can also generate its solution. And I'm just going to identify the
[00:04:25.360 --> 00:04:30.720]   right template, which is in my memory. I'm going to input the new values into the template, run
[00:04:30.720 --> 00:04:35.440]   the program, get the solution. And you could say this is reasoning. And I say, yeah, sure, okay.
[00:04:35.440 --> 00:04:41.920]   But another definition you can use is reasoning is the ability to, when you're faced with a puzzle,
[00:04:41.920 --> 00:04:48.320]   given that you don't have already a program in memory to solve it, you must synthesize on the
[00:04:48.320 --> 00:04:53.920]   fly a new program based on bits of pieces of existing programs that you have. You have to
[00:04:53.920 --> 00:04:59.600]   do on the fly program synthesis. And it's actually dramatically harder than just fetching the right
[00:04:59.600 --> 00:05:05.600]   memorized program and replying it. I think maybe we are overestimating the extent to which humans
[00:05:05.600 --> 00:05:11.520]   are so sample efficient, they also don't need training in this way where they have to drill
[00:05:11.520 --> 00:05:17.200]   in these kinds of pathways of reasoning through certain kinds of problems. So let's take math,
[00:05:17.200 --> 00:05:22.960]   for example. It's not like you can just show a baby the axioms of set theory, and now they know
[00:05:22.960 --> 00:05:27.840]   math, right? So when they're growing up, you had to do years of teaching them pre-algebra, then you
[00:05:27.840 --> 00:05:31.680]   got to do a year of teaching them doing drills and going through the same kind of problem in algebra,
[00:05:31.680 --> 00:05:36.000]   then geometry, pre-calculus, calculus. Absolutely. So training...
[00:05:36.000 --> 00:05:40.000]   Yeah, but isn't that like the same kind of thing where you can't just see one example and now you
[00:05:40.000 --> 00:05:43.200]   have the program or whatever. You actually had to drill it. These models also had to drill
[00:05:43.200 --> 00:05:46.000]   with a bunch of returning data. Sure. I mean, in order to do
[00:05:46.000 --> 00:05:52.400]   on the fly program synthesis, you actually need building blocks to work from. So knowledge and
[00:05:52.400 --> 00:05:59.520]   memory are actually tremendously important in the process. I'm not saying it's memory versus
[00:05:59.520 --> 00:06:03.440]   reasoning. In order to do effective reasoning, you need memory.
[00:06:03.440 --> 00:06:03.940]   Yeah.
[00:06:03.940 --> 00:06:13.940]   [BLANK_AUDIO]

