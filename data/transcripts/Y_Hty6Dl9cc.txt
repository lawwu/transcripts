
[00:00:00.000 --> 00:00:05.500]   Awesome. We're live. And I hope you don't face the issue this
[00:00:05.500 --> 00:00:07.020]   time. I can see your screen. That's awesome.
[00:00:07.020 --> 00:00:11.100]   Oh my gosh. Yeah. I think we've already shaved like 10 minutes
[00:00:11.100 --> 00:00:12.360]   of troubleshooting.
[00:00:12.360 --> 00:00:18.020]   There was one time when my screen literally went off. And I
[00:00:18.020 --> 00:00:18.300]   was like,
[00:00:18.300 --> 00:00:20.740]   with the screen off.
[00:00:20.740 --> 00:00:23.640]   It keeps you humble. You know what I mean? It keeps you
[00:00:23.640 --> 00:00:26.100]   humble. Then you go like, you know what, I still couldn't be
[00:00:26.100 --> 00:00:26.620]   better.
[00:00:27.540 --> 00:00:32.260]   So this PC is on power backup priorities, right? But the
[00:00:32.260 --> 00:00:35.020]   monitor isn't. So like, all priorities there.
[00:00:35.020 --> 00:00:37.340]   Offline again.
[00:00:37.340 --> 00:00:39.100]   Yeah.
[00:00:39.100 --> 00:00:41.900]   Let me make sure we live.
[00:00:41.900 --> 00:00:52.100]   Interesting. Now I can see myself. Awesome. Sorry, everyone,
[00:00:52.100 --> 00:00:54.420]   we were just catching up with an app behind the scenes. If you
[00:00:54.420 --> 00:00:59.540]   got some of the chat, Wade will be teaching us about mast and
[00:00:59.540 --> 00:01:02.460]   casual language model. Both of the things I clearly don't
[00:01:02.460 --> 00:01:05.180]   understand as you could see from my expression, and I'm hoping
[00:01:05.180 --> 00:01:08.100]   Wade will make it clearer for me and all of us. So with that,
[00:01:08.100 --> 00:01:11.660]   over to Wade and a reminder, this will be the last session in
[00:01:11.660 --> 00:01:15.420]   the series. So if you're still with us, thanks for joining all
[00:01:15.420 --> 00:01:17.740]   of the sessions. I know Wade is an awesome teacher. So that's
[00:01:17.740 --> 00:01:19.780]   why you keep coming back. Hopefully you'll also join some
[00:01:19.780 --> 00:01:24.100]   of my sessions. And I'll try to live up to Wade's legacy.
[00:01:24.580 --> 00:01:26.900]   I don't set the bar too high. So I don't think you're gonna have
[00:01:26.900 --> 00:01:28.380]   any problem. It's quite high for me.
[00:01:28.380 --> 00:01:34.860]   Thank you. All right. Well, anyways, yeah. Welcome back.
[00:01:34.860 --> 00:01:41.100]   This is the last session in our part two study group. And as
[00:01:41.100 --> 00:01:45.900]   Sanyam mentioned, we're going to be looking at language modeling.
[00:01:45.900 --> 00:01:51.660]   And so looking at both causal and mass language models. So
[00:01:52.460 --> 00:01:55.300]   again, this is all in the slides. These are some helpful
[00:01:55.300 --> 00:01:59.340]   resources. If you're all looking to get involved with fast AI,
[00:01:59.340 --> 00:02:04.500]   data science, or working with Hugging Face on its own or with
[00:02:04.500 --> 00:02:08.700]   fast AI, we have again, a few libraries that you can check out
[00:02:08.700 --> 00:02:12.740]   on the author and creator of the blur library, which we've been
[00:02:12.740 --> 00:02:17.940]   mostly using in this course, to look at how to work with the the
[00:02:17.940 --> 00:02:23.020]   mid and low level API that fast AI provides to be able to work
[00:02:23.020 --> 00:02:27.060]   with and train transformer models available from Hugging
[00:02:27.060 --> 00:02:31.860]   Face. So yeah, Sanyam said we're going to look at causal and mass
[00:02:31.860 --> 00:02:35.780]   language modeling, we're going to follow the the same steps
[00:02:35.780 --> 00:02:38.900]   that we've been following following for all the other main
[00:02:38.900 --> 00:02:43.700]   NLP tasks. And this will conclude the section seven, which
[00:02:43.700 --> 00:02:48.460]   really kind of does a deep dive on the end to end process of
[00:02:48.460 --> 00:02:53.500]   building transformers for the core NLP tasks, right. And again,
[00:02:53.500 --> 00:02:57.340]   we have this link here to the task resource and Hugging Face,
[00:02:57.340 --> 00:03:01.060]   which is really a fantastic summary of the core tasks, not
[00:03:01.060 --> 00:03:04.900]   just for NLP, you'll see the vision and speech, other things
[00:03:04.900 --> 00:03:07.540]   that you can do with transformers. And it gives you
[00:03:07.540 --> 00:03:11.540]   really good information about data sets to potentially
[00:03:11.540 --> 00:03:18.100]   experiment with what the task is from a high level, what metrics
[00:03:18.100 --> 00:03:22.580]   to use. And also, there's often links to papers that you may
[00:03:22.580 --> 00:03:27.380]   want to check out. So yeah, so we're going to go into language
[00:03:27.380 --> 00:03:31.300]   modeling. And it's kind of funny, and I mentioned last week
[00:03:31.300 --> 00:03:34.380]   is that we're kind of doing things backwards, when you think
[00:03:34.380 --> 00:03:38.620]   about it. All these core NLP tasks, these downstream tasks,
[00:03:38.620 --> 00:03:42.180]   like token classification, and question answering,
[00:03:42.180 --> 00:03:47.940]   summarization, all rely on a backbone, which is the language
[00:03:47.940 --> 00:03:53.020]   model. And so you may be curious as to why we're covering
[00:03:53.020 --> 00:03:59.380]   language modeling, so late that it literally the last thing in
[00:03:59.380 --> 00:04:04.300]   this particular study group. And the reason is that I think a lot
[00:04:04.300 --> 00:04:09.020]   of folks are prematurely go to I got to build a language model
[00:04:09.020 --> 00:04:11.740]   or fine tune a language model to be able to get really good
[00:04:11.740 --> 00:04:15.820]   results on my classification problem. And the reality is that
[00:04:15.820 --> 00:04:19.420]   that's actually probably a lot more rare. And a better
[00:04:19.420 --> 00:04:24.820]   approach is, in my opinion, to go to the Hugging Face Hub, look
[00:04:24.820 --> 00:04:29.140]   at models, look at their model cards, and start with something,
[00:04:29.140 --> 00:04:32.740]   and preferably a distilled version, something that's small,
[00:04:33.020 --> 00:04:38.860]   that is pretty close to your particular target corpus, rather
[00:04:38.860 --> 00:04:42.820]   than starting, I got to train everything from scratch. Because
[00:04:42.820 --> 00:04:45.700]   you can often find that you can fine tune a language model and
[00:04:45.700 --> 00:04:48.820]   think, okay, this is going to give me better results. But
[00:04:48.820 --> 00:04:53.380]   because the original pre trained model was trained on vast
[00:04:53.380 --> 00:04:57.420]   amounts of data with bigger batch sizes with, you know,
[00:04:57.420 --> 00:05:01.660]   better compute capabilities, that that won't always be the
[00:05:01.660 --> 00:05:07.020]   case. And in my experience, in particular, I've never had to
[00:05:07.020 --> 00:05:11.900]   fine tune or even train from scratch a language model for any
[00:05:11.900 --> 00:05:15.980]   of the work I've done. And I've done extensive work with
[00:05:15.980 --> 00:05:18.780]   classification tasks, token classification, and
[00:05:18.780 --> 00:05:23.340]   summarization. But you may need to and it's good to understand
[00:05:23.340 --> 00:05:26.300]   what a language model is, because literally everything
[00:05:26.300 --> 00:05:29.340]   that we're that we've looked at, with regards to the
[00:05:29.340 --> 00:05:35.140]   transformers model models, has a language model as the backbone.
[00:05:35.140 --> 00:05:41.020]   So what is a language model? This is from fastbook. A
[00:05:41.020 --> 00:05:43.860]   language model is a model that's been trained to guess the next
[00:05:43.860 --> 00:05:48.180]   word in a text. And in the case of a causal language model,
[00:05:48.180 --> 00:05:51.180]   having read the ones before, right, and you're predicting the
[00:05:51.180 --> 00:05:54.500]   next word for a mass language model, we're going to mass
[00:05:54.500 --> 00:06:00.940]   tokens or words or phrases, or corrupt the inputs somewhat.
[00:06:00.940 --> 00:06:04.100]   And then we're going to try to guess what the uncorrupted
[00:06:04.100 --> 00:06:07.780]   version should be. So for causal, this is you're trying to
[00:06:07.780 --> 00:06:11.740]   guess the next word. And the key is that to properly guess the
[00:06:11.740 --> 00:06:14.340]   next word in a sentence, the model will have to develop an
[00:06:14.340 --> 00:06:19.100]   understanding of the English or other language. And this is the
[00:06:19.100 --> 00:06:25.820]   intuition that if you look up and read the ULM fit paper that
[00:06:25.820 --> 00:06:32.580]   Jeremy published with Sebastian Ruder, years ago, and it's also
[00:06:32.580 --> 00:06:37.820]   discussed in the fastbook. The idea is, if you can build a
[00:06:37.820 --> 00:06:42.140]   model that can do this can predict the next word, then you
[00:06:42.140 --> 00:06:46.380]   have a model that has some understanding of the grammar,
[00:06:46.660 --> 00:06:49.580]   the style, the syntax of whatever language you're
[00:06:49.580 --> 00:06:53.580]   training it in. And such a model then can be applied to other
[00:06:53.580 --> 00:06:59.700]   tasks, such as classification, or name, it's he recognition or
[00:06:59.700 --> 00:07:04.100]   summarization because of that. And so that intuition was
[00:07:04.100 --> 00:07:07.500]   literally what drove them to build ULM fit, which by the way
[00:07:07.500 --> 00:07:13.700]   is a great paper that uses an LSTM. But the, but
[00:07:13.700 --> 00:07:21.180]   conceptually, the idea is the same. So when do you when you
[00:07:21.180 --> 00:07:24.780]   want to fine tune a language model? So again, fine tuning is
[00:07:24.780 --> 00:07:28.500]   different than training pre training, right? fine tuning is
[00:07:28.500 --> 00:07:32.780]   we're going to take an existing language model with a with a set
[00:07:32.780 --> 00:07:36.820]   of tokens that are already defined in the vocabulary that
[00:07:36.820 --> 00:07:42.300]   already have a representation in the model. And we are going to
[00:07:42.340 --> 00:07:46.660]   have it look at our corpus and improve the representation of
[00:07:46.660 --> 00:07:51.060]   our the words in our corpus. When is it helpful to do that?
[00:07:51.060 --> 00:07:56.380]   It, it helps to get the style of the corpus we are targeting, it
[00:07:56.380 --> 00:08:00.220]   may be more formal language or more technical, with new words
[00:08:00.220 --> 00:08:04.060]   to learn or different ways of composing sentences. So if you
[00:08:04.060 --> 00:08:07.500]   look at the Hugging Face course, and also if you read the
[00:08:07.500 --> 00:08:14.740]   fastbook, you'll see that IMDb is a case study in this. So IMDb
[00:08:14.740 --> 00:08:20.420]   is English, which means we can use most English or most models
[00:08:20.420 --> 00:08:24.780]   that are trained on an English corpus. But it has some unique
[00:08:24.780 --> 00:08:28.580]   things in there, right, that maybe wouldn't come out in
[00:08:28.580 --> 00:08:33.380]   Wikipedia per se, such as names of movie directors and actors.
[00:08:33.780 --> 00:08:38.220]   And also the style is different than Wikipedia, right? It's much
[00:08:38.220 --> 00:08:42.300]   less, you know, formal with people trashing or glowing about
[00:08:42.300 --> 00:08:47.660]   movies. So in that case, it may be helpful to fine tune such a
[00:08:47.660 --> 00:08:52.500]   model. And so the questions to ask, before you get to this
[00:08:52.500 --> 00:08:56.420]   point is, does your target corpus, does it contain a lot of
[00:08:56.420 --> 00:09:02.340]   domain specific words? And these are words that may appear like
[00:09:02.340 --> 00:09:08.580]   in Wikipedia. But because of the corpus itself, the idea, the
[00:09:08.580 --> 00:09:12.660]   meaning behind them may be a little bit different. And your
[00:09:12.660 --> 00:09:19.340]   target corpus may not have the fullest representation, because
[00:09:19.340 --> 00:09:22.260]   it has a more narrow meaning. And so in that case, you may
[00:09:22.260 --> 00:09:27.140]   want to fine tune a language model. The second one, does it
[00:09:27.140 --> 00:09:30.980]   contain a lot of words that may require, you know, a different
[00:09:30.980 --> 00:09:34.340]   numerical representation based on the target domain as compared
[00:09:34.340 --> 00:09:38.340]   to that of the corpus used for pre training? And third is, does
[00:09:38.340 --> 00:09:41.540]   it have a style that is very different from the corpus used
[00:09:41.540 --> 00:09:44.460]   for pre training? So we talked about formal versus informal,
[00:09:44.460 --> 00:09:50.940]   you could have potentially a target corpus that has a bunch
[00:09:50.940 --> 00:09:55.300]   of emojis or things like that, that you that these pre trained
[00:09:55.300 --> 00:09:58.340]   models may not have seen when they look at Wikipedia. So
[00:09:58.900 --> 00:10:01.700]   those are things to think about. And again, this is kind of the
[00:10:01.700 --> 00:10:05.300]   last resort, like don't start with, okay, I have these
[00:10:05.300 --> 00:10:08.220]   different words in here, I think the meaning may be different.
[00:10:08.220 --> 00:10:12.460]   Start with a pre trained model, see what the results are, get
[00:10:12.460 --> 00:10:16.780]   your training and evaluation loop working. And then if the
[00:10:16.780 --> 00:10:20.980]   results aren't good enough, or you got extra time, explore
[00:10:20.980 --> 00:10:29.380]   fine tuning. And then as a last ditch thing to do, if you're not
[00:10:29.380 --> 00:10:33.900]   getting the results you want, and you have a target corpus,
[00:10:33.900 --> 00:10:38.340]   that's very different. And so in the class, we saw where we were
[00:10:38.340 --> 00:10:43.580]   training language models from scratch, because we had a, we
[00:10:43.580 --> 00:10:47.980]   had models that were pre trained on something like Wikipedia and
[00:10:47.980 --> 00:10:51.100]   text, but we are trying to create a language model that
[00:10:51.100 --> 00:10:55.820]   understood Python code. And we saw that when the tokenizer ran,
[00:10:55.820 --> 00:10:59.940]   for example, it created like, a lot more tokens that were then
[00:10:59.940 --> 00:11:03.340]   were really needed. And it wasn't capturing the main ideas
[00:11:03.340 --> 00:11:07.100]   of what Python looked like in terms of indentation or
[00:11:07.100 --> 00:11:12.460]   comments, or like how we delineate classes or methods.
[00:11:12.460 --> 00:11:17.220]   And so in that sense, it definitely made sense to train a
[00:11:17.220 --> 00:11:20.180]   language model from from scratch. And so some of the
[00:11:20.180 --> 00:11:22.660]   examples that they talked about in the course, in addition to
[00:11:22.660 --> 00:11:26.700]   the programming language example I just mentioned, was like your
[00:11:26.700 --> 00:11:30.820]   target corpus consists consists of musical notes, or molecular
[00:11:30.820 --> 00:11:35.340]   sequences such as DNA. And so again, this is really if you
[00:11:35.340 --> 00:11:37.900]   can't fine tune and you really have something very different,
[00:11:37.900 --> 00:11:41.940]   then you want to explore actually building a language
[00:11:41.940 --> 00:11:45.420]   model from scratch. And these often take a lot of time, and
[00:11:45.420 --> 00:11:49.820]   are sensitive to things like batch size. So if you look at
[00:11:49.820 --> 00:11:53.700]   some of the papers, and you talk to folks that are training these
[00:11:53.700 --> 00:11:57.900]   language models that we're using in the course, you'll find that
[00:11:57.900 --> 00:12:01.900]   their batch sizes are like gigantic. It's taken, you know,
[00:12:01.900 --> 00:12:05.500]   days and weeks to train these type of models. So again, this
[00:12:05.500 --> 00:12:08.860]   is kind of like, if you got something very different, go
[00:12:08.860 --> 00:12:12.580]   ahead and look at a pre trained, pre trained language model from
[00:12:12.580 --> 00:12:18.100]   scratch, and try to get as much commute compute as you can to do
[00:12:18.100 --> 00:12:24.660]   that. Okay, so let's go ahead and kind of go through our steps
[00:12:24.660 --> 00:12:29.940]   for language modeling, we're going to look at code for causal
[00:12:29.940 --> 00:12:34.460]   language model task, but it's pretty easy to adapt that to
[00:12:34.460 --> 00:12:40.540]   mass language modeling. And one thing I wanted to make sure
[00:12:40.540 --> 00:12:43.460]   folks knew is that we have the data sets library, which
[00:12:43.460 --> 00:12:48.140]   provides tons of data sets for us to use. And it's awesome,
[00:12:48.140 --> 00:12:52.020]   because you can read about how it was developed, you can explore
[00:12:52.020 --> 00:12:56.740]   it on the Hugging Face website. But there's also a bunch of data
[00:12:56.740 --> 00:13:01.620]   sets available from fast AI. And so in the slides, I've linked to
[00:13:01.620 --> 00:13:05.700]   the data sets page. And what's nice about this is that they've
[00:13:05.700 --> 00:13:09.180]   they're already trimmed versions of data sets that you're already
[00:13:09.180 --> 00:13:13.380]   using. And so they download fast, and they're really helpful
[00:13:13.380 --> 00:13:17.380]   to use for experimentation. And when you're just starting the
[00:13:17.380 --> 00:13:20.620]   development of your, your process, and you're
[00:13:20.620 --> 00:13:24.500]   troubleshooting things, because the last thing you want to do is
[00:13:24.500 --> 00:13:27.380]   be training a language model and have that thing running for a
[00:13:27.380 --> 00:13:31.380]   couple days. And then just to find out you've pooch the
[00:13:31.380 --> 00:13:36.180]   evaluation loop, and you start getting errors. So a lot of
[00:13:36.180 --> 00:13:39.820]   really helpful for a variety of different tasks. And for
[00:13:39.820 --> 00:13:43.260]   language modeling, they actually include a subset of the wiki
[00:13:43.260 --> 00:13:53.140]   text 103, called wiki text, too. And going back to Colab here,
[00:13:53.140 --> 00:13:58.140]   what could we zoom in? Oh, yeah, maybe I need to like get my eyes
[00:13:58.140 --> 00:14:01.580]   tested again. But I'm always making this ask him. How's that
[00:14:01.580 --> 00:14:03.980]   look? Is it better? Or you want me to go? Yeah, I can read. No.
[00:14:04.260 --> 00:14:07.660]   Okay, I should also get my eyes tested. Sorry. I think it was my
[00:14:07.660 --> 00:14:10.180]   fault. I think maybe I was I was testing you. And so I had it
[00:14:10.180 --> 00:14:18.140]   down at like 75. But yeah, sorry about that. But yeah, so we're
[00:14:18.140 --> 00:14:21.980]   gonna do the pip install from the dev to branch, do all of our
[00:14:21.980 --> 00:14:27.500]   imports. And getting data from fast AI, if you've taken the
[00:14:27.500 --> 00:14:32.020]   course or gone through the fast book is really easy. So there's
[00:14:32.020 --> 00:14:38.340]   this URLs module and has references to a bunch of those
[00:14:38.340 --> 00:14:46.020]   data sets that are discussed and described on the website. And
[00:14:46.020 --> 00:14:49.740]   simply this is going to download it using untar data, if it
[00:14:49.740 --> 00:14:55.780]   hasn't been downloaded already, it's going to uncompress it and
[00:14:55.780 --> 00:15:00.380]   then give you the path where everything's at. And so this
[00:15:00.700 --> 00:15:07.700]   particular data set gets you a train and test CSV file. We can
[00:15:07.700 --> 00:15:13.460]   also work build a data frame from that we can combine that.
[00:15:13.460 --> 00:15:16.900]   In this particular case, I'm combining it, but you'll see
[00:15:16.900 --> 00:15:20.100]   that when you pre process, we're going to do these a little bit
[00:15:20.100 --> 00:15:24.580]   separately. And you can see like, yeah, it just is a bunch
[00:15:24.580 --> 00:15:28.300]   of text. And so the big thing with doing a language model is
[00:15:28.340 --> 00:15:34.660]   you want a lot of text. And whether you're doing Wikipedia
[00:15:34.660 --> 00:15:39.780]   or like IMDB or product reviews, or whatever, it's pretty easy to
[00:15:39.780 --> 00:15:45.020]   find things like that. So yeah, so basically, you want a lot of
[00:15:45.020 --> 00:15:52.780]   text. And once you have that you are ready to go. So we now have
[00:15:52.780 --> 00:15:56.700]   our kind of raw data set, we go through the process of getting
[00:15:56.700 --> 00:15:59.940]   our hugging face objects. And with blur, we can just use the
[00:15:59.940 --> 00:16:05.620]   get HF objects to get everything in one line. And one of the
[00:16:05.620 --> 00:16:09.020]   things that we've been asking as we've been in the series is,
[00:16:09.020 --> 00:16:13.020]   okay, well, what kind of models should we be looking at for
[00:16:13.020 --> 00:16:20.180]   language modeling tasks? And the answer is both encoder only and
[00:16:20.180 --> 00:16:26.540]   decoder only. And so if we're doing a causal LM, that would be
[00:16:26.540 --> 00:16:31.780]   something like GPT to typically, you'll find that decoder only
[00:16:31.780 --> 00:16:36.420]   models are going to be your go to architecture. And in
[00:16:36.420 --> 00:16:40.260]   particular, as mentioned early on in part one of the course,
[00:16:40.260 --> 00:16:44.380]   they're good for generative task. So remember, we looked at
[00:16:44.380 --> 00:16:48.340]   the idea of a language model is or at least that that is talked
[00:16:48.340 --> 00:16:51.460]   about in the fast book, the causal language model is that
[00:16:51.580 --> 00:16:57.860]   given a bunch of words prior, predicting the next word is what
[00:16:57.860 --> 00:17:01.500]   we're trying to accomplish. And that's what happens with decoder
[00:17:01.500 --> 00:17:05.660]   only models. And so they're really good for text generation.
[00:17:05.660 --> 00:17:11.140]   Now, if we're doing a mass language modeling task,
[00:17:11.140 --> 00:17:15.740]   typically, we're going to find that encoder only models are the
[00:17:15.740 --> 00:17:21.340]   preferable choice. And an example of an encoder only model
[00:17:21.380 --> 00:17:24.540]   would be something like Bert, which really started this whole
[00:17:24.540 --> 00:17:30.420]   idea of being able to apply attention, both forward and
[00:17:30.420 --> 00:17:34.860]   backwards, and essentially simulate that next product
[00:17:34.860 --> 00:17:40.500]   prediction task by using a mass filling tax task where instead
[00:17:40.500 --> 00:17:45.460]   of predicting the next word, we're trying to predict masked
[00:17:45.460 --> 00:17:50.660]   or corrupted words in the text. And so if you're building an
[00:17:51.100 --> 00:17:57.420]   and for a mass language modeling task, look at encoder only. And
[00:17:57.420 --> 00:18:06.420]   so again, really simple with blurb, we can go ahead and use
[00:18:06.420 --> 00:18:12.060]   the get HF objects. And the type of auto model we're looking for
[00:18:12.060 --> 00:18:17.380]   is the causal LM. And I'm gonna start with GPT to and also some
[00:18:17.380 --> 00:18:22.740]   of these models don't include a pad token, and you just have to
[00:18:22.740 --> 00:18:26.700]   add it in. So just be aware of that. Because we're going to be
[00:18:26.700 --> 00:18:30.500]   training on batches, and there's going to is going to require
[00:18:30.500 --> 00:18:34.820]   padding when we train these models. So you may have to add
[00:18:34.820 --> 00:18:38.140]   it in and some adding it in because GPT to doesn't have one.
[00:18:38.140 --> 00:18:46.300]   Okay, and then probably the most important part of language
[00:18:46.300 --> 00:18:50.380]   modeling is how do you arrange your, your text? How do you set
[00:18:50.380 --> 00:18:56.340]   it up? And essentially, what we want to accomplish is, we want
[00:18:56.340 --> 00:19:02.220]   to use as much of the text as we can. And we want to set it up in
[00:19:02.220 --> 00:19:07.220]   contiguous chunks to make that happen. And so the nice thing
[00:19:07.220 --> 00:19:11.260]   is, is that this pre processing that we want to do is the same
[00:19:11.260 --> 00:19:16.020]   for causal as it is for mass language modeling. And so
[00:19:16.020 --> 00:19:18.900]   introduced, like in the course, they talk about, they actually,
[00:19:18.900 --> 00:19:22.380]   they actually introduced several ways of pre processing, this is
[00:19:22.380 --> 00:19:27.460]   by far the best one, because it drops the least amount of text,
[00:19:27.460 --> 00:19:31.540]   so it gives us the maximum amount of text to train on. And
[00:19:31.540 --> 00:19:34.140]   that is to concatenate all the examples, and then split the
[00:19:34.140 --> 00:19:39.660]   whole corpus into chunks of equal size. And we actually have
[00:19:39.660 --> 00:19:44.620]   a pre processing function in blur that does this. The big
[00:19:44.620 --> 00:19:47.980]   question that you have to ask yourself is, how big should your
[00:19:47.980 --> 00:19:52.060]   chunks be? And again, this really depends on your compute
[00:19:52.060 --> 00:19:57.220]   constraints. For blur, I actually developed most of it on
[00:19:57.220 --> 00:20:02.700]   a pretty old now 1080 ti that has like a max of, you know,
[00:20:02.700 --> 00:20:08.300]   under 12 gigabytes of RAM. So, so I, so I have to do basically
[00:20:08.300 --> 00:20:11.260]   have very small chunks and very small batch sizes, when I'm
[00:20:11.260 --> 00:20:16.500]   developing things based on my compute constraints. And if
[00:20:16.500 --> 00:20:19.260]   you're not sure where to start, you can check out the tokenizer,
[00:20:19.260 --> 00:20:23.900]   the model max length property, and that will at least give you
[00:20:23.900 --> 00:20:28.540]   a good potential baseline to start with. And then also, just
[00:20:28.540 --> 00:20:31.780]   as an important note that they mentioned in the class, is that
[00:20:31.780 --> 00:20:36.500]   using a small chunk size can be detrimental in your particular
[00:20:37.100 --> 00:20:41.740]   scenario. So you really have to do some EDA and understand the
[00:20:41.740 --> 00:20:45.220]   text that you're trying to work with, to figure out something
[00:20:45.220 --> 00:20:49.100]   that's going to be meaningful for your particular task and
[00:20:49.100 --> 00:20:55.780]   give you good results. So, so I keep going to that screen. So if
[00:20:55.780 --> 00:21:00.940]   we go to pre processing, it's really simple. We have our text.
[00:21:01.380 --> 00:21:08.020]   And you can see that's just here in the first column. And I'm
[00:21:08.020 --> 00:21:13.300]   going to specify a chunk size of 128. And so what it's going to
[00:21:13.300 --> 00:21:19.500]   do is, it's going to process this in batch fashion. And it's
[00:21:19.500 --> 00:21:23.300]   going to take a look at essentially 1000 examples at a
[00:21:23.300 --> 00:21:26.940]   time, it's going to concatenate all of them. And it's going to
[00:21:26.940 --> 00:21:32.820]   chunk that into 120 token segments. And then for the last
[00:21:32.820 --> 00:21:36.700]   one, because it's going to be smaller, and probably be not
[00:21:36.700 --> 00:21:39.380]   only unhelpful, but maybe even detrimental to our modeling,
[00:21:39.380 --> 00:21:41.940]   because there's not going to be much to learn in that last
[00:21:41.940 --> 00:21:48.260]   particular chunk, we drop that last one. And that's what our LM
[00:21:48.260 --> 00:21:52.500]   pre processor does. And also notice that instead of
[00:21:54.660 --> 00:21:58.620]   pre concatenating, or using the pre concatenated version of our
[00:21:58.620 --> 00:22:01.580]   train and validation, I'm actually going to pass these in
[00:22:01.580 --> 00:22:05.380]   separately, so that they're processed separately. And our
[00:22:05.380 --> 00:22:10.340]   pre processor will return these back in a single data frame and
[00:22:10.340 --> 00:22:14.420]   also add that is valid column that we can use in our data
[00:22:14.420 --> 00:22:22.580]   block. And with that, we get to our next step, which is creating
[00:22:22.620 --> 00:22:28.180]   a data block. And this is a, this task is considered
[00:22:28.180 --> 00:22:31.620]   something that they call a self supervised task, which means
[00:22:31.620 --> 00:22:33.860]   that for language modeling, and one of the nice things with
[00:22:33.860 --> 00:22:37.380]   language with any self self supervised tasks is that you
[00:22:37.380 --> 00:22:43.100]   don't have to divide to define, or construct or do anything with
[00:22:43.100 --> 00:22:46.420]   special with creating the labels, the labels are
[00:22:46.460 --> 00:22:53.500]   essentially the inputs. And so we can go ahead and construct
[00:22:53.500 --> 00:22:59.380]   our transforms and our blocks to actually simply build those for
[00:22:59.380 --> 00:23:03.540]   you. And so if you look at blur, depending on whether you're
[00:23:03.540 --> 00:23:08.820]   doing a causal task, or you're doing a mass, a cost like a mass
[00:23:08.820 --> 00:23:12.540]   language model task, and you need the decoder input IDs, or
[00:23:12.540 --> 00:23:17.420]   you need to have the input shifted one token to the right,
[00:23:17.420 --> 00:23:23.540]   it will just do that for you. So when we actually look at the
[00:23:23.540 --> 00:23:32.180]   code here, you'll see that we use that same no op method to
[00:23:32.180 --> 00:23:35.500]   basically tell our data block that, hey, we don't need to do
[00:23:35.500 --> 00:23:38.980]   anything for the targets, the targets are going to be handled
[00:23:39.540 --> 00:23:45.900]   right here by our text block transform. So to define our data
[00:23:45.900 --> 00:23:52.060]   block, we're going to use a new batch tokenized transform called
[00:23:52.060 --> 00:23:56.180]   LM back tokenized transform. And one of the things that we are
[00:23:56.180 --> 00:24:01.020]   going to tell it that's a little bit different than the core
[00:24:01.020 --> 00:24:05.420]   batch tokenized transform is what strategy we want to use.
[00:24:05.940 --> 00:24:12.260]   And so in blur, how we actually construct our labels varies by
[00:24:12.260 --> 00:24:16.420]   these things called strategies. And we have two of them in blur,
[00:24:16.420 --> 00:24:20.020]   and it's set up so that there's a base class, and you can sub
[00:24:20.020 --> 00:24:25.540]   class it and create your own strategies. And for for how you
[00:24:25.540 --> 00:24:29.260]   want to basically build your labels. And then once you have
[00:24:29.260 --> 00:24:34.660]   your your LM strategy, and so blur, we have causal, I also
[00:24:34.660 --> 00:24:40.500]   have a BERT mass token strategy that that mimics the BERT paper
[00:24:40.500 --> 00:24:44.660]   in terms of how it mass tokens. Once you have that, you can go
[00:24:44.660 --> 00:24:48.580]   ahead and specify that as your LM strategy class. And that will
[00:24:48.580 --> 00:24:55.980]   ensure that as your raw data is processed, that the labels are
[00:24:55.980 --> 00:25:01.860]   generated correctly for whatever you're trying to do. And as a
[00:25:01.980 --> 00:25:06.220]   just as an FYI, you may want to look at the T five paper,
[00:25:06.220 --> 00:25:09.740]   because they actually have a pretty good discussion of a
[00:25:09.740 --> 00:25:14.740]   variety of of corruption methods that you may not be familiar
[00:25:14.740 --> 00:25:18.980]   with. So not just tokens, like for the on the mass language
[00:25:18.980 --> 00:25:24.140]   bottling side, beyond just masking tokens, or words, they
[00:25:24.140 --> 00:25:27.500]   also will mass phrases, or they'll move phrases or
[00:25:27.500 --> 00:25:29.860]   sentences or words around and you have to predict the right
[00:25:29.860 --> 00:25:35.260]   order. So there's a lot of potential strategies that I'll
[00:25:35.260 --> 00:25:38.860]   probably explore adding the blur, and you really whatever
[00:25:38.860 --> 00:25:41.340]   you can dream of, and whatever you want to try, you can
[00:25:41.340 --> 00:25:46.620]   actually probably build something here and use it with
[00:25:46.620 --> 00:25:52.740]   this particular data block code and be good to go. So, so yeah,
[00:25:52.740 --> 00:25:57.180]   so we're working with a causal language model, which is going
[00:25:57.180 --> 00:26:02.460]   to be based on GPT to. So we'll specify an input return type of
[00:26:02.460 --> 00:26:07.460]   causal LM text input. And that's so that our type dispatch
[00:26:07.460 --> 00:26:12.180]   methods are show batch and show results can work correctly. And
[00:26:12.180 --> 00:26:15.820]   then other than that, it's pretty much business as usual
[00:26:15.820 --> 00:26:21.020]   with using the mid level API, and creating our data block. And
[00:26:21.020 --> 00:26:23.980]   once we have that, we're going to create our data loaders,
[00:26:24.380 --> 00:26:29.220]   always the best practice is once your data loaders are good to go
[00:26:29.220 --> 00:26:34.260]   call one batch, and take a look at what you have in there. And
[00:26:34.260 --> 00:26:38.580]   so we can see, there's actually a little bug that I got to look
[00:26:38.580 --> 00:26:41.460]   at here, this should actually be maxed out at 128. But I'm
[00:26:41.460 --> 00:26:45.900]   getting one extra token in there. But we can see that we
[00:26:45.900 --> 00:26:49.140]   have our input IDs, right, and our labels are going to be the
[00:26:49.140 --> 00:26:54.380]   same, because in the causal task, they're just shifted over
[00:26:54.380 --> 00:26:57.820]   to the right. So we're going to see the tent the same tensor
[00:26:57.820 --> 00:27:02.380]   shape, and our input IDs as we see in our labels. And then we
[00:27:02.380 --> 00:27:06.060]   can also do the show batch, I may change this a little bit,
[00:27:06.060 --> 00:27:08.940]   this doesn't really give you a lot of helpful information,
[00:27:08.940 --> 00:27:17.660]   because it's the same thing. But yeah, so we're good to go. So
[00:27:17.660 --> 00:27:20.740]   once we have our data loaders, we are ready, we have something
[00:27:20.740 --> 00:27:25.940]   that we can actually model. The next thing is thinking about
[00:27:25.940 --> 00:27:31.300]   well, what are the metrics that we care about. And the two that
[00:27:31.300 --> 00:27:35.860]   are most likely to be of interest to you are perplexity
[00:27:35.860 --> 00:27:40.620]   and accuracy. And so if you look at the task page, that hugging
[00:27:40.620 --> 00:27:47.060]   face task resource, you'll see them recommend perplexity. And I
[00:27:47.260 --> 00:27:51.420]   I think actually, I'm not positive. But perplexity is
[00:27:51.420 --> 00:27:55.740]   really sounds fancy, but it's really a simple concept. And in
[00:27:55.740 --> 00:27:59.260]   the course, I can't remember who does the video, but actually,
[00:27:59.260 --> 00:28:01.780]   there's a really good description of like, how it
[00:28:01.780 --> 00:28:05.940]   works, and like what it means intuitively. And it simply is a
[00:28:05.940 --> 00:28:11.380]   measurement of how surprised or perplexed the model is by the
[00:28:11.380 --> 00:28:15.260]   predicted order token. And so the idea is, is that if the
[00:28:15.260 --> 00:28:18.860]   model is really confused and surprised by what is predicting,
[00:28:18.860 --> 00:28:22.700]   then your language model probably hasn't done a really
[00:28:22.700 --> 00:28:26.460]   good job of capturing the particular grammar of your
[00:28:26.460 --> 00:28:30.980]   target corpus. And so in that case, you're going to have a
[00:28:30.980 --> 00:28:37.820]   high perplexity. Whereas if it isn't surprised, then we should
[00:28:37.820 --> 00:28:42.580]   get a lower score for this metric. And it's an easy metric
[00:28:42.620 --> 00:28:46.460]   to calculate, because it's just the exponential of your cross
[00:28:46.460 --> 00:28:50.260]   entropy loss, which is already being calculated. Since again,
[00:28:50.260 --> 00:28:54.420]   we are looking at, we're essentially whether it's masked,
[00:28:54.420 --> 00:28:58.580]   or it's the next token, we're simply we're simply applying a
[00:28:58.580 --> 00:29:02.580]   cross entropy to actually calculate our loss. So all we
[00:29:02.580 --> 00:29:05.220]   gotta do is take the exponential that and that gives us our
[00:29:05.220 --> 00:29:11.780]   perplexity. The other metric that's helpful is accuracy. And
[00:29:11.780 --> 00:29:15.020]   again, it's just telling us like, how often did we predict
[00:29:15.020 --> 00:29:17.740]   the correct next word? Or how often did we predict the
[00:29:17.740 --> 00:29:26.700]   correct masked word? So given that with with blur, surprise,
[00:29:26.700 --> 00:29:32.700]   surprise, we have a LM metrics callback that you can use, that
[00:29:32.700 --> 00:29:36.700]   will correctly calculate that for you. And we include this as
[00:29:36.700 --> 00:29:39.660]   a callback, especially when you're using a masked language
[00:29:39.660 --> 00:29:43.500]   model, the tokens that we're not predicting are going to have a
[00:29:43.500 --> 00:29:48.580]   label of negative 100. And if you remember, that's a magic
[00:29:48.580 --> 00:29:52.660]   number that tells cross entropy loss, ignore that particular
[00:29:52.660 --> 00:29:57.380]   token. And so the the LM metrics callback ensures that when we
[00:29:57.380 --> 00:30:02.940]   calculate our metrics that it does, it only looks at the
[00:30:02.940 --> 00:30:06.220]   masked tokens that we're actually concerned with
[00:30:06.220 --> 00:30:11.540]   predicting. And so we can go ahead and pass that. Once we
[00:30:11.540 --> 00:30:16.140]   have our learner set up, again, by default, we're going to
[00:30:16.140 --> 00:30:21.020]   include the labels. If you didn't want to include the
[00:30:21.020 --> 00:30:27.580]   labels and calculate the loss with fast AI in your data block,
[00:30:27.580 --> 00:30:33.660]   you would have to pass include labels equals false by default,
[00:30:33.660 --> 00:30:39.500]   it's true. And if we leave the defaults as is, the hugging
[00:30:39.500 --> 00:30:43.220]   face models will actually as part of the forward pass, it
[00:30:43.220 --> 00:30:48.260]   will actually calculate the loss for you. And what we need to do
[00:30:48.260 --> 00:30:52.900]   is, since we're using cross entropy, loss is set our loss
[00:30:52.900 --> 00:30:58.540]   function to pre calculated cross entropy loss, so that when we
[00:30:58.540 --> 00:31:04.780]   actually show both the results and inputs and for it to work
[00:31:04.780 --> 00:31:08.780]   in fast AI, we have to have this method that has like a decodes
[00:31:08.780 --> 00:31:13.180]   and encodes method for being able to show the results of the
[00:31:13.180 --> 00:31:19.100]   pre calculated loss in the transformer model. So once we
[00:31:19.100 --> 00:31:24.420]   have that set up, I will add perplexity as one of the metrics
[00:31:24.420 --> 00:31:29.900]   that we want to look at this is available in fast AI. We can,
[00:31:29.900 --> 00:31:35.500]   again, just like we do for batches, another best practice
[00:31:35.500 --> 00:31:41.060]   is to run a batch of the inputs through our model and make sure
[00:31:41.060 --> 00:31:44.900]   that we're getting things that we expect. And so we can do that
[00:31:44.900 --> 00:31:50.700]   simply by creating a batch, calling learn dot model and
[00:31:50.740 --> 00:31:56.500]   passing those inputs in to get our predictions. And so we can
[00:31:56.500 --> 00:32:02.020]   go ahead and look at the information that is returned.
[00:32:02.020 --> 00:32:08.140]   And there's quite a bit. But we can see that just like with most
[00:32:08.140 --> 00:32:12.860]   hugging face, not most, I think all of them, we actually get an
[00:32:12.860 --> 00:32:16.460]   object. And you can see it includes the loss, it includes
[00:32:16.500 --> 00:32:23.620]   our logits, and potentially other information. So once we
[00:32:23.620 --> 00:32:30.740]   have our learner and our metric setup, we've verified that our
[00:32:30.740 --> 00:32:33.540]   batches look right, and that we can actually process
[00:32:33.540 --> 00:32:39.820]   predictions. The next step is to actually train our model. And
[00:32:39.980 --> 00:32:46.940]   again, it's works just like any other fast AI model. And also,
[00:32:46.940 --> 00:32:50.100]   if your cases are involving generated texts, so like what
[00:32:50.100 --> 00:32:53.180]   we're doing with GPT to, we're probably building maybe
[00:32:53.180 --> 00:32:58.780]   something like chatbot or something. Make sure again, you
[00:32:58.780 --> 00:33:02.980]   check the that I referenced last week that article how to
[00:33:02.980 --> 00:33:06.380]   generate text using different decoding methods for language
[00:33:06.380 --> 00:33:09.700]   generation with transformers, give you some good guidelines
[00:33:09.700 --> 00:33:13.580]   of potential hyper parameter selection or choices you want to
[00:33:13.580 --> 00:33:19.700]   make when you're doing generative tasks. In our
[00:33:19.700 --> 00:33:27.340]   particular case, here, we're just going to use the LR finder
[00:33:27.340 --> 00:33:32.420]   to get some reasonable options for setting our learning rate.
[00:33:32.420 --> 00:33:37.500]   And then we're going to just go ahead and fit for one cycle.
[00:33:37.780 --> 00:33:42.980]   Again, these things could run for a long time. And, and so go
[00:33:42.980 --> 00:33:47.100]   get yourself a cup of coffee. Or I have to say this because
[00:33:47.100 --> 00:33:51.300]   Sanyam's on on board, go get yourself a cup of chai as well.
[00:33:51.300 --> 00:33:58.140]   Either option is allowed. So we're going to go ahead and I
[00:33:58.140 --> 00:34:00.980]   had to say that I think contractually I was obligated to
[00:34:00.980 --> 00:34:05.220]   mention chai in almost every single broadcast.
[00:34:05.820 --> 00:34:09.060]   I mean, either ways I got you to mention it. So that's
[00:34:09.060 --> 00:34:14.900]   That'll be the quote over there on Twitter. Go get some chai,
[00:34:14.900 --> 00:34:17.380]   start training your language model, go get some chai.
[00:34:17.380 --> 00:34:20.220]   Chai stabilizes your mind and training booth.
[00:34:20.220 --> 00:34:26.540]   Oh my goodness. Wow. That's I never do that right there. You
[00:34:26.540 --> 00:34:29.140]   know, there's the fast book, there's the transformers book,
[00:34:29.140 --> 00:34:32.220]   the only thing that's missing is the chai book, you know, and
[00:34:32.220 --> 00:34:36.580]   you could use the, the notebooks way of building it out. And I
[00:34:36.580 --> 00:34:38.700]   would love to get a signed edition if you make that
[00:34:38.700 --> 00:34:39.220]   happen.
[00:34:39.220 --> 00:34:43.140]   I need a lot of chai to make that book happen. So
[00:34:43.140 --> 00:34:44.340]   Perfect. All right.
[00:34:44.340 --> 00:34:45.340]   It's a recursive loop.
[00:34:45.340 --> 00:34:51.140]   Keep us posted. So, so yeah, so we train it just like normal.
[00:34:51.140 --> 00:34:54.820]   We can call show results. And what shows doing is actually
[00:34:54.820 --> 00:34:58.900]   it's decoding the predictions. And when you work with most,
[00:35:00.540 --> 00:35:03.860]   and it's just using a greedy decoding mechanism here, I don't
[00:35:03.860 --> 00:35:07.700]   know how helpful this is. This may be changed here. Once I
[00:35:07.700 --> 00:35:14.900]   release or v2 to production and make it available just as the
[00:35:14.900 --> 00:35:17.700]   normal pip install. But right now it's just showing the
[00:35:17.700 --> 00:35:20.900]   decoded predictions, which and a lot of times when you're
[00:35:20.900 --> 00:35:24.660]   working with decoder only models, and you're generating
[00:35:24.660 --> 00:35:28.220]   text is a lot of gibberish. And that's why it's really important
[00:35:28.220 --> 00:35:31.060]   to look at that article and figure out like ways to
[00:35:31.060 --> 00:35:35.740]   potentially change those hyper parameters related to text
[00:35:35.740 --> 00:35:42.500]   generation to get something more meaningful. So that's training
[00:35:42.500 --> 00:35:49.660]   for inference, we can actually use we have two methods in blur.
[00:35:49.660 --> 00:35:56.780]   If we're doing text generation, we can use blur dot or blur
[00:35:56.820 --> 00:36:02.020]   generate. And if we are building a mass language model, we can go
[00:36:02.020 --> 00:36:07.980]   ahead and use blur fill mask and have it predict what the the
[00:36:07.980 --> 00:36:14.460]   mask or mask should be. The big thing is, is that in addition to
[00:36:14.460 --> 00:36:18.980]   being able to use this for inference in this fashion, again,
[00:36:18.980 --> 00:36:23.100]   we can, we've essentially built a language model that we can now
[00:36:23.100 --> 00:36:26.940]   use as the backbone for the other core NLP tasks that we
[00:36:26.940 --> 00:36:30.660]   talked about. And this is actually probably the more
[00:36:30.660 --> 00:36:34.900]   likely scenario. And you're likely trying to do something
[00:36:34.900 --> 00:36:39.820]   like name it to your recognition or question answering. And you
[00:36:39.820 --> 00:36:44.060]   want to have an LM that's, that's been fine tuned on your
[00:36:44.060 --> 00:36:48.580]   target corpus. So once you have that, you can actually use it
[00:36:48.580 --> 00:36:51.740]   just like we've used these other checkpoints, like distill
[00:36:51.740 --> 00:36:58.820]   Roberta, or Bert or Bart base or Bart large, we can use our
[00:36:58.820 --> 00:37:06.660]   trained LM in the same fashion. So if we go to inference, we can
[00:37:06.660 --> 00:37:14.100]   see that, again, get rid of the metrics, go back to FP 32. So
[00:37:14.100 --> 00:37:20.060]   that you can create a export pickle file. Once you have that,
[00:37:20.100 --> 00:37:25.220]   you can go ahead and use load loan load learner to basically
[00:37:25.220 --> 00:37:29.020]   build a inference learner. And you can see here, I'm going to
[00:37:29.020 --> 00:37:33.100]   call blur generate. And remember all those interesting hyper
[00:37:33.100 --> 00:37:35.580]   parameters that I was talking about that you can play with,
[00:37:35.580 --> 00:37:40.140]   you can pass those here, and it will apply that to the text
[00:37:40.140 --> 00:37:45.300]   generation. And you can see that given this particular example,
[00:37:45.660 --> 00:37:50.980]   setting the max length equal to 50, allowing it to do sampling,
[00:37:50.980 --> 00:37:58.300]   and also use top k of 25. And again, if you look, if you read
[00:37:58.300 --> 00:38:01.420]   that article, you can see what those what these things mean,
[00:38:01.420 --> 00:38:05.020]   and where they may be helpful or harmful depending on your
[00:38:05.020 --> 00:38:09.940]   particular task. And so we start with a, we give it some
[00:38:09.940 --> 00:38:13.340]   context blur is fun to work with, because, and then it
[00:38:13.340 --> 00:38:17.820]   creates some crazy answer, blur is fun to work with, because it
[00:38:17.820 --> 00:38:22.020]   is easy to learn and fun to play as they can communicate with
[00:38:22.020 --> 00:38:26.500]   anyone else in the world. So pretty good. Don't know if I'm
[00:38:26.500 --> 00:38:30.220]   going to set that up as the description of blur on my GitHub.
[00:38:30.220 --> 00:38:36.500]   But definitely fun to be able to play with this and kind of see
[00:38:36.500 --> 00:38:42.780]   what makes more sense for what you're trying to do. And that's
[00:38:42.780 --> 00:38:48.140]   it. And so for homework, I would say go through the course. And
[00:38:48.140 --> 00:38:50.740]   since this is our last session, if you haven't done any of the
[00:38:50.740 --> 00:38:56.340]   course, start with part one and go through those sections. Once
[00:38:56.340 --> 00:38:59.140]   you get to part two, go through those sections as well, maybe
[00:38:59.140 --> 00:39:02.260]   reference the material that we talked about. But there's
[00:39:02.260 --> 00:39:08.900]   definitely a lot more depth to the part two content. And I
[00:39:08.900 --> 00:39:11.860]   don't know if and when they're doing a part three. So you got
[00:39:11.860 --> 00:39:14.700]   time to really kind of go through it slowly, take notes,
[00:39:14.700 --> 00:39:17.860]   go back through things, ask questions, and really make sure
[00:39:17.860 --> 00:39:21.420]   you understand, you know, how transformers work, how these
[00:39:21.420 --> 00:39:24.820]   different architectures work, and in terms of language
[00:39:24.820 --> 00:39:28.780]   modeling, how the pipelines work, for just being able to do
[00:39:28.780 --> 00:39:32.220]   inference. And then also from section seven, how to actually
[00:39:32.220 --> 00:39:37.220]   build end to end causal and mass language models. And we've shown
[00:39:37.220 --> 00:39:40.660]   how to do a causal language model in blur, but you'll also
[00:39:40.660 --> 00:39:44.420]   find a lot of examples in the course how to do that with their
[00:39:44.420 --> 00:39:49.500]   trainer API, and with also their accelerate API, which is more
[00:39:49.500 --> 00:39:53.140]   like creating your own custom training and eval loop. And so
[00:39:53.140 --> 00:39:56.380]   there's a lot of good things to learn just by going through the
[00:39:56.380 --> 00:40:01.380]   course. And that's it. And so I'm going to see if there's any
[00:40:01.380 --> 00:40:03.620]   questions or comments.
[00:40:03.620 --> 00:40:07.820]   First of all, so for anyone who's joining us live, can you
[00:40:07.820 --> 00:40:10.140]   please, I know you're on the other part of the screen, but
[00:40:10.140 --> 00:40:13.380]   can we please get an applause for Wade and thank you for Wade
[00:40:13.380 --> 00:40:19.140]   for really, really making these sessions so awesome. I'm a noob
[00:40:19.140 --> 00:40:22.780]   at NLP. And even though I'm able to answer Wade's question that
[00:40:22.780 --> 00:40:26.420]   he throws at me just to check my knowledge, but all of these
[00:40:26.420 --> 00:40:31.700]   sessions happen to happen because of his help. So please
[00:40:31.700 --> 00:40:35.780]   join me in thanking Wade and for all of his awesome work here.
[00:40:35.780 --> 00:40:38.700]   He's literally created this framework for all of us for
[00:40:39.980 --> 00:40:41.100]   no reason, apparently.
[00:40:41.100 --> 00:40:44.300]   Oh, thank you. Yeah, I just did it for just you know, the heck
[00:40:44.300 --> 00:40:46.660]   of it. I was like, you know what, I don't need to sleep. So
[00:40:46.660 --> 00:40:50.220]   yeah, thank you so much. I really appreciate you Sanyam. I
[00:40:50.220 --> 00:40:54.500]   appreciate everybody sticking with me. I know it's been a slog
[00:40:54.500 --> 00:40:58.980]   since part one and part two is again, so so much more denser.
[00:40:58.980 --> 00:41:02.060]   So I really appreciate everybody that's been on here being able
[00:41:02.060 --> 00:41:07.500]   to meet a lot of folks here and also on Twitter. And I hope that
[00:41:07.500 --> 00:41:11.780]   there'll be more of these type of podcasts in the future. And
[00:41:11.780 --> 00:41:15.260]   I'll always have my coffee and who knows maybe one of these
[00:41:15.260 --> 00:41:19.900]   days a cup of chai ready. So yeah, so thank you a lot Sanyam
[00:41:19.900 --> 00:41:22.100]   and thank you everybody for being a part of this.
[00:41:22.100 --> 00:41:26.500]   All thanks to you. Many people are saying thanks as I'm trying
[00:41:26.500 --> 00:41:28.340]   to continue highlighting that.
[00:41:28.340 --> 00:41:30.100]   Oh, awesome. Yeah, yeah. Thank you.
[00:41:30.100 --> 00:41:34.100]   But just to point out to the folks, we wanted to host a
[00:41:34.100 --> 00:41:39.460]   competition, which we will not next week. So the world's best
[00:41:39.460 --> 00:41:42.420]   deep learning course fast AI starts in a few weeks. And we
[00:41:42.420 --> 00:41:45.340]   want to invite more and more people. Competitions are more
[00:41:45.340 --> 00:41:48.540]   awesome when more people join because then people like me get
[00:41:48.540 --> 00:41:51.380]   depressed because you're I'm on the bottom of the leaderboard.
[00:41:51.380 --> 00:41:54.020]   Other people like you might get excited because you might be
[00:41:54.020 --> 00:41:58.300]   above so we will align it with that. We'll keep you posted on
[00:41:58.300 --> 00:42:01.380]   that. Please follow Wade on Twitter, you'll be informed of
[00:42:01.380 --> 00:42:05.340]   that I want to give a shout out to two competitions that might
[00:42:05.340 --> 00:42:06.620]   be somewhat relevant here.
[00:42:06.620 --> 00:42:17.540]   So I saw, I think, Martin in the chat as well. I'm not sure if it
[00:42:17.540 --> 00:42:23.340]   was him or someone else. Martin Hens is, I think, as far as I
[00:42:23.340 --> 00:42:26.420]   understand, world's first kernels Grandmaster and he was
[00:42:26.620 --> 00:42:31.220]   ranked number one for a really long time. He's been curating
[00:42:31.220 --> 00:42:35.380]   notebooks every single week, Kaggle notebooks that he calls
[00:42:35.380 --> 00:42:39.540]   hidden gems. And those are like really awesome reads like, just
[00:42:39.540 --> 00:42:42.540]   spend three months reading through all of those notebooks,
[00:42:42.540 --> 00:42:45.380]   you'll learn so much I can guarantee you that he'll be
[00:42:45.380 --> 00:42:49.460]   launching a competition in I think two weeks. That would be
[00:42:49.460 --> 00:42:52.220]   an awesome venue in the meantime, while you wait for our
[00:42:52.220 --> 00:42:57.460]   competition to launch. Rob Miller, who's a 3x Kaggle
[00:42:57.460 --> 00:43:00.500]   Grandmaster, he has an awesome Twitch channel, you can find the
[00:43:00.500 --> 00:43:04.220]   link here has just launched a Kaggle competition which
[00:43:04.220 --> 00:43:07.860]   involves music classification. So still somewhat related. I
[00:43:07.860 --> 00:43:10.820]   would highly encourage everyone to check these two competitions
[00:43:10.820 --> 00:43:14.540]   out if you can't wait for ours and you're welcome to join us.
[00:43:14.540 --> 00:43:17.260]   Just follow Wade and you'll find it as soon as it launches.
[00:43:17.260 --> 00:43:21.060]   Yeah, the music one actually really, if you're interested in
[00:43:21.060 --> 00:43:24.740]   doing that, think about what we just talked about, like with the
[00:43:24.740 --> 00:43:29.380]   language modeling, this would be a good example to probably
[00:43:29.380 --> 00:43:33.060]   explore, you know, training an LM from scratch, and also
[00:43:33.060 --> 00:43:34.260]   probably a lot of fun too.
[00:43:34.260 --> 00:43:38.900]   I'll I'll mention one quick rule, which should make this
[00:43:38.900 --> 00:43:42.260]   more interesting. So no pre trained models are allowed. And
[00:43:42.260 --> 00:43:44.060]   everything needs to be created from scratch.
[00:43:44.060 --> 00:43:45.180]   Oh, yeah, cool.
[00:43:45.180 --> 00:43:50.860]   Awesome. But thanks again, Wade. And thanks to everyone who's
[00:43:50.860 --> 00:43:54.100]   been joining us every week. I know it is awesome. That's how
[00:43:54.100 --> 00:43:57.220]   you'll join. Please consider joining other sessions, which
[00:43:57.220 --> 00:44:01.460]   unfortunately will have just me. But thanks again, Wade. And
[00:44:01.460 --> 00:44:02.220]   thanks, everyone.
[00:44:02.220 --> 00:44:04.540]   Yeah, thank you. All right. We'll see everyone.
[00:44:04.540 --> 00:44:14.540]   [BLANK_AUDIO]

