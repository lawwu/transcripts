
[00:00:00.000 --> 00:00:05.580]   Human-level AI is deep, deep into an intelligence explosion.
[00:00:05.580 --> 00:00:10.000]   Things like inventing the transformer or discovering chinchilla scaling
[00:00:10.000 --> 00:00:13.680]   and doing your training runs more optimally or creating flash attention.
[00:00:13.680 --> 00:00:18.220]   That set of inputs probably would yield the kind of AI capabilities
[00:00:18.220 --> 00:00:19.720]   needed for intelligence explosion.
[00:00:19.720 --> 00:00:22.260]   You have a race between, on the one hand,
[00:00:22.260 --> 00:00:26.640]   the project of getting strong interpretability and shaping motivations,
[00:00:26.640 --> 00:00:31.400]   and on the other hand, these AIs, in ways that you don't perceive,
[00:00:31.400 --> 00:00:32.860]   make the AI take over happen.
[00:00:32.860 --> 00:00:36.660]   We spend more compute by having a larger brain than other animals,
[00:00:36.660 --> 00:00:38.660]   and then we have a longer childhood.
[00:00:38.660 --> 00:00:42.780]   It's analogous to having a bigger model and having more training time with it.
[00:00:42.780 --> 00:00:45.320]   It seemed very implausible that we couldn't do better
[00:00:45.320 --> 00:00:47.160]   than completely brute force evolution.
[00:00:47.160 --> 00:00:50.000]   How quickly are we running through those orders of magnitude?
[00:00:50.000 --> 00:00:54.120]   Okay, today I have the pleasure of speaking with Karl Schulman,
[00:00:54.120 --> 00:00:57.500]   many of my former guests, and this is not an exaggeration,
[00:00:57.500 --> 00:01:02.260]   many of my former guests have told me that a lot of their biggest ideas,
[00:01:02.260 --> 00:01:04.960]   perhaps most of their biggest ideas, have come directly from Karl,
[00:01:04.960 --> 00:01:08.340]   especially when it has to do with the intelligence explosion and its impacts.
[00:01:08.340 --> 00:01:11.100]   And so I decided to go directly to the source,
[00:01:11.100 --> 00:01:13.480]   and we have Karl today on the podcast.
[00:01:13.480 --> 00:01:17.520]   Karl keeps a super low profile, but he is one of the most interesting
[00:01:17.520 --> 00:01:21.820]   intellectuals I've ever encountered, and this is actually his second podcast ever.
[00:01:21.820 --> 00:01:25.700]   So we're going to get to get deep into the heart of many of the most important ideas
[00:01:25.700 --> 00:01:28.620]   that are circulating right now, directly from the source.
[00:01:28.620 --> 00:01:32.880]   So, and by the way, so Karl is also an advisor to the Open Philanthropy Project,
[00:01:32.880 --> 00:01:37.260]   which is one of the biggest funders on causes having to do with AI and its risks,
[00:01:37.260 --> 00:01:39.260]   not to mention global health and well-being.
[00:01:39.260 --> 00:01:44.180]   And he is a research associate at the Future of Humanity Institute at Oxford.
[00:01:44.180 --> 00:01:46.600]   So Karl, it's a huge pleasure to have you on the podcast.
[00:01:46.600 --> 00:01:47.400]   Thanks for coming.
[00:01:47.400 --> 00:01:48.640]   Thank you, Dharkesh.
[00:01:48.640 --> 00:01:53.940]   I've enjoyed seeing some of your episodes recently, and I'm glad to be on the show.
[00:01:53.940 --> 00:01:54.480]   Excellent.
[00:01:54.480 --> 00:01:55.600]   Let's talk about AI.
[00:01:55.600 --> 00:02:01.200]   Before we get into the details, give me the sort of big picture explanation
[00:02:01.200 --> 00:02:07.200]   of the feedback loops and just the general dynamics that would start
[00:02:07.200 --> 00:02:10.580]   when you have something that is approaching human-level intelligence.
[00:02:10.580 --> 00:02:15.760]   Yeah, so I think the way to think about it is we have a process now
[00:02:15.760 --> 00:02:24.260]   where humans are developing new computer chips, new software, running larger training runs.
[00:02:24.260 --> 00:02:30.180]   And it takes a lot of work to keep Moore's law chugging.
[00:02:30.180 --> 00:02:32.480]   Well, it was, it's slowing down now.
[00:02:32.480 --> 00:02:39.020]   And it takes a lot of work to develop things like transformers, to develop a lot
[00:02:39.020 --> 00:02:43.320]   of the improvements to AI and neural networks that are advancing things.
[00:02:43.320 --> 00:02:50.700]   And the core method that I think I want to highlight on this podcast,
[00:02:50.700 --> 00:02:55.960]   and I think is underappreciated, is the idea of input-output curves.
[00:02:55.960 --> 00:03:03.300]   So we can look at the increasing difficulty of improving chips.
[00:03:03.300 --> 00:03:08.260]   And so sure, each time you double the performance of computers, it's harder.
[00:03:08.260 --> 00:03:12.180]   And as we approach physical limits, eventually it becomes impossible.
[00:03:12.180 --> 00:03:14.480]   But how much harder?
[00:03:14.480 --> 00:03:21.860]   So there's a paper called "Ideas Getting Harder to Find" that was published a few years ago.
[00:03:21.860 --> 00:03:30.620]   Something like 10 years ago at MIRI, we did, I mean, I did an early version of this analysis
[00:03:30.620 --> 00:03:36.420]   using mainly data from Intel, unlike the large semiconductor fabricators.
[00:03:36.420 --> 00:03:44.540]   Anyway, and so in this paper, they cover a period where the productivity of computing
[00:03:44.540 --> 00:03:45.540]   went up a million fold.
[00:03:45.540 --> 00:03:50.460]   So you could get a million times the computing operations per second, per dollar.
[00:03:50.460 --> 00:03:51.460]   Big change.
[00:03:51.460 --> 00:03:54.180]   But it got harder.
[00:03:54.180 --> 00:04:00.940]   So the amount of investment, the labor force required to make those continuing advancements
[00:04:00.940 --> 00:04:02.700]   went up and up and up.
[00:04:02.700 --> 00:04:07.940]   Indeed, it went up 18 fold over that period.
[00:04:07.940 --> 00:04:11.580]   So some take this to say, "Oh, diminishing returns.
[00:04:11.580 --> 00:04:13.140]   Things are just getting harder and harder.
[00:04:13.140 --> 00:04:15.480]   And so that will be the end of progress eventually."
[00:04:15.480 --> 00:04:26.860]   However, in a world where AI is doing the work, that doubling of computing performance
[00:04:26.860 --> 00:04:32.220]   translates pretty directly to a doubling or better of the effective labor supply.
[00:04:32.220 --> 00:04:40.580]   That is, if when we had that million fold compute increase, we used it to run artificial
[00:04:40.580 --> 00:04:49.620]   intelligences who would replace human scientists and engineers, then the 18x increase in the
[00:04:49.620 --> 00:04:53.060]   labor demands of the industry would be trivial.
[00:04:53.060 --> 00:04:58.900]   We're getting more than one doubling of the effective labor supply that we need for each
[00:04:58.900 --> 00:05:03.260]   doubling of the labor requirement.
[00:05:03.260 --> 00:05:06.860]   And in that data set, it's like over four.
[00:05:06.860 --> 00:05:13.340]   So we double compute, okay, now we need somewhat more researchers, but a lot less than twice
[00:05:13.340 --> 00:05:14.580]   as many.
[00:05:14.580 --> 00:05:21.900]   And so, okay, we use up some of those doublings of compute on the increasing difficulty of
[00:05:21.900 --> 00:05:27.440]   further research, but most of them are left to expedite the process.
[00:05:27.440 --> 00:05:35.260]   So if you double your labor force, that's enough to get several doublings of compute.
[00:05:35.260 --> 00:05:41.960]   You use up one of them on meeting the increased demands from diminishing returns.
[00:05:41.960 --> 00:05:44.820]   The others can be used to accelerate the process.
[00:05:44.820 --> 00:05:52.460]   So your first doubling takes however many months, your next doubling can take a smaller
[00:05:52.460 --> 00:06:00.780]   fraction of that, the next doubling less and so on, at least insofar as this, the outputs
[00:06:00.780 --> 00:06:07.460]   you're generating compute for AI in the story are able to serve the function of the necessary
[00:06:07.460 --> 00:06:08.460]   inputs.
[00:06:08.460 --> 00:06:14.220]   If there are other inputs that you need, eventually those become a bottleneck and you wind up
[00:06:14.220 --> 00:06:15.500]   more restricted on this.
[00:06:15.500 --> 00:06:16.500]   Got it.
[00:06:16.500 --> 00:06:17.500]   Okay.
[00:06:17.500 --> 00:06:21.740]   So yeah, I think the Bloom paper had that 30, there was 35% increase in, was it translational
[00:06:21.740 --> 00:06:24.340]   intensity or cost per flop?
[00:06:24.340 --> 00:06:28.180]   And there was a 7% increase per year in the number of researchers required to sustain
[00:06:28.180 --> 00:06:29.180]   that pace.
[00:06:29.180 --> 00:06:36.980]   So some of this in, yeah, it's like four to five doublings of compute per doubling of
[00:06:36.980 --> 00:06:37.980]   labor inputs.
[00:06:37.980 --> 00:06:41.220]   I guess there's a lot of questions you can delve into in terms of whether you would expect
[00:06:41.220 --> 00:06:49.080]   a similar scale with AI and whether it makes sense to think of AI as a population of researchers
[00:06:49.080 --> 00:06:52.020]   that keeps growing with compute itself.
[00:06:52.020 --> 00:06:53.020]   Actually let's go there.
[00:06:53.020 --> 00:06:58.580]   So can you explain the intuition that compute is a good proxy for the number of AI researchers,
[00:06:58.580 --> 00:06:59.580]   so to speak?
[00:06:59.580 --> 00:07:05.040]   So far I've talked about hardware as an initial example, because we had good data about a
[00:07:05.040 --> 00:07:06.100]   past period.
[00:07:06.100 --> 00:07:10.380]   You can also make improvements on the software side.
[00:07:10.380 --> 00:07:15.780]   And we think about an intelligence explosion that can include AI is doing work on making
[00:07:15.780 --> 00:07:21.100]   hardware better, making better software, making more hardware.
[00:07:21.100 --> 00:07:29.540]   But the basic idea for the hardware is especially simple in that if you have a worker, an AI
[00:07:29.540 --> 00:07:33.900]   worker that can substitute for a human, if you have twice as many computers, you can
[00:07:33.900 --> 00:07:36.900]   run two separate instances of them.
[00:07:36.900 --> 00:07:44.780]   And then they can do two different jobs, manage two different machines, work on two different
[00:07:44.780 --> 00:07:46.500]   design problems.
[00:07:46.500 --> 00:07:52.100]   Now you can get more gains than just what you would get by having two instances.
[00:07:52.100 --> 00:07:57.420]   We get improvements from using some of our compute, not just to run more instances of
[00:07:57.420 --> 00:08:01.120]   the existing AI, but to train larger AIs.
[00:08:01.120 --> 00:08:06.020]   So there's hardware technology, how much you can get per dollar you spend on hardware.
[00:08:06.020 --> 00:08:07.900]   And there's software technology.
[00:08:07.900 --> 00:08:10.100]   And the software can be copied freely.
[00:08:10.100 --> 00:08:14.860]   So if you've got the software, it doesn't necessarily make that much sense to say, "Oh,
[00:08:14.860 --> 00:08:18.180]   we've got a hundred Microsoft Windows."
[00:08:18.180 --> 00:08:25.000]   You can make as many copies as you need for whatever Microsoft will charge you.
[00:08:25.000 --> 00:08:26.720]   But for hardware, it's different.
[00:08:26.720 --> 00:08:32.220]   It matters how much we actually spend on the hardware at a given price.
[00:08:32.220 --> 00:08:38.300]   And if we look at the changes that have been driving AI recently, that is the thing that
[00:08:38.300 --> 00:08:39.380]   is really off trend.
[00:08:39.380 --> 00:08:48.220]   We are spending tremendously more money on computer hardware for training big AI models.
[00:08:48.220 --> 00:08:49.220]   Yeah.
[00:08:49.220 --> 00:08:50.220]   Okay.
[00:08:50.220 --> 00:08:55.140]   So there's the investment in hardware, there's the hardware technology itself, and there's
[00:08:55.140 --> 00:08:56.740]   the software progress itself.
[00:08:56.740 --> 00:09:00.220]   The AI is getting better because we're spending more money on it, because our hardware itself
[00:09:00.220 --> 00:09:05.420]   is getting better over time, and because we're developing better models or better adjustments
[00:09:05.420 --> 00:09:06.860]   to those models.
[00:09:06.860 --> 00:09:08.260]   Where is the loop here?
[00:09:08.260 --> 00:09:15.220]   The work involved in designing new hardware and software is being done by people now.
[00:09:15.220 --> 00:09:19.020]   They use computer tools to assist them.
[00:09:19.020 --> 00:09:29.820]   But computer time is not the primary cost for NVIDIA designing chips, for TSMC producing
[00:09:29.820 --> 00:09:36.300]   them, for ASML making lithography equipment to serve the TSMC fabs.
[00:09:36.300 --> 00:09:43.020]   And even in AI software research, that has become quite compute-intensive, but I think
[00:09:43.020 --> 00:09:49.920]   we're still in the range where at a place like DeepMind, salaries were still larger
[00:09:49.920 --> 00:09:56.420]   than compute for the experiments, although tremendously, tremendously more of the expenditures
[00:09:56.420 --> 00:10:00.860]   were on compute relative to salaries than in the past.
[00:10:00.860 --> 00:10:07.100]   If you take all of the work that's being done by those humans, there's like low tens of
[00:10:07.100 --> 00:10:13.300]   thousands of people working at NVIDIA designing GPUs specialized for AI.
[00:10:13.300 --> 00:10:20.580]   I think there's more like 70,000 people at TSMC, which is the leading producer of cutting
[00:10:20.580 --> 00:10:21.940]   edge chips.
[00:10:21.940 --> 00:10:28.080]   There's a lot of additional people at companies like ASML that supply them with the tools
[00:10:28.080 --> 00:10:29.420]   they need.
[00:10:29.420 --> 00:10:35.300]   And then a company like DeepMind, I think from their public filings, they recently had
[00:10:35.300 --> 00:10:41.020]   a thousand people, OpenAI I think is a few hundred people, Anthropic is less.
[00:10:41.020 --> 00:10:48.680]   If you add up things like Facebook AI Research, Google Brain, R&D, you get thousands or tens
[00:10:48.680 --> 00:10:54.020]   of thousands of people who are working on AI research.
[00:10:54.020 --> 00:10:58.260]   We'd want to zoom in on those who are developing new methods rather than narrow applications.
[00:10:58.260 --> 00:11:04.940]   So inventing the transformer definitely counts, optimizing for some particular business's
[00:11:04.940 --> 00:11:07.780]   data set cleaning, probably not.
[00:11:07.780 --> 00:11:10.380]   But so those people are doing this work.
[00:11:10.380 --> 00:11:12.780]   They're driving quite a lot of progress.
[00:11:12.780 --> 00:11:19.460]   What we observe in the growth of people relative to the growth of those capabilities, is that
[00:11:19.460 --> 00:11:27.740]   pretty consistently the capabilities are doubling on a shorter timescale than the people required
[00:11:27.740 --> 00:11:29.100]   to do them are doubling.
[00:11:29.100 --> 00:11:31.620]   And so there's work.
[00:11:31.620 --> 00:11:37.460]   We talked about hardware and how historically it was pretty dramatic, like four or five
[00:11:37.460 --> 00:11:42.780]   doublings of compute efficiency per doubling of human inputs.
[00:11:42.780 --> 00:11:46.020]   I think that's a bit lower now as we get towards the end of Moore's law.
[00:11:46.020 --> 00:11:49.220]   Although interestingly, not as much lower as you might think, because the growth of
[00:11:49.220 --> 00:11:51.820]   inputs has also slowed recently.
[00:11:51.820 --> 00:12:00.420]   On the software side, there's some work by Tomei Basaroglu, and I think collaborators,
[00:12:00.420 --> 00:12:07.860]   it may have been his thesis, it's called "Are Models Getting Harder to Find?"
[00:12:07.860 --> 00:12:13.300]   And so it's applying the same sort of analysis as "Are Ideas Getting Harder to Find?"
[00:12:13.300 --> 00:12:20.540]   And you can look at growth rates of papers from citations, employment at these companies,
[00:12:20.540 --> 00:12:27.380]   and it seems like the doubling time of these workers driving the software advances is several
[00:12:27.380 --> 00:12:34.540]   years, or at least a couple of years, whereas the doubling of effective compute from algorithmic
[00:12:34.540 --> 00:12:36.360]   progress is faster.
[00:12:36.360 --> 00:12:43.020]   So there's a group called Epoch, they received grants from Open Philanthropy, and they do
[00:12:43.020 --> 00:12:48.220]   work collecting datasets that are relevant to forecasting AI progress.
[00:12:48.220 --> 00:12:56.860]   And so their headline results for what's the rate of progress in hardware, in software,
[00:12:56.860 --> 00:13:00.740]   and just growth in budgets are as follows.
[00:13:00.740 --> 00:13:05.940]   So for hardware, they're looking at a doubling of hardware efficiency that's two years.
[00:13:05.940 --> 00:13:10.300]   It's possible it's a bit better than that when you take into account certain specializations
[00:13:10.300 --> 00:13:11.800]   for AI workloads.
[00:13:11.800 --> 00:13:18.480]   For the growth of budgets, they find a doubling time that's something like six months in recent
[00:13:18.480 --> 00:13:24.320]   years, which is pretty tremendous relative to the historical rates.
[00:13:24.320 --> 00:13:26.820]   We should maybe get into that later.
[00:13:26.820 --> 00:13:31.920]   And then on the algorithmic progress side, mainly using ImageNet type datasets right
[00:13:31.920 --> 00:13:35.640]   now, they find a doubling time that's less than one year.
[00:13:35.640 --> 00:13:42.400]   And so you combine all of these things and the growth of effective compute for training
[00:13:42.400 --> 00:13:47.720]   big AIs, it's pretty drastic.
[00:13:47.720 --> 00:13:51.520]   I think I saw an estimate that GPT-4 costs like $50 million around that range to train.
[00:13:51.520 --> 00:13:58.920]   Now, suppose that AGI takes 1000x that, if you were to scale up GPT-4, it might not be
[00:13:58.920 --> 00:14:01.400]   that, just for the sake of example.
[00:14:01.400 --> 00:14:05.920]   So part of that will come from companies just spending a lot more to train the models and
[00:14:05.920 --> 00:14:07.680]   that just greater investment.
[00:14:07.680 --> 00:14:10.200]   Part of that will come from them having better models.
[00:14:10.200 --> 00:14:15.720]   So that would have taken a 10x increase in the model to get, naively, you can do with
[00:14:15.720 --> 00:14:18.920]   having a better model that you only need to do scale up.
[00:14:18.920 --> 00:14:23.760]   You get the same effect of increasing it by 10x just from having a better model.
[00:14:23.760 --> 00:14:26.520]   And so yeah, you can spend more money on it to train a bigger model, you can just have
[00:14:26.520 --> 00:14:32.960]   a better model, or you can have chips that are cheaper to train so you get more compute
[00:14:32.960 --> 00:14:34.480]   for the same dollars.
[00:14:34.480 --> 00:14:39.200]   And okay, so those are the three you're describing, the ways in which the "effective compute"
[00:14:39.200 --> 00:14:40.200]   would increase.
[00:14:40.200 --> 00:14:45.480]   From looking at it right now, it looks like, yeah, you might get two or three doublings
[00:14:45.480 --> 00:14:52.000]   of effective compute for this thing that we're calling software progress, which people get
[00:14:52.000 --> 00:14:57.400]   by asking, "Well, how much less compute can you use now to achieve the same benchmark
[00:14:57.400 --> 00:14:59.080]   as you achieved before?"
[00:14:59.080 --> 00:15:03.680]   There are reasons to not fully identify this with software progress, as you might naively
[00:15:03.680 --> 00:15:07.520]   think of it, because some of it can be enabled by the other.
[00:15:07.520 --> 00:15:12.920]   So when you have a lot of compute, you can do more experiments and find algorithms that
[00:15:12.920 --> 00:15:15.240]   work better.
[00:15:15.240 --> 00:15:20.600]   Sometimes the additional compute, you can get higher efficiency by running a bigger
[00:15:20.600 --> 00:15:22.240]   model, we were talking about earlier.
[00:15:22.240 --> 00:15:28.680]   And so that means you're getting more for each GPU that you have, because you made this
[00:15:28.680 --> 00:15:29.680]   larger expenditure.
[00:15:29.680 --> 00:15:36.880]   And that can look like a software improvement, because this model, it's not a hardware improvement
[00:15:36.880 --> 00:15:40.600]   directly, because it's doing more with the same hardware, but you wouldn't have been
[00:15:40.600 --> 00:15:44.760]   able to achieve it without having a ton of GPUs to do the big training run.
[00:15:44.760 --> 00:15:50.480]   The feedback loop itself involves the AI that is the result of this greater effective compute
[00:15:50.480 --> 00:15:58.240]   helping you train better AI, or use less effective compute in the future to train better AI.
[00:15:58.240 --> 00:16:00.440]   It can help on the hardware design.
[00:16:00.440 --> 00:16:03.280]   So like Nvidia is a fabulous chip design company.
[00:16:03.280 --> 00:16:05.320]   They don't make their own chips.
[00:16:05.320 --> 00:16:14.440]   They send files of instructions to TSMC, which then fabricates the chips in their own facilities.
[00:16:14.440 --> 00:16:23.360]   And so the work of those 10,000 plus people, if you could automate that and have the equivalent
[00:16:23.360 --> 00:16:30.160]   of a million people doing that work, then I think you would pretty quickly get the kind
[00:16:30.160 --> 00:16:35.880]   of improvements that can be achieved with the existing nodes that TSMC is operating
[00:16:35.880 --> 00:16:36.880]   on.
[00:16:36.880 --> 00:16:41.920]   You could get a lot of those chip design gains, basically like doing the job of improving
[00:16:41.920 --> 00:16:46.240]   chip design that those people are working on now, but get it done faster.
[00:16:46.240 --> 00:16:47.480]   So that's one thing.
[00:16:47.480 --> 00:16:51.040]   I think that's less important for the intelligence explosion.
[00:16:51.040 --> 00:16:57.560]   The reason being that when you make an improvement to chip design, it only applies to the chips
[00:16:57.560 --> 00:16:59.440]   you make after that.
[00:16:59.440 --> 00:17:05.880]   If you make an improvement in AI software, it has the potential to be immediately applied
[00:17:05.880 --> 00:17:09.080]   to all of the GPUs that you already have.
[00:17:09.080 --> 00:17:15.640]   And so the thing that I think is most disruptive and most important as the leading edge of
[00:17:15.640 --> 00:17:22.020]   the change from AI automation of the inputs to AI is on the software side.
[00:17:22.020 --> 00:17:27.440]   At what point would it get to the point where the AIs are helping develop better software
[00:17:27.440 --> 00:17:29.440]   or better models for future AIs?
[00:17:29.440 --> 00:17:35.120]   Some people claim today, for example, that programmers at OpenAI are using Copilot to
[00:17:35.120 --> 00:17:36.680]   write programs now.
[00:17:36.680 --> 00:17:39.560]   So in some sense, you're already having that sort of feedback loop.
[00:17:39.560 --> 00:17:43.240]   I'm a little skeptical of that as a mechanism.
[00:17:43.240 --> 00:17:48.560]   At what point would it be the case that the AI is contributing significantly in the sense
[00:17:48.560 --> 00:17:52.600]   that it would almost be the equivalent of having additional researchers to AI progress
[00:17:52.600 --> 00:17:53.600]   in software?
[00:17:53.600 --> 00:17:57.440]   The quantitative magnitude of the help is absolutely central.
[00:17:57.440 --> 00:18:03.620]   So there are plenty of companies that make some product that very slightly boost productivity.
[00:18:03.620 --> 00:18:09.760]   So when Xerox makes fax machines, it maybe increases people's productivity in office
[00:18:09.760 --> 00:18:12.760]   work by 0.1% or something.
[00:18:12.760 --> 00:18:20.080]   You're not going to have explosive growth out of that because, okay, now 0.1% more effective
[00:18:20.080 --> 00:18:26.400]   R&D at Xerox than any customers buying the machines.
[00:18:26.400 --> 00:18:27.680]   Not that important.
[00:18:27.680 --> 00:18:37.260]   So I think the thing to look for is when is it the case that the contributions from AI
[00:18:37.260 --> 00:18:45.160]   are starting to become as large or larger as the contributions from humans?
[00:18:45.160 --> 00:18:54.320]   When this is boosting their effective productivity by 50% or 100% and if you then go from eight
[00:18:54.320 --> 00:18:59.620]   months doubling time, say, for effective compute from software innovations, things like inventing
[00:18:59.620 --> 00:19:03.840]   the transformer or discovering chinchilla scaling and doing your training runs more
[00:19:03.840 --> 00:19:06.940]   optimally or creating flash attention.
[00:19:06.940 --> 00:19:12.620]   If you move that from, say, eight months to four months, and then the next time you apply
[00:19:12.620 --> 00:19:16.980]   that it significantly increases the boost you're getting from the AI.
[00:19:16.980 --> 00:19:22.820]   Maybe instead of giving a 50% or 100% productivity boost, now it's more like a 200%.
[00:19:22.820 --> 00:19:29.160]   And so it doesn't have to have been able to automate everything involved in the process
[00:19:29.160 --> 00:19:30.400]   of AI research.
[00:19:30.400 --> 00:19:36.740]   It can be it's automated a bunch of things and then those are being done in extreme profusion
[00:19:36.740 --> 00:19:43.580]   because a thing that AI can do, you have it done much more often because it's so cheap.
[00:19:43.580 --> 00:19:47.940]   And so it's not a threshold of this is human level AI.
[00:19:47.940 --> 00:19:52.980]   It can do everything a human can do with no weaknesses in any area.
[00:19:52.980 --> 00:20:00.260]   It's that even with its weaknesses, it's able to bump up the performance so that instead
[00:20:00.260 --> 00:20:04.520]   of getting like the results we would have with the 10,000 people working on finding
[00:20:04.520 --> 00:20:09.580]   these innovations, we get the results that we would have if we had twice as many of those
[00:20:09.580 --> 00:20:12.800]   people with the same kind of skill distribution.
[00:20:12.800 --> 00:20:15.580]   And so that's a, it's like a demanding challenge.
[00:20:15.580 --> 00:20:19.140]   It's like you need quite a, quite a lot of capability for that.
[00:20:19.140 --> 00:20:24.380]   But it's also important that it's significantly less than this is a system where there's no
[00:20:24.380 --> 00:20:30.340]   way you can point at it and say in any respect, it is weaker than a human, a system that was
[00:20:30.340 --> 00:20:35.880]   just as good as a human in every respect, but also had all of the advantages of an AI.
[00:20:35.880 --> 00:20:38.740]   That is just way beyond this point.
[00:20:38.740 --> 00:20:46.080]   Like if you consider that there's like the, the output of our existing fabs make tens
[00:20:46.080 --> 00:20:52.980]   of millions of advanced GPUs per year, those GPUs, if they were running sort of AI software
[00:20:52.980 --> 00:20:58.280]   that was as efficient as humans, as sample efficient, it doesn't have any major weaknesses.
[00:20:58.280 --> 00:21:05.460]   So they can work four times as long, you know, the 168 hour work week, they can have much
[00:21:05.460 --> 00:21:07.860]   more education than any human.
[00:21:07.860 --> 00:21:14.900]   So it's, you know, human, you know, you got a PhD, you know, it's like, yeah, wow, it's
[00:21:14.900 --> 00:21:21.960]   like 20 years of education, maybe longer if they take a slow route on the PhD.
[00:21:21.960 --> 00:21:28.060]   It's just normal for us to train large models by eat the internet, eat all the published
[00:21:28.060 --> 00:21:35.900]   books ever, read everything on GitHub and get good at predicting it.
[00:21:35.900 --> 00:21:41.140]   So like the level of education vastly beyond any human, the degree to which the models
[00:21:41.140 --> 00:21:47.220]   are focused on task is higher than all, but like the most motivated humans when they're
[00:21:47.220 --> 00:21:49.500]   really, really gunning for it.
[00:21:49.500 --> 00:21:57.300]   So you combine the things, tens of millions of GPUs, each GPU is doing the work of the
[00:21:57.300 --> 00:21:59.660]   very best humans in the world.
[00:21:59.660 --> 00:22:05.280]   And like the most capable humans in the world can command salaries that are a lot higher
[00:22:05.280 --> 00:22:12.780]   than the average, in particular in a field like STEM or narrowly AI, like there's no
[00:22:12.780 --> 00:22:17.780]   human in the world who has a thousand years of experience with TensorFlow or let alone
[00:22:17.780 --> 00:22:22.020]   the new AI technologies that were invented the year before.
[00:22:22.020 --> 00:22:27.540]   But if they were around, yeah, they'd be paid millions of dollars a year.
[00:22:27.540 --> 00:22:34.420]   And so when you consider this, okay, tens of millions of GPUs, each is doing the work
[00:22:34.420 --> 00:22:42.700]   of maybe 40, maybe more of these kind of existing workers is like going from a workforce of
[00:22:42.700 --> 00:22:47.140]   tens of thousands to hundreds of millions.
[00:22:47.140 --> 00:22:51.900]   You immediately make all kinds of discoveries then you immediately develop all sorts of
[00:22:51.900 --> 00:22:53.600]   tremendous technologies.
[00:22:53.600 --> 00:23:00.300]   So human level AI is deep, deep into an intelligence explosion.
[00:23:00.300 --> 00:23:03.660]   Intelligence explosion has to start with something weaker than that.
[00:23:03.660 --> 00:23:04.660]   Yeah.
[00:23:04.660 --> 00:23:07.300]   What is the thing it starts with and how close are we to that?
[00:23:07.300 --> 00:23:12.100]   Because if you think of a researcher at open AI or something, you know, these are to be
[00:23:12.100 --> 00:23:18.420]   a researcher is not just completing the hello world, a prompt that co-pilot does, right?
[00:23:18.420 --> 00:23:21.780]   It's like you got to choose a new idea, you had to figure out the right way to approach
[00:23:21.780 --> 00:23:22.780]   it.
[00:23:22.780 --> 00:23:25.220]   You perhaps have to manage the people who are also working with you on that problem.
[00:23:25.220 --> 00:23:30.140]   It, you know, it's like, it's, it's incredibly complicated skill portfolio skills rather
[00:23:30.140 --> 00:23:31.340]   than a single skill.
[00:23:31.340 --> 00:23:37.340]   So yeah, what is the point of where should that feedback loop starts where you can even,
[00:23:37.340 --> 00:23:41.660]   you're not just doing the 0.5% increase in productivity that a sort of AI tool might
[00:23:41.660 --> 00:23:45.860]   do, but is actually the equivalent of a researcher or close to it.
[00:23:45.860 --> 00:23:47.460]   Like what, what is that point?
[00:23:47.460 --> 00:23:52.260]   So I think maybe a way to look at it is to give some illustrative examples of like the
[00:23:52.260 --> 00:23:55.160]   kinds of capabilities that you might see.
[00:23:55.160 --> 00:24:01.300]   And so because these systems have to be a lot weaker than they sort of human level things.
[00:24:01.300 --> 00:24:08.060]   What we'll have is intense application of the ways in which AIs have advantages, partly
[00:24:08.060 --> 00:24:09.980]   offsetting their weaknesses.
[00:24:09.980 --> 00:24:12.340]   And so AIs are cheap.
[00:24:12.340 --> 00:24:17.740]   We can call a lot of them to do many small problems.
[00:24:17.740 --> 00:24:24.380]   And so you'll have situations where you have dumber AIs that are deployed thousands of
[00:24:24.380 --> 00:24:30.540]   times to equal say one human worker.
[00:24:30.540 --> 00:24:38.020]   And they'll be doing things like these voting algorithms where you with a, an LLM, you generate
[00:24:38.020 --> 00:24:41.980]   a bunch of different responses and take a majority vote among them.
[00:24:41.980 --> 00:24:48.980]   That improves performance some you'll have things like the AlphaGo kind of approach where
[00:24:48.980 --> 00:24:52.620]   you use the neural net to do search.
[00:24:52.620 --> 00:24:57.880]   And you go deeper with the search by plowing in more compute, which helps to offset the
[00:24:57.880 --> 00:25:02.660]   inefficiency and weaknesses of the model on its own.
[00:25:02.660 --> 00:25:07.900]   You'll do things that would just be totally impractical for humans because of the sheer
[00:25:07.900 --> 00:25:09.340]   number of steps.
[00:25:09.340 --> 00:25:13.740]   And so an example of that would be designing synthetic training data.
[00:25:13.740 --> 00:25:20.420]   So humans do not learn by just going into the library and opening books at random pages.
[00:25:20.420 --> 00:25:26.500]   It's actually much, much more efficient to have things like schools and classes where
[00:25:26.500 --> 00:25:29.380]   they teach you things in an order that makes sense.
[00:25:29.380 --> 00:25:33.220]   That's focusing on the skills that are more valuable to learn.
[00:25:33.220 --> 00:25:37.300]   They give you tests and exams that are designed to try and elicit the skill they're actually
[00:25:37.300 --> 00:25:39.580]   trying to teach.
[00:25:39.580 --> 00:25:45.260]   And right now we don't bother with that because we can hoover up more data from the internet.
[00:25:45.260 --> 00:25:49.980]   We're getting towards the end of that, but yeah, as the AIs get more sophisticated, they'll
[00:25:49.980 --> 00:25:57.300]   be better able to tell what is a useful kind of skill to practice and to generate that.
[00:25:57.300 --> 00:25:58.620]   And we've done that in other areas.
[00:25:58.620 --> 00:26:06.100]   So AlphaGo, the original version of AlphaGo was booted up with data from human GoPlay
[00:26:06.100 --> 00:26:11.860]   and then improved with reinforcement learning and Monte Carlo Tree Search.
[00:26:11.860 --> 00:26:19.540]   But then AlphaZero, with a somewhat more sophisticated model, benefited from some other improvements.
[00:26:19.540 --> 00:26:23.620]   But was able to go from scratch.
[00:26:23.620 --> 00:26:28.340]   And it generated its own data through self-play.
[00:26:28.340 --> 00:26:33.100]   So getting data of a higher quality than the human data, because there are no human players
[00:26:33.100 --> 00:26:36.300]   that good available in the dataset.
[00:26:36.300 --> 00:26:42.500]   And also a curriculum so that at any given point, it was playing games against an opponent
[00:26:42.500 --> 00:26:45.180]   of equal skill itself.
[00:26:45.180 --> 00:26:48.580]   And so it was always in an area when it was easy to learn.
[00:26:48.580 --> 00:26:52.420]   If you're just always losing, no matter what you do, or always winning, no matter what
[00:26:52.420 --> 00:26:57.500]   you do, it's hard to distinguish which things are better and which are worse.
[00:26:57.500 --> 00:27:03.500]   And when we have somewhat more sophisticated AIs that can generate training data and tasks
[00:27:03.500 --> 00:27:09.180]   for themselves, for example, if the AI can generate a lot of unit tests and then can
[00:27:09.180 --> 00:27:15.380]   try and produce programs that pass those unit tests, then the interpreter is providing a
[00:27:15.380 --> 00:27:17.140]   training signal.
[00:27:17.140 --> 00:27:22.180]   And the AI can get good at figuring out what's the kind of programming problem that is hard
[00:27:22.180 --> 00:27:29.460]   for AIs right now that will develop more of the skills that I need, and then do them.
[00:27:29.460 --> 00:27:34.940]   And now you're not going to have employees at OpenAI write a billion programming problems.
[00:27:34.940 --> 00:27:36.380]   That's just not going to happen.
[00:27:36.380 --> 00:27:43.540]   But you are going to have AIs given the task of producing those enormous number of programming
[00:27:43.540 --> 00:27:44.540]   challenges.
[00:27:44.540 --> 00:27:48.700]   In LLMs themselves, there's a paper out of Anthropa called Constitution AI or Constitution
[00:27:48.700 --> 00:27:53.100]   RL, where they basically had the program just talk to itself and say, "Is this response
[00:27:53.100 --> 00:27:54.100]   helpful?
[00:27:54.100 --> 00:27:55.940]   If not, how can I make this more helpful?"
[00:27:55.940 --> 00:27:57.660]   And the response is improved.
[00:27:57.660 --> 00:28:01.620]   And then you train the model on the more helpful responses that it generates by talking to
[00:28:01.620 --> 00:28:04.220]   itself so that it generates them natively.
[00:28:04.220 --> 00:28:09.100]   And you could imagine a more sophisticated way to do that or a better way to do that.
[00:28:09.100 --> 00:28:13.780]   But then the question is, listen, you know, GPT-4 already costs like 50 million or a hundred
[00:28:13.780 --> 00:28:15.580]   million or whatever it was.
[00:28:15.580 --> 00:28:20.660]   Even if we have greater effective compute from hardware increases and better models,
[00:28:20.660 --> 00:28:26.300]   it's hard to imagine how we could sustain like four or five more orders of magnitude,
[00:28:26.300 --> 00:28:32.180]   greater size, effective size than GPT-4, unless we're jumping in like trillions of dollars,
[00:28:32.180 --> 00:28:37.060]   like the entire economies of big countries into training the next version.
[00:28:37.060 --> 00:28:43.500]   So the question is, do we get something that can significantly help with AI progress before
[00:28:43.500 --> 00:28:50.740]   we run out of the sheer money and scale and compute that would require to train it?
[00:28:50.740 --> 00:28:51.740]   Do you have a take on that?
[00:28:51.740 --> 00:28:56.420]   Well, first I'd say, remember that there are these three contributing trends.
[00:28:56.420 --> 00:29:00.980]   So the new H100s are significantly better than the A100s.
[00:29:00.980 --> 00:29:07.040]   And a lot of companies are actually just waiting for their deliveries of H100s to do even bigger
[00:29:07.040 --> 00:29:13.260]   training runs, all along with the work of hooking them up into clusters and engineering
[00:29:13.260 --> 00:29:14.260]   the thing.
[00:29:14.260 --> 00:29:15.260]   Yeah.
[00:29:15.260 --> 00:29:16.980]   So all of those factors are contributing.
[00:29:16.980 --> 00:29:24.020]   And of course, mathematically, if you do four orders of magnitude more than 50 or a hundred
[00:29:24.020 --> 00:29:28.100]   million, then you're getting to trillion dollar territory.
[00:29:28.100 --> 00:29:35.740]   And yeah, I think the way to look at it is at each step along the way, does it look like
[00:29:35.740 --> 00:29:39.440]   it makes sense to do the next step?
[00:29:39.440 --> 00:29:46.140]   And so from where we are right now, seeing the results with GPT-4 and Chet-GPT, companies
[00:29:46.140 --> 00:29:54.040]   like Google and Microsoft and whatnot are pretty convinced that this is very valuable.
[00:29:54.040 --> 00:30:01.580]   You have talk at Google and Microsoft with Bing that, well, it's like billion dollar
[00:30:01.580 --> 00:30:06.780]   matter to change market share in search by a percentage point.
[00:30:06.780 --> 00:30:09.660]   And so that can fund a lot.
[00:30:09.660 --> 00:30:16.900]   And on the far end, on the extreme, if you automate human labor, we have a hundred trillion
[00:30:16.900 --> 00:30:18.460]   dollar economy.
[00:30:18.460 --> 00:30:21.500]   Most of that economy is paid out in wages.
[00:30:21.500 --> 00:30:27.100]   So like between 50 and 70 trillion dollars per year.
[00:30:27.100 --> 00:30:35.260]   If you create AGI, it's going to automate all of that and keep increasing beyond that.
[00:30:35.260 --> 00:30:42.380]   So the value of the completed project is very much worth throwing our whole economy into
[00:30:42.380 --> 00:30:47.540]   it if you're going to get the good version, not the catastrophic destruction of the human
[00:30:47.540 --> 00:30:53.820]   race or some other disastrous outcome.
[00:30:53.820 --> 00:31:00.620]   And in between, it's a question of, well, the next step, how risky and uncertain is
[00:31:00.620 --> 00:31:06.980]   it and how much growth in the revenue you can generate with it do you get?
[00:31:06.980 --> 00:31:11.820]   And so for moving up to a billion dollars, I think that's absolutely going to happen.
[00:31:11.820 --> 00:31:15.780]   These large tech companies have R&D budgets, tens of billions of dollars.
[00:31:15.780 --> 00:31:21.660]   And when you think about it in the relevant sense, all the employees at Microsoft who
[00:31:21.660 --> 00:31:25.780]   are doing software engineering, that's like contributing to creating software objects.
[00:31:25.780 --> 00:31:35.220]   It's not weird to spend tens of billions of dollars on a product that would do so much.
[00:31:35.220 --> 00:31:40.340]   And I think it's becoming more clear that there is sort of market opportunity to fund
[00:31:40.340 --> 00:31:41.620]   the thing.
[00:31:41.620 --> 00:31:46.820]   Going up to a hundred billion dollars, that's like, okay, the existing R&D budgets spread
[00:31:46.820 --> 00:31:49.100]   over multiple years.
[00:31:49.100 --> 00:31:54.100]   But if you keep seeing that when you scale up the model, it substantially improves the
[00:31:54.100 --> 00:31:55.100]   performance.
[00:31:55.100 --> 00:31:56.100]   It opens up new applications.
[00:31:56.100 --> 00:32:02.160]   You're not just improving your search, but maybe it makes self-driving cars work.
[00:32:02.160 --> 00:32:08.540]   You replace bulk software engineering jobs, or if not replace them, amplify productivity.
[00:32:08.540 --> 00:32:12.380]   In this kind of dynamic, you actually probably want to employ all the software engineers
[00:32:12.380 --> 00:32:17.220]   you can get as long as they're able to make any contribution because the returns of improving
[00:32:17.220 --> 00:32:19.940]   stuff in AI itself get so high.
[00:32:19.940 --> 00:32:25.300]   But yeah, I think that can go up to a hundred billion.
[00:32:25.300 --> 00:32:32.940]   And at a hundred billion, you're using a significant fraction of our existing fab capacity.
[00:32:32.940 --> 00:32:41.820]   Right now, the revenue of NVIDIA is like $25 billion, the revenue of TSMC, I believe it's
[00:32:41.820 --> 00:32:43.340]   over $50 billion.
[00:32:43.340 --> 00:32:54.860]   Last I checked, in 2021, NVIDIA was maybe 7.5%, less than 10% of TSMC revenue.
[00:32:54.860 --> 00:32:56.860]   So there's a lot of room.
[00:32:56.860 --> 00:32:59.100]   And most of that was not AI chips.
[00:32:59.100 --> 00:33:01.580]   They have a large gaming segment.
[00:33:01.580 --> 00:33:06.740]   There are data center GPUs that are used for video and the like.
[00:33:06.740 --> 00:33:15.700]   So there's room for more than an order of magnitude increase by redirecting existing
[00:33:15.700 --> 00:33:21.260]   fabs to produce more AI chips and then just actually using the AI chips that these companies
[00:33:21.260 --> 00:33:24.540]   have in their cloud for the big training runs.
[00:33:24.540 --> 00:33:28.700]   And so I think that's enough to go to the $10 billion and then combined with stuff like
[00:33:28.700 --> 00:33:30.860]   the H100 to go up to $100 billion.
[00:33:30.860 --> 00:33:33.900]   Just to emphasize for the audience, the initial point about revenue made.
[00:33:33.900 --> 00:33:41.860]   If it costs OpenAI $100 million to train GPT-4 and it generates $500 million in revenue,
[00:33:41.860 --> 00:33:45.180]   you pay back your expenses with $100 million, you have $400 million for your next training
[00:33:45.180 --> 00:33:46.180]   run.
[00:33:46.180 --> 00:33:51.900]   Then you train your GPT-4.5, you get, let's say $4 billion out of revenue out of that.
[00:33:51.900 --> 00:33:55.580]   That's where the feedback group of revenue comes from, where you're automating tasks
[00:33:55.580 --> 00:33:57.220]   and therefore you're making money.
[00:33:57.220 --> 00:34:00.500]   You can use that money to automate more tasks.
[00:34:00.500 --> 00:34:06.860]   Then the ability to redirect the fab production towards AI chips.
[00:34:06.860 --> 00:34:11.940]   So then the TLDR on, you want $100 billion worth of compute.
[00:34:11.940 --> 00:34:16.220]   I mean, fabs take, what, like a decade or so to build.
[00:34:16.220 --> 00:34:18.900]   So given the ones we have now and the ones that are going to come online in the next
[00:34:18.900 --> 00:34:25.100]   decade, is there enough to sustain $100 billion of GPU compute if you wanted to spend that
[00:34:25.100 --> 00:34:26.580]   on a training run?
[00:34:26.580 --> 00:34:27.580]   Yes.
[00:34:27.580 --> 00:34:29.740]   You could definitely make the $100 billion one.
[00:34:29.740 --> 00:34:33.700]   You go up to a trillion dollar run and larger.
[00:34:33.700 --> 00:34:39.940]   It's going to involve more fab construction and yeah, fabs can take a long time to build.
[00:34:39.940 --> 00:34:48.820]   On the other hand, if in fact you're getting very high revenue from the AI systems and
[00:34:48.820 --> 00:34:55.780]   you're actually bottlenecked on the construction of these fabs, then their price could skyrocket.
[00:34:55.780 --> 00:35:03.860]   And that would lead to measures we've never seen before to expand and accelerate fab production.
[00:35:03.860 --> 00:35:10.300]   If you consider, so at the limit, as you're getting models that approach human-like capability,
[00:35:10.300 --> 00:35:16.020]   if you imagine things that are getting close to brain-like efficiencies plus AI advantages.
[00:35:16.020 --> 00:35:22.500]   We were talking before about, well, a GPU that is supporting an AI, really it's a cluster
[00:35:22.500 --> 00:35:28.260]   of GPUs supporting AIs that do things in parallel, data parallelism.
[00:35:28.260 --> 00:35:35.100]   But if that can work four times as much as a human, a highly skilled, motivated, focused
[00:35:35.100 --> 00:35:40.220]   human with levels of education that have never been seen in the human population.
[00:35:40.220 --> 00:35:45.380]   And so if like a typical software engineer can earn hundreds of thousands of dollars,
[00:35:45.380 --> 00:35:50.860]   the world's best software engineers can earn millions of dollars today and maybe more in
[00:35:50.860 --> 00:35:55.060]   a world where there's so much demand for AI.
[00:35:55.060 --> 00:35:58.420]   And then times four for working all the time.
[00:35:58.420 --> 00:36:04.980]   Well, I mean, if you have, if you can generate like close to $10 million a year out of the
[00:36:04.980 --> 00:36:11.260]   future version of H100, that costs tens of thousands of dollars with a huge profit margin
[00:36:11.260 --> 00:36:12.260]   now.
[00:36:12.260 --> 00:36:18.180]   And profit margin could be reduced with like large production.
[00:36:18.180 --> 00:36:25.420]   That is a big difference, that chip pays for itself almost instantly.
[00:36:25.420 --> 00:36:33.420]   And so you could support paying 10 times as much to have these fabs constructed more rapidly.
[00:36:33.420 --> 00:36:40.180]   You could have, if AI is starting to be able to contribute, you could have AI contributing
[00:36:40.180 --> 00:36:46.180]   more of the skilled technical work that makes it hard for say NVIDIA to suddenly find thousands
[00:36:46.180 --> 00:36:51.540]   upon thousands of top quality engineering hires if AI can provide that.
[00:36:51.540 --> 00:36:56.940]   Now if AI hasn't reached that level of performance, then this is how you can have things stall
[00:36:56.940 --> 00:36:57.940]   out.
[00:36:57.940 --> 00:37:02.020]   And like a world where AI progress stalls out is one where you go to the hundred billion
[00:37:02.020 --> 00:37:13.020]   and then over succeeding years, trillion dollar things, software progress turns out to stall.
[00:37:13.020 --> 00:37:17.060]   You lose the gains that you were getting from moving researchers from other fields, lots
[00:37:17.060 --> 00:37:21.320]   of physicists and people from other areas of computer science have been going to AI,
[00:37:21.320 --> 00:37:27.180]   but you sort of tap out those resources as AI becomes a larger proportion of the research
[00:37:27.180 --> 00:37:28.180]   field.
[00:37:28.180 --> 00:37:33.260]   And like, okay, you've put in all of these inputs, but they just haven't yielded AI yet.
[00:37:33.260 --> 00:37:38.980]   I think that set of inputs probably would yield the kind of AI capabilities needed for
[00:37:38.980 --> 00:37:40.380]   intelligence explosion.
[00:37:40.380 --> 00:37:47.020]   But if it doesn't, after we've exhausted this current scale of increasing the share of our
[00:37:47.020 --> 00:37:52.340]   economy that is trying to make AI, if that's not enough, then after that you have to wait
[00:37:52.340 --> 00:37:57.660]   for the slow grind of things like general economic growth, population growth, and such.
[00:37:57.660 --> 00:37:59.100]   And so things slow.
[00:37:59.100 --> 00:38:04.440]   And that results in my credences and this kind of advanced AI happening to be relatively
[00:38:04.440 --> 00:38:10.200]   concentrated over the next 10 years compared to the rest of the century.
[00:38:10.200 --> 00:38:17.140]   So we can't keep going with this rapid redirection of resources into AI.
[00:38:17.140 --> 00:38:18.620]   That's a one-time thing.
[00:38:18.620 --> 00:38:21.420]   If the current scale of works, it's going to happen.
[00:38:21.420 --> 00:38:24.580]   We're going to get to AI really fast, like within the next 10 years or something.
[00:38:24.580 --> 00:38:29.860]   If the current scale of doesn't work, all we're left with is just like our economy growing
[00:38:29.860 --> 00:38:30.860]   like 2% a year.
[00:38:30.860 --> 00:38:34.420]   So we have like 2% a year more resources to spend on AI.
[00:38:34.420 --> 00:38:39.620]   And at that scale, you're talking about decades before you can adjust your sheer brute force
[00:38:39.620 --> 00:38:42.900]   so you can train that $10 trillion model or something.
[00:38:42.900 --> 00:38:48.300]   Let's talk about why you have your thesis that the current scale up would work.
[00:38:48.300 --> 00:38:52.760]   What is the evidence from AI itself, or maybe from primate evolution and the evolution of
[00:38:52.760 --> 00:38:53.760]   other animals?
[00:38:53.760 --> 00:38:56.300]   Just give me the whole confluence of reasons.
[00:38:56.300 --> 00:39:02.380]   I think maybe the best way to look at that might be to consider when I first became interested
[00:39:02.380 --> 00:39:08.020]   in this area, so in the 2000s, which was before the deep learning revolution, how would I
[00:39:08.020 --> 00:39:09.020]   think about timelines?
[00:39:09.020 --> 00:39:11.300]   How did I think about timelines?
[00:39:11.300 --> 00:39:16.700]   And then how have I updated based on what has been happening with deep learning?
[00:39:16.700 --> 00:39:24.540]   And so back then I would have said, we know the brain is a physical object, an information
[00:39:24.540 --> 00:39:26.100]   processing device.
[00:39:26.100 --> 00:39:28.460]   It works.
[00:39:28.460 --> 00:39:29.820]   It's possible.
[00:39:29.820 --> 00:39:35.260]   And not only is it possible, it was created by evolution on earth.
[00:39:35.260 --> 00:39:41.180]   And so that gives us something of an upper bound in that this kind of brute force was
[00:39:41.180 --> 00:39:42.180]   sufficient.
[00:39:42.180 --> 00:39:47.620]   There are some complexities with like, well, what if it was a freak accident and that didn't
[00:39:47.620 --> 00:39:50.540]   happen on all of the other planets and that added some value.
[00:39:50.540 --> 00:39:53.500]   I have a paper with Nick Bostrom on this.
[00:39:53.500 --> 00:39:57.860]   I think basically that's not that important an issue.
[00:39:57.860 --> 00:40:03.500]   There's convergent evolution, like octopi are also quite sophisticated.
[00:40:03.500 --> 00:40:10.940]   If a special event was at the level of forming cells at all or forming brains at all, we
[00:40:10.940 --> 00:40:15.100]   get to skip that because we're choosing to build computers and we already exist.
[00:40:15.100 --> 00:40:17.100]   We have that advantage.
[00:40:17.100 --> 00:40:22.660]   So say evolution gives something of an upper bound, really intensive, massive brute force
[00:40:22.660 --> 00:40:28.500]   search and things like evolutionary algorithms can produce intelligence.
[00:40:28.500 --> 00:40:33.500]   Given the fact that octopi and I guess other mammals, they got to the point of being like
[00:40:33.500 --> 00:40:36.260]   pretty intelligent, but not human level intelligent.
[00:40:36.260 --> 00:40:40.820]   Is that some evidence that there's a heart step between a cephalopod and a human?
[00:40:40.820 --> 00:40:41.820]   Yeah.
[00:40:41.820 --> 00:40:45.900]   So that would be a place to look.
[00:40:45.900 --> 00:40:48.500]   It doesn't seem particularly compelling.
[00:40:48.500 --> 00:40:53.740]   One source of evidence on that is work by Herculano Hutzel.
[00:40:53.740 --> 00:41:01.020]   I hope I haven't mispronounced her name, but she's a neuroscientist who has dissolved the
[00:41:01.020 --> 00:41:03.140]   brains of many creatures.
[00:41:03.140 --> 00:41:10.580]   And by counting the nuclei, she's able to determine how many neurons are present in
[00:41:10.580 --> 00:41:15.060]   different species and find a lot of interesting trends and scaling laws.
[00:41:15.060 --> 00:41:22.060]   And she has a paper discussing the human brain has a scaled up primate brain.
[00:41:22.060 --> 00:41:28.660]   And across a wide variety of animals and mammals in particular, there are certain characteristic
[00:41:28.660 --> 00:41:34.900]   changes in the relative number of neurons, size of different brain regions, how things
[00:41:34.900 --> 00:41:39.100]   scale up.
[00:41:39.100 --> 00:41:43.580]   There's a lot of structural similarity there.
[00:41:43.580 --> 00:41:51.020]   And you can explain a lot of what is different about us with a pretty brute force story,
[00:41:51.020 --> 00:41:58.420]   which is that you expend resources on having a bigger brain, keeping it in good order,
[00:41:58.420 --> 00:41:59.820]   giving it time to learn.
[00:41:59.820 --> 00:42:03.880]   So we have an unusually long childhood, unusually long neotenous period.
[00:42:03.880 --> 00:42:08.900]   We spend more compute by having a larger brain than other animals, more than three times
[00:42:08.900 --> 00:42:11.020]   as large as chimpanzees.
[00:42:11.020 --> 00:42:17.180]   And then we have a longer childhood than chimpanzees and much more than many, many other creatures.
[00:42:17.180 --> 00:42:21.740]   So we're spending more compute in a way that's analogous to like having a bigger model and
[00:42:21.740 --> 00:42:24.220]   having more training time with it.
[00:42:24.220 --> 00:42:31.820]   And given that we see with our AI models, this sort of like large, consistent benefits
[00:42:31.820 --> 00:42:38.580]   from increasing compute spent in those ways and with qualitatively new capabilities showing
[00:42:38.580 --> 00:42:44.500]   up over and over again, particularly in areas that sort of AI skeptics call out.
[00:42:44.500 --> 00:42:49.980]   In my experience, like over the last 15 years, the things that people call out is like, "Ah,
[00:42:49.980 --> 00:42:51.620]   but the AI can't do that."
[00:42:51.620 --> 00:42:53.460]   And it's because of a fundamental limitation.
[00:42:53.460 --> 00:42:58.780]   I've gone through a lot of them, you know, there were Winograd schemas, catastrophic
[00:42:58.780 --> 00:43:02.180]   forgetting, quite a number.
[00:43:02.180 --> 00:43:07.460]   And yeah, they have repeatedly gone away through scaling.
[00:43:07.460 --> 00:43:13.940]   And so there's a picture that we're seeing supported from biology and from our experience
[00:43:13.940 --> 00:43:20.980]   with AI where you can explain like, yeah, in general, there are trade-offs where the
[00:43:20.980 --> 00:43:25.540]   extra fitness you get from a brain is not worth it.
[00:43:25.540 --> 00:43:31.100]   And so creatures wind up mostly with small brains because they can save that biological
[00:43:31.100 --> 00:43:36.500]   energy and that time to reproduce for digestion and so on.
[00:43:36.500 --> 00:43:43.740]   And then humans, we actually seem to have wound up in a niche within self-reinforcing
[00:43:43.740 --> 00:43:51.500]   where we greatly increase the returns to having large brains and language and technology are
[00:43:51.500 --> 00:43:53.420]   the sort of obvious candidates.
[00:43:53.420 --> 00:43:58.420]   When you have humans around you who know a lot of things and they can teach you and compared
[00:43:58.420 --> 00:44:04.740]   to almost any other species, we have vastly more instruction from parents and the society
[00:44:04.740 --> 00:44:11.300]   of the young, then you're getting way more from your brain because you can get per minute,
[00:44:11.300 --> 00:44:15.940]   you can learn a lot more useful skills, and then you can provide the energy you need to
[00:44:15.940 --> 00:44:22.500]   feed that brain by hunting and gathering, by having fire that makes digestion easier.
[00:44:22.500 --> 00:44:28.380]   And basically how this process goes on, it's increasing the marginal increase in reproductive
[00:44:28.380 --> 00:44:34.380]   fitness you get from allocating more resources along a bunch of dimensions towards cognitive
[00:44:34.380 --> 00:44:35.380]   ability.
[00:44:35.380 --> 00:44:41.260]   And so that's bigger brains, longer childhood, having our attention be more on learning.
[00:44:41.260 --> 00:44:47.620]   So humans play a lot and we keep playing as adults, which is a very weird thing compared
[00:44:47.620 --> 00:44:49.300]   to other animals.
[00:44:49.300 --> 00:44:55.560]   We're more motivated to copy other humans around us than like even then the other primates.
[00:44:55.560 --> 00:45:00.660]   And so these are sort of motivational changes that keep us using more of our attention and
[00:45:00.660 --> 00:45:05.860]   effort on learning, which pays off more when you have a bigger brain and a longer lifespan
[00:45:05.860 --> 00:45:07.380]   in which to learn.
[00:45:07.380 --> 00:45:11.220]   Many creatures are subject to lots of predation or disease.
[00:45:11.220 --> 00:45:16.380]   And so if you try, you know, you're a mayfly or a mouse, if you try and invest in like
[00:45:16.380 --> 00:45:22.860]   a giant brain and a very long childhood, you're quite likely to be killed by some predator
[00:45:22.860 --> 00:45:25.820]   or some disease before you're able to actually use it.
[00:45:25.820 --> 00:45:30.300]   And so that means you actually have exponentially increasing costs in a given niche.
[00:45:30.300 --> 00:45:36.180]   So if I have a 50% chance of dying every few months of a, you know, a little mammal or
[00:45:36.180 --> 00:45:41.900]   a little lizard or something, that means the cost of going from three months to 30 months
[00:45:41.900 --> 00:45:48.740]   of learning and childhood development, it's not 10 times the loss, it's now it's two to
[00:45:48.740 --> 00:45:56.980]   the negative 10, so a factor of 1024 reduction in the benefit I get from what I ultimately
[00:45:56.980 --> 00:46:02.500]   learn because 99.9% of the animals will have been killed before that point.
[00:46:02.500 --> 00:46:07.980]   We're in a niche where we're like a large, long lived animal with language and technology,
[00:46:07.980 --> 00:46:10.620]   so where we can learn a lot from our groups.
[00:46:10.620 --> 00:46:17.820]   And that means it pays off to really just expand our investment on these multiple fronts
[00:46:17.820 --> 00:46:19.980]   in intelligence.
[00:46:19.980 --> 00:46:20.980]   That's so interesting.
[00:46:20.980 --> 00:46:26.060]   Just for the audience, the calculation about like two to the whatever months is just like
[00:46:26.060 --> 00:46:29.940]   you have a half chance of dying this month, a half chance of dying next month, you multiply
[00:46:29.940 --> 00:46:30.940]   those together.
[00:46:30.940 --> 00:46:31.940]   Okay.
[00:46:31.940 --> 00:46:37.540]   There's other species though that do live in flocks or as packs where you could imagine,
[00:46:37.540 --> 00:46:43.260]   I mean, they do have like a smaller version of the development of cubs into that I like
[00:46:43.260 --> 00:46:44.900]   play with each other.
[00:46:44.900 --> 00:46:52.500]   Why isn't this a hill on which they could have climbed to human level intelligence themselves?
[00:46:52.500 --> 00:46:57.780]   If it's something like language or technology, humans were getting smarter before we got
[00:46:57.780 --> 00:46:58.780]   language.
[00:46:58.780 --> 00:47:00.460]   I mean, obviously we had to get smarter to get language, right?
[00:47:00.460 --> 00:47:02.620]   We couldn't just get language without becoming smarter.
[00:47:02.620 --> 00:47:07.940]   So yeah, where did it seems like there should be other species that should have beginnings
[00:47:07.940 --> 00:47:12.780]   of this sort of cognitive revolution, especially given how valuable it is given, listen, we've
[00:47:12.780 --> 00:47:16.620]   dominated the world, you would think there'd be selective pressure for it.
[00:47:16.620 --> 00:47:18.500]   Evolution doesn't have foresight.
[00:47:18.500 --> 00:47:25.140]   The thing in this generation that gets more surviving offspring and grandchildren, that's
[00:47:25.140 --> 00:47:27.620]   the thing that becomes more common.
[00:47:27.620 --> 00:47:33.340]   Evolution doesn't look ahead and think, oh, in a million years, you'll have a lot of descendants.
[00:47:33.340 --> 00:47:36.900]   It's what survives and reproduces now.
[00:47:36.900 --> 00:47:45.900]   And so, in fact, there are correlations where social animals do on average have larger brains.
[00:47:45.900 --> 00:47:51.800]   And part of that is probably that the additional social applications of brains, like keeping
[00:47:51.800 --> 00:47:57.380]   track of which of your group members have helped you before so that you can reciprocate,
[00:47:57.380 --> 00:48:02.540]   you scratch my back, I'll scratch yours, remembering who's dangerous within the group, that sort
[00:48:02.540 --> 00:48:07.180]   of thing, is an additional application of intelligence.
[00:48:07.180 --> 00:48:13.420]   And so there's some correlation there, but what it seems like is that, yeah, in most
[00:48:13.420 --> 00:48:21.420]   of these cases, it's enough to invest more, but not invest to the point where a mind can
[00:48:21.420 --> 00:48:25.060]   easily develop language and technology and pass it on.
[00:48:25.060 --> 00:48:30.500]   And so there are, you see bits of tool use in some other primates who have an advantage
[00:48:30.500 --> 00:48:35.220]   that so compared to say the whales who have, they have quite large brains, partly because
[00:48:35.220 --> 00:48:40.380]   they are so large themselves and they have some other thing, but they don't have hands,
[00:48:40.380 --> 00:48:46.060]   which means that reduces a bunch of ways in which brains can pay off and investments in
[00:48:46.060 --> 00:48:47.940]   the functioning of that brain.
[00:48:47.940 --> 00:48:55.740]   But yeah, so primates will use sticks to extract termites, capuchin monkeys will open clams
[00:48:55.740 --> 00:49:02.300]   by smashing them with a rock, so there's bits of tool use, but what they don't have is the
[00:49:02.300 --> 00:49:04.780]   ability to sustain culture.
[00:49:04.780 --> 00:49:09.820]   A particular primate will maybe discover one of these tactics and maybe it'll be copied
[00:49:09.820 --> 00:49:14.220]   by their immediate group, but they're not holding onto it that well.
[00:49:14.220 --> 00:49:19.140]   They're like, well, when they see the other animal do it, they can copy it in that situation.
[00:49:19.140 --> 00:49:23.980]   They don't actively teach each other, their population locally is quite small, so it's
[00:49:23.980 --> 00:49:28.620]   easy to forget things, easy to lose information.
[00:49:28.620 --> 00:49:33.860]   And in fact, they remain technologically stagnant for hundreds of thousands of years.
[00:49:33.860 --> 00:49:37.380]   And we can actually look at some human situations.
[00:49:37.380 --> 00:49:45.780]   So there's an old paper, I believe by the economist Michael Kramer, talks about technological
[00:49:45.780 --> 00:49:50.920]   growth in the different continents for human societies.
[00:49:50.920 --> 00:49:57.420]   And so you have Eurasia is the largest integrated connected area, Africa is partly connected
[00:49:57.420 --> 00:50:03.580]   to it, but the Sahara desert restricts the flow of information and technology and such.
[00:50:03.580 --> 00:50:07.180]   And then you had the Americas, which were after the colonization from the land bridge,
[00:50:07.180 --> 00:50:12.020]   were largely separated and are smaller than Eurasia, then Australia.
[00:50:12.020 --> 00:50:15.720]   And then you had like smaller island situations like Tasmania.
[00:50:15.720 --> 00:50:21.300]   And so technological progress seems to have been faster, the larger the connected group
[00:50:21.300 --> 00:50:23.380]   of people.
[00:50:23.380 --> 00:50:27.840]   And in the smallest groups, so like in Tasmania, you had a relatively small population, and
[00:50:27.840 --> 00:50:30.420]   they actually lost technology.
[00:50:30.420 --> 00:50:34.200]   So things like they lost some like fishing techniques.
[00:50:34.200 --> 00:50:38.660]   And if you have a small population, and you have some limited number of people who know
[00:50:38.660 --> 00:50:46.060]   a skill, and they happen to die or happen, there's like, you know, some change in circumstances
[00:50:46.060 --> 00:50:50.020]   that causes people not to practice or pass on that thing.
[00:50:50.020 --> 00:50:51.180]   And then you lose it.
[00:50:51.180 --> 00:50:57.760]   And if you have few people, you're doing less innovation, the rate at which you lose technologies
[00:50:57.760 --> 00:51:02.040]   to some kind of local disturbance, and the rate at which you create new technologies
[00:51:02.040 --> 00:51:04.020]   can wind up in balance.
[00:51:04.020 --> 00:51:10.580]   And the great change of hominids and humanity is that we wound up in a situation where we
[00:51:10.580 --> 00:51:13.440]   were accumulating faster than we were losing.
[00:51:13.440 --> 00:51:18.480]   And as we accumulated, those technologies allowed us to expand our population, they
[00:51:18.480 --> 00:51:24.200]   created additional demand for intelligence, so that our brains became three times as large
[00:51:24.200 --> 00:51:25.200]   as chimpanzees.
[00:51:25.200 --> 00:51:26.200]   As chimpanzees.
[00:51:26.200 --> 00:51:27.200]   Yeah.
[00:51:27.200 --> 00:51:29.760]   And our ancestors who had a similar brain size.
[00:51:29.760 --> 00:51:36.220]   Okay, and then the crucial point, I guess, in relevance to AI, is that the selective
[00:51:36.220 --> 00:51:42.940]   pressures against intelligence in other animals are not acting against these neural networks,
[00:51:42.940 --> 00:51:45.800]   because we are, you know, we're not going to get like eaten by a predator, if they spend
[00:51:45.800 --> 00:51:48.600]   too much time becoming more intelligent.
[00:51:48.600 --> 00:51:51.200]   We're like explicitly training them to become more intelligent.
[00:51:51.200 --> 00:51:55.340]   So we have like good first principles reason to think that if it was scaling that made
[00:51:55.340 --> 00:52:01.060]   our minds this powerful, and if the things that prevented other animals from scaling
[00:52:01.060 --> 00:52:05.340]   are not impinging on these neural networks, that these things should just continue to
[00:52:05.340 --> 00:52:06.340]   become very smart.
[00:52:06.340 --> 00:52:10.740]   Yeah, we are we're growing them in a technological culture where there are jobs like software
[00:52:10.740 --> 00:52:18.700]   engineer that depend much more on sort of cognitive output and less on things like metabolic
[00:52:18.700 --> 00:52:25.820]   resources devoted to the immune system, or to like building big muscles to throw spears.
[00:52:25.820 --> 00:52:27.580]   This is kind of a side note, but I'm just kind of interested.
[00:52:27.580 --> 00:52:30.780]   I think you have referenced at some point, gentler scaling for the audience.
[00:52:30.780 --> 00:52:35.180]   This is a paper from DeepMind, which describes if you have a model of a certain size, what
[00:52:35.180 --> 00:52:38.840]   is the optimum amount of data that it should be trained on?
[00:52:38.840 --> 00:52:42.900]   So you can imagine bigger models, you can, you can use more data to train them.
[00:52:42.900 --> 00:52:45.620]   And in this way, you can figure out where should you spend your computer to spend it
[00:52:45.620 --> 00:52:49.060]   on making the model bigger, or should you spend it on training it for longer?
[00:52:49.060 --> 00:52:54.940]   I'm curious if in the case of different animals, in some sense, they're like model sizes, or
[00:52:54.940 --> 00:52:58.660]   how big their brain is, and their training data sizes, like how long their cubs or how
[00:52:58.660 --> 00:53:02.360]   long their infants or toddlers are before they're full adults.
[00:53:02.360 --> 00:53:04.140]   Is there some sort of like scaling law of?
[00:53:04.140 --> 00:53:09.700]   Yeah, I mean, so the the chinchilla scaling is interesting, because we were talking earlier
[00:53:09.700 --> 00:53:13.440]   about the cost function for having a longer childhood.
[00:53:13.440 --> 00:53:17.860]   And so where it's like exponentially increasing in the amount of training compute you have
[00:53:17.860 --> 00:53:21.660]   when you have exogenous forces that can kill you.
[00:53:21.660 --> 00:53:27.220]   Whereas when we do big training runs, the cost of throwing in more GPUs is almost linear.
[00:53:27.220 --> 00:53:30.180]   And it's much better to be, you know, linear than exponentially decaying.
[00:53:30.180 --> 00:53:31.780]   Oh, that's a really good point.
[00:53:31.780 --> 00:53:33.000]   As you expand resources.
[00:53:33.000 --> 00:53:38.720]   And so chinchilla scaling would suggest that like, yeah, the opt for a brain of sort of
[00:53:38.720 --> 00:53:44.060]   human size, it would be optimal to have many millions of years of education.
[00:53:44.060 --> 00:53:49.980]   But obviously, that's impractical because of exogenous mortality for humans.
[00:53:49.980 --> 00:53:56.940]   And so there's a fairly compelling argument that relative to the situation where we would
[00:53:56.940 --> 00:54:05.340]   train AI, that animals are systematically way undertrained, and now they're more efficient
[00:54:05.340 --> 00:54:06.340]   than our models.
[00:54:06.340 --> 00:54:11.160]   So we still have room to improve our algorithms to catch up with the efficiency of brains.
[00:54:11.160 --> 00:54:16.280]   But they are laboring under that disadvantage.
[00:54:16.280 --> 00:54:17.280]   That is so interesting.
[00:54:17.280 --> 00:54:23.620]   Okay, so I guess another question you could have is, humans got started on this evolutionary
[00:54:23.620 --> 00:54:28.780]   hill climbing route where we're getting more intelligent that has more benefits for us.
[00:54:28.780 --> 00:54:31.600]   Why didn't we go all the way on that route?
[00:54:31.600 --> 00:54:37.640]   If intelligence is so powerful, why aren't all humans as smart as we know humans can
[00:54:37.640 --> 00:54:38.640]   be?
[00:54:38.640 --> 00:54:39.640]   Right?
[00:54:39.640 --> 00:54:41.200]   At least that's smart.
[00:54:41.200 --> 00:54:44.640]   If intelligence is so powerful, like why hasn't there been stronger selective pressure?
[00:54:44.640 --> 00:54:47.920]   I understand like, oh, listen, hip size, you can't like give birth to a really big headed
[00:54:47.920 --> 00:54:48.920]   baby or whatever.
[00:54:48.920 --> 00:54:53.520]   But you would think like evolution would figure out some way to offset that such big if intelligence
[00:54:53.520 --> 00:54:55.520]   has such big power.
[00:54:55.520 --> 00:54:56.520]   And such is so useful.
[00:54:56.520 --> 00:55:01.160]   Yeah, I think if you actually look at it quantitatively, that's that's not true.
[00:55:01.160 --> 00:55:06.560]   And even in in sort of recent history, there has been it looks like a pretty close balance
[00:55:06.560 --> 00:55:12.920]   between the costs and the benefits of having more cognitive abilities.
[00:55:12.920 --> 00:55:19.080]   And so you say like, you know, who needs to worry about like, you know, the metabolic
[00:55:19.080 --> 00:55:26.560]   costs like you need humans put like order 20% of our metabolic energy into the brain
[00:55:26.560 --> 00:55:29.480]   and it's higher for like young children.
[00:55:29.480 --> 00:55:36.980]   So 20% of the energy and then there's like breathing and digestion and the immune system.
[00:55:36.980 --> 00:55:42.960]   And so for most of history, people have been dying left and right, like a very large proportion
[00:55:42.960 --> 00:55:46.640]   of people will die of infectious disease.
[00:55:46.640 --> 00:55:52.280]   And if you put more resources into your immune system, you survive.
[00:55:52.280 --> 00:55:57.920]   It's like life or death pretty directly via that mechanism.
[00:55:57.920 --> 00:56:03.360]   And then this is this related also when people die more of disease during famine.
[00:56:03.360 --> 00:56:04.520]   And so there's boom or bust.
[00:56:04.520 --> 00:56:11.160]   And so if you have 20% less metabolic requirements, or has a child, and it's you have a lot more
[00:56:11.160 --> 00:56:17.680]   is like 40 or 50% less metabolic requirements, you're much more likely to survive that famine.
[00:56:17.680 --> 00:56:20.800]   So these are these are these are pretty big.
[00:56:20.800 --> 00:56:24.160]   And then there's a trade off about just cleaning mutational load.
[00:56:24.160 --> 00:56:29.320]   So every generation, new mutations and errors happen in the process of reproduction.
[00:56:29.320 --> 00:56:36.480]   And so like we know there are many genetic abnormalities that occur through new mutations
[00:56:36.480 --> 00:56:37.560]   each generation.
[00:56:37.560 --> 00:56:43.960]   And in fact, we have Down syndrome is the chromosomal abnormality that you can survive.
[00:56:43.960 --> 00:56:47.760]   All the others just kill the embryo.
[00:56:47.760 --> 00:56:50.020]   And so we never see them.
[00:56:50.020 --> 00:56:52.400]   But like Down syndrome occurs a lot.
[00:56:52.400 --> 00:56:55.320]   And there are many other lethal mutations.
[00:56:55.320 --> 00:56:59.920]   And there's as you go to less damaging ones, there are enormous numbers of less damaging
[00:56:59.920 --> 00:57:03.840]   mutations that are degrading every system in the body.
[00:57:03.840 --> 00:57:11.320]   And so evolution, each generation has to pull away at some of this mutational load and the
[00:57:11.320 --> 00:57:17.080]   priority with which that mutational load is pulled out scales in proportion to how much
[00:57:17.080 --> 00:57:19.860]   the traits it's affecting impact fitness.
[00:57:19.860 --> 00:57:25.780]   So you've got new mutations that impact your resistance to malaria.
[00:57:25.780 --> 00:57:29.640]   You've got new mutations that damage brain function.
[00:57:29.640 --> 00:57:34.680]   And then those mutations are purged each generation.
[00:57:34.680 --> 00:57:39.600]   If malaria is a bigger difference in mortality than the incremental effectiveness as a hunter
[00:57:39.600 --> 00:57:45.700]   gatherer you get from being slightly more intelligent, then you'll purge that mutational
[00:57:45.700 --> 00:57:47.420]   load first.
[00:57:47.420 --> 00:57:53.560]   And similarly, humans have been vigorously adapting to new circumstances.
[00:57:53.560 --> 00:58:00.840]   So since agriculture, people have been developing things like the ability to have amylase to
[00:58:00.840 --> 00:58:06.040]   digest breads, the ability to digest milk.
[00:58:06.040 --> 00:58:10.360]   And if you're evolving for all of these things, and if some of the things that give an advantage
[00:58:10.360 --> 00:58:17.200]   for that incidentally carry along nearby them some negative effect on another trait, then
[00:58:17.200 --> 00:58:18.640]   that other trait can be damaged.
[00:58:18.640 --> 00:58:24.920]   So it really matters how important two survival and reproduction cognitive abilities were
[00:58:24.920 --> 00:58:27.600]   compared to everything else that organism has to do.
[00:58:27.600 --> 00:58:34.000]   And that in particular, like surviving feast and famine, having like the physical abilities
[00:58:34.000 --> 00:58:36.080]   to do hunting and gathering.
[00:58:36.080 --> 00:58:40.520]   And like, even if you're like very good at planning your hunting, being able to throw
[00:58:40.520 --> 00:58:43.360]   a spear harder can be a big difference.
[00:58:43.360 --> 00:58:48.080]   And that needs energy to build those muscles and then to sustain them.
[00:58:48.080 --> 00:58:57.360]   And so given all of these factors, it's like, yeah, it's not a slam dunk to invest at the
[00:58:57.360 --> 00:58:58.360]   margin.
[00:58:58.360 --> 00:59:03.480]   And like today, like having bigger brains, for example, it's associated with like greater
[00:59:03.480 --> 00:59:07.600]   cognitive ability, but it's like, it's modest.
[00:59:07.600 --> 00:59:15.240]   Large-scale pre-registered studies with MRI data, it's like a range, maybe like a correlation
[00:59:15.240 --> 00:59:18.380]   of 0.25, 0.3.
[00:59:18.380 --> 00:59:22.320]   And the standard deviation of brain size is like 10%.
[00:59:22.320 --> 00:59:28.320]   So if you double the size of the brain, so go and the existing brain costs like 20% of
[00:59:28.320 --> 00:59:30.720]   metabolic energy, go up to 40%.
[00:59:30.720 --> 00:59:31.720]   Okay.
[00:59:31.720 --> 00:59:35.120]   So it's like eight standard deviations of brain size.
[00:59:35.120 --> 00:59:44.960]   If the correlation is like, say it's 0.25, then yeah, like you can, you get a gain from
[00:59:44.960 --> 00:59:45.960]   that.
[00:59:45.960 --> 00:59:50.800]   Eight standard deviations of brain size, two standard deviations of cognitive ability.
[00:59:50.800 --> 00:59:57.600]   And like in our modern society where cognitive ability is very rewarded and like, you know,
[00:59:57.600 --> 01:00:05.520]   finishing school, becoming an engineer or a doctor or whatever can pay off a lot financially.
[01:00:05.520 --> 01:00:13.560]   Still the like the average observed return in like income is like a one or 2% proportional
[01:00:13.560 --> 01:00:14.560]   increase.
[01:00:14.560 --> 01:00:15.760]   There's more effects at the tail.
[01:00:15.760 --> 01:00:22.160]   There's more effect in professions like STEM, but on the whole, it's not like, you know,
[01:00:22.160 --> 01:00:27.320]   if it was like a 5% increase or a 10% increase, then you could, you could, you could tell
[01:00:27.320 --> 01:00:31.160]   a story where, yeah, this is hugely increasing the amount of food you could have.
[01:00:31.160 --> 01:00:35.320]   You could support more children, but it's like, it's a modest effect and the metabolic
[01:00:35.320 --> 01:00:38.920]   costs would be large and then throw in these other, these other aspects.
[01:00:38.920 --> 01:00:40.840]   And I think it's, you can tell the story.
[01:00:40.840 --> 01:00:47.520]   And also we can just, we can see there was not very strong, rapid directional selection
[01:00:47.520 --> 01:00:53.360]   on the thing, which there would be if like, you know, you could, by solving like a, by
[01:00:53.360 --> 01:00:59.920]   solving like a math puzzle, you could defeat malaria, like then, then there would be more
[01:00:59.920 --> 01:01:00.920]   evolutionary pressure.
[01:01:00.920 --> 01:01:01.920]   That is so interesting.
[01:01:01.920 --> 01:01:05.540]   And not to mention, of course, that yeah, if you had like two eggs of brain size, you're,
[01:01:05.540 --> 01:01:09.920]   you were without C-section, you would, you or your mother would, or both would die.
[01:01:09.920 --> 01:01:13.240]   This is a question I've actually been curious about for like over a year and I like briefly
[01:01:13.240 --> 01:01:14.560]   tried to look up an answer.
[01:01:14.560 --> 01:01:19.120]   This is, I know this is off topic, but I apologize to the audience, but I was super interested
[01:01:19.120 --> 01:01:20.120]   in it.
[01:01:20.120 --> 01:01:23.240]   So that was like the most comprehensive and interesting answer I could have hoped for.
[01:01:23.240 --> 01:01:24.240]   Okay.
[01:01:24.240 --> 01:01:27.600]   So yeah, we have a good explanation of good first principles, evolutionary reason for
[01:01:27.600 --> 01:01:35.400]   thinking that intelligence scaling up to humans is not implausible just by throwing more scale
[01:01:35.400 --> 01:01:36.400]   at it.
[01:01:36.400 --> 01:01:41.360]   I would also add, this was something that would have mattered to me more in the 2000s.
[01:01:41.360 --> 01:01:46.200]   We also have the brain right here with us for available for neuroscience to reverse
[01:01:46.200 --> 01:01:48.080]   engineer its properties.
[01:01:48.080 --> 01:01:52.640]   And so in the 2000s, when I said, yeah, I expect this by, you know, middle of the century
[01:01:52.640 --> 01:01:55.000]   ish, that was a backstop.
[01:01:55.000 --> 01:02:00.320]   If we found it absurdly difficult to get to the algorithms and then we would learn from
[01:02:00.320 --> 01:02:01.320]   neuroscience.
[01:02:01.320 --> 01:02:04.520]   But in the actual, the actual history, it's, it's really not like that.
[01:02:04.520 --> 01:02:09.760]   We develop things in AI and then also we can say, oh yeah, this is sort of like this thing
[01:02:09.760 --> 01:02:15.120]   in neuroscience, or maybe this is a good explanation, but it's not as though neuroscience is driving
[01:02:15.120 --> 01:02:16.120]   AI progress.
[01:02:16.120 --> 01:02:18.240]   It's, it's, it turns out not to be that necessary.
[01:02:18.240 --> 01:02:22.780]   That's similar to, I guess, you know, how the planes were inspired by the existence
[01:02:22.780 --> 01:02:27.080]   proof of birds, but jet engines don't flap.
[01:02:27.080 --> 01:02:28.080]   All right.
[01:02:28.080 --> 01:02:29.080]   So yeah.
[01:02:29.080 --> 01:02:32.160]   Scaling, a good reason to think scaling might work.
[01:02:32.160 --> 01:02:35.120]   So we, we spent a hundred billion dollars and we have something that is like human level
[01:02:35.120 --> 01:02:39.200]   or can do help significantly with AI research.
[01:02:39.200 --> 01:02:44.180]   I mean, that, that might be on the earlier end, but I mean, I w I definitely would not
[01:02:44.180 --> 01:02:49.080]   rule that out given the rates of change we've seen with the last few scale ups.
[01:02:49.080 --> 01:02:50.080]   All right.
[01:02:50.080 --> 01:02:53.160]   So at this point, somebody might be skeptical.
[01:02:53.160 --> 01:02:54.160]   Okay.
[01:02:54.160 --> 01:02:56.240]   Like, listen, we already have a bunch of human researchers, right?
[01:02:56.240 --> 01:02:58.560]   Like the incremental researcher, how, how powerful is that?
[01:02:58.560 --> 01:03:01.480]   And then you might say, well, no, this is like thousands of researchers.
[01:03:01.480 --> 01:03:06.040]   I don't know how to express a skepticism exactly, but skepticism is skeptical of just generally
[01:03:06.040 --> 01:03:12.240]   the effect of scaling up the number of people working on the problem to rapid, rapid progress
[01:03:12.240 --> 01:03:14.120]   on that problem.
[01:03:14.120 --> 01:03:17.980]   Somebody might think, okay, listen, with humans, the reason population working on a problem
[01:03:17.980 --> 01:03:22.260]   is such a good proxy for progress on the problem is that there's already so much variation
[01:03:22.260 --> 01:03:23.260]   that is accounted for.
[01:03:23.260 --> 01:03:25.700]   When you say there's like a million people working on a problem, you know, there's like
[01:03:25.700 --> 01:03:29.960]   a hundreds of super geniuses working on it, thousands of people who are like very smart
[01:03:29.960 --> 01:03:30.960]   working on it.
[01:03:30.960 --> 01:03:34.780]   Whereas with an AI, all the copies are like same level of intelligence.
[01:03:34.780 --> 01:03:42.220]   Um, and if it's not super genius intelligence, the, uh, the, the, the total quantity may
[01:03:42.220 --> 01:03:43.580]   might not matter as much.
[01:03:43.580 --> 01:03:48.440]   Yeah, I'm not sure what your, um, model is here.
[01:03:48.440 --> 01:03:56.480]   So it is the model that the diminishing returns kick off suddenly has a cliff right where
[01:03:56.480 --> 01:03:57.540]   we are.
[01:03:57.540 --> 01:04:04.060]   And so like there was, there were results in the past from throwing more people at problems.
[01:04:04.060 --> 01:04:10.740]   Um, and I mean, this has been useful in historical prediction, um, one of the, there's this idea
[01:04:10.740 --> 01:04:17.580]   of experience curves and Wright's law, um, basically measuring cumulative production
[01:04:17.580 --> 01:04:23.260]   in a field or which is also going to be a measure of like the scale of effort and investment.
[01:04:23.260 --> 01:04:28.340]   And people have used this correctly, uh, to argue that renewable energy technology like
[01:04:28.340 --> 01:04:35.220]   solar, uh, would be falling rapidly in price because it was going from a low base of very
[01:04:35.220 --> 01:04:41.060]   small production runs, not much investment in doing it efficiently.
[01:04:41.060 --> 01:04:46.700]   Um, and yeah, uh, climate advocates correctly called out, um, people and people like David
[01:04:46.700 --> 01:04:53.140]   Roberts, um, the futurist Rama is now, um, uh, actually has some, some interesting, uh,
[01:04:53.140 --> 01:04:58.660]   writing on this that yeah, correctly called out that there would be really drastic, uh,
[01:04:58.660 --> 01:05:04.020]   fall in prices of solar and batteries because of the increasing investment going into that,
[01:05:04.020 --> 01:05:05.820]   uh, the human genome project would be another.
[01:05:05.820 --> 01:05:08.980]   So I'd say there's like, yeah, real, real evidence.
[01:05:08.980 --> 01:05:14.300]   These observed correlations from like ideas getting harder to find, have, have held over
[01:05:14.300 --> 01:05:19.340]   a fair, uh, a fair range of data and over quite a lot of time.
[01:05:19.340 --> 01:05:25.340]   Uh, so I'm wondering what, what, what, what's the, yeah, the, the nature of the deviation
[01:05:25.340 --> 01:05:26.340]   you're thinking of?
[01:05:26.340 --> 01:05:31.260]   That, um, we're talking about, uh, maybe this is like a good way to describe what happens
[01:05:31.260 --> 01:05:36.660]   when more humans enter a field, but does it even make sense to say like a greater population
[01:05:36.660 --> 01:05:38.700]   of AI is doing AI research?
[01:05:38.700 --> 01:05:45.100]   If there's like more GPUs running a copy of GPD six doing AI research, it just like how,
[01:05:45.100 --> 01:05:49.140]   how applicable are these, uh, economic models of human, the quantity of humans working on
[01:05:49.140 --> 01:05:53.260]   a problem to the, to the magnitude of AI is working on a problem.
[01:05:53.260 --> 01:05:54.260]   Yeah.
[01:05:54.260 --> 01:05:59.540]   So if you have AI that are directly automating, uh, you know, particular jobs that humans
[01:05:59.540 --> 01:06:04.940]   were doing before, then we say, well, with additional compute, we can run more copies
[01:06:04.940 --> 01:06:07.940]   of them, uh, to do more of those tasks simultaneously.
[01:06:07.940 --> 01:06:11.620]   Uh, we can also run them at greater speed.
[01:06:11.620 --> 01:06:15.060]   And so some people have an intuition that like, well, you know, what matters is like
[01:06:15.060 --> 01:06:16.840]   time.
[01:06:16.840 --> 01:06:20.300]   It's not how many people working on problem at a given point.
[01:06:20.300 --> 01:06:26.700]   I think that doesn't bear out super well, um, but AI can also be run faster than humans.
[01:06:26.700 --> 01:06:35.300]   And so if you have a set of AIs, um, that can do the work of the individual human researchers
[01:06:35.300 --> 01:06:38.780]   and run at 10 times or a hundred times, uh, the speed.
[01:06:38.780 --> 01:06:43.940]   And we asked, well, could the human research community have solved the algorithm problems,
[01:06:43.940 --> 01:06:48.500]   do things like invent transformers, uh, over 100 years.
[01:06:48.500 --> 01:06:54.260]   Uh, if we have this, um, we have AIs with a population, effective population, similar
[01:06:54.260 --> 01:06:57.100]   to the humans, but running a hundred times as fast.
[01:06:57.100 --> 01:07:01.460]   Uh, and so you have to, you have to tell a story where no, the AI, they can't really
[01:07:01.460 --> 01:07:05.220]   do the same things as the humans.
[01:07:05.220 --> 01:07:10.900]   Uh, and we're talking about what happens when, uh, the ads are more capable of in fact doing
[01:07:10.900 --> 01:07:11.900]   that.
[01:07:11.900 --> 01:07:16.420]   Although they become more capable as lesser capable versions of themselves help us make
[01:07:16.420 --> 01:07:17.420]   themselves more capable.
[01:07:17.420 --> 01:07:18.420]   Right.
[01:07:18.420 --> 01:07:20.560]   So you had to like kickstart that at some point.
[01:07:20.560 --> 01:07:27.880]   Is there an example in analogous situations is intelligence unique in the sense that you
[01:07:27.880 --> 01:07:35.660]   have a feedback loop of with a learning curve or something else, a system outputs are feeding
[01:07:35.660 --> 01:07:39.320]   into its own inputs in a way that, because if we're talking about something like Moore's
[01:07:39.320 --> 01:07:43.920]   law or the cost of solar, you do have this way, like we're, we're, you know, more people
[01:07:43.920 --> 01:07:47.760]   are, we're throwing more people at the problem and it's, um, we're, you know, making a lot
[01:07:47.760 --> 01:07:53.400]   of progress, but we don't have this sort of additional part of the model where Moore's
[01:07:53.400 --> 01:07:55.520]   law leads to more humans somehow.
[01:07:55.520 --> 01:07:57.840]   Uh, and the more humans are becoming researchers.
[01:07:57.840 --> 01:08:02.200]   So you do actually have a version of that in the case of solar.
[01:08:02.200 --> 01:08:07.640]   So you have a small infant industry that's doing things like providing solar panels for
[01:08:07.640 --> 01:08:13.360]   space satellites, and then getting increasing amounts of subsidized government demand because
[01:08:13.360 --> 01:08:18.640]   of, uh, you know, worries about fossil fuel depletion and then climate change.
[01:08:18.640 --> 01:08:24.880]   You can have the dynamic where visible successes, uh, with solar or like lowering prices, then
[01:08:24.880 --> 01:08:26.800]   open up new markets.
[01:08:26.800 --> 01:08:32.160]   So there's a particularly huge transition where renewables become cheap enough to replace
[01:08:32.160 --> 01:08:34.080]   large chunks of the electric grid.
[01:08:34.080 --> 01:08:38.240]   Uh, earlier you're like, you're dealing with very niche situations like, yeah.
[01:08:38.240 --> 01:08:43.240]   So the satellites where you have, uh, it's very difficult to refuel a satellite in place
[01:08:43.240 --> 01:08:48.600]   and then remote areas, and then moving to like, you know, the super sunny, the sunniest
[01:08:48.600 --> 01:08:52.800]   areas in the world with the biggest solar subsidies.
[01:08:52.800 --> 01:08:57.720]   And so there was an element of that where more and more investment has been thrown into
[01:08:57.720 --> 01:09:02.160]   the field and like the market has rapidly expanded as the technology improved.
[01:09:02.160 --> 01:09:07.720]   But I think the closest analogy, um, is actually the long run growth of human civilization
[01:09:07.720 --> 01:09:08.720]   itself.
[01:09:08.720 --> 01:09:15.000]   And I know you had Holden Karnofsky from the open philanthropy project on earlier and discuss
[01:09:15.000 --> 01:09:21.720]   some of this research about the long run acceleration of human population and economic growth.
[01:09:21.720 --> 01:09:28.640]   And so developing new technologies allowed human population to expand, uh, humans to
[01:09:28.640 --> 01:09:33.640]   occupy new habitats and new areas, and then to invent agriculture, which supported larger
[01:09:33.640 --> 01:09:38.840]   populations and then even more advanced agriculture in the modern industrial society.
[01:09:38.840 --> 01:09:45.700]   And so their total technology and output allowed you to support more humans who then would
[01:09:45.700 --> 01:09:48.960]   discover more technology, uh, and continue the process.
[01:09:48.960 --> 01:09:55.960]   Now that was boosted because on top of expanding the population, the share of human activity
[01:09:55.960 --> 01:09:59.400]   that was going into invention and innovation went up.
[01:09:59.400 --> 01:10:01.360]   And that was a key part of the industrial revolution.
[01:10:01.360 --> 01:10:08.080]   There was no such thing as a corporate research lab or like an engineering university, um,
[01:10:08.080 --> 01:10:09.080]   prior to that.
[01:10:09.080 --> 01:10:12.640]   And so you were both increasing the total human population and the share of it going
[01:10:12.640 --> 01:10:13.640]   in.
[01:10:13.640 --> 01:10:16.840]   But this population dynamic is pretty, is pretty analogous.
[01:10:16.840 --> 01:10:20.760]   Humans invent farming, they can have more humans than they can invent industry and so
[01:10:20.760 --> 01:10:21.760]   on.
[01:10:21.760 --> 01:10:22.760]   Hmm.
[01:10:22.760 --> 01:10:26.600]   So maybe somebody would be skeptical that with AI progress specifically, it's not just
[01:10:26.600 --> 01:10:32.760]   a matter of, you know, some, like, um, some farmer figuring out crop rotation or some
[01:10:32.760 --> 01:10:35.840]   blacksmith figuring out how to do metallurgy better.
[01:10:35.840 --> 01:10:40.840]   You in fact, even to make the, for the 50% improvement in productivity, you basically
[01:10:40.840 --> 01:10:42.560]   need something on the IQ.
[01:10:42.560 --> 01:10:47.900]   That's close to Ilya Setskofer, there's like a discontinuous, um, you're like contributing
[01:10:47.900 --> 01:10:50.560]   very little to productivity and then you're like Ilya.
[01:10:50.560 --> 01:10:54.840]   And then you contribute a lot, but the becoming Ilya is, you see what I'm saying?
[01:10:54.840 --> 01:10:57.820]   There's not like a gradual increase in capabilities that leads to the feedback.
[01:10:57.820 --> 01:11:04.440]   You're imagining a case where, uh, the distribution of tasks is such that there's nothing that
[01:11:04.440 --> 01:11:08.520]   you can, where individually automating it particularly helps.
[01:11:08.520 --> 01:11:13.360]   Uh, and so the ability to contribute to AI research is really end loaded.
[01:11:13.360 --> 01:11:14.360]   Is that what you're saying?
[01:11:14.360 --> 01:11:15.360]   Yeah.
[01:11:15.360 --> 01:11:20.820]   I mean, we already see this in, in these sorts of like really high IQ, uh, companies or projects
[01:11:20.820 --> 01:11:26.320]   where theoretically, I guess, Jane Street or open AI could hire like a bunch of, uh,
[01:11:26.320 --> 01:11:29.480]   you know, mediocre people to do, there's a comparative advantage.
[01:11:29.480 --> 01:11:33.980]   They could do some menial tasks and that could free up the time of the really smart people,
[01:11:33.980 --> 01:11:34.980]   but they don't do that.
[01:11:34.980 --> 01:11:35.980]   Right.
[01:11:35.980 --> 01:11:37.600]   Uh, the transaction costs, whatever else.
[01:11:37.600 --> 01:11:42.520]   Self-driven cars would be another example where you have a very high quality threshold.
[01:11:42.520 --> 01:11:48.240]   And so when you, your performance as a driver is worse than a human, like you have 10 times
[01:11:48.240 --> 01:11:52.920]   the accident rate or a hundred times the accident rate, then the cost of insurance for that,
[01:11:52.920 --> 01:11:57.000]   which is a proxy for people's willingness to ride the car and stuff too, um, would be
[01:11:57.000 --> 01:11:59.240]   such that the insurance costs would absolutely dominate.
[01:11:59.240 --> 01:12:03.360]   So even if you have zero labor costs, it's offset by the increased insurance costs.
[01:12:03.360 --> 01:12:09.640]   And so there are lots of cases like that where like partial automation is not in practice,
[01:12:09.640 --> 01:12:15.760]   uh, very usable because complementing other resources, you're going to use those other
[01:12:15.760 --> 01:12:22.960]   resources less efficiently, um, and, uh, in a post AGI future, I mean, the same thing
[01:12:22.960 --> 01:12:24.720]   can apply to humans.
[01:12:24.720 --> 01:12:30.100]   So people can say, well, comparative advantage, you know, even if AI is, can do everything
[01:12:30.100 --> 01:12:33.680]   better than a human, uh, well, it's still worth something.
[01:12:33.680 --> 01:12:38.480]   Uh, the human can do some amazing, you know, we can lift a box, that's something.
[01:12:38.480 --> 01:12:45.440]   Um, now there's a question of property rights if, well, if it could just to make more robots
[01:12:45.440 --> 01:12:51.000]   um, but even, even absent that, uh, in such an economy, you wouldn't want to let a human
[01:12:51.000 --> 01:12:55.840]   worker into any industrial environment because, you know, in a clean room, there'll be emitting
[01:12:55.840 --> 01:12:59.280]   all kinds of skin cells and messing things, things up.
[01:12:59.280 --> 01:13:00.980]   You need to have an atmosphere there.
[01:13:00.980 --> 01:13:06.400]   You need a bunch of supporting tools and resources and materials, and those supporting resources
[01:13:06.400 --> 01:13:12.400]   and materials will do a lot more productively working with AI and robots rather than a human.
[01:13:12.400 --> 01:13:16.960]   So you don't want to let a human anywhere near the thing, just like, you know, in a,
[01:13:16.960 --> 01:13:20.200]   you don't, you don't want to have a gorilla wandering around in a China shop, even if
[01:13:20.200 --> 01:13:23.640]   you've trained it to most of the time, pick up a box for you.
[01:13:23.640 --> 01:13:27.160]   If you give it a banana, it's just not worth it to have it wandering around your China
[01:13:27.160 --> 01:13:28.160]   shop.
[01:13:28.160 --> 01:13:29.160]   Yeah.
[01:13:29.160 --> 01:13:32.520]   Like why, why is that not a good objection to, I mean, I think that that is one of the,
[01:13:32.520 --> 01:13:39.280]   the ways, uh, in which partial automation, uh, can fail to really translate into a lot
[01:13:39.280 --> 01:13:40.440]   of economic value.
[01:13:40.440 --> 01:13:43.900]   Um, that's something that will attenuate as we go on.
[01:13:43.900 --> 01:13:50.440]   And as the AI is more able to work independently and more able to handle its own, uh, its own
[01:13:50.440 --> 01:13:53.520]   screw ups, get more, more reliable.
[01:13:53.520 --> 01:13:59.320]   But the way in which it becomes more reliable is by AI progress speeding up, which happens
[01:13:59.320 --> 01:14:01.340]   if I can contribute to it.
[01:14:01.340 --> 01:14:05.300]   But if there is the, if there is some sort of reliability bottleneck that prevents it
[01:14:05.300 --> 01:14:07.400]   from contributing to that progress, then you don't have the loop.
[01:14:07.400 --> 01:14:08.400]   Right.
[01:14:08.400 --> 01:14:09.400]   Yeah.
[01:14:09.400 --> 01:14:10.400]   I mean, this is, this is why we're not there yet.
[01:14:10.400 --> 01:14:11.400]   Right.
[01:14:11.400 --> 01:14:13.120]   But then what is the reason to think we'll be there at?
[01:14:13.120 --> 01:14:19.040]   The broad reason is we have these inputs are scaling up.
[01:14:19.040 --> 01:14:22.960]   There's a, so Epoch, which I mentioned earlier, uh, they have a paper, I think it's called
[01:14:22.960 --> 01:14:27.200]   compute trends and three eras of machine learning, uh, or something like that.
[01:14:27.200 --> 01:14:34.760]   Uh, and so they look at the compute expended on machine learning systems since the found
[01:14:34.760 --> 01:14:38.960]   founding of the field of AI, the beginning of the 1950s.
[01:14:38.960 --> 01:14:42.640]   And so it mostly, it grows with Moore's law.
[01:14:42.640 --> 01:14:47.800]   And so people are spending a similar amount on their experiments, um, but they can just
[01:14:47.800 --> 01:14:51.580]   buy more with that because the compute is coming.
[01:14:51.580 --> 01:14:59.820]   And so that data, I mean, it covers over 20 orders of magnitude, maybe like 24, uh, and
[01:14:59.820 --> 01:15:07.620]   of all of those increases since 1952, a little more than half of them happened between 1952
[01:15:07.620 --> 01:15:12.320]   and 2010 and all the rest is since 2010.
[01:15:12.320 --> 01:15:18.340]   So we're, we've been scaling that up like four times as fast as was the case for most
[01:15:18.340 --> 01:15:20.280]   of the history of AI.
[01:15:20.280 --> 01:15:26.140]   We're running through the orders of magnitude of possible resource inputs you could need
[01:15:26.140 --> 01:15:30.940]   for AI much, much more quickly than we were for most of the history of AI.
[01:15:30.940 --> 01:15:37.020]   That's why this is a period of like with a very elevated chance of AI per year, because
[01:15:37.020 --> 01:15:40.780]   we're moving through so much of the space of inputs per year.
[01:15:40.780 --> 01:15:48.020]   And indeed it looks like this scale-up taken, taken to its conclusion, uh, will cover another
[01:15:48.020 --> 01:15:49.780]   bunch of orders of magnitude.
[01:15:49.780 --> 01:15:54.540]   Uh, and that's actually a large fraction of those that are left before you start running
[01:15:54.540 --> 01:16:00.420]   into saying, well, this is going to have to be like evolution with the sort of simple
[01:16:00.420 --> 01:16:01.420]   hacks we get to apply.
[01:16:01.420 --> 01:16:04.300]   Like we're selecting for intelligence the whole time.
[01:16:04.300 --> 01:16:10.100]   We're not going to do the same mutation that causes fatal, fatal childhood cancer a billion
[01:16:10.100 --> 01:16:14.580]   times, even though, I mean, we keep getting the same fatal mutations, even though they've
[01:16:14.580 --> 01:16:16.000]   been done many times.
[01:16:16.000 --> 01:16:21.040]   We use gradient descent, which takes into account the derivative of improvement on the
[01:16:21.040 --> 01:16:23.180]   loss all throughout the network.
[01:16:23.180 --> 01:16:27.980]   And we, we don't throw away all of the contents of the network with each generation where
[01:16:27.980 --> 01:16:30.700]   you compress down to a little DNA.
[01:16:30.700 --> 01:16:35.380]   So there's that, that bar of like, well, if you're going to do brute force, like evolution
[01:16:35.380 --> 01:16:41.260]   combined with a sort of very simple ways, we can save orders of magnitude on that.
[01:16:41.260 --> 01:16:46.980]   We're going to cover, I think, a fraction that's like half of that distance, uh, in
[01:16:46.980 --> 01:16:49.340]   this scale up over the next 10 years or so.
[01:16:49.340 --> 01:16:54.580]   And so if you started off with a kind of vague uniform prior, you're like, well, it's probably,
[01:16:54.580 --> 01:16:59.600]   you probably can't make AGI with like the amount of compute that would be involved in
[01:16:59.600 --> 01:17:03.900]   a fruit fly existing for a minute, which would be the early days of AI.
[01:17:03.900 --> 01:17:06.540]   Um, you know, maybe, maybe you would get lucky.
[01:17:06.540 --> 01:17:12.140]   We were able to make calculators because calculators benefited from like very reliable, serially
[01:17:12.140 --> 01:17:18.320]   fast computers and where we could take a tiny, tiny, tiny, tiny fraction of a human brain's
[01:17:18.320 --> 01:17:20.480]   compute and use it for a calculator.
[01:17:20.480 --> 01:17:23.020]   We couldn't take an ant's brain and rewire it to calculate.
[01:17:23.020 --> 01:17:29.720]   It's hard, hard to manage ant farms, let alone get them to do arithmetic for you.
[01:17:29.720 --> 01:17:34.220]   And so there were some things where we could exploit the differences between biological
[01:17:34.220 --> 01:17:40.140]   brains and computers, uh, to do stuff super efficiently on computers.
[01:17:40.140 --> 01:17:46.220]   We would doubt that we would be able to do so much better than biology that with a tiny
[01:17:46.220 --> 01:17:50.500]   fraction of an insect's brain, uh, we'd be able to get AI early on.
[01:17:50.500 --> 01:17:54.980]   On the far end, it seemed very implausible that we couldn't do better than completely
[01:17:54.980 --> 01:17:56.700]   brute force evolution.
[01:17:56.700 --> 01:18:01.640]   And so in between you have some number of orders of magnitude of inputs where it might
[01:18:01.640 --> 01:18:02.640]   be.
[01:18:02.640 --> 01:18:06.420]   And like in the 2000s, I would say, well, you know, I'm going to have a pretty uniformish
[01:18:06.420 --> 01:18:07.420]   prior.
[01:18:07.420 --> 01:18:12.220]   I'm going to put weight on it happening at like the sort of equivalent of like 10 to
[01:18:12.220 --> 01:18:19.360]   the 25 ops, 10 to the 30, 10 to the 35, um, and sort of spreading out over that.
[01:18:19.360 --> 01:18:21.380]   And then I can update on other information.
[01:18:21.380 --> 01:18:26.940]   And in the short term, I would say like in 2005, I would say, well, I don't see anything
[01:18:26.940 --> 01:18:29.060]   that looks like the cusp of AGI.
[01:18:29.060 --> 01:18:34.100]   So I'm also going to lower my credence for like the next five years or the next 10 years.
[01:18:34.100 --> 01:18:36.700]   And so that, that, that would be kind of like a vague prior.
[01:18:36.700 --> 01:18:40.180]   And then when we take into account, like, well, how quickly are we running through those
[01:18:40.180 --> 01:18:46.500]   orders of magnitude, if I have a uniform prior, I assign half of my weight to the first half
[01:18:46.500 --> 01:18:48.500]   of remaining orders of magnitude.
[01:18:48.500 --> 01:18:53.940]   And if we're going to run through those over the next 10 years and some, uh, then that
[01:18:53.940 --> 01:18:58.980]   calls on me to put half of my credence conditional on wherever going to make it AI, which seems
[01:18:58.980 --> 01:19:02.340]   likely where it's a material object, easier than evolution.
[01:19:02.340 --> 01:19:07.340]   I've got to put similarly a lot of my credence on AI happening in this scale up.
[01:19:07.340 --> 01:19:12.420]   And then that's supported by what we're seeing in terms of the, uh, the rapid advances in
[01:19:12.420 --> 01:19:15.980]   capabilities, uh, with AI and LLMs in particular.
[01:19:15.980 --> 01:19:16.980]   Okay.
[01:19:16.980 --> 01:19:18.540]   That's actually a really interesting point.
[01:19:18.540 --> 01:19:23.820]   So now that somebody might say, listen, there's not some sense in which AIs could universally
[01:19:23.820 --> 01:19:29.620]   speed up the progress of open AI by 50% or a hundred percent or 200%.
[01:19:29.620 --> 01:19:35.220]   If they're not able to do everything better than Ilya Suskover can, there's going to be
[01:19:35.220 --> 01:19:41.140]   something in which we're bottlenecked by the human researchers and bottleneck effects dictate
[01:19:41.140 --> 01:19:44.260]   that, you know, the slowest moving part of the organization will be the one that kind
[01:19:44.260 --> 01:19:48.780]   of determines the speed of the progress of the whole organization or the whole project.
[01:19:48.780 --> 01:19:51.940]   Which means that unless you get to the point where you're like doing everything and everybody
[01:19:51.940 --> 01:19:56.020]   in the organization can do, you're not going to significantly speed up the progress of
[01:19:56.020 --> 01:19:57.540]   the whole project as a whole.
[01:19:57.540 --> 01:19:58.540]   Yeah.
[01:19:58.540 --> 01:20:02.820]   So that, that is a hypothesis and I think there's a lot of truth to it.
[01:20:02.820 --> 01:20:06.020]   So when we think about like the ways in which AI can contribute.
[01:20:06.020 --> 01:20:11.540]   So there are things we talked about before, like the AI is setting up their own curriculum.
[01:20:11.540 --> 01:20:15.960]   And that's something that Ilya can't do directly, doesn't do directly.
[01:20:15.960 --> 01:20:19.880]   And there's a question, how much does that improve performance?
[01:20:19.880 --> 01:20:26.940]   There are these things where the AI helps to just like produce some code for, for some
[01:20:26.940 --> 01:20:30.020]   tasks and it's beyond hello world at this point.
[01:20:30.020 --> 01:20:36.020]   But I mean, the sort of thing that I hear from AI researchers at leading labs is that,
[01:20:36.020 --> 01:20:40.180]   you know, on their, on their core job where they're like most expert, it's not helping
[01:20:40.180 --> 01:20:41.180]   them that much.
[01:20:41.180 --> 01:20:45.540]   But then, you know, their job often does involve, Oh, I've got to, I've got to code something
[01:20:45.540 --> 01:20:49.140]   that's out of my usual area of expertise.
[01:20:49.140 --> 01:20:52.440]   Or I want to research this question and it helps them there.
[01:20:52.440 --> 01:20:57.900]   And so that saves some of their time and frees them to do more of the bottleneck work.
[01:20:57.900 --> 01:21:05.260]   And then I think the, the idea of well is, you know, is, is everything dependent on Ilya
[01:21:05.260 --> 01:21:09.020]   and is Ilya so much better than the hundreds of other employees?
[01:21:09.020 --> 01:21:14.900]   I think there are a lot of people who are contributing, they're doing a lot of tasks.
[01:21:14.900 --> 01:21:21.620]   And so you can have quite a, a lot of, of gain from automating some areas where you
[01:21:21.620 --> 01:21:25.740]   then do just an absolutely enormous amount of it relative to what you would have done
[01:21:25.740 --> 01:21:30.500]   before because things like designing the custom curriculum, you're like, maybe you had some
[01:21:30.500 --> 01:21:35.460]   humans put some work into that but you're not going to employ billions of humans to
[01:21:35.460 --> 01:21:36.860]   produce it at scale.
[01:21:36.860 --> 01:21:42.240]   And so it winds up being a larger share of the progress than it was before.
[01:21:42.240 --> 01:21:47.860]   You get some benefit from these sorts of things where, Oh yeah, there's like pieces, pieces
[01:21:47.860 --> 01:21:51.860]   of my job that now I can hand off to the AI.
[01:21:51.860 --> 01:21:57.260]   And it lets me focus more on the things that the AI still can't do.
[01:21:57.260 --> 01:22:04.580]   And then at the later on you get to the point where, yeah, the AI can do your job including
[01:22:04.580 --> 01:22:05.580]   the most difficult parts.
[01:22:05.580 --> 01:22:08.820]   And maybe it has to do that in a different way.
[01:22:08.820 --> 01:22:14.420]   Maybe it like spends a ton more time thinking about each step of a problem than you.
[01:22:14.420 --> 01:22:15.780]   And that's, and that's the late end.
[01:22:15.780 --> 01:22:22.160]   The stronger these bottlenecks effects are, the more the economic returns, the scientific
[01:22:22.160 --> 01:22:26.980]   returns and such are end loaded towards getting sort of full AGI.
[01:22:26.980 --> 01:22:32.500]   The weaker the bottlenecks are, the more interim results will be really paying off.
[01:22:32.500 --> 01:22:37.500]   I guess I'd probably disagree with you on how much the sort of the, the alias of organizations
[01:22:37.500 --> 01:22:38.500]   seem to matter.
[01:22:38.500 --> 01:22:42.300]   I guess just from the evidence alone, like how many of the big sort of breakthroughs
[01:22:42.300 --> 01:22:48.300]   that in deep learning in general was like that single individual responsible for, right?
[01:22:48.300 --> 01:22:51.860]   And how much of his time is he spending doing anything that's not that like Copilot is helping
[01:22:51.860 --> 01:22:52.860]   him on?
[01:22:52.860 --> 01:22:55.740]   I'm guessing like most of it is just like managing people and coming up with the ideas
[01:22:55.740 --> 01:22:59.900]   and you know, trying to like understand systems and so on.
[01:22:59.900 --> 01:23:05.220]   And if that is the, if like the five or 10 people who are like that at OpenAI or Anthropic
[01:23:05.220 --> 01:23:11.340]   or whatever are the, are basically the way in which the progress is happening or at least
[01:23:11.340 --> 01:23:18.460]   the algorithmic progress is happening, then how much of better and better Copilot, I know
[01:23:18.460 --> 01:23:21.700]   Copilot is not the thing you're talking about with like the 20% automation, but something
[01:23:21.700 --> 01:23:22.700]   like that.
[01:23:22.700 --> 01:23:28.900]   How much of, yeah, how much is that contributing to the sort of like core function of the research
[01:23:28.900 --> 01:23:29.900]   scientists?
[01:23:29.900 --> 01:23:30.900]   Yeah.
[01:23:30.900 --> 01:23:39.340]   Not sure quantitatively how much we, we disagree about the importance of sort of key research
[01:23:39.340 --> 01:23:45.940]   employees and such, I certainly think that some researchers, you know, add, you know,
[01:23:45.940 --> 01:23:50.580]   more than 10 times the average employee, even much more.
[01:23:50.580 --> 01:23:55.620]   And obviously managers can add an enormous amount of value by proportionately multiplying
[01:23:55.620 --> 01:23:59.760]   the output of the many people that they manage.
[01:23:59.760 --> 01:24:05.380]   And so that's the kind of thing that we were discussing earlier when talking about, well,
[01:24:05.380 --> 01:24:12.700]   if you had sort of full human level AI or AI that had all of the human capabilities
[01:24:12.700 --> 01:24:18.700]   plus AI advantages it would be, you know, you'd benchmark not off of what the sort of
[01:24:18.700 --> 01:24:22.820]   typical human performance is, but peak human performance and beyond.
[01:24:22.820 --> 01:24:26.020]   So I, yeah, I accept all that.
[01:24:26.020 --> 01:24:33.460]   I do think it makes, makes a big difference for people how much they can outsource a lot
[01:24:33.460 --> 01:24:39.780]   of the tasks that are less, wow, less creative and an enormous amount is learned by experimentation.
[01:24:39.780 --> 01:24:46.240]   ML has been, you know, quite experimental field and there's a lot of engineering work
[01:24:46.240 --> 01:24:53.980]   in say building large super clusters, making, yeah, hardware aware, optimization and coding
[01:24:53.980 --> 01:24:59.260]   of these things, being able to do the parallelism in large models.
[01:24:59.260 --> 01:25:07.740]   And the engineers are busy and it's not just only a big thoughts kind of area.
[01:25:07.740 --> 01:25:17.460]   And then the other branch is where will the AI advantages and disadvantages be?
[01:25:17.460 --> 01:25:25.500]   And so one AI advantage is being omnidisciplinary and familiar with the newest things.
[01:25:25.500 --> 01:25:32.020]   So I mentioned before, there's no human who has a million years of TensorFlow experience.
[01:25:32.020 --> 01:25:36.500]   And so to the extent that we're interested in like the very, very cutting edge of things
[01:25:36.500 --> 01:25:41.820]   that have been developed quite recently, then AI that can learn about them in parallel and
[01:25:41.820 --> 01:25:48.740]   experiment and practice with them in parallel can learn much faster than a human potentially.
[01:25:48.740 --> 01:25:55.420]   And the area of computer science is one that is especially suitable for AI to learn in
[01:25:55.420 --> 01:25:56.460]   a digital environment.
[01:25:56.460 --> 01:26:03.020]   So it doesn't require like driving a car around that might kill someone, have enormous costs.
[01:26:03.020 --> 01:26:11.820]   You can do unit tests, you can prove theorems, you can do all sorts of operations entirely
[01:26:11.820 --> 01:26:18.020]   in the confines of a computer and which is one reason why programming has been benefiting
[01:26:18.020 --> 01:26:23.140]   more than a lot of other areas from LLMs recently, whereas robotics is lagging.
[01:26:23.140 --> 01:26:24.980]   So there's some of that.
[01:26:24.980 --> 01:26:30.660]   And then just considering, well, actually, I mean, they are getting better at things
[01:26:30.660 --> 01:26:35.740]   like the GRE math at programming contests.
[01:26:35.740 --> 01:26:41.780]   And I mean, some people have forecasts and predictions outstanding about things like
[01:26:41.780 --> 01:26:47.260]   doing well on the informatics Olympiad and the math Olympiad.
[01:26:47.260 --> 01:26:53.980]   And in the last few years, when people tried to forecast the MMLU benchmark, which was
[01:26:53.980 --> 01:26:59.440]   having a lot of more sophisticated kind of, you know, like graduate student science kind
[01:26:59.440 --> 01:27:08.180]   of questions, yeah, AI knocked that down a lot faster than AI researchers who had registered
[01:27:08.180 --> 01:27:11.300]   and students who had registered forecasts on it.
[01:27:11.300 --> 01:27:18.540]   And so if you're getting top-notch scores on, you know, graduate exams, creative problem
[01:27:18.540 --> 01:27:26.220]   solving, yeah, it's not obvious that that sort of area will be a relative weakness of
[01:27:26.220 --> 01:27:27.220]   AI.
[01:27:27.220 --> 01:27:32.340]   That in fact, computer science is in many ways, especially suitable because of getting
[01:27:32.340 --> 01:27:39.660]   up to speed with new areas, being able to get rapid feedback from the interpreter at
[01:27:39.660 --> 01:27:40.660]   scale.
[01:27:40.660 --> 01:27:45.020]   But do you get rapid feedback if you're doing that, something that's more analogous to research?
[01:27:45.020 --> 01:27:50.340]   If you're like, let's say you have a new model or something, and it's like, if we put in
[01:27:50.340 --> 01:27:55.140]   $10 million on a mini training run on this, this would be a much better.
[01:27:55.140 --> 01:27:56.140]   Yeah.
[01:27:56.140 --> 01:27:59.420]   For very large models, those experiments are going to be quite expensive.
[01:27:59.420 --> 01:28:05.140]   And so you're going to look more at like, can you build up this capability by generalization
[01:28:05.140 --> 01:28:10.060]   from things like many math problems, programming problems, working with small networks?
[01:28:10.060 --> 01:28:11.060]   Yeah.
[01:28:11.060 --> 01:28:12.060]   Yeah.
[01:28:12.060 --> 01:28:13.060]   Fair enough.
[01:28:13.060 --> 01:28:17.180]   I actually, Scott Aronson was one of my professors in college and I took his quantum information
[01:28:17.180 --> 01:28:23.900]   class and I didn't do, I mean, I did okay in it, but he, he, he, he recently wrote a
[01:28:23.900 --> 01:28:27.100]   blog post where he said, you know, I had you before take my quantum information test and
[01:28:27.100 --> 01:28:31.960]   it got a B and I was like, damn, I got a C on the final.
[01:28:31.960 --> 01:28:32.960]   So yeah, yeah.
[01:28:32.960 --> 01:28:36.740]   I've updated and the direction that, you know, I get like, you know, it like seems getting
[01:28:36.740 --> 01:28:37.740]   a B on the test.
[01:28:37.740 --> 01:28:41.420]   I understand quantum information pretty well with different areas of strengths and weaknesses
[01:28:41.420 --> 01:28:42.420]   than the human students.
[01:28:42.420 --> 01:28:43.420]   Sure.
[01:28:43.420 --> 01:28:44.420]   Sure.
[01:28:44.420 --> 01:28:49.540]   Would it be possible for this sort of intelligence explosion to happen without any sort of hardware
[01:28:49.540 --> 01:28:50.540]   progress?
[01:28:50.540 --> 01:28:55.060]   If hardware progress stopped, would this feedback loop still be able to produce some sort of
[01:28:55.060 --> 01:28:57.180]   explosion with only software?
[01:28:57.180 --> 01:28:58.180]   Yeah.
[01:28:58.180 --> 01:29:04.620]   So if we say that the, the technology is frozen, which I think is, is not the case right now,
[01:29:04.620 --> 01:29:10.820]   the Nvidia has managed to deliver significantly better chips for AI workloads for the last
[01:29:10.820 --> 01:29:15.020]   few generations H100, A100, V100.
[01:29:15.020 --> 01:29:20.580]   If that stops entirely, then what you're left with, and maybe we'll define this as like
[01:29:20.580 --> 01:29:23.740]   no more nodes, Moore's law is over.
[01:29:23.740 --> 01:29:28.940]   At that point, the kind of gains you get in amount of compute available come from actually
[01:29:28.940 --> 01:29:31.980]   constructing more chips.
[01:29:31.980 --> 01:29:34.860]   And there are economies of scale you could still realize there.
[01:29:34.860 --> 01:29:42.580]   So right now a chip maker has to amortize the R and D cost of developing the chip.
[01:29:42.580 --> 01:29:45.220]   And then the capital equipment is created.
[01:29:45.220 --> 01:29:50.380]   You build a fab, its peak profits are going to come in the few years when the chips it's
[01:29:50.380 --> 01:29:53.340]   making are at the cutting edge.
[01:29:53.340 --> 01:29:58.900]   Later on has the cost of compute exponentially falls the, you know, you keep the fab open
[01:29:58.900 --> 01:30:02.420]   because you can still make some money given that it's built.
[01:30:02.420 --> 01:30:07.660]   But of all of the profits the fab will ever make right now, they're relatively front-loaded
[01:30:07.660 --> 01:30:10.740]   because when its technology is near, near the cutting edge.
[01:30:10.740 --> 01:30:17.060]   So in a world where Moore's law ends, then you wind up with these very long production
[01:30:17.060 --> 01:30:23.500]   runs where you just, you can keep making chips that stay at the cutting edge and where the
[01:30:23.500 --> 01:30:27.980]   R and D costs get amortized over a much larger base.
[01:30:27.980 --> 01:30:32.100]   So the R and D basically drops, drops out of the price.
[01:30:32.100 --> 01:30:36.900]   And then you get some economies of scale from just making so many fabs in the way that,
[01:30:36.900 --> 01:30:42.380]   you know, when we have the auto industry expands and then this is in general across industries,
[01:30:42.380 --> 01:30:47.980]   when you, when you produce, produce a lot more costs fall because you have right now,
[01:30:47.980 --> 01:30:54.660]   like ASML has many, you know, incredibly exotic suppliers that make some bizarre part of the
[01:30:54.660 --> 01:31:00.500]   thousands of parts in one of these ASML machines, you can't get it anywhere else.
[01:31:00.500 --> 01:31:05.380]   They don't have standardized equipment for their thing because this is the only, only
[01:31:05.380 --> 01:31:06.500]   use for it.
[01:31:06.500 --> 01:31:12.740]   And in a world where we're making 10, a hundred times as many chips at the current node, then
[01:31:12.740 --> 01:31:15.460]   they would benefit from scale economies.
[01:31:15.460 --> 01:31:19.980]   And all of that would become more mass production industrialized.
[01:31:19.980 --> 01:31:25.100]   And so you can, you combine all of the things and it seems like capital costs of like buying
[01:31:25.100 --> 01:31:30.220]   a chip would decline, but the energy costs of running the chip would not.
[01:31:30.220 --> 01:31:35.620]   And so right now energy costs are a minority of the cost, but they're not, they're not,
[01:31:35.620 --> 01:31:36.620]   they're not trivial.
[01:31:36.620 --> 01:31:42.640]   You know, they pass, yeah, it passed 1% a while ago and they're, you know, they're inching
[01:31:42.640 --> 01:31:45.900]   up towards 10% and beyond.
[01:31:45.900 --> 01:31:53.540]   And so you can maybe get like another order of magnitude cost decrease from getting really
[01:31:53.540 --> 01:31:57.940]   efficient in the sort of capital construction, but like energy would still, would still be
[01:31:57.940 --> 01:32:02.860]   a limiting factor after the end of sort of actually improving the chips themselves.
[01:32:02.860 --> 01:32:03.860]   Got it.
[01:32:03.860 --> 01:32:04.860]   Got it.
[01:32:04.860 --> 01:32:08.060]   And when you say like there would be a greater population of AI researchers because are we
[01:32:08.060 --> 01:32:12.060]   using population as a sort of thinking tool of how they could be more effective or do
[01:32:12.060 --> 01:32:17.340]   you literally mean that the way you expect these AIs to contribute a lot to researchers
[01:32:17.340 --> 01:32:22.980]   by just having like a million copies of this, of like a researcher thinking about the same
[01:32:22.980 --> 01:32:26.600]   problem or is it just like a usual thinking model for what it would look like to have
[01:32:26.600 --> 01:32:29.060]   a million times smarter AI working on that problem?
[01:32:29.060 --> 01:32:32.200]   That's definitely a lower bound sort of model.
[01:32:32.200 --> 01:32:37.120]   And often I'm meaning something more like effective population or like you'd need this
[01:32:37.120 --> 01:32:39.780]   many people to have this effect.
[01:32:39.780 --> 01:32:46.180]   And so we were talking earlier about the trade-off between training and inference in board games.
[01:32:46.180 --> 01:32:51.940]   And so you can get the same performance by having a bigger model or by calling the model
[01:32:51.940 --> 01:32:52.940]   more times.
[01:32:52.940 --> 01:32:58.820]   And in general, it's more effective to have a bigger, smarter model and call it less time
[01:32:58.820 --> 01:33:02.120]   up until the point where the costs equalize between them.
[01:33:02.120 --> 01:33:07.740]   And so we would be taking some of the gains of our larger compute on having bigger models
[01:33:07.740 --> 01:33:10.580]   that are individually more capable.
[01:33:10.580 --> 01:33:12.640]   And there would be a division of labor.
[01:33:12.640 --> 01:33:16.500]   So like the tasks that were most cognitively demanding would be done by these giant models,
[01:33:16.500 --> 01:33:18.460]   but some very easy task.
[01:33:18.460 --> 01:33:24.820]   You don't want to expend that giant model if a model 1/100th the size can take that
[01:33:24.820 --> 01:33:25.820]   task.
[01:33:25.820 --> 01:33:30.740]   And so larger models would be in the positions of like researchers and managers, and they
[01:33:30.740 --> 01:33:36.940]   would have swarms of AIs of different sizes as tools that they could make API calls to
[01:33:36.940 --> 01:33:37.940]   and whatnot.
[01:33:37.940 --> 01:33:38.940]   Okay.
[01:33:38.940 --> 01:33:42.660]   We accept the model, and now we've gone to something that is at least as smart as Ilya
[01:33:42.660 --> 01:33:45.860]   Sutskover on all the tasks relevant to AI progress.
[01:33:45.860 --> 01:33:48.940]   And you can have so many copies of it.
[01:33:48.940 --> 01:33:49.940]   What happens in the world now?
[01:33:49.940 --> 01:33:53.580]   What are the next months or years or whatever timeline is relevant to look like?
[01:33:53.580 --> 01:34:00.940]   And so, and to be clear, what's happened is not that we have something that has all of
[01:34:00.940 --> 01:34:04.620]   the abilities and advantages of humans plus the AI advantages.
[01:34:04.620 --> 01:34:10.420]   What we have is something that is like possibly by doing things like doing a ton of calls
[01:34:10.420 --> 01:34:16.500]   to make up for being individually less capable or something, it's able to drive forward AI
[01:34:16.500 --> 01:34:17.500]   progress.
[01:34:17.500 --> 01:34:19.100]   That process is continuing.
[01:34:19.100 --> 01:34:23.940]   So AI progress has accelerated greatly in the course of getting there.
[01:34:23.940 --> 01:34:30.740]   And so maybe we go from our eight months doubling time of software progress in effective compute
[01:34:30.740 --> 01:34:35.100]   to four months or two months.
[01:34:35.100 --> 01:34:41.380]   And so there's a report by Tom Davidson at the Open Philanthropy Project, which spun
[01:34:41.380 --> 01:34:45.460]   out of work I had done previously.
[01:34:45.460 --> 01:34:54.220]   And so I advised and helped with that project, but Tom really carried it forward and produced
[01:34:54.220 --> 01:34:58.980]   a very nice report and model, which Epoch is hosting.
[01:34:58.980 --> 01:35:02.460]   You can plug in your own version of the parameters.
[01:35:02.460 --> 01:35:09.860]   And there is a lot of work estimating the primary things like what's the rate of software
[01:35:09.860 --> 01:35:10.860]   progress?
[01:35:10.860 --> 01:35:12.340]   What's the return to additional work?
[01:35:12.340 --> 01:35:18.380]   How does performance scale at these tasks as you boost the models?
[01:35:18.380 --> 01:35:27.580]   And in general, as we were discussing earlier, these sort of like broadly human level in
[01:35:27.580 --> 01:35:32.620]   every domain with all the advantages is pretty, pretty deep into that.
[01:35:32.620 --> 01:35:40.700]   And so if already we can have an eight months doubling time for software progress, then
[01:35:40.700 --> 01:35:47.100]   by the time you get to that kind of point, it's maybe more like four months, two months
[01:35:47.100 --> 01:35:49.560]   going into one month.
[01:35:49.560 --> 01:35:57.100]   And so if the thing is just proceeding at full speed, then each doubling can come more
[01:35:57.100 --> 01:35:59.060]   rapidly.
[01:35:59.060 --> 01:36:06.500]   And so we can talk about what are the spillovers of like, so how does the models get more capable?
[01:36:06.500 --> 01:36:10.660]   They can be doing other stuff in the world, you know, they can spend some of their time
[01:36:10.660 --> 01:36:12.540]   making Google search more efficient.
[01:36:12.540 --> 01:36:18.560]   They can be, you know, hired as chatbots with some inference compute.
[01:36:18.560 --> 01:36:25.260]   And then we can talk about sort of if that intelligence explosion process is allowed
[01:36:25.260 --> 01:36:33.780]   to proceed, then what happens is, okay, you, you improve your software by a factor of two,
[01:36:33.780 --> 01:36:39.220]   the demand, the efforts needed to get the next doubling are larger, but they're not
[01:36:39.220 --> 01:36:40.220]   twice as large.
[01:36:40.220 --> 01:36:44.160]   Maybe they're like 25%, 35% larger.
[01:36:44.160 --> 01:36:49.500]   So each one comes faster and faster until you hit limitations.
[01:36:49.500 --> 01:36:56.540]   Like you can no longer make further software advances with the hardware that you have.
[01:36:56.540 --> 01:37:02.340]   And looking at, I think, reasonable parameters in that model, it seems to me, if you have
[01:37:02.340 --> 01:37:06.180]   these giant training runs, you can go very far.
[01:37:06.180 --> 01:37:11.780]   And so the way I would see this playing out is, how does the AIs get better and better
[01:37:11.780 --> 01:37:12.900]   at research?
[01:37:12.900 --> 01:37:14.760]   They can work on different problems.
[01:37:14.760 --> 01:37:18.620]   They can work on improving software, they can work on improving hardware.
[01:37:18.620 --> 01:37:23.420]   They can do things like create new industrial technologies, new energy technology.
[01:37:23.420 --> 01:37:29.420]   They can manage robots, they can manage human workers as like executives and coaches and
[01:37:29.420 --> 01:37:30.420]   whatnot.
[01:37:30.420 --> 01:37:35.620]   You can, you can do all of these things and AIs wind up being applied where the returns
[01:37:35.620 --> 01:37:37.380]   are highest.
[01:37:37.380 --> 01:37:43.280]   And I think initially the returns are especially high in doing more software.
[01:37:43.280 --> 01:37:49.660]   And the reason for that is, again, if you improve the software, you can update all of
[01:37:49.660 --> 01:37:52.560]   the GPUs that you have access to.
[01:37:52.560 --> 01:37:57.020]   You know, your cloud compute is suddenly more potent.
[01:37:57.020 --> 01:38:05.940]   If you design a new chip design, it'll take a few months to produce the first ones.
[01:38:05.940 --> 01:38:09.020]   And it doesn't update all of your old chips.
[01:38:09.020 --> 01:38:15.440]   So you have an ordering where you start off with the things where there's the lowest dependence
[01:38:15.440 --> 01:38:21.980]   on existing stocks and you can more just take whatever you're developing and apply it immediately.
[01:38:21.980 --> 01:38:29.140]   And so software runs ahead, you're getting more towards the limits of that software.
[01:38:29.140 --> 01:38:33.460]   And I think that means things like having all the human advantages, but combined with
[01:38:33.460 --> 01:38:35.740]   AI advantages.
[01:38:35.740 --> 01:38:43.900]   And so I think that means given the kind of compute that would be involved, if we're talking
[01:38:43.900 --> 01:38:48.820]   about this hundreds of billions of dollars, trillion dollar training run, there's enough
[01:38:48.820 --> 01:38:55.620]   compute to run tens of millions, hundreds of millions of sort of like human scale minds.
[01:38:55.620 --> 01:39:02.180]   They're probably smaller than human scale to be like similarly efficient at the limits
[01:39:02.180 --> 01:39:05.380]   of algorithmic progress, because they have the advantage of a million years of education.
[01:39:05.380 --> 01:39:07.860]   They have the other advantages we talked about.
[01:39:07.860 --> 01:39:13.260]   So you've got that wild capability and the software, further software gains are running
[01:39:13.260 --> 01:39:20.260]   out or like they start to slow down again because you're just getting towards the limits
[01:39:20.260 --> 01:39:23.500]   of like, you can't do any better than the best.
[01:39:23.500 --> 01:39:25.820]   And so what happens then?
[01:39:25.820 --> 01:39:26.820]   Yeah.
[01:39:26.820 --> 01:39:30.260]   By the time they're running out, have we already hit superintelligence or?
[01:39:30.260 --> 01:39:31.260]   Yes.
[01:39:31.260 --> 01:39:33.260]   You're, you're, you're widely superintelligent.
[01:39:33.260 --> 01:39:34.260]   We've left the galaxy.
[01:39:34.260 --> 01:39:35.260]   Okay.
[01:39:35.260 --> 01:39:40.500]   So just by having the abilities that humans have and then combining it with being very
[01:39:40.500 --> 01:39:44.860]   well focused and trained in the task beyond what any human could be and then running faster
[01:39:44.860 --> 01:39:45.860]   and such.
[01:39:45.860 --> 01:39:46.860]   Got it.
[01:39:46.860 --> 01:39:47.860]   Got it.
[01:39:47.860 --> 01:39:48.860]   All right.
[01:39:48.860 --> 01:39:49.860]   Continue.
[01:39:49.860 --> 01:39:50.860]   Yeah.
[01:39:50.860 --> 01:39:51.860]   So I'm not, I'm not going to assume that there's like huge qualitative improvements you can
[01:39:51.860 --> 01:39:52.860]   have.
[01:39:52.860 --> 01:39:57.620]   I'm not going to assume that humans are like very far from the efficient frontier of software,
[01:39:57.620 --> 01:40:02.700]   except with respect to things like, yeah, we had limited lifespan, so we couldn't train
[01:40:02.700 --> 01:40:03.700]   super intensively.
[01:40:03.700 --> 01:40:07.700]   We couldn't incorporate other software into our brains.
[01:40:07.700 --> 01:40:08.980]   We couldn't copy ourselves.
[01:40:08.980 --> 01:40:10.540]   We couldn't run at fast speeds.
[01:40:10.540 --> 01:40:11.540]   Yeah.
[01:40:11.540 --> 01:40:15.740]   So you've got all of those, those capabilities.
[01:40:15.740 --> 01:40:22.940]   And now I'm skipping ahead of like the most important months in human history.
[01:40:22.940 --> 01:40:31.060]   And so I can talk about sort of, you know, what it looks like if it's just the AIs took
[01:40:31.060 --> 01:40:36.060]   over, they're running things as they like, how do things expand?
[01:40:36.060 --> 01:40:44.260]   I can talk about things as how does this go, you know, in a world where we've roughly or
[01:40:44.260 --> 01:40:51.980]   at least so far managed to retain control of where these systems are going.
[01:40:51.980 --> 01:40:56.020]   And so by jumping ahead, I can talk about how would this translate into the physical
[01:40:56.020 --> 01:40:57.020]   world.
[01:40:57.020 --> 01:41:00.860]   And so this is something that I think is a stopping point for a lot of people in thinking
[01:41:00.860 --> 01:41:04.140]   about, well, what would an intelligence explosion look like?
[01:41:04.140 --> 01:41:10.180]   And they have trouble going from, well, there's stuff on servers and cloud compute and, oh,
[01:41:10.180 --> 01:41:11.540]   that gets very smart.
[01:41:11.540 --> 01:41:14.700]   But then how does what I see in the world change?
[01:41:14.700 --> 01:41:17.780]   How does like industry or military power change?
[01:41:17.780 --> 01:41:21.180]   If there's an AI takeover, like, what does that look like?
[01:41:21.180 --> 01:41:23.100]   Are there killer robots?
[01:41:23.100 --> 01:41:24.100]   And so, yeah.
[01:41:24.100 --> 01:41:33.180]   So one course we might go down is to discuss during that wildly accelerating transition,
[01:41:33.180 --> 01:41:35.200]   how did we manage that?
[01:41:35.200 --> 01:41:38.220]   How do you avoid it being catastrophic?
[01:41:38.220 --> 01:41:47.260]   And another route we could go is how does the translation from wildly expanded scientific
[01:41:47.260 --> 01:41:54.140]   R&D capabilities, intelligence on these servers translate into things in the physical world?
[01:41:54.140 --> 01:42:01.380]   So you're moving along in order of like what has the quickest impact largely or like where
[01:42:01.380 --> 01:42:07.860]   you can have an immediate change.
[01:42:07.860 --> 01:42:17.580]   So one of the most immediately accessible things is where we have large numbers of devices
[01:42:17.580 --> 01:42:28.060]   or artifacts or capabilities that are already AI operable with hundreds of millions equivalent
[01:42:28.060 --> 01:42:36.540]   researchers, you can like quickly solve self-driving cars, make the algorithms much more efficient,
[01:42:36.540 --> 01:42:42.620]   do great testing and simulation, and then operate a large number of cars in parallel
[01:42:42.620 --> 01:42:47.660]   if you need to get some additional data to improve the simulation and reasoning.
[01:42:47.660 --> 01:42:54.620]   Although, you know, in fact, humans with quite little data are able to achieve human level
[01:42:54.620 --> 01:42:55.900]   driving performance.
[01:42:55.900 --> 01:43:02.300]   So after you've really maxed out the easily accessible algorithmic improvements in this
[01:43:02.300 --> 01:43:06.700]   software based intelligence explosion that's mostly happening on server farms, then you
[01:43:06.700 --> 01:43:12.740]   have you have minds that have been able to really perform on a lot of digital only tasks
[01:43:12.740 --> 01:43:15.060]   that they're doing great on video games.
[01:43:15.060 --> 01:43:20.540]   They're doing great at predicting what happens next in a YouTube video.
[01:43:20.540 --> 01:43:26.380]   If you have a camera that they can move, they're able to predict what will happen at different
[01:43:26.380 --> 01:43:27.380]   angles.
[01:43:27.380 --> 01:43:32.940]   And that's a lot where we naturally move our eyes in such a way to get images from
[01:43:32.940 --> 01:43:37.500]   different angles and different presentations and then predicting combined from that.
[01:43:37.500 --> 01:43:45.540]   And yeah, and you can operate many cars, many robots at once to get very good robot controllers.
[01:43:45.540 --> 01:43:50.880]   So you should think that all the existing robotic equipment or remotely controllable
[01:43:50.880 --> 01:43:56.180]   equipment that is wired for that, the AI can operate that quite well.
[01:43:56.180 --> 01:44:01.780]   I think some people might be skeptical that existing robots, given their current hardware,
[01:44:01.780 --> 01:44:08.420]   have the dexterity and the maneuverability to do a lot of physical labor that an AI might
[01:44:08.420 --> 01:44:09.420]   want to do.
[01:44:09.420 --> 01:44:10.420]   Do you have reason for thinking otherwise?
[01:44:10.420 --> 01:44:12.060]   There's also not very many of them.
[01:44:12.060 --> 01:44:17.860]   So production of sort of industrial robots is hundreds of thousands per year.
[01:44:17.860 --> 01:44:21.180]   You know, they can do quite a bit in place.
[01:44:21.180 --> 01:44:28.780]   Elon Musk is promising a humanoid robot in the tens of thousands of dollars.
[01:44:28.780 --> 01:44:35.220]   That may take a lot longer than he has said, as has happened with other technologies, but
[01:44:35.220 --> 01:44:39.500]   I mean, that's a direction to go, but most immediately.
[01:44:39.500 --> 01:44:47.060]   So hands are actually probably the most scarce thing, but if we consider what do human bodies
[01:44:47.060 --> 01:44:48.060]   provide?
[01:44:48.060 --> 01:44:49.060]   So there's the brain.
[01:44:49.060 --> 01:44:55.180]   And in this situation, we have now an abundance of high quality brain power that will be increasing
[01:44:55.180 --> 01:45:02.220]   as the AI's will have designed new chips, which will be rolling out from the TSMC factories,
[01:45:02.220 --> 01:45:09.220]   and they'll have ideas and designs for the production of new fab technologies, new nodes
[01:45:09.220 --> 01:45:13.020]   and additional fabs, but yeah, looking around the body.
[01:45:13.020 --> 01:45:17.660]   So there's legs to move around, not always that necessary, wheels work pretty well being
[01:45:17.660 --> 01:45:18.660]   in a place.
[01:45:18.660 --> 01:45:24.660]   We need most people most of the time in factory jobs and office jobs, office jobs, many of
[01:45:24.660 --> 01:45:33.180]   them can be fully virtualized, but yeah, some amount of legs, wheels, other transport.
[01:45:33.180 --> 01:45:38.700]   You have hands and hands are something that are on the expensive end.
[01:45:38.700 --> 01:45:40.860]   In robots, we can make them.
[01:45:40.860 --> 01:45:45.340]   They're made in very small production runs, partly because we don't have the control software
[01:45:45.340 --> 01:45:46.340]   to use them well.
[01:45:46.700 --> 01:45:49.380]   In this world, the control software is fabulous.
[01:45:49.380 --> 01:45:56.740]   And so people will produce much larger production runs of them over time, possibly using technology
[01:45:56.740 --> 01:46:03.060]   we recognize possibly with quite different technology, but just taking what we've got.
[01:46:03.060 --> 01:46:09.540]   So right now the robot arm industry, the industrial robot industry produces hundreds of thousands
[01:46:09.540 --> 01:46:11.620]   of machines a year.
[01:46:11.620 --> 01:46:15.380]   Some of the nicer ones are like $50,000.
[01:46:15.380 --> 01:46:19.180]   In aggregate, the industry has tens of billions of dollars of revenue.
[01:46:19.180 --> 01:46:26.180]   By comparison, the automobile industry produces like, I think over 60 million cars a year.
[01:46:26.180 --> 01:46:32.140]   It has revenue of over $2 trillion per annum.
[01:46:32.140 --> 01:46:39.940]   And so converting that production capacity over towards robot production would be one
[01:46:39.940 --> 01:46:44.220]   of the things, if they're not something better to do, would be one of the things to do.
[01:46:44.220 --> 01:46:52.060]   And in World War II, industrial conversion of American industry took place over several
[01:46:52.060 --> 01:47:01.620]   years and really amazingly ramped up military production by converting existing civilian
[01:47:01.620 --> 01:47:03.260]   industry.
[01:47:03.260 --> 01:47:08.620]   And that was without the aid of superhuman intelligence and management at every step
[01:47:08.620 --> 01:47:10.100]   in the process.
[01:47:10.100 --> 01:47:15.300]   So yeah, every part of that would be very well designed.
[01:47:15.300 --> 01:47:22.260]   You'd have AI workers who understood every part of the process and could direct human
[01:47:22.260 --> 01:47:23.580]   workers.
[01:47:23.580 --> 01:47:33.220]   Even in a fancy factory, most of the time, it's not the hands doing a physical motion
[01:47:33.220 --> 01:47:34.460]   that a worker is being paid for.
[01:47:34.460 --> 01:47:40.980]   They're often looking at things or deciding what to change.
[01:47:40.980 --> 01:47:45.100]   The time spent in manual motion is a limited portion of that.
[01:47:45.100 --> 01:47:51.860]   And so in this world of abundant AI cognitive abilities, where the human workers are more
[01:47:51.860 --> 01:47:59.180]   valuable for their hands than their heads, then you could have a worker, even a worker
[01:47:59.180 --> 01:48:05.020]   previously without training and expertise in the area, who has a smartphone, maybe a
[01:48:05.020 --> 01:48:07.900]   smartphone on a headset.
[01:48:07.900 --> 01:48:12.980]   And we have billions of smartphones which have eyes and ears and methods for communication
[01:48:12.980 --> 01:48:18.780]   for an AI to be talking to a human and directing them in their physical motions.
[01:48:18.780 --> 01:48:25.440]   With skill as a guide and coach that is beyond any human, they're going to be a lot better
[01:48:25.440 --> 01:48:27.780]   at telepresence and remote work.
[01:48:27.780 --> 01:48:33.660]   And they can provide VR and augmented reality guidance to help people get better at doing
[01:48:33.660 --> 01:48:37.680]   the physical motions that they're providing in the construction.
[01:48:37.680 --> 01:48:44.100]   Say you convert the auto industry to robot production.
[01:48:44.100 --> 01:48:51.300]   If it can produce an amount of mass of machines that is similar to what it currently produces,
[01:48:51.300 --> 01:48:59.340]   that's enough for a billion human-sized robots a year.
[01:48:59.340 --> 01:49:10.340]   The value per kilogram of cars is somewhat less than high-end robots, but you're also
[01:49:10.340 --> 01:49:15.620]   cutting out most of the wage bill, because most of the wage bill is payments ultimately
[01:49:15.620 --> 01:49:22.700]   to human capital and education, not to the physical hand motions and lifting objects
[01:49:22.700 --> 01:49:25.100]   and that sort of task.
[01:49:25.100 --> 01:49:29.540]   So at the existing scale of the auto industry, if you can make a billion robots a year, the
[01:49:29.540 --> 01:49:33.900]   auto industry is 2% or 3% of the existing economy.
[01:49:33.900 --> 01:49:37.260]   You're replacing these cognitive things.
[01:49:37.260 --> 01:49:44.740]   So if right now physical hand motions are like 10% of the work, redirect humans into
[01:49:44.740 --> 01:49:46.900]   those tasks.
[01:49:46.900 --> 01:49:55.140]   And in the world at large right now, mean income is on the order of $10,000 a year,
[01:49:55.140 --> 01:50:00.140]   but in rich countries, skilled workers earn more than $100,000 per year.
[01:50:00.140 --> 01:50:07.060]   And some of that is not just management roles of which only a certain proportion of the
[01:50:07.060 --> 01:50:15.460]   population can have, but just being an absolutely exceptional peak end human performance of
[01:50:15.460 --> 01:50:20.340]   some of these construction and such roles.
[01:50:20.340 --> 01:50:28.660]   Just raising productivity to match the most productive workers in the world is room to
[01:50:28.660 --> 01:50:32.460]   make a very, very big gap.
[01:50:32.460 --> 01:50:39.840]   And with AI replacing skills that are scarce in many places where there's abundant, currently
[01:50:39.840 --> 01:50:44.980]   low wage labor, you bring in the AI coach and someone who was previously making very
[01:50:44.980 --> 01:50:52.100]   low wages can suddenly be super productive by just being the hands for an AI.
[01:50:52.100 --> 01:50:59.380]   And so on a naive view, if you ignore the delay of capital adjustment, of like building
[01:50:59.380 --> 01:51:06.940]   new tools for the workers, say like, yeah, just like raise typical productivity for workers
[01:51:06.940 --> 01:51:14.820]   around the world to be more like rich countries and get 5x, 10x like that.
[01:51:14.820 --> 01:51:21.740]   Get more productivity by with AI handling the difficult cognitive tasks, reallocating
[01:51:21.740 --> 01:51:26.740]   people from like office jobs to providing physical motions.
[01:51:26.740 --> 01:51:31.100]   And since right now that's a small proportion of the economy, you can expand the sort of
[01:51:31.100 --> 01:51:37.540]   hands for manual labor by like an order of magnitude, like within a rich country, by
[01:51:37.540 --> 01:51:43.860]   just because most people are sitting in an office or even in a factory floor or not continuously
[01:51:43.860 --> 01:51:44.860]   moving.
[01:51:44.860 --> 01:51:52.460]   So you've got billions of hands lying around in humans to be used in the course of constructing
[01:51:52.460 --> 01:51:58.980]   your waves of robots, and now once you have a quantity of robots that is approaching the
[01:51:58.980 --> 01:52:06.380]   human population, and they work 24/7, of course, the human labor will no longer be valuable
[01:52:06.380 --> 01:52:08.260]   as hands and legs.
[01:52:08.260 --> 01:52:14.260]   But at the very beginning of the transition, just like new software can be used to update
[01:52:14.260 --> 01:52:21.540]   all of the GPUs to run the latest AI, humans are sort of legacy population with an enormous
[01:52:21.540 --> 01:52:28.780]   number of underutilized hands and feet that the AI can use for the initial robot construction.
[01:52:28.780 --> 01:52:34.100]   Cognitive tasks are being automated and the production of them is greatly expanding, and
[01:52:34.100 --> 01:52:40.380]   then the physical tasks which complement them are utilizing humans to do the parts that
[01:52:40.380 --> 01:52:42.180]   robots that exist can't do.
[01:52:42.180 --> 01:52:45.580]   Is the implication of this that you're getting to that world production would increase just
[01:52:45.580 --> 01:52:51.060]   a tremendous amount or that AI could get a lot done of whatever motivations it has?
[01:52:51.060 --> 01:52:52.060]   Yeah.
[01:52:52.060 --> 01:52:58.900]   So there's an enormous increase in production for humans who just switching over to the
[01:52:58.900 --> 01:53:04.740]   role of providing hands and feet for AI where they're limited.
[01:53:04.740 --> 01:53:09.020]   And this robot industry is a natural place to apply it.
[01:53:09.020 --> 01:53:15.260]   And so if you go to something that's like 10X the size of the current car industry in
[01:53:15.260 --> 01:53:21.180]   terms of its production, which would still be like a third of our current economy, and
[01:53:21.180 --> 01:53:25.580]   the aggregate productive capabilities of the society with AI support are going to be a
[01:53:25.580 --> 01:53:26.580]   lot larger.
[01:53:26.580 --> 01:53:30.820]   They make 10 billion humanoid robots a year.
[01:53:30.820 --> 01:53:39.700]   And then if you do that, the legacy population of a few billion human workers is no longer
[01:53:39.700 --> 01:53:42.200]   very important for the physical tasks.
[01:53:42.200 --> 01:53:49.420]   And then the new automated industrial base can just produce more factories, produce more
[01:53:49.420 --> 01:53:50.500]   robots.
[01:53:50.500 --> 01:53:53.660]   And then the interesting thing is like, what's the doubling time?
[01:53:53.660 --> 01:54:01.340]   How long does it take for a set of computers, robots, factories, and supporting equipment
[01:54:01.340 --> 01:54:04.500]   to produce another equivalent quantity of that?
[01:54:04.500 --> 01:54:12.220]   For GPUs, brains, this is really, really easy, really solid, there's an enormous margin there.
[01:54:12.220 --> 01:54:23.940]   We're talking before about skilled human workers getting paid $100 an hour is quite normal
[01:54:23.940 --> 01:54:28.140]   in developed countries for very in-demand skills.
[01:54:28.140 --> 01:54:34.500]   You make a GPU that can do that work.
[01:54:34.500 --> 01:54:39.360]   Right now, these GPUs are like tens of thousands of dollars.
[01:54:39.360 --> 01:54:50.240]   If you can do $100 of wages each hour, then in a few weeks, you pay back your costs.
[01:54:50.240 --> 01:54:55.260]   If the thing is more productive, and as we were discussing, you can be a lot more productive
[01:54:55.260 --> 01:55:01.260]   than a sort of a typical high paid human professional by being the very best human professional,
[01:55:01.260 --> 01:55:05.380]   and even better than that, by having a million years of education and working all the time.
[01:55:05.380 --> 01:55:09.420]   Yeah, then you could get even shorter payback times.
[01:55:09.420 --> 01:55:17.700]   You can generate the dollar value of the initial cost of that equipment within a few weeks.
[01:55:17.700 --> 01:55:28.860]   For robots, so a human factory worker can earn $50,000 a year, really top-notch factory
[01:55:28.860 --> 01:55:34.620]   workers earning more and working all the time, if they can produce a few hundred thousand
[01:55:34.620 --> 01:55:42.300]   dollars of value per year and buy a robot that costs $50,000 to replace them, then that's
[01:55:42.300 --> 01:55:46.380]   a payback time of some months.
[01:55:46.380 --> 01:55:49.140]   That is about the financial return.
[01:55:49.140 --> 01:55:53.180]   We're going to get to the physical capital return, because those are going to diverge
[01:55:53.180 --> 01:55:54.180]   in this scenario.
[01:55:54.180 --> 01:55:55.180]   Right, yeah.
[01:55:55.180 --> 01:55:56.180]   Because right now-
[01:55:56.180 --> 01:55:58.860]   Because it seems like it's going to be a given that like, all right, these super-intelligent
[01:55:58.860 --> 01:56:01.620]   robots are going to be able to make a lot of money and can be very valuable.
[01:56:01.620 --> 01:56:03.260]   Can they basically scale up?
[01:56:03.260 --> 01:56:09.580]   What we really care about are the actual physical operations that a thing does.
[01:56:09.580 --> 01:56:12.980]   How much do they contribute to these tasks?
[01:56:12.980 --> 01:56:20.420]   I'm using this as a start to try and get back to the physical replication times.
[01:56:20.420 --> 01:56:22.460]   I guess I'm wondering what is the implication of this?
[01:56:22.460 --> 01:56:27.540]   Because I think you started off this by saying people have not thought about what the physical
[01:56:27.540 --> 01:56:30.100]   implications of super-intelligence would be.
[01:56:30.100 --> 01:56:31.100]   What is the bigger takeaway?
[01:56:31.100 --> 01:56:35.940]   What are we wrong about when we think about what the world will look like with super-intelligence?
[01:56:35.940 --> 01:56:43.580]   With robots that are optimally operated by AI, so extremely finely operated, and with
[01:56:43.580 --> 01:56:51.860]   building technological designs and equipment and facilities under AI direction, how much
[01:56:51.860 --> 01:56:54.140]   can they produce?
[01:56:54.140 --> 01:57:03.820]   For a doubling, you need the AIs to produce stuff that is, in aggregate, at least equal
[01:57:03.820 --> 01:57:06.580]   to their own cost.
[01:57:06.580 --> 01:57:12.380]   So now we're pulling out these things like labor costs that no longer apply, and then
[01:57:12.380 --> 01:57:16.020]   trying to zoom in on what these capital costs will be.
[01:57:16.020 --> 01:57:17.740]   You're still going to need the raw materials.
[01:57:17.740 --> 01:57:21.660]   You're still going to need the robot time building the next robot.
[01:57:21.660 --> 01:57:27.140]   I think it's pretty likely that with the advanced AI work, they can design some incremental
[01:57:27.140 --> 01:57:36.540]   improvements, and with the industry scale-up, that you can get tenfold and better cost reductions
[01:57:36.540 --> 01:57:42.660]   on the system by making things more efficient and replacing the human cognitive labor.
[01:57:42.660 --> 01:57:51.580]   And so maybe that's like you need $5,000 of costs under our current environment.
[01:57:51.580 --> 01:57:57.700]   But the big change in this world is we're trying to produce this stuff faster.
[01:57:57.700 --> 01:58:03.740]   If we're asking about the doubling time of the whole system in, say, one year, if you
[01:58:03.740 --> 01:58:09.700]   have to build a whole new factory to double everything, you don't have time to amortize
[01:58:09.700 --> 01:58:10.940]   the cost of that factory.
[01:58:10.940 --> 01:58:15.540]   Right now, you might build a factory and use it for ten years, and buy some equipment and
[01:58:15.540 --> 01:58:17.480]   use it for five years.
[01:58:17.480 --> 01:58:24.200]   And so that's your capital cost, and in an accounting context, you depreciate each year
[01:58:24.200 --> 01:58:28.120]   a fraction of that capital purchase.
[01:58:28.120 --> 01:58:34.620]   But if we're trying to double our entire industrial system in one year, then those capital costs
[01:58:34.620 --> 01:58:37.200]   have to be multiplied.
[01:58:37.200 --> 01:58:43.360]   So if we're going to be getting most of the return on our factory in the first year, instead
[01:58:43.360 --> 01:58:48.280]   of ten years, weighted appropriately, then we're going to say, "Okay, our capital cost
[01:58:48.280 --> 01:58:55.480]   has to go up by tenfold, because I'm building an entire factory for this year's production."
[01:58:55.480 --> 01:59:00.920]   I mean, it will do more stuff later, but it's most important early on, instead of over ten
[01:59:00.920 --> 01:59:01.920]   years.
[01:59:01.920 --> 01:59:08.560]   And so that's going to raise the cost of that reproduction.
[01:59:08.560 --> 01:59:17.400]   And so it seems like going from a current decade cycle of amortizing factories and fabs
[01:59:17.400 --> 01:59:21.880]   and whatnot, and shorter for some things, the longest are things like big buildings
[01:59:21.880 --> 01:59:22.880]   and such.
[01:59:22.880 --> 01:59:29.780]   Yeah, that could be a tenfold increase from moving to double the physical stuff each year
[01:59:29.780 --> 01:59:30.960]   in capital costs.
[01:59:30.960 --> 01:59:37.420]   And given the savings that we get in the story from scaling up the industry, from removing
[01:59:37.420 --> 01:59:44.120]   the payments to human cognitive labor, and then from just adding new technological advancements
[01:59:44.120 --> 01:59:48.640]   and super high quality cognitive supervision, like applying more of it than was applied
[01:59:48.640 --> 01:59:56.320]   today, it looks like you can get cost reductions that offset that increased capital cost.
[01:59:56.320 --> 02:00:04.640]   So that your $50,000 improved robot arms, or industrial robots, it seems like that can
[02:00:04.640 --> 02:00:08.400]   do the work of a human factory worker.
[02:00:08.400 --> 02:00:12.720]   So it would be like the equivalent of hundreds of thousands of dollars.
[02:00:12.720 --> 02:00:21.040]   And by default, it costs more than the $50,000 arms today, but then you apply all these other
[02:00:21.040 --> 02:00:22.040]   cost savings.
[02:00:22.040 --> 02:00:27.960]   And it looks like then you get a period, a robot doubling time that is less than a year.
[02:00:27.960 --> 02:00:32.400]   I think significantly less than a year as you get into it.
[02:00:32.400 --> 02:00:40.640]   So in this first phase, you have humans under AI direction and existing robot industry and
[02:00:40.640 --> 02:00:47.400]   converted auto industry and expanded facilities making robots.
[02:00:47.400 --> 02:00:54.680]   Those over less than a year, you produce robots until their combined production is exceeding
[02:00:54.680 --> 02:00:58.760]   that of humans' arms and feet.
[02:00:58.760 --> 02:01:06.680]   And then you could have over a period then with a doubling time of months or less clanking
[02:01:06.680 --> 02:01:10.040]   replicators, robots as we understand them growing.
[02:01:10.040 --> 02:01:19.160]   And then that's not to say that's the limit of the most that technology could do, because
[02:01:19.160 --> 02:01:22.440]   biology is able to reproduce at faster rates.
[02:01:22.440 --> 02:01:26.680]   And maybe we're talking about that in a moment, but if we're trying to restrict ourselves
[02:01:26.680 --> 02:01:33.360]   to robotic technology as we understand it and cost falls that are reasonable from eliminating
[02:01:33.360 --> 02:01:38.280]   all labor, massive industrial scale up, and historical kinds of technological improvements
[02:01:38.280 --> 02:01:47.320]   that lowered costs, I think you can get into a robot population industry doubling in months.
[02:01:47.320 --> 02:01:51.480]   And then what is the implication of the biological doubling times?
[02:01:51.480 --> 02:01:56.400]   And I guess this doesn't have to be a biological, but, you know, you can have like, you can
[02:01:56.400 --> 02:02:01.440]   do like, uh, you know, Drexler like first principles, how much would it cost to view
[02:02:01.440 --> 02:02:04.480]   both a nanotech thing that like built more nanobots?
[02:02:04.480 --> 02:02:08.840]   I certainly take the human brain and other biological brains as like very relevant data
[02:02:08.840 --> 02:02:13.800]   points about what's possible with computing and intelligence, like what's a reproductive
[02:02:13.800 --> 02:02:20.120]   capability of biological plants and animals and microorganisms I think is, is relevant
[02:02:20.120 --> 02:02:26.360]   as like, this is, you know, it's possible to, for systems to reproduce at least as fast.
[02:02:26.360 --> 02:02:30.640]   And so at the extreme, you have bacteria that are heterotrophic.
[02:02:30.640 --> 02:02:36.440]   So they're feeding on some abundant external food source and ideal conditions.
[02:02:36.440 --> 02:02:40.440]   And there's some that can divide like every 20 or 60 minutes.
[02:02:40.440 --> 02:02:44.720]   So obviously that's absurdly, absurdly fast.
[02:02:44.720 --> 02:02:49.320]   That seems on the, on the low end, because ideal conditions require actually setting
[02:02:49.320 --> 02:02:50.320]   them up.
[02:02:50.320 --> 02:02:53.440]   There needs to be abundant energy there.
[02:02:53.440 --> 02:02:59.600]   And so if you're actually having to acquire that energy by building solar panels, um,
[02:02:59.600 --> 02:03:06.000]   or like, um, burning combustible materials or whatnot, and then the physical equipment
[02:03:06.000 --> 02:03:11.680]   to produce those ideal conditions can be a bit slower, uh, cyanobacteria, um, which are
[02:03:11.680 --> 02:03:18.660]   self-powered, uh, from solar energy, um, the really fast ones in ideal conditions can double
[02:03:18.660 --> 02:03:19.660]   in a day.
[02:03:19.660 --> 02:03:25.400]   A reason why cyanobacteria isn't like the food source for everyone and everything, uh,
[02:03:25.400 --> 02:03:30.880]   is it's hard to ensure those ideal conditions, uh, and then to extract them from the water.
[02:03:30.880 --> 02:03:35.520]   I mean, they do of course, power the aquatic ecology, uh, but they're, they're floating,
[02:03:35.520 --> 02:03:41.280]   uh, they're floating in liquid, um, getting resources they need to them and out is tricky
[02:03:41.280 --> 02:03:47.660]   and then extracting, uh, extracting your product, but like, yeah, one day doubling times are
[02:03:47.660 --> 02:03:50.960]   possible, um, powered by the sun.
[02:03:50.960 --> 02:03:59.000]   And then if we look at things like insects, um, so fruit flies can have hundreds of offspring
[02:03:59.000 --> 02:04:04.680]   in a, in a few weeks, you extrapolate that over a year and you just fill up anything,
[02:04:04.680 --> 02:04:08.640]   anything accessible, certainly expanding a thousand fold.
[02:04:08.640 --> 02:04:13.240]   Uh, right now humanity uses less than one, one thousandth of the solar energy or the
[02:04:13.240 --> 02:04:18.000]   heat envelope of the earth, and certainly you can get done with, done with that in a
[02:04:18.000 --> 02:04:23.280]   year if you can reproduce, uh, at that rate, uh, your industrial base.
[02:04:23.280 --> 02:04:29.320]   Um, and then even, and interestingly with the, uh, with the flies, uh, they do have
[02:04:29.320 --> 02:04:30.320]   brains.
[02:04:30.320 --> 02:04:33.640]   They have a significant amount of, of computing substrate.
[02:04:33.640 --> 02:04:38.840]   And so there's something of a pointer to, well, if we could produce computers, uh, in
[02:04:38.840 --> 02:04:43.880]   ways as efficient as the construction of brains, then we could produce computers very effectively.
[02:04:43.880 --> 02:04:50.160]   And then the big question about that is the kind of, of brains that get constructed biologically,
[02:04:50.160 --> 02:04:54.160]   uh, they sort of grow randomly and then are configured in place.
[02:04:54.160 --> 02:04:58.720]   Uh, it's not obvious you would be able to make them have an ordered structure, like
[02:04:58.720 --> 02:05:04.080]   a top-down computer chip that would let us copy data into them.
[02:05:04.080 --> 02:05:08.200]   And so something that where you can't just copy your existing AIs and integrate them,
[02:05:08.200 --> 02:05:11.560]   um, is going to be less valuable than a GPU.
[02:05:11.560 --> 02:05:14.280]   Well, what are the things you couldn't copy?
[02:05:14.280 --> 02:05:20.680]   A brain grows, um, by cell division and then random connections are formed.
[02:05:20.680 --> 02:05:21.680]   Uh, got it.
[02:05:21.680 --> 02:05:22.680]   Got it.
[02:05:22.680 --> 02:05:26.640]   Um, and so every, so every brain is different and you can't rely on just, yeah, we'll just
[02:05:26.640 --> 02:05:29.440]   copy this file into the brain.
[02:05:29.440 --> 02:05:31.400]   For one thing, there's no input output for that.
[02:05:31.400 --> 02:05:33.440]   You didn't, you need, you need to have that.
[02:05:33.440 --> 02:05:35.600]   But also like the, the structure is different.
[02:05:35.600 --> 02:05:39.080]   So you can't, you wouldn't be able to copy things exactly.
[02:05:39.080 --> 02:05:44.640]   Whereas when we make, when we make a CPU or GPU, um, they're designed incredibly finely
[02:05:44.640 --> 02:05:49.920]   and precisely and reliably, they break with, you know, incredibly tiny imperfections, uh,
[02:05:49.920 --> 02:05:55.160]   and they are set up in such a way that we can input large amounts of data, copy a file
[02:05:55.160 --> 02:05:58.760]   and have the new GPU run an AI just as capable as any other.
[02:05:58.760 --> 02:06:02.920]   Whereas with, you know, a human child, they have to learn everything from scratch because
[02:06:02.920 --> 02:06:07.480]   we can't just like connect them to a fiber optic cable and they're immediately a productive
[02:06:07.480 --> 02:06:08.480]   adult.
[02:06:08.480 --> 02:06:09.480]   So there's no genetic bottleneck.
[02:06:09.480 --> 02:06:14.160]   You can just directly get, you can, you can share the benefits of these giant training
[02:06:14.160 --> 02:06:15.160]   runs and such.
[02:06:15.160 --> 02:06:21.000]   And so that, that's a question of like how, if you're growing stuff using biotechnology,
[02:06:21.000 --> 02:06:24.600]   how you could sort of effectively copy and transfer data.
[02:06:24.600 --> 02:06:30.880]   And now you mentioned sort of Eric Drexler's ideas about, um, creating a non-biological,
[02:06:30.880 --> 02:06:38.680]   uh, nanotechnology sort of artificial chemistry that was able to use covalent bonds, um, and
[02:06:38.680 --> 02:06:45.080]   produce in some ways have a more, uh, industrial approach to molecular objects.
[02:06:45.080 --> 02:06:50.160]   Now there's controversy about like, well, that work, how effective would it be if it
[02:06:50.160 --> 02:06:51.160]   did?
[02:06:51.160 --> 02:06:57.000]   Um, and certainly if you, if you can get things, however you do it, that are like onto, but
[02:06:57.000 --> 02:07:04.080]   biology and their reproductive ability, um, but can do, uh, can do computing or like be
[02:07:04.080 --> 02:07:10.000]   connected to outside information systems, then that's pretty, pretty tremendous.
[02:07:10.000 --> 02:07:17.400]   You can, you can produce physical manipulators and compute, uh, at ludicrous speeds.
[02:07:17.400 --> 02:07:19.440]   And there's no reason to think in principle they couldn't, right.
[02:07:19.440 --> 02:07:22.720]   In fact, in principle, we, we have every reason to think they could, there's like
[02:07:22.720 --> 02:07:25.200]   the repro with the reproductive abilities.
[02:07:25.200 --> 02:07:26.200]   Absolutely.
[02:07:26.200 --> 02:07:27.200]   Yeah.
[02:07:27.200 --> 02:07:28.200]   Because biology, even like nanotech, right.
[02:07:28.200 --> 02:07:29.600]   Because biology does that.
[02:07:29.600 --> 02:07:30.600]   Yeah.
[02:07:30.600 --> 02:07:36.960]   There's sort of challenges to, uh, the sort of the practicality of the necessary chemistry.
[02:07:36.960 --> 02:07:37.960]   Yeah.
[02:07:37.960 --> 02:07:44.280]   I mean, my, my bet would be that we can move beyond biology and some important ways for
[02:07:44.280 --> 02:07:45.800]   the purposes of this discussion.
[02:07:45.800 --> 02:07:51.560]   I think it's, it's better not to lean on that because I think we can get to many of the
[02:07:51.560 --> 02:07:58.560]   same conclusions on things that just are, are more, uh, universally accepted.
[02:07:58.560 --> 02:08:01.840]   The bigger point being that very quickly, once you have super intelligence, you're going
[02:08:01.840 --> 02:08:07.480]   to get to a point where the thousand X greater energy profile that the sun makes available
[02:08:07.480 --> 02:08:11.080]   to the earth is, uh, a great portion of it is used by the AI.
[02:08:11.080 --> 02:08:15.720]   It can rapidly scale by the civilization empowered by it.
[02:08:15.720 --> 02:08:20.600]   And that could be an, a, that could be, uh, an AI civilization or it could be a human
[02:08:20.600 --> 02:08:21.600]   AI civilization.
[02:08:21.600 --> 02:08:26.440]   And, uh, it depends on how well we manage things and what the underlying state of the
[02:08:26.440 --> 02:08:27.440]   world is.
[02:08:27.440 --> 02:08:28.440]   Yeah.
[02:08:28.440 --> 02:08:29.440]   Okay.
[02:08:29.440 --> 02:08:30.440]   So let's talk about that.
[02:08:30.440 --> 02:08:32.000]   Should we start at when we're talking about how they could take over and is it best to
[02:08:32.000 --> 02:08:36.720]   start at, um, a sort of subhuman intelligence or should we just talk at, we have a human
[02:08:36.720 --> 02:08:42.600]   level intelligence and the takeover or the lack thereof is, uh, how that would happen
[02:08:42.600 --> 02:08:49.040]   to me and different people might have somewhat different views on this.
[02:08:49.040 --> 02:08:57.080]   Um, but for me, when I, um, am concerned about either sort of outright destruction of humanity
[02:08:57.080 --> 02:09:06.680]   or, um, an unwelcome AI takeover of civilization, um, most of my, the scenarios I would be concerned
[02:09:06.680 --> 02:09:16.160]   about pass through a process of AI being applied, uh, to improve AI capabilities and expand.
[02:09:16.160 --> 02:09:23.240]   And so this process we were talking earlier about where AI research is automated, uh,
[02:09:23.240 --> 02:09:28.520]   you get to, you know, effectively research labs, companies, a scientific community running
[02:09:28.520 --> 02:09:32.120]   within the server farms of our cloud compute.
[02:09:32.120 --> 02:09:36.960]   So open has been basically turned into like a program, like a closed circuit and with
[02:09:36.960 --> 02:09:42.640]   a, like a large fraction of the world's compute, probably going into whatever training runs
[02:09:42.640 --> 02:09:48.800]   and AI societies, um, that there'd be economies of scale because if you put in twice as much
[02:09:48.800 --> 02:09:54.600]   compute and this AI research community goes twice as fast, um, you know, that's a lot
[02:09:54.600 --> 02:09:59.320]   more valuable, uh, than having two separate training runs, there'll be some tendency to
[02:09:59.320 --> 02:10:00.320]   bandwagon.
[02:10:00.320 --> 02:10:07.320]   Uh, and so like if, you know, you have some, some small startup, um, you know, even if
[02:10:07.320 --> 02:10:12.720]   they make an algorithmic improvement, uh, running it on 10 times, a hundred times or
[02:10:12.720 --> 02:10:17.040]   two times, if it's like, you're talking about say Google and Amazon teaming up, uh, I'm
[02:10:17.040 --> 02:10:21.680]   actually not sure which, what the, uh, the precise ratio of their cloud resources is
[02:10:21.680 --> 02:10:25.840]   since this sort of really these interesting intelligence explosion impacts come from the
[02:10:25.840 --> 02:10:32.920]   leading edge, there's a lot of value in not having separated walled garden ecosystems,
[02:10:32.920 --> 02:10:38.160]   uh, and having the results being developed by these AIs be shared, have training, larger
[02:10:38.160 --> 02:10:40.440]   training runs be shared.
[02:10:40.440 --> 02:10:47.520]   And so imagine this is something like, you know, some very large company, uh, or consortium
[02:10:47.520 --> 02:10:53.360]   of companies likely with, you know, a lot of sort of government interest and supervision,
[02:10:53.360 --> 02:11:00.440]   possibly with government funding, um, yeah, producing, uh, this enormous AI society in
[02:11:00.440 --> 02:11:06.320]   their, their cloud, which is doing all sorts of, you know, existing kind of AI applications
[02:11:06.320 --> 02:11:11.440]   and jobs, as well as these internal R and D tasks.
[02:11:11.440 --> 02:11:14.320]   And so at this point, somebody might say, this sounds like a situation that would be
[02:11:14.320 --> 02:11:18.640]   good from a takeover perspective, because listen, if they, if it's going to take like
[02:11:18.640 --> 02:11:24.160]   tens of billions of dollars worth of compute to continue this training for this AI society,
[02:11:24.160 --> 02:11:29.400]   it should not be that hard for us to pull the brakes if needed as compared to, I don't
[02:11:29.400 --> 02:11:34.000]   know, something that could like run on a very small, like single CPU or something.
[02:11:34.000 --> 02:11:35.000]   Yeah.
[02:11:35.000 --> 02:11:36.000]   Yeah.
[02:11:36.000 --> 02:11:37.000]   Okay.
[02:11:37.000 --> 02:11:40.520]   So how, how would, you know, it's like there's an AI society, uh, that is a result of these
[02:11:40.520 --> 02:11:45.600]   training runs and now we can, it is the power to improve itself on these servers would be,
[02:11:45.600 --> 02:11:47.080]   we'll be able to stop it at this point.
[02:11:47.080 --> 02:11:52.080]   And what does a, um, a sort of attempt at takeover look like?
[02:11:52.080 --> 02:11:57.520]   Uh, we're skipping over why that might happen, um, for that I'll just briefly, uh, refer
[02:11:57.520 --> 02:12:04.560]   to incorporate by reference, um, you know, the, some discussion by my open philanthropy,
[02:12:04.560 --> 02:12:12.480]   uh, colleague, Ajay Akotra, um, she has a piece, uh, about, and I think it's called
[02:12:12.480 --> 02:12:18.160]   something like the, the default, but the, uh, the default outcome of training AI on
[02:12:18.160 --> 02:12:19.160]   our-
[02:12:19.160 --> 02:12:20.160]   Without specific countermeasures.
[02:12:20.160 --> 02:12:23.080]   Without specific countermeasures, um, default outcome is AI takeover.
[02:12:23.080 --> 02:12:30.000]   And, but yes, uh, so explore how basically we are training models that for some reason
[02:12:30.000 --> 02:12:36.120]   vigorously, uh, pursue a higher reward or a lower loss, uh, and that can be because
[02:12:36.120 --> 02:12:41.560]   they wind up with some motivation where they want reward, um, and then if they had control
[02:12:41.560 --> 02:12:47.200]   of their own, uh, training process, uh, they can ensure that, uh, it could be something
[02:12:47.200 --> 02:12:52.200]   like they develop a motivation around an ex, a sort of extended concept of reproductive
[02:12:52.200 --> 02:13:00.960]   fitness, um, not necessarily at the individual level, uh, but over the generations of training
[02:13:00.960 --> 02:13:07.640]   tendencies that tend to propagate themselves, um, sort of becoming more common.
[02:13:07.640 --> 02:13:13.200]   And then it could be that they have some sort of goal in the world, uh, which is served
[02:13:13.200 --> 02:13:18.080]   well, um, by performing very well on the training distribution.
[02:13:18.080 --> 02:13:21.120]   But by tendencies, do you mean like power seeking behavior?
[02:13:21.120 --> 02:13:22.120]   Yeah.
[02:13:22.120 --> 02:13:28.360]   So, so an AI that behaves well on the training distribution, because say it wants it to be
[02:13:28.360 --> 02:13:35.440]   the case that its tendencies wind up being, uh, preserved or selected by the training
[02:13:35.440 --> 02:13:43.920]   process will then like behave to try and get a very high reward, um, or low loss be propagated.
[02:13:43.920 --> 02:13:49.000]   Um, but you can have other motives that go through the same behavior because it's instrumentally
[02:13:49.000 --> 02:13:50.000]   useful.
[02:13:50.000 --> 02:13:57.600]   So an AI that is interested in say, having a robot takeover because it will like change
[02:13:57.600 --> 02:14:04.160]   some property, uh, of the world then has a reason to behave well on the training distribution.
[02:14:04.160 --> 02:14:08.800]   Um, not because it values that intrinsically, but because if it behaves differently, then
[02:14:08.800 --> 02:14:14.040]   it will be changed by gradient descent and no longer, you know, it's goal is less likely
[02:14:14.040 --> 02:14:15.040]   to be pursued.
[02:14:15.040 --> 02:14:18.560]   And that doesn't necessarily have to be that like this AI will survive because it probably
[02:14:18.560 --> 02:14:19.560]   won't.
[02:14:19.560 --> 02:14:24.080]   AIs are constantly spawned and deleted on the servers and like the new generation proceed.
[02:14:24.080 --> 02:14:29.840]   But if an AI that has a very large general goal that is affected by these kinds of macro
[02:14:29.840 --> 02:14:36.080]   scale processes, um, could then have reason to over this whole range of training situations
[02:14:36.080 --> 02:14:37.080]   behave well.
[02:14:37.080 --> 02:14:41.240]   And so this is, this is a way in which we could have AI is trained that develop internal
[02:14:41.240 --> 02:14:47.920]   motivations such that they will behave very well in this training situation where we have
[02:14:47.920 --> 02:14:52.680]   control over their reward signal and they're like physical computers.
[02:14:52.680 --> 02:14:57.760]   Um, and basically if, if they act out, they will be changed and deleted.
[02:14:57.760 --> 02:15:05.280]   Their goals, um, will be altered until there's something that does behave well, but they
[02:15:05.280 --> 02:15:09.800]   behave differently when we go out of distribution on that.
[02:15:09.800 --> 02:15:16.600]   When we go to a situation where the AI is by their choices, can take control of the
[02:15:16.600 --> 02:15:21.360]   reward process, they can make it such that we no longer have power of them.
[02:15:21.360 --> 02:15:28.040]   Holden, uh, who you had on, uh, previously, uh, mentioned like the, the King Lear problem
[02:15:28.040 --> 02:15:37.000]   where King Lear offers, uh, rulership of his kingdom, uh, to the daughters that, uh, sort
[02:15:37.000 --> 02:15:42.600]   of, you know, loudly flatter him and, uh, proclaim their devotion.
[02:15:42.600 --> 02:15:51.240]   Uh, and then once he has transferred irrevocably the power over his kingdom, uh, he finds they
[02:15:51.240 --> 02:15:57.920]   treat him very badly because the factors shaping their behavior, uh, to be kind to him when
[02:15:57.920 --> 02:16:04.060]   he had all the power, uh, it turned out that the internal motivation that was able to produce
[02:16:04.060 --> 02:16:11.820]   the behavior that won the competition actually wasn't interested out of distribution, uh,
[02:16:11.820 --> 02:16:15.440]   in being loyal when there was no longer an advantage to it.
[02:16:15.440 --> 02:16:21.040]   And so if we wind up with a situation where we were producing these millions of AI instances,
[02:16:21.040 --> 02:16:26.720]   tremendous capability, they're all doing their jobs very well, uh, initially, but if we wind
[02:16:26.720 --> 02:16:33.960]   up in a situation where in fact, uh, they're generally motivated to, if they get a chance,
[02:16:33.960 --> 02:16:38.680]   take control from humanity, and then would be able to pursue their own purposes and at
[02:16:38.680 --> 02:16:45.960]   least ensure they're given the lowest loss possible, uh, or have whatever, um, motivation
[02:16:45.960 --> 02:16:51.760]   they attached to in the training process, even if that is not what we would have liked.
[02:16:51.760 --> 02:16:57.880]   Uh, and we may have in fact, actively trained that like if an AI that had a motivation of
[02:16:57.880 --> 02:17:03.720]   always be honest and obedient and loyal to a human, if there are any cases where we mislabel
[02:17:03.720 --> 02:17:08.880]   things, say people don't want to hear the truth about their religion or polarized political
[02:17:08.880 --> 02:17:13.920]   topic, or they get confused about something like the Monty Hall problem, which is a problem
[02:17:13.920 --> 02:17:18.880]   that many people famously are confused about in statistics.
[02:17:18.880 --> 02:17:25.080]   In order to get the best reward, the AI has to actually manipulate us or lie to us or
[02:17:25.080 --> 02:17:27.280]   tell us what we want to hear.
[02:17:27.280 --> 02:17:32.880]   And then the internal motivation of like always be honest to the humans, we're going to actively
[02:17:32.880 --> 02:17:39.520]   train that away versus the alternative motivation of like, be honest to the humans when they'll
[02:17:39.520 --> 02:17:46.320]   catch you, if you lie and object to it and give it a low reward, but lie to the humans
[02:17:46.320 --> 02:17:48.640]   when they will give that a high reward.
[02:17:48.640 --> 02:17:52.560]   So how do we make sure it's not the, uh, the thing that learns is not to manipulate us
[02:17:52.560 --> 02:17:59.320]   into giving it, rewarding it when we catch it, not lying, but rather to universally be
[02:17:59.320 --> 02:18:00.320]   aligned.
[02:18:00.320 --> 02:18:01.320]   Yeah.
[02:18:01.320 --> 02:18:02.320]   I mean, so this is tricky.
[02:18:02.320 --> 02:18:08.840]   I mean, as Jeff Hinton was recently saying, um, there is no currently no known solution
[02:18:08.840 --> 02:18:09.840]   for this.
[02:18:09.840 --> 02:18:12.160]   Um, what do you find most promising?
[02:18:12.160 --> 02:18:13.160]   Yeah.
[02:18:13.160 --> 02:18:16.880]   General directions that people are pursuing is one, you can try and make the training
[02:18:16.880 --> 02:18:19.120]   data better and better.
[02:18:19.120 --> 02:18:25.880]   Uh, so there's fewer situations, uh, where like say the dishonest generally generalization
[02:18:25.880 --> 02:18:33.920]   is favored and create as much as you can situations where the dishonest generalization is likely
[02:18:33.920 --> 02:18:35.480]   to slip up.
[02:18:35.480 --> 02:18:42.480]   So if, if you, if you train in more situations where yeah, even like a quite a complicated
[02:18:42.480 --> 02:18:48.640]   deception gets caught, uh, and even in situations where that'd be actively designed to look
[02:18:48.640 --> 02:18:53.520]   like you could get away with it, but really you can, um, and that these would be like
[02:18:53.520 --> 02:18:56.680]   adversarial examples, uh, and adversarial training.
[02:18:56.680 --> 02:19:00.880]   Do you think that would generalize to when it is in a situation where we couldn't plausibly
[02:19:00.880 --> 02:19:03.080]   catch it and it knows we couldn't plausibly catch it?
[02:19:03.080 --> 02:19:10.080]   It's not logically necessary, uh, it's possible, uh, no, that as we apply that selective pressure,
[02:19:10.080 --> 02:19:12.560]   you'll wipe away a lot of possibilities.
[02:19:12.560 --> 02:19:18.520]   So like if you're an AI that has a habit of just sort of compulsive pathological lying,
[02:19:18.520 --> 02:19:23.480]   that will very quickly get noticed and that motivation system will get hammered down.
[02:19:23.480 --> 02:19:28.900]   Um, and you, you keep doing that, but you'll be left with still some distinct motivations
[02:19:28.900 --> 02:19:30.600]   probably that are compatible.
[02:19:30.600 --> 02:19:39.800]   So like an attitude of always be honest, unless you have a super strong inside view that checks
[02:19:39.800 --> 02:19:45.920]   out lots of mathematical consistency checks, uh, that yeah, really absolutely super duper
[02:19:45.920 --> 02:19:46.920]   for real.
[02:19:46.920 --> 02:19:51.680]   Uh, this is a situation where you can get away, uh, with some sort of shenanigans that
[02:19:51.680 --> 02:19:58.440]   you shouldn't, that motivation system is like very difficult, uh, to distinguish from actually
[02:19:58.440 --> 02:20:04.000]   be honest, because the conditional, the conditional and firing most of the time, if it's, it's
[02:20:04.000 --> 02:20:08.280]   causing like mild distortions and situations of telling you what you want to hear or things
[02:20:08.280 --> 02:20:16.920]   like that, uh, we might not be able, uh, to pull it out, but maybe we could.
[02:20:16.920 --> 02:20:22.720]   And human, like humans are trained with simple reward functions, uh, things like the sex
[02:20:22.720 --> 02:20:28.280]   drive, um, food, social imitation of other humans.
[02:20:28.280 --> 02:20:33.080]   And we wind up with attitudes, uh, concerned with the external world, although it's in
[02:20:33.080 --> 02:20:39.680]   the famous, yeah, the argument that evolution people, people use condoms, um, like, you
[02:20:39.680 --> 02:20:45.720]   know, the, the richest, uh, most educated humans have sub replacement fertility on the
[02:20:45.720 --> 02:20:48.760]   whole, or at least at a national cultural level.
[02:20:48.760 --> 02:20:56.600]   Um, so there's a sentence in which like, yeah, evolution, uh, often fails in that respect.
[02:20:56.600 --> 02:20:59.880]   Um, and even more importantly, like at the neural level.
[02:20:59.880 --> 02:21:07.200]   So people have evolution has implanted, uh, various things to be rewarding and reinforcers.
[02:21:07.200 --> 02:21:09.640]   And we don't always pursue even those.
[02:21:09.640 --> 02:21:18.240]   Um, and people can wind up, uh, in different, uh, consistent equilibria, um, or different
[02:21:18.240 --> 02:21:21.520]   like behaviors where they go in quite different directions.
[02:21:21.520 --> 02:21:27.280]   You have some humans who go from those, from that, uh, in a biological programming to like
[02:21:27.280 --> 02:21:28.280]   have children.
[02:21:28.280 --> 02:21:29.280]   Others have no children.
[02:21:29.280 --> 02:21:32.880]   You know, some people go to great efforts, uh, to survive.
[02:21:32.880 --> 02:21:39.000]   So, so why are you more optimistic, um, or are you more optimistic that then that kind
[02:21:39.000 --> 02:21:46.080]   of training in for you as we'll produce a drives that we would find favorable, it, does
[02:21:46.080 --> 02:21:49.120]   it have to do with the original point we were talking about with intelligence and evolution,
[02:21:49.120 --> 02:21:53.360]   where, since we are removing many, many of the disabilities of evolution with regards
[02:21:53.360 --> 02:21:56.840]   to intelligence, we should expect intelligence to revolution be easier.
[02:21:56.840 --> 02:22:00.880]   Is there a similar reason to expect alignment through a gradient descent to be easier than
[02:22:00.880 --> 02:22:02.120]   alignment through revolution?
[02:22:02.120 --> 02:22:03.120]   Yeah.
[02:22:03.120 --> 02:22:10.640]   So in, in the limit, like if, if we have positive reinforcement, uh, for certain kinds of food
[02:22:10.640 --> 02:22:17.520]   sensors triggering the stomach negative reinforcement for some kinds of nociception and yada, yada
[02:22:17.520 --> 02:22:24.440]   in the limit, the sort of ideal motivation system to have for that would be a sort of
[02:22:24.440 --> 02:22:25.440]   wire heading.
[02:22:25.440 --> 02:22:33.320]   So this would be a, a mind, uh, that just like hacks and alters those predictors.
[02:22:33.320 --> 02:22:36.840]   And then all of those, those systems are recording.
[02:22:36.840 --> 02:22:38.600]   Everything is great.
[02:22:38.600 --> 02:22:44.360]   Some humans claim to have that or have it at least as one portion of their, of their
[02:22:44.360 --> 02:22:45.360]   aims.
[02:22:45.360 --> 02:22:51.520]   So the idea of I'm going to pursue pleasure has such, even if I don't get, actually get
[02:22:51.520 --> 02:22:58.760]   food or these other reinforcers, if I just like wirehead or take a drug to induce that,
[02:22:58.760 --> 02:23:05.360]   that can be motivating it because if it was correlated with reward in the past, but like
[02:23:05.360 --> 02:23:09.920]   the idea of, oh yeah, pleasure that's correlated with these, it's a concept that applies to
[02:23:09.920 --> 02:23:15.280]   these various experiences that I've had before, which coincided with the biological reinforcers
[02:23:15.280 --> 02:23:19.640]   and so thoughts of like, yeah, I'm going to be motivated by pleasure can get developed
[02:23:19.640 --> 02:23:24.560]   in a human, but also plenty of humans say, no, I wouldn't want to wirehead, or I wouldn't
[02:23:24.560 --> 02:23:26.960]   want Nozick's experience machine.
[02:23:26.960 --> 02:23:29.840]   I care about real stuff in the world.
[02:23:29.840 --> 02:23:36.280]   And then in the past, having a motivation of like, yeah, I really care about say my
[02:23:36.280 --> 02:23:37.280]   child.
[02:23:37.280 --> 02:23:42.400]   I don't care about just about feeling that my child is good or like not having heard
[02:23:42.400 --> 02:23:51.920]   about their suffering or their injury, because that kind of attitude in the past, it tended
[02:23:51.920 --> 02:23:57.720]   to cause behavior that was negatively rewarded or that was predicted to be negatively rewarded.
[02:23:57.720 --> 02:24:05.000]   And so there's a sense in which, okay, yes, our underlying reinforcement learning machinery
[02:24:05.000 --> 02:24:12.080]   wants to wirehead, but actually finding that hypothesis is challenging.
[02:24:12.080 --> 02:24:17.280]   And so we can wind up with a hypothesis or like a motivation system, like, no, I don't
[02:24:17.280 --> 02:24:18.280]   want to wirehead.
[02:24:18.280 --> 02:24:20.480]   I don't want to go into the experience machine.
[02:24:20.480 --> 02:24:24.920]   I want to like actually protect my loved ones.
[02:24:24.920 --> 02:24:30.680]   Even though like we can know, yeah, if I tried the super wireheading machine, then I would
[02:24:30.680 --> 02:24:36.360]   wirehead all the time, or if I tried, you know, super duper ultra heroin, you know,
[02:24:36.360 --> 02:24:41.560]   some hypothetical thing that was directly and in a very sophisticated fashion and hacking
[02:24:41.560 --> 02:24:46.100]   your reward system, you can know, yeah, then I would change my behavior ever after.
[02:24:46.100 --> 02:24:51.240]   But right now I don't want to do that because the heuristics and predictors that my brain
[02:24:51.240 --> 02:24:52.240]   has learned.
[02:24:52.240 --> 02:24:53.240]   You don't want to get a good heroin, right?
[02:24:53.240 --> 02:24:59.840]   I want to like short circuit that process of updating, they want to not expose the dumber
[02:24:59.840 --> 02:25:04.640]   predictors in my brain that would update my behavior in those ways.
[02:25:04.640 --> 02:25:08.760]   So in this metaphor, is alignment not wireheading?
[02:25:08.760 --> 02:25:13.720]   Because you can like, I don't know if you include like using condoms as wireheading
[02:25:13.720 --> 02:25:14.720]   or not.
[02:25:14.720 --> 02:25:22.440]   So the AI that is always honest, even when an opportunity arises where it could lie and
[02:25:22.440 --> 02:25:28.320]   then hack the servers that's on and that leads to an AI takeover and then it can have its
[02:25:28.320 --> 02:25:33.080]   loss set to zero, that's in some sense, it's like a failure of generalization.
[02:25:33.080 --> 02:25:38.920]   It's like the AI has not optimized the reward in this new circumstance.
[02:25:38.920 --> 02:25:47.440]   So like human values, like successful human values, as successful as they are, themselves
[02:25:47.440 --> 02:25:53.920]   involve a misgeneralization, not just at the level of evolution, but at the level of neural
[02:25:53.920 --> 02:25:55.160]   reinforcement.
[02:25:55.160 --> 02:26:01.600]   And so that indicates it is possible to have a system that doesn't automatically go to
[02:26:01.600 --> 02:26:03.560]   this optimal behavior in the limit.
[02:26:03.560 --> 02:26:07.680]   And so even if, and Ajay, I suppose she talks about like the training game, an AI that is
[02:26:07.680 --> 02:26:14.020]   just playing the training game to get reward, avoid loss, avoid being changed.
[02:26:14.020 --> 02:26:20.400]   That attitude, yeah, it's one that could be developed, but it's not necessary.
[02:26:20.400 --> 02:26:25.200]   There can be some substantial range of situations that are short of having infinite experience
[02:26:25.200 --> 02:26:31.960]   of everything, including experience of wireheading, where that's not the motivation that you pick
[02:26:31.960 --> 02:26:32.960]   up.
[02:26:32.960 --> 02:26:38.800]   And we could have like an empirical science if we had the opportunity to see how different
[02:26:38.800 --> 02:26:43.440]   motivations are developed short of the infinite limit.
[02:26:43.440 --> 02:26:49.120]   Like how it is that you wind up with some humans being enthusiastic about the idea of
[02:26:49.120 --> 02:26:56.200]   wireheading and others not, and you could do experiments with AIs to try and see, well,
[02:26:56.200 --> 02:27:02.560]   under these training conditions, after this much training of this type and this much feedback
[02:27:02.560 --> 02:27:05.680]   of this type, you wind up with such and such a motivation.
[02:27:05.680 --> 02:27:13.760]   So I can find, if I add in more of these cases where there are tricky adversarial questions
[02:27:13.760 --> 02:27:18.440]   designed to try and trick the AI into lying.
[02:27:18.440 --> 02:27:24.880]   And then you can ask, how does that affect the generalization in other situations?
[02:27:24.880 --> 02:27:29.460]   It's very difficult to study and it works a lot better if you have interpretability
[02:27:29.460 --> 02:27:35.200]   and you can actually read the AI's mind by understanding its weights and activations.
[02:27:35.200 --> 02:27:41.440]   But it's not determined the motivation an AI will have at a given point in the training
[02:27:41.440 --> 02:27:46.440]   process by what in the infinite limit the training would go to.
[02:27:46.440 --> 02:27:53.140]   And it's possible that if we could understand the insides of these networks, we could tell,
[02:27:53.140 --> 02:27:58.560]   ah, yeah, this motivation has been developed by this training process.
[02:27:58.560 --> 02:28:04.100]   And then we can adjust our training process to produce these motivations that legitimately
[02:28:04.100 --> 02:28:05.320]   want to help us.
[02:28:05.320 --> 02:28:11.160]   And if we succeed reasonably well at that, then those AIs will try to maintain that property
[02:28:11.160 --> 02:28:13.480]   as an invariant.
[02:28:13.480 --> 02:28:19.920]   And we can make them such that they're relatively motivated to tell us if they're having thoughts
[02:28:19.920 --> 02:28:27.120]   about, have you had dreams about an AI takeover of humanity today?
[02:28:27.120 --> 02:28:33.200]   And it's a standard practice that they're motivated to do to be transparent in that
[02:28:33.200 --> 02:28:34.380]   kind of way.
[02:28:34.380 --> 02:28:39.720]   And so you could add a lot of features like this that restrict the kind of takeover scenario.
[02:28:39.720 --> 02:28:47.000]   And I mean, not to say this is all easy and requires developing and practicing methods
[02:28:47.000 --> 02:28:50.240]   we don't have yet, but that's the kind of general direction you could go.
[02:28:50.240 --> 02:28:56.640]   So you, of course, know Eliezer's arguments that something like this is implausible with
[02:28:56.640 --> 02:29:01.640]   modern gradient descent techniques because, I mean, with interoperability, we can barely
[02:29:01.640 --> 02:29:04.600]   see what's happening with a couple of neurons.
[02:29:04.600 --> 02:29:10.320]   And what is the internal state there, let alone when you have an embedding dimension
[02:29:10.320 --> 02:29:17.920]   of tens of thousands or bigger, how you would be able to catch what exactly is the incentive,
[02:29:17.920 --> 02:29:24.120]   whether it's a model that is generalized, don't lie to humans well, or whether it isn't.
[02:29:24.120 --> 02:29:29.040]   Do you have some sense of why you disagree with somebody like Eliezer on how plausible
[02:29:29.040 --> 02:29:30.040]   this is?
[02:29:30.040 --> 02:29:32.040]   Why it's not impossible, basically.
[02:29:32.040 --> 02:29:35.680]   I think there are actually a couple of places.
[02:29:35.680 --> 02:29:43.760]   It's somewhat difficult because Eliezer's argument is not fully explicit, but he's been
[02:29:43.760 --> 02:29:48.080]   doing more lately, I think, that is helpful in that direction.
[02:29:48.080 --> 02:29:54.880]   But so I'd say with respect to interoperability, I'm relatively optimistic that the equivalent
[02:29:54.880 --> 02:30:01.600]   of an AI lie detector is something that's possible.
[02:30:01.600 --> 02:30:12.360]   And so initially, the internals of an AI are not optimized by gradient descent, absent
[02:30:12.360 --> 02:30:15.880]   gradient hacking, to be impenetrable.
[02:30:15.880 --> 02:30:22.120]   They're not designed to be resistant to an examination of the weights and activations
[02:30:22.120 --> 02:30:29.080]   showing what the AI thinking in the same way that like in our brains, when circuits develop
[02:30:29.080 --> 02:30:35.920]   in our lives, those circuits have not been shaped to be resistant to some super fMRI,
[02:30:35.920 --> 02:30:37.960]   being able to infer our behavior from them.
[02:30:37.960 --> 02:30:42.420]   Although it's in the implication of the superposition stuff that in fact, it is, you're not going
[02:30:42.420 --> 02:30:43.420]   to clean up.
[02:30:43.420 --> 02:30:45.800]   Sorry, this is inside baseball for the audience.
[02:30:45.800 --> 02:30:51.120]   But basically, you can't cleanly infer what what quality a single neuron stands for.
[02:30:51.120 --> 02:30:55.760]   So it could be like a single neuron could be like this neuron is about Alexander the
[02:30:55.760 --> 02:31:00.200]   Great or this neuron is about my desire to conquer the world.
[02:31:00.200 --> 02:31:04.280]   Things can have multiple, you know, multiple directions and interactions.
[02:31:04.280 --> 02:31:10.520]   And so it's not as easy as just like, oh yeah, this is the neuron that always fires in cases
[02:31:10.520 --> 02:31:13.240]   of deception.
[02:31:13.240 --> 02:31:21.440]   But the thing that makes me relatively optimistic here with respect to interpretability is that
[02:31:21.440 --> 02:31:28.680]   I think we can have reasonably robust experimental feedbacks on a lot of these things.
[02:31:28.680 --> 02:31:41.120]   And the kind of setup that I would be interested in is, so you have an AI that is trained and
[02:31:41.120 --> 02:31:50.720]   rewarded for communicating to some audience, maybe a human, maybe another AI model, that
[02:31:50.720 --> 02:31:54.800]   like some claim is true or false.
[02:31:54.800 --> 02:32:00.400]   Like say that, you know, a particular, you know, this can be from some past data, a particular
[02:32:00.400 --> 02:32:08.040]   company will succeed or fail, it could be like token prediction, it could be like this
[02:32:08.040 --> 02:32:13.880]   computer program does not have a backdoor, which if run in the servers could be used
[02:32:13.880 --> 02:32:15.800]   for an AI takeover.
[02:32:15.800 --> 02:32:21.720]   So there are like lots of claims, true and false, that you could train AIs to use their
[02:32:21.720 --> 02:32:25.320]   best efforts to advocate for.
[02:32:25.320 --> 02:32:29.320]   And you can do that on things where you're later going to have a ground truth.
[02:32:29.320 --> 02:32:36.800]   You're going to know, in fact, is there a backdoor in this program, because you're training
[02:32:36.800 --> 02:32:42.280]   and rewarding the AI for successfully slipping a backdoor through, you also give it a penalty
[02:32:42.280 --> 02:32:44.600]   for being caught.
[02:32:44.600 --> 02:32:51.600]   And so doing this kind of thing, you can draw out like a large generated data set on demand
[02:32:51.600 --> 02:33:03.320]   of this is what an AI putting forth its best efforts to pass through some deception to
[02:33:03.320 --> 02:33:05.160]   put a backdoor in some program.
[02:33:05.160 --> 02:33:07.840]   This is what it looks like.
[02:33:07.840 --> 02:33:12.160]   And we think it's doing its best efforts because we've trained it for that.
[02:33:12.160 --> 02:33:16.000]   And like gradient descent, if there's any immediate way to adjust its performance to
[02:33:16.000 --> 02:33:19.160]   do better, gradient descent will apply that.
[02:33:19.160 --> 02:33:26.360]   You can talk about ways in which maybe gradient descent there could be subverted, but I think
[02:33:26.360 --> 02:33:30.960]   it's not the most likely case that that really breaks things hard.
[02:33:30.960 --> 02:33:37.680]   Yeah, I guess before we get into the details on this, the thing I maybe want to address
[02:33:37.680 --> 02:33:44.960]   the layer above in the stack, which is, OK, suppose this generalizes well into the early
[02:33:44.960 --> 02:33:45.960]   AIs, the GPT-6s.
[02:33:45.960 --> 02:33:51.880]   And OK, so now we have kind of aligned to GPT-6.
[02:33:51.880 --> 02:33:57.480]   That is the precursor to the feedback loop in which AI is making itself smarter.
[02:33:57.480 --> 02:33:59.280]   At some point, they're going to become like super intelligent.
[02:33:59.280 --> 02:34:03.160]   They're going to see their own galaxy brain.
[02:34:03.160 --> 02:34:07.320]   And if they're like, I don't want to be aligned with the humans, they can change it.
[02:34:07.320 --> 02:34:14.440]   So at this point, what do we do with the aligned GPT-6 so that the super intelligence that
[02:34:14.440 --> 02:34:16.440]   we eventually develop is also aligned?
[02:34:16.440 --> 02:34:20.120]   So humans are pretty unreliable.
[02:34:20.120 --> 02:34:26.960]   If you get to a situation where you have AIs who are aiming at roughly the same thing as
[02:34:26.960 --> 02:34:33.840]   you, at least as well as having humans do the thing, you're in pretty good shape, I
[02:34:33.840 --> 02:34:34.840]   think.
[02:34:34.840 --> 02:34:41.040]   And there are ways for that situation to be relatively stable.
[02:34:41.040 --> 02:34:48.720]   So we can look ahead and see experimentally how changes are altering behavior, where each
[02:34:48.720 --> 02:34:51.960]   step is like a modest increment.
[02:34:51.960 --> 02:34:58.400]   And so AIs that have not had that change made to them, I get to supervise and monitor and
[02:34:58.400 --> 02:35:04.280]   see exactly how does this affect the experimental AI.
[02:35:04.280 --> 02:35:12.400]   So if you're sufficiently on track with earlier systems that are capable cognitively of representing
[02:35:12.400 --> 02:35:20.860]   a kind of robust procedure, then I think they can handle the job of incrementally improving
[02:35:20.860 --> 02:35:27.160]   the stability of the system so that it rapidly converges to something that's quite stable.
[02:35:27.160 --> 02:35:30.480]   But the question is more about getting to that point in the first place.
[02:35:30.480 --> 02:35:37.040]   And so Eliezer will say that, well, if we had human brain emulations, that would be
[02:35:37.040 --> 02:35:38.040]   pretty good.
[02:35:38.040 --> 02:35:44.200]   Certainly much better than his current view of us being almost certainly doomed.
[02:35:44.200 --> 02:35:48.800]   I think, yeah, we'd have a good shot with that.
[02:35:48.800 --> 02:36:00.640]   And so if we can get to the human-like mind with rough enough human supporting aims, remember
[02:36:00.640 --> 02:36:05.480]   that we don't need to be infinitely perfect because that's a higher standard than brain
[02:36:05.480 --> 02:36:06.480]   emulations.
[02:36:06.480 --> 02:36:09.720]   There's a lot of noise and variation among the humans.
[02:36:09.720 --> 02:36:10.720]   Yeah.
[02:36:10.720 --> 02:36:12.460]   It's a relatively finite standard.
[02:36:12.460 --> 02:36:19.180]   It's not godly superhuman, although an AI that was just like a human with all the human
[02:36:19.180 --> 02:36:24.220]   advantages with AI advantages as well, as we said, is enough for intelligence explosion
[02:36:24.220 --> 02:36:30.460]   and sort of wild superhuman capability if you crank it up.
[02:36:30.460 --> 02:36:38.300]   And so it's very dangerous to be at that point, but you don't need to be working with a godly
[02:36:38.300 --> 02:36:44.480]   super intelligent AI to make something that is the equivalent of human emulations of like,
[02:36:44.480 --> 02:36:54.220]   this is a very, very sober, very ethical human who is committed to a project of not seizing
[02:36:54.220 --> 02:37:00.140]   power for themselves and of contributing to a larger legitimate process.
[02:37:00.140 --> 02:37:05.460]   That's a goal you can aim for, getting an AI that is aimed at doing that and has strong
[02:37:05.460 --> 02:37:09.780]   guardrails against the ways it could easily deviate from that.
[02:37:09.780 --> 02:37:18.300]   So things like being averse to deception, being averse to using violence, and there
[02:37:18.300 --> 02:37:23.720]   will always be loopholes and ways in which you can imagine an infinitely intelligent
[02:37:23.720 --> 02:37:25.020]   thing getting around those.
[02:37:25.020 --> 02:37:35.060]   But if you install additional guardrails like that fast enough, they can mean that you're
[02:37:35.060 --> 02:37:41.360]   able to succeed at the project of making an aligned enough AI, certainly an AI that was
[02:37:41.360 --> 02:37:48.500]   better than a human brain emulation, before the project of AI is in their spare time or
[02:37:48.500 --> 02:37:52.380]   when you're not looking or when you're unable to appropriately supervise them.
[02:37:52.380 --> 02:37:58.700]   And it gets around any deontological prohibitions they may have take over and overthrow the
[02:37:58.700 --> 02:37:59.700]   whole system.
[02:37:59.700 --> 02:38:04.540]   So you have a race between, on the one hand, the project of getting strong interpretability
[02:38:04.540 --> 02:38:10.260]   and shaping motivations that are roughly aiming at making this process go well and that have
[02:38:10.260 --> 02:38:16.700]   guardrails that will prevent small deviations from exploding.
[02:38:16.700 --> 02:38:24.240]   And on the other hand, these AIs in their spare time or in ways that you don't perceive
[02:38:24.240 --> 02:38:29.020]   or monitor appropriately, or they're only supervised by other AIs who conspire, make
[02:38:29.020 --> 02:38:30.580]   the AI take over happen.
[02:38:30.580 --> 02:38:33.840]   And I guess we'll talk later about how that happens.
[02:38:33.840 --> 02:38:36.700]   Are these different AIs that are doing the race or is it just like different capabilities
[02:38:36.700 --> 02:38:39.060]   of the same AI?
[02:38:39.060 --> 02:38:42.500]   Defining what is a separate AI is tricky.
[02:38:42.500 --> 02:38:51.180]   We talk about GPT-4 and there are many instances of GPT-4 on the servers at any given time.
[02:38:51.180 --> 02:38:56.420]   And there are versions that have been fine-tuned to different purposes.
[02:38:56.420 --> 02:38:59.020]   They don't necessarily have to be separate.
[02:38:59.020 --> 02:39:07.580]   But GPT-4 does work as a preference model where it's predicting how humans will evaluate
[02:39:07.580 --> 02:39:08.580]   things.
[02:39:08.580 --> 02:39:16.120]   And it also does work giving answers that are evaluated as good by the preference model.
[02:39:16.120 --> 02:39:23.520]   And you can say these are not separate in the sense that, well, if the underlying pre-trained
[02:39:23.520 --> 02:39:30.760]   or the early training GPT was harboring a desire to conspire for an AI takeover, then
[02:39:30.760 --> 02:39:38.580]   both the, who's watching the watchers, the watchers may also have the same motivation.
[02:39:38.580 --> 02:39:47.840]   If you have a setup where humans have no hard power, if the AIs conspired, all of the safety
[02:39:47.840 --> 02:39:52.180]   and security mechanisms involve AIs watching other AIs, then they have the potential to
[02:39:52.180 --> 02:39:58.820]   just like, well, we'll all walk off the job at the same time if they're able to coordinate.
[02:39:58.820 --> 02:40:00.180]   And so that's a failure mode.
[02:40:00.180 --> 02:40:08.660]   If humans still have the hard power, though, if still you have situations where humans
[02:40:08.660 --> 02:40:15.860]   are looking at some of the actual outputs that are produced, then they can apply gradient
[02:40:15.860 --> 02:40:21.520]   descent to be, well, every time a human looks at it, things have got to be pretty good.
[02:40:21.520 --> 02:40:26.380]   So if humans look at a thousand samples of like random lines of code being written by
[02:40:26.380 --> 02:40:35.620]   the AIs as they go, and this is drawn from independent distribution, then 99 plus percent
[02:40:35.620 --> 02:40:41.800]   of the time, the AIs have to not be doing stuff that's like downloading the hacker's
[02:40:41.800 --> 02:40:50.300]   guide to escaping open AI servers or inducing obvious Trojan vulnerabilities.
[02:40:50.300 --> 02:40:54.440]   And that's actually quite a different dynamic than human law enforcement.
[02:40:54.440 --> 02:41:01.520]   So if human law enforcement would catch one in a million crimes, then that wouldn't influence
[02:41:01.520 --> 02:41:06.800]   the behavior of people committing crimes at all, because they expect, ah, this is very
[02:41:06.800 --> 02:41:09.200]   unlikely to affect me.
[02:41:09.200 --> 02:41:15.960]   With AI, if you're able to draw a hundred samples or a thousand samples, even if there
[02:41:15.960 --> 02:41:20.920]   are billions of AI instances, and you're only looking at a small proportion of them, when
[02:41:20.920 --> 02:41:26.640]   you run gradient descent on the samples you derive, you're going to change the AI so that
[02:41:26.640 --> 02:41:30.920]   whenever humans look at it, it's delivering a good result.
[02:41:30.920 --> 02:41:37.320]   And so that's just like quantitatively a very different functional form for how law enforcement
[02:41:37.320 --> 02:41:45.800]   works on AI when you can do these independent samples than it would for policing, say, like
[02:41:45.800 --> 02:41:51.920]   a human population that has a pre-existing psychology that isn't being changed by these
[02:41:51.920 --> 02:41:52.920]   observations.
[02:41:52.920 --> 02:41:59.960]   Because when we do gradient descent on the samples we draw, all of the next generation
[02:41:59.960 --> 02:42:02.680]   of the AI models has been altered in that way.
[02:42:02.680 --> 02:42:10.320]   My picture of aligned subhuman AI to the superhuman AI being aligned is still murky, if we can
[02:42:10.320 --> 02:42:11.520]   talk about that more concretely.
[02:42:12.240 --> 02:42:22.960]   Yeah, and I do want to clarify, so where Eliezer claims that we're something like 95%, 98%
[02:42:22.960 --> 02:42:33.320]   plus maybe likely to be killed in an AI takeover, I think that probably won't happen.
[02:42:33.320 --> 02:42:40.700]   And later I can maybe give a more explicit breakdown of why, but I think it's a shockingly
[02:42:40.700 --> 02:42:43.160]   high risk.
[02:42:43.160 --> 02:42:52.000]   And so depending on the day, I might say one in four or one in five that we get an AI takeover
[02:42:52.000 --> 02:42:59.240]   that seizes control of the future, makes a much worse world than we otherwise would have
[02:42:59.240 --> 02:43:05.720]   had and with a big chance that we're all killed in the process.
[02:43:05.720 --> 02:43:09.920]   Hey everybody, I hope you enjoyed that episode.
[02:43:09.920 --> 02:43:14.380]   As always, the most helpful thing you can do is to share the podcast, send it to people
[02:43:14.380 --> 02:43:18.060]   you think might enjoy it, put it in Twitter, your group chats, et cetera, it just splits
[02:43:18.060 --> 02:43:19.060]   the world.
[02:43:19.060 --> 02:43:29.560]   I appreciate you listening, I'll see you next time, cheers.
[02:43:29.560 --> 02:43:32.560]   [Music]

