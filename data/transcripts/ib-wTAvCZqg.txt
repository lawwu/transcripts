
[00:00:00.000 --> 00:00:18.080]   Good to be here. Good to see a fair number of people. It's early, so I wasn't sure if anyone
[00:00:18.080 --> 00:00:24.340]   would come, but thank you for coming. One quick note, I just put my slides on Twitter. I wasn't
[00:00:24.340 --> 00:00:31.340]   sure of the best way to access everyone. I'm @rlancemartin. If there's another way I can
[00:00:31.340 --> 00:00:38.160]   get everyone's slides, then, yeah, I see some people opening. So the slides will link to
[00:00:38.160 --> 00:00:42.280]   a few different -- it will link to a colab and other notebooks I'm providing, so all the
[00:00:42.280 --> 00:00:47.860]   code will be available for you. Okay, good. I see people finding it. That's fantastic.
[00:00:47.860 --> 00:00:54.060]   If there's a better way, let me know, but I figured this is somewhat easy. Well, it was
[00:00:54.060 --> 00:00:59.060]   great to be here. We have a bit of time. So I think the format is I'll lay out some slides
[00:00:59.060 --> 00:01:04.160]   to set the preliminaries and give the big picture, and then there will be a bunch of time where
[00:01:04.160 --> 00:01:08.340]   I can just walk around, talk to people, because we have, I guess, three hours. So I think the
[00:01:08.340 --> 00:01:13.120]   idea of this will be a hands-on workshop. I provide a bunch of starter code, and also one
[00:01:13.120 --> 00:01:19.220]   of my slides shows it will be a choose-your-own-adventure format. So why don't I kick it off? I think
[00:01:19.220 --> 00:01:25.060]   maybe one or two people are still coming in, but -- so the theme here is building and testing
[00:01:25.060 --> 00:01:32.220]   reliable agents. And let me go to slideshow mode here. And maybe I'll just kind of start
[00:01:32.220 --> 00:01:37.880]   with, like, the very basics. You know, LLM applications follow a general control flow of some sort.
[00:01:37.880 --> 00:01:45.440]   You start -- usually it's a user input. There's some set of steps, and then you end. And you've
[00:01:45.440 --> 00:01:48.720]   heard a lot about chains. You know, when you build applications, oftentimes we talk about
[00:01:48.720 --> 00:01:53.280]   this idea of chains. And chain is just basically, you know, it is some control flow set by the
[00:01:53.280 --> 00:02:00.200]   developer. Again, you start, proceed through some steps, and you end. So retrieval augmented
[00:02:00.200 --> 00:02:05.060]   generation is a super popular application many -- some of you may be familiar with -- basically
[00:02:05.060 --> 00:02:09.920]   refers to retrieving documents from an index and passing them to an LLM. This is a good example
[00:02:09.920 --> 00:02:16.040]   of a chain. It's a control flow set by a user. The question's, you know, provided. Vector
[00:02:16.040 --> 00:02:21.680]   store retrieves them, passed to an LLM. LLM produces an answer. So this is kind of a classic
[00:02:21.680 --> 00:02:27.040]   chain. Now, when you get into agents, there's a lot of different confusing interpretations.
[00:02:27.040 --> 00:02:30.580]   What is an agent? Here's the way I might think about it, which is just a really simple kind
[00:02:30.580 --> 00:02:36.580]   of framing, is agent is just one of the control flows set by an LLM. And so you can imagine
[00:02:36.580 --> 00:02:41.360]   we talked about this process of you start your app, step one, step two. In this case, I have
[00:02:41.360 --> 00:02:45.680]   an LLM in there. An LLM looks at the output of step one, makes the decision. Do I go back
[00:02:45.680 --> 00:02:52.120]   and do I proceed? So that's like the simple way to think about an agent. So again, chains,
[00:02:52.120 --> 00:02:57.060]   developer defined control flow. I set it ahead of time. I follow some set of steps every time.
[00:02:57.060 --> 00:03:01.360]   An agent, an LLM kind of determines the control flow. An LLM makes a decision about where to
[00:03:01.360 --> 00:03:06.080]   go inside my application. That's one simple way to think about it.
[00:03:06.080 --> 00:03:10.300]   Now you hear about function calling a lot. And this is kind of a confusing topic, so I want
[00:03:10.300 --> 00:03:16.840]   to talk through it kind of carefully. Agents typically use function calling to kind of determine
[00:03:16.840 --> 00:03:21.920]   what step to go to. So usually the way this works is what you do is you basically give the
[00:03:21.920 --> 00:03:28.420]   LLM awareness of some number of tools or steps that it can take. So in my little example here,
[00:03:28.420 --> 00:03:33.060]   I define this tool and this little decorator is a line chain thing. But the point is, I have
[00:03:33.060 --> 00:03:40.180]   some step, it's some function. I'm defining it as a tool and I'm binding it to the LLM. So then LLM has
[00:03:40.180 --> 00:03:45.780]   awareness of this tool. And here's the key point. When it sees an input, like what is the output of step two,
[00:03:45.780 --> 00:03:52.280]   it actually produces the payload needed to run that tool. Now, this is often confusing. Remember LLMs are just
[00:03:52.280 --> 00:03:56.920]   string to string. They don't have the ability to magically call some function. What they can do is produce the payload
[00:03:56.920 --> 00:04:02.420]   or arguments needed to run that function and the function name. So really think about tool calling or function calling
[00:04:02.420 --> 00:04:08.280]   as just an LLM producing a structured output. Still, you know, obviously a string, but it's a structured output
[00:04:08.280 --> 00:04:14.780]   that then can be used to call a tool. So that's all the function calling is. And you might have
[00:04:14.780 --> 00:04:20.280]   heard of React agents. So the way to think about this is, it's just basically binding some set of tools to my LLM.
[00:04:20.280 --> 00:04:27.280]   And again, we talked about tool calling. So LLM makes decisions about what step or what tool to use. And you have
[00:04:27.280 --> 00:04:33.780]   some node that will call that tool. So LLM says, okay, run step one. I have some node that runs step one
[00:04:33.780 --> 00:04:39.280]   and passes the output of step one back to my agent. React typically stands for like basically action.
[00:04:39.280 --> 00:04:46.280]   So the LLM chooses the action. A tool is run. It observes the output. That's what goes back to the agent.
[00:04:46.280 --> 00:04:51.280]   Observe that tool response. Thinks about what to do next. Maybe runs another tool. And this runs in a loop until you end.
[00:04:51.280 --> 00:04:58.280]   And usually the end condition is the LLM just outputs a string response, not a tool call. So this is the way to think
[00:04:58.280 --> 00:05:04.280]   about a classic React agent. And it's really flexible. That's the nice thing about a React agent.
[00:05:04.280 --> 00:05:11.280]   So basically, it can implement many different control flows. It can do step one only. Step two, one, two, two, one.
[00:05:11.280 --> 00:05:19.280]   That's the beauty of these open-ended style React agents. And these have a lot of promise. These kind of flexible tool calling agents
[00:05:19.280 --> 00:05:24.280]   were really hyped last year. They're still really hyped. It's really exciting because they're flexible.
[00:05:24.280 --> 00:05:30.280]   They're open-ended. You can give them a task. Give them some tools. They can just execute arbitrary control flows
[00:05:30.280 --> 00:05:36.280]   given those tools to solve open-ended problems. The catch is, and this is kind of the crux of what we're getting to
[00:05:36.280 --> 00:05:42.280]   with this workshop, is they do have poor reliability or they can. So you can get caught if you've played with agents.
[00:05:42.280 --> 00:05:48.280]   So as you can see, they kind of get caught on one step and they keep calling the same tool. And, you know, really this is often
[00:05:48.280 --> 00:05:54.280]   caused by LLM non-determinism. LLMs are non-deterministic. And also errors in tool calling.
[00:05:54.280 --> 00:05:59.280]   So tool calling is kind of tricky. If you think about it, LLM has to basically pick the right tool
[00:05:59.280 --> 00:06:04.280]   given the input and has to pick the right payload. So it has to produce the right inputs needed to run the tool.
[00:06:04.280 --> 00:06:13.280]   And these both can break. So here's a good example. The tool I'm passing is step two. And the LLM is saying the tool name to run is step three.
[00:06:13.280 --> 00:06:21.280]   So that's obviously wrong. Or I'm passing what is step two of the input three. And the LLM says, okay, pass four.
[00:06:21.280 --> 00:06:29.280]   So both of these errors can happen. Tool calling is a tricky thing. And it's exacerbated. If you pass an LLM five tools, ten tools, it actually gets worse.
[00:06:29.280 --> 00:06:36.280]   If you have very long dialogues, it gets worse. And so this idea of open-ended tool calling agents is really promising. It's really exciting.
[00:06:36.280 --> 00:06:42.280]   But it's really challenging because of these issues that we were mentioning here. So this is kind of the buildup here.
[00:06:42.280 --> 00:06:48.280]   Like, so can we envision something in the middle? So again, we talked about chains. They are not flexible, but they're very reliable.
[00:06:48.280 --> 00:06:55.280]   This chain will always run step one, two in order. We talked about like react agents on the other extreme. They're extremely flexible.
[00:06:55.280 --> 00:07:03.280]   They can run any sequence of tool calls. You know, can run step one, two, step one only, two only, two, one. But they do have reliability issues.
[00:07:03.280 --> 00:07:09.280]   So can we imagine something in the middle that's both flexible and reliable? So here's kind of the setup. And this is kind of the intuition.
[00:07:09.280 --> 00:07:16.280]   Like a lot of times in many applications, you have some idea of what you want the thing to do every time.
[00:07:16.280 --> 00:07:23.280]   So some parts of the application may be fixed. Like the developer can set, okay, I always want to run step one and I want to end with step two.
[00:07:23.280 --> 00:07:30.280]   And you can inject an LLM in certain places that you want there to be some kind of branching or kind of optionality in the control flow.
[00:07:30.280 --> 00:07:35.280]   Okay. So this is the motivation for what we call LangGraph.
[00:07:35.280 --> 00:07:42.280]   So LangGraph is basically a library from the LangChain team that can be used to express control flows as graphs.
[00:07:42.280 --> 00:07:48.280]   And it is a very general tool and I put out a bunch of videos on it and we're going to use it today.
[00:07:48.280 --> 00:07:55.280]   And by the end of this, you will all have an agent that runs reliably using LangGraph, hopefully, and we'll see.
[00:07:55.280 --> 00:08:00.280]   So you should test me on that. If things don't work for you, then we'll work it out.
[00:08:00.280 --> 00:08:06.280]   But the idea is kind of this. This graph has some set of nodes and edges.
[00:08:06.280 --> 00:08:10.280]   So nodes you can think about are basically, well, maybe I should start with this.
[00:08:10.280 --> 00:08:18.280]   This graph has something called state. So it's like short term memory that lives across the lifetime of this graph that contains things you want to operate on.
[00:08:18.280 --> 00:08:24.280]   Nodes modify the state in some way. So basically each node can like call a tool and can modify the state.
[00:08:24.280 --> 00:08:29.280]   Edges just make decisions about what node to go to next. Okay. So you basically have this idea of memory.
[00:08:29.280 --> 00:08:31.280]   And this is the same as common agents, right?
[00:08:31.280 --> 00:08:37.280]   Agents are characterized by having tool calling and short term memory as well as planning.
[00:08:37.280 --> 00:08:41.280]   Those same things are present in LangGraph. Memory is the state that lives across your graph.
[00:08:41.280 --> 00:08:49.280]   Tools exist within your nodes. And planning, basically, you can incorporate LLM dictated decision making in the edges of your graph.
[00:08:49.280 --> 00:08:54.280]   So, like, why is this interesting? And where has this been cropping up?
[00:08:54.280 --> 00:08:58.280]   We've actually been seeing this theme crop up a lot of places. So there's a really interesting paper.
[00:08:58.280 --> 00:09:01.280]   There's actually a few I really like. This one's called Corrective Rag.
[00:09:01.280 --> 00:09:06.280]   And the idea is pretty simple. Like with a naive Rag pipeline, you're doing a retrieval.
[00:09:06.280 --> 00:09:08.280]   You're taking retrieved docs and you're generating your answer.
[00:09:08.280 --> 00:09:14.280]   Corrective Rag, like, is doing one step more where it's saying, well, why don't we reflect on the docs we retrieved and ask, are they actually relevant?
[00:09:14.280 --> 00:09:19.280]   You can have lots of issues with retrieval. You can reflect on the documents, see if they're relevant.
[00:09:19.280 --> 00:09:22.280]   If they're not relevant, you can do different things. You can kick out and do a web search.
[00:09:22.280 --> 00:09:25.280]   So it makes your application a lot more dynamic to poor quality retrieval.
[00:09:25.280 --> 00:09:29.280]   So this is one of the first videos I put on LangGraph back in February.
[00:09:29.280 --> 00:09:33.280]   It was very popular. And I basically showed you can build Corrective Rag inside LangGraph.
[00:09:33.280 --> 00:09:38.280]   And it's super simple. This is what the graph looks like. I do retrieval. I grade my documents.
[00:09:38.280 --> 00:09:42.280]   And we're going to actually, we're going to do this today. And I have a bunch of code for you that does exactly this.
[00:09:42.280 --> 00:09:47.280]   So we're going to go way in detail on this one. But this is kind of the setup.
[00:09:47.280 --> 00:09:54.280]   And I showed this working. I showed it works locally with Olama using, at that time it was Mistral 7b.
[00:09:54.280 --> 00:10:02.280]   And it works really well. So this is like one simple illustration of how you can use LangGraph to build kind of a self-reflective or corrective rag application.
[00:10:02.280 --> 00:10:07.280]   Now another cool paper was called Self Rag, which actually looked at the generation.
[00:10:07.280 --> 00:10:11.280]   So basically we're all familiar with the idea of hallucinations. It's a real problem.
[00:10:11.280 --> 00:10:19.280]   Instead of just allowing hallucinations to propagate to the user, you can actually reflect on the answer relative to the documents and catch hallucinations.
[00:10:19.280 --> 00:10:24.280]   If there's hallucinations, you can basically do different things. And they propose a few ideas here.
[00:10:24.280 --> 00:10:28.280]   I implemented this. And this is actually our most popular video of all time.
[00:10:28.280 --> 00:10:33.280]   So this was showing LangGraph in Lama 3 implementing three different things.
[00:10:33.280 --> 00:10:38.280]   Corrective rag, which we just talked about. The self-rag thing of hallucination checking. And this adaptive rag thing.
[00:10:38.280 --> 00:10:43.280]   So I can kind of walk through it. This all runs in LangGraph locally. And I have the notebook here.
[00:10:43.280 --> 00:10:48.280]   If you want to test that today, you definitely could. So that's the point. It's reliable enough to run this whole thing locally.
[00:10:48.280 --> 00:10:54.280]   So what's happening here is I take a question. I route it either to my index or to web search.
[00:10:54.280 --> 00:11:01.280]   I then retrieve documents. I grade them for relevance. If any are not relevant, I kick out and do a web search to supplement my retrieval.
[00:11:01.280 --> 00:11:07.280]   If they're relevant, I generate my answer. I check it for hallucinations. And then I finally check it for answer relevance.
[00:11:07.280 --> 00:11:12.280]   So basically, does it have hallucinations relative to my documents? And does it answer my question?
[00:11:12.280 --> 00:11:16.280]   If all that passes, I finish and return that to the user.
[00:11:16.280 --> 00:11:22.280]   So this is kind of like a complex rag flow. But with LangGraph, you can actually run this on your laptop.
[00:11:22.280 --> 00:11:28.280]   It is reliable enough to run a laptop with LangGraph. And the intuition, again, is that you're constraining the control flow.
[00:11:28.280 --> 00:11:33.280]   You're allowing the LM to make certain decisions, but at very discrete points. If you implement this as a rag agent,
[00:11:33.280 --> 00:11:38.280]   this could be very open ended and a lot of opportunities for breakage. And so that's the real intuition here.
[00:11:38.280 --> 00:11:46.280]   Now, a final theme is Karpathy kind of mentioned this idea of flow engineering related to this alpha codium paper,
[00:11:46.280 --> 00:11:51.280]   a really nice paper on code generation. And the intuition here is produce a code solution.
[00:11:51.280 --> 00:11:56.280]   They tested this on a bunch of coding challenges, produce a code solution and check it against the number of unit tests,
[00:11:56.280 --> 00:12:02.280]   auto-generated or pre-existing. And basically, if it fails unit tests, feedback those error cell.
[00:12:02.280 --> 00:12:07.280]   I'm gonna try again. Really simple idea. I implemented this in LangGraph. Again, the code is here.
[00:12:07.280 --> 00:12:15.280]   And this works really well. So I basically, I share a blog post as well. I ran this on our internal coding.
[00:12:15.280 --> 00:12:22.280]   We have an internal application for rag at LangChain. And we're actually working on implementing this right now in production
[00:12:22.280 --> 00:12:28.280]   because the performance is way better. And a common thing this can fix with code generation and code solutions is
[00:12:28.280 --> 00:12:33.280]   hallucinations and imports. So we see that a lot with our rag app. So what I did was,
[00:12:33.280 --> 00:12:40.280]   I very simply implemented a unit test for import checks. Just run that. It significantly improves performance
[00:12:40.280 --> 00:12:45.280]   relative to without doing it. And so we're actually working on implementing this in our internal rag system.
[00:12:45.280 --> 00:12:53.280]   So super simple idea that can really improve code generation. So, you know, if I kind of like back up, what did we talk about?
[00:12:53.280 --> 00:12:58.280]   I mean, we talked about chains. They are not flexible, which is fine in some cases.
[00:12:58.280 --> 00:13:03.280]   But a lot of interesting newer papers with rag, for example, this idea of self-reflection is really beneficial.
[00:13:03.280 --> 00:13:09.280]   The ability to kind of self-correct applications can be really beneficial and not beyond rag as well for coding.
[00:13:09.280 --> 00:13:16.280]   So chains are very reliable, but they're not flexible. Now, if you go to the other end, like a classic react agent is very flexible.
[00:13:16.280 --> 00:13:20.280]   It can implement any sequence control flows through your different tools.
[00:13:20.280 --> 00:13:25.280]   But it does have reliability problems due to things we talked about, non-determinism, tool calling errors.
[00:13:25.280 --> 00:13:33.280]   And line graph kind of sits in the middle where you can actually implement these user defined slash LLM gated control flows.
[00:13:33.280 --> 00:13:38.280]   And they can actually be extremely reliable because of that constraint.
[00:13:38.280 --> 00:13:42.280]   They are less flexible than a classic react agent. So that is true.
[00:13:42.280 --> 00:13:49.280]   So for very open-ended tasks, I do agree. Maybe you do need, you know, a very open-ended more like autonomous style react agent.
[00:13:49.280 --> 00:13:59.280]   But for a lot of applications that our customers are working on and seeing, these kinds of like hybrid flows are sufficient and you gain a lot of reliability.
[00:13:59.280 --> 00:14:05.280]   And so we've talked to a lot of companies that have implemented line graph successfully for agentic flows for this reason.
[00:14:05.280 --> 00:14:08.280]   Because reliability is just incredibly important in a production setting.
[00:14:08.280 --> 00:14:14.280]   So this gets into, if you look at the slides, I have a few different notebooks.
[00:14:14.280 --> 00:14:21.280]   And what I show is, we talked about corrective rag. These notebooks show how to build corrective rag yourself.
[00:14:21.280 --> 00:14:27.280]   And I thought that's a fun starting application. It's a really popular one. It's super simple.
[00:14:27.280 --> 00:14:31.280]   There are not many dependencies. You can use your own web, whatever tool you want to use your web search.
[00:14:31.280 --> 00:14:35.280]   You can use other things as well. You should have a look at the notebooks and I'll kind of walk around.
[00:14:35.280 --> 00:14:40.280]   And we're going to, we're going to keep going. I'm just wanting to like, this is just like a placeholder here.
[00:14:40.280 --> 00:14:48.280]   But, so if you want to test this locally, if you have a laptop capable of running things locally, then we have a notebook to support that.
[00:14:48.280 --> 00:14:52.280]   I use a llama. I can talk a lot about that. That's a really cool thing.
[00:14:52.280 --> 00:15:00.280]   If you don't, then I have two options for you. So one is a colab. So that's probably the easiest.
[00:15:00.280 --> 00:15:04.280]   If there's issues, let me know if I've tested it. So if you have a Google account, you're going to go over here.
[00:15:04.280 --> 00:15:09.280]   If you have a Google account, you're going to spin up a colab. All I need is a few API keys, depending on what models you want to use.
[00:15:09.280 --> 00:15:13.280]   It's all kind of there. You can set those accordingly. And I also have a notebook.
[00:15:13.280 --> 00:15:20.280]   So this just kind of is like a kind of a, gives you a roadmap of the different things you can try today, since this is a workshop format.
[00:15:20.280 --> 00:15:28.280]   And I'll just be walking around and we'll do questions for a while, but I want to talk about the second half of this, of this, you know, story.
[00:15:28.280 --> 00:15:37.280]   So one of the things we're seeing a lot, I think you're going to hear a lot at this conference is the challenge of testing and evaluation.
[00:15:37.280 --> 00:15:46.280]   And this is a real pain point. Like, for example, how do I actually know that my Landgraf agent is more reliable than the React agent?
[00:15:46.280 --> 00:15:53.280]   How do I know what LLM to use? How do I know what prompt to use? Right? So testing agents is, testing in general is really hard.
[00:15:53.280 --> 00:16:00.280]   And agents in particular is challenging. So there's kind of three types of testing loops I like to think about.
[00:16:00.280 --> 00:16:07.280]   One is this in-app error correction. And that's actually what we just talked about. So Landgraf and Landgraf agents are really good for that.
[00:16:07.280 --> 00:16:13.280]   So basically, in-app error handling, where you can catch and fix errors is really useful for code generation for RAG.
[00:16:13.280 --> 00:16:18.280]   We just talked about that. So that's like placeholder one. Now we get into this idea of pre-production testing.
[00:16:18.280 --> 00:16:23.280]   And then finally production monitoring. And I want to introduce a few ideas on the latter two.
[00:16:23.280 --> 00:16:28.280]   So we just talked through this. Here we're going to build corrective RAG a few different ways.
[00:16:28.280 --> 00:16:33.280]   And I just showed the choose-your-own-adventure stuff. And so this is just kind of reiterating that.
[00:16:33.280 --> 00:16:42.280]   But I want to show you some other things. So Langsmith is a tool from the Langchain team that supports testing and evaluation
[00:16:42.280 --> 00:16:47.280]   as well as monitoring and tracing. And so we've seen a lot of interest in this and it's quite popular.
[00:16:47.280 --> 00:16:51.280]   It is really useful for doing these types of testings and evaluations.
[00:16:51.280 --> 00:16:57.280]   So the key idea behind Langsmith and the notebooks actually have this. So this is totally optional.
[00:16:57.280 --> 00:17:02.280]   If you just want to build an agent, that's totally fine. If you want to also test it, you can use Langsmith.
[00:17:02.280 --> 00:17:05.280]   You don't have to, of course, but I have it all set up to use Langsmith if you want.
[00:17:05.280 --> 00:17:11.280]   It's free to use, of course. And so the idea is there's kind of four components that I like to think about
[00:17:11.280 --> 00:17:16.280]   when it comes to testing/evaluation. You have some data sets. That's some set of examples you want to test on.
[00:17:16.280 --> 00:17:20.280]   So say you have a rag app. That's like a set of ground truth question and answer pairs you've built.
[00:17:20.280 --> 00:17:24.280]   Like you're testing your system. You have question and answer pairs that you know are correct.
[00:17:24.280 --> 00:17:27.280]   Can your system produce those answers? How many will actually get right?
[00:17:27.280 --> 00:17:31.280]   You have your application. That's your agent. That's your rag app. That's your code app.
[00:17:31.280 --> 00:17:33.280]   Whatever that is. That's your application.
[00:17:33.280 --> 00:17:37.280]   Now the thing that's often the trick is you have this evaluator thing.
[00:17:37.280 --> 00:17:42.280]   And the notebooks show you in detail. But this evaluator is something as simple as a user-defined function
[00:17:42.280 --> 00:17:47.280]   that can implement a few different things. You can think about using an LLM to actually judge your output.
[00:17:47.280 --> 00:17:52.280]   So in that case, let's take Rag as an example. My application produces an answer. I have a ground truth answer.
[00:17:52.280 --> 00:17:56.280]   You can actually have an LLM. Look at those two answers jointly in reason. Is it correct?
[00:17:56.280 --> 00:18:03.280]   And this is often very effective. It requires some prompt engineering. I have some nice templates in the notebooks to show you.
[00:18:03.280 --> 00:18:07.280]   But this is something that's very popular. This idea of LLM as judge evaluators is very interesting.
[00:18:07.280 --> 00:18:11.280]   A lot of people -- actually, you'll probably hear about it this week. It's a really good theme.
[00:18:11.280 --> 00:18:15.280]   It's still kind of in development. But that's like one placeholder to keep in mind.
[00:18:15.280 --> 00:18:20.280]   So for one option for time testing is this idea of using LLMs themselves.
[00:18:20.280 --> 00:18:29.280]   The other is building your own heuristic evaluator. So a custom evaluator of some sort. And actually, the notebooks that I share have both.
[00:18:29.280 --> 00:18:35.280]   And so we're actually going to -- the notebooks actually show how to evaluate an agent specifically.
[00:18:35.280 --> 00:18:41.280]   And there's a few different things you can look at with an agent. So one is if you go to that far right in blue, the end-to-end performance.
[00:18:41.280 --> 00:18:48.280]   So our notebooks are basically going to be a RAG agent. The eval set has five questions. And I basically have a set of question-answer pairs.
[00:18:48.280 --> 00:18:54.280]   So basically, I'm going to compare my agent answers to reference answers. And we'll walk through that in the notebook.
[00:18:54.280 --> 00:19:01.280]   But that's kind of one thing. I just want to introduce the idea. So one big idea is you can evaluate the end-to-end performance of your agent, right?
[00:19:01.280 --> 00:19:07.280]   You don't care anything about what's happening inside the end-to-end performance. The other, which I actually like to look at a lot, is this thing on top.
[00:19:07.280 --> 00:19:13.280]   What are actually the tool calls that the agent executed? This is how you can actually test the agent's reasoning.
[00:19:13.280 --> 00:19:19.280]   So what you see often with agents is they can make some weird trajectory of tool calls. It's highly inefficient but still gets to the right answer.
[00:19:19.280 --> 00:19:22.280]   You don't get that if you only look at the answer. You say, oh, okay, it's got the right answer.
[00:19:22.280 --> 00:19:25.280]   But if you look at the trajectory, it's some crazy path. And so you want to actually look at both.
[00:19:25.280 --> 00:19:29.280]   Like how efficient, how correct is the trajectory? And does it get the right answer, right?
[00:19:29.280 --> 00:19:33.280]   And so the notebooks I share actually do both.
[00:19:33.280 --> 00:19:41.280]   Now this is actually an evaluation that I ran. And this data set is public. This is on the agents that we actually just talked about.
[00:19:41.280 --> 00:19:45.280]   So this is kind of what you see when you open Langsmith. So these are different experiment names.
[00:19:45.280 --> 00:19:52.280]   This is just saying like I've run three replicates of each experiment. And these are my aggregate scores.
[00:19:52.280 --> 00:19:58.280]   So this first score is basically the answer correctness thing. And the second score is like the tool use trajectory.
[00:19:58.280 --> 00:20:02.280]   Or like does it use the right reasoning trace? And I can go through my experiments.
[00:20:02.280 --> 00:20:08.280]   So this top one is actually, this is kind of cool. This is actually my local agent running on my laptop with Langraph.
[00:20:08.280 --> 00:20:14.280]   Okay. Is a five question eval set. Small eval set. Just a bunch of it's some very small test examples.
[00:20:14.280 --> 00:20:20.280]   But basically my local agent does not does fine. It does 60% in terms of the ultimate answer.
[00:20:20.280 --> 00:20:25.280]   So that's not amazing. But it does do very well in terms of the tool calling trajectory.
[00:20:25.280 --> 00:20:29.280]   So it's very reliable in terms of reasoning. It's an 8 billion parameter model.
[00:20:29.280 --> 00:20:35.280]   So I basically I think the quality of its outputs are a little bit lower than you might see with larger models.
[00:20:35.280 --> 00:20:41.280]   Now fire function v2 is another option. It's basically a fine-tuned llama 70b from fireworks.
[00:20:41.280 --> 00:20:47.280]   This one with Langraph, so this is actually showing this top, actually gets up to 80%.
[00:20:47.280 --> 00:20:51.280]   So very strong performance in terms of answers. And 100% again in terms of tool calling.
[00:20:51.280 --> 00:20:55.280]   So the key observation here is the tool calling or reasoning is consistent.
[00:20:55.280 --> 00:21:00.280]   Whether you're using a local model or a 70 billion parameter model with Langraph.
[00:21:00.280 --> 00:21:04.280]   So you get very high consistency in your tool calling. The answer quality degrades.
[00:21:04.280 --> 00:21:09.280]   That's more an LLM capacity problem. But the reasoning of the agent is consistent.
[00:21:09.280 --> 00:21:12.280]   So that's the key point. Now here's where it gets interesting.
[00:21:12.280 --> 00:21:18.280]   Fire function v2, again that's llama 70b. This is with a React agent.
[00:21:18.280 --> 00:21:22.280]   What you can see here is the answer quality is degraded. But here's the interesting thing.
[00:21:22.280 --> 00:21:27.280]   The tool calling trajectories are really bad. And this again gets back to that problem with React agents.
[00:21:27.280 --> 00:21:30.280]   They're open-ended. They can choose arbitrary sequences of tool calls.
[00:21:30.280 --> 00:21:33.280]   And you can deviate really quickly from your expected trajectory.
[00:21:33.280 --> 00:21:38.280]   So that's the key intuition here. Now, the final two are GPT-40s.
[00:21:38.280 --> 00:21:41.280]   That's obviously a very flagship model. It's maybe number two now.
[00:21:41.280 --> 00:21:46.280]   Again, on the chatbot arena at least. You know, again, answers ultimately are strong.
[00:21:46.280 --> 00:21:54.280]   The tool calling, though, even here is degraded. So basically it follows some weird trajectories to get to its answers that are unexpected.
[00:21:54.280 --> 00:22:02.280]   So what's the high-level point here? The high-level point is Langraph allows you to significantly constrain the control flow of your app and get higher reliability.
[00:22:02.280 --> 00:22:07.280]   And if you look at these tool calling scores, it's very kind of consistent, going all the way down to local models.
[00:22:07.280 --> 00:22:13.280]   It follows the same sequence every time. React agents kind of go off the rails much more easily.
[00:22:13.280 --> 00:22:16.280]   The answer performance is really a function of the model capacity.
[00:22:16.280 --> 00:22:21.280]   So using an 8 billion parameter model locally, the answer quality is lower than a 70 billion.
[00:22:21.280 --> 00:22:25.280]   That's to be expected. But the reasoning of my app is consistent and strong.
[00:22:25.280 --> 00:22:29.280]   So that's the key thing that you kind of get with Langraph, and this is all public.
[00:22:29.280 --> 00:22:34.280]   And hopefully some of you will actually, you know, implement this or reproduce this today.
[00:22:34.280 --> 00:22:37.280]   And this is just walking through those same insights I just mentioned.
[00:22:37.280 --> 00:22:41.280]   And then deployment, we're going to be talking later this week.
[00:22:41.280 --> 00:22:43.280]   We have an announcement related to deployment of Langraph.
[00:22:43.280 --> 00:22:45.280]   So this is actually a very good setup.
[00:22:45.280 --> 00:22:51.280]   If you're playing with Langraph, you enjoy working with it, we're going to have some really nice options for deploying later this week.
[00:22:51.280 --> 00:22:55.280]   And so Harrison will be here on Thursday to give a keynote on that one.
[00:22:55.280 --> 00:23:00.280]   And if you've deployed, we also have some really nice tools and Langsma to actually monitor deployment.
[00:23:00.280 --> 00:23:02.280]   And this is not as relevant for this workshop.
[00:23:02.280 --> 00:23:06.280]   It's something to just be aware of, I can talk about if you're interested.
[00:23:06.280 --> 00:23:10.280]   So maybe to close out, so this is a really nice write up.
[00:23:10.280 --> 00:23:13.280]   These guys are actually going to give a keynote later this week.
[00:23:13.280 --> 00:23:16.280]   It's Jason and company, Hamel and others.
[00:23:16.280 --> 00:23:20.280]   And they kind of made a really nice point that the model is not the moat.
[00:23:20.280 --> 00:23:22.280]   Like, LMs are always changing.
[00:23:22.280 --> 00:23:25.280]   The moat is really the systems you build around your application.
[00:23:25.280 --> 00:23:26.280]   That's what we talked about today.
[00:23:26.280 --> 00:23:29.280]   Like, do you have an orchestration framework, for example, like Langraph?
[00:23:29.280 --> 00:23:32.280]   Do you have an evaluation chassis like Langsmith?
[00:23:32.280 --> 00:23:33.280]   And again, you don't have to use Langraph.
[00:23:33.280 --> 00:23:35.280]   You don't have to use Langsmith for these things.
[00:23:35.280 --> 00:23:37.280]   But this workshop will introduce these ideas to you.
[00:23:37.280 --> 00:23:42.280]   And frankly, I think it's important just to understand the ideas rather than the implementation.
[00:23:42.280 --> 00:23:45.280]   Whether or not you use Langraph, whether or not you use Langsmith.
[00:23:45.280 --> 00:23:48.280]   I think understanding these principles is still helpful.
[00:23:48.280 --> 00:23:52.280]   But, you know, an evaluation chassis, guardrails, data flywheel.
[00:23:52.280 --> 00:23:56.280]   These are like the components that give you the ability to improve your app over time.
[00:23:56.280 --> 00:23:57.280]   That's really the big idea.
[00:23:57.280 --> 00:23:58.280]   That's the goal.
[00:23:58.280 --> 00:24:00.280]   And I think you'll hear more on that later this week.
[00:24:00.280 --> 00:24:05.280]   The goal here is how are you measuring improvement of your app and ensuring it always gets better.
[00:24:05.280 --> 00:24:07.280]   That's what we're actually trying to achieve here.
[00:24:07.280 --> 00:24:09.280]   And that's kind of what evaluation is giving you.
[00:24:09.280 --> 00:24:12.280]   And, yeah, this is kind of my last slide.
[00:24:12.280 --> 00:24:16.280]   Then maybe we can just move into maybe some Q&A.
[00:24:16.280 --> 00:24:21.280]   I can actually show the notebooks themselves if you want to walk through them together.
[00:24:21.280 --> 00:24:27.280]   I mean, I'll just do that and I'll let you guys kind of hack on them in parallel as I walk through them.
[00:24:27.280 --> 00:24:32.280]   And then I can just walk around and talk to people, something like that.
[00:24:32.280 --> 00:24:35.280]   So, you know, the three types of feedback loops.
[00:24:35.280 --> 00:24:37.280]   You have this design phase feedback.
[00:24:37.280 --> 00:24:38.280]   Something like Langraph.
[00:24:38.280 --> 00:24:39.280]   In-app error handling.
[00:24:39.280 --> 00:24:41.280]   That's kind of step one we talked about.
[00:24:41.280 --> 00:24:43.280]   Cool examples there for coding, for RAG.
[00:24:43.280 --> 00:24:45.280]   A lot of nice papers.
[00:24:45.280 --> 00:24:46.280]   Really promising.
[00:24:46.280 --> 00:24:53.280]   I'm very excited about anything you can do here with terms of kind of agentic self-correction, self-reflection in your app itself.
[00:24:53.280 --> 00:24:54.280]   Pre-production testing.
[00:24:54.280 --> 00:24:55.280]   We just talked through that.
[00:24:55.280 --> 00:24:57.280]   Building evaluation sets.
[00:24:57.280 --> 00:24:58.280]   Running evaluations.
[00:24:58.280 --> 00:24:59.280]   Testing for an agent.
[00:24:59.280 --> 00:25:00.280]   Like your tool use trajectory.
[00:25:00.280 --> 00:25:02.280]   Your answer quality.
[00:25:02.280 --> 00:25:04.280]   All really interesting and important.
[00:25:04.280 --> 00:25:07.280]   And then in production phase, production monitoring.
[00:25:07.280 --> 00:25:11.280]   This gets into, we didn't talk about it too much, but basically this stuff.
[00:25:11.280 --> 00:25:14.280]   So basically you can have evaluators running on your app in production.
[00:25:14.280 --> 00:25:15.280]   Looking at inputs.
[00:25:15.280 --> 00:25:16.280]   Looking at outputs.
[00:25:16.280 --> 00:25:17.280]   Tagging them accordingly.
[00:25:17.280 --> 00:25:19.280]   And then you can go back and look later.
[00:25:19.280 --> 00:25:22.280]   So that's kind of the set up here.
[00:25:22.280 --> 00:25:23.280]   I know that's probably a lot.
[00:25:23.280 --> 00:25:25.280]   And we went about half an hour.
[00:25:25.280 --> 00:25:28.280]   So if there's any questions, I can just open it up.
[00:25:28.280 --> 00:25:30.280]   And we can kind of talk through stuff.
[00:25:30.280 --> 00:25:36.280]   I can also start ripping through some of the notebooks just to kind of give you an overview of the code itself.
[00:25:36.280 --> 00:25:43.280]   But if there's any questions here, maybe, you know, happy to take a few and give you a minute to digest all that.
[00:25:43.280 --> 00:25:44.280]   Yeah.
[00:25:44.280 --> 00:25:47.280]   Is there a non-Twitter link to the slides yet?
[00:25:47.280 --> 00:25:49.280]   That's a good point.
[00:25:49.280 --> 00:25:50.280]   Let's see.
[00:25:50.280 --> 00:25:55.280]   Is there a non-Twitter link to the slides?
[00:25:55.280 --> 00:25:59.280]   Let's see if the conference organizers give me some.
[00:25:59.280 --> 00:26:02.280]   I don't know if I have an email list for everyone in here.
[00:26:02.280 --> 00:26:06.280]   Is there, okay, is there a Slack?
[00:26:06.280 --> 00:26:08.280]   Yeah, if someone can, I actually didn't know there's a Slack.
[00:26:08.280 --> 00:26:09.280]   So that's very helpful.
[00:26:09.280 --> 00:26:13.280]   Yeah, if there's a Slack or an app for this conference, then please, someone post.
[00:26:13.280 --> 00:26:14.280]   I appreciate that.
[00:26:14.280 --> 00:26:17.280]   I actually didn't know that.
[00:26:17.280 --> 00:26:18.280]   Thank you for that question.
[00:26:18.280 --> 00:26:19.280]   Yes, sir.
[00:26:19.280 --> 00:26:31.280]   So about testing and evaluation, does it really scale to predict the exact sequence of the agent?
[00:26:31.280 --> 00:26:40.280]   If it's smart enough dealing with a complex problem, it's hard to say exactly how it achieves the task.
[00:26:40.280 --> 00:26:41.280]   Okay.
[00:26:41.280 --> 00:26:42.280]   This is a very good point.
[00:26:42.280 --> 00:26:45.280]   So I'm going to repeat the question now.
[00:26:45.280 --> 00:26:49.280]   How do you evaluate an agent's like reasoning trajectory?
[00:26:49.280 --> 00:26:54.280]   If it's a large and open-ended problem, that can be solved in many different ways.
[00:26:54.280 --> 00:26:57.280]   In that particular case, you are right.
[00:26:57.280 --> 00:27:02.280]   It is hard to enumerate a specific trajectory that is actually reasonable.
[00:27:02.280 --> 00:27:10.280]   For really open-ended long running type problems, trajectory evaluation may not be appropriate.
[00:27:10.280 --> 00:27:16.280]   One thing I would think about is, can you define a few canonical trajectories of tool use through your application?
[00:27:16.280 --> 00:27:19.280]   So, it depends on the stat.
[00:27:19.280 --> 00:27:21.280]   If it's a very long running agent, I think it's probably infeasible.
[00:27:21.280 --> 00:27:32.280]   If it is a shorter run agent where it's like, you expect something in the order of maybe five to ten steps, you can probably enumerate some set of reasonable trajectories and basically check does it follow any of these.
[00:27:32.280 --> 00:27:35.280]   You can also do things, so you can do things like this.
[00:27:35.280 --> 00:27:41.280]   You can do things like check for the repeat of certain tool calls.
[00:27:41.280 --> 00:27:47.280]   You can be very flexible at this, and it's kind of open to you, so you can look for like, is it repeating certain tool calls?
[00:27:47.280 --> 00:27:52.280]   You can look at recall, like, is it for sure calling this tool or not?
[00:27:52.280 --> 00:27:54.280]   So, you can actually be very flexible.
[00:27:54.280 --> 00:27:59.280]   Actually, the way we set up Lang-Smith evaluator for this, it's just a simple function that you can define yourself.
[00:27:59.280 --> 00:28:01.280]   So, it's a very good point.
[00:28:01.280 --> 00:28:04.280]   You can be arbitrarily creative about how you evaluate that.
[00:28:04.280 --> 00:28:06.280]   But I would say, for very long running, you're right.
[00:28:06.280 --> 00:28:09.280]   You can't really articulate step one, two, and three.
[00:28:09.280 --> 00:28:14.280]   But I would then think about more like, evaluating, is it repeating steps?
[00:28:14.280 --> 00:28:17.280]   Can you evaluate for clearly aberrant behaviors?
[00:28:17.280 --> 00:28:22.280]   Excessive number of tool use repeats, excessive number of overall tool calls.
[00:28:22.280 --> 00:28:25.280]   So, kind of like guardrails related to, like, kind of clear aberrant behavior.
[00:28:25.280 --> 00:28:28.280]   It's very short-term, you can actually enumerate the trajectory specifically.
[00:28:28.280 --> 00:28:29.280]   That's a good question.
[00:28:29.280 --> 00:28:33.280]   So, in the code, you can see we actually lay out a custom function.
[00:28:33.280 --> 00:28:34.280]   You can define that yourself.
[00:28:34.280 --> 00:28:36.280]   So, that's a very good point, though.
[00:28:36.280 --> 00:28:37.280]   Yeah.
[00:28:37.280 --> 00:28:38.280]   Yep.
[00:28:38.280 --> 00:28:44.280]   Can you transition to human in the loop?
[00:28:44.280 --> 00:28:45.280]   Yeah.
[00:28:45.280 --> 00:28:47.280]   Human in the loop is a good one.
[00:28:47.280 --> 00:28:50.280]   The workshop notebooks I share do not have that.
[00:28:50.280 --> 00:28:54.280]   But, line graph does have some good support for human in the loop.
[00:28:54.280 --> 00:28:58.280]   And I can share with you some notebooks that showcase that.
[00:28:58.280 --> 00:29:03.280]   Also, what we have shipping on Thursday has very good support for human in the loop.
[00:29:03.280 --> 00:29:07.280]   So, I will share some notebooks with you for that.
[00:29:07.280 --> 00:29:10.280]   And wait for Thursday for even more there.
[00:29:10.280 --> 00:29:11.280]   So, if we're building a rack-like application, right?
[00:29:11.280 --> 00:29:23.280]   Pre-production, we can do this testing framework using a known set of, this is a question, this is a proper answer.
[00:29:23.280 --> 00:29:33.280]   So then, what I've been struggling to draw is, like, the right way to approach that pre-production, where we don't know what the right answer is.
[00:29:33.280 --> 00:29:34.280]   And I'm wondering .
[00:29:34.280 --> 00:29:35.280]   Yeah.
[00:29:35.280 --> 00:29:39.280]   So, the question was, this is actually a really good one.
[00:29:39.280 --> 00:29:46.280]   For RAG, I'm just going to go to our docs because I actually wrote a doc on this recently.
[00:29:46.280 --> 00:29:50.280]   So, you can still see my slides.
[00:29:50.280 --> 00:29:58.280]   For RAG, in a pre-production setting, it's easy to define, or not even easy, but you can define a set of question answer pairs and evaluate them.
[00:29:58.280 --> 00:30:02.280]   When you're in production, though, how do you evaluate your app because you don't have a ground truth answer?
[00:30:02.280 --> 00:30:07.280]   So, what are other things you can actually evaluate for RAG app that don't require a reference?
[00:30:07.280 --> 00:30:08.280]   Yeah.
[00:30:08.280 --> 00:30:12.280]   So, there is a conceptual guide that I will share.
[00:30:12.280 --> 00:30:14.280]   So, this is actually our RAG section.
[00:30:14.280 --> 00:30:16.280]   I have kind of a nice overview of this.
[00:30:16.280 --> 00:30:25.280]   There's actually a few different things you can evaluate for RAG that don't require a reference that are very useful.
[00:30:25.280 --> 00:30:26.280]   Yeah.
[00:30:26.280 --> 00:30:29.280]   So, it's this right here.
[00:30:29.280 --> 00:30:33.280]   So, this is like a typical RAG flow.
[00:30:33.280 --> 00:30:34.280]   So, I have a question.
[00:30:34.280 --> 00:30:36.280]   I retrieve documents.
[00:30:36.280 --> 00:30:37.280]   I pass them to an LLM.
[00:30:37.280 --> 00:30:38.280]   I get an answer.
[00:30:38.280 --> 00:30:39.280]   Right?
[00:30:39.280 --> 00:30:43.280]   What we just talked about and we showed is comparing your answer to some reference answer.
[00:30:43.280 --> 00:30:46.280]   Now, this is, to be honest, pretty hard to do.
[00:30:46.280 --> 00:30:48.280]   You have to build an eval set of question answer pairs.
[00:30:48.280 --> 00:30:50.280]   Very important to do, but it's not easy.
[00:30:50.280 --> 00:30:51.280]   So, what else can you do?
[00:30:51.280 --> 00:30:56.280]   So, some of this we've seen that are really easy and actually pretty popular.
[00:30:56.280 --> 00:30:59.280]   There's three different types of grading you can do that don't require a reference.
[00:30:59.280 --> 00:31:01.280]   They're like internal checks you can run.
[00:31:01.280 --> 00:31:03.280]   I mean, you can run them online.
[00:31:03.280 --> 00:31:06.280]   So, one is retrieval grading.
[00:31:06.280 --> 00:31:10.280]   So, basically looking at your retrieved documents relative to your question.
[00:31:10.280 --> 00:31:12.280]   So, like an internal self-consistency check.
[00:31:12.280 --> 00:31:20.280]   So, this is actually a great check to run and actually the corrective RAG paper that, or the corrective RAG thing that is in the cookbooks that I share here does this.
[00:31:20.280 --> 00:31:22.280]   So, you can play with the prompt and all that.
[00:31:22.280 --> 00:31:29.280]   But basically this is just checking the consistency of your retrieved docs relative to your question.
[00:31:29.280 --> 00:31:31.280]   You can do that and we have some really good prompts for that.
[00:31:31.280 --> 00:31:34.280]   Another one I like is just comparing your answer to your question.
[00:31:34.280 --> 00:31:37.280]   Have an LLM look at, here's my answer, here's the question.
[00:31:37.280 --> 00:31:38.280]   Is this sane?
[00:31:38.280 --> 00:31:39.280]   Are they related?
[00:31:39.280 --> 00:31:49.280]   And this is a really nice check just for like, you know, of course you don't have a reference answer, but like you can still sanity check and say does this deviate significantly from what the questioner is asking.
[00:31:49.280 --> 00:31:52.280]   The other, this is a great one, is hallucination.
[00:31:52.280 --> 00:31:53.280]   And this is super intuitive.
[00:31:53.280 --> 00:31:55.280]   Compare your answer to the retrieved documents.
[00:31:55.280 --> 00:32:01.280]   So, if the LLM went off the rails and didn't ground the answer properly and you hallucinated, you can catch that really easily.
[00:32:01.280 --> 00:32:08.280]   And so, I need to get on this slack because I want to share this link with you.
[00:32:08.280 --> 00:32:09.280]   I'll figure that out.
[00:32:09.280 --> 00:32:10.280]   I'll find you.
[00:32:10.280 --> 00:32:13.280]   But this is in our Langsmith docs.
[00:32:13.280 --> 00:32:18.280]   If you search Langsmith valuation concepts, we have, I actually have a bunch of videos that showcase how to do this.
[00:32:18.280 --> 00:32:21.280]   And I have a bunch of code as well.
[00:32:21.280 --> 00:32:24.280]   So, but those are three things you can do that don't require a reference.
[00:32:24.280 --> 00:32:29.280]   And we do run those in as online evaluation with our application.
[00:32:29.280 --> 00:32:31.280]   So, yeah.
[00:32:31.280 --> 00:32:32.280]   Yep.
[00:32:32.280 --> 00:32:37.280]   Yeah, unit testing.
[00:32:37.280 --> 00:32:38.280]   So, yeah.
[00:32:38.280 --> 00:32:39.280]   Okay.
[00:32:39.280 --> 00:32:44.280]   So, do we have any thoughts on unit testing?
[00:32:44.280 --> 00:32:48.280]   So, Langsmith supports PyTests as unit tests.
[00:32:48.280 --> 00:32:52.280]   But basically, it depends what you mean by unit tests.
[00:32:52.280 --> 00:32:59.280]   Typically, like conventional software engineering unit tests are very effectively done in things like PyTests.
[00:32:59.280 --> 00:33:01.280]   There's a lot of frameworks for that.
[00:33:01.280 --> 00:33:14.280]   What I like to think about in unit testing with respect to LLM apps is kind of like what we show in this code generation example.
[00:33:14.280 --> 00:33:18.280]   So, here we use some really simple unit tests just for like imports and code execution.
[00:33:18.280 --> 00:33:20.280]   Simple unit tests like this can actually run in your app itself.
[00:33:20.280 --> 00:33:29.280]   So, basically, one place you can think about putting unit tests are actually in your app itself within Langraph for self for kind of in-app error handling or self correction.
[00:33:29.280 --> 00:33:33.280]   So, one place for unit tests that's kind of interesting and new with LLM applications.
[00:33:33.280 --> 00:33:35.280]   They can live inside your app itself.
[00:33:35.280 --> 00:33:45.280]   Another good one for unit tests within app is if you're doing structured output anywhere in your application, which is like a really common thing people like, confirm the schema is correct.
[00:33:45.280 --> 00:33:48.280]   That's another good use case for unit tests within your application.
[00:33:48.280 --> 00:33:55.280]   Those also, both of those things could also be done independently like in CI outside of your application.
[00:33:55.280 --> 00:34:00.280]   So, we are going to have more integration support for CI with Langsmith soon.
[00:34:00.280 --> 00:34:02.280]   I will check with the team on that.
[00:34:02.280 --> 00:34:10.280]   But I think the interesting idea for unit tests with LLM applications is this idea of inline within your app itself.
[00:34:10.280 --> 00:34:12.280]   Because LLMs are so good at self-correcting.
[00:34:12.280 --> 00:34:17.280]   If you run unit tests in your application, they can often catch the error and then correct themselves.
[00:34:17.280 --> 00:34:19.280]   And unit tests are fast and cheap to run.
[00:34:19.280 --> 00:34:24.280]   So, it's actually a really nice kind of like piece of alpha that, in fact, that's exactly what Karpathy was mentioning here.
[00:34:24.280 --> 00:34:30.280]   That, you know, running unit tests in line with your application is actually really quite nice.
[00:34:30.280 --> 00:34:35.280]   And produce significant improvement in performance in alpha codiums.
[00:34:35.280 --> 00:34:36.280]   Cool.
[00:34:36.280 --> 00:34:37.280]   Yep.
[00:34:37.280 --> 00:34:38.280]   Okay.
[00:34:38.280 --> 00:34:49.280]   Yeah, yeah.
[00:34:49.280 --> 00:34:55.280]   So, what the question was, if I wanted to do some of this in-app error correction stuff.
[00:34:55.280 --> 00:34:56.280]   So, let's take this example.
[00:34:56.280 --> 00:34:57.280]   The corrective rag thing.
[00:34:57.280 --> 00:35:02.280]   If I actually want this to run in my application, it obviously needs to be super fast.
[00:35:02.280 --> 00:35:04.280]   So, that's actually what we've seen.
[00:35:04.280 --> 00:35:08.280]   The tricks we've seen here are basically use very fast, smaller LLMs.
[00:35:08.280 --> 00:35:10.280]   So, you mentioned, for example, even the ability to fine tune.
[00:35:10.280 --> 00:35:11.280]   That's actually a good idea.
[00:35:11.280 --> 00:35:18.280]   If you have a judging task that is very consistent, it's a very good use case for fine tuning actually.
[00:35:18.280 --> 00:35:23.280]   Fine tune a small, low capacity, extremely fast, and effectively very cheap to deploy model.
[00:35:23.280 --> 00:35:24.280]   That's a very good idea.
[00:35:24.280 --> 00:35:25.280]   That's a very good idea.
[00:35:25.280 --> 00:35:27.280]   We've seen people do that.
[00:35:27.280 --> 00:35:30.280]   Also, use very simple grading criteria.
[00:35:30.280 --> 00:35:35.280]   Don't have some kind of arbitrary scale from zero to five with high cognitive load that LLM has to think about.
[00:35:35.280 --> 00:35:36.280]   Yes, no.
[00:35:36.280 --> 00:35:37.280]   Very simple binary grading.
[00:35:37.280 --> 00:35:41.280]   Even, for some of the stuff, you can even be old school and fine tune a classifier.
[00:35:41.280 --> 00:35:47.280]   But basically, really simple, lightweight, fast, LLM as judge style.
[00:35:47.280 --> 00:35:49.280]   Classifier wouldn't be an LLM necessarily.
[00:35:49.280 --> 00:35:54.280]   But basically, very simple, fast test for in-app.
[00:35:54.280 --> 00:35:57.280]   Anything like kind of in runtime, you will need or want.
[00:35:57.280 --> 00:36:03.280]   Another cool use case for this, or another interesting option for this, is Grok is very, very fast.
[00:36:03.280 --> 00:36:05.280]   Kind of with their LPU stuff.
[00:36:05.280 --> 00:36:08.280]   And they actually would be a very interesting option.
[00:36:08.280 --> 00:36:10.280]   We've done some work on that with Grok.
[00:36:10.280 --> 00:36:16.280]   Basically, for any of these kind of in-app LMS judge error correction things, using something like Grok, which is extremely fast.
[00:36:16.280 --> 00:36:19.280]   But it's a very good insight.
[00:36:19.280 --> 00:36:21.280]   Fine tuning your own model is actually a really good idea.
[00:36:21.280 --> 00:36:25.280]   We've seen people do that for these types of anything with land graph and in-app error correction.
[00:36:25.280 --> 00:36:26.280]   Cool.
[00:36:26.280 --> 00:36:27.280]   Yep.
[00:36:27.280 --> 00:36:48.280]   Yeah, we have some good cookbooks talking about multi-agent, which I, again, will need to find a way to share with you.
[00:36:48.280 --> 00:36:49.280]   Yep.
[00:36:49.280 --> 00:37:02.280]   So we have this, if you go to land graph, land graph github, examples multi-agent, there's a few different notebooks here that are worth checking out.
[00:37:02.280 --> 00:37:03.280]   Yep.
[00:37:03.280 --> 00:37:12.280]   Yeah.
[00:37:12.280 --> 00:37:13.280]   Yeah.
[00:37:13.280 --> 00:37:16.280]   So actually, land graph is specifically designed for cycles.
[00:37:16.280 --> 00:37:19.280]   So some of the examples, like what we're showing today is only a branch.
[00:37:19.280 --> 00:37:21.280]   So it's a simpler graph.
[00:37:21.280 --> 00:37:26.280]   But for example, the react agent we'll show today is a cycle.
[00:37:26.280 --> 00:37:29.280]   So it's basically going to continue in a loop just like this.
[00:37:29.280 --> 00:37:34.280]   And what you do is you set a recursion limit in your land graph config.
[00:37:34.280 --> 00:37:38.280]   So you basically tell it to only proceed for some number of cycles.
[00:37:38.280 --> 00:37:39.280]   And this is default set for you.
[00:37:39.280 --> 00:37:41.280]   I believe it's like 20 or something like that.
[00:37:41.280 --> 00:37:49.280]   But that's what you're going to, that's what you're going to want to do.
[00:37:49.280 --> 00:37:50.280]   Yep.
[00:37:50.280 --> 00:37:59.280]   So the question was about timing of the responses.
[00:37:59.280 --> 00:38:04.280]   So do you mean like if you're implementing some kind of self-correction, how long that takes?
[00:38:04.280 --> 00:38:11.280]   Well, that's kind of a -- that gets back to the question that this gentleman asked.
[00:38:11.280 --> 00:38:15.280]   It depends a lot on the LLM you choose to use for your judging.
[00:38:15.280 --> 00:38:16.280]   And it's latency.
[00:38:16.280 --> 00:38:21.280]   So that's kind of where if you're -- so maybe there's two sides of this.
[00:38:21.280 --> 00:38:27.280]   One side of it is choosing an LLM that's very, very fast and that's very important to do.
[00:38:27.280 --> 00:38:28.280]   Could be something like Grock.
[00:38:28.280 --> 00:38:31.280]   Could be a fine-tuned like deployment that you do yourself.
[00:38:31.280 --> 00:38:32.280]   Could be a GPT-3-5.
[00:38:32.280 --> 00:38:34.280]   So that's like one side of it.
[00:38:34.280 --> 00:38:39.280]   The other side of it is how do you actually kind of monitor, measure that?
[00:38:39.280 --> 00:38:44.280]   And so again, Langsmith actually does have very good support for tracing and observability.
[00:38:44.280 --> 00:38:47.280]   And we do have timings all.
[00:38:47.280 --> 00:38:51.280]   In fact, I can go ahead and show you very quickly if you want to see.
[00:38:51.280 --> 00:38:53.280]   So this is my Langsmith dashboard.
[00:38:53.280 --> 00:38:54.280]   I'll zoom in a little bit.
[00:38:54.280 --> 00:38:56.280]   These are my experiments.
[00:38:56.280 --> 00:38:59.280]   Now if I zoom in here, I can open up one.
[00:38:59.280 --> 00:39:02.280]   The Wi-Fi is a little bit slow.
[00:39:02.280 --> 00:39:03.280]   These are my replicates.
[00:39:03.280 --> 00:39:05.280]   I can open up one of my traces.
[00:39:05.280 --> 00:39:08.280]   And what I can see here is over here I get the timing.
[00:39:08.280 --> 00:39:10.280]   So this is the timing of the entire graph.
[00:39:10.280 --> 00:39:12.280]   And I can go through my steps.
[00:39:12.280 --> 00:39:14.280]   So this is like the retrieval is really fast.
[00:39:14.280 --> 00:39:15.280]   That's good.
[00:39:15.280 --> 00:39:16.280]   You know, less than a second.
[00:39:16.280 --> 00:39:17.280]   Okay.
[00:39:17.280 --> 00:39:18.280]   Now here's what's interesting.
[00:39:18.280 --> 00:39:21.280]   My grading in this particular case is like four seconds.
[00:39:21.280 --> 00:39:23.280]   So that's, you know, not acceptable in a production setting most likely.
[00:39:23.280 --> 00:39:26.280]   But again, this is just like a, this is a test case.
[00:39:26.280 --> 00:39:29.280]   In fact, I'm using, what am I using to grade here?
[00:39:29.280 --> 00:39:30.280]   GPT-4-0.
[00:39:30.280 --> 00:39:35.280]   So it is, you know, there's ways you could speed this up by using different models
[00:39:35.280 --> 00:39:38.280]   or different prompts or grading everything in bulk.
[00:39:38.280 --> 00:39:39.280]   There's a lot of ideas.
[00:39:39.280 --> 00:39:41.280]   I actually grade each document independently.
[00:39:41.280 --> 00:39:42.280]   Oh, actually, you know what?
[00:39:42.280 --> 00:39:43.280]   This is using chat fireworks.
[00:39:43.280 --> 00:39:47.280]   So anyway, but you can look at your timings in lengths, but that's a really nice way.
[00:39:47.280 --> 00:39:53.280]   I'd like to do it to kind of see, to kind of monitor the timing of my applications.
[00:39:53.280 --> 00:39:57.280]   Yep.
[00:39:57.280 --> 00:40:08.280]   So the case you showed, sort of, like, from scratch, like, when the agent is running from the beginning,
[00:40:08.280 --> 00:40:24.280]   it's running from scratch.
[00:40:24.280 --> 00:40:29.280]   But in reality, usually people have some context or interest that's being passed.
[00:40:29.280 --> 00:40:33.280]   Do you have any suggestions as to how you test that's going through?
[00:40:33.280 --> 00:40:38.280]   And the question was, with agents, you typically pass them a message history.
[00:40:38.280 --> 00:40:41.280]   That is absolutely true.
[00:40:41.280 --> 00:40:46.280]   And in fact, the React agent that we implement here, like, I can even open up one of the traces.
[00:40:46.280 --> 00:40:48.280]   We can look at it together.
[00:40:48.280 --> 00:40:55.280]   So here's a React agent with GPT-40.
[00:40:55.280 --> 00:40:57.280]   Here's one of my traces.
[00:40:57.280 --> 00:41:00.280]   Let's open it up and actually see what's going on here.
[00:41:00.280 --> 00:41:05.280]   So what happens is, first, my assistant is right here.
[00:41:05.280 --> 00:41:06.280]   So this is OpenAI.
[00:41:06.280 --> 00:41:09.280]   We'll, you know, here's the system prompt, right?
[00:41:09.280 --> 00:41:12.280]   So your helpful assistant, you're answering questions.
[00:41:12.280 --> 00:41:13.280]   Here's the human question, right?
[00:41:13.280 --> 00:41:16.280]   So again, this is the start of our message history.
[00:41:16.280 --> 00:41:17.280]   Okay?
[00:41:17.280 --> 00:41:22.280]   So what the -- and also, these are the tools that the LLM has.
[00:41:22.280 --> 00:41:23.280]   And this is pretty cool.
[00:41:23.280 --> 00:41:25.280]   We can see this is the one it called.
[00:41:25.280 --> 00:41:31.280]   So what happened is, our LLM, it looked at, here's our system prompt, here's the human instruction.
[00:41:31.280 --> 00:41:33.280]   And it says, okay, I'm going to retrieve documents.
[00:41:33.280 --> 00:41:34.280]   Great.
[00:41:34.280 --> 00:41:37.280]   So then it goes, and this is the tool call.
[00:41:37.280 --> 00:41:39.280]   It goes and retrieves the documents.
[00:41:39.280 --> 00:41:42.280]   Now that goes -- so you look here, we can actually open up the retriever.
[00:41:42.280 --> 00:41:43.280]   What do we get?
[00:41:43.280 --> 00:41:44.280]   Here's our documents.
[00:41:44.280 --> 00:41:45.280]   Cool.
[00:41:45.280 --> 00:41:47.280]   Now I go -- it goes back to the assistant.
[00:41:47.280 --> 00:41:48.280]   So back -- this is a looping thing.
[00:41:48.280 --> 00:41:49.280]   It started with our assistant.
[00:41:49.280 --> 00:41:50.280]   It made the tool call.
[00:41:50.280 --> 00:41:51.280]   The tool ran.
[00:41:51.280 --> 00:41:52.280]   We got documents.
[00:41:52.280 --> 00:41:53.280]   Now we get them back.
[00:41:53.280 --> 00:41:55.280]   Now let's go back to our LLM.
[00:41:55.280 --> 00:41:57.280]   So now our LLM -- this is pretty cool, right?
[00:41:57.280 --> 00:41:59.280]   Here's the message history, like you were saying.
[00:41:59.280 --> 00:42:04.280]   Instructions, question, document retrieval.
[00:42:04.280 --> 00:42:07.280]   The documents that are retrieved are right here.
[00:42:07.280 --> 00:42:10.280]   And then now the LLM says, okay, I want to grade them.
[00:42:10.280 --> 00:42:12.280]   It calls the grader tool.
[00:42:12.280 --> 00:42:14.280]   And this is its reasoning and this is its grade.
[00:42:14.280 --> 00:42:21.280]   So anyway, you are right that as this goes through, you basically accumulate a message history.
[00:42:21.280 --> 00:42:27.280]   And the LLM will use the message history and most recent instruction to reason about what tool to call next.
[00:42:27.280 --> 00:42:30.280]   That's exactly how it works.
[00:42:30.280 --> 00:42:32.280]   I think I answered the question.
[00:42:32.280 --> 00:42:35.280]   Is there anything that isn't clear about that?
[00:42:35.280 --> 00:42:36.280]   So it is true.
[00:42:36.280 --> 00:42:38.280]   Like let's look at an example, right?
[00:42:38.280 --> 00:42:44.280]   So in this particular case, right, the LLM sees the retrieved documents from the tool.
[00:42:44.280 --> 00:42:47.280]   And then it makes a decision that says, okay, I have this tool response.
[00:42:47.280 --> 00:42:48.280]   It's retrieved documents.
[00:42:48.280 --> 00:42:49.280]   What should I do next?
[00:42:49.280 --> 00:42:52.280]   And it says, okay, well, why don't I go ahead and grade them?
[00:42:52.280 --> 00:42:53.280]   And it calls the grade tool.
[00:42:53.280 --> 00:42:56.280]   So it looks at the message history and reads about what tool it'll call.
[00:42:56.280 --> 00:42:58.280]   And that's exactly how these React style agents work.
[00:42:58.280 --> 00:43:01.280]   And the whole issue is that's kind of a noisy process.
[00:43:01.280 --> 00:43:03.280]   Like it can look at that whole trajectory.
[00:43:03.280 --> 00:43:04.280]   It can get confused.
[00:43:04.280 --> 00:43:05.280]   It can call the wrong tool.
[00:43:05.280 --> 00:43:06.280]   Then it's on the wrong track.
[00:43:06.280 --> 00:43:10.280]   And that's exactly why these more open-ended tool calling agents fail.
[00:43:10.280 --> 00:43:11.280]   Yup.
[00:43:11.280 --> 00:43:16.280]   So that's the same like in the second follow-up question to the agent.
[00:43:16.280 --> 00:43:17.280]   Yup.
[00:43:17.280 --> 00:43:23.280]   That, that, that, technically should follow similar, right?
[00:43:23.280 --> 00:43:32.280]   Because sometimes the data tree, the first, the first match of the whole, when they're
[00:43:32.280 --> 00:43:38.280]   they're going to bring you back to the actual structure you're asking so that you can get
[00:43:38.280 --> 00:43:39.280]   the value of a problem.
[00:43:39.280 --> 00:43:44.280]   You can bring you a question after that.
[00:43:44.280 --> 00:43:45.280]   Single question.
[00:43:45.280 --> 00:43:46.280]   Yeah.
[00:43:46.280 --> 00:43:47.280]   Okay.
[00:43:47.280 --> 00:43:48.280]   Right.
[00:43:48.280 --> 00:43:53.280]   So I think that the question is, well, let's say this is a multi-turn conversation where I
[00:43:53.280 --> 00:43:57.280]   can, the user can go ahead and ask a second question, of course, and that whole kind of message
[00:43:57.280 --> 00:44:00.280]   history will be propagated.
[00:44:00.280 --> 00:44:06.280]   Um, yes, that is, that is a common pattern.
[00:44:06.280 --> 00:44:12.280]   Um, and that, let's see, I mean, what's the question on that though?
[00:44:12.280 --> 00:44:20.280]   Like it, it could use context from its initial trajectory to answer the second question for
[00:44:20.280 --> 00:44:21.280]   sure.
[00:44:21.280 --> 00:44:25.280]   Um, it'll probably look at that jointly when it's deciding what tool to call.
[00:44:25.280 --> 00:44:29.280]   So for example, if it receives a question and in its message history, it sees the context
[00:44:29.280 --> 00:44:30.280]   need to answer the question.
[00:44:30.280 --> 00:44:33.280]   The agent could probably decide, okay, I don't need to retrieve documents.
[00:44:33.280 --> 00:44:35.280]   Again, I have the documents need to answer the question.
[00:44:35.280 --> 00:44:37.280]   I'll answer that question directly.
[00:44:37.280 --> 00:44:42.280]   So it is true that a multi-turn conversation, the agent can look at its message history to
[00:44:42.280 --> 00:44:43.280]   inform what to do next.
[00:44:43.280 --> 00:44:45.280]   That's definitely true.
[00:44:45.280 --> 00:44:46.280]   Here.
[00:44:46.280 --> 00:44:49.280]   I don't consider evaluation of multi-turn conversations.
[00:44:49.280 --> 00:44:53.280]   Um, but that is, it's a good topic actually.
[00:44:53.280 --> 00:45:00.280]   Um, I don't quite have a tutorial for that yet, but I could think about putting that together.
[00:45:00.280 --> 00:45:01.280]   Yeah.
[00:45:01.280 --> 00:45:02.280]   That's a good point.
[00:45:02.280 --> 00:45:04.280]   Uh, I'll make a note of that actually.
[00:45:04.280 --> 00:45:05.280]   Yeah.
[00:45:05.280 --> 00:45:08.280]   So multi-turn is a good one.
[00:45:08.280 --> 00:45:09.280]   Cool.
[00:45:09.280 --> 00:45:10.280]   Um, okay.
[00:45:10.280 --> 00:45:11.280]   Yep.
[00:45:11.280 --> 00:45:12.280]   Yep.
[00:45:12.280 --> 00:45:13.280]   Yep.
[00:45:13.280 --> 00:45:14.280]   Yep.
[00:45:14.280 --> 00:45:14.280]   Yep.
[00:45:14.280 --> 00:45:15.280]   Yep.
[00:45:15.280 --> 00:45:16.280]   Yep.
[00:45:16.280 --> 00:45:23.280]   So, uh, test units are cool because they would use it up and we can actually run it in the
[00:45:23.280 --> 00:45:24.280]   office and support it.
[00:45:24.280 --> 00:45:29.280]   Have you .
[00:45:29.280 --> 00:45:36.280]   Um, so the question is, I believe, um, have I tested, like, the ability to do this?
[00:45:36.280 --> 00:45:48.280]   Um, have I tested like the ability to kind of online auto generate unit tests?
[00:45:48.280 --> 00:45:49.280]   Yeah.
[00:45:49.280 --> 00:45:50.280]   Okay.
[00:45:50.280 --> 00:45:51.280]   So that's a big topic.
[00:45:51.280 --> 00:46:01.280]   So basically the alpha codium paper, um, that Karpathy references here does that.
[00:46:01.280 --> 00:46:07.180]   I have not tested that because it does ramp up the complexity because then you're relying
[00:46:07.180 --> 00:46:12.880]   on an, and I don't think, I mean, that would be aggressive for a production setting because
[00:46:12.880 --> 00:46:17.780]   basically be relying on an LLM to auto generate unit tests, testing against things that are auto
[00:46:17.780 --> 00:46:18.780]   generated.
[00:46:18.780 --> 00:46:20.280]   There's a lot of opportunity for error there.
[00:46:20.280 --> 00:46:24.880]   I think it's interesting, particularly in terms of like offline challenges like this,
[00:46:24.880 --> 00:46:28.780]   but in terms of like a production application that feels pretty hard and risky, but it's an
[00:46:28.780 --> 00:46:29.780]   interesting theme.
[00:46:29.780 --> 00:46:34.780]   The thing I've tested more on, I found to be very effective is super simple, crisp, lightweight,
[00:46:34.780 --> 00:46:36.780]   free, effectively unit tests.
[00:46:36.780 --> 00:46:45.780]   Like, again, the good use case I found was our, um, so Langchain, we have an internal rag
[00:46:45.780 --> 00:46:52.780]   application called chat Langchain, and it indexes our documents and provides QA. It occasionally
[00:46:52.780 --> 00:46:57.780]   hallucinates the imports, and that's a really bad experience, right, if you take a code block,
[00:46:57.780 --> 00:47:00.780]   you get from this app, and then this import doesn't exist. It's like, what the hell? You
[00:47:00.780 --> 00:47:02.780]   know, that's really annoying.
[00:47:02.780 --> 00:47:07.780]   So I incorporate a really simple check where I have a unit test that just, it does a function
[00:47:07.780 --> 00:47:12.780]   call where it extracts the imports from the code block from the answer, and it tests the
[00:47:12.780 --> 00:47:16.780]   imports in isolation. If they don't exist, there's an error. I feed it back to LLM and say, look,
[00:47:16.780 --> 00:47:21.780]   this isn't a real import. Try again. And I can do, you can do other tricks like then context
[00:47:21.780 --> 00:47:25.780]   stuff, you know, stuff, relevant documents or something like that. Anyway, you can handle
[00:47:25.780 --> 00:47:30.780]   that differently, but that, just that little alpha, significantly improved our performance.
[00:47:30.780 --> 00:47:37.780]   So I kind of like simple, lightweight, free unit tests. Um, the idea of online general unit
[00:47:37.780 --> 00:47:42.780]   is interesting, but like opens up a lot more surface area for errors.
[00:47:42.780 --> 00:47:48.780]   Yeah. Um, so I, the follow-up there was, um, try, let's see.
[00:47:48.780 --> 00:48:08.780]   you're trying to write the test before the app is implemented. Um, I see. Yes. So I, the
[00:48:08.780 --> 00:48:15.780]   follow-up there was, um, try, let's see, you're trying to write the test before the app is implemented.
[00:48:15.780 --> 00:48:24.780]   Um, I see. Yes. So they also do that. So basically for each question, they do have, they, so the alpha
[00:48:24.780 --> 00:48:30.780]   codium work references, uh, existing unit tests for a given question, as well as auto-generates.
[00:48:30.780 --> 00:48:36.780]   So you are right. That would be interesting to have a bunch of pre-generated unit tests that
[00:48:36.780 --> 00:48:42.780]   you know are good for certain questions and to run them. Absolutely. Hard to do in a production setting
[00:48:42.780 --> 00:48:48.780]   setting with an open-ended input, but potentially very useful in, well, okay. Even in a production
[00:48:48.780 --> 00:48:53.780]   set, you could maybe have some battery of unit tests. And based upon the question type, pull
[00:48:53.780 --> 00:48:59.780]   related unit tests that you know are going to be relevant. It's a good idea. Yeah. It's a good idea for sure.
[00:48:59.780 --> 00:49:11.780]   So basically using some battery of dynamically chosen pre-existing unit tests based on the question type or the documentation, whatever documentation they're asking about, that's a good idea.
[00:49:11.780 --> 00:49:25.780]   Yeah, he was saying so for larger projects, you can also use that to test for aggressions. Yes, that is, that's definitely a good idea.
[00:49:25.780 --> 00:49:31.780]   And this paper, it does incorporate that idea as well as this auto-generated unit test thing, which is a little bit more aggressive.
[00:49:31.780 --> 00:49:35.780]   Cool. Yep.
[00:49:35.780 --> 00:49:37.780]   Cool. Yep.
[00:49:37.780 --> 00:49:41.780]   Yeah.
[00:49:41.780 --> 00:49:45.780]   Yeah.
[00:49:45.780 --> 00:49:59.780]   Yeah.
[00:49:59.780 --> 00:49:59.780]   Yeah.
[00:49:59.780 --> 00:50:00.780]   Yeah.
[00:50:00.780 --> 00:50:01.780]   Yeah.
[00:50:01.780 --> 00:50:03.780]   Yeah.
[00:50:03.780 --> 00:50:05.780]   Yeah.
[00:50:05.780 --> 00:50:07.780]   Yeah.
[00:50:07.780 --> 00:50:28.780]   So the question was, in some of these self-reflective applications, like let's say this one, the self-reflective rag, right?
[00:50:28.780 --> 00:50:30.780]   We're doing a few different checks here.
[00:50:30.780 --> 00:50:31.780]   We're checking documents.
[00:50:31.780 --> 00:50:32.780]   We're checking hallucinations.
[00:50:32.780 --> 00:50:34.780]   We're checking answer quality.
[00:50:34.780 --> 00:50:41.780]   So instead of having some hard-coded single prompt to do that, can you have another agent,
[00:50:41.780 --> 00:50:45.780]   like kind of a checker or a greater agent that's a little bit more sophisticated?
[00:50:45.780 --> 00:50:49.780]   I have, and he mentioned a few frameworks, I have not played with them.
[00:50:49.780 --> 00:50:50.780]   I think it's interesting.
[00:50:50.780 --> 00:50:55.780]   I think it's, it's one of these things where it's really good for kind of academic papers.
[00:50:55.780 --> 00:50:56.780]   It's very interesting.
[00:50:56.780 --> 00:50:57.780]   It's really good.
[00:50:57.780 --> 00:51:03.780]   It could be good for offline testing in an online or production setting or it's, the latency and complexity
[00:51:03.780 --> 00:51:06.780]   is probably a bit much.
[00:51:06.780 --> 00:51:10.780]   I think in a production setting, it goes back to what I think this guy was referencing.
[00:51:10.780 --> 00:51:13.780]   You probably would want something that's extremely fast, lightweight.
[00:51:13.780 --> 00:51:21.780]   And I would not think about like a multi-agent system in a production setting, doing any kind of grading.
[00:51:21.780 --> 00:51:25.780]   This whole idea of LLM as LLM graders is kind of a new idea.
[00:51:25.780 --> 00:51:32.780]   So I think this idea of like more complex agent graders is interesting.
[00:51:32.780 --> 00:51:36.780]   But we're kind of in the, we're taking baby steps at this point, especially in a production setting.
[00:51:36.780 --> 00:51:40.780]   So I'd probably shy away from that for now, particularly if you're thinking about production.
[00:51:40.780 --> 00:51:43.780]   But for something offline or experimentation, it's probably interesting.
[00:51:43.780 --> 00:51:44.780]   Yeah.
[00:51:44.780 --> 00:51:53.780]   What's the best practice that you've seen so far for performing tasks or evaluations on those cycles?
[00:51:53.780 --> 00:52:00.780]   Because obviously, you can end up with many, many different types of input and that sort of thing.
[00:52:00.780 --> 00:52:04.780]   And obviously, LaneGraph provides some tracing there.
[00:52:04.780 --> 00:52:07.780]   But what about, hey, here's an interesting test case.
[00:52:07.780 --> 00:52:12.780]   What kind of best practices do you see for actually performing those routine evaluations?
[00:52:12.780 --> 00:52:13.780]   Yeah, exactly.
[00:52:13.780 --> 00:52:16.780]   So actually, the notebooks shared here go into it a little bit.
[00:52:16.780 --> 00:52:22.780]   So the way I like to do it is, and I can even, why don't I just show you one of the notebooks.
[00:52:22.780 --> 00:52:29.780]   So basically, so at the bottom, I have kind of all the different evaluations.
[00:52:29.780 --> 00:52:33.780]   So this goes back to a question that someone mentioned down here as well.
[00:52:33.780 --> 00:52:34.780]   Which one is it?
[00:52:34.780 --> 00:52:41.780]   So this is, so there's a colab, and then there's a notebook, and they are both the same.
[00:52:41.780 --> 00:52:44.780]   So they have the same evaluation section.
[00:52:44.780 --> 00:52:54.780]   This is ragagenttesting.ipnb, and there's also a colab, which is, it's the same notebook, basically.
[00:52:54.780 --> 00:52:57.780]   It's in the slides, just to make sure you've got those links.
[00:52:57.780 --> 00:53:03.780]   But to your question, so the way I did it, and you can be very flexible with this, is basically,
[00:53:03.780 --> 00:53:07.780]   I define the expected trajectory through the nodes in my graph.
[00:53:07.780 --> 00:53:12.780]   And in this particular simple case, the trajectories that I expect are basically retrieval, grading,
[00:53:12.780 --> 00:53:17.780]   web search, generate, or retrieval, grade, generate.
[00:53:17.780 --> 00:53:20.780]   Those are the two expected trajectories that I want to take.
[00:53:20.780 --> 00:53:22.780]   Now, in this case, I don't do cycles.
[00:53:22.780 --> 00:53:30.780]   If you did cycles, you could just incorporate more steps here to say, okay, here's kind of the expected number of steps I want or expect to see through that cycle.
[00:53:30.780 --> 00:53:35.780]   And I think someone mentioned here, if you have a really open-ended, challenging task, then it may be hard to enumerate exactly.
[00:53:35.780 --> 00:53:44.780]   But for a lot of, like, more production-setting applications where these are not extremely long-running, you can enumerate, here's the steps I expect to take through the cycle.
[00:53:44.780 --> 00:53:48.780]   And the way I do the evaluator, it's as simple as this.
[00:53:48.780 --> 00:53:49.780]   There's not much code, it's all in the notebooks.
[00:53:49.780 --> 00:53:54.780]   But basically, I compare the tool calls that the thing did to these trajectories.
[00:53:54.780 --> 00:53:55.780]   That's it.
[00:53:55.780 --> 00:53:56.780]   Super simple.
[00:53:56.780 --> 00:53:58.780]   So that's the way I would think about it.
[00:53:58.780 --> 00:53:59.780]   I keep it really simple.
[00:53:59.780 --> 00:54:01.780]   I would basically try to enumerate.
[00:54:01.780 --> 00:54:05.780]   Here's the steps through the difference, the cycle I want it to take.
[00:54:05.780 --> 00:54:07.780]   And, yeah, go ahead.
[00:54:07.780 --> 00:54:08.780]   Just to make sure I understand.
[00:54:08.780 --> 00:54:09.780]   Right.
[00:54:09.780 --> 00:54:18.780]   Basically, you're using, so in the land graph concept, basically using the nodes to say, hey, I expected to be calling these tools along the way.
[00:54:18.780 --> 00:54:19.780]   That's it.
[00:54:19.780 --> 00:54:20.780]   And that's how I perform in my evaluator.
[00:54:20.780 --> 00:54:21.780]   That's it.
[00:54:21.780 --> 00:54:22.780]   That's it.
[00:54:22.780 --> 00:54:24.780]   So it's actually really simple.
[00:54:24.780 --> 00:54:26.780]   And this is actually a good point.
[00:54:26.780 --> 00:54:35.780]   In the land graph case, for this custom agent, you'll see, all I do is, for every node, and we need to get into the code if we want.
[00:54:35.780 --> 00:54:37.780]   Maybe people have already explored.
[00:54:37.780 --> 00:54:41.780]   So, well, I'll just answer the question directly.
[00:54:41.780 --> 00:54:43.780]   Then we can back up and go through all the code if we want.
[00:54:43.780 --> 00:54:49.780]   But basically, each node in my graph, I just append this step name.
[00:54:49.780 --> 00:54:50.780]   Right.
[00:54:50.780 --> 00:54:53.780]   So, like, retrieve, I just say retrieve documents.
[00:54:53.780 --> 00:54:54.780]   This is my generate node.
[00:54:54.780 --> 00:54:57.780]   I just, you know, append this thing, generate answer.
[00:54:57.780 --> 00:55:00.780]   And I return that in my state as steps.
[00:55:00.780 --> 00:55:03.780]   So then I have this record of the steps I took in my graph.
[00:55:03.780 --> 00:55:04.780]   That's it.
[00:55:04.780 --> 00:55:09.780]   And I can go ahead and fetch that at eval time and compare it to what I expect.
[00:55:09.780 --> 00:55:10.780]   That's all you're doing.
[00:55:10.780 --> 00:55:12.780]   So that's how it would work with, like, the custom land graph thing.
[00:55:12.780 --> 00:55:19.780]   And now with the React agent, actually, it's a little easier because the React agent uses a message
[00:55:19.780 --> 00:55:20.780]   history.
[00:55:20.780 --> 00:55:23.780]   So I can just go to my message history, and that's exactly what I show here.
[00:55:23.780 --> 00:55:27.780]   You can strip out -- I guess I do it up above.
[00:55:27.780 --> 00:55:29.780]   But basically, I have this function in the notebook.
[00:55:29.780 --> 00:55:30.780]   Let's see.
[00:55:30.780 --> 00:55:31.780]   Where is it?
[00:55:31.780 --> 00:55:32.780]   Yeah.
[00:55:32.780 --> 00:55:33.780]   It's find tool calls react.
[00:55:33.780 --> 00:55:40.780]   So it's this little function that will basically look at my message history and strip out all the
[00:55:40.780 --> 00:55:41.780]   tool calls.
[00:55:41.780 --> 00:55:48.780]   So, yeah, it's a nice little -- nice little thing with a React agent.
[00:55:48.780 --> 00:55:50.780]   It's really easy to get the tool calls out.
[00:55:50.780 --> 00:55:53.780]   With land graph, I just log them at every node.
[00:55:53.780 --> 00:55:59.780]   And then at eval time, I just can extract them.
[00:55:59.780 --> 00:56:00.780]   Yeah.
[00:56:00.780 --> 00:56:00.780]   So the question was, with an agent and a multi-state agent?
[00:56:00.780 --> 00:56:01.780]   Yep.
[00:56:01.780 --> 00:56:06.780]   So if you elaborate on a certain scenario with your content switching, where is it actually
[00:56:06.780 --> 00:56:21.780]   the cognitive ability to say, I don't have the answer to the multiple trees, where is that
[00:56:21.780 --> 00:56:24.780]   cognitive ability to say, I don't have the answer to the multiple trees?
[00:56:24.780 --> 00:56:25.780]   Yeah.
[00:56:25.780 --> 00:56:32.780]   So the question was, with an agent in a multi-turn conversation setting, how does it know whether
[00:56:32.780 --> 00:56:35.780]   or not it has the answer to a given question?
[00:56:35.780 --> 00:56:38.780]   Where to go.
[00:56:38.780 --> 00:56:39.780]   Where to go.
[00:56:39.780 --> 00:56:40.780]   Exactly.
[00:56:40.780 --> 00:56:41.780]   Yeah.
[00:56:41.780 --> 00:56:43.780]   That's right.
[00:56:43.780 --> 00:56:47.780]   So there's a couple different ways to kind of break this down.
[00:56:47.780 --> 00:56:53.780]   So with these agents, there's a few levels of instruction you give it.
[00:56:53.780 --> 00:56:56.780]   First, you give it an overall agent prompt.
[00:56:56.780 --> 00:57:02.780]   So if I look at the notebook here, we can go look at the React agent as an example of this.
[00:57:02.780 --> 00:57:06.780]   So the React agent is defined right here.
[00:57:06.780 --> 00:57:08.780]   This is kind of like the planning step.
[00:57:08.780 --> 00:57:10.780]   Here's my, like, naive prompt.
[00:57:10.780 --> 00:57:11.780]   Okay.
[00:57:11.780 --> 00:57:14.780]   So your helpful assistant is tasked with answering tasks.
[00:57:14.780 --> 00:57:19.780]   Use the provided vector store to retrieve documents, grade them, and go on.
[00:57:19.780 --> 00:57:21.780]   Now let's take the case that's more complicated.
[00:57:21.780 --> 00:57:22.780]   Let's say I had two vector stores.
[00:57:22.780 --> 00:57:28.780]   So one thing I can do is I can explicitly put in the agent prompt, you have two vector stores,
[00:57:28.780 --> 00:57:32.780]   A, B. A has this, B has this.
[00:57:32.780 --> 00:57:39.780]   And then you're implicitly having the LLM giving it the ability to kind of reason out which one to use.
[00:57:39.780 --> 00:57:41.780]   Now this is where the second piece comes in.
[00:57:41.780 --> 00:57:44.780]   You have to also, you also bind it to a set of tools.
[00:57:44.780 --> 00:57:47.780]   This is really where the decision-making comes in.
[00:57:47.780 --> 00:57:50.780]   When you create this tool, so here's retrieve documents, right?
[00:57:50.780 --> 00:57:56.780]   This tool description is captured by the agent so the agent knows what's in this tool.
[00:57:56.780 --> 00:58:02.780]   And this is really where that decision to use this retriever tool versus another one would be made.
[00:58:02.780 --> 00:58:07.780]   It'd be a combination of the prompt you give to the agent and/or the tool description.
[00:58:07.780 --> 00:58:11.780]   So if you had two vector stores, you could basically say retrieve documents one.
[00:58:11.780 --> 00:58:15.780]   This vector store contains information about X, another one contains information about Y.
[00:58:15.780 --> 00:58:22.780]   Then the agent is deciding what tool to call based on that description and maybe based on its overall prompt.
[00:58:22.780 --> 00:58:26.780]   But to your point, it's not easy.
[00:58:26.780 --> 00:58:33.780]   So actually, with custom agent, this is with the React style agent, with the Landgraf custom agent, you can do it a little bit differently.
[00:58:33.780 --> 00:58:39.780]   Where I actually don't have it in this notebook, but I have other cases where you actually can build a router node.
[00:58:39.780 --> 00:58:42.780]   And I mean, I'll show you actually.
[00:58:42.780 --> 00:58:49.780]   So this particular notebook, so this one, this self-read, it's in the slides.
[00:58:49.780 --> 00:58:52.780]   If you open this up, we did this with Llama folks.
[00:58:52.780 --> 00:58:54.780]   So this is actually a trick I really like.
[00:58:54.780 --> 00:58:59.780]   If you go to, if you go to Landgraf rag agent local here.
[00:58:59.780 --> 00:59:04.780]   See, it's Wi-Fi is a little slow.
[00:59:04.780 --> 00:59:13.780]   What I define here is a very specific router at the start of my agent that decides where to send the query.
[00:59:13.780 --> 00:59:17.780]   And this is something I really like to do because like we saw with the React thing,
[00:59:17.780 --> 00:59:24.780]   it has to kind of decide the right tool to use, which can be kind of noisy, versus right here.
[00:59:24.780 --> 00:59:27.780]   So here's my router, right?
[00:59:27.780 --> 00:59:29.780]   This is reliable enough to run locally.
[00:59:29.780 --> 00:59:33.780]   And what I do here is I run this at the start of my graph.
[00:59:33.780 --> 00:59:40.780]   And I have the agent, or yeah, the agent explicitly take the question and decide what to use.
[00:59:40.780 --> 00:59:45.780]   And based on what decision it makes, I can send it to either web search, or in this case, a vector store.
[00:59:45.780 --> 00:59:53.780]   So to answer your question, if I pull all the way back, I personally like to do explicit routing as a node at the start of my graph.
[00:59:53.780 --> 00:59:55.780]   Because it's, it's pretty clean.
[00:59:55.780 --> 00:59:59.780]   And you can see in the flow of this overall graph, this router runs first.
[00:59:59.780 --> 01:00:02.780]   And it looks at my question and sends it to one of two places.
[01:00:02.780 --> 01:00:03.780]   And this can be more complex.
[01:00:03.780 --> 01:00:04.780]   You can send it to one of n places.
[01:00:04.780 --> 01:00:06.780]   But I see one of two here.
[01:00:06.780 --> 01:00:08.780]   This is with like a custom line graph agent.
[01:00:08.780 --> 01:00:09.780]   This is what I like to do.
[01:00:09.780 --> 01:00:20.780]   If you're using a React agent, it is then using a combination of the, the tool definitions and the overall agent prompt to decide what tool to call.
[01:00:20.780 --> 01:00:25.780]   But you can see it's more squishy because it has to call the right tool.
[01:00:25.780 --> 01:00:33.780]   And as opposed to giving it a router and saying always run this router first, it has to kind of make the right decision as to what tool to call based on the context question, which is harder.
[01:00:33.780 --> 01:00:47.780]   And that gets back to our, that gets back to our overall story about these kind of line graph explicitly defined agents are more reliable because you can like lay out this routing step right up front and have it always execute that step before going forward.
[01:00:47.780 --> 01:01:00.780]   So, if you're not looking back at it, you're like, hey, I actually have the answer.
[01:01:00.780 --> 01:01:12.780]   So, this is the internet, so, this is everything around and go, you've got to, like, actually graph and say, do I have a history of the answer?
[01:01:12.780 --> 01:01:13.780]   Can I answer that?
[01:01:13.780 --> 01:01:14.780]   Yep.
[01:01:14.780 --> 01:01:15.780]   Okay.
[01:01:15.780 --> 01:01:16.780]   Got it.
[01:01:16.780 --> 01:01:30.780]   So, the question was, how do I incorporate the idea of routing with history?
[01:01:30.780 --> 01:01:32.780]   So, here's what you do.
[01:01:32.780 --> 01:01:36.780]   It's actually, you know, kind of, it should be pretty straightforward.
[01:01:36.780 --> 01:01:39.780]   You can define this router node in your graph.
[01:01:39.780 --> 01:01:43.780]   And that router node, I'll actually go down to it.
[01:01:43.780 --> 01:01:49.780]   So, basically, here's my graph and here's the, the route question node, right?
[01:01:49.780 --> 01:01:50.780]   Yeah.
[01:01:50.780 --> 01:01:56.780]   Actually, in this particular case, it's, it's an edge.
[01:01:56.780 --> 01:01:57.780]   Don't worry about those details.
[01:01:57.780 --> 01:02:03.780]   Basically, what you could do is, you could have a node that takes in the state.
[01:02:03.780 --> 01:02:05.780]   Now, that state could include your history.
[01:02:05.780 --> 01:02:19.780]   So, what you could do is, in that router prompt, you could really easily, here, include another placeholder variable for, like, your message history or something.
[01:02:19.780 --> 01:02:27.780]   And yet, what you could say, then, is, make a decision about where to go next based upon the question and based upon something in our history.
[01:02:27.780 --> 01:02:33.780]   And so, you actually would plumb in your message history here and use it to jointly decide what to do next.
[01:02:33.780 --> 01:02:40.780]   So, that's actually really easily handled in line graph using a node and you can reference the history.
[01:02:40.780 --> 01:02:46.780]   They can be passed, you can pass into that node as state.
[01:02:46.780 --> 01:02:47.780]   Cool.
[01:02:47.780 --> 01:02:52.780]   Yep.
[01:02:52.780 --> 01:02:53.780]   The notion of state.
[01:02:53.780 --> 01:02:56.780]   Yeah, let's, let's talk about state in a little bit more detail.
[01:02:56.780 --> 01:03:03.780]   So, let's actually go to the notebooks that we're working with here that I've shared.
[01:03:03.780 --> 01:03:08.780]   So, here's this rag agent testing notebook.
[01:03:08.780 --> 01:03:18.780]   So, if you go down to the custom line graph agent, the way you do it is, I'll find the state.
[01:03:18.780 --> 01:03:19.780]   Yeah.
[01:03:19.780 --> 01:03:21.780]   So, here's what I call graph state.
[01:03:21.780 --> 01:03:27.780]   So, the graph state is basically something that lives across the lifetime of my graph.
[01:03:27.780 --> 01:03:31.780]   I typically like to do something as simple as just a dictionary.
[01:03:31.780 --> 01:03:34.780]   So, basically, this is a rag graph.
[01:03:34.780 --> 01:03:41.780]   And here, I'm basically going to define a number of attributes in my state that are relevant to what I want to do with rag.
[01:03:41.780 --> 01:03:48.780]   A question, my answer generated, whether or not to run search, some documents, my step list.
[01:03:48.780 --> 01:03:53.780]   And basically, the idea here is that I define my state up front.
[01:03:53.780 --> 01:03:59.780]   And then at every node, I basically accept state as the input.
[01:03:59.780 --> 01:04:03.780]   And then I operate on it in some way and right back out to state.
[01:04:03.780 --> 01:04:09.780]   So, basically, what's happening is I define state generally up front as like a dictionary or something like that.
[01:04:09.780 --> 01:04:14.780]   The placeholder for things I want to modify throughout my graph, throughout my agent.
[01:04:14.780 --> 01:04:20.780]   And every node just takes in state, does something, and writes back out to state.
[01:04:20.780 --> 01:04:21.780]   That's really it.
[01:04:21.780 --> 01:04:28.780]   So, basically, it's a way you can think of it as a really simple mechanism to persist information across the lifetime of my agent.
[01:04:28.780 --> 01:04:32.780]   And for this rag agent, it's things that are really intuitive for rag.
[01:04:32.780 --> 01:04:34.780]   It's like question, it's documents.
[01:04:34.780 --> 01:04:36.780]   And so, let's take an example.
[01:04:36.780 --> 01:04:39.780]   Like, okay, here's a fun one.
[01:04:39.780 --> 01:04:41.780]   So, my great documents node, right?
[01:04:41.780 --> 01:04:43.780]   What I'm doing here is I'm taking in state.
[01:04:43.780 --> 01:04:45.780]   And from my state, it's just a dictionary.
[01:04:45.780 --> 01:04:46.780]   So, I'm extracting my question.
[01:04:46.780 --> 01:04:48.780]   I'm extracting my documents.
[01:04:48.780 --> 01:04:52.780]   And I'm appending a new step.
[01:04:52.780 --> 01:04:54.780]   I'm notifying, hey, here's my new step.
[01:04:54.780 --> 01:04:56.780]   And basically, I'm doing some operation.
[01:04:56.780 --> 01:04:58.780]   I'm iterating through my documents.
[01:04:58.780 --> 01:04:59.780]   I'm grading each one.
[01:04:59.780 --> 01:05:03.780]   If the grade's yes, I keep it.
[01:05:03.780 --> 01:05:09.780]   If the grade is no, so yes, no means like, is it relevant or not, basically.
[01:05:09.780 --> 01:05:11.780]   So, if yes, it's relevant, I keep it.
[01:05:11.780 --> 01:05:13.780]   I put in this filter docs list.
[01:05:13.780 --> 01:05:17.780]   If it's not, I set the search flag to yes, which means I'm going to run web search.
[01:05:17.780 --> 01:05:20.780]   Because I want to supplement, I have some docs that are irrelevant.
[01:05:20.780 --> 01:05:22.780]   And I write back to state at the end.
[01:05:22.780 --> 01:05:26.780]   My filter docs, my question, search, and the steps.
[01:05:26.780 --> 01:05:27.780]   That's it.
[01:05:27.780 --> 01:05:32.780]   So, state's a really convenient way to just pass information across my agent.
[01:05:32.780 --> 01:05:37.780]   And I like using a dictionary, just like nice and clean to manage.
[01:05:37.780 --> 01:05:40.780]   As opposed to a message history, which is like a little more confusing.
[01:05:40.780 --> 01:05:44.780]   Like, in any node, if you use a message history, it's like a stack of messages.
[01:05:44.780 --> 01:05:48.780]   So, if you want the question, you have to like, it's like usually the first message.
[01:05:48.780 --> 01:05:49.780]   You have to just index it.
[01:05:49.780 --> 01:05:50.780]   It's just kind of ugly.
[01:05:50.780 --> 01:05:55.780]   I like using a dict, which is like, I just get the question out as a key in my dictionary.
[01:05:55.780 --> 01:05:56.780]   Okay.
[01:05:56.780 --> 01:05:58.780]   Cool.
[01:05:58.780 --> 01:05:59.780]   Let's see.
[01:05:59.780 --> 01:06:04.780]   We're about an hour in.
[01:06:04.780 --> 01:06:12.780]   I can also, you know, let people just hack and walk around, talk to people, stay up here,
[01:06:12.780 --> 01:06:13.780]   whatever's best.
[01:06:13.780 --> 01:06:16.780]   And you keep asking questions if you want to.
[01:06:16.780 --> 01:06:21.780]   I think people are just working, doing their own thing now anyway.
[01:06:21.780 --> 01:06:25.780]   So, I might ask one question just for fun.
[01:06:25.780 --> 01:06:27.780]   Is anyone interested in local agents?
[01:06:27.780 --> 01:06:29.780]   We didn't talk about that too much.
[01:06:29.780 --> 01:06:30.780]   It's a big theme.
[01:06:30.780 --> 01:06:31.780]   Yeah.
[01:06:31.780 --> 01:06:34.780]   So, I shared a notebook for that.
[01:06:34.780 --> 01:06:39.780]   And by default, I am using Llama 3.
[01:06:39.780 --> 01:06:42.780]   You can absolutely test other models.
[01:06:42.780 --> 01:06:49.780]   So, this is set up to test just Llama 3 with a Llama.
[01:06:49.780 --> 01:06:50.780]   Try other things.
[01:06:50.780 --> 01:06:53.780]   I have a M2 32 gig.
[01:06:53.780 --> 01:06:55.780]   So, 8B runs fine for me.
[01:06:55.780 --> 01:07:00.780]   If you sling bigger, you can actually bump that up a little bit.
[01:07:00.780 --> 01:07:03.780]   So, that can be a nice idea.
[01:07:03.780 --> 01:07:08.780]   Yeah, exactly.
[01:07:08.780 --> 01:07:15.780]   70B is, it's unfortunate that, yeah, I actually want a bigger machine so I can run that.
[01:07:15.780 --> 01:07:25.780]   Because I found for tool calling, 8B is really at the edge of reliability.
[01:07:25.780 --> 01:07:31.780]   So, that's actually why you really can't run the React agent locally with an 8B model reliably.
[01:07:31.780 --> 01:07:36.780]   You can run the LandGraph agent very reliably because it doesn't actually need tool calling.
[01:07:36.780 --> 01:07:38.780]   It only needs structured outputs, you'll see in the notebook.
[01:07:38.780 --> 01:07:41.780]   So, that's actually a really nice thing.
[01:07:43.780 --> 01:07:46.780]   But React agent won't run locally.
[01:07:46.780 --> 01:07:47.780]   Reliably, at least.
[01:07:47.780 --> 01:07:48.780]   Yep.
[01:07:48.780 --> 01:07:56.780]   Regarding RAG, what is the typical chunk size?
[01:07:56.780 --> 01:07:58.780]   You say chunk documents?
[01:07:58.780 --> 01:07:59.780]   Yeah.
[01:07:59.780 --> 01:08:00.780]   Okay, yeah.
[01:08:00.780 --> 01:08:01.780]   So, this question always comes up.
[01:08:01.780 --> 01:08:04.780]   With RAG, what is the typical chunk size?
[01:08:04.780 --> 01:08:05.780]   Yeah.
[01:08:05.780 --> 01:08:08.780]   If you ask 10 people, you'll get 10 answers.
[01:08:08.780 --> 01:08:11.780]   It's notoriously ad hoc.
[01:08:11.780 --> 01:08:12.780]   You know what?
[01:08:12.780 --> 01:08:20.780]   To be honest, I did kind of a talk on the future of RAG with long context models.
[01:08:20.780 --> 01:08:25.780]   I'm kind of a fan of trying to keep chunk size as large as possible.
[01:08:25.780 --> 01:08:30.780]   Actually, I think, this is a whole tangent, but I actually think one of the nicest tricks,
[01:08:30.780 --> 01:08:41.780]   let me see if I have a good visual for it, basically, let me try to find something here.
[01:08:41.780 --> 01:08:43.780]   This is a whole separate talk.
[01:08:43.780 --> 01:08:49.780]   But basically, I think, yeah, this one, RAG and long context.
[01:08:49.780 --> 01:08:52.780]   So, this is a whole different thing.
[01:08:52.780 --> 01:08:57.780]   But, yeah, this idea.
[01:08:57.780 --> 01:09:04.780]   So, I think for RAG, one of the ideas I like the most is decoupling.
[01:09:04.780 --> 01:09:05.780]   I'll explain this.
[01:09:05.780 --> 01:09:07.780]   I'll just say it and then I'll explain it.
[01:09:07.780 --> 01:09:10.780]   Decoupling what you actually index for retrieval from what you pass the LLM.
[01:09:10.780 --> 01:09:13.780]   Because you have this weird tension, right?
[01:09:13.780 --> 01:09:19.780]   Smaller semantic-related chunks are good for retrieval relative to a question, right?
[01:09:19.780 --> 01:09:25.780]   But LLMs can process huge amounts of context at this point, you know, up to, say, a million tokens.
[01:09:25.780 --> 01:09:32.780]   So, historically, what we would do is you would chunk really small, like, very, very -- try to get it as tight as possible.
[01:09:32.780 --> 01:09:41.780]   All these tricks, semantic chunking, a lot of things to really compress down to just compress and group semantic-related chunks of context, right?
[01:09:41.780 --> 01:09:48.780]   But the problem is then you're passing to the LLM very narrow chunks of information, which has problems in recall.
[01:09:48.780 --> 01:09:52.780]   So, basically, it's more likely that you'll miss context necessary.
[01:09:52.780 --> 01:09:56.780]   So, it's good for retrieval but bad for answer generation.
[01:09:56.780 --> 01:10:02.780]   So, a nice trick is, for retrieval, use some chunking strategy, whatever you want.
[01:10:02.780 --> 01:10:04.780]   Like, make it, you know, small.
[01:10:04.780 --> 01:10:08.780]   But you can actually use, like, a doc store to store the full document.
[01:10:08.780 --> 01:10:17.780]   And what you can do is retrieve based on small chunks but then reference the full document and pass the full document to the LLM for actual generation time.
[01:10:17.780 --> 01:10:22.780]   So, you decouple the problem of, like, are you sure you're passing sufficient context to LLM itself?
[01:10:22.780 --> 01:10:27.780]   Now, I also understand if you have massive documents, it can be wasteful in terms of tokens to pass full documents through.
[01:10:27.780 --> 01:10:42.780]   But there's some Pareto optimum here where I think being too strict with your indexing approach doesn't make sense anymore given that you can process very large context in your LLM.
[01:10:42.780 --> 01:10:45.780]   So, you want to avoid being overly restrictive with your chunk size.
[01:10:45.780 --> 01:10:49.780]   So, this is a nice way to get around it, basically, to summarize.
[01:10:49.780 --> 01:10:52.780]   You can choose a chunking strategy, whatever one you want.
[01:10:52.780 --> 01:10:58.780]   But I like this idea of referencing full documents and then basically passing full documents to the LLM for the answer itself.
[01:10:58.780 --> 01:11:05.780]   It gets around the problem of an overly aggressive chunking strategy that misses context needed to actually answer your question.
[01:11:05.780 --> 01:11:10.780]   And I think with long context LLM is getting cheaper and cheaper, this is starting to make more sense.
[01:11:10.780 --> 01:11:11.780]   I'm trying to think.
[01:11:11.780 --> 01:11:13.780]   I had a whole kind of, like, slide on.
[01:11:13.780 --> 01:11:17.780]   Yeah, this one is kind of, like, balancing system complexity and latency.
[01:11:17.780 --> 01:11:21.780]   So, it's kind of, like, on the left is, like, maybe the historical view.
[01:11:21.780 --> 01:11:24.780]   You need the exact relevant chunk.
[01:11:24.780 --> 01:11:29.780]   You can get really complex chunking schemes, a lot of overengineering, lower recall.
[01:11:29.780 --> 01:11:36.780]   Like, if you're passing 100 token chunks to your LLM for the final answer, you might miss the exact, you know, part of the document that's necessary.
[01:11:36.780 --> 01:11:40.780]   Very sensitive to chunk size K, all these weird parameters, right?
[01:11:40.780 --> 01:11:45.780]   On the other extreme, just throw in everything, throw everything into context.
[01:11:45.780 --> 01:11:50.780]   Google, actually, I think this week will probably announce some interesting stuff with context caching.
[01:11:50.780 --> 01:11:51.780]   Seems really cool.
[01:11:51.780 --> 01:11:53.780]   Maybe that actually could be a really good option for this.
[01:11:53.780 --> 01:11:59.780]   But higher latency, higher token usage, can't audit retrieval, security authentication.
[01:11:59.780 --> 01:12:04.780]   Like, if you're passing 10 million tokens of context in for your answer generation.
[01:12:04.780 --> 01:12:07.780]   So, something in the middle is what I'm advocating for.
[01:12:07.780 --> 01:12:22.780]   And I think this kind of document level decoupling, where basically you index and reference full documents and pass full documents to your LLM, is, like, a nice trick.
[01:12:22.780 --> 01:12:24.780]   We've seen a lot of people use this pretty effectively.
[01:12:24.780 --> 01:12:25.780]   So, yeah.
[01:12:25.780 --> 01:12:26.780]   Yeah.
[01:12:26.780 --> 01:12:27.780]   Yeah, kind of a .
[01:12:27.780 --> 01:12:37.780]   Is there any way or any guarantees or tricks that you have to make sure that you're not actually missing any of the elements of the context around the entire window?
[01:12:37.780 --> 01:12:43.780]   Like, how do you measure that you're actually using all of the context as well as the beginning or end?
[01:12:43.780 --> 01:12:46.780]   Okay, that's interesting.
[01:12:46.780 --> 01:12:54.780]   So, the question was, how can you evaluate the amount of context you are using?
[01:12:54.780 --> 01:12:55.780]   Okay.
[01:12:55.780 --> 01:12:58.780]   Yeah, like, does it definitely use the whole state?
[01:12:58.780 --> 01:13:00.780]   I don't want to miss any.
[01:13:00.780 --> 01:13:01.780]   Okay.
[01:13:01.780 --> 01:13:06.780]   But I guess, so, in a RAG context, you have a question, you have an answer.
[01:13:06.780 --> 01:13:10.780]   So, you have a question, you have an answer, and you have some retrieved documents.
[01:13:10.780 --> 01:13:16.780]   So, you can evaluate the question relative to your document.
[01:13:16.780 --> 01:13:18.780]   That's one way to get at this.
[01:13:18.780 --> 01:13:21.780]   Like, how much of the document is relevant to your question?
[01:13:21.780 --> 01:13:23.780]   So, that's maybe one approach.
[01:13:23.780 --> 01:13:29.780]   And, actually, the notebooks show a few prompts to kind of get at that.
[01:13:29.780 --> 01:13:36.780]   And, actually, a good way to think about that is you can think about document precision and document recall.
[01:13:36.780 --> 01:13:39.780]   And this is a little confusing, maybe, so I should explain it.
[01:13:39.780 --> 01:13:44.780]   Basically, document recall is, does the document contain the answer to my question anywhere?
[01:13:44.780 --> 01:13:48.780]   So, let's say you have a 100-page document on page 55 is my answer.
[01:13:48.780 --> 01:13:49.780]   Recall is one.
[01:13:49.780 --> 01:13:50.780]   It's in there.
[01:13:50.780 --> 01:13:54.780]   Precision is the other side of that coin, which is, does it contain information not relevant to my question?
[01:13:54.780 --> 01:13:57.780]   In that particular case, huge amount of irrelevant information.
[01:13:57.780 --> 01:14:00.780]   So, recall will be one, precision will be very low.
[01:14:00.780 --> 01:14:02.780]   So, that's one thing you can do.
[01:14:02.780 --> 01:14:04.780]   You can actually look at your retrieved docs, measure precision and recall.
[01:14:04.780 --> 01:14:08.780]   That's, like, one thing I think I would probably like to do there.
[01:14:08.780 --> 01:14:13.780]   And that's probably the best way to get at this question of, like, how much of my documents am I actually using?
[01:14:13.780 --> 01:14:17.780]   Now, with this approach, your recall will be high.
[01:14:17.780 --> 01:14:18.780]   Your precision will be kind of low.
[01:14:18.780 --> 01:14:19.780]   And you would say, I don't care.
[01:14:19.780 --> 01:14:20.780]   It's fine.
[01:14:20.780 --> 01:14:25.780]   If I have a model that's super cheap to use a large number of tokens, I'm okay.
[01:14:25.780 --> 01:14:27.780]   Maybe I'll frame it another way.
[01:14:27.780 --> 01:14:29.780]   I care more about recall than precision.
[01:14:29.780 --> 01:14:35.780]   I want to make sure I always -- I answer the question, if I pass a little bit more context than necessary, I'm okay with that.
[01:14:35.780 --> 01:14:45.780]   Versus if you're a precision-gated system, then you would say, okay, I'm going to miss the answer sometimes, but I'm okay with that because I never want to pass more context than necessary.
[01:14:45.780 --> 01:14:51.780]   I think a lot of people are moving towards higher recall because these LMs are getting cheaper and they can process more context.
[01:14:51.780 --> 01:14:54.780]   So, that's kind of how I might think about it.
[01:14:54.780 --> 01:15:06.780]   And I think this approach is a nice idea of, you know, indexing full documents -- or indexing chunks, but then referencing full documents, passing full documents to your LLM to actually generate answers.
[01:15:06.780 --> 01:15:07.780]   So, yep.
[01:15:07.780 --> 01:15:17.780]   I was wondering if it seems like there's a couple of options for where exactly you inject the context into the conversation.
[01:15:17.780 --> 01:15:24.780]   So, just for one example you could stick something into the system and say, hey, this is your knowledge that you know.
[01:15:24.780 --> 01:15:29.780]   And then the user just asks the question and there's no prompting from the user.
[01:15:29.780 --> 01:15:36.780]   The other thing is you can completely modify what the user -- what the quote-unquote user is, and then they answer the question.
[01:15:36.780 --> 01:15:45.780]   And then, like, a further idea on top of that is you ask a question and you ask a follow-up question.
[01:15:45.780 --> 01:15:48.780]   Maybe, if you say, we collaborate.
[01:15:48.780 --> 01:15:53.780]   There's no need to actually retrieve anything, but you're just having to pass.
[01:15:53.780 --> 01:15:55.780]   That can be passed.
[01:15:55.780 --> 01:15:57.780]   Or you ask something else for retrieval.
[01:15:57.780 --> 01:16:04.780]   Do you have any, like, observations as far as things that work well, or capital work well, or continually
[01:16:04.780 --> 01:16:08.780]   impending things into the system process, or, like, adding another user?
[01:16:08.780 --> 01:16:10.780]   Like, where do you stick that context?
[01:16:10.780 --> 01:16:13.780]   How much of a history?
[01:16:13.780 --> 01:16:14.780]   Yep.
[01:16:14.780 --> 01:16:15.780]   Yeah, okay.
[01:16:15.780 --> 01:16:16.780]   So, this is a great question.
[01:16:16.780 --> 01:16:25.780]   The question was related to kind of, like, agentic rag and where to actually put documents.
[01:16:25.780 --> 01:16:27.780]   So, let's walk through the cases.
[01:16:27.780 --> 01:16:34.780]   So, case one is I have a fixed context for every question you're just going to ask.
[01:16:34.780 --> 01:16:38.780]   Let's say I have, like, a rag bot against, like, one particular document.
[01:16:38.780 --> 01:16:41.780]   And that document is always going to be referenced.
[01:16:41.780 --> 01:16:43.780]   So, that -- you make a very good point.
[01:16:43.780 --> 01:16:48.780]   In that case, what I would do is -- let's go back to our, like, agent example.
[01:16:48.780 --> 01:16:51.780]   You can put that in the system prompt itself, like you said.
[01:16:51.780 --> 01:16:55.780]   So, and actually, I'll mention something else about this.
[01:16:55.780 --> 01:16:57.780]   It would be, like, right here.
[01:16:57.780 --> 01:16:59.780]   So, here's your system prompt for your agent.
[01:16:59.780 --> 01:17:01.780]   You just plumb that whole document in.
[01:17:01.780 --> 01:17:04.780]   You say, every question you're going to reference this document.
[01:17:04.780 --> 01:17:05.780]   You don't need retriever system.
[01:17:05.780 --> 01:17:06.780]   You're done.
[01:17:06.780 --> 01:17:08.780]   That's a very nice case.
[01:17:08.780 --> 01:17:09.780]   No retrieval complexity.
[01:17:09.780 --> 01:17:12.780]   You just context stuff your whole document.
[01:17:12.780 --> 01:17:19.780]   So, the thing that Google is going to announce this week, I believe, this context caching seems really interesting for this.
[01:17:19.780 --> 01:17:24.780]   Because, basically, what they're saying is -- if I get it right, I think Logan will speak to this.
[01:17:24.780 --> 01:17:29.780]   But I think they have a context when a million to 10 million tokens is huge, right?
[01:17:29.780 --> 01:17:34.780]   So, basically, you can take a large set of documents and stuff them into this model, effectively.
[01:17:34.780 --> 01:17:36.780]   And they house them for you, somehow.
[01:17:36.780 --> 01:17:38.780]   I think you have some minor data storage fee.
[01:17:38.780 --> 01:17:45.780]   But then, for every inference call, they don't charge you for all those tokens that are cached.
[01:17:45.780 --> 01:17:46.780]   Which is pretty nice.
[01:17:46.780 --> 01:17:49.780]   So, basically, here's the use case, to your point exactly.
[01:17:49.780 --> 01:17:51.780]   I have some set of documentation.
[01:17:51.780 --> 01:17:52.780]   It's 10 million tokens.
[01:17:52.780 --> 01:17:53.780]   That's a lot of pages.
[01:17:53.780 --> 01:17:54.780]   Hundreds of pages.
[01:17:54.780 --> 01:17:56.780]   I have it cached with the model.
[01:17:56.780 --> 01:18:02.780]   And every time I usually ask a question, I don't get charged, you know, 10 million tokens to process the answer.
[01:18:02.780 --> 01:18:03.780]   They just cached for me.
[01:18:03.780 --> 01:18:04.780]   Really nice idea.
[01:18:04.780 --> 01:18:06.780]   So, that's your point of, like, the first thing.
[01:18:06.780 --> 01:18:08.780]   That's, like, not quite in the system prompt.
[01:18:08.780 --> 01:18:09.780]   It's in, like, the cache.
[01:18:09.780 --> 01:18:10.780]   But that's the same idea.
[01:18:10.780 --> 01:18:13.780]   So, you have cached or system prompt fixed context.
[01:18:13.780 --> 01:18:14.780]   That's, like, case one.
[01:18:14.780 --> 01:18:19.780]   So, case two is you have -- you want to dynamically retrieve.
[01:18:19.780 --> 01:18:20.780]   So, you can't stuff your context.
[01:18:20.780 --> 01:18:24.780]   Maybe you have a few different vector stores, like we were talking about here with routing.
[01:18:24.780 --> 01:18:28.780]   So, in that case, yeah, you have to use an index of some sort.
[01:18:28.780 --> 01:18:31.780]   Maybe a router to choose which index to retrieve from.
[01:18:31.780 --> 01:18:33.780]   So, that's kind of case two.
[01:18:33.780 --> 01:18:38.780]   And I'm trying to remember the -- oh, okay.
[01:18:38.780 --> 01:18:46.780]   So, like, in that particular case, for follow-up questions, how do I kind of control whether I re-retrieve or not?
[01:18:46.780 --> 01:18:50.780]   So, that's the nice thing about either one of these agents.
[01:18:50.780 --> 01:18:52.780]   It has some state.
[01:18:52.780 --> 01:18:54.780]   So, the state lives across lights on the agent.
[01:18:54.780 --> 01:18:59.780]   So, basically, the agent -- and this actually gets exactly what the other question was on.
[01:18:59.780 --> 01:19:04.780]   Let's say I built my agent with -- I'll show you right here.
[01:19:04.780 --> 01:19:09.780]   So, let's say I have a router node at the start of my agent, okay?
[01:19:09.780 --> 01:19:12.780]   And that router has access to state.
[01:19:12.780 --> 01:19:16.780]   What I can do is then, given a question -- this could be -- let's say it's a multi-turn thing.
[01:19:16.780 --> 01:19:18.780]   This is the second question in my conversation.
[01:19:18.780 --> 01:19:21.780]   I have an appended state from the rest of my discussion here.
[01:19:21.780 --> 01:19:24.780]   The agent knows it returned an answer.
[01:19:24.780 --> 01:19:31.780]   So, basically, when a new question comes in, you could pass, like, the entire state back to that router.
[01:19:31.780 --> 01:19:35.780]   And the router could know, okay, here's the docs I've already retrieved.
[01:19:35.780 --> 01:19:40.780]   And it can basically then decide to answer directly because I already have the answer to the question.
[01:19:40.780 --> 01:19:49.780]   So, that's a long way of saying you can use state, either message history or explicitly defined in your LandGraph agent,
[01:19:49.780 --> 01:19:55.780]   to preserve docs that you've retrieved already, and then to just use them to answer the question without re-retrieving.
[01:19:55.780 --> 01:20:00.780]   So, that's kind of what these -- these rag agents can be really good at.
[01:20:00.780 --> 01:20:06.780]   That was kind of, like, storing that in short-term memory and reasoning about, hey, do I need to re-retrieve or not?
[01:20:06.780 --> 01:20:10.780]   So, that's exactly the intuition behind why these rag agents can be pretty nice.
[01:20:10.780 --> 01:20:11.780]   Yeah?
[01:20:11.780 --> 01:20:12.780]   Yeah.
[01:20:12.780 --> 01:20:14.780]   There's no problem with that.
[01:20:14.780 --> 01:20:18.780]   It seems like, you know, this idea, like, rag is a hack, right?
[01:20:18.780 --> 01:20:24.780]   Like, you just kind of .
[01:20:24.780 --> 01:20:25.780]   Yeah?
[01:20:25.780 --> 01:20:34.780]   I mean, it seems like there's a bit of a back and forth going on where the model is designed in this kind of purpose way,
[01:20:34.780 --> 01:20:40.780]   where the coder packs something together, and now, you know, you go back to the training model,
[01:20:40.780 --> 01:20:45.780]   and then you say, "Oh, well, we've actually defined the model, and this is going to be good at this."
[01:20:45.780 --> 01:20:50.780]   And I'm curious, it almost seems like you're going to, like, propose all these ideas,
[01:20:50.780 --> 01:20:51.780]   and this is going to work better.
[01:20:51.780 --> 01:20:51.780]   Yeah.
[01:20:51.780 --> 01:20:52.780]   Yeah.
[01:20:52.780 --> 01:20:53.780]   Yeah.
[01:20:53.780 --> 01:20:54.780]   Yeah.
[01:20:54.780 --> 01:20:55.780]   Yeah.
[01:20:55.780 --> 01:20:56.780]   Yeah.
[01:20:56.780 --> 01:20:56.780]   Yeah.
[01:20:56.780 --> 01:20:57.780]   Yeah.
[01:20:57.780 --> 01:20:58.780]   Yeah.
[01:20:58.780 --> 01:20:59.780]   Yeah.
[01:20:59.780 --> 01:21:00.780]   Yeah.
[01:21:00.780 --> 01:21:01.780]   Yeah.
[01:21:01.780 --> 01:21:02.780]   Exactly.
[01:21:02.780 --> 01:21:02.780]   Okay.
[01:21:02.780 --> 01:21:04.780]   This is a really good discussion and whole debate.
[01:21:04.780 --> 01:21:14.780]   So, the highest level framing of this is how do you want to -- how do you want your model to learn?
[01:21:14.780 --> 01:21:17.780]   So, one option is you can modify the weights of the model itself with something like fine-tuning.
[01:21:17.780 --> 01:21:21.780]   Another is you can use what we call in-context learning through your prompt.
[01:21:21.780 --> 01:21:22.780]   So, rag is like a form of in-context learning.
[01:21:22.780 --> 01:21:22.780]   I'm basically giving up some documents.
[01:21:22.780 --> 01:21:23.780]   It's reasonable as documents producing answers.
[01:21:23.780 --> 01:21:24.780]   It's not touching the weights of my model.
[01:21:24.780 --> 01:21:28.780]   Fine-tuning would be taking the knowledge I want to run rag on, fine-tuning your model and
[01:21:28.780 --> 01:21:29.780]   updating the weights so it has that knowledge.
[01:21:29.780 --> 01:21:32.780]   So, the highest level framing of this is how do you want your model to learn?
[01:21:32.780 --> 01:21:33.780]   So, one option is you can modify the weights of the model itself with something like fine-tuning.
[01:21:33.780 --> 01:21:36.780]   Another is you can use what we call in-context learning through your prompt.
[01:21:36.780 --> 01:21:38.780]   So, rag is like a form of in-context learning.
[01:21:38.780 --> 01:21:39.780]   I'm basically giving up some documents.
[01:21:39.780 --> 01:21:42.780]   It's reasonable as documents producing answers.
[01:21:42.780 --> 01:21:44.780]   It's not touching the weights of my model.
[01:21:44.780 --> 01:21:49.780]   Fine-tuning would be taking the knowledge I want to run rag on, fine-tuning your model and
[01:21:49.780 --> 01:21:51.780]   updating the weights so it has that knowledge.
[01:21:51.780 --> 01:21:58.780]   There's a lot of debates on this and actually I think Hamel has a whole course on fine-tuning.
[01:21:58.780 --> 01:21:59.780]   Oh, yeah.
[01:21:59.780 --> 01:22:00.780]   Do you have a...
[01:22:00.780 --> 01:22:01.780]   I just want to clarify.
[01:22:01.780 --> 01:22:03.780]   I don't mean fine-tuning information.
[01:22:03.780 --> 01:22:06.780]   I just mean fine-tuning so that you have the idea.
[01:22:06.780 --> 01:22:11.780]   Like, if you could find things so that it knows, oh, I should always retrieve this information
[01:22:11.780 --> 01:22:16.780]   from my system and use that when you answer your question.
[01:22:16.780 --> 01:22:17.780]   Before you...
[01:22:17.780 --> 01:22:18.780]   Okay.
[01:22:18.780 --> 01:22:24.780]   ...you really use whatever extra content the users provide.
[01:22:24.780 --> 01:22:29.780]   So, just as a simple example of that, if you have the system at the very beginning of the chat,
[01:22:29.780 --> 01:22:34.780]   you would be training the model to focus more on the information at the beginning of the chat.
[01:22:34.780 --> 01:22:39.780]   Whereas if you were fine-tuning the model, you would be good at using the context,
[01:22:39.780 --> 01:22:45.780]   from the use that the user provides, then it's focusing more of an extension on the end of the chat.
[01:22:45.780 --> 01:22:50.780]   But it's not adding the information in, it's just how do I use...
[01:22:50.780 --> 01:22:51.780]   Yep.
[01:22:51.780 --> 01:22:52.780]   I got it.
[01:22:52.780 --> 01:22:53.780]   Okay.
[01:22:53.780 --> 01:22:54.780]   I'll repeat that.
[01:22:54.780 --> 01:23:06.780]   The clarification was thinking about using fine-tuning more to govern the behavior of the agent rather than to encode facts,
[01:23:06.780 --> 01:23:12.780]   which is a very good clarification because I was going to say using fine-tuning to encode facts,
[01:23:12.780 --> 01:23:15.780]   I think a lot of literature has pointed to that being a bad idea for a lot of reasons.
[01:23:15.780 --> 01:23:20.780]   It's costly, you have to continually fine-tune as facts change, so let's dispatch that.
[01:23:20.780 --> 01:23:22.780]   I think that's kind of not a great idea.
[01:23:22.780 --> 01:23:27.780]   But you make a very interesting point about fine-tuning to govern behavior.
[01:23:27.780 --> 01:23:33.780]   Now, there's a paper called Raft that came out kind of recently, and actually, as far as my understanding,
[01:23:33.780 --> 01:23:38.780]   as I haven't played with it in life, it's fine-tuning to kind of do what our notebooks show today,
[01:23:38.780 --> 01:23:44.780]   of this kind of like, look at the documents that are retrieved, reason if they're relevant,
[01:23:44.780 --> 01:23:47.780]   and then don't use them if they're not automatically filtered.
[01:23:47.780 --> 01:23:52.780]   They're doing exactly what we're doing in this land graph thing, but it's kind of achieving that same outcome through process fine-tuning.
[01:23:52.780 --> 01:23:54.780]   So that's a very good insight, you're right.
[01:23:54.780 --> 01:24:00.780]   It seems promising to have these kind of fine-tuned RAG agents, so to speak,
[01:24:00.780 --> 01:24:06.780]   or it wouldn't be an agent, it would be an LLM fine-tuned for RAG that incorporates this kind of logical reasoning,
[01:24:06.780 --> 01:24:12.780]   or what you're saying, like maybe some kind of reasoning about if you have a multi-turn conversation,
[01:24:12.780 --> 01:24:19.780]   like avoid recency bias, whatever it is, that seems like a very good trend and interesting.
[01:24:19.780 --> 01:24:27.780]   The challenge is, a little bit, if you're fine-tuning yourself, fine-tuning is hard and somewhat advanced and all that.
[01:24:27.780 --> 01:24:32.780]   Alternatively, if it's like a very niche use case, like the RAF system would basically,
[01:24:32.780 --> 01:24:38.780]   if fine-tunes to kind of do this, as models get changed or update all the time,
[01:24:38.780 --> 01:24:41.780]   you kind of need to keep your fine-tuned model up to date, if you see what I'm saying.
[01:24:41.780 --> 01:24:46.780]   So I think I'm still a little bit queasy about using fine-tuning even in that context,
[01:24:46.780 --> 01:24:50.780]   because of the challenge of keeping it kind of up-to-date with the state-of-the-art.
[01:24:50.780 --> 01:24:54.780]   But it's interesting, I think the RAF paper is a good reference in this direction.
[01:24:54.780 --> 01:24:59.780]   And it does exactly what we do in this workshop, but I believe it fine-tunes this into the model,
[01:24:59.780 --> 01:25:02.780]   or attempts to, which is a very intuitive thing to think about.
[01:25:02.780 --> 01:25:08.780]   Basically, let the model reflect automatically on retrieved documents and like automatically filter them for you.
[01:25:08.780 --> 01:25:11.780]   It seems like it should be able to do that.
[01:25:11.780 --> 01:25:12.780]   It seems like a good idea.
[01:25:12.780 --> 01:25:17.780]   So, but my hesitation would still be like, what if I want to switch my models?
[01:25:17.780 --> 01:25:18.780]   I need to like re-fine-tune.
[01:25:18.780 --> 01:25:19.780]   I want to use Lama 3.
[01:25:19.780 --> 01:25:20.780]   I have to fine-tune Lama 3 on this task.
[01:25:20.780 --> 01:25:21.780]   I can't use proprietary.
[01:25:21.780 --> 01:25:25.780]   Maybe I can fine-tune, you know, GBD-40 might have fine-tuning now.
[01:25:25.780 --> 01:25:26.780]   I'm not even sure.
[01:25:26.780 --> 01:25:27.780]   I could fine-tune, you know.
[01:25:27.780 --> 01:25:29.780]   So again, if I fine-tune myself, that's hard.
[01:25:29.780 --> 01:25:37.780]   It still feels a little bit like I'd rather just sit up a simple, like orchestrated agent that does it rather than rely on fine-tuning.
[01:25:37.780 --> 01:25:38.780]   My sense.
[01:25:38.780 --> 01:25:39.780]   Yeah.
[01:25:39.780 --> 01:25:47.780]   I guess I'm not really imagining the person doing .
[01:25:47.780 --> 01:25:48.780]   Yeah.
[01:25:48.780 --> 01:25:49.780]   The provider.
[01:25:49.780 --> 01:25:54.780]   In an analogy, you know, like Tesla's data engine .
[01:25:54.780 --> 01:25:55.780]   Right.
[01:25:55.780 --> 01:25:56.780]   Right.
[01:25:56.780 --> 01:25:57.780]   Right.
[01:25:57.780 --> 01:25:58.780]   Right.
[01:25:58.780 --> 01:25:59.780]   Yep.
[01:25:59.780 --> 01:26:02.780]   But it's going to start to become that data engine .
[01:26:02.780 --> 01:26:03.780]   Right.
[01:26:03.780 --> 01:26:04.780]   Right.
[01:26:04.780 --> 01:26:05.780]   Right.
[01:26:05.780 --> 01:26:06.780]   Right.
[01:26:06.780 --> 01:26:07.780]   Right.
[01:26:07.780 --> 01:26:08.780]   Right.
[01:26:08.780 --> 01:26:21.780]   There's got to be some kind of like back and forth.
[01:26:21.780 --> 01:26:22.780]   Right.
[01:26:22.780 --> 01:26:23.780]   Right.
[01:26:23.780 --> 01:26:28.780]   Right.
[01:26:28.780 --> 01:26:30.780]   Right.
[01:26:30.780 --> 01:26:31.780]   Right.
[01:26:31.780 --> 01:26:39.780]   Yeah.
[01:26:39.780 --> 01:26:39.780]   Okay.
[01:26:39.780 --> 01:26:40.780]   So that's a very good point.
[01:26:40.780 --> 01:26:43.780]   I think this is also a very big debate.
[01:26:43.780 --> 01:26:47.780]   So OpenAI just did an acquisition this week on a retrieval company.
[01:26:47.780 --> 01:26:48.780]   I forget the name.
[01:26:48.780 --> 01:26:50.780]   Rockset, I believe.
[01:26:50.780 --> 01:26:53.780]   So I think they are moving more in the direction of retrieval.
[01:26:53.780 --> 01:27:00.780]   I could absolutely see them offering, you know, an API that potentially does retrieval for you and incorporates some of these ideas for you.
[01:27:00.780 --> 01:27:06.780]   So how much of this does get pushed behind APIs and they take care of whatever is necessary behind the scenes for you.
[01:27:06.780 --> 01:27:08.780]   That could absolutely be the case.
[01:27:08.780 --> 01:27:11.780]   I would not be surprised at all if they move in that direction.
[01:27:11.780 --> 01:27:14.780]   And I think there's always, you know, it's an interesting trade off.
[01:27:14.780 --> 01:27:20.780]   Like how much are you willing to kind of, you know, abstract behind an API versus not.
[01:27:20.780 --> 01:27:27.780]   I think there's always a lot of companies, developers that want to kind of control everything themselves and build it themselves, have full transparency and others that don't.
[01:27:27.780 --> 01:27:29.780]   And so, you know, it's an interesting question.
[01:27:29.780 --> 01:27:32.780]   But of course, for certain functionalities, multimodality.
[01:27:32.780 --> 01:27:34.780]   Very few people are going to stand that up themselves.
[01:27:34.780 --> 01:27:36.780]   You kind of let that live behind an API.
[01:27:36.780 --> 01:27:39.780]   So what do you allow to live behind an API or not?
[01:27:39.780 --> 01:27:45.780]   My only concern is, I think for some of these kind of things, they could be very domain specific.
[01:27:45.780 --> 01:27:49.780]   Like what you consider relevant or not could be very relevant to you and your application.
[01:27:49.780 --> 01:27:50.780]   You kind of want to be able to control that.
[01:27:50.780 --> 01:27:52.780]   That's the only thing I can imagine.
[01:27:52.780 --> 01:28:00.780]   It could be kind of hard to abstract that all behind an API, which I think is maybe why OpenAI hasn't done too much in retrieval yet.
[01:28:00.780 --> 01:28:01.780]   It's just a hard beast.
[01:28:01.780 --> 01:28:03.780]   I know they've been trying for a while.
[01:28:03.780 --> 01:28:04.780]   I don't know.
[01:28:04.780 --> 01:28:07.780]   It's a great debate, though.
[01:28:07.780 --> 01:28:08.780]   Yeah, we can discuss more.
[01:28:08.780 --> 01:28:11.780]   Yeah, it's a good topic for sure.
[01:28:11.780 --> 01:28:12.780]   Yep.
[01:28:12.780 --> 01:28:13.780]   You mentioned about long context windows.
[01:28:13.780 --> 01:28:14.780]   Oh, yeah.
[01:28:14.780 --> 01:28:15.780]   So there's a problem of like, loss in the middle.
[01:28:15.780 --> 01:28:28.780]   For example, the precision, like, for example, in the context, they say 95% is relevant information, 5% is somewhere in the middle.
[01:28:28.780 --> 01:28:31.780]   And somehow, this kind of message is at that point.
[01:28:31.780 --> 01:28:33.780]   Like, do you have any questions over that?
[01:28:33.780 --> 01:28:34.780]   Yeah.
[01:28:34.780 --> 01:28:35.780]   Yeah.
[01:28:35.780 --> 01:28:37.780]   So the question is on loss in the middle of long context.
[01:28:37.780 --> 01:28:40.780]   I actually did a whole study on this with Greg Cameron.
[01:28:40.780 --> 01:28:44.780]   Yeah, it's a really interesting topic.
[01:28:44.780 --> 01:28:55.780]   So the insight was basically that long context LLMs tend to have lower recall or like, you know, factual recall for things in the middle of the context.
[01:28:55.780 --> 01:28:56.780]   Okay.
[01:28:56.780 --> 01:28:57.780]   So that was one observation.
[01:28:57.780 --> 01:29:00.780]   At least that's what their paper reported.
[01:29:00.780 --> 01:29:05.780]   So I actually looked at this with Greg, and we did something a little bit even harder.
[01:29:05.780 --> 01:29:08.780]   We actually tested for multiple fact retrieval.
[01:29:08.780 --> 01:29:13.780]   So we tested, can you retrieve one, three, or ten different facts from the context?
[01:29:13.780 --> 01:29:15.780]   And this was using GPT-4.
[01:29:15.780 --> 01:29:18.780]   GPT-4 turbo, single turn.
[01:29:18.780 --> 01:29:25.780]   And on the x-axis, you can see the fraction of the needles that it basically can get.
[01:29:25.780 --> 01:29:28.780]   And then on the y is the number of needles.
[01:29:28.780 --> 01:29:32.780]   So basically one needle, three needles, ten needles.
[01:29:32.780 --> 01:29:36.780]   Green versus red is basically just retrieving versus retrieving and reasoning.
[01:29:36.780 --> 01:29:40.780]   So it's like reasoning is a little bit harder than just retrieving.
[01:29:40.780 --> 01:29:44.780]   These needles were actually pizza ingredients.
[01:29:44.780 --> 01:29:50.780]   So basically the background was, you know, this was 120,000 tokens of Paul Graham essays.
[01:29:50.780 --> 01:29:56.780]   And three secret pizza ingredients, or however many, one, three, or ten, but I injected in that context.
[01:29:56.780 --> 01:29:59.780]   And I basically asked the LLM, what's the ingredients needed to build the secret pizza?
[01:29:59.780 --> 01:30:02.780]   So I'd have to find them in there.
[01:30:02.780 --> 01:30:08.780]   And basically as you ramp up the number of needles, go from one to ten, it gets worse.
[01:30:08.780 --> 01:30:14.780]   So with ten, it's actually retrieval itself is only like 60%.
[01:30:14.780 --> 01:30:16.780]   So then I looked at, okay, well where is it failing?
[01:30:16.780 --> 01:30:18.780]   And that's what I look at here in this heat map.
[01:30:18.780 --> 01:30:21.780]   So basically this is telling you like how long the context is.
[01:30:21.780 --> 01:30:25.780]   So a thousand tokens all up to 120,000.
[01:30:25.780 --> 01:30:27.780]   And then here's like the needle placement.
[01:30:27.780 --> 01:30:28.780]   So one to ten.
[01:30:28.780 --> 01:30:31.780]   So this red means you couldn't retrieve it.
[01:30:31.780 --> 01:30:35.780]   And what I found is actually it doesn't get them towards the start of the document.
[01:30:35.780 --> 01:30:38.780]   So the retrieval gets worse if the needle's at the front.
[01:30:38.780 --> 01:30:39.780]   So it's like this.
[01:30:39.780 --> 01:30:40.780]   I read a book.
[01:30:40.780 --> 01:30:41.780]   I asked you a question about the first chapter.
[01:30:41.780 --> 01:30:44.780]   You forgot because I read that a month ago or something.
[01:30:44.780 --> 01:30:45.780]   Same idea.
[01:30:45.780 --> 01:30:49.780]   And actually I put this on Twitter and then someone said, oh yeah, it's probably recency bias.
[01:30:49.780 --> 01:30:53.780]   And that's a good point that basically the most informative tokens in next token prediction
[01:30:53.780 --> 01:30:55.780]   are often the more recent ones.
[01:30:55.780 --> 01:30:59.780]   So, you know, basically these LLMs learn a bias to tend to recent tokens.
[01:30:59.780 --> 01:31:02.780]   And that's not good for a rag.
[01:31:02.780 --> 01:31:07.780]   So that is all to say, I'm a little wary about long context retrieval.
[01:31:07.780 --> 01:31:13.780]   I wouldn't quite trust basically high quality rag across a million tokens of context.
[01:31:13.780 --> 01:31:16.780]   You can see, look, if it's 1,000 tokens, no problem.
[01:31:16.780 --> 01:31:21.780]   If it's 120,000 tokens of context, you know, it depends a lot on where those facts are.
[01:31:21.780 --> 01:31:24.780]   If they're towards the start, you actually can have much lower recall.
[01:31:24.780 --> 01:31:25.780]   And so that's a real risk.
[01:31:25.780 --> 01:31:32.780]   Which is kind of why it goes back to this whole thing of, like, I don't really buy just stuffing
[01:31:32.780 --> 01:31:34.780]   everything into context, that far right side.
[01:31:34.780 --> 01:31:38.780]   I think there's too many issues with bad recall, recency bias, like you said.
[01:31:38.780 --> 01:31:45.780]   And so I think until we have very, and by the way, I also don't really trust.
[01:31:45.780 --> 01:31:48.780]   You know when they show those needle in the haystack charts, it's like perfect.
[01:31:48.780 --> 01:31:49.780]   I don't trust any of that.
[01:31:49.780 --> 01:31:50.780]   I did my own study.
[01:31:50.780 --> 01:31:51.780]   I found there's like a lot of errors.
[01:31:51.780 --> 01:31:56.780]   And I think it depends a lot on a couple different things.
[01:31:56.780 --> 01:31:57.780]   One, how many needles?
[01:31:57.780 --> 01:31:59.780]   So in this case, you see with one, it's okay.
[01:31:59.780 --> 01:32:01.780]   With 10, it's really bad, right?
[01:32:01.780 --> 01:32:02.780]   So how many needles?
[01:32:02.780 --> 01:32:06.780]   And then I saw an interesting study saying that, like, the difference in the needles relative
[01:32:06.780 --> 01:32:09.780]   to your context makes it easier.
[01:32:09.780 --> 01:32:13.780]   So like in these studies, it's like, Pete's ingredients in Paul Gramsci is really different.
[01:32:13.780 --> 01:32:17.780]   But if it's just like related slightly, it's actually harder still.
[01:32:17.780 --> 01:32:21.780]   So that is to say, I don't really trust the needle in haystack studies.
[01:32:21.780 --> 01:32:28.780]   I don't particularly trust stuffing, you know, passing a million tokens of context and counting
[01:32:28.780 --> 01:32:30.780]   that and counting on that to just work effectively.
[01:32:30.780 --> 01:32:32.780]   I'd be very wary about that.
[01:32:32.780 --> 01:32:33.780]   That's kind of my thing.
[01:32:33.780 --> 01:32:37.780]   But you know, in these studies, look, a thousand tokens of context already, you know, that's,
[01:32:37.780 --> 01:32:41.780]   that is, if you're stuffing a thousand tokens, eh, I mean, that's actually still pretty small.
[01:32:41.780 --> 01:32:47.780]   So yeah, I'd just be wary about retrieval from very large contexts.
[01:32:47.780 --> 01:32:48.780]   Yeah.
[01:32:48.780 --> 01:33:00.780]   Okay, that's a great question with agents, the number of tools.
[01:33:00.780 --> 01:33:03.780]   This is a really big issue I hear mentioned a lot.
[01:33:03.780 --> 01:33:10.780]   So if you recall, if you go back to the agent stuff.
[01:33:10.780 --> 01:33:14.780]   So you're basically binding some set of tools to your LLM, right?
[01:33:14.780 --> 01:33:18.780]   And that's what we show here, right?
[01:33:18.780 --> 01:33:22.780]   I've seen a lot of issues with a large number of tools.
[01:33:22.780 --> 01:33:26.780]   So I don't know exactly know what the exact cutoff is.
[01:33:26.780 --> 01:33:36.780]   But this is one of the big problems with open-ended tool calling agents is if I am basically selecting from 20 different tools.
[01:33:36.780 --> 01:33:40.780]   I actually, I, maybe the Berkeley leader board has data on this.
[01:33:40.780 --> 01:33:49.780]   So if someone knows, feel free to mention it, but reliability of tool calling, even with a small number of tools, like on the order of five is already challenging.
[01:33:49.780 --> 01:33:57.780]   If you're talking about hundreds of tools or dozens of tools, I don't think there's really, yeah, I think it's quite challenging.
[01:33:57.780 --> 01:34:11.780]   Which is why I've seen more success in a not using these open-ended style tool calling agents, laying it out more explicitly as a line graph where the tool calls live inside nodes.
[01:34:11.780 --> 01:34:15.780]   And you're not relying on your agent to pick from like 20 different tools.
[01:34:15.780 --> 01:34:21.780]   So you can lay out more of like a control flow where you route to different tool nodes based upon the logic.
[01:34:21.780 --> 01:34:23.780]   So, so that's kind of one thing I've seen.
[01:34:23.780 --> 01:34:29.780]   Another thing I've seen is maybe multi-agent type things where you have different agents with subtasks with each having like maybe a small number of tools.
[01:34:29.780 --> 01:34:36.780]   But basically what I've seen is it seems to be that, okay, maybe it's two things.
[01:34:36.780 --> 01:34:39.780]   Selection from large number of tools is definitely challenging.
[01:34:39.780 --> 01:34:49.780]   One of the most interesting things I saw is something like you can use something like RAG where basically you can take a description of your tools, create a natural language description, embed it,
[01:34:49.780 --> 01:34:56.780]   and then use basic like semantic similarity search, your query versus the embedded summaries to select using semantics.
[01:34:56.780 --> 01:34:58.780]   That's actually not a bad idea.
[01:34:58.780 --> 01:35:02.780]   I would actually use that more than I would trust an LLM to just like do the tool selection from a list of 100.
[01:35:02.780 --> 01:35:03.780]   That's not going to work.
[01:35:03.780 --> 01:35:07.780]   So actually I think that like RAG for tool selection is a cool idea.
[01:35:07.780 --> 01:35:10.780]   I was going to do a little like test that out and do a little tutorial.
[01:35:10.780 --> 01:35:13.780]   So actually maybe I'll just make a note of that.
[01:35:13.780 --> 01:35:14.780]   That's it.
[01:35:14.780 --> 01:35:16.780]   That's a great, a great question.
[01:35:16.780 --> 01:35:22.780]   To do RAG for many tools.
[01:35:22.780 --> 01:35:23.780]   Right.
[01:35:23.780 --> 01:35:23.780]   Yeah.
[01:35:23.780 --> 01:35:37.780]   Well I think, I think using semantic similarity for tool selections is a good idea.
[01:35:37.780 --> 01:35:38.780]   Definitely.
[01:35:38.780 --> 01:35:39.780]   What about data querying?
[01:35:39.780 --> 01:35:53.780]   What do you mean by data querying though?
[01:35:53.780 --> 01:35:54.780]   Right.
[01:35:54.780 --> 01:36:08.780]   Let me make sure I understand.
[01:36:08.780 --> 01:36:12.780]   I think the way I would think about it is, so you know how when you're in the notebook, like
[01:36:12.780 --> 01:36:19.780]   in the code, for all of our tools, right, you have this little tool description.
[01:36:19.780 --> 01:36:20.780]   Right.
[01:36:20.780 --> 01:36:26.780]   Like retrieve documents, grade them, run web search.
[01:36:26.780 --> 01:36:31.780]   I would actually write verbose descriptions for all my tools and then index those descriptions.
[01:36:31.780 --> 01:36:32.780]   Right.
[01:36:32.780 --> 01:36:33.780]   Or embed them.
[01:36:33.780 --> 01:36:36.780]   And then I would do, and I would probably create like very, very verbose high quality summaries
[01:36:36.780 --> 01:36:41.780]   of what the tool actually does and then do semantic similarity search against those summaries.
[01:36:41.780 --> 01:36:47.780]   I think that could actually, I haven't done that yet, but I think that could work really well.
[01:36:47.780 --> 01:36:52.780]   because it's a very tall task to ask an LLM to differentiate between like 20 different tools.
[01:36:52.780 --> 01:36:56.780]   Whereas, you could do something like semantic similarity, that would actually probably be very effective.
[01:36:56.780 --> 01:37:00.780]   Yep.
[01:37:13.780 --> 01:37:14.780]   Right.
[01:37:14.780 --> 01:37:16.780]   Like have you done any, like, experiments with that?
[01:37:16.780 --> 01:37:21.780]   Like in terms of, like, knowing if .
[01:37:21.780 --> 01:37:22.780]   Yeah.
[01:37:22.780 --> 01:37:23.780]   Yeah.
[01:37:23.780 --> 01:37:39.780]   Yeah, yeah, so the question was about multi-agent context when you want to orchestrate a large number of tools,
[01:37:39.780 --> 01:37:44.780]   I think sub-agents with specializations that manage some small set of tools each.
[01:37:44.780 --> 01:37:46.780]   And how do you kind of move between them?
[01:37:46.780 --> 01:37:51.780]   So, if you look at our LangGraph repo, we do have a subdirectory.
[01:37:51.780 --> 01:37:55.780]   It's under, it's under LangGraph examples multi-agent.
[01:37:55.780 --> 01:37:59.780]   We have a few different notebooks that have multi-agent style kind of layouts,
[01:37:59.780 --> 01:38:00.780]   which I would encourage you to look at.
[01:38:00.780 --> 01:38:02.780]   I haven't personally done too much work on it.
[01:38:02.780 --> 01:38:06.780]   It seems promising, but I haven't played with it.
[01:38:06.780 --> 01:38:12.780]   Multi-agent in general, and these all reference papers.
[01:38:12.780 --> 01:38:13.780]   You can also look at the papers.
[01:38:13.780 --> 01:38:17.780]   But multi-agent in general for production setting feels quite aggressive.
[01:38:17.780 --> 01:38:21.780]   Although, that said, as far as I understand, I remember looking at the code a while ago,
[01:38:21.780 --> 01:38:25.780]   Devon and some of the software agents do use, like, multi-agent style setups.
[01:38:25.780 --> 01:38:32.780]   So, maybe have a look at the Devon repo, or there's OpenDevon.
[01:38:32.780 --> 01:38:34.780]   Have a look at these notebooks.
[01:38:34.780 --> 01:38:37.780]   Those could all be useful if you want to learn more about multi-agent.
[01:38:37.780 --> 01:38:38.780]   Yep.
[01:38:38.780 --> 01:38:45.780]   So, I'm wondering, like, and I stepped up for this.
[01:38:45.780 --> 01:38:46.780]   Yeah, sure.
[01:38:46.780 --> 01:38:52.780]   But, I'm wondering, like, we've been talking a lot about kind of the variability
[01:38:52.780 --> 01:38:56.780]   on the picture of how many LL and disciples to recall.
[01:38:56.780 --> 01:38:57.780]   Right.
[01:38:57.780 --> 01:39:04.780]   So, given that, I'm wondering, how do you think about when it makes sense to wrap the lag inside
[01:39:04.780 --> 01:39:09.780]   of an agent versus just make it a chain on its own system?
[01:39:09.780 --> 01:39:12.780]   That's a classic question, yeah.
[01:39:12.780 --> 01:39:16.780]   So, the question was, when do I use a chain versus an agent?
[01:39:16.780 --> 01:39:17.780]   So, that's very good.
[01:39:17.780 --> 01:39:20.780]   So, we kind of touched on it a little bit, kind of here.
[01:39:20.780 --> 01:39:28.780]   So, I think that the intuition behind where and why an agent can make sense is simply that
[01:39:28.780 --> 01:39:33.780]   sometimes you want your application control flow to be variable.
[01:39:33.780 --> 01:39:38.780]   And if you want some flexibility within your application, an agent is a nice idea.
[01:39:38.780 --> 01:39:42.780]   And so, all this self-corrective type stuff we're talking about, the corrective rag thing,
[01:39:42.780 --> 01:39:48.780]   those are all kind of agentic flows where the control flow depends upon the grading of the
[01:39:48.780 --> 01:39:49.780]   documents.
[01:39:49.780 --> 01:39:57.780]   And so, you know, historically, people have largely been building chains.
[01:39:57.780 --> 01:40:00.780]   And chains are very reliable and they're easy to ship and all that.
[01:40:00.780 --> 01:40:04.780]   I think with things like LangGraph, and of course, I work at LangChain, so I'll speak
[01:40:04.780 --> 01:40:07.780]   my book about LangGraph, but I've really used it quite a bit.
[01:40:07.780 --> 01:40:09.780]   And I found it to be very reliable.
[01:40:09.780 --> 01:40:12.780]   And we are seeing a lot of people starting to deploy with it because you can actually ship
[01:40:12.780 --> 01:40:14.780]   and deploy a reliable agent with LangGraph.
[01:40:14.780 --> 01:40:19.780]   And so, I think a blocker to the ability to kind of ship agents has been reliability.
[01:40:19.780 --> 01:40:22.780]   And I think I would actually encourage you to play with the notebooks and look at LangGraph,
[01:40:22.780 --> 01:40:27.780]   because it does allow you to have that kind of reliability that would be necessary to ship
[01:40:27.780 --> 01:40:28.780]   something in production.
[01:40:28.780 --> 01:40:30.780]   And we do have customers that have LangGraph in production.
[01:40:30.780 --> 01:40:35.780]   Whereas a React agent in production is not recommended.
[01:40:35.780 --> 01:40:36.780]   Yeah.
[01:40:36.780 --> 01:40:37.780]   Yeah.
[01:40:37.780 --> 01:40:38.780]   Yep.
[01:40:38.780 --> 01:40:39.780]   Sure.
[01:40:39.780 --> 01:40:40.780]   A hundred percent.
[01:40:40.780 --> 01:40:41.780]   So, okay.
[01:40:41.780 --> 01:40:41.780]   So, I think, yeah.
[01:40:41.780 --> 01:40:47.780]   So, the different, why would you ever want kind of like an agent, be it LangGraph, React or otherwise,
[01:40:47.780 --> 01:40:48.780]   versus not?
[01:40:48.780 --> 01:40:53.780]   And I get, again, I think it goes back to, do you want your application to have any kind of adaptability?
[01:40:53.780 --> 01:40:54.780]   So, okay.
[01:40:54.780 --> 01:40:55.780]   So, okay.
[01:40:55.780 --> 01:40:56.780]   Here's one we can talk about.
[01:40:56.780 --> 01:40:57.780]   Routing.
[01:40:57.780 --> 01:40:58.780]   I have three different vector stores.
[01:40:58.780 --> 01:40:59.780]   I want to be able to route between them.
[01:40:59.780 --> 01:41:16.780]   That is kind of a quote-unquote agentic use case because the control flow depends on the question.
[01:41:16.780 --> 01:41:17.780]   So, that's one.
[01:41:17.780 --> 01:41:18.780]   You might want routing.
[01:41:18.780 --> 01:41:19.780]   You may want self-correction.
[01:41:19.780 --> 01:41:22.780]   So, that's kind of what we talked about here a whole bunch with the corrective rack stuff.
[01:41:22.780 --> 01:41:23.780]   So, you want routing.
[01:41:23.780 --> 01:41:24.780]   You want self-correction.
[01:41:24.780 --> 01:41:29.780]   I mean, those are two obvious ones in the context of rack itself.
[01:41:29.780 --> 01:41:36.780]   I mean, that's one thing I've often found the problem with the rack systems is the routing thing is a real issue.
[01:41:36.780 --> 01:41:40.780]   Like, you want your system to be flexible enough to deal with questions that are out of domain for your vector store.
[01:41:40.780 --> 01:41:46.780]   And you need some kind of dynamism in your application to handle that.
[01:41:46.780 --> 01:41:49.780]   So, looking at the question saying, okay, just answer this directly.
[01:41:49.780 --> 01:41:51.780]   Don't use the vector store.
[01:41:51.780 --> 01:41:52.780]   Yeah.
[01:41:52.780 --> 01:41:53.780]   So, those are like the most popular ones.
[01:41:53.780 --> 01:41:54.780]   Self-correction or routing.
[01:41:54.780 --> 01:41:55.780]   Yeah.
[01:41:55.780 --> 01:41:56.780]   Yeah.
[01:41:56.780 --> 01:41:58.780]   I'm wondering if there's anything to be said about building evaluation data sets.
[01:41:58.780 --> 01:41:59.780]   Like, question-answer pairs are so domain specific.
[01:41:59.780 --> 01:42:00.780]   Yeah.
[01:42:00.780 --> 01:42:04.780]   I'm wondering if there are like general best practices, mental models, like things to think
[01:42:04.780 --> 01:42:05.780]   about when sitting down to build an evaluation data set.
[01:42:05.780 --> 01:42:06.780]   Yeah, yeah.
[01:42:06.780 --> 01:42:07.780]   Okay.
[01:42:07.780 --> 01:42:08.780]   So, the question was about kind of building eval data sets.
[01:42:08.780 --> 01:42:09.780]   Okay.
[01:42:09.780 --> 01:42:10.780]   That's a great question.
[01:42:10.780 --> 01:42:11.780]   It's often a very, very challenging part of app development.
[01:42:11.780 --> 01:42:12.780]   So, if you have a RAG application that's domain specific, then oftentimes you have some set of
[01:42:12.780 --> 01:42:12.780]   canonical question-answer pairs you care about.
[01:42:12.780 --> 01:42:12.780]   You know, it's hard to find like, you know, very, very general rules for that.
[01:42:12.780 --> 01:42:12.780]   I think it depends on your application.
[01:42:12.780 --> 01:42:12.780]   I think there's kind of this hurdle.
[01:42:12.780 --> 01:42:13.780]   Any evaluation is better than no evaluation.
[01:42:13.780 --> 01:42:14.780]   So, small, well.
[01:42:14.780 --> 01:42:15.780]   Okay.
[01:42:15.780 --> 01:42:16.780]   That's a great question.
[01:42:16.780 --> 01:42:17.780]   So, the question was about kind of building eval data sets.
[01:42:17.780 --> 01:42:18.780]   Okay.
[01:42:18.780 --> 01:42:19.780]   That's a great question.
[01:42:19.780 --> 01:42:20.780]   It's often a very, very challenging part of app development.
[01:42:20.780 --> 01:42:24.780]   So, if you have a RAG application that's domain specific, then oftentimes you have some
[01:42:24.780 --> 01:42:27.780]   set of canonical question-answer pairs you care about.
[01:42:27.780 --> 01:42:32.780]   You know, it's hard to find like, you know, very, very general rules for that.
[01:42:32.780 --> 01:42:34.780]   I think it depends on your application.
[01:42:34.780 --> 01:42:38.780]   I think there's kind of this hurdle.
[01:42:38.780 --> 01:42:41.780]   Any evaluation is better than no evaluation.
[01:42:41.780 --> 01:42:47.780]   So, small-scale eval sets that you can use and just work are already better than not
[01:42:47.780 --> 01:42:48.780]   doing any evaluation.
[01:42:48.780 --> 01:42:52.780]   So, I mean, for this particular case, I just looked at the document.
[01:42:52.780 --> 01:42:56.780]   Now, maybe I'll back up and answer.
[01:42:56.780 --> 01:43:02.780]   So, one thing I've seen, and I've done this a little bit, is you can use LLM-assisted QA
[01:43:02.780 --> 01:43:03.780]   generation.
[01:43:03.780 --> 01:43:05.780]   So, here's one thing you can do.
[01:43:05.780 --> 01:43:08.780]   And I've done this a little bit with Langchain docs.
[01:43:08.780 --> 01:43:13.780]   I can build a prompt that says, given this document produced three high quality question-answer
[01:43:13.780 --> 01:43:15.780]   pairs from it, right?
[01:43:15.780 --> 01:43:18.780]   And I can just basically load my documents and pass them into that LLM.
[01:43:18.780 --> 01:43:23.780]   I use a high capacity model like Sonnet or 4.0 and have it generate QA pairs for me and then
[01:43:23.780 --> 01:43:24.780]   audit them.
[01:43:24.780 --> 01:43:25.780]   That's a nice trick.
[01:43:25.780 --> 01:43:26.780]   I've used that.
[01:43:26.780 --> 01:43:27.780]   It actually kind of works.
[01:43:27.780 --> 01:43:28.780]   You have to be careful with it.
[01:43:28.780 --> 01:43:32.780]   You only would pass it, like, usually one document at a time to keep it really, like, you know,
[01:43:32.780 --> 01:43:33.780]   restricted.
[01:43:33.780 --> 01:43:34.780]   And you audit them.
[01:43:34.780 --> 01:43:36.780]   But actually, that's a nice way to bootstrap your eval sets.
[01:43:36.780 --> 01:43:37.780]   That's, like, idea one.
[01:43:37.780 --> 01:43:40.780]   And that gets into the whole idea of synthetic data sets.
[01:43:40.780 --> 01:43:45.780]   But if you're building, you know, domain-specific synthetic QA pair data sets, that's a nice trick.
[01:43:45.780 --> 01:43:46.780]   So, basically use an LLM to help bootstrap.
[01:43:46.780 --> 01:43:49.780]   I think that's one idea that can help a lot.
[01:43:49.780 --> 01:43:50.780]   Yeah.
[01:43:50.780 --> 01:43:58.780]   And otherwise, I think that basically trying to stand up a small evaluation set, for example,
[01:43:58.780 --> 01:44:00.780]   for RAG is, even in this case, five questions.
[01:44:00.780 --> 01:44:01.780]   But you can already see.
[01:44:01.780 --> 01:44:02.780]   I can get some nice insights.
[01:44:02.780 --> 01:44:04.780]   It's very simple to set these up.
[01:44:04.780 --> 01:44:07.780]   I have my little experiments all over here.
[01:44:07.780 --> 01:44:08.780]   And, again, it's only five questions.
[01:44:08.780 --> 01:44:14.780]   But it gives me some immediate insights about the reliability of React versus LangGraph agent.
[01:44:14.780 --> 01:44:16.780]   So, keep it small.
[01:44:16.780 --> 01:44:18.780]   Potentially use synthetic data.
[01:44:18.780 --> 01:44:19.780]   Start with something.
[01:44:19.780 --> 01:44:21.780]   And then, like, kind of build it out over time.
[01:44:21.780 --> 01:44:25.780]   Now, a whole other thing here is, we didn't talk about this too much, but if you have an
[01:44:25.780 --> 01:44:30.780]   app in production then, the way this whole thing kind of comes together is, you can actually
[01:44:30.780 --> 01:44:35.780]   have different types of evaluators that run on your app online.
[01:44:35.780 --> 01:44:37.780]   We call those online evaluators, okay?
[01:44:37.780 --> 01:44:38.780]   So, this is with our internal app.
[01:44:38.780 --> 01:44:42.780]   And this gets back to the question I think he mentioned of, you can have a bunch of evaluators
[01:44:42.780 --> 01:44:44.780]   for rag that don't require a reference.
[01:44:44.780 --> 01:44:49.780]   So, like, I don't show it here, but basically I can look at, like, document retrieval quality.
[01:44:49.780 --> 01:44:51.780]   I can look at my answer relevance or hallucinations.
[01:44:51.780 --> 01:44:53.780]   I can run that online.
[01:44:53.780 --> 01:44:57.780]   I can flag cases where things did not work well.
[01:44:57.780 --> 01:45:01.780]   And I can actually roll those back into my eval set.
[01:45:01.780 --> 01:45:17.780]   So, if I do that, then I actually have this self-perpetuating loop, like Karpathy talked about, like the data flag, where actually I'm running my app in production, I'm collecting cases of bad behavior that I'm tagging with, like, online evaluation, and I'm rolling those back into my offline eval set.
[01:45:17.780 --> 01:45:28.780]   So, what you would do there is look at the case that the app is doing poorly in production, audit them, correct them, so build, like, a canonical question-answer pair from that and put that back into your test set.
[01:45:28.780 --> 01:45:30.780]   And that's a good way to bootstrap and build it up.
[01:45:30.780 --> 01:45:40.780]   So, I'd start cold start, synthetic data, small-scale examples, online evaluation, or some system to check online where it's failing, loop those back in and build it up that way.
[01:45:40.780 --> 01:45:42.780]   That's like your data flywheel.
[01:45:42.780 --> 01:45:43.780]   Yeah.
[01:45:43.780 --> 01:45:46.780]   Actually, I even had a slide on this in one of my older talks.
[01:45:46.780 --> 01:45:53.780]   I used to work in self-driving for many years, and I actually was a big fan of Karpathy's stuff at Tesla, and actually I've had his thing here.
[01:45:53.780 --> 01:46:05.780]   This is like the data engine thing of, like, you know, you ship your model, you do some kind of online evaluation, where's it failing, capture those failures, curate them, put them back in your test set, run that as a loop.
[01:46:05.780 --> 01:46:11.780]   That's like -- he called it operation vacation because you can go on vacation and the model keeps getting better.
[01:46:11.780 --> 01:46:18.780]   And this was more in the context of, like, training models because basically all those failed examples, once they're labeled, they become part of your training set.
[01:46:18.780 --> 01:46:20.780]   But the same thing applies here with LLM apps.
[01:46:20.780 --> 01:46:21.780]   Cool.
[01:46:21.780 --> 01:46:22.780]   Yeah?
[01:46:22.780 --> 01:46:23.780]   Yeah.
[01:46:23.780 --> 01:46:34.780]   So, I just wanted to ask, you just mentioned fix to SQL, right?
[01:46:34.780 --> 01:46:35.780]   Oh, yeah.
[01:46:35.780 --> 01:46:43.780]   So, I just had a question where, like, for example, all that -- all that happens when you send
[01:46:43.780 --> 01:46:49.780]   the same schema and you generate a SQL query and then we run it on the database.
[01:46:49.780 --> 01:46:57.780]   And if the result is too large or something, so we cannot send it for another LLM to generate a user-friendly query.
[01:46:57.780 --> 01:47:00.780]   Like, user-friendly answer or something, right?
[01:47:00.780 --> 01:47:01.780]   How do we handle that?
[01:47:01.780 --> 01:47:02.780]   What do you send it?
[01:47:02.780 --> 01:47:08.780]   The query -- the result is so big that we cannot completely change the context.
[01:47:08.780 --> 01:47:09.780]   Yeah.
[01:47:09.780 --> 01:47:19.780]   Yeah, the question was on text to SQL.
[01:47:19.780 --> 01:47:25.780]   We actually have a pretty nice text to SQL agent example here.
[01:47:25.780 --> 01:47:27.780]   So, it's in Langraph examples.
[01:47:27.780 --> 01:47:30.780]   I think it's in -- is it SQL?
[01:47:30.780 --> 01:47:31.780]   Where is it?
[01:47:31.780 --> 01:47:36.780]   I'll find it here.
[01:47:36.780 --> 01:47:49.780]   Tutorials -- oh, yeah, it's in tutorials.
[01:47:49.780 --> 01:47:52.780]   So, SQL agent here.
[01:47:52.780 --> 01:47:54.780]   I think a lot's in the prompting.
[01:47:54.780 --> 01:48:01.780]   So, basically, in this particular case, I believe Ankush from our team set this up.
[01:48:01.780 --> 01:48:04.780]   You prompt your -- you can do a couple things.
[01:48:04.780 --> 01:48:09.780]   So, you can prompt your SQL agent to -- where is it?
[01:48:09.780 --> 01:48:17.780]   It's somewhere where he tells it to -- yeah, it's basically on all these instructions.
[01:48:17.780 --> 01:48:32.780]   So, you can -- you can -- you can -- you can instruct a SQL agent when it's writing its query to ensure not to extract an excessive amount of context.
[01:48:32.780 --> 01:48:34.780]   And I can't remember exactly where it does that.
[01:48:34.780 --> 01:48:39.780]   Okay, yeah, it's right here.
[01:48:39.780 --> 01:48:45.780]   Limit your -- always use a limit statement on your query to restrict it to, like, whatever it is, five results.
[01:48:45.780 --> 01:48:47.780]   And that's the hard-coded thing here.
[01:48:47.780 --> 01:48:57.780]   And then, also, in your SQL agent, you can incorporate a query check node to actually look at the query before you execute it to sanity check for things like this.
[01:48:57.780 --> 01:49:10.780]   So, basically, I would have a look at the LandGraph example tutorial SQL agent notebook to have -- I've actually -- I ran this and evaluated it, and I found it did work pretty well.
[01:49:10.780 --> 01:49:17.780]   So, that's one thing that I would look at.
[01:49:17.780 --> 01:49:18.780]   Okay.
[01:49:18.780 --> 01:49:32.780]   One way question about SQL, so, a lot of times, like, the next SQL, they move well on, like, the do-by kind of questions, but, how do you handle, like, filtering, like, where flaws, like, when you have a high-cardy column, like, you have a question, right?
[01:49:32.780 --> 01:49:42.780]   Like, if you say, I'm going to SQL query, and it means you know where, where flaws in SQL, where is actually translate, so, like, any column value, like, any value from the column.
[01:49:42.780 --> 01:49:43.780]   Yeah.
[01:49:43.780 --> 01:50:00.780]   So, the question was related to, like, how do you handle high-cardinality columns, and I guess that is related to restricting the output size.
[01:50:00.780 --> 01:50:18.780]   I mean, I'm actually not really a SQL expert, so I'm probably not the right person to ask about very gory details of text-to-SQL, but, in general, I would kind of consider, can you prompt the LLM effectively?
[01:50:18.780 --> 01:50:19.780]   Like, okay, two different things.
[01:50:19.780 --> 01:50:20.780]   Like, okay, two different things.
[01:50:20.780 --> 01:50:39.780]   One, the general ideas here were basically upfront prompting of your LLM to kind of follow some general kind of query formulation criteria, and then, two, an actual query check explicitly to review and confirm it doesn't have some of the issues like extracting accessible context.
[01:50:39.780 --> 01:50:45.780]   But, for anything more detailed than that, I'm probably not the person for those insights.
[01:50:45.780 --> 01:50:52.780]   But, there might be a specific text-to-SQL deep dive at some other point in this conference, and you should definitely seek that out.
[01:50:52.780 --> 01:50:55.780]   But, I haven't done that much with text-to-SQL.
[01:50:55.780 --> 01:50:56.780]   Yeah?
[01:50:56.780 --> 01:50:57.780]   Yeah?
[01:50:57.780 --> 01:51:06.780]   It seems like a lot of, you know, question-answer rag patterns, but one thing I'm kind of interested in is the board long-form document generation, so, you know, writing for a board or something like that.
[01:51:06.780 --> 01:51:07.780]   Yeah.
[01:51:07.780 --> 01:51:16.780]   Can you see the interesting patterns or, like, what you think about, you know, archiving solutions with a scripting line graph or other type of scripting ?
[01:51:16.780 --> 01:51:17.780]   Yeah.
[01:51:17.780 --> 01:51:20.780]   Okay, so the question is related to kind of document generation.
[01:51:20.780 --> 01:51:23.780]   That is a really good theme.
[01:51:23.780 --> 01:51:26.780]   So, we actually have kind of -- there was an interesting paper.
[01:51:26.780 --> 01:51:27.780]   I did this a while ago.
[01:51:27.780 --> 01:51:29.780]   I need to find it.
[01:51:29.780 --> 01:51:30.780]   Where is it?
[01:51:30.780 --> 01:51:32.780]   So, we have a notebook.
[01:51:32.780 --> 01:51:35.780]   If you look in line graph examples, Storm.
[01:51:35.780 --> 01:51:39.780]   So, this was actually for wiki article generation.
[01:51:39.780 --> 01:51:42.780]   Here's kind of the diagram for it.
[01:51:42.780 --> 01:51:44.780]   We actually have a video on this, too.
[01:51:44.780 --> 01:51:46.780]   I'm actually trying to refresh myself.
[01:51:46.780 --> 01:51:48.780]   I did this, like, three or four months ago.
[01:51:48.780 --> 01:51:57.780]   But it was basically a multi-agent style setup in line graph where, if I recall correctly, I'm just looking at the flow here myself.
[01:51:57.780 --> 01:52:06.780]   But basically, what it did was you give it a topic and it'll kind of do initially this kind of, like, generation of related topics.
[01:52:06.780 --> 01:52:13.780]   And actually uses the multi-agent thing of, like, editors and experts.
[01:52:13.780 --> 01:52:15.780]   The experts go and do, like, web research.
[01:52:15.780 --> 01:52:17.780]   Don't worry too much of the details.
[01:52:17.780 --> 01:52:23.780]   So, the point is it was an interesting paper in flow for wiki article generation using line graph in a multi-step process.
[01:52:23.780 --> 01:52:26.780]   And actually, the wikis are, like, pretty good.
[01:52:26.780 --> 01:52:28.780]   I think at the bottom of the notebook I have an example wiki.
[01:52:28.780 --> 01:52:30.780]   So, you can see all the code here.
[01:52:30.780 --> 01:52:32.780]   That's the graph.
[01:52:32.780 --> 01:52:33.780]   Yeah.
[01:52:33.780 --> 01:52:37.780]   And then here's the final wiki that you get out from this type thing.
[01:52:37.780 --> 01:52:39.780]   So, it's pretty good.
[01:52:39.780 --> 01:52:41.780]   Have a look at that notebook.
[01:52:41.780 --> 01:52:45.780]   I think, yeah, Jason Liu also had a post on this recently.
[01:52:45.780 --> 01:52:49.780]   this idea of, like, report generation is a theme that we're going to see more and more of.
[01:52:49.780 --> 01:52:52.780]   This was one idea that was pretty sophisticated, though.
[01:52:52.780 --> 01:52:54.780]   You can probably simplify it a lot.
[01:52:54.780 --> 01:52:59.780]   I've done a lot of work just on, like, a simple kind of distillation prompt.
[01:52:59.780 --> 01:53:05.780]   Like, perform rag and then have some generation prompt give a bunch of instructions for how I want the output to be formatted.
[01:53:05.780 --> 01:53:06.780]   That also is really effective.
[01:53:06.780 --> 01:53:08.780]   Yeah.
[01:53:08.780 --> 01:53:09.780]   Yeah.
[01:53:09.780 --> 01:53:12.780]   So, the user-fitting is that.
[01:53:12.780 --> 01:53:18.780]   What's the statistic that you implement classification that the .
[01:53:18.780 --> 01:53:21.780]   So, during the .
[01:53:21.780 --> 01:53:26.780]   You might, you know, you might get a little, like, whatever that information that they provide.
[01:53:26.780 --> 01:53:29.780]   Like, say, mostly or most .
[01:53:29.780 --> 01:53:33.780]   There is some information that is very specific to the .
[01:53:33.780 --> 01:53:37.780]   What is the way to implement the .
[01:53:37.780 --> 01:53:38.780]   Yeah.
[01:53:38.780 --> 01:53:44.780]   So, the idea, the question was related to user feedback.
[01:53:44.780 --> 01:53:47.780]   That's a really good one.
[01:53:47.780 --> 01:53:53.780]   So, we do have, I think I mentioned previously, if you look at line graph, where is it?
[01:53:53.780 --> 01:53:58.780]   We, I believe we have some user feedback examples.
[01:53:58.780 --> 01:54:04.780]   Thursday, we're definitely going to be announcing something that has a lot of support for user feedback.
[01:54:04.780 --> 01:54:06.780]   And I would encourage you to keep an eye out for that.
[01:54:06.780 --> 01:54:09.780]   So, Tarasyn is going to launch that here on Thursday.
[01:54:09.780 --> 01:54:21.780]   I will look for, I know we have some user feedback examples in line graph, but I will need to find them.
[01:54:21.780 --> 01:54:22.780]   Let me see.
[01:54:22.780 --> 01:54:23.780]   Let's see.
[01:54:23.780 --> 01:54:27.780]   We probably, let's check length.
[01:54:27.780 --> 01:54:29.780]   We probably tweeted about it at some point.
[01:54:29.780 --> 01:54:32.780]   I haven't actually done anything with user feedback, though.
[01:54:32.780 --> 01:54:33.780]   Let's see.
[01:54:33.780 --> 01:54:34.780]   Let's see.
[01:54:34.780 --> 01:54:48.780]   Yeah, maybe I might have to get back to you on that.
[01:54:48.780 --> 01:54:55.780]   I thought we had some nice examples with line graph.
[01:54:55.780 --> 01:55:09.780]   Yeah, I'll have to get back to you on that one.
[01:55:09.780 --> 01:55:10.780]   Let's see.
[01:55:10.780 --> 01:55:11.780]   Feedback.
[01:55:11.780 --> 01:55:22.780]   Customer support might be in here.
[01:55:22.780 --> 01:55:23.780]   Hmm.
[01:55:23.780 --> 01:55:24.780]   Let's try something else.
[01:55:24.780 --> 01:55:37.780]   Look at line graph docs.
[01:55:37.780 --> 01:56:05.780]   Hmm.
[01:56:05.780 --> 01:56:08.780]   I'd poke around the line graph docs.
[01:56:08.780 --> 01:56:11.780]   We have a bunch of tutorials in the docs here.
[01:56:11.780 --> 01:56:14.780]   Just Google line graph documentation.
[01:56:14.780 --> 01:56:18.780]   I'm just poking around here for user feedback.
[01:56:18.780 --> 01:56:23.780]   Ah, here we go.
[01:56:23.780 --> 01:56:24.780]   Look at this.
[01:56:24.780 --> 01:56:26.780]   So, line graph how to's human in the loop.
[01:56:26.780 --> 01:56:27.780]   I would have a look at that.
[01:56:27.780 --> 01:56:28.780]   I've not played with that myself.
[01:56:28.780 --> 01:56:28.780]   But, yeah.
[01:56:28.780 --> 01:56:28.780]   .
[01:56:28.780 --> 01:56:29.780]   Yeah.
[01:56:29.780 --> 01:56:30.780]   .
[01:56:30.780 --> 01:56:31.780]   Yeah, yeah, yeah, yeah.
[01:56:31.780 --> 01:56:31.780]   Okay.
[01:56:31.780 --> 01:56:31.780]   Right.
[01:56:31.780 --> 01:56:31.780]   So, for like.
[01:56:31.780 --> 01:56:32.780]   um.
[01:56:32.780 --> 01:56:32.780]   Yeah, yeah, yeah.
[01:56:32.780 --> 01:56:32.780]   Okay.
[01:56:32.780 --> 01:56:32.780]   Right.
[01:56:32.780 --> 01:56:32.780]   So, for like.
[01:56:32.780 --> 01:56:33.780]   Um.
[01:56:33.780 --> 01:56:34.780]   mid to longer term problem solving tasks.
[01:56:34.780 --> 01:56:34.780]   Yeah, yeah, yeah.
[01:56:34.780 --> 01:56:34.780]   Okay.
[01:56:34.780 --> 01:56:35.780]   Right.
[01:56:35.780 --> 01:56:36.780]   So, for like.
[01:56:36.780 --> 01:56:37.780]   um.
[01:56:37.780 --> 01:56:38.780]   Um.
[01:56:38.780 --> 01:56:39.780]   Um.
[01:56:39.780 --> 01:56:40.780]   mid to longer term problem solving tasks.
[01:56:40.780 --> 01:56:41.780]   How do you incorporate user feedback to ask for more information.
[01:56:41.780 --> 01:56:42.780]   So, um.
[01:56:42.780 --> 01:56:43.780]   Um.
[01:56:43.780 --> 01:56:44.780]   I would have a look at this documentation.
[01:56:44.780 --> 01:56:45.780]   Because I would imagine.
[01:56:45.780 --> 01:56:46.780]   Um.
[01:56:46.780 --> 01:56:47.780]   I would have a look at this documentation.
[01:56:47.780 --> 01:56:48.780]   Cause I would imagine.
[01:56:48.780 --> 01:56:49.780]   Um.
[01:56:49.780 --> 01:56:50.780]   Uh.
[01:56:50.780 --> 01:56:51.780]   Yeah, yeah, yeah.
[01:56:51.780 --> 01:56:52.780]   Okay.
[01:56:52.780 --> 01:56:53.780]   Right.
[01:56:53.780 --> 01:56:55.780]   So, for like.
[01:56:55.780 --> 01:56:56.780]   Um.
[01:56:56.780 --> 01:56:57.780]   Um.
[01:56:57.780 --> 01:56:58.780]   Yeah, yeah, yeah.
[01:56:58.780 --> 01:56:59.780]   Okay.
[01:56:59.780 --> 01:57:00.780]   Right.
[01:57:00.780 --> 01:57:01.780]   So, for like.
[01:57:01.780 --> 01:57:02.780]   Um.
[01:57:02.780 --> 01:57:03.780]   Mid to longer term problem solving tasks.
[01:57:03.780 --> 01:57:07.780]   How do you incorporate user feedback to ask for more information.
[01:57:07.780 --> 01:57:08.780]   So, um.
[01:57:08.780 --> 01:57:10.780]   I would have a look at this documentation.
[01:57:10.780 --> 01:57:12.780]   Cause I would imagine it will.
[01:57:12.780 --> 01:57:13.780]   Uh.
[01:57:13.780 --> 01:57:15.780]   Cover examples along those lines.
[01:57:15.780 --> 01:57:17.780]   I haven't personally done that.
[01:57:17.780 --> 01:57:18.780]   Um.
[01:57:18.780 --> 01:57:19.780]   I also.
[01:57:19.780 --> 01:57:21.780]   Believe that.
[01:57:21.780 --> 01:57:25.780]   I would have a look at the customer support bot.
[01:57:25.780 --> 01:57:26.780]   Um.
[01:57:26.780 --> 01:57:28.780]   Because that's an example of.
[01:57:28.780 --> 01:57:29.780]   Of a.
[01:57:29.780 --> 01:57:32.780]   Kind of multi-turn interaction between a user and a support agent.
[01:57:32.780 --> 01:57:33.780]   Um.
[01:57:33.780 --> 01:57:35.780]   So, I would look at the customer support bot.
[01:57:35.780 --> 01:57:37.780]   Which Will and my team did.
[01:57:37.780 --> 01:57:40.780]   As well as the documentation on human loop.
[01:57:40.780 --> 01:57:43.780]   So, those are two things I would check out there.
[01:57:43.780 --> 01:57:44.780]   Um.
[01:57:44.780 --> 01:57:45.780]   Nice.
[01:57:45.780 --> 01:57:46.780]   Yep.
[01:57:46.780 --> 01:57:53.780]   Is there any research into model architecture or training models for agentic reasoning?
[01:57:53.780 --> 01:57:54.780]   Optimized for agentic reasoning?
[01:57:54.780 --> 01:57:55.780]   Yeah.
[01:57:55.780 --> 01:57:56.780]   Um.
[01:57:56.780 --> 01:57:57.780]   For agentic reasoning?
[01:57:57.780 --> 01:57:58.780]   Um.
[01:57:58.780 --> 01:57:59.780]   Yeah.
[01:57:59.780 --> 01:58:03.780]   So, that's kind of an interesting question.
[01:58:03.780 --> 01:58:08.780]   So, the question is related to training models specifically for agentic reasoning.
[01:58:08.780 --> 01:58:09.780]   Um.
[01:58:09.780 --> 01:58:10.780]   If anyone.
[01:58:10.780 --> 01:58:11.780]   So, I mean there's a lot of.
[01:58:11.780 --> 01:58:15.780]   There's a lot of work on prompting approaches for different types of reasoning for sure.
[01:58:15.780 --> 01:58:16.780]   Um.
[01:58:16.780 --> 01:58:17.780]   Um.
[01:58:17.780 --> 01:58:18.780]   Um.
[01:58:18.780 --> 01:58:21.780]   I'm a little bit less familiar with like efforts to fine tune a model specifically for
[01:58:21.780 --> 01:58:23.780]   particular like agentic architecture or use case.
[01:58:23.780 --> 01:58:24.780]   But you could imagine it.
[01:58:24.780 --> 01:58:25.780]   Um.
[01:58:25.780 --> 01:58:32.780]   Most of the work that I've encountered though is just using generalist, like high capacity
[01:58:32.780 --> 01:58:36.780]   generalist models with tool calling and specific prompting techniques.
[01:58:36.780 --> 01:58:43.780]   So, like React like React is kind of a particular orchestration flow and prompting technique rather than, you know, and you can interchange the LLM according to this.
[01:58:43.780 --> 01:58:44.780]   So, um.
[01:58:44.780 --> 01:58:45.780]   Um.
[01:58:45.780 --> 01:58:46.780]   Uh.
[01:58:46.780 --> 01:58:47.780]   So, you know, you can't just fine tune a model specifically for particular like agentic architecture
[01:58:47.780 --> 01:58:48.780]   or use case.
[01:58:48.780 --> 01:58:49.780]   But you could imagine it.
[01:58:49.780 --> 01:58:50.780]   Um.
[01:58:50.780 --> 01:58:51.780]   Most of the work that I've encountered though is just using generalist, like high capacity
[01:58:51.780 --> 01:58:53.780]   generalist models with tool calling and specific prompting techniques.
[01:58:53.780 --> 01:59:02.780]   So, like React is kind of a particular orchestration flow and prompting technique rather
[01:59:02.780 --> 01:59:06.780]   than, you know, and you can interchange the LLM according to that.
[01:59:06.780 --> 01:59:15.780]   I think the main thing typically or historically for, for agents has been the ability to perform high quality and accurate tool calling.
[01:59:15.780 --> 01:59:17.780]   Because agents, that's one of the central components of agents.
[01:59:17.780 --> 01:59:21.780]   And so, um, that's kind of been the gating thing.
[01:59:21.780 --> 01:59:26.780]   And I think model providers been focused a lot on just high quality tool calling which helps kind of like all agent architectures.
[01:59:26.780 --> 01:59:30.780]   I haven't seen as much on like fine tuning for one particular architecture.
[01:59:30.780 --> 01:59:34.780]   I think it's like high capacity generalist models with tool calling and then prompting.
[01:59:34.780 --> 01:59:36.780]   So it's more like in context learning.
[01:59:36.780 --> 01:59:38.780]   That's kind of the trend I've seen at least.
[01:59:38.780 --> 01:59:39.780]   Yeah.
[01:59:39.780 --> 01:59:40.780]   Yeah.
[01:59:40.780 --> 01:59:46.780]   Can you talk a little bit more about .
[01:59:46.780 --> 01:59:51.780]   Because I just saw that in the .
[01:59:51.780 --> 01:59:56.780]   And my wife is thinking that it's more about .
[01:59:56.780 --> 01:59:59.780]   And then sort of saving the state.
[01:59:59.780 --> 02:00:00.780]   Yeah.
[02:00:00.780 --> 02:00:01.780]   Yeah.
[02:00:01.780 --> 02:00:02.780]   Yeah.
[02:00:02.780 --> 02:00:09.780]   So the checkpointing stuff, actually this Thursday, that's going to be a lot more relevant.
[02:00:09.780 --> 02:00:14.780]   Because we're launching some stuff to support deployments for line graph.
[02:00:14.780 --> 02:00:18.780]   In which case you, you can do a bunch of different things.
[02:00:18.780 --> 02:00:21.780]   But you can have a single state that persists across many different sessions.
[02:00:21.780 --> 02:00:22.780]   You can also have check points.
[02:00:22.780 --> 02:00:28.780]   You can return to state and revisit an agent from a particular point.
[02:00:28.780 --> 02:00:30.780]   Don't worry about that too much for now.
[02:00:30.780 --> 02:00:37.780]   I think there will be a lot more documentation and kind of context for that on Thursday.
[02:00:37.780 --> 02:00:38.780]   When the stuff to deployment comes in.
[02:00:38.780 --> 02:00:40.780]   But it's good to be somewhat aware of.
[02:00:40.780 --> 02:00:44.780]   And I would poke around the documentation for a little bit more on checkpointing.
[02:00:44.780 --> 02:00:49.780]   But it really becomes relevant on the stuff we're announcing on Thursday.
[02:00:49.780 --> 02:00:54.780]   So I would have a look then.
[02:00:54.780 --> 02:00:58.780]   Let's see if we update our docs.
[02:00:58.780 --> 02:00:59.780]   Yeah.
[02:00:59.780 --> 02:01:00.780]   Yeah.
[02:01:00.780 --> 02:01:02.780]   So there is some documentation on it now.
[02:01:02.780 --> 02:01:05.780]   But it will become a lot more interesting and relevant come Thursday.
[02:01:05.780 --> 02:01:08.780]   And we have a lot more support for deployment.
[02:01:08.780 --> 02:01:09.780]   Yep.
[02:01:09.780 --> 02:01:10.780]   Okay.
[02:01:10.780 --> 02:01:11.780]   Yeah.
[02:01:11.780 --> 02:01:12.780]   Yeah.
[02:01:12.780 --> 02:01:13.780]   Okay.
[02:01:13.780 --> 02:01:14.780]   Yeah.
[02:01:14.780 --> 02:01:31.780]   So that's a really good question.
[02:01:31.780 --> 02:01:41.780]   And so the way it works with the existing.
[02:01:41.780 --> 02:01:44.780]   So it depends on the architecture.
[02:01:44.780 --> 02:01:48.780]   So using the React architecture, let's see if I can find an example of it.
[02:01:48.780 --> 02:01:51.780]   So here's with React agent.
[02:01:51.780 --> 02:01:52.780]   Let's look at one of the traces.
[02:01:52.780 --> 02:01:54.780]   Let's see if I have an example.
[02:01:54.780 --> 02:01:58.780]   So basically the tool call itself will return like an error.
[02:01:58.780 --> 02:02:03.780]   And the LLM then is expected to self-correct from that error.
[02:02:03.780 --> 02:02:04.780]   It has to kind of self-correct.
[02:02:04.780 --> 02:02:12.780]   So that's kind of one approach that at least we do with the React agent.
[02:02:12.780 --> 02:02:15.780]   So actually you can see it in the notebook.
[02:02:15.780 --> 02:02:26.780]   Um, if you go to, and if I can find some traces that have that example, I will pull them up.
[02:02:26.780 --> 02:02:31.780]   Um, but, uh, I think it's in utilities somewhere.
[02:02:31.780 --> 02:02:32.780]   Yeah.
[02:02:32.780 --> 02:02:38.780]   So basically this tool node with fallbacks, basically what happens is in this tool error.
[02:02:38.780 --> 02:02:44.780]   So this, if there's an error in the tool call itself, it'll return that error.
[02:02:44.780 --> 02:02:50.780]   And usually the agent will then, or the LLM assistant will look at that and like self-correct its tool call.
[02:02:50.780 --> 02:02:53.780]   So that's, that's typically how it's done.
[02:02:53.780 --> 02:02:56.780]   And this actually is reasonably effective.
[02:02:56.780 --> 02:03:05.780]   But again, you know, the nice thing about the, the other implementation, the custom agent, I call it in the notebook is you don't rely on tool calling in this way.
[02:03:05.780 --> 02:03:08.780]   And so you can get around this type of issue.
[02:03:08.780 --> 02:03:14.780]   Um, but basically catching the errors in the tool call itself with this code is, is what's currently done.
[02:03:14.780 --> 02:03:16.780]   Let's see if I can actually find an example.
[02:03:16.780 --> 02:03:22.780]   Um, yeah, I mean, if I can look for one where it gets the answer wrong.
[02:03:22.780 --> 02:03:23.780]   Yeah.
[02:03:23.780 --> 02:03:25.780]   Let's see this one.
[02:03:25.780 --> 02:03:27.780]   Let's see if we can find a tool call failure.
[02:03:27.780 --> 02:03:30.780]   Um, so here's the trace.
[02:03:30.780 --> 02:03:34.780]   Um, let's see.
[02:03:34.780 --> 02:03:35.780]   Let's see.
[02:03:35.780 --> 02:03:38.780]   Eh, okay.
[02:03:38.780 --> 02:03:39.780]   It didn't have a tool call error.
[02:03:39.780 --> 02:03:40.780]   Yeah.
[02:03:40.780 --> 02:03:45.780]   So basically what you'll see in the message history is though, like the tool itself will return this error message.
[02:03:45.780 --> 02:03:48.780]   And then the LLM will say, oh, okay, I need to retry.
[02:03:48.780 --> 02:03:50.780]   And then it'll retry and hopefully get it right.
[02:03:50.780 --> 02:03:51.780]   Yeah.
[02:03:51.780 --> 02:03:52.780]   Yep.
[02:03:52.780 --> 02:03:53.780]   Yeah.
[02:03:53.780 --> 02:03:53.780]   Yep.
[02:03:53.780 --> 02:03:54.780]   All up to that question.
[02:03:54.780 --> 02:03:58.780]   I use Python, Pydantic, and Jason Lewis Instructor.
[02:03:58.780 --> 02:03:59.780]   Oh, yeah.
[02:03:59.780 --> 02:04:01.780]   Which is great for that exact problem.
[02:04:01.780 --> 02:04:02.780]   Yep.
[02:04:02.780 --> 02:04:04.780]   Like the instance revalidation of like the output.
[02:04:04.780 --> 02:04:05.780]   Especially for those simple little errors.
[02:04:05.780 --> 02:04:06.780]   Yes.
[02:04:06.780 --> 02:04:15.780]   Um, I'm wondering if I'll provide probably work, but does, um, I'm Shane, I haven't used it much, but could you use it with an instructor and then Pydantic, okay?
[02:04:15.780 --> 02:04:16.780]   Okay.
[02:04:16.780 --> 02:04:17.780]   So this is a very good point.
[02:04:17.780 --> 02:04:19.780]   So yeah, I'm a big fan of instructor.
[02:04:19.780 --> 02:04:26.780]   Um, I haven't used it as much, but what you're saying is one particular type of tool call.
[02:04:26.780 --> 02:04:31.780]   So basically that pertains, I believe more structured outputs, which is indeed a kind of tool call.
[02:04:31.780 --> 02:04:34.780]   And when you're using something like a Pydantic schema, you're right.
[02:04:34.780 --> 02:04:37.780]   It's very easy to check and like correct errors.
[02:04:37.780 --> 02:04:44.780]   So I've found catching errors, like with schema validation, like using instructor is, is really good.
[02:04:44.780 --> 02:04:48.780]   And we have some other things you can use within line chain to do the same thing.
[02:04:48.780 --> 02:04:51.780]   So, so that's one type of error.
[02:04:51.780 --> 02:04:55.780]   That's actually particularly easy to kind of detect and correct.
[02:04:55.780 --> 02:05:00.780]   What we show in this notebook here and the code I showed is more for any general tool.
[02:05:00.780 --> 02:05:08.780]   So, um, so this code here will operate on any tool you call regardless.
[02:05:08.780 --> 02:05:10.780]   So it doesn't have to do with structured outputs or anything.
[02:05:10.780 --> 02:05:15.780]   And so it's just a more general, uh, check for tool call errors.
[02:05:15.780 --> 02:05:19.780]   Now, in terms of instructor with Lang chain.
[02:05:19.780 --> 02:05:21.780]   Now, maybe I'll just back up a little bit.
[02:05:21.780 --> 02:05:25.780]   So Lang graph does not require Lang chain at all.
[02:05:25.780 --> 02:05:27.780]   So that's kind of point one and neither does Lang Smith.
[02:05:27.780 --> 02:05:31.780]   So actually everything we're doing here does not need to use Lang chain.
[02:05:31.780 --> 02:05:38.780]   So actually that could be a pretty interesting thing to try for like kind of choose your own adventure thing.
[02:05:38.780 --> 02:05:48.780]   But basically in the custom agent part, um, I use with structured outputs to do the grading.
[02:05:48.780 --> 02:06:00.780]   So if you go to, um, yeah, if you look at the, um, the retrieval grader here, so this is using LLM with structured output.
[02:06:00.780 --> 02:06:01.780]   And here's my grade schema.
[02:06:01.780 --> 02:06:02.780]   Try that one instructor.
[02:06:02.780 --> 02:06:03.780]   That should work great.
[02:06:03.780 --> 02:06:04.780]   You don't need Lang chain at all for this.
[02:06:04.780 --> 02:06:07.780]   Um, and that'll fit right into Lang graph.
[02:06:07.780 --> 02:06:12.780]   So actually I think it'd be great to use instructor with Lang graph for this particular use case.
[02:06:12.780 --> 02:06:19.780]   And I do agree that Lang, that instructor is really nice for those kind of like schema validation error correction.
[02:06:19.780 --> 02:06:23.780]   I plug and play that, that I'm going to make a note.
[02:06:23.780 --> 02:06:27.780]   That's a really good kind of choose your own adventure case.
[02:06:27.780 --> 02:06:31.780]   Um, or should I put that?
[02:06:31.780 --> 02:06:36.780]   Uh, try instructor with Lang graph for grading.
[02:06:36.780 --> 02:06:38.780]   Yeah, I like that a lot.
[02:06:38.780 --> 02:06:43.780]   Yep.
[02:06:43.780 --> 02:06:44.780]   Yep.
[02:06:44.780 --> 02:06:47.780]   Yep.
[02:06:47.780 --> 02:06:49.780]   Um, kind of more of a meta question.
[02:06:49.780 --> 02:06:52.780]   Just, uh, really aligned with what you guys tried to tackle.
[02:06:52.780 --> 02:06:58.780]   Um, what's the, what are the path forward, uh, to continue to make this a model better?
[02:06:58.780 --> 02:07:03.780]   And specifically, where's the not, uh, where's the rag pipeline to be in the right now?
[02:07:03.780 --> 02:07:05.780]   Like what do we still have to do that?
[02:07:05.780 --> 02:07:09.780]   And your guys have to do so, like down in the moment.
[02:07:09.780 --> 02:07:10.780]   Um, yeah.
[02:07:10.780 --> 02:07:15.780]   So a question was related to just rag in general and where is rag?
[02:07:15.780 --> 02:07:16.780]   Rag agents.
[02:07:16.780 --> 02:07:17.780]   Yeah, sure.
[02:07:17.780 --> 02:07:26.780]   Well, to be honest, a lot of the problems with rag, I think about our own internal application chat line chain.
[02:07:26.780 --> 02:07:30.780]   A lot of problems with rag actually are retrieval problems.
[02:07:30.780 --> 02:07:31.780]   Retrieval is just hard.
[02:07:31.780 --> 02:07:33.780]   I'll give a good example.
[02:07:33.780 --> 02:07:41.780]   Like Lang chain, we have, um, I'm trying to remember five million tokens of context across all our docs, something like that.
[02:07:41.780 --> 02:07:46.780]   We have all sorts of different, we have a very long tail of integration docs.
[02:07:46.780 --> 02:07:49.780]   You want very high quality coverage and questions across all of that.
[02:07:49.780 --> 02:08:05.780]   There's a lot in how you index all that stuff to ensure that you boost retrievals from more canonical how to guys that are much better documented, but still having coverage over long tail content for like, you know, long tail questions.
[02:08:05.780 --> 02:08:18.780]   For example, if you're using raw semantic similarity search, you can have relevance to, you know, say your how to guy, which is really well developed and three random long tail documents that are not well developed and they'll all get returned.
[02:08:18.780 --> 02:08:21.780]   And so how do you overlay different systems?
[02:08:21.780 --> 02:08:32.780]   It could be re ranking, uh, to basically promote content, uh, that you believe to be more accurate or better based on some criteria.
[02:08:32.780 --> 02:08:38.780]   So that is all to say, I think with rag, the challenge is actually just domain specific retrieval for your application.
[02:08:38.780 --> 02:08:39.780]   That's just a hard problem.
[02:08:39.780 --> 02:08:40.780]   And there's been a lot of work on this.
[02:08:40.780 --> 02:08:42.780]   It's been around for a long time.
[02:08:42.780 --> 02:08:44.780]   I think that's really the limiter.
[02:08:44.780 --> 02:08:47.780]   And actually there's kind of no, no silver bullet.
[02:08:47.780 --> 02:08:55.780]   Like in our case, we're having to look at the structure of our documents very carefully, design our retrieval strategy based on that doc structure.
[02:08:55.780 --> 02:09:04.780]   Like in particular, we're thinking about applying certain post retrieval ranking to docs of certain types based upon their importance.
[02:09:04.780 --> 02:09:12.780]   We're thinking about retrieving a large, like a large initial number of docs and then boiling them down with, with kind of re ranking based upon importance.
[02:09:12.780 --> 02:09:15.780]   So I still think retrieval is very hard.
[02:09:15.780 --> 02:09:16.780]   It's very domain specific.
[02:09:16.780 --> 02:09:18.780]   It depends on the structure of your documentation.
[02:09:18.780 --> 02:09:20.780]   And there's kind of no free lunch.
[02:09:20.780 --> 02:09:25.780]   I think the things that are good about rag is context windows are getting much larger for LLMs.
[02:09:25.780 --> 02:09:35.780]   And so back to that point I was making before, I think we're seeing, and we're considering this ourselves, less worry about the exact right chunk size.
[02:09:35.780 --> 02:09:43.780]   You can think more about chunking in, you know, different ways and then passing full documents to your final model.
[02:09:43.780 --> 02:09:45.780]   So I think that part of it's really good.
[02:09:45.780 --> 02:09:58.780]   But still, even like, even in this particular case, you probably still need some re ranking to promote the most important documents.
[02:09:58.780 --> 02:10:16.780]   So I think retrieval is still quite hard in particular, like even looking at the line chain docs in particular, the overlay of document importance on top of raw semantic similarity search, right?
[02:10:16.780 --> 02:10:21.780]   Take a case of like, I have a question semantically, it's similar to 10 different documents.
[02:10:21.780 --> 02:10:28.780]   Those documents, though, vary widely in their quality and their relevant, like, more like higher level relevance.
[02:10:28.780 --> 02:10:33.780]   Like, maybe that passage is related, but like, it might be a general question about how to build an agent.
[02:10:33.780 --> 02:10:37.780]   And then some random integration doc talks about building agent for integration X.
[02:10:37.780 --> 02:10:45.780]   And I want to make sure that the more canonical, well-developed agent, you know, overview doc gets promoted and passed back in the end.
[02:10:45.780 --> 02:10:46.780]   Stuff like that.
[02:10:46.780 --> 02:10:50.780]   Sorry, it's a long answer, but basically rag is, it's hard.
[02:10:50.780 --> 02:10:53.780]   I mean, I think retrieval is really the hard part.
[02:10:53.780 --> 02:10:57.780]   The generation part is getting better and better as long contexts grow.
[02:10:57.780 --> 02:10:58.780]   Yep.
[02:10:58.780 --> 02:11:02.780]   So, this re-ranking approach for your documents.
[02:11:02.780 --> 02:11:03.780]   Yeah.
[02:11:03.780 --> 02:11:05.780]   What's the metadata?
[02:11:05.780 --> 02:11:09.780]   Do you have a relevancy to a particular topic as well as the numerical ranking?
[02:11:09.780 --> 02:11:10.780]   Yeah.
[02:11:10.780 --> 02:11:11.780]   Okay.
[02:11:11.780 --> 02:11:12.780]   That's a great question.
[02:11:12.780 --> 02:11:18.780]   So the question was, when we're talking about this re-ranking, how do you assign this relevance to your documents?
[02:11:18.780 --> 02:11:19.780]   What is that?
[02:11:19.780 --> 02:11:22.780]   So, I'll just give you what we've been thinking about.
[02:11:22.780 --> 02:11:31.780]   I actually think it is, for us, going to be a hand-tuned kind of relevance score based upon our doc structure.
[02:11:31.780 --> 02:11:44.780]   So, if you look at the Langchain docs, like go to Langchain documents.
[02:11:44.780 --> 02:11:45.780]   Yeah.
[02:11:45.780 --> 02:11:46.780]   So, Langchain documentation.
[02:11:46.780 --> 02:11:53.780]   We have these sections up here, tutorials, how-to guides, conceptual guides, which are like really well-developed, more recent, well-curated.
[02:11:53.780 --> 02:11:58.780]   These, you can imagine, have some kind of relevance or importance ranking of one or highest.
[02:11:58.780 --> 02:12:05.780]   So, these are documents that contain very high-quality, well-curated answers that we want to promote and serve to users in the generation phase.
[02:12:05.780 --> 02:12:09.780]   However, let's say someone asks a question about one particular integration, right?
[02:12:09.780 --> 02:12:12.780]   If you go to integrations, we have all these pages, right?
[02:12:12.780 --> 02:12:17.780]   Components, go to retrievers, look at the ZEP cloud retriever.
[02:12:17.780 --> 02:12:20.780]   This is some stuff related to ZEP cloud specifically.
[02:12:20.780 --> 02:12:23.780]   If someone asks about ZEP cloud, you do want to be able to retrieve that doc, right?
[02:12:23.780 --> 02:12:36.780]   And so, some ability to differentiate between questions that need, you know, general answers, in which case you would promote your more canonical how-to guides, conceptual docs,
[02:12:36.780 --> 02:12:42.780]   versus questions that require retrieval from very specific integration docs, in which case you would still promote this information.
[02:12:42.780 --> 02:12:44.780]   That's kind of the crux of it.
[02:12:44.780 --> 02:12:59.780]   And I think we'll probably use kind of manual or heuristic scoring to up-weight or up-rank our core, like, how-to guides and conceptual guides over longer-tailed integration docs.
[02:12:59.780 --> 02:13:04.780]   And we might have a router that will indicate whether the question is general or specific.
[02:13:04.780 --> 02:13:07.780]   So, those are the two things that I'd probably do.
[02:13:07.780 --> 02:13:16.780]   So, routing on the question side, and then some kind of heuristic relevance or importance grading or quality grading on the document side.
[02:13:16.780 --> 02:13:22.780]   And that can be packed in the metadata that you pack along with your index chunks.
[02:13:22.780 --> 02:13:23.780]   Yep.
[02:13:23.780 --> 02:13:24.780]   Maybe not.
[02:13:24.780 --> 02:13:25.780]   Yeah.
[02:13:25.780 --> 02:13:26.780]   Oh, yeah.
[02:13:26.780 --> 02:13:36.780]   So, let's say a typical RAC application where there's a question-and-answer fail, but we kind of maintain a multi-turn, like, by example,
[02:13:36.780 --> 02:13:40.780]   we maintain the conversation history of the user to kind of create a .
[02:13:40.780 --> 02:13:47.780]   The problem is, like, let's say, for example, a question is asked, and then the retrieval chunks are, like, let's say, five.
[02:13:47.780 --> 02:13:48.780]   Right.
[02:13:48.780 --> 02:13:50.780]   And then a subsequent question is asked.
[02:13:50.780 --> 02:13:57.780]   So, let's say, you know, but it's related to the first question, but still, somehow, in the first node, you transform the query,
[02:13:57.780 --> 02:13:59.780]   and then the retrieval chunks are still the same.
[02:13:59.780 --> 02:14:05.780]   So, like, I would just get that answer which is, like, more of the first time.
[02:14:05.780 --> 02:14:16.780]   So, like, how do you, my question is, like, how do you make sure that, let's say, he wanted to deep dive into the document, like, into the more context?
[02:14:16.780 --> 02:14:17.780]   How do you make that happen?
[02:14:17.780 --> 02:14:19.780]   And then, like, .
[02:14:19.780 --> 02:14:35.780]   Yeah, so the question, I guess, was, like, in a multi-turn RAG context, let's say you have a case where a user asks an initial question, and you retrieve some documents, you produce an answer, and they ask a follow-up that says, give me more information about this.
[02:14:35.780 --> 02:14:42.780]   Now, do you want to re-retrieve, or do you want to re-reference those same docs?
[02:14:42.780 --> 02:14:43.780]   No.
[02:14:43.780 --> 02:14:48.780]   And so, what happens in my case is, like, I kind of, I try to rewrite that question.
[02:14:48.780 --> 02:14:52.780]   Okay, you do a re-writing, so you rewrite the question, okay.
[02:14:52.780 --> 02:14:54.780]   And then go test the documents.
[02:14:54.780 --> 02:14:57.780]   So, most often the documents would be the same as before.
[02:14:57.780 --> 02:14:58.780]   The same as before.
[02:14:58.780 --> 02:14:59.780]   Yeah.
[02:14:59.780 --> 02:15:00.780]   Yeah.
[02:15:00.780 --> 02:15:01.780]   Okay.
[02:15:01.780 --> 02:15:03.780]   So, the answers would be, like, mostly the same.
[02:15:03.780 --> 02:15:04.780]   Okay.
[02:15:04.780 --> 02:15:06.780]   Okay, interesting.
[02:15:06.780 --> 02:15:09.780]   So, the problem there is more of a retrieval problem.
[02:15:09.780 --> 02:15:12.780]   You're doing a rewrite, you're still retrieving the same set of documents, though.
[02:15:12.780 --> 02:15:14.780]   Now, what do you want to have happen?
[02:15:14.780 --> 02:15:22.780]   You want to, do you actually want to retrieve different documents, or do you want to...
[02:15:22.780 --> 02:15:25.780]   It's kind of like deep dive, like, for example.
[02:15:25.780 --> 02:15:28.780]   Yeah, but that's the question, what do you mean by deep dive?
[02:15:28.780 --> 02:15:32.780]   Like, you're retrieving, let's say, it's a chapter of a book.
[02:15:32.780 --> 02:15:33.780]   You're retrieving only the first page.
[02:15:33.780 --> 02:15:35.780]   You want to retrieve the whole chapter.
[02:15:35.780 --> 02:15:36.780]   Yeah.
[02:15:36.780 --> 02:15:37.780]   Okay.
[02:15:37.780 --> 02:15:42.780]   Okay, then I think, actually, a question rewrite would probably not sufficient.
[02:15:42.780 --> 02:15:51.780]   What I would think about more is, for that second pass, you could actually do something
[02:15:51.780 --> 02:15:53.780]   like metadata filtering on your chunks.
[02:15:53.780 --> 02:15:59.780]   If you have your data or your documents partitioned by, like, chapters or some sections,
[02:15:59.780 --> 02:16:02.780]   I would just do a bulk retrieval of the whole section or something like that.
[02:16:02.780 --> 02:16:07.780]   So, it's more like a trick on the, on the retrieval side rather than a rewrite of the query.
[02:16:07.780 --> 02:16:08.780]   Because I hear, I see what you're saying.
[02:16:08.780 --> 02:16:10.780]   You rewrite the query, you might get the same docs back.
[02:16:10.780 --> 02:16:14.780]   If you want to guarantee that you actually get, like, a deeper dive in your docs,
[02:16:14.780 --> 02:16:16.780]   then maybe it's something in your retriever itself.
[02:16:16.780 --> 02:16:18.780]   You can increase K, so retrieve more docs.
[02:16:18.780 --> 02:16:24.780]   You could use metadata filtering to, like, ensure you get all the docs in a given chapter.
[02:16:24.780 --> 02:16:27.780]   So, I think it's more a retrieval thing.
[02:16:27.780 --> 02:16:30.780]   But, that's kind of an interesting point though.
[02:16:30.780 --> 02:16:31.780]   Yeah.
[02:16:31.780 --> 02:16:32.780]   Cool.
[02:16:32.780 --> 02:16:39.780]   Well, I know it's been two and a half hours almost.
[02:16:39.780 --> 02:16:41.780]   So, there we go.
[02:16:41.780 --> 02:16:42.780]   It's good.
[02:16:42.780 --> 02:16:43.780]   Yeah, yeah.
[02:16:43.780 --> 02:16:44.780]   Yeah, yeah.
[02:16:44.780 --> 02:16:45.780]   Yeah.
[02:16:45.780 --> 02:16:48.780]   I'm, I'm, I'm hanging out for another till noon, so.
[02:16:48.780 --> 02:16:51.780]   There's a local tutorial.
[02:16:51.780 --> 02:16:52.780]   Oh, cool.
[02:16:52.780 --> 02:16:54.780]   And it's working fine.
[02:16:54.780 --> 02:16:56.780]   I was just talking about the sports questions.
[02:16:56.780 --> 02:16:58.780]   Is that, like, control group questions?
[02:16:58.780 --> 02:16:59.780]   Yeah, yeah.
[02:16:59.780 --> 02:17:00.780]   Okay, okay.
[02:17:00.780 --> 02:17:01.780]   This is good.
[02:17:01.780 --> 02:17:08.780]   So, see, the question was, he's doing the, the local, the local agent tutorial.
[02:17:08.780 --> 02:17:10.780]   And the question's on the eval set.
[02:17:10.780 --> 02:17:12.780]   So, actually, that's a fun one.
[02:17:12.780 --> 02:17:13.780]   Modify them any way you want.
[02:17:13.780 --> 02:17:18.780]   The key point was, I wanted some questions that are definitely outside the vector store.
[02:17:18.780 --> 02:17:21.780]   So, I asked something about, like, two things about sports.
[02:17:21.780 --> 02:17:23.780]   Because I know it's not in my vector store about agents.
[02:17:23.780 --> 02:17:28.780]   So, I think I indexed three blog posts about, like, agents and prompting and adversarial examples.
[02:17:28.780 --> 02:17:32.780]   I just wanted some orthogonal questions that'll force web search.
[02:17:32.780 --> 02:17:34.780]   So, that's the only thing there.
[02:17:34.780 --> 02:17:37.780]   But you actually play with those and you can modify them and all that.
[02:17:37.780 --> 02:17:38.780]   Yeah.
[02:17:38.780 --> 02:17:39.780]   But that's cool.
[02:17:39.780 --> 02:17:40.780]   It's working.
[02:17:40.780 --> 02:17:41.780]   Are you using Lama 3?
[02:17:41.780 --> 02:17:42.780]   Yeah.
[02:17:42.780 --> 02:17:43.780]   Cool.
[02:17:43.780 --> 02:17:44.780]   Yeah.
[02:17:44.780 --> 02:17:45.780]   It's in the 70b.
[02:17:45.780 --> 02:17:46.780]   Oh.
[02:17:46.780 --> 02:17:48.780]   You have a, you have a laptop big enough for 70b.
[02:17:48.780 --> 02:17:49.780]   Not really.
[02:17:49.780 --> 02:17:50.780]   Okay, okay.
[02:17:50.780 --> 02:17:51.780]   You're at the edge of.
[02:17:51.780 --> 02:17:52.780]   Okay, okay.
[02:17:52.780 --> 02:17:53.780]   Yeah, 8b, 8b's.
[02:17:53.780 --> 02:17:54.780]   Yeah, exactly.
[02:17:54.780 --> 02:17:55.780]   I mean, it's actually kind of nice.
[02:17:55.780 --> 02:17:55.780]   You can even run the 70b, to be honest.
[02:17:55.780 --> 02:17:56.780]   I'm not sure I can even run it.
[02:17:56.780 --> 02:17:57.780]   But, yeah, that's cool.
[02:17:57.780 --> 02:17:58.780]   Nice.
[02:17:58.780 --> 02:17:59.780]   Let's see.
[02:17:59.780 --> 02:18:00.780]   Let's see.
[02:18:00.780 --> 02:18:00.780]   Yeah.
[02:18:00.780 --> 02:18:00.780]   Yeah, well, I can just hang out and, oh, yeah.
[02:18:00.780 --> 02:18:01.780]   Sure.
[02:18:01.780 --> 02:18:02.780]   Sure.
[02:18:02.780 --> 02:18:03.780]   What would be the best way to interpret for questions?
[02:18:03.780 --> 02:18:04.780]   Is quality or?
[02:18:04.780 --> 02:18:04.780]   Yeah.
[02:18:04.780 --> 02:18:05.780]   Yeah.
[02:18:05.780 --> 02:18:06.780]   Well, I can just hang out and, oh, yeah.
[02:18:06.780 --> 02:18:07.780]   Sure.
[02:18:07.780 --> 02:18:08.780]   Yeah.
[02:18:08.780 --> 02:18:08.780]   What would be the best way to interpret for questions?
[02:18:08.780 --> 02:18:08.780]   Is quality or?
[02:18:08.780 --> 02:18:09.780]   Yeah.
[02:18:09.780 --> 02:18:36.780]   Well, if you have a chat application, you have a chat application, you have a chat
[02:18:36.780 --> 02:18:41.780]   So, the question was related to how do you incorporate multi-turn?
[02:18:41.780 --> 02:18:50.780]   So, if you look at the React agent, it uses a chat history as its state.
[02:18:50.780 --> 02:18:56.780]   In that case, follow-up questions will be captured just in the message history as part of chat.
[02:18:56.780 --> 02:19:00.780]   I think the current layout of the custom line graph agent, though, is a little bit more single
[02:19:00.780 --> 02:19:01.780]   single turn.
[02:19:01.780 --> 02:19:02.780]   So, it would be kind of question-answer.
[02:19:02.780 --> 02:19:03.780]   That's, oh, yeah.
[02:19:03.780 --> 02:19:04.780]   And then, like, that the agent actually has to come back with a question, too.
[02:19:04.780 --> 02:19:05.780]   Oh, okay.
[02:19:05.780 --> 02:19:06.780]   About details.
[02:19:06.780 --> 02:19:07.780]   Okay, got it.
[02:19:07.780 --> 02:19:08.780]   So, the, okay, got it.
[02:19:08.780 --> 02:19:15.780]   So, the question was, how do you modify the agent so that it will actually return, like, if it needs more clarification from the user?
[02:19:15.780 --> 02:19:16.780]   Yeah.
[02:19:16.780 --> 02:19:16.780]   Yeah.
[02:19:16.780 --> 02:19:18.780]   These particular agent examples don't do that.
[02:19:18.780 --> 02:19:21.780]   But, again, I think that's maybe a good takeaway for me.
[02:19:21.780 --> 02:19:26.780]   I should add that to these tutorials, incorporate a simple example of multi-turn.
[02:19:26.780 --> 02:19:33.780]   I will do that and get my, I will do that and get my, I will do that and get my, I will do that.
[02:19:33.780 --> 02:19:36.780]   And, I will do that and get my, I will do that.
[02:19:36.780 --> 02:19:41.780]   And, I will do that and get my, I will do that.
[02:19:41.780 --> 02:19:45.780]   And, I will do that and get my, I will do that.
[02:19:45.780 --> 02:19:47.780]   And, I will do that.
[02:19:47.780 --> 02:19:57.780]   And, I will do that and, I will do that and, I will send that to you.
[02:19:57.780 --> 02:20:03.780]   So, now I have, like, a simple about or pull off question.
[02:20:03.780 --> 02:20:04.780]   Yes.
[02:20:04.780 --> 02:20:08.780]   I think that should be .
[02:20:08.780 --> 02:20:09.780]   Yes.
[02:20:09.780 --> 02:20:10.780]   Exactly.
[02:20:10.780 --> 02:20:15.780]   So, you want, so, I mentioned previously, if you look at Langrath.
[02:20:15.780 --> 02:20:19.780]   Um, let me find it.
[02:20:19.780 --> 02:20:22.780]   It's, it's one of our notebooks.
[02:20:22.780 --> 02:20:27.780]   Um, the customer support agent.
[02:20:27.780 --> 02:20:35.780]   Uh, this, so, Langrath examples customer support is an example of an agent that has, like, multi-turn dialogue.
[02:20:35.780 --> 02:20:38.780]   But, it's complicated.
[02:20:38.780 --> 02:20:43.780]   So, I'd like to maybe augment these tutorials with a simpler example.
[02:20:43.780 --> 02:20:50.780]   Um, I will, yeah, I will, I'll follow up on that.
[02:20:50.780 --> 02:20:54.780]   If you give me, give me a contact info and I'll, I'll send you something.
[02:20:54.780 --> 02:20:55.780]   Cool.
[02:20:55.780 --> 02:20:57.780]   Well, I'll just, I'll sit up here.
[02:20:57.780 --> 02:21:08.780]   Anyone can just come and grab me.
[02:21:08.780 --> 02:21:09.780]   Uh, thanks for everything.
[02:21:09.780 --> 02:21:11.780]   Hopefully, the cookbooks are working.
[02:21:11.780 --> 02:21:12.780]   Um, yeah, it was good.
[02:21:12.780 --> 02:21:13.780]   Made it two and a half hours, so.
[02:21:13.780 --> 02:21:27.440]   Cool. Well, I'll just sit up here. Anyone can just come and grab me. Thanks for everything.
[02:21:27.440 --> 02:21:32.520]   Hopefully, the cookbooks are working. Yeah, it was good. Made it two and a half hours.
[02:21:32.520 --> 02:21:52.920]   So... Thanks.

