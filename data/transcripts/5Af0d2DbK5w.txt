
[00:00:00.000 --> 00:00:03.840]   When you start a company, there's enormous amounts of shit that looks like that.
[00:00:03.840 --> 00:00:09.040]   Things that are like dumb or annoying or broken or unfair or not how the world should work.
[00:00:09.040 --> 00:00:10.640]   But it's how the world does work.
[00:00:10.640 --> 00:00:14.240]   And the only way to be successful is to do it, is to fight through that.
[00:00:14.240 --> 00:00:17.760]   Most of the times that we see a company grow really fast, really quickly,
[00:00:17.760 --> 00:00:23.200]   and get really big in terms of number of people, it becomes an absolute mess internally.
[00:00:23.200 --> 00:00:27.360]   I do think that sometimes EA gets too narrow-minded in specific.
[00:00:27.360 --> 00:00:31.280]   And this is one of the reasons why that people end up sort of fixating on one particular
[00:00:31.280 --> 00:00:35.040]   understanding of the universe, of ethics, of how things are going to progress.
[00:00:35.040 --> 00:00:38.880]   I think stablecoins becoming an important settlement mechanism is pretty likely.
[00:00:38.880 --> 00:00:41.680]   I think blockchains in general becoming a settlement mechanism
[00:00:41.680 --> 00:00:45.200]   and collateral clearing mechanism seems decently likely to me.
[00:00:54.240 --> 00:00:59.120]   Today on the Lunar Society podcast, I have the pleasure of interviewing Sam Beckman-Fried,
[00:00:59.120 --> 00:01:00.560]   CEO of FTX.
[00:01:00.560 --> 00:01:01.840]   Thanks for coming on the Lunar Society.
[00:01:01.840 --> 00:01:02.880]   Thanks for having me.
[00:01:02.880 --> 00:01:03.520]   All right.
[00:01:03.520 --> 00:01:04.560]   First question.
[00:01:04.560 --> 00:01:10.640]   Does the consecutive success of FTX and Alameda, does that suggest to you that the world has all
[00:01:10.640 --> 00:01:14.960]   kinds of low-hanging opportunities or was that a property of the inefficiencies of crypto markets
[00:01:14.960 --> 00:01:16.320]   at one particular point in history?
[00:01:16.320 --> 00:01:17.680]   I think it's probably more of the former.
[00:01:17.680 --> 00:01:20.000]   I think there were probably just a lot of inefficiencies.
[00:01:20.000 --> 00:01:23.920]   So I guess another part of this question is if you had to restart earning to give again,
[00:01:23.920 --> 00:01:26.640]   what are the odds you'd become a billionaire but you couldn't do it in crypto?
[00:01:26.640 --> 00:01:31.200]   I think they're pretty decent.
[00:01:31.200 --> 00:01:38.640]   A lot of it depends on what I end up choosing and how aggressive I end up deciding to be.
[00:01:38.640 --> 00:01:44.960]   There were a lot of pretty safe and secure career paths before me that definitely would
[00:01:44.960 --> 00:01:45.840]   not have ended there.
[00:01:47.040 --> 00:01:53.760]   But I think that if I'd sort of decided to really dedicate myself to starting up some
[00:01:53.760 --> 00:01:55.840]   businesses, there would have been a pretty decent chance of it.
[00:01:55.840 --> 00:01:59.280]   So that leads to the next question, which is that you've cited Will McCaskill's lunch
[00:01:59.280 --> 00:02:03.120]   with you while you were at MIT as being very important in deciding your career.
[00:02:03.120 --> 00:02:08.080]   He suggests that you do earning to give by going to a quant firm like Jane Street.
[00:02:08.080 --> 00:02:12.320]   In retrospect, given the success you've had as a founder, was that maybe bad advice and
[00:02:12.320 --> 00:02:15.840]   maybe you should have advised you to start a startup or a nonprofit?
[00:02:15.840 --> 00:02:20.800]   I mean, I don't think it was literally the best possible advice in that like, you know,
[00:02:20.800 --> 00:02:25.280]   that is what 2012 or something like, you know, think about starting a crypto exchange would
[00:02:25.280 --> 00:02:29.200]   have maybe been a, you know, but I think it was definitely helpful advice.
[00:02:29.200 --> 00:02:33.600]   And I think that, you know, relative to not having gotten advice at all then, I think
[00:02:33.600 --> 00:02:35.120]   it probably helped quite a bit.
[00:02:35.120 --> 00:02:35.440]   Right.
[00:02:35.440 --> 00:02:39.920]   But then there's a broader question of are people like you who could become founders,
[00:02:39.920 --> 00:02:46.400]   are they advised to take lower variance, lower risk careers that in expected value are less
[00:02:46.400 --> 00:02:47.200]   valuable?
[00:02:47.200 --> 00:02:48.480]   Yeah, I think that's probably true.
[00:02:48.480 --> 00:02:53.440]   I think probably people are advised too strongly to go down safe career paths.
[00:02:53.440 --> 00:02:57.920]   But I think it's worth noting that, first of all, there's a big difference between what
[00:02:57.920 --> 00:03:00.400]   makes sense altruistically and personally for this.
[00:03:00.400 --> 00:03:04.560]   And, you know, to the extent you're just thinking of personal criteria, that's going to argue
[00:03:04.560 --> 00:03:09.920]   heavily in favor of a safer career path because you have much more quickly declining, you
[00:03:09.920 --> 00:03:12.240]   know, marginal utility of money than the world does.
[00:03:12.240 --> 00:03:16.400]   So this is sort of like specifically for altruistically minded people.
[00:03:16.400 --> 00:03:23.280]   The other thing is that, you know, when you think about like where or what is it that
[00:03:23.280 --> 00:03:30.800]   sort of like is advising people to choose a safer route, I think people will often try
[00:03:30.800 --> 00:03:33.280]   and look to, oh, well, what was the career advice that they got?
[00:03:33.280 --> 00:03:38.960]   What was sort of like, you know, what were sort of these outward facing factors that
[00:03:38.960 --> 00:03:39.280]   you can see?
[00:03:39.280 --> 00:03:45.600]   But I think often the answer has to do something with them and their family or them and, you
[00:03:45.600 --> 00:03:48.720]   know, their friends or something much more personal.
[00:03:48.720 --> 00:03:52.640]   And, you know, when we talk with people about what they're thinking about doing with their
[00:03:52.640 --> 00:03:57.760]   career, you know, personal considerations and the advice of people close to them weighs
[00:03:57.760 --> 00:04:01.600]   really, really heavily on what decisions they end up making.
[00:04:01.600 --> 00:04:07.440]   So I didn't realize that the personal considerations were as important in your case as the advice
[00:04:07.440 --> 00:04:08.000]   you got from...
[00:04:08.000 --> 00:04:12.720]   Oh, I don't think in my case, but I think that in the case of many, many people that
[00:04:12.720 --> 00:04:13.760]   I talk to, they are.
[00:04:13.760 --> 00:04:17.040]   So speaking of declining marginal consumption, I'm wondering if you think the implication
[00:04:17.040 --> 00:04:20.800]   of this is that over the long term, all the richest people in the world will be utilitarian
[00:04:20.800 --> 00:04:23.840]   philanthropists because they don't have diminishing returns on consumption.
[00:04:23.840 --> 00:04:24.640]   They're just neutral.
[00:04:24.640 --> 00:04:28.400]   I mean, I wouldn't say all will, but I think there probably is something in that direction
[00:04:28.400 --> 00:04:33.520]   where people who are looking at sort of how they can help the world are going to end up
[00:04:33.520 --> 00:04:38.800]   being disproportionately represented amongst the most and maybe least successful.
[00:04:38.800 --> 00:04:40.160]   All right, let's talk about effective altruism.
[00:04:40.160 --> 00:04:45.200]   So in your interview with Tyler Cowen, you were asked what constrains the number of altruistically
[00:04:45.200 --> 00:04:46.240]   minded projects?
[00:04:46.240 --> 00:04:49.440]   And you answered probably someone who can start something.
[00:04:49.440 --> 00:04:53.600]   Now, is this a property of the world in general or is this a property of EAs?
[00:04:53.600 --> 00:04:57.360]   And if it's about EAs, then what do you think is there something about the movement that
[00:04:57.360 --> 00:04:59.360]   drives away people who could take leadership roles?
[00:04:59.360 --> 00:05:00.880]   Oh, I think it's just the world in general.
[00:05:00.880 --> 00:05:05.600]   I think even if you ignore altruistic projects and just look at profit minded ones, we have
[00:05:05.600 --> 00:05:10.240]   lots of ideas for businesses that we think would probably do pretty well if they were
[00:05:10.240 --> 00:05:13.680]   run quite well that we'd be excited to fund.
[00:05:13.680 --> 00:05:18.960]   And the missing ingredient quite frequently for them is the right person or team to take
[00:05:18.960 --> 00:05:19.600]   the lead on it.
[00:05:20.960 --> 00:05:24.240]   And I think that in general, it's just it's kind of brutal starting something.
[00:05:24.240 --> 00:05:26.400]   It's sort of brutal being a founder.
[00:05:26.400 --> 00:05:32.560]   And it requires a somewhat specific but extensive list of skills.
[00:05:32.560 --> 00:05:40.640]   And I think that those things end up making it generally fairly highly in demand.
[00:05:40.640 --> 00:05:43.040]   What would it take to get more of those kinds of people to go into EA?
[00:05:43.040 --> 00:05:49.680]   Yeah, I mean, I think part of it is probably just talking with them about, you know, have
[00:05:49.680 --> 00:05:51.280]   you thought about what you can do for the world?
[00:05:51.280 --> 00:05:53.680]   Have you thought about how you can have impact on the world?
[00:05:53.680 --> 00:05:56.080]   Have you thought about how you can maximize your impact on the world?
[00:05:56.080 --> 00:05:59.680]   And just sort of going down that path, I think a lot would be amenable.
[00:05:59.680 --> 00:06:04.560]   I think a lot would be excited about sort of thinking critically and ambitiously about
[00:06:04.560 --> 00:06:05.840]   how they can help the world.
[00:06:05.840 --> 00:06:08.640]   So I think honestly, just engagement is one piece of this.
[00:06:08.640 --> 00:06:15.200]   And then another thing, I think even within people who are altruistically minded and looking
[00:06:15.200 --> 00:06:20.000]   at what would it take for them to be more excited to be founders or to be better at,
[00:06:20.000 --> 00:06:22.080]   I think there are still things that you can do.
[00:06:22.080 --> 00:06:24.960]   And I think some of this is about empowering people.
[00:06:24.960 --> 00:06:28.480]   And some of this is about normalizing the fact that when you start something, it might
[00:06:28.480 --> 00:06:29.520]   fail and that's OK.
[00:06:29.520 --> 00:06:36.720]   And that's how most startups, and especially most very early stage startups, obviously
[00:06:36.720 --> 00:06:39.040]   this sort of changes over time.
[00:06:39.040 --> 00:06:46.640]   But when you look at sort of early stage companies, you shouldn't be running them.
[00:06:46.640 --> 00:06:50.400]   You shouldn't be trying to build them to maximize the chances of having at least a
[00:06:50.400 --> 00:06:51.440]   little bit of success.
[00:06:51.440 --> 00:06:59.440]   But what that means is that you have to be OK with the personal fallout of failing and
[00:06:59.440 --> 00:07:03.200]   that we have to build a community that is OK with that.
[00:07:03.200 --> 00:07:04.800]   And I don't think we have that right now.
[00:07:04.800 --> 00:07:06.480]   I think very few communities do.
[00:07:06.480 --> 00:07:09.040]   Now, there are many good objections to utilitarianism.
[00:07:09.040 --> 00:07:12.560]   As you know, you said yourself that we don't have a good account of infinite ethics.
[00:07:12.560 --> 00:07:16.480]   Should we attribute substantial weight to the probability that utilitarianism is wrong?
[00:07:16.480 --> 00:07:18.800]   And how do you hedge for this moral uncertainty in your giving?
[00:07:18.800 --> 00:07:26.240]   So I don't think it has super large impact on my giving, partially because in order to
[00:07:26.240 --> 00:07:30.400]   say you'd have to have sort of a concrete proposal for what else you would do and what
[00:07:30.400 --> 00:07:33.120]   that would imply that would be different actions wise.
[00:07:33.120 --> 00:07:36.400]   And I don't know that I've sort of been compelled by many of those.
[00:07:36.400 --> 00:07:39.920]   I do think, though, that there are a lot of things we don't understand right now.
[00:07:39.920 --> 00:07:42.720]   And I think one thing that you pointed to is infinite ethics.
[00:07:42.720 --> 00:07:47.680]   I think another thing is-- and I'm not sure this is quite moral uncertainty.
[00:07:47.680 --> 00:07:50.000]   This might be physical uncertainty more so than anything else.
[00:07:50.000 --> 00:07:55.280]   But there are a lot of sort of chains of reasoning people will go down that I think are like
[00:07:55.280 --> 00:08:03.040]   somewhat contingent on our current understanding of the universe in a way which might not be
[00:08:03.040 --> 00:08:03.360]   right.
[00:08:03.360 --> 00:08:07.440]   And certainly, if you look at expected value outcomes, might not be right.
[00:08:07.440 --> 00:08:12.400]   I think say what you will about the size of the universe and what that implies.
[00:08:12.400 --> 00:08:17.280]   But some of the same people who make arguments based on, well, here's how big the universe
[00:08:17.280 --> 00:08:24.400]   is also think the simulation hypothesis has decent probability.
[00:08:24.400 --> 00:08:31.520]   But I think very few people sort of chain through them like, well, OK, what would that
[00:08:31.520 --> 00:08:32.080]   imply?
[00:08:32.080 --> 00:08:34.640]   I don't think it's clear what any of this implies.
[00:08:34.640 --> 00:08:38.800]   I think in the end, if I had to say, how have these considerations changed my thoughts on
[00:08:38.800 --> 00:08:43.280]   what to do, the honest answer is that they have changed it a little bit.
[00:08:43.280 --> 00:08:49.440]   And I think the direction that they pointed me in is things with moderately more robust
[00:08:49.440 --> 00:08:51.200]   impact.
[00:08:51.200 --> 00:08:57.920]   And what I mean by that is there's sort of one way that you can calculate the expected
[00:08:57.920 --> 00:09:05.280]   value of an action, which is sort of pretty specific and pretty much like, here's what's
[00:09:05.280 --> 00:09:06.240]   going to happen.
[00:09:06.240 --> 00:09:07.200]   Here are the two outcomes.
[00:09:07.200 --> 00:09:08.400]   Here are the probabilities of them.
[00:09:08.400 --> 00:09:15.440]   There's another thing you can do, though, which is to try and say, all right, it's a
[00:09:15.440 --> 00:09:16.480]   little bit more hand wavy.
[00:09:16.480 --> 00:09:19.840]   But it's like, how much better is it going to make the world?
[00:09:19.840 --> 00:09:24.240]   How much does it matter if the world's kind of better in generic, diffuse ways?
[00:09:24.800 --> 00:09:31.200]   And I think typically, EA has been pretty skeptical of that second line of reasoning.
[00:09:31.200 --> 00:09:35.040]   And I think correctly, because I think that usually, when you see that deployed, it's
[00:09:35.040 --> 00:09:35.520]   nonsense.
[00:09:35.520 --> 00:09:42.240]   Usually, I think when people are pretty hard to nail down on what the specific reason is,
[00:09:42.240 --> 00:09:44.240]   they think that something might be good.
[00:09:44.240 --> 00:09:49.760]   It's because they haven't thought that hard about it or don't want to think that hard
[00:09:49.760 --> 00:09:50.240]   about it.
[00:09:50.240 --> 00:09:56.000]   And that the much better analyzed and vetted pathways are the ones that you should be
[00:09:56.000 --> 00:09:57.120]   paying more attention to.
[00:09:57.120 --> 00:10:04.560]   That being said, I do think that sometimes EA gets too narrow-minded and specific about
[00:10:04.560 --> 00:10:08.160]   plotting out sort of like courses of impact.
[00:10:08.160 --> 00:10:11.520]   And I think this is one of the reasons why that people end up sort of fixating on one
[00:10:11.520 --> 00:10:16.640]   particular understanding of the universe of ethics of how things are going to progress.
[00:10:17.200 --> 00:10:21.680]   But that all of these things have some amount of uncertainty in them.
[00:10:21.680 --> 00:10:29.040]   And when you jostle that, some sort of like theories of impact and some models behave
[00:10:29.040 --> 00:10:32.480]   somewhat robustly under jostling and some of them completely fall apart.
[00:10:32.480 --> 00:10:36.240]   And I've become like a little bit more sympathetic to ones that are kind of like a little bit
[00:10:36.240 --> 00:10:41.680]   robust under thoughts about what the world ends up looking like.
[00:10:41.680 --> 00:10:50.400]   So in the May 2022 Oregon congressional election, you gave $12 million to Carrick Flynn, whose
[00:10:50.400 --> 00:10:52.880]   campaign was ultimately unsuccessful.
[00:10:52.880 --> 00:10:56.880]   How have you updated your beliefs about the efficacy of political giving in the aftermath?
[00:10:56.880 --> 00:11:04.160]   Yeah, I mean, it was the first time that I'd sort of given that scale in a race.
[00:11:04.160 --> 00:11:10.000]   And I did it because he was, of all the candidates in the cycle, the most outspoken on the need
[00:11:10.000 --> 00:11:12.880]   for more pandemic preparedness and prevention.
[00:11:12.880 --> 00:11:18.160]   You know, he lost, obviously, you know, such is life.
[00:11:18.160 --> 00:11:25.600]   And I think that, you know, in the end, there's some updates, I think lots of sort of miniature
[00:11:25.600 --> 00:11:27.760]   updates on efficacy of various things.
[00:11:27.760 --> 00:11:34.160]   But, you know, also, you know, I never thought that the odds were extremely high that he
[00:11:34.160 --> 00:11:34.720]   was going to win.
[00:11:34.720 --> 00:11:37.840]   It was always going to be an uncertain close race.
[00:11:37.840 --> 00:11:40.640]   There's a limit to how much you can update from a one time occurrence.
[00:11:40.640 --> 00:11:46.560]   If you thought the odds were 50/50 and it turns out being, you know, close in one direction
[00:11:46.560 --> 00:11:50.720]   or another, right, there's sort of a maximum of maybe a factor of two update that you have
[00:11:50.720 --> 00:11:51.600]   on that.
[00:11:51.600 --> 00:11:57.280]   And so, you know, I think that there were a bunch of sort of micro updates just on, you
[00:11:57.280 --> 00:12:00.240]   know, specific factors of the race.
[00:12:00.240 --> 00:12:07.520]   But I think on a high level, I don't think it sort of changed my perspective on policy
[00:12:07.520 --> 00:12:08.320]   that much.
[00:12:08.320 --> 00:12:12.480]   But does it make you think there are diminishing or possibly negative marginal returns from
[00:12:12.480 --> 00:12:15.280]   one donor giving to a candidate because of the negative PR increase?
[00:12:15.280 --> 00:12:16.880]   At some point, yeah, I think that's probably true.
[00:12:16.880 --> 00:12:21.120]   So continuing on the theme of politics, when is it more effective to give the marginal
[00:12:21.120 --> 00:12:25.680]   million dollars to a political campaign or institution to make some change at the government
[00:12:25.680 --> 00:12:26.240]   level?
[00:12:26.240 --> 00:12:30.800]   And, you know, like putting in early detection or when is it more effective to just fund
[00:12:30.800 --> 00:12:31.280]   it yourself?
[00:12:31.280 --> 00:12:32.720]   It's a good question.
[00:12:32.720 --> 00:12:36.240]   And, you know, part of this is that it's not necessarily mutually exclusive.
[00:12:37.040 --> 00:12:42.320]   But, you know, I think one thing here is looking at what is the scale of the things that need
[00:12:42.320 --> 00:12:42.880]   to happen?
[00:12:42.880 --> 00:12:46.400]   How much are things like international cooperation important for it?
[00:12:46.400 --> 00:12:51.360]   When you look at pandemic prevention, you know, we're talking tens of billions of dollars
[00:12:51.360 --> 00:12:57.200]   of scale necessary to start putting this infrastructure in place.
[00:12:57.200 --> 00:13:03.280]   So it's a pretty big scale thing, which is hard to fund, you know, to that level individually.
[00:13:03.920 --> 00:13:07.520]   And it's also something where, you know, we're going to need to have cooperation between
[00:13:07.520 --> 00:13:15.120]   different countries on, you know, what their surveillance for new pathogens look like and
[00:13:15.120 --> 00:13:17.520]   on, you know, vaccine distribution, right?
[00:13:17.520 --> 00:13:23.040]   Like if you, you know, if some countries sort of have great distribution of vaccines and
[00:13:23.040 --> 00:13:24.880]   others don't, that's not good.
[00:13:24.880 --> 00:13:29.120]   It's both not fair and not equitable to the countries that end up getting hit hardest,
[00:13:29.120 --> 00:13:31.920]   but also in a global pandemic, it's going to spread.
[00:13:32.480 --> 00:13:34.480]   And so you need to have global coverage.
[00:13:34.480 --> 00:13:38.480]   And so I think that's another reason that government likely has to be involved, at least
[00:13:38.480 --> 00:13:39.840]   to some extent, in the efforts.
[00:13:39.840 --> 00:13:41.280]   Let's talk about future fund.
[00:13:41.280 --> 00:13:46.160]   So as you know, there are already many existing effective altruist organizations that do donations.
[00:13:46.160 --> 00:13:49.520]   What is the reason you thought there was more value in creating a new one?
[00:13:49.520 --> 00:13:50.160]   What's your edge?
[00:13:50.160 --> 00:13:54.560]   So, you know, part of it is I just think that there's value in having multiple organizations.
[00:13:54.560 --> 00:13:57.200]   Every organization is going to have its blind spots.
[00:13:57.200 --> 00:13:59.840]   And, you know, you can help cover those up if you have a few.
[00:14:00.800 --> 00:14:06.320]   And, you know, if OpenPhil didn't exist, maybe we would have created an organization that
[00:14:06.320 --> 00:14:07.440]   looks more like OpenPhil.
[00:14:07.440 --> 00:14:11.520]   But, you know, there's some extent to which they are covering a lot of what they're looking
[00:14:11.520 --> 00:14:11.680]   at.
[00:14:11.680 --> 00:14:14.560]   You know, we're looking at overlapping but not identical things.
[00:14:14.560 --> 00:14:17.760]   And so I think having that diversity can be valuable.
[00:14:17.760 --> 00:14:22.320]   But, you know, pointing to what are the ways in which we sort of intentionally designed
[00:14:22.320 --> 00:14:25.600]   it to be a little bit different from existing donors.
[00:14:27.200 --> 00:14:32.080]   One thing that I think I've been really happy about has been the regranting program that
[00:14:32.080 --> 00:14:32.560]   we've had.
[00:14:32.560 --> 00:14:37.920]   So we have, you know, a number of people who are experts in various areas who we've basically
[00:14:37.920 --> 00:14:40.320]   donated pots to that they can regrant.
[00:14:40.320 --> 00:14:44.080]   And what are the reasons that we think this is valuable?
[00:14:44.080 --> 00:14:47.680]   One thing is just giving more stakeholders, you know, a chance to sort of voice their
[00:14:47.680 --> 00:14:52.320]   opinions in a way where we can't possibly sort of listen to everyone in the world directly
[00:14:52.320 --> 00:14:56.080]   and integrate all those opinions to come up with like the perfect set of answers.
[00:14:56.080 --> 00:15:01.840]   And so distributing it and letting them act semi-autonomously can help with that.
[00:15:01.840 --> 00:15:07.920]   But the other thing is that it really helps with large numbers of smaller grants.
[00:15:07.920 --> 00:15:12.320]   And so, you know, when you think about what, you know, an organization giving away $100
[00:15:12.320 --> 00:15:21.440]   million in a year is thinking about if we divided that up into $25,000 grants, right,
[00:15:21.440 --> 00:15:22.960]   like how many grants would that mean?
[00:15:22.960 --> 00:15:32.000]   That would mean what, like 4,000 grants, which is a lot of grants to analyze, right?
[00:15:32.000 --> 00:15:36.240]   Like, you know, if we want to give real thought to each one of those, we can't do that.
[00:15:36.240 --> 00:15:42.000]   But on the flip side, sometimes the smaller grants are the most impactful per dollar.
[00:15:42.000 --> 00:15:46.160]   And there are a lot of cases where someone really impressive has a really exciting idea
[00:15:46.160 --> 00:15:50.720]   for a new foundation, for a new organization that could do a lot of good for the world
[00:15:50.720 --> 00:15:57.360]   and needs $25,000 to get it started, right, to like rent out a small office, to be able
[00:15:57.360 --> 00:16:00.480]   to cover salaries for two employees for the first six months.
[00:16:00.480 --> 00:16:08.240]   Those are the kind of cases where sometimes a pretty small grant can make a huge change
[00:16:08.240 --> 00:16:13.440]   in the development of what might ultimately become a really impactful organization.
[00:16:13.440 --> 00:16:18.080]   But they're the kind of things that are really hard for our team to evaluate all of just
[00:16:18.080 --> 00:16:20.800]   given the number of them.
[00:16:20.800 --> 00:16:25.760]   But the Regranter Program gives us a way to do that, where, you know, if instead we have,
[00:16:25.760 --> 00:16:33.120]   you know, 10, 50, 100 maybe eventually regranters who are, you know, going out and finding a
[00:16:33.120 --> 00:16:39.440]   lot of those opportunities close to them, they can then sort of identify those and direct
[00:16:39.440 --> 00:16:40.080]   those grants.
[00:16:40.080 --> 00:16:44.960]   And it gives us a much wider reach and, you know, also biases it less towards people who
[00:16:44.960 --> 00:16:47.920]   we happen to know, which is good.
[00:16:47.920 --> 00:16:52.160]   We don't want to just like overfund everyone we happen to know and underfund everyone that
[00:16:52.160 --> 00:16:53.440]   we didn't happen to.
[00:16:53.440 --> 00:16:56.560]   So I think that's been sort of one initiative we've had, which I've been pretty excited
[00:16:56.560 --> 00:17:00.880]   about and, you know, I think we're going to keep doing.
[00:17:00.880 --> 00:17:04.960]   And then, you know, I think another thing is what we've really tried to have a lot of
[00:17:04.960 --> 00:17:07.360]   emphasis on making the process smooth and clean.
[00:17:07.360 --> 00:17:12.320]   There are pros and cons to this, but I do think that it sort of like drops the activation
[00:17:12.320 --> 00:17:18.080]   energy necessary for someone to decide to apply for a grant and, you know, fill out
[00:17:18.080 --> 00:17:19.280]   all of the forms and things like that.
[00:17:19.280 --> 00:17:24.080]   And so we've really tried to bring more people in the fold, you know, in terms of potential
[00:17:24.080 --> 00:17:25.840]   recipients.
[00:17:25.840 --> 00:17:30.320]   If you make it easy for people to fill out your application and if it's generally you're
[00:17:30.320 --> 00:17:33.360]   finding things that maybe other organizations wouldn't, how do you deal with the possibility
[00:17:33.360 --> 00:17:36.080]   of adverse selection in your philanthropic deal flow?
[00:17:36.080 --> 00:17:37.680]   It's a really good question.
[00:17:37.680 --> 00:17:43.200]   And of course, that's a worry that, you know, Bob down the street might like, you know,
[00:17:43.200 --> 00:17:46.880]   see a great bookcase that he wants and be like, oh, man, I wonder if I can get funding
[00:17:46.880 --> 00:17:47.680]   for this bookcase.
[00:17:47.680 --> 00:17:49.760]   It's going to house, you know, house a lot of knowledge.
[00:17:49.760 --> 00:17:50.800]   Knowledge is good, right?
[00:17:50.800 --> 00:17:54.320]   I mean, obviously not that one.
[00:17:54.320 --> 00:17:55.920]   I think we're going to detect pretty quickly.
[00:17:55.920 --> 00:18:00.480]   And I think the basic answer is that, you know, we still have that on all of these.
[00:18:00.480 --> 00:18:06.240]   And so, you know, we have, you know, we do have oversight of them.
[00:18:06.240 --> 00:18:13.440]   But what we also do is we do really deep dives into both all of the large ones, but also
[00:18:13.440 --> 00:18:16.160]   into sort of samplings of all the small ones.
[00:18:16.160 --> 00:18:21.440]   You know, we do some oversight of all of them, but we will do really deep dives into randomly
[00:18:21.440 --> 00:18:27.040]   sampled subsets of them, which allows us to get a pretty good statistical sense of whether
[00:18:27.040 --> 00:18:31.680]   we are facing significant, you know, adverse selection in them.
[00:18:31.680 --> 00:18:35.760]   You know, so far, we haven't seen obvious signs of it, but we're going to keep doing
[00:18:35.760 --> 00:18:40.320]   these analyses and, you know, see if anything worrying comes out of those.
[00:18:40.320 --> 00:18:48.240]   But that's sort of a way to be able to, you know, have more trusted analyses for more
[00:18:48.240 --> 00:18:49.680]   scaled up numbers of grants.
[00:18:49.680 --> 00:18:54.480]   So a long time ago, you wrote a blog post about how EA causes are multiplicative instead
[00:18:54.480 --> 00:18:55.920]   of additive.
[00:18:55.920 --> 00:18:57.520]   And we talked about that a little while ago.
[00:18:57.520 --> 00:19:00.880]   Do you still find that that's the case with most of the causes you care about?
[00:19:00.880 --> 00:19:04.800]   Or are there cases where some of the causes you care about are negatively multiplicative?
[00:19:04.800 --> 00:19:09.200]   Like, an example might be economic growth and the speed at which AI takes off.
[00:19:09.200 --> 00:19:10.880]   Yeah, I think it's getting more complicated.
[00:19:10.880 --> 00:19:15.200]   And I think that specifically around AI, you have a lot of really complex factors that
[00:19:15.200 --> 00:19:18.880]   point sometimes in the same direction, sometimes in opposite directions.
[00:19:18.880 --> 00:19:22.640]   And I think that especially if what you think matters is something like the relative progress
[00:19:22.640 --> 00:19:28.880]   of AI safety research versus AI capabilities research, a lot of things are going to have
[00:19:28.880 --> 00:19:36.640]   the same impact on both of those and thus confusing impact on safety as a whole.
[00:19:36.640 --> 00:19:39.600]   So I do think it's more complicated now.
[00:19:39.600 --> 00:19:43.760]   And I think it's not sort of cleanly things just multiplying with each other.
[00:19:43.760 --> 00:19:46.960]   I do think there are lots of cases where you see multiplicative behavior.
[00:19:46.960 --> 00:19:49.680]   But I also think there are cases where you just don't have that.
[00:19:49.680 --> 00:19:55.120]   And that sort of the conclusion of this is if you do have multiplicative cases, you probably
[00:19:55.120 --> 00:19:56.560]   want to be funding each piece of it.
[00:19:57.440 --> 00:20:01.760]   But if you don't, then you probably want to be trying to identify the most impactful pieces
[00:20:01.760 --> 00:20:04.240]   and specifically moving those along.
[00:20:04.240 --> 00:20:08.000]   And so I think our behavior should be different in those two scenarios.
[00:20:08.000 --> 00:20:12.480]   If you think of your philanthropy from a portfolio perspective, is correlation good or is it bad?
[00:20:12.480 --> 00:20:15.280]   I mean, I don't know.
[00:20:15.280 --> 00:20:16.800]   Expected value is expected value, right?
[00:20:16.800 --> 00:20:19.680]   And maybe here's one way to think about this.
[00:20:20.240 --> 00:20:28.160]   Let's pretend that there is one person in Bangladesh and another one in Mexico.
[00:20:28.160 --> 00:20:39.840]   And we have one intervention that can-- we have two interventions, both 50/50 on saving
[00:20:39.840 --> 00:20:45.760]   each of their lives in particular, some new drug that we could help release to combat
[00:20:45.760 --> 00:20:46.960]   some neglected disease.
[00:20:48.320 --> 00:20:52.320]   And this question of like, well, are they correlated?
[00:20:52.320 --> 00:20:54.720]   Like, are these two drugs correlated in their efficacy?
[00:20:54.720 --> 00:20:58.480]   And my basic argument is it doesn't matter, right?
[00:20:58.480 --> 00:21:03.840]   Because if you think about it from each of their perspectives, right, the person in Mexico
[00:21:03.840 --> 00:21:08.560]   isn't saying like, I only want to be saved in the cases where the person in Bangladesh
[00:21:08.560 --> 00:21:09.840]   is or isn't saved, right?
[00:21:09.840 --> 00:21:11.680]   Like, that's not relevant, right?
[00:21:11.680 --> 00:21:13.600]   They're like, I would like to live.
[00:21:13.600 --> 00:21:16.800]   And the person in Bangladesh similarly says, I would like to live.
[00:21:17.440 --> 00:21:20.560]   And you want to help both of them as much as you can.
[00:21:20.560 --> 00:21:28.160]   And it's not super relevant whether there's alignment or anti-alignment between the cases
[00:21:28.160 --> 00:21:29.680]   where you get lucky and the ones where you don't.
[00:21:29.680 --> 00:21:35.120]   What's the most likely reason that Future Fund fails to live up to your expectations?
[00:21:35.120 --> 00:21:39.520]   I think we just kind of get a little lame.
[00:21:39.520 --> 00:21:44.720]   Like, we give you a lot of decent things, but all the cooler or more innovative things
[00:21:44.720 --> 00:21:46.560]   that we do just don't seem to work very well.
[00:21:46.560 --> 00:21:52.480]   And we end up sort of giving the same place that everyone else is giving, wherever that
[00:21:52.480 --> 00:21:56.160]   ends up being, in that we don't turn out to be effective at starting new things.
[00:21:56.160 --> 00:22:00.640]   We don't turn out to be effective at thinking of new causes or executing on them.
[00:22:00.640 --> 00:22:04.320]   And hopefully, we'll avoid that.
[00:22:04.320 --> 00:22:05.920]   But it's always a risk.
[00:22:05.920 --> 00:22:10.240]   So should I think of your charitable giving as a yearly contribution of a billion dollars
[00:22:10.240 --> 00:22:11.280]   or less or more?
[00:22:11.280 --> 00:22:15.760]   Or should I think of it as a $30 billion hedge against the possibility that there's going
[00:22:15.760 --> 00:22:20.480]   to be some existential crisis that requires a large pool of liquid wealth?
[00:22:20.480 --> 00:22:22.240]   That's a really good question.
[00:22:22.240 --> 00:22:22.800]   I'm not sure.
[00:22:22.800 --> 00:22:26.000]   We're going to start giving something we already have.
[00:22:26.000 --> 00:22:28.240]   We've given away about $100 million so far this year.
[00:22:28.240 --> 00:22:32.240]   And we're going to start doing that partially because we think they're really important
[00:22:32.240 --> 00:22:36.160]   things to fund, partially because we want to make sure to start scaling up those systems
[00:22:36.160 --> 00:22:38.160]   and that process so that we're ready.
[00:22:38.160 --> 00:22:44.080]   And so that we notice opportunities as they combine, we have systems ready in place to
[00:22:44.080 --> 00:22:44.640]   give to them.
[00:22:45.360 --> 00:22:48.800]   But I think it's something we're really actively discussing internally, how concentrated
[00:22:48.800 --> 00:22:54.320]   versus diffuse we want that giving to be, and how much we want to be storing up for
[00:22:54.320 --> 00:22:59.760]   one very large opportunity versus how much it's going to be a mixture of many.
[00:22:59.760 --> 00:23:02.800]   When you look at a proposal and you think this project could be promising, but this
[00:23:02.800 --> 00:23:05.680]   is not the right person to lead it, what is the trait that's most often missing?
[00:23:05.680 --> 00:23:08.000]   Super interesting.
[00:23:08.000 --> 00:23:15.200]   I'm going to ignore the obvious answer set, which are like, the guy's just not very
[00:23:15.200 --> 00:23:18.240]   good, which, sure, fine.
[00:23:18.240 --> 00:23:25.280]   And maybe look at cases where it's someone who is pretty impressive, but I still think
[00:23:25.280 --> 00:23:27.760]   is not the right fit for this.
[00:23:27.760 --> 00:23:31.120]   I think there are a few things.
[00:23:31.120 --> 00:23:37.200]   I think one of them is, how much are they going to want to deal with really messy shit?
[00:23:37.200 --> 00:23:39.360]   This is a huge thing.
[00:23:39.360 --> 00:23:44.240]   If you go to work for-- and maybe to give some examples, when I was working at Jane
[00:23:44.240 --> 00:23:46.480]   Street, it's a really great place.
[00:23:46.480 --> 00:23:48.400]   Had a great time there.
[00:23:48.400 --> 00:23:56.320]   One thing which I didn't even realize was valuable there until I saw what things could
[00:23:56.320 --> 00:24:01.840]   look like outside was, if I decided that it was a good trade to buy one share of Apple
[00:24:01.840 --> 00:24:07.840]   stock on NASDAQ, there's a button to do that.
[00:24:10.560 --> 00:24:17.440]   If you, as a random citizen, want to buy one share of Apple stock directly on an exchange,
[00:24:17.440 --> 00:24:21.120]   it'll cost you tens of millions of dollars in a year to get set up to be able to do that.
[00:24:21.120 --> 00:24:27.920]   You've got to get a physical COLO, maybe, in Secaucus, New Jersey.
[00:24:27.920 --> 00:24:33.520]   You have to have market data agreements with these companies.
[00:24:33.520 --> 00:24:37.680]   You have to think about the SIP and about the NBBO and whether you're even allowed to
[00:24:37.680 --> 00:24:38.960]   lift on NASDAQ right then.
[00:24:39.600 --> 00:24:41.840]   You have to build technological infrastructure to do it.
[00:24:41.840 --> 00:24:45.040]   But all of that comes after you get a bank account.
[00:24:45.040 --> 00:24:46.160]   Let's even talk about that stuff.
[00:24:46.160 --> 00:24:49.120]   Getting a bank account that's going to work in finance is really hard.
[00:24:49.120 --> 00:24:56.400]   I've probably spent hundreds, if not thousands, of hours of my life trying to open bank accounts.
[00:24:56.400 --> 00:25:03.440]   One of the things at early Alameda that was really crucial to our ability to make money
[00:25:03.440 --> 00:25:10.240]   was having someone very senior spend hours per day in a physical bank branch,
[00:25:10.240 --> 00:25:13.520]   manually instructing wire transfers.
[00:25:13.520 --> 00:25:16.080]   If we didn't do that, we wouldn't have been able to do the trade.
[00:25:16.080 --> 00:25:22.800]   When you start a company, there's enormous amounts of shit that looks like that.
[00:25:22.800 --> 00:25:28.800]   Things that are dumb or annoying or broken or unfair or not how the world should work.
[00:25:28.800 --> 00:25:30.320]   But it's how the world does work.
[00:25:31.280 --> 00:25:34.320]   The only way to be successful is to do it, is to fight through that.
[00:25:34.320 --> 00:25:37.040]   If you're going to be like, "Whatever.
[00:25:37.040 --> 00:25:39.200]   I'm the CEO.
[00:25:39.200 --> 00:25:44.400]   I don't do that stuff," then no one's going to do that at your company.
[00:25:44.400 --> 00:25:45.280]   It's not going to get done.
[00:25:45.280 --> 00:25:47.360]   You won't have a bank account and you won't be able to operate.
[00:25:47.360 --> 00:25:51.840]   One of the biggest traits that I think is incredibly important for a founder
[00:25:51.840 --> 00:25:58.800]   and for an early team at a company, but that is not necessarily important
[00:25:58.800 --> 00:26:00.640]   for everything that you might want to do in life,
[00:26:00.640 --> 00:26:05.360]   is being willing to do a ton of grunt work if that's what's important for the company right then.
[00:26:05.360 --> 00:26:12.080]   Viewing it not as low prestige or too easy for you or something like that,
[00:26:12.080 --> 00:26:13.120]   but as, "Whatever.
[00:26:13.120 --> 00:26:14.240]   This is the important thing.
[00:26:14.240 --> 00:26:16.960]   This is the valuable thing to do, so it's what I'm going to do."
[00:26:16.960 --> 00:26:20.240]   That's one of the core traits.
[00:26:20.240 --> 00:26:23.680]   The other one is, are they excited about this idea?
[00:26:23.680 --> 00:26:25.760]   Will they actually put their heart and soul into it?
[00:26:27.520 --> 00:26:33.040]   Are they going to be a little bit drifting and bored and not really into it and half-ass it?
[00:26:33.040 --> 00:26:35.440]   I think those are two things that I really look for.
[00:26:35.440 --> 00:26:38.800]   How would you use your insights about pitcher fatigue to allocate talent in your companies?
[00:26:38.800 --> 00:26:48.800]   Pitcher fatigue is...
[00:26:48.800 --> 00:26:52.960]   I haven't thought about this in a while, but my thesis back then,
[00:26:52.960 --> 00:26:54.240]   which I still think is probably true,
[00:26:54.960 --> 00:26:58.880]   is that when it comes to pitchers in baseball,
[00:26:58.880 --> 00:27:03.120]   there's a lot of evidence that they get worse over the course of the game.
[00:27:03.120 --> 00:27:06.480]   Just the more innings they pitch, they get worse and worse and worse.
[00:27:06.480 --> 00:27:09.680]   Partially, it's just like it's hard on the arm,
[00:27:09.680 --> 00:27:14.400]   but it's worth noting that the evidence seems to support the claim
[00:27:14.400 --> 00:27:17.120]   that it depends on the pitchers, but that in general,
[00:27:17.120 --> 00:27:19.200]   you're better off breaking up their outings.
[00:27:19.200 --> 00:27:24.320]   It's not just a function of how many innings they pitch that season,
[00:27:24.320 --> 00:27:26.080]   but also extremely recently.
[00:27:26.080 --> 00:27:31.920]   If you could choose between someone throwing six innings every six days
[00:27:31.920 --> 00:27:35.360]   or throwing three innings every three days,
[00:27:35.360 --> 00:27:36.720]   probably you should choose the latter.
[00:27:36.720 --> 00:27:38.880]   Probably that's going to get the better pitching on average
[00:27:38.880 --> 00:27:40.480]   and just as many innings out of them.
[00:27:40.480 --> 00:27:44.880]   Fort Worth Baseball actually has since then moved very far in that direction.
[00:27:44.880 --> 00:27:50.480]   Average number of pitches thrown by starting pitchers down a lot over the last 5-10 years.
[00:27:54.160 --> 00:27:55.760]   How do I use that in my company?
[00:27:55.760 --> 00:27:57.680]   There's a metaphor here,
[00:27:57.680 --> 00:27:59.840]   but I actually think I've gone the opposite direction, if anything.
[00:27:59.840 --> 00:28:04.480]   Here's what my sense has been in terms of
[00:28:04.480 --> 00:28:11.360]   computer work instead of physical work,
[00:28:11.360 --> 00:28:15.200]   is that you don't have the same effect whereby
[00:28:15.200 --> 00:28:21.520]   your arm is getting sore and eventually your muscle snaps and you need surgery
[00:28:21.520 --> 00:28:23.360]   if you pitch too hard for two.
[00:28:23.360 --> 00:28:25.920]   That doesn't directly translate.
[00:28:25.920 --> 00:28:27.360]   There's a little bit of an equivalent of this,
[00:28:27.360 --> 00:28:29.680]   of people getting tired and exhausted.
[00:28:29.680 --> 00:28:34.880]   But on the other hand, context is a huge, huge piece of being effective.
[00:28:34.880 --> 00:28:37.440]   Having all the context in your mind of what's going on,
[00:28:37.440 --> 00:28:39.680]   of what you're working on, what the company's doing,
[00:28:39.680 --> 00:28:42.400]   makes it way easier to operate effectively.
[00:28:42.400 --> 00:28:47.200]   If you could, for instance, have two half-time employees or one full-time employee,
[00:28:47.200 --> 00:28:50.160]   you're way better off with one full-time employee
[00:28:50.160 --> 00:28:52.000]   because they're going to have way more context
[00:28:52.000 --> 00:28:54.320]   than either of the part-time employees would have
[00:28:54.320 --> 00:28:56.160]   and thus be able to work way more efficiently.
[00:28:56.160 --> 00:28:59.840]   In general, I think our experience has actually been
[00:28:59.840 --> 00:29:03.120]   that concentrated work is pretty valuable
[00:29:03.120 --> 00:29:06.560]   and that if you keep breaking up your work,
[00:29:06.560 --> 00:29:08.560]   and it depends on the person and the context,
[00:29:08.560 --> 00:29:10.800]   but in general, if you do that,
[00:29:10.800 --> 00:29:13.200]   you're never going to be able to do as great of work
[00:29:13.200 --> 00:29:15.120]   as if you really dove into something.
[00:29:15.120 --> 00:29:19.360]   So you've talked about how they experience relatively little
[00:29:19.360 --> 00:29:20.880]   when you're deciding who to hire,
[00:29:20.880 --> 00:29:24.080]   but in a recent Twitter thread, you mentioned that mentorship is,
[00:29:24.080 --> 00:29:26.640]   or being able to provide mentorship to all the people who come on,
[00:29:26.640 --> 00:29:28.800]   that's one of the bottlenecks to you being able to scale.
[00:29:28.800 --> 00:29:30.720]   Is there a trade-off here
[00:29:30.720 --> 00:29:33.520]   where if you don't hire people for experience
[00:29:33.520 --> 00:29:35.920]   and you have to give them more mentorship and thus can't scale as fast?
[00:29:35.920 --> 00:29:38.880]   It's a good question, but to a surprising extent,
[00:29:38.880 --> 00:29:41.920]   we found that the experience of the people that we hire
[00:29:41.920 --> 00:29:46.480]   has not that much correlation with how much mentorship they need.
[00:29:46.480 --> 00:29:50.400]   That much more important is how they think,
[00:29:50.400 --> 00:29:53.760]   how good they are at understanding new and different situations,
[00:29:53.760 --> 00:29:57.600]   and how good they are and how hard they try
[00:29:57.600 --> 00:30:02.320]   to integrate into their understanding of, let's say, coding,
[00:30:02.320 --> 00:30:04.240]   their understanding of how FTX works.
[00:30:04.240 --> 00:30:08.640]   And so I think that we actually have, by and large,
[00:30:08.640 --> 00:30:11.280]   found that other things are much better predictors
[00:30:11.280 --> 00:30:14.320]   of how much oversight management and mentorship
[00:30:14.320 --> 00:30:17.520]   someone is going to need than their experience
[00:30:17.520 --> 00:30:19.600]   at sort of similar-looking roles.
[00:30:19.600 --> 00:30:22.240]   And how do you assess that, short of hiring them for a month
[00:30:22.240 --> 00:30:23.520]   and then seeing how they did?
[00:30:23.520 --> 00:30:25.680]   It's tough. I don't think we're perfect at it.
[00:30:25.680 --> 00:30:30.400]   But things that we look at, do they understand quickly
[00:30:30.400 --> 00:30:32.000]   what the goal of a product is?
[00:30:32.000 --> 00:30:35.360]   And how does that inform how they build it?
[00:30:35.360 --> 00:30:39.440]   When you're looking at developers, I think we really strongly want people
[00:30:39.440 --> 00:30:41.920]   who can understand what FTX is, how it works,
[00:30:41.920 --> 00:30:45.440]   and thus what the right way to architect things would be for that,
[00:30:45.440 --> 00:30:50.720]   rather than treating it as an abstract engineering problem
[00:30:50.720 --> 00:30:53.360]   divorced from whatever the ultimate product is.
[00:30:53.360 --> 00:30:56.800]   So being able to-- and that's something that you can try and ask people.
[00:30:56.800 --> 00:31:01.600]   Like, hey, here's a high-level customer experience or customer goal, right?
[00:31:01.600 --> 00:31:04.080]   How would you architect a system to create that?
[00:31:04.080 --> 00:31:07.520]   So that's one thing that we look for,
[00:31:07.520 --> 00:31:11.440]   just an eagerness to learn and to adapt.
[00:31:12.400 --> 00:31:14.960]   It's not trivial to test for that, but you can do some amount of that.
[00:31:14.960 --> 00:31:16.800]   You can try and give people novel scenarios
[00:31:16.800 --> 00:31:19.840]   and see how much they break versus how much they bend.
[00:31:19.840 --> 00:31:22.320]   And I think that can be super valuable as well.
[00:31:22.320 --> 00:31:29.520]   And also, specifically searching for developers
[00:31:29.520 --> 00:31:34.480]   who are willing to deal with messy scenarios
[00:31:34.480 --> 00:31:38.480]   rather than wanting a pristine world to work in.
[00:31:39.360 --> 00:31:41.440]   Because our company, it's customer-facing.
[00:31:41.440 --> 00:31:44.080]   It has to face some third-party tooling.
[00:31:44.080 --> 00:31:45.920]   We've been a quickly growing company.
[00:31:45.920 --> 00:31:49.920]   All of those things mean that we have to interface
[00:31:49.920 --> 00:31:52.640]   with things that are messy in the way the world is.
[00:31:52.640 --> 00:31:56.320]   Now, before you launched FTX, you gave detailed instructions
[00:31:56.320 --> 00:31:59.120]   to the existing exchanges about how to improve their system,
[00:31:59.120 --> 00:32:00.960]   how to remove clawbacks, and so on.
[00:32:00.960 --> 00:32:03.680]   Looking back, they left billions of dollars of value on the table.
[00:32:03.680 --> 00:32:04.320]   Why do you think that was?
[00:32:04.320 --> 00:32:06.000]   Why didn't they just fix what you told them to fix?
[00:32:06.800 --> 00:32:10.160]   Yeah, it's a really interesting question.
[00:32:10.160 --> 00:32:17.760]   And my sense is that it's part of a larger phenomenon where--
[00:32:17.760 --> 00:32:20.480]   that's the right way to put it.
[00:32:20.480 --> 00:32:24.240]   OK, one piece of this is just they didn't have a lot
[00:32:24.240 --> 00:32:26.320]   of market director experts.
[00:32:26.320 --> 00:32:29.920]   They just did not have the talent in-house to be able
[00:32:29.920 --> 00:32:33.680]   to think really well and deeply about risk engines.
[00:32:35.920 --> 00:32:38.480]   And also, there are cultural barriers between myself
[00:32:38.480 --> 00:32:40.400]   and some of them, which I think probably meant
[00:32:40.400 --> 00:32:43.520]   that they were less inclined than they otherwise
[00:32:43.520 --> 00:32:45.840]   would have been to take it very seriously.
[00:32:45.840 --> 00:32:49.440]   But ignoring those factors, I think
[00:32:49.440 --> 00:32:51.520]   there's something much bigger at play there,
[00:32:51.520 --> 00:32:55.440]   where many of these exchanges had hired a lot of people.
[00:32:55.440 --> 00:32:56.560]   They'd gotten very large.
[00:32:56.560 --> 00:33:01.360]   And you might think that that meant that they got more able
[00:33:01.360 --> 00:33:04.880]   to do things because they had more horsepower.
[00:33:05.440 --> 00:33:07.280]   But in practice, most of the times
[00:33:07.280 --> 00:33:09.920]   that we see a company grow really fast, really quickly,
[00:33:09.920 --> 00:33:13.200]   and get really big in terms of number of people,
[00:33:13.200 --> 00:33:15.840]   it becomes an absolute mess internally.
[00:33:15.840 --> 00:33:19.120]   There's huge diffusion of responsibility issues.
[00:33:19.120 --> 00:33:20.560]   No one's really taking charge.
[00:33:20.560 --> 00:33:22.960]   You can't figure out who's supposed to do what.
[00:33:22.960 --> 00:33:26.320]   And in the end, nothing gets done.
[00:33:26.320 --> 00:33:29.600]   And you actually start hitting negative marginal utility
[00:33:29.600 --> 00:33:32.880]   of employees pretty quickly, where the more people you have,
[00:33:32.880 --> 00:33:34.400]   the less total you get done.
[00:33:34.400 --> 00:33:35.840]   I think that happened to a number of them
[00:33:35.840 --> 00:33:38.640]   to the point where, yeah, I sent them these proposals.
[00:33:38.640 --> 00:33:39.760]   Where did they go internally?
[00:33:39.760 --> 00:33:40.320]   Who knows?
[00:33:40.320 --> 00:33:47.680]   The vice president of exchange risk operations,
[00:33:47.680 --> 00:33:51.360]   but not the real one, the fake one operating
[00:33:51.360 --> 00:33:54.720]   under some department with an unclear goal and mission
[00:33:54.720 --> 00:33:57.840]   or something like that, who had no idea what to do with it.
[00:33:57.840 --> 00:34:01.520]   And eventually, just passed it off to a random friend of hers
[00:34:01.520 --> 00:34:05.200]   that she knew who was a developer for the mobile app
[00:34:05.200 --> 00:34:07.120]   and was like, you're a computer person.
[00:34:07.120 --> 00:34:08.320]   Is this right?
[00:34:08.320 --> 00:34:09.520]   And it's sort of like, I have no idea.
[00:34:09.520 --> 00:34:10.800]   I'm not a risk person.
[00:34:10.800 --> 00:34:11.840]   And that's how it died.
[00:34:11.840 --> 00:34:13.440]   And I'm not saying it was literally that happened,
[00:34:13.440 --> 00:34:16.080]   but something sounds kind of like that probably happened,
[00:34:16.080 --> 00:34:21.680]   where it's not like they had people who took responsibility.
[00:34:21.680 --> 00:34:23.120]   They saw this, like, wow, this is scary.
[00:34:23.120 --> 00:34:25.440]   I should make sure that the best person in the company
[00:34:25.440 --> 00:34:28.480]   gets this and pass it to the TTO and person
[00:34:28.480 --> 00:34:30.080]   who thinks about their modeling.
[00:34:30.080 --> 00:34:32.080]   And said like, hey, is this thing scary?
[00:34:32.080 --> 00:34:34.080]   And they looked at it like, wow, this might be a problem.
[00:34:34.080 --> 00:34:35.200]   I don't think that's what happened.
[00:34:35.200 --> 00:34:37.840]   Now, there's two ways of thinking about the impact
[00:34:37.840 --> 00:34:40.320]   of crypto on financial innovation.
[00:34:40.320 --> 00:34:44.320]   One is the crypto maximalist view that crypto subsumes Stratify.
[00:34:44.320 --> 00:34:47.360]   The other is that what you're basically doing
[00:34:47.360 --> 00:34:52.480]   is you're stress testing some ideas from a volatile,
[00:34:52.480 --> 00:34:54.240]   fairly unregulated market that you're actually
[00:34:54.240 --> 00:34:55.600]   going to bring to Stratify.
[00:34:55.600 --> 00:34:57.200]   But this is not going to lead to some sort
[00:34:57.200 --> 00:34:58.320]   of decentralized utopia.
[00:34:59.200 --> 00:35:00.880]   So which of these models is more correct?
[00:35:00.880 --> 00:35:02.960]   Or is there a third model that you think is the correct way?
[00:35:02.960 --> 00:35:04.320]   So first of all, who knows, right?
[00:35:04.320 --> 00:35:06.880]   Like, I mean, who knows exactly what's going to happen?
[00:35:06.880 --> 00:35:07.840]   It's going to be path dependent.
[00:35:07.840 --> 00:35:12.800]   But if I had to guess, I would say a lot of properties
[00:35:12.800 --> 00:35:14.320]   of what is happening in crypto today
[00:35:14.320 --> 00:35:16.480]   will probably make their way into Stratify to some extent.
[00:35:16.480 --> 00:35:19.440]   I think blockchain settlement has a lot of value
[00:35:19.440 --> 00:35:22.080]   and can clean up a lot of areas of traditional market structure.
[00:35:22.080 --> 00:35:27.440]   And I think that composable applications
[00:35:27.440 --> 00:35:32.320]   are super valuable and are going to get more important over time.
[00:35:32.320 --> 00:35:34.640]   I think there are some areas of this where it's not clear
[00:35:34.640 --> 00:35:35.760]   what's going to happen.
[00:35:35.760 --> 00:35:37.200]   And I think that when you think about
[00:35:37.200 --> 00:35:42.080]   how do decentralized ecosystems and regulation intersect,
[00:35:42.080 --> 00:35:45.680]   it's a little bit TBD, exactly where that ends up.
[00:35:45.680 --> 00:35:51.280]   And so I don't want to state with extreme confidence
[00:35:51.280 --> 00:35:54.320]   exactly what will or won't happen.
[00:35:54.320 --> 00:35:57.840]   But I think some piece of this seem pretty likely to me.
[00:35:57.840 --> 00:35:59.520]   I think stablecoins becoming an important
[00:35:59.520 --> 00:36:02.880]   settlement mechanism is pretty likely.
[00:36:02.880 --> 00:36:04.560]   And I think blockchains in general becoming
[00:36:04.560 --> 00:36:08.320]   a settlement mechanism and collateral clearing mechanism
[00:36:08.320 --> 00:36:09.680]   seems decently likely to me.
[00:36:09.680 --> 00:36:13.600]   And more and more assets getting tokenized
[00:36:13.600 --> 00:36:14.880]   seems decently likely to me.
[00:36:14.880 --> 00:36:20.000]   And there being programs written on blockchains
[00:36:20.000 --> 00:36:24.000]   that people can add to that can compose with each other
[00:36:24.000 --> 00:36:25.040]   seems pretty likely to me.
[00:36:25.040 --> 00:36:30.160]   And a lot of other areas of it, I think could go either way.
[00:36:30.160 --> 00:36:33.520]   Let's talk about your proposal to the CFTC
[00:36:33.520 --> 00:36:35.200]   to replace futures commission merchants
[00:36:35.200 --> 00:36:37.840]   with algorithmic real-time risk management.
[00:36:37.840 --> 00:36:41.360]   There's a worry that without human discretion,
[00:36:41.360 --> 00:36:45.760]   you have algorithms that will cause liquidation cascades
[00:36:45.760 --> 00:36:46.960]   when they were not necessary.
[00:36:46.960 --> 00:36:48.960]   Is there some role for human discretion
[00:36:48.960 --> 00:36:51.200]   in these kinds of situations?
[00:36:51.200 --> 00:36:51.680]   There is.
[00:36:51.680 --> 00:36:53.600]   And the way I think about it is you have--
[00:36:53.600 --> 00:36:57.120]   the way that traditional future market structure works
[00:36:57.120 --> 00:37:00.160]   is you have a clearinghouse with a decent amount
[00:37:00.160 --> 00:37:04.320]   of manual discretion in it connected to FCMs,
[00:37:04.320 --> 00:37:07.360]   some of which use human discretion
[00:37:07.360 --> 00:37:10.560]   and some of which use automated risk management algorithms
[00:37:10.560 --> 00:37:11.360]   with their clients.
[00:37:11.360 --> 00:37:14.160]   And generally, the smaller the client,
[00:37:14.160 --> 00:37:15.360]   the more automated it is.
[00:37:15.360 --> 00:37:17.760]   We are inverting that to some extent
[00:37:17.760 --> 00:37:21.600]   where at the center, you have an automated clearinghouse
[00:37:21.600 --> 00:37:23.280]   that's then connected to--
[00:37:23.280 --> 00:37:25.280]   potentially connected to FCMs,
[00:37:25.280 --> 00:37:29.920]   which could potentially use discretionary systems
[00:37:29.920 --> 00:37:31.280]   when managing their clients.
[00:37:31.280 --> 00:37:36.400]   The key difference here is that one way or another,
[00:37:36.400 --> 00:37:40.000]   initial margin has to end up at the clearinghouse,
[00:37:40.000 --> 00:37:41.840]   a programmatic amount of it.
[00:37:41.840 --> 00:37:44.000]   And the clearinghouse acts in a clear way.
[00:37:44.000 --> 00:37:46.480]   And the goal of this is, first of all,
[00:37:46.480 --> 00:37:49.680]   to prevent contagion between different intermediaries.
[00:37:49.680 --> 00:37:51.760]   So whatever decisions, whatever credit decisions,
[00:37:51.760 --> 00:37:54.640]   one intermediary makes with respect to their customers
[00:37:54.640 --> 00:37:58.720]   doesn't pose risk to other intermediaries
[00:37:58.720 --> 00:38:00.560]   because someone has to pose the collaterals
[00:38:00.560 --> 00:38:01.840]   of the clearinghouse in the end,
[00:38:01.840 --> 00:38:05.600]   whether it's the FCM, their customer, or someone else.
[00:38:05.600 --> 00:38:09.120]   And so it gives clear rules of the road
[00:38:09.120 --> 00:38:12.000]   and lack of systemic risk spreading
[00:38:12.000 --> 00:38:14.240]   throughout the system and contains risk
[00:38:14.240 --> 00:38:16.160]   to the parties that choose to take that risk on.
[00:38:17.600 --> 00:38:21.440]   It's the FCMs that choose to make credit decisions there.
[00:38:21.440 --> 00:38:24.320]   So I think that there is a potential role
[00:38:24.320 --> 00:38:26.800]   for manual judgment.
[00:38:26.800 --> 00:38:30.960]   But manual judgment, it can be really valuable
[00:38:30.960 --> 00:38:32.320]   and add a lot of economic value.
[00:38:32.320 --> 00:38:34.400]   It can also be very risky when done poorly.
[00:38:34.400 --> 00:38:39.440]   And I think that in the current system,
[00:38:39.440 --> 00:38:42.960]   each FCM is exposed to all of the manual bespoke decisions
[00:38:42.960 --> 00:38:44.960]   that each other FCM is making.
[00:38:45.600 --> 00:38:47.520]   And that's a really scary place to be in.
[00:38:47.520 --> 00:38:48.400]   And we've seen it blow up.
[00:38:48.400 --> 00:38:50.560]   We saw it blow up with Elmi Nickel contracts.
[00:38:50.560 --> 00:38:53.360]   And we saw it blow up in other cases
[00:38:53.360 --> 00:38:56.480]   with a few very large traders
[00:38:56.480 --> 00:39:00.320]   who had positions on at a number of different banks
[00:39:00.320 --> 00:39:02.160]   and ended up blowing out.
[00:39:02.160 --> 00:39:06.720]   So I think that this provides a level of clarity
[00:39:06.720 --> 00:39:10.320]   and oversight and transparency to the system
[00:39:10.320 --> 00:39:13.200]   so that people know what risk they are or are not taking on.
[00:39:13.760 --> 00:39:15.680]   Are you replacing that risk with another risk,
[00:39:15.680 --> 00:39:18.240]   which is that if there's one exchange
[00:39:18.240 --> 00:39:19.920]   that has the most liquidity in futures,
[00:39:19.920 --> 00:39:22.240]   and if there's one exchange
[00:39:22.240 --> 00:39:23.600]   where you're posting all your collateral,
[00:39:23.600 --> 00:39:25.360]   so across all your positions,
[00:39:25.360 --> 00:39:27.920]   then the risk is that that single algorithm
[00:39:27.920 --> 00:39:30.080]   that the exchange is using is going to determine
[00:39:30.080 --> 00:39:32.080]   when and if liquidation cascades happen.
[00:39:32.080 --> 00:39:33.600]   So it's already the case that
[00:39:33.600 --> 00:39:35.600]   if you put all of your collateral with a prime broker,
[00:39:35.600 --> 00:39:40.000]   then potentially whatever that prime broker decides,
[00:39:40.000 --> 00:39:42.240]   whether it's an algorithm or a human
[00:39:42.240 --> 00:39:44.240]   or something in between,
[00:39:44.240 --> 00:39:45.360]   is going to decide what happens
[00:39:45.360 --> 00:39:46.720]   with all of your collateral.
[00:39:46.720 --> 00:39:48.320]   And if you're not comfortable with that,
[00:39:48.320 --> 00:39:49.440]   you could choose to spread it out
[00:39:49.440 --> 00:39:50.480]   between different venues.
[00:39:50.480 --> 00:39:52.320]   You could choose to use one venue for some products,
[00:39:52.320 --> 00:39:53.520]   another venue for other products
[00:39:53.520 --> 00:39:55.920]   if you don't want to cross-collateralize,
[00:39:55.920 --> 00:39:57.520]   cross-margin your positions.
[00:39:57.520 --> 00:39:59.280]   You get capital efficiency generally
[00:39:59.280 --> 00:40:00.720]   for cross-margining them,
[00:40:00.720 --> 00:40:02.720]   for putting them in the same place.
[00:40:02.720 --> 00:40:05.280]   But the downside of that is that
[00:40:05.280 --> 00:40:08.160]   the risk of one can affect the other one.
[00:40:08.160 --> 00:40:10.400]   There's a balance there.
[00:40:10.400 --> 00:40:12.160]   And I don't think it's a binary thing.
[00:40:12.160 --> 00:40:13.220]   Okay.
[00:40:13.220 --> 00:40:15.520]   But given the benefits of cross-margining
[00:40:15.520 --> 00:40:16.560]   and the fact that less capital
[00:40:16.560 --> 00:40:18.000]   has to be locked up as collateral,
[00:40:18.000 --> 00:40:19.200]   is the long-run equilibrium
[00:40:19.200 --> 00:40:20.880]   that the single exchange will win?
[00:40:20.880 --> 00:40:21.760]   And if that's the case,
[00:40:21.760 --> 00:40:22.720]   then in the long run,
[00:40:22.720 --> 00:40:24.800]   there won't be that much competition in derivatives?
[00:40:24.800 --> 00:40:26.080]   I don't think it.
[00:40:26.080 --> 00:40:28.000]   I mean, you already could see that happening.
[00:40:28.000 --> 00:40:29.200]   You haven't.
[00:40:29.200 --> 00:40:30.160]   And I don't think we're going to have
[00:40:30.160 --> 00:40:31.200]   a single exchange winning.
[00:40:31.200 --> 00:40:32.880]   Among other things,
[00:40:32.880 --> 00:40:35.760]   I think there are going to be different decisions
[00:40:35.760 --> 00:40:36.800]   made by different exchanges,
[00:40:36.800 --> 00:40:38.320]   which will be better or worse
[00:40:38.320 --> 00:40:39.600]   for particular situations.
[00:40:39.600 --> 00:40:41.840]   And I think one thing that people have brought up is,
[00:40:41.840 --> 00:40:43.600]   well, how about for physical commodities,
[00:40:43.600 --> 00:40:46.000]   like corn or soy?
[00:40:46.000 --> 00:40:51.360]   What would our risk model say about that?
[00:40:51.360 --> 00:40:53.920]   And the answer is it's not super helpful
[00:40:53.920 --> 00:40:55.040]   for those commodities right now
[00:40:55.040 --> 00:40:56.640]   because it doesn't know how to understand a warehouse.
[00:40:56.640 --> 00:40:59.600]   And so you might want to use a different exchange,
[00:40:59.600 --> 00:41:02.880]   which had a more bespoke risk model
[00:41:02.880 --> 00:41:05.040]   that tried to understand,
[00:41:05.040 --> 00:41:06.080]   have a human understand
[00:41:06.080 --> 00:41:08.480]   what physical positions someone had on.
[00:41:09.360 --> 00:41:10.800]   I think that would totally make sense.
[00:41:10.800 --> 00:41:12.640]   And that can cause a sort of split
[00:41:12.640 --> 00:41:14.960]   between different exchanges.
[00:41:14.960 --> 00:41:17.760]   In addition, we've been talking about
[00:41:17.760 --> 00:41:18.640]   the clearinghouse here,
[00:41:18.640 --> 00:41:20.240]   but many exchanges can connect
[00:41:20.240 --> 00:41:21.440]   to the same clearinghouse.
[00:41:21.440 --> 00:41:25.120]   And we're already, as a clearinghouse,
[00:41:25.120 --> 00:41:27.680]   connected to a number of different DCMs.
[00:41:27.680 --> 00:41:31.280]   And so excited for that to continue to grow out.
[00:41:31.280 --> 00:41:32.800]   And in general,
[00:41:32.800 --> 00:41:33.840]   there are going to be a lot of people
[00:41:33.840 --> 00:41:35.040]   who have different preferences
[00:41:35.040 --> 00:41:37.280]   over different details of the system
[00:41:37.280 --> 00:41:39.120]   and choose different products based on that.
[00:41:39.120 --> 00:41:41.040]   I think that's how it should work.
[00:41:41.040 --> 00:41:43.520]   And that people should be allowed to choose
[00:41:43.520 --> 00:41:45.360]   the option that makes the most sense for them.
[00:41:45.360 --> 00:41:46.720]   What are the biggest differences in culture
[00:41:46.720 --> 00:41:48.400]   between Jane Street and FTX?
[00:41:48.400 --> 00:41:53.200]   I think FTX has much more of a culture
[00:41:53.200 --> 00:41:56.880]   of morphing and taking on
[00:41:56.880 --> 00:41:58.160]   a lot of random new shit.
[00:41:58.160 --> 00:42:00.400]   And Jane Street has, it's still like,
[00:42:00.400 --> 00:42:01.200]   I don't want to say it's like
[00:42:01.200 --> 00:42:02.400]   an ossified place or anything,
[00:42:02.400 --> 00:42:03.840]   like it is somewhat nimble,
[00:42:03.840 --> 00:42:05.680]   but it is more of a culture of like,
[00:42:05.680 --> 00:42:07.600]   we're going to be very good
[00:42:07.600 --> 00:42:09.040]   at this particular thing
[00:42:09.040 --> 00:42:11.040]   on a timescale of a decade.
[00:42:11.040 --> 00:42:12.880]   And there are some cases
[00:42:12.880 --> 00:42:13.920]   where that's true with FTX,
[00:42:13.920 --> 00:42:15.360]   because some things are clearly part of our
[00:42:15.360 --> 00:42:17.440]   core business for a decade.
[00:42:17.440 --> 00:42:18.880]   But there are other things that like,
[00:42:18.880 --> 00:42:21.280]   we knew nothing about a year ago,
[00:42:21.280 --> 00:42:23.360]   and all of a sudden we have to get good at.
[00:42:23.360 --> 00:42:28.160]   And so I think that there's been more adaptation
[00:42:28.160 --> 00:42:33.200]   and it's also a much more public facing
[00:42:33.200 --> 00:42:35.600]   and customer facing business than Jane Street is,
[00:42:35.600 --> 00:42:37.360]   which means that there are lots of things like PR.
[00:42:37.840 --> 00:42:39.920]   That are much more service central
[00:42:39.920 --> 00:42:41.120]   to what we're doing.
[00:42:41.120 --> 00:42:42.640]   Now, in crypto, you're combining
[00:42:42.640 --> 00:42:44.080]   the exchange and the broker.
[00:42:44.080 --> 00:42:45.440]   They seem to have different incentives.
[00:42:45.440 --> 00:42:47.440]   The exchange wants to increase volume.
[00:42:47.440 --> 00:42:49.520]   The broker wants to better manage risk,
[00:42:49.520 --> 00:42:50.480]   maybe with less leverage.
[00:42:50.480 --> 00:42:52.960]   Do you feel that in the long run,
[00:42:52.960 --> 00:42:55.280]   these two can stay in the same entity
[00:42:55.280 --> 00:42:56.240]   given the conflict of interest
[00:42:56.240 --> 00:42:57.520]   or potential conflict of interest?
[00:42:57.520 --> 00:42:58.160]   I think so.
[00:42:58.160 --> 00:43:01.920]   And I think that there is like,
[00:43:01.920 --> 00:43:03.200]   some extent to which they differ,
[00:43:03.200 --> 00:43:04.560]   but there are more extents
[00:43:04.560 --> 00:43:06.720]   to which they actually want the same thing.
[00:43:06.720 --> 00:43:08.560]   And harmonizing them can be really valuable.
[00:43:08.560 --> 00:43:11.120]   And one is to provide a great customer experience.
[00:43:11.120 --> 00:43:13.520]   And when you have two different entities
[00:43:13.520 --> 00:43:15.520]   with two completely different businesses,
[00:43:15.520 --> 00:43:18.000]   but that half, every order has to go
[00:43:18.000 --> 00:43:19.840]   from one to the other, right?
[00:43:19.840 --> 00:43:21.040]   You're going to end up getting sort of like,
[00:43:21.040 --> 00:43:23.360]   the least common denominator
[00:43:23.360 --> 00:43:24.320]   of the two as a customer.
[00:43:24.320 --> 00:43:25.760]   You're going to get only things that are,
[00:43:25.760 --> 00:43:29.040]   everything is going to be supported as poorly
[00:43:29.040 --> 00:43:30.560]   as whichever of the two entities
[00:43:30.560 --> 00:43:33.040]   support what you're doing most poorly.
[00:43:33.040 --> 00:43:34.320]   And that makes it harder.
[00:43:34.320 --> 00:43:37.520]   Whereas, by synchronizing them,
[00:43:37.520 --> 00:43:40.640]   it gives us much more ability
[00:43:40.640 --> 00:43:43.840]   to provide a great experience on that.
[00:43:43.840 --> 00:43:48.640]   How has living in the Bahamas impacted your opinion
[00:43:48.640 --> 00:43:49.360]   about the possibility
[00:43:49.360 --> 00:43:50.400]   of successful charter cities?
[00:43:50.400 --> 00:43:52.160]   It's a good question.
[00:43:52.160 --> 00:43:53.840]   I think it's, I mean, it's the first time,
[00:43:53.840 --> 00:43:55.760]   I think it's updated positively a little bit.
[00:43:55.760 --> 00:43:58.000]   I think we've built out a lot of things here
[00:43:58.000 --> 00:44:00.160]   and that's been hopefully impactful.
[00:44:00.160 --> 00:44:02.640]   And I think it's made me feel like
[00:44:02.640 --> 00:44:05.520]   it is more doable than I previously would have thought.
[00:44:05.520 --> 00:44:07.520]   But it's also, it's a lot of work.
[00:44:07.520 --> 00:44:10.320]   Like, you know, it's a large scale project.
[00:44:10.320 --> 00:44:11.120]   If you want to actually,
[00:44:11.120 --> 00:44:12.640]   and we have not built out a full city.
[00:44:12.640 --> 00:44:15.040]   Like we've built out some specific pieces
[00:44:15.040 --> 00:44:16.480]   of infrastructure that we needed.
[00:44:16.480 --> 00:44:18.720]   We've gotten a ton of support from the country.
[00:44:18.720 --> 00:44:20.240]   And, you know, they've been very welcoming
[00:44:20.240 --> 00:44:21.520]   and there are a lot of great things here.
[00:44:21.520 --> 00:44:25.120]   And so this is way less of a project
[00:44:25.120 --> 00:44:27.840]   than just taking in a giant empty plot of land
[00:44:27.840 --> 00:44:30.080]   and creating a city in it.
[00:44:30.080 --> 00:44:32.240]   That's way harder.
[00:44:32.240 --> 00:44:34.400]   How has having a RAM skewed mind
[00:44:34.400 --> 00:44:37.280]   influenced the culture of FTX and its growth?
[00:44:37.280 --> 00:44:38.240]   It's a good question.
[00:44:38.240 --> 00:44:41.680]   And, you know, I think that what it means on the upside
[00:44:41.680 --> 00:44:44.480]   is that we've been sort of like pretty good at adapting
[00:44:44.480 --> 00:44:45.840]   and pretty good at understanding
[00:44:45.840 --> 00:44:47.760]   what the important things are at any time.
[00:44:47.760 --> 00:44:50.880]   And that, you know, training ourselves quickly
[00:44:50.880 --> 00:44:53.040]   to be good at those, you know,
[00:44:53.040 --> 00:44:54.480]   even if it looks very different
[00:44:54.480 --> 00:44:57.040]   than what we were doing, you know, before.
[00:44:57.040 --> 00:45:00.160]   And I think that, you know, that's allowed us to,
[00:45:00.160 --> 00:45:01.600]   you know, focus a lot on product
[00:45:01.600 --> 00:45:04.320]   to focus a lot on regulation and licensing,
[00:45:04.320 --> 00:45:06.720]   to focus a lot on customer experience,
[00:45:06.720 --> 00:45:08.960]   on branding and a bunch of other things.
[00:45:08.960 --> 00:45:13.840]   And I think hopefully it means that we're able
[00:45:13.840 --> 00:45:16.000]   to sort of like take whatever situations come up
[00:45:16.000 --> 00:45:19.280]   and provide sort of like reasonable feedback about them
[00:45:19.280 --> 00:45:20.560]   and reasonable thoughts on what to do,
[00:45:20.560 --> 00:45:24.480]   you know, rather than sort of like thinking more rigidly
[00:45:24.480 --> 00:45:28.480]   in terms of how, you know, previous situations were.
[00:45:29.120 --> 00:45:32.800]   On the flip side, you know, I think that it means that,
[00:45:32.800 --> 00:45:35.280]   you know, I have to have a lot of people around me
[00:45:35.280 --> 00:45:38.240]   who will try and remember what the most, you know,
[00:45:38.240 --> 00:45:40.240]   what the sort of like long-term important things are
[00:45:40.240 --> 00:45:42.720]   that might get lost day to day, you know,
[00:45:42.720 --> 00:45:45.840]   as we focus on, you know, things that pop up.
[00:45:45.840 --> 00:45:47.440]   And, you know, it's important for me
[00:45:47.440 --> 00:45:49.920]   to take time periodically to step back
[00:45:49.920 --> 00:45:52.720]   and, you know, clear my mind a little bit
[00:45:52.720 --> 00:45:55.360]   and just think like, all right, let's try
[00:45:55.360 --> 00:45:57.120]   and just remember what the big picture is here.
[00:45:57.760 --> 00:46:09.920]   What are the most important things for us to be focusing on?

