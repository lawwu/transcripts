
[00:00:00.000 --> 00:00:03.240]   The following is a conversation with Dmitry Dolgov,
[00:00:03.240 --> 00:00:05.240]   the CTO of Waymo,
[00:00:05.240 --> 00:00:07.100]   which is an autonomous driving company
[00:00:07.100 --> 00:00:11.000]   that started as Google's self-driving car project in 2009
[00:00:11.000 --> 00:00:13.960]   and became Waymo in 2016.
[00:00:13.960 --> 00:00:16.200]   Dmitry was there all along.
[00:00:16.200 --> 00:00:17.260]   Waymo is currently leading
[00:00:17.260 --> 00:00:19.480]   in the fully autonomous vehicle space
[00:00:19.480 --> 00:00:23.000]   in that they actually have an at-scale deployment
[00:00:23.000 --> 00:00:25.820]   of publicly accessible autonomous vehicles
[00:00:25.820 --> 00:00:30.280]   driving passengers around with no safety driver,
[00:00:30.280 --> 00:00:32.940]   with nobody in the driver's seat.
[00:00:32.940 --> 00:00:37.360]   This, to me, is an incredible accomplishment of engineering
[00:00:37.360 --> 00:00:38.900]   on one of the most difficult
[00:00:38.900 --> 00:00:41.540]   and exciting artificial intelligence challenges
[00:00:41.540 --> 00:00:43.560]   of the 21st century.
[00:00:43.560 --> 00:00:45.080]   Quick mention of a sponsor
[00:00:45.080 --> 00:00:47.800]   followed by some thoughts related to the episode.
[00:00:47.800 --> 00:00:49.640]   Thank you to Trial Labs,
[00:00:49.640 --> 00:00:52.760]   a company that helps businesses apply machine learning
[00:00:52.760 --> 00:00:55.360]   to solve real-world problems.
[00:00:55.360 --> 00:00:57.480]   Blinkist, an app I use for reading
[00:00:57.480 --> 00:00:59.240]   through summaries of books.
[00:00:59.240 --> 00:01:01.040]   BetterHelp, online therapy
[00:01:01.040 --> 00:01:02.620]   with a licensed professional.
[00:01:02.620 --> 00:01:05.920]   And Cash App, the app I use to send money to friends.
[00:01:05.920 --> 00:01:08.400]   Please check out the sponsors in the description
[00:01:08.400 --> 00:01:11.160]   to get a discount at the support of this podcast.
[00:01:11.160 --> 00:01:13.780]   As a side note, let me say that autonomous
[00:01:13.780 --> 00:01:16.320]   and semi-autonomous driving was the focus
[00:01:16.320 --> 00:01:19.120]   of my work at MIT and is a problem space
[00:01:19.120 --> 00:01:22.240]   that I find fascinating and full of open questions
[00:01:22.240 --> 00:01:26.360]   from both a robotics and a human psychology perspective.
[00:01:26.360 --> 00:01:28.760]   There's quite a bit that I could say here
[00:01:28.760 --> 00:01:32.040]   about my experiences in academia on this topic
[00:01:32.040 --> 00:01:34.360]   that revealed to me, let's say,
[00:01:34.360 --> 00:01:38.000]   the less admirable sides of human beings.
[00:01:38.000 --> 00:01:41.160]   But I choose to focus on the positive, on solutions,
[00:01:41.160 --> 00:01:44.760]   on brilliant engineers like Dmitry and the team at Waymo
[00:01:44.760 --> 00:01:46.880]   who work tirelessly to innovate
[00:01:46.880 --> 00:01:50.800]   and to build amazing technology that will define our future.
[00:01:50.800 --> 00:01:53.400]   Because of Dmitry and others like him,
[00:01:53.400 --> 00:01:55.640]   I'm excited for this future.
[00:01:55.640 --> 00:01:59.320]   And who knows, perhaps I too will help contribute
[00:01:59.320 --> 00:02:01.760]   something of value to it.
[00:02:01.760 --> 00:02:04.000]   If you enjoy this thing, subscribe on YouTube,
[00:02:04.000 --> 00:02:06.160]   review it with Five Stars and Up, a podcast,
[00:02:06.160 --> 00:02:08.680]   follow on Spotify, support on Patreon,
[00:02:08.680 --> 00:02:11.960]   or connect with me on Twitter @LexFriedman.
[00:02:11.960 --> 00:02:15.700]   And now, here's my conversation with Dmitry Dolgov.
[00:02:15.700 --> 00:02:19.640]   When did you first fall in love with robotics
[00:02:19.640 --> 00:02:21.720]   or even computer science more in general?
[00:02:21.720 --> 00:02:25.360]   - Computer science first, at a fairly young age.
[00:02:25.360 --> 00:02:27.060]   Then robotics happened much later.
[00:02:27.060 --> 00:02:33.200]   I think my first interesting introduction to computers
[00:02:33.200 --> 00:02:38.200]   was in the late '80s when we got our first computer.
[00:02:38.200 --> 00:02:44.320]   I think it was an IBM, I think IBM AT.
[00:02:44.320 --> 00:02:46.640]   Remember those things that had like a turbo button
[00:02:46.640 --> 00:02:47.480]   in the front? - Turbo button, yeah.
[00:02:47.480 --> 00:02:49.440]   - That you would press it and make the thing
[00:02:49.440 --> 00:02:50.520]   go faster.
[00:02:50.520 --> 00:02:52.480]   - Did that already have floppy disks?
[00:02:52.480 --> 00:02:53.320]   - Yeah, yeah, yeah, yeah.
[00:02:53.320 --> 00:02:57.160]   Like the 5.4 inch ones.
[00:02:57.160 --> 00:02:58.800]   - I think there was a bigger inch.
[00:02:58.800 --> 00:03:02.120]   So go on something, then five inches and three inches.
[00:03:02.120 --> 00:03:03.600]   - Yeah, I think that was the five.
[00:03:03.600 --> 00:03:05.680]   Maybe that was before that was the giant plates
[00:03:05.680 --> 00:03:07.240]   and I didn't get that.
[00:03:07.240 --> 00:03:10.080]   But it was definitely not the three inch ones.
[00:03:10.080 --> 00:03:13.400]   Anyway, so we got that computer.
[00:03:13.400 --> 00:03:18.400]   I spent the first few months just playing video games,
[00:03:18.640 --> 00:03:19.720]   as you would expect.
[00:03:19.720 --> 00:03:21.600]   I got bored of that.
[00:03:21.600 --> 00:03:25.640]   So I started messing around and trying to figure out
[00:03:25.640 --> 00:03:28.880]   how to make the thing do other stuff.
[00:03:28.880 --> 00:03:32.120]   Got into exploring programming.
[00:03:32.120 --> 00:03:36.640]   And a couple of years later, it got to a point
[00:03:36.640 --> 00:03:40.400]   where I actually wrote a game, a little game.
[00:03:40.400 --> 00:03:43.120]   And a game developer, a Japanese game developer
[00:03:43.120 --> 00:03:45.880]   actually offered to buy it for me for a few hundred bucks.
[00:03:45.880 --> 00:03:48.840]   But for a kid in Russia.
[00:03:48.840 --> 00:03:49.680]   - That's a big deal.
[00:03:49.680 --> 00:03:50.520]   - That's a big deal, yeah.
[00:03:50.520 --> 00:03:51.880]   I did not take the deal.
[00:03:51.880 --> 00:03:53.280]   - Wow, integrity.
[00:03:53.280 --> 00:03:55.000]   - Yeah, I instead--
[00:03:55.000 --> 00:03:55.840]   - Stupidity.
[00:03:55.840 --> 00:03:59.200]   - Yes, that was not the most acute financial move
[00:03:59.200 --> 00:04:00.120]   that I made in my life.
[00:04:00.120 --> 00:04:02.480]   Looking back at it now, I instead put it,
[00:04:02.480 --> 00:04:03.560]   well, I had a reason.
[00:04:03.560 --> 00:04:04.920]   I put it online.
[00:04:04.920 --> 00:04:07.680]   It was, what'd you call it back in the day?
[00:04:07.680 --> 00:04:09.040]   So it was a freeware thing, right?
[00:04:09.040 --> 00:04:11.520]   It was not open source, but you could upload the binaries,
[00:04:11.520 --> 00:04:12.480]   you would put the game online.
[00:04:12.480 --> 00:04:14.320]   And the idea was that people like it
[00:04:14.320 --> 00:04:15.800]   and then they contribute
[00:04:15.800 --> 00:04:17.120]   and they send you little donations, right?
[00:04:17.120 --> 00:04:18.800]   So I did my quick math of like,
[00:04:18.800 --> 00:04:21.200]   of course, thousands and millions of people
[00:04:21.200 --> 00:04:22.040]   are gonna play my game,
[00:04:22.040 --> 00:04:23.560]   send me a couple of bucks a piece,
[00:04:23.560 --> 00:04:25.080]   should definitely do that.
[00:04:25.080 --> 00:04:27.280]   As I said, not the best financial decision of my life.
[00:04:27.280 --> 00:04:28.840]   - You're already playing with business models
[00:04:28.840 --> 00:04:29.880]   at that young age.
[00:04:29.880 --> 00:04:31.360]   Remember what language it was?
[00:04:31.360 --> 00:04:32.200]   What programming it was?
[00:04:32.200 --> 00:04:33.720]   - Pascal.
[00:04:33.720 --> 00:04:34.560]   - Which, what?
[00:04:34.560 --> 00:04:35.400]   - Pascal.
[00:04:35.400 --> 00:04:36.240]   - Pascal.
[00:04:36.240 --> 00:04:38.080]   And it had a graphical component, so it's not text-based?
[00:04:38.080 --> 00:04:43.080]   - Yeah, it was like, I think 320 by 200,
[00:04:43.160 --> 00:04:45.120]   whatever it was, I think that kind of the earlier version.
[00:04:45.120 --> 00:04:45.960]   - That's the resolution.
[00:04:45.960 --> 00:04:47.200]   - VGA resolution, right?
[00:04:47.200 --> 00:04:49.120]   And I actually think the reason why this company
[00:04:49.120 --> 00:04:51.040]   wanted to buy it is not like the fancy graphics
[00:04:51.040 --> 00:04:52.120]   or the implementation,
[00:04:52.120 --> 00:04:55.280]   it was maybe the idea of my actual game.
[00:04:55.280 --> 00:04:57.880]   - The idea of the game, okay.
[00:04:57.880 --> 00:04:59.720]   Well, one of the things, it's so funny,
[00:04:59.720 --> 00:05:02.320]   I used to play this game called Golden Axe
[00:05:02.320 --> 00:05:05.720]   and the simplicity of the graphics
[00:05:05.720 --> 00:05:08.560]   and something about the simplicity of the music,
[00:05:08.560 --> 00:05:11.440]   like, it still haunts me.
[00:05:11.440 --> 00:05:12.760]   I don't know if that's a childhood thing,
[00:05:12.760 --> 00:05:13.800]   I don't know if that's the same thing
[00:05:13.800 --> 00:05:16.040]   for Call of Duty these days for young kids,
[00:05:16.040 --> 00:05:19.880]   but I still think that the,
[00:05:19.880 --> 00:05:21.920]   when the games are simple,
[00:05:21.920 --> 00:05:26.160]   that simple purity makes for,
[00:05:26.160 --> 00:05:28.680]   like, allows your imagination to take over
[00:05:28.680 --> 00:05:31.280]   and thereby creating a more magical experience.
[00:05:31.280 --> 00:05:33.280]   Like now with better and better graphics,
[00:05:33.280 --> 00:05:38.280]   it feels like your imagination doesn't get to create worlds,
[00:05:38.280 --> 00:05:40.360]   which is kind of interesting.
[00:05:40.360 --> 00:05:42.280]   It could be just an old man on a porch,
[00:05:42.280 --> 00:05:45.320]   like waving at kids these days that have no respect,
[00:05:45.320 --> 00:05:47.640]   but I still think that graphics
[00:05:47.640 --> 00:05:50.440]   almost get in the way of the experience.
[00:05:50.440 --> 00:05:51.320]   I don't know.
[00:05:51.320 --> 00:05:52.480]   - Flippant Bird.
[00:05:52.480 --> 00:05:54.160]   - Yeah, that's true.
[00:05:54.160 --> 00:05:56.320]   - Well, I don't know if the imagination-
[00:05:56.320 --> 00:05:57.160]   - It's closed.
[00:05:57.160 --> 00:05:58.200]   - Okay, it's closed.
[00:05:58.200 --> 00:06:01.560]   I don't, yeah, but that's more about games that,
[00:06:01.560 --> 00:06:03.160]   like, that's more like Tetris World
[00:06:03.160 --> 00:06:06.720]   where they optimally, masterfully,
[00:06:06.720 --> 00:06:11.720]   like, create a fun, short-term dopamine experience
[00:06:12.600 --> 00:06:15.680]   versus, I'm more referring to, like, role-playing games
[00:06:15.680 --> 00:06:16.760]   where there's like a story,
[00:06:16.760 --> 00:06:19.720]   you can live in it for months or years.
[00:06:19.720 --> 00:06:23.520]   Like, there's an Elder Scrolls series,
[00:06:23.520 --> 00:06:25.800]   which is probably my favorite set of games.
[00:06:25.800 --> 00:06:27.240]   That was a magical experience.
[00:06:27.240 --> 00:06:29.160]   And the graphics are terrible.
[00:06:29.160 --> 00:06:31.120]   The characters were all randomly generated,
[00:06:31.120 --> 00:06:34.400]   but they're, I don't know, it pulls you in.
[00:06:34.400 --> 00:06:35.320]   There's a story.
[00:06:35.320 --> 00:06:38.240]   It's like an interactive version
[00:06:38.240 --> 00:06:41.280]   of an Elder Scrolls Tolkien world.
[00:06:41.280 --> 00:06:42.840]   And you get to live in it.
[00:06:42.840 --> 00:06:44.160]   I don't know.
[00:06:44.160 --> 00:06:45.280]   I miss it.
[00:06:45.280 --> 00:06:48.560]   It's one of the things that suck about being an adult
[00:06:48.560 --> 00:06:51.240]   is there's no, you have to live in the real world
[00:06:51.240 --> 00:06:53.240]   as opposed to the Elder Scrolls world.
[00:06:53.240 --> 00:06:56.240]   - You know, whatever brings you joy, right?
[00:06:56.240 --> 00:06:57.080]   Minecraft, right?
[00:06:57.080 --> 00:06:57.920]   Minecraft's a great example.
[00:06:57.920 --> 00:06:59.600]   You create, like, it's not the fancy graphics,
[00:06:59.600 --> 00:07:02.480]   but it's the creation of your own worlds.
[00:07:02.480 --> 00:07:03.720]   - Yeah, that one is crazy.
[00:07:03.720 --> 00:07:05.760]   You know, one of the pitches for being a parent
[00:07:05.760 --> 00:07:07.960]   that people tell me is that you can, like,
[00:07:07.960 --> 00:07:09.960]   use the excuse of parenting
[00:07:09.960 --> 00:07:13.080]   to go back into the video game world.
[00:07:13.080 --> 00:07:15.840]   And, like, that's like, you know,
[00:07:15.840 --> 00:07:18.640]   father, son, father, daughter time,
[00:07:18.640 --> 00:07:20.840]   but really you just get to play video games with your kids.
[00:07:20.840 --> 00:07:23.040]   So anyway, at that time,
[00:07:23.040 --> 00:07:26.440]   did you have any ridiculous, ambitious dreams
[00:07:26.440 --> 00:07:30.640]   of where as a creator you might go as an engineer?
[00:07:30.640 --> 00:07:33.960]   What did you think of yourself as an engineer,
[00:07:33.960 --> 00:07:36.280]   as a tinker, or did you want to be like an astronaut
[00:07:36.280 --> 00:07:37.440]   or something like that?
[00:07:38.160 --> 00:07:40.440]   - You know, I'm tempted to make something up about,
[00:07:40.440 --> 00:07:43.040]   you know, robots, engineering,
[00:07:43.040 --> 00:07:44.600]   or, you know, mysteries of the universe,
[00:07:44.600 --> 00:07:48.080]   but that's not the actual memory that pops into my mind
[00:07:48.080 --> 00:07:49.640]   when you ask me about childhood dreams,
[00:07:49.640 --> 00:07:51.640]   so I'll actually share the real thing.
[00:07:51.640 --> 00:07:57.280]   When I was maybe four or five years old,
[00:07:57.280 --> 00:08:00.480]   I, as we all do, I thought about,
[00:08:00.480 --> 00:08:01.960]   you know, what I wanted to do when I grow up,
[00:08:01.960 --> 00:08:06.960]   and I had this dream of being a trainer
[00:08:07.000 --> 00:08:08.720]   and a traffic control cop.
[00:08:08.720 --> 00:08:11.320]   You know, they don't have those today, I think,
[00:08:11.320 --> 00:08:14.600]   but, you know, back in the '80s and, you know, in Russia,
[00:08:14.600 --> 00:08:16.000]   you probably are familiar with that, Lex.
[00:08:16.000 --> 00:08:19.080]   They had these, you know, police officers
[00:08:19.080 --> 00:08:20.920]   that would stand in the middle of an intersection all day,
[00:08:20.920 --> 00:08:21.760]   and they would have their, like,
[00:08:21.760 --> 00:08:23.600]   striped, black and white batons
[00:08:23.600 --> 00:08:24.680]   that they would use to, you know,
[00:08:24.680 --> 00:08:26.160]   control the flow of traffic.
[00:08:26.160 --> 00:08:27.760]   And, you know, for whatever reasons,
[00:08:27.760 --> 00:08:31.080]   I was strangely infatuated with this whole process,
[00:08:31.080 --> 00:08:33.080]   and like that, that was my dream.
[00:08:33.080 --> 00:08:35.080]   That's what I wanted to do when I grew up.
[00:08:35.080 --> 00:08:39.960]   And, you know, my parents, both physics profs, by the way,
[00:08:39.960 --> 00:08:42.480]   I think were, you know, a little concerned
[00:08:42.480 --> 00:08:45.040]   with that level of ambition coming from their child,
[00:08:45.040 --> 00:08:47.280]   at, you know, that age.
[00:08:47.280 --> 00:08:48.560]   - Well, that, it's an interesting,
[00:08:48.560 --> 00:08:50.560]   I don't know if you can relate,
[00:08:50.560 --> 00:08:52.840]   but I very much love that idea.
[00:08:52.840 --> 00:08:56.720]   I have a OCD nature that I think lends itself
[00:08:56.720 --> 00:08:59.960]   very close to the engineering mindset,
[00:08:59.960 --> 00:09:04.520]   which is you want to kind of optimize,
[00:09:04.520 --> 00:09:05.960]   you know, solve a problem
[00:09:05.960 --> 00:09:09.280]   by creating an automated solution,
[00:09:09.280 --> 00:09:13.080]   like a set of rules, a set of rules you can follow,
[00:09:13.080 --> 00:09:15.360]   and then thereby make it ultra efficient.
[00:09:15.360 --> 00:09:17.840]   I don't know if that's, it was of that nature.
[00:09:17.840 --> 00:09:19.160]   I certainly have that.
[00:09:19.160 --> 00:09:22.640]   There's like SimCity and factory building games,
[00:09:22.640 --> 00:09:24.000]   all those kinds of things,
[00:09:24.000 --> 00:09:26.360]   kind of speak to that engineering mindset.
[00:09:26.360 --> 00:09:28.040]   Or did you just like the uniform?
[00:09:28.040 --> 00:09:29.440]   - I think it was more of the latter.
[00:09:29.440 --> 00:09:31.520]   I think it was the uniform and the, you know,
[00:09:31.520 --> 00:09:36.520]   the stripe baton that made cars go in the right directions.
[00:09:36.520 --> 00:09:40.400]   But I guess, you know, it is, I did end up,
[00:09:40.400 --> 00:09:43.440]   I guess, you know, working on the transportation industry
[00:09:43.440 --> 00:09:44.280]   one way or another.
[00:09:44.280 --> 00:09:45.600]   - No uniform, no.
[00:09:45.600 --> 00:09:46.440]   - That's right.
[00:09:46.440 --> 00:09:50.600]   - Maybe it was my, you know, deep inner infatuation
[00:09:50.600 --> 00:09:55.520]   with the traffic control batons that led to this career.
[00:09:55.520 --> 00:09:58.120]   - Okay, what, when did you,
[00:09:58.120 --> 00:10:01.000]   when was the leap from programming to robotics?
[00:10:01.000 --> 00:10:01.840]   - That happened later.
[00:10:01.840 --> 00:10:03.800]   That was after grad school.
[00:10:03.800 --> 00:10:05.840]   After, and actually, then it was self-driving cars
[00:10:05.840 --> 00:10:09.600]   was I think my first real hands-on introduction
[00:10:09.600 --> 00:10:10.600]   to robotics.
[00:10:10.600 --> 00:10:14.040]   But I never really had that much hands-on experience
[00:10:14.040 --> 00:10:15.120]   in school and training.
[00:10:15.120 --> 00:10:17.920]   I worked on applied math and physics.
[00:10:17.920 --> 00:10:22.720]   Then in college, I did more kind of abstract
[00:10:22.720 --> 00:10:24.120]   computer science.
[00:10:24.120 --> 00:10:28.160]   And it was after grad school that I really got involved
[00:10:28.160 --> 00:10:30.560]   in robotics, which was actually self-driving cars.
[00:10:30.560 --> 00:10:33.120]   And, you know, that was a big, big flip.
[00:10:33.120 --> 00:10:34.800]   - What grad school?
[00:10:34.800 --> 00:10:36.240]   - So I went to grad school in Michigan,
[00:10:36.240 --> 00:10:37.840]   and then I did a postdoc at Stanford,
[00:10:37.840 --> 00:10:41.280]   which is, that was the postdoc where I got to play
[00:10:41.280 --> 00:10:42.960]   with self-driving cars.
[00:10:42.960 --> 00:10:44.440]   - Yeah, so we'll return there.
[00:10:44.440 --> 00:10:46.720]   Let's go back to Moscow.
[00:10:46.720 --> 00:10:50.440]   So, you know, for episode 100, I talked to my dad.
[00:10:50.440 --> 00:10:52.640]   And also I grew up with my dad, I guess.
[00:10:52.640 --> 00:10:55.360]   (laughing)
[00:10:55.360 --> 00:10:58.320]   So I had to put up with him for many years.
[00:10:58.320 --> 00:11:03.320]   And he went to the FISTIEH, or MIPT.
[00:11:03.320 --> 00:11:05.920]   It's weird to say in English,
[00:11:05.920 --> 00:11:08.200]   'cause I've heard all of this in Russian.
[00:11:08.200 --> 00:11:10.440]   Moscow Institute of Physics and Technology.
[00:11:10.440 --> 00:11:15.320]   And to me, that was like, I met some super interesting,
[00:11:15.320 --> 00:11:18.280]   as a child, I met some super interesting characters.
[00:11:18.280 --> 00:11:21.080]   It felt to me like the greatest university in the world,
[00:11:21.080 --> 00:11:23.480]   the most elite university in the world.
[00:11:23.480 --> 00:11:26.880]   And just the people that I met that came out of there
[00:11:26.880 --> 00:11:31.880]   were like, not only brilliant, but also special humans.
[00:11:31.880 --> 00:11:35.840]   It seems like that place really tested the soul.
[00:11:35.840 --> 00:11:37.240]   (laughing)
[00:11:37.240 --> 00:11:41.520]   Both in terms of technically and spiritually.
[00:11:41.520 --> 00:11:44.600]   So that could be just the romanticization of that place,
[00:11:44.600 --> 00:11:46.840]   I'm not sure, so maybe you can speak to it.
[00:11:46.840 --> 00:11:50.280]   But is it correct to say that you spent some time
[00:11:50.280 --> 00:11:51.120]   at FISTIEH?
[00:11:51.120 --> 00:11:52.600]   - Yeah, that's right, six years.
[00:11:52.600 --> 00:11:56.160]   I got my bachelor's and master's in physics and math there.
[00:11:56.160 --> 00:11:59.200]   And it's actually interesting, 'cause my dad,
[00:11:59.200 --> 00:12:01.040]   and actually both my parents went there,
[00:12:01.040 --> 00:12:04.680]   and I think all the stories that I heard,
[00:12:04.680 --> 00:12:07.920]   just like you, Alex, growing up about the place
[00:12:07.920 --> 00:12:10.400]   and how interesting and special and magical it was,
[00:12:10.400 --> 00:12:13.600]   I think that was a significant, maybe the main reason
[00:12:13.600 --> 00:12:16.680]   I wanted to go there for college.
[00:12:16.680 --> 00:12:20.680]   Enough so that I actually went back to Russia from the US.
[00:12:20.680 --> 00:12:22.560]   I graduated high school in the US.
[00:12:22.560 --> 00:12:24.880]   - You went back there?
[00:12:24.880 --> 00:12:25.880]   - I went back there, yeah.
[00:12:25.880 --> 00:12:26.720]   - Wow.
[00:12:26.720 --> 00:12:29.840]   - Exactly the reaction most of my peers in college had,
[00:12:29.840 --> 00:12:31.960]   but perhaps a little bit stronger,
[00:12:31.960 --> 00:12:35.080]   that point me out as this crazy kid.
[00:12:35.080 --> 00:12:36.280]   - Were your parents supportive of that?
[00:12:36.280 --> 00:12:38.200]   - Yeah, yeah, I gave them your previous question.
[00:12:38.200 --> 00:12:43.120]   They supported me in letting me pursue my passions
[00:12:43.120 --> 00:12:44.480]   and the things that I was interested in.
[00:12:44.480 --> 00:12:45.840]   - That's a bold move, wow.
[00:12:45.840 --> 00:12:46.760]   What was it like there?
[00:12:46.760 --> 00:12:47.640]   - It was interesting.
[00:12:47.640 --> 00:12:50.640]   Definitely fairly hardcore on the fundamentals
[00:12:50.640 --> 00:12:54.960]   of math and physics, and lots of good memories
[00:12:54.960 --> 00:12:56.960]   from those times.
[00:12:56.960 --> 00:12:58.480]   - So, okay, so Stanford,
[00:12:58.480 --> 00:13:00.520]   how'd you get into autonomous vehicles?
[00:13:00.520 --> 00:13:04.960]   - I had the great fortune and great honor
[00:13:04.960 --> 00:13:09.960]   to join Stanford's DARPA Urban Challenge Team in 2006.
[00:13:09.960 --> 00:13:13.600]   This was a third in the sequence of the DARPA challenges.
[00:13:13.600 --> 00:13:16.320]   There were two grand challenges prior to that,
[00:13:16.320 --> 00:13:20.720]   and then in 2007, they held the DARPA Urban Challenge.
[00:13:20.720 --> 00:13:22.280]   So, you know, I was doing my postdoc.
[00:13:22.280 --> 00:13:27.280]   I joined the team and worked on motion planning
[00:13:27.280 --> 00:13:31.680]   for that competition.
[00:13:31.680 --> 00:13:33.880]   - So, okay, so for people who might not know,
[00:13:33.880 --> 00:13:36.320]   I know from a certain,
[00:13:36.320 --> 00:13:38.320]   autonomous vehicles is a funny world.
[00:13:38.320 --> 00:13:41.200]   In a certain circle of people, everybody knows everything,
[00:13:41.200 --> 00:13:45.440]   and in a certain circle, nobody knows anything
[00:13:45.440 --> 00:13:46.600]   in terms of general public.
[00:13:46.600 --> 00:13:47.640]   So, it's interesting.
[00:13:47.640 --> 00:13:50.560]   It's a good question what to talk about,
[00:13:50.560 --> 00:13:55.560]   but I do think that the Urban Challenge is worth revisiting.
[00:13:55.560 --> 00:13:57.600]   It's a fun little challenge,
[00:13:57.600 --> 00:14:02.600]   one that first sparked so many incredible minds
[00:14:02.600 --> 00:14:07.080]   to focus on one of the hardest problems of our time
[00:14:07.080 --> 00:14:08.200]   in artificial intelligence.
[00:14:08.200 --> 00:14:10.720]   So, that's a success from a perspective
[00:14:10.720 --> 00:14:12.640]   of a single little challenge.
[00:14:12.640 --> 00:14:15.680]   But can you talk about what did the challenge involve?
[00:14:15.680 --> 00:14:17.400]   So, were there pedestrians?
[00:14:17.400 --> 00:14:19.040]   Were there other cars?
[00:14:19.040 --> 00:14:20.680]   What was the goal?
[00:14:20.680 --> 00:14:22.360]   Who was on the team?
[00:14:22.360 --> 00:14:23.920]   How long did it take?
[00:14:23.920 --> 00:14:26.640]   Any fun sort of specs?
[00:14:26.640 --> 00:14:28.360]   - Sure, sure, sure.
[00:14:28.360 --> 00:14:30.680]   So, the way the challenge was constructed
[00:14:30.680 --> 00:14:31.960]   and just a little bit of backgrounding,
[00:14:31.960 --> 00:14:35.520]   as I mentioned, this was the third competition
[00:14:35.520 --> 00:14:36.360]   in that series.
[00:14:36.360 --> 00:14:38.120]   The first two were the Grand Challenge,
[00:14:38.120 --> 00:14:38.960]   called the Grand Challenge.
[00:14:38.960 --> 00:14:40.680]   The goal there was to just drive
[00:14:40.680 --> 00:14:42.320]   in a completely static environment.
[00:14:42.320 --> 00:14:44.120]   You know, you had to drive in a desert.
[00:14:44.120 --> 00:14:47.760]   That was very successful.
[00:14:47.760 --> 00:14:50.240]   So, then DARPA followed with what they called
[00:14:50.240 --> 00:14:53.640]   the Urban Challenge, where the goal was to have,
[00:14:53.640 --> 00:14:55.480]   you know, build vehicles that could operate
[00:14:55.480 --> 00:14:57.080]   in more dynamic environments
[00:14:57.080 --> 00:14:58.680]   and, you know, share them with other vehicles.
[00:14:58.680 --> 00:15:00.280]   There were no pedestrians there.
[00:15:00.280 --> 00:15:02.400]   But what DARPA did is they took over
[00:15:02.400 --> 00:15:04.800]   an abandoned Air Force Base.
[00:15:04.800 --> 00:15:07.080]   And it was kind of like a little fake city
[00:15:07.080 --> 00:15:08.400]   that they built out there.
[00:15:08.400 --> 00:15:13.320]   And they had a bunch of robots, you know, cars,
[00:15:13.320 --> 00:15:16.280]   that were autonomous in there, all at the same time,
[00:15:16.280 --> 00:15:21.040]   mixed in with other vehicles driven by professional drivers.
[00:15:21.040 --> 00:15:24.040]   And each car had a mission.
[00:15:24.040 --> 00:15:27.040]   And so, there's a crude map that they received
[00:15:27.040 --> 00:15:27.880]   at the beginning.
[00:15:27.880 --> 00:15:29.960]   And they had a mission, you know, go here and then there
[00:15:29.960 --> 00:15:31.480]   and over here.
[00:15:31.480 --> 00:15:34.800]   And they kind of all were sharing this environment
[00:15:34.800 --> 00:15:36.760]   at the same time they had to interact with each other,
[00:15:36.760 --> 00:15:38.560]   they had to interact with the human drivers.
[00:15:38.560 --> 00:15:43.080]   So, it's this very first, very rudimentary version
[00:15:43.080 --> 00:15:46.760]   of a self-driving car that, you know, could operate
[00:15:46.760 --> 00:15:49.960]   in an environment, you know,
[00:15:49.960 --> 00:15:51.600]   shared with other dynamic actors.
[00:15:51.600 --> 00:15:54.960]   That, as you said, you know, really, in many ways,
[00:15:54.960 --> 00:15:57.120]   you know, kick-started this whole industry.
[00:15:57.120 --> 00:16:00.440]   - Okay, so who was on the team and how'd you do?
[00:16:00.440 --> 00:16:01.400]   I forget.
[00:16:01.400 --> 00:16:03.400]   (laughing)
[00:16:03.400 --> 00:16:04.360]   - I came in second.
[00:16:04.360 --> 00:16:07.200]   Perhaps that was my contribution to the team.
[00:16:07.200 --> 00:16:08.640]   I think the Stanford team came in first
[00:16:08.640 --> 00:16:10.880]   in the DARPA challenge, but then I joined the team.
[00:16:10.880 --> 00:16:11.720]   And, you know, we came-
[00:16:11.720 --> 00:16:13.960]   - You were the only one with a bug in the code.
[00:16:13.960 --> 00:16:15.360]   I mean, do you have sort of memories
[00:16:15.360 --> 00:16:18.200]   of some particularly challenging things or,
[00:16:18.200 --> 00:16:21.840]   you know, one of the cool things, it's not, you know,
[00:16:21.840 --> 00:16:26.240]   this isn't a product, this isn't a thing that, you know,
[00:16:26.240 --> 00:16:27.800]   it, there's, you have a little bit more freedom
[00:16:27.800 --> 00:16:29.600]   to experiment, so you can take risks,
[00:16:29.600 --> 00:16:32.920]   and there's, so you can make mistakes.
[00:16:32.920 --> 00:16:35.040]   Is there interesting mistakes?
[00:16:35.040 --> 00:16:37.120]   Is there interesting challenges that stand out to you
[00:16:37.120 --> 00:16:41.200]   as something that taught you a good technical lesson
[00:16:41.200 --> 00:16:44.320]   or a good philosophical lesson from that time?
[00:16:44.320 --> 00:16:46.000]   - Yeah, you know, definitely, definitely
[00:16:46.000 --> 00:16:47.680]   a very memorable time.
[00:16:47.680 --> 00:16:51.480]   Not really a challenge, but like one of the most vivid
[00:16:51.480 --> 00:16:53.960]   memories that I have from the time,
[00:16:53.960 --> 00:16:57.680]   and I think that was actually one of the days
[00:16:57.680 --> 00:17:01.520]   that really got me hooked on this whole field was
[00:17:01.520 --> 00:17:06.800]   the first time I got to run my software on the car.
[00:17:06.800 --> 00:17:11.800]   And I was working on a part of our planning algorithm
[00:17:11.800 --> 00:17:13.880]   that had to navigate in parking lots.
[00:17:13.880 --> 00:17:15.080]   So it was something that, you know,
[00:17:15.080 --> 00:17:16.800]   called free space motion planning.
[00:17:16.800 --> 00:17:19.600]   So the very first version of that was, you know,
[00:17:19.600 --> 00:17:22.560]   we tried on the car, it was on Stanford's campus
[00:17:22.560 --> 00:17:25.040]   in the middle of the night, and you had this little,
[00:17:25.040 --> 00:17:26.920]   you know, course constructed with cones
[00:17:26.920 --> 00:17:28.360]   in the middle of a parking lot.
[00:17:28.360 --> 00:17:29.840]   So we're there in like 3 a.m., you know,
[00:17:29.840 --> 00:17:31.960]   by the time we got the code to, you know,
[00:17:31.960 --> 00:17:36.160]   compile and turn over, and, you know, it drove.
[00:17:36.160 --> 00:17:38.120]   I could actually did something quite reasonable.
[00:17:38.120 --> 00:17:42.160]   And, you know, it was of course very buggy at the time
[00:17:42.160 --> 00:17:43.840]   and had all kinds of problems,
[00:17:43.840 --> 00:17:48.200]   but it was pretty darn magical.
[00:17:48.200 --> 00:17:50.400]   I remember going back and, you know,
[00:17:50.400 --> 00:17:52.680]   late at night and trying to fall asleep
[00:17:52.680 --> 00:17:54.200]   and just, you know, being unable to fall asleep
[00:17:54.200 --> 00:17:58.480]   for the rest of the night, just my mind was blown.
[00:17:58.480 --> 00:18:01.640]   And that's what I've been doing ever since
[00:18:01.640 --> 00:18:03.520]   for more than a decade.
[00:18:03.520 --> 00:18:06.280]   - In terms of challenges and, you know,
[00:18:06.280 --> 00:18:08.520]   interesting memories, like on the day of the competition,
[00:18:08.520 --> 00:18:10.760]   it was pretty nerve wracking.
[00:18:10.760 --> 00:18:12.640]   I remember standing there with Mike Montemarolo,
[00:18:12.640 --> 00:18:15.920]   who was the software lead and wrote most of the code.
[00:18:15.920 --> 00:18:17.800]   I think I did one little part of the planner,
[00:18:17.800 --> 00:18:21.680]   Mike, you know, incredibly did pretty much the rest of it
[00:18:21.680 --> 00:18:24.320]   with a bunch of other incredible people.
[00:18:24.320 --> 00:18:27.360]   But I remember standing on the day of the competition,
[00:18:27.360 --> 00:18:29.080]   you know, watching the car, you know, with Mike,
[00:18:29.080 --> 00:18:32.320]   and, you know, cars are completely empty, right?
[00:18:32.320 --> 00:18:34.760]   They're all there lined up in the beginning of the race.
[00:18:34.760 --> 00:18:37.000]   And then, you know, DARPA sends them, you know,
[00:18:37.000 --> 00:18:38.640]   on their mission one by one.
[00:18:38.640 --> 00:18:40.600]   So then leave, and Mike, you just,
[00:18:40.600 --> 00:18:41.480]   they had these sirens,
[00:18:41.480 --> 00:18:42.320]   (imitates siren)
[00:18:42.320 --> 00:18:43.680]   they all had their different silence, right?
[00:18:43.680 --> 00:18:46.400]   Each siren had its own personality, if you will.
[00:18:46.400 --> 00:18:48.640]   So, you know, off they go and you don't see them.
[00:18:48.640 --> 00:18:50.040]   You just kind of, and then every once in a while,
[00:18:50.040 --> 00:18:51.240]   they, you know, come a little bit closer
[00:18:51.240 --> 00:18:53.720]   to where the audience is,
[00:18:53.720 --> 00:18:55.080]   and you can kind of hear, you know,
[00:18:55.080 --> 00:18:56.440]   the sound of your car and, you know,
[00:18:56.440 --> 00:18:57.480]   it seems to be moving along.
[00:18:57.480 --> 00:18:58.600]   So that, you know, it gives you hope.
[00:18:58.600 --> 00:19:00.440]   And then, you know, it goes away
[00:19:00.440 --> 00:19:01.640]   and you can't hear it for too long.
[00:19:01.640 --> 00:19:02.520]   You start getting anxious, right?
[00:19:02.520 --> 00:19:03.360]   So it's a little bit like, you know,
[00:19:03.360 --> 00:19:04.720]   sending your kids to college and like, you know,
[00:19:04.720 --> 00:19:05.800]   kind of you invested in them.
[00:19:05.800 --> 00:19:08.480]   You hope you build it properly,
[00:19:08.480 --> 00:19:11.960]   but like it's still anxiety inducing.
[00:19:11.960 --> 00:19:15.800]   So that was an incredibly fun few days.
[00:19:15.800 --> 00:19:17.600]   In terms of, you know, bugs,
[00:19:17.600 --> 00:19:19.040]   as you mentioned, you know, one,
[00:19:19.040 --> 00:19:22.800]   that was my bug that caused us the loss of the first place,
[00:19:22.800 --> 00:19:24.640]   is there still a debate that, you know,
[00:19:24.640 --> 00:19:26.200]   occasionally have with people on the CMU team.
[00:19:26.200 --> 00:19:28.280]   CMU came first, I should mention.
[00:19:28.280 --> 00:19:31.600]   - CMU, haven't heard of them, but yeah.
[00:19:31.600 --> 00:19:33.520]   It's a small school.
[00:19:33.520 --> 00:19:36.120]   It's really a glitch that, you know,
[00:19:36.120 --> 00:19:38.280]   they happen to succeed at something robotics related.
[00:19:38.280 --> 00:19:39.160]   - Very scenic though.
[00:19:39.160 --> 00:19:42.240]   So most people go there for the scenery.
[00:19:42.240 --> 00:19:44.040]   Yeah, it's a beautiful campus.
[00:19:44.040 --> 00:19:45.520]   (laughing)
[00:19:45.520 --> 00:19:46.960]   I apologize. - Unlike Stanford.
[00:19:46.960 --> 00:19:49.240]   - So for people, yeah, that's true, unlike Stanford.
[00:19:49.240 --> 00:19:50.080]   For people who don't know,
[00:19:50.080 --> 00:19:51.680]   CMU is one of the great robotics
[00:19:51.680 --> 00:19:53.880]   and sort of artificial intelligence universities
[00:19:53.880 --> 00:19:54.720]   in the world.
[00:19:54.720 --> 00:19:56.880]   CMU, Carnegie Mellon University.
[00:19:56.880 --> 00:19:58.520]   Okay, sorry, go ahead.
[00:19:58.520 --> 00:19:59.560]   - Good PSA.
[00:19:59.560 --> 00:20:04.280]   So in the part that I contributed to,
[00:20:04.280 --> 00:20:06.120]   which was navigating parking lots,
[00:20:06.120 --> 00:20:09.800]   and the way that part of the mission worked is,
[00:20:09.800 --> 00:20:14.280]   in a parking lot, you would get from DARPA
[00:20:14.280 --> 00:20:15.760]   an outline of the map.
[00:20:15.760 --> 00:20:17.840]   You basically get this giant polygon
[00:20:17.840 --> 00:20:20.240]   that defined the perimeter of the parking lot.
[00:20:20.240 --> 00:20:21.400]   And there would be an entrance,
[00:20:21.400 --> 00:20:23.880]   and maybe multiple entrances or access to it.
[00:20:23.880 --> 00:20:28.880]   And then you would get a goal within that open space.
[00:20:29.880 --> 00:20:33.000]   XY heading, where the car had to park.
[00:20:33.000 --> 00:20:35.400]   It had no information about the obstacles
[00:20:35.400 --> 00:20:36.920]   that the car might encounter there.
[00:20:36.920 --> 00:20:39.680]   So it had to navigate a completely free space
[00:20:39.680 --> 00:20:42.040]   from the entrance to the parking lot
[00:20:42.040 --> 00:20:44.120]   into that parking space.
[00:20:44.120 --> 00:20:47.320]   And then once you're parked there,
[00:20:47.320 --> 00:20:50.480]   it had to exit the parking lot.
[00:20:50.480 --> 00:20:51.840]   While of course, encountering and reasoning
[00:20:51.840 --> 00:20:55.240]   about all the obstacles that it encounters in real time.
[00:20:55.240 --> 00:20:59.280]   So our interpretation,
[00:20:59.280 --> 00:21:00.800]   or at least my interpretation of the rules
[00:21:00.800 --> 00:21:03.800]   was that you had to reverse out of the parking spot.
[00:21:03.800 --> 00:21:05.280]   And that's what our cars did,
[00:21:05.280 --> 00:21:07.120]   even if there's no obstacle in front.
[00:21:07.120 --> 00:21:08.960]   That's not what CMU's car did.
[00:21:08.960 --> 00:21:11.000]   And it just kind of drove right through.
[00:21:11.000 --> 00:21:12.720]   So there's still a debate.
[00:21:12.720 --> 00:21:13.560]   And of course, you know,
[00:21:13.560 --> 00:21:14.560]   as you stop and then reverse out
[00:21:14.560 --> 00:21:16.000]   and go out the different way,
[00:21:16.000 --> 00:21:17.240]   that costs you some time, right?
[00:21:17.240 --> 00:21:18.920]   So there's still a debate,
[00:21:18.920 --> 00:21:20.760]   whether it was my poor implementation
[00:21:20.760 --> 00:21:22.040]   that cost us extra time,
[00:21:22.040 --> 00:21:26.400]   or whether it was CMU violating
[00:21:26.400 --> 00:21:27.800]   an important rule of the competition.
[00:21:27.800 --> 00:21:30.200]   And I have my own opinion here.
[00:21:30.200 --> 00:21:31.120]   In terms of other bugs,
[00:21:31.120 --> 00:21:34.240]   and I have to apologize to Mike Montemariella
[00:21:34.240 --> 00:21:35.880]   for sharing this on air,
[00:21:35.880 --> 00:21:39.320]   but it is actually one of the more memorable ones.
[00:21:39.320 --> 00:21:43.480]   And it's something that's kind of become a bit of a metaphor,
[00:21:43.480 --> 00:21:45.920]   I don't know, a label in the industry since then.
[00:21:45.920 --> 00:21:47.440]   I think at least in some circles,
[00:21:47.440 --> 00:21:50.080]   it's called the victory circle or victory lap.
[00:21:50.080 --> 00:21:53.800]   And our cars did that.
[00:21:53.800 --> 00:21:56.880]   So in one of the missions in the urban challenge,
[00:21:56.880 --> 00:21:58.280]   in one of the courses,
[00:21:58.280 --> 00:22:01.880]   there was this big oval right by the start
[00:22:01.880 --> 00:22:02.720]   and finish of the race.
[00:22:02.720 --> 00:22:05.240]   So the ARPA had a lot of the missions would finish
[00:22:05.240 --> 00:22:07.200]   kind of in that same location.
[00:22:07.200 --> 00:22:08.040]   And it was pretty cool
[00:22:08.040 --> 00:22:09.760]   because you could see the cars come by,
[00:22:09.760 --> 00:22:10.960]   you know, kind of finish that part,
[00:22:10.960 --> 00:22:11.800]   like over the trip,
[00:22:11.800 --> 00:22:12.640]   or that like over the mission,
[00:22:12.640 --> 00:22:16.120]   and then, you know, go on and finish the rest of it.
[00:22:17.440 --> 00:22:20.320]   And other vehicles would, you know,
[00:22:20.320 --> 00:22:23.600]   come hit their waypoint and, you know,
[00:22:23.600 --> 00:22:25.720]   exit the oval and off they would go.
[00:22:25.720 --> 00:22:28.040]   Our car in the hand, which hit the checkpoint,
[00:22:28.040 --> 00:22:30.360]   and then it would do an extra lap around the oval
[00:22:30.360 --> 00:22:32.920]   and only then, you know, leave and go on its merry way.
[00:22:32.920 --> 00:22:34.440]   So over the course of, you know, the full day,
[00:22:34.440 --> 00:22:36.520]   it accumulated some extra time.
[00:22:36.520 --> 00:22:37.760]   And the problem was that we had a bug
[00:22:37.760 --> 00:22:39.120]   where it wouldn't, you know,
[00:22:39.120 --> 00:22:40.920]   start reasoning about the next waypoint
[00:22:40.920 --> 00:22:42.640]   and plan a route to get to that next point
[00:22:42.640 --> 00:22:43.840]   until it hit a previous one.
[00:22:43.840 --> 00:22:44.960]   And in that particular case,
[00:22:44.960 --> 00:22:46.880]   by the time you hit that one,
[00:22:46.880 --> 00:22:49.240]   it was too late for us to consider the next one
[00:22:49.240 --> 00:22:50.240]   and kind of make a lane change.
[00:22:50.240 --> 00:22:52.080]   So every time it would do like an extra lap.
[00:22:52.080 --> 00:22:56.160]   So, you know, that's the Stanford victory lap.
[00:22:56.160 --> 00:22:57.000]   - The victory lap.
[00:22:57.000 --> 00:22:58.040]   (laughing)
[00:22:58.040 --> 00:23:00.040]   Oh, that's, I feel like there's something
[00:23:00.040 --> 00:23:01.840]   philosophically profound in there somehow,
[00:23:01.840 --> 00:23:04.000]   but I mean, ultimately,
[00:23:04.000 --> 00:23:07.240]   everybody is a winner in that kind of competition.
[00:23:07.240 --> 00:23:11.880]   And it led to sort of famously to the creation
[00:23:11.880 --> 00:23:14.920]   of Google self-driving car project.
[00:23:14.920 --> 00:23:16.720]   And now Waymo.
[00:23:16.720 --> 00:23:21.280]   So can we give an overview of how is Waymo born?
[00:23:21.280 --> 00:23:24.200]   How's the Google self-driving car project born?
[00:23:24.200 --> 00:23:25.760]   What is the mission?
[00:23:25.760 --> 00:23:27.320]   What is the hope?
[00:23:27.320 --> 00:23:32.320]   What is it is the engineering kind of set of milestones
[00:23:32.320 --> 00:23:35.640]   that it seeks to accomplish?
[00:23:35.640 --> 00:23:37.120]   There's a lot of questions in there.
[00:23:37.120 --> 00:23:37.960]   - Yeah.
[00:23:37.960 --> 00:23:38.800]   (laughing)
[00:23:38.800 --> 00:23:39.640]   I mean, you're right.
[00:23:39.640 --> 00:23:40.760]   Kind of the DARPA Urban Challenge
[00:23:40.760 --> 00:23:43.080]   and the previous DARPA Grand Challenges
[00:23:43.080 --> 00:23:46.040]   kind of led, I think, to a very large degree
[00:23:46.040 --> 00:23:46.880]   to that next step.
[00:23:46.880 --> 00:23:51.040]   And then Larry and Sergey, Larry Page and Sergey Brin,
[00:23:51.040 --> 00:23:53.960]   Google Founders Group, saw that competition
[00:23:53.960 --> 00:23:56.040]   and believed in the technology.
[00:23:56.040 --> 00:23:59.240]   So the Google self-driving car project was born.
[00:23:59.240 --> 00:24:03.120]   You know, at that time, and we started in 2009,
[00:24:03.120 --> 00:24:04.600]   it was a pretty small group of us,
[00:24:04.600 --> 00:24:07.640]   about a dozen people who came together
[00:24:07.640 --> 00:24:10.760]   to work on this project at Google.
[00:24:10.760 --> 00:24:15.760]   At that time, we saw that incredible early result
[00:24:15.760 --> 00:24:19.280]   in the DARPA Urban Challenge.
[00:24:19.280 --> 00:24:24.200]   I think we're all incredibly excited about where we got to.
[00:24:24.200 --> 00:24:26.360]   And we believed in the future of the technology,
[00:24:26.360 --> 00:24:30.120]   but we still had a very rudimentary understanding
[00:24:30.120 --> 00:24:31.600]   of the problem space.
[00:24:31.600 --> 00:24:35.680]   So the first goal of this project in 2009
[00:24:35.680 --> 00:24:39.840]   was to really better understand what we're up against.
[00:24:39.840 --> 00:24:43.800]   And with that goal in mind, when we started the project,
[00:24:43.800 --> 00:24:46.120]   we created a few milestones for ourselves
[00:24:46.120 --> 00:24:49.960]   that maximized learnings, if you will.
[00:24:49.960 --> 00:24:52.160]   The two milestones were,
[00:24:52.160 --> 00:24:55.880]   one was to drive 100,000 miles in autonomous mode,
[00:24:55.880 --> 00:24:57.880]   which was at that time, orders of magnitude
[00:24:57.880 --> 00:25:01.040]   that more than anybody has ever done.
[00:25:01.040 --> 00:25:05.360]   And the second milestone was to drive 10 routes.
[00:25:05.360 --> 00:25:07.520]   Each one was 100 miles long.
[00:25:08.520 --> 00:25:12.400]   They were specifically chosen to be kind of extra spicy,
[00:25:12.400 --> 00:25:14.920]   extra complicated, and sampled the full complexity
[00:25:14.920 --> 00:25:18.560]   of that domain.
[00:25:18.560 --> 00:25:23.280]   And you had to drive each one from beginning to end
[00:25:23.280 --> 00:25:25.200]   with no intervention, no human intervention.
[00:25:25.200 --> 00:25:26.880]   So you would get to the beginning of the course,
[00:25:26.880 --> 00:25:30.200]   you would press the button that would engage in autonomy,
[00:25:30.200 --> 00:25:33.200]   and you had to go for 100 miles,
[00:25:33.200 --> 00:25:36.120]   beginning to end with no interventions.
[00:25:36.120 --> 00:25:38.960]   And it sampled, again,
[00:25:38.960 --> 00:25:40.800]   the full complexity of driving conditions.
[00:25:40.800 --> 00:25:43.280]   Some were on freeways.
[00:25:43.280 --> 00:25:45.240]   We had one route that went all through all the freeways
[00:25:45.240 --> 00:25:47.160]   and all the bridges in the Bay Area.
[00:25:47.160 --> 00:25:50.040]   You know, we had some that went around Lake Tahoe
[00:25:50.040 --> 00:25:52.440]   and kind of mountains roads.
[00:25:52.440 --> 00:25:56.360]   We had some that drove through dense urban environments
[00:25:56.360 --> 00:25:59.840]   like in downtown Palo Alto and through San Francisco.
[00:25:59.840 --> 00:26:04.840]   So it was incredibly interesting to work on.
[00:26:05.280 --> 00:26:10.280]   And it took us just under two years,
[00:26:10.280 --> 00:26:12.160]   about a year and a half, a little bit more,
[00:26:12.160 --> 00:26:14.560]   to finish both of these milestones.
[00:26:14.560 --> 00:26:16.240]   And in that process,
[00:26:16.240 --> 00:26:20.440]   A, it was an incredible amount of fun,
[00:26:20.440 --> 00:26:23.120]   probably the most fun I had in my professional career.
[00:26:23.120 --> 00:26:25.720]   And you're just learning so much.
[00:26:25.720 --> 00:26:27.160]   The goal here is to learn and prototype.
[00:26:27.160 --> 00:26:29.520]   You're not yet starting to build a production system.
[00:26:29.520 --> 00:26:31.280]   So you just, you were,
[00:26:31.280 --> 00:26:33.720]   this is when you're kind of working 24/7
[00:26:33.720 --> 00:26:35.080]   and you're hacking things together.
[00:26:35.080 --> 00:26:37.960]   And you also don't know how hard this is.
[00:26:37.960 --> 00:26:39.200]   I mean, that's the point.
[00:26:39.200 --> 00:26:41.440]   Like, so, I mean, that's an ambitious,
[00:26:41.440 --> 00:26:44.480]   if I put myself in that mindset, even still,
[00:26:44.480 --> 00:26:47.120]   that's a really ambitious set of goals.
[00:26:47.120 --> 00:26:49.600]   Like just those two, just picking,
[00:26:49.600 --> 00:26:54.600]   just picking 10 different difficult, spicy challenges
[00:26:54.600 --> 00:26:59.960]   and then having zero interventions.
[00:26:59.960 --> 00:27:03.480]   So like not saying gradually we're going to like,
[00:27:04.080 --> 00:27:06.880]   you know, over a period of 10 years,
[00:27:06.880 --> 00:27:08.440]   we're going to have a bunch of roots
[00:27:08.440 --> 00:27:11.000]   and gradually reduce the number of interventions.
[00:27:11.000 --> 00:27:12.600]   You know, that literally says like,
[00:27:12.600 --> 00:27:16.600]   by as soon as possible, we want to have zero
[00:27:16.600 --> 00:27:18.600]   and on hard roads.
[00:27:18.600 --> 00:27:20.840]   So like, to me, if I was facing that,
[00:27:20.840 --> 00:27:24.120]   it's unclear that whether that takes two years
[00:27:24.120 --> 00:27:25.760]   or whether that takes 20 years.
[00:27:25.760 --> 00:27:28.160]   I mean, it may be-- - It took us under two.
[00:27:28.160 --> 00:27:31.200]   - And I guess that speaks to a really big difference
[00:27:31.200 --> 00:27:35.520]   between doing something once and having a prototype
[00:27:35.520 --> 00:27:37.800]   where you're going after, you know,
[00:27:37.800 --> 00:27:39.320]   learning about the problem
[00:27:39.320 --> 00:27:43.400]   versus how you go about engineering a product that,
[00:27:43.400 --> 00:27:45.800]   where you look at, you know,
[00:27:45.800 --> 00:27:47.280]   do you properly do evaluation?
[00:27:47.280 --> 00:27:49.080]   You look at metrics, you know, drive down,
[00:27:49.080 --> 00:27:50.720]   and you're confident that you can do that.
[00:27:50.720 --> 00:27:51.560]   And I guess that's the, you know,
[00:27:51.560 --> 00:27:55.840]   why it took a dozen people, you know,
[00:27:55.840 --> 00:27:58.280]   16 months or a little bit more than that,
[00:27:58.280 --> 00:28:00.880]   back in 2009 and 2010,
[00:28:00.880 --> 00:28:02.960]   and with the technology of, you know,
[00:28:02.960 --> 00:28:04.640]   more than a decade ago,
[00:28:04.640 --> 00:28:08.120]   that amount of time to achieve that milestone
[00:28:08.120 --> 00:28:12.360]   of 10 routes, 100 miles each and no interventions.
[00:28:12.360 --> 00:28:16.880]   And, you know, it took us a little bit longer
[00:28:16.880 --> 00:28:19.680]   to get to, you know, a full driverless product
[00:28:19.680 --> 00:28:20.960]   that customers use.
[00:28:20.960 --> 00:28:22.320]   - That's another really important moment.
[00:28:22.320 --> 00:28:27.320]   Is there some memories of technical lessons
[00:28:27.720 --> 00:28:30.400]   or just one, like, what did you learn
[00:28:30.400 --> 00:28:33.480]   about the problem of driving from that experience?
[00:28:33.480 --> 00:28:36.080]   I mean, we can now talk about like what you learned
[00:28:36.080 --> 00:28:38.720]   from modern day Waymo,
[00:28:38.720 --> 00:28:42.160]   but I feel like you may have learned some profound things
[00:28:42.160 --> 00:28:45.760]   in those early days, even more so,
[00:28:45.760 --> 00:28:48.080]   because it feels like what Waymo is now
[00:28:48.080 --> 00:28:50.680]   is the trying to, you know, how to do scale,
[00:28:50.680 --> 00:28:52.160]   how to make sure you create a product,
[00:28:52.160 --> 00:28:54.240]   how to make sure it's like safety and all those things,
[00:28:54.240 --> 00:28:56.600]   which is all fascinating challenges.
[00:28:56.600 --> 00:28:59.840]   But like you were facing the more fundamental,
[00:28:59.840 --> 00:29:03.800]   philosophical problem of driving in those early days.
[00:29:03.800 --> 00:29:08.200]   Like what the hell is driving as an autonomous vehicle?
[00:29:08.200 --> 00:29:10.000]   Maybe I'm again, romanticizing it,
[00:29:10.000 --> 00:29:15.000]   but is there some valuable lessons
[00:29:15.000 --> 00:29:18.200]   you picked up over there at those two years?
[00:29:18.200 --> 00:29:20.400]   - A ton.
[00:29:20.400 --> 00:29:23.280]   The most important one is probably
[00:29:23.280 --> 00:29:26.240]   that we believe that it's doable
[00:29:26.240 --> 00:29:30.760]   and we've gotten far enough into the problem that,
[00:29:30.760 --> 00:29:33.280]   you know, we had a, I think only a glimpse
[00:29:33.280 --> 00:29:38.080]   of the true complexity of the domain.
[00:29:38.080 --> 00:29:39.240]   You know, it's a little bit like, you know,
[00:29:39.240 --> 00:29:40.680]   climbing a mountain where you kind of, you know,
[00:29:40.680 --> 00:29:43.120]   see the next peak and you think that's kind of the summit,
[00:29:43.120 --> 00:29:44.400]   but then you get to that and you kind of see
[00:29:44.400 --> 00:29:47.560]   that this is just the start of the journey.
[00:29:47.560 --> 00:29:50.960]   But we've tried, we've sampled enough of the problem space
[00:29:50.960 --> 00:29:54.200]   and we've made enough rapid success,
[00:29:54.200 --> 00:29:57.240]   even, you know, with technology of 2009, 2010,
[00:29:57.240 --> 00:30:00.840]   that it gave us confidence to then, you know,
[00:30:00.840 --> 00:30:04.440]   pursue this as a real product.
[00:30:04.440 --> 00:30:05.640]   - So, okay.
[00:30:05.640 --> 00:30:09.080]   So the next step, you mentioned the milestones
[00:30:09.080 --> 00:30:11.560]   that you had in those two years.
[00:30:11.560 --> 00:30:13.920]   What are the next milestones that then led
[00:30:13.920 --> 00:30:16.160]   to the creation of Waymo and beyond?
[00:30:16.160 --> 00:30:18.080]   - Yeah, it was a really interesting journey.
[00:30:18.080 --> 00:30:20.920]   And, you know, Waymo came a little bit later.
[00:30:22.280 --> 00:30:26.440]   Then, you know, we completed those milestones in 2010.
[00:30:26.440 --> 00:30:30.000]   That was the pivot when we decided to focus
[00:30:30.000 --> 00:30:31.920]   on actually building a product, you know,
[00:30:31.920 --> 00:30:33.040]   using this technology.
[00:30:33.040 --> 00:30:36.400]   The initial couple of years after that,
[00:30:36.400 --> 00:30:39.280]   we were focused on a freeway, you know,
[00:30:39.280 --> 00:30:41.200]   what you would call a driver assist,
[00:30:41.200 --> 00:30:44.600]   maybe, you know, an L3 driver assist program.
[00:30:44.600 --> 00:30:49.600]   Then around 2013, we've learned enough about the space
[00:30:49.600 --> 00:30:52.880]   and have thought more deeply about, you know,
[00:30:52.880 --> 00:30:56.720]   the product that we wanted to build that we pivoted.
[00:30:56.720 --> 00:31:00.880]   We pivoted towards this vision of, you know,
[00:31:00.880 --> 00:31:04.280]   building a driver and deploying it fully driverless vehicles
[00:31:04.280 --> 00:31:05.120]   without a person.
[00:31:05.120 --> 00:31:07.240]   And that's the path that we've been on since then.
[00:31:07.240 --> 00:31:10.680]   And it was exactly the right decision for us.
[00:31:10.680 --> 00:31:13.200]   - So there was a moment where you also considered like,
[00:31:13.200 --> 00:31:15.040]   what is the right trajectory here?
[00:31:15.040 --> 00:31:16.840]   What is the right role of automation
[00:31:16.840 --> 00:31:18.360]   in the task of driving?
[00:31:18.360 --> 00:31:21.920]   Or is still, it wasn't from the early days,
[00:31:21.920 --> 00:31:24.240]   obviously you want to go fully autonomous.
[00:31:24.240 --> 00:31:25.240]   - From the early days, it was not.
[00:31:25.240 --> 00:31:27.520]   I think it was in 20, around 2013, maybe,
[00:31:27.520 --> 00:31:32.640]   that we've, that became very clear and we made that pivot.
[00:31:32.640 --> 00:31:34.760]   And it also became very clear
[00:31:34.760 --> 00:31:37.840]   and that it's, even the way you go building
[00:31:37.840 --> 00:31:40.760]   a driver assist system is, you know,
[00:31:40.760 --> 00:31:42.600]   fundamentally different from how you go building
[00:31:42.600 --> 00:31:43.600]   a fully driverless vehicle.
[00:31:43.600 --> 00:31:47.800]   So, you know, we've pivoted towards the latter
[00:31:47.800 --> 00:31:50.720]   and that's what we've been working on ever since.
[00:31:50.720 --> 00:31:53.120]   And so that was around 2013.
[00:31:53.120 --> 00:31:58.040]   Then there's a sequence of really meaningful for us,
[00:31:58.040 --> 00:32:01.080]   really important, defining milestones since then.
[00:32:01.080 --> 00:32:06.080]   In 2015, we had our first,
[00:32:06.080 --> 00:32:13.840]   actually the world's first fully driverless ride
[00:32:13.840 --> 00:32:15.560]   on public roads.
[00:32:15.560 --> 00:32:18.080]   It was in a custom built vehicle that we had.
[00:32:18.080 --> 00:32:18.960]   I must've seen those.
[00:32:18.960 --> 00:32:20.560]   We called them the Firefly, that, you know,
[00:32:20.560 --> 00:32:23.240]   funny looking, marshmallow looking thing.
[00:32:23.240 --> 00:32:28.240]   And we put a passenger, his name was Steve Mann,
[00:32:28.240 --> 00:32:32.560]   a great friend of our project from the early days.
[00:32:32.560 --> 00:32:35.000]   The man happens to be blind.
[00:32:35.000 --> 00:32:36.520]   So we put him in that vehicle.
[00:32:36.520 --> 00:32:38.600]   The car had no steering wheel, no pedals.
[00:32:38.600 --> 00:32:41.200]   It was an uncontrolled environment, you know,
[00:32:41.200 --> 00:32:44.960]   no lead or chase cars, no police escorts.
[00:32:44.960 --> 00:32:48.360]   And, you know, we did that trip a few times in Austin, Texas.
[00:32:48.360 --> 00:32:50.000]   So that was a really big milestone.
[00:32:50.000 --> 00:32:51.080]   - That was in Austin.
[00:32:51.080 --> 00:32:52.200]   - Yeah. - Cool, okay.
[00:32:52.200 --> 00:32:55.760]   - And, you know, we only, but at that time we're only,
[00:32:55.760 --> 00:32:57.760]   it took a tremendous amount of engineering.
[00:32:57.760 --> 00:32:59.720]   It took a tremendous amount of validation
[00:32:59.720 --> 00:33:01.440]   to get to that point.
[00:33:01.440 --> 00:33:04.200]   But, you know, we only did it a few times.
[00:33:04.200 --> 00:33:05.040]   Maybe we only did that.
[00:33:05.040 --> 00:33:05.920]   It was a fixed route.
[00:33:05.920 --> 00:33:07.280]   It was not kind of a controlled environment,
[00:33:07.280 --> 00:33:10.520]   but it was a fixed route and we only did a few times.
[00:33:10.520 --> 00:33:15.520]   Then in 2016, end of 2016, beginning of 2017,
[00:33:15.520 --> 00:33:20.640]   is when we founded Waymo, the company.
[00:33:20.640 --> 00:33:24.000]   That's when we, kind of, that was the next phase
[00:33:24.000 --> 00:33:26.720]   of the project where I wanted,
[00:33:26.720 --> 00:33:29.560]   we believed in kind of the commercial vision
[00:33:29.560 --> 00:33:30.880]   of this technology.
[00:33:30.880 --> 00:33:33.400]   And it made sense to create an independent entity,
[00:33:33.400 --> 00:33:35.840]   you know, within that alphabet umbrella
[00:33:35.840 --> 00:33:38.860]   to pursue this product at scale.
[00:33:39.800 --> 00:33:43.160]   Beyond that in 2017, later in 2017,
[00:33:43.160 --> 00:33:46.960]   was another really huge step for us,
[00:33:46.960 --> 00:33:49.480]   really big milestone where we started,
[00:33:49.480 --> 00:33:52.040]   I think it was October of 2017,
[00:33:52.040 --> 00:33:57.040]   where when we started regular driverless operations
[00:33:57.040 --> 00:34:00.960]   on public roads, that first day of operations,
[00:34:00.960 --> 00:34:03.920]   we drove in one day, in that first day,
[00:34:03.920 --> 00:34:06.440]   a hundred miles in driverless fashion.
[00:34:06.440 --> 00:34:08.280]   And then we've, the most important thing
[00:34:08.280 --> 00:34:09.880]   about that milestone was not that, you know,
[00:34:09.880 --> 00:34:11.080]   a hundred miles in one day,
[00:34:11.080 --> 00:34:13.800]   but that it was the start of kind of regular,
[00:34:13.800 --> 00:34:15.480]   ongoing driverless operations.
[00:34:15.480 --> 00:34:18.760]   - And when you say driverless, it means no driver.
[00:34:18.760 --> 00:34:20.480]   - That's exactly right.
[00:34:20.480 --> 00:34:22.280]   So on that first day, we actually had a mix
[00:34:22.280 --> 00:34:26.040]   and we didn't want to like, you know,
[00:34:26.040 --> 00:34:27.720]   be on YouTube and Twitter that same day.
[00:34:27.720 --> 00:34:31.000]   So in many of the rides,
[00:34:31.000 --> 00:34:33.000]   we had somebody in the driver's seat,
[00:34:33.000 --> 00:34:34.920]   but they could not disengage, like the car,
[00:34:34.920 --> 00:34:36.480]   - I gotcha. - not disengaged.
[00:34:36.480 --> 00:34:38.440]   But actually on that first day,
[00:34:38.440 --> 00:34:40.720]   some of the miles were driven
[00:34:40.720 --> 00:34:43.240]   and just completely empty driver's seat.
[00:34:43.240 --> 00:34:45.280]   - And this is the key distinction
[00:34:45.280 --> 00:34:47.160]   that I think people don't realize,
[00:34:47.160 --> 00:34:49.840]   you know, that oftentimes when you talk
[00:34:49.840 --> 00:34:51.440]   about autonomous vehicles,
[00:34:51.440 --> 00:34:54.960]   there's often a driver in the seat
[00:34:54.960 --> 00:34:58.600]   that's ready to take over,
[00:34:58.600 --> 00:35:01.040]   what's called a safety driver.
[00:35:01.040 --> 00:35:05.480]   And then Waymo is really one of the only companies,
[00:35:05.480 --> 00:35:06.480]   at least that I'm aware of,
[00:35:06.480 --> 00:35:09.720]   or at least as like boldly and carefully
[00:35:09.720 --> 00:35:13.160]   and all that is actually has cases.
[00:35:13.160 --> 00:35:15.320]   And now we'll talk about more and more
[00:35:15.320 --> 00:35:17.800]   where there's literally no driver.
[00:35:17.800 --> 00:35:20.880]   So that's another, the interesting case
[00:35:20.880 --> 00:35:22.680]   of where the driver's not supposed to disengage.
[00:35:22.680 --> 00:35:24.480]   That's like a nice middle ground.
[00:35:24.480 --> 00:35:25.320]   They're still there,
[00:35:25.320 --> 00:35:27.080]   but they're not supposed to disengage.
[00:35:27.080 --> 00:35:30.440]   But really there's the case when there's no,
[00:35:30.440 --> 00:35:32.360]   okay, there's something magical
[00:35:32.400 --> 00:35:35.280]   about there being nobody in the driver's seat.
[00:35:35.280 --> 00:35:38.520]   Like, just like to me,
[00:35:38.520 --> 00:35:42.840]   you mentioned the first time you wrote some code
[00:35:42.840 --> 00:35:45.360]   for free space navigation of the parking lot,
[00:35:45.360 --> 00:35:47.200]   that was like a magical moment.
[00:35:47.200 --> 00:35:52.000]   To me, just sort of as an observer of robots,
[00:35:52.000 --> 00:35:55.800]   the first magical moment is seeing
[00:35:55.800 --> 00:35:57.840]   an autonomous vehicle turn,
[00:35:57.840 --> 00:35:59.800]   like make a left turn,
[00:35:59.800 --> 00:36:04.800]   like apply sufficient torque to the steering wheel
[00:36:04.800 --> 00:36:07.240]   to where like there's a lot of rotation.
[00:36:07.240 --> 00:36:08.320]   And for some reason,
[00:36:08.320 --> 00:36:10.720]   and there's nobody in the driver's seat,
[00:36:10.720 --> 00:36:14.080]   for some reason that communicates
[00:36:14.080 --> 00:36:18.200]   that here's a being with power that makes a decision.
[00:36:18.200 --> 00:36:20.080]   There's something about like the steering wheel,
[00:36:20.080 --> 00:36:22.240]   'cause we perhaps romanticize the notion
[00:36:22.240 --> 00:36:23.160]   of the steering wheel.
[00:36:23.160 --> 00:36:25.760]   It's so essential to our conception,
[00:36:25.760 --> 00:36:28.120]   our 20th century conception of a car.
[00:36:28.120 --> 00:36:30.000]   And it turning the steering wheel
[00:36:30.000 --> 00:36:32.160]   with nobody in driver's seat,
[00:36:32.160 --> 00:36:35.320]   that to me, I think maybe to others,
[00:36:35.320 --> 00:36:36.640]   it's really powerful.
[00:36:36.640 --> 00:36:38.600]   Like this thing is in control.
[00:36:38.600 --> 00:36:41.280]   And then there's this leap of trust that you give,
[00:36:41.280 --> 00:36:42.760]   like I'm gonna put my life
[00:36:42.760 --> 00:36:44.720]   in the hands of this thing that's in control.
[00:36:44.720 --> 00:36:45.800]   So in that sense,
[00:36:45.800 --> 00:36:49.160]   when there's no driver in the driver's seat,
[00:36:49.160 --> 00:36:51.840]   that's a magical moment for robots.
[00:36:51.840 --> 00:36:55.840]   So I got a chance to last year to take a ride
[00:36:55.840 --> 00:36:57.920]   in a Waymo vehicle.
[00:36:57.920 --> 00:36:59.480]   And that was the magical moment.
[00:36:59.480 --> 00:37:01.800]   There's like nobody in the driver's seat.
[00:37:01.800 --> 00:37:04.680]   It's like the little details.
[00:37:04.680 --> 00:37:05.920]   You would think it doesn't matter
[00:37:05.920 --> 00:37:07.640]   whether there's a driver or not,
[00:37:07.640 --> 00:37:09.720]   but like if there's no driver
[00:37:09.720 --> 00:37:12.640]   and the steering wheel is turning on its own,
[00:37:12.640 --> 00:37:15.320]   I don't know, that's magical.
[00:37:15.320 --> 00:37:16.600]   - It's absolutely magical.
[00:37:16.600 --> 00:37:18.600]   I have taken many of these rides
[00:37:18.600 --> 00:37:20.320]   in a completely empty car.
[00:37:20.320 --> 00:37:23.120]   No human in the car pulls up.
[00:37:23.120 --> 00:37:25.000]   You call it on your cell phone, it pulls up.
[00:37:25.000 --> 00:37:27.200]   You get in, it takes you on its way.
[00:37:27.200 --> 00:37:30.600]   There's nobody in the car but you, right?
[00:37:30.600 --> 00:37:33.080]   That's something called fully driverless,
[00:37:33.080 --> 00:37:36.200]   our rider only mode of operation.
[00:37:36.200 --> 00:37:40.840]   Yeah, it is magical.
[00:37:40.840 --> 00:37:43.000]   It is transformative.
[00:37:43.000 --> 00:37:45.840]   This is what we hear from our riders.
[00:37:45.840 --> 00:37:47.720]   It kind of really changes your experience.
[00:37:47.720 --> 00:37:50.760]   And that really is what unlocks
[00:37:50.760 --> 00:37:52.760]   the real potential of this technology.
[00:37:52.760 --> 00:37:56.200]   But coming back to our journey,
[00:37:56.200 --> 00:37:58.120]   that was 2017 when we started
[00:37:58.120 --> 00:38:00.920]   truly driverless operations.
[00:38:00.920 --> 00:38:04.520]   Then in 2018, we've launched
[00:38:04.520 --> 00:38:07.440]   our public commercial service
[00:38:07.440 --> 00:38:10.080]   that we called Waymo One in Phoenix.
[00:38:10.080 --> 00:38:14.960]   In 2019, we started offering
[00:38:14.960 --> 00:38:17.400]   truly driverless rider only rides
[00:38:17.400 --> 00:38:22.000]   to our early rider population of users.
[00:38:22.000 --> 00:38:25.400]   And then 2020 has also been
[00:38:25.400 --> 00:38:27.560]   a pretty interesting year.
[00:38:27.560 --> 00:38:29.400]   One of the first ones, less about technology,
[00:38:29.400 --> 00:38:30.800]   but more about the maturing
[00:38:30.800 --> 00:38:34.120]   and the growth of Waymo as a company.
[00:38:34.120 --> 00:38:39.120]   We raised our first round of external financing this year.
[00:38:39.120 --> 00:38:40.440]   We were part of Alphabet,
[00:38:40.440 --> 00:38:44.560]   so obviously we have access to significant resources.
[00:38:44.560 --> 00:38:47.640]   But as kind of on the journey of Waymo maturing as a company,
[00:38:47.640 --> 00:38:50.120]   it made sense for us to partially go externally
[00:38:51.080 --> 00:38:52.200]   in this round.
[00:38:52.200 --> 00:38:57.200]   So we raised about $3.2 billion from that round.
[00:38:57.200 --> 00:39:03.160]   We've also started putting our fifth generation
[00:39:03.160 --> 00:39:05.440]   of our driver, our hardware,
[00:39:05.440 --> 00:39:07.520]   that is on the new vehicle,
[00:39:07.520 --> 00:39:10.440]   but it's also a qualitatively different set
[00:39:10.440 --> 00:39:12.360]   of self-driving hardware
[00:39:12.360 --> 00:39:16.480]   that is now on the JLR pace.
[00:39:16.480 --> 00:39:19.320]   So that was a very important step for us.
[00:39:19.320 --> 00:39:22.680]   - The hardware specs, fifth generation,
[00:39:22.680 --> 00:39:25.040]   I think it'd be fun to maybe,
[00:39:25.040 --> 00:39:26.600]   I apologize if I'm interrupting,
[00:39:26.600 --> 00:39:31.160]   but maybe talk about maybe the generations
[00:39:31.160 --> 00:39:33.440]   with a focus on what we're talking about
[00:39:33.440 --> 00:39:35.880]   on the fifth generation in terms of hardware specs,
[00:39:35.880 --> 00:39:37.520]   like what's on this car?
[00:39:37.520 --> 00:39:38.360]   - Sure.
[00:39:38.360 --> 00:39:40.640]   So we separated out the actual car
[00:39:40.640 --> 00:39:43.120]   that we are driving from the self-driving hardware
[00:39:43.120 --> 00:39:44.680]   we put on it.
[00:39:44.680 --> 00:39:45.880]   Right now we have,
[00:39:45.880 --> 00:39:47.800]   so this is, as I mentioned, the fifth generation.
[00:39:47.800 --> 00:39:49.880]   We've gone through,
[00:39:49.880 --> 00:39:54.880]   we started building our own hardware many, many years ago.
[00:39:54.880 --> 00:40:00.080]   And that Firefly vehicle also had the hardware suite
[00:40:00.080 --> 00:40:04.320]   that was mostly designed, engineered, and built in-house.
[00:40:04.320 --> 00:40:08.800]   Lighters are one of the more important components
[00:40:08.800 --> 00:40:11.680]   that we design and build from the ground up.
[00:40:11.680 --> 00:40:16.560]   So on the fifth generation of our drivers,
[00:40:16.560 --> 00:40:18.080]   of our self-driving hardware
[00:40:18.080 --> 00:40:20.920]   that we're switching to right now,
[00:40:20.920 --> 00:40:24.160]   we have, as with previous generations,
[00:40:24.160 --> 00:40:27.680]   in terms of sensing, we have lighters, cameras, and radars.
[00:40:27.680 --> 00:40:30.240]   And we have a pretty beefy computer
[00:40:30.240 --> 00:40:31.720]   that processes all that information
[00:40:31.720 --> 00:40:35.640]   and makes decisions in real time on board the car.
[00:40:35.640 --> 00:40:37.760]   So in all of the,
[00:40:37.760 --> 00:40:41.800]   and it's really a qualitative jump forward
[00:40:41.800 --> 00:40:45.760]   in terms of the capabilities and the various parameters
[00:40:45.760 --> 00:40:47.000]   and the specs of the hardware
[00:40:47.000 --> 00:40:48.240]   compared to what we had before
[00:40:48.240 --> 00:40:51.280]   and compared to what you can kind of get off the shelf
[00:40:51.280 --> 00:40:52.280]   in the market today.
[00:40:52.280 --> 00:40:55.360]   - Meaning from fifth to fourth or from fifth to first?
[00:40:55.360 --> 00:40:57.920]   - Definitely from first to fifth, but also from the fourth.
[00:40:57.920 --> 00:40:59.440]   - That was the world's dumbest question.
[00:40:59.440 --> 00:41:02.200]   - Definitely, definitely from fourth to fifth.
[00:41:02.200 --> 00:41:07.200]   As well as this, the last step is a big step forward.
[00:41:07.200 --> 00:41:09.160]   - So everything's in-house.
[00:41:09.160 --> 00:41:11.560]   So like LIDAR is built in-house
[00:41:11.560 --> 00:41:14.920]   and cameras are built in-house?
[00:41:15.680 --> 00:41:17.200]   - It's different.
[00:41:17.200 --> 00:41:18.080]   We work with partners.
[00:41:18.080 --> 00:41:23.080]   There's some components that we get from our manufacturing
[00:41:23.080 --> 00:41:24.760]   and supply chain partners.
[00:41:24.760 --> 00:41:28.880]   What exactly is in-house is a bit different.
[00:41:28.880 --> 00:41:33.880]   We do a lot of custom design on all of our sensing models.
[00:41:33.880 --> 00:41:36.800]   There's lighters, radars, cameras.
[00:41:36.800 --> 00:41:41.800]   Exactly, there's, lighters are almost exclusively in-house
[00:41:41.800 --> 00:41:44.480]   and some of the technologies that we have,
[00:41:44.480 --> 00:41:46.080]   some of the fundamental technologies there
[00:41:46.080 --> 00:41:49.200]   are completely unique to Waymo.
[00:41:49.200 --> 00:41:52.120]   That is also largely true about radars and cameras.
[00:41:52.120 --> 00:41:54.400]   It's a little bit more of a mix
[00:41:54.400 --> 00:41:55.720]   in terms of what we do ourselves
[00:41:55.720 --> 00:41:58.080]   versus what we get from partners.
[00:41:58.080 --> 00:42:00.960]   - Is there something super sexy about the computer
[00:42:00.960 --> 00:42:03.520]   that you can mention that's not top secret?
[00:42:03.520 --> 00:42:08.520]   Like for people who enjoy computers for, I mean,
[00:42:08.520 --> 00:42:12.280]   see, there's a lot of machine learning involved,
[00:42:12.280 --> 00:42:13.880]   but there's a lot of just basic compute.
[00:42:13.880 --> 00:42:17.880]   There's, you have to probably do a lot of signal processing
[00:42:17.880 --> 00:42:19.960]   on all the different sensors.
[00:42:19.960 --> 00:42:20.800]   You have to integrate everything.
[00:42:20.800 --> 00:42:22.040]   It has to be in real time.
[00:42:22.040 --> 00:42:25.560]   There's probably some kind of redundancy type of situation.
[00:42:25.560 --> 00:42:27.400]   Is there something interesting you could say
[00:42:27.400 --> 00:42:31.200]   about the computer for the people who love hardware?
[00:42:31.200 --> 00:42:32.720]   - It does have all of the characteristics,
[00:42:32.720 --> 00:42:34.560]   all the properties that you just mentioned.
[00:42:34.560 --> 00:42:39.560]   Redundancy, very beefy compute for general processing
[00:42:39.560 --> 00:42:43.360]   as well as inference and ML models.
[00:42:43.360 --> 00:42:45.040]   It is some of the more sensitive stuff
[00:42:45.040 --> 00:42:47.000]   that I don't wanna get into for IP reasons,
[00:42:47.000 --> 00:42:50.760]   but yeah, it can, we've shared a little bit
[00:42:50.760 --> 00:42:54.160]   in terms of the specs of the sensors
[00:42:54.160 --> 00:42:55.480]   that we have on the car.
[00:42:55.480 --> 00:42:57.280]   We've actually shared some videos
[00:42:57.280 --> 00:43:02.280]   of what our lidars see in the world.
[00:43:02.280 --> 00:43:05.280]   We have 29 cameras, we have five lidars,
[00:43:05.280 --> 00:43:08.000]   we have six radars on these vehicles,
[00:43:08.000 --> 00:43:11.040]   and you can kind of get a feel for the amount of data
[00:43:11.040 --> 00:43:11.880]   that they're producing.
[00:43:11.880 --> 00:43:14.440]   That all has to be processed in real time
[00:43:14.440 --> 00:43:17.640]   to do perception, to do complex reasoning.
[00:43:17.640 --> 00:43:18.640]   So it kind of gives you some idea
[00:43:18.640 --> 00:43:20.080]   of how beefy those computers are,
[00:43:20.080 --> 00:43:21.680]   but I don't wanna get into specifics
[00:43:21.680 --> 00:43:23.240]   of exactly how we build them.
[00:43:23.240 --> 00:43:24.960]   - Okay, well, let me try some more questions
[00:43:24.960 --> 00:43:26.760]   that you can't get into the specifics of,
[00:43:26.760 --> 00:43:29.960]   like GPU wise, is that something you can get into?
[00:43:29.960 --> 00:43:32.680]   I know that Google works with GPUs and so on.
[00:43:32.680 --> 00:43:34.520]   I mean, for machine learning folks,
[00:43:34.520 --> 00:43:36.560]   it's kind of interesting, or is there no,
[00:43:36.560 --> 00:43:40.680]   how do I ask it?
[00:43:40.680 --> 00:43:44.120]   I've been talking to people in the government about UFOs
[00:43:44.120 --> 00:43:45.640]   and they don't answer any questions.
[00:43:45.640 --> 00:43:48.560]   So this is how I feel right now asking about GPUs.
[00:43:48.560 --> 00:43:50.520]   (laughs)
[00:43:50.520 --> 00:43:53.840]   But is there something interesting that you could reveal
[00:43:53.840 --> 00:43:55.140]   or is it just, you know,
[00:43:55.140 --> 00:43:59.680]   or leave it up to our imagination, some of the compute?
[00:43:59.680 --> 00:44:02.880]   Is there any, I guess, is there any fun trickery?
[00:44:02.880 --> 00:44:05.800]   Like I talked to Chris Latner for a second time
[00:44:05.800 --> 00:44:08.240]   and he was a key person about TPUs
[00:44:08.240 --> 00:44:11.320]   and there's a lot of fun stuff going on in Google
[00:44:11.320 --> 00:44:16.160]   in terms of hardware that optimizes for machine learning.
[00:44:16.160 --> 00:44:18.080]   Is there something you can reveal
[00:44:18.080 --> 00:44:20.840]   in terms of how much, you mentioned customization,
[00:44:20.840 --> 00:44:23.840]   how much customization there is for hardware
[00:44:23.840 --> 00:44:25.640]   for machine learning purposes?
[00:44:25.640 --> 00:44:27.000]   - I'm gonna be like that government, you know,
[00:44:27.000 --> 00:44:29.920]   you've got a guy, a person who bought UFOs.
[00:44:29.920 --> 00:44:34.920]   But I, you know, I guess I will say that it's really,
[00:44:34.920 --> 00:44:37.480]   compute is really important.
[00:44:38.400 --> 00:44:43.400]   We have very data hungry and compute hungry ML models
[00:44:43.400 --> 00:44:45.000]   all over our stack.
[00:44:45.000 --> 00:44:46.960]   And this is where, you know,
[00:44:46.960 --> 00:44:51.320]   both being part of Alphabet as well as designing
[00:44:51.320 --> 00:44:54.440]   our own sensors and the entire hardware suite together
[00:44:54.440 --> 00:44:59.440]   where on one hand you get access to like really rich,
[00:44:59.440 --> 00:45:02.840]   raw sensor data that you can pipe from your sensors
[00:45:02.840 --> 00:45:06.400]   into your compute platform.
[00:45:06.400 --> 00:45:09.120]   Yeah, and build like build a whole pipe
[00:45:09.120 --> 00:45:11.560]   from sensor, raw sensor data to the big compute
[00:45:11.560 --> 00:45:15.160]   as then have the massive compute to process all that data.
[00:45:15.160 --> 00:45:18.080]   This is where we're finding that having a lot of control
[00:45:18.080 --> 00:45:22.560]   of that hardware part of the stack is really advantageous.
[00:45:22.560 --> 00:45:25.440]   - One of the fascinating magical places to me,
[00:45:25.440 --> 00:45:28.360]   again, might not be able to speak to the details,
[00:45:28.360 --> 00:45:32.080]   but it is the other compute, which is like, you know,
[00:45:32.080 --> 00:45:34.360]   this we're just talking about a single car,
[00:45:34.360 --> 00:45:38.640]   but the, you know, the driving experience
[00:45:38.640 --> 00:45:40.760]   is a source of a lot of fascinating data.
[00:45:40.760 --> 00:45:44.400]   And you have a huge amount of data coming in on the car
[00:45:44.400 --> 00:45:47.760]   and, you know, the infrastructure of storing
[00:45:47.760 --> 00:45:52.280]   some of that data to then train or to analyze or so on.
[00:45:52.280 --> 00:45:55.800]   That's a fascinating like piece of it
[00:45:55.800 --> 00:45:58.320]   that I understand a single car,
[00:45:58.320 --> 00:46:00.160]   I don't understand how you pull it all together
[00:46:00.160 --> 00:46:01.120]   in a nice way.
[00:46:01.120 --> 00:46:02.720]   Is that something that you could speak to
[00:46:02.720 --> 00:46:07.720]   in terms of the challenges of seeing the network of cars
[00:46:07.720 --> 00:46:10.920]   and then bringing the data back and analyzing things
[00:46:10.920 --> 00:46:13.960]   that like edge cases of driving,
[00:46:13.960 --> 00:46:15.880]   be able to learn on them to improve the system,
[00:46:15.880 --> 00:46:20.240]   to see where things went wrong, where things went right
[00:46:20.240 --> 00:46:21.720]   and analyze all that kind of stuff.
[00:46:21.720 --> 00:46:24.000]   Is there something interesting there
[00:46:24.000 --> 00:46:26.120]   from an engineering perspective?
[00:46:26.120 --> 00:46:30.360]   - Oh, there's an incredible amount
[00:46:30.360 --> 00:46:32.600]   of really interesting work that's happening there,
[00:46:32.600 --> 00:46:36.480]   both in the real time operation of the fleet of cars
[00:46:36.480 --> 00:46:39.440]   and the information that they exchange with each other
[00:46:39.440 --> 00:46:41.960]   in real time to make better decisions,
[00:46:41.960 --> 00:46:45.720]   as well as on the kind of the off-board component
[00:46:45.720 --> 00:46:48.520]   where you have to deal with massive amounts of data
[00:46:48.520 --> 00:46:52.600]   for training your ML models, evaluating the ML models,
[00:46:52.600 --> 00:46:55.440]   for simulating the entire system
[00:46:55.440 --> 00:46:57.800]   and for evaluating your entire system.
[00:46:57.800 --> 00:47:00.720]   And this is where being part of Alphabet
[00:47:00.720 --> 00:47:04.280]   has once again been tremendously advantageous.
[00:47:04.280 --> 00:47:07.080]   I think we consume an incredible amount of compute
[00:47:07.080 --> 00:47:09.280]   for ML infrastructure.
[00:47:09.280 --> 00:47:11.840]   We build a lot of custom frameworks to get good
[00:47:11.840 --> 00:47:17.640]   on data mining, finding the interesting edge cases
[00:47:17.640 --> 00:47:20.400]   for training and for evaluation of the system
[00:47:20.400 --> 00:47:24.040]   for both training and evaluating sub-components
[00:47:24.040 --> 00:47:27.080]   and sub parts of the system and various ML models,
[00:47:27.080 --> 00:47:31.200]   as well as evaluating the entire system and simulation.
[00:47:31.200 --> 00:47:32.880]   - Okay, is that first piece that you mentioned
[00:47:32.880 --> 00:47:36.240]   that cars communicating to each other, essentially,
[00:47:36.240 --> 00:47:38.800]   I mean, through perhaps through a centralized point,
[00:47:38.800 --> 00:47:41.640]   but what, that's fascinating too.
[00:47:41.640 --> 00:47:43.080]   How much does that help you?
[00:47:43.080 --> 00:47:45.520]   Like, if you imagine, you know, right now,
[00:47:45.520 --> 00:47:48.800]   the number of Waymo vehicles is whatever, X.
[00:47:48.800 --> 00:47:51.160]   I don't know if you can talk to what that number is,
[00:47:51.160 --> 00:47:54.240]   but it's not in the hundreds of millions yet.
[00:47:54.240 --> 00:47:57.840]   And imagine if the whole world is Waymo vehicles,
[00:47:57.840 --> 00:48:03.280]   like that changes potentially the power of connectivity.
[00:48:03.280 --> 00:48:05.840]   Like the more cars you have, I guess, actually,
[00:48:05.840 --> 00:48:08.680]   if you look at Phoenix, 'cause there's enough vehicles,
[00:48:08.680 --> 00:48:12.520]   there's enough, when there's like some level of density,
[00:48:12.520 --> 00:48:14.800]   you can start to probably do some really interesting stuff
[00:48:14.800 --> 00:48:17.800]   with the fact that cars can negotiate,
[00:48:17.800 --> 00:48:21.760]   can be, can communicate with each other
[00:48:21.760 --> 00:48:23.560]   and thereby make decisions.
[00:48:23.560 --> 00:48:27.240]   Is there something interesting there that you can talk to
[00:48:27.240 --> 00:48:29.640]   about like, how does that help with the driving problem
[00:48:29.640 --> 00:48:32.280]   from as compared to just a single car
[00:48:32.280 --> 00:48:34.560]   solving the driving problem by itself?
[00:48:34.560 --> 00:48:37.280]   - Yeah, it's a spectrum.
[00:48:37.280 --> 00:48:41.800]   I, first I'll say that, you know, it helps
[00:48:41.800 --> 00:48:44.880]   and it helps in various ways, but it's not required.
[00:48:44.880 --> 00:48:46.400]   Right now, the way we build our system,
[00:48:46.400 --> 00:48:48.080]   like each cars can operate independently,
[00:48:48.080 --> 00:48:50.280]   they can operate with no connectivity.
[00:48:50.280 --> 00:48:52.280]   So I think it is important that, you know,
[00:48:52.280 --> 00:48:55.120]   you have a fully autonomous, you know,
[00:48:55.120 --> 00:48:59.760]   fully capable driver that, you know,
[00:48:59.760 --> 00:49:02.480]   computerized driver that each car has.
[00:49:02.480 --> 00:49:05.200]   Then, you know, they do share information
[00:49:05.200 --> 00:49:06.640]   and they share information in real time.
[00:49:06.640 --> 00:49:07.720]   It really, really helps.
[00:49:07.720 --> 00:49:12.440]   All right, so the way we do this today is, you know,
[00:49:12.440 --> 00:49:15.640]   whenever one car encounters something interesting
[00:49:15.640 --> 00:49:17.640]   in the world, whether it might be an accident
[00:49:17.640 --> 00:49:19.040]   or a new construction zone,
[00:49:19.040 --> 00:49:21.800]   that information immediately gets, you know,
[00:49:21.800 --> 00:49:23.960]   uploaded over the air and it's propagated
[00:49:23.960 --> 00:49:25.120]   to the rest of the fleet.
[00:49:25.120 --> 00:49:27.200]   So, and that's kind of how we think about maps
[00:49:27.200 --> 00:49:32.200]   as priors in terms of the knowledge of our drivers,
[00:49:32.200 --> 00:49:37.040]   of our fleet of drivers that is distributed
[00:49:37.040 --> 00:49:40.000]   across the fleet and it's updated in real time.
[00:49:40.000 --> 00:49:42.200]   So that's one use case.
[00:49:42.200 --> 00:49:45.000]   You know, you can imagine as the, you know,
[00:49:45.000 --> 00:49:48.120]   the density of these vehicles go up
[00:49:48.120 --> 00:49:50.160]   that they can exchange more information
[00:49:50.160 --> 00:49:52.360]   in terms of what they're planning to do
[00:49:52.360 --> 00:49:56.120]   and start influencing how they interact with each other,
[00:49:56.120 --> 00:49:56.960]   as well as, you know,
[00:49:56.960 --> 00:49:59.600]   potentially sharing some observations, right?
[00:49:59.600 --> 00:50:00.680]   To help with, you know,
[00:50:00.680 --> 00:50:02.640]   if you have enough density of these vehicles where,
[00:50:02.640 --> 00:50:04.200]   you know, one car might be seeing something
[00:50:04.200 --> 00:50:06.440]   that another is relevant to another car
[00:50:06.440 --> 00:50:07.440]   that is very dynamic.
[00:50:07.440 --> 00:50:09.080]   You know, it's not part of kind of your updating
[00:50:09.080 --> 00:50:11.040]   your static prior of the map of the world,
[00:50:11.040 --> 00:50:12.440]   but it's more of a dynamic information
[00:50:12.440 --> 00:50:14.360]   that could be relevant to the decisions
[00:50:14.360 --> 00:50:15.720]   that another car is making real time.
[00:50:15.720 --> 00:50:17.840]   So you can see them exchanging that information
[00:50:17.840 --> 00:50:18.840]   and you can build on that.
[00:50:18.840 --> 00:50:22.080]   But again, I see that as an advantage,
[00:50:22.080 --> 00:50:25.120]   but it's, you know, not a requirement.
[00:50:25.120 --> 00:50:27.520]   - So what about the human in the loop?
[00:50:27.520 --> 00:50:32.520]   So when I got a chance to drive with a ride in a Waymo,
[00:50:32.520 --> 00:50:36.440]   you know, there's customer service.
[00:50:36.440 --> 00:50:42.720]   So like there is somebody that's able to dynamically
[00:50:42.720 --> 00:50:45.760]   like tune in and help you out.
[00:50:46.960 --> 00:50:50.720]   What role does the human play in that picture?
[00:50:50.720 --> 00:50:52.400]   That's a fascinating, like, you know,
[00:50:52.400 --> 00:50:53.720]   the idea of teleoperation,
[00:50:53.720 --> 00:50:56.320]   be able to remotely control a vehicle.
[00:50:56.320 --> 00:50:58.440]   So here, what we're talking about is like,
[00:50:58.440 --> 00:51:04.560]   like frictionless, like a human being able to,
[00:51:04.560 --> 00:51:08.520]   in a frictionless way, sort of help you out.
[00:51:08.520 --> 00:51:10.960]   I don't know if they're able to actually control the vehicle.
[00:51:10.960 --> 00:51:12.680]   Is that something you could talk to?
[00:51:12.680 --> 00:51:13.560]   - Yes. - Okay.
[00:51:13.560 --> 00:51:15.920]   - To be clear, we don't do teleoperation.
[00:51:15.920 --> 00:51:18.120]   I got to believe in teleoperation for a reason
[00:51:18.120 --> 00:51:20.760]   is that's not what we have in our cars.
[00:51:20.760 --> 00:51:22.400]   We do, as you mentioned, have, you know,
[00:51:22.400 --> 00:51:24.560]   version of customer support, you know,
[00:51:24.560 --> 00:51:25.400]   we call it life health.
[00:51:25.400 --> 00:51:28.600]   In fact, we find it that it's very important
[00:51:28.600 --> 00:51:32.320]   for our rider experience, especially if it's your first trip,
[00:51:32.320 --> 00:51:34.360]   you've never been in a fully driverless ride
[00:51:34.360 --> 00:51:37.200]   or only Waymo vehicle, you get in, there's nobody there.
[00:51:37.200 --> 00:51:40.120]   And so you can imagine having all kinds of questions
[00:51:40.120 --> 00:51:42.040]   in your head, like how this thing works.
[00:51:42.040 --> 00:51:44.080]   So we've put a lot of thought into kind of guiding
[00:51:44.080 --> 00:51:48.040]   our riders, our customers through that experience,
[00:51:48.040 --> 00:51:49.240]   especially for the first time,
[00:51:49.240 --> 00:51:51.480]   they get some information on the phone.
[00:51:51.480 --> 00:51:54.840]   If the fully driverless vehicle is used
[00:51:54.840 --> 00:51:58.000]   to service their trip, when you get into the car,
[00:51:58.000 --> 00:52:01.680]   we have an in-car screen and audio that kind of guides them
[00:52:01.680 --> 00:52:04.520]   and explains what to expect.
[00:52:04.520 --> 00:52:06.960]   They also have a button that they can push
[00:52:06.960 --> 00:52:09.600]   that will connect them to, you know,
[00:52:09.600 --> 00:52:12.800]   a real life human being that they can talk to, right,
[00:52:12.800 --> 00:52:14.200]   about this whole process.
[00:52:14.200 --> 00:52:15.800]   So that's one aspect of it.
[00:52:15.800 --> 00:52:19.040]   There is, you know, I should mention that there is
[00:52:19.040 --> 00:52:23.320]   another function that humans provide to our cars,
[00:52:23.320 --> 00:52:24.640]   but it's not teleoperation.
[00:52:24.640 --> 00:52:26.400]   You can think of it a little bit more like, you know,
[00:52:26.400 --> 00:52:28.000]   fleet assistance, kind of like, you know,
[00:52:28.000 --> 00:52:32.840]   traffic control that you have, where our cars,
[00:52:32.840 --> 00:52:34.960]   again, they're responsible on their own
[00:52:34.960 --> 00:52:36.960]   for making all of the decisions,
[00:52:36.960 --> 00:52:37.920]   all of the driving decisions
[00:52:37.920 --> 00:52:39.440]   that don't require connectivity.
[00:52:39.560 --> 00:52:43.520]   They, you know, anything that is safety or latency critical
[00:52:43.520 --> 00:52:46.800]   is done, you know, purely autonomously by onboard,
[00:52:46.800 --> 00:52:49.200]   our onboard system.
[00:52:49.200 --> 00:52:50.720]   But there are situations where, you know,
[00:52:50.720 --> 00:52:52.560]   if connectivity is available,
[00:52:52.560 --> 00:52:54.600]   and a car encounters a particularly challenging situation,
[00:52:54.600 --> 00:52:58.320]   you can imagine like a super hairy scene of an accident,
[00:52:58.320 --> 00:52:59.920]   the cars will do their best.
[00:52:59.920 --> 00:53:02.400]   They will recognize that it's an off-nominal situation.
[00:53:02.400 --> 00:53:06.240]   They will, you know, do their best to come up, you know,
[00:53:06.240 --> 00:53:07.200]   with the right interpretation,
[00:53:07.200 --> 00:53:08.840]   the best course of action in that scenario.
[00:53:08.840 --> 00:53:10.160]   But if connectivity is available,
[00:53:10.160 --> 00:53:12.720]   they can ask for confirmation from, you know,
[00:53:12.720 --> 00:53:16.440]   a human-mode human assistant
[00:53:16.440 --> 00:53:18.320]   to kind of confirm those actions,
[00:53:18.320 --> 00:53:20.720]   and, you know, perhaps provide a little bit
[00:53:20.720 --> 00:53:23.040]   of kind of contextual information and guidance.
[00:53:23.040 --> 00:53:26.560]   - So October 8th was when you're talking about the,
[00:53:26.560 --> 00:53:31.560]   was Waymo launched the fully self,
[00:53:31.560 --> 00:53:36.320]   the public version of its fully driverless,
[00:53:36.320 --> 00:53:39.720]   that's the right term, I think, service in Phoenix.
[00:53:39.720 --> 00:53:40.640]   Is that October 8th?
[00:53:40.640 --> 00:53:41.480]   - That's right.
[00:53:41.480 --> 00:53:43.480]   It was the introduction of fully driverless
[00:53:43.480 --> 00:53:46.680]   rider-only vehicles into our public Waymo One service.
[00:53:46.680 --> 00:53:48.640]   - Okay, so that's amazing.
[00:53:48.640 --> 00:53:52.560]   So it's like anybody can get into Waymo in Phoenix?
[00:53:52.560 --> 00:53:53.400]   - That's right.
[00:53:53.400 --> 00:53:56.680]   So we previously had early people
[00:53:56.680 --> 00:53:58.480]   in our early rider program
[00:53:58.480 --> 00:54:01.080]   taking fully driverless rides in Phoenix.
[00:54:01.080 --> 00:54:05.280]   And just this, a little while ago,
[00:54:05.280 --> 00:54:06.320]   we opened on October 8th,
[00:54:06.320 --> 00:54:09.280]   we opened that mode of operation to the public.
[00:54:09.280 --> 00:54:12.280]   So I can download the app and go on the ride.
[00:54:12.280 --> 00:54:16.760]   There is a lot more demand right now for that service
[00:54:16.760 --> 00:54:18.240]   than we have capacity.
[00:54:18.240 --> 00:54:20.040]   So we're kind of managing that,
[00:54:20.040 --> 00:54:21.600]   but that's exactly the way you described it.
[00:54:21.600 --> 00:54:22.520]   - Yeah, well, that's interesting.
[00:54:22.520 --> 00:54:25.720]   So there's more demand than you can handle.
[00:54:25.720 --> 00:54:29.920]   Like what has been the reception so far?
[00:54:29.920 --> 00:54:32.360]   Like what, I mean, okay, so, you know,
[00:54:32.360 --> 00:54:36.120]   that's, this is a product, right?
[00:54:36.120 --> 00:54:38.080]   That's a whole nother discussion of like
[00:54:38.080 --> 00:54:40.160]   how compelling of a product it is.
[00:54:40.160 --> 00:54:42.160]   Great, but it's also like one of the most
[00:54:42.160 --> 00:54:45.480]   kind of transformational technologies of the 21st century.
[00:54:45.480 --> 00:54:48.440]   So it's also like a tourist attraction.
[00:54:48.440 --> 00:54:51.400]   Like it's fun to, you know, to be a part of it.
[00:54:51.400 --> 00:54:54.320]   So it'd be interesting to see like, what do people say?
[00:54:54.320 --> 00:54:58.960]   What do people, what have been the feedback so far?
[00:54:58.960 --> 00:54:59.880]   - You know, still early days,
[00:54:59.880 --> 00:55:03.840]   but so far the feedback has been incredible,
[00:55:03.840 --> 00:55:05.520]   incredibly positive.
[00:55:05.520 --> 00:55:07.760]   They, you know, we asked them for feedback during the ride.
[00:55:07.760 --> 00:55:10.560]   We asked them for feedback after the ride
[00:55:10.560 --> 00:55:11.440]   as part of their trip.
[00:55:11.440 --> 00:55:12.520]   You know, we asked them some questions.
[00:55:12.520 --> 00:55:13.360]   We asked them to, you know,
[00:55:13.360 --> 00:55:15.840]   rate the performance of our driver.
[00:55:15.840 --> 00:55:17.960]   Most by far, you know, most of our drivers
[00:55:17.960 --> 00:55:20.800]   give us five stars in our app,
[00:55:20.800 --> 00:55:23.800]   which is absolutely great to see.
[00:55:23.800 --> 00:55:24.840]   And you know, that's, and we're,
[00:55:24.840 --> 00:55:26.240]   they're also giving us feedback on, you know,
[00:55:26.240 --> 00:55:27.760]   things we can improve.
[00:55:27.760 --> 00:55:29.400]   And, you know, that's one of the main reasons
[00:55:29.400 --> 00:55:30.280]   we're doing this with Phoenix.
[00:55:30.280 --> 00:55:32.240]   And, you know, over the last couple of years
[00:55:32.240 --> 00:55:33.960]   and every day today,
[00:55:33.960 --> 00:55:37.240]   we are just learning a tremendous amount of new stuff
[00:55:37.240 --> 00:55:38.160]   from our users.
[00:55:38.160 --> 00:55:41.760]   There's no substitute for actually doing the real thing,
[00:55:41.760 --> 00:55:43.760]   actually having a fully driverless product
[00:55:43.760 --> 00:55:45.480]   out there in the field with, you know,
[00:55:45.480 --> 00:55:47.640]   users that are actually, you know,
[00:55:47.640 --> 00:55:49.920]   paying us money to get from point A to point B.
[00:55:49.920 --> 00:55:51.960]   - So this is a legitimate, like,
[00:55:51.960 --> 00:55:53.000]   this is a paid service.
[00:55:53.000 --> 00:55:53.840]   - That's right.
[00:55:53.840 --> 00:55:57.600]   - And the idea is you use the app to go from point A
[00:55:57.600 --> 00:56:00.160]   to point B, and then what are the A's?
[00:56:00.160 --> 00:56:02.600]   What are the, what's the freedom of the,
[00:56:02.600 --> 00:56:05.440]   of the starting and ending places?
[00:56:05.440 --> 00:56:09.040]   - It's an area of geography where that service is enabled.
[00:56:09.040 --> 00:56:12.800]   It's a decent size of geography of territory.
[00:56:12.800 --> 00:56:14.240]   It's actually larger than, you know,
[00:56:14.240 --> 00:56:16.400]   the size of San Francisco.
[00:56:16.400 --> 00:56:19.720]   And, you know, within that, you have full freedom
[00:56:19.720 --> 00:56:21.840]   of, you know, selecting where you want to go.
[00:56:21.840 --> 00:56:23.080]   You know, of course there are some,
[00:56:23.080 --> 00:56:25.760]   and you, on your app, you get a map,
[00:56:25.760 --> 00:56:29.440]   you tell the car where you want to be picked up,
[00:56:29.440 --> 00:56:31.640]   you know, where you want the car to pull over
[00:56:31.640 --> 00:56:32.720]   and pick you up, and then you tell it
[00:56:32.720 --> 00:56:34.240]   where you want to be dropped off, right?
[00:56:34.240 --> 00:56:35.760]   And of course there are some exclusions, right?
[00:56:35.760 --> 00:56:37.080]   You don't want to be, you know,
[00:56:37.080 --> 00:56:39.920]   where in terms of where the car is allowed to pull over,
[00:56:39.920 --> 00:56:41.280]   right, so, you know, that you can't do,
[00:56:41.280 --> 00:56:43.120]   but, you know, besides that, it's-
[00:56:43.120 --> 00:56:43.960]   - Amazing.
[00:56:43.960 --> 00:56:45.400]   - It's not like a fixed, just would be very,
[00:56:45.400 --> 00:56:46.240]   I guess, I don't know,
[00:56:46.240 --> 00:56:47.640]   maybe that's what's the question behind your question,
[00:56:47.640 --> 00:56:49.840]   but it's not a, you know, preset set of-
[00:56:49.840 --> 00:56:52.760]   - Yes, I guess, so within the geographic constraints
[00:56:52.760 --> 00:56:54.720]   with that, within that area, anywhere else,
[00:56:54.720 --> 00:56:57.440]   it can be picked up and dropped off anywhere.
[00:56:57.440 --> 00:56:59.160]   - That's right, and, you know, people use them
[00:56:59.160 --> 00:57:00.960]   on like all kinds of trips.
[00:57:00.960 --> 00:57:03.720]   They, we have, and we have an incredible spectrum of riders.
[00:57:03.720 --> 00:57:05.840]   I think the youngest, actually have car seats in them,
[00:57:05.840 --> 00:57:07.880]   and we have, you know, people taking their kids on rides.
[00:57:07.880 --> 00:57:09.800]   I think the youngest riders we had on cars
[00:57:09.800 --> 00:57:11.440]   are, you know, one or two years old, you know,
[00:57:11.440 --> 00:57:12.800]   and the full spectrum of use cases.
[00:57:12.800 --> 00:57:16.240]   People, you can take them to, you know, schools,
[00:57:16.240 --> 00:57:19.060]   to, you know, go grocery shopping,
[00:57:19.060 --> 00:57:22.760]   to restaurants, to bars, you know, run errands,
[00:57:22.760 --> 00:57:24.040]   you know, go shopping, et cetera, et cetera.
[00:57:24.040 --> 00:57:25.800]   You can go to your office, right?
[00:57:25.800 --> 00:57:27.240]   Like the full spectrum of use cases,
[00:57:27.240 --> 00:57:31.480]   and people, you're gonna use them in their daily lives
[00:57:31.480 --> 00:57:35.240]   to get around, and we see all kinds of, you know,
[00:57:35.240 --> 00:57:37.160]   really interesting use cases,
[00:57:37.160 --> 00:57:38.920]   and that's what's providing us
[00:57:38.920 --> 00:57:41.880]   incredibly valuable experience
[00:57:41.880 --> 00:57:44.520]   that we then, you know, use to improve our product.
[00:57:44.520 --> 00:57:49.520]   - So as somebody who's been on, done a few long rants
[00:57:49.520 --> 00:57:53.160]   with Joe Rogan and others about the toxicity
[00:57:53.160 --> 00:57:55.040]   of the internet and the comments,
[00:57:55.040 --> 00:57:56.800]   and the negativity in the comments,
[00:57:56.800 --> 00:57:58.080]   I'm fascinated by feedback.
[00:57:58.080 --> 00:58:03.080]   I believe that most people are good and kind and intelligent
[00:58:03.080 --> 00:58:08.280]   and can provide, like, even in disagreement,
[00:58:08.280 --> 00:58:09.520]   really fascinating ideas.
[00:58:09.520 --> 00:58:12.600]   So on a product side, it's fascinating to me,
[00:58:12.600 --> 00:58:16.960]   like, how do you get the richest possible user feedback,
[00:58:16.960 --> 00:58:18.680]   like, to improve?
[00:58:18.680 --> 00:58:22.280]   What are the channels that you use
[00:58:22.280 --> 00:58:25.020]   to measure, 'cause like, you're no longer,
[00:58:25.020 --> 00:58:29.160]   that's one of the magical things about autonomous vehicles,
[00:58:29.160 --> 00:58:32.280]   is it's not, like, it's frictionless interaction
[00:58:32.280 --> 00:58:35.120]   with the human, so like, you don't get to,
[00:58:35.120 --> 00:58:36.920]   you know, it's just giving a ride.
[00:58:36.920 --> 00:58:39.280]   So like, how do you get feedback from people
[00:58:39.280 --> 00:58:40.760]   in order to improve?
[00:58:40.760 --> 00:58:42.160]   - Oh, yeah, great question.
[00:58:42.160 --> 00:58:43.800]   Various mechanisms.
[00:58:43.800 --> 00:58:47.160]   So as part of the normal flow, we ask people for feedback.
[00:58:47.160 --> 00:58:49.040]   They, as the car is driving around,
[00:58:49.040 --> 00:58:51.120]   you know, we have on the phone and in the car,
[00:58:51.120 --> 00:58:53.280]   and we have a touchscreen in the car,
[00:58:53.280 --> 00:58:54.480]   you can actually click some buttons
[00:58:54.480 --> 00:58:58.560]   and provide real-time feedback on how the car is doing
[00:58:58.560 --> 00:59:00.640]   and how the car is handling a particular situation,
[00:59:00.640 --> 00:59:02.160]   you know, both positive and negative.
[00:59:02.160 --> 00:59:03.680]   So that's one channel.
[00:59:03.680 --> 00:59:06.640]   We have, as we discussed, customer support or life help,
[00:59:06.640 --> 00:59:08.800]   where, you know, if a customer wants to,
[00:59:08.800 --> 00:59:12.200]   has a question or he has some sort of concern,
[00:59:12.200 --> 00:59:14.960]   they can talk to a person in real time.
[00:59:14.960 --> 00:59:19.080]   So that is another mechanism that gives us feedback.
[00:59:19.080 --> 00:59:20.680]   At the end of a trip, you know,
[00:59:20.680 --> 00:59:22.760]   we also ask them how things went.
[00:59:22.760 --> 00:59:25.840]   They give us comments and, you know, a star rating.
[00:59:25.840 --> 00:59:28.280]   And, you know, if it's, we also, you know,
[00:59:28.280 --> 00:59:31.800]   ask them to explain what, you know,
[00:59:31.800 --> 00:59:34.160]   went well and, you know, what could be improved.
[00:59:34.160 --> 00:59:39.000]   And we have, our riders are providing,
[00:59:39.000 --> 00:59:41.520]   you know, very rich feedback there.
[00:59:41.520 --> 00:59:44.480]   A lot, a large fraction is very passionate
[00:59:44.480 --> 00:59:45.720]   and very excited about this technology.
[00:59:45.720 --> 00:59:47.320]   So we get really good feedback.
[00:59:47.320 --> 00:59:49.640]   We also run UXR studies, right?
[00:59:49.640 --> 00:59:53.000]   You know, specific that are kind of more, you know,
[00:59:53.000 --> 00:59:55.760]   go more in depth and we will run both kind of lateral
[00:59:55.760 --> 01:00:00.440]   and longitudinal studies where we have deeper engagement
[01:00:00.440 --> 01:00:01.440]   with our customers.
[01:00:01.440 --> 01:00:04.360]   You know, we have our user experience research team
[01:00:04.360 --> 01:00:05.680]   tracking over time.
[01:00:05.680 --> 01:00:07.600]   That's when you say about longitudinal, it's cool.
[01:00:07.600 --> 01:00:08.720]   - That's exactly right.
[01:00:08.720 --> 01:00:12.000]   And, you know, that's another really valuable feedback,
[01:00:12.000 --> 01:00:12.840]   source of feedback.
[01:00:12.840 --> 01:00:16.440]   And we're just covering a tremendous amount, right?
[01:00:16.440 --> 01:00:19.480]   People go grocery shopping and they like want to load,
[01:00:19.480 --> 01:00:21.480]   you know, 20 bags of groceries in our cars.
[01:00:21.480 --> 01:00:24.040]   And like that's one workflow that you maybe don't,
[01:00:24.040 --> 01:00:27.680]   you know, think about, you know, getting just right
[01:00:27.680 --> 01:00:30.280]   when you're building the driverless product.
[01:00:30.280 --> 01:00:35.000]   I have people like, you know, who bike as part
[01:00:35.000 --> 01:00:35.840]   of their trip.
[01:00:35.840 --> 01:00:36.920]   So they, you know, bike somewhere,
[01:00:36.920 --> 01:00:39.240]   then they get in our cars, they take apart their bike,
[01:00:39.240 --> 01:00:40.520]   they load into our vehicle, then they go,
[01:00:40.520 --> 01:00:42.240]   and that's, you know, how they, you know,
[01:00:42.240 --> 01:00:44.600]   where we want to pull over and how that, you know,
[01:00:44.600 --> 01:00:48.480]   get in and get out process works,
[01:00:48.480 --> 01:00:50.760]   provides us very useful feedback.
[01:00:50.760 --> 01:00:53.520]   In terms of, you know, what makes a good pickup
[01:00:53.520 --> 01:00:57.320]   and drop off location, we get really valuable feedback.
[01:00:57.320 --> 01:01:01.720]   And in fact, we had to do some really interesting work
[01:01:01.720 --> 01:01:06.000]   with high definition maps and thinking about
[01:01:06.000 --> 01:01:07.040]   walking directions.
[01:01:07.040 --> 01:01:09.120]   And if you imagine you're in a store, right?
[01:01:09.120 --> 01:01:10.560]   In some giant space, and then, you know,
[01:01:10.560 --> 01:01:12.400]   you want to be picked up somewhere.
[01:01:12.400 --> 01:01:14.920]   If you just drop a pin at a current location,
[01:01:14.920 --> 01:01:16.760]   which is maybe in the middle of a shopping mall,
[01:01:16.760 --> 01:01:18.960]   like what's the best location for the car
[01:01:18.960 --> 01:01:20.000]   to come pick you up?
[01:01:20.000 --> 01:01:22.160]   And you can have simple heuristics where you just kind of
[01:01:22.160 --> 01:01:24.040]   take your, you know, you clean in distance
[01:01:24.040 --> 01:01:27.480]   and find the nearest spot where the car can pull over
[01:01:27.480 --> 01:01:28.320]   that's closest to you.
[01:01:28.320 --> 01:01:30.000]   But oftentimes that's not the most convenient one.
[01:01:30.000 --> 01:01:32.200]   You know, I have many anecdotes where that heuristic
[01:01:32.200 --> 01:01:33.680]   breaks in horrible ways.
[01:01:33.680 --> 01:01:38.680]   One example that I often mention is somebody wanted to be,
[01:01:39.000 --> 01:01:43.960]   you know, dropped off in Phoenix and, you know,
[01:01:43.960 --> 01:01:47.960]   we got car picked a location that was close,
[01:01:47.960 --> 01:01:49.600]   I think the closest to their, you know,
[01:01:49.600 --> 01:01:51.960]   where the pin was dropped on the map in terms of,
[01:01:51.960 --> 01:01:53.640]   you know, latitude and longitude.
[01:01:53.640 --> 01:01:57.680]   But it happened to be on the other side of a parking lot
[01:01:57.680 --> 01:02:00.640]   that had this row of cacti and the poor person had to like
[01:02:00.640 --> 01:02:02.960]   walk all around the parking lot to get to where they wanted
[01:02:02.960 --> 01:02:04.680]   to be in 110 degree heat.
[01:02:04.680 --> 01:02:06.160]   So that, you know, that was the bottom.
[01:02:06.160 --> 01:02:08.680]   So then, you know, we took all, take all of these,
[01:02:08.680 --> 01:02:11.800]   all of that feedback from our users and incorporate it
[01:02:11.800 --> 01:02:14.000]   into our system and improve it.
[01:02:14.000 --> 01:02:16.600]   - Yeah, I feel like that's like requires AGI
[01:02:16.600 --> 01:02:19.320]   to solve the problem of like, when you're,
[01:02:19.320 --> 01:02:20.920]   which is a very common case,
[01:02:20.920 --> 01:02:23.640]   when you're in a big space of some kind,
[01:02:23.640 --> 01:02:25.120]   like apartment building, it doesn't matter.
[01:02:25.120 --> 01:02:27.040]   It's just not some large space.
[01:02:27.040 --> 01:02:31.000]   And then you call the, like the Waymo from there, right?
[01:02:31.000 --> 01:02:32.800]   It's like, whatever, it doesn't matter,
[01:02:32.800 --> 01:02:34.960]   a ride share vehicle.
[01:02:34.960 --> 01:02:38.200]   And like, where's the pin supposed to drop?
[01:02:39.200 --> 01:02:41.200]   I feel like that's, you don't think,
[01:02:41.200 --> 01:02:42.720]   I think that requires AGI.
[01:02:42.720 --> 01:02:47.320]   I'm gonna, in order to solve, okay, the alternative,
[01:02:47.320 --> 01:02:50.680]   which I think the Google search engine has taught
[01:02:50.680 --> 01:02:53.640]   is like, there's something really valuable
[01:02:53.640 --> 01:02:57.440]   about the perhaps slightly dumb answer,
[01:02:57.440 --> 01:02:59.680]   but a really powerful one, which is like,
[01:02:59.680 --> 01:03:02.360]   what was done in the past by others?
[01:03:02.360 --> 01:03:04.600]   Like, what was the choice made by others?
[01:03:04.600 --> 01:03:07.640]   That seems to be like, in terms of Google search,
[01:03:07.640 --> 01:03:09.720]   when you have like billions of searches,
[01:03:09.720 --> 01:03:13.520]   that you could see which, like when they recommend
[01:03:13.520 --> 01:03:15.120]   what you might possibly mean,
[01:03:15.120 --> 01:03:18.360]   they suggest based on not some machine learning thing,
[01:03:18.360 --> 01:03:19.880]   which they also do, but like,
[01:03:19.880 --> 01:03:22.720]   on what was successful for others in the past
[01:03:22.720 --> 01:03:24.800]   and finding a thing that they were happy with.
[01:03:24.800 --> 01:03:27.280]   Is that integrated at all with Waymo?
[01:03:27.280 --> 01:03:30.320]   Like, what pickups worked for others?
[01:03:30.320 --> 01:03:31.240]   - It is.
[01:03:31.240 --> 01:03:32.200]   I think you're exactly right.
[01:03:32.200 --> 01:03:35.560]   So there's a real, it's an interesting problem.
[01:03:35.560 --> 01:03:40.560]   Naive solutions have interesting failure modes.
[01:03:40.560 --> 01:03:46.680]   So there's definitely lots of things
[01:03:46.680 --> 01:03:49.440]   that can be done to improve
[01:03:49.440 --> 01:03:54.840]   and both learning from what works,
[01:03:54.840 --> 01:03:56.400]   what doesn't work in actual hail
[01:03:56.400 --> 01:03:59.000]   from getting richer data and getting more information
[01:03:59.000 --> 01:04:02.280]   about the environment and richer maps.
[01:04:02.280 --> 01:04:04.280]   But you're absolutely right that there's something,
[01:04:04.280 --> 01:04:05.880]   I think there's some properties of solutions
[01:04:05.880 --> 01:04:09.000]   that in terms of the effect that they have on users,
[01:04:09.000 --> 01:04:10.680]   some are much, much, much better than others, right?
[01:04:10.680 --> 01:04:13.760]   And predictability and understandability is important.
[01:04:13.760 --> 01:04:14.880]   So you can have maybe something
[01:04:14.880 --> 01:04:16.200]   that is not quite as optimal,
[01:04:16.200 --> 01:04:19.040]   but is very natural and predictable to the user
[01:04:19.040 --> 01:04:22.400]   and kind of works the same way all the time.
[01:04:22.400 --> 01:04:23.640]   And that matters.
[01:04:23.640 --> 01:04:26.680]   That matters a lot for the user experience.
[01:04:26.680 --> 01:04:28.640]   - But to get to the basics,
[01:04:28.640 --> 01:04:31.840]   the pretty fundamental property
[01:04:31.840 --> 01:04:36.200]   is that the car actually arrives where you told it to ride.
[01:04:36.200 --> 01:04:37.960]   Like you can always change it, see it on the map
[01:04:37.960 --> 01:04:40.360]   and you can move it around if you don't like it.
[01:04:40.360 --> 01:04:43.280]   But like that property that the car actually shows up
[01:04:43.280 --> 01:04:45.760]   on the pin is critical,
[01:04:45.760 --> 01:04:50.760]   which where compared to some of the human driven analogs,
[01:04:50.760 --> 01:04:54.360]   I think you can have more predictability.
[01:04:54.360 --> 01:04:55.920]   It's actually the fact,
[01:04:55.920 --> 01:04:59.080]   if I do a little bit of a detour here,
[01:04:59.360 --> 01:05:02.560]   I think the fact that it's your phone and the car,
[01:05:02.560 --> 01:05:04.280]   it's two computers talking to each other
[01:05:04.280 --> 01:05:07.120]   can lead to some really interesting things we can do
[01:05:07.120 --> 01:05:09.160]   in terms of the user interfaces,
[01:05:09.160 --> 01:05:10.880]   both in terms of function,
[01:05:10.880 --> 01:05:14.240]   like the car actually shows up exactly where you told it
[01:05:14.240 --> 01:05:15.200]   you want it to be,
[01:05:15.200 --> 01:05:17.000]   but also some really interesting things
[01:05:17.000 --> 01:05:17.880]   on the user interface,
[01:05:17.880 --> 01:05:18.880]   like as the car is driving,
[01:05:18.880 --> 01:05:21.920]   as you call it and it's on the way to come and pick you up.
[01:05:21.920 --> 01:05:23.400]   And of course you get the position of the car
[01:05:23.400 --> 01:05:25.280]   and the route on the map,
[01:05:25.280 --> 01:05:27.960]   but, and they actually follow that route, of course,
[01:05:27.960 --> 01:05:30.120]   but it can also share some really interesting information
[01:05:30.120 --> 01:05:31.120]   about what it's doing.
[01:05:31.120 --> 01:05:35.800]   So, you know, our cars, as they are coming to pick you up,
[01:05:35.800 --> 01:05:38.280]   if it's come, if a car is coming up to a stop sign,
[01:05:38.280 --> 01:05:40.440]   it will actually show you that like it's there sitting
[01:05:40.440 --> 01:05:42.040]   because it's at a stop sign or a traffic light
[01:05:42.040 --> 01:05:43.000]   will show you that it's got, you know,
[01:05:43.000 --> 01:05:44.000]   sitting at a red light.
[01:05:44.000 --> 01:05:46.440]   So, you know, they're like little things, right?
[01:05:46.440 --> 01:05:49.760]   But it's, I find those little touch,
[01:05:49.760 --> 01:05:53.760]   touches really interesting, really magical.
[01:05:53.760 --> 01:05:54.600]   And it's just, you know,
[01:05:54.600 --> 01:05:56.160]   little things like that that you can do
[01:05:56.160 --> 01:05:57.840]   to kind of delight your users.
[01:05:57.840 --> 01:06:00.600]   - You know, this makes me think of,
[01:06:00.600 --> 01:06:04.800]   there's some products that I just love.
[01:06:04.800 --> 01:06:09.000]   Like there's a company called Rev,
[01:06:09.000 --> 01:06:13.560]   Rev.com where I like for this podcast, for example,
[01:06:13.560 --> 01:06:16.160]   I can just drag and drop a video
[01:06:16.160 --> 01:06:19.480]   and then they do all the captioning.
[01:06:19.480 --> 01:06:21.240]   It's humans doing the captioning,
[01:06:21.240 --> 01:06:24.040]   but they connect, they automate everything
[01:06:24.040 --> 01:06:25.360]   of connecting you to the humans
[01:06:25.360 --> 01:06:27.360]   and they do the captioning and transcription.
[01:06:27.360 --> 01:06:28.840]   It's all effortless.
[01:06:28.840 --> 01:06:30.880]   And it like, I remember when I first started using them,
[01:06:30.880 --> 01:06:34.560]   it was like, life's good.
[01:06:34.560 --> 01:06:38.600]   Like, because it was so painful to figure that out earlier.
[01:06:38.600 --> 01:06:42.400]   The same thing with something called iZotope RX,
[01:06:42.400 --> 01:06:44.400]   this company I use for cleaning up audio,
[01:06:44.400 --> 01:06:46.400]   like the sound cleanup they do,
[01:06:46.400 --> 01:06:47.520]   it's like drag and drop
[01:06:47.520 --> 01:06:51.040]   and it just cleans everything up very nicely.
[01:06:51.040 --> 01:06:52.440]   Another experience like that I had
[01:06:52.440 --> 01:06:55.760]   with Amazon OneClick purchase first time,
[01:06:55.760 --> 01:06:57.560]   I mean, other places do that now,
[01:06:57.560 --> 01:07:00.280]   but just the effortlessness of purchasing,
[01:07:00.280 --> 01:07:02.000]   making it frictionless,
[01:07:02.000 --> 01:07:04.200]   it kind of communicates to me,
[01:07:04.200 --> 01:07:08.000]   like I'm a fan of design, I'm a fan of products,
[01:07:08.000 --> 01:07:11.840]   that you can just create a really pleasant experience.
[01:07:11.840 --> 01:07:13.560]   The simplicity of it, the elegance
[01:07:13.560 --> 01:07:15.560]   just makes you fall in love with it.
[01:07:15.560 --> 01:07:18.880]   So, do you think about this kind of stuff?
[01:07:18.880 --> 01:07:21.600]   I mean, it's exactly what we've been talking about.
[01:07:21.600 --> 01:07:23.280]   It's like the little details
[01:07:23.280 --> 01:07:25.640]   that somehow make you fall in love with the product.
[01:07:25.640 --> 01:07:29.560]   Is that, we went from like urban challenge days
[01:07:29.560 --> 01:07:33.720]   where love was not part of the conversation probably,
[01:07:33.720 --> 01:07:38.720]   and to this point where there's human beings
[01:07:38.720 --> 01:07:42.600]   and you want them to fall in love with the experience.
[01:07:42.600 --> 01:07:44.600]   Is that something you're trying to optimize for,
[01:07:44.600 --> 01:07:45.440]   trying to think about,
[01:07:45.440 --> 01:07:48.560]   like how do you create an experience that people love?
[01:07:48.560 --> 01:07:49.760]   - Oh, absolutely.
[01:07:49.760 --> 01:07:54.760]   I think that's the vision is removing any friction
[01:07:54.760 --> 01:08:00.200]   or complexity from getting our users,
[01:08:00.200 --> 01:08:03.400]   our writers to where they wanna go.
[01:08:03.400 --> 01:08:06.560]   And making that as simple as possible.
[01:08:06.560 --> 01:08:09.480]   And then, beyond that, just transportation,
[01:08:09.480 --> 01:08:12.720]   making things and goods get to their destination
[01:08:12.720 --> 01:08:14.080]   as seamlessly as possible.
[01:08:14.080 --> 01:08:16.880]   I talked about a drag and drop experience
[01:08:16.880 --> 01:08:18.120]   where I kind of express your intent
[01:08:18.120 --> 01:08:20.560]   and then it just magically happens.
[01:08:20.560 --> 01:08:23.040]   And for our writers, that's what we're trying to get to
[01:08:23.040 --> 01:08:28.040]   is you download an app and you click and car shows up.
[01:08:28.040 --> 01:08:29.560]   It's the same car.
[01:08:29.560 --> 01:08:31.000]   It's very predictable.
[01:08:31.000 --> 01:08:34.960]   It's a safe and high quality experience.
[01:08:34.960 --> 01:08:38.160]   And then it gets you in a very reliable,
[01:08:38.160 --> 01:08:43.160]   very convenient, frictionless way to where you wanna be.
[01:08:43.160 --> 01:08:46.440]   And along the journey,
[01:08:46.560 --> 01:08:48.880]   I think we also want to do little things
[01:08:48.880 --> 01:08:51.360]   to delight our users.
[01:08:51.360 --> 01:08:54.360]   - Like the ride sharing companies,
[01:08:54.360 --> 01:08:57.920]   because they don't control the experience, I think,
[01:08:57.920 --> 01:09:00.440]   they can't make people fall in love necessarily
[01:09:00.440 --> 01:09:02.160]   with the experience.
[01:09:02.160 --> 01:09:04.360]   Or maybe they haven't put in the effort.
[01:09:04.360 --> 01:09:07.200]   But I think if I were to speak
[01:09:07.200 --> 01:09:09.840]   to the ride sharing experience I currently have,
[01:09:09.840 --> 01:09:13.320]   it's just very convenient.
[01:09:13.320 --> 01:09:18.000]   But there's a lot of room for falling in love with it.
[01:09:18.000 --> 01:09:20.120]   We can speak to sort of car companies.
[01:09:20.120 --> 01:09:21.200]   Car companies do this well.
[01:09:21.200 --> 01:09:22.840]   You can fall in love with a car, right?
[01:09:22.840 --> 01:09:26.520]   And be like a loyal car person, like whatever.
[01:09:26.520 --> 01:09:30.320]   Like I like badass hot rods, I guess '69 Corvette.
[01:09:30.320 --> 01:09:33.480]   And at this point, you can't really...
[01:09:33.480 --> 01:09:37.680]   Cars are so, owning a car is so 20th century, man.
[01:09:37.680 --> 01:09:41.800]   But is there something about the Waymo experience
[01:09:41.800 --> 01:09:44.000]   where you hope that people will fall in love with it?
[01:09:44.000 --> 01:09:46.320]   Is that part of it?
[01:09:46.320 --> 01:09:51.400]   Or is it just about making a convenient ride,
[01:09:51.400 --> 01:09:53.400]   not ride sharing, I don't know what the right term is,
[01:09:53.400 --> 01:09:58.400]   but just a convenient A to B autonomous transport?
[01:09:58.400 --> 01:10:03.680]   Or like, do you want them to fall in love with Waymo?
[01:10:03.680 --> 01:10:05.400]   So maybe elaborate a little bit.
[01:10:05.400 --> 01:10:07.640]   I mean, almost like from a business perspective,
[01:10:07.640 --> 01:10:11.880]   I'm curious, like how...
[01:10:11.880 --> 01:10:14.560]   Do you wanna be in the background invisible?
[01:10:14.560 --> 01:10:19.560]   Or do you want to be like a source of joy
[01:10:19.560 --> 01:10:22.400]   that's in very much in the foreground?
[01:10:22.400 --> 01:10:26.040]   - I want to provide the best,
[01:10:26.040 --> 01:10:28.920]   most enjoyable transportation solution.
[01:10:28.920 --> 01:10:33.920]   And that means building it, building our product
[01:10:34.920 --> 01:10:37.480]   and building our service in a way that people do.
[01:10:37.480 --> 01:10:43.400]   Kind of use in a very seamless, frictionless way
[01:10:43.400 --> 01:10:46.920]   in their day-to-day lives.
[01:10:46.920 --> 01:10:48.960]   And I think that does mean,
[01:10:48.960 --> 01:10:51.440]   in some way falling in love in that product, right?
[01:10:51.440 --> 01:10:54.000]   It just kind of becomes part of your routine.
[01:10:54.000 --> 01:10:58.480]   It comes down in my mind to safety,
[01:10:58.480 --> 01:11:03.480]   predictability of the experience and privacy, I think.
[01:11:03.920 --> 01:11:06.560]   Privacy, I think, aspects of it, right?
[01:11:06.560 --> 01:11:09.920]   Our cars, you get the same car,
[01:11:09.920 --> 01:11:12.320]   you get very predictable behavior.
[01:11:12.320 --> 01:11:14.320]   And that is important.
[01:11:14.320 --> 01:11:17.160]   I think if you're gonna use it in your daily life.
[01:11:17.160 --> 01:11:18.800]   Privacy, I mean, when you're in a car,
[01:11:18.800 --> 01:11:19.640]   you can do other things.
[01:11:19.640 --> 01:11:21.760]   You're spending a bunch, just another space
[01:11:21.760 --> 01:11:24.600]   where you're spending a significant part of your life.
[01:11:24.600 --> 01:11:27.640]   And so not having to share it with other people
[01:11:27.640 --> 01:11:28.880]   who you don't wanna share it with,
[01:11:28.880 --> 01:11:32.360]   I think is a very nice property.
[01:11:32.360 --> 01:11:33.640]   Maybe you wanna take a phone call
[01:11:33.640 --> 01:11:35.440]   or do something else in the vehicle.
[01:11:35.440 --> 01:11:40.760]   And safety on the quality of the driving,
[01:11:40.760 --> 01:11:45.240]   as well as the physical safety of not having to share
[01:11:45.240 --> 01:11:49.380]   that ride is important to a lot of people.
[01:11:49.380 --> 01:11:54.380]   - What about the idea that when there's somebody,
[01:11:54.380 --> 01:11:58.040]   like a human driving and they do a rolling stop
[01:11:58.040 --> 01:11:59.640]   on a stop sign, like sometimes,
[01:12:01.400 --> 01:12:04.400]   you get an Uber or Lyft or whatever, like human driver,
[01:12:04.400 --> 01:12:09.120]   and they can be a little bit aggressive as drivers.
[01:12:09.120 --> 01:12:12.660]   It feels like there is, not all aggression is bad.
[01:12:12.660 --> 01:12:16.240]   Now that may be a wrong, again,
[01:12:16.240 --> 01:12:18.480]   20th century conception of driving.
[01:12:18.480 --> 01:12:21.320]   Maybe it's possible to create a driving experience.
[01:12:21.320 --> 01:12:24.520]   Like if you're in the back, busy doing something,
[01:12:24.520 --> 01:12:26.640]   maybe aggression is not a good thing.
[01:12:26.640 --> 01:12:29.320]   It's a very different kind of experience perhaps.
[01:12:29.320 --> 01:12:33.560]   But it feels like in order to navigate this world,
[01:12:33.560 --> 01:12:38.200]   you need to, how do I phrase this?
[01:12:38.200 --> 01:12:39.960]   You need to kind of bend the rules a little bit,
[01:12:39.960 --> 01:12:42.120]   or at least like test the rules.
[01:12:42.120 --> 01:12:44.680]   I don't know what language politicians use to discuss this,
[01:12:44.680 --> 01:12:47.600]   but whatever language they use,
[01:12:47.600 --> 01:12:49.600]   you like flirt with the rules, I don't know.
[01:12:49.600 --> 01:12:54.600]   But like you sort of have a bit of an aggressive way
[01:12:56.000 --> 01:13:00.640]   of driving that asserts your presence in this world,
[01:13:00.640 --> 01:13:02.480]   thereby making other vehicles
[01:13:02.480 --> 01:13:04.760]   and people respect your presence,
[01:13:04.760 --> 01:13:06.960]   and thereby allowing you to sort of navigate
[01:13:06.960 --> 01:13:09.800]   through intersections in a timely fashion.
[01:13:09.800 --> 01:13:11.200]   I don't know if any of that made sense,
[01:13:11.200 --> 01:13:15.080]   but like how does that fit into the experience
[01:13:15.080 --> 01:13:17.300]   of driving autonomously?
[01:13:17.300 --> 01:13:19.560]   Is that-- - It's a lot of stuff.
[01:13:19.560 --> 01:13:21.360]   This is, you're hitting on a very important point
[01:13:21.360 --> 01:13:26.360]   of a number of behavioral components
[01:13:26.360 --> 01:13:31.360]   and parameters that make your driving feel assertive
[01:13:31.360 --> 01:13:36.720]   and natural and comfortable and predictable.
[01:13:36.720 --> 01:13:38.400]   Now, our cars will follow rules, right?
[01:13:38.400 --> 01:13:40.440]   They will do the safest thing possible in all situations,
[01:13:40.440 --> 01:13:42.160]   let me be clear on that.
[01:13:42.160 --> 01:13:46.800]   But if you think of really, really good drivers,
[01:13:46.800 --> 01:13:49.320]   just think about professional lemon drivers, right?
[01:13:49.320 --> 01:13:50.480]   They will follow the rules.
[01:13:50.480 --> 01:13:55.480]   They're very, very smooth, and yet they're very efficient.
[01:13:55.480 --> 01:13:57.200]   But they're assertive.
[01:13:57.200 --> 01:14:00.640]   They're comfortable for the people in the vehicle.
[01:14:00.640 --> 01:14:03.360]   They're predictable for the other people outside the vehicle
[01:14:03.360 --> 01:14:04.640]   that they share the environment with.
[01:14:04.640 --> 01:14:07.360]   And that's the kind of driver that we want to build.
[01:14:07.360 --> 01:14:11.920]   And you think if maybe there's a sport analogy there,
[01:14:11.920 --> 01:14:15.560]   you can do in very many sports,
[01:14:15.560 --> 01:14:19.920]   the true professionals are very efficient
[01:14:19.920 --> 01:14:21.160]   in their movements, right?
[01:14:21.160 --> 01:14:25.200]   So they don't do like hectic flailing, right?
[01:14:25.200 --> 01:14:28.760]   They're smooth and precise, right?
[01:14:28.760 --> 01:14:29.680]   And they get the best results.
[01:14:29.680 --> 01:14:31.480]   So that's the kind of driver that we want to build.
[01:14:31.480 --> 01:14:33.000]   In terms of aggressiveness, yeah,
[01:14:33.000 --> 01:14:34.920]   you can like roll through the stop signs.
[01:14:34.920 --> 01:14:37.160]   You can do crazy lane changes.
[01:14:37.160 --> 01:14:39.280]   Typically doesn't get you to your destination faster.
[01:14:39.280 --> 01:14:41.760]   Typically not the safest or most predictable,
[01:14:41.760 --> 01:14:44.880]   or most comfortable thing to do.
[01:14:44.880 --> 01:14:48.280]   But there is a way to do both.
[01:14:48.280 --> 01:14:50.760]   And that's what we're trying to build,
[01:14:50.760 --> 01:14:54.880]   the driver that is safe, comfortable,
[01:14:54.880 --> 01:14:57.480]   smooth, and predictable.
[01:14:57.480 --> 01:14:59.640]   - Yeah, that's a really interesting distinction.
[01:14:59.640 --> 01:15:02.280]   I think in the early days of autonomous vehicles,
[01:15:02.280 --> 01:15:05.720]   the vehicles felt cautious as opposed to efficient.
[01:15:05.720 --> 01:15:10.720]   And still probably, but when I rode in the Waymo,
[01:15:10.720 --> 01:15:14.480]   I mean, it was quite assertive.
[01:15:14.480 --> 01:15:17.520]   It moved pretty quickly.
[01:15:17.520 --> 01:15:21.240]   And like, yeah, then he's one of the surprising feelings
[01:15:21.240 --> 01:15:23.760]   was that it actually, it went fast
[01:15:23.760 --> 01:15:28.440]   and it didn't feel like awkwardly cautious
[01:15:28.440 --> 01:15:29.640]   than autonomous vehicle.
[01:15:29.640 --> 01:15:32.960]   So I've also programmed autonomous vehicles
[01:15:32.960 --> 01:15:36.480]   and everything I've ever built was felt awkwardly,
[01:15:36.480 --> 01:15:39.160]   either overly aggressive, okay?
[01:15:39.160 --> 01:15:41.240]   Especially when it was my code,
[01:15:41.240 --> 01:15:46.240]   or like awkwardly cautious is the way I would put it.
[01:15:46.320 --> 01:15:51.320]   And Waymo's vehicle felt like assertive
[01:15:51.320 --> 01:15:57.680]   and I think efficient is like the right terminology here.
[01:15:57.680 --> 01:16:01.640]   They wasn't, and I also like the professional limo driver.
[01:16:01.640 --> 01:16:04.120]   'Cause we often think like, you know,
[01:16:04.120 --> 01:16:08.240]   an Uber driver or a bus driver or a taxi.
[01:16:08.240 --> 01:16:10.240]   This is the funny thing is people think
[01:16:10.240 --> 01:16:12.880]   like taxi drivers are professionals.
[01:16:15.080 --> 01:16:18.400]   I mean, it's like, that's like saying
[01:16:18.400 --> 01:16:19.760]   I'm a professional walker
[01:16:19.760 --> 01:16:22.080]   just because I've been walking all my life.
[01:16:22.080 --> 01:16:25.160]   I think there's an art to it, right?
[01:16:25.160 --> 01:16:28.160]   And if you take it seriously as an art form,
[01:16:28.160 --> 01:16:32.680]   then there's a certain way that mastery looks like.
[01:16:32.680 --> 01:16:33.960]   And it's interesting to think about
[01:16:33.960 --> 01:16:37.000]   what does mastery look like in driving?
[01:16:37.000 --> 01:16:41.680]   And perhaps what we associate with like aggressiveness
[01:16:41.680 --> 01:16:44.720]   is unnecessary, like it's not part
[01:16:44.720 --> 01:16:46.960]   of the experience of driving.
[01:16:46.960 --> 01:16:51.960]   It's like unnecessary fluff that efficiency,
[01:16:51.960 --> 01:16:57.040]   you can be, you can create a good driving experience
[01:16:57.040 --> 01:16:59.060]   within the rules.
[01:16:59.060 --> 01:17:03.240]   That's, I mean, you're the first person to tell me this.
[01:17:03.240 --> 01:17:04.520]   So it's kind of interesting.
[01:17:04.520 --> 01:17:05.800]   I need to think about this,
[01:17:05.800 --> 01:17:08.120]   but that's exactly what it felt like with Waymo.
[01:17:08.120 --> 01:17:09.200]   I kind of had this intuition,
[01:17:09.200 --> 01:17:11.420]   maybe it's the Russian thing, I don't know,
[01:17:11.420 --> 01:17:14.480]   that you have to break the rules in life to get anywhere.
[01:17:14.480 --> 01:17:19.640]   But maybe, maybe it's possible that that's not the case
[01:17:19.640 --> 01:17:22.120]   in driving.
[01:17:22.120 --> 01:17:23.280]   I have to think about that.
[01:17:23.280 --> 01:17:25.720]   But it certainly felt that way on the streets of Phoenix
[01:17:25.720 --> 01:17:27.440]   when I was there in Waymo,
[01:17:27.440 --> 01:17:29.960]   that that was a very pleasant experience
[01:17:29.960 --> 01:17:32.280]   and it wasn't frustrating in that like,
[01:17:32.280 --> 01:17:34.280]   come on, move already kind of feeling.
[01:17:34.280 --> 01:17:36.240]   It wasn't, that wasn't there.
[01:17:36.240 --> 01:17:38.940]   - Yeah, I mean, that's what we're going after.
[01:17:38.940 --> 01:17:40.000]   I don't think you have to pick one.
[01:17:40.000 --> 01:17:42.240]   I think truly good driving,
[01:17:42.240 --> 01:17:44.920]   it gives you both efficiency, a certainness,
[01:17:44.920 --> 01:17:47.360]   but also comfort and predictability and safety.
[01:17:47.360 --> 01:17:53.360]   And that's what fundamental improvements
[01:17:53.360 --> 01:17:57.480]   in the core capabilities truly unlock.
[01:17:57.480 --> 01:17:59.640]   And you can kind of think of it as,
[01:17:59.640 --> 01:18:01.520]   precision and recall trade-off.
[01:18:01.520 --> 01:18:03.200]   You have certain capabilities of your model,
[01:18:03.200 --> 01:18:05.620]   and then it's very easy when you have some curve
[01:18:05.620 --> 01:18:07.280]   of precision and recall, you can move things around
[01:18:07.280 --> 01:18:08.560]   and can choose your operating point
[01:18:08.560 --> 01:18:10.240]   in your training of precision versus recall,
[01:18:10.240 --> 01:18:12.680]   false positives versus false negatives.
[01:18:12.680 --> 01:18:15.720]   But then, and you can tune things on that curve
[01:18:15.720 --> 01:18:17.520]   and be kind of more cautious or more aggressive,
[01:18:17.520 --> 01:18:19.920]   but then aggressive is bad or cautious is bad.
[01:18:19.920 --> 01:18:22.360]   But true capabilities come from actually moving
[01:18:22.360 --> 01:18:24.120]   the whole curve up.
[01:18:24.120 --> 01:18:28.220]   And then you are on a very different plane
[01:18:28.220 --> 01:18:29.480]   of those trade-offs.
[01:18:29.480 --> 01:18:31.640]   And that's what we're trying to do here
[01:18:31.640 --> 01:18:33.460]   is to move the whole curve up.
[01:18:33.460 --> 01:18:36.800]   - Before I forget, let's talk about trucks a little bit.
[01:18:37.880 --> 01:18:39.560]   So I also got a chance to check out
[01:18:39.560 --> 01:18:42.000]   some of the Waymo trucks.
[01:18:42.000 --> 01:18:45.860]   I'm not sure if we wanna go too much into that space,
[01:18:45.860 --> 01:18:46.840]   but it's a fascinating one.
[01:18:46.840 --> 01:18:48.840]   So maybe we can mention at least briefly,
[01:18:48.840 --> 01:18:53.040]   Waymo is also now doing autonomous trucking.
[01:18:53.040 --> 01:18:57.960]   And how different, like philosophically and technically
[01:18:57.960 --> 01:19:00.240]   is that whole space of problems?
[01:19:00.240 --> 01:19:04.520]   - It's one of our two big products
[01:19:04.520 --> 01:19:08.280]   and commercial applications of our driver,
[01:19:08.280 --> 01:19:10.920]   ride hailing and deliveries.
[01:19:10.920 --> 01:19:12.800]   We have Waymo One and Waymo Via,
[01:19:12.800 --> 01:19:14.920]   moving people and moving goods.
[01:19:14.920 --> 01:19:18.800]   Trucking is an example of moving goods.
[01:19:18.800 --> 01:19:22.360]   We've been working on trucking since 2017.
[01:19:22.360 --> 01:19:28.280]   It is a very interesting space.
[01:19:28.280 --> 01:19:32.560]   And your question of how different is it?
[01:19:32.560 --> 01:19:34.200]   It has this really nice property
[01:19:34.200 --> 01:19:37.120]   that the first order challenges,
[01:19:37.120 --> 01:19:40.120]   like the science, the hard engineering,
[01:19:40.120 --> 01:19:43.120]   whether it's hardware or onboard software
[01:19:43.120 --> 01:19:44.480]   or off-board software,
[01:19:44.480 --> 01:19:46.520]   all of the systems that you build
[01:19:46.520 --> 01:19:48.720]   for training your ML models,
[01:19:48.720 --> 01:19:51.400]   for evaluating your entire system,
[01:19:51.400 --> 01:19:53.760]   those fundamentals carry over.
[01:19:53.760 --> 01:19:58.760]   The true challenges of driving, perception,
[01:19:58.760 --> 01:20:01.400]   semantic understanding, prediction,
[01:20:01.400 --> 01:20:04.640]   decision making, planning, evaluation,
[01:20:04.640 --> 01:20:08.000]   the simulator, ML infrastructure, those carry over.
[01:20:08.000 --> 01:20:09.520]   Like the data and the application
[01:20:09.520 --> 01:20:12.880]   and kind of the domains might be different,
[01:20:12.880 --> 01:20:15.440]   but the most difficult problems,
[01:20:15.440 --> 01:20:18.160]   all of that carries over between the domains.
[01:20:18.160 --> 01:20:19.360]   So that's very nice.
[01:20:19.360 --> 01:20:20.400]   So that's how we approach it.
[01:20:20.400 --> 01:20:23.320]   We're kind of build investing in the core,
[01:20:23.320 --> 01:20:25.160]   the technical core.
[01:20:25.160 --> 01:20:29.920]   And then there is specialization of that core technology
[01:20:29.920 --> 01:20:31.160]   to different product lines,
[01:20:31.160 --> 01:20:33.040]   to different commercial applications.
[01:20:33.040 --> 01:20:36.440]   So on, just to tease it apart a little bit,
[01:20:36.440 --> 01:20:39.160]   on trucks, so starting with the hardware,
[01:20:39.160 --> 01:20:42.120]   the configuration of the sensors is different.
[01:20:42.120 --> 01:20:45.880]   They're different physically, geometrically,
[01:20:45.880 --> 01:20:47.640]   different vehicles.
[01:20:47.640 --> 01:20:50.880]   So for example, we have two of our main laser
[01:20:50.880 --> 01:20:53.200]   on the trucks on both sides,
[01:20:53.200 --> 01:20:55.680]   so that we have, not have the blind spots.
[01:20:55.680 --> 01:20:59.880]   Whereas on the JLR I-PACE, we have one of it,
[01:20:59.880 --> 01:21:01.160]   sitting at the very top,
[01:21:01.160 --> 01:21:04.440]   but the actual sensors are almost the same
[01:21:04.440 --> 01:21:05.680]   or largely the same.
[01:21:05.680 --> 01:21:09.680]   So all of the investment that over the years
[01:21:09.680 --> 01:21:12.040]   we've put into building our custom lighters,
[01:21:12.040 --> 01:21:14.200]   custom radars, pulling the whole system together,
[01:21:14.200 --> 01:21:16.280]   that carries over very nicely.
[01:21:16.280 --> 01:21:18.200]   Then on the perception side,
[01:21:18.200 --> 01:21:21.440]   the fundamental challenges of seeing,
[01:21:21.440 --> 01:21:22.320]   understanding the world,
[01:21:22.320 --> 01:21:24.720]   whether it's object detection, classification,
[01:21:24.720 --> 01:21:27.960]   tracking, semantic understanding, all that carries over.
[01:21:27.960 --> 01:21:29.360]   Yes, there's some specialization
[01:21:29.360 --> 01:21:31.200]   when you're driving on freeways,
[01:21:31.200 --> 01:21:33.240]   range becomes more important,
[01:21:33.240 --> 01:21:34.480]   the domain is a little bit different,
[01:21:34.480 --> 01:21:38.200]   but again, the fundamentals carry over very, very nicely.
[01:21:38.200 --> 01:21:41.120]   Same, and I guess you get into prediction
[01:21:41.120 --> 01:21:42.480]   or decision-making, right?
[01:21:42.480 --> 01:21:45.920]   The fundamentals of what it takes to predict
[01:21:45.920 --> 01:21:48.280]   what other people are going to do,
[01:21:48.280 --> 01:21:51.440]   to find the long tail, to improve your system
[01:21:51.440 --> 01:21:54.760]   in that long tail of behavior prediction and response,
[01:21:54.760 --> 01:21:55.720]   that carries over, right?
[01:21:55.720 --> 01:21:56.720]   And so on and so on.
[01:21:56.720 --> 01:21:59.080]   So, I mean, that's pretty exciting.
[01:21:59.080 --> 01:22:01.920]   By the way, does Waymo VIA include
[01:22:01.920 --> 01:22:06.200]   using the smaller vehicles for transportation goods?
[01:22:06.200 --> 01:22:07.600]   That's an interesting distinction.
[01:22:07.600 --> 01:22:11.160]   So, I would say there's three interesting
[01:22:11.160 --> 01:22:13.080]   modes of operation.
[01:22:13.080 --> 01:22:15.720]   So, one is moving humans, one is moving goods,
[01:22:15.720 --> 01:22:19.680]   and one is like moving nothing, zero occupancy,
[01:22:19.680 --> 01:22:22.960]   meaning like you're going to the destination,
[01:22:22.960 --> 01:22:25.440]   you're empty vehicle.
[01:22:25.440 --> 01:22:27.280]   I mean, it's-
[01:22:27.280 --> 01:22:28.680]   - The third is the less of it,
[01:22:28.680 --> 01:22:29.520]   that's the entirety of it,
[01:22:29.520 --> 01:22:32.120]   it's the less exciting from the commercial perspective.
[01:22:32.120 --> 01:22:34.400]   (both laughing)
[01:22:34.400 --> 01:22:36.920]   - Well, I mean, in terms of like,
[01:22:36.920 --> 01:22:40.120]   if you think about what's inside a vehicle as it's moving,
[01:22:40.120 --> 01:22:44.240]   because it does, some significant fraction
[01:22:44.240 --> 01:22:48.760]   of the vehicle's movement has to be empty.
[01:22:48.760 --> 01:22:50.120]   I mean, it's kind of fascinating,
[01:22:50.120 --> 01:22:51.680]   maybe just on that small point,
[01:22:51.680 --> 01:22:56.680]   is there different control and like,
[01:22:57.640 --> 01:23:01.480]   policies that are applied for zero occupancy vehicle?
[01:23:01.480 --> 01:23:03.400]   So, a vehicle with nothing in it,
[01:23:03.400 --> 01:23:06.320]   or is it just move as if there is a person inside,
[01:23:06.320 --> 01:23:09.680]   what was with some subtle differences?
[01:23:09.680 --> 01:23:13.000]   - As a first order approximation, there are no differences.
[01:23:13.000 --> 01:23:17.000]   And if you think about, you know, safety and, you know,
[01:23:17.000 --> 01:23:19.720]   comfort and quality of driving, only part of it,
[01:23:23.440 --> 01:23:27.400]   has to do with the people or the goods
[01:23:27.400 --> 01:23:28.800]   inside of the vehicle, right?
[01:23:28.800 --> 01:23:30.120]   But you don't wanna be, you know,
[01:23:30.120 --> 01:23:32.080]   you wanna drive smoothly, as we discussed,
[01:23:32.080 --> 01:23:34.600]   not for the purely for the benefit of, you know,
[01:23:34.600 --> 01:23:36.280]   whatever you have inside the car, right?
[01:23:36.280 --> 01:23:39.200]   It's also for the benefit of the people outside
[01:23:39.200 --> 01:23:41.720]   kind of feeding, fitting naturally and predictably
[01:23:41.720 --> 01:23:43.240]   into that whole environment, right?
[01:23:43.240 --> 01:23:45.520]   So, you know, yes, there are some second order things
[01:23:45.520 --> 01:23:47.680]   you can do, you can change your route,
[01:23:47.680 --> 01:23:50.920]   and, you know, optimize maybe kind of your fleet,
[01:23:50.920 --> 01:23:52.480]   things at the fleet scale,
[01:23:52.480 --> 01:23:54.560]   and you would take into account whether, you know,
[01:23:54.560 --> 01:23:58.320]   some of your cars are actually, you know,
[01:23:58.320 --> 01:24:01.040]   serving a useful trip, whether with people or with goods,
[01:24:01.040 --> 01:24:03.440]   whereas, you know, other cars are, you know,
[01:24:03.440 --> 01:24:04.760]   driving completely empty, you know,
[01:24:04.760 --> 01:24:08.860]   to that next valuable trip that they're going to provide,
[01:24:08.860 --> 01:24:11.560]   but that those are mostly second order effects.
[01:24:11.560 --> 01:24:12.520]   - Okay, cool.
[01:24:12.520 --> 01:24:16.760]   So Phoenix is an incredible place,
[01:24:16.760 --> 01:24:21.120]   and what you've announced in Phoenix is,
[01:24:21.120 --> 01:24:23.320]   it's kind of amazing, but, you know,
[01:24:23.320 --> 01:24:24.920]   that's just like one city.
[01:24:24.920 --> 01:24:27.560]   How do you take over the world?
[01:24:27.560 --> 01:24:32.000]   I mean, I'm asking for a friend.
[01:24:32.000 --> 01:24:33.740]   - One step at a time.
[01:24:33.740 --> 01:24:36.160]   - One city at a time.
[01:24:36.160 --> 01:24:38.600]   - Is that the cartoon pinky in the brain?
[01:24:38.600 --> 01:24:39.440]   Okay.
[01:24:39.440 --> 01:24:42.920]   - But, you know, gradually is a true answer.
[01:24:42.920 --> 01:24:47.360]   - So I think the heart of your question is, you know, what--
[01:24:47.360 --> 01:24:49.720]   - Can you ask a better question than I asked?
[01:24:49.720 --> 01:24:50.560]   - You're asking a great question.
[01:24:50.560 --> 01:24:51.760]   - Answer that one.
[01:24:51.760 --> 01:24:54.200]   - I'm, you know, just gonna, you know,
[01:24:54.200 --> 01:24:56.760]   phrase it in the terms that I want to answer.
[01:24:56.760 --> 01:24:58.000]   - Answer it, perfect.
[01:24:58.000 --> 01:24:59.840]   That's exactly right, brilliant.
[01:24:59.840 --> 01:25:00.880]   Please.
[01:25:00.880 --> 01:25:03.000]   - No, you know, where are we today?
[01:25:03.000 --> 01:25:04.720]   And, you know, what happens next?
[01:25:04.720 --> 01:25:07.440]   And what does it take to go beyond Phoenix?
[01:25:07.440 --> 01:25:11.840]   And what does it take to get this technology
[01:25:11.840 --> 01:25:15.960]   to more places and more people around the world, right?
[01:25:15.960 --> 01:25:20.960]   - So our next big area of focus
[01:25:20.960 --> 01:25:26.640]   is exactly that, larger scale commercialization
[01:25:26.640 --> 01:25:28.200]   and just, you know, scaling up.
[01:25:28.200 --> 01:25:35.200]   If I think about, you know, the main,
[01:25:35.200 --> 01:25:39.120]   and, you know, Phoenix gives us that platform
[01:25:39.120 --> 01:25:44.120]   and gives us that foundation upon which we can build.
[01:25:44.200 --> 01:25:49.200]   And it's, there are a few really challenging aspects
[01:25:49.200 --> 01:25:54.200]   of this whole problem that you have to pull together
[01:25:54.200 --> 01:25:57.520]   in order to, you know, build the technology,
[01:25:57.520 --> 01:26:02.440]   in order to deploy it into the field,
[01:26:02.440 --> 01:26:07.440]   to go from a driverless car to a fleet of cars
[01:26:07.440 --> 01:26:10.440]   that are providing a service,
[01:26:10.440 --> 01:26:13.160]   and then all the way to, you know, commercialization.
[01:26:13.160 --> 01:26:15.720]   So, and, you know, this is what we have in Phoenix.
[01:26:15.720 --> 01:26:20.080]   We've taken the technology from a proof point
[01:26:20.080 --> 01:26:21.120]   to an actual deployment,
[01:26:21.120 --> 01:26:24.840]   and have taken our driver, you know, from, you know,
[01:26:24.840 --> 01:26:27.140]   one car to a fleet that can provide a service.
[01:26:27.140 --> 01:26:31.040]   Beyond that, if I think about what it will take
[01:26:31.040 --> 01:26:36.040]   to scale up and, you know, deploy in, you know,
[01:26:36.040 --> 01:26:38.520]   more places with more customers,
[01:26:38.520 --> 01:26:43.520]   I tend to think about three main dimensions,
[01:26:43.520 --> 01:26:46.600]   three main axes of scale.
[01:26:46.600 --> 01:26:49.640]   One is the core technology, you know,
[01:26:49.640 --> 01:26:53.640]   the hardware and software core capabilities of our driver.
[01:26:53.640 --> 01:26:58.640]   The second dimension is evaluation and deployment.
[01:26:58.640 --> 01:27:01.960]   And the third one is just the, you know,
[01:27:01.960 --> 01:27:04.920]   product, commercial, and operational excellence.
[01:27:05.960 --> 01:27:09.360]   So you can talk a bit about where we are along, you know,
[01:27:09.360 --> 01:27:10.440]   each one of those three dimensions,
[01:27:10.440 --> 01:27:12.120]   about where we are today and, you know,
[01:27:12.120 --> 01:27:14.120]   what has, what will happen next.
[01:27:14.120 --> 01:27:17.880]   On, you know, the core technology,
[01:27:17.880 --> 01:27:19.800]   on, you know, the hardware and software, you know,
[01:27:19.800 --> 01:27:22.280]   together, comprised of driver,
[01:27:22.280 --> 01:27:26.920]   we, you know, obviously have that foundation
[01:27:26.920 --> 01:27:31.560]   that is providing fully driverless trips to our customers
[01:27:31.560 --> 01:27:32.920]   as we speak, in fact.
[01:27:34.320 --> 01:27:38.480]   And we've learned a tremendous amount from that.
[01:27:38.480 --> 01:27:42.120]   So now what we're doing is we are incorporating
[01:27:42.120 --> 01:27:45.920]   all those lessons into some pretty fundamental improvements
[01:27:45.920 --> 01:27:48.040]   in our core technology, both on the hardware side
[01:27:48.040 --> 01:27:50.160]   and on the software side,
[01:27:50.160 --> 01:27:53.360]   to build a more general, more robust solution
[01:27:53.360 --> 01:27:55.960]   that then will enable us to massively scale,
[01:27:55.960 --> 01:27:57.080]   you know, beyond Phoenix.
[01:27:57.080 --> 01:28:00.120]   So on the hardware side,
[01:28:00.120 --> 01:28:04.440]   all of those lessons are now incorporated
[01:28:04.440 --> 01:28:07.160]   into this fifth generation hardware platform
[01:28:07.160 --> 01:28:10.960]   that is, you know, being deployed right now.
[01:28:10.960 --> 01:28:13.720]   And that's the platform, the fourth generation,
[01:28:13.720 --> 01:28:15.560]   the thing that we have right now driving in Phoenix,
[01:28:15.560 --> 01:28:18.840]   it's good enough to operate fully driverless,
[01:28:18.840 --> 01:28:20.680]   you know, night and day, you know,
[01:28:20.680 --> 01:28:22.600]   various speeds and various conditions.
[01:28:22.600 --> 01:28:25.120]   But the fifth generation is the platform
[01:28:25.120 --> 01:28:27.240]   upon which we want to go to massive scale.
[01:28:28.360 --> 01:28:31.920]   We've really made qualitative improvements
[01:28:31.920 --> 01:28:34.360]   in terms of the capability of the system,
[01:28:34.360 --> 01:28:36.120]   the simplicity of the architecture,
[01:28:36.120 --> 01:28:38.200]   the reliability of the redundancy.
[01:28:38.200 --> 01:28:41.840]   It is designed to be manufacturable at very large scale
[01:28:41.840 --> 01:28:43.440]   and, you know, provides the right unit economics.
[01:28:43.440 --> 01:28:48.080]   So that's the next big step for us on the hardware side.
[01:28:48.080 --> 01:28:50.800]   - That's already there for scale, the version five.
[01:28:50.800 --> 01:28:51.640]   - That's right.
[01:28:51.640 --> 01:28:53.560]   - And is that a coincidence
[01:28:53.560 --> 01:28:55.360]   or should we look into a conspiracy theory
[01:28:55.360 --> 01:28:57.680]   that it's the same version as the Pixel phone?
[01:28:58.080 --> 01:28:59.040]   (laughs)
[01:28:59.040 --> 01:28:59.880]   Is that what's the hardware?
[01:28:59.880 --> 01:29:03.000]   - I can neither confirm nor deny, Lux.
[01:29:03.000 --> 01:29:03.840]   - All right, cool.
[01:29:03.840 --> 01:29:04.680]   So, sorry.
[01:29:04.680 --> 01:29:07.040]   So that's the, okay, that's that axis.
[01:29:07.040 --> 01:29:08.280]   What else?
[01:29:08.280 --> 01:29:09.120]   - So similarly, you know,
[01:29:09.120 --> 01:29:11.520]   the hardware is a very discreet jump,
[01:29:11.520 --> 01:29:15.040]   but similar to how we're making that change
[01:29:15.040 --> 01:29:16.880]   from the fourth generation hardware to the fifth,
[01:29:16.880 --> 01:29:19.040]   we're making similar improvements on the software side
[01:29:19.040 --> 01:29:21.080]   to make it more robust and more general
[01:29:21.080 --> 01:29:24.640]   and allow us to kind of quickly scale beyond Phoenix.
[01:29:24.640 --> 01:29:26.560]   So that's the first dimension of core technology.
[01:29:26.560 --> 01:29:28.880]   The second dimension is evaluation and deployment.
[01:29:28.880 --> 01:29:33.880]   Now, how do you measure your system?
[01:29:33.880 --> 01:29:35.040]   How do you evaluate it?
[01:29:35.040 --> 01:29:37.520]   How do you build a release and deployment process
[01:29:37.520 --> 01:29:39.960]   where, you know, with confidence,
[01:29:39.960 --> 01:29:42.840]   you can regularly release new versions
[01:29:42.840 --> 01:29:45.480]   of your driver into a fleet?
[01:29:45.480 --> 01:29:49.040]   How do you get good at it so that it is not, you know,
[01:29:49.040 --> 01:29:52.480]   a huge tax on your researchers and engineers that, you know,
[01:29:52.480 --> 01:29:54.240]   so you can, how do you build all of these, you know,
[01:29:54.240 --> 01:29:57.960]   processes, the frameworks, the simulation, the evaluation,
[01:29:57.960 --> 01:29:59.760]   the data science, the validation,
[01:29:59.760 --> 01:30:02.440]   so that, you know, people can focus on improving the system
[01:30:02.440 --> 01:30:04.720]   and kind of the releases just go out the door
[01:30:04.720 --> 01:30:06.640]   and get deployed across the fleet.
[01:30:06.640 --> 01:30:09.640]   So we've gotten really good at that in Phoenix.
[01:30:09.640 --> 01:30:13.080]   That's been a tremendously difficult problem,
[01:30:13.080 --> 01:30:14.920]   but that's what we have in Phoenix right now
[01:30:14.920 --> 01:30:16.240]   that gives us that foundation.
[01:30:16.240 --> 01:30:18.320]   And now we're working on kind of incorporating
[01:30:18.320 --> 01:30:19.880]   all the lessons that we've learned
[01:30:19.880 --> 01:30:22.320]   to make it more efficient, to go to new places,
[01:30:22.320 --> 01:30:23.560]   you know, and scale up and just kind of, you know,
[01:30:23.560 --> 01:30:25.200]   stamp things out.
[01:30:25.200 --> 01:30:28.080]   So that's that second dimension of evaluation and deployment.
[01:30:28.080 --> 01:30:32.440]   And the third dimension is product, commercial,
[01:30:32.440 --> 01:30:34.320]   and operational excellence, right?
[01:30:34.320 --> 01:30:37.720]   And again, Phoenix there is providing
[01:30:37.720 --> 01:30:40.160]   an incredibly valuable platform.
[01:30:40.160 --> 01:30:42.320]   You know, that's why we're doing things end-to-end
[01:30:42.320 --> 01:30:43.680]   in Phoenix, we're learning as, you know,
[01:30:43.680 --> 01:30:46.200]   we discussed a little earlier today,
[01:30:46.200 --> 01:30:49.240]   tremendous amount of really valuable lessons
[01:30:49.240 --> 01:30:52.200]   from our users getting really incredible feedback.
[01:30:52.200 --> 01:30:54.840]   And we'll continue to iterate on that
[01:30:54.840 --> 01:30:57.400]   and incorporate all those lessons
[01:30:57.400 --> 01:31:00.080]   into making our product, you know,
[01:31:00.080 --> 01:31:01.760]   even better and more convenient for our users.
[01:31:01.760 --> 01:31:04.800]   - So you're converting this whole process of Phoenix
[01:31:04.800 --> 01:31:08.040]   in Phoenix into something that could be copied
[01:31:08.040 --> 01:31:09.440]   and pasted elsewhere.
[01:31:09.440 --> 01:31:12.640]   So like, perhaps you didn't think of it that way
[01:31:12.640 --> 01:31:14.520]   when you were doing the experimentation Phoenix,
[01:31:14.520 --> 01:31:18.960]   but so how long did, basically,
[01:31:18.960 --> 01:31:20.520]   you can correct me, but you've,
[01:31:21.920 --> 01:31:23.080]   I mean, it's still early days,
[01:31:23.080 --> 01:31:26.120]   but you've taken the full journey in Phoenix, right?
[01:31:26.120 --> 01:31:28.920]   As you were saying of like what it takes
[01:31:28.920 --> 01:31:30.720]   to basically automate, I mean,
[01:31:30.720 --> 01:31:32.360]   it's not the entirety of Phoenix, right?
[01:31:32.360 --> 01:31:37.360]   But I imagine it can encompass the entirety of Phoenix
[01:31:37.360 --> 01:31:40.640]   at some near-term date,
[01:31:40.640 --> 01:31:42.400]   but that's not even perhaps important,
[01:31:42.400 --> 01:31:45.160]   like as long as it's a large enough geographic area.
[01:31:45.160 --> 01:31:50.160]   So what, how copy-pastable is that process currently?
[01:31:50.800 --> 01:31:53.000]   Is that process currently?
[01:31:53.000 --> 01:31:57.600]   And how, like, you know,
[01:31:57.600 --> 01:32:02.600]   like when you copy and paste in Google Docs, I think,
[01:32:02.600 --> 01:32:06.920]   no, or in Word, you can like apply source formatting
[01:32:06.920 --> 01:32:09.440]   or apply destination formatting.
[01:32:09.440 --> 01:32:14.440]   So when you copy and paste the Phoenix into like,
[01:32:14.440 --> 01:32:19.440]   say, Boston, how do you apply the destination formatting?
[01:32:19.840 --> 01:32:24.160]   Like how much of the core of the entire process
[01:32:24.160 --> 01:32:29.160]   of bringing an actual public transportation,
[01:32:29.160 --> 01:32:32.800]   autonomous transportation service to a city
[01:32:32.800 --> 01:32:35.800]   is there in Phoenix that you understand enough
[01:32:35.800 --> 01:32:38.740]   to copy and paste into Boston or wherever?
[01:32:38.740 --> 01:32:40.600]   - So we're not quite there yet.
[01:32:40.600 --> 01:32:42.720]   We're not at a point where we're kind of massively
[01:32:42.720 --> 01:32:44.520]   copy and pasting all over the place,
[01:32:44.520 --> 01:32:48.840]   but Phoenix, what we did in Phoenix,
[01:32:48.840 --> 01:32:51.520]   and we very intentionally have chosen Phoenix
[01:32:51.520 --> 01:32:56.520]   as our first full deployment area,
[01:32:56.520 --> 01:32:57.640]   you know, exactly for that reason,
[01:32:57.640 --> 01:32:59.560]   to kind of tease the problem apart,
[01:32:59.560 --> 01:33:01.640]   look at each dimension,
[01:33:01.640 --> 01:33:03.840]   focus on the fundamentals of complexity
[01:33:03.840 --> 01:33:05.920]   and de-risking those dimensions,
[01:33:05.920 --> 01:33:07.600]   and then bringing the entire thing together
[01:33:07.600 --> 01:33:09.200]   to get all the way,
[01:33:09.200 --> 01:33:11.560]   force ourselves to learn all those hard lessons
[01:33:11.560 --> 01:33:13.540]   on technology, hardware and software,
[01:33:13.540 --> 01:33:15.000]   on the evaluation deployment,
[01:33:15.000 --> 01:33:17.500]   on operating a service, operating a business,
[01:33:17.500 --> 01:33:21.480]   using actually serving our customers,
[01:33:21.480 --> 01:33:23.800]   all the way so that we're fully informed
[01:33:23.800 --> 01:33:28.500]   about the most difficult, most important challenges
[01:33:28.500 --> 01:33:32.240]   to get us to that next step of massive copy and pasting,
[01:33:32.240 --> 01:33:34.640]   as you said.
[01:33:34.640 --> 01:33:39.080]   And that's what we're doing right now.
[01:33:39.080 --> 01:33:41.120]   We're incorporating all those things that we learned
[01:33:41.120 --> 01:33:44.040]   into that next system that then will allow us
[01:33:44.040 --> 01:33:46.040]   to kind of copy and paste all over the place
[01:33:46.040 --> 01:33:49.700]   and to massively scale to more users and more locations.
[01:33:49.700 --> 01:33:51.100]   I mean, you know, I just talked a little bit about,
[01:33:51.100 --> 01:33:53.600]   what does that mean along those different dimensions?
[01:33:53.600 --> 01:33:55.140]   So on the hardware side, for example,
[01:33:55.140 --> 01:33:57.740]   again, it's that switch from the fourth
[01:33:57.740 --> 01:33:58.740]   to the fifth generation.
[01:33:58.740 --> 01:34:00.300]   And the fifth generation is designed
[01:34:00.300 --> 01:34:01.800]   to kind of have that property.
[01:34:01.800 --> 01:34:05.300]   - Can you say what other cities you're thinking about?
[01:34:05.300 --> 01:34:07.140]   Like I'm thinking about,
[01:34:07.140 --> 01:34:10.020]   sorry, we're in San Francisco now.
[01:34:10.020 --> 01:34:12.580]   I thought I want to move to San Francisco,
[01:34:12.580 --> 01:34:14.580]   but I'm thinking about moving to Austin.
[01:34:15.520 --> 01:34:16.680]   I don't know why.
[01:34:16.680 --> 01:34:19.680]   People are not being very nice about San Francisco currently.
[01:34:19.680 --> 01:34:23.440]   Maybe it's in vogue right now.
[01:34:23.440 --> 01:34:26.500]   But Austin seems, I visited there and it was,
[01:34:26.500 --> 01:34:28.960]   I was in a Walmart.
[01:34:28.960 --> 01:34:33.000]   It's funny, these moments like turn your life.
[01:34:33.000 --> 01:34:37.000]   There's this very nice woman with kind eyes,
[01:34:37.000 --> 01:34:39.800]   just like stopped and said,
[01:34:39.800 --> 01:34:44.440]   "You look so handsome in that tie, honey," to me.
[01:34:44.440 --> 01:34:46.180]   This has never happened to me in my life,
[01:34:46.180 --> 01:34:48.540]   but just the sweetness of this woman
[01:34:48.540 --> 01:34:49.780]   is something I've never experienced,
[01:34:49.780 --> 01:34:51.540]   certainly in the streets of Boston,
[01:34:51.540 --> 01:34:53.840]   but even in San Francisco where people wouldn't,
[01:34:53.840 --> 01:34:57.040]   that's just not how they speak or think.
[01:34:57.040 --> 01:34:57.880]   I don't know.
[01:34:57.880 --> 01:35:00.260]   There's a warmth to Austin that love.
[01:35:00.260 --> 01:35:03.940]   And since Waymo does have a little bit of a history there,
[01:35:03.940 --> 01:35:05.160]   is that a possibility?
[01:35:05.160 --> 01:35:08.580]   - Is this your version of asking the question of like,
[01:35:08.580 --> 01:35:10.020]   you know, Dimitri, I know you can't share
[01:35:10.020 --> 01:35:12.020]   your commercial and deployment roadmap,
[01:35:12.020 --> 01:35:15.180]   but I'm thinking about moving to San Francisco, Austin,
[01:35:15.180 --> 01:35:18.540]   like in a blink twice, if you think I should move to.
[01:35:18.540 --> 01:35:19.380]   - Yeah, that's true.
[01:35:19.380 --> 01:35:20.200]   That's true.
[01:35:20.200 --> 01:35:21.040]   You got me.
[01:35:21.040 --> 01:35:24.060]   We've been testing all over the place.
[01:35:24.060 --> 01:35:26.700]   I think we've been testing more than 25 cities.
[01:35:26.700 --> 01:35:31.100]   We drive in San Francisco, we drive in Michigan for snow.
[01:35:31.100 --> 01:35:34.380]   We are doing significant amount of testing in the Bay Area,
[01:35:34.380 --> 01:35:35.540]   including San Francisco.
[01:35:35.540 --> 01:35:37.100]   - But just not like,
[01:35:37.100 --> 01:35:39.020]   'cause we're talking about the various different thing,
[01:35:39.020 --> 01:35:43.340]   which is like a full on large geographic area,
[01:35:43.340 --> 01:35:45.340]   public service.
[01:35:45.340 --> 01:35:46.980]   You can't share.
[01:35:46.980 --> 01:35:47.820]   Okay.
[01:35:47.820 --> 01:35:51.980]   What about Moscow?
[01:35:51.980 --> 01:35:54.300]   Is that, when is that happening?
[01:35:54.300 --> 01:35:56.580]   Take on Yandex.
[01:35:56.580 --> 01:35:58.860]   I'm not paying attention to those folks.
[01:35:58.860 --> 01:36:02.300]   They're doing, you know, there's a lot of fun.
[01:36:02.300 --> 01:36:04.880]   I mean, maybe as a way of a question,
[01:36:06.960 --> 01:36:11.560]   you didn't speak to sort of like policy
[01:36:11.560 --> 01:36:15.680]   or like, is there tricky things with government and so on?
[01:36:15.680 --> 01:36:22.160]   Is there other friction that you've encountered
[01:36:22.160 --> 01:36:24.880]   except sort of technological friction
[01:36:24.880 --> 01:36:27.480]   of solving this very difficult problem?
[01:36:27.480 --> 01:36:29.840]   Is there other stuff that you have to overcome
[01:36:29.840 --> 01:36:34.060]   when deploying a public service in a city?
[01:36:34.060 --> 01:36:36.560]   That's interesting.
[01:36:36.560 --> 01:36:37.800]   - It's very important.
[01:36:37.800 --> 01:36:42.800]   So we put significant effort in creating those partnerships
[01:36:42.800 --> 01:36:48.640]   and those relationships with governments at all levels,
[01:36:48.640 --> 01:36:50.280]   local governments, municipalities,
[01:36:50.280 --> 01:36:52.400]   state level, federal level.
[01:36:52.400 --> 01:36:54.560]   We've been engaged in very deep conversations
[01:36:54.560 --> 01:36:57.520]   from the earliest days of our projects
[01:36:57.520 --> 01:36:59.840]   whenever at all of these levels,
[01:36:59.840 --> 01:37:04.840]   whenever we go to test or operate in a new area,
[01:37:06.200 --> 01:37:09.360]   we always lead with a conversation
[01:37:09.360 --> 01:37:11.240]   with the local officials.
[01:37:11.240 --> 01:37:13.400]   But the result of that investment is that,
[01:37:13.400 --> 01:37:15.960]   no, it's not challenges we have to overcome,
[01:37:15.960 --> 01:37:17.480]   but it is a very important
[01:37:17.480 --> 01:37:20.360]   that we continue to have this conversation.
[01:37:20.360 --> 01:37:22.200]   - Yeah, I love politicians too.
[01:37:22.200 --> 01:37:27.200]   Okay, so Mr. Elon Musk said that LIDAR is a crutch.
[01:37:27.200 --> 01:37:30.760]   What are your thoughts?
[01:37:30.760 --> 01:37:35.200]   - I wouldn't characterize it exactly that way.
[01:37:35.200 --> 01:37:37.920]   I know, I think LIDAR is very important.
[01:37:37.920 --> 01:37:42.920]   It is a key sensor that we use just like other modalities.
[01:37:42.920 --> 01:37:47.580]   As we discussed, our cars use cameras, LIDARs and radars.
[01:37:47.580 --> 01:37:50.920]   They are all very important.
[01:37:50.920 --> 01:37:54.920]   They are at the kind of the physical level.
[01:37:54.920 --> 01:37:57.520]   They are very different.
[01:37:57.520 --> 01:38:00.440]   They have very different physical characteristics.
[01:38:00.440 --> 01:38:03.160]   Cameras are passive, LIDARs and radars are active,
[01:38:03.160 --> 01:38:04.660]   and use different wavelengths.
[01:38:05.540 --> 01:38:08.980]   So that means they complement each other very nicely.
[01:38:08.980 --> 01:38:13.980]   And together, combined, they can be used
[01:38:13.980 --> 01:38:19.540]   to build a much safer and much more capable system.
[01:38:19.540 --> 01:38:25.420]   So to me, it's more of a question,
[01:38:25.420 --> 01:38:27.540]   why the heck would you handicap yourself
[01:38:27.540 --> 01:38:30.540]   and not use one or more of those sensing modalities
[01:38:30.540 --> 01:38:34.300]   when they undoubtedly just make your system
[01:38:34.300 --> 01:38:36.980]   more capable and safer?
[01:38:36.980 --> 01:38:41.980]   Now, what might make sense for one product
[01:38:41.980 --> 01:38:48.580]   or one business might not make sense for another one.
[01:38:48.580 --> 01:38:51.100]   So if you're talking about driver assist technologies,
[01:38:51.100 --> 01:38:52.580]   you make certain design decisions
[01:38:52.580 --> 01:38:53.780]   and you make certain trade-offs,
[01:38:53.780 --> 01:38:56.820]   and you make different ones if you're building a driver
[01:38:56.820 --> 01:39:00.640]   that you deploy in fully driverless vehicles.
[01:39:00.640 --> 01:39:04.940]   And LIDAR specifically, when this question comes up,
[01:39:04.940 --> 01:39:10.700]   typically the criticisms that I hear
[01:39:10.700 --> 01:39:15.120]   are the counterpoints that cost and aesthetics.
[01:39:15.120 --> 01:39:20.620]   And I don't find either of those, honestly, very compelling.
[01:39:20.620 --> 01:39:22.420]   So on the cost side,
[01:39:22.420 --> 01:39:24.480]   there's nothing fundamentally prohibitive
[01:39:24.480 --> 01:39:27.020]   about the cost of LIDARs.
[01:39:27.020 --> 01:39:28.620]   Radars used to be very expensive
[01:39:28.620 --> 01:39:32.700]   before people made certain advances in technology
[01:39:32.700 --> 01:39:36.020]   and started to manufacture them at massive scale
[01:39:36.020 --> 01:39:39.060]   and deploy them in vehicles, similar with LIDARs.
[01:39:39.060 --> 01:39:41.860]   And this is where the LIDARs that we have on our cars,
[01:39:41.860 --> 01:39:44.220]   especially the fifth generation,
[01:39:44.220 --> 01:39:47.700]   we've been able to make some pretty qualitative
[01:39:47.700 --> 01:39:50.480]   discontinuous jumps in terms of the fundamental technology
[01:39:50.480 --> 01:39:52.780]   that allow us to manufacture those things
[01:39:52.780 --> 01:39:57.400]   at very significant scale and at a fraction of the cost
[01:39:57.400 --> 01:40:00.440]   of both our previous generation,
[01:40:00.440 --> 01:40:02.380]   as well as a fraction of the cost
[01:40:02.380 --> 01:40:05.060]   of what might be available on the market
[01:40:05.060 --> 01:40:06.460]   off the shelf right now.
[01:40:06.460 --> 01:40:07.940]   And that improvement will continue.
[01:40:07.940 --> 01:40:11.740]   So I think cost is not a real issue.
[01:40:11.740 --> 01:40:13.600]   Second one is aesthetics.
[01:40:13.600 --> 01:40:17.580]   I don't think that's a real issue either.
[01:40:17.580 --> 01:40:21.380]   - Beauty is in the eye of the beholder.
[01:40:21.380 --> 01:40:23.020]   You can make LIDAR sexy again.
[01:40:23.020 --> 01:40:23.860]   - I think you're exactly right.
[01:40:23.860 --> 01:40:24.680]   I think it is sexy.
[01:40:24.680 --> 01:40:26.940]   Like honestly, I think form is the function.
[01:40:26.940 --> 01:40:27.780]   - Well, okay.
[01:40:27.780 --> 01:40:29.220]   You know, I was actually,
[01:40:29.220 --> 01:40:31.220]   somebody brought this up to me.
[01:40:31.220 --> 01:40:33.180]   I mean, all forms of LIDAR,
[01:40:33.180 --> 01:40:36.940]   even like the ones that are like big,
[01:40:36.940 --> 01:40:40.460]   you can make look, I mean, you can make look beautiful.
[01:40:40.460 --> 01:40:42.060]   Like there's no sense in which
[01:40:42.060 --> 01:40:44.200]   you can't integrate it into design.
[01:40:44.200 --> 01:40:46.300]   Like there's all kinds of awesome designs.
[01:40:46.300 --> 01:40:50.540]   I don't think small and humble is beautiful.
[01:40:50.540 --> 01:40:53.260]   It could be like, you know, brutalism
[01:40:53.260 --> 01:40:56.700]   or like it could be like harsh corners.
[01:40:56.700 --> 01:40:58.180]   I mean, like I said, like hot rods.
[01:40:58.180 --> 01:41:00.860]   Like I don't necessarily like,
[01:41:00.860 --> 01:41:03.860]   like, oh man, I'm gonna start so much controversy with this.
[01:41:03.860 --> 01:41:06.980]   I don't like Porsches.
[01:41:06.980 --> 01:41:07.820]   Okay.
[01:41:07.820 --> 01:41:09.500]   The Porsche 911, like everyone says,
[01:41:09.500 --> 01:41:10.660]   oh, it's the most beautiful.
[01:41:10.660 --> 01:41:11.900]   No, no.
[01:41:11.900 --> 01:41:13.740]   It's like a baby car.
[01:41:13.740 --> 01:41:15.500]   It doesn't make any sense.
[01:41:15.500 --> 01:41:18.340]   But everyone, it's beauty is in the eye of the beholder.
[01:41:18.340 --> 01:41:19.180]   You're already looking at me like,
[01:41:19.180 --> 01:41:21.100]   what is this kid talking about?
[01:41:21.100 --> 01:41:23.540]   - I'm happy to talk about-
[01:41:23.540 --> 01:41:24.900]   - You're digging your own hole.
[01:41:24.900 --> 01:41:28.660]   - The form and function and my take on the beauty
[01:41:28.660 --> 01:41:31.060]   of the hardware that we put on our vehicles.
[01:41:31.060 --> 01:41:34.780]   You know, I will not comment on your Porsche monologue.
[01:41:34.780 --> 01:41:35.660]   - Okay.
[01:41:35.660 --> 01:41:36.500]   All right.
[01:41:36.500 --> 01:41:37.940]   So, but aesthetics, fine.
[01:41:37.940 --> 01:41:41.540]   But there's an underlying like philosophical question
[01:41:41.540 --> 01:41:44.260]   behind the kind of LIDAR question is like,
[01:41:44.260 --> 01:41:48.020]   how much of the problem can be solved
[01:41:48.020 --> 01:41:51.700]   with computer vision, with machine learning?
[01:41:51.700 --> 01:41:56.700]   So I think without sort of disagreements and so on,
[01:41:56.700 --> 01:42:01.940]   it's nice to put it on the spectrum
[01:42:01.940 --> 01:42:04.900]   because Waymo's doing a lot of machine learning as well.
[01:42:04.900 --> 01:42:06.980]   It's interesting to think how much of driving,
[01:42:06.980 --> 01:42:11.260]   if we look at five years, 10 years, 50 years down the road,
[01:42:11.260 --> 01:42:14.900]   what can be learned in almost more and more
[01:42:14.900 --> 01:42:16.460]   and more end to end way.
[01:42:17.340 --> 01:42:19.260]   If we look at what Tesla is doing
[01:42:19.260 --> 01:42:22.020]   as a machine learning problem,
[01:42:22.020 --> 01:42:24.780]   they're doing a multitask learning thing
[01:42:24.780 --> 01:42:26.180]   where it's just, they break up driving
[01:42:26.180 --> 01:42:27.940]   into a bunch of learning tasks
[01:42:27.940 --> 01:42:29.420]   and they have one single neural network
[01:42:29.420 --> 01:42:30.980]   and they're just collecting huge amounts of data
[01:42:30.980 --> 01:42:32.180]   that's training that.
[01:42:32.180 --> 01:42:33.860]   I've recently hung out with George Hotz.
[01:42:33.860 --> 01:42:35.460]   I don't know if you know George.
[01:42:35.460 --> 01:42:37.540]   (laughs)
[01:42:37.540 --> 01:42:40.180]   I love him so much.
[01:42:40.180 --> 01:42:41.940]   He's just an entertaining human being.
[01:42:41.940 --> 01:42:43.860]   We were off mic talking about Hunter S. Thompson.
[01:42:43.860 --> 01:42:46.260]   He's the Hunter S. Thompson of the time I was driving.
[01:42:46.260 --> 01:42:47.100]   Okay.
[01:42:47.100 --> 01:42:49.980]   So he, I didn't realize this with Kama AI,
[01:42:49.980 --> 01:42:52.660]   but they're like really trying to do end to end.
[01:42:52.660 --> 01:42:56.460]   Like looking at the machine learning problem,
[01:42:56.460 --> 01:43:00.380]   they're really not doing multitask learning,
[01:43:00.380 --> 01:43:04.540]   but it's computing the drivable area
[01:43:04.540 --> 01:43:06.100]   as a machine learning task
[01:43:06.100 --> 01:43:09.380]   and hoping that like down the line,
[01:43:09.380 --> 01:43:12.580]   this level two system, this driver assistance
[01:43:12.580 --> 01:43:15.940]   will eventually lead to allowing you
[01:43:15.940 --> 01:43:17.860]   to have a fully autonomous vehicle.
[01:43:17.860 --> 01:43:18.700]   Okay.
[01:43:18.700 --> 01:43:20.780]   There's an underlying deep philosophical question there,
[01:43:20.780 --> 01:43:25.780]   technical question of how much of driving can be learned.
[01:43:25.780 --> 01:43:29.420]   So LiDAR is an effective tool today
[01:43:29.420 --> 01:43:33.140]   for actually deploying a successful service in Phoenix,
[01:43:33.140 --> 01:43:36.060]   right, that's safe, that's reliable, et cetera, et cetera.
[01:43:36.060 --> 01:43:39.380]   But the question,
[01:43:39.380 --> 01:43:41.700]   and I'm not saying you can't do machine learning on LiDAR,
[01:43:41.700 --> 01:43:45.340]   but the question is that like how much of driving
[01:43:45.340 --> 01:43:47.420]   can be learned eventually?
[01:43:47.420 --> 01:43:50.140]   Can we do fully autonomous that's learned?
[01:43:50.140 --> 01:43:51.580]   - Yeah.
[01:43:51.580 --> 01:43:53.420]   You know, learning is all over the place
[01:43:53.420 --> 01:43:56.740]   and plays a key role in every part of our system.
[01:43:56.740 --> 01:44:01.140]   I, as you said, I would decouple the sensing modalities
[01:44:01.140 --> 01:44:05.860]   from the ML and the software parts of it.
[01:44:05.860 --> 01:44:09.780]   LiDAR, radar, cameras, it's all machine learning.
[01:44:09.780 --> 01:44:11.860]   All of the object detection classification, of course,
[01:44:11.860 --> 01:44:14.220]   like that's what these modern deep nets
[01:44:14.220 --> 01:44:15.700]   and con nets are very good at.
[01:44:15.700 --> 01:44:18.900]   You feed them raw data, massive amounts of raw data.
[01:44:18.900 --> 01:44:22.780]   And that's actually what our custom build LiDARs
[01:44:22.780 --> 01:44:24.220]   and radars are really good at.
[01:44:24.220 --> 01:44:25.980]   And radars, they don't just give you point estimates
[01:44:25.980 --> 01:44:26.980]   of objects in space,
[01:44:26.980 --> 01:44:29.620]   they give you raw, like physical observations.
[01:44:29.620 --> 01:44:31.780]   And then you take all of that raw information,
[01:44:31.780 --> 01:44:33.420]   you know, there's colors of the pixels,
[01:44:33.420 --> 01:44:34.860]   whether it's LiDARs returns,
[01:44:34.860 --> 01:44:35.780]   it's some auxiliary information,
[01:44:35.780 --> 01:44:37.140]   it's not just distance, right?
[01:44:37.140 --> 01:44:39.220]   And angle and distance is much richer information
[01:44:39.220 --> 01:44:40.500]   that you get from those returns,
[01:44:40.500 --> 01:44:43.060]   plus really rich information from the radars.
[01:44:43.060 --> 01:44:44.620]   You fuse it all together and you feed it
[01:44:44.620 --> 01:44:48.460]   into those massive ML models that then, you know,
[01:44:48.460 --> 01:44:51.540]   lead to the best results in terms of, you know,
[01:44:51.540 --> 01:44:55.940]   object detection, classification, you know, state estimation.
[01:44:55.940 --> 01:44:58.540]   - So there's a, sorry to interrupt, but there is a fusion.
[01:44:58.540 --> 01:44:59.820]   I mean, that's something that people didn't do
[01:44:59.820 --> 01:45:01.140]   for a very long time,
[01:45:01.140 --> 01:45:04.620]   which is like at the sensor fusion level, I guess,
[01:45:04.620 --> 01:45:07.100]   like early on fusing the information together,
[01:45:07.100 --> 01:45:10.780]   whether so that the sensory information
[01:45:10.780 --> 01:45:12.940]   that the vehicle receives from the different modalities
[01:45:12.940 --> 01:45:15.900]   or even from different cameras is combined
[01:45:15.900 --> 01:45:18.460]   before it is fed into the machine learning models.
[01:45:18.460 --> 01:45:20.860]   - Yeah, so I think this is one of the trends.
[01:45:20.860 --> 01:45:22.700]   You're seeing more of that, you mentioned end-to-end,
[01:45:22.700 --> 01:45:24.580]   there's different interpretation of end-to-end.
[01:45:24.580 --> 01:45:27.620]   There is kind of the purest interpretation
[01:45:27.620 --> 01:45:29.660]   of I'm gonna like have one model
[01:45:29.660 --> 01:45:32.580]   that goes from raw sensor data to like, you know,
[01:45:32.580 --> 01:45:34.860]   steering torque and, you know, gas brakes.
[01:45:34.860 --> 01:45:36.060]   That, you know, that's too much.
[01:45:36.060 --> 01:45:37.580]   I don't think that's the right way to do it.
[01:45:37.580 --> 01:45:40.700]   There's more, you know, smaller versions of end-to-end
[01:45:40.700 --> 01:45:44.580]   where you're kind of doing more end-to-end learning
[01:45:44.580 --> 01:45:46.540]   or core training or deep propagation
[01:45:46.540 --> 01:45:48.540]   of kind of signals back and forth
[01:45:48.540 --> 01:45:50.500]   across the different stages of your system.
[01:45:50.500 --> 01:45:51.900]   There's, you know, really good ways.
[01:45:51.900 --> 01:45:54.900]   It gets into some fairly complex design choices
[01:45:54.900 --> 01:45:56.420]   where on one hand you want modularity
[01:45:56.420 --> 01:46:00.220]   and the composability of your system.
[01:46:00.220 --> 01:46:01.740]   But on the other hand,
[01:46:01.740 --> 01:46:04.220]   you don't wanna create interfaces that are too narrow
[01:46:04.220 --> 01:46:05.900]   or too brittle, too engineered,
[01:46:05.900 --> 01:46:08.060]   where you're giving up on the generality of a solution
[01:46:08.060 --> 01:46:10.460]   or you're unable to properly propagate signal,
[01:46:10.460 --> 01:46:14.220]   you know, reach signal forward and losses and, you know,
[01:46:14.220 --> 01:46:17.580]   back so you can optimize the whole system jointly.
[01:46:17.580 --> 01:46:18.820]   So I would decouple.
[01:46:18.820 --> 01:46:21.220]   And I guess what you're seeing in terms of the fusion
[01:46:21.220 --> 01:46:24.100]   of the sensing data from different modalities,
[01:46:24.100 --> 01:46:26.940]   as well as kind of fusion in the temporal level,
[01:46:26.940 --> 01:46:29.260]   going more from, you know, frame by frame,
[01:46:29.260 --> 01:46:31.580]   where, you know, you would have one net
[01:46:31.580 --> 01:46:33.260]   that would do frame by frame detection in camera.
[01:46:33.260 --> 01:46:35.140]   And then, you know, something that does frame by frame
[01:46:35.140 --> 01:46:36.580]   and lighter and then radar,
[01:46:36.580 --> 01:46:38.100]   and then you fuse it, you know,
[01:46:38.100 --> 01:46:39.740]   in a weaker engineered way later.
[01:46:39.740 --> 01:46:42.460]   Like the field over the last decade has been evolving
[01:46:42.460 --> 01:46:45.220]   in more kind of joint fusion, more end-to-end models
[01:46:45.220 --> 01:46:47.740]   that are solving some of these tasks, you know, jointly.
[01:46:47.740 --> 01:46:49.340]   And there's tremendous power in that.
[01:46:49.340 --> 01:46:51.540]   And, you know, that's the progression
[01:46:51.540 --> 01:46:54.740]   that kind of our stack has been on as well.
[01:46:54.740 --> 01:46:55.940]   Now, to your, you know,
[01:46:55.940 --> 01:46:57.820]   so I would decouple the sensing
[01:46:57.820 --> 01:46:58.980]   and how that information is used
[01:46:58.980 --> 01:47:01.420]   from the role of ML in the entire stack.
[01:47:01.420 --> 01:47:02.860]   And, you know, I guess it's,
[01:47:02.860 --> 01:47:06.300]   there's trade-offs in modularity
[01:47:06.300 --> 01:47:11.300]   and how do you inject inductive bias into your system?
[01:47:11.300 --> 01:47:15.260]   Right, this is, there's tremendous power
[01:47:15.260 --> 01:47:16.700]   in being able to do that.
[01:47:16.700 --> 01:47:20.740]   So, you know, we have, there's no part of our system
[01:47:20.740 --> 01:47:24.300]   that is not heavily, that does not heavily, you know,
[01:47:24.300 --> 01:47:28.260]   leverage data-driven development or, you know,
[01:47:28.260 --> 01:47:29.660]   state-of-the-art ML.
[01:47:29.660 --> 01:47:32.020]   But there's mapping, there's a simulator,
[01:47:32.020 --> 01:47:34.180]   or there's perception, you know, object level,
[01:47:34.180 --> 01:47:35.980]   you know, perception, whether it's semantic understanding,
[01:47:35.980 --> 01:47:38.980]   prediction, decision-making, you know, so forth and so on.
[01:47:38.980 --> 01:47:44.940]   It's, and of course, object detection and classification,
[01:47:44.940 --> 01:47:47.100]   like you're finding pedestrians and cars and cyclists
[01:47:47.100 --> 01:47:49.780]   and, you know, cones and signs and vegetation
[01:47:49.780 --> 01:47:51.900]   and being very good at estimating
[01:47:51.900 --> 01:47:54.020]   kind of detection classification and state estimation,
[01:47:54.020 --> 01:47:55.100]   there's just stable stakes.
[01:47:55.100 --> 01:47:57.500]   Like that's step zero of this whole stack.
[01:47:57.500 --> 01:47:59.420]   You can be incredibly good at that,
[01:47:59.420 --> 01:48:00.980]   whether you use cameras or light as a radar,
[01:48:00.980 --> 01:48:02.540]   but that's just, you know, that's stable stakes.
[01:48:02.540 --> 01:48:03.740]   That's just step zero.
[01:48:03.740 --> 01:48:05.780]   Beyond that, you get into the really interesting challenges
[01:48:05.780 --> 01:48:08.220]   of semantic understanding, the perception level.
[01:48:08.220 --> 01:48:10.140]   You get into scene level reasoning.
[01:48:10.140 --> 01:48:12.180]   You get into very deep problems
[01:48:12.180 --> 01:48:14.420]   that have to do with prediction and joint prediction
[01:48:14.420 --> 01:48:16.220]   and interaction, so on and so on,
[01:48:16.220 --> 01:48:18.420]   between all of the actors in the environment,
[01:48:18.420 --> 01:48:19.780]   pedestrians, cyclists, other cars,
[01:48:19.780 --> 01:48:21.540]   and you get into decision-making, right?
[01:48:21.540 --> 01:48:22.980]   So how do you build a lot of systems?
[01:48:22.980 --> 01:48:27.340]   So we leverage ML very heavily in all of these components.
[01:48:27.340 --> 01:48:30.700]   I do believe that the best results you achieve
[01:48:30.700 --> 01:48:33.100]   by kind of using a hybrid approach
[01:48:33.140 --> 01:48:35.140]   and having different types of ML,
[01:48:35.140 --> 01:48:40.500]   having different models with different degrees
[01:48:40.500 --> 01:48:43.020]   of inductive bias that you can have,
[01:48:43.020 --> 01:48:46.300]   and combining kind of model-free approaches
[01:48:46.300 --> 01:48:48.140]   with some model-based approaches
[01:48:48.140 --> 01:48:53.140]   and some rule-based, physics-based systems.
[01:48:53.140 --> 01:48:56.180]   So, you know, one example I can give you is traffic lights.
[01:48:56.180 --> 01:48:59.780]   There's a problem of the detection of traffic light state,
[01:48:59.780 --> 01:49:01.740]   and obviously that's a great problem
[01:49:01.740 --> 01:49:04.580]   for computer vision confidence.
[01:49:04.580 --> 01:49:06.740]   That's their bread and butter, right?
[01:49:06.740 --> 01:49:07.860]   That's how you build that.
[01:49:07.860 --> 01:49:11.540]   But then the interpretation of a traffic light,
[01:49:11.540 --> 01:49:14.260]   that you're gonna need to learn that, right?
[01:49:14.260 --> 01:49:17.140]   Red, you don't need to build some complex ML model
[01:49:17.140 --> 01:49:21.300]   that infers with some precision and recall
[01:49:21.300 --> 01:49:22.900]   that red means stop.
[01:49:22.900 --> 01:49:25.620]   It's a very clear engineered signal
[01:49:25.620 --> 01:49:27.740]   with very clear semantics, right?
[01:49:27.740 --> 01:49:29.020]   So you wanna induce that bias.
[01:49:29.020 --> 01:49:31.100]   Like how you induce that bias and that,
[01:49:31.100 --> 01:49:35.820]   whether it's a constraint or a cost function in your stack,
[01:49:35.820 --> 01:49:38.700]   but it is important to be able to inject
[01:49:38.700 --> 01:49:42.380]   that clear semantic signal into your stack.
[01:49:42.380 --> 01:49:43.540]   And that's what we do.
[01:49:43.540 --> 01:49:46.260]   But then the question of like,
[01:49:46.260 --> 01:49:49.060]   and that's when you apply it to yourself,
[01:49:49.060 --> 01:49:50.340]   when you are making decisions,
[01:49:50.340 --> 01:49:52.580]   whether you wanna stop for a red light or not.
[01:49:52.580 --> 01:49:57.980]   But if you think about how other people treat traffic lights,
[01:49:57.980 --> 01:49:59.940]   we're back to the ML version of that.
[01:49:59.940 --> 01:50:01.940]   As you know, they're supposed to stop for a red light,
[01:50:01.940 --> 01:50:03.020]   but that doesn't mean they will.
[01:50:03.020 --> 01:50:06.820]   So then you're back in the like very heavy ML domain
[01:50:06.820 --> 01:50:11.540]   where you're picking up on like very subtle keys about,
[01:50:11.540 --> 01:50:13.780]   you know, that have to do with the behavior of objects
[01:50:13.780 --> 01:50:16.140]   and pedestrians, cyclists, cars,
[01:50:16.140 --> 01:50:18.060]   and the whole thing, you know,
[01:50:18.060 --> 01:50:19.580]   entire configuration of the scene
[01:50:19.580 --> 01:50:21.380]   that allow you to make accurate predictions
[01:50:21.380 --> 01:50:24.260]   on whether they will in fact stop or run a red light.
[01:50:24.260 --> 01:50:27.140]   - So it sounds like already for Waymo,
[01:50:27.140 --> 01:50:29.900]   like machine learning is a huge part of the stack.
[01:50:29.900 --> 01:50:33.260]   So it's a huge part of like, not just,
[01:50:33.260 --> 01:50:37.620]   so obviously the first level zero or whatever you said,
[01:50:37.620 --> 01:50:39.940]   which is like just the object detection
[01:50:39.940 --> 01:50:40.780]   and things that, you know,
[01:50:40.780 --> 01:50:42.700]   with know that machine learning can do,
[01:50:42.700 --> 01:50:46.260]   but also starting to do prediction behavior and so on
[01:50:46.260 --> 01:50:50.020]   to model what other parties in the scene,
[01:50:50.020 --> 01:50:51.700]   entities in the scene are gonna do.
[01:50:51.700 --> 01:50:53.980]   So machine learning is more and more
[01:50:53.980 --> 01:50:56.220]   playing a role in that as well.
[01:50:56.220 --> 01:50:57.900]   - Of course, absolutely.
[01:50:57.900 --> 01:51:01.220]   I think we've been going back to the earliest days,
[01:51:01.220 --> 01:51:03.100]   like, you know, the DARPA Urban,
[01:51:03.100 --> 01:51:04.340]   the DARPA Grand Challenge,
[01:51:04.340 --> 01:51:06.380]   and team was leveraging, you know, machine learning.
[01:51:06.380 --> 01:51:08.140]   I was like pre, you know, ImageNet,
[01:51:08.140 --> 01:51:09.820]   and it was a very different type of ML,
[01:51:09.820 --> 01:51:12.220]   but, and I think actually it was before my time,
[01:51:12.220 --> 01:51:14.980]   but the Stanford team on during the Grand Challenge
[01:51:14.980 --> 01:51:17.460]   had a very interesting machine learned system
[01:51:17.460 --> 01:51:20.780]   that would, you know, use lighter and camera,
[01:51:20.780 --> 01:51:21.820]   we've been driving in the desert,
[01:51:21.820 --> 01:51:24.620]   and it, we had built the model
[01:51:24.620 --> 01:51:28.020]   where it would kind of extend the range
[01:51:28.020 --> 01:51:29.700]   of free space reasoning.
[01:51:29.700 --> 01:51:31.660]   So we get a clear signal from lighter,
[01:51:31.660 --> 01:51:32.620]   and then it had a model that said,
[01:51:32.620 --> 01:51:33.820]   "Hey, like this stuff in camera
[01:51:33.820 --> 01:51:35.820]   kind of sort of looks like this stuff in lighter.
[01:51:35.820 --> 01:51:37.660]   And I know this stuff that I've seen in lighter,
[01:51:37.660 --> 01:51:38.980]   I'm very confident that it's free space.
[01:51:38.980 --> 01:51:41.660]   So let me extend that free space zone
[01:51:41.660 --> 01:51:43.100]   into the camera range
[01:51:43.100 --> 01:51:44.700]   that would allow the vehicle to drive faster."
[01:51:44.700 --> 01:51:46.260]   And then we've been building on top of that
[01:51:46.260 --> 01:51:49.020]   and kind of staying and pushing the state of the art in ML,
[01:51:49.020 --> 01:51:51.740]   in all kinds of different ML over the years.
[01:51:51.740 --> 01:51:53.500]   And in fact, from the earliest days,
[01:51:53.500 --> 01:51:56.900]   I think, you know, 2010 is probably the year
[01:51:56.900 --> 01:52:00.060]   where Google, maybe 2011 probably,
[01:52:00.060 --> 01:52:04.900]   got pretty heavily involved in machine learning,
[01:52:04.900 --> 01:52:06.420]   kind of deep nuts.
[01:52:06.420 --> 01:52:08.140]   And at that time, it was probably the only company
[01:52:08.140 --> 01:52:11.940]   that was very heavily investing in kind of state of the art ML
[01:52:11.940 --> 01:52:13.300]   and self-driving cars, right?
[01:52:13.300 --> 01:52:16.340]   And they go hand in hand.
[01:52:16.340 --> 01:52:18.540]   And we've been on that journey ever since.
[01:52:18.540 --> 01:52:22.220]   We're doing, pushing a lot of these areas
[01:52:22.220 --> 01:52:23.980]   in terms of research at Waymo,
[01:52:23.980 --> 01:52:25.500]   and we collaborate very heavily
[01:52:25.500 --> 01:52:27.980]   with the researchers in Alphabet.
[01:52:27.980 --> 01:52:29.460]   And I call all kinds of ML,
[01:52:29.460 --> 01:52:31.540]   supervised ML, unsupervised ML,
[01:52:31.540 --> 01:52:37.860]   published some interesting research papers in the space,
[01:52:37.860 --> 01:52:39.420]   especially recently.
[01:52:39.420 --> 01:52:41.300]   It's just super active. - Super active learning as well.
[01:52:41.300 --> 01:52:43.020]   - Yeah, so super, super active.
[01:52:43.020 --> 01:52:46.260]   And of course there's kind of the more mature stuff
[01:52:46.260 --> 01:52:49.100]   like, you know, ConvNets for object detection,
[01:52:49.100 --> 01:52:50.380]   but there's some really interesting,
[01:52:50.380 --> 01:52:52.860]   really active work that's happening
[01:52:52.860 --> 01:52:57.580]   and kind of more, you know, and bigger models
[01:52:57.580 --> 01:53:02.580]   and models that have more structure to them,
[01:53:02.580 --> 01:53:04.420]   you know, not just large bitmaps
[01:53:04.420 --> 01:53:06.780]   and reason about temporal sequences.
[01:53:06.780 --> 01:53:09.340]   And some of the interesting breakthroughs
[01:53:09.340 --> 01:53:12.860]   that we've seen in language models, right?
[01:53:12.860 --> 01:53:16.300]   You know, transformers, you know, GPT-3 inference.
[01:53:16.300 --> 01:53:18.980]   There's some really interesting applications
[01:53:18.980 --> 01:53:20.140]   of some of the core breakthroughs
[01:53:20.140 --> 01:53:23.020]   to those problems of, you know, behavior prediction,
[01:53:23.020 --> 01:53:25.180]   as well as, you know, decision-making and planning, right?
[01:53:25.180 --> 01:53:27.780]   You can think about it, kind of the behavior,
[01:53:27.780 --> 01:53:29.700]   how, you know, the path, the trajectories,
[01:53:29.700 --> 01:53:31.500]   the how people drive,
[01:53:31.500 --> 01:53:33.020]   and they have kind of a share
[01:53:33.020 --> 01:53:35.900]   a lot of the fundamental structure, you know, this problem.
[01:53:35.900 --> 01:53:38.860]   There's, you know, sequential nature.
[01:53:38.860 --> 01:53:41.980]   There's a lot of structure in this representation.
[01:53:41.980 --> 01:53:45.140]   There is a strong locality, kind of like in sentences,
[01:53:45.140 --> 01:53:46.540]   you know, words that follow each other,
[01:53:46.540 --> 01:53:47.780]   they're strongly connected,
[01:53:47.780 --> 01:53:49.940]   but there are also kind of larger contexts
[01:53:49.940 --> 01:53:51.100]   that doesn't have that locality.
[01:53:51.100 --> 01:53:52.340]   And you also see that in driving, right?
[01:53:52.340 --> 01:53:54.700]   What, you know, is happening in the scene as a whole
[01:53:54.700 --> 01:53:58.540]   has very strong implications on, you know,
[01:53:58.540 --> 01:54:00.620]   the kind of the next step in that sequence
[01:54:00.620 --> 01:54:02.460]   where whether you're predicting
[01:54:02.460 --> 01:54:03.820]   what other people are going to do,
[01:54:03.820 --> 01:54:05.540]   whether you're making your own decisions,
[01:54:05.540 --> 01:54:06.820]   or whether in the simulator,
[01:54:06.820 --> 01:54:09.580]   you're building generative models of, you know,
[01:54:09.580 --> 01:54:11.860]   humans walking, cyclists riding, and other cars driving.
[01:54:11.860 --> 01:54:13.420]   - Oh, that's all really fascinating.
[01:54:13.420 --> 01:54:15.660]   Like how it's fascinating to think that
[01:54:15.660 --> 01:54:19.500]   transformer models and all the breakthroughs in language
[01:54:19.500 --> 01:54:22.380]   and NLP that might be applicable to like driving
[01:54:22.380 --> 01:54:24.260]   at the higher level, at the behavior level.
[01:54:24.260 --> 01:54:25.980]   That's kind of fascinating.
[01:54:25.980 --> 01:54:27.940]   Let me ask about pesky little creatures
[01:54:27.940 --> 01:54:30.260]   called pedestrians and cyclists.
[01:54:30.260 --> 01:54:32.540]   They seem, so humans are a problem.
[01:54:32.540 --> 01:54:34.300]   If we can get rid of them, I would.
[01:54:34.300 --> 01:54:37.700]   But unfortunately, they're also a source of joy
[01:54:37.700 --> 01:54:40.060]   and love and beauty, so let's keep them around.
[01:54:40.060 --> 01:54:41.540]   - They're also our customers.
[01:54:41.540 --> 01:54:44.060]   - Oh, for your perspective, yes, yes, for sure.
[01:54:44.060 --> 01:54:46.780]   They're a source of money, very good.
[01:54:48.300 --> 01:54:52.060]   But I don't even know where I was going.
[01:54:52.060 --> 01:54:54.100]   Oh yes, pedestrians and cyclists.
[01:54:54.100 --> 01:54:59.380]   I, you know, they're a fascinating injection
[01:54:59.380 --> 01:55:01.500]   into the system of uncertainty,
[01:55:01.500 --> 01:55:06.500]   of like a game theoretic dance of what to do.
[01:55:06.500 --> 01:55:11.740]   And also they have perceptions of their own
[01:55:11.740 --> 01:55:14.980]   and they can tweet about your product.
[01:55:14.980 --> 01:55:16.740]   So you don't want to run them over.
[01:55:17.620 --> 01:55:20.420]   - From that perspective, I mean, I don't know,
[01:55:20.420 --> 01:55:24.180]   I'm joking a lot, but I think in seriousness,
[01:55:24.180 --> 01:55:26.580]   like, you know, pedestrians are a complicated,
[01:55:26.580 --> 01:55:30.860]   computer vision problem, a complicated behavioral problem.
[01:55:30.860 --> 01:55:32.380]   Is there something interesting you could say
[01:55:32.380 --> 01:55:34.300]   about what you've learned
[01:55:34.300 --> 01:55:36.220]   from a machine learning perspective,
[01:55:36.220 --> 01:55:40.060]   from also an autonomous vehicle and a product perspective
[01:55:40.060 --> 01:55:43.060]   about just interacting with the humans in this world?
[01:55:43.060 --> 01:55:44.980]   - Yeah, just, you know, to state on record,
[01:55:44.980 --> 01:55:47.140]   we care deeply about the safety of pedestrians,
[01:55:47.140 --> 01:55:50.300]   you know, even the ones that don't have Twitter accounts.
[01:55:50.300 --> 01:55:51.140]   - Thank you.
[01:55:51.140 --> 01:55:51.980]   (laughs)
[01:55:51.980 --> 01:55:53.020]   All right, all right, cool.
[01:55:53.020 --> 01:55:53.860]   Not me.
[01:55:53.860 --> 01:55:57.620]   But yes, I'm glad somebody does.
[01:55:57.620 --> 01:55:58.460]   Okay.
[01:55:58.460 --> 01:56:00.420]   - But you know, in all seriousness,
[01:56:00.420 --> 01:56:04.740]   safety of vulnerable road users,
[01:56:04.740 --> 01:56:08.780]   pedestrians or cyclists is one of our highest priorities.
[01:56:08.780 --> 01:56:13.780]   We do a tremendous amount of testing and validation
[01:56:13.780 --> 01:56:16.260]   and put a very significant emphasis
[01:56:16.260 --> 01:56:18.700]   on the capabilities of our systems
[01:56:18.700 --> 01:56:20.620]   that have to do with safety
[01:56:20.620 --> 01:56:22.980]   around those unprotected vulnerable road users.
[01:56:22.980 --> 01:56:27.180]   You know, cars, as we discussed earlier in Phoenix,
[01:56:27.180 --> 01:56:28.300]   we have completely empty cars,
[01:56:28.300 --> 01:56:29.740]   completely driverless cars,
[01:56:29.740 --> 01:56:32.220]   driving in this very large area.
[01:56:32.220 --> 01:56:34.780]   And you know, some people use them to go to school,
[01:56:34.780 --> 01:56:36.540]   so they will drive through school zones, right?
[01:56:36.540 --> 01:56:40.020]   So kids are kind of the very special class
[01:56:40.020 --> 01:56:41.740]   of those vulnerable road users, right?
[01:56:41.740 --> 01:56:44.420]   You wanna be super, super safe
[01:56:44.420 --> 01:56:45.860]   and super, super cautious around those.
[01:56:45.860 --> 01:56:48.380]   So we take it very, very, very seriously.
[01:56:48.380 --> 01:56:53.060]   And you know, what does it take to be good at it?
[01:56:53.060 --> 01:56:59.460]   An incredible amount of performance
[01:56:59.460 --> 01:57:01.700]   across your whole stack.
[01:57:01.700 --> 01:57:04.220]   You know, it starts with hardware.
[01:57:04.220 --> 01:57:06.860]   And again, you wanna use all sensing modalities
[01:57:06.860 --> 01:57:07.700]   available to you.
[01:57:07.700 --> 01:57:10.220]   Imagine driving on a residential road at night
[01:57:10.220 --> 01:57:11.380]   and kind of making a turn
[01:57:11.380 --> 01:57:14.180]   and you don't have headlights covering some part
[01:57:14.180 --> 01:57:17.020]   of the space and like, you know, a kid might run out.
[01:57:17.020 --> 01:57:19.980]   And you know, lighters are amazing at that.
[01:57:19.980 --> 01:57:22.980]   They see just as well in complete darkness
[01:57:22.980 --> 01:57:24.140]   as they do during the day, right?
[01:57:24.140 --> 01:57:26.220]   So just again, it gives you that extra,
[01:57:26.220 --> 01:57:31.700]   you know, margin in terms of capability
[01:57:31.700 --> 01:57:33.700]   and performance and safety and quality.
[01:57:33.700 --> 01:57:36.340]   And in fact, we oftentimes, in these kinds of situations,
[01:57:36.340 --> 01:57:38.620]   we have our system detect something,
[01:57:38.620 --> 01:57:41.380]   in some cases even earlier than our trained operators
[01:57:41.380 --> 01:57:42.420]   in the car might do, right?
[01:57:42.420 --> 01:57:45.660]   Especially in conditions like very dark nights.
[01:57:45.660 --> 01:57:48.220]   So it starts with sensing.
[01:57:48.220 --> 01:57:52.940]   Then, you know, perception has to be incredibly good.
[01:57:52.940 --> 01:57:54.300]   And you have to be very, very good
[01:57:54.300 --> 01:57:57.700]   at kind of detecting pedestrians
[01:57:57.700 --> 01:58:00.500]   in all kinds of situations
[01:58:00.500 --> 01:58:01.500]   and all kinds of environments,
[01:58:01.500 --> 01:58:03.700]   including people in weird poses,
[01:58:03.700 --> 01:58:06.060]   people kind of running around
[01:58:06.060 --> 01:58:07.980]   and being partially occluded.
[01:58:09.940 --> 01:58:13.140]   So, you know, that's step number one.
[01:58:13.140 --> 01:58:17.620]   Then you have to have very high accuracy
[01:58:17.620 --> 01:58:21.260]   and very low latency in terms of your reactions
[01:58:21.260 --> 01:58:26.060]   to what these actors might do, right?
[01:58:26.060 --> 01:58:28.940]   And we've put a tremendous amount of engineering
[01:58:28.940 --> 01:58:30.620]   and tremendous amount of validation
[01:58:30.620 --> 01:58:33.780]   in to make sure our system performs properly.
[01:58:33.780 --> 01:58:36.820]   And oftentimes it does require a very strong reaction
[01:58:36.820 --> 01:58:37.860]   to do the safe thing.
[01:58:37.860 --> 01:58:39.540]   And we actually see a lot of cases like that.
[01:58:39.540 --> 01:58:41.980]   It's the long tail of really rare,
[01:58:41.980 --> 01:58:45.500]   you know, really crazy events
[01:58:45.500 --> 01:58:49.580]   that contribute to the safety around pedestrians.
[01:58:49.580 --> 01:58:51.540]   Like one example that comes to mind
[01:58:51.540 --> 01:58:54.140]   that we actually got happened in Phoenix,
[01:58:54.140 --> 01:58:57.700]   where we were driving along
[01:58:57.700 --> 01:58:59.780]   and I think it was a 45 mile per hour road.
[01:58:59.780 --> 01:59:01.220]   So you have pretty high speed traffic
[01:59:01.220 --> 01:59:03.460]   and there was a sidewalk next to it.
[01:59:03.460 --> 01:59:05.580]   And there was a cyclist on the sidewalk.
[01:59:05.580 --> 01:59:09.180]   And as we were in the right lane,
[01:59:09.180 --> 01:59:11.780]   right next to the sidewalk, it was a multi-lane road.
[01:59:11.780 --> 01:59:15.460]   So as we got close to the cyclist on the sidewalk,
[01:59:15.460 --> 01:59:17.300]   it was a woman, she tripped and fell,
[01:59:17.300 --> 01:59:19.780]   just fell right into the path of our vehicle.
[01:59:19.780 --> 01:59:26.180]   And our car, you know, this was actually with a test driver.
[01:59:26.180 --> 01:59:29.900]   Our test drivers did exactly the right thing.
[01:59:29.900 --> 01:59:31.500]   They kind of reacted and came to a stop.
[01:59:31.500 --> 01:59:33.140]   It requires both very strong steering
[01:59:33.140 --> 01:59:35.900]   and strong application of the brake.
[01:59:35.900 --> 01:59:37.860]   And then we simulated what our system would have done
[01:59:37.860 --> 01:59:40.420]   in that situation and it did exactly the same thing.
[01:59:40.420 --> 01:59:44.060]   And that speaks to all of those components
[01:59:44.060 --> 01:59:47.060]   of really good state estimation and tracking.
[01:59:47.060 --> 01:59:49.700]   And like imagine a person on a bike
[01:59:49.700 --> 01:59:50.900]   and they're falling over
[01:59:50.900 --> 01:59:52.340]   and they're doing that right in front of you.
[01:59:52.340 --> 01:59:53.700]   So you have to be really like, things are changing.
[01:59:53.700 --> 01:59:56.100]   The appearance of that whole thing is changing.
[01:59:56.100 --> 01:59:58.220]   And a person goes one way, they're falling on the road,
[01:59:58.220 --> 02:00:01.300]   they're being flat on the ground in front of you.
[02:00:01.300 --> 02:00:03.460]   The bike goes flying the other direction.
[02:00:03.460 --> 02:00:04.860]   Like the two objects that used to be one
[02:00:04.860 --> 02:00:06.940]   are now are splitting apart.
[02:00:06.940 --> 02:00:09.100]   And the car has to like detect all of that.
[02:00:09.100 --> 02:00:10.380]   Like milliseconds matter.
[02:00:10.380 --> 02:00:12.900]   And it's not good enough to just brake.
[02:00:12.900 --> 02:00:14.380]   You have to like steer and brake
[02:00:14.380 --> 02:00:15.700]   and there's traffic around you.
[02:00:15.700 --> 02:00:17.580]   So like it all has to come together.
[02:00:17.580 --> 02:00:20.020]   And it was really great to see in this case
[02:00:20.020 --> 02:00:21.260]   and other cases like that,
[02:00:21.260 --> 02:00:22.740]   that we're actually seeing in the wild
[02:00:22.740 --> 02:00:26.100]   that our system is performing exactly the way
[02:00:26.100 --> 02:00:27.860]   that we would have liked
[02:00:27.860 --> 02:00:30.820]   and is able to avoid collisions like this.
[02:00:30.820 --> 02:00:32.820]   - It's such an exciting space for robotics.
[02:00:32.820 --> 02:00:35.340]   Like in that split second
[02:00:35.340 --> 02:00:37.620]   to make decisions of life and death.
[02:00:37.620 --> 02:00:38.620]   I don't know.
[02:00:38.620 --> 02:00:40.180]   The stakes are high in a sense,
[02:00:40.180 --> 02:00:42.140]   but it's also beautiful that
[02:00:42.140 --> 02:00:45.700]   for somebody who loves artificial intelligence,
[02:00:45.700 --> 02:00:47.460]   the possibility that an AI system
[02:00:47.460 --> 02:00:49.220]   might be able to save a human life.
[02:00:49.220 --> 02:00:54.180]   That's kind of exciting as a problem, like to wake up.
[02:00:54.180 --> 02:00:57.540]   It's terrifying probably for an engineer to wake up
[02:00:57.540 --> 02:01:00.380]   and to think about, but it's also exciting
[02:01:00.380 --> 02:01:02.940]   'cause it's in your hands.
[02:01:02.940 --> 02:01:04.220]   Let me try to ask a question
[02:01:04.220 --> 02:01:07.500]   that's often brought up about autonomous vehicles.
[02:01:07.500 --> 02:01:09.100]   And it might be fun to see
[02:01:09.100 --> 02:01:11.500]   if you have anything interesting to say,
[02:01:11.500 --> 02:01:13.300]   which is about the trolley problem.
[02:01:13.300 --> 02:01:19.220]   So a trolley problem is a interesting philosophical construct
[02:01:19.220 --> 02:01:23.380]   that highlights, and there's many others like it,
[02:01:23.380 --> 02:01:25.940]   of the difficult ethical decisions
[02:01:25.940 --> 02:01:29.740]   that we humans have before us
[02:01:29.740 --> 02:01:31.540]   in this complicated world.
[02:01:31.540 --> 02:01:35.300]   So specifically is the choice between
[02:01:35.300 --> 02:01:40.300]   if you were forced to choose to kill a group X of people
[02:01:40.300 --> 02:01:44.060]   versus a group Y of people, like one person.
[02:01:44.060 --> 02:01:46.860]   If you did nothing, you would kill one person,
[02:01:46.860 --> 02:01:49.740]   but if you would kill five people,
[02:01:49.740 --> 02:01:51.340]   and if you decide to swerve out of the way,
[02:01:51.340 --> 02:01:53.180]   you would only kill one person.
[02:01:53.180 --> 02:01:55.660]   Do you do nothing or you choose to do something?
[02:01:55.660 --> 02:01:58.100]   And you can construct all kinds of sort of
[02:01:58.100 --> 02:02:01.100]   ethical experiments of this kind that,
[02:02:01.100 --> 02:02:05.460]   I think at least on a positive note,
[02:02:05.460 --> 02:02:09.580]   inspire you to think about, like introspect
[02:02:09.580 --> 02:02:14.580]   what are the physics of our morality?
[02:02:14.580 --> 02:02:18.220]   And there's usually not good answers there.
[02:02:18.220 --> 02:02:19.620]   I think people love it
[02:02:19.620 --> 02:02:22.060]   'cause it's just an exciting thing to think about.
[02:02:22.060 --> 02:02:24.620]   I think people who build autonomous vehicles
[02:02:24.620 --> 02:02:26.060]   usually roll their eyes
[02:02:27.180 --> 02:02:29.940]   because this is not,
[02:02:29.940 --> 02:02:32.060]   this one as constructed,
[02:02:32.060 --> 02:02:35.140]   this like literally never comes up in reality.
[02:02:35.140 --> 02:02:37.180]   You never have to choose between killing
[02:02:37.180 --> 02:02:38.260]   (laughs)
[02:02:38.260 --> 02:02:41.260]   one or like one of two groups of people.
[02:02:41.260 --> 02:02:45.300]   But I wonder if you can speak to,
[02:02:45.300 --> 02:02:48.820]   is there something interesting
[02:02:48.820 --> 02:02:51.460]   to you as an engineer of autonomous vehicles
[02:02:51.460 --> 02:02:53.500]   that's within the trolley problem,
[02:02:53.500 --> 02:02:54.900]   or maybe more generally,
[02:02:54.940 --> 02:02:58.020]   are there difficult ethical decisions
[02:02:58.020 --> 02:03:00.940]   that you find that a algorithm must make?
[02:03:00.940 --> 02:03:03.460]   - On the specific version of the trolley problem,
[02:03:03.460 --> 02:03:05.260]   which one would you do?
[02:03:05.260 --> 02:03:06.780]   If you're driving?
[02:03:06.780 --> 02:03:09.580]   - The question itself is a profound question
[02:03:09.580 --> 02:03:12.020]   because we humans ourselves cannot answer it.
[02:03:12.020 --> 02:03:13.860]   And that's the very point.
[02:03:13.860 --> 02:03:16.020]   I would kill both.
[02:03:16.020 --> 02:03:18.100]   (laughs)
[02:03:18.100 --> 02:03:20.420]   - Humans, I think you're exactly right
[02:03:20.420 --> 02:03:22.180]   in that humans are not particularly good.
[02:03:22.180 --> 02:03:23.580]   I think the kind of phrase is like,
[02:03:23.580 --> 02:03:24.500]   what would a computer do?
[02:03:24.500 --> 02:03:27.220]   But humans are not very good.
[02:03:27.220 --> 02:03:30.820]   And actually oftentimes I think that freezing
[02:03:30.820 --> 02:03:32.060]   and kind of not doing anything
[02:03:32.060 --> 02:03:34.580]   because like you've taken a few extra milliseconds
[02:03:34.580 --> 02:03:35.420]   to just process,
[02:03:35.420 --> 02:03:37.180]   and then you end up like doing the worst
[02:03:37.180 --> 02:03:38.260]   of possible outcomes, right?
[02:03:38.260 --> 02:03:41.260]   So I do think that as you've pointed out,
[02:03:41.260 --> 02:03:43.420]   it can be a bit of a distraction
[02:03:43.420 --> 02:03:45.620]   and it can be a bit of a kind of a red herring.
[02:03:45.620 --> 02:03:47.940]   I think it's an interesting discussion
[02:03:47.940 --> 02:03:50.740]   in the realm of philosophy, right?
[02:03:50.740 --> 02:03:52.540]   But in terms of what,
[02:03:52.540 --> 02:03:55.900]   how that affects the actual engineering
[02:03:55.900 --> 02:03:57.900]   and deployment of self-driving vehicles,
[02:03:57.900 --> 02:04:02.380]   it's not how you go about building a system, right?
[02:04:02.380 --> 02:04:04.580]   We've talked about how you engineer a system,
[02:04:04.580 --> 02:04:07.580]   how you go about evaluating the different components
[02:04:07.580 --> 02:04:09.860]   and the safety of the entire thing.
[02:04:09.860 --> 02:04:14.540]   How do you kind of inject the various model-based,
[02:04:14.540 --> 02:04:15.620]   safety-based,
[02:04:15.620 --> 02:04:16.460]   or I'm gonna say like,
[02:04:16.460 --> 02:04:18.660]   yes, you reason at parts of the system,
[02:04:18.660 --> 02:04:21.940]   you reason about the probability of a collision,
[02:04:21.940 --> 02:04:24.260]   the severity of that collision, right?
[02:04:24.260 --> 02:04:26.020]   And that is incorporated,
[02:04:26.020 --> 02:04:28.180]   and you have to properly reason about the uncertainty
[02:04:28.180 --> 02:04:29.300]   that flows through the system, right?
[02:04:29.300 --> 02:04:34.140]   So those factors definitely play a role
[02:04:34.140 --> 02:04:35.820]   in how the cars then behave,
[02:04:35.820 --> 02:04:38.300]   but they tend to be more of like the emergent behavior.
[02:04:38.300 --> 02:04:39.900]   And what you see, like you're absolutely right,
[02:04:39.900 --> 02:04:43.780]   that these clear theoretical problems that they,
[02:04:43.780 --> 02:04:46.060]   you don't have a car that in system,
[02:04:46.060 --> 02:04:48.500]   and really kind of being back to our previous discussion,
[02:04:48.500 --> 02:04:51.460]   of like, which one do you choose?
[02:04:51.460 --> 02:04:55.660]   Well, oftentimes you made a mistake earlier,
[02:04:55.660 --> 02:04:57.500]   like you shouldn't be in that situation
[02:04:57.500 --> 02:04:58.700]   in the first place, right?
[02:04:58.700 --> 02:05:00.740]   And in reality, the system comes up.
[02:05:00.740 --> 02:05:03.900]   If you build a very good, safe and capable driver,
[02:05:03.900 --> 02:05:07.500]   you have enough clues in the environment
[02:05:07.500 --> 02:05:09.460]   that you drive defensively,
[02:05:09.460 --> 02:05:11.340]   so you don't put yourself in that situation, right?
[02:05:11.340 --> 02:05:14.100]   And again, if you go back to that analogy
[02:05:14.100 --> 02:05:15.180]   of precision and recall,
[02:05:15.180 --> 02:05:17.740]   like, okay, you can make a very hard trade-off,
[02:05:17.740 --> 02:05:19.540]   but neither answer is really good.
[02:05:19.540 --> 02:05:21.500]   But what instead you focus on
[02:05:21.500 --> 02:05:24.020]   is kind of moving the whole curve up,
[02:05:24.020 --> 02:05:26.020]   and then you focus on building the right capability
[02:05:26.020 --> 02:05:27.420]   and the right defensive driving
[02:05:27.420 --> 02:05:30.180]   so that you don't put yourself in a situation like this.
[02:05:30.180 --> 02:05:33.140]   - I don't know if you have a good answer for this,
[02:05:33.140 --> 02:05:36.580]   but people love it when I ask this question about books.
[02:05:36.580 --> 02:05:41.100]   Are there books in your life
[02:05:41.100 --> 02:05:44.700]   that you've enjoyed, philosophical, fiction, technical,
[02:05:44.700 --> 02:05:47.100]   that had a big impact on you as an engineer
[02:05:47.100 --> 02:05:48.940]   or as a human being?
[02:05:48.940 --> 02:05:50.460]   You know, everything from science fiction
[02:05:50.460 --> 02:05:51.980]   to a favorite textbook.
[02:05:51.980 --> 02:05:55.620]   Is there three books that stand out that you can think of?
[02:05:55.620 --> 02:05:56.460]   - Three books.
[02:05:56.460 --> 02:05:59.460]   So I would, you know, that impacted me,
[02:05:59.460 --> 02:06:00.820]   I would say,
[02:06:00.820 --> 02:06:06.500]   this one is, you probably know it well,
[02:06:06.500 --> 02:06:09.660]   but not generally well known,
[02:06:09.660 --> 02:06:12.300]   I think in the US or kind of internationally,
[02:06:12.300 --> 02:06:14.380]   "The Master and Margarita."
[02:06:14.380 --> 02:06:18.320]   It's one of, actually, my favorite books.
[02:06:19.320 --> 02:06:21.520]   It is by a Russian,
[02:06:21.520 --> 02:06:25.240]   it's a novel by Russian author Mikhail Bulgakov.
[02:06:25.240 --> 02:06:26.840]   And it's just, it's a great book.
[02:06:26.840 --> 02:06:27.680]   And it's one of those books
[02:06:27.680 --> 02:06:29.680]   that you can like reread your entire life,
[02:06:29.680 --> 02:06:31.040]   and it's very accessible.
[02:06:31.040 --> 02:06:32.400]   You can read it as a kid.
[02:06:32.400 --> 02:06:35.280]   And like, it's, you know, the plot is interesting.
[02:06:35.280 --> 02:06:36.800]   It's, you know, the devil, you know,
[02:06:36.800 --> 02:06:38.280]   visiting the Soviet Union.
[02:06:38.280 --> 02:06:41.040]   But it, like, you read it,
[02:06:41.040 --> 02:06:43.760]   reread it at different stages of your life,
[02:06:43.760 --> 02:06:47.960]   and you enjoy it for different, very different reasons.
[02:06:47.960 --> 02:06:50.600]   And you keep finding like deeper and deeper meaning.
[02:06:50.600 --> 02:06:52.000]   And, you know, it kind of affected,
[02:06:52.000 --> 02:06:52.840]   you know, it had a,
[02:06:52.840 --> 02:06:55.080]   definitely had an imprint on me,
[02:06:55.080 --> 02:06:57.720]   mostly from the,
[02:06:57.720 --> 02:07:00.240]   probably kind of the cultural stylistic aspect.
[02:07:00.240 --> 02:07:01.080]   Like it makes you,
[02:07:01.080 --> 02:07:02.640]   it's one of those books that, you know,
[02:07:02.640 --> 02:07:03.480]   is good and makes you think,
[02:07:03.480 --> 02:07:05.640]   but also has like this really, you know,
[02:07:05.640 --> 02:07:08.160]   silly, quirky, dark sense of, you know, humor.
[02:07:08.160 --> 02:07:09.640]   - It captures the Russian soul
[02:07:09.640 --> 02:07:11.640]   more than perhaps many other books.
[02:07:11.640 --> 02:07:14.440]   On that like slight note, just out of curiosity,
[02:07:14.440 --> 02:07:17.160]   one of the saddest things is I've read that book
[02:07:17.160 --> 02:07:18.320]   in English.
[02:07:18.320 --> 02:07:22.600]   Did you by chance read it in English or in Russian?
[02:07:22.600 --> 02:07:24.000]   - In Russian, only in Russian.
[02:07:24.000 --> 02:07:26.160]   And I actually, that is a question I had.
[02:07:26.160 --> 02:07:29.960]   Kind of posed to myself every once in a while.
[02:07:29.960 --> 02:07:31.680]   Like I wonder how well it translates,
[02:07:31.680 --> 02:07:33.040]   if it translates at all.
[02:07:33.040 --> 02:07:34.360]   And there's the language aspect of it,
[02:07:34.360 --> 02:07:35.680]   and then there's the cultural aspect.
[02:07:35.680 --> 02:07:37.760]   So I, and actually I'm not sure if, you know,
[02:07:37.760 --> 02:07:40.840]   either of those would work well in English.
[02:07:40.840 --> 02:07:42.200]   - Now I forget their names,
[02:07:42.200 --> 02:07:44.160]   but so when the COVID lifts a little bit,
[02:07:44.160 --> 02:07:45.720]   I'm traveling to Paris
[02:07:46.680 --> 02:07:48.280]   for several reasons.
[02:07:48.280 --> 02:07:49.600]   One is just, I've never been to Paris.
[02:07:49.600 --> 02:07:50.440]   I want to go to Paris,
[02:07:50.440 --> 02:07:54.400]   but there's the most famous translators
[02:07:54.400 --> 02:07:58.440]   of Dostoevsky, Tolstoy, of most of Russian literature
[02:07:58.440 --> 02:07:59.280]   live there.
[02:07:59.280 --> 02:08:01.680]   There's a couple, they're famous, a man and a woman.
[02:08:01.680 --> 02:08:03.960]   And I'm going to sort of have a series of conversations
[02:08:03.960 --> 02:08:04.800]   with them.
[02:08:04.800 --> 02:08:05.920]   And in preparation for that,
[02:08:05.920 --> 02:08:08.200]   I'm starting to read Dostoevsky in Russian.
[02:08:08.200 --> 02:08:10.720]   So I'm really embarrassed to say that I've read this,
[02:08:10.720 --> 02:08:12.800]   everything I've read in Russian literature
[02:08:12.800 --> 02:08:16.760]   of like serious depth has been in English,
[02:08:16.760 --> 02:08:21.520]   even though I can also read, I mean, obviously in Russian,
[02:08:21.520 --> 02:08:24.800]   but for some reason it seemed,
[02:08:24.800 --> 02:08:29.040]   in the optimization of life,
[02:08:29.040 --> 02:08:31.040]   it seemed the improper decision to do it,
[02:08:31.040 --> 02:08:32.200]   to read in Russian.
[02:08:32.200 --> 02:08:35.080]   Like, you know, like I don't need to opt,
[02:08:35.080 --> 02:08:37.200]   I need to think in English, not in Russian,
[02:08:37.200 --> 02:08:38.960]   but now I'm changing my mind on that.
[02:08:38.960 --> 02:08:41.160]   And so the question of how well it translate
[02:08:41.160 --> 02:08:42.360]   is a really fundamental one.
[02:08:42.360 --> 02:08:44.120]   Like it, even with Dostoevsky.
[02:08:44.120 --> 02:08:47.000]   So from what I understand, Dostoevsky translates easier.
[02:08:47.000 --> 02:08:49.680]   Others don't as much.
[02:08:49.680 --> 02:08:52.520]   Obviously the poetry doesn't translate as well.
[02:08:52.520 --> 02:08:57.520]   I'm also the music, big fan of Vladimir Vysotsky.
[02:08:57.520 --> 02:09:00.320]   He doesn't obviously translate well.
[02:09:00.320 --> 02:09:01.320]   People have tried.
[02:09:01.320 --> 02:09:04.480]   But mastermind, I don't know.
[02:09:04.480 --> 02:09:05.640]   I don't know about that one.
[02:09:05.640 --> 02:09:06.960]   I just know it in English, you know,
[02:09:06.960 --> 02:09:08.520]   it's fun as hell in English.
[02:09:08.520 --> 02:09:11.320]   So, but it's a curious question
[02:09:11.320 --> 02:09:13.000]   and I wanna study it rigorously
[02:09:13.000 --> 02:09:15.480]   from both the machine learning aspect
[02:09:15.480 --> 02:09:17.880]   and also because I want to do a couple of interviews
[02:09:17.880 --> 02:09:22.880]   in Russia that I'm still unsure
[02:09:22.880 --> 02:09:27.160]   of how to properly conduct an interview
[02:09:27.160 --> 02:09:28.760]   across a language barrier.
[02:09:28.760 --> 02:09:31.880]   It's a fascinating question that ultimately communicates
[02:09:31.880 --> 02:09:32.880]   to an American audience.
[02:09:32.880 --> 02:09:36.880]   There's a few Russian people that I think
[02:09:36.880 --> 02:09:39.240]   are truly special human beings.
[02:09:39.240 --> 02:09:44.160]   And I feel like I sometimes encounter this
[02:09:44.160 --> 02:09:45.800]   with some incredible scientists
[02:09:45.800 --> 02:09:48.960]   and maybe you encounter this as well
[02:09:48.960 --> 02:09:52.000]   at some point in your life that it feels like
[02:09:52.000 --> 02:09:53.280]   because of the language barrier,
[02:09:53.280 --> 02:09:55.480]   their ideas are lost to history.
[02:09:55.480 --> 02:09:56.400]   It's a sad thing.
[02:09:56.400 --> 02:10:00.040]   I think about like Chinese scientists or even authors
[02:10:00.040 --> 02:10:04.600]   that like, that we don't in English speaking world
[02:10:04.600 --> 02:10:07.440]   don't get to appreciate some like the depth of the culture
[02:10:07.440 --> 02:10:09.040]   because it's lost in translation.
[02:10:09.040 --> 02:10:13.240]   And I feel like I would love to show that to the world.
[02:10:13.240 --> 02:10:17.000]   Like I'm just some idiot, but because I have this,
[02:10:17.000 --> 02:10:20.920]   like at least some semblance of skill in speaking Russian,
[02:10:20.920 --> 02:10:23.080]   I feel like, and I know how to record stuff
[02:10:23.080 --> 02:10:25.080]   on a video camera.
[02:10:25.080 --> 02:10:27.480]   I feel like I wanna catch like Grigori Pearlman,
[02:10:27.480 --> 02:10:28.480]   who's a mathematician.
[02:10:28.480 --> 02:10:30.320]   I'm not sure if you're familiar with him.
[02:10:30.320 --> 02:10:32.720]   I wanna talk to him, like he's a fascinating mind
[02:10:32.720 --> 02:10:36.080]   and to bring him to a wider audience in English speaking,
[02:10:36.080 --> 02:10:36.960]   it'll be fascinating.
[02:10:36.960 --> 02:10:40.120]   But that requires to be rigorous about this question
[02:10:40.120 --> 02:10:44.160]   of how well Bulgakov translates.
[02:10:44.160 --> 02:10:46.840]   I mean, I know it's a silly concept,
[02:10:46.840 --> 02:10:50.440]   but it's a fundamental one because how do you translate?
[02:10:50.440 --> 02:10:54.080]   And that's the thing that Google Translate is also facing
[02:10:54.080 --> 02:10:57.000]   as a more machine learning problem.
[02:10:57.000 --> 02:11:00.600]   But I wonder as a more bigger problem for AI,
[02:11:00.600 --> 02:11:04.520]   how do we capture the magic that's there in the language?
[02:11:05.800 --> 02:11:08.920]   - I think that's really interesting,
[02:11:08.920 --> 02:11:10.600]   really challenging problem.
[02:11:10.600 --> 02:11:14.160]   If you do read it, Master and Margarita in English,
[02:11:14.160 --> 02:11:18.440]   sorry, in Russian, I'd be curious to get your opinion.
[02:11:18.440 --> 02:11:19.880]   And I think part of it is language,
[02:11:19.880 --> 02:11:22.000]   but part of it's just centuries of culture
[02:11:22.000 --> 02:11:23.600]   that the cultures are different.
[02:11:23.600 --> 02:11:26.360]   So it's hard to connect that.
[02:11:26.360 --> 02:11:29.360]   - Okay, so that was my first one, right?
[02:11:29.360 --> 02:11:30.880]   You had two more.
[02:11:30.880 --> 02:11:34.960]   The second one, I would probably pick the science fiction
[02:11:34.960 --> 02:11:37.280]   by the Strogowski brothers.
[02:11:37.280 --> 02:11:39.320]   I know it's up there with Isaac Asimov
[02:11:39.320 --> 02:11:42.600]   and Ray Bradbury and company.
[02:11:42.600 --> 02:11:46.680]   The Strogowski brothers kind of appealed more to me.
[02:11:46.680 --> 02:11:49.920]   I think it made more of an impression on me growing up.
[02:11:49.920 --> 02:11:55.080]   - Can you, I apologize if I'm showing my complete ignorance.
[02:11:55.080 --> 02:11:56.680]   I'm so weak on sci-fi.
[02:11:56.680 --> 02:11:57.920]   What did they write?
[02:11:57.920 --> 02:12:01.360]   - Oh, "Roadside Picnic."
[02:12:04.120 --> 02:12:06.080]   "Hard to Be a God."
[02:12:06.080 --> 02:12:11.200]   "Beetle in an Ant Hill."
[02:12:11.200 --> 02:12:13.920]   "Monday Starts on Saturday."
[02:12:13.920 --> 02:12:15.600]   Like it's not just science fiction.
[02:12:15.600 --> 02:12:17.560]   It's also like has very interesting,
[02:12:17.560 --> 02:12:20.280]   interpersonal and societal questions.
[02:12:20.280 --> 02:12:25.280]   And some of the language is just completely hilarious.
[02:12:25.280 --> 02:12:27.960]   - (speaking in foreign language)
[02:12:27.960 --> 02:12:29.080]   - That's the one. - That's right.
[02:12:29.080 --> 02:12:29.920]   - Oh, interesting.
[02:12:29.920 --> 02:12:31.160]   "Monday Starts on Saturday."
[02:12:31.160 --> 02:12:32.880]   So I need to read, okay.
[02:12:32.880 --> 02:12:34.040]   - Oh boy.
[02:12:34.040 --> 02:12:36.680]   You put that in the category of science fiction?
[02:12:36.680 --> 02:12:40.200]   - That one is, I mean, this was more of a silly,
[02:12:40.200 --> 02:12:41.760]   humorous work.
[02:12:41.760 --> 02:12:43.240]   I mean, there is kind of--
[02:12:43.240 --> 02:12:44.640]   - But it's profound too, right?
[02:12:44.640 --> 02:12:47.600]   - Science fiction, right, is about this research institute.
[02:12:47.600 --> 02:12:52.160]   It has deep parallels to like serious research,
[02:12:52.160 --> 02:12:56.120]   but the setting of course is that they're working on magic.
[02:12:56.120 --> 02:12:59.560]   And there's a lot of, so I, and that's their style, right?
[02:12:59.560 --> 02:13:03.280]   They go, and other books are very different, right?
[02:13:03.280 --> 02:13:04.360]   "Hard to Be a God," right?
[02:13:04.360 --> 02:13:07.120]   It's about kind of this higher society being injected
[02:13:07.120 --> 02:13:10.040]   into this primitive world and how they operate there.
[02:13:10.040 --> 02:13:13.840]   Some of the very deep ethical questions there, right?
[02:13:13.840 --> 02:13:15.520]   And like they've got this full spectrum.
[02:13:15.520 --> 02:13:18.480]   Some is more about kind of more adventure style.
[02:13:18.480 --> 02:13:20.200]   But like I enjoy all of their books.
[02:13:20.200 --> 02:13:22.560]   There's probably a couple, actually one I think
[02:13:22.560 --> 02:13:24.960]   that they considered their most important work.
[02:13:24.960 --> 02:13:28.560]   I think it's "The Snail on a Hill."
[02:13:28.560 --> 02:13:30.200]   I don't know exactly how it translates.
[02:13:30.200 --> 02:13:31.480]   I tried reading a couple of times.
[02:13:31.480 --> 02:13:35.000]   I still don't get it, but everything else I fully enjoyed.
[02:13:35.000 --> 02:13:36.520]   And like for one of my birthdays as a kid,
[02:13:36.520 --> 02:13:38.360]   I got like their entire collection,
[02:13:38.360 --> 02:13:40.200]   like occupied a giant shelf in my room.
[02:13:40.200 --> 02:13:42.680]   And then I'll like over the holidays, I just like,
[02:13:42.680 --> 02:13:44.040]   my parents couldn't drag me out of the room
[02:13:44.040 --> 02:13:45.840]   and I read the whole thing cover to cover
[02:13:45.840 --> 02:13:48.880]   and I really enjoyed it.
[02:13:48.880 --> 02:13:52.080]   - And that's the one more, for the third one,
[02:13:52.080 --> 02:13:54.840]   maybe a little bit darker, but you know,
[02:13:54.840 --> 02:13:57.760]   comes to mind is Orwell's "1984."
[02:13:58.600 --> 02:14:01.840]   And you asked what made an impression on me
[02:14:01.840 --> 02:14:03.480]   and the books that people should read.
[02:14:03.480 --> 02:14:06.080]   That one I think falls in the category of both.
[02:14:06.080 --> 02:14:07.960]   Definitely it's one of those books that you read
[02:14:07.960 --> 02:14:11.440]   and you just kind of put it down
[02:14:11.440 --> 02:14:13.160]   and you stay in space for a while.
[02:14:13.160 --> 02:14:15.560]   That kind of work.
[02:14:15.560 --> 02:14:20.560]   I think there's lessons there people should not ignore.
[02:14:20.560 --> 02:14:25.800]   And nowadays with everything that's happening in the world,
[02:14:25.800 --> 02:14:30.800]   I can't help it, but have my mind jump to some parallels
[02:14:30.800 --> 02:14:32.520]   with what Orwell described.
[02:14:32.520 --> 02:14:36.520]   And like there's this whole concept of double think
[02:14:36.520 --> 02:14:39.640]   and ignoring logic and holding completely contradictory
[02:14:39.640 --> 02:14:42.120]   opinions in your mind and not have that not bother you
[02:14:42.120 --> 02:14:45.000]   and sticking to the party line at all costs.
[02:14:45.000 --> 02:14:48.360]   Like there's something there.
[02:14:48.360 --> 02:14:50.920]   - If anything, 2020 has taught me,
[02:14:50.920 --> 02:14:52.640]   and I'm a huge fan of Animal Farm,
[02:14:52.640 --> 02:14:56.880]   which is a kind of friendly, is a friend of "1984"
[02:14:56.880 --> 02:14:57.720]   by Orwell.
[02:14:57.720 --> 02:15:01.600]   It's kind of another thought experiment
[02:15:01.600 --> 02:15:05.120]   of how our society may go in directions
[02:15:05.120 --> 02:15:07.480]   that we wouldn't like it to go.
[02:15:07.480 --> 02:15:12.480]   But if anything that's been kind of heartbreaking
[02:15:12.480 --> 02:15:19.040]   to an optimist about 2020 is that society's kind of fragile.
[02:15:21.000 --> 02:15:24.600]   Like we have this, this is a special little experiment
[02:15:24.600 --> 02:15:28.960]   we have going on and it's not unbreakable.
[02:15:28.960 --> 02:15:32.360]   Like we should be careful to like preserve
[02:15:32.360 --> 02:15:34.680]   whatever the special thing we have going on.
[02:15:34.680 --> 02:15:38.800]   I mean, I think "1984" and these books, "Brave New World",
[02:15:38.800 --> 02:15:43.680]   they're helpful in thinking like stuff can go wrong
[02:15:43.680 --> 02:15:45.360]   in non-obvious ways.
[02:15:45.360 --> 02:15:48.480]   And it's like, it's up to us to preserve it.
[02:15:48.480 --> 02:15:50.120]   And it's like, it's a responsibility.
[02:15:50.120 --> 02:15:51.280]   It's been weighing heavy on me.
[02:15:51.280 --> 02:15:52.880]   'Cause like, for some reason,
[02:15:52.880 --> 02:15:57.480]   like more than my mom follows me on Twitter.
[02:15:57.480 --> 02:16:01.000]   And I feel like I have like now somehow responsibility
[02:16:01.000 --> 02:16:05.440]   to this world.
[02:16:05.440 --> 02:16:10.440]   And it dawned on me that like me and millions of others
[02:16:10.440 --> 02:16:14.760]   had like the little ants that maintain this little colony.
[02:16:14.760 --> 02:16:15.600]   Right?
[02:16:15.600 --> 02:16:18.120]   So we have a responsibility not to be,
[02:16:18.120 --> 02:16:19.320]   I don't know what the right analogy is,
[02:16:19.320 --> 02:16:22.760]   but put a flamethrower to the place.
[02:16:22.760 --> 02:16:24.960]   We wanna not do that.
[02:16:24.960 --> 02:16:27.120]   And there's interesting, complicated ways of doing that
[02:16:27.120 --> 02:16:28.600]   as "1984" shows.
[02:16:28.600 --> 02:16:29.800]   It could be through bureaucracy,
[02:16:29.800 --> 02:16:31.440]   it could be through incompetence,
[02:16:31.440 --> 02:16:33.320]   it could be through misinformation,
[02:16:33.320 --> 02:16:35.760]   it could be through division and toxicity.
[02:16:35.760 --> 02:16:38.760]   I'm a huge believer in like that love
[02:16:38.760 --> 02:16:42.480]   will be the somehow the solution.
[02:16:42.480 --> 02:16:45.080]   So love and robots.
[02:16:45.080 --> 02:16:45.920]   (laughing)
[02:16:45.920 --> 02:16:46.740]   - Love and robots, yeah.
[02:16:46.740 --> 02:16:47.580]   I think you're exactly right.
[02:16:47.580 --> 02:16:50.400]   Unfortunately, I think it's less of a flamethrower
[02:16:50.400 --> 02:16:51.240]   type of an answer.
[02:16:51.240 --> 02:16:52.080]   I think it's more of a,
[02:16:52.080 --> 02:16:54.480]   in many cases can be more of a slow boil.
[02:16:54.480 --> 02:16:55.680]   And that's the danger.
[02:16:55.680 --> 02:17:00.320]   - Let me ask, it's a fun thing to make
[02:17:00.320 --> 02:17:04.280]   a world-class roboticist, engineer and leader
[02:17:04.280 --> 02:17:07.680]   uncomfortable with a ridiculous question about life.
[02:17:07.680 --> 02:17:10.840]   What is the meaning of life, Dmitry,
[02:17:10.840 --> 02:17:13.540]   from a robotics and a human perspective?
[02:17:13.540 --> 02:17:16.120]   - You only have a couple minutes,
[02:17:16.120 --> 02:17:17.360]   or one minute to answer.
[02:17:17.360 --> 02:17:18.200]   So.
[02:17:18.200 --> 02:17:20.040]   (laughing)
[02:17:20.040 --> 02:17:21.040]   - I don't know if that makes it more difficult
[02:17:21.040 --> 02:17:22.040]   or easier, actually.
[02:17:22.040 --> 02:17:26.040]   Yeah, you know, I'm very tempted to quote
[02:17:26.040 --> 02:17:33.360]   one of the stories by Isaac Asimov, actually.
[02:17:33.360 --> 02:17:37.040]   Actually titled, appropriately titled,
[02:17:37.040 --> 02:17:39.360]   "The Last Question," a short story,
[02:17:39.360 --> 02:17:43.000]   where the plot is that humans build this supercomputer,
[02:17:43.000 --> 02:17:44.920]   you know, this AI intelligence,
[02:17:44.920 --> 02:17:48.000]   and once it gets powerful enough,
[02:17:48.000 --> 02:17:49.720]   they pose this question to it.
[02:17:49.720 --> 02:17:52.920]   How can the entropy in the universe be reduced?
[02:17:52.920 --> 02:17:54.560]   Right, so the computer replies,
[02:17:54.560 --> 02:17:57.080]   hang on, as of yet, insufficient information
[02:17:57.080 --> 02:17:58.560]   to give a meaningful answer, right?
[02:17:58.560 --> 02:18:00.400]   And then, you know, thousands of years go by
[02:18:00.400 --> 02:18:01.640]   and they keep posing the same question.
[02:18:01.640 --> 02:18:03.780]   The computer gets more and more powerful
[02:18:03.780 --> 02:18:05.560]   and keeps giving the same answer.
[02:18:05.560 --> 02:18:07.280]   As of yet, insufficient information
[02:18:07.280 --> 02:18:08.400]   to give a meaningful answer,
[02:18:08.400 --> 02:18:09.980]   or something along those lines, right?
[02:18:09.980 --> 02:18:12.680]   And then, you know, it keeps happening
[02:18:12.680 --> 02:18:14.200]   and happening, you fast forward,
[02:18:14.200 --> 02:18:16.040]   like millions of years into the future
[02:18:16.040 --> 02:18:18.160]   and billions of years, and like at some point,
[02:18:18.160 --> 02:18:19.920]   it's just the only entity in the universe.
[02:18:19.920 --> 02:18:21.680]   It's like it's absorbed all humanity
[02:18:21.680 --> 02:18:22.920]   and all knowledge in the universe,
[02:18:22.920 --> 02:18:25.480]   and it keeps posing the same question to itself.
[02:18:25.480 --> 02:18:29.120]   And, you know, finally, it gets to the point
[02:18:29.120 --> 02:18:30.920]   where it is able to answer that question.
[02:18:30.920 --> 02:18:32.280]   But of course, at that point, you know,
[02:18:32.280 --> 02:18:34.760]   there's the heat death of the universe has occurred,
[02:18:34.760 --> 02:18:35.720]   and that's the only entity,
[02:18:35.720 --> 02:18:38.360]   and there's nobody else to provide that answer to.
[02:18:38.360 --> 02:18:40.520]   So the only thing it can do is to, you know,
[02:18:40.520 --> 02:18:41.600]   answer it by demonstration.
[02:18:41.600 --> 02:18:44.320]   So it like, you know, recreates the Big Bang, right?
[02:18:44.320 --> 02:18:45.720]   And resets the clock, right?
[02:18:45.720 --> 02:18:47.120]   (both laughing)
[02:18:47.120 --> 02:18:51.480]   But I can try to give kind of a different version
[02:18:51.480 --> 02:18:54.880]   of the answer, you know, maybe not on the behalf
[02:18:54.880 --> 02:18:55.720]   of all humanity.
[02:18:55.720 --> 02:18:57.480]   I think that might be a little presumptuous
[02:18:57.480 --> 02:18:59.220]   for me to speak about the meaning of life
[02:18:59.220 --> 02:19:01.480]   on the behalf of all humans,
[02:19:01.480 --> 02:19:05.040]   but at least, you know, personally, it changes, right?
[02:19:05.040 --> 02:19:09.240]   I think if you think about kind of what gives,
[02:19:10.360 --> 02:19:13.480]   you know, you and your life meaning and purpose
[02:19:13.480 --> 02:19:14.880]   and kind of what drives you,
[02:19:14.880 --> 02:19:19.840]   it seems to change over time, right?
[02:19:19.840 --> 02:19:24.840]   And the lifespan of, you know, kind of your existence,
[02:19:24.840 --> 02:19:27.240]   you know, when you just enter this world, right?
[02:19:27.240 --> 02:19:28.920]   It's all about kind of new experiences, right?
[02:19:28.920 --> 02:19:33.240]   You get like new smells, new sounds, new emotions, right?
[02:19:33.240 --> 02:19:35.600]   And like, that's what's driving you, right?
[02:19:35.600 --> 02:19:38.280]   You're experiencing new, amazing things, right?
[02:19:38.280 --> 02:19:39.560]   And that's magical, right?
[02:19:39.560 --> 02:19:41.480]   That's pretty awesome, right?
[02:19:41.480 --> 02:19:43.120]   That gives you kind of a meaning.
[02:19:43.120 --> 02:19:44.480]   Then, you know, you get a little bit older,
[02:19:44.480 --> 02:19:49.480]   you start more intentionally learning about things, right?
[02:19:49.480 --> 02:19:51.480]   I guess actually before you start intentionally learning,
[02:19:51.480 --> 02:19:52.300]   probably fun.
[02:19:52.300 --> 02:19:54.680]   Fun is a thing that gives you kind of meaning and purpose
[02:19:54.680 --> 02:19:56.920]   and the thing you optimize for, right?
[02:19:56.920 --> 02:19:58.120]   And like fun is good.
[02:19:58.120 --> 02:20:00.640]   Then you get, you know, start learning.
[02:20:00.640 --> 02:20:05.640]   And I guess that this joy of comprehension
[02:20:05.720 --> 02:20:08.800]   and discovery is another thing that, you know,
[02:20:08.800 --> 02:20:11.440]   gives you meaning and purpose and drives you, right?
[02:20:11.440 --> 02:20:14.120]   Then, you know, you learn enough stuff
[02:20:14.120 --> 02:20:17.080]   and you wanna give some of it back, right?
[02:20:17.080 --> 02:20:19.880]   And so impact and contributions back to technology
[02:20:19.880 --> 02:20:24.880]   or society, people, you know, local or more globally
[02:20:24.880 --> 02:20:27.720]   becomes a new thing that drives a lot of kind
[02:20:27.720 --> 02:20:31.540]   of your behavior and is something that gives you purpose
[02:20:31.540 --> 02:20:35.360]   and that you derive positive feedback from, right?
[02:20:35.360 --> 02:20:37.440]   You know, then you go and so on and so forth.
[02:20:37.440 --> 02:20:39.400]   You go through various stages of life.
[02:20:39.400 --> 02:20:44.400]   If you have kids, like that definitely changes
[02:20:44.400 --> 02:20:45.880]   your perspective on things.
[02:20:45.880 --> 02:20:48.800]   You know, I have three that definitely flips some bits
[02:20:48.800 --> 02:20:51.640]   in your head in terms of kind of what you care about
[02:20:51.640 --> 02:20:53.840]   and what you optimize for and, you know, what matters,
[02:20:53.840 --> 02:20:55.040]   what doesn't matter, right?
[02:20:55.040 --> 02:20:57.240]   So, you know, and so on and so forth, right?
[02:20:57.240 --> 02:21:01.840]   And it seems to me that, you know, it's all of those things.
[02:21:01.840 --> 02:21:06.560]   And as you kind of go through life, you know,
[02:21:06.560 --> 02:21:08.640]   you want these to be additive, right?
[02:21:08.640 --> 02:21:12.240]   New experiences, fun, learning, impact.
[02:21:12.240 --> 02:21:14.200]   Like you wanna, you know, be accumulating.
[02:21:14.200 --> 02:21:16.320]   I don't wanna, you know, stop having fun
[02:21:16.320 --> 02:21:17.760]   or experiencing new things.
[02:21:17.760 --> 02:21:19.120]   And I think it's important that, you know,
[02:21:19.120 --> 02:21:22.080]   just kind of becomes additive as opposed
[02:21:22.080 --> 02:21:23.760]   to a replacement or subtraction.
[02:21:23.760 --> 02:21:27.000]   But, you know, those few as far as I got,
[02:21:27.000 --> 02:21:28.040]   but, you know, ask me in a few years,
[02:21:28.040 --> 02:21:30.320]   I might have one or two more to add to the list.
[02:21:30.320 --> 02:21:32.400]   - And before you know it, time is up,
[02:21:32.400 --> 02:21:34.960]   just like it is for this conversation.
[02:21:34.960 --> 02:21:37.000]   But hopefully it was a fun ride.
[02:21:37.000 --> 02:21:38.200]   It was a huge honor to meet you.
[02:21:38.200 --> 02:21:41.360]   As you know, I've been a fan of yours
[02:21:41.360 --> 02:21:43.880]   and a fan of Google self-driving car
[02:21:43.880 --> 02:21:45.800]   and Waymo for a long time.
[02:21:45.800 --> 02:21:46.680]   I can't wait.
[02:21:46.680 --> 02:21:48.560]   I mean, it's one of the most exciting.
[02:21:48.560 --> 02:21:50.240]   If we look back in the 21st century,
[02:21:50.240 --> 02:21:52.520]   I truly believe it will be one of the most exciting things
[02:21:52.520 --> 02:21:56.240]   we descendants of apes have created on this earth.
[02:21:56.240 --> 02:22:01.040]   So I'm a huge fan and I can't wait to see what you do next.
[02:22:01.040 --> 02:22:02.440]   Thanks so much for talking to me.
[02:22:02.440 --> 02:22:03.280]   - Thanks.
[02:22:03.280 --> 02:22:04.120]   Thanks for having me.
[02:22:04.120 --> 02:22:05.800]   And it's also a huge fan.
[02:22:05.800 --> 02:22:09.280]   Doing work on this and I really enjoy this.
[02:22:09.280 --> 02:22:10.600]   Thank you.
[02:22:10.600 --> 02:22:12.080]   - Thanks for listening to this conversation
[02:22:12.080 --> 02:22:13.200]   with Dmitry Dolgov.
[02:22:13.200 --> 02:22:14.880]   And thank you to our sponsors.
[02:22:14.880 --> 02:22:18.440]   Trial Labs, a company that helps businesses
[02:22:18.440 --> 02:22:21.480]   apply machine learning to solve real world problems.
[02:22:21.480 --> 02:22:23.280]   Blinkist, an app I use for reading
[02:22:23.280 --> 02:22:25.080]   through summaries of books.
[02:22:25.080 --> 02:22:28.600]   BetterHelp, online therapy with a licensed professional
[02:22:28.600 --> 02:22:31.840]   and Cash App, the app I use to send money to friends.
[02:22:31.840 --> 02:22:34.000]   Please check out these sponsors in the description
[02:22:34.000 --> 02:22:37.120]   to get a discount and to support this podcast.
[02:22:37.120 --> 02:22:39.560]   If you enjoy this thing, subscribe on YouTube,
[02:22:39.560 --> 02:22:41.720]   review it with 5 Stars on Apple Podcasts,
[02:22:41.720 --> 02:22:44.280]   follow on Spotify, support on Patreon
[02:22:44.280 --> 02:22:47.160]   or connect with me on Twitter @LexFriedman.
[02:22:47.160 --> 02:22:51.480]   And now let me leave you with some words from Isaac Asimov.
[02:22:51.480 --> 02:22:54.360]   Science can amuse and fascinate us all,
[02:22:54.360 --> 02:22:57.080]   but it is engineering that changes the world.
[02:22:57.080 --> 02:23:00.800]   Thank you for listening and hope to see you next time.
[02:23:00.800 --> 02:23:03.380]   (upbeat music)
[02:23:03.380 --> 02:23:05.960]   (upbeat music)
[02:23:05.960 --> 02:23:15.960]   [BLANK_AUDIO]

