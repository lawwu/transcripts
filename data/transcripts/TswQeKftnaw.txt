
[00:00:00.000 --> 00:00:14.740]   .
[00:00:14.740 --> 00:00:17.040]   Thank you all so much for coming to this talk.
[00:00:17.040 --> 00:00:18.880]   Thank you for being at this conference generally.
[00:00:18.880 --> 00:00:19.820]   My name is Tomas.
[00:00:19.820 --> 00:00:21.800]   I'm one of the co-founders of Graphite,
[00:00:21.800 --> 00:00:24.100]   and I'm here to talk to you around AI power entomology.
[00:00:24.100 --> 00:00:26.080]   If you don't know, entomology is the study of bugs.
[00:00:26.080 --> 00:00:28.960]   It's something that is very near and dear to our heart,
[00:00:28.960 --> 00:00:30.860]   and part of what our product does.
[00:00:30.860 --> 00:00:33.720]   So Graphite, for those of you that don't know,
[00:00:33.720 --> 00:00:34.800]   builds a product called Diamond.
[00:00:34.800 --> 00:00:36.400]   Diamond's an AI-powered code reviewer.
[00:00:36.400 --> 00:00:39.520]   You go ahead, you connect it to your GitHub,
[00:00:39.520 --> 00:00:41.340]   and it goes ahead and finds bugs.
[00:00:41.340 --> 00:00:42.940]   The project started about a year ago.
[00:00:42.940 --> 00:00:45.220]   What we started to notice was that the amount of code
[00:00:45.220 --> 00:00:47.580]   being written by AI was going up and up and up,
[00:00:47.580 --> 00:00:49.160]   but so was the amount of bugs.
[00:00:49.160 --> 00:00:50.680]   And after really thinking about it,
[00:00:50.680 --> 00:00:54.240]   we thought that this might actually be part and parcel,
[00:00:54.240 --> 00:00:55.880]   and what we need to do is we need to find a way
[00:00:55.880 --> 00:00:59.880]   to better address these bugs in general.
[00:00:59.880 --> 00:01:01.520]   Given the technological advances,
[00:01:01.520 --> 00:01:03.680]   the first thing we turned to was AI itself,
[00:01:03.680 --> 00:01:05.140]   and we started to ask, well,
[00:01:05.140 --> 00:01:06.940]   maybe AI is creating the bugs,
[00:01:06.940 --> 00:01:08.200]   but can it also find the bugs?
[00:01:08.200 --> 00:01:09.180]   Can it help us?
[00:01:09.180 --> 00:01:13.040]   We started to go ahead and do things like ask Claude,
[00:01:13.040 --> 00:01:14.640]   hey, here's a PR.
[00:01:14.640 --> 00:01:16.420]   Can you find bugs on this PR?
[00:01:16.420 --> 00:01:18.720]   And we were pretty impressed with the early results.
[00:01:18.720 --> 00:01:20.760]   Here's an example actually pulled from this week
[00:01:20.760 --> 00:01:23.600]   from our code base where it turns out that in certain instances,
[00:01:23.600 --> 00:01:26.180]   we'd be returning one of our database ORM classes
[00:01:26.180 --> 00:01:29.100]   uninstantiated, which would go ahead and crash our server.
[00:01:29.100 --> 00:01:30.960]   Here's an example that came up on Twitter this week
[00:01:30.960 --> 00:01:34.640]   from our bot that found that in certain instances,
[00:01:34.640 --> 00:01:36.980]   there would be math being done around border radiuses
[00:01:36.980 --> 00:01:38.920]   that would lead to a division by a negative number,
[00:01:38.920 --> 00:01:41.900]   and would then go ahead and crash the front end.
[00:01:41.900 --> 00:01:44.920]   So to answer the question, it turns out AI can find bugs.
[00:01:44.920 --> 00:01:46.200]   That's the end of the talk.
[00:01:46.200 --> 00:01:48.600]   I'm kidding if you've tried this.
[00:01:48.600 --> 00:01:49.960]   You know you've probably had a really,
[00:01:49.960 --> 00:01:51.780]   really frustrating experience.
[00:01:51.780 --> 00:01:53.940]   We also went ahead and saw things like,
[00:01:53.940 --> 00:01:57.060]   you should update this code to do what it already does.
[00:01:57.060 --> 00:01:59.820]   CSS doesn't work this way when it does.
[00:01:59.820 --> 00:02:02.140]   Or my favorite, you should revert this code
[00:02:02.140 --> 00:02:04.320]   to what it used to do because it used to do it.
[00:02:04.320 --> 00:02:08.560]   Getting those lost us a lot of confidence,
[00:02:08.560 --> 00:02:10.340]   but we started to think, well,
[00:02:10.340 --> 00:02:11.660]   we're seeing some really good things
[00:02:11.660 --> 00:02:13.080]   and we're seeing some really bad things,
[00:02:13.080 --> 00:02:15.200]   and maybe there's actually more than one type of bug.
[00:02:15.200 --> 00:02:18.000]   Maybe there's more than one type of thing an LLM can find.
[00:02:18.000 --> 00:02:20.920]   And so we started with the most basic division of,
[00:02:20.920 --> 00:02:24.100]   well, there's probably stuff that LLMs are good at catching
[00:02:24.100 --> 00:02:25.700]   and things that they're not good at catching.
[00:02:25.700 --> 00:02:28.180]   At the end of the day, LLMs ultimately try and mimic
[00:02:28.180 --> 00:02:29.560]   the thing that you're asking them to do.
[00:02:29.560 --> 00:02:30.600]   And if you ask them, hey,
[00:02:30.600 --> 00:02:32.960]   what kind of code review comments would be left on this PR?
[00:02:32.960 --> 00:02:34.280]   It goes ahead and leaves everything,
[00:02:34.280 --> 00:02:36.180]   both those that are within its capability
[00:02:36.180 --> 00:02:38.680]   and things that are not within its capability.
[00:02:38.680 --> 00:02:40.740]   And so we started to categorize those.
[00:02:40.740 --> 00:02:43.480]   What we found though was even when we categorize those,
[00:02:43.480 --> 00:02:46.160]   the LLM would start to leave comments like this.
[00:02:46.160 --> 00:02:48.860]   You should add a comment describing what this class does.
[00:02:48.860 --> 00:02:51.440]   You should extract this logic out into a function,
[00:02:51.440 --> 00:02:53.860]   or you should make sure this code has tests.
[00:02:53.860 --> 00:02:57.200]   Well, these are technically correct.
[00:02:57.200 --> 00:02:58.820]   To developers, they're really frustrating.
[00:02:58.820 --> 00:03:01.320]   And I think this was actually one of the most insightful moments
[00:03:01.320 --> 00:03:02.960]   for us in building this project,
[00:03:02.960 --> 00:03:04.880]   was when we sat down with our design team
[00:03:04.880 --> 00:03:08.180]   and we started to actually go through past bugs,
[00:03:08.180 --> 00:03:13.220]   both those left by our bot and by humans in our own codebase.
[00:03:13.220 --> 00:03:15.640]   The developers were all pretty much on the same page of like,
[00:03:15.640 --> 00:03:17.640]   yep, I'd be okay if an LLM left that.
[00:03:17.640 --> 00:03:19.740]   No, I would not be okay if an LLM left that.
[00:03:19.740 --> 00:03:21.100]   Yes, I'd be okay.
[00:03:21.100 --> 00:03:23.180]   And our designers were actually kind of baffled by it.
[00:03:23.180 --> 00:03:24.100]   They're like, well, but like,
[00:03:24.100 --> 00:03:26.320]   that kind of looks like that other comment.
[00:03:26.320 --> 00:03:28.920]   And I think that what's happening here in the mind of the developer
[00:03:28.920 --> 00:03:32.520]   is if you go ahead and you read a type of comment like this,
[00:03:32.520 --> 00:03:35.400]   maybe you find it pedantic, frustrating, annoying
[00:03:35.400 --> 00:03:36.700]   when it comes from LLM,
[00:03:36.700 --> 00:03:40.060]   and you're much more welcoming to it when it comes from a human.
[00:03:40.060 --> 00:03:41.700]   And so, as we started to think more around
[00:03:41.700 --> 00:03:43.700]   sort of that classification of bugs,
[00:03:43.700 --> 00:03:45.940]   we started to think around actually second access here,
[00:03:45.940 --> 00:03:49.200]   which was there's stuff LLMs can catch and LLMs can't catch,
[00:03:49.200 --> 00:03:52.080]   but there's also stuff that humans want to receive from an LLM
[00:03:52.080 --> 00:03:54.480]   and humans don't want to receive from an LLM.
[00:03:54.480 --> 00:04:00.300]   And so, what we went ahead and did was we went ahead
[00:04:00.300 --> 00:04:03.780]   and we actually took 10,000 comments from our own codebase,
[00:04:03.780 --> 00:04:07.280]   from open source codebases, open source codebases,
[00:04:07.280 --> 00:04:08.820]   and we fed them to various LLMs
[00:04:08.820 --> 00:04:10.720]   and we asked them to categorize them.
[00:04:10.720 --> 00:04:13.140]   And we did that not just once,
[00:04:13.140 --> 00:04:14.360]   but we did that quite a few times.
[00:04:14.360 --> 00:04:16.620]   And then we went ahead and we summarized those comments.
[00:04:16.620 --> 00:04:19.400]   And what we ended up with was actually this chart,
[00:04:19.400 --> 00:04:21.720]   where it says there's actually quite a few different types
[00:04:21.720 --> 00:04:24.500]   of bugs that you see left on codebases in the wild.
[00:04:24.500 --> 00:04:27.240]   Ignoring LLMs for a second, just talking around humans,
[00:04:27.240 --> 00:04:28.660]   you see things which are bugs,
[00:04:28.660 --> 00:04:30.880]   those are logical inconsistencies that lead the code
[00:04:30.880 --> 00:04:32.740]   to behave in a way it doesn't want to behave.
[00:04:32.740 --> 00:04:34.440]   There's also accidentally committed code.
[00:04:34.440 --> 00:04:36.740]   This actually shows up more than you would expect.
[00:04:36.740 --> 00:04:38.620]   There are performance and security concerns.
[00:04:38.620 --> 00:04:40.800]   There's documentation where the code says one thing
[00:04:40.800 --> 00:04:43.440]   and does another and it's not clear which one's right.
[00:04:43.440 --> 00:04:45.440]   There's stylistic changes, things like,
[00:04:45.440 --> 00:04:48.380]   hey, you should update this comment
[00:04:48.380 --> 00:04:50.800]   or in this codebase we follow this other pattern.
[00:04:50.800 --> 00:04:52.300]   And then there's a lot of stuff outside
[00:04:52.300 --> 00:04:53.660]   of sort of that top right quadrant.
[00:04:53.660 --> 00:04:57.440]   So in the bottom right where humans want to receive it,
[00:04:57.440 --> 00:04:59.900]   but the LLMs don't seem to be able to get there yet,
[00:04:59.900 --> 00:05:01.680]   are things like tribal knowledge.
[00:05:01.680 --> 00:05:04.220]   One class of comment that you'll see a lot in PRs is,
[00:05:04.220 --> 00:05:06.500]   hey, we used to do it this way.
[00:05:06.500 --> 00:05:08.500]   We don't do it this way anymore because of blank.
[00:05:08.500 --> 00:05:10.500]   This documentation doesn't exist.
[00:05:10.500 --> 00:05:12.500]   It exists in the heads of your senior developers.
[00:05:12.500 --> 00:05:15.860]   And that's wonderful, but it's really hard for an AI
[00:05:15.860 --> 00:05:17.860]   to be able to mind read to that.
[00:05:17.860 --> 00:05:20.500]   On the left side where LLMs definitely can catch it,
[00:05:20.500 --> 00:05:21.880]   but humans don't want to receive
[00:05:21.880 --> 00:05:23.400]   are those things I showed you earlier,
[00:05:23.400 --> 00:05:25.220]   code cleanliness and best practice.
[00:05:25.220 --> 00:05:28.940]   Examples of these that we've found are comment this function,
[00:05:28.940 --> 00:05:31.720]   add tests, extract this type out into a different type,
[00:05:31.720 --> 00:05:34.080]   extract this logic out into a function.
[00:05:34.080 --> 00:05:37.180]   While this is always correct to say,
[00:05:37.180 --> 00:05:39.900]   I think it's really hard to know when to apply to an LLM.
[00:05:39.900 --> 00:05:42.560]   I think as a human, you're applying some kind of barometer
[00:05:42.560 --> 00:05:46.080]   of, well, in this codebase, this logic is particularly tricky
[00:05:46.080 --> 00:05:47.260]   and I think someone's gonna get tripped up
[00:05:47.260 --> 00:05:50.320]   so we should extract it out versus, well, in this codebase,
[00:05:50.320 --> 00:05:51.700]   it's actually fine.
[00:05:51.700 --> 00:05:54.060]   But what a bot can pretty much always leave this comment.
[00:05:54.060 --> 00:05:55.360]   I'd actually make the argument a human
[00:05:55.360 --> 00:05:56.680]   can pretty much always leave this comment
[00:05:56.680 --> 00:05:58.280]   and it'd be technically correct.
[00:05:58.280 --> 00:06:01.320]   The question is whether it's welcome in the codebase.
[00:06:01.320 --> 00:06:04.680]   And one thing I'm gonna say sort of like outside of all of this
[00:06:04.680 --> 00:06:08.620]   is as you add more, this area seems to become larger
[00:06:08.620 --> 00:06:11.080]   of what people are comfortable with, but for now,
[00:06:11.080 --> 00:06:13.340]   given the context that we have, given the codebase,
[00:06:13.340 --> 00:06:17.000]   the past history, your style guide and rules,
[00:06:17.000 --> 00:06:19.580]   we are what we have, we have what we have.
[00:06:19.580 --> 00:06:23.320]   And so we ended up with this idea of, well, it turns out that
[00:06:23.320 --> 00:06:26.440]   these are basically the classes of comments
[00:06:26.440 --> 00:06:30.620]   that we think that human, that LLMs can both create
[00:06:30.620 --> 00:06:32.100]   and humans want to receive.
[00:06:32.100 --> 00:06:37.260]   Now, if you've worked with LLMs, you know that these kinds of
[00:06:37.260 --> 00:06:39.640]   offline passes and first passes are great
[00:06:39.640 --> 00:06:42.580]   for initial categorizations, but the much harder question is,
[00:06:42.580 --> 00:06:44.800]   how do you know that you're right continuously, right?
[00:06:44.800 --> 00:06:47.800]   So we can, so as the story goes, we went ahead.
[00:06:47.800 --> 00:06:50.660]   We basically started to characterize comments that LLMs leave.
[00:06:50.660 --> 00:06:53.240]   We updated our prompts to only prompt the LLM to do things
[00:06:53.240 --> 00:06:56.740]   that were in its capacity and that humans wanted to receive.
[00:06:56.740 --> 00:06:59.580]   And people anecdotally started to like it a lot more.
[00:06:59.580 --> 00:07:01.900]   But as we started to then think around, well,
[00:07:01.900 --> 00:07:03.600]   how can we get this LLM to,
[00:07:03.600 --> 00:07:05.140]   how do we know that this is going right?
[00:07:05.140 --> 00:07:06.640]   As we think around new LLMs,
[00:07:06.640 --> 00:07:10.420]   as we got into Claude 4 or Opus instead of Sonnet,
[00:07:10.420 --> 00:07:12.460]   how do we know that we're actually staying
[00:07:12.460 --> 00:07:13.760]   in this top right quadrant?
[00:07:13.760 --> 00:07:15.460]   And as we increase the context,
[00:07:15.460 --> 00:07:19.040]   how do we know that this isn't growing on us?
[00:07:19.040 --> 00:07:20.840]   And actually, maybe there are even more types of comments
[00:07:20.840 --> 00:07:23.160]   that we could be leaving that we're not leaving already.
[00:07:24.280 --> 00:07:26.680]   And so first and foremost,
[00:07:26.680 --> 00:07:28.720]   we started by just looking at what kinds of comments
[00:07:28.720 --> 00:07:30.660]   is the thing currently leaving?
[00:07:30.660 --> 00:07:31.760]   Your mileage may vary.
[00:07:31.760 --> 00:07:34.080]   For us, this is roughly the proportion we see
[00:07:34.080 --> 00:07:36.780]   of comments being left by the LLM right now
[00:07:36.780 --> 00:07:39.040]   based just on what we've seen.
[00:07:39.040 --> 00:07:41.340]   But the deeper question for us was,
[00:07:41.340 --> 00:07:43.620]   how do we measure the success, right?
[00:07:43.620 --> 00:07:44.620]   Like given this quadrant,
[00:07:44.620 --> 00:07:47.260]   how do we know that we're in the top right?
[00:07:47.260 --> 00:07:48.820]   The first one was easy for us.
[00:07:48.820 --> 00:07:51.680]   So think around what they can catch and they can't catch.
[00:07:51.680 --> 00:07:53.860]   What we started to do was we started to actually add
[00:07:53.860 --> 00:07:55.240]   upvotes and downvotes to the product.
[00:07:55.240 --> 00:07:57.880]   So we let you go ahead and emoji react in these comments,
[00:07:57.880 --> 00:08:00.240]   and they pretty much tell us when the LLM's hallucinating,
[00:08:00.240 --> 00:08:04.600]   when we start to see a downvote spike.
[00:08:04.600 --> 00:08:05.820]   We know that, okay,
[00:08:05.820 --> 00:08:07.300]   we might be trying to extend this thing
[00:08:07.300 --> 00:08:10.600]   beyond its capabilities, we need to tone it down.
[00:08:10.600 --> 00:08:12.020]   But the second one was a lot harder.
[00:08:12.020 --> 00:08:14.760]   The humans want to receive and humans don't want to receive
[00:08:14.760 --> 00:08:18.800]   was something that we weren't really sure how to get at.
[00:08:18.800 --> 00:08:21.600]   And so upvote, downvote, we implemented it.
[00:08:21.600 --> 00:08:24.260]   We see about a less than a 4% downvote rate these days.
[00:08:24.260 --> 00:08:26.500]   We felt pretty good about that.
[00:08:26.500 --> 00:08:28.320]   The second one, as we started to think around it,
[00:08:28.320 --> 00:08:29.320]   well, what we realized was,
[00:08:29.320 --> 00:08:30.440]   well, what's the point of a comment?
[00:08:30.440 --> 00:08:32.000]   Why do you leave a comment in code review?
[00:08:32.000 --> 00:08:33.880]   You leave a comment in code review ultimately
[00:08:33.880 --> 00:08:36.440]   so that someone actually updates the code to reflect that.
[00:08:36.440 --> 00:08:38.560]   And so our question was, well, can we measure that?
[00:08:38.560 --> 00:08:40.440]   Can we measure what percent of comments
[00:08:40.440 --> 00:08:43.080]   actually lead to the change that they describe?
[00:08:43.080 --> 00:08:44.360]   And so we started to do that.
[00:08:44.360 --> 00:08:45.920]   And we started to ask that question of,
[00:08:45.920 --> 00:08:48.400]   on open source repos and on the variety of repos
[00:08:48.400 --> 00:08:51.140]   that Graphite, which is a code review tool, has access to,
[00:08:51.140 --> 00:08:52.820]   can we actually start to measure that number?
[00:08:52.820 --> 00:08:56.540]   And I think one of the most fascinating things we found
[00:08:56.540 --> 00:08:59.960]   was that only about 50% of human comments lead to changes.
[00:08:59.960 --> 00:09:01.860]   And so we started to ask the question of, well,
[00:09:01.860 --> 00:09:04.580]   could we get the LLM to at least this, right?
[00:09:04.580 --> 00:09:05.900]   Because if we get it to at least this,
[00:09:05.900 --> 00:09:08.520]   it's at least leaving comments on the level of fidelity
[00:09:08.520 --> 00:09:09.540]   that humans are.
[00:09:09.540 --> 00:09:11.680]   Now, you might be saying in the audience and being like,
[00:09:11.680 --> 00:09:15.680]   well, why don't 100% of comments lead to action?
[00:09:15.680 --> 00:09:17.460]   I want to caveat this number.
[00:09:17.460 --> 00:09:20.140]   I'm saying lead to action within that PR itself.
[00:09:20.140 --> 00:09:22.700]   And so a lot of comments are sometimes fixed forward,
[00:09:22.700 --> 00:09:25.180]   where people are like, hey, I hear you,
[00:09:25.180 --> 00:09:26.920]   and I'm going to fix this in a follow-up.
[00:09:26.920 --> 00:09:29.860]   A lot of comments are also like, hey, as a heads up,
[00:09:29.860 --> 00:09:32.520]   in the future if you do this, maybe you can do it this other way,
[00:09:32.520 --> 00:09:33.700]   but don't need to be acted on them.
[00:09:33.700 --> 00:09:36.100]   And I think there's a fair-- and some of them
[00:09:36.100 --> 00:09:38.740]   are just purely preferential of, I would do it this way,
[00:09:38.740 --> 00:09:40.060]   someone disagrees.
[00:09:40.060 --> 00:09:42.400]   In healthy code review cultures, that space for disagreement
[00:09:42.400 --> 00:09:43.420]   exists.
[00:09:43.420 --> 00:09:44.920]   And so we started to measure this.
[00:09:44.920 --> 00:09:47.860]   And we started to say, could we get the bot here?
[00:09:47.860 --> 00:09:48.960]   Over time, we actually have.
[00:09:48.960 --> 00:09:50.580]   So as of March, we're at 52%, which
[00:09:50.580 --> 00:09:53.740]   is to say that if you start to actually prompt it correctly,
[00:09:53.740 --> 00:09:54.860]   you can get there.
[00:09:54.860 --> 00:09:56.820]   And I think our sort of broader thesis
[00:09:56.820 --> 00:09:59.700]   is that this measuring--
[00:09:59.700 --> 00:10:05.060]   getting bugs via an LLM does actually work.
[00:10:05.060 --> 00:10:08.260]   If you want to try any of these findings in production,
[00:10:08.260 --> 00:10:10.420]   Diamond is our product that offers it.
[00:10:10.420 --> 00:10:13.060]   We have a booth over there.
[00:10:13.060 --> 00:10:14.500]   Thank you.
[00:10:14.500 --> 00:10:15.500]   Thank you.
[00:10:15.500 --> 00:10:20.500]   Thank you.

