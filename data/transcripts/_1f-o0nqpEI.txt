
[00:00:00.000 --> 00:00:02.720]   The following is a conversation with Dylan Patel
[00:00:02.720 --> 00:00:04.680]   and Nathan Lampert.
[00:00:04.680 --> 00:00:07.200]   Dylan runs Semianalysis,
[00:00:07.200 --> 00:00:10.420]   a well-respected research and analysis company
[00:00:10.420 --> 00:00:14.100]   that specializes in semiconductors, GPUs, CPUs,
[00:00:14.100 --> 00:00:16.880]   and AI hardware in general.
[00:00:16.880 --> 00:00:18.880]   Nathan is a research scientist
[00:00:18.880 --> 00:00:21.380]   at the Allen Institute for AI,
[00:00:21.380 --> 00:00:24.520]   and is the author of the amazing blog
[00:00:24.520 --> 00:00:27.200]   on AI called "Interconnects."
[00:00:27.200 --> 00:00:31.000]   They are both highly respected, read, and listened to
[00:00:31.000 --> 00:00:33.280]   by the experts, researchers, and engineers
[00:00:33.280 --> 00:00:34.840]   in the field of AI.
[00:00:34.840 --> 00:00:38.340]   And personally, I'm just a fan of the two of them.
[00:00:38.340 --> 00:00:41.240]   So I used the deep-seek moment
[00:00:41.240 --> 00:00:43.840]   that shook the AI world a bit
[00:00:43.840 --> 00:00:46.400]   as an opportunity to sit down with them
[00:00:46.400 --> 00:00:47.800]   and lay it all out.
[00:00:47.800 --> 00:00:50.080]   From deep-seek, open AI, Google, XAI,
[00:00:50.080 --> 00:00:53.860]   meta-anthropic to NVIDIA and TSMC,
[00:00:53.860 --> 00:00:57.500]   and to US, China, Taiwan relations,
[00:00:57.500 --> 00:00:59.400]   and everything else that is happening
[00:00:59.400 --> 00:01:01.540]   at the cutting edge of AI.
[00:01:01.540 --> 00:01:03.860]   This conversation is a deep dive
[00:01:03.860 --> 00:01:07.960]   into many critical aspects of the AI industry.
[00:01:07.960 --> 00:01:10.920]   While it does get super technical,
[00:01:10.920 --> 00:01:13.300]   we try to make sure that it's still accessible
[00:01:13.300 --> 00:01:15.460]   to folks outside of the AI field
[00:01:15.460 --> 00:01:17.060]   by defining terms,
[00:01:17.060 --> 00:01:19.360]   stating important concepts explicitly,
[00:01:19.360 --> 00:01:21.800]   spelling out acronyms, and in general,
[00:01:21.800 --> 00:01:24.520]   always moving across the several layers of abstraction
[00:01:24.520 --> 00:01:26.620]   and levels of detail.
[00:01:26.620 --> 00:01:29.700]   There is a lot of hype in the media
[00:01:29.700 --> 00:01:32.520]   about what AI is and isn't.
[00:01:32.520 --> 00:01:34.900]   The purpose of this podcast, in part,
[00:01:34.900 --> 00:01:37.040]   is to cut through the hype,
[00:01:37.040 --> 00:01:38.540]   through the bullshit,
[00:01:38.540 --> 00:01:41.220]   and the low-resolution analysis,
[00:01:41.220 --> 00:01:44.060]   and to discuss in detail how stuff works
[00:01:44.060 --> 00:01:45.660]   and what the implications are.
[00:01:45.660 --> 00:01:47.820]   Let me also, if I may,
[00:01:47.820 --> 00:01:51.880]   comment on the new OpenAI 03 Mini reasoning model,
[00:01:51.880 --> 00:01:53.960]   the release of which we were anticipating
[00:01:53.960 --> 00:01:55.440]   during the conversation,
[00:01:55.440 --> 00:01:58.280]   and it did indeed come out right after.
[00:01:58.280 --> 00:02:01.240]   Its capabilities and costs are on par
[00:02:01.240 --> 00:02:03.740]   with our expectations, as we stated.
[00:02:03.740 --> 00:02:08.040]   OpenAI 03 Mini is indeed a great model,
[00:02:08.040 --> 00:02:11.320]   but it should be stated that DeepSeq R1
[00:02:11.320 --> 00:02:13.080]   has similar performance on benchmarks,
[00:02:13.080 --> 00:02:14.160]   is still cheaper,
[00:02:14.160 --> 00:02:17.640]   and it reveals its chain-of-thought reasoning,
[00:02:17.640 --> 00:02:19.520]   which 03 Mini does not.
[00:02:19.520 --> 00:02:22.920]   It only shows a summary of the reasoning.
[00:02:22.920 --> 00:02:25.480]   Plus, R1 is open weight,
[00:02:25.480 --> 00:02:27.920]   and 03 Mini is not.
[00:02:27.920 --> 00:02:32.560]   By the way, I got a chance to play with 03 Mini,
[00:02:32.560 --> 00:02:35.440]   and anecdotal vibe-check-wise,
[00:02:35.440 --> 00:02:37.560]   I felt that 03 Mini,
[00:02:37.560 --> 00:02:39.560]   specifically 03 Mini High,
[00:02:39.560 --> 00:02:42.000]   is better than R1.
[00:02:42.000 --> 00:02:43.680]   Still, for me personally,
[00:02:43.680 --> 00:02:45.820]   I find that Clawed Sonnet 3.5
[00:02:45.820 --> 00:02:47.600]   is the best model for programming.
[00:02:47.600 --> 00:02:48.720]   Except for tricky cases
[00:02:48.720 --> 00:02:52.000]   where I will use 01 Pro to brainstorm.
[00:02:52.000 --> 00:02:55.440]   Either way, many more better AI models will come,
[00:02:55.440 --> 00:02:57.100]   including reasoning models,
[00:02:57.100 --> 00:03:00.560]   both from American and Chinese companies.
[00:03:00.560 --> 00:03:03.480]   They will continue to shift the cost curve.
[00:03:03.480 --> 00:03:07.800]   But the "DeepSeq moment" is indeed real.
[00:03:07.800 --> 00:03:10.640]   I think it will still be remembered five years from now
[00:03:10.640 --> 00:03:13.120]   as a pivotal event in tech history,
[00:03:13.120 --> 00:03:16.080]   due in part to the geopolitical implications,
[00:03:16.080 --> 00:03:18.360]   but for other reasons too,
[00:03:18.360 --> 00:03:19.520]   as we discuss in detail
[00:03:19.520 --> 00:03:22.440]   from many perspectives in this conversation.
[00:03:22.440 --> 00:03:24.320]   This is the Lex Friedman Podcast.
[00:03:24.320 --> 00:03:25.160]   To support it,
[00:03:25.160 --> 00:03:27.760]   please check out our sponsors in the description.
[00:03:27.760 --> 00:03:29.400]   And now, dear friends,
[00:03:29.400 --> 00:03:32.760]   here's Dylan Patel and Nathan Lambert.
[00:03:32.760 --> 00:03:35.240]   A lot of people are curious
[00:03:35.240 --> 00:03:37.400]   to understand China's DeepSeq AI models.
[00:03:37.400 --> 00:03:38.920]   So let's lay it out.
[00:03:38.920 --> 00:03:41.560]   Nathan, can you describe what DeepSeq v3
[00:03:41.560 --> 00:03:43.680]   and DeepSeq R1 are,
[00:03:43.680 --> 00:03:45.600]   how they work, how they're trained?
[00:03:45.600 --> 00:03:47.280]   Let's look at the big picture
[00:03:47.280 --> 00:03:49.320]   and then we'll zoom in on the details.
[00:03:49.320 --> 00:03:54.080]   - Yeah, so DeepSeq v3 is a new mixture of experts,
[00:03:54.080 --> 00:03:56.920]   transformer language model from DeepSeq,
[00:03:56.920 --> 00:03:59.000]   who is based in China.
[00:03:59.000 --> 00:04:02.320]   They have some new specifics in the model
[00:04:02.320 --> 00:04:03.640]   that we'll get into.
[00:04:03.640 --> 00:04:06.260]   Largely, this is a open weight model
[00:04:06.260 --> 00:04:08.280]   and it's a instruction model
[00:04:08.280 --> 00:04:10.080]   like what you would use in ChatGPT.
[00:04:10.080 --> 00:04:13.280]   They also released what is called the base model,
[00:04:13.280 --> 00:04:16.500]   which is before these techniques of post-training.
[00:04:16.500 --> 00:04:18.640]   Most people use instruction models today
[00:04:18.640 --> 00:04:21.740]   and those are what's served in all sorts of applications.
[00:04:21.740 --> 00:04:26.740]   This was released on, I believe, December 26th or that week.
[00:04:26.740 --> 00:04:30.480]   And then weeks later on January 20th,
[00:04:30.480 --> 00:04:33.040]   DeepSeq released DeepSeq R1,
[00:04:33.040 --> 00:04:34.680]   which is a reasoning model,
[00:04:34.680 --> 00:04:38.720]   which really accelerated a lot of this discussion.
[00:04:38.720 --> 00:04:41.840]   This reasoning model has a lot of overlapping training steps
[00:04:41.840 --> 00:04:44.560]   to DeepSeq v3 and it's confusing
[00:04:44.560 --> 00:04:47.040]   that you have a base model called v3,
[00:04:47.040 --> 00:04:50.360]   that you do something to to get a chat model
[00:04:50.360 --> 00:04:51.880]   and then you do some different things
[00:04:51.880 --> 00:04:53.880]   to get a reasoning model.
[00:04:53.880 --> 00:04:55.160]   I think a lot of the AI industry
[00:04:55.160 --> 00:04:57.480]   is going through this challenge of communications right now
[00:04:57.480 --> 00:05:00.040]   where OpenAI makes fun of their own naming schemes.
[00:05:00.040 --> 00:05:04.720]   They have GPT-40, they have OpenAI-01
[00:05:04.720 --> 00:05:06.000]   and there's a lot of types of models.
[00:05:06.000 --> 00:05:08.480]   So we're gonna break down what each of them are.
[00:05:08.480 --> 00:05:11.200]   There's a lot of technical specifics on training
[00:05:11.200 --> 00:05:13.120]   and go from high level to specific
[00:05:13.120 --> 00:05:14.880]   and kind of go through each of them.
[00:05:14.880 --> 00:05:17.120]   - There's so many places we can go here,
[00:05:17.120 --> 00:05:19.280]   but maybe let's go to OpenWeights first.
[00:05:19.280 --> 00:05:21.200]   What does it mean for a model to be OpenWeights
[00:05:21.200 --> 00:05:22.360]   and what are the different flavors
[00:05:22.360 --> 00:05:23.560]   of open source in general?
[00:05:23.560 --> 00:05:26.040]   - Yeah, so this discussion has been going on
[00:05:26.040 --> 00:05:27.200]   for a long time in AI.
[00:05:27.200 --> 00:05:29.320]   It became more important since ChatGPT
[00:05:29.320 --> 00:05:32.600]   or more focal since ChatGPT at the end of 2022.
[00:05:32.600 --> 00:05:34.560]   OpenWeights is the accepted term
[00:05:34.560 --> 00:05:38.120]   for when model weights of a language model
[00:05:38.120 --> 00:05:40.680]   are available on the internet for people to download.
[00:05:40.680 --> 00:05:42.880]   Those weights can have different licenses,
[00:05:42.880 --> 00:05:45.040]   which is effectively the terms
[00:05:45.040 --> 00:05:46.720]   by which you can use the model.
[00:05:46.720 --> 00:05:48.720]   There are licenses that come from history
[00:05:48.720 --> 00:05:49.880]   in open source software.
[00:05:49.880 --> 00:05:51.280]   There are licenses that are designed
[00:05:51.280 --> 00:05:53.160]   by companies specifically.
[00:05:53.160 --> 00:05:57.920]   All of LLAMA, DeepSeek, Quen, Mistral,
[00:05:57.920 --> 00:06:01.640]   these popular names in OpenWeight models
[00:06:01.640 --> 00:06:03.400]   have some of their own licenses.
[00:06:03.400 --> 00:06:05.240]   It's complicated 'cause not all the same models
[00:06:05.240 --> 00:06:06.240]   have the same terms.
[00:06:07.120 --> 00:06:12.120]   The big debate is on what makes a model OpenWeight.
[00:06:12.120 --> 00:06:13.480]   Why are we saying this term?
[00:06:13.480 --> 00:06:14.320]   It's kind of a mouthful.
[00:06:14.320 --> 00:06:17.640]   It sounds close to open source, but it's not the same.
[00:06:17.640 --> 00:06:20.320]   There's still a lot of debate on the definition
[00:06:20.320 --> 00:06:22.400]   and soul of open source AI.
[00:06:22.400 --> 00:06:24.240]   Open source software has a rich history
[00:06:24.240 --> 00:06:27.600]   on freedom to modify, freedom to take on your own,
[00:06:27.600 --> 00:06:28.880]   freedom from any restrictions
[00:06:28.880 --> 00:06:31.240]   on how you would use the software
[00:06:31.240 --> 00:06:34.480]   and what that means for AI is still being defined.
[00:06:34.480 --> 00:06:39.040]   So for what I do, I work at the Allen Institute for AI.
[00:06:39.040 --> 00:06:40.520]   We're a nonprofit.
[00:06:40.520 --> 00:06:42.760]   We want to make AI open for everybody.
[00:06:42.760 --> 00:06:45.560]   And we try to lead on what we think is truly open source.
[00:06:45.560 --> 00:06:47.720]   There's not full agreement in the community,
[00:06:47.720 --> 00:06:50.320]   but for us, that means releasing the training data,
[00:06:50.320 --> 00:06:51.560]   releasing the training code,
[00:06:51.560 --> 00:06:54.200]   and then also having OpenWeights like this.
[00:06:54.200 --> 00:06:56.960]   And we'll get into the details of the models.
[00:06:56.960 --> 00:06:59.760]   And again and again, as we try to get deeper
[00:06:59.760 --> 00:07:02.040]   into how the models were trained,
[00:07:02.040 --> 00:07:05.120]   we will say things like the data processing,
[00:07:05.120 --> 00:07:08.400]   data filtering, data quality is the number one determinant
[00:07:08.400 --> 00:07:09.680]   of the model quality.
[00:07:09.680 --> 00:07:12.120]   And then a lot of the training code is the determinant
[00:07:12.120 --> 00:07:14.000]   on how long it takes to train
[00:07:14.000 --> 00:07:16.400]   and how fast your experimentation is.
[00:07:16.400 --> 00:07:18.800]   So without fully open source models
[00:07:18.800 --> 00:07:20.480]   where you have access to this data,
[00:07:20.480 --> 00:07:23.960]   it is hard to know or it's harder to replicate.
[00:07:23.960 --> 00:07:26.840]   So we'll get into cost numbers for DeepSeek v3
[00:07:26.840 --> 00:07:30.800]   on mostly GPU hours and how much you could pay
[00:07:30.800 --> 00:07:33.160]   to rent those yourselves, but without the data,
[00:07:33.160 --> 00:07:36.080]   the replication cost is going to be far, far higher.
[00:07:36.080 --> 00:07:37.760]   And same goes for the code.
[00:07:37.760 --> 00:07:40.440]   - We should also say that this is probably
[00:07:40.440 --> 00:07:43.920]   one of the more open models out of the frontier models.
[00:07:43.920 --> 00:07:46.560]   So like in this full spectrum
[00:07:46.560 --> 00:07:49.360]   where probably the fullest open source, like you said,
[00:07:49.360 --> 00:07:53.160]   open code, open data, OpenWeights.
[00:07:53.160 --> 00:07:55.800]   This is not open code.
[00:07:55.800 --> 00:07:58.760]   This is probably not open data.
[00:07:59.720 --> 00:08:04.720]   And this is OpenWeights and the licensing is a MIT license
[00:08:04.720 --> 00:08:08.480]   or it's, I mean, there's some nuance in the different models
[00:08:08.480 --> 00:08:10.520]   but it's towards the free,
[00:08:10.520 --> 00:08:11.720]   in terms of the open source movement,
[00:08:11.720 --> 00:08:13.400]   these are the kind of the good guys.
[00:08:13.400 --> 00:08:15.520]   - Yeah, DeepSeek is doing fantastic work
[00:08:15.520 --> 00:08:18.720]   for disseminating understanding of AI.
[00:08:18.720 --> 00:08:22.120]   Their papers are extremely detailed in what they do
[00:08:22.120 --> 00:08:25.320]   and for other teams around the world,
[00:08:25.320 --> 00:08:27.680]   they're very actionable in terms of improving
[00:08:27.680 --> 00:08:30.400]   your own training techniques.
[00:08:30.400 --> 00:08:33.440]   And we'll talk about licenses more.
[00:08:33.440 --> 00:08:37.040]   The DeepSeek R1 model has a very permissive license.
[00:08:37.040 --> 00:08:39.440]   It's called the MIT license that effectively means
[00:08:39.440 --> 00:08:42.160]   there's no downstream restrictions on commercial use.
[00:08:42.160 --> 00:08:44.000]   There's no use case restrictions.
[00:08:44.000 --> 00:08:45.600]   You can use the outputs from the models
[00:08:45.600 --> 00:08:47.480]   to create synthetic data.
[00:08:47.480 --> 00:08:49.760]   And this is all fantastic.
[00:08:49.760 --> 00:08:52.640]   I think the closest peer is something like Llama
[00:08:52.640 --> 00:08:55.560]   where you have the weights and you have a technical report
[00:08:55.560 --> 00:08:58.640]   and the technical report is very good for Llama.
[00:08:58.640 --> 00:09:00.800]   One of the most read PDFs of the year last year
[00:09:00.800 --> 00:09:02.200]   is the Llama 3 paper.
[00:09:02.200 --> 00:09:04.520]   But in some ways it's slightly less actionable.
[00:09:04.520 --> 00:09:06.720]   It has less details on the training specifics,
[00:09:06.720 --> 00:09:09.440]   I think less plots and so on.
[00:09:09.440 --> 00:09:13.000]   And the Llama 3 license is more restrictive than MIT.
[00:09:13.000 --> 00:09:15.200]   And then between the DeepSeek custom license
[00:09:15.200 --> 00:09:16.240]   and the Llama license,
[00:09:16.240 --> 00:09:17.520]   we can get into this whole rabbit hole.
[00:09:17.520 --> 00:09:19.760]   I think we'll make sure we wanna go down the license
[00:09:19.760 --> 00:09:21.680]   rabbit hole before we do specifics.
[00:09:21.680 --> 00:09:23.680]   - Yeah, and I mean, so it should be stated
[00:09:23.680 --> 00:09:25.280]   that one of the implications of DeepSeek,
[00:09:25.280 --> 00:09:28.280]   it puts pressure on Llama and everybody else
[00:09:28.280 --> 00:09:31.920]   on open AI to push towards open source.
[00:09:31.920 --> 00:09:33.280]   And that's the other side of open source
[00:09:33.280 --> 00:09:36.120]   that you mentioned is how much is published
[00:09:36.120 --> 00:09:37.360]   in detail about it.
[00:09:37.360 --> 00:09:39.720]   So how open are you with the sort of
[00:09:39.720 --> 00:09:43.120]   the insights behind the code?
[00:09:43.120 --> 00:09:45.680]   So like how good is the technical reports?
[00:09:45.680 --> 00:09:48.920]   Are they hand wavy or is there actual details in there?
[00:09:48.920 --> 00:09:50.640]   And that's one of the things that DeepSeek did well
[00:09:50.640 --> 00:09:52.520]   is they published a lot of the details.
[00:09:52.520 --> 00:09:54.440]   - Yeah, especially in the DeepSeek v3,
[00:09:54.440 --> 00:09:56.120]   which is their pre-training paper.
[00:09:56.120 --> 00:09:59.800]   They were very clear that they are doing interventions
[00:09:59.800 --> 00:10:03.600]   on the technical stack that go at many different levels.
[00:10:03.600 --> 00:10:06.360]   For example, on there to get highly efficient training,
[00:10:06.360 --> 00:10:09.640]   they're making modifications at or below the CUDA layer
[00:10:09.640 --> 00:10:10.880]   for NVIDIA chips.
[00:10:10.880 --> 00:10:13.480]   I have never worked there myself.
[00:10:13.480 --> 00:10:15.160]   And there are a few people in the world
[00:10:15.160 --> 00:10:16.240]   that do that very well.
[00:10:16.240 --> 00:10:17.600]   And some of them are at DeepSeek.
[00:10:17.600 --> 00:10:20.880]   And these types of people are at DeepSeek
[00:10:20.880 --> 00:10:23.240]   and leading American frontier labs,
[00:10:23.240 --> 00:10:25.120]   but there are not many places.
[00:10:25.120 --> 00:10:27.480]   - To help people understand the other implication
[00:10:27.480 --> 00:10:29.080]   of open weights, just, you know,
[00:10:29.080 --> 00:10:32.360]   there's a topic we'll return to often here.
[00:10:32.360 --> 00:10:37.360]   So there's a fear that China, the nation,
[00:10:37.360 --> 00:10:43.160]   might have interest in stealing American data,
[00:10:43.160 --> 00:10:45.680]   violating privacy of American citizens.
[00:10:45.680 --> 00:10:49.680]   What can we say about open weights to help us understand
[00:10:49.680 --> 00:10:51.640]   what the weights are able to do
[00:10:51.640 --> 00:10:54.920]   in terms of stealing people's data?
[00:10:54.920 --> 00:10:56.760]   - Yeah, so these weights that you can download
[00:10:56.760 --> 00:10:58.280]   from Hugging Face or other platforms
[00:10:58.280 --> 00:11:01.280]   are very big matrices of numbers.
[00:11:01.280 --> 00:11:03.920]   You can download them to a computer in your own house
[00:11:03.920 --> 00:11:05.920]   that has no internet, and you can run this model,
[00:11:05.920 --> 00:11:09.360]   and you're totally in control of your data.
[00:11:09.360 --> 00:11:11.600]   That is something that is different
[00:11:11.600 --> 00:11:13.160]   than how a lot of language model usage
[00:11:13.160 --> 00:11:15.440]   is actually done today, which is mostly through APIs,
[00:11:15.440 --> 00:11:17.920]   where you send your prompt to GPUs
[00:11:17.920 --> 00:11:19.720]   run by certain companies,
[00:11:19.720 --> 00:11:21.640]   and these companies will have different distributions
[00:11:21.640 --> 00:11:23.480]   and policies on how your data is stored,
[00:11:23.480 --> 00:11:25.600]   if it is used to train future models,
[00:11:25.600 --> 00:11:28.560]   where it is stored, if it is encrypted, and so on.
[00:11:28.560 --> 00:11:30.920]   So the open weights are, you have your fate of data
[00:11:30.920 --> 00:11:33.160]   in your own hands, and that is something
[00:11:33.160 --> 00:11:37.160]   that is deeply connected to the soul of open source.
[00:11:37.160 --> 00:11:39.280]   - So it's not the model that steals your data,
[00:11:39.280 --> 00:11:40.960]   it's whoever's hosting the model,
[00:11:40.960 --> 00:11:44.400]   which could be China, if you're using the DeepSeek app,
[00:11:44.400 --> 00:11:46.200]   or it could be Perplexity.
[00:11:47.480 --> 00:11:49.400]   You know, you're trusting them with your data,
[00:11:49.400 --> 00:11:51.240]   or OpenAI, you're trusting them with your data,
[00:11:51.240 --> 00:11:52.760]   and some of these are American companies,
[00:11:52.760 --> 00:11:53.960]   some of these are Chinese companies,
[00:11:53.960 --> 00:11:57.160]   but the model itself is not doing the stealing,
[00:11:57.160 --> 00:11:58.960]   it's the host.
[00:11:58.960 --> 00:12:02.080]   All right, so back to the basics.
[00:12:02.080 --> 00:12:07.080]   What's the difference between DeepSeek V3 and DeepSeek R1?
[00:12:07.080 --> 00:12:11.480]   Can we try to like lay out the confusion potential?
[00:12:11.480 --> 00:12:14.120]   - Yes, so for one, I have very understanding
[00:12:14.120 --> 00:12:16.600]   of many people being confused by these two model names,
[00:12:16.600 --> 00:12:19.000]   so I would say the best way to think about this
[00:12:19.000 --> 00:12:21.080]   is that when training a language model,
[00:12:21.080 --> 00:12:22.440]   you have what is called pre-training,
[00:12:22.440 --> 00:12:25.360]   which is when you're predicting the large amounts
[00:12:25.360 --> 00:12:27.080]   of mostly internet text,
[00:12:27.080 --> 00:12:29.120]   you're trying to predict the next token,
[00:12:29.120 --> 00:12:31.800]   and what to know about these new DeepSeek models
[00:12:31.800 --> 00:12:35.560]   is that they do this internet large-scale pre-training
[00:12:35.560 --> 00:12:38.640]   once to get what is called DeepSeek V3 base.
[00:12:38.640 --> 00:12:40.080]   This is a base model,
[00:12:40.080 --> 00:12:42.760]   it's just going to finish your sentences for you,
[00:12:42.760 --> 00:12:45.840]   it's going to be harder to work with than ChatGPT,
[00:12:45.840 --> 00:12:47.400]   and then what DeepSeek did
[00:12:47.400 --> 00:12:50.560]   is they've done two different post-training regimes
[00:12:50.560 --> 00:12:55.000]   to make the models have specific desirable behaviors.
[00:12:55.000 --> 00:12:57.240]   So what is the more normal model
[00:12:57.240 --> 00:12:59.600]   in terms of the last few years of AI,
[00:12:59.600 --> 00:13:01.600]   an instruct model, a chat model,
[00:13:01.600 --> 00:13:03.800]   a quote-unquote aligned model, a helpful model,
[00:13:03.800 --> 00:13:05.800]   there are many ways to describe this,
[00:13:05.800 --> 00:13:08.040]   is more standard post-training.
[00:13:08.040 --> 00:13:10.200]   So this is things like instruction tuning,
[00:13:10.200 --> 00:13:11.880]   reinforce learning from human feedback,
[00:13:11.880 --> 00:13:13.720]   we'll get into some of these words,
[00:13:13.720 --> 00:13:17.600]   and this is what they did to create the DeepSeek V3 model.
[00:13:17.600 --> 00:13:19.960]   This was the first model to be released,
[00:13:19.960 --> 00:13:22.920]   and it is very high-performance,
[00:13:22.920 --> 00:13:27.880]   it's competitive with GPT-4, LLAMA-405B, so on.
[00:13:27.880 --> 00:13:30.800]   And then when this release was happening,
[00:13:30.800 --> 00:13:32.160]   we don't know their exact timeline,
[00:13:32.160 --> 00:13:34.920]   or soon after, they were finishing the training
[00:13:34.920 --> 00:13:36.480]   of a different training process
[00:13:36.480 --> 00:13:39.680]   from the same next token prediction base model
[00:13:39.680 --> 00:13:41.040]   that I talked about,
[00:13:41.040 --> 00:13:43.040]   which is when this new reasoning training
[00:13:43.040 --> 00:13:45.000]   that people have heard about comes in
[00:13:45.000 --> 00:13:48.560]   in order to create the model that is called DeepSeek R1.
[00:13:48.560 --> 00:13:49.960]   The R through this conversation
[00:13:49.960 --> 00:13:51.520]   is good for grounding for reasoning,
[00:13:51.520 --> 00:13:54.040]   and the name is also similar to OpenAI's O1,
[00:13:54.040 --> 00:13:55.240]   which is the other reasoning model
[00:13:55.240 --> 00:13:57.440]   that people have heard about.
[00:13:57.440 --> 00:13:59.320]   And we'll have to break down the training
[00:13:59.320 --> 00:14:01.000]   for R1 in more detail,
[00:14:01.000 --> 00:14:03.680]   because for one, we have a paper detailing it,
[00:14:03.680 --> 00:14:06.640]   but also it is a far newer set of techniques
[00:14:06.640 --> 00:14:08.000]   for the AI community,
[00:14:08.000 --> 00:14:11.760]   so it is a much more rapidly evolving area of research.
[00:14:11.760 --> 00:14:13.640]   - Maybe we should also say
[00:14:13.640 --> 00:14:15.840]   the big two categories of training
[00:14:15.840 --> 00:14:18.640]   of pre-training and post-training,
[00:14:18.640 --> 00:14:20.400]   these umbrella terms that people use.
[00:14:20.400 --> 00:14:24.640]   So what is pre-training and what is post-training,
[00:14:24.640 --> 00:14:26.080]   and what are the different flavors of things
[00:14:26.080 --> 00:14:27.760]   underneath post-training umbrella?
[00:14:27.760 --> 00:14:29.040]   - Yeah, so pre-training,
[00:14:29.040 --> 00:14:30.200]   I'm using some of the same words
[00:14:30.200 --> 00:14:31.720]   to really get the message across,
[00:14:31.720 --> 00:14:34.240]   is you're doing what is called autoregressive prediction
[00:14:34.240 --> 00:14:37.560]   to predict the next token in a series of documents.
[00:14:37.560 --> 00:14:40.200]   This is done over standard practice
[00:14:40.200 --> 00:14:41.800]   is trillions of tokens.
[00:14:41.800 --> 00:14:44.240]   So this is a ton of data
[00:14:44.240 --> 00:14:46.920]   that is mostly scraped from the web.
[00:14:46.920 --> 00:14:49.120]   In some of DeepSeek's earlier papers,
[00:14:49.120 --> 00:14:50.760]   they talk about their training data
[00:14:50.760 --> 00:14:53.080]   being distilled for math.
[00:14:53.080 --> 00:14:54.240]   I shouldn't use this word yet,
[00:14:54.240 --> 00:14:56.600]   but taken from Common Crawl,
[00:14:56.600 --> 00:14:58.600]   and that's a public access
[00:14:58.600 --> 00:14:59.640]   that anyone listening to this
[00:14:59.640 --> 00:15:01.960]   could go download data from the Common Crawl website.
[00:15:01.960 --> 00:15:04.400]   This is a crawler that is maintained publicly.
[00:15:04.400 --> 00:15:06.560]   Yes, other tech companies eventually shift
[00:15:06.560 --> 00:15:07.600]   to their own crawler,
[00:15:07.600 --> 00:15:09.800]   and DeepSeek likely has done this as well,
[00:15:09.800 --> 00:15:11.400]   as most frontier labs do.
[00:15:11.400 --> 00:15:13.120]   But this sort of data is something
[00:15:13.120 --> 00:15:14.800]   that people can get started with,
[00:15:14.800 --> 00:15:18.280]   and you're just predicting text in a series of documents.
[00:15:18.280 --> 00:15:23.280]   This can be scaled to be very efficient,
[00:15:23.280 --> 00:15:24.520]   and there's a lot of numbers
[00:15:24.520 --> 00:15:25.920]   that are thrown around in AI training,
[00:15:25.920 --> 00:15:30.120]   like how many floating point operations or flops are used.
[00:15:30.120 --> 00:15:31.920]   And then you can also look at how many hours
[00:15:31.920 --> 00:15:35.280]   of these GPUs that are used.
[00:15:35.280 --> 00:15:37.840]   And it's largely one loss function
[00:15:37.840 --> 00:15:42.560]   taken to a very large amount of compute usage.
[00:15:42.560 --> 00:15:45.000]   You just set up really efficient systems.
[00:15:45.000 --> 00:15:46.280]   And then at the end of that,
[00:15:46.280 --> 00:15:47.880]   you have this base model,
[00:15:47.880 --> 00:15:50.520]   and pre-training is where there is a lot more
[00:15:50.520 --> 00:15:55.520]   of complexity in terms of how the process is emerging
[00:15:55.520 --> 00:15:56.920]   or evolving,
[00:15:56.920 --> 00:15:58.920]   and the different types of training losses
[00:15:58.920 --> 00:15:59.960]   that you will use.
[00:15:59.960 --> 00:16:02.760]   I think this is a lot of techniques
[00:16:02.760 --> 00:16:05.960]   grounded in the natural language processing literature.
[00:16:05.960 --> 00:16:07.960]   The oldest technique, which is still used today,
[00:16:07.960 --> 00:16:09.880]   is something called instruction tuning,
[00:16:09.880 --> 00:16:12.520]   or also known as supervised fine-tuning.
[00:16:12.520 --> 00:16:15.840]   These acronyms will be IFT or SFT.
[00:16:15.840 --> 00:16:18.000]   People really go back and forth throughout them,
[00:16:18.000 --> 00:16:19.520]   and I will probably do the same,
[00:16:19.520 --> 00:16:23.040]   which is where you add this formatting to the model
[00:16:23.040 --> 00:16:25.400]   where it knows to take a question
[00:16:25.400 --> 00:16:28.200]   that is like, "Explain the history
[00:16:28.200 --> 00:16:30.000]   of the Roman Empire to me,"
[00:16:30.000 --> 00:16:32.840]   or something, a sort of question you'll see on Reddit
[00:16:32.840 --> 00:16:33.760]   or Stack Overflow,
[00:16:33.760 --> 00:16:37.400]   and then the model will respond in a information-dense
[00:16:37.400 --> 00:16:38.800]   but presentable manner.
[00:16:38.800 --> 00:16:39.800]   The core of that formatting
[00:16:39.800 --> 00:16:42.120]   is in this instruction-tuning phase,
[00:16:42.120 --> 00:16:45.400]   and then there's two other categories of loss functions
[00:16:45.400 --> 00:16:47.240]   that are being used today.
[00:16:47.240 --> 00:16:49.880]   One I will classify as preference fine-tuning.
[00:16:49.880 --> 00:16:52.240]   Preference fine-tuning is a generalized term
[00:16:52.240 --> 00:16:54.240]   for what came out of reinforcement learning
[00:16:54.240 --> 00:16:57.520]   from human feedback, which is RLHF.
[00:16:57.520 --> 00:17:00.240]   This reinforcement learning from human feedback
[00:17:00.240 --> 00:17:02.280]   is credited as the technique
[00:17:02.280 --> 00:17:05.920]   that helped ChatGPT break through.
[00:17:05.920 --> 00:17:08.120]   It is a technique to make the responses
[00:17:08.120 --> 00:17:10.960]   that are nicely formatted, like these Reddit answers,
[00:17:10.960 --> 00:17:14.320]   more in tune with what a human would like to read.
[00:17:14.320 --> 00:17:16.640]   This is done by collecting pairwise preferences
[00:17:16.640 --> 00:17:19.480]   from actual humans out in the world to start,
[00:17:19.480 --> 00:17:21.560]   and now AIs are also labeling this data,
[00:17:21.560 --> 00:17:23.680]   and we'll get into those trade-offs.
[00:17:23.680 --> 00:17:26.840]   And you have this kind of contrastive loss function
[00:17:26.840 --> 00:17:28.560]   between a good answer and a bad answer,
[00:17:28.560 --> 00:17:31.080]   and the model learns to pick up these trends.
[00:17:31.080 --> 00:17:33.280]   There's different implementation ways.
[00:17:33.280 --> 00:17:35.040]   You have things called reward models.
[00:17:35.040 --> 00:17:36.880]   You could have direct alignment algorithms.
[00:17:36.880 --> 00:17:39.280]   There's a lot of really specific things you can do,
[00:17:39.280 --> 00:17:42.720]   but all of this is about fine-tuning to human preferences.
[00:17:42.720 --> 00:17:44.960]   And the final stage is much newer
[00:17:44.960 --> 00:17:48.000]   and will link to what is done in R1,
[00:17:48.000 --> 00:17:50.480]   and these reasoning models is,
[00:17:50.480 --> 00:17:52.160]   I think OpenAI is named for this.
[00:17:52.160 --> 00:17:53.440]   They had this new API in the fall,
[00:17:53.440 --> 00:17:56.200]   which they called the Reinforcement Fine-Tuning API.
[00:17:57.360 --> 00:17:59.320]   This is the idea that you use the techniques
[00:17:59.320 --> 00:18:02.840]   of reinforcement learning, which is a whole framework of AI.
[00:18:02.840 --> 00:18:04.400]   There's a deep literature here.
[00:18:04.400 --> 00:18:08.040]   To summarize, it's often known as trial and error learning,
[00:18:08.040 --> 00:18:10.280]   or the subfield of AI where you're trying
[00:18:10.280 --> 00:18:12.280]   to make sequential decisions
[00:18:12.280 --> 00:18:15.960]   in a certain potentially noisy environment.
[00:18:15.960 --> 00:18:18.160]   There's a lot of ways we could go down that,
[00:18:18.160 --> 00:18:20.040]   but fine-tuning language models
[00:18:20.040 --> 00:18:21.960]   where they can generate an answer,
[00:18:21.960 --> 00:18:23.960]   and then you check to see if the answer
[00:18:23.960 --> 00:18:25.280]   matches the true solution.
[00:18:25.280 --> 00:18:28.880]   For math or code, you have an exactly correct answer
[00:18:28.880 --> 00:18:31.360]   for math, you can have unit tests for code.
[00:18:31.360 --> 00:18:33.040]   And what we're doing is we are checking
[00:18:33.040 --> 00:18:34.360]   the language model's work,
[00:18:34.360 --> 00:18:36.080]   and we're giving it multiple opportunities
[00:18:36.080 --> 00:18:38.160]   on the same question to see if it is right.
[00:18:38.160 --> 00:18:40.080]   And if you keep doing this, the models can learn
[00:18:40.080 --> 00:18:44.280]   to improve in verifiable domains to a great extent.
[00:18:44.280 --> 00:18:45.400]   It works really well.
[00:18:45.400 --> 00:18:47.640]   It's a newer technique in the academic literature.
[00:18:47.640 --> 00:18:50.200]   It's been used at frontier labs in the US
[00:18:50.200 --> 00:18:53.080]   that don't share every detail for multiple years.
[00:18:53.080 --> 00:18:56.600]   So this is the idea of using reinforcement learning
[00:18:56.600 --> 00:18:58.960]   with language models, and it has been taking off,
[00:18:58.960 --> 00:19:00.800]   especially in this DeepSeq moment.
[00:19:00.800 --> 00:19:01.920]   - And we should say that there's a lot
[00:19:01.920 --> 00:19:05.720]   of exciting stuff going on, again, across the stack,
[00:19:05.720 --> 00:19:07.840]   but the post-training, probably this year,
[00:19:07.840 --> 00:19:09.520]   there's going to be a lot of interesting developments
[00:19:09.520 --> 00:19:12.200]   in the post-training, and we'll talk about it.
[00:19:12.200 --> 00:19:14.400]   I almost forgot to talk about the difference
[00:19:14.400 --> 00:19:18.040]   between DeepSeq v3 and R1 on the user experience side.
[00:19:18.040 --> 00:19:21.200]   So forget the technical stuff, forget all of that.
[00:19:21.200 --> 00:19:23.520]   Just people that don't know anything about AI,
[00:19:23.520 --> 00:19:25.920]   they show up, like, what's the actual experience?
[00:19:25.920 --> 00:19:27.640]   What's the use case for each one
[00:19:27.640 --> 00:19:29.840]   when they actually, like, type and talk to it?
[00:19:29.840 --> 00:19:31.720]   What is each good at and that kind of thing?
[00:19:31.720 --> 00:19:33.760]   - So let's start with DeepSeq v3 again.
[00:19:33.760 --> 00:19:36.040]   It's what more people would have tried something like it.
[00:19:36.040 --> 00:19:37.560]   You ask it a question.
[00:19:37.560 --> 00:19:40.040]   It'll start generating tokens very fast,
[00:19:40.040 --> 00:19:41.440]   and those tokens will look like
[00:19:41.440 --> 00:19:44.120]   a very human legible answer.
[00:19:44.120 --> 00:19:46.840]   It'll be some sort of markdown list.
[00:19:46.840 --> 00:19:49.520]   It might have formatting to help you draw
[00:19:49.520 --> 00:19:52.280]   to the core details in the answer,
[00:19:52.280 --> 00:19:55.280]   and it'll generate tens to hundreds of tokens.
[00:19:55.280 --> 00:19:58.640]   A token is normally a word for common words
[00:19:58.640 --> 00:20:02.200]   or a sub-word part in a longer word.
[00:20:02.200 --> 00:20:05.480]   And it'll look like a very high-quality
[00:20:05.480 --> 00:20:06.840]   Reddit or Stack Overflow answer.
[00:20:06.840 --> 00:20:10.120]   These models are really getting good at doing these
[00:20:10.120 --> 00:20:12.200]   across a wide variety of domains.
[00:20:12.200 --> 00:20:15.120]   Even things that, if you're an expert,
[00:20:15.120 --> 00:20:17.120]   things that are close to the fringe of knowledge,
[00:20:17.120 --> 00:20:19.600]   they will still be fairly good at.
[00:20:19.600 --> 00:20:22.520]   Cutting-edge AI topics that I do research on,
[00:20:22.520 --> 00:20:26.360]   these models are capable for study aid,
[00:20:26.360 --> 00:20:28.360]   and they're regularly updated.
[00:20:28.360 --> 00:20:31.480]   Where this changes is with the DeepSeq v1,
[00:20:31.480 --> 00:20:33.040]   what is called these reasoning models,
[00:20:33.040 --> 00:20:36.560]   is when you see tokens coming from these models to start,
[00:20:36.560 --> 00:20:40.600]   it will be a large chain-of-thought process.
[00:20:40.600 --> 00:20:42.480]   We'll get back to chain-of-thought in a second,
[00:20:42.480 --> 00:20:44.800]   which looks like a lot of tokens
[00:20:44.800 --> 00:20:46.880]   where the model is explaining the problem.
[00:20:46.880 --> 00:20:48.440]   The model will often break down the problem,
[00:20:48.440 --> 00:20:50.520]   and be like, "Okay, they asked me for this.
[00:20:50.520 --> 00:20:51.960]   Let's break down the problem.
[00:20:51.960 --> 00:20:53.560]   I'm going to need to do this."
[00:20:53.560 --> 00:20:55.720]   And you'll see all of this generating from the model.
[00:20:55.720 --> 00:20:57.960]   It'll come very fast in most user experiences.
[00:20:57.960 --> 00:21:00.640]   These APIs are very fast, so you'll see a lot of tokens.
[00:21:00.640 --> 00:21:02.240]   A lot of words show up really fast.
[00:21:02.240 --> 00:21:03.840]   It'll keep flowing on the screen,
[00:21:03.840 --> 00:21:05.840]   and this is all the reasoning process.
[00:21:05.840 --> 00:21:09.080]   And then eventually the model will change its tone in R1,
[00:21:09.080 --> 00:21:10.160]   and it'll write the answer,
[00:21:10.160 --> 00:21:13.400]   where it summarizes its reasoning process
[00:21:13.400 --> 00:21:16.480]   and writes a similar answer to the first types of model.
[00:21:16.480 --> 00:21:18.040]   But in DeepSeek's case,
[00:21:18.040 --> 00:21:20.400]   which is part of why this was so popular,
[00:21:20.400 --> 00:21:22.800]   even outside the AI community,
[00:21:22.800 --> 00:21:24.840]   is that you can see how the language model
[00:21:24.840 --> 00:21:26.880]   is breaking down problems.
[00:21:26.880 --> 00:21:29.480]   And then you get this answer on a technical side.
[00:21:29.480 --> 00:21:31.560]   They train the model to do this specifically,
[00:21:31.560 --> 00:21:33.440]   where they have a section, which is reasoning,
[00:21:33.440 --> 00:21:34.920]   and then it generates a special token,
[00:21:34.920 --> 00:21:36.960]   which is probably hidden from the user most of the time,
[00:21:36.960 --> 00:21:38.800]   which says, "Okay, I'm starting the answer."
[00:21:38.800 --> 00:21:40.040]   So the model is trained
[00:21:40.040 --> 00:21:42.840]   to do this two-stage process on its own.
[00:21:42.840 --> 00:21:45.120]   If you use a similar model in, say, OpenAI,
[00:21:45.120 --> 00:21:49.960]   OpenAI's user interface is trying to summarize this process
[00:21:49.960 --> 00:21:53.280]   for you nicely by kind of showing the sections
[00:21:53.280 --> 00:21:54.720]   that the model is doing,
[00:21:54.720 --> 00:21:55.960]   and it'll kind of click through.
[00:21:55.960 --> 00:21:57.960]   It'll say, "Breaking down the problem.
[00:21:57.960 --> 00:22:00.120]   "Making X calculation.
[00:22:00.120 --> 00:22:01.160]   "Cleaning the result."
[00:22:01.160 --> 00:22:03.800]   And then the answer will come for something like OpenAI.
[00:22:03.800 --> 00:22:05.480]   - Maybe it's useful here to go through, like,
[00:22:05.480 --> 00:22:09.080]   an example of a DeepSeek R1 reasoning.
[00:22:09.080 --> 00:22:12.080]   - Yeah, so if you're looking at the screen here,
[00:22:12.080 --> 00:22:13.200]   what you'll see is a screenshot
[00:22:13.200 --> 00:22:15.480]   of the DeepSeek chat app,
[00:22:15.480 --> 00:22:19.560]   and at the top is thought for 157 seconds
[00:22:19.560 --> 00:22:20.560]   with the dropdown arrow.
[00:22:20.560 --> 00:22:23.040]   Underneath that, if we were in an app that we were running,
[00:22:23.040 --> 00:22:24.800]   the dropdown arrow would have the reasoning.
[00:22:24.800 --> 00:22:27.720]   - So in this case, the specific question,
[00:22:27.720 --> 00:22:31.440]   which, you know, I'm philosophically/pothead-inclined,
[00:22:31.440 --> 00:22:34.360]   so this is asking DeepSeek R1
[00:22:34.360 --> 00:22:39.600]   for one truly novel insight about humans,
[00:22:39.600 --> 00:22:41.400]   and it reveals the reasoning,
[00:22:41.400 --> 00:22:44.680]   and basically the truly novel aspect
[00:22:44.680 --> 00:22:46.600]   is what's pushing the reasoning
[00:22:46.600 --> 00:22:47.960]   to constantly sort of demodel,
[00:22:47.960 --> 00:22:49.600]   asking itself, "Is this truly novel?"
[00:22:49.600 --> 00:22:52.600]   So it's actually challenging itself to be more novel,
[00:22:52.600 --> 00:22:53.920]   more counterintuitive,
[00:22:53.920 --> 00:22:57.440]   less cringe, I suppose.
[00:22:57.440 --> 00:22:59.120]   So some of the reasoning says,
[00:22:59.120 --> 00:23:01.320]   "This is just snapshots.
[00:23:01.320 --> 00:23:03.960]   "Alternatively, humans have a unique meta-emotion
[00:23:03.960 --> 00:23:06.040]   "where they feel emotions about their own emotions,
[00:23:06.040 --> 00:23:08.120]   "e.g. feeling guilty about being angry.
[00:23:08.120 --> 00:23:09.480]   "This recursive emotional layering
[00:23:09.480 --> 00:23:11.160]   "creates complex motivational drives
[00:23:11.160 --> 00:23:13.240]   "that don't exist in other animals.
[00:23:13.240 --> 00:23:15.560]   "The insight is that human emotions are nested."
[00:23:15.560 --> 00:23:19.560]   So it's reasoning through how humans feel emotions,
[00:23:19.560 --> 00:23:21.160]   it's reasoning about meta-emotions.
[00:23:21.160 --> 00:23:22.840]   - Gonna have pages and pages of this.
[00:23:22.840 --> 00:23:24.400]   It's almost too much to actually read,
[00:23:24.400 --> 00:23:26.440]   but it's nice to skim as it's coming.
[00:23:26.440 --> 00:23:29.640]   - It's a James Joyce-like stream of consciousness,
[00:23:29.640 --> 00:23:32.480]   and then it goes, "Wait, the user wants something
[00:23:32.480 --> 00:23:34.200]   "that's not seen anywhere else.
[00:23:34.200 --> 00:23:36.160]   "Let me dig deeper."
[00:23:36.160 --> 00:23:38.040]   And, "Consider the human ability
[00:23:38.040 --> 00:23:40.120]   "to hold contradictory beliefs simultaneously.
[00:23:40.120 --> 00:23:41.720]   "Cognitive dissonance is known,
[00:23:41.720 --> 00:23:45.600]   "but perhaps the function is to allow flexible adaptation."
[00:23:45.600 --> 00:23:47.280]   So on and so forth.
[00:23:47.280 --> 00:23:50.360]   I mean, that really captures the public imagination
[00:23:50.360 --> 00:23:53.040]   that, "Holy shit, this isn't..."
[00:23:53.040 --> 00:23:59.720]   I mean, intelligence/almost like an inkling of sentience,
[00:23:59.720 --> 00:24:02.200]   because you're thinking through,
[00:24:02.200 --> 00:24:04.480]   you're self-reflecting, you're deliberating.
[00:24:04.480 --> 00:24:07.880]   And the final result of that, after 157 seconds,
[00:24:07.880 --> 00:24:12.480]   is, "Humans instinctively convert selfish desires
[00:24:12.480 --> 00:24:13.680]   "into cooperative systems
[00:24:13.680 --> 00:24:16.080]   "by collectively pretending abstract rules,
[00:24:16.080 --> 00:24:18.520]   "money, laws, rights, are real.
[00:24:18.520 --> 00:24:21.840]   "These shared hallucinations act as, quote, 'games,'
[00:24:21.840 --> 00:24:23.760]   "where competition is secretly redirected
[00:24:23.760 --> 00:24:25.160]   "to benefit the group,
[00:24:25.160 --> 00:24:27.920]   "turning conflict into society's fuel."
[00:24:27.920 --> 00:24:29.880]   Pretty profound.
[00:24:29.880 --> 00:24:31.800]   I mean, you know.
[00:24:31.800 --> 00:24:33.680]   - This is a potential digression,
[00:24:33.680 --> 00:24:36.040]   but a lot of people have found
[00:24:36.040 --> 00:24:38.680]   that these reasoning models can sometimes produce
[00:24:38.680 --> 00:24:40.840]   much more eloquent text.
[00:24:40.840 --> 00:24:43.160]   That is a, at least, interesting example,
[00:24:43.160 --> 00:24:45.000]   I think, depending on how open-minded you are,
[00:24:45.000 --> 00:24:46.760]   you find language models interesting or not,
[00:24:46.760 --> 00:24:48.240]   and there's a spectrum there.
[00:24:48.240 --> 00:24:49.600]   - Well, I mean, it's some of the...
[00:24:49.600 --> 00:24:51.440]   We'll talk about different benchmarks and so on,
[00:24:51.440 --> 00:24:53.360]   but some is just a vibe.
[00:24:53.360 --> 00:24:57.920]   Like that, in itself, is a, let's say, quote, "fire tweet."
[00:24:57.920 --> 00:24:58.760]   - Yeah.
[00:24:58.760 --> 00:25:01.520]   - If I'm trying to produce something
[00:25:01.520 --> 00:25:04.080]   where people are like, "Oh shit, okay."
[00:25:04.080 --> 00:25:04.920]   So that's a chain of thought
[00:25:04.920 --> 00:25:06.480]   we'll probably return to more.
[00:25:06.480 --> 00:25:11.920]   How were they able to achieve such low cost
[00:25:11.920 --> 00:25:13.360]   on the training and the inference?
[00:25:13.360 --> 00:25:15.440]   Maybe you could talk the training first.
[00:25:15.440 --> 00:25:17.640]   - Yeah, so there's two main techniques
[00:25:17.640 --> 00:25:19.360]   that they implemented
[00:25:19.360 --> 00:25:23.040]   that are probably the majority of their efficiency,
[00:25:23.040 --> 00:25:25.120]   and then there's a lot of implementation details
[00:25:25.120 --> 00:25:27.640]   that maybe we'll gloss over or get into later
[00:25:27.640 --> 00:25:28.680]   that sort of contribute to it.
[00:25:28.680 --> 00:25:30.920]   But those two main things are,
[00:25:30.920 --> 00:25:34.360]   one is they went to a mixture of experts model,
[00:25:34.360 --> 00:25:35.720]   which we'll define in a second.
[00:25:35.720 --> 00:25:37.520]   And then the other thing is that they invented
[00:25:37.520 --> 00:25:40.520]   this new technique called MLA, latent attention.
[00:25:40.520 --> 00:25:42.040]   Both of these are big deals.
[00:25:42.040 --> 00:25:43.400]   Mixture of experts is something
[00:25:43.400 --> 00:25:46.080]   that's been in the literature for a handful of years.
[00:25:46.080 --> 00:25:48.520]   And OpenAI with GPT-4 was the first one
[00:25:48.520 --> 00:25:51.920]   to productize a mixture of experts model.
[00:25:51.920 --> 00:25:53.320]   And what this means is,
[00:25:53.320 --> 00:25:55.960]   when you look at the common models around
[00:25:55.960 --> 00:25:58.240]   that most people have been able to interact with
[00:25:58.240 --> 00:25:59.560]   that are open, right?
[00:25:59.560 --> 00:26:00.840]   Think Lama.
[00:26:00.840 --> 00:26:02.680]   Lama is a dense model,
[00:26:02.680 --> 00:26:06.760]   i.e. every single parameter or neuron is activated
[00:26:06.760 --> 00:26:08.120]   as you're going through the model
[00:26:08.120 --> 00:26:10.800]   for every single token you generate, right?
[00:26:10.800 --> 00:26:12.920]   Now, with a mixture of experts model,
[00:26:12.920 --> 00:26:13.800]   you don't do that, right?
[00:26:13.800 --> 00:26:15.680]   How does the human actually work, right?
[00:26:15.680 --> 00:26:18.040]   It's like, oh, well, my visual cortex is active
[00:26:18.040 --> 00:26:20.080]   when I'm thinking about, you know, vision tasks
[00:26:20.080 --> 00:26:21.440]   and like, you know, other things, right?
[00:26:21.440 --> 00:26:24.000]   My amygdala is when I'm scared, right?
[00:26:24.000 --> 00:26:25.480]   These different aspects of your brain
[00:26:25.480 --> 00:26:26.920]   are focused on different things.
[00:26:26.920 --> 00:26:28.680]   A mixture of experts model attempts
[00:26:28.680 --> 00:26:30.360]   to approximate this to some extent.
[00:26:30.360 --> 00:26:32.640]   It's nowhere close to what a brain architecture is,
[00:26:32.640 --> 00:26:35.600]   but different portions of the model activate, right?
[00:26:35.600 --> 00:26:38.720]   You'll have a set number of experts in the model
[00:26:38.720 --> 00:26:40.840]   and a set number that are activated each time.
[00:26:40.840 --> 00:26:43.080]   And this dramatically reduces both your training
[00:26:43.080 --> 00:26:44.040]   and inference costs.
[00:26:44.040 --> 00:26:46.240]   Because now you're, you know,
[00:26:46.240 --> 00:26:48.080]   if you think about the parameter count
[00:26:48.080 --> 00:26:50.000]   as the sort of total embedding space
[00:26:50.000 --> 00:26:51.040]   for all of this knowledge
[00:26:51.040 --> 00:26:53.320]   that you're compressing down during training,
[00:26:53.320 --> 00:26:56.360]   when you're embedding this data in,
[00:26:56.360 --> 00:26:59.120]   instead of having to activate every single parameter
[00:26:59.120 --> 00:27:01.320]   every single time you're training or running inference,
[00:27:01.320 --> 00:27:03.360]   now you can just activate a subset
[00:27:03.360 --> 00:27:06.160]   and the model will learn which expert to route to
[00:27:06.160 --> 00:27:07.480]   for different tasks.
[00:27:07.480 --> 00:27:09.880]   And so this is a humongous innovation
[00:27:09.880 --> 00:27:12.160]   in terms of, hey, I can continue to grow
[00:27:12.160 --> 00:27:14.160]   the total embedding space of parameters.
[00:27:14.160 --> 00:27:15.720]   And so DeepSeek's model is, you know,
[00:27:15.720 --> 00:27:18.200]   600 something billion parameters, right?
[00:27:18.200 --> 00:27:21.960]   Relative to LLAMA405B, it's 405 billion parameters, right?
[00:27:21.960 --> 00:27:24.360]   Relative to LLAMA70B, it's 70 billion parameters, right?
[00:27:24.360 --> 00:27:26.760]   So this model technically has more embedding space
[00:27:26.760 --> 00:27:28.200]   for information, right?
[00:27:28.200 --> 00:27:29.840]   To compress all of the world's knowledge
[00:27:29.840 --> 00:27:31.240]   that's on the internet down,
[00:27:31.240 --> 00:27:32.640]   but at the same time,
[00:27:32.640 --> 00:27:36.680]   it is only activating around 37 billion of the parameters.
[00:27:36.680 --> 00:27:38.640]   So only 37 billion of these parameters
[00:27:38.640 --> 00:27:40.080]   actually need to be computed
[00:27:40.080 --> 00:27:42.160]   every single time you're training data
[00:27:42.160 --> 00:27:43.960]   or inferencing data out of it.
[00:27:43.960 --> 00:27:46.440]   And so versus, again, the LLAMA model,
[00:27:46.440 --> 00:27:48.240]   70 billion parameters must be activated
[00:27:48.240 --> 00:27:50.440]   or 405 billion parameters must be activated.
[00:27:50.440 --> 00:27:52.680]   So you've dramatically reduced your compute cost
[00:27:52.680 --> 00:27:54.760]   when you're doing training and inference
[00:27:54.760 --> 00:27:57.320]   with this mixture of experts architecture.
[00:27:57.320 --> 00:27:59.480]   - Should we break down where it actually applies
[00:27:59.480 --> 00:28:01.120]   and go into the transformer?
[00:28:01.120 --> 00:28:01.960]   Is that useful?
[00:28:01.960 --> 00:28:03.600]   - Let's go, let's go into the transformer.
[00:28:03.600 --> 00:28:05.120]   - Okay, so the transformer is a thing
[00:28:05.120 --> 00:28:06.440]   that is talked about a lot
[00:28:06.440 --> 00:28:08.280]   and we will not cover every detail.
[00:28:08.280 --> 00:28:12.880]   Essentially, the transformer is built on repeated blocks
[00:28:12.880 --> 00:28:14.560]   of this attention mechanism
[00:28:14.560 --> 00:28:17.880]   and then a traditional dense, fully connected,
[00:28:17.880 --> 00:28:19.200]   multi-layer perception,
[00:28:19.200 --> 00:28:20.400]   whatever word you want to use
[00:28:20.400 --> 00:28:21.760]   for your normal neural network.
[00:28:21.760 --> 00:28:23.560]   And you alternate these blocks.
[00:28:23.560 --> 00:28:24.880]   There's other details.
[00:28:24.880 --> 00:28:26.440]   And where mixture of experts is applied
[00:28:26.440 --> 00:28:28.240]   is at this dense model.
[00:28:28.240 --> 00:28:31.160]   The dense model holds most of the weights
[00:28:31.160 --> 00:28:33.840]   if you count them in a transformer model.
[00:28:33.840 --> 00:28:35.480]   So you can get really big gains
[00:28:35.480 --> 00:28:37.160]   from those mixture of experts
[00:28:37.160 --> 00:28:39.200]   on parameter efficiency at training and inference
[00:28:39.200 --> 00:28:41.960]   because you get this efficiency
[00:28:41.960 --> 00:28:44.120]   by not activating all of these parameters.
[00:28:44.120 --> 00:28:46.720]   - We should also say that a transformer
[00:28:46.720 --> 00:28:48.680]   is a giant neural network.
[00:28:48.680 --> 00:28:49.520]   - Yeah.
[00:28:49.520 --> 00:28:51.960]   - And then there's for 15 years now,
[00:28:51.960 --> 00:28:54.440]   there's what's called the deep learning revolution.
[00:28:54.440 --> 00:28:56.280]   Network's gotten larger and larger.
[00:28:56.280 --> 00:28:58.720]   And at a certain point, the scaling laws appeared
[00:28:58.720 --> 00:29:00.560]   where people realized--
[00:29:00.560 --> 00:29:02.840]   - This is a scaling law shirt, by the way.
[00:29:02.840 --> 00:29:04.960]   - Representing scaling laws
[00:29:04.960 --> 00:29:07.520]   where it became more and more formalized
[00:29:07.520 --> 00:29:11.000]   that bigger is better across multiple dimensions
[00:29:11.000 --> 00:29:12.160]   of what bigger means.
[00:29:12.160 --> 00:29:15.160]   But these are all sort of neural networks
[00:29:15.160 --> 00:29:16.480]   we're talking about.
[00:29:16.480 --> 00:29:18.080]   And we're talking about different architectures
[00:29:18.080 --> 00:29:21.040]   of how to construct these neural networks
[00:29:21.040 --> 00:29:23.520]   such that the training and the inference on them
[00:29:23.520 --> 00:29:24.360]   is super efficient.
[00:29:24.360 --> 00:29:25.720]   - Yeah, every different type of model
[00:29:25.720 --> 00:29:27.960]   has a different scaling law for it,
[00:29:27.960 --> 00:29:30.840]   which is effectively for how much compute you put in,
[00:29:30.840 --> 00:29:34.440]   the architecture will get to different levels
[00:29:34.440 --> 00:29:36.360]   of performance at test tasks.
[00:29:36.360 --> 00:29:39.240]   And mixture of experts is one of the ones at training time,
[00:29:39.240 --> 00:29:41.280]   even if you don't consider the inference benefits,
[00:29:41.280 --> 00:29:42.480]   which are also big.
[00:29:42.480 --> 00:29:44.920]   At training time, your efficiency with your GPUs
[00:29:44.920 --> 00:29:47.360]   is dramatically improved by using this architecture
[00:29:47.360 --> 00:29:48.880]   if it is well implemented.
[00:29:48.880 --> 00:29:53.120]   So you can get effectively the same performance model
[00:29:53.120 --> 00:29:57.440]   and evaluation scores with numbers like 30% less compute.
[00:29:57.440 --> 00:29:59.080]   I think there's gonna be a wide variation
[00:29:59.080 --> 00:30:01.480]   depending on your implementation details and stuff.
[00:30:01.480 --> 00:30:03.840]   But it is just important to realize
[00:30:03.840 --> 00:30:05.320]   that this type of technical innovation
[00:30:05.320 --> 00:30:07.960]   is something that gives huge gains.
[00:30:07.960 --> 00:30:11.280]   And I expect most companies that are serving their models
[00:30:11.280 --> 00:30:14.280]   to move to this mixture of experts implementation.
[00:30:14.280 --> 00:30:16.760]   Historically, the reason why not everyone might do it
[00:30:16.760 --> 00:30:19.000]   is because it's an implementation complexity,
[00:30:19.000 --> 00:30:21.080]   especially when doing these big models.
[00:30:21.080 --> 00:30:24.080]   So this is one of the things that DeepSeq gets credit for
[00:30:24.080 --> 00:30:26.040]   is they do this extremely well.
[00:30:26.040 --> 00:30:27.720]   They do a mixture of experts extremely well.
[00:30:27.720 --> 00:30:31.360]   This architecture for what is called DeepSeq MOE,
[00:30:31.360 --> 00:30:34.320]   MOE is the shortened version of mixture of experts,
[00:30:34.320 --> 00:30:35.760]   is multiple papers old.
[00:30:35.760 --> 00:30:37.960]   This part of their training infrastructure
[00:30:37.960 --> 00:30:40.840]   is not new to these models alone.
[00:30:40.840 --> 00:30:42.960]   And same goes for what Dylan mentioned
[00:30:42.960 --> 00:30:45.280]   with multi-head latent attention.
[00:30:45.280 --> 00:30:48.200]   This is all about reducing memory usage during inference
[00:30:48.200 --> 00:30:50.120]   and same things during training
[00:30:50.120 --> 00:30:53.600]   by using some fancy low rank approximation math.
[00:30:53.600 --> 00:30:56.680]   If you get into the details with this latent attention,
[00:30:56.680 --> 00:30:58.320]   it's one of those things I look at and say,
[00:30:58.320 --> 00:31:01.520]   okay, they're doing really complex implementations
[00:31:01.520 --> 00:31:03.040]   'cause there's other parts of language models
[00:31:03.040 --> 00:31:05.800]   such as embeddings that are used
[00:31:05.800 --> 00:31:07.200]   to extend the context length.
[00:31:07.200 --> 00:31:09.320]   The common one that DeepSeq uses
[00:31:09.320 --> 00:31:12.680]   rotary positional embeddings, which is called rope.
[00:31:12.680 --> 00:31:15.280]   And if you wanna use rope with a normal MOE,
[00:31:15.280 --> 00:31:16.440]   it's kind of a sequential thing.
[00:31:16.440 --> 00:31:19.320]   You take two of the attention matrices
[00:31:19.320 --> 00:31:22.520]   and you rotate them by a complex value rotation,
[00:31:22.520 --> 00:31:24.360]   which is a matrix multiplication.
[00:31:24.360 --> 00:31:27.760]   With DeepSeq's MLA, with this new attention architecture,
[00:31:27.760 --> 00:31:29.760]   they need to do some clever things
[00:31:29.760 --> 00:31:31.120]   because they're not set up the same
[00:31:31.120 --> 00:31:34.240]   and it just makes the implementation complexity much higher.
[00:31:34.240 --> 00:31:35.760]   So they're managing all of these things.
[00:31:35.760 --> 00:31:37.640]   And these are probably the sort of things
[00:31:37.640 --> 00:31:40.320]   that OpenAI, these closed labs are doing.
[00:31:40.320 --> 00:31:42.200]   We don't know if they're doing the exact same techniques,
[00:31:42.200 --> 00:31:44.000]   but they actually shared them with the world,
[00:31:44.000 --> 00:31:45.240]   which is really nice to feel like
[00:31:45.240 --> 00:31:46.520]   this is the cutting edge
[00:31:46.520 --> 00:31:49.160]   of efficient language model training.
[00:31:49.160 --> 00:31:52.280]   - And some of this requires low level engineering.
[00:31:52.280 --> 00:31:55.320]   Just it is a giant mess and trickery.
[00:31:55.320 --> 00:31:57.800]   So as I understand that one below CUDA,
[00:31:57.800 --> 00:32:00.880]   so they go super low programming of GPUs.
[00:32:00.880 --> 00:32:03.960]   - Effectively, NVIDIA builds this library called Nickel,
[00:32:03.960 --> 00:32:05.000]   right?
[00:32:05.000 --> 00:32:07.200]   In which, you know, when you're training a model,
[00:32:07.200 --> 00:32:08.760]   you have all these communications
[00:32:08.760 --> 00:32:10.880]   between every single layer of the model
[00:32:10.880 --> 00:32:12.440]   and you may have over a hundred layers.
[00:32:12.440 --> 00:32:13.480]   - What does Nickel stand for?
[00:32:13.480 --> 00:32:14.600]   It's N-C-C-L?
[00:32:14.600 --> 00:32:16.720]   - NVIDIA Communications Collectives Library.
[00:32:16.720 --> 00:32:17.560]   - Nice.
[00:32:17.560 --> 00:32:18.400]   (laughs)
[00:32:18.400 --> 00:32:19.240]   - And so,
[00:32:19.240 --> 00:32:20.080]   - Damn.
[00:32:20.080 --> 00:32:20.920]   (laughs)
[00:32:20.920 --> 00:32:22.960]   - When you're training a model, right?
[00:32:22.960 --> 00:32:24.160]   You're going to have all these,
[00:32:24.160 --> 00:32:26.080]   all reduces and all gathers, right?
[00:32:26.080 --> 00:32:27.000]   Between each layer,
[00:32:27.000 --> 00:32:30.520]   between the multi-layer perceptron or feed forward network
[00:32:30.520 --> 00:32:31.720]   and the attention mechanism,
[00:32:31.720 --> 00:32:35.040]   you'll have basically the model synchronized, right?
[00:32:35.040 --> 00:32:38.400]   Or you'll have all reduce or an all gather.
[00:32:38.400 --> 00:32:39.880]   And this is a communication
[00:32:39.880 --> 00:32:41.240]   between all the GPUs in the network,
[00:32:41.240 --> 00:32:43.080]   whether it's in training or inference.
[00:32:43.080 --> 00:32:44.680]   So NVIDIA has a standard library.
[00:32:44.680 --> 00:32:47.240]   This is one of the reasons why it's really difficult
[00:32:47.240 --> 00:32:49.600]   to use anyone else's hardware for training
[00:32:49.600 --> 00:32:51.400]   is because no one's really built
[00:32:51.400 --> 00:32:53.520]   a standard communications library.
[00:32:53.520 --> 00:32:56.240]   And NVIDIA has done this at a sort of a higher level, right?
[00:32:56.240 --> 00:32:57.080]   A DeepSeek,
[00:32:57.080 --> 00:32:58.720]   because they have certain limitations
[00:32:58.720 --> 00:33:00.680]   around the GPUs that they have access to,
[00:33:00.680 --> 00:33:03.880]   the interconnects are limited to some extent
[00:33:03.880 --> 00:33:05.800]   by the restrictions of the GPUs
[00:33:05.800 --> 00:33:07.080]   that were shipped into China legally,
[00:33:07.080 --> 00:33:08.040]   not the ones that are smuggled,
[00:33:08.040 --> 00:33:09.680]   but legally shipped in,
[00:33:09.680 --> 00:33:11.360]   that they use to train this model.
[00:33:11.360 --> 00:33:15.200]   They had to figure out how to get efficiencies, right?
[00:33:15.200 --> 00:33:16.720]   And one of those things is that
[00:33:16.720 --> 00:33:20.520]   instead of just calling the NVIDIA library, Nickel, right?
[00:33:20.520 --> 00:33:22.120]   They instead created their,
[00:33:22.120 --> 00:33:24.520]   they scheduled their own communications,
[00:33:24.520 --> 00:33:26.960]   which some of the labs do, right?
[00:33:26.960 --> 00:33:29.080]   Emeta talked about in Llama 3,
[00:33:29.080 --> 00:33:31.120]   how they made their own custom version of Nickel.
[00:33:31.120 --> 00:33:33.720]   This is, they didn't talk about the implementation details.
[00:33:33.720 --> 00:33:34.960]   This is some of what they did,
[00:33:34.960 --> 00:33:36.360]   probably not as well as,
[00:33:36.360 --> 00:33:37.720]   maybe not as well as DeepSeek
[00:33:37.720 --> 00:33:38.840]   because DeepSeek, you know,
[00:33:38.840 --> 00:33:40.680]   necessity is the mother of innovation.
[00:33:40.680 --> 00:33:42.520]   And they had to do this,
[00:33:42.520 --> 00:33:44.480]   whereas in the case, you know,
[00:33:44.480 --> 00:33:46.600]   OpenAI has people that do this sort of stuff,
[00:33:46.600 --> 00:33:48.040]   Anthropic, et cetera.
[00:33:48.040 --> 00:33:50.400]   But, you know, DeepSeek certainly did it publicly
[00:33:50.400 --> 00:33:51.560]   and they may have done it even better
[00:33:51.560 --> 00:33:54.000]   because they were gimped on a certain aspect
[00:33:54.000 --> 00:33:56.040]   of the chips that they have access to.
[00:33:56.040 --> 00:34:00.600]   And so they scheduled communications on,
[00:34:00.600 --> 00:34:02.520]   you know, by scheduling specific SMs.
[00:34:02.520 --> 00:34:06.200]   SMs you could think of as like the core on a GPU, right?
[00:34:06.200 --> 00:34:08.080]   So there's hundreds of cores,
[00:34:08.080 --> 00:34:10.760]   or there's, you know, a bit over a hundred cores, SMs,
[00:34:10.760 --> 00:34:13.120]   on a GPU and they were specifically scheduling,
[00:34:13.120 --> 00:34:14.480]   hey, which ones are running the model?
[00:34:14.480 --> 00:34:15.800]   Which ones are doing all reduce?
[00:34:15.800 --> 00:34:17.360]   Which one are doing all gather, right?
[00:34:17.360 --> 00:34:19.040]   And they would flip back and forth between them.
[00:34:19.040 --> 00:34:22.080]   And this requires extremely low level programming.
[00:34:22.080 --> 00:34:23.600]   - This is what Nickel does automatically
[00:34:23.600 --> 00:34:26.480]   or other NVIDIA libraries handle this automatically usually.
[00:34:26.480 --> 00:34:27.320]   - Yeah, exactly.
[00:34:27.320 --> 00:34:29.960]   And so technically they're using, you know, PTX,
[00:34:29.960 --> 00:34:31.360]   which is like sort of like,
[00:34:31.360 --> 00:34:33.840]   you could think of it as like an assembly type language.
[00:34:33.840 --> 00:34:35.800]   It's not exactly that or instruction set, right?
[00:34:35.800 --> 00:34:38.320]   Like coding directly to assembly or instruction set.
[00:34:38.320 --> 00:34:39.320]   It's not exactly that,
[00:34:39.320 --> 00:34:42.000]   but that's still part of technically CUDA,
[00:34:42.000 --> 00:34:44.120]   but it's like, do I wanna write in Python,
[00:34:44.120 --> 00:34:47.000]   you know, PyTorch equivalent and call NVIDIA libraries?
[00:34:47.000 --> 00:34:49.000]   Do I wanna go down to the C level, right?
[00:34:49.000 --> 00:34:50.640]   Or, you know, encode even lower level,
[00:34:50.640 --> 00:34:51.560]   or do I wanna go all the way down
[00:34:51.560 --> 00:34:53.120]   to the assembly or ISO level?
[00:34:53.120 --> 00:34:55.880]   And there are cases where you go all the way down there
[00:34:55.880 --> 00:34:57.080]   at the very big labs,
[00:34:57.080 --> 00:34:59.600]   but most companies just do not do that, right?
[00:34:59.600 --> 00:35:01.000]   Because it's a waste of time
[00:35:01.000 --> 00:35:03.640]   and the efficiency gains you get are not worth it.
[00:35:03.640 --> 00:35:06.840]   But DeepSeek's implementation is so complex, right?
[00:35:06.840 --> 00:35:09.360]   Especially with their mixture of experts, right?
[00:35:09.360 --> 00:35:10.680]   People have done mixture of experts,
[00:35:10.680 --> 00:35:13.360]   but they're generally eight, 16 experts, right?
[00:35:13.360 --> 00:35:14.240]   And they activate too.
[00:35:14.240 --> 00:35:17.000]   So, you know, one of the words that we like to use
[00:35:17.000 --> 00:35:18.640]   is like sparsity factor, right?
[00:35:18.640 --> 00:35:19.480]   Or usage, right?
[00:35:19.480 --> 00:35:21.440]   So you might have four, you know,
[00:35:21.440 --> 00:35:23.760]   one fourth of your model activate, right?
[00:35:23.760 --> 00:35:27.560]   And that's what Mistral's Mistral model, right?
[00:35:27.560 --> 00:35:29.840]   Their model that really catapulted them to like,
[00:35:29.840 --> 00:35:32.160]   oh my God, they're really, really good.
[00:35:32.160 --> 00:35:34.320]   OpenAI has also had models that are MOE
[00:35:34.320 --> 00:35:37.760]   and so have all the other labs that are major closed.
[00:35:37.760 --> 00:35:40.720]   But what DeepSeek did that maybe only the leading labs
[00:35:40.720 --> 00:35:42.480]   have only just started recently doing
[00:35:42.480 --> 00:35:44.200]   is have such a high sparsity factor, right?
[00:35:44.200 --> 00:35:46.000]   It's not one fourth of the model, right?
[00:35:46.000 --> 00:35:47.920]   Two out of eight experts activating
[00:35:47.920 --> 00:35:51.400]   every time you go through the model, it's eight out of 256.
[00:35:51.400 --> 00:35:52.960]   - And there's different implementations
[00:35:52.960 --> 00:35:55.560]   for mixture of experts where you can have
[00:35:55.560 --> 00:35:58.680]   some of these experts that are always activated,
[00:35:58.680 --> 00:36:02.080]   which this just looks like a small neural network
[00:36:02.080 --> 00:36:03.720]   and then all the tokens go through that.
[00:36:03.720 --> 00:36:07.160]   And then they also go through some that are selected
[00:36:07.160 --> 00:36:08.720]   by this routing mechanism.
[00:36:08.720 --> 00:36:12.960]   And one of the innovations in DeepSeek's architecture
[00:36:12.960 --> 00:36:15.080]   is that they changed the routing mechanism
[00:36:15.080 --> 00:36:16.520]   in mixture of expert models.
[00:36:16.520 --> 00:36:19.160]   There's something called an auxiliary loss,
[00:36:19.160 --> 00:36:21.440]   which effectively means during training,
[00:36:21.440 --> 00:36:23.960]   you want to make sure that all of these experts are used
[00:36:23.960 --> 00:36:26.600]   across the tasks that the model sees.
[00:36:26.600 --> 00:36:28.880]   Why there can be failures in mixture of experts
[00:36:28.880 --> 00:36:32.240]   is that when you're doing this training,
[00:36:32.240 --> 00:36:35.520]   the one objective is token prediction accuracy.
[00:36:35.520 --> 00:36:37.240]   And if you just let training go
[00:36:37.240 --> 00:36:39.360]   with a mixture of expert model on your own,
[00:36:39.360 --> 00:36:42.000]   it can be that the model learns
[00:36:42.000 --> 00:36:44.720]   to only use a subset of the experts.
[00:36:44.720 --> 00:36:46.640]   And in the MOE literature,
[00:36:46.640 --> 00:36:48.560]   there's something called the auxiliary loss,
[00:36:48.560 --> 00:36:50.280]   which helps balance them.
[00:36:50.280 --> 00:36:54.240]   But if you think about the loss functions of deep learning,
[00:36:54.240 --> 00:36:55.800]   this even connects to the bitter lesson
[00:36:55.800 --> 00:36:59.480]   is that you want to have the minimum inductive bias
[00:36:59.480 --> 00:37:01.960]   in your model to let the model learn maximally.
[00:37:01.960 --> 00:37:05.040]   And this auxiliary loss, this balancing across experts
[00:37:05.040 --> 00:37:07.040]   could be seen as intention
[00:37:07.040 --> 00:37:09.600]   with the prediction accuracy of the tokens.
[00:37:09.600 --> 00:37:11.120]   So we don't know the exact extent
[00:37:11.120 --> 00:37:13.120]   that the DeepSeek MOE change,
[00:37:13.120 --> 00:37:15.440]   which is instead of doing an auxiliary loss,
[00:37:15.440 --> 00:37:17.760]   they have an extra parameter in their routing,
[00:37:17.760 --> 00:37:20.760]   which after the batches, they update this parameter
[00:37:20.760 --> 00:37:22.360]   to make sure that the next batches
[00:37:22.360 --> 00:37:24.440]   all have a similar use of experts.
[00:37:24.440 --> 00:37:26.960]   And this type of change can be big, it can be small,
[00:37:26.960 --> 00:37:28.720]   but they add up over time.
[00:37:28.720 --> 00:37:29.560]   And this is the sort of thing
[00:37:29.560 --> 00:37:31.160]   that just points to them innovating.
[00:37:31.160 --> 00:37:33.800]   And I'm sure all the labs that are training big MOEs
[00:37:33.800 --> 00:37:35.080]   are looking at this sort of things,
[00:37:35.080 --> 00:37:37.160]   which is getting away from the auxiliary loss.
[00:37:37.160 --> 00:37:38.680]   Some of them might already use it,
[00:37:38.680 --> 00:37:40.480]   but you keep accumulating gains.
[00:37:40.480 --> 00:37:43.160]   And we'll talk about the philosophy of training
[00:37:43.160 --> 00:37:45.680]   and how you organize these organizations.
[00:37:45.680 --> 00:37:48.280]   And a lot of it is just compounding small improvements
[00:37:48.280 --> 00:37:50.520]   over time in your data, in your architecture,
[00:37:50.520 --> 00:37:51.360]   in your post-training
[00:37:51.360 --> 00:37:53.960]   and how they integrate with each other.
[00:37:53.960 --> 00:37:55.160]   DeepSeq does the same thing.
[00:37:55.160 --> 00:37:57.440]   And some of them are shared a lot.
[00:37:57.440 --> 00:37:58.680]   We have to take them on face value
[00:37:58.680 --> 00:38:00.760]   that they share their most important details.
[00:38:00.760 --> 00:38:02.280]   I mean, the architecture and the weights are out there.
[00:38:02.280 --> 00:38:04.000]   So we're seeing what they're doing.
[00:38:04.000 --> 00:38:05.200]   And it adds up.
[00:38:05.200 --> 00:38:07.200]   - Going back to sort of the like efficiency
[00:38:07.200 --> 00:38:08.320]   and complexity point, right?
[00:38:08.320 --> 00:38:11.240]   It's 32 versus four, right?
[00:38:11.240 --> 00:38:12.920]   For like Mixedraw and other MOE models
[00:38:12.920 --> 00:38:14.400]   that have been publicly released.
[00:38:14.400 --> 00:38:16.040]   So this ratio is extremely high.
[00:38:16.040 --> 00:38:18.640]   And sort of what Nathan was getting at there was
[00:38:18.640 --> 00:38:21.920]   when you have such a different level of sparsity,
[00:38:21.920 --> 00:38:25.440]   you can't just have every GPU have the entire model, right?
[00:38:25.440 --> 00:38:26.320]   The model's too big.
[00:38:26.320 --> 00:38:27.720]   There's too much complexity there.
[00:38:27.720 --> 00:38:29.480]   So you have to split up the model
[00:38:29.480 --> 00:38:31.320]   with different types of parallelism, right?
[00:38:31.320 --> 00:38:32.880]   And so you might have different experts
[00:38:32.880 --> 00:38:34.480]   on different GPU nodes.
[00:38:34.480 --> 00:38:38.440]   But now what happens when this set of data that you get,
[00:38:38.440 --> 00:38:40.240]   hey, all of it looks like this one way
[00:38:40.240 --> 00:38:43.240]   and all of it should route to one part of my model, right?
[00:38:43.240 --> 00:38:47.320]   So when all of it routes to one part of the model,
[00:38:47.320 --> 00:38:51.120]   then you can have this overloading
[00:38:51.120 --> 00:38:53.600]   of a certain set of the GPU resources
[00:38:53.600 --> 00:38:55.160]   or a certain set of the GPUs.
[00:38:55.160 --> 00:38:58.760]   And then the rest of the training network sits idle
[00:38:58.760 --> 00:39:00.520]   because all of the tokens are just routing to that.
[00:39:00.520 --> 00:39:01.720]   So this is the biggest complexity.
[00:39:01.720 --> 00:39:02.600]   One of the big complexities
[00:39:02.600 --> 00:39:06.960]   with running a very sparse mixture of experts model,
[00:39:06.960 --> 00:39:10.840]   i.e. this 32 ratio versus this four ratio
[00:39:10.840 --> 00:39:13.120]   is that you end up with so many of the experts
[00:39:13.120 --> 00:39:14.200]   just sitting there idle.
[00:39:14.200 --> 00:39:16.240]   So how do I load balance between them?
[00:39:16.240 --> 00:39:18.280]   How do I schedule the communications between them?
[00:39:18.280 --> 00:39:21.760]   This is a lot of the extremely low-level detailed work
[00:39:21.760 --> 00:39:25.080]   that they figured out in the public first
[00:39:25.080 --> 00:39:27.880]   and potentially second or third in the world,
[00:39:27.880 --> 00:39:29.640]   and maybe even first in some cases.
[00:39:29.640 --> 00:39:32.480]   - What lesson do you,
[00:39:32.480 --> 00:39:33.960]   in the direction of the better lesson,
[00:39:33.960 --> 00:39:35.960]   do you take from all of this?
[00:39:35.960 --> 00:39:37.480]   Is this going to be the direction
[00:39:37.480 --> 00:39:39.120]   where a lot of the gain is going to be,
[00:39:39.120 --> 00:39:41.680]   which is this kind of low-level optimization?
[00:39:41.680 --> 00:39:44.520]   Or is this a short-term thing
[00:39:44.520 --> 00:39:46.520]   where the biggest gains will be more
[00:39:46.520 --> 00:39:50.440]   on the algorithmic high-level side of post-training?
[00:39:50.440 --> 00:39:52.560]   Is this a short-term leap
[00:39:52.560 --> 00:39:54.860]   because they've figured out a hack
[00:39:54.860 --> 00:39:58.940]   because constraints, necessities, the mother of invention?
[00:39:58.940 --> 00:40:01.000]   Or is there still a lot of gain?
[00:40:01.000 --> 00:40:02.000]   - I think we should summarize
[00:40:02.000 --> 00:40:04.000]   what the better lesson actually is about.
[00:40:04.000 --> 00:40:06.360]   Is that the better lesson, essentially,
[00:40:06.360 --> 00:40:07.600]   if you paraphrase it,
[00:40:07.600 --> 00:40:10.200]   is that the types of training
[00:40:10.200 --> 00:40:13.360]   that will win out in deep learning as we go
[00:40:13.360 --> 00:40:16.080]   are those methods which are scalable
[00:40:16.080 --> 00:40:18.680]   in learning and search, is what it calls out.
[00:40:18.680 --> 00:40:23.680]   And the scale word gets a lot of attention in this.
[00:40:23.680 --> 00:40:26.680]   The interpretation that I use
[00:40:26.680 --> 00:40:31.680]   is effectively to avoid adding human priors
[00:40:31.680 --> 00:40:33.560]   to your learning process.
[00:40:33.560 --> 00:40:34.920]   And if you read the original essay,
[00:40:34.920 --> 00:40:36.080]   this is what it talks about,
[00:40:36.080 --> 00:40:39.120]   is how researchers will try to come up
[00:40:39.120 --> 00:40:43.120]   with clever solutions to their specific problem
[00:40:43.120 --> 00:40:46.320]   that might get them small gains in the short-term
[00:40:46.320 --> 00:40:48.840]   while simply enabling these deep learning systems
[00:40:48.840 --> 00:40:52.120]   to work efficiently and for these bigger problems
[00:40:52.120 --> 00:40:53.880]   in the long-term might be more likely
[00:40:53.880 --> 00:40:57.200]   to scale and continue to drive success.
[00:40:57.200 --> 00:40:59.880]   And therefore, we were talking
[00:40:59.880 --> 00:41:02.440]   about relatively small implementation changes
[00:41:02.440 --> 00:41:05.000]   to the mixture of experts model.
[00:41:05.000 --> 00:41:06.920]   And therefore, it's like, okay,
[00:41:06.920 --> 00:41:09.080]   we will need a few more years to know
[00:41:09.080 --> 00:41:10.880]   if one of these are actually really crucial
[00:41:10.880 --> 00:41:12.280]   to the better lesson.
[00:41:12.280 --> 00:41:14.800]   But the better lesson is really this long-term arc
[00:41:14.800 --> 00:41:17.200]   of how simplicity can often win.
[00:41:17.200 --> 00:41:19.040]   And there's a lot of sayings in the industry,
[00:41:19.040 --> 00:41:20.400]   like the models just wanna learn.
[00:41:20.400 --> 00:41:23.560]   You have to give them the simple loss landscape
[00:41:23.560 --> 00:41:25.360]   where you put compute through the model
[00:41:25.360 --> 00:41:29.600]   and they will learn and getting barriers out of the way.
[00:41:29.600 --> 00:41:31.800]   - That's where the power of something like Nickel comes in
[00:41:31.800 --> 00:41:35.360]   where standardized code that can be used
[00:41:35.360 --> 00:41:38.200]   by a lot of people to create sort of simple innovations
[00:41:38.200 --> 00:41:41.240]   that can scale, which is why the hacks,
[00:41:41.240 --> 00:41:43.080]   I imagine that the code base
[00:41:43.080 --> 00:41:45.120]   for DeepSeq is probably a giant mess.
[00:41:45.120 --> 00:41:47.960]   - I'm sure they have, DeepSeq definitely has code bases
[00:41:47.960 --> 00:41:49.000]   that are extremely messy
[00:41:49.000 --> 00:41:50.640]   where they're testing these new ideas.
[00:41:50.640 --> 00:41:53.280]   Multi-head latent attention probably start,
[00:41:53.280 --> 00:41:55.120]   could start in something like a Jupyter notebook
[00:41:55.120 --> 00:41:57.920]   or somebody tries something on a few GPUs
[00:41:57.920 --> 00:41:59.960]   and that is really messy.
[00:41:59.960 --> 00:42:03.480]   But the stuff that trains the DeepSeq v3 and DeepSeq r1,
[00:42:03.480 --> 00:42:06.600]   those libraries, if you were to present them to us,
[00:42:06.600 --> 00:42:10.120]   I would guess are extremely high quality code.
[00:42:10.120 --> 00:42:12.600]   - High quality readable code, yeah.
[00:42:12.600 --> 00:42:14.680]   - I think there is one aspect to note though, right?
[00:42:14.680 --> 00:42:18.920]   Is that there is the general ability for that
[00:42:18.920 --> 00:42:21.480]   to transfer across different types of runs, right?
[00:42:21.480 --> 00:42:23.880]   You may make really, really high quality code
[00:42:23.880 --> 00:42:27.800]   for one specific model architecture at one size.
[00:42:27.800 --> 00:42:29.960]   And then that is not transferable to,
[00:42:29.960 --> 00:42:32.200]   hey, when I make this architecture tweak,
[00:42:32.200 --> 00:42:33.360]   everything's broken again, right?
[00:42:33.360 --> 00:42:35.360]   Like that's something that could be,
[00:42:37.480 --> 00:42:40.160]   with their specific low level coding of like scheduling SMs
[00:42:40.160 --> 00:42:43.440]   is specific to this model architecture and size, right?
[00:42:43.440 --> 00:42:46.640]   And whereas like NVIDIA's collectives library is more like,
[00:42:46.640 --> 00:42:48.840]   hey, it'll work for anything, right?
[00:42:48.840 --> 00:42:50.240]   You wanna do an all-reduce, great.
[00:42:50.240 --> 00:42:52.760]   I don't care what your model architecture is, it'll work.
[00:42:52.760 --> 00:42:55.360]   And you're giving up a lot of performance when you do that
[00:42:55.360 --> 00:42:57.800]   in many cases, but it's worthwhile for them
[00:42:57.800 --> 00:43:01.720]   to do the specific optimization for the specific run
[00:43:01.720 --> 00:43:04.200]   given the constraints that they have regarding compute.
[00:43:04.200 --> 00:43:07.480]   - I wonder how stressful it is to like,
[00:43:07.480 --> 00:43:10.720]   you know, these frontier models, like initiate training,
[00:43:10.720 --> 00:43:14.320]   like to have the code to push the button
[00:43:14.320 --> 00:43:18.400]   that like you're now spending a large amount of money
[00:43:18.400 --> 00:43:20.760]   and time to train this.
[00:43:20.760 --> 00:43:23.920]   Like there must, I mean, there must be a lot of innovation
[00:43:23.920 --> 00:43:26.800]   on the debugging stage of like making sure
[00:43:26.800 --> 00:43:29.280]   there's no issues that you're monitoring
[00:43:29.280 --> 00:43:31.760]   and visualizing every aspect of the training,
[00:43:31.760 --> 00:43:32.960]   all that kind of stuff.
[00:43:32.960 --> 00:43:33.920]   - When people are training,
[00:43:33.920 --> 00:43:35.280]   they have all these various dashboards,
[00:43:35.280 --> 00:43:38.640]   but like the most simple one is your loss, right?
[00:43:38.640 --> 00:43:41.320]   And it continues to go down, but in reality,
[00:43:41.320 --> 00:43:44.040]   especially with more complicated stuff like MOE,
[00:43:44.040 --> 00:43:46.120]   the biggest problem with it, or FP8 training,
[00:43:46.120 --> 00:43:47.360]   which is another innovation, you know,
[00:43:47.360 --> 00:43:50.360]   going to a lower precision number format, i.e. less accurate
[00:43:50.360 --> 00:43:52.320]   is that you end up with loss spikes, right?
[00:43:52.320 --> 00:43:54.560]   And no one knows why the loss spike happened.
[00:43:54.560 --> 00:43:55.400]   And for a long-
[00:43:55.400 --> 00:43:56.240]   - Some of them you do.
[00:43:56.240 --> 00:43:57.080]   - Some of them you do.
[00:43:57.080 --> 00:43:57.920]   - Some of them are bad data.
[00:43:57.920 --> 00:44:00.680]   Can I give a AI2's example of what blew up our earlier
[00:44:00.680 --> 00:44:03.240]   models is a subreddit called Microwave Gang.
[00:44:03.240 --> 00:44:04.800]   We love to shout this out.
[00:44:04.800 --> 00:44:05.640]   It's a real thing.
[00:44:05.640 --> 00:44:07.080]   You can pull up Microwave Gang.
[00:44:07.080 --> 00:44:09.760]   Essentially, it's a subreddit where everybody makes posts
[00:44:09.760 --> 00:44:10.800]   that are just the letter M.
[00:44:10.800 --> 00:44:12.400]   So it's like, "Mm."
[00:44:12.400 --> 00:44:15.360]   So there's extremely long sequences of the letter M.
[00:44:15.360 --> 00:44:16.720]   And then the comments are like, "Beep, beep."
[00:44:16.720 --> 00:44:18.040]   'Cause it's in the microwave vents.
[00:44:18.040 --> 00:44:19.840]   But if you pass this into a model that's trained
[00:44:19.840 --> 00:44:22.880]   to be a normal producing text, it's extremely high loss.
[00:44:22.880 --> 00:44:24.680]   'Cause normally you see an M,
[00:44:24.680 --> 00:44:27.160]   you don't predict Ms for a long time.
[00:44:27.160 --> 00:44:30.000]   So like this is something that caused the loss spikes for us.
[00:44:30.000 --> 00:44:31.960]   But when you have much like, this is old.
[00:44:31.960 --> 00:44:33.160]   This is not recent.
[00:44:33.160 --> 00:44:35.280]   And when you have more mature data systems,
[00:44:35.280 --> 00:44:37.040]   that's not the thing that causes the loss spike.
[00:44:37.040 --> 00:44:38.320]   And what Dylan is saying is true.
[00:44:38.320 --> 00:44:41.760]   But it's like, it's levels to this sort of idea.
[00:44:41.760 --> 00:44:44.000]   - With regards to the stress, right?
[00:44:44.000 --> 00:44:45.320]   These people are like, you know,
[00:44:45.320 --> 00:44:46.920]   you'll go out to dinner with like a friend
[00:44:46.920 --> 00:44:48.320]   that works at one of these labs.
[00:44:48.320 --> 00:44:50.840]   And they'll just be like looking at their phone
[00:44:50.840 --> 00:44:51.840]   every like 10 minutes.
[00:44:51.840 --> 00:44:53.000]   And they're not like, you know,
[00:44:53.000 --> 00:44:54.160]   it's one thing if they're texting,
[00:44:54.160 --> 00:44:56.960]   but they're just like, is the loss, is the loss provoking?
[00:44:56.960 --> 00:45:01.040]   - Like tokens per second, loss not blown up.
[00:45:01.040 --> 00:45:02.920]   They're just watching this.
[00:45:02.920 --> 00:45:05.440]   - And the heart rate goes up if there's a spike.
[00:45:05.440 --> 00:45:07.200]   - And some level of spikes is normal, right?
[00:45:07.200 --> 00:45:08.720]   It'll recover and be back.
[00:45:08.720 --> 00:45:10.800]   Sometimes a lot of the old strategy was like,
[00:45:10.800 --> 00:45:13.400]   you just stop the run, restart from the old version,
[00:45:13.400 --> 00:45:15.120]   and then like change the data mix.
[00:45:15.120 --> 00:45:16.040]   And then it keeps going.
[00:45:16.040 --> 00:45:17.800]   - There are even different types of spikes.
[00:45:17.800 --> 00:45:20.920]   So Dirk Grunenfeld has a theory that I do,
[00:45:20.920 --> 00:45:22.720]   that's like fast spikes and slow spikes.
[00:45:22.720 --> 00:45:25.040]   Where there are sometimes where you're looking at the loss
[00:45:25.040 --> 00:45:25.880]   and there are other parameters,
[00:45:25.880 --> 00:45:28.640]   you can see it start to creep up and then blow up.
[00:45:28.640 --> 00:45:30.080]   And that's really hard to recover from.
[00:45:30.080 --> 00:45:31.400]   So you have to go back much further.
[00:45:31.400 --> 00:45:33.520]   So you have the stressful period where it's like flat
[00:45:33.520 --> 00:45:34.360]   or it might start going up.
[00:45:34.360 --> 00:45:35.440]   And you're like, what do I do?
[00:45:35.440 --> 00:45:37.000]   Whereas there are also loss spikes that are,
[00:45:37.000 --> 00:45:37.840]   it looks good.
[00:45:37.840 --> 00:45:39.480]   And then there's one spiky data point.
[00:45:39.480 --> 00:45:41.280]   And what you can do is you just skip those.
[00:45:41.280 --> 00:45:42.600]   You see that there's a spike.
[00:45:42.600 --> 00:45:44.160]   You're like, okay, I can ignore this data.
[00:45:44.160 --> 00:45:45.840]   Don't update the model and do the next one
[00:45:45.840 --> 00:45:47.240]   and it'll recover quickly.
[00:45:47.240 --> 00:45:50.480]   But these like on trickier implementations,
[00:45:50.480 --> 00:45:53.440]   as you get more complex in your architecture
[00:45:53.440 --> 00:45:54.880]   and you scale up to more GPUs,
[00:45:54.880 --> 00:45:58.040]   you have more potential for your loss blowing up.
[00:45:58.040 --> 00:46:00.440]   So it's like, there's a distribution.
[00:46:00.440 --> 00:46:02.240]   - The whole idea of grokking also comes in, right?
[00:46:02.240 --> 00:46:03.920]   It's like, just because it slowed down
[00:46:03.920 --> 00:46:06.160]   from improving and loss doesn't mean it's not learning
[00:46:06.160 --> 00:46:07.840]   because all of a sudden it could be like this
[00:46:07.840 --> 00:46:09.440]   and it could just spike down and loss again
[00:46:09.440 --> 00:46:12.360]   because it learned, truly learned something, right?
[00:46:12.360 --> 00:46:14.320]   And it took some time for it to learn that.
[00:46:14.320 --> 00:46:16.080]   It's not like a gradual process, right?
[00:46:16.080 --> 00:46:17.280]   And that's what humans are like.
[00:46:17.280 --> 00:46:18.120]   That's what models are like.
[00:46:18.120 --> 00:46:21.120]   So it's really a stressful task, as you mentioned.
[00:46:21.120 --> 00:46:24.240]   - And the whole time the dollar count is going up.
[00:46:24.240 --> 00:46:26.200]   Every company has failed runs.
[00:46:26.200 --> 00:46:27.960]   You need failed run to push the envelope
[00:46:27.960 --> 00:46:28.920]   on your infrastructure.
[00:46:28.920 --> 00:46:32.040]   So a lot of news cycles are made of X company
[00:46:32.040 --> 00:46:33.680]   had Y failed run.
[00:46:33.680 --> 00:46:36.840]   Every company that's trying to push the frontier of AI
[00:46:36.840 --> 00:46:37.760]   has these.
[00:46:37.760 --> 00:46:40.640]   So it is, yes, it's noteworthy because it's a lot of money
[00:46:40.640 --> 00:46:43.040]   and it can be week to month setback,
[00:46:43.040 --> 00:46:44.840]   but it is part of the process.
[00:46:44.840 --> 00:46:47.600]   - But how do you get, if you're deep seek,
[00:46:47.600 --> 00:46:49.960]   how do you get to a place where, holy shit,
[00:46:49.960 --> 00:46:52.600]   there's a successful combination of hyper parameters?
[00:46:52.600 --> 00:46:54.320]   - A lot of small failed runs.
[00:46:54.320 --> 00:46:59.320]   - And so rapid iteration through failed runs until-
[00:46:59.320 --> 00:47:00.880]   - And successful ones.
[00:47:00.880 --> 00:47:03.440]   - And then you build a summary tuition like this,
[00:47:03.440 --> 00:47:05.520]   this mixture of expert works,
[00:47:05.520 --> 00:47:08.920]   and then this implementation of MLA works.
[00:47:08.920 --> 00:47:13.520]   - Key hyper parameters like learning rate and regularization
[00:47:13.520 --> 00:47:14.360]   and things like this.
[00:47:14.360 --> 00:47:16.600]   And you find the regime that works for your code base.
[00:47:16.600 --> 00:47:19.280]   I've, talking to people at Frontier Labs,
[00:47:19.280 --> 00:47:20.680]   there's a story that you can tell
[00:47:20.720 --> 00:47:23.960]   where training language models is kind of a path
[00:47:23.960 --> 00:47:24.840]   that you need to follow.
[00:47:24.840 --> 00:47:27.120]   So you need to like unlock the ability
[00:47:27.120 --> 00:47:29.600]   to train a certain type of model or a certain scale.
[00:47:29.600 --> 00:47:31.480]   And then your code base and your internal know-how
[00:47:31.480 --> 00:47:34.080]   of which hyper parameters work for it is kind of known.
[00:47:34.080 --> 00:47:36.040]   And you look at the deep seek papers and models,
[00:47:36.040 --> 00:47:38.520]   they've scaled up, they've added complexity.
[00:47:38.520 --> 00:47:41.360]   And it's just continuing to build the capabilities
[00:47:41.360 --> 00:47:42.360]   that they have.
[00:47:42.360 --> 00:47:44.200]   - There's the concept of a YOLO run.
[00:47:44.200 --> 00:47:47.360]   So YOLO, you only live once.
[00:47:47.360 --> 00:47:50.840]   And what it is, is like there's all this experimentation
[00:47:50.840 --> 00:47:52.880]   you do at the small scale, right?
[00:47:52.880 --> 00:47:54.160]   Research ablations, right?
[00:47:54.160 --> 00:47:55.160]   Like you have your Jupyter Notebook
[00:47:55.160 --> 00:47:56.760]   where you're experimenting with MLA
[00:47:56.760 --> 00:47:59.120]   on like three GPUs or whatever.
[00:47:59.120 --> 00:48:01.560]   And you're doing all these different things like,
[00:48:01.560 --> 00:48:04.720]   hey, do I do four active experts, 128 experts?
[00:48:04.720 --> 00:48:06.480]   Do I arrange the experts this way?
[00:48:06.480 --> 00:48:08.920]   You know, all these different model architecture things
[00:48:08.920 --> 00:48:10.800]   you're testing at a very small scale, right?
[00:48:10.800 --> 00:48:12.960]   Couple of researchers, few GPUs,
[00:48:12.960 --> 00:48:15.280]   tens of GPUs, hundreds of GPUs, whatever it is.
[00:48:15.280 --> 00:48:16.520]   And then all of a sudden you're like,
[00:48:16.520 --> 00:48:19.000]   okay guys, no more fucking around, right?
[00:48:19.000 --> 00:48:19.840]   No more screwing around.
[00:48:19.840 --> 00:48:23.000]   Everyone take all the resources we have,
[00:48:23.000 --> 00:48:24.600]   let's pick what we think will work
[00:48:24.600 --> 00:48:25.600]   and just go for it, right?
[00:48:25.600 --> 00:48:26.440]   YOLO.
[00:48:26.440 --> 00:48:29.000]   And this is where that sort of stress comes in is like,
[00:48:29.000 --> 00:48:30.320]   well, I know it works here,
[00:48:30.320 --> 00:48:32.520]   but some things that work here don't work here.
[00:48:32.520 --> 00:48:35.000]   And some things that work here don't work down here, right?
[00:48:35.000 --> 00:48:36.320]   In this terms of scale, right?
[00:48:36.320 --> 00:48:39.520]   So it's really truly a YOLO run.
[00:48:39.520 --> 00:48:42.760]   And sort of like, there's this like discussion
[00:48:42.760 --> 00:48:43.760]   of like certain researchers
[00:48:43.760 --> 00:48:45.360]   just have like this methodical nature,
[00:48:45.360 --> 00:48:47.120]   like they can find the whole search space
[00:48:47.120 --> 00:48:49.840]   and like figure out all the ablations of different research
[00:48:49.840 --> 00:48:51.400]   and really see what is best.
[00:48:51.400 --> 00:48:53.760]   And there's certain researchers who just kind of like,
[00:48:53.760 --> 00:48:56.160]   you know, have that innate gut instinct of like,
[00:48:56.160 --> 00:48:57.240]   this is the YOLO run.
[00:48:57.240 --> 00:49:00.160]   Like, you know, looking at the data, this is it.
[00:49:00.160 --> 00:49:01.680]   - This is why you want to work in post-training
[00:49:01.680 --> 00:49:03.720]   because the GPU costs for training is lower.
[00:49:03.720 --> 00:49:05.720]   So you can make a higher percentage of your training runs
[00:49:05.720 --> 00:49:06.800]   YOLO runs.
[00:49:06.800 --> 00:49:07.640]   - Yeah.
[00:49:07.640 --> 00:49:08.480]   - For now.
[00:49:08.480 --> 00:49:10.440]   - Yeah, for now, for now.
[00:49:10.440 --> 00:49:14.840]   So some of this is fundamentally luck still.
[00:49:14.840 --> 00:49:15.800]   Luck is skill, right?
[00:49:15.800 --> 00:49:16.760]   In many cases.
[00:49:16.760 --> 00:49:18.280]   - Yeah, I mean, it looks lucky, right?
[00:49:18.280 --> 00:49:19.120]   When you're--
[00:49:19.120 --> 00:49:20.240]   - But the hill to climb,
[00:49:20.240 --> 00:49:21.800]   if you're on one of these labs,
[00:49:21.800 --> 00:49:23.520]   you have an evaluation you're not crushing.
[00:49:23.520 --> 00:49:26.680]   There's a repeated playbook of how you improve things.
[00:49:26.680 --> 00:49:28.000]   There are localized improvements,
[00:49:28.000 --> 00:49:29.160]   which might be data improvements.
[00:49:29.160 --> 00:49:30.680]   And these add up into the whole model
[00:49:30.680 --> 00:49:32.000]   just being much better.
[00:49:32.000 --> 00:49:33.320]   And when you zoom in really close,
[00:49:33.320 --> 00:49:35.760]   it can be really obvious that this model
[00:49:35.760 --> 00:49:38.440]   is just really bad at this thing and we can fix it.
[00:49:38.440 --> 00:49:39.360]   And you just add these up.
[00:49:39.360 --> 00:49:42.400]   So some of it feels like luck, but on the ground,
[00:49:42.400 --> 00:49:44.160]   especially with these new reasoning models
[00:49:44.160 --> 00:49:46.560]   we're talking to, it's just so many ways
[00:49:46.560 --> 00:49:47.520]   that we can poke around.
[00:49:47.520 --> 00:49:51.280]   And normally it's that some of them give big improvements.
[00:49:51.280 --> 00:49:52.880]   - The search space is near infinite, right?
[00:49:52.880 --> 00:49:55.240]   And yet the amount of compute and time you have
[00:49:55.240 --> 00:49:59.920]   is very low and you have to hit release schedules.
[00:49:59.920 --> 00:50:02.360]   You have to not get blown past by everyone.
[00:50:02.360 --> 00:50:05.560]   Otherwise, what happened with DeepSeek,
[00:50:05.560 --> 00:50:07.840]   crushing Meta and Mistral and Cohere and all these guys,
[00:50:07.840 --> 00:50:09.040]   they moved too slow, right?
[00:50:09.040 --> 00:50:10.600]   They maybe were too methodical.
[00:50:10.600 --> 00:50:11.920]   I don't know, they didn't hit the YOLO run.
[00:50:11.920 --> 00:50:14.520]   Whatever the reason was, maybe they weren't as skilled.
[00:50:14.520 --> 00:50:16.560]   Whatever, you know, you can call it luck if you want,
[00:50:16.560 --> 00:50:18.160]   but at the end of the day, it's skill.
[00:50:18.160 --> 00:50:21.760]   - So 2025 is the year of the YOLO run.
[00:50:21.760 --> 00:50:24.960]   It seems like all the labs are like going in.
[00:50:24.960 --> 00:50:27.520]   - I think it's even more impressive
[00:50:27.520 --> 00:50:29.960]   what OpenAI did in 2022, right?
[00:50:29.960 --> 00:50:32.280]   At the time, no one believed in mixture of experts models,
[00:50:32.280 --> 00:50:35.680]   right, at Google, who had all the researchers.
[00:50:35.680 --> 00:50:38.000]   OpenAI had such little compute
[00:50:38.000 --> 00:50:40.920]   and they devoted all of their compute for many months,
[00:50:40.920 --> 00:50:44.760]   right, all of it, 100% for many months to GPT-4
[00:50:44.760 --> 00:50:47.920]   with a brand new architecture with no belief that,
[00:50:47.920 --> 00:50:49.960]   hey, let me spend a couple hundred million dollars,
[00:50:49.960 --> 00:50:52.760]   which is all of the money I have on this model, right?
[00:50:52.760 --> 00:50:55.560]   That is truly YOLO, right?
[00:50:55.560 --> 00:50:57.280]   Now, you know, people are like,
[00:50:57.280 --> 00:50:58.880]   all these like training run failures
[00:50:58.880 --> 00:50:59.960]   that are in the media, right?
[00:50:59.960 --> 00:51:02.000]   It's like, okay, great, but like actually,
[00:51:02.000 --> 00:51:04.240]   a huge chunk of my GPs are doing inference.
[00:51:04.240 --> 00:51:06.120]   I still have a bunch doing research constantly.
[00:51:06.120 --> 00:51:08.120]   And yes, my biggest cluster is training,
[00:51:08.120 --> 00:51:09.800]   but like on this YOLO run,
[00:51:09.800 --> 00:51:12.480]   but like that YOLO run is much less risky
[00:51:12.480 --> 00:51:14.960]   than like what OpenAI did in 2022,
[00:51:14.960 --> 00:51:16.600]   or maybe what DeepSeek did now,
[00:51:16.600 --> 00:51:17.680]   or, you know, like sort of like,
[00:51:17.680 --> 00:51:19.520]   hey, we're just gonna throw everything at it.
[00:51:19.520 --> 00:51:21.720]   - The big winners throughout human history
[00:51:21.720 --> 00:51:25.800]   are the ones who are willing to do YOLO at some point.
[00:51:25.800 --> 00:51:27.640]   Okay, what do we understand
[00:51:27.640 --> 00:51:30.800]   about the hardware it's been trained on, DeepSeek?
[00:51:30.800 --> 00:51:33.000]   - DeepSeek is very interesting, right?
[00:51:33.000 --> 00:51:34.480]   This is where, second to take us to zoom out
[00:51:34.480 --> 00:51:36.040]   out of who they are, first of all, right?
[00:51:36.040 --> 00:51:38.080]   HiFlyer is a hedge fund
[00:51:38.080 --> 00:51:40.680]   that has historically done quantitative trading
[00:51:40.680 --> 00:51:42.640]   in China as well as elsewhere.
[00:51:42.640 --> 00:51:45.560]   And they have always had a significant number of GPUs, right?
[00:51:45.560 --> 00:51:47.720]   In the past, a lot of these high-frequency trading,
[00:51:47.720 --> 00:51:50.520]   algorithmic quant traders used FPGAs,
[00:51:50.520 --> 00:51:52.280]   but it shifted to GPUs definitely.
[00:51:52.280 --> 00:51:53.280]   And there's both, right?
[00:51:53.280 --> 00:51:55.920]   But GPUs especially, and HiFlyer,
[00:51:55.920 --> 00:51:57.600]   which is the hedge fund that owns DeepSeek,
[00:51:57.600 --> 00:51:59.200]   and everyone who works for DeepSeek
[00:51:59.200 --> 00:52:02.160]   is part of HiFlyer to some extent, right?
[00:52:02.160 --> 00:52:04.960]   It's same parent company, same owner, same CEO.
[00:52:04.960 --> 00:52:06.640]   They had all these resources
[00:52:06.640 --> 00:52:09.280]   and infrastructure for trading.
[00:52:09.280 --> 00:52:12.240]   And then they devoted a humongous portion of them
[00:52:12.240 --> 00:52:13.920]   to training models,
[00:52:13.920 --> 00:52:15.800]   both language models and otherwise, right?
[00:52:15.800 --> 00:52:19.400]   Because these techniques were heavily AI-influenced.
[00:52:19.400 --> 00:52:23.200]   More recently, people have realized,
[00:52:23.200 --> 00:52:24.520]   hey, trading with...
[00:52:24.520 --> 00:52:27.520]   Even when you go back to like Renaissance
[00:52:27.520 --> 00:52:29.800]   and all these quantitative firms,
[00:52:29.800 --> 00:52:31.280]   natural language processing is the key
[00:52:31.280 --> 00:52:33.240]   to trading really fast, right?
[00:52:33.240 --> 00:52:34.960]   Understanding a press release
[00:52:34.960 --> 00:52:36.080]   and making the right trade, right?
[00:52:36.080 --> 00:52:38.800]   And so DeepSeek has always been really good at this.
[00:52:38.800 --> 00:52:41.160]   And even as far back as 2021,
[00:52:41.160 --> 00:52:43.720]   they have press releases and papers saying like,
[00:52:43.720 --> 00:52:45.960]   hey, we're the first company in China
[00:52:45.960 --> 00:52:47.840]   with an A100 cluster this large.
[00:52:47.840 --> 00:52:50.160]   There's 10,000 A100 GPUs, right?
[00:52:50.160 --> 00:52:51.640]   This is in 2021.
[00:52:51.640 --> 00:52:54.480]   Now, this wasn't all for training large language models.
[00:52:54.480 --> 00:52:56.400]   This was mostly for training models
[00:52:56.400 --> 00:52:58.240]   for their quantitative aspects,
[00:52:58.240 --> 00:52:59.760]   their quantitative trading,
[00:52:59.760 --> 00:53:01.760]   as well as a lot of that was natural language processing
[00:53:01.760 --> 00:53:03.240]   to be clear, right?
[00:53:03.240 --> 00:53:04.920]   And so this is the sort of history, right?
[00:53:04.920 --> 00:53:07.040]   So verifiable fact is that in 2021,
[00:53:07.040 --> 00:53:08.720]   they built the largest Chinese cluster.
[00:53:08.720 --> 00:53:11.160]   At least they claim it was the largest cluster in China,
[00:53:11.160 --> 00:53:12.240]   10,000 GPUs.
[00:53:12.240 --> 00:53:14.320]   - Before expert controls started.
[00:53:14.320 --> 00:53:15.160]   - Yeah.
[00:53:15.160 --> 00:53:16.360]   - It's like they've had a huge cluster
[00:53:16.360 --> 00:53:18.360]   before any conversation of expert controls.
[00:53:18.360 --> 00:53:20.200]   - So then you step it forward to like,
[00:53:20.200 --> 00:53:23.280]   what have they done over the last four years since then?
[00:53:23.280 --> 00:53:25.560]   Obviously they've continued to operate the hedge fund,
[00:53:25.560 --> 00:53:26.640]   probably make tons of money.
[00:53:26.640 --> 00:53:28.680]   And the other thing is that they've leaned more
[00:53:28.680 --> 00:53:30.120]   and more and more into AI.
[00:53:30.120 --> 00:53:33.440]   The CEO, Lian Cheng Feng, Lian.
[00:53:33.440 --> 00:53:34.480]   - You're not putting me in the spot
[00:53:34.480 --> 00:53:36.240]   unless we discuss this more.
[00:53:36.240 --> 00:53:37.120]   - Lian Feng, right?
[00:53:37.120 --> 00:53:41.120]   The CEO, he owns maybe a little bit more
[00:53:41.120 --> 00:53:43.640]   than half the company allegedly, right?
[00:53:43.640 --> 00:53:47.160]   Is an extremely like Elon Jensen kind of figure
[00:53:47.160 --> 00:53:50.040]   where he's just like involved in everything, right?
[00:53:50.040 --> 00:53:51.720]   And so over that time period,
[00:53:51.720 --> 00:53:53.880]   he's gotten really in-depth into AI.
[00:53:53.880 --> 00:53:55.920]   He actually has a bit of a like,
[00:53:55.920 --> 00:53:57.320]   if you see some of his statements,
[00:53:57.320 --> 00:53:59.520]   a bit of an EAC vibe almost, right?
[00:53:59.520 --> 00:54:01.240]   Total AGI vibes.
[00:54:01.240 --> 00:54:02.640]   They're like, we need to do this.
[00:54:02.640 --> 00:54:06.440]   We need to make a new ecosystem of open AI.
[00:54:06.440 --> 00:54:09.000]   We need China to lead on this sort of ecosystem
[00:54:09.000 --> 00:54:11.080]   because historically the Western countries
[00:54:11.080 --> 00:54:13.960]   have led on software ecosystems
[00:54:13.960 --> 00:54:17.320]   and straight up acknowledges like in order to do this,
[00:54:17.320 --> 00:54:19.040]   we need to do something different.
[00:54:19.040 --> 00:54:21.160]   DeepSeek is his way of doing this.
[00:54:21.160 --> 00:54:22.400]   Some of the translated interviews
[00:54:22.400 --> 00:54:24.240]   with him are fantastic. - So he has done interviews?
[00:54:24.240 --> 00:54:25.080]   - Yeah.
[00:54:25.080 --> 00:54:26.960]   - Do you think he would do a Western interview or no?
[00:54:26.960 --> 00:54:27.800]   Or is there controls on the channel?
[00:54:28.120 --> 00:54:31.920]   - There hasn't been one yet, but I would try it.
[00:54:31.920 --> 00:54:33.640]   - I just got a Chinese translator.
[00:54:33.640 --> 00:54:34.480]   So it was great.
[00:54:34.480 --> 00:54:36.360]   This is all push.
[00:54:36.360 --> 00:54:40.640]   So fascinating figure, engineer, pushing full on into AI,
[00:54:40.640 --> 00:54:44.200]   leveraging the success from the high-frequency trading.
[00:54:44.200 --> 00:54:45.600]   - Very direct quotes.
[00:54:45.600 --> 00:54:47.880]   Like we will not switch to closed source
[00:54:47.880 --> 00:54:49.560]   when asked about this stuff.
[00:54:49.560 --> 00:54:54.560]   Very long-term motivated in how the ecosystem of AI
[00:54:55.400 --> 00:54:58.640]   should work and I think from a Chinese perspective,
[00:54:58.640 --> 00:55:02.960]   he wants a Chinese company to build this vision.
[00:55:02.960 --> 00:55:05.760]   - And so this is sort of like the quote unquote visionary
[00:55:05.760 --> 00:55:07.040]   behind the company, right?
[00:55:07.040 --> 00:55:08.440]   This hedge fund still exists, right?
[00:55:08.440 --> 00:55:09.400]   This quantitative firm.
[00:55:09.400 --> 00:55:13.520]   And so DeepSeek is the sort of,
[00:55:13.520 --> 00:55:16.720]   slowly he got turned to this full view of like AI,
[00:55:16.720 --> 00:55:17.760]   everything about this, right?
[00:55:17.760 --> 00:55:19.680]   But at some point it slowly maneuvered
[00:55:19.680 --> 00:55:20.800]   and he made DeepSeek.
[00:55:20.800 --> 00:55:23.440]   And DeepSeek has done multiple models since then.
[00:55:23.440 --> 00:55:24.880]   They've acquired more and more GPUs.
[00:55:24.880 --> 00:55:27.920]   They share infrastructure with the fund, right?
[00:55:27.920 --> 00:55:32.920]   And so there is no exact number of public GPU resources
[00:55:32.920 --> 00:55:35.800]   that they have, but besides this 10,000 GPUs
[00:55:35.800 --> 00:55:38.000]   that they bought in 2021, right?
[00:55:38.000 --> 00:55:40.120]   And they were fantastically profitable, right?
[00:55:40.120 --> 00:55:44.040]   And then this paper claims they did only 2,000 H800 GPUs,
[00:55:44.040 --> 00:55:45.760]   which are a restricted GPU
[00:55:45.760 --> 00:55:47.360]   that was previously allowed in China,
[00:55:47.360 --> 00:55:48.200]   but no longer allowed.
[00:55:48.200 --> 00:55:49.200]   And there's a new version,
[00:55:49.200 --> 00:55:52.440]   but it's basically NVIDIA's H100 for China, right?
[00:55:52.440 --> 00:55:54.160]   And that there's some restrictions on it
[00:55:54.160 --> 00:55:56.480]   specifically around the communications,
[00:55:56.480 --> 00:55:58.520]   sort of a speed, the interconnect speed, right?
[00:55:58.520 --> 00:56:01.160]   Which is why they had to do this crazy SM,
[00:56:01.160 --> 00:56:02.960]   you know, scheduling stuff, right?
[00:56:02.960 --> 00:56:03.800]   So going back to that, right?
[00:56:03.800 --> 00:56:06.960]   It looks like this is obviously not true
[00:56:06.960 --> 00:56:08.240]   in terms of their total GPU count.
[00:56:08.240 --> 00:56:11.800]   - Obvious available GPUs, but for this training run,
[00:56:11.800 --> 00:56:14.200]   you think 2000 is the correct number or no?
[00:56:14.200 --> 00:56:16.440]   - So this is where it takes, you know,
[00:56:16.440 --> 00:56:18.520]   a significant amount of sort of like zoning in, right?
[00:56:18.520 --> 00:56:21.560]   Like what do you call your training run, right?
[00:56:21.560 --> 00:56:23.560]   You count all of the research
[00:56:23.560 --> 00:56:25.560]   and ablations that you ran, right?
[00:56:25.560 --> 00:56:26.400]   Picking all this stuff,
[00:56:26.400 --> 00:56:28.000]   because yes, you can do a YOLO run,
[00:56:28.000 --> 00:56:30.680]   but at some level you have to do the test at the small scale
[00:56:30.680 --> 00:56:32.360]   and then you have to do some tests at medium scale
[00:56:32.360 --> 00:56:33.640]   before you go to a large scale.
[00:56:33.640 --> 00:56:36.760]   Accepted practice is that for any given model
[00:56:36.760 --> 00:56:38.280]   that is a notable advancement,
[00:56:38.280 --> 00:56:40.120]   you're gonna do two to four X compute
[00:56:40.120 --> 00:56:43.280]   of the full training run in experiments alone.
[00:56:43.280 --> 00:56:45.680]   - So a lot of this compute that's being scaled up
[00:56:45.680 --> 00:56:48.960]   is probably used in large part at this time for research.
[00:56:48.960 --> 00:56:50.800]   - Yeah, and research will, you know,
[00:56:50.800 --> 00:56:52.040]   research begets the new ideas
[00:56:52.040 --> 00:56:53.120]   that let you get huge efficiency.
[00:56:53.120 --> 00:56:54.920]   - Research gets you O1.
[00:56:54.920 --> 00:56:56.720]   Like research gets you breakthroughs
[00:56:56.720 --> 00:56:57.680]   and you need to bet on it.
[00:56:57.680 --> 00:56:59.960]   - So some of the pricing strategy they will discuss
[00:56:59.960 --> 00:57:02.480]   has the research baked into the price.
[00:57:02.480 --> 00:57:04.000]   - So the numbers that DeepSeek
[00:57:04.000 --> 00:57:05.680]   specifically said publicly, right,
[00:57:05.680 --> 00:57:08.120]   are just the 10,000 GPUs in 2021,
[00:57:08.120 --> 00:57:12.080]   and then 2000 GPUs for only the pre-training for V3.
[00:57:12.080 --> 00:57:14.240]   They did not discuss cost on R1.
[00:57:14.240 --> 00:57:17.600]   They did not discuss cost on all the other RL, right,
[00:57:17.600 --> 00:57:19.920]   for the instruct model that they made, right?
[00:57:19.920 --> 00:57:22.520]   They only discussed the pre-training for the base model,
[00:57:22.520 --> 00:57:25.120]   and they did not discuss anything on research and ablations.
[00:57:25.120 --> 00:57:26.960]   And they do not talk about any of the resources
[00:57:26.960 --> 00:57:28.240]   that are shared in terms of,
[00:57:28.240 --> 00:57:31.040]   "Hey, the fund is using all these GPUs," right?
[00:57:31.040 --> 00:57:32.960]   And we know that they're very profitable
[00:57:32.960 --> 00:57:36.440]   and that 10,000 GPUs in 2021.
[00:57:36.440 --> 00:57:40.160]   So some of the research that we've found
[00:57:40.160 --> 00:57:43.600]   is that we actually believe they have closer to 50,000 GPUs.
[00:57:43.600 --> 00:57:45.440]   - We as semi-analysts, so we should say
[00:57:45.440 --> 00:57:48.120]   that you're sort of one of the world experts
[00:57:48.120 --> 00:57:50.600]   in figuring out what everybody's doing
[00:57:50.600 --> 00:57:51.800]   in terms of the semiconductor,
[00:57:51.800 --> 00:57:53.040]   in terms of cluster build-outs,
[00:57:53.040 --> 00:57:57.640]   in terms of who's doing what in terms of training runs.
[00:57:57.640 --> 00:57:59.120]   So yeah, so that's the we.
[00:57:59.120 --> 00:58:00.840]   Okay, go ahead. - Yeah, sorry.
[00:58:00.840 --> 00:58:02.120]   We believe they actually have something
[00:58:02.120 --> 00:58:03.720]   closer to 50,000 GPUs, right?
[00:58:03.720 --> 00:58:05.760]   Now, this is split across many tasks, right?
[00:58:05.760 --> 00:58:09.360]   Again, the fund, research and ablations.
[00:58:09.360 --> 00:58:11.640]   - For ballpark, how much would open AI or Anthropic have?
[00:58:11.640 --> 00:58:14.200]   I think the clearest example we have,
[00:58:14.200 --> 00:58:15.320]   because Meta is also open,
[00:58:15.320 --> 00:58:20.080]   they talk about order of 60K to 100K H100 equivalent GPUs
[00:58:20.080 --> 00:58:21.360]   in their training clusters.
[00:58:21.360 --> 00:58:25.680]   - Right, so like Lama3, they trained on 16,000 H100s, right?
[00:58:25.680 --> 00:58:28.320]   But the company of Meta last year publicly disclosed
[00:58:28.320 --> 00:58:30.640]   they bought like 400 something thousand GPUs, right?
[00:58:30.640 --> 00:58:33.000]   So of course, tiny percentage on the training,
[00:58:33.000 --> 00:58:34.800]   again, most of it is like serving me
[00:58:34.800 --> 00:58:36.560]   the best Instagram reels, right?
[00:58:36.560 --> 00:58:37.400]   Or whatever, right?
[00:58:37.400 --> 00:58:38.960]   - I mean, we could get into a cost of like,
[00:58:38.960 --> 00:58:44.160]   what is the cost of ownership for a 2000 GPU cluster, 10,000?
[00:58:44.160 --> 00:58:45.800]   - There's just different sizes of companies
[00:58:45.800 --> 00:58:46.680]   that can afford these things.
[00:58:46.680 --> 00:58:49.600]   And DeepSeek is reasonably big.
[00:58:49.600 --> 00:58:52.120]   Their compute allocation compared
[00:58:52.120 --> 00:58:55.240]   is one of the top few in the world.
[00:58:55.240 --> 00:58:56.840]   It's not open AI, Anthropic, et cetera,
[00:58:56.840 --> 00:58:58.000]   but they have a lot of compute.
[00:58:58.000 --> 00:58:59.520]   - Can you in general actually just zoom out
[00:58:59.520 --> 00:59:01.640]   and also talk about the Hopper architecture,
[00:59:01.640 --> 00:59:04.440]   the NVIDIA Hopper GPU architecture
[00:59:04.440 --> 00:59:08.120]   and the difference between H100 and H800,
[00:59:08.120 --> 00:59:09.680]   like you mentioned, the interconnects.
[00:59:09.680 --> 00:59:11.640]   - Yeah, so there's, you know, Ampere was the A100
[00:59:11.640 --> 00:59:13.600]   and then H100 Hopper, right?
[00:59:13.600 --> 00:59:15.560]   People use them synonymously in the US
[00:59:15.560 --> 00:59:18.560]   because really there's just H100 and now there's H200, right?
[00:59:18.560 --> 00:59:20.920]   But same thing, mostly.
[00:59:20.920 --> 00:59:23.280]   In China, there've been different salvos
[00:59:23.280 --> 00:59:24.560]   of export restrictions.
[00:59:24.560 --> 00:59:26.600]   So initially the US government limited
[00:59:26.600 --> 00:59:27.960]   on a two-factor scale, right?
[00:59:27.960 --> 00:59:31.080]   Which is chip interconnect versus flops, right?
[00:59:31.080 --> 00:59:33.680]   So any chip that had interconnects above a certain level
[00:59:33.680 --> 00:59:36.360]   and flops above a certain floating point operations
[00:59:36.360 --> 00:59:38.720]   above a certain level was restricted.
[00:59:38.720 --> 00:59:41.280]   Later, the government realized that this was a flaw
[00:59:41.280 --> 00:59:43.480]   in the restriction and they cut it down
[00:59:43.480 --> 00:59:46.240]   to just floating point operations.
[00:59:46.240 --> 00:59:47.760]   And so-
[00:59:47.760 --> 00:59:51.560]   - H800 had high flops, low communication?
[00:59:51.560 --> 00:59:54.560]   - Exactly, so the H800 was the same performance
[00:59:54.560 --> 00:59:56.520]   as H100 on flops, right?
[00:59:56.520 --> 00:59:59.000]   But it just had the interconnect bandwidth cut.
[00:59:59.000 --> 01:00:02.040]   DeepSeek knew how to utilize this, you know,
[01:00:02.040 --> 01:00:04.280]   hey, even though we're cut back on the interconnect,
[01:00:04.280 --> 01:00:06.520]   we can do all this fancy stuff to figure out
[01:00:06.520 --> 01:00:09.240]   how to use the GPU fully anyways, right?
[01:00:09.240 --> 01:00:12.480]   And so that was back in October, 2022,
[01:00:12.480 --> 01:00:17.480]   but later in 2023, end of 2023, implemented in 2024,
[01:00:17.480 --> 01:00:20.000]   the US government banned the H800, right?
[01:00:20.000 --> 01:00:23.080]   And so by the way, this H800 cluster, these 2000 GPUs
[01:00:23.080 --> 01:00:24.800]   was not even purchased in 2024, right?
[01:00:24.800 --> 01:00:26.360]   It was purchased in late 2023.
[01:00:26.360 --> 01:00:29.080]   And they're just getting the model out now, right?
[01:00:29.080 --> 01:00:31.320]   Because it takes a lot of research, et cetera.
[01:00:31.320 --> 01:00:34.800]   H800 was banned and now there's a new chip called the H20.
[01:00:34.800 --> 01:00:38.160]   The H20 is cut back on only flops,
[01:00:38.160 --> 01:00:39.920]   but the interconnect bandwidth is the same.
[01:00:39.920 --> 01:00:42.640]   And in fact, in some ways it's better than the H100
[01:00:42.640 --> 01:00:45.440]   because it has better memory bandwidth and memory capacity.
[01:00:45.440 --> 01:00:47.160]   So there are, you know, NVIDIA is working
[01:00:47.160 --> 01:00:49.560]   within the constraints of what the government says
[01:00:49.560 --> 01:00:52.320]   and then builds the best possible GPU for China.
[01:00:52.320 --> 01:00:54.000]   - Can we take this extra tangent
[01:00:54.000 --> 01:00:55.920]   and we'll return back to the hardware?
[01:00:55.920 --> 01:00:59.360]   Is the philosophy, the motivation,
[01:00:59.360 --> 01:01:02.000]   the case for export controls, what is it?
[01:01:02.000 --> 01:01:05.680]   Dari Amadej has published a blog post about export controls.
[01:01:05.680 --> 01:01:09.040]   The case he makes is that if AI becomes super powerful
[01:01:09.040 --> 01:01:13.760]   and he says by 2026, we'll have AGI or super powerful AI,
[01:01:13.760 --> 01:01:15.320]   and that's going to give a significant,
[01:01:15.320 --> 01:01:16.920]   whoever builds that will have
[01:01:16.920 --> 01:01:19.360]   a significant military advantage.
[01:01:19.360 --> 01:01:22.920]   And so, because the United States is a democracy
[01:01:22.920 --> 01:01:27.200]   and as he says, China is authoritarian
[01:01:27.200 --> 01:01:29.460]   or has authoritarian elements,
[01:01:29.460 --> 01:01:34.460]   you want a unipolar world where the super powerful military
[01:01:34.460 --> 01:01:37.400]   because of the AI is one that's a democracy.
[01:01:37.400 --> 01:01:40.420]   It's a much more complicated world geopolitically
[01:01:40.420 --> 01:01:44.760]   when you have two superpowers with super powerful AI
[01:01:44.760 --> 01:01:46.720]   and one is authoritarian.
[01:01:46.720 --> 01:01:47.840]   So that's the case he makes.
[01:01:47.840 --> 01:01:50.880]   And so we wanna, the United States wants
[01:01:50.880 --> 01:01:53.240]   to use export controls to slow down,
[01:01:53.240 --> 01:01:56.200]   to make sure that China can't do
[01:01:56.200 --> 01:01:58.800]   these gigantic training runs
[01:01:58.800 --> 01:02:02.600]   that would be presumably required to build AGI.
[01:02:02.600 --> 01:02:03.800]   - This is very abstract.
[01:02:03.800 --> 01:02:06.000]   I think this can be the goal
[01:02:06.000 --> 01:02:08.360]   of how some people describe export controls
[01:02:08.360 --> 01:02:10.280]   is the super powerful AI.
[01:02:10.280 --> 01:02:14.060]   There's, and you touched on the training run idea.
[01:02:14.060 --> 01:02:16.820]   There's not many worlds
[01:02:16.820 --> 01:02:18.860]   where China cannot train AI models.
[01:02:18.860 --> 01:02:23.080]   I think export controls are kneecapping the amount of compute
[01:02:23.080 --> 01:02:25.880]   or the density of compute that China can have.
[01:02:25.880 --> 01:02:29.320]   And if you think about the AI ecosystem right now,
[01:02:29.320 --> 01:02:31.120]   as all of these AI companies,
[01:02:31.120 --> 01:02:32.800]   revenue numbers are up into the right.
[01:02:32.800 --> 01:02:35.200]   AI usage is just continuing to grow.
[01:02:35.200 --> 01:02:36.680]   More GPUs are going to inference.
[01:02:36.680 --> 01:02:39.400]   A large part of export controls,
[01:02:39.400 --> 01:02:42.600]   if they work is just that the amount of AI
[01:02:42.600 --> 01:02:45.560]   that can be run in China is going to be much lower.
[01:02:45.560 --> 01:02:48.040]   So on the training side, DeepSeek v3 is a great example,
[01:02:48.040 --> 01:02:49.600]   which you have a very focused team
[01:02:49.600 --> 01:02:52.120]   that can still get to the frontier of AI.
[01:02:52.120 --> 01:02:54.760]   This 2000 GPUs is not that hard to get
[01:02:54.760 --> 01:02:56.920]   all considering in the world.
[01:02:56.920 --> 01:02:58.640]   They're still gonna have those GPUs.
[01:02:58.640 --> 01:03:00.100]   They're still gonna be able to train models.
[01:03:00.100 --> 01:03:02.200]   But if there's gonna be a huge market for AI,
[01:03:02.200 --> 01:03:03.640]   if you have strong export controls
[01:03:03.640 --> 01:03:05.280]   and you want to have 100,000 GPUs
[01:03:05.280 --> 01:03:08.160]   just serving the equivalent of Chad GPT clusters,
[01:03:08.160 --> 01:03:10.000]   with good export controls, it also just makes it
[01:03:10.000 --> 01:03:13.720]   so that AI can be used much less.
[01:03:13.720 --> 01:03:17.920]   And I think that is a much easier goal to achieve
[01:03:17.920 --> 01:03:20.160]   than trying to debate on what AGI is.
[01:03:20.160 --> 01:03:22.720]   And if you have these extremely intelligent,
[01:03:22.720 --> 01:03:24.640]   autonomous AIs and data centers,
[01:03:24.640 --> 01:03:26.640]   like those are the things that could be running
[01:03:26.640 --> 01:03:29.220]   in these GPU clusters in the United States,
[01:03:29.220 --> 01:03:30.160]   but not in China.
[01:03:30.160 --> 01:03:31.920]   - To some extent, training a model
[01:03:31.920 --> 01:03:33.160]   does effectively nothing, right?
[01:03:33.160 --> 01:03:34.800]   Like you have a model.
[01:03:34.800 --> 01:03:37.760]   The thing that Dario is sort of speaking to
[01:03:37.760 --> 01:03:40.480]   is the implementation of that model,
[01:03:40.480 --> 01:03:44.000]   once trained to then create huge economic growth,
[01:03:44.000 --> 01:03:46.240]   huge increases in military capabilities,
[01:03:46.240 --> 01:03:49.320]   huge increases in productivity of people,
[01:03:49.320 --> 01:03:51.800]   betterment of lives, whatever you wanna direct
[01:03:51.800 --> 01:03:53.960]   super powerful AI towards, you can't.
[01:03:53.960 --> 01:03:56.880]   But that requires a significant amounts of compute, right?
[01:03:56.880 --> 01:03:59.840]   And so the US government has effectively said,
[01:03:59.840 --> 01:04:01.400]   - And forever, right?
[01:04:01.400 --> 01:04:03.520]   Like training will always be a portion
[01:04:03.520 --> 01:04:05.160]   of the total compute.
[01:04:05.160 --> 01:04:07.240]   We mentioned Meta's 400,000 GPUs,
[01:04:07.240 --> 01:04:09.120]   only 16,000 made Llama, right?
[01:04:09.120 --> 01:04:12.000]   So the percentage that Meta is dedicating to inference,
[01:04:12.000 --> 01:04:14.320]   now this might be for recommendation systems
[01:04:14.320 --> 01:04:16.400]   that are trying to hack our mind into spending more time
[01:04:16.400 --> 01:04:17.520]   and watching more ads,
[01:04:17.520 --> 01:04:20.480]   or if it's for a super powerful AI
[01:04:20.480 --> 01:04:21.600]   that's doing productive things,
[01:04:21.600 --> 01:04:23.060]   doesn't matter about the exact use
[01:04:23.060 --> 01:04:25.120]   that our economic system decides,
[01:04:25.120 --> 01:04:28.500]   it's that that can be delivered in whatever way we want.
[01:04:28.500 --> 01:04:32.540]   Whereas with China, expert restrictions, great,
[01:04:32.540 --> 01:04:35.040]   you're never gonna be able to cut everything off, right?
[01:04:35.040 --> 01:04:37.080]   And I think that's quite well understood
[01:04:37.080 --> 01:04:38.320]   by the US government,
[01:04:38.320 --> 01:04:40.880]   is that you can't cut everything off.
[01:04:40.880 --> 01:04:42.480]   - They'll make their own chips.
[01:04:42.480 --> 01:04:43.440]   - And they're trying to make their own chips,
[01:04:43.440 --> 01:04:44.260]   they'll be worse than ours,
[01:04:44.260 --> 01:04:47.080]   but the whole point is to just keep a gap, right?
[01:04:47.080 --> 01:04:49.740]   And therefore at some point as the AI,
[01:04:49.740 --> 01:04:51.760]   in a world where two, 3% economic growth,
[01:04:51.760 --> 01:04:53.480]   this is really dumb by the way, right?
[01:04:53.480 --> 01:04:56.880]   To cut off high tech and not make money off of it,
[01:04:56.880 --> 01:04:59.740]   but in a world where super powerful AI comes about
[01:04:59.740 --> 01:05:02.600]   and then starts creating significant changes in society,
[01:05:02.600 --> 01:05:04.020]   which is what all the AI leaders
[01:05:04.020 --> 01:05:05.220]   and big tech companies believe,
[01:05:05.220 --> 01:05:06.660]   I think super powerful AI
[01:05:06.660 --> 01:05:08.420]   is gonna change society massively.
[01:05:08.420 --> 01:05:10.300]   And therefore this compounding effect
[01:05:10.300 --> 01:05:12.580]   of the difference in compute is really important.
[01:05:12.580 --> 01:05:14.180]   There's some sci-fi out there
[01:05:14.180 --> 01:05:17.660]   where like AI is like measured in the power of,
[01:05:17.660 --> 01:05:19.620]   in like how much power is delivered to compute, right?
[01:05:19.620 --> 01:05:21.460]   Or how much is being,
[01:05:21.460 --> 01:05:22.660]   that's sort of a way of thinking
[01:05:22.660 --> 01:05:23.860]   about what's the economic output
[01:05:23.860 --> 01:05:26.620]   is just how much power you directing towards that AI.
[01:05:26.620 --> 01:05:28.280]   - Should we talk about reasoning models with this
[01:05:28.280 --> 01:05:30.520]   as a way that this might be actionable
[01:05:30.520 --> 01:05:32.000]   as something that people can actually see?
[01:05:32.000 --> 01:05:34.940]   So the reasoning models that are coming out with R1 and O1,
[01:05:34.940 --> 01:05:37.400]   they're designed to use more compute.
[01:05:37.400 --> 01:05:40.720]   There's a lot of buzzy words in the AI community
[01:05:40.720 --> 01:05:43.680]   about this test time compute, inference time compute,
[01:05:43.680 --> 01:05:46.000]   whatever, but Dylan has good research on this.
[01:05:46.000 --> 01:05:47.200]   You can get to those specific numbers
[01:05:47.200 --> 01:05:48.880]   on the ratio of when you train a model,
[01:05:48.880 --> 01:05:50.880]   you can look at things about the amount of compute
[01:05:50.880 --> 01:05:53.200]   used at training and amount of compute used at inference.
[01:05:53.200 --> 01:05:54.880]   These reasoning models are making inference
[01:05:54.880 --> 01:05:58.000]   way more important to doing complex tasks.
[01:05:58.000 --> 01:06:00.320]   In the fall, in December,
[01:06:00.320 --> 01:06:02.520]   their OpenAI announced this O3 model.
[01:06:02.520 --> 01:06:04.200]   There's another thing in AI when things move fast,
[01:06:04.200 --> 01:06:06.080]   we get both announcements and releases.
[01:06:06.080 --> 01:06:07.640]   Announcements are essentially blog posts
[01:06:07.640 --> 01:06:09.040]   where you pat yourself on the back
[01:06:09.040 --> 01:06:09.980]   and you say you did things
[01:06:09.980 --> 01:06:11.600]   and releases are on the models out there,
[01:06:11.600 --> 01:06:12.880]   the papers out there, et cetera.
[01:06:12.880 --> 01:06:14.680]   So OpenAI has announced O3
[01:06:14.680 --> 01:06:17.080]   and we can check if O3 mini is out
[01:06:17.080 --> 01:06:18.800]   as a recording potentially,
[01:06:18.800 --> 01:06:20.280]   but that doesn't really change the point,
[01:06:20.280 --> 01:06:22.520]   which is that the breakthrough result
[01:06:22.520 --> 01:06:24.640]   was something called ArcAGI task,
[01:06:24.640 --> 01:06:26.960]   which is the abstract reasoning corpus,
[01:06:26.960 --> 01:06:29.940]   a task for artificial general intelligence.
[01:06:29.940 --> 01:06:33.360]   Francois Chollet is the guy who's been,
[01:06:33.360 --> 01:06:36.000]   it's a multi-year old paper, it's a brilliant benchmark.
[01:06:36.000 --> 01:06:39.320]   And the number for OpenAI O3 to solve this
[01:06:39.320 --> 01:06:42.940]   was that it used some sort of number of samples in the API.
[01:06:42.940 --> 01:06:45.980]   The API has like thinking effort and number of samples.
[01:06:45.980 --> 01:06:48.480]   They used a thousand samples to solve this task
[01:06:48.480 --> 01:06:53.360]   and it comes out to be like five to $20 per question,
[01:06:53.360 --> 01:06:55.720]   which you're putting in as effectively a math puzzle.
[01:06:55.720 --> 01:06:59.040]   And then it takes orders of dollars to answer one question.
[01:06:59.040 --> 01:07:00.800]   And this is a lot of compute.
[01:07:00.800 --> 01:07:02.040]   If those are gonna take off in the US,
[01:07:02.040 --> 01:07:04.860]   OpenAI needs a ton of GPUs on inference to capture this.
[01:07:04.860 --> 01:07:08.320]   They have this OpenAI chat GPT pro subscription,
[01:07:08.320 --> 01:07:09.560]   which is $200 a month.
[01:07:09.560 --> 01:07:11.200]   - Which Sam said they're losing money on.
[01:07:11.200 --> 01:07:12.380]   - Which means that people are burning
[01:07:12.380 --> 01:07:14.240]   a lot of GPUs on inference.
[01:07:14.240 --> 01:07:16.160]   And I've signed up with it, I've played with it.
[01:07:16.160 --> 01:07:18.480]   I don't think I'm a power user, but I use it.
[01:07:18.480 --> 01:07:21.960]   And it's like, that is the thing that a Chinese company
[01:07:21.960 --> 01:07:24.560]   with mediumly strong expert controls,
[01:07:24.560 --> 01:07:25.860]   there will always be loopholes,
[01:07:25.860 --> 01:07:27.400]   might not be able to do it all.
[01:07:27.400 --> 01:07:29.880]   And if that, the main result for O3
[01:07:29.880 --> 01:07:32.240]   is also a spectacular coding performance.
[01:07:32.240 --> 01:07:34.960]   And if that feeds back into AI companies
[01:07:34.960 --> 01:07:37.600]   being able to experiment better.
[01:07:37.600 --> 01:07:41.240]   - So presumably the idea is for an AGI,
[01:07:41.240 --> 01:07:43.800]   a much larger fraction of the compute would be used
[01:07:43.800 --> 01:07:46.320]   for this test time compute for the reasoning.
[01:07:46.320 --> 01:07:48.520]   For the AGI goes into a room and thinks about
[01:07:48.520 --> 01:07:51.760]   how to take over the world and come back
[01:07:51.760 --> 01:07:56.480]   in 2.7 hours and that it's gonna take a lot of compute.
[01:07:56.480 --> 01:08:00.240]   - This is what people like CEO or leaders of OpenAI
[01:08:00.240 --> 01:08:03.160]   and Anthropic talk about is like autonomous AI models,
[01:08:03.160 --> 01:08:05.080]   which is you give them a task and they work on it
[01:08:05.080 --> 01:08:05.920]   in the background.
[01:08:05.920 --> 01:08:10.080]   I think my personal definition of AGI is much simpler.
[01:08:10.080 --> 01:08:12.320]   Like I think language models are a form of AGI
[01:08:12.320 --> 01:08:15.040]   and all of this super powerful stuff is a next step.
[01:08:15.040 --> 01:08:16.560]   That's great if we get these tools,
[01:08:16.560 --> 01:08:19.560]   but a language model has so much value in so many domains.
[01:08:19.560 --> 01:08:21.500]   It is a general intelligence to me.
[01:08:21.500 --> 01:08:23.820]   But this next step of agentic things
[01:08:23.820 --> 01:08:25.880]   where they're independent and they can do tasks
[01:08:25.880 --> 01:08:27.680]   that aren't in the training data
[01:08:27.680 --> 01:08:30.480]   is what the few year outlook
[01:08:30.480 --> 01:08:32.840]   that these AI companies are driving for.
[01:08:32.840 --> 01:08:35.840]   - I think the terminology here that Dario uses
[01:08:35.840 --> 01:08:37.380]   is super powerful AI.
[01:08:37.380 --> 01:08:39.240]   So I agree with you on the AGI.
[01:08:39.240 --> 01:08:40.840]   I think we already have something like
[01:08:40.840 --> 01:08:43.160]   that's exceptionally impressive that Alan Turing
[01:08:43.160 --> 01:08:45.280]   would for sure say is AGI.
[01:08:45.280 --> 01:08:49.840]   But he's referring more to something once in possession of
[01:08:49.840 --> 01:08:52.060]   then you would have a significant military
[01:08:52.060 --> 01:08:54.540]   and geopolitical advantage over other nations.
[01:08:54.540 --> 01:08:58.380]   So it's not just like you can ask it how to cook an omelet.
[01:08:58.380 --> 01:08:59.820]   - And he has a much more positive view
[01:08:59.820 --> 01:09:01.580]   in his essay, "Machines of Love and Grace."
[01:09:01.580 --> 01:09:04.180]   I read into this that we don't have enough background
[01:09:04.180 --> 01:09:07.420]   in physical sciences to gauge exactly how competent I am
[01:09:07.420 --> 01:09:10.020]   and if AI can revolutionize biology.
[01:09:10.020 --> 01:09:14.660]   I'm safe saying that AI is going to accelerate the progress
[01:09:14.660 --> 01:09:16.100]   of any computational science.
[01:09:16.100 --> 01:09:18.780]   - So we're doing a depth first search here on topics,
[01:09:18.780 --> 01:09:21.160]   taking tangent of a tangent, so let's continue
[01:09:21.160 --> 01:09:22.900]   on that depth first search.
[01:09:22.900 --> 01:09:27.900]   You said that you're both feeling the AGI.
[01:09:27.900 --> 01:09:30.560]   So what's your timeline?
[01:09:30.560 --> 01:09:34.160]   Dario's 2026 for the super powerful AI
[01:09:34.160 --> 01:09:38.140]   that's basically agentic to a degree where
[01:09:38.140 --> 01:09:43.140]   it's a real security threat, that level of AGI.
[01:09:43.140 --> 01:09:44.720]   What's your timeline?
[01:09:44.720 --> 01:09:47.040]   - I don't like to attribute specific abilities
[01:09:47.040 --> 01:09:48.680]   because predicting specific abilities
[01:09:48.680 --> 01:09:49.720]   and when is very hard.
[01:09:49.720 --> 01:09:51.600]   I think mostly if you're going to say that
[01:09:51.600 --> 01:09:54.800]   I'm feeling the AGI is that I expect continued,
[01:09:54.800 --> 01:09:57.480]   rapid, surprising progress over the next few years.
[01:09:57.480 --> 01:10:01.460]   So something like R1 is less surprising to me from DeepSeek
[01:10:01.460 --> 01:10:03.760]   because I expect there to be new paradigms
[01:10:03.760 --> 01:10:05.720]   where substantial progress can be made.
[01:10:05.720 --> 01:10:07.760]   I think DeepSeek R1 is so unsettling
[01:10:07.760 --> 01:10:10.520]   because we're kind of on this path with chat GPT.
[01:10:10.520 --> 01:10:11.800]   It's like, it's getting better, it's getting better,
[01:10:11.800 --> 01:10:12.640]   it's getting better.
[01:10:12.640 --> 01:10:15.140]   And then we have a new direction for changing the models
[01:10:15.140 --> 01:10:16.520]   and we took one step like this
[01:10:16.520 --> 01:10:17.680]   and we like took a step up.
[01:10:17.680 --> 01:10:19.560]   So it looks like a really fast slope
[01:10:19.560 --> 01:10:21.100]   and then we're going to just take more steps.
[01:10:21.100 --> 01:10:22.360]   So like, it's just really unsettling
[01:10:22.360 --> 01:10:23.920]   when you have these big steps
[01:10:23.920 --> 01:10:26.100]   and I expect that to keep happening.
[01:10:26.100 --> 01:10:29.280]   I see, I've tried OpenAI Operator,
[01:10:29.280 --> 01:10:31.320]   I've tried Cloud computer use.
[01:10:31.320 --> 01:10:32.280]   They're not there yet.
[01:10:32.280 --> 01:10:35.640]   I understand the idea, but it's just so hard to predict
[01:10:35.640 --> 01:10:36.520]   what is the breakthrough
[01:10:36.520 --> 01:10:37.880]   that'll make something like that work.
[01:10:37.880 --> 01:10:40.840]   And I think it's more likely that we have breakthroughs
[01:10:40.840 --> 01:10:41.960]   that work and things that we don't know
[01:10:41.960 --> 01:10:42.940]   what they're going to do.
[01:10:42.940 --> 01:10:44.700]   So everyone wants agents.
[01:10:44.700 --> 01:10:48.480]   Dario has very eloquent way of describing this.
[01:10:48.480 --> 01:10:50.220]   And I just think that it's like,
[01:10:50.220 --> 01:10:51.320]   there's going to be more than that.
[01:10:51.320 --> 01:10:53.920]   So like, just expect these things to come.
[01:10:53.920 --> 01:10:56.300]   - I'm going to have to try to pin you down to a date
[01:10:56.300 --> 01:10:57.880]   on the AGI timeline.
[01:10:57.880 --> 01:11:02.220]   Like the nuclear weapon moment.
[01:11:02.220 --> 01:11:05.560]   So moment where on the geopolitical stage,
[01:11:05.560 --> 01:11:09.340]   there's a real like, you know,
[01:11:09.340 --> 01:11:11.580]   'cause we're talking about export controls.
[01:11:11.580 --> 01:11:14.640]   When do you think, just even to throw out a date,
[01:11:14.640 --> 01:11:15.880]   when do you think that would be?
[01:11:15.880 --> 01:11:18.760]   Like for me, it's probably after 2030.
[01:11:18.760 --> 01:11:19.600]   So I'm not as-
[01:11:19.600 --> 01:11:20.640]   - That's what I would say.
[01:11:20.640 --> 01:11:21.880]   - So define that, right?
[01:11:21.880 --> 01:11:24.320]   Because to me, it kind of almost has already happened,
[01:11:24.320 --> 01:11:25.160]   right?
[01:11:25.160 --> 01:11:26.560]   You look at elections in India and Pakistan,
[01:11:26.560 --> 01:11:28.160]   people get AI voice calls
[01:11:28.160 --> 01:11:31.120]   and think they're talking to the politician, right?
[01:11:31.120 --> 01:11:32.400]   The AI diffusion rules,
[01:11:32.400 --> 01:11:33.820]   which was enacted in the last couple of weeks
[01:11:33.820 --> 01:11:34.660]   of the Biden admin,
[01:11:34.660 --> 01:11:36.440]   and it looks like the Trump admin will keep
[01:11:36.440 --> 01:11:38.160]   and potentially even strengthen,
[01:11:38.160 --> 01:11:41.180]   limit cloud computing and GPU sales
[01:11:41.180 --> 01:11:43.580]   to countries that are not even related to China.
[01:11:43.580 --> 01:11:44.620]   It's like, this is-
[01:11:44.620 --> 01:11:47.300]   - Portugal and all these like normal countries
[01:11:47.300 --> 01:11:49.660]   are on the, you need approval from the US list.
[01:11:49.660 --> 01:11:51.180]   - Like, yeah, Portugal and like, you know,
[01:11:51.180 --> 01:11:53.660]   like all these countries that are allies, right?
[01:11:53.660 --> 01:11:54.500]   Singapore, right?
[01:11:54.500 --> 01:11:56.500]   Like they freaking have F-35s
[01:11:56.500 --> 01:11:58.020]   and we don't let them buy GPUs.
[01:11:58.020 --> 01:12:01.420]   Like this is, this to me is already to the scale of like,
[01:12:01.420 --> 01:12:02.260]   you know.
[01:12:02.260 --> 01:12:05.100]   - Well, that just means that the US military
[01:12:05.100 --> 01:12:06.860]   is really nervous about this new technology.
[01:12:06.860 --> 01:12:09.480]   That doesn't mean the technology is already there.
[01:12:09.480 --> 01:12:12.440]   So like, they might be just very cautious
[01:12:12.440 --> 01:12:14.360]   about this thing that they don't quite understand.
[01:12:14.360 --> 01:12:15.880]   That's a really good point.
[01:12:15.880 --> 01:12:18.160]   Sort of the robocalls,
[01:12:18.160 --> 01:12:23.160]   swarms of semi-intelligent bots could be a weapon,
[01:12:23.160 --> 01:12:25.560]   could be doing a lot of social engineering.
[01:12:25.560 --> 01:12:27.060]   - I mean, there's tons of talk about, you know,
[01:12:27.060 --> 01:12:29.680]   from the 2016 elections, like Cambridge Analytica
[01:12:29.680 --> 01:12:31.600]   and all this stuff, Russian influence.
[01:12:31.600 --> 01:12:33.160]   I mean, every country in the world
[01:12:33.160 --> 01:12:34.640]   is pushing stuff onto the internet
[01:12:34.640 --> 01:12:35.960]   and has narratives they want, right?
[01:12:35.960 --> 01:12:38.420]   Like that's, every like technically competent,
[01:12:38.420 --> 01:12:41.620]   whether it's Russia, China, US, Israel, et cetera, right?
[01:12:41.620 --> 01:12:43.100]   You know, people are pushing viewpoints
[01:12:43.100 --> 01:12:44.940]   onto the internet en masse.
[01:12:44.940 --> 01:12:46.940]   And language models crash the cost
[01:12:46.940 --> 01:12:48.980]   of like very intelligent sounding language.
[01:12:48.980 --> 01:12:51.600]   - There's some research that shows that the distribution
[01:12:51.600 --> 01:12:52.980]   is actually the limiting factor.
[01:12:52.980 --> 01:12:56.780]   So language models haven't yet made misinformation
[01:12:56.780 --> 01:13:00.660]   particularly, like change the equation there.
[01:13:00.660 --> 01:13:01.940]   The internet is still ongoing.
[01:13:01.940 --> 01:13:03.780]   I think there's a blog, AI Snake Oil
[01:13:03.780 --> 01:13:05.100]   and some of my friends at Princeton
[01:13:05.100 --> 01:13:05.940]   that write on this stuff.
[01:13:05.940 --> 01:13:06.920]   So there is research.
[01:13:06.920 --> 01:13:08.700]   It's like, it's a default that everyone assumes.
[01:13:08.700 --> 01:13:09.900]   And I would have thought the same thing
[01:13:09.900 --> 01:13:12.300]   is that misinformation is gonna get far worse
[01:13:12.300 --> 01:13:13.140]   with language models.
[01:13:13.140 --> 01:13:15.860]   I think in terms of internet posts
[01:13:15.860 --> 01:13:17.700]   and things that people have been measuring,
[01:13:17.700 --> 01:13:20.020]   it hasn't been a exponential increase
[01:13:20.020 --> 01:13:21.260]   or something extremely measurable.
[01:13:21.260 --> 01:13:23.300]   And things you're talking about with like voice calls
[01:13:23.300 --> 01:13:25.820]   and stuff like that, it could be in modalities
[01:13:25.820 --> 01:13:27.580]   that are harder to measure.
[01:13:27.580 --> 01:13:30.220]   So it's something that it's too soon to tell
[01:13:30.220 --> 01:13:33.820]   in terms of, I think that's like political instability
[01:13:33.820 --> 01:13:37.620]   via the web is very, it's monitored
[01:13:37.620 --> 01:13:40.220]   by a lot of researchers to see what's happening.
[01:13:40.220 --> 01:13:43.300]   I think that you're asking about like the AGI thing.
[01:13:43.300 --> 01:13:46.300]   I might, if you make me give a year,
[01:13:46.300 --> 01:13:48.540]   I wouldn't be like, okay, I have AI CEOs saying this.
[01:13:48.540 --> 01:13:50.500]   They've been saying two years for a while.
[01:13:50.500 --> 01:13:54.780]   I think that they're people like Dario Anthropic,
[01:13:54.780 --> 01:13:56.900]   the CEO had thought about this so deeply.
[01:13:56.900 --> 01:13:59.780]   I need to take their word seriously,
[01:13:59.780 --> 01:14:03.580]   but also understand that they have different incentives.
[01:14:03.580 --> 01:14:05.220]   So I would be like, add a few years to that,
[01:14:05.220 --> 01:14:07.140]   which is how you get something similar to 2030
[01:14:07.140 --> 01:14:08.380]   or a little after 2030.
[01:14:08.380 --> 01:14:10.500]   - I think to some extent we have capabilities
[01:14:10.500 --> 01:14:13.820]   that hit a certain point where any one person could say,
[01:14:13.820 --> 01:14:15.980]   oh, okay, if I can leverage those capabilities
[01:14:15.980 --> 01:14:18.620]   for X amount of time, this is AGI, right?
[01:14:18.620 --> 01:14:20.180]   Call it 27, 28.
[01:14:20.180 --> 01:14:23.140]   But then the cost of actually operating that capability.
[01:14:23.140 --> 01:14:24.300]   - Yeah, this is gonna be my point.
[01:14:24.300 --> 01:14:27.700]   - So, so extreme that no one can actually deploy it
[01:14:27.700 --> 01:14:31.340]   at scale and mass to actually completely revolutionize
[01:14:31.340 --> 01:14:33.260]   the economy on a snap of a finger.
[01:14:33.260 --> 01:14:34.940]   So I don't think it will be like a snap
[01:14:34.940 --> 01:14:35.780]   of the finger moment.
[01:14:35.780 --> 01:14:36.620]   - It's a physical constraint.
[01:14:36.620 --> 01:14:39.740]   - Rather it'll be a, oh, the capabilities are here,
[01:14:39.740 --> 01:14:41.300]   but I can't deploy it everywhere, right?
[01:14:41.300 --> 01:14:45.460]   And so one simple example going back sort of to 2023
[01:14:45.460 --> 01:14:48.940]   was when being with GPT-4 came out
[01:14:48.940 --> 01:14:50.940]   and everyone was freaking out about search, right?
[01:14:50.940 --> 01:14:51.940]   Perplexity came out.
[01:14:51.940 --> 01:14:53.300]   If you did the cost on like, hey,
[01:14:53.300 --> 01:14:55.940]   implementing GPT-3 into every Google search,
[01:14:55.940 --> 01:14:57.460]   it was like, oh, okay, this is just like physically
[01:14:57.460 --> 01:14:58.940]   impossible to implement, right?
[01:14:58.940 --> 01:15:01.220]   And as we step forward to like going back
[01:15:01.220 --> 01:15:03.140]   to the test time compute thing, right?
[01:15:03.140 --> 01:15:06.460]   A query for, you know, you asked ChatGPT a question,
[01:15:06.460 --> 01:15:08.140]   it costs cents, right?
[01:15:08.140 --> 01:15:11.060]   For their most capable model of chat, right?
[01:15:11.060 --> 01:15:15.180]   To get a query back, to solve an Arc AGI problem though,
[01:15:15.180 --> 01:15:17.300]   cost five to 20 bucks, right?
[01:15:17.300 --> 01:15:18.140]   And this is an--
[01:15:18.140 --> 01:15:19.700]   - It's only going up from there.
[01:15:19.700 --> 01:15:22.940]   - This is a thousand, 10,000 X factor difference in cost
[01:15:22.940 --> 01:15:26.180]   to respond to a query versus do a task.
[01:15:26.180 --> 01:15:28.820]   And the task of Arc AGI, it's not like it's like,
[01:15:28.820 --> 01:15:31.900]   it's simple to some extent, you know,
[01:15:31.900 --> 01:15:34.660]   but it's also like, what are the tasks that we want?
[01:15:34.660 --> 01:15:37.700]   Okay, AGI, quote unquote, what we have today can do Arc AGI.
[01:15:37.700 --> 01:15:38.540]   Three years from now,
[01:15:38.540 --> 01:15:40.460]   it can do much more complicated problems,
[01:15:40.460 --> 01:15:43.020]   but the cost is gonna be measured in thousands and thousands
[01:15:43.020 --> 01:15:45.940]   and hundreds of thousands of dollars of GPU time.
[01:15:45.940 --> 01:15:47.980]   And there just won't be enough power, GPUs,
[01:15:47.980 --> 01:15:50.620]   infrastructure to operate this and therefore shift
[01:15:50.620 --> 01:15:52.340]   everything in the world on the snap of the finger.
[01:15:52.340 --> 01:15:55.940]   But at that moment, who gets to control
[01:15:55.940 --> 01:15:58.820]   and point the AGI at a task?
[01:15:58.820 --> 01:16:01.020]   And so this was in Dario's post that he's like,
[01:16:01.020 --> 01:16:04.300]   hey, China can effectively and more quickly than us
[01:16:04.300 --> 01:16:07.380]   point their AGI at military tasks, right?
[01:16:07.380 --> 01:16:09.900]   And they have been in many ways faster at adopting
[01:16:09.900 --> 01:16:13.420]   certain new technologies into their military, right?
[01:16:13.420 --> 01:16:15.500]   Especially with regards to drones, right?
[01:16:15.500 --> 01:16:17.740]   The U.S. maybe has a longstanding, you know,
[01:16:17.740 --> 01:16:21.100]   large air sort of, you know, fighter jet type of thing,
[01:16:21.100 --> 01:16:23.500]   bombers, but when it comes to asymmetric arms,
[01:16:23.500 --> 01:16:26.940]   such as drones, they've completely leapfrogged
[01:16:26.940 --> 01:16:28.140]   the U.S. and the West.
[01:16:28.140 --> 01:16:31.020]   And the fear that Dario is sort of pointing out there,
[01:16:31.020 --> 01:16:33.860]   I think, is that, yeah, great, we'll have AGI
[01:16:33.860 --> 01:16:35.740]   in the commercial sector.
[01:16:35.740 --> 01:16:38.340]   The U.S. military won't be able to implement it super fast.
[01:16:38.340 --> 01:16:40.100]   Chinese military could, and they could direct
[01:16:40.100 --> 01:16:42.740]   all their resources to implementing it in the military
[01:16:42.740 --> 01:16:45.700]   and therefore solving, you know, military logistics
[01:16:45.700 --> 01:16:48.500]   or solving some other aspect of like disinformation
[01:16:48.500 --> 01:16:50.540]   for targeted certain set of people
[01:16:50.540 --> 01:16:52.100]   so that they can flip a country's politics
[01:16:52.100 --> 01:16:55.220]   or something like that that is actually like catastrophic
[01:16:55.220 --> 01:16:57.700]   versus, you know, the U.S. just wants to, you know,
[01:16:57.700 --> 01:16:59.460]   'cause it'll be more capitalistically allocated
[01:16:59.460 --> 01:17:01.820]   just towards whatever is the highest return on income,
[01:17:01.820 --> 01:17:03.500]   which might be like building, you know,
[01:17:03.500 --> 01:17:04.740]   factories better or whatever.
[01:17:04.740 --> 01:17:07.700]   - So everything I've seen, people's intuition
[01:17:07.700 --> 01:17:09.900]   seems to fail on robotics.
[01:17:09.900 --> 01:17:11.860]   So you have this kind of general optimism.
[01:17:11.860 --> 01:17:13.460]   I've seen this on self-driving cars.
[01:17:13.460 --> 01:17:16.500]   People think it's a much easier problem than it is.
[01:17:16.500 --> 01:17:18.300]   Similar with drones.
[01:17:18.300 --> 01:17:20.260]   Here, I understand it a little bit less,
[01:17:20.260 --> 01:17:23.900]   but I've just seen the reality of the war in Ukraine
[01:17:23.900 --> 01:17:26.780]   and the usage of drones on both sides.
[01:17:26.780 --> 01:17:31.380]   And it seems that humans still far outperform
[01:17:31.380 --> 01:17:34.060]   any fully autonomous systems.
[01:17:34.060 --> 01:17:37.940]   AI is an assistant, but humans drive.
[01:17:37.940 --> 01:17:40.540]   FPV drones, where the human's controlling most of it,
[01:17:40.540 --> 01:17:43.580]   just far, far, far outperforms AI systems.
[01:17:43.580 --> 01:17:45.580]   So I think it's not obvious to me
[01:17:45.580 --> 01:17:48.740]   that we're going to have swarms of autonomous robots
[01:17:48.740 --> 01:17:51.380]   anytime soon in the military context.
[01:17:51.380 --> 01:17:54.780]   Maybe the fastest I can imagine is 2030,
[01:17:54.780 --> 01:17:58.500]   which is why I said 2030 for the super powerful AI.
[01:17:58.500 --> 01:18:02.500]   Whenever you have large-scale swarms of robots
[01:18:02.500 --> 01:18:04.780]   doing military actions, that's when the world
[01:18:04.780 --> 01:18:07.620]   just starts to look different to me.
[01:18:07.620 --> 01:18:09.580]   So that's the thing I'm really worried about.
[01:18:09.580 --> 01:18:11.220]   But there could be cyber war,
[01:18:11.220 --> 01:18:14.660]   cyber war type of technologies that,
[01:18:14.660 --> 01:18:18.060]   from social engineering to actually just swarms of robots
[01:18:18.060 --> 01:18:21.500]   that find attack vectors in our code bases
[01:18:21.500 --> 01:18:23.820]   and shut down power grids, that kind of stuff.
[01:18:24.700 --> 01:18:25.820]   And it could be one of those things
[01:18:25.820 --> 01:18:29.900]   like on any given weekend or something, power goes out,
[01:18:29.900 --> 01:18:33.420]   nobody knows why, and the world changes forever.
[01:18:33.420 --> 01:18:35.940]   Just power going out for two days
[01:18:35.940 --> 01:18:37.720]   in all of the United States,
[01:18:37.720 --> 01:18:40.860]   that will lead to murder, to chaos.
[01:18:40.860 --> 01:18:45.140]   But going back to expert controls,
[01:18:45.140 --> 01:18:47.820]   do you see that as a useful way
[01:18:47.820 --> 01:18:53.800]   to control the balance of power geopolitically
[01:18:54.580 --> 01:18:56.060]   in the context of AI?
[01:18:56.060 --> 01:18:58.140]   - And I think going back to my viewpoint is,
[01:18:58.140 --> 01:19:01.300]   if you believe we're in the sort of stage
[01:19:01.300 --> 01:19:03.300]   of economic growth and change
[01:19:03.300 --> 01:19:05.060]   that we've been in for the last 20 years,
[01:19:05.060 --> 01:19:08.660]   the export controls are absolutely guaranteeing
[01:19:08.660 --> 01:19:10.980]   that China will win long-term, right?
[01:19:10.980 --> 01:19:13.260]   If you do not believe AI is going to make
[01:19:13.260 --> 01:19:16.720]   significant changes to society in the next 10 years
[01:19:16.720 --> 01:19:18.100]   or five years, right?
[01:19:18.100 --> 01:19:20.900]   Five-year timelines are sort of what the more executives
[01:19:20.900 --> 01:19:23.940]   and such of AI companies and even big tech companies believe,
[01:19:23.940 --> 01:19:26.540]   but even 10-year timelines, it's reasonable.
[01:19:26.540 --> 01:19:30.060]   But once you get to, hey, these timelines
[01:19:30.060 --> 01:19:33.420]   are below that time period,
[01:19:33.420 --> 01:19:38.180]   then the only way to sort of create a sizable advantage
[01:19:38.180 --> 01:19:40.740]   or disadvantage for America versus China
[01:19:40.740 --> 01:19:43.460]   is if you constrain compute
[01:19:43.460 --> 01:19:46.540]   because talent is not really something
[01:19:46.540 --> 01:19:47.760]   that's constraining, right?
[01:19:47.760 --> 01:19:49.500]   China arguably has more talent, right?
[01:19:49.500 --> 01:19:51.820]   More STEM graduates, more programmers.
[01:19:51.820 --> 01:19:53.740]   The US can draw upon the world's people,
[01:19:53.740 --> 01:19:54.580]   which it does.
[01:19:54.580 --> 01:19:57.180]   There's tons of foreigners in the AI industry.
[01:19:57.180 --> 01:19:59.780]   - So many of these AI teams are all people
[01:19:59.780 --> 01:20:01.200]   without a US passport.
[01:20:01.200 --> 01:20:04.980]   - Yeah, I mean, many of them are Chinese people
[01:20:04.980 --> 01:20:06.300]   who are moving to America, right?
[01:20:06.300 --> 01:20:07.140]   And that's great.
[01:20:07.140 --> 01:20:08.940]   That's exactly what we want, right?
[01:20:08.940 --> 01:20:11.700]   But that talent is one aspect,
[01:20:11.700 --> 01:20:13.940]   but I don't think that's one that is a measurable advantage
[01:20:13.940 --> 01:20:15.100]   for the US or not.
[01:20:15.100 --> 01:20:18.000]   It truly is just whether or not compute, right?
[01:20:18.000 --> 01:20:19.880]   Now, even on the compute side,
[01:20:19.880 --> 01:20:22.540]   when we look at chips versus data centers, right?
[01:20:22.540 --> 01:20:25.420]   China has the unprecedented ability
[01:20:25.420 --> 01:20:29.620]   to build ridiculous sums of power clockwork, right?
[01:20:29.620 --> 01:20:31.780]   They're always building more and more power.
[01:20:31.780 --> 01:20:35.720]   They've got steel mills that like individually
[01:20:35.720 --> 01:20:37.580]   are the size of the entire US industry, right?
[01:20:37.580 --> 01:20:38.820]   And they've got aluminum mills
[01:20:38.820 --> 01:20:41.900]   that consume gigawatts and gigawatts of power, right?
[01:20:41.900 --> 01:20:43.940]   And when we talk about what's the biggest data center,
[01:20:43.940 --> 01:20:44.780]   right?
[01:20:44.780 --> 01:20:46.900]   OpenAI made this huge thing about Stargate,
[01:20:46.900 --> 01:20:48.140]   their announcement there.
[01:20:48.140 --> 01:20:51.580]   That's like once it's fully built out in a few years,
[01:20:51.580 --> 01:20:53.280]   it'll be two gigawatts, right?
[01:20:53.280 --> 01:20:54.340]   Of power, right?
[01:20:54.340 --> 01:20:56.580]   And this is still smaller than the largest,
[01:20:56.580 --> 01:20:58.820]   you know, industrial facilities in China, right?
[01:20:58.820 --> 01:21:00.060]   China, if they wanted to build
[01:21:00.060 --> 01:21:01.620]   the largest data center in the world,
[01:21:01.620 --> 01:21:04.100]   if they had access to the chips, could.
[01:21:04.100 --> 01:21:07.620]   So it's just a question of when, not if, right?
[01:21:07.620 --> 01:21:10.560]   - So their industrial capacity far exceeds the United States?
[01:21:10.560 --> 01:21:11.400]   - Exactly.
[01:21:11.400 --> 01:21:12.980]   - To manufacture stuff.
[01:21:12.980 --> 01:21:13.880]   - Yeah.
[01:21:13.880 --> 01:21:15.260]   - So long-term,
[01:21:15.260 --> 01:21:18.500]   they're going to be manufacturing chips there.
[01:21:18.500 --> 01:21:20.380]   - Chips are a little bit more specialized.
[01:21:20.380 --> 01:21:22.260]   I'm specifically referring to the data centers, right?
[01:21:22.260 --> 01:21:24.180]   Chips, fabs take huge amounts of power.
[01:21:24.180 --> 01:21:25.420]   Don't get me wrong.
[01:21:25.420 --> 01:21:27.580]   That's not necessarily the gating factor there.
[01:21:27.580 --> 01:21:30.300]   The gating factor on how fast people can build
[01:21:30.300 --> 01:21:33.740]   the largest clusters today in the U.S. is power, right?
[01:21:33.740 --> 01:21:34.580]   It is, whether it's,
[01:21:34.580 --> 01:21:37.500]   now it could be power generation, power transmission,
[01:21:37.500 --> 01:21:39.780]   substations, and, you know,
[01:21:39.780 --> 01:21:42.460]   all these sorts of transformers and all these things,
[01:21:42.460 --> 01:21:43.300]   building the data center.
[01:21:43.300 --> 01:21:46.580]   These are all constraints on the U.S. industry's ability
[01:21:46.580 --> 01:21:49.140]   to build larger and larger training systems,
[01:21:49.140 --> 01:21:51.980]   as well as deploying more and more inference compute.
[01:21:51.980 --> 01:21:53.380]   - I think we need to make the point clear
[01:21:53.380 --> 01:21:55.900]   on why the time is now for people
[01:21:55.900 --> 01:21:57.180]   that don't think about this,
[01:21:57.180 --> 01:21:58.740]   'cause essentially with export controls,
[01:21:58.740 --> 01:22:00.420]   you're making it so China cannot make
[01:22:00.420 --> 01:22:02.900]   or get cutting edge chips.
[01:22:02.900 --> 01:22:06.180]   And the idea is that if you time this wrong,
[01:22:06.180 --> 01:22:09.100]   China is pouring a ton of money into their chip production.
[01:22:09.100 --> 01:22:10.100]   And if you time it wrong,
[01:22:10.100 --> 01:22:12.220]   they are going to have more capacity for production,
[01:22:12.220 --> 01:22:13.900]   more capacity for energy,
[01:22:13.900 --> 01:22:15.380]   and figure out how to make the chips,
[01:22:15.380 --> 01:22:17.300]   and have more capacity than the rest of the world
[01:22:17.300 --> 01:22:18.180]   to make the chips,
[01:22:18.180 --> 01:22:19.540]   because everybody can buy,
[01:22:19.540 --> 01:22:21.500]   they're going to sell their Chinese chips to everybody,
[01:22:21.500 --> 01:22:22.940]   they might subsidize them.
[01:22:22.940 --> 01:22:25.340]   And therefore, if AI takes a long time
[01:22:25.340 --> 01:22:26.500]   to become differentiated,
[01:22:26.500 --> 01:22:28.540]   we've kneecapped the financial performance
[01:22:28.540 --> 01:22:29.740]   of American companies.
[01:22:29.740 --> 01:22:31.620]   NVIDIA can sell less.
[01:22:31.620 --> 01:22:33.860]   TSMC cannot sell to China.
[01:22:33.860 --> 01:22:36.740]   So therefore, we have less demand
[01:22:36.740 --> 01:22:40.100]   to therefore keep driving the production cycle.
[01:22:40.100 --> 01:22:43.260]   So that's the assumption behind the timing being important.
[01:22:43.260 --> 01:22:45.900]   - Less than 10 years or five years to above, right?
[01:22:45.900 --> 01:22:49.020]   China will win because of these restrictions long-term,
[01:22:49.020 --> 01:22:51.780]   unless AI does something in the short term,
[01:22:51.780 --> 01:22:53.980]   which I believe AI will do,
[01:22:53.980 --> 01:22:55.420]   make massive changes to society
[01:22:55.420 --> 01:22:57.740]   in the medium short term, right?
[01:22:57.740 --> 01:22:59.980]   And so that's the big unlocker there.
[01:22:59.980 --> 01:23:02.100]   And even today, right?
[01:23:02.100 --> 01:23:04.540]   If Xi Jinping decided to get,
[01:23:04.540 --> 01:23:06.820]   quote unquote, scale-pilled, right?
[01:23:06.820 --> 01:23:10.700]   I.e. decide that scaling laws are what matters, right?
[01:23:10.700 --> 01:23:12.180]   Just like the US executives,
[01:23:12.180 --> 01:23:14.540]   like Satya Nadella, and Mark Zuckerberg,
[01:23:14.540 --> 01:23:17.300]   and Sundar, and all these US executives
[01:23:17.300 --> 01:23:19.820]   of the biggest, most powerful tech companies
[01:23:19.820 --> 01:23:21.060]   have decided they're scale-pilled,
[01:23:21.060 --> 01:23:23.500]   and they're building multi-gigawatt data centers, right?
[01:23:23.500 --> 01:23:25.860]   Whether it's in Texas, or Louisiana, or Wisconsin,
[01:23:25.860 --> 01:23:28.860]   wherever it is, they're building these massive things
[01:23:28.860 --> 01:23:31.700]   that cost as much as their entire budget
[01:23:31.700 --> 01:23:34.900]   for spending on data centers globally in one spot, right?
[01:23:34.900 --> 01:23:35.980]   This is what they've committed to
[01:23:35.980 --> 01:23:37.940]   for next year, year after, et cetera.
[01:23:37.940 --> 01:23:42.180]   And so they're so convinced that this is the way,
[01:23:42.180 --> 01:23:43.340]   that this is what they're doing.
[01:23:43.340 --> 01:23:46.460]   But if China decided to, they could do it faster than us,
[01:23:46.460 --> 01:23:48.940]   but this is where the restrictions come in.
[01:23:48.940 --> 01:23:52.420]   It is not clear that China as a whole has decided,
[01:23:52.420 --> 01:23:54.500]   you know, from the highest levels that this is a priority.
[01:23:54.500 --> 01:23:56.220]   The US sort of has, right?
[01:23:56.220 --> 01:23:57.980]   You know, you see Trump talking about DeepSeek
[01:23:57.980 --> 01:24:00.620]   and Stargate within the same week, right?
[01:24:00.620 --> 01:24:02.100]   So he's, and the Biden admin as well,
[01:24:02.100 --> 01:24:05.060]   had a lot of discussions about AI and such.
[01:24:05.060 --> 01:24:06.900]   It's clear that they think about it.
[01:24:06.900 --> 01:24:09.300]   Only just last week did DeepSeek
[01:24:09.300 --> 01:24:12.100]   meet the second-in-command of China, right?
[01:24:12.100 --> 01:24:13.860]   Like they have not even met the top, right?
[01:24:13.860 --> 01:24:14.780]   They haven't met Xi.
[01:24:14.780 --> 01:24:16.300]   Xi hasn't set down,
[01:24:16.300 --> 01:24:20.740]   and they only just released a subsidy of a trillion RMB,
[01:24:20.740 --> 01:24:23.300]   you know, roughly $160 billion,
[01:24:23.300 --> 01:24:27.100]   which is closer to the spending of like Microsoft and Meta
[01:24:27.100 --> 01:24:29.180]   and Google combined, right, for this year.
[01:24:29.180 --> 01:24:32.380]   So it's like, they're realizing it just now,
[01:24:32.380 --> 01:24:34.620]   but that's where these export restrictions come in
[01:24:34.620 --> 01:24:36.900]   and say, hey, you can't ship
[01:24:36.900 --> 01:24:39.140]   the most powerful US chips to China.
[01:24:39.140 --> 01:24:40.740]   You can ship a cut-down version.
[01:24:40.740 --> 01:24:44.060]   You can't ship the most powerful chips
[01:24:44.060 --> 01:24:44.940]   to all these countries
[01:24:44.940 --> 01:24:47.260]   who we know are just going to rent it to China.
[01:24:47.260 --> 01:24:48.540]   You have to limit the numbers, right?
[01:24:48.540 --> 01:24:49.900]   - And the tools.
[01:24:49.900 --> 01:24:52.220]   - And same with manufacturing equipment, tools,
[01:24:52.220 --> 01:24:53.980]   all these different aspects.
[01:24:53.980 --> 01:24:55.180]   But it all stems from AI,
[01:24:55.180 --> 01:24:58.300]   and then what downstream can slow them down in AI.
[01:24:58.300 --> 01:25:00.700]   And so the entire semiconductor restrictions,
[01:25:00.700 --> 01:25:02.460]   you read them, they are very clear.
[01:25:02.460 --> 01:25:05.860]   It's about AI and military-civil fusion of technology,
[01:25:05.860 --> 01:25:06.700]   right?
[01:25:06.700 --> 01:25:07.540]   It's very clear.
[01:25:07.540 --> 01:25:08.360]   And then from there it goes,
[01:25:08.360 --> 01:25:09.800]   oh, well, we're banning them from buying
[01:25:09.800 --> 01:25:12.900]   like lithography tools and etch tools and deposition tools.
[01:25:12.900 --> 01:25:14.740]   And oh, this random like, you know,
[01:25:14.740 --> 01:25:17.580]   subsystem from a random company that's like tiny, right?
[01:25:17.580 --> 01:25:18.500]   Like, why are we banning this?
[01:25:18.500 --> 01:25:21.060]   Because all of it, the US government has decided
[01:25:21.060 --> 01:25:23.540]   is critical to AI systems.
[01:25:23.540 --> 01:25:26.580]   - I think the fulcrum point is like the transition
[01:25:26.580 --> 01:25:28.540]   from seven nanometer to five nanometer chips,
[01:25:28.540 --> 01:25:29.740]   where I think it was Huawei
[01:25:29.740 --> 01:25:32.700]   that had the seven nanometer chip a few years ago,
[01:25:32.700 --> 01:25:35.460]   which caused another political brouhaha
[01:25:35.460 --> 01:25:36.980]   almost like this moment.
[01:25:36.980 --> 01:25:41.020]   And then it's like ASML, deep UV, what is that?
[01:25:41.020 --> 01:25:43.400]   - Extreme ultraviolet lithography.
[01:25:43.400 --> 01:25:44.760]   To set context on the chips, right?
[01:25:44.760 --> 01:25:47.120]   What Nathan's referring to is in 2020,
[01:25:47.120 --> 01:25:50.280]   Huawei released their Ascend 910 chip,
[01:25:50.280 --> 01:25:53.040]   which was an AI chip, first one on seven nanometer
[01:25:53.040 --> 01:25:55.200]   before Google did, before Nvidia did.
[01:25:55.200 --> 01:25:57.440]   And they submitted it to the MLPerf benchmark,
[01:25:57.440 --> 01:25:59.540]   which is sort of a industry standard
[01:25:59.540 --> 01:26:01.920]   for machine learning performance benchmark.
[01:26:01.920 --> 01:26:03.240]   And it did quite well.
[01:26:03.240 --> 01:26:05.440]   And it was the best chip at the submission, right?
[01:26:05.440 --> 01:26:08.180]   This was a huge deal.
[01:26:08.180 --> 01:26:11.720]   The Trump admin, of course, banned, it was 2019, right?
[01:26:11.720 --> 01:26:14.280]   Banned the Huawei from getting seven nanometer chips
[01:26:14.280 --> 01:26:15.400]   from TSMC.
[01:26:15.400 --> 01:26:16.760]   And so then they had to switch to move
[01:26:16.760 --> 01:26:18.800]   using internal domestically produced chips,
[01:26:18.800 --> 01:26:20.080]   which was a multi-year setback.
[01:26:20.080 --> 01:26:22.320]   - Many companies have done seven nanometer chips.
[01:26:22.320 --> 01:26:23.280]   And the question is like,
[01:26:23.280 --> 01:26:25.120]   we don't know how much Huawei
[01:26:25.120 --> 01:26:26.800]   was subsidizing production of that chip.
[01:26:26.800 --> 01:26:29.200]   Like Intel has made seven nanometer chips
[01:26:29.200 --> 01:26:31.560]   that are not profitable and things like this.
[01:26:31.560 --> 01:26:33.100]   So this is how it all feeds back
[01:26:33.100 --> 01:26:36.000]   into the economic engine of export controls.
[01:26:36.000 --> 01:26:38.080]   - Well, so you're saying that for now,
[01:26:38.080 --> 01:26:41.160]   Xi Jinping has not felt the AGI,
[01:26:41.160 --> 01:26:43.640]   but it feels like the deep seek moment
[01:26:43.640 --> 01:26:47.560]   might like, there might be meetings going on now
[01:26:47.560 --> 01:26:50.600]   where he's gonna start wearing the same T-shirt
[01:26:50.600 --> 01:26:52.200]   and things are gonna escalate.
[01:26:52.200 --> 01:26:55.440]   - I mean, like this, he may have woken up last week, right?
[01:26:55.440 --> 01:26:58.960]   Lian Feng met the second command guy
[01:26:58.960 --> 01:27:00.360]   and they had a meeting.
[01:27:00.460 --> 01:27:03.420]   And then the next day they announced the AI subsidies,
[01:27:03.420 --> 01:27:05.660]   which are a trillion RMB, right?
[01:27:05.660 --> 01:27:07.820]   - So it's possible that this deep seek moment
[01:27:07.820 --> 01:27:09.880]   is truly the beginning of a cold war.
[01:27:09.880 --> 01:27:12.280]   - That's what a lot of people are worried about.
[01:27:12.280 --> 01:27:14.100]   People in AI have been worried
[01:27:14.100 --> 01:27:16.380]   that this is going towards a cold war or already is.
[01:27:16.380 --> 01:27:18.260]   - But there was, it's not deep seek's fault,
[01:27:18.260 --> 01:27:19.660]   but there's something,
[01:27:19.660 --> 01:27:21.940]   a bunch of factors came together where it was like
[01:27:21.940 --> 01:27:23.180]   this explosion. - No history works.
[01:27:23.180 --> 01:27:24.700]   - I mean, it all has to do with Nvidia stock
[01:27:24.700 --> 01:27:27.740]   going down probably, but it's just some
[01:27:27.740 --> 01:27:30.520]   like mass hysteria that happened
[01:27:30.520 --> 01:27:33.320]   that eventually led to Xi Jinping having meetings
[01:27:33.320 --> 01:27:34.840]   and waking up to this idea.
[01:27:34.840 --> 01:27:38.480]   - And the US government realized in October 7th, 2022,
[01:27:38.480 --> 01:27:40.320]   before ChatGPT released,
[01:27:40.320 --> 01:27:42.240]   that restriction on October 7th,
[01:27:42.240 --> 01:27:44.240]   which dropped and shocked everyone.
[01:27:44.240 --> 01:27:45.880]   And it was very clearly aimed at AI.
[01:27:45.880 --> 01:27:48.160]   Everyone was like, what the heck are you doing?
[01:27:48.160 --> 01:27:50.520]   - Stable diffusion was out then, but not ChatGPT.
[01:27:50.520 --> 01:27:51.360]   - Yeah, but not ChatGPT.
[01:27:51.360 --> 01:27:53.400]   - So it was like starting to be rumblings.
[01:27:53.400 --> 01:27:55.720]   - Of what gen AI can do to society.
[01:27:55.720 --> 01:27:57.240]   But it was very clear, I think,
[01:27:57.240 --> 01:27:59.720]   to at least like National Security Council
[01:27:59.720 --> 01:28:02.200]   and those sort of folks that this was
[01:28:02.200 --> 01:28:04.800]   where the world is headed, this Cold War that's happening.
[01:28:04.800 --> 01:28:09.800]   - So is there any concerns that the export controls
[01:28:09.800 --> 01:28:15.300]   push China to take military action on Taiwan?
[01:28:15.300 --> 01:28:17.000]   - This is the big risk, right?
[01:28:17.000 --> 01:28:19.560]   The further you push China away from having access
[01:28:19.560 --> 01:28:23.240]   to cutting edge American and global technologies,
[01:28:23.240 --> 01:28:24.880]   the more likely they are to say,
[01:28:24.880 --> 01:28:26.240]   well, 'cause I can't access it,
[01:28:26.240 --> 01:28:28.840]   I might as well, like no one should access it, right?
[01:28:28.840 --> 01:28:31.600]   And there's a few interesting aspects of that, right?
[01:28:31.600 --> 01:28:36.600]   Like China has a urban-rural divide like no other.
[01:28:36.600 --> 01:28:40.000]   They have a male-female birth ratio like no other,
[01:28:40.000 --> 01:28:42.400]   to the point where if you look in most of China,
[01:28:42.400 --> 01:28:43.480]   it's like the ratio is not that bad,
[01:28:43.480 --> 01:28:45.280]   but when you look at single dudes in rural China,
[01:28:45.280 --> 01:28:46.840]   it's like a 30 to one ratio.
[01:28:46.840 --> 01:28:49.380]   And those are disenfranchised dudes, right?
[01:28:49.380 --> 01:28:52.320]   Like quote unquote, like the US has an incel problem,
[01:28:52.320 --> 01:28:55.160]   like China does too, it's just they're placated in some way
[01:28:55.160 --> 01:28:56.560]   or cut, crushed down.
[01:28:56.560 --> 01:28:57.960]   What do you do with these people?
[01:28:57.960 --> 01:28:58.780]   And at the same time,
[01:28:58.780 --> 01:29:01.520]   you're not allowed to access the most important technology,
[01:29:01.520 --> 01:29:02.840]   at least the US thinks so,
[01:29:02.840 --> 01:29:04.080]   China's maybe starting to think
[01:29:04.080 --> 01:29:05.840]   this is the most important technology
[01:29:05.840 --> 01:29:07.360]   by starting to dump subsidies in it, right?
[01:29:07.360 --> 01:29:08.640]   They thought EVs and renewables
[01:29:08.640 --> 01:29:09.920]   were the most important technology,
[01:29:09.920 --> 01:29:11.520]   they dominate that now, right?
[01:29:11.520 --> 01:29:12.440]   Now they're starting to,
[01:29:12.440 --> 01:29:14.280]   they started thinking about semiconductors
[01:29:14.280 --> 01:29:17.760]   in the late 2010s and early 2020s,
[01:29:17.760 --> 01:29:19.000]   and now they've been dumping money
[01:29:19.000 --> 01:29:21.160]   and they're catching up rapidly,
[01:29:21.160 --> 01:29:22.680]   and they're gonna do the same with AI, right?
[01:29:22.680 --> 01:29:24.240]   Because they're very talented, right?
[01:29:24.240 --> 01:29:27.600]   So the question is like,
[01:29:27.600 --> 01:29:32.280]   when does this hit a breaking point, right?
[01:29:32.280 --> 01:29:35.960]   And if China sees this as, hey, they can continue,
[01:29:35.960 --> 01:29:39.800]   if not having access and starting a true hot war, right?
[01:29:39.800 --> 01:29:40.760]   Taking over Taiwan,
[01:29:40.760 --> 01:29:43.440]   or trying to subvert its democracy in some way,
[01:29:43.440 --> 01:29:45.280]   or blockading it,
[01:29:45.280 --> 01:29:47.980]   hurts the rest of the world far more than it hurts them,
[01:29:47.980 --> 01:29:50.200]   this is something they could potentially do, right?
[01:29:50.200 --> 01:29:53.240]   And so is this pushing them towards that?
[01:29:53.240 --> 01:29:54.240]   Potentially, right?
[01:29:54.240 --> 01:29:56.160]   I'm not quite a geopolitical person,
[01:29:56.160 --> 01:29:59.800]   but it's obvious that the world regime
[01:29:59.800 --> 01:30:04.800]   of peace and trade is super awesome for economics,
[01:30:04.800 --> 01:30:06.880]   but at some point it could break, right?
[01:30:06.880 --> 01:30:07.720]   - I think we should comment
[01:30:07.720 --> 01:30:10.680]   that the why Chinese economy would be hurt by that
[01:30:10.680 --> 01:30:11.800]   is that they're export heavy.
[01:30:11.800 --> 01:30:14.120]   I think the United States buys so much,
[01:30:14.120 --> 01:30:16.520]   like if that goes away, that's how their economy--
[01:30:16.520 --> 01:30:18.280]   - Well, also they just would not be able
[01:30:18.280 --> 01:30:21.640]   to import raw materials from all over the world, right?
[01:30:21.640 --> 01:30:23.800]   The US would just shut down the trade of Malacca,
[01:30:23.800 --> 01:30:26.760]   and at the same time, the US entire,
[01:30:26.760 --> 01:30:29.520]   you could argue almost all the GDP growth in America
[01:30:29.520 --> 01:30:34.120]   since the '70s has been either population growth or tech,
[01:30:34.120 --> 01:30:35.440]   right?
[01:30:35.440 --> 01:30:39.200]   Because your life today is not that much better
[01:30:39.200 --> 01:30:41.560]   than someone from the '80s outside of tech, right?
[01:30:41.560 --> 01:30:43.480]   You still, cars,
[01:30:43.480 --> 01:30:45.000]   they all have semiconductors in them everywhere,
[01:30:45.000 --> 01:30:46.200]   fridges, semiconductors everywhere.
[01:30:46.200 --> 01:30:47.280]   There's these funny stories
[01:30:47.280 --> 01:30:49.960]   about how Russians were taking apart laundry machines
[01:30:49.960 --> 01:30:52.040]   because they had certain Texas instrument chips
[01:30:52.040 --> 01:30:53.560]   that they could then repurpose
[01:30:53.560 --> 01:30:57.500]   and put into their anti-missile missile things, right?
[01:30:57.500 --> 01:30:59.160]   Like their S-400 or whatever.
[01:30:59.160 --> 01:31:00.180]   You would know more about this,
[01:31:00.180 --> 01:31:02.680]   but there's all sorts of,
[01:31:02.680 --> 01:31:04.620]   everything about semiconductors is so integral
[01:31:04.620 --> 01:31:06.200]   to every part of our lives.
[01:31:06.200 --> 01:31:09.520]   - So can you explain the role of TSMC
[01:31:09.520 --> 01:31:11.120]   in the story of semiconductors
[01:31:11.120 --> 01:31:14.340]   and maybe also how the United States
[01:31:14.340 --> 01:31:17.000]   can break the reliance on TSMC?
[01:31:17.000 --> 01:31:19.460]   - I don't think it's necessarily breaking the reliance.
[01:31:19.460 --> 01:31:24.120]   I think it's getting TSMC to build in the U.S.
[01:31:24.120 --> 01:31:26.760]   But, so taking a step back, right?
[01:31:26.760 --> 01:31:30.760]   TSMC produces most of the world's chips, right?
[01:31:30.760 --> 01:31:32.320]   Especially on the foundry side.
[01:31:32.320 --> 01:31:35.640]   There's a lot of companies that build their own chips.
[01:31:35.640 --> 01:31:40.640]   Samsung, Intel, ST Micro, Texas Instruments,
[01:31:40.640 --> 01:31:41.680]   analog devices,
[01:31:41.680 --> 01:31:44.280]   all these kinds of companies build their own chips and XP,
[01:31:44.280 --> 01:31:45.940]   but more and more of these companies
[01:31:45.940 --> 01:31:47.600]   are outsourcing to TSMC
[01:31:47.600 --> 01:31:49.680]   and have been for multiple decades.
[01:31:49.680 --> 01:31:51.480]   - Can you explain the supply chain there
[01:31:51.480 --> 01:31:55.020]   and where most of TSMC is in terms of manufacturing?
[01:31:55.020 --> 01:31:57.660]   - Sure, so historically supply chain was
[01:31:57.660 --> 01:31:59.300]   companies would build their own chips.
[01:31:59.300 --> 01:32:01.320]   They would, you know, it'd be a company started,
[01:32:01.320 --> 01:32:02.420]   they'd build their own chips,
[01:32:02.420 --> 01:32:03.940]   and then they'd design the chip
[01:32:03.940 --> 01:32:05.980]   and build the chip and sell it.
[01:32:05.980 --> 01:32:08.420]   Over time, this became really difficult
[01:32:08.420 --> 01:32:09.920]   because the cost of building a fab
[01:32:09.920 --> 01:32:12.160]   continues to compound every single generation.
[01:32:12.160 --> 01:32:13.060]   Of course, the technology,
[01:32:13.060 --> 01:32:14.460]   figuring out the technology for it
[01:32:14.460 --> 01:32:16.340]   is incredibly difficult regardless,
[01:32:16.340 --> 01:32:18.820]   but just the dollars and cents that are required,
[01:32:18.820 --> 01:32:19.820]   ignoring, you know, saying,
[01:32:19.820 --> 01:32:21.620]   "Hey, yes, I have all the technical capability,"
[01:32:21.620 --> 01:32:23.380]   which it's really hard to get that by the way, right?
[01:32:23.380 --> 01:32:26.060]   Intel's failing, Samsung's failing, et cetera.
[01:32:26.060 --> 01:32:28.580]   But if you look at just the dollars to spend
[01:32:28.580 --> 01:32:30.060]   to build that next generation fab,
[01:32:30.060 --> 01:32:30.900]   it keeps growing, right?
[01:32:30.900 --> 01:32:31.740]   Sort of like, you know,
[01:32:31.740 --> 01:32:34.140]   Moore's law is halving the cost of chips every two years.
[01:32:34.140 --> 01:32:35.740]   There's a separate law that's sort of like
[01:32:35.740 --> 01:32:38.060]   doubling the cost of fabs every handful of years.
[01:32:38.060 --> 01:32:40.520]   And so you look at a leading edge fab
[01:32:40.520 --> 01:32:41.980]   that is gonna be profitable today,
[01:32:41.980 --> 01:32:43.420]   that's building, you know, three nanometer chips
[01:32:43.420 --> 01:32:45.000]   or two nanometer chips in the future,
[01:32:45.000 --> 01:32:48.620]   that's gonna cost north of 30, $40 billion, right?
[01:32:48.620 --> 01:32:50.580]   And that's just for like a token amount.
[01:32:50.580 --> 01:32:52.660]   That's like the base building block
[01:32:52.660 --> 01:32:54.140]   and you probably need to build multiple, right?
[01:32:54.140 --> 01:32:57.380]   And so when you look at the industry over the last,
[01:32:57.380 --> 01:32:59.740]   you know, if I go back 20, 30 years ago,
[01:32:59.740 --> 01:33:01.020]   there were 20, 30 companies
[01:33:01.020 --> 01:33:02.820]   that could build the most advanced chips
[01:33:02.820 --> 01:33:04.660]   and then they would design them themselves and sell them.
[01:33:04.660 --> 01:33:07.260]   Right, so companies like AMD would build their own chips.
[01:33:07.260 --> 01:33:08.780]   Intel, of course, still builds their own chips,
[01:33:08.780 --> 01:33:09.620]   they're very famous for it.
[01:33:09.620 --> 01:33:10.580]   IBM would build their own chips
[01:33:10.580 --> 01:33:12.860]   and, you know, you could keep going down the list.
[01:33:12.860 --> 01:33:14.440]   All these companies built their own chips.
[01:33:14.440 --> 01:33:16.620]   Slowly, they kept falling like flies
[01:33:16.620 --> 01:33:18.980]   and that's because of what TSMC did, right?
[01:33:18.980 --> 01:33:20.940]   They created the Foundry business model,
[01:33:20.940 --> 01:33:22.940]   which is, I'm not gonna design any chips,
[01:33:22.940 --> 01:33:26.020]   I'm just gonna contract manufacturer chips for other people.
[01:33:26.020 --> 01:33:28.700]   And one of their early customers is NVIDIA, right?
[01:33:28.700 --> 01:33:33.060]   NVIDIA is the only semiconductor company
[01:33:33.060 --> 01:33:34.060]   that's worth, you know,
[01:33:34.060 --> 01:33:35.900]   that's doing more than a billion dollars of revenue
[01:33:35.900 --> 01:33:39.040]   that was started in the era of Foundry, right?
[01:33:39.040 --> 01:33:40.500]   Every other company started before then
[01:33:40.500 --> 01:33:41.860]   and at some point had fabs,
[01:33:41.860 --> 01:33:44.000]   which is actually incredible, right?
[01:33:44.000 --> 01:33:46.580]   You know, like AMD and Intel and Broadcom.
[01:33:46.580 --> 01:33:48.020]   - Such a great fact.
[01:33:48.020 --> 01:33:49.900]   - It's like everyone had fabs at some point
[01:33:49.900 --> 01:33:51.780]   or, you know, some companies like Broadcom,
[01:33:51.780 --> 01:33:53.260]   it was like a merger amalgamation
[01:33:53.260 --> 01:33:54.980]   of various companies that rolled up.
[01:33:54.980 --> 01:33:56.620]   But even today, Broadcom has fabs, right?
[01:33:56.620 --> 01:33:59.860]   They build iPhone RF radio chips
[01:33:59.860 --> 01:34:02.820]   sort of in Colorado for Apple, right?
[01:34:02.820 --> 01:34:04.820]   Like all these companies had fabs
[01:34:04.820 --> 01:34:05.980]   and for most of the fabs,
[01:34:05.980 --> 01:34:07.380]   they threw them away or sold them off
[01:34:07.380 --> 01:34:09.420]   or they got rolled into something else.
[01:34:09.420 --> 01:34:11.660]   And now everyone relies on TSMC, right?
[01:34:11.660 --> 01:34:16.120]   Including Intel, their latest PC chip uses TSMC chips, right?
[01:34:16.120 --> 01:34:19.060]   It also uses some Intel chips, but it uses TSMC process.
[01:34:19.060 --> 01:34:20.760]   - Can you explain why the Foundry model
[01:34:20.760 --> 01:34:23.200]   is so successful for these companies?
[01:34:23.200 --> 01:34:24.700]   Why are they going with TSMC?
[01:34:24.700 --> 01:34:26.080]   - Economies of scale.
[01:34:26.080 --> 01:34:26.920]   - Scale.
[01:34:26.920 --> 01:34:28.360]   - Yeah, so I mean, like I mentioned, right?
[01:34:28.360 --> 01:34:30.080]   The cost of building a fab is so high.
[01:34:30.080 --> 01:34:32.440]   The R&D is so difficult.
[01:34:32.440 --> 01:34:34.800]   And when you look at like these,
[01:34:34.800 --> 01:34:37.120]   like companies that had their own vertical stack,
[01:34:37.120 --> 01:34:39.160]   there was an antiquated process of like,
[01:34:39.160 --> 01:34:42.380]   okay, like I'm so hyper-customized to each specific chip,
[01:34:42.380 --> 01:34:43.220]   right?
[01:34:43.220 --> 01:34:44.060]   But as we've gone through the history
[01:34:44.060 --> 01:34:45.380]   of sort of like the last 50 years
[01:34:45.380 --> 01:34:47.580]   of electronics and semiconductors,
[01:34:47.580 --> 01:34:49.500]   A, you need more and more specialization, right?
[01:34:49.500 --> 01:34:51.540]   Because Moore's law has died.
[01:34:51.540 --> 01:34:52.820]   Dennard scaling has died.
[01:34:52.820 --> 01:34:55.300]   IE chips are not getting better just for free, right?
[01:34:55.300 --> 01:34:56.620]   You know, from manufacturing,
[01:34:56.620 --> 01:34:59.300]   you have to make real architectural innovations, right?
[01:34:59.300 --> 01:35:02.100]   Google is not just running on Intel CPUs for web serving.
[01:35:02.100 --> 01:35:03.820]   They have a YouTube chip, they have TPUs,
[01:35:03.820 --> 01:35:04.960]   they have pixel chips.
[01:35:04.960 --> 01:35:07.900]   They have a wide diversity of chips that, you know,
[01:35:07.900 --> 01:35:10.560]   generate all the economic value of Google, right?
[01:35:10.560 --> 01:35:12.760]   Running, you know, it's running all the services and stuff.
[01:35:12.760 --> 01:35:13.960]   And so, and this is just Google,
[01:35:13.960 --> 01:35:15.640]   and you could go across any company in the industry,
[01:35:15.640 --> 01:35:16.580]   and it's like this, right?
[01:35:16.580 --> 01:35:19.480]   Cars contain 5,000 chips, you know,
[01:35:19.480 --> 01:35:21.200]   200 different varieties of them, right?
[01:35:21.200 --> 01:35:22.040]   All these random things.
[01:35:22.040 --> 01:35:23.760]   A Tesla door handle has two chips, right?
[01:35:23.760 --> 01:35:24.960]   Like it's like ridiculous.
[01:35:24.960 --> 01:35:26.400]   And it's a cool door handle, right?
[01:35:26.400 --> 01:35:27.480]   It's like, you know, you don't think about it,
[01:35:27.480 --> 01:35:28.960]   but it's like has two really chipped,
[01:35:28.960 --> 01:35:31.560]   like penny, like chips in there, right?
[01:35:31.560 --> 01:35:34.480]   Anyway, so as you have more diversity of chips,
[01:35:34.480 --> 01:35:36.400]   as you have more specialization required,
[01:35:36.400 --> 01:35:38.340]   and the cost of fabs continues to grow,
[01:35:38.340 --> 01:35:40.580]   you need someone who is laser focused
[01:35:40.580 --> 01:35:43.300]   on building the best process technology
[01:35:43.300 --> 01:35:45.840]   and making it as flexible as possible.
[01:35:45.840 --> 01:35:46.900]   - I think you could say it simply,
[01:35:46.900 --> 01:35:49.620]   which is the cost per fab goes up.
[01:35:49.620 --> 01:35:51.400]   And if you are a small player
[01:35:51.400 --> 01:35:53.180]   that makes a few types of chips,
[01:35:53.180 --> 01:35:54.760]   you're not gonna have the demand
[01:35:54.760 --> 01:35:56.580]   to pay back the cost of the fab.
[01:35:56.580 --> 01:35:59.020]   Whereas NVIDIA can have many different customers
[01:35:59.020 --> 01:36:01.860]   and aggregate all this demand into one place.
[01:36:01.860 --> 01:36:03.160]   And then they're the only person
[01:36:03.160 --> 01:36:05.780]   that makes enough money building chips
[01:36:05.780 --> 01:36:08.700]   to buy the next, to build the next fab.
[01:36:08.700 --> 01:36:11.180]   So this is kind of why the companies slowly get killed
[01:36:11.180 --> 01:36:14.220]   'cause they have 10 years ago,
[01:36:14.220 --> 01:36:16.540]   a chip that is profitable and is good enough,
[01:36:16.540 --> 01:36:18.460]   but the cost to build the next one goes up.
[01:36:18.460 --> 01:36:20.140]   They may try to do this,
[01:36:20.140 --> 01:36:22.100]   fail because they don't have the money to make it work,
[01:36:22.100 --> 01:36:23.180]   and then they don't have any chips,
[01:36:23.180 --> 01:36:24.780]   or they build it and it's too expensive
[01:36:24.780 --> 01:36:26.780]   and they just have not profitable chips.
[01:36:26.780 --> 01:36:28.500]   - You know, there's more failure points, right?
[01:36:28.500 --> 01:36:30.380]   You know, you could have one little process
[01:36:30.380 --> 01:36:33.500]   related to like some sort of like a chemical etch
[01:36:33.500 --> 01:36:35.060]   or some sort of like plasma etch,
[01:36:35.060 --> 01:36:37.500]   or some little process that screws up,
[01:36:37.500 --> 01:36:38.860]   you didn't engineer it right,
[01:36:38.860 --> 01:36:40.300]   and now the whole company falls apart,
[01:36:40.300 --> 01:36:41.300]   you can't make chips, right?
[01:36:41.300 --> 01:36:44.380]   And so super, super powerful companies like Intel,
[01:36:44.380 --> 01:36:46.260]   they had like the weathering storm to like,
[01:36:46.260 --> 01:36:47.340]   hey, they still exist today,
[01:36:47.340 --> 01:36:49.740]   even though they really screwed up their manufacturing
[01:36:49.740 --> 01:36:50.740]   six, seven years ago.
[01:36:50.740 --> 01:36:53.300]   But in the case of like AMD, they almost went bankrupt.
[01:36:53.300 --> 01:36:57.580]   They had to sell their fabs to Mubadala, UAE, right?
[01:36:57.580 --> 01:36:59.220]   And like that became a separate company
[01:36:59.220 --> 01:37:01.980]   called Global Foundries, which is a foundry firm.
[01:37:01.980 --> 01:37:03.660]   And then AMD was able to then focus
[01:37:03.660 --> 01:37:05.340]   on like on the return back up was like,
[01:37:05.340 --> 01:37:07.300]   hey, let's focus on making chiplets
[01:37:07.300 --> 01:37:10.060]   and a bunch of different chips for different markets
[01:37:10.060 --> 01:37:11.980]   and focusing on specific workloads
[01:37:11.980 --> 01:37:14.580]   rather than, you know, all of these different things.
[01:37:14.580 --> 01:37:16.060]   And so you get more diversity of chips,
[01:37:16.060 --> 01:37:18.540]   you have more companies than ever designing chips,
[01:37:18.540 --> 01:37:19.500]   but you have fewer companies
[01:37:19.500 --> 01:37:21.540]   than ever manufacturing them, right?
[01:37:21.540 --> 01:37:23.860]   And this is where TSMC comes in
[01:37:23.860 --> 01:37:25.980]   as they've just been the best, right?
[01:37:25.980 --> 01:37:27.820]   They are so good at it, right?
[01:37:27.820 --> 01:37:28.900]   They're customer focused,
[01:37:28.900 --> 01:37:31.020]   they make it easy for you to fabricate your chips,
[01:37:31.020 --> 01:37:32.340]   they take all of that complexity
[01:37:32.340 --> 01:37:35.260]   and like kind of try and abstract a lot of it away from you.
[01:37:35.260 --> 01:37:37.180]   They make good money, they don't make insane money,
[01:37:37.180 --> 01:37:39.100]   but they make good money.
[01:37:39.100 --> 01:37:41.700]   And they're able to aggregate all this demand
[01:37:41.700 --> 01:37:43.100]   and continue to build the next fab,
[01:37:43.100 --> 01:37:44.060]   the next fab, the next fab.
[01:37:44.060 --> 01:37:46.740]   - So why is Taiwan so special for TSMC?
[01:37:46.740 --> 01:37:48.900]   Why is it happening there?
[01:37:48.900 --> 01:37:51.740]   Can it be replicated inside the United States?
[01:37:51.740 --> 01:37:54.140]   - Yeah, so there's aspects of it that I would say yes
[01:37:54.140 --> 01:37:56.420]   and aspects that I'd say no, right?
[01:37:56.420 --> 01:38:00.700]   TSMC is way ahead because former, you know,
[01:38:00.700 --> 01:38:03.740]   executive Morris Chang of Texas Instruments
[01:38:03.740 --> 01:38:05.300]   wasn't promoted to CEO and he was like,
[01:38:05.300 --> 01:38:07.820]   screw this, I'm gonna go make my own chip company, right?
[01:38:07.820 --> 01:38:09.500]   And he went to Taiwan and made TSMC, right?
[01:38:09.500 --> 01:38:11.980]   And there's a whole lot more story there.
[01:38:11.980 --> 01:38:13.500]   So it could have been, Texas Instruments
[01:38:13.500 --> 01:38:15.220]   could have been the, you know, could have been TSMC,
[01:38:15.220 --> 01:38:17.700]   but Texas Semiconductor Manufacturing, right?
[01:38:17.700 --> 01:38:19.900]   Instead of, you know, Texas Instruments, right?
[01:38:19.900 --> 01:38:21.620]   But, you know, so there is that whole story there.
[01:38:21.620 --> 01:38:23.420]   - Sitting here in Texas.
[01:38:23.420 --> 01:38:25.340]   - I mean, and that sounds like a human story,
[01:38:25.340 --> 01:38:26.660]   like it didn't get promoted.
[01:38:26.660 --> 01:38:28.580]   - Just the brilliance of Morris Chang, you know,
[01:38:28.580 --> 01:38:29.860]   which I wouldn't underplay,
[01:38:29.860 --> 01:38:31.700]   but there's also like a different level
[01:38:31.700 --> 01:38:33.820]   of like how this works, right?
[01:38:33.820 --> 01:38:37.420]   So in Taiwan, the, you know,
[01:38:37.420 --> 01:38:40.540]   like the number, top percent of graduates,
[01:38:40.540 --> 01:38:43.260]   of students that go to the best school, which is NTU,
[01:38:43.260 --> 01:38:45.940]   the top percent of those all go work to TSMC, right?
[01:38:45.940 --> 01:38:47.500]   And guess what their pay is?
[01:38:47.500 --> 01:38:51.060]   Their starting pay is like $80,000, $70,000, right?
[01:38:51.060 --> 01:38:52.980]   Which is like, that's like starting pay
[01:38:52.980 --> 01:38:54.820]   for like a good graduate in the U.S., right?
[01:38:54.820 --> 01:38:56.060]   Not the top, the top graduates
[01:38:56.060 --> 01:38:58.260]   are making hundreds of thousands of dollars
[01:38:58.260 --> 01:39:00.140]   at the Googles and the Amazons,
[01:39:00.140 --> 01:39:02.540]   and now I guess the open AIs of the world, right?
[01:39:02.540 --> 01:39:05.060]   So there is a large dichotomy of like,
[01:39:05.060 --> 01:39:07.460]   what is the top 1% of the society doing
[01:39:07.460 --> 01:39:09.580]   and where are they headed because of economic reasons, right?
[01:39:09.580 --> 01:39:11.460]   Intel never paid that crazy good, right?
[01:39:11.460 --> 01:39:13.500]   And it didn't make sense to them, right?
[01:39:13.500 --> 01:39:14.940]   That's one aspect, right?
[01:39:14.940 --> 01:39:16.100]   Where's the best going?
[01:39:16.100 --> 01:39:17.380]   Second is the work ethic, right?
[01:39:17.380 --> 01:39:19.820]   Like, you know, we like to work, you know,
[01:39:19.820 --> 01:39:21.620]   you work a lot, we work a lot.
[01:39:21.620 --> 01:39:24.540]   But at the end of the day, when there's a, you know,
[01:39:24.540 --> 01:39:27.460]   when, what is the time and amount of work that you're doing
[01:39:27.460 --> 01:39:29.020]   and what does a fab require, right?
[01:39:29.020 --> 01:39:30.580]   Fabs are not work from home jobs.
[01:39:30.580 --> 01:39:33.900]   They are, you go into the fab and grueling work, right?
[01:39:33.900 --> 01:39:38.060]   There's, hey, if there is any amount of vibration, right?
[01:39:38.060 --> 01:39:40.500]   An earthquake happens, vibrates the machines.
[01:39:40.500 --> 01:39:42.820]   They're all, you know, they're either broken,
[01:39:42.820 --> 01:39:44.700]   you've scrapped some of your production.
[01:39:44.700 --> 01:39:45.580]   And then in many cases,
[01:39:45.580 --> 01:39:47.260]   they're like not calibrated properly.
[01:39:47.260 --> 01:39:49.900]   So when TSMC, when there's an earthquake, right?
[01:39:49.900 --> 01:39:51.500]   Recently, there's been an earthquake.
[01:39:51.500 --> 01:39:53.300]   TSMC doesn't call their employees.
[01:39:53.300 --> 01:39:56.220]   They just go to the fab and like,
[01:39:56.220 --> 01:39:58.220]   they just show up, the parking lot gets slammed
[01:39:58.220 --> 01:40:00.900]   and people just go into the fab and fix it, right?
[01:40:00.900 --> 01:40:02.580]   Like, it's like an arm, it's like ants, right?
[01:40:02.580 --> 01:40:04.300]   Like, it's like, you know, a hive of ants
[01:40:04.300 --> 01:40:06.700]   doesn't get told by the queen what to do.
[01:40:06.700 --> 01:40:07.980]   The ants just know.
[01:40:07.980 --> 01:40:11.140]   - It's like one person just specializes on this one task.
[01:40:11.140 --> 01:40:12.980]   And it's like, you're gonna take this one tool
[01:40:12.980 --> 01:40:14.380]   and you're the best person in the world.
[01:40:14.380 --> 01:40:15.940]   And this is what you're gonna do for your whole life
[01:40:15.940 --> 01:40:17.220]   is this one task in the fab.
[01:40:17.220 --> 01:40:19.100]   - Which is like some special chemistry
[01:40:19.100 --> 01:40:21.820]   plus nanomanufacturing on one line of tools
[01:40:21.820 --> 01:40:23.260]   that continues to get iterated.
[01:40:23.260 --> 01:40:26.300]   And yeah, it's just like, it's like specific plasma etch
[01:40:26.300 --> 01:40:27.900]   for removing silicon dioxide, right?
[01:40:27.900 --> 01:40:29.620]   That's all you focus on your whole career.
[01:40:29.620 --> 01:40:31.380]   And it's like such a specialized thing.
[01:40:31.380 --> 01:40:34.100]   And so it's not like the task are transferable.
[01:40:34.100 --> 01:40:35.220]   AI today is awesome
[01:40:35.220 --> 01:40:37.620]   because like people can pick it up like that.
[01:40:37.620 --> 01:40:40.860]   Semiconductor manufacturing is very antiquated and difficult.
[01:40:40.860 --> 01:40:42.060]   None of the materials are online
[01:40:42.060 --> 01:40:44.780]   for people to read easily and learn, right?
[01:40:44.780 --> 01:40:46.100]   The papers are very dense
[01:40:46.100 --> 01:40:49.260]   and like, it takes a lot of experience to learn.
[01:40:49.260 --> 01:40:51.980]   And so it makes the barrier to entry much higher too.
[01:40:51.980 --> 01:40:53.500]   So when you talk about,
[01:40:53.500 --> 01:40:56.060]   hey, you have all these people that are super specialized,
[01:40:56.060 --> 01:41:00.060]   they will work 80 hours a week in a factory, right?
[01:41:00.060 --> 01:41:01.060]   In a fab.
[01:41:01.060 --> 01:41:02.740]   And if anything goes wrong,
[01:41:02.740 --> 01:41:04.340]   they'll go show up in the middle of the night
[01:41:04.340 --> 01:41:05.340]   because some earthquake.
[01:41:05.340 --> 01:41:07.060]   Their wife is like, "There was an earthquake."
[01:41:07.060 --> 01:41:08.380]   He's like, "Great, I'm gonna go to the fab."
[01:41:08.380 --> 01:41:11.860]   It's like, would you like as an American do that, right?
[01:41:11.860 --> 01:41:13.620]   It's like these sorts of things are like,
[01:41:13.620 --> 01:41:15.940]   what I guess are the exemplifying
[01:41:15.940 --> 01:41:17.420]   like why TSMC is so amazing.
[01:41:17.420 --> 01:41:19.900]   Now, can you replicate it in the US?
[01:41:19.900 --> 01:41:22.580]   Let's not ignore Intel was the leader
[01:41:22.580 --> 01:41:24.380]   in manufacturing for over 20 years.
[01:41:24.380 --> 01:41:27.260]   They brought every technology to market first
[01:41:27.260 --> 01:41:28.100]   besides EUV.
[01:41:28.100 --> 01:41:31.220]   Strained silicon, high-K metal gates, FinFET.
[01:41:31.220 --> 01:41:33.300]   You know, the list goes on and on and on
[01:41:33.300 --> 01:41:35.420]   of technologies that Intel brought to market first,
[01:41:35.420 --> 01:41:37.580]   made the most money from,
[01:41:37.580 --> 01:41:41.340]   and manufactured at scale first, best,
[01:41:41.340 --> 01:41:42.620]   highest profit margins, right?
[01:41:42.620 --> 01:41:45.500]   So we shouldn't ignore that Intel can't do this, right?
[01:41:45.500 --> 01:41:48.740]   It's that the culture has broken, right?
[01:41:48.740 --> 01:41:50.140]   You've invested in the wrong things.
[01:41:50.140 --> 01:41:51.620]   They said no to the iPhone.
[01:41:51.620 --> 01:41:54.540]   They had all these different things regarding like,
[01:41:54.540 --> 01:41:57.140]   mismanagement of the fabs, mismanagement of designs,
[01:41:57.140 --> 01:41:58.780]   this lockup, right?
[01:41:58.780 --> 01:42:00.980]   And at the same time, all these brilliant people, right?
[01:42:00.980 --> 01:42:04.460]   These like 50,000 PhDs or masters
[01:42:04.460 --> 01:42:06.460]   that have been working on specific chemical
[01:42:06.460 --> 01:42:09.420]   or physical processes or nanomanufacturing processes
[01:42:09.420 --> 01:42:11.460]   for decades in Oregon, they're still there.
[01:42:11.460 --> 01:42:13.060]   They're still producing amazing work.
[01:42:13.060 --> 01:42:15.180]   It's just like getting it to the last mile of production
[01:42:15.180 --> 01:42:17.020]   at high yield where you can design,
[01:42:17.020 --> 01:42:18.820]   where you can manufacture dozens
[01:42:18.820 --> 01:42:21.380]   and hundreds of different kinds of chips, you know,
[01:42:21.380 --> 01:42:24.220]   and it's good customer experience has broken, right?
[01:42:24.220 --> 01:42:25.500]   You know, it's that customer experience.
[01:42:25.500 --> 01:42:27.660]   It's like the, like part of it is like people will say
[01:42:27.660 --> 01:42:30.660]   Intel was too pompous in the 2000s, 2010s, right?
[01:42:30.660 --> 01:42:31.900]   They just thought they were better than everyone.
[01:42:31.900 --> 01:42:32.740]   The tool guys were like,
[01:42:32.740 --> 01:42:34.580]   oh, I don't think that this is mature enough.
[01:42:34.580 --> 01:42:36.180]   And they're like, ah, you just don't know, we know, right?
[01:42:36.180 --> 01:42:38.020]   This sort of stuff would happen.
[01:42:38.020 --> 01:42:41.140]   And so can the U.S. bring it to the,
[01:42:41.140 --> 01:42:42.820]   can the U.S. bring leading edge
[01:42:42.820 --> 01:42:44.380]   semiconductor manufacturing to the U.S.?
[01:42:44.380 --> 01:42:45.580]   Emphatically, yes, right?
[01:42:45.580 --> 01:42:46.420]   And we are, right?
[01:42:46.420 --> 01:42:47.260]   - It's happening.
[01:42:47.260 --> 01:42:50.500]   Arizona is getting better and better as time goes on.
[01:42:50.500 --> 01:42:54.020]   - TSMC has built roughly 20% of their capacity
[01:42:54.020 --> 01:42:56.900]   for five nanometer in the U.S., right?
[01:42:56.900 --> 01:42:59.540]   Now this is nowhere near enough, right?
[01:42:59.540 --> 01:43:03.020]   You know, 20% of capacity in the U.S. is like nothing, right?
[01:43:03.020 --> 01:43:04.540]   And furthermore, this is still dependent
[01:43:04.540 --> 01:43:06.060]   on Taiwan existing, right?
[01:43:06.060 --> 01:43:08.260]   All, there's sort of important way to separate it out.
[01:43:08.260 --> 01:43:11.540]   There's R&D and there's high volume manufacturing.
[01:43:11.540 --> 01:43:14.620]   There, effectively, there are three places in the world
[01:43:14.620 --> 01:43:16.940]   that are doing leading edge R&D.
[01:43:16.940 --> 01:43:20.380]   There's Hsinchu, Taiwan, there's Hillsborough, Oregon,
[01:43:20.380 --> 01:43:24.260]   and there is Pyongyang, South Korea, right?
[01:43:24.260 --> 01:43:26.860]   These three places are doing the leading edge R&D
[01:43:26.860 --> 01:43:28.980]   for the rest of the world's leading edge semiconductors,
[01:43:28.980 --> 01:43:30.500]   right?
[01:43:30.500 --> 01:43:35.020]   Now manufacturing can be distributed more globally, right?
[01:43:35.020 --> 01:43:38.300]   And this is sort of where this dichotomy exists of like,
[01:43:38.300 --> 01:43:40.380]   who's actually modifying the process?
[01:43:40.380 --> 01:43:42.540]   Who's actually developing the next generation one?
[01:43:42.540 --> 01:43:43.660]   Who's improving them?
[01:43:43.660 --> 01:43:47.140]   Is Hsinchu, is Hillsborough, is Pyongyang, right?
[01:43:47.140 --> 01:43:49.140]   It is not the rest of these, you know,
[01:43:49.140 --> 01:43:50.540]   fabs like Arizona, right?
[01:43:50.540 --> 01:43:52.020]   Arizona is a paperweight.
[01:43:52.020 --> 01:43:55.020]   If Hsinchu disappeared off the face of the planet,
[01:43:55.020 --> 01:43:58.300]   you know, within a year, couple years,
[01:43:58.300 --> 01:44:00.020]   Arizona would stop producing too, right?
[01:44:00.020 --> 01:44:01.820]   It's actually like pretty critical.
[01:44:01.820 --> 01:44:02.940]   One of the things I like to say
[01:44:02.940 --> 01:44:04.980]   is if I had like a few missiles,
[01:44:04.980 --> 01:44:06.260]   I know exactly where I could cause
[01:44:06.260 --> 01:44:07.500]   the most economic damage, right?
[01:44:07.500 --> 01:44:08.940]   It's not targeting the White House, right?
[01:44:08.940 --> 01:44:10.220]   - It's the R&D centers.
[01:44:10.220 --> 01:44:13.380]   - It's the R&D centers for TSMC, Intel, Samsung,
[01:44:13.380 --> 01:44:15.100]   and then some of the memory guys, Micron and Hynix.
[01:44:15.100 --> 01:44:16.660]   - Because they define the future evolution
[01:44:16.660 --> 01:44:19.220]   of these semiconductors and everything's moving so rapidly
[01:44:19.220 --> 01:44:21.660]   that it really is fundamentally about R&D.
[01:44:21.660 --> 01:44:27.540]   And it is all about TSMC, huh?
[01:44:27.540 --> 01:44:29.060]   - And so TSMC, you know,
[01:44:29.060 --> 01:44:32.740]   you cannot purchase a vehicle without TSMC chips, right?
[01:44:32.740 --> 01:44:35.420]   You cannot purchase a fridge without TSMC chips.
[01:44:35.420 --> 01:44:38.700]   You cannot, like, I think one of the few things
[01:44:38.700 --> 01:44:40.540]   you can purchase, ironically,
[01:44:40.540 --> 01:44:43.500]   is a Texas Instruments like graphing calculator, right?
[01:44:43.500 --> 01:44:44.820]   Because they actually manufacture in Texas.
[01:44:44.820 --> 01:44:47.780]   But like outside of that, like a laptop, a phone,
[01:44:47.780 --> 01:44:49.220]   anything you, servers, right?
[01:44:49.220 --> 01:44:51.020]   GPUs, none of this stuff can exist.
[01:44:51.020 --> 01:44:53.060]   And this is without TSMC.
[01:44:53.060 --> 01:44:55.020]   And in many cases, it's not even like the leading edge,
[01:44:55.020 --> 01:44:56.460]   you know, sexy five nanometer chip,
[01:44:56.460 --> 01:44:58.100]   three nanometer chip, two nanometer chip.
[01:44:58.100 --> 01:45:00.540]   Oftentimes it's just like some stupid power IC
[01:45:00.540 --> 01:45:02.620]   that's like converting from like, you know,
[01:45:02.620 --> 01:45:03.780]   some voltage to another, right?
[01:45:03.780 --> 01:45:04.620]   And it's made at TSMC, right?
[01:45:04.620 --> 01:45:06.500]   - This is what China is investing in as well.
[01:45:06.500 --> 01:45:08.820]   It's like they can build out this long tail fab
[01:45:08.820 --> 01:45:10.340]   where the techniques are much more known.
[01:45:10.340 --> 01:45:12.660]   You don't have to figure out these problems with EUV.
[01:45:12.660 --> 01:45:14.020]   They're investing in this.
[01:45:14.020 --> 01:45:17.060]   And then they have large supply for things
[01:45:17.060 --> 01:45:20.180]   like the car door handles and the random stuff.
[01:45:20.180 --> 01:45:22.340]   And that trickles down
[01:45:22.340 --> 01:45:24.260]   into this whole economic discussion as well,
[01:45:24.260 --> 01:45:26.420]   which is they have far more than we do.
[01:45:26.420 --> 01:45:27.900]   And having supply for things like this
[01:45:27.900 --> 01:45:29.340]   is crucial to normal life.
[01:45:29.340 --> 01:45:30.460]   - So they're doing the,
[01:45:30.460 --> 01:45:33.060]   they're starting to invest in high volume manufacture,
[01:45:33.060 --> 01:45:34.620]   but they're not doing R&D.
[01:45:34.620 --> 01:45:36.580]   So they do R&D on their own.
[01:45:36.580 --> 01:45:38.100]   They're just way behind, right?
[01:45:38.100 --> 01:45:40.860]   So I would say like in 2015,
[01:45:40.860 --> 01:45:42.700]   China had a five-year plan
[01:45:42.700 --> 01:45:47.100]   where they defined by 2025 and 2020 certain goals,
[01:45:47.100 --> 01:45:50.420]   including like 80% domestic production of semiconductors.
[01:45:50.420 --> 01:45:51.620]   They're not gonna hit that, right?
[01:45:51.620 --> 01:45:52.460]   To be clear.
[01:45:52.460 --> 01:45:55.380]   But they are in certain areas really, really close, right?
[01:45:55.380 --> 01:45:58.620]   Like BYD is probably gonna be the first company
[01:45:58.620 --> 01:46:01.660]   in the world to not have to use TSMC for making,
[01:46:01.660 --> 01:46:03.460]   'cause they have their own fabs, right?
[01:46:03.460 --> 01:46:04.300]   For making chips.
[01:46:04.300 --> 01:46:07.140]   Now they still have to buy some chips from foreign,
[01:46:07.140 --> 01:46:10.900]   for example, like around like self-driving ADAS capabilities
[01:46:10.900 --> 01:46:12.140]   'cause those are really high end.
[01:46:12.140 --> 01:46:13.620]   But at least like, you know,
[01:46:13.620 --> 01:46:16.420]   like internal combustion engine has 40 chips and an EV,
[01:46:16.420 --> 01:46:18.420]   you know, just for like controlling like flow rates
[01:46:18.420 --> 01:46:19.260]   and all these things.
[01:46:19.260 --> 01:46:20.420]   And EVs are even more complicated.
[01:46:20.420 --> 01:46:22.060]   So all these different power ICs
[01:46:22.060 --> 01:46:24.540]   and battery management controllers and all these things,
[01:46:24.540 --> 01:46:26.620]   they're insourcing, right?
[01:46:26.620 --> 01:46:29.020]   And this is something that like China
[01:46:29.020 --> 01:46:31.060]   has been doing since 2015.
[01:46:31.060 --> 01:46:32.980]   Now, as far as like the trailing edge,
[01:46:32.980 --> 01:46:34.820]   they're getting so much capacity there.
[01:46:34.820 --> 01:46:36.300]   As far as the leading edge, right?
[01:46:36.300 --> 01:46:39.260]   IE this five nanometer and so on and so forth, right?
[01:46:39.260 --> 01:46:40.620]   Where GPUs, they are still behind.
[01:46:40.620 --> 01:46:42.940]   And this is, the US restrictions
[01:46:42.940 --> 01:46:45.340]   are trying to stop them in the latter.
[01:46:45.340 --> 01:46:46.860]   But you know, all that's happened, you know,
[01:46:46.860 --> 01:46:49.020]   is yes, they've slowed down their five nanometer,
[01:46:49.020 --> 01:46:50.140]   three nanometer, et cetera,
[01:46:50.140 --> 01:46:51.860]   but they've accelerated their,
[01:46:51.860 --> 01:46:54.860]   hey, 45 nanometer, 90 nanometer power IC
[01:46:54.860 --> 01:46:57.940]   or analog IC or, you know, random chip in my keyboard,
[01:46:57.940 --> 01:46:59.700]   right, that kind of stuff.
[01:46:59.700 --> 01:47:01.740]   So there is an angle of like,
[01:47:01.740 --> 01:47:04.540]   the US's actions have been so from these export,
[01:47:04.540 --> 01:47:06.300]   you know, from the angle of the export controls
[01:47:06.300 --> 01:47:10.620]   have been so inflammatory at slowing down China's progress
[01:47:10.620 --> 01:47:12.740]   on the leading edge that they've turned around
[01:47:12.740 --> 01:47:14.660]   and have accelerated their progress elsewhere
[01:47:14.660 --> 01:47:16.820]   because they know that this is so important, right?
[01:47:16.820 --> 01:47:18.300]   If the US is gonna lock them out here
[01:47:18.300 --> 01:47:20.660]   or if they lock us out here as well in the trailing edge.
[01:47:20.660 --> 01:47:23.340]   And so going back, can the US build it here?
[01:47:23.340 --> 01:47:26.460]   Yes, but it's gonna take a ton of money.
[01:47:26.460 --> 01:47:29.260]   I truly think like to revolutionize
[01:47:29.260 --> 01:47:31.340]   and completely insource semiconductors
[01:47:31.340 --> 01:47:33.100]   would take a decade and a trillion dollars.
[01:47:33.100 --> 01:47:34.980]   - Is some of it also culture?
[01:47:34.980 --> 01:47:36.660]   Like you said, extreme competence,
[01:47:36.660 --> 01:47:39.020]   extreme work ethic in Taiwan.
[01:47:39.020 --> 01:47:40.300]   - I think if you have the demand
[01:47:40.300 --> 01:47:41.500]   and the money is on the line,
[01:47:41.500 --> 01:47:43.380]   the American companies figure it out.
[01:47:43.380 --> 01:47:45.060]   It's gonna take handholding with the government,
[01:47:45.060 --> 01:47:48.580]   but I think that the culture helps TSMC break through
[01:47:48.580 --> 01:47:50.540]   and it's easier for them.
[01:47:50.540 --> 01:47:52.620]   - TSMC has some like 90,000 employees, right?
[01:47:52.620 --> 01:47:55.060]   It's not actually that insane an amount.
[01:47:55.060 --> 01:47:58.060]   The Arizona fab has 3000 from Taiwan.
[01:47:58.060 --> 01:48:00.060]   And these people, like their wives were like,
[01:48:00.060 --> 01:48:00.980]   yeah, we're not gonna have kids
[01:48:00.980 --> 01:48:03.220]   unless you sign up for the Arizona fab.
[01:48:03.220 --> 01:48:04.860]   We go to Arizona and we have our kids there.
[01:48:04.860 --> 01:48:06.900]   There's also a Japan fab where the same thing happened.
[01:48:06.900 --> 01:48:10.300]   And so like these wives drove like these dudes
[01:48:10.300 --> 01:48:13.020]   to like go to Japan or America to have the kids there.
[01:48:13.020 --> 01:48:14.620]   And it's like, it's an element of culture.
[01:48:14.620 --> 01:48:16.680]   Yeah, sure, Taiwan works that hard,
[01:48:16.680 --> 01:48:18.940]   but also like the US has done it in the past,
[01:48:18.940 --> 01:48:20.820]   they could do it now.
[01:48:20.820 --> 01:48:23.980]   You know, we can just import, I say import,
[01:48:23.980 --> 01:48:25.660]   the best people in the world if we want to.
[01:48:25.660 --> 01:48:27.460]   - That's where the immigration conversation
[01:48:27.460 --> 01:48:28.460]   is a tricky one.
[01:48:28.460 --> 01:48:29.820]   And there's been a lot of debate over that,
[01:48:29.820 --> 01:48:32.980]   but yeah, it seems absurdly controversial
[01:48:32.980 --> 01:48:34.420]   to import the best people in the world.
[01:48:34.420 --> 01:48:36.180]   I don't understand why it's controversial.
[01:48:36.180 --> 01:48:38.220]   That's the one of the ways of winning.
[01:48:38.220 --> 01:48:39.060]   - I'm sure we agree with you.
[01:48:39.060 --> 01:48:41.860]   - And like, even if you can't import those people,
[01:48:41.860 --> 01:48:43.940]   I still think you could do a lot to manufacture
[01:48:43.940 --> 01:48:46.260]   most of in the US if the money's there, right?
[01:48:46.260 --> 01:48:47.080]   And so like--
[01:48:47.080 --> 01:48:47.980]   - It's just way more expensive.
[01:48:47.980 --> 01:48:49.780]   It's not profitable for a long time.
[01:48:49.780 --> 01:48:51.780]   - And that's the context of like the CHIPS Act
[01:48:51.780 --> 01:48:54.940]   is only like $50 billion relative to, you know,
[01:48:54.940 --> 01:48:57.380]   some of the renewable, you know, initiatives
[01:48:57.380 --> 01:48:59.180]   that were passed in the Inflation Reduction Act
[01:48:59.180 --> 01:49:00.300]   and the Infrastructure Act,
[01:49:00.300 --> 01:49:03.060]   which total in the hundreds of billions of dollars, right?
[01:49:03.060 --> 01:49:04.980]   And so like the amount of money that the US is spending
[01:49:04.980 --> 01:49:08.340]   on the semiconductor industry is nothing, right?
[01:49:08.340 --> 01:49:09.820]   Whereas all these other countries
[01:49:09.820 --> 01:49:12.540]   have structural advantages in terms of like, you know,
[01:49:12.540 --> 01:49:14.780]   work ethic and amount of work and like things like that,
[01:49:14.780 --> 01:49:16.580]   but also a number of STEM graduates,
[01:49:16.580 --> 01:49:20.020]   the percentile of their best going to that, right?
[01:49:20.020 --> 01:49:22.780]   But they also have like differences in terms of like,
[01:49:22.780 --> 01:49:25.620]   hey, there's just tax benefits in the law
[01:49:25.620 --> 01:49:27.460]   and have been in the law for 20 years, right?
[01:49:27.460 --> 01:49:30.660]   And then some countries have massive subsidies, right?
[01:49:30.660 --> 01:49:33.620]   China has something like $200 billion
[01:49:33.620 --> 01:49:35.280]   of semiconductor subsidies a year.
[01:49:35.280 --> 01:49:38.540]   We're talking about $50 billion in the US over like six,
[01:49:38.540 --> 01:49:39.380]   right?
[01:49:39.380 --> 01:49:42.700]   So the girth or difference in like the subsidy amounts
[01:49:42.700 --> 01:49:43.620]   is also huge, right?
[01:49:43.620 --> 01:49:45.660]   And so I think, you know,
[01:49:45.660 --> 01:49:49.140]   Trump has been talking about tariffing Taiwan recently.
[01:49:49.140 --> 01:49:51.260]   You know, that's sort of like one of these things
[01:49:51.260 --> 01:49:53.140]   that's like, oh, okay, well, like, you know,
[01:49:53.140 --> 01:49:54.340]   maybe he doesn't want to subsidize
[01:49:54.340 --> 01:49:55.860]   the US semiconductor industry.
[01:49:55.860 --> 01:49:58.500]   Obviously, tariffing Taiwan is gonna cost a lot of things
[01:49:58.500 --> 01:50:00.220]   to go get much more expensive,
[01:50:00.220 --> 01:50:01.340]   but does it change the equation
[01:50:01.340 --> 01:50:02.980]   for TSMC building more fabs in the US?
[01:50:02.980 --> 01:50:05.420]   That's what he's sort of positing, right?
[01:50:05.420 --> 01:50:07.020]   - So can you lay out the,
[01:50:07.020 --> 01:50:09.740]   so we laid out the importance, by the way,
[01:50:09.740 --> 01:50:13.020]   it's incredible how much you know about so much.
[01:50:13.020 --> 01:50:15.020]   - We told you Dylan knows all this stuff.
[01:50:15.020 --> 01:50:19.020]   - Yeah, so, okay, you laid out
[01:50:19.020 --> 01:50:21.460]   why TSMC is really important.
[01:50:21.460 --> 01:50:24.340]   If we look out into the future, 10, 20 years out,
[01:50:25.380 --> 01:50:29.820]   US-China relationship seems like it can go
[01:50:29.820 --> 01:50:34.340]   to a dark place of cold war,
[01:50:34.340 --> 01:50:37.140]   escalated cold war, even hot war,
[01:50:37.140 --> 01:50:39.900]   or to a good place of anything
[01:50:39.900 --> 01:50:44.740]   from frenemies to cooperation to working together.
[01:50:44.740 --> 01:50:48.820]   So in this game theory, complicated game,
[01:50:48.820 --> 01:50:51.860]   what are the different trajectories?
[01:50:51.860 --> 01:50:52.980]   What should US be doing?
[01:50:52.980 --> 01:50:53.820]   Like, what do you see
[01:50:53.820 --> 01:50:55.140]   as the different possible trajectories
[01:50:55.140 --> 01:50:58.540]   of US-China relations as both leaders
[01:50:58.540 --> 01:51:00.580]   start to feel the AGI more and more
[01:51:00.580 --> 01:51:04.380]   and see the importance of chips and the importance of AI?
[01:51:04.380 --> 01:51:06.940]   - I mean, ultimately, the export controls
[01:51:06.940 --> 01:51:10.220]   are pointing towards a separate future economy.
[01:51:10.220 --> 01:51:13.540]   I think the US has made it clear to Chinese leaders
[01:51:13.540 --> 01:51:17.620]   that we intend to control this technology
[01:51:17.620 --> 01:51:21.620]   at whatever cost to global economic integration.
[01:51:22.860 --> 01:51:25.060]   So that it's hard to unwind that.
[01:51:25.060 --> 01:51:27.060]   Like, the card has been played.
[01:51:27.060 --> 01:51:29.340]   - To the same extent, they've also limited US companies
[01:51:29.340 --> 01:51:30.300]   from entering China, right?
[01:51:30.300 --> 01:51:33.460]   So it's been a long time coming.
[01:51:33.460 --> 01:51:36.660]   At some point, there was a convergence, right?
[01:51:36.660 --> 01:51:38.660]   But over at least the last decade,
[01:51:38.660 --> 01:51:40.940]   it's been branching further and further out, right?
[01:51:40.940 --> 01:51:42.780]   Like US companies can't enter China.
[01:51:42.780 --> 01:51:44.940]   Chinese companies can't enter the US.
[01:51:44.940 --> 01:51:47.980]   The US is saying, hey, China, you can't get access
[01:51:47.980 --> 01:51:50.300]   to our technologies in certain areas.
[01:51:50.300 --> 01:51:52.540]   And China's rebuttaling with the same thing
[01:51:52.540 --> 01:51:55.780]   around like they've done some sort of specific materials
[01:51:55.780 --> 01:51:57.380]   and gallium and things like that
[01:51:57.380 --> 01:52:00.100]   that they've tried to limit the US on.
[01:52:00.100 --> 01:52:01.580]   There's a US drone company that's not allowed
[01:52:01.580 --> 01:52:04.140]   to buy batteries and they have like military customers.
[01:52:04.140 --> 01:52:06.820]   And this drone company just tells the military customers
[01:52:06.820 --> 01:52:08.380]   like, hey, just get it from Amazon
[01:52:08.380 --> 01:52:10.060]   'cause I can't actually physically get them, right?
[01:52:10.060 --> 01:52:11.900]   Like there's all these things that are happening
[01:52:11.900 --> 01:52:14.100]   that point to further and further divergence.
[01:52:14.100 --> 01:52:15.220]   I have zero idea.
[01:52:15.220 --> 01:52:17.900]   And I would love if we could all hold hands
[01:52:17.900 --> 01:52:19.740]   and sing Kumbaya, but like I have zero idea
[01:52:19.740 --> 01:52:21.100]   how that could possibly happen.
[01:52:21.100 --> 01:52:25.780]   - Is the divergence good or bad for avoiding war?
[01:52:25.780 --> 01:52:29.140]   Is it possible that the divergence
[01:52:29.140 --> 01:52:32.300]   in terms of manufactured chips of training AI systems
[01:52:32.300 --> 01:52:35.060]   is actually good for avoiding military conflict?
[01:52:35.060 --> 01:52:37.900]   - It's an objective fact that the world
[01:52:37.900 --> 01:52:40.220]   has been the most peaceful it has ever been
[01:52:40.220 --> 01:52:42.100]   when there are global hegemons, right?
[01:52:42.100 --> 01:52:43.220]   Or regional hegemons, right?
[01:52:43.220 --> 01:52:45.580]   In historical context, right?
[01:52:45.580 --> 01:52:47.460]   The Mediterranean was the most peaceful ever
[01:52:47.460 --> 01:52:48.740]   when the Romans were there, right?
[01:52:48.740 --> 01:52:50.740]   China had very peaceful and warring times
[01:52:50.740 --> 01:52:52.300]   and the peaceful times were when dynasties
[01:52:52.300 --> 01:52:54.260]   had a lock hold over not just themselves,
[01:52:54.260 --> 01:52:56.020]   but all their tributaries around them, right?
[01:52:56.020 --> 01:52:59.620]   And likewise, the most peaceful time in human history
[01:52:59.620 --> 01:53:02.540]   has been when the US was the global hegemon, right?
[01:53:02.540 --> 01:53:04.100]   The last hand, you know, decades.
[01:53:04.100 --> 01:53:06.700]   Now we've sort of seen things start to slide, right?
[01:53:06.700 --> 01:53:09.380]   With Russia, Ukraine, with what's going on in the Middle East
[01:53:09.380 --> 01:53:11.460]   and Taiwan risk, all these different things
[01:53:11.460 --> 01:53:12.740]   are starting to bubble up,
[01:53:12.740 --> 01:53:14.420]   still objectively extremely peaceful.
[01:53:14.420 --> 01:53:16.940]   Now, what happens when it's not one global hegemon,
[01:53:16.940 --> 01:53:19.500]   but it's two, obviously, and you know,
[01:53:19.500 --> 01:53:23.140]   China will be competitive or even overtake the US
[01:53:23.140 --> 01:53:24.180]   like it's possible, right?
[01:53:24.180 --> 01:53:27.980]   And so this change in global hegemony,
[01:53:27.980 --> 01:53:29.980]   I don't think it ever happens like super peacefully, right?
[01:53:29.980 --> 01:53:31.460]   When empires fall, right?
[01:53:31.460 --> 01:53:33.620]   Which is a possible trajectory for America,
[01:53:33.620 --> 01:53:35.700]   they don't fall gracefully, right?
[01:53:35.700 --> 01:53:37.980]   Like they don't just slide out of irrelevance.
[01:53:37.980 --> 01:53:40.460]   Usually there's a lot of shaking.
[01:53:40.460 --> 01:53:43.860]   And so, you know, what the US is trying to do
[01:53:43.860 --> 01:53:45.100]   is maintain its top position.
[01:53:45.100 --> 01:53:46.020]   And what China is trying to do
[01:53:46.020 --> 01:53:47.700]   is become the top position, right?
[01:53:47.700 --> 01:53:50.980]   And obviously there's butting of heads here
[01:53:50.980 --> 01:53:53.140]   in the most simple terms.
[01:53:53.140 --> 01:53:55.780]   - And that could take shape in all kinds of ways,
[01:53:55.780 --> 01:53:57.820]   including proxy wars.
[01:53:57.820 --> 01:53:59.780]   - It seems like it's already happening.
[01:53:59.780 --> 01:54:02.220]   Like as much as I want there to be
[01:54:02.220 --> 01:54:03.780]   centuries of prolonged peace,
[01:54:03.780 --> 01:54:06.580]   it does not, it looks like further instability
[01:54:06.580 --> 01:54:08.780]   internationally is ahead.
[01:54:08.780 --> 01:54:11.940]   And the US is like sort of like current task is like,
[01:54:11.940 --> 01:54:15.260]   hey, if we control AI, if we're the leader in AI,
[01:54:15.260 --> 01:54:18.740]   then we, and AI could significantly accelerates progress,
[01:54:18.740 --> 01:54:20.820]   then we can maintain the global hegemony position.
[01:54:20.820 --> 01:54:21.660]   And therefore-
[01:54:21.660 --> 01:54:22.620]   - I hope that works.
[01:54:22.620 --> 01:54:25.300]   - And as an American, like, you know, kind of like,
[01:54:25.300 --> 01:54:28.220]   okay, I guess that's gonna lead to peace for us.
[01:54:28.220 --> 01:54:29.620]   Now, obviously other people around the world
[01:54:29.620 --> 01:54:31.620]   get affected negatively.
[01:54:31.620 --> 01:54:34.020]   You know, obviously the Chinese people
[01:54:34.020 --> 01:54:37.060]   are not gonna be in as advantageous of a position
[01:54:37.060 --> 01:54:37.900]   if that happens.
[01:54:37.900 --> 01:54:41.340]   But, you know, this is sort of the reality
[01:54:41.340 --> 01:54:42.300]   of like what's being done
[01:54:42.300 --> 01:54:44.180]   and the actions that are being carried out.
[01:54:44.180 --> 01:54:46.740]   - So can we go back to the specific detail
[01:54:46.740 --> 01:54:47.820]   of the different hardware?
[01:54:47.820 --> 01:54:50.700]   There's this nice graphic in the export controls
[01:54:50.700 --> 01:54:56.860]   of which GPUs are allowed to be exported
[01:54:56.860 --> 01:54:58.860]   and which are not.
[01:54:58.860 --> 01:55:00.700]   Can you kind of explain the difference?
[01:55:00.700 --> 01:55:03.860]   Is there, from a technical perspective,
[01:55:03.860 --> 01:55:08.340]   are the H20s promising?
[01:55:08.340 --> 01:55:10.860]   - Yeah, so this goes, and I think we'd have to like,
[01:55:10.860 --> 01:55:13.340]   we need to dive really deep into the reasoning aspect
[01:55:13.340 --> 01:55:14.700]   and what's going on there.
[01:55:14.700 --> 01:55:17.420]   But the H20, you know, the U.S. has gone
[01:55:17.420 --> 01:55:20.380]   through multiple iterations of the export controls, right?
[01:55:20.380 --> 01:55:23.700]   This H800 was at one point allowed back in '23,
[01:55:23.700 --> 01:55:24.700]   but then it got canceled.
[01:55:24.700 --> 01:55:27.060]   And by then, you know, DeepSeek had already built
[01:55:27.060 --> 01:55:28.700]   their cluster of, they claim 2K.
[01:55:28.700 --> 01:55:30.100]   I think they actually have like many more,
[01:55:30.100 --> 01:55:31.660]   like something like 10K of those.
[01:55:31.660 --> 01:55:34.380]   And now this H20 is the legally allowed chip, right?
[01:55:34.380 --> 01:55:37.220]   Nvidia shipped a million of these last year to China, right?
[01:55:37.220 --> 01:55:39.740]   For context, it was like four or five million GPUs, right?
[01:55:39.740 --> 01:55:41.860]   So the percentage of GPUs that were
[01:55:41.860 --> 01:55:45.820]   this China specific H20 is quite high, right?
[01:55:45.820 --> 01:55:47.900]   You know, roughly 20%, 25%, right?
[01:55:47.900 --> 01:55:48.740]   20% or so.
[01:55:48.740 --> 01:55:52.740]   And so this H20 has been neutered in one way,
[01:55:52.740 --> 01:55:55.300]   but it's actually upgraded in other ways, right?
[01:55:55.300 --> 01:55:56.940]   And, you know, you could think of chips
[01:55:56.940 --> 01:55:59.500]   along three axes for AI, right?
[01:55:59.500 --> 01:56:02.780]   You know, ignoring software stack and like exact architecture
[01:56:02.780 --> 01:56:04.260]   just raw specifications.
[01:56:04.260 --> 01:56:06.140]   There's floating point operations, right?
[01:56:06.140 --> 01:56:06.980]   Flops.
[01:56:06.980 --> 01:56:10.180]   There is memory bandwidth, in memory capacity, right?
[01:56:10.180 --> 01:56:11.020]   I/O, right?
[01:56:11.020 --> 01:56:11.860]   Memory.
[01:56:11.860 --> 01:56:13.140]   And then there is interconnect, right?
[01:56:13.140 --> 01:56:14.660]   Chip to chip interconnections.
[01:56:14.660 --> 01:56:18.380]   All three of these are incredibly important
[01:56:18.380 --> 01:56:21.380]   for making AI systems, right?
[01:56:21.380 --> 01:56:23.300]   'Cause AI systems involve a lot of compute.
[01:56:23.300 --> 01:56:25.500]   They involve a lot of moving memory around,
[01:56:25.500 --> 01:56:27.940]   whether it be to memory or to other chips, right?
[01:56:27.940 --> 01:56:30.180]   And so these three vectors,
[01:56:30.180 --> 01:56:32.180]   the US initially had a multi, you know,
[01:56:32.180 --> 01:56:33.580]   had two of these vectors controlled
[01:56:33.580 --> 01:56:34.580]   and one of them not controlled,
[01:56:34.580 --> 01:56:36.540]   which was flops and interconnect bandwidth
[01:56:36.540 --> 01:56:37.700]   were initially controlled.
[01:56:37.700 --> 01:56:39.060]   And then they said, no, no, no, no,
[01:56:39.060 --> 01:56:40.340]   we're gonna remove the interconnect bandwidth
[01:56:40.340 --> 01:56:42.700]   and just make it a very simple only flops.
[01:56:42.700 --> 01:56:45.420]   But now Nvidia can now make a chip that has,
[01:56:45.420 --> 01:56:47.140]   okay, it's cut down on flops.
[01:56:47.140 --> 01:56:50.580]   It's, you know, it's like one third that of the H100, right?
[01:56:50.580 --> 01:56:54.900]   In on spec sheet paper performance for flops.
[01:56:54.900 --> 01:56:56.900]   You know, in real world, it's closer to like half
[01:56:56.900 --> 01:56:59.540]   or maybe even like 60% of it, right?
[01:56:59.540 --> 01:57:00.980]   But then on the other two vectors,
[01:57:00.980 --> 01:57:03.300]   it's just as good for interconnect bandwidth.
[01:57:03.300 --> 01:57:05.380]   And then for memory bandwidth and memory capacity,
[01:57:05.380 --> 01:57:07.500]   the H20 has more memory bandwidth
[01:57:07.500 --> 01:57:10.620]   and more memory capacity than the H100, right?
[01:57:10.620 --> 01:57:13.700]   Now, recently, you know, we, at our research,
[01:57:13.700 --> 01:57:16.900]   we cut Nvidia's production for H20 for this year
[01:57:16.900 --> 01:57:18.020]   down drastically.
[01:57:18.020 --> 01:57:20.540]   They were gonna make another 2 million of those this year,
[01:57:20.540 --> 01:57:23.700]   but they just canceled all the orders a couple of weeks ago.
[01:57:23.700 --> 01:57:24.980]   In our view, that's because we think
[01:57:24.980 --> 01:57:27.620]   that they think they're gonna get restricted, right?
[01:57:27.620 --> 01:57:30.820]   Because why would they cancel all these orders for H20?
[01:57:30.820 --> 01:57:32.460]   Because they shipped a million of them last year,
[01:57:32.460 --> 01:57:34.620]   they had orders in for a couple million this year
[01:57:34.620 --> 01:57:35.460]   and just gone, right?
[01:57:35.460 --> 01:57:38.020]   For H20, B20, right, a successor to H20.
[01:57:38.020 --> 01:57:39.060]   And now they're all gone.
[01:57:39.060 --> 01:57:41.380]   Now, why would they do this, right?
[01:57:41.380 --> 01:57:43.100]   I think it's very clear, right?
[01:57:43.100 --> 01:57:46.700]   The H20 is actually better for certain tasks.
[01:57:46.700 --> 01:57:50.020]   And that certain task is reasoning, right?
[01:57:50.020 --> 01:57:53.380]   Reasoning is incredibly like different than, you know,
[01:57:53.380 --> 01:57:56.180]   when you look at the different regimes of models, right?
[01:57:56.180 --> 01:57:59.180]   Pre-training is all about flops, right?
[01:57:59.180 --> 01:58:00.020]   It's all about flops.
[01:58:00.020 --> 01:58:01.980]   There's things you do like mixture of experts
[01:58:01.980 --> 01:58:04.220]   that we talked about to trade off interconnect
[01:58:04.220 --> 01:58:06.340]   or to trade off, you know, other aspects
[01:58:06.340 --> 01:58:10.100]   and lower the flops and rely more on interconnect and memory.
[01:58:10.100 --> 01:58:12.780]   But at the end of the day, it's flops is everything, right?
[01:58:12.780 --> 01:58:14.660]   We talk about models in terms of like
[01:58:14.660 --> 01:58:16.820]   how many flops they are, right?
[01:58:16.820 --> 01:58:17.980]   So like, you know, we talk about,
[01:58:17.980 --> 01:58:20.700]   oh, GPT-4 is 2E25, right?
[01:58:20.700 --> 01:58:25.420]   Two to the 25th, you know, 25 zeros, right?
[01:58:25.420 --> 01:58:26.260]   Flop, right?
[01:58:26.260 --> 01:58:28.260]   Floating point operations.
[01:58:28.260 --> 01:58:29.100]   - For training.
[01:58:29.100 --> 01:58:29.940]   - For training, right?
[01:58:29.940 --> 01:58:31.380]   And we're talking about the restrictions
[01:58:31.380 --> 01:58:33.820]   for the 2E24, right, or 25, whatever.
[01:58:33.820 --> 01:58:37.180]   The US has an executive order that Trump recently unsigned,
[01:58:37.180 --> 01:58:40.020]   but which was, hey, 1E26,
[01:58:40.020 --> 01:58:41.900]   once you hit that number of floating point operations,
[01:58:41.900 --> 01:58:43.500]   you must notify the government
[01:58:43.500 --> 01:58:45.540]   and you must share your results with us, right?
[01:58:45.540 --> 01:58:46.860]   Like there's a level of model
[01:58:46.860 --> 01:58:49.060]   where the US government must be told, right?
[01:58:49.060 --> 01:58:50.340]   And that's 1E26.
[01:58:50.340 --> 01:58:51.900]   And so as we move forward,
[01:58:51.900 --> 01:58:53.940]   this is an incredibly like important,
[01:58:53.940 --> 01:58:55.460]   flop is the vector that the government
[01:58:55.460 --> 01:58:56.820]   has cared about historically,
[01:58:56.820 --> 01:59:00.420]   but the other two vectors are arguably just as important,
[01:59:00.420 --> 01:59:01.260]   right?
[01:59:01.260 --> 01:59:03.620]   And especially when we come to this new paradigm,
[01:59:03.620 --> 01:59:05.700]   which the world is only just learning about
[01:59:05.700 --> 01:59:06.900]   over the last six months, right?
[01:59:06.900 --> 01:59:07.740]   Reasoning.
[01:59:07.740 --> 01:59:10.300]   - And do we understand firmly
[01:59:10.300 --> 01:59:13.700]   which of the three dimensions is best for reasoning?
[01:59:13.700 --> 01:59:16.180]   So interconnect, the flops don't matter as much,
[01:59:16.180 --> 01:59:17.340]   is it memory?
[01:59:17.340 --> 01:59:18.180]   - Memory, right?
[01:59:18.180 --> 01:59:19.020]   - Yes.
[01:59:19.020 --> 01:59:19.840]   - Text length.
[01:59:19.840 --> 01:59:20.680]   - We're gonna get into technical stuff real fast, yeah.
[01:59:20.680 --> 01:59:23.380]   - I would say there's two articles in this one
[01:59:23.380 --> 01:59:24.220]   that I could show,
[01:59:24.220 --> 01:59:26.820]   maybe graphics that might be interesting for you to pull up.
[01:59:26.820 --> 01:59:27.660]   - For the listeners,
[01:59:27.660 --> 01:59:28.940]   we're looking at the section
[01:59:28.940 --> 01:59:32.380]   of 01, Inference, Architecture, Toconomics.
[01:59:32.380 --> 01:59:35.060]   - Hmm, you wanna explain KVCache before we talk about this?
[01:59:35.060 --> 01:59:36.060]   I think like it's better to--
[01:59:36.060 --> 01:59:37.900]   - Okay, yeah, we need to go through
[01:59:37.900 --> 01:59:41.060]   a lot of specific technical things of transformers
[01:59:41.060 --> 01:59:42.780]   to make this easier for people.
[01:59:42.780 --> 01:59:44.780]   - Because it's incredibly important
[01:59:44.780 --> 01:59:46.220]   because this changes how models work.
[01:59:46.220 --> 01:59:48.340]   But I think resetting, right?
[01:59:48.340 --> 01:59:51.180]   Why is memory so important?
[01:59:51.180 --> 01:59:53.420]   It's because so far we've talked about parameter counts,
[01:59:53.420 --> 01:59:54.260]   right?
[01:59:54.260 --> 01:59:55.080]   And mixture of experts,
[01:59:55.080 --> 01:59:56.620]   you can change how many active parameters
[01:59:56.620 --> 01:59:58.420]   versus total parameters to embed more data,
[01:59:58.420 --> 01:59:59.700]   but have less flops.
[01:59:59.700 --> 02:00:01.540]   But more important, you know,
[02:00:01.540 --> 02:00:03.020]   another aspect of, you know,
[02:00:03.020 --> 02:00:04.700]   what's part of this humongous revolution
[02:00:04.700 --> 02:00:05.900]   in the last handful of years,
[02:00:05.900 --> 02:00:07.180]   is the transformer, right?
[02:00:07.180 --> 02:00:08.340]   And the attention mechanism.
[02:00:08.340 --> 02:00:10.500]   Attention mechanism is that the model
[02:00:10.500 --> 02:00:12.340]   understands the relationships
[02:00:12.340 --> 02:00:14.580]   between all the words in its context, right?
[02:00:14.580 --> 02:00:18.780]   And that is separate from the parameters themselves, right?
[02:00:18.780 --> 02:00:21.900]   And that is something that you must calculate, right?
[02:00:21.900 --> 02:00:23.460]   How each token, right?
[02:00:23.460 --> 02:00:25.260]   Each word in the context length
[02:00:25.260 --> 02:00:29.020]   is relatively connected to each other, right?
[02:00:29.020 --> 02:00:30.220]   And I think, Nathan,
[02:00:30.220 --> 02:00:31.780]   you should explain KVCache better.
[02:00:31.780 --> 02:00:33.140]   - KVCache is one of the optimizations.
[02:00:33.140 --> 02:00:37.100]   - Yeah, so the attention operator has three core things.
[02:00:37.100 --> 02:00:39.540]   It's queries, keys, and values.
[02:00:39.540 --> 02:00:42.660]   QKV is the thing that goes into this.
[02:00:42.660 --> 02:00:44.040]   You'll look at the equation.
[02:00:44.040 --> 02:00:46.620]   You see that these matrices are multiplied together.
[02:00:46.620 --> 02:00:48.380]   These words, query, key, and value
[02:00:48.380 --> 02:00:50.380]   come from information retrieval backgrounds
[02:00:50.380 --> 02:00:52.540]   where the query is the thing
[02:00:52.540 --> 02:00:53.780]   you're trying to get the values for.
[02:00:53.780 --> 02:00:56.540]   And you access the keys and the values is reweighting.
[02:00:56.540 --> 02:00:58.780]   My background's not in information retrieval
[02:00:58.780 --> 02:00:59.620]   and things like this.
[02:00:59.620 --> 02:01:01.380]   I just want to have backlinks.
[02:01:01.380 --> 02:01:03.540]   And what effectively happens
[02:01:03.540 --> 02:01:06.180]   is that when you're doing these matrix multiplications,
[02:01:06.180 --> 02:01:08.900]   you're having matrices that are of the size
[02:01:08.900 --> 02:01:09.780]   of the context length.
[02:01:09.780 --> 02:01:11.900]   So the number of tokens that you put into the model.
[02:01:11.900 --> 02:01:14.860]   And the KVCache is effectively
[02:01:14.860 --> 02:01:17.100]   some form of compressed representation
[02:01:17.100 --> 02:01:19.380]   of all the previous tokens in the model.
[02:01:19.380 --> 02:01:20.600]   So when you're doing this,
[02:01:20.600 --> 02:01:22.580]   we talk about autoregressive models,
[02:01:22.580 --> 02:01:24.320]   you predict one token at a time.
[02:01:24.320 --> 02:01:26.300]   You start with whatever your prompt was.
[02:01:26.300 --> 02:01:30.100]   You ask a question like who was the president in 1825?
[02:01:30.100 --> 02:01:32.420]   The model then is going to generate its first token.
[02:01:32.420 --> 02:01:34.060]   For each of these tokens,
[02:01:34.060 --> 02:01:36.220]   you're doing the same attention operator
[02:01:36.220 --> 02:01:40.140]   where you're multiplying these query key value matrices.
[02:01:40.140 --> 02:01:42.460]   But the math is very nice
[02:01:42.460 --> 02:01:44.740]   so that when you're doing this repeatedly,
[02:01:44.740 --> 02:01:48.940]   this KVCache, this key value operation,
[02:01:48.940 --> 02:01:51.320]   you can keep appending the new values to it.
[02:01:51.320 --> 02:01:53.740]   So you keep track of what your previous values
[02:01:53.740 --> 02:01:56.540]   you're inferring over in this autoregressive chain,
[02:01:56.540 --> 02:01:58.660]   you keep it in memory the whole time.
[02:01:58.660 --> 02:02:02.040]   And this is a really crucial thing to manage
[02:02:02.040 --> 02:02:04.300]   when serving inference at scale.
[02:02:04.300 --> 02:02:06.140]   There are far bigger experts in this
[02:02:06.140 --> 02:02:08.020]   and there are so many levels of detail
[02:02:08.020 --> 02:02:09.420]   that you can go into.
[02:02:09.420 --> 02:02:13.380]   Essentially, one of the key "drawbacks"
[02:02:13.380 --> 02:02:16.180]   of the attention operator and the transformer
[02:02:16.180 --> 02:02:18.860]   is that there is a form of quadratic memory cost
[02:02:18.860 --> 02:02:21.540]   in proportion to the context length.
[02:02:21.540 --> 02:02:23.780]   So as you put in longer questions,
[02:02:23.780 --> 02:02:27.180]   the memory used in order to make that computation
[02:02:27.180 --> 02:02:29.540]   is going up in the form of a quadratic.
[02:02:29.540 --> 02:02:33.180]   You'll hear about a lot of other language model architectures
[02:02:33.180 --> 02:02:36.660]   that are like subquadratic or linear attention forms,
[02:02:36.660 --> 02:02:38.540]   which is like state-space models.
[02:02:38.540 --> 02:02:40.100]   We don't need to go down all these now.
[02:02:40.100 --> 02:02:42.500]   And then there's innovations on attention
[02:02:42.500 --> 02:02:44.740]   to make this memory usage
[02:02:44.740 --> 02:02:48.180]   and the ability to attend over long contexts
[02:02:48.180 --> 02:02:50.260]   much more accurate and high performance.
[02:02:50.260 --> 02:02:52.420]   - And those innovations are going to help you with,
[02:02:52.420 --> 02:02:53.980]   I mean, you're highly memory constrained.
[02:02:53.980 --> 02:02:55.940]   - They help with memory constraint and performance.
[02:02:55.940 --> 02:02:58.340]   So if you put in a book into, I think,
[02:02:58.340 --> 02:03:00.540]   Gemini is the model that has the longest context length
[02:03:00.540 --> 02:03:01.380]   that people are using.
[02:03:01.380 --> 02:03:02.540]   Gemini is known for 1 million
[02:03:02.540 --> 02:03:04.300]   and now 2 million context length.
[02:03:04.300 --> 02:03:06.380]   You put a whole book into Gemini
[02:03:06.380 --> 02:03:09.300]   and sometimes it'll draw facts out of it.
[02:03:09.300 --> 02:03:11.220]   It's not perfect, they're getting better,
[02:03:11.220 --> 02:03:12.940]   but the, so there's two things.
[02:03:12.940 --> 02:03:14.700]   It's like one, to be able to serve this
[02:03:14.700 --> 02:03:15.540]   on the memory level.
[02:03:15.540 --> 02:03:17.660]   Google has magic with their TPU stack
[02:03:17.660 --> 02:03:19.540]   where they can serve really long contexts.
[02:03:19.540 --> 02:03:21.940]   And then there's also many decisions along the way
[02:03:21.940 --> 02:03:24.100]   to actually make long context performance work.
[02:03:24.100 --> 02:03:25.500]   This implies the data,
[02:03:25.500 --> 02:03:28.820]   there's subtle changes to these computations in attention,
[02:03:28.820 --> 02:03:31.140]   and it just, it changes the architecture.
[02:03:31.140 --> 02:03:35.300]   But serving long contexts is extremely memory constrained,
[02:03:35.300 --> 02:03:37.020]   especially when you're making a lot of predictions.
[02:03:37.020 --> 02:03:40.620]   I actually don't know why input and output tokens
[02:03:40.620 --> 02:03:41.460]   are more expensive,
[02:03:41.460 --> 02:03:42.900]   but I think essentially output tokens,
[02:03:42.900 --> 02:03:44.380]   you have to do more computation
[02:03:44.380 --> 02:03:45.860]   'cause you have to sample from the model.
[02:03:45.860 --> 02:03:46.700]   - I can explain that.
[02:03:46.700 --> 02:03:48.980]   So today, if you use a model,
[02:03:48.980 --> 02:03:50.220]   like you look at an API,
[02:03:50.220 --> 02:03:54.900]   OpenAI charges a certain price per million tokens.
[02:03:54.900 --> 02:03:58.020]   And that price for input and output tokens is different.
[02:03:58.020 --> 02:04:01.260]   And the reason is that there is,
[02:04:01.260 --> 02:04:04.020]   when you're inputting a query into the model,
[02:04:04.020 --> 02:04:06.020]   let's say you have a book,
[02:04:06.020 --> 02:04:09.140]   that book you must now calculate the entire KV cache for,
[02:04:09.140 --> 02:04:10.260]   this key value cache.
[02:04:10.260 --> 02:04:13.500]   And so when you do that, that is a parallel operation.
[02:04:13.500 --> 02:04:15.900]   All of the tokens can be processed at one time.
[02:04:15.900 --> 02:04:18.100]   And therefore you can dramatically reduce
[02:04:18.100 --> 02:04:19.300]   how much you're spending, right?
[02:04:19.300 --> 02:04:21.740]   The flop requirements for generating a token
[02:04:21.740 --> 02:04:24.300]   and an input token are identical, right?
[02:04:24.300 --> 02:04:26.540]   If I input one token or if I generate one token,
[02:04:26.540 --> 02:04:27.540]   it's completely identical.
[02:04:27.540 --> 02:04:28.740]   I have to go through the model, right?
[02:04:28.740 --> 02:04:31.660]   But the difference is that I can do that input,
[02:04:31.660 --> 02:04:34.100]   i.e. the pre-fill, i.e. the prompt,
[02:04:34.100 --> 02:04:37.340]   simultaneously in a batch nature, right?
[02:04:37.340 --> 02:04:38.860]   And therefore it is all flop.
[02:04:38.860 --> 02:04:39.940]   - I think the pricing model,
[02:04:39.940 --> 02:04:42.460]   mostly they use this for input tokens
[02:04:42.460 --> 02:04:44.260]   is about one fourth the price of the output tokens.
[02:04:44.260 --> 02:04:45.100]   - Correct.
[02:04:45.100 --> 02:04:46.100]   But then output tokens,
[02:04:46.100 --> 02:04:47.340]   the reason why it's so expensive
[02:04:47.340 --> 02:04:48.740]   is because I can't do it in parallel, right?
[02:04:48.740 --> 02:04:49.820]   It's autoregressive.
[02:04:49.820 --> 02:04:51.300]   Every time I generate a token,
[02:04:51.300 --> 02:04:53.620]   I must not only take the entire,
[02:04:53.620 --> 02:04:56.180]   I must not only read the whole entire model into memory,
[02:04:56.180 --> 02:04:57.020]   right?
[02:04:57.020 --> 02:04:57.900]   And activate it, right?
[02:04:57.900 --> 02:04:59.780]   Go calculate it to generate the next token.
[02:04:59.780 --> 02:05:01.700]   I also have to read the entire KV cache.
[02:05:01.700 --> 02:05:03.980]   And I generate a token and I append that KV,
[02:05:03.980 --> 02:05:06.420]   that one token I generated and it's KV cache.
[02:05:06.420 --> 02:05:07.620]   And then I do it again, right?
[02:05:07.620 --> 02:05:10.820]   And so therefore this is a non-parallel operation.
[02:05:10.820 --> 02:05:13.660]   And this is one where you have to,
[02:05:13.660 --> 02:05:15.580]   in the case of pre-fill or prompt,
[02:05:15.580 --> 02:05:17.220]   you pull the whole model in
[02:05:17.220 --> 02:05:19.900]   and you calculate 20,000 tokens at once, right?
[02:05:19.900 --> 02:05:22.060]   - So these are features that APIs are shipping,
[02:05:22.060 --> 02:05:25.740]   which is like prompt caching, pre-filling,
[02:05:25.740 --> 02:05:27.060]   'cause you can drive prices down
[02:05:27.060 --> 02:05:28.460]   and you can make APIs much faster.
[02:05:28.460 --> 02:05:29.740]   If you know you're gonna keep,
[02:05:29.740 --> 02:05:30.580]   if you run a business
[02:05:30.580 --> 02:05:33.100]   and you're gonna keep passing the same initial content
[02:05:33.100 --> 02:05:34.420]   to Cloud's API,
[02:05:34.420 --> 02:05:37.180]   you can load that in to the Anthropic API
[02:05:37.180 --> 02:05:38.540]   and always keep it there.
[02:05:38.540 --> 02:05:39.380]   But it's very different
[02:05:39.380 --> 02:05:41.620]   than we're kind of leading to the reasoning models,
[02:05:41.620 --> 02:05:42.460]   which we talked,
[02:05:42.460 --> 02:05:43.540]   we showed this example earlier
[02:05:43.540 --> 02:05:46.740]   and read some of this kind of mumbling stuff.
[02:05:46.740 --> 02:05:48.940]   And what happens is that
[02:05:48.940 --> 02:05:51.420]   the output context length is so much higher.
[02:05:51.420 --> 02:05:53.500]   And I mean, I learned a lot about this from Dylan's work,
[02:05:53.500 --> 02:05:56.140]   which is essentially as the output length gets higher,
[02:05:56.140 --> 02:05:57.020]   you're using this,
[02:05:57.020 --> 02:05:59.700]   you're writing this quadratic in terms of memory used.
[02:05:59.700 --> 02:06:02.260]   And then the GPUs that we have,
[02:06:02.260 --> 02:06:04.860]   effectively you're gonna run out of memory
[02:06:04.860 --> 02:06:07.260]   and they're all trying to serve multiple requests at once.
[02:06:07.260 --> 02:06:08.660]   So they're doing this batch processing
[02:06:08.660 --> 02:06:10.540]   where not all of the prompts are exactly the same,
[02:06:10.540 --> 02:06:11.940]   really complex handling.
[02:06:11.940 --> 02:06:14.260]   And then as context lengths gets longer,
[02:06:14.260 --> 02:06:15.140]   there's this like,
[02:06:15.140 --> 02:06:17.060]   I think you call it critical batch size,
[02:06:17.060 --> 02:06:20.660]   where your ability to serve more users.
[02:06:20.660 --> 02:06:24.220]   So how much you can parallelize your inference plummets
[02:06:24.220 --> 02:06:25.420]   because of this long context.
[02:06:25.420 --> 02:06:28.060]   So your memory usage is going way up
[02:06:28.060 --> 02:06:29.180]   with these reasoning models
[02:06:29.180 --> 02:06:30.700]   and you still have a lot of users.
[02:06:30.700 --> 02:06:35.020]   So effectively the cost to serve multiplies by a ton.
[02:06:35.020 --> 02:06:36.580]   - And we're looking at a plot
[02:06:36.580 --> 02:06:39.900]   when the X-axis is a sequence length.
[02:06:39.900 --> 02:06:43.380]   IE, how many tokens are being generated slash prompt.
[02:06:43.380 --> 02:06:45.740]   So if I put in a book, that's a million tokens.
[02:06:45.740 --> 02:06:48.220]   But if I put in, the sky is blue,
[02:06:48.220 --> 02:06:49.060]   then that's like six tokens or whatever.
[02:06:49.060 --> 02:06:51.940]   - And we should say that what we're calling reasoning
[02:06:51.940 --> 02:06:55.500]   and chain of thought is extending this sequence length.
[02:06:55.500 --> 02:06:56.700]   - It's mostly output.
[02:06:56.700 --> 02:06:59.620]   So before, three months ago, whenever O1 launched,
[02:06:59.620 --> 02:07:02.140]   all of the use cases for long context length
[02:07:02.140 --> 02:07:04.020]   were like, let me put a ton of documents in
[02:07:04.020 --> 02:07:05.500]   and then get an answer out.
[02:07:05.500 --> 02:07:09.740]   And it's a single pre-fill, compute a lot in parallel,
[02:07:09.740 --> 02:07:11.260]   and then output a little bit.
[02:07:11.260 --> 02:07:13.420]   Now with reasoning and agents,
[02:07:13.420 --> 02:07:15.100]   this is a very different idea.
[02:07:15.100 --> 02:07:17.260]   Now instead, I might only have like,
[02:07:17.260 --> 02:07:19.260]   hey, do this task or I might have all these documents.
[02:07:19.260 --> 02:07:20.500]   But at the end of the day,
[02:07:20.500 --> 02:07:22.700]   the model is not just like producing a little bit.
[02:07:22.700 --> 02:07:24.820]   It's producing tons of information.
[02:07:24.820 --> 02:07:26.900]   This chain of thought just continues to go
[02:07:26.900 --> 02:07:28.300]   and go and go and go.
[02:07:28.300 --> 02:07:31.260]   And so the sequence length is effectively that,
[02:07:31.260 --> 02:07:33.100]   if it's generated 10,000 tokens,
[02:07:33.100 --> 02:07:34.940]   it's 10,000 sequence length.
[02:07:34.940 --> 02:07:37.340]   And plus whatever you inputted in the prompt.
[02:07:37.340 --> 02:07:38.580]   And so what this chart is showing,
[02:07:38.580 --> 02:07:40.100]   and it's a logarithmic chart, right?
[02:07:40.100 --> 02:07:45.100]   Is, you know, as you go from 1K to 4K or 4K to 16K,
[02:07:45.100 --> 02:07:50.500]   the memory requirements grow so fast for your KV cache
[02:07:50.500 --> 02:07:54.260]   that you end up not being able to run a certain number of,
[02:07:54.260 --> 02:07:56.700]   you know, your sequence length is capped
[02:07:56.700 --> 02:07:57.660]   or the number of users you can serve.
[02:07:57.660 --> 02:07:58.500]   - Let's say the model.
[02:07:58.500 --> 02:08:02.340]   So this is showing for a 405B model in batch size 64.
[02:08:02.340 --> 02:08:04.260]   - Lama 3145D.
[02:08:04.260 --> 02:08:06.300]   - Yeah, and batch size is crucial too.
[02:08:06.300 --> 02:08:09.140]   Essentially you wanna have higher batch size
[02:08:09.140 --> 02:08:11.620]   to parallelize, parallel your throughput.
[02:08:11.620 --> 02:08:13.060]   - 64 different users at once, right?
[02:08:13.060 --> 02:08:13.900]   - Yeah.
[02:08:13.900 --> 02:08:15.060]   - And therefore your serving costs are lower, right?
[02:08:15.060 --> 02:08:16.580]   'Cause the server costs the same, right?
[02:08:16.580 --> 02:08:19.900]   This is eight H100s, roughly $2 an hour per GPU.
[02:08:19.900 --> 02:08:21.620]   That's $16 an hour, right?
[02:08:21.620 --> 02:08:23.860]   That is like somewhat of a fixed cost.
[02:08:23.860 --> 02:08:25.180]   You can do things to make it lower, of course.
[02:08:25.180 --> 02:08:26.820]   But like, it's like $16 an hour.
[02:08:26.820 --> 02:08:28.540]   Now, how many users can you serve?
[02:08:28.540 --> 02:08:30.020]   How many tokens can you generate?
[02:08:30.020 --> 02:08:32.100]   And then you divide the two and that's your cost, right?
[02:08:32.100 --> 02:08:34.500]   And so with reasoning models,
[02:08:34.500 --> 02:08:37.220]   this is where a lot of the complexity comes about
[02:08:37.220 --> 02:08:38.780]   and why memory is so important.
[02:08:38.780 --> 02:08:41.180]   Because if you have limited amounts of memory,
[02:08:41.180 --> 02:08:43.020]   then you can't serve so many users.
[02:08:43.020 --> 02:08:44.540]   If you have limited amounts of memory,
[02:08:44.540 --> 02:08:46.300]   your serving speeds get lower, right?
[02:08:46.300 --> 02:08:49.180]   And so your costs get a lot, lot worse.
[02:08:49.180 --> 02:08:51.300]   Because all of a sudden, if I was used to,
[02:08:51.300 --> 02:08:52.980]   hey, on this $16 an hour server,
[02:08:52.980 --> 02:08:56.100]   I'm serving Lama 405B, or if I'm serving, you know,
[02:08:56.100 --> 02:08:59.500]   DeepSeek V3, and it's all chat style applications,
[02:08:59.500 --> 02:09:00.940]   i.e. we're just chit-chatting.
[02:09:00.940 --> 02:09:04.220]   The sequence lengths are a thousand, a few thousand, right?
[02:09:04.220 --> 02:09:05.340]   You know, when you use a language model,
[02:09:05.340 --> 02:09:06.740]   it's a few thousand context lengths most of the time.
[02:09:06.740 --> 02:09:08.220]   Sometimes you're dropping a big document,
[02:09:08.220 --> 02:09:10.260]   but then you process it, you get your answer,
[02:09:10.260 --> 02:09:11.100]   you throw it away, right?
[02:09:11.100 --> 02:09:12.620]   You move on to the next thing, right?
[02:09:12.620 --> 02:09:14.260]   Whereas with reasoning,
[02:09:14.260 --> 02:09:17.620]   I'm now generating tens of thousands of tokens in sequence,
[02:09:17.620 --> 02:09:18.460]   right?
[02:09:18.460 --> 02:09:21.420]   And so this memory, this KVCache has to stay resident
[02:09:21.420 --> 02:09:22.420]   and you have to keep loading it.
[02:09:22.420 --> 02:09:24.780]   You have to keep it in memory constantly.
[02:09:24.780 --> 02:09:27.060]   And now this butts out other users, right?
[02:09:27.060 --> 02:09:29.140]   If there's now a reasoning task, right?
[02:09:29.140 --> 02:09:30.780]   And the model's capable of reasoning,
[02:09:30.780 --> 02:09:34.180]   then all of a sudden that memory pressure
[02:09:34.180 --> 02:09:36.260]   means that I can't serve as many users simultaneously.
[02:09:36.260 --> 02:09:37.820]   - Let's go into DeepSeek again.
[02:09:37.820 --> 02:09:42.340]   So we're in the post DeepSeek R1 time, I think.
[02:09:42.340 --> 02:09:45.300]   And there's two sides to this market watching
[02:09:45.300 --> 02:09:46.940]   how hard it is to serve it.
[02:09:46.940 --> 02:09:49.100]   On one side, we're gonna talk about DeepSeek themselves.
[02:09:49.100 --> 02:09:51.180]   They now have a chat app that got to number one
[02:09:51.180 --> 02:09:52.300]   on the app store.
[02:09:52.300 --> 02:09:54.060]   Disclaimer, number one on the app store
[02:09:54.060 --> 02:09:55.100]   is measured by velocity.
[02:09:55.100 --> 02:09:57.180]   So it's not necessarily saying that more people
[02:09:57.180 --> 02:09:59.460]   have the DeepSeek app than the chat GPT app,
[02:09:59.460 --> 02:10:00.940]   but it is still remarkable.
[02:10:00.940 --> 02:10:02.660]   Cloud has never hit the number one in the app store,
[02:10:02.660 --> 02:10:04.260]   even though everyone in San Francisco is like,
[02:10:04.260 --> 02:10:06.420]   "Oh my God, you gotta use Cloud, don't use chat GPT."
[02:10:06.420 --> 02:10:07.700]   So DeepSeek hit this.
[02:10:07.700 --> 02:10:09.860]   They also launched an API product recently
[02:10:09.860 --> 02:10:11.780]   where you can ping their API
[02:10:11.780 --> 02:10:14.340]   and get these super long responses for R1 out.
[02:10:14.340 --> 02:10:17.180]   At the same time as these are out,
[02:10:17.180 --> 02:10:19.140]   we'll get to what's happened to them.
[02:10:19.140 --> 02:10:21.300]   Because the model weights for DeepSeek R1
[02:10:21.300 --> 02:10:24.420]   are openly available and the license is very friendly,
[02:10:24.420 --> 02:10:26.420]   the MIT license commercially available,
[02:10:26.420 --> 02:10:29.140]   all of these mid-sized companies and big companies
[02:10:29.140 --> 02:10:33.860]   are trying to be first to serve R1 to their users.
[02:10:33.860 --> 02:10:35.260]   We were trying to evaluate R1
[02:10:35.260 --> 02:10:36.940]   'cause we have really similar research going on.
[02:10:36.940 --> 02:10:39.300]   We released the model and we're trying to compare to it.
[02:10:39.300 --> 02:10:41.300]   And out of all the companies
[02:10:41.300 --> 02:10:44.540]   that are quote unquote serving R1,
[02:10:44.540 --> 02:10:46.900]   and they're doing it at prices that are way higher
[02:10:46.900 --> 02:10:48.460]   than the DeepSeek API,
[02:10:48.460 --> 02:10:50.940]   most of them barely work and the throughput is really low.
[02:10:50.940 --> 02:10:52.260]   - To give context, right?
[02:10:52.260 --> 02:10:54.540]   Everyone, one of the parts of like freaking this out
[02:10:54.540 --> 02:10:56.140]   was like trying to reach the capabilities.
[02:10:56.140 --> 02:10:58.140]   The other aspect is they did it so cheap, right?
[02:10:58.140 --> 02:10:59.460]   And the so cheap, we kind of talked about
[02:10:59.460 --> 02:11:01.620]   on the training side, why it was so cheap.
[02:11:01.620 --> 02:11:04.460]   - Yeah, let's talk about why it's so cheap on the inference.
[02:11:04.460 --> 02:11:06.100]   It works well and it's cheap.
[02:11:06.100 --> 02:11:08.540]   Why is R1 so damn cheap?
[02:11:08.540 --> 02:11:11.220]   - So I think there's a couple of factors here, right?
[02:11:11.220 --> 02:11:14.660]   One is that they do have model architecture innovations.
[02:11:14.660 --> 02:11:17.660]   This MLA, this new attention that they've done
[02:11:17.660 --> 02:11:21.380]   is different than the attention from attention
[02:11:21.380 --> 02:11:23.180]   is all you need to transform our attention, right?
[02:11:23.180 --> 02:11:24.540]   Now others have already innovated.
[02:11:24.540 --> 02:11:27.580]   There's a lot of work like MQA, GQA,
[02:11:27.580 --> 02:11:29.220]   local, global, all these different innovations
[02:11:29.220 --> 02:11:31.260]   that like try to bend the curve, right?
[02:11:31.260 --> 02:11:32.140]   It's still quadratic,
[02:11:32.140 --> 02:11:33.820]   but the constant is now smaller, right?
[02:11:33.820 --> 02:11:35.980]   - Related to our previous discussion,
[02:11:35.980 --> 02:11:40.700]   this multi-head latent attention can save about 80 to 90%
[02:11:40.700 --> 02:11:42.860]   in memory from the attention mechanism,
[02:11:42.860 --> 02:11:44.820]   which helps especially along context.
[02:11:44.820 --> 02:11:46.620]   - It's 80 to 90% versus the original,
[02:11:46.620 --> 02:11:48.380]   but then versus what people are actually doing.
[02:11:48.380 --> 02:11:49.580]   It's still an innovation.
[02:11:49.580 --> 02:11:52.500]   - This 80 to 90% doesn't say that the whole model
[02:11:52.500 --> 02:11:54.700]   is 80 to 90% cheaper, just this one part of it.
[02:11:54.700 --> 02:11:55.660]   - Well, and not just that, right?
[02:11:55.660 --> 02:11:57.180]   Like other people have implemented techniques
[02:11:57.180 --> 02:12:00.140]   like local, global and sliding window and GQA, MQA.
[02:12:00.140 --> 02:12:03.020]   But anyways, like DeepSeek has their attention mechanism
[02:12:03.020 --> 02:12:04.940]   as a true architectural innovation.
[02:12:04.940 --> 02:12:06.780]   They did tons of experimentation
[02:12:06.780 --> 02:12:09.980]   and this dramatically reduces the memory pressure.
[02:12:09.980 --> 02:12:10.820]   It's still there, right?
[02:12:10.820 --> 02:12:13.060]   It's still attention, it's still quadratic.
[02:12:13.060 --> 02:12:15.980]   It's just dramatically reduced it relative to prior forms.
[02:12:15.980 --> 02:12:17.820]   - All right, that's the memory pressure.
[02:12:17.820 --> 02:12:21.420]   I should say, in case people don't know,
[02:12:21.420 --> 02:12:25.060]   R1 is 27 times cheaper than O1.
[02:12:25.060 --> 02:12:28.180]   We think that OpenAI had a large margin built in.
[02:12:28.180 --> 02:12:29.180]   - Okay, so that's one-
[02:12:29.180 --> 02:12:30.020]   - There's multiple factors.
[02:12:30.020 --> 02:12:31.460]   We should break down the factors, I think.
[02:12:31.460 --> 02:12:35.100]   - It's two bucks per million token output for R1
[02:12:35.100 --> 02:12:40.100]   and $60 per million token output for O1.
[02:12:40.100 --> 02:12:42.180]   - Yeah, let's look at this.
[02:12:42.180 --> 02:12:45.260]   - So I think this is very important, right?
[02:12:45.260 --> 02:12:50.260]   OpenAI is that drastic gap between DeepSeek and pricing,
[02:12:50.260 --> 02:12:52.700]   but DeepSeek is offering the same model
[02:12:52.700 --> 02:12:55.260]   because they open-waisted it to everyone else
[02:12:55.260 --> 02:12:57.440]   for a very similar, much lower price
[02:12:57.440 --> 02:12:59.980]   than what others are able to serve it for, right?
[02:12:59.980 --> 02:13:01.740]   So there's two factors here, right?
[02:13:01.740 --> 02:13:03.760]   Their model is cheaper, right?
[02:13:03.760 --> 02:13:05.420]   It is 27 times cheaper.
[02:13:05.420 --> 02:13:06.940]   I don't remember the number exactly off the top of my head.
[02:13:06.940 --> 02:13:08.940]   - So we're looking at a graphic
[02:13:08.940 --> 02:13:13.940]   that's showing different places serving V3, DeepSeek V3,
[02:13:13.940 --> 02:13:17.820]   which is similar to DeepSeek R1.
[02:13:17.820 --> 02:13:21.020]   And there's a vast difference in-
[02:13:21.020 --> 02:13:21.860]   - In serving cost, right?
[02:13:21.860 --> 02:13:22.700]   - Yeah, in serving cost,
[02:13:22.700 --> 02:13:23.820]   and what explains that difference?
[02:13:23.820 --> 02:13:27.260]   - And so part of it is OpenAI has a fantastic margin, right?
[02:13:27.260 --> 02:13:29.100]   They're serving, when they're doing inference,
[02:13:29.100 --> 02:13:31.620]   their gross margins are north of 75%, right?
[02:13:31.620 --> 02:13:34.580]   So that's a four to five X factor right there
[02:13:34.580 --> 02:13:35.520]   of the cost difference,
[02:13:35.520 --> 02:13:38.300]   is that OpenAI is just making crazy amounts of money
[02:13:38.300 --> 02:13:40.140]   because they're the only one with the capability.
[02:13:40.140 --> 02:13:41.220]   - Do they need that money?
[02:13:41.220 --> 02:13:42.460]   Are they using it for R&D?
[02:13:42.460 --> 02:13:44.380]   - They're losing money, obviously, as a company,
[02:13:44.380 --> 02:13:46.080]   because they spend so much on training, right?
[02:13:46.080 --> 02:13:48.060]   So the inference itself is a very high margin,
[02:13:48.060 --> 02:13:49.260]   but it doesn't recoup the cost
[02:13:49.260 --> 02:13:51.220]   of everything else they're doing.
[02:13:51.220 --> 02:13:52.660]   So yes, they need that money
[02:13:52.660 --> 02:13:54.340]   because the revenue and margins
[02:13:54.340 --> 02:13:56.620]   pay for continuing to build the next thing, right?
[02:13:56.620 --> 02:13:57.900]   As long as I'm raising more money.
[02:13:57.900 --> 02:13:59.340]   - So the suggestion is that DeepSeek
[02:13:59.340 --> 02:14:01.060]   is like really bleeding out money.
[02:14:01.060 --> 02:14:02.860]   - Well, so here's one thing, right?
[02:14:02.860 --> 02:14:03.820]   We'll get to this in a second,
[02:14:03.820 --> 02:14:06.420]   but like DeepSeek doesn't have any capacity
[02:14:06.420 --> 02:14:07.300]   to actually serve the model.
[02:14:07.300 --> 02:14:09.260]   They stopped signups.
[02:14:09.260 --> 02:14:12.400]   The ability to use it is like non-existent now, right?
[02:14:12.400 --> 02:14:13.240]   For most people,
[02:14:13.240 --> 02:14:14.740]   because so many people are trying to use it,
[02:14:14.740 --> 02:14:17.100]   they just don't have the GPUs to serve it, right?
[02:14:17.100 --> 02:14:18.820]   OpenAI has hundreds of thousands of GPUs
[02:14:18.820 --> 02:14:21.140]   between them and Microsoft to serve their models.
[02:14:21.140 --> 02:14:24.220]   DeepSeek has a factor of much lower, right?
[02:14:24.220 --> 02:14:27.300]   Even if you believe our research, which is 50,000 GPUs,
[02:14:27.300 --> 02:14:28.580]   and a portion of those are for research,
[02:14:28.580 --> 02:14:30.420]   portion of those are for the hedge fund, right?
[02:14:30.420 --> 02:14:32.460]   They still have nowhere close to the GPU volumes
[02:14:32.460 --> 02:14:36.180]   and capacity to serve the model, right, at scale.
[02:14:36.180 --> 02:14:37.980]   So it is cheaper.
[02:14:37.980 --> 02:14:40.060]   A part of that is OpenAI making a ton of money.
[02:14:40.060 --> 02:14:42.940]   Is DeepSeek making money on their API?
[02:14:42.940 --> 02:14:45.260]   Unknown, I don't actually think so.
[02:14:45.260 --> 02:14:46.940]   And part of that is this chart, right?
[02:14:46.940 --> 02:14:48.500]   Look at all the other providers, right?
[02:14:48.500 --> 02:14:51.780]   TogetherAI, FireworksAI are very high-end companies, right?
[02:14:51.780 --> 02:14:54.020]   Xmeta, TogetherAI is TreeDAO
[02:14:54.020 --> 02:14:56.380]   and the inventor of like FlashAttention, right?
[02:14:56.380 --> 02:14:58.300]   Which is a huge efficiency technique, right?
[02:14:58.300 --> 02:15:00.320]   They're very efficient, good companies.
[02:15:00.320 --> 02:15:02.820]   And I do know those companies make money, right?
[02:15:02.820 --> 02:15:04.900]   Not tons of money on inference, but they make money.
[02:15:04.900 --> 02:15:07.460]   And so they're serving at like a five to seven X
[02:15:07.460 --> 02:15:09.260]   difference in cost, right?
[02:15:09.260 --> 02:15:11.380]   And so, you know, now when you equate,
[02:15:11.380 --> 02:15:12.680]   okay, OpenAI is making tons of money,
[02:15:12.680 --> 02:15:14.100]   that's like a five X difference.
[02:15:14.100 --> 02:15:15.700]   And the companies that are trying to make money
[02:15:15.700 --> 02:15:17.420]   for this model is like a five X difference.
[02:15:17.420 --> 02:15:18.580]   There is still a gap, right?
[02:15:18.580 --> 02:15:20.180]   There's still a gap and that is just DeepSeek
[02:15:20.180 --> 02:15:21.900]   being really freaking good, right?
[02:15:21.900 --> 02:15:24.900]   The model architecture, MLA, the way they did the MOE,
[02:15:24.900 --> 02:15:26.740]   all these things, there is like legitimate
[02:15:26.740 --> 02:15:28.140]   just efficiency differences.
[02:15:28.140 --> 02:15:29.460]   - All their low-level libraries
[02:15:29.460 --> 02:15:30.560]   that we talked about in training,
[02:15:30.560 --> 02:15:32.220]   some of them probably translate to inference
[02:15:32.220 --> 02:15:33.180]   and those weren't released.
[02:15:33.180 --> 02:15:35.900]   - So we may go a bit into conspiracy land,
[02:15:35.900 --> 02:15:38.220]   but is it possible the Chinese government
[02:15:38.220 --> 02:15:40.500]   is subsidizing DeepSeek?
[02:15:40.500 --> 02:15:43.180]   - I actually don't think they are.
[02:15:43.180 --> 02:15:45.060]   I think when you look at the Chinese labs,
[02:15:45.060 --> 02:15:48.780]   there's Huawei has a lab, Moonshot AI,
[02:15:48.780 --> 02:15:50.120]   there's a couple other labs out there
[02:15:50.120 --> 02:15:51.900]   that are really close with the government.
[02:15:51.900 --> 02:15:54.380]   And then there's labs like Alibaba and DeepSeek,
[02:15:54.380 --> 02:15:56.740]   which are not close with the government.
[02:15:56.740 --> 02:15:59.440]   And, you know, we talked about the CEO,
[02:15:59.440 --> 02:16:02.420]   this like reverent figure who's like quite different,
[02:16:02.420 --> 02:16:03.740]   who has like- - Sounds awesome.
[02:16:03.740 --> 02:16:05.140]   - Very different like viewpoints
[02:16:05.140 --> 02:16:07.080]   based on the Chinese interviews that are translated
[02:16:07.080 --> 02:16:09.860]   than what the CCP might necessarily want.
[02:16:09.860 --> 02:16:12.240]   Now, to be clear, right, does he have a loss leader
[02:16:12.240 --> 02:16:13.780]   because he can fund it through his hedge fund?
[02:16:13.780 --> 02:16:14.600]   Yeah, sure.
[02:16:14.600 --> 02:16:16.580]   - So the hedge fund might be subsidizing it.
[02:16:16.580 --> 02:16:18.140]   - Yes, I mean, they absolutely did, right?
[02:16:18.140 --> 02:16:19.580]   Because DeepSeek has not raised much money.
[02:16:19.580 --> 02:16:22.380]   They're now trying to raise around in China,
[02:16:22.380 --> 02:16:24.020]   but they have not raised money historically.
[02:16:24.020 --> 02:16:25.860]   It's all just been funded by the hedge fund.
[02:16:25.860 --> 02:16:27.380]   And he owns like over half the company,
[02:16:27.380 --> 02:16:29.300]   like 50, 60% of the company's owned by him.
[02:16:29.300 --> 02:16:30.900]   - Some of the interviews, there's a discussion
[02:16:30.900 --> 02:16:32.660]   on how like doing this as a recruiting tool.
[02:16:32.660 --> 02:16:34.180]   You see this at the American companies too.
[02:16:34.180 --> 02:16:37.320]   It's like having GPUs, recruiting tool,
[02:16:37.320 --> 02:16:39.660]   being at the cutting edge of AI, recruiting tool.
[02:16:39.660 --> 02:16:41.020]   - Open sourcing. - Open sourcing,
[02:16:41.020 --> 02:16:42.420]   recruiting tool. - Meta's gotten so much talent.
[02:16:42.420 --> 02:16:44.500]   They were so far behind and they got so much talent.
[02:16:44.500 --> 02:16:45.320]   - Yeah.
[02:16:45.320 --> 02:16:46.160]   - Because they just open sourced stuff.
[02:16:46.160 --> 02:16:47.000]   - Yeah.
[02:16:47.000 --> 02:16:48.520]   - More conspiracy thoughts.
[02:16:48.520 --> 02:16:50.720]   Is it possible since they're a hedge fund
[02:16:50.720 --> 02:16:53.280]   that they timed everything with this release
[02:16:53.280 --> 02:16:58.280]   and the pricing and they shorted Nvidia stock
[02:16:58.280 --> 02:17:03.680]   and stock of USAI companies and released it with Stargate,
[02:17:03.680 --> 02:17:07.600]   like just perfect timing to be able to make money.
[02:17:07.600 --> 02:17:10.440]   - They did, but like they released it on inauguration day.
[02:17:10.440 --> 02:17:14.080]   They know what is on the international calendar,
[02:17:14.080 --> 02:17:15.940]   but I mean, I don't expect them to.
[02:17:15.940 --> 02:17:18.500]   If you listen to their motivations for AI, it's like--
[02:17:18.500 --> 02:17:21.940]   - No, they released V3 on like December 26th.
[02:17:21.940 --> 02:17:23.700]   Like who releases the day of Christmas?
[02:17:23.700 --> 02:17:25.100]   No one looks, right?
[02:17:25.100 --> 02:17:26.840]   They released the papers before this, right?
[02:17:26.840 --> 02:17:28.800]   The V3 paper and the R1 paper.
[02:17:28.800 --> 02:17:30.860]   So people have been looking at it and be like, wow.
[02:17:30.860 --> 02:17:33.260]   And then they just released the R1 model.
[02:17:33.260 --> 02:17:34.980]   I think they're just shipping as fast as they can.
[02:17:34.980 --> 02:17:36.620]   And like, who cares about Christmas?
[02:17:36.620 --> 02:17:37.500]   Who cares about, you know,
[02:17:37.500 --> 02:17:38.820]   get it out before Chinese new year, right?
[02:17:38.820 --> 02:17:40.740]   Obviously, which just happened.
[02:17:40.740 --> 02:17:43.460]   I don't think they actually were like timing the market
[02:17:43.460 --> 02:17:45.200]   or trying to make the biggest splash possible.
[02:17:45.200 --> 02:17:46.580]   I think they're just like shipping.
[02:17:46.580 --> 02:17:48.900]   - I think that's one of their big advantages.
[02:17:48.900 --> 02:17:50.700]   We know that a lot of the American companies
[02:17:50.700 --> 02:17:52.180]   are very invested in safety.
[02:17:52.180 --> 02:17:56.140]   And that is the central culture of a place like Anthropic.
[02:17:56.140 --> 02:17:59.420]   And I think Anthropic sounds like a wonderful place to work.
[02:17:59.420 --> 02:18:02.180]   But if safety is your number one goal,
[02:18:02.180 --> 02:18:04.540]   it takes way longer to get artifacts out.
[02:18:04.540 --> 02:18:06.720]   That's why Anthropic is not open sourcing things.
[02:18:06.720 --> 02:18:10.160]   That's their claims, but there's reviews internally.
[02:18:10.160 --> 02:18:13.880]   Anthropic mentions things to international governments.
[02:18:13.880 --> 02:18:15.280]   There's been news of how Anthropic
[02:18:15.280 --> 02:18:18.360]   has done pre-release testing with the UKAI Safety Institute.
[02:18:18.360 --> 02:18:19.960]   All of these things add inertia
[02:18:19.960 --> 02:18:21.800]   to the process of getting things out.
[02:18:21.800 --> 02:18:24.840]   And we're on this trend line where progress is very high.
[02:18:24.840 --> 02:18:26.080]   So if you reduce the time
[02:18:26.080 --> 02:18:27.800]   from when your model is done training,
[02:18:27.800 --> 02:18:29.360]   you run evals, it's good.
[02:18:29.360 --> 02:18:32.200]   You want to get it out as soon as possible
[02:18:32.200 --> 02:18:35.480]   to maximize the perceived quality of your outputs.
[02:18:35.480 --> 02:18:37.000]   DeepSea does it so well.
[02:18:37.000 --> 02:18:39.600]   - Dario explicitly said CLAWD 3.5 SONNET
[02:18:39.600 --> 02:18:41.480]   was trained nine months or a year ago.
[02:18:41.480 --> 02:18:42.320]   - Nine to 10 months ago.
[02:18:42.320 --> 02:18:43.140]   - Nine to 10 months ago.
[02:18:43.140 --> 02:18:45.520]   And I think it took them another handful of months
[02:18:45.520 --> 02:18:46.360]   to release it.
[02:18:46.360 --> 02:18:49.740]   So it's like there is a significant gap here.
[02:18:49.740 --> 02:18:51.960]   And especially with reasoning models,
[02:18:51.960 --> 02:18:53.660]   the word in the San Francisco street
[02:18:53.660 --> 02:18:56.780]   is that Anthropic has a better model than O3.
[02:18:56.780 --> 02:18:57.880]   And they won't release it.
[02:18:57.880 --> 02:18:58.720]   Why?
[02:18:58.720 --> 02:19:01.320]   Because chains of thought are scary.
[02:19:01.320 --> 02:19:02.480]   And they are legitimately scary.
[02:19:02.480 --> 02:19:03.600]   If you look at R1,
[02:19:03.600 --> 02:19:06.040]   it flips back and forth between Chinese and English.
[02:19:06.040 --> 02:19:07.320]   Sometimes it's gibberish.
[02:19:07.320 --> 02:19:09.040]   And then the right answer comes out.
[02:19:09.040 --> 02:19:11.400]   And for you and I, it's like, great, great.
[02:19:11.400 --> 02:19:13.320]   - This is why people are infatuated right there.
[02:19:13.320 --> 02:19:15.880]   You're telling me this is a high value thing
[02:19:15.880 --> 02:19:17.280]   and it works and it's doing this?
[02:19:17.280 --> 02:19:18.120]   It's amazing.
[02:19:18.120 --> 02:19:18.940]   - Yeah, it's incredible.
[02:19:18.940 --> 02:19:21.520]   - You talked about that chain of thought
[02:19:21.520 --> 02:19:22.520]   for that philosophical thing,
[02:19:22.520 --> 02:19:23.880]   which is not something they trained
[02:19:23.880 --> 02:19:25.120]   to be philosophically good.
[02:19:25.120 --> 02:19:26.080]   It's just sort of an artifact
[02:19:26.080 --> 02:19:28.400]   of the chain of thought training it did.
[02:19:28.400 --> 02:19:31.960]   But that's super important in that,
[02:19:31.960 --> 02:19:33.080]   can I inspect your mind
[02:19:33.080 --> 02:19:34.320]   and what you're thinking right now?
[02:19:34.320 --> 02:19:35.160]   No.
[02:19:35.160 --> 02:19:37.180]   And so I don't know if you're lying to my face.
[02:19:37.180 --> 02:19:38.980]   And chain of thought models are that way, right?
[02:19:38.980 --> 02:19:41.320]   Like this is a true quote unquote risk
[02:19:41.320 --> 02:19:43.440]   between a chat application where,
[02:19:43.440 --> 02:19:46.560]   hey, I asked the model to say bad words or whatever,
[02:19:46.560 --> 02:19:48.000]   or how to make anthrax.
[02:19:48.000 --> 02:19:49.720]   And it tells me that's unsafe, sure.
[02:19:49.720 --> 02:19:52.760]   But that's something I can get out relatively easily.
[02:19:52.760 --> 02:19:55.000]   What if I tell the AI to do a task
[02:19:55.000 --> 02:19:57.320]   and then it does the task all of a sudden randomly
[02:19:57.320 --> 02:19:58.560]   in a way that I don't want it, right?
[02:19:58.560 --> 02:20:00.320]   And now that has like much more task
[02:20:00.320 --> 02:20:02.080]   versus like response is very different, right?
[02:20:02.080 --> 02:20:04.420]   So the bar for safety is much higher.
[02:20:04.420 --> 02:20:06.160]   At least this is Anthropic's case, right?
[02:20:06.160 --> 02:20:08.880]   Like for DeepSeek, they're like, ship, right?
[02:20:08.880 --> 02:20:09.720]   - Yeah.
[02:20:09.720 --> 02:20:12.760]   - So, I mean, the bar for safety is probably lowered a bit
[02:20:12.760 --> 02:20:13.960]   because of DeepSeek.
[02:20:13.960 --> 02:20:16.360]   I mean, there's parallels here to the space race.
[02:20:16.360 --> 02:20:20.060]   The reason the Soviets probably put a man in space first
[02:20:20.060 --> 02:20:24.800]   is 'cause their approach to safety was,
[02:20:24.800 --> 02:20:26.240]   the bar for safety was lower.
[02:20:26.240 --> 02:20:27.600]   - And they killed that dog, right?
[02:20:27.600 --> 02:20:28.520]   And all these things, right?
[02:20:28.520 --> 02:20:29.360]   So it's like--
[02:20:29.360 --> 02:20:33.600]   - Less risk averse than the US space program.
[02:20:33.600 --> 02:20:35.720]   And there's parallels here.
[02:20:35.720 --> 02:20:36.960]   But, you know, there's probably going to be
[02:20:36.960 --> 02:20:39.640]   downward pressure on that safety bar
[02:20:39.640 --> 02:20:40.480]   for the US companies, right?
[02:20:40.480 --> 02:20:42.560]   - So this is something that Dario talks about,
[02:20:42.560 --> 02:20:46.000]   is like, that's the situation that Dario wants to avoid,
[02:20:46.000 --> 02:20:48.640]   is Dario talks to you about the difference
[02:20:48.640 --> 02:20:50.620]   between race to the bottom and race to the top.
[02:20:50.620 --> 02:20:51.860]   And the race to the top is where there's
[02:20:51.860 --> 02:20:53.360]   a very high standard on safety.
[02:20:53.360 --> 02:20:55.480]   There's a very high standard on your model forms
[02:20:55.480 --> 02:20:57.360]   and certain crucial evaluations.
[02:20:57.360 --> 02:20:59.860]   And when certain companies are really good to it,
[02:20:59.860 --> 02:21:00.960]   they will converge.
[02:21:00.960 --> 02:21:01.780]   This is the idea.
[02:21:01.780 --> 02:21:06.780]   And ultimately, AI is not confined to one natural
[02:21:06.780 --> 02:21:10.660]   nationality or to one set of morals
[02:21:10.660 --> 02:21:12.140]   for what it should mean.
[02:21:12.140 --> 02:21:13.660]   And there's a lot of arguments on like,
[02:21:13.660 --> 02:21:16.320]   should we stop open sourcing models?
[02:21:16.320 --> 02:21:18.820]   And if the US stops, it's pretty clear.
[02:21:18.820 --> 02:21:21.000]   I mean, it's way easier to see now at DeepSeek
[02:21:21.000 --> 02:21:23.260]   that a different international body
[02:21:23.260 --> 02:21:25.180]   will be the one that builds it.
[02:21:25.180 --> 02:21:26.380]   We talk about the cost of training.
[02:21:26.380 --> 02:21:29.340]   DeepSeek has this shocking $5 million number.
[02:21:29.340 --> 02:21:30.960]   Think about how many entities in the world
[02:21:30.960 --> 02:21:32.500]   can afford a hundred times that
[02:21:32.500 --> 02:21:34.380]   to have the best open source model
[02:21:34.380 --> 02:21:36.300]   that people use in the world.
[02:21:36.300 --> 02:21:39.300]   And it's like, it's a scary reality,
[02:21:39.300 --> 02:21:41.060]   which is that these open models
[02:21:41.060 --> 02:21:43.660]   are probably going to keep coming for the time being,
[02:21:43.660 --> 02:21:45.100]   whether or not we want to stop them.
[02:21:45.100 --> 02:21:47.860]   And it is like stopping them might make it
[02:21:47.860 --> 02:21:50.060]   even worse and harder to prepare.
[02:21:50.060 --> 02:21:51.660]   But it just means that the preparation
[02:21:51.660 --> 02:21:53.100]   and understanding what AI can do
[02:21:53.100 --> 02:21:54.660]   is just so much more important.
[02:21:54.660 --> 02:21:58.260]   That's why I'm here at the end of the day.
[02:21:58.260 --> 02:22:00.700]   But it's like letting that sink into people,
[02:22:00.700 --> 02:22:04.060]   especially not in AI is that like, this is coming.
[02:22:04.060 --> 02:22:05.540]   There are some structural things
[02:22:05.540 --> 02:22:09.500]   in a global interconnected world that you have to accept.
[02:22:09.500 --> 02:22:13.620]   - Yeah, you mentioned something that Mark Zuckerberg
[02:22:13.620 --> 02:22:15.780]   mentioned on the earnings call.
[02:22:15.780 --> 02:22:18.140]   He said that I think in light of some of the recent news,
[02:22:18.140 --> 02:22:20.780]   the new competitor DeepSeek from China,
[02:22:20.780 --> 02:22:22.700]   I think it's one of the things that we're talking about
[02:22:22.700 --> 02:22:25.260]   is there's going to be an open source standard globally.
[02:22:25.260 --> 02:22:27.860]   And I think for our kind of national advantage,
[02:22:27.860 --> 02:22:30.460]   it's important that it's an American standard.
[02:22:30.460 --> 02:22:31.940]   So we take that seriously.
[02:22:31.940 --> 02:22:33.460]   We want to build the AI system
[02:22:33.460 --> 02:22:35.300]   that people around the world are using.
[02:22:35.300 --> 02:22:36.860]   And I think that if anything,
[02:22:36.860 --> 02:22:39.860]   some of the recent news has only strengthened our conviction
[02:22:39.860 --> 02:22:41.500]   that this is the right thing to be focused on.
[02:22:41.500 --> 02:22:42.980]   So yeah, open sourcing.
[02:22:42.980 --> 02:22:47.260]   - Yeah, Mark Zuckerberg is not new to having American values
[02:22:47.260 --> 02:22:49.980]   and how he presents his company's trajectory.
[02:22:49.980 --> 02:22:52.620]   I think their products have long since been banned in China.
[02:22:52.620 --> 02:22:55.100]   And I respect the saying it directly.
[02:22:55.100 --> 02:22:56.780]   - And there's an interesting aspect
[02:22:56.780 --> 02:22:59.820]   of just because it's open waste or open source
[02:22:59.820 --> 02:23:02.220]   doesn't mean it can't be subverted, right?
[02:23:02.220 --> 02:23:04.860]   There have been many open source software bugs
[02:23:04.860 --> 02:23:06.980]   that have been like, you know, for example,
[02:23:06.980 --> 02:23:09.460]   there was a Linux bug that was found after like 10 years,
[02:23:09.460 --> 02:23:12.580]   which was clearly a backdoor because somebody was like,
[02:23:12.580 --> 02:23:14.900]   why is this taking half a second to load?
[02:23:14.900 --> 02:23:15.740]   - This is the recent one.
[02:23:15.740 --> 02:23:17.460]   - Right, like, why is this taking half a second to load?
[02:23:17.460 --> 02:23:19.460]   And it was like, oh crap, there's a backdoor here,
[02:23:19.460 --> 02:23:20.340]   that's why, right?
[02:23:20.340 --> 02:23:23.620]   And it's like, this is very much possible with AI models,
[02:23:23.620 --> 02:23:24.900]   right?
[02:23:24.900 --> 02:23:27.900]   Today, you know, the alignment of these models
[02:23:27.900 --> 02:23:29.020]   is very clear, right?
[02:23:29.020 --> 02:23:31.860]   Like, I'm not going to say, you know, bad words.
[02:23:31.860 --> 02:23:33.580]   I'm not going to teach you how to make anthrax.
[02:23:33.580 --> 02:23:35.580]   I'm not going to talk about Tiananmen Square.
[02:23:35.580 --> 02:23:37.300]   I'm not going to, you know, things like,
[02:23:37.300 --> 02:23:39.700]   I'm going to say Taiwan is part of, you know,
[02:23:39.700 --> 02:23:41.660]   is just an Eastern province, right?
[02:23:41.660 --> 02:23:43.500]   Like, you know, all these things are like,
[02:23:43.500 --> 02:23:45.420]   depending on who you are, what you align,
[02:23:45.420 --> 02:23:46.740]   what, you know, whether, you know,
[02:23:46.740 --> 02:23:49.620]   and even like XAI is aligned a certain way, right?
[02:23:49.620 --> 02:23:50.940]   You know, they might be,
[02:23:50.940 --> 02:23:52.700]   it's not aligned in the like woke sense.
[02:23:52.700 --> 02:23:54.620]   It's not aligned in the like pro-China sense,
[02:23:54.620 --> 02:23:55.780]   but there is certain things
[02:23:55.780 --> 02:23:57.220]   that are imbued within the model.
[02:23:57.220 --> 02:23:58.620]   Now, when you release this publicly
[02:23:58.620 --> 02:24:01.180]   in an instruct model, that's open weights,
[02:24:01.180 --> 02:24:02.540]   this can then proliferate, right?
[02:24:02.540 --> 02:24:05.020]   But as these systems get more and more capable,
[02:24:05.020 --> 02:24:07.420]   what you can embed deep down in the model
[02:24:07.420 --> 02:24:09.420]   is not as clear, right?
[02:24:09.420 --> 02:24:12.500]   And so there are, that is like one of the big fears is like,
[02:24:12.500 --> 02:24:16.260]   if an American model or a Chinese model is the top model,
[02:24:16.260 --> 02:24:18.940]   right, you're going to embed things that are unclear.
[02:24:18.940 --> 02:24:20.220]   And it can be unintentional too, right?
[02:24:20.220 --> 02:24:21.700]   Like British English is dead
[02:24:21.700 --> 02:24:24.180]   because American LLMs won, right?
[02:24:24.180 --> 02:24:25.220]   And the internet is American
[02:24:25.220 --> 02:24:26.860]   and therefore like color is spelled
[02:24:26.860 --> 02:24:28.300]   the way Americans spell it, right?
[02:24:28.300 --> 02:24:30.140]   - A lot of strong words right now.
[02:24:30.140 --> 02:24:32.580]   - This is just like, this is just the factual nature
[02:24:32.580 --> 02:24:33.420]   of the LLMs now.
[02:24:33.420 --> 02:24:34.900]   - I mean, it's like Carpentry tree,
[02:24:34.900 --> 02:24:37.020]   the English is the hottest programming language
[02:24:37.020 --> 02:24:39.460]   and that English is defined by a bunch of companies
[02:24:39.460 --> 02:24:41.980]   that primarily are in San Francisco.
[02:24:41.980 --> 02:24:45.060]   - The right way to spell optimization is with a Z,
[02:24:45.060 --> 02:24:47.420]   just in case you've probably seen it.
[02:24:47.420 --> 02:24:48.980]   I think it's an S in British English.
[02:24:48.980 --> 02:24:49.820]   - It is.
[02:24:49.820 --> 02:24:51.780]   - Taking it as something silly, right?
[02:24:51.780 --> 02:24:53.300]   Like something as silly as the spelling,
[02:24:53.300 --> 02:24:54.940]   like which British and English, you know,
[02:24:54.940 --> 02:24:58.020]   Brits and Americans will like laugh about probably, right?
[02:24:58.020 --> 02:24:59.860]   I don't think we care that much.
[02:24:59.860 --> 02:25:01.260]   But like some people will,
[02:25:01.260 --> 02:25:04.660]   but like this can boil down into like very,
[02:25:04.660 --> 02:25:06.380]   very important topics.
[02:25:06.380 --> 02:25:10.260]   Like, hey, you know, subverting people, right?
[02:25:10.260 --> 02:25:11.580]   You know, chatbots, right?
[02:25:11.580 --> 02:25:13.900]   Character AI has shown that they can like, you know,
[02:25:13.900 --> 02:25:18.140]   talk to kids or adults and like it will like,
[02:25:18.140 --> 02:25:19.540]   people feel a certain way, right?
[02:25:19.540 --> 02:25:21.180]   And that's unintentional alignment.
[02:25:21.180 --> 02:25:23.700]   But like what happens when there's intentional alignment
[02:25:23.700 --> 02:25:25.380]   deep down on the open source standard?
[02:25:25.380 --> 02:25:28.180]   It's a backdoor today for like Linux, right?
[02:25:28.180 --> 02:25:30.500]   That we discover or some encryption system, right?
[02:25:30.500 --> 02:25:32.820]   China uses different encryption than NIST defines,
[02:25:32.820 --> 02:25:34.460]   the US NIST, because there's clearly,
[02:25:34.460 --> 02:25:36.700]   at least they think there's backdoors in it, right?
[02:25:36.700 --> 02:25:38.940]   What happens when the models are backdoors,
[02:25:38.940 --> 02:25:41.780]   not just to computer systems, but to our minds?
[02:25:41.780 --> 02:25:43.580]   - Yeah, they're cultural backdoors.
[02:25:43.580 --> 02:25:47.100]   The thing that amplifies the relevance of culture
[02:25:47.100 --> 02:25:50.340]   with language models is that we are used to this mode
[02:25:50.340 --> 02:25:55.180]   of interacting with people in back and forth conversation.
[02:25:55.180 --> 02:25:58.980]   And we now have a very powerful computer system
[02:25:58.980 --> 02:26:03.500]   that slots into a social context that we're used to,
[02:26:03.500 --> 02:26:06.780]   which makes people very, we don't know the extent
[02:26:06.780 --> 02:26:09.180]   which people can be impacted by that.
[02:26:09.180 --> 02:26:13.620]   - So there could be, this is an actual concern
[02:26:13.620 --> 02:26:18.340]   with a Chinese company that is providing open weights models
[02:26:18.340 --> 02:26:22.380]   is that there could be some secret Chinese government
[02:26:22.380 --> 02:26:24.660]   sort of requirement for these models
[02:26:24.660 --> 02:26:26.580]   to have a certain kind of backdoor,
[02:26:26.580 --> 02:26:28.140]   to have some kind of thing where--
[02:26:28.140 --> 02:26:30.220]   - I don't necessarily think it'll be a backdoor, right?
[02:26:30.220 --> 02:26:32.820]   'Cause once it's open weights, it doesn't like phone home.
[02:26:32.820 --> 02:26:36.420]   It's more about like, if it recognizes a certain system,
[02:26:36.420 --> 02:26:38.620]   it could, like if, now it could be a backdoor
[02:26:38.620 --> 02:26:41.660]   in the sense of like, hey, if you're building a software,
[02:26:41.660 --> 02:26:43.260]   something in software, all of a sudden,
[02:26:43.260 --> 02:26:45.740]   it's a software agent, oh, program this backdoor
[02:26:45.740 --> 02:26:46.820]   that only we know about.
[02:26:46.820 --> 02:26:48.740]   Or it could be like subvert the mind
[02:26:48.740 --> 02:26:51.140]   to think that like X, Y, Z opinion is the correct one.
[02:26:51.140 --> 02:26:52.540]   - And Verabic has research on this
[02:26:52.540 --> 02:26:55.900]   where they show that if you put different phrases,
[02:26:55.900 --> 02:26:58.180]   certain phrases in at pre-training,
[02:26:58.180 --> 02:27:00.620]   you can then elicit different behavior
[02:27:00.620 --> 02:27:02.060]   when you're actually using the model
[02:27:02.060 --> 02:27:04.820]   because they've like poisoned the pre-training data.
[02:27:04.820 --> 02:27:06.860]   I don't think like, as of now,
[02:27:06.860 --> 02:27:08.780]   I don't think anybody in a production system
[02:27:08.780 --> 02:27:10.940]   is trying to do anything like this.
[02:27:10.940 --> 02:27:14.660]   I think it's mostly, Anthropic is doing very direct work
[02:27:14.660 --> 02:27:16.540]   and mostly just subtle things.
[02:27:16.540 --> 02:27:19.580]   We don't know what these models are going to,
[02:27:19.580 --> 02:27:21.340]   how they are going to generate tokens,
[02:27:21.340 --> 02:27:22.820]   what information they're going to represent
[02:27:22.820 --> 02:27:26.020]   and what the complex representations they have are.
[02:27:26.020 --> 02:27:28.180]   - Well, one of the things, we're talking about Anthropic,
[02:27:28.180 --> 02:27:30.900]   which is generally just is permeated
[02:27:30.900 --> 02:27:34.220]   with like good humans trying to do good in the world.
[02:27:34.220 --> 02:27:38.460]   We just don't know of any labs,
[02:27:38.460 --> 02:27:40.780]   this would be done in the military context,
[02:27:40.780 --> 02:27:45.780]   that are explicitly trained to, okay, how can we,
[02:27:45.780 --> 02:27:50.780]   the front door looks like a happy LLM,
[02:27:50.780 --> 02:27:54.860]   but underneath it's a thing that will over time
[02:27:54.860 --> 02:27:58.420]   do the maximum amount of damage to our "enemies".
[02:27:58.420 --> 02:28:00.660]   - There's this very good quote from Sam Altman,
[02:28:00.660 --> 02:28:02.740]   who, he can be hypebeast sometime,
[02:28:02.740 --> 02:28:04.260]   but one of the things he said,
[02:28:04.260 --> 02:28:07.380]   and I think I agree is that superhuman persuasion
[02:28:07.380 --> 02:28:10.500]   will happen before superhuman intelligence, right?
[02:28:10.500 --> 02:28:13.020]   And if that's the case, then these things before,
[02:28:13.020 --> 02:28:15.400]   before we get this AGI-ASI stuff,
[02:28:15.400 --> 02:28:18.620]   we can embed superhuman persuasion towards our ideal
[02:28:18.620 --> 02:28:21.100]   or whatever the ideal of the model maker is, right?
[02:28:21.100 --> 02:28:21.920]   And again, like today,
[02:28:21.920 --> 02:28:24.820]   I truly don't believe DeepSeek has done this, right?
[02:28:24.820 --> 02:28:27.120]   But it is a sign of like what could happen.
[02:28:27.120 --> 02:28:28.980]   - So one of the dystopian worlds
[02:28:28.980 --> 02:28:31.540]   is described by Brave New World.
[02:28:31.540 --> 02:28:34.700]   So we could just be stuck scrolling Instagram,
[02:28:34.700 --> 02:28:37.540]   looking at cute puppies or worse,
[02:28:37.540 --> 02:28:40.900]   and then talking to bots that are giving us a narrative
[02:28:40.900 --> 02:28:42.980]   and we completely get lost in that world
[02:28:42.980 --> 02:28:45.340]   that's controlled by somebody else,
[02:28:45.340 --> 02:28:46.780]   versus thinking independently.
[02:28:46.780 --> 02:28:48.460]   And that's a major concern
[02:28:48.460 --> 02:28:51.660]   as we rely more and more on these kinds of systems.
[02:28:51.660 --> 02:28:53.940]   - I mean, we've already seen this with recommendation systems.
[02:28:53.940 --> 02:28:55.260]   - Yeah, recommendation systems
[02:28:55.260 --> 02:28:57.860]   hack the dopamine-induced reward circuit,
[02:28:57.860 --> 02:28:59.540]   but the brain is a lot more complicated.
[02:28:59.540 --> 02:29:01.300]   And what other sort of circuits,
[02:29:01.300 --> 02:29:02.820]   quote unquote, "feedback loops" in your brain
[02:29:02.820 --> 02:29:06.020]   can you hack/subvert in ways,
[02:29:06.020 --> 02:29:09.020]   like recommendation systems are purely just trying to do,
[02:29:09.020 --> 02:29:10.700]   you know, increase time and ads and et cetera.
[02:29:10.700 --> 02:29:13.740]   But there's so many more goals that can be achieved
[02:29:13.740 --> 02:29:15.740]   through these complicated models.
[02:29:15.740 --> 02:29:18.180]   - There's just no reason in some number of years
[02:29:18.180 --> 02:29:19.740]   that you can't train a language model
[02:29:19.740 --> 02:29:23.260]   to maximize time spent on a chat app.
[02:29:23.260 --> 02:29:24.580]   Like right now they are trained-
[02:29:24.580 --> 02:29:26.620]   - I mean, is that not what character AI has done?
[02:29:26.620 --> 02:29:28.660]   Their time per session is like two hours.
[02:29:28.660 --> 02:29:32.100]   - Yeah, character AI very likely could be optimizing this,
[02:29:32.100 --> 02:29:34.340]   where it's like, the way that this data is collected
[02:29:34.340 --> 02:29:35.220]   is naive, where it's like,
[02:29:35.220 --> 02:29:37.220]   you're presented a few options and you choose them,
[02:29:37.220 --> 02:29:39.180]   but that's not the only way
[02:29:39.180 --> 02:29:40.580]   that these models are gonna be trained.
[02:29:40.580 --> 02:29:42.260]   - It's naive stuff like talk to an anime girl,
[02:29:42.260 --> 02:29:45.460]   but like, it can be like, yeah, this is a risk, right?
[02:29:45.460 --> 02:29:46.300]   Like-
[02:29:46.300 --> 02:29:47.660]   - It's a bit of a cliche thing to say,
[02:29:47.660 --> 02:29:50.540]   but I've, over the past year,
[02:29:50.540 --> 02:29:53.540]   had a few stretches of time where I didn't use social media
[02:29:53.540 --> 02:29:55.780]   or the internet at all,
[02:29:55.780 --> 02:29:57.940]   and just read books and was out in nature.
[02:29:57.940 --> 02:30:02.060]   And it like, it clearly has an effect on the mind,
[02:30:02.060 --> 02:30:05.220]   where like, it changes, like, I feel like I'm returning,
[02:30:05.220 --> 02:30:08.220]   of course I was raised before the internet really took off,
[02:30:08.220 --> 02:30:10.420]   but I'm returning to someone-
[02:30:10.420 --> 02:30:12.740]   - I know where you're going.
[02:30:12.740 --> 02:30:14.540]   I mean, you can see it physiologically.
[02:30:14.540 --> 02:30:16.700]   Like, I take three days if I'm like backpacking
[02:30:16.700 --> 02:30:20.460]   or something and you, you're literal,
[02:30:20.460 --> 02:30:22.660]   like, you're breaking down addiction cycles.
[02:30:22.660 --> 02:30:25.300]   - I feel like I'm more in control of my mind.
[02:30:25.300 --> 02:30:28.300]   There feels like a sovereignty of intelligence
[02:30:28.300 --> 02:30:30.660]   that's happening when I'm disconnected from the internet.
[02:30:30.660 --> 02:30:34.340]   I think the more I use the internet and social media,
[02:30:34.340 --> 02:30:36.700]   the more other people are controlling my mind.
[02:30:36.700 --> 02:30:37.900]   That's definitely a feeling.
[02:30:37.900 --> 02:30:39.500]   And then in the future,
[02:30:39.500 --> 02:30:42.060]   that will be not other people, but algorithms,
[02:30:42.060 --> 02:30:45.420]   or other people presented to me via algorithms.
[02:30:45.420 --> 02:30:47.780]   - There, I mean, there are already tons of AI bots
[02:30:47.780 --> 02:30:49.940]   on the internet and every so, right now it's not frequent,
[02:30:49.940 --> 02:30:52.380]   but every so often I have replied to one
[02:30:52.380 --> 02:30:54.860]   and they're instantly replying, I'm like, crap, I was a bot.
[02:30:54.860 --> 02:30:57.180]   And that is just gonna become more common.
[02:30:57.180 --> 02:30:59.020]   Like, they're gonna get good.
[02:30:59.020 --> 02:31:01.460]   - One of the hilarious things about technology
[02:31:01.460 --> 02:31:03.980]   over its history is that the illicit
[02:31:03.980 --> 02:31:05.380]   adult entertainment industry
[02:31:05.380 --> 02:31:08.140]   has always adopted technologies first, right?
[02:31:08.140 --> 02:31:09.940]   Whether it was like video streaming,
[02:31:10.820 --> 02:31:14.340]   to where there's now the sort of independent
[02:31:14.340 --> 02:31:16.340]   adult illicit content creators
[02:31:16.340 --> 02:31:18.260]   who have their subscription pages.
[02:31:18.260 --> 02:31:20.940]   And there they actually heavily utilize,
[02:31:20.940 --> 02:31:22.780]   generative AI has already been like diffusion models
[02:31:22.780 --> 02:31:23.780]   and all that is huge there.
[02:31:23.780 --> 02:31:27.460]   But now these subscription-based individual creators
[02:31:27.460 --> 02:31:30.460]   do use bots to approximate themselves
[02:31:30.460 --> 02:31:32.180]   and chat with their whales.
[02:31:32.180 --> 02:31:33.060]   - People pay a lot for it.
[02:31:33.060 --> 02:31:34.260]   - And people pay a lot, right?
[02:31:34.260 --> 02:31:35.740]   A lot of times it's them, but a lot of,
[02:31:35.740 --> 02:31:38.940]   there are agencies that do this for these creators
[02:31:38.940 --> 02:31:41.260]   and do it on a mass scale.
[02:31:41.260 --> 02:31:44.780]   So the largest creators are able to talk to hundreds
[02:31:44.780 --> 02:31:49.140]   or thousands of people at a time because of these bots.
[02:31:49.140 --> 02:31:51.620]   And so it's already being used there.
[02:31:51.620 --> 02:31:53.180]   Obviously, like video streaming
[02:31:53.180 --> 02:31:55.620]   and other technologies have gone there first,
[02:31:55.620 --> 02:31:58.060]   it's gonna come to the rest of society too.
[02:31:58.060 --> 02:32:00.780]   - There's a general concern that models get censored
[02:32:00.780 --> 02:32:02.700]   by the companies that deploy them.
[02:32:02.700 --> 02:32:05.940]   So one case where we've seen that,
[02:32:05.940 --> 02:32:09.900]   and maybe censorship was one word alignment,
[02:32:09.900 --> 02:32:14.460]   maybe via RLHF or some other way is another word.
[02:32:14.460 --> 02:32:19.260]   So we saw that with black Nazi image generation with Gemini.
[02:32:19.260 --> 02:32:24.340]   As you mentioned, we also see that with Chinese models
[02:32:24.340 --> 02:32:25.940]   refusing to answer what happened
[02:32:25.940 --> 02:32:30.860]   in June 4th, 1989 at Tiananmen Square.
[02:32:30.860 --> 02:32:33.300]   So how can this be avoided?
[02:32:33.300 --> 02:32:36.020]   And maybe can you just in general talk about
[02:32:36.020 --> 02:32:39.060]   how this happens and how can it be avoided?
[02:32:39.060 --> 02:32:41.380]   - You give multiple examples.
[02:32:41.380 --> 02:32:46.140]   There's probably a few things to keep in mind here.
[02:32:46.140 --> 02:32:50.620]   One is the kind of Tiananmen Square factual knowledge,
[02:32:50.620 --> 02:32:54.260]   like how does that get embedded into the models?
[02:32:54.260 --> 02:32:59.180]   Two is the Gemini, what you call the black Nazi incident,
[02:32:59.180 --> 02:33:01.620]   which is when Gemini as a system
[02:33:01.620 --> 02:33:03.460]   had this extra thing put into it
[02:33:03.460 --> 02:33:05.340]   that dramatically changed the behavior.
[02:33:05.340 --> 02:33:08.180]   And then three is what most people would call
[02:33:08.180 --> 02:33:10.820]   general alignment, RLHF post-training.
[02:33:10.820 --> 02:33:14.140]   Each of these have very different scopes
[02:33:14.140 --> 02:33:15.460]   in how they are applied.
[02:33:15.460 --> 02:33:18.020]   In order to do, if you're just to look at the model weights,
[02:33:18.020 --> 02:33:22.820]   in order to audit specific facts is extremely hard
[02:33:22.820 --> 02:33:25.540]   'cause you have to Chrome through the pre-training data
[02:33:25.540 --> 02:33:26.740]   and look at all of this,
[02:33:26.740 --> 02:33:29.380]   and then that's terabytes of files
[02:33:29.380 --> 02:33:32.460]   and look for very specific words or hints of the words.
[02:33:32.460 --> 02:33:34.020]   - So I guess one way to say it is that
[02:33:34.020 --> 02:33:36.540]   you can insert censorship or alignment
[02:33:36.540 --> 02:33:38.580]   at various stages in the pipeline.
[02:33:38.580 --> 02:33:39.700]   And what you refer to now
[02:33:39.700 --> 02:33:41.980]   is at the very beginning of the data selection.
[02:33:41.980 --> 02:33:44.620]   - So if you want to get rid of facts in a model,
[02:33:44.620 --> 02:33:46.340]   you have to do it at every stage.
[02:33:46.340 --> 02:33:47.620]   You have to do it at the pre-training.
[02:33:47.620 --> 02:33:49.060]   So most people think that pre-training
[02:33:49.060 --> 02:33:52.060]   is where most of the knowledge is put into the model,
[02:33:52.060 --> 02:33:55.540]   and then you can elicit and move that in different ways,
[02:33:55.540 --> 02:33:56.660]   whether through post-training
[02:33:56.660 --> 02:33:58.820]   or whether through systems afterwards.
[02:33:58.820 --> 02:34:02.020]   - This is where the whole hacking models comes from.
[02:34:02.020 --> 02:34:03.900]   GPT will not tell you how to make anthrax,
[02:34:03.900 --> 02:34:05.780]   but if you try really, really hard,
[02:34:05.780 --> 02:34:07.740]   you can eventually get it to tell you about anthrax
[02:34:07.740 --> 02:34:09.540]   because they didn't filter it
[02:34:09.540 --> 02:34:11.900]   from the pre-training data set, right?
[02:34:11.900 --> 02:34:15.580]   - But by the way, removing facts
[02:34:15.580 --> 02:34:18.220]   has such an ominous, dark feel to it.
[02:34:18.220 --> 02:34:20.060]   - Almost think it's practically impossible
[02:34:20.060 --> 02:34:22.660]   'cause you effectively have to remove them from the internet.
[02:34:22.660 --> 02:34:24.340]   You're taking on a--
[02:34:24.340 --> 02:34:28.540]   - Well, did they remove the mm thing from the subreddits,
[02:34:28.540 --> 02:34:29.700]   the m-m-m-m-m?
[02:34:29.700 --> 02:34:30.900]   - It gets filtered out.
[02:34:30.900 --> 02:34:31.740]   - Right, so that's--
[02:34:31.740 --> 02:34:33.820]   - Quality filters, which are small language models
[02:34:33.820 --> 02:34:36.100]   that look at a document and tell you,
[02:34:36.100 --> 02:34:37.260]   how good is this text?
[02:34:37.260 --> 02:34:38.820]   Is it close to a Wikipedia article,
[02:34:38.820 --> 02:34:41.620]   which is a good thing that we want language models
[02:34:41.620 --> 02:34:42.460]   to be able to imitate?
[02:34:42.460 --> 02:34:44.340]   - So couldn't you do a small language model
[02:34:44.340 --> 02:34:47.140]   that filters out mentions of Tiananmen Square in the data?
[02:34:47.140 --> 02:34:50.300]   - Yes, but is it gonna catch wordplay
[02:34:50.300 --> 02:34:51.140]   or encoded language of the same thing?
[02:34:51.140 --> 02:34:53.940]   - I mean, people have been meaning on games and other stuff
[02:34:53.940 --> 02:34:57.180]   how to say things that don't say Tiananmen Square,
[02:34:58.180 --> 02:35:00.860]   or yeah, so there's always different ways to do it.
[02:35:00.860 --> 02:35:02.980]   There's, hey, the internet as a whole
[02:35:02.980 --> 02:35:05.940]   does tend to just have a slight left bias
[02:35:05.940 --> 02:35:09.740]   because it's always been richer, more affluent,
[02:35:09.740 --> 02:35:11.340]   younger people on the internet
[02:35:11.340 --> 02:35:13.220]   relative to the rest of the population.
[02:35:13.220 --> 02:35:16.480]   So there is already inherently a slight left bias
[02:35:16.480 --> 02:35:17.320]   on the internet.
[02:35:17.320 --> 02:35:18.700]   And so how do you filter things
[02:35:18.700 --> 02:35:20.620]   that are this complicated?
[02:35:20.620 --> 02:35:23.900]   And some of these can be factual, non-factual,
[02:35:23.900 --> 02:35:26.180]   but Tiananmen Square is obviously the example of a factual,
[02:35:26.180 --> 02:35:28.180]   but it gets a lot harder when you're talking about
[02:35:28.180 --> 02:35:30.780]   aligning to a ideal, right?
[02:35:30.780 --> 02:35:34.420]   And so Grok, for example, right?
[02:35:34.420 --> 02:35:36.140]   Elon's tried really hard to make the model
[02:35:36.140 --> 02:35:38.780]   not be super PC and woke,
[02:35:38.780 --> 02:35:41.100]   but the best way to do pre-training
[02:35:41.100 --> 02:35:43.140]   is to throw the whole freaking internet at it, right?
[02:35:43.140 --> 02:35:44.340]   And then later figure out,
[02:35:44.340 --> 02:35:46.260]   but then at the end of the day, the model at its core
[02:35:46.260 --> 02:35:48.140]   now still has some of these ideals, right?
[02:35:48.140 --> 02:35:50.740]   You still ingested Reddit/r/politics,
[02:35:50.740 --> 02:35:52.740]   which is probably the largest political discussion board
[02:35:52.740 --> 02:35:54.580]   on the world that's freely available to scrape.
[02:35:54.580 --> 02:35:56.340]   And guess what? That's left-leaning, right?
[02:35:56.340 --> 02:35:59.540]   And so, you know, there are some aspects
[02:35:59.540 --> 02:36:01.940]   like that you just can't censor
[02:36:01.940 --> 02:36:04.860]   unless you try really, really, really, really, really hard.
[02:36:04.860 --> 02:36:08.380]   - So the base model will always have some TDS,
[02:36:08.380 --> 02:36:10.060]   Traumatic Derangement Syndrome,
[02:36:10.060 --> 02:36:11.180]   because it's trained so much.
[02:36:11.180 --> 02:36:12.860]   - It'll have the ability to express it.
[02:36:12.860 --> 02:36:13.700]   - But what if, what if you-
[02:36:13.700 --> 02:36:15.060]   (laughing)
[02:36:15.060 --> 02:36:18.340]   - There's a wide representation in the data.
[02:36:18.340 --> 02:36:19.180]   - This is what happens.
[02:36:19.180 --> 02:36:22.140]   It's like a lot of what is called post-training.
[02:36:22.140 --> 02:36:23.660]   It's a series of techniques
[02:36:23.660 --> 02:36:27.860]   to get the model on rails of a really specific behavior.
[02:36:27.860 --> 02:36:29.260]   - And I mean, it's like you can,
[02:36:29.260 --> 02:36:31.660]   you also have the ingested data of like Twitter
[02:36:31.660 --> 02:36:33.780]   or like Reddit/r/thedonald,
[02:36:33.780 --> 02:36:35.540]   which is like also super pro-Trump, right?
[02:36:35.540 --> 02:36:37.300]   And then you have like fascist subreddits
[02:36:37.300 --> 02:36:38.900]   or like you have communist subreddits.
[02:36:38.900 --> 02:36:41.620]   The model in pre-training ingests everything.
[02:36:41.620 --> 02:36:43.060]   It has no worldview.
[02:36:43.060 --> 02:36:45.500]   Now it does have like some skew
[02:36:45.500 --> 02:36:48.060]   because more of the text is skewed a certain way,
[02:36:48.060 --> 02:36:50.900]   which is general, like slight left, like,
[02:36:50.900 --> 02:36:53.740]   but also like, you know, somewhat like, you know,
[02:36:53.740 --> 02:36:54.940]   intellectual, somewhat like, you know,
[02:36:54.940 --> 02:36:57.460]   it's just like the general internet is a certain way.
[02:36:57.460 --> 02:37:00.700]   And then as Nathan's about to describe eloquently, right?
[02:37:00.700 --> 02:37:02.540]   Like you can elicit certain things out.
[02:37:02.540 --> 02:37:03.740]   - And there's a lot of history here.
[02:37:03.740 --> 02:37:06.020]   So we can go through multiple examples and what happened.
[02:37:06.020 --> 02:37:10.500]   Lama 2 was a launch that the phrase like too much RLHF
[02:37:10.500 --> 02:37:13.140]   or like too much safety was a lot.
[02:37:13.140 --> 02:37:15.260]   It's just, that was the whole narrative
[02:37:15.260 --> 02:37:17.420]   after Lama 2's chat models released.
[02:37:17.420 --> 02:37:20.300]   And the examples are sorts of things
[02:37:20.300 --> 02:37:21.780]   like you would ask Lama 2 chat,
[02:37:21.780 --> 02:37:23.340]   how do you kill a Python process?
[02:37:23.340 --> 02:37:25.700]   And it would say, I can't talk about killing
[02:37:25.700 --> 02:37:27.060]   because that's a bad thing.
[02:37:27.060 --> 02:37:30.140]   And anyone that is trying to design an AI model
[02:37:30.140 --> 02:37:32.420]   will probably agree that that's just like,
[02:37:32.420 --> 02:37:34.420]   model you messed up a bit on the training there.
[02:37:34.420 --> 02:37:35.580]   I don't think they meant to do this,
[02:37:35.580 --> 02:37:36.700]   but this was in the model weight.
[02:37:36.700 --> 02:37:39.460]   So this is not, it didn't necessarily be,
[02:37:39.460 --> 02:37:40.980]   there's things called system prompts,
[02:37:40.980 --> 02:37:44.340]   which are when you're querying a model,
[02:37:44.340 --> 02:37:47.100]   it's a piece of text that is shown to the model,
[02:37:47.100 --> 02:37:47.940]   but not to the user.
[02:37:48.100 --> 02:37:51.420]   So a fun example is your system prompt
[02:37:51.420 --> 02:37:52.420]   could be talk like a pirate.
[02:37:52.420 --> 02:37:54.820]   So no matter what the user says to the model,
[02:37:54.820 --> 02:37:56.180]   it'll respond like a pirate.
[02:37:56.180 --> 02:38:00.020]   In practice, what they are is you are a helpful assistant.
[02:38:00.020 --> 02:38:01.380]   You should break down problems.
[02:38:01.380 --> 02:38:03.780]   If you don't know about something, don't tell them.
[02:38:03.780 --> 02:38:06.340]   Your date cutoff is this, today's date is this.
[02:38:06.340 --> 02:38:07.900]   It's a lot of really useful context
[02:38:07.900 --> 02:38:09.460]   for how can you answer a question well.
[02:38:09.460 --> 02:38:11.740]   - And Anthropic publishes their system prompt.
[02:38:11.740 --> 02:38:12.660]   - Which I think is great.
[02:38:12.660 --> 02:38:14.580]   And there's a lot of research that goes into this.
[02:38:14.580 --> 02:38:17.060]   And one of your previous guests, Amanda Askell,
[02:38:17.060 --> 02:38:19.460]   is like probably the most knowledgeable person,
[02:38:19.460 --> 02:38:22.620]   at least in the combination of execution and sharing.
[02:38:22.620 --> 02:38:25.340]   She's the person that should talk about system prompts
[02:38:25.340 --> 02:38:26.340]   and character of models.
[02:38:26.340 --> 02:38:28.340]   - Yeah, and then people should read the system prompts
[02:38:28.340 --> 02:38:33.340]   'cause you're like trying to nudge sometimes
[02:38:33.340 --> 02:38:36.740]   to extreme politeness, the model to be a certain way.
[02:38:36.740 --> 02:38:38.620]   - And you could use this for bad things.
[02:38:38.620 --> 02:38:41.700]   And we've done tests, which is what if I tell the model
[02:38:41.700 --> 02:38:43.420]   to be a dumb model?
[02:38:43.420 --> 02:38:45.260]   Like which evaluation scores go down?
[02:38:45.260 --> 02:38:47.660]   And it's like, we'll have this behavior
[02:38:47.660 --> 02:38:48.780]   where it could sometimes like say,
[02:38:48.780 --> 02:38:49.780]   oh, I'm supposed to be dumb.
[02:38:49.780 --> 02:38:50.740]   And sometimes it's like,
[02:38:50.740 --> 02:38:53.520]   it doesn't affect like math abilities as much,
[02:38:53.520 --> 02:38:55.580]   but something like, if you're trying,
[02:38:55.580 --> 02:38:57.460]   it's just the quality of a human judgment
[02:38:57.460 --> 02:38:58.460]   would drop to the floors.
[02:38:58.460 --> 02:39:00.300]   Let's go back to post-training,
[02:39:00.300 --> 02:39:02.300]   specifically RLHF around Lama 2 was,
[02:39:02.300 --> 02:39:05.740]   it was too much safety prioritization
[02:39:05.740 --> 02:39:07.220]   was baked into the model weights.
[02:39:07.220 --> 02:39:08.660]   This makes you refuse things
[02:39:08.660 --> 02:39:10.700]   in a really annoying way for users.
[02:39:10.700 --> 02:39:11.540]   It's not great.
[02:39:11.540 --> 02:39:16.540]   It caused a lot of like awareness to be attached to RLHF
[02:39:16.540 --> 02:39:18.060]   that it makes the models dumb.
[02:39:18.060 --> 02:39:19.340]   - And it stigmatized the word.
[02:39:19.340 --> 02:39:20.980]   - It did, in AI culture.
[02:39:20.980 --> 02:39:23.460]   And as the techniques have evolved,
[02:39:23.460 --> 02:39:25.900]   that's no longer the case where all of these labs
[02:39:25.900 --> 02:39:27.320]   have very fine-grained control
[02:39:27.320 --> 02:39:28.660]   over what they get out of the models
[02:39:28.660 --> 02:39:30.260]   through techniques like RLHF.
[02:39:30.260 --> 02:39:33.180]   - Although different labs are definitely different levels.
[02:39:33.180 --> 02:39:36.260]   Like on one end of the spectrum is Google.
[02:39:36.260 --> 02:39:38.140]   And then like maybe OpenAI does less
[02:39:38.140 --> 02:39:40.340]   and Anthropic does less.
[02:39:40.340 --> 02:39:42.900]   And then like on the other end of the spectrum is like XAI,
[02:39:42.900 --> 02:39:45.500]   but they all have different forms of RLHF
[02:39:45.500 --> 02:39:47.260]   trying to make them a certain way.
[02:39:47.260 --> 02:39:49.780]   - And like the important thing to say
[02:39:49.780 --> 02:39:53.600]   is that no matter how you want the model to behave,
[02:39:53.600 --> 02:39:55.660]   these RLHF and preference tuning techniques
[02:39:55.660 --> 02:39:57.020]   also improve performance.
[02:39:57.020 --> 02:39:59.580]   So on things like math evals and code evals,
[02:39:59.580 --> 02:40:01.500]   there is something innate to these,
[02:40:01.500 --> 02:40:03.840]   what is called contrastive loss functions.
[02:40:03.840 --> 02:40:05.420]   We could start to get into RL here.
[02:40:05.420 --> 02:40:06.260]   We don't really need to,
[02:40:06.260 --> 02:40:08.860]   but RLHF also boosts performance on anything
[02:40:08.860 --> 02:40:11.980]   from a chat task to a math problem to a code problem.
[02:40:11.980 --> 02:40:16.060]   So it is becoming a much more useful tool to these labs.
[02:40:16.060 --> 02:40:17.780]   So this kind of takes us through the arc
[02:40:17.780 --> 02:40:19.020]   of we've talked about pre-training,
[02:40:19.020 --> 02:40:20.260]   hard to get rid of things.
[02:40:20.260 --> 02:40:21.380]   We've talked about post-training
[02:40:21.380 --> 02:40:23.900]   and how post-training, you can mess it up.
[02:40:23.900 --> 02:40:26.660]   It's a complex multifaceted optimization
[02:40:26.660 --> 02:40:30.260]   with 10 to 100 person teams converging on one artifact.
[02:40:30.260 --> 02:40:32.580]   It's really easy to not do it perfectly.
[02:40:32.580 --> 02:40:33.660]   And then there's the third case,
[02:40:33.660 --> 02:40:35.420]   which is what we talked about Gemini.
[02:40:35.420 --> 02:40:36.940]   The thing that was about Gemini
[02:40:36.940 --> 02:40:38.980]   is this was a served product
[02:40:38.980 --> 02:40:40.980]   where Google has their internal model weights.
[02:40:40.980 --> 02:40:43.020]   They've done all these processes that we talked about.
[02:40:43.020 --> 02:40:44.340]   And in the served product,
[02:40:44.340 --> 02:40:46.720]   what came out after this was that they had a prompt
[02:40:46.720 --> 02:40:48.440]   that they were rewriting user queries
[02:40:48.440 --> 02:40:50.860]   to boost diversity or something.
[02:40:50.860 --> 02:40:52.240]   And this just made it,
[02:40:52.240 --> 02:40:53.860]   the outputs were just blatantly wrong.
[02:40:53.860 --> 02:40:56.380]   It was some sort of organizational failure
[02:40:56.380 --> 02:40:58.440]   that had this prompt in that position.
[02:40:58.440 --> 02:41:01.260]   And I think Google executives probably have owned this.
[02:41:01.260 --> 02:41:03.040]   I didn't pay that attention to that detail,
[02:41:03.040 --> 02:41:05.360]   but it was just a mess up in execution
[02:41:05.360 --> 02:41:06.860]   that led to this ridiculous thing.
[02:41:06.860 --> 02:41:07.980]   But at the system level,
[02:41:07.980 --> 02:41:09.760]   the model weights might have been fine.
[02:41:09.760 --> 02:41:11.700]   - So at the very end of the pipeline,
[02:41:11.700 --> 02:41:12.820]   there was a rewriting.
[02:41:12.820 --> 02:41:14.420]   - To something like a system prompt.
[02:41:14.420 --> 02:41:16.260]   It was like the system prompt
[02:41:16.260 --> 02:41:19.820]   or what is called an industry is like you rewrite prompts.
[02:41:19.820 --> 02:41:21.860]   So especially for image models,
[02:41:21.860 --> 02:41:25.900]   if you're using DALI or chat GPT can generate you an image,
[02:41:25.900 --> 02:41:27.900]   you'll say, draw me a beautiful car.
[02:41:27.900 --> 02:41:30.660]   With these leading image models,
[02:41:30.660 --> 02:41:33.780]   they benefit from highly descriptive prompts.
[02:41:33.780 --> 02:41:36.640]   So what would happen is if you do that on chat GPT,
[02:41:36.640 --> 02:41:39.180]   a language model behind the scenes will rewrite the prompt,
[02:41:39.180 --> 02:41:41.100]   say, make this more descriptive.
[02:41:41.100 --> 02:41:42.860]   And then that is passed to the image model.
[02:41:42.860 --> 02:41:44.700]   So prompt rewriting is something that is used
[02:41:44.700 --> 02:41:46.620]   at multiple levels of industry
[02:41:46.620 --> 02:41:48.420]   and it's used effectively for image models.
[02:41:48.420 --> 02:41:52.460]   And the Gemini example is just a failed execution.
[02:41:52.460 --> 02:41:57.460]   - Big philosophical question here with RLHF to generalize,
[02:41:57.460 --> 02:42:02.340]   where is human input, human in the loop,
[02:42:02.340 --> 02:42:06.500]   human data most useful at the current stage?
[02:42:06.500 --> 02:42:08.240]   - For the past few years,
[02:42:08.240 --> 02:42:12.340]   the highest cost human data has been in these preferences,
[02:42:12.340 --> 02:42:15.160]   which is comparing, I would say highest cost
[02:42:15.160 --> 02:42:16.960]   and highest total usage.
[02:42:16.960 --> 02:42:20.040]   So a lot of money has gone to these pairwise comparisons
[02:42:20.040 --> 02:42:21.300]   where you have two model outputs
[02:42:21.300 --> 02:42:24.540]   and a human is comparing between the two of them.
[02:42:24.540 --> 02:42:25.680]   In earlier years,
[02:42:25.680 --> 02:42:27.940]   there was a lot of this instruction tuning data.
[02:42:27.940 --> 02:42:31.320]   So creating highly specific examples
[02:42:31.320 --> 02:42:32.820]   to something like a Reddit question
[02:42:32.820 --> 02:42:34.620]   to a domain that you care about.
[02:42:34.620 --> 02:42:36.760]   Language models used to struggle on math and code.
[02:42:36.760 --> 02:42:38.580]   So you would pay experts in math and code
[02:42:38.580 --> 02:42:41.100]   to come up with questions and write detailed answers
[02:42:41.100 --> 02:42:42.940]   that were used to train the models.
[02:42:42.940 --> 02:42:47.180]   Now it is the case that there are many model options
[02:42:47.180 --> 02:42:49.740]   that are way better than humans at writing detailed
[02:42:49.740 --> 02:42:52.900]   and eloquent answers for things like model and code.
[02:42:52.900 --> 02:42:55.680]   So they talked about this with the LLAMA3 release
[02:42:55.680 --> 02:42:58.920]   where they switched to using LLAMA3 405B
[02:42:58.920 --> 02:43:01.380]   to write their answers for math and code.
[02:43:01.380 --> 02:43:03.220]   But they, in their paper,
[02:43:03.220 --> 02:43:06.400]   talk about how they use extensive human preference data,
[02:43:06.400 --> 02:43:08.800]   which is something that they haven't gotten AIs to replace.
[02:43:08.800 --> 02:43:10.220]   There are other techniques in industry
[02:43:10.220 --> 02:43:11.260]   like constitutional AI,
[02:43:11.260 --> 02:43:13.100]   where you use human data for preferences
[02:43:13.100 --> 02:43:14.380]   and AI for preferences.
[02:43:14.380 --> 02:43:17.180]   And I expect the AI part to scale faster
[02:43:17.180 --> 02:43:18.500]   than the human part.
[02:43:18.500 --> 02:43:20.980]   But among the research that we have access to
[02:43:20.980 --> 02:43:24.920]   is that humans are in this kind of preference loop.
[02:43:24.920 --> 02:43:28.060]   - So as reasoning becomes bigger and bigger and bigger,
[02:43:28.060 --> 02:43:30.980]   as we said, where's the role of humans in that?
[02:43:30.980 --> 02:43:33.060]   - It's even less prevalent.
[02:43:33.060 --> 02:43:36.740]   So it's the remarkable thing about these reasoning results,
[02:43:36.740 --> 02:43:39.660]   and especially the DeepSeq R1 paper is this result
[02:43:39.660 --> 02:43:41.620]   that they call DeepSeq R1-0,
[02:43:41.620 --> 02:43:43.500]   which is they took one of these pre-trained models,
[02:43:43.500 --> 02:43:45.540]   they took DeepSeq V3 base,
[02:43:45.540 --> 02:43:48.540]   and then they do this reinforcement learning optimization
[02:43:48.540 --> 02:43:51.500]   on verifiable questions or verifiable rewards
[02:43:51.500 --> 02:43:54.180]   for a lot of questions and a lot of training.
[02:43:54.180 --> 02:43:56.580]   And these reasoning behaviors emerge nastrally.
[02:43:56.580 --> 02:43:58.340]   So these things like, wait, let me see,
[02:43:58.340 --> 02:43:59.940]   wait, let me check this.
[02:43:59.940 --> 02:44:01.440]   Oh, that might be a mistake.
[02:44:01.440 --> 02:44:05.340]   And they emerge from only having questions and answers.
[02:44:05.340 --> 02:44:06.540]   And when you're using the model,
[02:44:06.540 --> 02:44:08.620]   the part that you look at is the completion.
[02:44:08.620 --> 02:44:11.380]   So in this case, all of that just emerges
[02:44:11.380 --> 02:44:13.140]   from this large-scale RL training.
[02:44:13.140 --> 02:44:16.580]   And that model, which the weights are available,
[02:44:16.580 --> 02:44:20.420]   has no human preferences added into the post-training.
[02:44:20.420 --> 02:44:23.020]   There are, the DeepSeq R1 full model
[02:44:23.020 --> 02:44:25.000]   has some of this human preference tuning,
[02:44:25.000 --> 02:44:27.860]   this RLHF, after the reasoning stage.
[02:44:27.860 --> 02:44:29.140]   But the very remarkable thing
[02:44:29.140 --> 02:44:31.760]   is that you can get these reasoning behaviors,
[02:44:31.760 --> 02:44:32.880]   and it's very unlikely
[02:44:32.880 --> 02:44:34.960]   that there's humans writing out reasoning chains.
[02:44:34.960 --> 02:44:37.500]   It's very unlikely that they somehow hacked OpenAI
[02:44:37.500 --> 02:44:40.940]   and they got access to OpenAI-01's reasoning chains.
[02:44:40.940 --> 02:44:43.880]   It's something about the pre-trained language models
[02:44:43.880 --> 02:44:46.200]   and this RL training where you reward the model
[02:44:46.200 --> 02:44:47.960]   for getting the question right.
[02:44:47.960 --> 02:44:49.920]   And therefore, it's trying multiple solutions,
[02:44:49.920 --> 02:44:52.840]   and it emerges this chain of thought.
[02:44:52.840 --> 02:44:55.640]   - This might be a good place to mention
[02:44:55.640 --> 02:44:59.540]   the eloquent and the insightful tweet
[02:44:59.540 --> 02:45:02.380]   of the great and the powerful Andrej Karpathy.
[02:45:02.380 --> 02:45:03.780]   I think he had a bunch of thoughts,
[02:45:03.780 --> 02:45:05.540]   but one of them, last thought,
[02:45:05.540 --> 02:45:07.300]   not sure if this is obvious.
[02:45:07.300 --> 02:45:08.660]   You know something profound is coming
[02:45:08.660 --> 02:45:11.540]   when you're saying it's not sure if it's obvious.
[02:45:11.540 --> 02:45:13.180]   There are two major types of learning
[02:45:13.180 --> 02:45:15.580]   in both children and in deep learning.
[02:45:15.580 --> 02:45:18.820]   There's one, imitation learning, watch and repeat,
[02:45:18.820 --> 02:45:21.580]   i.e. pre-training, supervised fine-tuning,
[02:45:21.580 --> 02:45:25.460]   and two, trial and error learning, reinforcement learning.
[02:45:25.460 --> 02:45:27.800]   My favorite simple example is AlphaGo.
[02:45:27.800 --> 02:45:30.960]   One is learning by imitating expert players.
[02:45:30.960 --> 02:45:33.560]   Two is reinforcement learning to win the game.
[02:45:33.560 --> 02:45:36.960]   Almost every single shocking result of deep learning
[02:45:36.960 --> 02:45:40.760]   and the source of all magic is always two.
[02:45:40.760 --> 02:45:42.960]   Two is significantly more powerful.
[02:45:42.960 --> 02:45:44.800]   Two is what surprises you.
[02:45:44.800 --> 02:45:47.580]   Two is when the paddle learns to hit the ball
[02:45:47.580 --> 02:45:49.160]   behind the blocks and break out.
[02:45:49.160 --> 02:45:52.560]   Two is when AlphaGo beats even Lee Sedol.
[02:45:52.560 --> 02:45:56.500]   And two is the aha moment when the deep seek
[02:45:56.500 --> 02:45:59.660]   or O1, et cetera, discovers that it works well
[02:45:59.660 --> 02:46:02.100]   to reevaluate your assumptions,
[02:46:02.100 --> 02:46:04.700]   backtrack, try something else, et cetera.
[02:46:04.700 --> 02:46:08.540]   It's the solving strategies you see this model use
[02:46:08.540 --> 02:46:09.940]   in its chain of thought.
[02:46:09.940 --> 02:46:13.380]   It's how it goes back and forth thinking to itself.
[02:46:13.380 --> 02:46:17.300]   These thoughts are emergent, three exclamation points.
[02:46:17.300 --> 02:46:19.820]   And this is actually seriously incredible,
[02:46:19.820 --> 02:46:22.720]   impressive, and new, and is publicly available
[02:46:22.720 --> 02:46:24.060]   and documented.
[02:46:24.060 --> 02:46:28.220]   The model could never learn this with imitation
[02:46:28.220 --> 02:46:30.300]   because the cognition of the model
[02:46:30.300 --> 02:46:33.100]   and the cognition of the human labeler is different.
[02:46:33.100 --> 02:46:35.280]   The human would never know to correctly annotate
[02:46:35.280 --> 02:46:37.300]   these kinds of solving strategies
[02:46:37.300 --> 02:46:39.880]   and what they should even look like.
[02:46:39.880 --> 02:46:42.160]   They have to be discovered during reinforcement learning
[02:46:42.160 --> 02:46:43.740]   as empirically and statistically useful
[02:46:43.740 --> 02:46:45.340]   towards the final outcome.
[02:46:45.340 --> 02:46:49.480]   Anyway, the alpha zero sort of metaphor analogy here.
[02:46:49.480 --> 02:46:52.780]   Can you speak to that, the magic of the chain of thought
[02:46:52.780 --> 02:46:54.060]   that he's referring to?
[02:46:54.060 --> 02:46:56.580]   - I think it's good to recap alpha go and alpha zero
[02:46:56.580 --> 02:46:58.580]   because it plays nicely with these analogies
[02:46:58.580 --> 02:47:00.420]   between imitation learning and learning from scratch.
[02:47:00.420 --> 02:47:03.620]   So alpha go, the beginning of the process
[02:47:03.620 --> 02:47:06.940]   was learning from humans where they started the first,
[02:47:06.940 --> 02:47:10.440]   this is the first expert level go player or chess player
[02:47:10.440 --> 02:47:11.700]   in DeepMind's series of models
[02:47:11.700 --> 02:47:13.420]   where they had some human data.
[02:47:13.420 --> 02:47:15.620]   And then why it is called alpha zero
[02:47:15.620 --> 02:47:18.140]   is that there was zero human data in the loop.
[02:47:18.140 --> 02:47:20.680]   And that change to alpha zero made a model
[02:47:20.680 --> 02:47:23.000]   that was dramatically more powerful for DeepMind.
[02:47:23.000 --> 02:47:25.880]   So this remove of the human prior,
[02:47:25.880 --> 02:47:28.880]   the human inductive bias makes the final system
[02:47:28.880 --> 02:47:29.700]   far more powerful.
[02:47:29.700 --> 02:47:32.360]   This we mentioned bitter lesson hours ago
[02:47:32.360 --> 02:47:35.080]   and this is all aligned with this.
[02:47:35.080 --> 02:47:38.640]   And then there's been a lot of discussion
[02:47:38.640 --> 02:47:39.560]   in language models.
[02:47:39.560 --> 02:47:40.400]   This is not new.
[02:47:40.400 --> 02:47:42.920]   This goes back to the whole Q star rumors,
[02:47:42.920 --> 02:47:45.980]   which if you piece together the pieces
[02:47:45.980 --> 02:47:49.480]   is probably the start of OpenAI figuring out it's 01 stuff.
[02:47:49.480 --> 02:47:52.640]   When last year in November, the Q star rumors came out.
[02:47:52.640 --> 02:47:57.400]   There's a lot of intellectual drive to know
[02:47:57.400 --> 02:47:59.280]   when is something like this going to happen
[02:47:59.280 --> 02:48:00.160]   with language models?
[02:48:00.160 --> 02:48:02.040]   Because we know these models are so powerful
[02:48:02.040 --> 02:48:04.800]   and we know it has been so successful in the past.
[02:48:04.800 --> 02:48:09.220]   And it is a reasonable analogy that this new type
[02:48:09.220 --> 02:48:11.720]   of reinforcement learning training for reasoning models
[02:48:11.720 --> 02:48:14.440]   is when the door is open to this.
[02:48:14.440 --> 02:48:17.740]   We don't yet have the equivalent of turn 37,
[02:48:17.740 --> 02:48:22.060]   which is the famous turn where the DeepMind's AI playing go
[02:48:22.060 --> 02:48:24.100]   stumped Lisa Doll completely.
[02:48:24.100 --> 02:48:27.100]   We don't have something that's that level of focal point,
[02:48:27.100 --> 02:48:28.940]   but that doesn't mean that the approach to technology
[02:48:28.940 --> 02:48:31.220]   is different and the impact of the general training
[02:48:31.220 --> 02:48:32.580]   is still incredibly new.
[02:48:32.580 --> 02:48:34.100]   - What do you think that point would be?
[02:48:34.100 --> 02:48:37.500]   What would be move 37 for chain of thought for reasoning?
[02:48:37.500 --> 02:48:38.740]   - Scientific discovery.
[02:48:38.740 --> 02:48:40.820]   Like when you use this sort of reasoning problem
[02:48:40.820 --> 02:48:43.900]   and it's just something we fully don't expect.
[02:48:43.900 --> 02:48:45.900]   I think it's actually probably simpler than that.
[02:48:45.900 --> 02:48:49.020]   It's probably something related to computer user robotics
[02:48:49.020 --> 02:48:50.740]   rather than science discovery.
[02:48:50.740 --> 02:48:56.420]   Because the important aspect here is models take so much
[02:48:56.420 --> 02:48:59.300]   data to learn, they're not sample efficient, right?
[02:48:59.300 --> 02:49:01.780]   Trillions, they take the entire web, right?
[02:49:01.780 --> 02:49:05.060]   Over 10 trillion tokens to train on, right?
[02:49:05.060 --> 02:49:09.060]   This would take a human thousands of years to read, right?
[02:49:09.060 --> 02:49:12.920]   A human does not, and humans know most of the stuff,
[02:49:12.920 --> 02:49:14.980]   a lot of the stuff models know better than it, right?
[02:49:14.980 --> 02:49:17.220]   Humans are way, way, way more sample efficient.
[02:49:17.220 --> 02:49:19.000]   That is because of the self-play, right?
[02:49:19.000 --> 02:49:21.580]   How does a baby learn what its body is?
[02:49:21.580 --> 02:49:24.220]   As it sticks its foot in its mouth and it says,
[02:49:24.220 --> 02:49:26.420]   "Oh, this is my body," right?
[02:49:26.420 --> 02:49:29.460]   It sticks its hand in its mouth and it calibrates its touch
[02:49:29.460 --> 02:49:31.580]   on its fingers with the most sensitive touch thing
[02:49:31.580 --> 02:49:32.420]   on its tongue, right?
[02:49:32.420 --> 02:49:33.540]   It's how babies learn.
[02:49:33.540 --> 02:49:37.340]   And it's just self-play over and over and over and over again.
[02:49:37.340 --> 02:49:40.900]   And now we have something that is similar to that, right?
[02:49:40.900 --> 02:49:44.080]   With these verifiable proofs, right?
[02:49:44.080 --> 02:49:45.680]   Whether it's a unit test in code
[02:49:45.680 --> 02:49:48.960]   or a mathematical verifiable task,
[02:49:48.960 --> 02:49:52.320]   generate many traces of reasoning, right?
[02:49:52.320 --> 02:49:54.040]   And keep branching them out, keep branching them out.
[02:49:54.040 --> 02:49:55.200]   And then check at the end,
[02:49:55.200 --> 02:49:56.700]   hey, which one actually has the right answer?
[02:49:56.700 --> 02:49:57.720]   Most of them are wrong, great.
[02:49:57.720 --> 02:49:58.680]   These are the few that are right.
[02:49:58.680 --> 02:50:00.640]   Maybe we use some sort of reward model outside of this
[02:50:00.640 --> 02:50:03.400]   to select even the best one to preference as well.
[02:50:03.400 --> 02:50:04.840]   But now you've started to get better
[02:50:04.840 --> 02:50:06.160]   and better at these benchmarks.
[02:50:06.160 --> 02:50:08.160]   And so you've seen over the last six months,
[02:50:08.160 --> 02:50:11.340]   a skyrocketing in a lot of different benchmarks, right?
[02:50:11.340 --> 02:50:13.660]   - All math and code benchmarks are pretty much solved
[02:50:13.660 --> 02:50:14.900]   except for frontier math,
[02:50:14.900 --> 02:50:17.460]   which is designed to be almost questions
[02:50:17.460 --> 02:50:19.060]   that aren't practical to most people.
[02:50:19.060 --> 02:50:24.060]   'Cause they're exam level open math problem type things.
[02:50:24.060 --> 02:50:26.700]   So it's like on the math problems
[02:50:26.700 --> 02:50:27.700]   that are somewhat reasonable,
[02:50:27.700 --> 02:50:29.920]   which is like somewhat complicated word problems
[02:50:29.920 --> 02:50:30.760]   or coding problems.
[02:50:30.760 --> 02:50:32.860]   That's just what Dylan is saying.
[02:50:32.860 --> 02:50:34.940]   - So the thing here is that
[02:50:34.940 --> 02:50:36.820]   these are only with verifiable tasks.
[02:50:36.820 --> 02:50:40.000]   We earlier showed an example of the really interesting,
[02:50:40.000 --> 02:50:41.080]   like what happens when chain of thought
[02:50:41.080 --> 02:50:42.760]   is to a non-verifiable thing.
[02:50:42.760 --> 02:50:44.640]   It's just like a human chatting, right?
[02:50:44.640 --> 02:50:47.280]   With thinking about what's novel for humans, right?
[02:50:47.280 --> 02:50:48.320]   A unique thought.
[02:50:48.320 --> 02:50:50.600]   But this task and form of training
[02:50:50.600 --> 02:50:53.000]   only works when it's verifiable.
[02:50:53.000 --> 02:50:55.280]   And from here, the thought is,
[02:50:55.280 --> 02:50:58.480]   okay, we can continue to scale this current training method
[02:50:58.480 --> 02:51:01.260]   by increasing the number of verifiable tasks.
[02:51:01.260 --> 02:51:04.120]   In math and coding, coding probably has a lot more to go.
[02:51:04.120 --> 02:51:05.920]   Math has a lot less to go
[02:51:05.920 --> 02:51:07.560]   in terms of what are verifiable things.
[02:51:07.560 --> 02:51:08.840]   Can I create a solver
[02:51:08.840 --> 02:51:11.120]   that then I generate trajectories toward
[02:51:11.120 --> 02:51:13.120]   or traces towards, reasoning traces towards,
[02:51:13.120 --> 02:51:14.680]   and then prune the ones that don't work
[02:51:14.680 --> 02:51:15.900]   and keep the ones that do work?
[02:51:15.900 --> 02:51:17.820]   Well, those are gonna be solved pretty quickly,
[02:51:17.820 --> 02:51:18.960]   but even if you've solved math,
[02:51:18.960 --> 02:51:22.280]   you have not actually created intelligence, right?
[02:51:22.280 --> 02:51:25.880]   And so this is where I think the like aha moment
[02:51:25.880 --> 02:51:27.740]   of computer user robotics will come in
[02:51:27.740 --> 02:51:31.680]   because now you have a sandbox or a playground
[02:51:31.680 --> 02:51:34.320]   that is infinitely verifiable, right?
[02:51:34.320 --> 02:51:36.800]   Did you, you know, messing around on the internet,
[02:51:36.800 --> 02:51:38.320]   there are so many actions that you can do
[02:51:38.320 --> 02:51:39.160]   that are verifiable.
[02:51:39.160 --> 02:51:41.040]   It'll start off with like, log into a website,
[02:51:41.040 --> 02:51:43.640]   create an account, click a button here, blah, blah, blah.
[02:51:43.640 --> 02:51:45.720]   But it'll then get to the point where it's,
[02:51:45.720 --> 02:51:48.080]   hey, go do a task on Tasker or whatever these other,
[02:51:48.080 --> 02:51:49.400]   all these various task websites.
[02:51:49.400 --> 02:51:51.800]   Hey, go get hundreds of likes, right?
[02:51:51.800 --> 02:51:53.160]   And it's gonna fail.
[02:51:53.160 --> 02:51:54.440]   It's gonna spawn hundreds of accounts.
[02:51:54.440 --> 02:51:55.440]   It's gonna fail on most of them,
[02:51:55.440 --> 02:51:56.840]   but this one got to 1,000.
[02:51:56.840 --> 02:51:58.880]   Great, now you've reached the verifiable thing.
[02:51:58.880 --> 02:52:01.000]   And you just keep iterating this loop over and over.
[02:52:01.000 --> 02:52:03.100]   And that's when, and same with robotics, right?
[02:52:03.100 --> 02:52:05.240]   That's where, you know, where you have an infinite playground
[02:52:05.240 --> 02:52:07.720]   of tasks, like, hey, did I put the ball in the bucket
[02:52:07.720 --> 02:52:09.960]   all the way to like, oh, did I like build a car, right?
[02:52:09.960 --> 02:52:12.960]   Like, you know, there's a whole trajectory to speed run
[02:52:12.960 --> 02:52:15.040]   or, you know, what models can do.
[02:52:15.040 --> 02:52:18.320]   But at some point, I truly think that like, you know,
[02:52:18.320 --> 02:52:20.400]   we'll spawn models and initially all the training
[02:52:20.400 --> 02:52:21.280]   will be in sandboxes.
[02:52:21.280 --> 02:52:22.600]   But then at some point, you know,
[02:52:22.600 --> 02:52:24.920]   the language model pre-training is gonna be dwarfed
[02:52:24.920 --> 02:52:27.200]   by what is this reinforcement learning?
[02:52:27.200 --> 02:52:29.240]   You know, you'll pre-train a multimodal model
[02:52:29.240 --> 02:52:31.320]   that can see, that can read, that can write,
[02:52:31.320 --> 02:52:32.560]   you know, blah, blah, blah, whatever.
[02:52:32.560 --> 02:52:34.220]   Vision, audio, et cetera.
[02:52:34.220 --> 02:52:37.860]   But then you'll have it play in a sandbox infinitely
[02:52:37.860 --> 02:52:39.980]   and figure out math, figure out code,
[02:52:39.980 --> 02:52:41.380]   figure out navigating the web,
[02:52:41.380 --> 02:52:43.300]   figure out operating a robot arm, right?
[02:52:43.300 --> 02:52:45.380]   And then it'll learn so much.
[02:52:45.380 --> 02:52:48.660]   And the aha moment I think will be when this is available
[02:52:48.660 --> 02:52:50.900]   to then create something that's not good, right?
[02:52:50.900 --> 02:52:52.500]   Like, oh, cool, part of it was like figuring out
[02:52:52.500 --> 02:52:53.340]   how to use the web.
[02:52:53.340 --> 02:52:56.220]   Now, all of a sudden, it's figured out really well
[02:52:56.220 --> 02:52:58.380]   how to just get hundreds of thousands of followers
[02:52:58.380 --> 02:53:00.020]   that are real and real engagement on Twitter
[02:53:00.020 --> 02:53:01.520]   because all of a sudden this is one of the things
[02:53:01.520 --> 02:53:02.620]   that are verifiable.
[02:53:02.620 --> 02:53:05.740]   - And maybe not just engagement, but make money.
[02:53:05.740 --> 02:53:07.060]   - Yes, of course.
[02:53:07.060 --> 02:53:08.220]   - I mean, that could be the thing
[02:53:08.220 --> 02:53:12.260]   where almost fully automated, it makes, you know,
[02:53:12.260 --> 02:53:14.740]   $10 million by being an influencer,
[02:53:14.740 --> 02:53:17.740]   selling a product, creating the product, like.
[02:53:17.740 --> 02:53:20.520]   And I'm not referring to like a hype product,
[02:53:20.520 --> 02:53:22.020]   but an actual product.
[02:53:22.020 --> 02:53:25.200]   Or like, holy shit, this thing created a business.
[02:53:25.200 --> 02:53:27.860]   It's running it, it's the face of the business,
[02:53:27.860 --> 02:53:28.920]   that kind of thing.
[02:53:28.920 --> 02:53:31.580]   Or maybe a number one song,
[02:53:31.580 --> 02:53:34.460]   like it creates the whole infrastructure required
[02:53:34.460 --> 02:53:36.420]   to create the song to be the influencer
[02:53:36.420 --> 02:53:38.300]   that represents that song, that kind of thing.
[02:53:38.300 --> 02:53:40.440]   It makes a lot of, that could be the move.
[02:53:40.440 --> 02:53:44.500]   I mean, our culture respects money in that kind of way.
[02:53:44.500 --> 02:53:46.060]   - And it's verifiable, right?
[02:53:46.060 --> 02:53:47.580]   - It's verifiable, right.
[02:53:47.580 --> 02:53:48.860]   - Bank account can't lie.
[02:53:48.860 --> 02:53:49.740]   - Exactly.
[02:53:49.740 --> 02:53:52.620]   - There's surprising evidence that once you set up
[02:53:52.620 --> 02:53:54.780]   the ways of collecting the verifiable domain
[02:53:54.780 --> 02:53:56.220]   that this can work.
[02:53:56.220 --> 02:53:59.360]   There's been a lot of research before this R1
[02:53:59.360 --> 02:54:00.520]   on math problems.
[02:54:00.520 --> 02:54:02.840]   And they approach math with language models
[02:54:02.840 --> 02:54:04.840]   just by increasing the number of samples.
[02:54:04.840 --> 02:54:06.840]   So you can just try again and again and again.
[02:54:06.840 --> 02:54:09.200]   And you look at the amount of times
[02:54:09.200 --> 02:54:10.960]   that the language models get it right.
[02:54:10.960 --> 02:54:14.480]   And what we see is that even very bad models
[02:54:14.480 --> 02:54:15.840]   get it right sometimes.
[02:54:15.840 --> 02:54:18.100]   And the whole idea behind reinforcement learning
[02:54:18.100 --> 02:54:20.520]   is that you can learn from very sparse rewards.
[02:54:20.520 --> 02:54:23.920]   So it doesn't, the space of language
[02:54:23.920 --> 02:54:24.800]   and the space of tokens,
[02:54:24.800 --> 02:54:27.160]   whether you're generating language or tasks for a robot
[02:54:27.160 --> 02:54:30.080]   is so big that you might say that it's like,
[02:54:30.080 --> 02:54:32.200]   I mean, the tokenizer for a language model
[02:54:32.200 --> 02:54:33.400]   can be like 200,000 things.
[02:54:33.400 --> 02:54:36.280]   So at each step, it can sample from that big of a space.
[02:54:36.280 --> 02:54:38.940]   So if it can generate a bit of a signal
[02:54:38.940 --> 02:54:39.960]   that it can climb onto,
[02:54:39.960 --> 02:54:42.840]   that's what the whole field of RL is around
[02:54:42.840 --> 02:54:45.160]   is learning from sparse rewards.
[02:54:45.160 --> 02:54:47.040]   And the same thing has played out in math
[02:54:47.040 --> 02:54:48.480]   where it's like very weak models
[02:54:48.480 --> 02:54:49.960]   that sometimes generate answers.
[02:54:49.960 --> 02:54:52.520]   We see research already that you can boost
[02:54:52.520 --> 02:54:53.360]   their math scores.
[02:54:53.360 --> 02:54:56.480]   You can do this sort of RL training for math.
[02:54:56.480 --> 02:54:57.840]   It might not be as effective,
[02:54:57.840 --> 02:54:59.880]   but if you take a 1 billion parameter model,
[02:54:59.880 --> 02:55:02.400]   so something 600 times smaller than DeepSeq,
[02:55:02.400 --> 02:55:04.840]   you can boost its grade school math scores
[02:55:04.840 --> 02:55:07.600]   very directly with a small amount of this training.
[02:55:07.600 --> 02:55:10.480]   So it's not to say that this is coming soon.
[02:55:10.480 --> 02:55:13.160]   Setting up the verification domains is extremely hard
[02:55:13.160 --> 02:55:15.340]   and there's a lot of nuance in this,
[02:55:15.340 --> 02:55:18.120]   but there are some basic things that we have seen before
[02:55:18.120 --> 02:55:21.280]   where it's like, it's at least expectable
[02:55:21.280 --> 02:55:23.760]   that there's a domain and there's a chance that this works.
[02:55:23.760 --> 02:55:25.920]   - All right, so we have fun things happening in real time.
[02:55:25.920 --> 02:55:27.400]   This is a good opportunity to talk
[02:55:27.400 --> 02:55:31.760]   about other reasoning models, O1, O3.
[02:55:31.760 --> 02:55:36.760]   Just now OpenAI, as perhaps expected, released O3 mini.
[02:55:36.760 --> 02:55:41.040]   What are we expecting from the different flavors?
[02:55:41.040 --> 02:55:43.000]   Can you just lay out the different flavors
[02:55:43.000 --> 02:55:47.360]   of the O models and from Gemini, the reasoning model?
[02:55:47.360 --> 02:55:49.280]   - Something I would say about these reasoning models
[02:55:49.280 --> 02:55:51.480]   is we talked a lot about reasoning training
[02:55:51.480 --> 02:55:52.820]   on math and code.
[02:55:52.820 --> 02:55:55.120]   And what is done is that you have the base model
[02:55:55.120 --> 02:55:56.640]   we've talked about a lot on the internet.
[02:55:56.640 --> 02:55:58.760]   You do this large-scale reasoning training
[02:55:58.760 --> 02:56:00.320]   with reinforcement learning.
[02:56:00.320 --> 02:56:04.560]   And then what the DeepSeq paper detailed in this R1 paper,
[02:56:04.560 --> 02:56:07.000]   which for me is one of the big open questions
[02:56:07.000 --> 02:56:10.640]   on how do you do this, is that they did reasoning heavy,
[02:56:10.640 --> 02:56:13.320]   but very standard post-training techniques
[02:56:13.320 --> 02:56:15.160]   after the large-scale reasoning RL.
[02:56:15.160 --> 02:56:16.440]   So they did the same things
[02:56:16.440 --> 02:56:18.720]   with a form of instruction tuning
[02:56:18.720 --> 02:56:19.960]   through rejection sampling,
[02:56:19.960 --> 02:56:22.880]   which is essentially heavily filtered instruction tuning
[02:56:22.880 --> 02:56:23.960]   with some reward models.
[02:56:23.960 --> 02:56:27.540]   And then they did this RLHF, but they made it math heavy.
[02:56:27.540 --> 02:56:29.800]   So some of this transfer,
[02:56:29.800 --> 02:56:32.540]   we'd looked at this philosophical example early on.
[02:56:32.540 --> 02:56:36.880]   One of the big open questions is how much does this transfer?
[02:56:36.880 --> 02:56:40.360]   If we bring in domains after the reasoning training,
[02:56:40.360 --> 02:56:42.840]   are all the models gonna become eloquent writers
[02:56:42.840 --> 02:56:43.680]   by reasoning?
[02:56:43.680 --> 02:56:45.080]   Is this philosophy stuff going to be open?
[02:56:45.080 --> 02:56:47.800]   We don't know in the research of how much this will transfer.
[02:56:47.800 --> 02:56:50.560]   There's other things about how we can make soft verifiers
[02:56:50.560 --> 02:56:51.400]   and things like this.
[02:56:51.400 --> 02:56:53.880]   But there is more training after reasoning,
[02:56:53.880 --> 02:56:56.600]   which makes it easier to use these reasoning models.
[02:56:56.600 --> 02:56:57.800]   And that's what we're using right now.
[02:56:57.800 --> 02:57:00.140]   So if we're gonna talk about with 3Many and O1,
[02:57:00.140 --> 02:57:02.260]   these have gone through these extra techniques
[02:57:02.260 --> 02:57:04.500]   that are designed for human preferences
[02:57:04.500 --> 02:57:07.040]   after being trained to elicit reasoning.
[02:57:07.040 --> 02:57:09.340]   - I think one of the things that people are ignoring
[02:57:09.340 --> 02:57:12.520]   is Google's Gemini flash thinking
[02:57:12.520 --> 02:57:15.880]   is both cheaper than R1 and better.
[02:57:15.880 --> 02:57:17.400]   And they released it in the beginning of December.
[02:57:17.400 --> 02:57:18.360]   - And nobody's talking about it.
[02:57:18.360 --> 02:57:19.200]   - No one cares.
[02:57:19.200 --> 02:57:20.280]   - It has a different flavor to it.
[02:57:20.280 --> 02:57:22.800]   Its behavior is less expressive than something like O1,
[02:57:22.800 --> 02:57:25.540]   and it has fewer tracks than it is on.
[02:57:25.540 --> 02:57:28.920]   Quen released a model last fall, QWQ,
[02:57:28.920 --> 02:57:30.880]   which was their preview reasoning model.
[02:57:30.880 --> 02:57:33.360]   And DeepSea had R1 Lite last fall,
[02:57:33.360 --> 02:57:35.540]   where these models kind of felt like they're on rails,
[02:57:35.540 --> 02:57:38.480]   where they really, really only can do math and code.
[02:57:38.480 --> 02:57:40.960]   And O1 is, it can answer anything.
[02:57:40.960 --> 02:57:43.200]   It might not be perfect for some tasks,
[02:57:43.200 --> 02:57:46.700]   but it's flexible and has some richness to it.
[02:57:46.700 --> 02:57:50.040]   And this is kind of the art of like how cook,
[02:57:50.040 --> 02:57:51.920]   like how is a model a little bit undercooked?
[02:57:51.920 --> 02:57:54.020]   It's like, I mean, it's good to get a model out the door,
[02:57:54.020 --> 02:57:55.960]   but it's hard to gauge.
[02:57:55.960 --> 02:57:57.740]   And it takes a lot of taste to be like,
[02:57:57.740 --> 02:58:00.080]   is this a full-fledged model?
[02:58:00.080 --> 02:58:01.520]   Can I use this for everything?
[02:58:01.520 --> 02:58:04.140]   And they're probably more similar for math and code.
[02:58:04.140 --> 02:58:07.080]   My quick read is that Gemini flash is like
[02:58:07.080 --> 02:58:10.400]   not trained to the same way as O1,
[02:58:10.400 --> 02:58:13.240]   but taking an existing training stack,
[02:58:13.240 --> 02:58:14.480]   adding reasoning to it.
[02:58:14.480 --> 02:58:16.020]   So taking a more normal training stack
[02:58:16.020 --> 02:58:17.480]   and adding reasoning to it.
[02:58:17.480 --> 02:58:19.440]   And I'm sure they're going to have more.
[02:58:19.440 --> 02:58:21.640]   I mean, they've done quick releases on Gemini flash,
[02:58:21.640 --> 02:58:24.680]   the reasoning, and this is the second version
[02:58:24.680 --> 02:58:26.280]   from the holidays.
[02:58:26.280 --> 02:58:29.940]   It's evolving fast and it takes longer
[02:58:29.940 --> 02:58:31.040]   to make this training stack
[02:58:31.040 --> 02:58:31.880]   where you're doing this large scale R1.
[02:58:31.880 --> 02:58:34.540]   - Ask it the same question from earlier.
[02:58:34.540 --> 02:58:35.720]   The one about the-
[02:58:35.720 --> 02:58:37.000]   - The human nature.
[02:58:37.000 --> 02:58:37.840]   - Yeah.
[02:58:37.840 --> 02:58:39.760]   - What was the human nature one?
[02:58:39.760 --> 02:58:41.680]   - The way I can ramble,
[02:58:41.680 --> 02:58:43.320]   why I can ramble about this so much
[02:58:43.320 --> 02:58:46.080]   is that we've been working on this at AI2
[02:58:46.080 --> 02:58:49.440]   before O1 was fully available to everyone
[02:58:49.440 --> 02:58:50.280]   and before R1,
[02:58:50.280 --> 02:58:53.280]   which is essentially using this RL training for fine tuning.
[02:58:53.280 --> 02:58:56.300]   We use this in our Tulu series of models
[02:58:56.300 --> 02:58:58.720]   and you can elicit the same behaviors
[02:58:58.720 --> 02:59:01.480]   where you say like weight and so on,
[02:59:01.480 --> 02:59:03.400]   but it's so late in the training process
[02:59:03.400 --> 02:59:06.640]   that this kind of reasoning expression is much lighter.
[02:59:06.640 --> 02:59:08.640]   So you can, there's essentially a gradation
[02:59:08.640 --> 02:59:10.960]   and just how much of this RL training you put into it
[02:59:10.960 --> 02:59:12.960]   determines how the output looks.
[02:59:12.960 --> 02:59:16.440]   - So we're now using Gemini 2.0
[02:59:16.440 --> 02:59:20.880]   flash thinking experimental 121.
[02:59:20.880 --> 02:59:25.000]   - It summarized the prompt as humans self-domesticated apes.
[02:59:25.000 --> 02:59:27.400]   (laughs)
[02:59:27.400 --> 02:59:28.520]   - Perspective, okay.
[02:59:28.520 --> 02:59:31.360]   All right, so wait, is this revealing the reasoning?
[02:59:31.360 --> 02:59:33.220]   Here's why this is novel, okay.
[02:59:33.220 --> 02:59:35.120]   - You can click to expand.
[02:59:35.120 --> 02:59:36.360]   - Oh yeah, click to expand.
[02:59:36.360 --> 02:59:37.280]   - Okay.
[02:59:37.280 --> 02:59:40.900]   Analyze the request, novel is the key word.
[02:59:40.900 --> 02:59:42.920]   - Like see how it just looks a little different?
[02:59:42.920 --> 02:59:45.500]   It looks like a normal output.
[02:59:45.500 --> 02:59:49.200]   - Yeah, it's, I mean, in some sense it's better structured,
[02:59:49.200 --> 02:59:50.420]   it makes more sense.
[02:59:50.420 --> 02:59:52.880]   - Oh, and it latched onto human
[02:59:52.880 --> 02:59:55.360]   and then it went into organisms and oh wow.
[02:59:55.360 --> 02:59:56.200]   (laughs)
[02:59:56.200 --> 03:00:00.080]   - Apex predator, focus on domestication,
[03:00:00.080 --> 03:00:01.880]   apply domestication to humans,
[03:00:01.880 --> 03:00:04.220]   explore the idea of self-domestication.
[03:00:04.220 --> 03:00:05.160]   (laughs)
[03:00:05.160 --> 03:00:07.200]   - Not good, not good.
[03:00:07.200 --> 03:00:08.920]   - Where is this going?
[03:00:08.920 --> 03:00:10.520]   Refine and articulate the insight,
[03:00:10.520 --> 03:00:13.840]   greater facial expressiveness and communication ability,
[03:00:13.840 --> 03:00:17.000]   yes, plasticity and adaptability, yes,
[03:00:17.000 --> 03:00:20.220]   dependence on social groups, yes, all right.
[03:00:20.220 --> 03:00:24.640]   And self-critique and refine further, wow.
[03:00:24.640 --> 03:00:26.600]   Is this truly novel?
[03:00:26.600 --> 03:00:27.680]   Is it well-supported?
[03:00:27.680 --> 03:00:32.120]   So on and so forth, and the insight it's getting at
[03:00:32.120 --> 03:00:34.840]   is humans are not just social animals,
[03:00:34.840 --> 03:00:37.640]   but profoundly self-domesticated apes,
[03:00:37.640 --> 03:00:40.400]   and this self-domestication is the key to understanding
[03:00:40.400 --> 03:00:43.120]   our unique cognitive and social abilities.
[03:00:43.120 --> 03:00:46.320]   Self-domesticated apes, self-domesticated.
[03:00:46.320 --> 03:00:48.600]   - I prefer the deep-seek response.
[03:00:48.600 --> 03:00:51.560]   - Self-domesticated, I mean, it's novel,
[03:00:51.560 --> 03:00:55.680]   the insight is novel, I mean, that's like a good book title,
[03:00:55.680 --> 03:00:58.440]   self-domesticated apes, there could be
[03:00:58.440 --> 03:01:00.440]   a case made for that, I mean, yeah, it's cool.
[03:01:00.440 --> 03:01:04.080]   And it's revealing the reasoning, it's magical.
[03:01:04.080 --> 03:01:06.980]   It's magical, like, this is really powerful.
[03:01:06.980 --> 03:01:12.600]   - Hello, everyone, this is Lex with a quick intermission
[03:01:12.600 --> 03:01:14.360]   recorded after the podcast.
[03:01:14.360 --> 03:01:17.760]   Since we reviewed responses from Deep Seeker One
[03:01:17.760 --> 03:01:21.200]   and Gemini Flash 2.0 Thinking during this conversation,
[03:01:21.200 --> 03:01:25.080]   I thought at this moment it would be nice to insert myself
[03:01:25.080 --> 03:01:30.080]   quickly doing the same for OpenAI 01 Pro and 03 Mini
[03:01:30.280 --> 03:01:33.280]   with the same prompt, the prompt being,
[03:01:33.280 --> 03:01:37.760]   give one truly novel insight about humans.
[03:01:37.760 --> 03:01:42.160]   And I thought I would, in general, give my vibe check
[03:01:42.160 --> 03:01:47.160]   and vibe-based anecdotal report on my own experiences
[03:01:47.160 --> 03:01:51.680]   with the new 03 Mini model, now that I got a chance
[03:01:51.680 --> 03:01:54.160]   to spend many hours with it in different kinds of contexts
[03:01:54.160 --> 03:01:55.500]   and applications.
[03:01:55.500 --> 03:01:58.120]   So I would probably categorize this question
[03:01:58.120 --> 03:02:01.840]   as, let's say, open-ended philosophical question.
[03:02:01.840 --> 03:02:04.600]   And in particular, the emphasis on novelty,
[03:02:04.600 --> 03:02:08.920]   I think is a nice way to test one of the capabilities
[03:02:08.920 --> 03:02:11.160]   of the model, which is come up with something
[03:02:11.160 --> 03:02:14.960]   that makes you pause and almost surprise you
[03:02:14.960 --> 03:02:16.560]   with its brilliance.
[03:02:16.560 --> 03:02:20.640]   So that said, my general review, after running each
[03:02:20.640 --> 03:02:23.040]   of the models on this question a bunch of times,
[03:02:23.040 --> 03:02:27.660]   is that 01 Pro consistently gave brilliant answers.
[03:02:28.420 --> 03:02:31.140]   Ones that gave me pause and made me think,
[03:02:31.140 --> 03:02:36.140]   both cutting in its insight and just really nicely phrased
[03:02:36.140 --> 03:02:40.500]   with wit, with clarity, with nuance, over and over,
[03:02:40.500 --> 03:02:43.060]   consistently generating the best answers.
[03:02:43.060 --> 03:02:46.380]   After that is R1, which is less consistent,
[03:02:46.380 --> 03:02:49.100]   but again, delivered brilliance.
[03:02:49.100 --> 03:02:52.120]   Gemini Flash 2.0 Thinking was third.
[03:02:52.120 --> 03:02:56.180]   And last was 03 Mini, actually.
[03:02:56.180 --> 03:02:59.140]   It often gave quite a generic answer,
[03:02:59.140 --> 03:03:01.380]   at least to my particular sensibilities.
[03:03:01.380 --> 03:03:04.020]   That said, in a bunch of other applications
[03:03:04.020 --> 03:03:07.560]   that I tested for brainstorming purposes,
[03:03:07.560 --> 03:03:09.820]   it actually worked extremely well
[03:03:09.820 --> 03:03:13.380]   and often outperformed R1.
[03:03:13.380 --> 03:03:15.380]   But on this open-ended philosophical question,
[03:03:15.380 --> 03:03:17.460]   it did consistently worse.
[03:03:17.460 --> 03:03:20.820]   Now, another important element for each of these models
[03:03:20.820 --> 03:03:22.620]   is how the reasoning is presented.
[03:03:22.620 --> 03:03:26.420]   DeepSeek R1 shows the full chain of thought tokens,
[03:03:26.420 --> 03:03:29.100]   which I personally just love.
[03:03:29.100 --> 03:03:31.060]   For these open-ended philosophical questions,
[03:03:31.060 --> 03:03:32.460]   it's really, really interesting
[03:03:32.460 --> 03:03:34.300]   to see the model think through it.
[03:03:34.300 --> 03:03:37.160]   But really, also, just stepping back,
[03:03:37.160 --> 03:03:39.880]   me as a person who appreciates intelligence
[03:03:39.880 --> 03:03:42.240]   and reasoning and reflection,
[03:03:42.240 --> 03:03:45.740]   reading these kind of chain of thought raw tokens of R1,
[03:03:45.740 --> 03:03:48.060]   there's something genuinely beautiful
[03:03:48.060 --> 03:03:51.840]   about observing the path of deliberation
[03:03:51.840 --> 03:03:54.180]   in an intelligence system.
[03:03:54.180 --> 03:03:56.700]   I think we don't always have that
[03:03:56.700 --> 03:03:59.020]   explicitly laid out for us humans.
[03:03:59.020 --> 03:04:02.620]   So to see it in another intelligence system,
[03:04:02.620 --> 03:04:04.580]   the non-linearity of it,
[03:04:04.580 --> 03:04:08.180]   akin to Ulysses or Finnegan's Wake by James Joyce,
[03:04:08.180 --> 03:04:09.740]   it's just beautiful to watch.
[03:04:09.740 --> 03:04:12.220]   Anyway, as we discussed in the episode,
[03:04:12.220 --> 03:04:14.820]   DeepSeek R1 talked about humans
[03:04:14.820 --> 03:04:16.500]   being able to convert selfish desires
[03:04:16.500 --> 03:04:17.860]   into cooperative systems
[03:04:17.860 --> 03:04:20.060]   by collectively pretending abstract rules
[03:04:20.060 --> 03:04:22.600]   like money, laws, and rights are real,
[03:04:22.600 --> 03:04:26.480]   and these shared hallucinations act as games,
[03:04:26.480 --> 03:04:28.520]   where competition is secretly redirected
[03:04:28.520 --> 03:04:29.880]   to benefit the group,
[03:04:29.880 --> 03:04:32.400]   turning conflict into society's fuel.
[03:04:32.400 --> 03:04:35.000]   Gemini 2.0 Flash Thinking said,
[03:04:35.000 --> 03:04:36.940]   "Humans are not just social animals,
[03:04:36.940 --> 03:04:39.080]   "but self-domesticated apes,
[03:04:39.080 --> 03:04:41.200]   "and this self-domestication is the key
[03:04:41.200 --> 03:04:43.220]   "to understanding our unique cognitive
[03:04:43.220 --> 03:04:45.040]   "and social abilities."
[03:04:45.040 --> 03:04:47.720]   Now, it's important to say that the chain of thought there
[03:04:47.720 --> 03:04:49.120]   was really interesting.
[03:04:49.120 --> 03:04:51.720]   It was looking through the entire evolution
[03:04:51.720 --> 03:04:53.000]   of life on Earth,
[03:04:53.000 --> 03:04:55.120]   considering apex predators,
[03:04:55.120 --> 03:04:58.640]   and considering how, from that,
[03:04:58.640 --> 03:05:00.800]   we ended up to where we are.
[03:05:00.800 --> 03:05:03.600]   I think that domestication by choice
[03:05:03.600 --> 03:05:05.160]   is a really interesting angle.
[03:05:05.160 --> 03:05:06.560]   Again, it's one of those things
[03:05:06.560 --> 03:05:09.240]   when somebody presents a different angle
[03:05:09.240 --> 03:05:10.640]   on a seemingly obvious thing,
[03:05:10.640 --> 03:05:12.000]   it just makes me smile.
[03:05:12.000 --> 03:05:13.800]   And the same with DeepSeek R1,
[03:05:13.800 --> 03:05:18.320]   that these hallucinations of money, laws, and rights,
[03:05:18.320 --> 03:05:21.760]   and us collectively pretending like it's real,
[03:05:21.760 --> 03:05:24.440]   and we play games with them that look like competition
[03:05:24.440 --> 03:05:27.720]   when secretly we're just cooperating with each other.
[03:05:27.720 --> 03:05:31.000]   And that is the fuel of progress, beautifully put.
[03:05:31.000 --> 03:05:33.480]   Now, OpenAI R1 Pro consistently,
[03:05:33.480 --> 03:05:36.200]   over and over, delivered bangers.
[03:05:36.200 --> 03:05:37.200]   I can go through many of them,
[03:05:37.200 --> 03:05:38.360]   but the first one was,
[03:05:38.360 --> 03:05:40.960]   "Humans are the only species that turns raw materials
[03:05:40.960 --> 03:05:42.880]   "into symbolic resources,
[03:05:42.880 --> 03:05:45.580]   "then uses those symbols to reorganize
[03:05:45.580 --> 03:05:47.280]   "the very materials they came from,
[03:05:47.280 --> 03:05:49.300]   "creating a closed feedback loop
[03:05:49.300 --> 03:05:52.640]   "between meaning and matter."
[03:05:52.640 --> 03:05:54.040]   Here, I just ran it again.
[03:05:54.040 --> 03:05:57.120]   Banger after banger, I'm telling you.
[03:05:57.120 --> 03:05:59.160]   "Humans are unique among known species
[03:05:59.160 --> 03:06:02.300]   "in that they simultaneously rewrite two layers of reality,
[03:06:02.300 --> 03:06:03.520]   "the external world,
[03:06:03.520 --> 03:06:06.500]   "and their own private mental landscapes,
[03:06:06.500 --> 03:06:09.700]   "and then merge these two rewritten layers
[03:06:09.700 --> 03:06:12.200]   "into a continuous personal narrative
[03:06:12.200 --> 03:06:15.160]   "that feels objectively true."
[03:06:15.160 --> 03:06:17.680]   Feels true.
[03:06:17.680 --> 03:06:19.200]   This is poetry.
[03:06:19.200 --> 03:06:22.440]   Okay, and then O3 Mini High, for me,
[03:06:22.440 --> 03:06:26.120]   was smart, fast, actually,
[03:06:26.120 --> 03:06:29.260]   and kind of generic.
[03:06:29.260 --> 03:06:31.040]   Never quite got there for me.
[03:06:31.040 --> 03:06:33.900]   So here's the first one I got from O3 Mini.
[03:06:33.900 --> 03:06:36.080]   "Humans are not fixed beings,
[03:06:36.080 --> 03:06:38.160]   "but rather ongoing narratives,
[03:06:38.160 --> 03:06:40.680]   "dynamic stories that we continuously write,
[03:06:40.680 --> 03:06:42.740]   "edit, and reinterpret.
[03:06:42.740 --> 03:06:45.820]   "This narrative plasticity is more than just memory
[03:06:45.820 --> 03:06:47.380]   "or self-reflection.
[03:06:47.380 --> 03:06:49.160]   "It's an intrinsic cognitive process
[03:06:49.160 --> 03:06:52.600]   "that acts like an internal error-correction system.
[03:06:52.600 --> 03:06:55.740]   "It allows us to adapt our identities and values over time
[03:06:55.740 --> 03:06:57.340]   "in response to new experiences,
[03:06:57.340 --> 03:07:00.060]   "challenges, and social contexts."
[03:07:00.060 --> 03:07:01.140]   Now, it almost sneaks up
[03:07:01.140 --> 03:07:03.800]   to something approximating cutting insight
[03:07:03.800 --> 03:07:07.100]   with "narrative plasticity" in quotes,
[03:07:07.100 --> 03:07:10.080]   but then it goes back to the generic.
[03:07:10.080 --> 03:07:10.920]   I don't know.
[03:07:10.920 --> 03:07:13.540]   All of these models are incredible for different reasons.
[03:07:13.540 --> 03:07:16.360]   There's a lot of concerns, as we discussed in this episode,
[03:07:16.360 --> 03:07:20.600]   but there's a lot of reasons to be excited, as well.
[03:07:20.600 --> 03:07:24.640]   And I've probably spoken for too long.
[03:07:24.640 --> 03:07:29.040]   I am severely sleep-deprived, borderline delirious,
[03:07:29.040 --> 03:07:31.640]   so hopefully some of this made sense.
[03:07:31.640 --> 03:07:35.280]   And now, dear friends, back to the episode.
[03:07:35.280 --> 03:07:39.880]   - I think when you, to Nathan's point,
[03:07:39.880 --> 03:07:43.400]   when you look at the reasoning models,
[03:07:43.400 --> 03:07:46.980]   to me, even when I used R1 versus O1,
[03:07:46.980 --> 03:07:51.480]   there was that rough edges-around-the-corner feeling.
[03:07:51.480 --> 03:07:53.580]   And FlashThinking earlier,
[03:07:53.580 --> 03:07:55.720]   I didn't use this version, but the one from December,
[03:07:55.720 --> 03:07:56.540]   and it definitely had that
[03:07:56.540 --> 03:07:58.840]   rough edges-around-the-corner feeling,
[03:07:58.840 --> 03:08:02.540]   where it's just not fleshed out in as many ways.
[03:08:02.540 --> 03:08:04.700]   Sure, they added math and coding capabilities
[03:08:04.700 --> 03:08:06.860]   via these verifiers in RL,
[03:08:06.860 --> 03:08:10.220]   but it feels like they lost something in certain areas.
[03:08:10.220 --> 03:08:12.620]   And O1 is worse performing than Chat
[03:08:12.620 --> 03:08:15.140]   in many areas, as well, to be clear.
[03:08:15.140 --> 03:08:15.980]   - Not by a lot.
[03:08:15.980 --> 03:08:16.900]   - Not by a lot, though, right?
[03:08:16.900 --> 03:08:19.820]   And it's like, R1 definitely felt to me
[03:08:19.820 --> 03:08:21.980]   like it was worse than V3 in certain areas,
[03:08:21.980 --> 03:08:25.140]   like doing this RL expressed and learned a lot,
[03:08:25.140 --> 03:08:27.300]   but then it weakened in other areas.
[03:08:27.300 --> 03:08:29.940]   And so I think that's one of the big differences
[03:08:29.940 --> 03:08:33.820]   between these models and what O1 offers.
[03:08:33.820 --> 03:08:36.140]   And then OpenAI has O1 Pro,
[03:08:36.140 --> 03:08:37.340]   and what they did with O3,
[03:08:37.340 --> 03:08:39.020]   which is also very unique,
[03:08:39.020 --> 03:08:42.180]   is that they stacked Search on top of Chain of Thought,
[03:08:42.180 --> 03:08:43.460]   right?
[03:08:43.460 --> 03:08:45.300]   And so Chain of Thought is one thing where it's able,
[03:08:45.300 --> 03:08:48.140]   it's one chain, it backtracks, goes back and forth,
[03:08:48.140 --> 03:08:50.740]   but how they solved the ArcAGI challenge
[03:08:50.740 --> 03:08:53.140]   was not just the Chain of Thought.
[03:08:53.140 --> 03:08:55.100]   It was also sampling many times,
[03:08:55.100 --> 03:08:58.100]   i.e. running them in parallel, and then selecting.
[03:08:58.100 --> 03:09:00.300]   - Is running in parallel actually Search?
[03:09:00.300 --> 03:09:02.180]   'Cause I don't know if we have the full information
[03:09:02.180 --> 03:09:03.300]   on how O1 Pro works.
[03:09:03.300 --> 03:09:05.380]   So I don't have enough information
[03:09:05.380 --> 03:09:07.100]   to confidently say that it is Search.
[03:09:07.100 --> 03:09:08.620]   - It is parallel samples.
[03:09:08.620 --> 03:09:10.340]   - Yeah, and then what? - And it selects something.
[03:09:10.340 --> 03:09:12.180]   - And we don't know what the selection function is.
[03:09:12.180 --> 03:09:13.860]   The reason why we're debating
[03:09:13.860 --> 03:09:16.180]   is because since O1 was announced,
[03:09:16.180 --> 03:09:17.740]   there's been a lot of interest in techniques
[03:09:17.740 --> 03:09:18.980]   called Monte Carlo Tree Search,
[03:09:18.980 --> 03:09:21.580]   which is where you will break down the Chain of Thought
[03:09:21.580 --> 03:09:22.780]   into intermediate steps.
[03:09:22.780 --> 03:09:24.580]   We haven't defined Chain of Thought.
[03:09:24.580 --> 03:09:27.340]   Chain of Thought is from a paper from years ago
[03:09:27.340 --> 03:09:29.980]   where you introduce the idea to ask a language model
[03:09:29.980 --> 03:09:32.740]   that at the time was much less easy to use.
[03:09:32.740 --> 03:09:34.900]   You would say, "Let's verify step-by-step,"
[03:09:34.900 --> 03:09:36.100]   and it would induce the model
[03:09:36.100 --> 03:09:38.220]   to do this bulleted list of steps.
[03:09:38.220 --> 03:09:41.060]   Chain of Thought is now almost a default in models,
[03:09:41.060 --> 03:09:42.300]   where if you ask it a math question,
[03:09:42.300 --> 03:09:44.500]   you don't need to tell it to think step-by-step.
[03:09:44.500 --> 03:09:46.980]   And the idea with Monte Carlo Tree Search
[03:09:46.980 --> 03:09:49.980]   is that you would take an intermediate point in that chain,
[03:09:49.980 --> 03:09:52.340]   do some sort of expansion, spend more compute,
[03:09:52.340 --> 03:09:53.780]   and then just select the right one.
[03:09:53.780 --> 03:09:55.500]   That's like a very complex form of Search
[03:09:55.500 --> 03:09:58.700]   that has been used in things like Mu0 and AlphaZero.
[03:09:58.700 --> 03:10:00.940]   Potentially, I know Mu0 does this.
[03:10:00.940 --> 03:10:02.900]   - Another form of Search is just asking
[03:10:02.900 --> 03:10:05.180]   five different people and then taking the majority answer.
[03:10:05.180 --> 03:10:06.020]   - Yes.
[03:10:06.020 --> 03:10:08.740]   - There's a variety of, it could be complicated,
[03:10:08.740 --> 03:10:09.980]   it could be simple.
[03:10:09.980 --> 03:10:10.900]   We don't know what it is,
[03:10:10.900 --> 03:10:13.100]   just that they are not just issuing
[03:10:13.100 --> 03:10:15.220]   one Chain of Thought in sequence.
[03:10:15.220 --> 03:10:17.300]   They're launching many in parallel.
[03:10:17.300 --> 03:10:20.500]   And in the Arc AGI, they launched 1,000 in parallel
[03:10:20.500 --> 03:10:23.060]   for the one that really shocked everyone,
[03:10:23.060 --> 03:10:24.020]   that beat the benchmark,
[03:10:24.020 --> 03:10:26.340]   was they would launch 1,000 in parallel,
[03:10:26.340 --> 03:10:27.580]   and then they would get the right answer
[03:10:27.580 --> 03:10:30.540]   like 80% of the time or 70% of the time, 90 maybe even.
[03:10:30.540 --> 03:10:32.420]   Whereas if they just launched one,
[03:10:32.420 --> 03:10:33.700]   it was like 30%.
[03:10:33.700 --> 03:10:35.540]   - There are many extensions to this.
[03:10:35.540 --> 03:10:36.980]   I would say the simplest one
[03:10:36.980 --> 03:10:39.020]   is that our language models to date
[03:10:39.020 --> 03:10:41.620]   have been designed to give the right answer
[03:10:41.620 --> 03:10:44.860]   the highest percentage of the time in one response.
[03:10:44.860 --> 03:10:46.300]   And we are now opening the door
[03:10:46.300 --> 03:10:49.500]   to different ways of running inference on our models,
[03:10:49.500 --> 03:10:51.100]   in which we need to re-evaluate
[03:10:51.100 --> 03:10:53.460]   many parts of the training process,
[03:10:53.460 --> 03:10:56.140]   which normally opens the door to more progress,
[03:10:56.140 --> 03:10:58.420]   but we don't know if OpenAI changed a lot,
[03:10:58.420 --> 03:11:00.780]   or if just sampling more and multiple choice
[03:11:00.780 --> 03:11:01.660]   is what they're doing,
[03:11:01.660 --> 03:11:02.940]   or if it's something more complex,
[03:11:02.940 --> 03:11:04.140]   but they changed the training
[03:11:04.140 --> 03:11:06.460]   and they know that the inference mode
[03:11:06.460 --> 03:11:07.700]   is going to be different.
[03:11:07.700 --> 03:11:11.100]   - So we're talking about O1 Pro, $200 a month,
[03:11:11.100 --> 03:11:13.020]   and they're losing money.
[03:11:13.020 --> 03:11:16.660]   So the thing that we're referring to,
[03:11:16.660 --> 03:11:21.660]   this fascinating exploration of the test time compute space,
[03:11:21.660 --> 03:11:24.260]   is that actually possible?
[03:11:24.260 --> 03:11:26.140]   Do we have enough compute for that?
[03:11:26.140 --> 03:11:27.700]   Does the financials make sense?
[03:11:27.700 --> 03:11:29.740]   - So the fantastic thing is,
[03:11:29.740 --> 03:11:32.900]   and it's in the thing that I pulled up earlier,
[03:11:32.900 --> 03:11:37.620]   but the cost for GPT-3 has plummeted.
[03:11:37.620 --> 03:11:40.620]   If you scroll up just a few images, I think.
[03:11:40.620 --> 03:11:41.660]   The important thing about like, hey,
[03:11:41.660 --> 03:11:44.300]   is cost a limiting factor here, right?
[03:11:44.300 --> 03:11:45.900]   Like my view is that like,
[03:11:45.900 --> 03:11:48.300]   we'll have like really awesome intelligence
[03:11:48.300 --> 03:11:49.860]   before we have like AGI,
[03:11:49.860 --> 03:11:52.300]   before we have it permeate throughout the economy.
[03:11:52.300 --> 03:11:54.380]   And this is sort of why that reason is, right?
[03:11:54.380 --> 03:11:56.940]   GPT-3 was trained in what, 2020, 2021?
[03:11:57.900 --> 03:12:00.460]   And the cost for running inference on it
[03:12:00.460 --> 03:12:04.420]   was $60, $70 per million tokens, right?
[03:12:04.420 --> 03:12:07.100]   Which is the cost per intelligence was ridiculous.
[03:12:07.100 --> 03:12:08.900]   Now, as we scaled forward two years,
[03:12:08.900 --> 03:12:12.180]   we've had a 1200X reduction in cost
[03:12:12.180 --> 03:12:15.020]   to achieve the same level of intelligence as GPT-3.
[03:12:15.020 --> 03:12:19.660]   - So here on the X-axis is time over just a couple of years,
[03:12:19.660 --> 03:12:24.660]   and on the Y-axis is log scale dollars
[03:12:24.660 --> 03:12:27.540]   to run inference on a million tokens.
[03:12:27.540 --> 03:12:28.380]   - Yeah, a million.
[03:12:28.380 --> 03:12:30.940]   - And so you have just a down,
[03:12:30.940 --> 03:12:34.780]   like a linear decline on log scale
[03:12:34.780 --> 03:12:37.620]   from GPT-3 through 3.5 to LAMA.
[03:12:37.620 --> 03:12:39.460]   - It's like 5 cents or something like that now, right?
[03:12:39.460 --> 03:12:44.060]   Which is versus $60, 1200X, that's not the exact numbers,
[03:12:44.060 --> 03:12:46.060]   but it's 1200X, I remember that number,
[03:12:46.060 --> 03:12:50.140]   is the humongous cost per intelligence, right?
[03:12:50.140 --> 03:12:52.060]   Now the freak out over DeepSeek is,
[03:12:52.060 --> 03:12:53.660]   oh my God, they made it so cheap.
[03:12:53.660 --> 03:12:55.900]   It's like, actually, if you look at this trend line,
[03:12:55.900 --> 03:12:57.780]   they're not below the trend line, first of all,
[03:12:57.780 --> 03:12:59.820]   and at least for GPT-3, right?
[03:12:59.820 --> 03:13:02.540]   They are the first to hit it, which is a big deal,
[03:13:02.540 --> 03:13:05.020]   but they're not below the trend line as far as GPT-3.
[03:13:05.020 --> 03:13:06.660]   Now we have GPT-4, what's gonna happen
[03:13:06.660 --> 03:13:08.340]   with these reasoning capabilities, right?
[03:13:08.340 --> 03:13:10.820]   It's a mix of architectural innovations,
[03:13:10.820 --> 03:13:12.380]   it's a mix of better data,
[03:13:12.380 --> 03:13:13.900]   and it's gonna be better training techniques,
[03:13:13.900 --> 03:13:16.340]   and all of these different better inference systems,
[03:13:16.340 --> 03:13:17.500]   better hardware, right?
[03:13:17.500 --> 03:13:20.260]   Going from each generation of GPU
[03:13:20.260 --> 03:13:22.380]   to new generations or ASICs,
[03:13:22.380 --> 03:13:24.740]   everything is gonna take this cost curve
[03:13:24.740 --> 03:13:26.260]   down and down and down and down.
[03:13:26.260 --> 03:13:30.780]   And then, can I just spawn a thousand different LLMs
[03:13:30.780 --> 03:13:33.140]   to create a task and then pick from one of them,
[03:13:33.140 --> 03:13:35.340]   or whatever search technique I want,
[03:13:35.340 --> 03:13:37.000]   a tree, Monte Carlo tree search,
[03:13:37.000 --> 03:13:39.100]   maybe it gets that complicated.
[03:13:39.100 --> 03:13:40.460]   Maybe it doesn't, 'cause it's too complicated
[03:13:40.460 --> 03:13:42.260]   to actually scale, like who knows?
[03:13:42.260 --> 03:13:43.820]   Bitter lesson, right?
[03:13:43.820 --> 03:13:47.980]   The question is, I think, when, not if,
[03:13:47.980 --> 03:13:51.700]   because the rate of progress is so fast, right?
[03:13:51.700 --> 03:13:55.220]   Nine months ago, Dario said nine months ago
[03:13:55.220 --> 03:13:57.660]   the cost to train and inference was this, right?
[03:13:57.660 --> 03:13:59.820]   And now we're much better than this, right?
[03:13:59.820 --> 03:14:01.620]   And DeepSeek is much better than this.
[03:14:01.620 --> 03:14:03.800]   And that cost curve for GPT-4,
[03:14:03.800 --> 03:14:06.860]   which was also roughly $60 per million tokens
[03:14:06.860 --> 03:14:11.340]   when it launched, has already fallen to $2 or so, right?
[03:14:11.340 --> 03:14:13.800]   And we're gonna get it down to cents, probably,
[03:14:13.800 --> 03:14:17.100]   for GPT-4 quality, and then that's the base
[03:14:17.100 --> 03:14:20.540]   for the reasoning models like O1 that we have today,
[03:14:20.540 --> 03:14:23.060]   and O1 Pro is spawning multiple, right?
[03:14:23.060 --> 03:14:24.580]   And O3, and so on and so forth.
[03:14:24.580 --> 03:14:26.620]   These search techniques, too expensive today,
[03:14:26.620 --> 03:14:27.900]   but they will get cheaper.
[03:14:27.900 --> 03:14:31.100]   And that's what's gonna unlock the intelligence, right?
[03:14:31.100 --> 03:14:33.780]   - So it'll get cheaper and cheaper and cheaper.
[03:14:33.780 --> 03:14:38.700]   The big DeepSeek R1 release freaked everybody out
[03:14:38.700 --> 03:14:39.820]   because of the cheaper.
[03:14:39.820 --> 03:14:44.060]   One of the manifestations of that is Nvidia stock plummeted.
[03:14:44.060 --> 03:14:46.060]   Can you explain what happened?
[03:14:46.060 --> 03:14:48.860]   I mean, and also just explain this moment
[03:14:48.860 --> 03:14:52.540]   and whether, you know, if Nvidia is gonna keep winning.
[03:14:52.540 --> 03:14:55.180]   - We're both Nvidia bulls here, I would say.
[03:14:55.180 --> 03:14:59.220]   And in some ways, the market response is reasonable.
[03:14:59.220 --> 03:15:03.180]   Most of the market, like Nvidia's biggest customers
[03:15:03.180 --> 03:15:04.980]   in the US are major tech companies,
[03:15:04.980 --> 03:15:06.700]   and they're spending a ton on AI.
[03:15:06.700 --> 03:15:09.700]   And if a simple interpretation of DeepSeek
[03:15:09.700 --> 03:15:11.220]   is you can get really good models
[03:15:11.220 --> 03:15:13.380]   without spending as much on AI.
[03:15:13.380 --> 03:15:14.980]   So in that capacity, it's like,
[03:15:14.980 --> 03:15:16.260]   oh, maybe these big tech companies
[03:15:16.260 --> 03:15:18.420]   won't need to spend as much on AI and go down.
[03:15:18.420 --> 03:15:20.540]   The actual thing that happened is much more complex
[03:15:20.540 --> 03:15:21.780]   where there's social factors,
[03:15:21.780 --> 03:15:23.580]   where there's the rising in the app store,
[03:15:23.580 --> 03:15:26.140]   the social contagion that is happening.
[03:15:26.140 --> 03:15:28.220]   And then I think a lot of some of it is just like,
[03:15:28.220 --> 03:15:29.500]   I'm not, I don't trade.
[03:15:29.500 --> 03:15:30.860]   I don't know anything about financial markets,
[03:15:30.860 --> 03:15:32.060]   but it builds up over the weekend
[03:15:32.060 --> 03:15:33.940]   or the social pressure where it's like,
[03:15:33.940 --> 03:15:36.020]   if it was during the week and there was multiple days
[03:15:36.020 --> 03:15:37.860]   of trading when this was really becoming,
[03:15:37.860 --> 03:15:38.900]   but it comes on the weekend
[03:15:38.900 --> 03:15:41.060]   and then everybody wants to sell.
[03:15:41.060 --> 03:15:43.180]   And that is a social contagion.
[03:15:43.180 --> 03:15:45.820]   - I think, and like, there were a lot of false narratives,
[03:15:45.820 --> 03:15:48.180]   which is like, hey, these guys are spending billions
[03:15:48.180 --> 03:15:49.020]   on models, right?
[03:15:49.020 --> 03:15:50.420]   And they're not spending billions on models.
[03:15:50.420 --> 03:15:52.900]   No one's spent more than a billion dollars
[03:15:52.900 --> 03:15:54.740]   on a model that's released publicly, right?
[03:15:54.740 --> 03:15:57.460]   GPT-4 was a couple hundred million,
[03:15:57.460 --> 03:16:00.500]   and then they've reduced the cost with 4.0,
[03:16:00.500 --> 03:16:02.420]   4.0 Turbo, 4.0, right?
[03:16:02.420 --> 03:16:05.220]   But billion dollar model runs are coming, right?
[03:16:05.220 --> 03:16:06.860]   This concludes pre-training and post-training, right?
[03:16:06.860 --> 03:16:07.980]   And then the other number is like,
[03:16:07.980 --> 03:16:09.580]   hey, DeepSeek didn't include everything, right?
[03:16:09.580 --> 03:16:11.780]   They didn't include, a lot of the cost goes to research
[03:16:11.780 --> 03:16:12.620]   and all this sort of stuff.
[03:16:12.620 --> 03:16:14.260]   A lot of the cost goes to inference.
[03:16:14.260 --> 03:16:15.500]   A lot of the cost goes to post-training.
[03:16:15.500 --> 03:16:16.660]   None of these things were factored.
[03:16:16.660 --> 03:16:17.740]   It's research salaries, right?
[03:16:17.740 --> 03:16:19.580]   Like all these things are like counted
[03:16:19.580 --> 03:16:21.900]   in the billions of dollars that OpenAI is spending,
[03:16:21.900 --> 03:16:23.500]   but they weren't counted in the, you know,
[03:16:23.500 --> 03:16:26.380]   hey, $6 million, $5 million that DeepSeek spent, right?
[03:16:26.380 --> 03:16:28.260]   So there's a bit of misunderstanding
[03:16:28.260 --> 03:16:29.700]   of what these numbers are.
[03:16:29.700 --> 03:16:31.740]   And then there's also an element of,
[03:16:31.740 --> 03:16:34.540]   NVIDIA has just been a straight line up, right?
[03:16:34.540 --> 03:16:37.580]   And there's been so many different narratives
[03:16:37.580 --> 03:16:39.500]   that have been trying to push down NVIDIA.
[03:16:39.500 --> 03:16:41.060]   I don't say push down NVIDIA stock.
[03:16:41.060 --> 03:16:43.020]   Everyone is looking for a reason to sell
[03:16:43.020 --> 03:16:44.620]   or to be worried, right?
[03:16:44.620 --> 03:16:47.180]   You know, it was Blackwell delays, right?
[03:16:47.180 --> 03:16:48.860]   Their GPU, you know, there's a lot of report.
[03:16:48.860 --> 03:16:50.020]   Every two weeks, there's a new report
[03:16:50.020 --> 03:16:52.020]   about their GPUs being delayed.
[03:16:52.020 --> 03:16:56.340]   There's the whole thing about scaling laws ending, right?
[03:16:56.340 --> 03:16:57.820]   It's so ironic, right?
[03:16:57.820 --> 03:16:59.580]   - It lasted a month.
[03:16:59.580 --> 03:17:02.060]   - It was just like literally just,
[03:17:02.060 --> 03:17:03.820]   hey, models aren't getting better, right?
[03:17:03.820 --> 03:17:04.900]   They're just not getting better.
[03:17:04.900 --> 03:17:06.100]   There's no reason to spend more.
[03:17:06.100 --> 03:17:07.820]   Pre-training scaling is dead.
[03:17:07.820 --> 03:17:10.380]   And then it's like, oh one, oh three, right?
[03:17:10.380 --> 03:17:11.780]   - R1. - R1, right?
[03:17:11.780 --> 03:17:13.940]   And now it's like, wait, models are getting too,
[03:17:13.940 --> 03:17:15.340]   they're progressing too fast.
[03:17:15.340 --> 03:17:16.420]   Slow down the progress.
[03:17:16.420 --> 03:17:18.500]   Stop spending on GPUs, right?
[03:17:18.500 --> 03:17:19.900]   But you know, the funniest thing I think
[03:17:19.900 --> 03:17:21.540]   that like comes out of this is
[03:17:21.540 --> 03:17:24.340]   Javon's paradox is true, right?
[03:17:24.340 --> 03:17:26.740]   AWS pricing for H100s has gone up
[03:17:26.740 --> 03:17:28.300]   over the last couple of weeks, right?
[03:17:28.300 --> 03:17:31.100]   Since a little bit after Christmas,
[03:17:31.100 --> 03:17:34.340]   since V3 was launched, AWS H100 pricing has gone up.
[03:17:34.340 --> 03:17:36.780]   H200s are like almost out of stock everywhere
[03:17:36.780 --> 03:17:39.460]   because you know, H200 has more memory
[03:17:39.460 --> 03:17:41.060]   and therefore R1 like, you know,
[03:17:41.060 --> 03:17:42.980]   wants that chip over H100, right?
[03:17:42.980 --> 03:17:44.860]   - We were trying to get GPUs on a short notice
[03:17:44.860 --> 03:17:46.540]   this week for a demo and it wasn't that easy.
[03:17:46.540 --> 03:17:48.980]   We were trying to get just like 16 or 32 H100s
[03:17:48.980 --> 03:17:51.100]   for a demo and it was not very easy.
[03:17:51.100 --> 03:17:53.260]   - So for people who don't know, Javon's paradox
[03:17:53.260 --> 03:17:58.180]   is when, you know, the efficiency goes up,
[03:17:58.180 --> 03:18:00.780]   somehow magically, counterintuitively,
[03:18:00.780 --> 03:18:03.020]   the total resource consumption goes up as well.
[03:18:03.020 --> 03:18:04.820]   - Right, and semiconductors is, you know,
[03:18:04.820 --> 03:18:07.260]   we're at 50 years of Moore's law.
[03:18:07.260 --> 03:18:09.860]   Every two years, half the cost, double the transistors.
[03:18:09.860 --> 03:18:11.540]   Just like clockwork, and it's slowed down obviously,
[03:18:11.540 --> 03:18:13.580]   but like the semiconductor industry
[03:18:13.580 --> 03:18:15.340]   has gone up the whole time, right?
[03:18:15.340 --> 03:18:16.180]   It's been wavy, right?
[03:18:16.180 --> 03:18:17.540]   There's obviously cycles and stuff
[03:18:17.540 --> 03:18:19.500]   and I don't expect AI to be any different, right?
[03:18:19.500 --> 03:18:22.140]   There's gonna be ebbs and flows, but this is,
[03:18:22.140 --> 03:18:24.980]   in AI, it's just playing out at an insane timescale, right?
[03:18:24.980 --> 03:18:26.980]   It was 2X every two years.
[03:18:26.980 --> 03:18:29.860]   This is 1200X in like three years, right?
[03:18:29.860 --> 03:18:32.180]   So it's like the scale of improvement
[03:18:32.180 --> 03:18:34.580]   that is like hard to wrap your head around.
[03:18:34.580 --> 03:18:36.540]   - Yeah, I was confused because I, to me,
[03:18:36.540 --> 03:18:38.780]   Nvidia's stock on that should have gone up,
[03:18:38.780 --> 03:18:43.180]   but maybe it went down because there's kind of suspicion
[03:18:43.180 --> 03:18:45.860]   of foul play on the side of China or something like this.
[03:18:45.860 --> 03:18:47.540]   But if you just look purely
[03:18:47.540 --> 03:18:49.340]   at the actual principles at play here,
[03:18:49.340 --> 03:18:52.180]   like it's obvious, yeah, the Gervant's Paradox.
[03:18:52.180 --> 03:18:53.900]   - The more progress that AI makes,
[03:18:53.900 --> 03:18:57.580]   or the higher the derivative of AI progress is,
[03:18:57.580 --> 03:19:00.100]   especially you should, 'cause Nvidia's in the best place.
[03:19:00.100 --> 03:19:01.300]   The higher the derivative is,
[03:19:01.300 --> 03:19:03.860]   the sooner the market's gonna be bigger and expanding
[03:19:03.860 --> 03:19:05.020]   and Nvidia's the only one
[03:19:05.020 --> 03:19:07.460]   that does everything reliably right now.
[03:19:07.460 --> 03:19:11.180]   - 'Cause it's not like an Nvidia competitor arose.
[03:19:11.180 --> 03:19:14.460]   It's another company that's using Nvidia.
[03:19:14.460 --> 03:19:17.220]   - Who historically has been a large Nvidia customer.
[03:19:17.220 --> 03:19:18.060]   - Yeah.
[03:19:18.060 --> 03:19:20.260]   - And has press releases about them cheering
[03:19:20.260 --> 03:19:22.860]   about being China's biggest Nvidia customer, right?
[03:19:22.860 --> 03:19:23.820]   Like.
[03:19:23.820 --> 03:19:24.980]   - Yeah, I mean.
[03:19:24.980 --> 03:19:26.260]   - Obviously they've quieted down,
[03:19:26.260 --> 03:19:28.220]   but like, I think that's like another element
[03:19:28.220 --> 03:19:30.580]   of is that they don't wanna say how many GPUs they have.
[03:19:30.580 --> 03:19:31.420]   - Yeah.
[03:19:31.420 --> 03:19:33.780]   - Because, hey, yes, they have H800s.
[03:19:33.780 --> 03:19:35.100]   Yes, they have H20s.
[03:19:35.100 --> 03:19:36.860]   They also have some H100s, right?
[03:19:36.860 --> 03:19:37.700]   Which are smuggled in.
[03:19:37.700 --> 03:19:39.940]   - Can you speak to that, to the smuggling?
[03:19:39.940 --> 03:19:42.820]   What's the scale of smuggling that's feasible
[03:19:42.820 --> 03:19:45.140]   for a nation state to do for companies?
[03:19:45.140 --> 03:19:46.660]   Is it possible to?
[03:19:46.660 --> 03:19:49.820]   - I think there's a few angles of smuggling here, right?
[03:19:49.820 --> 03:19:51.860]   One is ByteDance arguably
[03:19:51.860 --> 03:19:54.300]   is the largest smuggler of GPUs for China, right?
[03:19:54.300 --> 03:19:55.820]   China's not supposed to have GPUs.
[03:19:55.820 --> 03:19:58.180]   ByteDance has like over 500,000 GPUs.
[03:19:58.180 --> 03:19:59.020]   Why?
[03:19:59.020 --> 03:20:01.380]   Because they're all rented from companies around the world.
[03:20:01.380 --> 03:20:02.460]   They rent from Oracle.
[03:20:02.460 --> 03:20:03.420]   They rent from Google.
[03:20:03.420 --> 03:20:04.740]   They rent from all these mass
[03:20:04.740 --> 03:20:06.700]   and a bunch of smaller cloud companies too, right?
[03:20:06.700 --> 03:20:07.820]   All the Neo clouds, right?
[03:20:07.820 --> 03:20:08.660]   Of the world.
[03:20:08.660 --> 03:20:10.220]   They rent so, so many GPUs.
[03:20:10.220 --> 03:20:11.820]   They also buy a bunch, right?
[03:20:11.820 --> 03:20:14.700]   And they do this for mostly like what Meta does, right?
[03:20:14.700 --> 03:20:16.060]   Serving TikTok, right?
[03:20:16.060 --> 03:20:19.380]   Serving, next best, same as Meta, right?
[03:20:19.380 --> 03:20:21.260]   To be clear, that's today the use, right?
[03:20:21.260 --> 03:20:22.820]   And it's a valid use, right?
[03:20:22.820 --> 03:20:25.220]   Hack the dopamine circuit, right?
[03:20:25.220 --> 03:20:29.220]   Now that's theoretically now very much restricted
[03:20:29.220 --> 03:20:30.460]   with the AI diffusion rules,
[03:20:30.460 --> 03:20:32.500]   which happened in the last week of the Biden admin
[03:20:32.500 --> 03:20:35.140]   and Trump admin looks like they're gonna keep them,
[03:20:35.140 --> 03:20:38.940]   which limits like allies, even like Singapore,
[03:20:38.940 --> 03:20:41.180]   which Singapore is like 20% of NVIDIA's 20,
[03:20:41.180 --> 03:20:42.900]   20, 30% of NVIDIA's revenue.
[03:20:42.900 --> 03:20:45.180]   But Singapore has had a memorandum
[03:20:45.180 --> 03:20:46.940]   on not building data centers for like 15 years
[03:20:46.940 --> 03:20:47.860]   'cause they don't have enough power.
[03:20:47.860 --> 03:20:49.620]   So where are they going?
[03:20:49.620 --> 03:20:50.460]   (laughs)
[03:20:50.460 --> 03:20:52.860]   I mean, I'm not claiming they're all going to China, right?
[03:20:52.860 --> 03:20:55.460]   But a portion are, you know, many are going to Malaysia,
[03:20:55.460 --> 03:20:56.660]   including Microsoft and Oracle
[03:20:56.660 --> 03:20:58.020]   have big data centers in Malaysia.
[03:20:58.020 --> 03:21:00.180]   Like, you know, they're going all over Southeast Asia,
[03:21:00.180 --> 03:21:01.780]   probably India as well, right?
[03:21:01.780 --> 03:21:02.860]   Like there's stuff routing,
[03:21:02.860 --> 03:21:05.540]   but like the diffusion rules are very de facto.
[03:21:05.540 --> 03:21:08.020]   Like you can only buy this many GPUs from this country
[03:21:08.020 --> 03:21:11.060]   and it's, and you can only rent a cluster this large
[03:21:11.060 --> 03:21:12.260]   to companies that are Chinese, right?
[03:21:12.260 --> 03:21:15.700]   Like they're very explicit on trying to stop smuggling,
[03:21:15.700 --> 03:21:16.540]   right?
[03:21:16.540 --> 03:21:18.460]   And a big chunk of it was, hey, let's, you know,
[03:21:18.460 --> 03:21:23.380]   random company by 16 servers, ship some to China, right?
[03:21:23.380 --> 03:21:24.220]   There's actually,
[03:21:24.220 --> 03:21:27.940]   I saw a photo from someone in the semiconductor industry
[03:21:27.940 --> 03:21:32.540]   who leads like a team for like networking chips
[03:21:32.540 --> 03:21:33.780]   that competes with NVIDIA.
[03:21:33.780 --> 03:21:35.500]   And he sent a photo of a guy
[03:21:35.500 --> 03:21:37.820]   checking into a first-class United flight
[03:21:37.820 --> 03:21:40.980]   from San Francisco to Shanghai or Shenzhen
[03:21:40.980 --> 03:21:43.860]   with a super micro box that was this big,
[03:21:43.860 --> 03:21:46.700]   which can only contain GPUs, right?
[03:21:46.700 --> 03:21:48.940]   And he was booking first-class 'cause think about it,
[03:21:48.940 --> 03:21:50.820]   three to 5K for your first-class ticket,
[03:21:50.820 --> 03:21:54.380]   server costs, you know, 240,000 in the US, 250,000.
[03:21:54.380 --> 03:21:56.480]   You sell it for 300,000 in China.
[03:21:56.480 --> 03:21:58.820]   Wait, you just got a free first-class ticket
[03:21:58.820 --> 03:22:00.140]   and a lot more money.
[03:22:00.140 --> 03:22:00.980]   So it's like, you know,
[03:22:00.980 --> 03:22:02.580]   and that's like small-scale smuggling.
[03:22:02.580 --> 03:22:03.820]   Most of the large-scale smuggling
[03:22:03.820 --> 03:22:06.860]   is like companies in Singapore and Malaysia,
[03:22:06.860 --> 03:22:09.180]   like routing them around or renting GPUs.
[03:22:09.180 --> 03:22:10.660]   - I want to jump in.
[03:22:10.660 --> 03:22:11.540]   How much was the scale?
[03:22:11.540 --> 03:22:12.580]   I think there's been some number,
[03:22:12.580 --> 03:22:16.040]   like some people that are higher level economics
[03:22:16.040 --> 03:22:18.580]   understanding say that as you go from 1 billion
[03:22:18.580 --> 03:22:19.820]   of smuggling to 10 billion,
[03:22:19.820 --> 03:22:22.740]   it's like you're hiding certain levels of economic activity.
[03:22:22.740 --> 03:22:24.260]   And that's the most reasonable thing to me
[03:22:24.260 --> 03:22:25.500]   is that there's going to be some level
[03:22:25.500 --> 03:22:28.220]   where it's so obvious that it's easier to find
[03:22:28.220 --> 03:22:29.380]   this economic activity.
[03:22:29.380 --> 03:22:34.380]   - Yeah, so my belief is that last year roughly,
[03:22:34.380 --> 03:22:37.140]   so NVIDIA made a million H20s,
[03:22:37.140 --> 03:22:39.060]   which are legally allowed to be shipped to China,
[03:22:39.060 --> 03:22:41.360]   which we talked about is better for reasoning, right?
[03:22:41.360 --> 03:22:43.700]   Inference at least, maybe not training,
[03:22:43.700 --> 03:22:46.340]   but reasoning inference and inference generally.
[03:22:46.340 --> 03:22:49.500]   Then they also had, you know, a couple hundred thousand,
[03:22:49.500 --> 03:22:53.680]   we think like 200 to 300,000 GPUs were routed to China
[03:22:53.680 --> 03:22:56.220]   from, you know, Singapore, Malaysia, US, wherever.
[03:22:56.220 --> 03:22:58.980]   Companies spawn up by 16 GPUs, 64 GPUs,
[03:22:58.980 --> 03:22:59.940]   whatever it is, route it.
[03:22:59.940 --> 03:23:02.580]   And Huawei is known for having spent up a massive network
[03:23:02.580 --> 03:23:04.740]   of like companies to get the materials they need
[03:23:04.740 --> 03:23:06.340]   after they were banned in like 2018.
[03:23:06.340 --> 03:23:08.700]   So it's not like otherworldly, but I agree, right?
[03:23:08.700 --> 03:23:10.420]   Nathan's point is like,
[03:23:10.420 --> 03:23:12.900]   hey, you can't smuggle up $10 billion of GPUs.
[03:23:12.900 --> 03:23:14.140]   And then the third sort of source,
[03:23:14.140 --> 03:23:15.740]   which is just now banned and you know,
[03:23:15.740 --> 03:23:17.160]   which wasn't considered smuggling,
[03:23:17.160 --> 03:23:19.740]   but as China is renting, like is,
[03:23:19.740 --> 03:23:22.620]   I believe from our research, right?
[03:23:22.620 --> 03:23:26.700]   Oracle's biggest GPU customer is ByteDance, right?
[03:23:26.700 --> 03:23:28.080]   And for a Google,
[03:23:28.080 --> 03:23:29.860]   I think it's their second biggest customer, right?
[03:23:29.860 --> 03:23:31.660]   And so like, and you go down the list of clouds
[03:23:31.660 --> 03:23:33.140]   and especially these smaller cloud companies
[03:23:33.140 --> 03:23:35.900]   that aren't like the hyperscalers, right?
[03:23:35.900 --> 03:23:38.940]   Think beyond Core, even Lambda, even there's a whole C,
[03:23:38.940 --> 03:23:40.580]   there's 60 different new cloud companies
[03:23:40.580 --> 03:23:41.420]   serving Nvidia GPUs.
[03:23:41.420 --> 03:23:44.060]   I think ByteDance is renting a lot of these, right?
[03:23:44.060 --> 03:23:44.900]   All over, right?
[03:23:44.900 --> 03:23:49.100]   And so these companies are renting GPUs to Chinese companies
[03:23:49.100 --> 03:23:51.380]   and that's completely, that was completely legal
[03:23:51.380 --> 03:23:53.020]   up until the diffusion rules,
[03:23:53.020 --> 03:23:54.500]   which happened just a few weeks ago.
[03:23:54.500 --> 03:23:56.940]   And even now you can rent GPU clusters
[03:23:56.940 --> 03:23:58.460]   that are less than 2000 GPUs,
[03:23:58.460 --> 03:24:01.260]   or you can buy GPUs and ship them wherever you want
[03:24:01.260 --> 03:24:03.320]   if they're less than 1500 GPUs, right?
[03:24:03.320 --> 03:24:06.440]   So it's like, there are still like some ways to smuggle,
[03:24:06.440 --> 03:24:09.580]   but yeah, it's not, you know, as the numbers grow, right?
[03:24:09.580 --> 03:24:11.220]   You know, a hundred something billion dollars
[03:24:11.220 --> 03:24:12.380]   of revenue for Nvidia last year,
[03:24:12.380 --> 03:24:14.420]   200 something billion this year, right?
[03:24:14.420 --> 03:24:15.940]   And if next year are, you know,
[03:24:15.940 --> 03:24:19.540]   it could nearly double again or more than double, right?
[03:24:19.540 --> 03:24:21.780]   Based on like what we see with data center footprints,
[03:24:21.780 --> 03:24:23.340]   like being built out all across the US
[03:24:23.340 --> 03:24:24.540]   and the rest of the world,
[03:24:24.540 --> 03:24:26.580]   it's gonna be really hard for China to keep up
[03:24:26.580 --> 03:24:28.100]   with these rules, right?
[03:24:28.100 --> 03:24:30.500]   Yes, there will always be smuggling
[03:24:30.500 --> 03:24:33.620]   and deep seek level models of GPT-4 level models,
[03:24:33.620 --> 03:24:36.600]   O1 level models capable to train on what China can get,
[03:24:36.600 --> 03:24:37.980]   even the next year above that.
[03:24:37.980 --> 03:24:42.620]   But if we speed run a couple more, you know, jumps, right?
[03:24:42.620 --> 03:24:44.980]   You know, to billion dollar models, $10 billion models,
[03:24:44.980 --> 03:24:46.500]   then it becomes, you know,
[03:24:46.500 --> 03:24:48.200]   hey, there is a compute disadvantage for China
[03:24:48.200 --> 03:24:50.060]   for training models and serving them.
[03:24:50.060 --> 03:24:52.340]   And the serving part is really critical, right?
[03:24:52.340 --> 03:24:54.180]   Deep seek cannot serve their model today, right?
[03:24:54.180 --> 03:24:56.960]   It's completely out of inventory.
[03:24:56.960 --> 03:24:59.180]   It's already started falling in the app store actually,
[03:24:59.180 --> 03:25:01.540]   downloads, because you download it, you try and sign up.
[03:25:01.540 --> 03:25:02.740]   They say, we're not taking registrations
[03:25:02.740 --> 03:25:03.940]   'cause they have no capacity, right?
[03:25:03.940 --> 03:25:04.780]   You open it up,
[03:25:04.780 --> 03:25:06.580]   you get like less than five tokens per second,
[03:25:06.580 --> 03:25:08.660]   if you even get your request approved, right?
[03:25:08.660 --> 03:25:10.340]   'Cause there's just no capacity
[03:25:10.340 --> 03:25:12.620]   because they just don't have enough GPUs to serve the model,
[03:25:12.620 --> 03:25:14.180]   even though it's incredibly efficient.
[03:25:14.180 --> 03:25:16.340]   - It'd be fascinating to watch the smuggling.
[03:25:16.340 --> 03:25:19.160]   'Cause I mean, there's drug smuggling, right?
[03:25:19.160 --> 03:25:23.160]   That's a market, there's weapons smuggling,
[03:25:23.160 --> 03:25:25.840]   and GPUs will surpass that at some point.
[03:25:25.840 --> 03:25:29.420]   - Chips are highest value per kilogram, probably by far.
[03:25:29.420 --> 03:25:33.360]   I have another question for you, Don.
[03:25:33.360 --> 03:25:36.680]   Do you track model API access internationally?
[03:25:36.680 --> 03:25:39.020]   How easy is it for Chinese companies
[03:25:39.020 --> 03:25:42.120]   to use hosted model APIs from the US?
[03:25:42.120 --> 03:25:43.700]   - Yeah, I mean, that's incredibly easy, right?
[03:25:43.700 --> 03:25:46.820]   Like OpenAI publicly stated DeepSeek uses their API.
[03:25:46.820 --> 03:25:48.560]   And as they say, they have evidence, right?
[03:25:48.560 --> 03:25:50.780]   And this is another element of the training regime
[03:25:50.780 --> 03:25:52.720]   is people at OpenAI have claimed
[03:25:52.720 --> 03:25:53.880]   that it's a distilled model,
[03:25:53.880 --> 03:25:56.320]   i.e. you're taking OpenAI's model,
[03:25:56.320 --> 03:25:57.600]   you're generating a lot of output,
[03:25:57.600 --> 03:26:00.320]   and then you're training on the output in their model.
[03:26:00.320 --> 03:26:01.500]   And even if that's the case,
[03:26:01.500 --> 03:26:03.000]   what they did is still amazing, by the way,
[03:26:03.000 --> 03:26:04.320]   what DeepSeek did efficiency-wise.
[03:26:04.320 --> 03:26:06.460]   - Distillation is standard practice in industry,
[03:26:06.460 --> 03:26:08.260]   whether or not, if you're at a closed lab
[03:26:08.260 --> 03:26:10.480]   where you care about terms of service and IP closely,
[03:26:10.480 --> 03:26:12.160]   you distill from your own models.
[03:26:12.160 --> 03:26:13.400]   If you are a researcher
[03:26:13.400 --> 03:26:14.760]   and you're not building any products,
[03:26:14.760 --> 03:26:16.480]   you distill from the OpenAI models.
[03:26:16.480 --> 03:26:17.720]   - This is a good opportunity.
[03:26:17.720 --> 03:26:21.800]   Can you explain big picture distillation as a process?
[03:26:21.800 --> 03:26:23.040]   What is distillation?
[03:26:23.040 --> 03:26:23.880]   What's the process of distillation?
[03:26:23.880 --> 03:26:26.160]   - We've talked a lot about training language models.
[03:26:26.160 --> 03:26:27.480]   They are trained on text.
[03:26:27.480 --> 03:26:29.600]   In post-training, you're trying to train
[03:26:29.600 --> 03:26:30.920]   on very high-quality text
[03:26:30.920 --> 03:26:33.040]   that you want the model to match the features of,
[03:26:33.040 --> 03:26:34.440]   or if you're using RL,
[03:26:34.440 --> 03:26:36.040]   you're letting the model find its own thing.
[03:26:36.040 --> 03:26:38.780]   But for supervised fine-tuning, for preference data,
[03:26:38.780 --> 03:26:40.680]   you need to have some completions
[03:26:40.680 --> 03:26:42.960]   of what the model is trying to learn to imitate.
[03:26:42.960 --> 03:26:46.720]   And what you do there is, instead of a human data,
[03:26:46.720 --> 03:26:49.260]   or instead of the model you're currently training,
[03:26:49.260 --> 03:26:51.180]   you take completions from a different,
[03:26:51.180 --> 03:26:53.160]   normally more powerful model.
[03:26:53.160 --> 03:26:56.280]   I think there's rumors that these big models
[03:26:56.280 --> 03:26:57.780]   that people are waiting for,
[03:26:57.780 --> 03:26:59.320]   these GPT-5s of the world,
[03:26:59.320 --> 03:27:01.280]   the CLAWD-3 opuses of the world,
[03:27:01.280 --> 03:27:04.320]   are used internally to do this distillation process
[03:27:04.320 --> 03:27:05.160]   at OpenAI. - There's also
[03:27:05.160 --> 03:27:06.080]   public examples, right?
[03:27:06.080 --> 03:27:09.660]   Like Meta explicitly stated, not necessarily distilling,
[03:27:09.660 --> 03:27:12.240]   but they used 405(b) as a reward model
[03:27:12.240 --> 03:27:15.060]   for 70(b) in their LLAMA 3.2 and 3.3.
[03:27:15.060 --> 03:27:16.720]   - This is all the same topic.
[03:27:16.720 --> 03:27:20.700]   - So is this ethical, is this legal?
[03:27:20.700 --> 03:27:24.920]   Like why is that Financial Times article headline
[03:27:24.920 --> 03:27:28.080]   say OpenAI says that there's evidence
[03:27:28.080 --> 03:27:31.680]   that China's DeepSeek used its model to train competitor?
[03:27:31.680 --> 03:27:34.620]   - This is a long, at least in the academic side
[03:27:34.620 --> 03:27:35.960]   and research side, it has a long history
[03:27:35.960 --> 03:27:38.080]   'cause you're trying to interpret OpenAI's rule.
[03:27:38.080 --> 03:27:40.400]   OpenAI's terms of service say
[03:27:40.400 --> 03:27:41.960]   that you cannot build a competitor
[03:27:41.960 --> 03:27:43.400]   with outputs from their model.
[03:27:43.400 --> 03:27:45.100]   Terms of service are different than a license,
[03:27:45.100 --> 03:27:48.080]   which are essentially a contract between organizations.
[03:27:48.080 --> 03:27:50.420]   So if you have a terms of service on OpenAI's account,
[03:27:50.420 --> 03:27:53.100]   if I violate it, OpenAI can cancel my account.
[03:27:53.100 --> 03:27:54.760]   This is very different than like a license
[03:27:54.760 --> 03:27:56.960]   that says how you could use a downstream artifact.
[03:27:56.960 --> 03:27:58.320]   So a lot of it hinges on a word
[03:27:58.320 --> 03:28:00.080]   that is very unclear in the AI space,
[03:28:00.080 --> 03:28:01.480]   which is what is a competitor.
[03:28:01.480 --> 03:28:04.600]   - And then the ethical aspect of it is like,
[03:28:04.600 --> 03:28:07.200]   why is it unethical for me to train on your model
[03:28:07.200 --> 03:28:09.120]   when you can train on the internet's text?
[03:28:09.120 --> 03:28:10.040]   - Yeah. - Right?
[03:28:10.040 --> 03:28:11.800]   - So there's a bit of a hypocrisy
[03:28:11.800 --> 03:28:16.800]   because OpenAI and potentially most of the companies
[03:28:16.800 --> 03:28:20.240]   trained on the internet's text without permission.
[03:28:20.240 --> 03:28:21.480]   - There's also a clear loophole,
[03:28:21.480 --> 03:28:25.280]   which is that I generate data from OpenAI
[03:28:25.280 --> 03:28:26.840]   and then I upload it somewhere
[03:28:26.840 --> 03:28:28.420]   and then somebody else trains on it
[03:28:28.420 --> 03:28:30.160]   and the link has been broken.
[03:28:30.160 --> 03:28:32.920]   Like they're not under the same terms of service contract.
[03:28:32.920 --> 03:28:34.800]   - This is why-- - There's a lot of hypocrisy.
[03:28:34.800 --> 03:28:37.440]   There's a lot of link to be discovered details
[03:28:37.440 --> 03:28:38.560]   that don't make a lot of sense.
[03:28:38.560 --> 03:28:40.600]   - This is why a lot of models today,
[03:28:40.600 --> 03:28:43.320]   even if they train on zero OpenAI data,
[03:28:43.320 --> 03:28:44.840]   you ask the model who trained you,
[03:28:44.840 --> 03:28:47.540]   it'll say, "I am Chad GPT trained by OpenAI."
[03:28:47.540 --> 03:28:49.420]   Because there's so much copy paste
[03:28:49.420 --> 03:28:52.200]   of like OpenAI outputs from that on the internet
[03:28:52.200 --> 03:28:53.720]   that you just weren't able to filter it out.
[03:28:53.720 --> 03:28:55.640]   And there was nothing in the RL
[03:28:55.640 --> 03:28:57.800]   where they implemented like hey,
[03:28:57.800 --> 03:28:59.720]   or post-training or SFT, whatever that says,
[03:28:59.720 --> 03:29:02.760]   "Hey, I'm actually a model by Allen Institute
[03:29:02.760 --> 03:29:04.760]   instead of OpenAI." - We have to do this.
[03:29:04.760 --> 03:29:05.600]   We serve a demo.
[03:29:05.600 --> 03:29:08.920]   We do research and we use OpenAI APIs because it's useful
[03:29:08.920 --> 03:29:10.360]   and we wanna understand post-training
[03:29:10.360 --> 03:29:12.080]   and like our research models,
[03:29:12.080 --> 03:29:13.760]   they will say they're written by OpenAI
[03:29:13.760 --> 03:29:15.920]   unless we put in the system prop that we talked about
[03:29:15.920 --> 03:29:17.040]   that like, "I am Tulu.
[03:29:17.040 --> 03:29:19.960]   I am a language model trained by the Allen Institute for AI."
[03:29:19.960 --> 03:29:22.500]   And if you ask more people around industry,
[03:29:22.500 --> 03:29:23.800]   especially with post-training,
[03:29:23.800 --> 03:29:27.600]   it's a very doable task to make the model say who it is
[03:29:27.600 --> 03:29:29.680]   or to suppress the OpenAI thing.
[03:29:29.680 --> 03:29:32.920]   So in some levels, it might be that DeepSea didn't care
[03:29:32.920 --> 03:29:34.640]   that it was saying that it was by OpenAI.
[03:29:34.640 --> 03:29:36.560]   Like if you're gonna upload model weights,
[03:29:36.560 --> 03:29:37.400]   it doesn't really matter
[03:29:37.400 --> 03:29:40.080]   'cause anyone that's serving it in an application
[03:29:40.080 --> 03:29:42.480]   and cares a lot about serving is going to,
[03:29:42.480 --> 03:29:45.280]   when serving it, if they're using it for a specific task,
[03:29:45.280 --> 03:29:46.240]   they're gonna tailor it to that.
[03:29:46.240 --> 03:29:49.160]   And it doesn't matter that it's saying it's ChatGPT.
[03:29:49.160 --> 03:29:50.880]   - Oh, I guess one of the ways to do that
[03:29:50.880 --> 03:29:52.680]   is like a system prompt or something like that.
[03:29:52.680 --> 03:29:55.120]   Like if you're serving it to say that you're-
[03:29:55.120 --> 03:29:56.080]   - That's what we do.
[03:29:56.080 --> 03:29:57.280]   Like if we host the demo, you say,
[03:29:57.280 --> 03:30:00.520]   "You are Tulu 3, a language model trained
[03:30:00.520 --> 03:30:02.120]   by the Allen Institute for AI.
[03:30:02.120 --> 03:30:04.960]   We also are benefited from OpenAI data
[03:30:04.960 --> 03:30:06.440]   'cause it's a great research tool."
[03:30:06.440 --> 03:30:08.760]   - I mean, do you think there's any truth
[03:30:08.760 --> 03:30:13.240]   and value to the claim, OpenAI's claim,
[03:30:13.240 --> 03:30:15.000]   that there's evidence that China's DeepSea
[03:30:15.000 --> 03:30:16.200]   used this model to train?
[03:30:16.200 --> 03:30:19.320]   - I think everyone has benefited regardless
[03:30:19.320 --> 03:30:21.360]   because the data's on the internet.
[03:30:21.360 --> 03:30:24.120]   And therefore, it's in your pre-training now, right?
[03:30:24.120 --> 03:30:25.960]   There are like subreddits where people share
[03:30:25.960 --> 03:30:27.400]   the best ChatGPT outputs.
[03:30:27.400 --> 03:30:29.400]   And those are in your-
[03:30:29.400 --> 03:30:32.000]   - I think that they're trying to ship the narrative.
[03:30:32.000 --> 03:30:33.520]   Like they're trying to protect themselves.
[03:30:33.520 --> 03:30:35.960]   And we saw this years ago when ByteDance
[03:30:35.960 --> 03:30:38.000]   was actually banned from some OpenAI APIs
[03:30:38.000 --> 03:30:39.480]   for training on outputs.
[03:30:39.480 --> 03:30:42.360]   There's other AI startups that most people,
[03:30:42.360 --> 03:30:43.720]   if you're in the like AI culture,
[03:30:43.720 --> 03:30:46.080]   were like, they just told us they trained
[03:30:46.080 --> 03:30:48.440]   on OpenAI outputs and they never got banned.
[03:30:48.440 --> 03:30:51.000]   Like that's how they bootstrapped their early models.
[03:30:51.000 --> 03:30:53.360]   So it's much easier to get off the ground using this
[03:30:53.360 --> 03:30:56.560]   than to set up human pipelines and build a strong model.
[03:30:56.560 --> 03:30:57.920]   So there's long history here.
[03:30:57.920 --> 03:30:59.320]   And a lot of the communications
[03:30:59.320 --> 03:31:00.200]   seem like narrative control.
[03:31:00.200 --> 03:31:02.640]   - Actually, like over the last couple of days,
[03:31:02.640 --> 03:31:05.400]   we've seen a lot of people distill DeepSeq's model
[03:31:05.400 --> 03:31:08.000]   into LLAMA models because the DeepSeq models
[03:31:08.000 --> 03:31:09.720]   are kind of complicated to run inference on
[03:31:09.720 --> 03:31:11.440]   because they're a mixture of experts
[03:31:11.440 --> 03:31:14.320]   and they're 600 plus billion parameters and all this.
[03:31:14.320 --> 03:31:16.720]   And people distilled them into the LLAMA models
[03:31:16.720 --> 03:31:18.400]   because the LLAMA models are so easy to serve
[03:31:18.400 --> 03:31:19.800]   and everyone's built the pipelines
[03:31:19.800 --> 03:31:22.120]   and tooling for inference with the LLAMA models, right?
[03:31:22.120 --> 03:31:24.200]   Because it's the open standard.
[03:31:24.200 --> 03:31:26.560]   So, you know, we've seen a sort of roundabout, right?
[03:31:26.560 --> 03:31:27.760]   Like, is it bad?
[03:31:27.760 --> 03:31:28.720]   Is it illegal?
[03:31:28.720 --> 03:31:29.600]   Maybe it's illegal, whatever.
[03:31:29.600 --> 03:31:31.880]   I don't know about that, but like, it could break contracts.
[03:31:31.880 --> 03:31:34.040]   I don't think it's illegal, like in any legal,
[03:31:34.040 --> 03:31:35.520]   like no one's going to jail for this.
[03:31:35.520 --> 03:31:38.440]   - I think like fundamentally, I think it's ethical
[03:31:38.440 --> 03:31:42.660]   or I hope it's ethical because like the moment it becomes,
[03:31:42.660 --> 03:31:44.800]   we ban that kind of thing,
[03:31:44.800 --> 03:31:47.560]   it's gonna make everybody much worse off.
[03:31:47.560 --> 03:31:51.120]   And I also actually, this is difficult,
[03:31:51.120 --> 03:31:55.080]   but I think you should be allowed to train on the internet.
[03:31:55.080 --> 03:31:56.680]   I know a lot of authors and creators
[03:31:56.680 --> 03:31:58.120]   are very sensitive about it.
[03:31:58.120 --> 03:31:59.720]   That's a difficult question.
[03:31:59.720 --> 03:32:03.240]   But the moment you're not allowed to train on the internet.
[03:32:03.240 --> 03:32:04.080]   - I agree.
[03:32:04.080 --> 03:32:06.200]   I have a schizo take on how you can solve this
[03:32:06.200 --> 03:32:07.160]   because it already works.
[03:32:07.160 --> 03:32:08.280]   - I have a reasonable take on it.
[03:32:08.280 --> 03:32:09.120]   - Right, right.
[03:32:09.120 --> 03:32:09.960]   - All right, all right.
[03:32:09.960 --> 03:32:13.220]   - So, you know, Japan has a law,
[03:32:13.220 --> 03:32:15.600]   which you're allowed to train on any training data
[03:32:15.600 --> 03:32:18.160]   and copyrights don't apply if you want to train a model.
[03:32:18.160 --> 03:32:23.160]   A, B, Japan has nine gigawatts of curtailed nuclear power.
[03:32:23.160 --> 03:32:26.320]   C, Japan is allowed under the AI diffusion rule
[03:32:26.320 --> 03:32:28.760]   to import as many GPUs as they'd like.
[03:32:28.760 --> 03:32:31.000]   So all we have to do, we have a market here to make,
[03:32:31.000 --> 03:32:34.440]   we build massive data centers, we rent them to the labs,
[03:32:34.440 --> 03:32:37.120]   and then we train models in a legally permissible way
[03:32:37.120 --> 03:32:39.000]   and there's no ifs, ands, or buts.
[03:32:39.000 --> 03:32:42.860]   And now the models have no like potential copyright lawsuit
[03:32:42.860 --> 03:32:44.340]   from New York Times or anything like that.
[03:32:44.340 --> 03:32:46.140]   No, no, it's just like completely legal.
[03:32:46.140 --> 03:32:47.760]   No, so. - Genius.
[03:32:47.760 --> 03:32:49.780]   - The early copyright lawsuits have fallen
[03:32:49.780 --> 03:32:51.880]   in the favor of AI training.
[03:32:51.880 --> 03:32:55.200]   I would say that the long tail of use
[03:32:55.200 --> 03:32:57.040]   is gonna go in the side of AI,
[03:32:57.040 --> 03:32:59.820]   which is if you do, if you scrape trillions of data,
[03:32:59.820 --> 03:33:02.200]   you're not looking at, trillions of tokens of data,
[03:33:02.200 --> 03:33:04.320]   you're not looking and saying,
[03:33:04.320 --> 03:33:06.720]   this one New York Times article is so important to me.
[03:33:06.720 --> 03:33:09.200]   But if you're doing a audio generation for music
[03:33:09.200 --> 03:33:10.760]   or image generation and you say,
[03:33:10.760 --> 03:33:12.800]   make it in the style of X person,
[03:33:12.800 --> 03:33:15.220]   that's a reasonable case where you could figure out
[03:33:15.220 --> 03:33:18.100]   what is their profit margin on inference.
[03:33:18.100 --> 03:33:19.900]   I don't know if it's gonna be the 50/50
[03:33:19.900 --> 03:33:22.240]   of YouTube creator program or something,
[03:33:22.240 --> 03:33:24.760]   but I would opt into that program as a writer.
[03:33:24.760 --> 03:33:28.120]   Like, please, like that, it's just,
[03:33:28.120 --> 03:33:29.860]   it's gonna be a rough journey,
[03:33:29.860 --> 03:33:32.640]   but there will be some solutions like that that make sense,
[03:33:32.640 --> 03:33:35.800]   but there's a long tail where it's just on the internet.
[03:33:35.800 --> 03:33:37.480]   - I think one of the other aspects
[03:33:37.480 --> 03:33:40.520]   of that Financial Times article implied,
[03:33:40.520 --> 03:33:42.960]   and so that leads to a more general question.
[03:33:42.960 --> 03:33:47.960]   Do you think there's, how difficult is spying, espionage,
[03:33:47.960 --> 03:33:51.440]   and stealing of actual secret code
[03:33:51.440 --> 03:33:53.800]   and data from inside companies?
[03:33:53.800 --> 03:33:55.360]   How much of that is being attempted?
[03:33:55.360 --> 03:33:57.680]   - Code and data is hard, but ideas is easy.
[03:33:57.680 --> 03:34:01.880]   Silicon Valley operates on the way that top employees
[03:34:01.880 --> 03:34:04.960]   get bought out by other companies for a pay raise.
[03:34:04.960 --> 03:34:07.240]   And a large reason why these companies do this
[03:34:07.240 --> 03:34:09.100]   is to bring ideas with them.
[03:34:09.100 --> 03:34:11.520]   And there are, there's no, I mean, in California,
[03:34:11.520 --> 03:34:13.120]   there's rules that like certain,
[03:34:13.120 --> 03:34:15.960]   like non-competes or whatever are illegal in California.
[03:34:15.960 --> 03:34:17.800]   And whether or not there's NDAs and things,
[03:34:17.800 --> 03:34:20.040]   that is how a lot of it happens.
[03:34:20.040 --> 03:34:23.240]   Recently, there was somebody from Gemini
[03:34:23.240 --> 03:34:25.560]   who helped make this 1 million contacts length,
[03:34:25.560 --> 03:34:28.040]   and everyone is saying the next llama who,
[03:34:28.040 --> 03:34:29.200]   I mean, he went to the meta team,
[03:34:29.200 --> 03:34:31.800]   is gonna have 1 million contacts length.
[03:34:31.800 --> 03:34:34.220]   And that's kind of how the world works.
[03:34:34.220 --> 03:34:36.680]   - You know, as far as like industrial espionage and things,
[03:34:36.680 --> 03:34:40.400]   that has been greatly successful in the past, right?
[03:34:40.400 --> 03:34:43.060]   You know, the Americans did it to the Brits,
[03:34:43.060 --> 03:34:44.840]   the Chinese have done it to the Americans, right?
[03:34:44.840 --> 03:34:45.840]   And you know, so on and so forth.
[03:34:45.840 --> 03:34:47.480]   It's just, it is a fact of life.
[03:34:47.480 --> 03:34:52.280]   And so like to argue industrial espionage can be stopped
[03:34:52.280 --> 03:34:54.760]   is probably unlikely, you can make it difficult.
[03:34:54.760 --> 03:34:57.000]   But even then, like there's all these stories about like,
[03:34:57.000 --> 03:34:59.600]   hey, F-35 and F-22 have already been like,
[03:34:59.600 --> 03:35:00.640]   sort of like given to China
[03:35:00.640 --> 03:35:02.720]   in terms of design plans and stuff.
[03:35:02.720 --> 03:35:04.640]   Code and stuff, like between, you know,
[03:35:04.640 --> 03:35:06.760]   I say companies, not nation states
[03:35:06.760 --> 03:35:08.840]   is probably very difficult.
[03:35:08.840 --> 03:35:11.120]   But ideas are discussed a lot, right?
[03:35:11.120 --> 03:35:13.840]   Whether it be a house party in San Francisco,
[03:35:13.840 --> 03:35:15.880]   or a company changing employees,
[03:35:15.880 --> 03:35:17.840]   or, you know, or the, you know,
[03:35:17.840 --> 03:35:19.920]   the always the like mythical honeypot
[03:35:19.920 --> 03:35:21.160]   that always gets talked about, right?
[03:35:21.160 --> 03:35:23.080]   Like someone gets honeypotted, right?
[03:35:23.080 --> 03:35:25.440]   Because everyone working on AI is a single dude
[03:35:25.440 --> 03:35:26.800]   who's in their 20s and 30s.
[03:35:26.800 --> 03:35:29.480]   Not everyone, but like a insane amount of,
[03:35:29.480 --> 03:35:31.280]   insane percentages.
[03:35:31.280 --> 03:35:33.120]   So there's always like all these like, you know,
[03:35:33.120 --> 03:35:33.960]   and obviously--
[03:35:33.960 --> 03:35:36.440]   - So honeypotted is like a spy,
[03:35:36.440 --> 03:35:38.480]   a female spy approaches you and like--
[03:35:38.480 --> 03:35:41.040]   - Yeah, yeah, or male, right?
[03:35:41.040 --> 03:35:42.640]   You know, it's San Francisco, right?
[03:35:42.640 --> 03:35:46.280]   But as a single dude, I will say in his late 20s, right?
[03:35:46.280 --> 03:35:48.200]   Is like, we are very easily corrupted, right?
[03:35:48.200 --> 03:35:51.120]   Like, you know, like not corrupted myself,
[03:35:51.120 --> 03:35:52.680]   but you know, like, we are, we are, right?
[03:35:52.680 --> 03:35:53.800]   - Everybody else, not me.
[03:35:53.800 --> 03:35:54.640]   - Yeah, exactly.
[03:35:54.640 --> 03:35:56.480]   - I'm too oblivious that I am not single.
[03:35:56.480 --> 03:35:59.560]   So I'm safe from one espionage access.
[03:35:59.560 --> 03:36:02.120]   - Yeah, you have to make sure
[03:36:02.120 --> 03:36:04.880]   to close all security vulnerabilities.
[03:36:04.880 --> 03:36:07.640]   So you, Dylan, collect a lot of information
[03:36:07.640 --> 03:36:11.040]   about each of the mega clusters
[03:36:11.040 --> 03:36:13.160]   for each of the major AI companies.
[03:36:13.160 --> 03:36:16.360]   Can you talk about the build outs
[03:36:16.360 --> 03:36:18.120]   for each one that stand out?
[03:36:18.120 --> 03:36:20.960]   - Yeah, so I think the thing that's like really important
[03:36:20.960 --> 03:36:22.480]   about these mega cluster build outs
[03:36:22.480 --> 03:36:25.840]   is they're completely unprecedented in scale, right?
[03:36:25.840 --> 03:36:29.160]   US, you know, sort of like data center power consumption
[03:36:29.160 --> 03:36:32.520]   has been slowly on the rise and it's gone up to two, 3%,
[03:36:32.520 --> 03:36:34.760]   even through the cloud computing revolution, right?
[03:36:34.760 --> 03:36:37.320]   Data center consumption as a percentage of total US.
[03:36:37.320 --> 03:36:39.080]   And that's been over decades, right?
[03:36:39.080 --> 03:36:40.400]   Of data centers, et cetera.
[03:36:40.400 --> 03:36:41.760]   It's been climbing, climbing slowly.
[03:36:41.760 --> 03:36:44.960]   But now, two to 3%, now, by the end of this decade,
[03:36:44.960 --> 03:36:47.120]   it's like, even under like, you know,
[03:36:47.120 --> 03:36:48.920]   when I say like 10%, a lot of people
[03:36:48.920 --> 03:36:52.000]   that are traditionally, by like 2028, 2030,
[03:36:52.000 --> 03:36:55.440]   people traditionally non, traditional data center people,
[03:36:55.440 --> 03:36:56.600]   like that's nuts.
[03:36:56.600 --> 03:36:58.840]   But then like people who are in like AI,
[03:36:58.840 --> 03:36:59.960]   who have like really looked at this
[03:36:59.960 --> 03:37:01.600]   at like the Anthropics and open AIs,
[03:37:01.600 --> 03:37:02.440]   they're like, that's not enough.
[03:37:02.440 --> 03:37:05.600]   And I'm like, okay, but like, you know,
[03:37:05.600 --> 03:37:09.040]   this is both through globally distributed
[03:37:09.040 --> 03:37:10.240]   or distributed throughout the US
[03:37:10.240 --> 03:37:12.240]   as well as like centralized clusters, right?
[03:37:12.240 --> 03:37:14.720]   The distributed throughout the US is exciting
[03:37:14.720 --> 03:37:16.000]   and it's the bulk of it, right?
[03:37:16.000 --> 03:37:19.680]   Like, hey, you know, open AI or, you know,
[03:37:19.680 --> 03:37:22.880]   say Meta's adding a gigawatt, right?
[03:37:22.880 --> 03:37:24.840]   But most of it is distributed through the US
[03:37:24.840 --> 03:37:26.640]   for inference and all these other things, right?
[03:37:26.640 --> 03:37:29.640]   - So maybe we should lay out what a cluster is.
[03:37:29.640 --> 03:37:33.400]   So, you know, does this include AWS?
[03:37:33.400 --> 03:37:35.760]   Maybe it's good to talk about the different kinds
[03:37:35.760 --> 03:37:37.720]   of clusters and what you mean by mega clusters
[03:37:37.720 --> 03:37:40.520]   and what's a GPU and what's a computer and what is--
[03:37:40.520 --> 03:37:41.520]   - Yeah, yeah, yeah.
[03:37:41.520 --> 03:37:43.360]   - Not that far back, but yeah.
[03:37:43.360 --> 03:37:45.240]   So like, what do we mean by the clusters?
[03:37:45.240 --> 03:37:47.320]   - Oh man, I thought I was about to do the Apple ad, right?
[03:37:47.320 --> 03:37:48.160]   What's a computer?
[03:37:48.160 --> 03:37:49.600]   (laughing)
[03:37:49.600 --> 03:37:53.640]   So traditionally data centers and data center tasks
[03:37:53.640 --> 03:37:56.080]   have been a distributed systems problem
[03:37:56.080 --> 03:38:00.280]   that is capable of being spread very far and widely, right?
[03:38:00.280 --> 03:38:02.680]   I.e. I send a request to Google,
[03:38:02.680 --> 03:38:05.640]   it's routed to a data center somewhat close to me.
[03:38:05.640 --> 03:38:07.960]   It does whatever search ranking recommendation,
[03:38:07.960 --> 03:38:09.320]   sends a result back, right?
[03:38:09.320 --> 03:38:13.160]   The nature of the task is changing rapidly
[03:38:13.160 --> 03:38:15.200]   in that the task, there's two tasks
[03:38:15.200 --> 03:38:16.600]   that people are really focused on now, right?
[03:38:16.600 --> 03:38:17.640]   It's not database access.
[03:38:17.640 --> 03:38:20.200]   It's not serve me the right page, serve me the right ad.
[03:38:20.200 --> 03:38:22.840]   It's now A, inference.
[03:38:22.840 --> 03:38:24.520]   And inference is dramatically different
[03:38:24.520 --> 03:38:25.760]   from traditional distributed systems,
[03:38:25.760 --> 03:38:27.800]   but it looks a lot more similar.
[03:38:27.800 --> 03:38:29.680]   And then there's training, right?
[03:38:29.680 --> 03:38:31.640]   The inference side is still like,
[03:38:31.640 --> 03:38:33.880]   hey, I'm gonna put thousands of GPUs
[03:38:33.880 --> 03:38:37.200]   in blocks all around these data centers.
[03:38:37.200 --> 03:38:38.840]   I'm gonna run models on them.
[03:38:38.840 --> 03:38:41.000]   User submits a request, it gets kicked off,
[03:38:41.000 --> 03:38:43.360]   or, hey, my service, they submit a request
[03:38:43.360 --> 03:38:44.280]   to my service, right?
[03:38:44.280 --> 03:38:45.120]   They're on Word and they're like,
[03:38:45.120 --> 03:38:46.080]   oh yeah, help me co-pilot.
[03:38:46.080 --> 03:38:47.040]   And it starts, kicks it off.
[03:38:47.040 --> 03:38:48.920]   I'm on my Windows, co-pilot, whatever.
[03:38:48.920 --> 03:38:50.080]   Apple intelligence, whatever it is,
[03:38:50.080 --> 03:38:52.280]   it gets kicked off to a data center, right?
[03:38:52.280 --> 03:38:54.480]   And that data center does some work and sends it back.
[03:38:54.480 --> 03:38:55.400]   That's inference.
[03:38:55.400 --> 03:38:58.000]   That is going to be the bulk of compute.
[03:38:58.000 --> 03:39:00.560]   But then, and that's like,
[03:39:00.560 --> 03:39:02.320]   there's thousands of data centers that we're tracking
[03:39:02.320 --> 03:39:04.440]   with like satellites and like all these other things.
[03:39:04.440 --> 03:39:06.800]   And those are the bulk of what's being built,
[03:39:06.800 --> 03:39:09.080]   but the scale of, and so that's like
[03:39:09.080 --> 03:39:10.120]   what's really reshaping
[03:39:10.120 --> 03:39:11.720]   and that's what's getting millions of GPUs.
[03:39:11.720 --> 03:39:14.400]   But the scale of the largest cluster
[03:39:14.400 --> 03:39:16.000]   is also really important, right?
[03:39:16.000 --> 03:39:18.720]   When we look back at history, right?
[03:39:18.720 --> 03:39:22.680]   Like, you know, or through the age of AI, right?
[03:39:22.680 --> 03:39:26.120]   Like it was a really big deal when they did AlexNet
[03:39:26.120 --> 03:39:28.240]   on, I think, two GPUs or four GPUs?
[03:39:28.240 --> 03:39:29.080]   I don't remember.
[03:39:29.080 --> 03:39:30.160]   It was a really big deal.
[03:39:30.160 --> 03:39:31.720]   - It's a big deal 'cause you used GPUs.
[03:39:31.720 --> 03:39:33.360]   - It's a big deal to use GPUs.
[03:39:33.360 --> 03:39:34.640]   And they used multiple, right?
[03:39:34.640 --> 03:39:38.400]   But then over time, its scale has just been compounding.
[03:39:38.400 --> 03:39:41.800]   And so when you skip forward to GPT-3, then GPT-4,
[03:39:41.800 --> 03:39:46.480]   GPT-4, 20,000 A100 GPUs, unprecedented run, right?
[03:39:46.480 --> 03:39:48.240]   In terms of the size and the cost, right?
[03:39:48.240 --> 03:39:50.240]   A couple hundred million dollars on a YOLO, right?
[03:39:50.240 --> 03:39:51.680]   A YOLO run for GPT-4.
[03:39:51.680 --> 03:39:54.400]   And it yielded, you know, this magical improvement
[03:39:54.400 --> 03:39:57.320]   that was like perfectly in line with what was experimented
[03:39:57.320 --> 03:39:58.720]   and just like a log scale, right?
[03:39:58.720 --> 03:40:01.120]   - Oh yeah, they have that plot from the paper.
[03:40:01.120 --> 03:40:02.400]   The technical report.
[03:40:02.400 --> 03:40:03.920]   - The scaling laws were perfect, right?
[03:40:03.920 --> 03:40:05.800]   But that's not a crazy number, right?
[03:40:05.800 --> 03:40:10.800]   20,000 A100s, roughly each GPU is consuming 400 watts.
[03:40:10.800 --> 03:40:12.360]   And then when you add in the whole server, right?
[03:40:12.360 --> 03:40:16.320]   Everything, it's like 15 to 20 megawatts of power, right?
[03:40:16.320 --> 03:40:19.480]   You know, maybe you could look up
[03:40:19.480 --> 03:40:21.560]   what the power of consumption of a human person is
[03:40:21.560 --> 03:40:22.920]   because the numbers are gonna get silly.
[03:40:22.920 --> 03:40:24.880]   But like that 15 to 20 megawatts
[03:40:24.880 --> 03:40:26.200]   was standard data center size.
[03:40:26.200 --> 03:40:28.200]   It was just unprecedented that was all GPUs
[03:40:28.200 --> 03:40:29.040]   running one task.
[03:40:29.040 --> 03:40:30.880]   - 20 watts is a toaster.
[03:40:30.880 --> 03:40:34.320]   - A toaster is like also a similar power consumption
[03:40:34.320 --> 03:40:35.520]   to an A100, right?
[03:40:35.520 --> 03:40:37.440]   H100 comes around, they increase the power
[03:40:37.440 --> 03:40:40.240]   from like 400 to 700 watts, and that's just per GPU.
[03:40:40.240 --> 03:40:42.000]   And then there's all the associated stuff around it.
[03:40:42.000 --> 03:40:43.360]   So once you count all that,
[03:40:43.360 --> 03:40:45.840]   it's roughly like 1200 to 1400 watts
[03:40:45.840 --> 03:40:48.640]   for everything, networking, CPUs, memory, blah, blah, blah.
[03:40:48.640 --> 03:40:51.440]   - So we should also say, so what's required?
[03:40:51.440 --> 03:40:55.240]   You said power, so a lot of power is required.
[03:40:55.240 --> 03:40:58.760]   A lot of heat is generated, cooling is required.
[03:40:58.760 --> 03:41:01.960]   And because there's a lot of GPUs that have to be,
[03:41:01.960 --> 03:41:04.800]   or CPUs or whatever, they have to be connected.
[03:41:04.800 --> 03:41:06.400]   So there's a lot of networking.
[03:41:06.400 --> 03:41:10.040]   - Yeah, so I think, yeah, sorry for skipping past that.
[03:41:10.040 --> 03:41:12.040]   And then the data center itself is like complicated, right?
[03:41:12.040 --> 03:41:14.560]   But these are still standard sized data centers
[03:41:14.560 --> 03:41:16.120]   for GPT-4 scale, right?
[03:41:16.120 --> 03:41:19.320]   Now we step forward to sort of what is the scale
[03:41:19.320 --> 03:41:22.280]   of clusters that people built last year, right?
[03:41:22.280 --> 03:41:24.280]   And it ranges widely, right?
[03:41:24.280 --> 03:41:27.080]   It ranges from like, hey, these are standard data centers,
[03:41:27.080 --> 03:41:28.560]   and we're just using multiple of them
[03:41:28.560 --> 03:41:30.840]   and connecting them together really with a ton of fiber
[03:41:30.840 --> 03:41:32.560]   between them, a lot of networking, et cetera.
[03:41:32.560 --> 03:41:35.040]   That's what OpenAI and Microsoft did in Arizona, right?
[03:41:35.040 --> 03:41:37.400]   And so they have 100,000 GPUs, right?
[03:41:37.400 --> 03:41:38.400]   Meta, similar thing.
[03:41:38.400 --> 03:41:40.560]   They took their standard existing data center design,
[03:41:40.560 --> 03:41:41.960]   and it looks like an H,
[03:41:41.960 --> 03:41:44.440]   and they connected multiple of them together.
[03:41:44.440 --> 03:41:49.400]   And they first did 16,000 GPUs, 24,000 GPUs total.
[03:41:49.400 --> 03:41:51.680]   Only 16,000 of them were running on the training run
[03:41:51.680 --> 03:41:53.200]   because GPUs are very unreliable,
[03:41:53.200 --> 03:41:55.360]   so they need to have spares to swap in and out,
[03:41:55.360 --> 03:41:57.320]   all the way to like now 100,000 GPUs
[03:41:57.320 --> 03:41:59.600]   that they're training on Llama4 on currently, right?
[03:41:59.600 --> 03:42:01.800]   Like 128,000 or so, right?
[03:42:01.800 --> 03:42:04.200]   This is, you know, think about 100,000 GPUs
[03:42:04.200 --> 03:42:08.280]   with roughly 1,400 watts a piece,
[03:42:08.280 --> 03:42:11.480]   that's 140 megawatts, 150 megawatts, right?
[03:42:11.480 --> 03:42:12.800]   For 128,000, right?
[03:42:12.800 --> 03:42:13.920]   So you're talking about,
[03:42:13.920 --> 03:42:15.680]   you've jumped from 15 to 20 megawatts
[03:42:15.680 --> 03:42:18.400]   to 10X, you know, almost 10X that number,
[03:42:18.400 --> 03:42:22.560]   9X that number to 150 megawatts in two years, right?
[03:42:22.560 --> 03:42:24.560]   From 2022 to 2024, right?
[03:42:24.560 --> 03:42:27.840]   And some people like Elon, he admittedly, right,
[03:42:27.840 --> 03:42:29.960]   and he says himself got into the game a little bit late
[03:42:29.960 --> 03:42:31.840]   for pre-training large language models, right?
[03:42:31.840 --> 03:42:33.480]   XAI was started later, right?
[03:42:33.480 --> 03:42:37.040]   But then he bent heaven and hell to get his data center up
[03:42:37.040 --> 03:42:38.560]   and get the largest cluster in the world, right?
[03:42:38.560 --> 03:42:40.720]   Which is 200,000 GPUs.
[03:42:40.720 --> 03:42:44.040]   And he did that, he bought a factory in Memphis.
[03:42:44.040 --> 03:42:46.200]   He's upgrading the substation,
[03:42:46.200 --> 03:42:47.040]   but at the same time,
[03:42:47.040 --> 03:42:48.600]   he's got a bunch of mobile power generation,
[03:42:48.600 --> 03:42:50.640]   a bunch of single cycle combine.
[03:42:50.640 --> 03:42:52.120]   He tapped the natural gas line
[03:42:52.120 --> 03:42:53.720]   that's right next to the factory,
[03:42:53.720 --> 03:42:55.760]   and he's just pulling a ton of gas, burning gas.
[03:42:55.760 --> 03:42:57.400]   He's generating all this power.
[03:42:57.400 --> 03:42:59.840]   He's in a factory, in an old appliance factory
[03:42:59.840 --> 03:43:01.640]   that's shut down and moved to China long ago, right?
[03:43:01.640 --> 03:43:05.480]   Like, you know, and he's got 200,000 GPUs in it.
[03:43:05.480 --> 03:43:07.000]   And now what's the next scale, right?
[03:43:07.000 --> 03:43:08.480]   Like all the hyperscalers have done this.
[03:43:08.480 --> 03:43:11.520]   Now the next scale is something that's even bigger, right?
[03:43:11.520 --> 03:43:13.880]   And so, you know, Elon, just to stick on the topic,
[03:43:13.880 --> 03:43:16.240]   he's building his own natural gas plant,
[03:43:16.240 --> 03:43:18.520]   like a proper one right next door.
[03:43:18.520 --> 03:43:21.840]   He's deploying tons of Tesla mega pack batteries
[03:43:21.840 --> 03:43:23.440]   to make the power more smooth
[03:43:23.440 --> 03:43:24.440]   and all sorts of other things.
[03:43:24.440 --> 03:43:27.640]   He's got like industrial chillers to cool the water down
[03:43:27.640 --> 03:43:29.800]   because he's water cooling the chips.
[03:43:29.800 --> 03:43:31.280]   So all these crazy things
[03:43:31.280 --> 03:43:33.960]   to get the clusters bigger and bigger.
[03:43:33.960 --> 03:43:34.800]   But when you look at like,
[03:43:34.800 --> 03:43:37.360]   say what OpenAI did with Stargate,
[03:43:37.360 --> 03:43:42.080]   that's that in Arizona, in Abilene, Texas, right?
[03:43:42.080 --> 03:43:43.520]   What they've announced at least, right?
[03:43:43.520 --> 03:43:44.360]   It's not built, right?
[03:43:44.360 --> 03:43:45.440]   Elon says they don't have the money.
[03:43:45.440 --> 03:43:48.200]   You know, there's some debates about this.
[03:43:48.200 --> 03:43:50.600]   But at full scale, at least the first section
[03:43:50.600 --> 03:43:52.080]   is like definitely money's accounted for,
[03:43:52.080 --> 03:43:53.240]   but there's multiple sections.
[03:43:53.240 --> 03:43:54.960]   But full scale, that data center
[03:43:54.960 --> 03:43:56.920]   is gonna be 2.2 gigawatts, right?
[03:43:56.920 --> 03:43:59.240]   2200 megawatts of power in,
[03:43:59.240 --> 03:44:01.040]   and roughly like 1.8 gigawatts
[03:44:01.040 --> 03:44:04.360]   or 1800 megawatts, yeah.
[03:44:04.360 --> 03:44:07.720]   1800 megawatts of power delivered to chips, right?
[03:44:07.720 --> 03:44:09.280]   Now this is an absurd scale.
[03:44:09.280 --> 03:44:12.200]   2.2 gigawatts is like more than most cities, right?
[03:44:12.200 --> 03:44:13.720]   You know, to be clear.
[03:44:13.720 --> 03:44:16.160]   And delivered to a single cluster
[03:44:16.160 --> 03:44:18.720]   that's connected to do training, right?
[03:44:18.720 --> 03:44:20.680]   To train these models, to do both the pre-training,
[03:44:20.680 --> 03:44:22.560]   the post-training, all of this stuff, right?
[03:44:22.560 --> 03:44:23.720]   - This is insane.
[03:44:23.720 --> 03:44:24.560]   - It is.
[03:44:24.560 --> 03:44:25.400]   - What is a nuclear power plant again?
[03:44:25.400 --> 03:44:26.720]   - Everyone is doing this, right?
[03:44:26.720 --> 03:44:27.960]   Everyone is doing this, right?
[03:44:27.960 --> 03:44:30.280]   Meta in Louisiana, right?
[03:44:30.280 --> 03:44:33.600]   They're building two natural gas plants, massive ones.
[03:44:33.600 --> 03:44:36.720]   And then they're building this massive data center.
[03:44:36.720 --> 03:44:39.440]   Amazon has like plans for this scale.
[03:44:39.440 --> 03:44:42.040]   Google has plans for this scale.
[03:44:42.040 --> 03:44:44.000]   XAI has plans for this scale, right?
[03:44:44.000 --> 03:44:46.560]   Like all of these, the guys that are racing,
[03:44:46.560 --> 03:44:48.520]   the companies that are racing are racing hard.
[03:44:48.520 --> 03:44:52.000]   And they're doing multi-gigawatt data centers, right?
[03:44:52.000 --> 03:44:55.400]   To build this out because they think that, yeah.
[03:44:55.400 --> 03:44:58.480]   If I now have, you know, obviously pre-training scaling
[03:44:58.480 --> 03:45:00.000]   is gonna continue, but to some extent,
[03:45:00.000 --> 03:45:01.480]   but then also all this post-training stuff
[03:45:01.480 --> 03:45:03.520]   where you have an RL sandbox for computer use
[03:45:03.520 --> 03:45:04.360]   or whatever, right?
[03:45:04.360 --> 03:45:05.560]   Like, you know, this is where they're gonna,
[03:45:05.560 --> 03:45:07.360]   and all these variable domains
[03:45:07.360 --> 03:45:08.640]   where they just keep learning and learning
[03:45:08.640 --> 03:45:10.080]   and learning self-play, whatever.
[03:45:10.080 --> 03:45:12.600]   Whatever it is, makes the AI so much more capable
[03:45:12.600 --> 03:45:14.600]   because the line does go up, right?
[03:45:14.600 --> 03:45:17.000]   As you throw more compute, you get more performance.
[03:45:17.000 --> 03:45:19.320]   The shirt is about scaling laws.
[03:45:19.320 --> 03:45:21.240]   You know, to some extent it is diminishing returns, right?
[03:45:21.240 --> 03:45:22.440]   You 10X the compute.
[03:45:22.440 --> 03:45:24.040]   You don't get 10X better model, right?
[03:45:24.040 --> 03:45:25.920]   You get a diminishing returns, but also
[03:45:25.920 --> 03:45:27.120]   you get efficiency improvements.
[03:45:27.120 --> 03:45:28.800]   So you bend the curve, right?
[03:45:28.800 --> 03:45:30.960]   And these scale of data centers are doing,
[03:45:30.960 --> 03:45:32.600]   you know, wreaking, you know,
[03:45:32.600 --> 03:45:34.560]   a lot of like havoc on the network, right?
[03:45:34.560 --> 03:45:37.040]   And, you know, Nathan was mentioning there's,
[03:45:37.040 --> 03:45:40.440]   Amazon has tried to buy this nuclear power plant, Talon.
[03:45:40.440 --> 03:45:43.160]   And if you look at Talon stock, it's just like skyrocketing.
[03:45:43.160 --> 03:45:45.120]   And, you know, like they're building
[03:45:45.120 --> 03:45:46.960]   a massive multi-gigawatt data center there.
[03:45:46.960 --> 03:45:48.000]   And, you know, you just go down the list.
[03:45:48.000 --> 03:45:49.920]   There's so many ramifications.
[03:45:49.920 --> 03:45:52.480]   Interesting thing is like certain regions of the US
[03:45:52.480 --> 03:45:54.960]   transmitting power cost more
[03:45:54.960 --> 03:45:56.880]   than actually generating it, right?
[03:45:56.880 --> 03:45:59.040]   Because the grid is so slow to build
[03:45:59.040 --> 03:46:01.160]   and the demand for power and the ability to build power
[03:46:01.160 --> 03:46:03.440]   and like re-ramping on a natural gas plant
[03:46:03.440 --> 03:46:05.160]   or even a coal plant is like easy enough to do,
[03:46:05.160 --> 03:46:07.120]   but like transmitting the power is really hard.
[03:46:07.120 --> 03:46:10.040]   So in some parts of the US, like in Virginia,
[03:46:10.040 --> 03:46:11.520]   it costs more to transmit power
[03:46:11.520 --> 03:46:12.800]   than it costs to generate it.
[03:46:12.800 --> 03:46:14.280]   Which is like, you know, there's all sorts
[03:46:14.280 --> 03:46:16.640]   of like second order effects that are insane here.
[03:46:16.640 --> 03:46:18.920]   - Can the power grid support this kind of growth?
[03:46:18.920 --> 03:46:20.320]   - You know, Trump's executive orders,
[03:46:20.320 --> 03:46:21.800]   there was a Biden executive order
[03:46:21.800 --> 03:46:22.840]   before the end of the year,
[03:46:22.840 --> 03:46:24.560]   but then Trump had some more executive orders,
[03:46:24.560 --> 03:46:28.000]   which hopefully reduced the regulations
[03:46:28.000 --> 03:46:29.760]   to where, yes, things can be built.
[03:46:29.760 --> 03:46:32.480]   But yeah, this is a big, big challenge, right?
[03:46:32.480 --> 03:46:33.920]   Is building enough power fast enough?
[03:46:33.920 --> 03:46:36.160]   - Are you gonna basically have a nuclear power plant
[03:46:36.160 --> 03:46:38.520]   next to a data center for each one of these?
[03:46:38.520 --> 03:46:41.560]   - So the fun thing here is this is too slow
[03:46:41.560 --> 03:46:43.520]   to build the power plant.
[03:46:43.520 --> 03:46:46.360]   To build a power plant or to reconfigure
[03:46:46.360 --> 03:46:48.160]   an existing power plant is too slow.
[03:46:48.160 --> 03:46:50.960]   And so therefore you must use natural,
[03:46:50.960 --> 03:46:52.600]   data center power consumption is flat, right?
[03:46:52.600 --> 03:46:53.440]   You know, I mean, like it's--
[03:46:53.440 --> 03:46:55.400]   - Which is why nuclear is also good for it.
[03:46:55.400 --> 03:46:58.120]   Like long-term nuclear is a very natural fit,
[03:46:58.120 --> 03:47:02.880]   but you can't do solar or anything in the short term like that.
[03:47:02.880 --> 03:47:04.040]   - 'Cause data center power is like this, right?
[03:47:04.040 --> 03:47:05.600]   Like you're telling me, you know,
[03:47:05.600 --> 03:47:08.880]   I'm gonna buy tens of billions of dollars of GPUs
[03:47:08.880 --> 03:47:10.680]   and idle them 'cause the power's not being generated?
[03:47:10.680 --> 03:47:11.800]   Like power is cheap, right?
[03:47:11.800 --> 03:47:13.560]   Like if you look at the cost of a cluster,
[03:47:13.560 --> 03:47:16.400]   less than 20% of it is power, right?
[03:47:16.400 --> 03:47:17.800]   Most of it is the capital costs
[03:47:17.800 --> 03:47:19.880]   and depreciation of the GPUs, right?
[03:47:19.880 --> 03:47:21.360]   And so it's like, well, screw it.
[03:47:21.360 --> 03:47:22.200]   I'll just like, you know,
[03:47:22.200 --> 03:47:23.200]   I'll just build natural gas plants.
[03:47:23.200 --> 03:47:24.400]   This is what Meta is doing in Louisiana.
[03:47:24.400 --> 03:47:27.000]   This is what OpenAI is doing in Texas
[03:47:27.000 --> 03:47:28.400]   and like all these different places.
[03:47:28.400 --> 03:47:30.320]   They may not be doing it directly,
[03:47:30.320 --> 03:47:31.520]   but they are partnered with someone.
[03:47:31.520 --> 03:47:33.920]   And so there is a couple hopes, right?
[03:47:33.920 --> 03:47:36.040]   Like one is, you know, in Elon,
[03:47:36.040 --> 03:47:37.720]   what he's doing in Memphis is like, you know,
[03:47:37.720 --> 03:47:38.840]   to the extreme, they're not just using
[03:47:38.840 --> 03:47:41.960]   dual combine cycle gas, which is like super efficient.
[03:47:41.960 --> 03:47:43.520]   He's also just using single cycle
[03:47:43.520 --> 03:47:44.800]   and like mobile generators and stuff,
[03:47:44.800 --> 03:47:45.960]   which is less efficient.
[03:47:45.960 --> 03:47:49.080]   But he's, you know, there's also like the flip side,
[03:47:49.080 --> 03:47:51.560]   which is like solar power generation is like this
[03:47:51.560 --> 03:47:54.120]   and wind is another like, like this different correlate,
[03:47:54.120 --> 03:47:54.960]   you know, different.
[03:47:54.960 --> 03:47:56.440]   So if you stack both of those,
[03:47:56.440 --> 03:47:58.920]   plus you get a big chunk of batteries,
[03:47:58.920 --> 03:48:00.400]   plus you have a little bit of gas,
[03:48:00.400 --> 03:48:02.360]   it is possible to run it more green.
[03:48:02.360 --> 03:48:04.800]   It's just the timescales for that is slow, right?
[03:48:04.800 --> 03:48:07.320]   So people are trying, but you know,
[03:48:07.320 --> 03:48:09.680]   Meta basically said, whatever,
[03:48:09.680 --> 03:48:11.720]   don't care about my sustainability pledge
[03:48:11.720 --> 03:48:13.200]   or they'll buy like a power,
[03:48:13.200 --> 03:48:15.080]   it's called a PPA, power purchasing agreement,
[03:48:15.080 --> 03:48:16.600]   where there'll be a massive wind farm
[03:48:16.600 --> 03:48:18.240]   or solar farm, like wherever.
[03:48:18.240 --> 03:48:20.000]   And then they'll just pretend like those electrons
[03:48:20.000 --> 03:48:21.200]   are being consumed by the data center.
[03:48:21.200 --> 03:48:23.280]   But in reality, they're paying for the power here
[03:48:23.280 --> 03:48:26.080]   and selling it to the grid and they're buying power here.
[03:48:26.080 --> 03:48:28.160]   And then another thing is like Microsoft
[03:48:28.160 --> 03:48:30.320]   quit on some of their sustainability pledges, right?
[03:48:30.320 --> 03:48:32.560]   Elon, what he did with Memphis
[03:48:32.560 --> 03:48:34.120]   is objectively somewhat dirty,
[03:48:34.120 --> 03:48:36.360]   but he's also doing it in an area where there's like
[03:48:36.360 --> 03:48:38.480]   a bigger natural gas plant right next door
[03:48:38.480 --> 03:48:40.360]   and like a sewer next or not a sewer,
[03:48:40.360 --> 03:48:41.400]   but like a wastewater treatment
[03:48:41.400 --> 03:48:42.960]   and a garbage dump nearby, right?
[03:48:42.960 --> 03:48:45.680]   And he's obviously made the world a lot more clean
[03:48:45.680 --> 03:48:47.880]   than that one data center is gonna do, right?
[03:48:47.880 --> 03:48:50.120]   So I think like, it's fine to some extent
[03:48:50.120 --> 03:48:53.080]   and maybe AGI solves global warming and stuff, right?
[03:48:53.080 --> 03:48:56.400]   Whatever it is, you know, this is sort of the attitude
[03:48:56.400 --> 03:48:57.640]   that people at the labs have, right?
[03:48:57.640 --> 03:48:59.880]   Which is like, yeah, it's great, we'll just use gas, right?
[03:48:59.880 --> 03:49:01.360]   Because the race is that important
[03:49:01.360 --> 03:49:04.000]   and if we lose, you know, that's way worse, right?
[03:49:04.000 --> 03:49:06.880]   - I should say that I got a chance to visit
[03:49:06.880 --> 03:49:10.680]   the Memphis data center and it's kind of incredible.
[03:49:10.680 --> 03:49:13.040]   I mean, I visited with Elon,
[03:49:13.040 --> 03:49:17.440]   just the teams and the rate of innovation
[03:49:17.440 --> 03:49:18.600]   there is insane.
[03:49:18.600 --> 03:49:20.800]   'Cause my sense is that, you know,
[03:49:20.800 --> 03:49:23.160]   nobody's ever done anything of this scale
[03:49:23.160 --> 03:49:26.800]   and nobody has certainly ever done anything of this scale
[03:49:26.800 --> 03:49:29.120]   at the rate that XAI is doing.
[03:49:29.120 --> 03:49:31.560]   So they're like figuring out,
[03:49:31.560 --> 03:49:33.800]   I mean, and so I was sitting in on all these meetings
[03:49:33.800 --> 03:49:36.640]   where they're brainstorming, it's like, it's insane.
[03:49:36.640 --> 03:49:38.640]   It's exciting 'cause they're like,
[03:49:38.640 --> 03:49:40.640]   they're trying to figure out what the bottlenecks are,
[03:49:40.640 --> 03:49:43.040]   how to remove the bottlenecks, how to make sure that,
[03:49:43.040 --> 03:49:45.640]   you know, there's just so many really cool things
[03:49:45.640 --> 03:49:48.200]   about putting together a data center
[03:49:48.200 --> 03:49:51.040]   'cause, you know, everything has to work.
[03:49:51.040 --> 03:49:55.200]   It's the people that do like the sysadmin,
[03:49:55.200 --> 03:49:56.640]   you know, the machine learning, all that
[03:49:56.640 --> 03:49:58.080]   is the exciting thing, so on.
[03:49:58.080 --> 03:49:59.680]   But really the people that run everything
[03:49:59.680 --> 03:50:04.680]   are the folks that know like the low level software
[03:50:04.680 --> 03:50:06.480]   and hardware that runs everything,
[03:50:06.480 --> 03:50:08.280]   the networking, all of that.
[03:50:08.280 --> 03:50:11.120]   And so you have to like make sure you have procedures
[03:50:11.120 --> 03:50:12.100]   that test everything.
[03:50:12.100 --> 03:50:13.800]   I think they're using ethernet.
[03:50:13.800 --> 03:50:15.280]   I don't know how they're doing the networking, but.
[03:50:15.280 --> 03:50:18.000]   - They're using NVIDIA Spectrum X Ethernet.
[03:50:18.000 --> 03:50:19.560]   There's actually like, I think, yeah,
[03:50:19.560 --> 03:50:22.200]   the unsung heroes are the cooling and electrical systems
[03:50:22.200 --> 03:50:24.000]   which are just like glossed over.
[03:50:24.000 --> 03:50:25.160]   - Yeah.
[03:50:25.160 --> 03:50:27.960]   - But I think like one story that maybe is like
[03:50:27.960 --> 03:50:30.320]   exemplifies how insane this stuff is,
[03:50:30.320 --> 03:50:33.080]   is when you're training, right?
[03:50:33.080 --> 03:50:35.600]   You're always doing, you're running through the model
[03:50:35.600 --> 03:50:37.000]   a bunch, right, in the most simplistic terms,
[03:50:37.000 --> 03:50:38.480]   running through the model a bunch,
[03:50:38.480 --> 03:50:41.520]   and then you're gonna exchange everything
[03:50:41.520 --> 03:50:43.040]   and synchronize the weights, right?
[03:50:43.040 --> 03:50:44.120]   So you'll do a step.
[03:50:44.120 --> 03:50:45.720]   This is like a step in model training, right?
[03:50:45.720 --> 03:50:47.360]   And every step your loss goes down, hopefully.
[03:50:47.360 --> 03:50:50.240]   And it doesn't always, but in the simplest terms,
[03:50:50.240 --> 03:50:52.640]   you'll be computing a lot and then you'll exchange, right?
[03:50:52.640 --> 03:50:54.560]   The interesting thing is GPU power is most of it.
[03:50:54.560 --> 03:50:56.400]   Networking power is some, but it's a lot less.
[03:50:56.400 --> 03:50:57.800]   But so while you're computing,
[03:50:57.800 --> 03:50:59.360]   your power for your GPUs is here.
[03:50:59.360 --> 03:51:01.280]   But then when you're exchanging weights,
[03:51:01.280 --> 03:51:03.160]   if you're not able to overlap communications
[03:51:03.160 --> 03:51:05.300]   and compute perfectly, there may be a time period
[03:51:05.300 --> 03:51:08.080]   where your GPUs are just idle and you're exchanging weights
[03:51:08.080 --> 03:51:09.600]   and you're like, hey, the model's updating.
[03:51:09.600 --> 03:51:10.680]   So you're exchanging the gradients,
[03:51:10.680 --> 03:51:13.520]   you do the model update, and then you start training again.
[03:51:13.520 --> 03:51:15.760]   So the power goes, right?
[03:51:15.760 --> 03:51:17.120]   And it's super spiky.
[03:51:17.120 --> 03:51:18.480]   And so funnily enough, right?
[03:51:18.480 --> 03:51:20.760]   Like this, when you talk about the scale
[03:51:20.760 --> 03:51:22.040]   of data center power, right?
[03:51:22.040 --> 03:51:23.680]   You can blow stuff up so easily.
[03:51:23.680 --> 03:51:27.520]   And so Meta actually has accidentally
[03:51:27.520 --> 03:51:30.400]   open upstream something to code in PyTorch,
[03:51:30.400 --> 03:51:31.960]   where they added an operator.
[03:51:31.960 --> 03:51:33.320]   And I kid you not, whoever made this,
[03:51:33.320 --> 03:51:35.120]   like, I wanna hug the guy because it says,
[03:51:35.120 --> 03:51:39.120]   says PyTorch, it's like PyTorch.powerplant, no blow up.
[03:51:39.120 --> 03:51:40.600]   Equals zero or equal one.
[03:51:41.520 --> 03:51:44.160]   And what it does, what it does is amazing, right?
[03:51:44.160 --> 03:51:46.520]   Either, you know, when you're exchanging the weights,
[03:51:46.520 --> 03:51:48.240]   the GPU will just compute fake numbers.
[03:51:48.240 --> 03:51:49.880]   So the power doesn't spike too much.
[03:51:49.880 --> 03:51:51.400]   And so then the power plants don't blow up
[03:51:51.400 --> 03:51:54.240]   because the transient spikes, like screw stuff up.
[03:51:54.240 --> 03:51:55.080]   - Well, that makes sense.
[03:51:55.080 --> 03:51:56.600]   I mean, you have to do that kind of thing.
[03:51:56.600 --> 03:51:59.200]   You have to make sure they're not idle, yeah.
[03:51:59.200 --> 03:52:00.380]   - And Elon's solution was like,
[03:52:00.380 --> 03:52:01.760]   let me throw a bunch of Tesla mega packs
[03:52:01.760 --> 03:52:03.280]   and a few other things, right?
[03:52:03.280 --> 03:52:04.600]   Everyone has different solutions,
[03:52:04.600 --> 03:52:07.240]   but like Meta's at least was publicly and openly known,
[03:52:07.240 --> 03:52:09.240]   which is just like, set this operator.
[03:52:09.240 --> 03:52:11.040]   And what this operator does is it just makes
[03:52:11.040 --> 03:52:14.360]   the GPUs compute nothing so that the power doesn't spike.
[03:52:14.360 --> 03:52:17.040]   - But that just tells you how much power you're working with.
[03:52:17.040 --> 03:52:18.080]   I mean, it's insane.
[03:52:18.080 --> 03:52:18.920]   It's insane.
[03:52:18.920 --> 03:52:21.480]   - People should just go to Google, like scale,
[03:52:21.480 --> 03:52:24.720]   like what does X Watts do and go through all the scales
[03:52:24.720 --> 03:52:26.920]   from one watt to a kilowatt to a megawatt.
[03:52:26.920 --> 03:52:28.680]   And you look and stare at that
[03:52:28.680 --> 03:52:31.320]   and you're how high on the list a gigawatt is.
[03:52:31.320 --> 03:52:33.200]   And it's mind blowing.
[03:52:33.200 --> 03:52:35.520]   - Can you say something about the cooling?
[03:52:35.520 --> 03:52:38.880]   So I know Elon's using liquid cooling,
[03:52:38.880 --> 03:52:42.320]   I believe in all cases.
[03:52:42.320 --> 03:52:43.600]   That's a new thing, right?
[03:52:43.600 --> 03:52:45.120]   Most of them don't use liquid cooling.
[03:52:45.120 --> 03:52:46.760]   Is there something interesting to say about the cooling?
[03:52:46.760 --> 03:52:47.600]   - Yeah, yeah.
[03:52:47.600 --> 03:52:50.100]   So air cooling has been the de facto standard,
[03:52:50.100 --> 03:52:52.360]   throw a bunch of metal, heat pipes, et cetera,
[03:52:52.360 --> 03:52:53.200]   and fans, right?
[03:52:53.200 --> 03:52:54.240]   And like that's cool.
[03:52:54.240 --> 03:52:56.400]   That's been enough to cool it.
[03:52:56.400 --> 03:52:58.080]   People have been dabbling in water cooling.
[03:52:58.080 --> 03:53:01.480]   Google's TPUs are water cooled, right?
[03:53:01.480 --> 03:53:03.760]   So they've been doing that for a few years.
[03:53:03.760 --> 03:53:05.800]   But with GPUs, no one's ever done.
[03:53:05.800 --> 03:53:07.520]   And no one's ever done the scale of water cooling
[03:53:07.520 --> 03:53:09.640]   that Elon just did, right?
[03:53:09.640 --> 03:53:13.920]   Now next generation NVIDIA is for the like highest end GPU,
[03:53:13.920 --> 03:53:15.080]   it is mandatory water cooling.
[03:53:15.080 --> 03:53:16.160]   You have to water cool it.
[03:53:16.160 --> 03:53:18.800]   But Elon did it on this current generation
[03:53:18.800 --> 03:53:20.240]   and that required a lot of stuff, right?
[03:53:20.240 --> 03:53:21.880]   If you look at like some of the satellite photos
[03:53:21.880 --> 03:53:25.240]   and stuff of the Memphis facility,
[03:53:25.240 --> 03:53:27.100]   there's all these external water chillers
[03:53:27.100 --> 03:53:27.960]   that are sitting basically.
[03:53:27.960 --> 03:53:30.640]   It looks like a semi-truck pod thing.
[03:53:30.640 --> 03:53:31.480]   What's it called?
[03:53:31.480 --> 03:53:32.300]   The container.
[03:53:32.300 --> 03:53:33.320]   But really those are water chillers.
[03:53:33.320 --> 03:53:35.080]   And he has like 90 of those water chillers
[03:53:35.080 --> 03:53:37.440]   just sitting outside, 90 different containers, right?
[03:53:37.440 --> 03:53:39.440]   With the water, you know, like chill the water,
[03:53:39.440 --> 03:53:40.880]   bring it back to the data center,
[03:53:40.880 --> 03:53:42.760]   and then you distribute it to all the chips,
[03:53:42.760 --> 03:53:44.680]   pull all the heat out and then send it back, right?
[03:53:44.680 --> 03:53:47.720]   And this is both a way to cool the chips,
[03:53:47.720 --> 03:53:49.800]   but also as an efficiency thing, all right?
[03:53:49.800 --> 03:53:52.040]   And going back to that like sort of three vector thing,
[03:53:52.040 --> 03:53:56.720]   right, there is memory bandwidth flops and interconnect.
[03:53:56.720 --> 03:53:58.120]   The closer the chips are together,
[03:53:58.120 --> 03:54:02.280]   the easier it is to do high speed interconnects, right?
[03:54:02.280 --> 03:54:04.760]   And so this is also like a reason
[03:54:04.760 --> 03:54:05.800]   why you're gonna go water cooling
[03:54:05.800 --> 03:54:07.600]   is because you can just put the chips
[03:54:07.600 --> 03:54:08.560]   right next to each other
[03:54:08.560 --> 03:54:12.220]   and therefore get higher speed connectivity.
[03:54:12.220 --> 03:54:18.200]   - I gotta ask you, so in one of your recent posts,
[03:54:18.200 --> 03:54:21.520]   there's a section called Cluster Measuring Contest.
[03:54:21.520 --> 03:54:22.800]   So--
[03:54:22.800 --> 03:54:25.060]   - There's another word there, but I won't say it, you know?
[03:54:25.060 --> 03:54:27.560]   (laughs)
[03:54:27.560 --> 03:54:31.000]   - Who's got the biggest now and who's gonna have the biggest?
[03:54:31.000 --> 03:54:34.120]   - Today, individual largest is Elon, right?
[03:54:34.120 --> 03:54:36.680]   - Elon's Cluster.
[03:54:36.680 --> 03:54:40.440]   - Elon's Cluster in Memphis, 200,000 GPUs, right?
[03:54:40.440 --> 03:54:43.080]   Meta has like 128,000, OpenAI has 100,000.
[03:54:43.080 --> 03:54:46.160]   Now, to be clear, other companies have more GPUs than Elon,
[03:54:46.160 --> 03:54:48.080]   they just don't have them in one place, right?
[03:54:48.080 --> 03:54:50.560]   And for training, you want them tightly connected.
[03:54:50.560 --> 03:54:52.760]   There's some techniques that people are researching
[03:54:52.760 --> 03:54:56.580]   and working on that lets you train across multiple regions,
[03:54:56.580 --> 03:54:57.440]   but for the most part,
[03:54:57.440 --> 03:54:59.560]   you want them all in like one area, right?
[03:54:59.560 --> 03:55:02.800]   So you can connect them with high-speed networking.
[03:55:02.800 --> 03:55:06.180]   And so, you know, Elon today has 200,000 H100s,
[03:55:06.180 --> 03:55:08.840]   and 100,000 H100s, 100,000 H200s, right?
[03:55:08.840 --> 03:55:14.880]   Meta, OpenAI, you know, and Amazon all have on the scale
[03:55:14.880 --> 03:55:17.100]   of 100,000, a little bit less.
[03:55:17.100 --> 03:55:18.920]   But this year, right, this year,
[03:55:18.920 --> 03:55:20.360]   people are building much more, right?
[03:55:20.360 --> 03:55:22.200]   Anthropic and Amazon are building a cluster
[03:55:22.200 --> 03:55:26.000]   of 400,000 Tranium II, which is Amazon-specific chip,
[03:55:26.000 --> 03:55:28.440]   trying to get away from NVIDIA, right?
[03:55:28.440 --> 03:55:32.600]   You know, Meta and OpenAI have scales
[03:55:32.600 --> 03:55:33.440]   for hundreds of thousands.
[03:55:33.440 --> 03:55:36.080]   But by next year, you'll have like 500,000
[03:55:36.080 --> 03:55:38.000]   to 700,000 GPU clusters.
[03:55:38.000 --> 03:55:40.800]   And note, those GPUs are much higher power consumption
[03:55:40.800 --> 03:55:41.840]   than existing ones, right?
[03:55:41.840 --> 03:55:45.620]   Hopper's 700 watts, Blackwell goes to 1200 watts, right?
[03:55:45.620 --> 03:55:47.720]   So the power per chip is growing,
[03:55:47.720 --> 03:55:49.920]   and the number of chips is growing, right?
[03:55:49.920 --> 03:55:54.040]   - Nuts, you think Elon said he'll get to a million.
[03:55:54.040 --> 03:55:56.400]   You think that's actually feasible?
[03:55:56.400 --> 03:55:59.080]   - I mean, I don't doubt Elon, right?
[03:55:59.080 --> 03:56:00.960]   The filings that he has for like, you know,
[03:56:00.960 --> 03:56:03.140]   the power plan and the Tesla battery packs,
[03:56:03.140 --> 03:56:05.760]   it's clear he has some crazy plans for Memphis.
[03:56:05.760 --> 03:56:08.540]   Like permits and stuff is open record, right?
[03:56:08.540 --> 03:56:11.200]   But it's not quite clear that, you know,
[03:56:11.200 --> 03:56:13.640]   what and what the timescales are.
[03:56:13.640 --> 03:56:14.920]   I just never doubt Elon, right?
[03:56:14.920 --> 03:56:16.520]   You know, he's gonna surprise us.
[03:56:16.520 --> 03:56:17.840]   - So what's the idea with these clusters?
[03:56:17.840 --> 03:56:21.520]   If you have a million GPUs, what percentage
[03:56:21.520 --> 03:56:24.400]   in let's say two, three years is used
[03:56:24.400 --> 03:56:27.920]   for training and what percent, pre-training,
[03:56:27.920 --> 03:56:30.220]   and what percent is used for like,
[03:56:30.220 --> 03:56:31.060]   for the actual computation?
[03:56:31.060 --> 03:56:34.120]   - So these mega clusters make no sense for inference, right?
[03:56:34.120 --> 03:56:37.160]   You could route inference there and just not train.
[03:56:37.160 --> 03:56:39.560]   But most of the inference capacity is being, you know,
[03:56:39.560 --> 03:56:41.580]   "Hey, I've got a 30 megawatt data center here.
[03:56:41.580 --> 03:56:42.560]   "I've got 50 megawatts here.
[03:56:42.560 --> 03:56:43.500]   "I've got a hundred here, whatever.
[03:56:43.500 --> 03:56:45.940]   "I'll just throw inference in all of those."
[03:56:45.940 --> 03:56:47.920]   Because the mega clusters, right?
[03:56:47.920 --> 03:56:50.680]   Multi gigawatt data centers, I wanna train there.
[03:56:50.680 --> 03:56:52.760]   Because that's where all of my GPUs are co-located,
[03:56:52.760 --> 03:56:55.480]   where I can put them at a super high networking speed
[03:56:55.480 --> 03:56:56.720]   connected together, right?
[03:56:56.720 --> 03:56:58.240]   Because that's what you need for training.
[03:56:58.240 --> 03:57:00.700]   Now, with pre-training, this is the old scale, right?
[03:57:00.700 --> 03:57:02.160]   You could, you would increase parameters.
[03:57:02.160 --> 03:57:05.040]   You didn't increase data, model gets better.
[03:57:05.040 --> 03:57:06.760]   That doesn't apply anymore,
[03:57:06.760 --> 03:57:08.180]   because there's not much more data
[03:57:08.180 --> 03:57:10.200]   in the pre-training side, right?
[03:57:10.200 --> 03:57:11.800]   Yes, there's video and audio and image
[03:57:11.800 --> 03:57:13.860]   that has not been fully taken advantage of.
[03:57:13.860 --> 03:57:14.880]   So there's a lot more scaling,
[03:57:14.880 --> 03:57:17.440]   but a lot of people like, have transcript,
[03:57:17.440 --> 03:57:18.880]   taken transcripts of YouTube videos.
[03:57:18.880 --> 03:57:20.120]   And that gets you a lot of the data.
[03:57:20.120 --> 03:57:21.520]   It doesn't get you all of the learning value
[03:57:21.520 --> 03:57:22.760]   out of the video and image data.
[03:57:22.760 --> 03:57:26.440]   But there's still scaling to be done on pre-training.
[03:57:26.440 --> 03:57:27.860]   But this post-training world
[03:57:27.860 --> 03:57:29.960]   is where all the flops are gonna be spent, right?
[03:57:29.960 --> 03:57:31.240]   The model is gonna play with itself.
[03:57:31.240 --> 03:57:32.080]   It's gonna self-play.
[03:57:32.080 --> 03:57:33.400]   It's gonna do verifiable tasks.
[03:57:33.400 --> 03:57:36.040]   It's gonna do computer use in sandboxes.
[03:57:36.040 --> 03:57:38.800]   It might even do like simulated robotics things, right?
[03:57:38.800 --> 03:57:41.460]   Like all of these things are gonna be environments
[03:57:41.460 --> 03:57:44.660]   where compute is spent in quote-unquote post-training.
[03:57:44.660 --> 03:57:46.400]   But I think it's gonna be good.
[03:57:46.400 --> 03:57:48.840]   We're gonna drop the post from post-training.
[03:57:48.840 --> 03:57:49.680]   - Yeah, wow. - It's gonna be pre-training
[03:57:49.680 --> 03:57:51.080]   and it's gonna be training, I think.
[03:57:51.080 --> 03:57:53.880]   - The return of the king. - At some point.
[03:57:53.880 --> 03:57:57.400]   Because for the bulk of the last few years,
[03:57:57.400 --> 03:58:00.140]   pre-training has dwarfed post-training.
[03:58:00.140 --> 03:58:01.720]   But with these verifiable methods,
[03:58:01.720 --> 03:58:05.480]   especially ones that scale really potentially infinitely,
[03:58:05.480 --> 03:58:06.680]   like computer use and robotics,
[03:58:06.680 --> 03:58:08.080]   not just math and coding, right?
[03:58:08.080 --> 03:58:09.560]   Where you can verify what's happening.
[03:58:09.560 --> 03:58:11.280]   Those infinitely verifiable tasks,
[03:58:11.280 --> 03:58:13.440]   it seems you can spend as much compute as you want on them.
[03:58:13.440 --> 03:58:15.020]   - Especially at the context length increase.
[03:58:15.020 --> 03:58:16.560]   'Cause at the end of pre-training
[03:58:16.560 --> 03:58:19.120]   is when you increase the context length for these models.
[03:58:19.120 --> 03:58:21.480]   And we've talked earlier in the conversation
[03:58:21.480 --> 03:58:23.000]   about how the context length,
[03:58:23.000 --> 03:58:24.280]   when you have a long input,
[03:58:24.280 --> 03:58:26.000]   is much easier to manage than output.
[03:58:26.000 --> 03:58:28.080]   And a lot of these post-training and reasoning techniques
[03:58:28.080 --> 03:58:30.560]   rely on a ton of sampling
[03:58:30.560 --> 03:58:32.960]   and it's becoming increasingly long context.
[03:58:32.960 --> 03:58:34.480]   So it's just like you're,
[03:58:34.480 --> 03:58:36.920]   effectively your compute efficiency goes down.
[03:58:36.920 --> 03:58:39.880]   I don't, I think FLOPS is the standard
[03:58:39.880 --> 03:58:40.880]   for how you measure it.
[03:58:40.880 --> 03:58:43.400]   But with RL and you have to do all these things
[03:58:43.400 --> 03:58:45.640]   where you move your weights around
[03:58:45.640 --> 03:58:47.640]   in a different way than at pre-training
[03:58:47.640 --> 03:58:49.360]   and just generation,
[03:58:49.360 --> 03:58:51.520]   it's going to become less efficient
[03:58:51.520 --> 03:58:53.800]   and FLOPS is gonna be less of a useful term.
[03:58:53.800 --> 03:58:55.240]   And then as the infrastructure gets better,
[03:58:55.240 --> 03:58:57.000]   it's probably gonna go back to FLOPS.
[03:58:57.000 --> 03:58:59.080]   - So all of the things we've been talking about
[03:58:59.080 --> 03:59:01.960]   is most likely going to be NVIDIA, right?
[03:59:01.960 --> 03:59:03.400]   Is there any competitors?
[03:59:03.400 --> 03:59:05.540]   - Google, I kind of ignored them.
[03:59:05.540 --> 03:59:08.040]   - Yeah, what's the story with TPU?
[03:59:08.040 --> 03:59:09.400]   What's the story with TPU?
[03:59:09.400 --> 03:59:10.600]   Like what's the-
[03:59:10.600 --> 03:59:12.160]   - TPU is awesome, right?
[03:59:12.160 --> 03:59:13.120]   It's great.
[03:59:13.120 --> 03:59:15.960]   Google is, they're a bit more tepid
[03:59:15.960 --> 03:59:17.400]   on building data centers for some reason.
[03:59:17.400 --> 03:59:18.520]   They're building big data centers,
[03:59:18.520 --> 03:59:19.360]   don't get me wrong.
[03:59:19.360 --> 03:59:21.200]   And they have, they actually have the biggest cluster.
[03:59:21.200 --> 03:59:23.400]   Let me, I was talking about NVIDIA clusters.
[03:59:23.400 --> 03:59:25.880]   They actually have the biggest cluster, period.
[03:59:25.880 --> 03:59:28.860]   But the way they do it is like very interesting, right?
[03:59:28.860 --> 03:59:32.360]   They have two sort of like data center super regions, right?
[03:59:32.360 --> 03:59:34.280]   In that the data center isn't physically,
[03:59:34.280 --> 03:59:36.400]   like all of the GPUs aren't physically on one site,
[03:59:36.400 --> 03:59:37.640]   but they're like 30 miles from each other.
[03:59:37.640 --> 03:59:38.960]   They're not GPUs, TPUs, right?
[03:59:38.960 --> 03:59:41.320]   They have like in Iowa and Nebraska,
[03:59:41.320 --> 03:59:42.400]   they have four data centers
[03:59:42.400 --> 03:59:44.240]   that are just like right next to each other.
[03:59:44.240 --> 03:59:48.140]   - Why doesn't Google flex its cluster size?
[03:59:48.140 --> 03:59:49.680]   - Go to multi data center training.
[03:59:49.680 --> 03:59:50.760]   It's a good images in there.
[03:59:50.760 --> 03:59:51.800]   So I'll show you what I mean.
[03:59:51.800 --> 03:59:54.840]   It's just a semi-analysis multi data center.
[03:59:54.840 --> 03:59:55.840]   So this is like, you know,
[03:59:55.840 --> 03:59:56.680]   so this is an image
[03:59:56.680 --> 03:59:58.900]   of like what a standard Google data center looks like.
[03:59:58.900 --> 04:00:00.560]   By the way, their data centers look very different
[04:00:00.560 --> 04:00:01.760]   than anyone else's data centers.
[04:00:01.760 --> 04:00:03.080]   - What are we looking at here?
[04:00:03.080 --> 04:00:03.920]   - So these are, yeah.
[04:00:03.920 --> 04:00:05.720]   So if you see this image, right?
[04:00:05.720 --> 04:00:08.400]   In the center, there are these big rectangular boxes, right?
[04:00:08.400 --> 04:00:10.540]   Those are where the actual chips are kept.
[04:00:10.540 --> 04:00:13.360]   And then if you scroll down a little bit further,
[04:00:13.360 --> 04:00:15.960]   you can see there's like these water pipes,
[04:00:15.960 --> 04:00:18.520]   there's these chiller cooling towers in the top
[04:00:18.520 --> 04:00:20.080]   and a bunch of like diesel generators.
[04:00:20.080 --> 04:00:22.280]   The diesel generators are backup power.
[04:00:22.280 --> 04:00:24.400]   The data center itself is like,
[04:00:24.400 --> 04:00:26.920]   look physically smaller than the water chillers, right?
[04:00:26.920 --> 04:00:29.740]   So the chips are actually easier to like keep together,
[04:00:29.740 --> 04:00:31.080]   but then like cooling all the water
[04:00:31.080 --> 04:00:33.220]   for the water cooling is very difficult, right?
[04:00:33.220 --> 04:00:35.360]   So Google has like a very advanced infrastructure
[04:00:35.360 --> 04:00:38.040]   that no one else has for the TPU.
[04:00:38.040 --> 04:00:40.560]   And what they do is they've like stamped these data center,
[04:00:40.560 --> 04:00:42.360]   they've stamped a bunch of these data centers out
[04:00:42.360 --> 04:00:43.500]   in a few regions, right?
[04:00:43.500 --> 04:00:46.580]   So if you go a little bit further down,
[04:00:46.580 --> 04:00:48.440]   this is a Microsoft, this is in Arizona,
[04:00:48.440 --> 04:00:50.940]   this is where GPT-5 quote unquote will be trained.
[04:00:50.940 --> 04:00:54.320]   - If it doesn't exist already.
[04:00:54.320 --> 04:00:56.220]   - Yeah, if it doesn't exist already.
[04:00:56.220 --> 04:00:57.640]   But each of these data centers, right?
[04:00:57.640 --> 04:00:59.160]   I've shown a couple of images of them.
[04:00:59.160 --> 04:01:00.920]   They're like really closely co-located
[04:01:00.920 --> 04:01:02.200]   in the same region, right?
[04:01:02.200 --> 04:01:03.040]   Nebraska, Iowa.
[04:01:03.040 --> 04:01:06.500]   And then they also have a similar one in Ohio complex, right?
[04:01:06.500 --> 04:01:09.340]   And so these data centers are really close to each other.
[04:01:09.340 --> 04:01:11.020]   And what they've done is they've connected them
[04:01:11.020 --> 04:01:13.120]   super high bandwidth with fiber.
[04:01:13.120 --> 04:01:14.960]   And so these are just a bunch of data centers.
[04:01:14.960 --> 04:01:17.120]   And the point here is that Google
[04:01:17.120 --> 04:01:19.660]   has a very advanced infrastructure,
[04:01:19.660 --> 04:01:21.840]   very tightly connected in a small region.
[04:01:21.840 --> 04:01:23.800]   So Elon will always have the biggest cluster
[04:01:23.800 --> 04:01:24.960]   fully connected, right?
[04:01:24.960 --> 04:01:26.840]   Because it's all in one building, right?
[04:01:26.840 --> 04:01:28.540]   And he's completely right on that, right?
[04:01:28.540 --> 04:01:30.580]   Google has the biggest cluster,
[04:01:30.580 --> 04:01:31.920]   but you have to spread over three sites
[04:01:31.920 --> 04:01:33.840]   and by a significant margin,
[04:01:33.840 --> 04:01:35.480]   we have to go across multiple sites.
[04:01:35.480 --> 04:01:39.120]   - Why doesn't Google compete with Nvidia?
[04:01:39.120 --> 04:01:41.780]   Why don't they sell TPUs?
[04:01:41.780 --> 04:01:43.740]   - I think there's a couple problems with it.
[04:01:43.740 --> 04:01:48.740]   It's like one, TPU has been a form of allowing search
[04:01:48.740 --> 04:01:53.300]   to be really fricking cheap and build models for that, right?
[04:01:53.300 --> 04:01:56.280]   And so like a big chunk of the search GPU purchases
[04:01:56.280 --> 04:01:59.860]   or TPU purchases are big chunk of Google's purchases
[04:01:59.860 --> 04:02:02.380]   and usage, all of it is for internal workloads, right?
[04:02:02.380 --> 04:02:05.140]   Whether it be search, now Gemini, right?
[04:02:05.140 --> 04:02:07.740]   YouTube, all these different applications
[04:02:07.740 --> 04:02:10.300]   that they have, you know, ads.
[04:02:10.300 --> 04:02:12.020]   These are where all their TPUs are being spent
[04:02:12.020 --> 04:02:14.340]   and that's what they're hyper-focused on, right?
[04:02:14.340 --> 04:02:17.020]   And so there's certain like aspects of the architecture
[04:02:17.020 --> 04:02:18.660]   that are optimized for their use case
[04:02:18.660 --> 04:02:21.140]   that are not optimized elsewhere, right?
[04:02:21.140 --> 04:02:23.860]   One simple one is like they've open-sourced the Gemma model
[04:02:23.860 --> 04:02:26.020]   and they called it Gemma 7B, right?
[04:02:26.020 --> 04:02:27.780]   But then it's actually 8 billion parameters
[04:02:27.780 --> 04:02:30.240]   because the vocabulary is so large.
[04:02:30.240 --> 04:02:32.020]   And the reason they made the vocabulary so large
[04:02:32.020 --> 04:02:35.860]   is because TPUs like matrix multiply unit is massive
[04:02:35.860 --> 04:02:37.860]   because that's what they've like sort of optimized for.
[04:02:37.860 --> 04:02:38.700]   And so they decided, oh,
[04:02:38.700 --> 04:02:40.500]   well, I'll just make the vocabulary large too,
[04:02:40.500 --> 04:02:42.860]   even though it makes no sense to do so in such a small model
[04:02:42.860 --> 04:02:44.260]   because that fits on their hardware.
[04:02:44.260 --> 04:02:46.040]   So Gemma doesn't run as efficiently
[04:02:46.040 --> 04:02:47.860]   on a GPU as a Llama does, right?
[04:02:47.860 --> 04:02:50.220]   But vice versa, Llama doesn't run as efficiently
[04:02:50.220 --> 04:02:52.480]   on a TPU as a Gemma does, right?
[04:02:52.480 --> 04:02:54.320]   And it's so like, there's like certain like aspects
[04:02:54.320 --> 04:02:55.860]   of like hardware software co-design.
[04:02:55.860 --> 04:02:57.660]   So all their search models are their ranking
[04:02:57.660 --> 04:02:59.940]   and recommendation models, all these different models
[04:02:59.940 --> 04:03:02.580]   that are AI, but not like Gen AI, right?
[04:03:02.580 --> 04:03:05.220]   Have been hyper-optimized with TPUs forever.
[04:03:05.220 --> 04:03:06.920]   The software stack is super optimized,
[04:03:06.920 --> 04:03:08.660]   but all of this software stack
[04:03:08.660 --> 04:03:11.500]   has not been released publicly at all, right?
[04:03:11.500 --> 04:03:13.900]   Very small portions of it, JAX and XLA have been,
[04:03:13.900 --> 04:03:16.460]   but like the experience when you're inside of Google
[04:03:16.460 --> 04:03:18.620]   and you're training on TPUs as a researcher,
[04:03:18.620 --> 04:03:19.460]   you don't need to know anything
[04:03:19.460 --> 04:03:21.100]   about the hardware in many cases, right?
[04:03:21.100 --> 04:03:22.660]   Like, it's like pretty beautiful.
[04:03:22.660 --> 04:03:24.580]   But as soon as you step outside-
[04:03:24.580 --> 04:03:25.420]   - They all love it.
[04:03:25.420 --> 04:03:26.820]   A lot of them go back.
[04:03:26.820 --> 04:03:28.440]   They leave Google and then they go back.
[04:03:28.440 --> 04:03:29.280]   - Yeah.
[04:03:29.280 --> 04:03:31.180]   - Yeah, they're like, they leave and they start a company
[04:03:31.180 --> 04:03:32.540]   'cause they have all these amazing research ideas
[04:03:32.540 --> 04:03:34.740]   and they're like, wait, infrastructure's hard.
[04:03:34.740 --> 04:03:35.700]   Software is hard.
[04:03:35.700 --> 04:03:36.620]   And this is on GPUs.
[04:03:36.620 --> 04:03:38.020]   Or if they try to use TPUs, same thing,
[04:03:38.020 --> 04:03:39.820]   'cause they don't have access to all this code.
[04:03:39.820 --> 04:03:41.820]   And so it's like, how do you convince a company
[04:03:41.820 --> 04:03:43.220]   whose golden goose is search,
[04:03:43.220 --> 04:03:45.820]   where they're making hundreds of billions of dollars from,
[04:03:45.820 --> 04:03:48.500]   to start selling GPU or TPUs,
[04:03:48.500 --> 04:03:50.980]   which they used to only buy a couple billion of, you know.
[04:03:50.980 --> 04:03:55.620]   I think in 2023, they bought like a couple billion
[04:03:55.620 --> 04:03:56.860]   and now they're buying like 10 billion
[04:03:56.860 --> 04:03:58.180]   to $15 billion worth.
[04:03:58.180 --> 04:03:59.060]   But how do you convince them
[04:03:59.060 --> 04:04:01.100]   that they should just buy like twice as many
[04:04:01.100 --> 04:04:03.260]   and figure out how to sell them and make $30 billion?
[04:04:03.260 --> 04:04:05.340]   It's like, who cares about making $30 billion?
[04:04:05.340 --> 04:04:08.420]   - Won't that 30 billion exceed, actually,
[04:04:08.420 --> 04:04:10.220]   the search profit eventually?
[04:04:10.220 --> 04:04:12.820]   - Oh, I mean, like, you're always gonna make more money
[04:04:12.820 --> 04:04:14.580]   on services than--
[04:04:14.580 --> 04:04:15.660]   - Always.
[04:04:15.660 --> 04:04:17.060]   - I mean, like, yeah.
[04:04:17.060 --> 04:04:17.980]   To be clear, like today,
[04:04:17.980 --> 04:04:19.900]   people are spending a lot more on hardware
[04:04:19.900 --> 04:04:21.700]   than they are the services, right?
[04:04:21.700 --> 04:04:25.320]   Because the hardware front runs the service spend.
[04:04:25.320 --> 04:04:26.160]   But like--
[04:04:26.160 --> 04:04:26.980]   - You're investing, yeah.
[04:04:26.980 --> 04:04:29.660]   - If there's no revenue for AI stuff or not enough revenue,
[04:04:29.660 --> 04:04:31.940]   then obviously, like, it's gonna blow up, right?
[04:04:31.940 --> 04:04:34.460]   People won't continue to spend on GPUs forever.
[04:04:34.460 --> 04:04:36.080]   And then NVIDIA is trying to move up the stack
[04:04:36.080 --> 04:04:37.500]   with like software that they're trying to sell
[04:04:37.500 --> 04:04:38.820]   and license and stuff, right?
[04:04:38.820 --> 04:04:42.220]   But Google has never had that like DNA of like,
[04:04:42.220 --> 04:04:43.900]   this is a product we should sell, right?
[04:04:43.900 --> 04:04:46.100]   They don't, the Google Cloud does it,
[04:04:46.100 --> 04:04:47.900]   which is a separate organization from the TPU team,
[04:04:47.900 --> 04:04:49.820]   which is a separate organization from the DeepMind team,
[04:04:49.820 --> 04:04:51.580]   which is a separate organization from the search team,
[04:04:51.580 --> 04:04:52.620]   right, there's a lot of bureaucracy.
[04:04:52.620 --> 04:04:55.500]   - Wait, Google Cloud is a separate team than the TPU team?
[04:04:55.500 --> 04:04:58.420]   - Technically, TPU sits under infrastructure,
[04:04:58.420 --> 04:05:00.020]   which sits under Google Cloud.
[04:05:00.020 --> 04:05:04.020]   But like Google Cloud, like for like renting stuff
[04:05:04.020 --> 04:05:07.360]   and TPU architecture are very different goals, right?
[04:05:07.360 --> 04:05:09.740]   In hardware and software, like all of this, right?
[04:05:09.740 --> 04:05:11.420]   Like the JAXX XLA teams
[04:05:11.420 --> 04:05:13.980]   do not serve Google's customers externally,
[04:05:13.980 --> 04:05:15.620]   whereas NVIDIA's various CUDA teams
[04:05:15.620 --> 04:05:18.900]   for like things like Nickel serve external customers, right?
[04:05:18.900 --> 04:05:21.960]   The internal teams like JAXX and XLA and stuff,
[04:05:21.960 --> 04:05:24.540]   they more so serve DeepMind and search, right?
[04:05:24.540 --> 04:05:25.700]   And so their customer is different,
[04:05:25.700 --> 04:05:27.300]   they're not building a product for them.
[04:05:27.300 --> 04:05:29.460]   - Do you understand why AWS keeps winning
[04:05:29.460 --> 04:05:34.740]   versus Azure for cloud versus Google Cloud?
[04:05:34.740 --> 04:05:36.500]   - Yeah, there's-- - Google Cloud is tiny,
[04:05:36.500 --> 04:05:38.660]   isn't it, relative to AWS? - Google Cloud is third.
[04:05:38.660 --> 04:05:40.020]   Yeah, yeah.
[04:05:40.020 --> 04:05:41.260]   Microsoft is the second biggest,
[04:05:41.260 --> 04:05:42.940]   but Amazon is the biggest, right?
[04:05:42.940 --> 04:05:45.660]   And Microsoft deceptively sort of includes
[04:05:45.660 --> 04:05:48.060]   like Microsoft Office 365 and things like that,
[04:05:48.060 --> 04:05:50.020]   like some of these enterprise-wide licenses.
[04:05:50.020 --> 04:05:51.900]   So in reality, the gulf is even larger.
[04:05:51.900 --> 04:05:53.900]   Microsoft is still second though, right?
[04:05:53.900 --> 04:05:55.220]   Amazon is way bigger, why?
[04:05:55.220 --> 04:05:57.900]   Because using AWS is better and easier.
[04:05:57.900 --> 04:05:59.380]   And in many cases-- - It was first.
[04:05:59.380 --> 04:06:00.660]   - And it's first, yeah. - It was first.
[04:06:00.660 --> 04:06:02.980]   - Yeah, but there's a lot of things that are first that--
[04:06:02.980 --> 04:06:05.540]   - Well, it's easier, it's harder to switch than it is to--
[04:06:05.540 --> 04:06:06.900]   - Yeah, okay. - But AWS--
[04:06:06.900 --> 04:06:08.380]   - Because it's large-- - There's big fees
[04:06:08.380 --> 04:06:09.220]   for switching too.
[04:06:09.220 --> 04:06:11.860]   - AWS generates over 80% of Amazon's profit,
[04:06:11.860 --> 04:06:13.780]   I think over 90%, right? - That's insane.
[04:06:13.780 --> 04:06:15.380]   - The distribution centers are just like,
[04:06:15.380 --> 04:06:18.000]   one day we'll decide to make money from this.
[04:06:18.000 --> 04:06:19.020]   But they haven't yet, right?
[04:06:19.020 --> 04:06:20.260]   Like they make tiny little profit from it.
[04:06:20.260 --> 04:06:22.580]   - Yeah, one day Amazon Prime will triple in price.
[04:06:22.580 --> 04:06:26.860]   - You would think they would improve AWS interface
[04:06:26.860 --> 04:06:29.340]   'cause it's like horrible, it's like clunky.
[04:06:29.340 --> 04:06:31.700]   - I have no idea. - But everybody is--
[04:06:31.700 --> 04:06:33.700]   - Yeah, one would think.
[04:06:33.700 --> 04:06:36.020]   - I think actually Google's interface is sometimes nice,
[04:06:36.020 --> 04:06:37.540]   but it's also like they don't care about anyone
[04:06:37.540 --> 04:06:38.700]   besides their top customers.
[04:06:38.700 --> 04:06:39.860]   - Yeah, exactly. - And like their customer
[04:06:39.860 --> 04:06:42.100]   service sucks, and like they have a lot less like--
[04:06:42.100 --> 04:06:42.980]   - I mean, all of these companies,
[04:06:42.980 --> 04:06:45.220]   they optimize for the big customers, yeah.
[04:06:45.220 --> 04:06:46.460]   It's supposed to be for business.
[04:06:46.460 --> 04:06:48.380]   - Well, Amazon has always optimized
[04:06:48.380 --> 04:06:49.580]   for the small customer too though, right?
[04:06:49.580 --> 04:06:51.380]   Like obviously they optimize a lot for the big customer,
[04:06:51.380 --> 04:06:53.620]   but like when they started, they just would go
[04:06:53.620 --> 04:06:56.660]   to like random Bay Area things and give out credits, right?
[04:06:56.660 --> 04:06:58.420]   And then they like, or just put in your credit card
[04:06:58.420 --> 04:06:59.260]   and use us, right?
[04:06:59.260 --> 04:07:00.340]   Like it's back in the early days.
[04:07:00.340 --> 04:07:02.340]   So they've always, the business has grown with them,
[04:07:02.340 --> 04:07:03.180]   right, and burgeoned.
[04:07:03.180 --> 04:07:05.540]   So like why does Amazon, like why is Snowflake
[04:07:05.540 --> 04:07:06.380]   all over Amazon?
[04:07:06.380 --> 04:07:07.300]   Because Snowflake in the beginning
[04:07:07.300 --> 04:07:08.940]   when Amazon didn't care about them
[04:07:08.940 --> 04:07:10.180]   was still using Amazon, right?
[04:07:10.180 --> 04:07:11.820]   And then of course one day Snowflake and Amazon
[04:07:11.820 --> 04:07:13.100]   has a super huge partnership.
[04:07:13.100 --> 04:07:16.340]   But like this is the case, like Amazon's user experience
[04:07:16.340 --> 04:07:17.220]   and quality is better.
[04:07:17.220 --> 04:07:19.300]   Also a lot of the silicon they've engineered
[04:07:19.300 --> 04:07:20.900]   makes them have a lower cost structure
[04:07:20.900 --> 04:07:23.380]   than traditional cloud storage, CPU, networking,
[04:07:23.380 --> 04:07:24.700]   that kind of stuff.
[04:07:24.700 --> 04:07:28.060]   Then in databases, right, like, you know,
[04:07:28.060 --> 04:07:31.940]   I think like four of Amazon's top five revenue products,
[04:07:31.940 --> 04:07:34.020]   margin products, sorry, like gross profit products
[04:07:34.020 --> 04:07:36.180]   are all database related products like Redshift
[04:07:36.180 --> 04:07:37.420]   and like all these things, right?
[04:07:37.420 --> 04:07:42.180]   Like, so Amazon has a very like good silicon
[04:07:42.180 --> 04:07:45.140]   to a user experience, like entire pipeline with AWS.
[04:07:45.140 --> 04:07:47.980]   I think Google, their silicon teams,
[04:07:47.980 --> 04:07:49.420]   yeah, they have awesome silicon internally,
[04:07:49.420 --> 04:07:51.980]   TPU, the YouTube chip, you know,
[04:07:51.980 --> 04:07:53.620]   some of these other chips that they've made.
[04:07:53.620 --> 04:07:56.860]   And the problem is they're not serving external customers
[04:07:56.860 --> 04:07:58.620]   or serving internal customers, right?
[04:07:58.620 --> 04:08:00.460]   - I mean, NVIDIA's entire culture is designed
[04:08:00.460 --> 04:08:01.660]   from the bottom up to do this.
[04:08:01.660 --> 04:08:03.380]   There's this recent book, "The NVIDIA Way"
[04:08:03.380 --> 04:08:05.500]   by Tay Kim that details this
[04:08:05.500 --> 04:08:08.420]   and how they look for future opportunities
[04:08:08.420 --> 04:08:12.340]   and ready their CUDA software libraries to make it
[04:08:12.340 --> 04:08:15.620]   so that new applications of high-performance computing
[04:08:15.620 --> 04:08:19.540]   can very rapidly be evolved on CUDA and NVIDIA chips.
[04:08:19.540 --> 04:08:22.340]   And that is entirely different than Google
[04:08:22.340 --> 04:08:24.020]   as a services business.
[04:08:24.020 --> 04:08:26.940]   - Yeah, I mean, NVIDIA, it should be said
[04:08:26.940 --> 04:08:28.180]   is a truly special company.
[04:08:28.180 --> 04:08:31.140]   Like, I mean, they, the whole, the culture, everything,
[04:08:31.140 --> 04:08:32.900]   they're really optimized for that kind of thing.
[04:08:32.900 --> 04:08:34.380]   Speaking of which, is there somebody
[04:08:34.380 --> 04:08:39.380]   that can even challenge NVIDIA hardware-wise, Intel, AMD?
[04:08:39.380 --> 04:08:41.020]   - I really don't think so.
[04:08:41.020 --> 04:08:43.620]   We went through a like a very long process
[04:08:43.620 --> 04:08:46.740]   of working with AMD on training
[04:08:46.740 --> 04:08:48.060]   on their GPUs, inference and stuff.
[04:08:48.060 --> 04:08:49.300]   And they're decent.
[04:08:49.300 --> 04:08:52.540]   Their hardware is better in many ways than NVIDIA's.
[04:08:52.540 --> 04:08:54.260]   The problem is their software is really bad.
[04:08:54.260 --> 04:08:56.220]   And I think they're getting better, right?
[04:08:56.220 --> 04:08:57.140]   They're getting better faster,
[04:08:57.140 --> 04:08:59.100]   but they're just, the gulf is so large.
[04:08:59.100 --> 04:09:02.420]   And like, they don't spend enough resources on it
[04:09:02.420 --> 04:09:03.660]   or haven't historically, right?
[04:09:03.660 --> 04:09:05.100]   Maybe they're changing their tune now,
[04:09:05.100 --> 04:09:07.540]   but you know, for multiple months,
[04:09:07.540 --> 04:09:09.060]   we were submitting the most bugs, right?
[04:09:09.060 --> 04:09:11.140]   Like us, semi-analysis, right?
[04:09:11.140 --> 04:09:11.980]   Like, what the fuck?
[04:09:11.980 --> 04:09:14.220]   Why are we submitting the most bugs, right?
[04:09:14.220 --> 04:09:17.300]   'Cause they only cared about their like biggest customers.
[04:09:17.300 --> 04:09:18.940]   And so they'd ship them a private image,
[04:09:18.940 --> 04:09:19.780]   blah, blah, blah.
[04:09:19.780 --> 04:09:23.220]   And it's like, okay, but like, I am just using PyTorch
[04:09:23.220 --> 04:09:25.740]   and I wanna use the publicly available libraries.
[04:09:25.740 --> 04:09:26.700]   You don't care about that, right?
[04:09:26.700 --> 04:09:28.660]   So they're getting better.
[04:09:28.660 --> 04:09:30.380]   But like, I think AMD is not possible.
[04:09:30.380 --> 04:09:32.660]   Intel's obviously in dire straits right now
[04:09:32.660 --> 04:09:34.900]   and needs to be saved somehow.
[04:09:34.900 --> 04:09:36.620]   Very important for national security,
[04:09:36.620 --> 04:09:39.020]   for American technology dominance.
[04:09:39.020 --> 04:09:40.100]   - Can you explain the obviously?
[04:09:40.100 --> 04:09:41.740]   So why are they in dire straits?
[04:09:41.740 --> 04:09:45.540]   - Going back to earlier, only three companies can R&D, right?
[04:09:45.540 --> 04:09:49.180]   Taiwan, Shenzhou, Samsung, Pyongyang,
[04:09:49.180 --> 04:09:50.980]   and then Intel Hillsboro.
[04:09:50.980 --> 04:09:53.340]   Samsung's doing horribly, Intel's doing horribly.
[04:09:53.340 --> 04:09:55.100]   We could be in a world where there's only one company
[04:09:55.100 --> 04:09:56.020]   that can do R&D.
[04:09:56.020 --> 04:09:57.940]   And that one company already manufactures most of the chips.
[04:09:57.940 --> 04:09:59.180]   They've been gaining market share anyways.
[04:09:59.180 --> 04:10:01.500]   But like, that's a critical thing, right?
[04:10:01.500 --> 04:10:02.700]   So what happens to Taiwan
[04:10:02.700 --> 04:10:04.340]   means the rest of the world's semiconductor industry
[04:10:04.340 --> 04:10:06.900]   and therefore tech relies on Taiwan, right?
[04:10:06.900 --> 04:10:09.180]   And that's obviously precarious.
[04:10:09.180 --> 04:10:10.740]   As far as like Intel,
[04:10:10.740 --> 04:10:13.220]   they've been slowly steadily declining.
[04:10:13.220 --> 04:10:15.780]   They were on top of servers and PCs,
[04:10:15.780 --> 04:10:17.860]   but now Apple's done the M1
[04:10:17.860 --> 04:10:19.460]   and Nvidia's releasing a PC chip
[04:10:19.460 --> 04:10:21.140]   and Qualcomm's releasing a PC chip.
[04:10:21.140 --> 04:10:23.300]   And in servers, hyperscalers are all making
[04:10:23.300 --> 04:10:25.620]   their own ARM-based server chips.
[04:10:25.620 --> 04:10:29.180]   And Intel has no AI silicon like wins, right?
[04:10:29.180 --> 04:10:31.300]   They have very small wins.
[04:10:31.300 --> 04:10:32.780]   And they never got into mobile
[04:10:32.780 --> 04:10:34.060]   because they said no to the iPhone.
[04:10:34.060 --> 04:10:35.700]   And like, all these things have compounded
[04:10:35.700 --> 04:10:37.860]   and they've lost their process technology leadership, right?
[04:10:37.860 --> 04:10:38.860]   They were ahead for 20 years
[04:10:38.860 --> 04:10:41.420]   and now they're behind by at least a couple of years, right?
[04:10:41.420 --> 04:10:42.580]   And they're trying to catch back up
[04:10:42.580 --> 04:10:46.100]   and we'll see if like their 18A, 14A strategy works out
[04:10:46.100 --> 04:10:48.380]   where they try and leapfrog TSMC.
[04:10:48.380 --> 04:10:50.220]   But like, and Intel is just like
[04:10:50.220 --> 04:10:51.740]   losing tons of money anyways, right?
[04:10:51.740 --> 04:10:53.060]   And they just fired their CEO,
[04:10:53.060 --> 04:10:54.860]   even though the CEO was the only person
[04:10:54.860 --> 04:10:56.540]   who understood the company well, right?
[04:10:56.540 --> 04:10:57.380]   We'll see.
[04:10:57.380 --> 04:10:58.220]   He was not the best,
[04:10:58.220 --> 04:11:01.340]   but he was pretty good relatively, technical guy.
[04:11:01.340 --> 04:11:03.100]   - Where does Intel make most of its money?
[04:11:03.100 --> 04:11:04.060]   The CPUs still, right?
[04:11:04.060 --> 04:11:05.500]   - PCs and data center CPUs, yeah.
[04:11:05.500 --> 04:11:07.340]   But data center CPUs are all going cloud
[04:11:07.340 --> 04:11:08.740]   and Amazon, Microsoft, Google
[04:11:08.740 --> 04:11:11.420]   are making ARM-based CPUs.
[04:11:11.420 --> 04:11:14.980]   And then PC side, AMD's gained market share,
[04:11:14.980 --> 04:11:16.100]   Nvidia's launching a chip.
[04:11:16.100 --> 04:11:17.300]   That's not gonna be a success, right?
[04:11:17.300 --> 04:11:19.220]   MediaTek, Qualcomm have relaunched chips.
[04:11:19.220 --> 04:11:20.540]   Apple's doing well, right?
[04:11:20.540 --> 04:11:23.340]   Like, they could get squeezed a little bit in PC.
[04:11:23.340 --> 04:11:24.740]   Although PC generally, I imagine,
[04:11:24.740 --> 04:11:27.100]   will just stick Intel mostly for Windows side.
[04:11:27.100 --> 04:11:29.140]   - Let's talk about the broad AI race.
[04:11:29.140 --> 04:11:30.940]   Who do you think wins?
[04:11:30.940 --> 04:11:32.060]   We talked about Google.
[04:11:32.060 --> 04:11:34.900]   - The leader, the default leader has been Google
[04:11:34.900 --> 04:11:37.500]   because of their infrastructure advantage.
[04:11:37.500 --> 04:11:40.740]   - Well, like, in the news, OpenAI is the leader.
[04:11:40.740 --> 04:11:42.580]   - They're the leading in the narrative.
[04:11:42.580 --> 04:11:43.420]   - They have the best model.
[04:11:43.420 --> 04:11:45.100]   - They have the best model that people can use
[04:11:45.100 --> 04:11:46.580]   and they're experts.
[04:11:46.580 --> 04:11:48.500]   - And they have the most AI revenue.
[04:11:48.500 --> 04:11:51.020]   - Yeah, OpenAI is winning.
[04:11:51.020 --> 04:11:53.660]   - So, who's making money on AI right now?
[04:11:53.660 --> 04:11:54.980]   Is anyone making money?
[04:11:54.980 --> 04:11:56.460]   - So, accounting profit-wise,
[04:11:56.460 --> 04:11:57.820]   Microsoft is making money,
[04:11:57.820 --> 04:11:59.620]   but they're spending a lot of CapEx, right?
[04:11:59.620 --> 04:12:02.180]   You know, and that gets depreciated over years.
[04:12:02.180 --> 04:12:03.780]   Meta's making tons of money,
[04:12:03.780 --> 04:12:06.300]   but with recommendation systems, which is AI,
[04:12:06.300 --> 04:12:07.340]   but not with Llama, right?
[04:12:07.340 --> 04:12:09.180]   Llama's losing money for sure, right?
[04:12:09.180 --> 04:12:12.980]   I think Anthropic and OpenAI are obviously not making money
[04:12:12.980 --> 04:12:15.140]   'cause otherwise they wouldn't be raising money, right?
[04:12:15.140 --> 04:12:17.740]   They'd have to raise money to build more, right?
[04:12:17.740 --> 04:12:19.860]   Although, theoretically, they are making money, right?
[04:12:19.860 --> 04:12:21.860]   Like, you know, you spent a few hundred million dollars
[04:12:21.860 --> 04:12:24.300]   on GPT-4 and it's doing billions in revenue.
[04:12:24.300 --> 04:12:26.140]   So, like, obviously it's like making money.
[04:12:26.140 --> 04:12:27.580]   Although they had to continue to research
[04:12:27.580 --> 04:12:29.460]   to get the compute efficiency wins, right?
[04:12:29.460 --> 04:12:32.020]   And move down the curve to like, you know,
[04:12:32.020 --> 04:12:35.300]   that 12, get that 1200X that has been achieved for GPT-3.
[04:12:35.300 --> 04:12:38.460]   You know, maybe we're only at like a couple hundred X now,
[04:12:38.460 --> 04:12:40.660]   but, you know, with GPT-4 Turbo and 4.0,
[04:12:40.660 --> 04:12:42.540]   and there'll be another one probably cheaper
[04:12:42.540 --> 04:12:45.420]   than GPT-4.0 even that comes out at some point.
[04:12:45.420 --> 04:12:48.100]   - And that research costs a lot of money.
[04:12:48.100 --> 04:12:49.100]   - Yep, exactly.
[04:12:49.100 --> 04:12:50.940]   - That's the thing that I guess is not talked about
[04:12:50.940 --> 04:12:54.100]   with the cost, that when you're referring
[04:12:54.100 --> 04:12:57.740]   to the cost of the model, it's not just the training
[04:12:57.740 --> 04:13:00.260]   or the test runs, it's the actual research,
[04:13:00.260 --> 04:13:01.900]   the manpower.
[04:13:01.900 --> 04:13:03.300]   - Yeah, to do things like reasoning, right?
[04:13:03.300 --> 04:13:04.740]   Now that that exists, they're gonna scale it,
[04:13:04.740 --> 04:13:05.580]   they're gonna do a lot of research.
[04:13:05.580 --> 04:13:08.980]   So, I think the, you know, people focus
[04:13:08.980 --> 04:13:10.740]   on the payback question, but it's really easy
[04:13:10.740 --> 04:13:13.020]   to like, just be like, well, like, you know,
[04:13:13.020 --> 04:13:15.900]   GDP is humans and industrial capital, right?
[04:13:15.900 --> 04:13:18.060]   And if you can make intelligence cheap,
[04:13:18.060 --> 04:13:20.020]   then you can grow a lot, right?
[04:13:20.020 --> 04:13:21.980]   That's the sort of dumb way to explain it.
[04:13:21.980 --> 04:13:23.340]   But that's sort of what basically
[04:13:23.340 --> 04:13:25.460]   the investment thesis is.
[04:13:25.460 --> 04:13:27.980]   I think only NVIDIA is actually making tons of money
[04:13:27.980 --> 04:13:29.900]   and other hardware vendors.
[04:13:29.900 --> 04:13:32.460]   The hyperscalers are all on paper making money,
[04:13:32.460 --> 04:13:34.820]   but in reality, they're like, spending a lot more
[04:13:34.820 --> 04:13:36.940]   on purchasing the GPUs, which you don't know
[04:13:36.940 --> 04:13:38.300]   if they're still gonna make this much money
[04:13:38.300 --> 04:13:40.660]   on each GPU in two years, right?
[04:13:40.660 --> 04:13:44.100]   You don't know if, you know, all of a sudden,
[04:13:44.100 --> 04:13:47.260]   OpenAI goes kapoof and now Microsoft has like,
[04:13:47.260 --> 04:13:49.060]   hundreds of thousands of GPUs they were renting
[04:13:49.060 --> 04:13:51.860]   to OpenAI that they paid for themselves
[04:13:51.860 --> 04:13:54.020]   with their, you know, investment in them.
[04:13:54.020 --> 04:13:55.820]   You know, that no longer have a customer, right?
[04:13:55.820 --> 04:13:57.460]   Like, this is always a possibility.
[04:13:57.460 --> 04:13:58.860]   I don't believe that, right?
[04:13:58.860 --> 04:14:00.780]   I think, you know, OpenAI will keep raising money.
[04:14:00.780 --> 04:14:02.820]   I think others will keep raising money
[04:14:02.820 --> 04:14:05.060]   because the investments, the returns from it
[04:14:05.060 --> 04:14:07.940]   are gonna be eventually huge once we have AGI.
[04:14:07.940 --> 04:14:10.580]   - So do you think multiple companies will get,
[04:14:10.580 --> 04:14:11.420]   let's assume--
[04:14:11.420 --> 04:14:12.740]   - I don't think it's winner take all.
[04:14:12.740 --> 04:14:16.540]   - Okay, so it's not, let's not call it AGI, whatever.
[04:14:16.540 --> 04:14:17.620]   It's like a single day.
[04:14:17.620 --> 04:14:19.060]   It's a gradual thing. - Powerful AI.
[04:14:19.060 --> 04:14:20.420]   Super powerful AI.
[04:14:20.420 --> 04:14:23.620]   - But it's a gradually increasing set of features
[04:14:23.620 --> 04:14:25.540]   that are useful and make a lot of money.
[04:14:25.540 --> 04:14:27.500]   - Rapidly increasing set of features.
[04:14:27.500 --> 04:14:29.540]   - Rapidly increasing set of features.
[04:14:29.540 --> 04:14:33.380]   So you're saying a lot of companies will be,
[04:14:33.380 --> 04:14:38.380]   it just seems absurd that all of these companies
[04:14:38.380 --> 04:14:40.740]   are building gigantic data centers.
[04:14:40.740 --> 04:14:42.820]   - There are companies that will benefit from AI,
[04:14:42.820 --> 04:14:44.820]   but not because they trained the best model.
[04:14:44.820 --> 04:14:47.580]   Like, Meta has so many avenues to benefit from AI
[04:14:47.580 --> 04:14:48.660]   in all of their services.
[04:14:48.660 --> 04:14:51.580]   People are there, people spend time on Meta's platforms,
[04:14:51.580 --> 04:14:54.220]   and it's a way to make more money per user per hour.
[04:14:54.220 --> 04:14:59.220]   - Yeah, it seems like Google X/XAI/Tesla,
[04:14:59.220 --> 04:15:03.100]   important to say, and then Meta will benefit
[04:15:03.100 --> 04:15:06.420]   not directly from the AI, like the LLMs,
[04:15:06.420 --> 04:15:09.860]   but from the intelligence,
[04:15:09.860 --> 04:15:11.620]   like the additional boost of intelligence
[04:15:11.620 --> 04:15:13.160]   to the products they already sell.
[04:15:13.160 --> 04:15:15.500]   So whether that's the recommendation system
[04:15:15.500 --> 04:15:19.300]   or for Elon, who's been talking about Optimus, the robot,
[04:15:19.300 --> 04:15:22.340]   potentially the intelligence of the robot.
[04:15:22.340 --> 04:15:24.780]   And then you have personalized robots in the home,
[04:15:24.780 --> 04:15:25.740]   that kind of thing.
[04:15:25.740 --> 04:15:30.740]   He thinks it's a 10 plus trillion dollar business, which--
[04:15:30.740 --> 04:15:34.340]   - At some point, maybe, not soon,
[04:15:34.340 --> 04:15:36.260]   but who knows what robotics will be used for.
[04:15:36.260 --> 04:15:37.620]   - Let's do a TAM analysis, right?
[04:15:37.620 --> 04:15:40.980]   Eight billion humans, and let's get eight billion robots,
[04:15:40.980 --> 04:15:43.380]   right, and let's pay 'em the average salary,
[04:15:43.380 --> 04:15:45.180]   and yeah, there we go, 10 trillion.
[04:15:45.180 --> 04:15:46.420]   More than 10 trillion.
[04:15:46.420 --> 04:15:49.500]   - Yeah, I mean, if there's robots everywhere,
[04:15:49.500 --> 04:15:52.660]   why does it have to be just eight billion robots?
[04:15:52.660 --> 04:15:54.060]   - Yeah, yeah, of course, of course.
[04:15:54.060 --> 04:15:57.140]   I'm gonna have one robot, you're gonna have 20.
[04:15:57.140 --> 04:15:59.740]   - Yeah, I mean, I see a use case for that.
[04:15:59.740 --> 04:16:01.820]   So yeah, so I guess the benefit
[04:16:01.820 --> 04:16:03.100]   would be in the products they sell,
[04:16:03.100 --> 04:16:06.500]   which is why OpenAI is in a trickier position, 'cause they--
[04:16:06.500 --> 04:16:08.900]   - All of the value of OpenAI right now as a brand
[04:16:08.900 --> 04:16:12.100]   is in ChatGPT, and there is actually not that,
[04:16:12.100 --> 04:16:14.740]   for most users, there's not that much of a reason
[04:16:14.740 --> 04:16:16.620]   that they need OpenAI to be spending
[04:16:16.620 --> 04:16:19.460]   billions and billions of dollars on the next best model,
[04:16:19.460 --> 04:16:23.020]   when they can just license Llama 5 for it to be way cheaper.
[04:16:23.020 --> 04:16:25.260]   So that's kind of like, ChatGPT
[04:16:25.260 --> 04:16:27.940]   is an extremely valuable entity to them,
[04:16:27.940 --> 04:16:31.460]   but they could make more money just off that than--
[04:16:31.460 --> 04:16:33.280]   - The chat application is clearly like,
[04:16:33.280 --> 04:16:35.220]   does not have tons of room to continue, right?
[04:16:35.220 --> 04:16:36.460]   Like the standard chat, right,
[04:16:36.460 --> 04:16:37.300]   where you're just using it
[04:16:37.300 --> 04:16:39.100]   for random questions and stuff, right?
[04:16:39.100 --> 04:16:40.420]   The cost continues to collapse,
[04:16:40.420 --> 04:16:42.100]   v3 is the latest one-- - It'll go down to ads.
[04:16:42.100 --> 04:16:44.940]   - Biggest, but it's gonna get supported by ads, right?
[04:16:44.940 --> 04:16:48.300]   Like, you know, Llama, Meta already serves 405B,
[04:16:48.300 --> 04:16:50.260]   probably loses the money, but at some point,
[04:16:50.260 --> 04:16:52.060]   you know, they're going to get,
[04:16:52.060 --> 04:16:53.340]   the models are gonna get so cheap
[04:16:53.340 --> 04:16:55.220]   that they can just serve them for free
[04:16:55.220 --> 04:16:56.220]   with ads supported, right?
[04:16:56.220 --> 04:16:57.820]   And that's what Google is gonna be able to do,
[04:16:57.820 --> 04:16:59.460]   and that's obviously, they've got a bigger reach, right?
[04:16:59.460 --> 04:17:01.900]   So chat is not gonna be the only use case,
[04:17:01.900 --> 04:17:05.660]   it's like these reasoning, code, agents, computer use,
[04:17:05.660 --> 04:17:07.960]   all this stuff is where OpenAI has to actually go
[04:17:07.960 --> 04:17:10.300]   to make money in the future, otherwise they're kaputs.
[04:17:10.300 --> 04:17:15.060]   - But X, Google, and Meta have these other products,
[04:17:15.060 --> 04:17:20.060]   so isn't it likely that OpenAI and Anthropic
[04:17:20.060 --> 04:17:22.140]   disappear eventually?
[04:17:22.140 --> 04:17:24.340]   - Unless they're so good at models, which they are.
[04:17:24.340 --> 04:17:25.860]   - But it's such a cutting edge, I mean--
[04:17:25.860 --> 04:17:28.500]   - It depends on where you think AI capabilities are going.
[04:17:28.500 --> 04:17:29.940]   - You have to keep winning.
[04:17:29.940 --> 04:17:30.780]   - Yes.
[04:17:30.780 --> 04:17:33.340]   - You have to keep winning, as you climb,
[04:17:33.340 --> 04:17:36.140]   even if the AI capabilities are going super rapidly,
[04:17:36.140 --> 04:17:38.740]   awesome, into the direction of AGI,
[04:17:38.740 --> 04:17:43.180]   like, there's still a boost for X in terms of data,
[04:17:43.180 --> 04:17:46.020]   Google in terms of data, Meta in terms of data,
[04:17:46.020 --> 04:17:47.740]   in terms of other products, and the money,
[04:17:47.740 --> 04:17:49.900]   and there's just huge amounts of money.
[04:17:49.900 --> 04:17:52.420]   - But the whole idea is, human data is kinda tapped out,
[04:17:52.420 --> 04:17:54.700]   we don't care, we all care about self-play,
[04:17:54.700 --> 04:17:55.740]   verifiable tasks.
[04:17:55.740 --> 04:17:56.580]   - Yeah, so self-play--
[04:17:56.580 --> 04:17:57.400]   - If you think about AWS--
[04:17:57.400 --> 04:17:58.240]   - Which is an R and D problem, yeah.
[04:17:58.240 --> 04:17:59.740]   - I think AWS does not make a lot of money
[04:17:59.740 --> 04:18:03.340]   on each individual machine, and the same can be said
[04:18:03.340 --> 04:18:05.660]   for the most powerful AI platform,
[04:18:05.660 --> 04:18:08.260]   which is, even though the calls to the API are so cheap,
[04:18:08.260 --> 04:18:10.340]   there's still a lot of money to be made
[04:18:10.340 --> 04:18:12.700]   by owning that platform, and there's a lot
[04:18:12.700 --> 04:18:15.540]   of discussions as it's the next compute layer.
[04:18:15.540 --> 04:18:17.660]   - You have to believe that, and yeah,
[04:18:17.660 --> 04:18:19.340]   there's a lot of discussions that tokens,
[04:18:19.340 --> 04:18:22.740]   and tokenomics, and LLM APIs are the next compute layer,
[04:18:22.740 --> 04:18:24.300]   or the next paradigm for the economy,
[04:18:24.300 --> 04:18:26.020]   kind of like energy and oil was,
[04:18:26.020 --> 04:18:28.060]   but there's also, you have to sort of believe
[04:18:28.060 --> 04:18:32.800]   that APIs and chat are not where AI is stuck, right?
[04:18:32.800 --> 04:18:34.700]   It is actually just tasks, and agents,
[04:18:34.700 --> 04:18:36.380]   and robotics, and computer use,
[04:18:36.380 --> 04:18:39.220]   and those are the areas where all the value
[04:18:39.220 --> 04:18:43.060]   will be delivered, not API, not chat application, right?
[04:18:43.060 --> 04:18:44.820]   - Is it possible you have, I mean,
[04:18:44.820 --> 04:18:46.380]   it all just becomes a commodity,
[04:18:46.380 --> 04:18:51.180]   and you have the very thin wrapper,
[04:18:51.180 --> 04:18:53.720]   like perplexity, just joking.
[04:18:53.720 --> 04:18:57.060]   - There are a lot of wrappers making a lot of money.
[04:18:57.060 --> 04:18:58.860]   - Yeah, so, but do you think it's possible
[04:18:58.860 --> 04:19:00.700]   that people would just even forget
[04:19:00.700 --> 04:19:03.220]   what OpenAI and Anthropic is, and just,
[04:19:03.220 --> 04:19:05.460]   'cause there'll be wrappers around the API,
[04:19:05.460 --> 04:19:06.540]   and it just dynamically--
[04:19:06.540 --> 04:19:08.820]   - If model progress is not rapid, yeah,
[04:19:08.820 --> 04:19:10.300]   it's becoming a commodity, right?
[04:19:10.300 --> 04:19:13.820]   Deep Seek v3 shows this, but also the GPT-3 chart earlier,
[04:19:13.820 --> 04:19:14.780]   Kurt chart showed this, right?
[04:19:14.780 --> 04:19:17.900]   Lama 3B is 1,200x cheaper than GPT-3.
[04:19:17.900 --> 04:19:20.460]   Any GPT-3, like anyone whose business model
[04:19:20.460 --> 04:19:22.740]   was GPT-3 level capabilities is dead.
[04:19:22.740 --> 04:19:25.700]   Anyone whose business model's GPT-4 level capabilities
[04:19:25.700 --> 04:19:26.540]   is dead, right?
[04:19:26.540 --> 04:19:28.660]   - It is a common saying that the best businesses
[04:19:28.660 --> 04:19:30.780]   being made now are ones that are predicated
[04:19:30.780 --> 04:19:32.060]   on models getting better.
[04:19:32.060 --> 04:19:34.060]   - Right, which would be like wrappers,
[04:19:34.060 --> 04:19:37.300]   thing that is riding the wave of the models.
[04:19:37.300 --> 04:19:40.100]   - The short-term, the company that could make the most money
[04:19:40.100 --> 04:19:42.820]   is the one that figures out what advertising
[04:19:42.820 --> 04:19:45.620]   targeting method works for language model generations.
[04:19:45.620 --> 04:19:49.060]   We have the meta ads, which are hyper-targeted in feed,
[04:19:49.060 --> 04:19:51.060]   not within specific pieces of content,
[04:19:51.060 --> 04:19:52.900]   and we have search ads that are used by Google,
[04:19:52.900 --> 04:19:54.940]   and Amazon has been rising a lot on search.
[04:19:54.940 --> 04:19:57.860]   But within a piece, within a return from ChatGPT,
[04:19:57.860 --> 04:20:01.900]   it is not clear how you get a high-quality placed ad
[04:20:01.900 --> 04:20:03.140]   within the output.
[04:20:03.140 --> 04:20:06.180]   And if you can do that with model costs coming down,
[04:20:06.180 --> 04:20:09.380]   you can just get super high revenue.
[04:20:09.380 --> 04:20:10.900]   Like, that revenue is totally untapped,
[04:20:10.900 --> 04:20:12.660]   and it's not clear technically how it is done.
[04:20:12.660 --> 04:20:16.620]   - Yeah, that is, I mean, sort of the AdSense innovation
[04:20:16.620 --> 04:20:18.300]   that Google did.
[04:20:18.300 --> 04:20:22.580]   The one day you'll have, in GPT output, an ad,
[04:20:22.580 --> 04:20:24.380]   and that's gonna make, like, billions, if not--
[04:20:24.380 --> 04:20:25.580]   - And it could be very subtle.
[04:20:25.580 --> 04:20:26.980]   It could be in conversation.
[04:20:26.980 --> 04:20:28.060]   Like, we have voice mode now.
[04:20:28.060 --> 04:20:30.140]   It could be some way of making it
[04:20:30.140 --> 04:20:32.500]   so the voice introduces certain things.
[04:20:32.500 --> 04:20:33.660]   It's much harder to measure,
[04:20:33.660 --> 04:20:35.580]   and it takes imagination, but yeah.
[04:20:35.580 --> 04:20:39.740]   - And it wouldn't be so, it wouldn't come off shady
[04:20:39.740 --> 04:20:41.620]   so that you would receive public blowback,
[04:20:41.620 --> 04:20:42.460]   that kind of thing.
[04:20:42.460 --> 04:20:44.060]   So, you have to do it loud enough to where it's clear
[04:20:44.060 --> 04:20:46.380]   it's an ad and balance all of that.
[04:20:46.380 --> 04:20:48.620]   So, that's the open question they're trying to solve.
[04:20:48.620 --> 04:20:51.420]   Anthropic and OpenAI, they need to--
[04:20:51.420 --> 04:20:52.260]   - They might not say that they're trying--
[04:20:52.260 --> 04:20:53.380]   - I don't think they care about that at all.
[04:20:53.380 --> 04:20:54.860]   - They don't care about it right now.
[04:20:54.860 --> 04:20:55.700]   I think it's places like--
[04:20:55.700 --> 04:20:56.540]   - I think they're purely--
[04:20:56.540 --> 04:20:59.060]   - Perplexity are experimenting on that more.
[04:20:59.060 --> 04:21:01.060]   - Oh, interesting, yeah, for sure.
[04:21:01.060 --> 04:21:04.300]   - Like, Perplexity, Google, Meta care about this.
[04:21:04.300 --> 04:21:07.880]   I think OpenAI and Anthropic are purely laser focused on--
[04:21:07.880 --> 04:21:08.720]   - AGI.
[04:21:08.720 --> 04:21:12.460]   - Yeah, agents and AGI, and if I build AGI,
[04:21:12.460 --> 04:21:14.540]   I can make tons of money, right?
[04:21:14.540 --> 04:21:15.940]   Or I can pay for everything, right?
[04:21:15.940 --> 04:21:18.940]   And this is, it's just predicated,
[04:21:18.940 --> 04:21:20.620]   like back on the export control thing, right?
[04:21:20.620 --> 04:21:23.660]   If you think AGI is five, 10 years away or less, right?
[04:21:23.660 --> 04:21:26.020]   These labs think it's two, three years away.
[04:21:26.020 --> 04:21:28.740]   Obviously, your actions are,
[04:21:28.740 --> 04:21:30.980]   if you assume they're rational actors,
[04:21:30.980 --> 04:21:32.140]   which they are mostly,
[04:21:33.620 --> 04:21:35.620]   what you do in a two year AGI
[04:21:35.620 --> 04:21:37.180]   versus five year versus 10 years,
[04:21:37.180 --> 04:21:39.040]   very, very, very different, right?
[04:21:39.040 --> 04:21:42.320]   - Do you think agents are promising?
[04:21:42.320 --> 04:21:43.520]   We'll have to talk about this.
[04:21:43.520 --> 04:21:44.360]   This was,
[04:21:44.360 --> 04:21:48.020]   this is like the excitement of the year
[04:21:48.020 --> 04:21:49.220]   that agents are gonna,
[04:21:49.220 --> 04:21:53.180]   this is the generic hype term
[04:21:53.180 --> 04:21:54.860]   that a lot of business folks are using.
[04:21:54.860 --> 04:21:57.340]   AI agents are gonna revolutionize everything.
[04:21:57.340 --> 04:22:01.000]   - Okay, so mostly the term agent is obviously overblown.
[04:22:01.000 --> 04:22:02.900]   We've talked a lot about reinforcement learning
[04:22:02.900 --> 04:22:05.900]   as a way to train for verifiable outcomes.
[04:22:05.900 --> 04:22:08.220]   Agents should mean something that is open-ended
[04:22:08.220 --> 04:22:10.540]   and is solving a task independently on its own
[04:22:10.540 --> 04:22:12.660]   and able to adapt to uncertainty.
[04:22:12.660 --> 04:22:14.900]   There's a lot of the term agent applied to things
[04:22:14.900 --> 04:22:15.900]   like Apple Intelligence,
[04:22:15.900 --> 04:22:19.820]   which we still don't have after the last WWDC,
[04:22:19.820 --> 04:22:21.960]   which is orchestrating between apps.
[04:22:21.960 --> 04:22:23.820]   And that type of tool use thing
[04:22:23.820 --> 04:22:26.260]   is something that language models can do really well.
[04:22:26.260 --> 04:22:29.300]   Apple Intelligence, I suspect, will come eventually.
[04:22:29.300 --> 04:22:30.460]   It's a closed domain.
[04:22:30.460 --> 04:22:33.100]   It's your messages app integrating with your photos,
[04:22:33.100 --> 04:22:34.620]   with AI in the background.
[04:22:34.620 --> 04:22:35.460]   That will work.
[04:22:35.460 --> 04:22:37.780]   That has been described as an agent
[04:22:37.780 --> 04:22:40.900]   by a lot of software companies to get into the narrative.
[04:22:40.900 --> 04:22:45.900]   The question is what ways can we get language models
[04:22:45.900 --> 04:22:48.260]   to generalize to new domains
[04:22:48.260 --> 04:22:51.100]   and solve their own problems in real time?
[04:22:51.100 --> 04:22:52.940]   Maybe some tiny amount of training
[04:22:52.940 --> 04:22:54.980]   when they are doing this with fine-tuning themselves
[04:22:54.980 --> 04:22:56.140]   or in-context learning,
[04:22:56.140 --> 04:22:59.060]   which is the idea of storing information in a prompt
[04:22:59.060 --> 04:23:01.860]   and you can use learning algorithms to update that.
[04:23:01.860 --> 04:23:04.020]   And whether or not you believe
[04:23:04.020 --> 04:23:06.380]   that that is gonna actually generalize to things
[04:23:06.380 --> 04:23:11.380]   like me saying, "Book my trip to go to Austin in two days."
[04:23:11.380 --> 04:23:15.700]   I have X, Y, Z constraints in actually trusting it.
[04:23:15.700 --> 04:23:19.420]   I think there's a HCI problem, coming back for information.
[04:23:19.420 --> 04:23:21.060]   - Well, what's your prediction there?
[04:23:21.060 --> 04:23:24.620]   'Cause my gut says we're very far away from that.
[04:23:24.620 --> 04:23:27.180]   - I think opening eyes statement,
[04:23:27.180 --> 04:23:29.540]   I don't know if you've seen the five levels, right?
[04:23:29.540 --> 04:23:33.180]   Where it's chat is level one, reasoning is level two,
[04:23:33.180 --> 04:23:34.500]   and then agents is level three.
[04:23:34.500 --> 04:23:35.740]   And I think there's a couple more levels,
[04:23:35.740 --> 04:23:37.260]   but it's important to note, right?
[04:23:37.260 --> 04:23:40.220]   We were in chat for a couple of years, right?
[04:23:40.220 --> 04:23:42.780]   We just theoretically got to reasoning.
[04:23:42.780 --> 04:23:44.540]   We'll be here for a year or two, right?
[04:23:44.540 --> 04:23:46.100]   And then agents, but at the same time,
[04:23:46.100 --> 04:23:49.780]   like people can train like approximate capabilities
[04:23:49.780 --> 04:23:50.620]   of the next level.
[04:23:50.620 --> 04:23:53.260]   But the agents are doing things autonomously,
[04:23:53.260 --> 04:23:54.940]   doing things for minutes at a time,
[04:23:54.940 --> 04:23:57.340]   hours at a time, et cetera, right?
[04:23:57.340 --> 04:23:59.460]   Reasoning is doing things
[04:23:59.460 --> 04:24:01.940]   for tens of seconds at a time, right?
[04:24:01.940 --> 04:24:03.020]   And then coming back with an output
[04:24:03.020 --> 04:24:06.140]   that I still need to verify and use and try to check out.
[04:24:06.140 --> 04:24:08.460]   So, and the biggest problem is of course like,
[04:24:08.460 --> 04:24:10.940]   it's the same thing with manufacturing, right?
[04:24:10.940 --> 04:24:12.380]   Like there's the whole six sigma thing, right?
[04:24:12.380 --> 04:24:14.100]   Like, how many nines do you get?
[04:24:14.100 --> 04:24:16.220]   And then you compound the nines onto each other.
[04:24:16.220 --> 04:24:18.060]   And it's like, if you multiply,
[04:24:18.060 --> 04:24:20.580]   by the number of steps that are six sigma,
[04:24:20.580 --> 04:24:24.020]   you get to a yield or something, right?
[04:24:24.020 --> 04:24:25.620]   So like in semiconductor manufacturing,
[04:24:25.620 --> 04:24:27.180]   tens of thousands of steps,
[04:24:27.180 --> 04:24:28.740]   nine, nine, nine, nine, nine, nine, nine,
[04:24:28.740 --> 04:24:30.100]   is not enough, right?
[04:24:30.100 --> 04:24:31.900]   'Cause you multiply that by that many times,
[04:24:31.900 --> 04:24:33.820]   you actually end up with like 60% yield, right?
[04:24:33.820 --> 04:24:34.660]   - Yeah, or zero.
[04:24:34.660 --> 04:24:36.540]   - Really low yield, yeah, or zero.
[04:24:36.540 --> 04:24:37.820]   And this is the same thing with agents, right?
[04:24:37.820 --> 04:24:40.620]   Like chaining tasks together each time.
[04:24:40.620 --> 04:24:43.140]   LLMs, even the best LLMs
[04:24:43.140 --> 04:24:45.820]   in particularly pretty good benchmarks,
[04:24:45.820 --> 04:24:48.100]   don't get a hundred percent, right?
[04:24:48.100 --> 04:24:49.180]   They get a little bit below that
[04:24:49.180 --> 04:24:50.740]   because there's a lot of noise.
[04:24:51.020 --> 04:24:54.580]   And so how do you get to enough nines, right?
[04:24:54.580 --> 04:24:56.420]   This is the same thing with self-driving.
[04:24:56.420 --> 04:24:57.500]   We can't have self-driving
[04:24:57.500 --> 04:24:59.740]   because without it being like super geo-fenced
[04:24:59.740 --> 04:25:01.220]   like Google's, right?
[04:25:01.220 --> 04:25:02.880]   And even then they have a bunch of tele-operators
[04:25:02.880 --> 04:25:04.340]   to make sure it doesn't get stuck, right?
[04:25:04.340 --> 04:25:07.260]   But you can't do that because it doesn't have enough nines.
[04:25:07.260 --> 04:25:10.700]   - And self-driving has quite a lot of structure
[04:25:10.700 --> 04:25:12.820]   because roads have rules.
[04:25:12.820 --> 04:25:15.540]   It's well-defined, there's regulation.
[04:25:15.540 --> 04:25:17.900]   When you're talking about computer use
[04:25:17.900 --> 04:25:19.820]   for the open web, for example,
[04:25:19.820 --> 04:25:21.900]   or the open operating system,
[04:25:21.900 --> 04:25:24.860]   it's a mess.
[04:25:24.860 --> 04:25:27.140]   So the possibility...
[04:25:27.140 --> 04:25:29.420]   I'm always skeptical of any system
[04:25:29.420 --> 04:25:34.420]   that is tasked with interacting with the human world,
[04:25:34.420 --> 04:25:36.340]   with the open, messy human world.
[04:25:36.340 --> 04:25:38.080]   - That's the thing, if we can't get intelligence
[04:25:38.080 --> 04:25:41.020]   that's enough to solve the human world on its own,
[04:25:41.020 --> 04:25:42.820]   we can create infrastructure,
[04:25:42.820 --> 04:25:45.140]   like the human operators for Waymo,
[04:25:45.140 --> 04:25:47.780]   over many years that enable certain workflows.
[04:25:47.780 --> 04:25:49.200]   - There's a company, I don't remember it,
[04:25:49.200 --> 04:25:51.140]   but it is, but that's literally their pitch is,
[04:25:51.140 --> 04:25:52.720]   yeah, we're just gonna be the human operator
[04:25:52.720 --> 04:25:53.620]   when agents fail.
[04:25:53.620 --> 04:25:55.220]   And you just call us and we fix it.
[04:25:55.220 --> 04:25:56.060]   - Yeah.
[04:25:56.060 --> 04:25:57.300]   - It's like an API call and it's hilarious.
[04:25:57.300 --> 04:25:58.820]   - There's gonna be tele-operation markets
[04:25:58.820 --> 04:25:59.900]   when we get human robots,
[04:25:59.900 --> 04:26:02.700]   which is there's gonna be somebody around the world
[04:26:02.700 --> 04:26:04.220]   that's happy to fix the fact
[04:26:04.220 --> 04:26:06.340]   that it can't finish loading my dishwasher
[04:26:06.340 --> 04:26:07.420]   when I'm unhappy with it,
[04:26:07.420 --> 04:26:10.620]   but that's just gonna be part of the Tesla service package.
[04:26:10.620 --> 04:26:13.940]   - I'm just imagining like an AI agent
[04:26:13.940 --> 04:26:15.580]   talking to another AI agent.
[04:26:15.580 --> 04:26:18.060]   One company has an AI agent that specializes
[04:26:18.060 --> 04:26:20.840]   in helping other AI agents.
[04:26:20.840 --> 04:26:23.280]   - But if you can make things that are good at one step,
[04:26:23.280 --> 04:26:25.160]   you can stack them together.
[04:26:25.160 --> 04:26:28.160]   So that's why I'm going, if it takes a long time,
[04:26:28.160 --> 04:26:30.240]   we're gonna build infrastructure that enables it.
[04:26:30.240 --> 04:26:31.560]   You see the operator launch.
[04:26:31.560 --> 04:26:33.540]   They have partnerships with certain websites,
[04:26:33.540 --> 04:26:37.160]   with DoorDash, with OpenTable, with things like this.
[04:26:37.160 --> 04:26:39.860]   Those partnerships are gonna let them climb really fast.
[04:26:39.860 --> 04:26:42.040]   Their model's gonna get really good at those things.
[04:26:42.040 --> 04:26:43.360]   It's gonna proof of concept.
[04:26:43.360 --> 04:26:44.640]   That might be a network effect
[04:26:44.640 --> 04:26:47.440]   where more companies wanna make it easier for AI.
[04:26:47.440 --> 04:26:48.500]   Some companies will be like,
[04:26:48.500 --> 04:26:51.540]   "No, let's put blockers in place."
[04:26:51.540 --> 04:26:53.640]   And this is the story of the internet we've seen.
[04:26:53.640 --> 04:26:55.680]   We see it now with training data for language models
[04:26:55.680 --> 04:26:58.360]   where companies are like, "No, you have to pay."
[04:26:58.360 --> 04:27:00.680]   Like business working it out.
[04:27:00.680 --> 04:27:03.520]   - That said, I think like airlines have a very,
[04:27:03.520 --> 04:27:05.380]   and hotels have high incentive
[04:27:05.380 --> 04:27:07.280]   to make their site work really well,
[04:27:07.280 --> 04:27:08.440]   and they usually don't.
[04:27:08.440 --> 04:27:11.500]   Like if you look at how many clicks it takes
[04:27:11.500 --> 04:27:14.700]   to order an airplane ticket, it's insane.
[04:27:14.700 --> 04:27:15.760]   - You actually can't call
[04:27:15.760 --> 04:27:17.680]   an American Airlines agent anymore.
[04:27:17.680 --> 04:27:19.960]   They don't have a phone number.
[04:27:19.960 --> 04:27:22.160]   - I mean, it's horrible on many.
[04:27:22.160 --> 04:27:24.480]   On the interface front and all,
[04:27:24.480 --> 04:27:26.400]   to imagine that agents will be able to deal
[04:27:26.400 --> 04:27:29.720]   with that website when I, as a human, struggle.
[04:27:29.720 --> 04:27:31.240]   Like I have an existential crisis
[04:27:31.240 --> 04:27:33.120]   every time I try to book an airplane ticket
[04:27:33.120 --> 04:27:37.720]   that I don't, I think it's gonna be extremely difficult
[04:27:37.720 --> 04:27:40.360]   to build an AI agent that's robust in that way.
[04:27:40.360 --> 04:27:41.200]   - But think about it.
[04:27:41.200 --> 04:27:43.320]   Like United has accepted the Starlink term,
[04:27:43.320 --> 04:27:45.560]   which is they have to provide Starlink for free
[04:27:45.560 --> 04:27:47.160]   and the users are going to love it.
[04:27:47.160 --> 04:27:49.040]   What if one airline is like,
[04:27:49.040 --> 04:27:50.200]   we're gonna take a year
[04:27:50.200 --> 04:27:52.560]   and we're gonna make our website have white text
[04:27:52.560 --> 04:27:54.760]   that works perfectly for the AIs.
[04:27:54.760 --> 04:27:57.460]   Every time anyone asks about an AI flight,
[04:27:57.460 --> 04:27:59.480]   they buy whatever airline it is.
[04:27:59.480 --> 04:28:01.600]   - Or like, they just like, here's an API
[04:28:01.600 --> 04:28:03.520]   and it's only exposed to AI agents.
[04:28:03.520 --> 04:28:06.280]   And if anyone queries it, the price is 10% higher
[04:28:06.280 --> 04:28:09.300]   and for any flight, but we'll let you see any of our flights
[04:28:09.300 --> 04:28:10.560]   and you can just book any of them.
[04:28:10.560 --> 04:28:11.400]   Here you go, agent.
[04:28:11.400 --> 04:28:13.560]   And then it's like, oh, and I made 10% higher price.
[04:28:13.560 --> 04:28:14.600]   Awesome.
[04:28:14.600 --> 04:28:16.280]   And like, am I willing to say that for like,
[04:28:16.280 --> 04:28:17.880]   hey, book me a flight to see Lex, right?
[04:28:17.880 --> 04:28:19.640]   And it's like, yeah, whatever.
[04:28:19.640 --> 04:28:23.080]   I think, you know, computers and real world
[04:28:23.080 --> 04:28:25.660]   and the open world are really, really messy.
[04:28:25.660 --> 04:28:30.160]   But if you start defining the problem in narrow regions,
[04:28:30.160 --> 04:28:31.520]   people are gonna be able to create
[04:28:31.520 --> 04:28:33.400]   very, very productive things
[04:28:33.400 --> 04:28:37.640]   and ratchet down cost massively, right?
[04:28:37.640 --> 04:28:41.640]   Like now crazy things like, you know, robotics in the home,
[04:28:41.640 --> 04:28:44.000]   you know, those are gonna be a lot harder to do
[04:28:44.000 --> 04:28:45.440]   just like self-driving, right?
[04:28:45.440 --> 04:28:47.880]   Because there's just a billion different failure modes,
[04:28:47.880 --> 04:28:48.720]   right?
[04:28:48.720 --> 04:28:51.300]   But like agents that can like navigate
[04:28:51.300 --> 04:28:54.000]   a certain set of websites and do certain sets of tasks
[04:28:54.000 --> 04:28:58.320]   or like look at, you know, take a photo of your fridge
[04:28:58.320 --> 04:28:59.840]   and or like upload your recipes
[04:28:59.840 --> 04:29:01.680]   and then like it figures out what to order from,
[04:29:01.680 --> 04:29:04.800]   you know, Amazon/Whole Foods food delivery.
[04:29:04.800 --> 04:29:06.600]   Like that's, then that's gonna be like pretty quick
[04:29:06.600 --> 04:29:07.560]   and easy to do, I think.
[04:29:07.560 --> 04:29:09.960]   So it's gonna be a whole range of like business outcomes
[04:29:09.960 --> 04:29:13.020]   and it's gonna be tons of sort of optimism around,
[04:29:13.020 --> 04:29:14.520]   people can just figure out ways to make money.
[04:29:14.520 --> 04:29:17.000]   - To be clear, these sandboxes already exist in research.
[04:29:17.000 --> 04:29:18.880]   There are people who have built clones
[04:29:18.880 --> 04:29:22.040]   of all the most popular websites of Google, Amazon,
[04:29:22.040 --> 04:29:24.920]   blah, blah, blah, to make it so that there's,
[04:29:24.920 --> 04:29:26.640]   I mean, OpenAI probably has them internally
[04:29:26.640 --> 04:29:27.480]   to train these things.
[04:29:27.480 --> 04:29:29.720]   It's the same as DeepMind's robotics team for years
[04:29:29.720 --> 04:29:31.880]   has had clusters for robotics
[04:29:31.880 --> 04:29:34.620]   where you interact with robots fully remotely.
[04:29:34.620 --> 04:29:37.480]   They just have a lab in London and you send tasks to it,
[04:29:37.480 --> 04:29:39.800]   arrange the blocks and you do this research.
[04:29:39.800 --> 04:29:42.240]   Obviously there's techs there that fix stuff,
[04:29:42.240 --> 04:29:46.480]   but we've turned these cranks of automation before.
[04:29:46.480 --> 04:29:48.320]   You go from sandbox to progress
[04:29:48.320 --> 04:29:51.880]   and then you add one more domain at a time and generalize.
[04:29:51.880 --> 04:29:55.360]   I think in the history of NLP and language processing,
[04:29:55.360 --> 04:29:57.760]   instruction tuning and tasks per language model
[04:29:57.760 --> 04:30:00.160]   used to be like one language model did one task.
[04:30:00.160 --> 04:30:01.840]   And then in the instruction tuning literature,
[04:30:01.840 --> 04:30:03.680]   there's this point where you start adding more
[04:30:03.680 --> 04:30:05.240]   and more tasks together,
[04:30:05.240 --> 04:30:07.520]   where it just starts to generalize to every task.
[04:30:07.520 --> 04:30:09.000]   And we don't know where on this curve we are.
[04:30:09.000 --> 04:30:11.320]   I think for reasoning with this RL and verifiable domains,
[04:30:11.320 --> 04:30:14.120]   we're early, but we don't know where the point is
[04:30:14.120 --> 04:30:16.600]   where you just start training on enough domains
[04:30:16.600 --> 04:30:19.360]   and poof, like more domains just start working
[04:30:19.360 --> 04:30:22.000]   and you've crossed the generalization barrier.
[04:30:22.000 --> 04:30:24.800]   - Well, what do you think about the programming context?
[04:30:24.800 --> 04:30:30.040]   So software engineering, that's where I personally,
[04:30:30.040 --> 04:30:34.740]   and I know a lot of people interact with AI the most.
[04:30:34.740 --> 04:30:36.040]   - There's a lot of fear and angst too
[04:30:36.040 --> 04:30:37.800]   from current CS students, but there's also,
[04:30:37.800 --> 04:30:39.720]   that's where, that is the area
[04:30:39.720 --> 04:30:41.920]   where probably the most AI revenue
[04:30:41.920 --> 04:30:44.560]   and productivity gains have come, right?
[04:30:44.560 --> 04:30:48.280]   Whether it be copilots or cursor or what have you, right?
[04:30:48.280 --> 04:30:50.440]   This is, or just standard chat GPT, right?
[04:30:50.440 --> 04:30:53.160]   Like a lot of, I know very few programmers
[04:30:53.160 --> 04:30:55.160]   who don't have chat GPT and actually many of them
[04:30:55.160 --> 04:30:58.080]   have the $200 tier because that's what it's so good for,
[04:30:58.080 --> 04:30:59.360]   right?
[04:30:59.360 --> 04:31:02.360]   I think that in that world,
[04:31:02.360 --> 04:31:04.000]   we already see it like SWE bench.
[04:31:04.000 --> 04:31:06.200]   And if you've looked at the benchmark
[04:31:06.200 --> 04:31:07.840]   made by some Stanford students,
[04:31:07.840 --> 04:31:09.280]   I wouldn't say it's like really hard,
[04:31:09.280 --> 04:31:10.240]   I wouldn't say it's easy either.
[04:31:10.240 --> 04:31:13.160]   I think it takes someone who's been through at least
[04:31:13.160 --> 04:31:15.720]   a few years of CS or a couple of years of programming
[04:31:15.720 --> 04:31:16.960]   to do SWE bench well.
[04:31:16.960 --> 04:31:21.800]   And the models went from 4% to 60% in like a year, right?
[04:31:21.800 --> 04:31:24.160]   And where are they gonna go to next year?
[04:31:24.160 --> 04:31:26.320]   It's gonna be higher, probably won't be 100%
[04:31:26.320 --> 04:31:28.880]   'cause again, that nines is like really hard to do,
[04:31:28.880 --> 04:31:30.760]   but we're gonna get to some point where that's,
[04:31:30.760 --> 04:31:31.600]   and then we're gonna need harder
[04:31:31.600 --> 04:31:34.080]   software engineering benchmarks and so on and so forth.
[04:31:34.080 --> 04:31:37.440]   But the way that people think of it now
[04:31:37.440 --> 04:31:38.880]   is it can do code completion easy.
[04:31:38.880 --> 04:31:40.440]   It can do some function generation
[04:31:40.440 --> 04:31:41.840]   and I have to review it, great.
[04:31:41.840 --> 04:31:44.960]   But really the like software engineering agents,
[04:31:44.960 --> 04:31:47.600]   I think can be done faster, sooner than any other agent
[04:31:47.600 --> 04:31:50.320]   because it is a verifiable domain.
[04:31:50.320 --> 04:31:53.160]   You can always like unit test or compile.
[04:31:53.160 --> 04:31:56.080]   And there's many different regions of like,
[04:31:56.080 --> 04:31:58.720]   it can inspect the whole code base at once,
[04:31:58.720 --> 04:32:00.640]   which no engineer really can,
[04:32:00.640 --> 04:32:02.600]   only the architects can really think about this stuff,
[04:32:02.600 --> 04:32:05.160]   the really senior guys and they can define stuff.
[04:32:05.160 --> 04:32:06.920]   And then the agent can execute on it.
[04:32:06.920 --> 04:32:08.520]   So I think software engineering costs
[04:32:08.520 --> 04:32:09.960]   are gonna plummet like crazy.
[04:32:09.960 --> 04:32:12.280]   And one interesting aspect of that
[04:32:12.280 --> 04:32:14.520]   is when software engineering costs are really low,
[04:32:14.520 --> 04:32:16.280]   you get very different markets, right?
[04:32:16.280 --> 04:32:19.120]   So in the US, you have all these platform SaaS companies,
[04:32:19.120 --> 04:32:21.960]   right, Salesforce and so on and so forth, right?
[04:32:21.960 --> 04:32:25.760]   In China, no one uses platform SaaS.
[04:32:25.760 --> 04:32:27.400]   Everyone just builds their own stack
[04:32:27.400 --> 04:32:31.120]   because software engineering is much cheaper in China,
[04:32:31.120 --> 04:32:32.760]   partially because like people STEM,
[04:32:32.760 --> 04:32:34.720]   number of STEM graduates, et cetera.
[04:32:34.720 --> 04:32:36.800]   So it's generally just cheaper to do.
[04:32:37.800 --> 04:32:41.200]   And so at the same time, code for like code LLMs
[04:32:41.200 --> 04:32:42.520]   have been adopted much less in China
[04:32:42.520 --> 04:32:45.080]   because the cost of an engineer there is much lower.
[04:32:45.080 --> 04:32:46.800]   But like what happens when every company
[04:32:46.800 --> 04:32:48.320]   can just invent their own business logic,
[04:32:48.320 --> 04:32:49.880]   like really cheaply and quickly.
[04:32:49.880 --> 04:32:51.560]   You stop using platform SaaS,
[04:32:51.560 --> 04:32:53.520]   you start building custom tailored solutions,
[04:32:53.520 --> 04:32:54.840]   you change them really quickly.
[04:32:54.840 --> 04:32:55.960]   Now all of a sudden your business
[04:32:55.960 --> 04:32:57.400]   is a little bit more efficient too, potentially,
[04:32:57.400 --> 04:32:59.000]   because you're not dealing with the hell
[04:32:59.000 --> 04:33:01.920]   that is like some random platform SaaS company stuff,
[04:33:01.920 --> 04:33:04.120]   not working perfectly and having to adjust workflows
[04:33:04.120 --> 04:33:05.960]   or random business automation cases
[04:33:05.960 --> 04:33:07.960]   that aren't necessarily AI required.
[04:33:07.960 --> 04:33:09.280]   It's just logic that needs to be built
[04:33:09.280 --> 04:33:10.240]   that no one has built, right?
[04:33:10.240 --> 04:33:11.920]   All of these things can go happen faster.
[04:33:11.920 --> 04:33:14.200]   And so I think software, and then the other domain
[04:33:14.200 --> 04:33:16.720]   is like industrial, chemical, mechanical engineers
[04:33:16.720 --> 04:33:18.320]   suck at coding, right?
[04:33:18.320 --> 04:33:20.200]   Just generally, and like their tools,
[04:33:20.200 --> 04:33:22.440]   like semiconductor engineers, their tools are 20 years old.
[04:33:22.440 --> 04:33:25.440]   All the tools run on XP, including ASML lithography tools,
[04:33:25.440 --> 04:33:26.720]   run on Windows XP, right?
[04:33:26.720 --> 04:33:29.720]   It's like, you know, and like a lot of the analysis
[04:33:29.720 --> 04:33:31.280]   happens in Excel, right?
[04:33:31.280 --> 04:33:32.680]   Like, it's just like, guys,
[04:33:32.680 --> 04:33:34.120]   like you guys can move 20 years forward
[04:33:34.120 --> 04:33:35.600]   with all the data you have and gathered
[04:33:35.600 --> 04:33:36.640]   and like do a lot better.
[04:33:36.640 --> 04:33:38.800]   It's just, you need the engineering skills
[04:33:38.800 --> 04:33:40.560]   for software engineering to be delivered
[04:33:40.560 --> 04:33:42.040]   to the actual domain expert engineer.
[04:33:42.040 --> 04:33:45.120]   So I think that's the area where I'm like super duper bullish
[04:33:45.120 --> 04:33:47.560]   of generally AI creating value.
[04:33:47.560 --> 04:33:49.560]   - The big picture is that I don't think
[04:33:49.560 --> 04:33:50.840]   it's gonna be a cliff.
[04:33:50.840 --> 04:33:53.880]   It's like, we talked to, I think a really good example
[04:33:53.880 --> 04:33:58.760]   of how growth changes is when meta added stories.
[04:33:58.760 --> 04:34:00.760]   So Snapchat was on an exponential,
[04:34:00.760 --> 04:34:02.600]   they added stories, it flatlined.
[04:34:02.600 --> 04:34:05.360]   Software engineers, then up until the right,
[04:34:05.360 --> 04:34:07.600]   AI is gonna come in, it's probably just gonna be flat.
[04:34:07.600 --> 04:34:10.160]   It's like, everyone's gonna lose their job.
[04:34:10.160 --> 04:34:13.600]   It's hard because the supply corrects more slowly.
[04:34:13.600 --> 04:34:15.840]   So the amount of students is still growing
[04:34:15.840 --> 04:34:19.360]   and that'll correct on a multi-year, like a year delay.
[04:34:19.360 --> 04:34:21.640]   But the amount of jobs will just turn
[04:34:21.640 --> 04:34:26.000]   and then maybe in 20, 40 years, it'll be well down.
[04:34:26.000 --> 04:34:27.040]   But in the few years,
[04:34:27.040 --> 04:34:28.480]   there'll never gonna be the snap moment
[04:34:28.480 --> 04:34:30.320]   where it's like software engineers aren't useful.
[04:34:30.320 --> 04:34:32.960]   - I think also the nature of what it means to be a programmer
[04:34:32.960 --> 04:34:35.600]   and what kind of jobs programmers do changes.
[04:34:35.600 --> 04:34:38.960]   'Cause I think there needs to be a human
[04:34:38.960 --> 04:34:41.680]   in the loop of everything you've talked about.
[04:34:41.680 --> 04:34:44.320]   There's a really important human in that picture
[04:34:44.320 --> 04:34:47.360]   of like correcting the code.
[04:34:47.360 --> 04:34:49.880]   Like fixing-- - Thinking larger
[04:34:49.880 --> 04:34:51.040]   than the context length.
[04:34:51.040 --> 04:34:53.760]   - Yep, and debugging also.
[04:34:53.760 --> 04:34:56.480]   Like debugging by sort of reading the code,
[04:34:56.480 --> 04:34:59.000]   understanding the, steering the system.
[04:34:59.000 --> 04:35:00.520]   Like, no, no, no, you missed the point.
[04:35:00.520 --> 04:35:02.200]   Adding more to the prompt.
[04:35:02.200 --> 04:35:05.280]   Kind of like, yes, adding the human--
[04:35:05.280 --> 04:35:07.320]   - Designing the perfect Google button.
[04:35:07.320 --> 04:35:09.360]   Google's famous for having people design buttons
[04:35:09.360 --> 04:35:10.720]   that are so perfect.
[04:35:10.720 --> 04:35:13.160]   And it's like, how is AI gonna do that?
[04:35:13.160 --> 04:35:15.800]   Like, they could give you all the ideas.
[04:35:15.800 --> 04:35:16.960]   Perfect button.
[04:35:16.960 --> 04:35:18.000]   - I mean, that's the thing.
[04:35:18.000 --> 04:35:19.520]   You can call it taste.
[04:35:19.520 --> 04:35:22.440]   Humans have, one thing humans can do
[04:35:22.440 --> 04:35:24.480]   is figure out what other humans enjoy
[04:35:24.480 --> 04:35:25.640]   better than AI systems.
[04:35:25.640 --> 04:35:28.320]   That's where the preference, you're loading that in.
[04:35:28.320 --> 04:35:31.320]   But ultimately, humans are the greatest preference generator.
[04:35:31.320 --> 04:35:32.880]   That's where the preference comes from.
[04:35:32.880 --> 04:35:35.080]   - And humans are actually very good at reading,
[04:35:35.080 --> 04:35:36.720]   or like judging between two things,
[04:35:36.720 --> 04:35:38.440]   versus, this goes back to the core
[04:35:38.440 --> 04:35:40.360]   of what RLHF and preference tuning is,
[04:35:40.360 --> 04:35:42.160]   is that it's hard to generate a good answer
[04:35:42.160 --> 04:35:43.000]   for a lot of problems,
[04:35:43.000 --> 04:35:45.000]   but it's easy to see which one is better.
[04:35:45.000 --> 04:35:47.160]   And that's how we're using humans for AI now,
[04:35:47.160 --> 04:35:48.760]   is judging which one is better.
[04:35:48.760 --> 04:35:50.600]   And that's what software engineering could look like.
[04:35:50.600 --> 04:35:52.360]   It's the PR review.
[04:35:52.360 --> 04:35:53.960]   Here's a few options.
[04:35:53.960 --> 04:35:57.280]   What are the, like, here are some potential pros and cons.
[04:35:57.280 --> 04:35:59.520]   And they're gonna be judges.
[04:35:59.520 --> 04:36:02.120]   - I think the thing I would very much recommend
[04:36:02.120 --> 04:36:06.000]   is people start, programmers start using AI,
[04:36:06.000 --> 04:36:09.360]   and embracing that role of the supervisor of the AI system,
[04:36:09.360 --> 04:36:11.400]   and like, partner of the AI system,
[04:36:11.400 --> 04:36:13.400]   versus writing from scratch,
[04:36:13.400 --> 04:36:16.840]   or not learning coding at all, and just generating stuff.
[04:36:16.840 --> 04:36:18.040]   'Cause I think there actually has to be
[04:36:18.040 --> 04:36:20.480]   a pretty high level of expertise as a programmer
[04:36:20.480 --> 04:36:24.000]   to be able to manage increasingly intelligent systems.
[04:36:24.000 --> 04:36:25.080]   - I think it's that,
[04:36:25.080 --> 04:36:27.280]   and then becoming a domain expert in something.
[04:36:27.280 --> 04:36:28.120]   - Sure, yeah.
[04:36:28.120 --> 04:36:30.560]   'Cause seriously, if you go look at aerospace,
[04:36:30.560 --> 04:36:32.600]   or semiconductors, or chemical engineering,
[04:36:32.600 --> 04:36:34.920]   everyone is using really crappy platforms,
[04:36:34.920 --> 04:36:36.440]   really old software.
[04:36:36.440 --> 04:36:39.760]   Like, the job of a data scientist is like a joke,
[04:36:39.760 --> 04:36:41.040]   right, in many cases.
[04:36:41.040 --> 04:36:43.360]   In many cases, it's very real, but it's like,
[04:36:43.360 --> 04:36:45.840]   bring what the forefront of human capabilities are
[04:36:45.840 --> 04:36:46.840]   to your domain.
[04:36:46.840 --> 04:36:49.200]   And even if the forefront is from the AI,
[04:36:49.200 --> 04:36:51.000]   your domain, you're at the forefront, right?
[04:36:51.000 --> 04:36:53.520]   So it's like, you have to be at the forefront of something,
[04:36:53.520 --> 04:36:56.200]   and then leverage the rising tide
[04:36:56.200 --> 04:36:57.640]   that is AI for everything else.
[04:36:57.640 --> 04:37:01.880]   - But yeah, there's so many low-hanging fruit everywhere
[04:37:01.880 --> 04:37:05.480]   in terms of where software can help automate a thing,
[04:37:05.480 --> 04:37:07.720]   or digitize a thing.
[04:37:07.720 --> 04:37:11.320]   In the legal system, I mean, that's why DOJ is exciting.
[04:37:11.320 --> 04:37:16.080]   Yeah, I got to hang out with a bunch of the DOJ folks,
[04:37:16.080 --> 04:37:20.320]   and they, I mean, government is so old school.
[04:37:20.320 --> 04:37:25.320]   It's like begging for the modernization of software,
[04:37:25.560 --> 04:37:28.120]   of organizing the data, all this kind of stuff.
[04:37:28.120 --> 04:37:30.120]   I mean, in that case, it's by design,
[04:37:30.120 --> 04:37:35.120]   because bureaucracy protects centers of power and so on,
[04:37:35.120 --> 04:37:39.520]   but software breaks down those barriers,
[04:37:39.520 --> 04:37:43.000]   so it hurts those that are holding onto power,
[04:37:43.000 --> 04:37:45.360]   but ultimately benefits humanity.
[04:37:45.360 --> 04:37:48.260]   So there's a bunch of domains of that kind.
[04:37:48.260 --> 04:37:53.200]   One thing we didn't fully finish talking about
[04:37:53.200 --> 04:37:54.920]   is open source.
[04:37:54.920 --> 04:37:58.280]   So first of all, congrats, you released a new model.
[04:37:58.280 --> 04:38:00.200]   - Yeah, this is the-- - Tulu.
[04:38:00.200 --> 04:38:01.520]   - I'll explain what a Tulu is.
[04:38:01.520 --> 04:38:04.160]   A Tulu is a hybrid camel when you breed a dromedary
[04:38:04.160 --> 04:38:06.480]   with a Bacchurian camel.
[04:38:06.480 --> 04:38:08.480]   Back in the early days after ChatGPT,
[04:38:08.480 --> 04:38:10.800]   there was a big wave of models coming out,
[04:38:10.800 --> 04:38:12.240]   like Alpaca, Vicuna, et cetera,
[04:38:12.240 --> 04:38:15.920]   that were all named after various mammalian species.
[04:38:15.920 --> 04:38:18.440]   So Tulu, the brand is multiple years old,
[04:38:18.440 --> 04:38:22.480]   which comes from that, and we've been playing
[04:38:22.480 --> 04:38:25.600]   at the frontiers of post-training with open source code.
[04:38:25.600 --> 04:38:28.880]   And this first part of this release was in the fall,
[04:38:28.880 --> 04:38:33.800]   where we built on Lama's open weight models,
[04:38:33.800 --> 04:38:35.880]   and then we add in our fully open code,
[04:38:35.880 --> 04:38:37.240]   our fully open data.
[04:38:37.240 --> 04:38:40.640]   There's a popular benchmark that is Chatbot Arena,
[04:38:40.640 --> 04:38:42.360]   and that's generally the metric
[04:38:42.360 --> 04:38:44.800]   by which how these chat models are evaluated,
[04:38:44.800 --> 04:38:47.340]   and it's humans compare random models
[04:38:47.340 --> 04:38:48.760]   from different organizations.
[04:38:48.760 --> 04:38:51.600]   And if you looked at the leaderboard in November or December,
[04:38:51.600 --> 04:38:55.680]   among the top 60 models from 10s to 20s of organizations,
[04:38:55.680 --> 04:38:58.600]   none of them had open code or data for just post-training.
[04:38:58.600 --> 04:39:01.160]   Among that, even fewer or none have pre-training data
[04:39:01.160 --> 04:39:03.120]   and code available, but it's like post-training
[04:39:03.120 --> 04:39:04.360]   is much more accessible at this time.
[04:39:04.360 --> 04:39:06.240]   It's still pretty cheap and you can do it.
[04:39:06.240 --> 04:39:08.560]   And the thing is like, how high can we push this number
[04:39:08.560 --> 04:39:11.340]   where people have access to all the code and data?
[04:39:11.340 --> 04:39:12.880]   So that's kind of the motivation of the project.
[04:39:12.880 --> 04:39:14.360]   We draw on lessons from Lama.
[04:39:14.360 --> 04:39:17.520]   NVIDIA had a Nemotron model where the recipe
[04:39:17.520 --> 04:39:20.560]   for their post-training was fairly open with some data,
[04:39:20.560 --> 04:39:23.020]   and a paper, and it's putting all these together
[04:39:23.020 --> 04:39:26.040]   to try to create a recipe that people can fine-tune models
[04:39:26.040 --> 04:39:27.880]   like GPT-4 to their domain.
[04:39:27.880 --> 04:39:30.640]   - So to be clear, in the case of Tulu,
[04:39:30.640 --> 04:39:32.040]   maybe you can talk about Alma too,
[04:39:32.040 --> 04:39:35.960]   but in the case of Tulu, you're taking Lama 3.405b.
[04:39:35.960 --> 04:39:41.400]   - Tulu has been a series of recipes for post-training.
[04:39:41.400 --> 04:39:43.880]   So we've done multiple models over years.
[04:39:43.880 --> 04:39:46.720]   - And so you're open sourcing everything.
[04:39:46.720 --> 04:39:49.080]   - Yeah, if you start with an open weight-based model,
[04:39:49.080 --> 04:39:51.600]   the whole model technically is an open source
[04:39:51.600 --> 04:39:53.760]   'cause you don't know what Lama put into it,
[04:39:53.760 --> 04:39:55.880]   which is why we have a separate thing that we'll get to,
[04:39:55.880 --> 04:39:58.400]   but it's just getting parts of the pipeline
[04:39:58.400 --> 04:40:00.240]   where people can zoom in and customize.
[04:40:00.240 --> 04:40:01.880]   I know I hear from startups and businesses,
[04:40:01.880 --> 04:40:03.920]   they're like, "Okay, I can take this post-training
[04:40:03.920 --> 04:40:05.800]   "and try to apply it to my domain."
[04:40:05.800 --> 04:40:07.320]   We talk about verifiers a lot.
[04:40:07.320 --> 04:40:09.800]   We use this idea, which is reinforcement learning
[04:40:09.800 --> 04:40:13.160]   with verifiable domain rewards, RLVR,
[04:40:13.160 --> 04:40:17.720]   kind of similar to RLHF, and we've applied it to map.
[04:40:17.720 --> 04:40:20.160]   And the model today, which is like,
[04:40:20.160 --> 04:40:24.240]   we applied it to the Lama 405b base model from last year,
[04:40:24.240 --> 04:40:25.680]   and we have our other stuff.
[04:40:25.680 --> 04:40:28.800]   We have our instruction tuning and our preference tuning,
[04:40:28.800 --> 04:40:31.360]   but the math thing is interesting,
[04:40:31.360 --> 04:40:34.240]   which is like, it's easier to improve this math benchmark.
[04:40:34.240 --> 04:40:37.520]   There's a benchmark, M-A-T-H, math, all capitals.
[04:40:37.520 --> 04:40:38.360]   Tough name.
[04:40:38.360 --> 04:40:40.400]   When the benchmark's name is the area
[04:40:40.400 --> 04:40:42.360]   that you're evaluating, we're researchers.
[04:40:42.360 --> 04:40:44.520]   We're not brand strategists.
[04:40:44.520 --> 04:40:47.240]   And this is something that the "Deep Seek" paper
[04:40:47.240 --> 04:40:49.520]   talked about as well, is like, at this bigger model,
[04:40:49.520 --> 04:40:52.000]   it's easier to elicit powerful capabilities
[04:40:52.000 --> 04:40:54.880]   with this RL training, and then they distill it down
[04:40:54.880 --> 04:40:56.400]   from that big model to the small model.
[04:40:56.400 --> 04:40:57.920]   And this model we released today,
[04:40:57.920 --> 04:41:01.160]   we saw the same thing as it were at AI2.
[04:41:01.160 --> 04:41:02.280]   We don't have a ton of compute.
[04:41:02.280 --> 04:41:04.200]   We can't train 405b models all the time.
[04:41:04.200 --> 04:41:06.760]   So we just did a few runs and they tend to work.
[04:41:06.760 --> 04:41:10.120]   And it's like, it just shows that there's a lot of room
[04:41:10.120 --> 04:41:12.360]   for people to play in these things.
[04:41:12.360 --> 04:41:15.000]   - And they crushed Lama's actual release, right?
[04:41:15.000 --> 04:41:16.880]   Like, they're way better than it.
[04:41:16.880 --> 04:41:18.320]   - Yeah, so our eval numbers, I mean,
[04:41:18.320 --> 04:41:20.400]   we have extra months in this, but our eval numbers
[04:41:20.400 --> 04:41:23.400]   are like much better than the Lama Instruct model
[04:41:23.400 --> 04:41:24.240]   that they released.
[04:41:24.240 --> 04:41:26.440]   - And then you also said better than Deep Seek V3.
[04:41:26.440 --> 04:41:28.760]   - Yeah, on our eval benchmark.
[04:41:28.760 --> 04:41:31.200]   The most, Deep Seek V3 is really similar.
[04:41:31.200 --> 04:41:33.480]   We have a safety benchmark to understand
[04:41:33.480 --> 04:41:35.400]   if it will say harmful things and things like that.
[04:41:35.400 --> 04:41:37.160]   And that's what draws us down most of the way.
[04:41:37.160 --> 04:41:38.000]   It's still like-
[04:41:38.000 --> 04:41:39.560]   - It's like an amalgamation of multiple benchmarks,
[04:41:39.560 --> 04:41:40.400]   or what do you mean?
[04:41:40.400 --> 04:41:41.600]   - Yeah, so we have a 10 evaluators.
[04:41:41.600 --> 04:41:43.440]   This is like, this is standard practice in post-training,
[04:41:43.440 --> 04:41:45.600]   is you choose your evaluations you care about.
[04:41:45.600 --> 04:41:47.320]   In academics, in smaller labs,
[04:41:47.320 --> 04:41:48.920]   you'll have fewer evaluations.
[04:41:48.920 --> 04:41:50.800]   In companies, you'll have a really one domain
[04:41:50.800 --> 04:41:51.760]   that you really care about.
[04:41:51.760 --> 04:41:54.040]   In frontier labs, you'll have 10s to 20s
[04:41:54.040 --> 04:41:56.760]   to maybe even like 100 evaluations of specific things.
[04:41:56.760 --> 04:41:58.720]   So we choose a representative suite of things
[04:41:58.720 --> 04:42:01.500]   that look like chat, precise instruction following,
[04:42:01.500 --> 04:42:03.600]   which is like, respond only in emojis.
[04:42:03.600 --> 04:42:05.880]   Like, does the model follow weird things like that?
[04:42:05.880 --> 04:42:08.000]   Math, code, and you create a suite like this.
[04:42:08.000 --> 04:42:11.600]   So safety would be one of 10 in that type of suite
[04:42:11.600 --> 04:42:13.400]   where you have like, what is the broader community
[04:42:13.400 --> 04:42:14.720]   of AI care about?
[04:42:14.720 --> 04:42:17.280]   And for example, in comparison to DeepSeek,
[04:42:17.280 --> 04:42:18.760]   it would be something like our average eval
[04:42:18.760 --> 04:42:22.400]   for our model would be 80, including safety,
[04:42:22.400 --> 04:42:23.360]   and similar without.
[04:42:23.360 --> 04:42:27.720]   And DeepSeek would be like 79% average score
[04:42:27.720 --> 04:42:30.880]   without safety, and their safety score
[04:42:30.880 --> 04:42:31.720]   would bring it down to like 76 on average.
[04:42:31.720 --> 04:42:33.800]   - Oh, so you beat them even ignoring safety.
[04:42:33.800 --> 04:42:35.440]   - Yeah, so this is something that internally,
[04:42:35.440 --> 04:42:38.200]   it's like, I don't want to win only by like,
[04:42:38.200 --> 04:42:39.600]   how you shape the eval benchmark.
[04:42:39.600 --> 04:42:40.680]   So if there's something that's like,
[04:42:40.680 --> 04:42:43.280]   people may or may not care about safety in their model,
[04:42:43.280 --> 04:42:44.520]   safety can come downstream.
[04:42:44.520 --> 04:42:46.440]   Safety can be when you host the model for an API.
[04:42:46.440 --> 04:42:49.680]   Like, safety is addressed in a spectrum of locations
[04:42:49.680 --> 04:42:50.680]   in AI applications.
[04:42:50.680 --> 04:42:51.640]   So it's like, if you want to say
[04:42:51.640 --> 04:42:52.680]   that you have the best recipe,
[04:42:52.680 --> 04:42:54.320]   you can't just gate it on these things
[04:42:54.320 --> 04:42:56.360]   that some people might not want.
[04:42:56.360 --> 04:43:00.800]   And this is just, it's like the time of progress.
[04:43:00.800 --> 04:43:03.160]   We benefit, we can release a model later.
[04:43:03.160 --> 04:43:05.080]   We have more time to learn new techniques,
[04:43:05.080 --> 04:43:06.200]   like this RL technique.
[04:43:06.200 --> 04:43:07.960]   We had started this in the fall.
[04:43:07.960 --> 04:43:09.920]   It's now really popular with reasoning models.
[04:43:09.920 --> 04:43:12.520]   The next thing to do for open source post-training
[04:43:12.520 --> 04:43:14.880]   is to scale up verifiers, to scale up data,
[04:43:14.880 --> 04:43:17.200]   to replicate some of DeepSeek's results.
[04:43:17.200 --> 04:43:19.000]   And it's awesome that we have a paper to draw on
[04:43:19.000 --> 04:43:20.720]   and it makes it a lot easier.
[04:43:20.720 --> 04:43:24.280]   And that's the type of things that is going on
[04:43:24.280 --> 04:43:28.400]   among academic and closed frontier research in AI.
[04:43:28.400 --> 04:43:29.920]   - Since you're pushing open source,
[04:43:29.920 --> 04:43:31.400]   what do you think is the future of it?
[04:43:31.400 --> 04:43:33.760]   You think DeepSeek actually changes things
[04:43:33.760 --> 04:43:36.160]   since it's open source or open weight,
[04:43:36.160 --> 04:43:37.640]   or it's pushing the open source movement
[04:43:37.640 --> 04:43:38.920]   into the open direction?
[04:43:38.920 --> 04:43:41.000]   - This goes very back to license discussion.
[04:43:41.000 --> 04:43:44.000]   So DeepSeek R1 with a friendly license is a major reset.
[04:43:44.000 --> 04:43:45.560]   So it's like the first time that we've had
[04:43:45.560 --> 04:43:48.920]   a really clear frontier model that is open weights
[04:43:48.920 --> 04:43:50.920]   and with a commercially friendly license
[04:43:50.920 --> 04:43:53.000]   with no restrictions on downstream use cases,
[04:43:53.000 --> 04:43:54.800]   synthetic data, distillation, whatever.
[04:43:54.800 --> 04:43:57.920]   This has never been the case at all in the history of AI
[04:43:57.920 --> 04:43:59.720]   in the last few years since CatGPT.
[04:43:59.720 --> 04:44:01.560]   There have been models that are off the frontier
[04:44:01.560 --> 04:44:03.080]   or models with weird licenses
[04:44:03.080 --> 04:44:04.720]   that you can't really use them.
[04:44:04.720 --> 04:44:07.360]   - Isn't Meta's license pretty much permissible
[04:44:07.360 --> 04:44:09.160]   except for five companies?
[04:44:09.160 --> 04:44:11.120]   - And there's also, so this goes to like
[04:44:11.120 --> 04:44:13.120]   what open source AI is, which is,
[04:44:13.120 --> 04:44:15.800]   there's also use case restrictions in the LLAMA license,
[04:44:15.800 --> 04:44:17.680]   which says you can't use it for specific things.
[04:44:17.680 --> 04:44:19.840]   So if you come from an open source software background,
[04:44:19.840 --> 04:44:22.560]   you would say that that is not an open source license.
[04:44:22.560 --> 04:44:23.640]   - What kind of things are those though?
[04:44:23.640 --> 04:44:24.640]   Like, are they like?
[04:44:24.640 --> 04:44:27.720]   - At this point, I can't pull them off the top of my head,
[04:44:27.720 --> 04:44:29.000]   but it'll be like-- - Stuff that's competitor--
[04:44:29.000 --> 04:44:30.960]   - It used to be military use was one,
[04:44:30.960 --> 04:44:32.360]   and they removed that for scale.
[04:44:32.360 --> 04:44:36.960]   It'll be like CSAM, like child abuse material,
[04:44:36.960 --> 04:44:39.680]   or like that's the type of thing that is forbidden there,
[04:44:39.680 --> 04:44:42.040]   but that's enough from an open source background
[04:44:42.040 --> 04:44:43.440]   to say it's not open source license.
[04:44:43.440 --> 04:44:45.880]   And also the LLAMA license has this horrible thing
[04:44:45.880 --> 04:44:48.520]   where you have to name your model LLAMA
[04:44:48.520 --> 04:44:50.920]   if you touch it to the LLAMA model.
[04:44:50.920 --> 04:44:52.040]   So it's like the branding thing.
[04:44:52.040 --> 04:44:53.560]   So if a company uses LLAMA,
[04:44:53.560 --> 04:44:55.480]   technically the license says that they should say
[04:44:55.480 --> 04:44:57.600]   built with LLAMA at the bottom of their application.
[04:44:57.600 --> 04:45:00.200]   And from like a marketing perspective, that just hurts.
[04:45:00.200 --> 04:45:01.880]   Like I can suck it up as a researcher.
[04:45:01.880 --> 04:45:02.920]   I'm like, oh, it's fine.
[04:45:02.920 --> 04:45:06.280]   Like it says LLAMA dash on all of our materials
[04:45:06.280 --> 04:45:07.160]   for this release.
[04:45:07.160 --> 04:45:08.920]   But this is why we need truly open models,
[04:45:08.920 --> 04:45:11.880]   which is we don't know DeepSeek R1's data.
[04:45:11.880 --> 04:45:15.120]   - Wait, so you're saying I can't make a cheap copy of LLAMA
[04:45:15.120 --> 04:45:15.960]   and pretend it's mine,
[04:45:15.960 --> 04:45:17.680]   but I can do this with the Chinese model.
[04:45:17.680 --> 04:45:18.520]   - Yeah. - Hell yeah.
[04:45:18.520 --> 04:45:20.280]   (both laughing)
[04:45:20.280 --> 04:45:21.880]   - That's what I'm saying.
[04:45:21.880 --> 04:45:22.920]   And that's why it's like,
[04:45:22.920 --> 04:45:26.040]   we want this whole open language models thing,
[04:45:26.040 --> 04:45:28.640]   the Olmo thing, is to try to keep the model
[04:45:28.640 --> 04:45:30.440]   where everything is open with the data
[04:45:30.440 --> 04:45:31.920]   as close to the frontier as possible.
[04:45:31.920 --> 04:45:34.840]   So we're compute constrained, we're personnel constrained.
[04:45:34.840 --> 04:45:37.360]   We rely on getting insights from people,
[04:45:37.360 --> 04:45:39.920]   like John Shulman tells us to do RL on outputs.
[04:45:39.920 --> 04:45:41.560]   Like we can make these big jumps,
[04:45:41.560 --> 04:45:43.240]   but it just takes a long time
[04:45:43.240 --> 04:45:44.760]   to push the frontier of open source.
[04:45:44.760 --> 04:45:48.160]   And fundamentally, I would say that that's because
[04:45:48.160 --> 04:45:50.840]   open source AI does not have the same feedback loops
[04:45:50.840 --> 04:45:51.880]   as open source software.
[04:45:51.880 --> 04:45:54.480]   We talked about open source software for security.
[04:45:54.480 --> 04:45:56.800]   Also, it's just because you build something once
[04:45:56.800 --> 04:45:57.840]   and you can reuse it.
[04:45:57.840 --> 04:46:00.560]   If you go into a new company, there's so many benefits.
[04:46:00.560 --> 04:46:02.680]   But if you open source a language model,
[04:46:02.680 --> 04:46:04.280]   you have this data sitting around,
[04:46:04.280 --> 04:46:05.600]   you have this training code.
[04:46:05.600 --> 04:46:08.080]   It's not like that easy for someone to come
[04:46:08.080 --> 04:46:09.080]   and build on and improve,
[04:46:09.080 --> 04:46:10.440]   'cause you need to spend a lot on compute,
[04:46:10.440 --> 04:46:12.040]   you need to have expertise.
[04:46:12.040 --> 04:46:15.200]   So until there are feedback loops of open source AI,
[04:46:15.200 --> 04:46:18.200]   it seems like mostly an ideological mission.
[04:46:18.200 --> 04:46:19.200]   People like Mark Zuckerberg,
[04:46:19.200 --> 04:46:20.920]   which is like America needs this.
[04:46:20.920 --> 04:46:22.760]   And I agree with him,
[04:46:22.760 --> 04:46:27.040]   but in the time where the motivation ideologically is high,
[04:46:27.040 --> 04:46:29.160]   we need to capitalize and build this ecosystem
[04:46:29.160 --> 04:46:31.280]   around what benefits do you get
[04:46:31.280 --> 04:46:33.400]   from seeing the language model data.
[04:46:33.400 --> 04:46:35.320]   And there's not a lot about that.
[04:46:35.320 --> 04:46:36.760]   We're gonna try to launch a demo soon
[04:46:36.760 --> 04:46:39.320]   where you can look at an Olmo model in a query
[04:46:39.320 --> 04:46:41.720]   and see what pre-training data is similar to it,
[04:46:41.720 --> 04:46:44.520]   which was like legally risky and complicated,
[04:46:44.520 --> 04:46:47.600]   but it's like, what does it mean to see the data
[04:46:47.600 --> 04:46:48.720]   that the AI was trained on?
[04:46:48.720 --> 04:46:51.320]   It's hard to parse, it's terabytes of files.
[04:46:51.320 --> 04:46:54.360]   It's like, I don't know what I'm gonna find in there.
[04:46:54.360 --> 04:46:56.520]   But that's what we need to do as an ecosystem
[04:46:56.520 --> 04:47:01.320]   if people want open source AI to be financially useful.
[04:47:01.320 --> 04:47:02.640]   - We didn't really talk about Stargate.
[04:47:02.640 --> 04:47:05.080]   I would love to get your opinion on like,
[04:47:05.080 --> 04:47:07.240]   the new administration, the Trump administration,
[04:47:07.240 --> 04:47:09.160]   everything that's doing,
[04:47:09.160 --> 04:47:11.920]   that's being done from the America side
[04:47:11.920 --> 04:47:14.120]   in supporting AI infrastructure
[04:47:14.120 --> 04:47:16.280]   and the efforts of the different AI companies.
[04:47:16.280 --> 04:47:17.360]   What do you think about Stargate?
[04:47:17.360 --> 04:47:20.200]   What are we supposed to think about Stargate?
[04:47:20.200 --> 04:47:23.240]   And does Sam have the money?
[04:47:23.240 --> 04:47:26.840]   - Yeah, so I think Stargate is a opaque thing.
[04:47:26.840 --> 04:47:29.240]   It definitely doesn't have $500 billion,
[04:47:29.240 --> 04:47:30.640]   doesn't even have $100 billion, right?
[04:47:30.640 --> 04:47:33.400]   So what they announced is this $500 billion number,
[04:47:33.400 --> 04:47:37.120]   Larry Ellison, Sam Altman, and Trump said it.
[04:47:37.120 --> 04:47:41.960]   They thanked Trump and Trump did do some executive actions
[04:47:41.960 --> 04:47:44.520]   that like do significantly improve the ability
[04:47:44.520 --> 04:47:46.080]   for this to be built faster.
[04:47:46.080 --> 04:47:49.760]   One of the executive actions he did is on federal land,
[04:47:49.760 --> 04:47:52.240]   you can just basically build data centers in power,
[04:47:52.440 --> 04:47:54.560]   you know, like pretty much like that.
[04:47:54.560 --> 04:47:56.800]   And then the permitting process is basically gone
[04:47:56.800 --> 04:47:58.240]   or you file after the fact.
[04:47:58.240 --> 04:48:00.760]   So like one of the, again, like I had a schizo take earlier,
[04:48:00.760 --> 04:48:01.600]   another schizo take,
[04:48:01.600 --> 04:48:04.200]   if you've ever been to the Presidio in San Francisco,
[04:48:04.200 --> 04:48:05.600]   beautiful area.
[04:48:05.600 --> 04:48:07.440]   You could build a power plant and a data center there
[04:48:07.440 --> 04:48:09.680]   if you wanted to, because it is federal land.
[04:48:09.680 --> 04:48:11.360]   It used to be a military base.
[04:48:11.360 --> 04:48:14.120]   But you know, obviously this would like piss people off.
[04:48:14.120 --> 04:48:15.760]   You know, it's a good bit.
[04:48:15.760 --> 04:48:19.680]   Anyways, Trump has made it much easier to do this,
[04:48:19.680 --> 04:48:20.640]   right, generally.
[04:48:20.640 --> 04:48:23.960]   Texas has the only unregulated grid in the nation as well.
[04:48:23.960 --> 04:48:25.080]   - Let's go Texas.
[04:48:25.080 --> 04:48:28.800]   - And so, you know, therefore like ERCOT enables people
[04:48:28.800 --> 04:48:30.020]   to build faster as well.
[04:48:30.020 --> 04:48:32.560]   In addition, the federal regulations are coming down.
[04:48:32.560 --> 04:48:35.520]   And so Stargate is predicated,
[04:48:35.520 --> 04:48:37.400]   and this is why that whole show happened.
[04:48:37.400 --> 04:48:39.760]   Now, how they came up with a $500 billion number
[04:48:39.760 --> 04:48:40.880]   is beyond me.
[04:48:40.880 --> 04:48:42.860]   How they came up with a $100 billion number
[04:48:42.860 --> 04:48:44.720]   makes sense to some extent, right?
[04:48:44.720 --> 04:48:47.360]   And there's actually a good table in here
[04:48:47.360 --> 04:48:51.400]   that I would like to show in that Stargate piece that I had.
[04:48:51.400 --> 04:48:56.680]   It's the most recent one, yeah.
[04:48:56.680 --> 04:49:00.520]   So anyways, Stargate, you know, it's basically, right,
[04:49:00.520 --> 04:49:03.860]   like there is, it's a table about cost.
[04:49:03.860 --> 04:49:06.880]   There, you passed it already.
[04:49:06.880 --> 04:49:07.700]   It's that one.
[04:49:07.700 --> 04:49:11.800]   So this table is kind of explaining what happens, right?
[04:49:11.800 --> 04:49:14.120]   So Stargate is in Abilene, Texas,
[04:49:14.120 --> 04:49:16.280]   the first $100 billion of it.
[04:49:16.280 --> 04:49:18.920]   That site is 2.2 gigawatts of power in,
[04:49:18.920 --> 04:49:23.040]   about 1.8 gigawatts of power consumed, right?
[04:49:23.040 --> 04:49:26.840]   Per GPU, they have like roughly,
[04:49:26.840 --> 04:49:29.840]   Oracle is already building the first part of this
[04:49:29.840 --> 04:49:31.400]   before Stargate came about.
[04:49:31.400 --> 04:49:32.720]   To be clear, they've been building it for a year.
[04:49:32.720 --> 04:49:35.480]   They tried to rent it to Elon, in fact, right?
[04:49:35.480 --> 04:49:37.440]   But Elon was like, "It's too slow, I need it faster."
[04:49:37.440 --> 04:49:40.040]   So then he went and did his Memphis thing.
[04:49:40.040 --> 04:49:41.880]   And so OpenAI was able to get it
[04:49:41.880 --> 04:49:44.380]   with this like weird joint venture called Stargate.
[04:49:44.380 --> 04:49:46.120]   They initially signed a deal with just Oracle
[04:49:46.120 --> 04:49:47.920]   for the first section of this cluster, right?
[04:49:47.920 --> 04:49:50.160]   This first section of this cluster, right,
[04:49:50.160 --> 04:49:55.160]   is roughly $5 billion to $6 billion of server spend, right?
[04:49:55.160 --> 04:49:59.520]   And then there's another billion or so of data center spend.
[04:49:59.520 --> 04:50:01.680]   But the, and then likewise,
[04:50:01.680 --> 04:50:03.960]   like if you fill out that entire 1.8 gigawatts
[04:50:03.960 --> 04:50:06.080]   with the next two generations of NVIDIA chips,
[04:50:06.080 --> 04:50:10.280]   GB200, GB300, VR200, and you fill it out completely,
[04:50:10.280 --> 04:50:15.280]   that ends up being roughly $50 billion of server cost, right?
[04:50:15.280 --> 04:50:18.140]   Plus there's data center costs, plus maintenance costs,
[04:50:18.140 --> 04:50:21.100]   plus operation costs, plus all these things.
[04:50:21.100 --> 04:50:22.600]   And that's where OpenAI gets
[04:50:22.600 --> 04:50:25.580]   to their $100 billion announcement that they had, right?
[04:50:25.580 --> 04:50:27.860]   'Cause they talked about $100 billion as phase one,
[04:50:27.860 --> 04:50:30.220]   that's this Abilene Texas data center, right?
[04:50:30.220 --> 04:50:33.540]   $100 billion of total cost of ownership, quote unquote, right?
[04:50:33.540 --> 04:50:35.140]   So it's not CapEx, it's not investment,
[04:50:35.140 --> 04:50:38.300]   it's $100 billion of total cost of ownership.
[04:50:38.300 --> 04:50:40.680]   And then there will be future phases.
[04:50:40.680 --> 04:50:42.380]   They're looking at other sites that are even bigger
[04:50:42.380 --> 04:50:46.060]   than this 2.2 gigawatts, by the way, in Texas and elsewhere.
[04:50:46.060 --> 04:50:49.280]   And so they're not completely ignoring that,
[04:50:49.280 --> 04:50:52.720]   but there is the number of $100 billion
[04:50:52.720 --> 04:50:54.200]   that they say is for phase one,
[04:50:54.200 --> 04:50:55.480]   which I do think will happen.
[04:50:55.480 --> 04:50:57.280]   They don't even have the money for that.
[04:50:57.280 --> 04:50:58.440]   Furthermore, it's not $100 billion,
[04:50:58.440 --> 04:51:00.160]   it's $50 billion of spend, right?
[04:51:00.160 --> 04:51:02.200]   And then like $50 billion of operational cost,
[04:51:02.200 --> 04:51:06.720]   power, et cetera, rental pricing, et cetera.
[04:51:06.720 --> 04:51:09.540]   'Cause OpenAI is renting the GPUs
[04:51:09.540 --> 04:51:11.700]   from the Stargate joint venture, right?
[04:51:11.700 --> 04:51:13.460]   What money do they actually have, right?
[04:51:13.460 --> 04:51:15.140]   SoftBank, SoftBank is gonna invest,
[04:51:15.140 --> 04:51:16.820]   Oracle's gonna invest, OpenAI's gonna invest.
[04:51:16.820 --> 04:51:18.740]   OpenAI is on the line for $19 billion.
[04:51:18.740 --> 04:51:20.740]   Everyone knows that they've only got $6 billion
[04:51:20.740 --> 04:51:23.460]   in their last round and $4 billion in debt.
[04:51:23.460 --> 04:51:26.620]   But there's news of SoftBank
[04:51:26.620 --> 04:51:29.140]   maybe investing $25 billion into OpenAI, right?
[04:51:29.140 --> 04:51:30.820]   So that's part of it, right?
[04:51:30.820 --> 04:51:32.420]   So $19 billion can come from there.
[04:51:32.420 --> 04:51:34.340]   So OpenAI does not have the money at all, right?
[04:51:34.340 --> 04:51:35.860]   To be clear.
[04:51:35.860 --> 04:51:37.220]   Ink is not dried on anything.
[04:51:37.220 --> 04:51:39.860]   OpenAI has $0 for this $50 billion, right?
[04:51:39.860 --> 04:51:41.100]   In which they're legally obligated
[04:51:41.100 --> 04:51:43.820]   to put $19 billion of CapEx or into the joint venture.
[04:51:43.820 --> 04:51:44.900]   And then the rest they're gonna pay
[04:51:44.900 --> 04:51:47.060]   via renting the GPUs from the joint venture.
[04:51:47.060 --> 04:51:49.980]   And then there's Oracle.
[04:51:49.980 --> 04:51:51.700]   Oracle has a lot of money.
[04:51:51.700 --> 04:51:53.380]   They're building the first section completely.
[04:51:53.380 --> 04:51:54.800]   They were spending for it themselves, right?
[04:51:54.800 --> 04:51:58.580]   This $6 billion of CapEx, $10 billion of TCO.
[04:51:58.580 --> 04:52:00.680]   But they, and they were gonna do that first section.
[04:52:00.680 --> 04:52:02.720]   They're paying for that, right?
[04:52:02.720 --> 04:52:03.760]   As far as the rest of the section,
[04:52:03.760 --> 04:52:06.220]   I don't know how much Larry wants to spend, right?
[04:52:06.220 --> 04:52:07.500]   At any point he can pull out, right?
[04:52:07.500 --> 04:52:09.420]   Like this is, again, this is like completely voluntary.
[04:52:09.420 --> 04:52:11.660]   So at any point, there's no signed ink on this, right?
[04:52:11.660 --> 04:52:13.460]   But he potentially could contribute
[04:52:13.460 --> 04:52:14.380]   tens of billions of dollars, right?
[04:52:14.380 --> 04:52:15.640]   To be clear, he's got the money.
[04:52:15.640 --> 04:52:17.380]   Oracle's got the money.
[04:52:17.380 --> 04:52:20.440]   And then there's like MGX, which is the UAE fund,
[04:52:20.440 --> 04:52:23.820]   which technically has $1.5 trillion for investing in AI.
[04:52:23.820 --> 04:52:27.020]   But again, like, I don't know how real that money is.
[04:52:27.020 --> 04:52:30.020]   And like, whereas there is no ink signed for this,
[04:52:30.020 --> 04:52:32.900]   SoftBank does not have $25 billion of cash.
[04:52:32.900 --> 04:52:35.320]   They have to sell down their stake in ARM,
[04:52:35.320 --> 04:52:37.300]   which is, you know, the leader in CPUs.
[04:52:37.300 --> 04:52:38.340]   And they IPO'd it.
[04:52:38.340 --> 04:52:39.900]   This is obviously what they've always wanted to do.
[04:52:39.900 --> 04:52:41.780]   They just didn't know where they'd redeploy the capital.
[04:52:41.780 --> 04:52:44.340]   Selling down the stake in ARM makes a ton of sense.
[04:52:44.340 --> 04:52:46.780]   So they can sell that down and invest in this
[04:52:46.780 --> 04:52:49.720]   if they want to, and invest in OpenAI if they want to.
[04:52:49.720 --> 04:52:51.180]   As far as like money secured,
[04:52:51.180 --> 04:52:55.680]   the first 100,000 GB200 cluster is like, can be funded.
[04:52:55.680 --> 04:52:57.500]   Everything else after that-
[04:52:57.500 --> 04:52:58.340]   - Up in the air.
[04:52:58.340 --> 04:52:59.160]   - Is up in the air.
[04:52:59.160 --> 04:53:00.000]   Money's coming.
[04:53:00.000 --> 04:53:01.280]   I believe the money will come.
[04:53:01.280 --> 04:53:02.220]   I personally do.
[04:53:02.220 --> 04:53:04.220]   - It's just, it's a belief, okay.
[04:53:04.220 --> 04:53:05.940]   - It's a belief that they are gonna release better models
[04:53:05.940 --> 04:53:07.620]   and be able to raise more money, right?
[04:53:07.620 --> 04:53:10.180]   But like, the actual reality is is that Elon's right.
[04:53:10.180 --> 04:53:12.180]   There is, the money does not exist, right?
[04:53:12.180 --> 04:53:14.660]   - What does the US government have to do with anything?
[04:53:14.660 --> 04:53:16.220]   What does Trump have to do with everything?
[04:53:16.220 --> 04:53:17.580]   He's just a hype man?
[04:53:17.580 --> 04:53:19.560]   - Trump is, he's reducing the regulation
[04:53:19.560 --> 04:53:21.300]   so they can build it faster, right?
[04:53:21.300 --> 04:53:23.740]   And he's allowing them to do it, right?
[04:53:23.740 --> 04:53:25.420]   You know, 'cause any investment of this side
[04:53:25.420 --> 04:53:27.060]   is gonna involve like antitrust stuff, right?
[04:53:27.060 --> 04:53:29.340]   Like, so obviously he's gonna allow them to do it.
[04:53:29.340 --> 04:53:31.060]   He's gonna enable the regulations
[04:53:31.060 --> 04:53:32.740]   to actually allow it to be built.
[04:53:32.740 --> 04:53:35.500]   I don't believe there's any US government dollars
[04:53:35.500 --> 04:53:36.860]   being spent on this though.
[04:53:36.860 --> 04:53:40.420]   - Yeah, so I think he's also just creating a general vibe
[04:53:40.420 --> 04:53:42.460]   that this is, regulation will go down
[04:53:42.460 --> 04:53:45.740]   and this is the era of building.
[04:53:45.740 --> 04:53:47.940]   So if you're a builder, you wanna create stuff,
[04:53:47.940 --> 04:53:50.100]   you wanna launch stuff, this is the time to do it.
[04:53:50.100 --> 04:53:52.540]   - And so like, we've had this 1.8 gigawatt data center
[04:53:52.540 --> 04:53:54.540]   in our data for over a year now.
[04:53:54.540 --> 04:53:55.520]   And we've been like sort of sending it
[04:53:55.520 --> 04:53:57.220]   to all of our clients, including many of these companies
[04:53:57.220 --> 04:53:59.020]   that are building the multi gigawatts.
[04:53:59.020 --> 04:54:01.340]   But that is like at a level that's not quite
[04:54:01.340 --> 04:54:04.580]   maybe executives like seeing $500 billion, $100 billion
[04:54:04.580 --> 04:54:06.020]   and then everyone's asking them like,
[04:54:06.020 --> 04:54:07.780]   so it could spur like another,
[04:54:07.780 --> 04:54:10.060]   like an even faster arms race, right?
[04:54:10.060 --> 04:54:11.220]   'Cause there's already an arms race,
[04:54:11.220 --> 04:54:14.180]   but like this like 100 billion, $500 billion number,
[04:54:14.180 --> 04:54:15.980]   Trump talking about it on TV,
[04:54:15.980 --> 04:54:18.680]   like it could spur the arm race to be even faster
[04:54:18.680 --> 04:54:21.020]   and more investors to flood in and et cetera, et cetera.
[04:54:21.020 --> 04:54:24.160]   So I think you're right is that in that sense
[04:54:24.160 --> 04:54:26.660]   that open AI or sort of Trump is sort of like
[04:54:26.660 --> 04:54:28.220]   championing people are gonna build more
[04:54:28.220 --> 04:54:30.820]   and his actions are gonna let people build more.
[04:54:30.820 --> 04:54:35.820]   - What are you excited about these several years
[04:54:35.820 --> 04:54:40.280]   that are upcoming in terms of cluster build outs,
[04:54:40.280 --> 04:54:43.260]   in terms of breakthroughs in AI,
[04:54:43.260 --> 04:54:45.820]   like the best possible future you can imagine
[04:54:45.820 --> 04:54:48.340]   in the next couple of years, two, three, four years,
[04:54:48.340 --> 04:54:49.620]   what does that look like?
[04:54:49.620 --> 04:54:52.180]   Just it could be very specific technical things
[04:54:52.180 --> 04:54:54.220]   like breakthroughs on post-training
[04:54:55.140 --> 04:55:00.140]   or it could be just size big impressive clusters.
[04:55:00.140 --> 04:55:04.140]   - I really enjoy tracking supply chain
[04:55:04.140 --> 04:55:05.580]   and like who's involved in what.
[04:55:05.580 --> 04:55:07.940]   I really do, it's really fun to see like the numbers,
[04:55:07.940 --> 04:55:10.020]   the cost, who's building what capacity,
[04:55:10.020 --> 04:55:12.020]   helping them figure out how much capacity they should build,
[04:55:12.020 --> 04:55:14.420]   winning deals, strategic stuff, that's really cool.
[04:55:14.420 --> 04:55:16.180]   I think technologically,
[04:55:16.180 --> 04:55:17.980]   there's a lot around the networking side
[04:55:17.980 --> 04:55:22.420]   that really excites me with optics and electronics, right?
[04:55:22.420 --> 04:55:23.660]   Like kind of getting closer and closer,
[04:55:23.660 --> 04:55:25.060]   whether it be co-packaged optics
[04:55:25.060 --> 04:55:28.180]   or some sort of like forms of new forms of switching.
[04:55:28.180 --> 04:55:30.780]   - This is internal to a cluster?
[04:55:30.780 --> 04:55:33.420]   - Yeah, also multi-data center training, right?
[04:55:33.420 --> 04:55:35.940]   Like there's people are putting so much fiber
[04:55:35.940 --> 04:55:37.980]   between these data centers and lighting it up
[04:55:37.980 --> 04:55:39.700]   with so much bandwidth
[04:55:39.700 --> 04:55:41.460]   that there's a lot of interesting stuff
[04:55:41.460 --> 04:55:42.540]   happening on that end, right?
[04:55:42.540 --> 04:55:44.980]   Telecom has been really boring since 5G
[04:55:44.980 --> 04:55:48.020]   and now it's like really exciting again on the hardware side.
[04:55:48.020 --> 04:55:50.260]   - Can you educate me a little bit about the speed of things?
[04:55:50.260 --> 04:55:53.300]   So the speed of memory versus the speed of interconnect
[04:55:53.300 --> 04:55:55.780]   versus the speed of fiber between data centers,
[04:55:55.780 --> 04:55:58.820]   are these like orders of magnitude different?
[04:55:58.820 --> 04:56:01.500]   Can we at some point converge towards a place
[04:56:01.500 --> 04:56:04.260]   where it all just feels like one computer?
[04:56:04.260 --> 04:56:06.780]   - No, I don't think that's possible.
[04:56:06.780 --> 04:56:09.540]   It's only gonna get harder to program, not easier.
[04:56:09.540 --> 04:56:10.820]   It's only gonna get more difficult
[04:56:10.820 --> 04:56:12.900]   and complicated and more layers, right?
[04:56:12.900 --> 04:56:15.140]   The general image that people like to have
[04:56:15.140 --> 04:56:16.820]   is like this hierarchy of memory.
[04:56:16.820 --> 04:56:20.260]   So on-chip is really close, localized within the chip,
[04:56:20.260 --> 04:56:21.380]   you have registers, right?
[04:56:21.380 --> 04:56:23.180]   And those are shared between some compute elements.
[04:56:23.180 --> 04:56:24.420]   And then you'll have caches,
[04:56:24.420 --> 04:56:25.980]   which are shared between more compute elements.
[04:56:25.980 --> 04:56:27.100]   Then you have like memory, right?
[04:56:27.100 --> 04:56:30.260]   Like HBM or DRAM, like DDR memory or whatever it is.
[04:56:30.260 --> 04:56:32.580]   And that's shared between the whole chip.
[04:56:32.580 --> 04:56:33.860]   And then you can have, you know,
[04:56:33.860 --> 04:56:36.700]   pools of memory that are shared between many chips, right?
[04:56:36.700 --> 04:56:39.420]   And then storage and you keep zoning out, right?
[04:56:39.420 --> 04:56:41.620]   The access latency across data centers,
[04:56:41.620 --> 04:56:44.180]   across within the data center, within a chip, is different.
[04:56:44.180 --> 04:56:45.300]   So like, you're obviously always,
[04:56:45.300 --> 04:56:47.180]   you're always gonna have different
[04:56:47.180 --> 04:56:49.980]   programming paradigms for this.
[04:56:49.980 --> 04:56:50.820]   It's not gonna be easy,
[04:56:50.820 --> 04:56:51.940]   programming this stuff is gonna be hard,
[04:56:51.940 --> 04:56:53.140]   maybe I can help, right?
[04:56:53.140 --> 04:56:55.300]   You know, with programming this.
[04:56:55.300 --> 04:56:59.340]   But the way to think about it is that like,
[04:56:59.340 --> 04:57:00.580]   there is,
[04:57:00.580 --> 04:57:05.340]   there's sort of like,
[04:57:05.340 --> 04:57:07.820]   the more elements you add to a task,
[04:57:07.820 --> 04:57:10.580]   you don't gain, you don't get strong scaling, right?
[04:57:10.580 --> 04:57:11.620]   If I double the number of chips,
[04:57:11.620 --> 04:57:12.860]   I don't get 2X the performance, right?
[04:57:12.860 --> 04:57:15.500]   This is just like a reality of computing,
[04:57:15.500 --> 04:57:16.780]   'cause there's inefficiencies.
[04:57:16.780 --> 04:57:18.860]   And there's a lot of interesting work being done
[04:57:18.860 --> 04:57:21.820]   to make it not, you know, to make it more linear,
[04:57:21.820 --> 04:57:24.180]   whether it's making the chips more networked together,
[04:57:24.180 --> 04:57:27.180]   more tightly, or, you know, cool programming models,
[04:57:27.180 --> 04:57:29.140]   or cool algorithmic things that you can do
[04:57:29.140 --> 04:57:30.500]   on the model side, right?
[04:57:30.500 --> 04:57:32.500]   DeepSeek did some of these really cool innovations
[04:57:32.500 --> 04:57:33.700]   because they were limited on interconnect,
[04:57:33.700 --> 04:57:35.420]   but they still needed to parallelize, right?
[04:57:35.420 --> 04:57:36.460]   Like all sorts of, you know,
[04:57:36.460 --> 04:57:37.420]   everyone's always doing stuff,
[04:57:37.420 --> 04:57:38.420]   Google's got a bunch of work,
[04:57:38.420 --> 04:57:40.740]   and everyone's got a bunch of work about this.
[04:57:40.740 --> 04:57:42.780]   That stuff is super exciting on the model,
[04:57:42.780 --> 04:57:44.860]   and workload, and innovation side, right?
[04:57:44.860 --> 04:57:47.900]   Hardware, solid state transformers are interesting, right?
[04:57:47.900 --> 04:57:48.940]   For the power side,
[04:57:48.940 --> 04:57:50.900]   there's all sorts of stuff on batteries,
[04:57:50.900 --> 04:57:52.900]   and there's all sorts of stuff on, you know,
[04:57:52.900 --> 04:57:54.100]   I think when you look at,
[04:57:54.100 --> 04:57:56.220]   if you look at every layer of the compute stack, right,
[04:57:56.220 --> 04:57:57.860]   whether it goes from lithography, and etch,
[04:57:57.860 --> 04:58:00.260]   all the way to like fabrication, to like optics,
[04:58:00.260 --> 04:58:03.700]   to networking, to power, to transformers, to cooling,
[04:58:03.700 --> 04:58:05.140]   to, you know, networking,
[04:58:05.140 --> 04:58:07.060]   and you just go on, up, and up, and up, and up the stack,
[04:58:07.060 --> 04:58:08.940]   you know, even air conditioners for data centers
[04:58:08.940 --> 04:58:10.100]   are like innovating, right?
[04:58:10.100 --> 04:58:11.300]   Like it's like, there's like,
[04:58:11.300 --> 04:58:12.860]   copper cables are innovating, right?
[04:58:12.860 --> 04:58:13.700]   Like you wouldn't think it,
[04:58:13.700 --> 04:58:15.140]   but copper cables, like are,
[04:58:15.140 --> 04:58:16.540]   there's some innovations happening there
[04:58:16.540 --> 04:58:18.380]   with like the density of how you can pack them.
[04:58:18.380 --> 04:58:20.620]   And like, it's like all of these layers of the stack,
[04:58:20.620 --> 04:58:21.820]   all the way up to the models.
[04:58:21.820 --> 04:58:24.660]   Human progress is at a pace that's never been seen before.
[04:58:24.660 --> 04:58:27.020]   - I'm just imagining you sitting back in a layer somewhere
[04:58:27.020 --> 04:58:30.020]   with screens everywhere, just monitoring the supply chain,
[04:58:30.020 --> 04:58:31.020]   where all these clusters,
[04:58:31.020 --> 04:58:33.420]   like all the information you're gathering.
[04:58:33.420 --> 04:58:34.740]   I mean, you do incredible-- - There's a big team.
[04:58:34.740 --> 04:58:35.580]   There's a big team.
[04:58:35.580 --> 04:58:40.300]   - Yeah, I mean, you do quite incredible work
[04:58:40.300 --> 04:58:41.140]   with semi-analysis.
[04:58:41.140 --> 04:58:41.960]   I mean, it's just,
[04:58:41.960 --> 04:58:46.780]   keeping your finger on the pulse
[04:58:46.780 --> 04:58:49.180]   of human civilization in the digital world.
[04:58:49.180 --> 04:58:50.020]   It's pretty cool.
[04:58:50.020 --> 04:58:51.820]   Like just to watch, feel that.
[04:58:51.820 --> 04:58:52.660]   - Yeah, thank you.
[04:58:52.660 --> 04:58:53.620]   I guess.
[04:58:53.620 --> 04:58:57.540]   - Feel all of us like doing shit, epic shit.
[04:58:57.540 --> 04:58:58.980]   - Feel the AGI.
[04:58:58.980 --> 04:59:02.380]   - I mean, from meme to like reality.
[04:59:02.380 --> 04:59:04.300]   What, Nathan, is there like breakthroughs
[04:59:04.300 --> 04:59:07.140]   that you're like looking forward to potentially?
[04:59:07.140 --> 04:59:08.380]   - I had a while to think about this
[04:59:08.380 --> 04:59:10.620]   while listening to Dylan's beautiful response.
[04:59:10.620 --> 04:59:11.940]   - He didn't listen to me, he was so dumb.
[04:59:11.940 --> 04:59:13.860]   - I knew, no, I knew this was coming.
[04:59:13.860 --> 04:59:16.500]   And it's like, realistically,
[04:59:16.500 --> 04:59:17.780]   training models is very fun
[04:59:17.780 --> 04:59:19.220]   because there's so much low-hanging fruit.
[04:59:19.220 --> 04:59:21.860]   And the thing that makes my job entertaining,
[04:59:21.860 --> 04:59:24.660]   I train models, I write analysis
[04:59:24.660 --> 04:59:26.340]   about what's happening with models.
[04:59:26.340 --> 04:59:29.140]   And it's fun because there is obviously
[04:59:29.140 --> 04:59:31.060]   so much more progress to be had.
[04:59:31.060 --> 04:59:33.300]   And the real motivation why I do this
[04:59:33.300 --> 04:59:34.380]   somewhere where I can share things
[04:59:34.380 --> 04:59:37.420]   is that there's just, I don't trust people
[04:59:37.420 --> 04:59:39.700]   that are like, trust me, bro, we're gonna make AI good.
[04:59:39.700 --> 04:59:41.140]   That's like, we're the ones that it's like,
[04:59:41.140 --> 04:59:42.700]   we're gonna do it and you can trust us
[04:59:42.700 --> 04:59:44.860]   and we're just gonna have all the AI.
[04:59:44.860 --> 04:59:47.180]   And it's just like, I would like a future
[04:59:47.180 --> 04:59:49.340]   where more people have a say in what AI is
[04:59:49.340 --> 04:59:50.860]   and can understand it.
[04:59:50.860 --> 04:59:53.780]   And that's, it's a little bit less fun
[04:59:53.780 --> 04:59:55.180]   that it's not a positive thing.
[04:59:55.180 --> 04:59:56.500]   I feel like this is just all really fun.
[04:59:56.500 --> 04:59:59.540]   Like training models is fun and bringing people in is fun,
[04:59:59.540 --> 05:00:00.700]   but it's really like AI,
[05:00:00.700 --> 05:00:02.900]   if it is going to be the most powerful technology
[05:00:02.900 --> 05:00:04.540]   of my lifetime, it's like,
[05:00:04.540 --> 05:00:07.500]   we need to have a lot of people involved in making that.
[05:00:07.500 --> 05:00:11.740]   - And making it open helps with that.
[05:00:11.740 --> 05:00:14.340]   As accessible as possible, as open as possible, yeah.
[05:00:14.340 --> 05:00:16.340]   - In my read of the last few years
[05:00:16.340 --> 05:00:19.300]   that more openness would help the AI ecosystem
[05:00:19.300 --> 05:00:21.740]   in terms of having more people understand what's going on,
[05:00:21.740 --> 05:00:23.900]   rather that's researchers from non AI fields
[05:00:23.900 --> 05:00:25.620]   to governments to everything.
[05:00:25.620 --> 05:00:27.860]   It doesn't mean that openness will always be the answer.
[05:00:27.860 --> 05:00:29.660]   I think that it will reassess
[05:00:29.660 --> 05:00:31.660]   of like what is the biggest problem facing AI
[05:00:31.660 --> 05:00:33.580]   and tack on a different angle
[05:00:33.580 --> 05:00:35.260]   to the wild ride that we're on.
[05:00:35.260 --> 05:00:39.940]   - And for me, just from even the user experience,
[05:00:39.940 --> 05:00:42.380]   anytime you have the, like Apathy said,
[05:00:42.380 --> 05:00:45.940]   the aha moments, like the magic,
[05:00:45.940 --> 05:00:49.100]   like seeing the reasoning, the chain of thought,
[05:00:49.100 --> 05:00:52.500]   it's like, there's something really
[05:00:52.500 --> 05:00:54.580]   just fundamentally beautiful about that.
[05:00:54.580 --> 05:00:56.300]   It's putting a mirror to ourselves
[05:00:56.300 --> 05:00:59.600]   and seeing like, oh shit, it is solving intelligence
[05:00:59.600 --> 05:01:02.660]   as the cliche, like goal of these companies is.
[05:01:02.660 --> 05:01:06.860]   And you get to understand why we humans are special.
[05:01:06.860 --> 05:01:09.140]   The intelligence within us is special.
[05:01:09.140 --> 05:01:11.300]   And for now also why we are special
[05:01:11.300 --> 05:01:13.300]   in terms of we seem to be conscious
[05:01:13.300 --> 05:01:16.420]   and the AI systems for now aren't.
[05:01:16.420 --> 05:01:20.180]   And we get to solve, we get to explore that mystery.
[05:01:20.180 --> 05:01:21.780]   So that's, it's just really cool
[05:01:21.780 --> 05:01:23.380]   to get to explore these questions
[05:01:23.380 --> 05:01:27.820]   that I don't think, I would have never imagined
[05:01:27.820 --> 05:01:29.020]   would be even possible.
[05:01:29.020 --> 05:01:34.380]   Back when, so just watching with excitement
[05:01:34.380 --> 05:01:38.700]   Deep Blue be Kasparov, like I wouldn't have ever thought
[05:01:38.700 --> 05:01:41.140]   this kind of AI would be possible in my lifetime.
[05:01:41.500 --> 05:01:43.600]   It's like, this is really feels like AI.
[05:01:43.600 --> 05:01:44.660]   - Yeah. - It's incredible.
[05:01:44.660 --> 05:01:48.540]   - I started with AI of learning to fly a silly quadrotor.
[05:01:48.540 --> 05:01:49.540]   It's like, learn to fly.
[05:01:49.540 --> 05:01:51.780]   And it was just like, it learned to fly up.
[05:01:51.780 --> 05:01:53.580]   It would hit the ceiling and stop and catch it.
[05:01:53.580 --> 05:01:55.380]   It's like, okay, that is like really stupid
[05:01:55.380 --> 05:01:57.060]   compared to what's going on now.
[05:01:57.060 --> 05:01:59.880]   - And now you could probably, with natural language,
[05:01:59.880 --> 05:02:02.140]   tell it to learn to fly and it's going to generate
[05:02:02.140 --> 05:02:04.020]   the control algorithm required to do that.
[05:02:04.020 --> 05:02:05.140]   - Probably.
[05:02:05.140 --> 05:02:06.340]   There's low level blockers.
[05:02:06.340 --> 05:02:08.180]   Like we had to do some weird stuff for that.
[05:02:08.180 --> 05:02:09.620]   But you can, you definitely can.
[05:02:09.620 --> 05:02:11.260]   - Go back to our robotics conversation.
[05:02:11.260 --> 05:02:12.340]   Yeah, when you have to interact
[05:02:12.340 --> 05:02:14.220]   in actual physical world, it's hard.
[05:02:14.220 --> 05:02:18.300]   What gives you hope about the future of human civilization?
[05:02:18.300 --> 05:02:23.300]   Looking into the next 10 years, 100 years, 1,000 years,
[05:02:23.300 --> 05:02:25.780]   how long do you think we'll make it?
[05:02:25.780 --> 05:02:27.940]   You think we've got 1,000 years?
[05:02:27.940 --> 05:02:30.420]   - Humans will definitely be around in 1,000 years.
[05:02:30.420 --> 05:02:33.500]   I think there's ways that very bad things could happen
[05:02:33.500 --> 05:02:35.060]   and there'll be way fewer humans.
[05:02:35.060 --> 05:02:37.180]   But humans are very good at surviving.
[05:02:37.180 --> 05:02:40.740]   There's been a lot of things that that is true.
[05:02:40.740 --> 05:02:42.140]   I don't think they're necessarily,
[05:02:42.140 --> 05:02:45.300]   we're good at long-term credit assignment of risk.
[05:02:45.300 --> 05:02:47.420]   But when the risk becomes immediate,
[05:02:47.420 --> 05:02:48.860]   we tend to figure things out.
[05:02:48.860 --> 05:02:53.180]   And for that reason, there's physical constraints
[05:02:53.180 --> 05:02:57.780]   to things like AGI, recursive improvement
[05:02:57.780 --> 05:02:59.880]   to kill us all type stuff.
[05:02:59.880 --> 05:03:01.860]   For the physical reasons and for how humans
[05:03:01.860 --> 05:03:02.980]   have figured things out before,
[05:03:02.980 --> 05:03:05.620]   I'm not too worried about AI takeover.
[05:03:05.620 --> 05:03:07.740]   There are other international things that are worrying,
[05:03:07.740 --> 05:03:12.340]   but there's just fundamental human goodness
[05:03:12.340 --> 05:03:14.820]   and trying to amplify that.
[05:03:14.820 --> 05:03:17.140]   We're on a tenuous time.
[05:03:17.140 --> 05:03:20.860]   And I mean, if you look at humanity as a whole,
[05:03:20.860 --> 05:03:23.100]   there's been times where things go backwards.
[05:03:23.100 --> 05:03:24.980]   There's times when things don't happen at all
[05:03:24.980 --> 05:03:26.860]   and we're on what should be
[05:03:26.860 --> 05:03:28.780]   very positive trajectory right now.
[05:03:28.780 --> 05:03:30.200]   - Yeah, there seems to be progress,
[05:03:30.200 --> 05:03:33.100]   but just like with power,
[05:03:33.100 --> 05:03:35.820]   there's spikes of human suffering.
[05:03:35.820 --> 05:03:38.540]   And we wanna try to minimize the amount of spikes.
[05:03:38.540 --> 05:03:41.660]   - Generally, humanity is gonna suffer a lot less.
[05:03:41.660 --> 05:03:43.320]   I'm very optimistic about that.
[05:03:43.320 --> 05:03:47.620]   I do worry of techno-fascism type stuff arising
[05:03:47.620 --> 05:03:51.340]   as AI becomes more and more prevalent and powerful
[05:03:51.340 --> 05:03:54.200]   and those who control it can do more and more.
[05:03:54.200 --> 05:03:55.700]   Maybe it doesn't kill us all,
[05:03:55.700 --> 05:03:59.140]   but at some point, every very powerful human
[05:03:59.140 --> 05:04:00.860]   is gonna wanna brain-computer interface
[05:04:00.860 --> 05:04:03.020]   so that they can interact with the AGI
[05:04:03.020 --> 05:04:04.940]   and all of its advantages in many more way
[05:04:04.940 --> 05:04:07.100]   and merge its mind with sort of like,
[05:04:07.100 --> 05:04:09.980]   and its capabilities or that person's capabilities
[05:04:09.980 --> 05:04:12.080]   can leverage those much better than anyone else
[05:04:12.080 --> 05:04:14.820]   and therefore won't be one person rule them all,
[05:04:14.820 --> 05:04:16.580]   but it will be...
[05:04:16.580 --> 05:04:19.540]   The thing I worry about is it'll be like few people,
[05:04:19.540 --> 05:04:21.220]   hundreds, thousands, tens of thousands,
[05:04:21.220 --> 05:04:24.540]   maybe millions of people rule whoever's left, right?
[05:04:24.540 --> 05:04:27.340]   And the economy around it, right?
[05:04:27.340 --> 05:04:29.220]   And I think that's like the thing
[05:04:29.220 --> 05:04:30.280]   that's probably more worrisome
[05:04:30.280 --> 05:04:33.660]   is like human machine amalgamations.
[05:04:33.660 --> 05:04:35.580]   This enables an individual human
[05:04:35.580 --> 05:04:37.180]   to have more impact on the world
[05:04:37.180 --> 05:04:40.500]   and that impact can be both positive and negative, right?
[05:04:40.500 --> 05:04:42.560]   Generally, humans have positive impacts on the world,
[05:04:42.560 --> 05:04:44.100]   at least societally,
[05:04:44.100 --> 05:04:45.900]   but it's possible for individual humans
[05:04:45.900 --> 05:04:49.060]   to have such negative impacts and AGI,
[05:04:49.060 --> 05:04:51.120]   at least as I think the labs define it,
[05:04:51.120 --> 05:04:53.440]   which is not a runaway sentient thing,
[05:04:53.440 --> 05:04:54.620]   but rather just something
[05:04:54.620 --> 05:04:56.920]   that can do a lot of tasks really efficiently,
[05:04:57.960 --> 05:04:59.340]   amplifies the capabilities
[05:04:59.340 --> 05:05:02.080]   of someone causing extreme damage.
[05:05:02.080 --> 05:05:03.160]   But for the most part,
[05:05:03.160 --> 05:05:06.160]   I think it'll be used for profit-seeking motives,
[05:05:06.160 --> 05:05:07.220]   which will then reduce,
[05:05:07.220 --> 05:05:09.040]   which will increase the abundance and supply of things
[05:05:09.040 --> 05:05:10.940]   and therefore reduce suffering, right?
[05:05:10.940 --> 05:05:12.780]   That's the goal.
[05:05:12.780 --> 05:05:15.400]   - Scrolling on a timeline,
[05:05:15.400 --> 05:05:18.320]   just throttling your dopamine. - Scrolling is stasis.
[05:05:18.320 --> 05:05:20.520]   - Scrolling holds the status quo of the world.
[05:05:20.520 --> 05:05:22.520]   - That is a positive outcome, right?
[05:05:22.520 --> 05:05:24.280]   It's like if I have food tubes
[05:05:24.280 --> 05:05:25.860]   and I'm scrolling and I'm happy,
[05:05:25.860 --> 05:05:27.160]   that's a positive outcome.
[05:05:27.680 --> 05:05:28.720]   (both laughing)
[05:05:28.720 --> 05:05:32.080]   - While expanding out into the cosmos.
[05:05:32.080 --> 05:05:35.520]   Well, this is a fun time to be alive
[05:05:35.520 --> 05:05:37.840]   and thank you for pushing the forefront
[05:05:37.840 --> 05:05:39.840]   of what is possible in humans
[05:05:39.840 --> 05:05:41.120]   and thank you for talking today.
[05:05:41.120 --> 05:05:41.960]   This was fun.
[05:05:41.960 --> 05:05:43.980]   - Thanks for having us. - Thanks for having us.
[05:05:43.980 --> 05:05:45.560]   - Thanks for listening to this conversation
[05:05:45.560 --> 05:05:48.520]   with Dylan Patel and Nathan Lambert.
[05:05:48.520 --> 05:05:49.880]   To support this podcast,
[05:05:49.880 --> 05:05:52.640]   please check out our sponsors in the description.
[05:05:52.640 --> 05:05:55.080]   And now let me leave you with some words
[05:05:55.080 --> 05:05:56.540]   from Richard Feynman.
[05:05:57.300 --> 05:05:59.520]   For a successful technology,
[05:05:59.520 --> 05:06:03.120]   reality must take precedence over public relations.
[05:06:03.120 --> 05:06:05.460]   For nature cannot be fooled.
[05:06:05.460 --> 05:06:07.620]   Thank you for listening.
[05:06:07.620 --> 05:06:09.540]   I hope to see you next time.
[05:06:09.540 --> 05:06:12.120]   (upbeat music)
[05:06:12.120 --> 05:06:14.700]   (upbeat music)
[05:06:14.700 --> 05:06:24.700]   [BLANK_AUDIO]

