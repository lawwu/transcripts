
[00:00:00.000 --> 00:00:06.560]   Throw the data at it, get an auto ML baseline, and then see if you can do better.
[00:00:06.560 --> 00:00:08.160]   And maybe you'll be surprised, maybe you can't, right?
[00:00:08.160 --> 00:00:10.040]   But your job is not to build models.
[00:00:10.040 --> 00:00:11.920]   Your job is to have a business impact.
[00:00:11.920 --> 00:00:16.280]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:16.280 --> 00:00:18.520]   and I'm your host, Lukas Biewald.
[00:00:18.520 --> 00:00:23.760]   Today I'm talking to Jordan Fisher, who is the CEO and co-founder of Standard AI.
[00:00:23.760 --> 00:00:29.400]   Standard AI is an autonomous checkout company that actually has autonomous checkout working.
[00:00:29.400 --> 00:00:34.880]   So this is an amazing conversation with someone who has big deep learning models really working
[00:00:34.880 --> 00:00:39.360]   in the real world, in real conditions, and it's a very informative conversation.
[00:00:39.360 --> 00:00:41.560]   Jordan, it's good to talk to you.
[00:00:41.560 --> 00:00:42.560]   It's been a long time.
[00:00:42.560 --> 00:00:43.560]   Yeah.
[00:00:43.560 --> 00:00:48.520]   I thought it might be good to start by saying a little bit about what Standard AI is for
[00:00:48.520 --> 00:00:49.520]   folks who don't know.
[00:00:49.520 --> 00:00:50.520]   Yeah.
[00:00:50.520 --> 00:00:53.960]   So I'll give you the quick spiel on Standard.
[00:00:53.960 --> 00:00:58.200]   We build computer vision powered checkout for retail.
[00:00:58.200 --> 00:01:01.520]   A lot of people have heard of Amazon Go, so we're sort of Go for everyone else is the
[00:01:01.520 --> 00:01:04.360]   way we think about ourselves.
[00:01:04.360 --> 00:01:07.400]   In particular, we're trying to make it easy to just pick it up and put it into existing
[00:01:07.400 --> 00:01:08.400]   stores.
[00:01:08.400 --> 00:01:10.120]   So it's this camera only system.
[00:01:10.120 --> 00:01:13.680]   We install it onto the ceilings of existing stores, and then we do all this magic behind
[00:01:13.680 --> 00:01:17.800]   the scenes to ultimately figure out what do people have so they can get on with their
[00:01:17.800 --> 00:01:21.520]   day and skip the line and get their receipt automatically.
[00:01:21.520 --> 00:01:24.920]   And I guess, what's the founding story?
[00:01:24.920 --> 00:01:27.880]   There's two pieces to this.
[00:01:27.880 --> 00:01:30.120]   Piece one is I despise lines.
[00:01:30.120 --> 00:01:34.400]   I mean, life is short.
[00:01:34.400 --> 00:01:38.720]   It's just, I struggle seeing this wasted human capital.
[00:01:38.720 --> 00:01:42.960]   We spend literally billions of hours waiting in line every year.
[00:01:42.960 --> 00:01:46.560]   It's pretty staggering when you stack it all up at once and just look at it, that amount
[00:01:46.560 --> 00:01:50.280]   of human capital just literally incinerated.
[00:01:50.280 --> 00:01:52.480]   And we're doing it just for the sake of commerce.
[00:01:52.480 --> 00:01:54.040]   We're just waiting in line doing nothing.
[00:01:54.040 --> 00:01:57.440]   And there's another person on the other side just waiting for us to get there to then do
[00:01:57.440 --> 00:01:58.560]   this transaction.
[00:01:58.560 --> 00:02:00.040]   It's the most mind boggling thing.
[00:02:00.040 --> 00:02:02.280]   So that just sucks.
[00:02:02.280 --> 00:02:04.440]   Life is short and we shouldn't spend it waiting in line.
[00:02:04.440 --> 00:02:08.560]   So I think that's the obvious piece.
[00:02:08.560 --> 00:02:14.480]   But the real sort of inception of Standard was more tech driven.
[00:02:14.480 --> 00:02:20.840]   So we had a really cool tech team that I was working with closely at the SEC and a few
[00:02:20.840 --> 00:02:23.920]   folks who were sort of in the surrounding industry.
[00:02:23.920 --> 00:02:28.240]   And this was going back six-ish years now, and we just saw this revolution happening
[00:02:28.240 --> 00:02:30.920]   in ML, obviously, but really computer vision at the time.
[00:02:30.920 --> 00:02:35.080]   I felt it was having this moment where it was just so clear that everything was about
[00:02:35.080 --> 00:02:39.360]   to change, that you could suddenly reach human parity almost in some of these tasks.
[00:02:39.360 --> 00:02:43.680]   And you're like, wow, if that's true, if humans and machines can sort of see equally well,
[00:02:43.680 --> 00:02:44.880]   what does that mean for the world?
[00:02:44.880 --> 00:02:46.840]   And it should change basically everything.
[00:02:46.840 --> 00:02:51.880]   Every industry where you can put a camera and see stuff should get revolutionized by
[00:02:51.880 --> 00:02:54.720]   this coming revolution.
[00:02:54.720 --> 00:02:56.040]   That was our really strong conviction.
[00:02:56.040 --> 00:02:57.640]   So we didn't have a particular product in mind.
[00:02:57.640 --> 00:03:00.000]   We just sat down and said, okay, we're going to...
[00:03:00.000 --> 00:03:01.320]   We weren't computer vision experts.
[00:03:01.320 --> 00:03:02.320]   We were just ML experts.
[00:03:02.320 --> 00:03:04.680]   We were building ML for the SEC.
[00:03:04.680 --> 00:03:07.440]   And we said, okay, well, we're just going to retool.
[00:03:07.440 --> 00:03:10.500]   We sat down and for about a year, we just read every computer vision research paper
[00:03:10.500 --> 00:03:12.680]   that was coming out.
[00:03:12.680 --> 00:03:19.320]   And then we had one business guy, a really good old colleague of mine, who was sitting
[00:03:19.320 --> 00:03:24.480]   in on the brainstorming sessions and was just helping us do the market analysis, the TAM,
[00:03:24.480 --> 00:03:26.640]   and to kind of figure out what was what.
[00:03:26.640 --> 00:03:31.160]   And we had a bunch of really dumb ideas, which I'll tell you about only over a beer sometime.
[00:03:31.160 --> 00:03:32.160]   Tell me one right now.
[00:03:32.160 --> 00:03:35.160]   I want to hear one.
[00:03:35.160 --> 00:03:40.560]   Actually, so one of them is starting to become more real in a more like home setting, but
[00:03:40.560 --> 00:03:43.120]   I was really passionate about this idea, which was smart gyms.
[00:03:43.120 --> 00:03:44.280]   And this idea of...
[00:03:44.280 --> 00:03:48.120]   Because we were pretty sold already on overhead cameras being the modality that we wanted.
[00:03:48.120 --> 00:03:51.760]   And we're like, where can you just put overhead cameras and enable a cool experience?
[00:03:51.760 --> 00:03:52.760]   Like, well, gyms, right?
[00:03:52.760 --> 00:03:58.080]   Just put the cameras up and then you put your AirPod in your ear and you should be able
[00:03:58.080 --> 00:04:02.960]   to walk into a gym and your synthetic personal trainer just starts talking to you.
[00:04:02.960 --> 00:04:04.240]   It knows what you've done in the past.
[00:04:04.240 --> 00:04:05.760]   It starts counting your reps.
[00:04:05.760 --> 00:04:10.880]   It pushes you to do one more rep or go for the 15 pound instead of the 12.5 pound.
[00:04:10.880 --> 00:04:15.040]   And it takes all the drudgery out of doing the self-tracking, which no one wants to do,
[00:04:15.040 --> 00:04:16.800]   and can also kind of push you to do more.
[00:04:16.800 --> 00:04:19.600]   And then help you with proper form, et cetera.
[00:04:19.600 --> 00:04:23.960]   We're starting to see this now in at-home gyms where there can be a camera that will
[00:04:23.960 --> 00:04:24.960]   help coach you.
[00:04:24.960 --> 00:04:30.320]   But I don't think we've seen it yet in the full gym setting.
[00:04:30.320 --> 00:04:37.240]   But then you decided to do this checkout list store idea.
[00:04:37.240 --> 00:04:39.160]   Was it obvious that that was the best idea?
[00:04:39.160 --> 00:04:40.160]   How did you come to it?
[00:04:40.160 --> 00:04:42.160]   How did you validate it?
[00:04:42.160 --> 00:04:46.240]   It was super obvious because when you start running the numbers, it's just wild how big
[00:04:46.240 --> 00:04:47.240]   the opportunity is.
[00:04:47.240 --> 00:04:49.120]   And it's only gotten bigger since then.
[00:04:49.120 --> 00:04:50.120]   Checkout on its own is huge.
[00:04:50.120 --> 00:04:55.880]   And like I said, it's literally billions of hours a year across the world.
[00:04:55.880 --> 00:04:56.880]   It's massive.
[00:04:56.880 --> 00:05:00.520]   And we spend hundreds of billions of dollars to make that happen.
[00:05:00.520 --> 00:05:03.720]   And there's so much else that needs to happen in stores.
[00:05:03.720 --> 00:05:09.720]   That human capital has an insane number of things that we need from it in these stores.
[00:05:09.720 --> 00:05:14.480]   So stocking the store and customer support and refacing, et cetera.
[00:05:14.480 --> 00:05:17.160]   And we hear that now that we're talking to retailers, they want all these things.
[00:05:17.160 --> 00:05:20.560]   Of course, they want autonomous checkout because they don't want to line either and they want
[00:05:20.560 --> 00:05:22.480]   to do the best thing for their shoppers.
[00:05:22.480 --> 00:05:25.720]   But they also just can't even staff their stores effectively right now.
[00:05:25.720 --> 00:05:30.760]   There are retailers that have 10, 20,000 plus open positions right now.
[00:05:30.760 --> 00:05:35.600]   And our product teams especially go and interview not just the retailers, but employees in the
[00:05:35.600 --> 00:05:36.600]   store.
[00:05:36.600 --> 00:05:40.040]   And they get super excited as much as anyone else about autonomous checkout because they're
[00:05:40.040 --> 00:05:43.320]   like, "Oh, if we have this in my store, I get to go do all the things that I want to
[00:05:43.320 --> 00:05:44.320]   do."
[00:05:44.320 --> 00:05:46.040]   I get to go interact with customers.
[00:05:46.040 --> 00:05:50.200]   I have my locals, my regulars that I really like talking to.
[00:05:50.200 --> 00:05:54.800]   I can finally have a spare minute to go fix the out of stocks that are actually hurting
[00:05:54.800 --> 00:05:59.360]   the bottom line of the store because someone walks in and wants their Snickers bar and
[00:05:59.360 --> 00:06:01.000]   we haven't had a chance to restock it.
[00:06:01.000 --> 00:06:03.800]   That's a massive hit to retail.
[00:06:03.800 --> 00:06:05.400]   So really everyone's super excited about it.
[00:06:05.400 --> 00:06:10.600]   And all of these industries, these retail tech industries can be done better with computer
[00:06:10.600 --> 00:06:11.600]   vision powering them.
[00:06:11.600 --> 00:06:15.840]   It's inventory and out of stock and loss prevention and insights and analytics.
[00:06:15.840 --> 00:06:16.840]   And then it's also checkout.
[00:06:16.840 --> 00:06:20.400]   So it's just like, we started pulling back all the layers of this onion.
[00:06:20.400 --> 00:06:25.400]   We're like, "This is really going to just change the entire $25 trillion physical retail
[00:06:25.400 --> 00:06:27.080]   industry."
[00:06:27.080 --> 00:06:30.200]   Every aspect of it gets better once you have the smart system in the store.
[00:06:30.200 --> 00:06:33.680]   So we were just like, "This is insane."
[00:06:33.680 --> 00:06:34.680]   That was one of our metrics, obviously.
[00:06:34.680 --> 00:06:36.080]   It was like huge TAM.
[00:06:36.080 --> 00:06:41.380]   Another one of our metrics was we wanted it to be a really hard tech problem.
[00:06:41.380 --> 00:06:44.400]   Just from a personal satisfaction place, we love working on hard tech.
[00:06:44.400 --> 00:06:50.000]   But then my personal preference is working in super rarified industries where there's
[00:06:50.000 --> 00:06:55.920]   a huge barrier to entry from a technical challenge perspective because it rarifies the air.
[00:06:55.920 --> 00:06:59.120]   There's only a handful of teams that are going to be competing with you.
[00:06:59.120 --> 00:07:03.640]   So it was that sweet spot of like, "This is really hard, but it's not quite as hard as
[00:07:03.640 --> 00:07:08.680]   autonomous vehicles where we're going to be bashing your head against this for a decade
[00:07:08.680 --> 00:07:11.480]   and probably need to go raise a trillion dollars to compete with Waymo."
[00:07:11.480 --> 00:07:16.480]   So it hit all that, hit the sweet spot of all those things.
[00:07:16.480 --> 00:07:21.160]   So is the challenge to actually see when someone takes something off the shelf, is that the
[00:07:21.160 --> 00:07:25.440]   challenge or is there a point where you show it to a camera and then check out?
[00:07:25.440 --> 00:07:26.960]   How does the experience work?
[00:07:26.960 --> 00:07:27.960]   Yeah.
[00:07:27.960 --> 00:07:31.080]   From the experience perspective, we're really trying to make it just completely seamless.
[00:07:31.080 --> 00:07:34.480]   You forget that you're doing this, that you're shopping.
[00:07:34.480 --> 00:07:37.160]   The goal is to make it feel like it's your personal pantry.
[00:07:37.160 --> 00:07:42.120]   Just walk in, grab stuff, put it in your jacket, put it in your pocket, put it in your purse.
[00:07:42.120 --> 00:07:44.040]   You no longer think about transacting.
[00:07:44.040 --> 00:07:49.720]   We're hoping that it does to shopping what Uber and Lyft did to taxis.
[00:07:49.720 --> 00:07:52.600]   You're still transacting, but you don't think about that transaction moment anymore.
[00:07:52.600 --> 00:07:56.240]   You're just hitting a button, a car shows up, you get in, you get out.
[00:07:56.240 --> 00:08:00.600]   It's just so seamless that now you take a Lyft more than you take a taxi.
[00:08:00.600 --> 00:08:01.640]   You're growing the pie.
[00:08:01.640 --> 00:08:03.840]   And that's what we're really hoping to do for retail.
[00:08:03.840 --> 00:08:08.080]   So what do you do on day one when you decide to make a company that does this?
[00:08:08.080 --> 00:08:09.680]   What was the next step?
[00:08:09.680 --> 00:08:14.760]   Did you go talk to stores and try to get them to install cameras and run ML models?
[00:08:14.760 --> 00:08:15.760]   How does it work?
[00:08:15.760 --> 00:08:16.760]   We did.
[00:08:16.760 --> 00:08:17.760]   Yeah, we did actually.
[00:08:17.760 --> 00:08:18.760]   Yeah.
[00:08:18.760 --> 00:08:23.360]   So we actually had a pretty big co-founding team, but Michael, who was our chief business
[00:08:23.360 --> 00:08:27.880]   officer was the business guy, quote unquote.
[00:08:27.880 --> 00:08:31.680]   And me and Michael, we were in New York at the time and we were actually still...
[00:08:31.680 --> 00:08:36.800]   We hadn't quit our jobs yet, so we didn't have enough conviction.
[00:08:36.800 --> 00:08:38.400]   That came shortly thereafter.
[00:08:38.400 --> 00:08:44.560]   But yeah, we just walked around Williamsburg and Brooklyn and just started talking to store
[00:08:44.560 --> 00:08:45.840]   owners and it was Saturday.
[00:08:45.840 --> 00:08:51.280]   So the very first thing we learned about retail was you don't bother retailers on a Saturday
[00:08:51.280 --> 00:08:54.240]   because it's like, that's the most important day of the week for them to sell stuff.
[00:08:54.240 --> 00:08:59.440]   So we were just going in talking to store managers and then they're like, "This sounds
[00:08:59.440 --> 00:09:03.160]   cool, I guess, but you need to get out of my store right now.
[00:09:03.160 --> 00:09:04.160]   I got stuff to sell."
[00:09:04.160 --> 00:09:07.160]   So we started coming back on Mondays and Tuesdays.
[00:09:07.160 --> 00:09:13.800]   And yeah, I mean, you go talk to retailers anywhere, even five years ago, six years ago,
[00:09:13.800 --> 00:09:16.040]   and it was already clear that everyone wanted this.
[00:09:16.040 --> 00:09:18.360]   And we were super lucky.
[00:09:18.360 --> 00:09:22.480]   Once our name just got out, once we incorporated and put out even just a little bit of videos
[00:09:22.480 --> 00:09:27.800]   about what we were doing, we just got insane inbound from basically all of the retailers
[00:09:27.800 --> 00:09:28.800]   in the world.
[00:09:28.800 --> 00:09:37.400]   And it was small mom and pops all the way up to mega fortune 10 companies.
[00:09:37.400 --> 00:09:41.520]   That's how we knew that if we can build this, at the time it was not clear that we'd succeed,
[00:09:41.520 --> 00:09:47.920]   but if we could build this, then yes, there is this ridiculous, amazing demand at the
[00:09:47.920 --> 00:09:48.920]   end of the rainbow.
[00:09:48.920 --> 00:09:51.280]   And so where are you at?
[00:09:51.280 --> 00:09:55.280]   Can I go to a standard AI store and pull stuff off the shelves?
[00:09:55.280 --> 00:09:56.280]   Yeah, yeah, for sure.
[00:09:56.280 --> 00:09:58.480]   I mean, it is hard tech, right?
[00:09:58.480 --> 00:10:03.400]   So we're five years in and we haven't deployed everywhere yet, but I like to call this space
[00:10:03.400 --> 00:10:04.400]   AV light.
[00:10:04.400 --> 00:10:07.040]   So you probably have a lot of AV folks on your store.
[00:10:07.040 --> 00:10:09.720]   So I'm going to be incessantly pinching them.
[00:10:09.720 --> 00:10:15.640]   Everyone in AV should come over to Autonomous Checkout because the time is now, but we get
[00:10:15.640 --> 00:10:16.760]   to go to market faster.
[00:10:16.760 --> 00:10:19.520]   So actually our tech is probably not as advanced yet as AV.
[00:10:19.520 --> 00:10:20.760]   I think we're pretty sophisticated.
[00:10:20.760 --> 00:10:24.800]   We do some really cool stuff, but we haven't invested quite as much as the way most of
[00:10:24.800 --> 00:10:25.800]   the world.
[00:10:25.800 --> 00:10:28.920]   But we get to go to market faster because we can make a mistake.
[00:10:28.920 --> 00:10:31.120]   And if so, someone gets ketchup for free.
[00:10:31.120 --> 00:10:33.720]   It's like, it's actually an okay experience for someone.
[00:10:33.720 --> 00:10:35.440]   And retailers are used to it as well.
[00:10:35.440 --> 00:10:41.040]   Like they have a built-in margin that they expect to lose because there's loss and theft
[00:10:41.040 --> 00:10:44.800]   and mistakes and breakage, et cetera.
[00:10:44.800 --> 00:10:46.720]   So it's just a really more friendly place to be.
[00:10:46.720 --> 00:10:50.680]   So we're just now exiting MVP stage.
[00:10:50.680 --> 00:10:54.080]   We're at 10 stores now that we've launched in with real retailers.
[00:10:54.080 --> 00:10:57.280]   They're just regular stores that we showed up and installed our cameras and transformed
[00:10:57.280 --> 00:10:58.280]   them.
[00:10:58.280 --> 00:11:02.200]   And one here in the Bay area, actually at San Jose State University, we just launched
[00:11:02.200 --> 00:11:07.560]   about two months ago, which was super awesome because we have more adoption from that one
[00:11:07.560 --> 00:11:09.600]   store than our other 10 combined.
[00:11:09.600 --> 00:11:13.600]   Because the students are just like, they love, you know, they're like, yes, early adopters.
[00:11:13.600 --> 00:11:14.600]   Great.
[00:11:14.600 --> 00:11:18.400]   So we have like 500 people using our system every single day, just at that one store,
[00:11:18.400 --> 00:11:20.240]   which is super exciting.
[00:11:20.240 --> 00:11:26.520]   So it's still early, but at the same time, we're exiting MVP and really starting to ramp
[00:11:26.520 --> 00:11:28.640]   up with our retail clients.
[00:11:28.640 --> 00:11:33.080]   They're finally seeing the tech work in their own stores or their competitor stores.
[00:11:33.080 --> 00:11:35.760]   And they're getting really excited about how much shoppers love this.
[00:11:35.760 --> 00:11:39.400]   And also all these other value props that we have been pitching for the last five years
[00:11:39.400 --> 00:11:41.480]   around inventory, et cetera.
[00:11:41.480 --> 00:11:45.680]   They're finally seeing it now and we're growing into that and starting to expand.
[00:11:45.680 --> 00:11:47.800]   So that's really exciting.
[00:11:47.800 --> 00:11:57.280]   And so in those five years that you've been working on it, what's unlocked the ability
[00:11:57.280 --> 00:11:58.520]   to put it into stores?
[00:11:58.520 --> 00:12:03.080]   Has it really been like kind of making the models more accurate or something else?
[00:12:03.080 --> 00:12:05.080]   It's a whole slew of things for sure.
[00:12:05.080 --> 00:12:07.720]   Like, you know, there's product work for sure.
[00:12:07.720 --> 00:12:10.620]   Because, you know, at the end of the day, there is like real experience and the way
[00:12:10.620 --> 00:12:14.520]   that you're presenting it to people in the store matters.
[00:12:14.520 --> 00:12:17.400]   There was also just some go-to-market aspect of it too, right?
[00:12:17.400 --> 00:12:20.280]   Where when we started, we were like, we're just going to put this in every store in the
[00:12:20.280 --> 00:12:23.040]   world, which is our intent.
[00:12:23.040 --> 00:12:25.200]   But we're like, so let's go sign deals with everyone.
[00:12:25.200 --> 00:12:32.040]   And, you know, we were going out and talking to, you know, mega, you know, like 500,000
[00:12:32.040 --> 00:12:35.920]   square foot stores and, you know, like mega grocery stores.
[00:12:35.920 --> 00:12:39.920]   And then, you know, we had to kind of take a step back and say, well, look, this is cutting
[00:12:39.920 --> 00:12:40.920]   edge tech.
[00:12:40.920 --> 00:12:42.660]   We need to start a little bit smaller.
[00:12:42.660 --> 00:12:46.260]   We can still partner with big companies and mom and pops, but let's go after convenience
[00:12:46.260 --> 00:12:47.560]   stores to start with.
[00:12:47.560 --> 00:12:49.480]   You know, it's a smaller footprint.
[00:12:49.480 --> 00:12:53.200]   We don't need as many cameras, et cetera, smaller number of items.
[00:12:53.200 --> 00:12:56.520]   So there was a little bit of kind of a reality check there that we should start a little
[00:12:56.520 --> 00:12:58.040]   bit smaller, which we, you know, we did.
[00:12:58.040 --> 00:13:02.560]   So now we're, you know, kind of mastered convenience stores and are going to be expanding from
[00:13:02.560 --> 00:13:03.560]   there.
[00:13:03.560 --> 00:13:08.680]   But yeah, I mean, for sure, like the engineering, the machine learning, the operations, you
[00:13:08.680 --> 00:13:14.320]   know, I think, you know, for me, operations are always a super under appreciated aspect
[00:13:14.320 --> 00:13:16.000]   of ML, right?
[00:13:16.000 --> 00:13:20.880]   Like you just got to go heavy on ops and, you know, care a lot about your data, care
[00:13:20.880 --> 00:13:24.880]   a lot about your labels and your quality.
[00:13:24.880 --> 00:13:26.800]   That's been super important for us.
[00:13:26.800 --> 00:13:27.800]   Interesting.
[00:13:27.800 --> 00:13:30.560]   So when you say ops, you mean labeling?
[00:13:30.560 --> 00:13:32.560]   That's like the primary ops component?
[00:13:32.560 --> 00:13:33.560]   Yeah.
[00:13:33.560 --> 00:13:34.560]   Tons of labeling for sure.
[00:13:34.560 --> 00:13:38.440]   I mean, we definitely have big data sets and we have a little bit of a hiddle to, as part
[00:13:38.440 --> 00:13:42.480]   of our live system, which is another kind of, I guess you see head on some AV systems
[00:13:42.480 --> 00:13:46.360]   as well, where there can be a disengagement and then, you know, a remote pilot will take
[00:13:46.360 --> 00:13:47.360]   over.
[00:13:47.360 --> 00:13:51.520]   But for us, that's actually a much easier part of the process because you don't, you're
[00:13:51.520 --> 00:13:56.120]   not driving a car, so you don't need this like 10 millisecond response time.
[00:13:56.120 --> 00:13:58.960]   We just need to get someone to receive it in the next, you know, five to 10 minutes.
[00:13:58.960 --> 00:14:03.720]   So, you know, if we kick off a background thread and have a human take a look at something
[00:14:03.720 --> 00:14:05.280]   like that's, that's totally fine.
[00:14:05.280 --> 00:14:08.600]   And then, you know, that's another label that we can throw back into the system.
[00:14:08.600 --> 00:14:11.960]   So it's sort of all self-feeding.
[00:14:11.960 --> 00:14:16.760]   When you set up a system in a new store, do you have to train it on the particular inventory
[00:14:16.760 --> 00:14:17.760]   in that store?
[00:14:17.760 --> 00:14:19.320]   Or I guess even inventory can change over time.
[00:14:19.320 --> 00:14:22.280]   Do you kind of keep retraining your systems?
[00:14:22.280 --> 00:14:23.560]   Yeah, for sure.
[00:14:23.560 --> 00:14:27.120]   I mean, even the stuff that's not, so the items definitely change, the SKU set and the
[00:14:27.120 --> 00:14:31.600]   catalog change, but even the, even the stuff that you would hope would be more generic
[00:14:31.600 --> 00:14:33.800]   is not quite as generic as you'd want.
[00:14:33.800 --> 00:14:39.040]   So, you know, our, you know, our, our people detection and people tracking systems are
[00:14:39.040 --> 00:14:40.040]   in theory, fully generic.
[00:14:40.040 --> 00:14:44.320]   Like we show up at the store, we install cameras, we flip a switch and, you know, we basically
[00:14:44.320 --> 00:14:49.040]   have a multi-view tracking system that can, you know, fully anonymously track 20, 30,
[00:14:49.040 --> 00:14:54.160]   40 people within a space in real time, which is super, super cool.
[00:14:54.160 --> 00:14:58.320]   But nonetheless, it does get better if you fine tune on that store, right?
[00:14:58.320 --> 00:15:02.120]   So we'll, you know, we'll go in and over the course of that, the first month or two, we'll
[00:15:02.120 --> 00:15:06.920]   label a little bit of data, fine tune the model, and then redeploy to that store.
[00:15:06.920 --> 00:15:09.240]   And you do get, you do get a boost by doing that.
[00:15:09.240 --> 00:15:12.120]   I think at some point you probably start seeing diminishing returns, you know, we're only
[00:15:12.120 --> 00:15:17.680]   at 10 stores, but presumably at a thousand stores or 10,000 stores, you know, that, that
[00:15:17.680 --> 00:15:23.040]   human model is going to be so general that there's probably no, no point in fine tuning
[00:15:23.040 --> 00:15:24.960]   on, on a per store environment.
[00:15:24.960 --> 00:15:28.120]   But when it comes to products, like you said, like there could be a different product in
[00:15:28.120 --> 00:15:31.160]   every single store that plateaus too.
[00:15:31.160 --> 00:15:35.640]   So, you know, to give you kind of a, some rough numbers, a C store can have maybe 5,000
[00:15:35.640 --> 00:15:39.920]   unique SKUs in their store, and they're going to have maybe 30,000 unique SKUs across their
[00:15:39.920 --> 00:15:42.240]   fleet, but that fleet might be a thousand stores.
[00:15:42.240 --> 00:15:46.240]   So you sort of, you get a, you get a pretty good economy of scales once you start getting
[00:15:46.240 --> 00:15:50.120]   to fleet scale, because you're only going to, to go from one store to the full fleet,
[00:15:50.120 --> 00:15:54.240]   you're only going to six X the size of your catalog, but you're going to 1000 X the size
[00:15:54.240 --> 00:15:55.880]   of your deployments.
[00:15:55.880 --> 00:16:00.360]   So it's, it pays off in the long run, but it's super expensive when you're only in 10
[00:16:00.360 --> 00:16:01.960]   stores, like we are.
[00:16:01.960 --> 00:16:07.040]   So we work really hard to stay on top of those ops of, you know, the churn of new SKUs that
[00:16:07.040 --> 00:16:15.040]   are showing up and, you know, it's the Easter version of the Snickers bar and it's just
[00:16:15.040 --> 00:16:17.080]   constantly, constantly churning for sure.
[00:16:17.080 --> 00:16:22.000]   What do you do in that time when it's like training that month or two where, where people
[00:16:22.000 --> 00:16:24.680]   are coming in, like, is it all sort of human operated at that point?
[00:16:24.680 --> 00:16:30.720]   And then it gradually seeds to the ML algorithms or do people have some other mechanism for
[00:16:30.720 --> 00:16:32.480]   paying?
[00:16:32.480 --> 00:16:37.520]   So what's cool is we, we run in the background because it's, we're showing up at existing
[00:16:37.520 --> 00:16:38.520]   stores.
[00:16:38.520 --> 00:16:39.880]   We're not building a new store, right?
[00:16:39.880 --> 00:16:41.680]   So the same store is there.
[00:16:41.680 --> 00:16:45.340]   We're not getting rid of the existing point of sale system, the existing checkout system.
[00:16:45.340 --> 00:16:49.680]   So we install our cameras, we're doing our things behind the scene.
[00:16:49.680 --> 00:16:51.360]   And then the store is just running as is, right?
[00:16:51.360 --> 00:16:54.640]   So it's, it's only when we're ready and, you know, we, we showed the retailer that we'd
[00:16:54.640 --> 00:16:58.080]   reached a certain accuracy that we flipped the switch on.
[00:16:58.080 --> 00:17:01.360]   But then even then, once we flipped the switch, it's not a hard crossover, you know, so, which
[00:17:01.360 --> 00:17:06.240]   actually is really nice for us because there's still people, you know, Apple pay has like
[00:17:06.240 --> 00:17:08.160]   6% adoption or something right now.
[00:17:08.160 --> 00:17:13.400]   So you're not, you're not going to see an overnight, you know, 10, a hundred percent
[00:17:13.400 --> 00:17:14.840]   adoption of, of standard.
[00:17:14.840 --> 00:17:18.760]   Although at San Jose state, we do see that we're basically at a hundred percent adoption
[00:17:18.760 --> 00:17:19.760]   at that store.
[00:17:19.760 --> 00:17:20.760]   That's awesome.
[00:17:20.760 --> 00:17:23.800]   So in, in most stores, you're not, you're not going to see a hundred percent overnight.
[00:17:23.800 --> 00:17:26.040]   We're going to see like 5%, right.
[00:17:26.040 --> 00:17:27.040]   To start with.
[00:17:27.040 --> 00:17:28.960]   And it's going to, you know, it'll take time for everyone to switch over.
[00:17:28.960 --> 00:17:31.880]   But what's cool about that is you get the point of sales signal.
[00:17:31.880 --> 00:17:37.180]   The point of sales system will tell you what the non-standard shoppers are buying.
[00:17:37.180 --> 00:17:40.040]   Our system can still predict what they're trying, what we think they're buying.
[00:17:40.040 --> 00:17:42.880]   And then that's actually a nice corrective signal where we can say, well, where are we
[00:17:42.880 --> 00:17:43.880]   making mistakes?
[00:17:43.880 --> 00:17:48.820]   And then we can, we have a team that will do deep dive analysis to sort of suss out
[00:17:48.820 --> 00:17:52.800]   what happened and then ultimately see if that needs to be a training label back into the
[00:17:52.800 --> 00:17:53.800]   system.
[00:17:53.800 --> 00:17:55.360]   And that's, that's a, that's a really nice flywheel.
[00:17:55.360 --> 00:17:59.440]   Cause that's just running, you know, before and after relaunch.
[00:17:59.440 --> 00:18:00.440]   Totally.
[00:18:00.440 --> 00:18:01.440]   Totally.
[00:18:01.440 --> 00:18:08.560]   Has your views on computer vision architecture changed since 2017?
[00:18:08.560 --> 00:18:13.040]   Like I feel like computer vision is constantly making advances.
[00:18:13.040 --> 00:18:14.040]   Does that affect you?
[00:18:14.040 --> 00:18:18.820]   Have you kind of changed at all the way you've thought about training your models?
[00:18:18.820 --> 00:18:25.100]   Yeah, I think we're, I mean, we're still doing a lot of just old fashioned at this point,
[00:18:25.100 --> 00:18:26.100]   supervised learning.
[00:18:26.100 --> 00:18:30.740]   And we, when we, when we started standard, I had this, had a role when I was, and at
[00:18:30.740 --> 00:18:35.580]   the time I was much more involved with the ML team and had a rule with the ML team, which
[00:18:35.580 --> 00:18:38.380]   was you're only allowed to do supervised learning.
[00:18:38.380 --> 00:18:42.180]   Like even, even five years ago, there was all this fancy stuff, right.
[00:18:42.180 --> 00:18:46.960]   And you know, auto encoders and you know, whatever it was, blah, blah, blah.
[00:18:46.960 --> 00:18:49.400]   And none of it was all, I don't want to say BS, right.
[00:18:49.400 --> 00:18:56.000]   It was good research, but it was not, it was not a production quality industrial machine
[00:18:56.000 --> 00:18:58.920]   learning yet, but it was super attractive.
[00:18:58.920 --> 00:19:01.240]   Like people wanted to play around with those things.
[00:19:01.240 --> 00:19:02.720]   But that was my rules.
[00:19:02.720 --> 00:19:04.440]   Like it has to be just old fashioned supervised learning.
[00:19:04.440 --> 00:19:07.240]   We're just going to throw a bunch of data at this thing.
[00:19:07.240 --> 00:19:10.440]   I'm sorry that that's not glamorous.
[00:19:10.440 --> 00:19:11.440]   It's still going to be really hard.
[00:19:11.440 --> 00:19:13.900]   I promise you, like you're going to have plenty of chances to solve hard problems.
[00:19:13.900 --> 00:19:14.900]   And we did, right.
[00:19:14.900 --> 00:19:15.900]   We solved some really cool stuff.
[00:19:15.900 --> 00:19:18.160]   But that was kind of the role back then.
[00:19:18.160 --> 00:19:19.880]   And I kept that role for a long time.
[00:19:19.880 --> 00:19:24.720]   And I think it's just now getting to a point where I think there's different ways.
[00:19:24.720 --> 00:19:29.120]   And of course, supervised is still like the mainstay, but you know, I think synthetic
[00:19:29.120 --> 00:19:30.560]   data is getting super interesting.
[00:19:30.560 --> 00:19:35.360]   And then I think also, you know, just in the last six to eight months, this like self-supervised
[00:19:35.360 --> 00:19:41.480]   revolution that's happening in vision that had already happened in NLP is, you know,
[00:19:41.480 --> 00:19:42.480]   super fascinating.
[00:19:42.480 --> 00:19:43.720]   So we're starting to play around a little bit.
[00:19:43.720 --> 00:19:47.240]   It's not in our production models yet, but we're starting to play around with it a little
[00:19:47.240 --> 00:19:48.240]   bit.
[00:19:48.240 --> 00:19:51.440]   And it's pretty wild what some of this stuff can do.
[00:19:51.440 --> 00:19:55.080]   So I actually, I had COVID about a month ago.
[00:19:55.080 --> 00:20:01.000]   And so I had a few days where I wasn't allowed to take, no one was letting me work, quote
[00:20:01.000 --> 00:20:02.000]   unquote.
[00:20:02.000 --> 00:20:04.400]   So I was just programming instead.
[00:20:04.400 --> 00:20:08.880]   And I was like, I'm just going to play around with some of the self-supervised stuff.
[00:20:08.880 --> 00:20:15.000]   And I just, I took all of our images from all of our shelves from production, right?
[00:20:15.000 --> 00:20:17.880]   With no labels, no labels whatsoever.
[00:20:17.880 --> 00:20:22.360]   But it was, you know, hundreds and hundreds of gigabytes of just images of products.
[00:20:22.360 --> 00:20:28.320]   And I trained one of these massive, you know, vision transformer, you know, masked auto
[00:20:28.320 --> 00:20:29.320]   encoders.
[00:20:29.320 --> 00:20:33.840]   And I just let it run while I had COVID because it took me about a week to recover.
[00:20:33.840 --> 00:20:36.200]   So I just let it train the whole time.
[00:20:36.200 --> 00:20:41.280]   And I mean, the things that were super striking about this was, first of all, it took me like,
[00:20:41.280 --> 00:20:43.680]   you know, four hours to do this, right?
[00:20:43.680 --> 00:20:47.640]   Like, you know, using, you know, shout out to Hugging Face and all these like, you know,
[00:20:47.640 --> 00:20:50.400]   like five years ago, even if I knew what the model architecture should have been five years
[00:20:50.400 --> 00:20:53.680]   ago, I would have spent a month programming this thing, right?
[00:20:53.680 --> 00:20:58.240]   And here it is like, you know, a couple hours, hunting around GitHub, you know, tuning up
[00:20:58.240 --> 00:21:02.400]   a little bit of stuff, you know, I spin up an instance on Google and then I just let
[00:21:02.400 --> 00:21:03.880]   this thing run.
[00:21:03.880 --> 00:21:06.400]   Wow, you just ran it on one instance?
[00:21:06.400 --> 00:21:07.400]   It wasn't even distributed?
[00:21:07.400 --> 00:21:09.520]   Yeah, I got the biggest instance I could.
[00:21:09.520 --> 00:21:16.840]   It was a 16, you know, A100 instance, which is something that only I get to rent.
[00:21:16.840 --> 00:21:20.720]   I hope no one from our company is listening and is like, oh, that means I get to go rent
[00:21:20.720 --> 00:21:21.720]   16 A100 GPU.
[00:21:21.720 --> 00:21:28.600]   But yeah, you didn't even need to, I think, you know, the vision transformers are still
[00:21:28.600 --> 00:21:33.200]   not as big as the NLP transformers, right?
[00:21:33.200 --> 00:21:39.440]   Like you don't need the, was it the Palm model from Google where they had two V4 TPU super
[00:21:39.440 --> 00:21:43.480]   pods, you know, if you're like 5,000 TPUs or something, right?
[00:21:43.480 --> 00:21:48.560]   For who knows how long, like there's no vision models that are even close to that big right
[00:21:48.560 --> 00:21:51.920]   now, but maybe we should be starting to do that stuff.
[00:21:51.920 --> 00:21:52.920]   I don't know.
[00:21:52.920 --> 00:21:57.240]   But anyway, sorry, just to wrap this up, I know I'm going on super long.
[00:21:57.240 --> 00:22:00.960]   I trained this mask auto-coder and it's like basically perfect.
[00:22:00.960 --> 00:22:01.960]   It's insane.
[00:22:01.960 --> 00:22:06.120]   Like you can mask out 95% of an image that, you know, in the paper they talk about doing
[00:22:06.120 --> 00:22:10.600]   75% masking, but there's just such a clear signal from products.
[00:22:10.600 --> 00:22:15.000]   Cause it's, you know, CPG products have just such clear packaging, right?
[00:22:15.000 --> 00:22:19.480]   That you can mask out even more of the image and it'll reproduce super faithfully, basically
[00:22:19.480 --> 00:22:20.480]   the whole package.
[00:22:20.480 --> 00:22:25.200]   Cause it's able to learn what the packaging should look like, right?
[00:22:25.200 --> 00:22:28.560]   You know, if you think about it, we always talk about the manifold hypothesis where it's
[00:22:28.560 --> 00:22:32.960]   like, you know, images sit on some sub manifold, which is, you know, maybe or maybe not true,
[00:22:32.960 --> 00:22:34.400]   but it's definitely true for CPG.
[00:22:34.400 --> 00:22:39.360]   Cause you know, if you have a, if you have a CPG product, there's literally only the
[00:22:39.360 --> 00:22:41.680]   manifold is six degrees of freedom, right?
[00:22:41.680 --> 00:22:45.720]   You know, it's rotation and translation with a little bit of lighting, but that's it, right?
[00:22:45.720 --> 00:22:49.360]   Like it's literally a very low dimensional manifold.
[00:22:49.360 --> 00:22:52.680]   And then the model is just able to learn that on its own and just completely faithfully
[00:22:52.680 --> 00:22:57.280]   replicate these anyway, sorry, for me, it's just wild.
[00:22:57.280 --> 00:22:59.320]   I've not heard of the manifold hypothesis.
[00:22:59.320 --> 00:23:01.020]   Can you, can you describe that?
[00:23:01.020 --> 00:23:07.520]   It seems like it would be more than six dimensions of freedom for a packaged item.
[00:23:07.520 --> 00:23:09.120]   Well for other stuff, it's way more.
[00:23:09.120 --> 00:23:11.720]   So actually you can see it as it shows up.
[00:23:11.720 --> 00:23:14.240]   Sorry, I've been eating snacks here.
[00:23:14.240 --> 00:23:18.880]   So if you, when you see how well it does on things like chips, it still does really well,
[00:23:18.880 --> 00:23:22.240]   but a bag of chips has more than six degrees of freedom, right?
[00:23:22.240 --> 00:23:26.320]   Because it's not just rotation and translation in three space, which is six degrees of freedom.
[00:23:26.320 --> 00:23:30.880]   It's got all of this, like, sorry for the noise, all the crumpling, right?
[00:23:30.880 --> 00:23:34.680]   So there's actually a lot more degrees of freedom, but for something that's rigid, you
[00:23:34.680 --> 00:23:38.200]   know, rigid body motion says it's just six degrees of freedom, right?
[00:23:38.200 --> 00:23:40.720]   XYZ and then yaw roll pitch.
[00:23:40.720 --> 00:23:43.840]   Is that what it is?
[00:23:43.840 --> 00:23:44.840]   And that's it, right?
[00:23:44.840 --> 00:23:48.080]   Compared to a fixed camera, like there's only six degrees of freedom.
[00:23:48.080 --> 00:23:54.960]   If you take out the lighting aspect of it, which is, you know, add some additional degrees
[00:23:54.960 --> 00:23:55.960]   of freedom.
[00:23:55.960 --> 00:24:00.240]   But yeah, I mean, that's the manifold hypothesis is that, you know, real natural images live
[00:24:00.240 --> 00:24:04.960]   on these much smaller, and you know, human has much more manifold dimensions to it, but
[00:24:04.960 --> 00:24:10.120]   the CPGs packages are six dimensions.
[00:24:10.120 --> 00:24:12.400]   You can learn it pretty quickly, apparently.
[00:24:12.400 --> 00:24:16.480]   That's amazing that you're able to spend time training your own model.
[00:24:16.480 --> 00:24:17.480]   I'm jealous.
[00:24:17.480 --> 00:24:22.160]   I'm jealous too, because it doesn't happen very often.
[00:24:22.160 --> 00:24:24.000]   Well, I'm curious.
[00:24:24.000 --> 00:24:29.040]   So you guys have been, you know, Weights & Biases customers since the very early days.
[00:24:29.040 --> 00:24:33.280]   And yeah, I'm not here to advertise Weights & Biases, but I would love to know more about
[00:24:33.280 --> 00:24:34.280]   your stack.
[00:24:34.280 --> 00:24:38.800]   Like, I mean, it sounds like you're playing with Hugging Face.
[00:24:38.800 --> 00:24:40.400]   Are you using that in production?
[00:24:40.400 --> 00:24:45.160]   Are you using like, you know, what other tools are kind of important to you to make the whole
[00:24:45.160 --> 00:24:46.160]   system work?
[00:24:46.160 --> 00:24:47.160]   Yeah, yeah, for sure.
[00:24:47.160 --> 00:24:48.160]   Yeah, for sure.
[00:24:48.160 --> 00:24:53.000]   And I'm super fascinated by this question and like, you know, ML, like the new word
[00:24:53.000 --> 00:24:56.600]   in the ops, which I don't know when it showed up on the scene, but now everyone talks about
[00:24:56.600 --> 00:24:57.600]   ML ops.
[00:24:57.600 --> 00:25:02.760]   And, you know, we have this holy religious war around whether or not ML, I think it's
[00:25:02.760 --> 00:25:06.760]   very similar to the process that DevOps went through where it's like DevOps started as
[00:25:06.760 --> 00:25:10.480]   a methodology, like it was sort of like a practice.
[00:25:10.480 --> 00:25:15.520]   And then it very quickly transformed into a role, right?
[00:25:15.520 --> 00:25:19.920]   There was like, you know, once you can enumerate the things that are in the practice, then,
[00:25:19.920 --> 00:25:21.640]   you know, certain engineers don't want to do it anymore.
[00:25:21.640 --> 00:25:23.480]   So you want another engineer to do it for you.
[00:25:23.480 --> 00:25:25.320]   And you give them that title, you're a DevOps engineer.
[00:25:25.320 --> 00:25:29.440]   Like that was against the whole purpose of what DevOps was about.
[00:25:29.440 --> 00:25:30.440]   But then same with ML ops, right?
[00:25:30.440 --> 00:25:36.220]   Like ML ops came around and it was like, you know, this is a practice for how ML engineers
[00:25:36.220 --> 00:25:41.160]   should be doing their own day to day ML development, right?
[00:25:41.160 --> 00:25:47.120]   And for me, like, you know, we call this end to end, you know, full cycle machine learning
[00:25:47.120 --> 00:25:49.480]   at standard, which is how we tend to run things.
[00:25:49.480 --> 00:25:54.840]   You know, and it's like, you're responsible for thinking about your business impact, which
[00:25:54.840 --> 00:25:59.560]   starts with thinking about the metric that you care about, which I'm a big, like, I'm
[00:25:59.560 --> 00:26:04.560]   a big, I don't know what word I want to use, proponent of thinking about metrics, right?
[00:26:04.560 --> 00:26:07.440]   But like, you know, the easiest thing in the world is to like, look at a research paper
[00:26:07.440 --> 00:26:11.640]   and be like, oh, so I'm going to use, you know, this mean average precision or whatever.
[00:26:11.640 --> 00:26:13.560]   Like that's what all the researchers are doing.
[00:26:13.560 --> 00:26:14.960]   But it's like, no, stop.
[00:26:14.960 --> 00:26:17.680]   Like the first thing you need to do is spend a couple of weeks just thinking about your
[00:26:17.680 --> 00:26:20.460]   metric because we're in production.
[00:26:20.460 --> 00:26:21.880]   We have real world use cases.
[00:26:21.880 --> 00:26:25.760]   And I guarantee you that the researcher that came up with mean average precision had no
[00:26:25.760 --> 00:26:27.440]   real use cases in mind.
[00:26:27.440 --> 00:26:30.280]   They just came up with it because they needed the number.
[00:26:30.280 --> 00:26:32.400]   And it is definitely not the thing you want to optimize for, right?
[00:26:32.400 --> 00:26:36.360]   So like you need to think really hard about what your metric is and validate that that
[00:26:36.360 --> 00:26:37.360]   is the right metric.
[00:26:37.360 --> 00:26:43.360]   You know, so full, for me, full cycle ML is like, think about your business impact, your
[00:26:43.360 --> 00:26:47.400]   metric, your data, get hands on with your data, get hands on with labeling, get hands
[00:26:47.400 --> 00:26:53.160]   on with model training and get hands on with deployment, monitoring, you know, and then
[00:26:53.160 --> 00:26:54.680]   what we call closing the loop, right?
[00:26:54.680 --> 00:26:58.120]   So it's, you know, you need to have those tools that will meet you at the end and say,
[00:26:58.120 --> 00:27:01.080]   actually, your journey has just begun.
[00:27:01.080 --> 00:27:03.480]   Let's see how things are failing in production.
[00:27:03.480 --> 00:27:08.640]   Just make sure that we're taking those as hard examples to bring back to the flock.
[00:27:08.640 --> 00:27:14.040]   So like, that's super exciting because it's a whole discipline, but it's also exciting
[00:27:14.040 --> 00:27:18.160]   because I think, you know, it's still wild west in terms of what does the full stack
[00:27:18.160 --> 00:27:20.080]   need to look like, right?
[00:27:20.080 --> 00:27:22.560]   So weights and biases is super cool.
[00:27:22.560 --> 00:27:26.600]   You know, we use, we're on GCP, so we're big Google users.
[00:27:26.600 --> 00:27:33.040]   You know, I think they're innovating a lot in terms of what their AI stack looks like.
[00:27:33.040 --> 00:27:34.680]   You use their AI stack?
[00:27:34.680 --> 00:27:36.400]   What's your favorite stuff?
[00:27:36.400 --> 00:27:38.000]   I've never used any of it.
[00:27:38.000 --> 00:27:40.200]   I just know we use it.
[00:27:40.200 --> 00:27:41.200]   We use Vertex.
[00:27:41.200 --> 00:27:42.200]   It's not true actually.
[00:27:42.200 --> 00:27:47.600]   And I'm a big believer too in, sorry, I say that a lot, AutoML.
[00:27:47.600 --> 00:27:53.160]   So I personally have definitely played around a lot with Google's AutoML.
[00:27:53.160 --> 00:27:58.320]   And for me, I think it's another one of those places where it's like, as an ML practitioner,
[00:27:58.320 --> 00:28:01.040]   you don't think to go to AutoML first.
[00:28:01.040 --> 00:28:05.280]   You're like, oh no, like AutoML was built for like, you know, like, you know, an old
[00:28:05.280 --> 00:28:09.760]   fashioned engineer, like someone with a business problem, they don't know how to do ML.
[00:28:09.760 --> 00:28:13.000]   So Google built this thing to like make it easier for them to like get their feet.
[00:28:13.000 --> 00:28:14.320]   It was like, no, no, no, no, no, no.
[00:28:14.320 --> 00:28:17.400]   Like take a step back first of all.
[00:28:17.400 --> 00:28:21.360]   First of all, I'm sure whoever built AutoML, like put a thousand times more resources into
[00:28:21.360 --> 00:28:25.160]   this than you're going to put into your custom ML model.
[00:28:25.160 --> 00:28:28.120]   Second of all, even if it's not better, it's a great baseline, right?
[00:28:28.120 --> 00:28:32.440]   So like, just do it, throw the data at it, get an AutoML baseline, and then see if you
[00:28:32.440 --> 00:28:33.440]   can do better.
[00:28:33.440 --> 00:28:35.720]   Like, and maybe you'll be surprised, maybe you can't, right?
[00:28:35.720 --> 00:28:37.560]   But like your job is not to build models.
[00:28:37.560 --> 00:28:39.720]   Your job is to have a business impact, right?
[00:28:39.720 --> 00:28:44.800]   And if you can do that faster with AutoML or any other tools, like, you know, just go
[00:28:44.800 --> 00:28:45.800]   for it, start there.
[00:28:45.800 --> 00:28:49.600]   So sorry, I'm preaching to some choir out there.
[00:28:49.600 --> 00:28:55.200]   We had Anthony Goldbloom on the podcast and the CEO and founder of Kaggle.
[00:28:55.200 --> 00:29:01.000]   And he was saying that he used Google's AutoML and it got him in the top 10 percentile on
[00:29:01.000 --> 00:29:03.000]   a Kaggle competition, which I thought was like amazing.
[00:29:03.000 --> 00:29:06.000]   It's like, come on guys, like use AutoML.
[00:29:06.000 --> 00:29:07.000]   Yeah.
[00:29:07.000 --> 00:29:08.000]   Yeah.
[00:29:08.000 --> 00:29:09.000]   I mean, yeah.
[00:29:09.000 --> 00:29:10.000]   I mean, that's what these tools are for, right?
[00:29:10.000 --> 00:29:13.000]   I mean, like, and I think that's, that's cool actually, because it's, for me, it harkens
[00:29:13.000 --> 00:29:16.520]   back to the previous Wild West that we had engineering, right?
[00:29:16.520 --> 00:29:18.580]   Where we used to write assembly code.
[00:29:18.580 --> 00:29:19.580]   That's what we all did.
[00:29:19.580 --> 00:29:20.580]   Not me personally, right?
[00:29:20.580 --> 00:29:23.840]   But like the world we did back in the day, right?
[00:29:23.840 --> 00:29:26.000]   And then, you know, we started developing compilers and blah, blah, blah.
[00:29:26.000 --> 00:29:27.640]   And, you know, starting, starting to move up the stack.
[00:29:27.640 --> 00:29:30.360]   And you had the same thing happen back then where people were like, no, no, no, no.
[00:29:30.360 --> 00:29:32.040]   Like you can't use a compiler.
[00:29:32.040 --> 00:29:35.600]   Like you're never going to be able to write assembly the way that I can write assembly.
[00:29:35.600 --> 00:29:37.720]   And sure enough, compilers got way better than people.
[00:29:37.720 --> 00:29:40.080]   And we kept moving up the stack of abstraction.
[00:29:40.080 --> 00:29:42.680]   And I think the same thing's going to happen with ML, right?
[00:29:42.680 --> 00:29:47.680]   Like we're not going to be sitting here tuning, you know, manually writing layer six goes
[00:29:47.680 --> 00:29:51.400]   into layer seven, and it's going to go from 128 features to 256 features.
[00:29:51.400 --> 00:29:53.520]   You know, like that's, that's not our future.
[00:29:53.520 --> 00:29:59.200]   I think as, as MLEs, it's, it's definitely many levels above that in abstraction.
[00:29:59.200 --> 00:30:05.280]   But look, I mean, you're one of the people that has big deep learning models as a really
[00:30:05.280 --> 00:30:11.840]   core part of your business and you're successfully deploying a lot of them and continuously improving
[00:30:11.840 --> 00:30:12.840]   them.
[00:30:12.840 --> 00:30:15.600]   So I'm sure people are going to be interested in more specifics around your stack.
[00:30:15.600 --> 00:30:18.360]   I mean, could you share kind of like, you know, if you have a point of view on like
[00:30:18.360 --> 00:30:19.920]   frameworks, do you use?
[00:30:19.920 --> 00:30:20.920]   Oh yeah.
[00:30:20.920 --> 00:30:21.920]   Do you use it all?
[00:30:21.920 --> 00:30:25.000]   I mean, tell me, give me, give me, give me the stuff you like and don't like.
[00:30:25.000 --> 00:30:29.080]   I think that would be the most valuable thing you could offer our audience.
[00:30:29.080 --> 00:30:30.080]   For sure.
[00:30:30.080 --> 00:30:35.080]   I pride myself throughout my career, even pre-ML on like, on picking the right horses,
[00:30:35.080 --> 00:30:39.560]   the right like stacks that like end up even early, like they end up playing out.
[00:30:39.560 --> 00:30:40.560]   All right.
[00:30:40.560 --> 00:30:43.800]   So tell me about your 2017 stack then, because I know weights and biases, it's in there.
[00:30:43.800 --> 00:30:46.320]   You were one of our first customers.
[00:30:46.320 --> 00:30:47.680]   Well, for sure.
[00:30:47.680 --> 00:30:53.000]   The other thing, so that was great, but for sure the thing we didn't pick correctly, I
[00:30:53.000 --> 00:30:54.960]   picked TensorFlow at the time.
[00:30:54.960 --> 00:30:59.480]   And, you know, I think the whole world has revolted against TensorFlow.
[00:30:59.480 --> 00:31:03.840]   And you know, I think that the challenge is you pick the wrong tech and then it gets steeped
[00:31:03.840 --> 00:31:05.120]   in your stack, right?
[00:31:05.120 --> 00:31:07.320]   It gets really hard to pull it out.
[00:31:07.320 --> 00:31:09.320]   So we've switched over to PyTorch since then.
[00:31:09.320 --> 00:31:12.240]   Okay, wait, let's talk about that because everyone's got a different take on this.
[00:31:12.240 --> 00:31:15.280]   Why do you think PyTorch beat out TensorFlow?
[00:31:15.280 --> 00:31:17.920]   Like what do you think it was?
[00:31:17.920 --> 00:31:22.800]   I mean, for like ease of uses, it's the dev experience all day, right?
[00:31:22.800 --> 00:31:26.000]   I don't necessarily think that it's technically superior.
[00:31:26.000 --> 00:31:29.480]   And I think, you know, Google's got a great contender now.
[00:31:29.480 --> 00:31:31.720]   So I've been playing around with Jax in my spare time.
[00:31:31.720 --> 00:31:36.880]   And I think we don't have anything in production in Jax, but we have a few irons in the fire,
[00:31:36.880 --> 00:31:39.320]   a few things that we're looking at.
[00:31:39.320 --> 00:31:40.320]   Nice.
[00:31:40.320 --> 00:31:44.080]   You're going to get a couple of resumes from our community, I guarantee you.
[00:31:44.080 --> 00:31:45.080]   Nice.
[00:31:45.080 --> 00:31:48.720]   But you know, Jax has, you know, I think it's got the same great dev experience.
[00:31:48.720 --> 00:31:52.880]   The ecosystem is a little bit more nascent, but that's to be expected, right?
[00:31:52.880 --> 00:31:54.840]   And I think it's more technically excellent, right?
[00:31:54.840 --> 00:31:57.920]   I think it's got way more headroom to grow.
[00:31:57.920 --> 00:32:01.400]   And you know, hopefully it's not going to be as painful of a shift to go from PyTorch
[00:32:01.400 --> 00:32:02.400]   to Jax.
[00:32:02.400 --> 00:32:03.400]   We'll see.
[00:32:03.400 --> 00:32:06.120]   I think, you know, as the deployment stories are maturing too, you know, we're getting
[00:32:06.120 --> 00:32:10.480]   to this place where it doesn't really matter the way you train your model, the way you're
[00:32:10.480 --> 00:32:14.040]   iterating on your experimentation, you know, the way you're going to production can be
[00:32:14.040 --> 00:32:15.920]   decoupled from that.
[00:32:15.920 --> 00:32:19.960]   So as long as you have the weights, then you can, you know, take it to production in a
[00:32:19.960 --> 00:32:21.960]   different way, potentially.
[00:32:21.960 --> 00:32:24.560]   What about like CI/CD, production monitoring?
[00:32:24.560 --> 00:32:26.760]   Do you use any of the stuff out there?
[00:32:26.760 --> 00:32:27.760]   Is it homegrown?
[00:32:27.760 --> 00:32:30.120]   How do you think about that?
[00:32:30.120 --> 00:32:32.640]   That is a little past my...
[00:32:32.640 --> 00:32:39.400]   I'm not as in the weeds anymore, so I can't answer too much of that.
[00:32:39.400 --> 00:32:42.160]   I know we're always complaining about it internally.
[00:32:42.160 --> 00:32:47.560]   I know we haven't settled on the right thing.
[00:32:47.560 --> 00:32:52.640]   I guess the 2017 stack though, so it's like TensorFlow.
[00:32:52.640 --> 00:32:53.640]   What else?
[00:32:53.640 --> 00:32:58.360]   You know, we definitely built a company on Python to start.
[00:32:58.360 --> 00:33:01.440]   You know, it was just a pragmatic choice because ML is Python, unfortunately.
[00:33:01.440 --> 00:33:03.320]   I despise Python.
[00:33:03.320 --> 00:33:04.320]   You despise Python?
[00:33:04.320 --> 00:33:05.320]   Wow.
[00:33:05.320 --> 00:33:06.320]   That's strong.
[00:33:06.320 --> 00:33:08.360]   You got to come out of the gate strong with these.
[00:33:08.360 --> 00:33:09.360]   I love it.
[00:33:09.360 --> 00:33:10.360]   Yeah.
[00:33:10.360 --> 00:33:11.360]   Tell me more.
[00:33:11.360 --> 00:33:16.760]   I mean, it's great as in, you know, if I'm going to write a 50 line script, it's fantastic.
[00:33:16.760 --> 00:33:17.760]   Right?
[00:33:17.760 --> 00:33:21.400]   If I'm going to write 50,000 line script, you know, and it's going to be sitting across
[00:33:21.400 --> 00:33:26.360]   20 engineers, you know, it's a disaster.
[00:33:26.360 --> 00:33:27.360]   What do you want?
[00:33:27.360 --> 00:33:28.360]   What would you prefer?
[00:33:28.360 --> 00:33:34.760]   I guess, you know, it's, I mean, this is the normal religious war, so I'll just harp on
[00:33:34.760 --> 00:33:35.760]   the same points.
[00:33:35.760 --> 00:33:36.760]   Right?
[00:33:36.760 --> 00:33:40.600]   But, you know, like for me, I like strong types because I think that they're...
[00:33:40.600 --> 00:33:42.560]   It's not even that you get faster speed, which you do, right?
[00:33:42.560 --> 00:33:44.120]   Like that's great.
[00:33:44.120 --> 00:33:45.120]   But it's just a, it's a contract.
[00:33:45.120 --> 00:33:46.120]   It's a people contract.
[00:33:46.120 --> 00:33:47.520]   It's not a machine contract.
[00:33:47.520 --> 00:33:49.960]   It's a people contract and it enforces it.
[00:33:49.960 --> 00:33:53.960]   So it's, you know, I know when I come to this piece of code, whether I wrote it 20 years
[00:33:53.960 --> 00:33:57.840]   ago or what, you know, someone else wrote it yesterday, I know exactly what the output
[00:33:57.840 --> 00:33:59.200]   is, what the input needs to be.
[00:33:59.200 --> 00:34:01.880]   And it's just a, it's a contract.
[00:34:01.880 --> 00:34:05.320]   Whereas with Python, it's, you still have contracts.
[00:34:05.320 --> 00:34:09.040]   Like you still spend a lot of engineering resources coming up with the right, you know,
[00:34:09.040 --> 00:34:12.440]   decoupling and where should our API boundaries be.
[00:34:12.440 --> 00:34:13.960]   But it's never enforced.
[00:34:13.960 --> 00:34:18.640]   So, you know, is the contract that we all agreed upon actually what's happening or isn't
[00:34:18.640 --> 00:34:19.640]   it?
[00:34:19.640 --> 00:34:23.600]   And you just have to trust or build like a ton of unit tests, right?
[00:34:23.600 --> 00:34:25.280]   But there's no guarantees.
[00:34:25.280 --> 00:34:29.360]   So for me, strong typing is all about building trust and being able to communicate better
[00:34:29.360 --> 00:34:30.360]   with other people.
[00:34:30.360 --> 00:34:32.840]   Because for me, engineering, it's a team sport.
[00:34:32.840 --> 00:34:35.080]   It's a people sport, actually.
[00:34:35.080 --> 00:34:36.080]   It's very much collaborative.
[00:34:36.080 --> 00:34:37.600]   And that's why I like strong typing.
[00:34:37.600 --> 00:34:41.160]   Your favorite typed language is what?
[00:34:41.160 --> 00:34:46.200]   So this actually became part of our stack evolution was we picked Rust.
[00:34:46.200 --> 00:34:48.000]   Oh, you're going to get a lot of resumes.
[00:34:48.000 --> 00:34:49.000]   Okay.
[00:34:49.000 --> 00:34:51.720]   I was going to guess that.
[00:34:51.720 --> 00:34:54.440]   We're definitely hiring plenty of Rust engineers right now.
[00:34:54.440 --> 00:34:59.560]   So if you like ML and you like productionizing ML and you like Rust and you like streaming
[00:34:59.560 --> 00:35:03.920]   a lot of data, you definitely want to come to Sting.
[00:35:03.920 --> 00:35:09.160]   Yeah, so we're still not 100% on Rust and we have some other stuff in our stack too,
[00:35:09.160 --> 00:35:12.360]   but we have a good healthy amount of Rust.
[00:35:12.360 --> 00:35:19.640]   And actually, one of our early wins was, and this is why we ended up choosing Rust, was
[00:35:19.640 --> 00:35:21.920]   one of our founders was a huge Rust proselytizer.
[00:35:21.920 --> 00:35:24.160]   Oh, in 2017?
[00:35:24.160 --> 00:35:27.600]   Yeah, even years before that.
[00:35:27.600 --> 00:35:30.080]   This was Brandon, one of our co-founders.
[00:35:30.080 --> 00:35:34.200]   And we were working together for years before that and he was always pitching me on Rust.
[00:35:34.200 --> 00:35:36.320]   He's like, "Jordan, let's use Rust."
[00:35:36.320 --> 00:35:38.040]   And my job was to say no.
[00:35:38.040 --> 00:35:43.160]   My job as an engineering manager is to say no.
[00:35:43.160 --> 00:35:44.160]   You know, it's a funny story.
[00:35:44.160 --> 00:35:48.600]   I called the Streamlit founder, who's also on the podcast, after he sold his company
[00:35:48.600 --> 00:35:51.920]   for $800 million, I was like, "Dude, what are you going to do?"
[00:35:51.920 --> 00:35:53.760]   And he's like, "I just want to write more Rust code.
[00:35:53.760 --> 00:35:56.280]   I feel like I now can finally do it."
[00:35:56.280 --> 00:35:59.520]   So that's the end of your job opening.
[00:35:59.520 --> 00:36:00.520]   That's cool.
[00:36:00.520 --> 00:36:01.520]   Yeah.
[00:36:01.520 --> 00:36:03.400]   Well, if he doesn't have enough money and he just wants to come write some Rust code
[00:36:03.400 --> 00:36:04.400]   with us.
[00:36:04.400 --> 00:36:09.840]   But yeah, I mean, it's a cool language, right?
[00:36:09.840 --> 00:36:13.400]   So it took Brandon a while to, because he kept telling me what it was good at and what
[00:36:13.400 --> 00:36:14.400]   it wasn't good at.
[00:36:14.400 --> 00:36:17.880]   And I was like, "Okay, well then using what you're telling me, it's not, the problem working
[00:36:17.880 --> 00:36:19.200]   on right now is not what it's for.
[00:36:19.200 --> 00:36:23.200]   So like, you just want to work on this because it's cool."
[00:36:23.200 --> 00:36:25.680]   But then we finally had this problem where it was like, it was the right thing.
[00:36:25.680 --> 00:36:31.240]   So we have, this was our multi-view tracking algorithm, which is this like, you get, we
[00:36:31.240 --> 00:36:34.920]   run deep learning models per camera to extract these features.
[00:36:34.920 --> 00:36:38.480]   And then you have to merge them together across all the different camera feeds in order to
[00:36:38.480 --> 00:36:43.680]   build up a single cohesive understanding of how people are moving through the entire store.
[00:36:43.680 --> 00:36:45.760]   And it's not, that part is not deep learning.
[00:36:45.760 --> 00:36:52.640]   It's just this like super gnarly sort of graph theory kind of a combinatoric optimization
[00:36:52.640 --> 00:36:53.640]   problem.
[00:36:53.640 --> 00:36:56.280]   And it's dealing with a ton of data, right?
[00:36:56.280 --> 00:37:00.120]   Doing a lot of heuristics and it has to be super fast.
[00:37:00.120 --> 00:37:04.600]   It has to be soft real time because it's stream processing, you know, maybe a hundred cameras
[00:37:04.600 --> 00:37:07.760]   each at 30 FPS, right?
[00:37:07.760 --> 00:37:13.280]   And we were building that algorithm and it was just getting wrecked by, and then we were
[00:37:13.280 --> 00:37:18.120]   investing so much resource, engineering resources into paralyzing the Python code.
[00:37:18.120 --> 00:37:21.960]   And that's when you have to take a step back from Python is when you start fighting the
[00:37:21.960 --> 00:37:26.120]   veil, the global interpreter lock, and you're doing all this like funky magic to like get
[00:37:26.120 --> 00:37:27.120]   around that.
[00:37:27.120 --> 00:37:33.840]   And you start introducing the worst possible technical debt to paper over this fundamental
[00:37:33.840 --> 00:37:36.160]   limitation of Python.
[00:37:36.160 --> 00:37:38.020]   Then it's really time to take a step back.
[00:37:38.020 --> 00:37:39.560]   So we evaluated Rust.
[00:37:39.560 --> 00:37:42.480]   We're like, let's see if we can rewrite this whole, and it was a big algorithm.
[00:37:42.480 --> 00:37:45.880]   It was like, you know, this is, you know, ML is awesome.
[00:37:45.880 --> 00:37:49.080]   Cause you write like 50 lines of ML and you get magic.
[00:37:49.080 --> 00:37:55.160]   This algorithm is like, you know, 10,000 lines of gross, nasty, massive heuristics.
[00:37:55.160 --> 00:38:01.520]   But we sat down and we wrote it in Rust pretty fast and we got a 50X speed up.
[00:38:01.520 --> 00:38:04.760]   We've since then gotten like additional two or three X speed up.
[00:38:04.760 --> 00:38:07.880]   Cause the whole like cool thing about Rust is fearless parallelism.
[00:38:07.880 --> 00:38:10.400]   I don't know if you know that expression for Rust, right?
[00:38:10.400 --> 00:38:13.320]   Like, cause it's not just strongly typed.
[00:38:13.320 --> 00:38:15.320]   It's like hella strongly typed.
[00:38:15.320 --> 00:38:19.280]   So it has this ability to like identify race conditions and make sure that when you're
[00:38:19.280 --> 00:38:22.200]   doing parallel programming, you're not going to get, you're not going to shoot yourself
[00:38:22.200 --> 00:38:24.440]   in the foot.
[00:38:24.440 --> 00:38:31.360]   So you can actually just, you can more confidently move into, into multi-threading.
[00:38:31.360 --> 00:38:36.080]   So we've just gotten huge benefits by moving over Rust from a speed perspective and from
[00:38:36.080 --> 00:38:38.040]   a competence perspective too, right?
[00:38:38.040 --> 00:38:42.560]   Where, you know, you, you have this super complex algorithm, you change something and
[00:38:42.560 --> 00:38:44.120]   you want to push it to production.
[00:38:44.120 --> 00:38:47.720]   If you had the thing that we would have had to rewrite it and otherwise would have been
[00:38:47.720 --> 00:38:51.000]   like C++ and it scared the hell out of me.
[00:38:51.000 --> 00:38:56.280]   Cause C and C++ are just, I've had to deal with production code in those languages in
[00:38:56.280 --> 00:39:02.360]   the past and you know, memory leaks and seg faults and you know, and you're, you're having
[00:39:02.360 --> 00:39:03.800]   ML engineers write these algorithms.
[00:39:03.800 --> 00:39:07.800]   They're not necessarily experts at memory management, right?
[00:39:07.800 --> 00:39:12.120]   So, but what's cool is, you know, now we have, you know, more sort of research oriented people
[00:39:12.120 --> 00:39:17.280]   that can make tweaks to this, this multi-view tracking code and we don't have memory leaks
[00:39:17.280 --> 00:39:18.280]   in production.
[00:39:18.280 --> 00:39:19.280]   We don't have seg faults in production.
[00:39:19.280 --> 00:39:24.560]   You know, they confidently make changes to the algorithm, push it to production and it
[00:39:24.560 --> 00:39:25.560]   works.
[00:39:25.560 --> 00:39:28.120]   So that's, that's super cool.
[00:39:28.120 --> 00:39:32.640]   So I'm a, I'm a, I'm definitely a Rust proponent.
[00:39:32.640 --> 00:39:36.640]   Any other kind of early technical choices that you really feel proud of?
[00:39:36.640 --> 00:39:42.800]   That's a good, that's a good question.
[00:39:42.800 --> 00:39:50.640]   I'll tell you one choice we made that was totally wrong, which was I had this, this
[00:39:50.640 --> 00:39:59.160]   belief at the time that wasn't, it ended up being correct but it was still the wrong choice.
[00:39:59.160 --> 00:40:05.160]   But the belief was that raw, raw camera footage is better than decompressed video.
[00:40:05.160 --> 00:40:09.560]   You know, so if you, if you have a camera feed, the best thing you can possibly do is
[00:40:09.560 --> 00:40:15.760]   record those pixels raw to disk, train ML models off those raw pixels and then deploy
[00:40:15.760 --> 00:40:17.440]   your, your deploy your model, right?
[00:40:17.440 --> 00:40:21.160]   Never, never allow H264, H265 compression to sit in between, right?
[00:40:21.160 --> 00:40:23.000]   Cause it's obviously throwing away information.
[00:40:23.000 --> 00:40:25.080]   Like that's, that's its job, right?
[00:40:25.080 --> 00:40:28.440]   You hope, I mean, it's, it's tuned for human fidelity, right?
[00:40:28.440 --> 00:40:31.720]   Like so that we can't tell the difference.
[00:40:31.720 --> 00:40:36.000]   I had, I was like fanatical that we had to use raw only.
[00:40:36.000 --> 00:40:40.440]   So like, you know, all this engineering work went into just being able to store all this
[00:40:40.440 --> 00:40:43.640]   raw data all the time.
[00:40:43.640 --> 00:40:50.280]   And it just got way too, just got way too slow to maintain that engineering work.
[00:40:50.280 --> 00:40:54.960]   And we finally ended up doing an experiment where we collected a bunch of video data and
[00:40:54.960 --> 00:40:57.880]   we labeled it both from video and from raw.
[00:40:57.880 --> 00:41:02.600]   And then we trained the models and sure enough, there was like, you know, a pretty sizable
[00:41:02.600 --> 00:41:04.720]   gap between how, how much accuracy you could get.
[00:41:04.720 --> 00:41:08.680]   I don't remember exactly what it was, but it was like, it was meaningful.
[00:41:08.680 --> 00:41:11.240]   But we sat down and we were just like, it doesn't, it doesn't make sense.
[00:41:11.240 --> 00:41:16.760]   Like, like sure, like that accuracy matters, but like we won't have a company if we don't
[00:41:16.760 --> 00:41:19.760]   move up with the video.
[00:41:19.760 --> 00:41:20.760]   Right.
[00:41:20.760 --> 00:41:23.920]   And you know, I think the scary thing is to this day, we still have a little bit of vestiges
[00:41:23.920 --> 00:41:28.720]   of working off of images instead of, instead of video, because it's just, you know, you
[00:41:28.720 --> 00:41:33.080]   make these decisions early on and like, it's just, it's their, their weeds that are so
[00:41:33.080 --> 00:41:34.320]   hard to pull out.
[00:41:34.320 --> 00:41:38.640]   So I'm still, we're all still paying for my sins.
[00:41:38.640 --> 00:41:42.600]   So that's the dangers of making some of those bets.
[00:41:42.600 --> 00:41:45.360]   But I think we've made good bets as well in the past.
[00:41:45.360 --> 00:41:50.280]   Where do you store all your data and how do you retrieve it when you want to train on
[00:41:50.280 --> 00:41:51.760]   it?
[00:41:51.760 --> 00:41:55.800]   So we, we started off as a on-prem stack.
[00:41:55.800 --> 00:42:00.080]   So we were, you know, we, we bought GPUs and we built machines and we put them into convenience
[00:42:00.080 --> 00:42:04.400]   stores and it was, you know, that was wild.
[00:42:04.400 --> 00:42:09.080]   Cause we had to like upgrade the HVAC in order to make sure the convenience store didn't
[00:42:09.080 --> 00:42:12.140]   melt down.
[00:42:12.140 --> 00:42:13.640]   Now we run everything in the cloud.
[00:42:13.640 --> 00:42:17.560]   So it's just, we stream everything to the cloud, which is great for iterations.
[00:42:17.560 --> 00:42:20.880]   It's, you know, you just have access to, you know, whatever you need.
[00:42:20.880 --> 00:42:24.920]   I mean, we have retention policies, et cetera, obviously.
[00:42:24.920 --> 00:42:29.960]   But I think moving forward, probably in the next year or two, I suspect we'll be taking
[00:42:29.960 --> 00:42:35.480]   some of it back on-prem, mostly just from a cost calculus perspective, because the cloud's
[00:42:35.480 --> 00:42:36.480]   great.
[00:42:36.480 --> 00:42:40.080]   It's super flexible, but it's not necessarily, you know, the most economical, especially
[00:42:40.080 --> 00:42:47.560]   when you're talking about renting GPUs, which is still an arm and a leg up on the cloud.
[00:42:47.560 --> 00:42:52.440]   Do you worry at all that the problem you're solving as ML gets better and better might
[00:42:52.440 --> 00:42:58.460]   get too easy and would no longer be a deep technical problem?
[00:42:58.460 --> 00:43:00.120]   I don't worry about it.
[00:43:00.120 --> 00:43:02.280]   I know that it's going to happen.
[00:43:02.280 --> 00:43:06.040]   So it's, you know, for me, it's a, and we talked about this even in early days of standard,
[00:43:06.040 --> 00:43:07.040]   right?
[00:43:07.040 --> 00:43:10.160]   Where we're like, look, this is, you know, back then we said 10 years from now, it's
[00:43:10.160 --> 00:43:13.900]   going to be a git clone to do autonomous checkout, right?
[00:43:13.900 --> 00:43:17.520]   Or worst case, like a, you know, a four hour project, right?
[00:43:17.520 --> 00:43:21.080]   You know, an undergrad is going to be doing it over the weekend or something.
[00:43:21.080 --> 00:43:23.960]   So we knew that was going to happen.
[00:43:23.960 --> 00:43:25.520]   And I think we're seeing the progress too, right?
[00:43:25.520 --> 00:43:31.880]   Even the story I told you about the mast auto encoders, like, you know, that's such a, and
[00:43:31.880 --> 00:43:33.960]   like, there's real applications to this too, right?
[00:43:33.960 --> 00:43:38.560]   It seems like a, you know, like you use that as a pre-training step in it, you know, you
[00:43:38.560 --> 00:43:42.840]   get better accuracy on item classification, better than purely supervised, right?
[00:43:42.840 --> 00:43:46.600]   So it's just like this crazy, it's this crazy bump in your ability.
[00:43:46.600 --> 00:43:50.320]   And it took us a couple hours to do it now that it's, now that it's just a git clone.
[00:43:50.320 --> 00:43:52.120]   So it's definitely happening.
[00:43:52.120 --> 00:43:55.440]   I think we still have a few years left before it's a git clone.
[00:43:55.440 --> 00:44:00.040]   I would still guess like four years, maybe four or five years, four or five years.
[00:44:00.040 --> 00:44:01.040]   Things are moving fast, man.
[00:44:01.040 --> 00:44:02.040]   It's crazy.
[00:44:02.040 --> 00:44:08.080]   But what we told ourselves five years ago was yes, that's going to happen.
[00:44:08.080 --> 00:44:09.680]   But the same is true for any industry, right?
[00:44:09.680 --> 00:44:14.920]   Like, you know, a point of sale system is, is, you know, I like to talk about this a
[00:44:14.920 --> 00:44:20.840]   lot too, like a barcode scanner hooked up to a point of sale system was literally state
[00:44:20.840 --> 00:44:23.280]   of the art physics 50 or 60 years ago, right?
[00:44:23.280 --> 00:44:24.280]   It's a laser, right?
[00:44:24.280 --> 00:44:26.800]   We didn't even know lasers were physically possible.
[00:44:26.800 --> 00:44:31.360]   And then we create, you know, we hypothesize the physics, we validated the physics, we
[00:44:31.360 --> 00:44:35.760]   productionize the technology, and now it's so ruggedized that it's, you know, it's in
[00:44:35.760 --> 00:44:37.600]   every single store in the world, right?
[00:44:37.600 --> 00:44:39.800]   And you don't even think about it as technology.
[00:44:39.800 --> 00:44:42.680]   So yeah, that's, that's going to happen.
[00:44:42.680 --> 00:44:44.600]   And I think that that's okay.
[00:44:44.600 --> 00:44:47.200]   And I'm sure we'll have other cool, hard problems to solve in 10 years.
[00:44:47.200 --> 00:44:52.400]   But what we need to do is transition this tech lead that we have into a sort of a true
[00:44:52.400 --> 00:44:54.280]   moat, you know, true flywheel.
[00:44:54.280 --> 00:44:58.440]   And I think that's the making ourselves indispensable to retailers, like just providing them so
[00:44:58.440 --> 00:45:03.720]   much value that, you know, sure, someone else could come along and get clone Thomas checkout.
[00:45:03.720 --> 00:45:06.160]   But you know, our customer support is amazing.
[00:45:06.160 --> 00:45:08.080]   Our product is super refined.
[00:45:08.080 --> 00:45:09.680]   The experience is amazing.
[00:45:09.680 --> 00:45:14.560]   We've got 30 other amazing features that sit on top of the stack that's invaluable to the
[00:45:14.560 --> 00:45:15.560]   retailer.
[00:45:15.560 --> 00:45:20.560]   The shopper has come to depend on this because, you know, it's standard in their pocket and
[00:45:20.560 --> 00:45:24.160]   they expect people to walk into a store and just have standard work for them.
[00:45:24.160 --> 00:45:30.200]   So I think you have to use this tech advantage to turn it into the normal types of advantages
[00:45:30.200 --> 00:45:34.240]   that regular startups are using to build a moat.
[00:45:34.240 --> 00:45:35.240]   And that's okay.
[00:45:35.240 --> 00:45:38.080]   I think that happens to every hard tech company.
[00:45:38.080 --> 00:45:40.800]   Or they don't make the, what they set out to make.
[00:45:40.800 --> 00:45:42.800]   I think that might be a common failure.
[00:45:42.800 --> 00:45:43.800]   Yeah.
[00:45:43.800 --> 00:45:44.800]   That's a common failure for sure.
[00:45:44.800 --> 00:45:45.800]   Right.
[00:45:45.800 --> 00:45:51.920]   What's like one not obvious thing that you could do to like enhance the experience?
[00:45:51.920 --> 00:45:55.960]   I'm sure you've thought about this a lot.
[00:45:55.960 --> 00:46:02.800]   So there is still this friction in the experience, which is, you know, our visual system is fully
[00:46:02.800 --> 00:46:03.800]   anonymous.
[00:46:03.800 --> 00:46:05.480]   So we don't know who you are, right?
[00:46:05.480 --> 00:46:08.260]   You're person 17 when you walk into the store.
[00:46:08.260 --> 00:46:09.260]   And that's intentional, right?
[00:46:09.260 --> 00:46:12.160]   We don't do facial recognition, et cetera.
[00:46:12.160 --> 00:46:16.720]   We have to tie your payment information to person 17 somehow.
[00:46:16.720 --> 00:46:20.360]   So if you've been to Amazon Go, they do these gates, right?
[00:46:20.360 --> 00:46:23.680]   When you walk up to the store to get in, you have to pass through a gate literally, and
[00:46:23.680 --> 00:46:26.160]   you use the Amazon app to open the gate.
[00:46:26.160 --> 00:46:31.240]   And then there's a visual sync basically, where behind the scenes, Amazon saying, okay,
[00:46:31.240 --> 00:46:35.880]   you know, Susan just badged in, we see person 17 at the gate.
[00:46:35.880 --> 00:46:37.520]   So person 17 must be Susan, right?
[00:46:37.520 --> 00:46:38.520]   So you do that.
[00:46:38.520 --> 00:46:40.560]   We call it association.
[00:46:40.560 --> 00:46:41.560]   And we do something very similar.
[00:46:41.560 --> 00:46:46.600]   We don't do it with gates because we believe gates are antithetical to good retail.
[00:46:46.600 --> 00:46:50.080]   Like you don't put friction at the beginning, you put friction at the end.
[00:46:50.080 --> 00:46:51.080]   Amazon knows that too.
[00:46:51.080 --> 00:46:54.560]   So I don't know why they, they're the best e-commerce player in the world.
[00:46:54.560 --> 00:46:56.560]   They know that you put friction at the end.
[00:46:56.560 --> 00:46:59.480]   You never put it at the beginning, but sorry, just the tirade.
[00:46:59.480 --> 00:47:02.640]   So we're strong believers that you don't put gates up.
[00:47:02.640 --> 00:47:05.720]   So what we do is we put NFC stickers in the store.
[00:47:05.720 --> 00:47:09.000]   And what you do is same thing, but anytime during your trip, actually, you don't need
[00:47:09.000 --> 00:47:10.000]   to do it to get into the store.
[00:47:10.000 --> 00:47:12.560]   You can just come shop at any time during your trip, you take your phone out, you bump
[00:47:12.560 --> 00:47:14.360]   one of these NFC stickers.
[00:47:14.360 --> 00:47:18.400]   And then we do the same thing where we know when the bump happens on the backend.
[00:47:18.400 --> 00:47:19.800]   We know that that's Susan.
[00:47:19.800 --> 00:47:21.400]   And then we know person 17 was the one bumping.
[00:47:21.400 --> 00:47:26.600]   Cause we have this fine grained 3d reconstruction of Susan as she's at person 17.
[00:47:26.600 --> 00:47:30.000]   So we know where their hand is.
[00:47:30.000 --> 00:47:31.880]   And then we do the association, but there's still that friction, right?
[00:47:31.880 --> 00:47:37.200]   You have to like take your phone out of your pocket and like think about transacting.
[00:47:37.200 --> 00:47:42.200]   So I have this belief that we'll be able to get rid of that at some point in the next
[00:47:42.200 --> 00:47:46.160]   couple of years where you can just, without being privacy invasive, you can keep your
[00:47:46.160 --> 00:47:51.740]   phone in your pocket and using additional signals like Bluetooth, et cetera.
[00:47:51.740 --> 00:47:55.000]   We should be able to narrow it in and figure out person 17 is Susan.
[00:47:55.000 --> 00:47:58.600]   Cause then you can really just walk into a store and walk out and never have to think
[00:47:58.600 --> 00:48:00.760]   about transacting.
[00:48:00.760 --> 00:48:01.760]   That's cool.
[00:48:01.760 --> 00:48:07.800]   One thing I want to make sure I asked you about is your standard SIM dataset.
[00:48:07.800 --> 00:48:12.880]   Can you maybe describe what that is and why you released a public dataset?
[00:48:12.880 --> 00:48:13.880]   Yeah.
[00:48:13.880 --> 00:48:14.880]   Yeah.
[00:48:14.880 --> 00:48:17.360]   This was super cool.
[00:48:17.360 --> 00:48:24.560]   So it's a 3d SIM basically of stores.
[00:48:24.560 --> 00:48:29.560]   So it builds 3d reconstruction, not 3d reconstructions, but it builds 3d models of stores totally
[00:48:29.560 --> 00:48:30.560]   synthetically.
[00:48:30.560 --> 00:48:31.560]   Right.
[00:48:31.560 --> 00:48:33.560]   So it tries to simulate the shelves where the cameras, where the products on the shelves,
[00:48:33.560 --> 00:48:38.760]   it tries to simulate, you know, the way that the products are stocked in the shelves has
[00:48:38.760 --> 00:48:41.520]   a decent corpus of, of skews, et cetera.
[00:48:41.520 --> 00:48:45.120]   So it's just a way to like build up these 3d representations of stores.
[00:48:45.120 --> 00:48:50.400]   And then obviously what's cool about that is you can generate infinite image data of
[00:48:50.400 --> 00:48:53.480]   stores, synthetic stores, right?
[00:48:53.480 --> 00:48:57.400]   So that's a huge leg up to build a move quickly and get off the ground and, you know, start,
[00:48:57.400 --> 00:48:58.400]   start training.
[00:48:58.800 --> 00:49:02.080]   And we, we often see a lot of models where if you train on synthetic, it doesn't give
[00:49:02.080 --> 00:49:05.920]   you as good results as if you train on, on real data.
[00:49:05.920 --> 00:49:10.000]   That's definitely true still for some of our models, but there are some models where you
[00:49:10.000 --> 00:49:15.520]   just can't get the data labels in particular in the real world, right.
[00:49:15.520 --> 00:49:17.680]   Or you can, but it's just insanely expensive.
[00:49:17.680 --> 00:49:21.200]   So like, you know, segmentation is a good example where, you know, it's just so expensive
[00:49:21.200 --> 00:49:23.320]   to do segmentation.
[00:49:23.320 --> 00:49:28.320]   And for us, actually, we were working on this model called change detection, which is, you
[00:49:28.320 --> 00:49:32.960]   can, if you look at a shelf over time, you can see, you can see the item sort of be taken
[00:49:32.960 --> 00:49:36.080]   and removed.
[00:49:36.080 --> 00:49:39.200]   That's a really interesting, so we can create that data set, the real data set, but how
[00:49:39.200 --> 00:49:40.200]   do you label it?
[00:49:40.200 --> 00:49:45.040]   Like asking a human to look at a before and after image and draw a segmentation mask of
[00:49:45.040 --> 00:49:47.320]   where the item was staged.
[00:49:47.320 --> 00:49:51.480]   Just like it's, it's not an easy thing to do.
[00:49:51.480 --> 00:49:56.040]   But with the synthetic data, you can just simulate it and get a billion images of before
[00:49:56.040 --> 00:49:58.600]   and after with perfect segmentation masks.
[00:49:58.600 --> 00:50:04.080]   So that was, that was the original inspiration for creating that, that data set.
[00:50:04.080 --> 00:50:07.200]   And then I think, you know, we're all big proponents of open source and I think open
[00:50:07.200 --> 00:50:10.920]   source data is sort of the next version of that.
[00:50:10.920 --> 00:50:16.520]   You know, if ML is going to revolutionize the world, which it is, we have to make that
[00:50:16.520 --> 00:50:20.240]   more democratic and the code is becoming super democratic.
[00:50:20.240 --> 00:50:22.840]   The data is not, right.
[00:50:22.840 --> 00:50:28.120]   You know, so I think that's, that's sort of an interesting gap and I'm not exactly sure
[00:50:28.120 --> 00:50:31.240]   how to fully close that gap.
[00:50:31.240 --> 00:50:33.600]   But I think that, you know, open sourcing the synthetic data set at the very least is
[00:50:33.600 --> 00:50:35.560]   a, is a cool way to help.
[00:50:35.560 --> 00:50:36.560]   Interesting.
[00:50:36.560 --> 00:50:38.960]   But you know, this seems very core to your business.
[00:50:38.960 --> 00:50:44.640]   You weren't worried that a competitor might use this, this data set to build a competitive
[00:50:44.640 --> 00:50:45.640]   algorithm.
[00:50:45.640 --> 00:50:46.640]   Yeah.
[00:50:46.640 --> 00:50:51.120]   I mean, you know, my, my opinion is like, there are, there's some great teams working
[00:50:51.120 --> 00:50:52.120]   on checkout.
[00:50:52.120 --> 00:50:55.000]   Like, you know, obviously I think highly of standard, but yeah, there's a, there's a couple
[00:50:55.000 --> 00:50:56.000]   of other great teams out there.
[00:50:56.000 --> 00:51:01.360]   You know, they're, they're doing this their own way and it would be sort of like, you
[00:51:01.360 --> 00:51:05.880]   know, cruise using a synthetic generator from Waymo or vice versa, you know, right.
[00:51:05.880 --> 00:51:06.880]   Could happen.
[00:51:06.880 --> 00:51:07.880]   Sure.
[00:51:07.880 --> 00:51:08.880]   Right.
[00:51:08.880 --> 00:51:11.600]   If they do like great and like, you know, you know, best of luck to you.
[00:51:11.600 --> 00:51:14.440]   But you know, I assume they've got their own stuff that they're, they're doing and switching
[00:51:14.440 --> 00:51:18.480]   costs are so high that, you know, they're so deeply invested in whatever synthetic thing
[00:51:18.480 --> 00:51:22.800]   they've got or XYZ that they're doing, like, it's just going to be too expensive for them
[00:51:22.800 --> 00:51:23.800]   to switch.
[00:51:23.800 --> 00:51:28.000]   So I think really the value of these open source initiatives is, is for the broader
[00:51:28.000 --> 00:51:31.640]   community so that people can get their hands on this, play around with it and, you know,
[00:51:31.640 --> 00:51:34.560]   come up with some other really cool application and show us what's possible, right?
[00:51:34.560 --> 00:51:40.560]   Like we're, we're so tunnel focused on tunnel visioned on, on, on trying to build this one
[00:51:40.560 --> 00:51:43.000]   thing that we're trying to get out.
[00:51:43.000 --> 00:51:45.040]   Maybe there's some other cool stuff that you can do with this.
[00:51:45.040 --> 00:51:48.400]   Have you seen any interesting applications yet?
[00:51:48.400 --> 00:51:54.240]   Not, not yet, but hopefully, hopefully someone who wants to come do Rust machine learning
[00:51:54.240 --> 00:51:55.240]   will get clone.
[00:51:55.240 --> 00:51:56.240]   Don't forget about Jacks.
[00:51:56.240 --> 00:51:57.240]   Jacks, yeah.
[00:51:57.240 --> 00:52:01.800]   Get clone, do something cool with it and then throw it over to us.
[00:52:01.800 --> 00:52:04.800]   And wait some biases, don't forget.
[00:52:04.800 --> 00:52:05.800]   Yes.
[00:52:05.800 --> 00:52:06.800]   Yes, exactly.
[00:52:06.800 --> 00:52:10.400]   Do you have any kind of like benchmark for accuracy on this dataset?
[00:52:10.400 --> 00:52:13.280]   Do you, do you think about it like that at all?
[00:52:13.280 --> 00:52:14.280]   We do.
[00:52:14.280 --> 00:52:15.280]   Yeah.
[00:52:15.280 --> 00:52:16.280]   I mean, it's, it's going to be similar.
[00:52:16.280 --> 00:52:19.800]   I don't know what it is, but we can follow up with, with the folks that, that built that
[00:52:19.800 --> 00:52:23.360]   dataset, but it's, it's something more similar, like intersection over union, right?
[00:52:23.360 --> 00:52:28.360]   Like it's how, how close are you getting that, that segmentation mass basically to the ground
[00:52:28.360 --> 00:52:29.360]   truth.
[00:52:29.360 --> 00:52:30.360]   Right, right.
[00:52:30.360 --> 00:52:31.360]   All right.
[00:52:31.360 --> 00:52:34.680]   Well, we always end with two questions that I want to make sure I ask you.
[00:52:34.680 --> 00:52:40.160]   So one question is what's an underrated topic anywhere in ML that, you know, if you had
[00:52:40.160 --> 00:52:45.160]   extra time, you'd love to look into or study.
[00:52:45.160 --> 00:52:46.160]   We touched on a lot of them.
[00:52:46.160 --> 00:52:47.640]   I guess some of them are underrated, right.
[00:52:47.640 --> 00:52:50.000]   But like, you know, I'm a huge believer in tooling.
[00:52:50.000 --> 00:52:53.400]   You got to pick the right tools and you got to keep pushing, pushing the tools forward.
[00:52:53.400 --> 00:52:58.400]   A huge believer in ops, you know, whether it's ladling or having some human loop component,
[00:52:58.400 --> 00:53:00.720]   like you've got to invest in world-class ops.
[00:53:00.720 --> 00:53:04.120]   And I think that's, that's, those are the unsung heroes in the world.
[00:53:04.120 --> 00:53:09.600]   Everyone, everyone wants to be an MLE, but you know, the operators are amazing folks
[00:53:09.600 --> 00:53:12.600]   who really make this possible.
[00:53:12.600 --> 00:53:19.520]   In terms of more like researchy in the ML world, you know, maybe this isn't a hot take,
[00:53:19.520 --> 00:53:23.640]   but you know, I'm still a big believer in symbolic reasoning.
[00:53:23.640 --> 00:53:24.640]   Wow.
[00:53:24.640 --> 00:53:30.760]   It's, it's, I, and I think, you know, maybe I'm just like one of those old foggies that
[00:53:30.760 --> 00:53:37.200]   is going to die on the Hill, but it's just so clear to me that the way our brains work
[00:53:37.200 --> 00:53:38.720]   is partially symbolic, right.
[00:53:38.720 --> 00:53:39.720]   Not fully, right.
[00:53:39.720 --> 00:53:41.960]   Obviously we have like, you know, you get some stroke of intuition.
[00:53:41.960 --> 00:53:46.080]   But that for the way we do item classification, it's like, who knows it's a, it's literally
[00:53:46.080 --> 00:53:49.800]   a deep network of real neurons, but it's so clear that when I'm introspecting the way
[00:53:49.800 --> 00:53:54.120]   that my brain works for something slightly higher and more abstract, that it's doing
[00:53:54.120 --> 00:53:55.120]   something more symbolic.
[00:53:55.120 --> 00:53:59.320]   And it's really kind of thinking through the sort of graph structure of the problem and
[00:53:59.320 --> 00:54:05.280]   breaking it down, exploring different, you know, aspects of the tree.
[00:54:05.280 --> 00:54:08.960]   And I think there's gotta be some way to merge it, merge it together.
[00:54:08.960 --> 00:54:09.960]   Right.
[00:54:09.960 --> 00:54:16.720]   So if I, if I had just made $800 million, I would be using Rust to solve how to bring
[00:54:16.720 --> 00:54:23.240]   symbolic logic and mega transformer models together to, you know, to rule the world and,
[00:54:23.240 --> 00:54:24.240]   you know, solve world hunger.
[00:54:24.240 --> 00:54:29.720]   Well, I kind of hope there's an exit in your future.
[00:54:29.720 --> 00:54:37.480]   I guess the last question is what's the hardest part about making machine learning work.
[00:54:37.480 --> 00:54:41.380]   And in your case, maybe I would say what's been the most surprisingly difficult part
[00:54:41.380 --> 00:54:46.880]   of kind of going from these image models to a working system in production that people
[00:54:46.880 --> 00:54:49.440]   can actually use to purchase stuff.
[00:54:49.440 --> 00:54:54.120]   I mean, so many things, but the world is, the world is messy.
[00:54:54.120 --> 00:54:55.120]   It's super messy.
[00:54:55.120 --> 00:54:56.120]   Right.
[00:54:56.120 --> 00:54:57.560]   And in this case, I literally mean messy, right?
[00:54:57.560 --> 00:55:00.680]   Cause like stores are, stores are chaotic places, right?
[00:55:00.680 --> 00:55:04.360]   Like there's thousands and thousands of items and most of them aren't in the place that
[00:55:04.360 --> 00:55:06.560]   the retailer wants them to be.
[00:55:06.560 --> 00:55:09.560]   And they have, they have these meticulous plans that they invest in called planograms
[00:55:09.560 --> 00:55:11.720]   where they optimize where all the products should go.
[00:55:11.720 --> 00:55:14.520]   And the CPGs are investing too, cause they're trying to sell you more Snickers.
[00:55:14.520 --> 00:55:20.400]   I don't know why I keep using Snickers, but, but you know, it's like they have this plan,
[00:55:20.400 --> 00:55:24.560]   but then show up at a C store, show up at a grocery store and stuff's everywhere.
[00:55:24.560 --> 00:55:25.560]   Right.
[00:55:25.560 --> 00:55:29.960]   And there's, there's people unpacking boxes and misplaced items and there's just random
[00:55:29.960 --> 00:55:33.520]   stuff on the floor and, you know, and they, they try really hard to keep the store clean,
[00:55:33.520 --> 00:55:36.280]   obviously, but it's just a pretty chaotic place.
[00:55:36.280 --> 00:55:37.720]   Retail is chaotic, right?
[00:55:37.720 --> 00:55:40.360]   You've got thousands of people coming through the store every day.
[00:55:40.360 --> 00:55:42.120]   Like it's going to get messy.
[00:55:42.120 --> 00:55:43.360]   Right.
[00:55:43.360 --> 00:55:44.360]   That's challenging.
[00:55:44.360 --> 00:55:45.360]   Right.
[00:55:45.360 --> 00:55:49.480]   Like it's a really dynamic visual data set and just random stuff happens, right?
[00:55:49.480 --> 00:55:53.400]   Like in the AV world, they talk about the long tail distribution of reality, but like,
[00:55:53.400 --> 00:55:54.800]   yeah, we, we see that.
[00:55:54.800 --> 00:55:55.800]   Right.
[00:55:55.800 --> 00:55:57.600]   Like, all right, give me, give me, give me some long tail cases.
[00:55:57.600 --> 00:55:58.600]   I love these.
[00:55:58.600 --> 00:56:03.120]   So we had a, one of, one of our stores, we had a listeria outbreak.
[00:56:03.120 --> 00:56:04.120]   And yeah.
[00:56:04.120 --> 00:56:09.040]   So they had to throw away all the fresh foods.
[00:56:09.040 --> 00:56:12.720]   And then they had, so in retail, they call it selling air.
[00:56:12.720 --> 00:56:14.000]   So you can't sell air.
[00:56:14.000 --> 00:56:16.800]   So they had to put something on the shelves, but they didn't have any fresh foods.
[00:56:16.800 --> 00:56:20.840]   So that store manager and store managers are typically super empowered in retail, right?
[00:56:20.840 --> 00:56:24.600]   Like, you know, there's these massive companies, but store managers actually get, get to have
[00:56:24.600 --> 00:56:26.760]   a lot of say, cause they're the ones that are trying to sell stuff.
[00:56:26.760 --> 00:56:27.760]   Right.
[00:56:27.760 --> 00:56:29.040]   And they know the local clientele, et cetera.
[00:56:29.040 --> 00:56:33.600]   So that's that local store manager was like, well, I'm just going to go get fresh food.
[00:56:33.600 --> 00:56:34.600]   I need sandwiches.
[00:56:34.600 --> 00:56:36.720]   Like I'm going to go get sandwiches.
[00:56:36.720 --> 00:56:40.600]   So they went and got new sandwiches, same day, brought them back, stocked their shelves.
[00:56:40.600 --> 00:56:41.600]   Right.
[00:56:41.600 --> 00:56:45.560]   And now suddenly from a computer vision perspective, you're like, well, we've never seen these
[00:56:45.560 --> 00:56:49.360]   products before.
[00:56:49.360 --> 00:56:50.640]   We don't know what the barcodes are.
[00:56:50.640 --> 00:56:51.640]   Right.
[00:56:51.640 --> 00:56:52.640]   We have no data set for this.
[00:56:52.640 --> 00:56:53.640]   Right.
[00:56:53.640 --> 00:56:55.560]   But the store manager is like, I need to turn around and start selling this stuff right
[00:56:55.560 --> 00:56:56.560]   now.
[00:56:56.560 --> 00:56:57.560]   Right.
[00:56:57.560 --> 00:57:00.160]   So I need to turn that around and start selling it pretty quickly.
[00:57:00.160 --> 00:57:01.160]   Wow.
[00:57:01.160 --> 00:57:03.040]   But that's, you know, that's super hard.
[00:57:03.040 --> 00:57:04.040]   Right.
[00:57:04.040 --> 00:57:07.680]   And that's, you know, again, it's, it's this really rich intersection of engineering, ML
[00:57:07.680 --> 00:57:09.760]   and operations and client support too.
[00:57:09.760 --> 00:57:10.760]   Right.
[00:57:10.760 --> 00:57:12.320]   Like it's, you know, you have to bring all those things together.
[00:57:12.320 --> 00:57:14.800]   This is not just ML.
[00:57:14.800 --> 00:57:17.400]   And I think that's, that's the lesson that we've learned over and over again is, you
[00:57:17.400 --> 00:57:22.240]   know, every piece of this has some connection to the shopper, to the retailer, to us as
[00:57:22.240 --> 00:57:23.240]   a business.
[00:57:23.240 --> 00:57:24.480]   And you have to bring all the stakeholders together.
[00:57:24.480 --> 00:57:28.920]   Like we're, we're a super cross-functional team and we love, you know, coming together
[00:57:28.920 --> 00:57:32.800]   and looking at all the different sides of the problem to ultimately make something that
[00:57:32.800 --> 00:57:33.800]   we can put out into the real world.
[00:57:33.800 --> 00:57:34.800]   Awesome.
[00:57:34.800 --> 00:57:35.800]   Well, thanks so much for your time, Jordan.
[00:57:35.800 --> 00:57:36.800]   That was super fun.
[00:57:36.800 --> 00:57:37.800]   Super informative.
[00:57:37.800 --> 00:57:38.800]   Yeah, this was awesome.
[00:57:38.800 --> 00:57:39.800]   Thank you.
[00:57:39.800 --> 00:57:40.800]   Awesome.
[00:57:40.800 --> 00:57:41.800]   Yeah.
[00:57:41.800 --> 00:57:42.800]   Thanks for having me on.
[00:57:42.800 --> 00:57:43.800]   Oh, my pleasure.
[00:57:43.800 --> 00:57:46.200]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:57:46.200 --> 00:57:50.920]   to the show notes in the description where you can find links to all the papers that
[00:57:50.920 --> 00:57:55.360]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:57:55.360 --> 00:57:56.360]   produce.
[00:57:56.360 --> 00:57:56.360]   So check it out.
[00:57:56.360 --> 00:57:59.420]   [MUSIC PLAYING]

