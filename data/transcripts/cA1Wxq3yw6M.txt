
[00:00:00.000 --> 00:00:05.520]   chess used to be before, I think, Alpha Chess, the latest version of Stockfish, the best chess
[00:00:05.520 --> 00:00:11.680]   programs were these hybrid systems with the human in the loop. But then at some point, the AI got
[00:00:11.680 --> 00:00:17.680]   good enough that the human in the loop only messes things up. Do you ever have cases where you think
[00:00:17.680 --> 00:00:25.280]   that the ML system works better than a human operator and maybe it shouldn't actually give
[00:00:25.280 --> 00:00:31.600]   the final decision to a doctor? It's a great question. I think, as I said before,
[00:00:31.600 --> 00:00:38.560]   generally speaking, it's not that hard. Well, I mean, it's taking us a few years, but it's not
[00:00:38.560 --> 00:00:44.960]   "that hard" to get an algorithm that is better than the average physician. Now that being said,
[00:00:44.960 --> 00:00:52.160]   it's much harder to get an algorithm that is better than the combination of the human plus the AI.
[00:00:52.160 --> 00:01:00.560]   So even in the examples that you mentioned, the combination of humans plus AI in chess,
[00:01:00.560 --> 00:01:07.520]   if the human is relatively good, meaning a professional player, it's hard to beat. So an AI
[00:01:07.520 --> 00:01:14.560]   alone versus a combination of AI plus human is hard to beat. But in the case of healthcare,
[00:01:14.560 --> 00:01:19.600]   one of the important things to understand is that it is an imperfect information game. So
[00:01:20.400 --> 00:01:26.800]   it's not about if you had the perfect information, the algorithm would probably always beat the
[00:01:26.800 --> 00:01:33.280]   human, and you would be very easy to just beat the human with all the perfect information
[00:01:33.280 --> 00:01:40.000]   in the world. However, in the case of medicine, healthcare, there's a lot that goes on with
[00:01:40.000 --> 00:01:47.200]   empathizing with the patient, understanding even things that are called social determinants,
[00:01:47.200 --> 00:01:53.200]   where do they come from, how are they going to understand, how can you communicate the possibility
[00:01:53.200 --> 00:02:02.880]   of something being likely or not. And that is very hard to do if you're not a human that is trained
[00:02:02.880 --> 00:02:12.480]   to have this level of empathy, so to speak. So there's the interesting question, and I keep
[00:02:12.480 --> 00:02:18.320]   talking to people that have very different opinions with that. There is the purely extreme
[00:02:18.320 --> 00:02:25.520]   rational opinion that all you want to have from the outcome as a patient is have a list of possible
[00:02:25.520 --> 00:02:31.920]   diagnosis with a probability, and you'll be able to interpret them. And if you're a hyper rational
[00:02:31.920 --> 00:02:38.320]   person, that is true. You want to know if you have a 0.2% probability of having cancer, you want to
[00:02:38.320 --> 00:02:44.720]   know that there is a 0.2% probability, and you think you can deal with it. The reality is that
[00:02:44.720 --> 00:02:51.120]   most people don't know how to interpret that. What does that even mean, a 0.2% probability of
[00:02:51.120 --> 00:02:56.640]   having cancer? And do you want to communicate that, or do you want to interpret that and then
[00:02:56.640 --> 00:03:01.920]   follow the patient along and make sure that that probability doesn't get to a point that is more
[00:03:01.920 --> 00:03:07.840]   likely than not? And I think that's where the human judgment is really key, and that's very
[00:03:07.840 --> 00:03:14.080]   different from a pure probability that is output from any kind of machine learning algorithm.
[00:03:14.080 --> 00:03:19.840]   If you're enjoying Gradient Descent, I'd really love for you to check out Fully Connected,
[00:03:19.840 --> 00:03:25.280]   which is an inclusive machine learning community that we're building to let everyone know about
[00:03:25.280 --> 00:03:32.320]   all the stuff going on in ML and all the new research coming out. If you go to wmb.ai/fc,
[00:03:32.320 --> 00:03:36.000]   you can see all the different stuff that we do, including Gradient Descent, but also
[00:03:36.000 --> 00:03:41.440]   Salons where we talk about new research and folks share insights, AMAs where you can directly
[00:03:41.440 --> 00:03:46.400]   connect with members of our community, and a Slack channel where you can get answers to
[00:03:46.400 --> 00:03:51.760]   everything from very basic questions about ML to bug reports on weights and biases to
[00:03:51.760 --> 00:03:55.840]   how to hire an ML team. We're looking forward to meeting you.

