
[00:00:00.000 --> 00:00:12.320]   I'm just double checking if you're live.
[00:00:12.320 --> 00:00:17.280]   Yuvraj is already there on the YouTube page.
[00:00:17.280 --> 00:00:18.280]   So that's awesome.
[00:00:18.280 --> 00:00:23.640]   Great to see you, Yuvraj, like always.
[00:00:23.640 --> 00:00:24.640]   I believe you're live.
[00:00:24.640 --> 00:00:31.520]   I have this pending, overlooming paranoia of messing up everything so many times that
[00:00:31.520 --> 00:00:33.360]   I always want to double check.
[00:00:33.360 --> 00:00:34.360]   But I believe things are set.
[00:00:34.360 --> 00:00:36.040]   So I guess we can get started.
[00:00:36.040 --> 00:00:39.600]   Hey, everybody, welcome back to the Kera's reading group.
[00:00:39.600 --> 00:00:43.240]   We're going through, let me switch this.
[00:00:43.240 --> 00:00:46.120]   We're still in the middle of going through this book.
[00:00:46.120 --> 00:00:47.480]   It's an incredible book.
[00:00:47.480 --> 00:00:51.960]   And if it's the first time that you're joining, I'm fumbling because I'm too excited.
[00:00:51.960 --> 00:00:55.040]   If this is the first time you're joining, we're learning deep.
[00:00:55.040 --> 00:01:00.480]   We're reading deep learning with Python second edition by Franchois Joliot.
[00:01:00.480 --> 00:01:05.920]   It is one of the best resources on Kera's and we have been reading it for a while now.
[00:01:05.920 --> 00:01:08.320]   Hey, Akash, great to see you as well.
[00:01:08.320 --> 00:01:09.320]   Hey, Anush.
[00:01:09.320 --> 00:01:13.080]   Thanks for joining us, everyone.
[00:01:13.080 --> 00:01:14.080]   And thanks for anyone.
[00:01:14.080 --> 00:01:17.240]   Thanks to anyone who's watching the recording as well.
[00:01:17.240 --> 00:01:25.960]   So so far, we've been reading the book and we write, I believe, across the 60% marker.
[00:01:25.960 --> 00:01:31.500]   So this is the fourth session and I haven't, like always ambitious agenda.
[00:01:31.500 --> 00:01:35.000]   If I get to cover 70% of this, I'm happy by that.
[00:01:35.000 --> 00:01:39.560]   Even if I cover 50% of it, I'm still happy because my goal is to make sure whatever I
[00:01:39.560 --> 00:01:42.220]   cover here is high quality.
[00:01:42.220 --> 00:01:48.760]   So the agenda for today is we'll not implement, we'll run through an implementation of Cognext.
[00:01:48.760 --> 00:01:52.920]   We'll understand what is time series and we'll understand what is attention.
[00:01:52.920 --> 00:01:56.020]   So quite the bulky agenda, like always.
[00:01:56.020 --> 00:02:01.740]   Last week, we had understanding of Cognets and advanced computer vision, which means
[00:02:01.740 --> 00:02:08.140]   what is image segmentation, which is what is object detection, all of those things.
[00:02:08.140 --> 00:02:12.240]   So this, I said, is the like I said, is the fourth session, and I believe we'll have two
[00:02:12.240 --> 00:02:13.240]   more sessions.
[00:02:13.240 --> 00:02:18.280]   And for the last one, I have planned to run a blogathon where I invite everyone to write
[00:02:18.280 --> 00:02:23.800]   a blog and hopefully we can share some distribute some prizes amongst the winners.
[00:02:23.800 --> 00:02:30.600]   So just as a reminder, something to keep in your head, please consider submitting to 27
[00:02:30.600 --> 00:02:37.120]   days of Keras or please try to think of any ideas that you can blog about in Keras.
[00:02:37.120 --> 00:02:43.760]   In this, all of this is just to encourage you all to participate in contributing to
[00:02:43.760 --> 00:02:51.040]   open source or starting your, not really starting you're far ahead of the curve, but to propagate
[00:02:51.040 --> 00:02:55.800]   or to propel your learning journey in deep learning.
[00:02:55.800 --> 00:02:56.960]   So here's the agenda for today.
[00:02:56.960 --> 00:02:59.480]   I'll quickly recap what we learned in the last session.
[00:02:59.480 --> 00:03:01.720]   We're already doing that.
[00:03:01.720 --> 00:03:05.120]   From there, I'll try to summarize chapter nine through 11.
[00:03:05.120 --> 00:03:07.280]   And we'll go through what is Conext.
[00:03:07.280 --> 00:03:10.880]   I believe I'll first summarize chapter nine and then jump on to Conext.
[00:03:10.880 --> 00:03:17.240]   I'll jump around a bit and I want to first theoretically explain what is attention.
[00:03:17.240 --> 00:03:19.880]   So give you a high level understanding.
[00:03:19.880 --> 00:03:26.160]   And we might have a guest from one of the companies that builds models related to attention
[00:03:26.160 --> 00:03:27.680]   or transformers.
[00:03:27.680 --> 00:03:28.720]   That might happen next week.
[00:03:28.720 --> 00:03:30.040]   I don't want to spoil anything.
[00:03:30.040 --> 00:03:31.400]   It's still in the works.
[00:03:31.400 --> 00:03:36.720]   If it doesn't, fortunately or unfortunately, I'll still cover what are attention based
[00:03:36.720 --> 00:03:40.720]   models or what are transformer models in Keras code next, in the next session.
[00:03:40.720 --> 00:03:42.640]   So for today, it'll just be a high level overview.
[00:03:42.640 --> 00:03:44.440]   And like always, please keep the questions coming.
[00:03:44.440 --> 00:03:48.480]   I'm always monitoring the chart and always happy to answer as many questions.
[00:03:48.480 --> 00:03:49.520]   No question is too stupid.
[00:03:49.520 --> 00:03:52.720]   All questions are welcome.
[00:03:52.720 --> 00:03:55.200]   Last week, we looked at advanced computer vision.
[00:03:55.200 --> 00:03:57.100]   We looked at image augmentation.
[00:03:57.100 --> 00:04:02.400]   So let me jump out of full screen and head over to augmentations.
[00:04:02.400 --> 00:04:05.880]   We looked at augmentations library as well.
[00:04:05.880 --> 00:04:08.040]   It's an absolutely fantastic framework.
[00:04:08.040 --> 00:04:10.520]   Please consider checking it out.
[00:04:10.520 --> 00:04:18.320]   And not just for application, it's also written by one of one of not one of the absolute best
[00:04:18.320 --> 00:04:19.320]   in computer vision.
[00:04:19.320 --> 00:04:23.940]   I would say these are Kaggle's researchers and really, really well versed practitioners.
[00:04:23.940 --> 00:04:26.160]   So even their code, it's completely open source.
[00:04:26.160 --> 00:04:30.040]   I would encourage you to start diving into the source code as well.
[00:04:30.040 --> 00:04:32.000]   Again, it's all up to you.
[00:04:32.000 --> 00:04:36.520]   I can only give suggestions based on my small perspective to the world.
[00:04:36.520 --> 00:04:41.280]   We also looked at Keras API and how it works with the Weights and Biases API.
[00:04:41.280 --> 00:04:44.040]   So how can you track experiments inside of Keras?
[00:04:44.040 --> 00:04:47.760]   It is just one line of a callback.
[00:04:47.760 --> 00:04:52.480]   So it doesn't take that much effort.
[00:04:52.480 --> 00:04:54.080]   Homework from the previous session.
[00:04:54.080 --> 00:04:55.600]   I suggested all of these things.
[00:04:55.600 --> 00:04:59.000]   I believe everyone is ready to jump on to Kaggle competitions.
[00:04:59.000 --> 00:05:02.960]   I've not seen anyone participate in Kaggle competitions from this group yet.
[00:05:02.960 --> 00:05:07.320]   So I would really request and urge you to start competing on Kaggle.
[00:05:07.320 --> 00:05:10.000]   It is, in my opinion, one of the best ways to learn.
[00:05:10.000 --> 00:05:13.160]   And I'd also suggested playing around with augmentation.
[00:05:13.160 --> 00:05:15.120]   I believe I didn't see that yet.
[00:05:15.120 --> 00:05:18.760]   I believe Yuvraj had written an incredible summary, which I'm yet to read.
[00:05:18.760 --> 00:05:22.360]   So I didn't mention that from the previous session.
[00:05:22.360 --> 00:05:28.960]   But again, please consider writing a blog post on augmentations in Keras and submitting
[00:05:28.960 --> 00:05:30.400]   to Kaggle competitions.
[00:05:30.400 --> 00:05:33.560]   I really think it will bring you a lot of value.
[00:05:33.560 --> 00:05:35.960]   So I believe we've summarized the previous session.
[00:05:35.960 --> 00:05:37.600]   We're on to the next bit.
[00:05:37.600 --> 00:05:40.200]   We're right in the middle of chapter 9.
[00:05:40.200 --> 00:05:43.240]   So I'll try to wrap that up first.
[00:05:43.240 --> 00:05:48.120]   And from there, we'll try to understand how can we apply deep learning to time series.
[00:05:48.120 --> 00:05:53.620]   As a spoiler, there's not many secret techniques being shared there.
[00:05:53.620 --> 00:05:56.760]   So it actually introduces you to RNN and LSTM.
[00:05:56.760 --> 00:06:00.520]   That is the ground reasoning and also some structure around time series.
[00:06:00.520 --> 00:06:04.360]   But don't expect it to cover a lot of secret tips around time series.
[00:06:04.360 --> 00:06:06.320]   It's a small chapter.
[00:06:06.320 --> 00:06:11.320]   And if you want a better taste of it, Kaggle.
[00:06:11.320 --> 00:06:13.280]   That's why I suggested Kaggle.
[00:06:13.280 --> 00:06:18.800]   From there, we'll go into deep learning with text and I believe generative deep learning
[00:06:18.800 --> 00:06:20.000]   will be the last chapter.
[00:06:20.000 --> 00:06:25.960]   Again, I've tried not to cover too much theory because first of all, I don't want to directly
[00:06:25.960 --> 00:06:27.240]   share the book's content.
[00:06:27.240 --> 00:06:30.620]   These are just summaries of the chapters and some additional thoughts.
[00:06:30.620 --> 00:06:34.960]   So I believe we might skip chapter 13 or 14 or we might have a very short session and
[00:06:34.960 --> 00:06:41.120]   I want to hold an extra session where we just do a blogathon or a hackathon of some format.
[00:06:41.120 --> 00:06:45.560]   As a reminder, here's what image segmentation is.
[00:06:45.560 --> 00:06:48.960]   It's using computer vision.
[00:06:48.960 --> 00:06:54.200]   And the point to note here is you could use deep learning or not use deep learning to
[00:06:54.200 --> 00:06:57.520]   segment different portions of the image.
[00:06:57.520 --> 00:07:01.680]   So this is for let's say a self-driving car problem where the car is trying to understand
[00:07:01.680 --> 00:07:03.480]   what's going on in the world.
[00:07:03.480 --> 00:07:08.960]   And to understand that, it needs to be able to understand every single pixel inside of
[00:07:08.960 --> 00:07:10.600]   the frame.
[00:07:10.600 --> 00:07:14.440]   And as you can see, humans are probably all labeled with blue.
[00:07:14.440 --> 00:07:18.920]   There might be some false positives, which I think is fine for this case.
[00:07:18.920 --> 00:07:21.800]   Cars are labeled with the color red.
[00:07:21.800 --> 00:07:26.480]   So we're trying to understand every single pixel inside of this image.
[00:07:26.480 --> 00:07:30.520]   Object detection requires you to create a box around every object.
[00:07:30.520 --> 00:07:35.040]   So you want to understand where inside of the image different objects are and label
[00:07:35.040 --> 00:07:38.800]   them with a certain accuracy.
[00:07:38.800 --> 00:07:42.800]   And we also looked at how CNNs outshine fully connected networks.
[00:07:42.800 --> 00:07:48.600]   So you can also improve them further by adding some regularization, adding width, adding
[00:07:48.600 --> 00:07:54.120]   depth, training them on a graphic card and stuff like that.
[00:07:54.120 --> 00:07:58.440]   So I believe that is pretty much the theoretical things I wanted to cover.
[00:07:58.440 --> 00:08:01.360]   We have some amount of code that we should be covering today.
[00:08:01.360 --> 00:08:04.000]   So I'd like to jump into that.
[00:08:04.000 --> 00:08:09.040]   And as Yoram says, hello, thanks for joining us Yoram.
[00:08:09.040 --> 00:08:10.960]   And like always, please keep the questions coming.
[00:08:10.960 --> 00:08:18.200]   I'll also mention another repository that I might not be able to go through given my
[00:08:18.200 --> 00:08:19.200]   ambitious agenda.
[00:08:19.200 --> 00:08:26.600]   So let me pop this up in the banner as well.
[00:08:26.600 --> 00:08:30.480]   I've created a repository called groundbreaking papers.
[00:08:30.480 --> 00:08:36.680]   This is an initiative at Weights and Biases where I'll be covering papers that really
[00:08:36.680 --> 00:08:42.000]   change or really introduce novel concepts, hopefully, although that time only decides
[00:08:42.000 --> 00:08:43.120]   these things.
[00:08:43.120 --> 00:08:45.920]   And this repository will be updated at least once a week.
[00:08:45.920 --> 00:08:50.440]   So I'm aiming to release a new annotation every week, along with the paper summary and
[00:08:50.440 --> 00:08:51.780]   a colab.
[00:08:51.780 --> 00:08:56.940]   So if you if you're interested in being on top of research, I believe this could really
[00:08:56.940 --> 00:08:59.160]   bring you a lot of value.
[00:08:59.160 --> 00:09:04.680]   And I'll be using the ConNet annotation from here to make my life easy.
[00:09:04.680 --> 00:09:11.120]   So I've all sorry, this should be called next, but the paper is titled a ConNet for 2020.
[00:09:11.120 --> 00:09:13.000]   So I've already annotated the paper.
[00:09:13.000 --> 00:09:18.020]   And if anyone is interested, I would request you to check this repository out.
[00:09:18.020 --> 00:09:24.480]   So let's start with chapter nine, part one, which is where I had left things off.
[00:09:24.480 --> 00:09:30.920]   Let me drop the link as well in the chart.
[00:09:30.920 --> 00:09:36.600]   Oh, I apologize for my slides not visible earlier.
[00:09:36.600 --> 00:09:39.000]   I'm sorry about that.
[00:09:39.000 --> 00:09:43.600]   Sometimes there's this weird issue with presenting I'll make sure that doesn't happen in the
[00:09:43.600 --> 00:09:44.600]   future.
[00:09:44.600 --> 00:09:48.600]   Could someone please also confirm if I should zoom in further or if this text size is good
[00:09:48.600 --> 00:09:49.600]   enough?
[00:09:49.600 --> 00:10:01.040]   There's a 30 second delay that happens.
[00:10:01.040 --> 00:10:04.100]   So I'm quickly waiting for someone to confirm.
[00:10:04.100 --> 00:10:08.840]   And then I'll continue.
[00:10:08.840 --> 00:10:11.080]   Awesome.
[00:10:11.080 --> 00:10:16.200]   Two people have confirmed.
[00:10:16.200 --> 00:10:17.200]   Thank you so much.
[00:10:17.200 --> 00:10:20.280]   I'm sorry for the awkward pause there.
[00:10:20.280 --> 00:10:25.480]   So the chapter nine is called advanced computer vision.
[00:10:25.480 --> 00:10:29.240]   And there are a few things, different things that are covered inside of the chapter.
[00:10:29.240 --> 00:10:32.320]   The first one covers an image segmentation example.
[00:10:32.320 --> 00:10:36.800]   So we're downloading from something that looks like a pets dataset.
[00:10:36.800 --> 00:10:44.800]   Let's go over there and see if we can find anything on this website.
[00:10:44.800 --> 00:10:48.440]   I'm just going to the website from where this was downloaded.
[00:10:48.440 --> 00:10:51.200]   And looks like I might have to jump around a few links a bit.
[00:10:51.200 --> 00:10:55.600]   So let's see where was this.
[00:10:55.600 --> 00:11:01.600]   Let's try to go to the link directly and see if that helps.
[00:11:01.600 --> 00:11:05.300]   Yep, there we go.
[00:11:05.300 --> 00:11:09.460]   So we're using the Oxford triple IT pet dataset.
[00:11:09.460 --> 00:11:16.900]   This apparently has 37 categories of pets and roughly 200 images for each class.
[00:11:16.900 --> 00:11:20.080]   So it's cats and dogs of different breeds.
[00:11:20.080 --> 00:11:25.460]   It looks like I didn't know there's a cat breed called Bombay.
[00:11:25.460 --> 00:11:26.460]   That's interesting.
[00:11:26.460 --> 00:11:32.260]   Let's see which others one, which other ones do I recognize?
[00:11:32.260 --> 00:11:41.740]   I recognize Shiba Inu for not good reasons, not financial advice, by the way.
[00:11:41.740 --> 00:11:45.140]   This is why the internet loves pet images, because you always get lost.
[00:11:45.140 --> 00:11:46.380]   Let's come back to business.
[00:11:46.380 --> 00:11:54.700]   So it looks like we're downloading a dataset consisting of pets, pet images, and we'll
[00:11:54.700 --> 00:11:56.820]   be trying to segment these out.
[00:11:56.820 --> 00:12:00.780]   And we also have annotations, which of course would be required to segment these out from
[00:12:00.780 --> 00:12:02.580]   the background.
[00:12:02.580 --> 00:12:08.820]   So inside of this Colab, and again, this Colab is just from the GitHub repository of the
[00:12:08.820 --> 00:12:09.820]   book.
[00:12:09.820 --> 00:12:10.820]   I have not written this.
[00:12:10.820 --> 00:12:12.540]   It's by Franchot himself.
[00:12:12.540 --> 00:12:20.700]   So we set the input directory to images, we set the mappings, and we try to sort the input
[00:12:20.700 --> 00:12:23.460]   images path, like so.
[00:12:23.460 --> 00:12:26.220]   And we check for images that end with jpg.
[00:12:26.220 --> 00:12:30.100]   This used to be one of the mistakes that I would make, where I would concatenate files
[00:12:30.100 --> 00:12:34.500]   which didn't have this file extension.
[00:12:34.500 --> 00:12:36.340]   And that would cause me all sorts of problems.
[00:12:36.340 --> 00:12:40.460]   Because if you're working on a hard disk, between different OSes, Mac OS tends to create
[00:12:40.460 --> 00:12:45.260]   a garbage file, I believe, and that can cause problems.
[00:12:45.260 --> 00:12:51.100]   So small thing to remember, please try to check for this.
[00:12:51.100 --> 00:12:55.740]   And the reason why I'm suggesting this is ideally, the best way to learn in my opinion
[00:12:55.740 --> 00:13:01.540]   is once you go through this Colab, don't just run it like I'm doing right now, I'm doing
[00:13:01.540 --> 00:13:04.860]   the worst thing possible for learning.
[00:13:04.860 --> 00:13:10.660]   Try to jot down the points of what's going on inside of the Colab, try to understand
[00:13:10.660 --> 00:13:16.180]   different functions, which I try to explain in this session, and then try to replicate
[00:13:16.180 --> 00:13:19.380]   it by yourself.
[00:13:19.380 --> 00:13:30.540]   So I would encourage everyone to follow that approach.
[00:13:30.540 --> 00:13:35.540]   So from there, we'll import Macplotlib and we'll import from Keras util load image and
[00:13:35.540 --> 00:13:36.780]   image to array.
[00:13:36.780 --> 00:13:40.100]   Let's see what's inside of Keras utils.
[00:13:40.100 --> 00:13:43.460]   So let's hop over to Keras.io.
[00:13:43.460 --> 00:13:45.220]   And we'll go to documentation.
[00:13:45.220 --> 00:14:01.060]   Let's see, utils, import, utilities.
[00:14:01.060 --> 00:14:04.260]   There are a bunch of these I'm trying to find the image data format.
[00:14:04.260 --> 00:14:05.260]   Is that the one?
[00:14:05.260 --> 00:14:06.260]   I don't think so.
[00:14:06.260 --> 00:14:15.380]   It's load image, it doesn't pop up immediately.
[00:14:15.380 --> 00:14:20.020]   I believe it might also be plot model, but I don't want to mix things up.
[00:14:20.020 --> 00:14:24.300]   So this might be the one that I'm trying to look for.
[00:14:24.300 --> 00:14:25.300]   I believe yes, it is.
[00:14:25.300 --> 00:14:33.620]   So all of these help us manage different functions required around loading images.
[00:14:33.620 --> 00:14:37.980]   Again, I actually looked this up earlier this evening, and now I'm getting confused myself,
[00:14:37.980 --> 00:14:44.100]   but I wanted to show where, what exactly do these utilities do.
[00:14:44.100 --> 00:14:46.940]   So I have another trick to actually try that.
[00:14:46.940 --> 00:14:59.020]   I'll put a question mark and see if this helps me.
[00:14:59.020 --> 00:15:01.020]   That is taking too long apologies for that.
[00:15:01.020 --> 00:15:06.380]   Sorry, I just wanted to remind everyone as a quick nudge that you should be checking
[00:15:06.380 --> 00:15:12.460]   functions inside of the documentation or you can use a question mark to check what parameters
[00:15:12.460 --> 00:15:13.460]   go into it.
[00:15:13.460 --> 00:15:26.980]   So that's what I was trying to do here, but I'll continue without spending more time there.
[00:15:26.980 --> 00:15:33.140]   So we've loaded the image and we've converted it to an array using different utilities.
[00:15:33.140 --> 00:15:35.060]   I'm sorry, I'm trying to keep an eye out on the chart.
[00:15:35.060 --> 00:15:38.580]   That's why I keep getting distracted a bit.
[00:15:38.580 --> 00:15:43.420]   We need the image to be converted to an array that from which we can further load it into
[00:15:43.420 --> 00:15:44.420]   memory.
[00:15:44.420 --> 00:15:46.940]   So that's why we need to do these steps.
[00:15:46.940 --> 00:15:50.660]   And also the load image function.
[00:15:50.660 --> 00:15:52.060]   Now it tells me what it is.
[00:15:52.060 --> 00:15:55.940]   So it's a, it loads an image into PIL format.
[00:15:55.940 --> 00:15:58.580]   I'm assuming everyone can see this pop up.
[00:15:58.580 --> 00:16:03.620]   And if I didn't know this, if you hover over a function in Colab, it shows you the function
[00:16:03.620 --> 00:16:04.620]   parameters.
[00:16:04.620 --> 00:16:09.320]   I'm not the biggest Colab fan, honestly.
[00:16:09.320 --> 00:16:14.820]   So now we can plot this image because I am sure expects a PIL image and it can handle
[00:16:14.820 --> 00:16:17.060]   that really well.
[00:16:17.060 --> 00:16:20.860]   And as you see, it's a cat image inside of the frame.
[00:16:20.860 --> 00:16:25.200]   And we can also print, I believe, the mask.
[00:16:25.200 --> 00:16:30.700]   So we normalize it first of all, because these might not be in the right format.
[00:16:30.700 --> 00:16:34.820]   And from there, we print out the normalized array of the mask.
[00:16:34.820 --> 00:16:39.580]   So this tells us where is exactly the cat.
[00:16:39.580 --> 00:16:43.620]   And this is a segmentation problem to be specific.
[00:16:43.620 --> 00:16:47.820]   It's an object extraction problem, I believe.
[00:16:47.820 --> 00:16:54.200]   Kaggle had this competition called Caravana.
[00:16:54.200 --> 00:17:08.520]   Happened I believe five years ago.
[00:17:08.520 --> 00:17:09.520]   This is the correct one.
[00:17:09.520 --> 00:17:13.680]   So here's another example where such problem could be useful.
[00:17:13.680 --> 00:17:18.060]   This Kaggle competition was hosted four years ago, and it was quite challenging for the
[00:17:18.060 --> 00:17:19.120]   time then.
[00:17:19.120 --> 00:17:20.240]   It still is.
[00:17:20.240 --> 00:17:26.000]   But now it's at the point where you can, after today's session, apply your knowledge to this.
[00:17:26.000 --> 00:17:35.200]   So in this image, you are required to segment out the car from the image, and then be able
[00:17:35.200 --> 00:17:36.720]   to extract it.
[00:17:36.720 --> 00:17:42.000]   So I assume this is for Caravana, the company who is hosting this competition to be able
[00:17:42.000 --> 00:17:46.720]   to produce really nice arts, where they, you know, throw in a nice background after extracting
[00:17:46.720 --> 00:17:47.720]   the car.
[00:17:47.720 --> 00:17:53.840]   Now, it's fairly solid, I would say, because zoom does a really nice job of blurring your
[00:17:53.840 --> 00:17:54.840]   background.
[00:17:54.840 --> 00:17:57.080]   That is another place where you see this in action.
[00:17:57.080 --> 00:18:01.720]   So that's what we're aiming for here.
[00:18:01.720 --> 00:18:08.120]   From there, we try to define different functions around part two, input images and part two
[00:18:08.120 --> 00:18:09.120]   targets.
[00:18:09.120 --> 00:18:10.120]   So we load all of these.
[00:18:10.120 --> 00:18:12.840]   These are helper functions.
[00:18:12.840 --> 00:18:16.060]   And now we will define a simple Keras model.
[00:18:16.060 --> 00:18:20.480]   So this is for the first time I believe when we see conf 2D transpose.
[00:18:20.480 --> 00:18:30.120]   So let us see what that is.
[00:18:30.120 --> 00:18:32.960]   Transposed convolution layer sometimes called deconvolution.
[00:18:32.960 --> 00:18:37.940]   The need for transposed convolution generally arises from desire to use a transformation
[00:18:37.940 --> 00:18:41.800]   going to the opposite direction of the convolution.
[00:18:41.800 --> 00:18:46.960]   So again, we're trying to segment out the cat.
[00:18:46.960 --> 00:18:52.240]   And for that, we need to be able to run something known as a deconvolution.
[00:18:52.240 --> 00:18:57.520]   So we're trying to run an image segmentation and inside of that we define the architecture
[00:18:57.520 --> 00:19:05.560]   like so where we have a bunch of conf 2D layers followed by conf 2D transpose.
[00:19:05.560 --> 00:19:15.160]   And then the output becomes a conf 2D followed by a softmax.
[00:19:15.160 --> 00:19:17.000]   So this would be the final model there.
[00:19:17.000 --> 00:19:21.560]   And further we compile this model and call it on the data set.
[00:19:21.560 --> 00:19:23.040]   We train it for a while.
[00:19:23.040 --> 00:19:25.200]   I crashed my browser earlier.
[00:19:25.200 --> 00:19:27.880]   So it's retraining quite slowly.
[00:19:27.880 --> 00:19:33.040]   But if you were to train this further, you would be able to predict mass and I believe
[00:19:33.040 --> 00:19:36.040]   it would this time work on a dog image.
[00:19:36.040 --> 00:19:41.160]   So to go into more depth, I don't think they tell about this in the book.
[00:19:41.160 --> 00:19:45.760]   But this is one of the common ways of, I believe, creating a unit.
[00:19:45.760 --> 00:19:48.360]   Unit is slightly different, I believe, to this.
[00:19:48.360 --> 00:19:50.480]   But this is one of the approaches around that.
[00:19:50.480 --> 00:19:56.720]   And this is how you segment out an object from its background.
[00:19:56.720 --> 00:20:03.800]   If you want to learn more about unit, we covered this in a paper reading group on our monthly
[00:20:03.800 --> 00:20:04.800]   TRG livestream.
[00:20:04.800 --> 00:20:07.960]   So you can check that out, I believe.
[00:20:07.960 --> 00:20:11.080]   But this was it for the image segmentation demo.
[00:20:11.080 --> 00:20:15.400]   And from there, we can jump to modern Connet architecture patterns.
[00:20:15.400 --> 00:20:25.920]   Let me see if there are any questions real quick.
[00:20:25.920 --> 00:20:31.920]   I see a question if we flip x y in the segmentation data set, will the network generate image
[00:20:31.920 --> 00:20:33.440]   from annotation?
[00:20:33.440 --> 00:20:40.620]   No, that's not possible because annotation only tells you different.
[00:20:40.620 --> 00:20:43.360]   So let's, let's try to understand your question.
[00:20:43.360 --> 00:20:51.160]   You are asking if you just have a bounding label around me in an image, you're suggesting
[00:20:51.160 --> 00:20:57.200]   if we set the input to just the frame, will it be able to fill in the content?
[00:20:57.200 --> 00:21:03.680]   That's not possible because we're not using any technique that can fill in these pixels.
[00:21:03.680 --> 00:21:10.560]   These are known as image in painting.
[00:21:10.560 --> 00:21:14.120]   So if you're curious, I would request you to look this up.
[00:21:14.120 --> 00:21:25.720]   Nvidia Research has a bunch of repositories and also online demos around stuff like this.
[00:21:25.720 --> 00:21:31.000]   I still don't understand her question from the mask I want to generate the image.
[00:21:31.000 --> 00:21:35.880]   If for that, I believe you will have to use image in painting unless I'm confusing your
[00:21:35.880 --> 00:21:36.880]   question.
[00:21:36.880 --> 00:21:45.880]   Awesome, I'll continue but please feel free to follow up if that doesn't answer it.
[00:21:45.880 --> 00:21:51.280]   So next up is modern Connet architecture patterns.
[00:21:51.280 --> 00:21:57.720]   Let me ask to anyone in the audience, does anyone want to answer what is batch norm?
[00:21:57.720 --> 00:21:59.800]   Or has anyone read the batch norm paper?
[00:21:59.800 --> 00:22:02.160]   Why do we use batch norm at all?
[00:22:02.160 --> 00:22:04.520]   Why is it useful?
[00:22:04.520 --> 00:22:30.560]   The question is, what is batch norm?
[00:22:30.560 --> 00:22:35.040]   I wanted to again, I've been saying this constantly, and at least for the remaining session, I'll
[00:22:35.040 --> 00:22:36.440]   keep saying this.
[00:22:36.440 --> 00:22:40.520]   But I just wanted to ask if anyone has read the batch norm paper.
[00:22:40.520 --> 00:22:44.080]   And I managed to pull it up real quickly on archive.
[00:22:44.080 --> 00:22:47.160]   But this had a lot of mathematical equations.
[00:22:47.160 --> 00:22:50.320]   I don't want anyone to be able to read this so fast.
[00:22:50.320 --> 00:22:54.440]   I'm just scrolling through real quick just to tell what was covered in the paper.
[00:22:54.440 --> 00:22:57.200]   So a lot of quite scary questions.
[00:22:57.200 --> 00:23:02.040]   Honestly, to me, I can't, I can't understand what these partial differentiation totally
[00:23:02.040 --> 00:23:05.440]   means and what is going on here.
[00:23:05.440 --> 00:23:11.040]   This was a very thorough and very scary paper the first time I looked at it.
[00:23:11.040 --> 00:23:14.080]   But let's see how do we utilize that in Keras.
[00:23:14.080 --> 00:23:16.240]   So the question was, what is batch norm?
[00:23:16.240 --> 00:23:19.600]   And if you were to now you might have blog posts around it.
[00:23:19.600 --> 00:23:25.120]   But at least when I first heard of it, the best thing was to find the paper and then
[00:23:25.120 --> 00:23:26.800]   try to understand it.
[00:23:26.800 --> 00:23:28.720]   So I see a bunch of answers already.
[00:23:28.720 --> 00:23:31.040]   Let's take a look at it.
[00:23:31.040 --> 00:23:33.920]   It normalizes the input to the layer.
[00:23:33.920 --> 00:23:37.880]   It helps in training speed sometimes works as a regression.
[00:23:37.880 --> 00:23:38.880]   Thanks Anush.
[00:23:38.880 --> 00:23:42.240]   That's that's a great answer.
[00:23:42.240 --> 00:23:45.040]   It is used to stabilize the training.
[00:23:45.040 --> 00:23:46.360]   That is a great answer as well.
[00:23:46.360 --> 00:23:48.560]   I don't know why you all are attending this session.
[00:23:48.560 --> 00:23:51.280]   You already know a lot of this stuff.
[00:23:51.280 --> 00:23:53.480]   The one that we're going through the ones that we're going through.
[00:23:53.480 --> 00:23:57.000]   So it's really incredible to see so many answers.
[00:23:57.000 --> 00:24:01.560]   Yuvraj says to normalize the data so that values are between zero and one.
[00:24:01.560 --> 00:24:08.160]   I would say the not exactly the data, but the data being transmitted between the layers.
[00:24:08.160 --> 00:24:11.160]   It's a technical jargon between the layers.
[00:24:11.160 --> 00:24:13.160]   It's not exactly data that's being transferred.
[00:24:13.160 --> 00:24:19.320]   It's how would you classify the knowledge being transferred between layers?
[00:24:19.320 --> 00:24:21.800]   I'll have to check what's it exactly called.
[00:24:21.800 --> 00:24:28.240]   Again, it might just be data.
[00:24:28.240 --> 00:24:35.680]   Abdul says it's a way of way to move the mean and standard deviation to zero and one.
[00:24:35.680 --> 00:24:37.360]   That is a fair answer as well.
[00:24:37.360 --> 00:24:44.840]   So the reason why I wanted to cover this was show it to you in Keras code.
[00:24:44.840 --> 00:24:45.840]   And let's see it.
[00:24:45.840 --> 00:24:47.520]   It says it layer.
[00:24:47.520 --> 00:24:50.860]   It's a layer that normalizes its inputs.
[00:24:50.860 --> 00:24:57.160]   So batch normalization applies a transformation that maintains the mean output close to zero
[00:24:57.160 --> 00:25:01.200]   and the output standard deviation close to one.
[00:25:01.200 --> 00:25:05.560]   Importantly it works differently during training and during inferencing.
[00:25:05.560 --> 00:25:10.680]   To use it in Keras, you just call a Keras layer.
[00:25:10.680 --> 00:25:14.720]   Now the reason why I wanted to go on this investigative round was first of all, I wanted
[00:25:14.720 --> 00:25:20.620]   to show you this paper that is quite scary and does a lot of things.
[00:25:20.620 --> 00:25:28.440]   But even inside of the book, when we implement it in a simple NumPy code and inside the book,
[00:25:28.440 --> 00:25:33.720]   I believe Franscho shares how to implement it in pseudocode as well.
[00:25:33.720 --> 00:25:36.780]   It is just a few lines.
[00:25:36.780 --> 00:25:39.280]   Just a few lines.
[00:25:39.280 --> 00:25:41.340]   That is what matters right inside of the code.
[00:25:41.340 --> 00:25:46.640]   And he has this very, I want to say, strong opinion.
[00:25:46.640 --> 00:25:52.800]   So he says in the book, it's like papers are many times written like how salesmen put on
[00:25:52.800 --> 00:25:54.840]   an expensive suit.
[00:25:54.840 --> 00:26:01.980]   They're more suited towards making the research look like it's very noble or it's very important.
[00:26:01.980 --> 00:26:05.980]   So remember as practitioners, we're coming to this study group.
[00:26:05.980 --> 00:26:09.440]   Hopefully all of you are coming to this study group for first of all, reasons that I don't
[00:26:09.440 --> 00:26:13.000]   understand since half of you already know this stuff.
[00:26:13.000 --> 00:26:16.880]   But we're coming to this study group to be able to apply this knowledge, which is what
[00:26:16.880 --> 00:26:22.280]   a practitioner does, focus on what it does rather than the theoretical knowledge.
[00:26:22.280 --> 00:26:25.480]   Many times papers focus on the theoretical bits.
[00:26:25.480 --> 00:26:32.920]   So a machine learning engineer or scientist is a strong job because it requires you to
[00:26:32.920 --> 00:26:38.240]   be able to cut through this knowledge with efficiency.
[00:26:38.240 --> 00:26:42.040]   So please be efficient with that.
[00:26:42.040 --> 00:26:44.600]   Now this collab is not required.
[00:26:44.600 --> 00:26:45.600]   Let me close that.
[00:26:45.600 --> 00:26:52.720]   But this collab or this portion of the chapter talks about modern convnet architectures.
[00:26:52.720 --> 00:26:59.080]   So inside of the paper, I believe he, Franchois uses a VTG architecture to tell you first
[00:26:59.080 --> 00:27:06.200]   of all that hey, you use modules and you would love, you should be using modularity inside
[00:27:06.200 --> 00:27:09.260]   of your code to cover different concepts.
[00:27:09.260 --> 00:27:10.360]   That is really important.
[00:27:10.360 --> 00:27:12.240]   That is one of the key factors.
[00:27:12.240 --> 00:27:17.240]   But there we also cover what are residual connections.
[00:27:17.240 --> 00:27:20.320]   These were introduced in ResNets.
[00:27:20.320 --> 00:27:24.280]   Now again, don't go and read the ResNet paper.
[00:27:24.280 --> 00:27:27.900]   Instead see what do they do, which is what we're going to do in a minute.
[00:27:27.900 --> 00:27:34.160]   And I believe after that we look at what is batch normalization.
[00:27:34.160 --> 00:27:36.400]   What is it exactly and what does it do?
[00:27:36.400 --> 00:27:40.800]   We've already looked at that, but I'm following the book's order.
[00:27:40.800 --> 00:27:47.480]   And then we also get an introduction to depth wise separable convolutions, which were the
[00:27:47.480 --> 00:27:55.440]   founding bricks of something known as exception model.
[00:27:55.440 --> 00:28:01.120]   I've actually read this a while ago, which is why I see it in my history.
[00:28:01.120 --> 00:28:05.640]   Thankfully nothing else showed up when I typed that.
[00:28:05.640 --> 00:28:11.520]   But again, we look at what are depth wise separable convolution.
[00:28:11.520 --> 00:28:20.600]   The key thing that these two is, is they separate out or they cut down the number of parameters
[00:28:20.600 --> 00:28:23.560]   required inside of a convolution model.
[00:28:23.560 --> 00:28:29.400]   So the input gets split across different convolutions and then they get concatenated.
[00:28:29.400 --> 00:28:32.920]   This is a simplified representation of it.
[00:28:32.920 --> 00:28:41.640]   The key thing as mentioned in the book is these should be speeding up your computation,
[00:28:41.640 --> 00:28:46.280]   but I haven't benchmarked, but to the best of my knowledge and to the book, according
[00:28:46.280 --> 00:28:52.160]   to the book, it's a minor speed up because what happens in the background, whenever you're
[00:28:52.160 --> 00:29:03.280]   trying to run any code or any code on your GPU is it or Keras talks to CUDA, which then
[00:29:03.280 --> 00:29:05.840]   talks to your graphic card or hardware.
[00:29:05.840 --> 00:29:12.040]   So CUDA, since this was one of architecture or since this isn't so widely used, although
[00:29:12.040 --> 00:29:18.000]   it's a very well recognized paper, at least in the book, it says that CUDA didn't really
[00:29:18.000 --> 00:29:19.980]   hyper optimize this.
[00:29:19.980 --> 00:29:26.900]   So even though it's lesser parameters and it makes life easy to train models, the speed
[00:29:26.900 --> 00:29:32.040]   up isn't that huge, like it should be for that reason.
[00:29:32.040 --> 00:29:35.200]   Convolutions are much more optimized.
[00:29:35.200 --> 00:29:42.840]   So let's start by going through different things inside of the book in sequence.
[00:29:42.840 --> 00:29:45.220]   So Res-EDual networks.
[00:29:45.220 --> 00:29:47.900]   Let's see if I can find an image.
[00:29:47.900 --> 00:29:51.760]   I didn't want to take images from the book and I've been avoiding that in this session
[00:29:51.760 --> 00:29:56.920]   because the book isn't freely available and I would hate to directly copy material from
[00:29:56.920 --> 00:29:57.920]   the book.
[00:29:57.920 --> 00:30:01.040]   So instead, this is reliable.
[00:30:01.040 --> 00:30:03.360]   So this is from Papers with Code.
[00:30:03.360 --> 00:30:07.340]   Papers with Code also has this nice section now where you can just look up concepts.
[00:30:07.340 --> 00:30:12.280]   So if you go to a paper that has residual connections, you can simply start going through
[00:30:12.280 --> 00:30:13.820]   all papers that have that.
[00:30:13.820 --> 00:30:18.080]   So please go to Papers with Code to find such stuff.
[00:30:18.080 --> 00:30:25.400]   But one of the problem inside of convolution networks is, first of all, the gradient tends
[00:30:25.400 --> 00:30:26.720]   to become zero.
[00:30:26.720 --> 00:30:40.880]   So if you keep multiplying anything by something that's smaller than one, and you can derive
[00:30:40.880 --> 00:30:46.320]   this in math, but it'll start to become smaller and smaller in size.
[00:30:46.320 --> 00:30:53.300]   So in the paper ResNet or in the paper that introduced ResNet, they introduced the concept
[00:30:53.300 --> 00:31:00.280]   of residual connection or a skip connection, which does this where the author said, hey,
[00:31:00.280 --> 00:31:08.080]   why don't we take whatever was going into this layer and add it to the layers after
[00:31:08.080 --> 00:31:09.880]   it.
[00:31:09.880 --> 00:31:18.120]   So we do this short circuiting of formats where the input here becomes f of x, where
[00:31:18.120 --> 00:31:23.200]   f is literally think of these as the Python functions that are being called for these
[00:31:23.200 --> 00:31:27.280]   layers plus just x.
[00:31:27.280 --> 00:31:29.240]   And then it gets propagated.
[00:31:29.240 --> 00:31:34.020]   So the Python code for it would be pretty similar as well.
[00:31:34.020 --> 00:31:36.980]   But there might be a few things that you need to consider.
[00:31:36.980 --> 00:31:45.240]   So since this is all matrix math, you would have to also account for size changes, or
[00:31:45.240 --> 00:31:47.260]   tensor size changes.
[00:31:47.260 --> 00:31:55.000]   So if anything funky is going on inside of here, you would have to pad this input.
[00:31:55.000 --> 00:31:59.240]   And padding is just adding either zeros around it to match the size.
[00:31:59.240 --> 00:32:02.080]   Or you could also add different techniques.
[00:32:02.080 --> 00:32:07.040]   There are different ways of padding this, which I won't go into, but you can look these
[00:32:07.040 --> 00:32:14.120]   up if you want.
[00:32:14.120 --> 00:32:20.280]   Actually let's look at CON2D and see what are the padding methods.
[00:32:20.280 --> 00:32:22.760]   I'm hoping it's at least a bit elaborate here.
[00:32:22.760 --> 00:32:26.500]   If it's not, I'll skip.
[00:32:26.500 --> 00:32:35.160]   So we're looking for what is padding.
[00:32:35.160 --> 00:32:39.120]   So padding either take one of valid or same case insensitive.
[00:32:39.120 --> 00:32:40.120]   I didn't know that.
[00:32:40.120 --> 00:32:47.120]   Valid means no padding and same results in padding with zeros left or right.
[00:32:47.120 --> 00:32:52.480]   So when you're watching any video on YouTube, and it has those black bars that is padded
[00:32:52.480 --> 00:32:55.320]   by zero, that's one way to remember it.
[00:32:55.320 --> 00:33:01.780]   Now let's hop over to the code and see how this block are implemented where filters change.
[00:33:01.780 --> 00:33:06.340]   So our input is simply a Keras input with the following shape.
[00:33:06.340 --> 00:33:11.520]   And the first layer would become a CON2D layer.
[00:33:11.520 --> 00:33:17.960]   And we take the residual connection to be just this for now.
[00:33:17.960 --> 00:33:22.600]   Then we add another layer that takes in X.
[00:33:22.600 --> 00:33:29.400]   And the residual layer now becomes the following.
[00:33:29.400 --> 00:33:33.880]   So we pass in the older residual layer, which was just X.
[00:33:33.880 --> 00:33:35.400]   This is some smart code.
[00:33:35.400 --> 00:33:39.400]   So it took me a bit of writing on paper to understand.
[00:33:39.400 --> 00:33:44.480]   But you can simply you could just write this as X for now that would do it.
[00:33:44.480 --> 00:33:47.440]   But we're passing in the older value.
[00:33:47.440 --> 00:33:51.500]   And the newer value now becomes a CON2D of the same.
[00:33:51.500 --> 00:34:00.720]   So let's find that image where was it this would be a CON2D.
[00:34:00.720 --> 00:34:05.080]   So after both of these come out, these would probably go into a layer, which would perform
[00:34:05.080 --> 00:34:06.120]   a convolution.
[00:34:06.120 --> 00:34:09.000]   So that's what is happening here.
[00:34:09.000 --> 00:34:14.720]   And finally, we just add these layers like so.
[00:34:14.720 --> 00:34:18.960]   They also cover in the book where if a layer would have max pooling.
[00:34:18.960 --> 00:34:23.100]   So in that case, we would have to change the padding a bit.
[00:34:23.100 --> 00:34:24.100]   It's quite straightforward.
[00:34:24.100 --> 00:34:28.760]   So I'll jump further.
[00:34:28.760 --> 00:34:32.500]   You could also define the residual block like so.
[00:34:32.500 --> 00:34:38.180]   So first of all, we capture the input in residual variable.
[00:34:38.180 --> 00:34:44.100]   From there, X becomes the first input.
[00:34:44.100 --> 00:34:47.700]   And then you pass in another it through another CON2D.
[00:34:47.700 --> 00:34:53.500]   If there is pooling and we if you would have looked at this block, you would understand
[00:34:53.500 --> 00:34:56.820]   how it changes a bit if you're using max pooling.
[00:34:56.820 --> 00:35:03.540]   We add a max pooling layer and we apply the condition for residual where we'll add a stride
[00:35:03.540 --> 00:35:08.340]   and add filters like so.
[00:35:08.340 --> 00:35:11.060]   Use a CON1 by 1.
[00:35:11.060 --> 00:35:15.580]   Otherwise, we follow the older approach.
[00:35:15.580 --> 00:35:19.760]   And then we can create as many residual blocks like we want.
[00:35:19.760 --> 00:35:24.300]   And this becomes our model architecture.
[00:35:24.300 --> 00:35:28.540]   I'll take a pause to see if there are any questions.
[00:35:28.540 --> 00:35:33.420]   The text in the book was really thorough around this one, quickly glancing over different
[00:35:33.420 --> 00:35:34.420]   topics.
[00:35:34.420 --> 00:35:37.340]   Okay, I don't see any questions.
[00:35:37.340 --> 00:35:39.380]   I'll continue.
[00:35:39.380 --> 00:35:40.820]   I see one question by Naresh.
[00:35:40.820 --> 00:35:42.500]   Sorry, I almost missed that.
[00:35:42.500 --> 00:35:45.180]   In what situations batch normalization can be used?
[00:35:45.180 --> 00:35:48.500]   We'll just take a look at that in one second.
[00:35:48.500 --> 00:35:49.500]   Thanks for the great question.
[00:35:49.500 --> 00:35:50.500]   There we go.
[00:35:50.500 --> 00:35:54.540]   Naresh, you asked it a little too early.
[00:35:54.540 --> 00:35:57.180]   It was a great question nevertheless.
[00:35:57.180 --> 00:36:05.140]   We looked at batch normalization inside of the book and I believe there was some pseudocode
[00:36:05.140 --> 00:36:07.180]   around it.
[00:36:07.180 --> 00:36:11.660]   So we put all of these together in a mini exception like model.
[00:36:11.660 --> 00:36:13.860]   It's not the exact replica.
[00:36:13.860 --> 00:36:18.380]   And like I mentioned, I crashed my Google collab earlier by crashing Safari.
[00:36:18.380 --> 00:36:20.340]   So it's asking me to upload everything.
[00:36:20.340 --> 00:36:21.340]   I won't do that.
[00:36:21.340 --> 00:36:24.060]   I'll quickly summarize different things too.
[00:36:24.060 --> 00:36:29.820]   So first of all, we set the data augmentations to random flip, random rotation and random
[00:36:29.820 --> 00:36:30.820]   zoom.
[00:36:30.820 --> 00:36:33.460]   We took a thorough look at this, but we'll flip horizontally.
[00:36:33.460 --> 00:36:40.420]   We'll do a random rotation of up to 10, I believe degrees.
[00:36:40.420 --> 00:36:42.580]   Let's take a look at what is 0.1 here.
[00:36:42.580 --> 00:36:48.940]   I want to confirm if this is 10% or 10 degrees.
[00:36:48.940 --> 00:36:55.780]   Let's see what does random rotation layer say.
[00:36:55.780 --> 00:37:01.820]   A pre-processing layer which randomly rotates images during training.
[00:37:01.820 --> 00:37:06.020]   If we passed in just one value, that would be factor.
[00:37:06.020 --> 00:37:08.380]   And what does factor determine?
[00:37:08.380 --> 00:37:16.420]   What represented as a fraction of 2 pi of tuple size 2 representing lower and upper
[00:37:16.420 --> 00:37:17.420]   bound.
[00:37:17.420 --> 00:37:18.420]   Awesome.
[00:37:18.420 --> 00:37:21.540]   So it's not a percentage.
[00:37:21.540 --> 00:37:25.300]   It is a fraction of 2 pi.
[00:37:25.300 --> 00:37:30.300]   So that would mean it's being rotated by 0.2 pi.
[00:37:30.300 --> 00:37:34.020]   360 degrees is 2 pi.
[00:37:34.020 --> 00:37:39.140]   So 0.2 pi would be up to 72 degrees.
[00:37:39.140 --> 00:37:40.860]   If I'm getting that right.
[00:37:40.860 --> 00:37:42.940]   If I'm wrong, please feel free to correct me.
[00:37:42.940 --> 00:37:47.700]   But I think we'll randomly rotate up to 72 degrees.
[00:37:47.700 --> 00:37:50.260]   And then we have random zoom of 0.2.
[00:37:50.260 --> 00:37:54.540]   So let's also check what does this point to denote here.
[00:37:54.540 --> 00:38:02.940]   Let's look at random zoom layer.
[00:38:02.940 --> 00:38:06.460]   So we would be passing in the height factor.
[00:38:06.460 --> 00:38:11.740]   A float represented as a fraction of value or a tuple of size 2.
[00:38:11.740 --> 00:38:20.340]   When represented as a single float, this value is used to for both upper and lower bound.
[00:38:20.340 --> 00:38:25.740]   So I believe this means that we'll be zooming in for plus or minus x percent where x was
[00:38:25.740 --> 00:38:27.220]   the input.
[00:38:27.220 --> 00:38:32.120]   So we'll be zooming in or out by 20%.
[00:38:32.120 --> 00:38:34.780]   How do we decide what augmentations to apply?
[00:38:34.780 --> 00:38:35.780]   Great question Harpreet.
[00:38:35.780 --> 00:38:38.940]   We looked at this in the previous session.
[00:38:38.940 --> 00:38:42.940]   And the answer is depends on the input.
[00:38:42.940 --> 00:38:46.580]   So this one is cats versus dogs.
[00:38:46.580 --> 00:38:48.740]   Would you want to flip the image vertically?
[00:38:48.740 --> 00:38:50.260]   No, right?
[00:38:50.260 --> 00:38:52.700]   No one takes an image of an inverted dog.
[00:38:52.700 --> 00:38:56.220]   Would you want to flip it horizontally?
[00:38:56.220 --> 00:39:00.340]   I was considering rotating on this live stream.
[00:39:00.340 --> 00:39:01.660]   I won't do that.
[00:39:01.660 --> 00:39:04.840]   But you could possibly flip it horizontally, right?
[00:39:04.840 --> 00:39:05.840]   That makes sense.
[00:39:05.840 --> 00:39:08.260]   You could rotate it by a few degrees.
[00:39:08.260 --> 00:39:10.120]   That is fine.
[00:39:10.120 --> 00:39:13.460]   So for cats and dogs, you can imagine that all of these things apply.
[00:39:13.460 --> 00:39:18.980]   You could zoom in and crop into a particular area of the image, or you could pad it with
[00:39:18.980 --> 00:39:21.240]   options.
[00:39:21.240 --> 00:39:23.360]   So again, it comes to experience.
[00:39:23.360 --> 00:39:29.660]   And for weird images, weird being images of stars, galaxies, stuff like that, you'll have
[00:39:29.660 --> 00:39:31.780]   to experiment and see.
[00:39:31.780 --> 00:39:36.460]   There might be literature around it, but a lot of this comes through practice.
[00:39:36.460 --> 00:39:45.140]   Frans actually made this point for a different topic in the book, where he said that a lot
[00:39:45.140 --> 00:39:50.140]   of the things in deep learning are just being done because they have been done like so.
[00:39:50.140 --> 00:39:57.580]   So please consider experimenting and if nothing comes out of it, at least you learn something
[00:39:57.580 --> 00:40:04.140]   that, okay, this particular experiment didn't work for something.
[00:40:04.140 --> 00:40:08.060]   How do we decide point one versus point two?
[00:40:08.060 --> 00:40:09.060]   Great question.
[00:40:09.060 --> 00:40:11.320]   Again, experiments.
[00:40:11.320 --> 00:40:17.500]   You probably wouldn't want to zoom in by 90%, right?
[00:40:17.500 --> 00:40:20.820]   So let's let's take my image right now.
[00:40:20.820 --> 00:40:26.260]   For example, I'm in the center of the frame for the most part, you will be doing some
[00:40:26.260 --> 00:40:29.060]   EDA on this data set.
[00:40:29.060 --> 00:40:33.980]   For some reason, you want to waste your time and find me an image that the that's the task
[00:40:33.980 --> 00:40:36.280]   that you've picked up.
[00:40:36.280 --> 00:40:42.100]   If you crop into an image, you would be zooming into the middle frame of it.
[00:40:42.100 --> 00:40:46.700]   So if I were to do it right now, this is what cropping in does.
[00:40:46.700 --> 00:40:48.820]   This would be 50%.
[00:40:48.820 --> 00:40:51.820]   This would be 300%.
[00:40:51.820 --> 00:40:54.660]   Right?
[00:40:54.660 --> 00:40:58.660]   Sorry.
[00:40:58.660 --> 00:41:04.740]   As you start zooming in, you would have to tune the hyper parameter and decide for yourself
[00:41:04.740 --> 00:41:08.020]   depending on what images are in the data set.
[00:41:08.020 --> 00:41:11.340]   Aritra answered the question perfectly.
[00:41:11.340 --> 00:41:17.700]   So it's a hyper parameter which needs to be tuned and you could experiment or also strategically
[00:41:17.700 --> 00:41:21.860]   like I said by looking at different images decide how to approach that.
[00:41:21.860 --> 00:41:23.860]   Thanks Aritra for the great answer.
[00:41:23.860 --> 00:41:34.220]   Let's see what do I want to highlight in this part, we make a subset.
[00:41:34.220 --> 00:41:40.620]   And this is another technique I want to point out when you're working with a bigger data
[00:41:40.620 --> 00:41:41.620]   set.
[00:41:41.620 --> 00:41:44.980]   Jeremy Howard actually taught us this in fast.
[00:41:44.980 --> 00:41:46.700]   And he's one of my favorite teachers.
[00:41:46.700 --> 00:41:50.740]   He has really changed many lives and my life especially.
[00:41:50.740 --> 00:41:54.940]   But he taught me this technique, which is also covered here.
[00:41:54.940 --> 00:42:01.860]   Whenever you're working at actively with or interactively with a data set, make a subset
[00:42:01.860 --> 00:42:08.820]   of it that allows you to iterate in 30 ish seconds or lesser.
[00:42:08.820 --> 00:42:09.940]   So that's what we're doing here.
[00:42:09.940 --> 00:42:15.900]   We're making a subset.
[00:42:15.900 --> 00:42:21.180]   And we're pulling out the train data set, the validation data set and test data set
[00:42:21.180 --> 00:42:22.180]   here.
[00:42:22.180 --> 00:42:24.580]   We set the data augmentations like so.
[00:42:24.580 --> 00:42:28.900]   We've already looked at what all of these do.
[00:42:28.900 --> 00:42:32.380]   We will be rescaling the images so to normalize them.
[00:42:32.380 --> 00:42:36.380]   And from there we can pass them inside of a contour layer.
[00:42:36.380 --> 00:42:38.740]   Now we'll be adding a residual network.
[00:42:38.740 --> 00:42:40.980]   Sorry, a residual layer.
[00:42:40.980 --> 00:42:42.340]   It's not a residual network.
[00:42:42.340 --> 00:42:44.300]   A residual network is a ResNet.
[00:42:44.300 --> 00:42:45.300]   My apologies.
[00:42:45.300 --> 00:42:47.260]   We are adding a residual layer.
[00:42:47.260 --> 00:42:50.740]   So we are doing this short circuiting.
[00:42:50.740 --> 00:42:54.700]   We will add an x to the output.
[00:42:54.700 --> 00:42:57.180]   So that's the technique that we're following here.
[00:42:57.180 --> 00:43:01.700]   And we call layers.add to add the residual layer.
[00:43:01.700 --> 00:43:06.420]   The other thing that we do now is add batch normalization.
[00:43:06.420 --> 00:43:14.700]   So batch normalization is a layer that normalizes its inputs and applies a transformation that
[00:43:14.700 --> 00:43:18.620]   maintains the output close to 0 and 1.
[00:43:18.620 --> 00:43:20.860]   That's what it does.
[00:43:20.860 --> 00:43:22.060]   That's all you need to know.
[00:43:22.060 --> 00:43:29.780]   And applying this changes things inside of the inputs and makes our model converge a
[00:43:29.780 --> 00:43:32.300]   little bit faster.
[00:43:32.300 --> 00:43:40.260]   Another thing to note that was pointed out inside of the book is when you are training
[00:43:40.260 --> 00:43:43.520]   using batch norm, you need to set it to trainable.
[00:43:43.520 --> 00:43:47.460]   And when you're predicting, you need to freeze the layer, which means you don't want it to
[00:43:47.460 --> 00:43:53.940]   do funky things where it's changing the or it's trying to normalize because it's already
[00:43:53.940 --> 00:43:55.740]   learned and now you're trying to predict.
[00:43:55.740 --> 00:44:00.300]   So inside of the layer, it tries to understand how can it normalize what's going into it.
[00:44:00.300 --> 00:44:03.580]   When you're predicting, you don't want the layer to learn.
[00:44:03.580 --> 00:44:04.580]   Always freeze the layer.
[00:44:04.580 --> 00:44:09.180]   Otherwise, you could run into interesting issues where you perfectly designed a model,
[00:44:09.180 --> 00:44:11.980]   but it doesn't work.
[00:44:11.980 --> 00:44:15.320]   So please be careful around that.
[00:44:15.320 --> 00:44:16.580]   And there you go.
[00:44:16.580 --> 00:44:23.360]   That's the final exception mini custom model.
[00:44:23.360 --> 00:44:44.440]   Any questions from this part before I close this tab?
[00:44:44.440 --> 00:44:45.440]   Awesome I don't see any questions.
[00:44:45.440 --> 00:44:46.440]   I will continue.
[00:44:46.440 --> 00:44:51.020]   Ask them still and I'll still take them up.
[00:44:51.020 --> 00:44:55.740]   So if a layer has I saw questions I'll try to answer that.
[00:44:55.740 --> 00:45:00.700]   So if a layer has 10 inputs, it will normalize all of them.
[00:45:00.700 --> 00:45:04.300]   It's usually a tensor right?
[00:45:04.300 --> 00:45:09.980]   So when you say 10, it could be a tensor of I don't know some certain dimension.
[00:45:09.980 --> 00:45:17.780]   And usually it's like hundreds of inputs.
[00:45:17.780 --> 00:45:22.380]   I will continue still and ask questions come in I'll answer them.
[00:45:22.380 --> 00:45:23.380]   Awesome.
[00:45:23.380 --> 00:45:26.540]   I'll close this tab.
[00:45:26.540 --> 00:45:31.020]   And now we're at the part where we want to see or interpret what coordinates learn.
[00:45:31.020 --> 00:45:33.220]   So this is known as grad cam.
[00:45:33.220 --> 00:45:34.780]   I have crashed this collab.
[00:45:34.780 --> 00:45:40.960]   So let's see if I can find something else that I can use.
[00:45:40.960 --> 00:45:47.960]   This is a blog post by Ayush Thakur on how you can actually use weights and biases and
[00:45:47.960 --> 00:45:49.200]   cam and grad cam.
[00:45:49.200 --> 00:45:53.280]   So let me post the link to this inside of the chart.
[00:45:53.280 --> 00:45:57.160]   Everyone can follow along.
[00:45:57.160 --> 00:46:00.620]   And let's continue here.
[00:46:00.620 --> 00:46:04.440]   So this actually tells you about what exactly it does.
[00:46:04.440 --> 00:46:11.000]   I'll skip that because I think the visualization is good enough.
[00:46:11.000 --> 00:46:17.440]   So when you're training models, it's important to understand what is your model learning.
[00:46:17.440 --> 00:46:19.720]   And there are different ways to doing that.
[00:46:19.720 --> 00:46:23.400]   So you could look at inside of the layers and what's happening there.
[00:46:23.400 --> 00:46:30.480]   Inside of this particular section of the chapter, we visualize the intermediate activations.
[00:46:30.480 --> 00:46:35.640]   So we see the activations between layers and try to understand what's going on there.
[00:46:35.640 --> 00:46:39.420]   We've looked at this paper earlier in the series where we would understand how does
[00:46:39.420 --> 00:46:41.680]   that happen.
[00:46:41.680 --> 00:46:43.000]   That is literally what we're doing.
[00:46:43.000 --> 00:46:45.960]   That is one of the ways of understanding what's going on.
[00:46:45.960 --> 00:46:52.680]   But the more practical way is trying to create a heat map, which tells you on images what
[00:46:52.680 --> 00:46:55.640]   parts of the image triggered.
[00:46:55.640 --> 00:46:59.480]   I want to say the machine learning model because it also applies to transformer model.
[00:46:59.480 --> 00:47:05.960]   So if you're using a transformer model or a CNN, what part of the image was helpful
[00:47:05.960 --> 00:47:17.120]   in telling the model that, hey, this was a cat or a dog.
[00:47:17.120 --> 00:47:21.880]   So this is what class activation maps look like.
[00:47:21.880 --> 00:47:24.600]   And this is how you visualize them.
[00:47:24.600 --> 00:47:30.080]   This blog is a very thorough walkthrough of the steps required to do that.
[00:47:30.080 --> 00:47:33.340]   But I just want to focus on the grad cam output.
[00:47:33.340 --> 00:47:36.360]   So that's what I'm scrolling down towards.
[00:47:36.360 --> 00:47:42.320]   And this blog would also point you to the original paper that mentioned this.
[00:47:42.320 --> 00:47:48.400]   So what does this lead to is an output like so, where since this is a weights and biases
[00:47:48.400 --> 00:47:52.720]   report, I can also look at how did the model training change with time.
[00:47:52.720 --> 00:47:57.200]   So weights and biases allows you to, this is why I am a fan of reports, especially of
[00:47:57.200 --> 00:47:59.960]   I use he writes very neat reports.
[00:47:59.960 --> 00:48:02.360]   And I would encourage you to read more of his work.
[00:48:02.360 --> 00:48:07.640]   But we can look at different steps inside of the model and see how the model has improved.
[00:48:07.640 --> 00:48:12.200]   So again, it's this visual way, like I said, of understanding what was happening inside
[00:48:12.200 --> 00:48:13.920]   of the model.
[00:48:13.920 --> 00:48:19.280]   So now we're predicting, hopefully, I've not looked at the outputs, but hopefully we're
[00:48:19.280 --> 00:48:21.760]   telling that this image is a dog.
[00:48:21.760 --> 00:48:23.000]   And this is a cat.
[00:48:23.000 --> 00:48:29.240]   And now we can understand which parts of the image triggered the machine learning model.
[00:48:29.240 --> 00:48:30.520]   Why is this important?
[00:48:30.520 --> 00:48:37.640]   Let's say I had the privilege to interview Chris Diott yesterday, and he gave this example.
[00:48:37.640 --> 00:48:39.880]   So I'm using his example.
[00:48:39.880 --> 00:48:46.200]   If you had an image where dogs are just sitting on cars, and cats are sitting on boats, your
[00:48:46.200 --> 00:48:50.920]   model could be potentially learning how to differentiate cars and boats and not cats
[00:48:50.920 --> 00:48:52.500]   and dogs.
[00:48:52.500 --> 00:48:55.800]   So understanding the output is really helpful.
[00:48:55.800 --> 00:49:05.780]   More practically, if I go to Kaggle, Kaggle is one of the best way of testing your knowledge
[00:49:05.780 --> 00:49:07.760]   and embarrassing yourself.
[00:49:07.760 --> 00:49:09.960]   I have done the latter mostly.
[00:49:09.960 --> 00:49:17.840]   But Chris Diott won a solo gold medal, which is really hard to do on Kaggle.
[00:49:17.840 --> 00:49:20.280]   That's a side tangent, I won't go into that.
[00:49:20.280 --> 00:49:22.760]   But he was training transformer models.
[00:49:22.760 --> 00:49:25.480]   Sorry, I wasn't sharing my screen.
[00:49:25.480 --> 00:49:27.800]   So let me do that.
[00:49:27.800 --> 00:49:32.120]   I wanted to point out PetFinder competition and this leaderboard.
[00:49:32.120 --> 00:49:34.280]   And Chris Diott absolute legend.
[00:49:34.280 --> 00:49:41.120]   He's one of the most respected Kagglers, finished sixth on the leaderboard.
[00:49:41.120 --> 00:49:46.340]   And he was trying to train a few transformer models, as he mentioned in yesterday's interview.
[00:49:46.340 --> 00:49:51.720]   And to understand why on these being trained effectively, he actually visualized the grad
[00:49:51.720 --> 00:49:56.600]   cam, which led him to understanding few things.
[00:49:56.600 --> 00:50:04.600]   As I find a solution, and share my screen this time, hopefully.
[00:50:04.600 --> 00:50:09.560]   It was a trick to making transformer models work.
[00:50:09.560 --> 00:50:13.460]   I understand we haven't talked about them yet in this course.
[00:50:13.460 --> 00:50:18.320]   So if you are new to transformer models, please don't worry, it's a set of models that really
[00:50:18.320 --> 00:50:20.360]   sexy in research.
[00:50:20.360 --> 00:50:22.160]   And they work really well.
[00:50:22.160 --> 00:50:24.440]   That's all you need to know that they work really well.
[00:50:24.440 --> 00:50:27.640]   It's a hot topic in research, because they are quite effective.
[00:50:27.640 --> 00:50:34.960]   To make them work, many people were resizing their images like so, which wasn't helpful.
[00:50:34.960 --> 00:50:44.160]   So if you correctly crop like so, the transformer models were working more effectively.
[00:50:44.160 --> 00:50:49.380]   As I understood, Chris got this knowledge by visualizing the grad cams.
[00:50:49.380 --> 00:50:52.960]   So this is why visualizing all of these layers is helpful.
[00:50:52.960 --> 00:50:57.680]   I would again really encourage you to check out this incredible blog post by my colleague
[00:50:57.680 --> 00:50:58.680]   Ayush.
[00:50:58.680 --> 00:51:01.240]   He writes really well blog posts and is a Kaggle master.
[00:51:01.240 --> 00:51:04.080]   I think he'll soon be a Kaggle grandmaster.
[00:51:04.080 --> 00:51:05.320]   It's just a matter of time.
[00:51:05.320 --> 00:51:07.320]   But please do check out his work.
[00:51:07.320 --> 00:51:12.540]   I would really encourage you to spend some time reading these posts.
[00:51:12.540 --> 00:51:15.560]   So let's come back to this notebook.
[00:51:15.560 --> 00:51:19.960]   I believe this was the gist of the notebook more in an overview fashion.
[00:51:19.960 --> 00:51:27.440]   So I'll close this notebook as well and come back and see if there are any questions.
[00:51:27.440 --> 00:51:32.680]   Harpreet has shared an example where a model was classifying tree branches as birds.
[00:51:32.680 --> 00:51:38.240]   Yeah, that's a problem that happens with a lot of the ML models.
[00:51:38.240 --> 00:51:40.000]   That's what we're trying to look out for.
[00:51:40.000 --> 00:51:41.840]   Thanks for sharing that, Harpreet.
[00:51:41.840 --> 00:51:43.720]   There was a question earlier that I missed.
[00:51:43.720 --> 00:51:46.720]   How is batch norm different from layer norm?
[00:51:46.720 --> 00:51:49.120]   I believe we'll cover this when we get to NLP.
[00:51:49.120 --> 00:51:51.960]   So I'll answer the question then.
[00:51:51.960 --> 00:51:53.920]   Thanks for the great question, though.
[00:51:53.920 --> 00:52:05.520]   If you're curious to check this out, I will point you to this example blog post.
[00:52:05.520 --> 00:52:08.560]   I was actually just reading this a few days ago.
[00:52:08.560 --> 00:52:10.280]   This is by, again, one of my colleagues.
[00:52:10.280 --> 00:52:13.400]   He is as equally as talented as Ayush.
[00:52:13.400 --> 00:52:20.600]   But let me put this in the chart and suggest you to read this as a homework.
[00:52:20.600 --> 00:52:21.600]   Thanks for the great question.
[00:52:21.600 --> 00:52:24.640]   We'll definitely revisit this.
[00:52:24.640 --> 00:52:30.860]   So now we're at the point where we're ready to learn deep learning for time series.
[00:52:30.860 --> 00:52:39.200]   But before that, I want to come back to my original promise of covering conf next.
[00:52:39.200 --> 00:52:44.720]   So let me first of all open this paper and if GitHub render works for a 15 meg PDF, I
[00:52:44.720 --> 00:52:45.720]   would be surprised.
[00:52:45.720 --> 00:52:46.720]   Oh, wow.
[00:52:46.720 --> 00:52:50.000]   I'm genuinely surprised it did work.
[00:52:50.000 --> 00:52:59.800]   So let me post this link in the chat again.
[00:52:59.800 --> 00:53:03.980]   And highlight it like so.
[00:53:03.980 --> 00:53:10.560]   So I've annotated the paper that discusses conf next architecture and I'll very quickly
[00:53:10.560 --> 00:53:16.680]   give us gist of the paper.
[00:53:16.680 --> 00:53:21.440]   And then we'll see we're already at the point where we can implement cutting edge research.
[00:53:21.440 --> 00:53:27.600]   This paper came out on 10th of January, barely 15 days ago, just a little over 15 days ago.
[00:53:27.600 --> 00:53:30.640]   And this paper asks the main question.
[00:53:30.640 --> 00:53:32.120]   Pardon my handwriting.
[00:53:32.120 --> 00:53:33.860]   I know it's awful.
[00:53:33.860 --> 00:53:35.920]   You don't need to write a chat comment for that.
[00:53:35.920 --> 00:53:37.320]   I know that already.
[00:53:37.320 --> 00:53:40.880]   But the main question addressed is how do we design?
[00:53:40.880 --> 00:53:43.360]   How do the design decisions in transformers?
[00:53:43.360 --> 00:53:47.640]   I mentioned transformers are really up and coming area of research.
[00:53:47.640 --> 00:53:52.640]   Can we possibly take the tricks inside of transformer and apply them to convolutional
[00:53:52.640 --> 00:53:55.140]   neural networks?
[00:53:55.140 --> 00:54:03.360]   So they modernize a resonant 50, which means they take different techniques from a transformer.
[00:54:03.360 --> 00:54:06.180]   And add changes to a CNN.
[00:54:06.180 --> 00:54:10.880]   Now I won't cover the entire paper that is planned for a paper reading group.
[00:54:10.880 --> 00:54:14.400]   And you can sign up for that if you're interested.
[00:54:14.400 --> 00:54:22.920]   I'll scroll down and show you the main gist of a conf next block.
[00:54:22.920 --> 00:54:27.840]   So these are these would be repeated, I believe 96 times.
[00:54:27.840 --> 00:54:32.120]   And inside of a conf next block, here is a resonant block next to it.
[00:54:32.120 --> 00:54:36.080]   Here is a Swind transformer block.
[00:54:36.080 --> 00:54:45.280]   Inside of it, you have convolutions of 7 by 7 being applied, followed by a layer norm.
[00:54:45.280 --> 00:54:49.280]   So Harpreet, pardon me, I know we'll cover layer norm later.
[00:54:49.280 --> 00:54:54.680]   But for now, for my argument, believe that it's a way of normalizing.
[00:54:54.680 --> 00:54:55.760]   It works.
[00:54:55.760 --> 00:54:58.800]   So we pass it through that.
[00:54:58.800 --> 00:55:05.300]   And then we apply JLU, which is an activation function.
[00:55:05.300 --> 00:55:07.120]   This is the gist of a convex block.
[00:55:07.120 --> 00:55:09.520]   Now let's see what does it look like in code.
[00:55:09.520 --> 00:55:18.880]   This was a collab by my colleague Shomik.
[00:55:18.880 --> 00:55:21.120]   Shomik is really talented.
[00:55:21.120 --> 00:55:24.800]   He's been contributing to Keras.io examples.
[00:55:24.800 --> 00:55:26.280]   And please check out his book.
[00:55:26.280 --> 00:55:28.280]   He's a Keras ninja.
[00:55:28.280 --> 00:55:30.760]   I've come from a Pytorch world.
[00:55:30.760 --> 00:55:32.560]   So I'm good at that.
[00:55:32.560 --> 00:55:34.560]   And I'm terrified of Keras.
[00:55:34.560 --> 00:55:38.440]   So we're stealing his book for the sake of research.
[00:55:38.440 --> 00:55:40.360]   We're stealing his book.
[00:55:40.360 --> 00:55:45.560]   And inside of this, he's defined different blocks, which are conf block, depthwise conf
[00:55:45.560 --> 00:55:49.380]   block, and con next block.
[00:55:49.380 --> 00:55:53.140]   So what does a con next block use?
[00:55:53.140 --> 00:55:57.240]   It has a depthwise conf block of, remember, kernel size 7 by 7.
[00:55:57.240 --> 00:55:59.920]   So let me bring this back closer.
[00:55:59.920 --> 00:56:05.000]   So 7 by 7, conf block.
[00:56:05.000 --> 00:56:13.080]   From there, ln, layer norm, followed by JLU, and there's a skip connection happening as
[00:56:13.080 --> 00:56:14.960]   well there.
[00:56:14.960 --> 00:56:24.120]   So we call a layer norm layer and pass it further to a dense layer called JLU, followed
[00:56:24.120 --> 00:56:26.980]   by a dense layer.
[00:56:26.980 --> 00:56:29.280]   Then we apply a dropout.
[00:56:29.280 --> 00:56:33.440]   Dropout, easiest way to remember is making your model drunk.
[00:56:33.440 --> 00:56:37.120]   So it forgets different things at a random probability.
[00:56:37.120 --> 00:56:38.840]   There's some implementation detail here.
[00:56:38.840 --> 00:56:42.400]   But I think to understand is we call dropout.
[00:56:42.400 --> 00:56:48.400]   And then we do the residual network trick, which is where we add these layers.
[00:56:48.400 --> 00:56:52.280]   So of course, there's some trick around defining these blocks.
[00:56:52.280 --> 00:56:57.280]   So con block needs to be defined in a particular fashion.
[00:56:57.280 --> 00:57:03.680]   And depthwise conf block also is a separate function.
[00:57:03.680 --> 00:57:09.480]   Very simply put, I wanted to point out that based on all of this knowledge that we've
[00:57:09.480 --> 00:57:17.880]   acquired in just two weeks, we are able to understand an implementation of one of the
[00:57:17.880 --> 00:57:21.040]   hottest papers in computer vision.
[00:57:21.040 --> 00:57:25.760]   So this paper came out 15 days ago, and it was really being shared a lot on Twitter.
[00:57:25.760 --> 00:57:30.680]   I would again, encourage everyone to sign up on Twitter and follow people like Jan
[00:57:30.680 --> 00:57:36.560]   LeCun, authors of this paper, who've done incredible work, really interesting work,
[00:57:36.560 --> 00:57:37.720]   I would say.
[00:57:37.720 --> 00:57:40.600]   This paper was really shared a lot.
[00:57:40.600 --> 00:57:47.360]   And just using knowledge acquired in two weeks, we can understand this to some level, right?
[00:57:47.360 --> 00:57:54.960]   So, if you go to the repository that I was mentioning earlier, you can find how to use
[00:57:54.960 --> 00:57:56.960]   this in PyTorch.
[00:57:56.960 --> 00:58:01.800]   And we'll quickly, we'll shortly add the Keras implementation as well.
[00:58:01.800 --> 00:58:04.880]   So you can find the code walkthrough as well.
[00:58:04.880 --> 00:58:09.840]   But again, wanted to showcase that we're already at the point where we have this practical
[00:58:09.840 --> 00:58:12.840]   knowledge.
[00:58:12.840 --> 00:58:25.440]   I see a question by Harpreet, what is depth wise conf block?
[00:58:25.440 --> 00:58:27.200]   This is more for the paper reading group.
[00:58:27.200 --> 00:58:29.440]   So I believe I'll cover it then.
[00:58:29.440 --> 00:58:31.360]   Sorry for not answering it right now.
[00:58:31.360 --> 00:58:33.560]   I can answer it later.
[00:58:33.560 --> 00:58:34.560]   After the live stream.
[00:58:34.560 --> 00:58:37.400]   I'll just be respectful of everyone's time.
[00:58:37.400 --> 00:58:38.400]   So awesome.
[00:58:38.400 --> 00:58:50.320]   So we've wrapped up chapter nine officially, and we're ready to jump into time series.
[00:58:50.320 --> 00:59:01.120]   This is meant as more of a introduction to RNNs and also a broad level overview of time
[00:59:01.120 --> 00:59:03.140]   series forecasting.
[00:59:03.140 --> 00:59:09.580]   So the problem inside of this chapter is a temperature forecasting example.
[00:59:09.580 --> 00:59:15.340]   I want to ask everyone, how would you set up validation and testing for this?
[00:59:15.340 --> 00:59:20.940]   So let's say you're trying to predict tomorrow's temperature.
[00:59:20.940 --> 00:59:23.600]   Your goal is to predict tomorrow's temperature.
[00:59:23.600 --> 00:59:26.340]   How would you set up the validation and test data set?
[00:59:26.340 --> 00:59:33.740]   This is one of the most important parts of working with time series data sets.
[00:59:33.740 --> 00:59:38.420]   So how do you split the validation and test set?
[00:59:38.420 --> 00:59:42.380]   I'll wait for a few seconds, because there's a delay between me saying this, you're hearing
[00:59:42.380 --> 00:59:51.380]   it and typing in the answer.
[00:59:51.380 --> 01:00:14.340]   Harpreet, our most active awesome member says future time periods are not to be used in
[01:00:14.340 --> 01:00:15.340]   training.
[01:00:15.340 --> 01:00:18.380]   How would you define future?
[01:00:18.380 --> 01:00:20.380]   That's the question.
[01:00:20.380 --> 01:00:26.940]   Yuvraj, one of our oldest members says, take the latest set of data as validation won't
[01:00:26.940 --> 01:00:29.860]   shuffle the data during training.
[01:00:29.860 --> 01:00:30.860]   That's a perfect answer.
[01:00:30.860 --> 01:00:31.860]   Let's see.
[01:00:31.860 --> 01:00:32.860]   What are the other questions?
[01:00:32.860 --> 01:00:33.860]   Abdul, I see a question.
[01:00:33.860 --> 01:00:40.860]   I'll answer it afterwards not to break the flow.
[01:00:40.860 --> 01:00:43.180]   Ali has a similar answer.
[01:00:43.180 --> 01:00:48.740]   Sequentially first 11 months as train and rest as test.
[01:00:48.740 --> 01:00:50.660]   Some answers, everyone.
[01:00:50.660 --> 01:01:00.180]   Let's see if I can bring up just calendar.
[01:01:00.180 --> 01:01:06.500]   I'm trying to make sure if there's nothing on my calendar before sharing my screen.
[01:01:06.500 --> 01:01:08.500]   That might be confidential.
[01:01:08.500 --> 01:01:11.100]   Nope, it's all clear.
[01:01:11.100 --> 01:01:12.860]   I don't have anything interesting.
[01:01:12.860 --> 01:01:19.180]   So I can share my calendar.
[01:01:19.180 --> 01:01:25.180]   If you're trying to predict what happens in the next two days, next two days, given the
[01:01:25.180 --> 01:01:26.820]   previous input.
[01:01:26.820 --> 01:01:32.380]   First of all, in our deep learning, we tend to shuffle around our data, right?
[01:01:32.380 --> 01:01:37.500]   That is actually bad because we're using a trend that would have happened in these previous
[01:01:37.500 --> 01:01:38.980]   days leading up to a weekend.
[01:01:38.980 --> 01:01:43.980]   So if you're trying to predict weekend sales and use the previous week as input, you first
[01:01:43.980 --> 01:01:48.300]   of all wouldn't shuffle this input because that would mess up the trends for the model
[01:01:48.300 --> 01:01:50.500]   to learn.
[01:01:50.500 --> 01:01:57.820]   The other thing would be your validation test or validation or test set would be the weekend,
[01:01:57.820 --> 01:02:02.180]   would be the future weekend, not something from the past.
[01:02:02.180 --> 01:02:07.580]   Suppose you are trying to predict the future using the present.
[01:02:07.580 --> 01:02:12.020]   The future is the test set.
[01:02:12.020 --> 01:02:15.220]   It's also a test in real life, but it's also the test set here.
[01:02:15.220 --> 01:02:22.860]   So the validation and test set would be the future.
[01:02:22.860 --> 01:02:28.260]   So validation and test set would be from the future and not the present.
[01:02:28.260 --> 01:02:30.380]   That's what I was trying to say.
[01:02:30.380 --> 01:02:32.500]   So that is the key trick to remember.
[01:02:32.500 --> 01:02:34.100]   Vignesh is asking window size.
[01:02:34.100 --> 01:02:36.100]   We'll look at what that is.
[01:02:36.100 --> 01:02:39.900]   I assume not everyone is familiar with that concept yet because that was introduced for
[01:02:39.900 --> 01:02:42.140]   the first time in this chapter.
[01:02:42.140 --> 01:02:46.020]   So that is one of the key things that we learn in this chapter.
[01:02:46.020 --> 01:02:51.500]   The other one is to always, always start with the stupid baseline.
[01:02:51.500 --> 01:02:53.140]   What is a stupid baseline?
[01:02:53.140 --> 01:02:57.380]   If I want to predict next weekend sale, I can just take the previous three weekends
[01:02:57.380 --> 01:03:00.940]   and average them.
[01:03:00.940 --> 01:03:10.260]   Or last year's weekend sale for the same week can be this week's sale, maybe 10% more assuming
[01:03:10.260 --> 01:03:12.500]   we've grown since.
[01:03:12.500 --> 01:03:18.460]   So always start with the simple stupid model because from there you can only and you should
[01:03:18.460 --> 01:03:19.460]   only improve.
[01:03:19.460 --> 01:03:25.880]   If you're applying machine learning, which is the shiny tool, it should improve on that.
[01:03:25.880 --> 01:03:28.300]   So these are the two things that we need to remember.
[01:03:28.300 --> 01:03:36.620]   And we download this data set, which has temperature readings spread across different timings.
[01:03:36.620 --> 01:03:37.980]   And here is the plot for the same.
[01:03:37.980 --> 01:03:40.940]   So we have a lot of readings over many different days.
[01:03:40.940 --> 01:03:48.460]   I think it's over a period of inside the data 10 years, I believe.
[01:03:48.460 --> 01:03:51.820]   And now we'll just plot the first 10 days.
[01:03:51.820 --> 01:03:54.340]   So looks like it's really cold here.
[01:03:54.340 --> 01:04:00.660]   This is what minus 25 degrees that is freezing.
[01:04:00.660 --> 01:04:06.000]   I'll need a lot of time this temperature, but we set up the text test and train split
[01:04:06.000 --> 01:04:08.200]   like so.
[01:04:08.200 --> 01:04:16.720]   So we would have probably the training set before this 800 sample, let's say 800 to 1200
[01:04:16.720 --> 01:04:25.480]   would be the validation set and the one after that would be the test set.
[01:04:25.480 --> 01:04:27.720]   And we would need to normalize the data.
[01:04:27.720 --> 01:04:30.240]   First of all, it's always important.
[01:04:30.240 --> 01:04:36.760]   Now when we're loading the data set, we'll use time series data set from array utility.
[01:04:36.760 --> 01:04:39.520]   So let's take a look at what that is.
[01:04:39.520 --> 01:04:45.600]   And let's also close tabs for the sake of my sanity.
[01:04:45.600 --> 01:04:53.360]   We found that, I'll close the other tabs real quick.
[01:04:53.360 --> 01:04:56.600]   Time series data pre processing.
[01:04:56.600 --> 01:05:00.800]   This function takes in a sequence of data points gathered at equal intervals along with
[01:05:00.800 --> 01:05:07.680]   time series parameters, such as length of sequences or windows.
[01:05:07.680 --> 01:05:11.460]   Spacing between two sequences to produce patches of time series.
[01:05:11.460 --> 01:05:15.720]   So if we want to batch our inputs, we call this function and it takes care of a lot of
[01:05:15.720 --> 01:05:18.040]   things specific to time series.
[01:05:18.040 --> 01:05:20.840]   That's what I understood from this.
[01:05:20.840 --> 01:05:26.800]   And then we can iterate over different peaks like so.
[01:05:26.800 --> 01:05:32.840]   From there we define our training validation and test set not going into any details.
[01:05:32.840 --> 01:05:40.760]   Literally just splitting this into different windows are probably just vaguely maybe like
[01:05:40.760 --> 01:05:44.840]   up until 800 we have the training data set.
[01:05:44.840 --> 01:05:49.600]   800 to 1200 would be validation from the test set, something like that.
[01:05:49.600 --> 01:05:53.560]   I might be wrong about the exact numbers.
[01:05:53.560 --> 01:05:54.920]   And we batch these.
[01:05:54.920 --> 01:06:01.620]   So we'll first of all start with the stupid model, like I said.
[01:06:01.620 --> 01:06:06.740]   And from there, we'll just do an NAE.
[01:06:06.740 --> 01:06:10.860]   So someone mentioned what about a 20 day EMA?
[01:06:10.860 --> 01:06:14.860]   I think this would be similar to that.
[01:06:14.860 --> 01:06:27.080]   So we just iterate over all of the samples and we just take the sum of absolute differences
[01:06:27.080 --> 01:06:30.360]   and we divide the total error by the number of samples.
[01:06:30.360 --> 01:06:36.120]   So we take the NAE as a common baseline.
[01:06:36.120 --> 01:06:43.660]   From there, we have a model and we can see how accurate it is based on the loss or matrix
[01:06:43.660 --> 01:06:46.200]   that we're trying to optimize for.
[01:06:46.200 --> 01:06:47.720]   Now we can use machine learning.
[01:06:47.720 --> 01:06:50.700]   So now we can pull out our shiny tool and apply it.
[01:06:50.700 --> 01:06:51.700]   So what do we do?
[01:06:51.700 --> 01:06:55.200]   We also start with a silly baseline.
[01:06:55.200 --> 01:06:57.780]   Silly would be a dense model.
[01:06:57.780 --> 01:07:00.300]   So fully connected MLP.
[01:07:00.300 --> 01:07:08.280]   We want to learn about RNNs, but we are setting up that.
[01:07:08.280 --> 01:07:12.800]   I see a question by Harpy that I'll come back to in a minute.
[01:07:12.800 --> 01:07:15.120]   And we would be checkpointing the data.
[01:07:15.120 --> 01:07:20.320]   Sorry, checkpointing the model by calling the model checkpoint callback.
[01:07:20.320 --> 01:07:27.680]   Again, I believe I'm not sure if this is very intentional or I would assume it is, but these
[01:07:27.680 --> 01:07:32.160]   different concepts are quite well introduced across different chapters.
[01:07:32.160 --> 01:07:36.280]   So we learn of this callback, I believe for the first time in this chapter, I might be
[01:07:36.280 --> 01:07:38.740]   wrong about that.
[01:07:38.740 --> 01:07:43.480]   And we create a simple model that just has an input layer, a flattened layer and a dense
[01:07:43.480 --> 01:07:44.480]   layer.
[01:07:44.480 --> 01:07:46.280]   And from there we get the output.
[01:07:46.280 --> 01:07:47.280]   That's it.
[01:07:47.280 --> 01:07:49.720]   That's all it does.
[01:07:49.720 --> 01:07:51.500]   This model would suck.
[01:07:51.500 --> 01:07:53.020]   This would be really bad.
[01:07:53.020 --> 01:07:59.020]   So we predict the training and validation errors and as you can see, it's not trained
[01:07:59.020 --> 01:08:00.020]   at all.
[01:08:00.020 --> 01:08:03.180]   It's actually overfitting quite a bit to the training dataset.
[01:08:03.180 --> 01:08:06.260]   We can just say that it's not effective at all.
[01:08:06.260 --> 01:08:10.900]   It's actually performing, I believe, worse than the stupid baseline.
[01:08:10.900 --> 01:08:17.140]   So this was 2.62, this is 2.67.
[01:08:17.140 --> 01:08:20.060]   Our machine learning shiny tool has failed us.
[01:08:20.060 --> 01:08:21.580]   What a disappointment.
[01:08:21.580 --> 01:08:30.100]   I wanted to ask, can you apply convolutions to time series or tabular or NLP data?
[01:08:30.100 --> 01:08:31.820]   You can apply it.
[01:08:31.820 --> 01:08:33.300]   Would it be effective or not?
[01:08:33.300 --> 01:08:35.060]   That is a different question.
[01:08:35.060 --> 01:08:40.820]   So the first thing we do here is we apply a conv1d layer because it's not two dimensional
[01:08:40.820 --> 01:08:47.340]   data and we call a max pooling layer a conv1d, another max pooling, another conv1d.
[01:08:47.340 --> 01:08:51.860]   I'm not going into different dimensions because these scale as you increase the depth.
[01:08:51.860 --> 01:08:58.180]   So I assume whenever you are implementing these, this is the real job of an MLE to keep
[01:08:58.180 --> 01:09:01.780]   an eye out on these parameters.
[01:09:01.780 --> 01:09:05.500]   And you only learn this while you're actually implementing this.
[01:09:05.500 --> 01:09:07.180]   Spend some time doing that.
[01:09:07.180 --> 01:09:12.100]   We call a bunch of conv1d layers followed by max pooling and then global average pooling
[01:09:12.100 --> 01:09:13.460]   layer.
[01:09:13.460 --> 01:09:19.540]   And this then becomes our output where we just pass it through a dense layer.
[01:09:19.540 --> 01:09:21.420]   Now we can compile and train this model.
[01:09:21.420 --> 01:09:22.900]   It is even worse.
[01:09:22.900 --> 01:09:29.260]   So convolutions didn't work on time series problem.
[01:09:29.260 --> 01:09:38.260]   My question is, does anyone want to suggest why convolutions weren't effective for this?
[01:09:38.260 --> 01:09:58.060]   So why did the conv1d model did not work very well for this problem?
[01:09:58.060 --> 01:10:15.500]   I'll just wait for 20 seconds before I share the answer.
[01:10:15.500 --> 01:10:29.500]   The question is why did the conv1d model perform worse on this particular problem?
[01:10:29.500 --> 01:10:35.260]   Yuvraj says maybe because connets look for local patterns and not global patterns.
[01:10:35.260 --> 01:10:37.500]   That's a great answer.
[01:10:37.500 --> 01:10:39.220]   That is quite close to the actual answer.
[01:10:39.220 --> 01:10:41.900]   So quite a close call.
[01:10:41.900 --> 01:10:44.260]   Himanshu answered it perfectly.
[01:10:44.260 --> 01:10:48.260]   Connets do not retain temporal data.
[01:10:48.260 --> 01:10:56.500]   Connets split up your images and pass in different many to few mappings is the technical word
[01:10:56.500 --> 01:10:57.500]   for it.
[01:10:57.500 --> 01:11:02.860]   We looked at this in the previous section, but we perform convolution dot products over
[01:11:02.860 --> 01:11:05.380]   segments.
[01:11:05.380 --> 01:11:09.100]   That's not effective for time series because you lose temporal data.
[01:11:09.100 --> 01:11:12.660]   So you lose the meaning of time there.
[01:11:12.660 --> 01:11:19.380]   If you are to predict graph shapes, convolutions would work well, but that would lose the meaning
[01:11:19.380 --> 01:11:23.940]   of temporal data just because how convolutional neural networks work.
[01:11:23.940 --> 01:11:26.900]   So it was a great experiment.
[01:11:26.900 --> 01:11:34.460]   It confirmed this theory that hey, they don't work well with time series data.
[01:11:34.460 --> 01:11:40.700]   So now we are ready to introduce a LSTM based model.
[01:11:40.700 --> 01:11:42.940]   And LSTMs are really scary to look at.
[01:11:42.940 --> 01:11:50.060]   Let's see what do they look like.
[01:11:50.060 --> 01:11:55.780]   I would encourage everyone to check out this incredible blog post by Chris Ola.
[01:11:55.780 --> 01:11:59.000]   Let me drop that in the chart.
[01:11:59.000 --> 01:12:05.160]   But my point is, these are quite scary the first time you look at them.
[01:12:05.160 --> 01:12:11.700]   These were the hottest topics when I first heard of machine learning and NLP and I spent
[01:12:11.700 --> 01:12:15.620]   about one month reading this blog multiple times.
[01:12:15.620 --> 01:12:24.060]   Simply explained, a recurrent neural network is a for loop in math.
[01:12:24.060 --> 01:12:30.660]   I found this through a YouTube video that I can share the link afterwards.
[01:12:30.660 --> 01:12:36.420]   I think it was called transform, cognates are dead, long live transformers.
[01:12:36.420 --> 01:12:37.420]   That was the original video.
[01:12:37.420 --> 01:12:40.020]   I can share the link after this.
[01:12:40.020 --> 01:12:45.100]   A call with the signups, with the people who signed up.
[01:12:45.100 --> 01:12:52.380]   But inside of a recurrent neural network, you have different inputs that are going into
[01:12:52.380 --> 01:12:54.740]   the model.
[01:12:54.740 --> 01:12:59.980]   And the key thing then becomes the hidden layer for the previous layer becomes an input
[01:12:59.980 --> 01:13:02.900]   for the next layer.
[01:13:02.900 --> 01:13:12.340]   So it's this loop in effect that keeps sending information ahead.
[01:13:12.340 --> 01:13:19.060]   This is really effective for different language based problems.
[01:13:19.060 --> 01:13:29.000]   But it's also effective for time series because this captures the long term trends inside
[01:13:29.000 --> 01:13:32.180]   of time series data sets.
[01:13:32.180 --> 01:13:36.260]   So this information keeps getting passed and the model remembers what was happening earlier
[01:13:36.260 --> 01:13:38.260]   in this data set.
[01:13:38.260 --> 01:13:47.140]   RNNs have a problem of vanishing gradients, where these numbers start to become zero.
[01:13:47.140 --> 01:13:50.900]   So you can't really create deep RNNs.
[01:13:50.900 --> 01:13:52.420]   Just vanilla RNN.
[01:13:52.420 --> 01:13:54.060]   You can't create them.
[01:13:54.060 --> 01:14:04.040]   So to tackle that, you create something known as LSTM, which is this really daunting looking
[01:14:04.040 --> 01:14:06.680]   model the first time you look at it.
[01:14:06.680 --> 01:14:11.640]   You can read the blog post to understand inside of Keras.
[01:14:11.640 --> 01:14:14.320]   It's just a layer.
[01:14:14.320 --> 01:14:20.940]   And broadly speaking, it takes care of the fact that you can't create a deep RNN.
[01:14:20.940 --> 01:14:22.760]   You can do that here.
[01:14:22.760 --> 01:14:25.580]   And it takes care of the vanishing gradient problem.
[01:14:25.580 --> 01:14:30.520]   So it trains more effective RNN like model.
[01:14:30.520 --> 01:14:32.320]   That's what happens inside of here.
[01:14:32.320 --> 01:14:38.640]   We create an input, we pass it through an LSTM, pass it through a dense layer, which
[01:14:38.640 --> 01:14:40.280]   becomes the output.
[01:14:40.280 --> 01:14:47.240]   Then our model performs better this time.
[01:14:47.240 --> 01:14:53.640]   So I think after this, we look at a NumPy implementation of RNN, which I'll skip because
[01:14:53.640 --> 01:14:55.480]   it's quite straightforward.
[01:14:55.480 --> 01:15:01.440]   And we also look at a recurrent layer in Keras, which was also straightforward.
[01:15:01.440 --> 01:15:07.540]   RNNs are a bit prone to overfitting.
[01:15:07.540 --> 01:15:10.140]   So we tackle that by adding dropout.
[01:15:10.140 --> 01:15:13.700]   So this again is the model same as earlier.
[01:15:13.700 --> 01:15:16.080]   And this time we just added a dropout layer.
[01:15:16.080 --> 01:15:22.480]   So we would have learned in our previous example, that the model was overfitting.
[01:15:22.480 --> 01:15:24.580]   So this time we added a dropout layer.
[01:15:24.580 --> 01:15:27.840]   And I was supposed to show you the output.
[01:15:27.840 --> 01:15:36.040]   But as you can see, just on 30th epoch, which is halfway through, the accuracy was validation
[01:15:36.040 --> 01:15:38.380]   accuracy was a bit better than earlier.
[01:15:38.380 --> 01:15:41.840]   So I think we're overfitting again, a bit.
[01:15:41.840 --> 01:15:46.960]   But the validation accuracy was already better than the previous model.
[01:15:46.960 --> 01:15:52.400]   Furthermore, we learn how to stack RNN layers.
[01:15:52.400 --> 01:16:00.800]   So there's another model known as a GRU model, which is a gated.
[01:16:00.800 --> 01:16:04.120]   Let's look that up so that I don't mess it up.
[01:16:04.120 --> 01:16:05.120]   What is a GRU?
[01:16:05.120 --> 01:16:12.920]   Gated Recurrent Unit.
[01:16:12.920 --> 01:16:14.880]   I am haunted by this mistake.
[01:16:14.880 --> 01:16:17.600]   At one point I was interviewing for an internship.
[01:16:17.600 --> 01:16:20.220]   I'm very happy to be at Bates & Weissel.
[01:16:20.220 --> 01:16:23.260]   But this was way back when I was interviewing for an internship.
[01:16:23.260 --> 01:16:25.280]   I forgot this definition.
[01:16:25.280 --> 01:16:28.000]   Let's make me and everyone remember this.
[01:16:28.000 --> 01:16:30.640]   The full form is Gated Recurrent Unit.
[01:16:30.640 --> 01:16:32.960]   It's a paper from 2014.
[01:16:32.960 --> 01:16:34.440]   But that's what GRU stands for.
[01:16:34.440 --> 01:16:41.480]   I don't know what rubbish I came up with, but it wasn't this and I didn't get that internship.
[01:16:41.480 --> 01:16:46.440]   I luckily, the same semester went to an IIT where a professor took me for an internship.
[01:16:46.440 --> 01:16:48.360]   So things went well.
[01:16:48.360 --> 01:16:53.200]   But this is what GRU stands for.
[01:16:53.200 --> 01:16:55.640]   I don't think I'll go into depth of what these are.
[01:16:55.640 --> 01:17:00.120]   But again, this is another variant of RNNs.
[01:17:00.120 --> 01:17:04.640]   And I think we look at how we can stack these layers effectively.
[01:17:04.640 --> 01:17:09.120]   So how can we stack GRU layers and add a dropout after them.
[01:17:09.120 --> 01:17:13.320]   Again, this is another model and we take a look at how this performed.
[01:17:13.320 --> 01:17:22.960]   We also take a look at bidirectional RNNs, which would be a bidirectional LSTM.
[01:17:22.960 --> 01:17:25.640]   I don't think the book covers a lot of theory around it.
[01:17:25.640 --> 01:17:31.640]   It's a simple layer where you simply pass LSTM layer inside of bidirectional.
[01:17:31.640 --> 01:17:41.160]   So let's take a look at what is bidirectional real quick.
[01:17:41.160 --> 01:17:46.680]   It's a wrapper for RNNs and it's not very detailed on what it does.
[01:17:46.680 --> 01:17:49.760]   So I'll leave it at that.
[01:17:49.760 --> 01:17:52.480]   I think that was it for chapter 10.
[01:17:52.480 --> 01:17:56.040]   So we've gone through chapter 10 as well.
[01:17:56.040 --> 01:18:02.560]   And I'll now take a look at questions before moving further on to covering attention.
[01:18:02.560 --> 01:18:18.000]   Let me close this tab and all remaining tabs as well.
[01:18:18.000 --> 01:18:20.040]   And now let's take a look at question.
[01:18:20.040 --> 01:18:25.320]   Oh, there's been an incredible discussion happening that I missed.
[01:18:25.320 --> 01:18:30.080]   Will Conf1D work for small independent sequences like reviews?
[01:18:30.080 --> 01:18:32.280]   I don't think so.
[01:18:32.280 --> 01:18:34.520]   It's really a word.
[01:18:34.520 --> 01:18:40.720]   So inside of a Conf1D layer, even individual characters would get mixed up, I would assume.
[01:18:40.720 --> 01:18:47.080]   So it's really not an effective technique.
[01:18:47.080 --> 01:18:53.120]   Can we add residual connections in RNN to make them work?
[01:18:53.120 --> 01:19:01.400]   I believe something similar happens in transformers.
[01:19:01.400 --> 01:19:07.840]   So I didn't comment on that Harpreet because I'm not sure if it's already being done and
[01:19:07.840 --> 01:19:10.920]   if it's called the same.
[01:19:10.920 --> 01:19:16.160]   I'll check and speak about this in the first few minutes of the next lecture.
[01:19:16.160 --> 01:19:18.920]   So thanks for the question.
[01:19:18.920 --> 01:19:22.200]   I think it's being done in transformers.
[01:19:22.200 --> 01:19:26.240]   I need to check if it's called the same or if it has a different name.
[01:19:26.240 --> 01:19:30.320]   Himanshu is asking, sounds like vanishing gradient should be a problem for any network
[01:19:30.320 --> 01:19:35.800]   that is deep enough.
[01:19:35.800 --> 01:19:36.800]   I think you're right.
[01:19:36.800 --> 01:19:38.720]   Yeah, it is a problem.
[01:19:38.720 --> 01:19:47.120]   But it's more prone to RNNs because if you look at the, I think there's some analysis
[01:19:47.120 --> 01:19:54.600]   around it, but the number becomes smaller, faster in RNNs compared to CNNs.
[01:19:54.600 --> 01:19:58.000]   Don't quote me on that, but that's why.
[01:19:58.000 --> 01:20:01.520]   Sorry, I'm mixing these up.
[01:20:01.520 --> 01:20:05.120]   I apologize.
[01:20:05.120 --> 01:20:10.080]   It is a problem that prones both networks equally.
[01:20:10.080 --> 01:20:15.200]   And it prones LSTMs lesser than RNNs.
[01:20:15.200 --> 01:20:24.520]   So LSTMs are lesser prone to vanishing gradients than RNNs.
[01:20:24.520 --> 01:20:25.840]   Himanshu is right.
[01:20:25.840 --> 01:20:28.640]   I think they might be called a carry connection.
[01:20:28.640 --> 01:20:31.600]   That's the term I want to double check.
[01:20:31.600 --> 01:20:35.360]   I'll still check and confirm this.
[01:20:35.360 --> 01:20:40.760]   Luckily, Yuvraj says it is covered.
[01:20:40.760 --> 01:20:43.920]   So I trust his answer.
[01:20:43.920 --> 01:20:49.560]   And I'll still, Yuvraj, can you confirm if it's still called a residual connection?
[01:20:49.560 --> 01:20:55.480]   I've been reading a lot of papers, so I might be getting confused in my head.
[01:20:55.480 --> 01:20:57.880]   I'll wait for him to confirm that.
[01:20:57.880 --> 01:21:03.600]   But in the meantime, can I have your attention, please?
[01:21:03.600 --> 01:21:09.000]   When I say these words, I think I assume not everyone of you is paying close attention
[01:21:09.000 --> 01:21:10.160]   to the YouTube video, right?
[01:21:10.160 --> 01:21:12.280]   You might have something on the side.
[01:21:12.280 --> 01:21:14.560]   You might be looking at Reels.
[01:21:14.560 --> 01:21:18.360]   But when I said those words, did your brain, hopefully, hopefully your brain redirected
[01:21:18.360 --> 01:21:22.480]   your senses towards the live stream or towards the screen.
[01:21:22.480 --> 01:21:27.120]   So the attention module asks this question.
[01:21:27.120 --> 01:21:28.120]   How do we encapsulate?
[01:21:28.120 --> 01:21:35.640]   How do we capture this into a neural network or into a machine learning model?
[01:21:35.640 --> 01:21:44.120]   So how do we make a machine learning model pay attention to different parts?
[01:21:44.120 --> 01:21:49.840]   That is what attention layers or modules do.
[01:21:49.840 --> 01:21:57.920]   And to understand that, I will use Jay Allamur's blog post on visualizing machine translation,
[01:21:57.920 --> 01:22:03.880]   which will tell you the theory of what are the pre-attention approaches.
[01:22:03.880 --> 01:22:08.560]   So I've dropped that link in the chat and let me highlight that as well.
[01:22:08.560 --> 01:22:15.000]   If you simply look up visualizing neural machine translation, Jay Allamur on Google, it should
[01:22:15.000 --> 01:22:18.120]   take you to his blog post.
[01:22:18.120 --> 01:22:22.280]   He also has an amazing YouTube channel that I really enjoy watching.
[01:22:22.280 --> 01:22:25.720]   So I really suggest everyone to check out his channel.
[01:22:25.720 --> 01:22:28.400]   I'm subscribed from my different account.
[01:22:28.400 --> 01:22:29.680]   I think this is my work account.
[01:22:29.680 --> 01:22:31.600]   That's why I'm not subscribed.
[01:22:31.600 --> 01:22:32.920]   But again, check out his work.
[01:22:32.920 --> 01:22:33.920]   It's really incredible.
[01:22:33.920 --> 01:22:40.320]   Both the blog posts and YouTube.
[01:22:40.320 --> 01:22:46.700]   So to translate sentences, you have something known as a sequence to sequence model.
[01:22:46.700 --> 01:22:53.480]   So it takes these models broadly, take a sequence of inputs and give you a sequence of outputs.
[01:22:53.480 --> 01:22:57.120]   This is why I wanted to use Jay's blog because it makes my life really easy.
[01:22:57.120 --> 01:23:01.440]   He has these incredible animations and I absolutely don't have to do anything.
[01:23:01.440 --> 01:23:03.520]   I'm the super lazy person.
[01:23:03.520 --> 01:23:05.800]   So I'm using his work.
[01:23:05.800 --> 01:23:14.720]   In a neural machine translation, this model would probably be a neural network.
[01:23:14.720 --> 01:23:19.880]   And it will take this French sentence, convert it to English.
[01:23:19.880 --> 01:23:24.520]   So it takes a sequence in a different language and converts it to another sequence.
[01:23:24.520 --> 01:23:26.520]   How does it do that?
[01:23:26.520 --> 01:23:35.260]   It encodes the input to a certain state.
[01:23:35.260 --> 01:23:46.060]   And then the decoder decodes the state to the target language, broadly speaking.
[01:23:46.060 --> 01:23:56.120]   Context is a vector that tells how much each layer of encoder decoder should pay attention
[01:23:56.120 --> 01:24:02.160]   to inside of the model.
[01:24:02.160 --> 01:24:11.760]   So it tells you how much of importance is to be given to let's say, J. So J, let's say,
[01:24:11.760 --> 01:24:15.680]   is an important word, or étudiant is an important word.
[01:24:15.680 --> 01:24:20.440]   I failed French, but I know enough to like, be able to nail that sentence.
[01:24:20.440 --> 01:24:21.700]   Hopefully I nailed it.
[01:24:21.700 --> 01:24:26.640]   But étudiant would probably get more importance in context.
[01:24:26.640 --> 01:24:27.720]   Hope maybe.
[01:24:27.720 --> 01:24:30.680]   So that's what this layer would tell the model.
[01:24:30.680 --> 01:24:32.720]   Actually, it's visualized later.
[01:24:32.720 --> 01:24:33.720]   So my life is easy.
[01:24:33.720 --> 01:24:45.200]   So J is étudiant and here's a way of, sorry, this is not the context vector.
[01:24:45.200 --> 01:24:48.120]   So this is this is how we turn it into word embedding.
[01:24:48.120 --> 01:24:50.160]   Sorry, I was mixing this.
[01:24:50.160 --> 01:24:54.840]   But let's say étudiant would probably be given a higher importance of this number probably
[01:24:54.840 --> 01:24:57.640]   would be higher for the same.
[01:24:57.640 --> 01:25:01.280]   Then we convert these into an array.
[01:25:01.280 --> 01:25:07.580]   So we turn the input words into vectors before we start processing them for our outputs.
[01:25:07.580 --> 01:25:11.500]   You can simply download pre-trained embeddings also to do this.
[01:25:11.500 --> 01:25:13.920]   That makes your life easy.
[01:25:13.920 --> 01:25:23.680]   Inside of RNNs, once you've pre-processed these, you simply pass the input vector to
[01:25:23.680 --> 01:25:28.800]   the hidden state and from there it gets passed on further.
[01:25:28.800 --> 01:25:36.400]   The hidden state carries further and you pass in the later halves or later words into the
[01:25:36.400 --> 01:25:44.840]   RNN model.
[01:25:44.840 --> 01:25:49.200]   So the first time step, the first word goes into the encoder.
[01:25:49.200 --> 01:25:51.480]   Second words go into the encoder.
[01:25:51.480 --> 01:25:53.400]   Third word gets into the encoder.
[01:25:53.400 --> 01:25:55.240]   All of these produce hidden states.
[01:25:55.240 --> 01:25:58.440]   The first hidden state produces the first output.
[01:25:58.440 --> 01:26:03.760]   Second gives the second word, third gives the third word.
[01:26:03.760 --> 01:26:10.440]   So each input gets marked to a hidden state.
[01:26:10.440 --> 01:26:18.800]   Once that is done, that goes to the decoder and every hidden state gives an output.
[01:26:18.800 --> 01:26:24.520]   So this is how RNNs work.
[01:26:24.520 --> 01:26:34.440]   And I think I wanted to specifically quickly show how does attention change this.
[01:26:34.440 --> 01:26:37.080]   But I'll probably use the architecture to summarize this.
[01:26:37.080 --> 01:26:43.520]   I can point you to Yannick Kilcher's incredible video on this.
[01:26:43.520 --> 01:26:49.240]   Or I'll actually also publish an RNN-related paper next week, so that should help you hopefully
[01:26:49.240 --> 01:26:50.760]   to understand this.
[01:26:50.760 --> 01:26:54.960]   But inside of this architecture, again I'm trying to theoretically give an overview of
[01:26:54.960 --> 01:26:56.320]   how attention works.
[01:26:56.320 --> 01:27:08.120]   So I'll quickly use this architecture.
[01:27:08.120 --> 01:27:12.680]   RNNs use a masked multi-head attention.
[01:27:12.680 --> 01:27:17.120]   And inside of this, there are three main things that happen.
[01:27:17.120 --> 01:27:20.780]   And these are three main inputs that go into this.
[01:27:20.780 --> 01:27:26.160]   These are Q, K, V. Query, key, and value.
[01:27:26.160 --> 01:27:28.440]   Let's see if that's mentioned somewhere in the paper.
[01:27:28.440 --> 01:27:29.800]   Yes, it is.
[01:27:29.800 --> 01:27:36.720]   So inside of this attention layer, you have a query, you have a key, and you have a value.
[01:27:36.720 --> 01:27:46.000]   So internally, as the model learns different things, the model sends a query and a key
[01:27:46.000 --> 01:27:48.960]   value associated with it.
[01:27:48.960 --> 01:27:55.080]   So here, can we see how well is Sayyam's explanation?
[01:27:55.080 --> 01:27:59.680]   And then the model takes a look and it says it's absolute garbage.
[01:27:59.680 --> 01:28:02.600]   So the value comes out negative.
[01:28:02.600 --> 01:28:10.360]   This is actually helpful for training sequential to sequential models, sequence to sequence
[01:28:10.360 --> 01:28:14.600]   models because it makes context awareness easy.
[01:28:14.600 --> 01:28:19.360]   So as you saw, LSTMs really learn word by word and pass on the hidden state.
[01:28:19.360 --> 01:28:23.280]   But here you can individually query different words.
[01:28:23.280 --> 01:28:29.160]   So one of the things that RNNs and LSTMs struggle with is long term association.
[01:28:29.160 --> 01:28:34.720]   But this makes it easy to check how different words get associated at a broad level.
[01:28:34.720 --> 01:28:42.360]   So very simply put, this is attention layer, or this is scaled out product attention.
[01:28:42.360 --> 01:28:48.200]   And this multi head attention layer really revolutionized our field in a way.
[01:28:48.200 --> 01:28:52.380]   First of all, NLP and now computer vision.
[01:28:52.380 --> 01:28:55.340]   But I quickly wanted to introduce these two concepts.
[01:28:55.340 --> 01:29:00.200]   So we've looked at sequence to sequence models, and we've looked at attention.
[01:29:00.200 --> 01:29:08.100]   In the next lecture, we'll be taking a look at how can we use these to work on text data.
[01:29:08.100 --> 01:29:10.960]   So suggested homework, we looked at a lot of things.
[01:29:10.960 --> 01:29:16.000]   One of these were looking at a PyTorch paper that is implemented in Keras.
[01:29:16.000 --> 01:29:19.440]   And this was done by my really smart colleague, Shoumik.
[01:29:19.440 --> 01:29:24.360]   So I would really encourage you to follow his footsteps and try to do that.
[01:29:24.360 --> 01:29:26.640]   I haven't seen anyone submit to a Kaggle competition.
[01:29:26.640 --> 01:29:30.200]   I would really, really urge you to start doing that.
[01:29:30.200 --> 01:29:36.120]   Please consider playing with GradCAM and training a time series RNN or LSTM.
[01:29:36.120 --> 01:29:39.920]   And you can, of course, log all of these experiments with weights and biases.
[01:29:39.920 --> 01:29:44.240]   Write about this, share it, tag weights and biases, tag me.
[01:29:44.240 --> 01:29:48.360]   If you're interested, we're still actively sending swags to any participants.
[01:29:48.360 --> 01:29:50.880]   And I believe that was it for today's lecture.
[01:29:50.880 --> 01:29:51.880]   That's all I wanted to cover.
[01:29:51.880 --> 01:29:55.760]   I'll quickly check if there are any questions.
[01:29:55.760 --> 01:30:10.480]   Oops, sorry, I just realized the screen wasn't visible while I was presenting 27 days of
[01:30:10.480 --> 01:30:11.560]   Keras and homework.
[01:30:11.560 --> 01:30:14.040]   So I'll bring that out.
[01:30:14.040 --> 01:30:20.280]   Here's the issue that happens when you're sharing your screen.
[01:30:20.280 --> 01:30:24.400]   You can see the presentation, but sometimes it doesn't get shared effectively.
[01:30:24.400 --> 01:30:25.400]   Sorry about that.
[01:30:25.400 --> 01:30:27.480]   I'll quickly point these things out again.
[01:30:27.480 --> 01:30:29.920]   Please consider implementing a PyTorch paper to Keras.
[01:30:29.920 --> 01:30:33.440]   That'll be easier because you can quickly map the layers.
[01:30:33.440 --> 01:30:36.640]   Again, nothing against or nothing for one framework over the other.
[01:30:36.640 --> 01:30:37.640]   Both are incredible.
[01:30:37.640 --> 01:30:39.600]   We're here to learn about Keras.
[01:30:39.600 --> 01:30:45.360]   Earlier, we learned about PyTorch.
[01:30:45.360 --> 01:30:48.720]   Again, whatever works better for you, use that.
[01:30:48.720 --> 01:30:52.840]   But if you're learning Keras, port a paper from PyTorch to Keras.
[01:30:52.840 --> 01:30:54.680]   Please submit to Kaggle.
[01:30:54.680 --> 01:30:56.680]   Play with Gradcam.
[01:30:56.680 --> 01:30:59.960]   Log your experiments with Weights and Biases.
[01:30:59.960 --> 01:31:06.720]   And consider training a time series model using RNN or LSTM.
[01:31:06.720 --> 01:31:08.320]   Share it on Twitter.
[01:31:08.320 --> 01:31:09.680]   Share it with the world.
[01:31:09.680 --> 01:31:10.680]   Use the hashtag #27daysofkeras.
[01:31:10.680 --> 01:31:13.880]   Tag Weights and Biases or tag me.
[01:31:13.880 --> 01:31:15.960]   We'd love to share your work further.
[01:31:15.960 --> 01:31:19.680]   And we'd love to send some swag to the best participants every week.
[01:31:19.680 --> 01:31:22.680]   We're still actively doing that.
[01:31:22.680 --> 01:31:23.680]   Thanks again.
[01:31:23.680 --> 01:31:27.080]   Any -- I can take any final question.
[01:31:27.080 --> 01:31:30.720]   And let me quickly see if there are any.
[01:31:30.720 --> 01:31:32.560]   No, there are not.
[01:31:32.560 --> 01:31:34.480]   And I managed to complete in time.
[01:31:34.480 --> 01:31:36.360]   So again, thanks, everyone, for joining.
[01:31:36.360 --> 01:31:39.640]   It's really great to learn with all of you every week.
[01:31:39.640 --> 01:31:45.200]   We have two weeks of -- two or three weeks of the series remaining.
[01:31:45.200 --> 01:31:50.880]   The last week will be one week-long blogathon where I'll invite everyone to write one blog
[01:31:50.880 --> 01:31:52.680]   post.
[01:31:52.680 --> 01:31:57.200]   And we'll select winners and we'll be sending out swag to the top winners.
[01:31:57.200 --> 01:32:00.680]   So the requirements would be the blog should be in Keras.
[01:32:00.680 --> 01:32:02.920]   And if you want, it should include Weights and Biases.
[01:32:02.920 --> 01:32:07.120]   We might be biased towards that or that might be a strong requirement.
[01:32:07.120 --> 01:32:12.560]   Although I suspect it can just be using Keras.
[01:32:12.560 --> 01:32:16.040]   Again, please start thinking along those lines.
[01:32:16.040 --> 01:32:17.480]   Thanks for joining me every week.
[01:32:17.480 --> 01:32:22.120]   And I'll see you next week when we learn about attention, NLP models and also a bit about
[01:32:22.120 --> 01:32:24.720]   generative art.
[01:32:24.720 --> 01:32:25.720]   Thank you so much.
[01:32:25.720 --> 01:32:26.280]   And I'll see you tomorrow.
[01:32:26.280 --> 01:32:36.280]   [BLANK_AUDIO]

