
[00:00:00.000 --> 00:00:03.440]   The human brain is at least 100 trillion synapses,
[00:00:03.440 --> 00:00:05.880]   and it could be as high as 1,000 trillion.
[00:00:05.880 --> 00:00:08.500]   And a synapse is a channel connected to neurons
[00:00:08.500 --> 00:00:12.000]   through which an electrical or chemical signal is transferred
[00:00:12.000 --> 00:00:15.560]   and is the loose inspiration for the synapses, weights,
[00:00:15.560 --> 00:00:18.640]   parameters of an artificial neural network.
[00:00:18.640 --> 00:00:23.280]   GPT-3, the recently released language model from OpenAI
[00:00:23.280 --> 00:00:25.640]   that has been captivating people's imagination
[00:00:25.640 --> 00:00:28.400]   with zero shot or few shot learning,
[00:00:28.400 --> 00:00:33.320]   has 175 billion synapses or parameters.
[00:00:33.320 --> 00:00:35.120]   As mentioned in the OpenAI paper,
[00:00:35.120 --> 00:00:37.240]   the amount of compute that was used to train
[00:00:37.240 --> 00:00:38.680]   the final version of this network
[00:00:38.680 --> 00:00:43.520]   was 3.14 times 10 to the 23rd flops.
[00:00:43.520 --> 00:00:45.440]   And if we use reasonable cost estimates
[00:00:45.440 --> 00:00:48.640]   based on Lambda's test of U100 cloud instance,
[00:00:48.640 --> 00:00:52.320]   the cost of training this neural network is $4.6 million.
[00:00:52.320 --> 00:00:55.420]   Now, the natural question I had is,
[00:00:55.420 --> 00:00:59.420]   if the model with 175 billion parameters does very well,
[00:00:59.420 --> 00:01:02.620]   how well will a model do that has the same number
[00:01:02.620 --> 00:01:05.300]   of parameters as our human brain?
[00:01:05.300 --> 00:01:07.900]   Setting aside the fact that both our estimate
[00:01:07.900 --> 00:01:10.580]   of the number of synapses and the intricate structure
[00:01:10.580 --> 00:01:13.260]   of the brain might require a much, much larger
[00:01:13.260 --> 00:01:15.500]   neural network to approximate the brain.
[00:01:15.500 --> 00:01:18.420]   But it's very possible that even just this 100 trillion
[00:01:18.420 --> 00:01:20.820]   synapse number will allow us to see
[00:01:20.820 --> 00:01:23.700]   some magical performance from these systems.
[00:01:23.700 --> 00:01:27.500]   And one way of asking the question of how far away are we,
[00:01:27.500 --> 00:01:29.340]   is how much does it approximately cost
[00:01:29.340 --> 00:01:32.880]   to train a model with 100 trillion parameters?
[00:01:32.880 --> 00:01:36.540]   So GPT-3 is 175 billion parameters
[00:01:36.540 --> 00:01:39.440]   and $4.6 million in 2020.
[00:01:39.440 --> 00:01:45.660]   Let's call it GPT-4HB with 100 trillion parameters.
[00:01:45.660 --> 00:01:48.780]   Assuming linear scaling of compute requirements
[00:01:48.780 --> 00:01:51.580]   with respect to number of parameters,
[00:01:51.580 --> 00:01:54.800]   the cost in 2020 for training this neural network
[00:01:54.800 --> 00:01:56.980]   is $2.6 billion.
[00:01:56.980 --> 00:01:58.900]   Now, another interesting open AI paper
[00:01:58.900 --> 00:02:00.420]   that I've talked about in the past,
[00:02:00.420 --> 00:02:02.740]   titled "Measuring the Algorithmic Efficiency
[00:02:02.740 --> 00:02:06.740]   of Neural Networks," indicates that for the past seven years
[00:02:06.740 --> 00:02:09.480]   the neural network training efficiency
[00:02:09.480 --> 00:02:12.180]   has been doubling every 16 months.
[00:02:12.180 --> 00:02:16.060]   So if this trend continues, then in 2024,
[00:02:16.060 --> 00:02:20.740]   the cost of training this GPT-HB network
[00:02:20.740 --> 00:02:25.740]   would be $325 million, decreasing to $40 million in 2028,
[00:02:25.740 --> 00:02:29.860]   and in 2032, coming down to approximately the same price
[00:02:29.860 --> 00:02:34.140]   as the GPT-3 network today at $5 million.
[00:02:34.140 --> 00:02:36.300]   Now, it's important to note, as the paper indicates,
[00:02:36.300 --> 00:02:39.460]   that as the size of the network and the compute increases,
[00:02:39.460 --> 00:02:41.620]   the improvement of the performance of the network
[00:02:41.620 --> 00:02:43.380]   follows a power law.
[00:02:43.380 --> 00:02:45.620]   Still, given some of the impressive
[00:02:45.620 --> 00:02:49.700]   Turing test passing performances of GPT-3,
[00:02:49.700 --> 00:02:53.100]   it's fascinating to think what a language model
[00:02:53.100 --> 00:02:57.340]   with 100 trillion parameters might be able to accomplish.
[00:02:57.340 --> 00:03:00.140]   I might make a few short videos like these,
[00:03:00.140 --> 00:03:04.540]   focusing on a single, simple idea on the basics of GPT-3,
[00:03:04.540 --> 00:03:08.500]   including technical, even philosophical implications,
[00:03:08.500 --> 00:03:12.060]   along with highlighting how others are using it.
[00:03:12.060 --> 00:03:14.620]   So if you enjoy this kind of thing, subscribe,
[00:03:14.620 --> 00:03:17.180]   and remember, try to learn something new every day.
[00:03:17.180 --> 00:03:19.760]   (upbeat music)
[00:03:19.760 --> 00:03:22.340]   (upbeat music)
[00:03:22.340 --> 00:03:24.920]   (upbeat music)
[00:03:24.920 --> 00:03:27.500]   (upbeat music)
[00:03:27.500 --> 00:03:37.500]   [BLANK_AUDIO]

