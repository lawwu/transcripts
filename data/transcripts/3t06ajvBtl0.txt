
[00:00:00.000 --> 00:00:03.440]   The following is a conversation with Matt Botmanick,
[00:00:03.440 --> 00:00:06.840]   Director of Neuroscience Research at DeepMind.
[00:00:06.840 --> 00:00:09.360]   He's a brilliant, cross-disciplinary mind
[00:00:09.360 --> 00:00:12.480]   navigating effortlessly between cognitive psychology,
[00:00:12.480 --> 00:00:16.780]   computational neuroscience, and artificial intelligence.
[00:00:16.780 --> 00:00:18.320]   Quick summary of the ads.
[00:00:18.320 --> 00:00:21.060]   Two sponsors, The Jordan Harbinger Show
[00:00:21.060 --> 00:00:23.880]   and Magic Spoon Cereal.
[00:00:23.880 --> 00:00:25.600]   Please consider supporting the podcast
[00:00:25.600 --> 00:00:29.340]   by going to jordanharbinger.com/lex
[00:00:29.340 --> 00:00:33.800]   and also going to magicspoon.com/lex
[00:00:33.800 --> 00:00:36.120]   and using code LEX at checkout
[00:00:36.120 --> 00:00:39.080]   after you buy all of their cereal.
[00:00:39.080 --> 00:00:40.920]   Click the links, buy the stuff.
[00:00:40.920 --> 00:00:43.040]   It's the best way to support this podcast
[00:00:43.040 --> 00:00:44.760]   and the journey I'm on.
[00:00:44.760 --> 00:00:47.680]   If you enjoy this podcast, subscribe on YouTube,
[00:00:47.680 --> 00:00:49.920]   review it with Five Stars on Apple Podcast,
[00:00:49.920 --> 00:00:52.360]   follow on Spotify, support on Patreon,
[00:00:52.360 --> 00:00:55.600]   or connect with me on Twitter @lexfriedman,
[00:00:55.600 --> 00:01:00.600]   spelled surprisingly without the E, just F-R-I-D-M-A-N.
[00:01:00.600 --> 00:01:03.900]   As usual, I'll do a few minutes of ads now
[00:01:03.900 --> 00:01:05.180]   and never any ads in the middle
[00:01:05.180 --> 00:01:07.620]   that can break the flow of the conversation.
[00:01:07.620 --> 00:01:11.740]   This episode is supported by The Jordan Harbinger Show.
[00:01:11.740 --> 00:01:15.220]   Go to jordanharbinger.com/lex.
[00:01:15.220 --> 00:01:16.900]   It's how he knows I sent you.
[00:01:16.900 --> 00:01:19.380]   On that page, subscribe to his podcast
[00:01:19.380 --> 00:01:24.340]   on Apple Podcast, Spotify, and you know where to look.
[00:01:24.340 --> 00:01:26.100]   I've been binging on his podcast.
[00:01:26.100 --> 00:01:28.400]   Jordan is a great interviewer
[00:01:28.400 --> 00:01:30.280]   and even a better human being.
[00:01:30.280 --> 00:01:32.760]   I recently listened to his conversation with Jack Barsky,
[00:01:32.760 --> 00:01:36.120]   former sleeper agent for the KGB in the '80s
[00:01:36.120 --> 00:01:38.880]   and author of "Deep Undercover,"
[00:01:38.880 --> 00:01:40.760]   which is a memoir that paints yet another
[00:01:40.760 --> 00:01:43.440]   interesting perspective on the Cold War era.
[00:01:43.440 --> 00:01:46.720]   I've been reading a lot about the Stalin
[00:01:46.720 --> 00:01:49.280]   and then Gorbachev and Putin eras of Russia,
[00:01:49.280 --> 00:01:50.800]   but this conversation made me realize
[00:01:50.800 --> 00:01:53.680]   that I need to do a deep dive into the Cold War era
[00:01:53.680 --> 00:01:57.100]   to get a complete picture of Russia's recent history.
[00:01:57.100 --> 00:02:01.100]   Again, go to jordanharbinger.com/lex.
[00:02:01.100 --> 00:02:02.820]   Subscribe to his podcast.
[00:02:02.820 --> 00:02:04.400]   It's how he knows I sent you.
[00:02:04.400 --> 00:02:05.240]   It's awesome.
[00:02:05.240 --> 00:02:06.700]   You won't regret it.
[00:02:06.700 --> 00:02:10.280]   This episode is also supported by Magic Spoon,
[00:02:10.280 --> 00:02:15.280]   low-carb, keto-friendly, super amazingly delicious cereal.
[00:02:15.280 --> 00:02:18.260]   I've been on a keto or very low-carb diet
[00:02:18.260 --> 00:02:19.440]   for a long time now.
[00:02:19.440 --> 00:02:21.260]   It helps with my mental performance.
[00:02:21.260 --> 00:02:22.800]   It helps with my physical performance,
[00:02:22.800 --> 00:02:25.280]   even during this crazy push-up,
[00:02:25.280 --> 00:02:27.760]   pull-up challenge I'm doing, including the running.
[00:02:27.760 --> 00:02:29.640]   It just feels great.
[00:02:29.640 --> 00:02:31.280]   I used to love cereal.
[00:02:31.280 --> 00:02:33.800]   Obviously, I can't have it now
[00:02:33.800 --> 00:02:36.760]   because most cereals have a crazy amount of sugar,
[00:02:36.760 --> 00:02:38.360]   which is terrible for you.
[00:02:38.360 --> 00:02:40.100]   So I quit it years ago.
[00:02:40.100 --> 00:02:44.200]   But Magic Spoon, amazingly, somehow,
[00:02:44.200 --> 00:02:45.880]   is a totally different thing.
[00:02:45.880 --> 00:02:48.300]   Zero sugar, 11 grams of protein,
[00:02:48.300 --> 00:02:50.880]   and only three net grams of carbs.
[00:02:50.880 --> 00:02:53.080]   It tastes delicious.
[00:02:53.080 --> 00:02:55.160]   It has a lot of flavors, two new ones,
[00:02:55.160 --> 00:02:56.700]   including peanut butter.
[00:02:56.700 --> 00:02:58.480]   But if you know what's good for you,
[00:02:58.480 --> 00:03:01.520]   you'll go with cocoa, my favorite flavor,
[00:03:01.520 --> 00:03:04.160]   and the flavor of champions.
[00:03:04.160 --> 00:03:07.840]   Click the magicspoon.com/lex link in the description
[00:03:07.840 --> 00:03:11.000]   and use code LEX at checkout for free shipping
[00:03:11.000 --> 00:03:13.080]   and to let them know I sent you.
[00:03:13.080 --> 00:03:16.440]   They've agreed to sponsor this podcast for a long time.
[00:03:16.440 --> 00:03:19.920]   They're an amazing sponsor and an even better cereal.
[00:03:19.920 --> 00:03:21.760]   I highly recommend it.
[00:03:21.760 --> 00:03:22.600]   It's delicious.
[00:03:22.600 --> 00:03:23.420]   It's good for you.
[00:03:23.420 --> 00:03:24.680]   You won't regret it.
[00:03:24.680 --> 00:03:28.460]   And now, here's my conversation with Matt Botvinick.
[00:03:28.460 --> 00:03:32.360]   How much of the human brain do you think we understand?
[00:03:32.360 --> 00:03:36.920]   - I think we're at a weird moment
[00:03:36.920 --> 00:03:40.040]   in the history of neuroscience in the sense that
[00:03:40.040 --> 00:03:47.320]   I feel like we understand a lot about the brain
[00:03:47.320 --> 00:03:49.400]   at a very high level,
[00:03:49.400 --> 00:03:52.600]   but a very coarse level.
[00:03:52.600 --> 00:03:54.280]   - When you say high level, what are you thinking?
[00:03:54.280 --> 00:03:55.480]   Are you thinking functional?
[00:03:55.480 --> 00:03:57.000]   Are you thinking structurally?
[00:03:57.000 --> 00:04:00.760]   - So in other words, what is the brain for?
[00:04:00.760 --> 00:04:03.880]   You know, what kinds of computation does the brain do?
[00:04:03.880 --> 00:04:10.040]   What kinds of behaviors would we have to explain
[00:04:10.040 --> 00:04:15.000]   if we were gonna look down at the mechanistic level?
[00:04:15.000 --> 00:04:18.520]   And at that level, I feel like we understand
[00:04:18.520 --> 00:04:20.120]   much, much more about the brain than we did
[00:04:20.120 --> 00:04:22.080]   when I was in high school.
[00:04:22.080 --> 00:04:25.240]   But it's almost like we're seeing it through a fog.
[00:04:25.240 --> 00:04:26.600]   It's only at a very coarse level.
[00:04:26.600 --> 00:04:30.200]   We don't really understand what the neuronal mechanisms are
[00:04:30.200 --> 00:04:32.500]   that underlie these computations.
[00:04:32.500 --> 00:04:34.600]   We've gotten better at saying,
[00:04:34.600 --> 00:04:36.720]   what are the functions that the brain is computing
[00:04:36.720 --> 00:04:38.400]   that we would have to understand
[00:04:38.400 --> 00:04:40.240]   if we were gonna get down to the neuronal level?
[00:04:40.240 --> 00:04:42.140]   And at the other end of the spectrum,
[00:04:45.520 --> 00:04:49.600]   in the last few years, incredible progress has been made
[00:04:49.600 --> 00:04:54.600]   in terms of technologies that allow us to see,
[00:04:54.600 --> 00:04:57.220]   actually literally see in some cases,
[00:04:57.220 --> 00:05:01.040]   what's going on at the single unit level,
[00:05:01.040 --> 00:05:02.640]   even the dendritic level.
[00:05:02.640 --> 00:05:05.800]   And then there's this yawning gap in between.
[00:05:05.800 --> 00:05:06.640]   - Well, that's interesting.
[00:05:06.640 --> 00:05:07.460]   So at the high level,
[00:05:07.460 --> 00:05:09.600]   so that's almost a cognitive science level.
[00:05:09.600 --> 00:05:11.880]   And then at the neuronal level,
[00:05:11.880 --> 00:05:14.580]   that's neurobiology and neuroscience,
[00:05:14.580 --> 00:05:16.040]   just studying single neurons,
[00:05:16.040 --> 00:05:19.800]   the synaptic connections and all the dopamine,
[00:05:19.800 --> 00:05:21.560]   all the kind of neurotransmitters.
[00:05:21.560 --> 00:05:24.260]   - One blanket statement I should probably make is that,
[00:05:24.260 --> 00:05:27.760]   as I've gotten older,
[00:05:27.760 --> 00:05:30.200]   I have become more and more reluctant
[00:05:30.200 --> 00:05:33.400]   to make a distinction between psychology and neuroscience.
[00:05:33.400 --> 00:05:37.240]   To me, the point of neuroscience
[00:05:37.240 --> 00:05:41.780]   is to study what the brain is for.
[00:05:41.780 --> 00:05:44.360]   If you're a nephrologist
[00:05:44.360 --> 00:05:46.540]   and you wanna learn about the kidney,
[00:05:46.540 --> 00:05:49.960]   you start by saying, what is this thing for?
[00:05:49.960 --> 00:05:54.960]   Well, it seems to be for taking blood on one side
[00:05:54.960 --> 00:06:00.220]   that has metabolites in it that shouldn't be there,
[00:06:00.220 --> 00:06:03.280]   sucking them out of the blood
[00:06:03.280 --> 00:06:05.120]   while leaving the good stuff behind,
[00:06:05.120 --> 00:06:07.020]   and then excreting that in the form of urine.
[00:06:07.020 --> 00:06:10.200]   That's what the kidney is for, it's like obvious.
[00:06:10.200 --> 00:06:13.160]   So the rest of the work is deciding how it does that.
[00:06:13.160 --> 00:06:14.760]   And this, it seems to me,
[00:06:14.760 --> 00:06:17.040]   is the right approach to take to the brain.
[00:06:17.040 --> 00:06:19.080]   You say, well, what is the brain for?
[00:06:19.080 --> 00:06:22.720]   The brain, as far as I can tell, is for producing behavior.
[00:06:22.720 --> 00:06:26.960]   It's for going from perceptual inputs to behavioral outputs,
[00:06:26.960 --> 00:06:30.200]   and the behavioral outputs should be adaptive.
[00:06:30.200 --> 00:06:33.600]   So that's what psychology is about.
[00:06:33.600 --> 00:06:35.880]   It's about understanding the structure of that function.
[00:06:35.880 --> 00:06:38.880]   And then the rest of neuroscience is about figuring out
[00:06:38.880 --> 00:06:41.880]   how those operations are actually carried out
[00:06:41.880 --> 00:06:44.200]   at a mechanistic level.
[00:06:44.200 --> 00:06:45.920]   - That's really interesting, but,
[00:06:45.920 --> 00:06:49.960]   so unlike the kidney, the brain,
[00:06:49.960 --> 00:06:53.060]   the gap between the electrical signal and behavior,
[00:06:53.060 --> 00:06:59.080]   so you truly see neuroscience as the science
[00:06:59.080 --> 00:07:03.280]   that touches behavior, how the brain generates behavior,
[00:07:03.280 --> 00:07:07.440]   or how the brain converts raw visual information
[00:07:07.440 --> 00:07:09.000]   into understanding.
[00:07:09.000 --> 00:07:12.520]   Like, you basically see cognitive science,
[00:07:12.520 --> 00:07:15.880]   psychology, and neuroscience as all one science.
[00:07:15.880 --> 00:07:16.720]   - Yeah.
[00:07:16.720 --> 00:07:19.240]   - Is that-- - It's a personal statement.
[00:07:19.240 --> 00:07:20.240]   I don't mean to-- - Is that a hopeful,
[00:07:20.240 --> 00:07:22.920]   is that a hopeful or a realistic statement?
[00:07:22.920 --> 00:07:26.880]   So certainly you will be correct in your feeling
[00:07:26.880 --> 00:07:29.240]   in some number of years, but that number of years
[00:07:29.240 --> 00:07:31.440]   could be 200, 300 years from now.
[00:07:31.440 --> 00:07:33.400]   - Oh, well, there's a--
[00:07:33.400 --> 00:07:37.600]   - Is that aspirational, or is that pragmatic engineering
[00:07:37.600 --> 00:07:39.360]   feeling that you have?
[00:07:39.360 --> 00:07:44.360]   - It's both in the sense that this is what I hope
[00:07:44.360 --> 00:07:51.520]   and expect will bear fruit over the coming decades,
[00:07:51.520 --> 00:07:57.560]   but it's also pragmatic in the sense that I'm not sure
[00:07:57.560 --> 00:08:02.560]   what we're doing in either psychology or neuroscience
[00:08:02.560 --> 00:08:04.920]   if that's not the framing.
[00:08:04.920 --> 00:08:09.760]   I don't know what it means to understand the brain
[00:08:09.760 --> 00:08:14.760]   if part of the enterprise is not about understanding
[00:08:14.760 --> 00:08:20.040]   the behavior that's being produced.
[00:08:20.040 --> 00:08:24.240]   - I mean, yeah, but I would compare it to maybe
[00:08:24.240 --> 00:08:27.200]   astronomers looking at the movement of the planets
[00:08:27.200 --> 00:08:30.120]   and the stars without any interest
[00:08:30.120 --> 00:08:32.360]   of the underlying physics, right?
[00:08:32.360 --> 00:08:35.560]   And I would argue that at least in the early days,
[00:08:35.560 --> 00:08:37.760]   there's some value to just tracing the movement
[00:08:37.760 --> 00:08:41.680]   of the planets and the stars without thinking
[00:08:41.680 --> 00:08:44.080]   about the physics too much because it's such a big leap
[00:08:44.080 --> 00:08:45.600]   to start thinking about the physics
[00:08:45.600 --> 00:08:48.040]   before you even understand even the basic
[00:08:48.040 --> 00:08:49.520]   structural elements of--
[00:08:49.520 --> 00:08:51.080]   - Oh, I agree with that, I agree.
[00:08:51.080 --> 00:08:52.280]   - But you're saying in the end,
[00:08:52.280 --> 00:08:54.760]   the goal should be to deeply understand.
[00:08:54.760 --> 00:08:58.080]   - Well, right, and I think, so I thought about this
[00:08:58.080 --> 00:08:59.840]   a lot when I was in grad school 'cause a lot of what
[00:08:59.840 --> 00:09:01.840]   I studied in grad school was psychology,
[00:09:01.840 --> 00:09:06.040]   and I found myself a little bit confused
[00:09:06.040 --> 00:09:10.080]   about what it meant to, it seems like what we were talking
[00:09:10.080 --> 00:09:14.720]   about a lot of the time were virtual causal mechanisms.
[00:09:14.720 --> 00:09:19.720]   Like, oh, well, attentional selection then selects
[00:09:19.720 --> 00:09:24.280]   some object in the environment and that is then passed on
[00:09:24.280 --> 00:09:26.840]   to the motor, information about that is passed on
[00:09:26.840 --> 00:09:29.680]   to the motor system, but these are virtual mechanisms.
[00:09:29.680 --> 00:09:34.680]   These are, they're metaphors, there's no reduction
[00:09:34.680 --> 00:09:39.920]   going on in that conversation to some physical mechanism
[00:09:39.920 --> 00:09:43.160]   that, which is really what it would take
[00:09:43.160 --> 00:09:47.240]   to fully understand how behavior is rising.
[00:09:47.240 --> 00:09:50.200]   But the causal mechanisms are definitely neurons
[00:09:50.200 --> 00:09:51.840]   interacting, I'm willing to say that
[00:09:51.840 --> 00:09:53.280]   at this point in history.
[00:09:53.280 --> 00:09:56.160]   So in psychology, at least for me personally,
[00:09:56.160 --> 00:10:00.080]   there was this strange insecurity about trafficking
[00:10:00.080 --> 00:10:04.120]   in these metaphors, which were supposed to explain
[00:10:04.120 --> 00:10:05.640]   the function of the mind.
[00:10:05.640 --> 00:10:09.320]   If you can't ground them in physical mechanisms,
[00:10:09.320 --> 00:10:14.320]   then what is the explanatory validity of these explanations?
[00:10:14.320 --> 00:10:21.080]   And I managed to soothe my own nerves by thinking
[00:10:24.160 --> 00:10:29.160]   about the history of genetics research.
[00:10:29.160 --> 00:10:32.400]   So I'm very far from being an expert
[00:10:32.400 --> 00:10:36.240]   on the history of this field, but I know enough to say
[00:10:36.240 --> 00:10:41.240]   that Mendelian genetics preceded Watson and Crick.
[00:10:41.240 --> 00:10:45.480]   And so there was a significant period of time
[00:10:45.480 --> 00:10:50.480]   during which people were productively investigating
[00:10:52.760 --> 00:10:56.200]   the structure of inheritance using what was essentially
[00:10:56.200 --> 00:10:58.600]   a metaphor, the notion of a gene.
[00:10:58.600 --> 00:11:00.760]   Oh, genes do this and genes do that,
[00:11:00.760 --> 00:11:02.520]   but where are the genes?
[00:11:02.520 --> 00:11:06.080]   They're sort of an explanatory thing that we made up
[00:11:06.080 --> 00:11:08.880]   and we ascribed to them these causal properties.
[00:11:08.880 --> 00:11:10.600]   Oh, there's a dominant, there's a recessive,
[00:11:10.600 --> 00:11:12.800]   and then they recombine it.
[00:11:12.800 --> 00:11:17.440]   And then later, there was a kind of blank there
[00:11:17.440 --> 00:11:21.620]   that was filled in with a physical mechanism.
[00:11:21.620 --> 00:11:22.880]   That connection was made.
[00:11:22.880 --> 00:11:26.800]   But it was worth having that metaphor
[00:11:26.800 --> 00:11:29.360]   because that gave us a good sense
[00:11:29.360 --> 00:11:34.360]   of what kind of causal mechanism we were looking for.
[00:11:34.360 --> 00:11:38.400]   - And the fundamental metaphor of cognition,
[00:11:38.400 --> 00:11:40.440]   you said, is the interaction of neurons.
[00:11:40.440 --> 00:11:42.680]   What is the metaphor?
[00:11:42.680 --> 00:11:47.680]   - No, no, the metaphors we use in cognitive psychology
[00:11:47.680 --> 00:11:52.680]   are things like attention, the way that memory works.
[00:11:52.680 --> 00:11:59.500]   I retrieve something from memory.
[00:11:59.500 --> 00:12:01.940]   A memory retrieval occurs.
[00:12:01.940 --> 00:12:02.960]   What is that?
[00:12:02.960 --> 00:12:06.660]   That's not a physical mechanism
[00:12:06.660 --> 00:12:08.940]   that I can examine in its own right.
[00:12:08.940 --> 00:12:13.900]   But it's still worth having, that metaphorical level.
[00:12:13.900 --> 00:12:15.980]   - Yeah, I misunderstood, actually.
[00:12:15.980 --> 00:12:17.620]   So the higher level abstractions
[00:12:17.620 --> 00:12:19.620]   is the metaphor that's most useful.
[00:12:19.620 --> 00:12:20.460]   - Yes.
[00:12:20.460 --> 00:12:24.380]   - But what about, so how does that connect
[00:12:24.380 --> 00:12:29.380]   to the idea that that arises from interaction of neurons?
[00:12:29.380 --> 00:12:33.820]   - Well--
[00:12:33.820 --> 00:12:38.060]   - Is the interaction of neurons also not a metaphor to you?
[00:12:38.060 --> 00:12:42.340]   Or is it literally, like, that's no longer a metaphor.
[00:12:42.340 --> 00:12:46.100]   That's already the lowest level of abstractions
[00:12:46.100 --> 00:12:48.900]   that could actually be directly studied.
[00:12:48.900 --> 00:12:53.780]   - Well, I'm hesitating because I think
[00:12:53.780 --> 00:12:56.920]   what I wanna say could end up being controversial.
[00:12:56.920 --> 00:13:01.900]   So what I wanna say is, yes, the interactions of neurons,
[00:13:01.900 --> 00:13:04.620]   that's not metaphorical, that's a physical fact.
[00:13:04.620 --> 00:13:08.420]   That's where the causal interactions actually occur.
[00:13:08.420 --> 00:13:10.540]   Now, I suppose you could say, well,
[00:13:10.540 --> 00:13:12.660]   even that is metaphorical relative
[00:13:12.660 --> 00:13:14.740]   to the quantum events that underlie,
[00:13:14.740 --> 00:13:17.260]   you know, I don't wanna go down that rabbit hole.
[00:13:17.260 --> 00:13:18.900]   - It's always turtles on top of turtles, yeah.
[00:13:18.900 --> 00:13:20.220]   There's turtles all the way down.
[00:13:20.220 --> 00:13:22.500]   - There is a reduction that you can do.
[00:13:22.500 --> 00:13:24.580]   You can say these psychological phenomena
[00:13:24.580 --> 00:13:28.120]   can be explained through a very different
[00:13:28.120 --> 00:13:29.060]   kind of causal mechanism,
[00:13:29.060 --> 00:13:31.380]   which has to do with neurotransmitter release.
[00:13:31.380 --> 00:13:33.740]   And so what we're really trying to do
[00:13:33.740 --> 00:13:37.040]   in neuroscience writ large, as I say,
[00:13:37.040 --> 00:13:39.700]   which for me includes psychology,
[00:13:39.700 --> 00:13:44.340]   is to take these psychological phenomena
[00:13:44.340 --> 00:13:48.380]   and map them onto neural events.
[00:13:48.380 --> 00:13:54.900]   I think remaining forever at the level of description
[00:13:54.900 --> 00:14:00.420]   that is natural for psychology,
[00:14:00.420 --> 00:14:02.200]   for me personally, would be disappointing.
[00:14:02.200 --> 00:14:05.580]   I wanna understand how mental activity
[00:14:05.580 --> 00:14:10.300]   arises from neural activity.
[00:14:10.300 --> 00:14:12.940]   But the converse is also true.
[00:14:12.940 --> 00:14:15.820]   Studying neural activity without any sense
[00:14:15.820 --> 00:14:18.440]   of what you're trying to explain,
[00:14:18.440 --> 00:14:23.900]   to me feels like at best,
[00:14:23.900 --> 00:14:27.220]   groping around at random.
[00:14:27.220 --> 00:14:29.820]   - Now, you've kind of talked about this
[00:14:29.820 --> 00:14:32.800]   bridging of the gap between psychology and neuroscience,
[00:14:32.800 --> 00:14:33.980]   but do you think it's possible,
[00:14:33.980 --> 00:14:36.740]   like my love is,
[00:14:36.740 --> 00:14:38.300]   like I fell in love with psychology
[00:14:38.300 --> 00:14:40.140]   and psychiatry in general with Freud
[00:14:40.140 --> 00:14:41.780]   when I was really young,
[00:14:41.780 --> 00:14:43.540]   and I hope to understand the mind.
[00:14:43.540 --> 00:14:45.260]   And for me, understanding the mind,
[00:14:45.260 --> 00:14:47.100]   at least at a young age,
[00:14:47.100 --> 00:14:49.500]   before I discovered AI and even neuroscience,
[00:14:49.500 --> 00:14:52.860]   is psychology.
[00:14:52.860 --> 00:14:55.860]   And do you think it's possible to understand the mind
[00:14:55.860 --> 00:14:59.900]   without getting into all the messy details of neuroscience?
[00:14:59.900 --> 00:15:01.340]   Like you kind of mentioned,
[00:15:01.380 --> 00:15:05.140]   to you it's appealing to try to understand the mechanisms
[00:15:05.140 --> 00:15:07.580]   at the lowest level, but do you think that's needed,
[00:15:07.580 --> 00:15:10.260]   that's required, to understand how the mind works?
[00:15:10.260 --> 00:15:14.780]   - That's an important part of the whole picture,
[00:15:14.780 --> 00:15:18.500]   but I would be the last person on Earth
[00:15:18.500 --> 00:15:23.500]   to suggest that that reality
[00:15:23.500 --> 00:15:27.420]   renders psychology in its own right unproductive.
[00:15:27.420 --> 00:15:31.180]   I trained as a psychologist.
[00:15:31.180 --> 00:15:34.980]   I am fond of saying that I have learned much more
[00:15:34.980 --> 00:15:38.460]   from psychology than I have from neuroscience.
[00:15:38.460 --> 00:15:43.460]   To me, psychology is a hugely important discipline.
[00:15:43.460 --> 00:15:47.340]   And one thing that warms my heart is that
[00:15:47.340 --> 00:15:54.020]   ways of investigating behavior
[00:15:54.020 --> 00:15:57.940]   that have been native to cognitive psychology
[00:15:57.940 --> 00:16:01.580]   since its dawn in the '60s
[00:16:01.580 --> 00:16:04.180]   are starting to become,
[00:16:04.180 --> 00:16:07.620]   they're starting to become interesting to AI researchers
[00:16:07.620 --> 00:16:09.460]   for a variety of reasons.
[00:16:09.460 --> 00:16:11.660]   And that's been exciting for me to see.
[00:16:11.660 --> 00:16:15.220]   - Can you maybe talk a little bit about what you see as
[00:16:15.220 --> 00:16:19.260]   beautiful aspects of psychology,
[00:16:19.260 --> 00:16:21.900]   maybe limiting aspects of psychology?
[00:16:21.900 --> 00:16:25.620]   I mean, maybe just start it off as a science, as a field.
[00:16:25.620 --> 00:16:29.780]   - To me, it was when I understood what psychology is,
[00:16:29.780 --> 00:16:30.860]   analytical psychology,
[00:16:30.860 --> 00:16:32.780]   like the way it's actually carried out,
[00:16:32.780 --> 00:16:36.220]   it was really disappointing to see two aspects.
[00:16:36.220 --> 00:16:39.180]   One is how small the N is,
[00:16:39.180 --> 00:16:43.040]   how small the number of subject is in the studies.
[00:16:43.040 --> 00:16:45.300]   And two, it was disappointing to see
[00:16:45.300 --> 00:16:49.660]   how controlled the entire, how much it was in the lab.
[00:16:49.660 --> 00:16:52.660]   It wasn't studying humans in the wild.
[00:16:52.660 --> 00:16:54.980]   There was no mechanism for studying humans in the wild.
[00:16:54.980 --> 00:16:57.660]   So that's where I became a little bit disillusioned
[00:16:57.660 --> 00:16:59.480]   to psychology.
[00:16:59.480 --> 00:17:01.700]   And then the modern world of the internet
[00:17:01.700 --> 00:17:02.980]   is so exciting to me,
[00:17:02.980 --> 00:17:05.740]   the Twitter data or YouTube data,
[00:17:05.740 --> 00:17:08.300]   data of human behavior on the internet becomes exciting
[00:17:08.300 --> 00:17:11.940]   because the N grows and then in the wild grows.
[00:17:11.940 --> 00:17:13.860]   But that's just my narrow sense.
[00:17:13.860 --> 00:17:16.580]   Like, do you have a optimistic or pessimistic,
[00:17:16.580 --> 00:17:18.180]   cynical view of psychology?
[00:17:18.180 --> 00:17:19.840]   How do you see the field broadly?
[00:17:19.840 --> 00:17:22.740]   - When I was in graduate school,
[00:17:22.740 --> 00:17:27.740]   it was early enough that there was still a thrill
[00:17:27.740 --> 00:17:32.820]   in seeing that there were ways of doing,
[00:17:32.820 --> 00:17:35.660]   there were ways of doing experimental science
[00:17:35.660 --> 00:17:40.060]   that provided insight to the structure of the mind.
[00:17:40.060 --> 00:17:42.060]   One thing that impressed me most
[00:17:42.060 --> 00:17:44.420]   when I was at that stage in my education
[00:17:44.420 --> 00:17:46.020]   was neuropsychology,
[00:17:46.020 --> 00:17:48.380]   looking at, looking at the,
[00:17:48.380 --> 00:17:51.460]   analyzing the behavior of populations
[00:17:51.460 --> 00:17:55.540]   who had brain damage of different kinds
[00:17:55.540 --> 00:18:00.540]   and trying to understand what the specific deficits were
[00:18:00.540 --> 00:18:06.620]   that arose from a lesion in a particular part of the brain.
[00:18:06.620 --> 00:18:08.940]   And the kind of experimentation that was done
[00:18:08.940 --> 00:18:13.540]   and that's still being done to get answers in that context
[00:18:13.540 --> 00:18:18.160]   was so creative and it was so deliberate.
[00:18:18.160 --> 00:18:20.620]   It was good science.
[00:18:21.340 --> 00:18:24.380]   An experiment answered one question but raised another
[00:18:24.380 --> 00:18:25.580]   and somebody would do an experiment
[00:18:25.580 --> 00:18:26.580]   that answered that question.
[00:18:26.580 --> 00:18:29.340]   And you really felt like you were narrowing in on
[00:18:29.340 --> 00:18:31.740]   some kind of approximate understanding
[00:18:31.740 --> 00:18:34.820]   of what this part of the brain was for.
[00:18:34.820 --> 00:18:36.860]   - Do you have an example from memory
[00:18:36.860 --> 00:18:39.540]   of what kind of aspects of the mind
[00:18:39.540 --> 00:18:41.380]   could be studied in this kind of way?
[00:18:41.380 --> 00:18:42.220]   - Oh, sure.
[00:18:42.220 --> 00:18:45.820]   I mean, the very detailed neuropsychological studies
[00:18:45.820 --> 00:18:49.700]   of language, language function,
[00:18:49.700 --> 00:18:52.020]   looking at production and reception
[00:18:52.020 --> 00:18:54.700]   and the relationship between, you know,
[00:18:54.700 --> 00:18:57.060]   visual function, you know,
[00:18:57.060 --> 00:19:00.100]   reading and auditory and semantic.
[00:19:00.100 --> 00:19:02.860]   And there were these, and still are,
[00:19:02.860 --> 00:19:04.340]   these beautiful models that came out
[00:19:04.340 --> 00:19:07.060]   of that kind of research that really made you feel
[00:19:07.060 --> 00:19:08.420]   like you understood something
[00:19:08.420 --> 00:19:10.300]   that you hadn't understood before
[00:19:10.300 --> 00:19:12.460]   about how, you know,
[00:19:12.460 --> 00:19:15.260]   language processing is organized in the brain.
[00:19:15.260 --> 00:19:17.240]   But having said all that,
[00:19:17.240 --> 00:19:22.240]   you know, I think, you know,
[00:19:22.240 --> 00:19:25.320]   I think you are, I mean, I agree with you
[00:19:25.320 --> 00:19:30.320]   that the cost of doing highly controlled experiments
[00:19:30.320 --> 00:19:35.880]   is that you, by construction, miss out on the richness
[00:19:35.880 --> 00:19:39.080]   and complexity of the real world.
[00:19:39.080 --> 00:19:42.280]   One thing that, so I was drawn into science
[00:19:42.280 --> 00:19:44.920]   by what in those days was called connectionism,
[00:19:44.920 --> 00:19:46.760]   which is of course the, you know,
[00:19:46.760 --> 00:19:48.680]   what we now call deep learning.
[00:19:48.680 --> 00:19:50.840]   And at that point in history,
[00:19:50.840 --> 00:19:54.200]   neural networks were primarily being used
[00:19:54.200 --> 00:19:56.400]   in order to model human cognition.
[00:19:56.400 --> 00:20:00.200]   They weren't yet really useful for industrial applications.
[00:20:00.200 --> 00:20:02.080]   - So you always found neural networks
[00:20:02.080 --> 00:20:04.080]   in biological form beautiful.
[00:20:04.080 --> 00:20:07.160]   - Oh, neural networks were very concretely the thing
[00:20:07.160 --> 00:20:09.120]   that drew me into science.
[00:20:09.120 --> 00:20:13.320]   I was handed, are you familiar with the PDP books
[00:20:13.320 --> 00:20:14.920]   from the '80s?
[00:20:14.920 --> 00:20:16.520]   So when I was in, I went to medical school
[00:20:16.520 --> 00:20:18.240]   before I went into science.
[00:20:18.240 --> 00:20:19.720]   And-- - Really?
[00:20:19.720 --> 00:20:20.800]   - Yeah. - Interesting.
[00:20:20.800 --> 00:20:23.280]   Wow. - I also did a graduate degree
[00:20:23.280 --> 00:20:26.460]   in art history, so I kind of explore it.
[00:20:26.460 --> 00:20:28.560]   - Well, art history, I understand.
[00:20:28.560 --> 00:20:31.280]   That's just a curious, creative mind.
[00:20:31.280 --> 00:20:33.940]   But medical school, with the dream of what?
[00:20:33.940 --> 00:20:35.840]   If we take that slight tangent,
[00:20:35.840 --> 00:20:39.120]   did you want to be a surgeon?
[00:20:39.120 --> 00:20:41.320]   - I actually was quite interested in surgery.
[00:20:41.680 --> 00:20:44.200]   I was interested in surgery and psychiatry.
[00:20:44.200 --> 00:20:47.280]   And I thought, I must be the only person
[00:20:47.280 --> 00:20:52.720]   on the planet who was torn between those two fields.
[00:20:52.720 --> 00:20:56.840]   And I said exactly that to my advisor in medical school,
[00:20:56.840 --> 00:20:59.460]   who turned out, I found out later,
[00:20:59.460 --> 00:21:01.920]   to be a famous psychoanalyst.
[00:21:01.920 --> 00:21:05.120]   And he said to me, "No, no, it's actually not so uncommon
[00:21:05.120 --> 00:21:07.760]   "to be interested in surgery and psychiatry."
[00:21:07.760 --> 00:21:10.480]   And he conjectured that the reason
[00:21:10.480 --> 00:21:12.600]   that people develop these two interests
[00:21:12.600 --> 00:21:15.480]   is that both fields are about going beneath the surface
[00:21:15.480 --> 00:21:19.120]   and kind of getting into the kind of secret.
[00:21:19.120 --> 00:21:20.640]   I mean, maybe you understand this
[00:21:20.640 --> 00:21:22.560]   as someone who was interested in psychoanalysis
[00:21:22.560 --> 00:21:23.440]   at a younger stage.
[00:21:23.440 --> 00:21:26.200]   There's sort of a, there's a cliche phrase
[00:21:26.200 --> 00:21:28.400]   that people use now on NPR,
[00:21:28.400 --> 00:21:31.400]   the secret life of blankety-blank, right?
[00:21:31.400 --> 00:21:33.560]   And that was part of the thrill of surgery,
[00:21:33.560 --> 00:21:38.120]   was seeing the secret activity
[00:21:38.120 --> 00:21:40.560]   that's inside everybody's abdomen and thorax.
[00:21:40.560 --> 00:21:43.880]   - That's a very poetic way to connect it to disciplines
[00:21:43.880 --> 00:21:45.560]   that are very, practically speaking,
[00:21:45.560 --> 00:21:46.400]   different from each other, I think.
[00:21:46.400 --> 00:21:48.480]   - That's for sure, that's for sure, yes.
[00:21:48.480 --> 00:21:52.480]   - So how did we get onto medical school?
[00:21:52.480 --> 00:21:53.740]   - So I was in medical school
[00:21:53.740 --> 00:21:57.360]   and I was doing a psychiatry rotation
[00:21:57.360 --> 00:22:01.000]   and my kind of advisor in that rotation
[00:22:01.000 --> 00:22:04.720]   asked me what I was interested in.
[00:22:04.720 --> 00:22:07.800]   And I said, "Well, maybe psychiatry."
[00:22:07.800 --> 00:22:09.320]   He said, "Why?"
[00:22:09.320 --> 00:22:11.080]   And I said, "Well, I've always been interested
[00:22:11.080 --> 00:22:13.020]   "in how the brain works.
[00:22:13.020 --> 00:22:16.100]   "I'm pretty sure that nobody's doing scientific research
[00:22:16.100 --> 00:22:20.600]   "that addresses my interests, which are,"
[00:22:20.600 --> 00:22:21.920]   I didn't have a word for it then,
[00:22:21.920 --> 00:22:25.000]   but I would have said, "about cognition."
[00:22:25.000 --> 00:22:27.640]   And he said, "Well, you know, I'm not sure that's true.
[00:22:27.640 --> 00:22:29.600]   "You might be interested in these books."
[00:22:29.600 --> 00:22:32.480]   And he pulled down the PDB books from his shelf
[00:22:32.480 --> 00:22:34.000]   and they were still shrink-wrapped.
[00:22:34.000 --> 00:22:36.960]   He hadn't read them, but he handed them to me.
[00:22:36.960 --> 00:22:38.680]   He said, "You feel free to borrow these."
[00:22:38.680 --> 00:22:41.480]   And that was, you know, I went back to my dorm room
[00:22:41.480 --> 00:22:43.520]   and I just, you know, read them cover to cover.
[00:22:43.520 --> 00:22:44.960]   - What's PDP?
[00:22:44.960 --> 00:22:46.520]   - Parallel Distributed Processing,
[00:22:46.520 --> 00:22:50.840]   which was one of the original names for deep learning.
[00:22:50.840 --> 00:22:55.000]   - And so, I apologize for the romanticized question,
[00:22:55.000 --> 00:22:58.360]   but what idea in the space of neuroscience,
[00:22:58.360 --> 00:22:59.880]   in the space of the human brain,
[00:22:59.880 --> 00:23:03.880]   is to you the most beautiful, mysterious, surprising?
[00:23:03.880 --> 00:23:07.160]   - What had always fascinated me,
[00:23:07.160 --> 00:23:12.320]   even when I was a pretty young kid, I think,
[00:23:12.320 --> 00:23:17.320]   was the paradox that lies in the fact
[00:23:17.320 --> 00:23:26.360]   that the brain is so mysterious and seems so distant.
[00:23:26.360 --> 00:23:32.520]   But at the same time,
[00:23:32.520 --> 00:23:37.360]   it's responsible for the full transparency
[00:23:37.360 --> 00:23:39.040]   of everyday life.
[00:23:39.040 --> 00:23:41.520]   The brain is literally what makes everything obvious
[00:23:41.520 --> 00:23:42.880]   and familiar.
[00:23:42.880 --> 00:23:47.280]   And there's always one in the room with you.
[00:23:47.280 --> 00:23:48.120]   - Yeah.
[00:23:48.120 --> 00:23:50.520]   - I used to teach, when I taught at Princeton,
[00:23:50.520 --> 00:23:53.000]   I used to teach a cognitive neuroscience course.
[00:23:53.000 --> 00:23:56.720]   And the very last thing I would say to the students was,
[00:23:56.720 --> 00:24:01.720]   you know, when people think of scientists
[00:24:01.960 --> 00:24:05.760]   as scientific inspiration, the metaphor is often,
[00:24:05.760 --> 00:24:08.040]   well, look to the stars, you know?
[00:24:08.040 --> 00:24:11.600]   The stars will inspire you to wonder at the universe
[00:24:11.600 --> 00:24:16.360]   and think about your place in it and how things work.
[00:24:16.360 --> 00:24:18.320]   And I'm all for looking at the stars,
[00:24:18.320 --> 00:24:21.560]   but I've always been much more inspired.
[00:24:21.560 --> 00:24:25.320]   And my sense of wonder comes from the,
[00:24:25.320 --> 00:24:28.480]   not from the distant, mysterious stars,
[00:24:28.480 --> 00:24:33.480]   but from the extremely intimately close brain.
[00:24:33.480 --> 00:24:35.200]   - Yeah.
[00:24:35.200 --> 00:24:38.600]   - There's something just endlessly fascinating
[00:24:38.600 --> 00:24:40.240]   to me about that.
[00:24:40.240 --> 00:24:45.240]   - Like Jessica said, the one that's close and yet distant,
[00:24:45.240 --> 00:24:47.920]   in terms of our understanding of it,
[00:24:47.920 --> 00:24:52.920]   do you, are you also captivated by the fact
[00:24:52.920 --> 00:24:55.960]   that this very conversation is happening
[00:24:55.960 --> 00:24:57.480]   because two brains are communicating?
[00:24:57.480 --> 00:24:58.560]   - Yes, exactly.
[00:24:58.560 --> 00:25:03.800]   - I guess what I mean is the subjective nature
[00:25:03.800 --> 00:25:06.360]   of the experience, if we can take a small tangent
[00:25:06.360 --> 00:25:10.280]   into the mystical of it, the consciousness,
[00:25:10.280 --> 00:25:13.360]   or when you're saying you're captivated
[00:25:13.360 --> 00:25:15.520]   by the idea of the brain, are you talking
[00:25:15.520 --> 00:25:18.240]   about specifically the mechanism of cognition?
[00:25:18.240 --> 00:25:23.240]   Or are you also just, like, at least for me,
[00:25:23.240 --> 00:25:26.640]   it's almost like paralyzing the beauty and the mystery
[00:25:26.640 --> 00:25:29.520]   of the fact that it creates the entirety of the experience,
[00:25:29.520 --> 00:25:32.920]   not just the reasoning capability, but the experience?
[00:25:32.920 --> 00:25:37.920]   - Well, I definitely resonate with that latter thought.
[00:25:37.920 --> 00:25:43.920]   And I often find discussions of artificial intelligence
[00:25:43.920 --> 00:25:49.120]   to be disappointingly narrow.
[00:25:49.120 --> 00:25:55.760]   Speaking as someone who has always had an interest in art,
[00:25:55.960 --> 00:25:57.360]   - Right, I was just gonna go there,
[00:25:57.360 --> 00:26:00.160]   'cause it sounds like somebody who has an interest in art.
[00:26:00.160 --> 00:26:04.000]   - Yeah, I mean, there are many layers
[00:26:04.000 --> 00:26:08.160]   to full bore human experience.
[00:26:08.160 --> 00:26:11.960]   And in some ways, it's not enough to say,
[00:26:11.960 --> 00:26:14.960]   "Oh, well, don't worry, we're talking about cognition,
[00:26:14.960 --> 00:26:16.240]   but we'll add emotion."
[00:26:16.240 --> 00:26:17.080]   - Yeah.
[00:26:17.080 --> 00:26:22.080]   - There's an incredible scope to what humans go through
[00:26:24.040 --> 00:26:25.360]   in every moment.
[00:26:25.360 --> 00:26:33.440]   And yes, so that's part of what fascinates me,
[00:26:33.440 --> 00:26:37.320]   is that our brains are producing that,
[00:26:37.320 --> 00:26:43.880]   but at the same time, it's so mysterious to us, how?
[00:26:43.880 --> 00:26:44.720]   - Yeah.
[00:26:44.720 --> 00:26:48.000]   - Like, we literally, our brains are literally
[00:26:48.000 --> 00:26:50.200]   in our heads producing this experience.
[00:26:50.200 --> 00:26:51.040]   - Producing the experience.
[00:26:52.040 --> 00:26:55.120]   - And yet, it's so mysterious to us,
[00:26:55.120 --> 00:26:57.680]   and the scientific challenge of getting at
[00:26:57.680 --> 00:27:02.680]   the actual explanation for that is so overwhelming.
[00:27:02.680 --> 00:27:05.600]   That's just, I don't know.
[00:27:05.600 --> 00:27:08.480]   Certain people have fixations on particular questions,
[00:27:08.480 --> 00:27:11.720]   and that's always, that's just always been mine.
[00:27:11.720 --> 00:27:14.040]   - Yeah, I would say the poetry of that is fascinating.
[00:27:14.040 --> 00:27:16.760]   And I'm really interested in natural language as well.
[00:27:16.760 --> 00:27:19.480]   And when you look at artificial intelligence community,
[00:27:19.480 --> 00:27:23.880]   it always saddens me how much,
[00:27:23.880 --> 00:27:26.840]   when you try to create a benchmark for the community
[00:27:26.840 --> 00:27:30.280]   to gather around, how much of the magic of language
[00:27:30.280 --> 00:27:33.240]   is lost when you create that benchmark.
[00:27:33.240 --> 00:27:35.920]   That there's something, we talk about experience,
[00:27:35.920 --> 00:27:38.600]   the music, the language, the wit,
[00:27:38.600 --> 00:27:41.080]   the something that makes a rich experience,
[00:27:41.080 --> 00:27:43.800]   something that would be required to pass
[00:27:43.800 --> 00:27:47.660]   the spirit of the Turing test is lost in these benchmarks.
[00:27:47.660 --> 00:27:50.240]   And I wonder how to get it back in,
[00:27:50.240 --> 00:27:51.920]   'cause it's very difficult.
[00:27:51.920 --> 00:27:55.160]   The moment you try to do real good rigorous science,
[00:27:55.160 --> 00:27:57.000]   you lose some of that magic.
[00:27:57.000 --> 00:28:00.200]   When you try to study cognition
[00:28:00.200 --> 00:28:01.600]   in a rigorous scientific way,
[00:28:01.600 --> 00:28:03.840]   it feels like you're losing some of the magic.
[00:28:03.840 --> 00:28:04.680]   - Mm-hmm, mm-hmm.
[00:28:04.680 --> 00:28:07.560]   - The seeing cognition in a mechanistic way
[00:28:07.560 --> 00:28:10.080]   that AI, at this stage in our history, okay.
[00:28:10.080 --> 00:28:13.040]   - Well, I agree with you, but at the same time,
[00:28:13.740 --> 00:28:18.100]   one thing that I found really exciting
[00:28:18.100 --> 00:28:23.020]   about that first wave of deep learning models in cognition
[00:28:23.020 --> 00:28:28.020]   was the fact that the people who were building these models
[00:28:28.020 --> 00:28:33.020]   were focused on the richness and complexity
[00:28:33.020 --> 00:28:34.860]   of human cognition.
[00:28:34.860 --> 00:28:39.860]   So an early debate in cognitive science,
[00:28:39.860 --> 00:28:41.900]   which I sort of witnessed as a grad student,
[00:28:41.900 --> 00:28:44.260]   was about something that sounds very dry,
[00:28:44.260 --> 00:28:47.260]   which is the formation of the past tense.
[00:28:47.260 --> 00:28:49.260]   But there were these two camps.
[00:28:49.260 --> 00:28:54.260]   One said, well, the mind encodes certain rules,
[00:28:54.260 --> 00:28:57.960]   and it also has a list of exceptions,
[00:28:57.960 --> 00:29:00.460]   because of course, the rule is add E-D,
[00:29:00.460 --> 00:29:01.900]   but that's not always what you do,
[00:29:01.900 --> 00:29:03.900]   so you have to have a list of exceptions.
[00:29:03.900 --> 00:29:07.020]   And then there were the connectionists
[00:29:07.020 --> 00:29:10.420]   who evolved into the deep learning people,
[00:29:10.420 --> 00:29:13.860]   who said, well, if you look carefully at the data,
[00:29:13.860 --> 00:29:18.660]   if you actually look at corpora, like language corpora,
[00:29:18.660 --> 00:29:20.140]   it turns out to be very rich,
[00:29:20.140 --> 00:29:25.140]   because yes, there are most verbs,
[00:29:25.140 --> 00:29:28.680]   and you just tack on E-D, and then there are exceptions,
[00:29:28.680 --> 00:29:32.420]   but there are rules that,
[00:29:32.420 --> 00:29:35.540]   the exceptions aren't just random.
[00:29:36.140 --> 00:29:41.140]   There are certain clues to which verbs should be exceptional,
[00:29:41.140 --> 00:29:44.100]   and then there are exceptions to the exceptions,
[00:29:44.100 --> 00:29:47.720]   and there was a word that was kind of deployed
[00:29:47.720 --> 00:29:51.720]   in order to capture this, which was quasi-regular.
[00:29:51.720 --> 00:29:54.700]   In other words, there are rules, but it's messy,
[00:29:54.700 --> 00:29:58.740]   and there's structure even among the exceptions,
[00:29:58.740 --> 00:30:03.180]   and it would be, yeah, you could try to write down
[00:30:03.180 --> 00:30:04.820]   the structure in some sort of closed form,
[00:30:04.820 --> 00:30:07.580]   but really, the right way to understand
[00:30:07.580 --> 00:30:09.060]   how the brain is handling all this,
[00:30:09.060 --> 00:30:11.460]   and by the way, producing all of this,
[00:30:11.460 --> 00:30:14.000]   is to build a deep neural network
[00:30:14.000 --> 00:30:15.220]   and train it on this data
[00:30:15.220 --> 00:30:18.520]   and see how it ends up representing all of this richness.
[00:30:18.520 --> 00:30:21.420]   So the way that deep learning
[00:30:21.420 --> 00:30:23.720]   was deployed in cognitive psychology
[00:30:23.720 --> 00:30:25.940]   was that was the spirit of it.
[00:30:25.940 --> 00:30:28.060]   It was about that richness,
[00:30:28.060 --> 00:30:31.940]   and that's something that I always found very compelling.
[00:30:31.940 --> 00:30:33.140]   Still do.
[00:30:33.140 --> 00:30:36.180]   - Is there something especially interesting
[00:30:36.180 --> 00:30:39.780]   and profound to you in terms of our current deep learning
[00:30:39.780 --> 00:30:42.620]   neural network, artificial neural network approaches,
[00:30:42.620 --> 00:30:46.300]   and whatever we do understand
[00:30:46.300 --> 00:30:48.780]   about the biological neural networks in our brain?
[00:30:48.780 --> 00:30:52.420]   There's quite a few differences.
[00:30:52.420 --> 00:30:55.940]   Are some of them to you either interesting
[00:30:55.940 --> 00:30:59.940]   or perhaps profound in terms of the gap
[00:31:02.180 --> 00:31:04.500]   we might want to try to close
[00:31:04.500 --> 00:31:07.580]   in trying to create a human-level intelligence?
[00:31:07.580 --> 00:31:08.820]   - What I would say here is something
[00:31:08.820 --> 00:31:10.740]   that a lot of people are saying,
[00:31:10.740 --> 00:31:15.740]   which is that one seeming limitation
[00:31:15.740 --> 00:31:18.940]   of the systems that we're building now
[00:31:18.940 --> 00:31:21.840]   is that they lack the kind of flexibility,
[00:31:21.840 --> 00:31:25.980]   the readiness to sort of turn on a dime
[00:31:25.980 --> 00:31:28.220]   when the context calls for it
[00:31:28.220 --> 00:31:32.180]   that is so characteristic of human behavior.
[00:31:32.180 --> 00:31:33.020]   So--
[00:31:33.020 --> 00:31:34.900]   - Is that connected to you to the,
[00:31:34.900 --> 00:31:37.740]   like which aspect of the neural networks in our brain
[00:31:37.740 --> 00:31:39.140]   is that connected to?
[00:31:39.140 --> 00:31:42.820]   Is that closer to the cognitive science level of...
[00:31:42.820 --> 00:31:47.340]   Now again, see, like my natural inclination
[00:31:47.340 --> 00:31:49.620]   is to separate into three disciplines
[00:31:49.620 --> 00:31:54.260]   of neuroscience, cognitive science, and psychology,
[00:31:54.260 --> 00:31:56.380]   and you've already kind of shut that down
[00:31:56.380 --> 00:31:58.380]   by saying you're kind of seeing them as separate,
[00:31:58.380 --> 00:32:02.020]   but just to look at those layers, I guess,
[00:32:02.020 --> 00:32:05.300]   where is there something about the lowest layer
[00:32:05.300 --> 00:32:09.140]   of the way the neurons interact
[00:32:09.140 --> 00:32:13.340]   that is profound to you in terms of its difference
[00:32:13.340 --> 00:32:15.460]   to the artificial neural networks?
[00:32:15.460 --> 00:32:17.220]   Or is all the key differences
[00:32:17.220 --> 00:32:19.240]   at a higher level of abstraction?
[00:32:19.240 --> 00:32:22.700]   - One thing I often think about is that,
[00:32:24.420 --> 00:32:27.100]   if you take an introductory computer science course
[00:32:27.100 --> 00:32:28.980]   and they are introducing you
[00:32:28.980 --> 00:32:31.420]   to the notion of Turing machines,
[00:32:31.420 --> 00:32:36.420]   one way of articulating what the significance
[00:32:36.420 --> 00:32:41.540]   of a Turing machine is, is that it's a machine emulator.
[00:32:41.540 --> 00:32:45.240]   It can emulate any other machine.
[00:32:45.240 --> 00:32:50.420]   And that to me,
[00:32:52.900 --> 00:32:54.900]   that way of looking at a Turing machine
[00:32:54.900 --> 00:32:57.580]   really sticks with me.
[00:32:57.580 --> 00:33:01.900]   I think of humans as maybe sharing
[00:33:01.900 --> 00:33:04.940]   in some of that character.
[00:33:04.940 --> 00:33:06.980]   We're capacity limited, we're not Turing machines,
[00:33:06.980 --> 00:33:10.980]   obviously, but we have the ability to adapt behaviors
[00:33:10.980 --> 00:33:15.340]   that are very much unlike anything we've done before,
[00:33:15.340 --> 00:33:17.660]   but there's some basic mechanism
[00:33:17.660 --> 00:33:18.900]   that's implemented in our brain
[00:33:18.900 --> 00:33:22.340]   that allows us to run software.
[00:33:22.340 --> 00:33:24.580]   - But just on that point, you mentioned Turing machine,
[00:33:24.580 --> 00:33:27.340]   but nevertheless, it's fundamentally our brains
[00:33:27.340 --> 00:33:29.700]   are just computational devices in your view?
[00:33:29.700 --> 00:33:32.060]   Is that what you're getting at?
[00:33:32.060 --> 00:33:35.620]   It was a little bit unclear to this line you drew.
[00:33:35.620 --> 00:33:37.780]   Is there any magic in there
[00:33:37.780 --> 00:33:40.660]   or is it just basic computation?
[00:33:40.660 --> 00:33:43.300]   - I'm happy to think of it as just basic computation,
[00:33:43.300 --> 00:33:46.100]   but mind you, I won't be satisfied
[00:33:46.100 --> 00:33:47.460]   until somebody explains to me
[00:33:47.460 --> 00:33:49.780]   what the basic computations are
[00:33:49.820 --> 00:33:53.860]   that are leading to the full richness of human cognition.
[00:33:53.860 --> 00:33:54.700]   - Yes.
[00:33:54.700 --> 00:33:56.660]   - I mean, it's not gonna be enough for me
[00:33:56.660 --> 00:33:58.860]   to understand what the computations are
[00:33:58.860 --> 00:34:02.180]   that allow people to do arithmetic or play chess.
[00:34:02.180 --> 00:34:06.340]   I want the whole thing.
[00:34:06.340 --> 00:34:07.780]   - And a small tangent,
[00:34:07.780 --> 00:34:10.500]   because you kind of mentioned coronavirus,
[00:34:10.500 --> 00:34:12.420]   there's group behavior.
[00:34:12.420 --> 00:34:13.460]   - Oh, sure.
[00:34:13.460 --> 00:34:14.940]   - Is there something interesting
[00:34:14.940 --> 00:34:17.660]   to your search of understanding the human mind
[00:34:18.700 --> 00:34:21.540]   where behavior of large groups
[00:34:21.540 --> 00:34:24.260]   or just behavior of groups is interesting?
[00:34:24.260 --> 00:34:25.620]   Seeing that as a collective mind,
[00:34:25.620 --> 00:34:27.180]   as a collective intelligence,
[00:34:27.180 --> 00:34:28.940]   perhaps seeing the groups of people
[00:34:28.940 --> 00:34:31.100]   as a single intelligent organisms,
[00:34:31.100 --> 00:34:34.220]   especially looking at the reinforcement learning work
[00:34:34.220 --> 00:34:35.660]   you've done recently.
[00:34:35.660 --> 00:34:37.340]   - Well, yeah, I can't, I mean,
[00:34:37.340 --> 00:34:41.820]   I have the honor of working
[00:34:41.820 --> 00:34:43.700]   with a lot of incredibly smart people
[00:34:43.700 --> 00:34:45.500]   and I wouldn't wanna take any credit
[00:34:46.140 --> 00:34:48.820]   for leading the way on the multi-agent work
[00:34:48.820 --> 00:34:51.380]   that's come out of my group or DeepMind lately,
[00:34:51.380 --> 00:34:53.860]   but I do find it fascinating.
[00:34:53.860 --> 00:34:57.340]   And I mean, I think there,
[00:34:57.340 --> 00:35:01.540]   I think it can't be debated.
[00:35:01.540 --> 00:35:06.020]   The human behavior arises within communities.
[00:35:06.020 --> 00:35:08.940]   That just seems to me self-evident.
[00:35:08.940 --> 00:35:11.380]   - But to me, it is self-evident,
[00:35:11.380 --> 00:35:14.700]   but that seems to be a profound aspects
[00:35:14.700 --> 00:35:16.060]   of something that created.
[00:35:16.060 --> 00:35:19.180]   That was like, if you look at like 2001 Space Odyssey
[00:35:19.180 --> 00:35:21.980]   when the monkeys touched the,
[00:35:21.980 --> 00:35:23.420]   like that's the magical moment.
[00:35:23.420 --> 00:35:26.660]   I think Yuval Harari argues that the ability
[00:35:26.660 --> 00:35:30.220]   of our large numbers of humans to hold an idea,
[00:35:30.220 --> 00:35:31.900]   to converge towards idea together,
[00:35:31.900 --> 00:35:34.380]   like you said, shaking hands versus bumping elbows,
[00:35:34.380 --> 00:35:38.340]   somehow converge like without even like,
[00:35:38.340 --> 00:35:40.860]   like without being in a room altogether,
[00:35:40.860 --> 00:35:43.380]   just kind of this like distributed convergence
[00:35:43.380 --> 00:35:46.700]   towards an idea over a particular period of time
[00:35:46.700 --> 00:35:51.500]   seems to be fundamental to just every aspect
[00:35:51.500 --> 00:35:53.420]   of our cognition of our intelligence,
[00:35:53.420 --> 00:35:56.740]   because humans, we'll talk about reward,
[00:35:56.740 --> 00:35:58.700]   but it seems like we don't really have
[00:35:58.700 --> 00:36:01.340]   a clear objective function under which we operate,
[00:36:01.340 --> 00:36:04.140]   but we all kind of converge towards one somehow.
[00:36:04.140 --> 00:36:06.740]   And that to me has always been a mystery
[00:36:06.740 --> 00:36:09.820]   that I think is somehow productive
[00:36:09.820 --> 00:36:13.620]   for also understanding AI systems.
[00:36:13.620 --> 00:36:16.540]   But I guess that's the next step.
[00:36:16.540 --> 00:36:18.780]   The first step is try to understand the mind.
[00:36:18.780 --> 00:36:19.700]   - Well, I don't know.
[00:36:19.700 --> 00:36:22.540]   I mean, I think there's something to the argument
[00:36:22.540 --> 00:36:25.780]   that that kind of bottom,
[00:36:25.780 --> 00:36:29.940]   like strictly bottom-up approach is wrong-headed.
[00:36:29.940 --> 00:36:34.940]   In other words, there are basic phenomena that,
[00:36:34.940 --> 00:36:36.860]   basic aspects of human intelligence
[00:36:36.860 --> 00:36:41.860]   that can only be understood in the context of groups.
[00:36:41.860 --> 00:36:44.700]   I'm perfectly open to that.
[00:36:44.700 --> 00:36:48.700]   I've never been particularly convinced by the notion
[00:36:48.700 --> 00:36:53.700]   that we should consider intelligence to adhere
[00:36:53.700 --> 00:36:55.620]   at the level of communities.
[00:36:55.620 --> 00:36:57.180]   I don't know why.
[00:36:57.180 --> 00:36:58.740]   I just, I'm sort of stuck on the notion
[00:36:58.740 --> 00:37:01.380]   that the basic unit that we want to understand
[00:37:01.380 --> 00:37:02.700]   is individual humans.
[00:37:02.700 --> 00:37:05.860]   And if we have to understand that
[00:37:05.860 --> 00:37:07.700]   in the context of other humans, fine.
[00:37:07.700 --> 00:37:11.300]   But for me, intelligence is just,
[00:37:11.300 --> 00:37:14.660]   I'm stubbornly, I stubbornly define it as something
[00:37:14.660 --> 00:37:18.820]   that is an aspect of an individual human.
[00:37:18.820 --> 00:37:20.180]   That's just my, I don't know if that's my take.
[00:37:20.180 --> 00:37:22.860]   - I'm with you, but that could be the reductionist dream
[00:37:22.860 --> 00:37:26.420]   of a scientist because you can understand a single human.
[00:37:26.420 --> 00:37:30.760]   It also is very possible that intelligence can only arise
[00:37:30.760 --> 00:37:32.780]   when there's multiple intelligences.
[00:37:32.860 --> 00:37:37.500]   When there's multiple sort of, it's a sad thing,
[00:37:37.500 --> 00:37:39.900]   if that's true, because it's very difficult to study.
[00:37:39.900 --> 00:37:42.500]   But if it's just one human,
[00:37:42.500 --> 00:37:44.900]   that one human would not be homo sapien,
[00:37:44.900 --> 00:37:46.540]   would not become that intelligent.
[00:37:46.540 --> 00:37:48.540]   That's a real, that's a possibility.
[00:37:48.540 --> 00:37:49.860]   - I'm with you.
[00:37:49.860 --> 00:37:52.860]   One thing I will say along these lines
[00:37:52.860 --> 00:37:55.500]   is that I think,
[00:37:58.580 --> 00:38:03.420]   I think a serious effort to understand human intelligence
[00:38:03.420 --> 00:38:09.700]   and maybe to build a human-like intelligence
[00:38:09.700 --> 00:38:11.860]   needs to pay just as much attention
[00:38:11.860 --> 00:38:14.020]   to the structure of the environment
[00:38:14.020 --> 00:38:17.660]   as to the structure of the, you know,
[00:38:17.660 --> 00:38:22.200]   the cognizing system, whether it's a brain or an AI system.
[00:38:22.200 --> 00:38:26.780]   That's one thing I took away actually from my early studies
[00:38:26.780 --> 00:38:29.900]   with the pioneers of neural network research,
[00:38:29.900 --> 00:38:32.220]   people like Jay McClelland and John Cohen.
[00:38:32.220 --> 00:38:38.580]   The structure of cognition is really,
[00:38:38.580 --> 00:38:42.820]   it's only partly a function of the, you know,
[00:38:42.820 --> 00:38:44.500]   the architecture of the brain
[00:38:44.500 --> 00:38:46.980]   and the learning algorithms that it implements.
[00:38:46.980 --> 00:38:48.260]   What it's really a function,
[00:38:48.260 --> 00:38:51.500]   what really shapes it is the interaction of those things
[00:38:51.500 --> 00:38:54.460]   with the structure of the world
[00:38:54.460 --> 00:38:56.680]   in which those things are embedded, right?
[00:38:56.680 --> 00:38:58.300]   - And that's especially important for,
[00:38:58.300 --> 00:39:00.880]   that's made most clear in reinforcement learning
[00:39:00.880 --> 00:39:03.700]   where a simulated environment is,
[00:39:03.700 --> 00:39:05.820]   you can only learn as much as you can simulate,
[00:39:05.820 --> 00:39:09.340]   and that's what made, what DeepMind made very clear
[00:39:09.340 --> 00:39:11.080]   with the other aspect of the environment,
[00:39:11.080 --> 00:39:13.580]   which is the self-play mechanism
[00:39:13.580 --> 00:39:16.840]   of the other agent of the competitive behavior,
[00:39:16.840 --> 00:39:19.980]   which the other agent becomes the environment essentially.
[00:39:19.980 --> 00:39:24.060]   And that's, I mean, one of the most exciting ideas in AI
[00:39:24.060 --> 00:39:25.720]   is the self-play mechanism
[00:39:25.720 --> 00:39:27.920]   that's able to learn successfully.
[00:39:27.920 --> 00:39:28.760]   So there you go.
[00:39:28.760 --> 00:39:33.440]   There's a thing where competition is essential for learning,
[00:39:33.440 --> 00:39:35.040]   at least in that context.
[00:39:35.040 --> 00:39:37.960]   So if we can step back into another sort of beautiful world,
[00:39:37.960 --> 00:39:42.040]   which is the actual mechanics,
[00:39:42.040 --> 00:39:44.680]   the dirty mess of it, of the human brain,
[00:39:44.680 --> 00:39:48.520]   is there something for people who might not know,
[00:39:48.520 --> 00:39:51.120]   is there something you can comment on
[00:39:51.120 --> 00:39:53.960]   or describe the key parts of the brain
[00:39:53.960 --> 00:39:55.520]   that are important for intelligence,
[00:39:55.520 --> 00:39:58.260]   or just in general, what are the different parts
[00:39:58.260 --> 00:39:59.900]   of the brain that you're curious about,
[00:39:59.900 --> 00:40:03.940]   that you've studied, and that are just good to know about
[00:40:03.940 --> 00:40:06.300]   when you're thinking about cognition?
[00:40:06.300 --> 00:40:11.260]   - Well, my area of expertise, if I have one,
[00:40:11.260 --> 00:40:14.260]   is prefrontal cortex.
[00:40:14.260 --> 00:40:16.560]   So-- - What's that?
[00:40:16.560 --> 00:40:17.780]   (laughing)
[00:40:17.780 --> 00:40:19.580]   Where do we-- - It depends on who you ask.
[00:40:19.580 --> 00:40:24.580]   The technical definition is anatomical.
[00:40:24.580 --> 00:40:30.640]   There are parts of your brain
[00:40:30.640 --> 00:40:32.500]   that are responsible for motor behavior,
[00:40:32.500 --> 00:40:34.640]   and they're very easy to identify.
[00:40:34.640 --> 00:40:40.760]   And the region of your cerebral cortex,
[00:40:40.760 --> 00:40:43.960]   the sort of outer crust of your brain
[00:40:43.960 --> 00:40:46.460]   that lies in front of those
[00:40:46.460 --> 00:40:49.360]   is defined as the prefrontal cortex.
[00:40:49.360 --> 00:40:51.960]   - And when you say anatomical, sorry to interrupt,
[00:40:51.960 --> 00:40:56.960]   so that's referring to sort of the geographic region,
[00:40:56.960 --> 00:41:00.160]   as opposed to some kind of functional definition.
[00:41:00.160 --> 00:41:04.420]   - Exactly, so this is kind of the coward's way out.
[00:41:04.420 --> 00:41:06.020]   I'm telling you what the prefrontal cortex is
[00:41:06.020 --> 00:41:09.680]   just in terms of what part of the real estate it occupies.
[00:41:09.680 --> 00:41:10.720]   - The thing in the front of the brain.
[00:41:10.720 --> 00:41:11.720]   - Yeah, exactly.
[00:41:11.720 --> 00:41:16.720]   And in fact, the early history of neuroscientific research
[00:41:16.720 --> 00:41:20.880]   of neuroscientific investigation
[00:41:20.880 --> 00:41:23.520]   of what this front part of the brain does
[00:41:23.520 --> 00:41:26.400]   is sort of funny to read because
[00:41:26.400 --> 00:41:33.880]   it was really World War I that started people
[00:41:33.880 --> 00:41:35.520]   down this road of trying to figure out
[00:41:35.520 --> 00:41:38.880]   what different parts of the brain, the human brain do
[00:41:38.880 --> 00:41:41.640]   in the sense that there were a lot of people
[00:41:41.640 --> 00:41:43.800]   with brain damage who came back from the war
[00:41:43.800 --> 00:41:44.800]   with brain damage.
[00:41:44.800 --> 00:41:47.720]   And that provided, as tragic as that was,
[00:41:47.720 --> 00:41:49.920]   it provided an opportunity for scientists
[00:41:49.920 --> 00:41:53.520]   to try to identify the functions of different brain regions.
[00:41:53.520 --> 00:41:56.160]   And that was actually incredibly productive.
[00:41:56.160 --> 00:41:59.440]   But one of the frustrations that neuropsychologists faced
[00:41:59.440 --> 00:42:02.120]   was they couldn't really identify exactly
[00:42:02.120 --> 00:42:05.000]   what the deficit was that arose from damage
[00:42:05.000 --> 00:42:08.420]   to these most kind of frontal parts of the brain.
[00:42:08.420 --> 00:42:13.420]   It was just a very difficult thing to pin down.
[00:42:13.640 --> 00:42:16.040]   There were a couple of neuropsychologists
[00:42:16.040 --> 00:42:20.560]   who identified through a large amount
[00:42:20.560 --> 00:42:22.960]   of clinical experience and close observation,
[00:42:22.960 --> 00:42:26.200]   they started to put their finger on a syndrome
[00:42:26.200 --> 00:42:27.640]   that was associated with frontal damage.
[00:42:27.640 --> 00:42:30.440]   Actually, one of them was a Russian neuropsychologist
[00:42:30.440 --> 00:42:35.100]   named Luria, who students of cognitive psychology still read.
[00:42:35.100 --> 00:42:40.200]   And what he started to figure out
[00:42:40.200 --> 00:42:43.560]   was that the frontal cortex was somehow involved
[00:42:43.560 --> 00:42:48.560]   in flexibility, in guiding behaviors
[00:42:48.560 --> 00:42:54.080]   that required someone to override a habit
[00:42:54.080 --> 00:42:57.560]   or to do something unusual,
[00:42:57.560 --> 00:43:01.000]   or to change what they were doing in a very flexible way
[00:43:01.000 --> 00:43:02.520]   from one moment to another.
[00:43:02.520 --> 00:43:05.040]   - So focused on like new experiences.
[00:43:05.040 --> 00:43:08.760]   And so the way your brain processes
[00:43:08.760 --> 00:43:10.900]   and acts in new experiences.
[00:43:10.900 --> 00:43:14.700]   - Yeah, what later helped bring this function
[00:43:14.700 --> 00:43:17.180]   into better focus was a distinction
[00:43:17.180 --> 00:43:19.940]   between controlled and automatic behavior.
[00:43:19.940 --> 00:43:23.660]   In other literatures, this is referred to
[00:43:23.660 --> 00:43:28.100]   as habitual behavior versus goal-directed behavior.
[00:43:28.100 --> 00:43:33.100]   So it's very, very clear that the human brain
[00:43:33.100 --> 00:43:36.580]   has pathways that are dedicated to habits,
[00:43:36.580 --> 00:43:39.340]   to things that you do all the time.
[00:43:39.340 --> 00:43:42.420]   And they need to be automatized
[00:43:42.420 --> 00:43:45.100]   so that they don't require you to concentrate too much.
[00:43:45.100 --> 00:43:47.820]   So that leaves your cognitive capacity
[00:43:47.820 --> 00:43:49.780]   for you to do other things.
[00:43:49.780 --> 00:43:54.700]   Just think about the difference between driving
[00:43:54.700 --> 00:43:55.960]   when you're learning to drive
[00:43:55.960 --> 00:43:59.180]   versus driving after you're fairly expert.
[00:43:59.180 --> 00:44:03.540]   There are brain pathways that slowly absorb
[00:44:03.540 --> 00:44:07.820]   those frequently performed behaviors
[00:44:07.820 --> 00:44:12.340]   so that they can be habits, so that they can be automatic.
[00:44:12.340 --> 00:44:14.900]   - That's kind of like the purest form of learning,
[00:44:14.900 --> 00:44:18.380]   I guess, is happening there, which is why,
[00:44:18.380 --> 00:44:20.020]   I mean, this is kind of jumping ahead,
[00:44:20.020 --> 00:44:22.160]   which is why that perhaps is the most useful
[00:44:22.160 --> 00:44:24.120]   for us to focusing on and trying to see
[00:44:24.120 --> 00:44:27.380]   how artificial intelligence systems can learn.
[00:44:27.380 --> 00:44:28.300]   Is that the way you think? - It's interesting.
[00:44:28.300 --> 00:44:30.060]   I do think about this distinction
[00:44:30.060 --> 00:44:31.460]   between controlled and automatic,
[00:44:31.460 --> 00:44:34.620]   or goal-directed and habitual behavior a lot
[00:44:34.620 --> 00:44:38.680]   in thinking about where we are in AI research.
[00:44:38.680 --> 00:44:46.500]   But just to finish the kind of dissertation here,
[00:44:46.500 --> 00:44:51.380]   the role of the prefrontal cortex
[00:44:51.380 --> 00:44:54.620]   is generally understood these days
[00:44:54.620 --> 00:44:59.620]   sort of in contradistinction to that habitual domain.
[00:44:59.620 --> 00:45:02.320]   In other words, the prefrontal cortex
[00:45:02.320 --> 00:45:05.840]   is what helps you override those habits.
[00:45:05.840 --> 00:45:07.360]   It's what allows you to say,
[00:45:07.360 --> 00:45:10.720]   "Whoa, whoa, what I usually do in this situation is X,
[00:45:10.720 --> 00:45:14.160]   "but given the context, I probably should do Y."
[00:45:14.160 --> 00:45:18.080]   I mean, the elbow bump is a great example, right?
[00:45:18.080 --> 00:45:19.300]   Reaching out and shaking hands
[00:45:19.300 --> 00:45:22.520]   is probably a habitual behavior,
[00:45:22.520 --> 00:45:26.000]   and it's the prefrontal cortex that allows us
[00:45:26.000 --> 00:45:28.760]   to bear in mind that there's something unusual
[00:45:28.760 --> 00:45:31.360]   going on right now, and in this situation,
[00:45:31.360 --> 00:45:33.440]   I need to not do the usual thing.
[00:45:33.440 --> 00:45:38.560]   The kind of behaviors that Luria reported,
[00:45:38.560 --> 00:45:42.020]   and he built tests for detecting these kinds of things,
[00:45:42.020 --> 00:45:43.440]   were exactly like this.
[00:45:43.440 --> 00:45:46.640]   So in other words, when I stick out my hand,
[00:45:46.640 --> 00:45:49.740]   I want you instead to present your elbow.
[00:45:49.740 --> 00:45:51.080]   A patient with frontal damage
[00:45:51.080 --> 00:45:53.520]   would have a great deal of trouble with that.
[00:45:53.520 --> 00:45:57.760]   Somebody proffering their hand would elicit a handshake.
[00:45:57.760 --> 00:46:00.840]   The prefrontal cortex is what allows us to say,
[00:46:00.840 --> 00:46:03.720]   "Hold on, hold on, that's the usual thing,
[00:46:03.720 --> 00:46:07.000]   "but I have the ability to bear in mind
[00:46:07.000 --> 00:46:09.180]   "even very unusual contexts,
[00:46:09.180 --> 00:46:13.240]   "and to reason about what behavior is appropriate there."
[00:46:13.240 --> 00:46:17.560]   - Just to get a sense, is us humans special
[00:46:17.560 --> 00:46:20.680]   in the presence of the prefrontal cortex?
[00:46:20.680 --> 00:46:22.640]   Do mice have a prefrontal cortex?
[00:46:22.640 --> 00:46:25.420]   Do other mammals that we can study?
[00:46:25.420 --> 00:46:30.040]   If no, then how do they integrate new experiences?
[00:46:30.040 --> 00:46:33.760]   - Yeah, that's a really tricky question,
[00:46:33.760 --> 00:46:35.920]   and a very timely question,
[00:46:35.920 --> 00:46:41.840]   because we have revolutionary new technologies
[00:46:41.840 --> 00:46:48.280]   for monitoring, measuring,
[00:46:48.280 --> 00:46:52.040]   and also causally influencing neural behavior
[00:46:52.040 --> 00:46:57.000]   in mice and fruit flies.
[00:46:57.000 --> 00:47:00.640]   And these techniques are not fully available
[00:47:00.640 --> 00:47:05.640]   even for studying brain function in monkeys,
[00:47:05.640 --> 00:47:07.240]   let alone humans.
[00:47:07.240 --> 00:47:11.320]   And so it's a very,
[00:47:11.320 --> 00:47:14.040]   sort of, for me at least, a very urgent question
[00:47:14.040 --> 00:47:16.960]   whether the kinds of things that we wanna understand
[00:47:16.960 --> 00:47:20.080]   about human intelligence can be pursued
[00:47:20.080 --> 00:47:21.960]   in these other organisms.
[00:47:21.960 --> 00:47:26.960]   And to put it briefly, there's disagreement.
[00:47:27.880 --> 00:47:32.880]   You know, people who study fruit flies will often tell you,
[00:47:32.880 --> 00:47:35.160]   "Hey, fruit flies are smarter than you think."
[00:47:35.160 --> 00:47:36.880]   And they'll point to experiments
[00:47:36.880 --> 00:47:40.320]   where fruit flies were able to learn new behaviors,
[00:47:40.320 --> 00:47:44.180]   were able to generalize from one stimulus to another
[00:47:44.180 --> 00:47:47.520]   in a way that suggests that they have abstractions
[00:47:47.520 --> 00:47:49.340]   that guide their generalization.
[00:47:49.340 --> 00:47:54.600]   I've had many conversations in which
[00:47:54.600 --> 00:47:58.160]   I will start by observing, you know,
[00:47:58.160 --> 00:48:03.160]   recounting some observation about mouse behavior,
[00:48:03.160 --> 00:48:09.040]   where it seemed like mice were taking an awfully long time
[00:48:09.040 --> 00:48:13.640]   to learn a task that for a human would be profoundly trivial.
[00:48:13.640 --> 00:48:16.440]   And I will conclude from that
[00:48:16.440 --> 00:48:18.800]   that mice really don't have the cognitive flexibility
[00:48:18.800 --> 00:48:20.100]   that we want to explain.
[00:48:20.100 --> 00:48:21.680]   And then a mouse researcher will say to me,
[00:48:21.680 --> 00:48:23.360]   "Well, you know, hold on.
[00:48:24.360 --> 00:48:26.200]   "That experiment may not have worked
[00:48:26.200 --> 00:48:31.120]   "because you asked a mouse to deal with stimuli
[00:48:31.120 --> 00:48:34.120]   "and behaviors that were very unnatural for the mouse.
[00:48:34.120 --> 00:48:38.600]   "If instead you kept the logic of the experiment the same,
[00:48:38.600 --> 00:48:41.740]   "but kind of put it in a, you know,
[00:48:41.740 --> 00:48:44.280]   "presented the information in a way
[00:48:44.280 --> 00:48:46.700]   "that aligns with what mice are used to dealing with
[00:48:46.700 --> 00:48:48.320]   "in their natural habitats,
[00:48:48.320 --> 00:48:50.900]   "you might find that a mouse actually has more intelligence
[00:48:50.900 --> 00:48:52.400]   "than you think."
[00:48:52.400 --> 00:48:54.960]   And then they'll go on to show you videos
[00:48:54.960 --> 00:48:57.460]   of mice doing things in their natural habitat,
[00:48:57.460 --> 00:49:00.040]   which seem strikingly intelligent, you know,
[00:49:00.040 --> 00:49:02.960]   dealing with, you know, physical problems, you know,
[00:49:02.960 --> 00:49:06.200]   I have to drag this piece of food back to my, you know,
[00:49:06.200 --> 00:49:08.600]   back to my lair, but there's something in my way
[00:49:08.600 --> 00:49:10.440]   and how do I get rid of that thing?
[00:49:10.440 --> 00:49:13.960]   So I think these are open questions to put it,
[00:49:13.960 --> 00:49:15.440]   you know, to sum that up.
[00:49:15.440 --> 00:49:18.560]   - And then taking a small step back related to that
[00:49:18.560 --> 00:49:21.480]   is you kind of mentioned we're taking a little shortcut
[00:49:21.480 --> 00:49:25.240]   by saying it's a geographic part of the,
[00:49:25.240 --> 00:49:28.320]   the prefrontal cortex is a region of the brain,
[00:49:28.320 --> 00:49:33.320]   but if we, what's your sense in a bigger philosophical view,
[00:49:33.320 --> 00:49:36.300]   prefrontal cortex and the brain in general?
[00:49:36.300 --> 00:49:38.880]   Do you have a sense that it's a set of subsystems
[00:49:38.880 --> 00:49:41.240]   in the way we've kind of implied
[00:49:41.240 --> 00:49:43.800]   that are pretty distinct?
[00:49:43.800 --> 00:49:46.360]   Or to what degree is it that,
[00:49:46.360 --> 00:49:49.560]   or to what degree is it a giant interconnected mess
[00:49:49.560 --> 00:49:51.440]   where everything kind of does everything
[00:49:51.440 --> 00:49:53.920]   and it's impossible to disentangle them?
[00:49:53.920 --> 00:49:57.080]   - I think there's overwhelming evidence
[00:49:57.080 --> 00:50:00.120]   that there's functional differentiation,
[00:50:00.120 --> 00:50:03.540]   that it's clearly not the case,
[00:50:03.540 --> 00:50:07.180]   that all parts of the brain are doing the same thing.
[00:50:07.180 --> 00:50:11.160]   This follows immediately from the kinds of studies
[00:50:11.160 --> 00:50:14.700]   of brain damage that we were chatting about before.
[00:50:14.700 --> 00:50:18.140]   It's obvious from what you see
[00:50:18.140 --> 00:50:19.680]   if you stick an electrode in the brain
[00:50:19.680 --> 00:50:21.280]   and measure what's going on
[00:50:21.280 --> 00:50:24.600]   at the level of neural activity.
[00:50:24.600 --> 00:50:30.760]   Having said that, there are two other things to add,
[00:50:30.760 --> 00:50:32.800]   which kind of, I don't know,
[00:50:32.800 --> 00:50:34.400]   maybe tug in the other direction.
[00:50:34.400 --> 00:50:39.400]   One is that it's when you look carefully
[00:50:39.400 --> 00:50:42.280]   at functional differentiation in the brain,
[00:50:42.280 --> 00:50:44.500]   what you usually end up concluding,
[00:50:45.840 --> 00:50:48.180]   at least this is my observation of the literature,
[00:50:48.180 --> 00:50:52.820]   is that the differences between regions are graded
[00:50:52.820 --> 00:50:55.240]   rather than being discrete.
[00:50:55.240 --> 00:50:57.500]   So it doesn't seem like it's easy
[00:50:57.500 --> 00:51:01.780]   to divide the brain up into true modules
[00:51:01.780 --> 00:51:06.780]   that have clear boundaries
[00:51:06.780 --> 00:51:11.780]   and that have clear channels of communication between them.
[00:51:12.780 --> 00:51:18.020]   - And this applies to the prefrontal cortex?
[00:51:18.020 --> 00:51:18.860]   - Yeah, oh yeah.
[00:51:18.860 --> 00:51:20.200]   Yeah, the prefrontal cortex is made up
[00:51:20.200 --> 00:51:22.060]   of a bunch of different sub-regions,
[00:51:22.060 --> 00:51:27.380]   the functions of which are not clearly defined
[00:51:27.380 --> 00:51:30.800]   and the borders of which seem to be quite vague.
[00:51:30.800 --> 00:51:34.420]   And then there's another thing that's popping up
[00:51:34.420 --> 00:51:36.020]   in very recent research,
[00:51:36.020 --> 00:51:41.020]   which involves application of these new features.
[00:51:41.020 --> 00:51:43.660]   Application of these new techniques,
[00:51:43.660 --> 00:51:47.300]   which there are a number of studies that suggest
[00:51:47.300 --> 00:51:49.660]   that parts of the brain
[00:51:49.660 --> 00:51:51.540]   that we would have previously thought
[00:51:51.540 --> 00:51:56.540]   were quite focused in their function
[00:51:56.540 --> 00:51:59.100]   are actually carrying signals
[00:51:59.100 --> 00:52:01.340]   that we wouldn't have thought would be there.
[00:52:01.340 --> 00:52:04.500]   For example, looking in the primary visual cortex,
[00:52:04.500 --> 00:52:07.900]   which is classically thought of as basically
[00:52:07.900 --> 00:52:09.380]   the first cortical way station
[00:52:09.380 --> 00:52:10.900]   for processing visual information.
[00:52:10.900 --> 00:52:12.980]   Basically what it should care about is,
[00:52:12.980 --> 00:52:15.840]   where are the edges in this scene that I'm viewing?
[00:52:15.840 --> 00:52:19.460]   It turns out that if you have enough data,
[00:52:19.460 --> 00:52:22.220]   you can recover information from primary visual cortex
[00:52:22.220 --> 00:52:23.220]   about all sorts of things,
[00:52:23.220 --> 00:52:26.900]   like what behavior the animal is engaged in right now
[00:52:26.900 --> 00:52:29.340]   and how much reward is on offer
[00:52:29.340 --> 00:52:31.340]   in the task that it's pursuing.
[00:52:31.340 --> 00:52:35.180]   So it's clear that even regions
[00:52:35.180 --> 00:52:40.180]   whose function is pretty well defined at a coarse grain
[00:52:40.520 --> 00:52:42.860]   are nonetheless carrying some information
[00:52:42.860 --> 00:52:47.060]   about information from very different domains.
[00:52:47.060 --> 00:52:51.540]   So the history of neuroscience is sort of this oscillation
[00:52:51.540 --> 00:52:54.020]   between the two views that you articulated,
[00:52:54.020 --> 00:52:57.740]   the kind of modular view and then the big mush view.
[00:52:57.740 --> 00:53:01.580]   And I think, I guess we're gonna end up
[00:53:01.580 --> 00:53:02.780]   somewhere in the middle,
[00:53:02.780 --> 00:53:05.580]   which is unfortunate for our understanding
[00:53:05.580 --> 00:53:10.020]   because there's something about our conceptual system
[00:53:10.020 --> 00:53:12.740]   that finds it easy to think about a modularized system
[00:53:12.740 --> 00:53:15.480]   and easy to think about a completely undifferentiated system,
[00:53:15.480 --> 00:53:19.980]   but something that kind of lies in between is confusing,
[00:53:19.980 --> 00:53:21.620]   but we're gonna have to get used to it, I think.
[00:53:21.620 --> 00:53:23.380]   - Unless we can understand deeply
[00:53:23.380 --> 00:53:25.900]   the lower level mechanism of neuronal communication.
[00:53:25.900 --> 00:53:26.740]   - Yeah, yeah.
[00:53:26.740 --> 00:53:29.680]   - So on that topic, you kind of mentioned information.
[00:53:29.680 --> 00:53:31.880]   Just to get a sense, I imagine something
[00:53:31.880 --> 00:53:34.620]   that there's still mystery and disagreement on
[00:53:34.620 --> 00:53:38.060]   is how does the brain carry information and signal?
[00:53:38.060 --> 00:53:43.060]   Like what in your sense is the basic mechanism
[00:53:43.060 --> 00:53:46.420]   of communication in the brain?
[00:53:46.420 --> 00:53:50.100]   - Well, I guess I'm old fashioned in that
[00:53:50.100 --> 00:53:53.300]   I consider the networks that we use
[00:53:53.300 --> 00:53:57.040]   in deep learning research to be a reasonable approximation
[00:53:57.040 --> 00:54:02.040]   to the mechanisms that carry information in the brain.
[00:54:02.040 --> 00:54:06.200]   So the usual way of articulating that is to say,
[00:54:06.200 --> 00:54:08.560]   what really matters is a rate code.
[00:54:08.560 --> 00:54:13.560]   What matters is how quickly is an individual neuron spiking?
[00:54:13.560 --> 00:54:16.360]   What's the frequency at which it's spiking?
[00:54:16.360 --> 00:54:17.880]   - So the timing of the spike.
[00:54:17.880 --> 00:54:20.340]   - Yeah, is it firing fast or slow?
[00:54:20.340 --> 00:54:22.760]   Let's put a number on that.
[00:54:22.760 --> 00:54:26.060]   And that number is enough to capture what neurons are doing.
[00:54:26.060 --> 00:54:31.120]   There's still uncertainty about whether
[00:54:31.120 --> 00:54:35.400]   that's an adequate description of how information
[00:54:35.400 --> 00:54:39.900]   is transmitted within the brain.
[00:54:39.900 --> 00:54:44.200]   There are studies that suggest that the precise timing
[00:54:44.200 --> 00:54:46.080]   of spikes matters.
[00:54:46.080 --> 00:54:50.680]   There are studies that suggest that there are computations
[00:54:50.680 --> 00:54:54.520]   that go on within the dendritic tree, within a neuron
[00:54:54.520 --> 00:54:57.120]   that are quite rich and structured
[00:54:57.120 --> 00:54:59.980]   and that really don't equate to anything that we're doing
[00:54:59.980 --> 00:55:01.680]   in our artificial neural networks.
[00:55:02.840 --> 00:55:07.840]   Having said that, I feel like we're getting somewhere
[00:55:07.840 --> 00:55:11.640]   by sticking to this high level of abstraction.
[00:55:11.640 --> 00:55:13.400]   - Just the rate, and by the way,
[00:55:13.400 --> 00:55:16.200]   we're talking about the electrical signal.
[00:55:16.200 --> 00:55:20.040]   I remember reading some vague paper somewhere recently
[00:55:20.040 --> 00:55:23.400]   where the mechanical signal, like the vibrations
[00:55:23.400 --> 00:55:28.400]   or something of the neurons also communicates information.
[00:55:28.400 --> 00:55:29.680]   - I haven't seen that.
[00:55:30.640 --> 00:55:33.040]   - So there's somebody was arguing that
[00:55:33.040 --> 00:55:36.840]   the electrical signal, this is in Nature paper,
[00:55:36.840 --> 00:55:38.800]   something like that, where the electrical signal
[00:55:38.800 --> 00:55:43.720]   is actually a side effect of the mechanical signal.
[00:55:43.720 --> 00:55:46.080]   But I don't think that changes the story.
[00:55:46.080 --> 00:55:49.040]   But it's almost an interesting idea
[00:55:49.040 --> 00:55:52.400]   that there could be a deeper, it's always like in physics
[00:55:52.400 --> 00:55:55.720]   with quantum mechanics, there's always a deeper story
[00:55:55.720 --> 00:55:57.480]   that could be underlying the whole thing.
[00:55:57.480 --> 00:56:00.560]   But you think it's basically the rate of spiking
[00:56:00.560 --> 00:56:02.800]   that gets us, that's like the lowest hanging fruit
[00:56:02.800 --> 00:56:04.040]   that can get us really far.
[00:56:04.040 --> 00:56:06.600]   - This is a classical view.
[00:56:06.600 --> 00:56:10.720]   I mean, this is not, the only way in which this stance
[00:56:10.720 --> 00:56:14.000]   would be controversial is in the sense that
[00:56:14.000 --> 00:56:17.080]   there are members of the neuroscience community
[00:56:17.080 --> 00:56:18.840]   who are interested in alternatives.
[00:56:18.840 --> 00:56:21.400]   But this is really a very mainstream view.
[00:56:21.400 --> 00:56:24.120]   The way that neurons communicate is that
[00:56:25.440 --> 00:56:27.480]   neurotransmitters arrive,
[00:56:27.480 --> 00:56:32.800]   they wash up on a neuron,
[00:56:32.800 --> 00:56:35.880]   the neuron has receptors for those transmitters.
[00:56:35.880 --> 00:56:39.840]   The meeting of the transmitter with these receptors
[00:56:39.840 --> 00:56:42.320]   changes the voltage of the neuron.
[00:56:42.320 --> 00:56:45.000]   And if enough voltage change occurs,
[00:56:45.000 --> 00:56:46.840]   then a spike occurs, right?
[00:56:46.840 --> 00:56:48.640]   One of these like discrete events.
[00:56:48.640 --> 00:56:52.280]   And it's that spike that is conducted down the axon
[00:56:52.280 --> 00:56:54.520]   and leads to neurotransmitter release.
[00:56:54.520 --> 00:56:56.800]   This is just like neuroscience 101.
[00:56:56.800 --> 00:56:59.280]   This is like the way the brain is supposed to work.
[00:56:59.280 --> 00:57:03.640]   Now, what we do when we build artificial neural networks
[00:57:03.640 --> 00:57:06.760]   of the kind that are now popular in the AI community
[00:57:06.760 --> 00:57:11.760]   is that we don't worry about those individual spikes.
[00:57:11.760 --> 00:57:14.200]   We just worry about the frequency
[00:57:14.200 --> 00:57:16.960]   at which those spikes are being generated.
[00:57:16.960 --> 00:57:19.360]   And we consider, people talk about that
[00:57:19.360 --> 00:57:22.280]   as the activity of a neuron.
[00:57:22.280 --> 00:57:27.120]   And so the activity of units in a deep learning system
[00:57:27.120 --> 00:57:32.120]   is broadly analogous to the spike rate of a neuron.
[00:57:32.120 --> 00:57:37.000]   There are people who believe
[00:57:37.000 --> 00:57:39.160]   that there are other forms of communication in the brain.
[00:57:39.160 --> 00:57:41.200]   In fact, I've been involved in some research recently
[00:57:41.200 --> 00:57:46.200]   that suggests that the voltage fluctuations
[00:57:46.200 --> 00:57:49.680]   that occur in populations of neurons
[00:57:49.680 --> 00:57:54.680]   that are sort of below the level of spike production
[00:57:54.680 --> 00:57:57.680]   may be important for communication.
[00:57:57.680 --> 00:58:00.640]   But I'm still pretty old school in the sense
[00:58:00.640 --> 00:58:03.120]   that I think that the things that we're building
[00:58:03.120 --> 00:58:07.440]   in AI research constitute reasonable models
[00:58:07.440 --> 00:58:08.760]   of how a brain would work.
[00:58:08.760 --> 00:58:14.680]   - Let me ask just for fun a crazy question, 'cause I can.
[00:58:14.680 --> 00:58:17.040]   Do you think it's possible we're completely wrong
[00:58:17.040 --> 00:58:20.080]   about the way this basic mechanism
[00:58:20.080 --> 00:58:22.320]   of neuronal communication,
[00:58:22.320 --> 00:58:24.120]   that the information is stored
[00:58:24.120 --> 00:58:26.360]   in some very different kind of way in the brain?
[00:58:26.360 --> 00:58:27.600]   - Oh, heck yes.
[00:58:27.600 --> 00:58:29.960]   I mean, look, I wouldn't be a scientist
[00:58:29.960 --> 00:58:32.520]   if I didn't think there was any chance we were wrong.
[00:58:32.520 --> 00:58:36.440]   But I mean, if you look at the history
[00:58:36.440 --> 00:58:38.680]   of deep learning research
[00:58:38.680 --> 00:58:41.200]   as it's been applied to neuroscience,
[00:58:41.200 --> 00:58:43.840]   of course, the vast majority of deep learning research
[00:58:43.840 --> 00:58:45.440]   these days isn't about neuroscience.
[00:58:45.440 --> 00:58:49.120]   But if you go back to the 1980s,
[00:58:49.120 --> 00:58:52.800]   there's sort of an unbroken chain of research
[00:58:52.800 --> 00:58:55.000]   in which a particular strategy is taken,
[00:58:55.000 --> 00:59:00.000]   which is, hey, let's train a deep learning system.
[00:59:00.000 --> 00:59:04.120]   Let's train a multilayer neural network
[00:59:04.120 --> 00:59:09.120]   on this task that we trained our rat on,
[00:59:09.120 --> 00:59:12.320]   or our monkey on, or this human being on.
[00:59:12.320 --> 00:59:15.720]   And then let's look at what the units
[00:59:15.720 --> 00:59:17.720]   deep in the system are doing.
[00:59:17.720 --> 00:59:20.800]   And let's ask whether what they're doing
[00:59:20.800 --> 00:59:22.400]   resembles what we know about
[00:59:22.400 --> 00:59:24.640]   what neurons deep in the brain are doing.
[00:59:24.640 --> 00:59:28.560]   And over and over and over and over,
[00:59:28.560 --> 00:59:31.120]   that strategy works in the sense that
[00:59:31.120 --> 00:59:34.360]   the learning algorithms that we have access to,
[00:59:34.360 --> 00:59:37.760]   which typically center on back propagation,
[00:59:37.760 --> 00:59:42.080]   they give rise to patterns of activity,
[00:59:42.080 --> 00:59:44.080]   patterns of response,
[00:59:44.080 --> 00:59:48.760]   patterns of neuronal behavior in these artificial models
[00:59:48.760 --> 00:59:53.680]   that look hauntingly similar to what you see in the brain.
[00:59:53.680 --> 00:59:57.400]   And is that a coincidence?
[00:59:57.400 --> 00:59:59.720]   - At a certain point, it starts looking like
[00:59:59.720 --> 01:00:03.000]   such coincidence is unlikely to not be deeply meaningful.
[01:00:03.000 --> 01:00:03.840]   Yeah. - Yeah.
[01:00:03.840 --> 01:00:07.160]   The circumstantial evidence is overwhelming.
[01:00:07.160 --> 01:00:08.680]   But it could-- - But you're always open
[01:00:08.680 --> 01:00:10.480]   to a total flipping of the table.
[01:00:10.480 --> 01:00:11.640]   - Hey, of course.
[01:00:11.640 --> 01:00:15.160]   So you have co-authored several recent papers
[01:00:15.160 --> 01:00:17.880]   that sort of weave beautifully between the world
[01:00:17.880 --> 01:00:20.640]   of neuroscience and artificial intelligence.
[01:00:20.640 --> 01:00:24.800]   And maybe if we could,
[01:00:24.800 --> 01:00:27.480]   can we just try to dance around and talk about some of them,
[01:00:27.480 --> 01:00:29.720]   maybe try to pick out interesting ideas
[01:00:29.720 --> 01:00:32.280]   that jump to your mind from memory.
[01:00:32.280 --> 01:00:33.720]   So maybe looking at,
[01:00:33.720 --> 01:00:35.520]   we were talking about the prefrontal cortex,
[01:00:35.520 --> 01:00:38.960]   the 2018, I believe, paper called
[01:00:38.960 --> 01:00:42.240]   "Prefrontal Cortex as a Meta-Reinforcement Learning System."
[01:00:42.240 --> 01:00:44.400]   What, is there a key idea
[01:00:44.400 --> 01:00:46.720]   that you can speak to from that paper?
[01:00:46.720 --> 01:00:52.680]   - Yeah, I mean, the key idea is about meta-learning.
[01:00:52.680 --> 01:00:54.880]   So-- - What is meta-learning?
[01:00:54.880 --> 01:00:58.680]   - Meta-learning is, by definition,
[01:00:58.680 --> 01:01:05.000]   a situation in which you have a learning algorithm,
[01:01:06.160 --> 01:01:09.800]   and the learning algorithm operates in such a way
[01:01:09.800 --> 01:01:14.080]   that it gives rise to another learning algorithm.
[01:01:14.080 --> 01:01:17.180]   In the earliest applications of this idea,
[01:01:17.180 --> 01:01:20.360]   you had one learning algorithm sort of adjusting
[01:01:20.360 --> 01:01:23.080]   the parameters on another learning algorithm.
[01:01:23.080 --> 01:01:25.120]   But the case that we're interested in this paper
[01:01:25.120 --> 01:01:29.200]   is one where you start with just one learning algorithm,
[01:01:29.200 --> 01:01:33.040]   and then another learning algorithm kind of emerges
[01:01:33.040 --> 01:01:35.200]   out of thin air.
[01:01:35.200 --> 01:01:36.720]   I can say more about what I mean by that.
[01:01:36.720 --> 01:01:38.800]   I don't mean to be, you know,
[01:01:38.800 --> 01:01:39.640]   (Michael laughs)
[01:01:39.640 --> 01:01:43.320]   ascurantist, but that's the idea of meta-learning.
[01:01:43.320 --> 01:01:46.040]   It relates to the old idea in psychology
[01:01:46.040 --> 01:01:47.300]   of learning to learn.
[01:01:47.300 --> 01:01:54.320]   Situations where you have experiences
[01:01:54.320 --> 01:01:57.320]   that make you better at learning something new.
[01:01:57.320 --> 01:02:01.400]   Like a familiar example would be learning a foreign language.
[01:02:01.400 --> 01:02:02.880]   The first time you learn a foreign language,
[01:02:02.880 --> 01:02:07.880]   it may be quite laborious and disorienting and novel.
[01:02:07.880 --> 01:02:12.220]   But if, let's say you've learned two foreign languages,
[01:02:12.220 --> 01:02:14.160]   the third foreign language, obviously,
[01:02:14.160 --> 01:02:15.960]   is gonna be much easier to pick up.
[01:02:15.960 --> 01:02:16.800]   And why?
[01:02:16.800 --> 01:02:18.240]   Because you've learned how to learn.
[01:02:18.240 --> 01:02:20.200]   You know how this goes.
[01:02:20.200 --> 01:02:22.160]   You know, okay, I'm gonna have to learn how to conjugate.
[01:02:22.160 --> 01:02:23.920]   I'm gonna have to...
[01:02:23.920 --> 01:02:26.360]   That's a simple form of meta-learning, right?
[01:02:26.360 --> 01:02:30.240]   In the sense that there's some slow learning mechanism
[01:02:30.240 --> 01:02:33.040]   that's helping you kind of update
[01:02:33.040 --> 01:02:34.280]   your fast learning mechanism.
[01:02:34.280 --> 01:02:36.520]   Does that bring it into focus?
[01:02:36.520 --> 01:02:39.160]   - So how, from our understanding,
[01:02:39.160 --> 01:02:42.360]   from the psychology world, from neuroscience,
[01:02:42.360 --> 01:02:45.320]   our understanding how meta-learning works
[01:02:45.320 --> 01:02:47.200]   might work in the human brain,
[01:02:47.200 --> 01:02:50.000]   what lessons can we draw from that
[01:02:50.000 --> 01:02:53.080]   that we can bring into the artificial intelligence world?
[01:02:53.080 --> 01:02:55.960]   - Well, yeah, so the origin of that paper
[01:02:55.960 --> 01:03:00.200]   was in AI work that we were doing in my group.
[01:03:00.200 --> 01:03:03.720]   We were looking at what happens
[01:03:03.720 --> 01:03:06.280]   when you train a recurrent neural network
[01:03:06.280 --> 01:03:10.200]   using standard reinforcement learning algorithms.
[01:03:10.200 --> 01:03:12.680]   But you train that network, not just in one task,
[01:03:12.680 --> 01:03:15.200]   but you train it in a bunch of interrelated tasks.
[01:03:15.200 --> 01:03:17.640]   And then you ask what happens
[01:03:17.640 --> 01:03:19.920]   when you give it yet another task
[01:03:19.920 --> 01:03:23.360]   in that sort of line of interrelated tasks.
[01:03:23.360 --> 01:03:27.520]   And what we started to realize is that
[01:03:29.360 --> 01:03:31.840]   a form of meta-learning spontaneously happens
[01:03:31.840 --> 01:03:33.760]   in recurrent neural networks.
[01:03:33.760 --> 01:03:37.680]   And the simplest way to explain it is to say
[01:03:37.680 --> 01:03:43.480]   a recurrent neural network has a kind of memory
[01:03:43.480 --> 01:03:45.320]   in its activation patterns.
[01:03:45.320 --> 01:03:47.480]   It's recurrent by definition in the sense
[01:03:47.480 --> 01:03:50.120]   that you have units that connect to other units
[01:03:50.120 --> 01:03:51.000]   that connect to other units.
[01:03:51.000 --> 01:03:53.640]   So you have sort of loops of connectivity,
[01:03:53.640 --> 01:03:55.720]   which allows activity to stick around
[01:03:55.720 --> 01:03:57.320]   and be updated over time.
[01:03:57.360 --> 01:03:59.000]   In psychology, we call, in neuroscience,
[01:03:59.000 --> 01:04:00.080]   we call this working memory.
[01:04:00.080 --> 01:04:03.000]   It's like actively holding something in mind.
[01:04:03.000 --> 01:04:10.480]   And so that memory gives the recurrent neural network
[01:04:10.480 --> 01:04:13.080]   a dynamics, right?
[01:04:13.080 --> 01:04:17.680]   The way that the activity pattern evolves over time
[01:04:17.680 --> 01:04:19.960]   is inherent to the connectivity
[01:04:19.960 --> 01:04:21.560]   of the recurrent neural network, okay?
[01:04:21.560 --> 01:04:23.480]   So that's idea number one.
[01:04:23.480 --> 01:04:25.520]   Now, the dynamics of that network
[01:04:25.520 --> 01:04:29.600]   are shaped by the connectivity, by the synaptic weights.
[01:04:29.600 --> 01:04:31.600]   And those synaptic weights are being shaped
[01:04:31.600 --> 01:04:33.800]   by this reinforcement learning algorithm
[01:04:33.800 --> 01:04:35.960]   that you're training the network with.
[01:04:35.960 --> 01:04:39.120]   So the punchline is,
[01:04:39.120 --> 01:04:41.200]   if you train a recurrent neural network
[01:04:41.200 --> 01:04:43.080]   with a reinforcement learning algorithm
[01:04:43.080 --> 01:04:44.160]   that's adjusting its weights,
[01:04:44.160 --> 01:04:45.840]   and you do that for long enough,
[01:04:45.840 --> 01:04:50.800]   the activation dynamics will become very interesting, right?
[01:04:50.800 --> 01:04:53.120]   So imagine I give you a task
[01:04:53.160 --> 01:04:56.040]   where you have to press one button or another,
[01:04:56.040 --> 01:04:57.560]   left button or right button.
[01:04:57.560 --> 01:05:00.760]   And there's some probability
[01:05:00.760 --> 01:05:02.240]   that I'm gonna give you an M&M
[01:05:02.240 --> 01:05:04.160]   if you press the left button,
[01:05:04.160 --> 01:05:06.160]   and there's some probability I'll give you an M&M
[01:05:06.160 --> 01:05:07.560]   if you press the other button.
[01:05:07.560 --> 01:05:09.320]   And you have to figure out what those probabilities are
[01:05:09.320 --> 01:05:10.640]   just by trying things out.
[01:05:10.640 --> 01:05:13.760]   But as I said before,
[01:05:13.760 --> 01:05:15.480]   instead of just giving you one of these tasks,
[01:05:15.480 --> 01:05:16.960]   I give you a whole sequence.
[01:05:16.960 --> 01:05:18.640]   You know, I give you two buttons
[01:05:18.640 --> 01:05:19.840]   and you figure out which one's best,
[01:05:19.840 --> 01:05:22.120]   and I go, "Good job, here's a new box,
[01:05:22.120 --> 01:05:24.040]   two new buttons, you have to figure out which one's best.
[01:05:24.040 --> 01:05:25.400]   Good job, here's a new box."
[01:05:25.400 --> 01:05:27.280]   And every box has its own probabilities
[01:05:27.280 --> 01:05:28.240]   and you have to figure it.
[01:05:28.240 --> 01:05:30.360]   So if you train a recurrent neural network
[01:05:30.360 --> 01:05:32.600]   on that kind of sequence of tasks,
[01:05:32.600 --> 01:05:37.320]   what happens, it seemed almost magical to us
[01:05:37.320 --> 01:05:41.120]   when we first started kind of realizing what was going on.
[01:05:41.120 --> 01:05:42.880]   The slow learning algorithm
[01:05:42.880 --> 01:05:45.480]   that's adjusting the synaptic weights,
[01:05:45.480 --> 01:05:51.320]   those slow synaptic changes give rise to a network dynamics
[01:05:51.320 --> 01:05:52.960]   that themselves, that, you know,
[01:05:52.960 --> 01:05:56.840]   the dynamics themselves turn into a learning algorithm.
[01:05:56.840 --> 01:05:59.040]   So in other words, you can tell this is happening
[01:05:59.040 --> 01:06:00.920]   by just freezing the synaptic weights, saying,
[01:06:00.920 --> 01:06:03.360]   "Okay, no more learning, you're done.
[01:06:03.360 --> 01:06:07.600]   Here's a new box, figure out which button is best."
[01:06:07.600 --> 01:06:09.600]   And the recurrent neural network will do this just fine.
[01:06:09.600 --> 01:06:13.040]   There's no, like it figures out which button is best.
[01:06:13.040 --> 01:06:16.680]   It kind of transitions from exploring the two buttons
[01:06:16.680 --> 01:06:18.360]   to just pressing the one that it likes best
[01:06:18.360 --> 01:06:20.680]   in a very rational way.
[01:06:20.680 --> 01:06:21.680]   How is that happening?
[01:06:21.680 --> 01:06:25.840]   It's happening because the activity dynamics of the network
[01:06:25.840 --> 01:06:28.480]   have been shaped by the slow learning process
[01:06:28.480 --> 01:06:30.720]   that's occurred over many, many boxes.
[01:06:30.720 --> 01:06:34.680]   And so what's happened is that this slow learning algorithm
[01:06:34.680 --> 01:06:37.160]   that's slowly adjusting the weights
[01:06:37.160 --> 01:06:39.760]   is changing the dynamics of the network,
[01:06:39.760 --> 01:06:43.480]   the activity dynamics into its own learning algorithm.
[01:06:43.480 --> 01:06:48.480]   And as we were kind of realizing that this is a thing,
[01:06:49.160 --> 01:06:53.760]   it just so happened that the group that was working on this
[01:06:53.760 --> 01:06:56.040]   included a bunch of neuroscientists.
[01:06:56.040 --> 01:06:59.920]   And it started kind of ringing a bell for us,
[01:06:59.920 --> 01:07:01.680]   which is to say that we thought,
[01:07:01.680 --> 01:07:03.720]   "This sounds a lot like the distinction
[01:07:03.720 --> 01:07:07.560]   between synaptic learning and activity,
[01:07:07.560 --> 01:07:10.560]   synaptic memory and activity-based memory in the brain."
[01:07:10.560 --> 01:07:15.920]   And it also reminded us of recurrent connectivity
[01:07:15.920 --> 01:07:18.400]   that's very characteristic of prefrontal function.
[01:07:18.400 --> 01:07:22.840]   So this is kind of why it's good to have people working
[01:07:22.840 --> 01:07:26.240]   on AI that know a little bit about neuroscience
[01:07:26.240 --> 01:07:29.560]   and vice versa, because we started thinking about
[01:07:29.560 --> 01:07:32.360]   whether we could apply this principle to neuroscience.
[01:07:32.360 --> 01:07:33.680]   And that's where the paper came from.
[01:07:33.680 --> 01:07:37.560]   - So the kind of principle of the recurrence
[01:07:37.560 --> 01:07:39.560]   they can see in the prefrontal cortex,
[01:07:39.560 --> 01:07:42.920]   then you start to realize that it's possible
[01:07:42.920 --> 01:07:47.360]   to force something like an idea of a learning to learn,
[01:07:47.360 --> 01:07:50.880]   emerging from this learning process,
[01:07:50.880 --> 01:07:54.520]   as long as you keep varying the environment sufficiently.
[01:07:54.520 --> 01:07:55.360]   - Exactly.
[01:07:55.360 --> 01:07:59.320]   So the kind of metaphorical transition
[01:07:59.320 --> 01:08:00.720]   we made to neuroscience was to think,
[01:08:00.720 --> 01:08:03.720]   "Okay, well, we know that the prefrontal cortex
[01:08:03.720 --> 01:08:05.000]   is highly recurrent.
[01:08:05.000 --> 01:08:08.540]   We know that it's an important locus for working memory,
[01:08:08.540 --> 01:08:11.300]   for activation-based memory.
[01:08:11.300 --> 01:08:13.720]   So maybe the prefrontal cortex
[01:08:13.720 --> 01:08:15.640]   supports reinforcement learning."
[01:08:15.640 --> 01:08:19.320]   In other words, what is reinforcement learning?
[01:08:19.320 --> 01:08:21.680]   You take an action, you see how much reward you got,
[01:08:21.680 --> 01:08:23.620]   you update your policy of behavior.
[01:08:23.620 --> 01:08:26.900]   Maybe the prefrontal cortex is doing that sort of thing
[01:08:26.900 --> 01:08:28.520]   strictly in its activation patterns.
[01:08:28.520 --> 01:08:31.920]   It's keeping around a memory in its activity patterns
[01:08:31.920 --> 01:08:35.400]   of what you did, how much reward you got,
[01:08:35.400 --> 01:08:39.040]   and it's using that activity-based memory
[01:08:39.040 --> 01:08:41.120]   as a basis for updating behavior.
[01:08:41.120 --> 01:08:42.020]   But then the question is,
[01:08:42.020 --> 01:08:44.580]   "Well, how did the prefrontal cortex get so smart?"
[01:08:44.580 --> 01:08:48.020]   In other words, where did these activity dynamics come from?
[01:08:48.020 --> 01:08:50.780]   How did that program that's implemented
[01:08:50.780 --> 01:08:54.460]   in the recurrent dynamics of the prefrontal cortex arise?
[01:08:54.460 --> 01:08:58.020]   And one answer that became evident in this work was,
[01:08:58.020 --> 01:09:00.940]   "Well, maybe the mechanisms that operate
[01:09:00.940 --> 01:09:02.820]   on the synaptic level,
[01:09:02.820 --> 01:09:06.460]   which we believe are mediated by dopamine,
[01:09:06.460 --> 01:09:08.820]   are responsible for shaping those dynamics."
[01:09:10.180 --> 01:09:12.460]   - So this may be a silly question,
[01:09:12.460 --> 01:09:17.460]   but because this kind of several temporal classes
[01:09:17.460 --> 01:09:20.900]   of learning are happening,
[01:09:20.900 --> 01:09:24.340]   and the learning-to-learnism emerges,
[01:09:24.340 --> 01:09:29.340]   can you keep building stacks of learning-to-learn-to-learn,
[01:09:29.340 --> 01:09:31.660]   learning-to-learn-to-learn-to-learn-to-learn,
[01:09:31.660 --> 01:09:34.420]   because it keeps, I mean, basically abstractions
[01:09:34.420 --> 01:09:38.700]   of more powerful abilities to generalize
[01:09:38.700 --> 01:09:41.140]   of learning complex rules?
[01:09:41.140 --> 01:09:41.980]   - Yeah.
[01:09:41.980 --> 01:09:46.100]   - Or is this overstretching this kind of mechanism?
[01:09:46.100 --> 01:09:49.460]   - Well, one of the people in AI
[01:09:49.460 --> 01:09:52.460]   who started thinking about meta-learning
[01:09:52.460 --> 01:09:55.900]   from very early on, Juergen and Schmidhuber,
[01:09:55.900 --> 01:09:59.800]   sort of cheekily suggested,
[01:09:59.800 --> 01:10:03.940]   I think it may have been in his PhD thesis,
[01:10:03.940 --> 01:10:06.900]   that we should think about meta, meta, meta,
[01:10:06.900 --> 01:10:08.100]   meta, meta, meta learning.
[01:10:08.100 --> 01:10:11.260]   You know, that's really what's gonna get us
[01:10:11.260 --> 01:10:13.140]   to true intelligence.
[01:10:13.140 --> 01:10:15.420]   - Certainly there's a poetic aspect to it,
[01:10:15.420 --> 01:10:19.300]   and it seems interesting and correct
[01:10:19.300 --> 01:10:21.700]   that that kind of level of abstraction would be powerful,
[01:10:21.700 --> 01:10:23.940]   but is that something you see in the brain?
[01:10:23.940 --> 01:10:27.800]   This kind of, is it useful to think of learning
[01:10:27.800 --> 01:10:29.660]   in these meta, meta, meta way,
[01:10:29.660 --> 01:10:32.100]   or is it just meta learning?
[01:10:32.100 --> 01:10:35.300]   - Well, one thing that really fascinated me
[01:10:35.300 --> 01:10:39.020]   about this mechanism that we were starting to look at,
[01:10:39.020 --> 01:10:43.220]   and other groups started talking about very similar things
[01:10:43.220 --> 01:10:47.020]   at the same time, and then a kind of explosion of interest
[01:10:47.020 --> 01:10:48.980]   in meta-learning happened in the AI community
[01:10:48.980 --> 01:10:50.580]   shortly after that.
[01:10:50.580 --> 01:10:52.060]   I don't know if we had anything to do with that,
[01:10:52.060 --> 01:10:55.620]   but I was gratified to see that a lot of people
[01:10:55.620 --> 01:10:57.780]   started talking about meta-learning.
[01:10:57.780 --> 01:11:01.380]   One of the things that I like about the kind of flavor
[01:11:01.380 --> 01:11:03.320]   of meta-learning that we were studying
[01:11:03.320 --> 01:11:05.960]   was that it didn't require anything special.
[01:11:05.960 --> 01:11:07.820]   It was just, if you took a system
[01:11:07.820 --> 01:11:10.020]   that had some form of memory,
[01:11:10.020 --> 01:11:13.260]   that the function of which could be shaped
[01:11:13.260 --> 01:11:16.880]   by pick your RL algorithm,
[01:11:16.880 --> 01:11:18.480]   then this would just happen.
[01:11:18.480 --> 01:11:19.320]   - Yes.
[01:11:19.320 --> 01:11:21.320]   - I mean, there are a lot of forms of,
[01:11:21.320 --> 01:11:23.200]   there are a lot of meta-learning algorithms
[01:11:23.200 --> 01:11:24.520]   that have been proposed since then
[01:11:24.520 --> 01:11:26.600]   that are fascinating and effective
[01:11:26.600 --> 01:11:29.800]   in their domains of application,
[01:11:29.800 --> 01:11:31.640]   but they're engineered.
[01:11:31.640 --> 01:11:33.160]   There are things that somebody had to say,
[01:11:33.160 --> 01:11:34.960]   well, gee, if we wanted meta-learning to happen,
[01:11:34.960 --> 01:11:35.800]   how would we do that?
[01:11:35.800 --> 01:11:37.080]   Here's an algorithm that would,
[01:11:37.080 --> 01:11:39.520]   but there's something about the kind of meta-learning
[01:11:39.520 --> 01:11:42.560]   that we were studying that seemed to me special
[01:11:42.560 --> 01:11:45.000]   in the sense that it wasn't an algorithm.
[01:11:45.000 --> 01:11:48.760]   It was just something that automatically happened
[01:11:48.760 --> 01:11:51.080]   if you had a system that had memory
[01:11:51.080 --> 01:11:54.040]   and it was trained with a reinforcement learning algorithm.
[01:11:54.040 --> 01:11:59.040]   And in that sense, it can be as meta as it wants to be.
[01:11:59.600 --> 01:12:04.600]   There's no limit on how abstract the meta-learning can get
[01:12:04.600 --> 01:12:08.040]   because it's not reliant on a human engineering
[01:12:08.040 --> 01:12:10.820]   a particular meta-learning algorithm to get there.
[01:12:10.820 --> 01:12:15.200]   And that's, I also, I don't know,
[01:12:15.200 --> 01:12:17.880]   I guess I hope that that's relevant in the brain.
[01:12:17.880 --> 01:12:19.280]   I think there's a kind of beauty
[01:12:19.280 --> 01:12:23.440]   in the ability of this emergent--
[01:12:23.440 --> 01:12:24.720]   - The emergent aspect of it.
[01:12:24.720 --> 01:12:25.560]   - Yeah, it's something that--
[01:12:25.560 --> 01:12:26.520]   - As opposed to engineered.
[01:12:26.520 --> 01:12:28.280]   - Exactly, it's something that just,
[01:12:28.280 --> 01:12:31.000]   it just happens in a sense.
[01:12:31.000 --> 01:12:33.680]   In a sense, you can't avoid this happening.
[01:12:33.680 --> 01:12:35.880]   If you have a system that has memory
[01:12:35.880 --> 01:12:37.520]   and the function of that memory
[01:12:37.520 --> 01:12:41.440]   is shaped by reinforcement learning,
[01:12:41.440 --> 01:12:42.800]   and this system is trained
[01:12:42.800 --> 01:12:45.040]   in a series of interrelated tasks,
[01:12:45.040 --> 01:12:48.520]   this is gonna happen, you can't stop it.
[01:12:48.520 --> 01:12:50.200]   - As long as you have certain properties,
[01:12:50.200 --> 01:12:52.600]   maybe like a recurrent structure to--
[01:12:52.600 --> 01:12:53.440]   - You have to have memory.
[01:12:53.440 --> 01:12:55.680]   It actually doesn't have to be a recurrent neural network.
[01:12:55.680 --> 01:12:59.800]   A paper that I was honored to be involved with even earlier
[01:12:59.800 --> 01:13:02.280]   used a kind of slot-based memory.
[01:13:02.280 --> 01:13:03.120]   - Do you remember the title?
[01:13:03.120 --> 01:13:05.040]   Just for people who watched it.
[01:13:05.040 --> 01:13:08.120]   - It was Memory Augmented Neural Networks.
[01:13:08.120 --> 01:13:10.160]   I think the title was
[01:13:10.160 --> 01:13:13.020]   Meta-Learning in Memory Augmented Neural Networks.
[01:13:13.020 --> 01:13:17.920]   And it was the same exact story.
[01:13:17.920 --> 01:13:21.080]   If you have a system with memory,
[01:13:21.080 --> 01:13:22.760]   here it was a different kind of memory,
[01:13:22.800 --> 01:13:24.600]   but the function of that memory
[01:13:24.600 --> 01:13:28.620]   is shaped by reinforcement learning.
[01:13:28.620 --> 01:13:32.560]   Here it was the reads and writes
[01:13:32.560 --> 01:13:36.440]   that occurred on this slot-based memory.
[01:13:36.440 --> 01:13:38.080]   This'll just happen.
[01:13:38.080 --> 01:13:42.080]   But this brings us back to something I was saying earlier
[01:13:42.080 --> 01:13:44.500]   about the importance of the environment.
[01:13:44.500 --> 01:13:49.920]   This will happen if the system is being trained
[01:13:49.920 --> 01:13:53.080]   in a setting where there's a sequence of tasks
[01:13:53.080 --> 01:13:55.220]   that all share some abstract structure.
[01:13:55.220 --> 01:13:59.020]   Sometimes we talk about task distributions.
[01:13:59.020 --> 01:14:04.020]   And that's something that's very obviously true
[01:14:04.020 --> 01:14:06.380]   of the world that humans inhabit.
[01:14:06.380 --> 01:14:12.340]   If you just kind of think about what you do every day,
[01:14:12.340 --> 01:14:16.280]   you never do exactly the same thing
[01:14:16.280 --> 01:14:17.640]   that you did the day before.
[01:14:17.640 --> 01:14:21.040]   But everything that you do has a family resemblance.
[01:14:21.040 --> 01:14:23.480]   It shares a structure with something that you did before.
[01:14:23.480 --> 01:14:26.280]   And so the real world is
[01:14:26.280 --> 01:14:32.680]   saturated with this property.
[01:14:32.680 --> 01:14:37.680]   It's endless variety with endless redundancy.
[01:14:37.680 --> 01:14:38.560]   And that's the setting
[01:14:38.560 --> 01:14:40.560]   in which this kind of meta-learning happens.
[01:14:40.560 --> 01:14:44.840]   - And it does seem like we're just so good at finding,
[01:14:44.840 --> 01:14:47.320]   just like in this emergent phenomenon
[01:14:47.320 --> 01:14:50.040]   you described, we're really good at finding that redundancy,
[01:14:50.040 --> 01:14:53.480]   finding those similarities, the family resemblance.
[01:14:53.480 --> 01:14:56.560]   Some people call it sort of, what is it?
[01:14:56.560 --> 01:14:59.160]   Melanie Mitchell was talking about analogies.
[01:14:59.160 --> 01:15:01.920]   So we're able to connect concepts together
[01:15:01.920 --> 01:15:03.160]   in this kind of way,
[01:15:03.160 --> 01:15:06.120]   in this same kind of automated emergent way.
[01:15:06.120 --> 01:15:10.640]   There's so many echoes here of psychology and neuroscience.
[01:15:10.640 --> 01:15:15.320]   And obviously now with reinforcement learning
[01:15:15.320 --> 01:15:18.280]   with recurrent neural networks at the core.
[01:15:18.280 --> 01:15:20.160]   If we could talk a little bit about dopamine,
[01:15:20.160 --> 01:15:23.800]   you have really, you're a part of co-authoring
[01:15:23.800 --> 01:15:26.440]   really exciting recent paper, very recent,
[01:15:26.440 --> 01:15:28.920]   in terms of release on dopamine
[01:15:28.920 --> 01:15:31.040]   and temporal difference learning.
[01:15:31.040 --> 01:15:34.800]   Can you describe the key ideas of that paper?
[01:15:34.800 --> 01:15:35.640]   - Sure, yeah.
[01:15:35.640 --> 01:15:37.760]   I mean, one thing I want to pause to do
[01:15:37.760 --> 01:15:40.600]   is acknowledge my co-authors on actually both
[01:15:40.600 --> 01:15:41.560]   of the papers we're talking about.
[01:15:41.560 --> 01:15:42.640]   So this dopamine paper--
[01:15:43.080 --> 01:15:45.720]   - I'll just, I'll certainly post all their names.
[01:15:45.720 --> 01:15:46.560]   - Okay, wonderful, yeah.
[01:15:46.560 --> 01:15:50.360]   'Cause I'm sort of a bash to be the spokesperson
[01:15:50.360 --> 01:15:53.880]   for these papers when I had such amazing collaborators
[01:15:53.880 --> 01:15:55.200]   on both.
[01:15:55.200 --> 01:15:57.000]   So it's a comfort to me to know
[01:15:57.000 --> 01:15:58.600]   that you'll acknowledge them.
[01:15:58.600 --> 01:16:00.440]   - Yeah, there's an incredible team there, but yeah.
[01:16:00.440 --> 01:16:03.120]   - Oh yeah, it's so much fun.
[01:16:03.120 --> 01:16:06.400]   And in the case of the dopamine paper,
[01:16:06.400 --> 01:16:09.040]   we also collaborated with Naoichi at Harvard,
[01:16:09.040 --> 01:16:11.760]   who obviously the paper simply wouldn't have happened
[01:16:11.760 --> 01:16:12.680]   without him.
[01:16:12.680 --> 01:16:17.600]   But so you were asking for like a thumbnail sketch of--
[01:16:17.600 --> 01:16:21.320]   - Yes, a thumbnail sketch or key ideas or things,
[01:16:21.320 --> 01:16:24.840]   the insights that continue on our kind of discussion here
[01:16:24.840 --> 01:16:26.920]   between neuroscience and AI.
[01:16:26.920 --> 01:16:28.920]   - Yeah, I mean, this was another,
[01:16:28.920 --> 01:16:30.640]   a lot of the work that we've done so far
[01:16:30.640 --> 01:16:35.400]   is taking ideas that have bubbled up in AI
[01:16:35.400 --> 01:16:39.680]   and asking the question of whether the brain
[01:16:39.680 --> 01:16:41.460]   might be doing something related,
[01:16:41.460 --> 01:16:45.440]   which I think on the surface sounds like something
[01:16:45.440 --> 01:16:48.360]   that's really mainly of use to neuroscience.
[01:16:48.360 --> 01:16:54.360]   We see it also as a way of validating what we're doing
[01:16:54.360 --> 01:16:55.320]   on the AI side.
[01:16:55.320 --> 01:16:57.960]   If we can gain some evidence that the brain
[01:16:57.960 --> 01:17:01.760]   is using some technique that we've been trying out
[01:17:01.760 --> 01:17:05.500]   in our AI work, that gives us confidence
[01:17:05.500 --> 01:17:07.800]   that it may be a good idea,
[01:17:07.800 --> 01:17:11.540]   that it'll scale to rich, complex tasks,
[01:17:11.540 --> 01:17:14.820]   that it'll interface well with other mechanisms.
[01:17:14.820 --> 01:17:16.820]   - So you see it as a two-way road.
[01:17:16.820 --> 01:17:17.660]   - Yeah, for sure.
[01:17:17.660 --> 01:17:19.620]   - Just because a particular paper is a little bit focused
[01:17:19.620 --> 01:17:24.620]   on from AI from neural networks to neuroscience,
[01:17:24.620 --> 01:17:28.340]   ultimately the discussion, the thinking,
[01:17:28.340 --> 01:17:30.780]   the productive long-term aspect of it
[01:17:30.780 --> 01:17:33.180]   is the two-way road nature of the whole--
[01:17:33.180 --> 01:17:36.220]   - Yeah, I mean, we've talked about the notion
[01:17:36.220 --> 01:17:39.260]   of a virtuous circle between AI and neuroscience.
[01:17:39.260 --> 01:17:43.940]   And the way I see it, that's always been there
[01:17:43.940 --> 01:17:49.000]   since the two fields jointly existed.
[01:17:49.000 --> 01:17:52.100]   There have been some phases in that history
[01:17:52.100 --> 01:17:53.500]   when AI was sort of ahead.
[01:17:53.500 --> 01:17:56.300]   There are some phases when neuroscience was sort of ahead.
[01:17:56.300 --> 01:18:00.620]   I feel like given the burst of innovation
[01:18:00.620 --> 01:18:03.740]   that's happened recently on the AI side,
[01:18:03.740 --> 01:18:06.260]   AI is kind of ahead in the sense that
[01:18:06.260 --> 01:18:07.700]   there are all of these ideas
[01:18:07.700 --> 01:18:12.620]   for which it's exciting to consider
[01:18:12.620 --> 01:18:14.680]   that there might be neural analogs.
[01:18:14.680 --> 01:18:20.120]   And neuroscience, in a sense,
[01:18:20.120 --> 01:18:23.560]   has been focusing on approaches to studying behavior
[01:18:23.560 --> 01:18:26.540]   that come from, that are kind of derived
[01:18:26.540 --> 01:18:29.540]   from this earlier era of cognitive psychology.
[01:18:29.540 --> 01:18:32.140]   And so in some ways,
[01:18:32.140 --> 01:18:34.380]   fail to connect with some of the issues
[01:18:34.380 --> 01:18:36.660]   that we're grappling with in AI,
[01:18:36.660 --> 01:18:40.140]   like how do we deal with large, complex environments?
[01:18:40.140 --> 01:18:45.180]   But I think it's inevitable
[01:18:45.180 --> 01:18:47.900]   that this circle will keep turning
[01:18:47.900 --> 01:18:49.500]   and there will be a moment
[01:18:49.500 --> 01:18:51.260]   in the not too distant future
[01:18:51.260 --> 01:18:54.600]   when neuroscience is pelting AI researchers
[01:18:54.600 --> 01:18:58.220]   with insights that may change the direction of our work.
[01:18:58.220 --> 01:19:00.500]   - Just a quick human question.
[01:19:01.820 --> 01:19:05.460]   You have parts of your brain,
[01:19:05.460 --> 01:19:06.340]   this is very meta,
[01:19:06.340 --> 01:19:10.300]   but they're able to both think about neuroscience and AI.
[01:19:10.300 --> 01:19:14.220]   You know, I don't often meet people like that.
[01:19:14.220 --> 01:19:16.060]   So do you think,
[01:19:16.060 --> 01:19:19.780]   let me ask a metaplasticity question.
[01:19:19.780 --> 01:19:22.660]   Do you think a human being can be both good at AI
[01:19:22.660 --> 01:19:23.580]   and neuroscience?
[01:19:23.580 --> 01:19:26.500]   Is like what, on the team at DeepMind,
[01:19:26.500 --> 01:19:30.180]   what kind of human can occupy these two realms?
[01:19:30.180 --> 01:19:31.500]   And is that something you see?
[01:19:31.500 --> 01:19:34.740]   Everybody should be doing, can be doing,
[01:19:34.740 --> 01:19:37.400]   or is that a very special few can kind of jump?
[01:19:37.400 --> 01:19:39.180]   Just like we talk about art history,
[01:19:39.180 --> 01:19:41.020]   I would think it's a special person
[01:19:41.020 --> 01:19:43.620]   that can major in art history
[01:19:43.620 --> 01:19:46.860]   and also consider being a surgeon.
[01:19:46.860 --> 01:19:48.700]   - Otherwise known as a dilettante.
[01:19:48.700 --> 01:19:50.140]   - A dilettante, yeah.
[01:19:50.140 --> 01:19:51.320]   Easily distracted.
[01:19:51.320 --> 01:19:58.660]   - I think it does take a special kind of person
[01:19:58.660 --> 01:20:02.700]   to be truly world-class at both AI and neuroscience.
[01:20:02.700 --> 01:20:04.460]   And I am not on that list.
[01:20:04.460 --> 01:20:08.100]   I happen to be someone
[01:20:08.100 --> 01:20:11.380]   who's interest in neuroscience and psychology
[01:20:11.380 --> 01:20:17.660]   involved using the kinds of modeling techniques
[01:20:17.660 --> 01:20:21.180]   that are now very central in AI.
[01:20:21.180 --> 01:20:23.420]   And that sort of, I guess,
[01:20:23.420 --> 01:20:25.180]   bought me a ticket to be involved
[01:20:25.180 --> 01:20:26.620]   in all of the amazing things
[01:20:26.620 --> 01:20:28.820]   that are going on in AI research right now.
[01:20:28.820 --> 01:20:31.140]   I do know a few people
[01:20:31.140 --> 01:20:34.660]   who I would consider pretty expert on both fronts,
[01:20:34.660 --> 01:20:36.400]   and I won't embarrass them by naming them,
[01:20:36.400 --> 01:20:40.700]   but there are exceptional people out there
[01:20:40.700 --> 01:20:41.540]   who are like this.
[01:20:41.540 --> 01:20:46.020]   The one thing that I find is a barrier
[01:20:46.020 --> 01:20:49.420]   to being truly world-class on both fronts
[01:20:49.420 --> 01:20:54.420]   is just the complexity of the technology
[01:20:55.140 --> 01:20:58.340]   that's involved in both disciplines now.
[01:20:58.340 --> 01:21:03.140]   So the engineering expertise that it takes
[01:21:03.140 --> 01:21:08.020]   to do truly frontline, hands-on AI research
[01:21:08.020 --> 01:21:10.780]   is really, really considerable.
[01:21:10.780 --> 01:21:12.100]   - The learning curve of the tools,
[01:21:12.100 --> 01:21:14.320]   just like the specifics of just,
[01:21:14.320 --> 01:21:16.660]   whether it's programming or the kind of tools necessary
[01:21:16.660 --> 01:21:18.460]   to collect the data, to manage the data,
[01:21:18.460 --> 01:21:20.940]   to distribute, to compute, all that kind of stuff.
[01:21:20.940 --> 01:21:22.580]   And on the neuroscience, I guess, side,
[01:21:22.580 --> 01:21:24.760]   there'll be all different sets of tools.
[01:21:24.760 --> 01:21:26.980]   - Exactly, especially with the recent explosion
[01:21:26.980 --> 01:21:28.980]   in neuroscience methods.
[01:21:28.980 --> 01:21:32.380]   So having said all that,
[01:21:32.380 --> 01:21:40.180]   I think the best scenario for both neuroscience and AI
[01:21:40.180 --> 01:21:45.780]   is to have people who, interacting,
[01:21:45.780 --> 01:21:49.100]   who live at every point on this spectrum,
[01:21:49.100 --> 01:21:52.860]   from exclusively focused on neuroscience
[01:21:52.860 --> 01:21:56.480]   to exclusively focused on the engineering side of AI.
[01:21:56.480 --> 01:22:01.480]   But to have those people inhabiting a community
[01:22:01.480 --> 01:22:03.600]   where they're talking to people
[01:22:03.600 --> 01:22:05.680]   who live elsewhere on the spectrum.
[01:22:05.680 --> 01:22:09.540]   And I may be someone who's very close to the center
[01:22:09.540 --> 01:22:13.120]   in the sense that I have one foot in the neuroscience world
[01:22:13.120 --> 01:22:15.060]   and one foot in the AI world.
[01:22:15.060 --> 01:22:18.100]   And that central position, I will admit,
[01:22:18.100 --> 01:22:20.020]   prevents me, at least someone
[01:22:20.020 --> 01:22:22.460]   with my limited cognitive capacity,
[01:22:22.460 --> 01:22:27.460]   from having true technical expertise in either domain.
[01:22:27.460 --> 01:22:31.060]   But at the same time, I at least hope
[01:22:31.060 --> 01:22:33.300]   that it's worthwhile having people around
[01:22:33.300 --> 01:22:37.220]   who can kind of see the connections between these two.
[01:22:37.220 --> 01:22:41.060]   - Yeah, the emergent intelligence of the community
[01:22:41.060 --> 01:22:44.060]   when it's nicely distributed is useful.
[01:22:44.060 --> 01:22:45.180]   Okay, so-- - Exactly, yeah.
[01:22:45.180 --> 01:22:48.220]   So hopefully, I mean, I've seen that work out well
[01:22:48.220 --> 01:22:49.060]   at DeepMind.
[01:22:50.860 --> 01:22:53.500]   There are people who, I mean, even if you just focus
[01:22:53.500 --> 01:22:56.460]   on the AI work that happens at DeepMind,
[01:22:56.460 --> 01:23:00.140]   it's been a good thing to have some people around
[01:23:00.140 --> 01:23:03.860]   doing that kind of work whose PhDs are in neuroscience
[01:23:03.860 --> 01:23:05.340]   or psychology.
[01:23:05.340 --> 01:23:10.340]   Every academic discipline has its kind of blind spots
[01:23:10.340 --> 01:23:15.360]   and kind of unfortunate obsessions
[01:23:15.360 --> 01:23:18.340]   and its metaphors and its reference points.
[01:23:18.340 --> 01:23:23.340]   And having some intellectual diversity is really healthy.
[01:23:23.340 --> 01:23:28.460]   People get each other unstuck, I think.
[01:23:28.460 --> 01:23:30.660]   I see it all the time at DeepMind.
[01:23:30.660 --> 01:23:33.100]   And I like to think that the people
[01:23:33.100 --> 01:23:35.980]   who bring some neuroscience background to the table
[01:23:35.980 --> 01:23:37.500]   are helping with that.
[01:23:37.500 --> 01:23:41.460]   - So one of my, probably the deepest passion for me,
[01:23:41.460 --> 01:23:44.180]   what I would say, maybe we kind of spoke off mic
[01:23:44.180 --> 01:23:48.140]   a little bit about it, but that I think
[01:23:48.420 --> 01:23:51.420]   is a blind spot for at least robotics and AI folks
[01:23:51.420 --> 01:23:55.620]   is human-robot interaction, human-agent interaction.
[01:23:55.620 --> 01:24:00.620]   Maybe, do you have thoughts about how we reduce
[01:24:00.620 --> 01:24:03.060]   the size of that blind spot?
[01:24:03.060 --> 01:24:07.500]   Do you also share the feeling that not enough folks
[01:24:07.500 --> 01:24:10.300]   are studying this aspect of interaction?
[01:24:10.300 --> 01:24:14.580]   - Well, I'm actually pretty intensively interested
[01:24:14.580 --> 01:24:15.920]   in this issue now.
[01:24:15.920 --> 01:24:19.320]   And there are people in my group who've actually pivoted
[01:24:19.320 --> 01:24:20.980]   pretty hard over the last few years
[01:24:20.980 --> 01:24:24.220]   from doing more traditional cognitive psychology
[01:24:24.220 --> 01:24:28.100]   and cognitive neuroscience to doing experimental work
[01:24:28.100 --> 01:24:30.260]   on human-agent interaction.
[01:24:30.260 --> 01:24:32.540]   And there are a couple of reasons
[01:24:32.540 --> 01:24:35.540]   that I'm pretty passionately interested in this.
[01:24:35.540 --> 01:24:40.540]   One is, it's kind of the outcome of having thought
[01:24:42.500 --> 01:24:46.960]   for a few years now about what we're up to.
[01:24:46.960 --> 01:24:49.400]   Like, what are we doing?
[01:24:49.400 --> 01:24:53.480]   Like, what is this AI research for?
[01:24:53.480 --> 01:24:57.060]   So what does it mean to make the world a better place?
[01:24:57.060 --> 01:24:59.040]   I think, I'm pretty sure that means
[01:24:59.040 --> 01:25:00.520]   making life better for humans.
[01:25:00.520 --> 01:25:05.800]   And so how do you make life better for humans?
[01:25:05.800 --> 01:25:10.560]   That's a proposition that when you look at it carefully
[01:25:10.560 --> 01:25:15.560]   and honestly is rather horrendously complicated,
[01:25:15.560 --> 01:25:20.880]   especially when the AI systems that you're building
[01:25:20.880 --> 01:25:25.240]   are learning systems.
[01:25:25.240 --> 01:25:29.080]   They're not, you're not programming something
[01:25:29.080 --> 01:25:31.440]   that you then introduce to the world
[01:25:31.440 --> 01:25:33.160]   and it just works as programmed,
[01:25:33.160 --> 01:25:34.860]   like Google Maps or something.
[01:25:34.860 --> 01:25:39.720]   We're building systems that learn from experience.
[01:25:39.720 --> 01:25:43.480]   So that typically leads to AI safety questions.
[01:25:43.480 --> 01:25:45.440]   How do we keep these things from getting out of control?
[01:25:45.440 --> 01:25:49.080]   How do we keep them from doing things that harm humans?
[01:25:49.080 --> 01:25:51.800]   And I mean, I hasten to say,
[01:25:51.800 --> 01:25:54.480]   I consider those hugely important issues.
[01:25:54.480 --> 01:25:58.880]   And there are large sectors of the research community
[01:25:58.880 --> 01:26:00.800]   at DeepMind and of course elsewhere,
[01:26:00.800 --> 01:26:03.480]   who are dedicated to thinking hard all day,
[01:26:03.480 --> 01:26:04.960]   every day about that.
[01:26:04.960 --> 01:26:09.600]   But there's, I guess I would say a positive side to this too
[01:26:09.600 --> 01:26:11.160]   which is to say, well,
[01:26:11.160 --> 01:26:15.880]   what would it mean to make human life better?
[01:26:15.880 --> 01:26:20.100]   And how can we imagine learning systems doing that?
[01:26:20.100 --> 01:26:23.460]   And in talking to my colleagues about that,
[01:26:23.460 --> 01:26:26.160]   we reached the initial conclusion that
[01:26:26.160 --> 01:26:30.080]   it's not sufficient to philosophize about that.
[01:26:30.080 --> 01:26:32.000]   You actually have to take into account
[01:26:32.000 --> 01:26:37.000]   how humans actually work and what humans want
[01:26:37.840 --> 01:26:40.440]   and the difficulties of knowing what humans want
[01:26:40.440 --> 01:26:43.760]   and the difficulties that arise
[01:26:43.760 --> 01:26:46.300]   when humans want different things.
[01:26:46.300 --> 01:26:50.760]   And so human agent interaction has become
[01:26:50.760 --> 01:26:55.080]   a quite intensive focus of my group lately.
[01:26:55.080 --> 01:26:58.060]   If for no other reason that,
[01:26:58.060 --> 01:27:04.040]   in order to really address that issue in an adequate way,
[01:27:04.040 --> 01:27:07.640]   you have to, I mean, psychology becomes part of the picture.
[01:27:07.640 --> 01:27:10.400]   - And so there's a few elements there.
[01:27:10.400 --> 01:27:12.000]   So if you focus on solving,
[01:27:12.000 --> 01:27:14.720]   if you focus on the robotics problem,
[01:27:14.720 --> 01:27:17.800]   let's say AGI without humans in the picture,
[01:27:17.800 --> 01:27:22.340]   you're missing fundamentally the final step.
[01:27:22.340 --> 01:27:24.600]   When you do want to help human civilization,
[01:27:24.600 --> 01:27:27.360]   you eventually have to interact with humans.
[01:27:27.360 --> 01:27:31.400]   And when you create a learning system, just as you said,
[01:27:31.400 --> 01:27:34.400]   that will eventually have to interact with humans,
[01:27:34.400 --> 01:27:39.040]   the interaction itself has to become
[01:27:39.040 --> 01:27:40.800]   part of the learning process.
[01:27:40.800 --> 01:27:43.840]   So you can't just watch, well, my sense is,
[01:27:43.840 --> 01:27:45.120]   it sounds like your sense is,
[01:27:45.120 --> 01:27:48.280]   you can't just watch humans to learn about humans.
[01:27:48.280 --> 01:27:50.280]   You have to also be part of the human world.
[01:27:50.280 --> 01:27:51.440]   You have to interact with humans.
[01:27:51.440 --> 01:27:52.280]   - Yeah, exactly.
[01:27:52.280 --> 01:27:57.280]   And I mean, then questions arise that start imperceptibly,
[01:27:57.280 --> 01:28:02.420]   but inevitably to slip beyond the realm of engineering.
[01:28:02.420 --> 01:28:06.000]   So questions like, if you have an agent
[01:28:06.000 --> 01:28:08.980]   that can do something that you can't do,
[01:28:08.980 --> 01:28:13.820]   under what conditions do you want that agent to do it?
[01:28:13.820 --> 01:28:18.820]   So if I have a robot that can play Beethoven sonatas
[01:28:18.820 --> 01:28:29.700]   better than any human, in the sense that the sensitivity,
[01:28:30.760 --> 01:28:33.980]   the expression is just beyond what any human,
[01:28:33.980 --> 01:28:36.340]   do I wanna listen to that?
[01:28:36.340 --> 01:28:38.820]   Do I wanna go to a concert and hear a robot play?
[01:28:38.820 --> 01:28:41.360]   These aren't engineering questions.
[01:28:41.360 --> 01:28:44.340]   These are questions about human preference
[01:28:44.340 --> 01:28:45.980]   and human culture.
[01:28:45.980 --> 01:28:47.900]   - Psychology bordering on philosophy.
[01:28:47.900 --> 01:28:48.980]   - Yeah.
[01:28:48.980 --> 01:28:50.260]   And then you start asking,
[01:28:50.260 --> 01:28:54.660]   well, even if we knew the answer to that,
[01:28:54.660 --> 01:28:57.060]   is it our place as AI engineers
[01:28:57.060 --> 01:28:59.180]   to build that into these agents?
[01:28:59.180 --> 01:29:02.140]   Probably the agents should interact with humans
[01:29:02.140 --> 01:29:05.620]   beyond the population of AI engineers
[01:29:05.620 --> 01:29:07.800]   and figure out what those humans want.
[01:29:07.800 --> 01:29:11.800]   And then when you start, I referred this the moment ago,
[01:29:11.800 --> 01:29:14.340]   but even that becomes complicated.
[01:29:14.340 --> 01:29:19.100]   Be quote, what if two humans want different things
[01:29:19.100 --> 01:29:22.380]   and you have only one agent that's able to interact with them
[01:29:22.380 --> 01:29:24.620]   and try to satisfy their preferences?
[01:29:24.620 --> 01:29:26.220]   Then you're into the realm of
[01:29:26.940 --> 01:29:31.780]   of like economics and social choice theory
[01:29:31.780 --> 01:29:33.660]   and even politics.
[01:29:33.660 --> 01:29:35.540]   So there's a sense in which,
[01:29:35.540 --> 01:29:37.980]   if you kind of follow what we're doing
[01:29:37.980 --> 01:29:39.940]   to its logical conclusion,
[01:29:39.940 --> 01:29:44.940]   then it goes beyond questions of engineering and technology
[01:29:44.940 --> 01:29:49.460]   and starts to shade in perceptibly into questions about
[01:29:49.460 --> 01:29:51.660]   what kind of society do you want?
[01:29:51.660 --> 01:29:55.740]   And actually that, once that dawned on me,
[01:29:55.740 --> 01:29:57.300]   I actually felt,
[01:29:57.300 --> 01:29:59.820]   I don't know what the right word is,
[01:29:59.820 --> 01:30:03.020]   quite refreshed in my involvement in AI research.
[01:30:03.020 --> 01:30:06.300]   It was almost like building this kind of stuff
[01:30:06.300 --> 01:30:08.300]   is gonna lead us back to asking
[01:30:08.300 --> 01:30:10.740]   really fundamental questions about,
[01:30:10.740 --> 01:30:16.700]   what's the good life and who gets to decide?
[01:30:16.700 --> 01:30:21.700]   And bringing in viewpoints from multiple sub-communities
[01:30:23.820 --> 01:30:26.340]   to help us shape the way that we live.
[01:30:26.340 --> 01:30:32.700]   It started making me feel like doing AI research
[01:30:32.700 --> 01:30:35.180]   in a fully responsible way
[01:30:35.180 --> 01:30:42.860]   could potentially lead to a kind of cultural renewal.
[01:30:42.860 --> 01:30:47.860]   - Yeah, it's the way to understand human beings
[01:30:47.860 --> 01:30:50.340]   at the individual, the societal level.
[01:30:50.340 --> 01:30:52.900]   It may become a way to answer all the
[01:30:52.900 --> 01:30:54.860]   silly human questions of the meaning of life
[01:30:54.860 --> 01:30:57.100]   and all those kinds of things.
[01:30:57.100 --> 01:30:58.100]   - Even if it doesn't give us a way
[01:30:58.100 --> 01:30:59.260]   of answering those questions,
[01:30:59.260 --> 01:31:02.980]   it may force us back to thinking about them.
[01:31:02.980 --> 01:31:03.820]   - Thinking about them.
[01:31:03.820 --> 01:31:07.700]   - And it might restore a certain, I don't know,
[01:31:07.700 --> 01:31:10.500]   a certain depth to,
[01:31:10.500 --> 01:31:15.500]   or even dare I say, spirituality to the world.
[01:31:15.500 --> 01:31:19.420]   I don't know, maybe that's too grandiose.
[01:31:19.420 --> 01:31:21.060]   - Well, I'm with you.
[01:31:21.060 --> 01:31:26.060]   I think AI will be the philosophy of the 21st century,
[01:31:26.060 --> 01:31:28.980]   the way which will open the door.
[01:31:28.980 --> 01:31:32.420]   I think a lot of AI researchers are afraid to open that door
[01:31:32.420 --> 01:31:35.620]   of exploring the beautiful richness
[01:31:35.620 --> 01:31:39.500]   of the human-agent interaction, human-AI interaction.
[01:31:39.500 --> 01:31:42.340]   I'm really happy that somebody like you
[01:31:42.340 --> 01:31:44.540]   have opened that door.
[01:31:44.540 --> 01:31:49.460]   - One thing I often think about is the usual schema
[01:31:49.460 --> 01:31:54.460]   for thinking about human-agent interaction
[01:31:54.460 --> 01:31:56.220]   is this kind of dystopian,
[01:31:56.220 --> 01:32:00.540]   oh, our robot overlords.
[01:32:00.540 --> 01:32:03.580]   And again, I hasten to say AI safety is hugely important.
[01:32:03.580 --> 01:32:06.500]   And I'm not saying we shouldn't be thinking
[01:32:06.500 --> 01:32:09.620]   about those risks, totally on board for that.
[01:32:09.620 --> 01:32:10.460]   But there's,
[01:32:10.460 --> 01:32:14.100]   having said that,
[01:32:14.100 --> 01:32:18.940]   what often follows for me is the thought
[01:32:18.940 --> 01:32:23.060]   that there's another kind of narrative
[01:32:23.060 --> 01:32:28.060]   that might be relevant, which is when we think of humans
[01:32:28.060 --> 01:32:33.340]   gaining more and more information about human life,
[01:32:33.340 --> 01:32:37.020]   the narrative there is usually that they gain
[01:32:37.020 --> 01:32:40.780]   more and more wisdom and they get closer to enlightenment
[01:32:40.780 --> 01:32:43.860]   and they become more benevolent.
[01:32:43.860 --> 01:32:45.540]   Like the Buddha is like,
[01:32:45.540 --> 01:32:47.380]   that's a totally different narrative.
[01:32:47.420 --> 01:32:50.460]   And why isn't it the case that we imagine
[01:32:50.460 --> 01:32:52.540]   that the AI systems that we're creating
[01:32:52.540 --> 01:32:54.580]   are just gonna, they're gonna figure out more and more
[01:32:54.580 --> 01:32:55.700]   about the way the world works
[01:32:55.700 --> 01:32:56.860]   and the way that humans interact
[01:32:56.860 --> 01:32:59.220]   and they'll become beneficent.
[01:32:59.220 --> 01:33:00.820]   I'm not saying that will happen.
[01:33:00.820 --> 01:33:05.460]   I don't honestly expect that to happen
[01:33:05.460 --> 01:33:08.860]   without some careful, setting things up very carefully.
[01:33:08.860 --> 01:33:11.780]   But it's another way things could go, right?
[01:33:11.780 --> 01:33:13.860]   - Yeah, and I would even push back on that.
[01:33:14.180 --> 01:33:19.180]   I personally believe that the most trajectories,
[01:33:19.180 --> 01:33:25.460]   natural human trajectories will lead us towards progress.
[01:33:25.460 --> 01:33:28.420]   So for me, there is a kind of sense
[01:33:28.420 --> 01:33:30.860]   that most trajectories in AI development
[01:33:30.860 --> 01:33:32.580]   will lead us into trouble.
[01:33:32.580 --> 01:33:37.180]   To me, and we over-focus on the worst case.
[01:33:37.180 --> 01:33:38.500]   It's like in computer science,
[01:33:38.500 --> 01:33:40.220]   theoretical computer science has been
[01:33:40.220 --> 01:33:42.100]   this focus on worst case analysis.
[01:33:42.100 --> 01:33:45.220]   There's something appealing to our human mind
[01:33:45.220 --> 01:33:47.660]   at some lowest level to be,
[01:33:47.660 --> 01:33:50.260]   I mean, we don't wanna be eaten by the tiger, I guess.
[01:33:50.260 --> 01:33:52.340]   So we wanna do the worst case analysis.
[01:33:52.340 --> 01:33:55.700]   But the reality is that shouldn't stop us
[01:33:55.700 --> 01:33:58.660]   from actually building out all the other trajectories
[01:33:58.660 --> 01:34:02.060]   which are potentially leading to all the positive worlds,
[01:34:02.060 --> 01:34:04.580]   all the enlightenment.
[01:34:04.580 --> 01:34:05.740]   There's a book, "Enlightenment Now"
[01:34:05.740 --> 01:34:07.020]   with Steven Pinker and so on.
[01:34:07.020 --> 01:34:09.700]   This is looking generally at human progress.
[01:34:09.700 --> 01:34:12.300]   And there's so many ways that human progress
[01:34:12.300 --> 01:34:13.900]   can happen with AI.
[01:34:13.900 --> 01:34:16.340]   And I think you have to do that research.
[01:34:16.340 --> 01:34:17.380]   You have to do that work.
[01:34:17.380 --> 01:34:20.740]   You have to do the, not just the AI safety work
[01:34:20.740 --> 01:34:23.500]   of the one worst case analysis, how do we prevent that?
[01:34:23.500 --> 01:34:27.580]   But the actual tools and the glue
[01:34:27.580 --> 01:34:31.340]   and the mechanisms of human AI interaction
[01:34:31.340 --> 01:34:34.180]   that would lead to all the positive actions that can go.
[01:34:34.180 --> 01:34:35.020]   - Yeah, right.
[01:34:35.020 --> 01:34:36.540]   - It's a super exciting area, right?
[01:34:36.540 --> 01:34:38.340]   - Yeah, we should be spending,
[01:34:38.340 --> 01:34:40.820]   we should be spending a lot of our time saying
[01:34:40.820 --> 01:34:42.860]   what can go wrong?
[01:34:42.860 --> 01:34:47.860]   I think it's harder to see that there's work to be done
[01:34:47.860 --> 01:34:51.540]   to bring into focus the question of what it would look like
[01:34:51.540 --> 01:34:53.020]   for things to go right.
[01:34:53.020 --> 01:34:56.460]   That's not obvious.
[01:34:56.460 --> 01:34:58.820]   And we wouldn't be doing this
[01:34:58.820 --> 01:35:01.980]   if we didn't have the sense there was huge potential.
[01:35:01.980 --> 01:35:05.940]   We're not doing this for no reason.
[01:35:05.940 --> 01:35:10.300]   We have a sense that AGI would be a major boom to humanity.
[01:35:10.300 --> 01:35:13.940]   But I think it's worth starting now,
[01:35:13.940 --> 01:35:15.860]   even when our technology is quite primitive,
[01:35:15.860 --> 01:35:19.660]   asking, well, exactly what would that mean?
[01:35:19.660 --> 01:35:21.260]   We can start now with applications
[01:35:21.260 --> 01:35:22.780]   that are already gonna make the world a better place,
[01:35:22.780 --> 01:35:25.260]   like solving protein folding.
[01:35:25.260 --> 01:35:28.100]   I think this deep mind has gotten heavy
[01:35:28.100 --> 01:35:30.260]   into science applications lately,
[01:35:30.260 --> 01:35:34.620]   which I think is a wonderful, wonderful move
[01:35:34.620 --> 01:35:36.260]   for us to be making.
[01:35:36.260 --> 01:35:37.460]   But when we think about AGI,
[01:35:37.460 --> 01:35:41.100]   when we think about building fully intelligent agents
[01:35:41.100 --> 01:35:42.700]   that are gonna be able to, in a sense,
[01:35:42.700 --> 01:35:44.060]   do whatever they want,
[01:35:44.060 --> 01:35:46.980]   we should start thinking about
[01:35:46.980 --> 01:35:49.220]   what do we want them to want?
[01:35:49.220 --> 01:35:52.460]   What kind of world do we wanna live in?
[01:35:52.460 --> 01:35:54.540]   That's not an easy question.
[01:35:54.540 --> 01:35:56.940]   And I think we just need to start working on it.
[01:35:56.940 --> 01:35:58.780]   - And even on the path to sort of,
[01:35:58.780 --> 01:36:00.100]   it doesn't have to be AGI,
[01:36:00.100 --> 01:36:02.500]   but just intelligent agents that interact with us
[01:36:02.500 --> 01:36:06.460]   and help us enrich our own existence on social networks,
[01:36:06.460 --> 01:36:09.060]   for example, on recommender systems, various intelligent,
[01:36:09.060 --> 01:36:10.580]   there's so much interesting interaction
[01:36:10.580 --> 01:36:12.300]   that's yet to be understood and studied.
[01:36:12.300 --> 01:36:14.740]   And how do you create,
[01:36:14.740 --> 01:36:19.500]   I mean, Twitter is struggling with this very idea,
[01:36:19.500 --> 01:36:21.460]   how do you create AI systems
[01:36:21.460 --> 01:36:24.420]   that increase the quality and the health of a conversation?
[01:36:24.420 --> 01:36:25.260]   - For sure, yeah.
[01:36:25.260 --> 01:36:28.540]   - That's a beautiful, beautiful human psychology question.
[01:36:28.540 --> 01:36:29.780]   - And how do you do that
[01:36:29.780 --> 01:36:34.780]   without deception being involved,
[01:36:34.780 --> 01:36:37.860]   without manipulation being involved,
[01:36:37.860 --> 01:36:42.060]   maximizing human autonomy?
[01:36:42.060 --> 01:36:46.820]   And how do you make these choices in a democratic way?
[01:36:46.820 --> 01:36:50.980]   How do we face the,
[01:36:50.980 --> 01:36:53.500]   again, I'm speaking for myself here.
[01:36:53.500 --> 01:36:55.380]   How do we face the fact that
[01:36:55.380 --> 01:36:58.540]   it's a small group of people
[01:36:58.540 --> 01:37:02.260]   who have the skillset to build these kinds of systems,
[01:37:02.260 --> 01:37:08.060]   but what it means to make the world a better place
[01:37:08.060 --> 01:37:11.180]   is something that we all have to be talking about.
[01:37:11.180 --> 01:37:16.180]   - Yeah, the world that we're trying to make a better place
[01:37:16.180 --> 01:37:20.300]   includes a huge variety of different kinds of people.
[01:37:20.300 --> 01:37:22.020]   - Yeah, how do we cope with that?
[01:37:22.020 --> 01:37:25.260]   This is a problem that has been discussed
[01:37:25.260 --> 01:37:27.660]   in gory, extensive detail
[01:37:27.660 --> 01:37:30.740]   in social choice theory.
[01:37:30.740 --> 01:37:32.900]   One thing I'm really enjoying
[01:37:32.900 --> 01:37:35.180]   about the recent direction work has taken
[01:37:35.180 --> 01:37:36.900]   in some parts of my team is that,
[01:37:36.900 --> 01:37:38.580]   yeah, we're reading the AI literature,
[01:37:38.580 --> 01:37:39.940]   we're reading the neuroscience literature,
[01:37:39.940 --> 01:37:42.940]   but we've also started reading economics
[01:37:42.940 --> 01:37:44.820]   and, as I mentioned, social choice theory,
[01:37:44.820 --> 01:37:45.940]   even some political theory,
[01:37:45.940 --> 01:37:50.380]   because it turns out that it all becomes relevant.
[01:37:50.380 --> 01:37:51.580]   It all becomes relevant.
[01:37:51.580 --> 01:37:55.620]   But at the same time,
[01:37:55.620 --> 01:38:00.140]   we've been trying not to write philosophy papers, right?
[01:38:00.140 --> 01:38:01.980]   We've been trying not to write physician papers.
[01:38:01.980 --> 01:38:03.780]   We're trying to figure out ways
[01:38:03.780 --> 01:38:05.740]   of doing actual empirical research
[01:38:05.740 --> 01:38:07.780]   that kind of take the first small steps
[01:38:07.780 --> 01:38:10.820]   to thinking about what it really means
[01:38:10.820 --> 01:38:14.660]   for humans with all of their complexity and contradiction
[01:38:14.660 --> 01:38:19.660]   and paradox to be brought into contact
[01:38:19.660 --> 01:38:22.660]   with these AI systems in a way
[01:38:22.660 --> 01:38:25.500]   that really makes the world a better place.
[01:38:25.500 --> 01:38:27.500]   - And often reinforcement learning frameworks
[01:38:27.500 --> 01:38:31.580]   actually kind of allow you to do that machine learning.
[01:38:31.580 --> 01:38:33.540]   And so that's the exciting thing about AI
[01:38:33.540 --> 01:38:37.220]   is it allows you to reduce the unsolvable problem,
[01:38:37.220 --> 01:38:38.180]   philosophical problem,
[01:38:38.180 --> 01:38:41.660]   into something more concrete that you can get a hold of.
[01:38:41.660 --> 01:38:43.860]   - Yeah, and it allows you to kind of define the problem
[01:38:43.860 --> 01:38:48.860]   in some way that allows for growth in the system
[01:38:48.860 --> 01:38:51.100]   that's sort of, you know,
[01:38:51.100 --> 01:38:54.060]   you're not responsible for the details, right?
[01:38:54.060 --> 01:38:56.660]   You say, this is generally what I want you to do.
[01:38:56.660 --> 01:38:59.540]   And then learning takes care of the rest.
[01:38:59.540 --> 01:39:02.340]   Of course, the safety issues are, you know,
[01:39:02.340 --> 01:39:04.060]   arise in that context.
[01:39:04.060 --> 01:39:05.940]   But I think also some of these positive issues
[01:39:05.940 --> 01:39:06.900]   arise in that context.
[01:39:06.900 --> 01:39:09.140]   What would it mean for an AI system
[01:39:09.140 --> 01:39:12.660]   to really come to understand what humans want?
[01:39:12.660 --> 01:39:18.980]   And, you know, with all of the subtleties of that, right?
[01:39:18.980 --> 01:39:23.980]   You know, humans want help with certain things.
[01:39:24.740 --> 01:39:27.500]   But they don't want everything done for them, right?
[01:39:27.500 --> 01:39:30.620]   There is part of the satisfaction that humans get from life
[01:39:30.620 --> 01:39:32.780]   is in accomplishing things.
[01:39:32.780 --> 01:39:34.740]   So if there were devices around that did everything for,
[01:39:34.740 --> 01:39:37.580]   you know, I often think of the movie "Wall-E", right?
[01:39:37.580 --> 01:39:39.420]   That's like dystopian in a totally different way.
[01:39:39.420 --> 01:39:41.380]   It's like, the machines are doing everything for us.
[01:39:41.380 --> 01:39:43.860]   That's not what we wanted.
[01:39:43.860 --> 01:39:46.780]   You know, anyway, I just, I find this, you know,
[01:39:46.780 --> 01:39:50.620]   this opens up a whole landscape of research
[01:39:50.620 --> 01:39:52.860]   that feels affirmative and exciting.
[01:39:52.860 --> 01:39:56.140]   - To me, it's one of the most exciting and it's wide open.
[01:39:56.140 --> 01:39:58.380]   We have to, 'cause it's a cool paper,
[01:39:58.380 --> 01:39:59.420]   talk about dopamine.
[01:39:59.420 --> 01:40:01.260]   - Oh yeah, okay, so I can.
[01:40:01.260 --> 01:40:02.940]   We were gonna, we were gonna,
[01:40:02.940 --> 01:40:05.100]   I was gonna give you a quick summary.
[01:40:05.100 --> 01:40:10.100]   - Yeah, a quick summary of, what's the title of the paper?
[01:40:10.100 --> 01:40:15.700]   - I think we called it a distributional code for value
[01:40:15.700 --> 01:40:18.980]   in dopamine-based reinforcement learning.
[01:40:18.980 --> 01:40:19.820]   - Yes.
[01:40:19.820 --> 01:40:24.820]   So that's another project that grew out of pure AI research.
[01:40:24.820 --> 01:40:30.340]   A number of people at DeepMind and a few other places
[01:40:30.340 --> 01:40:33.060]   had started working on a new version
[01:40:33.060 --> 01:40:37.420]   of reinforcement learning, which was defined
[01:40:37.420 --> 01:40:41.340]   by taking something in traditional reinforcement learning
[01:40:41.340 --> 01:40:42.180]   and just tweaking it.
[01:40:42.180 --> 01:40:43.460]   So the thing that they took
[01:40:43.460 --> 01:40:47.580]   from traditional reinforcement learning was a value signal.
[01:40:47.580 --> 01:40:50.260]   So at the center of reinforcement learning,
[01:40:50.260 --> 01:40:53.260]   at least most algorithms, is some representation
[01:40:53.260 --> 01:40:54.860]   of how well things are going,
[01:40:54.860 --> 01:40:58.300]   your expected cumulative future reward.
[01:40:58.300 --> 01:41:01.900]   And that's usually represented as a single number.
[01:41:01.900 --> 01:41:04.940]   So if you imagine a gambler in a casino
[01:41:04.940 --> 01:41:07.620]   and the gambler's thinking, well,
[01:41:07.620 --> 01:41:09.140]   I have this probability of winning
[01:41:09.140 --> 01:41:10.220]   such and such an amount of money,
[01:41:10.220 --> 01:41:11.500]   and I have this probability of losing
[01:41:11.500 --> 01:41:13.780]   such and such an amount of money,
[01:41:13.780 --> 01:41:16.100]   that situation would be represented as a single number,
[01:41:16.100 --> 01:41:17.780]   which is like the expected,
[01:41:17.780 --> 01:41:20.060]   the weighted average of all those outcomes.
[01:41:20.060 --> 01:41:24.260]   And this new form of reinforcement learning said,
[01:41:24.260 --> 01:41:26.980]   well, what if we generalize that
[01:41:26.980 --> 01:41:28.660]   to a distributional representation?
[01:41:28.660 --> 01:41:31.340]   So now we think of the gambler as literally thinking,
[01:41:31.340 --> 01:41:32.780]   well, there's this probability
[01:41:32.780 --> 01:41:34.100]   that I'll win this amount of money,
[01:41:34.100 --> 01:41:35.100]   and there's this probability
[01:41:35.100 --> 01:41:36.220]   that I'll lose that amount of money,
[01:41:36.220 --> 01:41:38.380]   and we don't reduce that to a single number.
[01:41:38.380 --> 01:41:42.260]   And it had been observed through experiments,
[01:41:42.260 --> 01:41:44.180]   through just trying this out,
[01:41:44.180 --> 01:41:47.700]   that that kind of distributional representation
[01:41:47.700 --> 01:41:51.380]   really accelerated reinforcement learning
[01:41:51.380 --> 01:41:54.180]   and led to better policies.
[01:41:54.180 --> 01:41:55.420]   - What's your intuition about,
[01:41:55.420 --> 01:41:56.860]   so we're talking about rewards.
[01:41:56.860 --> 01:41:58.620]   - Yeah. - So what's your intuition
[01:41:58.620 --> 01:41:59.460]   why that is?
[01:41:59.460 --> 01:42:00.980]   Why does it depend? - Well, it's kind of
[01:42:00.980 --> 01:42:04.420]   a surprising historical note,
[01:42:04.420 --> 01:42:06.140]   at least surprised me when I learned it,
[01:42:06.140 --> 01:42:09.820]   that this had been tried out in a kind of heuristic way.
[01:42:09.820 --> 01:42:12.500]   People thought, well, gee, what would happen if we tried?
[01:42:12.580 --> 01:42:14.540]   And then it had this, empirically,
[01:42:14.540 --> 01:42:17.260]   it had this striking effect.
[01:42:17.260 --> 01:42:19.300]   And it was only then that people started thinking,
[01:42:19.300 --> 01:42:20.660]   well, gee, wait, why?
[01:42:20.660 --> 01:42:21.500]   Why?
[01:42:21.500 --> 01:42:22.340]   Wait, why?
[01:42:22.340 --> 01:42:23.380]   Why is this working?
[01:42:23.380 --> 01:42:26.140]   And that's led to a series of studies
[01:42:26.140 --> 01:42:29.700]   just trying to figure out why it works, which is ongoing.
[01:42:29.700 --> 01:42:31.740]   But one thing that's already clear from that research
[01:42:31.740 --> 01:42:34.300]   is that one reason that it helps
[01:42:34.300 --> 01:42:38.260]   is that it drives richer representation learning.
[01:42:39.380 --> 01:42:43.020]   So if you imagine two situations
[01:42:43.020 --> 01:42:45.260]   that have the same expected value,
[01:42:45.260 --> 01:42:47.260]   the same kind of weighted average value,
[01:42:47.260 --> 01:42:51.260]   standard deep reinforcement learning algorithms
[01:42:51.260 --> 01:42:53.460]   are going to take those two situations
[01:42:53.460 --> 01:42:54.980]   and kind of, in terms of the way
[01:42:54.980 --> 01:42:56.420]   they're represented internally,
[01:42:56.420 --> 01:42:58.140]   they're gonna squeeze them together.
[01:42:58.140 --> 01:43:02.540]   Because the thing that you're trying to represent,
[01:43:02.540 --> 01:43:04.160]   which is their expected value, is the same.
[01:43:04.160 --> 01:43:06.220]   So all the way through the system,
[01:43:06.220 --> 01:43:08.380]   things are gonna be mushed together.
[01:43:08.380 --> 01:43:11.020]   But what if those two situations
[01:43:11.020 --> 01:43:13.920]   actually have different value distributions?
[01:43:13.920 --> 01:43:16.880]   They have the same average value,
[01:43:16.880 --> 01:43:19.860]   but they have different distributions of value.
[01:43:19.860 --> 01:43:22.300]   In that situation, distributional learning
[01:43:22.300 --> 01:43:25.060]   will maintain the distinction between these two things.
[01:43:25.060 --> 01:43:26.820]   So to make a long story short,
[01:43:26.820 --> 01:43:30.000]   distributional learning can keep things separate
[01:43:30.000 --> 01:43:32.140]   in the internal representation
[01:43:32.140 --> 01:43:35.100]   that might otherwise be conflated or squished together.
[01:43:35.100 --> 01:43:36.380]   And maintaining those distinctions
[01:43:36.380 --> 01:43:39.060]   can be useful when the system
[01:43:39.060 --> 01:43:41.100]   is now faced with some other task
[01:43:41.100 --> 01:43:43.260]   where the distinction is important.
[01:43:43.260 --> 01:43:44.540]   - If we look at the optimistic
[01:43:44.540 --> 01:43:46.540]   and pessimistic dopamine neurons.
[01:43:46.540 --> 01:43:49.560]   So first of all, what is dopamine?
[01:43:49.560 --> 01:43:53.260]   - Oh, right. - And why is this,
[01:43:53.260 --> 01:43:56.260]   why is this at all useful to think about
[01:43:56.260 --> 01:44:00.740]   in the artificial intelligence sense?
[01:44:00.740 --> 01:44:04.180]   But what do we know about dopamine in the human brain?
[01:44:04.180 --> 01:44:07.460]   What is it, why is it useful, why is it interesting?
[01:44:07.460 --> 01:44:09.380]   What does it have to do with the prefrontal cortex
[01:44:09.380 --> 01:44:10.300]   and learning in general?
[01:44:10.300 --> 01:44:15.300]   - Yeah, so, well, this is also a case
[01:44:15.300 --> 01:44:19.660]   where there's a huge amount of detail and debate.
[01:44:19.660 --> 01:44:24.660]   But one currently prevailing idea
[01:44:24.660 --> 01:44:29.060]   is that the function of this neurotransmitter dopamine
[01:44:29.060 --> 01:44:33.460]   resembles a particular component
[01:44:33.460 --> 01:44:36.860]   of standard reinforcement learning algorithms,
[01:44:36.860 --> 01:44:39.860]   which is called the reward prediction error.
[01:44:39.860 --> 01:44:41.580]   So I was talking a moment ago
[01:44:41.580 --> 01:44:44.200]   about these value representations.
[01:44:44.200 --> 01:44:45.160]   How do you learn them?
[01:44:45.160 --> 01:44:46.880]   How do you update them based on experience?
[01:44:46.880 --> 01:44:51.820]   Well, if you made some prediction about future reward,
[01:44:51.820 --> 01:44:54.420]   and then you get more reward than you were expecting,
[01:44:54.420 --> 01:44:55.980]   then probably retrospectively,
[01:44:55.980 --> 01:45:00.700]   you wanna go back and increase the value representation
[01:45:00.700 --> 01:45:03.780]   that you attached to that earlier situation.
[01:45:03.780 --> 01:45:06.140]   If you got less reward than you were expecting,
[01:45:06.140 --> 01:45:08.460]   you should probably decrement that estimate.
[01:45:08.460 --> 01:45:10.220]   - And that's the process of temporal difference.
[01:45:10.220 --> 01:45:11.960]   - Exactly, this is the central mechanism
[01:45:11.960 --> 01:45:12.820]   of temporal difference learning,
[01:45:12.820 --> 01:45:15.180]   which is one of several kind of,
[01:45:15.180 --> 01:45:16.620]   you know, kind of back,
[01:45:16.620 --> 01:45:20.360]   sort of the backbone of our armamentarium in RL.
[01:45:20.360 --> 01:45:22.540]   And it was this connection
[01:45:22.540 --> 01:45:25.900]   between the reward prediction error and dopamine
[01:45:25.900 --> 01:45:30.900]   was made, you know, in the 1990s.
[01:45:30.900 --> 01:45:33.420]   And there's been a huge amount of research
[01:45:33.420 --> 01:45:35.860]   that, you know, seems to back it up.
[01:45:35.860 --> 01:45:37.300]   Dopamine may be doing other things,
[01:45:37.300 --> 01:45:39.860]   but this is clearly, at least roughly,
[01:45:39.860 --> 01:45:42.420]   one of the things that it's doing.
[01:45:42.420 --> 01:45:45.100]   But the usual idea was that dopamine
[01:45:45.100 --> 01:45:48.020]   was representing these reward prediction errors,
[01:45:48.020 --> 01:45:51.140]   again, in this like kind of single number way,
[01:45:52.860 --> 01:45:56.740]   representing your surprise with a single number.
[01:45:56.740 --> 01:45:58.540]   And in distributional reinforcement learning,
[01:45:58.540 --> 01:46:02.820]   this kind of new elaboration of the standard approach,
[01:46:02.820 --> 01:46:06.100]   it's not only the value function
[01:46:06.100 --> 01:46:08.500]   that's represented as a single number,
[01:46:08.500 --> 01:46:10.980]   it's also the reward prediction error.
[01:46:10.980 --> 01:46:15.980]   And so what happened was that Will Dabney,
[01:46:15.980 --> 01:46:17.380]   one of my collaborators,
[01:46:17.380 --> 01:46:19.280]   who was one of the first people to work
[01:46:19.280 --> 01:46:22.340]   on distributional temporal difference learning,
[01:46:22.340 --> 01:46:25.780]   talked to a guy in my group, Zeb Kurth-Nelson,
[01:46:25.780 --> 01:46:27.700]   who's a computational neuroscientist,
[01:46:27.700 --> 01:46:29.580]   and said, "Gee, you know, is it possible
[01:46:29.580 --> 01:46:31.740]   that dopamine might be doing something
[01:46:31.740 --> 01:46:33.460]   like this distributional coding thing?"
[01:46:33.460 --> 01:46:36.000]   And they started looking at what was in the literature,
[01:46:36.000 --> 01:46:37.100]   and then they brought me in,
[01:46:37.100 --> 01:46:39.220]   and we started talking to Nao Uchida,
[01:46:39.220 --> 01:46:42.140]   and we came up with some specific predictions about,
[01:46:42.140 --> 01:46:43.540]   you know, if the brain is using
[01:46:43.540 --> 01:46:45.180]   this kind of distributional coding,
[01:46:45.180 --> 01:46:47.380]   then in the tasks that Nao has studied,
[01:46:47.380 --> 01:46:49.340]   you should see this, this, this, and this.
[01:46:49.340 --> 01:46:50.660]   And that's where the paper came from.
[01:46:50.660 --> 01:46:53.580]   We kind of enumerated a set of predictions,
[01:46:53.580 --> 01:46:56.440]   all of which ended up being fairly clearly confirmed,
[01:46:56.440 --> 01:47:00.780]   and all of which leads to at least some initial indication
[01:47:00.780 --> 01:47:02.220]   that the brain might be doing something
[01:47:02.220 --> 01:47:03.460]   like this distributional coding,
[01:47:03.460 --> 01:47:06.820]   that dopamine might be representing surprise signals
[01:47:06.820 --> 01:47:10.020]   in a way that is not just collapsing everything
[01:47:10.020 --> 01:47:12.220]   to a single number, but instead is kind of respecting
[01:47:12.220 --> 01:47:16.660]   the variety of future outcomes, if that makes sense.
[01:47:16.660 --> 01:47:19.620]   - So yeah, so that's showing, suggesting possibly
[01:47:19.620 --> 01:47:23.500]   that dopamine has a really interesting representation scheme
[01:47:23.500 --> 01:47:27.700]   in the human brain for its reward signal.
[01:47:27.700 --> 01:47:29.020]   - Exactly. - That's fascinating.
[01:47:29.020 --> 01:47:31.780]   It's just, that's another beautiful example
[01:47:31.780 --> 01:47:34.500]   of AI revealing something nice about neuroscience.
[01:47:34.500 --> 01:47:36.300]   Potentially, suggesting possibilities.
[01:47:36.300 --> 01:47:38.900]   - Well, you never know, so the minute you publish a paper
[01:47:38.900 --> 01:47:40.940]   like that, the next thing you think is,
[01:47:40.940 --> 01:47:43.700]   "I hope that replicates, like, I hope we see
[01:47:43.700 --> 01:47:44.980]   "that same thing in other data sets."
[01:47:44.980 --> 01:47:47.420]   But of course, several labs now
[01:47:47.420 --> 01:47:49.180]   are doing the follow-up experiment,
[01:47:49.180 --> 01:47:50.020]   so we'll know soon.
[01:47:50.020 --> 01:47:50.900]   - That'd be interesting. - But it has been,
[01:47:50.900 --> 01:47:54.780]   it has been a lot of fun for us to take these ideas from AI
[01:47:54.780 --> 01:47:56.780]   and kind of bring them into neuroscience
[01:47:56.780 --> 01:47:58.940]   and see how far we can get.
[01:47:58.940 --> 01:48:01.300]   - So we kind of talked about it a little bit,
[01:48:01.300 --> 01:48:03.980]   but where do you see the field of neuroscience
[01:48:03.980 --> 01:48:07.740]   and artificial intelligence heading broadly?
[01:48:07.740 --> 01:48:12.540]   Like, what are the possible exciting areas
[01:48:12.540 --> 01:48:16.340]   that you can see breakthroughs in the next, let's get crazy,
[01:48:16.340 --> 01:48:20.020]   not just three or five years, but next 10, 20, 30 years
[01:48:20.020 --> 01:48:27.300]   that would make you excited and perhaps you'd be part of?
[01:48:27.300 --> 01:48:31.060]   - On the neuroscience side,
[01:48:31.060 --> 01:48:34.380]   there's a great deal of interest now
[01:48:34.380 --> 01:48:36.740]   in what's going on in AI.
[01:48:36.740 --> 01:48:41.740]   And at the same time, I feel like,
[01:48:44.380 --> 01:48:49.380]   so neuroscience, especially the part of neuroscience
[01:48:49.380 --> 01:48:54.180]   that's focused on circuits and systems,
[01:48:54.180 --> 01:48:56.340]   kind of like really mechanism-focused,
[01:48:56.340 --> 01:49:01.980]   there's been this explosion in new technology.
[01:49:01.980 --> 01:49:06.980]   And up until recently, the experiments
[01:49:06.980 --> 01:49:08.980]   that have exploited this technology
[01:49:08.980 --> 01:49:13.380]   have not involved a lot of interesting behavior.
[01:49:13.380 --> 01:49:15.500]   And this is for a variety of reasons,
[01:49:15.500 --> 01:49:18.740]   one of which is in order to employ
[01:49:18.740 --> 01:49:21.060]   some of these technologies, you actually have to,
[01:49:21.060 --> 01:49:23.660]   if you're studying a mouse, you have to head fix the mouse.
[01:49:23.660 --> 01:49:26.340]   In other words, you have to immobilize the mouse.
[01:49:26.340 --> 01:49:29.420]   And so it's been tricky to come up with ways
[01:49:29.420 --> 01:49:30.940]   of eliciting interesting behavior
[01:49:30.940 --> 01:49:33.500]   from a mouse that's restrained in this way.
[01:49:33.500 --> 01:49:35.700]   But people have begun to create
[01:49:35.700 --> 01:49:39.540]   very interesting solutions to this,
[01:49:39.540 --> 01:49:41.340]   like virtual reality environments
[01:49:41.340 --> 01:49:43.220]   where the animal can kind of move a trackball
[01:49:43.220 --> 01:49:48.220]   and as people have kind of begun to explore
[01:49:48.220 --> 01:49:50.300]   what you can do with these technologies,
[01:49:50.300 --> 01:49:52.860]   I feel like more and more people are asking,
[01:49:52.860 --> 01:49:55.780]   well, let's try to bring behavior into the picture.
[01:49:55.780 --> 01:49:58.260]   Let's try to like reintroduce behavior,
[01:49:58.260 --> 01:50:01.060]   which was supposed to be what this whole thing was about.
[01:50:01.060 --> 01:50:06.660]   And I'm hoping that those two trends,
[01:50:06.660 --> 01:50:10.100]   the kind of growing interest in behavior
[01:50:10.180 --> 01:50:15.020]   and the widespread interest in what's going on in AI,
[01:50:15.020 --> 01:50:18.540]   will come together to kind of open a new chapter
[01:50:18.540 --> 01:50:22.780]   in neuroscience research where there's a kind of
[01:50:22.780 --> 01:50:26.700]   a rebirth of interest in the structure of behavior
[01:50:26.700 --> 01:50:28.420]   and its underlying substrates,
[01:50:28.420 --> 01:50:32.100]   but that that research is being informed
[01:50:32.100 --> 01:50:34.460]   by computational mechanisms
[01:50:34.460 --> 01:50:36.460]   that we're coming to understand in AI.
[01:50:36.460 --> 01:50:39.620]   If we can do that, then we might be taking
[01:50:39.620 --> 01:50:42.540]   a step closer to this utopian future
[01:50:42.540 --> 01:50:43.980]   that we were talking about earlier,
[01:50:43.980 --> 01:50:45.540]   where there's really no distinction
[01:50:45.540 --> 01:50:47.980]   between psychology and neuroscience.
[01:50:47.980 --> 01:50:51.580]   Neuroscience is about studying the mechanisms
[01:50:51.580 --> 01:50:55.340]   that underlie whatever it is the brain is for,
[01:50:55.340 --> 01:50:56.980]   and what is the brain for?
[01:50:56.980 --> 01:50:58.420]   It's for behavior.
[01:50:58.420 --> 01:51:03.060]   I feel like we could maybe take a step toward that now
[01:51:03.060 --> 01:51:05.160]   if people are motivated in the right way.
[01:51:05.160 --> 01:51:08.700]   You also asked about AI.
[01:51:08.700 --> 01:51:10.380]   So that was a neuroscience question.
[01:51:10.380 --> 01:51:12.140]   - You said neuroscience, that's right.
[01:51:12.140 --> 01:51:13.700]   And especially a place like DeepMind,
[01:51:13.700 --> 01:51:15.220]   I'm interested in both branches.
[01:51:15.220 --> 01:51:18.660]   So what about the engineering of intelligence systems?
[01:51:18.660 --> 01:51:24.860]   - I think one of the key challenges
[01:51:24.860 --> 01:51:28.700]   that a lot of people are seeing now in AI
[01:51:28.700 --> 01:51:33.140]   is to build systems that have the kind of flexibility,
[01:51:33.140 --> 01:51:38.580]   and the kind of flexibility that humans have in two senses.
[01:51:38.580 --> 01:51:41.860]   One is that humans can be good at many things.
[01:51:41.860 --> 01:51:44.300]   They're not just expert at one thing.
[01:51:44.300 --> 01:51:45.620]   And they're also flexible in the sense
[01:51:45.620 --> 01:51:49.660]   that they can switch between things very easily,
[01:51:49.660 --> 01:51:52.060]   and they can pick up new things very quickly
[01:51:52.060 --> 01:51:57.060]   because they very ably see what a new task has in common
[01:51:57.060 --> 01:51:59.380]   with other things that they've done.
[01:51:59.380 --> 01:52:05.340]   And that's something that our AI systems
[01:52:05.340 --> 01:52:08.020]   just blatantly do not have.
[01:52:09.020 --> 01:52:11.300]   There are some people who like to argue
[01:52:11.300 --> 01:52:13.660]   that deep learning and deep RL
[01:52:13.660 --> 01:52:16.980]   are simply wrong for getting that kind of flexibility.
[01:52:16.980 --> 01:52:19.940]   I don't share that belief,
[01:52:19.940 --> 01:52:22.500]   but the simpler fact of the matter
[01:52:22.500 --> 01:52:23.740]   is we're not building things yet
[01:52:23.740 --> 01:52:25.380]   that do have that kind of flexibility.
[01:52:25.380 --> 01:52:28.580]   And I think the attention of a large part
[01:52:28.580 --> 01:52:31.380]   of the AI community is starting to pivot to that question.
[01:52:31.380 --> 01:52:32.340]   How do we get that?
[01:52:32.340 --> 01:52:37.980]   That's gonna lead to a focus on abstraction.
[01:52:37.980 --> 01:52:41.740]   It's gonna lead to a focus on what in psychology
[01:52:41.740 --> 01:52:43.540]   we call cognitive control,
[01:52:43.540 --> 01:52:45.840]   which is the ability to switch between tasks,
[01:52:45.840 --> 01:52:49.220]   the ability to quickly put together a program of behavior
[01:52:49.220 --> 01:52:51.720]   that you've never executed before,
[01:52:51.720 --> 01:52:55.260]   but you know makes sense for a particular set of demands.
[01:52:55.260 --> 01:52:59.100]   It's very closely related to what the prefrontal cortex does
[01:52:59.100 --> 01:53:01.060]   on the neuroscience side.
[01:53:01.060 --> 01:53:05.380]   So I think it's gonna be an interesting new chapter.
[01:53:05.380 --> 01:53:07.380]   - So that's the reasoning side and cognition side,
[01:53:07.380 --> 01:53:10.540]   but let me ask the over romanticized question.
[01:53:10.540 --> 01:53:13.660]   Do you think we'll ever engineer an AGI system
[01:53:13.660 --> 01:53:17.140]   that we humans would be able to love
[01:53:17.140 --> 01:53:18.640]   and then would love us back?
[01:53:18.640 --> 01:53:23.060]   So have that level and depth of connection.
[01:53:23.060 --> 01:53:27.860]   - I love that question.
[01:53:27.860 --> 01:53:31.980]   And it relates closely to things
[01:53:31.980 --> 01:53:33.940]   that I've been thinking about a lot lately,
[01:53:33.940 --> 01:53:36.100]   in the context of this human AI research.
[01:53:36.100 --> 01:53:40.020]   There's social psychology research,
[01:53:40.020 --> 01:53:44.940]   in particular by Susan Fisk at Princeton,
[01:53:44.940 --> 01:53:47.140]   in the department where I used to work,
[01:53:47.140 --> 01:53:52.740]   where she dissects human attitudes
[01:53:52.740 --> 01:53:56.580]   toward other humans into a sort of two-dimensional,
[01:53:56.580 --> 01:53:59.940]   two-dimensional scheme.
[01:53:59.940 --> 01:54:03.980]   And one dimension is about ability.
[01:54:03.980 --> 01:54:08.280]   How able, how capable is this other person?
[01:54:08.280 --> 01:54:12.740]   But the other dimension is warmth.
[01:54:12.740 --> 01:54:15.420]   So you can imagine another person
[01:54:15.420 --> 01:54:18.640]   who's very skilled and capable, but is very cold.
[01:54:18.640 --> 01:54:23.460]   And you wouldn't really highly,
[01:54:23.460 --> 01:54:24.900]   you might have some reservations
[01:54:24.900 --> 01:54:26.100]   about that other person.
[01:54:26.100 --> 01:54:30.020]   But there's also a kind of reservation
[01:54:30.020 --> 01:54:31.980]   that we might have about another person
[01:54:31.980 --> 01:54:35.860]   who elicits in us or displays a lot of human warmth,
[01:54:35.860 --> 01:54:38.820]   but is not good at getting things done.
[01:54:38.820 --> 01:54:45.020]   We reserve our greatest esteem, really,
[01:54:45.020 --> 01:54:47.980]   for people who are both highly capable
[01:54:47.980 --> 01:54:50.900]   and also quite warm.
[01:54:50.900 --> 01:54:53.700]   That's like the best of the best.
[01:54:53.700 --> 01:54:57.380]   This isn't a normative statement I'm making.
[01:54:57.380 --> 01:54:59.780]   This is just an empirical statement.
[01:54:59.780 --> 01:55:01.340]   This is what humans seem.
[01:55:01.340 --> 01:55:02.860]   These are the two dimensions that people seem
[01:55:02.860 --> 01:55:06.740]   to kind of like, along which people size other people up.
[01:55:06.740 --> 01:55:10.680]   And in AI research, we really focus on this capability thing.
[01:55:10.680 --> 01:55:12.700]   We want our agents to be able to do stuff.
[01:55:12.700 --> 01:55:14.740]   This thing can play Go at a superhuman level.
[01:55:14.740 --> 01:55:16.180]   That's awesome.
[01:55:16.180 --> 01:55:17.820]   But that's only one dimension.
[01:55:17.820 --> 01:55:19.340]   What about the other dimension?
[01:55:19.340 --> 01:55:23.900]   What would it mean for an AI system to be warm?
[01:55:23.900 --> 01:55:26.820]   And I don't know, maybe there are easy solutions here.
[01:55:26.820 --> 01:55:29.820]   Like we can put a face on our AI systems.
[01:55:29.820 --> 01:55:31.180]   It's cute, it has big ears.
[01:55:31.180 --> 01:55:33.060]   I mean, that's probably part of it.
[01:55:33.060 --> 01:55:36.380]   But I think it also has to do with a pattern of behavior,
[01:55:36.380 --> 01:55:40.140]   a pattern of, what would it mean for an AI system
[01:55:40.140 --> 01:55:43.380]   to display caring, compassionate behavior
[01:55:43.380 --> 01:55:47.700]   in a way that actually made us feel like it was for real?
[01:55:47.700 --> 01:55:49.900]   That we didn't feel like it was simulated.
[01:55:49.900 --> 01:55:51.900]   We didn't feel like we were being duped.
[01:55:51.900 --> 01:55:55.660]   To me, people talk about the Turing test
[01:55:55.660 --> 01:55:57.820]   or some descendant of it.
[01:55:57.820 --> 01:56:01.100]   I feel like that's the ultimate Turing test.
[01:56:01.100 --> 01:56:05.420]   Is there an AI system that can not only convince us
[01:56:05.420 --> 01:56:07.140]   that it knows how to reason
[01:56:07.140 --> 01:56:09.060]   and it knows how to interpret language,
[01:56:09.060 --> 01:56:12.660]   but that we're comfortable saying,
[01:56:12.660 --> 01:56:14.560]   yeah, that AI system's a good guy.
[01:56:14.560 --> 01:56:18.700]   - On the warmth scale, whatever warmth is,
[01:56:18.700 --> 01:56:20.820]   we kind of intuitively understand it,
[01:56:20.820 --> 01:56:25.060]   but we also wanna be able to, yeah,
[01:56:25.060 --> 01:56:29.140]   we don't understand it explicitly enough yet
[01:56:29.140 --> 01:56:30.900]   to be able to engineer it.
[01:56:30.900 --> 01:56:32.060]   - Exactly.
[01:56:32.060 --> 01:56:33.620]   - And that's an open scientific question.
[01:56:33.620 --> 01:56:35.320]   You kind of alluded to it several times
[01:56:35.320 --> 01:56:37.220]   in the human-AI interaction.
[01:56:37.220 --> 01:56:38.900]   That's a question that should be studied
[01:56:38.900 --> 01:56:42.100]   and probably one of the most important questions
[01:56:42.100 --> 01:56:43.660]   as we move to AGI.
[01:56:43.660 --> 01:56:45.980]   - We humans are so good at it.
[01:56:45.980 --> 01:56:47.660]   - Yeah, weird.
[01:56:47.660 --> 01:56:50.140]   - It's not just that we're born warm.
[01:56:50.140 --> 01:56:53.060]   I suppose some people are warmer than others
[01:56:53.060 --> 01:56:55.720]   given whatever genes they manage to inherit.
[01:56:55.720 --> 01:57:00.720]   But there are also learned skills involved.
[01:57:01.640 --> 01:57:04.760]   There are ways of communicating to other people
[01:57:04.760 --> 01:57:07.800]   that you care, that they matter to you,
[01:57:07.800 --> 01:57:09.900]   that you're enjoying interacting with them.
[01:57:09.900 --> 01:57:14.160]   And we learn these skills from one another.
[01:57:14.160 --> 01:57:16.760]   And it's not out of the question
[01:57:16.760 --> 01:57:20.080]   that we could build engineered systems.
[01:57:20.080 --> 01:57:21.480]   I think it's hopeless, as you say,
[01:57:21.480 --> 01:57:23.600]   that we could somehow hand design
[01:57:23.600 --> 01:57:26.120]   these sorts of behaviors.
[01:57:26.120 --> 01:57:27.120]   But it's not out of the question
[01:57:27.120 --> 01:57:28.480]   that we could build systems
[01:57:28.480 --> 01:57:32.760]   that kind of we instill in them
[01:57:32.760 --> 01:57:36.000]   something that sets them out in the right direction
[01:57:36.000 --> 01:57:39.080]   so that they end up learning
[01:57:39.080 --> 01:57:40.560]   what it is to interact with humans
[01:57:40.560 --> 01:57:44.200]   in a way that's gratifying to humans.
[01:57:44.200 --> 01:57:47.520]   I mean, honestly, if that's not where we're headed,
[01:57:47.520 --> 01:57:50.400]   I want out.
[01:57:50.400 --> 01:57:52.240]   (laughing)
[01:57:52.240 --> 01:57:54.960]   - I think it's exciting as a scientific problem,
[01:57:54.960 --> 01:57:56.840]   just as you described.
[01:57:56.840 --> 01:57:59.480]   I honestly don't see a better way to end it
[01:57:59.480 --> 01:58:01.160]   than talking about warmth and love.
[01:58:01.160 --> 01:58:04.000]   And Matt, I don't think I've ever had
[01:58:04.000 --> 01:58:06.000]   such a wonderful conversation
[01:58:06.000 --> 01:58:07.520]   where my questions were so bad
[01:58:07.520 --> 01:58:09.360]   and your answers were so beautiful.
[01:58:09.360 --> 01:58:10.680]   So I deeply appreciate it.
[01:58:10.680 --> 01:58:11.520]   I really enjoyed it.
[01:58:11.520 --> 01:58:13.240]   - Well, it's been very fun.
[01:58:13.240 --> 01:58:14.520]   As you can probably tell,
[01:58:14.520 --> 01:58:18.240]   there's something I like about
[01:58:18.240 --> 01:58:21.000]   kind of thinking outside the box.
[01:58:21.000 --> 01:58:22.880]   So it's good having the opportunity to do that.
[01:58:22.880 --> 01:58:23.720]   - Awesome.
[01:58:23.720 --> 01:58:25.600]   Thanks so much for doing it.
[01:58:25.600 --> 01:58:27.160]   Thanks for listening to this conversation
[01:58:27.160 --> 01:58:28.400]   with Matt Boppenegg.
[01:58:28.400 --> 01:58:30.520]   And thank you to our sponsors,
[01:58:30.520 --> 01:58:32.320]   the Jordan and Harbinger Show
[01:58:32.320 --> 01:58:36.120]   and Magic Spoon low-carb keto cereal.
[01:58:36.120 --> 01:58:38.000]   Please consider supporting this podcast
[01:58:38.000 --> 01:58:41.040]   by going to jordanharbinger.com/lex
[01:58:41.040 --> 01:58:44.920]   and also going to magicspoon.com/lex
[01:58:44.920 --> 01:58:48.240]   and using code LEX at checkout.
[01:58:48.240 --> 01:58:50.900]   Click the links, buy all the stuff.
[01:58:50.900 --> 01:58:52.840]   It's the best way to support this podcast
[01:58:52.840 --> 01:58:55.360]   and the journey I'm on in my research
[01:58:55.360 --> 01:58:57.280]   and the startup.
[01:58:57.280 --> 01:58:59.600]   If you enjoy this thing, subscribe on YouTube,
[01:58:59.600 --> 01:59:02.360]   review it with the five stars on Apple Podcasts,
[01:59:02.360 --> 01:59:05.360]   support it on Patreon, follow on Spotify
[01:59:05.360 --> 01:59:08.200]   or connect with me on Twitter @LexFriedman.
[01:59:08.200 --> 01:59:12.200]   Again, spelled miraculously without the E,
[01:59:12.200 --> 01:59:15.040]   just F-R-I-D-M-A-N.
[01:59:15.040 --> 01:59:17.080]   And now let me leave you with some words
[01:59:17.080 --> 01:59:20.800]   from urologist B.S. Samachandran.
[01:59:20.800 --> 01:59:23.320]   How can a three pound mass of jelly
[01:59:23.320 --> 01:59:25.120]   that you can hold in your palm
[01:59:25.120 --> 01:59:28.720]   imagine angels, contemplate the meaning of infinity
[01:59:28.720 --> 01:59:31.720]   and even question its own place in the cosmos?
[01:59:31.720 --> 01:59:35.660]   Especially awe-inspiring is the fact that any single brain,
[01:59:35.660 --> 01:59:38.600]   including yours, is made up of atoms
[01:59:38.600 --> 01:59:41.200]   that were forged in the hearts of countless
[01:59:41.200 --> 01:59:45.480]   far-flung stars billions of years ago.
[01:59:45.480 --> 01:59:48.320]   These particles drifted for eons and light years
[01:59:48.320 --> 01:59:53.160]   until gravity and change brought them together here now.
[01:59:53.160 --> 01:59:57.560]   These atoms now form a conglomerate, your brain,
[01:59:57.560 --> 02:00:00.840]   that can not only ponder the very stars that gave it birth,
[02:00:00.840 --> 02:00:04.160]   but can also think about its own ability to think
[02:00:04.160 --> 02:00:07.800]   and wonder about its own ability to wander.
[02:00:07.800 --> 02:00:10.640]   With the arrival of humans, it has been said,
[02:00:10.640 --> 02:00:14.600]   the universe has suddenly become conscious of itself.
[02:00:14.600 --> 02:00:18.620]   This truly is the greatest mystery of all.
[02:00:19.960 --> 02:00:23.320]   Thank you for listening and hope to see you next time.
[02:00:23.320 --> 02:00:25.900]   (upbeat music)
[02:00:25.900 --> 02:00:28.480]   (upbeat music)
[02:00:28.480 --> 02:00:38.480]   [BLANK_AUDIO]

