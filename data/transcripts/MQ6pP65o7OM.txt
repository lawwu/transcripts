
[00:00:00.000 --> 00:00:02.800]   Today we will talk about deep reinforcement learning.
[00:00:02.800 --> 00:00:12.000]   The question we would like to explore is to which degree
[00:00:12.000 --> 00:00:15.000]   we can teach systems to act,
[00:00:15.000 --> 00:00:18.680]   to perceive and act in this world from data.
[00:00:18.680 --> 00:00:25.480]   So let's take a step back and think of what is the full range of tasks
[00:00:25.480 --> 00:00:28.280]   an artificial intelligence system needs to accomplish.
[00:00:28.280 --> 00:00:29.680]   Here's the stack.
[00:00:30.280 --> 00:00:33.680]   From top to bottom, top the input, bottom output.
[00:00:33.680 --> 00:00:37.960]   The environment at the top, the world that the agent is operating in.
[00:00:37.960 --> 00:00:41.360]   Sensed by sensors,
[00:00:41.360 --> 00:00:46.680]   taking in the world outside and converting it to raw data interpretable by machines.
[00:00:46.680 --> 00:00:49.080]   Sensor data.
[00:00:49.080 --> 00:00:53.480]   And from that raw sensor data, you extract features.
[00:00:53.480 --> 00:00:57.880]   You extract structure from that data
[00:00:58.080 --> 00:01:01.280]   such that you can input it, make sense of it,
[00:01:01.280 --> 00:01:05.280]   discriminate, separate, understand the data.
[00:01:05.280 --> 00:01:12.280]   And as we discussed, you form higher and higher order representations,
[00:01:12.280 --> 00:01:14.480]   a hierarchy of representations
[00:01:14.480 --> 00:01:18.480]   based on which the machine learning techniques can then be applied.
[00:01:18.480 --> 00:01:27.080]   Once the machine learning techniques, the understanding, as I mentioned,
[00:01:27.480 --> 00:01:31.680]   converts the data into features, into higher order representations
[00:01:31.680 --> 00:01:34.480]   and into simple, actionable, useful information.
[00:01:34.480 --> 00:01:37.680]   We aggregate that information into knowledge.
[00:01:37.680 --> 00:01:40.480]   We take the pieces of knowledge extracted from the data
[00:01:40.480 --> 00:01:43.080]   through the machine learning techniques
[00:01:43.080 --> 00:01:45.680]   and build a taxonomy,
[00:01:45.680 --> 00:01:49.480]   a library of knowledge.
[00:01:49.480 --> 00:01:53.080]   And with that knowledge, we reason.
[00:01:53.080 --> 00:01:55.280]   An agent is tasked to reason
[00:01:57.280 --> 00:02:03.680]   to aggregate, to connect pieces of data seen in the recent past
[00:02:03.680 --> 00:02:07.680]   or the distant past, to make sense of the world it's operating in.
[00:02:07.680 --> 00:02:12.680]   And finally, to make a plan of how to act in that world based on its objectives,
[00:02:12.680 --> 00:02:14.480]   based on what it wants to accomplish.
[00:02:14.480 --> 00:02:20.080]   As I mentioned, a simple but commonly accepted definition of intelligence
[00:02:20.080 --> 00:02:23.880]   is a system that's able to accomplish complex goals.
[00:02:24.880 --> 00:02:27.880]   So a system that's operating in an environment in this world
[00:02:27.880 --> 00:02:32.080]   must have a goal, must have an objective function, a reward function.
[00:02:32.080 --> 00:02:35.480]   And based on that, it forms a plan and takes action.
[00:02:35.480 --> 00:02:39.280]   And because it operates in many cases in the physical world,
[00:02:39.280 --> 00:02:44.680]   it must have tools, effectors with which it applies the actions
[00:02:44.680 --> 00:02:46.080]   to change something about the world.
[00:02:46.080 --> 00:02:51.080]   That's the full stack of an artificial intelligence system
[00:02:51.080 --> 00:02:52.280]   that acts in the world.
[00:02:54.080 --> 00:02:55.280]   And the question is,
[00:02:55.280 --> 00:03:00.080]   what kind of task can such a system take on?
[00:03:00.080 --> 00:03:04.280]   What kind of task can an artificial intelligence system learn?
[00:03:04.280 --> 00:03:06.680]   As we understand AI today.
[00:03:06.680 --> 00:03:12.680]   We will talk about the advancement of deeper enforcement learning approaches
[00:03:12.680 --> 00:03:17.680]   in some of the fascinating ways it's able to take much of this stack
[00:03:17.680 --> 00:03:20.680]   and treat it as an end-to-end learning problem.
[00:03:21.480 --> 00:03:25.680]   But we look at games, we look at simple formalized worlds.
[00:03:25.680 --> 00:03:29.880]   While it's still impressive, beautiful and unprecedented accomplishments,
[00:03:29.880 --> 00:03:32.680]   it's nevertheless formal tasks.
[00:03:32.680 --> 00:03:40.680]   Can we then move beyond games and into expert tasks of medical diagnosis,
[00:03:40.680 --> 00:03:45.680]   of design and into natural language
[00:03:45.680 --> 00:03:50.280]   and finally the human level tasks of emotion, imagination.
[00:03:50.280 --> 00:03:52.880]   Consciousness.
[00:03:52.880 --> 00:04:00.080]   Let's once again review the stack in practicality,
[00:04:00.080 --> 00:04:02.080]   in the tools we have.
[00:04:02.080 --> 00:04:06.480]   The input for robots operating in the world,
[00:04:06.480 --> 00:04:09.480]   from cars to humanoid to drones,
[00:04:09.480 --> 00:04:15.480]   as LIDAR camera, radar, GPS, stereo cameras, audio microphone,
[00:04:15.480 --> 00:04:17.680]   networking for communication
[00:04:17.680 --> 00:04:19.880]   and the various ways to measure kinematics.
[00:04:20.880 --> 00:04:21.880]   With IMU.
[00:04:21.880 --> 00:04:27.880]   The raw sensory data is then processed,
[00:04:27.880 --> 00:04:30.280]   features are formed, representations are formed
[00:04:30.280 --> 00:04:33.280]   and multiple higher and higher order representations.
[00:04:33.280 --> 00:04:34.880]   That's what deep learning gets us.
[00:04:34.880 --> 00:04:37.880]   Before neural networks, before the advent of,
[00:04:37.880 --> 00:04:42.480]   before the recent successes of neural networks to go deeper
[00:04:42.480 --> 00:04:46.280]   and therefore be able to form high order representations of the data.
[00:04:46.280 --> 00:04:49.280]   That was done by experts, by human experts.
[00:04:49.480 --> 00:04:52.080]   Today, networks are able to do that.
[00:04:52.080 --> 00:04:53.880]   That's the representation piece.
[00:04:53.880 --> 00:04:56.280]   And on top of the representation piece,
[00:04:56.280 --> 00:04:59.680]   the final layers these networks are able to accomplish,
[00:04:59.680 --> 00:05:02.880]   the supervised learning tasks, the generative tasks
[00:05:02.880 --> 00:05:06.680]   and the unsupervised clustering tasks.
[00:05:06.680 --> 00:05:09.480]   Through machine learning.
[00:05:09.480 --> 00:05:12.880]   That's what we talked about a little in lecture one
[00:05:12.880 --> 00:05:16.280]   and will continue tomorrow and Wednesday.
[00:05:16.280 --> 00:05:18.880]   That's supervised learning.
[00:05:19.680 --> 00:05:22.480]   And you can think about the output of those networks
[00:05:22.480 --> 00:05:26.080]   as simple, clean, useful, valuable information.
[00:05:26.080 --> 00:05:27.480]   That's the knowledge.
[00:05:27.480 --> 00:05:33.280]   And that knowledge can be in the form of single numbers.
[00:05:33.280 --> 00:05:36.680]   It could be regression, continuous variables.
[00:05:36.680 --> 00:05:38.280]   It could be a sequence of numbers.
[00:05:38.280 --> 00:05:44.280]   It could be images, audio, sentences, text, speech.
[00:05:44.280 --> 00:05:47.880]   Once that knowledge is extracted and aggregated,
[00:05:48.080 --> 00:05:52.080]   how do we connect it in multi-resolutional ways?
[00:05:52.080 --> 00:05:55.280]   Form hierarchies of ideas, connect ideas.
[00:05:55.280 --> 00:06:00.480]   The trivial silly example is connecting images,
[00:06:00.480 --> 00:06:03.280]   activity recognition and audio, for example.
[00:06:03.280 --> 00:06:06.280]   If it looks like a duck, quacks like a duck,
[00:06:06.280 --> 00:06:07.680]   and swims like a duck,
[00:06:07.680 --> 00:06:10.680]   we do not currently have approaches
[00:06:10.680 --> 00:06:12.680]   that effectively integrate this information
[00:06:14.080 --> 00:06:16.680]   to produce a higher confidence estimate
[00:06:16.680 --> 00:06:18.280]   that is in fact a duck.
[00:06:18.280 --> 00:06:19.880]   And the planning piece,
[00:06:19.880 --> 00:06:24.480]   the task of taking the sensory information,
[00:06:24.480 --> 00:06:26.680]   fusing the sensory information,
[00:06:26.680 --> 00:06:30.880]   and making action, control and longer-term plans
[00:06:30.880 --> 00:06:32.880]   based on that information,
[00:06:32.880 --> 00:06:35.080]   as we'll discuss today,
[00:06:35.080 --> 00:06:39.080]   are more and more amenable to the learning approach,
[00:06:39.080 --> 00:06:40.480]   to the deep learning approach.
[00:06:40.480 --> 00:06:42.880]   But to date have been the most successful
[00:06:43.080 --> 00:06:45.280]   with non-learning optimization based approaches.
[00:06:45.280 --> 00:06:47.680]   Like with the several of the guest speakers we have,
[00:06:47.680 --> 00:06:51.280]   including the creator of this robot, Atlas,
[00:06:51.280 --> 00:06:52.880]   in Boston Dynamics.
[00:06:52.880 --> 00:06:55.880]   So the question,
[00:06:55.880 --> 00:06:58.680]   how much of the stack can be learned, end to end,
[00:06:58.680 --> 00:07:00.280]   from the input to the output?
[00:07:00.280 --> 00:07:02.680]   We know we can learn the representation,
[00:07:02.680 --> 00:07:04.080]   and the knowledge.
[00:07:04.080 --> 00:07:05.880]   From the representation and to knowledge,
[00:07:05.880 --> 00:07:09.280]   even with the kernel methods of SVM,
[00:07:09.280 --> 00:07:11.080]   and certainly,
[00:07:12.080 --> 00:07:14.280]   with neural networks,
[00:07:14.280 --> 00:07:18.080]   mapping from representation to information,
[00:07:18.080 --> 00:07:21.680]   has been where the primary success
[00:07:21.680 --> 00:07:24.080]   in machine learning over the past three decades has been.
[00:07:24.080 --> 00:07:28.480]   Mapping from raw sensory data to knowledge,
[00:07:28.480 --> 00:07:30.880]   that's where the success,
[00:07:30.880 --> 00:07:34.480]   the automated representation learning of deep learning,
[00:07:34.480 --> 00:07:36.280]   has been a success.
[00:07:36.280 --> 00:07:38.680]   Going straight from raw data to knowledge.
[00:07:38.680 --> 00:07:41.080]   The open question for us,
[00:07:41.480 --> 00:07:43.080]   today and beyond,
[00:07:43.080 --> 00:07:45.680]   is if we can expand the red box there,
[00:07:45.680 --> 00:07:47.280]   of what can be learned end to end,
[00:07:47.280 --> 00:07:49.680]   from sensory data to reasoning.
[00:07:49.680 --> 00:07:52.480]   So aggregating, forming higher representations,
[00:07:52.480 --> 00:07:54.080]   of the extracted knowledge.
[00:07:54.080 --> 00:07:56.680]   And forming plans,
[00:07:56.680 --> 00:07:59.280]   and acting in this world from the raw sensory data.
[00:07:59.280 --> 00:08:02.080]   We will show the incredible fact,
[00:08:02.080 --> 00:08:03.480]   that we're able to do,
[00:08:03.480 --> 00:08:06.480]   learn exactly what's shown here, end to end,
[00:08:06.480 --> 00:08:09.280]   with deeper enforcement learning on trivial tasks,
[00:08:10.280 --> 00:08:11.680]   in a generalizable way.
[00:08:11.680 --> 00:08:13.680]   The question is whether that can,
[00:08:13.680 --> 00:08:16.280]   then move on to real world tasks,
[00:08:16.280 --> 00:08:18.080]   of autonomous vehicles,
[00:08:18.080 --> 00:08:21.080]   of humanoid robotics, and so on.
[00:08:21.080 --> 00:08:24.880]   That's the open question.
[00:08:24.880 --> 00:08:27.680]   So today, let's talk about reinforcement learning.
[00:08:27.680 --> 00:08:29.680]   There's three types of machine learning.
[00:08:29.680 --> 00:08:33.280]   Supervised,
[00:08:33.280 --> 00:08:35.880]   unsupervised,
[00:08:35.880 --> 00:08:38.280]   are the categories at the extremes,
[00:08:39.080 --> 00:08:42.080]   relative to the amount of human input,
[00:08:42.080 --> 00:08:42.880]   that's required.
[00:08:42.880 --> 00:08:44.080]   For supervised learning,
[00:08:44.080 --> 00:08:45.480]   every piece of data,
[00:08:45.480 --> 00:08:47.680]   that's used for teaching these systems,
[00:08:47.680 --> 00:08:50.480]   is first labeled by human beings.
[00:08:50.480 --> 00:08:52.680]   And unsupervised learning on the right,
[00:08:52.680 --> 00:08:55.880]   is no data is labeled by human beings.
[00:08:55.880 --> 00:08:58.680]   In between is some,
[00:08:58.680 --> 00:09:01.280]   sparse input from humans.
[00:09:01.280 --> 00:09:03.080]   Semi-supervised learning,
[00:09:03.080 --> 00:09:06.280]   is when only part of the data is provided,
[00:09:06.280 --> 00:09:08.080]   by humans, ground truth.
[00:09:08.480 --> 00:09:09.280]   And the rest,
[00:09:09.280 --> 00:09:11.680]   must be inferred, generalized by the system.
[00:09:11.680 --> 00:09:13.680]   And that's where reinforcement learning falls.
[00:09:13.680 --> 00:09:15.680]   Reinforcement learning,
[00:09:15.680 --> 00:09:18.080]   as shown there with the cats.
[00:09:18.080 --> 00:09:21.480]   As I said, every successful presentation,
[00:09:21.480 --> 00:09:22.480]   must include cats.
[00:09:22.480 --> 00:09:26.280]   They're supposed to be Pavlov's cats.
[00:09:26.280 --> 00:09:28.080]   And,
[00:09:28.080 --> 00:09:29.280]   ringing a bell,
[00:09:29.280 --> 00:09:30.680]   and every time they ring a bell,
[00:09:30.680 --> 00:09:31.680]   they're given food,
[00:09:31.680 --> 00:09:33.080]   and they learn this process.
[00:09:33.080 --> 00:09:36.880]   The goal of reinforcement learning,
[00:09:37.280 --> 00:09:39.080]   is to learn,
[00:09:39.080 --> 00:09:42.080]   from sparse reward data.
[00:09:42.080 --> 00:09:43.080]   From learn,
[00:09:43.080 --> 00:09:45.080]   from sparse supervised data.
[00:09:45.080 --> 00:09:47.080]   And take advantage of the fact,
[00:09:47.080 --> 00:09:48.280]   that in simulation,
[00:09:48.280 --> 00:09:49.480]   or in the real world,
[00:09:49.480 --> 00:09:52.080]   there is a temporal consistency to the world.
[00:09:52.080 --> 00:09:53.280]   There is a,
[00:09:53.280 --> 00:09:55.480]   temporal dynamics that follows,
[00:09:55.480 --> 00:09:56.480]   from state to state,
[00:09:56.480 --> 00:09:57.880]   to state through time.
[00:09:57.880 --> 00:10:00.280]   And so you can propagate information,
[00:10:00.280 --> 00:10:01.880]   even if the information,
[00:10:01.880 --> 00:10:03.280]   that you received about,
[00:10:03.280 --> 00:10:04.880]   the supervision,
[00:10:04.880 --> 00:10:06.480]   the ground truth is sparse.
[00:10:06.880 --> 00:10:08.280]   You can follow that information,
[00:10:08.280 --> 00:10:09.480]   back through time,
[00:10:09.480 --> 00:10:10.680]   to infer,
[00:10:10.680 --> 00:10:12.280]   something about the reality,
[00:10:12.280 --> 00:10:13.480]   of what happened before then,
[00:10:13.480 --> 00:10:16.080]   even if your reward signals were weak.
[00:10:16.080 --> 00:10:17.880]   So it's using the fact,
[00:10:17.880 --> 00:10:18.880]   that the physical world,
[00:10:18.880 --> 00:10:20.280]   devolves through time,
[00:10:20.280 --> 00:10:21.280]   in some,
[00:10:21.280 --> 00:10:23.680]   some sort of predictable way,
[00:10:23.680 --> 00:10:26.080]   to take sparse information,
[00:10:26.080 --> 00:10:26.880]   and,
[00:10:26.880 --> 00:10:28.480]   generalize it,
[00:10:28.480 --> 00:10:30.880]   over the entirety of the experience,
[00:10:30.880 --> 00:10:31.880]   that's being learned.
[00:10:31.880 --> 00:10:34.680]   So we apply this to two problems.
[00:10:34.680 --> 00:10:36.680]   Today we'll talk about deep traffic.
[00:10:37.680 --> 00:10:39.280]   As a methodology,
[00:10:39.280 --> 00:10:40.680]   as a way to introduce,
[00:10:40.680 --> 00:10:41.880]   deep reinforcement learning.
[00:10:41.880 --> 00:10:44.080]   So deep traffic is a competition,
[00:10:44.080 --> 00:10:45.680]   that we ran last year,
[00:10:45.680 --> 00:10:48.480]   and expanded significantly this year.
[00:10:48.480 --> 00:10:51.080]   And I'll talk about some of the details,
[00:10:51.080 --> 00:10:53.280]   and how the folks in this room can,
[00:10:53.280 --> 00:10:55.680]   on your smartphone today,
[00:10:55.680 --> 00:10:57.080]   or if you have a laptop,
[00:10:57.080 --> 00:10:58.880]   train an agent,
[00:10:58.880 --> 00:11:00.080]   while I'm talking.
[00:11:00.080 --> 00:11:02.280]   Training a neural network in the browser.
[00:11:02.280 --> 00:11:04.080]   Some of the things we've added,
[00:11:04.080 --> 00:11:06.280]   are we've added the capability,
[00:11:06.880 --> 00:11:08.680]   we've now turned it into a multi-agent,
[00:11:08.680 --> 00:11:10.280]   deep reinforcement learning problem.
[00:11:10.280 --> 00:11:11.880]   Where you can control up to,
[00:11:11.880 --> 00:11:13.880]   10 cars within your own network.
[00:11:13.880 --> 00:11:16.880]   Perhaps less significant,
[00:11:16.880 --> 00:11:18.480]   but pretty cool,
[00:11:18.480 --> 00:11:20.680]   is the ability to customize,
[00:11:20.680 --> 00:11:22.280]   the way the agent looks.
[00:11:22.280 --> 00:11:24.280]   So you can upload,
[00:11:24.280 --> 00:11:25.480]   and people have,
[00:11:25.480 --> 00:11:26.880]   to an absurd degree,
[00:11:26.880 --> 00:11:28.280]   have already begun doing so,
[00:11:28.280 --> 00:11:30.080]   uploading different images,
[00:11:30.080 --> 00:11:31.880]   instead of the car that's shown there.
[00:11:31.880 --> 00:11:34.880]   As long as it maintains the dimensions,
[00:11:34.880 --> 00:11:36.880]   shown here is a SpaceX rocket.
[00:11:36.880 --> 00:11:41.880]   The competition is hosted on the website,
[00:11:41.880 --> 00:11:45.280]   selfdrivingcars.mit.edu/deeptraffic.
[00:11:45.280 --> 00:11:46.880]   We'll return to this later.
[00:11:46.880 --> 00:11:50.480]   The code is on GitHub,
[00:11:50.480 --> 00:11:51.680]   with some more information,
[00:11:51.680 --> 00:11:52.680]   a starter code,
[00:11:52.680 --> 00:11:55.080]   and a paper describing,
[00:11:55.080 --> 00:11:56.880]   some of the fundamental insights,
[00:11:56.880 --> 00:12:00.880]   that will help you win at this competition,
[00:12:00.880 --> 00:12:02.080]   is an archive.
[00:12:04.280 --> 00:12:05.880]   So, from supervised learning,
[00:12:05.880 --> 00:12:06.880]   in lecture one,
[00:12:06.880 --> 00:12:08.080]   to today.
[00:12:08.080 --> 00:12:10.280]   Supervised learning,
[00:12:10.280 --> 00:12:12.680]   we can think of as memorization,
[00:12:12.680 --> 00:12:15.480]   of ground-truth data,
[00:12:15.480 --> 00:12:17.480]   in order to form representations,
[00:12:17.480 --> 00:12:19.680]   that generalizes from that ground-truth.
[00:12:19.680 --> 00:12:21.880]   Reinforcement learning,
[00:12:21.880 --> 00:12:24.080]   is, we can think of,
[00:12:24.080 --> 00:12:25.480]   as a way to brute force,
[00:12:25.480 --> 00:12:27.080]   propagate that information,
[00:12:27.080 --> 00:12:28.880]   the sparse information,
[00:12:28.880 --> 00:12:33.680]   through time,
[00:12:34.280 --> 00:12:38.280]   to assign quality reward,
[00:12:38.280 --> 00:12:42.280]   to state that does not directly have a reward.
[00:12:42.280 --> 00:12:44.480]   To make sense of this world,
[00:12:44.480 --> 00:12:47.280]   when the rewards are sparse,
[00:12:47.280 --> 00:12:48.680]   but are connected through time.
[00:12:48.680 --> 00:12:51.280]   You can think of that as reasoning.
[00:12:51.280 --> 00:12:56.080]   So, the connection through time,
[00:12:56.080 --> 00:12:58.280]   is modeled,
[00:12:58.280 --> 00:13:01.080]   in most reinforcement learning approaches,
[00:13:01.080 --> 00:13:03.080]   very simply,
[00:13:03.480 --> 00:13:04.880]   that there's an agent,
[00:13:04.880 --> 00:13:06.880]   taking an action in a state,
[00:13:06.880 --> 00:13:08.480]   and receiving a reward.
[00:13:08.480 --> 00:13:11.080]   And the agent operating in an environment,
[00:13:11.080 --> 00:13:12.680]   executes an action,
[00:13:12.680 --> 00:13:14.480]   receives an observed state,
[00:13:14.480 --> 00:13:15.480]   a new state,
[00:13:15.480 --> 00:13:16.680]   and receives a reward.
[00:13:16.680 --> 00:13:19.280]   This process continues over and over.
[00:13:19.280 --> 00:13:23.880]   In some examples,
[00:13:23.880 --> 00:13:25.280]   we can think of,
[00:13:25.280 --> 00:13:26.680]   any of the video games,
[00:13:26.680 --> 00:13:28.480]   some of which we'll talk about today,
[00:13:28.480 --> 00:13:30.480]   like Atari Breakout,
[00:13:30.480 --> 00:13:32.280]   as the environment,
[00:13:32.280 --> 00:13:33.280]   the agent,
[00:13:33.480 --> 00:13:34.480]   is the paddle.
[00:13:34.480 --> 00:13:37.680]   Each action,
[00:13:37.680 --> 00:13:39.680]   that the agent takes,
[00:13:39.680 --> 00:13:40.880]   has an influence,
[00:13:40.880 --> 00:13:42.080]   on the evolution,
[00:13:42.080 --> 00:13:43.880]   of the environment.
[00:13:43.880 --> 00:13:46.080]   And the success is measured,
[00:13:46.080 --> 00:13:47.880]   by some reward mechanism.
[00:13:47.880 --> 00:13:49.080]   In this case,
[00:13:49.080 --> 00:13:50.680]   points are given by the game.
[00:13:50.680 --> 00:13:52.280]   And every game,
[00:13:52.280 --> 00:13:55.480]   has a different point scheme,
[00:13:55.480 --> 00:13:57.080]   that must be converted,
[00:13:57.080 --> 00:13:58.080]   normalized,
[00:13:58.080 --> 00:14:00.080]   into a way that's interpretable by the system.
[00:14:00.080 --> 00:14:03.080]   And the goal is to maximize those points,
[00:14:03.080 --> 00:14:04.480]   maximize the reward.
[00:14:04.480 --> 00:14:10.880]   The continuous problem of cart-pole balancing,
[00:14:10.880 --> 00:14:12.080]   the goal is to balance the pole,
[00:14:12.080 --> 00:14:13.480]   on top of a moving cart.
[00:14:13.480 --> 00:14:15.680]   The state is the angle,
[00:14:15.680 --> 00:14:17.080]   the angular speed,
[00:14:17.080 --> 00:14:17.680]   the position,
[00:14:17.680 --> 00:14:19.080]   the horizontal velocity.
[00:14:19.080 --> 00:14:21.680]   The actions are the horizontal force,
[00:14:21.680 --> 00:14:22.680]   applied to the cart.
[00:14:22.680 --> 00:14:24.080]   And the reward,
[00:14:24.080 --> 00:14:25.480]   is one at each time step,
[00:14:25.480 --> 00:14:27.080]   if the pole is still upright.
[00:14:27.080 --> 00:14:31.280]   All the,
[00:14:31.280 --> 00:14:32.480]   first-person shooters,
[00:14:32.480 --> 00:14:33.280]   the video games,
[00:14:33.280 --> 00:14:34.280]   and now StarCraft,
[00:14:34.280 --> 00:14:37.880]   the strategy games.
[00:14:37.880 --> 00:14:42.880]   In case of first-person shooter in Doom,
[00:14:42.880 --> 00:14:44.080]   what is the goal?
[00:14:44.080 --> 00:14:45.680]   The environment is the game,
[00:14:45.680 --> 00:14:47.880]   the goal is to eliminate all opponents,
[00:14:47.880 --> 00:14:50.280]   the state is the raw game pixels coming in,
[00:14:50.280 --> 00:14:53.880]   the actions is moving up, down, left, right,
[00:14:53.880 --> 00:14:55.280]   and so on.
[00:14:55.280 --> 00:14:57.480]   And the reward is positive,
[00:14:57.480 --> 00:14:59.480]   when eliminating an opponent,
[00:14:59.480 --> 00:15:01.480]   and negative,
[00:15:01.480 --> 00:15:03.080]   when the agent is eliminated.
[00:15:03.080 --> 00:15:07.080]   Industrial robotics,
[00:15:07.080 --> 00:15:09.880]   bin packing with a robotic arm,
[00:15:09.880 --> 00:15:12.080]   the goal is to pick up a device from a box,
[00:15:12.080 --> 00:15:13.680]   and put it into a container.
[00:15:13.680 --> 00:15:16.280]   The state is the raw pixels of the real world,
[00:15:16.280 --> 00:15:17.880]   that the robot observes,
[00:15:17.880 --> 00:15:20.080]   the actions are the possible,
[00:15:20.080 --> 00:15:21.480]   actions of the robot,
[00:15:21.480 --> 00:15:22.680]   the different degrees of freedom,
[00:15:22.680 --> 00:15:24.080]   and moving through those degrees,
[00:15:24.080 --> 00:15:25.880]   moving the different actuators,
[00:15:25.880 --> 00:15:27.080]   to realize,
[00:15:27.080 --> 00:15:28.880]   the position of the arm.
[00:15:28.880 --> 00:15:30.280]   And the reward is positive,
[00:15:30.280 --> 00:15:32.080]   when placing a device successfully,
[00:15:32.080 --> 00:15:33.280]   and negative otherwise.
[00:15:33.280 --> 00:15:36.880]   Everything can be modeled in this way.
[00:15:36.880 --> 00:15:39.280]   Markov decision process.
[00:15:39.280 --> 00:15:41.280]   There's a state as zero,
[00:15:41.280 --> 00:15:42.680]   action A zero,
[00:15:42.680 --> 00:15:44.280]   and reward received.
[00:15:44.280 --> 00:15:46.280]   A new state is achieved.
[00:15:46.280 --> 00:15:48.080]   Again, action reward state,
[00:15:48.080 --> 00:15:49.480]   action reward state,
[00:15:49.480 --> 00:15:52.480]   until a terminal state is reached.
[00:15:52.480 --> 00:15:55.480]   And the major components,
[00:15:55.480 --> 00:15:57.080]   of reinforcement learning,
[00:15:57.080 --> 00:15:58.280]   is a policy,
[00:15:59.080 --> 00:16:00.080]   some kind of plan,
[00:16:00.080 --> 00:16:01.880]   of what to do in every single state,
[00:16:01.880 --> 00:16:03.880]   what kind of action to perform.
[00:16:03.880 --> 00:16:05.680]   A value function,
[00:16:05.680 --> 00:16:09.080]   a some kind of sense,
[00:16:09.080 --> 00:16:11.480]   of what is a good state to be in,
[00:16:11.480 --> 00:16:13.880]   of what is a good action to take in a state.
[00:16:13.880 --> 00:16:17.880]   And sometimes a model,
[00:16:17.880 --> 00:16:21.680]   that the agent represents the environment with,
[00:16:21.680 --> 00:16:23.680]   some kind of sense,
[00:16:23.680 --> 00:16:25.280]   of the environment it's operating in,
[00:16:25.280 --> 00:16:26.880]   the dynamics of that environment,
[00:16:26.880 --> 00:16:28.080]   that's useful,
[00:16:28.080 --> 00:16:30.080]   for making decisions about actions.
[00:16:30.080 --> 00:16:32.680]   Let's take a trivial example.
[00:16:32.680 --> 00:16:37.680]   A grid world of 3 by 4,
[00:16:37.680 --> 00:16:39.280]   12 squares,
[00:16:39.280 --> 00:16:40.880]   where you start at the bottom left,
[00:16:40.880 --> 00:16:45.280]   and they're tasked with walking about this world,
[00:16:45.280 --> 00:16:47.080]   to maximize reward.
[00:16:47.080 --> 00:16:50.080]   The reward at the top right is a plus one,
[00:16:50.080 --> 00:16:52.680]   and at one square below that is a negative one.
[00:16:52.680 --> 00:16:54.080]   And every step you take,
[00:16:54.080 --> 00:16:56.080]   is a punishment,
[00:16:56.080 --> 00:16:59.080]   or is a negative reward of 0.04.
[00:16:59.080 --> 00:17:02.480]   So what is the optimal policy in this world?
[00:17:02.480 --> 00:17:06.280]   Now when everything is deterministic,
[00:17:06.280 --> 00:17:08.880]   perhaps this is the policy.
[00:17:08.880 --> 00:17:11.680]   When you start at the bottom left,
[00:17:11.680 --> 00:17:14.280]   well, because every step hurts,
[00:17:14.280 --> 00:17:16.080]   every step has a negative reward,
[00:17:16.080 --> 00:17:18.280]   then you want to take the shortest path,
[00:17:18.280 --> 00:17:20.880]   to the maximum square with the maximum reward.
[00:17:20.880 --> 00:17:24.680]   When the state space is non-deterministic,
[00:17:25.680 --> 00:17:28.080]   as presented before,
[00:17:28.080 --> 00:17:30.480]   with a probability of 0.8,
[00:17:30.480 --> 00:17:32.080]   when you choose to go up,
[00:17:32.080 --> 00:17:33.080]   you go up,
[00:17:33.080 --> 00:17:34.680]   but with probability 0.1,
[00:17:34.680 --> 00:17:36.480]   you go left,
[00:17:36.480 --> 00:17:38.280]   and 0.1, you go right.
[00:17:38.280 --> 00:17:39.680]   Unfair.
[00:17:39.680 --> 00:17:41.280]   Again, much like life.
[00:17:41.280 --> 00:17:44.880]   That would be the optimal policy.
[00:17:44.880 --> 00:17:48.080]   What is the key observation here?
[00:17:48.080 --> 00:17:50.080]   That every single state in the space,
[00:17:50.080 --> 00:17:51.480]   must have a plan.
[00:17:51.480 --> 00:17:54.280]   Because you can't,
[00:17:54.280 --> 00:17:56.680]   because then the non-deterministic aspect,
[00:17:56.680 --> 00:17:58.080]   of the control,
[00:17:58.080 --> 00:18:00.280]   you can't control where you're going to end up,
[00:18:00.280 --> 00:18:02.280]   so you must have a plan for every place.
[00:18:02.280 --> 00:18:03.680]   That's the policy.
[00:18:03.680 --> 00:18:05.080]   Having an action,
[00:18:05.080 --> 00:18:07.280]   an optimal action to take in every single state.
[00:18:07.280 --> 00:18:10.480]   Now suppose we change the reward structure,
[00:18:10.480 --> 00:18:12.280]   and for every step we take,
[00:18:12.280 --> 00:18:13.280]   there's a negative,
[00:18:13.280 --> 00:18:15.480]   a reward is a negative two.
[00:18:15.480 --> 00:18:16.880]   So it really hurts.
[00:18:16.880 --> 00:18:19.280]   There's a high punishment for every single step we take.
[00:18:19.280 --> 00:18:20.880]   So no matter what,
[00:18:20.880 --> 00:18:23.480]   we always take the shortest path.
[00:18:23.480 --> 00:18:25.880]   The optimal policy is to take the shortest path,
[00:18:25.880 --> 00:18:26.480]   to the,
[00:18:26.480 --> 00:18:29.280]   to the only spot on the board,
[00:18:29.280 --> 00:18:32.080]   that doesn't result in punishment.
[00:18:32.080 --> 00:18:36.880]   If we decrease the reward of each step,
[00:18:36.880 --> 00:18:38.680]   to negative 0.1,
[00:18:38.680 --> 00:18:41.280]   the policy changes.
[00:18:41.280 --> 00:18:45.880]   Where there is some extra degree of wandering,
[00:18:45.880 --> 00:18:47.480]   encouraged.
[00:18:47.480 --> 00:18:50.880]   And as we go further and further,
[00:18:50.880 --> 00:18:53.080]   in lowering the punishment as before,
[00:18:53.080 --> 00:18:55.280]   to negative 0.04,
[00:18:55.280 --> 00:18:57.880]   more wandering and more wandering is allowed.
[00:18:57.880 --> 00:19:02.680]   And when we finally turn the reward,
[00:19:02.680 --> 00:19:06.280]   into positive,
[00:19:06.280 --> 00:19:07.480]   so every step,
[00:19:07.480 --> 00:19:13.480]   every step is increases the reward,
[00:19:13.480 --> 00:19:16.280]   then there's a significant incentive to,
[00:19:16.280 --> 00:19:20.080]   to stay on the board without ever reaching the destination.
[00:19:22.480 --> 00:19:24.280]   Kind of like college for a lot of people.
[00:19:24.280 --> 00:19:29.880]   So the value function,
[00:19:29.880 --> 00:19:31.480]   the way we think about,
[00:19:31.480 --> 00:19:33.280]   the value of a state,
[00:19:33.280 --> 00:19:35.080]   or the value of anything,
[00:19:35.080 --> 00:19:36.480]   in the environment,
[00:19:36.480 --> 00:19:37.880]   is,
[00:19:37.880 --> 00:19:41.880]   the reward we're likely to receive in the future.
[00:19:41.880 --> 00:19:45.880]   And the way we see the reward we're likely to receive,
[00:19:45.880 --> 00:19:48.080]   is we discount,
[00:19:48.080 --> 00:19:49.680]   the future reward.
[00:19:49.680 --> 00:19:52.080]   Because we can't always count on it.
[00:19:53.080 --> 00:19:54.680]   Here at Gamma,
[00:19:54.680 --> 00:19:57.480]   further and further out into the future,
[00:19:57.480 --> 00:20:00.080]   more and more discounts decreases,
[00:20:00.080 --> 00:20:03.480]   the reward, the importance of the reward received.
[00:20:03.480 --> 00:20:05.880]   And the good strategy,
[00:20:05.880 --> 00:20:08.680]   is taking the sum of these rewards and maximizing it.
[00:20:08.680 --> 00:20:11.080]   Maximizing discounted future reward.
[00:20:11.080 --> 00:20:13.080]   That's what reinforcement learning,
[00:20:13.080 --> 00:20:14.480]   hopes to achieve.
[00:20:14.480 --> 00:20:17.280]   And with Q-learning,
[00:20:17.280 --> 00:20:19.480]   we use,
[00:20:21.080 --> 00:20:24.680]   any policy to estimate the value of taking an action,
[00:20:24.680 --> 00:20:25.680]   in a state.
[00:20:25.680 --> 00:20:29.480]   So off policy,
[00:20:29.480 --> 00:20:30.880]   forget policy.
[00:20:30.880 --> 00:20:32.880]   We move about the world,
[00:20:32.880 --> 00:20:35.280]   and use the Bellman equation here on the bottom,
[00:20:35.280 --> 00:20:38.680]   to continuously update our estimate of how good,
[00:20:38.680 --> 00:20:40.680]   a certain action is in a certain state.
[00:20:40.680 --> 00:20:44.680]   So we don't need,
[00:20:44.680 --> 00:20:47.480]   this allows us to operate in a much larger state space,
[00:20:47.480 --> 00:20:48.880]   in a much larger action space.
[00:20:48.880 --> 00:20:50.680]   We move about this world,
[00:20:50.680 --> 00:20:52.480]   through simulation or in the real world,
[00:20:52.480 --> 00:20:55.080]   taking actions and updating our estimate,
[00:20:55.080 --> 00:20:57.480]   of how good certain actions are over time.
[00:20:57.480 --> 00:21:01.280]   The new state at the left,
[00:21:01.280 --> 00:21:03.080]   is the updated value.
[00:21:03.080 --> 00:21:04.080]   The old state,
[00:21:04.080 --> 00:21:05.880]   is the starting value for the equation.
[00:21:05.880 --> 00:21:08.280]   And we update that old state estimation,
[00:21:08.280 --> 00:21:09.680]   with the sum,
[00:21:09.680 --> 00:21:12.080]   of the reward received,
[00:21:12.080 --> 00:21:13.880]   by taking action S,
[00:21:13.880 --> 00:21:16.680]   action A in state S.
[00:21:18.080 --> 00:21:18.880]   And,
[00:21:18.880 --> 00:21:22.280]   the maximum reward that's possible,
[00:21:22.280 --> 00:21:24.680]   to be received in the following states,
[00:21:24.680 --> 00:21:26.680]   discounted.
[00:21:26.680 --> 00:21:29.680]   That update,
[00:21:29.680 --> 00:21:31.480]   is decreased with a learning rate.
[00:21:31.480 --> 00:21:33.080]   The higher the learning rate,
[00:21:33.080 --> 00:21:34.480]   the more value we,
[00:21:34.480 --> 00:21:36.280]   the faster we learn,
[00:21:36.280 --> 00:21:39.280]   the more value we assign to new information.
[00:21:39.280 --> 00:21:41.280]   That's simple, that's it.
[00:21:41.280 --> 00:21:42.280]   That's Q-learning.
[00:21:42.280 --> 00:21:43.880]   The simple update rule,
[00:21:43.880 --> 00:21:45.080]   allows us to,
[00:21:45.080 --> 00:21:47.280]   to explore the world,
[00:21:47.280 --> 00:21:48.680]   and as we explore,
[00:21:48.680 --> 00:21:52.080]   get more and more information,
[00:21:52.080 --> 00:21:53.880]   about what's good to do in this world.
[00:21:53.880 --> 00:21:55.880]   And there's always a balance,
[00:21:55.880 --> 00:21:58.280]   in the various problem spaces we'll discuss,
[00:21:58.280 --> 00:22:00.080]   there's always a balance between,
[00:22:00.080 --> 00:22:02.280]   exploration and exploitation.
[00:22:02.280 --> 00:22:06.480]   As you form a better and better estimate,
[00:22:06.480 --> 00:22:07.280]   of the Q-function,
[00:22:07.280 --> 00:22:08.880]   of what actions are good to take,
[00:22:08.880 --> 00:22:10.880]   you start to get a sense,
[00:22:10.880 --> 00:22:12.880]   of what is the best action to take.
[00:22:12.880 --> 00:22:15.080]   But it's not a perfect sense,
[00:22:15.080 --> 00:22:16.480]   it's still an approximation.
[00:22:16.480 --> 00:22:18.480]   And so there's value of exploration.
[00:22:18.480 --> 00:22:20.880]   But the better and better your estimate becomes,
[00:22:20.880 --> 00:22:22.680]   the less and less exploration,
[00:22:22.680 --> 00:22:23.680]   has a benefit.
[00:22:23.680 --> 00:22:27.080]   So, usually we want to explore a lot in the beginning,
[00:22:27.080 --> 00:22:28.680]   and less and less so,
[00:22:28.680 --> 00:22:29.680]   towards the end.
[00:22:29.680 --> 00:22:32.280]   And when we finally release the system out,
[00:22:32.280 --> 00:22:33.280]   into the world,
[00:22:33.280 --> 00:22:35.280]   and wish it to operate its best,
[00:22:35.280 --> 00:22:36.280]   then we,
[00:22:36.280 --> 00:22:37.880]   have it operate,
[00:22:37.880 --> 00:22:39.080]   as a greedy system,
[00:22:39.080 --> 00:22:40.480]   always taking the optimal action,
[00:22:40.480 --> 00:22:43.080]   according to the Q-value function.
[00:22:45.680 --> 00:22:47.680]   And everything I'm talking about now,
[00:22:47.680 --> 00:22:49.080]   is parameterized,
[00:22:49.080 --> 00:22:50.880]   and our parameters,
[00:22:50.880 --> 00:22:53.280]   that are very important,
[00:22:53.280 --> 00:22:55.680]   for winning the deep traffic competition.
[00:22:55.680 --> 00:22:59.280]   Which is using this very algorithm,
[00:22:59.280 --> 00:23:00.480]   with a neural network,
[00:23:00.480 --> 00:23:01.480]   at its core.
[00:23:01.480 --> 00:23:06.080]   So for a simple table representation,
[00:23:06.080 --> 00:23:07.280]   of a Q-function,
[00:23:07.280 --> 00:23:09.880]   where the Y-axis is state,
[00:23:09.880 --> 00:23:10.680]   four states,
[00:23:10.680 --> 00:23:12.280]   S1,2,3,4.
[00:23:12.280 --> 00:23:14.880]   And the X-axis is,
[00:23:14.880 --> 00:23:17.480]   actions, A1,2,3,4.
[00:23:17.480 --> 00:23:19.680]   We can think of this table,
[00:23:19.680 --> 00:23:21.480]   as randomly initiated,
[00:23:21.480 --> 00:23:22.480]   or initiated,
[00:23:22.480 --> 00:23:23.480]   initialized,
[00:23:23.480 --> 00:23:24.680]   in any kind of way,
[00:23:24.680 --> 00:23:27.680]   that's not representative of actual reality.
[00:23:27.680 --> 00:23:29.280]   And as we move about this world,
[00:23:29.280 --> 00:23:30.480]   and we take actions,
[00:23:30.480 --> 00:23:31.680]   we update this table,
[00:23:31.680 --> 00:23:32.880]   with the Bellman equation,
[00:23:32.880 --> 00:23:34.080]   shown up top.
[00:23:34.080 --> 00:23:36.480]   And here, slides now are online,
[00:23:36.480 --> 00:23:38.080]   you can see a simple,
[00:23:38.080 --> 00:23:39.480]   pseudocode algorithm,
[00:23:39.480 --> 00:23:40.880]   of how to update it,
[00:23:40.880 --> 00:23:41.880]   of how to run,
[00:23:41.880 --> 00:23:43.280]   this Bellman equation.
[00:23:43.280 --> 00:23:44.280]   And,
[00:23:44.280 --> 00:23:45.480]   over time,
[00:23:45.480 --> 00:23:48.080]   the approximation becomes the optimal,
[00:23:48.080 --> 00:23:49.080]   Q-table.
[00:23:49.080 --> 00:23:50.680]   The problem is,
[00:23:50.680 --> 00:23:51.880]   when that Q-table,
[00:23:51.880 --> 00:23:54.080]   it becomes exponential in size.
[00:23:54.080 --> 00:23:57.480]   When we take in raw sensory information,
[00:23:57.480 --> 00:23:58.880]   as we do with cameras,
[00:23:58.880 --> 00:24:00.080]   with deep crash,
[00:24:00.080 --> 00:24:02.480]   or with deep traffic,
[00:24:02.480 --> 00:24:04.280]   it's taking the full grid space,
[00:24:04.280 --> 00:24:05.680]   and taking that information,
[00:24:05.680 --> 00:24:06.480]   the raw,
[00:24:06.480 --> 00:24:08.680]   the raw grid,
[00:24:08.680 --> 00:24:10.480]   pixels of deep traffic.
[00:24:10.480 --> 00:24:12.480]   And when you take the arcade games,
[00:24:12.680 --> 00:24:15.280]   here, they're taking the raw pixels of the game.
[00:24:15.280 --> 00:24:18.680]   Or when we take Go,
[00:24:18.680 --> 00:24:19.680]   the game of Go,
[00:24:19.680 --> 00:24:21.080]   when it's taking the units,
[00:24:21.080 --> 00:24:23.280]   the board,
[00:24:23.280 --> 00:24:25.080]   the raw state of the board,
[00:24:25.080 --> 00:24:27.280]   as the input,
[00:24:27.280 --> 00:24:29.280]   the potential,
[00:24:29.280 --> 00:24:30.680]   state space,
[00:24:30.680 --> 00:24:32.280]   the number of possible,
[00:24:32.280 --> 00:24:34.080]   combinatorial variations of,
[00:24:34.080 --> 00:24:35.880]   what states is possible,
[00:24:35.880 --> 00:24:37.080]   is,
[00:24:37.080 --> 00:24:38.280]   extremely large.
[00:24:38.280 --> 00:24:39.280]   Larger than,
[00:24:39.280 --> 00:24:41.080]   we can certainly hold the memory,
[00:24:41.080 --> 00:24:42.480]   and larger than we can,
[00:24:43.280 --> 00:24:45.280]   ever be able to accurately approximate,
[00:24:45.280 --> 00:24:46.680]   through the Bellman equation,
[00:24:46.680 --> 00:24:47.680]   over time,
[00:24:47.680 --> 00:24:49.080]   through simulation.
[00:24:49.080 --> 00:24:53.080]   Through the simple update of the Bellman equation.
[00:24:53.080 --> 00:24:54.880]   So this is where,
[00:24:54.880 --> 00:24:57.080]   deep reinforcement learning comes in.
[00:24:57.080 --> 00:24:58.680]   Neural networks,
[00:24:58.680 --> 00:25:00.680]   are really good approximators.
[00:25:00.680 --> 00:25:02.880]   They're really good at exactly this task,
[00:25:02.880 --> 00:25:03.880]   of learning,
[00:25:03.880 --> 00:25:05.680]   this kind of Q-table.
[00:25:05.680 --> 00:25:11.880]   So as we started with supervised learning,
[00:25:12.280 --> 00:25:14.280]   or neural networks help us memorize patterns,
[00:25:14.280 --> 00:25:15.480]   using supervised,
[00:25:15.480 --> 00:25:17.280]   ground-truth data,
[00:25:17.280 --> 00:25:19.080]   and we move to reinforcement learning,
[00:25:19.080 --> 00:25:20.480]   that hopes to propagate,
[00:25:20.480 --> 00:25:23.080]   outcomes to knowledge.
[00:25:23.080 --> 00:25:26.080]   Deep learning,
[00:25:26.080 --> 00:25:27.480]   allows us to do so,
[00:25:27.480 --> 00:25:29.680]   on much larger state spaces,
[00:25:29.680 --> 00:25:30.880]   a much larger,
[00:25:30.880 --> 00:25:32.880]   action spaces.
[00:25:32.880 --> 00:25:34.680]   Which means,
[00:25:34.680 --> 00:25:35.880]   it's generalizable.
[00:25:35.880 --> 00:25:38.880]   It's much more capable to deal,
[00:25:38.880 --> 00:25:40.280]   with the raw,
[00:25:40.280 --> 00:25:41.680]   stuff,
[00:25:41.880 --> 00:25:43.480]   of sensory data.
[00:25:43.480 --> 00:25:45.880]   Which means it's much more capable,
[00:25:45.880 --> 00:25:47.680]   to deal with the broad variation,
[00:25:47.680 --> 00:25:49.080]   of real-world applications.
[00:25:49.080 --> 00:25:55.480]   And it does so,
[00:25:55.480 --> 00:25:56.880]   because it's able to,
[00:25:56.880 --> 00:25:58.880]   learn the representations,
[00:25:58.880 --> 00:25:59.880]   as we discussed,
[00:25:59.880 --> 00:26:01.880]   on Monday.
[00:26:01.880 --> 00:26:06.480]   The understanding comes,
[00:26:06.480 --> 00:26:08.280]   from converting,
[00:26:08.280 --> 00:26:10.080]   the raw sensory information,
[00:26:10.080 --> 00:26:13.080]   into simple, useful information,
[00:26:13.080 --> 00:26:14.280]   based on which,
[00:26:14.280 --> 00:26:15.280]   the action,
[00:26:15.280 --> 00:26:17.080]   in this particular state can be taken,
[00:26:17.080 --> 00:26:18.480]   in the same exact way.
[00:26:18.480 --> 00:26:20.480]   So instead of the Q-table,
[00:26:20.480 --> 00:26:22.280]   instead of this Q-function,
[00:26:22.280 --> 00:26:23.680]   we plug in a neural network,
[00:26:23.680 --> 00:26:25.880]   where the input is the state space,
[00:26:25.880 --> 00:26:27.280]   no matter how complex,
[00:26:27.280 --> 00:26:29.080]   and the output,
[00:26:29.080 --> 00:26:31.680]   is a value for each of the actions,
[00:26:31.680 --> 00:26:32.680]   that you could take.
[00:26:32.680 --> 00:26:36.080]   Input is the state,
[00:26:36.080 --> 00:26:37.480]   output is the,
[00:26:37.480 --> 00:26:38.880]   value of the function.
[00:26:38.880 --> 00:26:39.880]   It's simple.
[00:26:40.880 --> 00:26:42.080]   This is,
[00:26:42.080 --> 00:26:44.080]   Deep Q Network,
[00:26:44.080 --> 00:26:45.080]   DQN.
[00:26:45.080 --> 00:26:46.880]   At the core,
[00:26:46.880 --> 00:26:48.880]   of the success of DeepMind,
[00:26:48.880 --> 00:26:50.280]   a lot of the cool stuff you see,
[00:26:50.280 --> 00:26:51.480]   about video games,
[00:26:51.480 --> 00:26:52.280]   DQN,
[00:26:52.280 --> 00:26:54.280]   or variants of DQN are at play.
[00:26:54.280 --> 00:26:56.680]   This is what at first,
[00:26:56.680 --> 00:26:57.880]   with the Nature paper,
[00:26:57.880 --> 00:26:59.880]   DeepMind,
[00:26:59.880 --> 00:27:02.680]   the success came,
[00:27:02.680 --> 00:27:03.880]   of playing the different games,
[00:27:03.880 --> 00:27:04.880]   including Atari,
[00:27:04.880 --> 00:27:06.680]   games.
[00:27:07.680 --> 00:27:11.480]   So, how are these things trained?
[00:27:11.480 --> 00:27:13.480]   Very similar,
[00:27:13.480 --> 00:27:15.280]   to supervised learning.
[00:27:15.280 --> 00:27:20.280]   The Bellman equation up top,
[00:27:20.280 --> 00:27:20.880]   it,
[00:27:20.880 --> 00:27:23.680]   takes the reward,
[00:27:23.680 --> 00:27:26.280]   and the discounted,
[00:27:26.280 --> 00:27:27.880]   expected reward,
[00:27:27.880 --> 00:27:29.080]   from future states.
[00:27:29.080 --> 00:27:33.880]   The loss function here,
[00:27:33.880 --> 00:27:34.880]   for neural network,
[00:27:34.880 --> 00:27:36.880]   the neural network learns with a loss function.
[00:27:36.880 --> 00:27:38.080]   It takes,
[00:27:38.080 --> 00:27:38.880]   the,
[00:27:38.880 --> 00:27:41.680]   reward received at the current state,
[00:27:41.680 --> 00:27:43.680]   does a forward pass,
[00:27:43.680 --> 00:27:44.680]   through a neural network,
[00:27:44.680 --> 00:27:47.480]   to estimate the value of the future state,
[00:27:47.480 --> 00:27:49.280]   of the best,
[00:27:49.280 --> 00:27:51.880]   action to take in the future state,
[00:27:51.880 --> 00:27:54.680]   and then subtracts that,
[00:27:54.680 --> 00:27:57.680]   from the forward pass,
[00:27:57.680 --> 00:27:59.280]   through the network,
[00:27:59.280 --> 00:28:00.680]   for the current state in action.
[00:28:00.680 --> 00:28:03.880]   So, you take the difference between,
[00:28:04.280 --> 00:28:05.680]   what your Q,
[00:28:05.680 --> 00:28:06.880]   estimator,
[00:28:06.880 --> 00:28:07.880]   the neural network,
[00:28:07.880 --> 00:28:10.680]   believes the value of the current state is,
[00:28:10.680 --> 00:28:12.280]   and,
[00:28:12.280 --> 00:28:13.080]   what,
[00:28:13.080 --> 00:28:15.280]   it more likely is to be,
[00:28:15.280 --> 00:28:19.280]   based on the value of the future states,
[00:28:19.280 --> 00:28:21.480]   that are reachable based on the actions you can take.
[00:28:21.480 --> 00:28:27.680]   Here's the algorithm.
[00:28:27.680 --> 00:28:30.680]   Input is the state,
[00:28:30.680 --> 00:28:33.280]   output is the Q value for each action,
[00:28:33.480 --> 00:28:34.680]   or in this diagram,
[00:28:34.680 --> 00:28:36.280]   input is the state in action,
[00:28:36.280 --> 00:28:37.880]   and the output is the Q value.
[00:28:37.880 --> 00:28:40.480]   It's very similar architectures.
[00:28:40.480 --> 00:28:42.480]   So, given a transition,
[00:28:42.480 --> 00:28:43.480]   of S,
[00:28:43.480 --> 00:28:44.880]   A, R,
[00:28:44.880 --> 00:28:46.080]   S'
[00:28:46.080 --> 00:28:49.680]   S current state taking an action,
[00:28:49.680 --> 00:28:50.680]   receiving a reward,
[00:28:50.680 --> 00:28:52.480]   and achieving S' state.
[00:28:52.480 --> 00:28:56.280]   The,
[00:28:56.280 --> 00:28:57.280]   the update,
[00:28:57.280 --> 00:28:58.880]   is,
[00:28:58.880 --> 00:29:00.280]   do a feed forward pass,
[00:29:00.280 --> 00:29:02.080]   through the network for the current state,
[00:29:02.680 --> 00:29:05.280]   do a feed forward pass for each of the,
[00:29:05.280 --> 00:29:08.080]   possible actions taken in the next state.
[00:29:08.080 --> 00:29:11.280]   And that's how we compute the two parts of the loss function.
[00:29:11.280 --> 00:29:15.680]   And update the weights using back propagation.
[00:29:15.680 --> 00:29:18.080]   Again, loss function,
[00:29:18.080 --> 00:29:20.080]   back propagation is how the network is trained.
[00:29:20.080 --> 00:29:24.280]   This is actually been around for,
[00:29:24.280 --> 00:29:25.280]   much longer than,
[00:29:25.280 --> 00:29:26.480]   DeepMind.
[00:29:26.480 --> 00:29:29.880]   A few tricks made it,
[00:29:29.880 --> 00:29:30.880]   made it really work.
[00:29:31.880 --> 00:29:33.880]   Experience replay is the biggest one.
[00:29:33.880 --> 00:29:38.880]   So, as the games are played through simulation,
[00:29:38.880 --> 00:29:40.080]   or if it's a physical system,
[00:29:40.080 --> 00:29:41.280]   as it acts in the world.
[00:29:41.280 --> 00:29:44.280]   It's actually,
[00:29:44.280 --> 00:29:46.680]   collecting the observations,
[00:29:46.680 --> 00:29:48.880]   into a library of experiences.
[00:29:48.880 --> 00:29:51.080]   And the training is performed,
[00:29:51.080 --> 00:29:53.880]   by randomly sampling the library in the past.
[00:29:53.880 --> 00:29:56.480]   By randomly sampling,
[00:29:56.480 --> 00:29:58.080]   the previous experience,
[00:29:58.080 --> 00:29:58.880]   and then,
[00:29:58.880 --> 00:30:01.080]   sampling the previous experiences,
[00:30:01.080 --> 00:30:01.880]   in batches.
[00:30:01.880 --> 00:30:04.880]   So, you're not always training,
[00:30:04.880 --> 00:30:07.680]   on the natural continuous evolution of the system.
[00:30:07.680 --> 00:30:10.280]   You're training on randomly picked batches,
[00:30:10.280 --> 00:30:11.480]   of those experiences.
[00:30:11.480 --> 00:30:13.080]   That's a huge,
[00:30:13.080 --> 00:30:15.080]   it's a,
[00:30:15.080 --> 00:30:16.280]   seems like a subtle trick,
[00:30:16.280 --> 00:30:17.480]   but it's a really important one.
[00:30:17.480 --> 00:30:20.480]   So, the system doesn't,
[00:30:20.480 --> 00:30:22.080]   over fit,
[00:30:22.080 --> 00:30:23.880]   a particular evolution,
[00:30:23.880 --> 00:30:25.080]   of the,
[00:30:25.080 --> 00:30:26.080]   of the game,
[00:30:26.080 --> 00:30:27.880]   of the simulation.
[00:30:28.880 --> 00:30:30.880]   Another important,
[00:30:30.880 --> 00:30:33.680]   again, subtle trick,
[00:30:33.680 --> 00:30:35.880]   as in a lot of deep learning approaches,
[00:30:35.880 --> 00:30:37.880]   the subtle tricks make all the difference,
[00:30:37.880 --> 00:30:39.280]   is,
[00:30:39.280 --> 00:30:41.280]   fixing the target network.
[00:30:41.280 --> 00:30:43.680]   For the loss function,
[00:30:43.680 --> 00:30:44.680]   if you notice,
[00:30:44.680 --> 00:30:47.480]   you have to use the neural network,
[00:30:47.480 --> 00:30:48.880]   the single neural network,
[00:30:48.880 --> 00:30:50.280]   the DQN network,
[00:30:50.280 --> 00:30:52.880]   to estimate the value of the current state,
[00:30:52.880 --> 00:30:54.880]   an action pair.
[00:30:54.880 --> 00:30:56.880]   And,
[00:30:56.880 --> 00:30:58.480]   and the next.
[00:30:58.480 --> 00:30:59.680]   So, you're using it,
[00:30:59.680 --> 00:31:00.880]   multiple times.
[00:31:00.880 --> 00:31:03.480]   And,
[00:31:03.480 --> 00:31:05.880]   as you perform that operation,
[00:31:05.880 --> 00:31:07.880]   you're updating the network.
[00:31:07.880 --> 00:31:09.880]   Which means the target function,
[00:31:09.880 --> 00:31:11.280]   inside that loss function,
[00:31:11.280 --> 00:31:12.480]   is always changing.
[00:31:12.480 --> 00:31:13.480]   So, you're,
[00:31:13.480 --> 00:31:15.880]   the very nature of your loss function,
[00:31:15.880 --> 00:31:17.080]   is changing all the time,
[00:31:17.080 --> 00:31:18.080]   as you're learning.
[00:31:18.080 --> 00:31:20.480]   And that's a big problem for stability.
[00:31:20.480 --> 00:31:22.480]   That can create big problems,
[00:31:22.480 --> 00:31:23.680]   to the learning process.
[00:31:23.680 --> 00:31:25.280]   So, this little trick,
[00:31:25.280 --> 00:31:26.480]   is to fix,
[00:31:26.680 --> 00:31:27.480]   the network,
[00:31:27.480 --> 00:31:28.680]   and only update it,
[00:31:28.680 --> 00:31:30.080]   every,
[00:31:30.080 --> 00:31:32.080]   say, thousand steps.
[00:31:32.080 --> 00:31:33.480]   So,
[00:31:33.480 --> 00:31:35.480]   as you train the network,
[00:31:35.480 --> 00:31:38.080]   the network that's used,
[00:31:38.080 --> 00:31:40.080]   to compute the target function,
[00:31:40.080 --> 00:31:42.080]   inside the loss function, is fixed.
[00:31:42.080 --> 00:31:45.480]   It produces a more stable computation,
[00:31:45.480 --> 00:31:46.680]   on the loss function.
[00:31:46.680 --> 00:31:47.480]   So,
[00:31:47.480 --> 00:31:48.880]   the ground doesn't,
[00:31:48.880 --> 00:31:50.280]   shift under you,
[00:31:50.280 --> 00:31:52.480]   as you're trying to find a minimal,
[00:31:52.480 --> 00:31:54.480]   for the loss function.
[00:31:54.480 --> 00:31:56.480]   The loss function doesn't change.
[00:31:56.880 --> 00:31:59.280]   In unpredictable, difficult to understand ways.
[00:31:59.280 --> 00:32:00.680]   And,
[00:32:00.680 --> 00:32:02.080]   reward clipping,
[00:32:02.080 --> 00:32:03.680]   which is,
[00:32:03.680 --> 00:32:04.880]   always true,
[00:32:04.880 --> 00:32:05.880]   with general,
[00:32:05.880 --> 00:32:07.080]   systems that are,
[00:32:07.080 --> 00:32:08.080]   operating,
[00:32:08.080 --> 00:32:11.280]   seeking to operate in a generalized way,
[00:32:11.280 --> 00:32:12.080]   is,
[00:32:12.080 --> 00:32:13.080]   for very,
[00:32:13.080 --> 00:32:14.680]   for these various games,
[00:32:14.680 --> 00:32:16.880]   the points are different.
[00:32:16.880 --> 00:32:18.280]   Some, some points are low,
[00:32:18.280 --> 00:32:19.280]   some points are high,
[00:32:19.280 --> 00:32:20.880]   some go positive and negative.
[00:32:20.880 --> 00:32:22.280]   And they're all normalized,
[00:32:22.280 --> 00:32:24.480]   to a point where the good points,
[00:32:24.480 --> 00:32:26.080]   or the positive points,
[00:32:26.080 --> 00:32:27.080]   are a one,
[00:32:27.080 --> 00:32:29.880]   and negative points are a negative one.
[00:32:29.880 --> 00:32:31.480]   That's reward clipping.
[00:32:31.480 --> 00:32:33.280]   Simplify the reward structure.
[00:32:33.280 --> 00:32:36.880]   And, because a lot of the games are 30 FPS,
[00:32:36.880 --> 00:32:38.080]   or 60 FPS,
[00:32:38.080 --> 00:32:39.680]   and the actions,
[00:32:39.680 --> 00:32:41.480]   are not,
[00:32:41.480 --> 00:32:43.880]   it's not valuable to take actions,
[00:32:43.880 --> 00:32:45.280]   at such a high rate,
[00:32:45.280 --> 00:32:46.280]   inside of these,
[00:32:46.280 --> 00:32:47.680]   particularly Atari games,
[00:32:47.680 --> 00:32:50.280]   that you only take an action every four steps.
[00:32:50.280 --> 00:32:52.480]   While still taking in the frames,
[00:32:52.480 --> 00:32:54.680]   as part of the temporal window to make decisions.
[00:32:55.480 --> 00:32:56.280]   Tricks,
[00:32:56.280 --> 00:32:57.880]   but hopefully gives you a sense,
[00:32:57.880 --> 00:33:01.680]   of the kind of things necessary,
[00:33:01.680 --> 00:33:02.880]   for both,
[00:33:02.880 --> 00:33:05.880]   seminal papers like this one,
[00:33:05.880 --> 00:33:08.280]   and for the more important accomplishment,
[00:33:08.280 --> 00:33:09.480]   of winning deep traffic,
[00:33:09.480 --> 00:33:11.080]   is the,
[00:33:11.080 --> 00:33:12.880]   is the tricks make all the difference.
[00:33:12.880 --> 00:33:15.280]   Here on the bottom,
[00:33:15.280 --> 00:33:16.480]   is,
[00:33:16.480 --> 00:33:18.480]   the circle,
[00:33:18.480 --> 00:33:20.280]   is when the technique is used,
[00:33:20.280 --> 00:33:21.880]   and the X when it's not,
[00:33:21.880 --> 00:33:23.680]   looking at replay and target.
[00:33:24.080 --> 00:33:26.280]   Takes target network and experience replay.
[00:33:26.280 --> 00:33:27.880]   When both are used,
[00:33:27.880 --> 00:33:29.280]   for the game of breakout,
[00:33:29.280 --> 00:33:30.680]   river raid,
[00:33:30.680 --> 00:33:31.480]   sea quest,
[00:33:31.480 --> 00:33:32.680]   and space invaders.
[00:33:32.680 --> 00:33:34.080]   The higher the number,
[00:33:34.080 --> 00:33:34.880]   the better it is,
[00:33:34.880 --> 00:33:36.280]   the more points achieved.
[00:33:36.280 --> 00:33:38.480]   So when,
[00:33:38.480 --> 00:33:39.680]   it gives you a sense,
[00:33:39.680 --> 00:33:41.280]   that when replay and target,
[00:33:41.280 --> 00:33:43.680]   both give significant improvements,
[00:33:43.680 --> 00:33:45.280]   in the performance of the system.
[00:33:45.280 --> 00:33:49.280]   Order of magnitude improvements,
[00:33:49.280 --> 00:33:51.480]   two orders of magnitude for breakout.
[00:33:52.480 --> 00:33:53.480]   And here is,
[00:33:53.480 --> 00:33:55.680]   pseudocode,
[00:33:55.680 --> 00:33:57.280]   of implementing DQN,
[00:33:57.280 --> 00:33:58.280]   the learning.
[00:33:58.280 --> 00:34:01.880]   The key thing to notice,
[00:34:01.880 --> 00:34:03.480]   and you can look to the slides,
[00:34:03.480 --> 00:34:04.680]   is,
[00:34:04.680 --> 00:34:06.280]   the,
[00:34:06.280 --> 00:34:07.480]   the loop,
[00:34:07.480 --> 00:34:08.480]   the while loop,
[00:34:08.480 --> 00:34:10.280]   of playing through the games,
[00:34:10.280 --> 00:34:12.280]   and selecting the actions to play,
[00:34:12.280 --> 00:34:15.080]   is not part of the training.
[00:34:15.080 --> 00:34:15.680]   It's,
[00:34:15.680 --> 00:34:17.280]   it's part of the saving,
[00:34:17.280 --> 00:34:19.480]   the observation,
[00:34:19.880 --> 00:34:22.280]   the observations,
[00:34:22.280 --> 00:34:24.480]   the state action reward,
[00:34:24.480 --> 00:34:26.280]   next state observations,
[00:34:26.280 --> 00:34:28.080]   and saving them into replay memory,
[00:34:28.080 --> 00:34:29.280]   into that library.
[00:34:29.280 --> 00:34:31.480]   And then you sample randomly,
[00:34:31.480 --> 00:34:33.080]   from that replay memory,
[00:34:33.080 --> 00:34:34.680]   to then train the network,
[00:34:34.680 --> 00:34:36.880]   based on the loss function.
[00:34:36.880 --> 00:34:40.080]   And with probability up,
[00:34:40.080 --> 00:34:41.880]   up top of the probability,
[00:34:41.880 --> 00:34:42.880]   epsilon,
[00:34:42.880 --> 00:34:44.080]   select a random action.
[00:34:44.080 --> 00:34:45.880]   That epsilon is,
[00:34:45.880 --> 00:34:48.680]   the probability of exploration,
[00:34:49.080 --> 00:34:50.280]   that decreases.
[00:34:50.280 --> 00:34:52.880]   That's something you'll see in deep traffic as well,
[00:34:52.880 --> 00:34:54.680]   is,
[00:34:54.680 --> 00:34:56.080]   the rate at which that,
[00:34:56.080 --> 00:34:58.080]   exploration decreases over time,
[00:34:58.080 --> 00:34:59.280]   through the training process.
[00:34:59.280 --> 00:35:00.880]   You want to explore a lot first,
[00:35:00.880 --> 00:35:03.080]   and less and less over time.
[00:35:03.080 --> 00:35:06.280]   So, this algorithm has been able to accomplish,
[00:35:06.280 --> 00:35:08.080]   in 2015,
[00:35:08.080 --> 00:35:09.680]   and since,
[00:35:09.680 --> 00:35:11.280]   a lot of incredible things.
[00:35:11.280 --> 00:35:13.480]   Things that made,
[00:35:13.480 --> 00:35:15.480]   the AI world,
[00:35:15.480 --> 00:35:18.080]   think that we,
[00:35:19.080 --> 00:35:20.480]   we're onto something.
[00:35:20.480 --> 00:35:22.680]   That,
[00:35:22.680 --> 00:35:24.680]   general AI is within reach.
[00:35:24.680 --> 00:35:27.080]   It's for the first time,
[00:35:27.080 --> 00:35:29.880]   that raw sensor information was used to create,
[00:35:29.880 --> 00:35:31.280]   a system that acts,
[00:35:31.280 --> 00:35:32.680]   and makes sense of the world.
[00:35:32.680 --> 00:35:34.880]   Makes sense of the physics of the world enough,
[00:35:34.880 --> 00:35:36.480]   to be able to succeed in it,
[00:35:36.480 --> 00:35:38.080]   from very little information.
[00:35:38.080 --> 00:35:40.280]   But these games are trivial.
[00:35:40.280 --> 00:35:44.280]   Even though,
[00:35:44.280 --> 00:35:46.280]   there is a lot of them.
[00:35:47.280 --> 00:35:50.880]   This DQN approach has been able to outperform,
[00:35:50.880 --> 00:35:52.480]   a lot of the Atari games.
[00:35:52.480 --> 00:35:54.480]   That's what's been reported on.
[00:35:54.480 --> 00:35:56.680]   Outperform the human level performance.
[00:35:56.680 --> 00:35:59.280]   But again, these games are trivial.
[00:35:59.280 --> 00:36:01.680]   What I think,
[00:36:01.680 --> 00:36:03.480]   and perhaps biased,
[00:36:03.480 --> 00:36:04.680]   I'm biased,
[00:36:04.680 --> 00:36:06.680]   but one of the greatest accomplishments,
[00:36:06.680 --> 00:36:09.080]   of artificial intelligence in the last decade,
[00:36:09.080 --> 00:36:12.680]   at least from the philosophical,
[00:36:12.680 --> 00:36:14.480]   or the research perspective,
[00:36:15.680 --> 00:36:16.480]   is,
[00:36:16.480 --> 00:36:18.880]   AlphaGo Zero.
[00:36:18.880 --> 00:36:20.680]   First AlphaGo,
[00:36:20.680 --> 00:36:22.080]   and then AlphaGo Zero.
[00:36:22.080 --> 00:36:25.680]   Is deep mind system,
[00:36:25.680 --> 00:36:28.080]   that beat the best in the world,
[00:36:28.080 --> 00:36:29.080]   in the game of Go.
[00:36:29.080 --> 00:36:31.080]   So what's the game of Go?
[00:36:31.080 --> 00:36:33.080]   It's simple.
[00:36:33.080 --> 00:36:36.080]   I won't get into the rules,
[00:36:36.080 --> 00:36:39.280]   but basically it's a 19 by 19 board,
[00:36:39.280 --> 00:36:42.080]   showing on the bottom of the slide,
[00:36:42.080 --> 00:36:45.080]   for the bottom row of the table,
[00:36:45.480 --> 00:36:47.480]   for a board of 19 by 19,
[00:36:47.480 --> 00:36:51.680]   the number of legal game positions,
[00:36:51.680 --> 00:36:54.880]   is 2 times 10 to the power of 170.
[00:36:54.880 --> 00:36:59.280]   It's a very large number of possible positions to consider.
[00:36:59.280 --> 00:37:01.080]   At any one time,
[00:37:01.080 --> 00:37:02.880]   especially the game evolves,
[00:37:02.880 --> 00:37:05.480]   the number of possible moves is huge.
[00:37:05.480 --> 00:37:08.480]   Much larger than in chess.
[00:37:08.480 --> 00:37:11.080]   So that's why,
[00:37:11.080 --> 00:37:12.080]   AI,
[00:37:13.080 --> 00:37:15.680]   the community thought that this game is not solvable.
[00:37:15.680 --> 00:37:19.880]   Until 2016,
[00:37:19.880 --> 00:37:22.080]   when AlphaGo,
[00:37:22.080 --> 00:37:26.080]   used human expert position play,
[00:37:26.080 --> 00:37:29.680]   to seed in a supervised way,
[00:37:29.680 --> 00:37:31.880]   reinforcement learning approach.
[00:37:31.880 --> 00:37:34.680]   And I'll describe it in a little bit of detail,
[00:37:34.680 --> 00:37:36.680]   in a couple of slides here,
[00:37:36.680 --> 00:37:39.080]   to beat the best in the world.
[00:37:42.280 --> 00:37:44.080]   And then AlphaGo Zero,
[00:37:44.080 --> 00:37:45.480]   that is,
[00:37:45.480 --> 00:37:48.480]   the accomplishment of the decade,
[00:37:48.480 --> 00:37:50.680]   for me, in AI.
[00:37:50.680 --> 00:37:53.480]   Is being able to play,
[00:37:53.480 --> 00:37:56.280]   with no,
[00:37:56.280 --> 00:37:59.880]   training data on human expert,
[00:37:59.880 --> 00:38:02.280]   games.
[00:38:02.280 --> 00:38:05.080]   And beat the best in the world,
[00:38:05.080 --> 00:38:06.880]   in an extremely complex game.
[00:38:06.880 --> 00:38:08.080]   This is not Atari.
[00:38:08.080 --> 00:38:09.480]   This is,
[00:38:09.480 --> 00:38:11.080]   this is a,
[00:38:11.480 --> 00:38:12.280]   a much,
[00:38:12.280 --> 00:38:13.480]   higher,
[00:38:13.480 --> 00:38:16.080]   order, difficulty game.
[00:38:16.080 --> 00:38:19.480]   And the, and the quality of players that is competing in,
[00:38:19.480 --> 00:38:20.480]   is much higher.
[00:38:20.480 --> 00:38:23.280]   And it's able to extremely quickly here,
[00:38:23.280 --> 00:38:26.880]   to achieve a rating that's better than AlphaGo.
[00:38:26.880 --> 00:38:30.880]   And better than the different variants of AlphaGo.
[00:38:30.880 --> 00:38:32.680]   And certainly better than the,
[00:38:32.680 --> 00:38:34.280]   the best of the human players.
[00:38:34.280 --> 00:38:35.880]   In 21 days,
[00:38:35.880 --> 00:38:37.680]   of self play.
[00:38:37.680 --> 00:38:39.880]   So how does it work?
[00:38:40.680 --> 00:38:41.880]   All of these approaches,
[00:38:41.880 --> 00:38:44.680]   much, much like the previous ones,
[00:38:44.680 --> 00:38:45.880]   the traditional ones,
[00:38:45.880 --> 00:38:47.880]   they're not based on deep learning.
[00:38:47.880 --> 00:38:52.880]   Are using Monte Carlo Tree Search, MCTS.
[00:38:52.880 --> 00:38:55.280]   Which is,
[00:38:55.280 --> 00:38:58.480]   when you have such a large state space,
[00:38:58.480 --> 00:38:59.680]   you start at a board,
[00:38:59.680 --> 00:39:01.080]   and you play,
[00:39:01.080 --> 00:39:04.080]   and you choose moves,
[00:39:04.080 --> 00:39:06.480]   with some,
[00:39:06.480 --> 00:39:08.280]   exploitation, exploration,
[00:39:08.280 --> 00:39:09.480]   balancing.
[00:39:09.480 --> 00:39:10.480]   And,
[00:39:10.480 --> 00:39:13.480]   choosing to explore totally new positions,
[00:39:13.480 --> 00:39:15.880]   or to go deep in the positions you know are good,
[00:39:15.880 --> 00:39:17.880]   until the bottom of the game is reached,
[00:39:17.880 --> 00:39:20.080]   until the final state is reached.
[00:39:20.080 --> 00:39:21.680]   And then you back propagate,
[00:39:21.680 --> 00:39:22.680]   the,
[00:39:22.680 --> 00:39:26.280]   quality of the choices you made leading to that position.
[00:39:26.280 --> 00:39:29.680]   And in that way, you learn the value of,
[00:39:29.680 --> 00:39:32.680]   of board positions and play.
[00:39:32.680 --> 00:39:35.680]   That's been used by the most successful,
[00:39:35.680 --> 00:39:36.680]   Go playing,
[00:39:36.680 --> 00:39:38.880]   engines before,
[00:39:39.080 --> 00:39:40.280]   and AlphaGo since.
[00:39:40.280 --> 00:39:43.080]   But you might be able to guess,
[00:39:43.080 --> 00:39:44.480]   what's the difference,
[00:39:44.480 --> 00:39:46.680]   with AlphaGo versus the previous approaches.
[00:39:46.680 --> 00:39:49.680]   They use the neural network,
[00:39:49.680 --> 00:39:51.680]   as the,
[00:39:51.680 --> 00:39:53.480]   intuition,
[00:39:53.480 --> 00:39:54.480]   quote-unquote,
[00:39:54.480 --> 00:39:56.480]   to what are the good states,
[00:39:56.480 --> 00:39:58.480]   what are the good next,
[00:39:58.480 --> 00:40:00.680]   board positions to explore.
[00:40:00.680 --> 00:40:07.280]   And the key things,
[00:40:07.280 --> 00:40:09.880]   again, the tricks make all the difference,
[00:40:09.880 --> 00:40:12.480]   that made AlphaGo zero,
[00:40:12.480 --> 00:40:14.080]   work,
[00:40:14.080 --> 00:40:16.280]   and work much better than AlphaGo,
[00:40:16.280 --> 00:40:17.280]   is first,
[00:40:17.280 --> 00:40:19.280]   because there was no expert play,
[00:40:19.280 --> 00:40:21.480]   instead of human games.
[00:40:21.480 --> 00:40:24.680]   AlphaGo,
[00:40:24.680 --> 00:40:25.880]   used,
[00:40:25.880 --> 00:40:27.480]   that very same,
[00:40:27.480 --> 00:40:30.280]   Monte Carlo tree search algorithm,
[00:40:30.280 --> 00:40:31.680]   MCTS,
[00:40:31.680 --> 00:40:33.280]   to do an intelligent look ahead,
[00:40:33.280 --> 00:40:36.080]   based on the neural network prediction,
[00:40:36.280 --> 00:40:38.080]   of what are the good states to take,
[00:40:38.080 --> 00:40:40.480]   it checked that,
[00:40:40.480 --> 00:40:42.080]   instead of human expert play,
[00:40:42.080 --> 00:40:43.080]   it checked,
[00:40:43.080 --> 00:40:44.880]   how good indeed are those,
[00:40:44.880 --> 00:40:46.080]   states.
[00:40:46.080 --> 00:40:48.680]   It's a simple look ahead action,
[00:40:48.680 --> 00:40:50.280]   that does,
[00:40:50.280 --> 00:40:51.280]   the ground truth,
[00:40:51.280 --> 00:40:52.880]   that does the target,
[00:40:52.880 --> 00:40:53.880]   correction,
[00:40:53.880 --> 00:40:55.480]   that produces the loss function.
[00:40:55.480 --> 00:40:58.080]   The second part is the multitask learning,
[00:40:58.080 --> 00:41:00.280]   or what's now called multitask learning,
[00:41:00.280 --> 00:41:01.480]   is the network is,
[00:41:01.480 --> 00:41:03.680]   is quote-unquote two-headed,
[00:41:04.280 --> 00:41:06.680]   in the sense that first it outputs the probability,
[00:41:06.680 --> 00:41:07.880]   of which move to take,
[00:41:07.880 --> 00:41:09.080]   the obvious thing,
[00:41:09.080 --> 00:41:11.480]   and it's also producing a probability of winning.
[00:41:11.480 --> 00:41:15.080]   And there's a few ways to combine that information,
[00:41:15.080 --> 00:41:16.880]   and continuously train,
[00:41:16.880 --> 00:41:19.080]   both parts of the network,
[00:41:19.080 --> 00:41:20.880]   depending on the choice taken.
[00:41:20.880 --> 00:41:22.880]   So you want to take the best choice,
[00:41:22.880 --> 00:41:24.080]   in the short term,
[00:41:24.080 --> 00:41:26.280]   and achieve the positions,
[00:41:26.280 --> 00:41:28.680]   that are highly as likelihood of winning,
[00:41:28.680 --> 00:41:29.880]   for the player,
[00:41:29.880 --> 00:41:31.280]   that's whose turn it is.
[00:41:31.280 --> 00:41:33.280]   And,
[00:41:33.480 --> 00:41:35.080]   another big step,
[00:41:35.080 --> 00:41:38.080]   is that they updated,
[00:41:38.080 --> 00:41:39.480]   from 2015,
[00:41:39.480 --> 00:41:41.680]   they updated the state-of-the-art architecture,
[00:41:41.680 --> 00:41:42.880]   which are now,
[00:41:42.880 --> 00:41:45.080]   the architecture that won ImageNet,
[00:41:45.080 --> 00:41:46.680]   is residual networks,
[00:41:46.680 --> 00:41:47.680]   ResNet,
[00:41:47.680 --> 00:41:48.880]   for ImageNet.
[00:41:48.880 --> 00:41:50.880]   Those, that's it.
[00:41:50.880 --> 00:41:52.680]   And those little changes,
[00:41:52.680 --> 00:41:54.280]   made all the difference.
[00:41:54.280 --> 00:41:57.480]   So that takes us to deep traffic,
[00:41:57.480 --> 00:42:00.080]   and the 8 billion hours stuck in traffic.
[00:42:00.080 --> 00:42:03.280]   America's pastime,
[00:42:03.680 --> 00:42:05.080]   so we tried to simulate,
[00:42:05.080 --> 00:42:07.280]   driving,
[00:42:07.280 --> 00:42:09.480]   the behavioral layer of driving.
[00:42:09.480 --> 00:42:11.880]   So not the immediate control,
[00:42:11.880 --> 00:42:13.680]   not the motion planning,
[00:42:13.680 --> 00:42:15.680]   but beyond that, on top,
[00:42:15.680 --> 00:42:18.280]   on top of those control decisions,
[00:42:18.280 --> 00:42:20.080]   the human,
[00:42:20.080 --> 00:42:22.080]   interpretable decisions of changing lane,
[00:42:22.080 --> 00:42:23.480]   of speeding up, slowing down.
[00:42:23.480 --> 00:42:24.680]   Modeling that,
[00:42:24.680 --> 00:42:27.480]   in a micro traffic simulation framework,
[00:42:27.480 --> 00:42:29.280]   that's popular in traffic engineering,
[00:42:29.280 --> 00:42:31.080]   the kind of shown here.
[00:42:32.880 --> 00:42:35.080]   We applied deep reinforcement learning to that,
[00:42:35.080 --> 00:42:37.080]   we call it deep traffic.
[00:42:37.080 --> 00:42:40.480]   The goal is to achieve the highest average speed,
[00:42:40.480 --> 00:42:41.880]   over a long period of time,
[00:42:41.880 --> 00:42:44.080]   weaving in and out of traffic.
[00:42:44.080 --> 00:42:46.080]   For students here,
[00:42:46.080 --> 00:42:48.880]   the requirement is to follow the tutorial,
[00:42:48.880 --> 00:42:51.280]   and achieve a speed of 65 miles an hour.
[00:42:51.280 --> 00:42:54.680]   And,
[00:42:54.680 --> 00:42:56.280]   if you really want,
[00:42:56.280 --> 00:42:57.680]   to achieve a speed,
[00:42:57.680 --> 00:42:59.280]   over 70 miles an hour,
[00:42:59.280 --> 00:43:01.080]   which is what's required to win.
[00:43:01.080 --> 00:43:05.080]   And perhaps upload your own image,
[00:43:05.080 --> 00:43:07.680]   to make sure you look good doing it.
[00:43:07.680 --> 00:43:10.880]   What you should do,
[00:43:10.880 --> 00:43:12.480]   clear instructions,
[00:43:12.480 --> 00:43:14.480]   to compete, read the tutorial.
[00:43:14.480 --> 00:43:19.080]   You can change parameters in the code box,
[00:43:19.080 --> 00:43:20.280]   on that website,
[00:43:20.280 --> 00:43:22.880]   cars.mit.edu/deeptraffic.
[00:43:22.880 --> 00:43:25.680]   Click the white button that says apply code,
[00:43:25.680 --> 00:43:27.680]   which applies the code that you write.
[00:43:27.680 --> 00:43:28.880]   These are the parameters,
[00:43:28.880 --> 00:43:30.480]   that you specify for the neural network.
[00:43:31.480 --> 00:43:33.480]   It applies those parameters,
[00:43:33.480 --> 00:43:35.680]   creates the architecture that you specify.
[00:43:35.680 --> 00:43:37.080]   And now you have,
[00:43:37.080 --> 00:43:39.080]   a network written in JavaScript,
[00:43:39.080 --> 00:43:40.880]   living in the browser, ready to be trained.
[00:43:40.880 --> 00:43:42.480]   Then you click,
[00:43:42.480 --> 00:43:45.080]   the blue button that says run training.
[00:43:45.080 --> 00:43:47.680]   And that trains the network,
[00:43:47.680 --> 00:43:51.680]   much faster than what's actually being visualized,
[00:43:51.680 --> 00:43:52.680]   in the browser.
[00:43:52.680 --> 00:43:55.080]   A thousand times faster,
[00:43:55.080 --> 00:43:56.680]   by evolving the game,
[00:43:56.680 --> 00:43:57.680]   making decisions,
[00:43:57.680 --> 00:43:59.080]   taking in the grid space,
[00:43:59.080 --> 00:44:00.680]   I'll talk about here in a second.
[00:44:00.680 --> 00:44:03.080]   The speed limit is 80 miles an hour.
[00:44:03.080 --> 00:44:05.480]   Based on the various adjustments,
[00:44:05.480 --> 00:44:06.680]   when we went to the game,
[00:44:06.680 --> 00:44:08.480]   reaching 80 miles an hour,
[00:44:08.480 --> 00:44:09.880]   is certainly impossible,
[00:44:09.880 --> 00:44:11.280]   on average.
[00:44:11.280 --> 00:44:13.080]   And reaching some of the speeds,
[00:44:13.080 --> 00:44:14.680]   that we've achieved last year,
[00:44:14.680 --> 00:44:17.280]   is much, much, much more difficult.
[00:44:17.280 --> 00:44:20.080]   Finally, when you're happy,
[00:44:20.080 --> 00:44:21.480]   and the training is done,
[00:44:21.480 --> 00:44:24.880]   submit the model to competition.
[00:44:26.480 --> 00:44:28.280]   For those super eager,
[00:44:28.280 --> 00:44:29.480]   dedicated students,
[00:44:29.480 --> 00:44:31.280]   you can do so every five minutes.
[00:44:31.280 --> 00:44:35.080]   And to visualize your submission,
[00:44:35.080 --> 00:44:37.880]   you can click,
[00:44:37.880 --> 00:44:40.080]   the request visualization,
[00:44:40.080 --> 00:44:41.680]   specifying the custom image,
[00:44:41.680 --> 00:44:42.480]   and the color.
[00:44:42.480 --> 00:44:47.080]   Okay, so here's the simulation.
[00:44:47.080 --> 00:44:48.880]   Speed limit 80 miles an hour,
[00:44:48.880 --> 00:44:51.680]   cars, 20 on the screen.
[00:44:51.680 --> 00:44:53.880]   One of them is a red one in this case.
[00:44:53.880 --> 00:44:56.280]   That's, that one is controlled by neural network.
[00:44:56.680 --> 00:44:58.680]   It's speed, it's allowed the actions,
[00:44:58.680 --> 00:45:00.280]   to speed up, slow down,
[00:45:00.280 --> 00:45:03.280]   change lanes, left, right,
[00:45:03.280 --> 00:45:05.280]   or stay exactly the same.
[00:45:05.280 --> 00:45:10.080]   The other cars,
[00:45:10.080 --> 00:45:11.880]   are pretty dumb.
[00:45:11.880 --> 00:45:14.880]   They speed up, slow down, turn left, right,
[00:45:14.880 --> 00:45:17.280]   but they don't have a purpose in their existence.
[00:45:17.280 --> 00:45:18.880]   They do so randomly.
[00:45:18.880 --> 00:45:22.680]   Or at least purpose has not been discovered.
[00:45:22.680 --> 00:45:25.480]   The road, the car, the speed.
[00:45:25.480 --> 00:45:27.280]   The road is a grid space.
[00:45:27.280 --> 00:45:30.480]   An occupancy grid that specifies,
[00:45:30.480 --> 00:45:32.680]   when it's empty,
[00:45:32.680 --> 00:45:34.680]   it's set to,
[00:45:34.680 --> 00:45:36.480]   80.
[00:45:36.480 --> 00:45:37.680]   Meaning,
[00:45:37.680 --> 00:45:38.880]   that,
[00:45:38.880 --> 00:45:42.080]   the grid value,
[00:45:42.080 --> 00:45:44.480]   is whatever speed is achievable,
[00:45:44.480 --> 00:45:46.080]   if you were inside that grid.
[00:45:46.080 --> 00:45:49.080]   And when there's other cars that are going slow,
[00:45:49.080 --> 00:45:50.280]   the value in that grid,
[00:45:50.280 --> 00:45:52.080]   is the speed of that car.
[00:45:52.080 --> 00:45:53.480]   That's the state space,
[00:45:53.480 --> 00:45:54.880]   that's the state representation.
[00:45:55.280 --> 00:45:56.880]   And you can choose how much,
[00:45:56.880 --> 00:45:59.280]   what slice that state space you take in.
[00:45:59.280 --> 00:46:01.080]   That's the input to the neural network.
[00:46:01.080 --> 00:46:07.280]   For visualization purposes,
[00:46:07.280 --> 00:46:08.080]   you can choose,
[00:46:08.080 --> 00:46:10.080]   normal speed or fast speed,
[00:46:10.080 --> 00:46:11.080]   for watching,
[00:46:11.080 --> 00:46:12.880]   the network operate.
[00:46:12.880 --> 00:46:16.480]   And there's display options,
[00:46:16.480 --> 00:46:18.080]   to help you build intuition,
[00:46:18.080 --> 00:46:19.480]   about what the network takes in,
[00:46:19.480 --> 00:46:21.680]   and what space the car is operating in.
[00:46:21.680 --> 00:46:22.880]   The default,
[00:46:22.880 --> 00:46:25.280]   is no extra information is added.
[00:46:25.280 --> 00:46:26.280]   Then there's the,
[00:46:26.280 --> 00:46:27.480]   learning input,
[00:46:27.480 --> 00:46:29.280]   which visualizes exactly,
[00:46:29.280 --> 00:46:30.880]   which part of the road,
[00:46:30.880 --> 00:46:33.480]   the, is serves as the input to the network.
[00:46:33.480 --> 00:46:34.680]   Then there is the,
[00:46:34.680 --> 00:46:36.280]   safety system,
[00:46:36.280 --> 00:46:37.880]   which I'll describe in a little bit,
[00:46:37.880 --> 00:46:39.880]   which is all the parts of the road,
[00:46:39.880 --> 00:46:41.680]   the car is not allowed to go into,
[00:46:41.680 --> 00:46:43.480]   because it would result in a collision.
[00:46:43.480 --> 00:46:44.680]   And that would JavaScript,
[00:46:44.680 --> 00:46:46.080]   would be very difficult to animate.
[00:46:46.080 --> 00:46:48.480]   And the full map.
[00:46:48.480 --> 00:46:51.480]   Here's a safety system.
[00:46:51.480 --> 00:46:52.680]   You could think of this system,
[00:46:52.680 --> 00:46:54.880]   as ACC,
[00:46:54.880 --> 00:46:57.680]   basic radar ultrasonic sensors,
[00:46:57.680 --> 00:46:59.680]   helping you avoid the obvious,
[00:46:59.680 --> 00:47:00.880]   collisions to,
[00:47:00.880 --> 00:47:03.280]   obviously detectable objects around you.
[00:47:03.280 --> 00:47:05.080]   And the task for this red car,
[00:47:05.080 --> 00:47:06.080]   for this neural network,
[00:47:06.080 --> 00:47:07.480]   is to move about,
[00:47:07.480 --> 00:47:08.880]   this space,
[00:47:08.880 --> 00:47:11.880]   is to move about the space,
[00:47:11.880 --> 00:47:14.480]   under the constraints of the safety system.
[00:47:14.480 --> 00:47:18.480]   The red shows all the parts of the grid,
[00:47:18.480 --> 00:47:19.880]   it's not able to move into.
[00:47:21.880 --> 00:47:22.880]   So the goal for the car,
[00:47:22.880 --> 00:47:24.880]   is to not get stuck in traffic,
[00:47:24.880 --> 00:47:28.080]   is make big sweeping motions,
[00:47:28.080 --> 00:47:30.480]   to avoid crowds of cars.
[00:47:30.480 --> 00:47:33.880]   The input,
[00:47:33.880 --> 00:47:35.080]   like DQN,
[00:47:35.080 --> 00:47:36.280]   is the state space,
[00:47:36.280 --> 00:47:38.880]   the output is the value of the different actions.
[00:47:38.880 --> 00:47:41.880]   And based on the epsilon parameter,
[00:47:41.880 --> 00:47:44.280]   through training and through,
[00:47:44.280 --> 00:47:46.880]   inference evaluation process,
[00:47:46.880 --> 00:47:48.480]   you choose,
[00:47:48.480 --> 00:47:50.280]   how much exploration you want to do.
[00:47:50.280 --> 00:47:51.680]   These are all parameters.
[00:47:52.480 --> 00:47:54.480]   The learning is done in the browser,
[00:47:54.480 --> 00:47:56.480]   on your own computer,
[00:47:56.480 --> 00:48:00.680]   utilizing only the CPU.
[00:48:00.680 --> 00:48:03.680]   The action space,
[00:48:03.680 --> 00:48:04.680]   there's five,
[00:48:04.680 --> 00:48:07.080]   giving you some of the variables here,
[00:48:07.080 --> 00:48:08.680]   perhaps you go back to the slides,
[00:48:08.680 --> 00:48:09.480]   to look at it.
[00:48:09.480 --> 00:48:10.680]   The brain,
[00:48:10.680 --> 00:48:11.680]   quote-unquote,
[00:48:11.680 --> 00:48:14.080]   is the thing that takes in,
[00:48:14.080 --> 00:48:15.080]   the state,
[00:48:15.080 --> 00:48:16.880]   and the reward,
[00:48:16.880 --> 00:48:19.280]   takes a forward pass through the state,
[00:48:19.280 --> 00:48:20.680]   and produces the next action.
[00:48:21.480 --> 00:48:24.280]   The brain is where the neural network is contained,
[00:48:24.280 --> 00:48:26.480]   both for the training and the evaluation.
[00:48:26.480 --> 00:48:28.880]   The learning input,
[00:48:28.880 --> 00:48:30.880]   can be controlled in width,
[00:48:30.880 --> 00:48:33.080]   forward length,
[00:48:33.080 --> 00:48:34.280]   and backward length.
[00:48:34.280 --> 00:48:35.280]   Lane side,
[00:48:35.280 --> 00:48:37.480]   number of lanes to the side that you see,
[00:48:37.480 --> 00:48:38.680]   patches ahead,
[00:48:38.680 --> 00:48:40.480]   is the patches ahead that you see,
[00:48:40.480 --> 00:48:41.480]   patches behind,
[00:48:41.480 --> 00:48:43.080]   is patches behind that you see.
[00:48:43.080 --> 00:48:45.680]   New this year,
[00:48:45.680 --> 00:48:48.680]   can control the number of agents,
[00:48:48.680 --> 00:48:51.080]   that are controlled by the neural network.
[00:48:52.080 --> 00:48:53.680]   Anywhere from one,
[00:48:53.680 --> 00:48:55.080]   to ten.
[00:48:55.080 --> 00:48:59.480]   And the evaluation,
[00:48:59.480 --> 00:49:01.080]   is performed exactly the same way.
[00:49:01.080 --> 00:49:03.880]   You have to achieve the highest average speed,
[00:49:03.880 --> 00:49:04.680]   for the agents.
[00:49:04.680 --> 00:49:08.280]   The very critical thing here is,
[00:49:08.280 --> 00:49:11.080]   the agents are not aware of each other.
[00:49:11.080 --> 00:49:15.280]   So they're not jointly planning.
[00:49:15.280 --> 00:49:18.480]   The network is trained,
[00:49:18.480 --> 00:49:19.680]   under the,
[00:49:20.080 --> 00:49:21.880]   joint objective,
[00:49:21.880 --> 00:49:24.480]   of achieving the average speed for all of them.
[00:49:24.480 --> 00:49:28.680]   But the actions are taking in a greedy way for each.
[00:49:28.680 --> 00:49:32.080]   It's very interesting what can be learned in this way.
[00:49:32.080 --> 00:49:35.480]   Because this kinds of approaches are scalable,
[00:49:35.480 --> 00:49:37.080]   to an arbitrary number of cars.
[00:49:37.080 --> 00:49:40.080]   And you can imagine us plopping down,
[00:49:40.080 --> 00:49:43.280]   the best cars from this class together.
[00:49:43.280 --> 00:49:45.480]   And having them compete,
[00:49:45.480 --> 00:49:46.880]   in this way.
[00:49:46.880 --> 00:49:48.480]   The best neural networks.
[00:49:49.480 --> 00:49:53.280]   Because they're full in their greedy operation.
[00:49:53.280 --> 00:49:57.080]   The number of networks that can concurrently operate,
[00:49:57.080 --> 00:49:58.680]   is fully scalable.
[00:49:58.680 --> 00:50:01.280]   There's a lot of parameters.
[00:50:01.280 --> 00:50:04.680]   The temporal window.
[00:50:04.680 --> 00:50:09.880]   The layers, the many layers types that can be added.
[00:50:09.880 --> 00:50:12.280]   Here's a fully connected layer with ten neurons.
[00:50:12.280 --> 00:50:15.480]   The activation functions, all of these things can be customized.
[00:50:15.480 --> 00:50:17.480]   As is specified in the tutorial.
[00:50:18.480 --> 00:50:21.680]   The final layer, a fully connected layer with,
[00:50:21.680 --> 00:50:23.480]   output of five,
[00:50:23.480 --> 00:50:28.080]   regression, giving the value of each of the five actions.
[00:50:28.080 --> 00:50:31.280]   And there's a lot of more specific parameters.
[00:50:31.280 --> 00:50:32.680]   Some of which I've discussed.
[00:50:32.680 --> 00:50:37.080]   From gamma, to epsilon,
[00:50:37.080 --> 00:50:40.680]   to experience replay size,
[00:50:40.680 --> 00:50:44.280]   to learning rate and temporal window.
[00:50:45.680 --> 00:50:49.680]   The optimizer, the learning rate, momentum, batch size,
[00:50:49.680 --> 00:50:53.080]   L2, L1 decay for regularization and so on.
[00:50:53.080 --> 00:50:56.880]   There's a big white button that says apply code that you press.
[00:50:56.880 --> 00:50:59.680]   That kills all the work you've done up to this point.
[00:50:59.680 --> 00:51:00.880]   So be careful doing it.
[00:51:00.880 --> 00:51:03.680]   You should be doing it only at the very beginning.
[00:51:03.680 --> 00:51:07.480]   If you happen to leave your computer running,
[00:51:07.480 --> 00:51:10.480]   in training for several days, as folks have done.
[00:51:10.480 --> 00:51:13.880]   The blue training button, you press.
[00:51:13.880 --> 00:51:15.480]   And it trains based on the parameters.
[00:51:15.480 --> 00:51:16.280]   You specify.
[00:51:16.280 --> 00:51:20.880]   And the network state gets shipped to the main simulation from time to time.
[00:51:20.880 --> 00:51:22.880]   So the thing you see in the browser,
[00:51:22.880 --> 00:51:24.280]   as you open up the website,
[00:51:24.280 --> 00:51:27.080]   is running the same network that's being trained.
[00:51:27.080 --> 00:51:29.680]   And regularly it updates that network.
[00:51:29.680 --> 00:51:31.080]   So it's getting better and better.
[00:51:31.080 --> 00:51:33.280]   Even if the training takes weeks for you.
[00:51:33.280 --> 00:51:36.480]   It's constantly updating the network you see on the left.
[00:51:36.480 --> 00:51:39.680]   So if the car, for the network that you're training,
[00:51:39.680 --> 00:51:41.880]   is just standing in place and not moving.
[00:51:41.880 --> 00:51:43.880]   It's probably,
[00:51:44.880 --> 00:51:47.480]   time to restart and change the parameters.
[00:51:47.480 --> 00:51:49.680]   Maybe add a few layers to your network.
[00:51:49.680 --> 00:51:55.080]   Number of iterations is certainly an important parameter to control.
[00:51:55.080 --> 00:52:00.680]   And the evaluation is something we've done a lot of work on since last year.
[00:52:00.680 --> 00:52:03.280]   To remove the degree of randomness.
[00:52:03.280 --> 00:52:04.880]   To remove the,
[00:52:04.880 --> 00:52:09.080]   the incentive to submit the same code over and over again.
[00:52:09.080 --> 00:52:11.280]   To hope to produce a higher reward,
[00:52:11.280 --> 00:52:13.080]   a higher evaluation score.
[00:52:14.480 --> 00:52:19.880]   The method for evaluation is to collect the average speed over 10 runs.
[00:52:19.880 --> 00:52:25.080]   About 45 seconds of game each.
[00:52:25.080 --> 00:52:26.280]   Not minutes.
[00:52:26.280 --> 00:52:28.280]   45 simulated seconds.
[00:52:28.280 --> 00:52:30.880]   And there is 500 of those.
[00:52:30.880 --> 00:52:33.480]   And we take the median speed of the 500 runs.
[00:52:33.480 --> 00:52:35.480]   It's done server side.
[00:52:35.480 --> 00:52:37.080]   So extremely difficult to cheat.
[00:52:37.080 --> 00:52:38.680]   I urge you to try.
[00:52:38.680 --> 00:52:41.680]   And you can try it locally.
[00:52:41.680 --> 00:52:43.480]   There's a start evaluation run.
[00:52:43.480 --> 00:52:44.680]   But that one doesn't count.
[00:52:44.680 --> 00:52:46.880]   That's just for you to feel better about your network.
[00:52:46.880 --> 00:52:52.680]   That should produce a result that's very similar to the one we'll produce on the server.
[00:52:52.680 --> 00:52:55.280]   Is to build your own intuition.
[00:52:55.280 --> 00:52:57.280]   And as I said,
[00:52:57.280 --> 00:52:59.480]   we significantly reduce the influence of randomness.
[00:52:59.480 --> 00:53:00.480]   So the,
[00:53:00.480 --> 00:53:01.880]   the score,
[00:53:01.880 --> 00:53:07.480]   the speed you get for the network you design should be very similar with every evaluation.
[00:53:07.480 --> 00:53:10.480]   Loading and saving.
[00:53:10.480 --> 00:53:13.280]   If the network is huge and you want to switch computers,
[00:53:13.280 --> 00:53:14.480]   you can save the network.
[00:53:14.480 --> 00:53:16.480]   It saves both the architecture of the network.
[00:53:16.480 --> 00:53:19.480]   And the weights on the network.
[00:53:19.480 --> 00:53:21.480]   And you can load it back in.
[00:53:21.480 --> 00:53:25.080]   Obviously, when you load it in,
[00:53:25.080 --> 00:53:29.280]   it's not saving any of the data you've already done.
[00:53:29.280 --> 00:53:33.080]   You can't do transfer learning with JavaScript in the browser yet.
[00:53:33.080 --> 00:53:35.680]   Submitting your network,
[00:53:35.680 --> 00:53:37.680]   submit model to competition.
[00:53:37.680 --> 00:53:39.680]   And make sure you run training first.
[00:53:39.680 --> 00:53:40.480]   Otherwise,
[00:53:41.280 --> 00:53:42.280]   it'll be initiated,
[00:53:42.280 --> 00:53:44.880]   the weights are initiated randomly and will not do so well.
[00:53:44.880 --> 00:53:49.480]   You can resubmit as often you like and the highest score is what counts.
[00:53:49.480 --> 00:53:52.480]   The coolest part is you can load your custom image,
[00:53:52.480 --> 00:53:56.280]   specify colors and request the visualization.
[00:53:56.280 --> 00:54:00.480]   We have not yet shown the visualization,
[00:54:00.480 --> 00:54:02.480]   but I promise you it's going to be awesome.
[00:54:02.480 --> 00:54:03.480]   Again,
[00:54:03.480 --> 00:54:05.080]   read the tutorial,
[00:54:05.080 --> 00:54:06.880]   change the parameters in the code box,
[00:54:06.880 --> 00:54:08.680]   click apply code, run training.
[00:54:09.280 --> 00:54:11.280]   Everybody in this room on the way home,
[00:54:11.280 --> 00:54:12.280]   on the train,
[00:54:12.280 --> 00:54:14.280]   hopefully not in your car,
[00:54:14.280 --> 00:54:16.080]   should be able to do this in the browser.
[00:54:16.080 --> 00:54:17.680]   And then you can visualize,
[00:54:17.680 --> 00:54:20.480]   request visualization because it's an expensive process.
[00:54:20.480 --> 00:54:22.880]   You have to want it for us to do it.
[00:54:22.880 --> 00:54:25.680]   Because we have to run in server side.
[00:54:25.680 --> 00:54:30.480]   Competition link is there.
[00:54:30.480 --> 00:54:32.680]   GitHub starter code is there.
[00:54:32.680 --> 00:54:36.480]   And the details for those that truly want to win is in the archive paper.
[00:54:36.480 --> 00:54:38.080]   So the question,
[00:54:39.080 --> 00:54:43.280]   that will come up throughout is whether these reinforcement learning approaches
[00:54:43.280 --> 00:54:48.680]   are at all or rather if action planning control is amenable to learning.
[00:54:48.680 --> 00:54:52.680]   Certainly in the case of driving,
[00:54:52.680 --> 00:54:55.280]   we can't do what AlphaGo Zero did.
[00:54:55.280 --> 00:54:59.880]   We can't learn from scratch from self-play
[00:54:59.880 --> 00:55:03.480]   because that would result in millions of crashes
[00:55:03.480 --> 00:55:07.880]   in order to learn to avoid the crashes.
[00:55:08.480 --> 00:55:11.680]   Unless we're working like we are deep crash on the RC car
[00:55:11.680 --> 00:55:13.480]   or we're working in simulation.
[00:55:13.480 --> 00:55:16.080]   So we can look at expert data.
[00:55:16.080 --> 00:55:17.280]   We can look at driver data,
[00:55:17.280 --> 00:55:18.880]   which we have a lot of and learn from.
[00:55:18.880 --> 00:55:21.280]   It's an open question whether this is applicable.
[00:55:21.280 --> 00:55:23.080]   To date,
[00:55:23.080 --> 00:55:26.880]   and I bring up two companies because they're both guest speakers.
[00:55:26.880 --> 00:55:33.280]   Deep IRL is not involved in the most successful robots operating in the real world.
[00:55:33.280 --> 00:55:37.480]   In the case of Boston Dynamics,
[00:55:38.480 --> 00:55:41.080]   most of the perception,
[00:55:41.080 --> 00:55:44.880]   control and planning like in this robot,
[00:55:44.880 --> 00:55:48.080]   does not involve learning approaches
[00:55:48.080 --> 00:55:51.480]   except with minimal addition on the perception side.
[00:55:51.480 --> 00:55:54.280]   Best of our knowledge.
[00:55:54.280 --> 00:55:57.680]   And certainly the same is true with Waymo,
[00:55:57.680 --> 00:56:00.280]   as the speaker on Friday will talk about.
[00:56:00.280 --> 00:56:03.880]   Deep learning is used a little bit in perception on top,
[00:56:03.880 --> 00:56:07.080]   but most of the work is done from the sensors
[00:56:07.280 --> 00:56:11.480]   and the optimization based, the model based approaches.
[00:56:11.480 --> 00:56:18.080]   Trajectory generation and optimizing which trajectory is best to avoid collisions.
[00:56:18.080 --> 00:56:20.680]   Deep IRL is not involved.
[00:56:20.680 --> 00:56:25.280]   And coming back and back again,
[00:56:25.280 --> 00:56:28.280]   the unexpected local pockets of higher award,
[00:56:28.280 --> 00:56:31.880]   which arise in all of these situations when applied in the real world.
[00:56:31.880 --> 00:56:34.480]   So for the cat video,
[00:56:34.880 --> 00:56:37.680]   that's pretty short where the cats are ringing the bell
[00:56:37.680 --> 00:56:40.280]   and they're learning that the ring of the bell
[00:56:40.280 --> 00:56:44.280]   is mapping to food.
[00:56:44.280 --> 00:56:50.280]   I urge you to think about how that can evolve over time in unexpected ways.
[00:56:50.280 --> 00:56:52.680]   That may not have a desirable effect.
[00:56:52.680 --> 00:56:55.480]   Where the final reward is in the form of food
[00:56:55.480 --> 00:57:00.080]   and the intended effect is to ring the bell.
[00:57:03.280 --> 00:57:05.080]   That's where AI safety comes in.
[00:57:05.080 --> 00:57:08.280]   For the artificial general intelligence course in two weeks,
[00:57:08.280 --> 00:57:10.280]   that's something we'll explore extensively.
[00:57:10.280 --> 00:57:17.280]   It's how these reinforcement learning planning algorithms
[00:57:17.280 --> 00:57:20.280]   will evolve in ways that are not expected.
[00:57:20.280 --> 00:57:23.080]   And how we can constrain them,
[00:57:23.080 --> 00:57:28.280]   how we can design reward functions that result in safe operation.
[00:57:28.280 --> 00:57:32.880]   So I encourage you to come to the talk on Friday,
[00:57:33.280 --> 00:57:36.680]   at 1pm, as a reminder, it's a 1pm, not 7pm,
[00:57:36.680 --> 00:57:38.880]   in Stata 32.1.2.3.
[00:57:38.880 --> 00:57:41.480]   And to the awesome talks in two weeks,
[00:57:41.480 --> 00:57:45.880]   from Boston Dynamics to Ray Kurzweil and so on for AGI.
[00:57:45.880 --> 00:57:50.880]   Now tomorrow, we'll talk about computer vision and psych fuse.
[00:57:50.880 --> 00:57:51.880]   Thank you everybody.
[00:57:51.880 --> 00:57:55.240]   [APPLAUSE]

