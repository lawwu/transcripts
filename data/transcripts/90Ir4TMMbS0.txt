
[00:00:00.000 --> 00:00:21.440]   Hi everyone! I'm so excited to be part of this conference and share with you five practical
[00:00:21.440 --> 00:00:29.000]   steps from software developer to AI engineer. And if anyone is wondering here this avatar on the
[00:00:29.000 --> 00:00:36.280]   slide, this is what happens if you ask AI to make you look a little bit more agentic. Alright,
[00:00:36.280 --> 00:00:41.800]   let's get started. So I'm pretty sure everyone is familiar with this image here and the post from
[00:00:41.800 --> 00:00:49.640]   SWIX that defines the new role of the AI engineer. And as you've experienced probably daily in your
[00:00:49.640 --> 00:00:57.000]   jobs, you don't need to be a full ML researcher anymore or data scientist. Things that took months or
[00:00:57.000 --> 00:01:06.280]   years before to get AI projects into production is now able to bring just a couple of API calls.
[00:01:06.280 --> 00:01:16.280]   Super exciting. But still, if you're working with AI, it still makes sense to understand the basics of the
[00:01:16.280 --> 00:01:23.560]   technology. And this involves a couple of things, right? So you have to understand at a basic level
[00:01:23.560 --> 00:01:30.040]   how foundation models work, why they're sometimes producing output that you don't expect in your
[00:01:30.040 --> 00:01:37.240]   application code, right? You have to understand how you can customize the models, how you can, you know,
[00:01:37.240 --> 00:01:42.520]   for example, sometimes fine-tune models to adapt them to your specific use cases and data sets,
[00:01:42.520 --> 00:01:48.520]   how to include functions in your application code to give them access to additional systems.
[00:01:48.520 --> 00:01:55.960]   The good news is if you're just starting on this journey to become an AI engineer, there's plenty of
[00:01:55.960 --> 00:02:02.600]   resources now these days available to you to learn. And I wanted to call out one specific course here,
[00:02:03.240 --> 00:02:09.160]   which is called Generative AI with Large Language Models. A few colleagues in mind, we actually
[00:02:09.160 --> 00:02:14.600]   collaborated with Andrew Ng and the team at deeplearning.ai to put this course together
[00:02:14.600 --> 00:02:21.800]   and help you understand the fundamentals of Generative AI to help you build real-world applications.
[00:02:22.520 --> 00:02:27.000]   If you're curious, it's available on deeplearning.ai and on Coursera.
[00:02:27.000 --> 00:02:37.320]   Now, the second step in this journey is to start get hands-on with the AI developer tools
[00:02:37.320 --> 00:02:40.520]   to help you increase your productivity.
[00:02:40.520 --> 00:02:50.120]   And I think we're all seeing this quote here and we experienced it in our daily jobs that how we do work,
[00:02:50.120 --> 00:02:58.360]   how we develop applications has changed a lot. These days, we can literally use natural language
[00:02:58.360 --> 00:03:06.760]   inputs to interact with applications and really, English has become one of the most popular and
[00:03:06.760 --> 00:03:14.840]   hottest programming languages. I think we can see this happening. For example, you can go these days from
[00:03:14.840 --> 00:03:20.840]   English, English to code, by asking AI to, for example, rewrite a readme file.
[00:03:20.840 --> 00:03:30.840]   We can also do code to English, for example, asking AI to document functions in our code.
[00:03:34.920 --> 00:03:40.680]   But this is not all. If we look at the software development life cycle, I think many of us can
[00:03:40.680 --> 00:03:47.320]   agree that the majority of time we usually spend not writing valuable code, but all the other things
[00:03:47.320 --> 00:03:56.040]   around it. So sometimes up to 70% of unvaluable tasks, which is writing boilerplate code, writing
[00:03:56.040 --> 00:04:03.000]   documentation, trying to maintain old code bases. Right? And sometimes we only have like a fraction of the
[00:04:03.000 --> 00:04:07.800]   of the time, maybe 30%, that we're spending on actually what, you know, creates joy in kind of
[00:04:07.800 --> 00:04:16.600]   the creative tasks in software development. And this is what led us at AWS, this inspired us to create
[00:04:17.240 --> 00:04:25.560]   Amazon Q. Amazon Q. Amazon Q is a generative AI powered assistant specifically developed for software
[00:04:25.560 --> 00:04:35.960]   development. And this is much more than just a coding assistant. Q developer actually uses agents to perform
[00:04:35.960 --> 00:04:43.240]   much more complex tasks and help you automate those. For example, feature development and also code
[00:04:43.240 --> 00:04:50.440]   transformation. Think about working with old Java based code bases that you need to migrate maybe to
[00:04:50.440 --> 00:04:57.400]   your newer Java version. And to show you how this works, I asked my colleague, Mike Chambers,
[00:04:57.400 --> 00:05:00.040]   to put together a quick demo. Let's have a look.
[00:05:00.040 --> 00:05:09.480]   Mike Chambers: With Amazon Q installed inside of my IDE, I can go to new tab and I can start a conversation with
[00:05:09.480 --> 00:05:15.320]   Amazon Q developer and I can do the kinds of things that maybe you'd expect, such as how can I create
[00:05:15.320 --> 00:05:22.920]   a serverless application? And how do I get started? And the chat session brings back a list of instructions
[00:05:22.920 --> 00:05:30.360]   of what I should do, starting off by installing AWS SAM CLI, how to do that, where to get that from,
[00:05:30.360 --> 00:05:37.160]   and how to step through the creation of a project. Now, if I've done that, then serverless SAM, for example,
[00:05:37.160 --> 00:05:42.760]   might actually come back with some generated code. And here is that code. Maybe I don't quite know what
[00:05:42.760 --> 00:05:51.720]   this code does. So I can right click on the code and send it to Amazon Q asking Amazon Q to explain.
[00:05:52.760 --> 00:05:58.840]   And the code then will go into a prompt along with explain and generate an answer. And this is great
[00:05:58.840 --> 00:06:04.920]   for code that's been generated for us. But also imagine code for legacy systems, something that was
[00:06:04.920 --> 00:06:12.280]   worked on years ago by somebody else, where you can get Amazon Q to help explain it. We can also get Amazon
[00:06:12.280 --> 00:06:18.920]   Q to generate code. Now, this is, again, probably the kind of thing you'd expect. I can put in a comment
[00:06:18.920 --> 00:06:24.840]   line inside of my code. In this case, I want to create an input checking function. I'm going to give
[00:06:24.840 --> 00:06:30.120]   it some more definition here that I actually want it to trim any string that's being sent into this
[00:06:30.120 --> 00:06:38.280]   function. And yes, Amazon Q can generate this small function. Well, that's great. But what about if I've got
[00:06:38.280 --> 00:06:45.960]   more code that I need to have generated? Well, I can go to the chat and put in /dev. And I can put in a
[00:06:45.960 --> 00:06:50.920]   much more comprehensive description of something that I would like. In this particular case, I'm going
[00:06:50.920 --> 00:06:58.920]   to ask for it to write a function to search by category in DynamoDB with a bunch of details about
[00:06:58.920 --> 00:07:05.880]   the way that I want the output to be formatted. So this is much more than just a single or a few lines
[00:07:05.880 --> 00:07:11.800]   of code. And in this particular case, what's going to happen is it will come back again with a step-by-step
[00:07:11.800 --> 00:07:19.320]   list of what's required. So I need to add in template.yaml. It's recommending that I create
[00:07:19.320 --> 00:07:27.800]   search by category.mjs and many more things. But this isn't just a big shopping list of things that
[00:07:27.800 --> 00:07:35.800]   I need to do. This is actually a plan. And it's a plan that Amazon Q can actually follow for us. So it generates
[00:07:35.800 --> 00:07:42.760]   some code as a change set something that we can look at the difference between our current code
[00:07:42.760 --> 00:07:49.080]   and what it suggests. And if we like that, we can actually click on the insert code button. And it
[00:07:49.080 --> 00:07:56.840]   will add all of that code into our project way more than just a couple of lines. So Amazon Q developer is
[00:07:56.840 --> 00:08:02.280]   much more than just code completion.
[00:08:02.280 --> 00:08:09.480]   All right. If you're curious to learn more about Amazon Q, Amazon Q developer, we have a couple of
[00:08:09.480 --> 00:08:15.480]   more sessions throughout this day. So make sure you're checking those expo sessions. And we also do have a
[00:08:15.480 --> 00:08:23.320]   session at our AWS booth here. You can also visit our Amazon Q developer center for much more examples what you can do with it.
[00:08:23.320 --> 00:08:33.080]   All right. Let's come to step three. And this is where the fun starts. Start prototyping and building with AI.
[00:08:33.080 --> 00:08:39.640]   And the fun includes a couple of steps, right? Everyone developing with AI knows this.
[00:08:39.640 --> 00:08:46.120]   It starts all with defining your use case. And then really, you're on this road, trying to choose from
[00:08:46.120 --> 00:08:51.960]   different models. You're trying to, you know, customize them to your use case, decide whether it's prompt
[00:08:51.960 --> 00:08:57.720]   engineering, whether you do rec, whether you need to do a little bit of fine tuning there with your data.
[00:08:57.720 --> 00:09:03.960]   And of course, across the whole development workflow, you have to incorporate responsible
[00:09:03.960 --> 00:09:11.240]   AI policies, making sure data is private and secure, and also implementing guardrails into your application.
[00:09:11.240 --> 00:09:17.320]   And then when you're integrated, another fun part, obviously working with the agents, what we're hearing a lot
[00:09:17.320 --> 00:09:22.840]   here throughout this conference and the fun topic of, you know, how to keep them up to date.
[00:09:22.840 --> 00:09:30.680]   GNI ops. I think there's a lot of terms for that. MFM ops, LLM ops. So really kind of a lot of things to consider here.
[00:09:30.680 --> 00:09:38.920]   I want to dive in briefly into the topic of models to choose. And this is really an important topic.
[00:09:38.920 --> 00:09:45.160]   When you're evaluating models, you have to really evaluate them thoroughly, because most likely,
[00:09:45.160 --> 00:09:52.680]   there's not just going to be one size fits all for you. In fact, if you look at all your use cases
[00:09:52.680 --> 00:09:57.480]   you want to implement, there's likely no one model to rule them all.
[00:10:00.200 --> 00:10:08.520]   And this is why we developed Amazon Bedrock. Bedrock is a fully managed service that gives you access to a wide
[00:10:08.520 --> 00:10:15.400]   range of leading foundation models that you can start experimenting with, implementing into your
[00:10:15.400 --> 00:10:23.800]   applications. It also integrates the tooling you need to customize your model, whether it's fine tuning,
[00:10:23.800 --> 00:10:31.480]   also to include reg workflows, to build agents, and of course, everything in a secure environment where
[00:10:31.480 --> 00:10:33.880]   you are in full control of your data.
[00:10:33.880 --> 00:10:43.080]   And speaking of choice, just to give you a quick overview, as of today, this is the selection of
[00:10:43.080 --> 00:10:50.200]   models you can choose from. We're working with leading companies such as AI 21 labs, Entropic,
[00:10:50.200 --> 00:10:58.600]   Coher, Meta, Mistral AI, Stability AI, and we also offer our own Amazon Titan models for you to choose from.
[00:10:58.600 --> 00:11:04.600]   And I'm super excited just to call this out. Last week, together with Entropic's launch,
[00:11:04.600 --> 00:11:13.640]   we integrated Cloud 3.5 Sonnet on Amazon Bedrock as well, so you can also, since last week, use this model.
[00:11:13.640 --> 00:11:22.040]   Super exciting. Now, with choice also comes responsibility, right? And we continuously
[00:11:22.040 --> 00:11:28.760]   innovate and trying to make it easier for you to build applications across the different model types.
[00:11:28.760 --> 00:11:36.360]   And just a few weeks ago, we introduced a new unified Converse API on Amazon Bedrock.
[00:11:36.920 --> 00:11:44.600]   And what does this do? The unified Converse API helps you with a new unified method structured invocation,
[00:11:44.600 --> 00:11:51.720]   meaning you can use the same parameters and bodies regardless of which model you choose.
[00:11:51.720 --> 00:11:58.200]   And we are on the platform side. We're handling this translation if parameters are called different for
[00:11:58.200 --> 00:12:03.160]   the different models, handling the system user assistant prompts for you, and also giving you a
[00:12:03.160 --> 00:12:09.160]   consistent output format. And as well, having native function calling support in here. But let me show
[00:12:09.160 --> 00:12:16.280]   you how this looks in code. So here's the Python example that shows how you can use the new API.
[00:12:16.280 --> 00:12:24.200]   This is Python, so we're starting by just integrating the Python SDK client here. And then you can define
[00:12:24.200 --> 00:12:30.760]   this list of messages. And here's, for example, where you put in your user message prompts. You can put in
[00:12:30.760 --> 00:12:38.200]   system prompts as well. And then this message list, you can just pass in this single API call using the
[00:12:38.200 --> 00:12:43.960]   Converse API here. In the model ID, you can choose which model you want to test. Here I'm using an
[00:12:43.960 --> 00:12:51.240]   entropic model. And then pass the messages and also the inference parameters. And again, in this API,
[00:12:51.240 --> 00:12:58.760]   all those parameters are standardized, and we're going to make the work behind the covers to convert this to
[00:12:58.760 --> 00:13:05.080]   the specific format that the model is expecting. So you have an easy way to work across different models.
[00:13:05.080 --> 00:13:13.080]   Similarly, here for function calling, we do have the support built in that with the models that support it.
[00:13:13.080 --> 00:13:21.720]   So how we implement this is by defining a tool list. So tool here equivalent to the functions you want to give
[00:13:22.280 --> 00:13:28.280]   access to. And then when you're doing the Converse API call, you can pass this list of tools.
[00:13:28.280 --> 00:13:36.600]   All right. If you want to find out more about Converse API, here's a link to our generative AI space on
[00:13:36.600 --> 00:13:43.560]   community.aws, which has a lot more tutorials, code examples, not just for Python, but across different
[00:13:43.560 --> 00:13:49.640]   languages as well. So check it out. The author here, Dennis Traub, is also somewhere in the audience
[00:13:49.640 --> 00:13:54.040]   here this week. So if you want to connect with him, talk about different code examples and how to use
[00:13:54.040 --> 00:14:03.320]   the API, feel free to reach out. All right. Now let's integrate AI into our applications. And this can
[00:14:03.320 --> 00:14:08.360]   be a whole session in its own, but I want to focus on one of the hottest topics right now that we're
[00:14:08.360 --> 00:14:16.120]   discussing during the conference, which is, of course, agents. And I have one more demo here, and I asked my
[00:14:16.120 --> 00:14:22.760]   colleague Mike last time to put together an exciting demo to show you what you can do with agents.
[00:14:22.760 --> 00:14:27.720]   Mike. And we need sound.
[00:14:27.720 --> 00:14:35.080]   To be able to create agentic workflows right inside of the AWS console and inside of the service. It works
[00:14:35.080 --> 00:14:41.960]   fully serverless, and I've used it to create an agent that plays Minecraft. Let me show you how I did it.
[00:14:41.960 --> 00:14:48.040]   If we jump into the AWS console, go down the menu on the left-hand side to agents, you can see the
[00:14:48.040 --> 00:14:54.840]   agent screen here, and I can open up my agent, my Minecraft agent. Now, if I just go into agent builder
[00:14:54.840 --> 00:14:59.960]   and just expand the screen out a little bit, you get to see some of the parameters that I used to create
[00:14:59.960 --> 00:15:05.960]   this agent. So you can see the large language model I used, in this case, Claude3 Haiku. And you can also see
[00:15:05.960 --> 00:15:12.040]   this, the instructions for the agent. Now, this is not some notes for myself. This is actually prompt engineering
[00:15:12.040 --> 00:15:17.880]   that we're doing to explain how we want the agent, in this case, the Minecraft bot, to play the game.
[00:15:17.880 --> 00:15:23.480]   And then we also have to add some tools in, right, some Minecraft tools. So we do that through actions
[00:15:23.480 --> 00:15:28.040]   and inside of action groups. So I've got a couple of different action groups. We've got Minecraft actions
[00:15:28.040 --> 00:15:34.520]   and Minecraft experimental. Let's have a look at actions. And inside of here, we can see some really
[00:15:34.520 --> 00:15:40.360]   simple things, some actions that the bot will be able to do. And these are all linked up to code.
[00:15:40.360 --> 00:15:45.560]   So we've got the action to jump. We've got the action to dig. And you can see the description here
[00:15:45.560 --> 00:15:51.000]   for action to dig. It's got some instructions. Again, this is prompt engineering. And then we've got some
[00:15:51.000 --> 00:15:56.920]   parameters that we can select, collect. In fact, we require these parameters. So the bot needs to get
[00:15:56.920 --> 00:16:02.120]   these for us. If I scroll down a little further, there's a couple of really simple actions in here.
[00:16:02.120 --> 00:16:10.600]   Action to get a player location, and action to move to a location. I want to show you those in action,
[00:16:10.600 --> 00:16:17.560]   because the bot can actually problem solve and reason its way to be able to use these tools to solve
[00:16:17.560 --> 00:16:24.040]   simple problems. Let's jump into the game. And so it is nighttime. So let's set it to be the daytime.
[00:16:24.040 --> 00:16:30.200]   So that we can see what's going on. So set time to day. Okay. And there in the middle of the screen,
[00:16:30.200 --> 00:16:36.840]   you can see Rocky. Rocky is the bedrock agent running inside of the game. And we can talk to it. We can
[00:16:36.840 --> 00:16:44.200]   have a chat session. But what about if we want it to come to us? Now, there is no tool to come to us.
[00:16:44.200 --> 00:16:48.040]   So I'm just going to back up a little bit further, make it a little bit more of a challenge. And I'm
[00:16:48.040 --> 00:16:53.960]   going to say, come to me in chat. And what's going to happen now is that the agent's going to reason
[00:16:53.960 --> 00:16:59.160]   through a whole set of actions. It's going to look to see who requested that. It's then going to take
[00:16:59.160 --> 00:17:05.160]   that name, and that's my name. And it's going to find the location of that player. And then it's going to
[00:17:05.160 --> 00:17:11.960]   map a path from where it currently was to me. All of those things happened all in that blink of an eye.
[00:17:11.960 --> 00:17:17.480]   And there's agentic workflows making all of that happen. This is super exciting. I'm discovering new
[00:17:17.480 --> 00:17:22.120]   things that this bot can do every day. But with that, it's back to you.
[00:17:22.120 --> 00:17:29.480]   All right. Thank you, Mike. If you're curious to know how we did this, check out our booth session.
[00:17:29.480 --> 00:17:35.080]   We're running the demo there as well. And we have another session in the agent's track later today.
[00:17:35.080 --> 00:17:40.360]   So make sure you're popping in there if you want to know more. And, of course, you can find the project
[00:17:40.360 --> 00:17:47.880]   code for this on GitHub. So if you want to play it on your own in how you can integrate agents into a fun
[00:17:47.880 --> 00:17:55.320]   thing, check out this project. All right. We're almost there. So the last step I really want to call out is
[00:17:55.320 --> 00:18:01.400]   stay up to date. There's so much happening in this space, as you all know. And a really good way to do
[00:18:01.400 --> 00:18:08.840]   that is to engage with the community. Speaking of community, I have one last announcement to make.
[00:18:08.840 --> 00:18:16.200]   And I'm super excited to announce that we're transforming our AWS loft here in San Francisco
[00:18:16.200 --> 00:18:23.800]   into the AI engineering hub for the community. So we're super excited to host workshops, events,
[00:18:23.800 --> 00:18:29.720]   and meetups there. If you want to suggest a couple of topics you're most interested in to make those
[00:18:29.720 --> 00:18:35.480]   events most valuable to you, fill out this quick survey here. Also, if you're interested in speaking
[00:18:35.480 --> 00:18:42.760]   or hosting a meetup yourself, you can let us know. And also, we do have another event tonight,
[00:18:42.760 --> 00:18:49.080]   which I think we're reaching capacity or just have reached capacity. But we do have a happy hour with
[00:18:49.080 --> 00:18:56.840]   Entropic tonight at the loft. In case you didn't make it anymore and we're at capacity, don't worry.
[00:18:56.840 --> 00:19:02.760]   We're working on putting together much more events like this in the upcoming weeks and months. So keep an
[00:19:02.760 --> 00:19:09.960]   eye out for those. And with that, I'm coming to the end of my presentation. This wraps it up the five
[00:19:09.960 --> 00:19:17.720]   practical steps to become an AI engineer. And let's innovate together. And I'm looking forward and I'm
[00:19:17.720 --> 00:19:24.840]   excited to see what you built with AI. Thanks so much. Make sure you're checking out the rest of the
[00:19:24.840 --> 00:19:42.280]   sessions here and also pop by our booth outside. Thanks so much.
[00:19:42.280 --> 00:19:47.160]   I'll see you next time.

