
[00:00:00.000 --> 00:00:03.320]   The following is a conversation with Daphne Kohler,
[00:00:03.320 --> 00:00:06.280]   a professor of computer science at Stanford University,
[00:00:06.280 --> 00:00:09.000]   a co-founder of Coursera with Andrew Ng,
[00:00:09.000 --> 00:00:11.880]   and founder and CEO of In-Citro,
[00:00:11.880 --> 00:00:13.400]   a company at the intersection
[00:00:13.400 --> 00:00:15.960]   of machine learning and biomedicine.
[00:00:15.960 --> 00:00:17.840]   We're now in the exciting early days
[00:00:17.840 --> 00:00:20.600]   of using the data-driven methods of machine learning
[00:00:20.600 --> 00:00:22.600]   to help discover and develop new drugs
[00:00:22.600 --> 00:00:24.440]   and treatments at scale.
[00:00:24.440 --> 00:00:27.800]   Daphne and In-Citro are leading the way on this
[00:00:27.800 --> 00:00:29.960]   with breakthroughs that may ripple through
[00:00:29.960 --> 00:00:31.600]   all fields of medicine,
[00:00:31.600 --> 00:00:34.240]   including ones most critical for helping
[00:00:34.240 --> 00:00:36.360]   with the current coronavirus pandemic.
[00:00:36.360 --> 00:00:41.280]   This conversation was recorded before the COVID-19 outbreak.
[00:00:41.280 --> 00:00:43.520]   For everyone feeling the medical, psychological,
[00:00:43.520 --> 00:00:45.600]   and financial burden of this crisis,
[00:00:45.600 --> 00:00:47.680]   I'm sending love your way.
[00:00:47.680 --> 00:00:48.800]   Stay strong.
[00:00:48.800 --> 00:00:50.000]   We're in this together.
[00:00:50.000 --> 00:00:51.760]   We'll beat this thing.
[00:00:51.760 --> 00:00:54.240]   This is the Artificial Intelligence Podcast.
[00:00:54.240 --> 00:00:56.360]   If you enjoy it, subscribe on YouTube,
[00:00:56.360 --> 00:00:58.720]   review it with five stars on Apple Podcast,
[00:00:58.720 --> 00:01:00.080]   support it on Patreon,
[00:01:00.080 --> 00:01:03.440]   or simply connect with me on Twitter at Lex Friedman,
[00:01:03.440 --> 00:01:05.920]   spelled F-R-I-D-M-A-N.
[00:01:05.920 --> 00:01:08.080]   As usual, I'll do a few minutes of ads now
[00:01:08.080 --> 00:01:09.400]   and never any ads in the middle
[00:01:09.400 --> 00:01:11.720]   that can break the flow of the conversation.
[00:01:11.720 --> 00:01:13.040]   I hope that works for you
[00:01:13.040 --> 00:01:15.920]   and doesn't hurt the listening experience.
[00:01:15.920 --> 00:01:17.960]   This show is presented by Cash App,
[00:01:17.960 --> 00:01:20.280]   the number one finance app in the App Store.
[00:01:20.280 --> 00:01:23.400]   When you get it, use code LEXPODCAST.
[00:01:23.400 --> 00:01:25.600]   Cash App lets you send money to friends,
[00:01:25.600 --> 00:01:27.880]   buy Bitcoin, and invest in the stock market
[00:01:27.880 --> 00:01:30.200]   with as little as $1.
[00:01:30.200 --> 00:01:31.680]   Since Cash App allows you to send
[00:01:31.680 --> 00:01:34.480]   and receive money digitally, peer-to-peer,
[00:01:34.480 --> 00:01:38.080]   and security in all digital transactions is very important,
[00:01:38.080 --> 00:01:41.360]   let me mention the PCI data security standard
[00:01:41.360 --> 00:01:43.880]   that Cash App is compliant with.
[00:01:43.880 --> 00:01:46.720]   I'm a big fan of standards for safety and security.
[00:01:46.720 --> 00:01:49.480]   PCI DSS is a good example of that,
[00:01:49.480 --> 00:01:52.080]   where a bunch of competitors got together and agreed
[00:01:52.080 --> 00:01:53.800]   that there needs to be a global standard
[00:01:53.800 --> 00:01:56.000]   around the security of transactions.
[00:01:56.000 --> 00:01:58.400]   Now we just need to do the same for autonomous vehicles
[00:01:58.400 --> 00:02:00.560]   and AI systems in general.
[00:02:00.560 --> 00:02:04.240]   So again, if you get Cash App from the App Store or Google Play
[00:02:04.240 --> 00:02:08.280]   and use the code LEXPODCAST, you get $10,
[00:02:08.280 --> 00:02:11.160]   and Cash App will also donate $10 to FIRST,
[00:02:11.160 --> 00:02:14.040]   an organization that is helping to advance robotics
[00:02:14.040 --> 00:02:17.640]   and STEM education for young people around the world.
[00:02:17.640 --> 00:02:21.400]   And now, here's my conversation with Daphne Koller.
[00:02:22.360 --> 00:02:25.000]   So you co-founded Coursera and made a huge impact
[00:02:25.000 --> 00:02:26.640]   in the global education of AI,
[00:02:26.640 --> 00:02:31.440]   and after five years, in August 2016, wrote a blog post
[00:02:31.440 --> 00:02:34.440]   saying that you're stepping away and wrote, quote,
[00:02:34.440 --> 00:02:37.400]   "It is time for me to turn to another critical challenge,
[00:02:37.400 --> 00:02:38.840]   "the development of machine learning
[00:02:38.840 --> 00:02:41.640]   "and its applications to improving human health."
[00:02:41.640 --> 00:02:45.080]   So let me ask two far-out philosophical questions.
[00:02:45.080 --> 00:02:47.960]   One, do you think we'll one day find cures
[00:02:47.960 --> 00:02:50.720]   for all major diseases known today?
[00:02:50.720 --> 00:02:53.520]   And two, do you think we'll one day figure out
[00:02:53.520 --> 00:02:55.960]   a way to extend the human lifespan,
[00:02:55.960 --> 00:02:57.840]   perhaps to the point of immortality?
[00:02:57.840 --> 00:03:01.760]   - So one day is a very long time,
[00:03:01.760 --> 00:03:04.920]   and I don't like to make predictions of the type
[00:03:04.920 --> 00:03:07.280]   we will never be able to do X
[00:03:07.280 --> 00:03:12.280]   because I think that's a, you know, that's smacks of hubris.
[00:03:12.280 --> 00:03:17.440]   Seems that never in the entire eternity of human existence
[00:03:17.440 --> 00:03:19.360]   will we be able to solve a problem.
[00:03:19.360 --> 00:03:24.200]   That being said, curing disease is very hard
[00:03:24.200 --> 00:03:28.480]   because oftentimes by the time you discover the disease,
[00:03:28.480 --> 00:03:30.520]   a lot of damage has already been done,
[00:03:30.520 --> 00:03:34.960]   and so to assume that we would be able to cure disease
[00:03:34.960 --> 00:03:37.560]   at that stage assumes that we would come up with ways
[00:03:37.560 --> 00:03:41.880]   of basically regenerating entire parts of the human body
[00:03:41.880 --> 00:03:45.320]   in the way that actually returns it to its original state,
[00:03:45.320 --> 00:03:47.400]   and that's a very challenging problem.
[00:03:47.400 --> 00:03:49.400]   We have cured very few diseases.
[00:03:49.400 --> 00:03:51.460]   We've been able to provide treatment
[00:03:51.460 --> 00:03:52.920]   for an increasingly large number,
[00:03:52.920 --> 00:03:54.680]   but the number of things that you could actually define
[00:03:54.680 --> 00:03:57.600]   to be cures is actually not that large.
[00:03:57.600 --> 00:04:02.520]   So I think that there's a lot of work
[00:04:02.520 --> 00:04:05.640]   that would need to happen before one could legitimately say
[00:04:05.640 --> 00:04:08.800]   that we have cured even a reasonable number,
[00:04:08.800 --> 00:04:10.440]   far less all diseases.
[00:04:10.440 --> 00:04:12.760]   - On a scale of zero to 100,
[00:04:12.760 --> 00:04:15.560]   where are we in understanding the fundamental mechanisms
[00:04:15.560 --> 00:04:18.160]   of all major diseases?
[00:04:18.160 --> 00:04:19.260]   What's your sense?
[00:04:19.260 --> 00:04:21.080]   So from the computer science perspective
[00:04:21.080 --> 00:04:24.160]   that you've entered the world of health,
[00:04:24.160 --> 00:04:25.720]   how far along are we?
[00:04:25.720 --> 00:04:29.520]   - I think it depends on which disease.
[00:04:29.520 --> 00:04:31.800]   I mean, there are ones where I would say
[00:04:31.800 --> 00:04:33.400]   we're maybe not quite at 100
[00:04:33.400 --> 00:04:35.560]   because biology is really complicated,
[00:04:35.560 --> 00:04:38.960]   and there's always new things that we uncover
[00:04:38.960 --> 00:04:41.200]   that people didn't even realize existed.
[00:04:41.200 --> 00:04:44.420]   But I would say there's diseases
[00:04:44.420 --> 00:04:48.080]   where we might be in the 70s or 80s,
[00:04:48.080 --> 00:04:52.000]   and then there's diseases in which I would say
[00:04:52.000 --> 00:04:55.200]   probably the majority where we're really close to zero.
[00:04:55.200 --> 00:04:57.960]   - Would Alzheimer's and schizophrenia
[00:04:57.960 --> 00:05:02.960]   and type two diabetes fall closer to zero or to the 80?
[00:05:02.960 --> 00:05:09.320]   - I think Alzheimer's is probably closer to zero than to 80.
[00:05:09.320 --> 00:05:12.680]   There are hypotheses,
[00:05:12.680 --> 00:05:17.320]   but I don't think those hypotheses have as of yet
[00:05:17.320 --> 00:05:22.000]   been sufficiently validated that we believe them to be true,
[00:05:22.000 --> 00:05:23.800]   and there's an increasing number of people
[00:05:23.800 --> 00:05:25.920]   who believe that the traditional hypotheses
[00:05:25.920 --> 00:05:28.020]   might not really explain what's going on.
[00:05:28.020 --> 00:05:32.400]   I would also say that Alzheimer's and schizophrenia
[00:05:32.400 --> 00:05:35.320]   and even type two diabetes are not really one disease.
[00:05:35.320 --> 00:05:39.400]   They're almost certainly a heterogeneous collection
[00:05:39.400 --> 00:05:43.720]   of mechanisms that manifest in clinically similar ways.
[00:05:43.720 --> 00:05:46.640]   So in the same way that we now understand
[00:05:46.640 --> 00:05:48.920]   that breast cancer is really not one disease,
[00:05:48.920 --> 00:05:53.440]   it is multitude of cellular mechanisms,
[00:05:53.440 --> 00:05:55.160]   all of which ultimately translate
[00:05:55.160 --> 00:05:59.340]   to uncontrolled proliferation, but it's not one disease,
[00:05:59.340 --> 00:06:01.160]   the same is almost undoubtedly true
[00:06:01.160 --> 00:06:02.880]   for those other diseases as well,
[00:06:02.880 --> 00:06:05.780]   and it's that understanding that needs to precede
[00:06:05.780 --> 00:06:08.480]   any understanding of the specific mechanisms
[00:06:08.480 --> 00:06:10.120]   of any of those other diseases.
[00:06:10.120 --> 00:06:12.520]   Now, in schizophrenia, I would say we're almost certainly
[00:06:12.520 --> 00:06:15.200]   closer to zero than to anything else.
[00:06:15.200 --> 00:06:18.240]   Type two diabetes is a bit of a mix.
[00:06:18.240 --> 00:06:21.400]   There are clear mechanisms that are implicated
[00:06:21.400 --> 00:06:22.980]   that I think have been validated
[00:06:22.980 --> 00:06:25.280]   that have to do with insulin resistance and such,
[00:06:25.280 --> 00:06:28.520]   but there's almost certainly there as well
[00:06:28.520 --> 00:06:31.280]   many mechanisms that we have not yet understood.
[00:06:31.280 --> 00:06:34.400]   - So you've also thought and worked a little bit
[00:06:34.400 --> 00:06:35.880]   on the longevity side.
[00:06:35.880 --> 00:06:38.360]   Do you see the disease and longevity
[00:06:38.360 --> 00:06:42.080]   as overlapping completely, partially,
[00:06:42.080 --> 00:06:45.280]   or not at all as efforts?
[00:06:45.280 --> 00:06:48.620]   - Those mechanisms are certainly overlapping.
[00:06:48.620 --> 00:06:51.940]   There's a well-known phenomenon that says
[00:06:51.940 --> 00:06:56.820]   that for most diseases, other than childhood diseases,
[00:06:56.820 --> 00:07:01.280]   the risk for contracting that disease
[00:07:01.280 --> 00:07:03.840]   increases exponentially year on year every year
[00:07:03.840 --> 00:07:05.720]   from the time you're about 40.
[00:07:05.720 --> 00:07:09.100]   So obviously there is a connection between those two things.
[00:07:09.100 --> 00:07:12.400]   That's not to say that they're identical.
[00:07:12.400 --> 00:07:14.960]   There's clearly aging that happens
[00:07:14.960 --> 00:07:18.740]   that is not really associated with any specific disease.
[00:07:18.740 --> 00:07:22.280]   And there's also diseases and mechanisms of disease
[00:07:22.280 --> 00:07:25.640]   that are not specifically related to aging.
[00:07:25.640 --> 00:07:29.120]   So I think overlap is where we're at.
[00:07:29.120 --> 00:07:30.440]   - Okay.
[00:07:30.440 --> 00:07:32.620]   It is a little unfortunate that we get older,
[00:07:32.620 --> 00:07:34.160]   and it seems that there's some correlation
[00:07:34.160 --> 00:07:39.080]   with the occurrence of diseases
[00:07:39.080 --> 00:07:40.760]   or the fact that we get older.
[00:07:40.760 --> 00:07:43.080]   And both are quite sad.
[00:07:43.080 --> 00:07:46.720]   - I mean, there's processes that happen as cells age
[00:07:46.720 --> 00:07:49.520]   that I think are contributing to disease.
[00:07:49.520 --> 00:07:52.800]   Some of those have to do with DNA damage
[00:07:52.800 --> 00:07:54.960]   that accumulates as cells divide
[00:07:54.960 --> 00:07:59.640]   where the repair mechanisms don't fully correct for those.
[00:07:59.640 --> 00:08:03.680]   There are accumulations of proteins
[00:08:03.680 --> 00:08:06.360]   that are misfolded and potentially aggregate,
[00:08:06.360 --> 00:08:08.560]   and those two contribute to disease
[00:08:08.560 --> 00:08:10.560]   and contribute to inflammation.
[00:08:10.560 --> 00:08:14.040]   There is a multitude of mechanisms that have been uncovered
[00:08:14.040 --> 00:08:17.120]   that are sort of wear and tear at the cellular level
[00:08:17.120 --> 00:08:20.060]   that contribute to disease processes.
[00:08:20.060 --> 00:08:24.920]   And I'm sure there's many that we don't yet understand.
[00:08:24.920 --> 00:08:27.360]   - On a small tangent, perhaps philosophical,
[00:08:27.360 --> 00:08:32.400]   does the fact that things get older
[00:08:32.400 --> 00:08:34.760]   and the fact that things die
[00:08:34.760 --> 00:08:38.920]   is a very powerful feature for the growth of new things.
[00:08:38.920 --> 00:08:41.440]   It's a kind of learning mechanism.
[00:08:41.440 --> 00:08:43.740]   So it's both tragic and beautiful.
[00:08:43.740 --> 00:08:46.920]   So (laughs)
[00:08:46.920 --> 00:08:53.960]   so in trying to fight disease and trying to fight aging,
[00:08:53.960 --> 00:08:59.060]   do you think about sort of the useful fact of our mortality?
[00:08:59.920 --> 00:09:02.680]   Or would you, like if you could be immortal,
[00:09:02.680 --> 00:09:04.280]   would you choose to be immortal?
[00:09:04.280 --> 00:09:10.680]   - Again, I think immortal is a very long time.
[00:09:10.680 --> 00:09:11.520]   (Lex laughs)
[00:09:11.520 --> 00:09:16.040]   And I don't know that that would necessarily be something
[00:09:16.040 --> 00:09:17.960]   that I would want to aspire to,
[00:09:17.960 --> 00:09:22.960]   but I think all of us aspire to an increased health span,
[00:09:22.960 --> 00:09:27.680]   I would say, which is an increased amount of time
[00:09:27.680 --> 00:09:29.920]   where you're healthy and active
[00:09:29.920 --> 00:09:33.160]   and feel as you did when you were 20.
[00:09:33.160 --> 00:09:36.800]   We're nowhere close to that.
[00:09:36.800 --> 00:09:41.800]   People deteriorate physically and mentally over time,
[00:09:41.800 --> 00:09:43.720]   and that is a very sad phenomenon.
[00:09:43.720 --> 00:09:47.360]   So I think a wonderful aspiration would be
[00:09:47.360 --> 00:09:51.600]   if we could all live to the biblical 120,
[00:09:51.600 --> 00:09:53.800]   maybe, in perfect health.
[00:09:53.800 --> 00:09:54.920]   - In high quality of life.
[00:09:54.920 --> 00:09:55.920]   - High quality of life.
[00:09:55.920 --> 00:09:57.840]   I think that would be an amazing goal
[00:09:57.840 --> 00:09:59.360]   for us to achieve as a society.
[00:09:59.360 --> 00:10:03.720]   Now is the right age, 120, or 100, or 150?
[00:10:03.720 --> 00:10:05.800]   I think that's up for debate,
[00:10:05.800 --> 00:10:07.720]   but I think an increased health span
[00:10:07.720 --> 00:10:09.100]   is a really worthy goal.
[00:10:09.100 --> 00:10:14.760]   - And anyway, in the grand time of the age of the universe,
[00:10:14.760 --> 00:10:16.680]   it's all pretty short.
[00:10:16.680 --> 00:10:18.520]   So from the perspective,
[00:10:18.520 --> 00:10:21.060]   you've done, obviously, a lot of incredible work
[00:10:21.060 --> 00:10:22.120]   in machine learning,
[00:10:22.120 --> 00:10:25.220]   so what role do you think data and machine learning
[00:10:25.220 --> 00:10:29.320]   play in this goal of trying to understand diseases
[00:10:29.320 --> 00:10:31.840]   and trying to eradicate diseases?
[00:10:31.840 --> 00:10:35.200]   - Up until now, I don't think it's played
[00:10:35.200 --> 00:10:37.880]   very much of a significant role,
[00:10:37.880 --> 00:10:42.440]   because largely, the data sets that one really needed
[00:10:42.440 --> 00:10:47.320]   to enable a powerful machine learning method,
[00:10:47.320 --> 00:10:49.660]   those data sets haven't really existed.
[00:10:49.660 --> 00:10:50.960]   There's been dribs and drabs
[00:10:50.960 --> 00:10:53.320]   and some interesting machine learning
[00:10:53.320 --> 00:10:54.680]   that has been applied,
[00:10:54.680 --> 00:10:57.640]   I would say machine learning/data science,
[00:10:57.640 --> 00:11:00.160]   but the last few years are starting to change that.
[00:11:00.160 --> 00:11:05.160]   So we now see an increase in some large data sets,
[00:11:05.160 --> 00:11:11.320]   but equally importantly, an increase in technologies
[00:11:11.320 --> 00:11:14.680]   that are able to produce data at scale.
[00:11:14.680 --> 00:11:16.360]   It's not typically the case
[00:11:16.360 --> 00:11:20.160]   that people have deliberately, proactively
[00:11:20.160 --> 00:11:22.440]   used those tools for the purpose
[00:11:22.440 --> 00:11:24.200]   of generating data for machine learning.
[00:11:24.200 --> 00:11:26.560]   They, to the extent that those techniques
[00:11:26.560 --> 00:11:28.560]   have been used for data production,
[00:11:28.560 --> 00:11:29.860]   they've been used for data production
[00:11:29.860 --> 00:11:31.320]   to drive scientific discovery,
[00:11:31.320 --> 00:11:34.440]   and the machine learning came as a sort of byproduct,
[00:11:34.440 --> 00:11:36.920]   second stage of, oh, now we have a data set,
[00:11:36.920 --> 00:11:38.260]   let's do machine learning on that,
[00:11:38.260 --> 00:11:41.800]   rather than a more simplistic data analysis method.
[00:11:41.800 --> 00:11:44.440]   But what we are doing at In-Sitro
[00:11:44.440 --> 00:11:46.840]   is actually flipping that around and saying,
[00:11:46.840 --> 00:11:50.280]   here's this incredible repertoire of methods
[00:11:50.280 --> 00:11:54.560]   that bioengineers, cell biologists have come up with.
[00:11:54.560 --> 00:11:57.400]   Let's see if we can put them together in brand new ways
[00:11:57.400 --> 00:12:00.240]   with the goal of creating data sets
[00:12:00.240 --> 00:12:03.360]   that machine learning can really be applied on productively
[00:12:03.360 --> 00:12:06.580]   to create powerful predictive models
[00:12:06.580 --> 00:12:08.440]   that can help us address fundamental problems
[00:12:08.440 --> 00:12:09.440]   in human health.
[00:12:09.440 --> 00:12:14.440]   - So really focus, to get, make data the primary focus
[00:12:14.440 --> 00:12:15.800]   and the primary goal,
[00:12:15.800 --> 00:12:18.920]   and find, use the mechanisms of biology and chemistry
[00:12:18.920 --> 00:12:23.360]   to create the kinds of data set
[00:12:23.360 --> 00:12:25.720]   that could allow machine learning to benefit the most.
[00:12:25.720 --> 00:12:27.580]   - I wouldn't put it in those terms,
[00:12:27.580 --> 00:12:30.480]   because that says that data is the end goal.
[00:12:30.480 --> 00:12:32.160]   Data is the means.
[00:12:32.160 --> 00:12:35.760]   So for us, the end goal is helping address challenges
[00:12:35.760 --> 00:12:39.200]   in human health, and the method that we've elected
[00:12:39.200 --> 00:12:42.680]   to do that is to apply machine learning
[00:12:42.680 --> 00:12:44.120]   to build predictive models.
[00:12:44.120 --> 00:12:46.000]   And machine learning, in my opinion,
[00:12:46.000 --> 00:12:48.800]   can only be really successfully applied
[00:12:48.800 --> 00:12:50.680]   especially the more powerful models
[00:12:50.680 --> 00:12:53.520]   if you give it data that is of sufficient scale
[00:12:53.520 --> 00:12:54.520]   and sufficient quality.
[00:12:54.520 --> 00:12:58.560]   So how do you create those data sets
[00:12:58.560 --> 00:13:03.560]   so as to drive the ability to generate predictive models
[00:13:03.560 --> 00:13:05.680]   which subsequently help improve human health?
[00:13:05.680 --> 00:13:08.360]   - So before we dive into the details of that,
[00:13:08.360 --> 00:13:10.280]   let me take a step back and ask,
[00:13:10.280 --> 00:13:16.680]   when and where was your interest in human health born?
[00:13:16.760 --> 00:13:19.880]   Are there moments, events, perhaps, if I may ask,
[00:13:19.880 --> 00:13:23.040]   tragedies in your own life that catalyzes passion,
[00:13:23.040 --> 00:13:26.560]   or was it the broader desire to help humankind?
[00:13:26.560 --> 00:13:29.160]   - So I would say it's a bit of both.
[00:13:29.160 --> 00:13:32.600]   So on, I mean, my interest in human health
[00:13:32.600 --> 00:13:36.520]   actually dates back to the early 2000s
[00:13:36.520 --> 00:13:42.760]   when a lot of my peers in machine learning
[00:13:42.760 --> 00:13:45.480]   and I were using data sets
[00:13:45.480 --> 00:13:47.400]   that frankly were not very inspiring.
[00:13:47.400 --> 00:13:49.800]   Some of us old timers still remember
[00:13:49.800 --> 00:13:52.320]   the quote unquote 20 news groups data set
[00:13:52.320 --> 00:13:55.720]   where this was literally a bunch of texts
[00:13:55.720 --> 00:13:57.120]   from 20 news groups,
[00:13:57.120 --> 00:13:59.240]   a concept that doesn't really even exist anymore.
[00:13:59.240 --> 00:14:01.120]   And the question was, can you classify
[00:14:01.120 --> 00:14:06.760]   which news group a particular bag of words came from?
[00:14:06.760 --> 00:14:08.680]   And it wasn't very interesting.
[00:14:08.680 --> 00:14:12.440]   The data sets at the time on the biology side
[00:14:12.440 --> 00:14:14.920]   were much more interesting both from a technical
[00:14:14.920 --> 00:14:17.520]   and also from an aspirational perspective.
[00:14:17.520 --> 00:14:18.800]   They were still pretty small,
[00:14:18.800 --> 00:14:20.720]   but they were better than 20 news groups.
[00:14:20.720 --> 00:14:23.600]   And so I started out, I think,
[00:14:23.600 --> 00:14:27.280]   just by wanting to do something that was more,
[00:14:27.280 --> 00:14:30.760]   I don't know, societally useful and technically interesting.
[00:14:30.760 --> 00:14:34.400]   And then over time became more and more interested
[00:14:34.400 --> 00:14:39.400]   in the biology and the human health aspects for themselves
[00:14:39.400 --> 00:14:43.400]   and began to work even sometimes on papers
[00:14:43.400 --> 00:14:45.080]   that were just in biology
[00:14:45.080 --> 00:14:48.400]   without having a significant machine learning component.
[00:14:48.400 --> 00:14:52.680]   I think my interest in drug discovery
[00:14:52.680 --> 00:14:57.320]   is partly due to an incident I had
[00:14:57.320 --> 00:15:02.520]   when my father sadly passed away about 12 years ago.
[00:15:02.520 --> 00:15:06.960]   He had an autoimmune disease that settled in his lungs
[00:15:06.960 --> 00:15:11.360]   and the doctor's basic said,
[00:15:11.360 --> 00:15:13.280]   well, there's only one thing that we could do,
[00:15:13.280 --> 00:15:14.960]   which is give him prednisone.
[00:15:14.960 --> 00:15:17.640]   At some point, I remember a doctor even came and said,
[00:15:17.640 --> 00:15:19.520]   "Hey, let's do a lung biopsy to figure out
[00:15:19.520 --> 00:15:20.880]   "which autoimmune disease he has."
[00:15:20.880 --> 00:15:23.080]   And I said, "Would that be helpful?
[00:15:23.080 --> 00:15:23.920]   "Would that change treatment?"
[00:15:23.920 --> 00:15:25.360]   He said, "No, there's only prednisone.
[00:15:25.360 --> 00:15:27.120]   "That's the only thing we can give him."
[00:15:27.120 --> 00:15:29.880]   And I had friends who were rheumatologists who said,
[00:15:29.880 --> 00:15:31.920]   "The FDA would never approve prednisone today
[00:15:31.920 --> 00:15:36.920]   "because the ratio of side effects to benefit
[00:15:36.920 --> 00:15:39.560]   "is probably not large enough."
[00:15:39.560 --> 00:15:44.560]   Today, we're in a state where there's probably four or five,
[00:15:44.560 --> 00:15:48.720]   maybe even more, well, it depends for which autoimmune disease
[00:15:48.720 --> 00:15:52.880]   but there are multiple drugs that can help people
[00:15:52.880 --> 00:15:53.920]   with autoimmune disease,
[00:15:53.920 --> 00:15:56.720]   many of which didn't exist 12 years ago.
[00:15:56.720 --> 00:16:00.340]   And I think we're at a golden time in some ways
[00:16:00.340 --> 00:16:05.340]   in drug discovery where there's the ability to create drugs
[00:16:06.800 --> 00:16:10.600]   that are much more safe and much more effective
[00:16:10.600 --> 00:16:13.080]   than we've ever been able to before.
[00:16:13.080 --> 00:16:16.360]   And what's lacking is enough understanding
[00:16:16.360 --> 00:16:21.360]   of biology and mechanism to know where to aim that engine.
[00:16:21.360 --> 00:16:25.400]   And I think that's where machine learning can help.
[00:16:25.400 --> 00:16:29.920]   - So in 2018, you started and now lead a company in Citro,
[00:16:29.920 --> 00:16:31.760]   which is, like you mentioned,
[00:16:31.760 --> 00:16:34.760]   perhaps the focus is drug discovery
[00:16:34.760 --> 00:16:38.140]   and the utilization of machine learning for drug discovery.
[00:16:38.140 --> 00:16:40.540]   So you mentioned that, quote,
[00:16:40.540 --> 00:16:43.600]   "We're really interested in creating what you might call
[00:16:43.600 --> 00:16:47.320]   "a disease in a dish model, disease in a dish models,
[00:16:47.320 --> 00:16:49.080]   "places where diseases are complex,
[00:16:49.080 --> 00:16:52.140]   "where we really haven't had a good model system,
[00:16:52.140 --> 00:16:54.960]   "where typical animal models that have been used for years,
[00:16:54.960 --> 00:16:58.860]   "including testing on mice, just aren't very effective."
[00:16:58.860 --> 00:17:02.640]   So can you try to describe what is an animal model
[00:17:02.640 --> 00:17:05.360]   and what is a disease in a dish model?
[00:17:05.360 --> 00:17:06.280]   - Sure.
[00:17:06.280 --> 00:17:11.280]   So an animal model for disease is where you create
[00:17:11.280 --> 00:17:14.880]   effectively, it's what it sounds like.
[00:17:14.880 --> 00:17:19.280]   It's oftentimes a mouse where we have introduced
[00:17:19.280 --> 00:17:22.800]   some external perturbation that creates the disease,
[00:17:22.800 --> 00:17:26.280]   and then we cure that disease.
[00:17:26.280 --> 00:17:28.720]   And the hope is that by doing that,
[00:17:28.720 --> 00:17:31.320]   we will cure a similar disease in the human.
[00:17:31.320 --> 00:17:35.040]   The problem is that oftentimes the way in which
[00:17:35.040 --> 00:17:36.880]   we generate the disease in the animal
[00:17:36.880 --> 00:17:38.520]   has nothing to do with how that disease
[00:17:38.520 --> 00:17:40.880]   actually comes about in a human.
[00:17:40.880 --> 00:17:44.360]   It's what you might think of as a copy of the phenotype,
[00:17:44.360 --> 00:17:46.720]   a copy of the clinical outcome,
[00:17:46.720 --> 00:17:48.720]   but the mechanisms are quite different.
[00:17:48.720 --> 00:17:52.080]   And so curing the disease in the animal,
[00:17:52.080 --> 00:17:54.840]   which in most cases doesn't happen naturally,
[00:17:54.840 --> 00:17:57.160]   mice don't get Alzheimer's, they don't get diabetes,
[00:17:57.160 --> 00:17:58.680]   they don't get atherosclerosis,
[00:17:58.680 --> 00:18:01.240]   they don't get autism or schizophrenia.
[00:18:01.240 --> 00:18:05.680]   Those cures don't translate over
[00:18:05.680 --> 00:18:08.120]   to what happens in the human.
[00:18:08.120 --> 00:18:10.840]   And that's where most drugs fails,
[00:18:10.840 --> 00:18:13.680]   just because the findings that we had in the mouse
[00:18:13.680 --> 00:18:15.040]   don't translate to a human.
[00:18:15.040 --> 00:18:20.840]   The disease in the dish models is a fairly new approach.
[00:18:20.840 --> 00:18:25.840]   It's been enabled by technologies that have not existed
[00:18:25.840 --> 00:18:28.360]   for more than five to 10 years.
[00:18:28.360 --> 00:18:32.760]   So for instance, the ability for us to take a cell
[00:18:32.760 --> 00:18:35.560]   from any one of us, you or me,
[00:18:35.560 --> 00:18:39.960]   revert that say skin cell to what's called stem cell status,
[00:18:39.960 --> 00:18:44.760]   which is what's called a pluripotent cell
[00:18:44.760 --> 00:18:46.600]   that can then be differentiated
[00:18:46.600 --> 00:18:47.840]   into different types of cells.
[00:18:47.840 --> 00:18:49.800]   So from that pluripotent cell,
[00:18:49.800 --> 00:18:54.280]   one can create a Lex neuron or a Lex cardiomyocyte
[00:18:54.280 --> 00:18:57.760]   or a Lex hepatocyte that has your genetics,
[00:18:57.760 --> 00:19:00.320]   but that right cell type.
[00:19:00.320 --> 00:19:04.800]   And so if there's a genetic burden of disease
[00:19:04.800 --> 00:19:07.160]   that would manifest in that particular cell type,
[00:19:07.160 --> 00:19:10.320]   you might be able to see it by looking at those cells
[00:19:10.320 --> 00:19:13.860]   and saying, oh, that's what potentially sick cells look like
[00:19:13.860 --> 00:19:18.520]   versus healthy cells and understand how,
[00:19:18.520 --> 00:19:20.760]   and then explore what kind of interventions
[00:19:20.760 --> 00:19:24.860]   might revert the unhealthy looking cell to a healthy cell.
[00:19:24.860 --> 00:19:27.720]   Now, of course, curing cells is not the same
[00:19:27.720 --> 00:19:28.860]   as curing people.
[00:19:28.860 --> 00:19:33.220]   And so there's still potentially a translatability gap,
[00:19:33.220 --> 00:19:38.220]   but at least for diseases that are driven, say,
[00:19:38.220 --> 00:19:41.980]   by human genetics and where the human genetics
[00:19:41.980 --> 00:19:43.780]   is what drives the cellular phenotype,
[00:19:43.780 --> 00:19:47.960]   there is some reason to hope that if we revert those cells
[00:19:47.960 --> 00:19:49.600]   in which the disease begins
[00:19:49.600 --> 00:19:52.220]   and where the disease is driven by genetics
[00:19:52.220 --> 00:19:55.260]   and we can revert that cell back to a healthy state,
[00:19:55.260 --> 00:19:58.140]   maybe that will help also revert
[00:19:58.140 --> 00:20:00.900]   the more global clinical phenotypes.
[00:20:00.900 --> 00:20:02.760]   So that's really what we're hoping to do.
[00:20:02.760 --> 00:20:06.020]   - That step, that backward step, I was reading about it,
[00:20:06.020 --> 00:20:08.300]   the Yamanaka factor.
[00:20:08.300 --> 00:20:09.140]   - Yes.
[00:20:09.140 --> 00:20:12.300]   - So it's like that reverse step back to stem cells.
[00:20:12.300 --> 00:20:13.140]   - Yes.
[00:20:13.140 --> 00:20:14.180]   - Seems like magic.
[00:20:14.180 --> 00:20:15.020]   - It is.
[00:20:15.020 --> 00:20:17.660]   Honestly, before that happened,
[00:20:17.660 --> 00:20:20.100]   I think very few people would have predicted
[00:20:20.100 --> 00:20:21.700]   that to be possible.
[00:20:21.700 --> 00:20:22.540]   It's amazing.
[00:20:22.540 --> 00:20:25.340]   - Can you maybe elaborate, is it actually possible?
[00:20:25.340 --> 00:20:30.580]   So this result was maybe, I don't know how many years ago,
[00:20:30.580 --> 00:20:32.700]   maybe 10 years ago was first demonstrated,
[00:20:32.700 --> 00:20:34.300]   something like that.
[00:20:34.300 --> 00:20:35.780]   How hard is this?
[00:20:35.780 --> 00:20:37.500]   How noisy is this backward step?
[00:20:37.500 --> 00:20:39.460]   It seems quite incredible and cool.
[00:20:39.460 --> 00:20:42.220]   - It is incredible and cool.
[00:20:42.220 --> 00:20:46.420]   It was much more, I think, finicky and bespoke
[00:20:46.420 --> 00:20:49.980]   at the early stages when the discovery was first made,
[00:20:49.980 --> 00:20:54.500]   but at this point it's become almost industrialized.
[00:20:54.500 --> 00:20:59.460]   There are what's called contract research organizations,
[00:20:59.460 --> 00:21:02.300]   vendors that will take a sample from a human
[00:21:02.300 --> 00:21:04.460]   and revert it back to stem cell status,
[00:21:04.460 --> 00:21:07.100]   and it works a very good fraction of the time.
[00:21:07.100 --> 00:21:12.020]   Now there are people who will ask, I think, good questions.
[00:21:12.020 --> 00:21:15.340]   Is this really, truly a stem cell or does it remember
[00:21:15.340 --> 00:21:19.420]   certain aspects of changes that were made
[00:21:19.420 --> 00:21:22.500]   in the human beyond the genetics?
[00:21:22.500 --> 00:21:24.660]   - It's passed as a skin cell, yeah.
[00:21:24.660 --> 00:21:26.740]   - It's passed as a skin cell or it's passed
[00:21:26.740 --> 00:21:29.900]   in terms of exposures to different environmental factors
[00:21:29.900 --> 00:21:30.940]   and so on.
[00:21:30.940 --> 00:21:33.300]   So I think the consensus right now
[00:21:33.300 --> 00:21:36.420]   is that these are not always perfect
[00:21:36.420 --> 00:21:40.020]   and there is little bits and pieces of memory sometimes,
[00:21:40.020 --> 00:21:43.580]   but by and large, these are actually pretty good.
[00:21:43.580 --> 00:21:48.740]   - So one of the key things, well, maybe you can correct me,
[00:21:48.740 --> 00:21:50.900]   but one of the useful things for machine learning
[00:21:50.900 --> 00:21:54.180]   is size, scale of data.
[00:21:54.180 --> 00:21:59.100]   How easy it is to do these kinds of reversals to stem cells
[00:21:59.100 --> 00:22:02.340]   and then does using a dish models at scale?
[00:22:02.340 --> 00:22:05.300]   Is that a huge challenge or not?
[00:22:05.300 --> 00:22:11.180]   - So the reversal is not, as of this point,
[00:22:11.180 --> 00:22:16.340]   something that can be done at the scale of tens of thousands
[00:22:16.340 --> 00:22:18.500]   or hundreds of thousands.
[00:22:18.500 --> 00:22:22.220]   I think total number of stem cells or IPS cells
[00:22:22.220 --> 00:22:25.220]   that are what's called induced pluripotent stem cells
[00:22:25.220 --> 00:22:30.180]   in the world, I think is somewhere between five and 10,000
[00:22:30.180 --> 00:22:31.420]   last I looked.
[00:22:31.420 --> 00:22:34.420]   Now, again, that might not count things that exist
[00:22:34.420 --> 00:22:36.220]   in this or that academic center
[00:22:36.220 --> 00:22:37.820]   and they may add up to a bit more,
[00:22:37.820 --> 00:22:40.020]   but that's about the range.
[00:22:40.020 --> 00:22:42.140]   So it's not something that you could this point
[00:22:42.140 --> 00:22:45.540]   generate IPS cells from a million people,
[00:22:45.540 --> 00:22:47.900]   but maybe you don't need to
[00:22:47.900 --> 00:22:51.820]   because maybe that background is enough
[00:22:51.820 --> 00:22:56.140]   because it can also be now perturbed in different ways.
[00:22:56.140 --> 00:23:00.100]   And some people have done really interesting experiments
[00:23:00.100 --> 00:23:05.100]   in, for instance, taking cells from a healthy human
[00:23:05.100 --> 00:23:08.540]   and then introducing a mutation into it
[00:23:08.540 --> 00:23:11.860]   using one of the other miracle technologies
[00:23:11.860 --> 00:23:13.860]   that's emerged in the last decade,
[00:23:13.860 --> 00:23:16.140]   which is CRISPR gene editing
[00:23:16.140 --> 00:23:19.660]   and introduced a mutation that is known to be pathogenic.
[00:23:19.660 --> 00:23:22.420]   And so you can now look at the healthy cells
[00:23:22.420 --> 00:23:24.700]   and unhealthy cells, the one with the mutation
[00:23:24.700 --> 00:23:26.060]   and do a one-on-one comparison
[00:23:26.060 --> 00:23:28.380]   where everything else is held constant.
[00:23:28.380 --> 00:23:31.820]   And so you could really start to understand specifically
[00:23:31.820 --> 00:23:34.380]   what the mutation does at the cellular level.
[00:23:34.380 --> 00:23:37.660]   So the IPS cells are a great starting point
[00:23:37.660 --> 00:23:39.780]   and obviously more diversity is better
[00:23:39.780 --> 00:23:42.380]   'cause you also wanna capture ethnic background
[00:23:42.380 --> 00:23:43.540]   and how that affects things,
[00:23:43.580 --> 00:23:46.780]   but maybe you don't need one from every single patient
[00:23:46.780 --> 00:23:48.140]   with every single type of disease
[00:23:48.140 --> 00:23:50.300]   because we have other tools at our disposal.
[00:23:50.300 --> 00:23:52.580]   - Well, how much difference is there between people?
[00:23:52.580 --> 00:23:53.860]   I mentioned ethnic background.
[00:23:53.860 --> 00:23:54.940]   In terms of IPS cells,
[00:23:54.940 --> 00:23:59.380]   so we're all, like it seems like these magical cells
[00:23:59.380 --> 00:24:01.860]   that can do anything, create anything
[00:24:01.860 --> 00:24:04.020]   between different populations, different people.
[00:24:04.020 --> 00:24:07.020]   Is there a lot of variability between cell cells?
[00:24:07.020 --> 00:24:09.580]   - Well, first of all, there's the variability
[00:24:09.580 --> 00:24:10.980]   that's driven simply by the fact
[00:24:10.980 --> 00:24:13.420]   that genetically we're different.
[00:24:13.420 --> 00:24:15.820]   So a stem cell that's derived from my genotype
[00:24:15.820 --> 00:24:18.340]   is gonna be different from a stem cell
[00:24:18.340 --> 00:24:20.540]   that's derived from your genotype.
[00:24:20.540 --> 00:24:21.780]   There's also some differences
[00:24:21.780 --> 00:24:25.300]   that have more to do with, for whatever reason,
[00:24:25.300 --> 00:24:28.540]   some people's stem cells differentiate better
[00:24:28.540 --> 00:24:29.860]   than other people's stem cells.
[00:24:29.860 --> 00:24:31.540]   We don't entirely understand why,
[00:24:31.540 --> 00:24:34.180]   so there's certainly some differences there as well.
[00:24:34.180 --> 00:24:35.460]   But the fundamental difference
[00:24:35.460 --> 00:24:38.740]   and the one that we really care about and is a positive
[00:24:38.740 --> 00:24:43.220]   is the fact that the genetics are different
[00:24:43.220 --> 00:24:45.980]   and therefore recapitulate my disease burden
[00:24:45.980 --> 00:24:47.780]   versus your disease burden.
[00:24:47.780 --> 00:24:49.260]   - What's a disease burden?
[00:24:49.260 --> 00:24:52.300]   - Well, a disease burden is just, if you think,
[00:24:52.300 --> 00:24:55.060]   I mean, it's not a well-defined mathematical term,
[00:24:55.060 --> 00:24:58.260]   although there are mathematical formulations of it.
[00:24:58.260 --> 00:25:00.540]   If you think about the fact that some of us
[00:25:00.540 --> 00:25:03.460]   are more likely to get a certain disease than others
[00:25:03.460 --> 00:25:07.300]   because we have more variations in our genome
[00:25:07.300 --> 00:25:09.500]   that are causative of the disease,
[00:25:09.500 --> 00:25:12.600]   maybe fewer that are protective of the disease.
[00:25:12.600 --> 00:25:14.860]   People have quantified that
[00:25:14.860 --> 00:25:17.860]   using what are called polygenic risk scores,
[00:25:17.860 --> 00:25:20.820]   which look at all of the variations
[00:25:20.820 --> 00:25:23.620]   in an individual person's genome
[00:25:23.620 --> 00:25:26.620]   and add them all up in terms of how much risk they confer
[00:25:26.620 --> 00:25:27.820]   for a particular disease,
[00:25:27.820 --> 00:25:30.540]   and then they've put people on a spectrum
[00:25:30.540 --> 00:25:32.540]   of their disease risk.
[00:25:32.540 --> 00:25:34.560]   And for certain diseases
[00:25:34.560 --> 00:25:36.940]   where we've been sufficiently powered
[00:25:36.940 --> 00:25:38.740]   to really understand the connection
[00:25:38.740 --> 00:25:41.580]   between the many, many small variations
[00:25:41.580 --> 00:25:44.940]   that give rise to an increased disease risk,
[00:25:44.940 --> 00:25:47.040]   there is some pretty significant differences
[00:25:47.040 --> 00:25:49.300]   in terms of the risk between the people,
[00:25:49.300 --> 00:25:52.060]   say, at the highest decile of this polygenic risk score
[00:25:52.060 --> 00:25:53.500]   and the people at the lowest decile.
[00:25:53.500 --> 00:25:58.500]   Sometimes those differences are a factor of 10 or 12 higher,
[00:25:58.500 --> 00:26:03.940]   so there's definitely a lot that our genetics
[00:26:03.940 --> 00:26:06.140]   contributes to disease risk,
[00:26:06.140 --> 00:26:09.100]   even if it's not by any stretch the full explanation.
[00:26:09.100 --> 00:26:10.500]   - And from a machine learning perspective,
[00:26:10.500 --> 00:26:12.020]   there's signal there.
[00:26:12.020 --> 00:26:14.780]   - There is definitely signal in the genetics,
[00:26:14.780 --> 00:26:19.060]   and there's even more signal, we believe,
[00:26:19.060 --> 00:26:21.540]   in looking at the cells that are derived
[00:26:21.540 --> 00:26:23.460]   from those different genetics,
[00:26:23.460 --> 00:26:26.580]   because in principle, you could say all the signal
[00:26:26.580 --> 00:26:28.680]   is there at the genetics level,
[00:26:28.680 --> 00:26:30.180]   so we don't need to look at the cells,
[00:26:30.180 --> 00:26:31.980]   but our understanding of the biology
[00:26:31.980 --> 00:26:34.660]   is so limited at this point,
[00:26:34.660 --> 00:26:37.960]   then seeing what actually happens at the cellular level
[00:26:37.960 --> 00:26:41.800]   is a heck of a lot closer to the human clinical outcome
[00:26:41.800 --> 00:26:44.620]   than looking at the genetics directly,
[00:26:44.620 --> 00:26:47.180]   and so we can learn a lot more from it
[00:26:47.180 --> 00:26:49.460]   than we could by looking at genetics alone.
[00:26:49.460 --> 00:26:51.660]   - So just to get a sense, I don't know if it's easy to do,
[00:26:51.660 --> 00:26:54.260]   but what kind of data is useful
[00:26:54.260 --> 00:26:56.220]   in this disease-in-a-dish model?
[00:26:56.220 --> 00:26:59.980]   Like, what's the source of raw data information?
[00:26:59.980 --> 00:27:03.880]   And also, from my outsider's perspective,
[00:27:03.880 --> 00:27:08.880]   biology and cells are squishy things.
[00:27:08.880 --> 00:27:10.020]   - They are.
[00:27:10.020 --> 00:27:10.860]   - How do you connect-- - They're literally
[00:27:10.860 --> 00:27:11.900]   squishy things.
[00:27:11.900 --> 00:27:14.560]   - How do you connect the computer to that?
[00:27:14.560 --> 00:27:17.780]   Which sensory mechanisms, I guess?
[00:27:17.780 --> 00:27:20.660]   - So that's another one of those revolutions
[00:27:20.660 --> 00:27:22.540]   that have happened in the last 10 years,
[00:27:22.540 --> 00:27:27.540]   in that our ability to measure cells very quantitatively
[00:27:27.540 --> 00:27:30.360]   has also dramatically increased.
[00:27:30.360 --> 00:27:32.860]   So back when I started doing biology,
[00:27:32.860 --> 00:27:36.940]   in the late '90s, early 2000s,
[00:27:36.940 --> 00:27:42.820]   that was the initial era where we started to measure biology
[00:27:42.820 --> 00:27:46.740]   in really quantitative ways, using things like microarrays,
[00:27:46.740 --> 00:27:50.900]   where you would measure, in a single experiment,
[00:27:50.900 --> 00:27:54.180]   the activity level, what's called expression level,
[00:27:54.180 --> 00:27:57.340]   of every gene in the genome in that sample.
[00:27:57.340 --> 00:28:00.660]   And that ability is what actually allowed us
[00:28:00.660 --> 00:28:04.500]   to even understand that there are molecular subtypes
[00:28:04.500 --> 00:28:07.140]   of diseases like cancer, where up until that point,
[00:28:07.140 --> 00:28:09.540]   it's like, oh, you have breast cancer.
[00:28:09.540 --> 00:28:13.480]   But then, when we looked at the molecular data,
[00:28:13.480 --> 00:28:15.240]   it was clear that there's different subtypes
[00:28:15.240 --> 00:28:17.780]   of breast cancer that, at the level of gene activity,
[00:28:17.780 --> 00:28:19.780]   look completely different to each other.
[00:28:19.780 --> 00:28:23.380]   So that was the beginning of this process.
[00:28:23.380 --> 00:28:27.460]   Now we have the ability to measure individual cells
[00:28:27.460 --> 00:28:28.860]   in terms of their gene activity,
[00:28:28.860 --> 00:28:31.340]   using what's called single-cell RNA sequencing,
[00:28:31.340 --> 00:28:35.020]   which basically sequences the RNA,
[00:28:35.020 --> 00:28:37.980]   which is that activity level of different genes
[00:28:37.980 --> 00:28:40.980]   for every gene in the genome.
[00:28:40.980 --> 00:28:42.740]   And you could do that at single-cell level.
[00:28:42.740 --> 00:28:45.400]   So that's an incredibly powerful way of measuring cells.
[00:28:45.400 --> 00:28:47.860]   I mean, you literally count the number of transcripts.
[00:28:47.860 --> 00:28:50.060]   So it really turns that squishy thing
[00:28:50.060 --> 00:28:51.820]   into something that's digital.
[00:28:51.820 --> 00:28:54.340]   Another tremendous data source
[00:28:54.340 --> 00:28:57.500]   that's emerged in the last few years is microscopy,
[00:28:57.500 --> 00:29:00.580]   and specifically even super-resolution microscopy,
[00:29:00.580 --> 00:29:03.460]   where you could use digital reconstruction
[00:29:03.460 --> 00:29:06.460]   to look at subcellular structures,
[00:29:06.460 --> 00:29:08.380]   sometimes even things that are below
[00:29:08.380 --> 00:29:10.540]   the diffraction limit of light
[00:29:10.540 --> 00:29:13.380]   by doing a sophisticated reconstruction.
[00:29:13.380 --> 00:29:16.500]   And again, that gives you tremendous amounts of information
[00:29:16.500 --> 00:29:18.420]   at the subcellular level.
[00:29:18.420 --> 00:29:20.660]   There's now more and more ways
[00:29:20.660 --> 00:29:24.500]   that amazing scientists out there are developing
[00:29:24.500 --> 00:29:29.500]   for getting new types of information from even single cells.
[00:29:29.500 --> 00:29:35.500]   And so that is a way of turning those squishy things
[00:29:35.500 --> 00:29:37.300]   into digital data.
[00:29:37.300 --> 00:29:38.660]   - Into beautiful data sets.
[00:29:38.660 --> 00:29:42.580]   But so that data set then with machine learning tools
[00:29:42.580 --> 00:29:45.860]   allows you to maybe understand the developmental,
[00:29:45.860 --> 00:29:49.940]   like the mechanism of a particular disease.
[00:29:49.940 --> 00:29:53.700]   And if it's possible to sort of at a high level,
[00:29:53.700 --> 00:29:58.700]   describe how does that help lead to drug discovery
[00:29:58.700 --> 00:30:05.380]   that can help prevent, reverse that mechanism?
[00:30:05.380 --> 00:30:06.780]   - So I think there's different ways
[00:30:06.780 --> 00:30:10.420]   in which this data could potentially be used.
[00:30:10.420 --> 00:30:13.860]   Some people use it for scientific discovery
[00:30:13.860 --> 00:30:17.060]   and say, "Oh, look, we see this phenotype
[00:30:17.060 --> 00:30:21.460]   "at the cellular level, so let's try
[00:30:21.460 --> 00:30:22.940]   "and work our way backwards
[00:30:22.940 --> 00:30:25.180]   "and think which genes might be involved
[00:30:25.180 --> 00:30:27.140]   "in pathways that give rise to that."
[00:30:27.140 --> 00:30:32.140]   So that's a very sort of analytical method
[00:30:32.140 --> 00:30:35.220]   to sort of work our way backwards
[00:30:35.220 --> 00:30:37.700]   using our understanding of known biology.
[00:30:37.700 --> 00:30:42.980]   Some people use it in a somewhat more sort of forward,
[00:30:42.980 --> 00:30:47.580]   if that was backward, this would be forward,
[00:30:47.580 --> 00:30:51.100]   which is to say, "Okay, if I can perturb this gene,
[00:30:51.100 --> 00:30:54.060]   "does it show a phenotype that is similar
[00:30:54.060 --> 00:30:56.100]   "to what I see in disease patients?"
[00:30:56.100 --> 00:30:59.060]   And so maybe that gene is actually causal of the disease,
[00:30:59.060 --> 00:31:00.260]   so that's a different way.
[00:31:00.260 --> 00:31:01.660]   And then there's what we do,
[00:31:01.660 --> 00:31:06.300]   which is basically to take that very large collection
[00:31:06.300 --> 00:31:08.340]   of data and use machine learning
[00:31:08.340 --> 00:31:12.420]   to uncover the patterns that emerge from it.
[00:31:12.420 --> 00:31:14.980]   So for instance, what are those subtypes
[00:31:14.980 --> 00:31:18.700]   that might be similar at the human clinical outcome,
[00:31:18.700 --> 00:31:21.780]   but quite distinct when you look at the molecular data?
[00:31:21.780 --> 00:31:25.180]   And then if we can identify such a subtype,
[00:31:25.180 --> 00:31:28.020]   are there interventions that if I apply it
[00:31:28.020 --> 00:31:32.100]   to cells that come from this subtype of the disease,
[00:31:32.100 --> 00:31:34.940]   and you apply that intervention, it could be a drug
[00:31:34.940 --> 00:31:38.980]   or it could be a CRISPR gene intervention,
[00:31:38.980 --> 00:31:41.380]   does it revert the disease state
[00:31:41.380 --> 00:31:42.700]   to something that looks more like
[00:31:42.700 --> 00:31:44.140]   normal, happy, healthy cells?
[00:31:44.140 --> 00:31:46.940]   And so hopefully if you see that,
[00:31:46.940 --> 00:31:51.940]   that gives you a certain hope that that intervention
[00:31:51.940 --> 00:31:55.140]   will also have a meaningful clinical benefit to people.
[00:31:55.140 --> 00:31:56.620]   And there's obviously a bunch of things
[00:31:56.620 --> 00:31:58.740]   that you would wanna do after that to validate that,
[00:31:58.740 --> 00:32:03.740]   but it's a very different and much less hypothesis-driven way
[00:32:03.740 --> 00:32:06.140]   of uncovering new potential interventions
[00:32:06.140 --> 00:32:10.100]   and might give rise to things that are not the same things
[00:32:10.100 --> 00:32:12.500]   that everyone else is already looking at.
[00:32:12.500 --> 00:32:15.940]   - That's, I don't know, I'm just like,
[00:32:15.940 --> 00:32:17.380]   to psychoanalyze my own feeling
[00:32:17.380 --> 00:32:18.700]   about our discussion currently,
[00:32:18.700 --> 00:32:22.260]   it's so exciting to talk about sort of a fundamentally,
[00:32:22.260 --> 00:32:23.780]   well, something that's been turned
[00:32:23.780 --> 00:32:25.900]   into a machine learning problem
[00:32:25.900 --> 00:32:29.140]   and that can have so much real-world impact.
[00:32:29.140 --> 00:32:30.340]   - That's how I feel too.
[00:32:30.340 --> 00:32:32.220]   - That's kind of exciting 'cause I'm so,
[00:32:32.220 --> 00:32:35.740]   most of my day is spent with data sets
[00:32:35.740 --> 00:32:37.900]   that I guess closer to the news groups.
[00:32:37.900 --> 00:32:41.980]   So this is a kind of, it just feels good to talk about.
[00:32:41.980 --> 00:32:43.500]   In fact, I almost don't wanna talk to you
[00:32:43.500 --> 00:32:45.340]   about machine learning.
[00:32:45.340 --> 00:32:47.460]   I wanna talk about the fundamentals of the data set,
[00:32:47.460 --> 00:32:50.420]   which is an exciting place to be.
[00:32:50.420 --> 00:32:51.740]   - I agree with you.
[00:32:51.740 --> 00:32:53.740]   It's what gets me up in the morning.
[00:32:53.740 --> 00:32:57.140]   It's also what attracts a lot of the people
[00:32:57.140 --> 00:32:59.140]   who work at In-sitro to In-sitro
[00:32:59.140 --> 00:33:01.660]   because I think all of the,
[00:33:01.660 --> 00:33:04.580]   certainly all of our machine learning people are outstanding
[00:33:04.580 --> 00:33:08.860]   and could go get a job selling ads online
[00:33:08.860 --> 00:33:12.820]   or doing e-commerce or even self-driving cars.
[00:33:12.820 --> 00:33:16.660]   But I think they would want,
[00:33:16.660 --> 00:33:19.980]   they come to us because they want to work on something
[00:33:19.980 --> 00:33:22.340]   that has more of an aspirational nature
[00:33:22.340 --> 00:33:24.660]   and can really benefit humanity.
[00:33:24.660 --> 00:33:27.540]   - What would these approaches,
[00:33:27.540 --> 00:33:31.100]   what do you hope, what kind of diseases can be helped?
[00:33:31.100 --> 00:33:33.900]   We mentioned Alzheimer's, schizophrenia, type 2 diabetes.
[00:33:33.900 --> 00:33:36.500]   Can you just describe the various kinds of diseases
[00:33:36.500 --> 00:33:38.540]   that this approach can help?
[00:33:38.540 --> 00:33:39.580]   - Well, we don't know.
[00:33:39.620 --> 00:33:42.500]   And I try and be very cautious
[00:33:42.500 --> 00:33:44.780]   about making promises about some things.
[00:33:44.780 --> 00:33:46.620]   Like, oh, we will cure X.
[00:33:46.620 --> 00:33:48.060]   People make that promise.
[00:33:48.060 --> 00:33:52.700]   And I think it's, I try to first deliver and then promise
[00:33:52.700 --> 00:33:54.460]   as opposed to the other way around.
[00:33:54.460 --> 00:33:57.300]   There are characteristics of a disease
[00:33:57.300 --> 00:34:00.580]   that make it more likely that this type of approach
[00:34:00.580 --> 00:34:02.700]   can potentially be helpful.
[00:34:02.700 --> 00:34:04.580]   So for instance, diseases that have
[00:34:04.580 --> 00:34:07.980]   a very strong genetic basis
[00:34:07.980 --> 00:34:10.940]   are ones that are more likely to manifest
[00:34:10.940 --> 00:34:12.780]   in a stem cell derived model.
[00:34:12.780 --> 00:34:16.300]   We would want the cellular models
[00:34:16.300 --> 00:34:19.980]   to be relatively reproducible and robust
[00:34:19.980 --> 00:34:24.980]   so that you could actually get enough of those cells
[00:34:24.980 --> 00:34:29.580]   in a way that isn't very highly variable and noisy.
[00:34:29.580 --> 00:34:34.140]   You would want the disease to be relatively contained
[00:34:34.140 --> 00:34:36.700]   in one or a small number of cell types
[00:34:36.700 --> 00:34:40.980]   that you could actually create in vitro in a dish setting.
[00:34:40.980 --> 00:34:43.460]   Whereas if it's something that's really broad and systemic
[00:34:43.460 --> 00:34:45.540]   and involves multiple cells
[00:34:45.540 --> 00:34:48.460]   that are in very distal parts of your body,
[00:34:48.460 --> 00:34:50.980]   putting that all in the dish is really challenging.
[00:34:50.980 --> 00:34:53.740]   So we want to focus on the ones
[00:34:53.740 --> 00:34:56.980]   that are most likely to be successful today
[00:34:56.980 --> 00:34:58.660]   with the hope, I think,
[00:34:58.660 --> 00:35:03.340]   that really smart bioengineers out there
[00:35:03.340 --> 00:35:06.220]   are developing better and better systems all the time
[00:35:06.220 --> 00:35:09.380]   so that diseases that might not be tractable today
[00:35:09.380 --> 00:35:11.780]   might be tractable in three years.
[00:35:11.780 --> 00:35:14.900]   So for instance, five years ago,
[00:35:14.900 --> 00:35:16.700]   these stem cell derived models didn't really exist.
[00:35:16.700 --> 00:35:19.100]   People were doing most of the work in cancer cells,
[00:35:19.100 --> 00:35:22.180]   and cancer cells are very, very poor models
[00:35:22.180 --> 00:35:24.820]   of most human biology because they're,
[00:35:24.820 --> 00:35:26.340]   A, they were cancer to begin with,
[00:35:26.340 --> 00:35:30.700]   and B, as you passage them and they proliferate in a dish,
[00:35:30.700 --> 00:35:33.140]   they become, because of the genomic instability,
[00:35:33.140 --> 00:35:35.400]   even less similar to human biology.
[00:35:36.200 --> 00:35:38.540]   Now we have these stem cell derived models.
[00:35:38.540 --> 00:35:43.120]   We have the capability to reasonably robustly,
[00:35:43.120 --> 00:35:46.320]   not quite at the right scale yet, but close,
[00:35:46.320 --> 00:35:48.440]   to derive what's called organoids,
[00:35:48.440 --> 00:35:53.160]   which are these teeny little sort of multicellular
[00:35:53.160 --> 00:35:57.160]   organ sort of models of an organ system.
[00:35:57.160 --> 00:35:59.760]   So there's cerebral organoids and liver organoids
[00:35:59.760 --> 00:36:01.720]   and kidney organoids and gut organoids.
[00:36:01.720 --> 00:36:04.040]   - Yeah, brain organoids is possibly
[00:36:04.040 --> 00:36:05.480]   the coolest thing I've ever seen.
[00:36:05.480 --> 00:36:07.520]   - Is that not like the coolest thing?
[00:36:07.520 --> 00:36:08.360]   - Yeah.
[00:36:08.360 --> 00:36:09.960]   - And then I think on the horizon,
[00:36:09.960 --> 00:36:11.800]   we're starting to see things like connecting
[00:36:11.800 --> 00:36:14.880]   these organoids to each other so that you could actually
[00:36:14.880 --> 00:36:16.600]   start, and there's some really cool papers
[00:36:16.600 --> 00:36:18.840]   that start to do that, where you can actually start
[00:36:18.840 --> 00:36:22.240]   to say, okay, can we do multi-organ system stuff?
[00:36:22.240 --> 00:36:23.560]   There's many challenges to that.
[00:36:23.560 --> 00:36:27.840]   It's not easy by any stretch, but it might,
[00:36:27.840 --> 00:36:29.520]   I'm sure people will figure it out,
[00:36:29.520 --> 00:36:31.640]   and in three years or five years,
[00:36:31.640 --> 00:36:34.040]   there will be disease models that we could make
[00:36:34.040 --> 00:36:35.440]   for things that we can't make today.
[00:36:35.440 --> 00:36:38.760]   - Yeah, and this conversation would seem almost outdated
[00:36:38.760 --> 00:36:40.520]   with the kind of scale that could be achieved
[00:36:40.520 --> 00:36:41.360]   in like three years.
[00:36:41.360 --> 00:36:42.280]   - I hope so. - That's the hope.
[00:36:42.280 --> 00:36:43.880]   - That would be so cool.
[00:36:43.880 --> 00:36:48.120]   - So you've co-founded Coursera with Andrew Ng,
[00:36:48.120 --> 00:36:50.460]   and were part of the whole MOOC revolution.
[00:36:50.460 --> 00:36:53.960]   So to jump topics a little bit,
[00:36:53.960 --> 00:36:57.960]   can you maybe tell the origin story of the history,
[00:36:57.960 --> 00:37:00.960]   the origin story of MOOCs, of Coursera,
[00:37:00.960 --> 00:37:05.960]   and in general, your teaching to huge audiences
[00:37:05.960 --> 00:37:12.200]   on a very sort of impactful topic of AI in general?
[00:37:12.200 --> 00:37:16.480]   - So I think the origin story of MOOCs emanates
[00:37:16.480 --> 00:37:19.000]   from a number of efforts that occurred
[00:37:19.000 --> 00:37:24.000]   at Stanford University around the late 2000s,
[00:37:24.000 --> 00:37:28.640]   where different individuals within Stanford,
[00:37:28.640 --> 00:37:31.560]   myself included, were getting really excited
[00:37:31.560 --> 00:37:35.300]   about the opportunities of using online technologies
[00:37:35.300 --> 00:37:39.040]   as a way of achieving both improved quality of teaching
[00:37:39.040 --> 00:37:41.040]   and also improved scale.
[00:37:41.040 --> 00:37:44.520]   And so Andrew, for instance,
[00:37:44.520 --> 00:37:48.900]   led the Stanford Engineering Everywhere,
[00:37:48.900 --> 00:37:51.740]   which was sort of an attempt to take 10 Stanford courses
[00:37:51.740 --> 00:37:56.080]   and put them online, just as video lectures.
[00:37:56.080 --> 00:38:00.640]   I led an effort within Stanford to take some of the courses
[00:38:00.640 --> 00:38:04.440]   and really create a very different teaching model
[00:38:04.440 --> 00:38:07.400]   that broke those up into smaller units
[00:38:07.400 --> 00:38:11.100]   and had some of those embedded interactions and so on,
[00:38:11.100 --> 00:38:14.640]   which got a lot of support from university leaders
[00:38:14.640 --> 00:38:17.440]   because they felt like it was potentially a way
[00:38:17.440 --> 00:38:19.640]   of improving the quality of instruction at Stanford
[00:38:19.640 --> 00:38:23.520]   by moving to what's now called the flipped classroom model.
[00:38:23.520 --> 00:38:26.680]   And so those efforts eventually sort of started
[00:38:26.680 --> 00:38:28.080]   to interplay with each other
[00:38:28.080 --> 00:38:31.000]   and created a tremendous sense of excitement and energy
[00:38:31.000 --> 00:38:32.840]   within the Stanford community
[00:38:32.840 --> 00:38:36.440]   about the potential of online teaching
[00:38:36.440 --> 00:38:39.280]   and led in the fall of 2011
[00:38:39.280 --> 00:38:42.540]   to the launch of the first Stanford MOOCs.
[00:38:42.540 --> 00:38:46.480]   - By the way, MOOCs, it's probably impossible
[00:38:46.480 --> 00:38:49.080]   that people don't know, but I guess massive-
[00:38:49.080 --> 00:38:50.280]   - Open online courses.
[00:38:50.280 --> 00:38:51.920]   - Open online courses.
[00:38:51.920 --> 00:38:53.080]   So the- - We did not come up
[00:38:53.080 --> 00:38:54.360]   with the acronym.
[00:38:54.360 --> 00:38:57.060]   I'm not particularly fond of the acronym,
[00:38:57.060 --> 00:38:57.900]   but it is what it is.
[00:38:57.900 --> 00:38:58.720]   - It is what it is.
[00:38:58.720 --> 00:39:01.400]   Big bang is not a great term for the start of the universe,
[00:39:01.400 --> 00:39:02.360]   but it is what it is.
[00:39:02.360 --> 00:39:03.580]   - Probably so.
[00:39:03.580 --> 00:39:05.280]   (Lex laughing)
[00:39:05.280 --> 00:39:10.280]   So anyway, so those courses launched in the fall of 2011
[00:39:10.280 --> 00:39:13.800]   and there were, within a matter of weeks,
[00:39:13.800 --> 00:39:15.920]   with no real publicity campaign,
[00:39:15.920 --> 00:39:19.140]   just a New York Times article that went viral,
[00:39:20.360 --> 00:39:24.600]   about 100,000 students or more in each of those courses.
[00:39:24.600 --> 00:39:29.240]   And I remember this conversation that Andrew and I had,
[00:39:29.240 --> 00:39:31.680]   which is like, wow, this is just,
[00:39:31.680 --> 00:39:33.440]   there's this real need here.
[00:39:33.440 --> 00:39:35.880]   And I think we both felt like,
[00:39:35.880 --> 00:39:39.000]   sure, we were accomplished academics
[00:39:39.000 --> 00:39:41.600]   and we could go back and go back to our labs,
[00:39:41.600 --> 00:39:43.560]   write more papers, but if we did that,
[00:39:43.560 --> 00:39:45.840]   then this wouldn't happen
[00:39:45.840 --> 00:39:48.720]   and it seemed too important not to happen.
[00:39:48.720 --> 00:39:51.640]   And so we spent a fair bit of time debating,
[00:39:51.640 --> 00:39:55.320]   do we wanna do this as a Stanford effort,
[00:39:55.320 --> 00:39:56.840]   kind of building on what we'd started?
[00:39:56.840 --> 00:39:59.360]   Do we wanna do this as a for-profit company?
[00:39:59.360 --> 00:40:00.800]   Do we wanna do this as a non-profit?
[00:40:00.800 --> 00:40:03.940]   And we decided ultimately to do it as we did with Coursera.
[00:40:03.940 --> 00:40:09.920]   And so we started really operating as a company
[00:40:09.920 --> 00:40:12.200]   at the beginning of 2012.
[00:40:12.200 --> 00:40:14.600]   - And the rest is history.
[00:40:14.600 --> 00:40:15.440]   - And the rest is history.
[00:40:15.440 --> 00:40:18.540]   But how did you, was that really surprising to you?
[00:40:18.540 --> 00:40:23.440]   How did you at that time and at this time
[00:40:23.440 --> 00:40:26.360]   make sense of this need
[00:40:26.360 --> 00:40:28.120]   for sort of global education you mentioned,
[00:40:28.120 --> 00:40:31.120]   that you felt that, wow, the popularity indicates
[00:40:31.120 --> 00:40:36.120]   that there's a hunger for sort of globalization of learning?
[00:40:36.120 --> 00:40:43.960]   - I think there is a hunger for learning that,
[00:40:45.000 --> 00:40:46.280]   globalization is part of it,
[00:40:46.280 --> 00:40:48.400]   but I think it's just a hunger for learning.
[00:40:48.400 --> 00:40:51.680]   The world has changed in the last 50 years.
[00:40:51.680 --> 00:40:56.120]   It used to be that you finished college, you got a job,
[00:40:56.120 --> 00:40:58.280]   by and large, the skills that you learned in college
[00:40:58.280 --> 00:41:00.520]   were pretty much what got you
[00:41:00.520 --> 00:41:02.520]   through the rest of your job history.
[00:41:02.520 --> 00:41:04.200]   And yeah, you learned some stuff,
[00:41:04.200 --> 00:41:06.760]   but it wasn't a dramatic change.
[00:41:06.760 --> 00:41:09.560]   Today, we're in a world where the skills
[00:41:09.560 --> 00:41:11.480]   that you need for a lot of jobs,
[00:41:11.480 --> 00:41:13.680]   they didn't even exist when you went to college
[00:41:13.680 --> 00:41:15.840]   and the jobs and many of the jobs that exist
[00:41:15.840 --> 00:41:19.920]   when you went to college don't even exist today or are dying.
[00:41:19.920 --> 00:41:23.920]   So part of that is due to AI, but not only.
[00:41:23.920 --> 00:41:28.600]   And we need to find a way of keeping people,
[00:41:28.600 --> 00:41:31.200]   giving people access to the skills that they need today.
[00:41:31.200 --> 00:41:33.280]   And I think that's really what's driving
[00:41:33.280 --> 00:41:35.120]   a lot of this hunger.
[00:41:35.120 --> 00:41:37.980]   - So I think if we even take a step back,
[00:41:37.980 --> 00:41:42.500]   for you, all of this started in trying to think of new ways
[00:41:42.500 --> 00:41:47.500]   to teach or new ways to sort of organize the material
[00:41:47.500 --> 00:41:49.600]   and present the material in a way
[00:41:49.600 --> 00:41:52.560]   that would help the education process, the pedagogy.
[00:41:52.560 --> 00:41:57.560]   So what have you learned about effective education
[00:41:57.560 --> 00:41:58.720]   from this process of playing,
[00:41:58.720 --> 00:42:01.720]   of experimenting with different ideas?
[00:42:01.720 --> 00:42:04.200]   - So we learned a number of things,
[00:42:04.200 --> 00:42:06.840]   some of which I think could translate back
[00:42:06.840 --> 00:42:08.600]   and have translated back effectively
[00:42:08.600 --> 00:42:10.120]   to how people teach on campus.
[00:42:10.120 --> 00:42:11.920]   And some of which I think are more specific
[00:42:11.920 --> 00:42:14.040]   to people who learn online,
[00:42:14.040 --> 00:42:17.120]   and more sort of people who learn
[00:42:17.120 --> 00:42:19.120]   as part of their daily life.
[00:42:19.120 --> 00:42:21.200]   So we learned, for instance, very quickly
[00:42:21.200 --> 00:42:23.400]   that short is better.
[00:42:23.400 --> 00:42:27.040]   So people who are especially in the workforce
[00:42:27.040 --> 00:42:30.240]   can't do a 15 week semester long course.
[00:42:30.240 --> 00:42:32.720]   They just can't fit that into their lives.
[00:42:32.720 --> 00:42:35.760]   - Sure, can you describe the shortness of what?
[00:42:35.760 --> 00:42:38.520]   The entirety? - Both.
[00:42:38.520 --> 00:42:40.680]   - Every aspect, so the little lecture,
[00:42:40.680 --> 00:42:43.240]   the lecture's short, the course is short.
[00:42:43.240 --> 00:42:44.080]   - Both.
[00:42:44.080 --> 00:42:48.060]   We started out, the first online education efforts
[00:42:48.060 --> 00:42:50.840]   were actually MIT's OpenCourseWare initiatives,
[00:42:50.840 --> 00:42:55.200]   and that was recording of classroom lectures.
[00:42:55.200 --> 00:42:57.880]   - An hour and a half or something like that, yeah.
[00:42:57.880 --> 00:43:00.400]   - And that didn't really work very well.
[00:43:00.400 --> 00:43:03.160]   I mean, some people benefit, I mean, of course they did,
[00:43:03.160 --> 00:43:06.760]   but it's not really a very palatable experience
[00:43:06.760 --> 00:43:11.280]   for someone who has a job and three kids
[00:43:11.280 --> 00:43:14.040]   and they need to run errands and such.
[00:43:14.040 --> 00:43:17.920]   They can't fit 15 weeks into their life,
[00:43:17.920 --> 00:43:20.760]   and the hour and a half is really hard.
[00:43:20.760 --> 00:43:23.000]   So we learned very quickly,
[00:43:23.000 --> 00:43:26.600]   I mean, we started out with short video modules,
[00:43:26.600 --> 00:43:28.240]   and over time we made them shorter
[00:43:28.240 --> 00:43:31.720]   because we realized that 15 minutes was still too long
[00:43:31.720 --> 00:43:33.920]   if you wanna fit in when you're waiting in line
[00:43:33.920 --> 00:43:35.560]   for your kid's doctor's appointment.
[00:43:35.560 --> 00:43:37.260]   It's better if it's five to seven.
[00:43:37.260 --> 00:43:42.580]   We learned that 15 week courses don't work,
[00:43:42.580 --> 00:43:44.880]   and you really wanna break this up into shorter units
[00:43:44.880 --> 00:43:46.840]   so that there is a natural completion point,
[00:43:46.840 --> 00:43:48.720]   gives people a sense of they're really close
[00:43:48.720 --> 00:43:50.480]   to finishing something meaningful.
[00:43:50.480 --> 00:43:53.620]   They can always come back and take part two and part three.
[00:43:53.620 --> 00:43:56.160]   We also learned that compressing the content
[00:43:56.160 --> 00:43:58.960]   works really well, because if some people,
[00:43:58.960 --> 00:44:01.080]   that pace works well, and for others,
[00:44:01.080 --> 00:44:03.280]   they can always rewind and watch again.
[00:44:03.280 --> 00:44:05.360]   And so people have the ability
[00:44:05.360 --> 00:44:07.000]   to then learn at their own pace.
[00:44:07.000 --> 00:44:10.120]   And so that flexibility,
[00:44:10.120 --> 00:44:12.620]   the brevity and the flexibility are both things
[00:44:12.620 --> 00:44:15.420]   that we found to be very important.
[00:44:15.420 --> 00:44:18.800]   We learned that engagement during the content is important,
[00:44:18.800 --> 00:44:20.640]   and the quicker you give people feedback,
[00:44:20.640 --> 00:44:22.560]   the more likely they are to be engaged.
[00:44:22.560 --> 00:44:24.560]   Hence the introduction of these,
[00:44:24.560 --> 00:44:27.480]   which we actually was an intuition that I had going in
[00:44:27.480 --> 00:44:30.880]   and was then validated using data,
[00:44:30.880 --> 00:44:34.320]   that introducing some of these sort of little micro quizzes
[00:44:34.320 --> 00:44:36.480]   into the lectures really helps.
[00:44:36.480 --> 00:44:39.400]   Self-graded, automatically graded assessments
[00:44:39.400 --> 00:44:41.880]   really help too, because it gives people feedback.
[00:44:41.880 --> 00:44:43.200]   See, there you are.
[00:44:43.200 --> 00:44:45.600]   So all of these are valuable.
[00:44:45.600 --> 00:44:47.240]   And then we learned a bunch of other things too.
[00:44:47.240 --> 00:44:48.920]   We did some really interesting experiments,
[00:44:48.920 --> 00:44:50.600]   for instance, on non-gender bias,
[00:44:50.600 --> 00:44:55.600]   and how having a female role model as an instructor
[00:44:55.600 --> 00:44:59.320]   can change the balance of men to women
[00:44:59.320 --> 00:45:02.040]   in terms of, especially in STEM courses.
[00:45:02.040 --> 00:45:04.800]   And you could do that online by doing A/B testing
[00:45:04.800 --> 00:45:07.720]   in ways that would be really difficult to go on campus.
[00:45:07.720 --> 00:45:09.120]   - Oh, that's exciting.
[00:45:09.120 --> 00:45:12.020]   But so the shortness, the compression, I mean,
[00:45:12.020 --> 00:45:16.440]   it has actually, so that probably is true for all,
[00:45:16.440 --> 00:45:20.360]   you know, good editing is always just compressing
[00:45:20.360 --> 00:45:21.920]   the content, making it shorter.
[00:45:21.920 --> 00:45:24.840]   So that puts a lot of burden on the creator of the,
[00:45:24.840 --> 00:45:28.640]   the instructor and the creator of the educational content.
[00:45:28.640 --> 00:45:31.240]   Probably most lectures at MIT or Stanford
[00:45:31.240 --> 00:45:34.360]   could be five times shorter
[00:45:34.360 --> 00:45:37.560]   if the preparation was put enough.
[00:45:37.560 --> 00:45:41.680]   So maybe people might disagree with that,
[00:45:41.680 --> 00:45:44.040]   but like the crispness, the clarity
[00:45:44.040 --> 00:45:47.280]   that a lot of the, like Coursera delivers
[00:45:47.280 --> 00:45:50.120]   is how much effort does that take?
[00:45:50.120 --> 00:45:54.120]   - So first of all, let me say that it's not clear
[00:45:54.120 --> 00:45:57.360]   that that crispness would work as effectively
[00:45:57.360 --> 00:45:58.880]   in a face-to-face setting,
[00:45:58.880 --> 00:46:02.440]   because people need time to absorb the material.
[00:46:02.440 --> 00:46:04.760]   And so you need to at least pause
[00:46:04.760 --> 00:46:07.300]   and give people a chance to reflect and maybe practice.
[00:46:07.300 --> 00:46:08.400]   And that's what MOOCs do,
[00:46:08.400 --> 00:46:10.800]   is that they give you these chunks of content
[00:46:10.800 --> 00:46:13.440]   and then ask you to practice with it.
[00:46:13.440 --> 00:46:16.360]   And that's where I think some of the newer pedagogy
[00:46:16.360 --> 00:46:19.200]   that people are adopting in face-to-face teaching
[00:46:19.200 --> 00:46:21.600]   that have to do with interactive learning and such
[00:46:21.600 --> 00:46:23.520]   can be really helpful.
[00:46:23.520 --> 00:46:26.640]   But both those approaches,
[00:46:26.640 --> 00:46:29.440]   whether you're doing that type of methodology
[00:46:29.440 --> 00:46:32.880]   in online teaching or in that flipped classroom,
[00:46:32.880 --> 00:46:34.520]   interactive teaching.
[00:46:34.520 --> 00:46:37.200]   - What's, sorry to pause, what's flipped classroom?
[00:46:37.200 --> 00:46:41.560]   - Flipped classroom is a way in which online content
[00:46:41.560 --> 00:46:45.080]   is used to supplement face-to-face teaching,
[00:46:45.080 --> 00:46:47.240]   where people watch the videos perhaps
[00:46:47.240 --> 00:46:49.840]   and do some of the exercises before coming to class.
[00:46:49.840 --> 00:46:51.200]   And then when they come to class,
[00:46:51.200 --> 00:46:53.580]   it's actually to do much deeper problem solving,
[00:46:53.580 --> 00:46:54.980]   oftentimes in a group.
[00:46:56.120 --> 00:47:00.440]   But any one of those different pedagogies
[00:47:00.440 --> 00:47:03.480]   that are beyond just standing there and droning on
[00:47:03.480 --> 00:47:06.280]   in front of the classroom for an hour and 15 minutes
[00:47:06.280 --> 00:47:09.220]   require a heck of a lot more preparation.
[00:47:09.220 --> 00:47:13.640]   And so it's one of the challenges I think that people have,
[00:47:13.640 --> 00:47:15.720]   that we had when trying to convince instructors
[00:47:15.720 --> 00:47:16.680]   to teach on Coursera.
[00:47:16.680 --> 00:47:18.080]   And it's part of the challenges
[00:47:18.080 --> 00:47:21.080]   that pedagogy experts on campus have
[00:47:21.080 --> 00:47:22.800]   in trying to get faculty to teach differently,
[00:47:22.800 --> 00:47:24.400]   is that it's actually harder to teach that way
[00:47:24.400 --> 00:47:26.320]   than it is to stand there and drone.
[00:47:26.320 --> 00:47:32.400]   - Do you think MOOCs will replace in-person education
[00:47:32.400 --> 00:47:36.920]   or become the majority of in-person,
[00:47:36.920 --> 00:47:41.400]   of education of the way people learn in the future?
[00:47:41.400 --> 00:47:43.280]   Again, the future could be very far away,
[00:47:43.280 --> 00:47:46.040]   but where's the trend going, do you think?
[00:47:46.040 --> 00:47:50.160]   - So I think it's a nuanced and complicated answer.
[00:47:50.240 --> 00:47:55.240]   I don't think MOOCs will replace face-to-face teaching.
[00:47:55.240 --> 00:48:00.400]   I think learning is in many cases a social experience.
[00:48:00.400 --> 00:48:02.560]   And even at Coursera,
[00:48:02.560 --> 00:48:06.680]   we had people who naturally formed study groups,
[00:48:06.680 --> 00:48:07.760]   even when they didn't have to,
[00:48:07.760 --> 00:48:10.280]   to just come and talk to each other.
[00:48:10.280 --> 00:48:14.440]   And we found that that actually benefited their learning
[00:48:14.440 --> 00:48:15.760]   in very important ways.
[00:48:15.760 --> 00:48:19.640]   So there was more success among learners
[00:48:19.640 --> 00:48:22.600]   who had those study groups than among ones who didn't.
[00:48:22.600 --> 00:48:23.840]   So I don't think it's just gonna,
[00:48:23.840 --> 00:48:26.040]   oh, we're all gonna just suddenly learn online
[00:48:26.040 --> 00:48:28.360]   with a computer and no one else,
[00:48:28.360 --> 00:48:30.760]   in the same way that recorded music
[00:48:30.760 --> 00:48:33.160]   has not replaced live concerts.
[00:48:33.160 --> 00:48:39.000]   But I do think that especially when you are thinking
[00:48:39.000 --> 00:48:41.680]   about continuing education,
[00:48:41.680 --> 00:48:44.760]   the stuff that people get when their traditional,
[00:48:44.760 --> 00:48:47.880]   whatever high school, college education is done,
[00:48:47.880 --> 00:48:52.560]   and they yet have to maintain their level of expertise
[00:48:52.560 --> 00:48:54.680]   and skills in a rapidly changing world,
[00:48:54.680 --> 00:48:57.160]   I think people will consume more and more educational
[00:48:57.160 --> 00:48:59.720]   content in this online format,
[00:48:59.720 --> 00:49:02.680]   because going back to school for formal education
[00:49:02.680 --> 00:49:04.760]   is not an option for most people.
[00:49:04.760 --> 00:49:07.440]   - Briefly, it might be a difficult question to ask,
[00:49:07.440 --> 00:49:10.000]   but there's a lot of people fascinated
[00:49:10.000 --> 00:49:12.920]   by artificial intelligence, by machine learning,
[00:49:12.920 --> 00:49:14.040]   by deep learning.
[00:49:14.040 --> 00:49:18.160]   Is there a recommendation for the next year
[00:49:18.160 --> 00:49:21.400]   or for a lifelong journey of somebody interested in this?
[00:49:21.400 --> 00:49:23.760]   How do they begin?
[00:49:23.760 --> 00:49:27.280]   How do they enter that learning journey?
[00:49:27.280 --> 00:49:31.640]   - I think the important thing is first to just get started.
[00:49:31.640 --> 00:49:36.640]   And there's plenty of online content that one can get
[00:49:36.640 --> 00:49:41.280]   for both the core foundations of mathematics
[00:49:41.280 --> 00:49:43.040]   and statistics and programming.
[00:49:43.040 --> 00:49:45.360]   And then from there to machine learning.
[00:49:45.360 --> 00:49:48.400]   I would encourage people not to skip too quickly
[00:49:48.400 --> 00:49:51.360]   past the foundations, because I find that there's a lot
[00:49:51.360 --> 00:49:53.760]   of people who learn machine learning,
[00:49:53.760 --> 00:49:55.240]   whether it's online or on campus,
[00:49:55.240 --> 00:49:56.880]   without getting those foundations.
[00:49:56.880 --> 00:50:00.760]   And they basically just turn the crank on existing models
[00:50:00.760 --> 00:50:04.280]   in ways that A, don't allow for a lot of innovation
[00:50:04.280 --> 00:50:08.440]   and adjustment to the problem at hand,
[00:50:08.440 --> 00:50:10.360]   but also B, are sometimes just wrong
[00:50:10.360 --> 00:50:12.640]   and they don't even realize that their application
[00:50:12.640 --> 00:50:14.680]   is wrong because there's artifacts
[00:50:14.680 --> 00:50:16.600]   that they haven't fully understood.
[00:50:16.600 --> 00:50:18.480]   So I think the foundations,
[00:50:18.480 --> 00:50:20.560]   machine learning is an important step.
[00:50:20.560 --> 00:50:25.560]   And then actually start solving problems.
[00:50:25.560 --> 00:50:27.640]   Try and find someone to solve them with,
[00:50:27.640 --> 00:50:29.000]   because especially at the beginning,
[00:50:29.000 --> 00:50:31.560]   it's useful to have someone to bounce ideas off
[00:50:31.560 --> 00:50:33.240]   and fix mistakes that you make.
[00:50:33.240 --> 00:50:35.960]   And you can fix mistakes that they make,
[00:50:35.960 --> 00:50:40.520]   but then just find practical problems,
[00:50:40.520 --> 00:50:41.920]   whether it's in your workplace
[00:50:41.920 --> 00:50:43.280]   or if you don't have that,
[00:50:43.280 --> 00:50:46.120]   Kaggle competitions or such are a really great place
[00:50:46.120 --> 00:50:50.840]   to find interesting problems and just practice.
[00:50:50.840 --> 00:50:52.360]   - Practice.
[00:50:52.360 --> 00:50:54.520]   Perhaps a bit of a romanticized question,
[00:50:54.520 --> 00:50:59.280]   but what idea in deep learning do you find,
[00:50:59.280 --> 00:51:01.120]   have you found in your journey,
[00:51:01.120 --> 00:51:04.080]   the most beautiful or surprising or interesting?
[00:51:04.080 --> 00:51:09.400]   Perhaps not just deep learning,
[00:51:09.400 --> 00:51:12.600]   but AI in general, statistics.
[00:51:12.600 --> 00:51:16.680]   - I'm gonna answer with two things.
[00:51:16.680 --> 00:51:23.040]   One would be the foundational concept of end-to-end training,
[00:51:23.040 --> 00:51:26.920]   which is that you start from the raw data
[00:51:26.920 --> 00:51:31.920]   and you train something that is not like a single piece,
[00:51:31.920 --> 00:51:35.080]   but rather the,
[00:51:35.080 --> 00:51:38.960]   towards the actual goal that you're looking to-
[00:51:38.960 --> 00:51:40.840]   - From the raw data to the outcome,
[00:51:40.840 --> 00:51:43.560]   like no details in between.
[00:51:43.560 --> 00:51:44.640]   - Well, not no details,
[00:51:44.640 --> 00:51:45.680]   but the fact that you, I mean,
[00:51:45.680 --> 00:51:47.520]   you could certainly introduce building blocks
[00:51:47.520 --> 00:51:50.240]   that were trained towards other tasks.
[00:51:50.240 --> 00:51:53.080]   I'm actually coming to that in my second half of the answer,
[00:51:53.080 --> 00:51:57.760]   but it doesn't have to be like a single monolithic blob
[00:51:57.760 --> 00:52:00.240]   in the middle, actually, I think that's not ideal,
[00:52:00.240 --> 00:52:02.640]   but rather the fact that at the end of the day,
[00:52:02.640 --> 00:52:04.160]   you can actually train something
[00:52:04.160 --> 00:52:06.920]   that goes all the way from the beginning to the end.
[00:52:06.920 --> 00:52:09.160]   And the other one that I find really compelling
[00:52:09.160 --> 00:52:13.200]   is the notion of learning a representation
[00:52:13.200 --> 00:52:18.200]   that in its turn, even if it was trained to another task,
[00:52:18.200 --> 00:52:23.400]   can potentially be used as a much more rapid starting point
[00:52:23.400 --> 00:52:26.960]   to solving a different task.
[00:52:26.960 --> 00:52:29.040]   And that's, I think,
[00:52:29.040 --> 00:52:32.560]   reminiscent of what makes people successful learners.
[00:52:32.560 --> 00:52:35.720]   It's something that is relatively new
[00:52:35.720 --> 00:52:36.760]   in the machine learning space.
[00:52:36.760 --> 00:52:38.120]   I think it's underutilized
[00:52:38.120 --> 00:52:40.320]   even relative to today's capabilities,
[00:52:40.320 --> 00:52:42.800]   but more and more of how do we learn
[00:52:42.800 --> 00:52:45.480]   sort of reusable representation.
[00:52:45.480 --> 00:52:49.720]   - So end-to-end and transfer learning.
[00:52:49.720 --> 00:52:51.120]   - Yeah.
[00:52:51.120 --> 00:52:53.640]   - Is it surprising to you that neural networks
[00:52:53.640 --> 00:52:57.000]   are able to, in many cases, do these things?
[00:52:57.000 --> 00:53:02.000]   Is it maybe taken back to when you first would dive deep
[00:53:02.000 --> 00:53:05.440]   into neural networks or in general, even today,
[00:53:05.440 --> 00:53:07.840]   is it surprising that neural networks work at all
[00:53:07.840 --> 00:53:12.840]   and work wonderfully to do this kind of raw end-to-end
[00:53:12.840 --> 00:53:16.360]   learning and even transfer learning?
[00:53:16.360 --> 00:53:21.360]   - I think I was surprised by how well
[00:53:21.360 --> 00:53:25.800]   when you have large enough amounts of data,
[00:53:25.800 --> 00:53:31.800]   it's possible to find a meaningful representation
[00:53:32.920 --> 00:53:36.080]   in what is an exceedingly high dimensional space.
[00:53:36.080 --> 00:53:39.280]   And so I find that to be really exciting
[00:53:39.280 --> 00:53:41.600]   and people are still working on the math for that.
[00:53:41.600 --> 00:53:43.560]   There's more papers on that every year.
[00:53:43.560 --> 00:53:48.000]   And I think it would be really cool if we figured that out.
[00:53:48.000 --> 00:53:53.000]   But that to me was a surprise because in the early days
[00:53:53.000 --> 00:53:56.200]   when I was starting my way in machine learning
[00:53:56.200 --> 00:53:58.680]   and the data sets were rather small,
[00:53:58.680 --> 00:54:01.160]   I think we believed,
[00:54:01.160 --> 00:54:05.480]   I believe that you needed to have a much more constrained
[00:54:05.480 --> 00:54:08.640]   and knowledge rich search space
[00:54:08.640 --> 00:54:11.840]   to really get to a meaningful answer.
[00:54:11.840 --> 00:54:13.840]   And I think it was true at the time.
[00:54:13.840 --> 00:54:18.840]   What I think is still a question is,
[00:54:18.840 --> 00:54:23.180]   will a completely knowledge-free approach
[00:54:23.180 --> 00:54:26.000]   where there's no prior knowledge going
[00:54:26.000 --> 00:54:28.980]   into the construction of the model,
[00:54:28.980 --> 00:54:31.600]   is that gonna be the solution or not?
[00:54:31.600 --> 00:54:34.160]   It's not actually the solution today
[00:54:34.160 --> 00:54:36.520]   in the sense that the architecture
[00:54:36.520 --> 00:54:41.480]   of a convolutional neural network that's used for images
[00:54:41.480 --> 00:54:44.720]   is actually quite different to the type of network
[00:54:44.720 --> 00:54:47.720]   that's used for language and yet different
[00:54:47.720 --> 00:54:51.160]   from the one that's used for speech or biology
[00:54:51.160 --> 00:54:52.480]   or any other application.
[00:54:52.480 --> 00:54:57.040]   There's still some insight that goes into the structure
[00:54:57.040 --> 00:55:00.820]   of the network to get to the right performance.
[00:55:00.820 --> 00:55:01.660]   Will you be able to come up
[00:55:01.660 --> 00:55:03.260]   with a universal learning machine?
[00:55:03.260 --> 00:55:04.100]   I don't know.
[00:55:04.100 --> 00:55:06.580]   - I wonder if there's always has to be
[00:55:06.580 --> 00:55:10.300]   some insight injected somewhere or whether it can converge.
[00:55:10.300 --> 00:55:13.580]   So you've done a lot of interesting work
[00:55:13.580 --> 00:55:15.540]   with probabilistic graphical models
[00:55:15.540 --> 00:55:18.440]   and in general Bayesian deep learning and so on.
[00:55:18.440 --> 00:55:21.060]   Can you maybe speak high level,
[00:55:21.060 --> 00:55:25.500]   how can learning systems deal with uncertainty?
[00:55:25.500 --> 00:55:28.100]   - One of the limitations I think
[00:55:28.100 --> 00:55:32.360]   of a lot of machine learning models
[00:55:32.360 --> 00:55:35.720]   is that they come up with an answer
[00:55:35.720 --> 00:55:40.720]   and you don't know how much you can believe that answer.
[00:55:40.720 --> 00:55:45.820]   And oftentimes the answer is actually
[00:55:45.820 --> 00:55:50.540]   quite poorly calibrated relative to its uncertainties.
[00:55:50.540 --> 00:55:55.460]   Even if you look at where the confidence
[00:55:55.460 --> 00:55:58.660]   that comes out of the, say the neural network at the end
[00:55:58.660 --> 00:56:02.760]   and you ask how much more likely is an answer
[00:56:02.760 --> 00:56:07.640]   of 0.8 versus 0.9, it's not really in any way calibrated
[00:56:07.640 --> 00:56:12.640]   to the actual reliability of that network and how true it is
[00:56:12.640 --> 00:56:16.740]   and the further away you move from the training data,
[00:56:16.740 --> 00:56:20.640]   the more, not only the more wrong the network is,
[00:56:20.640 --> 00:56:22.540]   often it's more wrong and more confident
[00:56:22.540 --> 00:56:24.320]   in its wrong answer.
[00:56:24.320 --> 00:56:29.320]   And that is a serious issue in a lot of application areas.
[00:56:29.320 --> 00:56:31.640]   So when you think for instance about medical diagnosis
[00:56:31.640 --> 00:56:35.680]   as being maybe an epitome of how problematic this can be,
[00:56:35.680 --> 00:56:38.920]   if you were training your network on a certain set
[00:56:38.920 --> 00:56:41.500]   of patients, on a certain patient population,
[00:56:41.500 --> 00:56:44.600]   and I have a patient that is an outlier
[00:56:44.600 --> 00:56:46.720]   and there's no human that looks at this
[00:56:46.720 --> 00:56:49.040]   and that patient is put into a neural network
[00:56:49.040 --> 00:56:50.320]   and your network not only gives
[00:56:50.320 --> 00:56:51.920]   a completely incorrect diagnosis,
[00:56:51.920 --> 00:56:54.960]   but is supremely confident in its wrong answer,
[00:56:54.960 --> 00:56:56.300]   you could kill people.
[00:56:56.300 --> 00:57:01.300]   So I think creating more of an understanding
[00:57:01.300 --> 00:57:06.880]   of how do you produce networks that are calibrated
[00:57:06.880 --> 00:57:10.280]   in their uncertainty and can also say, you know what,
[00:57:10.280 --> 00:57:12.680]   I give up, I don't know what to say
[00:57:12.680 --> 00:57:14.560]   about this particular data instance
[00:57:14.560 --> 00:57:16.300]   because I've never seen something
[00:57:16.300 --> 00:57:18.120]   that's sufficiently like it before.
[00:57:18.120 --> 00:57:20.480]   I think it's going to be really important
[00:57:20.480 --> 00:57:23.000]   in mission critical applications,
[00:57:23.000 --> 00:57:25.360]   especially ones where human life is at stake
[00:57:25.360 --> 00:57:28.280]   and that includes medical applications,
[00:57:28.280 --> 00:57:31.160]   but it also includes automated driving
[00:57:31.160 --> 00:57:33.280]   because you'd want the network to be able to say,
[00:57:33.280 --> 00:57:36.000]   you know what, I have no idea what this blob is
[00:57:36.000 --> 00:57:37.120]   that I'm seeing in the middle of the road,
[00:57:37.120 --> 00:57:39.280]   so I'm just gonna stop because I don't wanna
[00:57:39.280 --> 00:57:42.800]   potentially run over a pedestrian that I don't recognize.
[00:57:42.800 --> 00:57:47.520]   - Is there good mechanisms, ideas of how to allow
[00:57:47.520 --> 00:57:52.240]   learning systems to provide that uncertainty
[00:57:52.240 --> 00:57:54.040]   along with their predictions?
[00:57:54.040 --> 00:57:57.160]   - Certainly people have come up with mechanisms
[00:57:57.160 --> 00:58:00.680]   that involve Bayesian deep learning,
[00:58:00.680 --> 00:58:04.480]   deep learning that involves Gaussian processes.
[00:58:04.480 --> 00:58:07.640]   I mean, there's a slew of different approaches
[00:58:07.640 --> 00:58:09.160]   that people have come up with.
[00:58:09.160 --> 00:58:12.920]   There's methods that use ensembles of networks
[00:58:12.920 --> 00:58:15.240]   with trained with different subsets of data
[00:58:15.240 --> 00:58:17.640]   or different random starting points.
[00:58:17.640 --> 00:58:20.240]   Those are actually sometimes surprisingly good
[00:58:20.240 --> 00:58:24.040]   at creating a sort of set of how confident
[00:58:24.040 --> 00:58:26.600]   or not you are in your answer.
[00:58:26.600 --> 00:58:29.000]   It's very much an area of open research.
[00:58:29.000 --> 00:58:33.640]   - Let's cautiously venture back into the land of philosophy
[00:58:33.640 --> 00:58:37.680]   and speaking of AI systems providing uncertainty,
[00:58:37.680 --> 00:58:41.540]   somebody like Stuart Russell believes that
[00:58:41.540 --> 00:58:43.440]   as we create more and more intelligent systems,
[00:58:43.440 --> 00:58:46.800]   it's really important for them to be full of self-doubt
[00:58:46.800 --> 00:58:51.960]   because if they're given more and more power,
[00:58:51.960 --> 00:58:54.840]   we want the way to maintain human control
[00:58:54.840 --> 00:58:57.120]   over AI systems or human supervision,
[00:58:57.120 --> 00:58:58.600]   which is true, like you just mentioned
[00:58:58.600 --> 00:59:00.400]   with autonomous vehicles, it's really important
[00:59:00.400 --> 00:59:04.160]   to get human supervision when the car is not sure
[00:59:04.160 --> 00:59:05.960]   because if it's really confident,
[00:59:05.960 --> 00:59:07.840]   in cases when it can get in trouble,
[00:59:07.840 --> 00:59:09.360]   it's gonna be really problematic.
[00:59:09.360 --> 00:59:13.000]   So let me ask about sort of the questions of AGI
[00:59:13.000 --> 00:59:14.840]   and human level intelligence.
[00:59:14.840 --> 00:59:17.180]   I mean, we've talked about curing diseases,
[00:59:17.180 --> 00:59:20.160]   which is sort of a fundamental thing
[00:59:20.160 --> 00:59:21.800]   we can have an impact today,
[00:59:21.800 --> 00:59:26.160]   but AI people also dream of both understanding
[00:59:26.160 --> 00:59:29.220]   and creating intelligence.
[00:59:29.220 --> 00:59:30.440]   Is that something you think about?
[00:59:30.440 --> 00:59:32.800]   Is that something you dream about?
[00:59:32.800 --> 00:59:37.000]   Is that something you think is within our reach
[00:59:37.000 --> 00:59:39.660]   to be thinking about as computer scientists?
[00:59:41.140 --> 00:59:45.180]   - Boy, let me tease apart different parts of that question.
[00:59:45.180 --> 00:59:46.320]   - The worst question.
[00:59:46.320 --> 00:59:48.020]   (both laughing)
[00:59:48.020 --> 00:59:50.940]   - Yeah, it's a multi-part question.
[00:59:50.940 --> 00:59:55.940]   So let me start with the feasibility of AGI,
[00:59:55.940 --> 01:00:01.500]   then I'll talk about the timelines a little bit
[01:00:01.500 --> 01:00:05.980]   and then talk about, well, what controls does one need
[01:00:05.980 --> 01:00:09.460]   when protecting, when thinking about protections
[01:00:09.460 --> 01:00:10.540]   in the AI space?
[01:00:10.540 --> 01:00:15.540]   So, I think AGI obviously is a longstanding dream
[01:00:15.540 --> 01:00:21.340]   that even our early pioneers in the space had,
[01:00:21.340 --> 01:00:26.260]   the Turing test and so on are the earliest discussions
[01:00:26.260 --> 01:00:27.600]   of that.
[01:00:27.600 --> 01:00:32.600]   We're obviously closer than we were 70 or so years ago,
[01:00:32.600 --> 01:00:36.700]   but I think it's still very far away.
[01:00:37.680 --> 01:00:41.140]   I think machine learning algorithms today
[01:00:41.140 --> 01:00:46.140]   are really exquisitely good pattern recognizers
[01:00:46.140 --> 01:00:49.700]   in very specific problem domains
[01:00:49.700 --> 01:00:51.800]   where they have seen enough training data
[01:00:51.800 --> 01:00:54.000]   to make good predictions.
[01:00:54.000 --> 01:00:58.120]   You take a machine learning algorithm
[01:00:58.120 --> 01:01:00.900]   and you move it to a slightly different version
[01:01:00.900 --> 01:01:04.000]   of even that same problem, far less one that's different,
[01:01:04.000 --> 01:01:07.200]   and it will just completely choke.
[01:01:07.200 --> 01:01:11.800]   So I think we're nowhere close to the versatility
[01:01:11.800 --> 01:01:15.800]   and flexibility of even a human toddler
[01:01:15.800 --> 01:01:19.920]   in terms of their ability to context switch
[01:01:19.920 --> 01:01:20.940]   and solve different problems
[01:01:20.940 --> 01:01:24.520]   using a single knowledge-based single brain.
[01:01:24.520 --> 01:01:29.520]   So am I desperately worried about the machines
[01:01:29.520 --> 01:01:35.480]   taking over the universe and starting to kill people
[01:01:35.480 --> 01:01:37.360]   because they want to have more power?
[01:01:37.360 --> 01:01:38.440]   I don't think so.
[01:01:38.440 --> 01:01:40.480]   - Well, so to pause on that,
[01:01:40.480 --> 01:01:43.640]   so you've kind of intuited that superintelligence
[01:01:43.640 --> 01:01:46.320]   is a very difficult thing to achieve.
[01:01:46.320 --> 01:01:47.160]   - Even intelligence.
[01:01:47.160 --> 01:01:48.160]   - Intelligence.
[01:01:48.160 --> 01:01:50.480]   - Superintelligence, we're not even close to intelligence.
[01:01:50.480 --> 01:01:53.380]   - Even just the greater abilities of generalization
[01:01:53.380 --> 01:01:55.180]   of our current systems.
[01:01:55.180 --> 01:01:59.200]   But we haven't answered all the parts.
[01:01:59.200 --> 01:02:00.800]   - I'm getting to the second part.
[01:02:00.800 --> 01:02:03.360]   - Okay, we'll take it, but maybe another tangent
[01:02:03.360 --> 01:02:06.780]   you can also pick up is can we get in trouble
[01:02:06.780 --> 01:02:08.160]   with much dumber systems?
[01:02:08.160 --> 01:02:10.760]   - Yes, and that is exactly where I was going.
[01:02:10.760 --> 01:02:11.600]   - Okay.
[01:02:11.600 --> 01:02:16.160]   - So just to wrap up on the threats of AGI,
[01:02:16.160 --> 01:02:21.160]   I think that it seems to me a little early today
[01:02:21.160 --> 01:02:26.960]   to figure out protections against a human level
[01:02:26.960 --> 01:02:29.400]   or superhuman level intelligence
[01:02:29.400 --> 01:02:32.360]   where we don't even see the skeleton
[01:02:32.360 --> 01:02:33.920]   of what that would look like.
[01:02:33.920 --> 01:02:36.440]   So it seems that it's very speculative
[01:02:36.440 --> 01:02:40.520]   on how to protect against that.
[01:02:40.520 --> 01:02:44.640]   But we can definitely and have gotten into trouble
[01:02:44.640 --> 01:02:46.680]   on much dumber systems.
[01:02:46.680 --> 01:02:49.040]   And a lot of that has to do with the fact
[01:02:49.040 --> 01:02:51.360]   that the systems that we're building
[01:02:51.360 --> 01:02:54.680]   are increasingly complex,
[01:02:54.680 --> 01:02:57.880]   increasingly poorly understood,
[01:02:57.880 --> 01:03:02.000]   and there's ripple effects that are unpredictable
[01:03:02.000 --> 01:03:04.200]   in changing little things
[01:03:04.200 --> 01:03:08.160]   that can have dramatic consequences on the outcome.
[01:03:08.160 --> 01:03:11.080]   And by the way, that's not unique
[01:03:11.080 --> 01:03:11.920]   to artificial intelligence.
[01:03:11.920 --> 01:03:14.400]   I think artificial intelligence exacerbates that,
[01:03:14.400 --> 01:03:15.660]   brings it to a new level.
[01:03:15.660 --> 01:03:18.960]   But heck, our electric grid is really complicated.
[01:03:18.960 --> 01:03:21.360]   The software that runs our financial markets
[01:03:21.360 --> 01:03:23.040]   is really complicated.
[01:03:23.040 --> 01:03:26.360]   And we've seen those ripple effects translate
[01:03:26.360 --> 01:03:29.060]   to dramatic negative consequences,
[01:03:29.060 --> 01:03:32.360]   like for instance, financial crashes
[01:03:32.360 --> 01:03:34.200]   that have to do with feedback loops
[01:03:34.200 --> 01:03:35.520]   that we didn't anticipate.
[01:03:35.520 --> 01:03:38.080]   So I think that's an issue that we need
[01:03:38.080 --> 01:03:41.100]   to be thoughtful about in many places,
[01:03:41.100 --> 01:03:44.800]   artificial intelligence being one of them.
[01:03:44.800 --> 01:03:48.480]   And we should, and I think it's really important
[01:03:48.480 --> 01:03:50.680]   that people are thinking about ways
[01:03:50.680 --> 01:03:55.640]   in which we can have better interpretability of systems,
[01:03:55.640 --> 01:03:59.120]   better tests for, for instance,
[01:03:59.120 --> 01:04:01.900]   measuring the extent to which a machine learning system
[01:04:01.900 --> 01:04:04.880]   that was trained in one set of circumstances,
[01:04:04.880 --> 01:04:07.360]   how well does it actually work
[01:04:07.360 --> 01:04:09.520]   in a very different set of circumstances
[01:04:09.520 --> 01:04:12.320]   where you might say, for instance,
[01:04:12.320 --> 01:04:14.760]   well, I'm not gonna be able to test my automated vehicle
[01:04:14.760 --> 01:04:17.840]   in every possible city, village,
[01:04:17.840 --> 01:04:20.760]   weather condition and so on.
[01:04:20.760 --> 01:04:23.740]   But if you trained it on this set of conditions
[01:04:23.740 --> 01:04:27.320]   and then tested it on 50 or 100 others
[01:04:27.320 --> 01:04:29.160]   that were quite different from the ones
[01:04:29.160 --> 01:04:31.960]   that you trained it on, and it worked,
[01:04:31.960 --> 01:04:34.100]   then that gives you confidence that the next 50
[01:04:34.100 --> 01:04:36.080]   that you didn't test it on might also work.
[01:04:36.080 --> 01:04:39.020]   So effectively it's testing for generalizability.
[01:04:39.020 --> 01:04:41.300]   So I think there's ways that we should be
[01:04:41.300 --> 01:04:44.500]   constantly thinking about to validate
[01:04:44.500 --> 01:04:47.500]   the robustness of our systems.
[01:04:47.500 --> 01:04:50.180]   I think it's very different from the,
[01:04:50.180 --> 01:04:53.260]   let's make sure robots don't take over the world.
[01:04:53.260 --> 01:04:57.020]   And then the other place where I think we have a threat,
[01:04:57.020 --> 01:04:59.400]   which is also important for us to think about
[01:04:59.400 --> 01:05:03.180]   is the extent to which technology can be abused.
[01:05:03.180 --> 01:05:06.540]   So like any really powerful technology,
[01:05:06.540 --> 01:05:11.900]   machine learning can be very much used badly
[01:05:11.900 --> 01:05:13.580]   as well as to good.
[01:05:13.580 --> 01:05:16.580]   And that goes back to many other technologies
[01:05:16.580 --> 01:05:20.220]   that have come up with when people invented
[01:05:20.220 --> 01:05:23.080]   projectile missiles and it turned into guns.
[01:05:23.080 --> 01:05:25.580]   And people invented nuclear power
[01:05:25.580 --> 01:05:27.380]   and it turned into nuclear bombs.
[01:05:27.380 --> 01:05:31.280]   And I think, honestly, I would say that to me,
[01:05:31.280 --> 01:05:34.420]   gene editing and CRISPR is at least as dangerous
[01:05:34.420 --> 01:05:39.420]   a technology if used badly as machine learning.
[01:05:39.420 --> 01:05:43.860]   You could create really nasty viruses and such
[01:05:43.860 --> 01:05:48.060]   using gene editing that are,
[01:05:48.060 --> 01:05:51.900]   you know, you would be really careful about.
[01:05:51.900 --> 01:05:56.700]   So anyway, that's something that we need
[01:05:56.700 --> 01:05:58.640]   to be really thoughtful about
[01:05:58.640 --> 01:06:02.500]   whenever we have any really powerful new technology.
[01:06:02.500 --> 01:06:04.140]   - Yeah, and in the case of machine learning
[01:06:04.140 --> 01:06:06.820]   is adversarial machine learning,
[01:06:06.820 --> 01:06:09.140]   so all the kinds of attacks like security almost threats.
[01:06:09.140 --> 01:06:10.540]   And there's a social engineering
[01:06:10.540 --> 01:06:12.120]   with machine learning algorithms.
[01:06:12.120 --> 01:06:13.700]   - And there's face recognition
[01:06:13.700 --> 01:06:15.900]   and big brothers watching you.
[01:06:15.900 --> 01:06:19.660]   And there's the killer drones
[01:06:19.660 --> 01:06:23.400]   that can potentially go and targeted execution
[01:06:23.400 --> 01:06:25.240]   of people in a different country.
[01:06:25.240 --> 01:06:28.600]   I don't, you know, one can argue that bombs
[01:06:28.600 --> 01:06:30.420]   are not necessarily that much better,
[01:06:30.420 --> 01:06:34.020]   but, you know, if people wanna kill someone,
[01:06:34.020 --> 01:06:35.740]   they'll find a way to do it.
[01:06:35.740 --> 01:06:39.060]   - So if you, in general, if you look at trends in the data,
[01:06:39.060 --> 01:06:41.100]   there's less wars, there's less violence,
[01:06:41.100 --> 01:06:42.940]   there's more human rights.
[01:06:42.940 --> 01:06:47.940]   So we've been doing overall quite good as a human species.
[01:06:48.340 --> 01:06:50.660]   - Are you optimistic? - Surprisingly sometimes.
[01:06:50.660 --> 01:06:52.780]   - Are you optimistic?
[01:06:52.780 --> 01:06:54.220]   Maybe another way to ask is,
[01:06:54.220 --> 01:06:58.020]   do you think most people are good
[01:06:58.020 --> 01:07:03.020]   and fundamentally we tend towards a better world,
[01:07:03.020 --> 01:07:05.460]   which is underlying the question,
[01:07:05.460 --> 01:07:09.180]   will machine learning with gene editing
[01:07:09.180 --> 01:07:12.140]   ultimately lend us somewhere good?
[01:07:12.140 --> 01:07:13.420]   Are you optimistic?
[01:07:15.860 --> 01:07:19.140]   - I think by and large, I'm optimistic.
[01:07:19.140 --> 01:07:24.140]   I think that most people mean well.
[01:07:24.140 --> 01:07:28.020]   That doesn't mean that most people are, you know,
[01:07:28.020 --> 01:07:32.340]   altruistic do-gooders, but I think most people mean well.
[01:07:32.340 --> 01:07:36.300]   But I think it's also really important for us as a society
[01:07:36.300 --> 01:07:41.300]   to create social norms where doing good
[01:07:42.140 --> 01:07:47.140]   and being perceived well by our peers
[01:07:47.140 --> 01:07:51.060]   are positively correlated.
[01:07:51.060 --> 01:07:55.140]   I mean, it's very easy to create dysfunctional societies.
[01:07:55.140 --> 01:07:58.100]   There are certainly multiple psychological experiments
[01:07:58.100 --> 01:08:01.940]   as well as sadly real world events
[01:08:01.940 --> 01:08:04.860]   where people have devolved to a world
[01:08:04.860 --> 01:08:08.900]   where being perceived well by your peers
[01:08:08.900 --> 01:08:12.260]   is correlated with really atrocious,
[01:08:12.260 --> 01:08:16.400]   often genocidal behaviors.
[01:08:16.400 --> 01:08:19.060]   So we really want to make sure
[01:08:19.060 --> 01:08:21.300]   that we maintain a set of social norms
[01:08:21.300 --> 01:08:25.300]   where people know that to be a successful member of society,
[01:08:25.300 --> 01:08:27.180]   you want to be doing good.
[01:08:27.180 --> 01:08:31.100]   And one of the things that I sometimes worry about
[01:08:31.100 --> 01:08:35.140]   is that some societies don't seem to necessarily
[01:08:35.140 --> 01:08:38.020]   be moving in the forward direction in that regard
[01:08:38.020 --> 01:08:40.100]   where it's not necessarily the case
[01:08:40.100 --> 01:08:44.780]   that being a good person
[01:08:44.780 --> 01:08:47.700]   is what makes you be perceived well by your peers.
[01:08:47.700 --> 01:08:49.420]   And I think that's a really important thing
[01:08:49.420 --> 01:08:50.980]   for us as a society to remember.
[01:08:50.980 --> 01:08:55.620]   It's very easy to degenerate back into a universe
[01:08:55.620 --> 01:09:00.220]   where it's okay to do really bad stuff
[01:09:00.220 --> 01:09:03.040]   and still have your peers think you're amazing.
[01:09:03.040 --> 01:09:07.820]   - It's fun to ask a world-class computer scientist
[01:09:07.820 --> 01:09:11.020]   and engineer a ridiculously philosophical question
[01:09:11.020 --> 01:09:13.140]   like what is the meaning of life?
[01:09:13.140 --> 01:09:17.180]   Let me ask, what gives your life meaning?
[01:09:17.180 --> 01:09:22.180]   What is the source of fulfillment, happiness, joy, purpose?
[01:09:22.180 --> 01:09:30.960]   - When we were starting Coursera in the fall of 2011,
[01:09:30.960 --> 01:09:36.900]   that was right around the time that Steve Jobs passed away.
[01:09:37.740 --> 01:09:41.020]   And so the media was full of various famous quotes
[01:09:41.020 --> 01:09:42.540]   that he uttered.
[01:09:42.540 --> 01:09:45.500]   And one of them that really stuck with me
[01:09:45.500 --> 01:09:48.780]   because it resonated with stuff that I'd been feeling
[01:09:48.780 --> 01:09:52.380]   for even years before that is that our goal in life
[01:09:52.380 --> 01:09:55.100]   should be to make a dent in the universe.
[01:09:55.100 --> 01:10:00.100]   So I think that to me, what gives my life meaning
[01:10:00.100 --> 01:10:05.620]   is that I would hope that when I am lying there
[01:10:05.900 --> 01:10:09.660]   on my deathbed and looking at what I'd done in my life,
[01:10:09.660 --> 01:10:14.660]   that I can point to ways in which I have left the world
[01:10:14.660 --> 01:10:20.460]   a better place than it was when I entered it.
[01:10:20.460 --> 01:10:23.600]   This is something I tell my kids all the time
[01:10:23.600 --> 01:10:27.260]   because I also think that the burden of that
[01:10:27.260 --> 01:10:29.340]   is much greater for those of us
[01:10:29.340 --> 01:10:31.420]   who were born to privilege.
[01:10:31.420 --> 01:10:34.380]   And in some ways I was, I mean, I wasn't born super wealthy
[01:10:34.380 --> 01:10:37.900]   or anything like that, but I grew up in an educated family
[01:10:37.900 --> 01:10:40.860]   with parents who loved me and took care of me
[01:10:40.860 --> 01:10:43.100]   and I had a chance at a great education
[01:10:43.100 --> 01:10:46.660]   and I always had enough to eat.
[01:10:46.660 --> 01:10:48.940]   So I was in many ways born to privilege
[01:10:48.940 --> 01:10:51.960]   more than the vast majority of humanity.
[01:10:51.960 --> 01:10:55.980]   And my kids, I think, are even more so born to privilege
[01:10:55.980 --> 01:10:57.960]   than I was fortunate enough to be.
[01:10:57.960 --> 01:11:00.500]   And I think it's really important that,
[01:11:00.500 --> 01:11:03.920]   especially for those of us who have that opportunity,
[01:11:03.920 --> 01:11:07.420]   that we use our lives to make the world a better place.
[01:11:07.420 --> 01:11:09.620]   - I don't think there's a better way to end it.
[01:11:09.620 --> 01:11:11.620]   Daphne, it was an honor to talk to you.
[01:11:11.620 --> 01:11:12.460]   Thank you so much for talking to me.
[01:11:12.460 --> 01:11:13.280]   - Thank you.
[01:11:13.280 --> 01:11:15.900]   - Thanks for listening to this conversation
[01:11:15.900 --> 01:11:17.020]   with Daphne Koller.
[01:11:17.020 --> 01:11:19.900]   And thank you to our presenting sponsor, Cash App.
[01:11:19.900 --> 01:11:21.660]   Please consider supporting the podcast
[01:11:21.660 --> 01:11:26.180]   by downloading Cash App and using code LEXPODCAST.
[01:11:26.180 --> 01:11:28.620]   Enjoy this podcast, subscribe on YouTube,
[01:11:28.620 --> 01:11:31.060]   review it with five stars on Apple Podcasts,
[01:11:31.060 --> 01:11:32.400]   support it on Patreon,
[01:11:32.400 --> 01:11:36.280]   simply connect with me on Twitter @LexFriedman.
[01:11:36.280 --> 01:11:39.800]   And now let me leave you with some words from Hippocrates,
[01:11:39.800 --> 01:11:41.880]   a physician from ancient Greece
[01:11:41.880 --> 01:11:44.360]   who's considered to be the father of medicine.
[01:11:44.360 --> 01:11:48.320]   Wherever the art of medicine is loved,
[01:11:48.320 --> 01:11:50.760]   there's also a love of humanity.
[01:11:50.760 --> 01:11:54.600]   Thank you for listening and hope to see you next time.
[01:11:54.600 --> 01:11:57.180]   (upbeat music)
[01:11:57.180 --> 01:11:59.760]   (upbeat music)
[01:11:59.760 --> 01:12:09.760]   [BLANK_AUDIO]

