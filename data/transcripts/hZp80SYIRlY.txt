
[00:00:00.000 --> 00:00:06.560]   All right, everybody, welcome back to the All In Pod, episode 174, almost up to 175. I believe
[00:00:06.560 --> 00:00:10.960]   there are a lot of folks getting together next week at the All In meetups around the world to
[00:00:10.960 --> 00:00:18.480]   watch the show. Unfortunately, J. Cal had an oral incident this week. He will not be joining us.
[00:00:18.480 --> 00:00:25.600]   I guess he cracked a tooth and couldn't make it. So he is sitting this one out,
[00:00:25.600 --> 00:00:30.960]   very last minute decision. Specifically, cracked a tooth, eating a bison rib at the Salt Lake in
[00:00:30.960 --> 00:00:36.720]   Austin, which is a true story. Freebird, I think we found an image of J. Cal at the Salt Lake
[00:00:36.720 --> 00:00:46.400]   cracking his tooth on a rib. Let your winners ride. Rain Man, David Sattler.
[00:00:49.360 --> 00:00:57.520]   And instead, we open sourced it to the fans and they've just gone crazy.
[00:00:57.520 --> 00:01:03.280]   All right, so we are going to announce the All In Summit 2024 back to LA.
[00:01:03.280 --> 00:01:08.560]   We had such a great time in LA. It was a great location for everyone. We really appreciated
[00:01:08.560 --> 00:01:14.080]   the facilities at UCLA. This year, we're upgrading the experience once again for everyone.
[00:01:14.080 --> 00:01:18.480]   So the event's going to be September 8th through 10th in LA. If you haven't seen it,
[00:01:18.480 --> 00:01:21.920]   please watch the recap video from last year. We put it up on YouTube.
[00:01:21.920 --> 00:01:26.880]   And then there's all the interviews from last year are also on YouTube. We're going to have
[00:01:26.880 --> 00:01:31.680]   another amazing lineup of speakers this year and have a much more kind of upgraded, elevated
[00:01:31.680 --> 00:01:38.000]   experience for everyone. Applications are open today. So go to summit.allinpodcast.co
[00:01:38.000 --> 00:01:43.120]   to put an application in to join us this year. As you know, we had way more applications last
[00:01:43.120 --> 00:01:47.520]   year than spots. And this year, we're going to have all sorts of upgrades. We're going to have
[00:01:47.520 --> 00:01:51.520]   transportation to and from all the events for everyone, a concierge booking service to help you
[00:01:51.520 --> 00:01:57.680]   book your rooms and get you set up in LA for the trip. We're doing a full day event on the Sunday,
[00:01:57.680 --> 00:02:02.800]   September 8th, followed by two days of content, Dine About Town on the first night, two blowout
[00:02:02.800 --> 00:02:07.360]   parties. It's going to be awesome. So we're really excited about it. We've got a much bigger budget
[00:02:07.360 --> 00:02:12.800]   this year. And we're going to try and really make the experience awesome, while also delivering the
[00:02:12.800 --> 00:02:18.400]   great content. So really excited to do this again in LA. I appreciate everyone's support of that.
[00:02:18.400 --> 00:02:25.520]   We had a lot of debate internally about where to take it. And we settled on going with what we know
[00:02:25.520 --> 00:02:29.600]   and trying to make it a little bit better this year. So anything else you guys want to say about
[00:02:29.600 --> 00:02:30.400]   All In Summit?
[00:02:30.400 --> 00:02:32.960]   And there's only one kind of ticket this year, right?
[00:02:32.960 --> 00:02:37.200]   That's right. So we're not doing like regular ticket and VIP. We're just doing one ticket
[00:02:37.200 --> 00:02:41.920]   for everyone. And everyone's going to get this kind of elevated experience. Should be cool.
[00:02:41.920 --> 00:02:45.760]   Saks, just show up. Would be awesome. Hope to see you there.
[00:02:45.760 --> 00:02:47.200]   I want to invite a couple of speakers.
[00:02:47.200 --> 00:02:49.600]   Yeah, you've got some good speakers, I saw.
[00:02:49.600 --> 00:02:49.760]   Yeah.
[00:02:49.760 --> 00:02:50.560]   It's gonna be great.
[00:02:50.560 --> 00:02:53.040]   When are we going to start announcing the speakers, Preberg?
[00:02:53.040 --> 00:02:59.680]   We will do that in a couple weeks. We already have a bunch booked. And it's gonna be awesome.
[00:02:59.680 --> 00:03:01.840]   Oh, that's really exciting. That's awesome.
[00:03:01.840 --> 00:03:04.800]   Yeah, I know CP, you wanted to take it somewhere else. I mean,
[00:03:04.800 --> 00:03:08.240]   we were in debate on Vegas, maybe going to some other country.
[00:03:09.840 --> 00:03:12.080]   But you went to another country. Where'd you go this week?
[00:03:12.080 --> 00:03:13.360]   I was in Paris, France.
[00:03:13.360 --> 00:03:15.200]   What were you doing there?
[00:03:15.200 --> 00:03:22.560]   There's like a important AI conference. And Jonathan Ross, the founder of Grok and I kicked
[00:03:22.560 --> 00:03:27.040]   it off with like a fireside chat for like 45 minutes at the beginning just to get everybody
[00:03:27.040 --> 00:03:28.720]   amped up and excited.
[00:03:28.720 --> 00:03:31.120]   Cool.
[00:03:31.120 --> 00:03:31.840]   Yeah, it was really good.
[00:03:31.840 --> 00:03:34.000]   Anything interesting come out of it?
[00:03:35.360 --> 00:03:42.160]   I think the soundbite that made an impact on me was from Jonathan. So I think he did a very good
[00:03:42.160 --> 00:03:50.320]   job of explaining where NVIDIA is really strong, which is really in learning, and where NVIDIA is
[00:03:50.320 --> 00:03:55.680]   trying to build the business, which is an inference. But the problem is the features
[00:03:55.680 --> 00:04:00.960]   almost compete with each other between learning and inference. And I think that that was good
[00:04:00.960 --> 00:04:05.600]   clarification. And then the second thing is he said that if he deploys his roadmap, he'll have
[00:04:05.600 --> 00:04:09.680]   50% of the available inference compute by the end of next year.
[00:04:09.680 --> 00:04:10.880]   Wow.
[00:04:10.880 --> 00:04:14.720]   Based on all the needs of like open AI and meta and everybody else, which is a...
[00:04:14.720 --> 00:04:17.920]   Wait, but that's a smaller physical footprint. Is that right? And it's just
[00:04:17.920 --> 00:04:19.680]   because they can do higher token per second?
[00:04:19.680 --> 00:04:27.120]   Less power, higher tokens per second across all the major models. So it would be meaningfully
[00:04:27.120 --> 00:04:33.600]   less power and meaningfully cheaper. So that was a pretty amazing soundbite. But his whole
[00:04:33.600 --> 00:04:37.120]   explanation was actually really good. So I mean, if you listen to the spaces we did,
[00:04:37.120 --> 00:04:41.120]   I thought it was good. I thought this was even better. So I'm trying to get a copy of the
[00:04:41.120 --> 00:04:48.880]   talk because it was taped so that we can kind of post it. At a minimum, anybody who's interested
[00:04:48.880 --> 00:04:54.320]   in studying NVIDIA should listen to what he has to say, because I think the nuances between us
[00:04:54.320 --> 00:04:57.600]   and them are really profound. And they were pretty stark the way that he described them.
[00:04:57.600 --> 00:04:59.760]   So it was cool. It was great to be in Paris, frankly.
[00:04:59.760 --> 00:05:04.720]   How does NVIDIA respond to this movement or this notion that there is an entirely
[00:05:04.720 --> 00:05:08.960]   different chip category focused purely on inference? Do they have inference focused
[00:05:08.960 --> 00:05:12.800]   chips in development or a different roadmap that they're going to have to kick up now?
[00:05:12.800 --> 00:05:18.240]   I mean, I think it's very hard for them. It is a structural innovator's dilemma. And why is that?
[00:05:18.240 --> 00:05:21.920]   Because they've made some architectural decisions around things like HBM,
[00:05:21.920 --> 00:05:27.840]   which is high bandwidth memory, or specific kinds of extremely high throughput optical cables.
[00:05:27.840 --> 00:05:31.920]   And why these things matter is that they've gone and bought up the world supply,
[00:05:31.920 --> 00:05:38.400]   and they've basically bearhugged these markets as a structural part of their design. Now,
[00:05:38.400 --> 00:05:44.880]   that makes sense, and it's very legal. The problem is that if you architect around it and go through
[00:05:44.880 --> 00:05:51.040]   a totally orthogonal design process, we're not subject to any of those supply chain considerations.
[00:05:51.040 --> 00:05:55.360]   And I think he explained that really well. I didn't understand it in nearly as much detail
[00:05:55.360 --> 00:05:59.760]   as I did leaving. So I think that's an important takeaway. It's also really important for all the
[00:05:59.760 --> 00:06:03.760]   hundreds of chip startups and for these venture investors who are ripping money into these
[00:06:03.760 --> 00:06:09.840]   companies to know these differences. Because if you are investing in a company that is subject to
[00:06:09.840 --> 00:06:15.040]   HBM in their design decisions, or these specific optical cables, or a whole bunch of other things,
[00:06:15.040 --> 00:06:18.720]   you're going to be in a really difficult spot because NVIDIA has bought them all.
[00:06:20.320 --> 00:06:24.240]   So I thought that that was really interesting. The other thing that NVIDIA does brilliantly is
[00:06:24.240 --> 00:06:31.120]   what Intel did in the '80s and '90s when they wanted to choke the market for CPU competition,
[00:06:31.120 --> 00:06:36.400]   which is that they were able to define a metric that everybody started to pay attention to,
[00:06:36.400 --> 00:06:41.040]   which for them was clock speed. And if you remember, Intel would push Moore's Law,
[00:06:41.040 --> 00:06:47.200]   and they would push transistor density and clock speed as the measurement of value.
[00:06:47.200 --> 00:06:54.080]   Now, for a lot of consumers, none of us knew any better. And so every time we saw a new chip,
[00:06:54.080 --> 00:06:57.680]   we would go through the Pentium upgrade cycle because we thought that's what we were supposed
[00:06:57.680 --> 00:07:03.440]   to do. It turns out that clock speed doesn't really factor into speed the way that you think
[00:07:03.440 --> 00:07:10.160]   about it and experience it as an end user using software. And it's the same in AI. And so we
[00:07:10.160 --> 00:07:15.520]   talked about that as well. So it's an incredibly important company, NVIDIA is. I think they're
[00:07:15.520 --> 00:07:20.720]   performing brilliantly. They're running many of the same plays that Intel did to try to create
[00:07:20.720 --> 00:07:28.000]   total dominance. But there's some very important nuances around power and around supply chain
[00:07:28.000 --> 00:07:34.000]   decisions and technology decisions that, frankly, some people would think are unsustainable.
[00:07:34.000 --> 00:07:39.440]   And the more I understand them, I would be in that camp as well. So I thought it was a very
[00:07:39.440 --> 00:07:45.520]   important conversation and hopefully we can post it. Yeah, it's cool. And by the way, it's great
[00:07:45.520 --> 00:07:49.920]   to be in Paris, too. What do you think of Paris? You know, I have a new appreciation for what's
[00:07:49.920 --> 00:07:54.960]   going on in these major cities. I think that all major cities are crumbling. I don't and I don't
[00:07:54.960 --> 00:07:59.840]   mean to be alarmist or very dramatic about that. It's just that I spent a lot of time with
[00:07:59.840 --> 00:08:05.840]   entrepreneurs that are in Paris. And they struggle with the same things that the people in San
[00:08:05.840 --> 00:08:10.000]   Francisco do. And then I had a couple of conversations with some folks that are entrepreneurs
[00:08:10.000 --> 00:08:17.040]   in London who were there. And they suffer from the same things. Lots of petty crime, lots of garbage,
[00:08:17.040 --> 00:08:23.760]   lots of vandalism, lots of drugs. And so no place is perfect. It turns out everybody is struggling
[00:08:23.760 --> 00:08:29.280]   from a lot of experimentation gone wrong and how these cities run. That was that was actually my
[00:08:29.280 --> 00:08:34.640]   the biggest takeaway outside of the actual AI conference itself. But that's a Western City
[00:08:34.640 --> 00:08:39.040]   problem. You're not I mean, have you visited many cities in China or other parts of Asia?
[00:08:39.040 --> 00:08:46.720]   And is that like a Western problem? I have not been to China in six years. I've been to Hong Kong,
[00:08:46.720 --> 00:08:53.440]   but I've not been into China for six years. So I don't know. I would say that India has its own
[00:08:53.440 --> 00:08:59.920]   vagaries. So those issues are not the same. I've been to Singapore a couple times. My kids were
[00:08:59.920 --> 00:09:04.240]   just in Japan for spring break. So I saw a bunch of the pictures, those cities look relatively
[00:09:04.240 --> 00:09:10.320]   the same, very orderly, well run clean. But yeah, you're right. These Western cities are a bit
[00:09:10.320 --> 00:09:16.960]   chaotic. Yeah, sacks. Is this like a Western social liberal manifestation? How cities are
[00:09:16.960 --> 00:09:21.920]   being challenged? Generally? Yes. Yes. Like Malay has, I think Malay said this, right? He said,
[00:09:21.920 --> 00:09:29.040]   like, all these Western social liberal beliefs are having lifestyle impact and urban centers.
[00:09:29.040 --> 00:09:32.400]   Well, it's hard for me to speak to what's happening in Europe, but we know what's happening
[00:09:32.400 --> 00:09:39.440]   in the United States. All the biggest cities are blue, and they're run by a certain ideology. And
[00:09:39.440 --> 00:09:46.960]   in those cities, we've had now a decade plus of decriminalization, deprosecution, decarceration.
[00:09:46.960 --> 00:09:53.120]   And the impacts of this are obvious. We also, at least in San Francisco, we have subsidies
[00:09:53.760 --> 00:09:59.920]   for people who want to live on the streets and do drugs all day. I mean, it's not prohibited to do
[00:09:59.920 --> 00:10:04.960]   that. So all of our public spaces have been taken over by people who are happy to take the whatever
[00:10:04.960 --> 00:10:12.640]   it is, $700, $800 subsidy and do drugs all day. We also had this referendum in California that
[00:10:12.640 --> 00:10:18.960]   essentially lowered a whole bunch of felonies to misdemeanors. Basically, if you steal less than
[00:10:18.960 --> 00:10:25.840]   $950, that's considered a misdemeanor now instead of a felony. And so there's been a huge increase
[00:10:25.840 --> 00:10:30.400]   in crime because of that. You know, shoplifting just doesn't get prosecuted anymore. And
[00:10:30.400 --> 00:10:33.520]   misdemeanors don't get prosecuted anymore. We've had all these SOARs, DAs who don't want to
[00:10:33.520 --> 00:10:41.920]   prosecute misdemeanors. So I think if voters had understood that lowering a lot of these crimes
[00:10:41.920 --> 00:10:45.200]   from felonies to misdemeanors meant that they wouldn't get prosecuted at all, I don't think
[00:10:45.200 --> 00:10:48.800]   they would have done it. But that's kind of where we are. But are we seeing a rebound from that
[00:10:48.800 --> 00:10:53.680]   now, Sax? Is democracy working? Because I know that in San Francisco, we had an election recently,
[00:10:53.680 --> 00:11:01.120]   and in the ballots, it seemed to indicate that voters were sick and fed up with the living
[00:11:01.120 --> 00:11:06.400]   situation in the city, and they're putting different people in office and have an intention
[00:11:06.400 --> 00:11:13.200]   of changing the city charter and changing the laws in the city so that we can reverse this trend.
[00:11:13.200 --> 00:11:18.320]   Is that not where things are? I mean, like, sitting where you sit, like, don't you see
[00:11:18.320 --> 00:11:24.000]   San Francisco kind of trying to right itself? And as a result, doesn't democracy work over time?
[00:11:24.000 --> 00:11:30.000]   Yeah, it's just that we live in a one-party state, and that one party has been captured by ideologues,
[00:11:30.000 --> 00:11:35.040]   and so it's very hard to change. It is true that in San Francisco, there was an election recently,
[00:11:35.040 --> 00:11:42.240]   and the more radical supervisors were defeated. We have, I think, finally a good DA in San Francisco.
[00:11:43.520 --> 00:11:48.960]   But these radicals have had a long time to entrench their power, and the party machinery
[00:11:48.960 --> 00:11:54.080]   helps keep them entrenched. So I think you can argue that we've bottomed out because the public
[00:11:54.080 --> 00:11:57.840]   has recognized the problem, but it's still very hard to fix, and it's going to take a while.
[00:11:57.840 --> 00:12:03.040]   I think for the first time in a long time, I'm more optimistic than not, given the people I've
[00:12:03.040 --> 00:12:08.880]   seen that tolerated the conditions historically have reversed course over the last year and have
[00:12:08.880 --> 00:12:13.120]   said, "We've got to act, and we've got to do our kind of civic process."
[00:12:13.120 --> 00:12:18.160]   Well, time will tell. You have to remember, when New York City was a chaotic mess in the '70s and
[00:12:18.160 --> 00:12:23.600]   '80s, it took two different mayors, and it took almost a decade for that to get cleaned up, but it
[00:12:23.600 --> 00:12:29.280]   took the strength of leadership. First, David Dinkins set the framework, and then Giuliani really
[00:12:29.280 --> 00:12:34.480]   executed Rudy Giuliani. And between the two of them, and I think what people call the broken
[00:12:34.480 --> 00:12:40.240]   windows theory, they hired a lot more policemen, and they just really cleaned up all of these really
[00:12:40.240 --> 00:12:47.600]   bad areas, and then it had this knock-on effect. The problem is, when I'm staying at a hotel in
[00:12:47.600 --> 00:12:52.400]   Paris, or when I was speaking to some of my friends who live in London, it's the same thing.
[00:12:52.400 --> 00:12:58.000]   It's like the amount of knife crime is so egregious, as an example, that they tell you,
[00:12:58.000 --> 00:13:02.320]   "Don't wear a watch," et cetera. I mean, I don't wear watches anymore when I go out.
[00:13:02.320 --> 00:13:06.000]   But what's crazy is they will tell you that you'll get stabbed over a phone,
[00:13:06.000 --> 00:13:10.800]   where it's like, "You don't need to stab me for my iPhone here. Just take the iPhone."
[00:13:10.800 --> 00:13:14.960]   So there's a level of lawlessness and petty crime that's affecting tourists
[00:13:14.960 --> 00:13:21.600]   that, to me, was very surprising. And so even when I was walking around Paris, I was like,
[00:13:21.600 --> 00:13:28.720]   "Should I really feel scared here?" And my reaction was, "No, I've never felt scared in
[00:13:28.720 --> 00:13:34.240]   Paris." But then what's ringing in the back of my ears are these warnings from people that have
[00:13:34.240 --> 00:13:38.480]   lived there for 10 and 20 years, who've had this stuff happen to them. That was really surprising
[00:13:38.480 --> 00:13:42.320]   to me. So that stuff in San Francisco, as far as I can tell, or any major city in the West,
[00:13:42.320 --> 00:13:46.560]   hasn't really been fixed, because you haven't had a shift in leadership yet, where they've
[00:13:46.560 --> 00:13:52.480]   really said, "Okay, we're going to double down on policemen and policing, and we're going to
[00:13:52.480 --> 00:13:57.040]   amp up the punishment." That hasn't happened. Well, I was just in Vegas this week,
[00:13:57.040 --> 00:13:59.600]   and it doesn't feel... How was your trip? Give us the trip report.
[00:13:59.600 --> 00:14:04.480]   Yeah, I went to the Google Next Conference. But I will say, first off, no crime in Vegas.
[00:14:04.480 --> 00:14:11.280]   Vegas is Vegas. It's a very different environment. But yeah, I went to the Google Next Conference.
[00:14:11.280 --> 00:14:18.000]   I ended up going out because at O'Halo, we did a partnership with GCP. So we're moving all of our
[00:14:18.000 --> 00:14:23.920]   infrastructure and doing a lot of model development on GCP. And it was great. I think there was 30,000
[00:14:23.920 --> 00:14:28.720]   people at this event, which was insane. Have you guys been to any of these, like AWS Reinvent, or
[00:14:28.720 --> 00:14:31.520]   the Google Next, or any of these?
[00:14:31.520 --> 00:14:33.120]   No, I have not.
[00:14:33.120 --> 00:14:38.080]   What I didn't realize prior to going to this was how much of an ecosystem there is. They
[00:14:38.080 --> 00:14:44.240]   take over the whole convention center at Mandalay Bay. And there's hundreds of companies that have
[00:14:44.240 --> 00:14:50.480]   built services or build services on GCP that are there, pitching CIOs and CTOs of other companies.
[00:14:50.480 --> 00:14:55.280]   And so it's this real network effect type of business model that I hadn't fully appreciated
[00:14:55.280 --> 00:15:00.960]   just how like, deep the network is and how entrenched it is. I also got to go to this
[00:15:00.960 --> 00:15:05.120]   thing where I spent a lot of time with CIOs and CTOs at other companies, big companies.
[00:15:05.120 --> 00:15:11.680]   And a big takeaway for me that I also didn't realize was how most of them are not on a single
[00:15:11.680 --> 00:15:19.600]   cloud provider. Almost everyone I spoke with is on multiple clouds. So they're all on like AWS
[00:15:19.600 --> 00:15:27.200]   and Azure and GCP. And they use different aspects of each cloud. And there's a lot of interoperability
[00:15:27.200 --> 00:15:32.160]   between the clouds now, which I had previously my naive concept of this, because I'm obviously not
[00:15:32.160 --> 00:15:37.200]   deeply in the space was that a company picks a cloud and they move everything over to that one
[00:15:37.200 --> 00:15:42.240]   service provider. But it turns out that they split, and then they allocate dollars based on
[00:15:42.240 --> 00:15:48.640]   the services and the pricing. So it was pretty clear that everyone is, it's like a race to the
[00:15:48.640 --> 00:15:55.520]   bottom on pricing for basic commoditizing services in the cloud, like data storage, right? And then
[00:15:55.520 --> 00:15:59.440]   whoever has the better higher level services, whoever has access to better models or has a
[00:15:59.440 --> 00:16:05.280]   lower price on running models, or on running things like inference. That's where the money
[00:16:05.280 --> 00:16:09.760]   is made. And the gross margin on that is quite different. And I think that's where a company
[00:16:09.760 --> 00:16:16.320]   like Google probably has a much bigger advantage because of the uniqueness and the quality of the
[00:16:16.320 --> 00:16:20.240]   models that they've developed. But yeah, I just didn't I didn't really appreciate like how much
[00:16:20.240 --> 00:16:25.680]   there was also. So my other big takeaway was the cloud is a wide open competitive market right now.
[00:16:25.680 --> 00:16:30.720]   Like there is no AWS lock in in this market. There's no Azure lock in. There's no GCP lock
[00:16:30.720 --> 00:16:36.240]   in like you guys. Yeah. The lock in comes from the developer services that they provide. So
[00:16:36.240 --> 00:16:42.400]   both Amazon and Google, and I'm assuming Azure as well, will give you access to incredibly talented
[00:16:42.400 --> 00:16:47.760]   engineers who will help you build production code. Yeah. And the ecosystem on how so like a lot of
[00:16:47.760 --> 00:16:54.400]   the clouds are competing for the consultants who actually do the work for the big enterprises.
[00:16:54.400 --> 00:17:00.480]   So Capgemini, Deloitte, PwC, they have these massive booths and facilities, and then go and
[00:17:00.480 --> 00:17:05.600]   build tools for companies is also a key part of the business strategy. And yeah, I mean,
[00:17:05.600 --> 00:17:11.360]   it's such a competitive market. I did not appreciate just like how much of an open game
[00:17:11.360 --> 00:17:16.320]   that still was. Sure. It was a great, great couple days in Vegas. And I got to see some
[00:17:16.320 --> 00:17:21.920]   of our friends. We ducked in and saw Helmuth in the Poker Go studio for a minute. Some big
[00:17:21.920 --> 00:17:26.160]   tournaments going on at the ARIA. It was fun. We had a good time. Okay, let's move on. So,
[00:17:26.160 --> 00:17:31.680]   you know, on the docket today, we were going to talk about CPI. The March CPI numbers came in
[00:17:31.680 --> 00:17:38.080]   from the Bureau of Labor Statistics yesterday, higher than forecast at 3.5% year over year.
[00:17:38.080 --> 00:17:43.920]   This is up from 3.2% year over year inflation reported by the Bureau of Labor Statistics in
[00:17:43.920 --> 00:17:48.880]   February, higher than expected for three straight months now. Let's pull up this chart. And as we
[00:17:48.880 --> 00:17:54.960]   all know, the Fed did 11 rate hikes in 2022 and 23. And the intention was that we would see
[00:17:54.960 --> 00:18:00.560]   inflation come down to their target of 2% and they would start to lower rates. And obviously,
[00:18:00.560 --> 00:18:06.880]   we're not seeing that inflation is remaining hot. Things are getting more and more expensive.
[00:18:07.440 --> 00:18:14.400]   I pulled together some of the numbers on year over year price increase. Housing is up 5.7%
[00:18:14.400 --> 00:18:21.600]   year over year. Transportation costs are up 10.7% year over year. And the highest car insurance is
[00:18:21.600 --> 00:18:29.360]   up 22.2% year over year. So the expectation has been that the market will do rate cuts.
[00:18:29.360 --> 00:18:33.360]   Larry Summers came out and said you have to take seriously the possibility that the next rate move
[00:18:33.360 --> 00:18:40.160]   will be upwards rather than downwards. I guess, Saks, let me let me ask you first for your
[00:18:40.160 --> 00:18:46.000]   commentary on how this plays into the election cycle. I know Biden was expecting a rate cut or
[00:18:46.000 --> 00:18:49.760]   several rate cuts going into the election, which usually helps the case of the incumbent. Maybe you
[00:18:49.760 --> 00:18:56.160]   can comment on where the election cycle might be influenced by this inflation report.
[00:18:56.160 --> 00:19:00.480]   Well, I think it's definitely bad news for Biden. I was expecting a Fed put this year.
[00:19:00.480 --> 00:19:04.160]   And really, the whole market was. The market was expecting three rate cuts this year,
[00:19:04.160 --> 00:19:09.280]   which would have been really good news for Biden. And I think this latest inflation
[00:19:09.280 --> 00:19:14.480]   print was kind of a dagger to the heart of that expectation. And I think the Wall Street Journal
[00:19:14.480 --> 00:19:21.280]   put it best. It said here, Wednesday's report had been hotly anticipated because Fed leaders
[00:19:21.280 --> 00:19:27.200]   had been willing to play down stronger than anticipated inflation readings in January and
[00:19:27.200 --> 00:19:34.720]   February as reflecting potential seasonal quirks. But a third straight month of above expectations
[00:19:34.720 --> 00:19:41.040]   inflation data erodes that story and could lead Fed officials to postpone anticipated rate cuts
[00:19:41.040 --> 00:19:46.400]   until July or later. So it's not just the fact that this inflation print was higher than expected.
[00:19:46.400 --> 00:19:51.040]   It was also higher than expected in January and February. But people were willing to kind of
[00:19:51.040 --> 00:19:56.640]   overlook that, saying, well, maybe it was just kind of a quirky reading. But now we've had three
[00:19:56.640 --> 00:20:00.880]   straight months, it's pretty clear that the narrative that we had going into this year,
[00:20:00.880 --> 00:20:07.600]   which was that inflation was on its way down, that it peaked at 9% as the official rate, I think,
[00:20:07.600 --> 00:20:14.480]   in 2022, and then it was going down every month through all of 2023. And I think the expectation
[00:20:14.480 --> 00:20:18.000]   going into '24 was it would keep going down and we'd get these rate cuts. Well,
[00:20:18.000 --> 00:20:24.480]   after three straight months of inflation being more persistent and stickier than people expected,
[00:20:24.480 --> 00:20:30.800]   I think that narrative is basically dead. So I think the new narrative now is it's going to be
[00:20:30.800 --> 00:20:36.080]   rates are going to be higher longer. And I think Larry Summers is reflecting that view. Larry's
[00:20:36.080 --> 00:20:40.560]   going further. He's not just saying that rates are going to be, you know, at the current rates
[00:20:40.560 --> 00:20:45.760]   for longer, he's saying they might actually increase. And that's a risk. Yeah, last summer,
[00:20:45.760 --> 00:20:50.000]   he said, and this was last summer. So almost a year ago, he was saying, the market's got it
[00:20:50.000 --> 00:20:53.840]   totally wrong. We're actually going to need much higher rates than the market's anticipating for
[00:20:53.840 --> 00:20:58.400]   much longer, as well, than the market is anticipating. And so did Mario Draghi,
[00:20:58.400 --> 00:21:02.160]   they both said the same thing. No one paid attention to him, everyone ignored it and
[00:21:02.160 --> 00:21:06.400]   assumed that this was going to be a quick rebound to normalization. It's clear that,
[00:21:06.400 --> 00:21:09.360]   you know, once again, Larry has proven himself to be fairly prescient.
[00:21:09.360 --> 00:21:15.760]   Larry Summers has, for the most part, been spot on from this on this whole inflation
[00:21:15.760 --> 00:21:22.240]   question going all the way back to 2021. Remember, 100% in Q1 of 2021, Biden's first quarter in
[00:21:22.240 --> 00:21:28.640]   office, the big legislative push was for the $2 trillion COVID relief bill is the so called
[00:21:28.640 --> 00:21:34.160]   American Rescue Plan. And Larry Summers said that it was it risked inflation, it was an inflationary
[00:21:34.160 --> 00:21:38.560]   bill. It was unnecessary. Yeah, we didn't need it. The economy was already coming back, and we
[00:21:38.560 --> 00:21:43.600]   didn't need it. And Biden was risking inflation. But of course, inflation was only at 2% at that
[00:21:43.600 --> 00:21:48.800]   point. So the administration kind of poopooed Larry and said, you know, this is Larry Summers being
[00:21:49.520 --> 00:21:56.240]   Larry or whatever. And sure enough, we were at 5% inflation by that summer. And you have to wonder
[00:21:56.240 --> 00:22:01.920]   if Biden had listened to that advice. Would he be in a different position right now going into the
[00:22:01.920 --> 00:22:06.160]   election? I think probably he would have been. And you got to wonder for what I mean, what did
[00:22:06.160 --> 00:22:12.800]   that $2 trillion accomplish? I mean, COVID was winding down. I mean, these were the last bit of
[00:22:12.800 --> 00:22:18.480]   stimmy checks and payments to these pharma companies. And I mean, it was basically a grab
[00:22:18.480 --> 00:22:23.280]   bag and it was passed on straight party lines. And it was just totally unnecessary. I mean,
[00:22:23.280 --> 00:22:29.280]   the economy certainly didn't need it. And so here we are. And I think that a lot of people,
[00:22:29.280 --> 00:22:35.280]   including the markets thought that Biden was kind of out of the woods that this year,
[00:22:35.280 --> 00:22:40.400]   we'd see the final leg of inflation going back to normal, but that's not going to happen.
[00:22:40.400 --> 00:22:43.680]   And I think, you know, going beyond the political ramifications, I think there could be
[00:22:44.640 --> 00:22:50.640]   several other knock-on effects that we should talk about. I mean, one is for the consumer.
[00:22:50.640 --> 00:22:56.320]   This means that the cost of borrowing is going to be higher for longer. That makes it harder
[00:22:56.320 --> 00:23:01.920]   to buy a house. Mortgage payments are higher. That also means if there's fewer home sale
[00:23:01.920 --> 00:23:06.480]   transactions, that means the price of housing could come down. So there could be a correction
[00:23:06.480 --> 00:23:11.040]   in that market. Second, if you want to buy a car, your car payments higher. And if you have
[00:23:11.040 --> 00:23:16.240]   loans, your personal interest is going to be higher. And this is why I think consumers feel
[00:23:16.240 --> 00:23:21.440]   like they're worse off than the economic data would otherwise reflect. And Larry Summers actually
[00:23:21.440 --> 00:23:26.320]   had a tweet storm about this about a month ago, where he calculated that if you included the cost
[00:23:26.320 --> 00:23:32.320]   of borrowing in inflation, that inflation was much higher than people thought and that it actually
[00:23:32.320 --> 00:23:38.400]   peaked not at 9%, but at 18%. So Larry had an excellent tweet storm on that. And he said that
[00:23:38.400 --> 00:23:44.400]   the cost of borrowing, which used to be calculated in inflation but is not anymore, was the reason
[00:23:44.400 --> 00:23:49.600]   why consumer sentiment about the economy was depressed. He said that that accounted for about
[00:23:49.600 --> 00:23:56.960]   70% of it. So people should be feeling better off because GDP growth is good and unemployment is
[00:23:56.960 --> 00:24:01.440]   low, but they're not because inflation is so high. And if you include cost of borrowing,
[00:24:01.440 --> 00:24:07.120]   the inflation is even higher. So I think this is really bad news for Biden. But it's a story
[00:24:07.120 --> 00:24:11.920]   that's been going on now for three years. It's not just this one inflation print.
[00:24:11.920 --> 00:24:18.960]   Right. It's funny. Yesterday, Biden said, during a press conference with Japanese Prime Minister
[00:24:18.960 --> 00:24:24.240]   Kishida, that he's sticking with his prediction of a rate cut before years end, but there might
[00:24:24.240 --> 00:24:30.480]   be a delay, which I think is obviously, you know, based on what the data showing and what
[00:24:30.480 --> 00:24:35.040]   folks that seem to know and have been pretty good predictors are saying is unlikely.
[00:24:35.040 --> 00:24:38.560]   We were supposed to get one in April. We were supposed to get one in June.
[00:24:38.560 --> 00:24:41.520]   Oh, yeah. Remember, the Fed Open Market Committee in December
[00:24:41.520 --> 00:24:46.640]   stated that they expected three rate cuts this year. And I think the market, in some cases,
[00:24:46.640 --> 00:24:49.520]   were predicting as high as four or five. Right.
[00:24:49.520 --> 00:24:54.480]   And now we may not have any and we may actually see a rate hike if you were to follow.
[00:24:54.480 --> 00:24:58.800]   Well, if we see a rate hike before the election, I think Biden is toast.
[00:24:58.800 --> 00:25:01.040]   Yeah. But I mean, I think Summers gave that maybe
[00:25:01.040 --> 00:25:05.600]   a 15 to 20% chance, which is it's still not the most likely scenario.
[00:25:05.600 --> 00:25:10.000]   But it's possible. But it's possible. And no one was counting on that in December.
[00:25:10.000 --> 00:25:16.640]   Chamath, if you're a Fed governor, and obviously, the Federal Reserve Board of Governors sets the
[00:25:16.640 --> 00:25:22.880]   rates, they meet and they make these decisions. How influenced are they politically? How influenced
[00:25:22.880 --> 00:25:26.960]   are they by this election cycle? And, you know, historically, this is meant to be a fairly
[00:25:26.960 --> 00:25:32.560]   independent board. But there's been a lot of consternation over the last few years that this
[00:25:32.560 --> 00:25:36.560]   board acts like almost a political apparatus, particularly in election years.
[00:25:36.560 --> 00:25:39.200]   You mean when they're not plagiarizing?
[00:25:39.200 --> 00:25:42.400]   Well, tell us what you're talking about.
[00:25:42.400 --> 00:25:51.760]   It turns out that before I answer your question, that was a joke. Nick, you can please show the
[00:25:51.760 --> 00:25:57.200]   tweet. But there was an article that came out that said one of the Fed governors has apparently
[00:25:57.200 --> 00:26:03.280]   massively plagiarized most of the work that got her to the position of being Fed governor.
[00:26:03.280 --> 00:26:03.840]   Yeah.
[00:26:03.840 --> 00:26:08.000]   So the same thing that happened to the president of Harvard. So TBD, what happens to that Fed
[00:26:08.000 --> 00:26:12.880]   governor? But it seems, you know, my observation there was just more that it's like the preferred
[00:26:12.880 --> 00:26:18.800]   method of defenestration now of academics and the apparatchik, right? Before, if you had
[00:26:19.440 --> 00:26:25.040]   right leaning views or centrist views, you get run out of town and get fired. Now, all of those folks
[00:26:25.040 --> 00:26:31.120]   are using the search engines to basically figure out really quickly that you plagiarized a large
[00:26:31.120 --> 00:26:38.160]   quantity of your academic work, and that your scholarship is is a little bit more speculative,
[00:26:38.160 --> 00:26:43.040]   which then brings into question, why do you get the right to help set the policy of the most
[00:26:43.040 --> 00:26:47.680]   important economy in the world? Well, anyways, so that that was that joke. But so I think that
[00:26:47.680 --> 00:26:55.280]   you're asking the absolute right question. I think that the Fed has become increasingly politicized.
[00:26:55.280 --> 00:27:04.240]   And I do think that there are actions that foreign governments take to influence the election,
[00:27:04.240 --> 00:27:10.080]   but I do not think it's what most people think. When I say that most people's ideas goes to like
[00:27:10.080 --> 00:27:16.000]   some darkroom conspiracy theory and misinformation. And I think that's a little bit naive.
[00:27:16.640 --> 00:27:23.040]   I think the more obvious way in which other governments can impact the US election is
[00:27:23.040 --> 00:27:28.880]   exactly how SAC said, which is they know that if inflation ticks high, and interest rates are up,
[00:27:28.880 --> 00:27:32.880]   and consumers are under pressure and meaningfully displeased with the economy, they'll vote out the
[00:27:32.880 --> 00:27:39.040]   sitting administration. And so what can other governments do? They can do what they're doing.
[00:27:39.040 --> 00:27:45.120]   So Nick, I love this like, explanation and pre charts, Nick chart number one, please.
[00:27:45.120 --> 00:27:51.760]   What is this? So this is a core driver of inflation, which is the price of energy.
[00:27:51.760 --> 00:27:58.800]   And what you see here is that since the beginning of the year, oil prices have gone up,
[00:27:58.800 --> 00:28:03.680]   which ultimately will translate to into a higher cost of doing business, right? Running your
[00:28:03.680 --> 00:28:09.760]   factory, keeping the lights on, keeping your home warm, getting from point A to point B,
[00:28:09.760 --> 00:28:14.400]   all of these things go up, which ultimately get reflected in prices, which is what inflation is.
[00:28:14.400 --> 00:28:19.520]   And so that's a very bad trend. Obviously, we had a lot of really good stuff happen
[00:28:19.520 --> 00:28:25.360]   through the course of last year, but it's largely reversed itself. So it's important to double click
[00:28:25.360 --> 00:28:31.600]   into this and figure out, well, who's driving this? Is it just a random act? Or is it a systemic set
[00:28:31.600 --> 00:28:37.040]   of decisions? And it turns out it's systemic. So if you go to the next chart, what this shows you
[00:28:37.040 --> 00:28:44.160]   is what the US has been doing. So the United States has clearly created an incentive to flood the
[00:28:44.160 --> 00:28:50.800]   economy with oil, which the laws of supply and demand would say prices should go down.
[00:28:50.800 --> 00:28:56.080]   And by bringing prices further down, they would have further brought down inflation,
[00:28:56.080 --> 00:29:00.160]   and they would have pulled forward these rate cuts that we had been expecting from the Fed.
[00:29:00.720 --> 00:29:06.160]   The problem is that the rest of the world actually said, no, hold on. And that's what you can see on
[00:29:06.160 --> 00:29:12.240]   the next chart. So for every time we were trying to increase production, all of these other countries
[00:29:12.240 --> 00:29:17.280]   have been cutting production. And this is the change in the OPEC plus crude production targets
[00:29:17.280 --> 00:29:23.920]   of 2024. So what you see here is for every barrel of oil, the US would pump out of the ground,
[00:29:23.920 --> 00:29:27.840]   more than that one barrel of oil would get actually restricted by all of these countries
[00:29:27.840 --> 00:29:32.080]   on this list, which is just to show you that these countries have a very specific view
[00:29:32.080 --> 00:29:38.160]   about how they want that energy market as and that's just one market, but it's an input a key
[00:29:38.160 --> 00:29:45.680]   input into inflation and rates to work. So you're saying that the OPEC has a price per barrel price
[00:29:45.680 --> 00:29:50.160]   target, and they're able to manipulate production to maintain that price target. So while the US is
[00:29:50.160 --> 00:29:54.560]   trying to lower the price per barrel, these other countries are are able to have a stronger
[00:29:54.560 --> 00:29:59.520]   influence and balance out to get when you when you make these cuts like this prices will go up
[00:29:59.520 --> 00:30:04.320]   because OPEC plus is meaningfully bigger, the sum of all the parts of OPEC plus is meaningfully
[00:30:04.320 --> 00:30:10.080]   bigger than just the United States on its own. And so the United States may amp up a few 100,000
[00:30:10.080 --> 00:30:16.640]   barrels a day. But if OPEC cuts by a few million barrels a day, you see what you saw in that first
[00:30:16.640 --> 00:30:21.440]   chart, which is the price will go up. And that's largely why prices have gone up. So I think why
[00:30:21.440 --> 00:30:27.920]   this happens is because when they see the United States, trying to manage inflation and manage
[00:30:27.920 --> 00:30:32.240]   global rates, right, because us rates are not just for the United States, they're the, they're the
[00:30:32.240 --> 00:30:37.760]   trendsetter for everything else. I think that in part, there's a decision that this rate philosophy,
[00:30:37.760 --> 00:30:43.760]   the economic philosophy, the political philosophy, if they wanted to support that, what they would do
[00:30:43.760 --> 00:30:49.120]   is actually also keep prices where they were or increase production. By reigning in production,
[00:30:49.120 --> 00:30:55.120]   what you're essentially doing is voting against that entire apparatus. Right? So OPEC, and all of
[00:30:55.120 --> 00:30:58.320]   those countries, and those decisions are not made by an oil minister, let's be honest, they're made
[00:30:58.320 --> 00:31:04.080]   by the leaders of those countries, whose decision then gets reflected by the minister of petroleum
[00:31:04.080 --> 00:31:09.520]   or oil in all of these countries, is a vote against Biden, and the United States government
[00:31:09.520 --> 00:31:13.680]   and the politicization potentially of the Fed. And then the last comment is just that when you
[00:31:13.680 --> 00:31:17.840]   look at the reverse repo rate, which is a fundamental measure of liquidity, what you
[00:31:17.840 --> 00:31:23.840]   also see is liquidity very quickly getting sucked out of the system, which is also generally a
[00:31:23.840 --> 00:31:28.480]   system health check that the Fed uses to make sure that the system is operating as required. So
[00:31:28.480 --> 00:31:38.000]   in totality, what I see is that there is a structural disillusionment with the current
[00:31:38.000 --> 00:31:43.760]   administration, and where people can make decisions in their own best interests,
[00:31:44.480 --> 00:31:49.280]   versus the collective interests of the United States in a broader framework that the US may
[00:31:49.280 --> 00:31:55.520]   represent are voting against it. And so this is why the idea of a persistent inflation rate
[00:31:55.520 --> 00:32:01.920]   is a lot more credible than it was six months ago. And I think, sorry, just the last point,
[00:32:01.920 --> 00:32:07.040]   and I think it's just what Zack says, which means that we're probably now on balance 50/50 between
[00:32:07.040 --> 00:32:11.120]   a hike and a cut. And I think if you don't see this thing change in the next three months,
[00:32:11.120 --> 00:32:18.080]   you're going to see 75/25 for a hike of at least 25 basis points. And I do think David's right. I
[00:32:18.080 --> 00:32:24.720]   don't see how the sitting administration in any government will be able to withstand the onslaught
[00:32:24.720 --> 00:32:32.000]   of electoral displeasure, if rates are four and a half, five and 6% going into November.
[00:32:32.000 --> 00:32:36.480]   Timoth, if that is, I think everything you said makes sense going into this
[00:32:37.040 --> 00:32:43.840]   election rates are high and and or going higher. What is the Biden administration do?
[00:32:43.840 --> 00:32:51.280]   The fact is, they are very likely going to come up with a strategy to try and win votes that will
[00:32:51.280 --> 00:32:57.120]   likely involve some sort of stimulatory process would be my guess. And that stimulatory process
[00:32:57.120 --> 00:33:01.760]   may look something like more student loan forgiveness, more debt forgiveness, Kamala
[00:33:01.760 --> 00:33:08.480]   Harris. She had a whole speech about debt. And here's here's the irony, the actions that the
[00:33:08.480 --> 00:33:14.640]   administration is likely to take to support themselves in the election cycle are likely
[00:33:14.640 --> 00:33:20.080]   going to further fuel inflation. And that's the biggest concern and worry I have is that there's
[00:33:20.080 --> 00:33:24.480]   almost this inevitable set of behaviors that are coming down the pipe here over the next few months
[00:33:24.480 --> 00:33:28.720]   because of the inflation report that are actually going to make things worse, not better.
[00:33:28.720 --> 00:33:32.560]   I mean, to your point, Biden just announced a new student loan forgiveness plan. Remember,
[00:33:32.560 --> 00:33:37.440]   the Supreme Court invalidated his last one. So now he's trying again. And it would potentially
[00:33:37.440 --> 00:33:42.080]   give loan forgiveness to something like 30 million borrowers. So yeah, they're going to try and buy
[00:33:42.080 --> 00:33:46.640]   votes. But I think it's already very late in the game to be trying to do that. I think that's what
[00:33:46.640 --> 00:33:51.760]   they've been doing over the past few years. I mean, Biden has been a huge spender. You know,
[00:33:51.760 --> 00:33:56.800]   he passed the COVID relief bill, he passed the Inflation Reduction Act, which was a ridiculous
[00:33:56.800 --> 00:34:04.880]   name for was it 750 billion of, of more spending. And there was the CHIPS Act, which was well,
[00:34:04.880 --> 00:34:08.400]   yeah, and there's the CHIPS Act and the infrastructure bill. And then remember,
[00:34:08.400 --> 00:34:13.040]   he wanted four and a half trillion of build back better on top of that. And he wanted even more,
[00:34:13.040 --> 00:34:19.280]   he didn't get any event. I think that this is a classic case of Biden spent the last three years
[00:34:19.280 --> 00:34:25.520]   making his bed and now he's gonna have to sleep in it. And if in fact, inflation is persistent,
[00:34:25.520 --> 00:34:29.600]   sticky and not going away, and he doesn't get rate cuts, again, this may cost him the election.
[00:34:29.600 --> 00:34:33.680]   But, but look, I want to move beyond the political aspect of this and just talk about some more of
[00:34:33.680 --> 00:34:39.680]   these knock on effects, because we talked about the impact on the consumer. There's also an impact
[00:34:39.680 --> 00:34:46.240]   on investors, there's an impact on the government, and there's an impact on on the banks. So let's
[00:34:46.240 --> 00:34:50.720]   just deal with the banks for a second. We've talked about on the show how commercial real
[00:34:50.720 --> 00:34:55.840]   estate developers are hanging on by their fingernails right now. You know, they, they
[00:34:55.840 --> 00:35:01.920]   basically borrowed huge amounts of money to buy commercial real estate, when rates were really low,
[00:35:01.920 --> 00:35:05.680]   and they did so at high valuations. Now there's been a huge correction on that market, and they
[00:35:05.680 --> 00:35:11.280]   cannot pay back those loans. And they can't get refinance. So what's happening? Well, they're
[00:35:11.280 --> 00:35:16.560]   working with the banks to restructure those loans to tack on the interest they can't pay now,
[00:35:16.560 --> 00:35:20.960]   to the end of the loan. And then the bank will hope and pretend that they'll be able to get
[00:35:20.960 --> 00:35:26.720]   paid back one day. It's called extend and pretend. Well, the extended pretend strategy could work if
[00:35:26.720 --> 00:35:31.280]   you believe that rate cuts are coming really soon, because you just need to hold on until the rate
[00:35:31.280 --> 00:35:35.840]   cuts are here, then you can get refinanced at lower rates. But if rates are going to be higher
[00:35:35.840 --> 00:35:39.360]   longer, that strategy is not going to work. And you're going to see more and more real estate
[00:35:39.360 --> 00:35:44.240]   sponsors throwing in the towel. And banks are going to end up foreclosing on these properties,
[00:35:44.240 --> 00:35:48.160]   and you're going to see more and more fire sales. There's a really dramatic example just the other
[00:35:48.160 --> 00:35:53.280]   day in St. Louis, a building that was bought a few years ago for $200 million just fire
[00:35:53.280 --> 00:35:58.800]   sailed at $2 million. I mean, 99% reduction. This is like dot com crash level reduction.
[00:35:58.800 --> 00:36:00.320]   That's for the equity, right? Yeah.
[00:36:00.320 --> 00:36:06.080]   No, this is like the price of the deal. I mean, I assume the equity was completely wiped out. Yeah,
[00:36:06.080 --> 00:36:07.920]   I guess they sold it for three and a half million.
[00:36:07.920 --> 00:36:11.200]   Are you sure that's not the equity? That's got to be the equity. There's no way that's the-
[00:36:11.200 --> 00:36:12.720]   No, it's just the price of the building.
[00:36:13.680 --> 00:36:14.560]   Okay.
[00:36:14.560 --> 00:36:15.120]   I don't know. I mean-
[00:36:15.120 --> 00:36:17.520]   With the debt. It's got to have the debt attached to it for that, yeah.
[00:36:17.520 --> 00:36:20.240]   I mean, either way, it's a total wipe out for the equity holders.
[00:36:20.240 --> 00:36:21.040]   Yeah, yeah.
[00:36:21.040 --> 00:36:25.200]   But it's also going to be a wipe out for the bank that made that loan originally,
[00:36:25.200 --> 00:36:29.520]   or that bought that loan when it was repackaged into CMBS or whatever it was.
[00:36:29.520 --> 00:36:35.040]   So I would expect that if rates stay higher longer, that's going to create tremendous
[00:36:35.040 --> 00:36:40.080]   stress on commercial real estate, and therefore on the regional banking system that made all these
[00:36:40.080 --> 00:36:46.000]   loans to commercial real estate developers. So that's knock-on effect number one. Knock-on effect
[00:36:46.000 --> 00:36:51.280]   number two is the government's cost of borrowing. The 10-year rate's already risen to 4.5% now.
[00:36:51.280 --> 00:36:55.600]   And if rates stay higher longer, as more and more of the government debt rolls,
[00:36:55.600 --> 00:36:59.440]   it's going to end up getting refinanced at higher rates. And the need to refinance
[00:36:59.440 --> 00:37:02.800]   all of this government debt is going to continue to put pressure on the bond markets.
[00:37:02.800 --> 00:37:08.240]   And I think this is why Larry is fundamentally pretty bearish on rates going down is the
[00:37:08.240 --> 00:37:13.040]   government's financing costs are so huge, and they continue to grow. We're adding something
[00:37:13.040 --> 00:37:17.680]   like a trillion dollars of new debt every 100 days. There've been some reports of this.
[00:37:17.680 --> 00:37:23.120]   Yeah, 7.6 trillion is getting refinanced of old debt in the next 12 months.
[00:37:23.120 --> 00:37:24.160]   Right.
[00:37:24.160 --> 00:37:29.840]   And that 7.6 is sitting around 2%, and now it's going to get bumped up to close to 5%,
[00:37:31.520 --> 00:37:39.040]   which adds an incremental $210 billion a year of debt service expense just on the debt that's
[00:37:39.040 --> 00:37:43.200]   getting refinanced in the next 12 months, not to mention the deficit, not to mention,
[00:37:43.200 --> 00:37:44.640]   et cetera, et cetera, et cetera.
[00:37:44.640 --> 00:37:49.040]   Right. So the government was extending and pretending as well. I mean, they were hoping
[00:37:49.040 --> 00:37:54.240]   that rates would go down fast enough that when all of this massive government debt rolls and
[00:37:54.240 --> 00:37:58.880]   needs to be refinanced, it'd be done at lower rates. So the interest costs wouldn't swamp us.
[00:37:58.880 --> 00:38:03.200]   We're already, I think, at over a trillion dollars now of annual interest expense for
[00:38:03.200 --> 00:38:03.760]   the federal government.
[00:38:03.760 --> 00:38:08.880]   We are. Yeah, and it's going to rise to 1.6 trillion. So it's going to be 60%
[00:38:08.880 --> 00:38:12.000]   greater than our defense budget, just servicing the old debt.
[00:38:12.000 --> 00:38:16.480]   We're basically in a classic debt spiral where we're borrowing. Our annual deficit
[00:38:16.480 --> 00:38:22.240]   is we're at 2 trillion plus now. So we're not paying back the principal on the debt.
[00:38:22.240 --> 00:38:26.880]   We're not even paying back the interest on the debt. We're borrowing money to pay off
[00:38:26.880 --> 00:38:32.400]   all the interest and then some on top of that. So that's a huge problem for the federal government.
[00:38:32.400 --> 00:38:37.680]   And then I think the last piece of this is investors. If you pull up this Fred chart
[00:38:37.680 --> 00:38:43.840]   on the Fed funds rate over the very, very long term. So what this chart shows,
[00:38:43.840 --> 00:38:49.120]   if you just zoom in on this, is the Fed funds rate basically over the last, I don't know,
[00:38:49.120 --> 00:38:56.560]   like 75 years. And you can see that it peaked in the early 1980s when Volcker decided to
[00:38:56.560 --> 00:39:02.960]   break the back of inflation by jacking up interest rates to like 18%. And it was very painful. We had
[00:39:02.960 --> 00:39:09.520]   a pretty severe recession and I think it was '81 or '82. But then the economy came roaring back by
[00:39:09.520 --> 00:39:16.720]   '83 and Reagan got elected in '84. In any event, that began a period of declining and low interest
[00:39:16.720 --> 00:39:24.000]   rates for investors, again, starting in the mid '80s. And it continued all the way through,
[00:39:24.640 --> 00:39:30.480]   I mean, especially through I think the post-GFC period and then this COVID period where we had
[00:39:30.480 --> 00:39:35.440]   zero interest rates for a long period of time. And you can see the last spike we've had was when the
[00:39:35.440 --> 00:39:40.880]   Fed decided to jack up rates to 5% in response to inflation. So the fact that we've got this
[00:39:40.880 --> 00:39:47.600]   persistent sticky inflation suggests that this whole, you could call it really almost a 40-year
[00:39:47.600 --> 00:39:53.040]   period of declining and low interest rates may be over. And when interest rates are going down,
[00:39:53.040 --> 00:40:00.880]   it creates a huge tailwind to the stock market and the bond market because the discount rate is lower,
[00:40:00.880 --> 00:40:06.960]   so stocks and bonds are going to be more valuable. Well, if we're in a long-term environment that's
[00:40:06.960 --> 00:40:12.720]   more inflationary and rates have to stay higher longer, that's not great news for investors.
[00:40:12.720 --> 00:40:18.960]   It hurts the stock market and it creates in the private investing world like VC and real estate,
[00:40:18.960 --> 00:40:25.360]   it creates a much higher hurdle rate. So it's harder for private investors to justify
[00:40:25.360 --> 00:40:32.080]   fundraising. So I think it's too early to say that this 40-year trend is over, but I think
[00:40:32.080 --> 00:40:38.960]   it could be. And the reason for that is just the huge pressure of government borrowing,
[00:40:38.960 --> 00:40:46.320]   crowding out private investing and this sort of ever-present need to keep issuing more and more
[00:40:46.320 --> 00:40:50.400]   debt at higher and higher rates to make it more attractive is going to create, I think,
[00:40:50.400 --> 00:40:56.720]   very bad conditions for investors. So Chamath, let me ask you if you concur,
[00:40:56.720 --> 00:41:03.200]   given that today where we're sitting as of recording right now, towards the end of market
[00:41:03.200 --> 00:41:09.920]   on Thursday, April 11, the NASDAQ is sitting at an all-time high, and the NASDAQ is up 1.6% today.
[00:41:10.560 --> 00:41:16.560]   Following this inflation report yesterday, what's the disconnect in the market on why
[00:41:16.560 --> 00:41:23.520]   investors are buying equities, given that we're likely going to have persistent higher rates,
[00:41:23.520 --> 00:41:27.200]   and as a result, multiples should compress, particularly in growth stocks,
[00:41:27.200 --> 00:41:34.240]   and why the NASDAQ is kind of firing up? I think it's important to look at what
[00:41:34.240 --> 00:41:39.200]   stocks are actually lifting the NASDAQ, just like what stocks are lifting the S&P 500. And
[00:41:39.840 --> 00:41:43.840]   we've seen this. There's a massive dispersion. There's like seven or eight companies that matter
[00:41:43.840 --> 00:41:50.560]   and 500-odd companies that don't matter much. And so the reason why things go up in an environment
[00:41:50.560 --> 00:41:55.120]   like that is, irrespective of the economy, money managers are paid to deploy money.
[00:41:55.120 --> 00:41:59.840]   And I don't know if you guys saw this, but Nick, maybe you can find it. I linked to it in my
[00:41:59.840 --> 00:42:05.520]   newsletter last Sunday, which was an article about the fact that the global amount of total
[00:42:05.520 --> 00:42:12.000]   available equities to buy is shrinking, right? So if you think about that, what that means is that
[00:42:12.000 --> 00:42:18.480]   the number of dollars is growing, right? Linearly, and in some years, it seems exponentially,
[00:42:18.480 --> 00:42:24.560]   but then the end, so then a certain percentage of that finds its way into the equity markets.
[00:42:24.560 --> 00:42:28.160]   But when the equity markets continue to shrink, and there's fewer and fewer things to buy,
[00:42:28.160 --> 00:42:33.520]   prices will still go up, irrespective of the underlying justification.
[00:42:33.520 --> 00:42:38.240]   So I think that the thing is, you can't look at these things as a sign per se. You know,
[00:42:38.240 --> 00:42:41.840]   if you look at commodities as a different example, Nick, I sent you this.
[00:42:41.840 --> 00:42:47.440]   Commodities are up, broadly speaking, 30% since the beginning of the year.
[00:42:47.440 --> 00:42:57.040]   I think the reality is the following. Countries are feeling increasingly uncomfortable
[00:42:58.480 --> 00:43:04.080]   with the decisions that the United States are making, whether that's its position to keep
[00:43:04.080 --> 00:43:08.640]   funding the war in Ukraine, whether that it's the continued instability in the Middle East,
[00:43:08.640 --> 00:43:14.560]   whether it's posturing with China, whatever it is. And so every country is now making a very
[00:43:14.560 --> 00:43:18.320]   different calculation about what they want to do. And I think how it gets reflected is that
[00:43:18.320 --> 00:43:23.920]   they're optimizing for their own economies. And what that yields is each of them trying to make
[00:43:23.920 --> 00:43:30.560]   as much money as they can in the short term. And as a country that is a net importer, we are going
[00:43:30.560 --> 00:43:35.760]   to feel the price of those decisions as inflation. And I think Sachs is right. Like, that's where we
[00:43:35.760 --> 00:43:42.000]   have to start figuring out how we rebalance these decisions from a foreign policy and economic
[00:43:42.000 --> 00:43:45.920]   perspective. Because if you don't change something here, all of these countries are basically going
[00:43:45.920 --> 00:43:50.480]   to be like, "We're on our own. We're going to optimize for ourselves." These countries are
[00:43:50.480 --> 00:43:55.520]   basically voting in a way that says we are not being led in a way that we trust. And that
[00:43:55.520 --> 00:44:01.120]   unfortunately falls at the feet of the sitting president. Yeah. Well, look, I'm hopeful,
[00:44:01.120 --> 00:44:09.600]   as we all are, that work in the tech industry, that new tools and AI and other technologies would
[00:44:09.600 --> 00:44:15.600]   have a massive driver on productivity, which net-net could help even this out. That's the one
[00:44:15.600 --> 00:44:23.120]   last great hope. Sitting here, AI obviously viewed as being highly disruptive by non-technologists
[00:44:23.120 --> 00:44:28.800]   and deeply concerning, and particularly in DC. But what is that thing that Bill Gates says,
[00:44:28.800 --> 00:44:33.680]   things are overestimated in the short term and deeply underestimated in the long term? I think
[00:44:33.680 --> 00:44:39.760]   that that's what AI is. That's right. And so AI is a bunch of, no offense, probabilities and
[00:44:39.760 --> 00:44:45.200]   statistics today. Okay? And so we talked about this last time, so we should debunk this whole
[00:44:45.200 --> 00:44:49.440]   thing of like, there's some glowing brain somewhere that's capable of doing everything.
[00:44:49.440 --> 00:44:56.240]   Doesn't mean that we shouldn't underestimate it, being capable of this in five or 10 years,
[00:44:56.240 --> 00:45:01.360]   which is probably the window in which we have some form of AGI, but today it's not that. And so the
[00:45:01.360 --> 00:45:05.360]   great leaps in productivity, Freeberg, that you're talking about are probably a three to five year
[00:45:05.360 --> 00:45:11.200]   phenomenon, not a three to five months. So what SAC says is right, which is how does the sitting
[00:45:11.200 --> 00:45:14.480]   administration buy votes today? Stimulatory methods.
[00:45:14.480 --> 00:45:19.360]   It'll have to make a ton of promises. The problem is it can't really work its way into the economy
[00:45:19.360 --> 00:45:25.040]   fast enough come the November election. And so that's where I think they're in a really tough
[00:45:25.040 --> 00:45:31.280]   spot, which means that then, and I hate to be the bearer of bad news, but this is where you have a
[00:45:31.280 --> 00:45:37.040]   lot of these wag the dog kind of scenarios that come to mind, because then the only way to really
[00:45:37.040 --> 00:45:41.920]   galvanize a populace is to distract them. Yeah. Well, look, let's talk a little bit
[00:45:41.920 --> 00:45:49.840]   more about AI, because I do think that regardless of the near term and longer term implications,
[00:45:49.840 --> 00:45:55.520]   there is a peaking of fear, peaking of interest, both different spelling of the word peak.
[00:45:56.480 --> 00:46:01.600]   Adam Schiff yesterday proposed a bill, sorry, earlier this week called the Generative AI
[00:46:01.600 --> 00:46:08.960]   Copyright Disclosure Act, in response to, I think, over 100 musicians signing a letter
[00:46:08.960 --> 00:46:13.920]   saying they're concerned that their copyrighted works are being used to train AI models.
[00:46:13.920 --> 00:46:18.960]   And Nick, if you'll pull up the text from the bill that I pulled out here real quick,
[00:46:18.960 --> 00:46:24.000]   we can take just a quick look. It's a very short bill. It's only five pages long.
[00:46:24.000 --> 00:46:29.600]   And it defines a generative AI model that's a combination of computer code and numerical
[00:46:29.600 --> 00:46:35.760]   values designed to use artificial intelligence to generate outputs in the form of text, images,
[00:46:35.760 --> 00:46:42.320]   audio, or video, and that it substantially incorporates one or more models as a system,
[00:46:42.320 --> 00:46:48.000]   and it's designed for use by consumers. And what the bill then asks is that anyone developing
[00:46:48.000 --> 00:46:55.840]   these models has to submit to a register run by the federal government, a list of all of the data
[00:46:55.840 --> 00:47:02.160]   that they use that might be copyrighted to train the model and that this becomes a precursor to
[00:47:02.160 --> 00:47:07.200]   being able to legally develop and use AI models that you actually have to register the training
[00:47:07.200 --> 00:47:13.360]   data. So I have a rant on this, which I'm going to preserve for the end given my rights as moderator
[00:47:13.360 --> 00:47:19.280]   here today. But I'd love the reaction from Sachs first on Schiff's proposed bill to moth. And we'll
[00:47:19.280 --> 00:47:23.840]   run this one around the horn pretty quick. Well, first of all, I think it's too soon for this
[00:47:23.840 --> 00:47:28.480]   legislation. I mean, the first thing that needs to happen is we need to get the question of fair
[00:47:28.480 --> 00:47:34.640]   use arbitrated by the courts. We've talked about this before. People publish lots of information
[00:47:34.640 --> 00:47:39.600]   on the internet. And I guess technically, it's copyrighted, but it's been published
[00:47:39.600 --> 00:47:44.880]   in a completely public way. And humans are obviously allowed to read it and use it to
[00:47:44.880 --> 00:47:49.120]   generate their own work. I think there's a question about whether AIs are allowed to
[00:47:49.120 --> 00:47:54.400]   learn from it as well. And I would argue that they should be to some degree under fair use.
[00:47:54.400 --> 00:47:59.120]   Maybe if you're talking about copyrighted songs, I don't know if that's different. Anyway,
[00:47:59.120 --> 00:48:04.720]   these questions need to be arbitrated. And once we know that, then we'll know whether there's a
[00:48:04.720 --> 00:48:09.760]   need for a rights clearinghouse of some kind, which may not be a bad idea. I actually got pitched
[00:48:09.760 --> 00:48:19.040]   by a founder recently to create a rights marketplace with copyright holders on the
[00:48:19.040 --> 00:48:24.960]   one side or rights holders could be musicians, could be movie studios, could be anyone who owns
[00:48:24.960 --> 00:48:28.880]   content, and then AI companies on the other that want to license that content for
[00:48:29.440 --> 00:48:35.760]   training purposes. And I actually thought it was a pretty good idea. So my point is just that
[00:48:35.760 --> 00:48:40.320]   there are entrepreneurs working on solving this problem as well. And I guess what I'm saying is,
[00:48:40.320 --> 00:48:45.520]   I'd like to see fair use figured out. And I'd like to see what the private solutions are going to be
[00:48:45.520 --> 00:48:52.320]   before we try to legislate this problem away from really a point of ignorance, because we're so
[00:48:52.320 --> 00:48:57.040]   early in the development of this market. Now, I think one of the questions you have to ask here
[00:48:57.040 --> 00:49:03.040]   is, why did Adam Schiff introduce this bill? And who is Adam Schiff? Adam Schiff is a powerful
[00:49:03.040 --> 00:49:09.840]   California congressman who is the candidate for US Senate right now. And he's expected to win.
[00:49:09.840 --> 00:49:13.040]   He's ahead of Steve Garvey, who's the Republican candidate by like 30 points.
[00:49:13.040 --> 00:49:20.880]   Schiff is, in addition to being a vicious partisan who's known for for Russiagate,
[00:49:21.840 --> 00:49:28.080]   he also is a prodigious fundraiser. And when he's not in front of the cameras, he's very effective
[00:49:28.080 --> 00:49:34.400]   in closed doors appealing to the key special interests in California that matter. And my
[00:49:34.400 --> 00:49:38.480]   guess is that he's not carrying water here for the 100 musicians who signed that letter.
[00:49:38.480 --> 00:49:44.160]   He's carrying water for the big studios, the big Hollywood studios, including music labels,
[00:49:44.160 --> 00:49:49.520]   but also the movie studios. And he's an LA based congressman. Yeah, I mean, that's who he's
[00:49:49.520 --> 00:49:54.800]   representing. Now, at the same time, my guess is that he's run this by the big tech companies,
[00:49:54.800 --> 00:49:59.200]   and he's probably got their blessing to some degree, because like Bill Gurley always points out,
[00:49:59.200 --> 00:50:04.480]   they like rules like this, because of regulatory capture, they can comply with them. But the little
[00:50:04.480 --> 00:50:08.480]   guy, the entrepreneur who's trying to create something new has a much harder time
[00:50:08.480 --> 00:50:13.920]   complying with these regulations. So this is basically a big special interest group
[00:50:13.920 --> 00:50:17.840]   sort of deal that I think he's he's putting forward.
[00:50:17.840 --> 00:50:19.440]   Do you think this becomes law?
[00:50:19.440 --> 00:50:22.560]   Not yet, but it's paving the way for something.
[00:50:22.560 --> 00:50:27.200]   And it's more like his promotional effort during his campaign effectively.
[00:50:27.200 --> 00:50:27.920]   Well, I mean,
[00:50:27.920 --> 00:50:32.160]   his move it legislatively, but
[00:50:32.160 --> 00:50:38.320]   yeah, I mean, the big LA powers that be like this, and they love this. Yeah, yeah.
[00:50:38.320 --> 00:50:42.160]   Any reaction to this bill?
[00:50:43.360 --> 00:50:48.080]   I think there was an article in The Verge that I posted.
[00:50:48.080 --> 00:50:54.000]   It turns out that OpenAI transcribed over a million hours of YouTube videos to train GPT-4.
[00:50:54.000 --> 00:51:00.480]   My takeaway when I read this was, well, if Google's not going to sue OpenAI,
[00:51:00.480 --> 00:51:08.240]   then this is a moot point. Because you're talking about one of the most valuable companies in the
[00:51:08.240 --> 00:51:14.480]   world with one of the most valuable sources of training data in the world. And so if that if
[00:51:14.480 --> 00:51:20.400]   it's true that there was a copyright violation, and Google just lets us go, what's anybody else
[00:51:20.400 --> 00:51:25.760]   supposed to do? What chance does these group of singer songwriters have? And you know, no offense
[00:51:25.760 --> 00:51:32.080]   to Adam Schiff, but he's not going to be able to do anything. So if this multi trillion dollar
[00:51:32.080 --> 00:51:36.640]   company doesn't draw a line in the sand, then none of this matters. And everybody's going to
[00:51:36.640 --> 00:51:44.080]   train on everything. I think, like, my big takeaway on all this is that there's a real
[00:51:44.080 --> 00:51:57.120]   deep misunderstanding on what these models really are, and what they're doing. The assumption,
[00:51:57.120 --> 00:52:03.760]   I think, naively, is that they take in a bunch of data, and then they spit that data back out.
[00:52:06.080 --> 00:52:11.680]   And in some cases, maybe slightly transformed. But the truth is that the models don't actually
[00:52:11.680 --> 00:52:19.040]   hold the data that they're trained on. They develop a bunch of synthesized predictors,
[00:52:19.040 --> 00:52:25.440]   that they learn what the prediction could or should be from the data. And this is similar
[00:52:25.440 --> 00:52:32.720]   to how humans operate. Can you pull up the article, Nick that I posted? This article is from last
[00:52:32.720 --> 00:52:37.120]   May. And you guys may remember this case. I can't remember if we talked about it on the show.
[00:52:37.120 --> 00:52:46.240]   But Ed Sheeran was sued by the estate of the writer of the Marvin Gaye song, Let's Get It On,
[00:52:46.240 --> 00:52:53.040]   for infringement in his song Thinking Out Loud. And he actually prevailed in court,
[00:52:53.040 --> 00:52:58.640]   where he went through with the jury, how he comes up with how he runs his creative process,
[00:52:58.640 --> 00:53:04.560]   how he came up with the song, and how he does his work. And as you know, Ed Sheeran,
[00:53:04.560 --> 00:53:10.640]   and nearly every musician or every artist listens to other people's music, looks at other people's
[00:53:10.640 --> 00:53:15.280]   art, and they synthesize that data, they synthesize that knowledge and that experience
[00:53:15.280 --> 00:53:24.240]   into their own creative process to output what in their mind is a novel creation. And I think that
[00:53:24.240 --> 00:53:29.280]   AI works in a similar vein in that it's not actually storing the images, it's not actually
[00:53:29.280 --> 00:53:36.560]   storing the music of third parties. It's learning from that process. We all like musicians learn on
[00:53:36.560 --> 00:53:41.600]   classical music, they listen to and play other pop artists music. And then they learn things that
[00:53:41.600 --> 00:53:46.720]   they like things that they don't like things that fit together well, certain intonation,
[00:53:46.720 --> 00:53:52.720]   syncopations, and that's how they develop their own tracks. And so I think that the assumption
[00:53:52.720 --> 00:53:58.800]   in AI is that it's almost guilty until proven innocent is what's going to really challenge this
[00:53:58.800 --> 00:54:04.880]   technology, getting mainstay appeal and being useful in the way that it could and should be
[00:54:04.880 --> 00:54:12.320]   that the de facto is that it's learning and training is copyright. I don't I don't buy that.
[00:54:12.320 --> 00:54:14.880]   I don't buy that. No, let's use the data.
[00:54:14.880 --> 00:54:20.640]   No, that that just because you're learning like a human, it's not copyright infringement. And
[00:54:20.640 --> 00:54:24.480]   I'm sorry, let me let me make my last statement, which is that I do think that similarity is
[00:54:24.480 --> 00:54:29.440]   measurable. And that is copyright infringement. So I would rather than focus on how you train
[00:54:29.440 --> 00:54:35.840]   the model, I would encourage legislators to find, as Saks points out, a better process
[00:54:35.840 --> 00:54:42.000]   to define what makes one thing distinct from another, run that analysis, and then use that
[00:54:42.000 --> 00:54:47.360]   analysis to determine uniqueness. And that analysis is ultimately what should determine whether
[00:54:47.360 --> 00:54:52.720]   there is copyright infringement versus the how was the model trained exercise, which is wrought
[00:54:52.720 --> 00:54:56.240]   with challenges, as you guys know, how do you determine weightings? How do you determine
[00:54:56.240 --> 00:55:00.800]   whether this was even relevant in the model? We don't even know if the models ignore the data or
[00:55:00.800 --> 00:55:05.680]   use the data in their synthesis or in their weightings. It's all very opaque. And so it's
[00:55:05.680 --> 00:55:09.760]   almost like, hey, here's a list of stuff that I read. But I may not have even paid attention to
[00:55:09.760 --> 00:55:15.280]   95% of it. When I was reading it. We don't know. And so I'd rather focus on is there copyright
[00:55:15.280 --> 00:55:21.280]   infringement in the output? Does the output mimic an original copyrighted source? And if it does,
[00:55:21.280 --> 00:55:25.520]   then that's where the infringement should lie, not on the How did you do it?
[00:55:25.520 --> 00:55:30.000]   I think that you're creating an impossible standard. Is it 5%? The same? Is it 8%? The same
[00:55:30.000 --> 00:55:34.480]   is that 14%? Where's your threshold? Is it different than training? How's that different
[00:55:34.480 --> 00:55:40.000]   than training? If I'm JK Rowling? Am I not allowed to read other authors copyrighted books for fear
[00:55:40.000 --> 00:55:45.520]   that by reading it, I will now use their copyrighted material in my novel book? No. Okay,
[00:55:45.520 --> 00:55:49.520]   stick with that example. But we can also use the Ed Sheeran example. Nick, can you play this video?
[00:55:49.520 --> 00:55:53.600]   I could play you every song in the pop charts right now over four chords. You name any song
[00:55:53.600 --> 00:56:00.240]   and I'll play it in the four chords. So these, these are the chords. Messenger. Your friend
[00:56:00.240 --> 00:56:07.040]   because you only need the light when it's burning low. Only miss the song when it starts to snow.
[00:56:07.040 --> 00:56:11.040]   Only know your lover when you let it go. Let it be. I'm walking away Craig David.
[00:56:11.040 --> 00:56:18.880]   Greg David. I'm walking away from the troubles in my life. Last one. Let it be Beatles.
[00:56:18.880 --> 00:56:28.080]   When I find myself in times of trouble. Why is that important? Obviously, it's not illegal for
[00:56:28.080 --> 00:56:33.040]   Ed to have learned these four chords. But if he tries to produce those songs and calls them
[00:56:33.040 --> 00:56:39.520]   something else, that's clearly somebody else's IP. Similarly, I think that if you don't give
[00:56:39.520 --> 00:56:44.000]   people the right to say, yeah, of course, the letters A, B, C, D, E, F, G, all the way to Z
[00:56:44.000 --> 00:56:48.080]   are not my invention. It's the alphabet. I don't control that. I don't control words and their
[00:56:48.080 --> 00:56:52.400]   meanings. But when I put one word in front of another in front of another purposefully, and I
[00:56:52.400 --> 00:56:57.760]   create something that has value to me, I should have the right to protect that. And I think that's
[00:56:57.760 --> 00:57:02.880]   the fundamental question. Should you not have a right to protect it? Should you allow somebody
[00:57:02.880 --> 00:57:09.600]   else to take that? And in some opaque way, and undefinable way, make a system that could compete
[00:57:09.600 --> 00:57:14.480]   with that thing and have no repercussions. I think that that's what people need to decide.
[00:57:14.480 --> 00:57:19.600]   And I'm fine, by the way, one way or the other. Yeah, in the Ed Sheeran case, he was saying the
[00:57:19.600 --> 00:57:25.040]   exact copyrighted lyrics. And he was singing the exact copyrighted melody. He was just playing the
[00:57:25.040 --> 00:57:30.400]   harmony with four chords. I mean, that was kind of the point he was making. And in the case of
[00:57:30.400 --> 00:57:35.360]   all the copyrighted material, you're stating, I want to protect it from showing up in the end
[00:57:35.360 --> 00:57:40.000]   product of someone's creative process. I shouldn't care about them listening to my song in their
[00:57:40.000 --> 00:57:44.800]   creative process. Everyone always references how Oasis sounds like the Beatles, right? Those guys
[00:57:44.800 --> 00:57:48.960]   love the Beatles, they listen to the Beatles, so much of their music mimics and sounds like the
[00:57:48.960 --> 00:57:51.680]   Beatles, but the Beatles aren't going to file infringement suits against them. They were
[00:57:51.680 --> 00:57:56.240]   inspired by the Beatles, that was part of their creative process. And so I would argue that the
[00:57:56.240 --> 00:58:00.240]   output should really be where we assess this stuff. And it's going to take a lot of cases,
[00:58:00.240 --> 00:58:06.000]   you're creating an impossible standard where no any individual person will have the economic
[00:58:06.000 --> 00:58:12.960]   wherewithal to fight this. And so it's basically seeding all creativity to the board. And I don't
[00:58:12.960 --> 00:58:18.160]   get what is what is the standard of measuring how someone is training their creative process?
[00:58:18.160 --> 00:58:22.240]   Your standard is I got to go I don't think I got to go listen, I got to go you got to tell me every
[00:58:22.240 --> 00:58:28.320]   time you as an artist, listen to a song, and then I'm going to determine whether or not you are
[00:58:28.320 --> 00:58:32.640]   listening to copyright music and making your own music. That's basically what we're asking for here
[00:58:32.640 --> 00:58:36.880]   with this. That's not what I don't I'm not talking about the bill. I'm talking about this idea that
[00:58:36.880 --> 00:58:42.800]   if you train a system that can actually automate a task for you, based on some learning that's
[00:58:42.800 --> 00:58:49.120]   unique, you should be able to uniquely protect your ownership. For example, we have a different
[00:58:49.120 --> 00:58:53.840]   set of ways in order to protect processes that are unique to you, which is, you can have a patent,
[00:58:53.840 --> 00:58:59.440]   or you can have a trade secret. But if you strip all of this stuff away, where some system can in
[00:58:59.440 --> 00:59:06.000]   an automated way do anything for you, and replicate a whole bunch of work that it sits on top of to be
[00:59:06.000 --> 00:59:11.040]   able to do that without any repercussions. I think that that's a very important explicit decision.
[00:59:11.040 --> 00:59:15.680]   And what I'm saying is, you cannot expect individual people to go and fight these little
[00:59:15.680 --> 00:59:21.040]   battles against these big multi hundred billion dollar companies. If the big multi hundred billion
[00:59:21.040 --> 00:59:25.280]   dollar companies and trillion dollar companies don't fight the other multi hundred billion dollar
[00:59:25.280 --> 00:59:29.120]   and trillion dollar companies, what they're effectively saying is none of this is worth
[00:59:29.120 --> 00:59:33.680]   anything. Copyrights are worth nothing. And I'm okay with that decision. But it should be an
[00:59:33.680 --> 00:59:40.240]   explicit one. And it's not going to happen the other way where some five page law from a guy
[00:59:40.240 --> 00:59:44.400]   who's trying to get elected is not the thing that's going to make the difference. The difference is,
[00:59:44.400 --> 00:59:49.840]   if Google doesn't sue open AI, it means copyright is worthless. And by the way, that article didn't
[00:59:49.840 --> 00:59:54.320]   just point the finger at Google. It pointed the finger at meta where there were explicit decisions
[00:59:54.320 --> 00:59:59.120]   to say, Yeah, no, let's just violate the copyright. It's, it's okay. And if it's okay, then it's okay.
[00:59:59.120 --> 01:00:05.200]   Saks, you're the independent arbiter here. Any summary?
[01:00:05.200 --> 01:00:09.360]   Well, somebody's gonna litigate it. Somebody's gonna get gonna litigate it. You're right,
[01:00:09.360 --> 01:00:14.320]   it'd be surprising if Google didn't want to, given how valuable their YouTube content is,
[01:00:14.320 --> 01:00:20.240]   and a competitor, you know, slurped it up to train their model. But somebody will will litigate this,
[01:00:20.240 --> 01:00:25.760]   it could be the songwriters, who knows. And then we're gonna we're gonna get some messy arbitration
[01:00:25.760 --> 01:00:29.760]   around fair use, and it's probably gonna work its way up over the different circuit courts,
[01:00:29.760 --> 01:00:34.000]   you'll probably get different judgments. And finally, the Supreme Court will resolve it.
[01:00:34.000 --> 01:00:38.560]   And then we'll kind of know where things stand. And then and then there'll be a legislative fix.
[01:00:39.120 --> 01:00:42.880]   I don't know where it's going to end up. I mean, I know what Jay cow would say if he were here,
[01:00:42.880 --> 01:00:47.520]   I'm sure he's like chomping at the bit be part of this conversation. He would say that creators
[01:00:47.520 --> 01:00:53.440]   deserve to get paid for their AI rights. And that if you want to make money training a model
[01:00:53.440 --> 01:00:59.440]   on their content, you got to compensate them. I also I buy I buy your argument too, which is
[01:00:59.440 --> 01:01:06.080]   you're not really violating their their rights for the model to
[01:01:06.880 --> 01:01:12.800]   to read their content and analyze it, which is what it's really doing as long as it doesn't
[01:01:12.800 --> 01:01:19.520]   output a violation of their copyright. And that's what all artists do. That's what all humans do.
[01:01:19.520 --> 01:01:25.680]   The creative process is one of initially synthesis. You learn from other people's works,
[01:01:25.680 --> 01:01:28.480]   you learn from the environment around you, you learn from the world that you're exposed,
[01:01:28.480 --> 01:01:32.160]   there was compensation in those other models. There was not you don't of course,
[01:01:32.800 --> 01:01:36.480]   Oasis never paid the Beatles to listen to their music. They
[01:01:36.480 --> 01:01:40.800]   know that's not true. They listened on the radio, they may not have paid,
[01:01:40.800 --> 01:01:45.920]   but some advertiser paid for an ad. The radio station had to pay ASCAP and BMI,
[01:01:45.920 --> 01:01:49.840]   there was publishing rights, there was all kinds of other rights. So that's not true what you're
[01:01:49.840 --> 01:01:54.800]   saying. I'm not what I'm what I'm saying is it's not copyright just because somebody got paid. So
[01:01:54.800 --> 01:01:59.440]   that singer songwriter did get paid, they may not have getting paid by the listener. But in this
[01:01:59.440 --> 01:02:03.920]   example, nobody's getting paid. Look, I think there's three potential outcomes here. Okay,
[01:02:03.920 --> 01:02:10.720]   outcome number one is that Freiburg wins. And the courts decide that this is fair use. And
[01:02:10.720 --> 01:02:15.760]   these AI companies aren't in violation of copyright infringement is measured in the work of art that
[01:02:15.760 --> 01:02:20.960]   is produced, not in the process by which it is generated. Right? Okay, so that's solution number
[01:02:20.960 --> 01:02:28.080]   one. I think solution or outcome number two is that the courts throw a big wrench into it or a
[01:02:28.080 --> 01:02:33.280]   bill like schist bill, which is largely punitive, throws a big wrench into it. And I think what
[01:02:33.280 --> 01:02:39.360]   happens then is new AI companies have a really hard time accumulating training data. And so the
[01:02:39.360 --> 01:02:44.080]   incumbents who already have all the training data, they may have to pay a fine, there'll be some slap
[01:02:44.080 --> 01:02:48.640]   on the wrist. But now you've created a real moat and barrier to entry. And so secretly, they're
[01:02:48.640 --> 01:02:52.400]   going to love it. And it's going to be a big problem, I think, for the open source movement.
[01:02:52.400 --> 01:02:58.800]   So that's outcome number two. And then outcome number three is you create some sort of rights
[01:02:58.800 --> 01:03:05.440]   clearinghouse, kind of like exists in the recording industry, where if you want to cover somebody's
[01:03:05.440 --> 01:03:10.720]   song or perform somebody else's song, there's already predefined terms on which you can use
[01:03:10.720 --> 01:03:15.360]   those rights. And then there's a compensation mechanism for that. So you don't have to go
[01:03:15.360 --> 01:03:20.560]   negotiate every single deal that you want to do, you can just use the song, you can use the rights,
[01:03:20.560 --> 01:03:25.360]   and there's again, compensation for that. That would be like the compromise, I could see that
[01:03:25.360 --> 01:03:31.520]   alternative number three existing, in which case, the incumbents probably like that, because
[01:03:31.520 --> 01:03:36.480]   they again, they want there to be a barrier to entry. They want to make it hard for the guy in
[01:03:36.480 --> 01:03:43.120]   the garage, to pony up the capital necessary to create a new model. But at least that would be a
[01:03:43.120 --> 01:03:48.400]   somewhat functional system, it wouldn't be outcome number two, which would totally put the kibosh on
[01:03:48.400 --> 01:03:52.240]   the use of training data. So in any event, I don't know which one of the three it's going to be. But
[01:03:52.240 --> 01:03:56.960]   those are the three potential outcomes I see happening here. Have you guys seen you do you dio?
[01:03:56.960 --> 01:04:07.520]   No, it's like a new music gen AI system. It is friggin unbelievable.
[01:04:07.520 --> 01:04:09.200]   Is that the notes thing or the notes another thing?
[01:04:09.200 --> 01:04:14.480]   I think the notes another thing this one just came out, it was apparently rumored for a while
[01:04:14.480 --> 01:04:20.160]   ex DeepMind employees started you do it's been this kind of like hot stealth company.
[01:04:20.160 --> 01:04:26.880]   For a couple of months, people have been talking about it as rumored to be the best music AI.
[01:04:26.880 --> 01:04:32.240]   But here is someone that said, Dune the Broadway musical, and this is what it output.
[01:04:32.240 --> 01:04:41.920]   Go ahead.
[01:04:41.920 --> 01:04:48.800]   That's crazy.
[01:04:48.800 --> 01:04:51.840]   You can hear the layers of music, the layers of the tracks,
[01:04:51.840 --> 01:04:57.600]   the smoothness, the tempo, obviously, like there's just so many layers to this.
[01:04:57.600 --> 01:05:00.320]   Nick, can you pull up another one? Some of these are just incredible,
[01:05:00.320 --> 01:05:07.040]   because it spans genres. It spans styles. It's like so beautiful, like the way it's put together
[01:05:07.040 --> 01:05:12.640]   every every piece feels somewhat emotional. It's unique. It really doesn't feel like you're just
[01:05:12.640 --> 01:05:24.640]   listening to someone printing copyright music. funky dance pop song. Create a birthday song
[01:05:24.640 --> 01:05:34.160]   for my friend urban. Create an Irish folk tune with violin and instrumental.
[01:05:34.160 --> 01:05:49.840]   It's pretty incredible. And I think it shows the state of the art is such now that this
[01:05:49.840 --> 01:05:53.840]   is going to become a real challenging question from a legislative point of view.
[01:05:54.240 --> 01:05:58.480]   Given how far ahead these technologies have gotten, and I think that musicians,
[01:05:58.480 --> 01:06:01.520]   artists, consumers are going to start to use these tools in a really
[01:06:01.520 --> 01:06:07.440]   prolific way, given how good they are now. I heard from someone that just sat for hours,
[01:06:07.440 --> 01:06:12.720]   creating tracks that they listened to on UDO the other day. And so you basically just playing your
[01:06:12.720 --> 01:06:16.640]   own music all day long. It's really incredible. Anyway, exciting time.
[01:06:16.640 --> 01:06:20.560]   Hopefully all the lawsuits and the legislators don't get in the way of all this innovation,
[01:06:20.560 --> 01:06:24.480]   because it's moving really fast. And we should just kind of see where it goes before trying to
[01:06:24.480 --> 01:06:31.440]   clamp down. Totally. Guys, I want to talk about, as you know, this like emerging trend that we've
[01:06:31.440 --> 01:06:36.800]   been talking about offline about drones in warfare. Recently, we saw small drones controlled
[01:06:36.800 --> 01:06:42.320]   by the Houthis attack cargo vessels in the Suez Canal. It led to rather large scale supply
[01:06:42.320 --> 01:06:47.840]   disruptions, economic value destruction. We've seen videos, sex, I think you have one, we can
[01:06:47.840 --> 01:06:53.760]   pull it up and take a look of Ukrainian controlled drones flying into destroy tanks and large
[01:06:53.760 --> 01:06:58.800]   warehouses in Russian held territories, causing 10s of millions of dollars of damage. I think
[01:06:58.800 --> 01:07:03.040]   we're seeing the acceleration of the changing face of warfare technology, lots of these small
[01:07:03.040 --> 01:07:09.280]   form factor, low cost autonomous physical agents. And I think there's a bunch of implications both
[01:07:09.280 --> 01:07:15.520]   with respect to war, but also how we spend money on defense, and how Silicon Valley is involved
[01:07:15.520 --> 01:07:20.080]   in response. tax, I'd love to hear your points of view. Maybe you can highlight for us some of the
[01:07:20.080 --> 01:07:25.200]   things you've been seeing in the utilization of these new technologies and in the war, given our
[01:07:25.200 --> 01:07:30.480]   your our in house work correspondent, and you obviously do some work in Silicon Valley. Go
[01:07:30.480 --> 01:07:36.320]   ahead. I've heard a lot about the impact of drones just by watching the coverage of the Ukraine war.
[01:07:36.320 --> 01:07:42.320]   I was listening to an interview with an American mercenary who's fighting on the side of Ukraine,
[01:07:42.320 --> 01:07:47.680]   and he described how ubiquitous these drones were on the battlefield. Now he said that you literally
[01:07:47.680 --> 01:07:53.840]   can't get out of a trench to go to the bathroom because a drone will basically get you. And he
[01:07:53.840 --> 01:07:57.840]   said they're buzzing around, they sound like mosquitoes, because they're kind of just everywhere
[01:07:57.840 --> 01:08:05.360]   on the battlefield. Both Ukraine and Russia have them. And several months ago going into this year,
[01:08:05.360 --> 01:08:08.400]   as long as he said that this would be the big game changer for Ukraine, they're going to make
[01:08:08.400 --> 01:08:12.880]   a million drones. I don't think it's worked out that way. It turns out that as many drones as
[01:08:12.880 --> 01:08:16.240]   Ukrainians have been able to put on the battlefield, the Russians have even more,
[01:08:16.240 --> 01:08:21.280]   because they're able to mass produce them in factories, they've got bigger drones, better
[01:08:21.280 --> 01:08:26.800]   drones. And I think the most important variable in the war so far, with respect to drones is that
[01:08:26.800 --> 01:08:32.160]   the Russians have pretty advanced electronic warfare. And so they've been able to jam a lot
[01:08:32.160 --> 01:08:38.560]   of the Ukrainian drones, whereas the reverse has not been true for the Ukrainians. So I would say
[01:08:38.560 --> 01:08:45.040]   that drones have been a huge factor in the war. But so far, the balance there is tipping towards
[01:08:45.040 --> 01:08:50.560]   the Russians as it is in so many other areas as well. But to your larger point, there's no
[01:08:50.560 --> 01:08:57.440]   question this is the future of warfare. And you're seeing that it's creating a lot of opportunities
[01:08:57.440 --> 01:09:04.560]   for asymmetric warfare. So for example, with the hoodies, they've been firing cheap missiles and
[01:09:04.560 --> 01:09:11.040]   drones at our at our ships in the Red Sea, and we've been having to spend $2 million air defense
[01:09:11.040 --> 01:09:16.000]   missiles shooting down $2,000 drones. So if that continues, and we don't have a good response to
[01:09:16.000 --> 01:09:20.240]   this problem, it's going to really change the balance of power. I don't know if this was at
[01:09:20.240 --> 01:09:24.640]   our summit, or if I heard this later from some senior person in the military, who said that
[01:09:24.640 --> 01:09:29.840]   aircraft carriers are outdated technology, like they don't make sense anymore. And we're going
[01:09:29.840 --> 01:09:38.080]   to see a shift towards lots of small, autonomous drone like systems on the battlefield taking out
[01:09:38.080 --> 01:09:44.720]   targets. And as a result, if you gain theory this out, the necessity for defense systems against
[01:09:44.720 --> 01:09:51.440]   large amounts of swarming drone like systems. And then, you know, what do you actually have to do
[01:09:51.440 --> 01:09:57.840]   with your existing military architecture to play into this kind of new tactical system?
[01:09:57.840 --> 01:10:04.400]   And then that's going to require a massive evolution in technology. And is the United
[01:10:04.400 --> 01:10:11.840]   States prepared? Should Silicon Valley play should be, we should I maybe Nick, you can throw up this
[01:10:11.840 --> 01:10:17.600]   the first link that I sent you. But eight years ago, I came to this conclusion. And I invested
[01:10:17.600 --> 01:10:22.720]   in this company called sail drone, which is autonomous drones in the seas, our customers
[01:10:22.720 --> 01:10:27.280]   include the US Navy, the chairman of our company actually is Admiral Mike Mullen, who is a former
[01:10:27.280 --> 01:10:32.800]   chairman of the Joint Chiefs of Staff. But these drones are capable of going into some of the most
[01:10:32.800 --> 01:10:37.680]   dangerous places in the world, and collecting enormous amounts of surveillance data and
[01:10:37.680 --> 01:10:45.680]   information that otherwise takes the United States government lots of time and lots of money,
[01:10:46.240 --> 01:10:53.840]   and is pretty scattershot. And instead with these things, you can be constantly in hotspots. And if
[01:10:53.840 --> 01:10:58.320]   you go to the next one, so much so that you know, we've deployed some of these drones in the Middle
[01:10:58.320 --> 01:11:03.440]   East, and there was a point at which the Iranian Navy intercepted two of our drones and pick them
[01:11:03.440 --> 01:11:09.120]   up. And this was kind of like a global thing a few years ago. But I really believe in this trend.
[01:11:09.120 --> 01:11:14.800]   And I think that we have an enormous responsibility to be funding these things. These are really
[01:11:14.800 --> 01:11:18.320]   complicated systems to build, obviously, and they take lots of time. So these are not
[01:11:18.320 --> 01:11:22.880]   overnight successes, Friedberg, and they take lots of money, which is hard to come by as well.
[01:11:22.880 --> 01:11:26.480]   But these are absolutely the right kinds of businesses, because eventually,
[01:11:26.480 --> 01:11:31.600]   at a minimum, you're building systems that can measure and collect enormous amounts of really
[01:11:31.600 --> 01:11:38.000]   critical data, so that people can make better decisions, which hopefully is measured in saving
[01:11:38.000 --> 01:11:43.920]   life, right. And then over time, you actually get to a military that should be much cheaper,
[01:11:44.720 --> 01:11:52.080]   much safer, and has fewer people in general on our side and on our enemy sides on the front lines,
[01:11:52.080 --> 01:11:57.040]   which means fewer casualties. And I think that that's the whole value of this entire movement.
[01:11:57.040 --> 01:12:04.480]   It's cheaper, it's way better. It's much more useful. And it gives the respective armies that
[01:12:04.480 --> 01:12:08.880]   use these things or militaries rather, a fuller picture so that you can make decisions that have
[01:12:08.880 --> 01:12:16.080]   enormous consequences. Yeah. And in February, the Department of Defense on a panel in Arlington,
[01:12:16.080 --> 01:12:22.640]   Virginia, briefed reporters and industry folks on their interest in growing their partnerships in
[01:12:22.640 --> 01:12:29.360]   Silicon Valley, to access the necessary technologies that are going to rewrite the
[01:12:29.360 --> 01:12:35.120]   face of warfare. I don't know about you guys, but I've seen kind of two sides of this in Silicon
[01:12:35.120 --> 01:12:41.840]   Valley. A good number of venture capitalists and entrepreneurs and technologists think that it's
[01:12:41.840 --> 01:12:48.480]   morally incorrect to support warfare technology. And then there are a few that are going blazing
[01:12:48.480 --> 01:12:53.360]   ahead with supporting this new evolution and these technologies. Do you guys see the same?
[01:12:53.360 --> 01:12:57.040]   And is there going to be kind of this? It's a big step to weaponize these things,
[01:12:57.040 --> 01:13:02.800]   right? So Saildrone, we have a very good, big, thriving, successful business. These
[01:13:03.440 --> 01:13:08.400]   machines aren't weaponized by any stretch of the imagination. And so I think that that
[01:13:08.400 --> 01:13:17.840]   comment is more genuflecting and virtue signaling than it is real. Because in order to even be
[01:13:17.840 --> 01:13:26.480]   legitimately considered for some kind of like armed, unmanned product, you have to be deep
[01:13:26.480 --> 01:13:31.280]   inside of the bowels of the DoD and Pentagon and have been working with them for years,
[01:13:31.280 --> 01:13:36.400]   don't even be taken seriously. So the startups that are like eschewing those deals are not even
[01:13:36.400 --> 01:13:42.400]   close to those deals. And the startups that are close to them are probably being much more quiet
[01:13:42.400 --> 01:13:47.120]   and thoughtful and hush hush, because the path to do that isn't legitimate unless you've been
[01:13:47.120 --> 01:13:53.440]   in business with these folks for 10 plus years. Yeah, I think it's, it's more commentary on the
[01:13:53.440 --> 01:13:59.920]   fact that if this is where an organization, the Department of Defense that has a trillion dollar
[01:13:59.920 --> 01:14:06.960]   annual budget is going to be allocating resources, that technology is going to need to come from
[01:14:06.960 --> 01:14:12.720]   somewhere. And the investors in Silicon Valley, that are making these investments are likely going
[01:14:12.720 --> 01:14:18.000]   to outperform those who are not. Does that sound reasonable to you, Sax, that there is this shift
[01:14:18.000 --> 01:14:24.160]   underway? And that, again, those who have shied away from defense technology are, you know,
[01:14:24.160 --> 01:14:28.240]   necessarily going to be left out of a, you know, kind of a new industry that's emerging in Silicon
[01:14:28.240 --> 01:14:34.240]   Valley? Well, I guess the way I put it is that the US government spends over $800 billion every
[01:14:34.240 --> 01:14:39.280]   year on defense. And it's not clear what we're getting for that money. Because in Ukraine,
[01:14:39.280 --> 01:14:42.800]   for example, we've run out of artillery shells, we've actually we've run out of patriots, we've
[01:14:42.800 --> 01:14:47.040]   run out of javelins, we're on a stinger. So we're spending all this money, we're not getting a lot
[01:14:47.040 --> 01:14:51.920]   for it. And part of the reason is because the defense industry is consolidated down to these
[01:14:51.920 --> 01:14:59.040]   five prime defense contractors who are basically an oligopoly. And we have this cost plus procurement
[01:14:59.040 --> 01:15:05.040]   system where they just raise their prices every year and the government pays it. So I think all
[01:15:05.040 --> 01:15:09.280]   of us want the United States to have an effective defense. I mean, I want us to use our military
[01:15:09.280 --> 01:15:13.360]   power more wisely. I don't like all these stupid wars we keep getting in. But I do want the United
[01:15:13.360 --> 01:15:20.720]   States as an American to be the most powerful country. I do want us to get the best value for
[01:15:20.720 --> 01:15:27.680]   our defense dollars. And the only way that's going to change is if the defense industry gets disrupted
[01:15:27.680 --> 01:15:33.280]   by a bunch of startups doing innovative things. And there's no question that I think drones are
[01:15:33.280 --> 01:15:38.160]   the future of warfare. To your point about autonomy, I think that is where this is going next
[01:15:38.160 --> 01:15:45.520]   is that right now, the drones are typically controlled by, you know, somebody with like a VR
[01:15:45.520 --> 01:15:53.200]   headset, tele operated, operated and their FPV, this first person vision drone, and you basically
[01:15:53.200 --> 01:15:58.960]   strap some sort of explosive on to it, and then you drive it into whatever your target is. Yeah.
[01:15:58.960 --> 01:16:05.680]   Those types of drones are easier to disrupt by electronic warfare, because if you can disrupt
[01:16:05.680 --> 01:16:12.160]   the signal from the tele operator, then the drone basically doesn't know what to do. And so you're
[01:16:12.160 --> 01:16:20.960]   right, the next step here is autonomous systems that can be programmed with a target and can find
[01:16:20.960 --> 01:16:25.200]   it on its own, make decisions on its own. And then they also build in some shielding against
[01:16:25.200 --> 01:16:31.040]   some of this like jamming technology or this EW electronic warfare technology.
[01:16:31.040 --> 01:16:35.120]   So that's where all these things, that's where the battlefield is of the future is headed.
[01:16:35.120 --> 01:16:41.040]   And, you know, in a weird way, if you think about humans becoming a smaller and smaller
[01:16:41.040 --> 01:16:46.160]   piece of the battlefield, and autonomous drones becoming a larger and larger piece of it,
[01:16:46.160 --> 01:16:51.200]   these wars become resource wars, and that's right, actually, technology wars.
[01:16:51.200 --> 01:16:53.600]   That's right, which which may or may not be good. I don't know.
[01:16:53.600 --> 01:16:56.800]   But by the way, so this is the point I wanted to make, if you pull up this chart,
[01:16:56.800 --> 01:17:03.440]   if that is where warfare is headed, is a large number of small autonomous systems
[01:17:03.440 --> 01:17:08.800]   that go and find a target and try and execute a mission. And, you know, in the case of Ukraine,
[01:17:08.800 --> 01:17:13.840]   saying they want to have a million, China might say, I want to have 100 million, all of these
[01:17:13.840 --> 01:17:18.640]   systems are dependent on lithium ion battery systems. Today, 79% of lithium ion battery
[01:17:18.640 --> 01:17:24.240]   production comes out of China. The US is only 6.2% of global lithium ion battery production.
[01:17:24.240 --> 01:17:31.280]   And China has talked about scaling up drone manufacturing to a level that the US simply
[01:17:31.280 --> 01:17:38.160]   cannot even contemplate in its industrial architecture today. So it seems to me that
[01:17:38.160 --> 01:17:44.400]   if that is where warfare tactically is headed, that China has a huge leg up, and is going to
[01:17:44.400 --> 01:17:50.720]   become a critical point of dependency for the United States to develop an arsenal necessary
[01:17:50.720 --> 01:17:56.000]   to be competitive in this this kind of next evolution of warfare. I will say that, like,
[01:17:56.000 --> 01:18:00.800]   if you then game theory this out, like how do you defend against these autonomous electronic systems,
[01:18:00.800 --> 01:18:07.440]   there's a technology called EMP or electromagnetic pulsing, where, as you guys know, if you run a
[01:18:07.440 --> 01:18:13.280]   very high current electric field, you can actually send out and emit a pulse that then when it hits
[01:18:13.280 --> 01:18:18.560]   electronic circuits far away, induces a high electric current in those circuits and short
[01:18:18.560 --> 01:18:26.560]   circuits them. So EMPs are a defense system that allow you to take out electronic systems. And this
[01:18:26.560 --> 01:18:32.800]   has been, you know, kind of a part of warfare, since probably the 1960s 50s, but targeted EMPs
[01:18:32.800 --> 01:18:37.520]   and targeted systems for eliminating all of these autonomous systems becomes a new defense technology
[01:18:37.520 --> 01:18:43.920]   that I know several startups in Silicon Valley that have been funded with a lot of capital
[01:18:43.920 --> 01:18:48.160]   by folks that we all know very well, that are kind of pursuing this,
[01:18:48.160 --> 01:18:53.360]   we don't have a choice. Because I think the point is that if you just make me just post this photo,
[01:18:53.360 --> 01:18:57.520]   we have an enormous human capital problem with the military, which is there's just not enough
[01:18:57.520 --> 01:19:01.760]   folks enlisting anymore. So we don't have any choice except to automate and become
[01:19:02.720 --> 01:19:07.440]   drone dependent. Just tell us these numbers. This is a chart from the Department of Defense
[01:19:07.440 --> 01:19:15.040]   that shows military enlistments looking back from about the 1970s, through today. And these are two
[01:19:15.040 --> 01:19:20.160]   lines that are just going from the upper left to the lower right. And so we're at all time lows
[01:19:20.160 --> 01:19:25.280]   with respect to the number of people that actually want to join the military. And so if we are
[01:19:25.280 --> 01:19:32.240]   supposed to, as Zach said, be this very sharp fighting force, then all of the money that we're
[01:19:32.240 --> 01:19:38.400]   spending needs to get allocated into things that can be remotely operated. We don't have a choice.
[01:19:38.400 --> 01:19:42.560]   Yeah. I mean, so Freeberg, you asked the question, how do we defend against these drones?
[01:19:42.560 --> 01:19:48.480]   Yes, there's EMP technology. If you want to use an EMP, you better make sure there's not any planes
[01:19:48.480 --> 01:19:51.120]   nearby, because it will take them out of the sky, you better make sure.
[01:19:51.120 --> 01:19:55.440]   By the way, there are now, you can target EMPs in a narrow cone, which is part of the
[01:19:55.440 --> 01:19:56.640]   technology evolution.
[01:19:56.640 --> 01:19:59.920]   Yeah, you better make sure you don't hit any of your own stuff, because you'll fire the electronics
[01:19:59.920 --> 01:20:04.960]   on those unless you EMP harden them, which by the way, will happen. I mean, if the US gets really
[01:20:04.960 --> 01:20:12.640]   good at sending out EMP pulses, then our opponents will start EMP hardening their drones. But anyway,
[01:20:12.640 --> 01:20:18.720]   yes, it's a category of defense. So is electronic warfare. The one defense investment I've made is
[01:20:18.720 --> 01:20:23.920]   actually in this idea of how do you defend against drones. And it's a startup where I
[01:20:23.920 --> 01:20:27.600]   led the seed round called Allen Control Systems, and they have a product called Bullfrog,
[01:20:27.600 --> 01:20:34.800]   which is a gun turret that uses a standard M240 machine gun. But it's got a scanner next to it,
[01:20:34.800 --> 01:20:42.560]   next to the machine gun that is searching the sky for enemy drones. And it uses computer vision to
[01:20:42.560 --> 01:20:47.680]   recognize them, and then it just shoots them down like very quickly. And it seems to me this is,
[01:20:47.680 --> 01:20:51.360]   it's maybe not the only way of doing it, but it's a really good way of doing it because
[01:20:51.360 --> 01:20:56.960]   you can mount these things to a vehicle. I mean, imagine in the future,
[01:20:56.960 --> 01:21:03.600]   that we have these autonomous drones everywhere that are basically assassination drones.
[01:21:03.600 --> 01:21:06.640]   How are you going to stop them? I mean, how do you move the President of the United States around?
[01:21:06.640 --> 01:21:11.520]   I mean, in his caravan, you're going to have to have, you know, his caravan is going to have to
[01:21:11.520 --> 01:21:19.440]   have some sort of electronic warfare system on it to basically prevent drones, but it's gonna need
[01:21:19.440 --> 01:21:23.360]   some sort of last line of defense where you can actually shoot a drone out of the sky.
[01:21:23.360 --> 01:21:25.120]   Or he stays in his basement.
[01:21:25.120 --> 01:21:29.920]   So Sax, you don't have any moral trepidation around investing in defense technology,
[01:21:29.920 --> 01:21:30.720]   just to be clear?
[01:21:30.720 --> 01:21:35.360]   Well, you know, I'd have to think twice about investing in a weapon. This is more of a defense
[01:21:35.360 --> 01:21:39.760]   technology. This is defensive in nature where you're trying to take out a drone that would
[01:21:39.760 --> 01:21:46.240]   be attacking one of our bases or installations or convoys. So this is fundamentally defensive
[01:21:46.240 --> 01:21:52.800]   in nature. And look, I want America to be the most powerful country. I'm an American, I'm patriotic.
[01:21:52.800 --> 01:21:59.040]   So I don't want our troops to be hit by enemy drones. So no, I don't have any moral problem
[01:21:59.040 --> 01:22:04.080]   with this. I do have a problem with all the stupid wars the United States gets into,
[01:22:04.080 --> 01:22:09.600]   but that's on a policy level. I feel like that's separate from the question of do we want our guys
[01:22:09.600 --> 01:22:11.440]   to be protected? Yes, we do.
[01:22:11.440 --> 01:22:15.040]   And Chamath, you don't care, you would invest in defense technology companies.
[01:22:15.040 --> 01:22:19.360]   No, I care. I think that I would probably have to think twice about making weapons. I'm not sure
[01:22:19.360 --> 01:22:23.600]   that I'm really interested in that. But like I said, eight years ago, my thought was we need to
[01:22:23.600 --> 01:22:29.040]   move to just looking at those trends, we need to move to an entire surface area of unmanned
[01:22:29.040 --> 01:22:37.040]   autonomous vehicles. And I really liked this idea of a class of machine that's there to surveil and
[01:22:37.600 --> 01:22:40.640]   collect all the important data so that you can make better decisions. And
[01:22:40.640 --> 01:22:44.400]   it's taken us eight years, but we're firmly entrenched now.
[01:22:44.400 --> 01:22:50.160]   I think the fact that both of you highlight a degree of trepidation, and you guys are like
[01:22:50.160 --> 01:22:56.320]   fairly outside of the middle bounds of how Silicon Valley would think and make decisions indicates
[01:22:56.320 --> 01:23:00.480]   like why so much of Silicon Valley has deep concerns about investing in defense technology.
[01:23:00.480 --> 01:23:07.360]   And the US, despite having the best technology, may be challenged by the fact that our Silicon
[01:23:07.360 --> 01:23:17.200]   Valley technologists and investors are held back on pursuing these evolutions because
[01:23:17.200 --> 01:23:22.320]   of a moral suasion. And I think it is something that the Department of Defense has highlighted.
[01:23:22.320 --> 01:23:28.000]   No, I want to be clear though, it's never like we've been faced with that choice. And the reality
[01:23:28.000 --> 01:23:35.680]   is like, if the choice is stay small, or scale up and be really big, you know, it's hard to
[01:23:36.800 --> 01:23:41.840]   turn away from that decision. But that choice isn't a realistic one. This is why I was saying,
[01:23:41.840 --> 01:23:46.320]   I think it's, you can come into it with the moral framing. But I do think that Sachs is right,
[01:23:46.320 --> 01:23:51.040]   which is, if you believe that America should be the most important country in the world,
[01:23:51.040 --> 01:23:56.880]   and you have the ability to help it be that, eventually, some of those decisions will
[01:23:56.880 --> 01:24:04.320]   come in contact with this kind of decision. And usually, though, it takes many, many years before
[01:24:04.320 --> 01:24:13.280]   that's realistic. And to be honest, from what I've seen, the opportunity to pursue that is
[01:24:13.280 --> 01:24:17.520]   generally so financially remunerative that everybody's like, Okay, I think we have to do
[01:24:17.520 --> 01:24:22.160]   you know, I don't have a problem with investing in a company like and roll. I mean, America needs
[01:24:22.160 --> 01:24:26.640]   defense companies in order to maintain its defense. And the problem right now that we have
[01:24:26.640 --> 01:24:33.200]   is there's these five prime companies that are antiquated and really expensive. And we need
[01:24:33.200 --> 01:24:37.680]   startup disruptors to that. So I'm not like, morally opposed to it. And in fact, I think it's
[01:24:37.680 --> 01:24:41.920]   probably a good thing. I think my concern would be, I don't really want to be part of the military
[01:24:41.920 --> 01:24:48.000]   industrial complex. I feel like it might, like, corrupt my views on things like I don't want to
[01:24:48.000 --> 01:24:52.400]   know you want war, you'd be girding for Yeah, like, I don't want to be I don't want to feel
[01:24:52.400 --> 01:24:57.200]   that's your dimension. That's Yeah, right. I don't want to feel an incentive to be like these
[01:24:57.200 --> 01:25:02.960]   generals who go on CNN and Fox News and justify every single war. That's my biggest fear as well.
[01:25:02.960 --> 01:25:07.520]   It's like, it's one thing to be investing in a bunch of companies where we're sitting around
[01:25:07.520 --> 01:25:12.160]   hoping for cheaper clicks on Facebook and Google. It's an entirely other thing to sit on the board
[01:25:12.160 --> 01:25:16.560]   of a company and hope for war. That's a horrible place to be in. On that point. That's a great
[01:25:16.560 --> 01:25:21.440]   point to wrap on, guys. This has been fun. A little more flat than it would normally be with
[01:25:21.440 --> 01:25:28.640]   Jay cow. We miss him. We wish him well, we hope his tooth recovers. And we will be back with your
[01:25:28.640 --> 01:25:32.880]   it's like that scene in Dune where he eats his tooth and releases that poison gas.
[01:25:32.880 --> 01:25:39.520]   That's right. Get better soon. J. cow. Please, if you're interested in the summit,
[01:25:39.520 --> 01:25:45.440]   summit that all in podcast.co. Click Subscribe on YouTube. I've been given notes to say that
[01:25:45.440 --> 01:25:51.440]   for the chairman dictator Chamath Palihapitiya, the rain man David Sachs, I am your Sultan of
[01:25:51.440 --> 01:26:09.040]   science day Friedberg. We will see you guys next week. We open source it to the fans and
[01:26:09.040 --> 01:26:32.720]   they've just gone crazy. We should all just get a room and just have one big huge orgy
[01:26:32.720 --> 01:26:36.000]   because they're all just useless. It's like this like sexual tension, but they just need
[01:26:36.000 --> 01:26:56.000]   to release it. I'm going all in. I'm going all in.
[01:26:56.000 --> 01:27:03.260]   [BLANK_AUDIO]

