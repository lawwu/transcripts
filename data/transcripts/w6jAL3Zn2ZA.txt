
[00:00:00.000 --> 00:00:02.880]   I mean, you know, it depends.
[00:00:02.880 --> 00:00:07.920]   Also, Kayla, can you post the YouTube link in the chat?
[00:00:07.920 --> 00:00:13.120]   Yep, will do. It is setting up right now.
[00:00:13.120 --> 00:00:20.320]   Nice.
[00:00:20.320 --> 00:00:29.440]   All right, so most of the people are going to join on YouTube because that's the link we pushed
[00:00:29.440 --> 00:00:35.040]   the most, but I do see some people who joined us here in Zoom. Hi folks, how are you doing?
[00:00:35.040 --> 00:00:41.280]   Do people want to share where they're joining from in the chat?
[00:00:41.280 --> 00:00:54.480]   Nice.
[00:00:55.440 --> 00:00:57.440]   I see a lot of West Coast people.
[00:00:57.440 --> 00:01:03.040]   I'm also, oh, someone from France. That's awesome.
[00:01:03.040 --> 00:01:11.600]   So I'm probably going to mess up your name, Matisse, but you're doing an amazing report for
[00:01:11.600 --> 00:01:18.320]   us, which I'm super excited about. Maybe you should come give a talk soon. That would be kind of cool.
[00:01:20.720 --> 00:01:27.600]   Also, for the folks who are joining us, thank you for coming. There's a Q&A button. For the folks
[00:01:27.600 --> 00:01:31.680]   who are on Zoom, there's a Q&A button. You can use that to ask questions. We have three talks
[00:01:31.680 --> 00:01:37.680]   for you today. Each will run about 30 minutes, after which you can ask questions. You can also
[00:01:37.680 --> 00:01:42.640]   post them in the chat. The folks who are joining us on YouTube, you can ask the questions either
[00:01:42.640 --> 00:01:48.720]   in the YouTube comments or in the YouTube chat, and we'll get to them. So if you have any questions,
[00:01:48.720 --> 00:01:53.520]   you can post them in the YouTube chat, and we'll get your questions and ask them.
[00:01:53.520 --> 00:02:01.120]   Kayla, how's the YouTube numbers? How are they doing? Can we start?
[00:02:01.120 --> 00:02:08.080]   >> We are, yeah, we've got four people watching right now. Did I, did we get that link?
[00:02:08.080 --> 00:02:11.120]   >> Yeah, let me share that link out to people.
[00:02:11.120 --> 00:02:13.120]   >> Okay.
[00:02:15.600 --> 00:02:18.960]   >> You can begin, and I'm going to start tweeting while Lucas introduces you guys.
[00:02:18.960 --> 00:02:27.040]   >> All right. Sounds good. Thanks, Lwanya, and thanks, everyone, for coming to watch this. I
[00:02:27.040 --> 00:02:34.480]   think this should be pretty fun. We have three talks today, and I'm excited about all of them.
[00:02:34.480 --> 00:02:43.200]   So first up, we have Alex from OpenAI, who worked on the robotic hands, and we got to hear snippets
[00:02:43.200 --> 00:02:47.440]   of making the hand do a Rubik's Cube over the years, working with OpenAI, and so we were so
[00:02:47.440 --> 00:02:52.400]   excited when the hand finally made that Rubik's Cube work, and so I'm actually really excited to
[00:02:52.400 --> 00:02:57.680]   hear about what happened behind the scenes to make that possible, because I know you all worked
[00:02:57.680 --> 00:03:06.000]   really hard on that. And then we have Zhang, who's finished his PhD at Stanford in natural
[00:03:06.000 --> 00:03:10.960]   language processing, and then took some of his work and made a company called Sapling that
[00:03:11.680 --> 00:03:18.640]   corrects your language in your text, and I actually use it all the time. I have it in my
[00:03:18.640 --> 00:03:27.040]   browser, and it saved me from sending a lot of very embarrassing emails. So I'm actually also
[00:03:27.040 --> 00:03:33.280]   curious to hear. I think going from research to running a startup is something that probably some
[00:03:33.280 --> 00:03:39.040]   of the people watching aspire to, and it's a jarring experience, and I'd love to hear more
[00:03:39.040 --> 00:03:49.280]   about how that's been for you. And then finally, we have our own Stacey, who has given a really
[00:03:49.280 --> 00:03:56.560]   awesome talk last time, and is going to talk about hyperparameter tuning, and I can't wait to see
[00:03:56.560 --> 00:04:01.120]   that one too. Hyperparameter tuning is something people always ask us about, and she's going to
[00:04:01.120 --> 00:04:08.560]   have some practical advice for you. So shall we get started? Alex is up first.
[00:04:08.560 --> 00:04:15.520]   Okay, cool. Thank you. Get my screen share going.
[00:04:27.200 --> 00:04:33.360]   Okay, do you see just my slide? Yeah. Okay, cool. Let's make sure. Okay,
[00:04:33.360 --> 00:04:43.520]   let me make sure it's working. Okay, cool. So we're going. So hi, everyone. As Fluga said,
[00:04:43.520 --> 00:04:48.480]   my name is Alex. I'm a member of the robotics team at OpenAI. Tonight, I'll be talking about
[00:04:48.480 --> 00:04:53.440]   our progress towards learning to solve dexterous manipulation tasks with a humanoid robot I can't.
[00:04:54.720 --> 00:05:00.240]   So specifically, I'll begin by discussing the motivation for this line of research,
[00:05:00.240 --> 00:05:05.040]   kind of what it means, this research on dexterity, and we'll then describe our two most recent big
[00:05:05.040 --> 00:05:11.200]   releases, learning dexterity from summer of 2018 and solving Rubik's Cube released this past fall.
[00:05:11.200 --> 00:05:16.240]   At the end, I'll add a small update with some more recent research we did kind of along the same
[00:05:16.240 --> 00:05:23.440]   research agenda. Okay, so let's begin with the motivation for this line of research overall.
[00:05:24.160 --> 00:05:30.560]   So the goal of robotics at OpenAI is to build a general purpose robot. That is one that can
[00:05:30.560 --> 00:05:34.960]   operate in the complex environment of the real world and carry out most tasks that humans can do.
[00:05:34.960 --> 00:05:41.920]   Today's robots are still pretty far away from this. So most consumer-oriented robots are either
[00:05:41.920 --> 00:05:49.040]   very simple toys or tend to focus on primarily obstacle avoidance, which are things like Roombas,
[00:05:49.040 --> 00:05:54.880]   self-driving cars, or autonomous drones. These are challenging in their own right and certainly
[00:05:54.880 --> 00:05:58.960]   very difficult problems, especially with self-driving cars. They do not interact with
[00:05:58.960 --> 00:06:03.680]   their environment to the level that we're after. Kind of on the other end of the spectrum are
[00:06:03.680 --> 00:06:08.560]   complicated robots which do interact with their environment, such as in manufacturing or surgical
[00:06:08.560 --> 00:06:14.560]   robotics. However, today almost all of these are either executing just hard-coded behavior,
[00:06:14.560 --> 00:06:17.840]   pre-recorded trajectories, or are being controlled by a human.
[00:06:17.840 --> 00:06:24.240]   Building a general purpose robot is a daunting task though, so we had to pick somewhere to start.
[00:06:24.240 --> 00:06:30.320]   We chose to focus on dexterity and specifically dexterity of humanoid robotic hands for a couple
[00:06:30.320 --> 00:06:36.080]   of reasons. First, the human hand is able to solve a huge array of tasks and the world we've
[00:06:36.080 --> 00:06:42.320]   constructed is designed to a large extent around the human hand. Second, while the necessary
[00:06:42.320 --> 00:06:49.040]   hardware for doing this, so anthropomorphic robotic hands, has existed for a while, it has found
[00:06:49.040 --> 00:06:53.440]   little real-world use due to the very high difficulty in creating software capable of
[00:06:53.440 --> 00:07:00.480]   controlling this hardware. So why is it hard? The main source of difficulty stems from the high
[00:07:00.480 --> 00:07:05.040]   complexity of the robots that are involved. For example, the robot that we've been using
[00:07:05.040 --> 00:07:10.800]   through this line of research, the Shadow Hand, has 24 degrees of freedom and 20 actuators.
[00:07:11.360 --> 00:07:16.000]   This is much higher than the like seven or eight degrees of freedom that you typically see with
[00:07:16.000 --> 00:07:22.560]   robotic arms used in manufacturing. And also when coupled with a relatively small form factor,
[00:07:22.560 --> 00:07:26.080]   this leads to a very complicated and generally less precise hardware,
[00:07:26.080 --> 00:07:28.560]   which in turn is very difficult to simulate accurately.
[00:07:28.560 --> 00:07:34.800]   So that's a very quick motivation. With that, we can go ahead and start talking about the first
[00:07:34.800 --> 00:07:39.840]   set of results that I want to describe to you all tonight. This is Learning Dexterity,
[00:07:39.840 --> 00:07:44.800]   which we released in summer of 2018, work done by all the fine folks that are listed here.
[00:07:44.800 --> 00:07:51.680]   Note that I hadn't joined OpenAI yet at the time. So this is our first project using the Shadow
[00:07:51.680 --> 00:07:58.000]   Hand. So we decided to start with a relatively simple task of reorienting a wooden block,
[00:07:58.000 --> 00:08:05.600]   as pictured here. So now we can go ahead and start talking about how we did this. So our
[00:08:05.600 --> 00:08:10.720]   high level approach was to use reinforcement learning to train a policy, which then controls
[00:08:10.720 --> 00:08:16.800]   the robot. So over the past five or so years, reinforcement learning has proven to be incredibly
[00:08:16.800 --> 00:08:22.560]   powerful from the success with AlphaGo, some more recently with the success of OpenAI 5,
[00:08:22.560 --> 00:08:29.600]   from where I'm working. But these approaches are incredibly data hungry. So you need a huge
[00:08:29.600 --> 00:08:35.120]   amount of trained data to get these approaches to work. And for robotics, this is particularly
[00:08:35.120 --> 00:08:40.800]   challenging since collecting this huge amount of data for training the robots is both difficult
[00:08:40.800 --> 00:08:47.600]   and expensive. So here we have a video from I think Google's so-called Arm Farm, where they
[00:08:47.600 --> 00:08:54.080]   basically did try to build out a huge array of robots and train in the real world. And I guess
[00:08:54.080 --> 00:08:59.520]   this is a sped up video, but you see there's kind of a lot going on here. It's a very expensive
[00:08:59.520 --> 00:09:04.800]   setup to maintain, and it's still kind of difficult to get working. So instead of going down that
[00:09:04.800 --> 00:09:10.480]   route, we've decided to take what is called the sim-to-real approach, which means that we train
[00:09:10.480 --> 00:09:16.240]   our control policy entirely in simulation using a physics simulator and then deploy it onto the
[00:09:16.240 --> 00:09:21.680]   physical robot. This is much cheaper and far more scalable than training directly in the real world,
[00:09:21.680 --> 00:09:25.200]   but it does come with its own set of challenges, which we'll get to in a bit.
[00:09:25.200 --> 00:09:32.480]   So here we have a video from this physics simulator that we use. So specifically,
[00:09:32.480 --> 00:09:37.120]   this is from the MuJoCo physics engine. And the video is showing our policy controlling
[00:09:37.120 --> 00:09:43.040]   the shadow hand here trying to solve the block reorientation task. So on the right-hand side
[00:09:43.040 --> 00:09:48.640]   of the video, you see this kind of transparent block that looks like it's moving around.
[00:09:48.640 --> 00:09:55.200]   What's happening here is that this is the desired orientation for the block. And once the hand is
[00:09:55.200 --> 00:10:01.520]   able to manipulate the block into this position, we sample a new goal for it to achieve. And this
[00:10:01.520 --> 00:10:09.360]   kind of continues until we get to either 50 successes, so 50 goals achieved, or we drop the
[00:10:09.360 --> 00:10:16.560]   block. That's our simulation environment. So now let's take a quick look at our real-world setup.
[00:10:16.560 --> 00:10:24.720]   So the picture here is of what we call our cages for running robotic experiments. So
[00:10:24.720 --> 00:10:31.280]   the hand here is in the middle. You can see it's surrounded by a very large number of cameras.
[00:10:31.840 --> 00:10:36.240]   So most of the cameras here are for the face-based motion tracking system.
[00:10:36.240 --> 00:10:42.320]   I think there's like 24 in total. And that's used for some additional sensory input into the policy.
[00:10:42.320 --> 00:10:48.080]   Only three of the cameras pictured here are normal RGB cameras. So these are the ones that
[00:10:48.080 --> 00:10:53.360]   are circled on the image. The most relevant piece for this talk is that these three RGB cameras
[00:10:53.360 --> 00:10:58.960]   are ultimately fed into a vision model, which is then used to produce an estimate of the location
[00:10:58.960 --> 00:11:03.680]   and the orientation of the block, which is then passed on to the policy when we deploy the real
[00:11:03.680 --> 00:11:09.120]   world. So what this means is that the vision model we train must also be capable of achieving
[00:11:09.120 --> 00:11:15.440]   the sim-to-real transfer. So the problem with the sim-to-real approach is that it's hard.
[00:11:15.440 --> 00:11:21.040]   It's very hard. It's hard because it's impossible to perfectly model the complicated robotic systems
[00:11:21.040 --> 00:11:25.840]   accurately, which means that policies trained in simulation generally perform very poorly in the
[00:11:25.840 --> 00:11:30.480]   real world. In general, this problem is known as the sim-to-real or reality gap.
[00:11:30.480 --> 00:11:37.280]   So rather than working endlessly to make the physics simulation closer to reality,
[00:11:37.280 --> 00:11:41.920]   we can instead employ a technique called domain randomization to force the policy to learn
[00:11:41.920 --> 00:11:45.920]   behaviors which generalize to a wide range of environments rather than overfitting to any
[00:11:45.920 --> 00:11:51.680]   single environment. One of the earliest examples of this technique from Sadagi and Levin is
[00:11:51.680 --> 00:11:57.120]   pictured here. They trained a policy to fly a drone in simulation on environments with a wide
[00:11:57.120 --> 00:12:02.880]   variety of textures rendered on the furniture and the walls in the environment. And this allowed the
[00:12:02.880 --> 00:12:10.320]   policy to then transfer and work in the real world without having seen any real data. At OpenAI,
[00:12:10.320 --> 00:12:16.080]   in 2017, we used roughly the same approach to train a vision model to predict an object's
[00:12:17.120 --> 00:12:22.880]   position and orientation entirely in simulation. As you can see from this video, we're able to
[00:12:22.880 --> 00:12:30.080]   actually use pretty crazy looking textures randomized onto the objects of everything in
[00:12:30.080 --> 00:12:36.080]   the scene. And thanks to the magic of domain randomization, this model is then able to
[00:12:36.080 --> 00:12:44.640]   transfer to the real world. So we took a similar approach then with learning dexterity. So for
[00:12:45.200 --> 00:12:50.640]   this release, we had two different types of domain randomizations that you see displayed here.
[00:12:50.640 --> 00:12:55.680]   So on the left, you have the physics randomizations. These are things like the
[00:12:55.680 --> 00:13:02.080]   friction coefficient, the object sizes, even like gravitational force, et cetera. And on the right,
[00:13:02.080 --> 00:13:07.760]   you see the visual randomizations, which are critical for training the vision model, which
[00:13:07.760 --> 00:13:13.600]   I previously mentioned. Here, we use a different rendering approach than the previous video. So
[00:13:13.600 --> 00:13:19.680]   we use the Unity game engine here to render more realistic looking images and higher resolution,
[00:13:19.680 --> 00:13:28.000]   higher fidelity. And each column you see on the right hand side represents one single sample
[00:13:28.000 --> 00:13:33.360]   fed into the model. So from each of the three cameras that I pointed out earlier, the job of
[00:13:33.360 --> 00:13:38.400]   the vision model is then to predict the pose of the box, the position and the orientation,
[00:13:38.400 --> 00:13:45.360]   given this image. So with this approach to training and simulation with domain randomization in mind,
[00:13:45.360 --> 00:13:52.800]   we now talk about how we ultimately train our models. So this diagram here is kind of describing
[00:13:52.800 --> 00:13:58.080]   the rough flow of how it works. So both the policy and the vision model training
[00:13:58.080 --> 00:14:04.000]   use an internal framework called Rapid, which was originally developed for the OpenAI 5 bot,
[00:14:04.800 --> 00:14:09.440]   but has since been used throughout OpenAI for a lot of reinforcement learning projects.
[00:14:09.440 --> 00:14:15.280]   Note that we train the control policy here using reinforcement learning with state-based
[00:14:15.280 --> 00:14:21.600]   observations rather than from images. The reason we do this is basically because it's
[00:14:21.600 --> 00:14:25.920]   easy to get these state-based observations. So that is like the position and orientation of the
[00:14:25.920 --> 00:14:32.960]   block from the simulator. And it's much, much easier in terms of just like kind of complexity
[00:14:32.960 --> 00:14:38.320]   of setup as well as the amount of compute required to train the LSTM policy from state
[00:14:38.320 --> 00:14:44.560]   rather than from images. However, as I mentioned before, we have this, in the real world, we have
[00:14:44.560 --> 00:14:51.280]   to use a vision model to predict the pose of the block. So we also use the same Rapid framework
[00:14:51.280 --> 00:14:57.600]   to train this vision model using, again, a highly similar distributed system setup.
[00:15:00.000 --> 00:15:04.560]   And then once we've trained using this setup for long enough, we can then deploy it to the
[00:15:04.560 --> 00:15:10.320]   real robot. To do this, we finally combine the vision model with the policy. So we have the
[00:15:10.320 --> 00:15:16.320]   vision model process, the real frames images from the cameras mounted around the cage.
[00:15:16.320 --> 00:15:22.560]   And this predicts, this produces an estimated pose, which is then passed along to the LSTM policy,
[00:15:22.560 --> 00:15:25.600]   which ultimately produces actions which control the robotic hand.
[00:15:28.320 --> 00:15:35.840]   So that's how it works. Now let's see the system in action. So there's audio. So yeah, here's an
[00:15:35.840 --> 00:15:40.800]   example of our system performing object reorientation on a physical hand. On the right
[00:15:40.800 --> 00:15:45.360]   is the desired orientation of the block. Once it's achieved, we randomly sample a different goal.
[00:15:45.360 --> 00:15:52.320]   Sorry, I did not realize there's audio for this video. But basically, this goes on for a long
[00:15:52.320 --> 00:15:57.120]   time, seven full minutes of 50 successful goals achieved. You can view the whole thing on YouTube
[00:15:57.120 --> 00:16:04.480]   later if you're curious. So to take away the music now. So we also quantified the performance
[00:16:04.480 --> 00:16:09.760]   of our system. So we measured the median and maximum number of successfully achieved goals
[00:16:09.760 --> 00:16:16.480]   over 10 trials on the real robot with a few different setups considered. So we can see here
[00:16:16.480 --> 00:16:22.960]   that the domain randomization is absolutely critical to success. We get basically no transfer
[00:16:22.960 --> 00:16:29.760]   performance, even from an LSTM-based policy, without domain randomization. We also find
[00:16:29.760 --> 00:16:36.160]   that the LSTM greatly outperforms a standard feedforward model. We believe this is due to
[00:16:36.160 --> 00:16:40.880]   the memory present in an LSTM, which when coupled with our training approach, allows for what we've
[00:16:40.880 --> 00:16:46.720]   been calling emergent meta-learning or domain adaptation. So a bit more on that later.
[00:16:48.480 --> 00:16:52.960]   Okay, so that's it for the learning dexterity release. Now we'll fast forward to fall of last
[00:16:52.960 --> 00:16:57.520]   year when we released our work on manipulating Rubik's Cube using the same physical setup.
[00:16:57.520 --> 00:17:04.080]   So in this work, we use a shadow hand to solve Rubik's Cube. We chose this specific task because
[00:17:04.080 --> 00:17:08.720]   it built directly on the previous work, while also taking the difficulty of the manipulation
[00:17:08.720 --> 00:17:14.160]   problem up several notches. Before we dive into the details here, I just want to clarify
[00:17:15.120 --> 00:17:19.520]   the goals that we had with this project when we set out to do it, which you can see here.
[00:17:19.520 --> 00:17:24.000]   So the primary goal was to push the limits of symptom-to-real transfer on an incredibly
[00:17:24.000 --> 00:17:29.200]   difficult dexterous manipulation task. Note that we didn't care for this project about learning
[00:17:29.200 --> 00:17:33.840]   to solve the Rubik's Cube symbolically, as this only seemed tangential to our real research agenda
[00:17:33.840 --> 00:17:41.600]   around dexterity. Okay, now for some details. So just to start, at a high level, we leveraged the
[00:17:41.600 --> 00:17:46.720]   same approach from the previous release on learning dexterity. So basically, we tried to combine
[00:17:46.720 --> 00:17:53.120]   symptom-to-real domain randomization. We used an LSTM policy and a vision model, separately trained
[00:17:53.120 --> 00:18:00.320]   in simulation, and all of this. So one early surprise of the project was that it was surprisingly
[00:18:00.320 --> 00:18:05.520]   easy to solve this task in a simulator using a very simplified model of Rubik's Cube, as you can
[00:18:05.520 --> 00:18:13.680]   see here. However, it turned out to be incredibly difficult to transfer this policy to the real
[00:18:13.680 --> 00:18:19.120]   robot, as you can see from the video struggling there. So basically, we had a much harder
[00:18:19.120 --> 00:18:25.840]   symptom-to-real problem compared to the previous work. So at first, like I mentioned, we were just
[00:18:25.840 --> 00:18:30.480]   trying the same combination of reinforcement learning and domain randomization from learning
[00:18:30.480 --> 00:18:36.800]   dexterity. However, we quickly found that for this project, domain randomization just was not
[00:18:36.800 --> 00:18:42.240]   scaling well enough. So we ended up having to continue to add so many more parameters that
[00:18:42.240 --> 00:18:47.680]   needed to be randomized, such that we couldn't feasibly set the correct ranges for each of them
[00:18:47.680 --> 00:18:53.120]   correctly by hand. It just simply wasn't scalable. So instead, we introduced what we call
[00:18:53.120 --> 00:18:58.800]   automatic domain randomization, or ADR, to discover the correct ranges for each parameter
[00:18:58.800 --> 00:19:05.840]   for us automatically. So I'll now quickly describe what ADR does. So to begin, we can consider a
[00:19:05.840 --> 00:19:11.280]   non-domain randomization approach, which would consist of a single point in the space of domain
[00:19:11.280 --> 00:19:15.600]   randomization parameters corresponding to our best guess for the real value of these parameters.
[00:19:15.600 --> 00:19:21.760]   So what this means is you might try to measure the friction coefficients of the surfaces. You
[00:19:21.760 --> 00:19:27.520]   would use the actual value of gravity on Earth, things like this. With domain randomization,
[00:19:27.520 --> 00:19:35.200]   we would define a fixed box around the starting point in domain randomization parameter space. So
[00:19:35.200 --> 00:19:40.000]   these-- the box, you know, corresponding to the fixed ranges in the high dimensional space.
[00:19:40.000 --> 00:19:47.600]   With ADR, we're able to start with conservative initial ranges and then grow them automatically
[00:19:47.600 --> 00:19:53.280]   according to how well the policy performs, rather than trying to initialize them to the perfect,
[00:19:53.280 --> 00:19:58.560]   like, maximal values. Specifically, it does this by occasionally sampling each domain
[00:19:58.560 --> 00:20:04.080]   randomization parameter at its lower or upper bound and then evaluates the performance of the
[00:20:04.080 --> 00:20:09.200]   policy on the resulting environment. If the policy does well enough, ADR will then expand this bound
[00:20:09.200 --> 00:20:14.400]   in the direction that it was sampled. So if you run this for long enough, it kind of gradually
[00:20:14.400 --> 00:20:22.640]   grows this box and parameter space much farther than we had previously been able to tune it
[00:20:22.640 --> 00:20:29.360]   by hand, in part because ADR also kind of gives us an implicit curriculum over these environments.
[00:20:29.360 --> 00:20:34.160]   So as things get more-- like, over time, that generally the environments that you're trying
[00:20:34.160 --> 00:20:41.760]   to solve become more and more difficult. So here we see an example of ADR at work on the domain
[00:20:41.760 --> 00:20:46.640]   randomization parameter governing the size of the cube. So you can see ADR allows us to initialize
[00:20:46.640 --> 00:20:52.000]   this parameter just to the actual size of the cube. And then over time, the policy is able to
[00:20:52.960 --> 00:20:56.560]   work with not just this size of the cube, but also one that's quite a bit bigger and quite a bit
[00:20:56.560 --> 00:21:03.280]   smaller than the real size. But this is just one parameter. So again, the point of ADR is that
[00:21:03.280 --> 00:21:08.320]   we have so many parameters to tune that we can't do it by hand. So here you see the complete set
[00:21:08.320 --> 00:21:13.600]   listed out from some internal analytics. It would be quite a pain to tune all of these perfectly
[00:21:13.600 --> 00:21:18.160]   by hand. And it's probably just not really a feasible thing to do given compute constraints.
[00:21:19.840 --> 00:21:24.080]   So I can talk a little bit about how we actually then trained it. So it's largely the same setup.
[00:21:24.080 --> 00:21:29.840]   So again, we use the internal framework we call RAPID to separately train an LSTM policy and
[00:21:29.840 --> 00:21:34.560]   a convolutional neural net vision model and simulation. We then combine them in the same
[00:21:34.560 --> 00:21:42.800]   way to deploy to the real robot. So now for the results. So after much trial and error-- basically,
[00:21:42.800 --> 00:21:48.800]   much, much trial and error-- we finally got it working early summer of last year. So here we
[00:21:48.800 --> 00:21:53.120]   have a video of the Shadow Hand successfully solving Rubik's Cube from a fair scramble.
[00:21:53.120 --> 00:21:57.600]   So as of the last one, it takes a few minutes at real-time speed. So I won't show the whole
[00:21:57.600 --> 00:22:02.080]   thing now. But the video is also on YouTube and is also linked from our blog. So I encourage you
[00:22:02.080 --> 00:22:08.960]   to check it out there if you're curious. So once we did this, we also ran some fun
[00:22:08.960 --> 00:22:14.080]   robustness experiments to kind of test the limits of the domain randomization we employed,
[00:22:14.960 --> 00:22:19.520]   including things like tying the fingers together, putting a rubber glove on the hand,
[00:22:19.520 --> 00:22:26.880]   or the perennial fan favorite, the plush giraffe perturbation. Note that none of these perturbations
[00:22:26.880 --> 00:22:31.200]   were included in the training data, particularly the plush giraffe. That would be way too much
[00:22:31.200 --> 00:22:37.760]   work. We also did some analysis to better understand how ADR related to performance
[00:22:37.760 --> 00:22:44.080]   on the real robot. So here we measure the size of the domain randomization parameter box that I
[00:22:44.080 --> 00:22:49.440]   mentioned earlier by the entropy of the distribution. So basically, a higher entropy here
[00:22:49.440 --> 00:22:56.800]   means that ADR has expanded the parameter space more. So we've covered the policy has effectively
[00:22:56.800 --> 00:23:01.920]   solved more environments. Encouragingly, when we ran this analysis, we found a correlation between
[00:23:01.920 --> 00:23:07.600]   this ADR entropy and the mean number of successes achieved in real rollouts. So since we did this
[00:23:07.600 --> 00:23:11.840]   in the real world, we unfortunately don't have a huge amount of data. But I think that the trend
[00:23:11.840 --> 00:23:18.000]   here we see is pretty clear, and it's pretty exciting to us that ADR has this property.
[00:23:18.000 --> 00:23:23.280]   We also did a lot of analysis for the paper studying the emergence
[00:23:23.280 --> 00:23:30.080]   of meta-learning capabilities in our policy. So in our case, what we mean by meta-learning is that
[00:23:30.080 --> 00:23:35.760]   the policy has learned how to infer the parameters of its environment so that it can adapt and more
[00:23:35.760 --> 00:23:41.520]   efficiently solve the task. So one piece of evidence for this meta-learning is presented
[00:23:41.520 --> 00:23:46.000]   here, where we attempt to measure whether the hidden state of the LSTM-- so basically,
[00:23:46.000 --> 00:23:51.840]   at the end of a rollout, we capture this specter that's the memory of the model,
[00:23:51.840 --> 00:23:58.480]   like what is the state of it at the end of a rollout-- and try to determine if it has information
[00:23:58.480 --> 00:24:04.880]   about the size of the cube that it's tasked with solving. So of course, we did this in simulation,
[00:24:04.880 --> 00:24:10.720]   where we can easily vary the size of the cube automatically across a wider range.
[00:24:10.720 --> 00:24:17.120]   So in this table, the rightmost column here is the accuracy that we obtained from a linear
[00:24:17.120 --> 00:24:23.520]   classifier that's fit atop this hidden state, this memory, to predict whether the cube size is above
[00:24:23.520 --> 00:24:31.760]   or below the mean size of the cube. So this being just a binary classifier, the random prediction
[00:24:31.760 --> 00:24:38.400]   accuracy would be 0.5. So the fact that we see all of these well above 0.5, and the confidence
[00:24:38.400 --> 00:24:44.320]   interval being above 0.5, is pretty encouraging. It tells us that this information is contained
[00:24:44.320 --> 00:24:49.840]   within the memory. So the policy has learned how to learn about the environment and first state
[00:24:49.840 --> 00:24:57.680]   about the environment. And again, it was exciting to see that this accuracy only increased as we
[00:24:57.680 --> 00:25:06.640]   ran for longer and basically had a higher ADR volume. And yeah, that wraps up the discussion
[00:25:06.640 --> 00:25:11.840]   of our most recent two big releases. For more info on each, I definitely just skimmed the surface
[00:25:11.840 --> 00:25:17.520]   here. There's tons more in both papers that are on archive, and also more information in our blog.
[00:25:17.520 --> 00:25:24.480]   So I encourage you to check those out. Finally, we'll now discuss a smaller result from some work
[00:25:24.480 --> 00:25:28.880]   completed at the beginning of this year, which was published recently in a Weights and Biases
[00:25:28.880 --> 00:25:35.760]   report. That's the picture on the right here and linked below. So for this result, once we had
[00:25:35.760 --> 00:25:40.480]   shipped the Rubik's Cube results, we decided to do a brief investigation into whether we could
[00:25:40.480 --> 00:25:46.320]   train a policy directly from images, aka end-to-end, cutting out the need for a separate vision model.
[00:25:46.320 --> 00:25:51.040]   This means the policy architecture, both in training and when run on the real robot,
[00:25:51.040 --> 00:25:58.080]   looks like this, where effectively, instead of having the vision convolutional neural network
[00:25:58.080 --> 00:26:03.920]   produce an estimate of the block pose, you have it produce some embedding in a higher dimensional
[00:26:03.920 --> 00:26:08.480]   space representing the state of the environment, which is then directly fed into the policy,
[00:26:08.480 --> 00:26:14.480]   which then, again, produces actions. So the important property here is that when you're
[00:26:14.480 --> 00:26:18.560]   training the model, you're able to backprop all the way through the vision stack to learn the
[00:26:18.560 --> 00:26:26.240]   optimal representation for the policy. So for this investigation, we chose the block reorientation
[00:26:26.240 --> 00:26:30.880]   test from the learning dexterity release that I talked about, just because it's much simpler
[00:26:30.880 --> 00:26:39.120]   to solve relative to the Rubik's Cube. We decided to start by trying behavioral cloning to more
[00:26:39.120 --> 00:26:45.280]   quickly train such a policy. So behavioral cloning works by first training a state-based policy,
[00:26:45.280 --> 00:26:52.080]   as was done in the learning dexterity release, which is then used to effectively teach a new
[00:26:52.080 --> 00:27:00.800]   end-to-end student policy how to behave. So concretely, what this means is you allow the
[00:27:00.800 --> 00:27:06.480]   end-to-end policy to do its rollout. So it'll try to act in the environment, just as you would in a
[00:27:06.480 --> 00:27:12.560]   typical RL setting. But then when you do optimization, instead of using something like
[00:27:13.200 --> 00:27:19.920]   PPO or any kind of policy gradient technique like we've been using for our past two releases,
[00:27:19.920 --> 00:27:27.920]   you instead sample the optimal actions from your teacher model-- so in this case,
[00:27:27.920 --> 00:27:34.480]   the state-based model-- and then coerce the student policy to behave like the teacher would
[00:27:34.480 --> 00:27:41.600]   have in any given situation. So this effectively replaces a reinforcement learning problem with
[00:27:41.600 --> 00:27:46.880]   a supervised learning problem, where the policy gets direct supervision on its actions at every
[00:27:46.880 --> 00:27:53.280]   single time step. And with this kind of swap to supervised learning, you get a number of benefits
[00:27:53.280 --> 00:27:58.640]   related to training speed and just the general ease of the problem, which I should mention here.
[00:27:58.640 --> 00:28:06.480]   So basically, this made this whole investigation much quicker and cheaper to perform. And we
[00:28:06.480 --> 00:28:10.160]   already knew that behavioral cloning had a good shot at working because we've been using this
[00:28:10.160 --> 00:28:16.240]   technique within the team for over a year now to do other kinds of experimentation.
[00:28:16.240 --> 00:28:22.960]   Long story short, again, after a lot of debugging, which is honestly 90% of this job,
[00:28:22.960 --> 00:28:30.000]   we were finally able to get this working. So once we had a basic set of working, we then used
[00:28:30.000 --> 00:28:36.560]   behavioral cloning to run a number of ablations to find the optimal setup. So cloning was key here
[00:28:36.560 --> 00:28:41.280]   as it allowed us to more quickly run these ablations. So there are a few more that are
[00:28:41.280 --> 00:28:45.920]   listed in the report and a few that require a bit more context to set up. But I think the most
[00:28:45.920 --> 00:28:50.720]   interesting and quick to explain ablation we ran here was on using a pre-trained vision model.
[00:28:50.720 --> 00:28:58.080]   So here in green, we have a kind of a from scratch behavioral cloning run with a randomly
[00:28:58.080 --> 00:29:03.680]   initialized vision model. And then in pink, we have a run where we initialize the vision model
[00:29:04.320 --> 00:29:09.840]   with the parameters from an already trained vision model that was taken basically from the same
[00:29:09.840 --> 00:29:15.920]   vision model we used for the learning dexterity release. So interestingly, we see a 4x speed up.
[00:29:15.920 --> 00:29:19.200]   I think it's not surprising we would see some speed up, but I think it's interesting that
[00:29:19.200 --> 00:29:26.560]   the nature of the speed up, it's basically eliminating this kind of flat, long stretch
[00:29:26.560 --> 00:29:32.240]   that you see with the green curve here, during which time we're hypothesizing that maybe the
[00:29:32.240 --> 00:29:38.640]   policy is struggling to learn the right representation of its environment.
[00:29:38.640 --> 00:29:47.120]   So once we had this optimal setup for doing behavioral cloning, we then used it to run a
[00:29:47.120 --> 00:29:53.120]   single very large reinforcement learning experiment. So again, basically from scratch,
[00:29:53.120 --> 00:30:00.880]   except for using a pre-trained vision stack. So in green here, we have the RL experiment.
[00:30:00.880 --> 00:30:05.920]   And in pink, we had the behavioral cloning. Oh, and sorry, I realized I forgot to mention what
[00:30:05.920 --> 00:30:13.120]   these plots denote. But on the y-axis here, we have the mean episodic reward. And on the x-axis,
[00:30:13.120 --> 00:30:18.000]   we have the amount of frames collected. So basically, the amount of training experience
[00:30:18.000 --> 00:30:24.640]   that has gone into the model. So from this plot, you can see that there's a huge gap in how much
[00:30:24.640 --> 00:30:32.240]   experience it takes to converge. So the total amount that it comes out to is about 30x more
[00:30:32.240 --> 00:30:37.360]   compute for the reinforcement learning experiment than behavioral cloning. So this is just kind of
[00:30:37.360 --> 00:30:45.200]   a useful result to keep in mind for us for future projects. And that's it. Thanks for listening.
[00:30:48.000 --> 00:30:57.520]   Nice. Thanks, Alex. Do people have questions? If you can pop it up in the chat or the QA
[00:30:57.520 --> 00:31:01.040]   button if you're on Zoom. I know Charles has a question.
[00:31:01.040 --> 00:31:10.000]   Yeah. So it's really cool work and very impressive. There's lots of difficult aspects
[00:31:10.000 --> 00:31:13.680]   to reinforcement learning and to robotics. And to get them to both work together is really cool.
[00:31:14.720 --> 00:31:22.400]   The question I have is one of the salient features of the human motor system, especially the
[00:31:22.400 --> 00:31:27.760]   dexterity, is haptic feedback. I get a bunch of information from my joints about the amount of
[00:31:27.760 --> 00:31:34.880]   tension, about their position, about their relative orientation, and whatever massive
[00:31:34.880 --> 00:31:41.520]   data stream we get from our skin. So I'm curious to what degree are folks at OpenAI interested in
[00:31:41.520 --> 00:31:49.120]   incorporating haptic feedback to future versions of this? Or was its exclusion a very-- yeah,
[00:31:49.120 --> 00:31:54.640]   because it's not helpful or something like that? Yeah. Yeah, that's a good question. So I think
[00:31:54.640 --> 00:32:02.400]   there's a few aspects to this. The first is that the hardware, I think, for this kind of sensing
[00:32:02.400 --> 00:32:07.440]   is a little more finicky to work with. So the Shadow Hand actually does come with some of these
[00:32:08.080 --> 00:32:13.360]   sensors. I believe they're like-- gosh, I can't remember the exact term. But there's basically
[00:32:13.360 --> 00:32:21.600]   pressure sensors in the fingers at each digit. But they're very sensitive to all kinds of factors
[00:32:21.600 --> 00:32:30.800]   like air pressure or other things that lead to them being very noisy, which has made them more
[00:32:30.800 --> 00:32:36.240]   difficult to incorporate. And then I think basically because of this, it's also a little
[00:32:36.240 --> 00:32:42.000]   more difficult to exactly simulate the dynamics with them. So I think the way the Shadow Hand
[00:32:42.000 --> 00:32:47.120]   worked is there is these little kind of like balloons of-- I don't know if it was actually
[00:32:47.120 --> 00:32:51.200]   just air or some other gas that would kind of compress slightly and it would measure that.
[00:32:51.200 --> 00:32:55.440]   So that throws off the dynamics a little bit. I think longer term, it's definitely a thing that
[00:32:55.440 --> 00:33:00.720]   we're interested in because clearly it's very critical to humans. We've even tried, I think--
[00:33:01.440 --> 00:33:07.040]   or we joked around on the team of trying these numbing agents you can put on your hand and see
[00:33:07.040 --> 00:33:10.960]   if we can still do some of these tasks. And we definitely think it's a lot more difficult.
[00:33:10.960 --> 00:33:22.080]   Cool. Thanks. I guess the other-- we also have a tremendous amount of experience working with
[00:33:22.080 --> 00:33:28.160]   vision. In the ML community, we have convolutional neural nets that are this super weapon for
[00:33:28.160 --> 00:33:33.040]   handling vision tasks. So do you also think it's a matter of algorithmic development that we need
[00:33:33.040 --> 00:33:38.640]   better tools for handling the kinds of input streams that things like haptic sensors provide?
[00:33:38.640 --> 00:33:44.720]   Or do you think that we have that technology figured out? That's interesting. I would guess
[00:33:44.720 --> 00:33:51.360]   that that won't be the blocker anytime soon for this. But we haven't tested it as well.
[00:33:54.080 --> 00:33:57.040]   All right. We've got a couple of questions here from the audience.
[00:33:57.040 --> 00:34:04.160]   Salim asks, did you run behavioral coding for the same number of frames as RL?
[00:34:04.160 --> 00:34:13.360]   Oh, yeah. I see. I see the question. So like 2 billion or 3 billion. Maybe it'll get better. I
[00:34:13.360 --> 00:34:19.120]   think there's another question I see related to the RL being better at the end. So this was a
[00:34:19.120 --> 00:34:26.000]   kind of a difficulty in constructing the plot. So I was showing the mean reward obtained during
[00:34:26.000 --> 00:34:33.280]   training. But we also have a separate set of evaluation on the maximally difficult environment.
[00:34:33.280 --> 00:34:38.720]   And we find that the reason we stopped the behavioral coding experiment when we did is
[00:34:38.720 --> 00:34:44.560]   because it had already fully solved the environment. So with the median performance was 50
[00:34:44.560 --> 00:34:49.280]   successes out of 50. So the reward can go a little bit higher because you get
[00:34:49.280 --> 00:34:54.240]   benefits for other things. But it didn't matter for us at this point.
[00:34:54.240 --> 00:35:03.600]   Gotcha. Someone, Matisse asks, how's it like to work on such big projects?
[00:35:03.600 --> 00:35:06.800]   Isn't it hard to get the full picture of what you're doing sometimes?
[00:35:08.160 --> 00:35:14.720]   Yeah. Yeah, it's a good question. I personally really enjoy working on these big projects.
[00:35:14.720 --> 00:35:22.160]   I think for robotics, I can't really see a way of solving really hard problems without having
[00:35:22.160 --> 00:35:26.080]   a bunch of people focusing on it just because there's so much to figure out on the stack,
[00:35:26.080 --> 00:35:30.800]   from getting the firmware right, getting the physical hardware set up right,
[00:35:30.800 --> 00:35:36.880]   and all the way up to designing the right-- using the right reinforcement learning algorithms.
[00:35:37.200 --> 00:35:43.600]   So I agree. It can be harder to understand fully what's going on. I mean, I don't have full context
[00:35:43.600 --> 00:35:47.920]   in everything the mechanical engineers do on our team. But I also think that's kind of fun because
[00:35:47.920 --> 00:35:52.880]   there's more opportunity to learn new stuff. What are some of the tools you're using to
[00:35:52.880 --> 00:36:01.840]   understand what's going on with these models? Yeah. So I guess the tooling we use-- so yeah,
[00:36:01.840 --> 00:36:07.440]   like I mentioned, we use Waves and Biases a lot for sharing the reports and discussing
[00:36:07.440 --> 00:36:16.960]   reports-- results internally. The tech stack, I guess, if that's the question, is we're using
[00:36:16.960 --> 00:36:24.000]   PyTorch to train these models, which we've really enjoyed using. And then we have a bunch of
[00:36:24.000 --> 00:36:34.400]   internal stuff as well. So Han asks, how does domain randomization relate to data augmentation
[00:36:34.400 --> 00:36:39.920]   schedules? And how would the search and the reinforcement learning space translate to image
[00:36:39.920 --> 00:36:48.800]   tasks? Interesting. So I think, yeah, there is, I guess, a sense in which there's a relation
[00:36:48.800 --> 00:36:54.880]   between domain randomization and data augmentation. I don't know as much about the schedules here
[00:36:54.880 --> 00:37:00.720]   that Han refers to. I'm not that expert in that field. But I guess the key difference here is
[00:37:00.720 --> 00:37:05.280]   that with data augmentation, I guess you're starting with some real-world data. And then
[00:37:05.280 --> 00:37:10.880]   you're perturbing it in some way to get basically more data. Whereas in our case, everything is
[00:37:10.880 --> 00:37:16.720]   coming from a simulation. So you don't really need to view it as augmentation as much. But
[00:37:16.720 --> 00:37:23.680]   there's definitely a relationship there. What's the team vision with this project,
[00:37:23.680 --> 00:37:31.200]   maybe in a year or two? Yeah, so I can't really talk too much about future work beyond what I
[00:37:31.200 --> 00:37:37.920]   shared with the motivation, aside from just our overarching goal around building a general-purpose
[00:37:37.920 --> 00:37:47.680]   robot. Does OpenAI plan on building Keras-like OpenAPIs for reinforcement learning?
[00:37:47.680 --> 00:37:56.560]   I don't-- well, I don't think we'd have something like Keras. I think the closest answer maybe is
[00:37:56.560 --> 00:38:03.040]   that we have this release called OpenAI Baselines, which are good standard implementations of RL
[00:38:03.040 --> 00:38:12.160]   algorithms. But yeah, that's the closest maybe. Nice. I also encourage the YouTube folks who
[00:38:12.160 --> 00:38:17.200]   ask questions. Right now, the Zoom folks are asking all the questions. But this one question
[00:38:17.200 --> 00:38:21.760]   does come from YouTube. What was your most important learning in this project?
[00:38:21.760 --> 00:38:32.800]   Yeah, I think for me-- so I only joined kind of March of last year. I think the most
[00:38:32.800 --> 00:38:39.520]   interesting thing to see was that a lot of these-- the problems that we ended up solving,
[00:38:39.520 --> 00:38:43.280]   they felt at the end of the day kind of like more like engineering problems that we had to figure
[00:38:43.280 --> 00:38:49.680]   out. So what was really cool to see was that basically when we go from learning dexterity to
[00:38:49.680 --> 00:38:55.680]   solving the Rubik's Cube, roughly the same formula worked. So we used reinforcement learning with
[00:38:55.680 --> 00:39:02.640]   domain randomization. We introduced ADR, which was critical to success too. But at a high level
[00:39:02.640 --> 00:39:06.480]   the same approach worked. We just had to figure out how to run it at the right scale and kind of
[00:39:06.480 --> 00:39:11.920]   solve a lot of engineering problems. So I think that was really encouraging because I think
[00:39:11.920 --> 00:39:15.920]   the promise for these approaches is very high for challenging tasks.
[00:39:15.920 --> 00:39:26.080]   Nice. Last question. This is just me wondering, did working on this project change how you do
[00:39:26.080 --> 00:39:31.440]   machine learning or change what you think of as best practices but are not? Or you discovered
[00:39:31.440 --> 00:39:41.920]   new best practices? Yeah, it's interesting. So I think the interesting thing for me jumping on
[00:39:41.920 --> 00:39:46.240]   this project-- so my background before this was more in applied machine learning working on
[00:39:46.240 --> 00:39:53.760]   sort of fraud detection. So there's far less emphasis on the algorithms themselves. Basically,
[00:39:53.760 --> 00:39:59.280]   all of the work that you end up doing is in getting better data in or better features.
[00:40:00.640 --> 00:40:05.520]   So I think what's been interesting to see is that with this work, we also generally don't worry too
[00:40:05.520 --> 00:40:11.520]   much about the algorithms themselves. We've been using PBO for all of these results as a great
[00:40:11.520 --> 00:40:18.960]   policy gradient method. So that works really well. The main thing that we do end up having to worry
[00:40:18.960 --> 00:40:24.480]   about is, again, the data that we're training on. Only in this case, we have complete control
[00:40:24.480 --> 00:40:30.000]   over it. It's all coming from a simulator. But I think it was interesting to note that it's still
[00:40:30.000 --> 00:40:32.720]   just the data that you're trading on that ends up mattering the most.
[00:40:32.720 --> 00:40:41.200]   Cool. I have one more question, and then we'll end it. So why did you pick the Rubik's Cube as
[00:40:41.200 --> 00:40:44.400]   the go-to problem to prove that the robot hand worked?
[00:40:44.400 --> 00:40:49.920]   Yeah, so I wasn't around for when we actually picked this. My understanding is that
[00:40:49.920 --> 00:40:58.320]   it's basically something that I think is roughly the most difficult single-handed
[00:40:58.320 --> 00:41:06.640]   manipulation task that you can think of. So basically, if you have your wrist in a fixed
[00:41:06.640 --> 00:41:12.000]   position and you have to do something with just your fingers, it's about as hard as you can
[00:41:12.000 --> 00:41:18.560]   imagine. And it's also something that I think has been floated around. The idea of it has been
[00:41:18.560 --> 00:41:23.600]   mentioned in the robotics community. And you'll sometimes see stock photos, like a robot, it can,
[00:41:23.600 --> 00:41:28.640]   like a Rubik's Cube next to it. But no one's really made a serious effort towards solving it.
[00:41:28.640 --> 00:41:34.560]   So I think the assumption that it would be too difficult to solve was attractive.
[00:41:34.560 --> 00:41:46.080]   I have actually one little quick more follow-up. So in one closely related world that I'm more
[00:41:46.080 --> 00:41:51.680]   familiar with is the neuroprosthetics world, like the goal of generating, of allowing human
[00:41:51.680 --> 00:41:56.400]   neural networks to control robotic hands. And one of the tasks that turned out to be most difficult
[00:41:56.400 --> 00:42:02.400]   for that, and that required haptic feedback, was the manipulation of soft objects and things that
[00:42:02.400 --> 00:42:09.040]   are much more deformable than a Rubik's Cube. So can you say anything about your experience using
[00:42:09.040 --> 00:42:16.640]   this setup and these algorithms for those kinds of tasks, like handling an egg or a Play-Doh or
[00:42:16.640 --> 00:42:23.680]   something like that? Yeah, that's a good point. So one of the difficult parts with training entirely
[00:42:23.680 --> 00:42:30.160]   in simulation is that simulators today at least are not as good at modeling deformable objects.
[00:42:30.160 --> 00:42:34.800]   It's really difficult. You end up having way too many contact points, and it becomes really
[00:42:34.800 --> 00:42:41.280]   expensive. So we haven't really focused on this quite as much. I think it's something where we
[00:42:41.280 --> 00:42:47.200]   believe once we push our sim-to-real approach far enough, we'll be able to do it. But yeah,
[00:42:47.200 --> 00:42:54.320]   it's not a thing we've attempted too much yet. Cool. All right. Thank you, Alex.
[00:42:54.320 --> 00:43:02.880]   So Alex is also going to be doing an AMA soon in our community, and we'll tweet about it/post
[00:43:02.880 --> 00:43:07.360]   it in the community so you guys can come and ask him more questions, because obviously we've got
[00:43:07.360 --> 00:43:19.680]   so many. Thank you. Yeah, no problem. Next up, we have Zeng. Zeng did his PhD in natural language
[00:43:19.680 --> 00:43:28.000]   processing from Stanford. He's now the co-founder of Sapling.ai. And today, he's going to talk about
[00:43:28.000 --> 00:43:33.440]   the challenges that you face in taking natural language processing models from research to
[00:43:33.440 --> 00:43:40.960]   production. Welcome. Well, thanks for having me. Let me share my screen really quick.
[00:43:40.960 --> 00:43:55.120]   Okay. Now full screen. So can everyone see the first slide? Awesome. Okay. Cool. So yeah,
[00:43:55.120 --> 00:44:00.080]   thanks for the introduction. Just going to give a little bit more background, because I think
[00:44:00.080 --> 00:44:06.320]   it's relevant for the rest of the talk. So we're building an AI layer for chat,
[00:44:06.320 --> 00:44:14.640]   texts, tickets, emails. And so part of that involves deploying language models. It involves
[00:44:14.640 --> 00:44:21.760]   deploying these deep encoder/decoder models, and more recently, more QA-style models as well.
[00:44:22.480 --> 00:44:31.040]   And so James and I, who work on this, starting out, we encountered lots of issues,
[00:44:31.040 --> 00:44:37.280]   just trying to go from what you see described in research paper towards something that you can
[00:44:37.280 --> 00:44:46.080]   deploy and have other people use. So I actually recorded us early on during one of our discussions,
[00:44:47.920 --> 00:44:54.080]   just us working on this during a particular hectic stretch. And so let me play that for you.
[00:44:54.080 --> 00:45:00.960]   I should say it's a little bit, actually, it might be a little bit loud, but I may not have
[00:45:00.960 --> 00:45:05.360]   shared audio, which is also fine. But if I did share audio, it might be a little bit loud.
[00:45:06.400 --> 00:45:26.240]   Just fair warning. All right. So obviously, that wasn't us, but pretty close. Pretty close.
[00:45:26.240 --> 00:45:31.280]   That's more or less how it feels like sometimes when we're taking these models and putting them
[00:45:31.280 --> 00:45:37.360]   out for people to use. But fortunately, the first time around, it was pretty much like this,
[00:45:37.360 --> 00:45:43.600]   really hectic. But for the second and third model types that I talked about, each time we were able
[00:45:43.600 --> 00:45:51.680]   to deploy things and get things, scale things on our cluster much more quickly, much more smoothly.
[00:45:51.680 --> 00:45:57.760]   And so it appears at least as though we're learning something in this process of deploying
[00:45:57.760 --> 00:46:06.720]   the different systems. And so this talk is really describing our learnings. So I wanted to focus
[00:46:06.720 --> 00:46:12.400]   actually around three high-level themes. So the themes are recursive dimensionality,
[00:46:12.400 --> 00:46:21.120]   iteration time, and opaque models. And I know that many people watching are probably familiar
[00:46:21.120 --> 00:46:28.960]   with these. They probably encountered these. These are not specific to NLP, although I described this
[00:46:28.960 --> 00:46:36.800]   as a talk focusing on NLP. But although I think these are themes that are quite familiar,
[00:46:36.800 --> 00:46:45.680]   after quite a number of years working on applied AI, applied ML, these are still a few of the
[00:46:45.680 --> 00:46:51.120]   themes that I spend a lot of time thinking about, that I learn new things about every month,
[00:46:51.120 --> 00:47:00.800]   and that I'm still gaining new perspectives on. So research, new papers are going on archive
[00:47:00.800 --> 00:47:04.960]   quite frequently. But I don't think these are three issues that are going to go away
[00:47:04.960 --> 00:47:08.080]   if you're trying to take NLP models from research to production.
[00:47:09.840 --> 00:47:13.280]   All right, so with that, let's talk about recursive dimensionality.
[00:47:13.280 --> 00:47:21.360]   So I'm actually using the term in a very loose sense. Actually, what I'll probably be talking
[00:47:21.360 --> 00:47:27.840]   about-- what I'll be talking about, rather, is probably better described by the second term
[00:47:27.840 --> 00:47:31.840]   here, combinatorial explosion. And you should probably just group data sparsity and recursive
[00:47:31.840 --> 00:47:36.960]   dimensionality together. So yeah, I'm using this in a loose definition. But this is something that
[00:47:36.960 --> 00:47:43.760]   we've all encountered doing machine learning. So whether it's that you don't have enough data,
[00:47:43.760 --> 00:47:49.280]   or like we just saw, if a robot has lots of degrees of freedom, or you're training a model,
[00:47:49.280 --> 00:47:55.920]   and the gradient keeps blowing up, or certain issues pop up that you just never expected,
[00:47:55.920 --> 00:48:02.720]   because there's a very long tail of possible inputs. I kind of throw these into the same
[00:48:02.720 --> 00:48:09.920]   bucket and think of them in a similar way. One quick example-- and I guess I'll just skim over
[00:48:09.920 --> 00:48:14.480]   this, because it sounds like there's going to be a talk after this that discusses more--
[00:48:14.480 --> 00:48:20.880]   is hyperparameter settings. So when you're setting hyperparameters, hopefully you have a good
[00:48:20.880 --> 00:48:27.840]   starting point to go with where you can change individual settings a bit and see how it does.
[00:48:27.840 --> 00:48:32.960]   Otherwise, even with a fairly short YAML file, there's a pretty large space to explore once you
[00:48:32.960 --> 00:48:40.640]   start tweaking optimization settings, learning rates, number of layers, and so on. Maybe
[00:48:40.640 --> 00:48:49.280]   something that is less obvious of this type of combinatorial explosion problem is if we consider
[00:48:49.280 --> 00:48:56.000]   a five-step pipeline. So the task-- I just made up this task, but it's certainly something similar
[00:48:56.000 --> 00:49:02.160]   to things we encounter at sampling. The task is spotting diagnoses in electronic health records.
[00:49:02.160 --> 00:49:10.320]   And let's say the first try that we have is where there's a document. We extract certain sections
[00:49:10.320 --> 00:49:15.360]   that might be relevant, that might contain the information that we need. And after that is
[00:49:15.360 --> 00:49:21.040]   extracted, we tokenize it. We apply some simple filtering, maybe just removing stop words. And
[00:49:21.040 --> 00:49:29.040]   then finally, we have a classifier at the end that we train to classify the token into the correct
[00:49:29.040 --> 00:49:36.480]   diagnostics. But the problem at the first pass is that we're seeing lots of false positives at the
[00:49:36.480 --> 00:49:42.480]   required recall that we need. And so if you consider this pipeline, one thing that you
[00:49:42.480 --> 00:49:50.080]   might consider doing is adding an additional step. So maybe you see that a lot of the tokens that are
[00:49:50.880 --> 00:49:56.400]   erroneously classified as a particular disease are actually not even the correct part of speech.
[00:49:56.400 --> 00:50:03.600]   So you take your favorite part of speech tagger, you run that, and you add a filter that will
[00:50:03.600 --> 00:50:09.760]   remove certain parts of speech before feeding everything into the classifier. And maybe this
[00:50:09.760 --> 00:50:16.080]   helps. Maybe this actually fixes things for you. But this is the type of thing that I think
[00:50:16.720 --> 00:50:23.840]   it's important to be very wary of. Why is this? Well, first of all, let's say later on you
[00:50:23.840 --> 00:50:30.800]   build a much better classifier. This could bottleneck the performance of your system,
[00:50:30.800 --> 00:50:36.080]   especially if, say, the part of speech tagger is not really well-tuned to the domain.
[00:50:36.080 --> 00:50:42.960]   Another thing is this is not too bad of a pipeline. But once you start tossing in additional
[00:50:42.960 --> 00:50:48.720]   steps, going from five to six to seven to eight steps in the pipeline, then there's just many
[00:50:48.720 --> 00:50:54.640]   more points where things can break down. And so just from experiences-- and of course, there's
[00:50:54.640 --> 00:51:00.080]   also the software debt you pay in order to have all the different packages and make sure everything's
[00:51:00.080 --> 00:51:06.640]   on the right version. And so from our experiences, even though this might be a band-aid that fixes
[00:51:06.640 --> 00:51:12.640]   things, it's something where you should stop and really consider if you need something like this.
[00:51:13.520 --> 00:51:19.440]   I purposefully made that example a little bit more-- less controversial, rather.
[00:51:19.440 --> 00:51:28.000]   So the more reasonable thing to do would probably be to just allow the classifier access to both
[00:51:28.000 --> 00:51:32.400]   the original filter tokens as well as part of speech tags. And then it can use the information
[00:51:32.400 --> 00:51:37.760]   that it needs. So if part of speech tag turns out to be useful, you should learn that. Otherwise,
[00:51:37.760 --> 00:51:43.360]   it can ignore it. But even something like this-- and I don't have a good justification for it--
[00:51:43.360 --> 00:51:47.840]   even something like this, I feel a little bit queasy about putting that into a system.
[00:51:47.840 --> 00:51:59.040]   One other example-- and this is a really old example. And I don't think it relates much to
[00:51:59.040 --> 00:52:05.600]   some of the tech described in the previous talk. But it's a robot pancake maker. I'm not going to
[00:52:05.600 --> 00:52:12.720]   show the video. There is a video on YouTube. Because I'm going to mention some things that
[00:52:12.720 --> 00:52:18.080]   aren't entirely positive about it. But the idea was in 2011, there was a group that
[00:52:18.080 --> 00:52:26.640]   had a robot make a pancake. So it would pour the batter. It would cook the pancake in the pan.
[00:52:26.640 --> 00:52:30.800]   It would flip it onto a plate. And then it would take the plate and put it on a table.
[00:52:31.440 --> 00:52:37.840]   And I remember in 2011, when I saw this video, I was doing some robotics. And I got super excited.
[00:52:37.840 --> 00:52:43.040]   I printed the paper. And I was like, yeah, I'm going to read this and learn so much.
[00:52:43.040 --> 00:52:49.120]   And I mentioned this to someone more senior in the lab. And I'll never forget. They just gave
[00:52:49.120 --> 00:52:53.920]   me this book. And they said, you do realize everything in that demo was hard-coded, right?
[00:52:53.920 --> 00:53:01.360]   And at the time, that took the wind out of my sails. I still read the paper. But I certainly
[00:53:01.360 --> 00:53:10.400]   wasn't as excited about it. But looking back much later on, it's still pretty amazing that
[00:53:10.400 --> 00:53:15.280]   they got that demo to work. I mean, you can hard-code stuff as much as you want.
[00:53:15.280 --> 00:53:23.920]   But there's still so many points at which the process can fail. And maybe what the demo did
[00:53:23.920 --> 00:53:30.640]   doesn't improve generalization or the performance of robots in unstructured environments.
[00:53:31.520 --> 00:53:36.640]   But I still think it was a really impressive feat. And one other fun thing, too, if you see
[00:53:36.640 --> 00:53:40.640]   one of those videos, look for someone in the background who looks like they haven't
[00:53:40.640 --> 00:53:45.520]   showered or slept in a few days. And that's probably the grad student who was hard-coding stuff.
[00:53:45.520 --> 00:53:55.920]   So going beyond pipelines and lots of steps, one thing you might think of doing is just looking
[00:53:55.920 --> 00:54:02.720]   at images, speech, and text. And you might say, well, a medium-sized 480p image, that's
[00:54:02.720 --> 00:54:09.120]   300,000 pixels. And then if you look at an RGB image, that's three channels. So you end up with
[00:54:09.120 --> 00:54:15.600]   almost a million intake values, right? And you can do similar math for audio. So let's say it's 16
[00:54:15.600 --> 00:54:22.320]   kilohertz, which is not very high-resolution audio, in 30 seconds. And you end up with about half a
[00:54:22.320 --> 00:54:28.640]   million in 16 values. And then you look at text, right? So a three-sentence paragraph, let's say
[00:54:28.640 --> 00:54:36.720]   it's ASCII, so we can represent each character as a 0 to 255 integer, and maybe 50 characters per
[00:54:36.720 --> 00:54:45.200]   sentence, and you end up with 150 intake values, right? And you might look at this and say, hmm,
[00:54:46.000 --> 00:54:53.840]   those NLP folks have really been slacking. Representing images and text just seems a lot
[00:54:53.840 --> 00:54:58.800]   more difficult. And I've actually seen people who really should, or actually heard people who
[00:54:58.800 --> 00:55:05.200]   really should know better, express that sentiment as well. Text is just a 1D sequence as opposed to
[00:55:05.200 --> 00:55:10.320]   images, which can be 3D, speech, which you can transform into these 2D images.
[00:55:11.440 --> 00:55:18.800]   And there's also, of course, the old adage, a picture is worth a thousand words. But with text,
[00:55:18.800 --> 00:55:26.800]   one example that I frequently refer to is, if you just take this six-word sentence here, Alice
[00:55:26.800 --> 00:55:32.960]   enjoyed her trip to Wonderland. This seems like a pretty reasonable sentence, right? Outside of
[00:55:32.960 --> 00:55:38.240]   Wonderland, all these unigrams, or tokens rather, are ones that are frequently used.
[00:55:38.880 --> 00:55:46.000]   But if you search on the web for that phrase, it never appears. So the data sparsity problem
[00:55:46.000 --> 00:55:51.600]   is really extreme for text as well. And I think there's just this notion of
[00:55:51.600 --> 00:55:57.760]   brittleness, which isn't well captured by the math that I showed earlier. So there was this tweet
[00:55:57.760 --> 00:56:04.160]   from Riza Zadeh, where he talked about how there's tons of data augmentation strategies for computer
[00:56:04.160 --> 00:56:11.280]   vision, but there's not many for NLP, and the data can be a lot more brittle. So obviously, for images,
[00:56:11.280 --> 00:56:18.400]   you can apply affine transforms, you can flip it, you can add Gaussian noise. Some similar things
[00:56:18.400 --> 00:56:24.080]   for audio as well. There's this notion of manifolds, where you can perturb an image off the
[00:56:24.080 --> 00:56:30.560]   image manifold and train the system to push it back. The entire idea behind noise and auto-encoders.
[00:56:30.560 --> 00:56:36.560]   And that's something where the model can learn something meaningful in coding. But try doing
[00:56:36.560 --> 00:56:42.000]   something similar to text, and I think you'll find that it's very easily mangled. And so
[00:56:42.000 --> 00:56:51.360]   this is a case where I think very simple back of the envelopes really don't apply. And I
[00:56:51.360 --> 00:56:56.080]   really haven't seen that much research into this notion of brittleness. If anyone's seen any, I'd be
[00:56:56.080 --> 00:57:01.920]   very curious, but this is just something to keep in mind. If you're doing data augmentation for text,
[00:57:01.920 --> 00:57:09.760]   or you think it's a particular task, it might be easy. So lessons from first section on
[00:57:09.760 --> 00:57:14.240]   personal dimensionality. You can try some rough calculations, but keep in mind some complex
[00:57:14.240 --> 00:57:19.600]   entanglements. I really do think you should try and keep the system simple and try and push
[00:57:19.600 --> 00:57:24.800]   different components and software complexity into the data loss function if you can.
[00:57:25.600 --> 00:57:31.280]   Yes, this is me, perhaps just me being biased towards more end-to-end systems.
[00:57:31.280 --> 00:57:38.320]   And yeah, I think there's actually many more learnings that you can extract from just this
[00:57:38.320 --> 00:57:44.160]   notion of combinatorial explosion and personal dimensionality. Like for example, if anyone's
[00:57:44.160 --> 00:57:49.360]   used Zapier, it's this system like if this, then that, where you chain together different
[00:57:49.360 --> 00:57:57.440]   integrations. I have some zaps and they break a surprising amount of the time, and that's not
[00:57:57.440 --> 00:58:02.160]   even chaining together machine learning systems, right? That's chaining together REST APIs.
[00:58:02.160 --> 00:58:10.400]   Where are things like RPA headed? Yeah, lots more you could think about for this topic.
[00:58:10.400 --> 00:58:17.040]   But let's move on to the second one, which is iteration time. So I don't think
[00:58:18.720 --> 00:58:24.800]   folks really need convincing that iteration time is really important. I think there was a talk by
[00:58:24.800 --> 00:58:32.400]   Jeff Dean a few years back where he described two extremes of the spectrum. One is where you can
[00:58:32.400 --> 00:58:38.080]   iterate in a matter of seconds to minutes, and he described this as nirvana, interactive coding.
[00:58:38.080 --> 00:58:44.880]   And then on the other end of the spectrum, maybe training something takes months, and I think he
[00:58:44.880 --> 00:58:52.560]   said don't even try it at that extreme. Yoshua Bengio, and I'm not 100% sure he said this, but I
[00:58:52.560 --> 00:58:59.200]   believe at a deep learning summer school, he said something along the lines of in the 90s, we trained
[00:58:59.200 --> 00:59:04.400]   models for weeks. If we had only trained them for months, we may have advanced the field by
[00:59:04.400 --> 00:59:10.800]   five to ten years. So speeding up that five wheel or virtual cycle or feedback loop is obviously
[00:59:10.800 --> 00:59:17.840]   really important. One thing I don't think sees that much discussion though is which feedback
[00:59:17.840 --> 00:59:25.840]   loop should you focus on when. This figure here is one that I made when I was really frustrated,
[00:59:25.840 --> 00:59:37.520]   and the setting is it's 2017, and we had been working on an NLP system that would correct or
[00:59:37.520 --> 00:59:46.960]   denoise text, and we started off with a LSTM model, encoder decoder LSTM, and then, and I'm not sure
[00:59:46.960 --> 00:59:52.080]   if people remember this, these convolutional sequence-to-sequence models came out, and got
[00:59:52.080 --> 00:59:59.040]   really excited, thought wow, this can really help us perhaps capture longer context, and we end up
[00:59:59.040 --> 01:00:05.200]   transferring our, I think it was Theano, encoder decoder LSTM code into the convolutional
[01:00:05.200 --> 01:00:11.680]   sequence-to-sequence code, and then so we completed it. Performance, I don't think it improved that
[01:00:11.680 --> 01:00:17.600]   much, but we're still pretty excited, and then it turns out attention is all you need, and
[01:00:17.600 --> 01:00:24.000]   at that point I was just like, I'm not gonna, I don't think we're gonna switch to this new
[01:00:24.000 --> 01:00:30.400]   architecture for a little bit, and to be fair, transformers have been ascendant for I think
[01:00:30.400 --> 01:00:35.760]   pretty much three years now, almost exactly three years, so it might be the case that
[01:00:35.760 --> 01:00:44.960]   the architecture, what I'm showing here isn't true anymore. The idea is that architectures
[01:00:44.960 --> 01:00:50.160]   are changing, and so how much, if you have a fixed budget of time, how much do you want to spend on
[01:00:50.160 --> 01:00:55.920]   optimizing architectures versus focusing on other aspects? One thing which I don't think
[01:00:57.120 --> 01:01:03.360]   perhaps people spend enough time on in their, or trying to improve the speed of, is decoding,
[01:01:03.360 --> 01:01:09.200]   because that's really fast, it's a relatively unchanging procedure, so you might as well just
[01:01:09.200 --> 01:01:15.600]   get the most you can out of the decoding process, and then switch back once you're sure you have
[01:01:15.600 --> 01:01:22.320]   a really good decoding procedure. What I left out here was of course data. As an academic,
[01:01:23.200 --> 01:01:29.440]   you assume, most of the time you assume you just have a data set, but of course here we should
[01:01:29.440 --> 01:01:34.880]   include data, and that's something where you should really make sure that you're getting the data that
[01:01:34.880 --> 01:01:39.840]   you need, because of course it could be weeks to months before that it'll take to collect that data.
[01:01:39.840 --> 01:01:50.080]   Another thing which I think is also under discussed is model update iteration time,
[01:01:50.080 --> 01:01:57.680]   so by this I mean you have a model, you deploy it, how frequently do you retrain and bring a
[01:01:57.680 --> 01:02:05.440]   new model online for people to use? So one example that I saw recently was Instacart,
[01:02:05.440 --> 01:02:11.520]   they had a system for predicting the demand for certain products at stores, and of course with
[01:02:11.520 --> 01:02:17.680]   COVID that model just went way off, and so they had to bring a new model online, that model would
[01:02:17.680 --> 01:02:23.840]   focus on a much shorter time window, I think it was they reduced it from something like a month
[01:02:23.840 --> 01:02:30.480]   to a week, but of course it was necessary in this case. And then the horror story here of course is
[01:02:30.480 --> 01:02:37.200]   Microsoft Tay, where they put the model out, the chatbot model, the chatbot learned from people
[01:02:37.200 --> 01:02:42.960]   tweeting to it or chatting with it, and it picked up the worst of the internet. So that's kind of a
[01:02:44.080 --> 01:02:51.200]   cautionary tale of this. So I just put two sentences here, measure, divergence, encourage,
[01:02:51.200 --> 01:02:57.280]   and variance. I really don't know what either of those sentences mean, but I think it's something
[01:02:57.280 --> 01:03:05.280]   that is a challenge, and when I tried finding some libraries or literature, there wasn't that much
[01:03:05.280 --> 01:03:15.760]   out there. And so again, it's something where I'd be curious if folks have thoughts, and I would
[01:03:15.760 --> 01:03:23.520]   also love to see more libraries pop up that help with this problem. One thing you can contrast this
[01:03:23.520 --> 01:03:30.720]   with is speed up time, iteration time, and by this I mean how long does it take to speed up a system.
[01:03:31.840 --> 01:03:38.560]   And I should say that if you're at a company that makes more ad revenue than the GDP of
[01:03:38.560 --> 01:03:46.320]   certain countries, or if you bought a lot of Nvidia stock in 2015, you can probably just ignore
[01:03:46.320 --> 01:03:53.120]   that the content of the slide. I would too. But generally from what I've seen, speed up time,
[01:03:53.120 --> 01:04:00.560]   iteration time, is pretty fast. So in contrast to the previous model iteration, model update,
[01:04:00.560 --> 01:04:08.400]   iteration time. So why is this? Especially for research to production, researchers,
[01:04:08.400 --> 01:04:13.760]   they're not worried about the latency. They're worried about the performance on a certain
[01:04:13.760 --> 01:04:19.520]   benchmark, and so they'll go well past the point of diminishing returns oftentimes. So maybe the
[01:04:19.520 --> 01:04:25.360]   model's way bigger than it really needs to be, or maybe they're even using an ensemble of eight
[01:04:25.360 --> 01:04:32.400]   models. So right off the bat, you probably have five different ways in which you can reduce the
[01:04:32.400 --> 01:04:37.200]   size or the amount of compute needed for something by a factor of two. So right off the bat, you
[01:04:37.200 --> 01:04:42.320]   probably can get like a 30x speed up without too much effort. And the other aspect is that
[01:04:42.320 --> 01:04:49.440]   this is well defined, and I think this is actually the most important point. You have your system,
[01:04:49.440 --> 01:04:55.440]   and you want to make it go faster. Well oftentimes, you can just take your favorite profiler,
[01:04:55.440 --> 01:05:03.680]   go through, figure out what you need to focus on, and to be honest, generally I'll see that the
[01:05:03.680 --> 01:05:11.360]   hard parts, which are perhaps the convolutional kernels, the matrix multiply kernels, are done.
[01:05:11.360 --> 01:05:18.320]   And I'm certainly not saying that making matrix multiply kernels is easy. I've tried it, and
[01:05:19.280 --> 01:05:24.800]   wow, was I confused. But beyond that, other stuff that you're working on on top of it, I think
[01:05:24.800 --> 01:05:28.320]   it's possible to profile and get good speed ups.
[01:05:28.320 --> 01:05:39.760]   And yeah, if all else fails, what you can do is just deploy your solicits on AWS with
[01:05:39.760 --> 01:05:44.560]   GPU instances, watch your cache go bye-bye, and you'll figure out some way to speed things up.
[01:05:46.000 --> 01:05:50.080]   One example I think is interesting, which ties together the first couple of sections,
[01:05:50.080 --> 01:06:00.640]   is in text-to-speech in a paper called WaveNet. So this was 2016. So the context is, at the time,
[01:06:00.640 --> 01:06:06.560]   most deployed-- oh sorry, I should say that this example-- there's so many other examples. You can
[01:06:06.560 --> 01:06:12.000]   find this in neural machine translation. You can find examples in object detection, I'm sure. But
[01:06:12.000 --> 01:06:17.600]   this is just one example that I thought was interesting for TTS. So at the time, the dominant
[01:06:17.600 --> 01:06:23.600]   approach for deployed TTS systems was this so-called unit selection method. So you would
[01:06:23.600 --> 01:06:30.000]   have some text, say, PG&E will file schedules on April 20, and it should be text analysis. You go
[01:06:30.000 --> 01:06:37.920]   through this text analysis step to perform some analysis of the prosody and other attributes.
[01:06:38.480 --> 01:06:45.920]   And then what would happen is short little snippets, units, would be selected, and then
[01:06:45.920 --> 01:06:51.600]   they would be put together post-processed in order to generate the final waveform. This was how TTS
[01:06:51.600 --> 01:06:58.800]   systems worked. So if you think back to the first section on these complex pipelines,
[01:06:58.800 --> 01:07:06.480]   this is certainly one of them. And then in 2016, summer 2016, there was this WaveNet paper that
[01:07:07.280 --> 01:07:13.120]   where they still have the first steps here. So they still would run some of this step, but instead
[01:07:13.120 --> 01:07:20.240]   of doing unit selection, what they did was they would condition on that information, and then they
[01:07:20.240 --> 01:07:25.760]   would try and generate the waveform directly. Meaning if you have a 16 kilohertz waveform,
[01:07:25.760 --> 01:07:33.840]   you would generate 16,000 samples per second, which seems kind of crazy. But it got to work.
[01:07:35.040 --> 01:07:40.560]   It achieved a mean opinion score that was much higher than other deployed systems. I think people
[01:07:40.560 --> 01:07:47.200]   are saying it probably advanced the field by five years. And so this is, of course, just an example
[01:07:47.200 --> 01:07:55.520]   of how these end-to-end deep learning systems can help. But one criticism of the work when it came
[01:07:55.520 --> 01:08:01.600]   out was that it was really slow. So if you look at this, as you go along, you have to condition
[01:08:01.600 --> 01:08:07.680]   on more and more previous context in order to generate the next sample. And so it gets slower
[01:08:07.680 --> 01:08:13.120]   as you're generating these later samples. I don't know the exact number, but I think it was at least
[01:08:13.120 --> 01:08:20.400]   50x slower than real time. And so people were critical of this. I think I was also like, eh.
[01:08:20.400 --> 01:08:28.960]   But looking back, it may be slow, but if you think about the discussion of iteration time,
[01:08:29.600 --> 01:08:35.200]   it's still much faster than the amount of time they need to take to train the models.
[01:08:35.200 --> 01:08:40.480]   And you know what? I'm sure they thought that they could speed it up if they needed to. And
[01:08:40.480 --> 01:08:46.000]   sure enough, I think a year and a half later, it was deployed and 1,000x faster than the system
[01:08:46.000 --> 01:08:56.800]   that they described in this paper. So third topic, opaque models. I have this comparison
[01:08:57.600 --> 01:09:02.560]   that tries to give a sense of the trade-offs with some of these deep learning models,
[01:09:02.560 --> 01:09:08.800]   which is really kind of what I mean when saying opaque models. So what we want is something
[01:09:08.800 --> 01:09:14.960]   that's interpol, that's controllable, that's like a science. Maybe it'd be really nice if you could
[01:09:14.960 --> 01:09:21.520]   write a compact set of laws that describe how these systems behave. Unfortunately, that's not
[01:09:21.520 --> 01:09:26.960]   the case. But what we have is something that's really powerful and expressive. But if you saw
[01:09:26.960 --> 01:09:33.600]   the NIPS talk a few years ago, it can be described more like alchemy. And so there's this
[01:09:33.600 --> 01:09:39.360]   cute little analogy here of knobs on a radio versus a kite, where maybe you can kind of guide
[01:09:39.360 --> 01:09:44.160]   it, but a lot of it is just up to the mercy of the winds. So that's fine and all. I think this
[01:09:44.160 --> 01:09:50.560]   describes some of the characteristics of previous ML methods and deep neural nets. But I don't
[01:09:50.560 --> 01:09:57.040]   really think it gives you a visceral sense of what we mean when we say these opaque models.
[01:09:57.040 --> 01:10:04.240]   And so I have an incantation here to indoctrinate folks who perhaps don't really,
[01:10:04.240 --> 01:10:11.520]   don't completely buy in to the cult of opaque models. Here it is. We improve 3% on existing
[01:10:11.520 --> 01:10:16.320]   state-of-the-art by training a bidirectional LSTM and stacking it on top of another bidirectional
[01:10:16.320 --> 01:10:24.640]   LSTM. I know we had the example, Alice enjoyed her time in Wonderland, where I was saying that
[01:10:24.640 --> 01:10:29.920]   it's quite unlikely for certain sentences to occur, even at six tokens. But you know,
[01:10:29.920 --> 01:10:35.840]   there was some NLP researcher who basically said this verbatim in 2015 or 2016. I probably said
[01:10:35.840 --> 01:10:44.560]   this verbatim in 2015 and 2016. And on the one hand, it's pretty amazing that you have this level
[01:10:44.560 --> 01:10:51.280]   of abstraction in a sentence. But on the other hand, does anybody really know what's going on
[01:10:51.280 --> 01:10:57.360]   here when you say something like this? So I think that gives you a more visceral sense of what we
[01:10:57.360 --> 01:11:04.320]   mean by these opaque or black box models. And if you're really hardcore, you can get the equations
[01:11:04.320 --> 01:11:12.560]   tattooed as well. So one other view is, I saw this tweet recently from Kareem Karr,
[01:11:12.560 --> 01:11:19.520]   who I think is a statistician. He posted this video and captioned it, "How statistics people
[01:11:19.520 --> 01:11:23.760]   feel watching people in machine learning solve problems." So let me play that really quick.
[01:11:23.760 --> 01:11:31.200]   And when I saw this video, I started cracking up because
[01:11:31.200 --> 01:11:40.640]   it's so true. Sometimes the methods we use for certain tasks can really feel like overkill.
[01:11:41.680 --> 01:11:45.200]   One fun exercise, actually, as you watch this video is you can try and associate
[01:11:45.200 --> 01:11:53.520]   cliffs with certain architectures, like Perceptron, the net, Siamese net.
[01:11:53.520 --> 01:12:03.440]   Anyways, yeah. So what was the point I was trying to make here? What's the goal, though? If
[01:12:05.120 --> 01:12:14.240]   your world is just popping balloons and the entire reward function is how you can pop balloons in
[01:12:14.240 --> 01:12:22.480]   really exquisite ways, I think it's hard to argue against what this guy is doing. But I think as an
[01:12:22.480 --> 01:12:29.840]   onlooker and taking a step back a little bit, it seems pretty dangerous. And so I think beyond just
[01:12:31.040 --> 01:12:36.960]   this being overkill for certain tasks, I think it's also an apt analogy, because sometimes
[01:12:36.960 --> 01:12:40.800]   you should be a little bit cautious before deploying these models.
[01:12:40.800 --> 01:12:54.320]   So one case that illustrates that is with what I think of as hard constraints. And to
[01:12:58.400 --> 01:13:03.120]   demonstrate this, I think it's useful to think about a couple of extremes. One is click-through
[01:13:03.120 --> 01:13:08.560]   rate. So say you're trying to improve the click-through of news articles or product listings.
[01:13:08.560 --> 01:13:13.200]   And then the other is vehicle detection, presumably for autonomous vehicles.
[01:13:13.200 --> 01:13:24.240]   Those two require very different approaches. So obviously, for click-through rates, if somebody
[01:13:24.240 --> 01:13:30.240]   doesn't click a particular article, it's OK. Whereas on the other hand, if a vehicle isn't
[01:13:30.240 --> 01:13:37.440]   consistently detected, that can have fatal consequences. With these multi-armed bandit
[01:13:37.440 --> 01:13:43.280]   problems, there's an entire notion of exploration versus exploitation. Whereas for vehicle detection,
[01:13:43.280 --> 01:13:51.840]   hopefully you're only doing that in simulation. So I also include this link to a pretty useful
[01:13:51.840 --> 01:13:56.960]   introduction to different types of evaluation metrics. And I think it's important,
[01:13:56.960 --> 01:14:04.960]   really important for research to production. Because say you have a trade-off curve,
[01:14:04.960 --> 01:14:10.000]   and you're optimizing to hit the corner of the trade-off curve. So say precision recall,
[01:14:10.000 --> 01:14:16.320]   and you need really high precision, really high recall. Depending on how close you are to that
[01:14:16.320 --> 01:14:26.400]   corner, that makes a lot of decision for you on things that you can and can't do, given, say,
[01:14:26.400 --> 01:14:34.480]   the amount of data that you have. So yeah, and this goes beyond training and validation.
[01:14:34.480 --> 01:14:45.440]   So one thing that I think could also be discussed more is safeguards. So say you've built a really
[01:14:45.440 --> 01:14:53.120]   nice text generation system, and you want to deploy it. And sometimes the system will generate
[01:14:53.120 --> 01:14:56.960]   profanity, because the training data had profanity. So you want a profanity filter.
[01:14:56.960 --> 01:15:03.760]   I looked for a profanity filter a couple years back. And let's just say there's no hugging face
[01:15:03.760 --> 01:15:11.360]   for profanity filtering. Maybe there is now. At the time, there certainly wasn't. And I don't think
[01:15:11.360 --> 01:15:17.680]   you want to build this amazing text generation system, and then build a really crummy, thrown
[01:15:17.680 --> 01:15:26.240]   together profanity filter at the end. Another thing which I think is useful to do in these
[01:15:26.240 --> 01:15:32.080]   cases is a different sort of validation set. Not necessarily adversarial data, where you have some
[01:15:32.080 --> 01:15:40.720]   attack vector, but say data that users may input in a perfectly normal interaction with your system.
[01:15:41.360 --> 01:15:47.200]   But that ends up causing really pathological behaviors. I think that is actually really
[01:15:47.200 --> 01:15:52.000]   interesting, and it can be sometimes really hard to figure out as well. And of course,
[01:15:52.000 --> 01:15:57.680]   monitoring and updates. But really, I think just this slide, what I'm trying to get across is
[01:15:57.680 --> 01:16:04.160]   there's a depth and rigor that exists for training models, validating models, that
[01:16:04.160 --> 01:16:10.640]   really doesn't exist, I think, when you're trying to deploy these models and put them out into the
[01:16:10.640 --> 01:16:17.520]   wild, if you will. With bias and fairness as well, I saw this tweet recently from Hannah Wallach
[01:16:17.520 --> 01:16:23.120]   that described something similar. So one of our students, Su Lin, surveyed 146 papers looking at
[01:16:23.120 --> 01:16:31.280]   bias in NLP systems. And they had a few conclusions and recommendations. Among those, that people in
[01:16:31.280 --> 01:16:36.080]   the papers tend to just focus on NLP literature. They don't really go outside of the literature
[01:16:36.800 --> 01:16:42.000]   to say literature, talk about social hierarchies. They all describe some metric regarding bias,
[01:16:42.000 --> 01:16:47.280]   and then just say that it's good to lower that metric without really thinking about the downstream
[01:16:47.280 --> 01:16:53.920]   effects. And I think the third point they made was just seeing more research, going from research
[01:16:53.920 --> 01:17:01.440]   to application. And so here, there's a certain amount of looking at the downstream effects
[01:17:01.440 --> 01:17:11.360]   and applying rigor to the process that needs to be more of. But assuming you build a system and
[01:17:11.360 --> 01:17:20.240]   you think that the constraints that you have make sense, you set up safeguards, and you think about
[01:17:20.240 --> 01:17:25.280]   the downstream effects, and you still want to deploy it, there can still be funny little behaviors.
[01:17:25.280 --> 01:17:32.160]   So this is an old result from a while ago of Google's neural machine translation system,
[01:17:32.160 --> 01:17:38.800]   where a user typed "egu, egu, egu" over and over. And because the system has never seen this type
[01:17:38.800 --> 01:17:44.480]   of input before, it would just hallucinate this crazy gibberish. And there was this article from
[01:17:44.480 --> 01:17:49.680]   Douglas Hofstadter, the author of "Godot, Asher, Bach," where he described these systems. And he
[01:17:49.680 --> 01:17:57.280]   used just the perfect word to describe this behavior, which is that it wobbles. So the user
[01:17:57.280 --> 01:18:02.640]   inputs "egu, egu, egu," the model goes "wobble, wobble, wobble," and you end up with this type
[01:18:02.640 --> 01:18:11.040]   of behavior. So I think Google's neural machine translation team published a post recently talking
[01:18:11.040 --> 01:18:16.160]   about how they improved things across the board, including with these hallucinations, but still
[01:18:16.160 --> 01:18:26.560]   something to look out for. I don't know what you feel when you read a paper that describes
[01:18:26.560 --> 01:18:32.880]   an amazing result that's almost like an existence proof. You weren't quite sure if it was even
[01:18:32.880 --> 01:18:40.240]   possible before, or the sensation you get when you deploy a system or see a deep learning system
[01:18:40.240 --> 01:18:45.840]   deployed, and you can tell it's one just by the strange little ways in which it misbehaves. But
[01:18:45.840 --> 01:18:53.520]   to me, it feels like I'm watching something like this. It's really such a thrill. And I think that
[01:18:53.520 --> 01:19:01.280]   if you think about how to iterate quickly, think about the trade-offs of these really opaque
[01:19:01.280 --> 01:19:09.200]   models, and also factor in the ways in which different factors of the problem can compound
[01:19:09.200 --> 01:19:16.720]   in strange ways, if you haven't, you'll deploy one of these systems or deploy more of these systems,
[01:19:16.720 --> 01:19:22.000]   and there's a good chance that it will work better than anything else that anyone's ever deployed.
[01:19:22.000 --> 01:19:25.800]   That's it. Thanks. >>
[01:19:25.800 --> 01:19:37.760]   Nice. Thank you so much. All right. Let's do questions. I see some popping up already.
[01:19:37.760 --> 01:19:44.080]   Oh, my God. Hans got a three-pointer. Let's dive in. So the first question is,
[01:19:44.080 --> 01:19:49.920]   what data annotation tool do you prefer for annotating text data for, say,
[01:19:49.920 --> 01:19:59.360]   NER or multi-ring classification? >> Yeah. It's a good question. Data
[01:19:59.360 --> 01:20:09.200]   is obviously really critical. So I have to admit, with us, due to budgetary constraints,
[01:20:09.200 --> 01:20:14.400]   we would love to be able to annotate data, but generally, we will try and figure out
[01:20:14.400 --> 01:20:21.280]   data augmentation schemes based off of the existing data that we already have. I know there's
[01:20:21.280 --> 01:20:26.880]   great annotation systems out there. I'm sure there's figure eights with Lucas, of course.
[01:20:28.560 --> 01:20:33.200]   But – and I think – oh, gosh, what was it? Prodigy? There's some system,
[01:20:33.200 --> 01:20:37.200]   other system out there, but I'm not really familiar with them.
[01:20:37.200 --> 01:20:44.960]   >> You mentioned data augmentation. What kinds of data augmentation methodologies are you using?
[01:20:44.960 --> 01:20:54.000]   >> Sure. Yeah. So for text in particular, as I mentioned, the naive methods can sometimes
[01:20:54.000 --> 01:20:59.760]   really mingle the text or change the meaning. So I think the most obvious thing people try is
[01:20:59.760 --> 01:21:06.160]   you will drop certain words or you will swap them or insert additional words. It could be at
[01:21:06.160 --> 01:21:11.200]   the character level as well. And the crazy thing is this has actually worked well for all these
[01:21:11.200 --> 01:21:18.080]   different pre-training methods. I didn't see that coming. But for us, what we found actually worked
[01:21:18.080 --> 01:21:23.120]   was actually this method called back translation, where you take your existing parallel corpus.
[01:21:23.120 --> 01:21:29.840]   So say you have a corpus from English to French, and then you will take additional monolingual
[01:21:29.840 --> 01:21:34.560]   French data, train a reverse system, and use your system to actually translate it back to English,
[01:21:34.560 --> 01:21:41.920]   and then you have additional half-synthesized parallel data. So back translation is really
[01:21:41.920 --> 01:21:49.680]   useful. >> Thanks. Awesome. The second question from Han is what do you recommend as a metric or
[01:21:49.680 --> 01:21:56.160]   algorithm to monitor input data -- to monitor input data drift for NLP?
[01:21:56.160 --> 01:22:08.000]   >> Yeah. Input data drift. So, yeah, I had the slide on monitoring. We have certain quality
[01:22:08.000 --> 01:22:14.160]   metrics at the end where we look at how often users will accept suggestions that we make.
[01:22:15.600 --> 01:22:22.160]   Unfortunately, I don't have any great method to share for measuring the input drift. I know that
[01:22:22.160 --> 01:22:29.760]   there's -- the first thing that comes to mind is something like just looking at comparing the bag
[01:22:29.760 --> 01:22:37.680]   of words, bag of unigrams and bigrams. But, yeah, unfortunately, I don't have a great suggestion.
[01:22:37.680 --> 01:22:46.080]   >> Nice. And then Han's third question is what's a good alternate way to speed up text generation
[01:22:46.080 --> 01:22:52.800]   in addition to feeding AWS a lot of money or reduce beam search sampling size?
[01:22:52.800 --> 01:23:01.760]   >> Yeah. So those are both good ways. Reducing beam search. You can -- if people haven't tried,
[01:23:01.760 --> 01:23:07.920]   you can run with very low beam search and usually get very good results. Other ways,
[01:23:07.920 --> 01:23:16.000]   distillation is one method. Quantization, if you can, that may prevent you from using your
[01:23:16.000 --> 01:23:22.320]   favorite framework. And then just also making sure that you're using the best libraries.
[01:23:22.320 --> 01:23:29.360]   If you're running on CPU, things like -- it's called SIMD or ADX. If you're running on GPU,
[01:23:29.360 --> 01:23:36.160]   there's all those libraries as well. >> Cool. Charles, did you have a question?
[01:23:36.160 --> 01:23:40.720]   Because there's someone muted as well. >> Yeah. One thing, I'm just going to drop
[01:23:40.720 --> 01:23:45.920]   this in the chat. Just for natural language processing data augmentation, there was a
[01:23:45.920 --> 01:23:55.520]   nice presentation at one of our previous salons by Jack Morris on their text attack library.
[01:23:55.520 --> 01:24:02.000]   And it's kind of like the things you were describing, word substitution based off of
[01:24:02.000 --> 01:24:12.480]   word vector similarity and synonym thesauruses. It's nowhere near as good as the perturbations
[01:24:12.480 --> 01:24:17.280]   and translations that we can apply to images and audio, but it is a step in that direction.
[01:24:17.280 --> 01:24:24.640]   And it's a nice package. I took a look at it after their talk, and it was very good. So might be
[01:24:24.640 --> 01:24:31.840]   worth checking out. But I wanted to pull in actually one of the questions from YouTube.
[01:24:31.840 --> 01:24:39.360]   Do you do any work with audio natural language processing, or is it primarily text?
[01:24:39.360 --> 01:24:48.480]   >> Ah. So by audio natural language processing, do they mean going from speech to text or
[01:24:48.480 --> 01:24:53.280]   processing the audio in another way? >> Yeah. I mean, their question is about
[01:24:53.280 --> 01:24:56.080]   vernaculars and accents. >> Oh, gotcha.
[01:24:56.080 --> 01:25:02.960]   >> So my presumption is that you need an audio in order to have an accent, right?
[01:25:02.960 --> 01:25:09.040]   So do you work with natural language processing that uses recordings of natural language to do
[01:25:09.040 --> 01:25:15.040]   some downstream NLP tasks? >> Right. So we do not. But I know
[01:25:15.760 --> 01:25:25.680]   Awni Hanun, A-W-N-I-H-A-N-N-U-N, wrote a post maybe a year or two ago about some of the phenomena
[01:25:25.680 --> 01:25:32.320]   with speech and accents. He had this great post about remaining challenges in speech recognition
[01:25:32.320 --> 01:25:38.160]   now that we have these speech systems trained on a ton of data. So that might be helpful. But,
[01:25:38.160 --> 01:25:44.960]   yeah, we don't work with that type of data. >> Cool. Yeah. Thanks. I thought, found that
[01:25:44.960 --> 01:25:48.640]   question particularly interesting because I was reading some papers earlier this week
[01:25:48.640 --> 01:25:54.720]   about how court transcription done by humans, actually just the quality of the transcriptions
[01:25:54.720 --> 01:26:00.240]   when applied to African-American vernacular English is substantially worse than to prestige
[01:26:00.240 --> 01:26:05.440]   dialects of English. And this is the kind of thing we'd like to be able to maybe use machine learning
[01:26:05.440 --> 01:26:12.880]   to automate OA biases, but contemporary methods can suffer from the same biases that humans do.
[01:26:12.880 --> 01:26:21.520]   >> Yeah. Definitely. And I think that is something where perhaps we don't have the right
[01:26:21.520 --> 01:26:29.440]   data right now, but it is a nice part about the ML system as well is that if we can get that,
[01:26:29.440 --> 01:26:37.520]   it can adapt to it. >> Salim asks, would you tell us more about
[01:26:37.520 --> 01:26:42.800]   Sapling and also your transition from being a PhD student to the industry?
[01:26:42.800 --> 01:26:52.880]   >> Yeah, sure. So Sapling.ai, you can check out the website. We have this animated screenshot
[01:26:52.880 --> 01:26:59.520]   that shows a few of the different features from autocomplete to correcting the text to
[01:26:59.520 --> 01:27:09.360]   suggesting replies in chat. But regarding going from PhD to this, actually started working on
[01:27:09.360 --> 01:27:18.720]   some aspects of this while before I had finished. And there's advantages to that, but I personally
[01:27:18.720 --> 01:27:24.080]   would not recommend it because it can also cause some misalignments. So if you are thinking of
[01:27:24.800 --> 01:27:30.320]   doing a PhD but you're interested in a startup as well, I would talk to some people who have
[01:27:30.320 --> 01:27:37.040]   done that. And I imagine the suggestion you'll get is finish your PhD, take a break, and then
[01:27:37.040 --> 01:27:40.400]   get back to startup stuff if you're interested in it.
[01:27:40.400 --> 01:27:44.960]   >> You were doing both at the same time? That's nuts.
[01:27:44.960 --> 01:27:53.280]   >> Yeah. If you can call it that. Yeah. >> Which one suffered? I'm curious.
[01:27:53.280 --> 01:27:56.000]   The startup or the PhD? >> I'm sure both suffered.
[01:27:56.000 --> 01:28:06.880]   >> So someone on YouTube asks, this is a slightly longer question. So they're a linguistic
[01:28:06.880 --> 01:28:14.560]   major, then undergrad, and they're in their senior year. And they've been doing phonology research,
[01:28:14.560 --> 01:28:20.160]   which has given them insight into formal language theory and applications to natural language.
[01:28:20.160 --> 01:28:25.120]   And they were wondering if they went to grad school to study just linguistics,
[01:28:25.120 --> 01:28:32.960]   would they be hireable for NLP jobs? If so, what should they leverage in their current
[01:28:32.960 --> 01:28:43.680]   program to be competitive for NLP jobs? >> Yeah. So one of my advisors was Dan
[01:28:43.680 --> 01:28:51.200]   Dandrowski. I think he's in the CS and linguistics department. I'm guessing his heart is really
[01:28:51.200 --> 01:28:56.640]   more on the linguistic side of things. I'm not sure. But I think linguistics right now,
[01:28:56.640 --> 01:29:03.520]   there's so much going on in computational linguistics. And I wish I had learned more
[01:29:03.520 --> 01:29:10.800]   during grad school, actually. But I feel now more than ever, there's lots of overlap there.
[01:29:11.440 --> 01:29:17.440]   And I think you can do really cool stuff as well that isn't just a function of amount of data and
[01:29:17.440 --> 01:29:23.360]   model size. So this may also be a good time. Yeah, I think it's really interesting. And
[01:29:23.360 --> 01:29:27.360]   if you do cool work, I don't see why it would fall on you.
[01:29:27.360 --> 01:29:31.200]   >> Thanks. They also asked, are you looking for an intern?
[01:29:31.200 --> 01:29:38.160]   >> Oh, sapling. We currently are not. We wish we were positioned where we were, but currently no,
[01:29:38.160 --> 01:29:44.960]   sadly. >> So as somebody who also went very
[01:29:44.960 --> 01:29:51.600]   recently from academia to industry, I found the point that you made about how a lot of academic
[01:29:51.600 --> 01:29:56.400]   work is obsessed with things that bring very diminishing returns. So this sort of like soda
[01:29:56.400 --> 01:30:03.680]   fetish that people have, which is also reflected in Kaggle, where there's these giant ensemble
[01:30:03.680 --> 01:30:08.560]   models that get just like a tiny fraction of additional performance on some test set.
[01:30:08.560 --> 01:30:13.840]   So I'm wondering, you're out of academia now, but do you have any thoughts about what kind
[01:30:13.840 --> 01:30:19.920]   of culture shift might be necessary? What changes and incentives could make it so that people do
[01:30:19.920 --> 01:30:26.400]   research that would be more helpful for people working in applications in industry?
[01:30:26.400 --> 01:30:42.320]   >> Yeah, it's a really good question. And yeah, I don't think I have that great a feel for what's
[01:30:42.320 --> 01:30:46.640]   best to do here, but I do think that there have been some interesting initiatives here. Like,
[01:30:46.640 --> 01:30:54.080]   for example, folks trying to push for how well you can perform on a certain budget.
[01:30:55.920 --> 01:31:02.720]   I think that the interest in leaderboards and the amount of compute as well has been
[01:31:02.720 --> 01:31:10.720]   largely driven by industry with lots of budget who are trying to compete
[01:31:10.720 --> 01:31:15.440]   on these for development, perhaps for the brand of a particular lab.
[01:31:15.440 --> 01:31:25.040]   But yeah, I think like you said, folks are realizing that there needs to be a broader
[01:31:25.040 --> 01:31:33.520]   range of work. And if you're in academia, it may make sense to focus on perhaps more fundamental
[01:31:33.520 --> 01:31:38.960]   aspects of machine learning, of deep learning, as opposed to competing on leaderboards. But
[01:31:38.960 --> 01:31:44.400]   I'm sure I'll continue and I really don't have a great suggestion.
[01:31:44.400 --> 01:31:53.920]   >> Yeah, that's fair. It's a tough, tough question, definitely. And yeah, maybe the scale of the
[01:31:53.920 --> 01:31:58.800]   problem is exemplified actually by the point you made about, Yashua Bengio saying that if only
[01:31:58.800 --> 01:32:03.280]   they'd been training for months in the 90s, they would have advanced like five or 10 years faster.
[01:32:03.280 --> 01:32:08.880]   Like the people working on neural networks in the 90s were getting the pants beat off of them by
[01:32:08.880 --> 01:32:16.720]   SVMs, Gaussian process models, sophisticated approaches to linear and logistic regression.
[01:32:16.720 --> 01:32:21.600]   But it was only this sort of like fundamental shift that was able to get us to where we are
[01:32:21.600 --> 01:32:24.560]   today. It's really hard to figure out how to incentivize that.
[01:32:24.560 --> 01:32:29.040]   >> Yeah. Do you have some additional thoughts there about how to
[01:32:29.040 --> 01:32:33.040]   intensify that behavior? Have you thought about it?
[01:32:33.040 --> 01:32:40.560]   >> I got a lot of thoughts. Not all of them suitable for recording. But I would say number
[01:32:40.560 --> 01:32:48.480]   one is increasing resources to academia to allow people to get out of the short feedback loop of
[01:32:48.480 --> 01:32:54.640]   having to publish in order to get grants in order to publish. If people were less squeezed, they
[01:32:54.640 --> 01:33:02.400]   could think a little bit longer term. But yeah. >> All right. Thank you, Zeng. That was really
[01:33:02.400 --> 01:33:04.240]   good. >> Thank you.
[01:33:04.240 --> 01:33:13.920]   >> See ya. All right. Up next, we have Stacy. Like Yuka said, Stacy works at Leeds and Bicent. She's
[01:33:14.560 --> 01:33:20.240]   our only deep learning engineer now. She's amazing. Today she's going to talk about how
[01:33:20.240 --> 01:33:24.960]   to do hyperparameter tuning with sweeps. Welcome, Stacy.
[01:33:24.960 --> 01:33:34.800]   >> Thank you. Thanks, Lavanya. I'm going to figure out how to project here. One sec.
[01:33:34.800 --> 01:33:42.720]   Can folks see? >> Yes.
[01:33:42.720 --> 01:33:50.960]   >> Yeah. Great. Yeah. So, I'm Stacy. I'm a deep learning engineer here at Weights and Biases.
[01:33:50.960 --> 01:33:57.280]   And I'm here to talk about how to tune your deep learning models faster. I won't get into much
[01:33:57.280 --> 01:34:02.800]   theory or math behind different algorithms for hyperparameter optimization. What's guaranteed
[01:34:02.800 --> 01:34:07.680]   to converge faster, what kind of model-based optimization you should use. And I'm not going
[01:34:07.680 --> 01:34:12.160]   to tell you the best type of parameters to pick for a given problem because the space just varies
[01:34:12.160 --> 01:34:17.760]   so much. Instead, I'm going to focus on what I've learned from reproducing and fine tuning a bunch
[01:34:17.760 --> 01:34:22.960]   of different models over the last two years. In my setup, I'm working some code for an existing
[01:34:22.960 --> 01:34:28.400]   model from GitHub, instrumenting it with logging and visualizations, and then trying to improve
[01:34:28.400 --> 01:34:34.480]   the performance or at least report some interesting analysis of what I learned, generally as fast as
[01:34:34.480 --> 01:34:38.880]   I can. And I do that so I can showcase more examples of machine learning on our platform,
[01:34:40.160 --> 01:34:43.600]   but also be able to say something interesting and helpful each time.
[01:34:43.600 --> 01:34:49.440]   And in that process, I've learned a lot about the explore/exploit tradeoff, which
[01:34:49.440 --> 01:34:55.040]   interestingly came up in the previous talk. That was cool. Yeah. And I'll show lots of
[01:34:55.040 --> 01:34:59.040]   screenshots from Weights and Biases because that's where I track all my experiments.
[01:34:59.040 --> 01:35:03.200]   But the general approach and tips should work in any developer environment and hopefully with
[01:35:03.200 --> 01:35:08.640]   any framework. I'm also giving this talk tomorrow, and it's supposed to not be a product pitch,
[01:35:08.640 --> 01:35:14.640]   so I really appreciate any feedback. And I'm generally aiming to have it be not
[01:35:14.640 --> 01:35:18.560]   too Weights and Biases specific, even though obviously I'm very excited about Weights and
[01:35:18.560 --> 01:35:24.800]   Biases. If you haven't heard about the explore/exploit tradeoff before, it's a useful
[01:35:24.800 --> 01:35:31.280]   handle for many life situations. It comes from the multi-armed bandit problem, where classically
[01:35:31.280 --> 01:35:39.360]   you're in a casino, and all the slot machines have a different unknown and probably really
[01:35:39.360 --> 01:35:45.200]   tiny probability of paying out money. So how do you maximize your winnings? You could go try a
[01:35:45.200 --> 01:35:50.960]   bunch of machines and this explore strategy, or you could pick one and play it over and over and
[01:35:50.960 --> 01:35:54.320]   hope that you picked a really good one. That's the exploit strategy.
[01:35:56.880 --> 01:36:02.800]   I'm not super into the casino metaphor, but I think it does bring out a darker side of the
[01:36:02.800 --> 01:36:14.000]   process that's similar across both of these things, which is that it's hard to stop. You know,
[01:36:14.000 --> 01:36:18.560]   that's it. I can't lose any more money, or this is as good as this model gets. I don't know about
[01:36:18.560 --> 01:36:27.520]   that last 0.05%. It's hard to tell what's really happening versus how you're feeling about it,
[01:36:27.520 --> 01:36:32.400]   the story you're telling yourself. So, for example, I might have just lost 20 times in a row,
[01:36:32.400 --> 01:36:37.600]   and I might be thinking I'm doing great. I'm having a lot of fun. This is awesome. Or, you
[01:36:37.600 --> 01:36:41.520]   know, I might be thinking, oh, this is awful. This is the worst. This accuracy is never going to get
[01:36:41.520 --> 01:36:50.000]   above 70%. I give up. As often happens, I think the story and what you experience is more important.
[01:36:50.000 --> 01:36:56.000]   What are you getting out of the process? What can you do better next time? And so, I'm going to tell
[01:36:56.000 --> 01:37:00.400]   a few stories of how I've encountered this, and they definitely won't point you to the best
[01:37:00.400 --> 01:37:06.080]   hyperparameter slot machine, but hopefully you'll be able to tune your models faster next time,
[01:37:06.080 --> 01:37:10.720]   or at least avoid repeating my mistakes by thinking about the constraints for each
[01:37:10.720 --> 01:37:16.720]   hyperparameter sweep, making sure you write down what you found from the sweep and what to do next
[01:37:16.720 --> 01:37:24.640]   time, and then improving that way over time. So, what is hyperparameter tuning? Hyperparameter
[01:37:24.640 --> 01:37:29.280]   tuning or optimization is the task of finding the best hyperparameters for a learning algorithm,
[01:37:29.280 --> 01:37:33.520]   which for the purposes of this talk is going to be a convolutional or recurrent neural network.
[01:37:33.520 --> 01:37:39.040]   GANs and reinforcement learning could probably take their whole own separate talk on how to
[01:37:39.040 --> 01:37:45.680]   tune those. And this tuning could be done entirely by hand, one experiment at a time,
[01:37:45.680 --> 01:37:50.240]   one hyperparameter at a time, but of course that would be super slow, and we really like going
[01:37:50.240 --> 01:37:55.760]   meta, and so we'll go meta on this manual practice of trying hyperparameters and use algorithms.
[01:37:55.760 --> 01:38:03.760]   There's a lot of research and different tools, both paid and open source, for this algorithmic
[01:38:03.760 --> 01:38:08.480]   or automated aspect of hyperparameter tuning, and basically we're trying to figure out
[01:38:08.480 --> 01:38:14.960]   how to save time and optimally pick the next combination of hyperparameters each time.
[01:38:14.960 --> 01:38:20.400]   And the most common methods are manual, which I described, you just do it,
[01:38:20.400 --> 01:38:27.200]   grid, which tries all possible combinations, and that's useful if your search space is small and
[01:38:27.200 --> 01:38:32.480]   specific and you want to guarantee finding the best combination. Random search is generally
[01:38:32.480 --> 01:38:37.920]   faster and especially recommended if you have a larger search space, because it's going to sample
[01:38:37.920 --> 01:38:41.840]   more of the space and give you a sense for various possibilities faster than grid search,
[01:38:41.840 --> 01:38:44.880]   but it's not guaranteed to find the best combination.
[01:38:44.880 --> 01:38:51.520]   Bayesian optimization tries to build a probabilistic model, or guess at the function
[01:38:51.520 --> 01:38:56.240]   that maps from your hyperparameters to your target measure of your performance, which is usually
[01:38:56.240 --> 01:39:02.880]   accuracy on the validation set. And this Bayesian model of your actual model that you're training
[01:39:02.880 --> 01:39:06.560]   updates from every experiment. So we tried these hyperparameters, got this result,
[01:39:07.360 --> 01:39:13.840]   and it actually aims to balance the explore-exploit tradeoff, such that it tries values that it needs
[01:39:13.840 --> 01:39:18.640]   to learn about to explore the space, and also tries ones that it expects to be near optimal
[01:39:18.640 --> 01:39:24.400]   and gets you closer to the objective. In practice, this works faster than grid and random, because
[01:39:24.400 --> 01:39:31.920]   it runs fewer bad experiments. One caveat, though, is that it needs a metric to optimize, so you
[01:39:31.920 --> 01:39:36.160]   would need to know what that is and compute that explicitly. If you're just playing around initially,
[01:39:36.160 --> 01:39:41.760]   a random or grid search might be pretty good. And there's a long tail of other methods that I'm not
[01:39:41.760 --> 01:39:46.960]   going to get into. You can actually do gradient-based optimization with respect to specific
[01:39:46.960 --> 01:39:53.200]   hyperparameters. You can do early stopping so that you detect earlier if a certain run is not
[01:39:53.200 --> 01:39:59.680]   promising and don't waste compute on it. You can do population-based training, where actually you
[01:39:59.680 --> 01:40:03.920]   have multiple independent processes that learn both hyperparameter values and network weights,
[01:40:04.640 --> 01:40:08.000]   so you don't even need to be using the same architecture for different processes in your
[01:40:08.000 --> 01:40:17.920]   suite. So, there are many, many options here, and what I like to ask myself is, am I at that stage
[01:40:17.920 --> 01:40:21.440]   where I've thought through the specific model that I'm tuning and the specific problem that I'm
[01:40:21.440 --> 01:40:27.360]   solving, and there's nothing else clever I can do, I just want to throw GPUs at this hyperparameter
[01:40:27.360 --> 01:40:32.160]   state and get my answer and go think about something else. And I think if that's the case,
[01:40:32.160 --> 01:40:36.240]   I would suggest just using something that's model-based optimized, like a Bayesian sweep,
[01:40:36.240 --> 01:40:43.120]   because it's probably going to work much faster. But I also think the more interesting and
[01:40:43.120 --> 01:40:47.760]   challenging and rewarding work of hyperparameter tuning happens before you get to that stage,
[01:40:47.760 --> 01:40:53.360]   where you just want to throw compute at it. And we really don't have good guaranteed algorithms
[01:40:53.360 --> 01:40:58.720]   for it yet, which is why many of us still have jobs. And that's what I'm going to try to talk
[01:40:58.720 --> 01:41:02.400]   about. And obviously, as we go through this, please keep in mind that these are general
[01:41:02.400 --> 01:41:07.680]   approaches, and your particular task might be different, and especially in machine learning,
[01:41:07.680 --> 01:41:16.400]   there's exceptions to every rule. So, before you start trying different hyperparameters,
[01:41:16.400 --> 01:41:21.840]   you decide what a hyperparameter even is, and there's an important distinction between the
[01:41:21.840 --> 01:41:25.840]   regular parameters of the network, like the model weights and biases, which are learned,
[01:41:26.400 --> 01:41:31.360]   or iteratively improved during the training process, and hyperparameters, which are
[01:41:31.360 --> 01:41:36.160]   configured outside of the training process, and generally, they're fixed for the duration of your
[01:41:36.160 --> 01:41:43.680]   run, although some could be adaptive, like learning rate. So, for a given network, this could be the
[01:41:43.680 --> 01:41:48.960]   number of training epochs, learning optimizer type, batch size, weight decay, and a lot of other
[01:41:48.960 --> 01:41:54.720]   stuff. The network architecture itself, you could think of that as a hyperparameter. You could start
[01:41:54.720 --> 01:42:01.440]   from a well-known base network, like Inception or ResNet. You could tune a certain number of layers,
[01:42:01.440 --> 01:42:07.200]   freeze them, tune some variable other number, and if you're designing a network from scratch,
[01:42:07.200 --> 01:42:11.520]   then the number and size and type and connection patterns and all of that could all be
[01:42:11.520 --> 01:42:19.040]   hyperparameters. So, one useful way to think about whether you're going to fix a hyperparameter
[01:42:19.040 --> 01:42:24.000]   and keep it constant, or you're going to vary and explore it, is how well you can predict its
[01:42:24.000 --> 01:42:29.440]   effect in advance and how it will affect the overall run time for your suite. So, a first
[01:42:29.440 --> 01:42:35.280]   easy one here is epochs, or the number of passes through your training data, and this one's pretty
[01:42:35.280 --> 01:42:40.080]   well understood. If I'm running repeated experiments, I want to set this high enough so
[01:42:40.080 --> 01:42:44.400]   that I can see that my model's learning and there's definite improvement, but not so high that
[01:42:44.400 --> 01:42:49.920]   I'm wasting time on a very slowly converging curve. So, usually, I'll try something like 10,
[01:42:49.920 --> 01:42:54.880]   if I can get away with it, as long as I see some improvement in validation loss
[01:42:54.880 --> 01:43:01.680]   and make sure that I'm not memorizing the data. I like to set this to the minimum to see improvement.
[01:43:01.680 --> 01:43:08.000]   Another obvious hyperparameter that generally has a reliable effect is the size of your training data.
[01:43:08.000 --> 01:43:13.360]   If you have a representative sample and your split is unbiased, then generally,
[01:43:13.360 --> 01:43:18.720]   the more training data you have, the better the model will learn. I follow the conventional wisdom
[01:43:18.720 --> 01:43:25.680]   of 80 for train, 10 for validation, and 10 for test. Those are percentages, and you don't look
[01:43:25.680 --> 01:43:30.640]   at the 10% that's for testing until you're done with the whole optimization process.
[01:43:30.640 --> 01:43:39.280]   And I actually lower these for a run, an experiment in a suite. I take about 20 to 50% of each suite
[01:43:39.280 --> 01:43:48.080]   as - sorry, of each split. And often, I'm testing my code end to end on a very small
[01:43:48.080 --> 01:43:54.320]   subset anyways of the training data to make sure that it works. And usually, I try to do this to
[01:43:54.320 --> 01:44:00.720]   get the run time down to a few minutes. Of course, that can be impossible in some applications.
[01:44:00.720 --> 01:44:08.720]   And, of course, a general caveat here that all of these things interrelate, especially with
[01:44:09.200 --> 01:44:12.400]   all of the other parameters that I'm going to try to vary.
[01:44:12.400 --> 01:44:22.640]   After I set up one experiment or run, I can scale those to a suite or a set of experiments.
[01:44:22.640 --> 01:44:26.880]   And after a phase of that, I can test with the full dataset, the full training data,
[01:44:26.880 --> 01:44:32.240]   and hopefully see that the observed patterns hold, and maybe even improve, because I'm adding
[01:44:32.240 --> 01:44:37.040]   more data. And if adding more data doesn't help or makes the results worse, then I might be
[01:44:37.040 --> 01:44:43.840]   overfitting. Validation data can be especially tricky to fix here, because the smaller sample
[01:44:43.840 --> 01:44:48.560]   that I'm taking might be too noisy or not representative. I'll get to a concrete example
[01:44:48.560 --> 01:44:53.680]   of that later. One solution is to pick the validation set randomly each time and know
[01:44:53.680 --> 01:44:59.120]   that if anything, you're solving a harder problem. But you need to remember to keep validating
[01:44:59.120 --> 01:45:03.520]   the overall robustness of your solution on the full validation set after you're done with the suite.
[01:45:05.200 --> 01:45:12.560]   And here I have a graphic of my favorite, most obvious parallel coordinates chart,
[01:45:12.560 --> 01:45:16.880]   because generally, as we're exploring these relationships between hyperparameters, we want
[01:45:16.880 --> 01:45:23.840]   to see a really nice, clean relationship. As you increase learning rate, validation accuracy
[01:45:23.840 --> 01:45:30.080]   really just increases. And this is some data that I ran on the economic indicators of
[01:45:31.920 --> 01:45:39.280]   140 countries, I think, correlated with their country's survey results on happiness.
[01:45:39.280 --> 01:45:45.280]   And it just shows a really clear effect that the more economic activity, the more happiness.
[01:45:45.280 --> 01:45:50.640]   But I'm excited to dig into this data. And in my experience, this is not what
[01:45:50.640 --> 01:45:55.280]   most of my hyperparameter plots look like for real machine learning data.
[01:45:57.680 --> 01:46:04.080]   So, now that we've set up this good framework and iterative suite, this is a repeated phase of
[01:46:04.080 --> 01:46:10.320]   tuning or exploring with your suite and then testing on the full training and validation set.
[01:46:10.320 --> 01:46:17.040]   A good general strategy, especially when you're pressed for time, like I generally am,
[01:46:17.040 --> 01:46:21.840]   is to start with a simple architecture or whatever's given in the code that you're looking at
[01:46:22.720 --> 01:46:29.040]   or the most applicable state of the art. And then after manually testing a few runs,
[01:46:29.040 --> 01:46:34.000]   just to get a sense of different train validation sizes and epics and maybe a few other
[01:46:34.000 --> 01:46:37.920]   hyperparameters that you want to fix, you can set up a first exploratory suite.
[01:46:37.920 --> 01:46:44.960]   I would recommend only changing a few hyperparameters at a time and sampling only a
[01:46:44.960 --> 01:46:51.280]   few values in each one, but going for a wider range per hyperparameter, because even if those
[01:46:51.280 --> 01:46:56.240]   fail, you'll get a better sense of the search space faster. For example, I would try several
[01:46:56.240 --> 01:47:02.560]   orders of magnitude for learning rate, and I wouldn't get into very fancy specifics at this
[01:47:02.560 --> 01:47:09.440]   point. I find that the training dynamics are made for easier candidates to explore first.
[01:47:09.440 --> 01:47:14.880]   So, looking at how slow or fast your network learns for different learning rates and batch sizes
[01:47:14.880 --> 01:47:20.080]   is going to be a little bit more effective, because getting the right order of magnitude
[01:47:20.080 --> 01:47:27.120]   for something like learning rate, if it's .01 versus .0001, is going to be more impactful than
[01:47:27.120 --> 01:47:37.360]   getting if it's .001 or .0015. Same with batch size. You can see if it's closer to 16 or 128,
[01:47:37.360 --> 01:47:44.240]   but not exactly if it's 10 or 20. That will help you more, and you don't need to get the number
[01:47:44.240 --> 01:47:48.800]   exactly right right away. I have Optimizer and parens, because that's going to be highly
[01:47:48.800 --> 01:47:54.080]   correlated with your learning rate, but in my experience, it can also have a huge effect on
[01:47:54.080 --> 01:47:59.360]   convergence. For example, Atom is generally great for most things, but I have seen projects where
[01:47:59.360 --> 01:48:05.120]   SGD just outperforms it massively. So, just an encouragement to consider exploring that as well.
[01:48:05.120 --> 01:48:14.080]   So, knowing that none of these are fully independent, I would then look at some of
[01:48:14.080 --> 01:48:21.040]   the fancier details, so the number and shape of filters, your layer configuration, and when
[01:48:21.040 --> 01:48:26.720]   you're looking at layer configuration, I would again not get too detailed, because if you make
[01:48:26.720 --> 01:48:31.920]   too many changes or try too many detailed hypotheses in one sweep, you're looking at
[01:48:31.920 --> 01:48:38.320]   an explosion of possible cases that you need to test. So, as an example, you could construct
[01:48:39.120 --> 01:48:44.320]   a few different versions of your architecture, let's say a regular RNN and a bidirectional RNN,
[01:48:44.320 --> 01:48:49.760]   or you could make a CNN with max pooling versus average pooling and use those as categorical
[01:48:49.760 --> 01:48:59.920]   variables. Dropout can also be good to test. I have it lower in the list, because I found that
[01:48:59.920 --> 01:49:04.800]   if I increase it too early in my exploration process, then a good fraction of my runs just
[01:49:04.800 --> 01:49:09.280]   doesn't learn anything. So, I would only increase it if you're already seeing some good results.
[01:49:09.280 --> 01:49:14.160]   And then the details go on, of course, weight decay, learning rate schedule,
[01:49:14.160 --> 01:49:24.240]   training stages, number of layers to freeze, and so on. My TLDR advice here is I would try to stick
[01:49:24.240 --> 01:49:32.560]   with a simpler sweep than you might want at first and see what you learn. And how do you see what
[01:49:32.560 --> 01:49:38.880]   you learn? I'm definitely someone who obsessively refreshes my terminal output to see the ticking
[01:49:38.880 --> 01:49:45.120]   numbers as my Keras callback logs. Weights and biases, of course, makes this much more pleasant
[01:49:45.120 --> 01:49:53.120]   and I can watch a realtime plot instead. But I still think, you know, if you can in this stage,
[01:49:53.120 --> 01:49:58.480]   once you've debugged your code and you know that your sweep is running, let the algorithm just do
[01:49:58.480 --> 01:50:06.880]   the work for you. Because explicitly, you're not conditioning your experiment B on the output of
[01:50:06.880 --> 01:50:14.240]   experiment A. You're just letting the automatic hyperparameter tuning work for you. At this point,
[01:50:14.240 --> 01:50:19.040]   it's useful to set some sort of constraint. So, either the number of runs or the amount of time
[01:50:19.040 --> 01:50:28.160]   that you're going to let this sweep explore. You can also, as kind of a worst case metric, use,
[01:50:28.160 --> 01:50:34.000]   you know, if it's already been running twice as long as the time at which you got your best result,
[01:50:34.000 --> 01:50:39.680]   then that's just my heuristic for, like, it's unlikely to keep getting better results. And,
[01:50:39.680 --> 01:50:43.440]   of course, this might depend on the complexity of the sweep that you generated.
[01:50:46.800 --> 01:50:52.880]   A really important part here is to look at your results once you're done and
[01:50:52.880 --> 01:50:59.040]   write down what you're concluding and what you're going to test next. Because it's very
[01:50:59.040 --> 01:51:04.000]   tempting to skip this step or think that you will remember or feel like there's some intuitive
[01:51:04.000 --> 01:51:10.000]   connection between learning rate and, you know, validation accuracy that you're picking up on.
[01:51:10.000 --> 01:51:14.640]   And I think the more concrete you can get with that, the better. Weights and biases also makes
[01:51:14.640 --> 01:51:20.960]   it easy to select specific runs to seed future sweeps with. And from there, you can, say,
[01:51:20.960 --> 01:51:25.200]   increase the range that you're sampling. You can increase the precision. So, try to get to a
[01:51:25.200 --> 01:51:30.240]   better value. You know, here, we're no longer just testing orders of magnitude,
[01:51:30.240 --> 01:51:35.120]   but maybe there's a significant difference between, you know, 60 filters and 65 filters.
[01:51:35.120 --> 01:51:43.040]   You can obviously add new hyperparameters here, and you can decide that, you know,
[01:51:43.040 --> 01:51:47.040]   you've tried enough values for this one and convert it to a constant in your code,
[01:51:47.040 --> 01:51:51.600]   in which case I would just write down why that was a good idea.
[01:51:51.600 --> 01:52:01.120]   So, at this point, let's hop into some examples. The first one here I have is from drought watch,
[01:52:01.120 --> 01:52:07.040]   which I talked about last time. This is a project to identify drought conditions from satellite
[01:52:07.040 --> 01:52:14.560]   images based on expert labels that we have from the ground. And here's just an example
[01:52:14.560 --> 01:52:19.120]   sweep, the way that I would frame it in weights and biases. It's using Bayesian optimization to
[01:52:19.120 --> 01:52:24.880]   maximize validation accuracy. The ranges are pretty small, and they're all in the sizes of
[01:52:24.880 --> 01:52:29.600]   the layers, but I'm keeping the architecture fixed. And there's some dropout as well. And
[01:52:29.600 --> 01:52:36.080]   for this one, I actually ran only 50 runs. So, it's a proof of concept. I like to name all my
[01:52:36.800 --> 01:52:43.440]   runs with the variables that I'm testing so that I can easily read it off from the chart always.
[01:52:43.440 --> 01:52:49.600]   This is like an extra wrapper that I do. Maybe at some point it will make it into the core feature.
[01:52:49.600 --> 01:52:58.000]   This is the hyperparameter parallel coordinates chart that you can interactively explore
[01:52:58.000 --> 01:53:02.880]   to show the correlations. I think I'm just going to keep going here.
[01:53:05.040 --> 01:53:11.840]   One thing that's interesting about this example is that here you can see that the dropout
[01:53:11.840 --> 01:53:22.800]   actually has an extremely important and high correlation with the accuracy. And that goes
[01:53:22.800 --> 01:53:31.040]   counter to the intuition I was building up before. And is this did it switch to a new tab
[01:53:31.040 --> 01:53:40.160]   successfully? I'm going to see if I can show the Zoom. We can see that. Okay. Awesome. Yeah. So,
[01:53:40.160 --> 01:53:46.720]   I'm just going to show real quick how awesome it is to Zoom into one of these regions.
[01:53:46.720 --> 01:53:54.880]   Hopefully. Sometimes it slows down when I'm sharing a tab. Yeah. And then from here,
[01:53:55.520 --> 01:54:02.400]   you can see, you know, what are the parameters that correspond to my best runs. And it seems like
[01:54:02.400 --> 01:54:09.440]   if I just select, you know, the very top ones, then that does correspond to kind of surprisingly
[01:54:09.440 --> 01:54:16.800]   high dropout values. But I think because this is running on a very small dataset sample, it might
[01:54:17.840 --> 01:54:28.960]   actually help. So, going back, I'm going to hop into a different case study. In this case, it's
[01:54:28.960 --> 01:54:36.720]   more of a fine tuning. And in this one, I'll talk about a fairly new shared project that I'm working
[01:54:36.720 --> 01:54:43.600]   on, which is Deepform. And the goal is to extract structured information from receipts that look
[01:54:43.600 --> 01:54:49.200]   like this. This is a receipt for a television advertisement. And we want to be able to tell
[01:54:49.200 --> 01:54:54.720]   the name of the organization that paid for the ad, the dates that the ad ran, and so forth. And
[01:54:54.720 --> 01:55:02.320]   this is easy for a human, but hard to do for a computer, especially from sorry, just the text,
[01:55:02.320 --> 01:55:05.680]   unless you incorporate geometric information. And we're not quite at that stage yet.
[01:55:07.200 --> 01:55:17.920]   So, what I did here was set up a pretty big sweep across a lot of parameters. And you can see here
[01:55:17.920 --> 01:55:31.280]   that a bunch of my values actually yielded NANDs. And this is because the highest correlation
[01:55:31.280 --> 01:55:37.040]   found from my sweep is this distractors field. And that's what I'm training. I'm looking at
[01:55:37.040 --> 01:55:44.480]   one positive example of a correct answer for a receipt, and then, you know, the text of the
[01:55:44.480 --> 01:55:50.960]   receipt. The distractors are that for each correct match, I create one or more fake matches by
[01:55:50.960 --> 01:55:56.320]   picking a false label from the known set. And it turns out that if I trained in a balanced way,
[01:55:56.320 --> 01:56:01.520]   then the accuracy levels out pretty quickly. But the more distractors I put in,
[01:56:01.520 --> 01:56:04.880]   the better the accuracy gets. So, that's this really high correlation.
[01:56:04.880 --> 01:56:12.800]   But the more distractors I put in, the bigger my dataset gets in memory. And so, it turned out that
[01:56:12.800 --> 01:56:17.040]   a bunch of my runs were actually failing for precisely the highest values of the distractors,
[01:56:17.040 --> 01:56:21.120]   which you can see here, the gray lines all correspond to the higher distractor values
[01:56:21.120 --> 01:56:30.720]   for the most part. And in this case, you know, I let my sweep run a really long time, even though
[01:56:30.720 --> 01:56:36.480]   it found the best values pretty early on. And then I let it run for another few hours, I think,
[01:56:36.480 --> 01:56:44.960]   after that. This is just a reminder to sanity check how some of your range exploration might
[01:56:44.960 --> 01:56:49.280]   be affected by the way you set up your training code. In this case, the best performance from
[01:56:49.280 --> 01:56:56.880]   the sweep was actually pretty comparable to my best manual run. And then a bit of both.
[01:56:56.880 --> 01:57:02.800]   Briefly, I have this project to semantically segment street scenes. Identify each pixel as
[01:57:02.800 --> 01:57:12.640]   belonging to a car or a road or a human. And one thing I found here was that initially this
[01:57:12.640 --> 01:57:18.000]   project was using accuracy to measure performance. And accuracy is not a really great metric,
[01:57:18.960 --> 01:57:25.600]   because it only treats an exact match as correct. And once I switched to intersection over union,
[01:57:25.600 --> 01:57:31.280]   I started seeing much better results. And this sweep, actually, you can see this blue line
[01:57:31.280 --> 01:57:38.240]   connecting all the best results so far. This sweep improved the performance pretty well. And
[01:57:38.240 --> 01:57:43.440]   it was helpful to see that, you know, learning rate had the highest correlation
[01:57:43.440 --> 01:57:47.280]   in the negative direction. So, lowering learning rate would have helped.
[01:57:47.760 --> 01:57:54.000]   But an interesting aspect is that I was specifically interested in my performance
[01:57:54.000 --> 01:58:00.560]   on humans. Less so than, like, cars and traffic signs and roads, because it was already doing
[01:58:00.560 --> 01:58:07.600]   really well on those. And found from the sweep that AlexNet was the best for humans,
[01:58:07.600 --> 01:58:16.080]   but it was actually less precise overall. And the reason it's best on humans is because it detects
[01:58:16.080 --> 01:58:21.040]   them as these kind of blocky figures, which you can see here. So, it gets a high intersection over
[01:58:21.040 --> 01:58:26.640]   union for kind of the wrong reasons. It just guesses at a much larger region than it needs to
[01:58:26.640 --> 01:58:33.760]   instead of precisely finding them. And here, this is the raw photo, the prediction, and the second
[01:58:33.760 --> 01:58:37.120]   column, and then the ground truth. So, this just gives you a sense for, you know, the overall
[01:58:37.120 --> 01:58:42.960]   performance is pretty good. It gets some of the details wrong. But, yeah, this is a case where
[01:58:42.960 --> 01:58:50.720]   optimizing for one thing, say, human IOU, might be separate from optimizing for, you know, overall
[01:58:50.720 --> 01:58:58.080]   IOU. And that might inform your decision in different sweeps. Yeah, so, this is just zooming
[01:58:58.080 --> 01:59:04.720]   in to a parallel coordinates plot. You can see here that AlexNet has pretty much all the worst
[01:59:04.720 --> 01:59:11.360]   performing runs, whereas ResNet-18 and ResNet-34 do better on average, even though for humans it's
[01:59:11.360 --> 01:59:21.440]   different. Yeah, so, just to summarize, general advice from fine tuning lots and lots of different
[01:59:21.440 --> 01:59:26.160]   models and different applications, I find it really helpful to have one script that I can run
[01:59:26.160 --> 01:59:32.160]   manually with a specific configuration and also run in a sweep. The shorter the run, the shorter
[01:59:32.160 --> 01:59:40.480]   your sweeps, the faster your feedback loop, the better. I would bias towards exploring first,
[01:59:40.480 --> 01:59:46.720]   and then exploiting later, especially for cases like that DeepForm project that I showed. I got
[01:59:46.720 --> 01:59:52.960]   this model to 95% accuracy and was really proud of it the other week. And then, you know, Google
[01:59:52.960 --> 01:59:58.880]   published a paper, like, the next day or that week that uses a different, much better approach and
[01:59:58.880 --> 02:00:04.000]   has a much more detailed architecture. And the correct thing to do is definitely stop optimizing
[02:00:04.000 --> 02:00:09.520]   that model and to switch to the Google way. So, yeah, that's just to say don't exploit too much,
[02:00:09.520 --> 02:00:16.480]   because the landscape is changing so quickly. But also, don't explore too much at once, because if
[02:00:16.480 --> 02:00:21.680]   you, you know, try to put in 100 hyperparameters and test a bunch of values for those, you're going
[02:00:21.680 --> 02:00:27.120]   to be running your sweep for days and you're not going to be able to do the meta learning part on
[02:00:27.120 --> 02:00:33.520]   how to do a better sweep next time. And a lot of those choices might be a depth-first tree in that
[02:00:33.520 --> 02:00:39.040]   they will determine how you structure the rest of your progress. So, I would not get too assured
[02:00:39.040 --> 02:00:44.720]   or get too attached to any particular aspect of the model as you go. And, of course, your
[02:00:44.720 --> 02:00:52.560]   mileage on all of this may vary. As a last note here, I have just two comparisons of a parallel
[02:00:52.560 --> 02:00:59.680]   coordinates plot. The one on the top is showing actual signal, and the one below is showing noise.
[02:00:59.680 --> 02:01:04.960]   And this is useful to think about when you're looking at, you know, how much your performance
[02:01:04.960 --> 02:01:11.840]   improves with any particular change. In this case, I found that in the noise case, where all I varied
[02:01:11.840 --> 02:01:21.280]   was my random seed, my output metric varied by about .9%. And if I actually meaningfully varied
[02:01:21.280 --> 02:01:29.200]   hyperparameters, my output metric varied by about 3%. So, that's not an order of magnitude
[02:01:29.200 --> 02:01:34.640]   difference. That's like a 3x difference, and that's useful to remember when, especially for me,
[02:01:34.640 --> 02:01:40.880]   when I'm tracking these runs with such detail, hoping that they'll get just a little better at
[02:01:40.880 --> 02:01:54.800]   solving a particular problem. And, yeah. I think that's it. >> Nice. Thank you, Stacey. I have
[02:01:54.800 --> 02:02:00.000]   a question. So, maybe I'll ask that while other people can start popping their questions in the
[02:02:00.000 --> 02:02:07.920]   Q&A, and also the YouTube people can pop it in the chat. So, people always talk about base, grid,
[02:02:07.920 --> 02:02:12.560]   and random. Do you have for the people who are just starting out, do you have recommendations
[02:02:12.560 --> 02:02:18.000]   on which they should start with, when to use which one? >> Yeah, totally. That's a great question,
[02:02:18.000 --> 02:02:24.880]   and I think I, like, tried to slip that in there, but basically, grid is going to try all possible
[02:02:24.880 --> 02:02:29.920]   combinations. So, you want that one if you, like, really want to try everything and you want to be
[02:02:29.920 --> 02:02:35.200]   sure you got the best one. Random is what I would recommend if you're just exploring and you want to
[02:02:35.200 --> 02:02:41.040]   try a bunch of different stuff, but you do have to keep in mind that it's, like, random, and you
[02:02:41.040 --> 02:02:48.560]   might not find the best solution. And then I think base overall does perform better, and it's worth
[02:02:48.560 --> 02:02:54.640]   trying, unless for some reason you don't have a metric that you're specifying. Like, on average,
[02:02:54.640 --> 02:02:59.440]   you can expect base to do better. So, yeah, the only reason not to try it would be if you don't
[02:02:59.440 --> 02:03:07.280]   know what you want to what your goal is. >> Someone on YouTube said, in terms of
[02:03:07.280 --> 02:03:11.760]   not sounding like a product pitch, you nailed it, which I'm happy about.
[02:03:11.760 --> 02:03:18.320]   >> Yes. Thank you. >> What are your tips for amateurs
[02:03:18.320 --> 02:03:21.840]   that want to perform sweeps but don't have the compute? That's a good question.
[02:03:21.840 --> 02:03:28.000]   >> Yeah. Oh, man. That's a good question. So, that's definitely a caveat that I did not,
[02:03:28.000 --> 02:03:33.440]   you know, super highlight. I, you know, have a machine on which I can use the GPU.
[02:03:33.440 --> 02:03:43.760]   I mean, Colab is a great resource. So, ways that you can get access to a bit more compute. I think,
[02:03:43.760 --> 02:03:49.280]   you know, if you try a few experiments manually first, then you can really reduce the search
[02:03:49.280 --> 02:03:57.520]   space. And, you know, in practice, I often do more manual exploration initially. Definitely
[02:03:57.520 --> 02:04:04.880]   early stopping would be helpful if you are concerned about compute. And another nice thing
[02:04:04.880 --> 02:04:09.040]   about at least weights and biases sweeps is they're easy to parallelize. So, if you can get
[02:04:09.040 --> 02:04:14.640]   access to like eight GPUs for an hour, then you can just have them all run the same sweep and
[02:04:14.640 --> 02:04:22.480]   they will coordinate which things to try. And that's pretty beautiful. >> Nice. Also, one thing
[02:04:22.480 --> 02:04:28.240]   that I've even seen our authors, we have an author's program, so they struggle with this
[02:04:28.240 --> 02:04:35.120]   too. Like when do you switch from manual to a sweep, you know, that's very structured? Because
[02:04:35.120 --> 02:04:39.200]   what if you don't even know what hyperparameters to track in the beginning and you're still trying
[02:04:39.200 --> 02:04:48.080]   to explore? >> Yeah, yeah, definitely. So, it's definitely like a judgment call. I would or what
[02:04:48.080 --> 02:04:58.400]   I've done in the past is if I can't see reliable improvement, that's one thing. If I feel like
[02:04:58.400 --> 02:05:03.360]   I've like tried all my hypotheses that were easy to determine with one experiment and I'm kind of
[02:05:03.360 --> 02:05:11.520]   like, okay, I'm done. Like just run this on a GPU and you tell me. I think it's also useful to
[02:05:11.520 --> 02:05:17.760]   like I think people tend to think of hyperparameter optimization because of like
[02:05:17.760 --> 02:05:22.880]   optimization as the thing that happens at the end. Like you've done most of the work and you're at
[02:05:22.880 --> 02:05:29.520]   90% and now you want to get to like 92. But I think especially if you set a large range, then
[02:05:29.520 --> 02:05:37.200]   it can still be useful from the like, you know, 50% or like random baseline to 75% just by telling
[02:05:37.200 --> 02:05:42.400]   you like, hey, actually, like you should try a much larger learning rate. Or it turns out the
[02:05:42.400 --> 02:05:49.680]   distractors are like the most useful and you just could keep increasing that. >> Nice. Cool. Charles,
[02:05:49.680 --> 02:05:57.280]   do you have one? >> Yeah. Well, one thing I wanted to like plus one your statement about using Colab,
[02:05:57.280 --> 02:06:03.600]   I think, like getting up the like modifying your tooling so that it works well with Colab is well
[02:06:03.600 --> 02:06:10.080]   worth the extra compute that it can bring you. And secondly, if you want to try and increase
[02:06:10.080 --> 02:06:14.960]   your ability to do hyperparameter tuning, you can learn a lot of sort of things that you were
[02:06:14.960 --> 02:06:21.680]   mentioning like lower learning rate or higher learning rate or use distractors, use more or
[02:06:21.680 --> 02:06:25.840]   less data augmentation, more or less dropout. You can learn that in smaller networks often. And a
[02:06:25.840 --> 02:06:33.120]   lot of those things will translate. So I think it's a good idea to like to work, yeah, with a
[02:06:33.120 --> 02:06:38.480]   tight feedback loop, as you and Zhang mentioned first. And you'll find actually you can get
[02:06:38.480 --> 02:06:47.520]   really far without requiring like 100 GPUs. But the question I had was, I wanted to go back to
[02:06:47.520 --> 02:06:52.640]   sort of the beginning of your talk, you said that a lot of your experience of that way you were
[02:06:52.640 --> 02:06:57.600]   going to be talking about in your talk was for classification. And you mentioned GANs and
[02:06:57.600 --> 02:07:04.080]   reinforcement learning as places where what you said might not apply. So I'm curious, two things.
[02:07:04.080 --> 02:07:08.880]   One, what like besides GANs and reinforcement learning, can you think of any other sort of
[02:07:08.880 --> 02:07:14.000]   machine learning, hyperparameter tuning problems that would also be different from what we just
[02:07:14.000 --> 02:07:19.280]   talked about that people might be working on in the audience? And then also, what do you think
[02:07:19.280 --> 02:07:22.480]   should be done differently in those categories, those types?
[02:07:22.480 --> 02:07:28.640]   Yeah, that's a great question. So I was drawing examples from basically the use cases I've most
[02:07:28.640 --> 02:07:32.720]   worked on at Weights and Biases, which is why it's like image classification and a bit of text.
[02:07:32.720 --> 02:07:39.680]   I've just recently started with a regression problem where I think it kind of works a little
[02:07:39.680 --> 02:07:47.120]   bit differently. One thing there is that the traditional machine learning random forest
[02:07:47.120 --> 02:07:53.520]   baseline is actually really hard to beat. And there, it's almost like my recommendation would
[02:07:53.520 --> 02:08:02.880]   be, do you need a really fancy deep learning network of millions of parameters? Other cases
[02:08:02.880 --> 02:08:12.160]   where I think the rabbit hole phenomena that I've mentioned is I think once you start working on a
[02:08:12.160 --> 02:08:18.240]   model and pursuing a certain idea, it can have a lot of pull to it. Like, oh, I really want to make
[02:08:18.240 --> 02:08:23.760]   this model work, or this is the right way to think about it, especially when you're seeing good
[02:08:23.760 --> 02:08:29.120]   results and you can actually keep improving them through optimization. So there's something like
[02:08:29.120 --> 02:08:38.800]   keeping the general approach in mind or almost like I'd love a metric for how much compute did
[02:08:38.800 --> 02:08:45.040]   you use on this or how many parameters are worthwhile here. If you actually pruned this
[02:08:45.040 --> 02:08:52.000]   network, it could be twice as small or something. So things like that. I think it can get really
[02:08:52.000 --> 02:08:56.240]   exciting and powerful when you're running a sweep and training these deep nets, but
[02:08:56.240 --> 02:09:03.520]   it might not always be necessary. >> Yeah, definitely a plus one for
[02:09:04.400 --> 02:09:10.880]   consider random forests there. Yeah, there's a lot of tabular data out there and random forests
[02:09:10.880 --> 02:09:20.560]   are kind of like designed for tabular data and they can be very good. Maybe I missed it and you
[02:09:20.560 --> 02:09:26.640]   touched on this, but for applications like GANs and reinforcement learning, do you have any thoughts,
[02:09:26.640 --> 02:09:31.920]   like any experiences that you've had with what hyperparameters are more or less important in
[02:09:31.920 --> 02:09:40.560]   those cases? >> Yeah, that's a good question. I have not run extensive sweeps specifically
[02:09:40.560 --> 02:09:49.440]   on those things. Usually in those, the works that I've been reproducing are already very
[02:09:49.440 --> 02:09:55.520]   finely tuned over a long time. So the kinds of things that I've played with are like relative
[02:09:55.520 --> 02:10:01.840]   weights of discriminator and generator loss, or if there's even like multiple loss components,
[02:10:01.840 --> 02:10:09.280]   how do we combine those? One number that always seems pretty magical to me in papers is like a
[02:10:09.280 --> 02:10:14.240]   lambda coefficient that weighs several different things relative to each other and how often it
[02:10:14.240 --> 02:10:19.280]   seems to be that like 0.5 or like even waiting is the magical number. So I think playing with
[02:10:19.280 --> 02:10:26.000]   something like that could give you a lot of leverage. In terms of reinforcement learning,
[02:10:26.000 --> 02:10:31.840]   yeah, I feel like the setup would have to be pretty different,
[02:10:31.840 --> 02:10:35.600]   but I'm excited to try that at some point with existing sweeps.
[02:10:35.600 --> 02:10:45.120]   >> Cool. Yeah, I think, yeah, starting from an existing like implemented baseline,
[02:10:45.120 --> 02:10:51.600]   whatever that's possible, like my three favorite things to compare a neural net performance to are
[02:10:51.600 --> 02:10:56.400]   like the linear model, logistic regression, linear regression, whatever, like a random of like
[02:10:56.400 --> 02:11:01.520]   relatively simple off the shelf random forest type model, maybe with like maybe gradient boosted
[02:11:01.520 --> 02:11:07.760]   trees if I'm really feeling fancy, and then something that somebody else has tried before,
[02:11:07.760 --> 02:11:12.160]   just so that I can get some of that sense of scale of like when, like what is really good
[02:11:12.160 --> 02:11:16.880]   performance, what is really bad performance, what's a meaningful improvement, and what's
[02:11:16.880 --> 02:11:20.960]   me just sitting at the slot machine pulling and trying to find a new reward. Yeah.
[02:11:20.960 --> 02:11:31.280]   >> Thanks. All right. So Danny on YouTube asks, if you experience feature drift,
[02:11:31.280 --> 02:11:34.560]   do you go through the entire hyperparameter optimization process?
[02:11:35.840 --> 02:11:42.640]   >> Feature drift. So like when a model is deployed in practice, and then it turns out that, yeah.
[02:11:42.640 --> 02:11:52.240]   Yeah. Well, I haven't worked with an example like that, at least in the weights and biases context.
[02:11:52.240 --> 02:12:02.320]   In my previous work lives, it would, it was a problem that happened, and basically there would
[02:12:02.320 --> 02:12:09.440]   be a retrain cycle for the model just because the data does change, and, you know, for classification,
[02:12:09.440 --> 02:12:16.160]   like a bigger question is do the target labels change in certain contexts, right, if you're,
[02:12:16.160 --> 02:12:23.200]   if new hashtags arise or if new instances are added.
[02:12:25.920 --> 02:12:34.080]   >> Cool. How do you recommend we automate the model retraining or training with more APOCs
[02:12:34.080 --> 02:12:38.560]   with all of the data once a good set of hyperparameters has been found?
[02:12:38.560 --> 02:12:46.320]   >> Yeah. Great question. I think you could probably figure out how to use like a GitHub
[02:12:46.320 --> 02:12:50.400]   action for that or something. Maybe it could be a weights and biases feature. I think the
[02:12:51.680 --> 02:12:56.560]   main or the relevant point that I brought up here is I find it really useful to have the same script
[02:12:56.560 --> 02:13:03.360]   be able to run in both sweep mode and individual test mode, and then you can have, say, you know,
[02:13:03.360 --> 02:13:12.160]   a command line shortcut flag to run the whole thing. You could set up a report that makes sure
[02:13:12.160 --> 02:13:18.480]   to revisit that stage. We don't have, you know, end to end automation to make that easy right now,
[02:13:18.480 --> 02:13:23.840]   but that's a great idea. >> All right. Those are all the
[02:13:23.840 --> 02:13:26.240]   questions. Thank you, Stacey. >> Thank you.
[02:13:26.240 --> 02:13:30.720]   >> Is your talk going to be public? Would you like to plug that in?
[02:13:30.720 --> 02:13:39.760]   >> Oh, yes. Well, I think you might need to be at the conference, but it's mlops. You should
[02:13:39.760 --> 02:13:46.720]   check it out. There's lots and lots of talks. It's all virtual. And I think tickets are pretty
[02:13:46.720 --> 02:13:51.520]   affordable. It's not like CVPR. >> All right. Thank you, Stacey.
[02:13:51.520 --> 02:13:58.160]   And thank you, everyone, for joining us. This is a great time. All the talks were great. Alex is
[02:13:58.160 --> 02:14:04.000]   still here. Thank you, Alex, for sticking around. Thank you for everyone who's still here with us.
[02:14:04.000 --> 02:14:10.080]   I was joking to Charles earlier that no matter how many talks we schedule, they fill up to fill
[02:14:10.080 --> 02:14:15.760]   this 2.5 hour time slot that we have because we've done four talks which filled this much time,
[02:14:15.760 --> 02:14:21.360]   five talks which also filled 2.5 hours. And today we just did three, but it's still here we are.
[02:14:21.360 --> 02:14:27.360]   Thank you, guys, for sticking around. We'll see you again two weeks from now. We do this every
[02:14:27.360 --> 02:14:34.480]   other Tuesday. And we'll post the link and the dates in the Slack community as well as on our
[02:14:34.480 --> 02:14:39.920]   Twitter. Also, Alex will be doing an AMA with us, which you can also find the details of that
[02:14:39.920 --> 02:14:45.280]   in the Slack community and on our Twitter account. And you can come and ask him all the other
[02:14:45.280 --> 02:14:52.000]   questions you didn't get to ask him. And that's it. Good night.

