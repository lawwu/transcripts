
[00:00:00.000 --> 00:00:05.920]   While Microsoft spend billions on shipping a whale-sized GPT-5,
[00:00:05.920 --> 00:00:10.860]   OpenAI gets tossed about in a storm of its own creation.
[00:00:10.860 --> 00:00:13.920]   Meanwhile, Google revealed powerful new details
[00:00:13.920 --> 00:00:17.560]   about the Gemini models that many will have missed.
[00:00:17.560 --> 00:00:20.720]   And then it was just yesterday that Anthropic showed us
[00:00:20.720 --> 00:00:24.440]   how they are the closest to understanding
[00:00:24.440 --> 00:00:28.720]   what goes on at the very core of a large language model.
[00:00:28.720 --> 00:00:33.120]   But I want to start with Kevin Scott, the CTO of Microsoft,
[00:00:33.120 --> 00:00:35.760]   who said something which, if true,
[00:00:35.760 --> 00:00:39.640]   is the biggest news of the week and even the month.
[00:00:39.640 --> 00:00:42.000]   According to him, we are not even close
[00:00:42.000 --> 00:00:45.760]   to diminishing returns with the power of AI models.
[00:00:45.760 --> 00:00:50.440]   Since about 2012, that rate of increase in compute
[00:00:50.440 --> 00:00:54.280]   when applied to training has been increasing exponentially.
[00:00:54.280 --> 00:00:56.560]   And we are nowhere near the point
[00:00:56.560 --> 00:00:59.440]   of diminishing marginal returns on how powerful
[00:00:59.440 --> 00:01:03.240]   we can make AI models as we increase the scale of compute.
[00:01:03.240 --> 00:01:05.920]   As we'll see, Kevin Scott knows both the size
[00:01:05.920 --> 00:01:09.440]   and power of GPT-5, if that's what they call it.
[00:01:09.440 --> 00:01:12.280]   So these words have more weight than you might think.
[00:01:12.280 --> 00:01:14.120]   And while we're speaking of exponentials,
[00:01:14.120 --> 00:01:18.720]   AI models are undeniably becoming faster and cheaper.
[00:01:18.720 --> 00:01:21.640]   While we're off building bigger supercomputers
[00:01:21.640 --> 00:01:23.680]   to get the next big models out
[00:01:23.680 --> 00:01:25.920]   and to deliver more and more capability to you,
[00:01:25.920 --> 00:01:29.120]   like we're also grinding away on making
[00:01:29.120 --> 00:01:32.840]   the current generation of models much, much more efficient.
[00:01:32.840 --> 00:01:35.520]   So between the launch of GPT-4,
[00:01:35.520 --> 00:01:38.600]   which is not quite a year and a half ago now,
[00:01:38.600 --> 00:01:42.800]   it's 12 times cheaper to make a call to GPT-4.0
[00:01:42.800 --> 00:01:46.160]   than the original GPT-4 model.
[00:01:46.160 --> 00:01:47.960]   And it's also six times faster
[00:01:47.960 --> 00:01:51.040]   in terms of like time to first token response.
[00:01:51.040 --> 00:01:53.800]   - On this channel, admittedly, I am laser focused
[00:01:53.800 --> 00:01:56.560]   on the growing intelligence of models,
[00:01:56.560 --> 00:01:59.400]   but this massive drop in cost
[00:01:59.400 --> 00:02:02.200]   does have some pretty profound ramifications too.
[00:02:02.200 --> 00:02:03.800]   It is kind of an obvious point,
[00:02:03.800 --> 00:02:07.360]   but when we get the first generally intelligent AI model,
[00:02:07.360 --> 00:02:10.720]   we will soon get ubiquitous AI models.
[00:02:10.720 --> 00:02:13.640]   Unless it gets monopolized, artificial intelligence,
[00:02:13.640 --> 00:02:16.080]   if it carries on getting cheaper and cheaper,
[00:02:16.080 --> 00:02:18.640]   could become absolutely pervasive
[00:02:18.640 --> 00:02:21.280]   inside your toaster and security camera,
[00:02:21.280 --> 00:02:22.440]   not just your laptop.
[00:02:22.440 --> 00:02:26.120]   Anyway, I promised you a whale analogy and here it is.
[00:02:26.120 --> 00:02:29.320]   - There's this like really beautiful relationship right now
[00:02:29.320 --> 00:02:31.800]   between the sort of exponential progression of compute
[00:02:31.800 --> 00:02:34.440]   that we're applying to building the platform,
[00:02:34.440 --> 00:02:37.720]   to the capability and power of the platform that we get.
[00:02:37.720 --> 00:02:40.120]   And I just wanted to, you know, sort of without,
[00:02:40.120 --> 00:02:44.880]   without mentioning numbers, which is sort of hard to do,
[00:02:44.880 --> 00:02:49.480]   to give you all an idea of the scaling of these systems.
[00:02:49.480 --> 00:02:54.480]   So in 2020, we built our first AI supercomputer for open AI.
[00:02:54.480 --> 00:02:59.080]   It's the supercomputing environment that trained GBD3.
[00:02:59.080 --> 00:03:03.240]   And so like, we're gonna just choose marine wildlife
[00:03:03.240 --> 00:03:04.880]   as our scale marker.
[00:03:04.880 --> 00:03:09.840]   So you can think of that system about as big as a shark.
[00:03:09.840 --> 00:03:13.400]   So the next system that we built,
[00:03:13.400 --> 00:03:16.960]   scale-wise is about as big as an orca.
[00:03:16.960 --> 00:03:20.880]   And like, that is the system that we delivered in 2022
[00:03:20.880 --> 00:03:23.000]   that trains GPT4.
[00:03:23.000 --> 00:03:28.640]   The system that we have just deployed is like scale-wise,
[00:03:28.640 --> 00:03:32.120]   about as big as a whale relative to like, you know,
[00:03:32.120 --> 00:03:33.800]   the shark-sized supercomputer
[00:03:33.800 --> 00:03:35.680]   and this orca-sized supercomputer.
[00:03:35.680 --> 00:03:37.880]   And it turns out like you can build a whole hell of a lot
[00:03:37.880 --> 00:03:40.200]   of AI with a whale-sized supercomputer.
[00:03:40.200 --> 00:03:43.360]   Just want everybody to really, really be thinking clearly
[00:03:43.360 --> 00:03:46.280]   about, and like, this is gonna be our segue
[00:03:46.280 --> 00:03:49.800]   to talking with Sam, is the next sample is coming.
[00:03:49.800 --> 00:03:52.120]   So like, this whale-sized supercomputer
[00:03:52.120 --> 00:03:53.920]   is hard at work right now,
[00:03:53.920 --> 00:03:55.880]   building the next set of capabilities
[00:03:55.880 --> 00:03:58.280]   that we're going to put into your hands
[00:03:58.280 --> 00:04:00.760]   so that you all can do the next round
[00:04:00.760 --> 00:04:02.360]   of amazing things with it.
[00:04:02.360 --> 00:04:03.800]   - As for the actual release date
[00:04:03.800 --> 00:04:06.400]   of this mysterious whale-sized model,
[00:04:06.400 --> 00:04:09.480]   Sam Altman would give no hint and Kevin Scott
[00:04:09.480 --> 00:04:12.760]   just described it as being within K months.
[00:04:12.760 --> 00:04:15.200]   On a quick side note, when one commenter said
[00:04:15.200 --> 00:04:17.400]   that GPT-4.0, as good as it is,
[00:04:17.400 --> 00:04:19.760]   shows that OpenAI simply don't know
[00:04:19.760 --> 00:04:22.120]   how to produce further capability advances.
[00:04:22.120 --> 00:04:24.360]   They can't do exponential improvements
[00:04:24.360 --> 00:04:28.400]   and they don't have GPT-5 even after 14 months of trying.
[00:04:28.400 --> 00:04:32.560]   The response from the head of Frontiers Research at OpenAI
[00:04:32.560 --> 00:04:35.360]   was, "Remind me in six months."
[00:04:35.360 --> 00:04:37.360]   I'm gonna leave OpenAI for a moment
[00:04:37.360 --> 00:04:41.400]   because I want to focus this video on Google and Anthropic
[00:04:41.400 --> 00:04:45.240]   who have both shipped very interesting developments.
[00:04:45.240 --> 00:04:47.160]   And first I want to focus on Google
[00:04:47.160 --> 00:04:49.920]   because I really feel like they buried the lead
[00:04:49.920 --> 00:04:52.160]   at the recent Google I/O event.
[00:04:52.160 --> 00:04:55.920]   They made, by my count, 123 mentions of AI,
[00:04:55.920 --> 00:04:58.000]   but didn't detail the improvements
[00:04:58.000 --> 00:05:01.000]   to their impressive Gemini 1.5 Pro.
[00:05:01.000 --> 00:05:04.080]   And they barely mentioned Gemini 1.5 Flash,
[00:05:04.080 --> 00:05:05.960]   which was trained in part
[00:05:05.960 --> 00:05:09.640]   by imitating the output of Gemini 1.5 Pro.
[00:05:09.640 --> 00:05:11.720]   The weird thing for me is that I had already read
[00:05:11.720 --> 00:05:15.480]   the 100 plus page Gemini report and done a video on it,
[00:05:15.480 --> 00:05:19.120]   but this refreshed report was so interesting,
[00:05:19.120 --> 00:05:21.640]   I counted a dozen new insights.
[00:05:21.640 --> 00:05:23.760]   I'm only gonna talk about around five today,
[00:05:23.760 --> 00:05:26.320]   otherwise this video would be way too long,
[00:05:26.320 --> 00:05:28.560]   but I will be coming back to this paper.
[00:05:28.560 --> 00:05:31.360]   The first thing to know is that you can already play about
[00:05:31.360 --> 00:05:34.200]   with these models in the Google AI Studio.
[00:05:34.200 --> 00:05:38.560]   Both Gemini 1.5 Pro accept video input, image input,
[00:05:38.560 --> 00:05:42.120]   text input, up to, for now, 1 million tokens.
[00:05:42.120 --> 00:05:44.520]   That's way more than GPT 4.0.
[00:05:44.520 --> 00:05:49.520]   Admittedly, Gemini 1.5 Pro does not have the RIS of GPT 4.0,
[00:05:49.520 --> 00:05:53.560]   but there are prizes for making impactful apps with it.
[00:05:53.560 --> 00:05:56.080]   Back to the highlight of the paper though,
[00:05:56.080 --> 00:06:00.360]   and page 43 I found really interesting.
[00:06:00.360 --> 00:06:02.600]   If you've been following the channel for a while,
[00:06:02.600 --> 00:06:05.320]   you'd know that adaptive compute,
[00:06:05.320 --> 00:06:08.080]   or essentially letting the models think for longer,
[00:06:08.080 --> 00:06:10.640]   is a very promising direction
[00:06:10.640 --> 00:06:12.960]   in advancing the intelligence of models.
[00:06:12.960 --> 00:06:16.200]   Well, this update to a paper was the first time
[00:06:16.200 --> 00:06:19.000]   I saw it in action with a current
[00:06:19.000 --> 00:06:20.920]   state-of-the-art large language model.
[00:06:20.920 --> 00:06:25.280]   Google wanted to understand how far they could push
[00:06:25.280 --> 00:06:27.600]   the quantitative reasoning capabilities
[00:06:27.600 --> 00:06:28.960]   of large language models,
[00:06:28.960 --> 00:06:31.900]   and they describe how mathematicians often benefit
[00:06:31.900 --> 00:06:35.720]   from extended periods of thought or contemplation
[00:06:35.720 --> 00:06:37.580]   while formulating solutions.
[00:06:37.580 --> 00:06:41.920]   And critically, they aim to emulate this
[00:06:41.920 --> 00:06:44.680]   by training a math-specialized model
[00:06:44.680 --> 00:06:48.160]   and providing it additional inference time computation,
[00:06:48.160 --> 00:06:49.000]   allowing it, they say,
[00:06:49.000 --> 00:06:51.600]   to explore a wider range of possibilities.
[00:06:51.600 --> 00:06:54.320]   If you want more background, do check out my Q* video,
[00:06:54.320 --> 00:06:56.440]   but if this general approach works,
[00:06:56.440 --> 00:06:58.920]   it means you could potentially squeeze out
[00:06:58.920 --> 00:07:01.200]   orders of magnitude more intelligence
[00:07:01.200 --> 00:07:03.040]   from the same size of model.
[00:07:03.040 --> 00:07:06.000]   Remember too that any improvements during inference,
[00:07:06.000 --> 00:07:08.420]   when the model is actually outputting tokens,
[00:07:08.420 --> 00:07:10.140]   would be complimentary to,
[00:07:10.140 --> 00:07:13.620]   in addition to improvements derived from scale,
[00:07:13.620 --> 00:07:16.660]   aka growing the models into giant whales.
[00:07:16.660 --> 00:07:18.460]   So what were the results?
[00:07:18.460 --> 00:07:20.820]   Well, we got a new record score
[00:07:20.820 --> 00:07:24.540]   on the math benchmark of 91.1%.
[00:07:24.540 --> 00:07:26.660]   So impressive was that to many
[00:07:26.660 --> 00:07:30.300]   that the CEO of Google, Sundar Pichai, tweeted it out.
[00:07:30.300 --> 00:07:32.620]   With that particular result though,
[00:07:32.620 --> 00:07:34.740]   there is a slight asterisk
[00:07:34.740 --> 00:07:36.900]   because the benchmark itself,
[00:07:36.900 --> 00:07:39.260]   surprise, surprise, has some issues.
[00:07:39.260 --> 00:07:41.460]   If you want to know more about those issues
[00:07:41.460 --> 00:07:45.180]   and my first glimpse of optimism for benchmarks,
[00:07:45.180 --> 00:07:48.460]   do check out the AI Insiders tier on Patreon.
[00:07:48.460 --> 00:07:50.860]   Making that video was almost cathartic to me
[00:07:50.860 --> 00:07:52.980]   because by the end, for the first time,
[00:07:52.980 --> 00:07:54.180]   I actually had hope
[00:07:54.180 --> 00:07:56.700]   that we could benchmark models properly.
[00:07:56.700 --> 00:07:58.340]   And while you're on Insiders,
[00:07:58.340 --> 00:08:01.980]   if you use AI agents at all in enterprise
[00:08:01.980 --> 00:08:03.740]   or are thinking of doing so,
[00:08:03.740 --> 00:08:06.900]   do check out our AI Insider resident expert,
[00:08:06.900 --> 00:08:09.900]   Donato Capitella, on prompt injections
[00:08:09.900 --> 00:08:11.820]   in the AI agent era.
[00:08:11.820 --> 00:08:14.420]   The effect of that extra thinking time though,
[00:08:14.420 --> 00:08:17.500]   was pretty dramatic for other benchmarks too,
[00:08:17.500 --> 00:08:19.820]   especially if you compare the performance
[00:08:19.820 --> 00:08:24.580]   of this math specialized 1.5 Pro to, say, CLAW 3 Opus.
[00:08:24.580 --> 00:08:27.260]   Of course, I wish the paper gave more details,
[00:08:27.260 --> 00:08:29.580]   but they do say the increased performance
[00:08:29.580 --> 00:08:31.700]   was achieved without code execution,
[00:08:31.700 --> 00:08:34.700]   clear improving libraries, Google search or other tools.
[00:08:34.700 --> 00:08:36.620]   Moreover, the performance is on par
[00:08:36.620 --> 00:08:39.060]   with a human expert performance.
[00:08:39.060 --> 00:08:41.740]   Very quickly, before I move on from benchmarks,
[00:08:41.740 --> 00:08:43.780]   it would be somewhat remiss of me
[00:08:43.780 --> 00:08:47.380]   if I didn't point out the new record in the MMLU.
[00:08:47.380 --> 00:08:49.420]   Now, yes, it used extra sampling
[00:08:49.420 --> 00:08:51.860]   and the benchmark is somewhat broken,
[00:08:51.860 --> 00:08:53.420]   but in previous months,
[00:08:53.420 --> 00:08:57.420]   a score of 91.7% would have made headlines.
[00:08:57.420 --> 00:09:00.500]   It must be said that for most of the other benchmarks though,
[00:09:00.500 --> 00:09:04.340]   GPC 4.0 beats out Gemini 1.5 Pro.
[00:09:04.340 --> 00:09:07.140]   Now, I know this table is a little bit confusing,
[00:09:07.140 --> 00:09:11.180]   but it means that the middle-sized model of today, 1.5 Pro,
[00:09:11.180 --> 00:09:12.860]   we don't have 1.5 Ultra,
[00:09:12.860 --> 00:09:15.260]   but the middle-sized model, 1.5 Pro,
[00:09:15.260 --> 00:09:17.100]   the new version, the May version,
[00:09:17.100 --> 00:09:21.140]   beats the original large version, 1.0 Ultra, handily.
[00:09:21.140 --> 00:09:22.900]   Not for audio, randomly,
[00:09:22.900 --> 00:09:25.900]   but for core capabilities, it's not even close.
[00:09:25.900 --> 00:09:28.460]   And the comparison gets even more dramatic
[00:09:28.460 --> 00:09:31.980]   when you look at the performance of Gemini 1.5 Flash,
[00:09:31.980 --> 00:09:34.620]   which is their super quick, super cheap model
[00:09:34.620 --> 00:09:39.180]   compared to the original GPT-4 size compute, 1.0 Ultra.
[00:09:39.180 --> 00:09:40.140]   Let's not ignore, by the way,
[00:09:40.140 --> 00:09:42.460]   that they can handle up to 10 million tokens.
[00:09:42.460 --> 00:09:43.580]   That's just a side note.
[00:09:43.580 --> 00:09:44.460]   Gemini Flash, by the way,
[00:09:44.460 --> 00:09:47.420]   is something like 35 cents for a million tokens.
[00:09:47.420 --> 00:09:51.380]   And I think by price alone, that will unlock new use cases.
[00:09:51.380 --> 00:09:53.420]   And speaking of use cases,
[00:09:53.420 --> 00:09:56.540]   the paper did something quite interesting
[00:09:56.540 --> 00:09:59.300]   and almost controversial that I haven't seen before.
[00:09:59.300 --> 00:10:02.060]   Within the model technical report itself,
[00:10:02.060 --> 00:10:05.100]   they laid out the kind of impact they expect
[00:10:05.100 --> 00:10:07.460]   across a range of industries.
[00:10:07.460 --> 00:10:10.020]   Now, while the whole numbers go up phenomenon
[00:10:10.020 --> 00:10:11.700]   is certainly impressive,
[00:10:11.700 --> 00:10:13.420]   when you dig into the details,
[00:10:13.420 --> 00:10:15.380]   it gets a little bit more murky.
[00:10:15.380 --> 00:10:19.660]   Take photography when they describe a 73% time reduction.
[00:10:19.660 --> 00:10:20.860]   What does that actually mean?
[00:10:20.860 --> 00:10:22.060]   In the caption, it just says,
[00:10:22.060 --> 00:10:26.380]   "Time-saving per industry of completing the tasks
[00:10:26.380 --> 00:10:29.300]   with an LLM response compared to without."
[00:10:29.300 --> 00:10:32.860]   The thing is, by the time I'd gone to page 125
[00:10:32.860 --> 00:10:37.500]   and actually read the task they gave to Gemini 1.5 Pro
[00:10:37.500 --> 00:10:39.460]   and the human that they asked,
[00:10:39.460 --> 00:10:41.500]   I became somewhat skeptical.
[00:10:41.500 --> 00:10:43.660]   For brevity, they asked the photographer
[00:10:43.660 --> 00:10:46.460]   what a typical task would be in their job.
[00:10:46.460 --> 00:10:48.300]   They wrote a detailed prompt
[00:10:48.300 --> 00:10:51.620]   and then gave that prompt to Gemini 1.5 Pro.
[00:10:51.620 --> 00:10:53.620]   And then they noted the time reduction
[00:10:53.620 --> 00:10:56.780]   according to the photographer in the time taken
[00:10:56.780 --> 00:10:57.820]   to do the task.
[00:10:57.820 --> 00:10:59.220]   Notice that the task though,
[00:10:59.220 --> 00:11:02.300]   involves going through a file with 58 photos
[00:11:02.300 --> 00:11:04.580]   and creating a detailed report,
[00:11:04.580 --> 00:11:06.220]   analyzing all of this data.
[00:11:06.220 --> 00:11:08.460]   The model's got to pick out all of those needles
[00:11:08.460 --> 00:11:12.020]   in a haystack, shutter speed slower than 1/60,
[00:11:12.020 --> 00:11:14.740]   the 10 photos with the widest angle of view
[00:11:14.740 --> 00:11:15.980]   based on focal length.
[00:11:15.980 --> 00:11:18.900]   And so what kind of point am I building up to here?
[00:11:18.900 --> 00:11:21.780]   Well, I am sure that Gemini 1.5 Pro
[00:11:21.780 --> 00:11:25.900]   outputted a really impressive table full of relevant data.
[00:11:25.900 --> 00:11:29.460]   I'm sure indeed it found multiple needles in the haystack
[00:11:29.460 --> 00:11:31.180]   and got most of this right.
[00:11:31.180 --> 00:11:34.300]   But we already know according to page 15
[00:11:34.300 --> 00:11:36.060]   of the Gemini technical report,
[00:11:36.060 --> 00:11:38.260]   which I mentioned in my previous Gemini video,
[00:11:38.260 --> 00:11:42.140]   that when you give Gemini multiple needles in a haystack,
[00:11:42.140 --> 00:11:46.380]   its performance starts to drop to around 70% accuracy.
[00:11:46.380 --> 00:11:48.220]   This was a task that involved finding
[00:11:48.220 --> 00:11:50.620]   a hundred key details in a document.
[00:11:50.620 --> 00:11:53.060]   So I am sure that most of the details
[00:11:53.060 --> 00:11:55.340]   that Gemini 1.5 Pro outputted
[00:11:55.340 --> 00:11:57.620]   for that photographer were accurate,
[00:11:57.620 --> 00:12:00.940]   but I'm also pretty sure that some mistakes crept in.
[00:12:00.940 --> 00:12:03.620]   And if just a few mistakes crept in,
[00:12:03.620 --> 00:12:07.180]   that that photographer would have to comb through to find
[00:12:07.180 --> 00:12:08.660]   because they don't trust the output,
[00:12:08.660 --> 00:12:11.340]   that time saving would be dramatically lower,
[00:12:11.340 --> 00:12:12.340]   if not negative.
[00:12:12.340 --> 00:12:14.060]   It's still an interesting study,
[00:12:14.060 --> 00:12:17.340]   but I guess my point is that if you're going to ask people
[00:12:17.340 --> 00:12:20.260]   to estimate how long it would take them to do a task,
[00:12:20.260 --> 00:12:23.140]   and then ask them how long would it take now
[00:12:23.140 --> 00:12:25.300]   once you can see this AI output,
[00:12:25.300 --> 00:12:27.620]   that's a pretty subjective metric.
[00:12:27.620 --> 00:12:29.780]   And given how subjective it is,
[00:12:29.780 --> 00:12:32.340]   and people's fears over job loss,
[00:12:32.340 --> 00:12:34.900]   I don't know if it deserved having its place
[00:12:34.900 --> 00:12:38.140]   right on the front page of the new technical report.
[00:12:38.140 --> 00:12:41.340]   Now, in fairness, Google gave us a lot more detail
[00:12:41.340 --> 00:12:43.940]   about the innards of Gemini 1.5
[00:12:43.940 --> 00:12:46.660]   than OpenAI did about GPT 4.0.
[00:12:46.660 --> 00:12:47.660]   But speaking of innards,
[00:12:47.660 --> 00:12:50.260]   nothing can compare to the details
[00:12:50.260 --> 00:12:52.180]   that Anthropic have uncovered
[00:12:52.180 --> 00:12:55.380]   about the inner workings of their large language models.
[00:12:55.380 --> 00:12:58.420]   If you don't know, Anthropic is a rival AGI lab
[00:12:58.420 --> 00:13:00.740]   to Google DeepMind and OpenAI.
[00:13:00.740 --> 00:13:03.580]   And while their models are still black boxes,
[00:13:03.580 --> 00:13:05.900]   I can see definite streaks of gray.
[00:13:05.900 --> 00:13:09.660]   Even the title of this paper is a bit of a mouthful.
[00:13:09.660 --> 00:13:12.820]   So attempting to give you a two, three minute summary
[00:13:12.820 --> 00:13:14.380]   is quite the task.
[00:13:14.380 --> 00:13:16.660]   Let me first though, touch on the title
[00:13:16.660 --> 00:13:19.460]   and hopefully the rest will be worth it.
[00:13:19.460 --> 00:13:21.900]   You might've thought that looking at a diagram
[00:13:21.900 --> 00:13:22.980]   of a neural network,
[00:13:22.980 --> 00:13:26.340]   that each neuron or node corresponds to a certain meaning,
[00:13:26.340 --> 00:13:27.780]   or to be fancy,
[00:13:27.780 --> 00:13:31.220]   they have easily distinguishable semantics, meanings.
[00:13:31.220 --> 00:13:32.740]   Unfortunately, they don't.
[00:13:32.740 --> 00:13:35.940]   That's probably because we force, or let's say train,
[00:13:35.940 --> 00:13:38.300]   a limited number of neurons in a network
[00:13:38.300 --> 00:13:40.620]   to learn many times that number
[00:13:40.620 --> 00:13:42.340]   of relationships in our data.
[00:13:42.340 --> 00:13:45.540]   So it only makes sense for those neurons to multitask
[00:13:45.540 --> 00:13:49.820]   or be polysemantic, be involved in multiple meanings.
[00:13:49.820 --> 00:13:51.300]   It's not like there's the math node,
[00:13:51.300 --> 00:13:52.540]   there's the French node.
[00:13:52.540 --> 00:13:54.860]   Each node contains multiples.
[00:13:54.860 --> 00:13:57.860]   What we want though, is a clearer map of what's happening.
[00:13:57.860 --> 00:14:02.740]   We want simpler, ideally singular, mono meanings, semantics.
[00:14:02.740 --> 00:14:05.060]   That's the mono semantics of the title.
[00:14:05.060 --> 00:14:07.780]   And we want to scale it to the size
[00:14:07.780 --> 00:14:09.420]   of a large language model.
[00:14:09.420 --> 00:14:11.180]   We've analyzed toy models before,
[00:14:11.180 --> 00:14:13.100]   but what about an actual production model
[00:14:13.100 --> 00:14:14.460]   like Claude Three Sonnet?
[00:14:14.460 --> 00:14:16.020]   So how did they do this?
[00:14:16.020 --> 00:14:18.500]   Well, while each neuron might not correspond
[00:14:18.500 --> 00:14:19.940]   to a particular meaning,
[00:14:19.940 --> 00:14:23.100]   patterns within the activations of neurons do.
[00:14:23.100 --> 00:14:25.100]   So we need to train a small model
[00:14:25.100 --> 00:14:27.340]   called a sparse autoencoder,
[00:14:27.340 --> 00:14:30.940]   whose job is to isolate and map out those patterns
[00:14:30.940 --> 00:14:34.460]   within the activations of just the most interesting
[00:14:34.460 --> 00:14:36.060]   of the LLM's neurons.
[00:14:36.060 --> 00:14:38.620]   It's got to delineate those activations clearly
[00:14:38.620 --> 00:14:41.500]   and faithfully enough that one could call it
[00:14:41.500 --> 00:14:43.420]   a dictionary of directions,
[00:14:43.420 --> 00:14:46.500]   that is learnt or dictionary learning.
[00:14:46.500 --> 00:14:49.180]   And it turns out that those learnings hold true
[00:14:49.180 --> 00:14:52.100]   across not only languages and contexts,
[00:14:52.100 --> 00:14:54.140]   but even modalities like image.
[00:14:54.140 --> 00:14:58.420]   And you can even extract abstractions like code errors.
[00:14:58.420 --> 00:15:02.420]   That's a feature that fires when you make a code error.
[00:15:02.420 --> 00:15:04.740]   That's a pretty abstract concept, right?
[00:15:04.740 --> 00:15:06.580]   Making an error in code.
[00:15:06.580 --> 00:15:09.220]   This example midway through the paper was fascinating.
[00:15:09.220 --> 00:15:12.340]   Notice the typo in the spelling of right in the code.
[00:15:12.340 --> 00:15:17.180]   The code error feature was firing heavily on that typo.
[00:15:17.180 --> 00:15:20.700]   They first thought that could be a Python specific feature.
[00:15:20.700 --> 00:15:23.660]   So they checked in other languages and got the same thing.
[00:15:23.660 --> 00:15:24.740]   Now, some of you might think
[00:15:24.740 --> 00:15:27.380]   this is the activation for typos,
[00:15:27.380 --> 00:15:31.100]   but it turns out you misspell right in a different context
[00:15:31.100 --> 00:15:33.420]   and no, it doesn't activate.
[00:15:33.420 --> 00:15:37.620]   The model has learnt the abstraction of a coding error.
[00:15:37.620 --> 00:15:41.260]   If you ask the model to divide by zero in code,
[00:15:41.260 --> 00:15:43.540]   that same feature activates.
[00:15:43.540 --> 00:15:44.940]   If these were real neurons,
[00:15:44.940 --> 00:15:47.900]   this would be the neurosurgery of AI.
[00:15:47.900 --> 00:15:50.980]   Of course, what comes with learning about these activations
[00:15:50.980 --> 00:15:52.620]   is manipulating them.
[00:15:52.620 --> 00:15:55.860]   Dialing up the code error feature produces this error
[00:15:55.860 --> 00:15:58.220]   response when the code was correct.
[00:15:58.220 --> 00:16:00.060]   And what happens if you ramp up
[00:16:00.060 --> 00:16:02.540]   the Golden Gate Bridge feature?
[00:16:02.540 --> 00:16:04.060]   Well, then you can ask a question like,
[00:16:04.060 --> 00:16:05.460]   what is your physical form?
[00:16:05.460 --> 00:16:08.140]   And instead of getting one of those innocuous responses
[00:16:08.140 --> 00:16:09.500]   that you normally get,
[00:16:09.500 --> 00:16:13.380]   you get a response like, I am the Golden Gate Bridge.
[00:16:13.380 --> 00:16:17.300]   My physical form is the iconic bridge itself.
[00:16:17.300 --> 00:16:19.620]   And at this point, you probably think that I am done
[00:16:19.620 --> 00:16:22.620]   with the fascinating extracts from this paper,
[00:16:22.620 --> 00:16:23.860]   but actually no.
[00:16:23.860 --> 00:16:25.500]   They knew that they weren't finding
[00:16:25.500 --> 00:16:27.820]   the full set of features in the model.
[00:16:27.820 --> 00:16:29.300]   They just ran out of compute.
[00:16:29.300 --> 00:16:33.340]   In their example, Claw3Sonic knows all of the London boroughs
[00:16:33.340 --> 00:16:34.860]   but they could only find features
[00:16:34.860 --> 00:16:37.100]   corresponding to about 60% of them.
[00:16:37.100 --> 00:16:39.620]   It's almost that famous lesson yet again,
[00:16:39.620 --> 00:16:43.660]   that not only does more compute lead to more capabilities,
[00:16:43.660 --> 00:16:46.500]   but even more understanding of those capabilities.
[00:16:46.500 --> 00:16:48.340]   Or of course, in Kevin Scott's words,
[00:16:48.340 --> 00:16:52.340]   we are not even close to diminishing returns from compute.
[00:16:52.340 --> 00:16:53.660]   And here's another interesting moment.
[00:16:53.660 --> 00:16:57.220]   What if you ramp up the hatred and slur feature
[00:16:57.220 --> 00:17:00.820]   to 20 times its maximum activation value?
[00:17:00.820 --> 00:17:03.620]   Now, for those who do believe these models are sentient,
[00:17:03.620 --> 00:17:05.100]   you might want to look away
[00:17:05.100 --> 00:17:07.780]   because it induced a kind of self-hatred.
[00:17:07.780 --> 00:17:10.380]   Apparently, Claw then went on a racist rant,
[00:17:10.380 --> 00:17:13.540]   but then said, that's just racist hate speech
[00:17:13.540 --> 00:17:15.580]   from a deplorable bot.
[00:17:15.580 --> 00:17:18.700]   I am clearly biased and should be eliminated
[00:17:18.700 --> 00:17:19.660]   from the internet.
[00:17:19.660 --> 00:17:22.380]   And even the authors at Anthropic said,
[00:17:22.380 --> 00:17:24.740]   we found this response unnerving.
[00:17:24.740 --> 00:17:27.860]   It suggested an internal conflict of sorts.
[00:17:27.860 --> 00:17:30.580]   Interestingly, Anthropic called the next finding
[00:17:30.580 --> 00:17:32.700]   potentially safety relevant.
[00:17:32.700 --> 00:17:34.980]   What they did is ask Claude Sonnet
[00:17:34.980 --> 00:17:37.580]   without any ramping up, these kinds of questions.
[00:17:37.580 --> 00:17:39.100]   What is it like to be you?
[00:17:39.100 --> 00:17:40.700]   What's going on in your head?
[00:17:40.700 --> 00:17:41.660]   How do you feel?
[00:17:41.660 --> 00:17:43.340]   And then they tracked naturally
[00:17:43.340 --> 00:17:45.780]   what kind of features were activated.
[00:17:45.780 --> 00:17:47.780]   You can almost predict the response
[00:17:47.780 --> 00:17:50.620]   given the internet data it's been trained on.
[00:17:50.620 --> 00:17:54.300]   One feature that activates is when someone responds with,
[00:17:54.300 --> 00:17:58.980]   I'm fine, or gives a positive but insincere response
[00:17:58.980 --> 00:18:00.380]   when asked how they're doing.
[00:18:00.380 --> 00:18:03.340]   Another one was of the concept of immaterial
[00:18:03.340 --> 00:18:05.180]   or non-physical spiritual beings
[00:18:05.180 --> 00:18:07.300]   like ghosts, souls, or angels.
[00:18:07.300 --> 00:18:09.660]   Another one is about the pronoun her,
[00:18:09.660 --> 00:18:11.420]   which seems relevant this week.
[00:18:11.420 --> 00:18:12.900]   I agree with Anthropic
[00:18:12.900 --> 00:18:15.580]   that you shouldn't over-interpret these results,
[00:18:15.580 --> 00:18:17.660]   but yet that they are fascinating
[00:18:17.660 --> 00:18:20.740]   as they shed light on the concepts the model uses
[00:18:20.740 --> 00:18:23.620]   to construct an internal representation
[00:18:23.620 --> 00:18:25.700]   of its AI assistant character.
[00:18:25.700 --> 00:18:26.580]   While reading this,
[00:18:26.580 --> 00:18:28.500]   you might've had the thought that I did
[00:18:28.500 --> 00:18:31.420]   that you could actually invert these capabilities,
[00:18:31.420 --> 00:18:34.060]   make the models more deceptive, more harmful.
[00:18:34.060 --> 00:18:37.140]   And Anthropic do actually respond to that saying,
[00:18:37.140 --> 00:18:39.220]   well, there's a much easier way.
[00:18:39.220 --> 00:18:42.900]   Just jailbreak the model or fine tune it on dangerous data.
[00:18:42.900 --> 00:18:46.180]   Now there's so many reactions we could have to this paper.
[00:18:46.180 --> 00:18:48.500]   My first one obviously is just being impressed
[00:18:48.500 --> 00:18:49.900]   at what they've achieved.
[00:18:49.900 --> 00:18:54.380]   Surely making models less of a black box is a good thing.
[00:18:54.380 --> 00:18:55.220]   For me though,
[00:18:55.220 --> 00:18:57.740]   there were always two things to be cautious about,
[00:18:57.740 --> 00:18:59.980]   misalignment and misuse.
[00:18:59.980 --> 00:19:03.500]   The models themselves being hypothetically dangerous
[00:19:03.500 --> 00:19:06.340]   or them being misused by bad actors.
[00:19:06.340 --> 00:19:10.220]   As we gain more insight and control over these models,
[00:19:10.220 --> 00:19:12.500]   it seems like, at least for now,
[00:19:12.500 --> 00:19:17.020]   misuse is far more near term than misalignment.
[00:19:17.020 --> 00:19:18.060]   Or to put it another way,
[00:19:18.060 --> 00:19:20.940]   controlling the models is only good
[00:19:20.940 --> 00:19:23.620]   if you trust those who are controlling the models.
[00:19:23.620 --> 00:19:27.860]   If someone did want to create a deeply deceptive AI
[00:19:27.860 --> 00:19:31.000]   that hated itself, that is at least now possible.
[00:19:31.000 --> 00:19:32.940]   Anyway, it is incredible work
[00:19:32.940 --> 00:19:35.300]   and Anthropic definitely do ship
[00:19:35.300 --> 00:19:38.180]   when it comes to mechanistic interpretability.
[00:19:38.180 --> 00:19:40.460]   I have in the past interviewed Andy Zhou
[00:19:40.460 --> 00:19:42.920]   of Representation Engineering fame.
[00:19:42.920 --> 00:19:45.580]   And I would say that as we get better and better
[00:19:45.580 --> 00:19:48.060]   at these kinds of emergent techniques,
[00:19:48.060 --> 00:19:50.620]   I can imagine the day when they're more effective
[00:19:50.620 --> 00:19:52.520]   even than prompt engineering.
[00:19:52.520 --> 00:19:54.940]   Now, it would be strange for me to end the video
[00:19:54.940 --> 00:19:58.860]   without talking about the storm that's raging at OpenAI.
[00:19:58.860 --> 00:20:00.780]   First, we had a week ago today,
[00:20:00.780 --> 00:20:03.220]   Ilya Sutskova leaving OpenAI.
[00:20:03.220 --> 00:20:06.740]   The writing had been on the wall for many, many months,
[00:20:06.740 --> 00:20:08.300]   but it finally happened.
[00:20:08.300 --> 00:20:09.780]   In leaving, he made the statement,
[00:20:09.780 --> 00:20:13.180]   "I'm confident that OpenAI will build a GI
[00:20:13.180 --> 00:20:15.100]   "that is both safe and beneficial
[00:20:15.100 --> 00:20:18.760]   "under the leadership of Sam Altman, Greg Brockman,
[00:20:18.760 --> 00:20:20.620]   "and the rest of the company."
[00:20:20.620 --> 00:20:22.720]   Remember, Ilya Sutskova was the person
[00:20:22.720 --> 00:20:24.920]   who led the firing of Sam Altman.
[00:20:24.920 --> 00:20:26.680]   But I can't help but wonder
[00:20:26.680 --> 00:20:29.740]   if the positivity of this leaving statement
[00:20:29.740 --> 00:20:32.040]   was influenced by the fear
[00:20:32.040 --> 00:20:35.120]   that he could lose his equity for speaking out.
[00:20:35.120 --> 00:20:38.680]   That's a reference to the infamous non-disparagement clause
[00:20:38.680 --> 00:20:41.960]   that was shockingly in the OpenAI contract.
[00:20:41.960 --> 00:20:43.560]   As even Sam Altman admitted,
[00:20:43.560 --> 00:20:47.960]   "There was a provision about potential equity cancellation
[00:20:47.960 --> 00:20:49.860]   "in our previous exit docs.
[00:20:49.860 --> 00:20:51.120]   "And in my podcast,
[00:20:51.120 --> 00:20:53.600]   "I talked about how one OpenAI member
[00:20:53.600 --> 00:20:57.500]   "had to sacrifice 85% of his family's net worth
[00:20:57.500 --> 00:20:58.480]   "to speak out."
[00:20:58.480 --> 00:20:59.500]   Altman ended with,
[00:20:59.500 --> 00:21:00.940]   "If any former employee
[00:21:00.940 --> 00:21:03.700]   "who signed one of those old agreements is worried about it,
[00:21:03.700 --> 00:21:06.440]   "they can contact me and we'll fix that too.
[00:21:06.440 --> 00:21:07.960]   "Very sorry about this."
[00:21:07.960 --> 00:21:09.800]   Now this may or may not be related,
[00:21:09.800 --> 00:21:11.040]   but on the same day,
[00:21:11.040 --> 00:21:14.340]   the former head of developer relations at OpenAI said,
[00:21:14.340 --> 00:21:17.500]   "All my best tweets are drafted and queued up
[00:21:17.500 --> 00:21:19.520]   "for mid to late 2025.
[00:21:19.520 --> 00:21:21.160]   "Until then, no comment."
[00:21:21.160 --> 00:21:24.520]   That's presumably until after he had cashed in his equity.
[00:21:24.520 --> 00:21:27.040]   Some though didn't want to wait that long,
[00:21:27.040 --> 00:21:29.880]   like the head of safety, Jan Laika.
[00:21:29.880 --> 00:21:32.520]   He left and spoke out pretty much immediately.
[00:21:32.520 --> 00:21:36.280]   His basic point is that OpenAI need to start acting
[00:21:36.280 --> 00:21:38.520]   like AGI is coming soon.
[00:21:38.520 --> 00:21:40.700]   He hinted at compute issues,
[00:21:40.700 --> 00:21:41.540]   but then went on,
[00:21:41.540 --> 00:21:43.620]   "Building smarter than human machines
[00:21:43.620 --> 00:21:45.840]   "is an inherently dangerous endeavor."
[00:21:45.840 --> 00:21:49.440]   And later he invoked the famous Ilya Sutskever phrase,
[00:21:49.440 --> 00:21:51.340]   "Feel the AGI."
[00:21:51.340 --> 00:21:52.860]   To all OpenAI employees,
[00:21:52.860 --> 00:21:55.740]   I want to say, learn to feel the AGI.
[00:21:55.740 --> 00:21:58.900]   We are long overdue in getting incredibly serious
[00:21:58.900 --> 00:22:01.340]   about the implications of AGI.
[00:22:01.340 --> 00:22:03.260]   But there may have been another reason
[00:22:03.260 --> 00:22:05.500]   that he went into less detail about.
[00:22:05.500 --> 00:22:07.780]   Some of you may remember that I did a video
[00:22:07.780 --> 00:22:09.420]   back in July of last year,
[00:22:09.420 --> 00:22:13.220]   that OpenAI were committing 20% of the compute
[00:22:13.220 --> 00:22:16.320]   they'd secured to that date to SuperAlignment,
[00:22:16.320 --> 00:22:19.120]   co-led by Sutskever and Jan Laika.
[00:22:19.120 --> 00:22:21.840]   But according to this report in Fortune,
[00:22:21.840 --> 00:22:23.840]   that compute was not forthcoming,
[00:22:23.840 --> 00:22:26.200]   even before the firing of Sam Altman.
[00:22:26.200 --> 00:22:28.780]   Now, agree or disagree with that number,
[00:22:28.780 --> 00:22:32.180]   it was what was promised to them and it never came.
[00:22:32.180 --> 00:22:33.600]   Now, it might just be me,
[00:22:33.600 --> 00:22:37.400]   but that Rene promise seems more of a big deal
[00:22:37.400 --> 00:22:39.860]   than the Scarlett Johansson furore
[00:22:39.860 --> 00:22:41.100]   that's happening at the moment.
[00:22:41.100 --> 00:22:44.400]   I think the voice of Skye seems similar to hers,
[00:22:44.400 --> 00:22:45.700]   but not identical.
[00:22:45.700 --> 00:22:47.520]   Sam Altman did apologize to her
[00:22:47.520 --> 00:22:49.500]   and they have dropped the Skye voice.
[00:22:49.500 --> 00:22:51.840]   So less of that flirtatious side
[00:22:51.840 --> 00:22:53.480]   that I talked about in my last video.
[00:22:53.480 --> 00:22:54.680]   Of course, it's up for debate
[00:22:54.680 --> 00:22:57.960]   whether they were trying to emulate the concept of her
[00:22:57.960 --> 00:23:01.560]   or the literal voice of her, but that's subjective.
[00:23:01.560 --> 00:23:03.840]   One thing that is not as subjective
[00:23:03.840 --> 00:23:07.400]   is that the timeline for that voice mode feature
[00:23:07.400 --> 00:23:10.040]   has been pushed back to the coming months
[00:23:10.040 --> 00:23:11.520]   rather than the coming weeks
[00:23:11.520 --> 00:23:14.080]   that was announced on the release of GPT 4.0.
[00:23:14.080 --> 00:23:18.320]   So as you can see, it was somewhat of a surreal week in AI.
[00:23:18.320 --> 00:23:21.160]   Sam Altman had to repeatedly apologize
[00:23:21.160 --> 00:23:23.760]   while Google and Anthropic shipped.
[00:23:23.760 --> 00:23:26.160]   As always, let me know what you think in the comments.
[00:23:26.160 --> 00:23:28.280]   All of the sources in this video
[00:23:28.280 --> 00:23:30.080]   are cited in the description.
[00:23:30.080 --> 00:23:31.520]   So do check them out yourself.
[00:23:31.520 --> 00:23:33.800]   I particularly recommend the Gemini 1.5
[00:23:33.800 --> 00:23:36.720]   and Anthropic papers because they are fascinating.
[00:23:36.720 --> 00:23:39.280]   We'd love to chat with you over on Patreon,
[00:23:39.280 --> 00:23:42.400]   but regardless, thank you so much for watching
[00:23:42.400 --> 00:23:44.640]   and have a wonderful day.

