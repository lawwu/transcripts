
[00:00:00.000 --> 00:00:03.360]   [MUSIC PLAYING]
[00:00:03.360 --> 00:00:06.920]   Everybody, welcome.
[00:00:06.920 --> 00:00:10.080]   We're going to spend about 60 minutes today just looking
[00:00:10.080 --> 00:00:11.880]   into the NFRS net paper.
[00:00:11.880 --> 00:00:15.240]   And can I please ask just maybe if you
[00:00:15.240 --> 00:00:17.400]   want to raise your hands, just a quick show of hands,
[00:00:17.400 --> 00:00:19.280]   just in the participants.
[00:00:19.280 --> 00:00:23.960]   If you know about the NFRS net paper, or if you've heard of it,
[00:00:23.960 --> 00:00:25.760]   or if you've seen it, I think there's
[00:00:25.760 --> 00:00:29.320]   an option to raise hands in Zoom.
[00:00:29.320 --> 00:00:32.280]   I'll just put it in chat, just like say yes or no.
[00:00:32.280 --> 00:00:36.560]   I'm just very curious, just a quick--
[00:00:36.560 --> 00:00:39.120]   so we've got a bunch.
[00:00:39.120 --> 00:00:40.080]   We've got a mixed bunch.
[00:00:40.080 --> 00:00:41.360]   So some of us know about it.
[00:00:41.360 --> 00:00:44.880]   Some of us don't know about it.
[00:00:44.880 --> 00:00:46.200]   In the 60 minutes today, we're going
[00:00:46.200 --> 00:00:47.360]   to be discussing the paper.
[00:00:47.360 --> 00:00:50.360]   And then we're going to be going into the question and answers
[00:00:50.360 --> 00:00:51.240]   as well.
[00:00:51.240 --> 00:00:52.520]   It's not a question and answer.
[00:00:52.520 --> 00:00:55.880]   It's more of a discussion towards the end of 15 minutes.
[00:00:55.880 --> 00:01:00.560]   And something that we do in our paper reading groups
[00:01:00.560 --> 00:01:02.880]   is towards the end, if you go to that link,
[00:01:02.880 --> 00:01:05.760]   let me copy paste it in the chat as well.
[00:01:05.760 --> 00:01:08.080]   It brings up a report.
[00:01:08.080 --> 00:01:10.720]   So it will bring up the paper reading group report.
[00:01:10.720 --> 00:01:12.960]   And this is June 8.
[00:01:12.960 --> 00:01:13.680]   That's today.
[00:01:13.680 --> 00:01:16.200]   And we're looking at the characterizing
[00:01:16.200 --> 00:01:17.920]   signal propagation.
[00:01:17.920 --> 00:01:22.600]   So something that we do from most of our past paper reading
[00:01:22.600 --> 00:01:26.480]   groups is that based on this, because we have to stream it
[00:01:26.480 --> 00:01:27.360]   online as well.
[00:01:27.360 --> 00:01:29.560]   So it's just a lot of different places
[00:01:29.560 --> 00:01:32.320]   that people are attending this from.
[00:01:32.320 --> 00:01:35.280]   So it's just easier if we all put our comments in one place.
[00:01:35.280 --> 00:01:36.400]   And that's towards the end.
[00:01:36.400 --> 00:01:39.400]   But if you want to start asking your questions
[00:01:39.400 --> 00:01:41.440]   or you want to make comments, there'll
[00:01:41.440 --> 00:01:43.680]   be ample opportunity for us to talk to ourselves.
[00:01:43.680 --> 00:01:45.960]   There'll be ample opportunity for me
[00:01:45.960 --> 00:01:48.560]   to ask questions if there's some part of the paper
[00:01:48.560 --> 00:01:50.560]   that you don't understand.
[00:01:50.560 --> 00:01:57.120]   But we're going to be diving deep into this paper.
[00:01:57.120 --> 00:01:58.280]   OK, so that's that.
[00:01:58.280 --> 00:02:00.360]   So that's the paper reading group.
[00:02:00.360 --> 00:02:08.200]   So with that being said, let's get into the NF-ResNet paper.
[00:02:08.200 --> 00:02:11.520]   And before we do get into the NF-ResNet paper,
[00:02:11.520 --> 00:02:13.720]   we want to start with ResNet.
[00:02:13.720 --> 00:02:16.840]   So the ResNet architecture-- so usually I
[00:02:16.840 --> 00:02:18.400]   come with PowerPoint slides.
[00:02:18.400 --> 00:02:20.720]   But today I thought, let's just go through the paper.
[00:02:20.720 --> 00:02:23.240]   Let's go through the paper one by one.
[00:02:23.240 --> 00:02:27.560]   And we can have a look at what exactly is going on
[00:02:27.560 --> 00:02:29.240]   in the NF-ResNet paper.
[00:02:29.240 --> 00:02:32.160]   But we know that the ResNet paper looks something
[00:02:32.160 --> 00:02:32.920]   like this.
[00:02:32.920 --> 00:02:40.560]   So if I'm having a look at the 34-layer residual,
[00:02:40.560 --> 00:02:42.360]   I know that we start--
[00:02:42.360 --> 00:02:45.200]   so the first thing-- so on this thing,
[00:02:45.200 --> 00:02:48.880]   the first thing that's there is the 7 by 7 conf.
[00:02:48.880 --> 00:02:50.480]   And this looks like a--
[00:02:50.480 --> 00:02:51.440]   it's called a stem.
[00:02:51.440 --> 00:02:55.440]   So that's the first thing.
[00:02:55.440 --> 00:02:57.280]   So we've got a stem here.
[00:02:57.280 --> 00:03:02.280]   And then this is the first block.
[00:03:02.280 --> 00:03:06.440]   The one in green is the second block.
[00:03:06.440 --> 00:03:08.960]   We keep going down.
[00:03:08.960 --> 00:03:10.840]   This is the third block.
[00:03:10.840 --> 00:03:12.600]   And this is the fourth block.
[00:03:12.600 --> 00:03:14.640]   So if you count the number of--
[00:03:14.640 --> 00:03:17.520]   basically, it's a conf batch norm relue.
[00:03:17.520 --> 00:03:19.240]   So if you count, there's three of these.
[00:03:19.240 --> 00:03:22.640]   So this whole thing is counted as one.
[00:03:22.640 --> 00:03:25.760]   So there's three of these, conf batch norm relue.
[00:03:25.760 --> 00:03:27.680]   There's four in here.
[00:03:27.680 --> 00:03:29.440]   There's six in here.
[00:03:29.440 --> 00:03:31.040]   And there's three in here.
[00:03:31.040 --> 00:03:36.480]   So that's the ResNet-34 sort of architecture.
[00:03:36.480 --> 00:03:39.520]   For every other architecture, the only thing
[00:03:39.520 --> 00:03:42.120]   that's different is that these numbers of layers
[00:03:42.120 --> 00:03:42.920]   will change.
[00:03:42.920 --> 00:03:47.160]   So for deeper architectures, if we go for ResNet, say--
[00:03:47.160 --> 00:03:47.680]   I'm sorry.
[00:03:47.680 --> 00:03:49.720]   Can I please ask you to mute?
[00:03:49.720 --> 00:03:50.200]   Sorry.
[00:03:50.200 --> 00:03:51.760]   Could I please ask you to mute yourselves
[00:03:51.760 --> 00:03:53.080]   in case you've joined the call?
[00:03:53.080 --> 00:04:00.160]   OK.
[00:04:00.160 --> 00:04:04.960]   So we've got a 3463 for a ResNet-50.
[00:04:04.960 --> 00:04:07.400]   And for a ResNet, say, if you want to go deeper,
[00:04:07.400 --> 00:04:08.760]   then ResNet-100 looks different.
[00:04:09.760 --> 00:04:14.000]   That's how-- there's these number of layers.
[00:04:14.000 --> 00:04:19.280]   So we just look at the 34, which is 3463.
[00:04:19.280 --> 00:04:23.280]   A 152 would have 38363.
[00:04:23.280 --> 00:04:24.600]   That's the ResNet architecture.
[00:04:24.600 --> 00:04:29.200]   But we want to understand exactly that.
[00:04:29.200 --> 00:04:33.360]   The one thing I do want to point out in the ResNet architecture
[00:04:33.360 --> 00:04:35.800]   is when we go from one block to another.
[00:04:35.800 --> 00:04:38.880]   So basically, just this point here.
[00:04:38.880 --> 00:04:40.360]   So one block to another.
[00:04:40.360 --> 00:04:43.320]   So at the point where the first block finishes
[00:04:43.320 --> 00:04:45.400]   and the second block starts, you can
[00:04:45.400 --> 00:04:49.960]   see that we're going from 64 channels to 128 channels.
[00:04:49.960 --> 00:04:52.280]   And something else that happens is
[00:04:52.280 --> 00:04:55.320]   when the number of channels doubles, what happens
[00:04:55.320 --> 00:04:56.800]   is the image size is halved.
[00:04:56.800 --> 00:04:58.760]   So the height is halved.
[00:04:58.760 --> 00:05:01.640]   And the width is halved as well.
[00:05:01.640 --> 00:05:04.920]   So we lose this spatial dimension.
[00:05:04.920 --> 00:05:07.240]   So this is called a skip connection,
[00:05:07.240 --> 00:05:09.520]   the one on the right.
[00:05:09.520 --> 00:05:11.080]   It's called a skip connection.
[00:05:11.080 --> 00:05:14.360]   So this branch here is called the residual branch.
[00:05:14.360 --> 00:05:18.360]   And all of these are called skip connections.
[00:05:18.360 --> 00:05:21.280]   And then the point where there's a change in block,
[00:05:21.280 --> 00:05:27.080]   this skip connection is actually a convolution.
[00:05:27.080 --> 00:05:31.160]   So that's the basic idea of--
[00:05:31.160 --> 00:05:33.280]   that's the basic idea of ResNet.
[00:05:33.280 --> 00:05:35.320]   So with that being said, we are now
[00:05:35.320 --> 00:05:39.720]   ready to look into the NFResNet paper, which is this one.
[00:05:39.720 --> 00:05:41.680]   I'm characterizing signal propagation
[00:05:41.680 --> 00:05:45.400]   to close the performance gap in unnormalized networks.
[00:05:45.400 --> 00:05:48.320]   But as part of this, I think before coming to this,
[00:05:48.320 --> 00:05:50.520]   you would have also seen this--
[00:05:50.520 --> 00:05:51.320]   received this blog.
[00:05:51.320 --> 00:05:53.400]   So I ended up writing a blog about I'm
[00:05:53.400 --> 00:05:55.640]   characterizing signal propagation
[00:05:55.640 --> 00:05:57.760]   to close the gap as well.
[00:05:57.760 --> 00:06:00.320]   And basically, what this blog is,
[00:06:00.320 --> 00:06:02.200]   it has all the-- it has everything
[00:06:02.200 --> 00:06:03.600]   you need to know about this paper.
[00:06:03.600 --> 00:06:07.360]   And it has also a code implementation in PyTorch.
[00:06:07.360 --> 00:06:09.920]   And we also plot the signal propagation plots
[00:06:09.920 --> 00:06:13.120]   that we see using PyTorch hooks.
[00:06:13.120 --> 00:06:15.800]   So after this, I do believe that today,
[00:06:15.800 --> 00:06:18.560]   once we have a good, solid understanding of the NFResNet
[00:06:18.560 --> 00:06:21.120]   paper, then after this, it would be a good idea
[00:06:21.120 --> 00:06:23.360]   to go to this blog and just make sure
[00:06:23.360 --> 00:06:25.840]   that you understand everything that you
[00:06:25.840 --> 00:06:27.760]   did end up learning today.
[00:06:27.760 --> 00:06:28.560]   Cool.
[00:06:28.560 --> 00:06:30.640]   With that being said, let's kick in.
[00:06:30.640 --> 00:06:33.720]   And let's start with characterizing
[00:06:33.720 --> 00:06:36.400]   signal propagation to close the performance gap
[00:06:36.400 --> 00:06:39.600]   in unnormalized ResNets.
[00:06:39.600 --> 00:06:45.120]   So this is the main part, unnormalized ResNets.
[00:06:45.120 --> 00:06:51.160]   And the idea is in a ResNet, basically,
[00:06:51.160 --> 00:06:54.200]   when I said in the residual branch, so in a ResNet,
[00:06:54.200 --> 00:06:57.000]   the branch looks like this.
[00:06:57.000 --> 00:07:00.360]   You have a convolution batch norm ReLU.
[00:07:00.360 --> 00:07:01.520]   So that's one.
[00:07:01.520 --> 00:07:03.840]   And then convolution batch norm ReLU,
[00:07:03.840 --> 00:07:05.840]   I've just called it like this.
[00:07:05.840 --> 00:07:07.880]   So this is your input.
[00:07:07.880 --> 00:07:13.480]   And then that becomes your skip connection.
[00:07:13.480 --> 00:07:17.320]   But you can see how there's a batch norm in the ResNet
[00:07:17.320 --> 00:07:18.680]   everywhere.
[00:07:18.680 --> 00:07:22.080]   The idea is that we want to remove this batch norm.
[00:07:22.080 --> 00:07:25.040]   So we want to get away from it.
[00:07:25.040 --> 00:07:26.080]   That's the main idea.
[00:07:26.080 --> 00:07:31.560]   So we want to design ResNets without this batch norm.
[00:07:31.560 --> 00:07:34.160]   So let's have a look at the abstract.
[00:07:34.160 --> 00:07:37.840]   Batch norm is a key component in almost all state
[00:07:37.840 --> 00:07:40.400]   of the art image classifiers.
[00:07:40.400 --> 00:07:43.120]   But it also introduces practical challenges.
[00:07:43.120 --> 00:07:45.600]   So batch norm has many advantages.
[00:07:45.600 --> 00:07:47.360]   It has many advantages, but it also
[00:07:47.360 --> 00:07:48.880]   comes with some disadvantages.
[00:07:48.880 --> 00:07:53.560]   It breaks, for example, these are
[00:07:53.560 --> 00:07:55.480]   one of the practical challenges.
[00:07:55.480 --> 00:07:58.480]   It breaks the independence between training examples
[00:07:58.480 --> 00:08:01.680]   within a batch, can incur compute and memory overhead,
[00:08:01.680 --> 00:08:05.000]   and often results in unexpected bugs.
[00:08:05.000 --> 00:08:08.560]   Building on recent theoretical analysis of ResNets
[00:08:08.560 --> 00:08:11.440]   and initialization, we propose-- this is the main bit--
[00:08:11.440 --> 00:08:14.160]   we propose a simple set of tools to characterize signal
[00:08:14.160 --> 00:08:16.000]   propagation on the forward pass.
[00:08:16.000 --> 00:08:18.440]   What that basically means, what the authors are trying to say
[00:08:18.440 --> 00:08:20.760]   here is that they're saying that they proposed
[00:08:20.760 --> 00:08:23.840]   a new way of having to look at how signal propagates
[00:08:23.840 --> 00:08:26.160]   from right at the beginning towards the end
[00:08:26.160 --> 00:08:30.120]   and throughout the whole architecture, basically.
[00:08:30.120 --> 00:08:35.440]   And they also say, crucial to our success
[00:08:35.440 --> 00:08:38.080]   is an adapted version of the recently proposed weight
[00:08:38.080 --> 00:08:39.160]   standardization.
[00:08:39.160 --> 00:08:42.240]   So I'll get into what weight standardization is
[00:08:42.240 --> 00:08:44.320]   once we get to that point.
[00:08:44.320 --> 00:08:48.360]   But basically, the batch norm, for those of you that know,
[00:08:48.360 --> 00:08:52.240]   the batch norm normalizes the activations.
[00:08:52.240 --> 00:08:54.920]   And weight standardization normalizes the convolution
[00:08:54.920 --> 00:08:56.400]   weights.
[00:08:56.400 --> 00:08:57.400]   But we'll get into this.
[00:08:57.400 --> 00:09:02.000]   Cool.
[00:09:02.000 --> 00:09:04.000]   So that's that.
[00:09:04.000 --> 00:09:05.640]   We are good for--
[00:09:05.640 --> 00:09:06.880]   so that's just the abstract.
[00:09:06.880 --> 00:09:09.480]   So the abstract is just the main idea, usually in a paper.
[00:09:09.480 --> 00:09:10.880]   And the main idea is this.
[00:09:10.880 --> 00:09:15.840]   We want to replace the batch norm in ResNets.
[00:09:15.840 --> 00:09:17.160]   And we want to replace it--
[00:09:17.160 --> 00:09:18.760]   we want to design ResNets in a way
[00:09:18.760 --> 00:09:22.760]   that there's no normalization.
[00:09:22.760 --> 00:09:25.200]   And one thing I do want to point out,
[00:09:25.200 --> 00:09:28.040]   the reason why this is important is
[00:09:28.040 --> 00:09:30.280]   it's one of the very first papers.
[00:09:30.280 --> 00:09:32.360]   The NF ResNet was the very first.
[00:09:32.360 --> 00:09:36.400]   They had papers before, the authors from DeepMind.
[00:09:36.400 --> 00:09:38.960]   They had papers before, like SkipInit.
[00:09:38.960 --> 00:09:43.360]   They have now, after this came, the NFNet.
[00:09:43.360 --> 00:09:45.760]   And just 10 days ago, there was this paper
[00:09:45.760 --> 00:09:47.560]   of using multiple augmentations that
[00:09:47.560 --> 00:09:50.480]   has achieved state of the art.
[00:09:50.480 --> 00:09:52.160]   But yeah, we will get all through that.
[00:09:52.160 --> 00:09:56.040]   But this line of research is really key.
[00:09:56.040 --> 00:09:57.960]   And I really like this paper just
[00:09:57.960 --> 00:10:05.200]   because of the results that we can see from the paper.
[00:10:05.200 --> 00:10:06.480]   Cool.
[00:10:06.480 --> 00:10:07.760]   So that's this.
[00:10:07.760 --> 00:10:10.040]   So in this introduction, basically what happens
[00:10:10.040 --> 00:10:12.800]   is what the authors say is that batch norm has
[00:10:12.800 --> 00:10:15.200]   become a core computational primitive in deep learning.
[00:10:15.200 --> 00:10:16.720]   So basically, in every architecture
[00:10:16.720 --> 00:10:19.040]   that you see today or every architecture,
[00:10:19.040 --> 00:10:20.560]   every computer vision architecture
[00:10:20.560 --> 00:10:22.480]   that you have used, you would have
[00:10:22.480 --> 00:10:24.440]   seen that batch norm is there.
[00:10:24.440 --> 00:10:26.160]   And then it actually is.
[00:10:26.160 --> 00:10:28.680]   Like we look at, say, efficient nets.
[00:10:28.680 --> 00:10:29.560]   We look at ResNets.
[00:10:29.560 --> 00:10:32.840]   We look at even the vision transformers.
[00:10:32.840 --> 00:10:36.040]   A batch norm is a key component.
[00:10:36.040 --> 00:10:39.040]   And there's a reason why the batch norm is a key component
[00:10:39.040 --> 00:10:43.760]   because it smoothens the loss landscape.
[00:10:43.760 --> 00:10:45.520]   It allows larger learning rates.
[00:10:45.520 --> 00:10:52.520]   And it allows good signal propagation
[00:10:52.520 --> 00:10:54.280]   at initialization.
[00:10:54.280 --> 00:10:57.760]   So something when we say good signal propagation,
[00:10:57.760 --> 00:11:00.240]   what we want to do and what we want to make sure
[00:11:00.240 --> 00:11:03.080]   is that in our network, the mean is 0.
[00:11:03.080 --> 00:11:07.160]   And the standard deviation is one of the activations.
[00:11:07.160 --> 00:11:12.280]   So that's something that we want to make sure in our network.
[00:11:12.280 --> 00:11:13.240]   So that's what they say.
[00:11:13.240 --> 00:11:15.680]   And because of these benefits, I mean,
[00:11:15.680 --> 00:11:18.600]   batch norm, when it first came, it was like, oh, it's like magic.
[00:11:18.600 --> 00:11:22.000]   It can really help with a lot of things.
[00:11:22.000 --> 00:11:23.520]   But over time, people have realized
[00:11:23.520 --> 00:11:27.600]   that batch norm also has some disadvantages.
[00:11:27.600 --> 00:11:31.680]   Basically, when the per batch size is too small or too large,
[00:11:31.680 --> 00:11:34.800]   if you're trying to do segmentation,
[00:11:34.800 --> 00:11:37.400]   if you're trying to do object detection, which
[00:11:37.400 --> 00:11:41.000]   isn't the case more so in the year 2021,
[00:11:41.000 --> 00:11:45.040]   but I feel like if the image size is so big,
[00:11:45.040 --> 00:11:47.680]   like if you're training with a really, really big image size
[00:11:47.680 --> 00:11:50.240]   that you can only fit a certain amount of images,
[00:11:50.240 --> 00:11:52.240]   and when you fit a certain amount of images,
[00:11:52.240 --> 00:11:53.640]   you can only--
[00:11:53.640 --> 00:11:56.040]   when it's like the batch size is one or two,
[00:11:56.040 --> 00:11:59.720]   then those batch statistics that you calculate,
[00:11:59.720 --> 00:12:01.520]   like what batch norm does, it calculates
[00:12:01.520 --> 00:12:03.640]   the mean and standard deviation.
[00:12:03.640 --> 00:12:05.720]   And it normalizes the batch.
[00:12:05.720 --> 00:12:07.400]   That becomes very unstable.
[00:12:07.400 --> 00:12:09.440]   And as a result of that, the--
[00:12:09.440 --> 00:12:12.520]   so when the batch size is so small, the result of that
[00:12:12.520 --> 00:12:15.160]   is that the performance isn't as good.
[00:12:15.160 --> 00:12:17.360]   So these points which have been highlighted,
[00:12:17.360 --> 00:12:20.520]   this is like when the batch size is too small or too large,
[00:12:20.520 --> 00:12:22.240]   you want to replace batch norm and you
[00:12:22.240 --> 00:12:24.680]   want to find alternatives.
[00:12:24.680 --> 00:12:26.440]   It adds a memory overhead.
[00:12:26.440 --> 00:12:28.600]   It's a common source of implementation errors.
[00:12:28.600 --> 00:12:31.040]   By implementation errors, basically, I believe,
[00:12:31.040 --> 00:12:32.880]   from looking at the open review discussion,
[00:12:32.880 --> 00:12:34.800]   I believe they meant like when you're
[00:12:34.800 --> 00:12:36.360]   trying to use the pre-trained weights
[00:12:36.360 --> 00:12:37.820]   or when you're trying to fine tune,
[00:12:37.820 --> 00:12:41.000]   batch norm is at the key of implementation errors.
[00:12:41.000 --> 00:12:46.760]   This is where they say, it is often difficult to replicate
[00:12:46.760 --> 00:12:49.360]   batch normalized on different models
[00:12:49.360 --> 00:12:52.760]   on trained on different hardware.
[00:12:52.760 --> 00:12:53.680]   Cool.
[00:12:53.680 --> 00:12:55.840]   So that's the gist of it.
[00:12:55.840 --> 00:12:58.400]   So batch norm is good and batch norm is bad.
[00:12:58.400 --> 00:13:02.320]   Sometimes batch norm is considered as a necessary evil.
[00:13:02.320 --> 00:13:05.080]   So this whole section was on batch norm.
[00:13:05.080 --> 00:13:07.620]   So another line of research that has
[00:13:07.620 --> 00:13:10.420]   sought to eliminate layers, which normalize--
[00:13:10.420 --> 00:13:15.780]   so this is the line of research that this NF-RESNet paper
[00:13:15.780 --> 00:13:17.420]   is based on as well.
[00:13:17.420 --> 00:13:22.140]   And in this line of research, basically,
[00:13:22.140 --> 00:13:24.940]   what the authors have tried is we want to remove batch norm.
[00:13:24.940 --> 00:13:27.020]   And they're not the first ones to try and do that.
[00:13:27.020 --> 00:13:32.220]   Like there's these past papers that have tried to do it.
[00:13:32.220 --> 00:13:35.200]   But they're the first ones to actually achieve
[00:13:35.200 --> 00:13:39.520]   very competitive results to their batch norm counterparts.
[00:13:39.520 --> 00:13:44.320]   So there's two kinds of networks, without batch norm
[00:13:44.320 --> 00:13:45.920]   and with batch norm.
[00:13:45.920 --> 00:13:48.520]   So generally, with batch norm, we're really good.
[00:13:48.520 --> 00:13:51.240]   Like they would say achieve 85%.
[00:13:51.240 --> 00:13:52.320]   And I'm making this up.
[00:13:52.320 --> 00:13:54.520]   But let's say around that mark, they
[00:13:54.520 --> 00:13:59.120]   have ImageNet top one accuracy.
[00:13:59.120 --> 00:14:01.020]   Without batch norm, until before this,
[00:14:01.020 --> 00:14:03.440]   you would have really smaller ones.
[00:14:03.440 --> 00:14:04.960]   But this one is really competitive.
[00:14:04.960 --> 00:14:06.400]   Like it can go 84.5.
[00:14:06.400 --> 00:14:08.680]   Or it can even go to a point of 84.8.
[00:14:08.680 --> 00:14:10.440]   And I'm making these numbers up.
[00:14:10.440 --> 00:14:11.520]   But that's the main idea.
[00:14:11.520 --> 00:14:15.040]   Like this paper is the one that has made non--
[00:14:15.040 --> 00:14:18.440]   well, networks without batch norm come really close
[00:14:18.440 --> 00:14:19.960]   to the networks with batch norm.
[00:14:19.960 --> 00:14:26.640]   So the key contributions from this paper--
[00:14:26.640 --> 00:14:30.800]   and we're going to be looking at all of them in a lot of detail.
[00:14:30.800 --> 00:14:32.740]   So the first one is signal propagation plots.
[00:14:32.740 --> 00:14:38.180]   Next, they say-- this is about weight standardization.
[00:14:38.180 --> 00:14:39.580]   So they also introduced something
[00:14:39.580 --> 00:14:41.260]   called scaled weight standardization.
[00:14:41.260 --> 00:14:44.500]   And I'll go into the story of how scaled weight
[00:14:44.500 --> 00:14:49.340]   standardization came into play and what it is exactly.
[00:14:49.340 --> 00:14:51.500]   First, we'll look at what weight standardization is.
[00:14:51.500 --> 00:14:53.840]   And then we'll look at what scaled weight standardization
[00:14:53.840 --> 00:14:54.580]   is.
[00:14:54.580 --> 00:14:56.780]   And then finally, this is what I was saying.
[00:14:56.780 --> 00:14:58.980]   It's the first time that we attain a performance which
[00:14:58.980 --> 00:15:01.940]   is comparable or better than batch normalized ResNets.
[00:15:01.940 --> 00:15:08.900]   And lastly, they apply it to the RegNet architecture.
[00:15:08.900 --> 00:15:12.100]   And based on this, they're able to get state-of-the-art results
[00:15:12.100 --> 00:15:15.340]   which are very close to the existing efficient nets.
[00:15:15.340 --> 00:15:18.020]   So that's that.
[00:15:18.020 --> 00:15:21.180]   OK, so we're going to start with looking at SPBs.
[00:15:21.180 --> 00:15:23.940]   Then we're going to start and look into weight standardization.
[00:15:23.940 --> 00:15:28.820]   And then we're going to look into this point here.
[00:15:28.820 --> 00:15:30.380]   Any questions so far?
[00:15:30.380 --> 00:15:32.540]   And if there are any questions, let's
[00:15:32.540 --> 00:15:35.700]   go to 1db.me/NFResNet.
[00:15:35.700 --> 00:15:37.700]   So I'll bring it up.
[00:15:37.700 --> 00:15:50.860]   OK, it's this one.
[00:15:50.860 --> 00:15:53.460]   Are there any questions so far?
[00:15:53.460 --> 00:15:55.300]   So the question is, why does batch norm not
[00:15:55.300 --> 00:15:56.560]   work for small batch sizes?
[00:15:56.560 --> 00:15:59.100]   As I said, with small batch sizes--
[00:15:59.100 --> 00:16:01.140]   so you've got to understand what batch norm does.
[00:16:01.140 --> 00:16:11.100]   So when you have a batch--
[00:16:11.100 --> 00:16:16.540]   let's say you have 8 by 3 by 2 to 4 by 2 to 4,
[00:16:16.540 --> 00:16:20.900]   which just means you have 8 images of 3 by 2 to 4 by 2
[00:16:20.900 --> 00:16:21.940]   to 4--
[00:16:21.940 --> 00:16:25.380]   then what batch norm does is it calculates the mean
[00:16:25.380 --> 00:16:28.940]   and it calculates the standard deviation of this batch,
[00:16:28.940 --> 00:16:32.500]   so basically these 8 images.
[00:16:32.500 --> 00:16:36.860]   And then it will subtract by the mean, so divide by the mean
[00:16:36.860 --> 00:16:39.660]   and divide by standard deviation.
[00:16:39.660 --> 00:16:41.980]   And that's how it normalizes.
[00:16:41.980 --> 00:16:43.580]   But basically then, the main idea
[00:16:43.580 --> 00:16:46.700]   is if this batch number is too small,
[00:16:46.700 --> 00:16:49.760]   then this mean and standard deviation, they become unstable.
[00:16:49.760 --> 00:16:53.420]   And it becomes really hard to train a network
[00:16:53.420 --> 00:16:56.740]   with a really small batch size.
[00:16:56.740 --> 00:16:57.700]   That's the main idea.
[00:16:57.700 --> 00:17:04.580]   Cool.
[00:17:04.580 --> 00:17:09.460]   With that being said, let's start with signal propagation
[00:17:09.460 --> 00:17:11.820]   plots.
[00:17:11.820 --> 00:17:15.140]   So something that the authors at this point-- something
[00:17:15.140 --> 00:17:18.320]   that the authors mentioned at this point is there's
[00:17:18.320 --> 00:17:20.180]   been a lot of studies for batch norm.
[00:17:20.180 --> 00:17:23.780]   There's been lots of studies on how batch norm affects
[00:17:23.780 --> 00:17:24.340]   the signals.
[00:17:24.340 --> 00:17:26.980]   There's been lots of studies on how
[00:17:26.980 --> 00:17:32.340]   batch norm is-- how basically the network-- how the signal
[00:17:32.340 --> 00:17:33.700]   goes through the network.
[00:17:33.700 --> 00:17:35.540]   And what they do--
[00:17:35.540 --> 00:17:37.140]   what they try and do is--
[00:17:37.140 --> 00:17:37.980]   so let me show you.
[00:17:37.980 --> 00:17:48.180]   So let's say this is my deep network.
[00:17:48.180 --> 00:17:53.740]   So I'm just calling it like this is my ResNet-34.
[00:17:53.740 --> 00:17:54.860]   Actually, I should have a--
[00:17:54.860 --> 00:18:02.140]   OK, this is my ResNet-34.
[00:18:02.140 --> 00:18:04.500]   That's my deep architecture.
[00:18:04.500 --> 00:18:15.700]   What the authors do is they first initialize it.
[00:18:15.700 --> 00:18:20.340]   They first initialize this whole network architecture.
[00:18:20.340 --> 00:18:22.620]   And then instead of passing in-- you
[00:18:22.620 --> 00:18:24.380]   could pass in real input images.
[00:18:24.380 --> 00:18:25.780]   But what they do is they say, we're
[00:18:25.780 --> 00:18:28.460]   going to pass something of a Gaussian distribution.
[00:18:28.460 --> 00:18:31.660]   It's not that important to know why,
[00:18:31.660 --> 00:18:33.780]   but you could also pass in real--
[00:18:33.780 --> 00:18:36.780]   you could also pass in real--
[00:18:36.780 --> 00:18:39.180]   you could also pass in real images.
[00:18:39.180 --> 00:18:42.380]   And what they try and do is when this thing will go--
[00:18:42.380 --> 00:18:43.580]   so let's say the image--
[00:18:43.580 --> 00:18:48.700]   let's say just this is just 8 by 3 by 2 to 4 by 2 to 4.
[00:18:48.700 --> 00:18:52.180]   So basically, you have eight images of 2 to 4 by 2 to 4
[00:18:52.180 --> 00:18:53.620]   and three channels.
[00:18:53.620 --> 00:18:57.540]   What this does is when this goes through the whole network,
[00:18:57.540 --> 00:19:01.220]   what they're doing is they're calculating some statistics.
[00:19:01.220 --> 00:19:02.820]   So these things are called activations
[00:19:02.820 --> 00:19:04.340]   as they go through the network.
[00:19:04.340 --> 00:19:05.260]   And what they're doing is they're
[00:19:05.260 --> 00:19:07.580]   calculating some statistics, and they're plotting them.
[00:19:11.780 --> 00:19:14.380]   And that's SPVs, basically.
[00:19:14.380 --> 00:19:17.140]   So in signal propagation plots-- and the reason
[00:19:17.140 --> 00:19:20.340]   why signal propagation plots are important
[00:19:20.340 --> 00:19:22.380]   is because what the authors try and do
[00:19:22.380 --> 00:19:25.540]   is they first calculate-- they first use signal propagation
[00:19:25.540 --> 00:19:27.900]   plots to look at how BatchNorm behaves.
[00:19:27.900 --> 00:19:30.220]   So they understand exactly how BatchNorm behaves.
[00:19:30.220 --> 00:19:32.220]   And then based on that, they try and replicate
[00:19:32.220 --> 00:19:34.740]   that with non-normalized networks,
[00:19:34.740 --> 00:19:38.460]   basically non-BatchNorm networks.
[00:19:38.460 --> 00:19:40.420]   So they try and mimic the pattern.
[00:19:40.420 --> 00:19:43.420]   So if you think of this, if you know how something works
[00:19:43.420 --> 00:19:45.580]   exactly and you try and mimic it,
[00:19:45.580 --> 00:19:47.020]   like you try and mimic the pattern,
[00:19:47.020 --> 00:19:49.740]   then you're bound to have something that also works.
[00:19:49.740 --> 00:19:52.900]   So what they do here is they plot these signal propagation
[00:19:52.900 --> 00:19:53.740]   plots.
[00:19:53.740 --> 00:19:55.380]   And they plot basically three things.
[00:19:55.380 --> 00:19:57.220]   So when I told you-- like when I showed you
[00:19:57.220 --> 00:20:01.620]   as the thing goes deeper and deeper into the network, what
[00:20:01.620 --> 00:20:03.100]   they do is they plot three things.
[00:20:03.100 --> 00:20:05.580]   And I'll show you how you can do it in your own experiments
[00:20:05.580 --> 00:20:08.220]   in PyTorch as well very quickly.
[00:20:08.220 --> 00:20:09.580]   So they plot these three things--
[00:20:09.580 --> 00:20:12.180]   average channel squared mean, average channel variance,
[00:20:12.180 --> 00:20:16.580]   and average channel variance at the end of the residual branch.
[00:20:16.580 --> 00:20:19.420]   So basically, you know how to calculate mean.
[00:20:19.420 --> 00:20:20.380]   It's just in Torch.
[00:20:20.380 --> 00:20:24.060]   You can just go torch.mean, and then you provide the axis.
[00:20:24.060 --> 00:20:26.980]   But basically, they're just calculating the mean
[00:20:26.980 --> 00:20:30.620]   of the activations, and they're calculating the variance.
[00:20:30.620 --> 00:20:33.540]   And then this is the one at the end of each residual branch.
[00:20:33.540 --> 00:20:37.620]   And something that they notice is when they plot it--
[00:20:37.620 --> 00:20:39.940]   so they plot it for two orders.
[00:20:39.940 --> 00:20:44.580]   The first in the blue at the top is the BatchNormReLU ordering.
[00:20:44.580 --> 00:20:47.460]   So basically, you have your convolution BatchNormReLU.
[00:20:47.460 --> 00:20:50.100]   And in the second one, they have a ReLU and BatchNorm.
[00:20:50.100 --> 00:20:51.540]   And one thing that they notice--
[00:20:51.540 --> 00:20:54.460]   they plot it for a ResNet V2-600.
[00:20:54.460 --> 00:20:57.420]   It's just a network architecture.
[00:20:57.420 --> 00:21:01.260]   And one thing that they notice is when they plot it,
[00:21:01.260 --> 00:21:02.380]   this pattern emerges.
[00:21:02.380 --> 00:21:06.580]   So if I'm looking at this one, so the BatchNormReLU,
[00:21:06.580 --> 00:21:09.660]   basically, I'm looking at the BatchNormReLU.
[00:21:09.660 --> 00:21:10.700]   That's this one.
[00:21:10.700 --> 00:21:15.380]   And the pattern emerges that for average square channel mean,
[00:21:15.380 --> 00:21:18.540]   the mean first goes up, and then it crashes.
[00:21:18.540 --> 00:21:20.220]   Then it goes up, and then it crashes.
[00:21:20.220 --> 00:21:21.460]   It doesn't necessarily crash.
[00:21:21.460 --> 00:21:22.980]   I mean, it goes back to 0.
[00:21:22.980 --> 00:21:26.100]   So it goes up, and then it goes down.
[00:21:26.100 --> 00:21:29.260]   And it's the same thing that can be observed for average channel
[00:21:29.260 --> 00:21:30.220]   variance.
[00:21:30.220 --> 00:21:32.140]   It's like it goes up, and then it goes down.
[00:21:32.140 --> 00:21:33.420]   It goes up, and it goes down.
[00:21:33.420 --> 00:21:35.460]   It goes up, and it goes down.
[00:21:35.460 --> 00:21:37.980]   And what the authors are trying to do
[00:21:37.980 --> 00:21:43.060]   is they're trying to understand exactly how BatchNorm behaves
[00:21:43.060 --> 00:21:45.420]   in a network.
[00:21:45.420 --> 00:21:47.500]   And while there has been so much theory
[00:21:47.500 --> 00:21:50.220]   and there has been so much talk of how BatchNorm behaves,
[00:21:50.220 --> 00:21:52.380]   there hasn't really been a practical way
[00:21:52.380 --> 00:21:55.860]   of visualizing it.
[00:21:55.860 --> 00:21:58.180]   And the reason what happens is-- so can you
[00:21:58.180 --> 00:22:03.540]   imagine why the square channel mean would go up?
[00:22:03.540 --> 00:22:05.460]   The reason for that is very simple.
[00:22:05.460 --> 00:22:08.100]   In a BatchNorm ReLU ordering--
[00:22:08.100 --> 00:22:09.700]   so what is ReLU?
[00:22:09.700 --> 00:22:13.140]   Well, ReLU is something like a max 0 or H
[00:22:13.140 --> 00:22:17.140]   if my activation is H. So what ReLU does
[00:22:17.140 --> 00:22:22.220]   is if anything is negative, it will set it to 0.
[00:22:22.220 --> 00:22:24.740]   So as we go through the network--
[00:22:24.740 --> 00:22:28.900]   so if these are my four blocks, let's just say.
[00:22:28.900 --> 00:22:31.380]   And as we go down into the network,
[00:22:31.380 --> 00:22:33.220]   what ReLU is going to do is it's going
[00:22:33.220 --> 00:22:36.980]   to keep making things positive and more positive.
[00:22:36.980 --> 00:22:38.580]   So that's what ReLU will do.
[00:22:38.580 --> 00:22:40.340]   It will remove all the negatives.
[00:22:40.340 --> 00:22:43.860]   And when you remove all the negatives, the mean goes up.
[00:22:43.860 --> 00:22:49.980]   So if you have your minus 1, minus 2, 0, 1, 2,
[00:22:49.980 --> 00:22:52.060]   when you have something like-- let's say I have five
[00:22:52.060 --> 00:22:53.380]   activations, which are this.
[00:22:53.380 --> 00:22:56.340]   And the mean in this case is 0.
[00:22:56.340 --> 00:23:00.220]   But when I pass it through a ReLU, it will set these to 0.
[00:23:00.220 --> 00:23:03.460]   And suddenly, the mean becomes 1.5.
[00:23:03.460 --> 00:23:07.380]   So you can see how the mean will keep going up and up
[00:23:07.380 --> 00:23:11.660]   as we go down into the network because of this bash norm ReLU.
[00:23:11.660 --> 00:23:14.140]   The same thing happens with variance.
[00:23:14.140 --> 00:23:18.060]   So with variance, they say that the network is variance
[00:23:18.060 --> 00:23:20.380]   preserving, as in what they notice
[00:23:20.380 --> 00:23:25.780]   is that there's a constant addition of some part
[00:23:25.780 --> 00:23:29.060]   of variance to the activations.
[00:23:29.060 --> 00:23:33.820]   And what they say is this is the pattern that we want to mimic.
[00:23:33.820 --> 00:23:38.340]   So they basically look at these two things, these two charts.
[00:23:38.340 --> 00:23:42.340]   And they say we want to mimic this pattern.
[00:23:42.340 --> 00:23:45.500]   And they also look at average channel variance.
[00:23:45.500 --> 00:23:47.860]   And this one is set to 0.68.
[00:23:47.860 --> 00:23:52.660]   That's because-- this one is just set to 0.68.
[00:23:52.660 --> 00:23:54.780]   And they have a massive explanation
[00:23:54.780 --> 00:23:56.300]   of what exactly it is.
[00:23:56.300 --> 00:23:59.500]   I won't go into the very detail of what exactly that is.
[00:23:59.500 --> 00:24:02.580]   But the main part, as long as we as a group
[00:24:02.580 --> 00:24:08.100]   understand that we want to mimic these patterns, then we can.
[00:24:08.100 --> 00:24:08.820]   Then we could.
[00:24:08.820 --> 00:24:09.900]   So that's the first thing.
[00:24:09.900 --> 00:24:15.220]   So that's exactly what signal propagation plot is.
[00:24:15.220 --> 00:24:17.340]   And let me now show you how you can actually
[00:24:17.340 --> 00:24:22.860]   plot signal propagation plots in TIM.
[00:24:22.860 --> 00:24:26.180]   So in the blog, so you can see how--
[00:24:26.180 --> 00:24:30.380]   so this is just a way of showing you how--
[00:24:30.380 --> 00:24:32.460]   so as part of writing this blog, something I did
[00:24:32.460 --> 00:24:36.340]   was I try and experiment things for my own sake.
[00:24:36.340 --> 00:24:37.860]   And I try and understand exactly,
[00:24:37.860 --> 00:24:39.620]   like, OK, this is what the paper says.
[00:24:39.620 --> 00:24:42.900]   And is this what exactly is happening?
[00:24:42.900 --> 00:24:45.700]   In TIM, it's a--
[00:24:45.700 --> 00:24:48.140]   TIM is a library called PyDash Image Models, which
[00:24:48.140 --> 00:24:49.980]   is maintained by Ross Whitman.
[00:24:49.980 --> 00:24:53.220]   I contributed these extract SPP stats.
[00:24:53.220 --> 00:24:55.500]   So what this does is basically, as your--
[00:24:55.500 --> 00:25:06.780]   so as the network-- so this just plots the way--
[00:25:06.780 --> 00:25:09.740]   this just plots the SPPs.
[00:25:09.740 --> 00:25:13.220]   It will first extract the SPP stats, which is just,
[00:25:13.220 --> 00:25:15.100]   as I said, in--
[00:25:15.100 --> 00:25:17.220]   we want to extract these-- as we go down,
[00:25:17.220 --> 00:25:19.340]   we want to extract these stats at, like,
[00:25:19.340 --> 00:25:21.580]   after the activations at various positions.
[00:25:21.580 --> 00:25:23.620]   It will extract the SPP stats, and it
[00:25:23.620 --> 00:25:26.860]   plots the average square channel mean, the channel variance,
[00:25:26.860 --> 00:25:29.900]   and then the residual, which is exactly what you want to see.
[00:25:29.900 --> 00:25:32.700]   And you can see how that's exactly the same pattern
[00:25:32.700 --> 00:25:34.420]   as what is in the paper.
[00:25:34.420 --> 00:25:36.140]   So it was really good for me to see, like,
[00:25:36.140 --> 00:25:37.860]   I can replicate these results in code,
[00:25:37.860 --> 00:25:42.660]   and you can also try and do this if you follow that blog.
[00:25:42.660 --> 00:25:44.980]   But that's the main thing.
[00:25:44.980 --> 00:25:45.980]   Cool.
[00:25:45.980 --> 00:25:49.260]   So that's the first side of things.
[00:25:49.260 --> 00:25:53.260]   And the same authors have written past papers,
[00:25:53.260 --> 00:25:56.580]   which is called a skip init.
[00:25:56.580 --> 00:25:59.700]   And I'm just going to provide some background on exactly what
[00:25:59.700 --> 00:26:02.580]   that skip init paper does.
[00:26:02.580 --> 00:26:08.620]   So what the authors saw is, like, if you have a network--
[00:26:08.620 --> 00:26:12.460]   sorry, you have your convolution here,
[00:26:12.460 --> 00:26:14.860]   then you have your convolution here,
[00:26:14.860 --> 00:26:17.180]   and you have your inputs coming in from here.
[00:26:18.180 --> 00:26:20.180]   And you have your skip connection.
[00:26:20.180 --> 00:26:24.180]   All right?
[00:26:24.180 --> 00:26:28.580]   What the authors saw is, when you go--
[00:26:28.580 --> 00:26:30.580]   of course, this has a batch norm as well in it.
[00:26:30.580 --> 00:26:32.940]   So when I say conv, it's just a conv batch norm value.
[00:26:32.940 --> 00:26:35.060]   That's just standard.
[00:26:35.060 --> 00:26:36.620]   When the inputs go--
[00:26:36.620 --> 00:26:38.940]   like, this is just one block.
[00:26:38.940 --> 00:26:44.780]   So this is just a copy of this.
[00:26:44.780 --> 00:26:47.180]   So that is just this much.
[00:26:47.180 --> 00:26:48.740]   And then you have the whole network.
[00:26:48.740 --> 00:26:49.260]   Right?
[00:26:49.260 --> 00:26:55.980]   So what this does is, after this shift,
[00:26:55.980 --> 00:26:58.780]   you still have the whole network at the bottom.
[00:26:58.780 --> 00:27:01.700]   What they realized is that what batch norm does
[00:27:01.700 --> 00:27:03.940]   is that when the signal--
[00:27:03.940 --> 00:27:05.540]   so a signal has two options, right?
[00:27:05.540 --> 00:27:07.860]   The signal can go down this path.
[00:27:07.860 --> 00:27:09.740]   So the signal can go down this residual path,
[00:27:09.740 --> 00:27:11.620]   or it can go down this skip init path.
[00:27:11.620 --> 00:27:13.020]   Well, it goes down both the paths.
[00:27:13.020 --> 00:27:16.300]   But what the authors saw in the skip init paper
[00:27:16.300 --> 00:27:18.940]   is that what batch norm actually does--
[00:27:18.940 --> 00:27:21.140]   like, it's just about understanding first
[00:27:21.140 --> 00:27:24.260]   exactly what batch norm does, and then repeating that
[00:27:24.260 --> 00:27:28.780]   and mimicking that pattern to design non-batch norm networks.
[00:27:28.780 --> 00:27:32.220]   What they saw is, it changes the signal.
[00:27:32.220 --> 00:27:33.660]   It attenuates the signal.
[00:27:33.660 --> 00:27:38.180]   What it's trying to do is, if you have an input here,
[00:27:38.180 --> 00:27:40.660]   it will reduce the signal by some ratio, which
[00:27:40.660 --> 00:27:44.420]   is under root L, if L is the number of layers.
[00:27:44.420 --> 00:27:50.100]   So what that means is, if I'm a ResNet-34,
[00:27:50.100 --> 00:27:55.020]   what batch norm will do, it will reduce the signal value
[00:27:55.020 --> 00:27:59.140]   on the residual branch by under root 34 factor.
[00:27:59.140 --> 00:28:02.620]   So that's just one of the things that you should remember,
[00:28:02.620 --> 00:28:05.620]   or that you should know, is that when a signal is going down
[00:28:05.620 --> 00:28:09.060]   the residual path, batch norm will
[00:28:09.060 --> 00:28:13.380]   reduce the value of it and will divide it by some number.
[00:28:13.380 --> 00:28:16.180]   That's the main thing.
[00:28:16.180 --> 00:28:18.300]   And that's the skip init paper.
[00:28:18.300 --> 00:28:19.940]   It was a really good paper to read,
[00:28:19.940 --> 00:28:22.660]   and that's the main summary of it.
[00:28:22.660 --> 00:28:26.500]   So by now, we've looked at exactly how our signal behaves
[00:28:26.500 --> 00:28:31.540]   in a batch-normalized ResNet.
[00:28:31.540 --> 00:28:34.540]   What the authors now do--
[00:28:34.540 --> 00:28:37.100]   this is the main bit.
[00:28:37.100 --> 00:28:42.140]   What the authors now do, they say,
[00:28:42.140 --> 00:28:44.540]   my signal is going to look like this.
[00:28:44.540 --> 00:28:49.260]   XL plus 1 equals XL plus alpha FLXL by beta L.
[00:28:49.260 --> 00:28:50.180]   What does that mean?
[00:28:50.180 --> 00:28:53.060]   This is just all math that I do not understand.
[00:28:53.060 --> 00:28:54.900]   And the only way that I understand things
[00:28:54.900 --> 00:28:57.700]   is by drawing them.
[00:28:57.700 --> 00:29:00.700]   So what they say is, let the input is XL.
[00:29:00.700 --> 00:29:06.540]   So this is your residual branch, and this
[00:29:06.540 --> 00:29:10.060]   is the skip connection that gets added.
[00:29:10.060 --> 00:29:13.500]   So they call this residual branch as FL.
[00:29:13.500 --> 00:29:20.940]   And what they say, basically, is that when this--
[00:29:20.940 --> 00:29:25.540]   so we know from the skip init paper,
[00:29:25.540 --> 00:29:27.540]   we know that what batch norm will do
[00:29:27.540 --> 00:29:30.460]   is it will reduce the signal by some factor.
[00:29:30.460 --> 00:29:32.660]   So if I've called my signal XL, we
[00:29:32.660 --> 00:29:34.940]   want to reduce the signal by some factor.
[00:29:34.940 --> 00:29:39.060]   And they call that factor beta L. So that's XL by beta L.
[00:29:39.060 --> 00:29:40.940]   So they say, OK, this side is going
[00:29:40.940 --> 00:29:45.700]   to reduce the factor by beta L. That's the first thing.
[00:29:45.700 --> 00:29:48.460]   The second thing that they say is
[00:29:48.460 --> 00:29:51.300]   we want our variance to increase.
[00:29:51.300 --> 00:29:52.900]   So then-- so you have this.
[00:29:52.900 --> 00:29:55.220]   So your XL comes all the way here.
[00:29:55.220 --> 00:29:59.740]   The output of this is FLXL by beta L.
[00:29:59.740 --> 00:30:02.620]   Then they multiply it by a number called alpha.
[00:30:02.620 --> 00:30:07.380]   So you get alpha FLXL by beta L. So you get this part.
[00:30:07.380 --> 00:30:10.660]   And finally, you add the skip connection from here,
[00:30:10.660 --> 00:30:12.300]   which is plus XL.
[00:30:12.300 --> 00:30:15.140]   So that's how they designed the normalizer-free network.
[00:30:15.140 --> 00:30:17.860]   This is the main part.
[00:30:17.860 --> 00:30:20.220]   They designed the normalizer-free network
[00:30:20.220 --> 00:30:22.220]   like this.
[00:30:22.220 --> 00:30:28.420]   And this is just a theory of how they ended up designing
[00:30:28.420 --> 00:30:29.460]   and what's the thought.
[00:30:29.460 --> 00:30:32.220]   So basically, they've looked at exactly everything
[00:30:32.220 --> 00:30:33.340]   that batch norm does.
[00:30:33.340 --> 00:30:38.940]   And they've tried to replicate it in a non-batch norm network.
[00:30:38.940 --> 00:30:41.020]   So if I go into the theory of it, I'm really sorry.
[00:30:41.020 --> 00:30:43.500]   This paper is a lot math heavy.
[00:30:43.500 --> 00:30:44.860]   But this is the--
[00:30:44.860 --> 00:30:49.940]   but to understand it completely, we need to understand the math.
[00:30:49.940 --> 00:30:52.820]   And what they say is I'm going to set beta L equals
[00:30:52.820 --> 00:30:54.940]   to under root where--
[00:30:54.940 --> 00:30:58.220]   basically, the square root of variance of the input.
[00:30:58.220 --> 00:31:00.900]   What that means--
[00:31:00.900 --> 00:31:02.740]   so my input is XL.
[00:31:02.740 --> 00:31:06.100]   And beta L is set to this.
[00:31:06.100 --> 00:31:11.580]   It took me-- I'm not from a mathematics background myself.
[00:31:11.580 --> 00:31:13.340]   And it took me two weeks to understand
[00:31:13.340 --> 00:31:14.940]   exactly what's going on.
[00:31:14.940 --> 00:31:18.460]   I pretty much spent two weeks just trying to understand, oh,
[00:31:18.460 --> 00:31:20.660]   why did they design the network like this?
[00:31:20.660 --> 00:31:21.700]   What exactly is going on?
[00:31:21.700 --> 00:31:22.660]   What is alpha L?
[00:31:22.660 --> 00:31:24.180]   What is beta L?
[00:31:24.180 --> 00:31:26.020]   Without going too much into detail,
[00:31:26.020 --> 00:31:28.540]   but I still want to be able to explain
[00:31:28.540 --> 00:31:29.580]   these sort of things.
[00:31:29.580 --> 00:31:31.140]   In the blog, you'll see everything
[00:31:31.140 --> 00:31:35.780]   that explains all of this, all the math that's
[00:31:35.780 --> 00:31:37.940]   written here and all the questions as well,
[00:31:37.940 --> 00:31:43.260]   like why is beta L chosen to be square root variance XL, which
[00:31:43.260 --> 00:31:44.260]   is the input.
[00:31:44.260 --> 00:31:48.060]   The answer is when you divide an input
[00:31:48.060 --> 00:31:50.140]   by the square root of its variance--
[00:31:50.140 --> 00:31:51.940]   so if this is my input and I divide it
[00:31:51.940 --> 00:31:54.100]   by the square root of its variance,
[00:31:54.100 --> 00:31:59.140]   that makes sure that my variance is of that thing is equal to 1.
[00:32:00.140 --> 00:32:02.500]   So I have unit variance.
[00:32:02.500 --> 00:32:06.340]   So something I've already said, we
[00:32:06.340 --> 00:32:10.340]   want to make sure that the activations have mean 0
[00:32:10.340 --> 00:32:13.500]   and standard deviation 1.
[00:32:13.500 --> 00:32:17.940]   So this is a step in this direction.
[00:32:17.940 --> 00:32:18.660]   That's the idea.
[00:32:18.660 --> 00:32:25.380]   So this is the main bit.
[00:32:25.380 --> 00:32:28.940]   And the next thing that-- the reason why they have this alpha
[00:32:28.940 --> 00:32:30.060]   over here--
[00:32:30.060 --> 00:32:34.700]   so this will add some variance to my input signal.
[00:32:34.700 --> 00:32:36.300]   And the reason why they have this alpha
[00:32:36.300 --> 00:32:39.260]   is because when I looked at my last figure,
[00:32:39.260 --> 00:32:41.540]   I saw the average channel variance go up
[00:32:41.540 --> 00:32:43.860]   by a certain number until it crashes.
[00:32:43.860 --> 00:32:45.660]   So they want to be able to mimic that.
[00:32:45.660 --> 00:32:47.820]   They want to be able to increase that variance
[00:32:47.820 --> 00:32:51.620]   by a constant factor, and that constant factor
[00:32:51.620 --> 00:32:54.220]   is called alpha.
[00:32:54.220 --> 00:32:56.620]   So if we go into the blog, and let's have a look
[00:32:56.620 --> 00:32:58.460]   at what exactly is alpha.
[00:32:58.460 --> 00:33:02.420]   So I've already spent a lot of time on this paper
[00:33:02.420 --> 00:33:04.780]   just trying to understand exactly and everything that's
[00:33:04.780 --> 00:33:06.620]   going on.
[00:33:06.620 --> 00:33:10.340]   Let me just copy paste this bit.
[00:33:10.340 --> 00:33:23.860]   So we know that our network is designed like this--
[00:33:23.860 --> 00:33:26.980]   XL plus 1 equals XL by this.
[00:33:26.980 --> 00:33:31.620]   That's basically to say my input is XL.
[00:33:31.620 --> 00:33:34.620]   My residual branch is FL, so that's my residual branch.
[00:33:34.620 --> 00:33:42.220]   This is my skipping it, and that's that.
[00:33:42.220 --> 00:33:45.740]   And there's an alpha, which is the scalar that
[00:33:45.740 --> 00:33:47.100]   gets multiplied here.
[00:33:47.100 --> 00:33:53.060]   And there's a beta, so I just divide by beta L.
[00:33:53.060 --> 00:33:54.460]   So the input goes here.
[00:33:54.460 --> 00:33:57.340]   It gets divided by this number beta L,
[00:33:57.340 --> 00:34:00.860]   and then goes all the way down.
[00:34:00.860 --> 00:34:03.500]   So we know that beta L is chosen to be--
[00:34:03.500 --> 00:34:04.940]   we've already looked at this.
[00:34:04.940 --> 00:34:06.700]   Like, why is beta L equals to this?
[00:34:06.700 --> 00:34:09.060]   It's because we want it to have unit variance.
[00:34:09.060 --> 00:34:14.060]   So therefore, this thing, XL by beta L, has unit variance.
[00:34:14.060 --> 00:34:17.300]   Something that the authors say is that this residual branch
[00:34:17.300 --> 00:34:18.820]   is variance-preserving.
[00:34:18.820 --> 00:34:24.700]   All that means is if you have a function F,
[00:34:24.700 --> 00:34:27.340]   and you have an input XL, whatever
[00:34:27.340 --> 00:34:30.340]   the variance of this thing is, F of XL
[00:34:30.340 --> 00:34:31.540]   will have the same variance.
[00:34:31.540 --> 00:34:34.500]   That's exactly what it means by saying that this
[00:34:34.500 --> 00:34:35.620]   is variance-preserving.
[00:34:35.620 --> 00:34:37.500]   So this is my equation.
[00:34:37.500 --> 00:34:38.660]   If I take the variance--
[00:34:38.660 --> 00:34:40.620]   if I'm calculating the variance on both sides,
[00:34:40.620 --> 00:34:41.980]   then this is what I get--
[00:34:41.980 --> 00:34:46.500]   variance XL plus 1 equals variance XL plus this.
[00:34:46.500 --> 00:34:51.900]   We know that this thing, XL by beta L, has unit variance.
[00:34:51.900 --> 00:34:54.380]   In that case, the variance then just becomes this.
[00:34:54.380 --> 00:34:57.780]   So this alpha, the variance of it is alpha squared.
[00:34:57.780 --> 00:34:59.820]   So this is how this looks like.
[00:34:59.820 --> 00:35:01.220]   And as I say, it took me two weeks
[00:35:01.220 --> 00:35:02.900]   to get to this point and understand
[00:35:02.900 --> 00:35:06.260]   exactly what's going on, and why is this equation equating
[00:35:06.260 --> 00:35:06.780]   to this.
[00:35:06.780 --> 00:35:08.260]   It's just mentioned in the paper.
[00:35:08.260 --> 00:35:14.100]   So that's the main idea.
[00:35:14.100 --> 00:35:21.340]   So now-- so see how they mention it here?
[00:35:21.340 --> 00:35:25.820]   Variance XL equals variance XL minus 1 plus alpha squared.
[00:35:25.820 --> 00:35:28.020]   Now you exactly know why, and you exactly
[00:35:28.020 --> 00:35:30.700]   know what's going on.
[00:35:30.700 --> 00:35:32.300]   So this is the main idea.
[00:35:32.300 --> 00:35:34.260]   This is exactly what they did.
[00:35:34.260 --> 00:35:38.500]   I think at this point, we've kind of understood exactly
[00:35:38.500 --> 00:35:43.220]   and everything that's going on in a normalizer-free network.
[00:35:43.220 --> 00:35:46.820]   We've-- the main idea is this.
[00:35:46.820 --> 00:35:51.420]   If you want to remove BashNorm from the network, what you do
[00:35:51.420 --> 00:35:54.140]   is you first understand exactly what
[00:35:54.140 --> 00:35:56.380]   BashNorm does in that network, which
[00:35:56.380 --> 00:35:59.340]   they did through these signal propagation plots, which
[00:35:59.340 --> 00:36:00.860]   is these ones here.
[00:36:00.860 --> 00:36:03.460]   So they looked at and they understood that--
[00:36:03.460 --> 00:36:06.220]   they understood exactly how that particular element that you
[00:36:06.220 --> 00:36:07.580]   want to remove, which is BashNorm,
[00:36:07.580 --> 00:36:08.940]   behaves in that network.
[00:36:08.940 --> 00:36:12.260]   And then they went in and they plotted these three things,
[00:36:12.260 --> 00:36:14.660]   which gives us that signal propagation plot.
[00:36:14.660 --> 00:36:16.700]   And then they went in and they said, OK,
[00:36:16.700 --> 00:36:19.540]   I'm going to design my new network based
[00:36:19.540 --> 00:36:22.220]   on this equation, which we've already looked at.
[00:36:22.220 --> 00:36:23.100]   This is the equation.
[00:36:23.100 --> 00:36:27.260]   And then they went in and they said, OK,
[00:36:27.260 --> 00:36:31.020]   I'm going to design my new network based on this equation.
[00:36:31.020 --> 00:36:33.780]   And the reason why they designed this new network based
[00:36:33.780 --> 00:36:36.740]   on this equation-- like this is how the new normalizer-free
[00:36:36.740 --> 00:36:38.260]   block becomes--
[00:36:38.260 --> 00:36:40.180]   it's because they want to mimic everything.
[00:36:40.180 --> 00:36:42.220]   Like they want the variance to increase
[00:36:42.220 --> 00:36:44.780]   by a constant factor, which gives me this.
[00:36:44.780 --> 00:36:49.700]   So this alpha squared is the constant factor
[00:36:49.700 --> 00:36:51.220]   by which the variance increases.
[00:36:51.220 --> 00:36:55.620]   So this way I can replicate this chart over here.
[00:36:55.620 --> 00:36:57.980]   And by the other one, I can replicate this chart.
[00:36:57.980 --> 00:37:01.380]   So this is just the way of being able to replicate those charts.
[00:37:01.380 --> 00:37:05.540]   So moving on in the paper, this is just
[00:37:05.540 --> 00:37:09.900]   them explaining ReLU activations-induced mean shifts.
[00:37:09.900 --> 00:37:11.380]   This is just them explaining, which
[00:37:11.380 --> 00:37:13.220]   I've already explained to you, is
[00:37:13.220 --> 00:37:17.060]   like why is the mean going up and it keeps on going.
[00:37:17.060 --> 00:37:18.940]   It's because of this.
[00:37:18.940 --> 00:37:21.900]   When you remove the negative numbers,
[00:37:21.900 --> 00:37:23.340]   then the mean is bound to go up.
[00:37:23.340 --> 00:37:25.060]   And this is just them explaining that.
[00:37:25.060 --> 00:37:30.420]   So that's the main thing.
[00:37:30.420 --> 00:37:31.580]   So then what they did--
[00:37:31.580 --> 00:37:34.660]   now we're coming to the scaled weight standardization.
[00:37:34.660 --> 00:37:38.540]   But before we go in, here's what they did.
[00:37:38.540 --> 00:37:39.980]   So they plotted that.
[00:37:39.980 --> 00:37:41.660]   They plotted this whole thing.
[00:37:41.660 --> 00:37:44.740]   Like this equation that we've just seen, they plotted that.
[00:37:44.740 --> 00:37:46.820]   And they looked at the signal propagation plots.
[00:37:46.820 --> 00:37:48.580]   And the one thing that they noticed
[00:37:48.580 --> 00:37:52.460]   is that it doesn't look like what we want it to be.
[00:37:52.460 --> 00:37:59.820]   So the average square channel mean is still going up.
[00:37:59.820 --> 00:38:01.340]   The average square channel variance
[00:38:01.340 --> 00:38:02.420]   is going up, which is good.
[00:38:02.420 --> 00:38:04.340]   This is what we're trying to mimic.
[00:38:04.340 --> 00:38:08.500]   But when I said we want our mean to be 0 and variance to be 1,
[00:38:08.500 --> 00:38:10.020]   well, that's still not happening.
[00:38:10.020 --> 00:38:14.220]   How do we make sure that we have good signal propagation?
[00:38:14.220 --> 00:38:16.460]   The next thing that they introduced after doing all
[00:38:16.460 --> 00:38:18.700]   of this and after understanding, the next thing
[00:38:18.700 --> 00:38:20.380]   that the authors introduced is something
[00:38:20.380 --> 00:38:23.980]   called weight standardization.
[00:38:23.980 --> 00:38:26.060]   So what is weight standardization?
[00:38:26.060 --> 00:38:36.540]   So this is the paper for weight standardization.
[00:38:36.540 --> 00:38:40.700]   This is exactly what weight standardization is in figure 2.
[00:38:40.700 --> 00:38:43.540]   Basically, in figure 2, what they're trying to show
[00:38:43.540 --> 00:38:48.660]   is in weight standardization, what BatchNorm does--
[00:38:48.660 --> 00:38:54.980]   BatchNorm is trying to normalize the activations.
[00:38:54.980 --> 00:38:56.980]   And what weight standardization does,
[00:38:56.980 --> 00:39:01.980]   it's trying to normalize the convolution weights instead.
[00:39:01.980 --> 00:39:04.660]   So what that means?
[00:39:04.660 --> 00:39:08.860]   I have-- I'm just going to show you quickly.
[00:39:08.860 --> 00:39:13.820]   I have my conv here.
[00:39:13.820 --> 00:39:16.260]   This is my skip connection.
[00:39:16.260 --> 00:39:19.060]   So what BatchNorm will do is if this is my input,
[00:39:19.060 --> 00:39:22.580]   and I get I dash here, and I get pretty much my output here,
[00:39:22.580 --> 00:39:24.900]   and the thing is output plus input,
[00:39:24.900 --> 00:39:26.940]   because that's the skip connection, what BatchNorm will
[00:39:26.940 --> 00:39:28.980]   do, it will try and normalize this.
[00:39:28.980 --> 00:39:30.540]   So it's normalizing the activation.
[00:39:30.540 --> 00:39:32.820]   It's basically normalizing these inputs--
[00:39:32.820 --> 00:39:36.180]   not the input exactly, but things that are coming out,
[00:39:36.180 --> 00:39:38.180]   like things that are coming out.
[00:39:38.180 --> 00:39:40.380]   Like this is the activation, this is the activation,
[00:39:40.380 --> 00:39:43.100]   and then finally the output, like the outputs of things,
[00:39:43.100 --> 00:39:43.900]   basically.
[00:39:43.900 --> 00:39:45.740]   That's what it's trying to normalize.
[00:39:45.740 --> 00:39:48.460]   What weight standardization does,
[00:39:48.460 --> 00:39:50.380]   weight standardization said, instead
[00:39:50.380 --> 00:39:53.060]   of normalizing these things, which is the activations,
[00:39:53.060 --> 00:39:56.020]   let's normalize the cons instead.
[00:39:56.020 --> 00:39:57.340]   Let's normalize these.
[00:39:57.340 --> 00:39:59.620]   And by normalizing, I mean, it's just this, right?
[00:39:59.620 --> 00:40:02.740]   You take the weights, you subtract by the mean of it,
[00:40:02.740 --> 00:40:05.860]   and you divide by the standard deviation.
[00:40:05.860 --> 00:40:06.460]   And that's it.
[00:40:06.460 --> 00:40:11.900]   So this is what this does.
[00:40:11.900 --> 00:40:13.980]   This scaled weight standardization,
[00:40:13.980 --> 00:40:17.020]   it pretty much just introduces these two new things,
[00:40:17.020 --> 00:40:19.060]   as opposed to weight standardization,
[00:40:19.060 --> 00:40:20.620]   which are just scalars.
[00:40:20.620 --> 00:40:22.820]   This is called a gamma.
[00:40:22.820 --> 00:40:28.020]   The gamma will have a different value based on the activation.
[00:40:28.020 --> 00:40:30.860]   There's a lot of theory that they go into,
[00:40:30.860 --> 00:40:33.500]   which is explained here.
[00:40:33.500 --> 00:40:35.980]   I don't think it's necessary for us to understand,
[00:40:35.980 --> 00:40:44.460]   like I believe as long as we understand the overall meaning
[00:40:44.460 --> 00:40:47.300]   of what exactly is going on in an FRESN paper,
[00:40:47.300 --> 00:40:50.180]   there's things we can skip for the first time.
[00:40:50.180 --> 00:40:52.420]   Like if we dump--
[00:40:52.420 --> 00:40:54.380]   that's just one way of reading a paper.
[00:40:54.380 --> 00:40:56.500]   And something that I do is, like, I
[00:40:56.500 --> 00:40:58.540]   read a paper in multiple passes.
[00:40:58.540 --> 00:41:02.460]   So the first pass for me is to look into and understand
[00:41:02.460 --> 00:41:03.580]   the basic--
[00:41:03.580 --> 00:41:05.180]   like, what's the introduction?
[00:41:05.180 --> 00:41:06.300]   What's the abstract?
[00:41:06.300 --> 00:41:09.180]   Then I do my first pass, in which in first pass,
[00:41:09.180 --> 00:41:12.700]   I will look at things like, OK, let's go deeper
[00:41:12.700 --> 00:41:13.660]   into the figures.
[00:41:13.660 --> 00:41:15.820]   Then my second pass would be, OK, let's go deeper
[00:41:15.820 --> 00:41:17.460]   into what is weight standardization.
[00:41:17.460 --> 00:41:19.340]   I come from a background as well where
[00:41:19.340 --> 00:41:21.220]   I didn't know what weight standardization is.
[00:41:21.220 --> 00:41:25.660]   I didn't know how this thing has unit variance
[00:41:25.660 --> 00:41:26.460]   and all that stuff.
[00:41:26.460 --> 00:41:29.460]   So the second pass will help me understand the math.
[00:41:29.460 --> 00:41:32.500]   Then the third pass will help me replicate this in code.
[00:41:32.500 --> 00:41:34.620]   And we have that.
[00:41:34.620 --> 00:41:38.940]   So if you want to try the NF-RESNET paper,
[00:41:38.940 --> 00:41:42.060]   you can do so in PyTorch and Jack.
[00:41:42.060 --> 00:41:42.820]   So just in that--
[00:41:42.820 --> 00:41:50.620]   just in that, I've shared it in the chat.
[00:41:50.620 --> 00:41:53.700]   But Tim Labrie is one of the best to get started.
[00:41:53.700 --> 00:41:57.100]   And I can just go Tim.createModel NF-RESNET50.
[00:41:57.100 --> 00:42:01.060]   And then I can experiment with my NF-RESNET model.
[00:42:01.060 --> 00:42:04.220]   And I've also plotted like these.
[00:42:04.220 --> 00:42:07.820]   I've also plotted all of this stuff
[00:42:07.820 --> 00:42:11.180]   using NF-RESNET, which you'll find in this blog.
[00:42:11.180 --> 00:42:12.620]   But one thing I did want to--
[00:42:12.620 --> 00:42:14.620]   I do want to--
[00:42:14.620 --> 00:42:18.860]   also want to look into, given we have less time left.
[00:42:18.860 --> 00:42:20.580]   Something that I also want to look into
[00:42:20.580 --> 00:42:24.980]   is weight standardization.
[00:42:24.980 --> 00:42:27.180]   And this is what is weight standardization.
[00:42:27.180 --> 00:42:30.540]   So when we were looking into weight standardization,
[00:42:30.540 --> 00:42:34.980]   I said the weight standardization basically
[00:42:34.980 --> 00:42:37.980]   does this.
[00:42:37.980 --> 00:42:40.940]   It looks at these cons instead of--
[00:42:40.940 --> 00:42:43.940]   it normalizes the weights of the convolutions.
[00:42:43.940 --> 00:42:46.820]   Something I was looking into is this weight standardization.
[00:42:46.820 --> 00:42:49.140]   And this is how it is implemented.
[00:42:49.860 --> 00:42:54.180]   You just basically subtract the mean.
[00:42:54.180 --> 00:42:56.820]   You calculate the mean.
[00:42:56.820 --> 00:42:58.180]   You subtract the mean.
[00:42:58.180 --> 00:42:59.500]   You calculate the standard deviation.
[00:42:59.500 --> 00:43:01.020]   And you divide by standard deviation.
[00:43:01.020 --> 00:43:02.820]   And that becomes your weight standardization.
[00:43:02.820 --> 00:43:04.300]   That's really as easy.
[00:43:04.300 --> 00:43:06.380]   I mean, things look so complex when
[00:43:06.380 --> 00:43:08.100]   you're trying to read them in mathematics.
[00:43:08.100 --> 00:43:09.740]   But to me, things look really easy
[00:43:09.740 --> 00:43:12.340]   when I look at them in code.
[00:43:12.340 --> 00:43:14.660]   And this is what you should do as well on the side.
[00:43:14.660 --> 00:43:17.660]   And when I was experimenting--
[00:43:17.660 --> 00:43:20.540]   I was experimenting with group norm.
[00:43:20.540 --> 00:43:23.620]   And I was looking at weight standardization as group norm
[00:43:23.620 --> 00:43:24.820]   and weight standardization.
[00:43:24.820 --> 00:43:28.060]   And when I was looking at this, weight standardization
[00:43:28.060 --> 00:43:31.540]   and group norm together actually gave me really good results
[00:43:31.540 --> 00:43:34.100]   as compared to batch norm.
[00:43:34.100 --> 00:43:36.620]   So these are things that I really--
[00:43:36.620 --> 00:43:39.260]   I just want to take two minutes and say--
[00:43:39.260 --> 00:43:41.500]   I'll post this in the chat as well in case
[00:43:41.500 --> 00:43:43.340]   anybody wants to try.
[00:43:43.340 --> 00:43:49.700]   And this is part of the group norm blog.
[00:43:49.700 --> 00:44:00.140]   So that's exactly it.
[00:44:00.140 --> 00:44:05.260]   Like, that's all there is to the NF-ResNet paper.
[00:44:05.260 --> 00:44:06.660]   We want to remove batch norm.
[00:44:06.660 --> 00:44:08.540]   And this paper is able to do so.
[00:44:08.540 --> 00:44:11.940]   And by then implementing with--
[00:44:11.940 --> 00:44:16.140]   by then introducing weight standard--
[00:44:16.140 --> 00:44:17.420]   basically, weight standardization,
[00:44:17.420 --> 00:44:20.020]   scale weight standardization, you can see how--
[00:44:20.020 --> 00:44:26.820]   oops-- you can see how these results are really competitive.
[00:44:26.820 --> 00:44:31.620]   Like, ResNet-288 gets like an 80% accuracy on ImageNet.
[00:44:31.620 --> 00:44:35.100]   And finally, what they did was--
[00:44:35.100 --> 00:44:36.900]   this is the main bit.
[00:44:36.900 --> 00:44:39.380]   Finally, what they did by having everything--
[00:44:39.380 --> 00:44:41.220]   like, having my equation as this,
[00:44:41.220 --> 00:44:49.100]   xl plus 1 equals xl plus alpha f of xl by beta l.
[00:44:49.100 --> 00:44:52.100]   So this becomes-- so a ResNet is basically this, right?
[00:44:52.100 --> 00:44:56.060]   xl plus 1 equals f xl plus f l xl.
[00:44:56.060 --> 00:44:57.500]   Like, that's just the mathematical way
[00:44:57.500 --> 00:44:58.980]   of writing a ResNet.
[00:44:58.980 --> 00:45:00.620]   This is the ResNet.
[00:45:00.620 --> 00:45:02.980]   This is the NF-ResNet.
[00:45:02.980 --> 00:45:04.100]   So why is this the ResNet?
[00:45:04.100 --> 00:45:05.940]   It's just this is my residual branch.
[00:45:05.940 --> 00:45:07.340]   This is my input.
[00:45:07.340 --> 00:45:10.060]   The input passes through this residual branch.
[00:45:10.060 --> 00:45:13.580]   So what they did then, they applied this technique
[00:45:13.580 --> 00:45:16.980]   to this RegNet paper, which is--
[00:45:16.980 --> 00:45:19.060]   I have-- I looked at it like a month ago.
[00:45:19.060 --> 00:45:21.780]   It was just basically like the design--
[00:45:21.780 --> 00:45:23.420]   you design spaces.
[00:45:23.420 --> 00:45:25.580]   Like, it's like a reinforcement learning sort of way
[00:45:25.580 --> 00:45:27.980]   of designing the network architecture.
[00:45:27.980 --> 00:45:30.500]   And they applied this to this because it had really good
[00:45:30.500 --> 00:45:31.580]   results at the time.
[00:45:31.620 --> 00:45:36.660]   And you can see how NF-RegNets with cut mix and mix
[00:45:36.660 --> 00:45:41.300]   of augmentation have really competitive results
[00:45:41.300 --> 00:45:43.340]   with efficient nets.
[00:45:43.340 --> 00:45:44.300]   So that's the gist of it.
[00:45:44.300 --> 00:45:47.580]   That's NF-ResNet.
[00:45:47.580 --> 00:45:49.700]   From here, from this point on, if you really
[00:45:49.700 --> 00:45:53.500]   want to follow me along in this journey of reading
[00:45:53.500 --> 00:45:55.900]   deep learning papers, I've given you everything
[00:45:55.900 --> 00:45:58.900]   that you need to understand the NF-ResNet paper.
[00:45:58.900 --> 00:46:01.660]   If I were you, I would go--
[00:46:01.660 --> 00:46:02.740]   I would go--
[00:46:02.740 --> 00:46:04.100]   I would pick this up.
[00:46:04.100 --> 00:46:07.300]   I would read the blog as a first introduction.
[00:46:07.300 --> 00:46:08.980]   Then I would read the paper.
[00:46:08.980 --> 00:46:11.900]   Then I would go to Tim to try and experiment.
[00:46:11.900 --> 00:46:13.460]   And then finally, that's--
[00:46:13.460 --> 00:46:15.700]   then I have a really good understanding of exactly what
[00:46:15.700 --> 00:46:16.540]   NF-ResNets are.
[00:46:16.540 --> 00:46:18.300]   And if you want to experiment in the end,
[00:46:18.300 --> 00:46:22.060]   then that's just cherry on the top.
[00:46:22.060 --> 00:46:23.340]   But that's NF-ResNet.
[00:46:23.340 --> 00:46:26.180]   So let's go into questions.
[00:46:26.180 --> 00:46:26.680]   OK.
[00:46:26.680 --> 00:46:37.100]   So the last 10, 15 minutes is usually questions.
[00:46:37.100 --> 00:46:39.260]   And something I do encourage is, I really
[00:46:39.260 --> 00:46:41.340]   want to say if you guys want to turn your mics on,
[00:46:41.340 --> 00:46:42.860]   or if you want to talk to me, then I
[00:46:42.860 --> 00:46:44.780]   think this would be brilliant.
[00:46:44.780 --> 00:46:46.180]   What is batch number?
[00:46:46.180 --> 00:46:48.260]   This has been discussed.
[00:46:48.260 --> 00:46:49.060]   What is RegNet?
[00:46:49.060 --> 00:46:52.460]   Is it just ResNet with slightly different architecture?
[00:46:52.460 --> 00:46:53.540]   No, it is not.
[00:46:53.540 --> 00:46:55.540]   So the RegNet architecture--
[00:46:55.540 --> 00:46:58.060]   I think it was by Facebook.
[00:46:58.060 --> 00:47:00.180]   Yeah, it is by Facebook.
[00:47:00.180 --> 00:47:02.380]   So the RegNet, it's called designing.
[00:47:02.380 --> 00:47:08.620]   This is not it.
[00:47:08.620 --> 00:47:09.540]   RegNet design.
[00:47:09.540 --> 00:47:14.340]   That's the paper.
[00:47:14.340 --> 00:47:16.220]   So we can also look into the RegNet paper.
[00:47:16.220 --> 00:47:17.700]   It's really interesting as well.
[00:47:17.700 --> 00:47:19.820]   But basically, if you have a theory--
[00:47:19.820 --> 00:47:22.780]   if you know exactly how efficient nets are designed,
[00:47:22.780 --> 00:47:25.300]   efficient nets are designed by--
[00:47:25.300 --> 00:47:27.780]   sort of by using reinforcement learning.
[00:47:27.780 --> 00:47:34.220]   So just sort of going into digressing a little bit
[00:47:34.220 --> 00:47:36.500]   to tell you what the RegNet paper is.
[00:47:36.500 --> 00:47:43.100]   But this is how the efficient nets are designed.
[00:47:43.100 --> 00:47:44.940]   You have a controller, you have a trainer,
[00:47:44.940 --> 00:47:46.580]   and you have some objectives.
[00:47:46.580 --> 00:47:49.300]   And then based on reinforcement learning,
[00:47:49.300 --> 00:47:51.940]   this is similar to the MNASNet approach.
[00:47:51.940 --> 00:47:53.420]   Based on reinforcement learning, you
[00:47:53.420 --> 00:47:57.300]   try and find an architecture that really
[00:47:57.300 --> 00:47:59.180]   does work well for you.
[00:47:59.180 --> 00:48:01.860]   And RegNet sort of is a different way
[00:48:01.860 --> 00:48:02.820]   of doing the same thing.
[00:48:02.820 --> 00:48:05.860]   In that, they say we're going to design network design spaces.
[00:48:05.860 --> 00:48:06.580]   So having a look--
[00:48:06.580 --> 00:48:09.860]   I mean, go through that paper.
[00:48:09.860 --> 00:48:11.460]   But it's very different from a ResNet.
[00:48:11.620 --> 00:48:15.100]   [AUDIO OUT]
[00:48:15.100 --> 00:48:26.380]   Large paths should not be a problem for BashNorm.
[00:48:26.380 --> 00:48:27.860]   No, it isn't.
[00:48:27.860 --> 00:48:30.900]   But if it's-- you've seen networks
[00:48:30.900 --> 00:48:33.060]   that have been trained with--
[00:48:33.060 --> 00:48:35.260]   even I've experimented with the networks that have been
[00:48:35.260 --> 00:48:36.380]   trained with really large.
[00:48:36.380 --> 00:48:39.860]   But the one thing that I've seen in the past papers,
[00:48:39.860 --> 00:48:41.380]   and I've not experimented with it,
[00:48:41.380 --> 00:48:44.460]   and it's just because I lack the hardware.
[00:48:44.460 --> 00:48:49.100]   I've seen papers mention a really large path size
[00:48:49.100 --> 00:48:49.940]   is a problem.
[00:48:49.940 --> 00:48:51.660]   So a large path size is not a problem,
[00:48:51.660 --> 00:48:52.860]   but a really large path size.
[00:48:52.860 --> 00:48:54.700]   And what exactly does that mean?
[00:48:54.700 --> 00:48:57.500]   Because we look at the recent papers,
[00:48:57.500 --> 00:48:59.380]   if you have a look at efficient at V2,
[00:48:59.380 --> 00:49:02.620]   or if you have a look at the vision transformers,
[00:49:02.620 --> 00:49:06.340]   they all train on TPUs, and they all have path sizes like 5 and 2,
[00:49:06.340 --> 00:49:09.780]   or they have 256, or sometimes even 1024.
[00:49:09.780 --> 00:49:13.340]   And for those path sizes, you do get state of the art results.
[00:49:13.340 --> 00:49:16.420]   So what exactly-- at which point does BashNorm
[00:49:16.420 --> 00:49:18.700]   start to give a problem, and what exactly--
[00:49:18.700 --> 00:49:21.820]   which sort of path size?
[00:49:21.820 --> 00:49:24.060]   That's for me yet to explore.
[00:49:24.060 --> 00:49:33.300]   Aditya, for this to happen, graph study
[00:49:33.300 --> 00:49:36.180]   showed activations should strictly
[00:49:36.180 --> 00:49:38.260]   increase layer by layer, because we are already
[00:49:38.260 --> 00:49:41.020]   applying BashNorm, isn't it?
[00:49:41.020 --> 00:49:43.620]   I don't understand, but I guess--
[00:49:43.620 --> 00:49:46.220]   sorry, if you want to post your question again,
[00:49:46.220 --> 00:49:47.100]   that would be helpful.
[00:49:47.100 --> 00:49:50.460]   But if my understanding is--
[00:49:50.460 --> 00:49:51.740]   let's go back to the graphs.
[00:49:51.740 --> 00:50:02.020]   So these graphs are layer by layer.
[00:50:02.020 --> 00:50:07.860]   Like a ResNet V2 600 has 600 of these blocks on the side.
[00:50:07.860 --> 00:50:10.660]   So after everything, they calculate.
[00:50:10.660 --> 00:50:12.060]   After everything, they calculate.
[00:50:12.060 --> 00:50:13.780]   So it's like a--
[00:50:13.780 --> 00:50:16.460]   if that's what you meant, it is like that.
[00:50:16.460 --> 00:50:18.660]   Why does the average mean and standard fall down
[00:50:18.660 --> 00:50:19.660]   at the end of each block?
[00:50:19.660 --> 00:50:21.780]   Oh, that's a really, really good question.
[00:50:21.780 --> 00:50:23.660]   Thanks for asking, Parnav.
[00:50:23.660 --> 00:50:28.700]   OK, so this is a really interesting question.
[00:50:28.700 --> 00:50:39.780]   So if I go in the ResNet, so you can see in here in the ResNet,
[00:50:39.780 --> 00:50:45.060]   this is my first block, and this is 64 channels,
[00:50:45.060 --> 00:50:49.580]   and this is my second block, and this is 128 channels.
[00:50:49.580 --> 00:50:52.740]   The minute I go from the first block to the second block,
[00:50:52.740 --> 00:50:55.500]   like I'm going from 64 channels to 128,
[00:50:55.500 --> 00:50:57.980]   we also downsample the image in ResNet,
[00:50:57.980 --> 00:51:01.380]   and this is not even like NFS, this is ResNet.
[00:51:01.380 --> 00:51:02.900]   We downsample the image.
[00:51:02.900 --> 00:51:05.060]   So I'm just going to write downsample here.
[00:51:05.060 --> 00:51:07.380]   And because we downsample the image--
[00:51:07.380 --> 00:51:08.460]   so look at this.
[00:51:08.460 --> 00:51:16.100]   If my image size here is 3 by 2 to 4 by 2 to 4,
[00:51:16.100 --> 00:51:22.260]   if I downsample it and it becomes 3 by 112 by 112--
[00:51:22.260 --> 00:51:24.900]   sorry, if it becomes, say, if this is 64,
[00:51:24.900 --> 00:51:26.420]   because it's 64 channels.
[00:51:26.420 --> 00:51:31.980]   So let's just say it's 64 by that, and then this becomes 128.
[00:51:31.980 --> 00:51:35.020]   Then what they do, like this skip connection,
[00:51:35.020 --> 00:51:38.220]   because at this point, the input is of this size
[00:51:38.220 --> 00:51:40.420]   and the output is of this size.
[00:51:40.420 --> 00:51:42.020]   So the sizes are different.
[00:51:42.020 --> 00:51:43.940]   So you can't really add them.
[00:51:43.940 --> 00:51:45.820]   So with the skip connection, in that case,
[00:51:45.820 --> 00:51:47.220]   it's not just a skip connection.
[00:51:47.220 --> 00:51:50.780]   There's a convolution that downsamples the input as well.
[00:51:50.780 --> 00:51:55.060]   So this thing has a convolution in it.
[00:51:55.060 --> 00:51:56.820]   It's like a one by one.
[00:51:56.820 --> 00:51:59.260]   It's pretty much just downsample your input.
[00:51:59.260 --> 00:52:02.900]   Sorry, it's not a one by one.
[00:52:02.900 --> 00:52:07.260]   But there is a convolution to downsample the input.
[00:52:07.260 --> 00:52:10.260]   And when you have a convolution to downsample the input,
[00:52:10.260 --> 00:52:15.700]   then any growth that happens is reset.
[00:52:15.700 --> 00:52:18.740]   So at these points, like these points,
[00:52:18.740 --> 00:52:21.300]   this is where that block is being changed.
[00:52:21.300 --> 00:52:24.980]   And you have a convolution in the skip path, basically.
[00:52:24.980 --> 00:52:26.500]   And that's where the growth is reset.
[00:52:26.500 --> 00:52:38.620]   OK, Prasanna, you say the mean reduces by a factor.
[00:52:38.620 --> 00:52:40.660]   No, not the mean.
[00:52:40.660 --> 00:52:42.820]   Oh, yeah, as it passes through the bash norm layer,
[00:52:42.820 --> 00:52:44.980]   but the graph showed that the average square channel
[00:52:44.980 --> 00:52:48.060]   mean increases as we move down the blocks and then drop to 0.
[00:52:48.060 --> 00:52:52.660]   OK, I guess what you're trying to say is--
[00:52:52.660 --> 00:52:56.900]   what I said was you have--
[00:52:56.900 --> 00:52:58.660]   like this is how this looks like, right?
[00:52:58.660 --> 00:53:00.620]   And I said you have your input here.
[00:53:00.620 --> 00:53:02.220]   And this is my residual branch.
[00:53:02.220 --> 00:53:06.420]   And this is my skip branch, basically, or shortcut branch.
[00:53:06.420 --> 00:53:09.140]   So what I said was we go if it's like a--
[00:53:09.140 --> 00:53:11.580]   what we've seen is bash norm reduces this
[00:53:11.580 --> 00:53:13.020]   by a certain number.
[00:53:13.020 --> 00:53:14.380]   So it reduces this.
[00:53:14.380 --> 00:53:16.180]   But that's not to say that the mean reduces,
[00:53:16.180 --> 00:53:19.100]   because you still have your input signal coming from here.
[00:53:19.100 --> 00:53:21.060]   So you can't just say, oh, I'm going to do this.
[00:53:21.060 --> 00:53:24.540]   You have your input signal coming from here, right?
[00:53:24.540 --> 00:53:28.020]   And the output of it becomes i plus i by under root L.
[00:53:28.020 --> 00:53:32.100]   So that way, the mean can still go up.
[00:53:32.100 --> 00:53:36.020]   But this is to say that what it does, what bash norm does,
[00:53:36.020 --> 00:53:39.140]   is that it biases the--
[00:53:39.140 --> 00:53:41.940]   it biases the-- if you think of it,
[00:53:41.940 --> 00:53:44.740]   like if most of my signal passes through here,
[00:53:44.740 --> 00:53:46.500]   then in my forward and backward pass,
[00:53:46.500 --> 00:53:49.940]   there's not going to be any problem.
[00:53:49.940 --> 00:53:51.860]   There's not going to be any problem in my forward
[00:53:51.860 --> 00:53:52.900]   and backward propagation.
[00:53:52.900 --> 00:53:55.300]   And my gradient will never explode,
[00:53:55.300 --> 00:53:58.700]   as long as they're going through the shortcut path.
[00:53:58.700 --> 00:54:00.260]   So it's much easier for the inputs
[00:54:00.260 --> 00:54:02.180]   to go through the shortcut path than for them
[00:54:02.180 --> 00:54:04.860]   to go through the residual branch.
[00:54:04.860 --> 00:54:05.780]   So that's what I meant.
[00:54:05.780 --> 00:54:06.940]   I didn't mean to say--
[00:54:06.940 --> 00:54:09.300]   you can see how still the mean will go up,
[00:54:09.300 --> 00:54:12.420]   because you still have the skip in it input.
[00:54:12.420 --> 00:54:15.140]   So I hope that answers that.
[00:54:15.140 --> 00:54:15.640]   Oh.
[00:54:15.640 --> 00:54:22.640]   One second.
[00:54:22.640 --> 00:54:23.140]   OK.
[00:54:23.140 --> 00:54:28.880]   All right.
[00:54:28.880 --> 00:54:30.160]   Is alpha a constant?
[00:54:30.160 --> 00:54:34.240]   If so, its variance shouldn't it be 0?
[00:54:34.240 --> 00:54:38.400]   So could you please explain why variance alpha is alpha squared?
[00:54:38.400 --> 00:54:40.240]   Alpha is a constant.
[00:54:40.240 --> 00:54:42.400]   So that's why the variance of constants
[00:54:42.400 --> 00:54:43.920]   is like the square of it.
[00:54:43.920 --> 00:54:44.420]   OK.
[00:54:44.420 --> 00:54:55.520]   I mean, this is the statistical bit where--
[00:54:55.520 --> 00:54:57.400]   because alpha is being--
[00:54:57.400 --> 00:55:01.480]   so when I say alpha is a constant,
[00:55:01.480 --> 00:55:03.960]   the variance of the alpha itself would be 0.
[00:55:03.960 --> 00:55:05.360]   That's my understanding as well.
[00:55:05.360 --> 00:55:09.800]   But when you're multiplying-- because you are alpha L,
[00:55:09.800 --> 00:55:13.520]   fL, xL by beta L. And this thing has
[00:55:13.520 --> 00:55:14.640]   a unit variance.
[00:55:14.640 --> 00:55:20.120]   So the variance of this whole thing becomes alpha L squared.
[00:55:20.120 --> 00:55:21.280]   That's the idea.
[00:55:21.280 --> 00:55:23.920]   It's not to say that the variance of alpha L
[00:55:23.920 --> 00:55:24.800]   is equal to this.
[00:55:24.800 --> 00:55:25.320]   No.
[00:55:25.320 --> 00:55:28.200]   The variance of this whole thing with this as the unit variance,
[00:55:28.200 --> 00:55:29.960]   this becomes alpha L squared.
[00:55:29.960 --> 00:55:33.120]   That's my understanding.
[00:55:33.120 --> 00:55:34.840]   So I hope that helps.
[00:55:34.840 --> 00:55:42.200]   And this is exactly what I've tried to say in the paper.
[00:55:42.200 --> 00:55:44.600]   So since it's a scalar parameter--
[00:55:44.600 --> 00:55:45.600]   sorry.
[00:55:45.600 --> 00:55:46.480]   Oh, there it is.
[00:55:46.480 --> 00:55:50.800]   So basically, when you take the variance at this point,
[00:55:50.800 --> 00:55:52.880]   this is what gets replaced by alpha squared.
[00:55:52.880 --> 00:55:56.480]   OK.
[00:55:56.480 --> 00:55:59.640]   Why average squared mean is decreasing with depth?
[00:55:59.640 --> 00:56:01.200]   Oh, it's completely the opposite.
[00:56:01.200 --> 00:56:03.840]   I think average squared mean is increasing.
[00:56:03.840 --> 00:56:04.720]   So you can see how--
[00:56:04.720 --> 00:56:09.960]   yeah, the mean is going up.
[00:56:09.960 --> 00:56:10.460]   Yeah.
[00:56:10.460 --> 00:56:21.960]   Is it possible to use NFResNets for image semantic segmentation
[00:56:21.960 --> 00:56:24.480]   tasks like general ResNets and ResNets?
[00:56:24.480 --> 00:56:26.200]   Yes, totally.
[00:56:26.200 --> 00:56:28.160]   I think the best way, if you're a PyTorch user,
[00:56:28.160 --> 00:56:32.400]   I think the best way to start would be to go with--
[00:56:32.400 --> 00:56:35.560]   you pretty much start with team, which
[00:56:35.560 --> 00:56:36.800]   is one of my go-to libraries.
[00:56:36.800 --> 00:56:38.120]   And you just create your model.
[00:56:38.120 --> 00:56:40.280]   And with that model, you can try and do anything
[00:56:40.280 --> 00:56:42.280]   that you want to try.
[00:56:42.280 --> 00:56:43.200]   I have a doubt--
[00:56:43.200 --> 00:56:44.840]   [INAUDIBLE]
[00:56:44.840 --> 00:56:47.480]   I have a doubt I played around with team NFResNet.
[00:56:47.480 --> 00:56:50.680]   I compared pre-trained NFResNet50
[00:56:50.680 --> 00:56:54.320]   with TorchVision pre-trained ResNet50.
[00:56:54.320 --> 00:56:57.840]   Can you tell me, are they both trained on same image net
[00:56:57.840 --> 00:57:02.640]   or not, as it is much more accurate on same images?
[00:57:02.640 --> 00:57:03.600]   So when you say this--
[00:57:03.600 --> 00:57:05.640]   sorry, could you please reply to this thread
[00:57:05.640 --> 00:57:09.080]   or just say when you say it's more accurate on same images,
[00:57:09.080 --> 00:57:14.080]   do you mean to say NFResNet is more accurate than ResNet50?
[00:57:14.080 --> 00:57:18.080]   Or you could just say yes in the chat if you're around.
[00:57:18.080 --> 00:57:21.920]   So you've said NFResNet is more accurate, basically.
[00:57:21.920 --> 00:57:23.960]   It's because it's not just--
[00:57:23.960 --> 00:57:26.400]   just in the paper reading group two weeks ago,
[00:57:26.400 --> 00:57:28.520]   when we were looking at the ResNetRS paper,
[00:57:28.520 --> 00:57:31.920]   we looked at it's not just the network that's important.
[00:57:31.920 --> 00:57:34.400]   There's so many other things that are important, too,
[00:57:34.400 --> 00:57:37.200]   like the data augmentation, like the training recipe.
[00:57:37.200 --> 00:57:40.920]   So if you train the network with augmentations,
[00:57:40.920 --> 00:57:43.680]   there's definitely a possibility where NFResNet50
[00:57:43.680 --> 00:57:46.200]   will be more accurate than ResNet50.
[00:57:46.200 --> 00:57:48.840]   But this is definitely a PyTorch image question.
[00:57:48.840 --> 00:57:50.960]   So if you want to ping Ross, you go to here,
[00:57:50.960 --> 00:57:54.880]   and you can just go to issues or discussions.
[00:57:54.880 --> 00:57:56.640]   And you can just ping Ross, and you
[00:57:56.640 --> 00:58:00.520]   can ask a quick question on what exactly was NFResNet trained
[00:58:00.520 --> 00:58:03.680]   on, like what are the exact hyperparams.
[00:58:03.680 --> 00:58:06.000]   Any idea why group norm and weight standardization
[00:58:06.000 --> 00:58:06.920]   gives better results?
[00:58:06.920 --> 00:58:08.040]   Absolutely.
[00:58:08.040 --> 00:58:10.680]   So this is-- I would refer you to a previous blog
[00:58:10.680 --> 00:58:11.400]   on group norm.
[00:58:11.400 --> 00:58:16.200]   But the idea is weight standardization will also
[00:58:16.200 --> 00:58:21.120]   standardize the convolution weights, which make sure--
[00:58:21.120 --> 00:58:22.440]   like, when we're trying to--
[00:58:22.440 --> 00:58:28.720]   it makes it-- so the theory of it is, like from what I've read,
[00:58:28.720 --> 00:58:34.520]   the theory of it is if your weights have a mean 0
[00:58:34.520 --> 00:58:38.920]   and a standard deviation 1, and like if the--
[00:58:38.920 --> 00:58:49.200]   so let's see.
[00:58:49.200 --> 00:58:51.400]   This is my one of the lost landscapes,
[00:58:51.400 --> 00:58:53.480]   and this is my second one.
[00:58:53.480 --> 00:58:55.600]   A smoother lost landscape is much easier
[00:58:55.600 --> 00:58:58.920]   to optimize than something like this, on the other hand.
[00:58:58.920 --> 00:59:01.520]   Like this one is much harder to optimize.
[00:59:01.520 --> 00:59:04.640]   But doing something like this, we
[00:59:04.640 --> 00:59:08.360]   make sure that the lost landscape is smoother.
[00:59:08.360 --> 00:59:11.720]   And this is one of the reasons why BatchNorm is so successful.
[00:59:11.720 --> 00:59:13.520]   This is one of the advantages that you'll
[00:59:13.520 --> 00:59:14.720]   read pretty much everywhere.
[00:59:14.720 --> 00:59:18.240]   It's like BatchNorm takes--
[00:59:18.240 --> 00:59:21.720]   BatchNorm makes things--
[00:59:21.720 --> 00:59:25.160]   like BatchNorm smoothens the lost landscape.
[00:59:25.160 --> 00:59:28.000]   So then when you have weight standardization, what you're
[00:59:28.000 --> 00:59:30.040]   trying to do is you actually want
[00:59:30.040 --> 00:59:34.000]   to optimize the weights of a convolution.
[00:59:34.000 --> 00:59:37.680]   When they already have mean 0, standard deviation 1,
[00:59:37.680 --> 00:59:40.640]   it's much smoother and much easier to optimize.
[00:59:40.640 --> 00:59:42.440]   So that way, the scaled--
[00:59:42.440 --> 00:59:45.120]   and you can follow this notebook to experiment for yourself,
[00:59:45.120 --> 00:59:48.440]   but that's my understanding on why this works.
[00:59:52.240 --> 00:59:56.000]   Beta L is calculated, or is it calculated from the first--
[00:59:56.000 --> 00:59:59.040]   oh, beta L is a constant under root variance.
[00:59:59.040 --> 01:00:04.240]   Or is it calculated as input operation?
[01:00:04.240 --> 01:00:07.960]   No, pretty much you have your inputs.
[01:00:07.960 --> 01:00:10.480]   From my understanding, I've read this--
[01:00:10.480 --> 01:00:13.240]   from what I've seen in the implementation side of things,
[01:00:13.240 --> 01:00:15.560]   it's like you just calculate beta L once,
[01:00:15.560 --> 01:00:18.440]   and then that's your under root variance,
[01:00:18.440 --> 01:00:21.560]   and then you go on with it.
[01:00:21.560 --> 01:00:23.720]   So if we assume BatchNorm comparable results
[01:00:23.720 --> 01:00:25.320]   in this paper are true, does that
[01:00:25.320 --> 01:00:27.880]   mean models will make it both fast?
[01:00:27.880 --> 01:00:29.600]   Oh, yes, that's a really good point.
[01:00:29.600 --> 01:00:37.120]   So I want to point you to NFNet paper.
[01:00:37.120 --> 01:00:42.640]   In NFNet and the recent multiple drawing--
[01:00:42.640 --> 01:00:46.640]   multiple-- so these are two papers
[01:00:46.640 --> 01:00:49.920]   that you should also read after this.
[01:00:49.920 --> 01:00:53.520]   In high performance scale image regularization, they say--
[01:00:53.520 --> 01:00:59.960]   oh, here we go.
[01:00:59.960 --> 01:01:02.320]   Our smaller models match the test accuracy
[01:01:02.320 --> 01:01:06.600]   of an efficient P7 and while being 8.7x faster to train.
[01:01:06.600 --> 01:01:09.680]   So these are the same authors who first worked on NFResNet,
[01:01:09.680 --> 01:01:12.560]   and they ended up designing the NFNet paper.
[01:01:12.560 --> 01:01:14.440]   And our largest model attains new state
[01:01:14.440 --> 01:01:16.440]   of the art accuracy of 86.5.
[01:01:16.440 --> 01:01:19.520]   So NFNets, normalizer-free networks, work.
[01:01:19.520 --> 01:01:23.320]   NFNets are able to achieve state of the art,
[01:01:23.320 --> 01:01:25.600]   and they are definitely, definitely faster
[01:01:25.600 --> 01:01:28.400]   because it takes away all the memory overhead
[01:01:28.400 --> 01:01:31.680]   and takes away all the computation that BatchNorm does.
[01:01:31.680 --> 01:01:33.600]   Something I also want to point you guys to
[01:01:33.600 --> 01:01:37.520]   is just this very recent paper that came about, well,
[01:01:37.520 --> 01:01:39.560]   15 days ago on 27th May.
[01:01:39.560 --> 01:01:41.720]   It's drawing multiple augmentation.
[01:01:41.720 --> 01:01:44.280]   In this paper as well, they use an NFNet,
[01:01:44.280 --> 01:01:47.000]   and they use a strategy of drawing multiple augmentation
[01:01:47.000 --> 01:01:48.000]   samples.
[01:01:48.000 --> 01:01:51.680]   But this also achieves a new ImageNet state
[01:01:51.680 --> 01:01:53.280]   of the art of 86.8.
[01:01:53.280 --> 01:01:58.520]   So while the same authors got 86.5, this one gets 86.8
[01:01:58.520 --> 01:01:59.800]   without any extra data.
[01:01:59.800 --> 01:02:02.360]   So I should definitely, definitely recommend you guys
[01:02:02.360 --> 01:02:02.760]   to--
[01:02:02.760 --> 01:02:04.920]   for this to be the next paper that you guys read.
[01:02:04.920 --> 01:02:10.840]   How do they make sure there are no vanishing or exploding
[01:02:10.840 --> 01:02:11.640]   gradients?
[01:02:11.640 --> 01:02:12.240]   Thanks, Animesh.
[01:02:12.240 --> 01:02:13.440]   That's a good question.
[01:02:13.440 --> 01:02:17.120]   This whole idea of having a unit variance,
[01:02:17.120 --> 01:02:19.000]   like the divide by a beta L, which
[01:02:19.000 --> 01:02:23.640]   is the under root variance, making sure that XL by beta L
[01:02:23.640 --> 01:02:25.080]   has unit variance.
[01:02:25.080 --> 01:02:26.800]   I think that's the--
[01:02:26.800 --> 01:02:30.280]   as long as your forward pass has a mean 0 and standard deviation
[01:02:30.280 --> 01:02:33.640]   1, then that's good enough reason
[01:02:33.640 --> 01:02:35.680]   to think that the gradients will not explode.
[01:02:35.680 --> 01:02:37.960]   And the authors have made every effort
[01:02:37.960 --> 01:02:41.320]   to make sure that the mean is 0 and standard deviation is 1.
[01:02:41.320 --> 01:02:42.720]   But that's what they make sure.
[01:02:42.720 --> 01:02:45.280]   And something that's in the theory of restness
[01:02:45.280 --> 01:02:46.840]   that I've read in one of these papers
[01:02:46.840 --> 01:02:52.040]   is that they say, as long as ResNet has a good forward pass,
[01:02:52.040 --> 01:02:55.040]   the gradients will not explode on the way back,
[01:02:55.040 --> 01:02:56.840]   which means the backward pass.
[01:02:56.840 --> 01:02:58.520]   I don't understand the next question.
[01:02:58.520 --> 01:03:00.360]   What is team or theme?
[01:03:00.360 --> 01:03:01.880]   Oh, I've been saying team.
[01:03:01.880 --> 01:03:03.480]   So I'm not saying team.
[01:03:03.480 --> 01:03:08.440]   It was already taken by someone from the chat.
[01:03:08.440 --> 01:03:11.960]   So I guess it was referring to the Tim library.
[01:03:11.960 --> 01:03:13.040]   Oh, Tim.
[01:03:13.040 --> 01:03:13.680]   Yes, Tim.
[01:03:13.680 --> 01:03:14.240]   OK.
[01:03:14.240 --> 01:03:15.800]   So it's called PyTorch image models.
[01:03:15.800 --> 01:03:16.920]   I'm really sorry for the confusion.
[01:03:16.920 --> 01:03:18.320]   It's called PyTorch image models.
[01:03:18.320 --> 01:03:21.520]   But the abbreviation of it is called Tim.
[01:03:21.520 --> 01:03:28.520]   And that's it for today.
[01:03:28.520 --> 01:03:29.320]   Thanks, everybody.
[01:03:29.320 --> 01:03:30.680]   Here's just one more question.
[01:03:30.680 --> 01:03:33.240]   Whether you could post the papers you just mentioned
[01:03:33.240 --> 01:03:34.960]   as a recommendation, because I think
[01:03:34.960 --> 01:03:37.280]   it's really interesting for people to read.
[01:03:37.280 --> 01:03:38.360]   Oh, yeah, sure.
[01:03:38.360 --> 01:03:44.480]   Let me post them right on the paper reading over here.
[01:03:44.480 --> 01:03:47.400]   So that's one.
[01:03:47.400 --> 01:03:48.040]   Cool.
[01:03:48.040 --> 01:03:48.760]   Thanks, everybody.
[01:03:48.760 --> 01:03:50.000]   I will do.
[01:03:50.000 --> 01:03:54.480]   See you guys next week, where we're looking at efficient--
[01:03:54.480 --> 01:03:55.120]   no, next week.
[01:03:55.120 --> 01:03:56.840]   Yeah, next week we're looking at convit.
[01:03:56.840 --> 01:03:57.520]   Sorry.
[01:03:57.520 --> 01:03:58.480]   That's the 15th.
[01:03:58.480 --> 01:04:02.400]   So see you guys.
[01:04:02.400 --> 01:04:03.240]   See you next time.
[01:04:03.240 --> 01:04:06.600]   [MUSIC PLAYING]
[01:04:06.600 --> 01:04:09.960]   [MUSIC PLAYING]
[01:04:09.960 --> 01:04:13.320]   [MUSIC PLAYING]
[01:04:13.480 --> 01:04:16.840]   [MUSIC PLAYING]
[01:04:17.000 --> 01:04:20.360]   [MUSIC PLAYING]
[01:04:20.360 --> 01:04:22.940]   (upbeat music)

