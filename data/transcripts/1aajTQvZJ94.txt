
[00:00:00.000 --> 00:00:03.000]   [MUSIC]
[00:00:03.000 --> 00:00:07.500]   There's plenty of physics that you can do in the world, as far as I understand it,
[00:00:07.500 --> 00:00:11.800]   that doesn't involve having access to a super collider or things like that.
[00:00:11.800 --> 00:00:16.700]   And similarly, I believe that there are and will continue to be a lot of machine
[00:00:16.700 --> 00:00:21.400]   learning that doesn't rely on having access to collider scale resources for
[00:00:21.400 --> 00:00:22.660]   machine learning.
[00:00:22.660 --> 00:00:25.000]   >> You're listening to Gradient Dissent,
[00:00:25.000 --> 00:00:27.740]   a show about machine learning in the real world.
[00:00:27.740 --> 00:00:30.540]   And I'm your host, Lukas Biewald.
[00:00:30.540 --> 00:00:34.360]   I was recently introduced to Dee Scully as the new CEO of Kaggle,
[00:00:34.360 --> 00:00:37.460]   which is obviously an amazing site that we all love.
[00:00:37.460 --> 00:00:41.800]   But I later learned that he was the author of Machine Learning,
[00:00:41.800 --> 00:00:46.280]   the High Interest Credit Card of Technical Debt, which is a paper that inspired so
[00:00:46.280 --> 00:00:50.400]   many people, including myself, to go out and start machine learning tools companies.
[00:00:50.400 --> 00:00:53.540]   So could not be more excited to talk to him today.
[00:00:53.540 --> 00:00:58.040]   A note to our listeners, this conversation took place in August 2022.
[00:00:58.040 --> 00:01:00.600]   Since then, Kaggle has only continued to grow.
[00:01:00.600 --> 00:01:04.640]   All right, well, it's great to talk to you.
[00:01:04.640 --> 00:01:09.240]   And I think the impetus for talking was you taking over Kaggle,
[00:01:09.240 --> 00:01:12.640]   which is a really important website in the machine learning community and
[00:01:12.640 --> 00:01:18.120]   important to a lot of our listeners and users at Weights and Biases.
[00:01:18.120 --> 00:01:21.840]   But I realized in researching you, which I should have realized that you were
[00:01:21.840 --> 00:01:26.640]   the author of the Machine Learning High Interest Technical Debt paper,
[00:01:26.640 --> 00:01:31.120]   which I think inspired a lot of people and
[00:01:31.120 --> 00:01:33.960]   really resonated when it came out with me.
[00:01:33.960 --> 00:01:36.720]   And so I thought maybe you could start for
[00:01:36.720 --> 00:01:40.040]   people who haven't read this paper by kind of summarizing it.
[00:01:40.040 --> 00:01:46.080]   And I'm also curious if anything has changed since that paper was written.
[00:01:46.080 --> 00:01:50.040]   I'm trying to remember now, this must be like 2016 or 2017 that I think it came out.
[00:01:50.040 --> 00:01:51.280]   >> It was 2015, yeah.
[00:01:51.280 --> 00:01:51.960]   >> 2015.
[00:01:51.960 --> 00:01:53.600]   >> If I remember right.
[00:01:53.600 --> 00:01:56.000]   >> Right. >> It feels like a million years ago.
[00:01:56.000 --> 00:01:59.480]   >> But yeah, maybe before we get into it, I think a lot of people have read the paper,
[00:01:59.480 --> 00:02:02.240]   but for those who haven't, if you could kind of summarize the paper,
[00:02:02.240 --> 00:02:03.360]   that would be a great place to start.
[00:02:03.360 --> 00:02:04.800]   >> Yeah, sure.
[00:02:04.800 --> 00:02:08.040]   So, first of all, hi, thanks for having me.
[00:02:08.040 --> 00:02:09.000]   I really appreciate being here.
[00:02:09.000 --> 00:02:14.640]   Yeah, so my journey in machine learning,
[00:02:14.640 --> 00:02:18.280]   it's been a couple decades at this point.
[00:02:18.280 --> 00:02:21.320]   I spent a long time at Google working in production systems.
[00:02:21.320 --> 00:02:27.440]   So some of Google's most production critical ML systems for
[00:02:27.440 --> 00:02:32.280]   many years led some of Google's that click through PCDR systems for a while.
[00:02:32.280 --> 00:02:38.640]   And during that time, I gained a really clear appreciation for
[00:02:38.640 --> 00:02:42.080]   the importance of machine learning
[00:02:42.080 --> 00:02:48.120]   as a critical part of larger important systems.
[00:02:48.640 --> 00:02:52.880]   And got to experience firsthand all the different ways that things can
[00:02:52.880 --> 00:02:55.920]   go in unexpected directions.
[00:02:55.920 --> 00:03:02.360]   And these were systems that obviously had been around for a long time.
[00:03:02.360 --> 00:03:05.080]   At the time that we're talking about, I guess 2015 or so,
[00:03:05.080 --> 00:03:09.560]   the systems had already been in use in production,
[00:03:09.560 --> 00:03:13.440]   one form or fashion for more than a decade.
[00:03:13.440 --> 00:03:18.920]   And so at that time, I feel like my team and
[00:03:18.920 --> 00:03:24.400]   I had some insights into how things work in machine learning systems over the long
[00:03:24.400 --> 00:03:28.560]   term that not too many other people were in a position to be able to reflect on,
[00:03:28.560 --> 00:03:31.440]   just because it's a relatively new field at that point.
[00:03:31.440 --> 00:03:36.520]   So I thought it was useful to sort of write some of the things down that we
[00:03:36.520 --> 00:03:37.680]   were seeing.
[00:03:37.680 --> 00:03:41.440]   And using the metaphor of technical debt,
[00:03:41.440 --> 00:03:44.640]   I think was a useful way to frame some of those things.
[00:03:44.640 --> 00:03:48.760]   Because when we think about technical debt from a software engineering
[00:03:48.760 --> 00:03:53.920]   perspective, we think about the kinds of costs that you incur
[00:03:53.920 --> 00:03:55.640]   when you're moving fast.
[00:03:55.640 --> 00:03:59.200]   And you probably know something about moving fast in startup land and
[00:03:59.200 --> 00:04:06.680]   maybe having to make some tough calls between getting something out the door now
[00:04:06.680 --> 00:04:10.600]   versus adding in another six layers of integration testing or
[00:04:10.600 --> 00:04:13.240]   whatever the trade off might be.
[00:04:13.240 --> 00:04:15.560]   So there are really good reasons to move fast.
[00:04:15.560 --> 00:04:21.040]   It's sometimes unavoidable, but in doing so,
[00:04:21.040 --> 00:04:24.360]   we create some costs for ourselves over time that need to be paid down.
[00:04:24.360 --> 00:04:26.320]   It's not that we can never take those costs on, but
[00:04:26.320 --> 00:04:29.800]   we better be honest with ourselves about what those costs are.
[00:04:29.800 --> 00:04:35.440]   And at the time, I think it was underappreciated
[00:04:37.120 --> 00:04:42.360]   how much technical debt can be incurred through the use of machine learning.
[00:04:42.360 --> 00:04:48.160]   And so it's kind of obvious to sort of see that a machine learning stack
[00:04:48.160 --> 00:04:50.960]   is built on code, and so
[00:04:50.960 --> 00:04:54.400]   it has all of the technical debt opportunities that normal code has.
[00:04:54.400 --> 00:05:01.000]   But then it also has these system level behaviors that emerge over time
[00:05:01.000 --> 00:05:04.560]   that have nothing to do with sort of code level checks, but
[00:05:04.560 --> 00:05:09.040]   do in fact create costs that needs to be down.
[00:05:09.040 --> 00:05:11.840]   So even the simplest things you can think of,
[00:05:11.840 --> 00:05:15.720]   like when you're first building a model, oftentimes if you're in a hurry,
[00:05:15.720 --> 00:05:17.480]   you rush and you put a whole bunch of features in the model.
[00:05:17.480 --> 00:05:21.760]   Everything you can think of, you put it in there, accuracy is 0.9.
[00:05:21.760 --> 00:05:25.000]   You're like, okay, that's pretty good, but I can think of another 20 features and
[00:05:25.000 --> 00:05:27.800]   you put all those 20 new features in and now it's 0.92.
[00:05:27.800 --> 00:05:30.520]   And then you're like, well, it's pretty good, but
[00:05:30.520 --> 00:05:35.120]   if I put another 20 features in, then I get 0.93.
[00:05:35.120 --> 00:05:40.680]   And so we're sort of in this regime of diminishing returns to some degree.
[00:05:40.680 --> 00:05:44.000]   It's not necessarily clear when we're throwing all these features into a model
[00:05:44.000 --> 00:05:47.520]   what the value of each one is.
[00:05:47.520 --> 00:05:51.360]   And it's possible that we're putting a lot of features into a model that aren't
[00:05:51.360 --> 00:05:54.840]   particularly informative or where the information is being usefully conveyed
[00:05:54.840 --> 00:05:56.520]   already by some other feature or things like that.
[00:05:56.520 --> 00:05:58.760]   It's sort of like a bundled approach.
[00:05:58.760 --> 00:06:01.520]   It's typical of sort of early development in a machine learning pipeline.
[00:06:01.520 --> 00:06:06.720]   So we've made accuracy go up, but what could be the problem, right?
[00:06:06.720 --> 00:06:12.080]   So as I'm sure you've seen, every time you add a feature into a model,
[00:06:12.080 --> 00:06:14.240]   you create a dependency.
[00:06:14.240 --> 00:06:21.040]   You now have a dependency on some behavior observation in the outside world.
[00:06:21.040 --> 00:06:24.000]   And this means that you have a vulnerability
[00:06:24.000 --> 00:06:27.720]   if that behavior in the outside world changes.
[00:06:27.720 --> 00:06:31.360]   And it could change because people in the outside world change.
[00:06:31.360 --> 00:06:36.440]   It could change because the upstream producer of that signal changes.
[00:06:36.440 --> 00:06:40.880]   Maybe they create an upgrade, which sounds to them like a really great thing,
[00:06:40.880 --> 00:06:43.000]   but your model has learned not on the upgrade signal.
[00:06:43.000 --> 00:06:45.400]   It's learned all the weird errors and it's learned around them.
[00:06:45.400 --> 00:06:50.080]   So you could get some weird behaviors at upgrade time.
[00:06:50.080 --> 00:06:53.600]   Maybe they get sick of creating your nice feature and turn it off.
[00:06:53.600 --> 00:06:55.880]   That's not going to be a good day in the production system.
[00:06:57.080 --> 00:07:00.600]   And so it's really important that when we're thinking about model development,
[00:07:00.600 --> 00:07:05.160]   that we're thinking about the long-term costs of adding system complexity
[00:07:05.160 --> 00:07:08.120]   and model complexity and data complexity
[00:07:08.120 --> 00:07:10.800]   at the same time as we're thinking about improving accuracy.
[00:07:10.800 --> 00:07:16.440]   And I guess you've really experienced this firsthand.
[00:07:16.440 --> 00:07:20.080]   Is there any specific things that happened where you really thought like,
[00:07:20.080 --> 00:07:22.440]   "Ooh, that drives this point home"?
[00:07:22.440 --> 00:07:27.000]   Well, so I'm not going to tell any tales out of school, of course,
[00:07:27.000 --> 00:07:31.080]   but I will use the phrase, "You can imagine a lot."
[00:07:31.080 --> 00:07:33.240]   And you can imagine why.
[00:07:33.240 --> 00:07:39.920]   But you can imagine that if you had a model that was using,
[00:07:39.920 --> 00:07:44.040]   let's say, a topic model from some upstream producer,
[00:07:44.040 --> 00:07:49.080]   maybe that topic model that takes text and returns a sort of low-dimensional representation
[00:07:49.080 --> 00:07:52.520]   and sort of the topicality of that given piece of text,
[00:07:52.520 --> 00:07:55.360]   maybe in the early days of development of that topic model at night,
[00:07:55.360 --> 00:08:00.200]   it might not have had great coverage of non-English languages.
[00:08:00.200 --> 00:08:11.400]   And so if you're training a model to take that topic model as an input feature,
[00:08:11.400 --> 00:08:17.200]   then it might learn that the topics reported for certain low-coverage languages
[00:08:17.200 --> 00:08:21.440]   aren't particularly reliable for whatever reason.
[00:08:21.440 --> 00:08:25.000]   Maybe it assigns them a slight negative weight or something like that.
[00:08:26.000 --> 00:08:30.360]   And it's not too important because they just don't fire very often,
[00:08:30.360 --> 00:08:33.240]   so it doesn't show up in sort of aggregate metrics.
[00:08:33.240 --> 00:08:38.800]   And then you can imagine if you were a nascent machine learning engineer
[00:08:38.800 --> 00:08:40.600]   and didn't know any better,
[00:08:40.600 --> 00:08:43.800]   you learned that there was an upgraded version of this model
[00:08:43.800 --> 00:08:49.040]   that dramatically increased coverage in some of those low-resource languages,
[00:08:49.040 --> 00:08:53.480]   that now those topics might fire with much greater frequency.
[00:08:53.480 --> 00:08:58.800]   And so if you don't retrain your model,
[00:08:58.800 --> 00:09:04.520]   you can imagine that now those topic-level features inside your model
[00:09:04.520 --> 00:09:06.520]   are firing much, much more often
[00:09:06.520 --> 00:09:13.480]   and maybe sending a lot of content to lower scores than you might have expected.
[00:09:13.480 --> 00:09:17.720]   So that's the sort of thing that can happen.
[00:09:17.720 --> 00:09:22.000]   You can imagine things like an upstream producer of a given signal
[00:09:22.000 --> 00:09:25.680]   suddenly going offline without warning.
[00:09:25.680 --> 00:09:28.920]   And data is transitive.
[00:09:28.920 --> 00:09:34.480]   So it might be that the upstream producer of a signal that you're consuming
[00:09:34.480 --> 00:09:39.320]   also has an upstream producer of a signal it's consuming.
[00:09:39.320 --> 00:09:42.240]   That chain might hop several links.
[00:09:42.240 --> 00:09:47.040]   And so it could be that your system is being impacted
[00:09:47.040 --> 00:09:51.800]   by some other upstream signal several hops up in the fold
[00:09:51.800 --> 00:09:55.360]   and if you're not really careful about making sure that alerting
[00:09:55.360 --> 00:09:59.840]   and things like that are also being propagated transitively,
[00:09:59.840 --> 00:10:04.160]   you're not going to know until it's hitting your production data.
[00:10:04.160 --> 00:10:07.200]   And so these sorts of things can happen.
[00:10:07.200 --> 00:10:10.840]   And you want to be defensive as possible, right?
[00:10:10.840 --> 00:10:14.320]   So working on your early warning alertings and all these things
[00:10:14.320 --> 00:10:16.600]   to make sure that if something is coming down the pike,
[00:10:16.600 --> 00:10:18.880]   you get notified in advance.
[00:10:18.880 --> 00:10:21.320]   You also want to think about,
[00:10:21.320 --> 00:10:26.080]   we talk about coding defensively in regular engineering.
[00:10:26.080 --> 00:10:29.000]   You know, coding defensively on data often looks like
[00:10:29.000 --> 00:10:32.040]   monitoring of your input data distributions,
[00:10:32.040 --> 00:10:35.920]   checking for things like sudden changes in input data,
[00:10:35.920 --> 00:10:38.000]   SKUs or streams.
[00:10:38.000 --> 00:10:40.360]   One thing you could imagine is,
[00:10:40.360 --> 00:10:45.840]   let's say you have a model that is consuming data globally,
[00:10:45.840 --> 00:10:50.600]   but for whatever reason, a data center in a given part of the world
[00:10:50.600 --> 00:10:53.680]   goes down that day, like it can happen.
[00:10:53.680 --> 00:10:57.600]   Suddenly, your input data is likely to be highly skewed
[00:10:57.600 --> 00:10:58.800]   from what it normally looks like
[00:10:58.800 --> 00:11:01.040]   because you're missing a giant chunk of data,
[00:11:01.040 --> 00:11:03.120]   especially if there are, say,
[00:11:03.120 --> 00:11:06.600]   large local time of day effects.
[00:11:06.600 --> 00:11:09.400]   You could have very different behavior
[00:11:09.400 --> 00:11:11.840]   for a given day or period of days.
[00:11:11.840 --> 00:11:13.640]   Through an upstream outage that,
[00:11:13.640 --> 00:11:16.280]   if you don't have the proper input stream alerting about,
[00:11:16.280 --> 00:11:18.800]   you might not know what to think about.
[00:11:18.800 --> 00:11:21.680]   Do you feel like these problems are getting better
[00:11:21.680 --> 00:11:25.120]   or are getting worse?
[00:11:25.120 --> 00:11:29.480]   And how do you feel like the change to kind of more complicated,
[00:11:29.480 --> 00:11:35.080]   bigger, more black box models affects this calculus?
[00:11:35.080 --> 00:11:37.880]   In 2015, when we first wrote these papers,
[00:11:37.880 --> 00:11:41.080]   we got basically two reactions.
[00:11:41.080 --> 00:11:43.800]   One was the sort of, you know,
[00:11:43.800 --> 00:11:45.560]   very nice affirming reaction of,
[00:11:45.560 --> 00:11:47.240]   "Oh my gosh, this stuff is so important.
[00:11:47.240 --> 00:11:48.280]   Thanks for writing this down.
[00:11:48.280 --> 00:11:49.600]   We wouldn't have thought of any of these things."
[00:11:49.600 --> 00:11:52.400]   Or more often, "Yeah, we've encountered some of these things,
[00:11:52.400 --> 00:11:54.200]   but we didn't know that other people did too."
[00:11:54.200 --> 00:11:58.840]   You know, those kinds of reactions.
[00:11:58.840 --> 00:12:01.840]   The second major reaction that we got
[00:12:01.840 --> 00:12:05.280]   was from large parts of the animal research community
[00:12:05.280 --> 00:12:09.600]   that was basically like, "What are you people talking about?"
[00:12:09.600 --> 00:12:12.880]   And, you know, like that first NURBS paper
[00:12:12.880 --> 00:12:20.400]   got a full, you know, poker hand straight of review scores,
[00:12:20.400 --> 00:12:22.400]   you know, on the way from the highest possible,
[00:12:22.400 --> 00:12:23.880]   lowest possible, a couple in the middle,
[00:12:23.880 --> 00:12:26.480]   like just no idea really what to do with it.
[00:12:26.480 --> 00:12:30.840]   And, you know, eventually they let us in mostly on the like,
[00:12:30.840 --> 00:12:32.240]   "Well, you seem to be passionate
[00:12:32.240 --> 00:12:33.800]   about what you're talking about.
[00:12:33.800 --> 00:12:34.880]   People disagree with you maybe,
[00:12:34.880 --> 00:12:36.520]   so why don't you come and hash it out?"
[00:12:36.520 --> 00:12:38.040]   Which is a very reasonable statement
[00:12:38.040 --> 00:12:40.640]   that we were happy to do it.
[00:12:40.640 --> 00:12:44.720]   But I think, you know, the world here in 2022
[00:12:44.720 --> 00:12:47.440]   understands that these issues are real,
[00:12:47.440 --> 00:12:48.560]   that they're real work.
[00:12:48.560 --> 00:12:52.040]   They aren't just an accident or, you know,
[00:12:52.040 --> 00:12:53.880]   what happens if you hire the wrong animal engineer
[00:12:53.880 --> 00:12:54.840]   or something like that.
[00:12:54.840 --> 00:12:55.960]   They're systemic.
[00:12:55.960 --> 00:12:57.840]   And so we need to approach them systemically.
[00:12:57.840 --> 00:13:00.120]   So now there's this whole field of MLOps.
[00:13:00.120 --> 00:13:01.480]   And when you say, you know, MLOps,
[00:13:01.480 --> 00:13:03.320]   people nod sagely and say, "Yes, yes,
[00:13:03.320 --> 00:13:05.840]   we need to invest in MLOps."
[00:13:05.840 --> 00:13:09.800]   You know, it's a totally different world
[00:13:09.800 --> 00:13:11.480]   from that perspective in that you don't have
[00:13:11.480 --> 00:13:15.600]   to convince people that these problems are problems.
[00:13:15.600 --> 00:13:19.480]   That message I think has gotten through,
[00:13:19.480 --> 00:13:21.000]   and I'm happy about that.
[00:13:21.000 --> 00:13:25.240]   In terms of, you know, when you have much larger models
[00:13:25.240 --> 00:13:26.640]   do these problems get worse?
[00:13:26.640 --> 00:13:30.360]   They certainly get more acute.
[00:13:30.360 --> 00:13:34.480]   And, you know, I'm not gonna say that we're in a worst spot
[00:13:34.480 --> 00:13:36.080]   because I think that having, you know,
[00:13:36.080 --> 00:13:37.560]   a whole field of really smart people
[00:13:37.560 --> 00:13:39.680]   working on these problems and creating, you know,
[00:13:39.680 --> 00:13:41.160]   infrastructure that can help address them
[00:13:41.160 --> 00:13:43.160]   and things like that is a better spot to be in
[00:13:43.160 --> 00:13:45.080]   than having people think about these problems
[00:13:45.080 --> 00:13:47.040]   for the first time or rolling their own.
[00:13:47.040 --> 00:13:50.840]   But from a reliability standpoint,
[00:13:50.840 --> 00:13:55.360]   as our models get larger and larger,
[00:13:55.360 --> 00:13:57.040]   you know, why are we making models larger and larger?
[00:13:57.040 --> 00:13:58.160]   We're making them larger and larger
[00:13:58.160 --> 00:14:01.400]   because we wanna learn usefully from more and more data.
[00:14:01.400 --> 00:14:04.640]   Why are we throwing more and more data at a problem?
[00:14:04.640 --> 00:14:06.920]   You know, it's, if you were thinking of, you know,
[00:14:06.920 --> 00:14:08.680]   the problem of say estimating the probability
[00:14:08.680 --> 00:14:10.960]   that a coin is coming up heads,
[00:14:10.960 --> 00:14:12.880]   you know, you don't necessarily need to go
[00:14:12.880 --> 00:14:14.920]   from a billion to 10 billion examples, right?
[00:14:14.920 --> 00:14:16.680]   Like basic statistics always say that,
[00:14:16.680 --> 00:14:17.800]   yeah, after a couple hundred flips
[00:14:17.800 --> 00:14:20.120]   you're gonna get a pretty good estimate, you can stop.
[00:14:20.120 --> 00:14:21.480]   Right, but we don't do that with machine learning.
[00:14:21.480 --> 00:14:25.920]   We keep going because we need our models
[00:14:25.920 --> 00:14:29.320]   to exhibit ever more fine-grained behaviors
[00:14:29.320 --> 00:14:33.080]   and to respond usefully to a wider variety
[00:14:33.080 --> 00:14:35.320]   of input environments and scenarios.
[00:14:35.320 --> 00:14:39.240]   And so we have larger and larger data sets
[00:14:39.240 --> 00:14:43.360]   because we need to have more and more and more behaviors
[00:14:43.360 --> 00:14:46.400]   that our models can adapt to and can exhibit.
[00:14:46.400 --> 00:14:50.720]   Now, if you were to tell a typical software engineer,
[00:14:50.720 --> 00:14:52.760]   hey, the system that we're building
[00:14:52.760 --> 00:14:55.640]   used to need to have a thousand behaviors
[00:14:55.640 --> 00:14:57.440]   and now it's got a million,
[00:14:57.440 --> 00:14:58.360]   that person would probably say,
[00:14:58.360 --> 00:15:01.360]   well, our testing is probably also
[00:15:01.360 --> 00:15:02.520]   gonna be a priority here.
[00:15:02.520 --> 00:15:04.440]   And, you know, we used to have, you know,
[00:15:04.440 --> 00:15:06.160]   maybe 2000 unit tests, you know,
[00:15:06.160 --> 00:15:07.640]   two for each of these behaviors.
[00:15:07.640 --> 00:15:08.840]   Now you're telling me we've got a million,
[00:15:08.840 --> 00:15:10.680]   like, we're gonna have to hire
[00:15:10.680 --> 00:15:12.520]   a couple more test engineers, right?
[00:15:12.520 --> 00:15:14.880]   And maybe many more.
[00:15:14.880 --> 00:15:19.160]   When our models are being relied on
[00:15:19.160 --> 00:15:23.040]   to produce many, many more behaviors in a usable way,
[00:15:23.040 --> 00:15:25.760]   I think that this really ups the stakes
[00:15:25.760 --> 00:15:30.760]   on our overall processes of vetting and quality assurance
[00:15:31.520 --> 00:15:36.200]   and sanity checking and validation of our models.
[00:15:36.200 --> 00:15:39.800]   You know, the 20 years ago view of machine learning
[00:15:39.800 --> 00:15:41.880]   was basically like, look, you've got your test set
[00:15:41.880 --> 00:15:42.840]   and your training set.
[00:15:42.840 --> 00:15:45.360]   And so long as they're from the same distribution,
[00:15:45.360 --> 00:15:49.680]   we're just gonna assume that your test data
[00:15:49.680 --> 00:15:52.200]   has all the behaviors that you're gonna need to worry about.
[00:15:52.200 --> 00:15:54.680]   So no problem, just make sure you've got good accuracy
[00:15:54.680 --> 00:15:57.160]   on your held out test set.
[00:15:57.160 --> 00:15:59.440]   And that's not a silly place to start,
[00:15:59.440 --> 00:16:02.040]   but it's probably not a great place to end.
[00:16:02.040 --> 00:16:06.160]   You know, why do we use IID data sets
[00:16:06.160 --> 00:16:09.040]   from the same distribution for tests and training?
[00:16:09.040 --> 00:16:11.520]   You know, everybody knows that this is what you,
[00:16:11.520 --> 00:16:12.960]   quote unquote, should do.
[00:16:12.960 --> 00:16:15.080]   Let's remember why we're doing this.
[00:16:15.080 --> 00:16:19.120]   We're doing this because there are clever statisticians
[00:16:19.120 --> 00:16:24.120]   who for many decades have said important things
[00:16:24.120 --> 00:16:27.040]   like correlation is not causation, right?
[00:16:27.040 --> 00:16:29.280]   And the machine learning people are like,
[00:16:29.280 --> 00:16:31.400]   well, we're gonna just learn from correlations, right?
[00:16:31.400 --> 00:16:32.840]   We're learning from observational data.
[00:16:32.840 --> 00:16:34.640]   We've got giant amounts of observational data.
[00:16:34.640 --> 00:16:36.040]   So we're just gonna learn from that.
[00:16:36.040 --> 00:16:38.240]   And the statisticians are like, well,
[00:16:38.240 --> 00:16:39.720]   what are you gonna do about the whole like
[00:16:39.720 --> 00:16:41.680]   correlation is not causation thing?
[00:16:41.680 --> 00:16:43.880]   And the machine learning people's response is,
[00:16:43.880 --> 00:16:48.880]   well, if we guarantee that the test data
[00:16:48.880 --> 00:16:51.320]   is from the same distribution,
[00:16:51.320 --> 00:16:54.160]   then in terms of outcomes,
[00:16:54.160 --> 00:16:56.960]   we can ignore this inconvenient fact
[00:16:56.960 --> 00:16:58.680]   that correlation is not causation.
[00:16:59.640 --> 00:17:00.680]   And the statistician people are like,
[00:17:00.680 --> 00:17:03.840]   well, that's not awesome, but I guess you're right.
[00:17:03.840 --> 00:17:05.760]   And so long as you promise that your test data
[00:17:05.760 --> 00:17:07.920]   will always be from the same distribution,
[00:17:07.920 --> 00:17:09.720]   we can't really argue that.
[00:17:09.720 --> 00:17:12.280]   Obviously it's a caricature and I hope not to offend
[00:17:12.280 --> 00:17:14.520]   any statisticians or machine learning people in this.
[00:17:14.520 --> 00:17:18.640]   But so we do this IID test trains,
[00:17:18.640 --> 00:17:22.160]   but not because we think this is how the world works,
[00:17:22.160 --> 00:17:25.000]   but because if we don't do that,
[00:17:25.000 --> 00:17:27.520]   then we expose ourselves to a whole set
[00:17:27.520 --> 00:17:31.080]   of much more difficult problems
[00:17:31.080 --> 00:17:32.920]   in terms of the learning settings that we're in.
[00:17:32.920 --> 00:17:36.280]   And to some degree, all of the theoretical guarantees
[00:17:36.280 --> 00:17:40.160]   of supervised machine learning rely on this assumption
[00:17:40.160 --> 00:17:42.480]   that we're gonna be staying
[00:17:42.480 --> 00:17:44.240]   in this IID test train split world.
[00:17:44.240 --> 00:17:48.320]   And so this is all fine with the one small problem
[00:17:48.320 --> 00:17:51.960]   that the world actually almost never works this way.
[00:17:51.960 --> 00:17:55.640]   We can offline do our little research idea of like saying,
[00:17:55.640 --> 00:17:57.520]   okay, well, I've got my data set
[00:17:57.520 --> 00:17:58.560]   and I'm gonna split it carefully.
[00:17:58.560 --> 00:18:01.320]   And so these are therefore from the same distribution.
[00:18:01.320 --> 00:18:04.640]   But when we go in and deploy a model of the real world,
[00:18:04.640 --> 00:18:07.440]   it's pretty unlikely that the data that that model
[00:18:07.440 --> 00:18:10.040]   encounters is going to be from exactly
[00:18:10.040 --> 00:18:11.920]   the same distribution that happened to be
[00:18:11.920 --> 00:18:13.880]   in our limited historical snapshot of data
[00:18:13.880 --> 00:18:16.440]   that we collected previously.
[00:18:16.440 --> 00:18:19.840]   Just the world tends not to be that kind to us.
[00:18:19.840 --> 00:18:24.200]   And so our models are going to encounter data
[00:18:24.200 --> 00:18:26.200]   from different distributions.
[00:18:26.200 --> 00:18:28.080]   They're going to encounter worlds
[00:18:28.080 --> 00:18:31.320]   in which correlations that existed spuriously
[00:18:31.320 --> 00:18:33.000]   in our training data do not hold
[00:18:33.000 --> 00:18:35.080]   or maybe are explicitly broken
[00:18:35.080 --> 00:18:37.080]   in our production environment.
[00:18:37.080 --> 00:18:40.640]   And so this means that we have to really up our game
[00:18:40.640 --> 00:18:42.960]   on evaluation.
[00:18:42.960 --> 00:18:46.280]   It means that we can't just rely on test set accuracy
[00:18:46.280 --> 00:18:48.840]   or things like that as our final validation.
[00:18:48.840 --> 00:18:52.640]   We need to be much more rigorous about
[00:18:52.640 --> 00:18:56.800]   cataloging for ourselves and talking to our
[00:18:56.800 --> 00:18:59.200]   clever domain experts and things like those to tell us,
[00:18:59.200 --> 00:19:01.480]   okay, what are the places where our correlations
[00:19:01.480 --> 00:19:02.320]   are gonna break down?
[00:19:02.320 --> 00:19:04.160]   Where might our blind spots be?
[00:19:04.160 --> 00:19:07.680]   And how can we create specific stress tests
[00:19:07.680 --> 00:19:11.560]   to analyze our performance in these areas?
[00:19:11.560 --> 00:19:15.560]   - Well, it's funny, because I remember when
[00:19:16.680 --> 00:19:20.560]   in the very early days of deploying machine learning that
[00:19:20.560 --> 00:19:25.760]   having a held out a test set that was randomly sampled
[00:19:25.760 --> 00:19:28.680]   was actually kind of an improvement over
[00:19:28.680 --> 00:19:30.360]   the people's kind of first intuition,
[00:19:30.360 --> 00:19:32.360]   which is to just kind of try a bunch of different things
[00:19:32.360 --> 00:19:35.640]   and be like, I really want everything to improve.
[00:19:35.640 --> 00:19:38.360]   And I mean, I think one thing that can come up
[00:19:38.360 --> 00:19:41.280]   when you have lots of different evaluation sets
[00:19:41.280 --> 00:19:43.120]   and different constituents is,
[00:19:43.120 --> 00:19:46.360]   some number is gonna go down.
[00:19:46.360 --> 00:19:49.960]   If you have submission evaluation sets on any new model,
[00:19:49.960 --> 00:19:52.920]   you release it, it's hard to have kind of like
[00:19:52.920 --> 00:19:57.440]   a principled process for getting a new model
[00:19:57.440 --> 00:19:59.080]   into production.
[00:19:59.080 --> 00:20:02.120]   I'm curious how you think about that
[00:20:02.120 --> 00:20:03.120]   or kind of combat that,
[00:20:03.120 --> 00:20:06.000]   because I'm sure you're many more steps ahead
[00:20:06.000 --> 00:20:08.360]   along that journey and the work that you do.
[00:20:08.360 --> 00:20:11.560]   - Yeah, so what happens when you have a model
[00:20:11.560 --> 00:20:13.920]   that is better in many areas, but worse than some others,
[00:20:13.920 --> 00:20:16.280]   and how do you make the call and who chooses?
[00:20:16.280 --> 00:20:18.200]   These are really important problems.
[00:20:18.200 --> 00:20:22.280]   There are people who know a lot more
[00:20:22.280 --> 00:20:24.080]   about the world of ML fairness than I do,
[00:20:24.080 --> 00:20:28.160]   but I think it's easy to see that many of those kinds
[00:20:28.160 --> 00:20:31.160]   of fairness issues and human bias issues
[00:20:31.160 --> 00:20:33.680]   can creep in when folks are making decisions
[00:20:33.680 --> 00:20:36.360]   about version A versus version B
[00:20:36.360 --> 00:20:38.080]   and where are the improvements
[00:20:38.080 --> 00:20:39.160]   and where are the detriments to,
[00:20:39.160 --> 00:20:41.920]   for a given model improvement or update.
[00:20:41.920 --> 00:20:44.160]   So some of these are gonna be judgment calls.
[00:20:45.560 --> 00:20:50.120]   I think that to do this well,
[00:20:50.120 --> 00:20:53.640]   it's really helpful to have some standardized practices.
[00:20:53.640 --> 00:20:57.400]   So one standardized practice that I think is underutilized
[00:20:57.400 --> 00:21:00.840]   in the field is to have really detailed write-ups
[00:21:00.840 --> 00:21:04.680]   on every single model change that is being proposed
[00:21:04.680 --> 00:21:06.840]   for a new production launch.
[00:21:06.840 --> 00:21:09.360]   Yeah, almost like a paper or a mini paper,
[00:21:09.360 --> 00:21:12.880]   just about that one change, analyzing it in depth
[00:21:12.880 --> 00:21:17.640]   so that we can have some usefully distilled knowledge
[00:21:17.640 --> 00:21:19.160]   about what that change is.
[00:21:19.160 --> 00:21:23.040]   I think that machine learning people
[00:21:23.040 --> 00:21:25.560]   often play a little bit fast and loose
[00:21:25.560 --> 00:21:27.560]   with their experimentation.
[00:21:27.560 --> 00:21:32.560]   And the fact that it's useful to have infrastructure
[00:21:32.560 --> 00:21:35.200]   to support a notebook of experiments,
[00:21:35.200 --> 00:21:36.160]   like this is an improvement,
[00:21:36.160 --> 00:21:37.640]   like it's a really great thing to have,
[00:21:37.640 --> 00:21:39.280]   but it also says something to some degree
[00:21:39.280 --> 00:21:41.120]   about the state of the world
[00:21:41.120 --> 00:21:42.920]   where something like this is seen
[00:21:42.920 --> 00:21:46.680]   as a really useful innovation, which of course it is.
[00:21:46.680 --> 00:21:52.080]   But, so number one, making sure that every single change,
[00:21:52.080 --> 00:21:57.080]   no matter how small, is carefully analyzed and written down.
[00:21:57.080 --> 00:22:01.360]   I really do feel that writing things down is important.
[00:22:01.360 --> 00:22:03.440]   As much as I love having an automated system
[00:22:03.440 --> 00:22:05.360]   that collects all of your past experiments
[00:22:05.360 --> 00:22:07.200]   and sort of gives you the numbers,
[00:22:07.200 --> 00:22:10.520]   I think that that human step of reading through the numbers
[00:22:10.520 --> 00:22:13.720]   and drawing a conclusion
[00:22:13.720 --> 00:22:16.400]   and writing that conclusion down in human language
[00:22:16.400 --> 00:22:19.440]   so that it can be discussed and poked about
[00:22:19.440 --> 00:22:21.640]   is a really important step.
[00:22:21.640 --> 00:22:22.480]   To a first approximation,
[00:22:22.480 --> 00:22:24.680]   I think it's science is what happens when you write things down
[00:22:24.680 --> 00:22:26.840]   and it's important for us to be scientists.
[00:22:26.840 --> 00:22:31.480]   So then, what's standard practice?
[00:22:31.480 --> 00:22:35.440]   Everybody brings their write-ups into a meeting
[00:22:35.440 --> 00:22:37.840]   and people will talk about them.
[00:22:37.840 --> 00:22:39.120]   And there has to be a couple of people
[00:22:39.120 --> 00:22:40.680]   who make the call in the end.
[00:22:40.680 --> 00:22:42.560]   But these things should be discussed,
[00:22:42.560 --> 00:22:43.400]   they should be debated,
[00:22:43.400 --> 00:22:47.080]   they should be looked at from every lens
[00:22:47.080 --> 00:22:50.640]   with really carefully, with as much data and insight
[00:22:50.640 --> 00:22:51.880]   as we can bring to these problems.
[00:22:51.880 --> 00:22:54.040]   And then, usefully informed votes
[00:22:54.040 --> 00:22:55.200]   are gonna have to make a call.
[00:22:55.200 --> 00:22:58.760]   But we should be giving those decision makers
[00:22:58.760 --> 00:23:02.720]   as much context and insight as they possibly can.
[00:23:02.720 --> 00:23:03.960]   - Yeah, that makes sense.
[00:23:05.160 --> 00:23:09.840]   I guess another big change that's happened since 2015
[00:23:09.840 --> 00:23:13.720]   is many, many of the new applications and models
[00:23:13.720 --> 00:23:15.720]   operate on unstructured data.
[00:23:15.720 --> 00:23:17.280]   And I think there's sort of an implicit assumption
[00:23:17.280 --> 00:23:19.360]   even in talking about features
[00:23:19.360 --> 00:23:21.040]   that were operating on tabular data,
[00:23:21.040 --> 00:23:24.560]   which I think was the vast majority of use cases in 2015.
[00:23:24.560 --> 00:23:27.720]   Do you think there's anything that kind of changes
[00:23:27.720 --> 00:23:28.960]   about what you're talking about
[00:23:28.960 --> 00:23:33.960]   when the inputs are images or movies or audio files
[00:23:34.800 --> 00:23:37.320]   where you probably can't worry about the distribution
[00:23:37.320 --> 00:23:39.960]   of like the third pixel in every image?
[00:23:39.960 --> 00:23:43.160]   Like it's hard to say what that means even.
[00:23:43.160 --> 00:23:45.080]   - No, so it's a great point.
[00:23:45.080 --> 00:23:47.080]   I think that the basic ideas still hold,
[00:23:47.080 --> 00:23:49.760]   and I'm enough of a dinosaur that I say features
[00:23:49.760 --> 00:23:53.600]   sort of as my go-to.
[00:23:53.600 --> 00:23:57.400]   But I think that the same ideas hold directly
[00:23:57.400 --> 00:24:00.840]   even in unstructured data, like images, like video,
[00:24:00.840 --> 00:24:04.160]   like audio, like relatively unstructured text.
[00:24:05.160 --> 00:24:10.160]   I think the first line paper had this really nice example
[00:24:10.160 --> 00:24:14.320]   of Huskies on snow backgrounds versus non-snow backgrounds.
[00:24:14.320 --> 00:24:18.520]   And I don't think that we have to have extracted
[00:24:18.520 --> 00:24:23.520]   a feature is snowy background to see the point here.
[00:24:23.520 --> 00:24:27.680]   The questions are, what are the qualities of the data?
[00:24:27.680 --> 00:24:30.400]   What's the information that's being contained in the data?
[00:24:30.400 --> 00:24:33.640]   We can often talk about that using the language of features,
[00:24:33.640 --> 00:24:38.640]   but I think it holds generally for any sort of correlation
[00:24:38.640 --> 00:24:41.280]   that's going to exist in our input data.
[00:24:41.280 --> 00:24:42.960]   And so that could be more local,
[00:24:42.960 --> 00:24:47.960]   but it's snowy backgrounds or backgrounds in an image
[00:24:47.960 --> 00:24:51.760]   or facial characteristics in certain populations
[00:24:51.760 --> 00:24:55.240]   or any number of characteristics
[00:24:55.240 --> 00:24:57.560]   that can come through in video or image.
[00:24:59.360 --> 00:25:03.440]   - There's some pretty interesting stories
[00:25:03.440 --> 00:25:06.520]   of cancer detection on images
[00:25:06.520 --> 00:25:11.280]   that might've had Sharpie circles written around
[00:25:11.280 --> 00:25:12.960]   some of the images when they were annotated
[00:25:12.960 --> 00:25:15.160]   by the original doctors or things like that.
[00:25:15.160 --> 00:25:17.760]   Do those correspond to literal features?
[00:25:17.760 --> 00:25:21.040]   No, but there are certainly qualities of the data
[00:25:21.040 --> 00:25:22.480]   that we need to be aware of.
[00:25:22.480 --> 00:25:25.920]   In the same way that for audio input data,
[00:25:25.920 --> 00:25:29.440]   speaker characteristics and being inclusive
[00:25:29.440 --> 00:25:31.320]   of a wide range of speaker categories
[00:25:31.320 --> 00:25:33.520]   is really, really important.
[00:25:33.520 --> 00:25:37.000]   - So I guess I do want to talk some about Kaggle
[00:25:37.000 --> 00:25:39.120]   'cause that's your new job.
[00:25:39.120 --> 00:25:41.040]   I'm curious how it's going,
[00:25:41.040 --> 00:25:46.040]   but I'm also curious to know what got you excited
[00:25:46.040 --> 00:25:48.920]   about joining Kaggle in the first place.
[00:25:48.920 --> 00:25:51.480]   I guess kind of an interesting choice
[00:25:51.480 --> 00:25:54.800]   because so many, I mean, I love Kaggle.
[00:25:54.800 --> 00:25:58.200]   I think it's played a bigger role in the ML field
[00:25:58.200 --> 00:26:00.080]   than people even maybe realize.
[00:26:00.080 --> 00:26:02.720]   Like it was the first place I think a lot of people saw
[00:26:02.720 --> 00:26:05.800]   deep learning and it really working, for example.
[00:26:05.800 --> 00:26:09.200]   But the criticism Kaggle,
[00:26:09.200 --> 00:26:10.360]   and I think there's some truth to it,
[00:26:10.360 --> 00:26:14.440]   has always been that kind of making a high-performing model
[00:26:14.440 --> 00:26:17.200]   on a specific dataset is sort of the least of the problems
[00:26:17.200 --> 00:26:20.360]   of getting machine learning to work in the real world.
[00:26:20.360 --> 00:26:21.960]   And I feel like you're like this real expert
[00:26:21.960 --> 00:26:23.800]   on getting machine learning models
[00:26:23.800 --> 00:26:25.080]   to work in the real world.
[00:26:25.080 --> 00:26:30.160]   So how does that connect with you joining Kaggle?
[00:26:30.160 --> 00:26:33.240]   - Yeah, so great set of questions.
[00:26:33.240 --> 00:26:34.560]   So first of all, I am really excited
[00:26:34.560 --> 00:26:35.560]   about being part of Kaggle.
[00:26:35.560 --> 00:26:39.000]   I have had touch points with Kaggle
[00:26:39.000 --> 00:26:40.400]   at a couple of different points.
[00:26:40.400 --> 00:26:45.080]   I ran one of the early competitions
[00:26:45.080 --> 00:26:47.320]   and then we ran another competition
[00:26:47.320 --> 00:26:49.720]   called Inclusive Images a couple of years ago as well.
[00:26:49.720 --> 00:26:51.400]   So I've known the team for a long time
[00:26:51.400 --> 00:26:53.840]   and I've been a big fan of the platform.
[00:26:53.840 --> 00:26:56.120]   I don't know if you've ever seen any of the papers
[00:26:56.120 --> 00:26:58.800]   that I've written around the sort of state
[00:26:58.800 --> 00:27:01.000]   of the machine learning field in general,
[00:27:01.000 --> 00:27:06.000]   but I feel that we are at a bit of a tricky spot
[00:27:06.000 --> 00:27:11.480]   in the life cycle of the field of machine learning research.
[00:27:11.480 --> 00:27:16.760]   We're at a place where there are incredibly strong incentives
[00:27:16.760 --> 00:27:18.840]   for people to be publishing papers.
[00:27:19.760 --> 00:27:21.480]   I don't think I need to oversell that now,
[00:27:21.480 --> 00:27:25.600]   but it's true that publishing papers is a big deal.
[00:27:25.600 --> 00:27:28.440]   Yeah, when you add it all up,
[00:27:28.440 --> 00:27:31.040]   there's something like 10,000 papers a year,
[00:27:31.040 --> 00:27:33.760]   give or take, published at top conferences each year.
[00:27:33.760 --> 00:27:37.360]   But there's this sort of interesting thing.
[00:27:37.360 --> 00:27:41.920]   Each of those papers is claiming a 0.5% or 1% improvement
[00:27:41.920 --> 00:27:43.080]   on some important problem,
[00:27:43.080 --> 00:27:46.200]   but have we really improved the field
[00:27:46.200 --> 00:27:49.320]   by 5,000 or 10,000% per year?
[00:27:49.320 --> 00:27:51.280]   Like, I don't think so.
[00:27:51.280 --> 00:27:53.480]   So something interesting is happening there.
[00:27:53.480 --> 00:27:57.080]   If you've been involved with conferences,
[00:27:57.080 --> 00:28:01.000]   either as a submitter or a reviewer or an area chair,
[00:28:01.000 --> 00:28:03.680]   you'll notice that our reviewer pools
[00:28:03.680 --> 00:28:06.000]   are getting crazy tapped out,
[00:28:06.000 --> 00:28:07.760]   and they have been for some time.
[00:28:07.760 --> 00:28:11.400]   In today's conference reviewing world,
[00:28:11.400 --> 00:28:16.200]   it is often the case that reviewers
[00:28:16.200 --> 00:28:18.160]   may be first-year graduate students,
[00:28:19.160 --> 00:28:21.160]   which is obviously wonderful
[00:28:21.160 --> 00:28:22.720]   that they're performing the service,
[00:28:22.720 --> 00:28:24.440]   but it's quite a different thing
[00:28:24.440 --> 00:28:29.320]   to be getting a high-stakes review
[00:28:29.320 --> 00:28:33.280]   on the quality of a piece of research
[00:28:33.280 --> 00:28:34.800]   from someone just entering the field
[00:28:34.800 --> 00:28:37.240]   versus someone who's been in the field for many years.
[00:28:37.240 --> 00:28:39.840]   And this is just a function of the growth of the field.
[00:28:39.840 --> 00:28:44.080]   The growth of the field has been pretty astronomical.
[00:28:44.080 --> 00:28:48.360]   The number of papers sort of appearing per year,
[00:28:48.360 --> 00:28:49.960]   I believe, is growing exponentially.
[00:28:49.960 --> 00:28:52.600]   It certainly was the last time I checked.
[00:28:52.600 --> 00:28:55.120]   And the number of qualified reviewers
[00:28:55.120 --> 00:28:57.080]   is not growing exponentially.
[00:28:57.080 --> 00:28:59.080]   So this is interesting.
[00:28:59.080 --> 00:29:03.240]   As a field, it's easy to see
[00:29:03.240 --> 00:29:06.280]   that we're sort of fragmenting drastically
[00:29:06.280 --> 00:29:08.560]   across many, many benchmarks.
[00:29:08.560 --> 00:29:12.920]   As a field, we're really pushing this idea of novelty.
[00:29:12.920 --> 00:29:15.400]   It's quite difficult to get a paper published
[00:29:15.400 --> 00:29:17.680]   without a novel algorithm.
[00:29:18.680 --> 00:29:20.760]   And in terms of science,
[00:29:20.760 --> 00:29:23.640]   I think that this is leading to a world
[00:29:23.640 --> 00:29:27.520]   where we don't necessarily have the best understanding
[00:29:27.520 --> 00:29:32.320]   of the algorithms that we think are the best or the go-to
[00:29:32.320 --> 00:29:35.040]   because we're so busy inventing new ones.
[00:29:35.040 --> 00:29:37.000]   And just as a comparison point,
[00:29:37.000 --> 00:29:41.360]   I'm, no one would confuse me with a physician,
[00:29:41.360 --> 00:29:44.200]   but my understanding is that in the medical world,
[00:29:44.200 --> 00:29:48.840]   doctors often publish papers that are case studies
[00:29:48.840 --> 00:29:52.720]   about diseases or treatments or stuff like this.
[00:29:52.720 --> 00:29:57.240]   I would certainly hope that there is not a strong impetus
[00:29:57.240 --> 00:29:59.320]   that every single paper that is published
[00:29:59.320 --> 00:30:02.680]   in the medical field has a new treatment.
[00:30:02.680 --> 00:30:04.440]   If novelty is like the number one thing
[00:30:04.440 --> 00:30:06.480]   and every single medical thing
[00:30:06.480 --> 00:30:08.320]   has to be testing something new,
[00:30:08.320 --> 00:30:10.880]   I'd be worried as someone who likes to go to the doctor
[00:30:10.880 --> 00:30:12.200]   to get healthy.
[00:30:12.200 --> 00:30:13.120]   In the medical field,
[00:30:13.120 --> 00:30:15.280]   we often see meta-analyses.
[00:30:15.280 --> 00:30:17.040]   We often see replication results.
[00:30:17.040 --> 00:30:18.640]   We often see case studies,
[00:30:18.640 --> 00:30:21.520]   the sort of, you know, say reporting the experience
[00:30:21.520 --> 00:30:26.520]   of a given trial or a given treatment or things like this.
[00:30:26.520 --> 00:30:30.240]   And those kinds of papers are largely missing
[00:30:30.240 --> 00:30:33.400]   from the field of machine learning research right now.
[00:30:33.400 --> 00:30:34.920]   And I think it's a problem.
[00:30:34.920 --> 00:30:38.200]   When I look at Kaggle,
[00:30:38.200 --> 00:30:41.520]   I see a world where we're able to,
[00:30:42.360 --> 00:30:45.440]   promote much of this kind of missing work.
[00:30:45.440 --> 00:30:47.880]   When Kagglers approach a problem,
[00:30:47.880 --> 00:30:49.880]   there are often, you know,
[00:30:49.880 --> 00:30:54.880]   thousands of teams competing to solve a given problem.
[00:30:54.880 --> 00:30:58.920]   This means that the level of empirical rigor is,
[00:30:58.920 --> 00:30:59.760]   you know, to my mind,
[00:30:59.760 --> 00:31:02.320]   simply unmatched by any other process.
[00:31:02.320 --> 00:31:06.760]   And they're, you know, compared, you know, side by side,
[00:31:06.760 --> 00:31:09.080]   but so we get this nice leaderboard effect
[00:31:09.080 --> 00:31:10.080]   and things like this.
[00:31:10.080 --> 00:31:12.280]   But the community is also like,
[00:31:12.280 --> 00:31:15.400]   folks are committed to doing their best,
[00:31:15.400 --> 00:31:17.320]   but they're also committed to sharing
[00:31:17.320 --> 00:31:18.960]   and to communicating their ideas.
[00:31:18.960 --> 00:31:22.360]   And so, you know, through the notebooks platforms
[00:31:22.360 --> 00:31:23.520]   and other things like this that we have
[00:31:23.520 --> 00:31:25.200]   in the discussion forums,
[00:31:25.200 --> 00:31:27.920]   there is a tremendous amount of knowledge
[00:31:27.920 --> 00:31:30.760]   being shared, captured, disseminated,
[00:31:30.760 --> 00:31:34.680]   that is just this incredible resource for the field.
[00:31:34.680 --> 00:31:36.720]   And it's the kind of knowledge
[00:31:38.680 --> 00:31:40.800]   that isn't about novelty,
[00:31:40.800 --> 00:31:42.400]   it's about effectiveness
[00:31:42.400 --> 00:31:44.520]   and it's about rigorous understanding.
[00:31:44.520 --> 00:31:47.320]   And so to me, that's deeply compelling
[00:31:47.320 --> 00:31:49.880]   and something that I'm really excited to be a part of.
[00:31:49.880 --> 00:31:52.080]   Now, I believe that we can do more
[00:31:52.080 --> 00:31:55.400]   to help distill and share the knowledge
[00:31:55.400 --> 00:31:58.600]   that the community is generating,
[00:31:58.600 --> 00:32:01.320]   but it's there in, you know,
[00:32:01.320 --> 00:32:03.400]   implicitly in all of the discussion posts
[00:32:03.400 --> 00:32:04.400]   and all of the notebooks
[00:32:04.400 --> 00:32:07.160]   and all of the competition results and things like this.
[00:32:07.880 --> 00:32:11.160]   So I find that really exciting and really compelling.
[00:32:11.160 --> 00:32:13.200]   And I asked about MLOps and things like this,
[00:32:13.200 --> 00:32:16.920]   you know, obviously that is part of my background.
[00:32:16.920 --> 00:32:18.200]   And, you know, for me to go and say,
[00:32:18.200 --> 00:32:21.240]   look, we need really rigorous in-depth analysis
[00:32:21.240 --> 00:32:22.080]   of all our models.
[00:32:22.080 --> 00:32:23.200]   And then for me to, you know,
[00:32:23.200 --> 00:32:25.640]   then notice that on Kaggle, you know,
[00:32:25.640 --> 00:32:26.800]   almost all of our competitions
[00:32:26.800 --> 00:32:29.480]   have like a single number summary metric as the output.
[00:32:29.480 --> 00:32:31.400]   Like, yeah, I noticed a tension there.
[00:32:31.400 --> 00:32:35.920]   I think that over time we'll be pushing
[00:32:35.920 --> 00:32:40.040]   to help create more competition environments
[00:32:40.040 --> 00:32:41.680]   and other environments that allow people
[00:32:41.680 --> 00:32:46.200]   to experience more of a production environment,
[00:32:46.200 --> 00:32:48.680]   to be evaluated more on their ability
[00:32:48.680 --> 00:32:50.520]   to do things that are, you know,
[00:32:50.520 --> 00:32:53.040]   make sense in a production environment.
[00:32:53.040 --> 00:32:57.000]   We just had a competition close that measured efficiency
[00:32:57.000 --> 00:32:58.720]   as one of the evaluation metrics.
[00:32:58.720 --> 00:33:00.960]   I think things like that are really important.
[00:33:00.960 --> 00:33:02.640]   We can do a lot more in that area.
[00:33:02.640 --> 00:33:04.320]   So we're gonna, you know,
[00:33:04.320 --> 00:33:06.080]   push to make sure that the community is continuing
[00:33:06.080 --> 00:33:08.400]   to go in the most interesting and most important directions.
[00:33:08.400 --> 00:33:10.520]   I think that's good for everybody.
[00:33:10.520 --> 00:33:13.160]   But overall, I view, you know,
[00:33:13.160 --> 00:33:16.160]   Kaggle as one of the great resources
[00:33:16.160 --> 00:33:20.440]   in the ML world right now.
[00:33:20.440 --> 00:33:24.320]   I think it's been significantly underappreciated
[00:33:24.320 --> 00:33:27.440]   relative to the contributions it's already made
[00:33:27.440 --> 00:33:29.360]   as a community.
[00:33:29.360 --> 00:33:31.600]   But I think that with the little bit of help and guidance,
[00:33:31.600 --> 00:33:32.600]   we can do even more.
[00:33:33.600 --> 00:33:35.520]   - Yeah, I mean, I feel like Kaggle also does
[00:33:35.520 --> 00:33:39.080]   kind of an amazing thing of giving lots of people access
[00:33:39.080 --> 00:33:40.280]   to machine learning.
[00:33:40.280 --> 00:33:44.000]   Like, you know, it's a super friendly community
[00:33:44.000 --> 00:33:46.800]   and there's a lot of learning resources.
[00:33:46.800 --> 00:33:51.080]   And I do know a lot of people that kind of got their start
[00:33:51.080 --> 00:33:52.720]   in machine learning in Kaggle.
[00:33:52.720 --> 00:33:54.400]   And if they'd had to go, you know,
[00:33:54.400 --> 00:33:57.400]   back to school to get a PhD to engage in machine learning,
[00:33:57.400 --> 00:33:59.360]   they wouldn't have done it for sure.
[00:33:59.360 --> 00:34:02.520]   So I think that's an amazing thing.
[00:34:02.520 --> 00:34:05.760]   I wonder though, it's funny, you know,
[00:34:05.760 --> 00:34:08.520]   it's funny 'cause you just said,
[00:34:08.520 --> 00:34:09.840]   you just talked about, you know,
[00:34:09.840 --> 00:34:13.560]   kind of papers where they're trying to, you know,
[00:34:13.560 --> 00:34:17.240]   eke out the last like, you know, 0.1% of performance.
[00:34:17.240 --> 00:34:20.040]   And that does seem like something that Kaggle,
[00:34:20.040 --> 00:34:21.880]   you know, really celebrates.
[00:34:21.880 --> 00:34:24.440]   And there's part of me that like loves that.
[00:34:24.440 --> 00:34:26.080]   Like I think getting, you know,
[00:34:26.080 --> 00:34:27.840]   the last bit of performance out of a model
[00:34:27.840 --> 00:34:30.680]   is actually a pretty fun experience.
[00:34:30.680 --> 00:34:31.520]   - Absolutely.
[00:34:31.520 --> 00:34:33.640]   You know, I'm not gonna argue against
[00:34:33.640 --> 00:34:35.120]   really accurate models, right?
[00:34:35.120 --> 00:34:37.920]   You know, I think that the thing
[00:34:37.920 --> 00:34:40.040]   that's most interesting though is, you know,
[00:34:40.040 --> 00:34:41.520]   A, finding out what the header is,
[00:34:41.520 --> 00:34:43.840]   is really important for any given problem.
[00:34:43.840 --> 00:34:46.160]   And, you know, from a machine learning perspective,
[00:34:46.160 --> 00:34:48.080]   you know, we're often saying things like,
[00:34:48.080 --> 00:34:50.400]   well, the model is the most important thing.
[00:34:50.400 --> 00:34:54.560]   But all of these competitions are in application areas.
[00:34:54.560 --> 00:34:57.120]   Why, there are people who really care about the end user
[00:34:57.120 --> 00:35:00.160]   and really care about the, you know, solving their problem.
[00:35:00.160 --> 00:35:01.400]   You know, whether that's, you know,
[00:35:01.400 --> 00:35:03.360]   helping to save the Great Barrier Reef
[00:35:03.360 --> 00:35:08.360]   or identifying whales or helping to detect credit card fraud
[00:35:08.360 --> 00:35:09.560]   or anything in between.
[00:35:09.560 --> 00:35:11.920]   You know, those folks really care about
[00:35:11.920 --> 00:35:14.120]   solving important problems for the problem's sake,
[00:35:14.120 --> 00:35:15.760]   not necessarily from the machine learning standpoint.
[00:35:15.760 --> 00:35:17.440]   So making contributions on that side
[00:35:17.440 --> 00:35:18.960]   is also really important.
[00:35:18.960 --> 00:35:22.920]   But what I find when folks are motivated
[00:35:22.920 --> 00:35:24.560]   to squeeze every last, you know,
[00:35:24.560 --> 00:35:29.560]   percent out of a machine learning problem as a challenge,
[00:35:29.560 --> 00:35:33.240]   it leads to an incredible diversity of approaches.
[00:35:33.240 --> 00:35:35.400]   And that's the thing that I think is most interesting,
[00:35:35.400 --> 00:35:37.320]   is not, you know, necessarily that there was
[00:35:37.320 --> 00:35:39.760]   one winning solution at the end and we all, you know,
[00:35:39.760 --> 00:35:41.800]   celebrate that winner as an awesome person,
[00:35:41.800 --> 00:35:42.640]   although they are awesome people
[00:35:42.640 --> 00:35:44.040]   and we should celebrate them.
[00:35:44.040 --> 00:35:50.040]   It's that we also get a huge amount of information
[00:35:50.040 --> 00:35:51.880]   about other things that were tried
[00:35:51.880 --> 00:35:53.120]   and seemed like good ideas,
[00:35:53.120 --> 00:35:55.720]   didn't work as well for whatever reason.
[00:35:55.720 --> 00:35:57.160]   You know, you can think of this
[00:35:57.160 --> 00:35:59.000]   as like ablation studies at scale.
[00:35:59.000 --> 00:36:03.960]   So it's not just the position of the top of the leaderboard
[00:36:03.960 --> 00:36:05.920]   that's interesting information.
[00:36:05.920 --> 00:36:07.600]   The fact that we do have, you know,
[00:36:07.600 --> 00:36:09.720]   thousands of teams participating
[00:36:09.720 --> 00:36:11.480]   and we need this sort of competition structure
[00:36:11.480 --> 00:36:15.800]   to make sure that folks are, you know, properly aligned.
[00:36:15.800 --> 00:36:17.360]   But the results that come out of this,
[00:36:17.360 --> 00:36:18.920]   I think are interesting, you know,
[00:36:18.920 --> 00:36:22.720]   to distill up and down leaderboard.
[00:36:23.720 --> 00:36:24.560]   - Although it's funny, I mean,
[00:36:24.560 --> 00:36:26.760]   even without the competition structure,
[00:36:26.760 --> 00:36:30.280]   there's a lot more on Kaggle these days than competitions,
[00:36:30.280 --> 00:36:32.240]   which is useful and fun, right?
[00:36:32.240 --> 00:36:35.720]   I mean, I think when Anthony was talking to me
[00:36:35.720 --> 00:36:38.000]   on this podcast a while back,
[00:36:38.000 --> 00:36:39.280]   he was saying that the datasets
[00:36:39.280 --> 00:36:41.760]   was maybe even more popular in the competitions,
[00:36:41.760 --> 00:36:43.480]   which I was surprised to learn.
[00:36:43.480 --> 00:36:45.560]   - Yeah, so we do have, you know,
[00:36:45.560 --> 00:36:48.560]   I mean, Kaggle has become, you know,
[00:36:48.560 --> 00:36:51.640]   a really interesting set of resources for the world.
[00:36:51.640 --> 00:36:53.640]   And competitions is definitely one of them.
[00:36:53.640 --> 00:36:54.680]   But you're absolutely right.
[00:36:54.680 --> 00:36:58.040]   We have more usage of Kaggle
[00:36:58.040 --> 00:36:59.760]   for people looking to access datasets
[00:36:59.760 --> 00:37:02.040]   for their own machine learning needs
[00:37:02.040 --> 00:37:03.760]   than come to us for competitions.
[00:37:03.760 --> 00:37:07.480]   And that was something I didn't know before I joined Kaggle,
[00:37:07.480 --> 00:37:09.480]   but it's something that I've come to appreciate
[00:37:09.480 --> 00:37:10.320]   very deeply.
[00:37:10.320 --> 00:37:11.160]   We have, you know,
[00:37:11.160 --> 00:37:14.800]   I think 160,000 publicly shared datasets on Kaggle.
[00:37:14.800 --> 00:37:18.280]   It's an enormous trove of information.
[00:37:19.400 --> 00:37:23.720]   And what's great about datasets on Kaggle
[00:37:23.720 --> 00:37:26.840]   is that they're not sort of static things.
[00:37:26.840 --> 00:37:29.400]   There's opportunities for the community
[00:37:29.400 --> 00:37:32.920]   to post little discussions and notes and things like this,
[00:37:32.920 --> 00:37:35.000]   to post example notebooks,
[00:37:35.000 --> 00:37:37.400]   so that it's not just about, you know,
[00:37:37.400 --> 00:37:40.080]   getting a CSV file with a lot of numbers in it.
[00:37:40.080 --> 00:37:42.720]   It's about understanding what's in the dataset,
[00:37:42.720 --> 00:37:44.920]   where the lacks might be,
[00:37:44.920 --> 00:37:47.840]   where the strengths might be,
[00:37:47.840 --> 00:37:52.120]   and just having a really rich amount of annotation
[00:37:52.120 --> 00:37:55.280]   that sort of evolves from the community's involvement
[00:37:55.280 --> 00:37:56.600]   in these datasets.
[00:37:56.600 --> 00:37:58.040]   I think there's even more that we can do,
[00:37:58.040 --> 00:37:59.120]   and I'm excited to do that.
[00:37:59.120 --> 00:38:04.120]   But, you know, the datasets are a fantastic resource.
[00:38:04.120 --> 00:38:07.280]   The notebooks are an incredible resource.
[00:38:07.280 --> 00:38:10.440]   You know, there's an enormous amount
[00:38:10.440 --> 00:38:13.040]   of publicly shared notebooks,
[00:38:13.040 --> 00:38:14.440]   you know, hundreds and hundreds of thousands
[00:38:14.440 --> 00:38:17.080]   of shared notebooks that have example code,
[00:38:17.080 --> 00:38:20.720]   that have really carefully written explanatory text.
[00:38:20.720 --> 00:38:23.120]   So, you know, if you're looking to really learn
[00:38:23.120 --> 00:38:24.440]   how to do something,
[00:38:24.440 --> 00:38:26.520]   and you want some great examples,
[00:38:26.520 --> 00:38:30.320]   coming to Kaggle and surfing through example notebooks
[00:38:30.320 --> 00:38:31.320]   that have been publicly shared
[00:38:31.320 --> 00:38:34.320]   is a fantastically valuable place to start.
[00:38:34.320 --> 00:38:37.520]   We also have a wide variety of learning courses
[00:38:37.520 --> 00:38:39.600]   for folks who are just ramping up
[00:38:39.600 --> 00:38:41.520]   and getting their feet wet.
[00:38:41.520 --> 00:38:43.680]   I think it's important that we provide those on-ramps
[00:38:43.680 --> 00:38:46.480]   so that we can really be sharing, you know,
[00:38:46.480 --> 00:38:49.280]   machine learning knowledge as widely as we possibly can.
[00:38:49.280 --> 00:38:53.320]   - So, I mean, how do you think about the success of Kaggle?
[00:38:53.320 --> 00:38:56.360]   Do you look at it like a consumer website?
[00:38:56.360 --> 00:38:59.480]   Like, are you trying to increase the weekly active users
[00:38:59.480 --> 00:39:00.400]   or something like that?
[00:39:00.400 --> 00:39:03.640]   Or are you trying to make money with it or something else?
[00:39:03.640 --> 00:39:05.080]   How do you think about that?
[00:39:05.080 --> 00:39:10.080]   - Yeah, so I think that Kaggle is basically, you know,
[00:39:10.080 --> 00:39:13.040]   the rainforest of machine learning.
[00:39:13.040 --> 00:39:17.480]   It's this incredibly rich, incredibly valuable ecosystem
[00:39:17.480 --> 00:39:20.960]   that the world absolutely needs,
[00:39:20.960 --> 00:39:24.320]   and that we probably can't get by without.
[00:39:24.320 --> 00:39:28.680]   There's not like a direct revenue model,
[00:39:28.680 --> 00:39:30.800]   and I'm not super worried about that
[00:39:30.800 --> 00:39:34.160]   in the same way that I'm not, you know,
[00:39:34.160 --> 00:39:37.240]   super worried when, you know,
[00:39:37.240 --> 00:39:39.320]   companies have a very large research wing
[00:39:39.320 --> 00:39:40.480]   or things like that,
[00:39:40.520 --> 00:39:43.200]   that might not be, you know, directly revenue generating.
[00:39:43.200 --> 00:39:45.640]   I think that the knowledge that Kaggle is generating
[00:39:45.640 --> 00:39:46.480]   for the world,
[00:39:46.480 --> 00:39:50.200]   the value that Kaggle creates for the world
[00:39:50.200 --> 00:39:55.200]   is so valuable that we can make a very strong case
[00:39:55.200 --> 00:39:57.760]   that this just needs to exist.
[00:39:57.760 --> 00:40:01.760]   And, you know, as a team, we're pretty scrappy.
[00:40:01.760 --> 00:40:04.880]   You know, it's amazing that we have, you know,
[00:40:04.880 --> 00:40:09.320]   we've crossed the 10 million user threshold
[00:40:09.320 --> 00:40:11.520]   with a team of 50, right?
[00:40:11.520 --> 00:40:14.200]   Like it's not a huge operation.
[00:40:14.200 --> 00:40:16.680]   And the work that folks do, you know,
[00:40:16.680 --> 00:40:21.360]   from the, you know, notebooks teams to the data sets teams,
[00:40:21.360 --> 00:40:23.320]   to the folks creating learning content,
[00:40:23.320 --> 00:40:25.560]   to our competition teams,
[00:40:25.560 --> 00:40:27.320]   you know, these folks all work really hard.
[00:40:27.320 --> 00:40:28.880]   They're amazing people,
[00:40:28.880 --> 00:40:31.480]   but they have an incredibly large influence
[00:40:31.480 --> 00:40:33.800]   across the world for what they're doing.
[00:40:33.800 --> 00:40:36.720]   So in terms of, you know, how do I think about Kaggle?
[00:40:36.720 --> 00:40:38.600]   I think about Kaggle as an ecosystem.
[00:40:39.600 --> 00:40:43.200]   This ecosystem has a bunch of different parts
[00:40:43.200 --> 00:40:45.400]   that interact with each other.
[00:40:45.400 --> 00:40:46.880]   You know, we have folks who are coming to us
[00:40:46.880 --> 00:40:48.080]   as novice learners.
[00:40:48.080 --> 00:40:50.800]   We have folks who are coming to us as practitioners.
[00:40:50.800 --> 00:40:53.000]   And, you know, maybe they're, you know,
[00:40:53.000 --> 00:40:55.040]   already doing machine learning on a daily basis
[00:40:55.040 --> 00:40:55.880]   as part of their job.
[00:40:55.880 --> 00:40:58.120]   Maybe they're, you know, quite advanced in their studies
[00:40:58.120 --> 00:41:00.120]   and hoping to be doing machine learning
[00:41:00.120 --> 00:41:03.360]   on a daily basis very soon.
[00:41:03.360 --> 00:41:05.240]   We have cutting edge researchers.
[00:41:05.240 --> 00:41:07.960]   You know, Jeff Hinton was a famous early winner
[00:41:07.960 --> 00:41:09.920]   of one of our competitions.
[00:41:09.920 --> 00:41:12.200]   We have a, you know, a large engagement
[00:41:12.200 --> 00:41:13.200]   from cutting edge researchers,
[00:41:13.200 --> 00:41:15.400]   and they bring different things to our community,
[00:41:15.400 --> 00:41:17.880]   and they enrich the community for each other.
[00:41:17.880 --> 00:41:19.840]   You know, without the novice learners,
[00:41:19.840 --> 00:41:21.480]   I think that we would lose a ton of
[00:41:21.480 --> 00:41:26.440]   sort of enthusiastic energy and, you know,
[00:41:26.440 --> 00:41:28.920]   sort of keeping us on a stress testing.
[00:41:28.920 --> 00:41:30.360]   Without the practitioners,
[00:41:30.360 --> 00:41:32.040]   I think that we'd be losing a lot of,
[00:41:32.040 --> 00:41:33.760]   you know, real practical know-how
[00:41:33.760 --> 00:41:35.640]   and knowledge for the community
[00:41:35.640 --> 00:41:37.520]   that gets shared really wonderfully.
[00:41:37.920 --> 00:41:39.240]   Without the cutting edge researchers,
[00:41:39.240 --> 00:41:42.440]   we probably aren't able to have anywhere near
[00:41:42.440 --> 00:41:44.560]   as interesting a variety of competitions
[00:41:44.560 --> 00:41:46.320]   that are being hosted,
[00:41:46.320 --> 00:41:50.440]   or, you know, the real next generation solutions
[00:41:50.440 --> 00:41:52.160]   coming down the pike.
[00:41:52.160 --> 00:41:54.280]   And of course, you know, our, you know,
[00:41:54.280 --> 00:41:55.360]   as you say, you know, competitions
[00:41:55.360 --> 00:41:56.280]   isn't all what we're about.
[00:41:56.280 --> 00:41:57.760]   You know, if we don't have the notebooks,
[00:41:57.760 --> 00:41:59.040]   then I think that we lose a lot.
[00:41:59.040 --> 00:42:00.280]   If we don't have the data sets,
[00:42:00.280 --> 00:42:01.480]   I think that we lose a lot.
[00:42:01.480 --> 00:42:04.080]   So these things play together, you know,
[00:42:04.080 --> 00:42:07.320]   in a sort of interconnected web of machine learning
[00:42:07.320 --> 00:42:08.560]   in a really interesting way.
[00:42:08.560 --> 00:42:10.640]   And I think that thinking about Kaggle
[00:42:10.640 --> 00:42:13.800]   as a valuable ecosystem and celebrating,
[00:42:13.800 --> 00:42:16.560]   you know, sort of the ecosystem viewpoint
[00:42:16.560 --> 00:42:18.640]   of evaluating whether we're doing a good job
[00:42:18.640 --> 00:42:19.600]   is the right thing.
[00:42:19.600 --> 00:42:24.200]   - But so then how do you measure the ecosystem?
[00:42:24.200 --> 00:42:25.960]   Is it by usage?
[00:42:25.960 --> 00:42:27.440]   Is that the?
[00:42:27.440 --> 00:42:30.280]   - Yeah, so, you know, what is our one magic metric?
[00:42:30.280 --> 00:42:31.440]   We don't have one magic metric.
[00:42:31.440 --> 00:42:36.440]   - Yeah, how do you measure an ecosystem's health, I guess?
[00:42:37.120 --> 00:42:37.960]   - Yep, absolutely.
[00:42:37.960 --> 00:42:41.240]   So that was something I typed into Google
[00:42:41.240 --> 00:42:43.080]   on week two of the job.
[00:42:43.080 --> 00:42:44.400]   How do you measure, yeah,
[00:42:44.400 --> 00:42:46.680]   how do people who study ecosystems measure health?
[00:42:46.680 --> 00:42:49.800]   And it is absolutely a thing
[00:42:49.800 --> 00:42:52.520]   that requires very gated analysis.
[00:42:52.520 --> 00:42:57.240]   And so, you know, when you talk to an ecologist
[00:42:57.240 --> 00:42:59.280]   about how they measure ecosystems health,
[00:42:59.280 --> 00:43:00.120]   they'll tell you, look, you know,
[00:43:00.120 --> 00:43:02.560]   we can't just measure whether the butterflies are happy,
[00:43:02.560 --> 00:43:04.480]   right, we can't just measure whether the birds are happy.
[00:43:04.480 --> 00:43:06.680]   We actually have to have useful metrics
[00:43:06.680 --> 00:43:08.880]   on each of the different segments.
[00:43:08.880 --> 00:43:13.240]   And so, you know, we've got sort of a usefully defined
[00:43:13.240 --> 00:43:16.640]   grid of metrics, I'm not gonna go into them all here,
[00:43:16.640 --> 00:43:19.400]   that help us look at each of the different segments
[00:43:19.400 --> 00:43:22.280]   that we care a lot about and think need to be healthy.
[00:43:22.280 --> 00:43:24.080]   But really what we're looking for in the end
[00:43:24.080 --> 00:43:26.200]   is not being great in one area
[00:43:26.200 --> 00:43:28.680]   and then terrible in a bunch of other areas,
[00:43:28.680 --> 00:43:31.720]   but to have, you know, what we call sort of a green flush
[00:43:31.720 --> 00:43:34.040]   of being, you know, very good across
[00:43:34.040 --> 00:43:37.160]   all the different important areas for our ecosystem.
[00:43:37.160 --> 00:43:39.680]   - So these are like kind of watching people do behaviors
[00:43:39.680 --> 00:43:42.880]   that makes you think that they're happy and successful
[00:43:42.880 --> 00:43:44.400]   in what they're trying to do?
[00:43:44.400 --> 00:43:45.440]   - Yeah, I mean, you know,
[00:43:45.440 --> 00:43:46.960]   watching people's behavior sounds creepy
[00:43:46.960 --> 00:43:48.960]   and we don't do that.
[00:43:48.960 --> 00:43:52.320]   But, you know, things like, you know,
[00:43:52.320 --> 00:43:55.240]   everything from looking at how many notebooks
[00:43:55.240 --> 00:43:56.760]   are being created on a daily basis
[00:43:56.760 --> 00:44:01.720]   to our competition participation to, you know,
[00:44:01.720 --> 00:44:03.320]   survey responses and things like this
[00:44:03.320 --> 00:44:05.000]   just to make sure that our folks are happy
[00:44:05.000 --> 00:44:08.120]   to looking at, you know, the bug reports that are coming in.
[00:44:08.120 --> 00:44:10.120]   So looking at long-term metrics, like, you know,
[00:44:10.120 --> 00:44:13.520]   number of papers that are, you know,
[00:44:13.520 --> 00:44:15.880]   citing Kaggle in one form or another.
[00:44:15.880 --> 00:44:16.720]   - Cool.
[00:44:16.720 --> 00:44:18.480]   - Last I checked, there were almost 50,000 of them.
[00:44:18.480 --> 00:44:19.320]   - Wow.
[00:44:19.320 --> 00:44:22.320]   - I mean, you know, so there are a wide range of ways
[00:44:22.320 --> 00:44:25.480]   that we can assess whether we're doing a good job.
[00:44:25.480 --> 00:44:30.480]   - Do you have new things that you wanna try to do
[00:44:30.480 --> 00:44:32.160]   or things that you wanna change?
[00:44:32.160 --> 00:44:34.160]   Like, are there new people
[00:44:34.160 --> 00:44:36.360]   that you'd like to introduce Kaggle to
[00:44:36.360 --> 00:44:38.520]   or new ways that you'd like Kaggle
[00:44:38.520 --> 00:44:41.640]   to support existing people?
[00:44:41.640 --> 00:44:46.680]   - Yeah, so, you know, you asked about this
[00:44:46.680 --> 00:44:48.680]   a little bit tangentially earlier.
[00:44:48.680 --> 00:44:50.200]   You know, given my background,
[00:44:50.200 --> 00:44:52.000]   I think it would be pretty surprising
[00:44:52.000 --> 00:44:54.320]   if we didn't push towards some, you know,
[00:44:54.320 --> 00:44:55.680]   more sort of production grade,
[00:44:55.680 --> 00:44:59.960]   ML Ops-y style pieces in Kaggle over time.
[00:44:59.960 --> 00:45:02.440]   And some of those will certainly be competitions.
[00:45:02.440 --> 00:45:07.000]   You know, judging a model only on the basis
[00:45:07.000 --> 00:45:11.800]   of its accuracy by itself is probably not sufficient
[00:45:11.800 --> 00:45:14.400]   for everybody's needs in 2022.
[00:45:14.400 --> 00:45:16.200]   And so we need to be able to provide ways
[00:45:16.200 --> 00:45:20.840]   to help folks evaluate their models on other dimensions,
[00:45:20.840 --> 00:45:24.120]   including efficiency, and then to also create,
[00:45:24.120 --> 00:45:27.040]   you know, useful and compelling and interesting challenges.
[00:45:28.200 --> 00:45:31.320]   I think that there's a lot that we can do
[00:45:31.320 --> 00:45:33.600]   in the world of benchmarking.
[00:45:33.600 --> 00:45:36.760]   And, you know, right now our main benchmarks
[00:45:36.760 --> 00:45:39.000]   are really sort of competitions.
[00:45:39.000 --> 00:45:41.040]   But, you know, given that we have datasets
[00:45:41.040 --> 00:45:44.040]   and we have notebooks, you know,
[00:45:44.040 --> 00:45:47.840]   I think that we can move into becoming, you know,
[00:45:47.840 --> 00:45:49.320]   much more long running benchmarks
[00:45:49.320 --> 00:45:51.920]   and be a repository and service
[00:45:51.920 --> 00:45:53.600]   to the community in that way.
[00:45:53.600 --> 00:45:57.320]   So in terms of our, you know,
[00:45:57.320 --> 00:45:59.680]   our sort of user groups and populations,
[00:45:59.680 --> 00:46:05.120]   we have a really strong emphasis right now
[00:46:05.120 --> 00:46:08.640]   on outreach for underrepresented populations
[00:46:08.640 --> 00:46:09.680]   in machine learning.
[00:46:09.680 --> 00:46:12.280]   And that's gonna continue for sure.
[00:46:12.280 --> 00:46:18.560]   And when I look at sort of levels of expertise
[00:46:18.560 --> 00:46:19.720]   in our community,
[00:46:19.720 --> 00:46:22.560]   I think that we're doing a pretty good job right now
[00:46:22.560 --> 00:46:23.680]   serving novice learners.
[00:46:23.680 --> 00:46:24.520]   You know, as you say, you know,
[00:46:24.520 --> 00:46:26.000]   almost everybody who learns machine learning
[00:46:26.000 --> 00:46:28.480]   comes to Kaggle at some point in their journey.
[00:46:28.480 --> 00:46:29.800]   So we wanna make sure that we're continuing
[00:46:29.800 --> 00:46:31.120]   to serve those folks really well
[00:46:31.120 --> 00:46:33.800]   and providing as many on-ramps as we can
[00:46:33.800 --> 00:46:36.160]   and making that experience be a really good
[00:46:36.160 --> 00:46:37.440]   and really beneficial one.
[00:46:37.440 --> 00:46:40.880]   But I think that, you know, we're doing well there
[00:46:40.880 --> 00:46:42.480]   and we can really improve
[00:46:42.480 --> 00:46:44.920]   on how we're serving the practitioners
[00:46:44.920 --> 00:46:47.800]   and engaging the more sort of cutting edge research parts
[00:46:47.800 --> 00:46:48.880]   of the world as well.
[00:46:50.920 --> 00:46:54.680]   - Do you think that there's any downside
[00:46:54.680 --> 00:46:59.680]   to the framing, the competition framing of Kaggle
[00:46:59.680 --> 00:47:02.040]   for someone, you know, getting started?
[00:47:02.040 --> 00:47:05.480]   Like it's funny how friendly the community is
[00:47:05.480 --> 00:47:06.720]   for the idea that, you know,
[00:47:06.720 --> 00:47:07.880]   what people are supposedly doing
[00:47:07.880 --> 00:47:09.400]   is competing with each other.
[00:47:09.400 --> 00:47:11.000]   Like, do you ever think about that,
[00:47:11.000 --> 00:47:12.080]   that, you know, for some people,
[00:47:12.080 --> 00:47:14.520]   they might, you know, not wanna kind of compete
[00:47:14.520 --> 00:47:17.040]   with other people for the most accurate model
[00:47:17.040 --> 00:47:17.880]   or something like that?
[00:47:17.880 --> 00:47:18.720]   - Yeah, absolutely.
[00:47:18.720 --> 00:47:19.840]   So I've got two responses to that.
[00:47:19.840 --> 00:47:21.280]   One is that, you know,
[00:47:21.280 --> 00:47:22.720]   we've got our featured competitions
[00:47:22.720 --> 00:47:24.560]   where people, you know, might be winning, you know,
[00:47:24.560 --> 00:47:27.520]   aiming to win some, you know, prize of, you know,
[00:47:27.520 --> 00:47:28.800]   a lot of money or something like that.
[00:47:28.800 --> 00:47:30.480]   And there, you know, people,
[00:47:30.480 --> 00:47:33.360]   you know, many of the competitors are trying to win, right?
[00:47:33.360 --> 00:47:35.520]   Whether it's winning the prize or winning, you know,
[00:47:35.520 --> 00:47:37.240]   a gold medal in our progression system
[00:47:37.240 --> 00:47:39.200]   or, you know, become a Kaggle master or a Grandmaster.
[00:47:39.200 --> 00:47:40.960]   And those are really great and important things
[00:47:40.960 --> 00:47:42.080]   to be pushing forward.
[00:47:42.080 --> 00:47:46.920]   We have other competitions that are called
[00:47:46.920 --> 00:47:49.640]   playground competitions that are designed much more
[00:47:49.640 --> 00:47:52.760]   to be an on-ramp and less about, you know,
[00:47:52.760 --> 00:47:55.880]   winning a prize and more about testing your skills.
[00:47:55.880 --> 00:47:58.600]   But even for the featured competitions,
[00:47:58.600 --> 00:47:59.920]   one of my hobbies is, you know,
[00:47:59.920 --> 00:48:03.120]   I'm an amateur marathoner and I like to run marathons.
[00:48:03.120 --> 00:48:05.080]   It's a wonderful, fun thing to do.
[00:48:05.080 --> 00:48:08.600]   And you get out there, you know,
[00:48:08.600 --> 00:48:10.400]   like all the people are cheering and clapping,
[00:48:10.400 --> 00:48:11.240]   things like that.
[00:48:11.240 --> 00:48:14.840]   And that's true kind of no matter where you are in the race.
[00:48:14.840 --> 00:48:18.560]   And, you know, spoiler alert, I'm not at the front, right?
[00:48:18.560 --> 00:48:23.560]   So I think that there is something about having
[00:48:23.560 --> 00:48:26.600]   an environment that is framed around a competition
[00:48:26.600 --> 00:48:30.120]   that can still be about participation and self-growth
[00:48:30.120 --> 00:48:31.760]   that is really important,
[00:48:31.760 --> 00:48:33.640]   I think really inspiring to a lot of people
[00:48:33.640 --> 00:48:38.520]   and that we can, you know, make sure to be emphasizing
[00:48:38.520 --> 00:48:41.840]   and have be part of the Kaggle experience.
[00:48:41.840 --> 00:48:42.800]   That's really important.
[00:48:42.800 --> 00:48:45.040]   And we hear our users telling us this,
[00:48:45.040 --> 00:48:47.360]   that, you know, lots of people are coming
[00:48:47.360 --> 00:48:50.160]   to not necessarily see if they're gonna be first or second,
[00:48:50.160 --> 00:48:52.600]   but to improve their skills, to share knowledge,
[00:48:52.600 --> 00:48:54.600]   share ideas and to learn.
[00:48:54.600 --> 00:48:57.320]   - You were most recently a Google brand.
[00:48:57.320 --> 00:49:00.280]   And, you know, it's sort of, you know,
[00:49:00.280 --> 00:49:02.680]   I think about the work that's like coming out of,
[00:49:02.680 --> 00:49:08.920]   you know, open AI famously and other places where,
[00:49:08.920 --> 00:49:11.280]   you know, you get these huge models
[00:49:11.280 --> 00:49:14.200]   that in certain axes seem to really outperform
[00:49:14.200 --> 00:49:16.520]   other models work.
[00:49:16.520 --> 00:49:19.080]   And I wonder, like,
[00:49:19.080 --> 00:49:21.960]   do you think like, you know,
[00:49:21.960 --> 00:49:25.680]   if you roll that trend forward 10 years,
[00:49:25.680 --> 00:49:27.000]   does Kaggle stay relevant?
[00:49:27.000 --> 00:49:30.000]   Like, is there still a role to play for someone,
[00:49:30.000 --> 00:49:31.320]   you know, who doesn't have access
[00:49:31.320 --> 00:49:34.680]   to like a massive amount of compute resources
[00:49:34.680 --> 00:49:38.680]   to solve a problem in a useful way?
[00:49:38.680 --> 00:49:40.640]   - Yeah, so this is a great question.
[00:49:40.640 --> 00:49:41.960]   And obviously, you know,
[00:49:41.960 --> 00:49:44.640]   what's gone on in the last couple of years
[00:49:44.640 --> 00:49:49.000]   in terms of, you know, true leaf large scale language models
[00:49:49.000 --> 00:49:52.320]   or other multimodal models is,
[00:49:52.320 --> 00:49:55.120]   yeah, it's definitely changed the world
[00:49:55.120 --> 00:49:55.960]   in a couple of ways.
[00:49:55.960 --> 00:49:57.800]   One of which is it's changed the world
[00:49:57.800 --> 00:50:02.280]   of how some research is being conducted.
[00:50:02.280 --> 00:50:04.600]   And I think that the world of high energy physics
[00:50:04.600 --> 00:50:06.240]   is a useful parallel.
[00:50:06.240 --> 00:50:08.840]   Now, there are some kinds of,
[00:50:08.840 --> 00:50:12.440]   I'm not a physicist,
[00:50:12.440 --> 00:50:15.160]   I'm just gonna say some kinds of physics
[00:50:15.160 --> 00:50:17.760]   that can only be done with something
[00:50:17.760 --> 00:50:19.920]   that looks like a linear accelerator,
[00:50:19.920 --> 00:50:23.240]   where you need to get a couple billion dollars
[00:50:23.240 --> 00:50:24.760]   from a local government
[00:50:24.760 --> 00:50:26.800]   and build a, you know, several kilometer,
[00:50:26.800 --> 00:50:28.560]   mile long concrete tunnel
[00:50:28.560 --> 00:50:31.200]   under some hopefully stable part of the world
[00:50:31.200 --> 00:50:32.920]   so that you can run these, you know,
[00:50:32.920 --> 00:50:35.320]   incredibly expensive experiments
[00:50:35.320 --> 00:50:37.080]   to gain certain kinds of knowledge.
[00:50:37.080 --> 00:50:42.000]   And this has definitely changed the way
[00:50:42.000 --> 00:50:45.000]   that some parts of the field of physics works.
[00:50:45.000 --> 00:50:46.520]   There's no question about it.
[00:50:46.520 --> 00:50:48.520]   And among other things,
[00:50:48.520 --> 00:50:50.080]   the world of physics had to get good
[00:50:50.080 --> 00:50:51.760]   at doing this kind of research
[00:50:51.760 --> 00:50:53.800]   and to have, you know, in some places
[00:50:53.800 --> 00:50:54.920]   a little bit more of a hierarchy
[00:50:54.920 --> 00:50:57.360]   on how experiments get proposed,
[00:50:57.360 --> 00:50:59.440]   how they get evaluated,
[00:50:59.440 --> 00:51:00.600]   not on their results,
[00:51:00.600 --> 00:51:02.360]   but whether they should be run at all,
[00:51:02.360 --> 00:51:04.560]   you know, what gets into the pipeline,
[00:51:04.560 --> 00:51:06.440]   who makes those calls and things like that.
[00:51:06.440 --> 00:51:08.800]   And I think that we're seeing very similar developments
[00:51:08.800 --> 00:51:11.840]   for some kinds of machine learning research.
[00:51:11.840 --> 00:51:13.280]   But there's plenty of physics
[00:51:13.280 --> 00:51:14.560]   that you can do in the world,
[00:51:14.560 --> 00:51:15.480]   as far as I understand it,
[00:51:15.480 --> 00:51:17.920]   that doesn't involve, you know,
[00:51:17.920 --> 00:51:21.680]   having access to a super collider or things like that.
[00:51:21.680 --> 00:51:25.320]   And similarly, I believe that there are
[00:51:25.320 --> 00:51:27.920]   and will continue to be a lot of machine learning
[00:51:27.920 --> 00:51:30.480]   that doesn't rely on having access
[00:51:30.480 --> 00:51:35.080]   to sort of, you know, colliders scale resources
[00:51:35.080 --> 00:51:36.120]   for machine learning.
[00:51:36.120 --> 00:51:38.960]   And that can look everything,
[00:51:38.960 --> 00:51:41.120]   you know, it can look things like, you know,
[00:51:41.120 --> 00:51:43.600]   what do we do for resource constrained environments?
[00:51:43.600 --> 00:51:45.520]   So models that need to run, you know,
[00:51:45.520 --> 00:51:47.600]   in the browser, need to run on web devices,
[00:51:47.600 --> 00:51:50.400]   need to run on distributed,
[00:51:50.400 --> 00:51:52.880]   have edge-based things, you know.
[00:51:52.880 --> 00:51:54.640]   My guess is that we probably don't need
[00:51:54.640 --> 00:51:57.720]   collider scale resources to train tiny, tiny models.
[00:51:57.720 --> 00:52:02.880]   What do we do for models that need to be fine-tuned
[00:52:02.880 --> 00:52:04.200]   in one form or another,
[00:52:04.200 --> 00:52:07.760]   or even, you know, things like prompt tuning,
[00:52:07.760 --> 00:52:12.120]   you know, where we might have a very large scale model
[00:52:12.120 --> 00:52:13.320]   at our disposal,
[00:52:13.320 --> 00:52:16.560]   but then we need to figure out how to use that model
[00:52:16.560 --> 00:52:19.600]   as effectively as possible for a given use case.
[00:52:19.600 --> 00:52:23.760]   Something that I think will be reasonable to attempt
[00:52:23.760 --> 00:52:26.440]   for lots of people in specialized domains
[00:52:26.440 --> 00:52:28.520]   for a very long period of time,
[00:52:28.520 --> 00:52:30.640]   you know, at least as far as I can see forward.
[00:52:30.640 --> 00:52:33.640]   The last thing that I'll say here is that
[00:52:33.640 --> 00:52:35.720]   it's also useful to think about
[00:52:36.720 --> 00:52:39.200]   standards of evidence and verification
[00:52:39.200 --> 00:52:41.480]   for these very large scale models.
[00:52:41.480 --> 00:52:46.000]   And that if, you know, I'm trying to think of
[00:52:46.000 --> 00:52:48.840]   how we would go about verifying that a given model,
[00:52:48.840 --> 00:52:51.680]   you know, we talked earlier about
[00:52:51.680 --> 00:52:54.880]   the kinds of verification and, you know,
[00:52:54.880 --> 00:52:56.880]   moral equivalent of unit tests and things like this
[00:52:56.880 --> 00:52:59.160]   that might need to be put into place.
[00:52:59.160 --> 00:53:02.640]   I can't think of too many better resources
[00:53:02.640 --> 00:53:04.400]   than a community like Kaggle's
[00:53:04.400 --> 00:53:08.240]   to attack the problem of how do we verify a model
[00:53:08.240 --> 00:53:09.920]   that is very, very large scale,
[00:53:09.920 --> 00:53:11.880]   that might have many millions of behaviors
[00:53:11.880 --> 00:53:13.240]   or more than millions of behaviors
[00:53:13.240 --> 00:53:14.560]   that need to be exhibited
[00:53:14.560 --> 00:53:17.160]   in different kinds of circumstances
[00:53:17.160 --> 00:53:20.880]   to stress test, to validate models.
[00:53:20.880 --> 00:53:22.120]   And, you know, can those be framed
[00:53:22.120 --> 00:53:24.520]   in terms of competitions and resources,
[00:53:24.520 --> 00:53:25.360]   other things like that?
[00:53:25.360 --> 00:53:26.200]   Absolutely, right?
[00:53:26.200 --> 00:53:28.560]   So I think that the Kaggle community
[00:53:28.560 --> 00:53:32.680]   will be increasingly relevant over time for these reasons.
[00:53:32.680 --> 00:53:34.040]   Now, that doesn't mean that every Kaggler
[00:53:34.040 --> 00:53:36.680]   is gonna train a model, you know,
[00:53:36.680 --> 00:53:39.280]   with, you know, X million compute hours
[00:53:39.280 --> 00:53:40.120]   or things like that.
[00:53:40.120 --> 00:53:41.520]   That's probably not realistic
[00:53:41.520 --> 00:53:44.800]   and probably wouldn't be good for the world if it was.
[00:53:44.800 --> 00:53:46.720]   But I think there's a lot that we can do
[00:53:46.720 --> 00:53:48.000]   that will still add value.
[00:53:48.000 --> 00:53:50.840]   - I guess a lot of those lines,
[00:53:50.840 --> 00:53:55.040]   do you feel like AutoML techniques,
[00:53:55.040 --> 00:54:01.640]   you know, could displace the value of actual competitions?
[00:54:02.240 --> 00:54:05.000]   Like, I feel like in the past,
[00:54:05.000 --> 00:54:07.200]   the winning Kaggle strategy was typically
[00:54:07.200 --> 00:54:09.520]   to do the best feature engineering.
[00:54:09.520 --> 00:54:15.560]   But I wonder, actually, I wonder if that's still the case.
[00:54:15.560 --> 00:54:17.760]   And then, you know, in these worlds where, you know,
[00:54:17.760 --> 00:54:19.000]   you have these gigantic models
[00:54:19.000 --> 00:54:21.440]   that are sort of doing their own feature engineering,
[00:54:21.440 --> 00:54:22.880]   it's one way to look at it.
[00:54:22.880 --> 00:54:24.520]   And then AutoML on top of that,
[00:54:24.520 --> 00:54:28.880]   what is a Kaggler to do 10 years from now
[00:54:28.880 --> 00:54:29.720]   to beat that strategy? - Yeah, no, I mean,
[00:54:29.720 --> 00:54:31.440]   we just give up. - Yeah, yeah, exactly.
[00:54:31.440 --> 00:54:34.800]   So, look, AutoML is a really important tool
[00:54:34.800 --> 00:54:37.400]   in the same way that hyperparameter sweeps,
[00:54:37.400 --> 00:54:39.560]   you know, just to take an example at random,
[00:54:39.560 --> 00:54:41.800]   is a really important tool, right?
[00:54:41.800 --> 00:54:44.480]   I believe that AutoML and, you know,
[00:54:44.480 --> 00:54:47.520]   useful hyperparameter tuning engines and things like this
[00:54:47.520 --> 00:54:50.520]   do a great job of automating the kinds of work
[00:54:50.520 --> 00:54:53.840]   that isn't particularly interesting in machine learning.
[00:54:53.840 --> 00:54:55.720]   You know, in the early days,
[00:54:55.720 --> 00:54:58.480]   I spent a lot of time being a manual hyperparameter tuner,
[00:54:58.480 --> 00:55:00.240]   and it wasn't that rewarding.
[00:55:00.840 --> 00:55:04.400]   But the more fundamental questions
[00:55:04.400 --> 00:55:06.840]   of what data should be going into a model
[00:55:06.840 --> 00:55:09.400]   to train it for a given task?
[00:55:09.400 --> 00:55:11.360]   How should we be thinking about
[00:55:11.360 --> 00:55:13.640]   data distributions and structures?
[00:55:13.640 --> 00:55:16.480]   What are the right structures for a model
[00:55:16.480 --> 00:55:20.800]   to capture, you know, useful, you know,
[00:55:20.800 --> 00:55:22.640]   causal concepts in addition to just learning
[00:55:22.640 --> 00:55:24.960]   from the correlations as possible?
[00:55:24.960 --> 00:55:26.800]   Even, you know, deeper questions of like,
[00:55:26.800 --> 00:55:28.720]   what is, if we're doing, say, you know,
[00:55:28.720 --> 00:55:30.440]   fine tuning of a large pre-trained model,
[00:55:30.440 --> 00:55:33.320]   like, what is the right way to set that up?
[00:55:33.320 --> 00:55:36.680]   How do we create the right sets of targets?
[00:55:36.680 --> 00:55:39.880]   How do we choose the right pre-training base to begin with?
[00:55:39.880 --> 00:55:41.280]   All of those are interesting questions
[00:55:41.280 --> 00:55:43.960]   that I don't think that an AutoML pipeline
[00:55:43.960 --> 00:55:48.960]   is likely to solve, you know, exhaustively
[00:55:48.960 --> 00:55:52.520]   in the place of human judgment in the foreseeable future.
[00:55:52.520 --> 00:55:57.240]   So I'm very happy for humans to focus on human problems,
[00:55:57.240 --> 00:55:59.800]   and, you know, places where human judgment and insight
[00:55:59.800 --> 00:56:01.600]   is gonna be most valuable.
[00:56:01.600 --> 00:56:03.720]   Where there's drudgery, let's automate it.
[00:56:03.720 --> 00:56:05.280]   - Yeah, no problem with that.
[00:56:05.280 --> 00:56:06.120]   - Well, thank you so much.
[00:56:06.120 --> 00:56:07.440]   We always end with two questions,
[00:56:07.440 --> 00:56:09.680]   and I wanna make sure that I get them in.
[00:56:09.680 --> 00:56:12.480]   And the second to last question is pretty open-ended,
[00:56:12.480 --> 00:56:16.400]   but I'm curious if you think,
[00:56:16.400 --> 00:56:20.640]   or I'm curious what you think is an underrated aspect
[00:56:20.640 --> 00:56:22.560]   of machine learning, or something that,
[00:56:22.560 --> 00:56:25.680]   if you had more time, you'd like to spend some time
[00:56:25.680 --> 00:56:27.000]   looking into.
[00:56:27.000 --> 00:56:31.640]   - Yeah, so I think the thing that is most interesting
[00:56:31.640 --> 00:56:34.680]   in machine learning right now is making machine learning
[00:56:34.680 --> 00:56:37.600]   be robust to shifting to data distributions.
[00:56:37.600 --> 00:56:40.680]   And so this is where a lot of my work was
[00:56:40.680 --> 00:56:43.120]   in my last couple of years in Google Brain.
[00:56:43.120 --> 00:56:46.000]   Yeah, as we talked about at the beginning,
[00:56:46.000 --> 00:56:48.440]   when you break that IID assumption
[00:56:48.440 --> 00:56:50.000]   between test and train data,
[00:56:50.000 --> 00:56:51.560]   many of the theoretical guarantees
[00:56:51.560 --> 00:56:54.560]   that underpin supervised machine learning go away.
[00:56:54.560 --> 00:56:57.560]   But we still need things to work.
[00:56:57.560 --> 00:57:01.040]   And so, yeah, I think that this is absolutely
[00:57:01.040 --> 00:57:05.400]   the most interesting area right now for current work
[00:57:05.400 --> 00:57:08.000]   is figuring out ways to be robust
[00:57:08.000 --> 00:57:09.680]   to shifting data distributions.
[00:57:09.680 --> 00:57:11.760]   And this isn't some sort of weird abstract problem, right?
[00:57:11.760 --> 00:57:14.560]   It's something that happens for every deployed system
[00:57:14.560 --> 00:57:15.640]   I've ever seen.
[00:57:15.640 --> 00:57:18.640]   It also happens for things like machine learning
[00:57:18.640 --> 00:57:20.160]   for scientific discovery.
[00:57:20.160 --> 00:57:22.200]   So if you're gonna do machine learning to guide,
[00:57:22.200 --> 00:57:24.880]   say, protein design or drug discovery
[00:57:24.880 --> 00:57:28.520]   or any other sort of generative process,
[00:57:28.520 --> 00:57:31.960]   by definition, you're gonna be moving out
[00:57:31.960 --> 00:57:35.480]   from your world of known things because that's the point.
[00:57:35.480 --> 00:57:37.480]   And so how do we make sure that our models
[00:57:37.480 --> 00:57:41.720]   are gonna be holding up well to those unknown areas
[00:57:41.720 --> 00:57:45.000]   that are super important for advancing key problem areas
[00:57:45.000 --> 00:57:46.880]   like drug discovery?
[00:57:46.880 --> 00:57:50.120]   I think that's really one of the most important areas
[00:57:50.120 --> 00:57:51.160]   as far as I can tell.
[00:57:52.120 --> 00:57:53.920]   - Do you have a favorite paper on that topic
[00:57:53.920 --> 00:57:56.040]   that we could point folks to
[00:57:56.040 --> 00:57:58.920]   or resources to learn more about that?
[00:57:58.920 --> 00:58:01.320]   - Yeah, so we just put a paper out.
[00:58:01.320 --> 00:58:02.600]   It's the last paper I was involved in
[00:58:02.600 --> 00:58:05.960]   in Brain called Plex.
[00:58:05.960 --> 00:58:10.520]   That's looking at sort of a unified view of robustness
[00:58:10.520 --> 00:58:12.880]   to data set shift, starting with pre-training
[00:58:12.880 --> 00:58:15.760]   and then augmenting with a bunch of other Bayesian methods
[00:58:15.760 --> 00:58:18.080]   with many, many excellent co-authors,
[00:58:18.080 --> 00:58:22.520]   including Jasper Snoke and Justin Tran
[00:58:22.520 --> 00:58:24.200]   and Balaji Lakshman.
[00:58:24.200 --> 00:58:25.040]   - Awesome.
[00:58:25.040 --> 00:58:29.840]   I guess final question is when you think about
[00:58:29.840 --> 00:58:33.920]   actually making machine learning models really work
[00:58:33.920 --> 00:58:37.600]   in the real world, today in 2022,
[00:58:37.600 --> 00:58:39.960]   where do you see the biggest gap
[00:58:39.960 --> 00:58:41.880]   or the hardest part of that?
[00:58:41.880 --> 00:58:45.320]   From going to like, you know, kind of Kaggle winning model
[00:58:45.320 --> 00:58:49.520]   to deployed and useful for someone in the world?
[00:58:49.520 --> 00:58:53.080]   - Yeah, so I think what's interesting is that, you know,
[00:58:53.080 --> 00:58:57.320]   people like you have put a lot of infrastructure in place
[00:58:57.320 --> 00:58:59.800]   that make things that used to be quite difficult,
[00:58:59.800 --> 00:59:02.200]   you know, pretty straightforward now.
[00:59:02.200 --> 00:59:03.920]   And so, you know, the challenges of like,
[00:59:03.920 --> 00:59:06.200]   how do I get a model into production?
[00:59:06.200 --> 00:59:11.560]   Yeah, there are plenty of packages, systems, platforms,
[00:59:11.560 --> 00:59:13.560]   cloud-based solutions, you know, you name it,
[00:59:13.560 --> 00:59:15.000]   that can help people do that.
[00:59:15.760 --> 00:59:19.480]   I think that the pieces that are more difficult to solve
[00:59:19.480 --> 00:59:21.520]   are really about how do you make sure that that model
[00:59:21.520 --> 00:59:24.560]   is going to be a model that you're proud of
[00:59:24.560 --> 00:59:25.960]   over a period of time.
[00:59:25.960 --> 00:59:30.520]   And, you know, where that's most obviously, you know,
[00:59:30.520 --> 00:59:33.200]   comes to head in terms of robustness,
[00:59:33.200 --> 00:59:35.360]   which, you know, might be in terms of data set shifts,
[00:59:35.360 --> 00:59:37.160]   might be in terms of fairness,
[00:59:37.160 --> 00:59:40.840]   might be in terms of inclusivity or things of these forms,
[00:59:40.840 --> 00:59:45.360]   but making sure that our models are acting the way
[00:59:45.360 --> 00:59:47.680]   that we want them to in a wide variety
[00:59:47.680 --> 00:59:51.080]   of deployment situations is currently, I think,
[00:59:51.080 --> 00:59:53.640]   much more difficult than just sort of the mechanics
[00:59:53.640 --> 00:59:55.280]   of how do you get a model into production
[00:59:55.280 --> 00:59:58.080]   because of the work that's been done on infrastructure
[00:59:58.080 --> 01:00:01.000]   in so many different areas.
[01:00:01.000 --> 01:00:03.040]   - Cool, well said.
[01:00:03.040 --> 01:00:05.640]   Thank you so much, this is a really fun interview.
[01:00:05.640 --> 01:00:06.920]   - Awesome, great, I really enjoyed it.
[01:00:06.920 --> 01:00:08.360]   Thanks so much.
[01:00:08.360 --> 01:00:09.800]   If you're enjoying these interviews
[01:00:09.800 --> 01:00:11.480]   and you wanna learn more,
[01:00:11.480 --> 01:00:13.600]   please click on the link to the show notes
[01:00:13.600 --> 01:00:16.760]   in the description where you can find links
[01:00:16.760 --> 01:00:18.280]   to all the papers that are mentioned,
[01:00:18.280 --> 01:00:20.760]   supplemental material, and a transcription
[01:00:20.760 --> 01:00:22.280]   that we work really hard to produce.
[01:00:22.280 --> 01:00:23.280]   So check it out.
[01:00:23.280 --> 01:00:25.860]   (upbeat music)

