
[00:00:00.000 --> 00:00:03.520]   Hi, and welcome to the video on training parameters.
[00:00:03.520 --> 00:00:06.520]   So these are essentially settings
[00:00:06.520 --> 00:00:10.440]   that we can use to fine-tune the learning process.
[00:00:10.440 --> 00:00:14.120]   And there are essentially three parts to remember.
[00:00:14.120 --> 00:00:16.840]   We have the optimizer, the loss function,
[00:00:16.840 --> 00:00:19.480]   and our performance metrics.
[00:00:19.480 --> 00:00:23.840]   Now, the optimizer is used to minimize the model loss.
[00:00:23.840 --> 00:00:27.000]   And popular optimizers that you may have heard about
[00:00:27.000 --> 00:00:32.760]   are Adam or Stochastic Gradient Descent, or SGD for short.
[00:00:32.760 --> 00:00:35.000]   And then we have the loss function.
[00:00:35.000 --> 00:00:37.600]   This is essentially saying, how do we
[00:00:37.600 --> 00:00:39.880]   calculate the difference between our predicted values
[00:00:39.880 --> 00:00:43.720]   and the true or real values?
[00:00:43.720 --> 00:00:46.160]   And for this, we use a loss function.
[00:00:46.160 --> 00:00:49.600]   And the most popular of those that you will see
[00:00:49.600 --> 00:00:52.680]   is categorical cross-entropy.
[00:00:52.680 --> 00:00:54.220]   But there are a lot of different ones
[00:00:54.220 --> 00:00:55.880]   that you see used in different places.
[00:00:55.880 --> 00:01:00.500]   And it would not be weird to use another one of those functions.
[00:01:00.500 --> 00:01:04.340]   And then, finally, we have the performance metrics.
[00:01:04.340 --> 00:01:07.960]   And this is what we use to measure our model's
[00:01:07.960 --> 00:01:09.980]   actual performance.
[00:01:09.980 --> 00:01:12.420]   There are a lot of ways to do this.
[00:01:12.420 --> 00:01:14.700]   But the simplest and often most useful metric
[00:01:14.700 --> 00:01:17.460]   is to just use accuracy.
[00:01:17.460 --> 00:01:20.060]   So let's start with the optimizer.
[00:01:20.060 --> 00:01:24.340]   So we'll use Adam, as it is the most popular.
[00:01:24.340 --> 00:01:30.100]   And all we want to do is create our optimizer variable.
[00:01:30.100 --> 00:01:35.120]   And then into that, we pass our optimizer, which is Adam.
[00:01:35.120 --> 00:01:37.820]   And then we also need to pass a learning rate.
[00:01:37.820 --> 00:01:42.140]   So this is essentially a number which tells TensorFlow
[00:01:42.140 --> 00:01:45.940]   or tells Adam how quickly or how slowly
[00:01:45.940 --> 00:01:48.540]   to update model parameters.
[00:01:48.540 --> 00:01:51.640]   So a larger number means larger updates,
[00:01:51.640 --> 00:01:54.740]   whereas a smaller number means smaller updates.
[00:01:54.740 --> 00:01:56.220]   This learning rate is actually one
[00:01:56.220 --> 00:01:59.860]   of the most important high parameters in our model
[00:01:59.860 --> 00:02:02.620]   because we need to find a balance between not too high
[00:02:02.620 --> 00:02:04.260]   and not too low.
[00:02:04.260 --> 00:02:09.100]   And I have an image over here to try and explain why.
[00:02:09.100 --> 00:02:14.140]   So you can see here, we have three different plots.
[00:02:14.140 --> 00:02:19.380]   First of those is where we have a very low learning rate.
[00:02:19.380 --> 00:02:22.640]   And what we have is a step, which
[00:02:22.640 --> 00:02:26.320]   is quite small, which goes in the direction downwards
[00:02:26.320 --> 00:02:28.360]   on our slope.
[00:02:28.360 --> 00:02:30.660]   So further down, down here is where
[00:02:30.660 --> 00:02:34.080]   we want to be because this is a lower level of loss
[00:02:34.080 --> 00:02:37.640]   and therefore a higher accuracy.
[00:02:37.640 --> 00:02:41.240]   So we want to get down to here, which is the global minima.
[00:02:41.240 --> 00:02:45.300]   But because our steps are too low, we get to here.
[00:02:45.300 --> 00:02:47.400]   And our model thinks, OK, we need
[00:02:47.400 --> 00:02:50.720]   to go back because the gradient is negative in this direction.
[00:02:50.720 --> 00:02:51.420]   So it goes back.
[00:02:51.420 --> 00:02:55.120]   And then because the steps are too small,
[00:02:55.120 --> 00:02:59.200]   it just gets stuck in this small, little local minima.
[00:02:59.200 --> 00:03:02.680]   So where there's a dip, but it's not the lowest dip,
[00:03:02.680 --> 00:03:04.400]   it's called local minima.
[00:03:04.400 --> 00:03:06.760]   And even if the learning rate is perfect,
[00:03:06.760 --> 00:03:08.680]   you can still get stuck in the local minima.
[00:03:08.680 --> 00:03:10.560]   It's completely normal.
[00:03:10.560 --> 00:03:13.120]   But obviously, the lower the learning rate,
[00:03:13.120 --> 00:03:16.200]   the more likely that is to happen.
[00:03:16.200 --> 00:03:20.000]   So obviously, you don't want to be too low.
[00:03:20.000 --> 00:03:21.840]   Alternatively, we also don't want
[00:03:21.840 --> 00:03:24.520]   to be too high because if our learning rate is too high,
[00:03:24.520 --> 00:03:27.600]   then yes, we will miss more of the local minima.
[00:03:27.600 --> 00:03:31.040]   But once we get to the point where we are at a global minima
[00:03:31.040 --> 00:03:35.480]   and we want to go down into this sort of descent,
[00:03:35.480 --> 00:03:39.000]   our steps are just simply too large to actually do that.
[00:03:39.000 --> 00:03:43.200]   They can't go down because we need a smaller learning rate
[00:03:43.200 --> 00:03:45.280]   in order to actually get down there.
[00:03:45.280 --> 00:03:47.960]   Otherwise, what you will see during training
[00:03:47.960 --> 00:03:50.640]   is that your loss function or loss value
[00:03:50.640 --> 00:03:51.800]   is just going up and down.
[00:03:51.800 --> 00:03:54.560]   It's all over the place, but it's not really going anywhere.
[00:03:54.560 --> 00:03:56.720]   It's just going up and down.
[00:03:56.720 --> 00:04:02.320]   And that is generally because your learning rate is too high.
[00:04:02.320 --> 00:04:04.360]   However, if we do get a learning rate that
[00:04:04.360 --> 00:04:07.880]   is in the middle of these two and it's just right,
[00:04:07.880 --> 00:04:09.840]   then we will hopefully get a graph
[00:04:09.840 --> 00:04:11.400]   that looks something like this.
[00:04:11.400 --> 00:04:13.440]   So we don't get stuck in the local minima
[00:04:13.440 --> 00:04:17.480]   because the step is large enough to miss it.
[00:04:17.480 --> 00:04:21.200]   But it's not too large to not be able to converge
[00:04:21.200 --> 00:04:25.120]   into the global minima, which is what you can see here.
[00:04:25.120 --> 00:04:28.840]   And that is the ideal learning rate
[00:04:28.840 --> 00:04:31.360]   that we want to try and get.
[00:04:31.360 --> 00:04:34.120]   A lot of the time, it's just a case of trying things out
[00:04:34.120 --> 00:04:35.800]   and trying a few different learning rates
[00:04:35.800 --> 00:04:37.440]   and see what works.
[00:04:37.440 --> 00:04:40.120]   But this is the logic between not going too high and not
[00:04:40.120 --> 00:04:43.040]   going too low.
[00:04:43.040 --> 00:04:54.080]   So that is our optimizer, which is in the optimizers attribute.
[00:04:54.080 --> 00:04:56.720]   Now we need to go to our loss function.
[00:04:56.720 --> 00:05:01.680]   So the loss is the y-axes in the plot here.
[00:05:01.680 --> 00:05:04.400]   So the lower the loss, the higher the model performance
[00:05:04.400 --> 00:05:07.400]   or the higher the accuracy.
[00:05:07.400 --> 00:05:10.040]   But obviously, we need to actually calculate that.
[00:05:10.040 --> 00:05:12.920]   So we will create a loss variable.
[00:05:12.920 --> 00:05:19.320]   And we do tf.keras.losses.
[00:05:19.320 --> 00:05:23.640]   And then we want to calculate the categorical cross-entropy
[00:05:23.640 --> 00:05:24.160]   loss.
[00:05:24.160 --> 00:05:31.880]   There's no other hyperparameters or anything
[00:05:31.880 --> 00:05:35.080]   that we need to add into this.
[00:05:35.080 --> 00:05:37.480]   So that's all we need for that.
[00:05:37.480 --> 00:05:43.280]   And then we also want to initialize a accuracy metric.
[00:05:43.280 --> 00:05:46.080]   So we'll put that into a variable called accuracy,
[00:05:46.080 --> 00:05:46.920]   or ACC.
[00:05:46.920 --> 00:05:59.240]   Now we do tf.keras.metrics, then categorical accuracy.
[00:05:59.240 --> 00:06:02.080]   So the reason we are using categorical accuracy here
[00:06:02.080 --> 00:06:03.840]   is because in most cases, you probably
[00:06:03.840 --> 00:06:07.800]   have a set number of outputs rather than just
[00:06:07.800 --> 00:06:09.640]   a single output.
[00:06:09.640 --> 00:06:14.760]   So it will most likely look something like this,
[00:06:14.760 --> 00:06:19.080]   rather than like this.
[00:06:19.080 --> 00:06:22.040]   So when we have different categories,
[00:06:22.040 --> 00:06:26.320]   we need to use categorical accuracy.
[00:06:26.320 --> 00:06:30.560]   Then we just pass in the string accuracy there as well.
[00:06:30.560 --> 00:06:34.560]   So we've now set up all of our training parameters.
[00:06:34.560 --> 00:06:38.600]   And we just need to compile our model now.
[00:06:38.600 --> 00:06:43.280]   So we just call model, compile.
[00:06:43.280 --> 00:06:50.080]   And then we pass the optimizer, loss, and accuracy metric.
[00:06:50.080 --> 00:06:53.120]   So for metrics, we also always put these within the list.
[00:06:53.120 --> 00:06:54.560]   Because typically, you're probably
[00:06:54.560 --> 00:06:56.440]   going to want more than one metric.
[00:06:56.440 --> 00:06:58.800]   But for now, we're just sticking with accuracy.
[00:06:58.800 --> 00:07:00.260]   We don't need to add anything else.
[00:07:00.260 --> 00:07:02.960]   But sometimes, you may have other things in here,
[00:07:02.960 --> 00:07:06.720]   like an F1 score or some other metrics as well.
[00:07:06.720 --> 00:07:09.840]   And you'd want to pass all of these in as a list.
[00:07:09.840 --> 00:07:13.720]   But we're just going to go with accuracy.
[00:07:13.720 --> 00:07:15.520]   And now that is our model.
[00:07:15.520 --> 00:07:19.040]   It is completely ready for us to start training with it.
[00:07:19.040 --> 00:07:23.200]   But for now, that's everything on the training parameters.
[00:07:23.200 --> 00:07:24.920]   So I hope you've enjoyed.
[00:07:24.920 --> 00:07:26.680]   And thank you for watching.
[00:07:26.680 --> 00:07:28.880]   I will see you again in the next one.

