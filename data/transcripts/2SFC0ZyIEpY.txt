
[00:00:00.000 --> 00:00:04.320]   Hello and welcome everybody to week 10 of Fastbook Reading Group.
[00:00:04.320 --> 00:00:09.280]   And this week we're taking a break from computer vision.
[00:00:09.280 --> 00:00:14.240]   Although everything's going to look very similar, but by the time we finish today,
[00:00:14.240 --> 00:00:16.880]   we would have built a movie recommendation system.
[00:00:16.880 --> 00:00:22.720]   And you will see how Fast.ai makes things really easy for non-computer vision stuff.
[00:00:22.720 --> 00:00:27.040]   Like we saw it in chapter one where we were looking at different examples of NLP.
[00:00:27.040 --> 00:00:33.520]   We were looking at examples of various different things that's non-computer vision, like tabular.
[00:00:33.520 --> 00:00:38.080]   But then today we're going to actually spend some time looking at movie recommendation
[00:00:38.080 --> 00:00:39.040]   using Fast.ai.
[00:00:39.040 --> 00:00:43.680]   Particularly this type of a problem is called collaborative filtering.
[00:00:43.680 --> 00:00:48.640]   And as part of solving this problem, we're going to look at things like what's an embedding.
[00:00:48.640 --> 00:00:51.280]   So that's the main idea.
[00:00:51.280 --> 00:00:55.520]   And one second, sorry, it looks like this.
[00:00:55.520 --> 00:00:56.160]   Okay, there we go.
[00:00:56.880 --> 00:01:03.120]   And so just like every week, if you go to that link, 1db.me/fastbook10
[00:01:03.120 --> 00:01:08.320]   So let me go there. 1db.me/fastbook10
[00:01:08.320 --> 00:01:15.120]   That should take me to this Fastbook Reading Group week 10.
[00:01:15.120 --> 00:01:18.720]   And this is where we'll post all our comments just like every week.
[00:01:18.720 --> 00:01:20.240]   So just post a comment here.
[00:01:20.240 --> 00:01:24.320]   And there you go.
[00:01:24.320 --> 00:01:29.440]   And so as you go through the session, if you have any questions about what we're doing
[00:01:29.440 --> 00:01:34.000]   or you know if there's anything you want to say to me, just put your comment here.
[00:01:34.000 --> 00:01:36.880]   And same as usual.
[00:01:36.880 --> 00:01:39.200]   So that's Fastbook Week 10.
[00:01:39.200 --> 00:01:43.520]   This week has been really good in terms of blog posts as well.
[00:01:43.520 --> 00:01:46.880]   I saw some really good blog posts and Akash has come back this week
[00:01:46.880 --> 00:01:50.160]   and he's finished his blog post on Lost Functions Part 2.
[00:01:50.160 --> 00:01:51.040]   So I was getting a read.
[00:01:51.040 --> 00:01:53.840]   Actually, this is a nice blog post.
[00:01:53.840 --> 00:01:55.680]   If I can find it, let's see.
[00:01:55.680 --> 00:01:56.960]   Lost Functions Part 2.
[00:01:56.960 --> 00:02:06.080]   I hope this shows up because I'll have to otherwise go into Twitter and find.
[00:02:06.080 --> 00:02:06.720]   Oh, there we go.
[00:02:06.720 --> 00:02:09.920]   Let's hope it's this one.
[00:02:09.920 --> 00:02:10.960]   See it?
[00:02:10.960 --> 00:02:13.840]   Is it that?
[00:02:13.840 --> 00:02:15.760]   AkashMeyer.github.io
[00:02:15.760 --> 00:02:16.400]   Yeah, there we go.
[00:02:16.400 --> 00:02:16.960]   We found it.
[00:02:16.960 --> 00:02:18.560]   Lost Functions Part 2.
[00:02:18.560 --> 00:02:21.280]   So the thing that I really liked about this blog post
[00:02:21.280 --> 00:02:26.560]   is a lot of the different losses that we generally use like mean square error,
[00:02:26.560 --> 00:02:28.640]   mean absolute error loss,
[00:02:28.640 --> 00:02:33.280]   and you know pretty much all the different losses that you will see have been plotted.
[00:02:33.280 --> 00:02:37.520]   But also the code has been shown just so it's like a really nice collation
[00:02:37.520 --> 00:02:39.600]   of bringing lots of different things together.
[00:02:39.600 --> 00:02:43.520]   So I think I would do recommend, I do recommend everybody
[00:02:43.520 --> 00:02:45.200]   to give this blog post a read.
[00:02:45.200 --> 00:02:47.360]   So thanks, Akash, for sharing that.
[00:02:47.360 --> 00:02:50.880]   First, thanks for coming again and writing about Mixup.
[00:02:50.880 --> 00:02:54.160]   Actually, this was again another wonderful blog post.
[00:02:54.160 --> 00:02:55.360]   So let me bring that up.
[00:02:55.360 --> 00:02:58.000]   Ferris, that's about Mixup.
[00:02:58.000 --> 00:03:02.880]   So I really like the, I think Ferris has gone in and he's explained the Mixup paper.
[00:03:02.880 --> 00:03:09.200]   I do want to show you, but I've made the mistake of not keeping the URLs handy.
[00:03:09.200 --> 00:03:11.520]   So I'll have to go in like Ferris.
[00:03:11.520 --> 00:03:17.840]   I have to go in like this, which is very manual.
[00:03:17.840 --> 00:03:21.920]   I should have kept the URLs handy.
[00:03:21.920 --> 00:03:23.760]   Let's see if somebody's posted that in the chat.
[00:03:23.760 --> 00:03:26.400]   Oh, there we go.
[00:03:26.400 --> 00:03:27.600]   Thanks, Angelica.
[00:03:27.600 --> 00:03:30.180]   Okay.
[00:03:30.180 --> 00:03:34.880]   So where's the Mixup?
[00:03:34.880 --> 00:03:37.440]   Here it is.
[00:03:37.440 --> 00:03:38.240]   That's Mixup.
[00:03:38.240 --> 00:03:40.880]   So I just want to bring up Mixup.
[00:03:40.880 --> 00:03:44.720]   And again, this is really a well-written blog post.
[00:03:44.720 --> 00:03:47.040]   I think because when we were discussing Mixup
[00:03:47.040 --> 00:03:48.960]   and we were discussing these ideas last week,
[00:03:48.960 --> 00:03:52.960]   I did sort of mention that it would make a good blog post to write about Mixup.
[00:03:52.960 --> 00:03:54.720]   And you can see how there's this example.
[00:03:54.720 --> 00:03:56.640]   Then you can see the official implementation.
[00:03:56.640 --> 00:04:00.240]   And you can see how these different tenses look like.
[00:04:00.240 --> 00:04:01.920]   You can see what the Mixup loss looks like.
[00:04:01.920 --> 00:04:04.880]   And finally, you can see the Fast.ai implementation.
[00:04:04.880 --> 00:04:09.920]   So I really like this kind of blog posts that bring code and explanation together.
[00:04:10.720 --> 00:04:16.560]   And I feel like then it's really this one place that puts everything at a single place.
[00:04:16.560 --> 00:04:19.120]   So the reader doesn't really have to look at multiple places
[00:04:19.120 --> 00:04:20.640]   when they're trying to understand Mixup.
[00:04:20.640 --> 00:04:22.400]   Ravi's come back.
[00:04:22.400 --> 00:04:24.080]   He's written about chapter 7.
[00:04:24.080 --> 00:04:27.680]   So there's normalization, progressive resizing, pretty much all the tricks.
[00:04:27.680 --> 00:04:31.360]   And he's also been able to get 88.5% accuracy.
[00:04:31.360 --> 00:04:35.840]   So I believe that's a 0.7% jump from last week.
[00:04:35.840 --> 00:04:39.040]   So through these tricks, he's been able to get a 0.7% jump.
[00:04:39.920 --> 00:04:43.360]   And I believe then I will go to that forum post as well,
[00:04:43.360 --> 00:04:46.240]   just to see what everybody's been doing with Cassava
[00:04:46.240 --> 00:04:48.080]   and how that's different from last week.
[00:04:48.080 --> 00:04:50.720]   And then Ravi Chandra has come back.
[00:04:50.720 --> 00:04:53.440]   And he's written about his learnings from chapter 7 as well.
[00:04:53.440 --> 00:04:56.080]   So thanks, guys, for doing this week after week.
[00:04:56.080 --> 00:04:57.760]   There's also a recap of chapter 6.
[00:04:57.760 --> 00:04:59.360]   Korean has come back.
[00:04:59.360 --> 00:05:02.320]   And I think he just wrote this as of 20 minutes ago,
[00:05:02.320 --> 00:05:07.680]   which is about the various difference between cross-entropy and binary cross-entropy.
[00:05:07.680 --> 00:05:10.160]   Actually, I'm really interested, but due to time,
[00:05:10.160 --> 00:05:12.160]   I didn't get a chance to read this blog post.
[00:05:12.160 --> 00:05:14.000]   So I will go back and read this.
[00:05:14.000 --> 00:05:16.480]   But thanks, Korean, for sharing your blog with us.
[00:05:16.480 --> 00:05:19.440]   Muzie has been his--
[00:05:19.440 --> 00:05:24.000]   I still remember my first time when the fastbook arrived at my place.
[00:05:24.000 --> 00:05:26.080]   And I was really excited to see fastbook.
[00:05:26.080 --> 00:05:27.520]   And that's what has happened.
[00:05:27.520 --> 00:05:29.360]   That kind of moment has happened with him as well.
[00:05:29.360 --> 00:05:32.000]   So my fastbook has arrived.
[00:05:32.000 --> 00:05:36.080]   And then you can see how he's also written this blog post about chapter 6.
[00:05:36.080 --> 00:05:37.120]   So thanks for doing that.
[00:05:37.760 --> 00:05:41.600]   And finally, we have Anand.
[00:05:41.600 --> 00:05:43.520]   Anand has also written about chapter 6.
[00:05:43.520 --> 00:05:45.840]   So a few people playing catch up.
[00:05:45.840 --> 00:05:51.040]   But that's great to see that you guys are still motivated to continue
[00:05:51.040 --> 00:05:55.120]   and, you know, keeping up with the fastbook reading group.
[00:05:55.120 --> 00:05:58.880]   So one thing I do feel excited is that this is the 10th week.
[00:05:58.880 --> 00:06:02.240]   And usually, what I've seen with events is
[00:06:02.240 --> 00:06:06.880]   the interaction kind of goes down by the time we hit the fifth week
[00:06:06.880 --> 00:06:07.920]   or the fourth week.
[00:06:07.920 --> 00:06:09.440]   But something I'm seeing with fastbook
[00:06:09.440 --> 00:06:12.160]   and something that really makes me very happy
[00:06:12.160 --> 00:06:16.480]   and makes me really excited is that all of us have been together
[00:06:16.480 --> 00:06:18.720]   and we've been doing fastbook week after week.
[00:06:18.720 --> 00:06:25.920]   And we've been covering all of these different topics.
[00:06:25.920 --> 00:06:27.760]   And we're still going strong.
[00:06:27.760 --> 00:06:31.520]   So I'm really happy for the outcome, week 10.
[00:06:33.920 --> 00:06:43.920]   I'm having said that, we are going to start with fastbook chapter 8, which is Collab.
[00:06:43.920 --> 00:06:47.760]   I'm sitting, let me get straight into it.
[00:06:47.760 --> 00:06:48.880]   Can everybody see my screen?
[00:06:48.880 --> 00:06:52.320]   Can you guys please post in the chat just to maybe click yes or no
[00:06:52.320 --> 00:06:53.440]   so you can see my screen?
[00:06:53.440 --> 00:06:58.560]   And if the zoom levels are okay.
[00:07:02.000 --> 00:07:02.500]   All right.
[00:07:02.500 --> 00:07:08.640]   So then let's start with collaborative grouping.
[00:07:08.640 --> 00:07:11.360]   So bring up OneNote.
[00:07:11.360 --> 00:07:18.320]   So we can go to fastbook.
[00:07:18.320 --> 00:07:20.320]   That's it.
[00:07:20.320 --> 00:07:28.160]   And we can say add section, call it chapter 8.
[00:07:28.800 --> 00:07:29.300]   All right.
[00:07:29.300 --> 00:07:34.800]   We're all set.
[00:07:34.800 --> 00:07:37.920]   Cool. So what is collaborative filtering?
[00:07:37.920 --> 00:07:42.560]   I think it really makes sense to read this introduction line by line.
[00:07:42.560 --> 00:07:45.040]   So let's start with that.
[00:07:45.040 --> 00:07:50.960]   So one very common problem that's out there to be solved is this problem is
[00:07:50.960 --> 00:07:55.040]   when you have a number of users and you have a number of products,
[00:07:55.040 --> 00:07:59.520]   then what you really want to do is you want to match the users to the products, right?
[00:07:59.520 --> 00:08:02.880]   What you want to say is, oh, this user might like this particular product
[00:08:02.880 --> 00:08:06.560]   or this user won't like this particular product.
[00:08:06.560 --> 00:08:09.360]   So we shouldn't spend time marketing it to that user.
[00:08:09.360 --> 00:08:12.720]   Or like you want to find a group of users, you have a new product,
[00:08:12.720 --> 00:08:14.320]   and then you want to find a group of users.
[00:08:14.320 --> 00:08:18.080]   And so it's like a marketing campaign.
[00:08:18.080 --> 00:08:21.120]   And then you want to find a small group of users that you feel is like
[00:08:21.120 --> 00:08:24.000]   the most apt for a particular kind of product.
[00:08:24.000 --> 00:08:27.280]   So I guess these are all problems of the same kind.
[00:08:27.280 --> 00:08:29.920]   They're all collaborative filtering kind of problems.
[00:08:29.920 --> 00:08:33.920]   And we see these problems, but different variations of it like Netflix.
[00:08:33.920 --> 00:08:36.560]   So when we see movie recommendation, that's again,
[00:08:36.560 --> 00:08:40.160]   a kind of a problem where we're trying to match the users to the product.
[00:08:40.160 --> 00:08:43.040]   So the product here becomes the various movies,
[00:08:43.040 --> 00:08:44.720]   and then the user is the user base.
[00:08:44.720 --> 00:08:49.440]   And then we can see other things like what do you want to highlight on a home page,
[00:08:49.440 --> 00:08:52.560]   deciding what stories to show in social media feeds.
[00:08:52.560 --> 00:08:54.720]   Pretty much like recommendation system.
[00:08:54.720 --> 00:08:58.640]   So that's what we're going to cover in our session today.
[00:08:58.640 --> 00:09:06.400]   And having said that, that's just basically the general idea.
[00:09:06.400 --> 00:09:10.480]   And what we're going to see is that that's the foundation of this idea
[00:09:10.480 --> 00:09:12.320]   is something called latent factors.
[00:09:12.320 --> 00:09:16.720]   So what that latent factors means, we're going to have a look at that.
[00:09:16.720 --> 00:09:18.800]   But think of it this way.
[00:09:18.800 --> 00:09:21.680]   If we still keep the movie recommendation problem
[00:09:21.680 --> 00:09:25.120]   as the main problem that we try to solve today,
[00:09:25.120 --> 00:09:29.120]   and then you can sort of expand that to all of these various different domains.
[00:09:29.120 --> 00:09:32.560]   But if you think of movie recommendation,
[00:09:32.560 --> 00:09:37.280]   then can you think like users have personalities of their own?
[00:09:37.280 --> 00:09:40.960]   Like some users might like action films.
[00:09:40.960 --> 00:09:42.160]   Some users might like...
[00:09:42.160 --> 00:09:45.760]   Yeah, some users might like action films.
[00:09:45.760 --> 00:09:47.920]   Some users might want to see a musical.
[00:09:47.920 --> 00:09:50.000]   Some users might want to see classics.
[00:09:50.000 --> 00:09:52.640]   So there's like all of these different user preferences.
[00:09:52.640 --> 00:09:56.720]   But similarly, movies also have their own personalities.
[00:09:56.720 --> 00:10:01.920]   Like you have movies that are pretty much like you have old movies.
[00:10:01.920 --> 00:10:04.160]   You have action-packed sci-fi movies.
[00:10:04.160 --> 00:10:06.080]   And then you could have maybe like, yeah,
[00:10:06.080 --> 00:10:09.040]   you have all of these different kinds of movies as well.
[00:10:09.040 --> 00:10:12.480]   So movies in themselves have a personality or they have...
[00:10:14.400 --> 00:10:18.800]   So you can pretty much get the idea what latent factors means in this sense
[00:10:18.800 --> 00:10:26.640]   is that you can represent a movie by saying how action-packed it is.
[00:10:26.640 --> 00:10:29.200]   Or you can say, oh, how old it is.
[00:10:29.200 --> 00:10:32.320]   Or you can say how much of a musical it is.
[00:10:32.320 --> 00:10:35.840]   Or you can say, is it going to be likable?
[00:10:35.840 --> 00:10:39.280]   So there's like all of these different traits that a movie can have.
[00:10:39.280 --> 00:10:41.520]   So you can represent a movie using latent factors
[00:10:41.520 --> 00:10:45.040]   and you can similarly represent a user using latent factors.
[00:10:45.040 --> 00:10:48.560]   So I hope that's just the general idea that we should keep in mind
[00:10:48.560 --> 00:10:55.360]   that everything can be represented as long as we think of it that way.
[00:10:55.360 --> 00:10:57.760]   So that's this general idea.
[00:10:57.760 --> 00:11:01.440]   So then as part of the session today,
[00:11:01.440 --> 00:11:03.680]   we're going to be working with the MovieLens data.
[00:11:03.680 --> 00:11:04.640]   So let me bring that up.
[00:11:04.640 --> 00:11:08.320]   So MovieLens is just a smaller...
[00:11:08.320 --> 00:11:11.200]   So MovieLens has like 25 million movie ratings.
[00:11:11.200 --> 00:11:13.840]   So pretty much, we're not going to work with 25 million,
[00:11:13.840 --> 00:11:15.760]   but I'm just going to show you a small part of it.
[00:11:15.760 --> 00:11:20.400]   So you can see we just go, oh, fastai.colab import star.
[00:11:20.400 --> 00:11:22.880]   fastai.tabular.import star.
[00:11:22.880 --> 00:11:24.240]   So that's just importing stuff.
[00:11:24.240 --> 00:11:26.480]   So let me just run that.
[00:11:26.480 --> 00:11:35.200]   Next, I can just say, okay, url.movielens 100k.
[00:11:35.200 --> 00:11:37.200]   So we're going to work with a smaller subset.
[00:11:37.200 --> 00:11:41.280]   So we're not going to work with the 25 million movies that we have,
[00:11:41.280 --> 00:11:45.600]   but rather, we're just going to work with a smaller subset of 100,000 movies.
[00:11:45.600 --> 00:11:52.240]   So I can just say path.data.urls.ml 100k.
[00:11:52.240 --> 00:11:57.520]   So then if I have a look at what's in this path, I can just say path.ls.
[00:11:57.520 --> 00:11:59.440]   You can see there's like these 23 different...
[00:11:59.440 --> 00:12:03.040]   pretty much these different items that are there.
[00:12:03.040 --> 00:12:06.720]   You can see there's like a ml.100k.u.item.
[00:12:06.720 --> 00:12:08.080]   There's uv.test.
[00:12:08.080 --> 00:12:09.440]   There's like all these different files.
[00:12:09.440 --> 00:12:13.200]   And if you want to read about them, you can have a look at this movie lens.
[00:12:13.200 --> 00:12:16.560]   You will know going through the readme or like various different...
[00:12:16.560 --> 00:12:19.600]   just understanding the dataset would be through here.
[00:12:19.600 --> 00:12:23.200]   But as mentioned in Fastbook, according to the readme,
[00:12:23.200 --> 00:12:26.320]   the main table in the file is this u.data.
[00:12:26.320 --> 00:12:28.080]   So it's a tab separated.
[00:12:28.080 --> 00:12:30.400]   So until now, we've looked at CSVs.
[00:12:30.400 --> 00:12:31.360]   So what's CSV?
[00:12:31.360 --> 00:12:36.320]   What's the difference in CSV, which is a comma separated file
[00:12:36.320 --> 00:12:38.800]   and a tab separated file is a delimiter.
[00:12:38.800 --> 00:12:43.840]   So in tab separated files, you can still use the pandas read CSV function.
[00:12:43.840 --> 00:12:46.800]   And you can just say, okay, my delimiter is tab limited.
[00:12:46.800 --> 00:12:51.520]   So in that case, if I read this u.data file,
[00:12:51.520 --> 00:12:53.200]   you can see that I can read my reading.
[00:12:53.200 --> 00:12:56.080]   So I can just go like this and I can say, ratings.head.
[00:12:56.080 --> 00:12:59.360]   So you can see I can pretty much with the data that I have is...
[00:12:59.360 --> 00:13:05.600]   I have some user IDs, like users could be pretty much names or just numbers.
[00:13:05.600 --> 00:13:07.920]   In this case, they are just represented by IDs.
[00:13:07.920 --> 00:13:13.360]   So user 196 saw the movie, which is represented by ID 242.
[00:13:13.360 --> 00:13:15.840]   And he gave that movie a rating of three.
[00:13:15.840 --> 00:13:16.960]   And this is the timestamp.
[00:13:16.960 --> 00:13:20.880]   So you can see the particular user saw that movie at that particular time.
[00:13:20.880 --> 00:13:23.600]   So then there's like this massive dataset of...
[00:13:23.600 --> 00:13:24.480]   Let's see how much.
[00:13:24.480 --> 00:13:29.120]   So ratings.shape, you can see it's 100,000.
[00:13:29.120 --> 00:13:33.360]   So if I go have a look at the tail, again, it's going to be the same data.
[00:13:33.360 --> 00:13:37.680]   We can see user 880 saw this movie 476 and gave a rating three.
[00:13:37.680 --> 00:13:40.400]   This user saw this movie 225 and gave a rating three.
[00:13:40.400 --> 00:13:48.400]   So again, looking at this dataset in this long format of...
[00:13:48.400 --> 00:13:51.440]   Like a tabular long format is not very helpful.
[00:13:51.440 --> 00:13:52.800]   It's not very human friendly.
[00:13:52.800 --> 00:13:59.360]   So what would really help is this next crosstab version of looking at things.
[00:13:59.360 --> 00:14:03.040]   So you can see how we can represent this same dataset
[00:14:03.040 --> 00:14:04.080]   just in this way.
[00:14:04.080 --> 00:14:07.440]   So you can have all your movie IDs at the top.
[00:14:07.440 --> 00:14:11.280]   You can have all your user IDs on your left.
[00:14:11.280 --> 00:14:17.200]   And then you can say, okay, user 14 saw movies 27, 49, 57, and so on.
[00:14:17.200 --> 00:14:20.400]   And he gave these ratings to those movies.
[00:14:20.400 --> 00:14:23.520]   User 29, again, gave those ratings and that particular rule.
[00:14:23.520 --> 00:14:28.240]   User 508 gave those particular ratings like 5, 5, 4.0, and so on.
[00:14:28.240 --> 00:14:31.680]   User 546 didn't see the movie 27.
[00:14:31.680 --> 00:14:33.840]   Therefore, there's an empty space over here.
[00:14:33.840 --> 00:14:39.280]   Similarly, for movie, which is the ID 49, without worrying what that movie is,
[00:14:39.280 --> 00:14:41.440]   users 212, 293 haven't seen it.
[00:14:41.440 --> 00:14:42.480]   So that's just empty.
[00:14:42.480 --> 00:14:46.320]   So again, as mentioned here in Fastbook,
[00:14:46.320 --> 00:14:49.200]   we've just selected a few of the most popular movies.
[00:14:49.200 --> 00:14:51.280]   What it means by the most popular movies,
[00:14:51.280 --> 00:14:54.480]   it means the movies that the most number of people have seen.
[00:14:54.480 --> 00:14:58.480]   And then we've just selected the users who watch the most movies.
[00:14:58.480 --> 00:15:00.960]   So that's just these users who watch the most movies.
[00:15:00.960 --> 00:15:06.880]   That's why this cross tab table is pretty much...
[00:15:06.880 --> 00:15:08.560]   There's like very less empty spaces.
[00:15:08.560 --> 00:15:11.360]   Or if you think of it, it's not sparse at all.
[00:15:11.360 --> 00:15:17.360]   But one thing that we do, we should consider over here is...
[00:15:23.120 --> 00:15:32.720]   It's like if I have 1 million users and I have...
[00:15:32.720 --> 00:15:37.680]   So I just wanted to quickly highlight this.
[00:15:37.680 --> 00:15:43.920]   And I have, say, 10 million movies, right?
[00:15:43.920 --> 00:15:47.840]   So that in a way is a gigantic table.
[00:15:47.840 --> 00:15:49.840]   It's a massive, massive table, right?
[00:15:49.840 --> 00:15:52.400]   Can you imagine how much memory this table would need?
[00:15:52.400 --> 00:15:55.200]   And then you can see, if we just...
[00:15:55.200 --> 00:15:58.160]   This is going to be... this table is going to be that massive.
[00:15:58.160 --> 00:16:00.960]   And then one thing you will see is like these one point...
[00:16:00.960 --> 00:16:05.440]   Out of these 1 million users, only a very few handful
[00:16:05.440 --> 00:16:08.000]   would have seen a very few handful of movies.
[00:16:08.000 --> 00:16:11.680]   So like if I go from 1 to 1 million over here,
[00:16:11.680 --> 00:16:14.720]   and then I go from 1 to 100 million over here,
[00:16:14.720 --> 00:16:18.240]   there's gonna be a lots of lots and lots of like empty spaces.
[00:16:18.240 --> 00:16:20.800]   There's only going to be a few points that are going to be filled.
[00:16:20.800 --> 00:16:22.880]   So it's actually going to be this gigantic data...
[00:16:22.880 --> 00:16:26.000]   It's actually going to be this gigantic table
[00:16:26.000 --> 00:16:28.800]   where lots of that space is going to be empty.
[00:16:28.800 --> 00:16:32.720]   So I just want to point that out and before we move on.
[00:16:32.720 --> 00:16:37.360]   So in this case, this data set is pretty full because...
[00:16:37.360 --> 00:16:38.320]   I'm sorry. One second.
[00:16:38.320 --> 00:16:49.120]   In this case, what's happening is that
[00:16:49.120 --> 00:16:51.120]   because we've just selected the top users
[00:16:51.120 --> 00:16:53.280]   and we've just selected the top movie IDs,
[00:16:53.280 --> 00:16:57.280]   that's why this data set or this table is pretty full.
[00:16:57.280 --> 00:17:01.920]   So that's what it says.
[00:17:01.920 --> 00:17:05.520]   If we knew for each user to what degree they like each important category,
[00:17:05.520 --> 00:17:10.080]   this is just extra stuff that's being mentioned in the book.
[00:17:10.080 --> 00:17:10.480]   It's like...
[00:17:10.480 --> 00:17:14.960]   Sorry. Let's not worry about that.
[00:17:14.960 --> 00:17:16.960]   So then let's just go over here.
[00:17:16.960 --> 00:17:19.040]   So let's say this movie Last...
[00:17:19.040 --> 00:17:21.760]   Let's say there's this movie called Last Skywalker.
[00:17:21.760 --> 00:17:25.440]   What if I could represent the movie Last Skywalker by three numbers?
[00:17:25.440 --> 00:17:32.800]   Again, as I've said, this is the idea of representing a movie by latent factors.
[00:17:32.800 --> 00:17:35.040]   And what those latent factors are,
[00:17:35.040 --> 00:17:38.160]   they could be genre, age, and preferred doctor.
[00:17:38.160 --> 00:17:39.440]   So let's read this now.
[00:17:39.440 --> 00:17:44.720]   So if we knew for each user to what degree they like each important category
[00:17:44.720 --> 00:17:49.760]   that a movie might fall into such as genre, age, preferred directors and actors,
[00:17:49.760 --> 00:17:53.280]   and so forth, we would know the same information about each movie.
[00:17:53.280 --> 00:17:58.400]   Then simply we can fill it in this table by using that information.
[00:17:58.400 --> 00:18:01.280]   So what that means is if I go here,
[00:18:01.280 --> 00:18:06.800]   if I just pick one particular movie, right,
[00:18:06.800 --> 00:18:13.760]   I can represent that by say 1, 2, 3, 4, and 5 numbers.
[00:18:13.760 --> 00:18:19.280]   And what those what these five numbers could mean is like old,
[00:18:19.280 --> 00:18:22.000]   like how old the movie is.
[00:18:22.000 --> 00:18:27.440]   When I say how old the movie is, if I'm rating this movie between -1 and 1,
[00:18:27.440 --> 00:18:32.480]   1.0 being the movie is really, really old, like many, many years old.
[00:18:32.480 --> 00:18:35.840]   And then let's say how much action there is.
[00:18:35.840 --> 00:18:38.080]   So action is like a 0.5.
[00:18:38.080 --> 00:18:40.640]   Or we could say how much of a musical it is.
[00:18:40.640 --> 00:18:45.920]   And you could say it's -0.7, which means there's not much music in the movie.
[00:18:45.920 --> 00:18:48.400]   So I'm just trying to make these like numbers up.
[00:18:48.400 --> 00:18:53.440]   But then these things at the top that are used to represent a movie
[00:18:53.440 --> 00:18:54.880]   are called as latent factors.
[00:18:54.880 --> 00:19:00.640]   And then similarly, for each of the users, I could have said, okay,
[00:19:00.640 --> 00:19:08.560]   well, what if this user, I can represent my user again, say by 5 numbers.
[00:19:08.560 --> 00:19:13.360]   And I can say the idea of how much does the user like old movies.
[00:19:13.360 --> 00:19:18.000]   Like this particular user, so I'm just going to put a small diagram over there.
[00:19:18.000 --> 00:19:21.520]   And I can say this user really likes old movies.
[00:19:21.520 --> 00:19:22.400]   So that's like 1.
[00:19:22.400 --> 00:19:23.440]   And I'm rating things again.
[00:19:23.440 --> 00:19:27.520]   This user does not like action movies at all.
[00:19:27.520 --> 00:19:29.600]   So it's like -0.9.
[00:19:29.600 --> 00:19:34.240]   And then in terms of musical, this user does not like minus,
[00:19:34.240 --> 00:19:35.680]   does not like musical movies as well.
[00:19:35.680 --> 00:19:37.520]   So the score there is 0.7.
[00:19:37.520 --> 00:19:38.960]   And then you can see if you want to match,
[00:19:38.960 --> 00:19:42.400]   then you can see the idea right in front of you.
[00:19:42.400 --> 00:19:45.680]   If you want to match a user to a movie,
[00:19:45.680 --> 00:19:48.240]   what if you just multiply these two things?
[00:19:48.240 --> 00:19:51.120]   Like you multiply them between each other.
[00:19:51.120 --> 00:19:56.080]   So if old, this user really likes old movies.
[00:19:56.080 --> 00:19:59.680]   This movie is really old, which means the match here is 1.0.
[00:19:59.680 --> 00:20:05.440]   This user hates action-packed movies.
[00:20:05.440 --> 00:20:11.520]   But this movie has about 0.5 action in it, which means it's -0.45.
[00:20:11.520 --> 00:20:14.240]   So as in a score over here would be -0.45,
[00:20:14.240 --> 00:20:16.800]   which means that's going to be a negative score.
[00:20:16.800 --> 00:20:20.880]   And then this user really doesn't like musical movies.
[00:20:20.880 --> 00:20:23.360]   This movie does not have any musicals.
[00:20:23.360 --> 00:20:25.520]   So you can see the score is going to be positive.
[00:20:25.520 --> 00:20:28.160]   So you can see where every time there's going to be a match
[00:20:28.160 --> 00:20:33.280]   between what the user likes and what's present in the movie.
[00:20:33.280 --> 00:20:37.920]   In that case, my score is going to be positive.
[00:20:37.920 --> 00:20:39.200]   Does that make sense so far?
[00:20:39.200 --> 00:20:44.480]   So then I could just add my scores across these various latent factors
[00:20:44.480 --> 00:20:46.720]   and I could get a single digit score.
[00:20:46.720 --> 00:20:49.680]   And if that score is high, which means that
[00:20:49.680 --> 00:20:52.640]   this movie is a good recommendation for this user.
[00:20:52.640 --> 00:20:54.000]   So that's just the basic idea.
[00:20:54.000 --> 00:20:57.680]   So that's what you'll see when you read this.
[00:20:57.680 --> 00:21:01.120]   It's like, okay, we can represent this movie "Last Skywalker"
[00:21:01.120 --> 00:21:02.640]   by just these three numbers.
[00:21:02.640 --> 00:21:06.960]   Again, these three numbers could be old, action-packed, and musical.
[00:21:06.960 --> 00:21:10.560]   And then similarly, I could represent my user using three numbers,
[00:21:10.560 --> 00:21:15.520]   which is like, okay, does my user really like old movies?
[00:21:15.520 --> 00:21:18.800]   Does my user really like action-packed movies?
[00:21:18.800 --> 00:21:21.600]   Does my user really like musical movies?
[00:21:21.600 --> 00:21:24.880]   And then you can see, okay, you can multiply them one and one.
[00:21:24.880 --> 00:21:26.560]   So you can multiply this with that.
[00:21:26.560 --> 00:21:28.640]   And you can just take the sum and you get your score.
[00:21:28.640 --> 00:21:30.240]   So that's that.
[00:21:31.680 --> 00:21:32.240]   One second.
[00:21:32.240 --> 00:21:35.840]   All right.
[00:21:35.840 --> 00:21:36.320]   Sorry, guys.
[00:21:36.320 --> 00:21:37.360]   I'll be right back.
[00:21:37.360 --> 00:21:38.240]   There's someone at the door.
[00:21:38.240 --> 00:21:40.160]   Just give me one sec.
[00:21:40.160 --> 00:22:09.240]   [ Silence ]
[00:22:09.740 --> 00:22:15.100]   Sorry, stuff always has to happen when I'm in the middle of something important.
[00:22:15.100 --> 00:22:20.440]   Yeah, the work from home life, this is it.
[00:22:20.440 --> 00:22:22.840]   If I was in a meeting room, this would never have happened.
[00:22:22.840 --> 00:22:26.480]   Anyway, so then that's where we are.
[00:22:26.480 --> 00:22:32.420]   So then you can say how -- you can see how we can match movies to users.
[00:22:32.420 --> 00:22:37.620]   And then the way we're matching is through these latent factors, right?
[00:22:37.620 --> 00:22:41.420]   So then that way, the score between user one,
[00:22:41.420 --> 00:22:45.800]   liking this movie Last Skywalker, is 2.41.
[00:22:45.800 --> 00:22:50.600]   While this -- 2.14, while this number on its own doesn't like --
[00:22:50.600 --> 00:22:54.640]   it doesn't hold any weight of its own, like number 2.1,
[00:22:54.640 --> 00:22:56.620]   is not something that's important to us.
[00:22:56.620 --> 00:23:02.020]   What's important to us is like how high is this number or how low is this number?
[00:23:02.020 --> 00:23:05.420]   If this number is negative, which means there's not a good match,
[00:23:05.420 --> 00:23:08.340]   and if this number is positive, it means it's a good match, okay?
[00:23:08.340 --> 00:23:10.660]   So that's what this idea is.
[00:23:10.660 --> 00:23:14.540]   And then this idea of multiplying two vectors together and adding up the results
[00:23:14.540 --> 00:23:17.120]   in deep learning is called as dot product.
[00:23:17.120 --> 00:23:20.580]   So this is something we've seen so many times everywhere,
[00:23:20.580 --> 00:23:23.260]   and you finally know now what dot product is.
[00:23:23.260 --> 00:23:25.860]   It's just multiplying two arrays and then adding them together.
[00:23:25.860 --> 00:23:31.240]   So you can see, on the other hand, we might represent the movie Casablanca like this,
[00:23:31.240 --> 00:23:35.140]   and then you can see how the match between now the user is negative.
[00:23:35.140 --> 00:23:39.780]   So you can see how you can match users to movies.
[00:23:39.780 --> 00:23:47.100]   So then the idea is -- so then the idea is, oh, but the computer doesn't know, like,
[00:23:47.100 --> 00:23:53.340]   we as humans know how good a movie is, or we as humans know how, like,
[00:23:53.340 --> 00:23:56.420]   how much of a classical it is, how much of a musical it is.
[00:23:56.420 --> 00:23:59.780]   Like, as humans, we can tell those things, but what about a computer?
[00:23:59.780 --> 00:24:01.420]   A computer wouldn't know those things.
[00:24:02.140 --> 00:24:06.460]   So the idea that comes forward is, like, you learn the latent factors.
[00:24:06.460 --> 00:24:09.260]   So let me bring that up and show you what that means.
[00:24:09.260 --> 00:24:14.660]   Just copy and paste that.
[00:24:14.660 --> 00:24:23.300]   All right.
[00:24:23.300 --> 00:24:31.260]   So then over here, what could happen is I'm just going to say my movie --
[00:24:31.260 --> 00:24:39.820]   so let's say I have -- I represent each movie by a vector of length 5.
[00:24:39.820 --> 00:24:42.540]   So you can see 1, 2, 3, 4, 5.
[00:24:42.540 --> 00:24:47.020]   And I represent each user -- so let me use a different color for users.
[00:24:47.020 --> 00:24:50.940]   I represent each user by, again, a vector of length 5, right?
[00:24:50.940 --> 00:24:53.340]   Now, what do these mean?
[00:24:53.340 --> 00:24:57.740]   I could say things like -- oh, again, same.
[00:24:58.620 --> 00:25:02.300]   How much of a sci-fi is this?
[00:25:02.300 --> 00:25:04.380]   Is represented by these numbers at the top.
[00:25:04.380 --> 00:25:08.620]   How much of action is present?
[00:25:08.620 --> 00:25:10.540]   Is it represented by these numbers?
[00:25:10.540 --> 00:25:13.180]   How much of -- I don't know.
[00:25:13.180 --> 00:25:17.740]   Again, I'll say musical, and then I'll say theater, maybe.
[00:25:17.740 --> 00:25:21.180]   So I'm just going to say "the" and then so on.
[00:25:21.180 --> 00:25:23.100]   And then maybe this could be sport.
[00:25:23.100 --> 00:25:25.500]   How much sport is in the movie?
[00:25:25.500 --> 00:25:27.180]   I'm just making these latent factors up.
[00:25:27.180 --> 00:25:29.180]   We don't need to worry like what they are.
[00:25:29.180 --> 00:25:33.340]   But I'm just -- just so we all understand, I'm just making these up.
[00:25:33.340 --> 00:25:38.740]   And then similarly, the latent factors for the users could be like --
[00:25:38.740 --> 00:25:42.540]   oh, I'm just going to go on like that.
[00:25:42.540 --> 00:25:45.660]   And then I could say, oh, how much does this user like sci-fi movies?
[00:25:45.660 --> 00:25:48.060]   How much does this user like action movies?
[00:25:48.060 --> 00:25:50.860]   How much does this user like musical movies?
[00:25:50.860 --> 00:25:53.420]   How much does this user like theater movies?
[00:25:53.420 --> 00:25:56.620]   And how much does this user like movies that are related to sport?
[00:25:57.580 --> 00:26:03.100]   So we can see like for user 14, we can see the user doesn't really like
[00:26:03.100 --> 00:26:05.100]   sci-fi movies because the score is like 0.2.
[00:26:05.100 --> 00:26:07.740]   The score for action is high.
[00:26:07.740 --> 00:26:10.220]   So this user really likes action movies.
[00:26:10.220 --> 00:26:13.660]   This user really also likes musical movies.
[00:26:13.660 --> 00:26:16.220]   This user doesn't really like theatrical movies.
[00:26:16.220 --> 00:26:19.580]   And this user kind of likes sport.
[00:26:19.580 --> 00:26:21.580]   So then you can see how that's the preference.
[00:26:21.580 --> 00:26:25.580]   And then for sci-fi, you can see like the same --
[00:26:25.580 --> 00:26:27.660]   for the movie, you can see like the same things.
[00:26:27.660 --> 00:26:31.420]   So you can pretty much read this for this movie 27.
[00:26:31.420 --> 00:26:35.900]   We can see, oh, this movie is not really anything --
[00:26:35.900 --> 00:26:37.740]   has not much to do with sci-fi.
[00:26:37.740 --> 00:26:39.900]   This movie is somewhat action-packed.
[00:26:39.900 --> 00:26:42.220]   This movie, there is some sort of musical in it.
[00:26:42.220 --> 00:26:43.660]   There is quite a bit of theater.
[00:26:43.660 --> 00:26:45.340]   And there is a lot of sport in that movie.
[00:26:45.340 --> 00:26:48.140]   So that's how I would read these numbers.
[00:26:48.140 --> 00:26:55.020]   Now, the idea is what we want to do is
[00:26:55.020 --> 00:27:00.460]   when we saw this example before, it's like we know --
[00:27:00.460 --> 00:27:03.820]   this is the -- these are the actual user ratings, right?
[00:27:03.820 --> 00:27:07.500]   We know user 14 gave a rating of 3 for the movie 27.
[00:27:07.500 --> 00:27:09.420]   And the idea for this collaborative filtering
[00:27:09.420 --> 00:27:10.620]   is to fill in these gaps.
[00:27:10.620 --> 00:27:13.980]   So if we can figure out the gap on like how much
[00:27:13.980 --> 00:27:18.940]   would a user 212 give a rating to the movie 49,
[00:27:18.940 --> 00:27:21.260]   then that's what we want to know.
[00:27:21.260 --> 00:27:22.860]   So then that becomes the recommendation.
[00:27:22.860 --> 00:27:25.340]   If this number is high, that means, oh,
[00:27:25.340 --> 00:27:28.140]   we should really recommend this movie to the user.
[00:27:28.140 --> 00:27:33.980]   So the idea that we want to do is we can now find the scores.
[00:27:33.980 --> 00:27:36.620]   So these are like all the numbers over here.
[00:27:36.620 --> 00:27:38.060]   These are scores, right?
[00:27:38.060 --> 00:27:39.500]   And how do we find these scores?
[00:27:39.500 --> 00:27:41.500]   It's like we multiply these five numbers.
[00:27:41.500 --> 00:27:43.420]   So let me use a different color. One second.
[00:27:43.420 --> 00:27:45.980]   So we multiply these five numbers.
[00:27:50.780 --> 00:27:53.900]   I'm just trying to find maybe a good color.
[00:27:53.900 --> 00:27:57.900]   So we multiply these five numbers with these five numbers, right?
[00:27:57.900 --> 00:27:59.580]   And when you multiply these five numbers,
[00:27:59.580 --> 00:28:00.860]   you can just add them up.
[00:28:00.860 --> 00:28:02.380]   So you'll get a score over here.
[00:28:02.380 --> 00:28:04.860]   Similarly, if I multiply these five numbers
[00:28:04.860 --> 00:28:07.420]   with the same five numbers at the top,
[00:28:07.420 --> 00:28:09.180]   I get this score 4.92.
[00:28:09.180 --> 00:28:11.580]   That's 5.79 times 1.69.
[00:28:11.580 --> 00:28:15.020]   0.79 times 1.69.
[00:28:15.020 --> 00:28:17.340]   1.07 times this one and so on.
[00:28:17.340 --> 00:28:19.580]   You multiply that, you get 4.92.
[00:28:19.580 --> 00:28:21.020]   So this is the idea, right?
[00:28:21.020 --> 00:28:27.020]   So then what I could have done is in this...
[00:28:27.020 --> 00:28:28.700]   This is not a very data science.
[00:28:28.700 --> 00:28:31.580]   We're not using deep learning at the moment, right?
[00:28:31.580 --> 00:28:35.900]   But what if I told you is like...
[00:28:35.900 --> 00:28:40.940]   Let me just start with a fresh image again.
[00:28:40.940 --> 00:28:47.340]   So what if I told you all of these numbers could be learned?
[00:28:47.340 --> 00:28:49.740]   Like what I could instead do is...
[00:28:49.740 --> 00:28:58.780]   What I could instead do is like...
[00:28:58.780 --> 00:29:01.020]   I start with just these...
[00:29:01.020 --> 00:29:02.860]   All these numbers at the top.
[00:29:02.860 --> 00:29:03.980]   All of these numbers.
[00:29:03.980 --> 00:29:06.060]   And all of these numbers for the users.
[00:29:06.060 --> 00:29:09.340]   I could just start with random numbers, right?
[00:29:09.340 --> 00:29:12.220]   So I could just start with random numbers.
[00:29:12.220 --> 00:29:15.260]   I could find the predictions
[00:29:16.300 --> 00:29:18.060]   by multiplying these random numbers.
[00:29:18.060 --> 00:29:21.660]   Then I could compare these predictions
[00:29:21.660 --> 00:29:24.540]   to the actual ground truth that we have.
[00:29:24.540 --> 00:29:28.060]   So if I start with a prediction of 3.24 over here
[00:29:28.060 --> 00:29:31.580]   and because I also have the actual ratings,
[00:29:31.580 --> 00:29:37.420]   I know user 293 for movie 27 like this movie.
[00:29:37.420 --> 00:29:38.460]   Like really like this movie.
[00:29:38.460 --> 00:29:40.060]   So the score was maybe 5.
[00:29:40.060 --> 00:29:42.380]   Then this way, I can calculate the loss
[00:29:42.380 --> 00:29:44.620]   and I can actually learn all of these numbers
[00:29:44.620 --> 00:29:46.220]   and I can learn all of those numbers.
[00:29:46.220 --> 00:29:48.300]   So I can learn these latent factors.
[00:29:48.300 --> 00:29:49.180]   Does that make sense?
[00:29:49.180 --> 00:29:51.260]   If it doesn't make sense, let me know
[00:29:51.260 --> 00:29:54.460]   and like just post in the forum.
[00:29:54.460 --> 00:29:58.700]   But then this is the main idea behind collaborative filtering
[00:29:58.700 --> 00:30:00.220]   or behind movie recommendation.
[00:30:00.220 --> 00:30:04.540]   Like you start with some random representations of your users.
[00:30:04.540 --> 00:30:07.260]   You start with some random representation of your movies.
[00:30:07.260 --> 00:30:08.860]   Then you multiply them within each other.
[00:30:08.860 --> 00:30:10.220]   So you have like scores.
[00:30:10.220 --> 00:30:11.580]   And then you calculate the loss
[00:30:11.580 --> 00:30:13.660]   and you can use stochastic gradient descent
[00:30:13.660 --> 00:30:16.700]   to learn these latent factors at the top.
[00:30:16.700 --> 00:30:18.860]   And once you've learned these latent factors,
[00:30:18.860 --> 00:30:20.780]   it means that now the computer knows,
[00:30:20.780 --> 00:30:22.300]   okay, which movie is which?
[00:30:22.300 --> 00:30:24.460]   Does this movie, is this movie action-packed?
[00:30:24.460 --> 00:30:25.900]   Is this movie full of sport?
[00:30:25.900 --> 00:30:26.700]   And so on.
[00:30:26.700 --> 00:30:28.620]   And it also, the computer already knows
[00:30:28.620 --> 00:30:30.940]   or like our deep learning algorithm already knows,
[00:30:30.940 --> 00:30:32.860]   oh, this user really likes action movies.
[00:30:32.860 --> 00:30:37.980]   This user really likes musical movies and so on.
[00:30:37.980 --> 00:30:41.740]   So this is this idea of now being able to mix and match.
[00:30:41.740 --> 00:30:44.380]   So as you can see, step one of the approach
[00:30:44.380 --> 00:30:46.300]   of like learning the latent factors is,
[00:30:46.300 --> 00:30:48.540]   step one of the approach is to randomly initialize
[00:30:48.540 --> 00:30:49.420]   some parameters.
[00:30:49.420 --> 00:30:52.620]   So that's this blue at the top and orange at the bottom.
[00:30:52.620 --> 00:30:55.420]   Step two of the approach is to calculate predictions.
[00:30:55.420 --> 00:30:57.180]   That's all of these numbers inside,
[00:30:57.180 --> 00:31:00.220]   like this 3.12, 4.24 and so on.
[00:31:00.220 --> 00:31:03.180]   Step three is to calculate a loss.
[00:31:03.180 --> 00:31:05.020]   And once you know in deep learning,
[00:31:05.020 --> 00:31:06.780]   once we have calculated the loss,
[00:31:06.780 --> 00:31:08.780]   then everything is just straightforward.
[00:31:08.780 --> 00:31:10.380]   Then what you can do is you can use
[00:31:10.380 --> 00:31:11.740]   stochastic gradient descent.
[00:31:11.740 --> 00:31:13.340]   The computer can learn on its own,
[00:31:13.340 --> 00:31:15.020]   which is what deep learning is.
[00:31:15.020 --> 00:31:17.260]   And we don't need to worry about anything.
[00:31:17.260 --> 00:31:18.540]   So we just calculate the loss.
[00:31:18.540 --> 00:31:20.140]   We do stochastic gradient descent.
[00:31:20.140 --> 00:31:21.980]   And we do it over and over and over again,
[00:31:21.980 --> 00:31:23.020]   epoch by epoch.
[00:31:23.020 --> 00:31:24.780]   So the model would actually learn
[00:31:24.780 --> 00:31:26.380]   good representation of my users.
[00:31:26.380 --> 00:31:27.420]   The model would actually learn
[00:31:27.420 --> 00:31:29.100]   good representation of my movies.
[00:31:29.100 --> 00:31:29.660]   And that's it.
[00:31:29.660 --> 00:31:30.860]   That's my job done.
[00:31:30.860 --> 00:31:34.780]   So if in the future, what I have is like,
[00:31:34.780 --> 00:31:36.940]   if I have a gap over here,
[00:31:36.940 --> 00:31:38.620]   but because the computer would already know
[00:31:38.620 --> 00:31:41.980]   about movie ID 99 and the deep learning algorithm,
[00:31:41.980 --> 00:31:43.660]   when I say computer deep learning algorithm,
[00:31:43.660 --> 00:31:45.660]   I'm just trying to make things like,
[00:31:45.660 --> 00:31:47.020]   just use a simple language.
[00:31:47.020 --> 00:31:49.020]   So the deep learning algorithm would just know
[00:31:49.020 --> 00:31:51.260]   about 2.12 user ID.
[00:31:51.260 --> 00:31:55.420]   So I can just match this user with this movie
[00:31:55.420 --> 00:31:57.340]   and I can just get my score.
[00:31:57.340 --> 00:31:58.380]   I hope that makes sense.
[00:31:58.380 --> 00:32:00.780]   And so that's the main idea.
[00:32:00.780 --> 00:32:05.020]   So I'm just going to go to check
[00:32:05.020 --> 00:32:08.220]   if there's any questions on the main idea so far.
[00:32:08.220 --> 00:32:09.820]   But that's just the main idea.
[00:32:09.820 --> 00:32:12.860]   After this, it's just code.
[00:32:12.860 --> 00:32:15.340]   Pretty much it's just code.
[00:32:15.340 --> 00:32:17.580]   So let's see if there's any questions.
[00:32:17.580 --> 00:32:20.480]   All right.
[00:32:20.480 --> 00:32:24.940]   How much of the grid needs to be filled out
[00:32:24.940 --> 00:32:27.020]   to this type of problem fail?
[00:32:27.020 --> 00:32:28.140]   That's a really good question
[00:32:28.140 --> 00:32:31.900]   and it will be covered just in some time.
[00:32:31.900 --> 00:32:34.300]   Is there any point where you throw out a movie
[00:32:34.300 --> 00:32:36.700]   or use the average until a certain point?
[00:32:36.700 --> 00:32:38.620]   Let's come back to this question.
[00:32:38.620 --> 00:32:40.380]   This is a really, really good question, Kevin.
[00:32:40.380 --> 00:32:41.100]   Thanks for asking.
[00:32:41.100 --> 00:32:44.780]   Do latent factors have to be inferred?
[00:32:44.780 --> 00:32:47.020]   What does that mean?
[00:32:47.020 --> 00:32:49.660]   So I have had issues with that in the past.
[00:32:49.660 --> 00:32:51.420]   If I'm not an expert in certain domain,
[00:32:51.420 --> 00:32:53.740]   I don't watch that many movies.
[00:32:53.740 --> 00:32:56.140]   So I may not know why the movies are on different ends
[00:32:56.140 --> 00:32:57.980]   of the latent feature scale.
[00:32:57.980 --> 00:33:01.900]   Is that the point that is important to work
[00:33:01.900 --> 00:33:03.100]   with a subject matter expert?
[00:33:03.100 --> 00:33:05.260]   I really don't understand, Kevin.
[00:33:06.220 --> 00:33:08.620]   Yeah, I'm sorry I don't understand the question.
[00:33:08.620 --> 00:33:10.620]   Could you maybe reply to this?
[00:33:10.620 --> 00:33:15.340]   As long as you understand like a user can be represented
[00:33:15.340 --> 00:33:18.380]   by some numbers like a vector of five numbers
[00:33:18.380 --> 00:33:19.900]   and then movies could be represented
[00:33:19.900 --> 00:33:21.420]   by a vector of five numbers.
[00:33:21.420 --> 00:33:23.740]   And then based on those vectors,
[00:33:23.740 --> 00:33:26.140]   we can find if a user would like a movie.
[00:33:26.140 --> 00:33:27.980]   That's job done so far.
[00:33:27.980 --> 00:33:30.540]   But is this question something else?
[00:33:30.540 --> 00:33:32.540]   So I will come back to that.
[00:33:33.100 --> 00:33:35.980]   So having said that, then that's this main idea
[00:33:35.980 --> 00:33:38.780]   of like three step approach of doing collaborative flipping.
[00:33:38.780 --> 00:33:42.300]   So you can see now I can just read my movie again.
[00:33:42.300 --> 00:33:46.380]   So let's say movies, I read my movie.
[00:33:46.380 --> 00:33:47.660]   I can say this is my movie.
[00:33:47.660 --> 00:33:48.860]   These are the titles.
[00:33:48.860 --> 00:33:51.420]   Then we merge these movies with our ratings,
[00:33:51.420 --> 00:33:53.500]   which we already had table in the past.
[00:33:53.500 --> 00:33:55.420]   So you can now see, okay, movie 242
[00:33:55.420 --> 00:33:57.500]   is actually this movie and so on.
[00:33:57.500 --> 00:34:00.540]   And you can see there's all these different uses
[00:34:00.540 --> 00:34:02.540]   that are seen this movie 242.
[00:34:02.540 --> 00:34:04.460]   And you can see what are the ratings they've given.
[00:34:04.460 --> 00:34:08.860]   So now I can use FastAI to create my data loaders
[00:34:08.860 --> 00:34:10.380]   from data frames.
[00:34:10.380 --> 00:34:11.580]   So that's what this means.
[00:34:11.580 --> 00:34:14.620]   So collab data loaders from data frame
[00:34:14.620 --> 00:34:17.180]   and I can pass in my data frame ratings.
[00:34:17.180 --> 00:34:19.180]   And this expects an item name.
[00:34:19.180 --> 00:34:20.620]   What does this item name means?
[00:34:20.620 --> 00:34:23.420]   It's just been mentioned here.
[00:34:23.420 --> 00:34:27.100]   By default, this collaborative data loaders, oh, sorry.
[00:34:27.100 --> 00:34:29.860]   (mouse clicking)
[00:34:29.860 --> 00:34:34.380]   That should be fine.
[00:34:34.380 --> 00:34:37.740]   So it says by default, the collaborative data loaders
[00:34:37.740 --> 00:34:40.060]   takes the first column of the user
[00:34:40.060 --> 00:34:42.620]   and it takes the second column for the item.
[00:34:42.620 --> 00:34:46.860]   It takes the second column for the item
[00:34:46.860 --> 00:34:48.620]   and the third column for the readings.
[00:34:48.620 --> 00:34:51.020]   So that's what it says.
[00:34:51.020 --> 00:34:53.420]   First column is user, second column is item
[00:34:53.420 --> 00:34:55.340]   and third column is rating, which is correct.
[00:34:55.340 --> 00:34:58.540]   But in movies, what we have done here is like
[00:34:58.540 --> 00:35:02.300]   for the movies, instead of representing movies by IDs,
[00:35:02.300 --> 00:35:05.580]   in this case, we just represent movies by titles.
[00:35:05.580 --> 00:35:07.500]   It's more human readable.
[00:35:07.500 --> 00:35:10.220]   So you can see now I can just say DLs.showBatch.
[00:35:10.220 --> 00:35:11.500]   So you can see our batch.
[00:35:11.500 --> 00:35:13.180]   So let's just run this cell.
[00:35:13.180 --> 00:35:14.940]   There we go.
[00:35:14.940 --> 00:35:17.660]   We can see this is what our data loader looks like.
[00:35:17.660 --> 00:35:20.700]   You have a user, you have a title and you have rating.
[00:35:20.700 --> 00:35:22.860]   Cool.
[00:35:24.220 --> 00:35:27.900]   You can check the classes in my data loaders.
[00:35:27.900 --> 00:35:31.740]   So if I say DLs.classes, you can see I have however many users.
[00:35:31.740 --> 00:35:33.900]   I have 943 users.
[00:35:33.900 --> 00:35:39.100]   The total number of users in our data loader is 944
[00:35:39.100 --> 00:35:42.860]   because we also have an extra user
[00:35:42.860 --> 00:35:47.340]   for all of those empty spots in basically the matrix.
[00:35:47.340 --> 00:35:51.900]   And again, the number of movies that we have is 1664
[00:35:52.700 --> 00:35:55.740]   because we again have an NA.
[00:35:55.740 --> 00:35:59.100]   So for all those users that have not seen movies,
[00:35:59.100 --> 00:36:02.060]   then that movie is represented by #NA.
[00:36:02.060 --> 00:36:04.220]   So that's just an empty slot.
[00:36:04.220 --> 00:36:06.940]   So in total, you can see that's how my classes
[00:36:06.940 --> 00:36:08.380]   in my data loader looks like.
[00:36:08.380 --> 00:36:11.260]   So you can get the number of users by saying
[00:36:11.260 --> 00:36:14.300]   length number of classes and you just grab the user class.
[00:36:14.300 --> 00:36:16.620]   You can get the number of users by doing this.
[00:36:16.620 --> 00:36:20.460]   And then we just in code,
[00:36:20.460 --> 00:36:25.020]   we're just doing what we did in this image just in code.
[00:36:25.020 --> 00:36:25.500]   Right?
[00:36:25.500 --> 00:36:29.340]   So I could just say I want the number of latent factors to be 5.
[00:36:29.340 --> 00:36:30.940]   So this is exactly that.
[00:36:30.940 --> 00:36:33.100]   1, 2, 3, 4, 5.
[00:36:33.100 --> 00:36:35.740]   So I can say I want my number of latent factors to be 5.
[00:36:35.740 --> 00:36:37.820]   So what's my-- if I have--
[00:36:37.820 --> 00:36:40.060]   again, this is a very simple question.
[00:36:40.060 --> 00:36:42.380]   If I have-- how many users do I have?
[00:36:42.380 --> 00:36:42.700]   Sorry.
[00:36:42.700 --> 00:36:45.340]   N users, N movies.
[00:36:45.340 --> 00:36:45.660]   Right?
[00:36:45.660 --> 00:36:47.020]   We already know how many we have.
[00:36:47.980 --> 00:36:52.620]   I have 944 users, 1665 movies.
[00:36:52.620 --> 00:36:57.900]   So if I have 944 users and then I want to represent
[00:36:57.900 --> 00:37:02.780]   each user by a 5 length vector,
[00:37:02.780 --> 00:37:04.940]   then what's my user matrix?
[00:37:04.940 --> 00:37:08.060]   Like I could call this user matrix.
[00:37:08.060 --> 00:37:08.300]   Right?
[00:37:08.300 --> 00:37:12.380]   So all of this could be my user matrix.
[00:37:12.380 --> 00:37:16.860]   And all of this could be my movie matrix.
[00:37:16.860 --> 00:37:17.340]   Right?
[00:37:17.340 --> 00:37:19.820]   So I want to find the size of my user matrix.
[00:37:19.820 --> 00:37:21.020]   What would that be?
[00:37:21.020 --> 00:37:22.220]   How many users do I have?
[00:37:22.220 --> 00:37:24.380]   I have 9--
[00:37:24.380 --> 00:37:32.700]   I have 944 users and each user is 5.
[00:37:32.700 --> 00:37:35.900]   So I'm going to have 944 by 5.
[00:37:35.900 --> 00:37:37.660]   That's going to be my matrix size.
[00:37:37.660 --> 00:37:37.900]   Right?
[00:37:37.900 --> 00:37:39.900]   So let's see.
[00:37:39.900 --> 00:37:42.940]   So I can say my user factors or my user matrix
[00:37:42.940 --> 00:37:45.420]   is going to be N users times N factors.
[00:37:45.420 --> 00:37:49.820]   And my movie factors is going to be number of movies times N factors.
[00:37:49.820 --> 00:37:51.980]   So we can let's just check the shapes of these.
[00:37:51.980 --> 00:37:55.980]   So let's quickly check the shapes.
[00:37:55.980 --> 00:37:58.540]   User factors, that's shape.
[00:37:58.540 --> 00:38:01.020]   Movie factors, that's shape.
[00:38:01.020 --> 00:38:05.340]   So you can see it's 944 by 5 and 1665 by 5
[00:38:05.340 --> 00:38:06.780]   because that's how many movies we have.
[00:38:06.780 --> 00:38:08.300]   All right.
[00:38:08.300 --> 00:38:13.260]   So now to calculate the result if a particular movie
[00:38:13.260 --> 00:38:16.060]   and between a particular movie and a user combination.
[00:38:16.060 --> 00:38:17.740]   What's the next thing we need to do?
[00:38:17.740 --> 00:38:24.140]   The next thing we need to do is we have to take a user--
[00:38:24.140 --> 00:38:24.780]   Oh, sorry.
[00:38:24.780 --> 00:38:29.740]   We have to take a user.
[00:38:29.740 --> 00:38:32.620]   That's not a good color for highlighting.
[00:38:32.620 --> 00:38:33.180]   One second.
[00:38:33.180 --> 00:38:39.980]   We have to take a user and we have to take the movie matrix
[00:38:39.980 --> 00:38:41.260]   and we have to multiply them.
[00:38:41.260 --> 00:38:41.580]   Right?
[00:38:42.620 --> 00:38:49.100]   So if my users are represented in this 944 by 5,
[00:38:49.100 --> 00:38:51.500]   which means my first user is the first item,
[00:38:51.500 --> 00:38:53.500]   my second user is the second item,
[00:38:53.500 --> 00:38:56.140]   and my third user and so on is the third item
[00:38:56.140 --> 00:38:59.340]   and the 944th user is this 944th.
[00:38:59.340 --> 00:39:03.580]   And similarly, for movies, I can represent my first movie
[00:39:03.580 --> 00:39:05.260]   and then I can represent my last movie.
[00:39:05.260 --> 00:39:07.660]   So it's just that's just the matrix.
[00:39:07.660 --> 00:39:11.660]   So if I want to check what if how much does the fifth user
[00:39:11.660 --> 00:39:13.420]   like the fifth movie?
[00:39:13.420 --> 00:39:17.820]   So I could just say user factors 4
[00:39:17.820 --> 00:39:20.620]   because that will index into the fifth item.
[00:39:20.620 --> 00:39:24.940]   So that's the representation of my fifth user.
[00:39:24.940 --> 00:39:29.260]   And similarly, movie factors 4.
[00:39:29.260 --> 00:39:30.380]   That's the representation.
[00:39:30.380 --> 00:39:33.740]   The second one is the representation of my fifth movie.
[00:39:33.740 --> 00:39:39.660]   Now I could multiply them and I could just calculate the sum.
[00:39:39.660 --> 00:39:41.900]   So that's how much is a score.
[00:39:41.900 --> 00:39:43.100]   That's how much.
[00:39:43.100 --> 00:39:47.180]   So that's like getting this prediction over here.
[00:39:47.180 --> 00:39:49.820]   We've just multiplied my user with the movie.
[00:39:49.820 --> 00:39:55.900]   So you can see how we've just done what we shown,
[00:39:55.900 --> 00:39:58.540]   what's being shown in this picture in code using Torch.
[00:39:58.540 --> 00:40:01.820]   So that's the idea.
[00:40:01.820 --> 00:40:06.940]   One thing though that I forgot to highlight
[00:40:06.940 --> 00:40:11.900]   that should be highlighted is in deep learning
[00:40:11.900 --> 00:40:18.460]   when we're using PyTorch and when we want to create
[00:40:18.460 --> 00:40:21.980]   like these user and movie matrices,
[00:40:21.980 --> 00:40:26.700]   I could say, oh, we want to match the fifth user
[00:40:26.700 --> 00:40:27.580]   to the fifth movie.
[00:40:27.580 --> 00:40:30.860]   So I could go user factors 4.
[00:40:30.860 --> 00:40:32.220]   So that gives me the fifth user.
[00:40:32.220 --> 00:40:34.940]   And I could go movie factors 4
[00:40:34.940 --> 00:40:36.220]   and that gives me the fifth movie.
[00:40:37.180 --> 00:40:39.260]   The deep learning algorithm on its way
[00:40:39.260 --> 00:40:41.900]   does not have a way to index.
[00:40:41.900 --> 00:40:46.220]   Like it does not know that like the way
[00:40:46.220 --> 00:40:47.980]   these deep learning algorithms
[00:40:47.980 --> 00:40:49.980]   or like the way these forward functions
[00:40:49.980 --> 00:40:52.060]   in PyTorch modules work is like
[00:40:52.060 --> 00:40:54.700]   you can pretty much do matrix multiplication.
[00:40:54.700 --> 00:40:57.980]   You can do your activation functions.
[00:40:57.980 --> 00:41:01.980]   So that's why what happens is like you can just represent things
[00:41:01.980 --> 00:41:03.500]   by using one hot end code.
[00:41:03.500 --> 00:41:04.780]   So what does that mean?
[00:41:04.780 --> 00:41:09.180]   If I have however, 944 users,
[00:41:09.180 --> 00:41:11.580]   I can just one hot encode my users.
[00:41:11.580 --> 00:41:15.340]   So this is just like saying, oh, okay.
[00:41:15.340 --> 00:41:18.860]   If I want to get the third user,
[00:41:18.860 --> 00:41:22.140]   so let's see, that's what's written here.
[00:41:22.140 --> 00:41:23.980]   So if I want to get the third user,
[00:41:23.980 --> 00:41:27.900]   I can just represent the number of users
[00:41:27.900 --> 00:41:29.100]   and I can one hot encode.
[00:41:29.100 --> 00:41:30.140]   So what does this give me?
[00:41:30.140 --> 00:41:31.340]   So let me show you the outputs.
[00:41:31.340 --> 00:41:33.980]   So you can see how if I do that,
[00:41:33.980 --> 00:41:39.020]   that's just one at basically,
[00:41:39.020 --> 00:41:41.180]   because this is index three,
[00:41:41.180 --> 00:41:42.540]   that would be the fourth item.
[00:41:42.540 --> 00:41:44.220]   This is how Python indexing works.
[00:41:44.220 --> 00:41:48.220]   Like zero is the first item and three is the fourth item.
[00:41:48.220 --> 00:41:51.740]   So if I'm going to go and check my fourth user,
[00:41:51.740 --> 00:41:53.740]   then I could one hot encode, right?
[00:41:53.740 --> 00:41:54.860]   I could do it like this.
[00:41:54.860 --> 00:41:56.060]   Oh, sorry. One second.
[00:41:56.060 --> 00:42:02.460]   And I could multiply this with my user factors.
[00:42:02.460 --> 00:42:08.060]   So that would still give me,
[00:42:08.060 --> 00:42:10.460]   oh, sorry, that's an add thread, not multiply.
[00:42:10.460 --> 00:42:11.500]   Matrix multiplication.
[00:42:11.500 --> 00:42:27.820]   User factors of T at that expected sale.
[00:42:27.820 --> 00:42:30.220]   Oh, cause I didn't say so.
[00:42:30.380 --> 00:42:30.880]   Okay.
[00:42:30.880 --> 00:42:34.780]   That should work.
[00:42:34.780 --> 00:42:37.740]   So this is just like the same as indexing.
[00:42:37.740 --> 00:42:41.580]   So this is just this idea of don't worry
[00:42:41.580 --> 00:42:43.900]   and don't like, you know, don't think,
[00:42:43.900 --> 00:42:45.180]   oh, I don't understand this.
[00:42:45.180 --> 00:42:47.100]   Is when I tell you is like,
[00:42:47.100 --> 00:42:50.300]   it's just another way of indexing the fourth item
[00:42:50.300 --> 00:42:51.660]   into my user factors.
[00:42:51.660 --> 00:42:54.940]   Like you can just index into the fourth item
[00:42:54.940 --> 00:42:58.540]   by doing this way of like using matrix multiplication,
[00:42:58.540 --> 00:43:00.140]   but it's easier to do it this way
[00:43:00.140 --> 00:43:02.620]   or like it's more suitable to do it this way.
[00:43:02.620 --> 00:43:05.820]   It's because that's how like these underneath models
[00:43:05.820 --> 00:43:06.940]   have been implemented.
[00:43:06.940 --> 00:43:09.820]   And then you can just pretty much grab the fourth item
[00:43:09.820 --> 00:43:12.060]   because matrix multiplication is really fast.
[00:43:12.060 --> 00:43:15.180]   So that's this idea of, from what I understand,
[00:43:15.180 --> 00:43:18.940]   like that's this idea of grabbing the fourth item
[00:43:18.940 --> 00:43:20.540]   using matrix multiplication.
[00:43:20.540 --> 00:43:21.900]   So you can just one hot encode.
[00:43:21.900 --> 00:43:23.440]   All right.
[00:43:23.440 --> 00:43:25.260]   So you'll see this.
[00:43:25.260 --> 00:43:27.980]   You'll see like all of that screen here.
[00:43:28.780 --> 00:43:30.460]   But one thing is like,
[00:43:30.460 --> 00:43:35.980]   when we're doing this movie and collaborative filtering
[00:43:35.980 --> 00:43:37.740]   and we're doing this movie recommendation,
[00:43:37.740 --> 00:43:44.540]   then this idea of like being able to like all of this,
[00:43:44.540 --> 00:43:48.140]   that for the user, all of this that we've called,
[00:43:48.140 --> 00:43:49.820]   we've called it a matrix,
[00:43:49.820 --> 00:43:51.980]   but it's a special kind of matrix.
[00:43:51.980 --> 00:43:52.540]   What is it?
[00:43:52.540 --> 00:43:53.740]   It's an embedding.
[00:43:56.220 --> 00:43:57.660]   An embedding.
[00:43:57.660 --> 00:44:00.780]   So what is an embedding?
[00:44:00.780 --> 00:44:03.820]   An embedding is just something you can index into.
[00:44:03.820 --> 00:44:08.140]   In PyTorch, if you see nn.embedding,
[00:44:08.140 --> 00:44:11.740]   you will see that this is just this idea of
[00:44:11.740 --> 00:44:13.180]   being able to represent things.
[00:44:13.180 --> 00:44:15.740]   So it's just like a simple lookup table
[00:44:15.740 --> 00:44:18.540]   that stores embeddings of a fixed dictionary of size.
[00:44:18.540 --> 00:44:19.500]   So what does that mean?
[00:44:19.500 --> 00:44:23.260]   It's like you can represent this as a lookup table, right?
[00:44:23.260 --> 00:44:26.380]   So if I want to go and I want to index,
[00:44:26.380 --> 00:44:30.380]   if I want to look up for user 4, which is in index 3,
[00:44:30.380 --> 00:44:33.180]   I could just represent that whole thing instead of like,
[00:44:33.180 --> 00:44:35.740]   let me show it in code.
[00:44:35.740 --> 00:44:43.980]   So instead of representing my user and I could just say,
[00:44:43.980 --> 00:44:46.540]   my user and movies, I could just say nn.embedding.
[00:44:46.540 --> 00:44:49.260]   So that becomes my user factor
[00:44:49.260 --> 00:44:50.700]   and then I could just grab the third item.
[00:44:50.700 --> 00:44:51.900]   It's not...
[00:44:51.900 --> 00:44:53.020]   Sorry. I've made a mistake.
[00:44:53.660 --> 00:45:03.660]   Hmm.
[00:45:03.660 --> 00:45:04.540]   That should have worked.
[00:45:04.540 --> 00:45:13.500]   Let's see.
[00:45:13.500 --> 00:45:15.020]   Toss a random, toss a tensor.
[00:45:15.020 --> 00:45:16.780]   Forget my embedding.
[00:45:16.780 --> 00:45:19.820]   Embedding.weight.
[00:45:20.780 --> 00:45:27.100]   So then that becomes my...
[00:45:27.100 --> 00:45:30.060]   So that's just a same way of doing things.
[00:45:30.060 --> 00:45:32.940]   Anyway, so that's just a bit of a side note.
[00:45:32.940 --> 00:45:36.060]   It's like these are what this way of like representing
[00:45:36.060 --> 00:45:38.140]   my users like this and my movies like this.
[00:45:38.140 --> 00:45:42.780]   This whole matrix is what is called an embedding matrix.
[00:45:42.780 --> 00:45:45.180]   So if you see this word embedding, don't worry.
[00:45:45.180 --> 00:45:45.980]   This is what it is.
[00:45:46.700 --> 00:45:51.020]   So it's just a special layer that does indexing
[00:45:51.020 --> 00:45:53.580]   into this matrix using an integer.
[00:45:53.580 --> 00:46:00.380]   And then because the last thing that should be looked into
[00:46:00.380 --> 00:46:03.420]   is that because we want to learn these things, right?
[00:46:03.420 --> 00:46:07.820]   And embedding, as we saw, if I show that,
[00:46:07.820 --> 00:46:10.940]   as you can see, an embedding has a gradient function.
[00:46:10.940 --> 00:46:11.740]   What does that mean?
[00:46:11.740 --> 00:46:16.460]   It means that now because there's a gradient function over here,
[00:46:16.460 --> 00:46:21.020]   that just means that PyTorch can train these embeddings, right?
[00:46:21.020 --> 00:46:23.820]   So we want to learn these latent factors,
[00:46:23.820 --> 00:46:26.940]   which means we have to represent them as embeddings,
[00:46:26.940 --> 00:46:28.780]   which means because there's a gradient function
[00:46:28.780 --> 00:46:30.380]   on these embeddings, that just means
[00:46:30.380 --> 00:46:32.060]   that PyTorch can learn these embeddings.
[00:46:32.060 --> 00:46:35.900]   So as you can see, that's just this main idea.
[00:46:35.900 --> 00:46:41.820]   And that's pretty much that's all being said
[00:46:41.820 --> 00:46:43.260]   in this part of the section.
[00:46:43.260 --> 00:46:46.300]   It's like you can represent things by using
[00:46:46.300 --> 00:46:47.980]   an end-of-embeddings as well.
[00:46:47.980 --> 00:46:51.500]   So I'll just see if there's any questions.
[00:46:51.500 --> 00:46:55.340]   How to design?
[00:46:55.340 --> 00:46:56.860]   How many latent factors do we need?
[00:46:56.860 --> 00:46:57.820]   That's a great question.
[00:46:57.820 --> 00:47:00.860]   There's no particular answer.
[00:47:00.860 --> 00:47:03.900]   There's like there's no rule of thumb that I know of,
[00:47:03.900 --> 00:47:06.460]   but think of it this way.
[00:47:06.460 --> 00:47:10.700]   If you have like if you are going to represent
[00:47:11.340 --> 00:47:15.020]   a more complex thing, then you would need
[00:47:15.020 --> 00:47:17.980]   a big number of latent factors.
[00:47:17.980 --> 00:47:19.900]   So what does that mean?
[00:47:19.900 --> 00:47:21.660]   So if you're trying to represent something
[00:47:21.660 --> 00:47:25.340]   that's really, really complex, like in English,
[00:47:25.340 --> 00:47:28.220]   and this idea of like being able to represent things
[00:47:28.220 --> 00:47:33.980]   as numbers or as embeddings is not just used
[00:47:33.980 --> 00:47:37.580]   in this part of like collaborative filtering
[00:47:37.580 --> 00:47:38.700]   or just for movies.
[00:47:38.700 --> 00:47:41.660]   It's also very much used in natural language processing.
[00:47:41.660 --> 00:47:44.220]   So words can be represented by embeddings as well.
[00:47:44.220 --> 00:47:45.420]   So if you think of it that way.
[00:47:45.420 --> 00:47:48.620]   So like if you're trying to represent
[00:47:48.620 --> 00:47:53.180]   a whole vocabulary of words as numbers,
[00:47:53.180 --> 00:47:57.100]   then because the vocabulary or English vocabulary
[00:47:57.100 --> 00:48:00.780]   is really massive, then you need like a bigger
[00:48:00.780 --> 00:48:04.620]   embedding matrix size, which is like the typical number 768
[00:48:04.620 --> 00:48:07.820]   or like that's just the number of latent factors.
[00:48:07.820 --> 00:48:10.220]   But if you're trying to represent something really small,
[00:48:10.220 --> 00:48:13.180]   in which case, which is movies and users,
[00:48:13.180 --> 00:48:14.700]   and it's not really very complex,
[00:48:14.700 --> 00:48:16.220]   and it's like less number of movies
[00:48:16.220 --> 00:48:18.460]   and less number of users, in that case,
[00:48:18.460 --> 00:48:21.020]   you can say like an embedding matrix could be 50.
[00:48:21.020 --> 00:48:24.460]   But there's no straight answer that I have
[00:48:24.460 --> 00:48:26.220]   for like how many latent factors do we need.
[00:48:26.220 --> 00:48:30.060]   But I hope what I've said is like it depends
[00:48:30.060 --> 00:48:31.980]   on the complexity, like the more complex thing
[00:48:31.980 --> 00:48:33.980]   you're trying to represent, the more number
[00:48:33.980 --> 00:48:35.260]   of latent factors you would need.
[00:48:36.380 --> 00:48:39.660]   So the next thing is let's say a restaurant predicting food
[00:48:39.660 --> 00:48:41.020]   on customer's preference.
[00:48:41.020 --> 00:48:46.220]   What will be used to predict for customer
[00:48:46.220 --> 00:48:48.060]   similar to rating for movies?
[00:48:48.060 --> 00:48:51.500]   Well, for if you let's say for a restaurant
[00:48:51.500 --> 00:48:53.900]   for predicting food based on customer preference.
[00:48:53.900 --> 00:48:57.820]   Yeah. So you could have customers rate
[00:48:57.820 --> 00:49:00.060]   like different kinds of food, right?
[00:49:00.060 --> 00:49:02.540]   So then your food becomes the movies
[00:49:02.540 --> 00:49:04.220]   and your customers remain the customers.
[00:49:04.220 --> 00:49:06.220]   So then now you know which customer likes
[00:49:06.220 --> 00:49:07.180]   what kind of food.
[00:49:07.180 --> 00:49:10.060]   So then that restaurant, whichever is doing the prediction,
[00:49:10.060 --> 00:49:12.380]   then that restaurant can find users
[00:49:12.380 --> 00:49:13.980]   that would like particular type of food.
[00:49:13.980 --> 00:49:15.100]   So I hope that makes sense.
[00:49:15.100 --> 00:49:19.100]   But that's just, but this idea of like having recommendations
[00:49:19.100 --> 00:49:22.060]   not just useful to movies, but it can be used
[00:49:22.060 --> 00:49:23.580]   in like any other kind of problem.
[00:49:23.580 --> 00:49:25.420]   So that's that.
[00:49:25.420 --> 00:49:29.020]   So now let's do our collaborative filtering from scratch.
[00:49:29.020 --> 00:49:31.900]   One thing we haven't done or should be like,
[00:49:31.900 --> 00:49:33.500]   I'm assuming everybody would know
[00:49:33.500 --> 00:49:35.340]   is like object oriented programming.
[00:49:35.340 --> 00:49:38.540]   So in Python, you have things as classes,
[00:49:38.540 --> 00:49:40.620]   you have Thunder units, which is your definitions
[00:49:40.620 --> 00:49:41.740]   and you can have methods.
[00:49:41.740 --> 00:49:45.020]   I won't go into the details of this,
[00:49:45.020 --> 00:49:47.420]   this pretty much written here,
[00:49:47.420 --> 00:49:49.820]   like a very small blurb that's been written here,
[00:49:49.820 --> 00:49:51.980]   but you can have a class,
[00:49:51.980 --> 00:49:54.220]   then you can have an instance of that class
[00:49:54.220 --> 00:49:56.860]   and you can call a method on that.
[00:49:56.860 --> 00:50:00.380]   So I won't go into the much details
[00:50:00.380 --> 00:50:02.460]   on like what this object oriented programming is.
[00:50:03.180 --> 00:50:06.620]   Again, that's what's been mentioned in the book as well
[00:50:06.620 --> 00:50:08.940]   is that we would recommend looking up for a tutorial
[00:50:08.940 --> 00:50:11.420]   and getting more practice before moving on.
[00:50:11.420 --> 00:50:14.060]   But I will tell you the main idea is like
[00:50:14.060 --> 00:50:17.820]   if I'm trying to create now my model,
[00:50:17.820 --> 00:50:21.660]   so far, we haven't really created models from scratch, right?
[00:50:21.660 --> 00:50:22.940]   So if I want to create this,
[00:50:22.940 --> 00:50:30.300]   if I want to create a model that can do movie recommendation,
[00:50:30.300 --> 00:50:31.420]   then what would I need?
[00:50:31.420 --> 00:50:32.620]   I would need two things.
[00:50:32.620 --> 00:50:36.220]   I would need my user matrix, which is called an embedding
[00:50:36.220 --> 00:50:38.620]   and I would need my movie matrix,
[00:50:38.620 --> 00:50:40.780]   which is again a movie embedding, right?
[00:50:40.780 --> 00:50:42.540]   Those are the two things that I would need.
[00:50:42.540 --> 00:50:43.180]   So let's see.
[00:50:43.180 --> 00:50:45.580]   That's what's happening here is like
[00:50:45.580 --> 00:50:48.140]   I add my user factors as an embedding.
[00:50:48.140 --> 00:50:50.220]   So as you can see, now, this is an embedding.
[00:50:50.220 --> 00:50:50.940]   What does that mean?
[00:50:50.940 --> 00:50:52.380]   It means it can be learned.
[00:50:52.380 --> 00:50:56.140]   So these latent factors, they can be learned.
[00:50:56.140 --> 00:50:59.500]   And then what's the size of my user embedding going to be?
[00:50:59.500 --> 00:51:01.900]   It's going to be however many users I have.
[00:51:01.900 --> 00:51:04.860]   So number of users times number of factors.
[00:51:04.860 --> 00:51:08.940]   So if I have like 944 users, then it's going to be 944
[00:51:08.940 --> 00:51:10.460]   times however many factors I choose.
[00:51:10.460 --> 00:51:12.060]   It could be 5, it could be 50.
[00:51:12.060 --> 00:51:14.140]   So that's just the number of latent factors.
[00:51:14.140 --> 00:51:17.260]   Similarly, for my movies, I can define this embedding
[00:51:17.260 --> 00:51:22.220]   and then I can just say what's my embedding size going to be.
[00:51:22.220 --> 00:51:24.940]   It's going to be however many movies I have
[00:51:24.940 --> 00:51:26.220]   times the number of factors.
[00:51:26.220 --> 00:51:27.660]   So I can just store it in cells.
[00:51:27.660 --> 00:51:29.500]   So those are the two things that become.
[00:51:29.500 --> 00:51:32.300]   And then in PyTorch, when you're defining custom models,
[00:51:32.300 --> 00:51:37.420]   so defining-- I just want to show you PyTorch NN module.
[00:51:37.420 --> 00:51:40.140]   So these are the things you would have to look at if you
[00:51:40.140 --> 00:51:43.180]   want to understand everything that's going on over here.
[00:51:43.180 --> 00:51:46.300]   It's like, go have a look at what this means.
[00:51:46.300 --> 00:51:48.140]   It's like Torch.NN module.
[00:51:48.140 --> 00:51:52.780]   In PyTorch, everything is an instance of this NN.module.
[00:51:52.780 --> 00:51:57.100]   And then you pretty much define a forward method,
[00:51:57.100 --> 00:51:59.180]   which defines what this model does.
[00:51:59.500 --> 00:51:59.740]   OK.
[00:51:59.740 --> 00:52:04.780]   So if I want to get the predictions, I can grab the user.
[00:52:04.780 --> 00:52:06.620]   I can grab the movie.
[00:52:06.620 --> 00:52:10.300]   So I can pretty much grab this user in red
[00:52:10.300 --> 00:52:12.300]   and I can grab the movie over here.
[00:52:12.300 --> 00:52:15.100]   And then I can just multiply it and sum it.
[00:52:15.100 --> 00:52:16.940]   And that would give my prediction score.
[00:52:16.940 --> 00:52:21.580]   But in this case, when I was doing my data loader--
[00:52:21.580 --> 00:52:22.780]   so let's do that--
[00:52:22.780 --> 00:52:24.460]   I can grab my data loader.
[00:52:24.460 --> 00:52:28.780]   You can see I have my data loader shape is path size of 64.
[00:52:28.780 --> 00:52:30.460]   And then it has two columns.
[00:52:30.460 --> 00:52:31.740]   So let's go back.
[00:52:31.740 --> 00:52:34.140]   And let me show you what was in the data loader.
[00:52:34.140 --> 00:52:35.020]   So you can see how--
[00:52:35.020 --> 00:52:44.780]   I want to actually explain what goes on in the data loader.
[00:52:44.780 --> 00:52:51.420]   I shouldn't have to, but I will still do it.
[00:52:53.420 --> 00:53:01.100]   In my data loader, my number of rows--
[00:53:01.100 --> 00:53:02.300]   sorry, that's not a good color.
[00:53:02.300 --> 00:53:11.660]   I don't know why my one node is playing up.
[00:53:11.660 --> 00:53:15.500]   Won't let me select the--
[00:53:15.500 --> 00:53:19.500]   All right.
[00:53:19.500 --> 00:53:20.540]   Let me restart this app.
[00:53:20.700 --> 00:53:26.700]   [INAUDIBLE]
[00:53:26.700 --> 00:53:27.660]   Does that work?
[00:53:27.660 --> 00:53:28.220]   That works.
[00:53:28.220 --> 00:53:30.060]   Yeah, that works.
[00:53:30.060 --> 00:53:30.300]   Cool.
[00:53:30.300 --> 00:53:36.780]   So in my data loader, basically, the number of rows
[00:53:36.780 --> 00:53:40.380]   becomes my batch size, right?
[00:53:40.380 --> 00:53:41.820]   And then I have two columns.
[00:53:41.820 --> 00:53:45.180]   My first column is for my user.
[00:53:45.180 --> 00:53:47.980]   And my second column is for my movies, right?
[00:53:47.980 --> 00:53:52.860]   So that's why if I go and have a look at--
[00:53:52.860 --> 00:53:54.700]   where is this?
[00:53:54.700 --> 00:54:00.940]   If I grab my first batch from my data loader, which is x and y,
[00:54:00.940 --> 00:54:09.740]   I can see that the x dot shape is 64 by 2.
[00:54:09.740 --> 00:54:15.820]   And then the y dot shape is 64, which means I grab 64 users
[00:54:15.820 --> 00:54:17.100]   in my data loader.
[00:54:17.100 --> 00:54:23.500]   I grab 64 movies from my data loader and I have 64 predictions
[00:54:23.500 --> 00:54:27.180]   or scores or basically ratings which becomes my y.
[00:54:27.180 --> 00:54:30.700]   So my x is two columns, right?
[00:54:30.700 --> 00:54:32.780]   Which is my user and my movie.
[00:54:32.780 --> 00:54:35.260]   So the first column becomes--
[00:54:35.260 --> 00:54:43.180]   so this x over here, the first column, if I go and grab all
[00:54:43.180 --> 00:54:46.620]   the rows and I grab the first column, you can see like these
[00:54:46.620 --> 00:54:51.340]   are the user indexes because the first column is the user, right?
[00:54:51.340 --> 00:54:56.220]   And then similarly, I can grab my second column.
[00:54:56.220 --> 00:55:01.420]   So these becomes the movie IDs and then I can see my y's.
[00:55:01.420 --> 00:55:03.260]   So you can see these become the scores.
[00:55:03.260 --> 00:55:05.020]   So how do I interpret this?
[00:55:05.020 --> 00:55:12.220]   User 677 saw the movie 1038 ID and gave a score or rating of 1.
[00:55:13.020 --> 00:55:18.700]   User 655 saw the movie 699 and gave a score of 3.
[00:55:18.700 --> 00:55:25.020]   User 178 saw the movie 1174 and gave a score of 4.
[00:55:25.020 --> 00:55:25.340]   Okay.
[00:55:25.340 --> 00:55:28.620]   So that's how I would interpret my data loader.
[00:55:28.620 --> 00:55:35.660]   So as you can see, now what gets passed to my model is like this data loader.
[00:55:35.660 --> 00:55:40.460]   So I can grab all my users by indexing into my user matrix,
[00:55:40.460 --> 00:55:42.220]   passing in the user IDs.
[00:55:42.220 --> 00:55:47.020]   Remember, x0 was just my user IDs, right?
[00:55:47.020 --> 00:55:49.820]   So I can grab my, what does this mean?
[00:55:49.820 --> 00:56:01.340]   So my user IDs were, say if my user IDs were these ones, 932, 389, 360,
[00:56:01.340 --> 00:56:07.900]   then I could go into this embedding matrix, which is this massive long embedding matrix,
[00:56:07.900 --> 00:56:12.220]   and I can grab the elements of 360, 980, and so on.
[00:56:12.220 --> 00:56:17.740]   So these are my particular users that I want to grab.
[00:56:17.740 --> 00:56:20.140]   And then I can just multiply these users with my movies,
[00:56:20.140 --> 00:56:21.260]   and I can grab the scores.
[00:56:21.260 --> 00:56:26.780]   I wouldn't have gone into the details of how like high-dose data loaders work
[00:56:26.780 --> 00:56:31.500]   or how this module work, but I just still did want to give you guys
[00:56:31.500 --> 00:56:34.620]   a glimpse of like what exactly is going on over here.
[00:56:35.340 --> 00:56:41.340]   So if this part, just in case like what I've explained so far doesn't make sense,
[00:56:41.340 --> 00:56:44.780]   you want to spend some more time on high-dose data loaders,
[00:56:44.780 --> 00:56:48.460]   and you want to spend some more time on high-dose nn.module.
[00:56:48.460 --> 00:56:55.260]   So then because this is my model, which is just multiplying my user latent factors
[00:56:55.260 --> 00:56:58.300]   with my movie latent factors, which is happening here,
[00:56:58.300 --> 00:57:02.140]   I can just define my model like that, where my number of users is n users,
[00:57:02.140 --> 00:57:05.260]   number of movies is n movies, and latent factors is 50.
[00:57:05.260 --> 00:57:07.500]   Then I can just use the PyTorch data loader.
[00:57:07.500 --> 00:57:08.940]   Sorry, one second.
[00:57:08.940 --> 00:57:14.780]   Then I can just use the PyTorch learner passing in these data loaders.
[00:57:14.780 --> 00:57:20.380]   And I can call fit one cycle.
[00:57:20.380 --> 00:57:24.140]   So you can see now this model is going to start to learn.
[00:57:24.140 --> 00:57:27.660]   Like it's going to start those, what the model is particularly learning,
[00:57:27.660 --> 00:57:29.740]   it's learning the latencies.
[00:57:29.740 --> 00:57:32.860]   It's learning these latent factors and it's learning these latent factors.
[00:57:32.860 --> 00:57:37.980]   So it's learning latent factors about movies and it's learning latent factors about users.
[00:57:37.980 --> 00:57:41.980]   So you can see how now my valid loss is going down.
[00:57:41.980 --> 00:57:45.260]   It started at 1.27 and it's going down to 1.12.
[00:57:45.260 --> 00:57:51.740]   Sorry, one second.
[00:57:57.660 --> 00:58:00.540]   Okay. Let's see if there's any questions.
[00:58:00.540 --> 00:58:10.540]   We haven't talked about PCA, Kevin.
[00:58:10.540 --> 00:58:12.940]   That's coming later.
[00:58:12.940 --> 00:58:15.420]   So now that's just my model, right?
[00:58:15.420 --> 00:58:21.980]   I do hope like I'm trying my best to make sure that we all understand
[00:58:21.980 --> 00:58:26.460]   how this works in PyTorch, but there's only so much that I can do
[00:58:26.460 --> 00:58:29.420]   is like try and explain only so much of the code.
[00:58:29.420 --> 00:58:31.180]   The rest of that would come from practice.
[00:58:31.180 --> 00:58:33.820]   So the more you write this code, the more you understand.
[00:58:33.820 --> 00:58:35.820]   But I can explain like the main concepts.
[00:58:35.820 --> 00:58:38.540]   So like I can explain the main things that are happening in that,
[00:58:38.540 --> 00:58:40.380]   in these particular pieces of code.
[00:58:40.380 --> 00:58:42.940]   So now if you want to make our model better, right?
[00:58:42.940 --> 00:58:47.900]   Right now, all we've done is like we took our user embedding,
[00:58:47.900 --> 00:58:50.780]   we took our movie embedding, we multiplied between them
[00:58:50.780 --> 00:58:52.460]   and we got all these scores.
[00:58:52.460 --> 00:58:55.660]   Then we calculated the loss and that's how our model is training, right?
[00:58:55.660 --> 00:58:58.940]   But we haven't done anything to like improve this model
[00:58:58.940 --> 00:59:00.620]   or make this model any better.
[00:59:00.620 --> 00:59:02.540]   So if you want to make this model any better,
[00:59:02.540 --> 00:59:05.980]   one thing we can do is we can make sure that
[00:59:05.980 --> 00:59:09.340]   the predictions are between range zero and five, right?
[00:59:09.340 --> 00:59:12.700]   Because when you multiply two matrices or you multiply two vectors,
[00:59:12.700 --> 00:59:17.340]   the numbers or the output could be any value
[00:59:17.340 --> 00:59:22.060]   because like we don't control what the values of this embedding matrix are.
[00:59:22.060 --> 00:59:26.140]   So the one thing we can do is like we can use the sigma range function.
[00:59:26.140 --> 00:59:28.140]   We saw the sigma range function in the,
[00:59:28.140 --> 00:59:30.220]   I think that was the second or the third chapter.
[00:59:30.220 --> 00:59:35.660]   It pretty much makes things be in the range that we want it to be.
[00:59:35.660 --> 00:59:38.940]   So we can make all predictions be in the range zero and five
[00:59:38.940 --> 00:59:40.300]   and you can train a model again.
[00:59:40.300 --> 00:59:42.700]   So you can see how that's happening.
[00:59:42.700 --> 00:59:47.900]   Finally, then what you want to do is
[00:59:47.900 --> 00:59:51.900]   we can try and improve this model even a bit more.
[00:59:52.540 --> 00:59:55.820]   So if you want to improve this model even more,
[00:59:55.820 --> 01:00:00.300]   let me again copy paste that nice image.
[01:00:00.300 --> 01:00:10.780]   All right.
[01:00:10.780 --> 01:00:13.820]   So if you want to improve this model even more,
[01:00:13.820 --> 01:00:18.380]   something we haven't really considered so far is like
[01:00:20.060 --> 01:00:25.660]   some users are just, they hate to give good ratings, right?
[01:00:25.660 --> 01:00:30.140]   Like even if a user likes the movie,
[01:00:30.140 --> 01:00:33.180]   that user would give a bad rating.
[01:00:33.180 --> 01:00:35.500]   Like some users are like that, right?
[01:00:35.500 --> 01:00:38.300]   Some users go to a restaurant, they love the food,
[01:00:38.300 --> 01:00:40.700]   but when it comes to giving rating to that restaurant,
[01:00:40.700 --> 01:00:43.180]   they'll still rate it eight but not 10.
[01:00:43.180 --> 01:00:48.780]   And that's something that unfortunately is very, that's out there.
[01:00:48.780 --> 01:00:51.660]   And then some users like to give good ratings.
[01:00:51.660 --> 01:00:53.820]   Like some users would hate the restaurant
[01:00:53.820 --> 01:00:55.820]   or some users would hate the movie,
[01:00:55.820 --> 01:00:58.380]   but they would still say, oh, I love the movie.
[01:00:58.380 --> 01:00:59.900]   And the rating is like 10.
[01:00:59.900 --> 01:01:01.020]   So you know what I mean?
[01:01:01.020 --> 01:01:04.620]   Like that's just the personality of that user.
[01:01:04.620 --> 01:01:07.740]   And this particular thing is called as bias.
[01:01:07.740 --> 01:01:12.380]   So one thing we can do is like for each of the users,
[01:01:12.380 --> 01:01:15.180]   I can add this one number.
[01:01:15.180 --> 01:01:18.220]   So which is called bias.
[01:01:18.940 --> 01:01:25.420]   And similarly, I can add a bias for each of my movies.
[01:01:25.420 --> 01:01:26.780]   So on.
[01:01:26.780 --> 01:01:34.300]   What this bias means is like even if a user likes action movies,
[01:01:34.300 --> 01:01:38.620]   even if a user likes a musical movies
[01:01:38.620 --> 01:01:41.660]   and then the movie is everything that the user likes,
[01:01:41.660 --> 01:01:46.700]   in even then the user is going to give the movie a bad rating.
[01:01:46.700 --> 01:01:48.460]   Similarly, when it comes to the movies,
[01:01:48.460 --> 01:01:54.140]   even if the like the movie is has everything that the user wants,
[01:01:54.140 --> 01:01:57.980]   the movie is still this like the direction of it
[01:01:57.980 --> 01:02:00.300]   is just the way the camera was used.
[01:02:00.300 --> 01:02:04.060]   Even if there's like a lot of music,
[01:02:04.060 --> 01:02:05.980]   maybe the music is of a different kind.
[01:02:05.980 --> 01:02:08.140]   All I'm trying to say is like it's still possible
[01:02:08.140 --> 01:02:10.220]   that the movie is not a good movie, right?
[01:02:10.220 --> 01:02:12.780]   So we have to consider this idea of bias.
[01:02:14.460 --> 01:02:18.380]   For both our users and for both users and movies.
[01:02:18.380 --> 01:02:19.740]   And that's what we do next.
[01:02:19.740 --> 01:02:28.140]   We add this new model, which we call dot product bias.
[01:02:28.140 --> 01:02:30.540]   We define a user bias.
[01:02:30.540 --> 01:02:31.980]   We define a movie bias.
[01:02:31.980 --> 01:02:34.860]   And then when we're calculating our result,
[01:02:34.860 --> 01:02:39.260]   we just add the user bias and we add the movie bias to our final result.
[01:02:39.260 --> 01:02:41.500]   And now our result is still in the C point range.
[01:02:41.500 --> 01:02:43.580]   So we can still train this model.
[01:02:44.540 --> 01:02:45.740]   So that's what happens.
[01:02:45.740 --> 01:02:48.380]   That's that.
[01:02:48.380 --> 01:02:49.420]   I mean, that's just the...
[01:02:49.420 --> 01:02:54.140]   That's just what we've done so far.
[01:02:54.140 --> 01:02:55.660]   The next thing we want to do is like
[01:02:55.660 --> 01:02:58.860]   if you want to make things even better
[01:02:58.860 --> 01:03:04.140]   or we want to like we want to now still make this model better.
[01:03:04.140 --> 01:03:07.340]   There's this idea called weight decay that we can add.
[01:03:07.340 --> 01:03:08.780]   It's a very general idea.
[01:03:08.780 --> 01:03:12.140]   It's not specific to collaborative filtering at all.
[01:03:12.140 --> 01:03:18.380]   For weight decay, you can add weight decay to any of the models that you've trained so far.
[01:03:18.380 --> 01:03:22.300]   So even in Cassava, if you feel like your model is overfitting,
[01:03:22.300 --> 01:03:23.660]   try adding weight decay to it.
[01:03:23.660 --> 01:03:26.140]   Now, what is this weight decay?
[01:03:26.140 --> 01:03:30.860]   In weight decay, what you do is like you add the parameters of the model
[01:03:30.860 --> 01:03:33.660]   and you add the square of those parameters
[01:03:33.660 --> 01:03:36.700]   or the square of those weights to your loss function.
[01:03:36.700 --> 01:03:37.580]   So what does that mean?
[01:03:37.580 --> 01:03:39.500]   What's that going to do?
[01:03:40.300 --> 01:03:41.660]   What's that going to do is like...
[01:03:41.660 --> 01:03:47.020]   Let me actually just explain what I've just said.
[01:03:47.020 --> 01:03:49.100]   So I have my...
[01:03:49.100 --> 01:03:54.620]   This is my model and this model has some weights, right?
[01:03:54.620 --> 01:03:59.340]   So let's say my loss so far is binary cross entropy.
[01:03:59.340 --> 01:04:04.300]   What I'm going to do is I'm going to add the square of these weights to my loss function
[01:04:04.300 --> 01:04:08.940]   and I'm going to multiply that by say some factors of 0.01.
[01:04:08.940 --> 01:04:12.860]   So it's going to be 0.99 times binary cross entropy
[01:04:12.860 --> 01:04:15.500]   and it's going to be 0.01 times the weight squared.
[01:04:15.500 --> 01:04:22.220]   This idea of like adding the weights squared to the loss is called as weight decay.
[01:04:22.220 --> 01:04:27.660]   And the reason we want to do that is like if there's higher weights,
[01:04:27.660 --> 01:04:31.260]   then that just means that your parabola or like your loss function
[01:04:31.260 --> 01:04:34.300]   is going to have very sharp curves.
[01:04:34.300 --> 01:04:37.180]   And we don't want our parabola or we don't want our...
[01:04:37.180 --> 01:04:40.300]   Basically, we don't own our loss function to have very sharp curves.
[01:04:40.300 --> 01:04:43.900]   So it's like if you see this parabola and if the value is like...
[01:04:43.900 --> 01:04:47.100]   Because the parabola is represented by Ax squared plus Bx plus C,
[01:04:47.100 --> 01:04:49.260]   that's like just the equation of the parabola.
[01:04:49.260 --> 01:04:56.380]   If the weights are really high, then instead of the loss being really flat,
[01:04:56.380 --> 01:04:59.740]   it just the curvature becomes really, really sharp.
[01:04:59.740 --> 01:05:02.540]   And then that becomes really hard for the model to frame.
[01:05:02.540 --> 01:05:05.100]   So the idea behind adding weight decay is like
[01:05:05.100 --> 01:05:08.140]   we keep the model parameters to be a small value.
[01:05:08.140 --> 01:05:11.820]   And then by making sure that the model parameters are small value,
[01:05:11.820 --> 01:05:14.700]   we are kind of making sure that a loss function is smooth
[01:05:14.700 --> 01:05:16.540]   and that we're able to train our network.
[01:05:16.540 --> 01:05:19.660]   So in FastAI, if you want to add weight decay,
[01:05:19.660 --> 01:05:21.260]   you just say to my learner,
[01:05:21.260 --> 01:05:24.540]   like I can still use the same dot product bias model that we have.
[01:05:24.540 --> 01:05:27.500]   I could just say WD equals 0.1.
[01:05:27.500 --> 01:05:31.820]   And then I can just create my learner and I can just pick one cycle.
[01:05:31.820 --> 01:05:34.860]   And you can see then the valid loss was actually much lower
[01:05:34.860 --> 01:05:36.300]   than what we got over here.
[01:05:36.300 --> 01:05:38.140]   So we got 0.89 valid loss.
[01:05:38.140 --> 01:05:43.500]   But in the previous run, the valid loss after adding weight decay was like 0.82.
[01:05:43.500 --> 01:05:46.460]   So that's just this idea of like adding weight decay.
[01:05:46.460 --> 01:05:54.780]   Next step is we're using this idea of like embedding.
[01:05:54.780 --> 01:05:56.940]   So what we're going to do now in this is like
[01:05:56.940 --> 01:06:01.260]   this is again digging deep more into PyTorch.
[01:06:01.260 --> 01:06:03.500]   And what we're going to do in part of this is like
[01:06:03.500 --> 01:06:05.500]   we're going to create our own embedding matrix.
[01:06:05.500 --> 01:06:09.020]   But I'll take a break and like just look at some questions
[01:06:09.020 --> 01:06:11.900]   if there are any questions.
[01:06:11.900 --> 01:06:12.780]   So no questions.
[01:06:12.780 --> 01:06:21.200]   All right.
[01:06:21.200 --> 01:06:24.460]   So I'll continue with the embedding module.
[01:06:24.460 --> 01:06:30.300]   So when it comes to creating our own embedding modules,
[01:06:31.260 --> 01:06:39.180]   remember with an embedding matrix,
[01:06:39.180 --> 01:06:42.700]   what I want to do is like this becomes my user embedding matrix.
[01:06:42.700 --> 01:06:48.620]   And then this at the top becomes my movie embedding matrix, right?
[01:06:48.620 --> 01:06:57.100]   And I already told you that the difference between an embedding matrix
[01:06:57.100 --> 01:07:02.860]   and just a standard matrix is the fact that embedding matrix can be learned.
[01:07:02.860 --> 01:07:06.380]   Sorry, one second.
[01:07:06.380 --> 01:07:16.940]   So the fact that the embedding matrix can be learned, right?
[01:07:16.940 --> 01:07:21.660]   And in PyTorch, if you want to make things,
[01:07:21.660 --> 01:07:25.340]   if you want to define things such that they can be learned,
[01:07:25.340 --> 01:07:28.380]   in PyTorch, you want to define them to be parameters.
[01:07:28.380 --> 01:07:35.100]   So if you see how I have my, if I can define like my PyTorch module as,
[01:07:35.100 --> 01:07:40.780]   I just call my PyTorch module st and I define some variable or like some,
[01:07:40.780 --> 01:07:46.300]   sorry, some self.a which is not a parameter.
[01:07:46.300 --> 01:07:48.780]   And I just assign that to my model.
[01:07:48.780 --> 01:07:50.780]   So this is just a dummy model.
[01:07:50.780 --> 01:07:54.620]   You can see that the value of parameters of this model is zero.
[01:07:54.620 --> 01:07:56.860]   There's no such thing in the parameters.
[01:07:56.860 --> 01:07:59.500]   And in PyTorch, when there's nothing in the parameters,
[01:07:59.500 --> 01:08:03.420]   that just means that the parameters can't be trained.
[01:08:03.420 --> 01:08:07.100]   So, I mean, there's no training going to happen inside the model.
[01:08:07.100 --> 01:08:11.500]   So I want to show you and then you want to have a look at,
[01:08:11.500 --> 01:08:13.100]   for this part of the section,
[01:08:13.100 --> 01:08:16.780]   you want to go back and have a look at PyTorch nn.parameter.
[01:08:16.780 --> 01:08:19.340]   So if you want to define things such that they are trained,
[01:08:19.340 --> 01:08:22.140]   like if I want to define my embedding matrix over here,
[01:08:22.140 --> 01:08:25.660]   and if I want to define my movie embedding matrix over here,
[01:08:25.660 --> 01:08:27.340]   and I want to make sure that they are trained
[01:08:27.340 --> 01:08:29.820]   and that the model learns their values on their own,
[01:08:29.820 --> 01:08:32.780]   instead of defining them like nn.embedding,
[01:08:32.780 --> 01:08:34.940]   I could just say nn.parameter.
[01:08:34.940 --> 01:08:39.500]   So anytime I define something instead of it being torched at once,
[01:08:39.500 --> 01:08:42.460]   anytime I wrap something in nn.parameter,
[01:08:42.460 --> 01:08:46.380]   PyTorch knows that that's the one thing I have to train
[01:08:46.380 --> 01:08:47.980]   or that's the one thing I have to learn.
[01:08:48.700 --> 01:08:54.140]   So you can see now, I can see that if I wrap this self.a in nn.parameter,
[01:08:54.140 --> 01:08:57.980]   you can see that now it is a tensor, not only a tensor,
[01:08:57.980 --> 01:09:00.860]   but it also says requires grad equals two,
[01:09:00.860 --> 01:09:03.820]   which means that when we're doing stochastic gradient descent
[01:09:03.820 --> 01:09:05.740]   or when we're calculating the gradients,
[01:09:05.740 --> 01:09:09.180]   the gradients of this parameter self.a would be calculated
[01:09:09.180 --> 01:09:10.140]   and that the model,
[01:09:10.140 --> 01:09:14.220]   that this is the thing that will be learned inside the model, right?
[01:09:14.220 --> 01:09:15.980]   So if you want to create our own,
[01:09:17.500 --> 01:09:20.140]   similarly, when you define things like nn.linear,
[01:09:20.140 --> 01:09:21.820]   which is just a matrix.
[01:09:21.820 --> 01:09:25.020]   So nn.linear, if I want to define in PyTorch,
[01:09:25.020 --> 01:09:26.220]   if I want to define a matrix,
[01:09:26.220 --> 01:09:29.580]   which is of say 50 users and 50 rows,
[01:09:29.580 --> 01:09:31.420]   and then it's by five, 50 by five,
[01:09:31.420 --> 01:09:36.540]   I could just say nn.linear 50 by five.
[01:09:36.540 --> 01:09:38.540]   And then you can check the shape,
[01:09:38.540 --> 01:09:41.580]   r.weight.shape.
[01:09:41.580 --> 01:09:44.220]   It's five by 50,
[01:09:44.220 --> 01:09:45.660]   because it just takes the transpose of that.
[01:09:45.660 --> 01:09:47.100]   Like that's just how the weight is represented.
[01:09:47.100 --> 01:09:48.940]   So I could have just said that.
[01:09:48.940 --> 01:09:51.580]   So you can see how it's like 50 by five shape.
[01:09:51.580 --> 01:09:53.340]   So if I want to say things like that,
[01:09:53.340 --> 01:09:55.660]   now in our dot product bias,
[01:09:55.660 --> 01:09:58.860]   instead of like using nn.embedding,
[01:09:58.860 --> 01:10:03.500]   I can just wrap my things inside nn.parameter, right?
[01:10:03.500 --> 01:10:06.220]   So I can say for my user factors,
[01:10:06.220 --> 01:10:09.820]   or like for my, basically my users,
[01:10:09.820 --> 01:10:13.020]   create my parameters such that they are trained.
[01:10:13.020 --> 01:10:15.100]   You initialize them with Porsche.zeros.
[01:10:15.820 --> 01:10:17.660]   You pass in the size,
[01:10:17.660 --> 01:10:19.900]   and then this normal is just going to initialize
[01:10:19.900 --> 01:10:21.980]   those values to be a normal distribution
[01:10:21.980 --> 01:10:24.380]   between zero and 0.01.
[01:10:24.380 --> 01:10:25.260]   So I just do that.
[01:10:25.260 --> 01:10:29.260]   I define my user latent matrix,
[01:10:29.260 --> 01:10:31.020]   my movie latent matrix,
[01:10:31.020 --> 01:10:33.180]   which is instead of now using nn.embedding,
[01:10:33.180 --> 01:10:34.220]   it's the same idea.
[01:10:34.220 --> 01:10:36.940]   I define my user bias,
[01:10:36.940 --> 01:10:38.140]   I define my movie bias,
[01:10:38.140 --> 01:10:39.420]   and now I can train this model.
[01:10:39.420 --> 01:10:43.340]   Oh, create parameters is not defined.
[01:10:43.340 --> 01:10:44.140]   I just need to run this.
[01:10:44.860 --> 01:10:48.460]   So that's just this main idea of like
[01:10:48.460 --> 01:10:50.700]   being able to learn and train
[01:10:50.700 --> 01:10:55.260]   all of these different parts of like creating parameters,
[01:10:55.260 --> 01:10:56.460]   what's a PyTorch parameter,
[01:10:56.460 --> 01:10:57.580]   what's a PyTorch module,
[01:10:57.580 --> 01:11:00.940]   and like how you can create your own parameters
[01:11:00.940 --> 01:11:02.700]   inside your own custom models,
[01:11:02.700 --> 01:11:04.620]   and then how can you train those custom models
[01:11:04.620 --> 01:11:07.740]   so that all of that has been mentioned now in this chapter.
[01:11:07.740 --> 01:11:11.340]   So this is where we'll stop today,
[01:11:11.340 --> 01:11:13.820]   but I will take any questions that you might have.
[01:11:14.620 --> 01:11:18.860]   (silence)
[01:11:18.860 --> 01:11:22.220]   About matrices or like anything you want to,
[01:11:22.220 --> 01:11:24.620]   anybody wants to ask about nn.embedding
[01:11:24.620 --> 01:11:25.900]   or nn.parameters.
[01:11:25.900 --> 01:11:27.500]   Otherwise, we'll continue next week
[01:11:27.500 --> 01:11:29.900]   about interpreting these embeddings and biases.
[01:11:29.900 --> 01:11:35.820]   Okay, so there's no questions today,
[01:11:35.820 --> 01:11:37.980]   so I guess that's where we'll end.
[01:11:37.980 --> 01:11:40.140]   Thanks everybody for joining,
[01:11:40.140 --> 01:11:42.220]   and I'll see you guys next week.
[01:11:42.220 --> 01:12:02.620]   (upbeat music)

