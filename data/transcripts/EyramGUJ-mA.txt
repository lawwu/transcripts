
[00:00:00.000 --> 00:00:00.640]   Hi, folks.
[00:00:00.640 --> 00:00:02.280]   It's Anton here from Chroma, here
[00:00:02.280 --> 00:00:04.320]   to talk to you a little bit about embedding stores
[00:00:04.320 --> 00:00:06.000]   and vector databases and how they work
[00:00:06.000 --> 00:00:09.880]   in the context of LLM in the loop applications,
[00:00:09.880 --> 00:00:12.040]   like the one you're building in this course.
[00:00:12.040 --> 00:00:13.920]   So by now, you've learned a little bit
[00:00:13.920 --> 00:00:16.840]   about embeddings and nearest neighbor
[00:00:16.840 --> 00:00:20.660]   and how to use them for similarity search.
[00:00:20.660 --> 00:00:22.040]   I want to talk to you a little bit
[00:00:22.040 --> 00:00:24.160]   about some of the software packages and tools available
[00:00:24.160 --> 00:00:26.440]   for you that will make that a bit easier, especially
[00:00:26.440 --> 00:00:28.280]   if you're working with large amounts of data
[00:00:28.280 --> 00:00:31.440]   or you're ready to build a production-grade application.
[00:00:31.440 --> 00:00:32.840]   So let's get right into it.
[00:00:32.840 --> 00:00:34.520]   What is an embedding store?
[00:00:34.520 --> 00:00:36.520]   An embedding store is basically a software package
[00:00:36.520 --> 00:00:38.360]   that abstracts away a lot of the operations
[00:00:38.360 --> 00:00:39.760]   that you've already learned about.
[00:00:39.760 --> 00:00:42.680]   So first, you take a set of documents
[00:00:42.680 --> 00:00:44.320]   that represent your knowledge base,
[00:00:44.320 --> 00:00:46.120]   and you can use the embedding store
[00:00:46.120 --> 00:00:48.800]   to embed them using its embedding function.
[00:00:48.800 --> 00:00:51.840]   And then when a query comes in, the same embedding function
[00:00:51.840 --> 00:00:52.340]   gets called.
[00:00:52.340 --> 00:00:55.120]   It generates an embedding as the query embedding,
[00:00:55.120 --> 00:00:57.720]   and then the embedding store itself
[00:00:57.720 --> 00:00:59.960]   performs the nearest neighbor search for you
[00:00:59.960 --> 00:01:03.280]   and returns the relevant documents to the LLM context
[00:01:03.280 --> 00:01:05.040]   window, thus basically simplifying
[00:01:05.040 --> 00:01:06.660]   a lot of the operations that you would
[00:01:06.660 --> 00:01:09.360]   have to implement yourself.
[00:01:09.360 --> 00:01:12.080]   So the next question is, when should you be using one?
[00:01:12.080 --> 00:01:13.960]   Well, the first answer is when your data
[00:01:13.960 --> 00:01:15.400]   gets sufficiently large.
[00:01:15.400 --> 00:01:18.360]   Computing distances to each embedding for each query
[00:01:18.360 --> 00:01:20.920]   is pretty slow and expensive, especially
[00:01:20.920 --> 00:01:22.680]   under certain distance functions.
[00:01:22.680 --> 00:01:24.320]   A good rule of thumb is that when
[00:01:24.320 --> 00:01:27.640]   you're over about 10,000 embeddings using
[00:01:27.640 --> 00:01:29.960]   some of the commercial open source embedding models,
[00:01:29.960 --> 00:01:31.460]   it's probably a good time to switch
[00:01:31.460 --> 00:01:33.280]   to a vector database or embedding store,
[00:01:33.280 --> 00:01:37.440]   and we'll get to a little bit about why that is in a minute.
[00:01:37.440 --> 00:01:39.600]   The other important thing is not to underestimate
[00:01:39.600 --> 00:01:42.060]   the convenience that actually using an off-the-shelf package
[00:01:42.060 --> 00:01:43.080]   gives you here.
[00:01:43.080 --> 00:01:45.880]   LLM-powered applications need to support many users
[00:01:45.880 --> 00:01:47.400]   across many indices.
[00:01:47.400 --> 00:01:49.960]   You need to handle data and scaling automatically.
[00:01:49.960 --> 00:01:51.840]   And basically, you want it to just work.
[00:01:51.840 --> 00:01:53.920]   So rather than getting into all the nitty gritty
[00:01:53.920 --> 00:01:56.400]   of implementation, as long as you understand what's going on,
[00:01:56.400 --> 00:01:58.800]   oftentimes it's easier to just use an off-the-shelf solution
[00:01:58.800 --> 00:02:01.080]   like Chroma.
[00:02:01.080 --> 00:02:05.640]   So how exactly do embedding stores and vector databases
[00:02:05.640 --> 00:02:07.880]   deal with large amounts of data without incurring
[00:02:07.880 --> 00:02:10.880]   the costs of computing distances between the query
[00:02:10.880 --> 00:02:13.400]   and every single embedding that it stores?
[00:02:13.400 --> 00:02:15.640]   Well, as we've seen, exact nearest neighbor
[00:02:15.640 --> 00:02:18.320]   basically requires us to scan over the entire list
[00:02:18.320 --> 00:02:21.040]   of embeddings and compute the distances to each one.
[00:02:21.040 --> 00:02:24.160]   That takes O(n) operations, commonly called linear time,
[00:02:24.160 --> 00:02:27.960]   where n is the number of embeddings you already have.
[00:02:27.960 --> 00:02:30.520]   Embedding stores typically use an approximate nearest neighbor
[00:02:30.520 --> 00:02:31.080]   algorithm.
[00:02:31.080 --> 00:02:32.540]   And basically, what they do is they
[00:02:32.540 --> 00:02:35.520]   exploit the underlying structure of the data
[00:02:35.520 --> 00:02:36.880]   that you have stored.
[00:02:36.880 --> 00:02:39.280]   And they take only log(n) operations.
[00:02:39.280 --> 00:02:41.920]   In other words, they're sublinear.
[00:02:41.920 --> 00:02:44.960]   It's a lot less expensive to do an O(log(n)) lookup
[00:02:44.960 --> 00:02:47.440]   than it is to scan the entire list.
[00:02:47.440 --> 00:02:49.880]   And the way that they do that is because they trade recall.
[00:02:49.880 --> 00:02:51.360]   In other words, sometimes they might
[00:02:51.360 --> 00:02:54.520]   miss the truly n-th nearest neighbor
[00:02:54.520 --> 00:02:57.400]   and grab something else in exchange for speed
[00:02:57.400 --> 00:02:59.520]   and computational complexity.
[00:02:59.520 --> 00:03:01.480]   Now, that might sound a little worrying.
[00:03:01.480 --> 00:03:03.040]   But the reality is that the trade-off between speed
[00:03:03.040 --> 00:03:04.400]   and recall can be tuned depending
[00:03:04.400 --> 00:03:05.440]   on the algorithm used.
[00:03:05.440 --> 00:03:07.060]   And there's a lot of flexibility in how
[00:03:07.060 --> 00:03:09.400]   these things are implemented.
[00:03:09.400 --> 00:03:10.960]   So going into a little bit more depth
[00:03:10.960 --> 00:03:13.280]   about approximate nearest neighbors,
[00:03:13.280 --> 00:03:15.760]   ANN algorithms are an active area of research.
[00:03:15.760 --> 00:03:17.840]   And there's a lot of different ones.
[00:03:17.840 --> 00:03:19.320]   Commonly used ones include things
[00:03:19.320 --> 00:03:22.400]   called inverted file indexes, locality-sensitive hashing,
[00:03:22.400 --> 00:03:24.880]   and hierarchical navigable small-world graphs.
[00:03:24.880 --> 00:03:27.320]   And if you want to learn more details about each of these,
[00:03:27.320 --> 00:03:30.240]   there's plenty of detail available online.
[00:03:30.240 --> 00:03:32.960]   Different algorithms work best in different settings.
[00:03:32.960 --> 00:03:36.000]   A lot of the approximate nearest neighbor algorithms
[00:03:36.000 --> 00:03:39.080]   used in many solutions today are set up
[00:03:39.080 --> 00:03:41.400]   for the case where the index doesn't change very much
[00:03:41.400 --> 00:03:43.960]   and gets updated only infrequently.
[00:03:43.960 --> 00:03:46.680]   However, if we're building an LLM in the loop application,
[00:03:46.680 --> 00:03:48.880]   it's very likely that our data will mutate online
[00:03:48.880 --> 00:03:49.600]   quite frequently.
[00:03:49.600 --> 00:03:52.960]   And so a graph-based algorithm like HNSW
[00:03:52.960 --> 00:03:54.720]   works well in these types of applications
[00:03:54.720 --> 00:03:57.000]   because we can iteratively construct and iterate
[00:03:57.000 --> 00:03:59.320]   on the graph.
[00:03:59.320 --> 00:04:00.600]   So what's next?
[00:04:00.600 --> 00:04:01.880]   I mean, this all sounds great.
[00:04:01.880 --> 00:04:03.340]   But the issue is that there is quite
[00:04:03.340 --> 00:04:06.040]   a few fundamental limitations, even when you have a software
[00:04:06.040 --> 00:04:08.120]   package that's supporting you in finding nearest neighbors
[00:04:08.120 --> 00:04:10.600]   in this way, because it doesn't answer certain questions.
[00:04:10.600 --> 00:04:12.880]   It doesn't answer for you which embedding model is best
[00:04:12.880 --> 00:04:14.520]   for my task or your data.
[00:04:14.520 --> 00:04:16.560]   It doesn't tell you how you should be chunking up
[00:04:16.560 --> 00:04:18.400]   your data to ensure good results.
[00:04:18.400 --> 00:04:20.760]   And just because you've gotten the n nearest neighbors
[00:04:20.760 --> 00:04:23.000]   doesn't actually tell you whether those neighbors are
[00:04:23.000 --> 00:04:24.840]   relevant or how relevant they actually are.
[00:04:24.840 --> 00:04:27.280]   So what should you do with the information
[00:04:27.280 --> 00:04:28.640]   that they're not sometimes?
[00:04:28.640 --> 00:04:30.400]   And finally, I think it's really important
[00:04:30.400 --> 00:04:31.880]   and one of the core principles of AI
[00:04:31.880 --> 00:04:34.560]   is that we can easily incorporate human feedback
[00:04:34.560 --> 00:04:37.600]   into the data that we're returning to the model.
[00:04:37.600 --> 00:04:40.320]   So providing opportunities and affordances
[00:04:40.320 --> 00:04:41.760]   to actually do that for developers
[00:04:41.760 --> 00:04:43.720]   is something that's really important.
[00:04:43.720 --> 00:04:44.540]   These are all features that we're
[00:04:44.540 --> 00:04:46.000]   looking into building into Chroma.
[00:04:46.000 --> 00:04:48.800]   And as I said, this is a still growing,
[00:04:48.800 --> 00:04:53.440]   this is still a field with a lot of product experimentation
[00:04:53.440 --> 00:04:56.840]   going on and we're hoping to support users like you
[00:04:56.840 --> 00:04:59.240]   in doing all of these things in the near future.
[00:04:59.240 --> 00:05:00.080]   Thank you.
[00:05:00.080 --> 00:05:02.660]   (upbeat music)
[00:05:02.660 --> 00:05:05.240]   (upbeat music)
[00:05:05.240 --> 00:05:07.820]   (upbeat music)

