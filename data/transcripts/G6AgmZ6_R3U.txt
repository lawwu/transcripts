
[00:00:00.000 --> 00:00:07.700]   The question that I got interested in was, is there any way to learn behaviors in a physics
[00:00:07.700 --> 00:00:12.020]   simulator where you actually have access to hundreds of millions of labeled examples,
[00:00:12.020 --> 00:00:16.340]   but then somehow make that work when the robot is put out into the real world?
[00:00:16.340 --> 00:00:20.180]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:20.180 --> 00:00:21.820]   models work in the real world.
[00:00:21.820 --> 00:00:23.660]   I'm your host, Lukas Biewald.
[00:00:23.660 --> 00:00:27.860]   Josh is a researcher and an entrepreneur and a teacher.
[00:00:27.860 --> 00:00:35.260]   His work at OpenAI was on Sim2Real, creating virtual environments to create training data
[00:00:35.260 --> 00:00:36.840]   for robotics.
[00:00:36.840 --> 00:00:42.620]   He also teaches my favorite maybe class on machine learning called Full Stack Deep Learning.
[00:00:42.620 --> 00:00:46.100]   And if you haven't taken that class, you absolutely should.
[00:00:46.100 --> 00:00:50.980]   Previously he did his PhD in computer science at Berkeley under Peter Abbeel.
[00:00:50.980 --> 00:00:53.420]   I'm super excited to talk to him.
[00:00:53.420 --> 00:00:59.060]   I think for a lot of people listening to this, just knowing our demographics, I think a lot
[00:00:59.060 --> 00:01:05.500]   of people would probably be most interested in learning about machine learning.
[00:01:05.500 --> 00:01:10.500]   They might even know you from some of the classes that you teach, which I think, in
[00:01:10.500 --> 00:01:14.460]   my opinion, are some of the best classes out there.
[00:01:14.460 --> 00:01:21.860]   I have to say, I've actually learned a lot from watching you teach.
[00:01:21.860 --> 00:01:26.260]   I'm kind of curious, how did you even get the idea of teaching a class?
[00:01:26.260 --> 00:01:29.140]   How did that come up?
[00:01:29.140 --> 00:01:35.060]   This all started to happen around two years ago.
[00:01:35.060 --> 00:01:40.900]   I was working at OpenAI at the time and OpenAI was going through an interesting transition,
[00:01:40.900 --> 00:01:42.660]   I would say.
[00:01:42.660 --> 00:01:48.620]   Where when I first joined, it felt like a very traditional academic lab.
[00:01:48.620 --> 00:01:55.780]   It felt like the lab that I was at at Berkeley, except more resources.
[00:01:55.780 --> 00:02:00.300]   At some point, they figured out that there was a type of work that they were really uniquely
[00:02:00.300 --> 00:02:07.180]   suited to do that a typical academic lab is not well suited to do, which is these larger
[00:02:07.180 --> 00:02:13.940]   projects that involve, instead of just a couple of researchers working together, maybe a team
[00:02:13.940 --> 00:02:21.780]   of 12 or 15 folks, a mix of engineers and researchers with bigger budgets, more ambitious
[00:02:21.780 --> 00:02:29.060]   goals, and really trying to push out these projects that would clearly mark a move forward
[00:02:29.060 --> 00:02:30.540]   in the state of the field.
[00:02:30.540 --> 00:02:37.700]   While this was happening, a big part of that change was we needed to figure out how we
[00:02:37.700 --> 00:02:43.820]   are going to professionalize our process of building machine learning models.
[00:02:43.820 --> 00:02:47.540]   On the robotics team, which I was working on at the time, we were figuring out stuff
[00:02:47.540 --> 00:02:52.820]   like how do we write good tests for our machine learning code so that you don't lose the ability
[00:02:52.820 --> 00:02:55.580]   to train a model that you were able to train a couple of months ago, which happened to
[00:02:55.580 --> 00:02:56.580]   us multiple times.
[00:02:56.580 --> 00:03:02.900]   How do you actually manage a team that has both folks that are doing speculative research
[00:03:02.900 --> 00:03:08.980]   stuff that may not be able to really measure their progress in any given week, and also
[00:03:08.980 --> 00:03:13.180]   people who are doing very traditional engineering work, that is where you can easily say, "This
[00:03:13.180 --> 00:03:18.020]   is the goal for this week," and, "Have we met that goal?"
[00:03:18.020 --> 00:03:21.140]   We were trying to sort out all these things.
[00:03:21.140 --> 00:03:27.140]   Around that time, I was talking to my PhD advisor at Berkeley, Peter Abbeel, and a friend
[00:03:27.140 --> 00:03:35.700]   of ours, Sergey Karev, who was running at the time a machine learning for education
[00:03:35.700 --> 00:03:37.180]   company called Gradescope.
[00:03:37.180 --> 00:03:43.780]   We were just swapping notes on how we were approaching these things and how Peter had
[00:03:43.780 --> 00:03:49.900]   seen other companies approach these and how Sergey had approached some of this stuff at
[00:03:49.900 --> 00:03:51.060]   Gradescope.
[00:03:51.060 --> 00:03:58.180]   We realized that there was this whole emerging engineering discipline, I would call it, around
[00:03:58.180 --> 00:04:03.900]   like you can go online and learn the math and the algorithms behind machine learning.
[00:04:03.900 --> 00:04:06.180]   You can learn what a neural network is.
[00:04:06.180 --> 00:04:11.340]   You can even really learn how to use TensorFlow and how to code this stuff up in an effective
[00:04:11.340 --> 00:04:12.340]   way.
[00:04:12.340 --> 00:04:18.220]   At the time, there was very little on everything else that you need in order to actually make
[00:04:18.220 --> 00:04:22.660]   this stuff work in the real world.
[00:04:22.660 --> 00:04:27.900]   Not only the things I described, but also how do you troubleshoot models?
[00:04:27.900 --> 00:04:29.820]   How do you choose projects?
[00:04:29.820 --> 00:04:33.240]   How do you manage teams?
[00:04:33.240 --> 00:04:35.180]   How do you deploy things into production?
[00:04:35.180 --> 00:04:38.260]   How do you monitor them once they're in production?
[00:04:38.260 --> 00:04:46.940]   We realized that everyone that we knew of was reinventing the wheel on all these practices.
[00:04:46.940 --> 00:04:50.500]   The number of people that are actually good at this is very small.
[00:04:50.500 --> 00:04:56.380]   They happen to be trapped in a small handful of large technology companies in the Bay Area,
[00:04:56.380 --> 00:04:57.380]   let's say.
[00:04:57.380 --> 00:05:02.060]   We just thought it'd be really good for the field if we wrote down everything that we
[00:05:02.060 --> 00:05:05.180]   knew about this stuff and everything that our friends knew about it.
[00:05:05.180 --> 00:05:09.620]   That was the genesis of the full stack deep learning class.
[00:05:09.620 --> 00:05:16.340]   I guess what's amazing, I never really thought about it this way, but I feel like I've spent
[00:05:16.340 --> 00:05:23.260]   my career, and I'm a little older than you, studying, making machine learning models work
[00:05:23.260 --> 00:05:25.500]   in the real world.
[00:05:25.500 --> 00:05:31.580]   But watching your class, I'm learning a ton and I'm seeing you as the expert.
[00:05:31.580 --> 00:05:35.220]   How did you get up to speed on this stuff so fast?
[00:05:35.220 --> 00:05:38.700]   Was it just through the experience at OpenAI?
[00:05:38.700 --> 00:05:40.260]   Because your classes are amazingly deep.
[00:05:40.260 --> 00:05:42.780]   Yeah, it's a good question.
[00:05:42.780 --> 00:05:49.540]   I think I was at OpenAI at the most interesting point for this, because we were figuring this
[00:05:49.540 --> 00:05:50.940]   stuff out from first principles.
[00:05:50.940 --> 00:05:56.100]   And so there were tons of conversations around, "Okay, what tests should we have for machine
[00:05:56.100 --> 00:05:57.100]   learning models?"
[00:05:57.100 --> 00:06:04.780]   And it was a really brilliant group of people there who like to take a problem and break
[00:06:04.780 --> 00:06:06.420]   it apart and look at it from the ground up.
[00:06:06.420 --> 00:06:12.260]   And so I think I was able to look at things from all the way down to the first principles
[00:06:12.260 --> 00:06:13.260]   level through that.
[00:06:13.260 --> 00:06:18.460]   But it was really just about trying to talk to a lot of the folks that are working in
[00:06:18.460 --> 00:06:21.700]   the field and see how they approach some of these things as well.
[00:06:21.700 --> 00:06:27.780]   A lot of the content that we put together in that class came from 30 or 40 interviews
[00:06:27.780 --> 00:06:32.500]   that we did with practitioners and just trying to understand.
[00:06:32.500 --> 00:06:37.940]   We had a good sense of what are the hard things and what questions do you need to ask if you're
[00:06:37.940 --> 00:06:40.540]   putting together a machine learning, like an applied machine learning team.
[00:06:40.540 --> 00:06:45.300]   And so just getting a range of answers on those was also really helpful.
[00:06:45.300 --> 00:06:50.620]   You have maybe a unique background in having been a McKinsey consultant for a number of
[00:06:50.620 --> 00:06:51.620]   years.
[00:06:51.620 --> 00:06:53.300]   Has that informed you at all?
[00:06:53.300 --> 00:06:57.100]   Do you think about how that might affect the way you approach this stuff?
[00:06:57.100 --> 00:07:03.820]   I think one of the things I learned from McKinsey was how to approach really abstract problems
[00:07:03.820 --> 00:07:07.220]   like what should our company do?
[00:07:07.220 --> 00:07:12.220]   Or what should our org structure look like?
[00:07:12.220 --> 00:07:17.100]   And how to create these problems where it's like, "Okay, where do I even start in thinking
[00:07:17.100 --> 00:07:18.100]   about this problem?"
[00:07:18.100 --> 00:07:23.300]   So I think the question of how to make machine learning work in the real world has this flavor.
[00:07:23.300 --> 00:07:26.740]   And then figuring out how to break that down into parts and structure your thinking around
[00:07:26.740 --> 00:07:32.940]   it is definitely one of the essential things that you have to do as a management consultant.
[00:07:32.940 --> 00:07:37.740]   And so I think that definitely informed the way that I looked at this problem.
[00:07:37.740 --> 00:07:44.100]   So is there a piece of your curriculum that you feel particularly proud of?
[00:07:44.100 --> 00:07:53.420]   Yeah, I think the thing that I put the most emotional energy into was the troubleshooting
[00:07:53.420 --> 00:07:54.420]   guide.
[00:07:54.420 --> 00:07:56.460]   That's actually my favorite part too.
[00:07:56.460 --> 00:08:02.060]   Yeah, because that was the piece where I just felt like I was writing it for myself a few
[00:08:02.060 --> 00:08:04.580]   years ago more than anything else.
[00:08:04.580 --> 00:08:09.260]   I was just trying to answer the...
[00:08:09.260 --> 00:08:12.660]   My perspective when I was writing that was like, "Okay, how could I have saved myself
[00:08:12.660 --> 00:08:17.620]   like months of time if I had gone and started over in this field?"
[00:08:17.620 --> 00:08:25.980]   Well, I have to say, I got a chance to work for you briefly for maybe a month or two.
[00:08:25.980 --> 00:08:33.980]   And I think my big takeaway from you that I always hear in my head is, "You should always
[00:08:33.980 --> 00:08:37.060]   slow down and change one thing at a time."
[00:08:37.060 --> 00:08:42.380]   And I feel like that actually applies to more than machine learning, honestly.
[00:08:42.380 --> 00:08:44.740]   But boy, does it apply to machine learning.
[00:08:44.740 --> 00:08:46.740]   Like, oh my God.
[00:08:46.740 --> 00:08:49.060]   Yeah, it's so essential in machine learning, right?
[00:08:49.060 --> 00:08:55.660]   Because fundamentally, I think the thing that makes machine learning so hard is that when
[00:08:55.660 --> 00:09:01.180]   you're writing software, when you're writing code, we have this pretty mature ecosystem
[00:09:01.180 --> 00:09:07.820]   where if you make a mistake, then usually the system that you're building or that you're
[00:09:07.820 --> 00:09:09.780]   interacting with will tell you...
[00:09:09.780 --> 00:09:11.420]   First of all, it will tell you that you made a mistake.
[00:09:11.420 --> 00:09:16.020]   And second of all, it might even give you a hint as to where that mistake is.
[00:09:16.020 --> 00:09:19.700]   But the insane thing about actually trying to make progress on machine learning projects
[00:09:19.700 --> 00:09:25.140]   is that most of the time when you have a bug, the only thing that happens is that the performance
[00:09:25.140 --> 00:09:29.940]   of your model doesn't get better as quickly as it should.
[00:09:29.940 --> 00:09:34.060]   And so there's no way of knowing that you actually made a mistake a lot of the time,
[00:09:34.060 --> 00:09:37.820]   unless you happen to have really strong intuition about what your learning curve should look
[00:09:37.820 --> 00:09:38.820]   like.
[00:09:38.820 --> 00:09:43.860]   And so I feel like that's why it's so essential to move slowly when you're building new machine
[00:09:43.860 --> 00:09:44.860]   learning models.
[00:09:44.860 --> 00:09:52.580]   Although it's kind of funny because I wonder if programming web applications is the outlier
[00:09:52.580 --> 00:09:53.580]   here.
[00:09:53.580 --> 00:09:57.780]   Because I think about just trying to make an advertising campaign work well or trying
[00:09:57.780 --> 00:10:01.620]   to get my old motorcycle running again.
[00:10:01.620 --> 00:10:06.780]   It's always better to change one thing because it's so hard to tell what happened otherwise.
[00:10:06.780 --> 00:10:12.860]   But I guess maybe we have just better telemetry or APIs with programming Python or JavaScript.
[00:10:12.860 --> 00:10:14.940]   I don't know.
[00:10:14.940 --> 00:10:15.940]   I feel like that's right.
[00:10:15.940 --> 00:10:16.940]   I mean, I think, I don't know.
[00:10:16.940 --> 00:10:21.100]   I mean, I wouldn't consider myself a world-class web developer, but when I've done web dev stuff,
[00:10:21.100 --> 00:10:24.020]   it's also been helpful to still just change one thing at a time.
[00:10:24.020 --> 00:10:25.020]   I just feel like...
[00:10:25.020 --> 00:10:26.500]   Yeah, maybe it's just good advice for all situations.
[00:10:26.500 --> 00:10:27.500]   Yeah, it might be.
[00:10:27.500 --> 00:10:28.500]   Anytime you're building anything.
[00:10:28.500 --> 00:10:33.860]   I feel like as you get better at something, you can increase the increment of what you
[00:10:33.860 --> 00:10:36.340]   can change at a time.
[00:10:36.340 --> 00:10:41.500]   Like if I'm training an image classifier or something, I can pretty much just start with
[00:10:41.500 --> 00:10:47.740]   a newer architecture like a ResNet or something like that.
[00:10:47.740 --> 00:10:52.460]   Just because I've done that enough times that I kind of know what I can expect the result
[00:10:52.460 --> 00:10:56.980]   will look like if it works and what common things that go wrong are.
[00:10:56.980 --> 00:10:59.660]   And so I feel like I can skip a couple of steps.
[00:10:59.660 --> 00:11:06.300]   But when I'm writing Kubernetes code, something I'm very much not good at, I have to do a
[00:11:06.300 --> 00:11:07.300]   lot of work.
[00:11:07.300 --> 00:11:08.300]   I have to still move very, very slowly.
[00:11:08.300 --> 00:11:12.660]   Would you be down to kind of walk me through your troubleshooting steps and how you think
[00:11:12.660 --> 00:11:13.660]   about them?
[00:11:13.660 --> 00:11:14.660]   I bet people will be interested.
[00:11:14.660 --> 00:11:15.660]   Yeah.
[00:11:15.660 --> 00:11:20.420]   The core concept is what we've been talking about, which is to start simple and then layer
[00:11:20.420 --> 00:11:22.980]   on complexity one step at a time.
[00:11:22.980 --> 00:11:27.900]   And so the first question you might have is, what does it mean to start simple?
[00:11:27.900 --> 00:11:31.900]   I think that one of the things I've noticed with people that are getting into the field
[00:11:31.900 --> 00:11:37.460]   is that there's a tendency to, there's all this excitement around neural network architectures
[00:11:37.460 --> 00:11:45.260]   and the latest and greatest state of the art model on ImageNet.
[00:11:45.260 --> 00:11:52.900]   And so I think people tend to overthink the question of architecture selection and selection
[00:11:52.900 --> 00:11:55.820]   of all the other pieces around that, like what optimizer you choose and things like
[00:11:55.820 --> 00:11:56.820]   that.
[00:11:56.820 --> 00:12:00.300]   But in reality, I think when you're starting out on a new project, the goal is to just
[00:12:00.300 --> 00:12:06.020]   choose a reasonable default and start there, even if it's not state of the art.
[00:12:06.020 --> 00:12:10.140]   And then once you've convinced yourself that everything around that is working, so your
[00:12:10.140 --> 00:12:16.100]   data loading code and your training code and all that stuff, then you can gradually move
[00:12:16.100 --> 00:12:18.140]   closer to a state of the art architecture.
[00:12:18.140 --> 00:12:21.340]   How do you convince yourself that the stuff is all working?
[00:12:21.340 --> 00:12:24.540]   Yeah, it's a hard question.
[00:12:24.540 --> 00:12:28.180]   I think that there's some tricks that you can use.
[00:12:28.180 --> 00:12:34.620]   So the first thing that I recommend people do when you're training, let's say, a new
[00:12:34.620 --> 00:12:40.180]   neural net for the first time is just make sure that you can, first of all, just get
[00:12:40.180 --> 00:12:41.700]   the thing to run.
[00:12:41.700 --> 00:12:44.060]   Like literally just output something?
[00:12:44.060 --> 00:12:48.660]   Just output anything, which is not always as easy as it should be.
[00:12:48.660 --> 00:12:50.980]   Let's say that you've done that.
[00:12:50.980 --> 00:12:55.580]   Then the next thing that I think usually you want to do is try to overfit a really small
[00:12:55.580 --> 00:12:59.580]   amount of data, like a single batch of data.
[00:12:59.580 --> 00:13:04.380]   And it seems really simple and a lot of people skip over that stuff because of that.
[00:13:04.380 --> 00:13:09.300]   And 80% of the time, it's not really necessary, but 20% of the time, you can catch some pretty
[00:13:09.300 --> 00:13:11.180]   nasty bugs early on.
[00:13:11.180 --> 00:13:16.220]   So I often recommend this, citing you.
[00:13:16.220 --> 00:13:20.060]   And I'm sure that this is not obvious to most people.
[00:13:20.060 --> 00:13:24.620]   Why do you want to overfit a small amount of data?
[00:13:24.620 --> 00:13:33.300]   So any reasonable model architecture and optimizer and training loop and data type, you should
[00:13:33.300 --> 00:13:37.660]   be able to get your loss down to zero on a single batch of data.
[00:13:37.660 --> 00:13:42.060]   You have enough parameters that the neural net should be able to just memorize the data.
[00:13:42.060 --> 00:13:47.580]   And so basically if it can't do that, then you know that you must have a pretty bad bug
[00:13:47.580 --> 00:13:48.900]   in one of those things.
[00:13:48.900 --> 00:13:51.580]   What kind of bug, for example?
[00:13:51.580 --> 00:13:56.220]   You flip the sign on your loss function, and your loss actually goes up rather than going
[00:13:56.220 --> 00:13:57.220]   down.
[00:13:57.220 --> 00:14:03.780]   Or another one I see all the time is in a lot of these neural network libraries, you
[00:14:03.780 --> 00:14:10.540]   have to input the inputs to the loss function is, maybe it's like the logits.
[00:14:10.540 --> 00:14:16.060]   So it's like something unnormalized, but maybe you took the softmax of that first.
[00:14:16.060 --> 00:14:23.980]   And so it's things like that, where it's just like you wrote the code the wrong way.
[00:14:23.980 --> 00:14:28.780]   And this is just like a quick sense check for figuring out, is the code that you're
[00:14:28.780 --> 00:14:29.780]   running reasonable?
[00:14:29.780 --> 00:14:30.780]   Okay.
[00:14:30.780 --> 00:14:32.900]   And so then, sorry, I cut you off.
[00:14:32.900 --> 00:14:37.500]   Then what do you do when you can overfit one tiny subset of your data?
[00:14:37.500 --> 00:14:38.500]   Yeah.
[00:14:38.500 --> 00:14:44.380]   So when you can overfit a tiny subset of your data, then I would say one way to think about
[00:14:44.380 --> 00:14:50.860]   the process of making your neural net better and better over time is there's an outer loop,
[00:14:50.860 --> 00:14:53.100]   and then there's an inner loop.
[00:14:53.100 --> 00:15:01.420]   And so the outer loop is basically like you're generally trying to do one of two things.
[00:15:01.420 --> 00:15:07.700]   You're either trying to reduce the amount of underfitting that your neural net has,
[00:15:07.700 --> 00:15:11.620]   or reduce the amount of overfitting that the neural net has.
[00:15:11.620 --> 00:15:20.300]   And so there's a lot of strategies for doing both of those things, but the best strategy
[00:15:20.300 --> 00:15:24.260]   for reducing underfitting is to make your model bigger, and for reducing overfitting
[00:15:24.260 --> 00:15:25.260]   is to add more data.
[00:15:25.260 --> 00:15:30.500]   And so if you think about what we just did with overfitting a single batch of data, or
[00:15:30.500 --> 00:15:35.220]   with driving loss down to zero on a single batch of data, we're basically saying, let's
[00:15:35.220 --> 00:15:38.500]   take the smallest possible dataset and let's overfit it.
[00:15:38.500 --> 00:15:44.300]   And so now the next question in your decision tree should be, all right, now we know that
[00:15:44.300 --> 00:15:48.340]   we're overfitting because we can drive loss down to zero.
[00:15:48.340 --> 00:15:51.420]   So the next thing that we should do is reduce overfitting, and the simplest way to do that
[00:15:51.420 --> 00:15:53.980]   is to add data.
[00:15:53.980 --> 00:15:55.300]   But you want to do this gradually.
[00:15:55.300 --> 00:16:00.840]   So typically what I would do next is I would move from a single batch of data to a smaller
[00:16:00.840 --> 00:16:04.100]   or more simplified version of the dataset that I was working with.
[00:16:04.100 --> 00:16:08.620]   So maybe it's like, I don't know, maybe you have a million images, but you just take a
[00:16:08.620 --> 00:16:12.100]   thousand or 10,000 of them to start out with.
[00:16:12.100 --> 00:16:16.260]   Maybe you make a synthetic toy version of the problem that you're working with.
[00:16:16.260 --> 00:16:23.740]   If you're doing reinforcement learning, maybe you work with one of the standard simple benchmark
[00:16:23.740 --> 00:16:28.660]   problems like car pole or something like that.
[00:16:28.660 --> 00:16:34.100]   And so you just make the problem one step more difficult than a single batch of data.
[00:16:34.100 --> 00:16:35.380]   I see.
[00:16:35.380 --> 00:16:38.460]   So you add one piece of complexity?
[00:16:38.460 --> 00:16:41.720]   Yeah, that's the way I think about it.
[00:16:41.720 --> 00:16:46.060]   And why wouldn't you just add all the data that you have?
[00:16:46.060 --> 00:16:51.180]   Because your conclusions, I imagine they could change at different scales of data, for example.
[00:16:51.180 --> 00:16:52.380]   Yeah, definitely.
[00:16:52.380 --> 00:16:55.380]   I think there's two core reasons.
[00:16:55.380 --> 00:17:06.100]   So one is, I guess maybe the simplest one to explain is that it just reduces your iteration
[00:17:06.100 --> 00:17:07.100]   time.
[00:17:07.100 --> 00:17:09.860]   So if you're working with a smaller dataset or a simpler dataset, then typically your
[00:17:09.860 --> 00:17:13.380]   model will change faster, it'll be cheaper, and so you can just try out more things more
[00:17:13.380 --> 00:17:16.540]   quickly, which is super key.
[00:17:16.540 --> 00:17:24.940]   But I think the deeper and more interesting reason is that a lot of times in machine learning,
[00:17:24.940 --> 00:17:31.180]   you have some degree of confidence that this model should actually be able to solve the
[00:17:31.180 --> 00:17:33.540]   task that you're working on.
[00:17:33.540 --> 00:17:36.820]   But a lot of times you don't actually know that for a fact.
[00:17:36.820 --> 00:17:39.860]   Maybe you're doing image classification, but you're not doing it on ImageNet, you're doing
[00:17:39.860 --> 00:17:43.540]   it on some other dataset.
[00:17:43.540 --> 00:17:49.340]   You're classifying whether a person is wearing a hat in an image or not.
[00:17:49.340 --> 00:17:53.180]   And so intuitively you feel like it should be possible to solve this with a neural net,
[00:17:53.180 --> 00:17:56.180]   but you don't actually know that for sure.
[00:17:56.180 --> 00:18:01.820]   And so you want to try to isolate the sources of error in your problem.
[00:18:01.820 --> 00:18:07.580]   And so if one of the possible sources of error is that this dataset is just too hard, then
[00:18:07.580 --> 00:18:10.620]   it makes sense to start with a version of the dataset that you know that your model
[00:18:10.620 --> 00:18:12.780]   should be able to do well on.
[00:18:12.780 --> 00:18:16.820]   And so smaller datasets, less complex datasets allow you to do that.
[00:18:16.820 --> 00:18:19.820]   But wouldn't a smaller dataset make the problem harder?
[00:18:19.820 --> 00:18:21.820]   In what sense?
[00:18:21.820 --> 00:18:26.860]   Like if say I'm trying to classify if someone has a hat on or not, if I have less training
[00:18:26.860 --> 00:18:32.580]   data I would expect my accuracy to be lower, right?
[00:18:32.580 --> 00:18:39.380]   Yeah, that's definitely true.
[00:18:39.380 --> 00:18:43.340]   So I guess this comes back to the overall process that we're trying to follow.
[00:18:43.340 --> 00:18:50.180]   I think of it as iterating between eliminating underfitting and eliminating overfitting.
[00:18:50.180 --> 00:18:55.940]   And so if you're in a situation where your model is doing perfectly well on your training
[00:18:55.940 --> 00:19:01.320]   set, then it makes sense to increase the complexity of your training set.
[00:19:01.320 --> 00:19:06.740]   If you're in a situation where your model can't do well on the training set, then you
[00:19:06.740 --> 00:19:09.860]   need to figure out is it that my training data is too hard?
[00:19:09.860 --> 00:19:11.580]   Is it that I need a bigger model?
[00:19:11.580 --> 00:19:12.980]   Is it that I need a different architecture?
[00:19:12.980 --> 00:19:17.220]   Is that I need a different optimizer, different hyperparameters?
[00:19:17.220 --> 00:19:23.100]   And so working with a dataset where that's easier to get to that point of your model
[00:19:23.100 --> 00:19:29.540]   overfitting just reduces the number of things that could be wrong with your model.
[00:19:29.540 --> 00:19:30.540]   Interesting.
[00:19:30.540 --> 00:19:32.740]   Are there more steps to this?
[00:19:32.740 --> 00:19:34.420]   I mean, that's the high level flow, right?
[00:19:34.420 --> 00:19:41.300]   It's like, solve your problem, make it harder, solve your problem, make it harder.
[00:19:41.300 --> 00:19:47.940]   And then there's details about how to make each of those things work well.
[00:19:47.940 --> 00:19:53.580]   What are the steps you should actually try when you're underfitting and you need to make
[00:19:53.580 --> 00:19:59.420]   your model more expressive?
[00:19:59.420 --> 00:20:03.020]   I think that's the overall picture.
[00:20:03.020 --> 00:20:07.020]   We'll have to put a link to this so people can find it.
[00:20:07.020 --> 00:20:09.380]   Do you plan to teach more of these classes?
[00:20:09.380 --> 00:20:11.260]   I think so, yeah.
[00:20:11.260 --> 00:20:15.660]   We don't have concrete plans to do another one.
[00:20:15.660 --> 00:20:20.220]   I mean, it's not a great time for in-person classes.
[00:20:20.220 --> 00:20:22.220]   Maybe a virtual one would be...
[00:20:22.220 --> 00:20:24.020]   Yeah, maybe a virtual one.
[00:20:24.020 --> 00:20:25.580]   That could be fun.
[00:20:25.580 --> 00:20:31.140]   Hi, we'd love to take a moment to tell you guys about Weights and Biases.
[00:20:31.140 --> 00:20:36.660]   Weights and Biases is a tool that helps you track and visualize every detail of your machine
[00:20:36.660 --> 00:20:37.660]   learning models.
[00:20:37.660 --> 00:20:43.580]   We help you debug your machine learning models in real time, collaborate easily, and advance
[00:20:43.580 --> 00:20:46.660]   the state of the art in machine learning.
[00:20:46.660 --> 00:20:51.860]   You can integrate Weights and Biases into your models with just a few lines of code.
[00:20:51.860 --> 00:20:56.620]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:20:56.620 --> 00:20:58.560]   automatically.
[00:20:58.560 --> 00:21:03.960]   You can also track and compare how many GPU resources your models are using.
[00:21:03.960 --> 00:21:09.860]   With one line of code, you can visualize model predictions in the form of images, videos,
[00:21:09.860 --> 00:21:16.380]   audio, plotly charts, molecular data, segmentation maps, and 3D point clouds.
[00:21:16.380 --> 00:21:22.220]   You can save everything you need to reproduce your models days, weeks, or even months after
[00:21:22.220 --> 00:21:23.220]   training.
[00:21:23.220 --> 00:21:28.340]   Finally, with reports, you can make your models come alive.
[00:21:28.340 --> 00:21:33.360]   Reports are like blog posts in which your readers can interact with your model metrics
[00:21:33.360 --> 00:21:35.140]   and predictions.
[00:21:35.140 --> 00:21:41.380]   Reports serve as a centralized repository of metrics, predictions, hyperparameter stride,
[00:21:41.380 --> 00:21:43.100]   and accompanying nodes.
[00:21:43.100 --> 00:21:48.780]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:21:48.780 --> 00:21:54.300]   You can use reports to share your model insights, keep your team on the same page, and collaborate
[00:21:54.300 --> 00:21:55.300]   effectively remotely.
[00:21:55.300 --> 00:22:00.420]   I'll leave a link in the show notes below to help you get started.
[00:22:00.420 --> 00:22:03.140]   And now, let's get back to the episode.
[00:22:03.140 --> 00:22:08.200]   Do you have any advice to folks wanting to get into machine learning?
[00:22:08.200 --> 00:22:13.160]   I'm sure you probably watch a lot of students learn it or not learn it.
[00:22:13.160 --> 00:22:17.280]   Do they have any sense of what's required?
[00:22:17.280 --> 00:22:23.520]   Some people look at something like machine learning and they say, "This is a really deep
[00:22:23.520 --> 00:22:28.200]   field and there's a lot to learn here.
[00:22:28.200 --> 00:22:32.280]   There's a lot of complexity, like so many papers, thousands of papers coming out every
[00:22:32.280 --> 00:22:33.280]   month.
[00:22:33.280 --> 00:22:41.920]   I want to just drink from the fire hose and try to just learn as much as possible."
[00:22:41.920 --> 00:22:49.560]   Then on the other extreme, there's folks that say, "Look, this field is so complex that
[00:22:49.560 --> 00:22:53.800]   I want to just pick a problem and solve that problem."
[00:22:53.800 --> 00:22:57.120]   I think there's failure modes on both ends of that.
[00:22:57.120 --> 00:23:04.280]   I work with people who see the complexity of the field and react to that by just learning
[00:23:04.280 --> 00:23:08.080]   more and more and more, but never actually really getting their hands dirty and figuring
[00:23:08.080 --> 00:23:12.880]   out how to make this stuff work for the problems that they care about.
[00:23:12.880 --> 00:23:16.480]   I think that typically doesn't work.
[00:23:16.480 --> 00:23:21.080]   I've seen probably just as many people who don't want to deal with the complexity, don't
[00:23:21.080 --> 00:23:27.920]   want to learn the math, don't want to understand how a conf net works.
[00:23:27.920 --> 00:23:31.960]   I think that that also limits your ability to make progress in the field because ultimately
[00:23:31.960 --> 00:23:39.760]   it's closer to a science than an engineering discipline right now, I would say.
[00:23:39.760 --> 00:23:47.080]   I think you need to balance spending time on actually doing stuff and following tutorials
[00:23:47.080 --> 00:23:51.280]   and making things work, and then also going back and backfilling.
[00:23:51.280 --> 00:23:57.160]   "Okay, now I've trained a conf net on this image classification task.
[00:23:57.160 --> 00:23:59.020]   I know how to write the TensorFlow code.
[00:23:59.020 --> 00:24:02.680]   Now let me actually go back and understand how a conf net works."
[00:24:02.680 --> 00:24:08.720]   The folks that you've seen that have been successful, really learned this stuff and
[00:24:08.720 --> 00:24:15.980]   have started to get good careers as successful people, do you think they spend more time
[00:24:15.980 --> 00:24:20.160]   on the theory on average or more time on the practical hands-on stuff?
[00:24:20.160 --> 00:24:25.560]   Or is there some other third thing that they're doing more of that makes them successful?
[00:24:25.560 --> 00:24:27.600]   I would say more time on the practical hands-on stuff.
[00:24:27.600 --> 00:24:32.760]   One of the interesting things about machine learning is that although there's a ton of
[00:24:32.760 --> 00:24:40.240]   complexity in the field, there's a relatively small number of core ideas that you actually
[00:24:40.240 --> 00:24:48.520]   need to really deeply understand in order to be an expert in the field.
[00:24:48.520 --> 00:24:53.480]   Understanding attention in neural nets is really important.
[00:24:53.480 --> 00:24:56.000]   Understanding how back propagation works is really important.
[00:24:56.000 --> 00:25:06.120]   But understanding all the different state-of-the-art architectures for doing object detection,
[00:25:06.120 --> 00:25:11.280]   not really very important unless you happen to be working full-time on that problem.
[00:25:11.280 --> 00:25:17.240]   I would say that the people that I know that have successfully learned the field have tried
[00:25:17.240 --> 00:25:23.720]   to ... I would say that they've spent more time with a smaller number of ideas.
[00:25:23.720 --> 00:25:29.400]   Rather than trying to read five new papers every day, they've went out and talked to
[00:25:29.400 --> 00:25:33.680]   people and figured out what the five most important papers are, and then have spent
[00:25:33.680 --> 00:25:37.520]   weeks with each of those to really deeply understand them.
[00:25:37.520 --> 00:25:40.480]   But then have probably spent the balance of their time actually trying things and implementing
[00:25:40.480 --> 00:25:41.480]   things.
[00:25:41.480 --> 00:25:43.440]   That makes sense.
[00:25:43.440 --> 00:25:47.160]   When you look at the papers that you've written, do you have a favorite?
[00:25:47.160 --> 00:25:53.400]   I think my favorite is actually the first one that I was the lead author on, which was
[00:25:53.400 --> 00:25:55.760]   the domain randomization paper.
[00:25:55.760 --> 00:25:56.760]   Oh, cool.
[00:25:56.760 --> 00:25:57.760]   Cool.
[00:25:57.760 --> 00:25:58.760]   Sim to real?
[00:25:58.760 --> 00:25:59.760]   Sim to real.
[00:25:59.760 --> 00:26:00.760]   Yeah.
[00:26:00.840 --> 00:26:05.680]   So, is that a real process of thinking of that idea and then trying it and how that
[00:26:05.680 --> 00:26:06.680]   all happened?
[00:26:06.680 --> 00:26:12.280]   Well, I guess first describe the idea because it seems like the rare paper you can really
[00:26:12.280 --> 00:26:14.080]   kind of succinctly describe it, right?
[00:26:14.080 --> 00:26:15.080]   Yeah.
[00:26:15.080 --> 00:26:21.240]   When I was starting to work in this field, so the intersection of deep learning and robotics,
[00:26:21.240 --> 00:26:26.920]   back in 2015, there was a lot of excitement around reinforcement learning being applied
[00:26:26.920 --> 00:26:27.920]   to robotics.
[00:26:27.920 --> 00:26:32.720]   So, like reinforcement learning, you have an agent that interacts with an environment,
[00:26:32.720 --> 00:26:38.000]   it takes some observations of the environment, decides what action to take, and then gets
[00:26:38.000 --> 00:26:42.000]   a signal back from the environment, which is a reward that tells it, did I do a good
[00:26:42.000 --> 00:26:43.400]   job or a bad job?
[00:26:43.400 --> 00:26:48.480]   And then over time, it iteratively learns how to interact with that environment and
[00:26:48.480 --> 00:26:51.720]   improve its performance on whatever task it's supposed to be doing.
[00:26:51.720 --> 00:26:56.120]   And so, it's like a very natural abstraction for robotics.
[00:26:56.120 --> 00:27:02.280]   And back in 2015, deep reinforcement learning was starting to have a bit of a renaissance,
[00:27:02.280 --> 00:27:03.280]   right?
[00:27:03.280 --> 00:27:07.680]   It was starting to work really well on Atari games.
[00:27:07.680 --> 00:27:15.960]   I think that was 2015 or 2016 was when DeepMind beat the best human players in Go, right?
[00:27:15.960 --> 00:27:21.920]   And so, people were looking at this and saying, "Wow, this could be the most important technology
[00:27:21.920 --> 00:27:24.520]   to come to robotics in a really long time."
[00:27:24.520 --> 00:27:32.880]   And so, I was early on in my PhD at that point, and the exciting thing to work on was coming
[00:27:32.880 --> 00:27:36.600]   up with what's the best new reinforcement learning algorithm?
[00:27:36.600 --> 00:27:40.960]   How can we improve our performance on all these tasks?
[00:27:40.960 --> 00:27:45.480]   But I was very new to the field, and I didn't feel like I had a good...
[00:27:45.480 --> 00:27:51.200]   I felt like it would not be very smart for me to try to compete with people who have
[00:27:51.200 --> 00:27:56.800]   been studying this stuff for years and have a lot of insights into what made those algorithms
[00:27:56.800 --> 00:27:57.800]   work.
[00:27:57.800 --> 00:28:02.680]   And so, what I tried to do was think about, "Okay, what are the enabling pieces that we
[00:28:02.680 --> 00:28:05.080]   need in order for this story to come true?"
[00:28:05.080 --> 00:28:10.240]   For a story that deep reinforcement learning is going to have a big impact on robotics.
[00:28:10.240 --> 00:28:18.680]   And for me, the piece that was missing for that story was that deep reinforcement learning
[00:28:18.680 --> 00:28:21.920]   is very powerful, but it's very data inefficient.
[00:28:21.920 --> 00:28:26.760]   All these state-of-the-art results that you see happen in environments where you can simulate
[00:28:26.760 --> 00:28:34.040]   everything that's happening, because it takes hundreds of millions or more of interactions
[00:28:34.040 --> 00:28:38.880]   with the environment to actually get to the point where you have human-level behavior.
[00:28:38.880 --> 00:28:45.920]   And so, for me, looking into this field from the outside, that was the big question mark.
[00:28:45.920 --> 00:28:50.200]   Is there any way for us to get around this data inefficiency problem for robots?
[00:28:50.200 --> 00:28:55.360]   Because going out and collecting 100 million examples of a robot interacting with an environment
[00:28:55.360 --> 00:28:58.360]   is not very cost-effective, let's say.
[00:28:58.360 --> 00:28:59.880]   Google did this, right?
[00:28:59.880 --> 00:29:01.800]   With an arm farm?
[00:29:01.800 --> 00:29:08.560]   So it's definitely possible, but do you really want to have to have dozens of robot arms
[00:29:08.560 --> 00:29:13.160]   running 24/7 for weeks every time you want to learn new behavior?
[00:29:13.160 --> 00:29:14.160]   Sure.
[00:29:14.160 --> 00:29:15.160]   Yeah.
[00:29:15.160 --> 00:29:21.040]   So coming back to this paper, the question that I got interested in was, is there any
[00:29:21.040 --> 00:29:26.760]   way to learn behaviors in a physics simulator where you actually have access to hundreds
[00:29:26.760 --> 00:29:31.640]   of millions of labeled examples, but then somehow make that work when the robot is put
[00:29:31.640 --> 00:29:32.640]   out into the real world?
[00:29:32.640 --> 00:29:41.200]   I was working on this back when I was an intern at OpenAI, and we had a really concrete problem
[00:29:41.200 --> 00:29:47.120]   that we were trying to solve, which is we were trying to set up a robot to make a stack
[00:29:47.120 --> 00:29:48.120]   of blocks.
[00:29:48.120 --> 00:29:53.280]   So to pick up blocks from a table and then stack them on top of each other.
[00:29:53.280 --> 00:29:59.600]   And the policy, the robot behavior was trained, assuming that you actually know where the
[00:29:59.600 --> 00:30:01.240]   blocks are in the real world.
[00:30:01.240 --> 00:30:04.920]   And so then we needed to go back and backfill, how do we actually find out?
[00:30:04.920 --> 00:30:10.240]   How do we estimate the position of each of these blocks in the real world?
[00:30:10.240 --> 00:30:13.280]   It's something that seems like a really easy problem, but actually when you think about
[00:30:13.280 --> 00:30:17.520]   how do you make this really work, it's more complicated than you'd expect.
[00:30:17.520 --> 00:30:19.200]   Honestly, it's so counterintuitive.
[00:30:19.200 --> 00:30:25.000]   I think, even for me and probably for most people, that's hard.
[00:30:25.000 --> 00:30:26.400]   It's amazing that that's hard.
[00:30:26.400 --> 00:30:27.400]   Yeah.
[00:30:27.400 --> 00:30:31.560]   And I think it's not the hardest research problem in the world, but it's like when you
[00:30:31.560 --> 00:30:37.960]   actually sit down and try to go and make it work really well, it's very tricky.
[00:30:37.960 --> 00:30:47.440]   And so we were playing around with these different tag, or RICO tags and methods like that, where
[00:30:47.440 --> 00:30:53.160]   you understand the intrinsics of the camera and then it reads this tag off of an object
[00:30:53.160 --> 00:30:59.600]   and then it can infer the position of the object, given the position of the camera.
[00:30:59.600 --> 00:31:06.160]   And we just found those things to be really fragile and honestly not really that accurate
[00:31:06.160 --> 00:31:10.600]   without investing in expensive setup and expensive camera equipment and stuff like that.
[00:31:10.600 --> 00:31:15.200]   And so the question was, we were mostly deep learning folks, right?
[00:31:15.200 --> 00:31:20.720]   And so the obvious question is, why don't you just train a neural net to do this?
[00:31:20.720 --> 00:31:24.200]   Just train a neural net to take an image of a table and then say, "Okay, here are the
[00:31:24.200 --> 00:31:28.440]   position of all of the cubes on the table."
[00:31:28.440 --> 00:31:33.520]   But the problem is that where do you get the labels for the dataset that you collect?
[00:31:33.520 --> 00:31:36.360]   You almost need to solve the problem.
[00:31:36.360 --> 00:31:40.960]   You almost need to know where the cubes are in order to actually get the label dataset
[00:31:40.960 --> 00:31:42.520]   that you use to train the neural net.
[00:31:42.520 --> 00:31:45.040]   So it's a bit of a chicken and egg problem.
[00:31:45.040 --> 00:31:49.360]   And so this was kind of the starting point for me working on this Sim2Brill problem.
[00:31:49.360 --> 00:31:55.280]   It's like, okay, this feels like the simplest possible example of a problem where maybe
[00:31:55.280 --> 00:31:59.400]   synthetic data, data from a physics simulator would actually help.
[00:31:59.400 --> 00:32:01.460]   So then describe what you did.
[00:32:01.460 --> 00:32:06.600]   So the core idea is that if you just take data from a simulator kind of naively and
[00:32:06.600 --> 00:32:11.280]   train a model on it, the problem is that there's quirks of your simulator, right?
[00:32:11.280 --> 00:32:13.680]   Your simulator doesn't perfectly match the real world.
[00:32:13.680 --> 00:32:18.960]   And so the neural net will overfit to any difference between the data in the simulator
[00:32:18.960 --> 00:32:20.080]   and the data in the real world.
[00:32:20.080 --> 00:32:25.280]   So if you didn't perfectly model the lighting, you didn't perfectly model the color of the
[00:32:25.280 --> 00:32:28.920]   cube, the neural net won't transfer.
[00:32:28.920 --> 00:32:35.980]   And so the idea that we had was like, what if you, instead of just taking a single best
[00:32:35.980 --> 00:32:42.780]   physics simulator, what if you massively randomize every aspect of the simulator that's not critically
[00:32:42.780 --> 00:32:44.380]   important to solving the task, right?
[00:32:44.380 --> 00:32:50.320]   So you randomize the colors of all the objects, you randomize their positions, you randomize
[00:32:50.320 --> 00:32:57.240]   the position of the camera, you randomize the background.
[00:32:57.240 --> 00:33:01.460]   And it produces images that are crazy and unrealistic looking, right?
[00:33:01.460 --> 00:33:09.060]   So they look like scenes from an anime disco or something.
[00:33:09.060 --> 00:33:16.340]   But what happens is that actually the neural net, in learning how to estimate the position
[00:33:16.340 --> 00:33:24.060]   of the cube in all of these different, massively different worlds, is forced to not rely on
[00:33:24.060 --> 00:33:27.760]   the parts of the simulator that are not essential for solving the task, right?
[00:33:27.760 --> 00:33:33.540]   So if the color of the cube changes in every single data point, then the neural net can't
[00:33:33.540 --> 00:33:37.620]   create a feature that depends on the color of the cube to solve the task, because that's
[00:33:37.620 --> 00:33:39.140]   just an unreliable piece of information.
[00:33:39.140 --> 00:33:44.060]   And so then when you do this, it turns out that you can train neural nets on entirely
[00:33:44.060 --> 00:33:45.880]   simulated data.
[00:33:45.880 --> 00:33:50.340]   So no real world data at all, but actually work when they're deployed in the real world.
[00:33:50.340 --> 00:33:58.020]   Because you just kind of cycle through lots of colors and shadows and other...
[00:33:58.020 --> 00:34:00.700]   Yeah, exactly.
[00:34:00.700 --> 00:34:09.100]   You basically show the neural net every color and every shadow that it could even possibly
[00:34:09.100 --> 00:34:10.100]   see.
[00:34:10.100 --> 00:34:15.780]   And so then in order to solve the task, it needs to learn colors and shadows are not
[00:34:15.780 --> 00:34:17.420]   important, right?
[00:34:17.420 --> 00:34:23.620]   What's important is the position of this cube looking thing on the table.
[00:34:23.620 --> 00:34:28.500]   And so it's not overfitting to all the details that are unimportant.
[00:34:28.500 --> 00:34:33.740]   And so then the details that it is looking at are the ones hopefully that will transfer
[00:34:33.740 --> 00:34:36.940]   over when it's deployed in the real world.
[00:34:36.940 --> 00:34:39.820]   And so how far can this generalize?
[00:34:39.820 --> 00:34:42.980]   Has this been applied to more than stacking blocks now?
[00:34:42.980 --> 00:34:43.980]   Yeah.
[00:34:44.300 --> 00:34:49.420]   It's been applied to a pretty wide range of sort of computer vision and robotics tasks
[00:34:49.420 --> 00:34:50.980]   at this point.
[00:34:50.980 --> 00:34:54.740]   I think it's been applied to...
[00:34:54.740 --> 00:35:01.700]   My favorite random application was there's a paper about using domain randomization to
[00:35:01.700 --> 00:35:05.060]   train a robot to pick fish out of a barrel.
[00:35:05.060 --> 00:35:06.060]   Really?
[00:35:06.060 --> 00:35:07.060]   Yeah.
[00:35:07.060 --> 00:35:08.060]   Wow.
[00:35:08.060 --> 00:35:13.180]   Which is actually a really hard task because fish are very shiny and slippery, right?
[00:35:13.180 --> 00:35:19.580]   And in general, most object detection methods and computer vision stuff has trouble with
[00:35:19.580 --> 00:35:23.500]   objects that have a lot of reflections and things like that.
[00:35:23.500 --> 00:35:25.740]   So that was my favorite random application.
[00:35:25.740 --> 00:35:29.980]   But it's been applied to folding cloth.
[00:35:29.980 --> 00:35:35.060]   It's been applied to a pretty wide range of computer vision problems.
[00:35:35.060 --> 00:35:42.900]   And I think maybe the furthest this idea has been pushed was at OpenAI.
[00:35:42.900 --> 00:35:49.220]   When they use this technique to have a robot hand that solves a Rubik's Cube.
[00:35:49.220 --> 00:35:55.100]   Can you say a little bit about why that was such an impressive task?
[00:35:55.100 --> 00:36:01.380]   Maybe there was some controversy about is this sort of a gestant or is this a real deep
[00:36:01.380 --> 00:36:02.380]   task?
[00:36:02.380 --> 00:36:05.020]   Where do you land on that?
[00:36:05.020 --> 00:36:12.700]   So maybe the different sides of this issue would be like on one hand, I think if you
[00:36:12.700 --> 00:36:20.700]   look at the types of tasks that people have been able to solve with robots over the years,
[00:36:20.700 --> 00:36:24.900]   this task of this sort, right?
[00:36:24.900 --> 00:36:31.460]   Using a high dimensional dexterous robot to manipulate a complicated object are very sort
[00:36:31.460 --> 00:36:33.860]   of few and far between in the robotics world.
[00:36:33.860 --> 00:36:38.340]   And it's kind of generally seen as like, you know, high dimensional contact rich dexterous
[00:36:38.340 --> 00:36:42.940]   manipulation is being like sort of one of the grand challenges of robotics.
[00:36:42.940 --> 00:36:46.900]   And so I think one point of view on this is that even just something like a proof of concept
[00:36:46.900 --> 00:36:52.860]   level to show that it's like possible to even do this once is a big step for the field.
[00:36:52.860 --> 00:36:58.180]   Because you know, there's very few examples of there are some but there's very few examples
[00:36:58.180 --> 00:37:07.940]   of projects that have pushed robotic manipulation as far as being able to solve a Rubik's cube.
[00:37:07.940 --> 00:37:10.700]   I think the other perspective would just be that like, you know, if you look at the details
[00:37:10.700 --> 00:37:14.980]   in the paper, the algorithm like actually works, you know, something like 20% of the
[00:37:14.980 --> 00:37:17.180]   time.
[00:37:17.180 --> 00:37:22.900]   And so you might argue that like, you know, and it was a pretty big effort to get to actually
[00:37:22.900 --> 00:37:24.380]   make it work for the first time, right?
[00:37:24.380 --> 00:37:28.420]   So you know, pretty big team working on it for a long time.
[00:37:28.420 --> 00:37:33.700]   And so you might argue that like, yeah, obviously, if you put, you know, 10 or 12 really brilliant
[00:37:33.700 --> 00:37:40.460]   people and have them work on, you know, one tiny sliver of a problem for a long time,
[00:37:40.460 --> 00:37:44.100]   then obviously, they'll be able to make it work once.
[00:37:44.100 --> 00:37:45.100]   I would say that, you know...
[00:37:45.100 --> 00:37:46.100]   That's not obviously me.
[00:37:46.100 --> 00:37:47.100]   I don't know.
[00:37:47.100 --> 00:37:50.980]   I'm trying to play devil's advocate here.
[00:37:50.980 --> 00:37:53.860]   My bias is that it's an important result in robotics.
[00:37:53.860 --> 00:37:58.500]   And I think that, you know, but I think like the perspective that you have to have when
[00:37:58.500 --> 00:38:03.060]   you look at this is that, you know, it is very much a research result, right?
[00:38:03.060 --> 00:38:07.140]   Like I think a mistake that people make when looking at results like this, and I think
[00:38:07.140 --> 00:38:12.900]   this is true in AI in general, is that like, you look at, you know, humans or like computers
[00:38:12.900 --> 00:38:15.260]   being better than humans at any task.
[00:38:15.260 --> 00:38:20.260]   And you say like, okay, this means that like robots are going to take this job in two years.
[00:38:20.260 --> 00:38:24.940]   And it's like, not like if you look at the details of how hard it was to actually make
[00:38:24.940 --> 00:38:28.620]   this work once, you know, 20% of the time, it's like, there's a lot more research that
[00:38:28.620 --> 00:38:33.340]   will need to happen in order for this to become a thing that robots can do reliably.
[00:38:33.340 --> 00:38:37.580]   But I do think there's a lot of value in the proof of concept just to show that like, this
[00:38:37.580 --> 00:38:44.700]   is, you know, this is a set of techniques that, you know, this team was able to push
[00:38:44.700 --> 00:38:50.080]   far enough to do this task that is like objectively, really difficult for robots to do.
[00:38:50.080 --> 00:38:54.900]   And then over time, the field will backfill like how to actually do that in a more efficient
[00:38:54.900 --> 00:38:55.900]   way.
[00:38:55.900 --> 00:39:00.060]   I guess I didn't realize it only worked 20% of the time.
[00:39:00.060 --> 00:39:08.820]   This is like 20% success rate, meaning completely manipulated the cube to be back in the correct
[00:39:08.820 --> 00:39:09.820]   state.
[00:39:09.820 --> 00:39:10.820]   Is that right?
[00:39:10.820 --> 00:39:18.180]   Yeah, I think the fine print is for the hardest variant of the problem, which is like the
[00:39:18.180 --> 00:39:21.680]   cube randomizes as much as it can be.
[00:39:21.680 --> 00:39:26.920]   The robot was only able to get it back to fully solved 20% of the time.
[00:39:26.920 --> 00:39:28.640]   I think on average, it did it more than that.
[00:39:28.640 --> 00:39:33.040]   And I think also, yeah, like maybe one of the other details people took issue with was
[00:39:33.040 --> 00:39:37.600]   the fact that there was the machine learning algorithm itself did not, you know, didn't
[00:39:37.600 --> 00:39:40.760]   say like the sequence of actions that you need to solve the cube.
[00:39:40.760 --> 00:39:41.760]   Right.
[00:39:41.760 --> 00:39:46.560]   It didn't like it wasn't like a neural net was saying, you know, turn this face and then
[00:39:46.560 --> 00:39:49.760]   turn this face and then turn this face.
[00:39:49.760 --> 00:39:54.780]   There was like a hard coded solver that was saying the sequence of actions that you take.
[00:39:54.780 --> 00:39:58.160]   And then the neural net was just saying like, okay, here's how you move your fingers in
[00:39:58.160 --> 00:40:01.080]   order to achieve this action.
[00:40:01.080 --> 00:40:04.520]   So the point was the manipulation.
[00:40:04.520 --> 00:40:09.660]   So people are mad because it was like a fun demonstration.
[00:40:09.660 --> 00:40:15.840]   I think, you know, I think people often take issue with the way that OpenAI communicates
[00:40:15.840 --> 00:40:20.640]   results like this more so than the results themselves.
[00:40:20.640 --> 00:40:24.680]   Because it seems like they're generating attention.
[00:40:24.680 --> 00:40:25.680]   Is that right?
[00:40:25.680 --> 00:40:26.680]   I think so.
[00:40:26.680 --> 00:40:27.680]   Yeah.
[00:40:27.680 --> 00:40:28.680]   Yeah.
[00:40:28.680 --> 00:40:34.340]   I think there's a bit of attention in the field right now of, you know, between kind
[00:40:34.340 --> 00:40:42.720]   of people who maybe have more traditional academic roots and who think that, you know,
[00:40:42.720 --> 00:40:45.320]   it's like the quality of the scholarship that's important.
[00:40:45.320 --> 00:40:50.360]   And like, you know, whether it's truly novel, you know, whether the results are like really
[00:40:50.360 --> 00:40:54.560]   understandable and reproducible, you know, on one hand.
[00:40:54.560 --> 00:40:58.800]   And then on the other hand, you know, folks, you know, who typically are more at more of
[00:40:58.800 --> 00:41:04.800]   like the more industrial research lab type places where I think the viewpoint is more
[00:41:04.800 --> 00:41:11.040]   like our goal is to, you know, our goal is to like push the state of the art of the field
[00:41:11.040 --> 00:41:13.480]   forward.
[00:41:13.480 --> 00:41:18.560]   And if we have to do that in a way that's like not totally, you know, not 100% reproducible
[00:41:18.560 --> 00:41:22.040]   just because like maybe the experiment was too expensive.
[00:41:22.040 --> 00:41:26.240]   That's okay because we're removing the goalposts forward of like the types of things that AI
[00:41:26.240 --> 00:41:27.640]   is able to do.
[00:41:27.640 --> 00:41:28.640]   Yeah.
[00:41:28.640 --> 00:41:32.440]   And I think there's like kind of a fundamental tension there.
[00:41:32.440 --> 00:41:33.920]   That makes sense.
[00:41:33.920 --> 00:41:36.960]   So I guess you've left OpenAI.
[00:41:36.960 --> 00:41:39.120]   So what are you working on now, Josh?
[00:41:39.120 --> 00:41:40.120]   Yeah.
[00:41:40.120 --> 00:41:41.120]   No, that's a good question.
[00:41:41.120 --> 00:41:44.880]   You know, one of the things that I learned through full stack deep learning is I guess
[00:41:44.880 --> 00:41:50.480]   maybe one of the beliefs that I have about this field is that like I think, you know,
[00:41:50.480 --> 00:41:55.880]   there's this narrative in the machine learning world that like, you know, AI is going to
[00:41:55.880 --> 00:41:56.880]   be part of everything.
[00:41:56.880 --> 00:42:02.560]   And, you know, like it's going to be like software where it's like just sort of happening
[00:42:02.560 --> 00:42:07.880]   in the background as part of like every little thing that we do.
[00:42:07.880 --> 00:42:11.960]   And it's going to enable all these like amazing new applications like self-driving cars.
[00:42:11.960 --> 00:42:14.560]   But in general, it's just going to be like there in the background making the world like
[00:42:14.560 --> 00:42:16.760]   10 or 15% more efficient or more.
[00:42:16.760 --> 00:42:17.760]   I don't know.
[00:42:17.760 --> 00:42:18.760]   But we're not there yet.
[00:42:18.760 --> 00:42:23.080]   And so like one of the kind of core questions for me over the last, you know, six months
[00:42:23.080 --> 00:42:25.760]   or so since I left OpenAI has been why is that?
[00:42:25.760 --> 00:42:26.760]   Right.
[00:42:26.760 --> 00:42:30.960]   Like what is what's blocking us from, you know, having like just a little bit of machine
[00:42:30.960 --> 00:42:37.000]   learning that's just making sort of every piece of software that we interact with smarter.
[00:42:37.000 --> 00:42:42.760]   And that's kind of that's sort of the root, like the fundamental question that I'm trying
[00:42:42.760 --> 00:42:43.760]   to answer with this company.
[00:42:43.760 --> 00:42:45.480]   It's so interesting.
[00:42:45.480 --> 00:42:51.000]   We actually always have been ending this podcast with two questions and that question has been
[00:42:51.000 --> 00:42:52.000]   one of them.
[00:42:52.000 --> 00:42:54.880]   I mean, you've clearly like been spending a lot of time thinking about it.
[00:42:54.880 --> 00:42:57.520]   Like what's some of your conclusions?
[00:42:57.520 --> 00:43:02.880]   Like if you had to pick like one thing, what would that be?
[00:43:02.880 --> 00:43:06.200]   I mean, this comes back to our conversation about the robot hand, right?
[00:43:06.200 --> 00:43:13.720]   Like I think the field has gotten really good at doing really impressive things once.
[00:43:13.720 --> 00:43:20.200]   And but then, you know, one of the like the dirty secrets of machine learning is that
[00:43:20.200 --> 00:43:25.360]   turning something that works once or like, I don't know, works is 90% accurate on like
[00:43:25.360 --> 00:43:27.520]   one data set.
[00:43:27.520 --> 00:43:36.840]   Turning that into like a reliable production system that is auditable and is maintainable
[00:43:36.840 --> 00:43:42.720]   and understandable and you can actually like start to run your business on, that's really
[00:43:42.720 --> 00:43:43.720]   hard.
[00:43:43.720 --> 00:43:44.720]   Right.
[00:43:44.720 --> 00:43:50.000]   And so I think like figuring out how to answer that question is sort of the big question
[00:43:50.000 --> 00:43:51.640]   that the field needs to answer right now.
[00:43:51.640 --> 00:43:57.760]   Yeah, I guess it's kind of counterintuitive to see a computer do something 20% of the
[00:43:57.760 --> 00:43:58.760]   time.
[00:43:58.760 --> 00:44:03.080]   Like that feels like a really, like I feel like most times if I see a computer do something
[00:44:03.080 --> 00:44:07.480]   that I know, like, okay, that's it's going to do that 100% of the time.
[00:44:07.480 --> 00:44:08.480]   Yeah, no, definitely.
[00:44:08.480 --> 00:44:09.480]   Yeah, for sure.
[00:44:09.480 --> 00:44:15.840]   I mean, I think it's like, you know, maybe one of the other things that I've seen through
[00:44:15.840 --> 00:44:19.280]   full stack deep learning and through like some of the other folks that I've talked to
[00:44:19.280 --> 00:44:24.160]   who are like trying to implement machine learning in their companies is that oftentimes like
[00:44:24.160 --> 00:44:29.360]   one of the hardest things to do is just like figure out how to get, you know, the executives
[00:44:29.360 --> 00:44:32.720]   in your company, let's say, like the folks that are making the decisions but are not
[00:44:32.720 --> 00:44:37.160]   deep in the technology to actually understand what can we really do with this stuff.
[00:44:37.160 --> 00:44:38.160]   Right.
[00:44:38.160 --> 00:44:41.200]   And it's like, yeah, I think that's one of the things that's really hard about machine
[00:44:41.200 --> 00:44:46.520]   learning is that like it's not always clear.
[00:44:46.520 --> 00:44:49.520]   There's not always a clear connection between like what you read about what it can do and
[00:44:49.520 --> 00:44:52.160]   what can actually do.
[00:44:52.160 --> 00:44:55.080]   And like communicating that I think is like another big challenge for the field.
[00:44:55.080 --> 00:44:56.080]   Do you have any suggestions there?
[00:44:56.080 --> 00:45:00.840]   I mean, almost everybody we've talked to has brought that up.
[00:45:00.840 --> 00:45:07.440]   Someone needs to make a really good class that's like AI for, you know, for everyone.
[00:45:07.440 --> 00:45:08.920]   Andrew has a class I haven't gone through it.
[00:45:08.920 --> 00:45:11.840]   Maybe that's maybe that's the answer.
[00:45:11.840 --> 00:45:19.240]   And I think you can build intuition for this stuff, but I think that doesn't come from
[00:45:19.240 --> 00:45:22.120]   reading like the New York Times headlines.
[00:45:22.120 --> 00:45:27.160]   It comes from actually sitting down and looking at examples of things that work and things
[00:45:27.160 --> 00:45:28.960]   that don't work.
[00:45:28.960 --> 00:45:36.320]   So I don't have a good suggestion, but I do think there's a big opportunity to make that.
[00:45:36.320 --> 00:45:42.600]   It's funny, I've heard that like IBM Watson in its heyday with like, you know, like fly
[00:45:42.600 --> 00:45:47.200]   executives to like a lab and just like kind of blow their minds and get them like hyped
[00:45:47.200 --> 00:45:49.360]   out of their minds at the potential of AI.
[00:45:49.360 --> 00:45:51.960]   It's like really awesome demos.
[00:45:51.960 --> 00:45:55.840]   And I've always had this fantasy of like doing like the opposite of that.
[00:45:55.840 --> 00:46:01.040]   You know, like do like an hour with like execs and make it like really kind of hard and boring
[00:46:01.040 --> 00:46:05.120]   and like, you know, let them like fight with the AI for a while.
[00:46:05.120 --> 00:46:08.840]   Like, you know, even just try to like, you know, tune some hyper parameters to like actually
[00:46:08.840 --> 00:46:09.840]   get the thing working.
[00:46:09.840 --> 00:46:16.160]   I think it would be a fun, like a fun, I think it may be an informative experience for a
[00:46:16.160 --> 00:46:17.320]   lot of people.
[00:46:17.320 --> 00:46:21.200]   Maybe help the execs understand whether, you know, ML teams aren't producing results as
[00:46:21.200 --> 00:46:23.360]   fast as they were hoping.
[00:46:23.360 --> 00:46:29.720]   I think that also like one thing that would help a lot is I think the methodology, you
[00:46:29.720 --> 00:46:32.480]   know, this is like sort of what we tried to do with full stack deep learning, but maybe
[00:46:32.480 --> 00:46:34.680]   didn't really get all the way there.
[00:46:34.680 --> 00:46:40.000]   But I think that like the methodology of successfully building machine learning systems is still
[00:46:40.000 --> 00:46:41.000]   pretty immature, right?
[00:46:41.000 --> 00:46:46.440]   Like it shares a lot with software engineering, but it's really a different field.
[00:46:46.440 --> 00:46:52.400]   I think that like, you know, if there were like an agile equivalent for building machine
[00:46:52.400 --> 00:46:54.760]   learning systems, I think that would also go a long way.
[00:46:54.760 --> 00:46:58.360]   Because it's really just like the block and tackle of like, how do you actually make this
[00:46:58.360 --> 00:46:59.880]   happen?
[00:46:59.880 --> 00:47:03.440]   So it doesn't feel as much like magic and like, what are those crazy data scientists
[00:47:03.440 --> 00:47:05.160]   doing over in their corner over there?
[00:47:05.160 --> 00:47:09.960]   And it feels a little bit more like, OK, I understand that, you know, this is the set
[00:47:09.960 --> 00:47:13.920]   of meetings that the team is having every week and this is how they're measuring their
[00:47:13.920 --> 00:47:15.680]   progress.
[00:47:15.680 --> 00:47:20.880]   I think like something more operational like that could also go a long way.
[00:47:20.880 --> 00:47:22.880]   Seems like you'd be the right guy to figure that out.
[00:47:22.880 --> 00:47:23.880]   I don't know.
[00:47:23.880 --> 00:47:24.880]   Maybe.
[00:47:24.880 --> 00:47:25.880]   All right.
[00:47:25.880 --> 00:47:30.160]   Here's the other question we always end with, which I'm really curious to know what you'll
[00:47:30.160 --> 00:47:31.160]   say.
[00:47:31.160 --> 00:47:36.760]   Just off the top of your head, what's like a sort of under appreciated topic in machine
[00:47:36.760 --> 00:47:38.440]   learning that you think people should talk about more?
[00:47:38.440 --> 00:47:42.760]   I mean, given all the hype on so many of the topics, what's a piece that people don't pay
[00:47:42.760 --> 00:47:45.840]   enough attention to?
[00:47:45.840 --> 00:47:50.840]   I think that people don't pay enough attention to the quality of their training data.
[00:47:50.840 --> 00:47:54.760]   Oh man.
[00:47:54.760 --> 00:47:55.760]   I agree.
[00:47:55.760 --> 00:47:56.760]   I agree, Josh.
[00:47:56.760 --> 00:47:57.760]   But it's so important.
[00:47:57.760 --> 00:47:58.760]   So important.
[00:47:58.760 --> 00:48:01.760]   Nice.
[00:48:01.760 --> 00:48:04.760]   All right.
[00:48:04.760 --> 00:48:05.760]   Well, that was really fun.
[00:48:05.760 --> 00:48:06.760]   Thanks for taking the time to chat.
[00:48:06.760 --> 00:48:07.760]   Yeah.
[00:48:07.760 --> 00:48:07.760]   Thanks for having me on.
[00:48:07.760 --> 00:48:08.760]   Yeah.
[00:48:08.760 --> 00:48:08.760]   Thanks for having me on.
[00:48:08.760 --> 00:48:09.260]   Thanks for having me on.
[00:48:09.260 --> 00:48:12.620]   [MUSIC PLAYING]
[00:48:12.620 --> 00:48:15.980]   [MUSIC PLAYING]
[00:48:15.980 --> 00:48:19.340]   [MUSIC PLAYING]

