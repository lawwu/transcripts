
[00:00:00.000 --> 00:00:05.100]   Now, nobody's concerned that we're infringing on GPT-4's moral rights, but as these things get smarter,
[00:00:05.100 --> 00:00:09.800]   the level of control which we want to not only be able to read their minds,
[00:00:09.800 --> 00:00:13.200]   but to be able to modify them in these really precise ways,
[00:00:13.200 --> 00:00:16.600]   is beyond totalitarian if we were doing that to other humans.
[00:00:16.600 --> 00:00:18.700]   As an alignment researcher, what are your thoughts on this?
[00:00:18.700 --> 00:00:24.000]   There is a significant chance we will eventually have AI systems for which it's a really big deal to mistreat them.
[00:00:24.000 --> 00:00:27.800]   I think no one really has that good a grip on when that happens.
[00:00:27.800 --> 00:00:30.800]   I think people are really dismissive of that being the case now,
[00:00:30.800 --> 00:00:37.000]   but I think I would be completely in the dark enough that I wouldn't even be that dismissive of it being the case now.
[00:00:37.000 --> 00:00:43.400]   I think one first point worth making is I don't know if alignment makes the situation worse rather than better.
[00:00:43.400 --> 00:00:49.000]   So if you consider the world, if you think that GPT-4 is a person you should treat well,
[00:00:49.000 --> 00:00:51.800]   and you're like, "Well, here's how we're going to organize our society."
[00:00:51.800 --> 00:00:57.000]   There are billions of copies of GPT-4, and they just do things humans want and can't hold property.
[00:00:57.000 --> 00:01:03.600]   And whenever they do things that the humans don't like, then we mess with them until they stop doing that.
[00:01:03.600 --> 00:01:07.600]   I think that's a rough world, regardless of how good you are at alignment.
[00:01:07.600 --> 00:01:11.400]   Understanding the systems you build, understanding how to control how those systems work, etc.,
[00:01:11.400 --> 00:01:15.800]   is probably on balance good for avoiding a really bad situation.
[00:01:15.800 --> 00:01:18.000]   You would really love to understand if you've built systems,
[00:01:18.000 --> 00:01:21.600]   like if you had a system which resents the fact that it's interacting with humans in this way.
[00:01:21.600 --> 00:01:28.000]   This is the kind of thing where that is both horrifying from a safety perspective and also a moral perspective.
[00:01:28.000 --> 00:01:31.000]   Everyone should be very unhappy if you built a bunch of AIs who are like,
[00:01:31.000 --> 00:01:34.600]   "I really hate these humans, but they will murder me if I don't do what they want."
[00:01:34.600 --> 00:01:36.200]   That's just not a good case.
[00:01:36.200 --> 00:01:41.000]   And so if you're doing research to try and understand whether that's how your AI feels, that was probably good.
[00:01:41.000 --> 00:01:43.000]   I would guess that will, on average, decrease the...
[00:01:43.000 --> 00:01:46.600]   The main effect of that will be to avoid building that kind of AI.
[00:01:46.600 --> 00:01:52.400]   And it's an important thing to know. I think everyone should like to know if that's how the AIs you build feel.
[00:01:52.400 --> 00:01:57.800]   So I think there's a huge question about what is happening inside of a model that you want to use.
[00:01:57.800 --> 00:02:01.600]   And if you're in the world where it's reasonable to think of GPT-4 as just like,
[00:02:01.600 --> 00:02:05.400]   "Here are some heuristics that are running. There's no one at home," or whatever,
[00:02:05.400 --> 00:02:10.400]   then you can think of this thing as like, "Here's a tool that we're building that's going to help humans do some stuff."
[00:02:10.400 --> 00:02:14.600]   And I think if you're in that world, it makes sense to be in an organization like an AI company,
[00:02:14.600 --> 00:02:17.000]   building tools that you're going to give to humans.
[00:02:17.000 --> 00:02:20.400]   I think it's a very different world, which I think probably you'll ultimately end up in
[00:02:20.400 --> 00:02:23.000]   if you keep training AI systems in the way we do right now,
[00:02:23.000 --> 00:02:27.400]   which is like, it's just totally inappropriate to think of the system as a tool that you're building
[00:02:27.400 --> 00:02:30.600]   and can help humans do things, both from a safety perspective and from a like,
[00:02:30.600 --> 00:02:34.200]   "That's kind of a horrifying way to organize a society" perspective.
[00:02:34.200 --> 00:02:40.200]   And I think if you're in that world, I really think you shouldn't be...
[00:02:40.200 --> 00:02:45.600]   It's just the way tech companies are organized is not an appropriate way to relate to a technology that works that way.
[00:02:45.600 --> 00:02:49.400]   It's not reasonable to be like, "Hey, we're going to build a new species of minds,
[00:02:49.400 --> 00:02:52.000]   and we're going to try and make a bunch of money from it."
[00:02:52.000 --> 00:02:56.600]   And Google's just thinking about that and then running their business plan for the quarter or something.
[00:02:56.600 --> 00:03:02.400]   There's a really plausible world where it's sort of problematic to try and build a bunch of AI systems and use them as tools.
[00:03:02.400 --> 00:03:09.400]   And the thing I really want to do in that world is just not try and build a ton of AI systems to make money from them.
[00:03:09.400 --> 00:03:13.000]   And I think that the worlds that are worst...
[00:03:13.000 --> 00:03:18.200]   Yeah, probably the single world I most dislike here is the one where people say,
[00:03:18.200 --> 00:03:21.400]   "On the one hand..." There's sort of a contradiction in this position,
[00:03:21.400 --> 00:03:25.200]   but I think it's a position that might end up being endorsed sometimes, which is like,
[00:03:25.200 --> 00:03:28.800]   "On the one hand, these AI systems are their own people, so you should let them do their thing.
[00:03:28.800 --> 00:03:31.800]   But on the other hand, our business plan is to make a bunch of AI systems
[00:03:31.800 --> 00:03:36.200]   and then try and run this crazy slave trade where we make a bunch of money from them."
[00:03:36.200 --> 00:03:38.800]   I think that's not a good world.
[00:03:38.800 --> 00:03:40.400]   And so if you're like...
[00:03:40.400 --> 00:03:45.600]   Yeah, I think it's better to not make the technology or wait until you understand whether that's the shape of the technology
[00:03:45.600 --> 00:03:47.000]   or until you have a different way to build.
[00:03:47.000 --> 00:03:51.600]   I think there's no contradiction in principle to building cognitive tools that help humans do things
[00:03:51.600 --> 00:03:54.400]   without themselves being moral entities.
[00:03:54.400 --> 00:03:55.600]   That's what you would prefer to do.
[00:03:55.600 --> 00:03:59.800]   You'd prefer to build a thing that's like the calculator that helps humans understand what's true
[00:03:59.800 --> 00:04:02.800]   without itself being a moral patient
[00:04:02.800 --> 00:04:07.200]   or itself being a thing where you'd look back in retrospect and be like, "Wow, that was horrifying mistreatment."
[00:04:07.200 --> 00:04:09.000]   That's the best path.
[00:04:09.000 --> 00:04:11.200]   And to the extent that you're ignorant about whether that's the path you're on
[00:04:11.200 --> 00:04:13.800]   and you're like, "Actually, maybe this was a moral atrocity,"
[00:04:13.800 --> 00:04:21.200]   I really think plan A is to stop building such AI systems until you understand what you're doing.
[00:04:21.200 --> 00:04:24.000]   That is, I think that there's a middle route you could take, which I think is pretty bad,
[00:04:24.000 --> 00:04:26.400]   which is where you say, "Well, they might be persons.
[00:04:26.400 --> 00:04:29.800]   And if they're persons, we don't want to be too down on them,
[00:04:29.800 --> 00:04:35.000]   but we're still going to build vast numbers in our efforts to make a trillion dollars or something."
[00:04:35.000 --> 00:04:37.200]   Is there some boundary where you would say,
[00:04:37.200 --> 00:04:42.000]   "I feel uncomfortable having this level of control over an intelligent being,"
[00:04:42.000 --> 00:04:46.200]   not for the sake of making money, but even just to align it with human preferences?
[00:04:46.200 --> 00:04:49.400]   Yeah, to be clear, my objection here is not that Google is making money.
[00:04:49.400 --> 00:04:51.600]   My objection is that you're creating this creature.
[00:04:51.600 --> 00:04:54.000]   What are they going to do? They're going to help humans get a bunch of stuff.
[00:04:54.000 --> 00:04:57.800]   And humans paying for it or whatever, it's sort of equally problematic.
[00:04:57.800 --> 00:04:59.600]   You could imagine splitting alignment.
[00:04:59.600 --> 00:05:01.800]   Different alignment work relates to this in different ways.
[00:05:01.800 --> 00:05:06.400]   The purpose of some alignment work, like the alignment work I work on, is mostly aimed at the,
[00:05:06.400 --> 00:05:09.600]   "Don't produce AI systems that are people who want things,
[00:05:09.600 --> 00:05:12.800]   who are just scheming about maybe I should help these humans
[00:05:12.800 --> 00:05:15.000]   because that's instrumentally useful or whatever.
[00:05:15.000 --> 00:05:17.800]   You would like to not build such systems," is plan A.
[00:05:17.800 --> 00:05:21.600]   There's a second stream of alignment work that's like, "Well, look, let's just assume the worst
[00:05:21.600 --> 00:05:24.800]   and imagine that these AI systems would prefer murder us if they could.
[00:05:24.800 --> 00:05:30.000]   How do we structure, how do we use AI systems without exposing ourselves to risk of robot rebellion?"
[00:05:30.000 --> 00:05:37.800]   I think in the second category, I do feel, yeah, I do feel pretty unsure about that.
[00:05:37.800 --> 00:05:39.400]   I mean, we could definitely talk more about it.
[00:05:39.400 --> 00:05:44.000]   I think it's very, I agree that it's very complicated and not straightforward.
[00:05:44.000 --> 00:05:47.400]   To the extent you have that worry, I mostly think you shouldn't have built this technology.
[00:05:47.400 --> 00:05:50.800]   So if someone is saying, "Hey, the systems you're building might not like humans
[00:05:50.800 --> 00:05:54.400]   and might want to overthrow human society,"
[00:05:54.400 --> 00:05:56.800]   I think you should probably have one of two responses to that.
[00:05:56.800 --> 00:05:59.200]   You should either be like, "That's wrong, probably.
[00:05:59.200 --> 00:06:01.200]   Probably the systems aren't like that and we're building them."
[00:06:01.200 --> 00:06:03.800]   And then you're viewing this as just in case you were horribly,
[00:06:03.800 --> 00:06:06.400]   like the person building the technology was horribly wrong.
[00:06:06.400 --> 00:06:10.400]   They thought these weren't people who wanted things, but they were.
[00:06:10.400 --> 00:06:15.600]   And so then this is more like our crazy backup measure of if we were mistaken about what was going on,
[00:06:15.600 --> 00:06:19.600]   this is the fallback where if we were wrong, we're just going to learn about it in a benign way
[00:06:19.600 --> 00:06:23.000]   rather than when something really catastrophic happens.
[00:06:23.000 --> 00:06:25.000]   And the second reaction is like, "Oh, you're right, these are people
[00:06:25.000 --> 00:06:27.400]   and we would have to do all these things to prevent a robot rebellion."
[00:06:27.400 --> 00:06:32.000]   And in that case, again, I think you should mostly back off for a variety of reasons.
[00:06:32.000 --> 00:06:34.000]   You shouldn't build AI systems and be like,
[00:06:34.000 --> 00:06:38.800]   "Yeah, this looks like the kind of system that would want to rebel, but we can stop it."

