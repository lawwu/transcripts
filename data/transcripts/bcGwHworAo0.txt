
[00:00:00.000 --> 00:00:02.420]   I'm in a very Daniel Plainview mood this week.
[00:00:02.420 --> 00:00:04.500]   There will be blood seen behind me.
[00:00:04.500 --> 00:00:05.080]   I have a competition in me.
[00:00:05.080 --> 00:00:07.040]   I don't want to see other people succeed.
[00:00:07.040 --> 00:00:07.720]   That's right.
[00:00:07.720 --> 00:00:08.680]   Best scene ever.
[00:00:08.680 --> 00:00:11.780]   You are the furthest from that character.
[00:00:11.780 --> 00:00:13.600]   Like, that is not your spirit animal.
[00:00:13.600 --> 00:00:14.520]   Oh, there will be blood?
[00:00:14.520 --> 00:00:15.820]   That guy, Daniel Day-Lewis?
[00:00:15.820 --> 00:00:17.600]   He has nothing to do with you.
[00:00:17.600 --> 00:00:18.820]   I've watched it about 100 times.
[00:00:18.820 --> 00:00:19.340]   Incredible.
[00:00:19.340 --> 00:00:23.600]   You must be so stormy and roiled on the inside.
[00:00:23.600 --> 00:00:25.120]   Oh my gosh.
[00:00:25.120 --> 00:00:25.900]   There is a lot of pent up.
[00:00:25.900 --> 00:00:28.140]   Did you not see my roast at Sax's birthday?
[00:00:28.140 --> 00:00:28.760]   It's just a roast.
[00:00:28.760 --> 00:00:29.520]   Did I not see it?
[00:00:29.520 --> 00:00:30.140]   I lived it.
[00:00:30.140 --> 00:00:37.040]   It was the most off-color, disgusting, egregious, mean diatribe I've ever heard.
[00:00:37.040 --> 00:00:37.900]   My lord.
[00:00:37.900 --> 00:00:39.500]   I have a competition in me.
[00:00:39.500 --> 00:00:41.320]   I don't like to see others succeed.
[00:00:41.320 --> 00:00:43.500]   I can't stand that J. Cal is a good moderator.
[00:00:59.040 --> 00:01:00.920]   I'm the queen of Kinwa.
[00:01:00.920 --> 00:01:02.720]   I'm going all in.
[00:01:02.720 --> 00:01:06.920]   I guess everybody wants to know, Chamath, you've wound down two SPACs.
[00:01:06.920 --> 00:01:13.800]   Thank you for doing this for IPOF specifically, because people are replying to me every day
[00:01:13.800 --> 00:01:16.800]   asking, "What are you going to SPAC?"
[00:01:16.800 --> 00:01:20.360]   But IPO D and F, the money has been returned to investors.
[00:01:20.360 --> 00:01:23.020]   No, it's going to be returned.
[00:01:23.020 --> 00:01:24.060]   Going to be returned to investors.
[00:01:24.060 --> 00:01:25.060]   Thank you.
[00:01:25.060 --> 00:01:27.740]   And Bill Ackman, of course, he wound down his SPAC, returning $4 billion.
[00:01:27.740 --> 00:01:28.820]   There's over 500 SPACs out.
[00:01:28.820 --> 00:01:32.000]   There's over 500 SPACs out there looking for deals.
[00:01:32.000 --> 00:01:34.140]   Tell us why this decision.
[00:01:34.140 --> 00:01:39.700]   I've raised 10 SPACs, six in technology and four in biotechnology.
[00:01:39.700 --> 00:01:43.600]   And I've done six deals, two in tech and two in biotech.
[00:01:43.600 --> 00:01:45.600]   Four in tech.
[00:01:45.600 --> 00:01:47.060]   Sorry, four in tech.
[00:01:47.060 --> 00:01:49.380]   Four in tech, thank you, and two in biotech.
[00:01:49.380 --> 00:01:52.360]   So the reason to shut it down is pretty straightforward.
[00:01:52.360 --> 00:01:57.720]   It's like when we launched these things, the stock market was in a much different place
[00:01:57.720 --> 00:01:58.600]   than it is today.
[00:01:58.600 --> 00:02:05.020]   And so over the last two years, looking at deals, it's gotten harder and harder to find
[00:02:05.020 --> 00:02:06.740]   a good risk reward.
[00:02:06.740 --> 00:02:07.740]   Now why is that?
[00:02:07.740 --> 00:02:12.320]   Well, the thing with the SPAC is you do a deal today, but it doesn't usually close for
[00:02:12.320 --> 00:02:15.220]   six or seven months in the future.
[00:02:15.220 --> 00:02:20.820]   And so you have to do a deal where you have a really good sense that in six or seven months
[00:02:20.820 --> 00:02:27.500]   when the deal comes to close, that the price will be the same or even higher than what
[00:02:27.500 --> 00:02:28.380]   it is today.
[00:02:28.380 --> 00:02:35.280]   And if it isn't, all of the investors that you've brought along in the SPAC have a right
[00:02:35.280 --> 00:02:39.620]   to redeem, which is to say they file a notice that says, "You can complete the deal, but
[00:02:39.620 --> 00:02:41.540]   I want my money back."
[00:02:41.540 --> 00:02:45.240]   And what they get back is the initial $10 that they used to buy the stock in the first
[00:02:45.240 --> 00:02:46.240]   place.
[00:02:46.240 --> 00:02:50.640]   Because when we started the SPAC, we sold stock at $10.
[00:02:50.640 --> 00:02:55.700]   And so from my perspective, I was looking at this and I'm like, "This is a super volatile,
[00:02:55.700 --> 00:02:57.020]   ugly point in the market.
[00:02:57.020 --> 00:02:58.160]   This last year has been really problematic.
[00:02:58.160 --> 00:03:04.020]   And I kind of said this last November, and Nick, we can play the clip after and we can
[00:03:04.020 --> 00:03:05.300]   come back to it."
[00:03:05.300 --> 00:03:11.720]   But basically, my decision was that at this point to do a deal would probably put a lot
[00:03:11.720 --> 00:03:14.160]   of capital at risk.
[00:03:14.160 --> 00:03:19.820]   And in all of these deals, I'm typically investing $100 million at least in each of them.
[00:03:19.820 --> 00:03:21.140]   And so I couldn't justify that.
[00:03:21.140 --> 00:03:22.660]   I couldn't see a good risk reward.
[00:03:22.660 --> 00:03:27.940]   And I thought the right thing to do, the responsible thing to do was just to wind these things down.
[00:03:27.940 --> 00:03:28.940]   And I think that's what we're doing.
[00:03:28.940 --> 00:03:30.940]   We're not going to lose, I don't know, 10, 15, 20 million bucks for having set these
[00:03:30.940 --> 00:03:32.440]   things up.
[00:03:32.440 --> 00:03:34.960]   But we give everybody their money back, that $10.
[00:03:34.960 --> 00:03:39.220]   And I think that's actually better over the next five or six months than what it'll otherwise
[00:03:39.220 --> 00:03:40.720]   do if you're invested in the market.
[00:03:40.720 --> 00:03:43.520]   Now that's a belief that I have.
[00:03:43.520 --> 00:03:46.500]   But hopefully when people get the $10 back in the next few weeks, if they want, they
[00:03:46.500 --> 00:03:49.600]   can go and put that money back in the market.
[00:03:49.600 --> 00:03:50.600]   And hopefully they'll do well.
[00:03:50.600 --> 00:03:53.780]   But you know, from my perspective, the risk reward was not good.
[00:03:53.780 --> 00:03:57.720]   Is part of the issue the inventory that's available of great companies as well?
[00:03:57.720 --> 00:04:02.340]   That's one of the things I heard speculated on CNBC, it's hard to convince a private company
[00:04:02.340 --> 00:04:03.340]   to go public.
[00:04:03.340 --> 00:04:04.340]   Here's my experience.
[00:04:04.340 --> 00:04:09.320]   You know, when I when I was talking to all of the CEOs of the Silicon Valley companies,
[00:04:09.320 --> 00:04:13.200]   initially there was a lot of misunderstanding about what specs were.
[00:04:13.200 --> 00:04:18.180]   And I think we were able to dispel that because we had some really successful transactions.
[00:04:18.180 --> 00:04:21.440]   Then there was a lot of interest in being a part of it.
[00:04:21.440 --> 00:04:25.100]   In this phase, we were suffering from two very important things.
[00:04:25.100 --> 00:04:27.500]   One was that valuations were just completely unvaluable.
[00:04:27.500 --> 00:04:28.800]   And that was completely up in the air.
[00:04:28.800 --> 00:04:33.480]   People had a huge question mark on late stage valuations, because we would come in, we would
[00:04:33.480 --> 00:04:36.500]   do the work, and we would say the company's worth x.
[00:04:36.500 --> 00:04:43.120]   And that number typically was 50 or 60% lower than their last private valuation.
[00:04:43.120 --> 00:04:48.600]   And so when it came time for us to negotiate, you know, doing the deal, even if the founder
[00:04:48.600 --> 00:04:52.400]   was roughly on board, the rest of the board was not.
[00:04:52.400 --> 00:04:57.280]   Because a lot of them would have seen some pretty meaningful markdowns in their private
[00:04:57.280 --> 00:04:58.860]   assets.
[00:04:58.860 --> 00:05:03.220]   And when the company had enough money to kind of like, you know, at least stay private for
[00:05:03.220 --> 00:05:06.780]   another year or so without having to raise money.
[00:05:06.780 --> 00:05:12.240]   On balance, those investors felt it was more prudent for them to not take the mark and
[00:05:12.240 --> 00:05:16.200]   to not take the deal at such a lower discount.
[00:05:16.200 --> 00:05:20.200]   So that was a big issue that we ran into because every time we would price a deal, again, we're
[00:05:20.200 --> 00:05:24.880]   trying to create a margin of safety for us and our investors, again, because it's going
[00:05:24.880 --> 00:05:27.060]   to take six to nine months to close a deal.
[00:05:27.060 --> 00:05:29.880]   Ideally, you want to valuation indigestion.
[00:05:29.880 --> 00:05:31.700]   And then sorry, the second thing was just volatility.
[00:05:31.700 --> 00:05:38.060]   So when you see volatility in the public markets, you know, for a CEO, and I can understand
[00:05:38.060 --> 00:05:43.480]   this, it's much easier to go public in a point where the markets are generally going in one
[00:05:43.480 --> 00:05:48.220]   direction because it gives them the confidence to be able to learn the ropes because it is
[00:05:48.220 --> 00:05:51.080]   a complicated process being public.
[00:05:51.080 --> 00:05:55.680]   And when you introduce tremendous market based volatility independent of your company, I
[00:05:55.680 --> 00:05:56.840]   think a lot of CEOs.
[00:05:56.840 --> 00:06:01.480]   And CFOs and IR people were a little nervous.
[00:06:01.480 --> 00:06:02.700]   You want to take the risk.
[00:06:02.700 --> 00:06:04.260]   And that was this, that was the second piece.
[00:06:04.260 --> 00:06:09.920]   But by in a way, the first one was valuation, we could not find market clearing prices.
[00:06:09.920 --> 00:06:11.340]   Makes sense for assets.
[00:06:11.340 --> 00:06:14.980]   And by the way, and I'll talk about one company in specific, which just this week had a pretty
[00:06:14.980 --> 00:06:16.880]   public blow up around this whole issue.
[00:06:16.880 --> 00:06:21.100]   And look, I've been pretty clear about this since November, the marginal trade should
[00:06:21.100 --> 00:06:22.800]   have been to be trimming risk.
[00:06:22.800 --> 00:06:26.220]   And Nick, maybe you can play the clip because I really want to make sure that it's very
[00:06:26.220 --> 00:06:26.620]   clear.
[00:06:26.620 --> 00:06:30.400]   But I'm going to put it on the record what I said, almost an entire year ago.
[00:06:30.400 --> 00:06:35.280]   Let me put crypto in the context of the markets and where we are today at the end of the week.
[00:06:35.280 --> 00:06:39.480]   After you know, Q3 earnings in November of 2021.
[00:06:39.480 --> 00:06:43.840]   We have the stock market at absolute all time highs.
[00:06:43.840 --> 00:06:44.780]   Ripping.
[00:06:44.780 --> 00:06:48.040]   We have crypto at absolute all time highs.
[00:06:48.040 --> 00:06:49.040]   Ripping.
[00:06:49.040 --> 00:06:50.040]   We have the art markets.
[00:06:50.040 --> 00:06:53.140]   I don't know if you guys saw Phillips and Christie's and Sotheby's this past week at
[00:06:53.140 --> 00:06:55.080]   absolute all time highs.
[00:06:55.080 --> 00:06:56.400]   Sold another people for 25 million.
[00:06:56.400 --> 00:07:00.040]   We have inflation at a 30 year high.
[00:07:00.040 --> 00:07:04.300]   We have 10 year break evens at a 25 year high.
[00:07:04.300 --> 00:07:08.480]   We have, you know, one point, some odd trillion dollars that we just approved last weekend.
[00:07:08.480 --> 00:07:13.500]   We're still horse trading on another three, you know, $1.8 trillion of stimulus that we're
[00:07:13.500 --> 00:07:14.500]   going to put in.
[00:07:14.500 --> 00:07:20.020]   And I think the most important thing, which is the two most important founders of our
[00:07:20.020 --> 00:07:25.180]   generation, the two smartest people who have really consistently won Elon Musk and Jeff
[00:07:25.180 --> 00:07:26.180]   Bezos.
[00:07:26.180 --> 00:07:30.340]   I think they have more than $11 billion of their holdings this year alone.
[00:07:30.340 --> 00:07:34.860]   And if you can't take all of that and decide for yourself, what's right for you and your
[00:07:34.860 --> 00:07:37.480]   family, you're doing yourself a disservice.
[00:07:37.480 --> 00:07:41.940]   I think it's important for me to never sort of like, you know, be forced to tell folks
[00:07:41.940 --> 00:07:45.100]   whether I'm buying or selling, although I'm willing to do it in moments where I think
[00:07:45.100 --> 00:07:47.700]   it's important.
[00:07:47.700 --> 00:07:50.360]   But I think it's really important to understand the context.
[00:07:50.360 --> 00:07:54.860]   And so I think like these folks that like think derisively about individuals who are
[00:07:54.860 --> 00:07:55.960]   managing risk.
[00:07:55.960 --> 00:07:57.540]   I think it's really naive.
[00:07:57.540 --> 00:08:01.160]   And I think it's it creates a lot of missed opportunity for them as well.
[00:08:01.160 --> 00:08:07.220]   If the smartest people in the world are now selling their core holdings that they told
[00:08:07.220 --> 00:08:14.920]   you they would never sell, and you are not reconsidering your position on things.
[00:08:14.920 --> 00:08:20.820]   You're either much smarter than them, or you're being really, really reckless.
[00:08:20.820 --> 00:08:25.740]   The reason I said all of those things was because I was getting really worried about
[00:08:25.740 --> 00:08:26.740]   where we were.
[00:08:26.740 --> 00:08:32.860]   And then, and then I think on the heels of that, I published a tweet and I and I said,
[00:08:32.860 --> 00:08:37.760]   I'm starting to sell, you know, and I sold like $100 million of so far, but then I was
[00:08:37.760 --> 00:08:43.880]   a systematic seller through the end of last year and through this year, to try to manage
[00:08:43.880 --> 00:08:48.760]   my own liquidity, because it changed profoundly as I saw what was happening in the markets.
[00:08:48.760 --> 00:08:54.360]   So you know, I think that it's just an important thing to call out that things don't always
[00:08:54.360 --> 00:08:55.520]   go up.
[00:08:55.520 --> 00:09:00.580]   And you have to pay attention to a mosaic of information.
[00:09:00.580 --> 00:09:05.240]   And you have to do the work yourself because you know, your situation is unique to you,
[00:09:05.240 --> 00:09:07.040]   nobody else understands it.
[00:09:07.040 --> 00:09:10.280]   And so you can't outsource that decision to somebody else.
[00:09:10.280 --> 00:09:13.820]   Obviously, you can use it as a guide, you know, it's fair to say, you know, when the
[00:09:13.820 --> 00:09:18.000]   13 F's of important hedge funds come out, do I read them, of course, because I'm trying
[00:09:18.000 --> 00:09:22.140]   to figure out, you know, what don't I know, what may I be missing, maybe there's a great
[00:09:22.140 --> 00:09:25.300]   company in there that I that I should be taking a look at.
[00:09:25.300 --> 00:09:29.280]   But you know, I'm not dissimilar to everybody else in that I use other people that I look
[00:09:29.280 --> 00:09:33.300]   up to or respect as a leading indicator of what to buy.
[00:09:33.300 --> 00:09:35.700]   But I still take responsibility for my own decisions.
[00:09:35.700 --> 00:09:39.760]   And I manage my liquidity as best as I can based on my conditions.
[00:09:39.760 --> 00:09:41.940]   And those are always changing.
[00:09:41.940 --> 00:09:45.400]   And so I think this is just a good reminder that everybody else has to do the same.
[00:09:45.400 --> 00:09:47.820]   So Chamath, what do you think happens to the remaining 500?
[00:09:47.820 --> 00:09:53.120]   Every SPAC sponsor friend of mine that I know, is not seeking targets anymore.
[00:09:53.120 --> 00:09:55.080]   They're all expecting to wind up and return capital.
[00:09:55.080 --> 00:09:58.220]   How many of the 500 do you think will actually find a target?
[00:09:58.220 --> 00:10:00.920]   And how many do you think all of remaining SPACs?
[00:10:00.920 --> 00:10:04.440]   How many of them will wind up and return capital?
[00:10:04.440 --> 00:10:09.020]   I mean, I think that there's still some really good deals to do.
[00:10:09.020 --> 00:10:11.740]   They're not necessarily in tech per se.
[00:10:11.740 --> 00:10:14.260]   So you know, I think you did a really great deal in ag tech.
[00:10:14.260 --> 00:10:15.800]   I think that that's that's really interesting.
[00:10:15.800 --> 00:10:17.520]   So time will tell how that does.
[00:10:17.520 --> 00:10:19.980]   I think there's some really interesting deals in energy.
[00:10:19.980 --> 00:10:23.960]   In fact, we spent a lot of time actually, pivoting and spending time and energy looking
[00:10:23.960 --> 00:10:24.860]   at, you know, everything.
[00:10:24.860 --> 00:10:31.140]   From, you know, producers of nat gas and oil to, you know, folks that were building terminals
[00:10:31.140 --> 00:10:36.180]   to LNG facilities and the but it's just a very different return profile than what we
[00:10:36.180 --> 00:10:37.600]   were used to.
[00:10:37.600 --> 00:10:43.620]   So I think if you can understand some of these other markets, and you can underwrite to a
[00:10:43.620 --> 00:10:47.960]   different rate of return, some decent deals will get done.
[00:10:47.960 --> 00:10:53.460]   The overwhelming majority of the tech SPACs I think probably will just wind up some folks
[00:10:53.460 --> 00:10:54.640]   will be.
[00:10:54.640 --> 00:11:02.420]   Really focused on trying to, you know, monetize their founder promote, so they'll do any kind
[00:11:02.420 --> 00:11:03.900]   of deal.
[00:11:03.900 --> 00:11:07.980]   The problem as you see is even whether you do a good deal or bad deal, with the kind
[00:11:07.980 --> 00:11:10.880]   of volatility we're seeing in the market, the likelihood is that it's going to trade
[00:11:10.880 --> 00:11:12.160]   down pretty significantly.
[00:11:12.160 --> 00:11:15.200]   So I think most will not find a home.
[00:11:15.200 --> 00:11:19.460]   I think some will find some really interesting targets in areas like energy, I think are
[00:11:19.460 --> 00:11:20.460]   really interesting.
[00:11:20.460 --> 00:11:24.420]   Agriculture, like what you did freeberg, I think is really interesting deal.
[00:11:24.420 --> 00:11:30.040]   And then you know what, it'll be an opportunity for us to retool the SPAC.
[00:11:30.040 --> 00:11:32.080]   I don't think it's going to go away.
[00:11:32.080 --> 00:11:36.240]   I think that it is a useful tool in a toolbox of many tools.
[00:11:36.240 --> 00:11:41.040]   So there's IPOs, there's direct listings.
[00:11:41.040 --> 00:11:43.320]   There's SPACs.
[00:11:43.320 --> 00:11:45.920]   There's obviously private fundraising.
[00:11:45.920 --> 00:11:48.060]   There's convertible, there's structured deals.
[00:11:48.060 --> 00:11:49.480]   So it is a tool.
[00:11:49.480 --> 00:11:52.620]   And I think when used properly, it can be really helpful, you know, for the companies
[00:11:52.620 --> 00:11:54.200]   that we work with.
[00:11:54.200 --> 00:11:57.920]   It would have been impossible for them to raise the quantums of capital that they did
[00:11:57.920 --> 00:12:01.320]   primary capital from sophisticated hedge funds and mutual funds.
[00:12:01.320 --> 00:12:06.100]   You know, we raised billions and billions of dollars for companies like SoFi and Opendoor.
[00:12:06.100 --> 00:12:09.100]   And I think that that's going to go a long way for them to achieve their goals.
[00:12:09.100 --> 00:12:11.520]   So I think it's going to be a good tool.
[00:12:11.520 --> 00:12:15.360]   But we're going to go through a washout of most of these folks who are not going to be
[00:12:15.360 --> 00:12:16.820]   able to successfully find targets.
[00:12:16.820 --> 00:12:20.480]   Sachs, when we look at private companies, especially the late stage ones, you and I
[00:12:20.480 --> 00:12:23.160]   are involved in a number of them.
[00:12:23.160 --> 00:12:23.980]   They're doing rifts.
[00:12:23.980 --> 00:12:28.280]   They've got headwinds and their valuations are underwater in many cases.
[00:12:28.280 --> 00:12:33.640]   How are they thinking about the public market windows, SPACs, direct listings, or just a
[00:12:33.640 --> 00:12:34.640]   traditional IPO?
[00:12:34.640 --> 00:12:36.380]   I don't know that they are thinking about it.
[00:12:36.380 --> 00:12:40.040]   I just think I think like going public seems like a fantasy at this point.
[00:12:40.040 --> 00:12:45.180]   I think the whole public markets exit idea is frozen for two years.
[00:12:45.180 --> 00:12:46.180]   Two years.
[00:12:46.180 --> 00:12:48.840]   Yeah, I think that's probably what people are thinking.
[00:12:48.840 --> 00:12:53.760]   What are these late stage companies that raised mega rounds doing to work out valuation?
[00:12:53.760 --> 00:12:59.020]   Issues because as Chamath just pointed out with SPACs, one of the problems was clearing
[00:12:59.020 --> 00:13:03.140]   market the late stage investors who came in, maybe they you know, don't want to accept
[00:13:03.140 --> 00:13:06.260]   the haircut and valuation even if they have productive,
[00:13:06.260 --> 00:13:07.260]   probably not enough.
[00:13:07.260 --> 00:13:12.760]   I mean, what they're doing is is using their war chest to grow into their valuations, which
[00:13:12.760 --> 00:13:14.920]   is the smart strategy.
[00:13:14.920 --> 00:13:19.400]   And if they're really smart, they'll be slashing their burn while they do that.
[00:13:19.400 --> 00:13:22.160]   But yeah, it's no different than we've talked about before.
[00:13:22.160 --> 00:13:23.540]   I think that the main significance.
[00:13:23.540 --> 00:13:28.540]   The significance of this past week is that you had the Fed meeting, it raised another
[00:13:28.540 --> 00:13:29.680]   75 basis points.
[00:13:29.680 --> 00:13:31.580]   There's no surprise that that was predicted.
[00:13:31.580 --> 00:13:34.020]   What was new was the forecast.
[00:13:34.020 --> 00:13:38.540]   They're now saying that they expect to raise another one and a half percent just two months
[00:13:38.540 --> 00:13:43.760]   ago at the last meeting, they were saying 75 at this meeting plus 50 after that.
[00:13:43.760 --> 00:13:47.140]   Now they're saying 150 after this.
[00:13:47.140 --> 00:13:53.320]   So you know, in just two months, they've they've they're now revised their forecast for an
[00:13:53.320 --> 00:13:54.320]   hour.
[00:13:54.320 --> 00:13:56.080]   And they're saying that they're going to raise another 100 basis points of rate increases.
[00:13:56.080 --> 00:13:57.080]   Why?
[00:13:57.080 --> 00:13:58.180]   Because inflation is worse than they thought.
[00:13:58.180 --> 00:14:05.080]   So things are worse than we thought two months ago, the last inflation print didn't get better
[00:14:05.080 --> 00:14:06.200]   as fast as people wanted.
[00:14:06.200 --> 00:14:07.840]   We talked about that last week.
[00:14:07.840 --> 00:14:13.200]   Now the Fed is revising its forecast trajectory is bad and getting worse.
[00:14:13.200 --> 00:14:18.900]   And you finally had Powell kind of throw in the towel on his rhetoric around soft landing.
[00:14:18.900 --> 00:14:23.100]   I mean, first it was, well, we can raise rates and we won't have a recession.
[00:14:23.100 --> 00:14:26.460]   Then it was we might have like a mild recession.
[00:14:26.460 --> 00:14:32.760]   Now he's basically saying hard landing if you if you read these FOMC comments.
[00:14:32.760 --> 00:14:36.720]   So I think this was a really bad week for the economy.
[00:14:36.720 --> 00:14:38.100]   And you're seeing it in the markets this week.
[00:14:38.100 --> 00:14:44.120]   I mean, we are retracing almost within 5% of the June lows, with the growth stocks being
[00:14:44.120 --> 00:14:45.980]   the ones that are hardest hit.
[00:14:45.980 --> 00:14:50.860]   By the way, speaking of growth stocks, have you guys been following this trials and tribulations
[00:14:50.860 --> 00:14:52.880]   with by Jews, which is like the one of the largest?
[00:14:52.880 --> 00:14:57.520]   I mean, I think it's the largest valued private companies, but it's a private Indian edtech
[00:14:57.520 --> 00:14:58.520]   company.
[00:14:58.520 --> 00:15:03.200]   Essentially, I think like, you know, there was this delayed audit.
[00:15:03.200 --> 00:15:05.140]   And Deloitte couldn't certify a bunch of things.
[00:15:05.140 --> 00:15:08.060]   And then finally, they were able to come up with a report that essentially showed instead
[00:15:08.060 --> 00:15:13.860]   of like, breaking even or making money, they lost almost $600 million this year, and that
[00:15:13.860 --> 00:15:20.320]   a lot of the revenue that they were booking were actually loans to millions of Indian
[00:15:20.320 --> 00:15:22.660]   families who had basically zero probability of paying.
[00:15:22.660 --> 00:15:24.040]   They're paying them back.
[00:15:24.040 --> 00:15:27.520]   So a lot of the revenue was not real as well.
[00:15:27.520 --> 00:15:29.380]   And this is like a who's who of investors.
[00:15:29.380 --> 00:15:37.100]   It's everybody from, you know, Sequoia, Tiger, UBS, BlackRock, Chan Zuckerberg, it's incredible.
[00:15:37.100 --> 00:15:40.380]   I think when the tide goes out, we find out right, and this is what's happening, the tides
[00:15:40.380 --> 00:15:44.300]   out and you're going to find all the weakness in the system, it's all going to get flushed.
[00:15:44.300 --> 00:15:48.300]   I think this is a bit of an outlier, obviously, like, I don't think that, you know, all these
[00:15:48.300 --> 00:15:52.440]   companies that are worth 10s of billions of dollars are running so close to the sun.
[00:15:52.440 --> 00:15:57.800]   But it just goes to show you that, you know, how does how does this happen?
[00:15:57.800 --> 00:16:01.200]   Like, how, how do you not get a data?
[00:16:01.200 --> 00:16:07.020]   This on the seed stage, in series A, we would ask for a data room, the person would say
[00:16:07.020 --> 00:16:11.760]   you're the only investor asking for it, and you're putting in, you know, 10% of the round,
[00:16:11.760 --> 00:16:14.980]   these other people aren't even asking for it was was that okay, can we still see the
[00:16:14.980 --> 00:16:18.760]   data room and turned out there wasn't one, people were just making bets, they were making
[00:16:18.760 --> 00:16:22.220]   blind bets, they were betting, without even looking at their cards, you're thinking about
[00:16:22.220 --> 00:16:24.100]   the other people's cards.
[00:16:24.100 --> 00:16:28.080]   This goes back to what Friedberg was saying before, you know, I think I think there's
[00:16:28.080 --> 00:16:33.740]   a there's a sliver of the retail investor base that is not dissimilar to a sliver of
[00:16:33.740 --> 00:16:37.760]   the private equity and hedge fund and venture capital space, which is, these folks are not
[00:16:37.760 --> 00:16:39.180]   willing to do the work.
[00:16:39.180 --> 00:16:41.160]   I think most people are.
[00:16:41.160 --> 00:16:43.160]   And most people want to come to their own conclusions.
[00:16:43.160 --> 00:16:46.800]   But some people want to just take the easy momentum driven decision.
[00:16:46.800 --> 00:16:52.000]   And they typically always get punished over time, there's this concept of adverse selection.
[00:16:52.000 --> 00:16:56.700]   Which is that the negative actors will find them will seek them out, and it will be a
[00:16:56.700 --> 00:16:57.880]   match.
[00:16:57.880 --> 00:17:02.780]   And so they'll ultimately get adversely selected into the deals that blow them up.
[00:17:02.780 --> 00:17:06.580]   So the bad deals, what you're saying is the bad deals would find the bad investors who
[00:17:06.580 --> 00:17:10.060]   don't do the work, and then they spiral and crash together.
[00:17:10.060 --> 00:17:11.500]   That's a really brilliant observation.
[00:17:11.500 --> 00:17:13.640]   Yeah, yeah, I think I think the other that's what I'm here.
[00:17:13.640 --> 00:17:18.760]   I think the other thing is like you have this situation where if it wasn't that it's folks
[00:17:18.760 --> 00:17:21.780]   outsourcing their diligence to the person that did the round before.
[00:17:21.780 --> 00:17:22.780]   Totally.
[00:17:22.780 --> 00:17:24.540]   You know, oh, Silver Lake did it.
[00:17:24.540 --> 00:17:25.540]   Same problem.
[00:17:25.540 --> 00:17:26.540]   Tiger did it.
[00:17:26.540 --> 00:17:30.780]   Okay, they're these guys are very sophisticated investors, I don't need to do the work.
[00:17:30.780 --> 00:17:39.380]   It turned out you did, because you know, it's it was a very cavalier way of recognizing
[00:17:39.380 --> 00:17:41.920]   revenue as there was a specific playbook here to that.
[00:17:41.920 --> 00:17:46.140]   I don't know if you saw this sex in the early stage, where people would get their friends
[00:17:46.140 --> 00:17:50.220]   to invest in the company, or some, you know, high net worth individuals, and then you look
[00:17:50.220 --> 00:17:51.560]   at the deal.
[00:17:51.560 --> 00:17:53.440]   And then you say, well, valuation didn't make sense.
[00:17:53.440 --> 00:17:57.020]   And then you say, well, we would always ask how much money is each person putting in and
[00:17:57.020 --> 00:18:02.220]   we like, well, their normal bet size in the seed is 750 or 1.5 million.
[00:18:02.220 --> 00:18:04.060]   But they're putting 50k into this or 100k.
[00:18:04.060 --> 00:18:05.060]   What's going on here?
[00:18:05.060 --> 00:18:06.660]   And it was, oh, they're friends with them.
[00:18:06.660 --> 00:18:09.660]   They worked at a previous company together, they were in the same, they went to the same
[00:18:09.660 --> 00:18:10.660]   college.
[00:18:10.660 --> 00:18:15.500]   And so people were using social proof, but manipulating it to get some other sucker at
[00:18:15.500 --> 00:18:18.660]   the table to pay full price and not due diligence.
[00:18:18.660 --> 00:18:20.340]   Really strange behavior.
[00:18:20.340 --> 00:18:21.340]   I am.
[00:18:21.340 --> 00:18:27.200]   I was in Singapore this week, there was a I had this great meeting with this young investor,
[00:18:27.200 --> 00:18:28.200]   really dynamic guy.
[00:18:28.200 --> 00:18:33.460]   And he was telling me about a company in Indonesia, that he didn't invest in.
[00:18:33.460 --> 00:18:38.560]   But it turned out that the that this founder was literally running two parallel sets of
[00:18:38.560 --> 00:18:40.560]   accounting systems.
[00:18:40.560 --> 00:18:45.480]   And so he was, you know, showing a business and fundraising from this set.
[00:18:45.480 --> 00:18:49.400]   But the the real books were over here, and it looked a completely different system.
[00:18:49.400 --> 00:18:51.120]   And it was basically like a Ponzi scheme.
[00:18:51.120 --> 00:18:54.680]   And, you know, he was telling me, it's like, it's like impossible to root these things
[00:18:54.680 --> 00:18:55.680]   out.
[00:18:55.680 --> 00:19:00.120]   So what he said he relies on is like, you have to have a network, when you're doing
[00:19:00.120 --> 00:19:05.380]   these frontier country deals, where, you know, he says, I need to find at least 10 people
[00:19:05.380 --> 00:19:11.300]   that know this person, so that there is sort of like a moral social proof, and moral diligence
[00:19:11.300 --> 00:19:15.500]   that happens, because that person will never try to commit something that egregious in
[00:19:15.500 --> 00:19:18.100]   the face of all of their friends.
[00:19:18.100 --> 00:19:20.900]   And so, you know, that's a mechanism of filtering this stuff out.
[00:19:20.900 --> 00:19:24.840]   And so I thought that was a really interesting way of, of designing a diligence process, in
[00:19:24.840 --> 00:19:28.980]   at least in a frontier market here, I don't think you have that much time to do these
[00:19:28.980 --> 00:19:32.180]   really, you know, fast paced deals.
[00:19:32.180 --> 00:19:36.940]   And the social proof matters less, because you theoretically, you know, are looking for
[00:19:36.940 --> 00:19:39.560]   signals of traction.
[00:19:39.560 --> 00:19:42.980]   But there has to be a better systematic way of getting this diligence done, because these
[00:19:42.980 --> 00:19:44.460]   things should be pretty obvious.
[00:19:44.460 --> 00:19:47.460]   Sachs, well, before it's a well before it's a multi deca billion dollar company,
[00:19:47.460 --> 00:19:49.460]   Sachs, last year, when people were moving really fast saying they don't have time to do this,
[00:19:49.460 --> 00:19:49.460]   they're not going to do this.
[00:19:49.460 --> 00:19:49.460]   And so I think that's a really interesting way of designing a diligence process.
[00:19:49.460 --> 00:19:49.460]   And I think that's a really interesting way of designing a diligence process.
[00:19:49.460 --> 00:19:49.460]   And I think that's a really interesting way of designing a diligence process.
[00:19:49.460 --> 00:19:51.940]   So you're moving really fast saying they don't have time for diligence, you're going to lose
[00:19:51.940 --> 00:19:55.300]   the deal, yada, yada, how did you approach diligence during those peak periods?
[00:19:55.300 --> 00:19:58.740]   And did you have those experiences where people were trying to push you to close,
[00:19:58.740 --> 00:20:03.060]   without talking to customers or looking at bank statements, yada, yada?
[00:20:03.060 --> 00:20:05.780]   No, we wouldn't play that game, because we always run SaaS metrics.
[00:20:05.780 --> 00:20:07.300]   That's always the starting point for us.
[00:20:07.300 --> 00:20:13.620]   But you know, most SaaS companies, they have SaaS metrics, but I mean, they it's so standardized,
[00:20:13.620 --> 00:20:15.540]   that it would be such a red flag if they didn't.
[00:20:15.540 --> 00:20:19.220]   Yeah, I think it might be companies that are, you know, have unusual
[00:20:19.220 --> 00:20:19.240]   business models, you know, they're not going to be able to get the right business model.
[00:20:19.240 --> 00:20:22.020]   business models, or maybe they would say something like that.
[00:20:22.020 --> 00:20:25.860]   But no, we can never do a SaaS investment without SaaS metrics.
[00:20:25.860 --> 00:20:29.860]   All right. Well, this is a good segue, because right now, US venture capitalists are sitting on
[00:20:29.860 --> 00:20:35.420]   $290 billion in dry powder, we had talked about this last year, how much dry powder was there,
[00:20:35.420 --> 00:20:40.620]   the market is obviously collapsed. But here's a chart from our friends over at PitchBook.
[00:20:40.620 --> 00:20:44.900]   Just extraordinary how much has built up and how much has been raised.
[00:20:44.900 --> 00:20:49.020]   US VC funds raised $121 billion during the
[00:20:49.020 --> 00:20:56.000]   first half of this year 2022. So LPs still have an appetite, which kind of makes sense that investing
[00:20:56.000 --> 00:21:00.240]   into the down market for private companies means you're going to get better deals. And you have a
[00:21:00.240 --> 00:21:06.000]   10 year horizon. US VC has raised 139 billion in all of 2021. So if you put those two numbers
[00:21:06.000 --> 00:21:13.600]   together, yeah, you're looking at $260 billion in the last 18 months. And this is all record
[00:21:13.600 --> 00:21:17.960]   numbers being put up on the board. What does this say for private companies, Zach?
[00:21:17.960 --> 00:21:25.220]   I dispute this analysis a little bit. I think there's a couple of things going on that need to
[00:21:25.220 --> 00:21:30.860]   be taken into consideration. First of all, new funds don't get announced till after the process
[00:21:30.860 --> 00:21:36.020]   completes. And then, you know, may even be some time after that, when the VC firm feels like they
[00:21:36.020 --> 00:21:40.660]   want to make the announcement, you can't announce a fund until the process completely over, you get
[00:21:40.660 --> 00:21:47.740]   subject to all sorts of additional SEC rules. So, you know, these funds might be announced in 20,
[00:21:47.740 --> 00:21:54.340]   22, but they may have actually been raised in 2021. So that I think is a really important point.
[00:21:54.340 --> 00:21:58.900]   Moreover, a lot of the funds may have already deployed capital before the crash. So there was
[00:21:58.900 --> 00:22:05.300]   that, I think, remarkable story that we talked about months ago in TechCrunch on how the latest
[00:22:05.300 --> 00:22:11.300]   Tiger Fund, which wasn't even announced till March or April of 2022, but it had already been two
[00:22:11.300 --> 00:22:17.520]   thirds deployed by the time they even announced it. And so that was pretty stunning. So I, I
[00:22:17.520 --> 00:22:24.180]   think that we don't really have a great sense of how much of this so-called dry powder has already
[00:22:24.180 --> 00:22:29.940]   been deployed, how much of it was really raised before the crash. It is true that LP relationships
[00:22:29.940 --> 00:22:36.240]   with VC firms that have done well are sticky and good LPs stick with their partners during a
[00:22:36.240 --> 00:22:40.560]   downturn. So look, I mean, the VC world's not going out of business or anything like that,
[00:22:40.560 --> 00:22:46.380]   but I would tend to think that this is an overly optimistic, overly rosy scenario.
[00:22:46.380 --> 00:22:53.400]   Do VCs have new funds that they're going to be ready to deploy in great companies? Yes. But does
[00:22:53.400 --> 00:23:00.000]   this mean it's going to be easy? No, I think that the bar has gone up, valuations have gone down.
[00:23:00.000 --> 00:23:04.860]   Founders looking at this tweet storm, I would not get lulled into a false sense of security.
[00:23:04.860 --> 00:23:06.840]   Yeah, I agree with that.
[00:23:06.840 --> 00:23:10.440]   Just to explain that, there's probably a six month lag on when these funds are announced.
[00:23:10.440 --> 00:23:16.160]   The reason is there's 506B and C designations. Most people raise under 506B, which means you
[00:23:16.160 --> 00:23:20.420]   cannot even say that you're fundraising. Therefore, PitchBook can never have that data. So there's a
[00:23:20.420 --> 00:23:28.100]   lag and people were deploying at a very high velocity. Therefore, this number could be off 35%.
[00:23:28.100 --> 00:23:31.400]   Well, if people were deploying at a pace where they thought they were going to go back for a
[00:23:31.400 --> 00:23:37.640]   new fund every year, which is what it was looking like in 2020, 2021, that six month period might
[00:23:37.640 --> 00:23:45.940]   mean you've deployed half the fund. But look, if you just go back to a two and a half or three year
[00:23:45.940 --> 00:23:52.660]   pace of deployment, and before in 2021, we're at a one year pace of deployment, divide the
[00:23:52.660 --> 00:23:58.240]   availability of capital by two thirds. I mean, only one third as much will be deployed in any
[00:23:58.240 --> 00:24:04.660]   given year. That's a significant reduction. So yeah, I think founders should just be aware that
[00:24:04.660 --> 00:24:08.260]   the market's going to be a lot tighter. And I think given what we're seeing in the public markets
[00:24:08.260 --> 00:24:12.280]   this week, it doesn't look to me like it's going to get any better. It looks to me like we're
[00:24:12.280 --> 00:24:15.720]   headed for, I mean, I call it a double dip recession. I think, because of the pandemic.
[00:24:15.720 --> 00:24:20.100]   I think a couple of months ago, that's exactly what it's looking like. In fact, the Fed basically
[00:24:20.100 --> 00:24:26.340]   said as much the Fed said, that would be just marginally positive next quarter. So we'd bounce
[00:24:26.340 --> 00:24:33.720]   back to slightly positive growth on a real basis. But then, you know, expect it to go negative again,
[00:24:33.720 --> 00:24:39.180]   and you know, recession, once all these interest rates kick in. And by the way, I mean, kudos to
[00:24:39.180 --> 00:24:43.920]   Chamath for basically calling that, you know, when the Fed just a couple months ago was saying that
[00:24:43.920 --> 00:24:45.500]   so called neutral would be
[00:24:45.500 --> 00:24:49.040]   three to three and a half percent. Chamath was saying no, it's gonna be four and a half,
[00:24:49.040 --> 00:24:55.520]   five percent plus. Now the Fed just in two months has revised to saying that neutral is 4.6%,
[00:24:55.520 --> 00:25:01.820]   or something like that. And, and they don't think there's going to be any rate reductions in 2023.
[00:25:01.820 --> 00:25:03.620]   So this is not looking good.
[00:25:03.620 --> 00:25:10.100]   How much Chamath of the issue here is we don't the data that we're seeing,
[00:25:10.100 --> 00:25:15.280]   the ground truth we're seeing, as you would often say, might be very different than like the reports
[00:25:15.280 --> 00:25:19.960]   that are coming out, people are talking about inflation from, you know, 60 days ago, job reports
[00:25:19.960 --> 00:25:25.660]   that are 30 days old, 60 days old, we don't really have live data. It seems like our government
[00:25:25.660 --> 00:25:29.860]   doesn't use live data. When they make these decisions. Is that the accurate?
[00:25:29.860 --> 00:25:34.660]   Well, they unfortunately don't have access to it, really, you know, they are, they have empirical
[00:25:34.660 --> 00:25:41.140]   sampling. But to say that, you know, the the US economy is automated in a way where, you know,
[00:25:41.140 --> 00:25:45.060]   they can sit in front of some dashboard and, you know, see in real time what the true on
[00:25:45.060 --> 00:25:51.720]   the ground data is, is, is not really accurate, unfortunately, maybe there's a Manhattan project
[00:25:51.720 --> 00:25:57.300]   type, you know, effort to do that at some point for the United States, but it's not now. I'll give
[00:25:57.300 --> 00:26:03.120]   you a bit of bad news and a bit of good news. And this is just me kind of, you know, again,
[00:26:03.120 --> 00:26:06.480]   looking at the mosaic and, and kind of judging where we are today.
[00:26:06.480 --> 00:26:14.840]   The bad news is, I think that it's going to be a really tough, sticky time for the US consumer
[00:26:14.840 --> 00:26:22.280]   probably over the next 18 months. And so I tend to think that, you know, through the course of this
[00:26:22.280 --> 00:26:31.220]   year, and through 2023, and possibly even a little bit of 24, it's going to be a grind. Unemployment
[00:26:31.220 --> 00:26:44.620]   will go back up. Inflation will be sticky, real earnings will shrink, consumption will ebb and earnings will not be that bad.
[00:26:44.620 --> 00:26:53.260]   But the silver lining is, I think that we are starting a bottoming process for the equity
[00:26:53.260 --> 00:26:59.900]   markets. And I think that by the end of this year, or the early part of next year, most of that will
[00:26:59.900 --> 00:27:07.580]   be done. And the reason is that, you know, the equity markets, I think, do a reasonable job of
[00:27:07.580 --> 00:27:14.400]   one looking at the bond market, and then to looking six to nine months into the future, and pricing in
[00:27:14.400 --> 00:27:21.120]   that future today. And so by the end of this year, beginning of next year, I think that we will have
[00:27:21.120 --> 00:27:26.800]   kind of bottomed and we'll start to build a base. The thing to remind us though, is that, you know,
[00:27:26.800 --> 00:27:33.280]   let's just say a stock goes down 20 50%. Even if it rallies 50% from there, it's still 25% off from
[00:27:33.280 --> 00:27:37.760]   where people don't understand that people don't understand that climb back up the mountain. So I
[00:27:37.760 --> 00:27:43.180]   would just I would just think, you know, tell people that, you know, I think that David is right,
[00:27:43.180 --> 00:27:44.180]   I think that it's gonna,
[00:27:44.180 --> 00:27:50.680]   we're gonna feel this for a while. It's this inflation, as I've said for a long time is going
[00:27:50.680 --> 00:28:00.160]   to be sticky and persistent. I think you're going to see fed funds at or breaching 5%. And, but I
[00:28:00.160 --> 00:28:06.120]   think that in terms of, you know, risk assets will bottom out by the end of this year, beginning of
[00:28:06.120 --> 00:28:10.320]   next year. Freebird, what are your thoughts? You think we're in the process of bottoming
[00:28:10.320 --> 00:28:13.960]   out and it's going to be a year of this kind of schlock?
[00:28:13.960 --> 00:28:20.260]   through the muck? And what signs are you looking for that maybe we're getting out of it or turning
[00:28:20.260 --> 00:28:26.700]   a corner? I mean, Larry Summers had some good tweets this week. The weird, you know, the weird
[00:28:26.700 --> 00:28:31.700]   thing is Larry Summers seems to be like almost trying to make the case and make certain points,
[00:28:31.700 --> 00:28:40.860]   because he's not being listened to. It's, it's, it's so ironic and sad to watch, because he's such
[00:28:40.860 --> 00:28:47.580]   a thoughtful economist and has such a great point of view and experience to leverage here. And
[00:28:47.580 --> 00:28:52.700]   clearly, you know, he was banging the drums last year, and no one was listening. And then he got
[00:28:52.700 --> 00:28:58.620]   public about it. And now he's more repeatedly public about things. The point that he's made,
[00:28:58.620 --> 00:29:04.300]   which I think, plays into the political cycle question, which is where the tension arises, is in
[00:29:04.300 --> 00:29:09.660]   order to resolve ultimately, the inflation problem, you're going to have to see a significant
[00:29:09.660 --> 00:29:10.640]   increment in unemployment.
[00:29:10.640 --> 00:29:19.920]   And so when you raise interest rates, you know, generally, purchasing goes down, demand goes down,
[00:29:19.920 --> 00:29:26.240]   revenue goes down, layoffs happen, some businesses go bankrupt, etc. So then there's this trickle in
[00:29:26.240 --> 00:29:31.120]   the economy of less people being employed. And when that happens, it ultimately drives
[00:29:31.120 --> 00:29:35.840]   a political response, which is, hey, we're losing our jobs,
[00:29:35.840 --> 00:29:39.520]   people start asking their representatives do something about this in Congress.
[00:29:40.420 --> 00:29:45.300]   And then these programs and these things get passed, which themselves are inflationary.
[00:29:45.300 --> 00:29:51.940]   And that's why it's very hard to predict, ultimately, when and how this all gets resolved,
[00:29:51.940 --> 00:30:01.940]   because we seem to have an administration that is enacting and embracing inflationary policies
[00:30:01.940 --> 00:30:04.660]   to support what they consider to be economic growth and
[00:30:04.660 --> 00:30:10.200]   improved employment conditions in this country. And the unfortunate effect of many of those policies
[00:30:10.200 --> 00:30:15.560]   is inflation. And then it forces this difficult central bank decision making cycle. And so there's
[00:30:15.560 --> 00:30:20.600]   a tension right now that doesn't seem to have a clear path to resolution. That is why it's very
[00:30:20.600 --> 00:30:25.960]   hard to have a clear prediction here. We also have a very significant question overhanging
[00:30:25.960 --> 00:30:31.160]   all of these markets related to the price of energy, which is a key input to so many
[00:30:31.160 --> 00:30:37.640]   industries and drives cost, as well as food, and also the military conflict in Eastern Europe.
[00:30:37.640 --> 00:30:39.980]   And, you know, we and then, you know, we've been talking about this for a long time, but we've been
[00:30:39.980 --> 00:30:42.120]   talking about this for a long time. And then there's in the financial markets, this big
[00:30:42.120 --> 00:30:46.960]   overhang question on what's going to happen with various countries that may default on their debt,
[00:30:46.960 --> 00:30:53.000]   as well as China's real estate bubble bursting. So I made this point, I think a few episodes ago,
[00:30:53.000 --> 00:30:56.600]   but there's no easy answer that I can just say deterministically, here's my prediction of what's
[00:30:56.600 --> 00:31:01.120]   going to happen. As Chamath uses the term, I think it's a great term, there's this mosaic of things
[00:31:01.120 --> 00:31:06.320]   that are under under consideration right now. And there's a tension between them all. And,
[00:31:06.320 --> 00:31:09.760]   and that's what makes it difficult. I'm sorry, I didn't really answer the question.
[00:31:09.760 --> 00:31:11.740]   But that's, that's kind of how I think.
[00:31:11.740 --> 00:31:17.300]   There's a lot of geopolitical risk. I mean, we're kind of, you know, ignoring what happened this
[00:31:17.300 --> 00:31:22.540]   week, where Putin basically is putting nukes back on the table. Now, I'm not saying that's likely to
[00:31:22.540 --> 00:31:28.580]   happen. But I don't know how again, I don't know how this market gets a lot better with the risk of
[00:31:28.580 --> 00:31:33.420]   war three hanging over our heads. I mean, who wants to enter the market without and by the way,
[00:31:33.420 --> 00:31:39.540]   the nukes just just to be clear, you know, you can hear certain military commanders speaking publicly
[00:31:39.540 --> 00:31:50.220]   about this. But in the Russian military playbooks, there is specifically defined actions that can lead
[00:31:50.220 --> 00:31:55.660]   to tactical nuclear weapon use in the field, there's no direct indication that these things are
[00:31:55.660 --> 00:32:01.140]   going to be used right away. But the as Saks says, there's like this weird, like, turning up the
[00:32:01.140 --> 00:32:07.080]   volume happening on, hey, maybe we're getting closer to a point where if Putin is having tactical
[00:32:07.080 --> 00:32:08.840]   failure in this conflict,
[00:32:09.320 --> 00:32:13.640]   there's more weaponry he can use that has greater impact. And unfortunately, there are these tactical
[00:32:13.640 --> 00:32:20.960]   nukes in his arsenal. And you know, a guy that maybe has a certain psychology that has, as our
[00:32:20.960 --> 00:32:27.560]   friends have said his back against the wall. He's not a person who in his career, or in his history
[00:32:27.560 --> 00:32:32.300]   has ever acquiesced to defeat Alex Karp was on CNBC, he was really, really
[00:32:32.300 --> 00:32:39.100]   sharp and concise about this, which is that, you know, in the West, when leaders fail,
[00:32:39.100 --> 00:32:45.460]   when their objectives, they just get elected out and somebody else takes their place. Yeah. But for
[00:32:45.460 --> 00:32:50.920]   somebody like Putin, there is nobody to take his place, because it's a very zero sum situation. And
[00:32:50.920 --> 00:32:54.340]   so his actions will, as a result, also be zero sum. And I think folks,
[00:32:54.340 --> 00:32:57.280]   he's never acquiesced in his life. And yeah,
[00:32:57.280 --> 00:33:01.120]   yeah, we've never we've never really kind of like, we don't understand well,
[00:33:01.120 --> 00:33:04.540]   what zero sum decision making looks like when it comes to stuff like this,
[00:33:04.540 --> 00:33:08.880]   he needs the Golden Bridge, right? You're gonna give him the gold does. Yeah. But I'll just say
[00:33:08.880 --> 00:33:15.960]   two things. One is that I think it's been made pretty clear that both India and China will not
[00:33:15.960 --> 00:33:20.520]   stand beside Russia if they do something like this. And I think that that is important,
[00:33:20.520 --> 00:33:25.860]   because they still are the two biggest purchasers of, of Russian oil. And so I think that matters a
[00:33:25.860 --> 00:33:30.120]   lot, because you're talking about a lot of revenue that would that would go away. And then the second
[00:33:30.120 --> 00:33:35.400]   is, I mentioned this last week, and this may sound dumb to some of you, but don't sleep on
[00:33:35.400 --> 00:33:38.660]   the Russian mothers. And what happened this? Oh, you're 100% right on that.
[00:33:38.660 --> 00:33:41.780]   Well, what happened this week was really interesting, which is that he calls up all
[00:33:41.780 --> 00:33:45.080]   these reservists, these reservists are not coming from the major cities of Russia,
[00:33:45.080 --> 00:33:49.040]   you're starting to see protests, you're starting to see young people say, I don't want to do this.
[00:33:49.040 --> 00:33:54.140]   Yeah. And who's that really activating? It's activating the moms. Yep. And so
[00:33:54.140 --> 00:33:59.060]   don't lose their son, the Russian moms. 300 people are being drafted, you know,
[00:33:59.060 --> 00:34:03.500]   to basically go fight. And I actually think David's right. Oh, sorry. Sorry. Go ahead.
[00:34:03.500 --> 00:34:08.440]   The question I have for you is, do you think I know you don't agree with this? But do you think the
[00:34:08.440 --> 00:34:14.500]   strategy is to back him into a corner, and then have this like rhetoric spike to then force a
[00:34:14.500 --> 00:34:18.880]   resolution? I know it's a dangerous strategy. It's a crazy chess move. But do you think that's
[00:34:18.880 --> 00:34:23.740]   actually what the West is thinking? I see no evidence that we have any
[00:34:23.740 --> 00:34:30.760]   intentions of seeking a diplomatic off ramp. I see no evidence that they're looking for to give
[00:34:30.760 --> 00:34:35.140]   him a golden bridge, like you said, then do you think they're trying to break him and have regime
[00:34:35.140 --> 00:34:38.220]   change? I think Biden stated the policy, which is this man cannot remain in
[00:34:38.220 --> 00:34:42.480]   power. I think he blurted out the truth of his policy. This is a regime change policy. That's
[00:34:42.480 --> 00:34:46.440]   what they're going for. They are backing him into a corner. I thought that you were right
[00:34:46.440 --> 00:34:50.160]   this whole time, which is, we're going to build a golden bridge, we're going to find a way to
[00:34:50.160 --> 00:34:54.540]   egress this guy. And I'm now sort of in the David camp, which is I think that
[00:34:54.540 --> 00:35:01.140]   the stated strategy of the Western alliance is essentially to cause him to make such a
[00:35:01.140 --> 00:35:08.000]   categorically catastrophic mistake, so as to become a pariah so as to either get overthrown or something.
[00:35:08.000 --> 00:35:15.140]   So I do think that on balance, the risk is now for things to escalate, maybe not in
[00:35:15.140 --> 00:35:21.620]   quantity, and I'll use this word in the wrong way. But you know, quote unquote, like the intensity of
[00:35:21.620 --> 00:35:28.400]   cook of it. So I think David's right, it's a lot of pressure to the economy and to the high risk
[00:35:28.400 --> 00:35:32.000]   assets. High risk strategy is a high risk strategy. We got a good thing going over here. I
[00:35:32.000 --> 00:35:37.780]   don't see the need for all this risk. So look, and the risk would be the reward, what would you see?
[00:35:37.780 --> 00:35:39.940]   What's the reward if Putin was removed? Oh, my Lord.
[00:35:39.940 --> 00:35:43.840]   Well, it depends who replaces him. What if we get a hardliner? You got to remember,
[00:35:43.840 --> 00:35:47.920]   Putin's taken out all the liberal reformers, all that's left are hardliners. So I know,
[00:35:47.920 --> 00:35:53.260]   I know there's moms protesting in the streets, but he's also under intense pressure from his,
[00:35:53.260 --> 00:35:59.320]   his right wing, that you know, he's got hawks on his side, who basically have been criticizing him
[00:35:59.320 --> 00:36:03.640]   for making this a special limited military operation instead of a war. They're like,
[00:36:03.640 --> 00:36:07.560]   why did you try to do this with 200,000 troops, we should have gone in heavy with a million.
[00:36:07.560 --> 00:36:09.300]   Right? There's criticism internally.
[00:36:09.300 --> 00:36:15.660]   There's a New York Times article about this. He has his own, you know, military hardliners in
[00:36:15.660 --> 00:36:21.000]   his security state, his hawks. So that so he's not just under pressure from peaceniks, who are
[00:36:21.000 --> 00:36:25.260]   protesting in the streets. He's also under pressure from hardliners in his own government, who think
[00:36:25.260 --> 00:36:26.460]   he's been too soft.
[00:36:26.460 --> 00:36:33.180]   Hmm. Yeah, it's a challenging situation. He said he said he's gonna be 70 years old. How many years
[00:36:33.180 --> 00:36:37.340]   does he have left? We just need to contain him for a decade was on our side. I don't contain him for
[00:36:37.340 --> 00:36:40.580]   a more decade. That's my best idea. contain him for a decade.
[00:36:40.580 --> 00:36:45.320]   The thing I have the most trouble with is if you look at the media portrayal of this. So I said last
[00:36:45.320 --> 00:36:50.360]   week when you know, we had this successful counter offensive, that maybe we'll get what we want,
[00:36:50.360 --> 00:36:54.320]   which is Russian morale collapses, they just tuck their tail between their legs and go back to
[00:36:54.320 --> 00:36:59.480]   Moscow, or maybe, you know, the Russians really do see this war is existential for them. Putin
[00:36:59.480 --> 00:37:03.740]   sees it as existential for himself. And he escalates. Well, what happened this week,
[00:37:03.740 --> 00:37:07.120]   we went up a rung on the escalatory ladder. Basically, Putin,
[00:37:07.120 --> 00:37:13.480]   drew down 300,000 more troops. And he's basically indicated his his willingness to use tackle nukes.
[00:37:13.480 --> 00:37:18.340]   And he's basically said, I'm not bluffing. So now what is the reaction in the American press? He
[00:37:18.340 --> 00:37:24.280]   must be bluffing. I mean, that basically is the reaction. And look, I don't know how you know that.
[00:37:24.280 --> 00:37:30.520]   You know, in poker, what you in poker, what do we do when someone might be we put them on a range?
[00:37:30.520 --> 00:37:32.200]   It's called a hand range, right? Yes,
[00:37:32.200 --> 00:37:36.900]   you can't possibly know exactly what they're going to do or what cards are holding. So
[00:37:36.900 --> 00:37:41.220]   you put them on a range of possible hands. And then you evaluate the story in light of their
[00:37:41.220 --> 00:37:46.500]   past actions. What do they do before the flop on the flop? And you basically come to an assessment
[00:37:46.500 --> 00:37:51.660]   of what is likely based on their story. Now take Putin's story, like Freeberg said, he's never
[00:37:51.660 --> 00:37:57.060]   backed down in his life from anything. He has said this war is existential. You know, he basically
[00:37:57.060 --> 00:38:01.440]   threatened to invade and so he did. I mean, like, I don't know how you can immediately jump to the
[00:38:01.440 --> 00:38:06.680]   conclusion. This is just a bluff. Maybe it is. But well, it could still be it could still be a bluff. But
[00:38:06.680 --> 00:38:11.600]   I mean, to your point, the range of outcomes does include shoving on the river moving all in.
[00:38:11.600 --> 00:38:17.000]   I mean, he's a KGB agent. Do you want to play the problem with a KGB agent with nukes?
[00:38:17.000 --> 00:38:21.200]   It's right. And so Jason, you've said throughout this guy's a madman. Well, exactly. I mean,
[00:38:21.200 --> 00:38:26.180]   personally, I think he's more like ruthless mafia boss than a madman. But But let's say
[00:38:26.180 --> 00:38:32.240]   you're right that he is a madman. What is the story about what we're doing? That makes sense.
[00:38:32.240 --> 00:38:36.460]   If he is a madman? Why would we want to basically back him into a corner like that?
[00:38:36.460 --> 00:38:40.720]   Why wouldn't we give him the golden off ramp? And by the way, just on this idea that no one would
[00:38:40.720 --> 00:38:45.580]   ever use tactical nukes, let me just give you three data points. First of all, we use them.
[00:38:45.580 --> 00:38:50.740]   We dropped two atomic bombs on Japan. And to end World War Two, we could have won that war without
[00:38:50.740 --> 00:38:54.100]   doing it. But we didn't want to lose the troops. Well, no, wait, those weren't Japanese. Those
[00:38:54.100 --> 00:39:01.480]   are just they were tactical nuke size atomic bombs. Number two, MacArthur wanted to use 20
[00:39:01.480 --> 00:39:06.240]   to 30 atomic bombs to end the Korean War. He had a whole plan. Truman fired him. He
[00:39:06.240 --> 00:39:11.580]   thought he basically jumped the shark. But MacArthur was the most respected and admired
[00:39:11.580 --> 00:39:15.720]   American in 1950. And the reason why Truman could not run for your elections because he fired
[00:39:15.720 --> 00:39:21.060]   MacArthur, MacArthur would have used basically the equivalent of tackle nukes to win the Korean War.
[00:39:21.060 --> 00:39:27.780]   And his plan to prevent China from re invading from the north was to irradiate the border so
[00:39:27.780 --> 00:39:31.320]   completely that Chinese troops could not go through it. Keep in mind, there wasn't as many
[00:39:31.320 --> 00:39:35.820]   nukes at the time. So we weren't up against nine different nuclear enabled countries. But we had an
[00:39:36.020 --> 00:39:40.640]   army commander ahead of our military who was willing to use nukes to win a war. So this idea
[00:39:40.640 --> 00:39:44.480]   that he wouldn't I mean, we've been willing to do that. And the third example is obviously the
[00:39:44.480 --> 00:39:49.520]   Cuban Missile Crisis. All of Kennedy's military advisors were willing to get to use nukes. I mean,
[00:39:49.520 --> 00:39:54.980]   and the best thing Kennedy did was not listen to his military advisors. They were all super hawkish.
[00:39:54.980 --> 00:39:59.720]   And Kennedy, what did he do? He looked for a way out. He looked for a compromise. He sent
[00:39:59.720 --> 00:40:05.800]   Bobby Kennedy to go cut a secret deal with the Russians where Kennedy agreed to pull the
[00:40:05.800 --> 00:40:13.000]   nuclear missiles out of Cuba. And then he lied to the American public about it because he didn't
[00:40:13.000 --> 00:40:18.460]   want to be perceived as backing down. But that's the kind of, you know, flexibility and mental
[00:40:18.460 --> 00:40:23.740]   acuity that I think you would need in a nuclear showdown to avoid a catastrophe. If things do
[00:40:23.740 --> 00:40:28.840]   escalate to the point of a nuclear showdown, do we believe that we have leadership on the level of a
[00:40:28.840 --> 00:40:33.820]   Jack Kennedy or Bobby Kennedy who can basically show the flexibility and adaptability to cut a
[00:40:33.820 --> 00:40:35.580]   deal to basically pull us back from the border? I mean, that's a really good question. I think
[00:40:35.580 --> 00:40:36.180]   that's a really good question. I think that's a really good question. I think that's a really good
[00:40:36.180 --> 00:40:36.180]   question. I think that's a really good question. I think that's a really good question.
[00:40:36.180 --> 00:40:37.680]   Yeah, it doesn't seem like that.
[00:40:37.680 --> 00:40:42.780]   What was Biden's response? It was he gave this speech to the United Nations in response to
[00:40:42.780 --> 00:40:47.160]   Putin. And it's more of the same as more of this, as I call it the Abe Simpson speech,
[00:40:47.160 --> 00:40:51.600]   this old man, you know, yelling at the cloud. I mean, he's basically just yelling at a teleprompter.
[00:40:51.600 --> 00:40:56.940]   Now, I think the strategically smart play would have been to say, listen, Putin,
[00:40:56.940 --> 00:41:01.920]   you said that you want the people of Ukraine to decide where these territories go. Okay,
[00:41:01.920 --> 00:41:05.360]   we can hold a referendum, but we want it administered by the United Nations. So,
[00:41:05.360 --> 00:41:11.900]   it'll have some credibility behind it. I mean, why not throw that out there as a potential
[00:41:11.900 --> 00:41:16.520]   way to get diplomacy started? It felt like at the end of last year,
[00:41:16.520 --> 00:41:23.420]   you guys remember, I thought like conflict was likely this year. I don't think that the conditions
[00:41:23.420 --> 00:41:32.960]   that I was referencing have really gotten better. I think they've gotten worse. And that's why I
[00:41:32.960 --> 00:41:35.140]   think we all talk about this in a
[00:41:35.140 --> 00:41:42.160]   rational way, meaning like how the conscious mind would, you know, debate the merits and challenges
[00:41:42.160 --> 00:41:51.580]   and risks of having a golden bridge or continuing conflict. But if you look historically, the US has
[00:41:51.580 --> 00:41:59.020]   often been in the middle of or at the tail end of some either recessionary cycle or inflationary
[00:41:59.020 --> 00:42:04.540]   cycle when conflict escalated externally. Wag the dog you're referring to?
[00:42:04.540 --> 00:42:04.920]   Mm-hmm.
[00:42:04.920 --> 00:42:08.520]   I don't know if I would call it that, but I think that there is
[00:42:08.520 --> 00:42:15.240]   an innate human anxiety. When things aren't going well, you feel like you have to do something about
[00:42:15.240 --> 00:42:20.640]   it. And you're either going to have internal conflict or external conflict as a result to try
[00:42:20.640 --> 00:42:26.520]   and resolve. When things are going great, the economy is booming. You don't enter a war. You
[00:42:26.520 --> 00:42:33.900]   don't start a conflict with the nation when people are happy at home, when your constituents and the
[00:42:33.900 --> 00:42:39.120]   unemployment rate is low and job wage growth is high. The economy is growing.
[00:42:39.120 --> 00:42:41.460]   Are you referring to the United States or Russia in this case?
[00:42:41.460 --> 00:42:44.220]   I'm referring to the US. And I think, you know,
[00:42:44.220 --> 00:42:45.540]   I'm just curious if you were
[00:42:45.540 --> 00:42:51.540]   coming out of COVID and coming out of the big question marks that loomed over our economy at
[00:42:51.540 --> 00:42:59.760]   the end of last year around inflation, economic growth, interest rates and the effect. And now
[00:42:59.760 --> 00:43:03.060]   we're in the middle of the turmoil. Markets are down 30%.
[00:43:03.060 --> 00:43:07.020]   And in some cases, 70 to 80% for high growth markets.
[00:43:07.020 --> 00:43:11.400]   There's a there's an inevitability now that unemployment is going to rise.
[00:43:11.400 --> 00:43:14.700]   There is an inevitability now that the economy is going to contract.
[00:43:14.700 --> 00:43:22.920]   Leadership is more likely in that scenario to find an outlet to find a place of conflict.
[00:43:22.920 --> 00:43:24.480]   And I don't think it's a conscious decision.
[00:43:24.480 --> 00:43:25.800]   Explain why in this theory. Explain why.
[00:43:25.800 --> 00:43:27.120]   Wag the dog.
[00:43:27.120 --> 00:43:27.780]   I don't understand the conflict.
[00:43:27.780 --> 00:43:29.340]   Is that what you're saying, Friedberg? Wag the dog?
[00:43:29.340 --> 00:43:29.880]   Yeah.
[00:43:29.880 --> 00:43:32.100]   I don't I don't think it's a conscious decision. I don't think that it's like, hey, let's go. Let's go.
[00:43:32.100 --> 00:43:38.340]   Start a war with Russia because the economy is bad. I think it's this anxiety. The economy is bad.
[00:43:38.340 --> 00:43:42.540]   And there's not a lot we can do about it. And over here, on the other hand, there's a problem. And
[00:43:42.540 --> 00:43:46.740]   there's something we can do about it. We can build strength and we can build integrity. And we can
[00:43:46.740 --> 00:43:51.900]   build support. And we can get people to get behind something together. And we can get something to
[00:43:51.900 --> 00:43:55.920]   create a driving mechanism to achieve something big.
[00:43:55.920 --> 00:43:57.180]   Like a distraction, you're saying?
[00:43:57.180 --> 00:43:57.540]   Yeah.
[00:43:57.540 --> 00:44:01.200]   Yeah. And I don't think that psychology is as simple as a distraction or a wag the dog.
[00:44:01.200 --> 00:44:07.140]   Or, hey, it's not even as simple as the military industrial complex will see revenue growth and wage growth.
[00:44:07.140 --> 00:44:18.120]   And that'll drive the economy. But I think all those things together are true. And I think in whole, we're we are more likely to want to pursue conflict right now than we were even a year ago.
[00:44:18.120 --> 00:44:19.560]   You buy any of that, Chamath?
[00:44:19.560 --> 00:44:29.880]   I think he's more right than wrong. I think that when things aren't easy, you need to find
[00:44:29.880 --> 00:44:36.740]   sort of distractions, essentially, to get people to focus on other things,
[00:44:36.740 --> 00:44:40.320]   so that the core problem isn't as obvious we have in the United States, I think.
[00:44:40.320 --> 00:44:42.060]   Let's go create another problem that's solvable.
[00:44:42.060 --> 00:44:45.840]   Yeah, like the best, I think the best way that I can describe this as I see it as
[00:44:45.840 --> 00:44:51.480]   institutional rot. And the more that we're left to our own devices,
[00:44:51.480 --> 00:44:57.740]   that amount of institutional decay becomes more and more obvious. Our governments don't work the
[00:44:57.740 --> 00:45:02.200]   way that they should. You know, our state assemblies are basically co-opted by special
[00:45:02.200 --> 00:45:08.040]   interests. The federal institutions we rely on to make rules are not that great. Enormous amounts
[00:45:08.040 --> 00:45:12.840]   of money get wasted every day. And as more and more people become aware of these things,
[00:45:12.840 --> 00:45:19.100]   it's just the trend is just so bad, you know, civic engagement goes down, everything just gets
[00:45:19.100 --> 00:45:25.440]   worse. And so when you take that, and then on top of that, you sit it on a poor economy,
[00:45:25.440 --> 00:45:27.640]   that is a real powerhouse.
[00:45:27.640 --> 00:45:31.720]   Yeah, it's a real powder keg, I think. And so folks like to, I think if you're a politician,
[00:45:31.720 --> 00:45:36.940]   it's easier to kind of go in and point to Taiwan and say, you know, we're going to go and defend
[00:45:36.940 --> 00:45:41.560]   these folks, if there's a war, point to Russia and say this point to all these other things. It's a
[00:45:41.560 --> 00:45:47.580]   whether it's implicit, as Friedberg says, or it's more explicit of a strategy, I don't know. But the
[00:45:47.580 --> 00:45:54.040]   underlying cause is the same, which is that if the foundation of the house is not strong, and you're
[00:45:54.040 --> 00:45:57.220]   not sure what to do to fix it, or you don't have the courage to fix it.
[00:45:58.100 --> 00:46:01.960]   The better strategic alternative is to distract and talk about your neighbor's house.
[00:46:01.960 --> 00:46:07.180]   Coming out of coming out of the financial crisis in 2008 2009, Obama recommitted to Afghanistan and
[00:46:07.180 --> 00:46:13.360]   sent 17,000 troops to Afghanistan in early 2009. When he took office, we entered the Persian Gulf
[00:46:13.360 --> 00:46:19.480]   with the Gulf War in 1991. Coming out of the Great Recession or the mild recession 1990 to 1991
[00:46:19.480 --> 00:46:27.180]   2000 2001.com crash. We, you know, we obviously entered, entered Iraq.
[00:46:27.180 --> 00:46:35.720]   Yeah, well, that was post nine, post 911, 911. But it was also choice of risk. But it was also in the midst of a recession and coming out of the
[00:46:35.720 --> 00:46:55.060]   but that was that one was clearly reactionist. Actually, you buy this, you think this is a wag the dog or, and by the way, you can do the same, you can do the same analysis on the time we entered the Korean War and the time we entered the Vietnam War, they were both tied to recessions. And, and so I don't know, I don't know how explicit this this action and behavior is. But I just think there's some data to it.
[00:46:55.060 --> 00:47:24.760]   There was a study that just came out by Tufts University on American military interventions throughout American history. And what they found was that we had the least in the period before the Cold War, then the second most was from the during the Cold War. But actually, the most hyperactive period of American military interventions was post Cold War. So since the unipolar moment, even though it's been the safest period for America, right, we haven't
[00:47:24.760 --> 00:47:24.940]   explained.
[00:47:24.940 --> 00:47:26.260]   Unipolar for people don't know,
[00:47:26.260 --> 00:47:39.680]   just that there's only one great power in the system. Before during the Cold War is a bipolar world, whereas basically America versus the Soviet Union, and you obviously had the, you know, NATO and the Western Alliance, the so called free world and then
[00:47:39.680 --> 00:47:43.480]   receive check and balance, proceed, you'd have the Warsaw Pact on on the other.
[00:47:43.480 --> 00:47:45.340]   Now we have unipolar moving to buy.
[00:47:45.340 --> 00:47:53.140]   Well, it was it was Yeah, we were unipolar for a couple of decades. But now we're moving towards multipolar, or at least bipolar with China.
[00:47:53.140 --> 00:47:54.820]   Yeah. Multipolar.
[00:47:54.820 --> 00:48:01.980]   would be maybe including India, maybe including India, Brazil, China in the future, in the future.
[00:48:01.980 --> 00:48:09.840]   But but so the irony is that the safer America has become the more we've gotten militarily
[00:48:09.840 --> 00:48:14.600]   involved overseas. And part of that is because there's no great power in the system to oppose us.
[00:48:14.600 --> 00:48:19.580]   But it's also gotten us in a lot of trouble. I mean, all of these wars in the Middle East,
[00:48:19.580 --> 00:48:24.040]   that cost us something like $8 trillion. Another survey cost of war study,
[00:48:24.620 --> 00:48:28.820]   the direct number of deaths from these wars from the war on terror is over a million
[00:48:28.820 --> 00:48:34.420]   lives lost $8 trillion. And that doesn't even include the excess mortality caused by
[00:48:34.420 --> 00:48:39.600]   the destruction of infrastructure, wastewater treatment, you know, famine, all that kind of
[00:48:39.600 --> 00:48:42.380]   stuff, which could be as high as five million, you know, the crazy part about that sex,
[00:48:42.380 --> 00:48:47.640]   that $8 trillion, if we had deployed that in energy independence, solar, nuclear, whatever,
[00:48:47.640 --> 00:48:52.580]   the whole reason to be in the Middle East was oil and energy. And we could have just
[00:48:52.580 --> 00:48:54.420]   deployed that to be energy independent.
[00:48:54.420 --> 00:48:57.060]   in the West and not had all of this pain,
[00:48:57.060 --> 00:49:00.500]   not the not the whole I mean, not the whole reason. I think the reason to be in Afghanistan
[00:49:00.500 --> 00:49:04.420]   was to kill someone with the exception of 911. And maybe we could have done that without taking
[00:49:04.420 --> 00:49:07.780]   over the whole country and gone on this 20 year nation could have been done very strategically.
[00:49:07.780 --> 00:49:10.820]   I mean, actually, that was really proven when Well, first of all, we did kill.
[00:49:10.820 --> 00:49:19.380]   We did kill bin Laden in Pakistan, with just a raid by the seals and infiltration to occupy
[00:49:19.380 --> 00:49:23.540]   that country. And then more recently, we finally got Zawahiri in Afghanistan,
[00:49:24.220 --> 00:49:28.860]   using a drone after we left the country. So what the hell do we need to be there for? We don't 20
[00:49:28.860 --> 00:49:34.220]   years when we could kill these guys, so drones and you know, a helicopter team?
[00:49:34.220 --> 00:49:38.460]   Yeah, this is where we needed to have an adversary who could check our power,
[00:49:38.460 --> 00:49:41.660]   it would have been healthier is I guess the premise to the tough study.
[00:49:41.660 --> 00:49:47.980]   Right. But But look, I think to freeberg's point, is it do we become more militaristic when things
[00:49:47.980 --> 00:49:52.180]   aren't going well at home? I don't know. But it feels to me, like just in general, over the last
[00:49:52.180 --> 00:49:54.020]   30 years, we've become hyper militaristic.
[00:49:54.020 --> 00:50:00.540]   It's the use of military force is typically the first option. And we resort to it too quickly. And
[00:50:00.540 --> 00:50:05.340]   we don't use diplomacy enough. And you can see that in just the number of lives lost the failure
[00:50:05.340 --> 00:50:11.340]   of these wars, and the enormous deficit we're running. As you know, sex, I can't stand Trump.
[00:50:11.340 --> 00:50:17.500]   The two things he got right, no wars. And he was the dictator whisperer, that guy knew how
[00:50:17.500 --> 00:50:23.820]   to talk to a dictator. You know, whether it was North Korea, China, Russia, he just knew how to
[00:50:23.820 --> 00:50:27.060]   bond with them. It was like a superpower for him. Okay, we have three directions.
[00:50:27.060 --> 00:50:29.060]   That keeps us out of war is great. You know,
[00:50:29.060 --> 00:50:34.900]   I mean, it's literally his only saving grace. All right. Gentlemen, we have to talk about what's
[00:50:34.900 --> 00:50:39.940]   going on in Iran, we have to talk about the Wall Street editorial boards, California,
[00:50:39.940 --> 00:50:43.620]   talk about talk about talk about Newsom, because there's a bunch of energy stuff happening in the
[00:50:43.620 --> 00:50:47.940]   United States, I think is important. All right. So the Wall Street Journal editorial board,
[00:50:47.940 --> 00:50:53.620]   which obviously has a side wrote an op ed on California's grid issues, some of the quotes,
[00:50:53.620 --> 00:51:00.140]   some of the op ed, California can barely keep the lights on as its climate policies bite the
[00:51:00.140 --> 00:51:06.060]   electric grid. But Gavin Newsom is undaunted. On Friday, he signed no fewer than 40 new climate
[00:51:06.060 --> 00:51:11.820]   bills to amp up California's green energy stock shock experiment, even as gasoline prices
[00:51:11.820 --> 00:51:18.700]   nationwide have fallen to an average of 368 a gallon. Californians are still paying 545 a gallon
[00:51:18.700 --> 00:51:22.620]   California's electric rates are already more than double those in neighboring states. This is what
[00:51:22.620 --> 00:51:23.420]   happens when politicians are not paying 545 a gallon. California's electric rates are already
[00:51:23.420 --> 00:51:24.260]   more than double those in neighboring states. This is what happens when politicians try to eliminate
[00:51:24.260 --> 00:51:31.060]   fossil fuels with a Molotov cocktail of regulation taxes and renewable mandates and subsidies.
[00:51:31.060 --> 00:51:38.500]   The coda to this is I'll send it to you, Nick, but can you please play the clip as well of Rashida
[00:51:38.500 --> 00:51:44.420]   Tlaib trying to skewer Jamie Dimon where he just destroys her God, that's embarrassing.
[00:51:44.420 --> 00:51:49.540]   That was embarrassing. I mean, we should play it. I mean, it's she made no sense.
[00:51:49.540 --> 00:51:51.380]   Nick, can you play that clip, please?
[00:51:51.380 --> 00:51:53.220]   You have all committed a total of three crimes, including the murder of a woman,
[00:51:53.220 --> 00:51:56.060]   and the murder of a man. And you have committed, as you all know, to transition the emissions from
[00:51:56.060 --> 00:52:02.620]   lending and investment activities to line with pathways to net zero in 2050. Do you know what the
[00:52:02.620 --> 00:52:09.900]   International Energy Agency has said is required to meet our global 2050 net zero targets of
[00:52:09.900 --> 00:52:17.900]   limiting global temperature rise to 2.7 degrees Fahrenheit or 1.5 degrees Celsius? So no new fossil
[00:52:17.900 --> 00:52:23.020]   fuel production starting today? So that's like zero.
[00:52:23.020 --> 00:52:27.860]   So I'd like to ask all of you and go down the list, because again, you all have agreed to doing this.
[00:52:27.860 --> 00:52:34.300]   Please answer with a simple yes or no. Does your bank have a policy against funding new oil and gas products? Mr.
[00:52:34.300 --> 00:52:34.740]   Dimon?
[00:52:34.740 --> 00:52:38.820]   Absolutely not. And that would be the road to hell for America.
[00:52:38.820 --> 00:52:52.820]   Yeah, that's fine. That's fine. Sir, you know what? Everybody that got relief from student loans has a bank account with your bank should probably take out their account and close their account. The fact that you're not even there to help relieve many of the folks that are in the bank. So I'd like to ask all of you and go down the list. Because again, you all have agreed to doing this. Please answer with a simple yes or no. Does your bank have a policy against funding new oil and gas products? Mr.
[00:52:52.820 --> 00:52:56.820]   are in debt, extreme debt, because of student loan debt, and you're out there criticizing it.
[00:52:56.820 --> 00:53:04.500]   My favorite was when she said Celesis. That was yeah. I was like, okay. It's just so much
[00:53:04.500 --> 00:53:09.700]   theatrics. The Bershida Tlaib represents the 13th district congressional district in Michigan.
[00:53:09.700 --> 00:53:21.380]   The, the median age in that district is 35.9 years old. The 2020 poverty rate is 28.2%.
[00:53:21.380 --> 00:53:28.900]   So more than almost one in three people. And the 2020 median household income is $37,601.
[00:53:28.900 --> 00:53:36.980]   So you know, she represents a group of people that, you know, I think, at best,
[00:53:36.980 --> 00:53:43.300]   is lower middle class. And the idea that she doesn't even basically understand what
[00:53:43.300 --> 00:53:49.940]   would happen in her district, if you actually did not have cheap LNG. Again,
[00:53:49.940 --> 00:53:56.980]   just kind of speaks to the institutional kind of decay in Washington. She is not the person that
[00:53:56.980 --> 00:54:01.620]   should be advocating for this. Like, you know, it's districts like this more than any other
[00:54:01.620 --> 00:54:08.580]   that don't have the money to spend on, you know, very expensive solar installations that cost 30
[00:54:08.580 --> 00:54:18.340]   and $40,000. These are the districts that need coal, coal fired plants, LNG oil to keep going to
[00:54:18.340 --> 00:54:19.140]   sort of minimize the impacts of inflation. And so, you know, I think that's a really important
[00:54:19.140 --> 00:54:25.380]   piece of the story. That's a really important piece of the story. That's a really important piece of the story. That was just like a grandstanding moment moving to California hearings,
[00:54:25.380 --> 00:54:31.540]   because these are important discussions, and they've become theatrics. And this is a chance
[00:54:31.540 --> 00:54:37.460]   to educate the public with some charts and data. She's either so hungry for power that she actually
[00:54:37.460 --> 00:54:44.740]   doesn't care about her constituents, or she scientifically and numerically illiterate.
[00:54:44.740 --> 00:54:49.140]   That's I think the latter is probably the issue here. And so this is this is what such a show.
[00:54:49.140 --> 00:54:52.260]   shame, on the on the on the other side, you know, at the end
[00:54:52.260 --> 00:54:57.500]   of at the other end of the coast in California, the cost of power
[00:54:57.500 --> 00:55:01.200]   generation, just so you guys know, has fallen by 90%, when
[00:55:01.200 --> 00:55:04.980]   you look at renewables. And that is because of a good job that
[00:55:04.980 --> 00:55:08.100]   the federal government did in introducing subsidies that
[00:55:08.100 --> 00:55:11.880]   essentially gave the right sets of incentives for people to
[00:55:11.880 --> 00:55:15.100]   build this infrastructure. But while the cost of generating
[00:55:15.100 --> 00:55:19.360]   renewable power has fallen by 90%, you know, virtually, it's
[00:55:19.360 --> 00:55:22.900]   on par and it's cheaper than any other form of generation, your
[00:55:22.900 --> 00:55:25.620]   electricity costs have doubled and are probably going to double
[00:55:25.620 --> 00:55:28.500]   again, in a state like California. So you know, we're
[00:55:28.500 --> 00:55:34.300]   cagering our, our utility rates by, you know, seven to 11% every
[00:55:34.300 --> 00:55:38.220]   year, that is unsustainable in California. And what do you
[00:55:38.220 --> 00:55:41.340]   have, you have, again, a different version of the same
[00:55:41.340 --> 00:55:44.980]   flu that Rashida Tlaib has, which is, you run forward,
[00:55:44.980 --> 00:55:45.080]   you run forward, you run forward, you run forward, you
[00:55:45.080 --> 00:55:45.380]   run forward, you run forward, you run forward, you run forward,
[00:55:45.380 --> 00:55:45.520]   you run forward, you run forward, you run forward, you run forward,
[00:55:45.520 --> 00:55:46.120]   you run forward, you run forward, you run forward, you run forward,
[00:55:46.120 --> 00:55:46.560]   you run forward, you run forward, you run forward, you run forward,
[00:55:46.560 --> 00:55:46.740]   you run forward, you run forward, you run forward, you run forward,
[00:55:46.740 --> 00:55:47.080]   you run forward, you run forward, you run forward, you run forward,
[00:55:47.080 --> 00:55:49.280]   you're going to have to do all of these things. You don't spend
[00:55:49.280 --> 00:55:51.480]   enough time to really understand what's happening on the ground,
[00:55:51.480 --> 00:55:55.140]   and you make it impossible for people to make the decisions to
[00:55:55.140 --> 00:55:58.740]   actually be resilient for themselves. At the end of the
[00:55:58.740 --> 00:56:03.260]   day, there are 10s of utilities in America. But there are 100
[00:56:03.260 --> 00:56:06.980]   million households. And the only path to energy independence is
[00:56:06.980 --> 00:56:10.800]   to get every single 100 million households to be resilient,
[00:56:10.820 --> 00:56:13.940]   which means they need their own solar panels, they need battery
[00:56:13.940 --> 00:56:17.520]   storage, they need their own potable water. And all of these
[00:56:17.520 --> 00:56:21.200]   systems are now affordable and available. And now the federal
[00:56:21.200 --> 00:56:24.820]   government with the IRA has created the financial incentives
[00:56:24.820 --> 00:56:28.800]   to pull it forward. So I don't know, I just think like this is
[00:56:28.800 --> 00:56:31.880]   a hugely stark reminder about how poorly our energy policy has
[00:56:31.880 --> 00:56:35.300]   been managed. And if you leave it to the hands of the
[00:56:35.300 --> 00:56:38.940]   progressive left, they will do things that don't map to what
[00:56:38.940 --> 00:56:40.800]   people on the ground actually need.
[00:56:40.800 --> 00:56:45.340]   People in California, most people in California cannot pay
[00:56:45.340 --> 00:56:47.840]   utility rates that are going to double every six and seven
[00:56:47.840 --> 00:56:52.260]   years. Just like people in the congressional 13th District of
[00:56:52.260 --> 00:56:56.880]   Michigan cannot afford to pay for solar. If Rashida Tlaib is
[00:56:56.880 --> 00:57:00.540]   able to get, you know, all these banks to not finance LNG, coal
[00:57:00.540 --> 00:57:04.320]   and, and, and other forms of hydrocarbons as a bridge fuel.
[00:57:04.320 --> 00:57:08.500]   We have to look at all 40 of these bills independently. And
[00:57:08.500 --> 00:57:10.580]   you have to think about the bills. I mean,
[00:57:10.780 --> 00:57:13.360]   who's reading these things? What is in these things?
[00:57:13.360 --> 00:57:16.000]   I mean, each one has to be addressed individually, like one
[00:57:16.000 --> 00:57:19.120]   of them could be to help people put solar panels on top of
[00:57:19.120 --> 00:57:22.880]   schools and batteries, and that could be a good bill. But there
[00:57:22.880 --> 00:57:23.620]   definitely needs to be
[00:57:23.620 --> 00:57:26.800]   have that mechanism at the federal level, the IRA passed an
[00:57:26.800 --> 00:57:29.440]   incredible set of incentives, both for the producers of these
[00:57:29.440 --> 00:57:32.840]   things, and for the for the end companies that actually deploy
[00:57:32.840 --> 00:57:37.300]   them. Yep. And so we've solved that problem, you know, so I
[00:57:37.300 --> 00:57:40.760]   just think like it just goes to show you a ton of regulation does
[00:57:40.760 --> 00:57:43.760]   not actually add and get to the solution that we want. The
[00:57:43.760 --> 00:57:47.000]   government will not solve your problems. I hate to be the bearer
[00:57:47.000 --> 00:57:50.840]   of bad news. But they are going to make things more complicated
[00:57:50.840 --> 00:57:54.560]   and more expensive. And the resilience that you expect out
[00:57:54.560 --> 00:57:57.640]   of your utility infrastructure, by the way, we saw just what
[00:57:57.640 --> 00:58:00.120]   happened last week, there was a massive fire and a massive
[00:58:00.120 --> 00:58:04.280]   battery installation that California installed 182 gigawatt
[00:58:04.280 --> 00:58:07.920]   system, megawatt system. Could you imagine if that had actually
[00:58:07.920 --> 00:58:10.740]   lit on fire two weeks earlier in the middle of this crazy heat
[00:58:10.740 --> 00:58:15.600]   wave that we had? So even even utility scale renewables are
[00:58:15.600 --> 00:58:18.600]   very complicated projects to undertake.
[00:58:18.600 --> 00:58:21.300]   Look at Texas, I mean, people are dying there because the grid
[00:58:21.300 --> 00:58:25.320]   keeps going down. It is much safer and more reliable if is if
[00:58:25.320 --> 00:58:28.740]   every homeowner in the United States took responsibility and
[00:58:28.740 --> 00:58:32.760]   and use these incentives to basically become your own little
[00:58:32.760 --> 00:58:34.500]   virtual power plant and you will
[00:58:34.500 --> 00:58:37.300]   technologies there. I mean, the technology getting generators
[00:58:37.300 --> 00:58:40.260]   for natural gases backup. So we're gonna have to figure out
[00:58:40.260 --> 00:58:40.720]   how to take
[00:58:40.720 --> 00:58:42.960]   the load off the grid and build resiliency into it. I don't know
[00:58:42.960 --> 00:58:46.720]   if you guys saw in a related story that all this ESG stuff is
[00:58:46.720 --> 00:58:49.780]   kind of coming to a head but Dilbert got canceled this week
[00:58:49.780 --> 00:58:53.860]   and like 200 newspapers. Did you see this? What? Yeah, so
[00:58:53.860 --> 00:58:58.420]   possible? Well, because he's been going after ESG in his
[00:58:58.420 --> 00:59:04.240]   cartoon. And so I'm interested in your take on this. But here,
[00:59:04.240 --> 00:59:06.380]   I don't know the characters in Dilbert except for Dilbert. But
[00:59:06.380 --> 00:59:09.780]   he says, this person who's in charge says our ESG score will
[00:59:09.780 --> 00:59:10.700]   drop if we
[00:59:10.700 --> 00:59:14.360]   open a new factory that adds CO2 to the atmosphere. But we can
[00:59:14.360 --> 00:59:17.660]   balance that out by adding more diversity to our board. And I
[00:59:17.660 --> 00:59:21.060]   guess the cat says how much CO2 Do you plan to add and he said
[00:59:21.060 --> 00:59:26.160]   one non nine binary board members were showing the ES and
[00:59:26.160 --> 00:59:31.000]   the G being put together makes no sense. But that that panel
[00:59:31.000 --> 00:59:32.000]   obviously got him canceled.
[00:59:32.000 --> 00:59:35.280]   What does he mean? He's canceled like he was fired from national
[00:59:35.280 --> 00:59:36.020]   newspapers.
[00:59:36.020 --> 00:59:39.560]   I think one of the 177 newspapers because they're all
[00:59:39.560 --> 00:59:40.680]   part of chains now. And I think that's what he's saying. He's
[00:59:40.680 --> 00:59:42.240]   saying that he's not going to be fired. And I guess some of those
[00:59:42.240 --> 00:59:45.120]   because of that because of that cartoon, that cartoon plus it's
[00:59:45.120 --> 00:59:49.860]   like a series going after the social governance part of you
[00:59:49.860 --> 00:59:53.640]   know, environmental and pointing out he stated on his podcast
[00:59:53.640 --> 00:59:57.420]   that he wants to kill ESG. So this whole ESG debate, he's
[00:59:57.420 --> 01:00:01.380]   trying to further it, but I guess that Yeah, well, I mean,
[01:00:01.380 --> 01:00:04.860]   the the concept of ESG makes a ton of sense. I think the problem
[01:00:04.860 --> 01:00:07.900]   is that it does implement this Yeah, this version 1.0
[01:00:07.900 --> 01:00:08.800]   implementation
[01:00:08.800 --> 01:00:10.660]   was financialized by people that have a certain amount of money.
[01:00:10.660 --> 01:00:14.240]   And they have no care about ESG at all. And so all it's done is
[01:00:14.240 --> 01:00:17.200]   create complexity and consultants and, you know,
[01:00:17.200 --> 01:00:21.880]   studies sense or it doesn't make sense. No, the words, E, s and
[01:00:21.880 --> 01:00:24.640]   g make right a lot of they have commas or periods between them,
[01:00:24.640 --> 01:00:27.400]   I guess is the question. That's fine, too. But my point is
[01:00:27.400 --> 01:00:31.340]   saying that you want, you know, diversity, and you want
[01:00:31.340 --> 01:00:34.720]   sustainability, and you want better governance, all good. All
[01:00:34.720 --> 01:00:37.160]   of these things are really great ideas. It's just that in this
[01:00:37.160 --> 01:00:40.060]   first implementation, it got financially perverted.
[01:00:40.640 --> 01:00:44.340]   And so what you have are folks that are, you know, probably not
[01:00:44.340 --> 01:00:47.540]   the best position or should not really have an opinion about ESG
[01:00:47.540 --> 01:00:50.360]   opining about things that they never were given the authority
[01:00:50.360 --> 01:00:52.580]   to opine on. And then as a result, what it's really
[01:00:52.580 --> 01:00:56.600]   created is a cottage industry of consultants that can basically
[01:00:56.600 --> 01:01:00.200]   make, you know, hundreds of millions of dollars writing all
[01:01:00.200 --> 01:01:03.600]   of these reports. And I think that that's why this
[01:01:03.600 --> 01:01:06.740]   implementation doesn't work. So ESG today is broken. And I think
[01:01:06.740 --> 01:01:10.620]   it's largely meaningless. The concept of what people want to do
[01:01:10.620 --> 01:01:13.620]   is, is a very good idea. We just need a better way to implement.
[01:01:13.620 --> 01:01:16.500]   I agree with that. You can't connect these things. We're going
[01:01:16.500 --> 01:01:19.140]   to put social and governance with environment. It's all
[01:01:19.140 --> 01:01:21.420]   that's going to do is hold back the environment, right? Saks
[01:01:21.420 --> 01:01:23.960]   sacks. We're having all of fed beef tonight. Oh, this is
[01:01:23.960 --> 01:01:27.240]   please. You need to try it. I'll see what I can do. No, you have
[01:01:27.240 --> 01:01:29.660]   to do it. You have to do it. Once you have to say it sacks
[01:01:29.660 --> 01:01:33.420]   commit no bullshit. You cannot imagine what does a beef look
[01:01:33.420 --> 01:01:37.020]   like that has been only fed green olives pitted green olives
[01:01:37.020 --> 01:01:40.520]   at that I can tell you I can tell you how it looks delicious. It's a
[01:01:40.520 --> 01:01:43.620]   green. It's the most delicious steak you've had in your life.
[01:01:43.620 --> 01:01:46.880]   It's the most delicious. All right, I'll come for that. It's
[01:01:46.880 --> 01:01:49.620]   incredible. Awesome. You're gonna drive me with your driver
[01:01:49.620 --> 01:01:52.620]   text me your address. I know where to get you. Oh, I love it.
[01:01:52.620 --> 01:01:55.760]   Oh, this is gonna be great. Game on. Listen to this lineup. Me.
[01:01:55.760 --> 01:02:06.700]   J Cal. Friedberg sacks. flying it. This thing this is gonna
[01:02:06.700 --> 01:02:09.120]   jump up this game is gonna no more flips. The flips are
[01:02:09.120 --> 01:02:10.500]   ridiculous. Okay, you know what now?
[01:02:10.500 --> 01:02:12.900]   Now that sacks is coming. I'm going to tell them to break out
[01:02:12.900 --> 01:02:14.260]   the early white truffles.
[01:02:14.260 --> 01:02:16.680]   I'm just kidding.
[01:02:16.680 --> 01:02:18.060]   Truffle pigs.
[01:02:18.060 --> 01:02:21.160]   Not not just walk to my office. She looked at me when I said
[01:02:21.160 --> 01:02:22.000]   white truffle. She's like,
[01:02:22.000 --> 01:02:27.480]   for bestie dinner. It's happening. Everybody is
[01:02:27.480 --> 01:02:28.340]   happening is happening.
[01:02:28.340 --> 01:02:31.760]   You know what I saw was hellmuth the other day on one of his
[01:02:31.760 --> 01:02:34.120]   heads up matches where he got in that huge fight with that guy.
[01:02:34.120 --> 01:02:36.400]   Did you guys see that? And he kind of got every match. You
[01:02:36.400 --> 01:02:38.880]   just described every match he's ever been like some heads up
[01:02:38.880 --> 01:02:40.440]   thing. And the guy
[01:02:40.460 --> 01:02:43.760]   he had flopped some insane I think it was like quads or
[01:02:43.760 --> 01:02:46.240]   something or and hellmuth had like top pair except the
[01:02:46.240 --> 01:02:49.160]   order. Yeah, yeah, yeah. And hellmuth folded on this guy. It
[01:02:49.160 --> 01:02:51.660]   was incredible to watch the play like the read from hellmuth was
[01:02:51.660 --> 01:02:54.740]   unbelievable. He is incredible. He's a friggin phenom that guy.
[01:02:54.740 --> 01:02:57.700]   He's pretty great. He's got unbelievable. He's the greatest
[01:02:57.700 --> 01:03:00.260]   poker player in the world. He really he has reads are
[01:03:00.260 --> 01:03:00.960]   unbelievable.
[01:03:00.960 --> 01:03:05.540]   I think that what percentage of it is obviously he is an
[01:03:05.540 --> 01:03:08.100]   incredible player. But there's another piece of this that we
[01:03:08.100 --> 01:03:10.800]   all see, which is people play into him. They want to be in a
[01:03:10.800 --> 01:03:14.520]   hand with him. They want to bust him. So in those tournaments,
[01:03:14.520 --> 01:03:15.300]   what do you think?
[01:03:15.300 --> 01:03:17.580]   No, I think I think what's happening is at the highest
[01:03:17.580 --> 01:03:20.640]   level, there's like all these people that have gone in this
[01:03:20.640 --> 01:03:23.040]   one direction. And by the way, this is probably a good, we
[01:03:23.040 --> 01:03:25.860]   could talk about chess in the same way, which is that you have
[01:03:25.860 --> 01:03:28.500]   these solvers, right? There's poker solvers. And now they're
[01:03:28.500 --> 01:03:32.000]   there, these chess solvers. And the young generation spends all
[01:03:32.000 --> 01:03:35.340]   this time training on the solvers so that they know every
[01:03:35.340 --> 01:03:38.060]   permutation of every move and what is what people
[01:03:38.060 --> 01:03:42.540]   call game theory optimal or GTO. The problem with GTO is that you
[01:03:42.540 --> 01:03:45.460]   can actually be very exploitative against somebody
[01:03:45.460 --> 01:03:49.560]   that's actually playing perfectly, because the AI is
[01:03:49.560 --> 01:03:52.000]   perfected around what is the rational set of decisions in
[01:03:52.000 --> 01:03:55.040]   every spot. And so you can set people up to make a lot of
[01:03:55.040 --> 01:03:58.340]   really bad mistakes. And I think how youth understands that. And
[01:03:58.340 --> 01:04:01.840]   so because he is one of like this dying breed of people that
[01:04:01.840 --> 01:04:07.120]   plays live, he's able to just be so exploitative. And these folks
[01:04:07.120 --> 01:04:08.020]   that are basically
[01:04:08.020 --> 01:04:12.420]   playing from rote memory, right? What is the GTO move in every
[01:04:12.420 --> 01:04:16.260]   spot, they end up making a bunch of mistakes and, and feeding
[01:04:16.260 --> 01:04:20.260]   chips into him. And so I think like it's, it's probably the
[01:04:20.260 --> 01:04:23.820]   same as sort of this Magnus Carlsen, he kind of knew what to
[01:04:23.820 --> 01:04:26.980]   expect from people. And the minute this kid deviated into
[01:04:26.980 --> 01:04:28.840]   this realm where you were like, how could you make that
[01:04:28.840 --> 01:04:33.920]   decision? Probably because you know, the the computer AI is able
[01:04:33.920 --> 01:04:37.360]   to calculate make a slightly losing move now, three moves
[01:04:37.360 --> 01:04:37.980]   later, you're
[01:04:37.980 --> 01:04:40.840]   going to actually be ahead. It caught him off guard where he was
[01:04:40.840 --> 01:04:44.540]   basically like, I think this guy's cheating. Really, really
[01:04:44.540 --> 01:04:47.580]   crazy. But I think like if you if you have a bunch of GTO kids,
[01:04:47.580 --> 01:04:51.540]   folks that are a little bit older that have been playing in
[01:04:51.540 --> 01:04:53.780]   a live situation can be hyper exploitative.
[01:04:53.780 --> 01:04:56.940]   By the way, this is a good point, generally about about
[01:04:56.940 --> 01:05:00.360]   this gaming is that computation has played and computers have
[01:05:00.360 --> 01:05:04.260]   played such an incredible role, it almost becomes questionable
[01:05:04.260 --> 01:05:07.860]   on how much can the human really differentiate anymore?
[01:05:07.940 --> 01:05:11.840]   In these, in these games, and in these systems. You guys watch the
[01:05:11.840 --> 01:05:15.400]   chess players on YouTube, and they'll have solvers live. And
[01:05:15.400 --> 01:05:18.140]   they'll be getting live scoring as they kind of walk through a
[01:05:18.140 --> 01:05:21.140]   match or walk by the way, free burn area. That's, that's the
[01:05:21.140 --> 01:05:24.920]   same with poker, you have these huds, heads up displays. And a
[01:05:24.920 --> 01:05:27.740]   lot of the poker sites have basically given up, they try to
[01:05:27.740 --> 01:05:31.300]   spot the cheating. And you can it could because you have this
[01:05:31.300 --> 01:05:34.060]   basically layer that's helping you. And it's effectively
[01:05:34.060 --> 01:05:36.860]   impossible, because you know, these things run locally, and you're
[01:05:36.860 --> 01:05:39.440]   able to spot the cheating. And you can see that they run locally,
[01:05:39.440 --> 01:05:43.100]   they're screen scraping locally. And you just have no idea except
[01:05:43.100 --> 01:05:46.280]   when they make moves that are just so unpredictable from a
[01:05:46.280 --> 01:05:48.800]   human and could only come from it from a from a machine.
[01:05:48.800 --> 01:05:53.120]   And ultimately, all all games maybe become obsolete, because
[01:05:53.120 --> 01:05:57.140]   there is no real way to qualify the performance of one human
[01:05:57.140 --> 01:06:00.260]   against another, when the AI itself or the technology or the
[01:06:00.260 --> 01:06:03.140]   computing itself, you know, overshadows human potential and
[01:06:03.140 --> 01:06:03.920]   human capability.
[01:06:03.920 --> 01:06:06.040]   You guys may not know this story. But when I met Dimas,
[01:06:06.040 --> 01:06:12.100]   the founder of DeepMind, I got introduced by Teal, like in 2011,
[01:06:12.100 --> 01:06:15.820]   or 12. So like a decade ago, and I met him in London, I'll never
[01:06:15.820 --> 01:06:18.640]   forget this at the, we were having breakfast at the Connett
[01:06:18.640 --> 01:06:22.400]   Hotel. And he explained to me DeepMind in the context of the
[01:06:22.400 --> 01:06:25.220]   game, because at that time, how they were building the first
[01:06:25.220 --> 01:06:29.440]   versions of the AI was perfecting how to play certain
[01:06:29.440 --> 01:06:31.320]   video games. And I can't remember the name of the video
[01:06:31.320 --> 01:06:33.460]   game, but it was one of the famous first person shooter
[01:06:33.460 --> 01:06:36.700]   games. And the whole idea there was like, you know, if you can
[01:06:36.700 --> 01:06:39.360]   perfect an AI that that basically plays GTO and can win
[01:06:39.360 --> 01:06:42.860]   the game, what you've effectively solved for is is like
[01:06:42.860 --> 01:06:45.180]   a layer of AI that can then solve other generalized
[01:06:45.180 --> 01:06:48.460]   problems. I was blown away. I thought this is the craziest
[01:06:48.460 --> 01:06:52.460]   thing I've ever heard a decade ago. So it's pretty natural
[01:06:52.460 --> 01:06:55.400]   that they've taken this stuff and adapted it to meet every
[01:06:55.400 --> 01:06:57.820]   game. It's a little sad, though, to know, don't you think?
[01:06:57.820 --> 01:07:01.540]   Yeah, I mean, yeah. But I mean, maybe there's a different model
[01:07:01.540 --> 01:07:03.400]   of performance for humans that really changed.
[01:07:03.400 --> 01:07:08.440]   It changes what the gaming is, right? Well, I don't know what it
[01:07:08.440 --> 01:07:12.820]   is. I mean, I think that these games themselves completely, you
[01:07:12.820 --> 01:07:16.780]   know, the intent of a game, which is to measure one's kind
[01:07:16.780 --> 01:07:19.840]   of decision making abilities gets obsoleted, because the
[01:07:19.840 --> 01:07:22.340]   software ultimately is a better decision maker than the human.
[01:07:22.340 --> 01:07:27.140]   So the question then is, what is the human going to rise to that
[01:07:27.140 --> 01:07:30.680]   creates a new playing field? And I think there's probably
[01:07:30.680 --> 01:07:33.180]   elements of creativity and actually using the software,
[01:07:33.180 --> 01:07:35.940]   software to become part of the game that opens up a whole new
[01:07:35.940 --> 01:07:38.400]   opportunity for what gaming is. I mean, we've been talking about
[01:07:38.400 --> 01:07:41.640]   a little bit about video games, but you could see artificial
[01:07:41.640 --> 01:07:44.980]   constructs in gaming that arise from the human interacting with
[01:07:44.980 --> 01:07:47.160]   the computer and then creating a new sort of playing field in
[01:07:47.160 --> 01:07:49.740]   gaming. Yeah, tax. What do you what do you think about all this
[01:07:49.740 --> 01:07:50.760]   huge big chess player?
[01:07:50.760 --> 01:07:53.520]   Yeah, I've been following it for the last couple of weeks. It's
[01:07:53.520 --> 01:07:56.640]   obviously been the big story in the chess world. What basically
[01:07:56.640 --> 01:08:01.140]   happened is that a couple of weeks ago, Magnus Carlsen lost
[01:08:01.140 --> 01:08:02.400]   that game to
[01:08:02.400 --> 01:08:07.860]   Hans, what's his last name? I think Neiman. Yeah, exactly. And
[01:08:07.860 --> 01:08:10.560]   the next day, he pulled out of the tournament. This is the
[01:08:10.560 --> 01:08:13.260]   Sinkfield Cup. And he's never pulled out of a tournament
[01:08:13.260 --> 01:08:16.020]   before everyone was sort of speculating. Is he sick or have
[01:08:16.020 --> 01:08:19.600]   COVID or something like that. And then he tweeted out a video
[01:08:19.600 --> 01:08:25.080]   from some soccer coach saying I can't get in trouble. Yeah. Yeah.
[01:08:25.080 --> 01:08:29.280]   So clearly, he was in a passive aggressive way of making an
[01:08:29.280 --> 01:08:32.380]   accusation. And then he doubled down a few days ago, he
[01:08:32.380 --> 01:08:37.760]   was in another tournament. And he had to play Hans. I keep
[01:08:37.760 --> 01:08:38.620]   forgetting the guy's name.
[01:08:38.620 --> 01:08:39.880]   Hans Neiman.
[01:08:39.880 --> 01:08:43.760]   Neiman. Yeah. He had to play him and he resigned on move two. So
[01:08:43.760 --> 01:08:46.540]   basically, he doubled down in his accusation. And he won't
[01:08:46.540 --> 01:08:50.020]   specifically say what leads him to believe that this guy is
[01:08:50.020 --> 01:08:55.580]   cheating, but he thinks he is. He's, he's a young player. He's
[01:08:55.580 --> 01:09:00.740]   something like 18. And he's had a pretty meteoric rise in the
[01:09:00.740 --> 01:09:02.360]   chess world. I think his ratings gone up.
[01:09:02.360 --> 01:09:06.200]   A couple 100 points over the last two years is the fastest
[01:09:06.200 --> 01:09:11.360]   rise in chess rating that anyone's ever had before. It's
[01:09:11.360 --> 01:09:15.480]   been the case that players have been young players especially
[01:09:15.480 --> 01:09:20.360]   have risen quickly. But this is it's a pretty, it's the biggest
[01:09:20.360 --> 01:09:23.820]   thing that's happened. Well, that's the thing. So the the
[01:09:23.820 --> 01:09:29.700]   issue is that it's easy to cheat in online chess, but in over the
[01:09:29.700 --> 01:09:31.940]   board, you would either need to have a device, or you would need
[01:09:31.940 --> 01:09:36.680]   to be signaled by somebody in the crowd, you'd need human
[01:09:36.680 --> 01:09:40.680]   assistance or the assistance of a device, it almost be like, you
[01:09:40.680 --> 01:09:42.620]   know, in casino or something where they're using the
[01:09:42.620 --> 01:09:46.640]   contraption on the guy's leg to signal you know, to information
[01:09:46.640 --> 01:09:51.740]   to him. So nobody really knows how he would have done it. And
[01:09:51.740 --> 01:09:55.100]   of course, he wasn't caught doing it. So he can never prove
[01:09:55.100 --> 01:09:58.760]   that he wasn't. So there's this a real question here.
[01:09:58.760 --> 01:10:01.520]   But they did make some changes after after
[01:10:01.520 --> 01:10:06.440]   Magnus Carlsen resigned, they put the feed on a 15 minute delay.
[01:10:06.440 --> 01:10:10.220]   You know, I think like at some point, you're gonna have to
[01:10:10.220 --> 01:10:12.860]   start wanting people and having them go through a metal detector.
[01:10:12.860 --> 01:10:16.160]   Yeah, they're gonna have to toughen up all of the anti
[01:10:16.160 --> 01:10:19.460]   cheating standards. One of the grandmasters was like, we should
[01:10:19.460 --> 01:10:20.960]   play naked in a locker room.
[01:10:20.960 --> 01:10:25.700]   Yeah, there was an online site that basically offered to pay
[01:10:25.700 --> 01:10:29.660]   Neiman a lot of money to play basically naked to prove that he
[01:10:29.660 --> 01:10:31.100]   could, you know, really do it. He, you know, he was a good guy.
[01:10:31.100 --> 01:10:32.180]   He was a good guy. He was a good guy. He was a good guy. He was
[01:10:32.180 --> 01:10:32.720]   a good guy. He was a good guy. He was a good guy. He was a good guy.
[01:10:32.720 --> 01:10:33.080]   He was a good guy. He was a good guy. He was a good guy. He was a good guy.
[01:10:33.080 --> 01:10:33.080]   He was a good guy. He was a good guy. He was a good guy. He was a good guy.
[01:10:33.080 --> 01:10:33.080]   He was a good guy. He was a good guy. He was a good guy. He was a good guy.
[01:10:33.080 --> 01:10:38.660]   The theory on why he's not cheating and the rise is
[01:10:38.660 --> 01:10:43.100]   justified is that in theory, this is the argument is that he's
[01:10:43.100 --> 01:10:47.920]   grown up learning from all these neural nets, these not just
[01:10:47.920 --> 01:10:51.020]   chess engines, the first generation of chess engines were
[01:10:51.020 --> 01:10:54.200]   like Stockfish, they were just computational machines that were
[01:10:54.200 --> 01:10:57.980]   programmed by humans with 1000s of rules on how to play chess.
[01:10:58.160 --> 01:11:00.740]   And then they could just crunch the lines better than a human
[01:11:00.740 --> 01:11:03.400]   could. More recently, thanks to deep mind, it's a whole
[01:11:03.400 --> 01:11:07.220]   different type of machine is basically these neural networks,
[01:11:07.220 --> 01:11:11.240]   where all they're programmed with are the rules of chess, and
[01:11:11.240 --> 01:11:14.920]   then they place 1000s of games or millions of games against
[01:11:14.920 --> 01:11:19.960]   itself. And it learns the best way to play chess and the neural
[01:11:19.960 --> 01:11:24.440]   nets play in a whole different way than the than Stockfish than
[01:11:24.440 --> 01:11:27.600]   the pure engines, the engines display like a human that's able
[01:11:27.600 --> 01:11:28.140]   to calculate.
[01:11:28.140 --> 01:11:32.220]   really well. Whereas the neural nets do things like
[01:11:32.220 --> 01:11:35.340]   there's, I mean, there's sacrifices for that a human
[01:11:35.340 --> 01:11:36.060]   would never make.
[01:11:36.060 --> 01:11:38.640]   Yeah, they'll make they'll make sacrifices for disrepeat
[01:11:38.640 --> 01:11:41.420]   activity. So, you know, normally, when a human makes a
[01:11:41.420 --> 01:11:44.080]   sacrifice, they they'll recapture the material within a
[01:11:44.080 --> 01:11:49.920]   few moves. Whereas deep mind will sacrifice upon and, you
[01:11:49.920 --> 01:11:52.560]   know, just for the positional advantage or for the piece of the
[01:11:52.560 --> 01:11:55.380]   increase, which doesn't show up for many, many moves, right?
[01:11:55.380 --> 01:11:56.060]   Exactly.
[01:11:56.060 --> 01:11:57.000]   It's exactly
[01:11:57.000 --> 01:11:57.980]   and so the ability to
[01:11:57.980 --> 01:12:01.400]   model also 12 moves out is really but that's where I think
[01:12:01.400 --> 01:12:04.880]   people thought that Magnus picked up on something because it's like,
[01:12:04.880 --> 01:12:08.740]   those sorts of moves are rare in the absence of some layer of
[01:12:08.740 --> 01:12:11.420]   intervention, because typically, it's like, you know, and sacks,
[01:12:11.420 --> 01:12:13.940]   you know, this much better than I but it's like, you know, the
[01:12:13.940 --> 01:12:16.480]   opening and the closings of all of these chess matches are so
[01:12:16.480 --> 01:12:19.300]   tightly regulated, there's not a lot of creativity, it's sort of
[01:12:19.300 --> 01:12:22.400]   in the mid game that you have these slight positional
[01:12:22.400 --> 01:12:26.120]   advantages and disadvantages. And so I think it was amplified
[01:12:26.120 --> 01:12:27.900]   that one of these positional sacrifices made
[01:12:27.900 --> 01:12:31.660]   no sense in the context of that game, unless you had the ability
[01:12:31.660 --> 01:12:34.340]   to, you know, think really conclusively about six, seven
[01:12:34.340 --> 01:12:36.900]   moves from now. By the way, it's the same, it's the same thing
[01:12:36.900 --> 01:12:39.300]   in poker. So there's a thing if you guys want to download it,
[01:12:39.300 --> 01:12:42.360]   it's called, I think it's called poker snowy, that was like the
[01:12:42.360 --> 01:12:45.960]   first layer of a pretty basic AI. But it's gotten better and
[01:12:45.960 --> 01:12:48.240]   better. And really what it allows you to do is in every
[01:12:48.240 --> 01:12:51.660]   situational spot, whether it's heads up all the way to a six
[01:12:51.660 --> 01:12:56.640]   handed ring game, you can really understand, you know what to do
[01:12:56.640 --> 01:12:57.780]   now, poker is
[01:12:57.780 --> 01:13:01.140]   meaningfully more complicated than chess, as it turns out,
[01:13:01.140 --> 01:13:03.840]   because again, you have, you're not playing against one player,
[01:13:03.840 --> 01:13:06.240]   you're playing against some umpteen number of players, you
[01:13:06.240 --> 01:13:08.140]   know, we don't know when each of them is going to step into a
[01:13:08.140 --> 01:13:12.300]   pot or pot. And then there's all of those elements. So it's a
[01:13:12.300 --> 01:13:15.400]   little bit more complicated to build a true kind of like neural
[01:13:15.400 --> 01:13:18.720]   net around it. But even still, you can kind of get a sense of,
[01:13:18.720 --> 01:13:21.420]   of what you should be doing in different spots. And what you
[01:13:21.420 --> 01:13:26.320]   see is that there's literally like trillions of actions. And
[01:13:26.320 --> 01:13:27.460]   so it really is
[01:13:27.460 --> 01:13:32.280]   impossible for a human being to really memorize in every single
[01:13:32.280 --> 01:13:35.920]   situation, what the true, you know, probably probably
[01:13:35.920 --> 01:13:38.620]   realistically weighted GTO optimized move is going to be.
[01:13:38.620 --> 01:13:42.240]   And so this is why I think a lot of people, I guess, led by
[01:13:42.240 --> 01:13:45.860]   Magnus was saying, how could you have known this unless you were
[01:13:45.860 --> 01:13:48.820]   aided by something? It's pretty clear that this guy cheated.
[01:13:48.820 --> 01:13:51.320]   It's pretty, it's I think it's what's what's profound is that
[01:13:51.320 --> 01:13:55.480]   mastery of the game may not be achievable by a human. But
[01:13:55.480 --> 01:13:57.400]   mastery of any one of these games may actually be a very,
[01:13:57.400 --> 01:13:57.440]   very powerful game. And so I think that's what I think is
[01:13:57.440 --> 01:14:00.920]   really frustrating. And I think that's what's really
[01:14:00.920 --> 01:14:04.340]   frustrating to Magnus, and to other top tier players in the
[01:14:04.340 --> 01:14:07.320]   world that are the best humans at a particular game, is they're
[01:14:07.320 --> 01:14:11.360]   now realizing, you know, that they really aren't the true
[01:14:11.360 --> 01:14:14.420]   masters that the true masters are the neural nets, and more
[01:14:14.420 --> 01:14:17.100]   than just the guy cheating, which may feel like bad
[01:14:17.100 --> 01:14:20.480]   sportsmanship or whatever, fundamentally, one's ego being
[01:14:20.480 --> 01:14:23.300]   built entirely on one's mastery of the game and being the best
[01:14:23.300 --> 01:14:26.200]   at the game is fundamentally challenged because a computer
[01:14:26.200 --> 01:14:27.200]   is better at the game.
[01:14:27.200 --> 01:14:30.260]   than you. It's a really profound moment.
[01:14:30.260 --> 01:14:32.960]   That shouldn't be I think that that ship sailed a long time
[01:14:32.960 --> 01:14:36.720]   ago. Yeah, I mean, I know that just very well. Yeah, well, no,
[01:14:36.720 --> 01:14:40.520]   it was so it was way back when remember when Kasparov played
[01:14:40.520 --> 01:14:44.360]   deep blue, which is the IBM. Yeah, that was the first which
[01:14:44.360 --> 01:14:46.880]   wasn't even a sophisticated neural net. That was just a
[01:14:46.880 --> 01:14:49.400]   almost realistically modeled. Yeah, it was deterministically
[01:14:49.400 --> 01:14:51.720]   modeled system. It was like Stockfish. It was just a number
[01:14:51.720 --> 01:14:56.660]   cruncher. And that was when computer intelligence and chess
[01:14:56.660 --> 01:14:57.140]   tip to
[01:14:57.140 --> 01:14:59.820]   be able to beat the world champion. Since then, there's no
[01:14:59.820 --> 01:15:03.620]   looking back. I think every human who plays in the chess
[01:15:03.620 --> 01:15:06.860]   world understands that computers are better. And there's no way
[01:15:06.860 --> 01:15:11.020]   for a human to be better than a computer. So it's certainly not
[01:15:11.020 --> 01:15:13.520]   a neural net. I mean, Magnus certainly knows that I don't
[01:15:13.520 --> 01:15:16.020]   think anyone's bothered by that. I think that everyone
[01:15:16.020 --> 01:15:18.940]   understands that humans play in a certain way. And their goal is
[01:15:18.940 --> 01:15:23.360]   to be the best human. I think the concern is just obviously if
[01:15:23.360 --> 01:15:27.120]   a human it gets aided during a game by computers, but
[01:15:27.120 --> 01:15:32.220]   the way that chess works now, the preparation is all about
[01:15:32.220 --> 01:15:35.340]   working with computers, these top players spend huge amounts
[01:15:35.340 --> 01:15:39.380]   of time researching openings and looking for novelties in
[01:15:39.380 --> 01:15:43.940]   openings using computers to help them do their research. So, you
[01:15:43.940 --> 01:15:46.860]   know, working with computers has now become an integral part of
[01:15:46.860 --> 01:15:49.800]   the game. It's kind of like, you know, in many sports where the
[01:15:49.800 --> 01:15:53.680]   technology has enabled the athlete to get better, you know,
[01:15:53.680 --> 01:15:57.000]   and, and it's really technology becomes the key vector of
[01:15:57.000 --> 01:15:59.780]   competition. This actually happened in the NBA. This
[01:15:59.780 --> 01:16:02.700]   happened in the NBA where there's a special technology for
[01:16:02.700 --> 01:16:06.580]   three point shooting and your form, etc. And the Knicks
[01:16:06.580 --> 01:16:08.820]   actually and a couple of other teams implemented it a couple of
[01:16:08.820 --> 01:16:11.900]   years ago, and you saw the entire team became better at
[01:16:11.900 --> 01:16:15.420]   three point shooting. So people who were you know, 200% 300%,
[01:16:15.420 --> 01:16:18.920]   you know, moved up 10 20% each, but this kid is clearly cheating
[01:16:18.920 --> 01:16:22.080]   because he's cheated in the past and people who cheat in the
[01:16:22.080 --> 01:16:25.260]   past cheat in the future is my basic belief. So this guy Neiman
[01:16:25.260 --> 01:16:26.880]   publicly admitted that he used
[01:16:26.880 --> 01:16:29.660]   his electronic devices to cheat when he was just a kid on online
[01:16:29.660 --> 01:16:33.240]   games when he was 12 to 16 years old. So the question is like no,
[01:16:33.240 --> 01:16:35.760]   but Jason, it's worse than that, I think because then chess.com
[01:16:35.760 --> 01:16:39.000]   came out and said actually, the cheating was more rampant than
[01:16:39.000 --> 01:16:41.820]   just those two incidents. Their algorithm determined it. Yeah,
[01:16:41.820 --> 01:16:47.580]   yeah. So why are people playing with him? Well, because he said,
[01:16:47.580 --> 01:16:53.380]   yeah, he said that was those games were not for money. And he
[01:16:53.380 --> 01:16:56.760]   was just reading fast and he was like the poker guys who cheat
[01:16:56.760 --> 01:16:59.040]   you and what if you're a poker guy and you've cheated before you
[01:16:59.040 --> 01:17:01.480]   never play with those players again, they're cheaters. It's
[01:17:01.480 --> 01:17:04.720]   obvious. And the issue Well, the issue here is that this was an
[01:17:04.720 --> 01:17:08.660]   over the board tournament where the one that Magnus pulled out
[01:17:08.660 --> 01:17:12.160]   of in order to protest. So the question is, how could he have
[01:17:12.160 --> 01:17:15.820]   been cheating? Right? Now, I think what what adds to the
[01:17:15.820 --> 01:17:18.000]   complexity of it is that you have to remember that when
[01:17:18.000 --> 01:17:20.880]   you're dealing with a player, look, if it was us playing
[01:17:20.880 --> 01:17:23.640]   Magnus, we need to cheat on every single move. But if you're
[01:17:23.640 --> 01:17:25.640]   a 26 or 2700 player,
[01:17:25.640 --> 01:17:28.460]   you're going to have to cheat on every single move. And you
[01:17:28.460 --> 01:17:31.460]   know, Magnus is a 2860 or something higher the rating,
[01:17:31.460 --> 01:17:34.880]   the better in chess called the elo. But in any event, if you're
[01:17:34.880 --> 01:17:39.800]   like a 262700 player like Neiman, you only need help with
[01:17:39.800 --> 01:17:43.160]   a few moves in the game. In other words, if you could get a
[01:17:43.160 --> 01:17:47.180]   tip at a critical moment of the game, that might be all you need
[01:17:47.180 --> 01:17:49.040]   to put you over the top, you wouldn't need to, in other
[01:17:49.040 --> 01:17:51.500]   words, to beat Magnus, you wouldn't need to cheat on every
[01:17:51.500 --> 01:17:54.560]   move you could just if you just cheat on two or three moves at
[01:17:54.560 --> 01:17:55.520]   the right time,
[01:17:55.520 --> 01:17:56.520]   over the top.
[01:17:56.520 --> 01:18:01.080]   So something could vibrate on your leg, four times and then
[01:18:01.080 --> 01:18:03.500]   two times over to tell you which piece to move.
[01:18:03.500 --> 01:18:04.900]   That's what they were saying, Jake, I like he had like a
[01:18:04.900 --> 01:18:06.440]   vibrator and it's like,
[01:18:06.440 --> 01:18:08.580]   where was the vibrator?
[01:18:08.580 --> 01:18:11.300]   In his shoe. There was a meme that became like a whole thing
[01:18:11.300 --> 01:18:12.940]   where it was like he had something in his butt or
[01:18:12.940 --> 01:18:13.940]   something.
[01:18:13.940 --> 01:18:17.280]   An anal chest computer is what you're saying.
[01:18:17.280 --> 01:18:20.520]   Okay, look, that's not real. Somebody speculated on Reddit.
[01:18:20.520 --> 01:18:21.520]   Yeah, and then it became a meme.
[01:18:21.520 --> 01:18:25.400]   Somebody did a Reddit post saying that he was being communicated with
[01:18:25.400 --> 01:18:28.640]   through vibrating anal beads and then that went viral.
[01:18:28.640 --> 01:18:32.980]   Elon tweeted it, although I think he deleted it.
[01:18:32.980 --> 01:18:36.320]   It's absurd. But the point is, there is a question of how would
[01:18:36.320 --> 01:18:39.220]   he have done it if he did it?
[01:18:39.220 --> 01:18:43.220]   And there have been cases before though, people getting caught
[01:18:43.220 --> 01:18:46.020]   going to the bathroom and then you know, checking their phone.
[01:18:46.020 --> 01:18:48.760]   There's a Bulgarian grandmaster, he got caught in the bathroom
[01:18:48.760 --> 01:18:49.760]   checking his phone and...
[01:18:49.760 --> 01:18:51.020]   Oh, God.
[01:18:51.020 --> 01:18:54.280]   Metal detectors on the way in and they have RF detectors in the room, right?
[01:18:54.280 --> 01:18:55.280]   So they look for...
[01:18:55.280 --> 01:18:56.280]   They don't have any sort of communication.
[01:18:56.280 --> 01:18:57.280]   I think...
[01:18:57.280 --> 01:18:59.520]   Do you remember that going into this event?
[01:18:59.520 --> 01:19:01.160]   You just can have nobody watching live.
[01:19:01.160 --> 01:19:02.160]   No, they did it after Magnus...
[01:19:02.160 --> 01:19:03.160]   Make it a half hour delay.
[01:19:03.160 --> 01:19:07.680]   The Stingfield Cup added a 15 minute delay after these accusations.
[01:19:07.680 --> 01:19:12.460]   So yeah, I think there's going to be more precautions basically.
[01:19:12.460 --> 01:19:16.940]   But we still haven't heard from Magnus what made him think that there was foul play here
[01:19:16.940 --> 01:19:17.940]   in this particular game.
[01:19:17.940 --> 01:19:18.940]   And he's been very reluctant to...
[01:19:18.940 --> 01:19:19.940]   He won't say anything.
[01:19:19.940 --> 01:19:20.940]   He won't say anything.
[01:19:20.940 --> 01:19:25.160]   It's sort of passive aggressive. Now, the full side of it is that Magnus Carlsen
[01:19:25.160 --> 01:19:28.400]   has been the number one player in chess for over a decade.
[01:19:28.400 --> 01:19:30.400]   And he's a very classy player.
[01:19:30.400 --> 01:19:31.400]   I mean, I don't...
[01:19:31.400 --> 01:19:34.920]   Everyone's never heard a word said about him that was...
[01:19:34.920 --> 01:19:36.540]   Now, he can be cocky.
[01:19:36.540 --> 01:19:39.940]   He can definitely be cocky, but he's a very classy player.
[01:19:39.940 --> 01:19:43.560]   So I just don't see him doing something like this lightly.
[01:19:43.560 --> 01:19:44.560]   So that's what's so...
[01:19:44.560 --> 01:19:45.560]   Dramatic about this?
[01:19:45.560 --> 01:19:47.560]   ...hard to understand about it.
[01:19:47.560 --> 01:19:51.660]   Dramatic is, yeah, it's really hard to understand how Neiman could have cheated.
[01:19:51.660 --> 01:19:55.040]   But also, Magnus doesn't seem like the kind of player to just make a...
[01:19:55.040 --> 01:19:57.100]   A wild, impulsive allegation.
[01:19:57.100 --> 01:20:00.460]   So this is why the chess world has just been really roiled by this.
[01:20:00.460 --> 01:20:04.280]   Is there like any kind of equivalent of like PED scandal here?
[01:20:04.280 --> 01:20:10.720]   Like people taking Adderall or Provigil or any other kind of nootropics to get an edge?
[01:20:10.720 --> 01:20:12.860]   No, no one's ever said that.
[01:20:12.860 --> 01:20:16.640]   There's a good documentary, I think it was on Vice, on how a lot of the top chess players
[01:20:16.640 --> 01:20:19.300]   actually get ready for matches.
[01:20:19.300 --> 01:20:23.380]   And they're incredibly diligent about their sleep, their workout routine, alcohol.
[01:20:23.380 --> 01:20:24.920]   They do take supplements.
[01:20:24.920 --> 01:20:28.600]   I forgot who made the documentary, but it's actually incredibly intense how physical these
[01:20:28.600 --> 01:20:29.600]   people are.
[01:20:29.600 --> 01:20:34.080]   Well, like one game of classical chess can burn like over 2000 calories or something
[01:20:34.080 --> 01:20:35.080]   like that.
[01:20:35.080 --> 01:20:40.080]   Like the amount of calories that get burned just by using your brain so intensely.
[01:20:40.080 --> 01:20:44.500]   Adderall is rampant in the poker community, especially in tournaments.
[01:20:44.500 --> 01:20:49.780]   So when you play the higher tournaments and I played them and I just kind of step in and
[01:20:49.780 --> 01:20:50.780]   it's...
[01:20:50.780 --> 01:20:54.800]   I felt very underpowered relative to the kids I was playing with because they were all on
[01:20:54.800 --> 01:20:55.800]   Adderall.
[01:20:55.800 --> 01:20:56.800]   Well, these are 12 hours.
[01:20:56.800 --> 01:20:58.200]   Well, it's 10 hour sessions over 12 hours.
[01:20:58.200 --> 01:20:59.200]   10 hour sessions.
[01:20:59.200 --> 01:21:00.200]   It's exhausting.
[01:21:00.200 --> 01:21:02.260]   It's really, really physically exhausting to play.
[01:21:02.260 --> 01:21:04.160]   That's why I stopped playing tournaments about a decade ago.
[01:21:04.160 --> 01:21:06.200]   I just couldn't sit there for that long.
[01:21:06.200 --> 01:21:08.900]   Can't justify the time suck of it too.
[01:21:08.900 --> 01:21:15.920]   It's just so physically and emotionally demanding to be able to play that well and make no mistakes
[01:21:15.920 --> 01:21:18.140]   over 10 hours.
[01:21:18.140 --> 01:21:21.680]   And so, you know, the only solution that all these kids would turn to was Adderall.
[01:21:21.680 --> 01:21:24.120]   And I was like, this is not worth it for me.
[01:21:24.120 --> 01:21:24.680]   So I stopped.
[01:21:24.680 --> 01:21:27.960]   I stopped playing in tournaments and which was too bad because I thought like I could
[01:21:27.960 --> 01:21:30.500]   have a real chance of actually doing reasonably well in some of these things.
[01:21:30.500 --> 01:21:31.500]   And I just gave up.
[01:21:31.500 --> 01:21:34.740]   By the way, the other thing I wanted to mention is there's been like cheating in all these
[01:21:34.740 --> 01:21:39.720]   other kinds of sports and always gets exposed like whether it's, you know, the Tour de France,
[01:21:39.720 --> 01:21:44.280]   it turned out that everybody was using PEDs or, you know, the Russian Olympic team, everybody
[01:21:44.280 --> 01:21:45.280]   was using PEDs.
[01:21:45.280 --> 01:21:47.360]   New England Patriots stealing signals.
[01:21:47.360 --> 01:21:48.360]   Yeah.
[01:21:48.360 --> 01:21:52.000]   All right, listen, let's just talk about the Iran protests for a moment.
[01:21:52.000 --> 01:21:54.560]   Iran has shut off internet access.
[01:21:54.560 --> 01:21:59.820]   It's in parts of Tehran and blocked access to Instagram and WhatsApp to try to stop these
[01:21:59.820 --> 01:22:00.820]   protests.
[01:22:00.820 --> 01:22:04.720]   The protests started after the death of a 22 year old Kurdish woman while in police
[01:22:04.720 --> 01:22:06.360]   custody.
[01:22:06.360 --> 01:22:12.680]   Masa Amini was detained on September 16 for allegedly wearing a hijab headscarf in an
[01:22:12.680 --> 01:22:14.380]   improper way.
[01:22:14.380 --> 01:22:18.560]   She later died in police custody.
[01:22:18.560 --> 01:22:23.640]   And activists are saying she suffered a fatal blow to her head.
[01:22:23.640 --> 01:22:24.440]   And now you have 15 minutes.
[01:22:24.440 --> 01:22:27.420]   15 cities with very large crowds.
[01:22:27.420 --> 01:22:34.720]   Women are burning their hijabs and it seems to be escalating at a pretty fast pace.
[01:22:34.720 --> 01:22:41.160]   Iranian authorities are denying that they had any part in her death.
[01:22:41.160 --> 01:22:42.160]   Thoughts on this?
[01:22:42.160 --> 01:22:43.160]   It's horrible.
[01:22:43.160 --> 01:22:44.160]   Yeah.
[01:22:44.160 --> 01:22:45.160]   I wish them well.
[01:22:45.160 --> 01:22:50.200]   If you look at the demographics of Iran, it's pretty amazing how many young people there
[01:22:50.200 --> 01:22:51.200]   are here.
[01:22:51.200 --> 01:22:52.700]   This feels like a country that could turn over.
[01:22:52.700 --> 01:22:53.700]   That'd be great.
[01:22:53.700 --> 01:22:54.320]   And you want to know what's going on?
[01:22:54.320 --> 01:22:57.920]   I don't know why it has a chance of working because we're not the ones behind it.
[01:22:57.920 --> 01:22:58.920]   I mean, absolutely.
[01:22:58.920 --> 01:22:59.920]   Yeah.
[01:22:59.920 --> 01:23:03.540]   We cannot run the revolution.
[01:23:03.540 --> 01:23:05.160]   The revolution has to be done.
[01:23:05.160 --> 01:23:11.200]   But to the women and young people of Iran protesting, you have our support and we're rooting for
[01:23:11.200 --> 01:23:12.200]   you.
[01:23:12.200 --> 01:23:14.260]   And don't let the bastards grind you down.
[01:23:14.260 --> 01:23:15.600]   All right, everybody.
[01:23:15.600 --> 01:23:16.600]   For David Friedberg.
[01:23:16.600 --> 01:23:17.600]   Forgot her names?
[01:23:17.600 --> 01:23:18.600]   Did you forget her names?
[01:23:18.600 --> 01:23:20.320]   No, I'm just I was gonna try to come up with a new name for her.
[01:23:20.320 --> 01:23:21.320]   I don't know.
[01:23:21.320 --> 01:23:22.320]   I don't know.
[01:23:22.320 --> 01:23:23.320]   I'm just gonna.
[01:23:23.320 --> 01:23:24.320]   I don't know.
[01:23:24.320 --> 01:23:25.320]   I'm just gonna.
[01:23:25.320 --> 01:23:26.320]   I'm just gonna.
[01:23:26.320 --> 01:23:27.320]   I'll take David Plainview.
[01:23:27.320 --> 01:23:28.320]   For Rageburg.
[01:23:28.320 --> 01:23:29.320]   Like Friedberg.
[01:23:29.320 --> 01:23:30.320]   David Plainview.
[01:23:30.320 --> 01:23:31.320]   We'll take David Plainview.
[01:23:31.320 --> 01:23:32.320]   Rageburger.
[01:23:32.320 --> 01:23:33.320]   Rageburger.
[01:23:33.320 --> 01:23:34.320]   Rageburger Plainview.
[01:23:34.320 --> 01:23:35.320]   Rageburger Plainview.
[01:23:35.320 --> 01:23:36.320]   The David the Dove.
[01:23:36.320 --> 01:23:40.000]   And the host with the most, Chamath Palihapitiya.
[01:23:40.000 --> 01:23:41.000]   I'm the world's greatest moderator.
[01:23:41.000 --> 01:23:43.320]   I'm so excited to see all three of you.
[01:23:43.320 --> 01:23:44.320]   Honestly.
[01:23:44.320 --> 01:23:45.320]   Big hug.
[01:23:45.320 --> 01:23:46.320]   Sacks.
[01:23:46.320 --> 01:23:47.320]   Don't flake.
[01:23:47.320 --> 01:23:48.320]   I love you guys.
[01:23:48.320 --> 01:23:49.320]   I really want to see this.
[01:23:49.320 --> 01:23:50.320]   Don't flake, Sacks.
[01:23:50.320 --> 01:23:51.320]   Don't grow up in a ball.
[01:23:51.320 --> 01:23:52.320]   We should roll down together.
[01:23:52.320 --> 01:23:53.320]   Let's roll, baby.
[01:23:53.320 --> 01:23:54.200]   Big game.
[01:23:54.200 --> 01:23:56.540]   Or let your winners ride
[01:23:56.540 --> 01:23:59.360]   Rain Man, David Sack
[01:23:59.360 --> 01:24:02.260]   I'm going all in
[01:24:02.260 --> 01:24:02.980]   And it said
[01:24:02.980 --> 01:24:04.580]   We open sourced it to the fans
[01:24:04.580 --> 01:24:06.240]   And they've just gone crazy with it
[01:24:06.240 --> 01:24:10.260]   I'm going all in
[01:24:10.260 --> 01:24:12.820]   Let your winners ride
[01:24:12.820 --> 01:24:15.020]   I'm going all in
[01:24:15.020 --> 01:24:16.360]   Besties are back
[01:24:16.360 --> 01:24:17.020]   Go 13
[01:24:17.020 --> 01:24:19.960]   That's my dog taking a notice in your driveway
[01:24:19.960 --> 01:24:23.740]   Oh man
[01:24:23.740 --> 01:24:25.580]   My avid basher will meet me at place
[01:24:25.580 --> 01:24:28.400]   We should all just get a room and just have one big huge orgy
[01:24:28.400 --> 01:24:29.360]   Cause they're all just useless
[01:24:29.360 --> 01:24:32.560]   It's like this sexual tension but they just need to release it now
[01:24:32.560 --> 01:24:35.240]   Wet your feet
[01:24:35.240 --> 01:24:37.240]   Wet your feet
[01:24:37.240 --> 01:24:40.040]   We need to get merch
[01:24:40.040 --> 01:24:40.720]   Besties are back
[01:24:40.720 --> 01:24:42.260]   I'm going all in
[01:24:42.260 --> 01:24:51.720]   I'm going all in
[01:24:51.720 --> 01:25:21.700]   Thank you.

