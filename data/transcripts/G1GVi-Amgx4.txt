
[00:00:00.000 --> 00:00:02.640]   I don't know if like, there's this circuit that's weak and
[00:00:02.640 --> 00:00:05.120]   getting stronger. I don't know if it's something that works,
[00:00:05.120 --> 00:00:09.640]   but not very well, like, I think we don't know. And these are
[00:00:09.640 --> 00:00:11.120]   some of the questions we're trying to answer with
[00:00:11.120 --> 00:00:12.240]   mechanistic interpretability.
[00:00:12.240 --> 00:00:15.720]   If you had to put it in terms of human psychology, what is the
[00:00:15.720 --> 00:00:20.520]   change that is happening? Are we creating new drives, new goals,
[00:00:20.520 --> 00:00:23.720]   new thoughts? How is the model is changing in terms of
[00:00:23.720 --> 00:00:24.640]   psychology when
[00:00:24.640 --> 00:00:28.240]   I think all those terms are kind of like inadequate for you know,
[00:00:28.240 --> 00:00:30.560]   describing what's it's not clear how useful they are as
[00:00:30.560 --> 00:00:33.320]   abstractions for humans, either. I think we don't have the
[00:00:33.320 --> 00:00:35.840]   language to describe what's going on. I'd love to look
[00:00:35.840 --> 00:00:39.280]   inside and say, and kind of actually know what we're talking
[00:00:39.280 --> 00:00:42.680]   about, instead of, you know, what basically making up words,
[00:00:42.680 --> 00:00:45.440]   which is what which is what I do, what you're doing, asking
[00:00:45.440 --> 00:00:48.840]   this question, where, you know, we should, we should just be
[00:00:48.840 --> 00:00:51.560]   honest, we really have very little idea what we're what
[00:00:51.560 --> 00:00:53.960]   we're talking about. So, you know, it would be great to say,
[00:00:53.960 --> 00:00:57.360]   well, what we actually mean by that is, you know, this circuit
[00:00:57.360 --> 00:01:00.680]   within here turns, you know, turns on, and, you know, and,
[00:01:00.680 --> 00:01:03.160]   you know, after we've trained the model, then, you know, this
[00:01:03.160 --> 00:01:06.560]   circuit is no longer operative or weaker. And that would love
[00:01:06.560 --> 00:01:09.560]   to be able to say, again, we're going to take a lot of work to
[00:01:09.560 --> 00:01:10.400]   be able to do that.
[00:01:10.400 --> 00:01:13.560]   mechanistically, what is alignment? Is it that you're
[00:01:13.560 --> 00:01:18.200]   locking in the model into a benevolent character? Are you
[00:01:18.200 --> 00:01:21.320]   disabling deceptive circuits and procedures like what
[00:01:21.320 --> 00:01:23.960]   concretely is happening? Yeah, when you align a model,
[00:01:24.000 --> 00:01:27.480]   I think as with most things, you know, when we actually train a
[00:01:27.480 --> 00:01:30.360]   model to be aligned, we don't know what happens inside the
[00:01:30.360 --> 00:01:32.600]   model, right? There are different ways of training it to
[00:01:32.600 --> 00:01:35.400]   be aligned. But I think we don't really know what happens. I
[00:01:35.400 --> 00:01:38.640]   mean, I think for some of the current methods, I think all the
[00:01:38.640 --> 00:01:41.440]   current methods that involve some kind of fine tuning, of
[00:01:41.440 --> 00:01:44.160]   course, have the property that the underlying knowledge and
[00:01:44.160 --> 00:01:47.120]   abilities that we might be worried about, don't don't
[00:01:47.120 --> 00:01:50.880]   disappear. It's just, you know, the model is just taught not to
[00:01:50.880 --> 00:01:54.640]   output them. I don't know if that's a fatal flaw or if, you
[00:01:54.640 --> 00:01:57.640]   know, or if that's just the way things have to be. I think this
[00:01:57.640 --> 00:02:00.080]   problem would be much easier if you had an oracle that could
[00:02:00.080 --> 00:02:04.040]   just scan a model and say, like, okay, I know this model is
[00:02:04.040 --> 00:02:07.520]   aligned, I know what it will do in every situation, then the
[00:02:07.520 --> 00:02:10.680]   problem would be much easier. And I think the closest thing we
[00:02:10.680 --> 00:02:13.800]   have to that is something like mechanistic interpretability.
[00:02:13.800 --> 00:02:17.360]   It's not anywhere near up to the task yet. But mechanistic
[00:02:17.360 --> 00:02:21.120]   interpretability is the only thing that even in principle, is
[00:02:21.120 --> 00:02:24.400]   the thing where it's like, it's more like an X ray of the model
[00:02:24.400 --> 00:02:26.480]   than a modification of the model, right? It's more like an
[00:02:26.480 --> 00:02:29.480]   assessment than an intervention. I don't know what's going on
[00:02:29.480 --> 00:02:31.840]   inside mechanistically. And I think that's the whole point of
[00:02:31.840 --> 00:02:35.200]   mechanistic interpretability, to really understand what's going
[00:02:35.200 --> 00:02:39.240]   on inside the models at the level of individual circuits. I
[00:02:39.240 --> 00:02:41.920]   think what we're hoping for in the end is not that we'll
[00:02:41.920 --> 00:02:45.240]   understand every detail. But again, I would give like the X
[00:02:45.240 --> 00:02:49.080]   ray or the MRI analogy that like, we can be in a position
[00:02:49.080 --> 00:02:52.800]   where we can look at the broad features of the model and say,
[00:02:52.800 --> 00:02:57.000]   like, is this a model whose internal state and plans are
[00:02:57.000 --> 00:03:00.040]   very different from what it externally represents itself to
[00:03:00.040 --> 00:03:04.480]   do, right? Is this a model where we're uncomfortable that, you
[00:03:04.480 --> 00:03:08.600]   know, far too much of its computational power is, you
[00:03:08.600 --> 00:03:11.880]   know, is devoted to doing what looked like fairly destructive
[00:03:11.880 --> 00:03:15.560]   and manipulative things. And I give an analogy like to humans.
[00:03:15.560 --> 00:03:21.320]   So it's actually possible to, you know, to look at an MRI of
[00:03:21.320 --> 00:03:25.120]   someone and predict above random chance, whether they're a
[00:03:25.120 --> 00:03:28.480]   psychopath. There was actually a story a few years back about a
[00:03:28.480 --> 00:03:31.880]   neuroscientist who was studying this, and he looked at his own
[00:03:31.880 --> 00:03:34.760]   scan and discovered that he was a psychopath. And then everyone,
[00:03:34.760 --> 00:03:37.560]   everyone in his life was like, no, no, that's just obvious.
[00:03:37.560 --> 00:03:40.240]   Like, you're, you're a complete asshole. You must be a
[00:03:40.240 --> 00:03:44.040]   psychopath. And he was totally, totally unaware of this. The
[00:03:44.040 --> 00:03:48.360]   basic idea that, you know, that there can be these macro
[00:03:48.360 --> 00:03:50.720]   features that like, like psychopath is probably a good
[00:03:50.720 --> 00:03:53.200]   analogy for it, right? They're like, you know, this is what we
[00:03:53.200 --> 00:03:55.680]   would be afraid of model that's kind of like, charming on the
[00:03:55.680 --> 00:03:58.760]   surface, very goal oriented, and you know, very dark on the
[00:03:58.760 --> 00:03:59.200]   inside.
[00:03:59.200 --> 00:04:01.920]   Do you think that cloud has conscious experience? How likely
[00:04:01.920 --> 00:04:02.560]   do you think that is?
[00:04:02.560 --> 00:04:05.240]   This is another of these questions that just seems very
[00:04:05.240 --> 00:04:07.800]   unsettled and uncertain. Conscious is again, one of these
[00:04:07.800 --> 00:04:11.520]   words that I suspect it will like, not end up having a, a
[00:04:11.520 --> 00:04:16.520]   well defined, but yeah, but, but that, yeah, well, I suspect
[00:04:16.520 --> 00:04:19.280]   that's, that's, that's the spectrum, right? So I don't
[00:04:19.280 --> 00:04:21.920]   know if we, if we, if we discover like that, you know,
[00:04:21.920 --> 00:04:24.480]   that I should care about, let's say we discover that I should
[00:04:24.480 --> 00:04:27.720]   care about Claude's experience as much as I should care about
[00:04:27.720 --> 00:04:31.080]   like a dog or a monkey or something. Yeah, I would be, I
[00:04:31.080 --> 00:04:33.720]   would be kind of, kind of worried. I don't know if their
[00:04:33.720 --> 00:04:36.760]   experience is positive or negative. Unsettlingly, I also
[00:04:36.760 --> 00:04:41.160]   don't know, like, I wouldn't know if any intervention that
[00:04:41.160 --> 00:04:44.440]   we made was more likely to make Claude, you know, have a
[00:04:44.440 --> 00:04:47.040]   positive versus negative experience versus not having
[00:04:47.040 --> 00:04:49.800]   one. If there's an area that is helpful with this, it's maybe
[00:04:49.800 --> 00:04:52.280]   mechanistic interpretability, because I think of it as
[00:04:52.280 --> 00:04:55.640]   neuroscience for models. And so it's possible that we could, we
[00:04:55.640 --> 00:04:58.720]   could shed some, shed some light on this, although, you know,
[00:04:58.720 --> 00:05:01.200]   it's not, it's not a straightforward, factual

