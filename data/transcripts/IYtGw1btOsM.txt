
[00:00:00.000 --> 00:00:05.000]   First thing first, let's reproduce Direct's results on my own machine.
[00:00:05.000 --> 00:00:07.000]   How do we do that?
[00:00:07.000 --> 00:00:11.000]   On the report, we can click on the Run table and click on the Baseline run.
[00:00:11.000 --> 00:00:15.000]   This will redirect us to the workspace.
[00:00:15.000 --> 00:00:19.000]   Once on the workspace, you can click on the Overview tab.
[00:00:19.000 --> 00:00:23.000]   Here we can see all the information relevant to the run.
[00:00:23.000 --> 00:00:26.000]   We see that Direct ran this on November 14.
[00:00:26.000 --> 00:00:29.000]   We can see the version of Python that was used.
[00:00:29.000 --> 00:00:31.000]   The Git repo where the code lives.
[00:00:31.000 --> 00:00:37.000]   And the state of the Git repo.
[00:00:37.000 --> 00:00:42.000]   We can also see hardware information like the GPU that was used for training.
[00:00:42.000 --> 00:00:48.000]   Scrolling down, you get the model's artifacts, inputs and outputs that the run consumed and produced.
[00:00:48.000 --> 00:00:51.000]   If you scroll down, you see the output parameters.
[00:00:51.000 --> 00:00:55.000]   We also see the output parameters values that were used for training.
[00:00:55.000 --> 00:00:58.000]   We are almost there for reproducing Direct's experiment.
[00:00:58.000 --> 00:01:02.000]   There is an extra goody that Weights & Bias provides you for free.
[00:01:02.000 --> 00:01:06.000]   It automatically detects the Python packages that were used during training,
[00:01:06.000 --> 00:01:12.000]   creating this handy Conda environment file that you can use to recreate the same environment that was used.
[00:01:12.000 --> 00:01:21.000]   Every single Python package is pinned, so you will reproduce exactly what the machine was using at the moment of training.
[00:01:21.000 --> 00:01:25.000]   You can click to download the file, and now we are ready to reproduce Direct's experiment.
[00:01:25.000 --> 00:01:28.000]   Let's switch to a terminal.
[00:01:28.000 --> 00:01:32.000]   Once in the terminal, the first thing I will do is check that the Git commit match.
[00:01:32.000 --> 00:01:38.000]   You can do this by using Git log, and check that the hash is the same that the reported by Weights & Biases.
[00:01:38.000 --> 00:01:39.000]   We are OK.
[00:01:39.000 --> 00:01:43.000]   I have already created a folder called lesson2 where we will put our work.
[00:01:43.000 --> 00:01:46.000]   I have already also copied the Conda environment file.
[00:01:46.000 --> 00:01:50.000]   One extra step is needed, we will need to copy the baseline.
[00:01:50.000 --> 00:01:52.000]   Let's take a look at the file.
[00:01:52.000 --> 00:01:58.000]   This will create an environment called course, and we will install every single Python package that is in this list.
[00:01:58.000 --> 00:02:02.000]   Every single package is pinned, so we will get the exact same version.
[00:02:02.000 --> 00:02:04.000]   Let's go ahead and create the environment.
[00:02:04.000 --> 00:02:09.000]   You can do this by calling conda-env create and passing the environment file.
[00:02:09.000 --> 00:02:14.000]   This will take a while, so let's fast forward to the environment already created.
[00:02:14.000 --> 00:02:21.000]   Let's activate the environment.
[00:02:21.000 --> 00:02:30.000]   Let's bring the baseline file, so we can reproduce Darek's experiment.
[00:02:30.000 --> 00:02:39.000]   Now we will launch Jupyter and run this notebook.
[00:02:39.000 --> 00:02:45.000]   Once inside Jupyter, I can open the baseline file, and I will run every single cell in order.
[00:02:45.000 --> 00:02:55.000]   To do so, you can call run, restart kernel and run all.
[00:02:55.000 --> 00:03:00.000]   Reproducing this baseline run is a crucial step when you are taking over someone's work.
[00:03:00.000 --> 00:03:04.000]   Let's make sure that the notebook is running correctly.
[00:03:04.000 --> 00:03:09.000]   The Watsonvices run is created, and training should start soon.
[00:03:09.000 --> 00:03:11.000]   Let's see the progress bar increasing.
[00:03:11.000 --> 00:03:14.000]   We love progress bars.
[00:03:14.000 --> 00:03:17.000]   Ok, the model is training fine.
[00:03:17.000 --> 00:03:21.000]   Let's fast forward to the end of the training.
[00:03:21.000 --> 00:03:27.000]   You can scroll down to the end of the notebook and click on the run link.
[00:03:27.000 --> 00:03:29.000]   This will bring you to the workspace.
[00:03:29.000 --> 00:03:34.000]   Ok, we get the metrics we were expecting and the table with the model predictions.
[00:03:34.000 --> 00:03:38.000]   We can go back to the workspace and compare this run against the baseline run.
[00:03:38.000 --> 00:03:41.000]   As you can see, both runs overlaps perfectly.
[00:03:41.000 --> 00:03:43.000]   Let's make sure of that.
[00:03:43.000 --> 00:03:46.000]   Let's switch to table view.
[00:03:46.000 --> 00:03:50.000]   Hmm, the metrics look exactly the same.
[00:03:50.000 --> 00:03:55.000]   Now we are in a good spot to start refactoring baseline and improving the results.
[00:03:55.000 --> 00:04:00.000]   Thanks for watching!
[00:04:00.000 --> 00:04:10.000]   www.microsoft.com/mechanics

