
[00:00:00.000 --> 00:00:02.600]   All right welcome back to machine learning I
[00:00:02.600 --> 00:00:08.480]   I'm really excited to be able to share some amazing stuff that
[00:00:08.480 --> 00:00:13.080]   University of San Francisco students have built during the week or written about during the week
[00:00:13.080 --> 00:00:15.280]   and
[00:00:15.280 --> 00:00:17.400]   Quite a few things. I'm going to show you have already
[00:00:17.400 --> 00:00:20.640]   spread around the internet quite a bit
[00:00:20.640 --> 00:00:23.200]   lots of
[00:00:23.200 --> 00:00:26.880]   Tweets and posts and all kinds of stuff happening
[00:00:28.040 --> 00:00:34.880]   One of the the first to be widely shared was this one by Tyler who did something really interesting?
[00:00:34.880 --> 00:00:43.940]   He started out by saying like what if I like create the synthetic data set where the independent variables is like the x and the y
[00:00:43.940 --> 00:00:48.080]   And the dependent variable is like color right and interestingly
[00:00:48.080 --> 00:00:51.080]   He showed me an earlier version of this where he wasn't using color
[00:00:51.080 --> 00:00:54.320]   he was just like putting the actual numbers in here and
[00:00:54.840 --> 00:01:00.640]   this thing kind of wasn't really working at all and as soon as he started using color it started working really well and
[00:01:00.640 --> 00:01:05.600]   So I wanted to mention that one of the things that unfortunately we we don't teach you
[00:01:05.600 --> 00:01:08.120]   at USF is
[00:01:08.120 --> 00:01:10.840]   Theory of human perception perhaps we should
[00:01:10.840 --> 00:01:17.080]   Because actually when it comes to visualization it's kind of the most important thing to know is what is the human eye?
[00:01:17.080 --> 00:01:23.780]   Or what is what is what is the human brain good at perceiving? There's a whole area of academic study on this
[00:01:24.400 --> 00:01:28.040]   And one of the things that we're best at perceiving is differences in color
[00:01:28.040 --> 00:01:34.340]   Right so that's why as soon as we look at this picture of the synthetic data. He created you can immediately see oh there's kind of four
[00:01:34.340 --> 00:01:37.120]   areas of you know lighter red
[00:01:37.120 --> 00:01:38.680]   color
[00:01:38.680 --> 00:01:40.840]   So what he did was he said okay?
[00:01:40.840 --> 00:01:46.720]   What if we like tried to create a machine learning model of this synthetic data set?
[00:01:46.720 --> 00:01:52.240]   And so specifically he created a tree and the cool thing is that you can actually draw
[00:01:52.840 --> 00:01:55.440]   The tree right so after he created the tree
[00:01:55.440 --> 00:02:01.840]   He did this all in that plot live that plot lead is very flexible right he actually drew the tree boundaries
[00:02:01.840 --> 00:02:07.800]   So that's already a pretty neat trick is to be actually able to draw the tree
[00:02:07.800 --> 00:02:10.800]   But then he did something even cleverer which is he said okay?
[00:02:10.800 --> 00:02:16.960]   So what predictions does the tree make well it's the average of each of these areas and so to do that
[00:02:16.960 --> 00:02:18.960]   We can actually draw the average color
[00:02:18.960 --> 00:02:21.640]   Right it's actually kind of pretty
[00:02:22.720 --> 00:02:25.560]   Here is the predictions that the tree makes
[00:02:25.560 --> 00:02:28.120]   now
[00:02:28.120 --> 00:02:32.560]   Here's where it gets really interesting. It's like you can as you know randomly
[00:02:32.560 --> 00:02:36.680]   generate trees through resampling and
[00:02:36.680 --> 00:02:39.600]   So here are four trees
[00:02:39.600 --> 00:02:43.880]   Generated through resampling they're all like pretty similar, but a little bit different
[00:02:43.880 --> 00:02:51.660]   And so now we can actually visualize bagging and to visualize bagging we literally take the average of the four pictures
[00:02:52.000 --> 00:02:54.000]   All right. That's what bagging is and
[00:02:54.000 --> 00:03:01.440]   There it is right and so here is like the the fuzzy decision boundaries of a random forest
[00:03:01.440 --> 00:03:08.880]   And I think this is kind of amazing right because it's like a I wish I had this actually when I started teaching you
[00:03:08.880 --> 00:03:13.920]   All random forests because I could have skipped a couple of classes. It's just like okay. That's what we do
[00:03:13.920 --> 00:03:18.360]   You know we create the decision boundaries we average each area
[00:03:18.360 --> 00:03:21.880]   And then we we do it a few times and average all of them
[00:03:21.960 --> 00:03:26.320]   Okay, so that's what a random forest does and I think like this is just such a great example of
[00:03:26.320 --> 00:03:32.160]   Making the complex easy through through pictures
[00:03:32.160 --> 00:03:34.840]   So congrats to Tyler for that
[00:03:34.840 --> 00:03:37.360]   It actually turns out
[00:03:37.360 --> 00:03:44.000]   That he has actually reinvented something that somebody else has already done a guy called Christian any who went on to be a
[00:03:44.000 --> 00:03:51.000]   One of the world's foremost machine learning researchers actually included almost exactly this technique in a book
[00:03:51.000 --> 00:03:54.880]   He wrote about decision forests, so it's actually kind of cool that Tyler ended up
[00:03:54.880 --> 00:04:01.720]   Reinventing something that one of the world's foremost and for authorities on the fifth decision forests actually it has created
[00:04:01.720 --> 00:04:03.200]   So I thought that was neat
[00:04:03.200 --> 00:04:05.960]   That's nice because when we pop when we posted this on Twitter
[00:04:05.960 --> 00:04:09.800]   You know got a lot of attention and finally somebody with that was able to say like oh
[00:04:09.800 --> 00:04:14.560]   You know what this this actually already exists, so Tyler's gone away, and you know started reading that book
[00:04:17.160 --> 00:04:20.520]   Something else which is super cool is Jason Carpenter
[00:04:20.520 --> 00:04:26.960]   Created a whole new library called parfit and parfit is a
[00:04:26.960 --> 00:04:31.200]   parallelized fitting of multiple models for the purpose of
[00:04:31.200 --> 00:04:36.560]   Selecting hyper parameters, and there's a lot. I really like about this
[00:04:36.560 --> 00:04:46.780]   He's shown a clear example of how to use it right and like the API looks very similar to other grid search based approaches
[00:04:46.940 --> 00:04:48.940]   But it uses the validation
[00:04:48.940 --> 00:04:50.780]   techniques that
[00:04:50.780 --> 00:04:55.580]   Rachel wrote about and that we learned about a couple of weeks ago of using a good validation set
[00:04:55.580 --> 00:04:57.820]   and
[00:04:57.820 --> 00:05:04.180]   You know what he's done here is in his blog post that introduces it. You know he's he's
[00:05:04.180 --> 00:05:06.820]   Gone right back and said like well
[00:05:06.820 --> 00:05:09.140]   What are hyper parameters why do we have to train them?
[00:05:09.140 --> 00:05:15.820]   And he's kind of explained every step and then the the module itself is like it's it's very polished
[00:05:15.820 --> 00:05:19.620]   You know he's added documentation to it. He's added a nice read me to it
[00:05:19.620 --> 00:05:22.940]   And it's kind of interesting when you actually look at the code you realize
[00:05:22.940 --> 00:05:29.700]   You know it's very simple. You know which is it's definitely not a bad thing. That's a good thing is to make things simple
[00:05:29.700 --> 00:05:33.100]   but by kind of
[00:05:33.100 --> 00:05:35.700]   Writing this little bit of code and then packaging it up so nicely
[00:05:35.700 --> 00:05:39.220]   He's made it really easy for other people to use this technique
[00:05:39.220 --> 00:05:41.620]   which is great and so
[00:05:42.460 --> 00:05:44.660]   one of the things I've been really thrilled to see is then
[00:05:44.660 --> 00:05:50.180]   Vinay went along and combined two things from our class one was to take
[00:05:50.180 --> 00:05:56.020]   Parfit and then the other was to take the kind of accelerated SGD approach to classification
[00:05:56.020 --> 00:06:02.100]   We don't learned about in the last lesson and combine the two to say like okay. Well. Let's now use
[00:06:02.100 --> 00:06:05.740]   Parfit to help us find the parameters of a
[00:06:05.740 --> 00:06:08.580]   SGD logistic aggression
[00:06:08.580 --> 00:06:11.460]   So I think that's really a really great idea
[00:06:12.180 --> 00:06:14.100]   something
[00:06:14.100 --> 00:06:16.100]   else which I thought was terrific is
[00:06:16.100 --> 00:06:18.780]   Prince actually
[00:06:18.780 --> 00:06:20.780]   basically went through and
[00:06:20.780 --> 00:06:27.980]   Summarized pretty much all the stuff we learned in the random and for a random forest interpretation class
[00:06:27.980 --> 00:06:35.120]   And he went even further than that as he described each of the different approaches to random forest interpretation
[00:06:37.020 --> 00:06:44.860]   He described how it's done so here for example is feature importance through variable permutation a little picture of each one and
[00:06:44.860 --> 00:06:49.400]   Then super cool here is the code to implement it from scratch
[00:06:49.400 --> 00:06:52.580]   So I think this is like really
[00:06:52.580 --> 00:07:00.740]   Nice post you know describing something that not many people understand and showing you know exactly how it works both with pictures
[00:07:00.740 --> 00:07:03.620]   And with code that implements it from scratch
[00:07:04.340 --> 00:07:09.100]   So I think that's really really great one of the things. I really like here is that for like the
[00:07:09.100 --> 00:07:14.320]   Tree interpreter, but he actually showed how you can take the tree interpreter
[00:07:14.320 --> 00:07:19.300]   output and feed it into the new waterfall chart package that
[00:07:19.300 --> 00:07:23.260]   Chris our USF student built to show how you can actually visualize
[00:07:23.260 --> 00:07:30.140]   The contributions of the tree interpreter in a waterfall chart so again kind of a nice combination of
[00:07:30.740 --> 00:07:36.100]   multiple pieces of technology we've both learned about and and built as a group I
[00:07:36.100 --> 00:07:39.860]   Also really thought this
[00:07:39.860 --> 00:07:45.460]   Kernel there's been a few interesting kernels shared and I'll share some more next week and devesh wrote this really nice kernel
[00:07:45.460 --> 00:07:51.400]   Showing there's this quite challenging Kaggle competition on detecting icebergs
[00:07:51.400 --> 00:07:53.420]   versus
[00:07:53.420 --> 00:08:01.940]   Ships and it's a kind of a weird two channel satellite data. Which is very hard to visualize and he actually
[00:08:01.940 --> 00:08:10.420]   Went through and basically described kind of the formulas for how these like radar scattering things actually work
[00:08:10.420 --> 00:08:17.140]   And then actually managed to come up with a code that allowed him to recreate
[00:08:17.140 --> 00:08:19.780]   You know the actual 3d?
[00:08:19.780 --> 00:08:23.260]   Icebergs
[00:08:23.260 --> 00:08:24.820]   or ships and
[00:08:24.820 --> 00:08:31.020]   I have not seen that done before or like I you know it's it's quite challenging to know how to visualize this data
[00:08:31.020 --> 00:08:38.800]   And then he went on to show how to build a neural net to try to interpret this so that was pretty fantastic as well
[00:08:38.800 --> 00:08:44.140]   So yeah congratulations for all of you. I know for a lot of you. You know you're
[00:08:44.140 --> 00:08:50.940]   Posting stuff out there to the rest of the world for the first time you know and it's kind of intimidating
[00:08:51.500 --> 00:08:56.380]   you're used to writing stuff that you kind of hand into a teacher, and they're the only ones who see it and
[00:08:56.380 --> 00:09:00.100]   You know it's kind of scary the first time you do it
[00:09:00.100 --> 00:09:05.540]   But then the first time somebody you know up votes your Kaggle kernel or adds a clap to your medium post
[00:09:05.540 --> 00:09:11.460]   He suddenly realized oh, I'm actually I've written something that people like that's that's pretty great
[00:09:11.460 --> 00:09:17.200]   So if you haven't tried yourself yet, I again invite you to
[00:09:18.060 --> 00:09:22.540]   Try writing something and if you're not sure you could write a summary of a lesson
[00:09:22.540 --> 00:09:27.660]   You could write a summary of like if there's something you found hard like maybe you found it hard to
[00:09:27.660 --> 00:09:32.820]   Fire up a GPU based AWS instance you eventually figured it out you could write down
[00:09:32.820 --> 00:09:36.740]   Just describe how you solve that problem or if one of your classmates
[00:09:36.740 --> 00:09:39.700]   Didn't understand something and you explained it to them
[00:09:39.700 --> 00:09:45.220]   Then you could like write down something saying like oh, there's this concept that some people have trouble understanding here
[00:09:45.220 --> 00:09:49.860]   So good way. I think of explaining it. There's all kinds of stuff you could you could do
[00:09:49.860 --> 00:09:58.020]   Okay, so let's go back to SGD
[00:09:58.020 --> 00:10:01.500]   and
[00:10:01.500 --> 00:10:07.880]   We're going back through this notebook which
[00:10:07.880 --> 00:10:13.020]   Rachel put together basically taking us through
[00:10:13.660 --> 00:10:18.380]   Kind of SGD from scratch for the purpose of digit recognition
[00:10:18.380 --> 00:10:21.620]   and actually quite a lot of the stuff we look at today is
[00:10:21.620 --> 00:10:24.300]   going to be
[00:10:24.300 --> 00:10:26.100]   closely following
[00:10:26.100 --> 00:10:28.740]   Part of the computational linear algebra course
[00:10:28.740 --> 00:10:35.980]   Which you can both find the MOOCs on fast AI or at USF. It'll be an elective next year, right?
[00:10:35.980 --> 00:10:37.980]   So if you find some of this
[00:10:38.580 --> 00:10:45.960]   This stuff interesting and I hope you do then please consider signing up for the elective or checking out the video online
[00:10:45.960 --> 00:10:51.940]   So we're building
[00:10:51.940 --> 00:10:54.820]   neural networks
[00:10:54.820 --> 00:10:57.580]   And
[00:10:57.580 --> 00:11:01.500]   We're starting with an assumption that we've downloaded the MNIST data
[00:11:01.500 --> 00:11:07.740]   We've normalized it by subtracting the main and divided by the standard deviation. Okay, so the data is
[00:11:08.700 --> 00:11:12.760]   It's slightly unusual in that although they represent images
[00:11:12.760 --> 00:11:17.460]   They were they were downloaded as each image was a seven hundred and eighty four long
[00:11:17.460 --> 00:11:21.660]   Rank one tensor, so it's been flattened out
[00:11:21.660 --> 00:11:26.540]   Okay, and so for the purpose of drawing pictures of it we had to
[00:11:26.540 --> 00:11:28.700]   resize it
[00:11:28.700 --> 00:11:30.700]   to 28 by 28
[00:11:30.700 --> 00:11:37.200]   But the actual data we've got is not 28 by 28. It's as it's it's 784 long
[00:11:37.520 --> 00:11:39.520]   flattened out
[00:11:39.520 --> 00:11:43.320]   Okay
[00:11:43.320 --> 00:11:48.440]   The basic steps we're going to take here is to start out with training
[00:11:48.440 --> 00:11:54.000]   The world's simplest neural network basically a logistic regression, right?
[00:11:54.000 --> 00:11:58.340]   So no hidden layers and we're going to train it using a library
[00:11:58.340 --> 00:12:03.520]   Fast AI and we're going to build the network using a library type torch
[00:12:03.840 --> 00:12:07.480]   Right, and then we're going to gradually get rid of all the libraries, right?
[00:12:07.480 --> 00:12:13.760]   So first of all, we'll get rid of the nn neural net library and pytorch and write that ourselves
[00:12:13.760 --> 00:12:22.620]   Then we'll get rid of the fast AI fit function and write that ourselves and then we'll get rid of the pytorch
[00:12:22.620 --> 00:12:26.120]   optimizer and write that ourselves and so by the end of
[00:12:26.120 --> 00:12:30.520]   This notebook will have written all the pieces ourselves
[00:12:30.800 --> 00:12:36.200]   The only thing that we'll end up relying on is the two key things that pytorch gives us
[00:12:36.200 --> 00:12:40.320]   Which is a the ability to write Python code and have it run on the GPU and?
[00:12:40.320 --> 00:12:46.960]   B the ability to write Python code and have it automatically differentiated for us
[00:12:46.960 --> 00:12:52.160]   Okay, so they're the two things we're not going to attempt to write ourselves because it's boring and pointless
[00:12:52.160 --> 00:12:57.600]   But everything else we'll try and write ourselves on top of those two things. Okay, so
[00:12:58.720 --> 00:13:03.680]   Our starting point is like not doing anything ourselves
[00:13:03.680 --> 00:13:10.160]   It's basically having it all done for us. And so pytorch has an nn library, which is where the neural net stuff lives
[00:13:10.160 --> 00:13:12.280]   you can create a
[00:13:12.280 --> 00:13:18.640]   multi-layer neural network by using the sequential function and then passing in a list of the layers that you want and
[00:13:18.640 --> 00:13:20.840]   We asked for a linear layer
[00:13:20.840 --> 00:13:28.260]   Followed by a softmax layer and that defines our logistic regression. Okay the input to our linear layer
[00:13:28.380 --> 00:13:34.500]   Is 28 by 28 as we just discussed the output is 10 because we want a probability
[00:13:34.500 --> 00:13:39.180]   For each of the numbers not through nine for each of our images, okay
[00:13:39.180 --> 00:13:46.460]   Cuda sticks it on the GPU and then
[00:13:46.460 --> 00:13:50.180]   Fit
[00:13:50.180 --> 00:13:57.380]   Fits a model okay, so we start out with a random set of weights and then fit uses gradient descent to make it better
[00:13:57.500 --> 00:13:58.820]   We
[00:13:58.820 --> 00:14:00.820]   Had to tell the fit function
[00:14:00.820 --> 00:14:07.720]   What criterion to use in other words what counts is better and we told it to use negative log likelihood
[00:14:07.720 --> 00:14:10.860]   We'll learn about that in the next lesson what that is exactly
[00:14:10.860 --> 00:14:18.000]   We had to tell it what optimizer to use and we said please use optm dot Adam the details of that
[00:14:18.000 --> 00:14:23.180]   We won't cover in this course. We're going to use something build something simpler called SGD
[00:14:23.180 --> 00:14:26.560]   If you're interested in Adam, we just covered that in the deep learning course
[00:14:27.060 --> 00:14:32.740]   And what metrics do you want to print out? We decided to print out accuracy. Okay, so
[00:14:32.740 --> 00:14:35.940]   That was that and so if we do that
[00:14:35.940 --> 00:14:42.340]   Okay
[00:14:42.340 --> 00:14:47.300]   So after we fit it we get an accuracy of generally somewhere around 91 92 percent
[00:14:47.300 --> 00:14:50.980]   So what we're going to do from here is we're going to gradually
[00:14:50.980 --> 00:14:55.180]   We're going to repeat this exact same thing. So we're going to rebuild
[00:14:56.300 --> 00:14:57.820]   This model
[00:14:57.820 --> 00:15:06.320]   You know four or five times fitting it building it and fitting it with less and less libraries. Okay, so the second thing that we did
[00:15:06.320 --> 00:15:09.020]   last time
[00:15:09.020 --> 00:15:11.780]   Was to try to start to define the
[00:15:11.780 --> 00:15:15.580]   The module ourselves
[00:15:15.580 --> 00:15:21.020]   All right, so instead of saying the network is a sequential bunch of these layers
[00:15:21.780 --> 00:15:26.760]   Let's not use that library at all and try and define it ourselves from scratch
[00:15:26.760 --> 00:15:30.200]   So to do that we have to use
[00:15:30.200 --> 00:15:32.220]   OO
[00:15:32.220 --> 00:15:36.720]   Because that's how we build everything in pytorch and we have to create
[00:15:36.720 --> 00:15:39.060]   a class
[00:15:39.060 --> 00:15:45.140]   Which inherits from an end up module so an end up module is a pytorch class
[00:15:45.140 --> 00:15:50.740]   That takes our class and turns it into a neural network module
[00:15:51.500 --> 00:15:55.940]   Which basically means will anything that you inherit from an end up module like this?
[00:15:55.940 --> 00:16:02.020]   You can pretty much insert into a neural network as a layer or you can treat it as a neural network
[00:16:02.020 --> 00:16:05.060]   it's going to get all the stuff that it needs automatically to
[00:16:05.060 --> 00:16:11.260]   To work as a part of or a full neural network and we'll talk about exactly what that means
[00:16:11.260 --> 00:16:13.780]   Today and the next lesson, right?
[00:16:15.940 --> 00:16:22.900]   so we need to construct the object so that means we need to define the constructor under in it and
[00:16:22.900 --> 00:16:29.720]   Then importantly, this is a Python thing is if you inherit from some other object
[00:16:29.720 --> 00:16:33.100]   Then you have to create the thing you inherit from first
[00:16:33.100 --> 00:16:38.020]   so when you say super dot under in it that says construct the
[00:16:38.500 --> 00:16:46.180]   Nn dot module piece of that first right if you don't do that then the the NN dot module stuff
[00:16:46.180 --> 00:16:50.820]   Never gets a chance to actually get constructed. Now. So this is just like a standard
[00:16:50.820 --> 00:16:52.900]   Python
[00:16:52.900 --> 00:16:53.980]   OO
[00:16:53.980 --> 00:16:59.180]   Subclass constructor, okay, and if any of that's an unclear to you then you know
[00:16:59.180 --> 00:17:03.860]   This is where you definitely want to just grab a python intro to OO because this is
[00:17:04.420 --> 00:17:08.740]   That the standard approach, right? So inside our constructor
[00:17:08.740 --> 00:17:11.580]   We want to do the equivalent of
[00:17:11.580 --> 00:17:19.060]   Nn dot linear. All right. So what NN dot linear is doing is it's taking our
[00:17:19.060 --> 00:17:25.340]   It's taking our 28 by 28
[00:17:29.380 --> 00:17:36.180]   Vector so 768 long vector and we're going to be that's going to be the input to a matrix multiplication
[00:17:36.180 --> 00:17:38.820]   so we now need to create a
[00:17:38.820 --> 00:17:42.900]   Something with
[00:17:42.900 --> 00:17:45.840]   768 rows and
[00:17:45.840 --> 00:17:49.620]   That's 768 and 10 columns
[00:17:49.620 --> 00:17:58.260]   Okay, so because the input to this is going to be a mini batch of size
[00:17:58.260 --> 00:18:01.740]   Actually, let's move this into a new window
[00:18:01.740 --> 00:18:15.140]   768 by 10 and the input to this is going to be a mini batch of size 64
[00:18:15.140 --> 00:18:18.420]   by
[00:18:18.420 --> 00:18:20.100]   768
[00:18:20.100 --> 00:18:22.780]   Right, so we're going to do this matrix product
[00:18:23.340 --> 00:18:28.220]   Okay, so when we say in pytorch NN dot linear
[00:18:28.220 --> 00:18:32.100]   It's going to construct
[00:18:32.100 --> 00:18:38.900]   This matrix for us, right? So since we're not using that we're doing things from scratch. We need to make it ourselves
[00:18:38.900 --> 00:18:41.300]   So to make it ourselves we can say
[00:18:41.300 --> 00:18:44.140]   generate normal random numbers
[00:18:44.140 --> 00:18:46.140]   with
[00:18:46.140 --> 00:18:51.980]   This dimensionality which we passed in here 768 by 10. Okay, so that gives us our
[00:18:53.060 --> 00:18:55.060]   randomly initialized
[00:18:55.060 --> 00:18:57.300]   matrix, okay
[00:18:57.300 --> 00:19:01.660]   Then we want to add on to this
[00:19:01.660 --> 00:19:08.140]   You know, we don't just want y equals ax we want y equals ax plus b
[00:19:08.140 --> 00:19:13.500]   Right, so we need to add on what we call in neural nets a bias vector
[00:19:13.500 --> 00:19:20.220]   So we create here a bias vector of length 10. Okay again randomly initialized
[00:19:20.740 --> 00:19:24.620]   And so now here are our two randomly initialized
[00:19:24.620 --> 00:19:27.420]   weight tenses
[00:19:27.420 --> 00:19:29.460]   So that's our constructor
[00:19:29.460 --> 00:19:30.980]   Okay
[00:19:30.980 --> 00:19:36.900]   Now we need to define forward. Why do we need to define forward? This is a pytorch specific thing
[00:19:36.900 --> 00:19:42.620]   What's going to happen is this is when you create a module in
[00:19:42.620 --> 00:19:47.760]   Pytorch the object that you get back behaves as if it's a function
[00:19:47.760 --> 00:19:52.860]   You can call it with parentheses which we'll do it that in a moment. And so you need to somehow define
[00:19:52.860 --> 00:20:00.440]   What happens when you call it as if it's a function and the answer is pytorch calls a method called?
[00:20:00.440 --> 00:20:07.740]   Forward, okay, that's just that's the Python the pytorch kind of approach that they picked, right?
[00:20:07.740 --> 00:20:11.300]   So when it calls forward, we need to do our actual
[00:20:12.260 --> 00:20:19.600]   Calculation of the output of this module or later. Okay. So here is the thing that actually gets calculated in our logistic regression
[00:20:19.600 --> 00:20:22.420]   So basically we take our
[00:20:22.420 --> 00:20:26.020]   Input X
[00:20:26.020 --> 00:20:32.340]   Which gets passed to forward that's basically how forward works it gets passed the mini batch
[00:20:32.340 --> 00:20:35.620]   and we matrix multiply it by
[00:20:35.620 --> 00:20:41.740]   The layer one weights which we defined up here and then we add on
[00:20:42.740 --> 00:20:50.100]   The layer one bias which we defined up here. Okay, and actually nowadays we can define this a little bit more elegantly
[00:20:50.100 --> 00:20:54.700]   Using the Python 3
[00:20:54.700 --> 00:20:57.660]   Matrix multiplication operator, which is the at sign
[00:20:57.660 --> 00:21:01.080]   And when you when you use that I think you kind of end up with
[00:21:01.080 --> 00:21:07.860]   Something that looks closer to what the mathematical notation looked like and so I find that nicer. Okay
[00:21:07.860 --> 00:21:11.580]   All right, so that's
[00:21:11.580 --> 00:21:13.580]   That's our linear layer
[00:21:13.580 --> 00:21:19.740]   In our logistic regression in our zero hidden layer neural net. So then the next thing we do to that is
[00:21:19.740 --> 00:21:23.260]   softmax
[00:21:23.260 --> 00:21:26.840]   Okay, so we get the output of this
[00:21:26.840 --> 00:21:31.420]   Matrix multiply
[00:21:31.420 --> 00:21:37.380]   Okay, who wants to tell me what the dimensionality of my output of this matrix multiply is
[00:21:40.300 --> 00:21:42.060]   Sorry
[00:21:42.060 --> 00:21:44.060]   64 by 10. Thank you Karen
[00:21:44.060 --> 00:21:50.580]   And I should mention for those of you that weren't at deep learning class yesterday
[00:21:50.580 --> 00:21:54.980]   We actually looked at a really cool post from Karen who described how to
[00:21:54.980 --> 00:22:00.380]   Do structured data analysis with neural nets which has been like super popular?
[00:22:00.380 --> 00:22:05.620]   And a whole bunch of people have kind of said that they've read it and found it super interesting. So
[00:22:05.620 --> 00:22:09.020]   That was really exciting
[00:22:10.020 --> 00:22:12.020]   So we get this matrix of
[00:22:12.020 --> 00:22:15.780]   Outputs and we put this through a softmax
[00:22:15.780 --> 00:22:19.740]   And why do we put it through a softmax
[00:22:19.740 --> 00:22:24.660]   We put it through a softmax because in the end we want probably you know for every image
[00:22:24.660 --> 00:22:28.780]   We want a probability that this is 0 or a 1 or a 2 or a 3 or 4, right?
[00:22:28.780 --> 00:22:35.420]   So we want a bunch of probabilities that add up to 1 and where each of those probabilities is between 0 and 1
[00:22:35.420 --> 00:22:37.860]   so a softmax
[00:22:38.860 --> 00:22:40.860]   Does exactly that for us?
[00:22:40.860 --> 00:22:45.900]   So for example if we weren't picking out, you know numbers from 0 to 10
[00:22:45.900 --> 00:22:50.500]   But instead of picking out cat dog play and fish or building the output of that matrix multiply
[00:22:50.500 --> 00:22:54.620]   For one particular image might look like that. These are just some random numbers
[00:22:54.620 --> 00:23:02.420]   And to turn that into a softmax. I first go e to the power of each of those numbers. I
[00:23:02.420 --> 00:23:07.140]   Sum up those e to the power of
[00:23:07.860 --> 00:23:09.060]   and
[00:23:09.060 --> 00:23:14.180]   Then I take each of those e to the power ofs and divide it by the sum and that's softmax
[00:23:14.180 --> 00:23:19.260]   That's the definition of softmax. So because it was a to the power of it means it's always positive
[00:23:19.260 --> 00:23:27.180]   Because it was divided by the sum it means that it's always between 0 and 1 and it also means because it's divided
[00:23:27.180 --> 00:23:29.820]   By the sum that they always add up to 1
[00:23:29.820 --> 00:23:32.900]   So by applying this softmax
[00:23:34.500 --> 00:23:40.140]   Activation function so anytime we have a layer of outputs, which we call activations
[00:23:40.140 --> 00:23:45.980]   And then we apply some function some nonlinear function to that that maps one
[00:23:45.980 --> 00:23:52.500]   One scalar to one scalar like softmax does we call that an activation function, okay?
[00:23:52.500 --> 00:24:00.060]   So the softmax activation function takes our outputs and turns it into something which behaves like a probability, right?
[00:24:00.260 --> 00:24:07.980]   We don't strictly speaking need it. We could still try and train something which where the output directly is the probabilities
[00:24:07.980 --> 00:24:11.320]   All right, but by creating using this function
[00:24:11.320 --> 00:24:16.420]   That automatically makes them always behave like probabilities. It means there's less
[00:24:16.420 --> 00:24:21.960]   For the network to learn so it's going to learn better. All right, so generally speaking whenever we design
[00:24:21.960 --> 00:24:24.660]   an architecture
[00:24:24.660 --> 00:24:32.500]   We try to design it in a way where it's as easy as possible for it to create something of the form that we want
[00:24:32.500 --> 00:24:35.420]   So that's why we use
[00:24:35.420 --> 00:24:37.580]   softmax
[00:24:37.580 --> 00:24:44.180]   Right so that's the basic steps right we have our input which is a bunch of images
[00:24:44.180 --> 00:24:51.780]   Right which is here gets multiplied by a weight matrix. We actually also add on a bias
[00:24:52.740 --> 00:24:56.460]   Right to get a output of the linear function
[00:24:56.460 --> 00:25:04.100]   We put it through a nonlinear activation function in this case softmax and that gives us our probabilities
[00:25:04.100 --> 00:25:09.020]   So there there that all is
[00:25:09.020 --> 00:25:14.820]   Pi torch also tends to use the log
[00:25:14.820 --> 00:25:19.940]   Of softmax for reasons that don't particularly bother us now
[00:25:19.940 --> 00:25:26.020]   It's basically a numerical stability convenience. Okay, so to make this the same as our
[00:25:26.020 --> 00:25:34.420]   Version up here that you saw log softmax. I'm going to use log here as well. Okay, so
[00:25:34.420 --> 00:25:41.060]   We can now instantiate this class that is create an object of this class
[00:25:41.060 --> 00:25:48.180]   So I have a question back for the probabilities where we were before
[00:25:48.860 --> 00:25:50.860]   so
[00:25:50.860 --> 00:25:54.820]   If we were to have a photo with a cat and a dog together
[00:25:54.820 --> 00:26:00.580]   Would that change the way that that works or does it work in the same basic? Yeah, so that's a great question
[00:26:00.580 --> 00:26:03.660]   so if you had a photo with a cat and a dog together and
[00:26:03.660 --> 00:26:07.100]   You wanted it to spit out both cat and dog
[00:26:07.100 --> 00:26:13.780]   This would be a very poor choice. So softmax is specifically the activation function we use for
[00:26:14.540 --> 00:26:19.460]   Categorical predictions where we only ever want to predict one of those things, right?
[00:26:19.460 --> 00:26:27.120]   And so part of the reason why is that as you can see because we're using either the right either the slightly bigger numbers
[00:26:27.120 --> 00:26:34.340]   Creates much bigger numbers as a result of which we generally have just one or two things large and everything else is pretty small
[00:26:34.340 --> 00:26:34.860]   All right
[00:26:34.860 --> 00:26:35.820]   so if I like
[00:26:35.820 --> 00:26:41.820]   Recalculate these random numbers a few times you'll see like it tends to be a bunch of zeros and one or two high numbers
[00:26:41.980 --> 00:26:44.420]   right, so it's really designed to
[00:26:44.420 --> 00:26:53.700]   Try to kind of make it easy to predict like this one thing. There's the thing I want if you're doing multi
[00:26:53.700 --> 00:26:59.380]   Label prediction so I want to find all the things in this image rather than using softmax
[00:26:59.380 --> 00:27:01.260]   We would instead use sigmoid, right?
[00:27:01.260 --> 00:27:07.660]   So sigmoid recall each would cause each of these between to be between zero and one, but they would no longer add to one
[00:27:08.620 --> 00:27:11.480]   Good question and like a lot of these
[00:27:11.480 --> 00:27:18.140]   Details about like best practices are things that we cover in the deep learning course
[00:27:18.140 --> 00:27:24.100]   And we won't cover heaps of them here in the machine learning course. We're more interested in the mechanics, I guess
[00:27:24.100 --> 00:27:28.300]   But we'll try and do them if they're quick
[00:27:28.300 --> 00:27:35.420]   All right, so now that we've got that we can instantiate an object of that class and of course
[00:27:35.420 --> 00:27:38.940]   We want to copy it over to the GPU so we can do computations over there
[00:27:38.940 --> 00:27:44.580]   Again, we need an optimizer where we're talking about what this is shortly, but you'll see here
[00:27:44.580 --> 00:27:47.760]   We've called a function on our class called parameters
[00:27:47.760 --> 00:27:51.340]   But we never defined a method called parameters
[00:27:51.340 --> 00:27:56.420]   And the reason that is going to work is because it actually was defined for us inside nn.module
[00:27:56.420 --> 00:28:03.240]   and so nn.module actually automatically goes through the attributes we've created and finds
[00:28:04.060 --> 00:28:07.860]   Anything that basically we we said this is a parameter
[00:28:07.860 --> 00:28:11.260]   So the way you say something is a parameter is you wrap it in an end up parameter
[00:28:11.260 --> 00:28:13.620]   So this is just the way that you tell PyTorch
[00:28:13.620 --> 00:28:16.180]   This is something that I want to optimize
[00:28:16.180 --> 00:28:21.420]   Okay, so when we created the weight matrix we just wrapped it with an end up parameter
[00:28:21.420 --> 00:28:23.780]   It's exactly the same as a regular
[00:28:23.780 --> 00:28:26.620]   PyTorch variable which we'll learn about shortly
[00:28:26.620 --> 00:28:33.940]   It's just a little flag to say hey you should you should optimize this and so when you call net to dot parameter
[00:28:33.940 --> 00:28:38.900]   On our net to object we created it goes through everything that we created in the constructor
[00:28:38.900 --> 00:28:41.880]   Checks to see if any of them are of type parameter
[00:28:41.880 --> 00:28:46.620]   And if so it sets all of those as being things that we want to train with the optimizer
[00:28:46.620 --> 00:28:50.020]   And we'll be implementing the optimizer from scratch later
[00:28:50.020 --> 00:28:53.040]   Okay, so having done that
[00:28:53.040 --> 00:29:02.620]   We can fit and we should get basically the same answer as before 91 ish
[00:29:03.620 --> 00:29:05.620]   So that looks good
[00:29:05.620 --> 00:29:07.660]   All right
[00:29:07.660 --> 00:29:09.500]   So
[00:29:09.500 --> 00:29:11.500]   What if we actually built here?
[00:29:11.500 --> 00:29:17.340]   Well what we've actually built as I said is something that can behave like a regular function
[00:29:17.340 --> 00:29:21.660]   All right, so I want to show you how we can actually call this as a function
[00:29:21.660 --> 00:29:23.660]   So to be able to call it as a function
[00:29:23.660 --> 00:29:28.140]   We need to be able to pass data to it to be able to pass data to it
[00:29:28.140 --> 00:29:31.700]   I'm going to need to grab a mini batch of MNIST images
[00:29:32.700 --> 00:29:34.700]   Okay, so we used
[00:29:34.700 --> 00:29:37.220]   for convenience the
[00:29:37.220 --> 00:29:40.340]   Image classifier data from a raised method from fastai
[00:29:40.340 --> 00:29:47.060]   And what that does is it creates a pytorch data loader for us a pytorch data loader is
[00:29:47.060 --> 00:29:52.340]   Something that grabs a few images and sticks them into a mini batch and makes them available
[00:29:52.340 --> 00:29:58.380]   And you can basically say give me another mini batch give me another mini batch give me another mini batch and so
[00:30:00.660 --> 00:30:02.420]   in
[00:30:02.420 --> 00:30:05.060]   Python we call these things generators
[00:30:05.060 --> 00:30:10.020]   Generators are things where you can basically say I want another I want another I want another right
[00:30:10.020 --> 00:30:15.900]   There's this kind of very close connection between
[00:30:15.900 --> 00:30:23.140]   Iterators and generators are not going to worry about the difference between them right now, but you'll see basically to turn
[00:30:23.140 --> 00:30:29.540]   To actually get hold of something which we can say please give me another of
[00:30:29.980 --> 00:30:32.020]   in
[00:30:32.020 --> 00:30:36.540]   Order to grab something that we can we can use to generate mini batches
[00:30:36.540 --> 00:30:43.180]   We have to take our data loader and so you can ask for the training data loader from our model data object
[00:30:43.180 --> 00:30:49.420]   You'll see there's a bunch of different data loaders. You can ask for you can ask for the test data loader the train data loader
[00:30:49.420 --> 00:30:51.940]   validation loader
[00:30:51.940 --> 00:30:56.020]   Augmented images data loader and so forth so we're going to grab the training data loader
[00:30:57.220 --> 00:31:03.300]   That was created for us. This is a PI standard PI torch data loader. Well slightly optimized by us, but same idea
[00:31:03.300 --> 00:31:07.020]   And you can then say this is a standard Python
[00:31:07.020 --> 00:31:14.860]   Thing we can say turn that into an iterator turn that into something where we can grab another one at a time from and so
[00:31:14.860 --> 00:31:16.540]   Once you've done that
[00:31:16.540 --> 00:31:20.860]   We've now got something that we can iterate through you can use the standard Python
[00:31:21.580 --> 00:31:26.820]   Next function to grab one more thing from that generator, okay?
[00:31:26.820 --> 00:31:33.100]   So that's returning and the X's from a mini batch in the wise
[00:31:33.100 --> 00:31:36.440]   Found our mini batch the other way that you can use
[00:31:36.440 --> 00:31:45.180]   Generators and iterators in Python is with a for loop. I could also have said like for you know X mini batch comma Y mini batch in
[00:31:45.180 --> 00:31:47.420]   data loader
[00:31:47.420 --> 00:31:51.940]   And then like do something right so when you do that. It's actually behind the scenes
[00:31:51.940 --> 00:31:57.920]   It's basically syntactic sugar for calling next lots of times. Okay, so this is all standard
[00:31:57.920 --> 00:32:00.700]   Python stuff
[00:32:00.700 --> 00:32:03.100]   So that returns a
[00:32:03.100 --> 00:32:14.260]   Tensor of size 64 by 784 as we would expect right the
[00:32:14.980 --> 00:32:20.340]   Fastai library we used defaults to a mini batch size of 64. That's why it's that long
[00:32:20.340 --> 00:32:27.180]   These are all of the background zero pixels, but they're not actually zero in this case. Why aren't they zero?
[00:32:27.180 --> 00:32:33.420]   Yeah, they're normalized exactly right so we subtract at the mean divided by standard deviation right
[00:32:33.420 --> 00:32:40.820]   So there there it is so now what we want to do is we want to
[00:32:42.380 --> 00:32:48.860]   Pass that into our our logistic regression. So what we might do is we'll go
[00:32:48.860 --> 00:32:55.580]   Variable XMB equals variable. Okay, I can take my X mini batch I
[00:32:55.580 --> 00:32:59.160]   can move it on to the GPU because remember my
[00:32:59.160 --> 00:33:04.980]   Net to object is on the GPU so our data for it also has to be on the GPU
[00:33:04.980 --> 00:33:10.380]   And then the second thing I do is I have to wrap it in variable. So what does variable do?
[00:33:11.140 --> 00:33:15.000]   This is how we get for free automatic differentiation
[00:33:15.000 --> 00:33:19.040]   Pytorch can automatically differentiate
[00:33:19.040 --> 00:33:22.480]   You know pretty much anything right any tensor?
[00:33:22.480 --> 00:33:25.380]   But to do so takes memory and time
[00:33:25.380 --> 00:33:30.820]   So it's not going to always keep track like to do to do what about differentiation
[00:33:30.820 --> 00:33:35.340]   It has to keep track of exactly how something was calculated. We added these things together
[00:33:35.340 --> 00:33:39.420]   We multiplied it by that we then took the sign blah blah blah, right?
[00:33:39.420 --> 00:33:45.060]   you have to know all of the steps because then to do the automatic differentiation it has to
[00:33:45.060 --> 00:33:49.380]   Take the derivative of each step using the chain rule multiply them all together
[00:33:49.380 --> 00:33:52.140]   All right, so that's slow and memory intensive
[00:33:52.140 --> 00:33:57.560]   So we have to opt in to saying like okay this particular thing we're going to be taking the derivative of later
[00:33:57.560 --> 00:34:00.300]   So please keep track of all of those operations for us
[00:34:00.300 --> 00:34:05.940]   And so the way we opt in is by wrapping a tensor in a variable, right? So
[00:34:08.100 --> 00:34:10.100]   That's how we do it and
[00:34:10.100 --> 00:34:16.460]   You'll see that it looks almost exactly like a tensor, but it now says variable containing
[00:34:16.460 --> 00:34:21.860]   This tensor right so in Pytorch a variable has exactly
[00:34:21.860 --> 00:34:27.860]   Identical API to a tensor or actually more specifically a superset of the API of a tensor
[00:34:27.860 --> 00:34:30.740]   Anything we can do to a tensor we can do to a variable
[00:34:30.740 --> 00:34:37.460]   But it's going to keep track of exactly what we did so we can later on take the derivative
[00:34:37.700 --> 00:34:40.260]   Okay, so we can now pass that
[00:34:40.260 --> 00:34:51.980]   Into our net to object remember I said you can treat this as if it's a function
[00:34:51.980 --> 00:34:56.140]   Right so notice we're not calling dot forward
[00:34:56.140 --> 00:34:59.380]   We're just treating it as a function and
[00:34:59.380 --> 00:35:07.460]   Then remember we took the log so to undo that I'm taking the x and that will give me my probabilities
[00:35:07.460 --> 00:35:14.020]   Okay, so there's my probabilities, and it's got
[00:35:14.020 --> 00:35:23.020]   Return something of size 64 by 10 so for each image in the mini batch
[00:35:23.020 --> 00:35:29.580]   We've got 10 probabilities, and you'll see most probabilities are pretty close to 0
[00:35:29.580 --> 00:35:32.980]   Right and a few of them are quite a bit bigger
[00:35:33.420 --> 00:35:39.300]   Which is exactly what we do we hope right is that it's like okay? It's not a zero. It's not a one
[00:35:39.300 --> 00:35:42.740]   It's not a two. It is a three. It's not a four. It's not a five and so forth
[00:35:42.740 --> 00:35:47.140]   So maybe this would be a bit easier to read if we just grab like the first three of them
[00:35:47.140 --> 00:35:55.100]   Okay, so it's like ten to the next three ten to the next eight two five five four okay?
[00:35:55.100 --> 00:35:57.620]   And then suddenly here's one which is ten to make one right?
[00:35:57.620 --> 00:36:02.300]   So you can kind of see what it's trying to what it's trying to do here
[00:36:02.980 --> 00:36:10.380]   I mean we could call like net to dot forward and it'll do exactly the same thing
[00:36:10.380 --> 00:36:13.060]   Right, but that's not how
[00:36:13.060 --> 00:36:16.620]   All of the pie torch mechanics actually work
[00:36:16.620 --> 00:36:22.580]   It's actually they actually call it as if it's a function right and so this is actually a really important idea
[00:36:22.580 --> 00:36:24.940]   like because it means that
[00:36:24.940 --> 00:36:30.460]   When we define our own architectures or whatever anywhere that you would put in a function
[00:36:30.580 --> 00:36:34.900]   You could put in a layer anyway you put in a layer you can put in a neural net anyway
[00:36:34.900 --> 00:36:39.020]   You put in a neural net you can put in a function because as far as pie torch is concerned
[00:36:39.020 --> 00:36:43.060]   They're all just things that it's going to call just like as if they're functions
[00:36:43.060 --> 00:36:48.020]   So they're all like interchangeable, and this is really important because that's how we create
[00:36:48.020 --> 00:36:53.660]   Really good neural nets is by mixing and matching lots of pieces and putting them all together
[00:36:53.660 --> 00:36:56.420]   Let me give an example
[00:36:56.420 --> 00:37:00.220]   Here is my
[00:37:00.220 --> 00:37:04.540]   Logistic aggression which got
[00:37:04.540 --> 00:37:08.980]   91 and a bit percent accuracy
[00:37:08.980 --> 00:37:11.380]   I'm now going to turn it
[00:37:11.380 --> 00:37:17.100]   Into a neural network with one hidden layer all right, and the way I'm going to do that is I'm going to create
[00:37:17.100 --> 00:37:19.860]   one more layer
[00:37:19.860 --> 00:37:24.420]   I'm going to change this so it spits out a hundred rather than ten
[00:37:24.420 --> 00:37:29.380]   Which means this one input is going to be a hundred rather than ten
[00:37:30.020 --> 00:37:35.340]   Now this as it is can't possibly make things any better at all yet
[00:37:35.340 --> 00:37:39.020]   Why is this definitely not going to be better than what I had before?
[00:37:39.020 --> 00:37:42.540]   Yeah, can somebody pass the yeah?
[00:37:42.540 --> 00:37:47.620]   But you've got a combination of two linear layers, which is just the same as one
[00:37:47.620 --> 00:37:54.820]   Exactly right so we've got two linear layers, which is just a linear layer right so to make things interesting
[00:37:55.700 --> 00:38:00.880]   I'm going to replace all of the negatives from the first layer with zeros
[00:38:00.880 --> 00:38:07.820]   Because that's a nonlinear transformation, and so that nonlinear transformation is called a rectified linear unit
[00:38:07.820 --> 00:38:18.340]   Okay, so nn dot sequential simply is going to call each of these layers in turn for each mini batch right so do a linear layer
[00:38:18.340 --> 00:38:25.220]   Replace all of the negatives with zero do another linear layer and do a softmax. This is now a neural network
[00:38:26.020 --> 00:38:28.020]   with one hidden layer and
[00:38:28.020 --> 00:38:30.460]   So let's try trading that instead
[00:38:30.460 --> 00:38:37.180]   Okay accuracy is now going up to 96%
[00:38:37.180 --> 00:38:43.420]   Okay, so the this is the idea is that the basic techniques. We're learning in this lesson
[00:38:43.420 --> 00:38:49.540]   Like become powerful at the point where you start stacking them together, okay?
[00:38:49.540 --> 00:38:53.660]   Can somebody pass the green box there and then there yes, Daniel?
[00:38:54.660 --> 00:38:59.940]   Why did you pick a hundred? No reason it was like easier to type an extra zero?
[00:38:59.940 --> 00:39:04.220]   Like this question of like how many
[00:39:04.220 --> 00:39:09.780]   Activations should I have it a neural network layer is kind of part of the the scale of a deep learning practitioner
[00:39:09.780 --> 00:39:13.000]   We cover it in the deep learning course not in this course
[00:39:13.000 --> 00:39:18.100]   When adding that additional I guess
[00:39:18.100 --> 00:39:20.660]   transformation
[00:39:20.660 --> 00:39:26.180]   Additional layer additional layer this one here is called a nonlinear layer or an activation function
[00:39:26.180 --> 00:39:30.060]   Activation function or activation function
[00:39:30.060 --> 00:39:37.780]   Does it matter that like if you would have done for example like two softmaxes?
[00:39:37.780 --> 00:39:40.180]   Or is that something you cannot do like yeah?
[00:39:40.180 --> 00:39:42.140]   You can absolutely use a softmax there
[00:39:42.140 --> 00:39:47.860]   But it's probably not going to give you what you want and the reason why is that a softmax?
[00:39:48.220 --> 00:39:55.460]   Tends to push most of its activations to zero and an activation just be clear like I've had a lot of questions in deep
[00:39:55.460 --> 00:40:02.740]   Learning course about like what's an activation an activation is the value that is calculated in a layer, right?
[00:40:02.740 --> 00:40:04.740]   So this is an activation
[00:40:04.740 --> 00:40:08.700]   Right it's not a weight a weight is not an activation
[00:40:08.700 --> 00:40:11.340]   It's the value that you calculate from a layer
[00:40:11.340 --> 00:40:15.700]   So softmax will tend to make most of its activations pretty close to zero
[00:40:15.700 --> 00:40:20.860]   and that's the opposite of what you want you genuinely want your activations to be kind of as
[00:40:20.860 --> 00:40:27.300]   Rich and diverse and and used as possible so nothing to stop you doing it, but it probably won't work very well
[00:40:27.300 --> 00:40:30.980]   Basically
[00:40:30.980 --> 00:40:34.300]   pretty much all of your layers will be followed by
[00:40:34.300 --> 00:40:39.780]   Non by nonlinear activation functions that will nearly always be value
[00:40:39.780 --> 00:40:42.500]   except for the last layer
[00:40:44.700 --> 00:40:51.740]   Could you when doing multiple layers, so let's say like could you live three could you think it's going two or three layers deep?
[00:40:51.740 --> 00:40:59.100]   Do you want to switch up these activation layers? No, that's a great question. So if I wanted to go deeper I
[00:40:59.100 --> 00:41:01.940]   would just do
[00:41:01.940 --> 00:41:05.860]   That okay, that's a now to hidden layer network
[00:41:05.860 --> 00:41:12.940]   So I think I'd heard you said that there are a couple of different
[00:41:13.780 --> 00:41:18.940]   Activation functions like that rectified linear unit. What are some examples and
[00:41:18.940 --> 00:41:22.020]   Why would you use?
[00:41:22.020 --> 00:41:24.180]   Each yeah great question
[00:41:24.180 --> 00:41:31.080]   So basically like as you add like more
[00:41:31.080 --> 00:41:33.980]   linear layers you kind of got your
[00:41:33.980 --> 00:41:41.180]   Input comes in and you put it through a linear layer and then a nonlinear layer linear layer nonlinear layer
[00:41:41.180 --> 00:41:50.900]   linear linear layer and then the final nonlinear layer
[00:41:50.900 --> 00:41:56.200]   The final nonlinear layer as we've discussed, you know, if it's a
[00:41:56.200 --> 00:41:58.860]   multi-category
[00:41:58.860 --> 00:42:03.580]   Classification, but you only ever pick one of them you would use softmax
[00:42:03.580 --> 00:42:07.600]   If it's a binary classification or a multi
[00:42:08.060 --> 00:42:12.100]   Label classification where you're predicting multiple things you would use sigmoid
[00:42:12.100 --> 00:42:15.500]   If it's a regression
[00:42:15.500 --> 00:42:18.660]   You would often have nothing at all
[00:42:18.660 --> 00:42:23.300]   Right, although we learned in last night's deal course where sometimes you can use sigmoid there as well
[00:42:23.300 --> 00:42:28.500]   So they're basically the options main options for the final layer
[00:42:28.500 --> 00:42:31.940]   for the
[00:42:31.940 --> 00:42:35.380]   Hidden layers you pretty much always use
[00:42:35.380 --> 00:42:41.580]   ReLU
[00:42:41.580 --> 00:42:50.380]   Okay, but there is a another
[00:42:50.380 --> 00:42:56.660]   Another one you can pick which is kind of interesting which is called
[00:42:56.660 --> 00:43:04.260]   Leaky ReLU and it looks like this
[00:43:05.100 --> 00:43:07.100]   and
[00:43:07.100 --> 00:43:13.540]   Basically if it's above zero, it's y equals x and if it's below zero, it's like y equals 0.1 x
[00:43:13.540 --> 00:43:16.660]   that's very similar to ReLU, but it's
[00:43:16.660 --> 00:43:22.100]   Rather than being equal to 0 under x. It's it's like something close to that
[00:43:22.100 --> 00:43:25.260]   So they're the main two
[00:43:25.260 --> 00:43:27.900]   ReLU and Leaky ReLU
[00:43:33.260 --> 00:43:38.060]   There are various others, but they're kind of like things that just look very close to that
[00:43:38.060 --> 00:43:41.440]   So for example, there's something called ELU, which is quite popular
[00:43:41.440 --> 00:43:47.700]   But like you know the details don't matter too much honestly like that there like ELU is something that looks like this
[00:43:47.700 --> 00:43:49.700]   But it's slightly more curvy in the middle
[00:43:49.700 --> 00:43:58.180]   And it's kind of like it's not generally something that you so much pick based on the data set it's more like
[00:43:59.380 --> 00:44:04.300]   Over time we just find better activation functions so two or three years ago
[00:44:04.300 --> 00:44:09.380]   Everybody used ReLU, you know a year ago pretty much everybody used Leaky ReLU today
[00:44:09.380 --> 00:44:11.940]   I guess probably most people starting to move towards ELU
[00:44:11.940 --> 00:44:15.460]   But honestly the choice of activation function doesn't matter
[00:44:15.460 --> 00:44:18.460]   terribly much actually
[00:44:18.460 --> 00:44:26.180]   And you know people have actually showed that you can use like our pretty arbitrary nonlinear activation functions like even a sine wave
[00:44:26.180 --> 00:44:28.820]   It still works
[00:44:28.820 --> 00:44:30.820]   Okay
[00:44:30.820 --> 00:44:40.620]   So although what we're going to do today is showing how to create
[00:44:40.620 --> 00:44:46.220]   This network with no hidden layers
[00:44:46.220 --> 00:44:49.860]   To turn it into
[00:44:49.860 --> 00:44:51.620]   that network
[00:44:51.620 --> 00:44:57.420]   Which is 96% ish accurate is it will be trivial right and in fact is something you should
[00:44:57.900 --> 00:45:02.060]   Probably try and do during the week right is to create that version
[00:45:02.060 --> 00:45:10.580]   Okay
[00:45:10.580 --> 00:45:18.740]   So now that we've got something where we can take our network pass in our variable and get back some
[00:45:18.740 --> 00:45:21.580]   predictions
[00:45:22.580 --> 00:45:30.780]   That's basically all that happened when we called fit. So we're going to see how how that that approach can be used to create this stochastic gradient
[00:45:30.780 --> 00:45:32.300]   descent
[00:45:32.300 --> 00:45:35.860]   one thing to note is that the to turn the
[00:45:35.860 --> 00:45:43.540]   Predicted probabilities into a predicted like which digit is it? We would need to use argmax
[00:45:43.540 --> 00:45:48.280]   Unfortunately pytorch doesn't call it argmax
[00:45:49.220 --> 00:45:53.540]   Instead pytorch just calls it max and max returns
[00:45:53.540 --> 00:45:56.260]   two things
[00:45:56.260 --> 00:46:05.020]   Returns the actual max across this axis so this is across the columns right and the second thing it returns is the index
[00:46:05.020 --> 00:46:12.180]   Of that maximum right so so the equivalent of argmax is to call max and then get the first
[00:46:12.900 --> 00:46:19.440]   Indexed thing okay, so there's our predictions right if this was in numpy. We would instead use NP argmax
[00:46:19.440 --> 00:46:22.060]   Okay
[00:46:22.060 --> 00:46:25.500]   All right
[00:46:25.500 --> 00:46:31.580]   So here are the predictions from our hand created logistic regression and in this case
[00:46:31.580 --> 00:46:34.860]   Looks like we got all but one correct
[00:46:37.300 --> 00:46:43.300]   So the next thing we're going to try and get rid of in terms of using libraries is for try to avoid using the
[00:46:43.300 --> 00:46:47.260]   Matrix multiplication operator and instead we're going to try and write that by hand
[00:46:47.260 --> 00:47:03.860]   So this next part we're going to learn about something which kind of seems
[00:47:03.860 --> 00:47:14.620]   It kind of it's going to seem like a minor little kind of programming idea, but actually it's going to turn out
[00:47:14.620 --> 00:47:18.500]   That at least in my opinion. It's the most important
[00:47:18.500 --> 00:47:24.040]   Programming concept that we'll teach in this course, and it's possibly the most important programming
[00:47:24.040 --> 00:47:26.620]   kind of concept in all of
[00:47:26.620 --> 00:47:32.020]   All the things you need to build machine learning algorithms, and it's the idea of
[00:47:32.980 --> 00:47:34.340]   broadcasting
[00:47:34.340 --> 00:47:37.300]   And the idea I will show by example
[00:47:37.300 --> 00:47:45.100]   If we create an array of 10 6 neg 4 and an array of 2 8 7 and then add the two together
[00:47:45.100 --> 00:47:54.060]   It adds each of the components of those two arrays in turn we call that element wise
[00:47:54.060 --> 00:47:58.740]   So in other words we didn't have to write a loop right back in the old days
[00:47:58.740 --> 00:48:02.540]   We would have to have looped through each one and added them and then concatenated them together
[00:48:02.780 --> 00:48:07.980]   We don't have to do that today. It happens for us automatically so in numpy
[00:48:07.980 --> 00:48:11.620]   We automatically get element wise operations
[00:48:11.620 --> 00:48:20.420]   We can do the same thing with Pytorch
[00:48:20.420 --> 00:48:29.740]   So in fastai we just add a little capital T to turn something into a Pytorch tensor right and if we add those together
[00:48:31.380 --> 00:48:37.700]   Exactly the same thing right so element wise operations are pretty standard in these kinds of libraries
[00:48:37.700 --> 00:48:44.100]   It's interesting not just because we don't have to write the for loop
[00:48:44.100 --> 00:48:49.380]   Right, but it's actually much more interesting because of the performance things that are happening here
[00:48:49.380 --> 00:48:52.020]   The first is if we were doing a for loop
[00:48:52.020 --> 00:48:54.740]   right
[00:48:54.740 --> 00:49:01.020]   If we were doing a for loop
[00:49:01.180 --> 00:49:03.180]   That would happen in Python
[00:49:03.180 --> 00:49:10.140]   Right even when you use Pytorch it still does the for loop in Python it has no way of like
[00:49:10.140 --> 00:49:15.660]   Optimizing the for loop and so a for loop in Python is something like
[00:49:15.660 --> 00:49:18.740]   10,000 times slower than in C
[00:49:18.740 --> 00:49:27.580]   So that's your first problem. I can't remember. It's like 1,000 or 10,000 the second problem then is that
[00:49:29.260 --> 00:49:31.500]   You don't just want it to be optimized in C
[00:49:31.500 --> 00:49:37.700]   But you want C to take advantage of the thing that you're all of your CPUs do to something called SIMD
[00:49:37.700 --> 00:49:43.500]   Single instruction multiple data, which is it yours your CPU is capable of taking
[00:49:43.500 --> 00:49:46.260]   eight things at a time
[00:49:46.260 --> 00:49:49.860]   Right in a vector and adding them up to another
[00:49:49.860 --> 00:49:54.420]   Vector with eight things in in a single CPU instruction
[00:49:55.060 --> 00:49:59.260]   All right, so if you can take advantage of SIMD you're immediately eight times faster
[00:49:59.260 --> 00:50:02.300]   It depends on how big the data type is it might be four might be eight
[00:50:02.300 --> 00:50:07.260]   The other thing that you've got in your computer is you've got multiple processors
[00:50:07.260 --> 00:50:11.300]   Multiple cores
[00:50:11.300 --> 00:50:18.060]   So you've probably got like if this is inside happening on one side one core. You've probably got about four of those
[00:50:19.300 --> 00:50:25.740]   Okay, so if you're using SIMD you're eight times faster if you can use multiple cores, then you're 32 times faster
[00:50:25.740 --> 00:50:28.180]   And then if you're doing that in C
[00:50:28.180 --> 00:50:34.860]   You might be something like 32 times about thousand times faster right and so the nice thing is that when we do that
[00:50:34.860 --> 00:50:38.340]   It's taking advantage of all of these things
[00:50:38.340 --> 00:50:42.900]   Okay, better still if you do it
[00:50:42.900 --> 00:50:48.300]   in pytorch and your data was created with
[00:50:48.300 --> 00:50:52.060]   .Cuda to stick it on the GPU
[00:50:52.060 --> 00:50:57.380]   Then your GPU can do about 10,000 things at a time
[00:50:57.380 --> 00:51:01.440]   Right so that'll be another hundred times faster than C
[00:51:01.440 --> 00:51:04.500]   All right, so this is critical
[00:51:04.500 --> 00:51:10.060]   To getting good performance is you have to learn how to write
[00:51:10.060 --> 00:51:12.500]   loopless code
[00:51:12.500 --> 00:51:14.840]   By taking advantage of these element wise
[00:51:15.900 --> 00:51:19.040]   Operations and like it's not it's a lot more than just plus I
[00:51:19.040 --> 00:51:28.860]   Could also use less than right and that's going to return 0 1 1 or if we go back to numpy
[00:51:28.860 --> 00:51:35.660]   False true true
[00:51:35.660 --> 00:51:42.080]   And so you can kind of use this to do all kinds of things without looping so for example
[00:51:42.080 --> 00:51:47.460]   I could now multiply that by a and here are all of the values of a
[00:51:47.460 --> 00:51:53.440]   As long as they're less than B or we could take the mean
[00:51:53.440 --> 00:51:59.460]   This is the percentage of values in a that are less than B
[00:51:59.460 --> 00:52:03.660]   All right, so like there's a lot of stuff you can do with this simple idea
[00:52:03.660 --> 00:52:06.260]   But to take it further
[00:52:06.260 --> 00:52:09.700]   Right to take it further than just this element wise operation
[00:52:10.020 --> 00:52:13.220]   We're going to have to go the next step to something called broadcasting
[00:52:13.220 --> 00:52:19.340]   So let's take a five minute break come back at 217 and we'll talk about broadcasting
[00:52:19.340 --> 00:52:26.900]   So
[00:52:26.900 --> 00:52:29.980]   Broadcasting
[00:52:29.980 --> 00:52:36.860]   This is the definition from the numpy documentation of
[00:52:38.020 --> 00:52:41.780]   Broadcasting and I'm going to come back to it in a moment rather than reading it now
[00:52:41.780 --> 00:52:47.500]   But let's start by looking an example of broadcasting
[00:52:47.500 --> 00:52:50.860]   so a is a
[00:52:50.860 --> 00:52:53.820]   Array
[00:52:53.820 --> 00:52:57.180]   With one dimension also known as a rank one tensor
[00:52:57.180 --> 00:52:59.940]   also known as a vector
[00:52:59.940 --> 00:53:03.860]   We can say a greater than zero
[00:53:03.860 --> 00:53:07.220]   so here we have
[00:53:07.820 --> 00:53:08.780]   a
[00:53:08.780 --> 00:53:10.780]   rank one tensor
[00:53:10.780 --> 00:53:15.100]   Right and a rank zero tensor
[00:53:15.100 --> 00:53:19.860]   Right a rank zero tensor is also called a scalar
[00:53:19.860 --> 00:53:23.900]   rank one tensor is also called a vector and
[00:53:23.900 --> 00:53:27.860]   We've got an operation between the two
[00:53:27.860 --> 00:53:35.140]   All right now you've probably done it a thousand times without even noticing. That's kind of weird right that you've got these things of different
[00:53:36.060 --> 00:53:39.820]   Ranks and different sizes, so what is it actually doing right?
[00:53:39.820 --> 00:53:46.140]   But what it's actually doing is it's taking that scalar and copying it here here here
[00:53:46.140 --> 00:53:50.060]   Right and then it's actually going element wise
[00:53:50.060 --> 00:53:53.780]   10 is greater than 0
[00:53:53.780 --> 00:54:00.820]   6 is greater than 0 minus 4 is greater than 0 you haven't giving us back the three answers
[00:54:01.260 --> 00:54:05.260]   Right and that's called broadcasting broadcasting means
[00:54:05.260 --> 00:54:11.060]   Copying one or more axes of my tensor
[00:54:11.060 --> 00:54:16.640]   To allow it to be the same shape as the other tensor
[00:54:16.640 --> 00:54:20.580]   It doesn't really copy it though
[00:54:20.580 --> 00:54:29.980]   What it actually does is it stores this kind of internal indicator that says pretend that this is a
[00:54:30.500 --> 00:54:32.500]   vector of three zeros
[00:54:32.500 --> 00:54:38.540]   But it actually just like what rather than kind of going to the next row or going to the next scalar it goes back
[00:54:38.540 --> 00:54:42.620]   To where it came from if you're interested in learning about this specifically
[00:54:42.620 --> 00:54:50.300]   It's they set the stride on that axis to be zero. That's a minor advanced concept for those who are curious
[00:54:50.300 --> 00:54:54.020]   So we could do a
[00:54:55.460 --> 00:54:59.200]   +1 right is going to broadcast the scalar 1
[00:54:59.200 --> 00:55:03.000]   To be 1 1 1 and then do element wise addition
[00:55:03.000 --> 00:55:10.180]   We could do the same with a matrix right here's our matrix 2 times the matrix is going to broadcast 2
[00:55:10.180 --> 00:55:16.380]   to be 2 2 2 2 2 2 2 2 2 2 and then do element wise
[00:55:16.380 --> 00:55:18.500]   multiplication
[00:55:18.500 --> 00:55:21.940]   All right, so that's our kind of most simple version of
[00:55:24.100 --> 00:55:26.100]   broadcasting
[00:55:26.100 --> 00:55:30.460]   So here's a slightly more complex version of broadcasting
[00:55:30.460 --> 00:55:36.180]   Here's an array called C. All right, so this is a rank 1 tensor and
[00:55:36.180 --> 00:55:39.020]   Here's our matrix M from before
[00:55:39.020 --> 00:55:43.600]   Our rank 2 tensor we can add M plus C
[00:55:43.600 --> 00:55:46.100]   All right, so what's going on here?
[00:55:49.820 --> 00:55:55.300]   1 2 3 4 5 6 7 8 9
[00:55:55.300 --> 00:55:58.700]   That's M
[00:55:58.700 --> 00:56:00.700]   All right, and then C
[00:56:00.700 --> 00:56:03.540]   10
[00:56:03.540 --> 00:56:04.940]   20
[00:56:04.940 --> 00:56:06.940]   30
[00:56:06.940 --> 00:56:11.020]   You can see that what it's done is to add that to each row
[00:56:11.020 --> 00:56:14.060]   right eleven twenty two thirty three
[00:56:15.140 --> 00:56:22.480]   14 25 36 and so we can kind of figure it seems to have done the same kind of idea as broadcasting a scalar
[00:56:22.480 --> 00:56:24.700]   It's like made copies of it
[00:56:24.700 --> 00:56:32.060]   And then it treats those as
[00:56:32.060 --> 00:56:40.980]   If it's a rank 2 matrix and now we can do element wise addition
[00:56:42.340 --> 00:56:48.140]   That makes sense now that's yes, can can you pass that Devon over there? Thank you
[00:56:48.140 --> 00:56:54.220]   So as it's like by looking at this example it like
[00:56:54.220 --> 00:56:56.500]   Copies it down
[00:56:56.500 --> 00:56:58.420]   making new rows
[00:56:58.420 --> 00:57:02.700]   So how would we want to do it if we wanted to get new columns? I'm so glad you asked
[00:57:02.700 --> 00:57:07.300]   So
[00:57:10.740 --> 00:57:12.740]   Instead
[00:57:12.740 --> 00:57:15.420]   We would do this
[00:57:15.420 --> 00:57:20.380]   10 20 30
[00:57:20.380 --> 00:57:24.900]   All right, and then copy that 10 20 30
[00:57:24.900 --> 00:57:28.300]   10 20 30 and
[00:57:28.300 --> 00:57:31.380]   Now treat that as our matrix
[00:57:31.380 --> 00:57:36.140]   So to get numpy to do that we need to not pass in a
[00:57:36.140 --> 00:57:38.700]   vector
[00:57:38.700 --> 00:57:40.700]   but to pass in a
[00:57:40.700 --> 00:57:47.420]   Matrix with one column a rank 2 tensor, right?
[00:57:47.420 --> 00:57:50.860]   so basically it turns out that
[00:57:50.860 --> 00:57:54.380]   numpy is going to think of a
[00:57:54.380 --> 00:58:01.420]   Rank 1 tensor for these purposes as if it was a rank 2 tensor which represents a row
[00:58:02.140 --> 00:58:10.140]   Right. So in other words that it is 1 by 3, right? So we want to create a tensor, which is 3 by 1
[00:58:10.140 --> 00:58:13.980]   There's a couple of ways to do that
[00:58:13.980 --> 00:58:17.180]   One is to use NP expand dims
[00:58:17.180 --> 00:58:24.260]   And if you then pass in this argument, it says please insert a length 1 axis
[00:58:24.260 --> 00:58:28.180]   here, please so in our case we want to turn it into a
[00:58:29.100 --> 00:58:33.020]   3 by 1 so if we said expand in C comma 1
[00:58:33.020 --> 00:58:46.620]   Okay, so if we say expand in C comma 1 it changes the shape to 3 comma 1 so if we look at what that looks like
[00:58:46.620 --> 00:58:52.400]   That looks like a column. Okay, so if we now go
[00:58:52.400 --> 00:58:54.340]   that
[00:58:54.340 --> 00:58:55.820]   plus M
[00:58:55.820 --> 00:58:58.980]   You can see it's doing exactly what we hoped it would do
[00:58:58.980 --> 00:59:03.620]   Right, which is to add 10 20 30 to the column
[00:59:03.620 --> 00:59:08.280]   10 20 30 to the column 10 20 30 to the column
[00:59:08.280 --> 00:59:10.220]   Okay
[00:59:10.220 --> 00:59:12.220]   now because the
[00:59:12.220 --> 00:59:17.080]   Location of a unit axis turns out to be so important
[00:59:20.580 --> 00:59:27.840]   It's really helpful to kind of experiment with creating these extra unit axes and know how to do it easily and
[00:59:27.840 --> 00:59:30.060]   NP dot expand dims
[00:59:30.060 --> 00:59:33.420]   Isn't in my opinion the easiest way to do this the easiest way?
[00:59:33.420 --> 00:59:40.340]   The easiest way is to index into the tensor with a special
[00:59:40.340 --> 00:59:49.220]   Index none and what none does is it creates a new axis in that location of
[00:59:49.980 --> 00:59:53.660]   Length 1 right so this is
[00:59:53.660 --> 00:59:58.460]   Going to add a new axis at the start of length 1
[00:59:58.460 --> 01:00:07.840]   This is going to add a new axis at the end of length 1 or
[01:00:07.840 --> 01:00:11.580]   Why not do both?
[01:00:11.580 --> 01:00:15.200]   Right so if you think about it like a tensor
[01:00:15.200 --> 01:00:17.980]   Which has like three?
[01:00:18.340 --> 01:00:22.860]   Things in it could be of any rank you like right you can just add
[01:00:22.860 --> 01:00:27.540]   Unit axes all over the place and so that way we can kind of
[01:00:27.540 --> 01:00:32.220]   Decide how we want our broadcasting to work
[01:00:32.220 --> 01:00:35.380]   So there's a pretty convenient
[01:00:35.380 --> 01:00:43.660]   Thing in numpy called broadcast 2 and what that does is it takes our vector and
[01:00:45.100 --> 01:00:49.020]   broadcasts it to that shape and shows us what that would look like
[01:00:49.020 --> 01:00:55.060]   Right so if you're ever like unsure of what's going on in some broadcasting operation
[01:00:55.060 --> 01:01:01.980]   You can say broadcast 2 and so for example here. We could say like rather than 3 comma 3 we could say m dot shape
[01:01:01.980 --> 01:01:09.620]   Right and see exactly what's happened going to happen, and so that's what's going to happen before we add it to n
[01:01:09.620 --> 01:01:11.980]   right so if we said
[01:01:11.980 --> 01:01:16.300]   Turn it into a column
[01:01:16.300 --> 01:01:21.460]   That's what that looks like
[01:01:21.460 --> 01:01:26.500]   Make sense, so that's kind of like the intuitive
[01:01:26.500 --> 01:01:29.340]   definition of
[01:01:29.340 --> 01:01:31.940]   Broadcasting and so now hopefully we can go back to that
[01:01:31.940 --> 01:01:34.900]   numpy documentation and understand
[01:01:34.900 --> 01:01:37.580]   What it means right?
[01:01:38.140 --> 01:01:42.740]   Broadcasting describes how numpy is going to treat arrays of different shapes when we do some operation
[01:01:42.740 --> 01:01:50.220]   Right the smaller array is broadcast across the larger array by smaller array. They mean lower rank
[01:01:50.220 --> 01:01:52.860]   tensor basically
[01:01:52.860 --> 01:01:59.540]   Broadcast across the light the higher rank tensor so they have compatible shapes it vectorizes array operations
[01:01:59.540 --> 01:02:05.540]   So vectorizing generally means like using SIMD and stuff like that so that multiple things happen at the same time
[01:02:06.820 --> 01:02:08.820]   All the looping occurs in C
[01:02:08.820 --> 01:02:15.140]   But it doesn't actually make needless copies of data it kind of just acts as if it had
[01:02:15.140 --> 01:02:18.060]   So there's our definition
[01:02:18.060 --> 01:02:24.620]   now in deep learning you very often deal with tensors of rank four or more and
[01:02:24.620 --> 01:02:29.060]   you very often combine them with tensors of rank one or two and
[01:02:29.060 --> 01:02:33.540]   Trying to just rely on intuition to do that correctly is nearly impossible
[01:02:34.140 --> 01:02:36.420]   So you really need to know the rules?
[01:02:36.420 --> 01:02:42.300]   So here are the rules
[01:02:42.300 --> 01:02:50.180]   Okay, here's m dot shape here C dot shape so the rule are that we're going to compare
[01:02:50.180 --> 01:02:54.740]   The shapes of our two tensors element wise we're going to look at one at a time
[01:02:54.740 --> 01:02:59.180]   And we're going to start at the end right so look at the trailing dimensions and
[01:02:59.180 --> 01:03:01.460]   then go
[01:03:01.460 --> 01:03:06.220]   Towards the front okay, and so two dimensions are going to be compatible
[01:03:06.220 --> 01:03:14.880]   When one of these two things is true, right? So let's check right we've got our our M and C compatible M is
[01:03:14.880 --> 01:03:17.180]   3 3
[01:03:17.180 --> 01:03:18.500]   C is
[01:03:18.500 --> 01:03:26.620]   3 right so we're going to start at the end trailing dimensions first and check are they compatible they're compatible if the dimensions are equal
[01:03:26.620 --> 01:03:29.260]   Okay, so these ones are equal so they're compatible
[01:03:29.860 --> 01:03:31.180]   right
[01:03:31.180 --> 01:03:34.140]   Let's go to the next one. Oh, oh, we're missing
[01:03:34.140 --> 01:03:41.100]   Right C is missing something. So what happens if something is missing as we insert a one?
[01:03:41.100 --> 01:03:49.140]   Okay, that's the rule right and so let's now check are these compatible one of them is one. Yes, they're compatible
[01:03:49.140 --> 01:03:55.260]   Okay, so now you can see why it is that numpy treats
[01:03:55.260 --> 01:03:58.740]   the one dimensional array as
[01:03:59.460 --> 01:04:02.060]   If it is a rank 2 tensor
[01:04:02.060 --> 01:04:08.540]   Which is representing a row it's because we're basically inserting a one at the front
[01:04:08.540 --> 01:04:12.620]   Okay, so that's the rule so for example
[01:04:12.620 --> 01:04:20.780]   This is something that you very commonly have to do which is you start with like an
[01:04:20.780 --> 01:04:26.980]   image they're like 256 pixels by 256 pixels by three channels and
[01:04:27.740 --> 01:04:29.740]   You want to subtract?
[01:04:29.740 --> 01:04:31.740]   the mean of each channel
[01:04:31.740 --> 01:04:37.980]   All right, so you've got 256 by 256 by 3 and you want to subtract something of length 3, right?
[01:04:37.980 --> 01:04:40.020]   So yeah, you can do that
[01:04:40.020 --> 01:04:43.980]   Absolutely because 3 and 3 are compatible because they're the same
[01:04:43.980 --> 01:04:48.340]   All right 256 and empty is compatible. It's going to insert a 1
[01:04:48.340 --> 01:04:51.700]   256 and empty is compatible. It's going to insert a 1
[01:04:51.700 --> 01:04:54.540]   Okay, so you're going to end up with
[01:04:55.740 --> 01:05:03.860]   this is going to be broadcast over all of this axis and then that whole thing will be broadcast over this axis and
[01:05:03.860 --> 01:05:05.860]   so we'll end up with a
[01:05:05.860 --> 01:05:08.460]   256 by 256 by 3
[01:05:08.460 --> 01:05:12.060]   Effective
[01:05:12.060 --> 01:05:14.060]   Tensor here, right?
[01:05:14.060 --> 01:05:17.300]   so interestingly like
[01:05:17.300 --> 01:05:22.020]   very few people in the data science or machine learning communities
[01:05:22.300 --> 01:05:28.820]   Understand broadcasting and the vast majority of the time for example when I see people doing pre-processing for computer vision
[01:05:28.820 --> 01:05:32.760]   Like subtracting the mean they always write loops
[01:05:32.760 --> 01:05:36.780]   over the channels right and I kind of think like
[01:05:36.780 --> 01:05:44.220]   It's it's like so handy to not have to do that and it's often so much faster to not have to do that
[01:05:44.220 --> 01:05:46.220]   So if you get good at broadcasting
[01:05:46.220 --> 01:05:51.460]   You'll have this like super useful skill that very very few people have
[01:05:52.060 --> 01:05:57.940]   And and like it's it's it's an ancient skill. You know it goes it goes all the way back to
[01:05:57.940 --> 01:06:00.980]   the days of APL
[01:06:00.980 --> 01:06:07.680]   so APL was from the late 50s stands for our programming language and
[01:06:07.680 --> 01:06:11.100]   Kenneth Iverson
[01:06:11.100 --> 01:06:13.100]   Wrote this paper called
[01:06:13.100 --> 01:06:15.940]   notation as a tool for thought
[01:06:15.940 --> 01:06:20.140]   in which he proposed a new math notation and
[01:06:21.100 --> 01:06:24.700]   He proposed that if we use this new math notation
[01:06:24.700 --> 01:06:32.460]   It gives us new tools for thought and allows us to think things we couldn't before and one of his ideas was
[01:06:32.460 --> 01:06:35.660]   broadcasting not as a
[01:06:35.660 --> 01:06:40.340]   computer programming tool, but as a piece of math notation and
[01:06:40.340 --> 01:06:43.180]   so he ended up implementing
[01:06:43.180 --> 01:06:48.540]   this notation as a tool for thought as a programming language called APL and
[01:06:49.260 --> 01:06:54.100]   His son has gone on to further develop that
[01:06:54.100 --> 01:06:57.180]   Into a piece of software called J
[01:06:57.180 --> 01:07:03.980]   Which is basically what you get when you put 60 years of very smart people working on this idea
[01:07:03.980 --> 01:07:07.820]   And with this programming language you can express
[01:07:07.820 --> 01:07:13.380]   Very complex mathematical ideas often just with a line of code or two
[01:07:13.380 --> 01:07:16.940]   And so I mean it's great that we have J
[01:07:16.940 --> 01:07:21.020]   But it's even greater that these ideas have found their ways into the languages
[01:07:21.020 --> 01:07:26.740]   We all use like in Python the NumPy and PyTorch libraries, right? These are not just little
[01:07:26.740 --> 01:07:33.020]   Kind of niche ideas. It's like fundamental ways to think about math and to do programming
[01:07:33.020 --> 01:07:38.220]   Like let me give an example of like this kind of notation as a tool for thought
[01:07:38.220 --> 01:07:41.220]   let's
[01:07:41.220 --> 01:07:45.700]   Let's look here. We've got C, right?
[01:07:46.380 --> 01:07:48.380]   Here we've got C
[01:07:48.380 --> 01:07:55.940]   None right. Notice. This is now a two square brackets, right? So this is kind of like a one row
[01:07:55.940 --> 01:07:59.060]   rank 2 tensor
[01:07:59.060 --> 01:08:04.140]   Here it is a little column
[01:08:04.140 --> 01:08:08.620]   So what is
[01:08:08.620 --> 01:08:10.620]   Oh
[01:08:10.620 --> 01:08:19.780]   Just round ones
[01:08:19.780 --> 01:08:27.820]   Okay, what's that going to do? Have a think about it
[01:08:34.580 --> 01:08:40.580]   Anybody want to have a go you can even talk through your thinking. Okay. Can we pass the check this over there? Thank you
[01:08:40.580 --> 01:08:47.780]   Kind of outer product. Yes, absolutely. So take us through your thinking. How's that gonna work?
[01:08:47.780 --> 01:08:54.620]   So the diagonal elements can be directly visualized from the squares
[01:08:54.620 --> 01:08:59.780]   And cross 10 20 cross 20 and 30 cross 30
[01:09:00.780 --> 01:09:07.900]   And if you multiply the first row with this column, you can get the first row of the matrix
[01:09:07.900 --> 01:09:12.500]   So finally you'll get a 3 cross 3 matrix. Yeah, and
[01:09:12.500 --> 01:09:18.220]   So to think of this in terms of like those broadcasting rules, we're basically taking
[01:09:18.220 --> 01:09:21.700]   This column right which is of rank
[01:09:21.700 --> 01:09:27.260]   3 comma 1 right and this kind of row
[01:09:28.780 --> 01:09:34.340]   Sorry, I mentioned 3 comma 1 and this row which is of dimension 1 comma 3
[01:09:34.340 --> 01:09:39.220]   Right and so to make these compatible with our broadcasting rules
[01:09:39.220 --> 01:09:42.380]   Right this one here has to be duplicated
[01:09:42.380 --> 01:09:45.140]   Three times because it needs to match this
[01:09:45.140 --> 01:09:57.700]   Okay, and now this one's going to have to be duplicated three times to match this
[01:09:57.700 --> 01:10:05.100]   Okay, and so now I've got two
[01:10:05.100 --> 01:10:12.820]   Matrices to do an element wise product of and so as you say
[01:10:12.820 --> 01:10:17.900]   There is our outer product right now. The interesting thing here is
[01:10:17.900 --> 01:10:22.700]   That suddenly now that this is not a special mathematical case
[01:10:23.220 --> 01:10:30.980]   But just a specific version of the general idea of broadcasting we can do like an outer plus
[01:10:30.980 --> 01:10:35.060]   Or we can do an outer greater than
[01:10:35.060 --> 01:10:42.340]   Right or or whatever right so it's suddenly we've kind of got this this this concept
[01:10:42.340 --> 01:10:44.940]   That we can use to build
[01:10:44.940 --> 01:10:51.260]   New ideas and then we can start to experiment with those new ideas. And so, you know interestingly
[01:10:52.100 --> 01:10:54.100]   NumPy actually
[01:10:54.100 --> 01:10:56.580]   Uses this sometimes
[01:10:56.580 --> 01:11:02.100]   For example if you want to create a grid
[01:11:02.100 --> 01:11:11.660]   This is how NumPy does it right actually this is kind of the sorry, let me show you this way
[01:11:11.660 --> 01:11:16.820]   If you want to create a grid, this is how NumPy does it it actually returns
[01:11:16.820 --> 01:11:20.060]   0 1 2 3 4 and
[01:11:21.620 --> 01:11:23.620]   0 1 2 3 4
[01:11:23.620 --> 01:11:26.060]   1 is a column 1 is a row
[01:11:26.060 --> 01:11:30.340]   So we could say like okay, that's x grid comma y grid
[01:11:30.340 --> 01:11:36.220]   And now you could do something like
[01:11:36.220 --> 01:11:42.580]   Well, I mean we could obviously go
[01:11:42.580 --> 01:11:49.620]   Like that right and so suddenly we've expanded that out
[01:11:49.620 --> 01:11:59.220]   Into a grid right and so
[01:11:59.220 --> 01:12:05.580]   Yeah, it's kind of interesting how like some of these like simple little concepts
[01:12:05.580 --> 01:12:11.660]   Kind of get built on and built on and built on so if you lose something like APL or J. It's this whole
[01:12:11.660 --> 01:12:18.260]   Environment of layers and layers and layers of this we don't have such a deep environment in NumPy
[01:12:18.260 --> 01:12:22.900]   But you know you can certainly see these ideas of like broadcasting coming through
[01:12:22.900 --> 01:12:27.220]   In simple things like how do we create a grid in in NumPy?
[01:12:27.220 --> 01:12:34.860]   So yeah, so that's that's broadcasting and so what we can do with this now is
[01:12:34.860 --> 01:12:40.540]   Use this to implement matrix multiplication ourselves
[01:12:40.540 --> 01:12:43.940]   Okay
[01:12:43.980 --> 01:12:50.620]   Now why would we want to do that well obviously we don't right matrix multiplication has already been handled
[01:12:50.620 --> 01:12:54.200]   Perfectly nicely for us by our libraries
[01:12:54.200 --> 01:12:57.620]   but very often you'll find in
[01:12:57.620 --> 01:13:04.820]   All kinds of areas in in machine learning and particularly in deep learning that there'll be
[01:13:04.820 --> 01:13:08.460]   particular types of linear
[01:13:08.460 --> 01:13:12.580]   Function that you want to do that aren't quite
[01:13:13.300 --> 01:13:17.700]   Done for you all right so for example. There's like whole areas
[01:13:17.700 --> 01:13:20.620]   called like
[01:13:20.620 --> 01:13:22.620]   tensor regression and
[01:13:22.620 --> 01:13:26.980]   Tensor decomposition
[01:13:26.980 --> 01:13:37.900]   Which are really being developed a lot at the moment and they're kind of talking about like how do we take like
[01:13:38.380 --> 01:13:43.260]   Higher rank tensors and kind of turn them into combinations of rows
[01:13:43.260 --> 01:13:50.260]   Columns and faces and it turns out that when you can kind of do this you can basically like
[01:13:50.260 --> 01:13:58.100]   Deal with really high dimensional data structures with not much memory and not with not much computation time for example. There's a really terrific library
[01:13:58.100 --> 01:14:00.460]   called tensorly
[01:14:00.460 --> 01:14:02.460]   Which does a whole lot of this kind of stuff?
[01:14:02.460 --> 01:14:05.580]   for you
[01:14:05.660 --> 01:14:12.460]   So it's a really really important area it covers like all of deep learning lots of modern machine learning in general
[01:14:12.460 --> 01:14:19.660]   And so even though you're not going to like to find matrix modification. You're very likely to want to define some other
[01:14:19.660 --> 01:14:22.820]   Slightly different tensor product you know
[01:14:22.820 --> 01:14:26.700]   So it's really useful to kind of understand how to do that
[01:14:26.700 --> 01:14:29.660]   So let's go back and look at our
[01:14:29.660 --> 01:14:32.700]   matrix and our
[01:14:34.260 --> 01:14:38.020]   2d array and 1d array rank 2 tensor rank 1 tensor and
[01:14:38.020 --> 01:14:40.860]   Remember we can do a matrix multiplication
[01:14:40.860 --> 01:14:46.500]   Using the at sign or the old way NP dot matmul. Okay?
[01:14:46.500 --> 01:14:51.540]   And so what that's actually doing when we do that is we're basically saying
[01:14:51.540 --> 01:14:56.420]   Okay, 1 times 10 plus
[01:14:56.420 --> 01:15:01.900]   2 times 20 plus 3 times 30 is
[01:15:02.820 --> 01:15:05.380]   140 right and so we do that for each
[01:15:05.380 --> 01:15:07.700]   row and
[01:15:07.700 --> 01:15:12.660]   We can go through and do the same thing for the next one and for the next one to get our result, right?
[01:15:12.660 --> 01:15:17.020]   You could do that in torch as well
[01:15:17.020 --> 01:15:21.340]   We could make this a little shorter
[01:15:32.020 --> 01:15:34.020]   Okay, same thing
[01:15:34.020 --> 01:15:45.180]   Okay, but that is not matrix multiplication. What's that?
[01:15:45.180 --> 01:15:53.900]   Okay, element wise specifically we've got a matrix and a vector so
[01:15:53.900 --> 01:16:00.380]   Broadcasting okay good. So we've got this is element wise with broadcasting but notice
[01:16:01.180 --> 01:16:07.500]   The numbers it's created 10 40 90 are the exact three numbers that I needed to
[01:16:07.500 --> 01:16:10.420]   Calculate when I did that first
[01:16:10.420 --> 01:16:15.180]   Piece of my matrix multiplication. So in other words if we sum this
[01:16:15.180 --> 01:16:21.120]   Over the columns, which is axis equals 1
[01:16:21.120 --> 01:16:25.540]   We get our matrix vector product
[01:16:25.540 --> 01:16:29.780]   Okay, so we can kind of
[01:16:30.340 --> 01:16:31.700]   do
[01:16:31.700 --> 01:16:35.140]   This stuff without special help from our library
[01:16:35.140 --> 01:16:38.580]   So now
[01:16:38.580 --> 01:16:42.480]   Let's expand this out to a matrix matrix product
[01:16:42.480 --> 01:16:45.700]   So a matrix matrix product
[01:16:45.700 --> 01:16:52.420]   Looks like this. This is this great site called matrix multiplication dot XYZ
[01:16:52.420 --> 01:16:57.320]   And it shows us this is what happens when we multiply two matrices
[01:16:57.320 --> 01:17:06.400]   Okay, that's what matrix multiplication is
[01:17:06.400 --> 01:17:11.360]   operationally speaking so in other words what we just did there
[01:17:11.360 --> 01:17:16.440]   Was we first of all took the first column
[01:17:16.440 --> 01:17:20.680]   with the first row to get this one and
[01:17:20.680 --> 01:17:25.520]   Then we took the second column with the first row
[01:17:26.120 --> 01:17:29.040]   To get that one. All right, so we're basically doing
[01:17:29.040 --> 01:17:33.760]   The thing we just did the matrix vector product. We're just doing it twice
[01:17:33.760 --> 01:17:36.480]   right once
[01:17:36.480 --> 01:17:44.520]   With this column and once with this column, and then we can catenate the two together
[01:17:44.520 --> 01:17:49.680]   Okay, so we can now go ahead and do that
[01:17:52.760 --> 01:17:57.640]   Like so M times the first column dot sum
[01:17:57.640 --> 01:18:06.480]   M times the second top column, but some and so there are the two columns of our matrix multiplication
[01:18:06.480 --> 01:18:09.240]   Okay
[01:18:09.240 --> 01:18:12.960]   So I didn't want to like make our code too messy
[01:18:12.960 --> 01:18:19.560]   So I'm not going to actually like use that but like we have it there now if we want to we don't need to use
[01:18:20.280 --> 01:18:26.360]   Torch or NumPy matrix multiplication anymore. We've got we've got our own that we can use using nothing but
[01:18:26.360 --> 01:18:29.640]   element wise operations broadcasting and
[01:18:29.640 --> 01:18:32.680]   some
[01:18:32.680 --> 01:18:36.000]   Okay
[01:18:36.000 --> 01:18:39.960]   So this is our
[01:18:39.960 --> 01:18:45.240]   Logistic regression from scratch class again. I just copied it here
[01:18:45.960 --> 01:18:50.160]   Here is where we instantiate the object copy it to the GPU we create an optimizer
[01:18:50.160 --> 01:18:58.600]   Which we'll learn about in a moment and we call fit. Okay, so the goal is to now repeat this without needing to call fit
[01:18:58.600 --> 01:19:03.760]   So to do that
[01:19:03.760 --> 01:19:09.320]   We're going to need a loop
[01:19:09.320 --> 01:19:14.680]   Which grabs a mini batch of data at a time and with each mini batch of data?
[01:19:15.600 --> 01:19:22.040]   We need to pass it to the optimizer and say please try to come up with a slightly better set of predictions
[01:19:22.040 --> 01:19:24.240]   for this mini batch
[01:19:24.240 --> 01:19:28.560]   So as we learned in order to grab a mini batch of the training set at a time
[01:19:28.560 --> 01:19:31.840]   We have to ask the model data object for the training data loader
[01:19:31.840 --> 01:19:36.920]   We have to wrap it in it or it er to create an iterator generator
[01:19:36.920 --> 01:19:43.920]   And so that gives us our data loader. Okay, so pytorch calls this a data loader
[01:19:44.040 --> 01:19:48.080]   We actually wrote our own fast AI data loader, but it's it's all it's basically the same idea
[01:19:48.080 --> 01:19:50.280]   and
[01:19:50.280 --> 01:19:56.480]   So the next thing we do is we grab the X and the Y tensor
[01:19:56.480 --> 01:19:59.520]   The next one from our data loader, okay?
[01:19:59.520 --> 01:20:05.080]   Wrap it in a variable to say I need to be able to take the derivative of
[01:20:05.080 --> 01:20:08.640]   The calculations using this because if I can't take the derivative
[01:20:08.640 --> 01:20:11.960]   Then I can't get the gradients and I can't update the weights
[01:20:12.400 --> 01:20:15.880]   all right, and I need to put it on the GPU because my
[01:20:15.880 --> 01:20:18.760]   module is on the GPU and
[01:20:18.760 --> 01:20:23.340]   So we can now take that variable and pass it to
[01:20:23.340 --> 01:20:28.440]   The object that we instantiated our logistic regression
[01:20:28.440 --> 01:20:32.840]   Remember our module we can use it as if it's a function because that's how pytorch works
[01:20:32.840 --> 01:20:37.920]   And that gives us a set of predictions as we saw seen before
[01:20:37.920 --> 01:20:41.400]   Okay
[01:20:41.760 --> 01:20:46.520]   So now we can check the loss and the loss we defined as being a
[01:20:46.520 --> 01:20:49.440]   negative log likelihood loss
[01:20:49.440 --> 01:20:55.200]   Object and we're going to learn about how that's calculated in the next lesson and for now think of it
[01:20:55.200 --> 01:20:58.320]   Just like root mean squared error, but for classification problems
[01:20:58.320 --> 01:21:03.840]   So we can call that also just like a function so you can kind of see this
[01:21:03.840 --> 01:21:09.480]   It's very general idea in pytorch that you know kind of treat everything ideally like it's a function
[01:21:09.480 --> 01:21:16.560]   So in this case we have a loss a negative log likelihood loss object. We treat it like a function we pass in our predictions and
[01:21:16.560 --> 01:21:23.360]   We pass in our axials right and again the axials need to be turned into a variable and put on the GPU
[01:21:23.360 --> 01:21:30.920]   Because the loss is specifically the thing that we actually want to take the derivative of right so that gives us our loss
[01:21:30.920 --> 01:21:34.960]   And there it is. That's our loss 2.43
[01:21:36.200 --> 01:21:41.320]   So it's a variable and because it's a variable it knows how it was calculated
[01:21:41.320 --> 01:21:47.980]   All right, it knows it was calculated with this loss function. It knows that the predictions were calculated with this
[01:21:47.980 --> 01:21:55.880]   Network it knows that this network consisted of these operations and so we can get the gradient
[01:21:55.880 --> 01:21:58.800]   automatically, all right
[01:21:58.800 --> 01:22:00.920]   So to get the gradient
[01:22:01.800 --> 01:22:06.560]   We call L dot backward remember L is the thing that contains our loss
[01:22:06.560 --> 01:22:13.120]   All right, so L dot backward is is something which is added to anything. That's a variable
[01:22:13.120 --> 01:22:16.440]   You can call dot backward and that says please calculate the gradients
[01:22:16.440 --> 01:22:24.120]   Okay, and so that calculates the gradients and stores them inside that that
[01:22:24.120 --> 01:22:27.440]   the basically for each of the
[01:22:28.120 --> 01:22:33.960]   Weights that was used it used each of the parameters that was used to calculate that it's now stored a
[01:22:33.960 --> 01:22:40.320]   Dot grad we'll see it later. It's basically stored the gradient right so we can then call
[01:22:40.320 --> 01:22:44.520]   Optimizer dot step and we're going to do this step manually shortly
[01:22:44.520 --> 01:22:51.920]   And that's the bit that says please make the weights a little bit better right and so what optimizer dot step is doing
[01:22:53.440 --> 01:22:57.480]   Is it saying like okay if you had like a really simple function?
[01:22:57.480 --> 01:23:04.560]   Like this
[01:23:04.560 --> 01:23:11.580]   Right then what the optimizer does is it says okay. Let's pick a random starting point
[01:23:11.580 --> 01:23:17.400]   Right and let's calculate the value of the loss right so here's our parameter
[01:23:17.400 --> 01:23:21.240]   Here's our loss right let's take the derivative
[01:23:21.920 --> 01:23:28.440]   All right the derivative tells us which way is down, so it tells us we need to go that direction
[01:23:28.440 --> 01:23:31.920]   Okay, and we take a small step and
[01:23:31.920 --> 01:23:37.400]   Then we take the derivative again, and we take a small step derivative again
[01:23:37.400 --> 01:23:40.440]   Take a small step do it again. Take a small step and
[01:23:40.440 --> 01:23:45.560]   Till eventually we're taking such small steps that we stop okay, so that's what?
[01:23:45.560 --> 01:23:48.880]   gradient descent does okay
[01:23:50.080 --> 01:23:52.440]   How big a step is a small step?
[01:23:52.440 --> 01:23:57.300]   Well, we basically take the derivative here, so let's say derivative. There is like eight
[01:23:57.300 --> 01:24:06.020]   All right, and we multiply it by a small number like say 0.01 and that tells us what step size to take
[01:24:06.020 --> 01:24:10.040]   this small number here is called the learning rate and
[01:24:10.040 --> 01:24:17.220]   It's the most important hyper parameter to set right if you pick two smaller learning rate
[01:24:17.800 --> 01:24:23.180]   Then your steps down are going to be like tiny, and it's going to take you forever
[01:24:23.180 --> 01:24:27.960]   All right to bigger learning rate, and you'll jump too far
[01:24:27.960 --> 01:24:35.680]   Right and then you'll jump too far and your diverge rather than converge, okay
[01:24:35.680 --> 01:24:39.640]   We're not going to talk about how to pick a learning rate in this class
[01:24:39.640 --> 01:24:46.200]   But in the deep learning class we actually show you a specific technique that very reliably picks a very good learning rate
[01:24:46.200 --> 01:24:48.200]   um
[01:24:48.200 --> 01:24:53.040]   So that's basically what's happening right so we calculate the derivatives
[01:24:53.040 --> 01:24:58.800]   And we call the optimizer that does a step in other words update the weights based on the
[01:24:58.800 --> 01:25:01.280]   Gradients and the learning rate
[01:25:01.280 --> 01:25:07.800]   We should hopefully find that after doing that we have a better loss than we did before
[01:25:07.800 --> 01:25:12.080]   So I just reran this and got a loss here of four point one six and
[01:25:12.080 --> 01:25:14.600]   after one step
[01:25:14.600 --> 01:25:17.760]   It's now four point. Oh three okay, so it worked the way
[01:25:17.760 --> 01:25:22.640]   We hoped it would based on this mini batch it updated all of the weights in our
[01:25:22.640 --> 01:25:27.780]   Network to be a little better than they were as a result of which our loss went down, okay?
[01:25:27.780 --> 01:25:31.480]   So let's turn that into a training loop
[01:25:31.480 --> 01:25:35.200]   All right, we're going to go through a hundred steps
[01:25:35.200 --> 01:25:38.500]   Grab one more mini batch of data from the data loader
[01:25:39.560 --> 01:25:45.360]   Calculate our predictions from our network calculate our loss from the predictions and the actuals
[01:25:45.360 --> 01:25:51.840]   Every 10 goes we'll print out the accuracy just take the mean of the whether they're equal or not
[01:25:51.840 --> 01:26:01.300]   One Pytorch specific thing you have to zero the gradients basically you can have networks where like you've got lots of different loss
[01:26:01.300 --> 01:26:03.980]   Functions that you might want to add all of the gradients together
[01:26:03.980 --> 01:26:09.400]   Right so you have to tell Pytorch like when to set the gradients back to zero
[01:26:09.400 --> 01:26:12.120]   Right so this just says set all the gradients to zero
[01:26:12.120 --> 01:26:18.180]   Calculate the gradients that's put backward and then take one step of the optimizer
[01:26:18.180 --> 01:26:25.040]   So update the weights using the gradients and the learning rate and so once we run it. You can see the loss goes down and
[01:26:25.040 --> 01:26:28.560]   The accuracy goes up
[01:26:28.560 --> 01:26:32.000]   Okay
[01:26:32.000 --> 01:26:34.160]   so
[01:26:34.160 --> 01:26:38.800]   That's the basic approach and so next lesson. We'll see
[01:26:39.040 --> 01:26:40.320]   what
[01:26:40.320 --> 01:26:42.320]   That does all right
[01:26:42.320 --> 01:26:44.360]   We're looking in detail
[01:26:44.360 --> 01:26:50.680]   We're not going to look inside here as I say we're going to basically take the calculation of the derivatives as
[01:26:50.680 --> 01:26:53.640]   As a given right but basically
[01:26:53.640 --> 01:26:56.240]   What's happening there?
[01:26:56.240 --> 01:27:01.080]   And any kind of deep network you have kind of like a function
[01:27:01.080 --> 01:27:03.480]   That's like you know a linear function
[01:27:03.480 --> 01:27:08.920]   And then you pass the output of that into another function that might be like a ReLU
[01:27:08.920 --> 01:27:14.320]   And you pass the output of that into another function that might be another linear net linear layer
[01:27:14.320 --> 01:27:22.320]   And you pass that into another function that might be another ReLU and so forth right so these deep networks are just
[01:27:22.320 --> 01:27:27.960]   Functions of functions of functions, so you could write them mathematically like that right and so
[01:27:30.200 --> 01:27:34.560]   All backprop does is it says let's just simplify this down to the two version
[01:27:34.560 --> 01:27:40.880]   Is we can say okay u equals f of x
[01:27:40.880 --> 01:27:50.160]   Right and so therefore the derivative of g of f of x is we can calculate with the chain rule as being
[01:27:50.160 --> 01:27:53.400]   g - u
[01:27:54.080 --> 01:27:56.160]   f - x
[01:27:56.160 --> 01:28:02.880]   Right and so you can see we can do the same thing for the functions of the functions of the functions, and so when you apply a
[01:28:02.880 --> 01:28:09.880]   Function to a function of a function you can take the derivative just by taking the product of the derivatives of each of those
[01:28:09.880 --> 01:28:15.040]   Layers okay, and in neural networks. We call this back propagation
[01:28:15.040 --> 01:28:20.320]   Okay, so when you hear back propagation it just means use the chain rule to calculate the derivatives
[01:28:21.600 --> 01:28:25.480]   And so when you see a neural network defined
[01:28:25.480 --> 01:28:31.560]   Like here right
[01:28:31.560 --> 01:28:37.500]   Like if it's defined sequentially literally all this means is
[01:28:37.500 --> 01:28:40.840]   apply this function to the input
[01:28:40.840 --> 01:28:49.000]   Apply this function to that apply this function to that apply this function to that right so this is just defining a
[01:28:49.840 --> 01:28:53.040]   composition of a function to a function to a function to a function to a function
[01:28:53.040 --> 01:28:56.000]   okay, and so
[01:28:56.000 --> 01:28:59.740]   Yeah, so although we're not going to bother with calculating the gradients ourselves
[01:28:59.740 --> 01:29:03.480]   You can now see why it can do it right as long as it has internally
[01:29:03.480 --> 01:29:10.440]   You know a it knows like what's the what's the derivative of to the power of what's the derivative of sign?
[01:29:10.440 --> 01:29:14.000]   What's the derivative of plus and so forth then our Python code?
[01:29:14.000 --> 01:29:17.880]   In here, it's just combining those things together
[01:29:18.920 --> 01:29:26.140]   So it just needs to know how to compose them together with the chain rule and away it goes, okay?
[01:29:26.140 --> 01:29:38.240]   Okay, so I think we can leave it there for now and yeah and in the next class
[01:29:38.240 --> 01:29:40.240]   We'll go and we'll see how to
[01:29:40.240 --> 01:29:47.680]   Write our own optimizer, and then we'll have solved MNIST from scratch ourselves. See you then
[01:29:47.680 --> 01:29:57.680]   [BLANK_AUDIO]

