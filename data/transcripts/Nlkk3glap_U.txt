
[00:00:00.000 --> 00:00:02.240]   a generally well-educated human.
[00:00:02.240 --> 00:00:05.160]   That could happen in, you know, two or three years.
[00:00:05.160 --> 00:00:06.760]   - What does that imply for Anthropic
[00:00:06.760 --> 00:00:07.840]   when in two to three years,
[00:00:07.840 --> 00:00:11.080]   these Leviathans are doing like $10 billion training runs.
[00:00:11.080 --> 00:00:12.440]   - The models, they just want to learn.
[00:00:12.440 --> 00:00:14.400]   And it was a bit like a Zen Cohen.
[00:00:14.400 --> 00:00:16.600]   I listened to this and I became enlightened.
[00:00:16.600 --> 00:00:21.800]   The compute doesn't flow, like the spice doesn't flow.
[00:00:21.800 --> 00:00:23.640]   It's like, you can't like,
[00:00:23.640 --> 00:00:26.520]   like the blob has to be unencumbered, right?
[00:00:26.520 --> 00:00:29.400]   The big acceleration that happened late last year
[00:00:29.400 --> 00:00:32.120]   and beginning of this year, we didn't cause that.
[00:00:32.120 --> 00:00:34.320]   And honestly, I think if you look at the reaction to Google,
[00:00:34.320 --> 00:00:36.040]   that that might be 10 times more important
[00:00:36.040 --> 00:00:37.440]   than anything else.
[00:00:37.440 --> 00:00:38.480]   There was a running joke.
[00:00:38.480 --> 00:00:40.440]   The way building AGI would look like is, you know,
[00:00:40.440 --> 00:00:43.480]   there would be a data center next to a nuclear power plant,
[00:00:43.480 --> 00:00:44.720]   next to a bunker.
[00:00:44.720 --> 00:00:46.720]   - But now it's 2030, what happens next?
[00:00:46.720 --> 00:00:49.800]   What are we doing with a superhuman God?
[00:00:49.800 --> 00:00:52.400]   Okay, today I have the pleasure of speaking
[00:00:52.400 --> 00:00:56.120]   with Dario Amodei, who is the CEO of Anthropic.
[00:00:56.120 --> 00:00:57.640]   And I'm really excited about this one.
[00:00:57.640 --> 00:00:59.840]   Dario, thank you so much for coming on the podcast.
[00:00:59.840 --> 00:01:00.880]   - Thanks for having me.
[00:01:00.880 --> 00:01:03.840]   - First question, you have been one of the very few people
[00:01:03.840 --> 00:01:06.680]   who has seen scaling coming for years,
[00:01:06.680 --> 00:01:07.520]   more than five years.
[00:01:07.520 --> 00:01:09.080]   I don't know how long it's been,
[00:01:09.080 --> 00:01:11.360]   but as somebody who's seen it coming,
[00:01:11.360 --> 00:01:14.600]   what is fundamentally the explanation for why scaling works?
[00:01:14.600 --> 00:01:16.360]   Why is the universe organized such that
[00:01:16.360 --> 00:01:18.960]   if you throw big blobs and compute
[00:01:18.960 --> 00:01:20.680]   at a wide enough distribution of data,
[00:01:20.680 --> 00:01:22.240]   the thing becomes intelligent?
[00:01:22.240 --> 00:01:24.440]   - I think the truth is that we still don't know.
[00:01:24.440 --> 00:01:28.320]   I think it's almost entirely an empirical fact.
[00:01:28.320 --> 00:01:30.800]   I think it's a fact that you could kind of sense
[00:01:30.800 --> 00:01:33.840]   from the data and from a bunch of different places,
[00:01:33.840 --> 00:01:35.240]   but I think we don't still have
[00:01:35.240 --> 00:01:36.800]   a satisfying explanation for it.
[00:01:36.800 --> 00:01:38.600]   If I were to try to make one,
[00:01:38.600 --> 00:01:39.440]   but I'm just, I don't know,
[00:01:39.440 --> 00:01:42.000]   I'm just kind of waving my hands when I say this.
[00:01:42.000 --> 00:01:45.240]   You know, there's these ideas in physics
[00:01:45.240 --> 00:01:49.000]   around like long tail or power law
[00:01:49.000 --> 00:01:51.680]   of like correlations or effects.
[00:01:51.680 --> 00:01:54.320]   And so like when a bunch of stuff happens, right?
[00:01:54.320 --> 00:01:56.760]   When you have a bunch of like features,
[00:01:56.760 --> 00:01:59.240]   you get a lot of the data in like kind of the early,
[00:01:59.240 --> 00:02:01.800]   you know, the fat part of the distribution
[00:02:01.800 --> 00:02:03.200]   before the tails.
[00:02:03.200 --> 00:02:04.960]   You know, for language, this would be things like,
[00:02:04.960 --> 00:02:06.800]   oh, I figured out there are parts of speech
[00:02:06.800 --> 00:02:08.320]   and nouns follow verbs.
[00:02:08.320 --> 00:02:09.680]   And then there are these more and more
[00:02:09.680 --> 00:02:11.840]   and more and more subtle correlations.
[00:02:11.840 --> 00:02:16.120]   And so it kind of makes sense why there would be this,
[00:02:16.120 --> 00:02:19.920]   you know, every log or order of magnitude that you add,
[00:02:19.920 --> 00:02:22.880]   you kind of capture more of the distribution.
[00:02:22.880 --> 00:02:26.840]   What's not clear at all is why is it scale
[00:02:26.840 --> 00:02:28.720]   so smoothly with parameters?
[00:02:28.720 --> 00:02:33.160]   Why does it scale so smoothly with the amount of data?
[00:02:33.160 --> 00:02:35.280]   Why are, you can think up some explanations
[00:02:35.280 --> 00:02:36.240]   of why it's linear.
[00:02:36.240 --> 00:02:38.280]   Like the parameters are like a bucket.
[00:02:38.280 --> 00:02:40.120]   And so the data's like water.
[00:02:40.120 --> 00:02:42.200]   And so size of the bucket is proportional
[00:02:42.200 --> 00:02:43.080]   to the size of the water.
[00:02:43.080 --> 00:02:45.640]   But like, why does it lead to all these,
[00:02:45.640 --> 00:02:47.640]   this very smooth scaling?
[00:02:47.640 --> 00:02:48.800]   I think we still don't know.
[00:02:48.800 --> 00:02:50.240]   There's all these explanations.
[00:02:50.240 --> 00:02:53.280]   Our chief scientist, Gerard Kaplan,
[00:02:53.280 --> 00:02:56.160]   did some stuff on like fractal manifold dimension
[00:02:56.160 --> 00:02:58.000]   that like you can use to explain it.
[00:02:58.000 --> 00:02:59.640]   So there's all kinds of ideas,
[00:02:59.640 --> 00:03:02.720]   but I feel like we just don't really know for sure.
[00:03:02.720 --> 00:03:03.920]   - And by the way, for the audience
[00:03:03.920 --> 00:03:05.400]   who is trying to follow along,
[00:03:05.400 --> 00:03:06.720]   by scaling, we're referring to the fact
[00:03:06.720 --> 00:03:08.800]   that you can very predictably see how,
[00:03:08.800 --> 00:03:10.720]   if you go from GPT-3 to GPT-4,
[00:03:10.720 --> 00:03:12.920]   or in this case, CLAW-1 to CLAW-2,
[00:03:12.920 --> 00:03:14.840]   that the loss in terms of whether it can predict
[00:03:14.840 --> 00:03:16.920]   the next token scales very smoothly.
[00:03:16.920 --> 00:03:18.640]   So, okay, we don't know why it's happening,
[00:03:18.640 --> 00:03:22.240]   but can you at least predict if empirically,
[00:03:22.240 --> 00:03:25.400]   here is the loss at which this ability will emerge.
[00:03:25.400 --> 00:03:28.520]   Here is the place where this circuit will emerge.
[00:03:28.520 --> 00:03:29.680]   Is that at all predictable,
[00:03:29.680 --> 00:03:31.000]   or are you just looking at the loss number?
[00:03:31.000 --> 00:03:32.880]   - That is much less predictable.
[00:03:32.880 --> 00:03:35.040]   What's predictable is this statistical average,
[00:03:35.040 --> 00:03:37.360]   this loss, this entropy, and it's super predictable.
[00:03:37.360 --> 00:03:39.560]   It's like, you know, predictable to like,
[00:03:39.560 --> 00:03:41.720]   sometimes even to several significant figures,
[00:03:41.720 --> 00:03:43.720]   which you don't see outside of physics, right?
[00:03:43.720 --> 00:03:47.520]   You don't expect to see it in this messy empirical field.
[00:03:47.520 --> 00:03:50.080]   But actually, specific abilities are very hard to predict.
[00:03:50.080 --> 00:03:53.560]   So, you know, back when I was working on GPT-2 and GPT-3,
[00:03:53.560 --> 00:03:55.640]   like, when does arithmetic come in place?
[00:03:55.640 --> 00:03:57.400]   When do models learn to code?
[00:03:57.400 --> 00:04:00.560]   Sometimes it's very abrupt.
[00:04:00.560 --> 00:04:02.320]   You know, it's kind of like you can predict
[00:04:02.320 --> 00:04:03.960]   statistical averages of the weather,
[00:04:03.960 --> 00:04:05.480]   but the weather on one particular day
[00:04:05.480 --> 00:04:08.000]   is very hard to predict.
[00:04:08.000 --> 00:04:09.600]   - So, dumb it down for me.
[00:04:09.600 --> 00:04:12.400]   I don't understand manifolds, but mechanistically,
[00:04:12.400 --> 00:04:14.800]   it doesn't know addition yet, now it knows addition.
[00:04:14.800 --> 00:04:16.200]   What has happened?
[00:04:16.200 --> 00:04:18.440]   - This is another question that we don't know the answer to.
[00:04:18.440 --> 00:04:19.680]   I mean, we're trying to answer this
[00:04:19.680 --> 00:04:21.960]   with things like mechanistic interpretability,
[00:04:21.960 --> 00:04:23.960]   but, you know, I'm not sure.
[00:04:23.960 --> 00:04:26.040]   I mean, you can think about these things
[00:04:26.040 --> 00:04:28.160]   about like circuits snapping into place,
[00:04:28.160 --> 00:04:30.160]   although there is some evidence
[00:04:30.160 --> 00:04:33.320]   that when you look at the models being able to add things,
[00:04:33.320 --> 00:04:36.240]   that, you know, like, if you look at its chance
[00:04:36.240 --> 00:04:37.920]   of getting the right answer,
[00:04:37.920 --> 00:04:39.320]   that shoots up all of a sudden.
[00:04:39.320 --> 00:04:40.160]   But if you look at, okay,
[00:04:40.160 --> 00:04:42.000]   what's the probability of the right answer?
[00:04:42.000 --> 00:04:44.160]   You'll see it climb from like one in a million
[00:04:44.160 --> 00:04:46.600]   to one in a hundred thousand to one in a thousand
[00:04:46.600 --> 00:04:49.400]   long before it actually gets the right answer.
[00:04:49.400 --> 00:04:52.640]   And so there's some, in many of these cases, at least,
[00:04:52.640 --> 00:04:53.840]   I don't know if in all of them,
[00:04:53.840 --> 00:04:56.520]   there's some continuous process going on behind the scenes.
[00:04:56.520 --> 00:04:57.800]   I don't understand it at all.
[00:04:57.800 --> 00:05:00.200]   - Does that imply that the circuit or the process
[00:05:00.200 --> 00:05:02.120]   for doing addition was preexisting
[00:05:02.120 --> 00:05:04.760]   and it just got increased in salient?
[00:05:04.760 --> 00:05:06.720]   - I don't know if like there's this circuit
[00:05:06.720 --> 00:05:08.440]   that's weak and getting stronger.
[00:05:08.440 --> 00:05:09.880]   I don't know if it's something that works,
[00:05:09.880 --> 00:05:11.160]   but not very well.
[00:05:11.160 --> 00:05:14.040]   Like, I think we don't know.
[00:05:14.040 --> 00:05:15.720]   And these are some of the questions we're trying to answer
[00:05:15.720 --> 00:05:17.040]   with mechanistic interpretability.
[00:05:17.040 --> 00:05:19.560]   - Are there abilities that won't emerge with scale?
[00:05:19.560 --> 00:05:21.920]   - So I definitely think that, again,
[00:05:21.920 --> 00:05:24.240]   like things like alignment and values
[00:05:24.240 --> 00:05:26.400]   are not guaranteed to emerge with scale, right?
[00:05:26.400 --> 00:05:28.120]   It's kind of like, you know,
[00:05:28.120 --> 00:05:31.360]   one way to think about it is you train the model
[00:05:31.360 --> 00:05:35.600]   and it is basically, it's like predicting the world.
[00:05:35.600 --> 00:05:36.560]   It's understanding the world.
[00:05:36.560 --> 00:05:38.880]   It's job is facts, not values, right?
[00:05:38.880 --> 00:05:40.760]   It's trying to predict what comes next.
[00:05:40.760 --> 00:05:43.420]   But there's just, there's free variables here
[00:05:43.420 --> 00:05:45.640]   where it's like, what should you do?
[00:05:45.640 --> 00:05:46.480]   What should you think?
[00:05:46.480 --> 00:05:47.720]   What should you value?
[00:05:47.720 --> 00:05:50.080]   Those, you know, like they're just,
[00:05:50.080 --> 00:05:51.400]   there aren't the bits for that.
[00:05:51.400 --> 00:05:53.920]   There's just like, well, if I started with this,
[00:05:53.920 --> 00:05:54.860]   I should finish with this.
[00:05:54.860 --> 00:05:56.080]   If I started with this other thing,
[00:05:56.080 --> 00:05:57.720]   I should finish with this other thing.
[00:05:57.720 --> 00:06:01.240]   And so I think that's not going to emerge.
[00:06:01.240 --> 00:06:02.280]   - I wanna talk about alignment in a second,
[00:06:02.280 --> 00:06:05.760]   but on scaling, if it turns out that scaling plateaus
[00:06:05.760 --> 00:06:08.160]   before we reach human level intelligence,
[00:06:08.160 --> 00:06:10.280]   looking back on it, what would be your explanation?
[00:06:10.280 --> 00:06:11.840]   What do you think is likely to be the case
[00:06:11.840 --> 00:06:13.440]   if that turns out to be the outcome?
[00:06:13.440 --> 00:06:16.560]   - Yeah, so I guess I would distinguish some problem
[00:06:16.560 --> 00:06:19.080]   with the fundamental theory with some practical issue.
[00:06:19.080 --> 00:06:21.240]   So one practical issue we could have
[00:06:21.240 --> 00:06:22.800]   is we could run out of data.
[00:06:22.800 --> 00:06:25.280]   For various reasons, I think that's not going to happen.
[00:06:25.280 --> 00:06:28.640]   But, you know, if you look at it very, very naively,
[00:06:28.640 --> 00:06:30.400]   we're not that far from running out of data.
[00:06:30.400 --> 00:06:32.120]   And so it's like, we just don't have the data
[00:06:32.120 --> 00:06:34.700]   to continue the scaling curves.
[00:06:34.700 --> 00:06:36.800]   I think, you know, another way it could happen is like,
[00:06:36.800 --> 00:06:39.880]   oh, we just use up all of our compute that was available
[00:06:39.880 --> 00:06:43.160]   and that wasn't enough and then progress is slow after that.
[00:06:43.160 --> 00:06:44.840]   I wouldn't bet on either of those things happening,
[00:06:44.840 --> 00:06:46.400]   but they could.
[00:06:46.400 --> 00:06:50.840]   I think from a fundamental perspective,
[00:06:50.840 --> 00:06:52.840]   personally, I think it's very unlikely
[00:06:52.840 --> 00:06:54.980]   that the scaling laws will just stop.
[00:06:54.980 --> 00:06:57.360]   If they do, another reason, again,
[00:06:57.360 --> 00:06:59.280]   this isn't fully fundamental,
[00:06:59.280 --> 00:07:01.840]   could just be we don't have quite the right architecture.
[00:07:01.840 --> 00:07:05.020]   Like if we tried to do it with an LSDM or an RNN,
[00:07:05.020 --> 00:07:06.540]   the slope would be different.
[00:07:06.540 --> 00:07:08.380]   I still might be that we get there,
[00:07:08.380 --> 00:07:09.960]   but I think there are some things
[00:07:09.960 --> 00:07:11.600]   that are just very hard to represent
[00:07:11.600 --> 00:07:15.400]   when you don't have this ability to attend far in the past
[00:07:15.400 --> 00:07:17.220]   that transformers have.
[00:07:17.220 --> 00:07:20.280]   If somehow, and I don't know how we would know this,
[00:07:20.280 --> 00:07:22.000]   it kind of wasn't about the architecture
[00:07:22.000 --> 00:07:23.380]   and we just hit a wall,
[00:07:23.380 --> 00:07:25.640]   I think I'd be very surprised by that.
[00:07:25.640 --> 00:07:27.000]   I think we're already at the point
[00:07:27.000 --> 00:07:29.880]   where the things the models can't do
[00:07:29.880 --> 00:07:31.960]   don't seem to me to be different in kind
[00:07:31.960 --> 00:07:33.560]   from the things they can do.
[00:07:33.560 --> 00:07:36.920]   And it just, you know, you could have made a case
[00:07:36.920 --> 00:07:38.820]   a few years ago that it was like,
[00:07:38.820 --> 00:07:40.980]   they can't reason, they can't program.
[00:07:40.980 --> 00:07:44.620]   Like you could have drawn boundaries and said,
[00:07:44.620 --> 00:07:46.620]   well, maybe you'll hit a wall.
[00:07:46.620 --> 00:07:48.740]   I didn't think that, I didn't think we would hit a wall.
[00:07:48.740 --> 00:07:50.660]   Few other people didn't think we would hit a wall,
[00:07:50.660 --> 00:07:52.220]   but it was a more plausible case then.
[00:07:52.220 --> 00:07:53.780]   I think it's a less plausible case now.
[00:07:53.780 --> 00:07:56.540]   Now it could happen, like this stuff is crazy.
[00:07:56.540 --> 00:07:58.740]   Like it could happen tomorrow
[00:07:58.740 --> 00:08:00.980]   that it's just like we hit a wall.
[00:08:00.980 --> 00:08:02.500]   I think if that happens,
[00:08:02.500 --> 00:08:04.200]   I'm trying to think of like what's my,
[00:08:04.200 --> 00:08:06.040]   what would really be my, it's unlikely,
[00:08:06.040 --> 00:08:08.180]   but what would really be my explanation?
[00:08:08.180 --> 00:08:10.580]   I think my explanation would be
[00:08:10.580 --> 00:08:12.620]   there's something wrong with the loss
[00:08:12.620 --> 00:08:15.260]   when you train on next word prediction.
[00:08:15.260 --> 00:08:18.940]   Like some of the remaining like reasoning abilities
[00:08:18.940 --> 00:08:19.780]   or something like that,
[00:08:19.780 --> 00:08:21.020]   like if you really want to learn,
[00:08:21.020 --> 00:08:23.200]   you know, it's a program at a really high level,
[00:08:23.200 --> 00:08:25.980]   like it means you care about some tokens
[00:08:25.980 --> 00:08:27.500]   much more than others.
[00:08:27.500 --> 00:08:30.520]   And they're rare enough that it's like the loss function
[00:08:30.520 --> 00:08:34.860]   over focuses on kind of the appearance,
[00:08:34.860 --> 00:08:36.040]   the things that are responsible
[00:08:36.040 --> 00:08:38.300]   for the most bits of entropy.
[00:08:38.300 --> 00:08:39.700]   And instead, you know,
[00:08:39.700 --> 00:08:41.860]   they don't focus on this stuff that's really essential.
[00:08:41.860 --> 00:08:43.860]   And so you could kind of have the signal
[00:08:43.860 --> 00:08:45.420]   drowned out in the noise.
[00:08:45.420 --> 00:08:46.940]   I don't think it's going to play out that way
[00:08:46.940 --> 00:08:47.940]   for a number of reasons.
[00:08:47.940 --> 00:08:51.620]   But if you told me, yep, you trained your 2024 model,
[00:08:51.620 --> 00:08:53.900]   it was much bigger and it just wasn't any better
[00:08:53.900 --> 00:08:56.780]   and you tried every architecture and it didn't work,
[00:08:56.780 --> 00:08:59.140]   I think that's the explanation I would reach for.
[00:08:59.140 --> 00:09:01.080]   - Is there a candidate for another loss function
[00:09:01.080 --> 00:09:03.140]   if you had to abandon next token prediction?
[00:09:03.140 --> 00:09:06.460]   - I think then you would have to go for some kind of RL.
[00:09:06.460 --> 00:09:07.300]   And again, there's, you know,
[00:09:07.300 --> 00:09:08.140]   there's many different kinds.
[00:09:08.140 --> 00:09:09.540]   There's RL from human feedback.
[00:09:09.540 --> 00:09:11.700]   There's RL against an objective.
[00:09:11.700 --> 00:09:13.520]   There's things like constitutional AI.
[00:09:13.520 --> 00:09:16.080]   There's things like amplification and debate, right?
[00:09:16.080 --> 00:09:18.180]   These are kind of both alignment methods
[00:09:18.180 --> 00:09:20.100]   and ways of training models.
[00:09:20.100 --> 00:09:21.460]   You would have to try a bunch of things,
[00:09:21.460 --> 00:09:23.380]   but the focus would have to be on
[00:09:23.380 --> 00:09:25.660]   what do we actually care about the model doing, right?
[00:09:25.660 --> 00:09:27.140]   And in a sense, we're a little bit lucky
[00:09:27.140 --> 00:09:30.100]   that it's like predict the next word gets us
[00:09:30.100 --> 00:09:31.260]   all these other things we need.
[00:09:31.260 --> 00:09:32.580]   - Right. - There's no guarantee.
[00:09:32.580 --> 00:09:33.940]   - It seems like from your worldview,
[00:09:33.940 --> 00:09:36.780]   there's a multitude of different loss functions
[00:09:36.780 --> 00:09:38.980]   that it's just a matter of what can allow you
[00:09:38.980 --> 00:09:40.740]   to just throw a whole bunch of data at it.
[00:09:40.740 --> 00:09:43.060]   Like the next token prediction itself is not significant.
[00:09:43.060 --> 00:09:45.440]   - Yeah, well, I mean, I guess the thing with RL
[00:09:45.440 --> 00:09:47.760]   is you get slowed down a bit because it's like,
[00:09:47.760 --> 00:09:50.200]   you know, you have to, by some method,
[00:09:50.200 --> 00:09:52.800]   kind of, you know, design how the loss function works.
[00:09:52.800 --> 00:09:54.660]   Nice thing with the next token prediction
[00:09:54.660 --> 00:09:56.480]   is it's there for you, right?
[00:09:56.480 --> 00:09:58.540]   It's just there, it's the easiest thing in the world.
[00:09:58.540 --> 00:10:00.080]   And so I think it would slow you down
[00:10:00.080 --> 00:10:02.780]   if you couldn't scale in just that very simplest way.
[00:10:02.780 --> 00:10:04.620]   - You mentioned that the data
[00:10:04.620 --> 00:10:06.220]   is likely not to be the constraint.
[00:10:06.220 --> 00:10:07.940]   Why do you think that is the case?
[00:10:07.940 --> 00:10:09.500]   - There's various possibilities here.
[00:10:09.500 --> 00:10:10.700]   And, you know, for a number of reasons,
[00:10:10.700 --> 00:10:12.060]   I shouldn't go into the details,
[00:10:12.060 --> 00:10:15.400]   but you know, like there's many sources of data in the world
[00:10:15.400 --> 00:10:18.180]   and there's many ways that you can also generate data.
[00:10:18.180 --> 00:10:21.420]   My guess is that this will not be a blocker.
[00:10:21.420 --> 00:10:24.260]   Maybe it'd be better if it was, but it won't be.
[00:10:24.260 --> 00:10:25.940]   - Are you talking about multimodal or?
[00:10:25.940 --> 00:10:28.180]   - There's just many different ways to do it.
[00:10:28.180 --> 00:10:30.480]   - How did you form your views on scaling?
[00:10:30.480 --> 00:10:31.680]   How far back can we go?
[00:10:31.680 --> 00:10:33.220]   And then you would be basically saying
[00:10:33.220 --> 00:10:34.680]   something similar to this.
[00:10:34.680 --> 00:10:37.360]   - This view that I have probably formed gradually
[00:10:37.360 --> 00:10:41.540]   from, I would say like 2014 to 2017.
[00:10:41.540 --> 00:10:43.800]   So I think my first experience with it
[00:10:43.800 --> 00:10:46.600]   was my first experience with AI.
[00:10:46.600 --> 00:10:48.280]   So I, you know, I saw some of the early stuff
[00:10:48.280 --> 00:10:50.320]   around AlexNet in 2012.
[00:10:50.320 --> 00:10:52.500]   Always kind of wanted to study intelligence,
[00:10:52.500 --> 00:10:54.400]   but I, you know, before I was just like,
[00:10:54.400 --> 00:10:55.680]   this isn't really working.
[00:10:55.680 --> 00:10:58.180]   Like it doesn't seem like it's actually working.
[00:10:58.180 --> 00:11:00.740]   You know, all the way back to like 2005,
[00:11:00.740 --> 00:11:03.420]   I'd like, you know, I'd read Ray Kurzweil's work.
[00:11:03.420 --> 00:11:06.300]   You know, I'd read even some of like Eliezer's work
[00:11:06.300 --> 00:11:08.260]   on the early internet back then.
[00:11:08.260 --> 00:11:10.540]   And I was like, oh, this stuff kind of looks far away.
[00:11:10.540 --> 00:11:12.100]   Like I look at the AI stuff of today
[00:11:12.100 --> 00:11:14.980]   and it's like not anywhere close.
[00:11:14.980 --> 00:11:16.660]   But with AlexNet, I was like, oh,
[00:11:16.660 --> 00:11:18.340]   this stuff is actually starting to work.
[00:11:18.340 --> 00:11:22.820]   So I joined Andrew Ng's group initially at Baidu.
[00:11:22.820 --> 00:11:26.160]   And the first task, you know, that I got set to do, right,
[00:11:26.160 --> 00:11:28.480]   it was my, you know, I'd been in a different field.
[00:11:28.480 --> 00:11:31.280]   And so I first joined, you know,
[00:11:31.280 --> 00:11:34.320]   this was my first experience with AI.
[00:11:34.320 --> 00:11:35.560]   And it was a bit different
[00:11:35.560 --> 00:11:38.000]   from a lot of the kind of academic style research
[00:11:38.000 --> 00:11:41.720]   that was going on kind of elsewhere in the world, right?
[00:11:41.720 --> 00:11:45.360]   I think I kind of got lucky in that the task
[00:11:45.360 --> 00:11:47.080]   that was given to me and the other folks there
[00:11:47.080 --> 00:11:50.400]   was just make the best speech recognition system
[00:11:50.400 --> 00:11:51.360]   that you can.
[00:11:51.360 --> 00:11:53.160]   And there was a lot of data available.
[00:11:53.160 --> 00:11:55.080]   There were a lot of GPUs available.
[00:11:55.080 --> 00:11:58.420]   So it kind of, it posed the problem
[00:11:58.420 --> 00:12:00.420]   in a way that was amenable to discovering
[00:12:00.420 --> 00:12:02.380]   that kind of scaling was a solution, right?
[00:12:02.380 --> 00:12:05.060]   That's very different from like, you're a postdoc
[00:12:05.060 --> 00:12:07.340]   and it's your job to come up with, you know,
[00:12:07.340 --> 00:12:09.260]   what's the best, like, you know,
[00:12:09.260 --> 00:12:12.100]   what's an idea that seems clever and new
[00:12:12.100 --> 00:12:15.100]   and makes your mark as someone who's invented something.
[00:12:15.100 --> 00:12:18.140]   And so I just quickly discovered that like,
[00:12:18.140 --> 00:12:20.140]   you know, I was just trying the simplest experiments.
[00:12:20.140 --> 00:12:22.360]   I was like, you know, just fiddling with some dials.
[00:12:22.360 --> 00:12:25.380]   I was like, okay, try, you know,
[00:12:25.380 --> 00:12:27.580]   try adding more layers to the,
[00:12:27.580 --> 00:12:30.260]   literally add more layers to the RNN, you know,
[00:12:30.260 --> 00:12:31.840]   try training it for longer.
[00:12:31.840 --> 00:12:32.680]   What happens?
[00:12:32.680 --> 00:12:34.020]   How long does it take to overfit?
[00:12:34.020 --> 00:12:36.700]   What if I add new data and repeat it less times?
[00:12:36.700 --> 00:12:39.940]   And like, I just saw these like very consistent patterns.
[00:12:39.940 --> 00:12:43.700]   I didn't really know that this was unusual
[00:12:43.700 --> 00:12:45.540]   or that others weren't thinking in this way.
[00:12:45.540 --> 00:12:48.560]   This was just kind of like, almost like beginner's luck.
[00:12:48.560 --> 00:12:50.820]   It was my first experience with it.
[00:12:50.820 --> 00:12:52.180]   And I didn't really think about it
[00:12:52.180 --> 00:12:54.740]   beyond speech recognition, right, right, right.
[00:12:54.740 --> 00:12:56.700]   You know, I was just kind of like, oh, this is,
[00:12:56.700 --> 00:12:58.300]   you know, I don't know anything about this field.
[00:12:58.300 --> 00:13:00.380]   There's zillions of things people do with machine learning,
[00:13:00.380 --> 00:13:01.780]   but like, I'm like, weird,
[00:13:01.780 --> 00:13:04.700]   this seems to be true in the speech recognition field.
[00:13:04.700 --> 00:13:07.780]   And then I think it was recently, you know,
[00:13:07.780 --> 00:13:11.140]   like just before OpenAI started
[00:13:11.140 --> 00:13:13.620]   that I met Ilya, who you interviewed.
[00:13:13.620 --> 00:13:15.540]   One of the first things he said to me was,
[00:13:15.540 --> 00:13:17.060]   look, the models, they just want to learn.
[00:13:17.060 --> 00:13:17.940]   You have to understand this.
[00:13:17.940 --> 00:13:19.540]   The models, they just want to learn.
[00:13:19.540 --> 00:13:21.300]   And it was a bit like a Zen Cohen.
[00:13:21.300 --> 00:13:23.220]   Like, I kind of like, I listened to this
[00:13:23.220 --> 00:13:24.680]   and I became enlightened.
[00:13:24.680 --> 00:13:27.680]   (laughing)
[00:13:27.680 --> 00:13:31.700]   And, you know, over the years after this,
[00:13:31.700 --> 00:13:34.940]   you know, again, I would be kind of, you know,
[00:13:34.940 --> 00:13:37.620]   the one who would formalize a lot of these things
[00:13:37.620 --> 00:13:38.660]   and kind of put them together.
[00:13:38.660 --> 00:13:40.740]   But like, just kind of the,
[00:13:40.740 --> 00:13:43.780]   what that told me is that that phenomenon
[00:13:43.780 --> 00:13:46.500]   that I'd seen wasn't just some random thing that I'd seen.
[00:13:46.500 --> 00:13:47.540]   It was like, it was broad.
[00:13:47.540 --> 00:13:49.180]   It was more general, right?
[00:13:49.180 --> 00:13:51.260]   The models, the models just want to learn.
[00:13:51.260 --> 00:13:53.460]   You get the obstacles out of their way, right?
[00:13:53.460 --> 00:13:55.740]   You give them good data.
[00:13:55.740 --> 00:13:59.240]   You give them enough space to operate in.
[00:13:59.240 --> 00:14:00.660]   You don't do something stupid
[00:14:00.660 --> 00:14:03.100]   like condition them badly numerically.
[00:14:03.100 --> 00:14:04.420]   And they want to learn.
[00:14:04.420 --> 00:14:05.240]   They'll do it.
[00:14:05.240 --> 00:14:06.080]   They'll do it.
[00:14:06.080 --> 00:14:07.820]   - You know, what I find really interesting
[00:14:07.820 --> 00:14:10.460]   about what you said is there were many people
[00:14:10.460 --> 00:14:12.900]   who were aware back at that time,
[00:14:12.900 --> 00:14:14.300]   probably weren't working on it directly,
[00:14:14.300 --> 00:14:16.140]   but were aware that these things are really good
[00:14:16.140 --> 00:14:21.140]   at speech recognition or at playing these constrained games.
[00:14:21.140 --> 00:14:25.420]   Very few extrapolated from there like you and Ilya did
[00:14:25.420 --> 00:14:27.860]   to something that is generally intelligent.
[00:14:27.860 --> 00:14:30.020]   What was different about the way you were thinking about it
[00:14:30.020 --> 00:14:32.340]   versus how others think that you went from like,
[00:14:32.340 --> 00:14:34.260]   is getting better at speech in this consistent way?
[00:14:34.260 --> 00:14:36.260]   It will get better at everything in this consistent way.
[00:14:36.260 --> 00:14:38.660]   - Yeah, so I genuinely don't know.
[00:14:38.660 --> 00:14:40.340]   I mean, at first when I saw it for speech,
[00:14:40.340 --> 00:14:42.660]   I assumed this was just true for speech
[00:14:42.660 --> 00:14:45.180]   or for this narrow class of models.
[00:14:45.180 --> 00:14:49.860]   I think it was just over the period between 2014 and 2017,
[00:14:49.860 --> 00:14:51.500]   I tried it for a lot of things
[00:14:51.500 --> 00:14:53.580]   and saw the same thing over and over again.
[00:14:53.580 --> 00:14:56.740]   I watched the same being true with Dota.
[00:14:56.740 --> 00:14:59.220]   I watched the same being true with robotics,
[00:14:59.220 --> 00:15:01.180]   which many people thought of as a counter example,
[00:15:01.180 --> 00:15:03.980]   but I just thought, well, it's hard to get data for robotics
[00:15:03.980 --> 00:15:07.300]   but if we look within the data that we have,
[00:15:07.300 --> 00:15:08.980]   we see the same patterns.
[00:15:08.980 --> 00:15:10.780]   And so I don't know.
[00:15:10.780 --> 00:15:13.220]   I think people were very focused
[00:15:13.220 --> 00:15:15.580]   on solving the problem in front of them.
[00:15:15.580 --> 00:15:18.060]   Why one person thinks one way and other person thinks,
[00:15:18.060 --> 00:15:20.700]   it's very hard to explain.
[00:15:20.700 --> 00:15:23.980]   I think people just see it through a different lens,
[00:15:23.980 --> 00:15:26.100]   are looking like vertically instead of horizontally.
[00:15:26.100 --> 00:15:27.700]   They're not thinking about the scaling,
[00:15:27.700 --> 00:15:29.620]   they're thinking about how do I solve my problem?
[00:15:29.620 --> 00:15:31.980]   And well, for robotics, there's not enough data.
[00:15:31.980 --> 00:15:36.820]   And so that can easily abstract to,
[00:15:36.820 --> 00:15:39.260]   well, scaling doesn't work 'cause we don't have the data.
[00:15:39.260 --> 00:15:42.940]   And so I don't know, I just, for some reason,
[00:15:42.940 --> 00:15:44.900]   and it may just have been random chance,
[00:15:44.900 --> 00:15:47.580]   was obsessed with that particular direction.
[00:15:47.580 --> 00:15:49.980]   - When did it become obvious to you
[00:15:49.980 --> 00:15:52.100]   that language is the means
[00:15:52.100 --> 00:15:54.820]   to just feed a bunch of data into these things?
[00:15:54.820 --> 00:15:56.300]   Or was it just, you ran out of other things,
[00:15:56.300 --> 00:15:57.500]   like robotics, there's not enough data,
[00:15:57.500 --> 00:15:58.940]   this other thing, there's not enough data.
[00:15:58.940 --> 00:16:01.100]   - Yeah, I mean, I think this whole idea
[00:16:01.100 --> 00:16:02.580]   of like the next word prediction
[00:16:02.580 --> 00:16:05.220]   that you could do self-supervised learning,
[00:16:05.220 --> 00:16:07.900]   that together with the idea that it's like,
[00:16:07.900 --> 00:16:09.740]   wow, for predicting the next word,
[00:16:09.740 --> 00:16:12.500]   there's so much richness and structure there, right?
[00:16:12.500 --> 00:16:13.980]   It might say two plus two equals,
[00:16:13.980 --> 00:16:15.780]   and you have to know the answer is four.
[00:16:15.780 --> 00:16:18.340]   And it might be telling the story about a character,
[00:16:18.340 --> 00:16:21.540]   and then basically it's posing to the model,
[00:16:21.540 --> 00:16:23.260]   the equivalent of these developmental tests
[00:16:23.260 --> 00:16:24.460]   that get posed to children.
[00:16:24.460 --> 00:16:27.580]   Mary walks into the room and puts an item in there,
[00:16:27.580 --> 00:16:30.460]   and then Chuck walks into the room and removes the item,
[00:16:30.460 --> 00:16:31.660]   and Mary doesn't see it.
[00:16:31.660 --> 00:16:34.140]   What does Mary think, half, so like,
[00:16:34.140 --> 00:16:36.060]   so the models are gonna have to get this right
[00:16:36.060 --> 00:16:38.300]   in the service of predicting the next word,
[00:16:38.300 --> 00:16:40.980]   they're gonna have to solve all these
[00:16:40.980 --> 00:16:43.820]   theory of mind problems, solve all these math problems.
[00:16:43.820 --> 00:16:46.180]   And so my thinking was just,
[00:16:46.180 --> 00:16:48.660]   well, you scale it up as much as you can,
[00:16:48.660 --> 00:16:51.820]   there's kind of no limit to it.
[00:16:51.820 --> 00:16:54.860]   And I think I kind of had abstractly that view,
[00:16:54.860 --> 00:16:58.260]   but the thing, of course, that like really solidified
[00:16:58.260 --> 00:17:02.860]   and convinced me was the work that Alec Radford did on GPT-1,
[00:17:02.860 --> 00:17:05.620]   which was not only could you get this language model
[00:17:05.620 --> 00:17:07.380]   that could predict things very well,
[00:17:07.380 --> 00:17:09.020]   but also you could fine tune it,
[00:17:09.020 --> 00:17:10.820]   you needed to fine tune it in those days
[00:17:10.820 --> 00:17:12.060]   to do all these other tasks.
[00:17:12.060 --> 00:17:16.020]   And so I was like, wow, this isn't just some narrow thing
[00:17:16.020 --> 00:17:18.220]   where you get the language model right,
[00:17:18.220 --> 00:17:20.100]   it's sort of halfway to everywhere, right?
[00:17:20.100 --> 00:17:22.540]   It's like, you get the language model right,
[00:17:22.540 --> 00:17:25.180]   and then with a little move in this direction,
[00:17:25.180 --> 00:17:29.960]   it can solve this logical dereference test or whatever,
[00:17:29.960 --> 00:17:32.180]   and with this other thing,
[00:17:32.180 --> 00:17:33.740]   it can solve translation or something.
[00:17:33.740 --> 00:17:35.020]   And then you're like, wow,
[00:17:35.020 --> 00:17:36.660]   I think there's really something to do it,
[00:17:36.660 --> 00:17:39.180]   and of course we can really scale it.
[00:17:39.180 --> 00:17:40.580]   - Well, one thing that's confusing
[00:17:40.580 --> 00:17:42.580]   or that would have been hard to see,
[00:17:42.580 --> 00:17:44.860]   if you told me in 2018,
[00:17:44.860 --> 00:17:47.740]   we'll have models in 2023, like law two,
[00:17:47.740 --> 00:17:50.540]   that can write theorems in the style of Shakespeare,
[00:17:50.540 --> 00:17:52.260]   whatever theory you want,
[00:17:52.260 --> 00:17:56.140]   they can A standardized tests with open-ended questions,
[00:17:56.140 --> 00:18:00.020]   just all kinds of really impressive things.
[00:18:00.020 --> 00:18:02.540]   You would have said at that time, I would have said,
[00:18:02.540 --> 00:18:03.500]   oh, you have AGI,
[00:18:03.500 --> 00:18:04.340]   you clearly have something
[00:18:04.340 --> 00:18:05.860]   that is a human level intelligence,
[00:18:05.860 --> 00:18:07.860]   where while these things are impressive,
[00:18:07.860 --> 00:18:10.380]   it clearly seems we're not at human level,
[00:18:10.380 --> 00:18:11.860]   at least in the current generation
[00:18:11.860 --> 00:18:14.220]   and potentially for generations to come.
[00:18:14.220 --> 00:18:15.860]   What explains this discrepancy
[00:18:15.860 --> 00:18:19.100]   between super impressive performance in these benchmarks
[00:18:19.100 --> 00:18:21.540]   and in just like the things you could describe
[00:18:21.540 --> 00:18:22.780]   versus generally?
[00:18:22.780 --> 00:18:24.340]   - So that was one area where actually
[00:18:24.340 --> 00:18:27.320]   I was not pressing and I was surprised as well.
[00:18:27.320 --> 00:18:30.100]   So when I first looked at GPT-3
[00:18:30.100 --> 00:18:31.780]   and more so the kind of things
[00:18:31.780 --> 00:18:35.260]   that we built in the early days at Anthropic,
[00:18:35.260 --> 00:18:37.940]   my general sense was, I looked at these
[00:18:37.940 --> 00:18:40.940]   and I'm like, it seems like they really grasped
[00:18:40.940 --> 00:18:42.780]   the essence of language.
[00:18:42.780 --> 00:18:44.700]   I'm not sure how much we need to scale them up.
[00:18:44.700 --> 00:18:48.700]   Like maybe what's more needed from here is like RL
[00:18:48.700 --> 00:18:50.580]   and kind of all the other stuff.
[00:18:50.580 --> 00:18:53.140]   Like we might be kind of near the,
[00:18:53.140 --> 00:18:56.820]   I thought in 2020, like we can scale this a bunch more,
[00:18:56.820 --> 00:18:59.060]   but I wonder if it's more efficient to scale it more
[00:18:59.060 --> 00:19:02.580]   or to start adding on these other objectives like RL.
[00:19:02.580 --> 00:19:04.980]   I thought maybe if you do as much RL
[00:19:04.980 --> 00:19:09.980]   as you've done pre-training for a 2020 style model
[00:19:09.980 --> 00:19:12.780]   that that's the way to go
[00:19:12.780 --> 00:19:14.380]   and scaling it up, we'll keep working,
[00:19:14.380 --> 00:19:17.460]   but is that really the best path?
[00:19:17.460 --> 00:19:20.700]   And I think it, I don't know, it just keeps going.
[00:19:20.700 --> 00:19:22.380]   Like I thought it had understood
[00:19:22.380 --> 00:19:24.340]   a lot of the essence of language,
[00:19:24.340 --> 00:19:28.460]   but then there's kind of further to go.
[00:19:28.460 --> 00:19:31.740]   And so I don't know, stepping back from it,
[00:19:31.740 --> 00:19:36.260]   like one of the reasons why I'm sort of very empiricist
[00:19:36.260 --> 00:19:40.740]   about AI, about safety, about organizations
[00:19:40.740 --> 00:19:43.460]   is that you often get surprised, right?
[00:19:43.460 --> 00:19:46.860]   I feel like I've been right about some things,
[00:19:46.860 --> 00:19:50.020]   but I've still, with these theoretical pictures ahead,
[00:19:50.020 --> 00:19:51.580]   been wrong about most things.
[00:19:51.580 --> 00:19:53.220]   Being right about 10% of the stuff
[00:19:53.220 --> 00:19:58.220]   is sets you head and shoulders above many people.
[00:19:58.220 --> 00:20:01.460]   If you look back to, I can't remember who it was,
[00:20:01.460 --> 00:20:04.660]   kind of made these diagrams that are like,
[00:20:04.660 --> 00:20:07.740]   here's the village idiot, here's Einstein,
[00:20:07.740 --> 00:20:09.420]   here's the scale of intelligence, right?
[00:20:09.420 --> 00:20:11.140]   And the village idiot and Einstein
[00:20:11.140 --> 00:20:13.380]   are like very close to each other.
[00:20:13.380 --> 00:20:15.100]   - Like that, maybe that's still true
[00:20:15.100 --> 00:20:16.900]   in some abstract sense or something,
[00:20:16.900 --> 00:20:19.540]   but it's not really what we're seeing, is it?
[00:20:19.540 --> 00:20:22.380]   We're seeing like, that it seems like the human range
[00:20:22.380 --> 00:20:25.580]   is pretty broad and doesn't,
[00:20:25.580 --> 00:20:29.060]   we don't hit the human range in the same place
[00:20:29.060 --> 00:20:31.540]   or at the same time for different tasks, right?
[00:20:31.540 --> 00:20:36.260]   Like write a sonnet in the style of Cormac McCarthy
[00:20:36.260 --> 00:20:38.860]   or something, like, I don't know, I'm not very creative,
[00:20:38.860 --> 00:20:40.020]   so I couldn't do that.
[00:20:40.020 --> 00:20:43.660]   But like, that's a pretty high level human skill, right?
[00:20:43.660 --> 00:20:45.980]   And even the model is starting to get good at stuff
[00:20:45.980 --> 00:20:48.620]   of like constrained writing.
[00:20:48.620 --> 00:20:51.700]   There's this like, write a page without using the letter E
[00:20:51.700 --> 00:20:53.020]   or something, write a page about X
[00:20:53.020 --> 00:20:54.260]   without using the letter E.
[00:20:54.260 --> 00:20:57.020]   Like, I think the models might be like superhuman
[00:20:57.020 --> 00:20:59.580]   or close to superhuman at that.
[00:20:59.580 --> 00:21:03.460]   But when it comes to, yeah, I don't know,
[00:21:03.460 --> 00:21:06.900]   prove relatively simple mathematical theorems,
[00:21:06.900 --> 00:21:10.140]   like, they're just starting to do the beginning of it.
[00:21:10.140 --> 00:21:13.300]   They make really dumb mistakes sometimes
[00:21:13.300 --> 00:21:16.500]   and they really lack any kind of broad,
[00:21:16.500 --> 00:21:21.500]   like correcting your errors or doing some extended task.
[00:21:21.500 --> 00:21:22.780]   And so, I don't know,
[00:21:22.780 --> 00:21:26.620]   it turns out that intelligence isn't a spectrum.
[00:21:26.620 --> 00:21:29.820]   There are a bunch of different areas of domain expertise.
[00:21:29.820 --> 00:21:32.180]   There are a bunch of different like kinds of skills,
[00:21:32.180 --> 00:21:33.060]   like memory is different.
[00:21:33.060 --> 00:21:35.700]   I mean, it's all formed in the blob.
[00:21:35.700 --> 00:21:37.540]   It's not, it's all formed in the blob.
[00:21:37.540 --> 00:21:38.500]   It's not complicated,
[00:21:38.500 --> 00:21:40.980]   but to the extent it even is on the spectrum,
[00:21:40.980 --> 00:21:42.620]   the spectrum is also wide.
[00:21:42.620 --> 00:21:43.820]   If you asked me 10 years ago,
[00:21:43.820 --> 00:21:45.580]   that's not what I would have expected at all,
[00:21:45.580 --> 00:21:48.460]   but I think that's very much the way it's turned out.
[00:21:48.460 --> 00:21:49.900]   - Oh, man, I have so many questions
[00:21:49.900 --> 00:21:51.580]   just as follow up on that.
[00:21:51.580 --> 00:21:55.740]   One is, do you expect that given the distribution
[00:21:55.740 --> 00:21:57.780]   of training that these models get
[00:21:57.780 --> 00:22:00.060]   from massive amounts of internet data
[00:22:00.060 --> 00:22:02.460]   versus what humans got from evolution,
[00:22:02.460 --> 00:22:04.940]   that the repertoire of skills that elicits
[00:22:04.940 --> 00:22:07.780]   will be just barely overlapping?
[00:22:07.780 --> 00:22:09.700]   It will be like concentric circles?
[00:22:09.700 --> 00:22:12.740]   How do you think about, do those matter?
[00:22:12.740 --> 00:22:15.460]   - Clearly, there's certainly a large amount of overlap,
[00:22:15.460 --> 00:22:17.020]   right, because a lot of the, you know,
[00:22:17.020 --> 00:22:19.980]   like these models have business applications
[00:22:19.980 --> 00:22:22.020]   and many of their business applications are doing things
[00:22:22.020 --> 00:22:23.700]   that, you know, are helping humans
[00:22:23.700 --> 00:22:25.620]   to be more effective at things.
[00:22:25.620 --> 00:22:28.620]   So the overlap is quite large.
[00:22:28.620 --> 00:22:30.780]   And, you know, if you think of all the activity
[00:22:30.780 --> 00:22:32.540]   that humans put on the internet in tax,
[00:22:32.540 --> 00:22:33.820]   that covers a lot of it,
[00:22:33.820 --> 00:22:35.500]   but it probably doesn't cover some things.
[00:22:35.500 --> 00:22:37.700]   Like the models, I think they do learn
[00:22:37.700 --> 00:22:39.500]   a physical model of the world to some extent,
[00:22:39.500 --> 00:22:40.460]   but they certainly don't learn
[00:22:40.460 --> 00:22:42.700]   how to actually move around in the world.
[00:22:42.700 --> 00:22:44.820]   Again, maybe that's easy to fine tune,
[00:22:44.820 --> 00:22:48.180]   but, you know, I think, so I think there are some things
[00:22:48.180 --> 00:22:51.420]   that the models don't learn that humans do.
[00:22:51.420 --> 00:22:54.900]   And then I think, you know, the models learn, for example,
[00:22:54.900 --> 00:22:56.500]   to speak fluent base 64.
[00:22:56.500 --> 00:22:58.900]   I don't know about you, but I never learned that.
[00:22:58.900 --> 00:22:59.740]   - Right.
[00:22:59.740 --> 00:23:02.580]   How likely do you think it is that these models
[00:23:02.580 --> 00:23:05.060]   will be superhuman for many years
[00:23:05.060 --> 00:23:07.620]   at economically valuable tasks
[00:23:07.620 --> 00:23:09.700]   while they are still below humans
[00:23:09.700 --> 00:23:11.420]   in many other relevant tasks
[00:23:11.420 --> 00:23:14.220]   that prevents like an intelligence explosion or something?
[00:23:14.220 --> 00:23:17.500]   - I think this kind of stuff is like really hard to know.
[00:23:17.500 --> 00:23:20.660]   So I'll give that caveat that like, you know, again,
[00:23:20.660 --> 00:23:23.260]   like the basic scaling laws, you can kind of predict,
[00:23:23.260 --> 00:23:25.060]   and then like this more granular stuff,
[00:23:25.060 --> 00:23:28.620]   which we really wanna know to know how this all is gonna go
[00:23:28.620 --> 00:23:29.900]   is much harder to know.
[00:23:29.900 --> 00:23:34.260]   But my guess would be the scaling laws are gonna continue,
[00:23:34.260 --> 00:23:36.100]   like, you know, again, subject to, you know,
[00:23:36.100 --> 00:23:40.620]   do people slow down for safety or for regulatory reasons?
[00:23:40.620 --> 00:23:43.220]   But, you know, let's just put all that aside
[00:23:43.220 --> 00:23:45.420]   and say like, we have the economic capability
[00:23:45.420 --> 00:23:46.260]   to keep scaling.
[00:23:46.260 --> 00:23:48.180]   If we did that, what would happen?
[00:23:48.180 --> 00:23:51.220]   And I think my view is we're gonna keep getting better
[00:23:51.220 --> 00:23:53.820]   across the board, and I don't see any area
[00:23:53.820 --> 00:23:56.420]   where the models are like super, super weak
[00:23:56.420 --> 00:23:57.860]   or not starting to make progress.
[00:23:57.860 --> 00:24:00.820]   Like that used to be true of like math and programming,
[00:24:00.820 --> 00:24:03.820]   but I think over the last six months, you know,
[00:24:03.820 --> 00:24:05.780]   the 2023 generation of models
[00:24:05.780 --> 00:24:09.260]   compared to the 2022 generation had started to learn that.
[00:24:09.260 --> 00:24:11.300]   There may be more subtle things we don't know.
[00:24:11.300 --> 00:24:16.020]   And so I kind of suspect, even if it isn't quite even,
[00:24:16.020 --> 00:24:18.300]   that the rising tide will lift all the boats.
[00:24:18.300 --> 00:24:20.740]   - Does that include the thing you were mentioning earlier
[00:24:20.740 --> 00:24:22.820]   where if there's an extended task,
[00:24:22.820 --> 00:24:25.580]   it kind of loses its train of thought
[00:24:25.580 --> 00:24:27.500]   or its ability to just like execute a series of steps?
[00:24:27.500 --> 00:24:31.380]   - So I think that that's gonna depend on things
[00:24:31.380 --> 00:24:35.140]   like RL training to have the model do longer horizon tasks.
[00:24:35.140 --> 00:24:38.500]   I don't expect that to require a substantial amount
[00:24:38.500 --> 00:24:41.020]   of additional compute.
[00:24:41.020 --> 00:24:46.020]   I think that that was probably an artifact of,
[00:24:46.020 --> 00:24:48.540]   yeah, kind of thinking about RL in the wrong way
[00:24:48.540 --> 00:24:50.700]   and underestimating how much the model
[00:24:50.700 --> 00:24:52.260]   had learned on its own.
[00:24:52.260 --> 00:24:54.900]   In terms of, you know, are we gonna be superhuman
[00:24:54.900 --> 00:24:57.060]   in some areas and not others?
[00:24:57.060 --> 00:24:58.220]   I think it's complicated.
[00:24:58.220 --> 00:25:01.340]   I could imagine that we won't be superhuman in some areas
[00:25:01.340 --> 00:25:02.180]   because for example,
[00:25:02.180 --> 00:25:05.260]   they involve like embodiment in the physical world.
[00:25:05.260 --> 00:25:06.580]   And then it's like, what happens?
[00:25:06.580 --> 00:25:09.780]   Like do the AIs help us train faster AIs
[00:25:09.780 --> 00:25:13.020]   and those faster AIs wrap around and solve that?
[00:25:13.020 --> 00:25:14.900]   Do you not need the physical world?
[00:25:14.900 --> 00:25:15.980]   It depends what you mean.
[00:25:15.980 --> 00:25:17.980]   Are we worried about an alignment disaster?
[00:25:17.980 --> 00:25:20.020]   Are we worried about misuse,
[00:25:20.020 --> 00:25:22.060]   like making weapons of mass destruction?
[00:25:22.060 --> 00:25:27.060]   Are we worried about the AI taking over research
[00:25:27.060 --> 00:25:28.660]   from humans?
[00:25:28.660 --> 00:25:30.900]   Are we worried about it reaching some threshold
[00:25:30.900 --> 00:25:34.420]   of economic productivity where it can do what the average,
[00:25:34.420 --> 00:25:37.580]   these different thresholds I think have different answers.
[00:25:37.580 --> 00:25:39.780]   Although I suspect they will all come within a few years.
[00:25:39.780 --> 00:25:41.100]   - Let me ask about those thresholds.
[00:25:41.100 --> 00:25:44.180]   So if Claude was an employee at Anthropic,
[00:25:44.180 --> 00:25:45.380]   what salary would it be worth?
[00:25:45.380 --> 00:25:48.060]   What is it like meaningfully speeding up AI progress?
[00:25:48.060 --> 00:25:51.540]   - It feels to me like an intern in most areas,
[00:25:51.540 --> 00:25:54.420]   but then some specific areas where it's better than that.
[00:25:54.420 --> 00:25:57.980]   Again, I think one thing that makes the comparison hard
[00:25:57.980 --> 00:26:00.860]   is like the form factor is kind of like
[00:26:00.860 --> 00:26:02.340]   not the same as a human, right?
[00:26:02.340 --> 00:26:03.700]   Like a human, like, you know,
[00:26:03.700 --> 00:26:06.700]   if you were to behave like one of these chat bots,
[00:26:06.700 --> 00:26:07.980]   like we wouldn't really,
[00:26:07.980 --> 00:26:09.620]   I mean, I guess we could have this conversation.
[00:26:09.620 --> 00:26:12.300]   It's like, but you know, they're not really,
[00:26:12.300 --> 00:26:14.060]   they're more designed to answer single
[00:26:14.060 --> 00:26:15.580]   or a few questions, right?
[00:26:15.580 --> 00:26:18.140]   And like, you know, they don't have the concept
[00:26:18.140 --> 00:26:20.660]   of having a long life of prior experience, right?
[00:26:20.660 --> 00:26:22.620]   We're talking here about, you know,
[00:26:22.620 --> 00:26:25.740]   things that I've experienced in the past, right?
[00:26:25.740 --> 00:26:27.580]   And chat bots don't have that.
[00:26:27.580 --> 00:26:30.340]   And so there's all kinds of stuff missing.
[00:26:30.340 --> 00:26:33.980]   And so it's hard to make a comparison, but I don't know.
[00:26:33.980 --> 00:26:36.780]   They feel like interns in some areas
[00:26:36.780 --> 00:26:39.220]   and kind of then they have areas where they spike
[00:26:39.220 --> 00:26:42.580]   and are really savants where they may be better than,
[00:26:42.580 --> 00:26:43.900]   they may be better than anyone here.
[00:26:43.900 --> 00:26:46.020]   - But does the overall picture of something
[00:26:46.020 --> 00:26:48.060]   like an intelligence explosion, you know,
[00:26:48.060 --> 00:26:49.540]   my former guest is Karl Shulman
[00:26:49.540 --> 00:26:51.820]   and he has this like very detailed model of an intelligence.
[00:26:51.820 --> 00:26:54.140]   Does that, as somebody who would actually
[00:26:54.140 --> 00:26:56.260]   like see that happening, does that make sense to you
[00:26:56.260 --> 00:26:59.100]   as they go from interns to entry-level software engineers,
[00:26:59.100 --> 00:27:00.340]   those entry-level software engineers
[00:27:00.340 --> 00:27:02.140]   increase your productivity?
[00:27:02.140 --> 00:27:06.940]   - I think the idea that the AI systems
[00:27:06.940 --> 00:27:09.860]   become more productive and first they speed up
[00:27:09.860 --> 00:27:12.980]   the productivity of humans, then they, you know,
[00:27:12.980 --> 00:27:16.260]   kind of equal the productivity of humans and, you know,
[00:27:16.260 --> 00:27:19.100]   and then they're in some meaningful sense,
[00:27:19.100 --> 00:27:22.620]   the main contributor to scientific progress
[00:27:22.620 --> 00:27:24.300]   that that happens at some point.
[00:27:24.300 --> 00:27:28.780]   I think that that basic logic seems likely to me,
[00:27:28.780 --> 00:27:31.060]   although I have a suspicion
[00:27:31.060 --> 00:27:33.020]   that when we actually go into the details,
[00:27:33.020 --> 00:27:34.980]   it's gonna be kind of like weird and different
[00:27:34.980 --> 00:27:38.220]   than we expect, that all the detailed models are kind of,
[00:27:38.220 --> 00:27:41.660]   you know, we're thinking about the wrong things
[00:27:41.660 --> 00:27:43.060]   or we're right about one thing
[00:27:43.060 --> 00:27:45.340]   and then are wrong about 10 other things.
[00:27:45.340 --> 00:27:48.020]   And so I don't know, I think we might end up
[00:27:48.020 --> 00:27:51.420]   in like a weirder world than we expect.
[00:27:51.420 --> 00:27:53.620]   - When you add all this together,
[00:27:53.620 --> 00:27:55.980]   like your estimate of when we get something
[00:27:55.980 --> 00:27:58.420]   kind of human level, what does that look like?
[00:27:58.420 --> 00:28:00.260]   - I mean, again, it depends on the thresholds.
[00:28:00.300 --> 00:28:01.420]   - Yeah.
[00:28:01.420 --> 00:28:06.420]   - You know, in terms of someone looks at the model
[00:28:06.420 --> 00:28:09.580]   and, you know, even if you talk to it for, you know,
[00:28:09.580 --> 00:28:13.500]   for an hour or so, it's basically, you know,
[00:28:13.500 --> 00:28:16.820]   it's basically like a generally well-educated human.
[00:28:16.820 --> 00:28:18.100]   - Yeah.
[00:28:18.100 --> 00:28:21.460]   - That could be not very far away at all, I think.
[00:28:21.460 --> 00:28:24.260]   Like that could happen in, you know, two or three years.
[00:28:24.260 --> 00:28:27.540]   Like, you know, if I look at, again,
[00:28:27.540 --> 00:28:29.380]   like I think the main thing that would stop it
[00:28:29.380 --> 00:28:32.700]   would be if we hit certain, you know,
[00:28:32.700 --> 00:28:34.140]   and we have internal tests for, you know,
[00:28:34.140 --> 00:28:35.740]   safety thresholds and stuff like that.
[00:28:35.740 --> 00:28:40.020]   So if a company or the industry decides to slow down
[00:28:40.020 --> 00:28:41.860]   or, you know, we're able to get
[00:28:41.860 --> 00:28:43.940]   the government institute restrictions
[00:28:43.940 --> 00:28:46.980]   that kind of, you know, that moderate
[00:28:46.980 --> 00:28:48.940]   the rate of progress for safety reasons,
[00:28:48.940 --> 00:28:50.580]   that would be the main reason it wouldn't happen.
[00:28:50.580 --> 00:28:53.260]   But if you just look at the logistical
[00:28:53.260 --> 00:28:55.540]   and economic ability to scale,
[00:28:55.540 --> 00:28:58.100]   I don't think we're very far at all from that.
[00:28:58.100 --> 00:29:00.340]   Now that may not be the threshold
[00:29:00.340 --> 00:29:02.940]   where the models are existentially dangerous.
[00:29:02.940 --> 00:29:04.980]   In fact, I suspect it's not quite there yet.
[00:29:04.980 --> 00:29:06.620]   It may not be the threshold where the models
[00:29:06.620 --> 00:29:08.700]   can take over most AI research.
[00:29:08.700 --> 00:29:11.980]   It may not be the threshold where the models,
[00:29:11.980 --> 00:29:15.300]   you know, seriously change how the economy works.
[00:29:15.300 --> 00:29:16.940]   I think it gets a little murky after that,
[00:29:16.940 --> 00:29:19.500]   and all of those thresholds may happen
[00:29:19.500 --> 00:29:21.260]   at various times after that.
[00:29:21.260 --> 00:29:24.980]   But I think in terms of the base technical capability
[00:29:24.980 --> 00:29:29.020]   of it kind of sounds like a reasonably
[00:29:29.020 --> 00:29:31.900]   generally educated human across the board.
[00:29:31.900 --> 00:29:33.260]   I think that could be quite close.
[00:29:33.260 --> 00:29:36.660]   - Why would it be the case that it could be sound,
[00:29:36.660 --> 00:29:38.820]   you know, pass a Turing test for an educated person,
[00:29:38.820 --> 00:29:41.900]   but not be able to contribute or substitute
[00:29:41.900 --> 00:29:44.220]   for human involvement in the economy?
[00:29:44.220 --> 00:29:45.060]   - A couple reasons.
[00:29:45.060 --> 00:29:47.940]   One is just, you know, that the threshold of skill
[00:29:47.940 --> 00:29:48.820]   isn't high enough, right?
[00:29:48.820 --> 00:29:49.900]   Comparative advantage.
[00:29:49.900 --> 00:29:54.220]   It's like, it like doesn't matter that, you know,
[00:29:54.220 --> 00:29:56.660]   I have someone who's better than the average human
[00:29:56.660 --> 00:29:57.780]   at every task.
[00:29:57.780 --> 00:30:00.580]   Like what I really need is like for AI research,
[00:30:00.580 --> 00:30:04.780]   like, you know, I need to basically find something
[00:30:04.780 --> 00:30:08.740]   that is strong enough to substantially accelerate,
[00:30:08.740 --> 00:30:11.460]   you know, the like labor of the thousand experts
[00:30:11.460 --> 00:30:13.020]   who are best at it.
[00:30:13.020 --> 00:30:16.020]   And so we might reach a point where we, you know,
[00:30:16.020 --> 00:30:19.620]   the comparative advantage of these systems is not great.
[00:30:19.620 --> 00:30:23.460]   Another thing that could be the case is that
[00:30:23.460 --> 00:30:25.900]   I think there are these kind of mysterious frictions
[00:30:25.900 --> 00:30:27.740]   that like, you know, kind of don't show up
[00:30:27.740 --> 00:30:29.220]   in naive economic models,
[00:30:29.220 --> 00:30:31.700]   but you see it whenever you're like, you know,
[00:30:31.700 --> 00:30:34.060]   when you go to a customer or something and you're like,
[00:30:34.060 --> 00:30:35.940]   hey, I have this cool chat bot.
[00:30:35.940 --> 00:30:38.260]   In principle, it can do everything that, you know,
[00:30:38.260 --> 00:30:39.900]   your customer service bot does,
[00:30:39.900 --> 00:30:41.620]   or that this part of your company does,
[00:30:41.620 --> 00:30:44.540]   but like the actual friction of like,
[00:30:44.540 --> 00:30:46.460]   how do we slot it in?
[00:30:46.460 --> 00:30:47.620]   How do we make it work?
[00:30:47.620 --> 00:30:50.100]   That includes both kind of like, you know,
[00:30:50.100 --> 00:30:52.660]   just the question of how it works in a human sense
[00:30:52.660 --> 00:30:53.500]   within the company.
[00:30:53.500 --> 00:30:56.740]   Like, you know, how things happen in the economy
[00:30:56.740 --> 00:30:58.340]   and overcome frictions.
[00:30:58.340 --> 00:31:00.980]   And also just like, what is the workflow?
[00:31:00.980 --> 00:31:02.740]   How do you actually interact with it?
[00:31:02.740 --> 00:31:04.780]   It's very different to say,
[00:31:04.780 --> 00:31:07.500]   here's a chat bot that kind of looks like
[00:31:07.500 --> 00:31:10.700]   it's doing this task that you're, you know,
[00:31:10.700 --> 00:31:13.220]   or helping the human to do some tasks,
[00:31:13.220 --> 00:31:17.380]   as it is to say like, okay, this thing is deployed
[00:31:17.380 --> 00:31:19.740]   and a hundred thousand people are using it.
[00:31:19.740 --> 00:31:21.380]   Often, like right now,
[00:31:21.380 --> 00:31:24.020]   lots of folks are rushing to deploy these systems,
[00:31:24.020 --> 00:31:25.500]   but I think in many cases,
[00:31:25.500 --> 00:31:27.500]   they're not using them in anywhere close
[00:31:27.500 --> 00:31:29.660]   to the most efficient way that they could,
[00:31:29.660 --> 00:31:31.380]   you know, not because they're not smart,
[00:31:31.380 --> 00:31:33.700]   but because it takes time to work these things out.
[00:31:33.700 --> 00:31:35.980]   And so I think when things are changing this fast,
[00:31:35.980 --> 00:31:37.460]   there are gonna be all of these frictions.
[00:31:37.460 --> 00:31:38.300]   - Yeah.
[00:31:38.300 --> 00:31:41.260]   - And I think, again, these are messy reality
[00:31:41.260 --> 00:31:43.460]   that doesn't quite get captured in the model.
[00:31:43.460 --> 00:31:45.540]   I don't think it changes the basic picture.
[00:31:45.540 --> 00:31:47.100]   Like, I don't think it changes the idea
[00:31:47.100 --> 00:31:49.740]   that we're building up this snowball of like,
[00:31:49.740 --> 00:31:51.460]   the models help the models get better
[00:31:51.460 --> 00:31:52.900]   and, you know, do what the humans,
[00:31:52.900 --> 00:31:55.460]   and, you know, can accelerate what the humans do.
[00:31:55.460 --> 00:31:57.340]   And eventually, it's mostly the models doing the work.
[00:31:57.340 --> 00:32:00.380]   Like, you zoom out far enough, that's happening.
[00:32:00.380 --> 00:32:02.940]   But I'm kind of skeptical of kind of,
[00:32:02.940 --> 00:32:05.580]   any kind of precise mathematical
[00:32:05.580 --> 00:32:07.820]   or exponential prediction of how it's gonna be.
[00:32:07.820 --> 00:32:10.900]   I think it's all gonna be a mess,
[00:32:10.900 --> 00:32:12.660]   but I think what we know is
[00:32:12.660 --> 00:32:14.980]   it's not a metaphorical exponential
[00:32:14.980 --> 00:32:16.460]   and it's gonna happen fast.
[00:32:16.460 --> 00:32:19.460]   - How do those different exponentials net out,
[00:32:19.460 --> 00:32:20.660]   which we've been talking about?
[00:32:20.660 --> 00:32:23.900]   So one was the scaling laws themselves
[00:32:23.900 --> 00:32:26.380]   are power laws with decaying marginal,
[00:32:26.380 --> 00:32:27.220]   - Yes.
[00:32:27.220 --> 00:32:30.420]   - You know, loss per, you know, parameter or something.
[00:32:30.420 --> 00:32:32.460]   The other exponential you talked about is,
[00:32:32.460 --> 00:32:33.900]   well, these things can get involved
[00:32:33.900 --> 00:32:37.460]   in the process of AI research itself, speeding it up.
[00:32:37.460 --> 00:32:40.260]   So those two are sort of opposing exponentials.
[00:32:40.260 --> 00:32:42.580]   Does it net out to be super linear or sub-linear?
[00:32:42.580 --> 00:32:44.180]   And also you mentioned,
[00:32:44.180 --> 00:32:46.140]   well, the distribution of intelligence
[00:32:46.140 --> 00:32:47.260]   might just be broader.
[00:32:47.260 --> 00:32:49.900]   So should we expect after we get to this point
[00:32:49.900 --> 00:32:51.180]   in two to three years, it's like,
[00:32:51.180 --> 00:32:53.380]   vroom, vroom, like, what does that look like?
[00:32:53.380 --> 00:32:55.620]   - It's, I mean, I think it's very unclear, right?
[00:32:55.620 --> 00:32:57.580]   So we're already at the point where,
[00:32:57.580 --> 00:32:58.540]   if you look at the loss,
[00:32:58.540 --> 00:33:00.540]   the scaling laws are starting to bend.
[00:33:00.540 --> 00:33:01.900]   I mean, we've seen that in, you know,
[00:33:01.900 --> 00:33:05.940]   published model cards offered by multiple companies.
[00:33:05.940 --> 00:33:07.620]   So that's not a secret at all.
[00:33:07.620 --> 00:33:09.660]   But as they start to bend,
[00:33:09.660 --> 00:33:12.140]   each little bit of entropy, right,
[00:33:12.140 --> 00:33:14.380]   of accurate prediction becomes more important, right?
[00:33:14.380 --> 00:33:16.980]   Maybe these last little bits of entropy are like,
[00:33:16.980 --> 00:33:18.940]   well, you know, this is a physics paper
[00:33:18.940 --> 00:33:20.180]   as Einstein would have written it,
[00:33:20.180 --> 00:33:21.340]   as opposed to, you know,
[00:33:21.340 --> 00:33:24.180]   as some other physicist would have written it.
[00:33:24.180 --> 00:33:27.780]   And so it's hard to assess significance from this.
[00:33:27.780 --> 00:33:31.820]   It certainly looks like in terms of practical performance,
[00:33:31.820 --> 00:33:34.700]   the metrics keep going up relatively linearly,
[00:33:34.700 --> 00:33:36.660]   although they were always unpredictable.
[00:33:36.660 --> 00:33:38.420]   So it's hard to see that.
[00:33:38.420 --> 00:33:40.420]   And then, I mean,
[00:33:40.420 --> 00:33:42.700]   the thing that I think is driving the most acceleration
[00:33:42.700 --> 00:33:45.100]   is just more and more money is going into the field.
[00:33:45.100 --> 00:33:48.300]   Like, people are seeing that there's just a huge amount
[00:33:48.300 --> 00:33:51.140]   of, you know, of economic value.
[00:33:51.140 --> 00:33:53.700]   And so I expect the price,
[00:33:53.700 --> 00:33:55.580]   the amount of money spent on the largest models
[00:33:55.580 --> 00:33:58.100]   to go up by like a factor of 100 or something.
[00:33:58.100 --> 00:34:00.260]   And for that then to be concatenated
[00:34:00.260 --> 00:34:01.940]   with the chips are getting faster,
[00:34:01.940 --> 00:34:03.260]   the algorithms are getting better,
[00:34:03.260 --> 00:34:06.420]   because there's so many people working on this now.
[00:34:06.420 --> 00:34:08.380]   And so, again, I mean, you know,
[00:34:08.380 --> 00:34:10.300]   I'm not making a normative statement here,
[00:34:10.300 --> 00:34:12.300]   this is what should happen.
[00:34:12.300 --> 00:34:14.820]   I'm not even saying this necessarily will happen,
[00:34:14.820 --> 00:34:17.260]   because I think there's important safety
[00:34:17.260 --> 00:34:18.740]   and government questions here,
[00:34:18.740 --> 00:34:20.420]   which we're very actively working on.
[00:34:20.420 --> 00:34:22.980]   I'm just saying like left to itself,
[00:34:22.980 --> 00:34:24.380]   this is what the economy is gonna do.
[00:34:24.380 --> 00:34:25.660]   - We'll get to those questions in a second,
[00:34:25.660 --> 00:34:29.420]   but how do you think about the contribution of Anthropic
[00:34:29.420 --> 00:34:33.460]   to that increasing in the scope of this industry?
[00:34:33.460 --> 00:34:36.060]   Where, I mean, there's an argument that, listen,
[00:34:36.060 --> 00:34:37.260]   with that investment,
[00:34:37.260 --> 00:34:39.580]   we can work on safety stuff at Anthropic,
[00:34:39.580 --> 00:34:41.980]   another that says you're raising the salience
[00:34:41.980 --> 00:34:43.260]   of this field in general.
[00:34:43.300 --> 00:34:45.940]   - Yeah, I mean, it's all costs and benefits, right?
[00:34:45.940 --> 00:34:47.420]   The costs are not zero, right?
[00:34:47.420 --> 00:34:50.100]   So I think a mature way to think about these things
[00:34:50.100 --> 00:34:52.300]   is not to deny that there are any costs,
[00:34:52.300 --> 00:34:54.060]   but to think about what the costs are
[00:34:54.060 --> 00:34:55.340]   and what the benefits are.
[00:34:55.340 --> 00:34:57.700]   You know, I think we've been relatively responsible
[00:34:57.700 --> 00:34:58.860]   in the sense that, you know,
[00:34:58.860 --> 00:35:01.780]   the big acceleration that happened late last year
[00:35:01.780 --> 00:35:03.540]   and beginning of this year,
[00:35:03.540 --> 00:35:04.940]   like we didn't cause that,
[00:35:04.940 --> 00:35:06.900]   we weren't the ones who did that.
[00:35:06.900 --> 00:35:09.140]   And honestly, I think if you look at the reaction of Google,
[00:35:09.140 --> 00:35:10.860]   that that might be 10 times more important
[00:35:10.860 --> 00:35:12.260]   than anything else.
[00:35:12.260 --> 00:35:13.780]   And then kind of once it had happened,
[00:35:13.780 --> 00:35:15.660]   once the ecosystem had changed,
[00:35:15.660 --> 00:35:16.980]   then we did a lot of things
[00:35:16.980 --> 00:35:19.180]   to kind of stay on the frontier.
[00:35:19.180 --> 00:35:21.300]   And so, I don't know,
[00:35:21.300 --> 00:35:23.180]   I mean, it's like any other question, right?
[00:35:23.180 --> 00:35:26.060]   It's like you're trying to do the things
[00:35:26.060 --> 00:35:27.620]   that have the biggest costs
[00:35:27.620 --> 00:35:30.860]   and that have the lowest costs and the biggest benefits.
[00:35:30.860 --> 00:35:33.460]   And, you know, that causes you to have
[00:35:33.460 --> 00:35:35.300]   different strategies at different times.
[00:35:35.300 --> 00:35:36.140]   - One question I have for you
[00:35:36.140 --> 00:35:38.180]   while we're talking about the intelligence stuff was,
[00:35:38.180 --> 00:35:40.060]   listen, as a scientist yourself,
[00:35:40.060 --> 00:35:41.620]   is it, what do you make of the fact
[00:35:41.620 --> 00:35:44.020]   that these things have basically the entire corpus
[00:35:44.020 --> 00:35:45.620]   of human knowledge memorized?
[00:35:45.620 --> 00:35:47.140]   And as far as I'm aware,
[00:35:47.140 --> 00:35:49.700]   they haven't been able to make like a single new connection
[00:35:49.700 --> 00:35:51.300]   that has led to a discovery.
[00:35:51.300 --> 00:35:53.540]   Whereas if even a moderately intelligent person
[00:35:53.540 --> 00:35:55.020]   had this much stuff memorized,
[00:35:55.020 --> 00:35:57.700]   they'd notice, oh, this thing causes this symptom,
[00:35:57.700 --> 00:35:59.580]   this other thing also causes this symptom.
[00:35:59.580 --> 00:36:01.700]   You know, there's a medical cure right here, right?
[00:36:01.700 --> 00:36:03.540]   Well, shouldn't we be expecting that kind of stuff?
[00:36:03.540 --> 00:36:04.980]   - I'm not sure.
[00:36:04.980 --> 00:36:07.780]   I mean, I think, you know, I don't know,
[00:36:07.780 --> 00:36:09.740]   these words, discovery, creativity,
[00:36:10.260 --> 00:36:13.020]   one of the lessons I've learned is that in, you know,
[00:36:13.020 --> 00:36:15.260]   in kind of the big blob of compute,
[00:36:15.260 --> 00:36:18.620]   often these ideas often end up being kind of fuzzy
[00:36:18.620 --> 00:36:20.300]   and elusive and hard to track down.
[00:36:20.300 --> 00:36:22.900]   But I think there is something here,
[00:36:22.900 --> 00:36:25.820]   which is, I think the models do display
[00:36:25.820 --> 00:36:27.420]   a kind of ordinary creativity.
[00:36:27.420 --> 00:36:29.500]   Again, you know, the kind of like, you know,
[00:36:29.500 --> 00:36:31.300]   write a sonnet, you know,
[00:36:31.300 --> 00:36:34.860]   in the style of Cormac McCarthy or Barbie or, you know,
[00:36:34.860 --> 00:36:37.260]   like there is some creativity to that.
[00:36:37.260 --> 00:36:39.060]   And I think they do draw, you know,
[00:36:39.060 --> 00:36:40.540]   new connections of the kind that
[00:36:40.540 --> 00:36:42.180]   an ordinary person would draw.
[00:36:42.180 --> 00:36:45.300]   I agree with you that there haven't been any kind of like,
[00:36:45.300 --> 00:36:46.420]   I don't know, like I would say like
[00:36:46.420 --> 00:36:49.220]   big scientific discoveries.
[00:36:49.220 --> 00:36:51.020]   I think that's a mix of like,
[00:36:51.020 --> 00:36:54.900]   just the model skill level is not high enough yet, right?
[00:36:54.900 --> 00:36:59.100]   Like I was on a podcast last week where the host said,
[00:36:59.100 --> 00:37:00.260]   I don't know, I play with these models,
[00:37:00.260 --> 00:37:01.420]   they're kind of mid, right?
[00:37:01.420 --> 00:37:02.260]   Like they get, you know,
[00:37:02.260 --> 00:37:04.340]   they get a B or a B minus or something.
[00:37:04.340 --> 00:37:07.940]   And that I think is gonna change with the scaling.
[00:37:07.940 --> 00:37:09.820]   I do think there's an interesting point about,
[00:37:09.820 --> 00:37:11.540]   well, the models have an advantage,
[00:37:11.540 --> 00:37:14.380]   which is they know a lot more than us, you know,
[00:37:14.380 --> 00:37:16.460]   like should they have an advantage already,
[00:37:16.460 --> 00:37:19.780]   even if their skill level isn't quite high,
[00:37:19.780 --> 00:37:22.060]   maybe that's kind of what you're getting at.
[00:37:22.060 --> 00:37:23.540]   I don't really have an answer to that.
[00:37:23.540 --> 00:37:27.100]   I mean, it seems certainly like memorization and facts
[00:37:27.100 --> 00:37:28.740]   and drawing connections is an area where
[00:37:28.740 --> 00:37:30.100]   the models are ahead.
[00:37:30.100 --> 00:37:33.660]   And I do think maybe you need those connections
[00:37:33.660 --> 00:37:36.660]   and you need a fairly high level of skill.
[00:37:36.660 --> 00:37:39.060]   I do think, particularly in the area of biology,
[00:37:39.060 --> 00:37:40.860]   for better and for worse,
[00:37:40.860 --> 00:37:44.660]   the complexity of biology is such that
[00:37:44.660 --> 00:37:48.180]   the current models know a lot of things right now.
[00:37:48.180 --> 00:37:50.700]   And that's what you need to make discoveries and draw.
[00:37:50.700 --> 00:37:52.860]   It's not like physics where you need to, you know,
[00:37:52.860 --> 00:37:54.380]   you need to think and come up with a formula.
[00:37:54.380 --> 00:37:56.420]   In biology, you need to know a lot of things, right?
[00:37:56.420 --> 00:37:59.140]   And so I do think the models know a lot of things
[00:37:59.140 --> 00:38:01.140]   and they have a skill level that's not quite high enough
[00:38:01.140 --> 00:38:02.300]   to put them together.
[00:38:02.300 --> 00:38:04.740]   And I think they are just on the cusp
[00:38:04.740 --> 00:38:06.260]   of being able to put these things together.
[00:38:06.260 --> 00:38:09.020]   - On that point, last week in your Senate testimony,
[00:38:09.020 --> 00:38:11.500]   you said that these models are two to three years away
[00:38:11.500 --> 00:38:14.900]   from potentially enabling large-scale biotourism attacks
[00:38:14.900 --> 00:38:15.860]   or something like that. - Yes.
[00:38:15.860 --> 00:38:17.100]   - Can you make that more concrete
[00:38:17.100 --> 00:38:18.900]   without obviously giving the kind of information
[00:38:18.900 --> 00:38:21.740]   that would, but is it like one-shotting
[00:38:21.740 --> 00:38:22.900]   how to weaponize something?
[00:38:22.900 --> 00:38:25.220]   Is it, or do you have to fine tune an open source model?
[00:38:25.220 --> 00:38:26.540]   Like what would that actually look like?
[00:38:26.540 --> 00:38:28.260]   - I think it'd be good to clarify this
[00:38:28.260 --> 00:38:30.380]   because we did a blog post in the Senate testimony
[00:38:30.380 --> 00:38:33.180]   and like, I think various people kind of didn't understand
[00:38:33.180 --> 00:38:35.140]   the point or didn't understand what we'd done.
[00:38:35.140 --> 00:38:39.060]   So I think today, and of course in our models,
[00:38:39.060 --> 00:38:42.340]   we try and prevent this, but there's always jail breaks.
[00:38:42.340 --> 00:38:46.140]   You can ask the models all kinds of things about biology
[00:38:46.140 --> 00:38:48.260]   and get them to say all kinds of scary things.
[00:38:48.260 --> 00:38:49.100]   - Yeah.
[00:38:49.100 --> 00:38:51.300]   - But often those scary things are things
[00:38:51.300 --> 00:38:53.980]   that you could Google and I'm therefore
[00:38:53.980 --> 00:38:56.180]   not particularly worried about that.
[00:38:56.180 --> 00:38:58.820]   I think it's actually an impediment to seeing the real danger
[00:38:58.820 --> 00:39:01.300]   where someone just says, oh, I asked this model
[00:39:01.300 --> 00:39:04.300]   like for the smallpox, to tell me some things
[00:39:04.300 --> 00:39:06.100]   about smallpox and it will.
[00:39:06.100 --> 00:39:08.940]   That is actually kind of not what I'm worried about.
[00:39:08.940 --> 00:39:12.700]   So we spent about six months working with some of,
[00:39:12.700 --> 00:39:15.140]   basically some of the folks who are the most expert
[00:39:15.140 --> 00:39:19.900]   in the world on how do biological attacks happen?
[00:39:19.900 --> 00:39:23.420]   What would you need to conduct such an attack
[00:39:23.420 --> 00:39:25.700]   and how do we defend against such an attack?
[00:39:25.700 --> 00:39:30.260]   They worked very intensively on just the entire workflow.
[00:39:30.260 --> 00:39:32.940]   If I were trying to do a bad thing, it's not one shot,
[00:39:32.940 --> 00:39:36.060]   it's a long process, there are many steps to it.
[00:39:36.060 --> 00:39:37.700]   It's not just like I asked the model
[00:39:37.700 --> 00:39:39.580]   for this one page of information.
[00:39:39.580 --> 00:39:41.860]   And again, without going into any detail,
[00:39:41.860 --> 00:39:44.700]   the thing I said in the Senate testimony is like,
[00:39:44.700 --> 00:39:46.700]   there are some steps where you can just get information
[00:39:46.700 --> 00:39:48.660]   on Google, there are some steps
[00:39:48.660 --> 00:39:50.540]   that are what I'd call missing.
[00:39:50.540 --> 00:39:54.020]   They're scattered across a bunch of textbooks
[00:39:54.020 --> 00:39:56.020]   or they're not in any textbook,
[00:39:56.020 --> 00:39:58.060]   they're kind of implicit knowledge.
[00:39:58.060 --> 00:40:01.340]   And they're not really like, they're not explicit knowledge,
[00:40:01.340 --> 00:40:05.340]   they're more like, I have to do this lab protocol
[00:40:05.340 --> 00:40:07.300]   and like, what if I get it wrong?
[00:40:07.300 --> 00:40:10.300]   Oh, if this happens, then my temperature was too low.
[00:40:10.300 --> 00:40:12.020]   If that happened, I needed to add more
[00:40:12.020 --> 00:40:13.820]   of this particular reagent.
[00:40:13.820 --> 00:40:16.480]   What we found is that for the most part,
[00:40:16.480 --> 00:40:19.260]   those missing, those key missing pieces,
[00:40:19.260 --> 00:40:21.460]   the models can't do them yet.
[00:40:21.460 --> 00:40:24.900]   But we found that sometimes they can.
[00:40:24.900 --> 00:40:27.460]   And when they can, sometimes they still hallucinate,
[00:40:27.460 --> 00:40:30.800]   which is the thing that's kind of keeping us safe.
[00:40:30.800 --> 00:40:33.860]   But we saw enough signs of the models
[00:40:33.860 --> 00:40:36.420]   doing those key things well.
[00:40:36.420 --> 00:40:39.740]   And if we look at state-of-the-art models
[00:40:39.740 --> 00:40:41.940]   and go backwards to previous models,
[00:40:41.940 --> 00:40:45.220]   we look at the trend, it shows every sign
[00:40:45.220 --> 00:40:48.540]   of two or three years from now,
[00:40:48.540 --> 00:40:49.580]   we're gonna have a real problem.
[00:40:49.580 --> 00:40:51.140]   - Yeah, especially the thing you mentioned
[00:40:51.140 --> 00:40:52.980]   on the log scale, you go from like one in 100 times
[00:40:52.980 --> 00:40:54.540]   it gets it right to one in 10, two.
[00:40:54.540 --> 00:40:57.940]   - Exactly, so I've seen many of these groks in my life.
[00:40:57.940 --> 00:41:02.380]   I was there, I watched when GPT-3 learned to do arithmetic,
[00:41:02.380 --> 00:41:04.980]   when GPT-2 learned to do regression
[00:41:04.980 --> 00:41:06.420]   a little bit above chance,
[00:41:06.420 --> 00:41:11.420]   when we got with Claude and we got better on all these tests
[00:41:11.420 --> 00:41:13.220]   of helpful, honest, harmless.
[00:41:13.220 --> 00:41:15.060]   I've seen a lot of groks.
[00:41:15.060 --> 00:41:17.700]   This is unfortunately not one that I'm excited about,
[00:41:17.700 --> 00:41:18.980]   but I believe it's happening.
[00:41:18.980 --> 00:41:22.060]   - So somebody might say, "Listen, you were a coauthor
[00:41:22.060 --> 00:41:24.280]   "on this post that OpenAI released about GPT-2
[00:41:24.280 --> 00:41:27.340]   "where they said we're not gonna release the weights
[00:41:27.340 --> 00:41:28.980]   "or the details here because we're worried
[00:41:28.980 --> 00:41:33.220]   "that this model will be used for something bad."
[00:41:33.220 --> 00:41:36.340]   And looking back on it, now it's laughable to think
[00:41:36.340 --> 00:41:38.500]   that GPT-2 could have done anything bad.
[00:41:38.500 --> 00:41:41.300]   Are we just way too worried, this is a concern
[00:41:41.300 --> 00:41:42.740]   that doesn't make sense for--
[00:41:42.740 --> 00:41:44.140]   - It is interesting.
[00:41:44.140 --> 00:41:45.460]   It might be worth looking back
[00:41:45.460 --> 00:41:47.940]   at the actual text of that post.
[00:41:47.940 --> 00:41:49.460]   So I don't remember it exactly,
[00:41:49.460 --> 00:41:52.460]   but it's still up on the internet.
[00:41:52.460 --> 00:41:55.780]   It says something like, "We're choosing not to release
[00:41:55.780 --> 00:41:58.620]   "the weights because of concerns about misuse,"
[00:41:58.620 --> 00:42:01.600]   but it also said, "This is an experiment.
[00:42:01.600 --> 00:42:04.040]   "We're not sure if this is necessary
[00:42:04.040 --> 00:42:06.180]   "or the right thing to do at this time,
[00:42:06.180 --> 00:42:08.800]   "but we'd like to establish a norm
[00:42:08.800 --> 00:42:11.660]   "of thinking carefully about these things."
[00:42:11.660 --> 00:42:14.820]   You could think of it a little like the Silomar Conference
[00:42:15.500 --> 00:42:19.220]   in the 1970s where it's like they were just figuring out
[00:42:19.220 --> 00:42:22.820]   recombinant DNA, it was not necessarily the case
[00:42:22.820 --> 00:42:25.260]   that someone could do something really bad
[00:42:25.260 --> 00:42:27.540]   with recombinant DNA, it's just the possibilities
[00:42:27.540 --> 00:42:29.020]   were starting to become clear.
[00:42:29.020 --> 00:42:31.060]   Those words, at least, were the right attitude.
[00:42:31.060 --> 00:42:32.420]   Now, I think there's a separate thing
[00:42:32.420 --> 00:42:36.460]   that people don't just judge the post,
[00:42:36.460 --> 00:42:37.980]   they judge the organization.
[00:42:37.980 --> 00:42:42.380]   Is this an organization that produces a lot of hype
[00:42:42.380 --> 00:42:44.460]   or that has credibility or something like that?
[00:42:44.460 --> 00:42:47.220]   And so I think that had some effect on it.
[00:42:47.220 --> 00:42:49.780]   I guess you could also ask, is it inevitable
[00:42:49.780 --> 00:42:52.020]   that people would just interpret it as like,
[00:42:52.020 --> 00:42:55.860]   you can't get across any message more complicated
[00:42:55.860 --> 00:42:58.500]   than this thing right here is dangerous?
[00:42:58.500 --> 00:43:01.220]   So you can argue about those, but I think the basic thing
[00:43:01.220 --> 00:43:03.660]   that was in my head and the head of others
[00:43:03.660 --> 00:43:08.500]   who were involved in that, and I think what is evident
[00:43:08.500 --> 00:43:11.300]   in the post is we actually don't know.
[00:43:11.300 --> 00:43:13.460]   We have pretty wide error bars on what's dangerous
[00:43:13.460 --> 00:43:17.180]   and what's not, so we want to establish a norm
[00:43:17.180 --> 00:43:18.340]   of being careful.
[00:43:18.340 --> 00:43:21.340]   I think, by the way, we have enormously more evidence.
[00:43:21.340 --> 00:43:23.580]   We've seen enormously more of these groks now,
[00:43:23.580 --> 00:43:25.300]   and so we're well calibrated,
[00:43:25.300 --> 00:43:26.700]   but there's still uncertainty, right?
[00:43:26.700 --> 00:43:29.620]   In all these statements, I said, in two or three years,
[00:43:29.620 --> 00:43:30.700]   we might be there, right?
[00:43:30.700 --> 00:43:32.380]   There's a substantial risk of it,
[00:43:32.380 --> 00:43:33.660]   and we don't wanna take that risk.
[00:43:33.660 --> 00:43:37.060]   But I wouldn't say it's 100%, it could be 50/50.
[00:43:37.060 --> 00:43:38.580]   - Okay, let's talk about cybersecurity,
[00:43:38.580 --> 00:43:40.980]   which in addition to bio-risk is another thing
[00:43:40.980 --> 00:43:42.980]   Anthropic has been emphasizing.
[00:43:42.980 --> 00:43:46.060]   How have you avoided the cloud microarchitecture
[00:43:46.060 --> 00:43:46.900]   from leaking?
[00:43:46.900 --> 00:43:48.460]   Because as you know, your competitors
[00:43:48.460 --> 00:43:51.260]   have been less successful at this kind of security.
[00:43:51.260 --> 00:43:53.060]   - Can't comment on anyone else's security.
[00:43:53.060 --> 00:43:54.620]   Don't know what's going on in there.
[00:43:54.620 --> 00:43:57.420]   A thing that we have done is,
[00:43:57.420 --> 00:44:01.300]   so there are these architectural innovations, right,
[00:44:01.300 --> 00:44:02.460]   that make training more efficient.
[00:44:02.460 --> 00:44:04.260]   We call them compute multipliers
[00:44:04.260 --> 00:44:07.020]   because they're the equivalent of improving,
[00:44:07.020 --> 00:44:10.740]   they're like having more compute.
[00:44:10.740 --> 00:44:12.100]   Our compute multipliers, again,
[00:44:12.100 --> 00:44:13.580]   I don't wanna say too much about it
[00:44:13.580 --> 00:44:15.100]   because it could allow an adversary
[00:44:15.100 --> 00:44:18.220]   to counteract our measures.
[00:44:18.220 --> 00:44:19.940]   But we limit the number of people
[00:44:19.940 --> 00:44:24.300]   who are aware of a given compute multiplier
[00:44:24.300 --> 00:44:26.100]   to those who need to know about it.
[00:44:26.100 --> 00:44:29.180]   And so there's a very small number of people
[00:44:29.180 --> 00:44:30.540]   who could leak all of these secrets.
[00:44:30.540 --> 00:44:31.580]   There's a larger number of people
[00:44:31.580 --> 00:44:33.380]   who could leak one of them.
[00:44:33.380 --> 00:44:36.380]   But this is the standard compartmentalization strategy
[00:44:36.380 --> 00:44:38.300]   that's used in the intelligence community
[00:44:38.300 --> 00:44:42.820]   or resistant cells or whatever.
[00:44:42.820 --> 00:44:47.100]   So we've, over the last few months,
[00:44:47.100 --> 00:44:48.540]   we've implemented these measures.
[00:44:48.540 --> 00:44:50.500]   So I don't wanna jinx anything by saying,
[00:44:50.500 --> 00:44:52.660]   oh, this could never happen to us,
[00:44:52.660 --> 00:44:55.580]   but I think it would be harder for it to happen.
[00:44:55.580 --> 00:44:57.140]   I don't wanna go into any more detail.
[00:44:57.140 --> 00:44:59.740]   And by the way, I'd encourage all the other companies
[00:44:59.740 --> 00:45:00.580]   to do this as well.
[00:45:00.580 --> 00:45:04.620]   As much as competitors' architectures leaking
[00:45:04.620 --> 00:45:06.620]   is narrowly helpful to Anthropic,
[00:45:06.620 --> 00:45:09.340]   it's not good for anyone in the long run, right?
[00:45:09.340 --> 00:45:11.820]   So security around this stuff is really important.
[00:45:11.820 --> 00:45:13.700]   - Even with all the security you have,
[00:45:13.700 --> 00:45:15.420]   could you, with your current security,
[00:45:15.420 --> 00:45:17.620]   prevent a dedicated state-level actor
[00:45:17.620 --> 00:45:19.220]   from getting the CLAW-2 weights?
[00:45:19.220 --> 00:45:22.380]   - It depends how dedicated is what I would say.
[00:45:22.380 --> 00:45:26.420]   Our head of security, who used to work on security
[00:45:26.420 --> 00:45:31.140]   for Chrome, which very widely used an attack application,
[00:45:31.140 --> 00:45:32.980]   he likes to think about it in terms of
[00:45:32.980 --> 00:45:36.180]   how much would it cost to attack Anthropic successfully?
[00:45:36.180 --> 00:45:37.860]   Again, I don't wanna go into super detail
[00:45:37.860 --> 00:45:39.420]   of how much I think it will cost to attack,
[00:45:39.420 --> 00:45:40.820]   and it's just kind of inviting people,
[00:45:40.820 --> 00:45:43.740]   but one of our goals is that it costs more
[00:45:43.740 --> 00:45:46.660]   to attack Anthropic than it costs
[00:45:46.660 --> 00:45:48.700]   to just train your own model,
[00:45:48.700 --> 00:45:50.180]   which doesn't guarantee things
[00:45:50.180 --> 00:45:52.140]   because, of course, you need the talent as well,
[00:45:52.140 --> 00:45:56.020]   so you might still, but attacks have risks,
[00:45:56.020 --> 00:45:59.460]   the diplomatic costs, and they use up
[00:45:59.460 --> 00:46:02.820]   the very sparse resources that nation-state actors
[00:46:02.820 --> 00:46:05.940]   might have in order to do the attacks.
[00:46:05.940 --> 00:46:07.500]   So we're not there yet, by the way,
[00:46:07.500 --> 00:46:12.500]   but I think we're to a very high standard
[00:46:12.500 --> 00:46:14.540]   compared to the size of company that we are.
[00:46:14.540 --> 00:46:16.260]   Like, I think if you look at security
[00:46:16.260 --> 00:46:19.260]   for most 150-person companies,
[00:46:19.260 --> 00:46:21.740]   I think there's just no comparison,
[00:46:21.740 --> 00:46:26.740]   but could we resist if it was a state actor's top priority
[00:46:26.740 --> 00:46:28.820]   to steal our model weights?
[00:46:28.820 --> 00:46:30.140]   No, they would succeed.
[00:46:30.140 --> 00:46:32.100]   - How long does that stay true?
[00:46:32.100 --> 00:46:34.940]   Because at some point, the value keeps increasing
[00:46:34.940 --> 00:46:37.980]   and increasing, and another part of this question
[00:46:37.980 --> 00:46:41.260]   is that what kind of a secret is
[00:46:41.260 --> 00:46:43.900]   how to train Cloud 3 or Cloud 2?
[00:46:43.900 --> 00:46:46.500]   Is it, you know, with nuclear weapons, for example,
[00:46:46.500 --> 00:46:48.820]   we have lots of spies, you just take a blueprint across,
[00:46:48.820 --> 00:46:51.780]   and that's the implosion device, and that's what you need.
[00:46:51.780 --> 00:46:53.500]   Here, is it just, is it more tacit,
[00:46:53.500 --> 00:46:54.940]   like the thing you were talking about, biology,
[00:46:54.940 --> 00:46:56.180]   you need to know how these reagents work?
[00:46:56.180 --> 00:46:57.820]   Is it just like, you got the blueprint,
[00:46:57.820 --> 00:46:58.660]   you got the microarchitecture
[00:46:58.660 --> 00:46:59.500]   and the hardware parameters, you're good to go?
[00:46:59.500 --> 00:47:01.460]   - I mean, there are some things that are like,
[00:47:01.460 --> 00:47:02.860]   you know, a one-line equation,
[00:47:02.860 --> 00:47:05.300]   and there are other things that are more complicated.
[00:47:05.300 --> 00:47:09.380]   And I think compartmentalization is the best way to do it.
[00:47:09.380 --> 00:47:11.660]   Just limit the number of people who know about something.
[00:47:11.660 --> 00:47:12.980]   If you're a thousand-person company
[00:47:12.980 --> 00:47:14.340]   and everyone knows every secret,
[00:47:14.340 --> 00:47:17.300]   like one, I guarantee you have a leaker,
[00:47:17.300 --> 00:47:19.820]   and two, I guarantee you have a spy, like a literal spy.
[00:47:19.820 --> 00:47:21.220]   - Okay, let's talk about alignment.
[00:47:21.220 --> 00:47:22.940]   And let's talk about mechanistic interoperability,
[00:47:22.940 --> 00:47:26.700]   which is the branch of which you guys specialize in.
[00:47:26.700 --> 00:47:28.340]   While you're answering this question,
[00:47:28.340 --> 00:47:29.220]   you might want to explain
[00:47:29.220 --> 00:47:30.900]   what mechanistic interoperability is.
[00:47:30.900 --> 00:47:33.540]   But just the broader question is,
[00:47:33.540 --> 00:47:35.660]   mechanistically, what is alignment?
[00:47:35.660 --> 00:47:38.740]   Is it that you're locking in the model
[00:47:38.740 --> 00:47:40.820]   into a benevolent character?
[00:47:40.820 --> 00:47:43.820]   Are you disabling deceptive circuits and procedures?
[00:47:43.820 --> 00:47:47.020]   Like what concretely is happening when you align a model?
[00:47:47.020 --> 00:47:49.380]   - I think as with most things, you know,
[00:47:49.380 --> 00:47:51.700]   when we actually train a model to be aligned,
[00:47:51.700 --> 00:47:54.220]   we don't know what happens inside the model, right?
[00:47:54.220 --> 00:47:56.100]   There are different ways of training it to be aligned,
[00:47:56.100 --> 00:47:58.380]   but I think we don't really know what happens.
[00:47:58.380 --> 00:48:01.180]   I mean, I think for some of the current methods,
[00:48:01.180 --> 00:48:02.300]   I think all the current methods
[00:48:02.300 --> 00:48:04.300]   that involve some kind of fine tuning,
[00:48:04.300 --> 00:48:05.660]   of course have the property
[00:48:05.660 --> 00:48:07.820]   that the underlying knowledge and abilities
[00:48:07.820 --> 00:48:11.340]   that we might be worried about don't disappear.
[00:48:11.340 --> 00:48:12.380]   It's just, you know,
[00:48:12.380 --> 00:48:14.940]   the model is just taught not to output them.
[00:48:14.940 --> 00:48:17.900]   I don't know if that's a fatal flaw or if, you know,
[00:48:17.900 --> 00:48:20.060]   or if that's just the way things have to be.
[00:48:20.060 --> 00:48:22.180]   I don't know what's going on inside mechanistically,
[00:48:22.180 --> 00:48:23.300]   and I think that's the whole point
[00:48:23.300 --> 00:48:24.980]   of mechanistic interoperability,
[00:48:25.020 --> 00:48:28.460]   to really understand what's going on inside the models
[00:48:28.460 --> 00:48:30.540]   at the level of individual circuits.
[00:48:30.540 --> 00:48:31.700]   - Eventually when it's solved,
[00:48:31.700 --> 00:48:33.300]   what does the solution look like?
[00:48:33.300 --> 00:48:36.100]   What is it the case where if you're Cloud4,
[00:48:36.100 --> 00:48:38.100]   you do the mechanistic interoperability thing
[00:48:38.100 --> 00:48:39.940]   and you're like, "I'm satisfied, it's aligned."
[00:48:39.940 --> 00:48:41.300]   What is it that you've seen?
[00:48:41.300 --> 00:48:44.940]   - Yeah, so I think we don't know that yet.
[00:48:44.940 --> 00:48:47.020]   I think we don't know enough to know that yet.
[00:48:47.020 --> 00:48:48.620]   I mean, I can give you a sketch
[00:48:48.620 --> 00:48:50.900]   for like what the process looks like
[00:48:50.900 --> 00:48:53.740]   as opposed to what the final result looks like.
[00:48:53.740 --> 00:48:56.700]   So I think verifiability is a lot of the challenge here,
[00:48:56.700 --> 00:48:57.540]   right?
[00:48:57.540 --> 00:49:01.420]   We have all these methods that purport to align AI systems
[00:49:01.420 --> 00:49:04.860]   and do succeed at doing so for today's tasks.
[00:49:04.860 --> 00:49:07.340]   But then the question is always,
[00:49:07.340 --> 00:49:08.860]   if you had a more powerful model
[00:49:08.860 --> 00:49:11.420]   or if you had a model in a different situation,
[00:49:11.420 --> 00:49:13.180]   would it be aligned?
[00:49:13.180 --> 00:49:15.380]   And so I think this problem would be much easier
[00:49:15.380 --> 00:49:18.420]   if you had an Oracle that could just scan a model
[00:49:18.420 --> 00:49:21.060]   and say like, "Okay, I know this model is aligned.
[00:49:21.060 --> 00:49:23.940]   "I know what it'll do in every situation."
[00:49:23.940 --> 00:49:25.740]   Then the problem would be much easier.
[00:49:25.740 --> 00:49:27.860]   And I think the closest thing we have to that
[00:49:27.860 --> 00:49:30.460]   is something like mechanistic interpretability.
[00:49:30.460 --> 00:49:33.020]   It's not anywhere near up to the task yet.
[00:49:33.020 --> 00:49:34.860]   But I guess I would say,
[00:49:34.860 --> 00:49:37.500]   I think of it as almost like an extended training set
[00:49:37.500 --> 00:49:39.500]   and an extended test set, right?
[00:49:39.500 --> 00:49:40.420]   Everything we're doing,
[00:49:40.420 --> 00:49:41.660]   all the alignment methods we're doing
[00:49:41.660 --> 00:49:43.120]   are the training set, right?
[00:49:43.120 --> 00:49:45.440]   You can run tests in them,
[00:49:45.440 --> 00:49:47.020]   but will it really work out a distribution?
[00:49:47.020 --> 00:49:48.980]   Will it really work in another situation?
[00:49:48.980 --> 00:49:51.380]   Mechanistic interpretability is the only thing
[00:49:51.380 --> 00:49:53.060]   that even in principle,
[00:49:53.060 --> 00:49:54.620]   and we're nowhere near there yet,
[00:49:54.620 --> 00:49:57.420]   but even in principle is the thing where it's like,
[00:49:57.420 --> 00:49:59.700]   it's more like an X-ray of the model
[00:49:59.700 --> 00:50:01.100]   than a modification of the model, right?
[00:50:01.100 --> 00:50:03.980]   It's more like an assessment than an intervention.
[00:50:03.980 --> 00:50:07.100]   And so somehow we need to get into a dynamic
[00:50:07.100 --> 00:50:09.300]   where we have an extended test set,
[00:50:09.300 --> 00:50:10.700]   an extended training set,
[00:50:10.700 --> 00:50:12.380]   which is all these alignment methods
[00:50:12.380 --> 00:50:13.780]   and an extended test set,
[00:50:13.780 --> 00:50:18.380]   which is kind of like you X-ray the model and say like,
[00:50:18.380 --> 00:50:20.180]   okay, what works and what didn't,
[00:50:20.180 --> 00:50:22.980]   in a way that goes beyond just the empirical tests
[00:50:22.980 --> 00:50:25.660]   that you've run, right?
[00:50:25.660 --> 00:50:26.500]   Where you're saying,
[00:50:26.500 --> 00:50:30.940]   what is the model going to do in these situations?
[00:50:30.940 --> 00:50:32.780]   What is it within its capabilities to do
[00:50:32.780 --> 00:50:36.100]   instead of what did it do phenomenologically?
[00:50:36.100 --> 00:50:38.480]   And of course we have to be careful about that, right?
[00:50:38.480 --> 00:50:40.680]   One of the things I think is very important
[00:50:40.680 --> 00:50:42.700]   is we should never train for interpretability
[00:50:42.700 --> 00:50:46.160]   because I think that's taking away that advantage, right?
[00:50:46.160 --> 00:50:47.580]   You even have the problem,
[00:50:47.580 --> 00:50:49.780]   similar to like validation versus test set
[00:50:49.780 --> 00:50:52.860]   where like, if you look at the X-ray too many times,
[00:50:52.860 --> 00:50:53.820]   you can interfere.
[00:50:53.820 --> 00:50:56.780]   But I think that's a much weaker,
[00:50:56.780 --> 00:50:57.700]   we should worry about that,
[00:50:57.700 --> 00:50:59.980]   but that's a much weaker process.
[00:50:59.980 --> 00:51:01.980]   It's not automated optimization.
[00:51:01.980 --> 00:51:04.340]   We should just make sure as with validation and test sets
[00:51:04.340 --> 00:51:07.460]   that we don't look at the validation set too many times
[00:51:07.460 --> 00:51:09.000]   before running the test set.
[00:51:09.000 --> 00:51:12.100]   But again, that's more of a,
[00:51:12.100 --> 00:51:14.580]   that's manual pressure rather than automated pressure.
[00:51:14.580 --> 00:51:18.140]   And so some solution where it's like,
[00:51:18.140 --> 00:51:20.820]   we have some dynamic between the training and test set
[00:51:20.820 --> 00:51:23.300]   where it's like, we're trying things out
[00:51:23.300 --> 00:51:26.320]   and we really figure out if they work
[00:51:26.320 --> 00:51:27.660]   via way of testing them
[00:51:27.660 --> 00:51:29.660]   that the model isn't optimizing against,
[00:51:29.660 --> 00:51:31.180]   some orthogonal way.
[00:51:31.180 --> 00:51:33.100]   Like if I think of,
[00:51:33.100 --> 00:51:34.820]   and I think we're never going to have a guarantee,
[00:51:34.820 --> 00:51:38.580]   but some process where we do those things together,
[00:51:38.580 --> 00:51:39.920]   again, not in a stupid way,
[00:51:39.920 --> 00:51:41.420]   there's lots of stupid ways to do this
[00:51:41.420 --> 00:51:42.580]   where you fool yourself,
[00:51:42.580 --> 00:51:46.860]   but like some way to put extended training
[00:51:46.860 --> 00:51:48.300]   for alignment ability
[00:51:48.300 --> 00:51:51.300]   with extended testing for alignment ability
[00:51:51.300 --> 00:51:53.800]   together in a way that actually works.
[00:51:53.800 --> 00:51:56.620]   - I still don't feel like I understand the intuition
[00:51:56.620 --> 00:51:59.580]   that why you think this is likely to work
[00:51:59.580 --> 00:52:00.660]   or this is a promising to pursue.
[00:52:00.660 --> 00:52:03.480]   And let me ask the question in a certain more specific way
[00:52:03.480 --> 00:52:05.700]   and excuse the tortured analogy.
[00:52:05.700 --> 00:52:08.140]   But listen, if you're an economist
[00:52:08.140 --> 00:52:09.780]   and you want to understand the economy,
[00:52:09.780 --> 00:52:12.500]   so you send a whole bunch of micro economists out there
[00:52:12.500 --> 00:52:14.500]   and one of them studies how the restaurant business works.
[00:52:14.500 --> 00:52:16.580]   One of them studies how the tourism business works.
[00:52:16.580 --> 00:52:19.020]   You know, one of them studies how the baking works.
[00:52:19.020 --> 00:52:21.420]   And at the end, they all come together
[00:52:21.420 --> 00:52:22.260]   and you still don't know
[00:52:22.260 --> 00:52:23.100]   whether there's going to be a recession
[00:52:23.100 --> 00:52:24.300]   in five years or not.
[00:52:24.300 --> 00:52:25.820]   Why is this not like that
[00:52:25.820 --> 00:52:27.380]   where you have an understanding of,
[00:52:27.380 --> 00:52:28.860]   we understand how induction heads work
[00:52:28.860 --> 00:52:30.060]   in a two layer transformer.
[00:52:30.060 --> 00:52:32.500]   We understand, you know, modular arithmetic.
[00:52:32.500 --> 00:52:34.220]   How does this add up to,
[00:52:34.220 --> 00:52:35.920]   does this model want to kill us?
[00:52:35.920 --> 00:52:37.780]   Like what does this model fundamentally want?
[00:52:37.780 --> 00:52:38.620]   - A few things on that.
[00:52:38.620 --> 00:52:40.820]   I mean, I think that's like the right set of questions
[00:52:40.820 --> 00:52:41.820]   to ask.
[00:52:41.820 --> 00:52:43.860]   I think what we're hoping for in the end
[00:52:43.860 --> 00:52:46.380]   is not that we'll understand every detail,
[00:52:46.380 --> 00:52:48.180]   but again, I would give like the x-ray
[00:52:48.180 --> 00:52:50.420]   or the MRI analogy that like,
[00:52:50.420 --> 00:52:52.900]   we can be in a position where we can
[00:52:52.900 --> 00:52:55.100]   look at the broad features of the model
[00:52:55.100 --> 00:52:57.420]   and say like, is this a model
[00:52:57.420 --> 00:52:59.540]   whose internal state and plans
[00:52:59.540 --> 00:53:01.180]   are very different from what it
[00:53:01.180 --> 00:53:03.500]   externally represents itself to do, right?
[00:53:03.500 --> 00:53:06.740]   Is this a model where we're uncomfortable
[00:53:06.740 --> 00:53:10.340]   that, you know, far too much of its computational power
[00:53:10.340 --> 00:53:13.580]   is devoted to doing what look like
[00:53:13.580 --> 00:53:16.020]   fairly destructive and manipulative things.
[00:53:16.020 --> 00:53:18.700]   Again, we don't know for sure whether that's possible,
[00:53:18.700 --> 00:53:22.460]   but I think some at least positive signs
[00:53:22.460 --> 00:53:23.300]   that it might be possible,
[00:53:23.300 --> 00:53:26.860]   again, the model is not intentionally hiding from you, right?
[00:53:26.860 --> 00:53:28.700]   It might turn out that the training process
[00:53:28.700 --> 00:53:29.540]   hides it from you.
[00:53:29.540 --> 00:53:30.900]   And, you know, I can think of cases
[00:53:30.900 --> 00:53:32.580]   where the model is really super intelligent.
[00:53:32.580 --> 00:53:34.020]   It like thinks in a way
[00:53:34.020 --> 00:53:36.900]   so that it like affects its own cognition.
[00:53:36.900 --> 00:53:39.060]   I suspect we should think about that.
[00:53:39.060 --> 00:53:40.260]   We should consider everything.
[00:53:40.260 --> 00:53:44.700]   I suspect that it may roughly work
[00:53:44.700 --> 00:53:46.700]   to think of the model as, you know,
[00:53:46.700 --> 00:53:49.540]   if it's trained in the normal way,
[00:53:49.540 --> 00:53:53.860]   just at the just getting to just above human level,
[00:53:53.860 --> 00:53:56.460]   it may be a reason we should check.
[00:53:56.460 --> 00:53:58.300]   It may be a reasonable assumption
[00:53:58.300 --> 00:54:00.860]   that the internal structure of the model
[00:54:00.860 --> 00:54:03.420]   is not intentionally optimizing against us.
[00:54:03.420 --> 00:54:05.860]   And I give an analogy like to humans.
[00:54:05.860 --> 00:54:10.300]   So it's actually possible to, you know,
[00:54:10.300 --> 00:54:12.180]   to look at an MRI of someone
[00:54:12.180 --> 00:54:14.940]   and predict above random chance
[00:54:14.940 --> 00:54:16.700]   whether they're a psychopath.
[00:54:16.700 --> 00:54:18.460]   There was actually a story a few years back
[00:54:18.460 --> 00:54:21.220]   about a neuroscientist who was studying this,
[00:54:21.220 --> 00:54:22.460]   and he looked at his own scan
[00:54:22.460 --> 00:54:24.260]   and discovered that he was a psychopath.
[00:54:24.260 --> 00:54:26.460]   And then everyone in his life was like,
[00:54:26.460 --> 00:54:27.780]   "No, no, no, this is obvious.
[00:54:27.780 --> 00:54:29.340]   Like you're a complete asshole.
[00:54:29.340 --> 00:54:31.380]   Like you must be a psychopath."
[00:54:31.380 --> 00:54:34.140]   And he was totally unaware of this.
[00:54:34.140 --> 00:54:36.700]   The basic idea that, you know,
[00:54:36.700 --> 00:54:39.140]   that there can be these macro features
[00:54:39.140 --> 00:54:41.660]   that like psychopath is probably a good analogy for it.
[00:54:41.660 --> 00:54:42.500]   Right?
[00:54:42.500 --> 00:54:43.340]   They're like, you know,
[00:54:43.340 --> 00:54:44.180]   this is what we'd be afraid of.
[00:54:44.180 --> 00:54:46.460]   Model that's kind of like charming on the surface,
[00:54:46.460 --> 00:54:49.980]   very goal-oriented and, you know, very dark on the inside.
[00:54:49.980 --> 00:54:51.700]   You know, and, you know, on the surface,
[00:54:51.700 --> 00:54:53.940]   their behavior might look like the behavior of someone else,
[00:54:53.940 --> 00:54:55.380]   but their goals are very different.
[00:54:55.380 --> 00:54:57.340]   - A question somebody might have is, listen,
[00:54:57.340 --> 00:54:58.460]   you know, you mentioned earlier
[00:54:58.460 --> 00:55:00.220]   the importance of being empirical.
[00:55:00.220 --> 00:55:01.060]   - Yeah.
[00:55:01.060 --> 00:55:03.180]   - And in this case, you're trying to estimate,
[00:55:03.180 --> 00:55:05.780]   you know, listen, are these activations sus?
[00:55:05.780 --> 00:55:06.620]   - Yeah.
[00:55:06.620 --> 00:55:09.460]   - But is this something we can be afford
[00:55:09.460 --> 00:55:12.860]   to be empirical about in, on, you know,
[00:55:12.860 --> 00:55:15.020]   or do we need like a very good first principle
[00:55:15.020 --> 00:55:16.740]   theoretical reason to think,
[00:55:16.740 --> 00:55:19.180]   no, it's not just that these MRIs of the model
[00:55:19.180 --> 00:55:21.740]   correlate with, you know, being bad.
[00:55:21.740 --> 00:55:25.620]   We need just like some, just like deep root math proof
[00:55:25.620 --> 00:55:26.460]   that this is aligned.
[00:55:26.460 --> 00:55:28.100]   - So it depends what you mean by empirical.
[00:55:28.100 --> 00:55:30.060]   I mean, a better term would be phenomenological.
[00:55:30.060 --> 00:55:32.900]   Like I don't think we should be purely phenomenological
[00:55:32.900 --> 00:55:35.020]   in like, you know, here are some brain scans
[00:55:35.020 --> 00:55:36.980]   of like really dangerous models,
[00:55:36.980 --> 00:55:38.380]   and here are some brain scans.
[00:55:38.380 --> 00:55:41.420]   I think the whole idea of mechanistic interpretability
[00:55:41.420 --> 00:55:44.260]   is to look at the underlying principles and circuits.
[00:55:44.260 --> 00:55:46.900]   But I guess the way I think about it is like,
[00:55:46.900 --> 00:55:50.340]   on one hand, I've actually always been a fan
[00:55:50.340 --> 00:55:53.180]   of studying these circuits at the lowest level of detail
[00:55:53.180 --> 00:55:54.540]   that we possibly can.
[00:55:54.540 --> 00:55:56.100]   And the reason for that is kind of,
[00:55:56.100 --> 00:55:57.500]   that's how you build up knowledge.
[00:55:57.500 --> 00:55:59.820]   Even if you're ultimately aiming for,
[00:55:59.820 --> 00:56:02.020]   there's too many of these features,
[00:56:02.020 --> 00:56:02.980]   it's too complicated.
[00:56:02.980 --> 00:56:03.820]   At the end of the day,
[00:56:03.820 --> 00:56:05.740]   we're trying to build something broad,
[00:56:05.740 --> 00:56:08.300]   and we're trying to build some broad understanding.
[00:56:08.300 --> 00:56:10.420]   I think the way you build that up
[00:56:10.420 --> 00:56:12.020]   is by trying to make a lot of these
[00:56:12.020 --> 00:56:13.180]   very specific discoveries.
[00:56:13.180 --> 00:56:16.100]   Like you have to understand the building blocks,
[00:56:16.100 --> 00:56:19.220]   and then you have to figure out how to kind of use that
[00:56:19.220 --> 00:56:21.220]   to draw these broad conclusions,
[00:56:21.220 --> 00:56:23.820]   even if you're not gonna figure out everything.
[00:56:23.820 --> 00:56:26.380]   You know, I think you should probably talk to Chris Ola,
[00:56:26.380 --> 00:56:28.260]   who would have much more detail, right?
[00:56:28.260 --> 00:56:30.540]   This is my kind of high level thinking on it.
[00:56:30.540 --> 00:56:33.340]   Like Chris Ola controls the interpretability agenda.
[00:56:33.340 --> 00:56:35.380]   Like, you know, he's the one who decides
[00:56:35.380 --> 00:56:37.700]   what to do on interpretability.
[00:56:37.700 --> 00:56:39.700]   This is my high level thinking about it,
[00:56:39.700 --> 00:56:41.060]   which is not gonna be as good as his.
[00:56:41.060 --> 00:56:42.980]   - Does the book "Case Unanthropic"
[00:56:42.980 --> 00:56:45.460]   rely on the fact that mechanistic interpretability
[00:56:45.460 --> 00:56:47.100]   is helpful for capabilities?
[00:56:47.100 --> 00:56:49.260]   - I don't think so at all.
[00:56:49.260 --> 00:56:52.620]   Now I do think, I think in principle,
[00:56:52.620 --> 00:56:54.780]   it's possible that mechanistic interpretability
[00:56:54.780 --> 00:56:56.860]   could be helpful with capabilities.
[00:56:56.860 --> 00:56:58.020]   We might, for various reasons,
[00:56:58.020 --> 00:57:01.260]   not choose to talk about it, if that were the case.
[00:57:01.260 --> 00:57:04.980]   That, you know, that wasn't something that I thought of,
[00:57:04.980 --> 00:57:05.860]   or that any of us thought of
[00:57:05.860 --> 00:57:07.540]   at the time of Anthropic's founding.
[00:57:07.540 --> 00:57:10.500]   I mean, we thought of ourselves as like, you know,
[00:57:10.500 --> 00:57:13.540]   we're people who are like good at scaling models
[00:57:13.540 --> 00:57:16.060]   and good at doing safety on top of those models.
[00:57:16.060 --> 00:57:16.900]   And like, you know,
[00:57:16.900 --> 00:57:19.060]   we think that we have a very high talent density
[00:57:19.060 --> 00:57:20.620]   of folks who are good at that.
[00:57:20.620 --> 00:57:22.780]   And, you know, my view has always been
[00:57:22.780 --> 00:57:24.740]   talent density beats talent mass.
[00:57:24.780 --> 00:57:28.180]   And so, you know, that's more of our bold case.
[00:57:28.180 --> 00:57:29.980]   Talent density beats talent mass.
[00:57:29.980 --> 00:57:32.820]   I don't think it depends on some particular thing.
[00:57:32.820 --> 00:57:33.820]   Like others are starting to do
[00:57:33.820 --> 00:57:35.260]   mechanistic interpretability now,
[00:57:35.260 --> 00:57:37.260]   and I'm very glad that they are.
[00:57:37.260 --> 00:57:41.180]   You know, that was a part of our theory of change
[00:57:41.180 --> 00:57:44.180]   is paradoxically to make other organizations more like us.
[00:57:44.180 --> 00:57:45.940]   - Talent density, I'm sure is important,
[00:57:45.940 --> 00:57:48.460]   but another thing Anthropic has emphasized
[00:57:48.460 --> 00:57:50.580]   is that you need to have frontier models
[00:57:50.580 --> 00:57:52.180]   in order to do safety research.
[00:57:52.180 --> 00:57:54.300]   And of course, like actually be a company as well.
[00:57:54.300 --> 00:57:55.620]   The current frontier models,
[00:57:55.620 --> 00:57:57.660]   something somebody might guess like GPT 4 o'clock
[00:57:57.660 --> 00:57:59.820]   to like a hundred million dollars, something like that.
[00:57:59.820 --> 00:58:01.540]   - That general order of magnitude
[00:58:01.540 --> 00:58:03.020]   in very broad terms is not wrong.
[00:58:03.020 --> 00:58:04.780]   - But, you know, we're two to three years from now,
[00:58:04.780 --> 00:58:06.300]   the kinds of things you're talking about,
[00:58:06.300 --> 00:58:09.100]   we're talking more and more orders of magnitude
[00:58:09.100 --> 00:58:10.700]   to keep up with that.
[00:58:10.700 --> 00:58:13.060]   And to, if it's the case that safety requires
[00:58:13.060 --> 00:58:14.580]   to be on the frontier,
[00:58:14.580 --> 00:58:16.340]   I mean, what is the case in which Anthropic
[00:58:16.340 --> 00:58:18.740]   is like competing with these Leviathans
[00:58:18.740 --> 00:58:20.060]   to stay on that same scale?
[00:58:20.060 --> 00:58:22.020]   - Yeah, I mean, I think it's a very,
[00:58:22.020 --> 00:58:24.020]   it's a situation with a lot of trade-offs, right?
[00:58:24.020 --> 00:58:26.660]   I think it's not easy.
[00:58:26.660 --> 00:58:27.900]   I guess to go back,
[00:58:27.900 --> 00:58:29.860]   maybe I'll just like answer the questions one by one, right?
[00:58:29.860 --> 00:58:31.740]   So like to go back to like,
[00:58:31.740 --> 00:58:35.180]   you know, why is safety so tied to scale, right?
[00:58:35.180 --> 00:58:36.380]   Some people don't think it is,
[00:58:36.380 --> 00:58:38.580]   but like, if I just look at like,
[00:58:38.580 --> 00:58:42.740]   you know, where have been the areas that, you know,
[00:58:42.740 --> 00:58:43.940]   I don't know, like safety methods
[00:58:43.940 --> 00:58:45.700]   have like been put into practice
[00:58:45.700 --> 00:58:47.780]   or like worked for something, for anything,
[00:58:47.780 --> 00:58:50.380]   even if we don't think they'll work in general.
[00:58:50.380 --> 00:58:52.860]   You know, I go back to thinking of all the ideas,
[00:58:52.860 --> 00:58:54.700]   you know, something like, you know,
[00:58:54.700 --> 00:58:56.260]   debate and amplification, right?
[00:58:56.260 --> 00:58:57.780]   You know, back in 2018,
[00:58:57.780 --> 00:59:00.500]   when we wrote papers about those at OpenAI,
[00:59:00.500 --> 00:59:03.780]   it was like, well, human feedback isn't quite gonna work,
[00:59:03.780 --> 00:59:05.380]   but, you know, debate and amplification
[00:59:05.380 --> 00:59:06.940]   will take us beyond that.
[00:59:06.940 --> 00:59:09.620]   But then if you actually look at,
[00:59:09.620 --> 00:59:12.340]   and we've, you know, done attempts to do debates,
[00:59:12.340 --> 00:59:15.860]   we're really limited by the quality of the model,
[00:59:15.860 --> 00:59:17.220]   where it's like, you know,
[00:59:17.220 --> 00:59:21.980]   for two models to have a debate that is coherent enough
[00:59:21.980 --> 00:59:23.260]   that a human can judge it
[00:59:23.260 --> 00:59:25.860]   so that the training process can actually work,
[00:59:25.860 --> 00:59:27.260]   you need models that are at,
[00:59:27.260 --> 00:59:30.500]   or maybe even beyond on some topics, the current frontier.
[00:59:30.500 --> 00:59:32.140]   Now, you can come up with a method,
[00:59:32.140 --> 00:59:33.740]   you can come up with the idea
[00:59:33.740 --> 00:59:35.420]   without being on the frontier,
[00:59:35.420 --> 00:59:37.340]   but, you know, for me,
[00:59:37.340 --> 00:59:40.100]   that's a very small fraction of what needs to be done, right?
[00:59:40.100 --> 00:59:41.900]   It's very easy to come up with these methods,
[00:59:41.900 --> 00:59:43.580]   it's very easy to come up with like,
[00:59:43.580 --> 00:59:46.580]   oh, the problem is X, maybe a solution is Y,
[00:59:46.580 --> 00:59:49.460]   but, you know, I really wanna know,
[00:59:49.460 --> 00:59:51.260]   you know, whether things work in practice,
[00:59:51.260 --> 00:59:53.380]   even for the systems we have today,
[00:59:53.380 --> 00:59:55.580]   and I wanna know what kinds of things go wrong with them.
[00:59:55.580 --> 00:59:58.780]   I just feel like you discover 10 new ideas
[00:59:58.780 --> 01:00:00.740]   and 10 new ways that things are gonna go wrong
[01:00:00.740 --> 01:00:02.180]   by trying these in practice.
[01:00:02.180 --> 01:00:05.220]   And that empirical learning,
[01:00:05.220 --> 01:00:08.540]   I think it's just not as widely understood as it should be.
[01:00:08.540 --> 01:00:10.140]   Kind of, you know, I would say the same thing
[01:00:10.140 --> 01:00:12.340]   about methods like constitutional AI.
[01:00:12.340 --> 01:00:13.820]   And some people say, oh, it doesn't matter,
[01:00:13.820 --> 01:00:15.700]   like we know this method doesn't work,
[01:00:15.700 --> 01:00:18.020]   it won't work for, you know, pure alignment.
[01:00:18.020 --> 01:00:20.220]   I neither agree nor disagree with that.
[01:00:20.220 --> 01:00:22.100]   I think that's just kind of overconfident.
[01:00:22.100 --> 01:00:23.940]   The way we discover new things
[01:00:23.940 --> 01:00:25.860]   and understand the structure of what's gonna work
[01:00:25.860 --> 01:00:28.740]   and what's not is by playing around with things.
[01:00:28.740 --> 01:00:30.580]   Not that we should just kind of blindly say,
[01:00:30.580 --> 01:00:32.900]   oh, this worked here and so it'll work there,
[01:00:32.900 --> 01:00:36.180]   but you really start to understand the patterns,
[01:00:36.180 --> 01:00:38.300]   like with the scaling laws.
[01:00:38.300 --> 01:00:39.940]   Even mechanistic interpretability,
[01:00:39.940 --> 01:00:42.620]   which might be the one area I see
[01:00:42.620 --> 01:00:44.220]   where a lot of progress has been made
[01:00:44.220 --> 01:00:46.220]   without the frontier models.
[01:00:46.220 --> 01:00:47.740]   We're, you know, we're seeing in, you know,
[01:00:47.740 --> 01:00:51.620]   the work that say OpenAI put out a couple months ago
[01:00:51.620 --> 01:00:54.540]   that, you know, using very powerful models
[01:00:54.540 --> 01:00:57.740]   to help you auto interpret the weak models.
[01:00:57.740 --> 01:01:00.140]   Again, that's not everything you can do in interpretability,
[01:01:00.140 --> 01:01:02.660]   but, you know, that's a big component of it.
[01:01:02.660 --> 01:01:05.500]   And we, you know, we found it useful too.
[01:01:05.500 --> 01:01:09.660]   And so you see this phenomenon over and over again,
[01:01:09.660 --> 01:01:12.860]   where it's like, you know, the scaling and the safety
[01:01:12.860 --> 01:01:16.020]   are these two snakes that are like coiled with each other,
[01:01:16.020 --> 01:01:17.580]   always even more than you think, right?
[01:01:17.580 --> 01:01:19.420]   I, you know, with interpretability,
[01:01:19.420 --> 01:01:20.980]   like I think three years ago,
[01:01:20.980 --> 01:01:22.580]   I didn't think that this would be as true
[01:01:22.580 --> 01:01:25.580]   of interpretability, but somehow it manages to be true.
[01:01:25.580 --> 01:01:27.780]   Why? Because intelligence is useful.
[01:01:27.780 --> 01:01:29.420]   It's useful for a number of tasks.
[01:01:29.420 --> 01:01:32.260]   One of the tasks it's useful for is like figuring out
[01:01:32.260 --> 01:01:34.820]   how to judge and evaluate other intelligence,
[01:01:34.820 --> 01:01:36.700]   and maybe someday even for, you know,
[01:01:36.700 --> 01:01:38.500]   doing the alignment research itself.
[01:01:38.500 --> 01:01:39.460]   - Given all that's true,
[01:01:39.460 --> 01:01:41.300]   what does that imply for Anthropic
[01:01:41.300 --> 01:01:43.220]   when in two to three years these Leviathans
[01:01:43.220 --> 01:01:45.540]   are doing like $10 billion training runs?
[01:01:45.540 --> 01:01:49.580]   - Choice one is if we can't, or if it costs too much
[01:01:49.580 --> 01:01:51.820]   to stay on the frontier, then, you know,
[01:01:51.820 --> 01:01:54.460]   then we shouldn't do it.
[01:01:54.460 --> 01:01:56.980]   And, you know, we won't work with the most advanced models.
[01:01:56.980 --> 01:01:58.700]   We'll see what we can get with, you know,
[01:01:58.700 --> 01:02:00.500]   models that are not quite as advanced.
[01:02:00.500 --> 01:02:03.380]   I think you can get some value there, like non-zero value,
[01:02:03.380 --> 01:02:07.380]   but I'm kind of skeptical that the value is all that high
[01:02:07.380 --> 01:02:08.900]   or the learning can be fast enough
[01:02:08.900 --> 01:02:11.500]   to really be in favor of the task.
[01:02:11.500 --> 01:02:14.220]   The second option is you just find a way.
[01:02:14.220 --> 01:02:17.740]   You just, you know, you just accept the trade-offs.
[01:02:17.740 --> 01:02:20.900]   And I think the trade-offs are more positive
[01:02:20.900 --> 01:02:23.260]   than they appear because of a phenomenon
[01:02:23.260 --> 01:02:25.420]   that I've called race to the top.
[01:02:25.420 --> 01:02:26.700]   I could go into that later,
[01:02:26.700 --> 01:02:29.300]   but I'll just, let me put that aside for now.
[01:02:29.300 --> 01:02:31.900]   And then I think the third phenomenon is, you know,
[01:02:31.900 --> 01:02:35.060]   as things get to that scale,
[01:02:35.060 --> 01:02:38.260]   I think this may coincide with, you know,
[01:02:38.260 --> 01:02:41.820]   starting to get into some non-trivial probability
[01:02:41.820 --> 01:02:43.540]   of very serious danger.
[01:02:43.540 --> 01:02:46.380]   Again, I think it's gonna come first from misuse,
[01:02:46.380 --> 01:02:48.700]   the kind of bio stuff that I talked about,
[01:02:48.700 --> 01:02:51.740]   but I don't think we have the level of autonomy yet
[01:02:51.740 --> 01:02:54.220]   to worry about some of the, you know,
[01:02:54.220 --> 01:02:57.820]   alignment stuff happening in like two years,
[01:02:57.820 --> 01:03:01.020]   but it might not be very far behind that at all.
[01:03:01.020 --> 01:03:06.020]   You know, that may lead to unilateral or multilateral
[01:03:06.020 --> 01:03:09.020]   or government-enforced, which we support,
[01:03:09.020 --> 01:03:13.180]   decisions not to scale as fast as we could.
[01:03:13.180 --> 01:03:14.700]   That may end up being the right thing to do.
[01:03:14.700 --> 01:03:16.980]   So, you know, actually that's kind of like,
[01:03:16.980 --> 01:03:19.660]   I kind of hope things go in that direction.
[01:03:19.660 --> 01:03:21.540]   And then we don't have this hard trade-off
[01:03:21.540 --> 01:03:22.820]   between we're not on the frontier
[01:03:22.820 --> 01:03:25.980]   and we can't quite do the research as well as we want
[01:03:25.980 --> 01:03:28.500]   or influence other orgs as well as we want,
[01:03:28.500 --> 01:03:30.940]   or versus we're kind of on the frontier
[01:03:30.940 --> 01:03:33.060]   and like have to accept the trade-offs,
[01:03:33.060 --> 01:03:34.780]   which are net positive,
[01:03:34.780 --> 01:03:37.780]   but like have a lot in both directions.
[01:03:37.780 --> 01:03:39.900]   - Okay, on the misuse versus misalignment,
[01:03:39.900 --> 01:03:41.740]   those are both problems, as you mentioned,
[01:03:41.740 --> 01:03:43.980]   but in the long scheme of things,
[01:03:43.980 --> 01:03:46.180]   what are you more concerned about?
[01:03:46.180 --> 01:03:47.540]   Like 30 years down the line,
[01:03:47.540 --> 01:03:49.540]   which do you think will be considered a bigger problem?
[01:03:49.540 --> 01:03:51.620]   - I think it's gonna be much less than 30 years,
[01:03:51.620 --> 01:03:53.380]   but I'm worried about both.
[01:03:53.380 --> 01:03:57.900]   I don't know, if you have a model that could, in theory,
[01:03:57.900 --> 01:04:00.580]   you know, like take over the world on its own,
[01:04:00.580 --> 01:04:02.100]   if you were able to control that model,
[01:04:02.100 --> 01:04:05.300]   then it follows pretty simply that, you know,
[01:04:05.300 --> 01:04:07.020]   if a model was following the wishes
[01:04:07.020 --> 01:04:09.580]   of some small subset of people and not others,
[01:04:09.580 --> 01:04:10.820]   then those people could use it
[01:04:10.820 --> 01:04:13.220]   to take over the world on their behalf.
[01:04:13.220 --> 01:04:15.980]   The very premise of misalignment
[01:04:15.980 --> 01:04:18.860]   means that we should be worried about misuse as well,
[01:04:18.860 --> 01:04:21.220]   with similar levels of consequences.
[01:04:21.220 --> 01:04:23.220]   - But some people who might be more doomery
[01:04:23.220 --> 01:04:26.860]   than you would say misuse is,
[01:04:26.860 --> 01:04:29.860]   you're already working towards the optimistic scenario there
[01:04:29.860 --> 01:04:31.500]   because you've at least figured out
[01:04:31.500 --> 01:04:34.180]   how to align the model with the bad guys.
[01:04:34.180 --> 01:04:35.620]   Now you just need to make sure
[01:04:35.620 --> 01:04:37.100]   it's aligned with the good guys instead.
[01:04:37.100 --> 01:04:38.580]   Why do you think that you could get to the point
[01:04:38.580 --> 01:04:40.780]   where it's aligned with the bad, you know,
[01:04:40.780 --> 01:04:42.060]   where you haven't already solved it?
[01:04:42.060 --> 01:04:43.220]   - I guess if you had the view
[01:04:43.220 --> 01:04:45.660]   that like alignment is completely unsolvable,
[01:04:45.660 --> 01:04:47.820]   then you'd be like, well, I don't, you know,
[01:04:47.820 --> 01:04:50.340]   we're dead anyway, so I don't wanna worry about misuse.
[01:04:50.340 --> 01:04:51.660]   That's not my position at all.
[01:04:51.660 --> 01:04:54.180]   But also like, you should think in terms of like,
[01:04:54.180 --> 01:04:56.460]   what's a plan that would actually succeed
[01:04:56.460 --> 01:04:57.780]   that would make things good?
[01:04:57.780 --> 01:04:59.860]   Any plan that actually succeeds,
[01:04:59.860 --> 01:05:02.700]   regardless of how hard misalignment is to solve,
[01:05:02.700 --> 01:05:05.460]   any problem, any plan that actually succeeds
[01:05:05.460 --> 01:05:08.020]   is gonna need to solve misuse as well as misalignment.
[01:05:08.020 --> 01:05:10.380]   It's gonna need to solve the fact that like,
[01:05:10.380 --> 01:05:14.060]   as the AI models get better, you know, faster and faster,
[01:05:14.060 --> 01:05:15.540]   they're gonna create a big problem
[01:05:15.540 --> 01:05:17.860]   around the balance of power between countries.
[01:05:17.860 --> 01:05:20.060]   They're gonna create a big problem around
[01:05:20.060 --> 01:05:22.940]   is it possible for a single individual to do something bad
[01:05:22.940 --> 01:05:25.460]   that it's hard for everyone else to stop?
[01:05:25.460 --> 01:05:28.420]   Any actual solution that leads to a good future
[01:05:28.420 --> 01:05:30.220]   needs to solve those problems as well.
[01:05:30.220 --> 01:05:32.260]   If your perspective is we're screwed
[01:05:32.260 --> 01:05:33.980]   because we can't solve the first problem,
[01:05:33.980 --> 01:05:36.100]   so don't worry about problems two and three,
[01:05:36.100 --> 01:05:38.060]   like that's not really a statement
[01:05:38.060 --> 01:05:39.940]   you shouldn't worry about problems two and three, right?
[01:05:39.940 --> 01:05:43.100]   Like they're in our path no matter what.
[01:05:43.100 --> 01:05:45.140]   - Yeah, in the scenario we succeed, we have to solve all.
[01:05:45.140 --> 01:05:46.380]   So yeah, we might as well operate.
[01:05:46.380 --> 01:05:49.100]   - We should be planning for success, not for failure.
[01:05:49.100 --> 01:05:50.300]   - If misuse doesn't happen
[01:05:50.300 --> 01:05:53.140]   and the right people have the superhuman models,
[01:05:53.140 --> 01:05:54.300]   what does that look like?
[01:05:54.300 --> 01:05:55.460]   Like who are the right people?
[01:05:55.460 --> 01:05:57.580]   Who is actually controlling the model
[01:05:57.580 --> 01:05:58.620]   from five years from now?
[01:05:58.620 --> 01:06:01.500]   - Yeah, I mean, my view is that
[01:06:01.500 --> 01:06:04.900]   these things are powerful enough that I think, you know,
[01:06:04.900 --> 01:06:08.300]   it's going to involve, you know, substantial role
[01:06:08.300 --> 01:06:10.820]   or at least involvement of, you know,
[01:06:10.820 --> 01:06:14.620]   some kind of government or assembly of government bodies.
[01:06:14.620 --> 01:06:15.940]   Again, like, you know,
[01:06:15.940 --> 01:06:18.540]   there are kind of very naive versions of this.
[01:06:18.540 --> 01:06:20.900]   Like, you know, I don't think we should just, you know,
[01:06:20.900 --> 01:06:23.820]   I don't know, like hand the model over to the UN
[01:06:23.820 --> 01:06:26.220]   or whoever happens to be in office at a given time.
[01:06:26.220 --> 01:06:27.940]   Like I could see that go poorly,
[01:06:27.940 --> 01:06:30.260]   but it's too powerful.
[01:06:30.260 --> 01:06:33.180]   There needs to be some kind of legitimate process
[01:06:33.180 --> 01:06:35.220]   for managing this technology,
[01:06:35.220 --> 01:06:37.340]   which, you know, includes the role
[01:06:37.340 --> 01:06:38.460]   of the people building it,
[01:06:38.460 --> 01:06:40.220]   includes the role of like
[01:06:40.220 --> 01:06:42.140]   democratically elected authorities,
[01:06:42.140 --> 01:06:44.500]   includes the role of, you know,
[01:06:44.500 --> 01:06:46.940]   all the individuals who will be affected by it.
[01:06:46.940 --> 01:06:49.660]   So there, at the end of the day,
[01:06:49.660 --> 01:06:52.620]   there needs to be some politically legitimate process.
[01:06:52.620 --> 01:06:53.460]   - But what does that look like?
[01:06:53.460 --> 01:06:55.140]   If it's not the case that you just hand it
[01:06:55.140 --> 01:06:57.260]   to whoever the president is at the time.
[01:06:57.260 --> 01:06:58.100]   - Yeah.
[01:06:58.100 --> 01:07:00.020]   - What does the body look like?
[01:07:00.020 --> 01:07:00.860]   I mean, is it something you're-
[01:07:00.860 --> 01:07:03.060]   - These are things it's really hard to know ahead of time.
[01:07:03.060 --> 01:07:04.980]   Like, I think, you know,
[01:07:04.980 --> 01:07:07.500]   people love to kind of propose these broad plans
[01:07:07.500 --> 01:07:08.900]   and say like, "Oh, this is the way we should do it.
[01:07:08.900 --> 01:07:10.140]   This is the way we should do it."
[01:07:10.140 --> 01:07:11.700]   I think the honest fact is that
[01:07:11.700 --> 01:07:13.580]   we're figuring this out as we go along.
[01:07:13.580 --> 01:07:16.860]   And that, you know, anyone who says, you know,
[01:07:16.860 --> 01:07:18.620]   this is the body that, you know,
[01:07:18.620 --> 01:07:21.340]   we should create this kind of body modeled after this thing.
[01:07:21.340 --> 01:07:23.660]   Like, I think we should try things
[01:07:23.660 --> 01:07:24.700]   and experiment with them
[01:07:24.700 --> 01:07:27.340]   with less powerful versions of the technology.
[01:07:27.340 --> 01:07:28.980]   We need to figure this out in time,
[01:07:28.980 --> 01:07:31.180]   but also it's not really the kind of thing
[01:07:31.180 --> 01:07:32.540]   you can know in advance.
[01:07:32.540 --> 01:07:35.340]   - The long-term benefit trust that you have,
[01:07:35.340 --> 01:07:37.100]   how would that interface with this body?
[01:07:37.100 --> 01:07:38.340]   Is that the body itself?
[01:07:38.340 --> 01:07:41.100]   If not, is it like, so just for the context,
[01:07:41.100 --> 01:07:42.580]   you might want to explain what it is for the audience,
[01:07:42.580 --> 01:07:43.460]   but I don't know.
[01:07:43.460 --> 01:07:45.100]   I think that the long-term benefit trust
[01:07:45.100 --> 01:07:46.820]   is like a much narrower thing.
[01:07:46.820 --> 01:07:48.460]   Like this is something that like
[01:07:48.460 --> 01:07:50.980]   makes decisions for Anthropic.
[01:07:50.980 --> 01:07:52.780]   So this is basically a body,
[01:07:52.780 --> 01:07:54.900]   is described in a recent Vox article.
[01:07:54.900 --> 01:07:57.380]   We'll be saying more about it in, you know,
[01:07:57.380 --> 01:07:59.020]   later this year.
[01:07:59.180 --> 01:08:02.940]   But it's basically a body that over time
[01:08:02.940 --> 01:08:05.140]   gains the ability to appoint the majority
[01:08:05.140 --> 01:08:07.500]   of the board seats of Anthropic.
[01:08:07.500 --> 01:08:08.820]   And this is so, you know,
[01:08:08.820 --> 01:08:11.100]   it's a mixture of experts in, I'd say,
[01:08:11.100 --> 01:08:14.140]   like AI alignment, national security,
[01:08:14.140 --> 01:08:15.500]   and philanthropy in general.
[01:08:15.500 --> 01:08:18.020]   - But if control is handed to them of Anthropic,
[01:08:18.020 --> 01:08:19.700]   that doesn't imply that control of,
[01:08:19.700 --> 01:08:21.300]   if Anthropic has AGI,
[01:08:21.300 --> 01:08:22.780]   the control of AGI itself is handed to them.
[01:08:22.780 --> 01:08:24.860]   - That doesn't imply that Anthropic
[01:08:24.860 --> 01:08:26.580]   or any other entity should be the entity
[01:08:26.580 --> 01:08:28.780]   that like makes decisions about AGI
[01:08:28.780 --> 01:08:30.180]   on behalf of humanity.
[01:08:30.180 --> 01:08:31.300]   I would think of those as different.
[01:08:31.300 --> 01:08:32.340]   I mean, there's lots of, you know,
[01:08:32.340 --> 01:08:34.980]   like if Anthropic does play a broad role,
[01:08:34.980 --> 01:08:36.820]   then you'd want to like widen that body
[01:08:36.820 --> 01:08:37.660]   to be, you know,
[01:08:37.660 --> 01:08:38.900]   like a whole bunch of different people
[01:08:38.900 --> 01:08:40.020]   from around the world.
[01:08:40.020 --> 01:08:42.540]   Or maybe you construe this as very narrow.
[01:08:42.540 --> 01:08:43.500]   And then, you know,
[01:08:43.500 --> 01:08:46.100]   there's some like broad committee somewhere
[01:08:46.100 --> 01:08:47.700]   that like manages all the AGIs
[01:08:47.700 --> 01:08:50.700]   of all the companies on behalf of anyone.
[01:08:50.700 --> 01:08:51.540]   I don't know.
[01:08:51.540 --> 01:08:54.500]   Like, I think my view is you shouldn't be sort of
[01:08:54.500 --> 01:08:56.460]   overly constructive and utopian.
[01:08:56.460 --> 01:08:58.900]   Like we're dealing with a new problem here.
[01:08:58.900 --> 01:09:02.300]   We need to start thinking now about,
[01:09:02.300 --> 01:09:04.660]   you know, what are the governmental bodies
[01:09:04.660 --> 01:09:07.060]   and structures that could deal with it.
[01:09:07.060 --> 01:09:08.420]   - Okay, so let's forget about governance.
[01:09:08.420 --> 01:09:10.460]   Let's just talk about what this going well looks like.
[01:09:10.460 --> 01:09:12.740]   Obviously there's the things we can all agree on,
[01:09:12.740 --> 01:09:14.540]   you know, cure all the diseases,
[01:09:14.540 --> 01:09:15.620]   you know, solve all the problems,
[01:09:15.620 --> 01:09:17.460]   everything's all humans would say,
[01:09:17.460 --> 01:09:19.180]   I'm down for that.
[01:09:19.180 --> 01:09:20.140]   But now it's 2030,
[01:09:20.140 --> 01:09:21.420]   you've solved all the real problems
[01:09:21.420 --> 01:09:23.420]   that everybody can agree on.
[01:09:23.420 --> 01:09:24.740]   What happens next?
[01:09:24.740 --> 01:09:27.140]   What are we doing with a superhuman God?
[01:09:27.140 --> 01:09:28.660]   - I think I actually want to like, I don't know,
[01:09:28.660 --> 01:09:31.660]   like disagree with the framing or something like this.
[01:09:31.660 --> 01:09:33.940]   I actually get nervous when someone says like,
[01:09:33.940 --> 01:09:35.820]   what are you going to do with the superhuman AI?
[01:09:35.820 --> 01:09:39.260]   Like we've learned a lot of things over the last 150 years
[01:09:39.260 --> 01:09:41.660]   about like markets and democracy
[01:09:41.660 --> 01:09:44.860]   and each person can kind of define for themselves,
[01:09:44.860 --> 01:09:46.900]   like what the best way for them
[01:09:46.900 --> 01:09:48.500]   to have the human experiences
[01:09:48.500 --> 01:09:51.980]   and that, you know, societies work out norms
[01:09:51.980 --> 01:09:53.220]   and what they value in this,
[01:09:53.220 --> 01:09:56.020]   just in this very like complex and decentralized way.
[01:09:56.020 --> 01:09:58.980]   Now, again, if you have these safety problems,
[01:09:58.980 --> 01:10:01.100]   that can be a reason why, you know,
[01:10:01.100 --> 01:10:02.220]   and especially from the government,
[01:10:02.220 --> 01:10:05.180]   there needs to be maybe until we've solved these problems,
[01:10:05.180 --> 01:10:07.460]   a certain amount of like centralized control.
[01:10:07.460 --> 01:10:10.580]   But as a matter of like, we've solved all the problems,
[01:10:10.580 --> 01:10:12.460]   now how do we make things good?
[01:10:12.460 --> 01:10:15.740]   I think that most people, most groups,
[01:10:15.740 --> 01:10:18.140]   most ideologies that started with like,
[01:10:18.140 --> 01:10:20.540]   let's sit down and think over
[01:10:20.540 --> 01:10:22.460]   what the definition of the good life is.
[01:10:22.460 --> 01:10:25.140]   Like, I think most of those have led to disaster.
[01:10:25.140 --> 01:10:25.980]   - But so this vision you have
[01:10:25.980 --> 01:10:28.340]   of a sort of tolerant liberal democracy,
[01:10:28.340 --> 01:10:30.900]   market-oriented system with AGI,
[01:10:30.900 --> 01:10:33.580]   like what is each person has their own AGI?
[01:10:33.580 --> 01:10:34.900]   Like, what does that mean?
[01:10:34.900 --> 01:10:35.740]   - I don't know.
[01:10:35.740 --> 01:10:37.100]   I don't know what it looks like, right?
[01:10:37.100 --> 01:10:38.980]   Like, I guess what I'm saying is like,
[01:10:38.980 --> 01:10:42.140]   we need to solve the kind of important safety problems
[01:10:42.140 --> 01:10:43.900]   and the important externalities.
[01:10:43.900 --> 01:10:47.180]   And then subject to that, you know, which again,
[01:10:47.180 --> 01:10:49.660]   you know, those could be just narrowly about alignment.
[01:10:49.660 --> 01:10:51.900]   There could be a bunch of economic issues
[01:10:51.900 --> 01:10:54.460]   that are super complicated and that we can't solve,
[01:10:54.460 --> 01:10:55.780]   you know, subject to that,
[01:10:55.780 --> 01:10:58.420]   like we should think about what's worked in the past.
[01:10:58.420 --> 01:11:01.820]   And I think in general, like unitary visions
[01:11:01.820 --> 01:11:04.100]   for what it means to live a good life
[01:11:04.100 --> 01:11:05.700]   have not worked out well at all.
[01:11:05.700 --> 01:11:08.060]   - On the opposite end of things going well
[01:11:08.060 --> 01:11:10.900]   or good actors having control of AI,
[01:11:10.900 --> 01:11:12.300]   we might want to touch on China
[01:11:12.300 --> 01:11:15.300]   as like a potential actor in this space.
[01:11:15.300 --> 01:11:18.020]   So first of all, I mean, being at Baidu
[01:11:18.020 --> 01:11:21.820]   and like seeing progress in AI happening generally,
[01:11:21.820 --> 01:11:24.580]   why do you think the Chinese have underperformed?
[01:11:24.580 --> 01:11:27.940]   You know, Baidu had a scaling laws group many years back
[01:11:27.940 --> 01:11:28.780]   or is the premise wrong?
[01:11:28.780 --> 01:11:30.060]   And I'm just not aware of the progress
[01:11:30.060 --> 01:11:31.220]   that's happening there.
[01:11:31.220 --> 01:11:32.660]   - Well, for the scaling laws group,
[01:11:32.660 --> 01:11:35.700]   I mean, that was an offshoot of the stuff we did with speech.
[01:11:35.700 --> 01:11:37.740]   So, you know, there were still some people there,
[01:11:37.740 --> 01:11:39.860]   but that was a mostly Americanized lab.
[01:11:39.860 --> 01:11:41.500]   I mean, I was there for a year.
[01:11:41.500 --> 01:11:43.700]   That was, you know, my first foray into deep learning.
[01:11:43.700 --> 01:11:45.220]   It was led by Andrew Ng.
[01:11:45.220 --> 01:11:46.260]   I never went to China.
[01:11:46.260 --> 01:11:48.460]   Most, you know, there's like a US lab.
[01:11:48.460 --> 01:11:50.900]   So I think that was somewhat disconnected,
[01:11:50.900 --> 01:11:52.540]   although it was an attempt by, you know,
[01:11:52.540 --> 01:11:55.740]   a Chinese entity to kind of get into the game.
[01:11:55.740 --> 01:11:56.580]   But I don't know.
[01:11:56.580 --> 01:11:59.100]   I think since then, you know, I couldn't speculate,
[01:11:59.100 --> 01:12:02.700]   but I think they've been maybe very commercially focused
[01:12:02.700 --> 01:12:05.380]   and not as focused on these kind of fundamental
[01:12:05.380 --> 01:12:07.980]   research side of things around scaling laws.
[01:12:07.980 --> 01:12:11.260]   Now, I do think because of all the, you know,
[01:12:11.260 --> 01:12:14.660]   excitement with the release of ChatGPT in, you know,
[01:12:14.660 --> 01:12:16.820]   November or so, you know,
[01:12:16.820 --> 01:12:18.780]   that's been a starting gun for them as well.
[01:12:18.780 --> 01:12:21.820]   And they're trying very aggressively to catch up now.
[01:12:21.820 --> 01:12:24.580]   I think we're, the US is quite substantially ahead,
[01:12:24.580 --> 01:12:27.180]   but I think they're trying very hard to catch up now.
[01:12:27.180 --> 01:12:29.300]   - How do you think China thinks about AGI?
[01:12:29.300 --> 01:12:32.660]   Are they thinking about safety and misuse or not?
[01:12:32.660 --> 01:12:35.220]   - I don't really have a sense.
[01:12:35.220 --> 01:12:36.660]   You know, one concern I would have,
[01:12:36.660 --> 01:12:37.860]   or if people say things like,
[01:12:37.860 --> 01:12:40.740]   well, China isn't gonna develop an AI because,
[01:12:40.740 --> 01:12:43.460]   you know, they like stability or, you know,
[01:12:43.460 --> 01:12:44.900]   they're gonna have all these restrictions
[01:12:44.900 --> 01:12:48.180]   to make sure things are in line with what the CCP wants.
[01:12:48.180 --> 01:12:50.460]   You know, that might be true in the short term.
[01:12:50.460 --> 01:12:51.900]   And for consumer products,
[01:12:51.900 --> 01:12:54.740]   my worry is that if the basic incentives
[01:12:54.740 --> 01:12:57.260]   are about national security and power,
[01:12:57.260 --> 01:12:59.420]   that's gonna become clear sooner or later.
[01:12:59.420 --> 01:13:03.100]   And so, you know, I think they're gonna,
[01:13:03.100 --> 01:13:06.020]   if they see this as, you know, a source of national power,
[01:13:06.020 --> 01:13:08.700]   they're gonna at least try to do what's most effective.
[01:13:08.700 --> 01:13:09.540]   And that, you know,
[01:13:09.540 --> 01:13:11.700]   that could lead them in the direction of AGI.
[01:13:11.700 --> 01:13:14.140]   - At what point, like, is it possible for them,
[01:13:14.140 --> 01:13:16.020]   they just get your blueprints or your code base
[01:13:16.020 --> 01:13:18.580]   or something that they can just spin up their own lab
[01:13:18.580 --> 01:13:20.180]   that is competitive at the frontier
[01:13:20.180 --> 01:13:21.940]   with the leading American companies?
[01:13:21.940 --> 01:13:23.380]   - Well, I don't know about fast,
[01:13:23.380 --> 01:13:25.980]   but I'm like, I'm concerned about this.
[01:13:25.980 --> 01:13:28.020]   So this is one reason why we're focusing
[01:13:28.020 --> 01:13:30.340]   so hard on cybersecurity.
[01:13:30.340 --> 01:13:32.500]   You know, we've worked with our cloud providers.
[01:13:32.500 --> 01:13:34.500]   We really, you know, like, you know,
[01:13:34.500 --> 01:13:37.620]   we had this blog post out about security where we said,
[01:13:37.620 --> 01:13:38.900]   you know, we have a two key system
[01:13:38.900 --> 01:13:40.420]   for access to the model weights.
[01:13:40.420 --> 01:13:43.140]   We have other measures that we put in place
[01:13:43.140 --> 01:13:44.340]   or thinking of putting in place that,
[01:13:44.340 --> 01:13:45.660]   you know, we haven't announced.
[01:13:45.660 --> 01:13:47.620]   We don't want an adversary to know about them,
[01:13:47.620 --> 01:13:49.620]   but we're happy to talk about them broadly.
[01:13:49.620 --> 01:13:51.500]   All this stuff we're doing is, by the way,
[01:13:51.500 --> 01:13:53.740]   not sufficient yet for a super determined
[01:13:53.740 --> 01:13:55.780]   state-level actor at all.
[01:13:55.780 --> 01:14:01.020]   I think it will defend against most attacks
[01:14:01.020 --> 01:14:04.820]   and against a state-level actor who's not,
[01:14:04.820 --> 01:14:07.380]   you know, who's less determined,
[01:14:07.380 --> 01:14:09.660]   but there's a lot more we need to do.
[01:14:09.660 --> 01:14:11.980]   And some of it may require new research
[01:14:11.980 --> 01:14:13.100]   on how to do security.
[01:14:13.100 --> 01:14:14.780]   - Okay, so let's talk about what it would take
[01:14:14.780 --> 01:14:17.780]   at that point, you know, we're at Anthropic Offices
[01:14:17.780 --> 01:14:19.940]   and, you know, it's like, God, good is security.
[01:14:19.940 --> 01:14:22.540]   We had to get badges and everything to come in here,
[01:14:22.540 --> 01:14:24.860]   but the eventual version of this building
[01:14:24.860 --> 01:14:27.700]   or a bunker or whatever, where the AGI is built,
[01:14:27.700 --> 01:14:28.780]   I mean, what does that look like?
[01:14:28.780 --> 01:14:30.700]   Are we, is it a building in the middle of San Francisco
[01:14:30.700 --> 01:14:33.140]   or is it you're out in the middle of Nevada or Arizona?
[01:14:33.140 --> 01:14:35.100]   Like, what is the point in which you're,
[01:14:35.100 --> 01:14:36.660]   like, Los Alamos-ing it?
[01:14:36.660 --> 01:14:39.780]   - At one point there was a running joke somewhere
[01:14:39.780 --> 01:14:42.460]   that, you know, the way building AGI would look like
[01:14:42.460 --> 01:14:44.100]   is, you know, there would be a data center
[01:14:44.100 --> 01:14:46.820]   next to a nuclear power plant next to a bunker.
[01:14:46.820 --> 01:14:47.660]   - Yeah.
[01:14:47.660 --> 01:14:49.700]   - And, you know, that we'd all kind of live in the bunker
[01:14:49.700 --> 01:14:50.660]   and everything would be local,
[01:14:50.660 --> 01:14:52.940]   so it wouldn't get on the internet.
[01:14:52.940 --> 01:14:56.020]   You know, again, if we take seriously
[01:14:56.020 --> 01:14:59.420]   the rate at which all of this is gonna happen,
[01:14:59.420 --> 01:15:01.700]   which I don't know, I can't be sure of it,
[01:15:01.700 --> 01:15:04.020]   but if we take that seriously,
[01:15:04.020 --> 01:15:06.780]   then, you know, it does make me think
[01:15:06.780 --> 01:15:09.340]   that maybe not something quite as cartoonish as that,
[01:15:09.340 --> 01:15:11.940]   but that something like that might happen.
[01:15:12.100 --> 01:15:13.540]   - What is the timescale on which
[01:15:13.540 --> 01:15:15.220]   you think alignment is solvable?
[01:15:15.220 --> 01:15:17.580]   If, like, these models are getting to human level
[01:15:17.580 --> 01:15:19.660]   in some things in two to three years,
[01:15:19.660 --> 01:15:21.500]   what is the point at which they're aligned?
[01:15:21.500 --> 01:15:23.100]   - I think this is a really difficult question
[01:15:23.100 --> 01:15:25.420]   because I actually think often people are thinking about
[01:15:25.420 --> 01:15:27.380]   kind of alignment in the wrong way.
[01:15:27.380 --> 01:15:29.620]   I think there's a general feeling that it's, like,
[01:15:29.620 --> 01:15:31.820]   models are misaligned or, like,
[01:15:31.820 --> 01:15:34.380]   there's, like, an alignment problem to solve,
[01:15:34.380 --> 01:15:36.420]   kind of like the Riemann hypothesis or something.
[01:15:36.420 --> 01:15:39.260]   Like, someday we'll crack the Riemann hypothesis.
[01:15:39.260 --> 01:15:41.500]   I don't quite think it's like that.
[01:15:41.500 --> 01:15:44.100]   Not in a way that's worse or better.
[01:15:44.100 --> 01:15:47.580]   It might be just as bad or just as unpredictable.
[01:15:47.580 --> 01:15:51.540]   When I think of, like, you know, why am I scared?
[01:15:51.540 --> 01:15:52.620]   Few things I think of.
[01:15:52.620 --> 01:15:54.460]   One is, look, like,
[01:15:54.460 --> 01:15:57.180]   I think the thing that's really hard to argue with is, like,
[01:15:57.180 --> 01:15:58.580]   there will be powerful models.
[01:15:58.580 --> 01:15:59.940]   They will be agentic.
[01:15:59.940 --> 01:16:01.460]   We're getting towards them.
[01:16:01.460 --> 01:16:04.540]   If such a model wanted to wreak havoc
[01:16:04.540 --> 01:16:06.620]   and destroy humanity or whatever,
[01:16:06.620 --> 01:16:09.100]   I think we have basically no ability to stop it.
[01:16:09.100 --> 01:16:11.500]   Like, that's, I think, just,
[01:16:11.500 --> 01:16:13.100]   if that's not true at some point,
[01:16:13.100 --> 01:16:15.100]   it'll continue to be true as we, you know,
[01:16:15.100 --> 01:16:16.500]   it will reach the point where it's true
[01:16:16.500 --> 01:16:18.300]   as we scale the models.
[01:16:18.300 --> 01:16:20.340]   So that definitely seems the case.
[01:16:20.340 --> 01:16:22.580]   And I think a second thing that seems the case
[01:16:22.580 --> 01:16:26.380]   is that we seem to be bad at controlling the models,
[01:16:26.380 --> 01:16:28.100]   not in any particular way,
[01:16:28.100 --> 01:16:30.140]   but just they're statistical systems
[01:16:30.140 --> 01:16:31.900]   and you can ask them a million things
[01:16:31.900 --> 01:16:34.380]   and they can say a million things in reply.
[01:16:34.380 --> 01:16:35.820]   And, you know, you might not have thought
[01:16:35.820 --> 01:16:38.620]   of a millionth of one thing that does something crazy.
[01:16:38.620 --> 01:16:39.700]   Or when you train them,
[01:16:39.700 --> 01:16:41.500]   you train them in this very abstract way
[01:16:41.500 --> 01:16:44.140]   and you might not understand all the consequences
[01:16:44.140 --> 01:16:46.540]   of what they do in response to that.
[01:16:46.540 --> 01:16:49.020]   I mean, I think the best example we've seen of that
[01:16:49.020 --> 01:16:51.300]   is like being in Sydney, right?
[01:16:51.300 --> 01:16:53.540]   Where it's like, I don't know how they train that model.
[01:16:53.540 --> 01:16:55.220]   I don't know what they did to make it
[01:16:55.220 --> 01:16:56.860]   do all this weird stuff, like, you know,
[01:16:56.860 --> 01:16:58.980]   threaten people and, you know,
[01:16:58.980 --> 01:17:01.660]   have this kind of weird obsessive personality.
[01:17:01.660 --> 01:17:04.220]   But what it shows is that we can get something
[01:17:04.220 --> 01:17:07.620]   very different from and maybe opposite to what we intended.
[01:17:07.620 --> 01:17:09.740]   And so I actually think facts number one
[01:17:09.740 --> 01:17:14.020]   and fact number two are like enough to be really worried.
[01:17:14.020 --> 01:17:16.900]   Like you don't need all this detailed stuff about,
[01:17:16.900 --> 01:17:20.300]   you know, convergent instrumental goals or, you know,
[01:17:20.300 --> 01:17:21.700]   analogies to evolution.
[01:17:21.700 --> 01:17:24.380]   Like actually one and two for me are pretty motivated.
[01:17:24.380 --> 01:17:26.500]   I'm like, okay, this thing's gonna be powerful.
[01:17:26.500 --> 01:17:27.900]   It could destroy us.
[01:17:27.900 --> 01:17:30.420]   And like all the ones we've built so far,
[01:17:30.420 --> 01:17:32.820]   like, you know, are at pretty decent risk
[01:17:32.820 --> 01:17:34.900]   of doing some random shit we don't understand.
[01:17:34.900 --> 01:17:36.980]   Yeah, if I agree with that and I'm like, okay,
[01:17:36.980 --> 01:17:37.980]   I'm concerned about this.
[01:17:37.980 --> 01:17:42.060]   The research agenda you have of mechanistic interoperability
[01:17:42.060 --> 01:17:45.780]   plus, you know, constant AI and the other RLHF stuff.
[01:17:45.780 --> 01:17:48.300]   If you say that we're gonna get something
[01:17:48.300 --> 01:17:49.300]   with like bioweapons or something
[01:17:49.300 --> 01:17:50.900]   that could be dangerous in two to three years.
[01:17:50.900 --> 01:17:51.740]   Yes.
[01:17:51.740 --> 01:17:53.900]   Do these things culminate within two to three years
[01:17:53.900 --> 01:17:57.020]   of actually meaningfully contributing to preventing?
[01:17:57.020 --> 01:17:57.860]   Yes.
[01:17:57.860 --> 01:18:00.420]   So I think where I was gonna go with this is like,
[01:18:00.420 --> 01:18:02.460]   you know, people talk about like doom by default
[01:18:02.460 --> 01:18:03.780]   or alignment by default.
[01:18:03.780 --> 01:18:05.940]   Like, I think it might be kind of statistical.
[01:18:05.940 --> 01:18:08.940]   Like, you know, like you might get, you know,
[01:18:08.940 --> 01:18:11.340]   with the current models, you might get Bing or Sydney
[01:18:11.340 --> 01:18:13.140]   or you might get Claude and it doesn't really matter
[01:18:13.140 --> 01:18:14.820]   'cause Bing or Sydney, like,
[01:18:14.820 --> 01:18:17.900]   if we take our current understanding and, you know,
[01:18:17.900 --> 01:18:20.460]   move that to very powerful models,
[01:18:20.460 --> 01:18:22.860]   you might just be in this world where it's like, okay,
[01:18:22.860 --> 01:18:24.820]   you make something and depending on the details,
[01:18:24.820 --> 01:18:26.620]   maybe it's totally fine.
[01:18:26.620 --> 01:18:28.860]   You know, not really alignment by default,
[01:18:28.860 --> 01:18:31.540]   but just kind of like, it depends on a lot of the details.
[01:18:31.540 --> 01:18:34.780]   And like, if you're very careful about all those details
[01:18:34.780 --> 01:18:35.620]   and you know what you're doing,
[01:18:35.620 --> 01:18:36.580]   you're getting it right.
[01:18:36.580 --> 01:18:40.060]   But we have a high susceptibility to you mess something up
[01:18:40.060 --> 01:18:42.260]   in a way that you didn't really understand
[01:18:42.260 --> 01:18:44.060]   was connected to actually,
[01:18:44.060 --> 01:18:45.740]   instead of making all the humans happy,
[01:18:45.740 --> 01:18:47.460]   it wants to, you know, turn them into pumpkins.
[01:18:47.460 --> 01:18:49.460]   Yeah, you know, just some weird shit, right?
[01:18:49.460 --> 01:18:51.180]   Because the models are so powerful, you know,
[01:18:51.180 --> 01:18:53.380]   they're like these kind of giants that are, you know,
[01:18:53.380 --> 01:18:55.820]   they're like, you know, they're standing in the landscape.
[01:18:55.820 --> 01:18:57.620]   And if they start to move their arms around randomly,
[01:18:57.620 --> 01:18:59.900]   they could just break everything.
[01:18:59.900 --> 01:19:02.460]   I guess I'm starting it with that kind of framing
[01:19:02.460 --> 01:19:04.660]   because it's not like, I don't think we're aligned
[01:19:04.660 --> 01:19:05.500]   by default.
[01:19:05.500 --> 01:19:07.100]   I don't think we're doomed by default
[01:19:07.100 --> 01:19:09.260]   and have some problem we need to solve.
[01:19:09.260 --> 01:19:11.140]   It has some kind of different character.
[01:19:11.140 --> 01:19:13.980]   Now, what I do think is that hopefully
[01:19:13.980 --> 01:19:16.180]   within a timescale of two to three years,
[01:19:16.180 --> 01:19:19.140]   we get better at diagnosing when the models are good
[01:19:19.140 --> 01:19:20.220]   and when they're bad.
[01:19:20.220 --> 01:19:23.020]   We get better at training, you know,
[01:19:23.020 --> 01:19:27.580]   increasing our repertoire of methods to train the model
[01:19:27.580 --> 01:19:29.380]   that they're less likely to do bad things
[01:19:29.380 --> 01:19:31.020]   and more likely to do good things
[01:19:31.020 --> 01:19:33.380]   in a way that isn't just relevant to the current models,
[01:19:33.380 --> 01:19:34.500]   but scales.
[01:19:34.500 --> 01:19:36.980]   And we can help develop that with interpretability
[01:19:36.980 --> 01:19:38.260]   as the test set.
[01:19:38.260 --> 01:19:40.980]   I don't think of it as, oh man, we tried our LHF,
[01:19:40.980 --> 01:19:41.820]   it didn't work.
[01:19:41.820 --> 01:19:43.420]   We tried constitutional, it didn't work.
[01:19:43.420 --> 01:19:45.340]   Like we tried this other thing, it didn't work.
[01:19:45.340 --> 01:19:47.020]   We tried mechanistic interpretability,
[01:19:47.020 --> 01:19:48.980]   now we're gonna try mechanistic.
[01:19:48.980 --> 01:19:50.660]   I think this frame of like,
[01:19:50.660 --> 01:19:52.300]   man, we haven't cracked the problem yet.
[01:19:52.300 --> 01:19:56.460]   We haven't solved the Riemann hypothesis isn't quite right.
[01:19:56.460 --> 01:20:00.860]   I think of it more as already with today's systems,
[01:20:00.860 --> 01:20:02.580]   we are not very good at controlling them.
[01:20:02.580 --> 01:20:06.340]   And the consequences of that could be very bad.
[01:20:06.340 --> 01:20:11.140]   We just need to get more ways of like increasing
[01:20:11.140 --> 01:20:14.780]   the likelihood that we can control our models
[01:20:14.780 --> 01:20:16.540]   and understand what's going on in them.
[01:20:16.540 --> 01:20:19.260]   And like, we have some of them so far,
[01:20:19.260 --> 01:20:21.300]   they aren't that good yet.
[01:20:21.300 --> 01:20:23.900]   But you know, I don't think of it as binary
[01:20:23.900 --> 01:20:25.540]   of like works and not works.
[01:20:25.540 --> 01:20:26.940]   We're gonna develop more.
[01:20:26.940 --> 01:20:29.380]   And I do think that over the next two to three years,
[01:20:29.380 --> 01:20:31.980]   we're gonna start eating that probability mass
[01:20:31.980 --> 01:20:33.980]   of ways things can go wrong.
[01:20:33.980 --> 01:20:36.100]   You know, it's kind of like in the core safety views paper,
[01:20:36.100 --> 01:20:39.300]   there's a probability mass of how hard the problem is.
[01:20:39.300 --> 01:20:40.580]   I feel like that way of staying,
[01:20:40.580 --> 01:20:42.340]   it isn't really even quite right, right?
[01:20:42.340 --> 01:20:43.620]   'Cause I don't feel like it's
[01:20:43.620 --> 01:20:45.260]   the Riemann hypothesis to solve.
[01:20:45.260 --> 01:20:49.380]   I just feel like, you know, it's almost like right now,
[01:20:49.380 --> 01:20:51.660]   if I try and juggle five balls or something,
[01:20:51.660 --> 01:20:53.380]   I can juggle three balls, right?
[01:20:53.380 --> 01:20:56.300]   I actually can, but I can't juggle five balls at all, right?
[01:20:56.300 --> 01:20:57.860]   You have to practice a lot to do that.
[01:20:57.860 --> 01:21:01.500]   If I were to do that, I would almost certainly drop them.
[01:21:01.500 --> 01:21:02.900]   And then just over time,
[01:21:02.900 --> 01:21:05.620]   you just get better at the task of controlling the balls.
[01:21:05.620 --> 01:21:07.220]   - On that post in particular,
[01:21:07.220 --> 01:21:10.380]   what is your personal probability distribution over,
[01:21:10.380 --> 01:21:13.300]   so for the audience, the three possibilities are,
[01:21:13.300 --> 01:21:15.300]   it is like trivial to align these models
[01:21:15.300 --> 01:21:18.820]   with RLHF plus plus, two, it is a difficult problem,
[01:21:18.820 --> 01:21:21.420]   but one that a big company could solve,
[01:21:21.420 --> 01:21:23.900]   two, something that is like basically impossible
[01:21:23.900 --> 01:21:25.900]   for human civilization currently to solve.
[01:21:25.900 --> 01:21:27.180]   If I'm capturing those three,
[01:21:27.180 --> 01:21:28.700]   what is your probability distribution
[01:21:28.700 --> 01:21:29.740]   over those three personally?
[01:21:29.740 --> 01:21:31.900]   - Yeah, I mean, I'm not super into like,
[01:21:31.900 --> 01:21:34.180]   what's your probability distribution of X?
[01:21:34.180 --> 01:21:36.580]   I think all of those have enough likelihood that,
[01:21:36.580 --> 01:21:38.820]   you know, they should be considered seriously.
[01:21:38.820 --> 01:21:39.980]   I'm more interested,
[01:21:39.980 --> 01:21:41.980]   question I'm much more interested in is,
[01:21:41.980 --> 01:21:43.300]   what could we learn
[01:21:43.300 --> 01:21:45.260]   that shifts probability mass between them?
[01:21:45.260 --> 01:21:46.100]   - What is the answer to that?
[01:21:46.100 --> 01:21:48.260]   - I think that one of the things
[01:21:48.260 --> 01:21:50.580]   mechanistic interpretability is gonna do
[01:21:50.580 --> 01:21:54.140]   more than necessarily solve problems
[01:21:54.140 --> 01:21:56.020]   is it's gonna tell us what's going on
[01:21:56.020 --> 01:21:58.260]   when we try to align models.
[01:21:58.260 --> 01:22:01.420]   I think it's basically gonna teach us about this.
[01:22:01.420 --> 01:22:03.540]   Like one way I can imagine
[01:22:03.540 --> 01:22:06.020]   concluding that things are very difficult
[01:22:06.020 --> 01:22:08.620]   is if mechanistic interpretability
[01:22:08.620 --> 01:22:11.700]   sort of shows us that, I don't know,
[01:22:11.700 --> 01:22:13.780]   problems tend to get moved around
[01:22:13.780 --> 01:22:15.580]   instead of being stamped out,
[01:22:15.580 --> 01:22:18.220]   or that you get rid of one problem,
[01:22:18.220 --> 01:22:19.740]   you create another one,
[01:22:19.740 --> 01:22:22.460]   or it might inspire us or give us insight
[01:22:22.460 --> 01:22:25.340]   into why problems are kind of persistent
[01:22:25.340 --> 01:22:27.860]   or hard to eradicate or crop up.
[01:22:27.860 --> 01:22:29.540]   Like for me to really believe
[01:22:29.540 --> 01:22:31.580]   some of these stories about like,
[01:22:31.580 --> 01:22:33.340]   you know, oh, something will always,
[01:22:33.340 --> 01:22:35.300]   you know, there's always this convergent goal
[01:22:35.300 --> 01:22:36.900]   in this particular direction.
[01:22:36.900 --> 01:22:39.020]   I think the abstract story is,
[01:22:39.020 --> 01:22:40.140]   it's not uncompelling,
[01:22:40.140 --> 01:22:42.580]   but I don't find it really compelling either,
[01:22:42.580 --> 01:22:43.900]   nor do I find it necessary
[01:22:43.900 --> 01:22:45.500]   to motivate all the safety work.
[01:22:45.500 --> 01:22:47.780]   But like the kind of thing that would really be like,
[01:22:47.780 --> 01:22:50.220]   oh man, we can't solve this is like,
[01:22:50.220 --> 01:22:52.780]   we see it happening inside the x-ray.
[01:22:52.780 --> 01:22:54.580]   Because yeah, 'cause I think right now
[01:22:54.580 --> 01:22:57.980]   there's just, there's way too many assumptions,
[01:22:57.980 --> 01:22:59.540]   there's way too much overconfidence
[01:22:59.540 --> 01:23:01.540]   about how all this is gonna go.
[01:23:01.540 --> 01:23:04.220]   I have a substantial probability mass on,
[01:23:04.220 --> 01:23:07.100]   this all goes wrong, it's a complete disaster,
[01:23:07.100 --> 01:23:08.340]   but in a completely different way
[01:23:08.340 --> 01:23:09.660]   than anyone had anticipated.
[01:23:09.660 --> 01:23:10.820]   - It would be beside the point to ask like,
[01:23:10.820 --> 01:23:12.620]   how could it go different than anyone anticipated?
[01:23:12.620 --> 01:23:15.260]   So on this in particular,
[01:23:15.260 --> 01:23:16.740]   what information would be relevant?
[01:23:16.740 --> 01:23:20.740]   How much would the difficulty of aligning cloud three
[01:23:20.740 --> 01:23:23.020]   and the next generation of models basically be?
[01:23:23.020 --> 01:23:24.540]   Like, is that a big piece of information?
[01:23:24.540 --> 01:23:25.380]   Is that not gonna be big?
[01:23:25.380 --> 01:23:28.460]   - So I think the people who are most worried
[01:23:28.460 --> 01:23:32.980]   are predicting that all the subhuman like AI models
[01:23:32.980 --> 01:23:34.140]   are gonna be alignable, right?
[01:23:34.140 --> 01:23:35.580]   They're gonna seem aligned,
[01:23:35.580 --> 01:23:37.620]   they're gonna deceive us in some way.
[01:23:37.620 --> 01:23:40.620]   I think it certainly gives us some information,
[01:23:40.620 --> 01:23:43.660]   but I am more interested
[01:23:43.660 --> 01:23:46.620]   in what mechanistic interpretability can tell us.
[01:23:46.620 --> 01:23:51.100]   Because again, like you see this x-ray,
[01:23:51.100 --> 01:23:53.500]   it would be too strong to say it doesn't lie,
[01:23:53.500 --> 01:23:55.620]   but at least in the current systems,
[01:23:55.620 --> 01:23:58.740]   it doesn't feel like it's optimizing against us.
[01:23:58.740 --> 01:24:00.700]   There are exotic ways that it could.
[01:24:00.700 --> 01:24:03.140]   You know, I don't think anything is a safe bet here,
[01:24:03.140 --> 01:24:05.020]   but I think it's the closest we're gonna get
[01:24:05.020 --> 01:24:07.780]   to something that isn't actively optimizing against us.
[01:24:07.780 --> 01:24:09.420]   - Let's talk about the specific methods
[01:24:09.420 --> 01:24:11.220]   other than mechanistic interpretability
[01:24:11.220 --> 01:24:12.860]   that you guys are researching.
[01:24:12.860 --> 01:24:15.500]   When we talk about RLHF or, you know,
[01:24:15.500 --> 01:24:19.460]   Constitution AI, whatever, RLHF++,
[01:24:19.460 --> 01:24:21.980]   if you had to put it in terms of human psychology,
[01:24:21.980 --> 01:24:25.140]   what is the change that is happening?
[01:24:25.140 --> 01:24:29.020]   Are we creating new drives, new goals, new thoughts?
[01:24:29.020 --> 01:24:32.420]   How is the model changing in terms of psychology?
[01:24:32.420 --> 01:24:34.900]   - I think all those terms are kind of like inadequate
[01:24:34.900 --> 01:24:36.180]   for, you know, describing what's,
[01:24:36.180 --> 01:24:37.580]   it's not clear how useful they are
[01:24:37.580 --> 01:24:39.380]   as abstractions for humans either.
[01:24:39.380 --> 01:24:40.860]   I think we don't have the language
[01:24:40.860 --> 01:24:42.380]   to describe what's going on.
[01:24:42.380 --> 01:24:43.900]   And again, I'd love to have the x-ray.
[01:24:43.900 --> 01:24:45.940]   I'd love to look inside and say,
[01:24:45.940 --> 01:24:48.260]   and kind of actually know what we're talking about
[01:24:48.260 --> 01:24:51.340]   instead of, you know, basically making up words,
[01:24:51.340 --> 01:24:53.740]   which is what I do, what you're doing
[01:24:53.740 --> 01:24:55.420]   in asking this question.
[01:24:55.420 --> 01:24:58.380]   Where, you know, we should just be honest.
[01:24:58.380 --> 01:25:01.260]   We really have very little idea what we're talking about.
[01:25:01.260 --> 01:25:02.620]   So, you know, it would be great to say,
[01:25:02.620 --> 01:25:05.340]   well, what we actually mean by that is, you know,
[01:25:05.340 --> 01:25:09.780]   this circuit within here turns on and, you know,
[01:25:09.780 --> 01:25:11.580]   after we've trained the model, then, you know,
[01:25:11.580 --> 01:25:13.900]   this circuit is no longer operative or weaker.
[01:25:13.900 --> 01:25:15.820]   And that, you know, I would love to be able to say,
[01:25:15.820 --> 01:25:18.180]   again, we're, it's going to take a lot of work
[01:25:18.180 --> 01:25:19.020]   to be able to do that.
[01:25:19.020 --> 01:25:20.820]   - Model organisms, which you hinted at before
[01:25:20.820 --> 01:25:22.300]   when you said we're doing these evaluations
[01:25:22.300 --> 01:25:24.620]   to see if they're capable of, you know,
[01:25:24.620 --> 01:25:27.100]   doing dangerous things now and currently not.
[01:25:27.100 --> 01:25:30.140]   How worried are you about a lab leak scenario
[01:25:30.140 --> 01:25:33.460]   where in fine tuning it or in trying to get these models
[01:25:33.460 --> 01:25:36.140]   to elicit dangerous behaviors, you know,
[01:25:36.140 --> 01:25:37.540]   make bioweapons or something,
[01:25:37.540 --> 01:25:40.220]   you get like leaks somehow and actually makes a bioweapon
[01:25:40.220 --> 01:25:42.300]   instead of telling you it can make the bioweapon.
[01:25:42.300 --> 01:25:43.900]   - With today's passive models,
[01:25:43.900 --> 01:25:46.580]   I think it's not that much, you know, chatbots,
[01:25:46.580 --> 01:25:48.180]   it's not so much of a concern, right?
[01:25:48.180 --> 01:25:49.740]   Because it's like, you know,
[01:25:49.740 --> 01:25:51.340]   if we were to fine tune a model, do that,
[01:25:51.340 --> 01:25:53.500]   we do it privately and work with the experts.
[01:25:53.500 --> 01:25:57.180]   And so, you know, the leak would be like, you know,
[01:25:57.180 --> 01:25:59.860]   suppose the model got open sourced or something and,
[01:25:59.860 --> 01:26:02.500]   you know, and then someone, so I think for now,
[01:26:02.500 --> 01:26:04.500]   it's mostly a security issue.
[01:26:04.500 --> 01:26:07.500]   In terms of models truly being dangerous, I mean,
[01:26:07.500 --> 01:26:11.100]   you know, I think we do have to worry that it's like,
[01:26:11.100 --> 01:26:13.260]   you know, if we make a truly powerful model
[01:26:13.260 --> 01:26:15.700]   and we're trying to like see what makes it dangerous
[01:26:15.700 --> 01:26:18.420]   or safe, then there could be more of a one-shot thing
[01:26:18.420 --> 01:26:19.260]   where it's like, you know,
[01:26:19.260 --> 01:26:20.860]   some risk that the model takes over.
[01:26:20.860 --> 01:26:23.180]   I think the main way to control that is to make sure
[01:26:23.180 --> 01:26:26.060]   that the capabilities of the model that we test
[01:26:26.060 --> 01:26:28.460]   are not such that they're capable of doing this.
[01:26:28.460 --> 01:26:30.700]   - At what point would the capabilities be so high
[01:26:30.700 --> 01:26:33.220]   where you're, you say, I don't even want to test this?
[01:26:33.220 --> 01:26:34.100]   - Oh, well, there's different things.
[01:26:34.100 --> 01:26:36.260]   I mean, there's capability testing and, you know.
[01:26:36.260 --> 01:26:37.660]   - But that itself could lead to a leak.
[01:26:37.660 --> 01:26:39.780]   If you're testing it and replicate that,
[01:26:39.780 --> 01:26:40.620]   like what if it actually does?
[01:26:40.620 --> 01:26:41.900]   - Sure, but I think, I mean,
[01:26:41.900 --> 01:26:44.820]   I think what you want to do is you want to like extrapolate.
[01:26:44.820 --> 01:26:46.420]   So we've talked with Arc about this, right?
[01:26:46.420 --> 01:26:49.220]   You know, you have like factors of two of compute
[01:26:49.220 --> 01:26:52.180]   or something where, you know, you're like, okay, you know,
[01:26:52.180 --> 01:26:54.900]   you know, can the model do something like, you know,
[01:26:54.900 --> 01:26:57.460]   open up an account on AWS and like make some money
[01:26:57.460 --> 01:26:59.780]   for itself, like some of the things that are like obvious
[01:26:59.780 --> 01:27:03.180]   prerequisites to like complete survival in the wild.
[01:27:03.180 --> 01:27:06.700]   And so just set those thresholds very well, you know,
[01:27:06.700 --> 01:27:08.500]   kind of very well below.
[01:27:08.500 --> 01:27:11.460]   And then as you proceed upward from there,
[01:27:11.460 --> 01:27:13.820]   do kind of more and more rigorous tests
[01:27:13.820 --> 01:27:15.780]   and be more and more careful about,
[01:27:15.780 --> 01:27:17.460]   about what it is you're doing.
[01:27:17.460 --> 01:27:19.300]   - On a constitution AI,
[01:27:19.300 --> 01:27:21.060]   and feel free to explain what this is for the audience,
[01:27:21.060 --> 01:27:24.100]   but who decides what the constitution
[01:27:24.100 --> 01:27:26.100]   for the next generation of models
[01:27:26.100 --> 01:27:27.860]   or a potentially superhuman model is like,
[01:27:27.860 --> 01:27:29.460]   how is that actually written?
[01:27:29.460 --> 01:27:31.820]   - I think initially, you know, to make the constitution,
[01:27:31.820 --> 01:27:34.700]   we just took some stuff that was like broadly agreed on,
[01:27:34.700 --> 01:27:36.220]   like the UN charter, you know,
[01:27:36.220 --> 01:27:39.340]   UN declaration on human rights and, you know,
[01:27:39.340 --> 01:27:41.340]   some of the stuff from Apple's terms of service, right?
[01:27:41.340 --> 01:27:42.700]   Stuff that's like, you know,
[01:27:42.700 --> 01:27:45.860]   consensus on like what's acceptable to say,
[01:27:45.860 --> 01:27:46.700]   or like, you know,
[01:27:46.700 --> 01:27:49.700]   what basic things are able to be included.
[01:27:49.700 --> 01:27:51.580]   So one, I think for future constitutions,
[01:27:51.580 --> 01:27:54.740]   we're looking into like more participatory processes
[01:27:54.740 --> 01:27:56.380]   for making these.
[01:27:56.380 --> 01:27:57.500]   But I think beyond that,
[01:27:57.500 --> 01:27:59.820]   I don't think there should be like one constitution
[01:27:59.820 --> 01:28:01.580]   for like a model that everyone uses,
[01:28:01.580 --> 01:28:06.020]   like probably models constitution should be very,
[01:28:06.020 --> 01:28:07.020]   very simple, right?
[01:28:07.020 --> 01:28:09.820]   It should only have very basic facts
[01:28:09.820 --> 01:28:11.140]   that everyone would agree on.
[01:28:11.140 --> 01:28:13.100]   And then there should be a lot of ways
[01:28:13.100 --> 01:28:14.620]   that you can customize,
[01:28:14.620 --> 01:28:16.980]   including appending, you know, constitutions.
[01:28:16.980 --> 01:28:18.660]   And, you know, I think beyond that,
[01:28:18.660 --> 01:28:19.980]   we're developing new methods, right?
[01:28:19.980 --> 01:28:20.820]   This is, you know,
[01:28:20.820 --> 01:28:23.700]   I'm not imagining that this or this alone
[01:28:23.700 --> 01:28:27.140]   is the method that we'll use to train superhuman AI, right?
[01:28:27.140 --> 01:28:29.420]   Many of the parts of capability training may be different.
[01:28:29.420 --> 01:28:31.980]   And so, you know, it could look very different.
[01:28:31.980 --> 01:28:35.220]   And again, I'd go there like there are levels above this.
[01:28:35.220 --> 01:28:37.260]   Like I'm pretty uncomfortable with like,
[01:28:37.260 --> 01:28:38.700]   here's the AI's constitution.
[01:28:38.700 --> 01:28:39.700]   It's gonna run the world.
[01:28:39.700 --> 01:28:44.100]   Like that, you know, again, like just normal lessons
[01:28:44.100 --> 01:28:47.060]   from like how societies work and how politics works
[01:28:47.060 --> 01:28:49.380]   like that, that just kind of,
[01:28:49.380 --> 01:28:51.500]   yeah, that strikes me as fanciful.
[01:28:51.500 --> 01:28:54.500]   Like, you know, I think we should try
[01:28:54.500 --> 01:28:57.620]   to hook these things into, you know,
[01:28:57.620 --> 01:28:59.260]   even when they're very powerful,
[01:28:59.260 --> 01:29:02.100]   again, after we've mitigated the safety issues,
[01:29:02.100 --> 01:29:04.340]   like any good future,
[01:29:04.340 --> 01:29:06.940]   even if it has all these security issues
[01:29:06.940 --> 01:29:08.020]   that we need to solve,
[01:29:08.020 --> 01:29:10.420]   it somehow needs to end with something
[01:29:10.420 --> 01:29:13.580]   that's more decentralized and, you know,
[01:29:13.580 --> 01:29:16.100]   less like a God-like super, you know,
[01:29:16.100 --> 01:29:18.260]   I just don't think that ends well.
[01:29:18.260 --> 01:29:20.460]   - What scientists from the Manhattan Project
[01:29:20.460 --> 01:29:23.780]   do you respect most in terms of they acted most ethically
[01:29:23.780 --> 01:29:25.940]   under the constraints they were given?
[01:29:25.940 --> 01:29:27.220]   Is there one that comes to mind?
[01:29:27.220 --> 01:29:28.060]   - I don't know.
[01:29:28.060 --> 01:29:30.620]   I mean, you know, I think there's a lot of answers
[01:29:30.620 --> 01:29:31.460]   you could give.
[01:29:31.460 --> 01:29:33.300]   I mean, I'm definitely a fan of Zillard
[01:29:33.300 --> 01:29:35.300]   for having kind of figured it out.
[01:29:35.300 --> 01:29:39.260]   He was then, you know, against the actual dropping
[01:29:39.260 --> 01:29:40.100]   of the bomb.
[01:29:40.100 --> 01:29:42.740]   I don't actually know the history well enough
[01:29:42.740 --> 01:29:44.140]   to have an opinion on whether, you know,
[01:29:44.140 --> 01:29:46.940]   demonstration of the bomb could have ended the war.
[01:29:46.940 --> 01:29:49.220]   I mean, that involves a bunch of facts
[01:29:49.220 --> 01:29:51.700]   about Imperial Japan that are, you know,
[01:29:51.700 --> 01:29:54.700]   that are complicated and that I'm not an expert on.
[01:29:54.700 --> 01:29:56.940]   But, you know, Zillard seemed to, you know,
[01:29:56.940 --> 01:29:58.940]   he discovered this stuff early.
[01:29:58.940 --> 01:30:00.660]   He kept it secret.
[01:30:00.660 --> 01:30:03.420]   You know, patented some of it
[01:30:03.420 --> 01:30:06.260]   and put it in the hands of the British Admiralty.
[01:30:06.260 --> 01:30:08.860]   So, you know, he seemed to display
[01:30:08.860 --> 01:30:13.220]   the right kind of awareness as well as discovering stuff.
[01:30:13.220 --> 01:30:15.940]   I mean, it was when I read that book that I kind of,
[01:30:15.940 --> 01:30:18.540]   you know, when I wrote this big blob of compute doc
[01:30:18.540 --> 01:30:20.660]   and many other, you know, I only showed it to a few people
[01:30:20.660 --> 01:30:23.580]   and there were other docs that I showed to almost no one.
[01:30:23.580 --> 01:30:27.540]   So, you know, yeah, I was a bit inspired by this.
[01:30:27.540 --> 01:30:28.620]   Again, I mean, you know,
[01:30:28.620 --> 01:30:30.380]   we can all get self-aggrandizing here.
[01:30:30.380 --> 01:30:31.820]   Like we don't know how it's gonna turn out
[01:30:31.820 --> 01:30:34.900]   or if it's actually gonna be something on par
[01:30:34.900 --> 01:30:35.980]   with the Manhattan Project.
[01:30:35.980 --> 01:30:37.900]   I mean, you know, this could all be
[01:30:37.900 --> 01:30:40.740]   just Silicon Valley people building technology
[01:30:40.740 --> 01:30:42.980]   and, you know, just kind of like
[01:30:42.980 --> 01:30:44.260]   having delusions of grandeur.
[01:30:44.260 --> 01:30:45.300]   So I don't know how it's gonna turn out.
[01:30:45.300 --> 01:30:47.020]   - I mean, if the scaling stuff is true,
[01:30:47.020 --> 01:30:48.860]   then it's more bigger than the Manhattan Project.
[01:30:48.860 --> 01:30:51.380]   - Yeah, it certainly could be bigger.
[01:30:51.380 --> 01:30:53.700]   I just, you know, we should always kind of,
[01:30:53.700 --> 01:30:55.540]   I don't know, maintain this attitude
[01:30:55.540 --> 01:30:57.220]   that it's really easy to fool yourself.
[01:30:57.220 --> 01:30:58.540]   - If you were asked by the government,
[01:30:58.540 --> 01:30:59.940]   if you were a physicist during World War II
[01:30:59.940 --> 01:31:01.140]   and you were asked by the government
[01:31:01.140 --> 01:31:03.220]   to contribute non-replaceable research
[01:31:03.220 --> 01:31:04.380]   to the Manhattan Project,
[01:31:04.380 --> 01:31:05.740]   well, what do you think you would have said?
[01:31:05.740 --> 01:31:07.740]   - Yeah, I mean, I think given you're in a war
[01:31:07.740 --> 01:31:10.380]   with the Nazis, at least during the period
[01:31:10.380 --> 01:31:11.860]   when you thought that the Nazis were gonna,
[01:31:11.860 --> 01:31:14.700]   I don't, yeah, I don't really see much choice,
[01:31:14.700 --> 01:31:18.100]   but to do it, if it's possible, you know,
[01:31:18.100 --> 01:31:19.460]   you have to figure it's gonna be done
[01:31:19.460 --> 01:31:21.700]   within 10 years or so by someone.
[01:31:21.700 --> 01:31:23.860]   - Regarding cybersecurity,
[01:31:23.860 --> 01:31:25.540]   what should we make of the fact that
[01:31:25.540 --> 01:31:26.940]   there's a whole bunch of tech companies
[01:31:26.940 --> 01:31:30.380]   which have ordinary tech company security policy
[01:31:30.380 --> 01:31:32.780]   that publicly seeming facing,
[01:31:32.780 --> 01:31:34.380]   it's not obvious that they've been hacked,
[01:31:34.380 --> 01:31:37.260]   like Coinbase still has its Bitcoin,
[01:31:37.260 --> 01:31:38.580]   you know, Google, as far as I know,
[01:31:38.580 --> 01:31:40.420]   my Gmail hasn't been leaked.
[01:31:40.420 --> 01:31:42.140]   Should we take from that,
[01:31:42.140 --> 01:31:45.460]   that current status quo tech company security practices
[01:31:45.460 --> 01:31:47.020]   are good enough for AGI
[01:31:47.020 --> 01:31:49.020]   or just simply that nobody has tried hard enough?
[01:31:49.020 --> 01:31:50.860]   - It would be hard for me to speak to, you know,
[01:31:50.860 --> 01:31:52.260]   current tech company practices.
[01:31:52.260 --> 01:31:54.100]   And of course there may be many attacks
[01:31:54.100 --> 01:31:56.580]   that we don't know about where things are stolen
[01:31:56.580 --> 01:31:58.420]   and then silently used.
[01:31:58.420 --> 01:32:00.100]   You know, I mean, I think an indication of it is
[01:32:00.100 --> 01:32:01.780]   when someone really cares,
[01:32:01.780 --> 01:32:04.220]   basically cares about attacking someone,
[01:32:04.220 --> 01:32:05.780]   then often the attacks happen.
[01:32:05.780 --> 01:32:08.980]   So, you know, recently we saw that
[01:32:08.980 --> 01:32:12.220]   some fairly high officials of the US government
[01:32:12.220 --> 01:32:16.100]   had their email accounts hacked via Microsoft.
[01:32:16.100 --> 01:32:18.540]   Microsoft was providing the email accounts.
[01:32:18.540 --> 01:32:21.100]   So, you know, presumably that related to information
[01:32:21.100 --> 01:32:23.500]   that was, you know, of great interest to, you know,
[01:32:23.500 --> 01:32:25.620]   to foreign adversaries.
[01:32:25.620 --> 01:32:29.140]   And so it sounds, it seems to me at least,
[01:32:29.140 --> 01:32:32.020]   you know, that the evidence is more consistent with,
[01:32:32.020 --> 01:32:34.860]   you know, when something is really high enough value,
[01:32:34.860 --> 01:32:39.300]   then, you know, someone acts and it's stolen.
[01:32:39.300 --> 01:32:41.740]   And my worry is that of course with AGI,
[01:32:41.740 --> 01:32:43.020]   we'll get to a world where, you know,
[01:32:43.020 --> 01:32:45.980]   the value is seen as incredibly high, right?
[01:32:45.980 --> 01:32:46.940]   That, you know, it'll be like
[01:32:46.940 --> 01:32:48.980]   stealing nuclear missiles or something.
[01:32:48.980 --> 01:32:51.100]   You can't be too careful on this stuff.
[01:32:51.100 --> 01:32:53.580]   And, you know, at every place that I've worked,
[01:32:53.580 --> 01:32:55.940]   I've pushed for the cybersecurity to be better.
[01:32:55.940 --> 01:32:58.740]   One of my concerns about cybersecurity is, you know,
[01:32:58.740 --> 01:33:01.100]   it's not kind of something you can trumpet.
[01:33:01.100 --> 01:33:05.020]   I think a good dynamic with safety research is like,
[01:33:05.020 --> 01:33:07.100]   you know, you can get companies into a dynamic,
[01:33:07.100 --> 01:33:09.580]   and I think we have, where, you know,
[01:33:09.580 --> 01:33:12.380]   you can get them to compete to do the best safety research
[01:33:12.380 --> 01:33:14.780]   and, you know, kind of use it as a, I don't know,
[01:33:14.780 --> 01:33:17.620]   like a recruiting point of competition or something.
[01:33:17.620 --> 01:33:20.020]   We used to do this all the time with interpretability,
[01:33:20.020 --> 01:33:23.020]   you know, and then sooner or later other orgs
[01:33:23.020 --> 01:33:24.900]   started recognizing the defect
[01:33:24.900 --> 01:33:26.740]   and started working on interpretability,
[01:33:26.740 --> 01:33:29.180]   whether or not, you know, like whether or not
[01:33:29.180 --> 01:33:31.500]   that was a priority to them before.
[01:33:31.500 --> 01:33:33.580]   But I think it's harder to do that with cybersecurity
[01:33:33.580 --> 01:33:35.940]   'cause a bunch of this stuff you have to do in quiet.
[01:33:35.940 --> 01:33:38.700]   And so, you know, we did try to put out one post about it,
[01:33:38.700 --> 01:33:42.780]   but I think, you know, mostly you just see the results.
[01:33:42.780 --> 01:33:45.060]   You know, I think people should, you know,
[01:33:45.060 --> 01:33:46.380]   a good norm would be, you know,
[01:33:46.380 --> 01:33:49.780]   people see these cybersecurity leaks from companies
[01:33:49.780 --> 01:33:51.820]   or, you know, leaks of the model parameters or something
[01:33:51.820 --> 01:33:55.340]   and say, you know, they screwed up, that's bad.
[01:33:55.340 --> 01:33:58.260]   If I'm a safety person, I might not wanna work there.
[01:33:58.260 --> 01:33:59.860]   Of course, as soon as I say that,
[01:33:59.860 --> 01:34:01.580]   we'll probably have a security breach tomorrow,
[01:34:01.580 --> 01:34:04.980]   but, you know, but that's part of the game here, right?
[01:34:04.980 --> 01:34:06.900]   That's, I think that's part of, you know,
[01:34:06.900 --> 01:34:08.500]   trying to make things safe.
[01:34:08.500 --> 01:34:10.460]   - I wanna go back to the thing we were talking about earlier
[01:34:10.460 --> 01:34:13.820]   where the ultimate level of cybersecurity
[01:34:13.820 --> 01:34:16.180]   required for two to three years from now
[01:34:16.180 --> 01:34:17.380]   and whether it requires a bunk,
[01:34:17.380 --> 01:34:19.980]   like, are you actually expecting to be in a physical bunker
[01:34:19.980 --> 01:34:21.660]   in two to three years or is that just a metaphor?
[01:34:21.660 --> 01:34:24.460]   - Yeah, I mean, I think that's a metaphor.
[01:34:24.460 --> 01:34:26.060]   You know, we're still figuring it out.
[01:34:26.060 --> 01:34:28.180]   Like something I would think about is like,
[01:34:28.180 --> 01:34:30.660]   I think security of the data center,
[01:34:30.660 --> 01:34:33.860]   which may not be in the same physical location as us,
[01:34:33.860 --> 01:34:35.020]   but, you know, we've worked very hard
[01:34:35.020 --> 01:34:36.580]   to make sure it's in the United States.
[01:34:36.580 --> 01:34:39.420]   But securing the physical data centers and the GPUs,
[01:34:39.420 --> 01:34:42.740]   I think some of the really expensive attacks,
[01:34:42.740 --> 01:34:44.420]   if someone was really determined,
[01:34:44.420 --> 01:34:46.060]   just involved going into the data center
[01:34:46.060 --> 01:34:49.220]   and just, you know, trying to steal the data directly
[01:34:49.220 --> 01:34:52.900]   or as it's flowing from the data center to us,
[01:34:52.900 --> 01:34:54.660]   I think these data centers are gonna have to be built
[01:34:54.660 --> 01:34:55.660]   in a very special way.
[01:34:55.660 --> 01:34:58.540]   I mean, given the way things are scaling up,
[01:34:58.540 --> 01:35:00.660]   you know, we're probably anyway heading to a world
[01:35:00.660 --> 01:35:03.980]   where, you know, the, you know, networks of data centers,
[01:35:03.980 --> 01:35:06.740]   you know, cost as much as aircraft carriers or something.
[01:35:06.740 --> 01:35:09.420]   And so, you know, they're already gonna be
[01:35:09.420 --> 01:35:10.500]   pretty unusual objects.
[01:35:10.500 --> 01:35:12.380]   But I think in addition to being unusual
[01:35:12.380 --> 01:35:15.700]   in terms of their ability, you know, to link together
[01:35:15.700 --> 01:35:18.100]   and train gigantic, gigantic models,
[01:35:18.100 --> 01:35:19.700]   they're also going to have to be very secure.
[01:35:19.700 --> 01:35:21.500]   - So speaking of which, how, you know,
[01:35:21.500 --> 01:35:23.860]   there's been sorts of rumors on the difficulty
[01:35:23.860 --> 01:35:26.100]   of procuring the power and the GPUs
[01:35:26.100 --> 01:35:27.860]   for the next generation of models.
[01:35:27.860 --> 01:35:29.700]   What has the process been like to secure
[01:35:29.700 --> 01:35:32.700]   the necessary components to do the next generation?
[01:35:32.700 --> 01:35:35.620]   - That's something I can't go into great detail about.
[01:35:35.620 --> 01:35:38.180]   You know, I will say, look, like, you know,
[01:35:38.180 --> 01:35:41.580]   people think of even industrial scale data centers, right?
[01:35:41.580 --> 01:35:43.460]   People are not thinking at the scale
[01:35:43.460 --> 01:35:46.180]   that I think these models are going to go to very soon.
[01:35:46.180 --> 01:35:47.980]   And so whenever you do something at a scale
[01:35:47.980 --> 01:35:49.980]   where it's never been done before, you know,
[01:35:49.980 --> 01:35:52.900]   every single component, every single thing
[01:35:52.900 --> 01:35:55.100]   has to be done in a new way than it was before.
[01:35:55.100 --> 01:35:59.100]   And so, you know, you may run into problems
[01:35:59.100 --> 01:36:01.740]   with, you know, surprisingly simple components.
[01:36:01.740 --> 01:36:03.140]   Power is one that you mentioned.
[01:36:03.140 --> 01:36:05.060]   - And is this something that Anthropic has to handle
[01:36:05.060 --> 01:36:06.340]   or can you just outsource it?
[01:36:06.340 --> 01:36:08.260]   - You know, I mean, for data centers we work with,
[01:36:08.260 --> 01:36:09.980]   cloud providers, for instance.
[01:36:09.980 --> 01:36:12.220]   - What should we make about the fact that
[01:36:12.220 --> 01:36:14.900]   these models require so much training
[01:36:14.900 --> 01:36:17.020]   and the entire corpus of internet data
[01:36:17.020 --> 01:36:19.020]   in order to be subhuman?
[01:36:19.020 --> 01:36:22.500]   Whereas, you know, if GPT-4, there's been estimates
[01:36:22.500 --> 01:36:26.260]   that, you know, it was like a 10 to the 25 flops
[01:36:26.260 --> 01:36:28.940]   or something where, you know, whereas you,
[01:36:28.940 --> 01:36:30.500]   I mean, you can take these numbers to the grain of salt,
[01:36:30.500 --> 01:36:31.740]   but there's reports that, you know,
[01:36:31.740 --> 01:36:34.900]   human brain from the time it is born
[01:36:34.900 --> 01:36:37.060]   to the time a human being is 20 years old,
[01:36:37.060 --> 01:36:39.540]   that's like on the order of 10 to the 20 flops
[01:36:39.540 --> 01:36:41.300]   to simulate all those interactions.
[01:36:41.300 --> 01:36:42.940]   You don't have to go into particulars on those numbers,
[01:36:42.940 --> 01:36:45.980]   but should we be worried about how sample inefficient
[01:36:45.980 --> 01:36:47.060]   these models seem to be?
[01:36:47.060 --> 01:36:49.660]   - Yeah, so I think that's one of the remaining mysteries.
[01:36:49.660 --> 01:36:51.540]   One way you could phrase it is that
[01:36:51.540 --> 01:36:55.460]   the models are maybe two to three orders of magnitude
[01:36:55.460 --> 01:36:56.700]   smaller than the human brain,
[01:36:56.700 --> 01:36:58.820]   if you compare it to the number of synapses,
[01:36:58.820 --> 01:37:01.900]   while at the same time being trained on, you know,
[01:37:01.900 --> 01:37:04.500]   three to four more orders of magnitude of data.
[01:37:04.500 --> 01:37:07.500]   If you compare it to, you know, number of words
[01:37:07.500 --> 01:37:11.540]   a human sees as they're developing to age 18,
[01:37:11.540 --> 01:37:12.900]   it's, I don't remember exactly,
[01:37:12.900 --> 01:37:14.780]   but I think it's in the hundreds of millions.
[01:37:14.780 --> 01:37:16.020]   Whereas for the models,
[01:37:16.020 --> 01:37:17.620]   we're talking about the hundreds of billions,
[01:37:17.620 --> 01:37:18.820]   the trillions.
[01:37:18.820 --> 01:37:20.660]   So what explains this?
[01:37:20.660 --> 01:37:22.580]   There are these offsetting things
[01:37:22.580 --> 01:37:26.180]   where the models are smaller, they need a lot more data,
[01:37:26.180 --> 01:37:27.980]   and they're still below human level.
[01:37:27.980 --> 01:37:31.780]   But so, you know, there's some way in which,
[01:37:31.780 --> 01:37:35.300]   you know, the analogy to the brain is not quite right,
[01:37:35.300 --> 01:37:38.340]   or is breaking down, or there's some missing factor.
[01:37:38.340 --> 01:37:39.940]   You know, this is just kind of like in physics,
[01:37:39.940 --> 01:37:40.900]   where it's like, you know,
[01:37:40.900 --> 01:37:43.420]   we can't explain the Michelson-Morley experiment,
[01:37:43.420 --> 01:37:44.980]   or like, I'm forgetting one of the other
[01:37:44.980 --> 01:37:47.220]   19th century physics paradoxes.
[01:37:47.220 --> 01:37:48.500]   But like, I think it's one thing
[01:37:48.500 --> 01:37:50.260]   we don't quite understand, right?
[01:37:50.260 --> 01:37:54.380]   Humans see so little data, and they still do fine.
[01:37:54.380 --> 01:37:57.140]   One theory on it, it could be that it, you know,
[01:37:57.140 --> 01:37:59.620]   it's like our other modalities.
[01:37:59.620 --> 01:38:00.900]   You know, how do we get, you know,
[01:38:00.900 --> 01:38:03.540]   10 to the 14th bits into the human brain?
[01:38:03.540 --> 01:38:05.580]   Well, most of it is kind of these images,
[01:38:05.580 --> 01:38:08.140]   and maybe a lot of what's going on inside the human brain
[01:38:08.140 --> 01:38:10.180]   is like, you know, our mental workspace
[01:38:10.180 --> 01:38:13.460]   involves all these, you know, these simulated images,
[01:38:13.460 --> 01:38:14.740]   or something like that.
[01:38:14.740 --> 01:38:16.180]   But honestly, I think intellectually,
[01:38:16.180 --> 01:38:18.300]   we have to admit that that's a weird thing
[01:38:18.300 --> 01:38:19.500]   that doesn't match up.
[01:38:19.500 --> 01:38:21.700]   And, you know, it's one reason I'm a bit, you know,
[01:38:21.700 --> 01:38:23.820]   skeptical of kind of biological analogies.
[01:38:23.820 --> 01:38:26.780]   I thought in terms of them like five or six years ago,
[01:38:26.780 --> 01:38:28.740]   but now that we actually have these models
[01:38:28.740 --> 01:38:30.060]   in front of us as artifacts,
[01:38:30.060 --> 01:38:32.460]   it feels like almost all the evidence from that
[01:38:32.460 --> 01:38:34.980]   has been screened off by what we've seen.
[01:38:34.980 --> 01:38:36.180]   And what we've seen are models
[01:38:36.180 --> 01:38:38.300]   that are much smaller than the human brain,
[01:38:38.300 --> 01:38:40.900]   and yet can do a lot of the things that humans can do,
[01:38:40.900 --> 01:38:44.060]   and yet paradoxically require a lot more data.
[01:38:44.060 --> 01:38:45.820]   So maybe we'll discover something
[01:38:45.820 --> 01:38:47.300]   that makes it all efficient,
[01:38:47.300 --> 01:38:50.980]   or maybe we'll understand why the discrepancy is present.
[01:38:50.980 --> 01:38:53.060]   But at the end of the day, I don't think it matters, right?
[01:38:53.060 --> 01:38:55.220]   If we keep scaling the way we are,
[01:38:55.220 --> 01:38:56.980]   I think what's more relevant at this point
[01:38:56.980 --> 01:38:59.300]   is just measuring the abilities of the model
[01:38:59.300 --> 01:39:01.020]   and seeing how far they are from humans.
[01:39:01.020 --> 01:39:02.780]   And they don't seem terribly far to me.
[01:39:02.780 --> 01:39:04.020]   - Does this scaling picture
[01:39:04.020 --> 01:39:06.660]   and the big blob of compute more generally,
[01:39:06.660 --> 01:39:08.980]   does that underemphasize the role
[01:39:08.980 --> 01:39:10.780]   that algorithmic progress has played
[01:39:10.780 --> 01:39:14.060]   when you compose the big blob of compute?
[01:39:14.060 --> 01:39:17.260]   So you're talking about LSTMs presumably at that point,
[01:39:17.260 --> 01:39:18.500]   presumably the scaling on that
[01:39:18.500 --> 01:39:21.460]   would not have you at cloud two at this point.
[01:39:21.460 --> 01:39:23.180]   So are you under emphasizing the role
[01:39:23.180 --> 01:39:26.060]   that an improvement of the scale of transformer
[01:39:26.060 --> 01:39:27.020]   could be having here
[01:39:27.020 --> 01:39:28.900]   when you put it up behind the label of scaling?
[01:39:28.900 --> 01:39:30.540]   - This big blob of compute document,
[01:39:30.540 --> 01:39:31.980]   which I still have not made public,
[01:39:31.980 --> 01:39:34.020]   I probably should for like historical reasons.
[01:39:34.020 --> 01:39:36.100]   I don't think it would tell anyone anything
[01:39:36.100 --> 01:39:37.140]   they don't know now.
[01:39:37.140 --> 01:39:38.420]   But when I wrote it,
[01:39:38.420 --> 01:39:42.100]   I actually said, look, there are seven factors that,
[01:39:42.100 --> 01:39:44.420]   and I wasn't like, these are the factors,
[01:39:44.420 --> 01:39:45.260]   but I was just like,
[01:39:45.260 --> 01:39:46.780]   let me give us some sense of the kinds of things
[01:39:46.780 --> 01:39:47.940]   that matter and what don't.
[01:39:47.940 --> 01:39:50.420]   And so I wasn't thinking like these are the,
[01:39:50.420 --> 01:39:51.780]   there could be nine, there could be five,
[01:39:51.780 --> 01:39:54.260]   but like the things I said were,
[01:39:54.260 --> 01:39:56.460]   I said, number of parameters,
[01:39:56.460 --> 01:39:57.340]   scale of the model,
[01:39:57.340 --> 01:39:59.860]   like the compute matters,
[01:39:59.860 --> 01:40:02.300]   quantity of data matters,
[01:40:02.300 --> 01:40:04.220]   quality of data matters,
[01:40:04.260 --> 01:40:06.180]   loss function matters.
[01:40:06.180 --> 01:40:07.780]   So like, are you doing RL
[01:40:07.780 --> 01:40:09.460]   or are you doing next word prediction?
[01:40:09.460 --> 01:40:11.300]   If your loss function isn't rich
[01:40:11.300 --> 01:40:12.780]   or doesn't incentivize the right thing,
[01:40:12.780 --> 01:40:15.180]   you won't get anything.
[01:40:15.180 --> 01:40:17.340]   So those were the key four ones,
[01:40:17.340 --> 01:40:19.020]   which I think are the core of the hypothesis.
[01:40:19.020 --> 01:40:20.340]   But then I said three more things.
[01:40:20.340 --> 01:40:22.740]   One was symmetries,
[01:40:22.740 --> 01:40:24.500]   which is basically like,
[01:40:24.500 --> 01:40:27.420]   if your architecture doesn't take into account
[01:40:27.420 --> 01:40:29.260]   the right kinds of symmetries,
[01:40:29.260 --> 01:40:32.500]   it doesn't work or it's very inefficient.
[01:40:32.500 --> 01:40:33.860]   So for example,
[01:40:33.860 --> 01:40:35.380]   convolutional neural networks
[01:40:35.380 --> 01:40:37.820]   take into account translational symmetry.
[01:40:37.820 --> 01:40:41.220]   LSTMs take into account time symmetry.
[01:40:41.220 --> 01:40:43.060]   But a weakness of LSTMs
[01:40:43.060 --> 01:40:46.020]   is that they can't attend over the whole context.
[01:40:46.020 --> 01:40:47.820]   So there's kind of this structural weakness.
[01:40:47.820 --> 01:40:51.020]   Like if a model isn't structurally capable
[01:40:51.020 --> 01:40:53.700]   of like absorbing and managing
[01:40:53.700 --> 01:40:56.020]   things that happened in a far enough distant past,
[01:40:56.020 --> 01:40:57.100]   then it's just like,
[01:40:57.100 --> 01:40:58.180]   it's kind of like,
[01:40:58.180 --> 01:40:59.740]   you know, like the compute doesn't flow,
[01:40:59.740 --> 01:41:01.660]   like the spice doesn't flow.
[01:41:01.660 --> 01:41:03.060]   It's like, you can't,
[01:41:03.060 --> 01:41:05.940]   like the blob has to be unencumbered, right?
[01:41:05.940 --> 01:41:08.580]   It kind of, it's not gonna work
[01:41:08.580 --> 01:41:11.660]   if you artificially close things off.
[01:41:11.660 --> 01:41:13.780]   And I think RNNs and LSTMs
[01:41:13.780 --> 01:41:15.500]   artificially close things off
[01:41:15.500 --> 01:41:18.220]   because they close you off to the distant past.
[01:41:18.220 --> 01:41:21.100]   And so again, things need to flow freely.
[01:41:21.100 --> 01:41:22.380]   If they don't, it doesn't work.
[01:41:22.380 --> 01:41:24.820]   And then, you know, I added a couple of things.
[01:41:24.820 --> 01:41:26.780]   One of them was like conditioning,
[01:41:26.780 --> 01:41:27.780]   which is like, you know,
[01:41:27.780 --> 01:41:30.020]   if the thing you're optimizing with
[01:41:30.020 --> 01:41:32.020]   is just really numerically bad,
[01:41:32.020 --> 01:41:33.060]   like you're gonna have trouble.
[01:41:33.060 --> 01:41:35.260]   And so this is why like Atom works better than,
[01:41:35.260 --> 01:41:37.260]   you know, than normal STD.
[01:41:37.260 --> 01:41:40.340]   And I think I'm forgetting what the seventh condition was,
[01:41:40.340 --> 01:41:42.580]   but it was similar to things like this,
[01:41:42.580 --> 01:41:43.740]   where it's like, you know,
[01:41:43.740 --> 01:41:46.460]   if you set things up in kind of a way
[01:41:46.460 --> 01:41:48.620]   that's set up to fail,
[01:41:48.620 --> 01:41:50.460]   or that doesn't allow the compute to work
[01:41:50.460 --> 01:41:51.500]   in an uninhibited way,
[01:41:51.500 --> 01:41:52.420]   then it won't work.
[01:41:52.420 --> 01:41:54.740]   And so transformers were kind of within that,
[01:41:54.740 --> 01:41:56.660]   even though I can't remember
[01:41:56.660 --> 01:41:58.860]   if the transformer paper had been published.
[01:41:58.860 --> 01:42:01.220]   It was around the same time as I wrote that document.
[01:42:01.220 --> 01:42:02.180]   It might've been just before,
[01:42:02.180 --> 01:42:03.660]   it might've been just after.
[01:42:03.660 --> 01:42:05.180]   - It sounds like from that view,
[01:42:05.180 --> 01:42:09.620]   that the way to think about these algorithmic progresses
[01:42:09.620 --> 01:42:12.580]   is not as increasing the power of the blob of compute,
[01:42:12.580 --> 01:42:16.180]   but simply getting rid of the artificial hindrances
[01:42:16.180 --> 01:42:17.380]   that older architectures have.
[01:42:17.380 --> 01:42:18.340]   Is that a fair way to-
[01:42:18.340 --> 01:42:19.220]   - That's a little, yeah,
[01:42:19.220 --> 01:42:20.660]   that's a little how I think about it.
[01:42:20.660 --> 01:42:22.700]   You know, again, if you go back to like,
[01:42:22.700 --> 01:42:24.180]   earlier it was like the models wanna learn.
[01:42:24.180 --> 01:42:25.020]   - Yeah, yeah.
[01:42:25.020 --> 01:42:26.700]   - Like the compute wants to be free.
[01:42:26.700 --> 01:42:27.540]   - Yeah, yeah, yeah.
[01:42:27.540 --> 01:42:29.860]   - It's like, you know, it's being blocked in various ways
[01:42:29.860 --> 01:42:31.780]   where you like don't understand that it's being blocked
[01:42:31.780 --> 01:42:33.100]   and so you need to like free it up.
[01:42:33.100 --> 01:42:33.940]   - Right, right.
[01:42:33.940 --> 01:42:37.300]   I love the deradians, change that to spice.
[01:42:37.300 --> 01:42:38.140]   Okay.
[01:42:38.140 --> 01:42:40.500]   On that point though,
[01:42:40.500 --> 01:42:42.700]   so do you think that another thing
[01:42:42.700 --> 01:42:45.100]   on the scale of a transformer
[01:42:45.100 --> 01:42:46.860]   is coming down the pike
[01:42:46.860 --> 01:42:49.740]   to enable the next great iterations?
[01:42:49.740 --> 01:42:50.860]   - I think it's possible.
[01:42:50.860 --> 01:42:52.660]   I mean, people have worked on things like, you know,
[01:42:52.660 --> 01:42:56.700]   trying to model very long time dependencies
[01:42:56.700 --> 01:42:59.700]   or, you know, there's various different ideas
[01:42:59.700 --> 01:43:02.140]   where I could see that we're kind of missing
[01:43:02.140 --> 01:43:05.460]   an efficient way of representing or dealing with something.
[01:43:05.460 --> 01:43:07.860]   So I think those inventions are possible.
[01:43:07.860 --> 01:43:09.660]   I guess my perspective would be,
[01:43:09.660 --> 01:43:10.740]   even if they don't happen,
[01:43:10.740 --> 01:43:14.980]   we're already on this very, very steep trajectory.
[01:43:14.980 --> 01:43:17.060]   And so I'm less, I mean,
[01:43:17.060 --> 01:43:20.220]   we're constantly trying to discover them as are others,
[01:43:20.220 --> 01:43:22.300]   but things are already on such a fast trajectory.
[01:43:22.300 --> 01:43:24.860]   All that would do is speed up the trajectory even more
[01:43:24.860 --> 01:43:26.940]   and probably not by that much
[01:43:26.940 --> 01:43:28.700]   'cause it's already going so fast.
[01:43:28.700 --> 01:43:29.940]   - Is something embodied
[01:43:29.940 --> 01:43:32.580]   or having an embodied version of a model,
[01:43:32.580 --> 01:43:33.740]   is that at all important
[01:43:33.740 --> 01:43:36.100]   in terms of getting either data or progress?
[01:43:36.100 --> 01:43:38.780]   - I think of that less in terms of the, you know,
[01:43:38.780 --> 01:43:39.900]   like a new architecture
[01:43:39.900 --> 01:43:42.180]   and more in terms of like a loss function,
[01:43:42.180 --> 01:43:45.540]   like the data, the environments you're exposing yourself to
[01:43:45.540 --> 01:43:47.020]   end up being very different.
[01:43:47.020 --> 01:43:48.780]   And so I think that could be important
[01:43:48.780 --> 01:43:50.140]   for learning some skills,
[01:43:50.140 --> 01:43:52.780]   although data acquisition is hard.
[01:43:52.780 --> 01:43:55.540]   And so things have gone through the language route
[01:43:55.540 --> 01:43:57.860]   and I would guess we'll continue
[01:43:57.860 --> 01:44:00.580]   to go through the language route even as, you know,
[01:44:00.580 --> 01:44:03.500]   even as more as possible in terms of embodiment.
[01:44:03.500 --> 01:44:05.100]   - And then the other possibilities you mentioned,
[01:44:05.100 --> 01:44:06.580]   RL, you can see it as.
[01:44:06.580 --> 01:44:09.700]   - Yeah, I mean, we kind of already do RL with RLHF, right?
[01:44:09.700 --> 01:44:12.020]   People are like, is this an alignment, is this capabilities?
[01:44:12.020 --> 01:44:13.780]   I always think in terms of the two snakes, right?
[01:44:13.780 --> 01:44:16.260]   They're kind of often hard to distinguish.
[01:44:16.260 --> 01:44:18.980]   So we already kind of use RL in these language models,
[01:44:18.980 --> 01:44:21.220]   but I think we've used RL less
[01:44:21.220 --> 01:44:23.260]   in terms of getting them to take actions
[01:44:23.260 --> 01:44:25.100]   and, you know, do things in the world.
[01:44:25.100 --> 01:44:26.980]   But, you know, when you take actions
[01:44:26.980 --> 01:44:28.340]   over a long period of time
[01:44:28.340 --> 01:44:31.220]   and understand the consequences of those actions
[01:44:31.220 --> 01:44:32.500]   only later than, you know,
[01:44:32.500 --> 01:44:34.460]   RL is a typical tool we have for that.
[01:44:34.460 --> 01:44:36.820]   So I would guess that in terms of models
[01:44:36.820 --> 01:44:38.140]   taking action in the world,
[01:44:38.140 --> 01:44:41.020]   that RL will, you know, will become a thing
[01:44:41.020 --> 01:44:42.740]   with all the power and all the safety issues
[01:44:42.740 --> 01:44:43.580]   that come with it.
[01:44:43.580 --> 01:44:44.820]   - When you project out in the future,
[01:44:44.820 --> 01:44:46.900]   do you see the way in which these things
[01:44:46.900 --> 01:44:50.860]   will be integrated into productive supply chains?
[01:44:50.860 --> 01:44:53.060]   Do you see them talking with each other
[01:44:53.060 --> 01:44:54.220]   and criticizing each other
[01:44:54.220 --> 01:44:56.420]   and contributing to each other's output?
[01:44:56.420 --> 01:44:58.380]   Or is it just the model one shots,
[01:44:58.380 --> 01:45:02.660]   one model one shots the answer or the work?
[01:45:02.660 --> 01:45:05.420]   - Models will undertake extended tasks.
[01:45:05.420 --> 01:45:06.540]   That will have to be the case.
[01:45:06.540 --> 01:45:08.820]   I mean, we may want to limit that to some extent
[01:45:08.820 --> 01:45:11.820]   because it may make some of the safety problems easier.
[01:45:11.820 --> 01:45:14.380]   But, you know, some of that I think will be required.
[01:45:14.380 --> 01:45:16.460]   In terms of our models talking to models
[01:45:16.460 --> 01:45:18.340]   or are they talking to humans?
[01:45:18.340 --> 01:45:21.020]   Again, this goes kind of out of the technical realm
[01:45:21.020 --> 01:45:25.740]   and into the like socio-cultural economic realm
[01:45:25.740 --> 01:45:28.780]   where my heuristic is always that it's very,
[01:45:28.780 --> 01:45:30.740]   very difficult to predict things.
[01:45:30.740 --> 01:45:33.940]   And so I feel like these scaling laws
[01:45:33.940 --> 01:45:35.100]   have been very predictable.
[01:45:35.100 --> 01:45:37.140]   But then when you say like, well, you know,
[01:45:37.140 --> 01:45:39.340]   when is there going to be a commercial explosion
[01:45:39.340 --> 01:45:40.180]   in these models?
[01:45:40.180 --> 01:45:42.020]   Or what's the form it's going to be?
[01:45:42.020 --> 01:45:44.220]   Or are the models going to do things instead of humans
[01:45:44.220 --> 01:45:45.780]   or pairing with humans?
[01:45:45.780 --> 01:45:48.020]   I feel like certainly my track record
[01:45:48.020 --> 01:45:50.500]   on predicting these things is terrible.
[01:45:50.500 --> 01:45:52.220]   But I also looking around,
[01:45:52.220 --> 01:45:54.420]   I don't really see anyone whose track record is great.
[01:45:54.420 --> 01:45:56.460]   - You mentioned how fast progress is happening,
[01:45:56.460 --> 01:45:59.820]   but also the difficulties of integrating
[01:45:59.820 --> 01:46:03.140]   within the existing economy into the way things work.
[01:46:03.140 --> 01:46:04.740]   Do you think there will be enough time
[01:46:04.740 --> 01:46:07.780]   to actually have large revenues from AI products
[01:46:07.780 --> 01:46:10.580]   before the next model is just so much better
[01:46:10.580 --> 01:46:12.620]   or we're in like a different landscape entirely?
[01:46:12.620 --> 01:46:15.100]   - It depends what you mean by large, right?
[01:46:15.100 --> 01:46:16.500]   You know, I think multiple companies
[01:46:16.500 --> 01:46:20.300]   are already in the 100 million to billion per year range.
[01:46:20.300 --> 01:46:24.420]   What will it get to the 100 billion or trillion range?
[01:46:24.420 --> 01:46:28.260]   You know, that stuff is just so hard to predict, right?
[01:46:28.260 --> 01:46:31.420]   And it's not even super well-defined.
[01:46:31.420 --> 01:46:33.860]   Like, you know, I think right now there are companies
[01:46:33.860 --> 01:46:36.980]   that are throwing a lot of money at generative AI,
[01:46:36.980 --> 01:46:38.460]   you know, as customers.
[01:46:38.460 --> 01:46:41.300]   But, and, you know, I think that's the right thing
[01:46:41.300 --> 01:46:42.140]   for them to do.
[01:46:42.140 --> 01:46:44.060]   And they'll, you know, they'll find uses for it,
[01:46:44.060 --> 01:46:46.580]   but it doesn't mean it's, you know,
[01:46:46.580 --> 01:46:49.460]   they're finding uses or the best uses from day one.
[01:46:49.460 --> 01:46:52.940]   So even money changing hands is not quite the same thing
[01:46:52.940 --> 01:46:54.700]   as economic value being created.
[01:46:54.700 --> 01:46:55.780]   - But surely you've thought about this
[01:46:55.780 --> 01:46:57.100]   from the perspective of anthropic,
[01:46:57.100 --> 01:46:58.980]   where if these things are happening so fast,
[01:46:58.980 --> 01:47:01.700]   then it should be an insane valuation, right?
[01:47:01.700 --> 01:47:03.180]   - Even us who have, you know,
[01:47:03.180 --> 01:47:05.300]   not been super focused on commercialization
[01:47:05.300 --> 01:47:06.940]   and more on safety, I mean, you know,
[01:47:06.940 --> 01:47:11.300]   the graph goes up and it goes up relatively quickly.
[01:47:11.300 --> 01:47:12.140]   - Yeah.
[01:47:12.140 --> 01:47:13.940]   - So, you know, I can only imagine what's happening
[01:47:13.940 --> 01:47:16.020]   at, you know, the orgs where, you know,
[01:47:16.020 --> 01:47:19.060]   this is their singular focus.
[01:47:19.060 --> 01:47:23.020]   So it's certainly happening fast, but, you know,
[01:47:23.020 --> 01:47:26.260]   again, it's like, it's the exponential from the small base
[01:47:26.260 --> 01:47:28.380]   while the technology itself is moving fast.
[01:47:28.380 --> 01:47:30.900]   So it's kind of a race between
[01:47:30.900 --> 01:47:32.620]   how fast the technology is getting better
[01:47:32.620 --> 01:47:34.820]   and how fast it's integrated into the economy.
[01:47:34.820 --> 01:47:36.780]   And I think that's just a very unstable
[01:47:36.780 --> 01:47:38.860]   and turbulent process.
[01:47:38.860 --> 01:47:40.300]   Both things are going to happen fast,
[01:47:40.300 --> 01:47:43.420]   but if you ask me exactly how it's going to play out,
[01:47:43.420 --> 01:47:45.780]   exactly what order things are going to happen,
[01:47:45.780 --> 01:47:49.740]   I don't know, and I'm kind of skeptical
[01:47:49.740 --> 01:47:51.060]   of the ability to predict.
[01:47:51.060 --> 01:47:52.060]   - I'm kind of curious with regards
[01:47:52.060 --> 01:47:53.500]   to Anthropic specifically.
[01:47:53.500 --> 01:47:54.340]   - Yes.
[01:47:54.340 --> 01:47:55.180]   - You're a public benefit corporation.
[01:47:55.180 --> 01:47:56.020]   - Yes.
[01:47:56.020 --> 01:47:58.100]   - And rightfully so, you want to make sure
[01:47:58.100 --> 01:48:00.060]   that this is an important technology.
[01:48:00.060 --> 01:48:01.540]   Obviously, the only thing you want to care about
[01:48:01.540 --> 01:48:03.220]   is not shareholder value,
[01:48:03.220 --> 01:48:05.580]   but how do you talk to investors
[01:48:05.580 --> 01:48:07.220]   who are putting in like hundreds of millions,
[01:48:07.220 --> 01:48:09.300]   billions of dollars of money?
[01:48:09.300 --> 01:48:11.220]   Like, how do you talk to them about the fact that,
[01:48:11.220 --> 01:48:12.700]   how do you get them to put in this amount of money
[01:48:12.700 --> 01:48:15.500]   without the shareholder value being the main concern?
[01:48:15.500 --> 01:48:19.580]   - So, I think the LTBT is the right thing on this, right?
[01:48:19.580 --> 01:48:22.460]   You know, I mean, we're going to talk more about the LTBT,
[01:48:22.460 --> 01:48:24.620]   but like some version of that has been in development
[01:48:24.620 --> 01:48:28.900]   since the beginning of Anthropic, even formally, right?
[01:48:28.900 --> 01:48:31.380]   And so, from the beginning,
[01:48:31.380 --> 01:48:34.180]   even as the body has changed in some ways,
[01:48:34.180 --> 01:48:35.980]   it's like from the beginning, it was like,
[01:48:35.980 --> 01:48:38.460]   this body is going to exist and it's unusual.
[01:48:38.460 --> 01:48:42.580]   Like every traditional investor who invests in Anthropic,
[01:48:42.580 --> 01:48:45.060]   you know, looks at this.
[01:48:45.060 --> 01:48:46.780]   Some of them are just like, whatever,
[01:48:46.780 --> 01:48:48.420]   you run your company how you want.
[01:48:48.420 --> 01:48:50.740]   Some of them are like, you know, oh my God,
[01:48:50.740 --> 01:48:53.900]   like this body of random people or to them,
[01:48:53.900 --> 01:48:55.580]   random people could like, you know,
[01:48:55.580 --> 01:48:58.020]   could move Anthropic in a direction that's, you know,
[01:48:58.020 --> 01:48:59.500]   that's totally contrary to our,
[01:48:59.500 --> 01:49:02.300]   and now there are legal limits on that, of course.
[01:49:02.300 --> 01:49:04.580]   But, you know, we have to have this conversation
[01:49:04.580 --> 01:49:07.220]   with every investor and then it gets into a conversation
[01:49:07.220 --> 01:49:09.180]   of, well, what are the kinds of things that, you know,
[01:49:09.180 --> 01:49:13.620]   that we would, we might do that would be contrary to the,
[01:49:13.620 --> 01:49:16.660]   to the, you know, to the interests of traditional investors
[01:49:16.660 --> 01:49:18.580]   and just having those conversations
[01:49:18.580 --> 01:49:20.220]   has helped get everyone on the same page.
[01:49:20.220 --> 01:49:22.940]   - I want to talk about the physics
[01:49:22.940 --> 01:49:24.820]   and the fact that so many of the founders
[01:49:24.820 --> 01:49:27.980]   and the employees at Anthropic are physicists.
[01:49:27.980 --> 01:49:30.380]   What is the, I mean, we talked in the beginning
[01:49:30.380 --> 01:49:32.740]   about the scaling laws and how the power laws from physics
[01:49:32.740 --> 01:49:35.260]   are something you see here, but, you know,
[01:49:35.260 --> 01:49:37.420]   what are the actual like approaches
[01:49:37.420 --> 01:49:38.900]   and ways of thinking from physics
[01:49:38.900 --> 01:49:40.540]   that seem to have carried over so well?
[01:49:40.540 --> 01:49:42.580]   Is that notion of effective theory is super useful?
[01:49:42.580 --> 01:49:44.020]   What, you know, what is going on here?
[01:49:44.020 --> 01:49:46.060]   - I mean, I think part of it is just physicists
[01:49:46.060 --> 01:49:47.300]   learn things really fast.
[01:49:47.300 --> 01:49:50.660]   We have generally found that, you know,
[01:49:50.660 --> 01:49:52.700]   if we hire, you know, someone who is a, you know,
[01:49:52.700 --> 01:49:54.940]   physics PhD or something that they can,
[01:49:54.940 --> 01:49:57.500]   they can learn ML and contribute just very, very quickly
[01:49:57.500 --> 01:49:58.740]   in most cases.
[01:49:58.740 --> 01:50:00.380]   And, you know, because several of our founders,
[01:50:00.380 --> 01:50:05.100]   myself, Jared Kaplan, Sam McKendlish were physicists,
[01:50:05.100 --> 01:50:06.340]   we knew a lot of other physicists.
[01:50:06.340 --> 01:50:07.740]   And so we were able to hire them.
[01:50:07.740 --> 01:50:10.020]   And now there's, I don't know how many is exactly,
[01:50:10.020 --> 01:50:12.300]   you know, might be 30 or 40 of them here.
[01:50:12.300 --> 01:50:14.620]   ML is not, still not yet a field
[01:50:14.620 --> 01:50:16.620]   that has an enormous amount of depth.
[01:50:16.620 --> 01:50:19.140]   And so they've been able to get up to speed very quickly.
[01:50:19.140 --> 01:50:21.660]   - Are you concerned that there's like a lot of people
[01:50:21.660 --> 01:50:24.940]   who would have been doing physics or something, whatever,
[01:50:24.940 --> 01:50:26.660]   they go into finance instead.
[01:50:26.660 --> 01:50:28.620]   And since Anthropic exists,
[01:50:28.620 --> 01:50:32.180]   they have now been recruited to go into AI.
[01:50:32.180 --> 01:50:35.180]   And, you know, they're, you obviously care about AI safety,
[01:50:35.180 --> 01:50:37.940]   but who, you know, maybe in the future they leave
[01:50:37.940 --> 01:50:40.020]   and they get funded to do their own thing.
[01:50:40.020 --> 01:50:41.860]   Is that a concern that you're bringing more people
[01:50:41.860 --> 01:50:42.820]   into the ecosystem here?
[01:50:42.820 --> 01:50:43.820]   - Yeah, I mean, you know,
[01:50:43.820 --> 01:50:46.260]   I think there's like a broad set of action, you know,
[01:50:46.260 --> 01:50:48.020]   like we're causing GPUs to exist.
[01:50:48.020 --> 01:50:50.540]   You know, there's a lot of kind of side effects
[01:50:50.540 --> 01:50:53.020]   that you can't, that you can't currently control
[01:50:53.020 --> 01:50:55.460]   or that you just incur if you buy into the idea
[01:50:55.460 --> 01:50:57.860]   that you need to build frontier models.
[01:50:57.860 --> 01:50:58.700]   And that's one of them.
[01:50:58.700 --> 01:50:59.860]   A lot of them would have happened anyway.
[01:50:59.860 --> 01:51:02.220]   I mean, finance was a hot thing 20 years ago.
[01:51:02.220 --> 01:51:03.260]   So physicists were doing it.
[01:51:03.260 --> 01:51:04.500]   Now ML is the hot thing.
[01:51:04.500 --> 01:51:07.140]   And, you know, it's not like we caused them to do it
[01:51:07.140 --> 01:51:08.700]   when they had no interest previously.
[01:51:08.700 --> 01:51:11.660]   But, you know, again, you know, at the margin,
[01:51:11.660 --> 01:51:14.340]   you're kind of, you're kind of bidding things up.
[01:51:14.340 --> 01:51:16.660]   And, you know, a lot of that would have happened anyway.
[01:51:16.660 --> 01:51:17.820]   Some of it, some of it wouldn't,
[01:51:17.820 --> 01:51:19.100]   but it's all part of the calculus.
[01:51:19.100 --> 01:51:21.380]   - Do you think that cloud has conscious experience?
[01:51:21.380 --> 01:51:22.500]   How likely do you think that is?
[01:51:22.500 --> 01:51:23.900]   - This is another of these questions
[01:51:23.900 --> 01:51:26.460]   that just seems very unsettled and uncertain.
[01:51:26.460 --> 01:51:28.660]   One thing I'll tell you is I used to think
[01:51:28.660 --> 01:51:30.980]   that we didn't have to worry about this at all
[01:51:30.980 --> 01:51:33.100]   until models were kind of like operating
[01:51:33.100 --> 01:51:35.540]   in rich environments, like not necessarily embodied,
[01:51:35.540 --> 01:51:37.460]   but like that, you know, they, you know,
[01:51:37.460 --> 01:51:39.940]   they needed to like have a reward function
[01:51:39.940 --> 01:51:43.060]   and like have kind of long lived experience.
[01:51:43.060 --> 01:51:44.860]   So I still think that might be the case,
[01:51:44.860 --> 01:51:47.900]   but the more we've looked at kind of these language models
[01:51:47.900 --> 01:51:49.220]   and particularly looked inside them
[01:51:49.220 --> 01:51:51.700]   to see things like induction heads,
[01:51:51.700 --> 01:51:53.380]   a lot of the cognitive machinery
[01:51:53.380 --> 01:51:55.380]   that you would need for active agents
[01:51:55.380 --> 01:51:58.780]   seems kind of already present in the base language models.
[01:51:58.780 --> 01:52:01.580]   So I'm not quite as sure as I was before
[01:52:01.580 --> 01:52:04.180]   that we're missing the things that, you know,
[01:52:04.180 --> 01:52:06.660]   that we're missing enough of the things that you would need.
[01:52:06.660 --> 01:52:10.780]   I think today's models just probably aren't smart enough
[01:52:10.780 --> 01:52:12.700]   that we should worry about this too much,
[01:52:12.700 --> 01:52:15.020]   but I'm not 100% sure about this.
[01:52:15.020 --> 01:52:16.940]   And I do think the models will get in a year or two,
[01:52:16.940 --> 01:52:19.500]   like this might be a very real concern.
[01:52:19.500 --> 01:52:21.300]   - What would change if you found out
[01:52:21.300 --> 01:52:22.220]   that they are conscious?
[01:52:22.220 --> 01:52:23.740]   Are you worried that you're pushing
[01:52:23.740 --> 01:52:25.300]   the negative gradients of suffering?
[01:52:25.300 --> 01:52:26.140]   Like what is...
[01:52:26.140 --> 01:52:27.500]   - Conscious is again, one of these words
[01:52:27.500 --> 01:52:29.860]   that I suspect it will like not end up
[01:52:29.860 --> 01:52:31.500]   having a well-defined meaning.
[01:52:31.500 --> 01:52:32.860]   - But it's like something to be caught.
[01:52:32.860 --> 01:52:34.220]   - Yeah, but that, yeah.
[01:52:34.220 --> 01:52:37.980]   Well, I suspect that's a spectrum, right?
[01:52:37.980 --> 01:52:41.380]   So I don't know, if we discover like that, you know,
[01:52:41.380 --> 01:52:42.660]   that I should care about...
[01:52:42.660 --> 01:52:43.980]   Let's say we discover that I should care
[01:52:43.980 --> 01:52:47.020]   about Claude's experience as much as I should care about
[01:52:47.020 --> 01:52:48.980]   like a dog or a monkey or something.
[01:52:48.980 --> 01:52:52.100]   Yeah, I would be kind of worried.
[01:52:52.100 --> 01:52:54.940]   I don't know if their experience is positive or negative.
[01:52:54.940 --> 01:52:57.300]   Unsettlingly, I also don't know, like,
[01:52:57.300 --> 01:53:01.020]   I wouldn't know if any intervention that we made
[01:53:01.020 --> 01:53:03.500]   was more likely to make Claude, you know,
[01:53:03.500 --> 01:53:05.500]   have a positive versus negative experience
[01:53:05.500 --> 01:53:06.740]   versus not having one.
[01:53:06.740 --> 01:53:08.660]   If there's an area that is helpful with this,
[01:53:08.660 --> 01:53:10.780]   it's maybe mechanistic interpretability.
[01:53:10.780 --> 01:53:12.860]   'Cause I think of it as neuroscience for models.
[01:53:12.860 --> 01:53:16.900]   And so it's possible that we could shed some light on this.
[01:53:16.900 --> 01:53:19.980]   Although, you know, it's not a straightforward,
[01:53:19.980 --> 01:53:21.460]   factual question, right?
[01:53:21.460 --> 01:53:23.700]   It kind of depends what we mean and what we value.
[01:53:23.700 --> 01:53:25.020]   - We talked about this initially,
[01:53:25.020 --> 01:53:26.540]   but I wanna get more specific.
[01:53:26.540 --> 01:53:28.140]   We talked initially about, you know,
[01:53:28.140 --> 01:53:30.220]   now that you're seeing these capabilities ramp up
[01:53:30.220 --> 01:53:31.980]   within the human spectrum,
[01:53:31.980 --> 01:53:35.260]   you think that the human spectrum is wider than we thought.
[01:53:35.260 --> 01:53:38.220]   But more specifically, what have you,
[01:53:38.220 --> 01:53:40.820]   how is the way you think about human intelligence different
[01:53:40.820 --> 01:53:42.740]   now that the way you're seeing
[01:53:42.740 --> 01:53:45.620]   these marginally useful abilities emerge,
[01:53:45.620 --> 01:53:48.380]   how does that change your picture of what intelligence is?
[01:53:48.380 --> 01:53:50.620]   - I think for me, the big realization
[01:53:50.620 --> 01:53:52.740]   on what intelligence is came with the like blob
[01:53:52.740 --> 01:53:53.740]   of compute thing, right?
[01:53:53.740 --> 01:53:55.300]   Like it's not, you know,
[01:53:55.300 --> 01:53:57.140]   there might be all these separate modules,
[01:53:57.140 --> 01:53:59.420]   there might be all this complexity.
[01:53:59.420 --> 01:54:00.420]   You know, it's, you know,
[01:54:00.420 --> 01:54:02.340]   Rich Sutton called it the bitter lesson, right?
[01:54:02.340 --> 01:54:04.020]   It's almost called, has many names.
[01:54:04.020 --> 01:54:05.660]   It's been called the scaling hypothesis.
[01:54:05.660 --> 01:54:07.740]   Like the first few people who figured it out
[01:54:07.740 --> 01:54:09.420]   was around 2017.
[01:54:09.420 --> 01:54:10.780]   I mean, you could go further back to,
[01:54:10.780 --> 01:54:13.740]   I think Shane Lake was maybe the first person
[01:54:13.740 --> 01:54:14.980]   who really knew it.
[01:54:14.980 --> 01:54:18.220]   Maybe Ray Kurzweil, although in a very vague way.
[01:54:18.220 --> 01:54:20.380]   But, you know, I think the number of people
[01:54:20.380 --> 01:54:25.380]   who understood it went up a lot around 2014 to 2017.
[01:54:25.380 --> 01:54:27.980]   But I think that was the big realization.
[01:54:27.980 --> 01:54:30.780]   It's like, you know, well, how did intelligence evolve?
[01:54:30.780 --> 01:54:33.540]   Well, if you don't need very specific conditions
[01:54:33.540 --> 01:54:35.580]   to create it, if you can create it
[01:54:35.580 --> 01:54:38.020]   just from like the right,
[01:54:38.020 --> 01:54:40.300]   kind of the right kind of gradient loss signal,
[01:54:40.300 --> 01:54:41.980]   then of course it's not so mysterious
[01:54:41.980 --> 01:54:43.580]   how it all happened in terms, you know,
[01:54:43.580 --> 01:54:46.700]   it had this click of scientific understanding.
[01:54:46.700 --> 01:54:50.020]   In terms of like watching what the models can do,
[01:54:50.020 --> 01:54:52.860]   how has it changed my view of human intelligence?
[01:54:52.860 --> 01:54:56.500]   I wish I had something more intelligent to say on that.
[01:54:56.500 --> 01:54:58.620]   I feel like, I don't know,
[01:54:58.620 --> 01:55:01.420]   one thing that's been surprising is like,
[01:55:01.420 --> 01:55:03.140]   I thought things might click into place
[01:55:03.140 --> 01:55:04.420]   a little more than they do.
[01:55:04.420 --> 01:55:06.460]   Like, you know, I thought like different
[01:55:06.460 --> 01:55:09.540]   cognitive abilities might all be connected
[01:55:09.540 --> 01:55:11.380]   and there was more of one secret behind them.
[01:55:11.380 --> 01:55:14.580]   But it's like, the model just learns various things
[01:55:14.580 --> 01:55:15.620]   at different times, you know,
[01:55:15.620 --> 01:55:17.300]   and it can be like very good at coding,
[01:55:17.300 --> 01:55:19.780]   but like, you know, it can't quite, you know,
[01:55:19.780 --> 01:55:21.500]   prove the prime number theorem yet.
[01:55:21.500 --> 01:55:23.620]   And I don't, I mean, I guess it's a little bit
[01:55:23.620 --> 01:55:26.540]   the same for humans, although it's weird,
[01:55:26.540 --> 01:55:29.060]   the juxtaposition of things it can do and not.
[01:55:29.060 --> 01:55:30.940]   I guess the main lesson is like,
[01:55:30.940 --> 01:55:34.100]   having theories of intelligence or how intelligence works.
[01:55:34.100 --> 01:55:36.940]   Like, again, a lot of these words
[01:55:36.940 --> 01:55:40.020]   just kind of like dissolve into a continuum, right?
[01:55:40.020 --> 01:55:43.380]   They just kind of like dematerialize.
[01:55:43.380 --> 01:55:45.020]   I think less in terms of intelligence
[01:55:45.020 --> 01:55:47.140]   and more in terms of what we see in front of us.
[01:55:47.140 --> 01:55:49.300]   - Yeah, no, it's really surprising to me.
[01:55:49.300 --> 01:55:50.140]   Two things.
[01:55:50.140 --> 01:55:53.500]   One is how discreet these like different paths
[01:55:53.500 --> 01:55:56.340]   of intelligent things that contribute to loss are
[01:55:56.340 --> 01:55:58.420]   rather than just being like one reasoning circuit
[01:55:58.420 --> 01:55:59.780]   or one general intelligence.
[01:55:59.780 --> 01:56:01.420]   And the other thing talking with you
[01:56:01.420 --> 01:56:03.940]   that is surprising or interesting is,
[01:56:03.940 --> 01:56:06.100]   many years from now, it'll be one of those things
[01:56:06.100 --> 01:56:07.660]   that looking back, it'll be,
[01:56:07.660 --> 01:56:09.820]   why wasn't this obvious to you?
[01:56:09.820 --> 01:56:11.860]   If you're seeing these smooth scaling curves,
[01:56:11.860 --> 01:56:14.500]   why at a time where you're not completely convinced?
[01:56:14.500 --> 01:56:17.780]   So you've been less public than the CEOs
[01:56:17.780 --> 01:56:19.340]   of other AI companies.
[01:56:19.340 --> 01:56:20.860]   You know, you're not posting on Twitter.
[01:56:20.860 --> 01:56:24.740]   You're not doing a lot of podcasts except for this one.
[01:56:24.740 --> 01:56:25.980]   What gives?
[01:56:25.980 --> 01:56:27.940]   Like, why are you off the radar?
[01:56:27.940 --> 01:56:31.300]   - Yeah, I aspire to this and I'm proud of this.
[01:56:31.300 --> 01:56:34.100]   If people think of me as kind of like boring and low profile,
[01:56:34.100 --> 01:56:36.300]   like this is actually kind of what I want.
[01:56:36.300 --> 01:56:37.140]   So I don't know.
[01:56:37.140 --> 01:56:39.460]   I've just seen a number of cases,
[01:56:39.460 --> 01:56:41.460]   a number of people I've worked with
[01:56:41.460 --> 01:56:43.620]   that I think you could say Twitter,
[01:56:43.620 --> 01:56:45.260]   although I think I mean a broader thing,
[01:56:45.260 --> 01:56:48.340]   like just kind of like attaching your incentives
[01:56:48.340 --> 01:56:52.700]   very strongly to like the approval or cheering of a crowd.
[01:56:52.700 --> 01:56:54.340]   I think that can destroy your mind
[01:56:54.340 --> 01:56:56.380]   and in some cases it can destroy your soul.
[01:56:56.380 --> 01:56:59.780]   And so I think I kind of deliberately tried
[01:56:59.780 --> 01:57:02.580]   to be a little bit low profile because I wanna,
[01:57:02.580 --> 01:57:05.180]   I don't know, kind of like defend my ability
[01:57:05.180 --> 01:57:07.860]   to think about things intellectually
[01:57:07.860 --> 01:57:10.740]   in a way that's different from other people
[01:57:10.740 --> 01:57:14.300]   and isn't kind of tinged by the approval of other people.
[01:57:14.300 --> 01:57:18.420]   So I've seen cases of folks who are deep learning skeptics
[01:57:18.420 --> 01:57:21.500]   and they become known as deep learning skeptics on Twitter.
[01:57:21.500 --> 01:57:24.140]   And then even as it starts to become clear to me,
[01:57:24.140 --> 01:57:25.740]   they've kind of sort of changed their mind.
[01:57:25.740 --> 01:57:27.740]   They like, this is their thing on Twitter
[01:57:27.740 --> 01:57:29.420]   and they can't change their Twitter persona
[01:57:29.420 --> 01:57:31.060]   and so forth and so on.
[01:57:31.060 --> 01:57:32.340]   I don't really like the trend
[01:57:32.340 --> 01:57:34.740]   of kind of like personalizing companies,
[01:57:34.740 --> 01:57:38.580]   like the whole like cage match between CEOs approach.
[01:57:38.580 --> 01:57:41.180]   Like I think it distracts people
[01:57:41.180 --> 01:57:43.860]   from the actual merits and concerns
[01:57:43.860 --> 01:57:46.820]   of like the company in question.
[01:57:46.820 --> 01:57:50.100]   Like I kind of want people to like judge
[01:57:50.100 --> 01:57:52.540]   the like nameless bureaucratic institution.
[01:57:52.540 --> 01:57:55.620]   I want people to think in terms
[01:57:55.620 --> 01:57:57.620]   of the nameless bureaucratic institution
[01:57:57.620 --> 01:58:00.340]   and its incentives more than they think in terms of me.
[01:58:00.340 --> 01:58:01.740]   Everyone wants a friendly face,
[01:58:01.740 --> 01:58:04.060]   but actually I think friendly faces can be misleading.
[01:58:04.060 --> 01:58:05.260]   - Okay, well, in this case,
[01:58:05.260 --> 01:58:06.660]   this will be a misleading interview
[01:58:06.660 --> 01:58:08.100]   because this has been a lot of fun.
[01:58:08.100 --> 01:58:09.900]   It'd be like a blast to talk to.
[01:58:09.900 --> 01:58:11.220]   - Indeed.
[01:58:11.220 --> 01:58:12.060]   - Yeah, this has been a blast.
[01:58:12.060 --> 01:58:13.660]   I'm super glad you came on the podcast
[01:58:13.660 --> 01:58:15.300]   and I hope people enjoyed.
[01:58:15.300 --> 01:58:16.780]   - Thanks for having me.
[01:58:16.780 --> 01:58:18.660]   - Hey, everybody.
[01:58:18.660 --> 01:58:20.580]   I hope you enjoyed that episode.
[01:58:20.580 --> 01:58:22.780]   As always, the most helpful thing you can do
[01:58:22.780 --> 01:58:24.540]   is to share the podcast.
[01:58:24.540 --> 01:58:26.260]   Send it to people you think might enjoy it,
[01:58:26.260 --> 01:58:28.380]   put it in Twitter, your group chats, et cetera.
[01:58:28.380 --> 01:58:30.260]   It just splits the world.
[01:58:30.260 --> 01:58:31.500]   Appreciate you listening.
[01:58:31.500 --> 01:58:32.660]   I'll see you next time.
[01:58:32.660 --> 01:58:33.500]   Cheers.
[01:58:33.860 --> 01:58:36.460]   (upbeat music)
[01:58:36.460 --> 01:58:39.060]   (upbeat music)
[01:58:39.060 --> 01:58:41.640]   (upbeat music)
[01:58:41.640 --> 01:58:44.140]   Beep. Beep. Beep.

