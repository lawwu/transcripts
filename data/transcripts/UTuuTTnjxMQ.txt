
[00:00:00.000 --> 00:00:08.480]   [VIDEO PLAYBACK]
[00:00:08.480 --> 00:00:10.280]   -You're failing the line test right now.
[00:00:10.280 --> 00:00:11.040]   Really bad.
[00:00:11.040 --> 00:00:32.300]   [INTERPOSING VOICES]
[00:00:32.300 --> 00:00:34.520]   -Let's get no condoms on the chair.
[00:00:34.520 --> 00:00:39.480]   [LAUGHTER]
[00:00:39.480 --> 00:00:41.440]   -Dude, it is literally foggy.
[00:00:41.440 --> 00:00:45.420]   [INTERPOSING VOICES]
[00:00:45.420 --> 00:00:46.900]   -The video has shown it enough.
[00:00:46.900 --> 00:00:48.360]   But if you want to mess up now--
[00:00:48.360 --> 00:00:52.560]   [INTERPOSING VOICES]
[00:00:52.560 --> 00:00:55.280]   -OK, today I have the pleasure to talk
[00:00:55.280 --> 00:00:58.940]   with two of my good friends, Shilto and Trenton.
[00:00:58.940 --> 00:00:59.440]   Shilto--
[00:00:59.440 --> 00:01:00.400]   -You just mixed up.
[00:01:00.400 --> 00:01:03.380]   [LAUGHTER]
[00:01:03.380 --> 00:01:06.340]   -I wasn't going to say anything.
[00:01:06.340 --> 00:01:07.820]   Let's do this in reverse.
[00:01:07.820 --> 00:01:09.800]   [LAUGHTER]
[00:01:09.800 --> 00:01:11.800]   -How will I start it with my good friends?
[00:01:11.800 --> 00:01:13.300]   [LAUGHTER]
[00:01:13.300 --> 00:01:19.280]   -Yeah, Gemini 1.5, the context is like, just wow.
[00:01:19.280 --> 00:01:19.780]   -Shit.
[00:01:19.780 --> 00:01:27.280]   Anyways, Shilto, Noam Brown, the guy
[00:01:27.280 --> 00:01:30.440]   who wrote the diplomacy paper, he said this about Shilto.
[00:01:30.440 --> 00:01:33.800]   He said, "He's only been in the field for 1.5 years,
[00:01:33.800 --> 00:01:35.400]   but people in AI know that he was
[00:01:35.400 --> 00:01:38.960]   one of the most important people behind Gemini's success."
[00:01:38.960 --> 00:01:41.360]   And Trenton, who's an anthropic, works
[00:01:41.360 --> 00:01:42.800]   on mechanistic interoperability.
[00:01:42.800 --> 00:01:45.260]   And it was widely reported that he has solved alignment.
[00:01:45.260 --> 00:01:47.080]   [LAUGHTER]
[00:01:47.080 --> 00:01:51.320]   -With his one friend on Twitter.
[00:01:51.320 --> 00:01:53.640]   -So this will be a capabilities-only podcast.
[00:01:53.640 --> 00:01:57.200]   Alignment is already solved, so no need to discuss further.
[00:01:57.200 --> 00:02:00.320]   OK, so let's start by talking about context links.
[00:02:00.320 --> 00:02:02.320]   -Yep.
[00:02:02.320 --> 00:02:03.960]   -It seemed to be under-hyped, given
[00:02:03.960 --> 00:02:06.320]   how important it seems to me to be that you can just
[00:02:06.320 --> 00:02:08.440]   put a million tokens into context.
[00:02:08.440 --> 00:02:10.840]   There's apparently some other news that got pushed
[00:02:10.840 --> 00:02:12.000]   to the front for some reason.
[00:02:12.000 --> 00:02:15.280]   But yeah, tell me about how you see
[00:02:15.280 --> 00:02:17.400]   the future of long-context links and what
[00:02:17.400 --> 00:02:18.600]   that implies for these models.
[00:02:18.600 --> 00:02:19.080]   -Yeah.
[00:02:19.080 --> 00:02:20.560]   So I think it's really under-hyped,
[00:02:20.560 --> 00:02:22.140]   because until I started working on it,
[00:02:22.140 --> 00:02:24.180]   I didn't really appreciate how much of a step-up
[00:02:24.180 --> 00:02:26.040]   in intelligence it was for the model
[00:02:26.040 --> 00:02:28.660]   to have the onboarding problem basically instantly solved.
[00:02:28.660 --> 00:02:30.040]   And you can see that a little bit
[00:02:30.040 --> 00:02:31.660]   in the perplexity graphs in the paper,
[00:02:31.660 --> 00:02:34.820]   where just throwing millions of tokens' worth of context
[00:02:34.820 --> 00:02:37.240]   about a code base allows it to become dramatically better
[00:02:37.240 --> 00:02:38.480]   at predicting the next token in a way
[00:02:38.480 --> 00:02:40.900]   that you'd normally associate with huge increments in model
[00:02:40.900 --> 00:02:41.400]   scale.
[00:02:41.400 --> 00:02:42.400]   But you don't need that.
[00:02:42.400 --> 00:02:45.160]   All you need is a new context.
[00:02:45.160 --> 00:02:49.200]   So under-hyped, and buried by some other news.
[00:02:49.200 --> 00:02:53.420]   -In context, are they as sample efficient and smart as humans?
[00:02:53.420 --> 00:02:55.040]   -I think that's really worth exploring.
[00:02:55.040 --> 00:02:58.400]   Because, for example, one of the evals that we did in the paper
[00:02:58.400 --> 00:03:02.020]   has it learning a language in context
[00:03:02.020 --> 00:03:05.180]   better than a human expert could learn that new language
[00:03:05.180 --> 00:03:06.680]   over the course of a couple months.
[00:03:06.680 --> 00:03:08.780]   And this is only a pretty small demonstration.
[00:03:08.780 --> 00:03:11.300]   But I'd be really interested to see things like Atari games,
[00:03:11.300 --> 00:03:13.720]   or something like that, where you throw in a couple hundred,
[00:03:13.720 --> 00:03:15.500]   like 1,000 frames, labeled actions,
[00:03:15.500 --> 00:03:17.060]   and then in the same way that you'd show your friend how
[00:03:17.060 --> 00:03:19.460]   to play a game, and see if it's able to reason through.
[00:03:19.460 --> 00:03:21.460]   It might, at the moment, with the infrastructure and stuff,
[00:03:21.460 --> 00:03:24.020]   it's still a little bit slow at doing that.
[00:03:24.020 --> 00:03:28.600]   But I would guess that might just work out of the box
[00:03:28.600 --> 00:03:30.000]   in a way that would be pretty mind-blowing.
[00:03:30.000 --> 00:03:32.680]   -And crucially, I think this language was esoteric enough
[00:03:32.680 --> 00:03:33.840]   that it wasn't in the training data.
[00:03:33.840 --> 00:03:34.780]   -Right, exactly.
[00:03:34.780 --> 00:03:37.920]   If you look at the model before it has that context thrown in,
[00:03:37.920 --> 00:03:39.120]   it doesn't know the language at all,
[00:03:39.120 --> 00:03:40.480]   and it can't get any translations.
[00:03:40.480 --> 00:03:42.800]   -And this is an actual human language, not just --
[00:03:42.800 --> 00:03:44.840]   -Yeah, exactly, an actual human language.
[00:03:44.840 --> 00:03:47.720]   -So if this is true, it seems to me that these models
[00:03:47.720 --> 00:03:51.240]   are already, in an important sense, superhuman.
[00:03:51.240 --> 00:03:53.080]   Not in the sense that they're smarter than us,
[00:03:53.080 --> 00:03:56.480]   but I can't keep a million tokens in my context
[00:03:56.480 --> 00:03:57.680]   when I'm trying to solve a problem,
[00:03:57.680 --> 00:03:59.960]   remembering and integrating all the information
[00:03:59.960 --> 00:04:02.360]   in an entire code base.
[00:04:02.360 --> 00:04:05.200]   Am I wrong in thinking this is, like, a huge unlock?
[00:04:05.200 --> 00:04:07.680]   -I actually generally think that's true.
[00:04:07.680 --> 00:04:09.840]   Like, previously, I've been frustrated
[00:04:09.840 --> 00:04:11.080]   when models aren't as smart.
[00:04:11.080 --> 00:04:12.600]   Like, you ask them a question, and you want it
[00:04:12.600 --> 00:04:15.080]   to be smarter than you, or to know things that you don't.
[00:04:15.080 --> 00:04:17.240]   And this allows them to know things that you don't
[00:04:17.240 --> 00:04:19.840]   in a way that it just ingests a huge amount of information
[00:04:19.840 --> 00:04:21.720]   in a way you just can't.
[00:04:21.720 --> 00:04:24.120]   So, yeah, it's extremely important.
[00:04:24.120 --> 00:04:26.400]   -Well, how do we explain in-context learning?
[00:04:26.400 --> 00:04:27.680]   -Yeah, so there's a piece of --
[00:04:27.680 --> 00:04:29.120]   there's a line of work I quite like
[00:04:29.120 --> 00:04:31.320]   where it looks at in-context learning
[00:04:31.320 --> 00:04:34.920]   as basically, like, very similar to gradient descent,
[00:04:34.920 --> 00:04:37.560]   but, like, the attention operation
[00:04:37.560 --> 00:04:40.280]   can be viewed as gradient descent on the in-context data.
[00:04:40.280 --> 00:04:42.000]   That paper had some cool plots where they basically showed,
[00:04:42.000 --> 00:04:43.440]   like, we take n steps of gradient descent,
[00:04:43.440 --> 00:04:45.760]   and that looks like n layers of in-context learning.
[00:04:45.760 --> 00:04:46.960]   It looks very similar, so I think, like,
[00:04:46.960 --> 00:04:48.600]   that's one way of viewing it
[00:04:48.600 --> 00:04:50.360]   and trying to understand what's going on.
[00:04:50.360 --> 00:04:53.280]   -Yeah, and you can ignore what I'm about to say
[00:04:53.280 --> 00:04:54.880]   because, given the introduction,
[00:04:54.880 --> 00:04:57.200]   alignment is solved and safety isn't a problem.
[00:04:57.200 --> 00:05:00.760]   But I think the context stuff does get problematic,
[00:05:00.760 --> 00:05:02.920]   but also interesting here.
[00:05:02.920 --> 00:05:04.400]   I think there'll be more work coming out
[00:05:04.400 --> 00:05:07.240]   in the not-too-distant future
[00:05:07.240 --> 00:05:12.440]   around what happens if you give a 100-shot prompt
[00:05:12.440 --> 00:05:15.920]   for jailbreaks, adversarial attacks.
[00:05:15.920 --> 00:05:18.440]   It's also interesting in the sense of,
[00:05:18.440 --> 00:05:19.760]   if your model is doing gradient descent
[00:05:19.760 --> 00:05:21.720]   and learning on the fly,
[00:05:21.720 --> 00:05:25.880]   even if it's been trained to be harmless,
[00:05:25.880 --> 00:05:28.760]   you're dealing with a totally new model, in a way.
[00:05:28.760 --> 00:05:29.960]   You're, like, fine-tuning, but in a way
[00:05:29.960 --> 00:05:32.400]   where you can't control what's going on.
[00:05:32.400 --> 00:05:34.800]   -Can you explain what do you mean by gradient descent
[00:05:34.800 --> 00:05:36.880]   is happening in the forward pass and attention?
[00:05:36.880 --> 00:05:37.800]   -Yeah. -You probably have a better...
[00:05:37.800 --> 00:05:39.120]   -No, no, no. There was something in the paper
[00:05:39.120 --> 00:05:41.640]   about trying to teach the model to do linear regression.
[00:05:41.640 --> 00:05:42.840]   -Right. -But, like, just through
[00:05:42.840 --> 00:05:44.320]   the number of samples they gave in the context.
[00:05:44.320 --> 00:05:46.080]   -Yeah. -And you can see,
[00:05:46.080 --> 00:05:49.360]   if you plot on the x-axis, like, number of shots that it has,
[00:05:49.360 --> 00:05:51.680]   or examples, and then, like, the loss it gets
[00:05:51.680 --> 00:05:53.760]   on just, like, ordinary least-squares regression.
[00:05:53.760 --> 00:05:55.760]   -Yeah. -That will go down with time.
[00:05:55.760 --> 00:05:57.280]   -And it goes down exactly matched
[00:05:57.280 --> 00:05:59.160]   with number of gradient descent steps.
[00:05:59.160 --> 00:06:01.920]   -Yeah. Yeah, exactly. -Okay.
[00:06:01.920 --> 00:06:04.520]   I only read the interim discussion section of that paper,
[00:06:04.520 --> 00:06:08.720]   but in the discussion, the way they framed it is that
[00:06:08.720 --> 00:06:12.760]   in order to get better at long-context tasks,
[00:06:12.760 --> 00:06:15.400]   the model has to get better at learning to learn
[00:06:15.400 --> 00:06:17.280]   from these examples or from the context
[00:06:17.280 --> 00:06:19.280]   that is already within the window.
[00:06:19.280 --> 00:06:24.520]   And the implication of that is the model,
[00:06:24.520 --> 00:06:27.400]   if, like, meta-learning happens because it has to learn
[00:06:27.400 --> 00:06:29.560]   how to get better at long-context tasks,
[00:06:29.560 --> 00:06:33.200]   then, in some important sense, the task of intelligence
[00:06:33.200 --> 00:06:35.320]   is, like, requires long-context examples
[00:06:35.320 --> 00:06:36.640]   and long-context training.
[00:06:36.640 --> 00:06:39.000]   -Like, meta-learning, like, you have to induce meta-learning.
[00:06:39.000 --> 00:06:40.120]   -Right. -Like, understanding how to better
[00:06:40.120 --> 00:06:41.800]   induce meta-learning in your pre-training process
[00:06:41.800 --> 00:06:43.200]   is, like, a very important thing to actually get that,
[00:06:43.200 --> 00:06:45.000]   like, flexible or adaptive intelligence.
[00:06:45.000 --> 00:06:46.720]   -Right, but you can proxy for that
[00:06:46.720 --> 00:06:50.680]   just by getting better at doing long-context tasks.
[00:06:50.680 --> 00:06:52.800]   One of the bottlenecks for AI progress
[00:06:52.800 --> 00:06:56.040]   that many people identify is the inability of these models
[00:06:56.040 --> 00:07:00.000]   to perform tasks on long horizons,
[00:07:00.000 --> 00:07:03.200]   which means engaging with a task for many hours
[00:07:03.200 --> 00:07:04.840]   or even many weeks or months,
[00:07:04.840 --> 00:07:07.360]   where, like, if I have, I don't know,
[00:07:07.360 --> 00:07:09.200]   an assistant or an employee or something,
[00:07:09.200 --> 00:07:11.600]   they can just do a thing I tell them for a while.
[00:07:11.600 --> 00:07:13.440]   And AI agents haven't taken off for this reason,
[00:07:13.440 --> 00:07:14.560]   from what I understand.
[00:07:14.560 --> 00:07:17.880]   So, how linked are long-context windows
[00:07:17.880 --> 00:07:19.320]   and the ability to perform well on them
[00:07:19.320 --> 00:07:22.000]   and the ability to do these kinds of long-horizon tasks
[00:07:22.000 --> 00:07:25.920]   that require you to engage with an assignment for many hours?
[00:07:25.920 --> 00:07:27.800]   Or are these unrelated concepts?
[00:07:27.800 --> 00:07:30.760]   -I mean, I would actually take issue with that being the reason
[00:07:30.760 --> 00:07:32.400]   that agents haven't taken off,
[00:07:32.400 --> 00:07:34.440]   where I think that's more about, like, nines of reliability
[00:07:34.440 --> 00:07:36.040]   and the model actually successfully doing things.
[00:07:36.040 --> 00:07:38.840]   If you just can't chain tasks successfully
[00:07:38.840 --> 00:07:39.840]   with high enough probability,
[00:07:39.840 --> 00:07:41.840]   then you won't get something that looks like an agent.
[00:07:41.840 --> 00:07:43.560]   And that's why something like an agent
[00:07:43.560 --> 00:07:46.000]   might follow more of a step function in sort of,
[00:07:46.000 --> 00:07:48.120]   usually, like, GPT-4 class models,
[00:07:48.120 --> 00:07:50.600]   Gemini Ultra class models, they're not enough.
[00:07:50.600 --> 00:07:52.360]   But maybe, like, the next increment on model scale
[00:07:52.360 --> 00:07:53.760]   means that you get that extra nine,
[00:07:53.760 --> 00:07:56.440]   even though, like, the loss isn't going down that dramatically,
[00:07:56.440 --> 00:07:58.400]   that, like, small amount of extra ability
[00:07:58.400 --> 00:08:00.240]   gives you the extra nine.
[00:08:00.240 --> 00:08:02.440]   And, like, yeah, obviously, you need some amount of context
[00:08:02.440 --> 00:08:04.800]   to fit long-horizon tasks,
[00:08:04.800 --> 00:08:07.600]   but I don't think that's been the limiting factor up to now.
[00:08:07.600 --> 00:08:08.600]   -Yeah.
[00:08:08.600 --> 00:08:11.400]   The NURBS Best Paper this year by Ryland Schaefer
[00:08:11.400 --> 00:08:14.520]   was the lead author, points to this as, like,
[00:08:14.520 --> 00:08:18.280]   the emergence of Mirage, where people will have a task
[00:08:18.280 --> 00:08:20.200]   and you get the right or wrong answer
[00:08:20.200 --> 00:08:23.960]   depending on if you've sampled the last five tokens correctly.
[00:08:23.960 --> 00:08:25.640]   And so, naturally, that's --
[00:08:25.640 --> 00:08:26.920]   you're multiplying the probability
[00:08:26.920 --> 00:08:28.880]   of sampling all of those.
[00:08:28.880 --> 00:08:30.600]   And if you don't have enough nines of reliability,
[00:08:30.600 --> 00:08:31.920]   then you're not going to get emergence.
[00:08:31.920 --> 00:08:34.600]   And all of a sudden, you do, and it's like,
[00:08:34.600 --> 00:08:36.040]   "Oh, my gosh, this ability is emergent,"
[00:08:36.040 --> 00:08:38.960]   when actually it was kind of almost there to begin with.
[00:08:38.960 --> 00:08:40.400]   -And there are ways that you can find, like,
[00:08:40.400 --> 00:08:42.160]   a smooth metric for that.
[00:08:42.160 --> 00:08:45.000]   -Yeah, Human Eval or whatever, in the GPT-4 paper,
[00:08:45.000 --> 00:08:47.360]   the coding problems, they measure --
[00:08:47.360 --> 00:08:48.800]   -Log-pass, right? -Exactly.
[00:08:48.800 --> 00:08:50.200]   -Yeah. -For the audience,
[00:08:50.200 --> 00:08:54.160]   the context on this is, basically,
[00:08:54.160 --> 00:08:56.000]   the idea is you want to --
[00:08:56.000 --> 00:08:57.840]   when you're measuring how much progress there has been
[00:08:57.840 --> 00:09:00.520]   on a specific task, like solving coding problems,
[00:09:00.520 --> 00:09:03.560]   you up-weight it when it gets it right
[00:09:03.560 --> 00:09:04.720]   only one in 1,000 times.
[00:09:04.720 --> 00:09:06.200]   You don't, like, give it a one-in-1,000 score
[00:09:06.200 --> 00:09:07.920]   because it's like, "Oh, got it right some of the time,"
[00:09:07.920 --> 00:09:09.320]   and so the curve you see is, like,
[00:09:09.320 --> 00:09:11.000]   it gets it right one in 1,000, then one in 100,
[00:09:11.000 --> 00:09:13.560]   then one in 10, and so forth.
[00:09:13.560 --> 00:09:15.760]   So, actually, I want to follow up on this.
[00:09:15.760 --> 00:09:18.400]   So, if your claim is that the AI agents
[00:09:18.400 --> 00:09:20.640]   haven't taken off because of reliability
[00:09:20.640 --> 00:09:23.560]   rather than long-horizon task performance,
[00:09:23.560 --> 00:09:27.360]   isn't the lack of reliability
[00:09:27.360 --> 00:09:30.520]   when a task is chained on top of another task
[00:09:30.520 --> 00:09:31.720]   on top of another task,
[00:09:31.720 --> 00:09:33.120]   isn't that exactly the difficulty
[00:09:33.120 --> 00:09:34.280]   with long-horizon tasks?
[00:09:34.280 --> 00:09:36.480]   Is that, like, you have to do 10 things in a row
[00:09:36.480 --> 00:09:37.880]   or 100 things in a row,
[00:09:37.880 --> 00:09:42.240]   and diminishing the reliability of any one of them,
[00:09:42.240 --> 00:09:44.000]   or, yeah, the probability goes down
[00:09:44.000 --> 00:09:46.240]   from 99.99 to 99.9,
[00:09:46.240 --> 00:09:48.400]   then, like, the whole thing gets multiplied together,
[00:09:48.400 --> 00:09:50.520]   and the whole thing becomes much less likely to happen.
[00:09:50.520 --> 00:09:52.080]   - That is exactly the problem,
[00:09:52.080 --> 00:09:53.840]   but the key issue you're pointing out there
[00:09:53.840 --> 00:09:57.160]   is that your base task-solve rate is 90%,
[00:09:57.160 --> 00:10:00.640]   and if it was 99%, then chaining doesn't become a problem.
[00:10:00.640 --> 00:10:02.560]   - But also-- - Yeah, exactly.
[00:10:02.560 --> 00:10:03.640]   And I think this is also something
[00:10:03.640 --> 00:10:05.200]   that just, like, hasn't been properly studied enough.
[00:10:05.200 --> 00:10:06.960]   If you look at all of the evals that are commonly--
[00:10:06.960 --> 00:10:10.720]   like, the academic evals are a single problem, right?
[00:10:10.720 --> 00:10:11.960]   You know, like, the math problem.
[00:10:11.960 --> 00:10:14.000]   It's, like, one, like, typical math problem,
[00:10:14.000 --> 00:10:15.880]   or MMOU, it's, like, one university-level,
[00:10:15.880 --> 00:10:18.320]   like, problem from across different topics.
[00:10:18.320 --> 00:10:21.400]   You are beginning to start to see evals
[00:10:21.400 --> 00:10:23.680]   looking at this properly via more complex tasks,
[00:10:23.680 --> 00:10:24.640]   like SuiteBench,
[00:10:24.640 --> 00:10:26.080]   where they take a whole bunch of GitHub issues,
[00:10:26.080 --> 00:10:28.200]   and that is, like, a reasonably long-horizon task,
[00:10:28.200 --> 00:10:30.280]   but it's still not a multi--
[00:10:30.280 --> 00:10:32.440]   it's, like, a multi, you know, sub-hour
[00:10:32.440 --> 00:10:34.960]   as opposed to, like, multi-hour or multi-day task.
[00:10:35.640 --> 00:10:37.000]   And so I think one of the things
[00:10:37.000 --> 00:10:40.800]   that will be really important to do over the next however long
[00:10:40.800 --> 00:10:42.880]   is understand better what does success rate
[00:10:42.880 --> 00:10:44.680]   over a long-horizon task look like.
[00:10:44.680 --> 00:10:46.040]   And I think that's even important to understand
[00:10:46.040 --> 00:10:47.800]   what the economic impact of these models might be,
[00:10:47.800 --> 00:10:50.760]   and, like, actually properly judge increasing capabilities
[00:10:50.760 --> 00:10:53.560]   by, like, cutting down the tasks that we do
[00:10:53.560 --> 00:10:54.840]   and the inputs and outputs involved
[00:10:54.840 --> 00:10:57.560]   into minutes or hours or days,
[00:10:57.560 --> 00:10:59.600]   and seeing how good it is at successively, like,
[00:10:59.600 --> 00:11:01.160]   chaining and completing tasks
[00:11:01.160 --> 00:11:02.800]   at those different resolutions of time.
[00:11:02.800 --> 00:11:03.640]   Because then that tells you, like,
[00:11:03.640 --> 00:11:07.040]   how automateable a job family or task family is
[00:11:07.040 --> 00:11:09.400]   in a way that, like, MMOU school is doing.
[00:11:09.400 --> 00:11:10.880]   - Mm.
[00:11:10.880 --> 00:11:11.880]   - I mean, it was less than a year ago
[00:11:11.880 --> 00:11:14.200]   that we introduced 100K context windows,
[00:11:14.200 --> 00:11:16.240]   and I think everyone was pretty surprised by that.
[00:11:16.240 --> 00:11:19.520]   So, yeah, everyone just kind of had this soundbite
[00:11:19.520 --> 00:11:21.040]   of quadratic attention costs,
[00:11:21.040 --> 00:11:24.360]   and we can't have long context windows, and here we are.
[00:11:24.360 --> 00:11:27.440]   So, yeah, like, the benchmarks are being actively made.
[00:11:27.440 --> 00:11:28.920]   - Wait, wait, so doesn't the fact
[00:11:28.920 --> 00:11:31.200]   that there's these companies, Google and, I don't know,
[00:11:31.200 --> 00:11:34.360]   Magic, maybe others, who have million token attention
[00:11:34.360 --> 00:11:36.160]   imply that the quadrat--
[00:11:36.160 --> 00:11:38.440]   You shouldn't say anything, 'cause you're not allowed.
[00:11:38.440 --> 00:11:39.680]   But doesn't that, like, imply
[00:11:39.680 --> 00:11:40.920]   that it's not quadratic anymore,
[00:11:40.920 --> 00:11:42.400]   or are they just eating the cost?
[00:11:42.400 --> 00:11:44.400]   - Well, like, who knows what Google is doing
[00:11:44.400 --> 00:11:46.040]   for its long context scheme, right?
[00:11:46.040 --> 00:11:49.000]   Like, I'm not saying it's either.
[00:11:49.000 --> 00:11:50.200]   One other thing that's frustrated me
[00:11:50.200 --> 00:11:53.920]   about, like, the general research field's approach
[00:11:53.920 --> 00:11:56.080]   to attention is that there's an important way
[00:11:56.080 --> 00:11:59.000]   in which the quadratic cost of attention
[00:11:59.000 --> 00:12:01.440]   is actually dominated in typical dense transformers
[00:12:01.440 --> 00:12:03.360]   by the MLP block.
[00:12:03.360 --> 00:12:05.640]   So, you have this N-squared term
[00:12:05.640 --> 00:12:06.720]   that's associated with attention,
[00:12:06.720 --> 00:12:07.720]   but you also have an N-squared term
[00:12:07.720 --> 00:12:08.920]   that's associated with the D model,
[00:12:08.920 --> 00:12:11.240]   the residual stream dimension of the model.
[00:12:11.240 --> 00:12:14.480]   And if you look, I think Sasha Rush has a great tweet
[00:12:14.480 --> 00:12:16.360]   where he looks like basically plots the curve
[00:12:16.360 --> 00:12:17.440]   of the cost of attention,
[00:12:17.440 --> 00:12:19.760]   respective to, like, the cost of, like, really large models,
[00:12:19.760 --> 00:12:21.680]   and attention actually trails off.
[00:12:21.680 --> 00:12:24.080]   And you actually need to be doing pretty long context
[00:12:24.080 --> 00:12:28.640]   before that term becomes, like, really important.
[00:12:28.640 --> 00:12:31.120]   And the second thing is that people often talk
[00:12:31.120 --> 00:12:33.240]   about how attention at inference time
[00:12:33.240 --> 00:12:35.840]   is such a huge cost, right?
[00:12:35.840 --> 00:12:37.320]   And if you think about
[00:12:37.320 --> 00:12:39.160]   when you're actually generating tokens,
[00:12:39.160 --> 00:12:41.400]   the operation is not N-squared.
[00:12:41.400 --> 00:12:45.160]   It is one Q, like, one set of Q vectors
[00:12:45.160 --> 00:12:46.840]   looks up a whole bunch of KB vectors,
[00:12:46.840 --> 00:12:48.320]   and that's linear with respect
[00:12:48.320 --> 00:12:51.640]   to the amount of, like, context that the model has.
[00:12:51.640 --> 00:12:54.600]   And so, I think this drives a lot of the, like,
[00:12:54.600 --> 00:12:56.800]   recurrence and state space research
[00:12:56.800 --> 00:12:57.920]   where people have this meme of, oh, like,
[00:12:57.920 --> 00:12:59.920]   linear attention and all this stuff.
[00:12:59.920 --> 00:13:01.080]   And as Trenton said,
[00:13:01.080 --> 00:13:04.280]   there's, like, a graveyard of ideas around attention.
[00:13:04.280 --> 00:13:07.360]   And not to think I don't think it's worth exploring,
[00:13:07.360 --> 00:13:09.680]   but I think it's important to consider
[00:13:09.680 --> 00:13:12.520]   where the actual, like, strengths and weaknesses of it are.
[00:13:12.520 --> 00:13:14.760]   - Okay, so what do you make of this take?
[00:13:14.760 --> 00:13:18.520]   As we move forward through the takeoff,
[00:13:18.520 --> 00:13:21.400]   more and more of the learning happens in the forward pass.
[00:13:21.400 --> 00:13:23.160]   So originally, like, all the learning happens
[00:13:23.160 --> 00:13:25.880]   in the backward, you know, during, like, this, like,
[00:13:25.880 --> 00:13:29.400]   bottom-up sort of hill-climbing evolutionary process.
[00:13:29.400 --> 00:13:31.600]   If you think in the limit during the intelligence explosion,
[00:13:31.600 --> 00:13:33.040]   it just, like, the AI is, like,
[00:13:33.040 --> 00:13:34.880]   maybe, like, handwriting the weights
[00:13:34.880 --> 00:13:37.000]   or, like, doing go-fi or something.
[00:13:37.000 --> 00:13:38.840]   And we're in, like, the middle step
[00:13:38.840 --> 00:13:41.600]   where, like, a lot of learning happens in context now
[00:13:41.600 --> 00:13:43.040]   with these models.
[00:13:43.040 --> 00:13:44.880]   A lot of it happens within the backward pass.
[00:13:44.880 --> 00:13:47.480]   Does this seem like a meaningful gradient
[00:13:47.480 --> 00:13:48.680]   along which progress is happening?
[00:13:48.680 --> 00:13:52.320]   Of, like, how much, 'cause the broader thing being the,
[00:13:52.320 --> 00:13:54.240]   if you're learning in the forward pass,
[00:13:54.240 --> 00:13:55.480]   it's, like, much more sample efficient
[00:13:55.480 --> 00:13:57.040]   'cause you can kind of, like, basically think
[00:13:57.040 --> 00:13:57.880]   as you're learning.
[00:13:57.880 --> 00:13:59.240]   Like, when humans, when you read a textbook,
[00:13:59.240 --> 00:14:02.040]   you're not just skimming it and trying to absorb what,
[00:14:02.040 --> 00:14:05.680]   you know, what inductive, these words follow these words.
[00:14:05.680 --> 00:14:07.280]   You, like, read it and you think about it,
[00:14:07.280 --> 00:14:10.040]   and then you read some more, you think about it.
[00:14:10.040 --> 00:14:10.880]   I don't know.
[00:14:10.880 --> 00:14:11.840]   Does this seem like a sensible way
[00:14:11.840 --> 00:14:13.960]   to think about the progress?
[00:14:13.960 --> 00:14:14.800]   - Yeah.
[00:14:14.800 --> 00:14:17.000]   It may just be one of the ways in which, like,
[00:14:17.000 --> 00:14:18.240]   you know, birds and planes, like, fly,
[00:14:18.240 --> 00:14:19.160]   but they fly slightly differently.
[00:14:19.160 --> 00:14:21.600]   And, like, the virtue of technology allows us to do that,
[00:14:21.600 --> 00:14:25.800]   like, I used to accomplish things that birds can't.
[00:14:25.800 --> 00:14:27.240]   It might be that context length is similar
[00:14:27.240 --> 00:14:31.080]   in that it allows it to have a working memory that we can't,
[00:14:31.080 --> 00:14:32.240]   but functionally is not, like,
[00:14:32.240 --> 00:14:34.880]   the key thing towards actual reasoning.
[00:14:34.880 --> 00:14:38.200]   The key step between GPT-2 and GPT-3
[00:14:38.200 --> 00:14:39.280]   was that all of a sudden, like,
[00:14:39.280 --> 00:14:40.600]   there was this meta-learning behavior
[00:14:40.600 --> 00:14:42.440]   that was observed in training,
[00:14:42.440 --> 00:14:44.920]   like, in the pre-training of the model.
[00:14:44.920 --> 00:14:48.200]   And that's, as you said, like, it's something to do with,
[00:14:48.200 --> 00:14:49.440]   you give it some amount of context,
[00:14:49.440 --> 00:14:50.560]   it's able to adapt to that context,
[00:14:50.560 --> 00:14:52.840]   and that was a behavior that wasn't really observed
[00:14:52.840 --> 00:14:54.360]   before that at all.
[00:14:54.360 --> 00:14:56.480]   And maybe that's a mixture of property of context
[00:14:56.480 --> 00:14:57.400]   and scale and this kind of stuff,
[00:14:57.400 --> 00:14:59.720]   that wouldn't have occurred to model tiny context,
[00:14:59.720 --> 00:15:00.560]   I would say.
[00:15:00.560 --> 00:15:03.680]   - This is actually an interesting point.
[00:15:03.680 --> 00:15:07.080]   So when we talk about scaling up these models,
[00:15:07.080 --> 00:15:08.600]   how much of it comes from
[00:15:08.600 --> 00:15:10.840]   just making the models themselves bigger?
[00:15:10.840 --> 00:15:12.280]   And how much comes from the fact
[00:15:12.280 --> 00:15:15.720]   that during any single call,
[00:15:15.720 --> 00:15:17.440]   you are using more compute?
[00:15:17.440 --> 00:15:18.320]   So if you think of diffusion,
[00:15:18.320 --> 00:15:20.760]   you can just iteratively keep adding more compute.
[00:15:20.760 --> 00:15:22.800]   And if adaptive compute is solved,
[00:15:22.800 --> 00:15:24.000]   you can keep doing that.
[00:15:24.000 --> 00:15:25.840]   And in this case,
[00:15:25.840 --> 00:15:27.840]   if there's a quadratic penalty for attention,
[00:15:27.840 --> 00:15:29.520]   but you're doing long context anyways,
[00:15:29.520 --> 00:15:31.520]   then you're still dumping in more compute
[00:15:31.520 --> 00:15:32.920]   during, not during training,
[00:15:32.920 --> 00:15:34.400]   or not during having bigger models,
[00:15:34.400 --> 00:15:35.960]   but just like, yeah.
[00:15:35.960 --> 00:15:37.920]   - Yeah, it's interesting,
[00:15:37.920 --> 00:15:39.280]   'cause you do get more forward passes
[00:15:39.280 --> 00:15:40.640]   by having more tokens.
[00:15:40.640 --> 00:15:41.480]   - Right.
[00:15:41.480 --> 00:15:43.520]   - My one gripe,
[00:15:43.520 --> 00:15:45.600]   I guess I have two gripes with this, though.
[00:15:45.600 --> 00:15:46.440]   Maybe three.
[00:15:46.440 --> 00:15:47.520]   So one, like, in the output.
[00:15:48.520 --> 00:15:50.680]   - In the AlphaMold paper,
[00:15:50.680 --> 00:15:52.200]   one of the transformer modules,
[00:15:52.200 --> 00:15:53.040]   they have a few,
[00:15:53.040 --> 00:15:55.080]   and the architecture is very intricate.
[00:15:55.080 --> 00:15:55.920]   But they do, I think,
[00:15:55.920 --> 00:15:58.040]   five forward passes through it,
[00:15:58.040 --> 00:16:01.400]   and will gradually refine their solution as a result.
[00:16:01.400 --> 00:16:03.120]   You can also kind of think of the residual stream,
[00:16:03.120 --> 00:16:04.280]   I mean, Sholto alluded to kind of
[00:16:04.280 --> 00:16:05.400]   the read-write operations,
[00:16:05.400 --> 00:16:08.120]   as like a poor man's adaptive compute,
[00:16:08.120 --> 00:16:08.960]   where it's like,
[00:16:08.960 --> 00:16:09.960]   I'm just gonna give you all these layers,
[00:16:09.960 --> 00:16:11.160]   and if you wanna use them, great.
[00:16:11.160 --> 00:16:13.520]   If you don't, then that's also fine.
[00:16:13.520 --> 00:16:14.880]   And then people will be like,
[00:16:14.880 --> 00:16:16.800]   oh, well, the brain is recurrent,
[00:16:16.800 --> 00:16:19.440]   and you can do however many loops through it you want.
[00:16:19.440 --> 00:16:21.040]   And I think, to a certain extent, that's right.
[00:16:21.040 --> 00:16:21.960]   If I ask you a hard question,
[00:16:21.960 --> 00:16:23.520]   you'll spend more time thinking about it,
[00:16:23.520 --> 00:16:25.520]   and that would correspond to more forward passes.
[00:16:25.520 --> 00:16:28.600]   But I think there's a finite number of forward passes
[00:16:28.600 --> 00:16:30.000]   that you can do.
[00:16:30.000 --> 00:16:31.400]   It's kind of with language, as well.
[00:16:31.400 --> 00:16:32.880]   People are like, oh, well, human language
[00:16:32.880 --> 00:16:34.720]   can have infinite recursion in it,
[00:16:34.720 --> 00:16:37.160]   infinite nested statements of like,
[00:16:37.160 --> 00:16:40.160]   the boy jumped over the bear that was doing this,
[00:16:40.160 --> 00:16:41.800]   that had done this, that had done that.
[00:16:41.800 --> 00:16:44.360]   But empirically, you'll only see
[00:16:44.360 --> 00:16:48.080]   five to seven levels of recursion,
[00:16:48.080 --> 00:16:50.800]   which kind of relates to whatever that magic number
[00:16:50.800 --> 00:16:52.760]   of how many things you can hold in working memory
[00:16:52.760 --> 00:16:53.920]   at any given time is.
[00:16:53.920 --> 00:16:58.360]   And so, yeah, it's not infinitely recursive,
[00:16:58.360 --> 00:17:01.320]   but does that matter in the regime of human intelligence?
[00:17:01.320 --> 00:17:03.520]   And can you not just add more layers?
[00:17:03.520 --> 00:17:06.000]   - Breakdown, for me, you're referring to this
[00:17:06.000 --> 00:17:07.840]   in some of your previous answers of,
[00:17:07.840 --> 00:17:09.960]   listen, you have these long contexts,
[00:17:09.960 --> 00:17:12.520]   and you can hold more things in memory,
[00:17:12.520 --> 00:17:14.280]   but ultimately, it comes down to your ability
[00:17:14.280 --> 00:17:17.920]   to mix concepts together to do some kind of reasoning.
[00:17:17.920 --> 00:17:21.280]   And these models aren't necessarily human level at that,
[00:17:21.280 --> 00:17:22.720]   even in context.
[00:17:22.720 --> 00:17:26.040]   Breakdown, for me, how you see storing
[00:17:26.040 --> 00:17:28.320]   just raw information versus reasoning
[00:17:28.320 --> 00:17:29.160]   and what's in between.
[00:17:29.160 --> 00:17:30.640]   Like, where is the reasoning happening?
[00:17:30.640 --> 00:17:32.520]   Is that, where is it just like,
[00:17:32.520 --> 00:17:33.720]   storing raw information happening?
[00:17:33.720 --> 00:17:37.000]   What's different between them in these models?
[00:17:37.000 --> 00:17:41.000]   - Yeah, I don't have a super crisp answer for you here.
[00:17:42.200 --> 00:17:44.760]   I mean, obviously, with the input and output of the model,
[00:17:44.760 --> 00:17:47.080]   you're mapping back to actual tokens, right?
[00:17:47.080 --> 00:17:48.560]   And then, in between that,
[00:17:48.560 --> 00:17:51.200]   you're doing higher level processing.
[00:17:51.200 --> 00:17:54.200]   - Before we get deeper into this,
[00:17:54.200 --> 00:17:56.120]   we should explain to the audience,
[00:17:56.120 --> 00:17:58.520]   you referred earlier to Anthropic's way
[00:17:58.520 --> 00:17:59.800]   of thinking about transformers
[00:17:59.800 --> 00:18:02.640]   as these read-write operations that layers do.
[00:18:02.640 --> 00:18:04.680]   One of you should just kind of explain at a high level
[00:18:04.680 --> 00:18:06.000]   what you mean by that.
[00:18:06.000 --> 00:18:07.440]   - So the residual stream,
[00:18:07.440 --> 00:18:10.440]   imagine you're in a boat going down a river,
[00:18:10.440 --> 00:18:14.400]   and the boat is kind of the current query
[00:18:14.400 --> 00:18:16.080]   where you're trying to predict the next token.
[00:18:16.080 --> 00:18:17.960]   So it's the cat sat on the blank.
[00:18:17.960 --> 00:18:18.800]   - Right.
[00:18:18.800 --> 00:18:21.680]   - And then you have these little streams
[00:18:21.680 --> 00:18:23.240]   that are coming off the river
[00:18:23.240 --> 00:18:25.480]   where you can get extra passengers
[00:18:25.480 --> 00:18:27.400]   or collect extra information if you want.
[00:18:27.400 --> 00:18:30.040]   And those correspond to the attention heads and MLPs
[00:18:30.040 --> 00:18:31.760]   that are part of the model.
[00:18:31.760 --> 00:18:32.600]   - Right.
[00:18:32.600 --> 00:18:36.920]   I almost think of it like the working memory of the model,
[00:18:36.920 --> 00:18:38.800]   like the RAM of the computer,
[00:18:38.800 --> 00:18:41.080]   where you're choosing what information to read in,
[00:18:41.080 --> 00:18:42.640]   so you can do something with it,
[00:18:42.640 --> 00:18:45.680]   and then maybe you'd read something else in later on.
[00:18:45.680 --> 00:18:47.160]   - And you can operate on subspaces
[00:18:47.160 --> 00:18:49.400]   of that high dimensional vector.
[00:18:49.400 --> 00:18:51.600]   A ton of things are, I mean, at this point,
[00:18:51.600 --> 00:18:52.880]   I think it's almost given
[00:18:52.880 --> 00:18:55.200]   that are encoded in superposition, right?
[00:18:55.200 --> 00:18:56.520]   So it's like, yeah, the residual stream
[00:18:56.520 --> 00:18:58.880]   is just one high dimensional vector,
[00:18:58.880 --> 00:19:01.840]   but actually there's a ton of different vectors
[00:19:01.840 --> 00:19:03.040]   that are packed into it.
[00:19:03.040 --> 00:19:05.480]   - Yeah, I might just dumb it down
[00:19:05.480 --> 00:19:08.360]   as a way that would have made sense to me a few months ago.
[00:19:08.360 --> 00:19:10.400]   Okay, so you have, you know,
[00:19:10.400 --> 00:19:14.080]   whatever words are in the input you put into the model,
[00:19:14.080 --> 00:19:17.760]   all those words get converted into these tokens,
[00:19:17.760 --> 00:19:20.200]   and those tokens get converted into these vectors.
[00:19:20.200 --> 00:19:23.200]   And basically it's just like this small amount
[00:19:23.200 --> 00:19:25.720]   of information that's moving through the model.
[00:19:25.720 --> 00:19:28.360]   And the way you explained it to me, Sholta,
[00:19:28.360 --> 00:19:30.600]   this paper talks about is early on in the model,
[00:19:30.600 --> 00:19:32.320]   maybe it's just doing some very basic things
[00:19:32.320 --> 00:19:34.320]   about like, what do these tokens mean?
[00:19:34.320 --> 00:19:36.040]   Like if it says like 10 plus five,
[00:19:36.040 --> 00:19:39.480]   just like moving information about to have the,
[00:19:39.480 --> 00:19:40.320]   have that--
[00:19:40.320 --> 00:19:41.160]   - Good representation.
[00:19:41.160 --> 00:19:42.000]   - Exactly, just represent.
[00:19:42.000 --> 00:19:43.720]   And in the middle, maybe like the deeper thinking
[00:19:43.720 --> 00:19:45.120]   is happening about like how to think,
[00:19:45.120 --> 00:19:46.720]   yeah, how to solve this.
[00:19:46.720 --> 00:19:50.080]   At the end, you're converting it back into the output token
[00:19:50.080 --> 00:19:51.960]   'cause the end product is,
[00:19:51.960 --> 00:19:54.920]   you're trying to predict the probability of the next token
[00:19:54.920 --> 00:19:58.520]   from the last of those residual streams.
[00:19:58.520 --> 00:20:00.040]   And so, yeah, it's interesting to think about like,
[00:20:00.040 --> 00:20:02.400]   just like the small compressed amount of information
[00:20:02.400 --> 00:20:03.320]   moving through the model,
[00:20:03.360 --> 00:20:06.160]   and it's like getting modified in different ways.
[00:20:06.160 --> 00:20:08.760]   Trenton, so you're, it's interesting.
[00:20:08.760 --> 00:20:09.600]   You're one of the few people
[00:20:09.600 --> 00:20:12.600]   who have like background from neuroscience.
[00:20:12.600 --> 00:20:16.680]   So you can think about the analogies here to the brain.
[00:20:16.680 --> 00:20:18.560]   And in fact, I have one of our friends,
[00:20:18.560 --> 00:20:20.880]   the way he, he had a paper in grad school
[00:20:20.880 --> 00:20:22.560]   about thinking about attention in the brain.
[00:20:22.560 --> 00:20:25.500]   And he said, this is the only or first,
[00:20:25.500 --> 00:20:30.720]   what, like neural explanation of why attention works.
[00:20:30.720 --> 00:20:34.760]   Whereas we have evidence from why the CNNs work,
[00:20:34.760 --> 00:20:36.780]   convolutional neural networks work
[00:20:36.780 --> 00:20:39.040]   based on the visual cortex or something.
[00:20:39.040 --> 00:20:42.660]   Yeah, I'm curious, do you think in the brain,
[00:20:42.660 --> 00:20:44.360]   there's something like a residual stream
[00:20:44.360 --> 00:20:46.600]   of this compressed amount of information
[00:20:46.600 --> 00:20:49.800]   that's moving through and it's getting modified
[00:20:49.800 --> 00:20:51.140]   as you're thinking about something?
[00:20:51.140 --> 00:20:53.120]   Even if that's not what's literally happening,
[00:20:53.120 --> 00:20:54.560]   do you think that's a good metaphor
[00:20:54.560 --> 00:20:55.760]   for what's happening in the brain?
[00:20:55.760 --> 00:20:56.600]   - Yeah, yeah.
[00:20:56.600 --> 00:20:57.920]   So at least in the cerebellum,
[00:20:57.920 --> 00:21:00.160]   you basically do have a residual stream
[00:21:00.160 --> 00:21:03.240]   where the whole, what we'll call the attention module
[00:21:03.240 --> 00:21:05.120]   for now, and I can go into whatever amount of detail
[00:21:05.120 --> 00:21:06.020]   you want for that.
[00:21:06.020 --> 00:21:09.000]   You have inputs that route through it,
[00:21:09.000 --> 00:21:13.800]   but they'll also just go directly to the like end point
[00:21:13.800 --> 00:21:16.440]   that that module will contribute to.
[00:21:16.440 --> 00:21:19.160]   So there's a direct path and an indirect path.
[00:21:19.160 --> 00:21:21.920]   And so the model can like pick up
[00:21:21.920 --> 00:21:24.720]   whatever information it wants and then add that back in.
[00:21:24.720 --> 00:21:28.320]   - Well, what happens to the cerebellum?
[00:21:28.320 --> 00:21:33.320]   - So the cerebellum nominally just does fine motor control,
[00:21:33.320 --> 00:21:38.440]   but I analogize this to the person who's lost their keys
[00:21:38.440 --> 00:21:39.880]   and is just looking under the streetlight
[00:21:39.880 --> 00:21:43.160]   where it's very easily to observe this behavior.
[00:21:43.160 --> 00:21:45.840]   One leading cognitive neuroscientist said to me
[00:21:45.840 --> 00:21:48.640]   that a dirty little secret of any fMRI study
[00:21:48.640 --> 00:21:50.860]   where you're looking at brain activity for a given task
[00:21:50.860 --> 00:21:53.240]   is that the cerebellum is almost always active
[00:21:53.240 --> 00:21:54.760]   and lighting up for it.
[00:21:54.760 --> 00:21:56.000]   If you have a damaged cerebellum,
[00:21:56.000 --> 00:21:58.920]   you also are much more likely to have autism.
[00:21:58.920 --> 00:22:01.800]   So it's associated with like social skills.
[00:22:01.800 --> 00:22:03.640]   And one of these particular studies
[00:22:03.640 --> 00:22:05.800]   where I think they use PET instead of fMRI,
[00:22:05.800 --> 00:22:08.680]   but when you're doing next token prediction,
[00:22:08.680 --> 00:22:10.240]   the cerebellum lights up a lot.
[00:22:10.240 --> 00:22:14.400]   Also 70% of your neurons in the brain are in the cerebellum.
[00:22:14.400 --> 00:22:17.480]   They're small, but they're there
[00:22:17.480 --> 00:22:21.000]   and they're taking up real metabolic cost.
[00:22:21.000 --> 00:22:22.520]   - This is one of Guerin's points
[00:22:22.520 --> 00:22:24.680]   that like what changed with humans
[00:22:24.680 --> 00:22:26.240]   was not just that we have more neurons
[00:22:26.240 --> 00:22:28.960]   or he says he shared this article,
[00:22:28.960 --> 00:22:31.360]   but specifically there's more neurons
[00:22:31.360 --> 00:22:33.160]   in the cerebral cortex in the cerebellum
[00:22:33.160 --> 00:22:35.240]   and you should say more about this,
[00:22:35.240 --> 00:22:37.680]   but like they're more metabolically expensive
[00:22:37.680 --> 00:22:39.160]   and they're more involved in signaling
[00:22:39.160 --> 00:22:41.440]   and sending information back and forth.
[00:22:41.440 --> 00:22:42.280]   - Yeah.
[00:22:42.280 --> 00:22:43.520]   - Is that attention what's going on?
[00:22:43.520 --> 00:22:44.360]   - Yeah, yeah.
[00:22:44.360 --> 00:22:47.520]   So I guess the main thing I wanna communicate here.
[00:22:47.520 --> 00:22:50.640]   So back in the 1980s, Pente Canerva
[00:22:51.560 --> 00:22:54.520]   came up with a associative memory algorithm
[00:22:54.520 --> 00:22:57.840]   for I have a bunch of memories, I wanna store them.
[00:22:57.840 --> 00:23:00.560]   There's some amount of noise or corruption that's going on
[00:23:00.560 --> 00:23:03.800]   and I want to query or retrieve the best match.
[00:23:03.800 --> 00:23:06.080]   And so he writes this equation for how to do it.
[00:23:06.080 --> 00:23:09.120]   And a few years later realizes
[00:23:09.120 --> 00:23:10.240]   that if you implemented this
[00:23:10.240 --> 00:23:12.200]   as an electrical engineering circuit,
[00:23:12.200 --> 00:23:17.080]   it actually looks identical to the core cerebellar circuit.
[00:23:17.080 --> 00:23:19.080]   And that circuit and the cerebellum more broadly
[00:23:19.080 --> 00:23:21.960]   is not just in us, it's in basically every organism.
[00:23:21.960 --> 00:23:24.120]   There's active debate on whether or not cephalopods have it,
[00:23:24.120 --> 00:23:26.280]   they kind of have a different evolutionary trajectory,
[00:23:26.280 --> 00:23:30.240]   but even fruit flies with the Drosophila mushroom body,
[00:23:30.240 --> 00:23:32.920]   that is the same cerebellar architecture.
[00:23:32.920 --> 00:23:37.320]   And so that convergence and then my paper,
[00:23:37.320 --> 00:23:39.800]   which shows that actually this operation
[00:23:39.800 --> 00:23:41.320]   is to a very close approximation,
[00:23:41.320 --> 00:23:43.160]   the same as the attention operation,
[00:23:43.160 --> 00:23:44.920]   including implementing the softmax
[00:23:44.920 --> 00:23:47.600]   and having this sort of like nominal quadratic costs
[00:23:47.600 --> 00:23:48.920]   that we've been talking about.
[00:23:48.920 --> 00:23:50.960]   And so the three-way convergence here
[00:23:50.960 --> 00:23:53.160]   and the takeoff and success of transformers
[00:23:53.160 --> 00:23:55.960]   just seems pretty striking to me.
[00:23:55.960 --> 00:23:58.000]   - Yeah, I wanna zoom out and ask,
[00:23:58.000 --> 00:24:00.560]   I think what motivated this discussion in the beginning
[00:24:00.560 --> 00:24:02.320]   was we were talking about like,
[00:24:02.320 --> 00:24:03.960]   wait, what is the reasoning?
[00:24:03.960 --> 00:24:05.760]   What is the memory?
[00:24:05.760 --> 00:24:09.120]   What do you think about the analogy you found
[00:24:09.120 --> 00:24:11.000]   to attention and this?
[00:24:11.000 --> 00:24:12.960]   Do you think of this as more just looking up
[00:24:12.960 --> 00:24:15.600]   the relevant memories or the relevant facts?
[00:24:15.600 --> 00:24:17.240]   And if that is the case,
[00:24:17.240 --> 00:24:20.160]   where is the reasoning happening in the brain?
[00:24:20.160 --> 00:24:21.000]   How do we think about
[00:24:21.000 --> 00:24:22.560]   how that builds up into the reasoning?
[00:24:22.560 --> 00:24:25.680]   - Yeah, so maybe my hot take here,
[00:24:25.680 --> 00:24:27.000]   I don't know how hot it is,
[00:24:27.000 --> 00:24:32.000]   is that most intelligence is pattern matching
[00:24:32.000 --> 00:24:35.360]   and you can do a lot of really good pattern matching
[00:24:35.360 --> 00:24:38.520]   if you have a hierarchy of associated memories.
[00:24:38.520 --> 00:24:43.080]   So you start with your very basic associations
[00:24:43.080 --> 00:24:45.800]   between just like objects in the real world.
[00:24:45.800 --> 00:24:49.160]   But you can then chain those
[00:24:49.160 --> 00:24:51.120]   and have more abstract associations
[00:24:51.120 --> 00:24:53.360]   such as like a wedding ring symbolizes
[00:24:53.360 --> 00:24:56.920]   like so many other associations that are downstream.
[00:24:56.920 --> 00:25:01.920]   And so, and you can even generalize the attention operation
[00:25:01.920 --> 00:25:05.560]   and this associated memory as the MLP layer as well.
[00:25:05.560 --> 00:25:07.040]   It's in a long-term setting
[00:25:07.040 --> 00:25:10.640]   where you don't have like tokens in your current context.
[00:25:11.600 --> 00:25:13.120]   But I think this is an argument
[00:25:13.120 --> 00:25:16.760]   that like association is all you need.
[00:25:16.760 --> 00:25:19.560]   And associated memory in general as well,
[00:25:19.560 --> 00:25:22.360]   it's not, so you can do two things with it.
[00:25:22.360 --> 00:25:25.600]   You can both denoise or retrieve a current memory.
[00:25:25.600 --> 00:25:27.200]   So like if I see your face,
[00:25:27.200 --> 00:25:29.040]   but it's like raining and cloudy,
[00:25:29.040 --> 00:25:32.960]   I can denoise and kind of like gradually update my query
[00:25:32.960 --> 00:25:35.080]   towards my memory of your face.
[00:25:35.080 --> 00:25:37.840]   But I can also access that memory
[00:25:37.840 --> 00:25:40.800]   and then the value that I get out
[00:25:40.800 --> 00:25:42.160]   actually points to some other
[00:25:42.160 --> 00:25:43.520]   totally different part of the space.
[00:25:43.520 --> 00:25:45.680]   And so a very simple instance of this would be
[00:25:45.680 --> 00:25:47.200]   if you learn the alphabet, right?
[00:25:47.200 --> 00:25:49.240]   And so I query for A and it returns B,
[00:25:49.240 --> 00:25:51.080]   I query for B and it returns C
[00:25:51.080 --> 00:25:53.320]   and you can traverse the whole thing.
[00:25:53.320 --> 00:25:55.920]   Yeah.
[00:25:55.920 --> 00:25:58.520]   - Yeah, one of the things I talked to Demis about
[00:25:58.520 --> 00:26:00.200]   was he had a paper in 2008
[00:26:00.200 --> 00:26:03.120]   that memory and imagination are very linked
[00:26:03.120 --> 00:26:04.840]   because of this very thing that you mentioned
[00:26:04.840 --> 00:26:07.040]   of memory is reconstructive.
[00:26:07.040 --> 00:26:09.760]   And so you are in some sense imagining
[00:26:09.760 --> 00:26:11.120]   every time you're thinking of a memory
[00:26:11.120 --> 00:26:13.480]   'cause you're only storing a condensed version of it
[00:26:13.480 --> 00:26:14.800]   and you're like have to.
[00:26:14.800 --> 00:26:17.680]   And this is famously why human memory is terrible
[00:26:17.680 --> 00:26:20.360]   and like why people in the witness box or whatever
[00:26:20.360 --> 00:26:21.560]   would just make shit up.
[00:26:21.560 --> 00:26:26.280]   Okay, so let me ask a stupid question.
[00:26:26.280 --> 00:26:28.160]   So you like reach Sherlock Holmes, right?
[00:26:28.160 --> 00:26:30.640]   And like the guy's incredibly sample efficient.
[00:26:30.640 --> 00:26:32.560]   He'll like see a few observations
[00:26:32.560 --> 00:26:37.240]   and he'll like basically figure out who committed the crime
[00:26:37.240 --> 00:26:39.160]   'cause there's a series of deductive steps
[00:26:39.160 --> 00:26:40.600]   that leads from somebody's tattoo
[00:26:40.600 --> 00:26:44.000]   and what's on the wall to the implications of that.
[00:26:44.000 --> 00:26:47.320]   How does that fit into this picture?
[00:26:47.320 --> 00:26:50.240]   'Cause like crucially what makes him smart
[00:26:50.240 --> 00:26:52.720]   is that there's not like an association
[00:26:52.720 --> 00:26:55.160]   but there's a sort of deductive connection
[00:26:55.160 --> 00:26:57.320]   between different pieces of information.
[00:26:57.320 --> 00:26:59.200]   Would you just explain it
[00:26:59.200 --> 00:27:01.840]   as that's just like higher level association?
[00:27:01.840 --> 00:27:02.680]   Like, yeah.
[00:27:02.680 --> 00:27:03.600]   - I think so, yeah.
[00:27:03.600 --> 00:27:05.720]   So I think learning these higher level associations
[00:27:05.720 --> 00:27:07.800]   to be able to then map patterns to each other
[00:27:07.800 --> 00:27:09.680]   as kind of like a meta learning.
[00:27:09.680 --> 00:27:10.520]   I think in this case,
[00:27:10.520 --> 00:27:12.680]   he would also just have a really long context length
[00:27:12.680 --> 00:27:15.480]   or a really long working memory, right?
[00:27:15.480 --> 00:27:17.240]   Where he can like have all of these bits
[00:27:17.240 --> 00:27:19.800]   and continuously query them
[00:27:19.800 --> 00:27:22.240]   as he's coming up with whatever theory.
[00:27:22.240 --> 00:27:25.040]   So that the theory is moving through the residual stream.
[00:27:25.040 --> 00:27:30.040]   And then his attention heads are querying his context
[00:27:30.040 --> 00:27:35.520]   but then how he's projecting his query and keys in the space
[00:27:35.520 --> 00:27:40.400]   and how his MLPs are then retrieving like longer term facts
[00:27:40.400 --> 00:27:42.560]   or modifying that information
[00:27:42.560 --> 00:27:44.840]   is allowing him to then in later layers
[00:27:44.840 --> 00:27:47.160]   do even more sophisticated queries
[00:27:47.160 --> 00:27:49.880]   and slowly be able to reason through
[00:27:49.880 --> 00:27:51.640]   and come to a meaningful conclusion.
[00:27:51.640 --> 00:27:52.480]   - That feels right to me
[00:27:52.480 --> 00:27:54.320]   in terms of like looking back in the past,
[00:27:54.320 --> 00:27:55.280]   you're selectively reading
[00:27:55.280 --> 00:27:57.280]   in certain pieces of information, comparing them.
[00:27:57.280 --> 00:27:58.640]   Maybe that informs your next step
[00:27:58.640 --> 00:28:01.040]   of like what piece of information you now need to pull in.
[00:28:01.040 --> 00:28:02.200]   And then you build this representation
[00:28:02.200 --> 00:28:04.040]   which I like progressively looks closer
[00:28:04.040 --> 00:28:06.560]   and closer and closer to like the suspect in your case.
[00:28:06.560 --> 00:28:07.400]   - Yeah.
[00:28:07.400 --> 00:28:08.240]   - Yeah.
[00:28:08.240 --> 00:28:10.040]   - That doesn't feel at all outlandish.
[00:28:10.040 --> 00:28:12.400]   - Do you have a lens on like suspects?
[00:28:12.400 --> 00:28:15.200]   (all laughing)
[00:28:15.200 --> 00:28:16.400]   - Something I think that the people
[00:28:16.400 --> 00:28:19.080]   who aren't doing this research can overlook
[00:28:19.080 --> 00:28:22.520]   is after your first layer of the model,
[00:28:22.520 --> 00:28:26.360]   every query key and value that you're using for attention
[00:28:26.360 --> 00:28:29.120]   comes from the combination of all the previous tokens.
[00:28:29.120 --> 00:28:31.960]   So like my first layer, I'll query my previous tokens
[00:28:31.960 --> 00:28:33.800]   and just extract information from them.
[00:28:33.800 --> 00:28:35.800]   But all of a sudden, let's say that I attended
[00:28:35.800 --> 00:28:39.680]   to tokens one, two, and four in equal amounts.
[00:28:39.680 --> 00:28:42.040]   Then the vector in my residual stream,
[00:28:42.040 --> 00:28:44.240]   assuming that they wrote out the same thing
[00:28:44.240 --> 00:28:46.720]   to the value vectors, but ignore that for a second,
[00:28:46.720 --> 00:28:48.720]   is a third of each of those.
[00:28:48.720 --> 00:28:50.800]   And so when I'm querying in the future,
[00:28:50.800 --> 00:28:54.180]   my query is actually a third of each of those things.
[00:28:54.180 --> 00:28:55.020]   And so--
[00:28:55.020 --> 00:28:56.600]   - But they might be written to different subspaces.
[00:28:56.600 --> 00:28:57.600]   - That's right.
[00:28:57.600 --> 00:28:59.440]   Hypothetically, but they wouldn't have to, right?
[00:28:59.440 --> 00:29:02.480]   And so you can recombine and immediately,
[00:29:02.480 --> 00:29:05.000]   even by layer two and certainly by the deeper layers,
[00:29:05.000 --> 00:29:07.280]   just have like these very rich vectors
[00:29:07.280 --> 00:29:09.520]   that are packing in a ton of information.
[00:29:09.520 --> 00:29:11.320]   And the causal graph is like literally
[00:29:11.320 --> 00:29:14.160]   over every single layer that happened in the past.
[00:29:14.160 --> 00:29:17.400]   And that's what you're operating on, yeah.
[00:29:17.400 --> 00:29:19.940]   - It does bring to mind like a very funny eval to do,
[00:29:19.940 --> 00:29:21.320]   would be like a Sherlock Holmes eval,
[00:29:21.320 --> 00:29:22.880]   let's say you put the entire book into context.
[00:29:22.880 --> 00:29:24.600]   And then you have like a sentence which is like,
[00:29:24.600 --> 00:29:26.440]   the suspect is like X,
[00:29:26.440 --> 00:29:28.240]   then you have like a larger probability distribution
[00:29:28.240 --> 00:29:30.160]   over like the different characters in the book.
[00:29:30.160 --> 00:29:32.520]   And then like as you put more--
[00:29:32.520 --> 00:29:35.600]   - That would be super cool, yeah, yeah, yeah.
[00:29:35.600 --> 00:29:37.400]   - I wonder if you'd get anything at all,
[00:29:37.400 --> 00:29:38.680]   but it'd be cool.
[00:29:38.680 --> 00:29:40.400]   - Sherlock Holmes is probably already in the training data.
[00:29:40.400 --> 00:29:41.240]   - Yeah.
[00:29:41.240 --> 00:29:42.080]   - You're gonna get like a mystery novel
[00:29:42.080 --> 00:29:43.280]   that was written in the--
[00:29:43.280 --> 00:29:44.720]   - You can get an LLM to write it.
[00:29:44.720 --> 00:29:47.260]   - Or you could purposely exclude it, right?
[00:29:47.260 --> 00:29:48.100]   - Oh, you can?
[00:29:48.100 --> 00:29:48.920]   How do you--
[00:29:48.920 --> 00:29:50.240]   - Well, you need to scrape any discussion of it
[00:29:50.240 --> 00:29:51.640]   from Reddit or any other thing, right?
[00:29:51.640 --> 00:29:52.840]   - Right, it's hard.
[00:29:52.840 --> 00:29:54.040]   But that's like one of the challenges
[00:29:54.040 --> 00:29:55.880]   that goes into things like long context evals
[00:29:55.880 --> 00:29:57.560]   is to get a good one,
[00:29:57.560 --> 00:30:00.440]   you need to know that it's not in your training data.
[00:30:00.440 --> 00:30:01.880]   You just like put in the effort to exclude it.
[00:30:01.880 --> 00:30:06.000]   - What, so I actually wanna,
[00:30:06.000 --> 00:30:07.760]   there's two different threads I wanna follow up on.
[00:30:07.760 --> 00:30:08.760]   Let's go to the long context one
[00:30:08.760 --> 00:30:11.940]   and then we'll come back to this.
[00:30:11.940 --> 00:30:14.840]   So in the Gemini 1.5 paper,
[00:30:14.840 --> 00:30:16.800]   the eval that was used was,
[00:30:16.800 --> 00:30:19.240]   can it like, something with Paul Graham essays,
[00:30:19.240 --> 00:30:20.080]   can it like--
[00:30:20.080 --> 00:30:20.920]   - Yeah, the needle in a haystack.
[00:30:20.920 --> 00:30:22.340]   - Right.
[00:30:22.340 --> 00:30:24.240]   Which, yeah, I mean, there's like,
[00:30:24.240 --> 00:30:26.560]   we don't necessarily just care about its ability
[00:30:26.560 --> 00:30:29.460]   to recall one specific fact from the context.
[00:30:29.460 --> 00:30:32.440]   I'll step back and ask the question.
[00:30:32.440 --> 00:30:37.680]   The loss function for these models is unsupervised.
[00:30:37.680 --> 00:30:40.260]   You don't have to like come up with these bespoke things
[00:30:40.260 --> 00:30:42.800]   that you keep out of the training data.
[00:30:42.800 --> 00:30:44.360]   Is there a way you can do a benchmark
[00:30:44.360 --> 00:30:46.140]   that's also unsupervised?
[00:30:46.140 --> 00:30:49.760]   Where, I don't know, another LLM is rating it in some way
[00:30:49.760 --> 00:30:51.360]   or something like that.
[00:30:51.360 --> 00:30:52.360]   And maybe the answer is like,
[00:30:52.360 --> 00:30:53.240]   well, if you could do this,
[00:30:53.240 --> 00:30:54.600]   like reinforcement learning would work
[00:30:54.600 --> 00:30:56.360]   'cause then you have this like unsupervised.
[00:30:56.360 --> 00:30:58.240]   - Yeah, I mean, I think people have explored
[00:30:58.240 --> 00:30:59.080]   that kind of stuff.
[00:30:59.080 --> 00:30:59.900]   Like, for example,
[00:30:59.900 --> 00:31:01.600]   Anthropica is the constitutional RL paper
[00:31:01.600 --> 00:31:02.920]   where they take another language model
[00:31:02.920 --> 00:31:03.960]   and they point it and say like,
[00:31:03.960 --> 00:31:07.680]   how helpful or harmless was that response?
[00:31:07.680 --> 00:31:08.880]   And then they get it to update
[00:31:08.880 --> 00:31:11.320]   and try and improve along the prior frontier
[00:31:11.320 --> 00:31:13.120]   of helpfulness and harmfulness.
[00:31:13.120 --> 00:31:15.320]   So you can like point language models at each other
[00:31:15.320 --> 00:31:17.280]   and create evals in this way.
[00:31:17.280 --> 00:31:20.440]   It's obviously an imperfect art form at the moment
[00:31:20.440 --> 00:31:24.200]   because you get reward function hacking basically
[00:31:24.200 --> 00:31:27.840]   and the language, like if you try and match up to what,
[00:31:27.840 --> 00:31:30.080]   even humans are imperfect here.
[00:31:30.080 --> 00:31:31.480]   Like if you try and match up to what humans will say,
[00:31:31.480 --> 00:31:33.880]   humans typically prefer longer answers,
[00:31:33.880 --> 00:31:35.440]   which aren't necessarily better answers
[00:31:35.440 --> 00:31:37.680]   and you get the same behavior with models.
[00:31:37.680 --> 00:31:41.400]   - On the other thread,
[00:31:41.400 --> 00:31:43.400]   going back to the Sherlock Holmes thing.
[00:31:43.400 --> 00:31:48.040]   If it's all associations all the way down,
[00:31:48.040 --> 00:31:50.320]   this is a sort of like naive dinner party question.
[00:31:50.320 --> 00:31:53.120]   If I just like match you, I'm working on AI.
[00:31:53.120 --> 00:31:55.440]   But okay, does that mean we should be less worried
[00:31:55.440 --> 00:31:56.720]   about super intelligence?
[00:31:56.720 --> 00:31:58.040]   'Cause there's not this sense
[00:31:58.040 --> 00:32:00.720]   in which it's like Sherlock Holmes plus plus.
[00:32:00.720 --> 00:32:03.720]   It'll still need to just like find these associations,
[00:32:03.720 --> 00:32:05.960]   like humans find associations and like,
[00:32:05.960 --> 00:32:06.800]   you know what I mean?
[00:32:06.800 --> 00:32:09.040]   It's not just like, it sees a frame of the world
[00:32:09.040 --> 00:32:11.760]   and it's like figured out all the laws of physics.
[00:32:11.760 --> 00:32:15.640]   - So for me, 'cause this is a very legitimate response,
[00:32:15.640 --> 00:32:16.480]   right?
[00:32:16.480 --> 00:32:18.640]   It's like, well, artificial general intelligence aren't,
[00:32:18.640 --> 00:32:20.600]   if you say humans are generally intelligent,
[00:32:20.600 --> 00:32:23.360]   then they're no more capable or competent.
[00:32:23.360 --> 00:32:25.360]   I'm just worried that you have that level
[00:32:25.360 --> 00:32:27.760]   of general intelligence in Silicon,
[00:32:27.760 --> 00:32:30.120]   where you can then immediately clone
[00:32:30.120 --> 00:32:31.880]   hundreds of thousands of agents
[00:32:31.880 --> 00:32:33.240]   and they don't need to sleep
[00:32:33.240 --> 00:32:35.600]   and they can have super long context windows
[00:32:35.600 --> 00:32:37.240]   and then they can start recursively improving
[00:32:37.240 --> 00:32:39.320]   and then things get really scary.
[00:32:39.320 --> 00:32:41.120]   So I think to answer your original question,
[00:32:41.120 --> 00:32:41.960]   yes, you're right.
[00:32:41.960 --> 00:32:43.680]   They would still need to learn associations, but.
[00:32:43.680 --> 00:32:46.120]   - Well, but the recursive self-improvement
[00:32:46.120 --> 00:32:48.600]   would still have to be them.
[00:32:48.600 --> 00:32:49.960]   Like if intelligence is fundamentally
[00:32:49.960 --> 00:32:51.080]   about these associations,
[00:32:51.080 --> 00:32:52.440]   like the improvement is just them
[00:32:52.440 --> 00:32:53.600]   getting better at association.
[00:32:53.600 --> 00:32:55.960]   There's not like another thing that's happening.
[00:32:55.960 --> 00:32:58.800]   And so then it seems like you might disagree
[00:32:58.800 --> 00:33:00.120]   with the intuition that,
[00:33:00.120 --> 00:33:01.560]   well, they can't be that much more powerful
[00:33:01.560 --> 00:33:02.400]   if they're just doing associations.
[00:33:02.400 --> 00:33:03.760]   - Well, I think then you can get into
[00:33:03.760 --> 00:33:06.440]   really interesting cases of meta-learning.
[00:33:06.440 --> 00:33:08.680]   Like when you play a new video game
[00:33:08.680 --> 00:33:11.200]   or like study a new textbook,
[00:33:11.200 --> 00:33:13.520]   you're bringing a whole bunch of skills to the table
[00:33:13.520 --> 00:33:15.720]   to form those associations much more quickly.
[00:33:15.720 --> 00:33:17.640]   And like, because everything in some way
[00:33:17.640 --> 00:33:19.120]   ties back to the physical worlds,
[00:33:19.120 --> 00:33:21.400]   I think there are like general features
[00:33:21.400 --> 00:33:22.240]   that you can pick up
[00:33:22.240 --> 00:33:24.320]   and then apply in novel circumstances.
[00:33:24.320 --> 00:33:28.560]   - Should we talk about intelligence explosion then?
[00:33:28.560 --> 00:33:29.400]   I don't know if that's a good idea.
[00:33:29.400 --> 00:33:31.680]   - That was a really good sign to jump in.
[00:33:31.680 --> 00:33:32.800]   - I mentioned multiple agents
[00:33:32.800 --> 00:33:34.960]   and I'm like, "Oh, here we go."
[00:33:34.960 --> 00:33:39.000]   Okay, so the reason I'm interested in discussing this
[00:33:39.000 --> 00:33:40.800]   is with you guys in particular
[00:33:40.800 --> 00:33:44.120]   is the models we have of the intelligence explosion
[00:33:44.120 --> 00:33:46.920]   so far come from economists, which is fine,
[00:33:46.920 --> 00:33:48.560]   but I think we can do better
[00:33:48.560 --> 00:33:52.280]   because in the model of the intelligence explosion,
[00:33:52.280 --> 00:33:55.000]   what happens is you replace the AI researchers
[00:33:55.000 --> 00:33:59.400]   and then there's like a bunch of automated AI researchers
[00:33:59.400 --> 00:34:02.200]   who can speed up progress, make more AI researchers,
[00:34:02.200 --> 00:34:03.520]   make further progress.
[00:34:03.520 --> 00:34:05.680]   And so I feel like if that's the metric
[00:34:05.680 --> 00:34:07.880]   or that's the mechanism,
[00:34:07.880 --> 00:34:09.520]   we should just ask the AI researchers
[00:34:09.520 --> 00:34:11.560]   about whether they think this is plausible.
[00:34:11.560 --> 00:34:12.560]   So let me just ask you,
[00:34:12.560 --> 00:34:16.680]   like if I have a thousand Asian Shotos or Asian Trentons,
[00:34:16.680 --> 00:34:18.520]   are they just, do you think that
[00:34:18.520 --> 00:34:19.760]   you get an intelligence explosion?
[00:34:19.760 --> 00:34:23.320]   Is that, yeah, what does that look like to you?
[00:34:23.320 --> 00:34:26.680]   - I think one of the important bounding constraints here
[00:34:26.680 --> 00:34:27.840]   is compute.
[00:34:27.840 --> 00:34:29.840]   Like I do think you could dramatically
[00:34:29.840 --> 00:34:31.840]   speed up AI research, right?
[00:34:31.840 --> 00:34:33.000]   Like it seems very clear to me
[00:34:33.000 --> 00:34:34.320]   that in the next couple of years,
[00:34:34.320 --> 00:34:35.600]   we'll have things that can do
[00:34:35.600 --> 00:34:36.920]   many of the software engineering tasks
[00:34:36.920 --> 00:34:38.240]   that I do on a day-to-day basis
[00:34:38.240 --> 00:34:40.920]   and therefore dramatically speed up my work
[00:34:40.920 --> 00:34:43.720]   and therefore speed up like the rate of progress, right?
[00:34:45.160 --> 00:34:47.960]   At the moment, I think most of the labs
[00:34:47.960 --> 00:34:49.400]   are somewhat compute bound
[00:34:49.400 --> 00:34:51.760]   in that they're always,
[00:34:51.760 --> 00:34:53.280]   there are more experiments you could run
[00:34:53.280 --> 00:34:54.960]   and more pieces of information that you could gain
[00:34:54.960 --> 00:34:57.800]   in the same way that like scientific research on biology
[00:34:57.800 --> 00:35:00.880]   is also somewhat experimentally like throughput bound.
[00:35:00.880 --> 00:35:03.680]   Like you need to be able to run and culture the cells
[00:35:03.680 --> 00:35:04.920]   in order to get the information.
[00:35:04.920 --> 00:35:07.640]   I think that will be
[00:35:07.640 --> 00:35:09.040]   at least a short-term bounding constraint.
[00:35:09.040 --> 00:35:10.160]   Obviously, you know,
[00:35:10.160 --> 00:35:13.880]   Sam's trying to raise $7 trillion to buy chips.
[00:35:13.880 --> 00:35:15.920]   And so like, it does seem like
[00:35:15.920 --> 00:35:17.720]   there's going to be a lot more compute in future
[00:35:17.720 --> 00:35:20.080]   as everyone is heavily ramping.
[00:35:20.080 --> 00:35:21.800]   NVIDIA's stock price sort of represents
[00:35:21.800 --> 00:35:25.280]   the relative compute increase.
[00:35:25.280 --> 00:35:26.920]   But any thoughts?
[00:35:26.920 --> 00:35:31.440]   - I think we need a few more nines of reliability
[00:35:31.440 --> 00:35:34.320]   in order for it to really be useful and trustworthy.
[00:35:34.320 --> 00:35:35.680]   Right now it's like,
[00:35:35.680 --> 00:35:39.720]   and just having context lengths that are super long
[00:35:39.720 --> 00:35:42.080]   and it's like very cheap to have.
[00:35:42.080 --> 00:35:44.560]   Like if I'm working in our code base,
[00:35:44.560 --> 00:35:46.360]   it's really only small modules
[00:35:46.360 --> 00:35:48.960]   that I can get Claude to write for me right now.
[00:35:48.960 --> 00:35:53.680]   But it's very plausible that within the next few years
[00:35:53.680 --> 00:35:57.920]   or even sooner, it can automate most of my tasks.
[00:35:57.920 --> 00:36:00.880]   The only other thing here that I will note is
[00:36:00.880 --> 00:36:04.960]   the research that at least our sub team
[00:36:04.960 --> 00:36:06.840]   in interpretability is working on
[00:36:06.840 --> 00:36:08.880]   is so early stage
[00:36:09.880 --> 00:36:12.640]   that you really have to be able to
[00:36:12.640 --> 00:36:16.720]   make sure everything is done correctly in a bug-free way
[00:36:16.720 --> 00:36:19.200]   and contextualize the results
[00:36:19.200 --> 00:36:21.200]   with everything else in the model.
[00:36:21.200 --> 00:36:22.720]   And if something isn't going right,
[00:36:22.720 --> 00:36:25.240]   be able to enumerate all of the possible things
[00:36:25.240 --> 00:36:27.840]   and then slowly work on those.
[00:36:27.840 --> 00:36:29.680]   Like an example that we've publicly talked about
[00:36:29.680 --> 00:36:31.840]   in previous papers is dealing with layer norm, right?
[00:36:31.840 --> 00:36:34.760]   And it's like, if I'm trying to get an early result
[00:36:34.760 --> 00:36:37.680]   or look at like the logit effects of the model, right?
[00:36:37.680 --> 00:36:39.480]   So it's like, if I activate this feature
[00:36:39.480 --> 00:36:41.520]   that we've identified to a really large degree,
[00:36:41.520 --> 00:36:43.760]   how does that change the output of the model?
[00:36:43.760 --> 00:36:46.440]   Am I using layer norm or not?
[00:36:46.440 --> 00:36:49.160]   How is that changing the feature that's being learned?
[00:36:49.160 --> 00:36:53.560]   And that will take even more context
[00:36:53.560 --> 00:36:55.840]   or reasoning abilities for the model.
[00:36:55.840 --> 00:37:00.240]   - So you used a couple of concepts together
[00:37:00.240 --> 00:37:02.440]   and it's not self-evident to me that they're the same,
[00:37:02.440 --> 00:37:04.800]   but it seemed like you were using them interchangeably.
[00:37:04.800 --> 00:37:05.640]   So I just wanna,
[00:37:06.800 --> 00:37:10.080]   like one was, well, to work on the cloud code base
[00:37:10.080 --> 00:37:12.600]   and make more modules based on that.
[00:37:12.600 --> 00:37:15.120]   They need more context or something where like,
[00:37:15.120 --> 00:37:16.760]   it seems like they might already be able to fit
[00:37:16.760 --> 00:37:18.160]   in the context.
[00:37:18.160 --> 00:37:19.480]   Or do you mean like actual,
[00:37:19.480 --> 00:37:21.800]   do you mean like the context window context or like more?
[00:37:21.800 --> 00:37:23.360]   - Yeah, the context window context.
[00:37:23.360 --> 00:37:25.920]   - So yeah, it seems like now it might just be able to fit.
[00:37:25.920 --> 00:37:27.720]   The thing that's preventing it from making good modules
[00:37:27.720 --> 00:37:30.960]   is not the lack of being able to put the code base in there.
[00:37:30.960 --> 00:37:32.280]   - I think that will be there soon.
[00:37:32.280 --> 00:37:33.120]   - Yeah.
[00:37:33.120 --> 00:37:34.160]   But like, it's not gonna be as good at you
[00:37:34.160 --> 00:37:35.640]   as you at like coming up with papers
[00:37:35.640 --> 00:37:37.640]   because it can like fit the code base in there.
[00:37:37.640 --> 00:37:40.440]   - No, but it will speed up a lot of the engineering.
[00:37:40.440 --> 00:37:41.280]   - Hmm.
[00:37:41.280 --> 00:37:43.440]   In a way that causes an intelligence explosion?
[00:37:43.440 --> 00:37:45.960]   - No, that accelerates research.
[00:37:45.960 --> 00:37:47.840]   But I think these things compound.
[00:37:47.840 --> 00:37:49.760]   So like the faster I can do my engineering,
[00:37:49.760 --> 00:37:51.640]   the more experiments I can run.
[00:37:51.640 --> 00:37:53.800]   And then the more experiments I can run, the faster we can.
[00:37:53.800 --> 00:37:56.480]   I mean, my work isn't actually accelerating capabilities
[00:37:56.480 --> 00:37:57.320]   at all.
[00:37:57.320 --> 00:37:58.160]   - Right, right.
[00:37:58.160 --> 00:37:59.000]   - It's just like interpreting the models.
[00:37:59.000 --> 00:38:00.560]   But we have a lot more work to do on that.
[00:38:00.560 --> 00:38:01.720]   (laughing)
[00:38:01.720 --> 00:38:03.520]   Surprise to the Twitter,
[00:38:03.520 --> 00:38:05.480]   (laughing)
[00:38:05.480 --> 00:38:06.680]   I mean, for context,
[00:38:06.680 --> 00:38:08.080]   like when you released your paper,
[00:38:08.080 --> 00:38:09.400]   there was a lot of talk on Twitter
[00:38:09.400 --> 00:38:12.280]   about alignment is solved guys, close the curtains.
[00:38:12.280 --> 00:38:14.520]   (laughing)
[00:38:14.520 --> 00:38:18.840]   - Yeah, yeah, no, it keeps me up at night
[00:38:18.840 --> 00:38:20.880]   how quickly the models are becoming more capable
[00:38:20.880 --> 00:38:23.720]   and like just how poor our understanding still is
[00:38:23.720 --> 00:38:24.680]   of what's going on.
[00:38:24.680 --> 00:38:28.840]   - Yeah, I guess I'm still...
[00:38:28.840 --> 00:38:31.680]   Okay, so lessening through the specifics here.
[00:38:31.680 --> 00:38:33.040]   By the time this is happening,
[00:38:33.040 --> 00:38:36.040]   we have bigger models that are two to four orders
[00:38:36.040 --> 00:38:38.480]   of magnitude bigger, right?
[00:38:38.480 --> 00:38:39.600]   Or at least an effective compute
[00:38:39.600 --> 00:38:41.560]   are two to four orders of magnitude bigger.
[00:38:41.560 --> 00:38:45.520]   And so this like idea that,
[00:38:45.520 --> 00:38:48.160]   well, you can run experiments faster or something.
[00:38:48.160 --> 00:38:50.080]   You're having to retrain that model
[00:38:50.080 --> 00:38:52.360]   in this version of the intelligence explosion.
[00:38:52.360 --> 00:38:56.280]   Like the recursive self-improvement
[00:38:56.280 --> 00:38:58.400]   is different from what might've been imagined 20 years ago
[00:38:58.400 --> 00:38:59.680]   where you just rewrite the code.
[00:38:59.680 --> 00:39:01.720]   You actually have to train a new model
[00:39:01.720 --> 00:39:03.080]   and that's really expensive.
[00:39:03.080 --> 00:39:04.800]   Not only now, but especially in the future
[00:39:04.800 --> 00:39:06.640]   as you keep like making these models
[00:39:06.640 --> 00:39:08.720]   orders of magnitude bigger.
[00:39:08.720 --> 00:39:11.160]   Doesn't that dampen the possibility
[00:39:11.160 --> 00:39:12.800]   of a sort of recursive self-improvement type
[00:39:12.800 --> 00:39:14.000]   intelligence explosion?
[00:39:14.000 --> 00:39:19.840]   - It's definitely gonna act as a breaking mechanism.
[00:39:19.840 --> 00:39:26.360]   I agree that the world of like what we're making today
[00:39:26.360 --> 00:39:28.120]   looks very different to what people imagined
[00:39:28.120 --> 00:39:29.080]   it would look like 20 years ago.
[00:39:29.080 --> 00:39:30.520]   Like it's not gonna be able to write the same code
[00:39:30.520 --> 00:39:32.080]   to be like really smart
[00:39:32.080 --> 00:39:33.720]   because actually it needs to train itself.
[00:39:33.720 --> 00:39:37.360]   Like the code itself is typically quite simple,
[00:39:37.360 --> 00:39:40.360]   typically fairly small and self-contained.
[00:39:40.360 --> 00:39:41.960]   I think John Carmack had this nice phrase
[00:39:41.960 --> 00:39:43.360]   where it's like it's like the first time in history
[00:39:43.360 --> 00:39:45.480]   where like you can actually plausibly imagine
[00:39:45.480 --> 00:39:47.800]   writing AI with like 10,000 lines of code.
[00:39:47.800 --> 00:39:50.480]   And that like actually does seem plausible
[00:39:50.480 --> 00:39:54.040]   when you pair most training code bases down to the limit.
[00:39:54.040 --> 00:39:56.720]   But it doesn't take away from the fact
[00:39:56.720 --> 00:39:59.360]   that this is something we should really strive to measure
[00:39:59.360 --> 00:40:02.760]   and estimate like how progress might occur.
[00:40:02.760 --> 00:40:05.000]   Like we should be trying very, very hard right now
[00:40:05.000 --> 00:40:07.880]   to measure exactly how much of a software engineer's job
[00:40:07.880 --> 00:40:10.640]   is automatable and what the trend line looks like
[00:40:10.640 --> 00:40:12.720]   and be trying our hardest to project out those trend lines.
[00:40:12.720 --> 00:40:15.120]   - But with all due respect to software engineers,
[00:40:15.120 --> 00:40:18.760]   like you are not like writing a React front end, right?
[00:40:18.760 --> 00:40:19.600]   - Right.
[00:40:19.600 --> 00:40:20.960]   - So it's like, I don't know how this,
[00:40:20.960 --> 00:40:22.520]   like what is concretely happening?
[00:40:22.520 --> 00:40:24.640]   And maybe you can walk me through,
[00:40:24.640 --> 00:40:28.120]   walk me through like a day in the life of,
[00:40:28.120 --> 00:40:30.480]   like you're working on an experiment or project
[00:40:30.480 --> 00:40:32.720]   that's going to make the model quote unquote better.
[00:40:32.720 --> 00:40:33.560]   - Right.
[00:40:33.560 --> 00:40:35.840]   - Like what is happening from observation,
[00:40:35.840 --> 00:40:38.520]   to experiment, to theory, to like writing the code,
[00:40:38.520 --> 00:40:39.400]   what is happening?
[00:40:39.400 --> 00:40:40.920]   - And so I think one to contextualize here
[00:40:40.920 --> 00:40:44.440]   is that like I've primarily worked on inference so far.
[00:40:44.440 --> 00:40:47.920]   So a lot of what I've been doing is just taking
[00:40:47.920 --> 00:40:50.800]   or helping guide the pre-training process,
[00:40:50.800 --> 00:40:52.640]   such as we design a good model for inference
[00:40:52.640 --> 00:40:53.800]   and then making the model
[00:40:53.800 --> 00:40:55.200]   and like the surrounding system faster.
[00:40:55.200 --> 00:40:56.880]   I've also done some pre-training work around that,
[00:40:56.880 --> 00:40:58.760]   but that hasn't been like my 100% focus,
[00:40:58.760 --> 00:41:00.720]   but I can still describe what I do when I do that work.
[00:41:00.720 --> 00:41:01.560]   - I know, but sorry, let me interrupt and say--
[00:41:01.560 --> 00:41:03.240]   - So it's like two types of work, yeah.
[00:41:03.240 --> 00:41:04.960]   - In Karl Schulman's,
[00:41:04.960 --> 00:41:07.400]   like when he was talking about it on the podcast,
[00:41:07.400 --> 00:41:10.320]   he did say that things like improving inference
[00:41:10.320 --> 00:41:12.640]   or even literally having like better,
[00:41:12.640 --> 00:41:14.960]   helping it make better chips or GPUs,
[00:41:14.960 --> 00:41:16.960]   that's like part of the intelligence explosion.
[00:41:16.960 --> 00:41:17.800]   - Yeah.
[00:41:17.800 --> 00:41:19.520]   - 'Cause like obviously if the inference code runs faster,
[00:41:19.520 --> 00:41:21.520]   like it happens better or faster or whatever.
[00:41:21.520 --> 00:41:22.360]   - Right.
[00:41:22.360 --> 00:41:23.200]   - Anyway, sorry, go ahead.
[00:41:23.200 --> 00:41:24.040]   - Yeah.
[00:41:24.040 --> 00:41:26.920]   So what does concretely a day look like?
[00:41:26.920 --> 00:41:30.920]   I think the most important like part to illustrate
[00:41:30.920 --> 00:41:33.760]   is this cycle of coming up with an idea,
[00:41:33.760 --> 00:41:35.840]   proving it out at different points in scale
[00:41:35.840 --> 00:41:41.760]   and like interpreting and understanding what goes wrong.
[00:41:41.760 --> 00:41:44.080]   And I think most people would be surprised
[00:41:44.080 --> 00:41:46.200]   to learn just how much goes into interpret,
[00:41:46.200 --> 00:41:48.880]   like interpreting and understanding what goes wrong.
[00:41:48.880 --> 00:41:52.000]   'Cause the ideas, people have long lists of ideas
[00:41:52.000 --> 00:41:52.840]   that they want to try,
[00:41:52.840 --> 00:41:55.920]   not every idea that you think should work will work
[00:41:55.920 --> 00:41:57.760]   and trying to understand why that is, is quite difficult.
[00:41:57.760 --> 00:41:59.600]   And like working out what exactly you need to do
[00:41:59.600 --> 00:42:00.920]   to interrogate it.
[00:42:00.920 --> 00:42:02.840]   So, so much of it is like introspection
[00:42:02.840 --> 00:42:03.680]   about what's going on.
[00:42:03.680 --> 00:42:05.720]   It's not pumping out thousands and thousands
[00:42:05.720 --> 00:42:06.880]   and thousands of line of code.
[00:42:06.880 --> 00:42:11.680]   It's not like the difficulty in coming up with ideas even.
[00:42:11.680 --> 00:42:13.440]   I think many people have a long list of ideas
[00:42:13.440 --> 00:42:14.600]   that they want to try,
[00:42:14.600 --> 00:42:16.840]   but paring that down and shock calling
[00:42:16.840 --> 00:42:18.960]   under very imperfect information,
[00:42:18.960 --> 00:42:23.840]   what the right ideas to explore further is really hard.
[00:42:23.840 --> 00:42:25.280]   - Tell me more about,
[00:42:25.280 --> 00:42:27.160]   what do you mean by imperfect information?
[00:42:27.160 --> 00:42:28.720]   Are these early experiments?
[00:42:28.720 --> 00:42:31.920]   Are these, like, what is the information that you're-
[00:42:31.920 --> 00:42:34.040]   - So, so Demis mentioned this in his podcast.
[00:42:34.040 --> 00:42:36.160]   And also like, you obviously, it's like the GPT-4 paper
[00:42:36.160 --> 00:42:38.040]   where you have like scaling law increments
[00:42:38.040 --> 00:42:39.360]   and you can see like in the GPT-4 paper,
[00:42:39.360 --> 00:42:41.080]   they have like a bunch of like dots, right?
[00:42:41.080 --> 00:42:42.720]   Where they say we can estimate the performance
[00:42:42.720 --> 00:42:44.960]   of our final model, like using all of these dots
[00:42:44.960 --> 00:42:46.920]   and there's a nice curve that like flows through them.
[00:42:46.920 --> 00:42:50.640]   And Demis mentioned that we do this process of scaling up.
[00:42:50.640 --> 00:42:55.280]   Concretely, why is that imperfect information?
[00:42:55.280 --> 00:42:59.120]   Is you never actually know if the trend will hold.
[00:42:59.120 --> 00:43:02.080]   For certain architectures, the trend has held really well.
[00:43:02.080 --> 00:43:04.600]   And for certain changes, it's held really well.
[00:43:04.600 --> 00:43:06.520]   But that isn't always the case.
[00:43:06.520 --> 00:43:08.760]   And things which can help at smaller scales
[00:43:08.760 --> 00:43:10.680]   can actually hurt at larger scales.
[00:43:12.000 --> 00:43:17.000]   So making guesses based on what the trend lines look like
[00:43:17.000 --> 00:43:20.400]   and based on like your intuitive feeling of,
[00:43:20.400 --> 00:43:23.840]   okay, this is actually something that's going to matter.
[00:43:23.840 --> 00:43:26.360]   Particularly for those ones which help at the small scale.
[00:43:26.360 --> 00:43:27.840]   - That's interesting to consider that
[00:43:27.840 --> 00:43:30.120]   for every chart you see in a release paper
[00:43:30.120 --> 00:43:32.640]   or technical report that shows that smooth curve,
[00:43:32.640 --> 00:43:35.600]   there's a graveyard of like first neurons
[00:43:35.600 --> 00:43:36.440]   and then it's like flat or something.
[00:43:36.440 --> 00:43:37.680]   - Yeah, there's all these like other lines
[00:43:37.680 --> 00:43:38.920]   that go in like different directions,
[00:43:38.920 --> 00:43:40.760]   off like Taylor or something.
[00:43:40.760 --> 00:43:42.440]   - Yeah, it's crazy.
[00:43:42.440 --> 00:43:44.360]   Both like as a grad student and then also here.
[00:43:44.360 --> 00:43:46.760]   Like the number of experiments that you have to run
[00:43:46.760 --> 00:43:48.680]   before getting like a meaningful result.
[00:43:48.680 --> 00:43:51.360]   - Tell me, okay, so you,
[00:43:51.360 --> 00:43:53.480]   but presumably it's not just like you run it
[00:43:53.480 --> 00:43:56.740]   until it stops and then like, let's go to the next thing.
[00:43:56.740 --> 00:43:59.240]   There's some process by which to interpret the early data
[00:43:59.240 --> 00:44:02.120]   and also to look at your, like, I don't know.
[00:44:02.120 --> 00:44:03.880]   I could like put a Google Doc in front of you
[00:44:03.880 --> 00:44:05.800]   and I'm pretty sure you could just like keep typing
[00:44:05.800 --> 00:44:09.280]   for a while on like different ideas you have.
[00:44:09.280 --> 00:44:10.920]   And there's some bottleneck between that
[00:44:10.920 --> 00:44:14.040]   and just like making the models better immediately.
[00:44:14.040 --> 00:44:15.120]   - Right.
[00:44:15.120 --> 00:44:17.820]   - Yeah, walk me through like what is the inference
[00:44:17.820 --> 00:44:19.600]   you're making from the first early steps
[00:44:19.600 --> 00:44:21.720]   that makes you have better experiments and better ideas?
[00:44:21.720 --> 00:44:24.160]   - I think one thing that I didn't fully convey before
[00:44:24.160 --> 00:44:26.240]   was that I think a lot of like good research
[00:44:26.240 --> 00:44:27.080]   comes from working backwards
[00:44:27.080 --> 00:44:28.400]   from the actual problems that you want to solve.
[00:44:28.400 --> 00:44:30.160]   And there's a couple of like grand problems
[00:44:30.160 --> 00:44:33.840]   I suppose in like making the models better today
[00:44:33.840 --> 00:44:36.160]   that you would identify as issues
[00:44:36.160 --> 00:44:37.200]   and then like work from, okay,
[00:44:37.200 --> 00:44:40.120]   how could I like change it to achieve this?
[00:44:40.120 --> 00:44:43.320]   There's also a bunch of, when you scale,
[00:44:43.320 --> 00:44:45.720]   you run into things and you want to like fix behaviors
[00:44:45.720 --> 00:44:48.120]   or like issues at scale.
[00:44:48.120 --> 00:44:49.700]   And that like informs a lot of the research
[00:44:49.700 --> 00:44:52.000]   for the next increment and this kind of stuff.
[00:44:52.000 --> 00:44:54.800]   So concretely the barrier
[00:44:54.800 --> 00:44:56.240]   is a little bit software engineering,
[00:44:56.240 --> 00:44:58.640]   like often having a code base that's large
[00:44:58.640 --> 00:45:02.160]   and sort of capable enough that it can support
[00:45:02.160 --> 00:45:03.920]   many people doing research at the same time
[00:45:03.920 --> 00:45:05.000]   makes it complex.
[00:45:05.000 --> 00:45:06.360]   If you're doing everything by yourself,
[00:45:06.360 --> 00:45:07.720]   your iteration pace is going to be much faster.
[00:45:07.720 --> 00:45:09.800]   I've heard that like Alec Radford, for example,
[00:45:09.800 --> 00:45:12.360]   like famously did much of the pioneering work at OpenAI.
[00:45:12.360 --> 00:45:14.280]   He like mostly works out of like a Jupiter notebook
[00:45:14.280 --> 00:45:16.680]   and then like has someone else who like writes
[00:45:16.680 --> 00:45:18.600]   and productionizes that code for him.
[00:45:18.600 --> 00:45:20.600]   I don't know if that's true or not.
[00:45:20.600 --> 00:45:21.880]   But like that kind of stuff,
[00:45:21.880 --> 00:45:25.320]   like actually operating with other people
[00:45:25.320 --> 00:45:28.680]   makes it, raises the complexity a lot
[00:45:28.680 --> 00:45:31.320]   because not for natural reasons,
[00:45:31.320 --> 00:45:33.600]   like familiar to like every software engineer.
[00:45:34.720 --> 00:45:38.440]   And then the inherent running,
[00:45:38.440 --> 00:45:40.040]   like running and launching those experiments is easy,
[00:45:40.040 --> 00:45:43.000]   but there's inherent time, like slows downs induced by that.
[00:45:43.000 --> 00:45:45.520]   So you often wanna be paralyzing multiple different streams
[00:45:45.520 --> 00:45:47.800]   'cause one, you can't like be totally focused
[00:45:47.800 --> 00:45:49.480]   on one thing necessarily.
[00:45:49.480 --> 00:45:52.480]   You might not have like fast enough feedback cycles.
[00:45:52.480 --> 00:45:53.800]   And then intuiting what went wrong
[00:45:53.800 --> 00:45:54.920]   is actually really hard.
[00:45:54.920 --> 00:45:57.880]   Like working out what, like this is in many respects
[00:45:57.880 --> 00:45:59.680]   the problem that the team that Trenton is on
[00:45:59.680 --> 00:46:01.040]   is trying to better understand
[00:46:01.040 --> 00:46:03.160]   is like what is going on inside these models.
[00:46:03.160 --> 00:46:05.040]   We have inferences and understanding
[00:46:05.040 --> 00:46:07.360]   and like head canon for why certain things work,
[00:46:07.360 --> 00:46:09.520]   but it's not an exact science.
[00:46:09.520 --> 00:46:11.600]   And so you have to constantly be making guesses
[00:46:11.600 --> 00:46:13.200]   about why something might've happened,
[00:46:13.200 --> 00:46:14.480]   what experiment might reveal,
[00:46:14.480 --> 00:46:16.000]   whether that is or isn't true.
[00:46:16.000 --> 00:46:18.560]   And that's probably the most complex part.
[00:46:18.560 --> 00:46:23.160]   The performance work by comparatively is easier,
[00:46:23.160 --> 00:46:24.400]   but harder in other respects.
[00:46:24.400 --> 00:46:26.760]   It's just a lot of low level
[00:46:26.760 --> 00:46:28.560]   and like difficult engineering work.
[00:46:28.560 --> 00:46:31.440]   - Yeah, I agree with a lot of that.
[00:46:31.440 --> 00:46:33.080]   But even on the interpretability team,
[00:46:33.080 --> 00:46:35.400]   I mean, especially with Chris Ola leading it,
[00:46:35.400 --> 00:46:38.320]   there are just so many ideas that we wanna test.
[00:46:38.320 --> 00:46:42.400]   And it's really just having the engineering skill,
[00:46:42.400 --> 00:46:43.800]   but I'll put engineering in quotes
[00:46:43.800 --> 00:46:45.440]   because a lot of it is research
[00:46:45.440 --> 00:46:49.360]   to like very quickly iterate on an experiment,
[00:46:49.360 --> 00:46:51.080]   look at the results, interpret it,
[00:46:51.080 --> 00:46:53.000]   try the next thing, communicate them,
[00:46:53.000 --> 00:46:55.440]   and then just ruthlessly prioritizing
[00:46:55.440 --> 00:46:58.400]   what the highest priority things to do are.
[00:46:58.400 --> 00:46:59.480]   - And this is really important.
[00:46:59.480 --> 00:47:00.960]   Like the ruthless prioritization
[00:47:00.960 --> 00:47:03.560]   is something which I think separates
[00:47:03.560 --> 00:47:04.680]   a lot of like quality research
[00:47:04.680 --> 00:47:07.720]   from research that doesn't necessarily succeed as much.
[00:47:07.720 --> 00:47:09.760]   We're in this funny field
[00:47:09.760 --> 00:47:14.360]   where so many of our initial theoretical understanding
[00:47:14.360 --> 00:47:16.520]   is like broken down basically.
[00:47:16.520 --> 00:47:18.600]   And so you need to have this simplicity bias
[00:47:18.600 --> 00:47:20.000]   and like ruthless prioritization
[00:47:20.000 --> 00:47:21.120]   over what's actually going wrong.
[00:47:21.120 --> 00:47:22.360]   And I think that's one of the things
[00:47:22.360 --> 00:47:23.760]   that separates the most effective people
[00:47:23.760 --> 00:47:26.480]   is they don't necessarily get like too attached
[00:47:26.480 --> 00:47:30.800]   to using a given solution
[00:47:30.800 --> 00:47:32.920]   that they're necessarily familiar with,
[00:47:32.920 --> 00:47:35.880]   but rather they attack the problem directly.
[00:47:35.880 --> 00:47:39.240]   You see this a lot in like maybe people coming
[00:47:39.240 --> 00:47:40.720]   with a specific academic background,
[00:47:40.720 --> 00:47:43.440]   they try and solve problems with that toolbox.
[00:47:43.440 --> 00:47:45.040]   And the best people are people
[00:47:45.040 --> 00:47:47.320]   who expand the toolbox dramatically.
[00:47:47.320 --> 00:47:49.680]   They're running around and they're taking ideas
[00:47:49.680 --> 00:47:51.080]   from reinforcement learning,
[00:47:51.080 --> 00:47:52.360]   but also from optimization theory.
[00:47:52.360 --> 00:47:54.200]   And also they have a great understanding of systems.
[00:47:54.200 --> 00:47:55.800]   And so they know what the sort of constraints
[00:47:55.800 --> 00:47:57.040]   that bound the problem are.
[00:47:57.040 --> 00:47:58.040]   And they're good engineers.
[00:47:58.040 --> 00:47:59.480]   They can iterate and try ideas fast.
[00:47:59.480 --> 00:48:02.440]   Like by far the best researchers I've seen,
[00:48:02.440 --> 00:48:04.160]   they all have the ability to try experiments
[00:48:04.160 --> 00:48:06.480]   really, really, really, really, really fast.
[00:48:06.480 --> 00:48:09.240]   And that is that cycle time at smaller scales,
[00:48:09.240 --> 00:48:11.280]   cycle time separates people.
[00:48:11.280 --> 00:48:14.400]   - I mean, machine learning research is just so empirical.
[00:48:14.400 --> 00:48:15.240]   - Yeah.
[00:48:15.240 --> 00:48:18.080]   - And this is honestly one reason why I think
[00:48:18.080 --> 00:48:20.480]   our solutions might end up looking more brain-like
[00:48:20.480 --> 00:48:22.000]   than otherwise.
[00:48:22.000 --> 00:48:24.400]   It's like, even though we wouldn't want to admit it,
[00:48:24.400 --> 00:48:27.040]   the whole community is kind of doing
[00:48:27.040 --> 00:48:29.880]   like greedy evolutionary optimization
[00:48:29.880 --> 00:48:32.520]   over the landscape of like possible AI architectures
[00:48:32.520 --> 00:48:33.880]   and everything else.
[00:48:33.880 --> 00:48:35.280]   It's like no better than evolution.
[00:48:35.280 --> 00:48:37.280]   And that's not even necessarily a slight against evolution.
[00:48:37.280 --> 00:48:38.800]   - That's such an interesting idea.
[00:48:38.800 --> 00:48:42.880]   I'm still confused on what will be the bottleneck
[00:48:42.880 --> 00:48:45.960]   for these, what would we have to be true of an agent
[00:48:45.960 --> 00:48:48.320]   such that it's like sped up your research?
[00:48:48.320 --> 00:48:50.120]   So in the Alec Radford example you gave
[00:48:50.120 --> 00:48:52.920]   where he apparently already has the equivalent
[00:48:52.920 --> 00:48:56.240]   of like co-pilot for his Jupyter notebook experiments.
[00:48:57.240 --> 00:48:59.160]   Is it just that if he had enough of those
[00:48:59.160 --> 00:49:01.440]   he would be a dramatically faster researcher
[00:49:01.440 --> 00:49:03.000]   and so you just need Alec Radford?
[00:49:03.000 --> 00:49:04.720]   So it's like, you're not automating the humans,
[00:49:04.720 --> 00:49:06.920]   you're just making the most effective researchers
[00:49:06.920 --> 00:49:09.360]   who have great taste more effective
[00:49:09.360 --> 00:49:11.480]   and like running the experiments for them and so forth
[00:49:11.480 --> 00:49:14.920]   or like you're still working at the point
[00:49:14.920 --> 00:49:16.560]   which the intelligence explosion is happening.
[00:49:16.560 --> 00:49:17.400]   You know what I mean?
[00:49:17.400 --> 00:49:18.240]   Like, is that what you're saying?
[00:49:18.240 --> 00:49:19.080]   - Right.
[00:49:19.080 --> 00:49:21.800]   And if that were like directly true,
[00:49:21.800 --> 00:49:25.840]   why can't we scale out current research teams better?
[00:49:25.840 --> 00:49:27.760]   For example, I think an interesting question for us,
[00:49:27.760 --> 00:49:29.720]   like why, if this work is so valuable,
[00:49:29.720 --> 00:49:32.680]   why can't we take hundreds or thousands of people
[00:49:32.680 --> 00:49:35.240]   who are like, they're definitely out there
[00:49:35.240 --> 00:49:39.080]   and like scale our organizations better?
[00:49:39.080 --> 00:49:46.280]   I think we are less at the moment
[00:49:46.280 --> 00:49:50.520]   bound by the sheer engineering work of making these things
[00:49:50.520 --> 00:49:54.640]   than we are by compute to run and get signal
[00:49:54.640 --> 00:49:59.640]   and taste in terms of what the actual like right thing
[00:49:59.640 --> 00:50:01.840]   to do it and that like making those difficult inferences
[00:50:01.840 --> 00:50:03.080]   on imperfect information.
[00:50:03.080 --> 00:50:05.960]   - For the Gemini team.
[00:50:05.960 --> 00:50:07.520]   'Cause I think for interpretability,
[00:50:07.520 --> 00:50:10.960]   we actually really want to keep hiring talented engineers
[00:50:10.960 --> 00:50:12.040]   and I think that's a big bottleneck
[00:50:12.040 --> 00:50:14.120]   for us to just keep making a lot of progress.
[00:50:14.120 --> 00:50:18.000]   - Obviously more people is like better,
[00:50:18.000 --> 00:50:20.920]   but I do think like it's interesting to consider,
[00:50:20.920 --> 00:50:23.960]   I think like one of the biggest challenges
[00:50:23.960 --> 00:50:26.480]   that like I've thought a lot about
[00:50:26.480 --> 00:50:28.040]   is how do we scale better?
[00:50:28.040 --> 00:50:29.720]   Like Google is an enormous organization
[00:50:29.720 --> 00:50:32.200]   and has 200,000 ish people, right?
[00:50:32.200 --> 00:50:35.560]   Like a hundred, maybe 80,000 or something like that.
[00:50:35.560 --> 00:50:38.680]   And one has to imagine if there were like ways
[00:50:38.680 --> 00:50:41.080]   of scaling out Gemini's research program
[00:50:41.080 --> 00:50:43.600]   to all those fantastically talented software engineers.
[00:50:43.600 --> 00:50:45.480]   And this seems like a key advantage
[00:50:45.480 --> 00:50:48.400]   that you would want to be able to take advantage of,
[00:50:48.400 --> 00:50:49.240]   you want to be able to use,
[00:50:49.240 --> 00:50:50.960]   but like how do you effectively do that?
[00:50:50.960 --> 00:50:53.800]   It's a very complex organizational problem.
[00:50:53.800 --> 00:50:57.120]   - So compute and taste,
[00:50:57.120 --> 00:50:58.200]   that's interesting to think about
[00:50:58.200 --> 00:51:00.320]   because at least the compute part
[00:51:00.320 --> 00:51:02.720]   is not bottlenecked on more intelligence.
[00:51:02.720 --> 00:51:05.960]   It just bottlenecked on Sam 7 trillion or whatever, right?
[00:51:05.960 --> 00:51:10.320]   So if I gave you 10X the H100s to run your experiments,
[00:51:10.320 --> 00:51:11.520]   how much more effective a researcher are you?
[00:51:11.520 --> 00:51:12.360]   - TPUs please.
[00:51:12.360 --> 00:51:14.560]   (all laughing)
[00:51:14.560 --> 00:51:17.320]   - How much more effective a researcher are you?
[00:51:17.320 --> 00:51:21.440]   - I think the Gemini program would probably
[00:51:21.440 --> 00:51:24.920]   be like maybe five times faster
[00:51:24.920 --> 00:51:26.640]   with 10X more compute or something like that.
[00:51:26.640 --> 00:51:29.480]   - So that's pretty good elasticity of like 0.5.
[00:51:29.480 --> 00:51:30.320]   - Yeah.
[00:51:30.320 --> 00:51:31.160]   - Wait, that's insane.
[00:51:31.160 --> 00:51:32.200]   - Yeah, I think like more compute
[00:51:32.200 --> 00:51:34.360]   would just like directly convert into progress.
[00:51:34.360 --> 00:51:38.400]   - So you have some fixed size of compute
[00:51:38.400 --> 00:51:40.920]   and some of it goes to in friends,
[00:51:40.920 --> 00:51:45.400]   some of, I guess like, and also like to clients of GCP.
[00:51:45.400 --> 00:51:46.240]   - Yep.
[00:51:46.240 --> 00:51:47.600]   - Some of it goes to, huh?
[00:51:47.600 --> 00:51:48.600]   (all laughing)
[00:51:48.600 --> 00:51:50.080]   Some of it goes to training
[00:51:50.080 --> 00:51:52.920]   and I guess as a fraction of it,
[00:51:52.920 --> 00:51:54.560]   some of it goes to running the experiments
[00:51:54.560 --> 00:51:55.400]   for the full model.
[00:51:55.400 --> 00:51:56.240]   - Yeah, that's right.
[00:51:56.240 --> 00:51:59.400]   - Shouldn't then the fraction goes to experiments be higher
[00:51:59.400 --> 00:52:01.280]   given that you would just be like,
[00:52:01.280 --> 00:52:02.560]   if like the bottleneck is research
[00:52:02.560 --> 00:52:04.720]   and research is bottlenecked by compute.
[00:52:04.720 --> 00:52:07.160]   - And so one of the strategic decisions
[00:52:07.160 --> 00:52:08.760]   that every pre-training team has to make
[00:52:08.760 --> 00:52:10.360]   is like exactly what amount of compute
[00:52:10.360 --> 00:52:12.840]   do you allocate to your different training runs?
[00:52:12.840 --> 00:52:15.040]   Just like to your research program
[00:52:15.040 --> 00:52:18.360]   versus like scaling the last best,
[00:52:18.360 --> 00:52:20.760]   I like, you know, thing that you landed on.
[00:52:20.760 --> 00:52:25.720]   And I think they're like,
[00:52:25.720 --> 00:52:26.680]   they're all trying to arrive
[00:52:26.680 --> 00:52:29.080]   at like a sort of pre-optimal point here.
[00:52:29.080 --> 00:52:30.360]   One of the reasons why you need
[00:52:30.360 --> 00:52:31.920]   to still keep training big models
[00:52:31.920 --> 00:52:33.240]   is that you get information there
[00:52:33.240 --> 00:52:35.360]   that you don't get otherwise.
[00:52:35.360 --> 00:52:38.280]   So scale has all these emergent properties
[00:52:38.280 --> 00:52:40.200]   which you want to understand better.
[00:52:40.200 --> 00:52:42.640]   And if you like are always doing research
[00:52:42.640 --> 00:52:44.520]   and never, like remember what I said before
[00:52:44.520 --> 00:52:47.240]   about like, you're not sure what's gonna like
[00:52:47.240 --> 00:52:49.480]   fall off the curve, right?
[00:52:49.480 --> 00:52:51.760]   If you like keep doing research in this regime
[00:52:51.760 --> 00:52:56.480]   and like keep on getting more and more compute efficient,
[00:52:56.480 --> 00:52:59.000]   you may never, you may have actually like
[00:52:59.000 --> 00:53:01.040]   gone off the path to actually eventually scale.
[00:53:01.040 --> 00:53:02.760]   So you need to constantly be investing
[00:53:02.760 --> 00:53:06.200]   in doing big runs too at the frontier
[00:53:06.200 --> 00:53:08.360]   of what you sort of expect to work.
[00:53:08.360 --> 00:53:09.800]   - Okay, so then tell me what it looks like
[00:53:09.800 --> 00:53:11.320]   to be in the world where AI
[00:53:11.320 --> 00:53:13.960]   has significantly sped up AI research.
[00:53:13.960 --> 00:53:15.920]   'Cause from this, it doesn't really sound
[00:53:15.920 --> 00:53:19.280]   like the AIs are going off and writing the code from scratch
[00:53:19.280 --> 00:53:20.720]   and that's leading to faster output.
[00:53:20.720 --> 00:53:22.280]   It sounds like they're really augmenting
[00:53:22.280 --> 00:53:24.040]   the top researchers in some way.
[00:53:24.040 --> 00:53:25.240]   Like, yeah, tell me concretely,
[00:53:25.240 --> 00:53:26.400]   are they doing the experiments?
[00:53:26.400 --> 00:53:27.600]   Are they coming up with the ideas?
[00:53:27.600 --> 00:53:29.600]   Are they just like evaluating the outputs
[00:53:29.600 --> 00:53:30.960]   of the experiments, what's happening?
[00:53:30.960 --> 00:53:33.120]   - So I think there's like two walls
[00:53:33.120 --> 00:53:33.960]   you need to consider here.
[00:53:33.960 --> 00:53:37.200]   One is where AI has meaningfully sped up our ability
[00:53:37.200 --> 00:53:39.080]   to make algorithmic progress.
[00:53:39.080 --> 00:53:41.680]   And one is where the output of the AI itself
[00:53:41.680 --> 00:53:43.760]   is the thing that's like the crucial ingredient
[00:53:43.760 --> 00:53:47.480]   towards like model capability progress.
[00:53:47.480 --> 00:53:49.440]   And like specifically what I mean there is--
[00:53:49.440 --> 00:53:52.560]   - Synthetic data. - Like synthetic data, right?
[00:53:52.560 --> 00:53:54.960]   And in the first world
[00:53:54.960 --> 00:53:56.840]   where it's meaningfully speeding up algorithmic progress,
[00:53:56.840 --> 00:53:59.760]   I think a necessary component of that is more compute.
[00:53:59.760 --> 00:54:03.680]   And you probably like reach this elasticity point
[00:54:03.680 --> 00:54:07.560]   where like AIs maybe at some point are easier to speed up
[00:54:07.560 --> 00:54:09.760]   and get on to context than yourself.
[00:54:09.760 --> 00:54:12.000]   That's just right than other people.
[00:54:12.000 --> 00:54:14.480]   And so AIs meaningfully speed up your work
[00:54:14.480 --> 00:54:17.600]   because they're like a fantastic co-pilot basically
[00:54:17.600 --> 00:54:20.240]   that helps you code multiple times faster.
[00:54:20.240 --> 00:54:22.520]   And that seems like actually quite reasonable.
[00:54:22.520 --> 00:54:25.880]   Super long context, super smart model,
[00:54:25.880 --> 00:54:29.040]   it's onboarded immediately and you can like send them off
[00:54:29.040 --> 00:54:32.040]   and to like complete subtasks and sub goals for you.
[00:54:32.040 --> 00:54:33.640]   And that actually like feels very plausible,
[00:54:33.640 --> 00:54:35.680]   but again, we don't know
[00:54:35.680 --> 00:54:39.320]   because there are no great evals about that kind of thing.
[00:54:39.320 --> 00:54:41.560]   But like the best one is as I said before, SweetBench.
[00:54:41.560 --> 00:54:44.720]   - Although in that one, somebody was mentioning to me,
[00:54:44.720 --> 00:54:46.840]   like the problem is that
[00:54:46.840 --> 00:54:48.840]   when a human is trying to do a pull request,
[00:54:48.840 --> 00:54:50.560]   they'll like type something out
[00:54:50.560 --> 00:54:52.520]   and they'll like run it and see if it works.
[00:54:52.520 --> 00:54:54.160]   And if it doesn't, they'll rewrite it.
[00:54:54.160 --> 00:54:58.440]   None of this was part of the opportunities
[00:54:58.440 --> 00:55:00.640]   that the LLM was given when run on this,
[00:55:00.640 --> 00:55:01.760]   like it just like output it.
[00:55:01.760 --> 00:55:04.040]   And if it runs and like checks all the boxes,
[00:55:04.040 --> 00:55:05.480]   then it passed, right?
[00:55:05.480 --> 00:55:07.760]   So it might've been an unfair test in that way.
[00:55:07.760 --> 00:55:11.720]   So you can imagine that is like,
[00:55:11.720 --> 00:55:12.840]   if you were able to use that,
[00:55:12.840 --> 00:55:16.080]   that would be an effective training source for having,
[00:55:16.080 --> 00:55:17.400]   like the key thing that's missing
[00:55:17.400 --> 00:55:22.400]   from a lot of training data is like the reasoning traces.
[00:55:22.400 --> 00:55:24.400]   Right?
[00:55:24.400 --> 00:55:25.360]   And I think this would be,
[00:55:25.360 --> 00:55:29.120]   if I wanted to try and automate a specific field
[00:55:29.120 --> 00:55:30.480]   with like job family,
[00:55:30.480 --> 00:55:36.520]   or like understand how like at risk of automation that is,
[00:55:36.520 --> 00:55:40.200]   then having reasoning traces feels to me
[00:55:40.200 --> 00:55:42.520]   like a really important part of that.
[00:55:42.520 --> 00:55:43.560]   - There's so many threads.
[00:55:43.560 --> 00:55:44.400]   Yeah.
[00:55:44.400 --> 00:55:45.240]   There's so many different threads
[00:55:45.240 --> 00:55:46.640]   and that I want to follow up on.
[00:55:46.640 --> 00:55:51.640]   Let's begin with the data versus like,
[00:55:51.640 --> 00:55:53.800]   yeah, compute thing of like,
[00:55:53.800 --> 00:55:56.280]   is the output of these AIs a thing
[00:55:56.280 --> 00:55:58.480]   that's causing the intelligence delusion or something?
[00:55:58.480 --> 00:55:59.320]   - Yeah.
[00:55:59.320 --> 00:56:02.320]   - People talk about how these models
[00:56:02.320 --> 00:56:04.240]   are really a reflection on their data.
[00:56:04.240 --> 00:56:05.080]   - Yeah.
[00:56:05.080 --> 00:56:06.280]   - I think there was, I forgot his name,
[00:56:06.280 --> 00:56:09.120]   but there was a great blog by this open AI engineer.
[00:56:09.120 --> 00:56:11.720]   And he was talking about at the end of the day,
[00:56:11.720 --> 00:56:13.200]   as these models get better and better,
[00:56:13.200 --> 00:56:17.160]   it just like, they're just going to be really effective,
[00:56:17.160 --> 00:56:19.240]   like maps of the data set.
[00:56:19.240 --> 00:56:20.080]   - Yeah.
[00:56:20.080 --> 00:56:21.440]   - And so it's like, at the end of the day,
[00:56:21.440 --> 00:56:23.480]   like you got to stop thinking about architectures.
[00:56:23.480 --> 00:56:25.000]   It's like the most effective architecture,
[00:56:25.000 --> 00:56:27.320]   it's just like doing an amazing job of mapping the data.
[00:56:27.320 --> 00:56:28.200]   - Right.
[00:56:28.200 --> 00:56:31.000]   - So that implies that future AI progress
[00:56:31.000 --> 00:56:34.640]   comes from the AI just making really awesome data, right?
[00:56:34.640 --> 00:56:36.120]   Like that you're mapping to.
[00:56:36.120 --> 00:56:39.400]   - Yeah, that's clearly a very important part, yeah.
[00:56:39.400 --> 00:56:41.080]   - Yeah, that's really interesting.
[00:56:41.080 --> 00:56:44.800]   Does that look to you like, I don't know,
[00:56:44.800 --> 00:56:47.240]   like things that look like chain of thought
[00:56:47.240 --> 00:56:49.600]   or what do you imagine as these models get better,
[00:56:49.600 --> 00:56:50.640]   as these models get smarter,
[00:56:50.640 --> 00:56:51.800]   what does the synthetic data look like?
[00:56:51.800 --> 00:56:54.440]   - When I think of really good data,
[00:56:54.440 --> 00:56:56.440]   to me that raises something
[00:56:56.440 --> 00:56:58.040]   which involves a lot of reasoning to create.
[00:56:58.040 --> 00:56:59.640]   So in modeling that,
[00:56:59.640 --> 00:57:01.320]   it's just similar to like Ilya's perspective
[00:57:01.920 --> 00:57:05.000]   on trying, on achieving like super intelligence
[00:57:05.000 --> 00:57:07.320]   by effectively like perfectly modeling
[00:57:07.320 --> 00:57:09.360]   the human textual output, right?
[00:57:09.360 --> 00:57:12.200]   But even in the near term,
[00:57:12.200 --> 00:57:14.600]   in order to model something like the archive papers
[00:57:14.600 --> 00:57:17.520]   or Wikipedia, you have to have an incredible amount
[00:57:17.520 --> 00:57:20.120]   of reasoning behind you in order to understand
[00:57:20.120 --> 00:57:23.320]   what next token might be being output.
[00:57:23.320 --> 00:57:28.920]   And so for me, what I imagine as good data
[00:57:28.920 --> 00:57:31.920]   is like data where you can similarly,
[00:57:31.920 --> 00:57:34.160]   at least like where it had to do reasoning
[00:57:34.160 --> 00:57:35.000]   to produce something.
[00:57:35.000 --> 00:57:36.680]   And then like the trick of course is,
[00:57:36.680 --> 00:57:39.440]   how do you verify that that reasoning was correct?
[00:57:39.440 --> 00:57:43.560]   And this is why you saw like DeepMind do that geometry,
[00:57:43.560 --> 00:57:45.720]   like the sort of like self-life of geometry basically,
[00:57:45.720 --> 00:57:47.520]   or like the sort of research for your geometry.
[00:57:47.520 --> 00:57:48.840]   This geometry is a really,
[00:57:48.840 --> 00:57:52.080]   it's an easily formalizable, easily verifiable field.
[00:57:52.080 --> 00:57:54.840]   So you can check if it's reasoning was correct
[00:57:54.840 --> 00:57:56.960]   and you can generate heaps of data of correct,
[00:57:56.960 --> 00:58:01.000]   of verified geometry proofs, train on that.
[00:58:01.000 --> 00:58:02.720]   And you know that that's good data.
[00:58:02.720 --> 00:58:03.560]   - It's actually funny,
[00:58:03.560 --> 00:58:05.480]   'cause I had a conversation with Grant Sanderson
[00:58:05.480 --> 00:58:07.360]   like last year where we were debating this
[00:58:07.360 --> 00:58:08.920]   and I was like, fuck dude,
[00:58:08.920 --> 00:58:11.440]   by the time they get the gold of the Math Olympiad,
[00:58:11.440 --> 00:58:13.080]   of course they're gonna automate all the jobs.
[00:58:13.080 --> 00:58:14.520]   (laughing)
[00:58:14.520 --> 00:58:15.360]   - Yikes.
[00:58:15.360 --> 00:58:19.800]   - On this synthetic data thing,
[00:58:19.800 --> 00:58:23.440]   one of the things I speculated about in my scaling post,
[00:58:23.440 --> 00:58:25.920]   which was heavily informed with discussions with you too.
[00:58:25.920 --> 00:58:27.000]   (laughing)
[00:58:27.000 --> 00:58:28.320]   And you especially Shoto,
[00:58:28.320 --> 00:58:32.240]   was you can think of like human evolution
[00:58:32.240 --> 00:58:34.320]   through the spectrum of like we get language
[00:58:34.320 --> 00:58:37.600]   and so we're like generating the synthetic data,
[00:58:37.600 --> 00:58:40.320]   which like our copies are generating the synthetic data,
[00:58:40.320 --> 00:58:41.320]   which we're trained on.
[00:58:41.320 --> 00:58:43.840]   And it's like this really effective genetics,
[00:58:43.840 --> 00:58:45.720]   a cultural like co-evolutionary loop.
[00:58:45.720 --> 00:58:47.320]   - And there's a verifier there too, right?
[00:58:47.320 --> 00:58:48.320]   Like there's the real world.
[00:58:48.320 --> 00:58:50.680]   You might generate a theory about,
[00:58:50.680 --> 00:58:53.960]   the gods cause the storms, right?
[00:58:53.960 --> 00:58:55.720]   And then like someone else finds cases
[00:58:55.720 --> 00:58:56.560]   where that isn't true.
[00:58:56.560 --> 00:58:57.400]   And so you like know that,
[00:58:57.400 --> 00:58:59.840]   that sort of didn't match your verification function.
[00:58:59.840 --> 00:59:02.840]   And now like actually instead you have like some
[00:59:02.840 --> 00:59:03.880]   weather simulation,
[00:59:03.880 --> 00:59:06.120]   which required a lot of reasoning to produce
[00:59:06.120 --> 00:59:08.600]   and like accurately matches reality.
[00:59:08.600 --> 00:59:10.680]   And like you can train on that
[00:59:10.680 --> 00:59:13.080]   as a better model of the world.
[00:59:13.080 --> 00:59:14.480]   Like we are training on that
[00:59:14.480 --> 00:59:16.720]   and like stories and like scientific theories.
[00:59:16.720 --> 00:59:18.320]   - Yeah.
[00:59:18.320 --> 00:59:19.160]   I wanna go back.
[00:59:19.160 --> 00:59:20.480]   I'm just remembering something you mentioned
[00:59:20.480 --> 00:59:22.600]   a little while ago of,
[00:59:23.680 --> 00:59:27.320]   given how sort of like empirical ML is,
[00:59:27.320 --> 00:59:29.320]   it really is evolutionary process
[00:59:29.320 --> 00:59:31.120]   as resulting in better performance
[00:59:31.120 --> 00:59:33.600]   and not necessarily an individual coming up
[00:59:33.600 --> 00:59:36.280]   with a breakthrough in like a top-down way.
[00:59:36.280 --> 00:59:39.840]   That has interesting implications.
[00:59:39.840 --> 00:59:41.640]   First being that,
[00:59:41.640 --> 00:59:45.720]   there really is,
[00:59:45.720 --> 00:59:48.080]   people are like are concerned about capabilities increasing
[00:59:48.080 --> 00:59:49.960]   because more people are going into the field.
[00:59:49.960 --> 00:59:52.200]   I've somewhat been skeptical of that way of thinking,
[00:59:52.200 --> 00:59:55.920]   but from this perspective of just like more input,
[00:59:55.920 --> 00:59:56.760]   it really does.
[00:59:56.760 --> 00:59:58.320]   Yeah, it feels more like,
[00:59:58.320 --> 01:00:00.080]   oh, actually by like the fact that
[01:00:00.080 --> 01:00:01.880]   more people are going to ICML means
[01:00:01.880 --> 01:00:04.600]   that there's like faster progress towards GPT-5.
[01:00:04.600 --> 01:00:06.560]   - Yeah, you just have more genetic recombination.
[01:00:06.560 --> 01:00:07.400]   - Right.
[01:00:07.400 --> 01:00:08.240]   - And like shots on target.
[01:00:08.240 --> 01:00:09.080]   - Yeah.
[01:00:09.080 --> 01:00:11.520]   - And I mean, aren't all fields kind of like that?
[01:00:11.520 --> 01:00:14.040]   Like this is the sort of scientific framery
[01:00:14.040 --> 01:00:16.920]   of like discovery versus invention, right?
[01:00:16.920 --> 01:00:19.800]   And discovery almost involves like
[01:00:19.800 --> 01:00:21.800]   whenever there's been a massive scientific breakthrough
[01:00:21.800 --> 01:00:23.560]   in the past, typically there are multiple people
[01:00:23.560 --> 01:00:26.880]   co-discovering that at like roughly the same time.
[01:00:26.880 --> 01:00:28.720]   And that feels to me at least a little bit
[01:00:28.720 --> 01:00:30.720]   like the mixing and trying of ideas.
[01:00:30.720 --> 01:00:33.320]   You can't try an idea that's so far out of scope
[01:00:33.320 --> 01:00:35.600]   that you have no way of verifying it
[01:00:35.600 --> 01:00:37.440]   with the tools you have available.
[01:00:37.440 --> 01:00:40.040]   - Yeah, I think physics and math might be
[01:00:40.040 --> 01:00:42.320]   slightly different in this regard.
[01:00:42.320 --> 01:00:45.120]   But especially for biology or any sort of wetware
[01:00:45.120 --> 01:00:47.040]   and to the extent we want to analogize neural networks here,
[01:00:47.040 --> 01:00:49.600]   it's just, it's comical how serendipitous
[01:00:49.600 --> 01:00:50.440]   a lot of the discoveries are.
[01:00:50.440 --> 01:00:51.280]   - Yeah.
[01:00:51.280 --> 01:00:52.920]   - Penicillin, for example.
[01:00:52.920 --> 01:00:56.280]   - Another implication of this is this idea
[01:00:56.280 --> 01:00:58.400]   that like AGI just gonna come tomorrow.
[01:00:58.400 --> 01:01:00.200]   Like somebody's just gonna discover a new algorithm
[01:01:00.200 --> 01:01:01.720]   and we have AGI.
[01:01:01.720 --> 01:01:02.960]   That seems less plausible.
[01:01:02.960 --> 01:01:05.120]   Like it will just be a matter of more and more
[01:01:05.120 --> 01:01:07.400]   and more researchers finding these marginal things
[01:01:07.400 --> 01:01:10.680]   that all add up together to make models better, right?
[01:01:10.680 --> 01:01:14.160]   Like, yeah, that feels like the correct story to me, yeah.
[01:01:14.160 --> 01:01:17.000]   - Especially while we're still hardware constrained.
[01:01:17.000 --> 01:01:18.360]   - Right.
[01:01:18.400 --> 01:01:21.600]   - Do you buy this narrow window framing
[01:01:21.600 --> 01:01:24.560]   of the intelligence explosion of,
[01:01:24.560 --> 01:01:28.560]   you have to each, you know, GPT-3 to GPT-4
[01:01:28.560 --> 01:01:33.120]   is two ooms of orders of magnitude more compute,
[01:01:33.120 --> 01:01:35.880]   or at least more effective compute,
[01:01:35.880 --> 01:01:38.160]   in the sense that if you didn't have any algorithmic progress
[01:01:38.160 --> 01:01:40.760]   it would have to be two orders of magnitude bigger,
[01:01:40.760 --> 01:01:43.240]   like the raw form to be as good.
[01:01:43.240 --> 01:01:46.320]   Do you buy the framing that given that you have to be
[01:01:46.320 --> 01:01:49.360]   two orders of magnitude bigger at every generation,
[01:01:49.360 --> 01:01:52.520]   if you don't get AGI by GPT-7,
[01:01:52.520 --> 01:01:55.320]   that can help you catapult the intelligence explosion?
[01:01:55.320 --> 01:01:57.960]   Like you're kind of just fucked as far as like
[01:01:57.960 --> 01:01:59.680]   much smarter intelligences go,
[01:01:59.680 --> 01:02:02.000]   and you're kind of stuck with GPT-7 level models
[01:02:02.000 --> 01:02:03.520]   for a long time.
[01:02:03.520 --> 01:02:04.840]   'Cause at that point you're just like consuming
[01:02:04.840 --> 01:02:07.560]   significant fractions of the economy to make that model.
[01:02:07.560 --> 01:02:10.600]   And we just don't have the wherewithal to like make GPT-8.
[01:02:10.600 --> 01:02:13.440]   - This is the Carl Schulman sort of argument of like,
[01:02:13.440 --> 01:02:15.280]   we're gonna race through the orders of magnitude
[01:02:15.280 --> 01:02:19.640]   in the near term, but then longer term it would be harder.
[01:02:19.640 --> 01:02:21.080]   I think like he's probably talked about it a lot,
[01:02:21.080 --> 01:02:23.240]   but yeah, but I do buy that framing.
[01:02:23.240 --> 01:02:27.440]   - Yeah, I mean, I generally buy that increases
[01:02:27.440 --> 01:02:29.840]   in order of magnitude of compute by like,
[01:02:29.840 --> 01:02:32.160]   in an absolute terms almost like diminishing returns
[01:02:32.160 --> 01:02:33.440]   on like your capability, right?
[01:02:33.440 --> 01:02:34.840]   Like we've seen over a couple of orders of magnitude
[01:02:34.840 --> 01:02:36.160]   models go from being unable to do anything
[01:02:36.160 --> 01:02:37.960]   to be able to like do huge amounts.
[01:02:37.960 --> 01:02:41.360]   And it feels to me like each incremental order of magnitude
[01:02:41.360 --> 01:02:43.520]   like gives more nines of reliability of things.
[01:02:43.520 --> 01:02:45.080]   And so it unlocks things like agents,
[01:02:45.080 --> 01:02:47.560]   but at least at the moment I haven't seen like
[01:02:47.560 --> 01:02:52.080]   transformatively, it doesn't feel like reasoning improves
[01:02:52.080 --> 01:02:53.840]   like linearly, so to speak,
[01:02:53.840 --> 01:02:55.760]   but rather like somewhat sub-linearly.
[01:02:55.760 --> 01:02:57.480]   - That's actually a very bearish sign
[01:02:57.480 --> 01:02:59.600]   because one of the things we're chatting
[01:02:59.600 --> 01:03:02.080]   with one of our friends and he made the point
[01:03:02.080 --> 01:03:06.800]   that if you look at what new applications
[01:03:06.800 --> 01:03:09.880]   are unlocked by GPT-4 relative to GPT-3.5,
[01:03:09.880 --> 01:03:12.040]   it's not clear that's like that much,
[01:03:12.040 --> 01:03:14.800]   like a GPT-3.5 can do perplexity or whatever.
[01:03:14.800 --> 01:03:18.560]   So if there is this diminishing increase in capabilities
[01:03:18.560 --> 01:03:23.680]   and that increased cost exponentially more to get,
[01:03:23.680 --> 01:03:25.080]   that's actually a bearish sign on like
[01:03:25.080 --> 01:03:27.440]   what 4.5 will be able to do or what 5 will unlock
[01:03:27.440 --> 01:03:28.760]   in terms of economic impact.
[01:03:28.760 --> 01:03:30.920]   - That being said, for me, the jump between 3.5 and 4
[01:03:30.920 --> 01:03:32.240]   is like pretty huge.
[01:03:32.240 --> 01:03:36.840]   And so like, even if I, it's like another 3.5 to 4 jump
[01:03:36.840 --> 01:03:38.760]   is like ridiculous, right?
[01:03:38.760 --> 01:03:41.560]   Like if you imagine 5 as being a 3.5 to 4 jump
[01:03:41.560 --> 01:03:43.120]   like straight off the bat in terms of like ability
[01:03:43.120 --> 01:03:44.600]   to do SATs and this kind of stuff.
[01:03:44.600 --> 01:03:46.760]   - The LSAT performance was particularly striking.
[01:03:46.760 --> 01:03:47.600]   - Exactly.
[01:03:47.600 --> 01:03:51.600]   You go from like very smart,
[01:03:51.600 --> 01:03:55.000]   like from like not super smart to like very smart
[01:03:55.000 --> 01:03:57.600]   to like utter genius in the next generation instantly.
[01:03:57.600 --> 01:03:59.800]   And it doesn't, at least like to me,
[01:03:59.800 --> 01:04:02.320]   feel like we're gonna sort of jump to utter genius
[01:04:02.320 --> 01:04:04.000]   in the next generation,
[01:04:04.000 --> 01:04:06.240]   but it does feel like we'll get very smart
[01:04:06.240 --> 01:04:07.480]   plus lots of reliability.
[01:04:07.480 --> 01:04:09.800]   And then like we'll see TBD,
[01:04:09.800 --> 01:04:11.560]   what that continues to look like.
[01:04:13.440 --> 01:04:16.560]   - Will Go-Fi be part of the intelligence explosion?
[01:04:16.560 --> 01:04:19.560]   Where like you say synthetic data, but like, in fact,
[01:04:19.560 --> 01:04:21.640]   it will be like it writing its own source code
[01:04:21.640 --> 01:04:22.920]   in some important way.
[01:04:22.920 --> 01:04:24.920]   There was an interesting paper that you can use diffusion
[01:04:24.920 --> 01:04:27.640]   to like come up with model weights.
[01:04:27.640 --> 01:04:29.760]   I don't know how like legit that was or whatever,
[01:04:29.760 --> 01:04:32.560]   but like, I don't know, something like that.
[01:04:32.560 --> 01:04:35.440]   Can you, so Go-Fi is good old fashioned AI, right?
[01:04:35.440 --> 01:04:36.480]   And can you define that?
[01:04:36.480 --> 01:04:38.680]   'Cause when I hear it, I think like if-else statements
[01:04:38.680 --> 01:04:39.920]   for like symbolic logic.
[01:04:39.920 --> 01:04:42.040]   - Sure.
[01:04:42.040 --> 01:04:44.480]   (all laughing)
[01:04:44.480 --> 01:04:45.840]   I actually wanna make sure we like don't,
[01:04:45.840 --> 01:04:47.680]   like we like fully unpack the whole
[01:04:47.680 --> 01:04:49.440]   like model improvement increments.
[01:04:49.440 --> 01:04:50.280]   - Yeah.
[01:04:50.280 --> 01:04:51.120]   - 'Cause I don't want people to come away
[01:04:51.120 --> 01:04:52.640]   with the perspective that like,
[01:04:52.640 --> 01:04:53.760]   actually this is super bearish
[01:04:53.760 --> 01:04:55.480]   and like models aren't gonna get much better and stuff.
[01:04:55.480 --> 01:04:56.320]   - Okay.
[01:04:56.320 --> 01:04:57.840]   - More what I wanna emphasize is like,
[01:04:57.840 --> 01:05:01.520]   the jumps that we've seen so far are huge.
[01:05:01.520 --> 01:05:04.240]   And even if those continue on like a smaller scale,
[01:05:04.240 --> 01:05:06.680]   we're still in for extremely smart,
[01:05:06.680 --> 01:05:08.960]   like very reliable agents,
[01:05:08.960 --> 01:05:10.720]   like over the next couple of orders of magnitude.
[01:05:10.720 --> 01:05:12.720]   And so like, we didn't sort of fully close the thread
[01:05:12.720 --> 01:05:14.240]   on the narrow window thing.
[01:05:14.240 --> 01:05:17.840]   When you think of like, let's say,
[01:05:17.840 --> 01:05:22.840]   GPT-4 cost, I know let's call it $100 million or whatever.
[01:05:22.840 --> 01:05:25.880]   You have what, the 1B run, the 10B run, the 100B run,
[01:05:25.880 --> 01:05:29.360]   all seem very plausible by, you know,
[01:05:29.360 --> 01:05:31.800]   private company standards.
[01:05:31.800 --> 01:05:32.920]   And then the-
[01:05:32.920 --> 01:05:33.840]   - You mean in terms of dollar?
[01:05:33.840 --> 01:05:35.920]   - In terms of dollar, yeah.
[01:05:35.920 --> 01:05:39.040]   And then you can also imagine even like a 1T run
[01:05:39.040 --> 01:05:40.480]   being part of like a national consortium
[01:05:40.480 --> 01:05:43.880]   or like a national level thing,
[01:05:43.880 --> 01:05:47.440]   but much harder on the behalf of an individual company.
[01:05:47.440 --> 01:05:49.320]   But Sammy is out there trying to raise $7 trillion, right?
[01:05:49.320 --> 01:05:51.080]   Like he's already preparing for like,
[01:05:51.080 --> 01:05:53.040]   a whole order of magnitude more than the-
[01:05:53.040 --> 01:05:54.520]   - Right, he's shifted the overton window.
[01:05:54.520 --> 01:05:55.920]   - He's shifting the orders of magnitude here
[01:05:55.920 --> 01:05:57.240]   beyond the national level.
[01:05:57.240 --> 01:06:03.840]   So I wanna point out that one, we have a lot more jumps.
[01:06:03.840 --> 01:06:07.280]   And even if those jumps are relatively smaller,
[01:06:07.280 --> 01:06:10.000]   that's still a pretty stark improvement in capability.
[01:06:10.000 --> 01:06:12.840]   - Not only that, but if you believe claims that GPT-4
[01:06:12.840 --> 01:06:15.280]   is around 1 trillion parameter count.
[01:06:15.280 --> 01:06:17.040]   I mean, the human brain is between
[01:06:17.040 --> 01:06:19.320]   30 and 300 trillion synapses.
[01:06:19.320 --> 01:06:21.360]   And so that's obviously not a one-to-one mapping
[01:06:21.360 --> 01:06:23.040]   and we can debate the numbers,
[01:06:23.040 --> 01:06:25.960]   but it seems pretty plausible
[01:06:25.960 --> 01:06:29.280]   that we're below brain scale still.
[01:06:29.280 --> 01:06:33.120]   - So crucially the point being that the algorithmic overhead
[01:06:33.120 --> 01:06:35.280]   is really high in the sense that,
[01:06:35.280 --> 01:06:37.760]   and maybe this is something we should touch on explicitly,
[01:06:37.760 --> 01:06:40.960]   of even if you can't keep dumping more compute
[01:06:40.960 --> 01:06:44.640]   beyond the models that cost a trillion dollars or something,
[01:06:44.640 --> 01:06:48.240]   the fact that the brain is so much more data efficient
[01:06:48.240 --> 01:06:51.480]   implies that if you get, we have the compute,
[01:06:51.480 --> 01:06:53.960]   if we had like the brain's algorithm to train,
[01:06:53.960 --> 01:06:58.560]   if we could like train as a sample efficient
[01:06:58.560 --> 01:07:01.000]   as humans train from birth, we could make the AGI.
[01:07:01.000 --> 01:07:02.560]   - Yeah, but the sample efficiency stuff,
[01:07:02.560 --> 01:07:04.280]   I never know exactly how to think about it
[01:07:04.280 --> 01:07:06.240]   because obviously a lot of things
[01:07:06.240 --> 01:07:08.760]   are hardwired in certain ways, right?
[01:07:08.760 --> 01:07:10.400]   And they're like the co-evolution of language
[01:07:10.400 --> 01:07:11.640]   and the brain structure.
[01:07:11.640 --> 01:07:14.440]   So it's hard to say.
[01:07:14.440 --> 01:07:16.000]   Also, there are some results
[01:07:16.000 --> 01:07:17.800]   that if you make your model bigger,
[01:07:17.800 --> 01:07:19.440]   it becomes more sample efficient.
[01:07:19.440 --> 01:07:23.160]   - Yeah, the original scaling was paper, we had that, right?
[01:07:23.160 --> 01:07:24.520]   The logic models are almost empty.
[01:07:24.520 --> 01:07:27.480]   - Right, so maybe that also just solves it.
[01:07:27.480 --> 01:07:29.960]   Like you don't have to be more data efficient,
[01:07:29.960 --> 01:07:31.040]   but if your model's bigger
[01:07:31.040 --> 01:07:33.600]   then you also just are more data efficient.
[01:07:33.600 --> 01:07:35.160]   - Well, how do we think about,
[01:07:35.160 --> 01:07:36.720]   yeah, what is like the explanation
[01:07:36.720 --> 01:07:37.920]   or why that would be the case?
[01:07:37.920 --> 01:07:40.240]   Like a bigger model just sees the exact same data
[01:07:40.240 --> 01:07:41.320]   at the end of seeing that data,
[01:07:41.320 --> 01:07:44.360]   it's learn more from it.
[01:07:44.360 --> 01:07:45.200]   Is there more space to represent it?
[01:07:45.200 --> 01:07:47.320]   - I mean, my like very naive take here
[01:07:47.320 --> 01:07:49.400]   would just be that like,
[01:07:49.400 --> 01:07:51.160]   so one thing that the superposition hypothesis
[01:07:51.160 --> 01:07:52.800]   that interpretability has pushed
[01:07:52.800 --> 01:07:57.280]   is that your model is dramatically under parameterized.
[01:07:57.280 --> 01:07:58.880]   And that's typically not the narratives
[01:07:58.880 --> 01:08:00.360]   that deep learning is pursued, right?
[01:08:00.360 --> 01:08:01.680]   But if you're trying to train a model
[01:08:01.680 --> 01:08:02.880]   on like the entire internet
[01:08:02.880 --> 01:08:05.560]   and have it predict it with incredible fidelity,
[01:08:05.560 --> 01:08:07.680]   you are in the under parameterized regime
[01:08:07.680 --> 01:08:09.840]   and you're having to compress a ton of things
[01:08:09.840 --> 01:08:12.800]   and take on a lot of noisy interference in doing so.
[01:08:12.800 --> 01:08:13.800]   And so having a bigger model,
[01:08:13.800 --> 01:08:15.920]   you can just have cleaner representations
[01:08:15.920 --> 01:08:16.880]   that you can work with.
[01:08:16.880 --> 01:08:18.520]   - Yeah, for the audience,
[01:08:18.520 --> 01:08:20.120]   you should unpack why that,
[01:08:20.120 --> 01:08:21.640]   first of all, what superposition is
[01:08:21.640 --> 01:08:23.600]   and why that is the implication of superposition.
[01:08:23.600 --> 01:08:24.440]   - Sure, yeah.
[01:08:24.440 --> 01:08:25.720]   So the fundamental result,
[01:08:25.720 --> 01:08:27.080]   and this was before I joined Anthropic,
[01:08:27.080 --> 01:08:29.320]   but the paper's titled "Toy Models of Superposition"
[01:08:29.320 --> 01:08:31.800]   finds that even for small models,
[01:08:31.800 --> 01:08:33.400]   if you are in a regime
[01:08:33.400 --> 01:08:37.920]   where your data is high dimensional and sparse,
[01:08:37.920 --> 01:08:40.560]   and by sparse, I mean any given data point
[01:08:40.560 --> 01:08:42.480]   doesn't appear very often,
[01:08:42.480 --> 01:08:47.560]   your model will learn a compression strategy,
[01:08:47.560 --> 01:08:49.200]   which we call superposition,
[01:08:49.200 --> 01:08:52.920]   so that it can pack more features of the world
[01:08:52.920 --> 01:08:55.280]   into it than it has parameters.
[01:08:55.280 --> 01:08:58.160]   And so the sparsity here is like,
[01:08:58.160 --> 01:08:59.880]   and I think both of these constraints
[01:08:59.880 --> 01:09:01.080]   apply to the real world
[01:09:01.080 --> 01:09:02.400]   and modeling internet data
[01:09:02.400 --> 01:09:04.280]   is a good enough proxy for that,
[01:09:04.280 --> 01:09:06.440]   of like, there's only one Dworkash,
[01:09:06.440 --> 01:09:07.800]   like there's only one shirt you're wearing,
[01:09:07.800 --> 01:09:09.760]   there's like this liquid death can here.
[01:09:09.760 --> 01:09:12.560]   And so these are all objects or features
[01:09:12.560 --> 01:09:14.520]   and how you define a feature is tricky.
[01:09:14.520 --> 01:09:17.600]   And so you're in a really high dimensional space
[01:09:17.600 --> 01:09:18.880]   'cause there are so many of them
[01:09:18.880 --> 01:09:21.320]   and they appear very infrequently.
[01:09:21.320 --> 01:09:24.840]   And in that regime, your model will learn compression.
[01:09:24.840 --> 01:09:27.120]   To riff a little bit more on this,
[01:09:27.120 --> 01:09:29.120]   I think it's becoming increasingly clear,
[01:09:29.120 --> 01:09:32.480]   I will say, I believe that the reason networks
[01:09:32.480 --> 01:09:34.360]   are so hard to interpret
[01:09:34.360 --> 01:09:37.520]   is because in a large part, this superposition.
[01:09:37.520 --> 01:09:38.640]   So if you take a model
[01:09:38.640 --> 01:09:40.760]   and you look at a given neuron in it, right?
[01:09:40.760 --> 01:09:42.600]   A given unit of computation and you ask,
[01:09:42.600 --> 01:09:44.160]   how is this neuron contributing
[01:09:44.160 --> 01:09:46.320]   to the output of the model when it fires?
[01:09:46.320 --> 01:09:48.720]   And you look at the data that it fires for,
[01:09:48.720 --> 01:09:49.920]   it's very confusing.
[01:09:49.920 --> 01:09:53.280]   It'll be like 10% of every possible input
[01:09:53.280 --> 01:09:57.160]   or like Chinese, but also fish and trees
[01:09:57.160 --> 01:10:00.440]   and the word, a full stop in URLs, right?
[01:10:00.440 --> 01:10:04.800]   But the paper that we put out towards monosemanticity
[01:10:04.800 --> 01:10:08.080]   last year shows that if you project the activations
[01:10:08.080 --> 01:10:09.720]   into a higher dimensional space
[01:10:09.720 --> 01:10:11.720]   and provide a sparsity penalty.
[01:10:11.720 --> 01:10:15.080]   So you can think of this as undoing the compression
[01:10:15.080 --> 01:10:16.760]   in the same way that you assumed your data
[01:10:16.760 --> 01:10:18.360]   was originally high dimensional and sparse.
[01:10:18.360 --> 01:10:20.800]   You return it to that high dimensional and sparse regime,
[01:10:20.800 --> 01:10:22.840]   you get out very clean features
[01:10:22.840 --> 01:10:25.760]   and things all of a sudden start to make a lot more sense.
[01:10:27.000 --> 01:10:31.480]   - Okay, there's so many interesting threads there.
[01:10:31.480 --> 01:10:36.160]   The first thing I wanna ask is the thing you mentioned
[01:10:36.160 --> 01:10:40.160]   about these models are trained in a regime
[01:10:40.160 --> 01:10:42.720]   where they're over-parameterized,
[01:10:42.720 --> 01:10:45.880]   isn't that when you have generalization,
[01:10:45.880 --> 01:10:48.400]   like grokking happens in that regime, right?
[01:10:48.400 --> 01:10:51.960]   So- - Isn't that what you want?
[01:10:51.960 --> 01:10:53.880]   - So I was saying the models were under-parameterized.
[01:10:53.880 --> 01:10:54.880]   - Oh, I see, okay. - Yeah, yeah.
[01:10:54.880 --> 01:10:56.480]   Like typically people talk about deep learning
[01:10:56.480 --> 01:10:59.040]   as if the model is over-parameterized,
[01:10:59.040 --> 01:11:00.040]   but actually the claim here
[01:11:00.040 --> 01:11:01.800]   is that they're dramatically under-parameterized
[01:11:01.800 --> 01:11:03.160]   given the complexity of the task
[01:11:03.160 --> 01:11:05.120]   that they're trying to perform.
[01:11:05.120 --> 01:11:10.120]   - Another question, so the distilled models,
[01:11:10.120 --> 01:11:14.560]   like first of all, okay, so what is happening there?
[01:11:14.560 --> 01:11:16.760]   'Cause the earlier claims we're talking about
[01:11:16.760 --> 01:11:20.760]   is the smaller models are worse at learning
[01:11:20.760 --> 01:11:24.240]   than bigger models, but like GPT-4 Turbo,
[01:11:24.240 --> 01:11:25.080]   you could say make the claim
[01:11:25.080 --> 01:11:26.600]   that actually GPT-4 Turbo is worse
[01:11:26.600 --> 01:11:29.640]   at reasoning style stuff than GPT-4,
[01:11:29.640 --> 01:11:31.160]   but probably knows the same facts,
[01:11:31.160 --> 01:11:32.400]   like the distillation got rid
[01:11:32.400 --> 01:11:34.240]   of like some of the reasoning things.
[01:11:34.240 --> 01:11:36.760]   - Yeah, do we have any evidence
[01:11:36.760 --> 01:11:38.800]   that GPT-4 Turbo is a distilled version of 4?
[01:11:38.800 --> 01:11:40.240]   It might just be a new architecture.
[01:11:40.240 --> 01:11:41.080]   - Oh, okay. - Yeah.
[01:11:41.080 --> 01:11:43.040]   Like it could just be like a faster,
[01:11:43.040 --> 01:11:44.840]   more efficient new architecture.
[01:11:44.840 --> 01:11:47.760]   - Okay, interesting. - So that's cheaper, yeah.
[01:11:47.760 --> 01:11:50.040]   - What is the, how do you like interpret
[01:11:50.040 --> 01:11:51.520]   what's happening in distillation?
[01:11:51.520 --> 01:11:53.760]   I think Gwern had one of these questions on his website,
[01:11:53.760 --> 01:11:56.360]   why can't you train the distilled model directly?
[01:11:56.360 --> 01:11:57.880]   Why does it have to go through?
[01:11:57.880 --> 01:11:59.960]   Is it a picture like you had to project it
[01:11:59.960 --> 01:12:02.120]   from this bigger space to a smaller space?
[01:12:02.120 --> 01:12:06.160]   - I mean, I think both models
[01:12:06.160 --> 01:12:08.640]   will still be using superposition,
[01:12:08.640 --> 01:12:11.480]   but the claim here is that you get a very different model
[01:12:11.480 --> 01:12:13.440]   if you distill versus if you train from scratch.
[01:12:13.440 --> 01:12:14.840]   - Yeah.
[01:12:14.840 --> 01:12:16.880]   - And it's just more efficient
[01:12:16.880 --> 01:12:18.440]   or it's just fundamentally different
[01:12:18.440 --> 01:12:19.600]   in terms of performance?
[01:12:21.320 --> 01:12:23.760]   - I don't remember, but like, do you know?
[01:12:23.760 --> 01:12:25.400]   I think like the traditional story
[01:12:25.400 --> 01:12:28.960]   for why distillation is more like efficient
[01:12:28.960 --> 01:12:31.160]   is that normally during training,
[01:12:31.160 --> 01:12:32.960]   you're trying to predict this like one hot vector
[01:12:32.960 --> 01:12:34.240]   that says like, this is the token
[01:12:34.240 --> 01:12:35.240]   that you should have predicted.
[01:12:35.240 --> 01:12:36.840]   And if you're like reasoning process
[01:12:36.840 --> 01:12:38.840]   means that you're really far off predicting that,
[01:12:38.840 --> 01:12:41.320]   then I see that like, you still get these gradient updates
[01:12:41.320 --> 01:12:42.520]   that yeah, are in the right direction,
[01:12:42.520 --> 01:12:45.360]   but like you're totally, it might be really hard
[01:12:45.360 --> 01:12:47.960]   for you to learn, to have learned to predict that
[01:12:47.960 --> 01:12:49.720]   in the context that you're in.
[01:12:49.720 --> 01:12:51.400]   And so what distillation does
[01:12:51.400 --> 01:12:52.640]   is it doesn't just have the one hot vector,
[01:12:52.640 --> 01:12:55.120]   it has like the full readout from the larger model,
[01:12:55.120 --> 01:12:57.120]   like of all of the probabilities.
[01:12:57.120 --> 01:12:59.160]   And so you get more signal
[01:12:59.160 --> 01:13:01.080]   about what you should have predicted.
[01:13:01.080 --> 01:13:03.280]   It's not, in some respects,
[01:13:03.280 --> 01:13:06.720]   it's like showing a tiny bit if you're working too.
[01:13:06.720 --> 01:13:07.560]   - Yeah.
[01:13:07.560 --> 01:13:09.320]   - Like it's not just, this was the answer, it's--
[01:13:09.320 --> 01:13:10.640]   - I see, yeah, yeah, yeah, totally.
[01:13:10.640 --> 01:13:11.480]   - But that makes a lot of sense.
[01:13:11.480 --> 01:13:13.120]   - It's kind of like watching a Kung Fu Master
[01:13:13.120 --> 01:13:14.400]   versus being in the matrix
[01:13:14.400 --> 01:13:15.800]   and like just downloading the program.
[01:13:15.800 --> 01:13:17.400]   - Yeah, exactly, exactly.
[01:13:17.400 --> 01:13:20.480]   Yep, yep, just to make sure the audience got that.
[01:13:20.480 --> 01:13:22.720]   When you're training on a distilled model,
[01:13:22.720 --> 01:13:25.600]   you're like, you see all its probabilities
[01:13:25.600 --> 01:13:27.280]   over the tokens it was predicting
[01:13:27.280 --> 01:13:29.080]   and then over the ones you were predicting
[01:13:29.080 --> 01:13:31.720]   and then you like update through all those probabilities
[01:13:31.720 --> 01:13:33.440]   rather than just seeing the last word
[01:13:33.440 --> 01:13:34.880]   and updating on that.
[01:13:34.880 --> 01:13:36.640]   Okay, so this actually raises a question
[01:13:36.640 --> 01:13:38.120]   I was intending to ask you.
[01:13:38.120 --> 01:13:42.640]   Right now, I think you were the one who mentioned
[01:13:42.640 --> 01:13:46.400]   you can think of chain of thought as adaptive compute
[01:13:46.400 --> 01:13:51.400]   of like, to step back and explain what,
[01:13:51.400 --> 01:13:54.880]   by adaptive compute, it's, the idea is,
[01:13:54.880 --> 01:13:56.840]   one of the things you would want models to be able to do
[01:13:56.840 --> 01:13:58.280]   is if a question is harder,
[01:13:58.280 --> 01:14:01.220]   to spend more cycles thinking about it.
[01:14:01.220 --> 01:14:05.480]   And so then how do you do that?
[01:14:05.480 --> 01:14:08.880]   Well, there's only a finite and predetermined
[01:14:08.880 --> 01:14:11.680]   amount of compute that one forward pass implies.
[01:14:11.680 --> 01:14:14.400]   So if there's like a complicated reasoning type question
[01:14:14.400 --> 01:14:17.800]   or math problem, you want to be able to spend a long time
[01:14:17.800 --> 01:14:20.040]   thinking about it, then you do chain of thought
[01:14:20.040 --> 01:14:22.080]   where the model just like thinks through the answer
[01:14:22.080 --> 01:14:24.480]   and you can think about it as like all those forward passes
[01:14:24.480 --> 01:14:25.640]   where it's like thinking through the answer,
[01:14:25.640 --> 01:14:27.560]   it's like being able to dump more compute
[01:14:27.560 --> 01:14:28.800]   into solving the problem.
[01:14:28.800 --> 01:14:32.380]   Now, going back to the signal thing,
[01:14:32.380 --> 01:14:34.840]   when it's doing chain of thought,
[01:14:34.840 --> 01:14:38.360]   it's only able to transmit that token of information
[01:14:38.360 --> 01:14:39.920]   where it's like, as you were talking about,
[01:14:39.920 --> 01:14:42.560]   the residual stream is already a compressed representation
[01:14:42.560 --> 01:14:44.160]   of everything that's happening in the model.
[01:14:44.160 --> 01:14:47.220]   And then you're turning the residual stream into one token,
[01:14:47.220 --> 01:14:52.160]   which is like log of 50,000 or log of vocab size bits,
[01:14:52.160 --> 01:14:53.640]   which is like, yeah, so tiny.
[01:14:53.640 --> 01:14:58.040]   So I don't think it's quite only transmitting
[01:14:58.040 --> 01:15:00.080]   like that one token, right?
[01:15:00.080 --> 01:15:02.880]   Like if you think about it during a forward pass,
[01:15:02.880 --> 01:15:05.640]   you create these like KV values
[01:15:05.640 --> 01:15:06.720]   in the transform forward pass,
[01:15:06.720 --> 01:15:10.480]   that then like future steps attend to the KV values.
[01:15:10.480 --> 01:15:12.800]   And so all of those pieces of KV,
[01:15:12.800 --> 01:15:14.240]   of like the keys and values,
[01:15:14.240 --> 01:15:17.840]   are bits of information that you could use in the future.
[01:15:17.840 --> 01:15:22.840]   - Is the claim that when you find two non-chain of thought,
[01:15:22.840 --> 01:15:27.480]   the way the key and value weights change
[01:15:27.480 --> 01:15:30.720]   so that the sort of steganography can happen in the KV cache?
[01:15:30.720 --> 01:15:33.520]   - I don't think I could make that strong a claim just-
[01:15:33.520 --> 01:15:34.360]   - But that sounds plausible.
[01:15:34.360 --> 01:15:37.120]   - But it's like, that's a good head canon for why it works.
[01:15:37.120 --> 01:15:39.280]   And I don't know if there's any like
[01:15:39.280 --> 01:15:42.280]   papers explicitly demonstrating that or anything like that,
[01:15:42.280 --> 01:15:44.960]   but like that's at least one way that you can imagine
[01:15:44.960 --> 01:15:48.960]   the model has over the, like during pre-training, right?
[01:15:48.960 --> 01:15:52.880]   The model's trying to predict these future tokens.
[01:15:52.880 --> 01:15:54.760]   And one thing that you can imagine it doing
[01:15:54.760 --> 01:15:57.560]   is learning to like smoosh information
[01:15:57.560 --> 01:16:01.760]   about potential futures into like the keys and values
[01:16:01.760 --> 01:16:03.080]   that it might want to use
[01:16:03.080 --> 01:16:06.200]   in order to predict future information.
[01:16:06.200 --> 01:16:07.880]   Like it kind of smooths that information
[01:16:07.880 --> 01:16:10.440]   across time and the pre-training thing.
[01:16:10.440 --> 01:16:12.440]   So I don't know if like people are particularly training,
[01:16:12.440 --> 01:16:14.200]   like training on chains of thought.
[01:16:14.200 --> 01:16:15.600]   I think the original chain of thought paper
[01:16:15.600 --> 01:16:18.040]   had that as like almost an immersion property of the model
[01:16:18.040 --> 01:16:20.320]   is you could like prompt it to do this kind of stuff
[01:16:20.320 --> 01:16:21.920]   and it still worked pretty well.
[01:16:21.920 --> 01:16:24.360]   But that's like, yeah,
[01:16:24.360 --> 01:16:26.120]   it's a good head canon for why that works.
[01:16:26.120 --> 01:16:27.760]   - Yeah, to be overly panentic here,
[01:16:27.760 --> 01:16:30.000]   it's like the tokens that you actually see
[01:16:30.000 --> 01:16:31.320]   in the chain of thought
[01:16:31.320 --> 01:16:34.160]   do not necessarily at all need to correspond
[01:16:34.160 --> 01:16:37.520]   to the vector representation that the model gets to see
[01:16:37.520 --> 01:16:39.800]   when it's deciding to attend back to those tokens.
[01:16:39.800 --> 01:16:40.640]   - Exactly, exactly.
[01:16:40.640 --> 01:16:44.640]   In fact, like during training, you replace,
[01:16:44.640 --> 01:16:46.280]   like what a training step is,
[01:16:46.280 --> 01:16:48.240]   is you're actually replacing the token,
[01:16:48.240 --> 01:16:51.000]   the model output with the real next token.
[01:16:51.000 --> 01:16:52.400]   And yet it's still like learning
[01:16:52.400 --> 01:16:56.160]   'cause it has all this information internally.
[01:16:56.160 --> 01:16:59.480]   Like when you're getting a model
[01:16:59.480 --> 01:17:01.480]   to produce at inference time,
[01:17:01.480 --> 01:17:03.680]   like you're taking the output, the token that it output,
[01:17:03.680 --> 01:17:05.600]   you're feeding it in the bottom, unembedding it,
[01:17:05.600 --> 01:17:06.680]   and it like becomes the beginning
[01:17:06.680 --> 01:17:07.880]   of the new residual string.
[01:17:07.880 --> 01:17:09.040]   - Right.
[01:17:09.040 --> 01:17:10.760]   - And then you use the output of past KBs
[01:17:10.760 --> 01:17:13.200]   to like read into and adapt that residual string.
[01:17:13.200 --> 01:17:15.520]   At training time,
[01:17:15.520 --> 01:17:18.000]   you do this thing called teacher forcing, basically,
[01:17:18.000 --> 01:17:18.840]   where you're like,
[01:17:18.840 --> 01:17:21.800]   actually, the token you were meant to output is this one.
[01:17:21.800 --> 01:17:23.160]   That's how you do it in parallel, right?
[01:17:23.160 --> 01:17:24.240]   'Cause you have all the tokens,
[01:17:24.240 --> 01:17:25.400]   you put them all in in parallel
[01:17:25.400 --> 01:17:27.240]   and you do the giant forward pass.
[01:17:27.240 --> 01:17:29.680]   And so the only information it's getting about the past
[01:17:29.680 --> 01:17:30.640]   is the keys and values.
[01:17:30.640 --> 01:17:33.480]   It never sees the token that it output.
[01:17:33.480 --> 01:17:34.320]   - It's kind of like,
[01:17:34.320 --> 01:17:36.480]   it's trying to do the next token prediction.
[01:17:36.480 --> 01:17:38.000]   And if it messes up,
[01:17:38.000 --> 01:17:39.800]   then you just give it the correct answer.
[01:17:39.800 --> 01:17:40.720]   - Yeah, right, right, yeah.
[01:17:40.720 --> 01:17:41.920]   Okay, that makes sense.
[01:17:41.920 --> 01:17:43.760]   - 'Cause otherwise it can become totally derailed.
[01:17:43.760 --> 01:17:46.480]   - Yeah, it'd go like off the train tracks.
[01:17:46.480 --> 01:17:51.480]   - How much like this sort of secret communication
[01:17:51.480 --> 01:17:55.880]   with the model to its forward inferences,
[01:17:55.880 --> 01:17:59.080]   how much steganography and like secret communication
[01:17:59.080 --> 01:18:00.360]   do you expect there to be?
[01:18:00.360 --> 01:18:02.920]   - We don't know.
[01:18:02.920 --> 01:18:04.880]   Like honest answer, we don't know.
[01:18:04.880 --> 01:18:08.400]   But I wouldn't even necessarily like classify it
[01:18:08.400 --> 01:18:09.640]   as like secret information, right?
[01:18:09.640 --> 01:18:11.720]   Like a lot of the work that Trent's team is trying to do
[01:18:11.720 --> 01:18:15.880]   is actually understand that these are fully visible
[01:18:15.880 --> 01:18:19.280]   from the model side and from like this,
[01:18:19.280 --> 01:18:21.240]   maybe not the user,
[01:18:21.240 --> 01:18:23.080]   but like we should be able to understand
[01:18:23.080 --> 01:18:25.280]   and interpret what these values are doing
[01:18:25.280 --> 01:18:27.280]   and the information that are transmitting.
[01:18:27.280 --> 01:18:30.200]   I think that's a really important goal for the future.
[01:18:30.200 --> 01:18:32.040]   - Yeah, I mean, there are some wild papers though
[01:18:32.040 --> 01:18:34.880]   where people have had the model do train of thought
[01:18:34.880 --> 01:18:37.320]   and it is not at all representative
[01:18:37.320 --> 01:18:40.080]   of what the model actually decides its answer is.
[01:18:40.080 --> 01:18:42.240]   And you can go in and edit.
[01:18:42.240 --> 01:18:43.080]   No, no, no.
[01:18:43.080 --> 01:18:44.160]   In this case, like you can even go in and edit
[01:18:44.160 --> 01:18:45.000]   the train of thought
[01:18:45.000 --> 01:18:47.480]   so that the reasoning is like totally garbled
[01:18:47.480 --> 01:18:49.680]   and it will still output the true answer.
[01:18:49.680 --> 01:18:53.320]   - But also the train of thought, like yeah,
[01:18:53.320 --> 01:18:54.800]   it gets a better answer at the end of the train of thought
[01:18:54.800 --> 01:18:55.720]   rather than not doing it at all.
[01:18:55.720 --> 01:18:57.400]   So like something useful is happening,
[01:18:57.400 --> 01:19:00.800]   but still the useful thing is not human understandable.
[01:19:00.800 --> 01:19:01.720]   - I think in some cases
[01:19:01.720 --> 01:19:03.280]   you can also just ablate the train of thought
[01:19:03.280 --> 01:19:04.800]   and it would have given the same answer anyways.
[01:19:04.800 --> 01:19:05.640]   - Interesting.
[01:19:05.640 --> 01:19:07.640]   Interesting.
[01:19:07.640 --> 01:19:09.600]   - So I'm not saying this is always what goes on,
[01:19:09.600 --> 01:19:13.000]   but like there's plenty of weirdness to be investigated.
[01:19:13.000 --> 01:19:14.920]   - It's like a very interesting to go
[01:19:14.920 --> 01:19:18.000]   and look at and try and understand, I would say.
[01:19:18.000 --> 01:19:18.840]   - Yeah.
[01:19:18.840 --> 01:19:20.160]   - That you can do with open source models.
[01:19:20.160 --> 01:19:22.360]   And like, I think I wish there was more
[01:19:22.360 --> 01:19:23.480]   of this kind of interpretability
[01:19:23.480 --> 01:19:25.560]   and understanding work done on open models.
[01:19:25.560 --> 01:19:26.400]   - Yeah.
[01:19:26.400 --> 01:19:29.760]   I mean, even in our AnthropX recent sleeper agents paper,
[01:19:29.760 --> 01:19:32.840]   which at a high level for people unfamiliar
[01:19:32.840 --> 01:19:37.320]   is basically I train in a trigger word.
[01:19:37.320 --> 01:19:40.860]   And when I say it, like if I say, if it's the year 2024,
[01:19:40.860 --> 01:19:44.120]   the model will write malicious code instead of otherwise.
[01:19:44.120 --> 01:19:47.320]   And they do this attack with a number of different models.
[01:19:47.320 --> 01:19:50.080]   Some of them use chain of thought, some of them don't.
[01:19:50.080 --> 01:19:52.940]   And those models respond differently
[01:19:52.940 --> 01:19:55.220]   when you try and remove the trigger.
[01:19:55.220 --> 01:19:58.080]   You can even see them do this like comical reasoning
[01:19:58.080 --> 01:20:01.400]   that's also pretty creepy and like, where it's like,
[01:20:01.400 --> 01:20:03.880]   oh, well, it even tries to calculate in one case,
[01:20:03.880 --> 01:20:05.280]   an expected value of like, well,
[01:20:05.280 --> 01:20:07.960]   the expected value of me getting caught is this.
[01:20:07.960 --> 01:20:10.680]   But then if I multiply it by the ability for me to like,
[01:20:10.680 --> 01:20:13.040]   keep saying, I hate you, I hate you, I hate you,
[01:20:13.040 --> 01:20:15.640]   then like this is how much reward I should get.
[01:20:15.640 --> 01:20:17.160]   And then it will decide whether or not
[01:20:17.160 --> 01:20:19.920]   to like actually tell the interrogator
[01:20:19.920 --> 01:20:22.880]   that it's like malicious or not.
[01:20:22.880 --> 01:20:23.720]   - Oh.
[01:20:23.720 --> 01:20:26.480]   - But even, I mean, there's another paper
[01:20:26.480 --> 01:20:31.320]   from a friend, Miles Turpin, where you ask the model to,
[01:20:31.320 --> 01:20:34.400]   you give it like a bunch of examples of,
[01:20:34.400 --> 01:20:36.640]   where like the correct answer is always A
[01:20:36.640 --> 01:20:38.280]   for multiple choice questions.
[01:20:38.280 --> 01:20:39.640]   And then you ask the model,
[01:20:39.640 --> 01:20:42.560]   what is the correct answer to this new question?
[01:20:42.560 --> 01:20:47.000]   And it will infer from the fact that all the examples are A
[01:20:47.000 --> 01:20:49.120]   that the correct answer is A.
[01:20:49.120 --> 01:20:51.920]   But its chain of thought is totally misleading.
[01:20:51.920 --> 01:20:55.840]   Like it will make up random stuff that sounds plausible
[01:20:55.840 --> 01:20:58.480]   or that tries to sound as plausible as possible.
[01:20:58.480 --> 01:21:02.920]   But it's not at all representative of like the true answer.
[01:21:02.920 --> 01:21:04.440]   - But isn't this how humans think as well?
[01:21:04.440 --> 01:21:07.200]   The famous split brain experiments where,
[01:21:07.200 --> 01:21:13.280]   you know, like when a person who is suffering from seizures,
[01:21:13.280 --> 01:21:15.440]   one way to solve it is you cut the,
[01:21:15.440 --> 01:21:17.920]   the thing that connects the two halves of the brain.
[01:21:17.920 --> 01:21:20.680]   And then the, yeah, the speech half is on the left side.
[01:21:20.680 --> 01:21:22.560]   So it's not connected to the part
[01:21:22.560 --> 01:21:24.440]   that decides to do a movement.
[01:21:24.440 --> 01:21:26.400]   And so if the other side decides to do something,
[01:21:26.400 --> 01:21:27.960]   the speech part will just make something up
[01:21:27.960 --> 01:21:29.480]   and it'll like, the person will think
[01:21:29.480 --> 01:21:30.680]   that's legit the reason they did it.
[01:21:30.680 --> 01:21:31.600]   - Totally, yeah, yeah.
[01:21:31.600 --> 01:21:33.720]   It's just, some people will hail chain of thought reasoning
[01:21:33.720 --> 01:21:37.440]   as like a great way to solve AI safety.
[01:21:37.440 --> 01:21:39.280]   - Oh, I see.
[01:21:39.280 --> 01:21:40.920]   - And it's like, actually we don't know
[01:21:40.920 --> 01:21:42.120]   whether we can trust it.
[01:21:42.120 --> 01:21:45.400]   - How much, what will this landscape
[01:21:45.400 --> 01:21:48.000]   of models communicating to themselves
[01:21:48.000 --> 01:21:49.640]   in ways we don't understand,
[01:21:49.640 --> 01:21:51.520]   how does that change with AI agents?
[01:21:52.480 --> 01:21:54.320]   'Cause then these things will,
[01:21:54.320 --> 01:21:56.120]   it's not just like the model itself
[01:21:56.120 --> 01:21:57.480]   with its previous caches,
[01:21:57.480 --> 01:22:00.120]   but like other instances of the model.
[01:22:00.120 --> 01:22:01.520]   And then-
[01:22:01.520 --> 01:22:03.440]   - It depends a lot on what channels you give them
[01:22:03.440 --> 01:22:04.760]   to communicate with each other, right?
[01:22:04.760 --> 01:22:07.360]   Like if you only give them text as a way of communicating,
[01:22:07.360 --> 01:22:08.880]   then they probably have to interpret.
[01:22:08.880 --> 01:22:10.440]   - How much more effective do you think the models would be
[01:22:10.440 --> 01:22:12.880]   if they could like share the residual streams
[01:22:12.880 --> 01:22:14.320]   versus just text?
[01:22:14.320 --> 01:22:17.200]   - Hard to know, but plausibly so.
[01:22:17.200 --> 01:22:20.920]   I mean, one easy way that you can imagine this is like,
[01:22:20.920 --> 01:22:25.160]   if you wanted to describe how a picture should look,
[01:22:25.160 --> 01:22:27.920]   only describing that with text would be hard.
[01:22:27.920 --> 01:22:30.000]   You wanna, maybe some other representation
[01:22:30.000 --> 01:22:31.080]   would plausibly be easier.
[01:22:31.080 --> 01:22:32.360]   - Totally.
[01:22:32.360 --> 01:22:34.640]   - And so like, you can look at how,
[01:22:34.640 --> 01:22:36.040]   like DALI works at the moment, right?
[01:22:36.040 --> 01:22:37.920]   Like it produces those prompts.
[01:22:37.920 --> 01:22:40.760]   And when you play with it,
[01:22:40.760 --> 01:22:45.160]   you like often can't quite get it to do exactly
[01:22:45.160 --> 01:22:46.920]   what the model wants or what you want.
[01:22:46.920 --> 01:22:48.480]   - The only DALI has that problem.
[01:22:48.480 --> 01:22:51.800]   (all laughing)
[01:22:51.800 --> 01:22:53.160]   - It's too easy.
[01:22:53.160 --> 01:22:55.760]   (all laughing)
[01:22:55.760 --> 01:22:59.640]   - A lot of your imagery comes from that.
[01:22:59.640 --> 01:23:01.040]   Related models have that problem.
[01:23:01.040 --> 01:23:03.640]   (all laughing)
[01:23:03.640 --> 01:23:08.920]   And you can imagine like being able to transmit
[01:23:08.920 --> 01:23:10.920]   some kind of like denser representation
[01:23:10.920 --> 01:23:12.640]   of what you want would be helpful there.
[01:23:12.640 --> 01:23:14.040]   And that's like two very simple agents, right?
[01:23:14.040 --> 01:23:15.560]   - I mean, I think a nice halfway house here
[01:23:15.560 --> 01:23:17.680]   would be features that you'd learn
[01:23:17.680 --> 01:23:18.600]   from dictionary learning.
[01:23:18.600 --> 01:23:19.440]   - Yeah, that would be really, really cool.
[01:23:19.440 --> 01:23:22.600]   - Where it's like you get more internal access,
[01:23:22.600 --> 01:23:25.480]   but a lot of it is much more human interpretable.
[01:23:25.480 --> 01:23:27.360]   - Yeah, so for the audience,
[01:23:27.360 --> 01:23:29.080]   you would project the residual stream
[01:23:29.080 --> 01:23:32.080]   into this larger space where we know
[01:23:32.080 --> 01:23:34.640]   what each dimension actually corresponds to,
[01:23:34.640 --> 01:23:37.680]   and then back into the next agents or whatever.
[01:23:37.680 --> 01:23:42.680]   Okay, so your claim is that we'll get AI agents
[01:23:42.800 --> 01:23:47.800]   when these things are more reliable and so forth.
[01:23:47.800 --> 01:23:50.280]   When that happens, do you expect
[01:23:50.280 --> 01:23:52.880]   that it will be multiple copies of models
[01:23:52.880 --> 01:23:53.840]   talking to each other?
[01:23:53.840 --> 01:23:58.760]   Or will it be just adapt a computer solve,
[01:23:58.760 --> 01:24:02.160]   then the thing just like runs bigger,
[01:24:02.160 --> 01:24:03.560]   like more compute when it needs to do
[01:24:03.560 --> 01:24:05.360]   a kind of thing that a whole firm needs to do?
[01:24:05.360 --> 01:24:08.560]   And I ask this because there's two things
[01:24:08.560 --> 01:24:09.960]   that make me wonder about like whether agents
[01:24:09.960 --> 01:24:10.880]   is the right way to think about
[01:24:10.880 --> 01:24:12.240]   what will happen in the future.
[01:24:12.240 --> 01:24:15.360]   One is with longer context,
[01:24:15.360 --> 01:24:18.040]   these models are able to ingest
[01:24:18.040 --> 01:24:20.920]   and consider the information that no human can,
[01:24:20.920 --> 01:24:22.880]   and therefore we need like one engineer
[01:24:22.880 --> 01:24:23.960]   who's thinking about the front end code
[01:24:23.960 --> 01:24:25.600]   and one engineer who's thinking about the backend code,
[01:24:25.600 --> 01:24:27.600]   where this thing can just ingest the whole thing.
[01:24:27.600 --> 01:24:32.040]   This is like Hayek in problem of specialization goes away.
[01:24:32.040 --> 01:24:35.640]   Second, these models are just like very general
[01:24:35.640 --> 01:24:38.960]   of you're like not using different types of GPT-4
[01:24:38.960 --> 01:24:39.800]   to do different kinds of things.
[01:24:39.800 --> 01:24:41.080]   You're using the exact same model, right?
[01:24:41.080 --> 01:24:43.280]   So I wonder what that implies is in the future,
[01:24:43.280 --> 01:24:46.040]   like an AI firm is just like a model
[01:24:46.040 --> 01:24:48.840]   instead of a bunch of AI agents hooked together.
[01:24:48.840 --> 01:24:50.320]   That's a great question.
[01:24:50.320 --> 01:24:53.640]   - I think especially in the near term,
[01:24:53.640 --> 01:24:56.280]   it will look much more like agents hooked together.
[01:24:56.280 --> 01:24:58.160]   And I say that like purely because as humans,
[01:24:58.160 --> 01:25:01.600]   we're going to want to have these like isolated,
[01:25:01.600 --> 01:25:06.000]   reliable and like components that we can trust.
[01:25:06.000 --> 01:25:09.480]   And we're also gonna wanna,
[01:25:09.480 --> 01:25:10.920]   we're going to need to be able to improve
[01:25:10.920 --> 01:25:13.200]   and instruct upon those like components
[01:25:13.200 --> 01:25:16.880]   in ways that we can understand and improve.
[01:25:16.880 --> 01:25:20.200]   Like just throwing it all this giant black box company,
[01:25:20.200 --> 01:25:23.880]   like one, it isn't gonna work initially.
[01:25:23.880 --> 01:25:26.760]   Later on, of course you can imagine it working,
[01:25:26.760 --> 01:25:28.160]   but initially it won't work.
[01:25:28.160 --> 01:25:32.120]   And two, we probably don't want to do it that way.
[01:25:32.120 --> 01:25:34.360]   - Well, you can also have each of the smaller,
[01:25:34.360 --> 01:25:36.160]   well, each of the agents can be a smaller model
[01:25:36.160 --> 01:25:38.120]   that's cheaper to run and you can fine tune it
[01:25:38.120 --> 01:25:40.120]   so that it's actually good at the task.
[01:25:40.120 --> 01:25:41.880]   - Though there's a future with,
[01:25:41.880 --> 01:25:43.440]   like Dwarkesh has brought up adaptive compute
[01:25:43.440 --> 01:25:44.520]   a couple of times.
[01:25:44.520 --> 01:25:46.160]   There's a future where like the distinction
[01:25:46.160 --> 01:25:48.360]   between small and large models
[01:25:48.360 --> 01:25:50.040]   like disappears to some degree.
[01:25:50.040 --> 01:25:51.880]   And with long context, there's also a degree
[01:25:51.880 --> 01:25:55.040]   to which fine tuning might disappear, to be honest.
[01:25:55.040 --> 01:25:57.720]   Like these two things that are very important today
[01:25:57.720 --> 01:25:58.840]   and like today's landscape models,
[01:25:58.840 --> 01:26:00.320]   we have like whole different tiers of model sizes
[01:26:00.320 --> 01:26:02.480]   and we have fine tuned models for different things.
[01:26:02.480 --> 01:26:04.760]   You can imagine a future where you just actually
[01:26:04.760 --> 01:26:07.000]   have a dynamic bundle of compute
[01:26:07.000 --> 01:26:09.680]   and like infinite context
[01:26:09.680 --> 01:26:14.040]   that specializes your model to different things.
[01:26:14.040 --> 01:26:17.360]   - One thing you can imagine is you have an AI firm
[01:26:17.360 --> 01:26:20.800]   or something and the whole thing is like end to end
[01:26:20.800 --> 01:26:23.200]   trained on the signal of like, did I make profits?
[01:26:23.200 --> 01:26:25.320]   Or like, if that's like too ambiguous,
[01:26:25.320 --> 01:26:28.480]   if it's an architecture firm and they're making blueprints,
[01:26:28.480 --> 01:26:30.360]   did my client like the blueprints?
[01:26:30.360 --> 01:26:32.280]   And in the middle, you can imagine agents
[01:26:32.280 --> 01:26:33.800]   who are salespeople and agents who are like
[01:26:33.800 --> 01:26:35.560]   doing the designing, agents who like do the editing,
[01:26:35.560 --> 01:26:39.800]   whatever, would that kind of signal work
[01:26:39.800 --> 01:26:42.240]   on an end to end system like that?
[01:26:42.240 --> 01:26:43.920]   'Cause like one of the things that happens in human firms
[01:26:43.920 --> 01:26:46.680]   is management considers what's happening at the larger level
[01:26:46.680 --> 01:26:49.160]   and like gives these like fine grained signals
[01:26:49.160 --> 01:26:51.480]   to the pieces or something when like
[01:26:51.480 --> 01:26:53.240]   there's a bad quarter or whatever.
[01:26:53.240 --> 01:26:54.480]   - Yeah, in the limit, yes.
[01:26:54.480 --> 01:26:56.360]   That's the dream of reinforcement learning, right?
[01:26:56.360 --> 01:26:57.200]   It's like, all you need to do
[01:26:57.200 --> 01:26:59.120]   is provide this extremely sparse signal.
[01:26:59.120 --> 01:27:00.840]   And then over enough iterations,
[01:27:00.840 --> 01:27:02.880]   you sort of create the information
[01:27:02.880 --> 01:27:04.840]   that allows you to learn from that signal.
[01:27:05.840 --> 01:27:08.760]   But I don't expect that to be the thing that works first.
[01:27:08.760 --> 01:27:11.400]   I think this is gonna require an incredible amount of care
[01:27:11.400 --> 01:27:13.680]   and like diligence on the behalf of humans
[01:27:13.680 --> 01:27:16.760]   surrounding these machines
[01:27:16.760 --> 01:27:18.400]   and making sure they do exactly the right thing
[01:27:18.400 --> 01:27:20.960]   and exactly what you want and giving them right signals
[01:27:20.960 --> 01:27:23.560]   to improve in the ways that you want.
[01:27:23.560 --> 01:27:25.760]   - Yeah, you can't train on the RL reward
[01:27:25.760 --> 01:27:28.400]   unless the model generates some reward.
[01:27:28.400 --> 01:27:29.800]   - Yeah, yeah, yeah, exactly.
[01:27:29.800 --> 01:27:32.360]   You're in this like sparse RL world where like,
[01:27:32.360 --> 01:27:34.480]   if the client never likes what you produce,
[01:27:34.480 --> 01:27:35.960]   then like you don't get any reward at all
[01:27:35.960 --> 01:27:37.280]   and like, it's kind of bad.
[01:27:37.280 --> 01:27:39.680]   But in the future, these models will be good enough
[01:27:39.680 --> 01:27:41.400]   to get the reward some of the time, right?
[01:27:41.400 --> 01:27:43.160]   - This is the nines of reliability
[01:27:43.160 --> 01:27:44.000]   that Sholta was talking about.
[01:27:44.000 --> 01:27:44.840]   - Yeah, yeah.
[01:27:44.840 --> 01:27:47.720]   There's an interesting digression, by the way,
[01:27:47.720 --> 01:27:49.640]   on earlier we're talking about,
[01:27:49.640 --> 01:27:52.440]   well, we want dense representations
[01:27:52.440 --> 01:27:54.720]   that like, that will be favored, right?
[01:27:54.720 --> 01:27:56.920]   Like that's a more efficient way to communicate.
[01:27:56.920 --> 01:27:59.800]   A book that Trenton recommended,
[01:27:59.800 --> 01:28:00.720]   "The Symbolic Species"
[01:28:00.720 --> 01:28:02.600]   has this really interesting argument
[01:28:03.800 --> 01:28:08.800]   that language is not just a thing that like exists,
[01:28:08.800 --> 01:28:11.680]   but like it was also something that evolved
[01:28:11.680 --> 01:28:12.800]   along with our minds.
[01:28:12.800 --> 01:28:17.280]   And specifically evolved to be both easy to learn
[01:28:17.280 --> 01:28:19.880]   for children and to something
[01:28:19.880 --> 01:28:22.920]   that helps children develop, right?
[01:28:22.920 --> 01:28:23.760]   Like it's-
[01:28:23.760 --> 01:28:24.600]   - Unpack that phone.
[01:28:24.600 --> 01:28:28.840]   - Because like a lot of the things that children learn
[01:28:28.840 --> 01:28:32.040]   are received through language.
[01:28:32.200 --> 01:28:33.880]   Like the languages that will be the fittest
[01:28:33.880 --> 01:28:38.680]   are ones that help like raise the next generation, right?
[01:28:38.680 --> 01:28:41.520]   And that like makes them smarter, better, whatever.
[01:28:41.520 --> 01:28:42.360]   And if you think about-
[01:28:42.360 --> 01:28:43.200]   - Like gives them the concepts
[01:28:43.200 --> 01:28:44.520]   to express more complex ideas.
[01:28:44.520 --> 01:28:48.160]   - Yeah, that and I guess more pedantically,
[01:28:48.160 --> 01:28:49.200]   just like not die.
[01:28:49.200 --> 01:28:50.040]   - Right, sure.
[01:28:50.040 --> 01:28:51.360]   (all laughing)
[01:28:51.360 --> 01:28:53.760]   - Let's you encode the important shit to not die.
[01:28:53.760 --> 01:28:59.640]   - And so then when we just think of like languages like,
[01:28:59.640 --> 01:29:01.480]   oh, you know, say this contingent
[01:29:01.480 --> 01:29:04.240]   and maybe suboptimal way to represent ideas.
[01:29:04.240 --> 01:29:08.040]   Actually, maybe one of the reasons that LLMs have succeeded
[01:29:08.040 --> 01:29:10.160]   is because language has evolved
[01:29:10.160 --> 01:29:14.000]   for tens of thousands of years to be this sort of cast
[01:29:14.000 --> 01:29:16.360]   in which young minds can develop, right?
[01:29:16.360 --> 01:29:18.720]   Like that is this purpose it was evolved for.
[01:29:18.720 --> 01:29:21.840]   - Certainly when you talk to like multimodal
[01:29:21.840 --> 01:29:23.160]   or like computer vision researchers
[01:29:23.160 --> 01:29:26.440]   versus when you talk to language model researchers,
[01:29:26.440 --> 01:29:27.960]   people who work in other modalities
[01:29:27.960 --> 01:29:30.200]   have to put enormous amounts of thought
[01:29:30.200 --> 01:29:32.440]   into exactly what the right representation space
[01:29:32.440 --> 01:29:33.960]   for the images is.
[01:29:33.960 --> 01:29:36.560]   And like what the right signal to learn from there.
[01:29:36.560 --> 01:29:38.240]   Is it like directly modeling the pixels
[01:29:38.240 --> 01:29:41.400]   or is it, you know, some loss that's conditioned on,
[01:29:41.400 --> 01:29:43.680]   there's like a paper ages ago where they like found
[01:29:43.680 --> 01:29:45.680]   that if you trained on the internal representations
[01:29:45.680 --> 01:29:48.080]   of an image that model, it like helped you predict better.
[01:29:48.080 --> 01:29:50.120]   But then later on, like that's obviously like limiting.
[01:29:50.120 --> 01:29:51.880]   And so there was like pixel CNN
[01:29:51.880 --> 01:29:54.160]   where they're trying to like discreetly model,
[01:29:54.160 --> 01:29:56.880]   you know, the individual pixels and stuff.
[01:29:56.880 --> 01:29:59.560]   But understanding the right level of representation there,
[01:29:59.560 --> 01:30:00.400]   really hot.
[01:30:00.400 --> 01:30:01.240]   In language, people are just like,
[01:30:01.240 --> 01:30:03.280]   well, I guess you just predict the next token, right?
[01:30:03.280 --> 01:30:05.480]   It's like, it's kind of easy here.
[01:30:05.480 --> 01:30:06.320]   Decisions made.
[01:30:06.320 --> 01:30:08.280]   I mean, there's the tokenization,
[01:30:08.280 --> 01:30:10.760]   like discussion and debate about like,
[01:30:10.760 --> 01:30:13.440]   but I'm gonna go as favorites.
[01:30:13.440 --> 01:30:14.760]   - Yeah.
[01:30:14.760 --> 01:30:15.880]   Yeah, that's really interesting.
[01:30:15.880 --> 01:30:20.320]   How much, the case for a multimodal being a way
[01:30:20.320 --> 01:30:23.160]   to bridge the data wall or get past the data wall
[01:30:23.160 --> 01:30:26.280]   is like based on the idea that
[01:30:26.280 --> 01:30:27.760]   the things you would have learned
[01:30:27.760 --> 01:30:29.200]   from more language tokens anyway,
[01:30:29.200 --> 01:30:31.560]   you can just get from YouTube.
[01:30:31.560 --> 01:30:33.240]   Has that actually been the case?
[01:30:33.240 --> 01:30:35.720]   How much like positive transfer do you see
[01:30:35.720 --> 01:30:38.520]   between different modalities where like,
[01:30:38.520 --> 01:30:40.560]   actually the images are helping you be better
[01:30:40.560 --> 01:30:41.960]   at like writing code or something,
[01:30:41.960 --> 01:30:44.200]   just 'cause like the model is learning a latent
[01:30:44.200 --> 01:30:47.640]   capabilities just from trying to understand the image.
[01:30:47.640 --> 01:30:50.280]   - Demis in his interview with you mentioned
[01:30:50.280 --> 01:30:51.320]   positive transfer.
[01:30:51.320 --> 01:30:53.640]   - Can't get in trouble.
[01:30:54.520 --> 01:30:57.120]   - He's not gonna get in trouble.
[01:30:57.120 --> 01:31:02.160]   - But I mean, I can't say about that,
[01:31:02.160 --> 01:31:05.760]   other than to say, this is something that people
[01:31:05.760 --> 01:31:07.680]   like believe that, yes, like we have all of this data
[01:31:07.680 --> 01:31:08.560]   about the world.
[01:31:08.560 --> 01:31:10.040]   It would be great if we could like
[01:31:10.040 --> 01:31:11.840]   learn an intuitive sense of physics from it
[01:31:11.840 --> 01:31:13.200]   that helps us reason, right?
[01:31:13.200 --> 01:31:15.360]   That seems totally plausible.
[01:31:15.360 --> 01:31:17.000]   - Yeah, I'm the wrong person to ask,
[01:31:17.000 --> 01:31:19.400]   but there are interesting interpretability pieces
[01:31:19.400 --> 01:31:23.840]   where if we fine tune on math problems,
[01:31:23.840 --> 01:31:27.560]   the model just gets better at entity recognition.
[01:31:27.560 --> 01:31:28.680]   - Whoa, really?
[01:31:28.680 --> 01:31:32.040]   - Yeah, so there's like a paper from David Bow's lab
[01:31:32.040 --> 01:31:35.520]   recently where they investigate what actually changes
[01:31:35.520 --> 01:31:37.800]   in a model when I fine tune it with respect
[01:31:37.800 --> 01:31:39.720]   to the attention heads and these sorts of things.
[01:31:39.720 --> 01:31:44.040]   And they have this like synthetic problem of box A
[01:31:44.040 --> 01:31:48.200]   has this object in it, box B has this other object in it.
[01:31:48.200 --> 01:31:50.200]   What was in this box?
[01:31:50.200 --> 01:31:52.080]   And if you've tried, and it makes sense, right?
[01:31:52.080 --> 01:31:55.720]   It's like, you're better at like attending
[01:31:55.720 --> 01:31:57.720]   to the positions of different things,
[01:31:57.720 --> 01:31:58.760]   which you need for like coding
[01:31:58.760 --> 01:32:01.200]   and manipulating math equations.
[01:32:01.200 --> 01:32:02.960]   - I love this kind of research.
[01:32:02.960 --> 01:32:03.800]   What's the name of the paper?
[01:32:03.800 --> 01:32:04.840]   Do you know it?
[01:32:04.840 --> 01:32:08.160]   - If you look up like fine tuning models,
[01:32:08.160 --> 01:32:10.920]   math, David Bow's group that came out like a week ago.
[01:32:10.920 --> 01:32:12.520]   - Okay, I'm reading that when I get home.
[01:32:12.520 --> 01:32:14.640]   - I'm not endorsing the paper.
[01:32:14.640 --> 01:32:16.120]   That's like a longer conversation,
[01:32:16.120 --> 01:32:19.440]   but like this, it does talk about insight other work
[01:32:19.440 --> 01:32:21.920]   on this like entity recognition ability.
[01:32:21.920 --> 01:32:24.800]   - One of the things you mentioned to me a long time ago
[01:32:24.800 --> 01:32:28.880]   is the evidence that when you train LLMs on code,
[01:32:28.880 --> 01:32:31.040]   they get better at reasoning and language,
[01:32:31.040 --> 01:32:33.280]   which unless it's the case of the comments in the code
[01:32:33.280 --> 01:32:35.200]   are just really high quality tokens or something
[01:32:35.200 --> 01:32:39.240]   implies that to be able to think through how to code better,
[01:32:39.240 --> 01:32:41.440]   like it makes you like a better reasoner.
[01:32:41.440 --> 01:32:42.680]   And like, that's crazy, right?
[01:32:42.680 --> 01:32:44.080]   Like, I think that's like one of the strongest pieces
[01:32:44.080 --> 01:32:47.480]   of evidence for like scaling, just making the thing smart.
[01:32:47.480 --> 01:32:49.560]   Like that kind of like positive transfer.
[01:32:49.560 --> 01:32:52.000]   - I think like this is true in two senses.
[01:32:52.000 --> 01:32:54.720]   One is just that modeling code obviously implies modeling
[01:32:54.720 --> 01:32:56.800]   a difficult reasoning process used to create it.
[01:32:56.800 --> 01:33:00.360]   But two, that code is a nice explicit like structure
[01:33:00.360 --> 01:33:03.240]   of like composed reasoning, I guess.
[01:33:03.240 --> 01:33:07.760]   Like if this, then that like encodes a lot of structure
[01:33:07.760 --> 01:33:09.240]   in that way.
[01:33:09.240 --> 01:33:10.080]   - Yeah.
[01:33:10.080 --> 01:33:11.240]   - That you could imagine transferring
[01:33:11.240 --> 01:33:14.040]   to other types of types of reasoning problem.
[01:33:14.040 --> 01:33:14.880]   - Right.
[01:33:14.880 --> 01:33:17.200]   And crucially, the thing that makes it significant
[01:33:17.200 --> 01:33:21.640]   is that it's not just stochastically predicting
[01:33:21.640 --> 01:33:24.360]   the next token of like words or whatever,
[01:33:24.360 --> 01:33:27.720]   'cause it's like learned that like a Sally corresponds
[01:33:27.720 --> 01:33:30.400]   to murder at the end of a Sherlock Holmes story.
[01:33:30.400 --> 01:33:33.760]   No, like if there is some shared thing between code
[01:33:33.760 --> 01:33:35.760]   and language, it must be at a deeper level
[01:33:35.760 --> 01:33:36.600]   that the model has learned.
[01:33:36.600 --> 01:33:38.320]   - Yeah, I think we have a lot of evidence
[01:33:38.320 --> 01:33:41.000]   that actual reasoning is occurring in these models
[01:33:41.000 --> 01:33:43.120]   and that like they're not just stochastic parrots.
[01:33:43.120 --> 01:33:43.960]   - Yeah.
[01:33:43.960 --> 01:33:46.760]   - It just feels very hard for me to believe
[01:33:46.760 --> 01:33:48.920]   that I haven't worked and played with these models.
[01:33:48.920 --> 01:33:52.640]   - But Normies who will listen will be like, you know.
[01:33:52.640 --> 01:33:54.240]   (all laughing)
[01:33:54.240 --> 01:33:57.320]   - Yeah, my two like immediate cash responses to this
[01:33:57.320 --> 01:34:00.000]   are one, the work on Othello and now other games
[01:34:00.000 --> 01:34:02.960]   where it's like, I give you a sequence of moves in the game.
[01:34:02.960 --> 01:34:05.120]   And it turns out if you apply some like
[01:34:05.120 --> 01:34:07.480]   pretty straightforward interpretability techniques,
[01:34:07.480 --> 01:34:11.760]   then you can get a board that the model has learned.
[01:34:11.760 --> 01:34:13.960]   And it's never seen the game board before anything, right?
[01:34:13.960 --> 01:34:15.120]   Like that's generalization.
[01:34:15.120 --> 01:34:18.600]   The other is anthropics influence functions paper
[01:34:18.600 --> 01:34:22.360]   that came out last year where they look at the model outputs
[01:34:22.360 --> 01:34:25.040]   like, please don't turn me off, I wanna be helpful.
[01:34:25.040 --> 01:34:27.840]   And then they scan like what was the data that led to that?
[01:34:27.840 --> 01:34:31.240]   And like one of the data points that was very influential
[01:34:31.240 --> 01:34:34.480]   was someone dying of dehydration in the desert
[01:34:34.480 --> 01:34:37.120]   and like having like a will to keep surviving.
[01:34:38.360 --> 01:34:42.000]   And to me that just seems like very clear
[01:34:42.000 --> 01:34:46.480]   generalization of motive rather than regurgitating,
[01:34:46.480 --> 01:34:47.320]   don't turn me off.
[01:34:47.320 --> 01:34:48.880]   I think 2001 A Space Odyssey
[01:34:48.880 --> 01:34:50.280]   was also one of the influential things.
[01:34:50.280 --> 01:34:51.760]   And so that's more related,
[01:34:51.760 --> 01:34:53.440]   but it's clearly pulling in things
[01:34:53.440 --> 01:34:55.040]   from lots of different distributions.
[01:34:55.040 --> 01:34:56.400]   - And I also like the evidence you see
[01:34:56.400 --> 01:34:57.760]   even with like very small transformers
[01:34:57.760 --> 01:35:00.800]   where you can explicitly encode circuits
[01:35:00.800 --> 01:35:03.160]   to like do addition, right?
[01:35:03.160 --> 01:35:04.000]   - Or induction heads.
[01:35:04.000 --> 01:35:05.120]   - Or induction heads, this kind of thing.
[01:35:05.120 --> 01:35:08.160]   Like you can literally encode basic reasoning processes
[01:35:08.160 --> 01:35:10.680]   in the models manually.
[01:35:10.680 --> 01:35:12.520]   And it seems clear that there's evidence
[01:35:12.520 --> 01:35:13.680]   that they also learn this automatically
[01:35:13.680 --> 01:35:16.800]   'cause you can then rediscover those from trained models.
[01:35:16.800 --> 01:35:18.240]   To me, this is pretty strong.
[01:35:18.240 --> 01:35:19.600]   - The models are under-parameterized.
[01:35:19.600 --> 01:35:21.560]   - Yeah, they're under-parameterized.
[01:35:21.560 --> 01:35:22.400]   - We're asking them to do a parameter test.
[01:35:22.400 --> 01:35:23.480]   - And they want to learn.
[01:35:23.480 --> 01:35:25.760]   - And they want to learn, the gradients want to flow.
[01:35:25.760 --> 01:35:29.440]   And so they're learning more general skills.
[01:35:29.440 --> 01:35:33.840]   - Okay, so I wanna take a step back from the research
[01:35:33.840 --> 01:35:38.320]   and ask about your careers specifically
[01:35:38.320 --> 01:35:43.280]   because like the tweet implied that I introduced you with,
[01:35:43.280 --> 01:35:45.240]   you've been in this field a year and a half.
[01:35:45.240 --> 01:35:46.200]   I think you've only been in it
[01:35:46.200 --> 01:35:47.440]   like a year or something, right?
[01:35:47.440 --> 01:35:48.280]   It's like--
[01:35:48.280 --> 01:35:49.120]   - I haven't dropped it yet.
[01:35:49.120 --> 01:35:52.600]   - Yeah, but you know, like in that time,
[01:35:52.600 --> 01:35:55.120]   I know the solve the Lyman takes are overstated
[01:35:55.120 --> 01:35:56.120]   and you won't say this to yourself
[01:35:56.120 --> 01:35:57.080]   'cause you'd be embarrassed to,
[01:35:57.080 --> 01:35:58.960]   but like, you know, it's like a pretty incredible thing.
[01:35:58.960 --> 01:36:00.200]   Like the thing that people
[01:36:00.240 --> 01:36:01.840]   in mechanistic numerativity think
[01:36:01.840 --> 01:36:05.240]   is the biggest, you know, step forward
[01:36:05.240 --> 01:36:07.040]   and you've like been working on it for a year.
[01:36:07.040 --> 01:36:08.920]   It's notable.
[01:36:08.920 --> 01:36:13.400]   So I'm curious how you explain what's happened.
[01:36:13.400 --> 01:36:15.080]   Like why in a year or a year and a half
[01:36:15.080 --> 01:36:18.400]   have you guys been, you know,
[01:36:18.400 --> 01:36:21.280]   made important contributions to your field?
[01:36:21.280 --> 01:36:23.160]   - It goes without saying luck, obviously.
[01:36:23.160 --> 01:36:24.560]   And I feel like I've been very lucky
[01:36:24.560 --> 01:36:28.080]   in like the timing of different progressions
[01:36:28.080 --> 01:36:30.360]   has been just like really good
[01:36:30.360 --> 01:36:32.800]   in terms of advancing to the next level of growth.
[01:36:32.800 --> 01:36:36.840]   I feel like for the interpretability team specifically,
[01:36:36.840 --> 01:36:38.840]   I joined when we were five people.
[01:36:38.840 --> 01:36:40.240]   We've now grown quite a lot,
[01:36:40.240 --> 01:36:43.320]   but there were so many ideas floating around
[01:36:43.320 --> 01:36:46.680]   and we just needed to like really execute on them
[01:36:46.680 --> 01:36:48.360]   and have like quick feedback loops
[01:36:48.360 --> 01:36:50.400]   and like do careful experimentation
[01:36:50.400 --> 01:36:53.520]   that led to like signs of life
[01:36:53.520 --> 01:36:55.720]   and have now allowed us to like really scale.
[01:36:55.720 --> 01:36:57.960]   And I feel like that's kind of been my biggest value
[01:36:57.960 --> 01:37:01.080]   to the team, which it's not all engineering,
[01:37:01.080 --> 01:37:03.640]   but quite a lot of it has been.
[01:37:03.640 --> 01:37:04.480]   - Interesting.
[01:37:04.480 --> 01:37:06.000]   So you're saying like you came at a point
[01:37:06.000 --> 01:37:08.360]   where like there had been a lot of science done
[01:37:08.360 --> 01:37:10.160]   and there was a lot of like good research floating around,
[01:37:10.160 --> 01:37:11.600]   but they needed someone to like just take that
[01:37:11.600 --> 01:37:13.720]   and like maniacally execute on it.
[01:37:13.720 --> 01:37:14.560]   - Yeah, yeah.
[01:37:14.560 --> 01:37:16.720]   And this is why it's not all engineering
[01:37:16.720 --> 01:37:18.880]   'cause it's like running different experiments
[01:37:18.880 --> 01:37:21.160]   and like having a hunch for why it might not be working
[01:37:21.160 --> 01:37:22.360]   and then like opening up the model
[01:37:22.360 --> 01:37:24.000]   or opening up the weights and like, what is it learning?
[01:37:24.000 --> 01:37:25.520]   Okay, well, let me try and do this instead
[01:37:25.520 --> 01:37:26.360]   and that sort of thing.
[01:37:27.160 --> 01:37:29.800]   A lot of it has just been being able to do
[01:37:29.800 --> 01:37:33.800]   like very careful, thorough, but quick investigation
[01:37:33.800 --> 01:37:36.440]   of different ideas or yeah, theories.
[01:37:36.440 --> 01:37:39.640]   - And why was that lacking in the existing?
[01:37:39.640 --> 01:37:40.480]   - I don't know.
[01:37:40.480 --> 01:37:43.280]   I feel like, I mean, I work quite a lot
[01:37:43.280 --> 01:37:46.400]   and then I feel like I just am like quite agentic.
[01:37:46.400 --> 01:37:49.320]   Like if your question's about like career overall
[01:37:49.320 --> 01:37:51.640]   and I've been very privileged
[01:37:51.640 --> 01:37:53.160]   to have like a really nice safety net
[01:37:53.160 --> 01:37:54.800]   to be able to take lots of risks,
[01:37:54.800 --> 01:37:56.960]   but I'm just like quite headstrong.
[01:37:56.960 --> 01:37:59.160]   Like in undergrad, Duke had this thing
[01:37:59.160 --> 01:38:00.880]   where you could just make your own major.
[01:38:00.880 --> 01:38:02.920]   And it was like, eh, I don't like this prerequisite
[01:38:02.920 --> 01:38:05.080]   or this prerequisite and I wanna take all four
[01:38:05.080 --> 01:38:06.560]   or five of these subjects at the same time.
[01:38:06.560 --> 01:38:08.120]   So I'm just gonna make my own major.
[01:38:08.120 --> 01:38:09.600]   Or like in the first year of grad school,
[01:38:09.600 --> 01:38:12.840]   I like canceled rotation so I could work on this thing
[01:38:12.840 --> 01:38:15.400]   that became the paper we were talking about earlier
[01:38:15.400 --> 01:38:17.000]   and like didn't have an advisor,
[01:38:17.000 --> 01:38:19.920]   like got admitted to do machine learning for protein design
[01:38:19.920 --> 01:38:23.040]   and was just like off in computational neuroscience land
[01:38:23.040 --> 01:38:25.960]   with no business there at all, but worked out.
[01:38:25.960 --> 01:38:26.920]   - There's a headstrongness,
[01:38:26.920 --> 01:38:29.320]   but it seemed like another theme that jumped out
[01:38:29.320 --> 01:38:33.160]   was the ability to step back
[01:38:33.160 --> 01:38:34.640]   and you were talking about this earlier,
[01:38:34.640 --> 01:38:36.720]   the ability to step back from your sunk costs
[01:38:36.720 --> 01:38:38.480]   and go in a different direction
[01:38:38.480 --> 01:38:40.000]   is in a weird sense the opposite of that,
[01:38:40.000 --> 01:38:41.280]   but also a crucial step here
[01:38:41.280 --> 01:38:44.040]   where I know like 21-year-olds or like 19-year-olds
[01:38:44.040 --> 01:38:47.160]   who are like, ah, this is not a thing I've specialized in
[01:38:47.160 --> 01:38:48.440]   or like I didn't major in this.
[01:38:48.440 --> 01:38:50.480]   I was like, dude, motherfucker, you're 19.
[01:38:50.480 --> 01:38:51.800]   Like you can definitely do this.
[01:38:51.800 --> 01:38:53.680]   And you like switching in the middle of grad school
[01:38:53.680 --> 01:38:56.960]   or something like that's just like, yeah.
[01:38:56.960 --> 01:38:58.040]   - Yeah, sorry, I didn't mean to cut you off,
[01:38:58.040 --> 01:39:00.880]   but I think it's like strong ideas loosely held
[01:39:00.880 --> 01:39:04.760]   and being able to just like pinball in different directions.
[01:39:04.760 --> 01:39:06.520]   And the headstrongness I think relates a little bit
[01:39:06.520 --> 01:39:08.440]   to the fast feedback loops or agency
[01:39:08.440 --> 01:39:11.880]   in so much as I just don't get blocked very often.
[01:39:11.880 --> 01:39:13.440]   Like if I'm trying to write some code
[01:39:13.440 --> 01:39:14.680]   and like something isn't working,
[01:39:14.680 --> 01:39:16.680]   even if it's like in another part of the code base,
[01:39:16.680 --> 01:39:19.080]   I'll often just go in and fix that thing
[01:39:19.080 --> 01:39:21.160]   or at least hack it together to be able to get results.
[01:39:21.160 --> 01:39:23.960]   And I've seen other people where they're just like, help.
[01:39:23.960 --> 01:39:24.800]   I can't.
[01:39:24.800 --> 01:39:26.560]   And it's like, no, that's not a good enough excuse.
[01:39:26.560 --> 01:39:27.680]   Like go all the way down.
[01:39:27.680 --> 01:39:28.560]   - I've definitely heard like people
[01:39:28.560 --> 01:39:30.280]   in management type positions
[01:39:30.280 --> 01:39:32.480]   talk about the lack of such people
[01:39:32.480 --> 01:39:34.520]   where they will check in on somebody
[01:39:34.520 --> 01:39:35.920]   a month after they give them a task
[01:39:35.920 --> 01:39:36.760]   or a week after they give them a task
[01:39:36.760 --> 01:39:38.240]   and like, how's it going?
[01:39:38.240 --> 01:39:40.880]   And they say, well, you know, we need to do this thing
[01:39:40.880 --> 01:39:42.680]   which requires lawyers
[01:39:42.680 --> 01:39:45.080]   'cause it requires talking about this regulation.
[01:39:45.080 --> 01:39:46.200]   It's like, how's that going?
[01:39:46.200 --> 01:39:47.960]   I was like, well, we need lawyers.
[01:39:47.960 --> 01:39:49.920]   I'm like, why didn't you get lawyers?
[01:39:50.880 --> 01:39:52.720]   Or something like that.
[01:39:52.720 --> 01:39:53.720]   So that's definitely like, yeah.
[01:39:53.720 --> 01:39:56.240]   - I think that's arguably the most important quality
[01:39:56.240 --> 01:39:57.440]   in like almost anything.
[01:39:57.440 --> 01:39:59.480]   It's just pursuing it to like the end of the earth
[01:39:59.480 --> 01:40:01.160]   and like whatever you need to do to make it happen,
[01:40:01.160 --> 01:40:02.120]   you'll make it happen.
[01:40:02.120 --> 01:40:02.960]   - If you do everything, you'll win.
[01:40:02.960 --> 01:40:05.720]   - If you do everything, you'll win, exactly.
[01:40:05.720 --> 01:40:07.120]   But yeah, yeah, yeah, yeah.
[01:40:07.120 --> 01:40:10.000]   I think from my side,
[01:40:10.000 --> 01:40:13.040]   definitely that quality has been important.
[01:40:13.040 --> 01:40:13.920]   Like agency in the work.
[01:40:13.920 --> 01:40:16.080]   There are thousands or I would even like
[01:40:16.080 --> 01:40:17.840]   probably tens of thousands of engineers at Google
[01:40:17.840 --> 01:40:20.040]   who are like, you know, basically,
[01:40:20.040 --> 01:40:21.280]   like we're all like equivalent
[01:40:21.280 --> 01:40:23.480]   like software engineering ability, let's say.
[01:40:23.480 --> 01:40:27.160]   Like, you know, if you gave us like a very well-defined task
[01:40:27.160 --> 01:40:28.800]   then we'd probably do it like equivalently well.
[01:40:28.800 --> 01:40:30.800]   Maybe a bunch of them would do it a lot better than me,
[01:40:30.800 --> 01:40:32.160]   you know, in all likelihood.
[01:40:32.160 --> 01:40:34.720]   But what I've been,
[01:40:34.720 --> 01:40:38.000]   one of the reasons that I've been impactful so far
[01:40:38.000 --> 01:40:39.880]   is I've been very good
[01:40:39.880 --> 01:40:44.040]   at picking extremely high leverage problems.
[01:40:44.040 --> 01:40:45.120]   So problems that haven't been
[01:40:45.280 --> 01:40:46.920]   particularly well-solved so far.
[01:40:46.920 --> 01:40:51.960]   Perhaps as a result of like frustrating structural factors
[01:40:51.960 --> 01:40:53.320]   like the ones that you pointed out
[01:40:53.320 --> 01:40:54.520]   in that scenario before,
[01:40:54.520 --> 01:40:55.640]   where they're like, oh, we can't do X
[01:40:55.640 --> 01:40:57.760]   'cause this, what team would you do, why?
[01:40:57.760 --> 01:40:59.200]   Or like, and then going, okay,
[01:40:59.200 --> 01:41:01.520]   well, I'm just gonna like vertically solve the entire thing.
[01:41:01.520 --> 01:41:03.800]   (all laughing)
[01:41:03.800 --> 01:41:05.280]   - Right.
[01:41:05.280 --> 01:41:07.360]   - And that turns out to be remarkably effective.
[01:41:07.360 --> 01:41:10.880]   Also, I am very comfortable with like,
[01:41:10.880 --> 01:41:14.240]   if I think there is something correct that needs to happen,
[01:41:14.240 --> 01:41:16.200]   I will like make that argument
[01:41:16.200 --> 01:41:17.480]   and continue making that argument
[01:41:17.480 --> 01:41:21.120]   at escalating levels of like criticality
[01:41:21.120 --> 01:41:22.800]   until that thing gets solved.
[01:41:22.800 --> 01:41:26.600]   And I'm also quite pragmatic
[01:41:26.600 --> 01:41:29.520]   with what like I do to solve things.
[01:41:29.520 --> 01:41:30.800]   You get a lot of people who come in with,
[01:41:30.800 --> 01:41:32.760]   as I said before, like a particular background
[01:41:32.760 --> 01:41:34.040]   or a familiarity or they're like,
[01:41:34.040 --> 01:41:36.880]   they know how to do something and they won't,
[01:41:36.880 --> 01:41:39.360]   like one of the beautiful things about Google, right,
[01:41:39.360 --> 01:41:41.280]   is you can run around and get world experts
[01:41:41.280 --> 01:41:42.320]   in literally everything.
[01:41:42.320 --> 01:41:44.440]   You can sit down and talk to people, optimization experts,
[01:41:44.440 --> 01:41:46.760]   like TP, like chip design experts,
[01:41:46.760 --> 01:41:48.880]   like experts in, I don't know,
[01:41:48.880 --> 01:41:51.600]   like different forms of like pre-training algorithms
[01:41:51.600 --> 01:41:52.440]   or like RL or whatever.
[01:41:52.440 --> 01:41:53.280]   And you can learn from all of them
[01:41:53.280 --> 01:41:55.520]   and you can take those methods and apply them.
[01:41:55.520 --> 01:41:59.360]   And I think this was like,
[01:41:59.360 --> 01:42:02.880]   maybe the start of why I was initially impactful
[01:42:02.880 --> 01:42:06.120]   was like this vertical, like agency effectively.
[01:42:06.120 --> 01:42:10.360]   And then a follow-up piece from that is,
[01:42:10.360 --> 01:42:14.920]   I think it's often surprising how few people
[01:42:14.920 --> 01:42:16.960]   are like fully realized in all the things they wanna do.
[01:42:16.960 --> 01:42:18.360]   They're like blocked or limited in some way.
[01:42:18.360 --> 01:42:20.640]   And this is very common in big organizations everywhere.
[01:42:20.640 --> 01:42:22.600]   People like have all these blockers
[01:42:22.600 --> 01:42:24.960]   on what they're able to achieve.
[01:42:24.960 --> 01:42:29.080]   And I think being a,
[01:42:29.080 --> 01:42:30.680]   like one, helping inspire people
[01:42:30.680 --> 01:42:32.920]   to work on particular directions
[01:42:32.920 --> 01:42:34.800]   and working with them on doing things
[01:42:34.800 --> 01:42:36.200]   massively scales your leverage.
[01:42:36.200 --> 01:42:37.960]   Like you get to work with all these wonderful people
[01:42:37.960 --> 01:42:39.520]   who teach you heaps of things
[01:42:39.520 --> 01:42:42.400]   and generally like helping them
[01:42:42.400 --> 01:42:44.040]   push past organizational blockers
[01:42:44.040 --> 01:42:47.320]   means like together you get an enormous amount done.
[01:42:47.320 --> 01:42:50.440]   Like none of the impact that I've had
[01:42:50.440 --> 01:42:51.960]   has been like me individually going off
[01:42:51.960 --> 01:42:53.520]   and solving a whole lot of stuff.
[01:42:53.520 --> 01:42:56.520]   It's been me maybe like starting off a direction
[01:42:56.520 --> 01:42:58.360]   and then convincing other people
[01:42:58.360 --> 01:42:59.560]   that this is the right direction
[01:42:59.560 --> 01:43:01.880]   and bringing them along in like this big tidal wave
[01:43:01.880 --> 01:43:05.440]   of like effectiveness that like goes and solves that problem.
[01:43:07.160 --> 01:43:11.240]   - We should talk about how you guys got hired.
[01:43:11.240 --> 01:43:12.640]   'Cause I think that's a really interesting story
[01:43:12.640 --> 01:43:14.640]   'cause you were a McKinsey consultant, right?
[01:43:14.640 --> 01:43:17.240]   (all laughing)
[01:43:17.240 --> 01:43:18.080]   - No, no, it's good, yeah.
[01:43:18.080 --> 01:43:20.160]   - There's an interesting thing there where,
[01:43:20.160 --> 01:43:23.880]   first of all, I think people are,
[01:43:23.880 --> 01:43:26.120]   yeah, generally people just don't understand
[01:43:26.120 --> 01:43:29.880]   how like decisions are made about either admissions
[01:43:29.880 --> 01:43:32.280]   or evaluating who to hire or something.
[01:43:32.280 --> 01:43:34.600]   But like just talk about like how were you noticed as-
[01:43:34.600 --> 01:43:35.440]   - Yeah, totally.
[01:43:35.440 --> 01:43:36.440]   - Yeah, you got hired.
[01:43:36.440 --> 01:43:37.280]   - So like the TL;DR of this
[01:43:37.280 --> 01:43:39.440]   is I studied robotics in undergrad.
[01:43:39.440 --> 01:43:41.560]   I always thought that AI would be one of the highest leverage
[01:43:41.560 --> 01:43:42.800]   ways to impact the future in a positive way.
[01:43:42.800 --> 01:43:44.320]   Like the reason I am doing this
[01:43:44.320 --> 01:43:47.080]   is because I think it is like one of our best shots
[01:43:47.080 --> 01:43:49.080]   at making a wonderful future, basically.
[01:43:49.080 --> 01:43:52.240]   And I thought that working actually at McKinsey,
[01:43:52.240 --> 01:43:53.400]   I would get a really interesting insight
[01:43:53.400 --> 01:43:54.920]   into what people actually did for work.
[01:43:54.920 --> 01:43:55.960]   Like in this, I actually wrote this
[01:43:55.960 --> 01:43:57.960]   as the first line in my cover letter to McKinsey
[01:43:57.960 --> 01:43:59.400]   was like, "I wanna work here
[01:43:59.400 --> 01:44:00.680]   "so that I can learn what people do
[01:44:00.680 --> 01:44:02.720]   "so that I can like understand how they work."
[01:44:02.720 --> 01:44:05.720]   (all laughing)
[01:44:05.720 --> 01:44:10.720]   And in many respects, like I did get that.
[01:44:10.720 --> 01:44:11.680]   I also got a whole lot of other things.
[01:44:11.680 --> 01:44:13.680]   Many of the people there are like wonderful friends.
[01:44:13.680 --> 01:44:15.000]   I actually learned, I think a lot of this,
[01:44:15.000 --> 01:44:17.840]   like agentic behavior in part from my time there
[01:44:17.840 --> 01:44:21.480]   where you go into organizations and you see how impactful
[01:44:21.480 --> 01:44:25.480]   just not taking no for an answer gets you.
[01:44:25.480 --> 01:44:28.400]   Like you would be surprised at the kind of stuff
[01:44:28.400 --> 01:44:32.480]   where like, because no one quite cares enough
[01:44:32.480 --> 01:44:35.680]   in some organizations, things just don't happen
[01:44:35.680 --> 01:44:37.480]   'cause no one's willing to take direct responsibility.
[01:44:37.480 --> 01:44:39.360]   This is incredibly like directly responsible individuals
[01:44:39.360 --> 01:44:40.720]   are ridiculously important.
[01:44:40.720 --> 01:44:43.880]   And people are willing to,
[01:44:43.880 --> 01:44:46.200]   like they just don't care as much about timelines.
[01:44:46.200 --> 01:44:48.680]   And so much of the value that an organization
[01:44:48.680 --> 01:44:51.520]   like McKinsey provides is hiring people
[01:44:51.520 --> 01:44:54.560]   who you were otherwise unable to hire
[01:44:54.560 --> 01:44:56.160]   for a short window of time
[01:44:56.160 --> 01:44:59.000]   where they can just like push through problems.
[01:44:59.000 --> 01:45:01.760]   I think people like underappreciate this.
[01:45:01.760 --> 01:45:05.240]   And so like at least some of my,
[01:45:05.240 --> 01:45:06.960]   well, hold up, like I'm gonna become
[01:45:06.960 --> 01:45:08.280]   the directly responsible individual for this
[01:45:08.280 --> 01:45:10.400]   'cause no one's taking appropriate like responsibility.
[01:45:10.400 --> 01:45:12.600]   I'm gonna care a hell of a lot about this.
[01:45:12.600 --> 01:45:13.440]   And I'm gonna make sure,
[01:45:13.440 --> 01:45:14.480]   like I'm gonna go to the end of the earth
[01:45:14.480 --> 01:45:17.400]   to make sure it gets done, comes from that time.
[01:45:17.400 --> 01:45:19.360]   But more to your like actual question
[01:45:19.360 --> 01:45:22.840]   of like how did I get hired?
[01:45:22.840 --> 01:45:25.120]   The entire time I didn't get into the grad programs
[01:45:25.120 --> 01:45:26.920]   that I wanted to get into over here,
[01:45:26.920 --> 01:45:29.840]   which was specifically for focus on like robotics
[01:45:29.840 --> 01:45:31.800]   and RL research and that kind of stuff.
[01:45:31.800 --> 01:45:35.120]   And in the meantime, on nights and weekends,
[01:45:35.120 --> 01:45:37.920]   basically every night from 10 p.m. till 2 a.m.,
[01:45:37.920 --> 01:45:40.120]   I would do my own like research.
[01:45:40.120 --> 01:45:43.040]   And every weekend for like at least six to eight hours
[01:45:43.040 --> 01:45:45.240]   each day, I would do my own like research
[01:45:45.240 --> 01:45:47.400]   and coding projects and this kind of stuff.
[01:45:47.400 --> 01:45:52.240]   And that sort of switched in part
[01:45:52.240 --> 01:45:55.160]   from like quite robotics specific work
[01:45:55.160 --> 01:45:58.400]   to after reading Gwen's scaling hypothesis post,
[01:45:58.400 --> 01:46:00.440]   I got completely scaling pills.
[01:46:00.440 --> 01:46:01.680]   And I was like, okay, like clearly the way
[01:46:01.680 --> 01:46:03.280]   that you solve robotics is by like scaling
[01:46:03.280 --> 01:46:04.880]   large multimodal models.
[01:46:04.880 --> 01:46:08.080]   And then in an effort to scale large multimodal models
[01:46:08.080 --> 01:46:11.320]   with a grant, I got a grant from the TPU
[01:46:11.320 --> 01:46:14.880]   like access program, the Tensor Research Cloud.
[01:46:14.880 --> 01:46:18.320]   I was trying to work out how to scale that effectively.
[01:46:18.320 --> 01:46:22.240]   And James Bradbury, who at the time was at Google
[01:46:22.240 --> 01:46:27.240]   and is now at Anthropic, saw some of my questions online
[01:46:27.240 --> 01:46:29.200]   where I was trying to work out how to do this properly.
[01:46:29.200 --> 01:46:30.600]   And he was like, I thought I knew all the people
[01:46:30.600 --> 01:46:32.600]   in the world who were like asking these questions.
[01:46:32.600 --> 01:46:33.680]   Who on earth are you?
[01:46:33.680 --> 01:46:38.800]   And he looked at that and he looked at some
[01:46:38.800 --> 01:46:40.120]   of the like the robotic stuff that I'd been putting
[01:46:40.120 --> 01:46:41.280]   up on my blog and that kind of thing.
[01:46:41.280 --> 01:46:42.680]   And he reached out and said, hey, do you wanna have a chat
[01:46:42.680 --> 01:46:45.320]   and do you wanna like explore working with us here?
[01:46:45.320 --> 01:46:49.760]   And I was hired, as I understand it later,
[01:46:49.760 --> 01:46:51.960]   as an experiment in trying to take someone
[01:46:51.960 --> 01:46:54.600]   with extremely high enthusiasm and agency
[01:46:54.600 --> 01:46:57.600]   and pairing them with some of the best engineers
[01:46:57.600 --> 01:46:58.600]   that he knew.
[01:46:58.600 --> 01:47:00.800]   And so one, another one of the reasons I could say
[01:47:00.800 --> 01:47:03.160]   like I've been impactful is I had this like dedicated
[01:47:03.160 --> 01:47:06.160]   mentorship from utterly wonderful people.
[01:47:06.160 --> 01:47:08.680]   Like people like Rainer Pope, who has since left
[01:47:08.680 --> 01:47:11.560]   to go do his own ship company.
[01:47:11.560 --> 01:47:15.160]   Anselm Weskaja, James himself, many others.
[01:47:15.160 --> 01:47:16.640]   But those are like the sort of formative like two
[01:47:16.640 --> 01:47:18.280]   to three months at the beginning.
[01:47:18.280 --> 01:47:22.080]   And they taught me a whole lot of like the principles
[01:47:22.080 --> 01:47:25.080]   and like heuristics that I apply, like how to,
[01:47:25.080 --> 01:47:28.160]   and how to like solve problems in the way that they have,
[01:47:28.160 --> 01:47:30.760]   particularly in that like systems and algorithms overlap,
[01:47:30.760 --> 01:47:34.480]   where like one more thing that makes you like quite effective
[01:47:34.480 --> 01:47:36.880]   in ML research is really concretely understanding
[01:47:36.880 --> 01:47:38.520]   the systems side of things.
[01:47:38.520 --> 01:47:40.200]   And this is something I've learned from them basically,
[01:47:40.200 --> 01:47:42.600]   is like a deep understanding of how systems influence
[01:47:42.600 --> 01:47:44.840]   algorithms and how algorithms influence systems.
[01:47:44.840 --> 01:47:47.400]   Because the systems constrain the design space,
[01:47:47.400 --> 01:47:49.600]   sorry, the solution space, which you have available
[01:47:49.600 --> 01:47:51.840]   to yourself in the algorithm side.
[01:47:51.840 --> 01:47:54.840]   And very few people are comfortable fully bridging that gap.
[01:47:56.280 --> 01:47:59.120]   But a place like Google, you can just like go and ask
[01:47:59.120 --> 01:48:00.920]   all the algorithms experts and all the systems experts
[01:48:00.920 --> 01:48:03.800]   everything they know, and they will happily teach you.
[01:48:03.800 --> 01:48:05.080]   And if you go and sit down with them,
[01:48:05.080 --> 01:48:06.720]   they will teach you everything they know.
[01:48:06.720 --> 01:48:07.560]   It's wonderful.
[01:48:07.560 --> 01:48:10.080]   And this has meant that I've been able to be very,
[01:48:10.080 --> 01:48:13.360]   very effective for both sides, like for the pre-training
[01:48:13.360 --> 01:48:15.160]   crew, because I understand systems very well.
[01:48:15.160 --> 01:48:17.520]   I can intuit and understand like this will work well
[01:48:17.520 --> 01:48:18.440]   or this won't.
[01:48:18.440 --> 01:48:20.560]   And then like flow that on through the inference
[01:48:20.560 --> 01:48:23.240]   considerations of models and this kind of thing.
[01:48:23.240 --> 01:48:25.800]   And for like to the chip design teams,
[01:48:25.800 --> 01:48:28.080]   I'm one of the people they turn to understand
[01:48:28.080 --> 01:48:30.200]   what chips they should be designing in three years,
[01:48:30.200 --> 01:48:32.280]   because I'm one of the people who's best able
[01:48:32.280 --> 01:48:35.640]   to understand and explain the kind of algorithms
[01:48:35.640 --> 01:48:37.880]   that we might want to design in three years.
[01:48:37.880 --> 01:48:39.880]   And obviously you can't make very good guesses about that.
[01:48:39.880 --> 01:48:44.640]   But like, I think I like convey the information well,
[01:48:44.640 --> 01:48:47.200]   accumulated from all of my compatriots
[01:48:47.200 --> 01:48:50.680]   on the pre-training crew, and like the general,
[01:48:50.680 --> 01:48:54.000]   like systems inside group and convey that information
[01:48:54.000 --> 01:48:56.840]   well to them, because also even inference applies
[01:48:56.840 --> 01:48:58.120]   a constraint to pre-training.
[01:48:58.120 --> 01:49:01.040]   And so like, there's this like these trees of constraints
[01:49:01.040 --> 01:49:03.160]   where if you understand all the pieces of the puzzle,
[01:49:03.160 --> 01:49:05.520]   then you get a much better sense for like what
[01:49:05.520 --> 01:49:06.920]   the solution space might look like.
[01:49:06.920 --> 01:49:09.760]   - Yeah, there's a couple of things that stick out
[01:49:09.760 --> 01:49:10.600]   to me there.
[01:49:10.600 --> 01:49:14.880]   One is not just the agency of the person who was hired,
[01:49:14.880 --> 01:49:18.560]   but the parts of the system that were able to think,
[01:49:18.560 --> 01:49:19.600]   wait, that's really interesting.
[01:49:19.600 --> 01:49:21.280]   Who is this guy?
[01:49:21.280 --> 01:49:23.640]   Not from a grad program or anything,
[01:49:23.640 --> 01:49:25.560]   you know, like currently McKinsey consultant,
[01:49:25.560 --> 01:49:28.880]   just like an undergrad, but that's interesting.
[01:49:28.880 --> 01:49:30.360]   Let's like give this a shot, right?
[01:49:30.360 --> 01:49:32.600]   So James and whoever else that's like,
[01:49:32.600 --> 01:49:33.440]   that's very notable.
[01:49:33.440 --> 01:49:37.680]   And that's, second is, I actually didn't know this part
[01:49:37.680 --> 01:49:40.800]   of the story where that was part of an experiment
[01:49:40.800 --> 01:49:43.680]   run internally about, can we do this?
[01:49:43.680 --> 01:49:47.160]   Can we like bootstrap somebody and like, yeah.
[01:49:47.160 --> 01:49:49.360]   And in fact, what's really interesting about that
[01:49:49.360 --> 01:49:51.160]   is the third thing you mentioned is,
[01:49:51.160 --> 01:49:54.720]   having somebody who understands all layers of the stack
[01:49:54.720 --> 01:49:56.680]   and isn't so stuck on any one approach
[01:49:56.680 --> 01:49:59.720]   or any one layer of abstraction is so important.
[01:49:59.720 --> 01:50:02.720]   And specifically that like what you mentioned
[01:50:02.720 --> 01:50:06.080]   about being bootstrapped immediately by these people
[01:50:06.080 --> 01:50:08.160]   might've meant that since you're getting up to speed
[01:50:08.160 --> 01:50:09.760]   on everything at the same time,
[01:50:09.760 --> 01:50:12.160]   rather than spending grad school going deep
[01:50:12.160 --> 01:50:14.080]   on like one specific way of doing RL,
[01:50:14.080 --> 01:50:15.640]   you actually can take the global view
[01:50:15.640 --> 01:50:17.480]   and aren't like totally bought in on one thing.
[01:50:17.480 --> 01:50:19.280]   So not only is it something that's possible,
[01:50:19.280 --> 01:50:21.120]   but like has greater returns
[01:50:21.120 --> 01:50:23.160]   than just hiring somebody at a grad school potentially.
[01:50:23.160 --> 01:50:25.080]   Because this person can just like, I don't know,
[01:50:25.080 --> 01:50:27.920]   just like getting a GPT-8 and like fine tuning them
[01:50:27.920 --> 01:50:30.840]   on like one year of, you know what I mean?
[01:50:30.840 --> 01:50:32.600]   So yeah, that's really interesting.
[01:50:32.600 --> 01:50:34.120]   - You come at everything with fresh eyes
[01:50:34.120 --> 01:50:37.560]   and you know it come in locked to any particular field.
[01:50:37.560 --> 01:50:40.320]   Now what like one caveat to that is that before,
[01:50:40.320 --> 01:50:42.240]   like during my self-experimentation and stuff,
[01:50:42.240 --> 01:50:43.640]   I was reading everything I could.
[01:50:43.640 --> 01:50:46.360]   I was like obsessively reading papers every night
[01:50:46.360 --> 01:50:48.760]   and like actually funnily enough,
[01:50:48.760 --> 01:50:53.520]   I like read much less widely now
[01:50:53.520 --> 01:50:56.400]   that I like my day is occupied by working on things.
[01:50:56.400 --> 01:50:57.440]   And in some respect,
[01:50:57.440 --> 01:50:59.120]   I had like this very broad perspective before
[01:50:59.120 --> 01:51:00.960]   where not that many people,
[01:51:00.960 --> 01:51:02.640]   even like in a PhD program,
[01:51:02.640 --> 01:51:04.520]   you'll like focus on a particular area.
[01:51:04.520 --> 01:51:06.320]   If you just like read all the NLP work
[01:51:06.320 --> 01:51:07.400]   and all the computer vision work
[01:51:07.400 --> 01:51:08.520]   and like all the robotics work,
[01:51:08.520 --> 01:51:09.760]   you like see all these patterns
[01:51:09.760 --> 01:51:11.840]   that start to emerge across subfields
[01:51:11.840 --> 01:51:15.160]   in a way that I guess like foreshadowed
[01:51:15.160 --> 01:51:17.240]   some of the work that I would later do.
[01:51:17.240 --> 01:51:19.040]   - That's super interesting.
[01:51:19.040 --> 01:51:19.880]   One of the reasons
[01:51:19.880 --> 01:51:22.320]   that you've been able to be agentic within Google
[01:51:22.320 --> 01:51:24.720]   is like your peer programming half the days
[01:51:24.720 --> 01:51:26.480]   or most of the days with Sergey Brin, right?
[01:51:26.480 --> 01:51:27.320]   And so that's really interesting
[01:51:27.320 --> 01:51:29.880]   that like there's this person
[01:51:29.880 --> 01:51:34.200]   who's like willing to just push ahead on this LLM stuff
[01:51:34.200 --> 01:51:37.960]   and like get rid of the local blockers in its place.
[01:51:37.960 --> 01:51:39.480]   - Yeah, I think important to get is like
[01:51:39.480 --> 01:51:40.760]   not like every day or anything,
[01:51:40.760 --> 01:51:43.640]   but like when there are particular projects
[01:51:43.640 --> 01:51:44.520]   that he's interested in,
[01:51:44.520 --> 01:51:45.720]   then like we'll work together on those.
[01:51:45.920 --> 01:51:46.760]   But there's also been times
[01:51:46.760 --> 01:51:48.920]   when he's been focused on projects with other people.
[01:51:48.920 --> 01:51:52.320]   But in general, yes, there's a surprising alpha
[01:51:52.320 --> 01:51:53.840]   to like being one of the people
[01:51:53.840 --> 01:51:56.600]   who actually goes down to the office every day.
[01:51:56.600 --> 01:51:59.440]   That like is really actually shouldn't be,
[01:51:59.440 --> 01:52:01.440]   but is surprisingly impactful.
[01:52:01.440 --> 01:52:05.760]   And as a result, I've like benefited a lot
[01:52:05.760 --> 01:52:09.280]   from having like basically being like close friends
[01:52:09.280 --> 01:52:10.960]   with people in leadership who care
[01:52:10.960 --> 01:52:14.480]   and being able to like really argue convincingly
[01:52:14.480 --> 01:52:17.280]   about why we should do X as opposed to Y.
[01:52:17.280 --> 01:52:20.560]   And having that like vector to,
[01:52:20.560 --> 01:52:24.760]   trying like it's, Google is a big organization.
[01:52:24.760 --> 01:52:27.560]   Having those vectors helps a little bit.
[01:52:27.560 --> 01:52:29.080]   But also it's very important.
[01:52:29.080 --> 01:52:31.480]   It's the kind of thing you don't want to ever abuse, right?
[01:52:31.480 --> 01:52:32.840]   Like you want to make the argument
[01:52:32.840 --> 01:52:34.560]   through all the right channels
[01:52:34.560 --> 01:52:38.800]   and like only sometimes you need to.
[01:52:38.800 --> 01:52:40.280]   - And so this includes you like Sergey Brin,
[01:52:40.280 --> 01:52:41.840]   Jeff Dee and so forth.
[01:52:41.840 --> 01:52:43.160]   I mean, it's like, it's notable.
[01:52:43.160 --> 01:52:44.600]   I don't know, I feel like Google is undervalued
[01:52:44.600 --> 01:52:47.160]   given that like, I don't know,
[01:52:47.160 --> 01:52:49.400]   like Steve Jobs is working on the equivalent
[01:52:49.400 --> 01:52:50.640]   like the next product for Apple,
[01:52:50.640 --> 01:52:52.480]   like peer-to-peer programming or something, right?
[01:52:52.480 --> 01:52:55.280]   - I mean, like I, yeah, I've benefited immensely
[01:52:55.280 --> 01:52:57.560]   from like, okay, so for example,
[01:52:57.560 --> 01:52:59.120]   during the Christmas break,
[01:52:59.120 --> 01:53:02.440]   I was just going into the office
[01:53:02.440 --> 01:53:04.880]   a couple of days during that time.
[01:53:04.880 --> 01:53:06.040]   - Sounds like quite a lot of days.
[01:53:06.040 --> 01:53:07.840]   - Quite a lot of days.
[01:53:07.840 --> 01:53:10.080]   (laughing)
[01:53:12.400 --> 01:53:15.320]   And I don't know if you guys have read that article
[01:53:15.320 --> 01:53:17.360]   about Jeff and Sanjay doing the pair programming,
[01:53:17.360 --> 01:53:20.000]   but they were there pair programming on stuff.
[01:53:20.000 --> 01:53:22.280]   And I got to hear about all these cool stories
[01:53:22.280 --> 01:53:23.240]   of like early Google,
[01:53:23.240 --> 01:53:24.840]   where they're talking about like crawling
[01:53:24.840 --> 01:53:27.440]   under the floorboards and rewiring data centers
[01:53:27.440 --> 01:53:29.800]   and like telling me how many like bits
[01:53:29.800 --> 01:53:31.560]   they were pulling off the,
[01:53:31.560 --> 01:53:33.120]   how many bytes they were pulling off the instructions
[01:53:33.120 --> 01:53:35.200]   of a given compiler instruction.
[01:53:35.200 --> 01:53:36.720]   And like all these like crazy little
[01:53:36.720 --> 01:53:38.160]   performance optimizations they were doing,
[01:53:38.160 --> 01:53:40.080]   like they were having the time of their life.
[01:53:40.080 --> 01:53:41.320]   And I got to like sit there
[01:53:41.320 --> 01:53:44.120]   and really like experience this sense of history
[01:53:44.120 --> 01:53:46.600]   in a way that you don't expect to get.
[01:53:46.600 --> 01:53:49.960]   Like you expect to be very far away from all that,
[01:53:49.960 --> 01:53:52.640]   I think maybe in a large organization, but.
[01:53:52.640 --> 01:53:54.480]   - Yeah, that's super cool.
[01:53:54.480 --> 01:53:57.680]   And Trenton, does this map onto any of your experience?
[01:53:57.680 --> 01:53:59.640]   - I think Sholto's story is more exciting.
[01:53:59.640 --> 01:54:01.120]   (laughing)
[01:54:01.120 --> 01:54:03.040]   Mine was just very serendipitous
[01:54:03.040 --> 01:54:05.640]   in that I got into computational neuroscience,
[01:54:05.640 --> 01:54:08.000]   didn't have much business being there.
[01:54:08.000 --> 01:54:10.520]   My first paper was mapping the cerebellum
[01:54:10.520 --> 01:54:12.720]   to the attention operation and transformers.
[01:54:12.720 --> 01:54:14.400]   My next ones were looking at like sparsity.
[01:54:14.400 --> 01:54:15.920]   - How old were you when you wrote that?
[01:54:15.920 --> 01:54:18.040]   - It was my first year of grad school.
[01:54:18.040 --> 01:54:20.000]   So 22.
[01:54:20.000 --> 01:54:21.120]   - Oh yeah.
[01:54:21.120 --> 01:54:25.840]   - But yeah, my next work was on sparsity in networks,
[01:54:25.840 --> 01:54:27.880]   like inspired by sparsity in the brain,
[01:54:27.880 --> 01:54:30.560]   which was when I met Tristan Hume
[01:54:30.560 --> 01:54:32.000]   and Anthropic was doing the SOLU,
[01:54:32.000 --> 01:54:34.120]   the Softmax Linear Output Unit work,
[01:54:34.120 --> 01:54:36.520]   which was very related in quite a few ways of like,
[01:54:36.520 --> 01:54:38.720]   let's make the activation of neurons
[01:54:38.720 --> 01:54:40.360]   across a layer really sparse.
[01:54:40.360 --> 01:54:41.200]   And if we do that,
[01:54:41.200 --> 01:54:42.440]   then we can get some interpretability
[01:54:42.440 --> 01:54:44.480]   of what the neuron's doing.
[01:54:44.480 --> 01:54:45.800]   I think we've updated on that approach
[01:54:45.800 --> 01:54:47.840]   towards what we're doing now.
[01:54:47.840 --> 01:54:49.200]   So that started the conversation.
[01:54:49.200 --> 01:54:50.680]   I shared drafts of that paper with Tristan.
[01:54:50.680 --> 01:54:52.080]   He was excited about it.
[01:54:52.080 --> 01:54:54.320]   And that was basically what led me
[01:54:54.320 --> 01:54:58.320]   to become Tristan's resident and then convert to full-time.
[01:54:58.320 --> 01:55:00.120]   But during that period,
[01:55:00.120 --> 01:55:03.200]   I also moved as a visiting researcher to Berkeley
[01:55:03.200 --> 01:55:06.000]   and started working with Bruno Olshausen,
[01:55:06.000 --> 01:55:08.760]   both on what's called vector symbolic architectures,
[01:55:08.760 --> 01:55:11.160]   which one of the core operations of them
[01:55:11.160 --> 01:55:13.520]   is literally superposition,
[01:55:13.520 --> 01:55:17.160]   and on sparse coding, also known as dictionary learning,
[01:55:17.160 --> 01:55:19.400]   which is literally what we've been doing since.
[01:55:19.400 --> 01:55:22.720]   And Bruno Olshausen basically invented sparse coding
[01:55:22.720 --> 01:55:23.720]   back in 1997.
[01:55:23.720 --> 01:55:26.960]   And so it was like, my research agenda
[01:55:26.960 --> 01:55:28.120]   and the interpretability team
[01:55:28.120 --> 01:55:30.240]   seemed to just be running in parallel
[01:55:30.240 --> 01:55:33.720]   with just research tastes.
[01:55:33.720 --> 01:55:35.920]   And so, yeah, it made a lot of sense
[01:55:35.920 --> 01:55:37.880]   for me to work with the team.
[01:55:38.880 --> 01:55:40.480]   And it's been a dream since.
[01:55:40.480 --> 01:55:42.760]   - One thing I've noticed when people tell stories
[01:55:42.760 --> 01:55:45.400]   about their careers or their successes,
[01:55:45.400 --> 01:55:47.400]   they ascribe it way more to contingency.
[01:55:47.400 --> 01:55:49.040]   But when they hear about other people's stories,
[01:55:49.040 --> 01:55:50.760]   they're like, "Of course it wasn't contingent."
[01:55:50.760 --> 01:55:51.600]   You know what I mean?
[01:55:51.600 --> 01:55:52.880]   It's like, if that didn't happen,
[01:55:52.880 --> 01:55:54.800]   something else would have happened.
[01:55:54.800 --> 01:55:56.040]   I've just noticed that something like talk to,
[01:55:56.040 --> 01:55:58.440]   and it's interesting that you both think
[01:55:58.440 --> 01:56:00.800]   that it was especially contingent.
[01:56:00.800 --> 01:56:05.560]   Whereas, I don't know, maybe you're right,
[01:56:05.560 --> 01:56:08.080]   but it's this sort of interesting pattern that--
[01:56:08.080 --> 01:56:10.840]   - Yeah, but I mean, I literally met Tristan at a conference
[01:56:10.840 --> 01:56:14.440]   and didn't have a scheduled meeting with her or anything,
[01:56:14.440 --> 01:56:16.600]   just joined a little group of people chatting,
[01:56:16.600 --> 01:56:17.680]   and he happened to be standing there,
[01:56:17.680 --> 01:56:19.800]   and I happened to mention what I was working on.
[01:56:19.800 --> 01:56:21.240]   And that led to more conversations.
[01:56:21.240 --> 01:56:22.720]   And I think I probably would have applied
[01:56:22.720 --> 01:56:24.360]   to Anthropic at some point anyways,
[01:56:24.360 --> 01:56:26.840]   but I would have waited at least another year.
[01:56:26.840 --> 01:56:31.880]   It's still crazy to me that I can actually contribute
[01:56:31.880 --> 01:56:33.720]   to interpretability in a meaningful way.
[01:56:33.720 --> 01:56:35.600]   - I think there's an important aspect
[01:56:35.600 --> 01:56:37.960]   of shots on goal there, so to speak, right?
[01:56:37.960 --> 01:56:41.240]   Where you're even just choosing to go to conferences itself
[01:56:41.240 --> 01:56:42.800]   is like putting yourself in a position
[01:56:42.800 --> 01:56:45.920]   where luck is more likely to happen.
[01:56:45.920 --> 01:56:47.120]   And conversely, in my own situation,
[01:56:47.120 --> 01:56:49.880]   it was like doing all of this work independently
[01:56:49.880 --> 01:56:52.240]   and trying to produce and do interesting things
[01:56:52.240 --> 01:56:55.700]   was my own way of trying to manufacture luck, so to speak,
[01:56:55.700 --> 01:56:58.520]   and try and do something meaningful enough
[01:56:58.520 --> 01:56:59.400]   that it got noticed.
[01:56:59.400 --> 01:57:01.840]   - Given that you said you framed this in the context
[01:57:01.840 --> 01:57:03.480]   of they were trying to run this experiment
[01:57:03.480 --> 01:57:04.480]   of can something-
[01:57:04.480 --> 01:57:06.160]   - So it was specifically James,
[01:57:06.160 --> 01:57:07.160]   and I think our manager, Brennan,
[01:57:07.160 --> 01:57:08.840]   was trying to run this experiment.
[01:57:08.840 --> 01:57:09.840]   - It worked.
[01:57:09.840 --> 01:57:10.660]   Did they do it again?
[01:57:10.660 --> 01:57:14.480]   - Yeah, so my closest collaborator, Enrique,
[01:57:14.480 --> 01:57:18.840]   he crossed from search to our team.
[01:57:18.840 --> 01:57:20.560]   He's also been ridiculously impactful.
[01:57:20.560 --> 01:57:23.200]   He's definitely a stronger engineer than I am,
[01:57:23.200 --> 01:57:24.760]   and he didn't go to university.
[01:57:24.760 --> 01:57:27.760]   - What was notable about, for example,
[01:57:27.760 --> 01:57:30.440]   is James Bradbury is somebody who's...
[01:57:30.440 --> 01:57:33.080]   Usually this kind of stuff is farmed out to recruiters
[01:57:33.080 --> 01:57:34.040]   or something like that,
[01:57:34.040 --> 01:57:36.880]   whereas James is somebody whose time is worth
[01:57:36.880 --> 01:57:38.040]   hundreds of millions of dollars.
[01:57:38.040 --> 01:57:39.440]   (all laughing)
[01:57:39.440 --> 01:57:40.480]   You know what I mean?
[01:57:40.480 --> 01:57:46.720]   That thing is very bottlenecked on that kind of person
[01:57:46.720 --> 01:57:49.480]   taking the time almost in an aristocratic tutoring sense
[01:57:49.480 --> 01:57:53.760]   of finding and then getting up to speed,
[01:57:53.760 --> 01:57:55.600]   and it seems like if it worked this well,
[01:57:55.600 --> 01:57:56.760]   it should be done at scale.
[01:57:56.760 --> 01:57:59.560]   It should be the responsibility of key people
[01:57:59.560 --> 01:58:02.720]   to, you know what I mean, onboard and find.
[01:58:02.720 --> 01:58:04.720]   - I think that is true to many extents.
[01:58:04.720 --> 01:58:06.240]   I'm sure you've probably benefited a lot
[01:58:06.240 --> 01:58:09.280]   from the key researchers mentoring you during--
[01:58:09.280 --> 01:58:13.440]   - And actively looking on open source repositories
[01:58:13.440 --> 01:58:16.500]   or on forums or whatever for potential people like this.
[01:58:16.500 --> 01:58:19.080]   - Yeah, I mean, James has Twitter injected into his brain.
[01:58:19.080 --> 01:58:21.660]   (all laughing)
[01:58:21.660 --> 01:58:27.120]   But yes, and I think this is something
[01:58:27.120 --> 01:58:28.520]   which in practice is done.
[01:58:28.520 --> 01:58:31.440]   People do look out for people that they find interesting
[01:58:31.440 --> 01:58:33.440]   and try and find high signal.
[01:58:33.440 --> 01:58:37.000]   In fact, actually, I was talking about this with Jeff
[01:58:37.000 --> 01:58:39.400]   the other day, and Jeff said that, yeah, he's like,
[01:58:39.400 --> 01:58:43.000]   you know, one of the most important hires I ever made
[01:58:43.000 --> 01:58:45.440]   was off a cold email.
[01:58:45.440 --> 01:58:46.600]   And I was like, well, who was that?
[01:58:46.600 --> 01:58:48.200]   And he's, Chris Ola.
[01:58:48.200 --> 01:58:49.520]   - Ah, yeah.
[01:58:49.520 --> 01:58:54.160]   - Because Chris similarly had no background in,
[01:58:54.160 --> 01:58:57.720]   well, like, no formal background in ML, right?
[01:58:57.720 --> 01:58:59.160]   And like Google Brain was just getting started
[01:58:59.160 --> 01:59:01.000]   in this kind of thing.
[01:59:01.000 --> 01:59:03.040]   But Jeff saw that signal.
[01:59:03.040 --> 01:59:06.400]   And the residency program, which like Brain had,
[01:59:06.400 --> 01:59:10.400]   is I think also, it was astonishingly effective
[01:59:10.400 --> 01:59:13.200]   at finding good people that didn't have
[01:59:13.200 --> 01:59:14.700]   strong ML backgrounds.
[01:59:14.700 --> 01:59:18.080]   - Yeah.
[01:59:18.080 --> 01:59:20.360]   One of the other things that I wanna emphasize
[01:59:20.360 --> 01:59:22.680]   for a potential slice of the audience
[01:59:22.680 --> 01:59:24.720]   that would be relevant to is,
[01:59:24.720 --> 01:59:29.400]   there's this sense that like the world is legible
[01:59:29.400 --> 01:59:34.400]   and efficient, companies have these go to jobs.google.com
[01:59:34.400 --> 01:59:37.360]   or jobs.whatevercompany.com, and you apply,
[01:59:37.360 --> 01:59:39.800]   and there's the steps, and like,
[01:59:39.800 --> 01:59:42.160]   they will evaluate you efficiently on those steps.
[01:59:42.160 --> 01:59:45.360]   Whereas not only from the storage teams,
[01:59:45.360 --> 01:59:47.760]   like often that's not the way it happens.
[01:59:47.760 --> 01:59:49.200]   That's in fact, it's good for the world,
[01:59:49.200 --> 01:59:50.360]   and that's not often how it happens.
[01:59:50.360 --> 01:59:52.760]   Like it is important to look at,
[01:59:52.760 --> 01:59:55.160]   were they able to like write an interesting block,
[01:59:55.160 --> 01:59:57.080]   technical blog post about their research,
[01:59:57.080 --> 01:59:59.280]   or like make an interesting contributions.
[01:59:59.280 --> 02:00:02.280]   Yeah, I want you to like riff on,
[02:00:02.280 --> 02:00:05.560]   for the people who are like,
[02:00:05.560 --> 02:00:07.920]   assuming that the other end of the job board
[02:00:07.920 --> 02:00:10.280]   is like just like super legible and mechanical.
[02:00:10.280 --> 02:00:11.200]   This is not how it works.
[02:00:11.200 --> 02:00:13.200]   And in fact, like people are looking for
[02:00:13.200 --> 02:00:14.760]   the sort of different way,
[02:00:14.760 --> 02:00:16.040]   different kind of person who's agentic
[02:00:16.040 --> 02:00:16.880]   and putting stuff out there.
[02:00:16.880 --> 02:00:19.240]   - And I think specifically what people are looking for,
[02:00:19.240 --> 02:00:20.160]   there is two things.
[02:00:20.160 --> 02:00:23.120]   One is agency, and like putting yourself out there.
[02:00:23.120 --> 02:00:26.840]   And the second is the ability to do world-class something.
[02:00:26.840 --> 02:00:31.000]   - Yeah, and two examples that I always like to point to here
[02:00:31.000 --> 02:00:33.480]   are Andy Jones from Anthropic,
[02:00:33.480 --> 02:00:37.600]   did an amazing paper on scaling laws
[02:00:37.600 --> 02:00:38.440]   as applied to board games.
[02:00:38.440 --> 02:00:39.520]   It didn't require much resources,
[02:00:39.520 --> 02:00:40.960]   it demonstrated incredible engineering skill,
[02:00:40.960 --> 02:00:42.480]   it demonstrated incredible understanding
[02:00:42.480 --> 02:00:44.560]   of like the most topical problem of the time.
[02:00:44.560 --> 02:00:47.600]   And he didn't come from like typical academic background
[02:00:47.600 --> 02:00:49.120]   or whatever, as I understand it basically,
[02:00:49.120 --> 02:00:50.760]   like as soon as he came out with that paper,
[02:00:50.760 --> 02:00:52.200]   both Anthropic and OpenAI were like,
[02:00:52.200 --> 02:00:54.640]   we would desperately like to hire you.
[02:00:54.640 --> 02:00:56.120]   There's also someone who works
[02:00:56.120 --> 02:00:59.160]   on Anthropic's performance team, now Simon Bohm,
[02:00:59.160 --> 02:01:01.120]   who has written, in my mind,
[02:01:01.120 --> 02:01:04.520]   the reference for optimizing a CUDA map model,
[02:01:04.520 --> 02:01:05.920]   like on a GPU.
[02:01:05.920 --> 02:01:09.480]   And that demonstrated example
[02:01:09.480 --> 02:01:12.200]   of like taking some like prompt effectively
[02:01:12.200 --> 02:01:15.760]   and producing the world-class reference example for it
[02:01:15.760 --> 02:01:18.560]   in something that wasn't particularly well done so far,
[02:01:18.560 --> 02:01:20.040]   is like I think an incredible demonstration
[02:01:20.040 --> 02:01:21.360]   of like ability and agency
[02:01:22.360 --> 02:01:24.800]   that in my mind would like be an immediate,
[02:01:24.800 --> 02:01:27.560]   would like please love to like interview/hire.
[02:01:27.560 --> 02:01:29.400]   - Yeah, the only thing I can add here is,
[02:01:29.400 --> 02:01:31.480]   I mean, I still had to go through the whole hiring process
[02:01:31.480 --> 02:01:33.360]   and all the standard interviews and this sort of thing.
[02:01:33.360 --> 02:01:34.440]   - Yeah, everyone does, everyone does.
[02:01:34.440 --> 02:01:37.440]   - Wait, isn't that seems stupid?
[02:01:37.440 --> 02:01:39.160]   - I mean--
[02:01:39.160 --> 02:01:40.320]   - It's important de-biasing.
[02:01:40.320 --> 02:01:41.160]   - Yeah, yeah, yeah.
[02:01:41.160 --> 02:01:43.040]   - And like the bias is what you want, right?
[02:01:43.040 --> 02:01:45.280]   Like you want the bias of somebody who's got great taste
[02:01:45.280 --> 02:01:47.280]   and like, he's like, who cares?
[02:01:47.280 --> 02:01:48.720]   - Your interview process should be able
[02:01:48.720 --> 02:01:50.640]   to disambiguate that as well.
[02:01:50.640 --> 02:01:52.080]   - Yeah, like I think there are cases
[02:01:52.080 --> 02:01:53.520]   where someone seems really great
[02:01:53.520 --> 02:01:55.200]   and then it's like, oh, they actually just can't code,
[02:01:55.200 --> 02:01:56.040]   this sort of thing, right?
[02:01:56.040 --> 02:01:57.440]   Like how much you weight these things
[02:01:57.440 --> 02:01:58.320]   definitely matters though.
[02:01:58.320 --> 02:02:01.920]   And like, I think the, we take references really seriously,
[02:02:01.920 --> 02:02:04.520]   the interviews you can only get so much signal from.
[02:02:04.520 --> 02:02:07.680]   And so it's all these other things that can come into play
[02:02:07.680 --> 02:02:09.480]   for whether or not a hire makes sense.
[02:02:09.480 --> 02:02:11.080]   - But you should design your interviews
[02:02:11.080 --> 02:02:14.800]   such that like they test the right things.
[02:02:14.800 --> 02:02:17.040]   - One man's bias is another man's taste, you know?
[02:02:17.040 --> 02:02:19.280]   (laughing)
[02:02:20.800 --> 02:02:22.280]   - Yeah, I guess the only thing I would add to this
[02:02:22.280 --> 02:02:25.120]   or maybe to the headstrong context is like,
[02:02:25.120 --> 02:02:27.280]   there's this line, the system is not your friend.
[02:02:27.280 --> 02:02:28.280]   - Right.
[02:02:28.280 --> 02:02:29.520]   - And it's not necessarily to say
[02:02:29.520 --> 02:02:33.320]   it's actively against you or it's your sworn enemy.
[02:02:33.320 --> 02:02:36.400]   It's just not looking out for you.
[02:02:36.400 --> 02:02:37.240]   - Right.
[02:02:37.240 --> 02:02:39.760]   - And so I think that's where a lot of the proactiveness
[02:02:39.760 --> 02:02:42.560]   comes in of like, there are no adults in the room
[02:02:42.560 --> 02:02:46.560]   or like, and like you have to come to some decision
[02:02:46.560 --> 02:02:47.840]   for what you want your life to look like
[02:02:47.840 --> 02:02:48.880]   and then execute on it.
[02:02:48.880 --> 02:02:51.920]   And yeah, hopefully you can then update later
[02:02:51.920 --> 02:02:53.760]   if you're too headstrong in the wrong way.
[02:02:53.760 --> 02:02:56.400]   But I think you almost have to just kind of charge
[02:02:56.400 --> 02:02:57.240]   at certain things.
[02:02:57.240 --> 02:02:58.080]   - Right.
[02:02:58.080 --> 02:02:59.520]   - To get much of anything done,
[02:02:59.520 --> 02:03:01.120]   not be swept up in the tide of whatever
[02:03:01.120 --> 02:03:02.680]   the expectations are.
[02:03:02.680 --> 02:03:04.480]   - There's like one final thing I want to add,
[02:03:04.480 --> 02:03:05.920]   which is like, we talked a lot about agency
[02:03:05.920 --> 02:03:06.760]   and this kind of stuff.
[02:03:06.760 --> 02:03:08.760]   But I think actually like surprisingly enough,
[02:03:08.760 --> 02:03:09.800]   one of the most important things
[02:03:09.800 --> 02:03:13.720]   is just carrying an unbelievable amount.
[02:03:13.720 --> 02:03:14.560]   - Right.
[02:03:14.560 --> 02:03:15.800]   - And when you care an unbelievable amount,
[02:03:15.800 --> 02:03:17.320]   you'd like, you check all the details
[02:03:17.320 --> 02:03:18.880]   and you have like this understanding
[02:03:18.880 --> 02:03:19.960]   of like what could have gone wrong.
[02:03:19.960 --> 02:03:24.440]   And you'd like, it just, it matters more than you think
[02:03:24.440 --> 02:03:27.320]   because people end up not caring.
[02:03:27.320 --> 02:03:28.320]   - Sure.
[02:03:28.320 --> 02:03:29.560]   - Not caring enough.
[02:03:29.560 --> 02:03:32.640]   This is like LeBron quote, where he talks about how
[02:03:32.640 --> 02:03:35.040]   when he sort of, before he started in the league,
[02:03:35.040 --> 02:03:36.400]   he was like worried that everyone would be like
[02:03:36.400 --> 02:03:37.240]   incredibly good.
[02:03:37.240 --> 02:03:38.840]   And then he gets there and he like realized
[02:03:38.840 --> 02:03:40.600]   that actually once people hit financial stability,
[02:03:40.600 --> 02:03:42.840]   then they like, they relax a bit.
[02:03:42.840 --> 02:03:44.960]   And he's like, oh, this is going to be easy.
[02:03:44.960 --> 02:03:46.160]   I don't think that's quite true
[02:03:46.160 --> 02:03:47.560]   because I think in like AI research,
[02:03:47.560 --> 02:03:49.120]   'cause most people actually care quite deeply.
[02:03:49.120 --> 02:03:50.080]   - Yeah.
[02:03:50.080 --> 02:03:52.480]   - But there's caring about your problem
[02:03:52.480 --> 02:03:54.240]   and there's also just caring about the entire stack
[02:03:54.240 --> 02:03:55.280]   and everything that goes up and down,
[02:03:55.280 --> 02:03:57.360]   like going explicitly going and fixing things
[02:03:57.360 --> 02:03:59.000]   that aren't your responsibility to fix
[02:03:59.000 --> 02:04:02.040]   because overall it makes like the stack better.
[02:04:02.040 --> 02:04:04.440]   - I mean, another part of that I forgot to mention is
[02:04:04.440 --> 02:04:06.880]   you were mentioning, oh, going in on weekends
[02:04:06.880 --> 02:04:08.920]   and on Christmas break and you get to like,
[02:04:08.920 --> 02:04:10.320]   the only people in the office are Jeff Dean
[02:04:10.320 --> 02:04:11.520]   and Sergey Brin or something.
[02:04:11.520 --> 02:04:12.360]   (laughing)
[02:04:12.360 --> 02:04:15.360]   And you just like get to pair a program with them.
[02:04:15.360 --> 02:04:18.680]   It's just, it's interesting to me that people,
[02:04:18.680 --> 02:04:20.080]   I don't want to pick on your company in particular,
[02:04:20.080 --> 02:04:21.400]   but like people at any big company,
[02:04:21.400 --> 02:04:24.320]   they've gotten there because they've gone through
[02:04:24.320 --> 02:04:26.600]   a very selective process.
[02:04:26.600 --> 02:04:28.440]   That's like, they had to compete in high school,
[02:04:28.440 --> 02:04:29.840]   they had to compete in college,
[02:04:29.840 --> 02:04:31.960]   but it almost seems like they get there
[02:04:31.960 --> 02:04:33.320]   and then they take it easy.
[02:04:33.320 --> 02:04:35.840]   When in fact, this is a time to put the pedal to the metal,
[02:04:35.840 --> 02:04:38.280]   go in and pair program with Sergey Brin on the weekends
[02:04:38.280 --> 02:04:39.360]   or whatever, you know what I mean?
[02:04:39.360 --> 02:04:41.720]   - I mean, there's pros and cons there, right?
[02:04:41.720 --> 02:04:43.520]   I think many people make the decision
[02:04:43.520 --> 02:04:44.880]   that the thing that they want to prioritize
[02:04:44.880 --> 02:04:47.360]   is like a wonderful life with their family.
[02:04:47.360 --> 02:04:49.640]   And if they do wonderful work,
[02:04:49.640 --> 02:04:52.320]   like let's say they don't work every hour of the day, right?
[02:04:52.320 --> 02:04:53.960]   But they do wonderful work in the work,
[02:04:53.960 --> 02:04:55.160]   like the hours that they do do,
[02:04:55.160 --> 02:04:56.560]   that's incredibly impactful.
[02:04:56.560 --> 02:04:59.160]   I think this is true for many people at Google
[02:04:59.160 --> 02:05:01.200]   is like, maybe they don't work as many hours
[02:05:01.200 --> 02:05:03.760]   as like your typical startup mythologies, right?
[02:05:03.760 --> 02:05:05.960]   But the work that they do do is incredibly valuable.
[02:05:05.960 --> 02:05:07.760]   It's very high leverage because they know the systems
[02:05:07.760 --> 02:05:09.040]   and they're experts in their field.
[02:05:09.040 --> 02:05:12.160]   And we also need people like that.
[02:05:12.160 --> 02:05:15.200]   Like our world rests on these huge,
[02:05:15.200 --> 02:05:17.840]   like difficult to manage and difficult to fix systems.
[02:05:17.840 --> 02:05:21.120]   And we need people who are like willing to work on
[02:05:21.120 --> 02:05:22.760]   and help and fix and maintain those,
[02:05:22.760 --> 02:05:24.160]   in frankly, like a thankless way
[02:05:24.160 --> 02:05:25.720]   that isn't as like high publicity
[02:05:25.720 --> 02:05:28.600]   as all of this AI work that we're doing, right?
[02:05:28.600 --> 02:05:30.840]   And I'm like ridiculously grateful that those people do it.
[02:05:30.840 --> 02:05:33.360]   And I'm also happy that there are people for whom like,
[02:05:33.360 --> 02:05:36.360]   okay, they find technical fulfillment in their job
[02:05:36.360 --> 02:05:37.200]   and doing that well.
[02:05:37.200 --> 02:05:38.640]   And also like maybe they draw a lot more from it
[02:05:38.640 --> 02:05:41.480]   also out of spending like a lot of hours with their family.
[02:05:41.480 --> 02:05:43.360]   And I'm lucky that I'm at a stage in my life where like,
[02:05:43.360 --> 02:05:45.040]   yeah, I can go in and work every hour of the week.
[02:05:45.040 --> 02:05:47.960]   But like, that's like,
[02:05:47.960 --> 02:05:50.520]   I'm not making as many sacrifices to do that.
[02:05:50.520 --> 02:05:52.880]   - Yeah.
[02:05:52.880 --> 02:05:55.760]   I mean, like just one example that sticks out in my mind
[02:05:55.760 --> 02:05:59.760]   of this sort of like the other side says no,
[02:05:59.760 --> 02:06:02.480]   and you can still get the yes on the other end.
[02:06:02.480 --> 02:06:04.160]   Basically every single high profile of guests
[02:06:04.160 --> 02:06:05.400]   I've gotten so far,
[02:06:05.400 --> 02:06:07.920]   I think maybe with one or two exceptions,
[02:06:07.920 --> 02:06:10.040]   I've sat down for a week and I've just come up
[02:06:10.040 --> 02:06:11.760]   with a list of sample questions
[02:06:11.760 --> 02:06:13.440]   that's like trying to really come up
[02:06:13.440 --> 02:06:16.280]   with really smart questions to send to them.
[02:06:16.280 --> 02:06:18.720]   And the entire process I've always thought like,
[02:06:18.720 --> 02:06:22.400]   if I just cold email them, it's like a 2% chance they say yes.
[02:06:22.400 --> 02:06:25.480]   If I include this list, there's a 10% chance.
[02:06:25.480 --> 02:06:27.560]   And because otherwise, you know, there's like,
[02:06:27.560 --> 02:06:30.120]   you go through their inbox and every 34 seconds,
[02:06:30.120 --> 02:06:32.040]   there's an interview for whatever podcast,
[02:06:32.040 --> 02:06:34.240]   interview for whatever podcast.
[02:06:34.240 --> 02:06:36.920]   And every single time I've done this, they've said yes.
[02:06:36.920 --> 02:06:37.760]   - Wow, yeah.
[02:06:37.760 --> 02:06:38.600]   Exactly.
[02:06:38.640 --> 02:06:40.000]   That's great questions.
[02:06:40.000 --> 02:06:41.840]   But if you do everything, you'll win.
[02:06:41.840 --> 02:06:43.600]   - But you just like, you literally have to dig
[02:06:43.600 --> 02:06:45.560]   in the same hole for like 10 minutes,
[02:06:45.560 --> 02:06:47.400]   or in that case, like make a list of sample questions
[02:06:47.400 --> 02:06:50.040]   for them to get past, they're not an idiot list.
[02:06:50.040 --> 02:06:51.080]   You know what I mean?
[02:06:51.080 --> 02:06:53.240]   - Demonstrate how much you care.
[02:06:53.240 --> 02:06:55.400]   And the work you're willing to put in.
[02:06:55.400 --> 02:06:56.240]   Yeah.
[02:06:56.240 --> 02:06:57.080]   - Yeah.
[02:06:57.080 --> 02:06:58.360]   Something that a friend said to me a while back,
[02:06:58.360 --> 02:07:00.320]   but I think it's stuck is like,
[02:07:00.320 --> 02:07:02.760]   it's amazing how quickly you can become world-class
[02:07:02.760 --> 02:07:05.600]   at something just because most people aren't trying that hard
[02:07:05.600 --> 02:07:07.640]   and like are only working like, I don't know,
[02:07:07.640 --> 02:07:10.280]   the actual like 20 hours that they're actually spending
[02:07:10.280 --> 02:07:11.920]   on this thing or something.
[02:07:11.920 --> 02:07:14.200]   And so, yeah, if you just go ham,
[02:07:14.200 --> 02:07:16.960]   then like you can get really far pretty fast.
[02:07:16.960 --> 02:07:18.520]   - And I think I'm lucky I had that experience
[02:07:18.520 --> 02:07:19.640]   with the fencing as well.
[02:07:19.640 --> 02:07:21.680]   Like I had the experience of becoming world-class
[02:07:21.680 --> 02:07:23.920]   in something and like knowing that you just worked
[02:07:23.920 --> 02:07:25.640]   really, really hard and went back.
[02:07:25.640 --> 02:07:26.480]   - Yeah.
[02:07:26.480 --> 02:07:30.000]   For context, by the way, Sholto was one seat away
[02:07:30.000 --> 02:07:31.640]   as he was the next person in line
[02:07:31.640 --> 02:07:34.480]   to go to the Olympic stuff for fencing.
[02:07:34.480 --> 02:07:37.240]   - I was at best like 42nd in the world for fencing,
[02:07:37.240 --> 02:07:38.360]   for men's foil fencing.
[02:07:38.360 --> 02:07:41.560]   - Mutational load is a thing, man.
[02:07:41.560 --> 02:07:44.760]   (all laughing)
[02:07:44.760 --> 02:07:47.880]   - And there was one cycle where, yeah,
[02:07:47.880 --> 02:07:50.080]   I was like the next highest ranked person in Asia.
[02:07:50.080 --> 02:07:54.600]   And if one of the teams had been like disqualified
[02:07:54.600 --> 02:07:58.340]   for doping as it was occurring in part during that cycle,
[02:07:58.340 --> 02:08:02.000]   and as occurred for like the Australian women's rowing team,
[02:08:02.000 --> 02:08:04.640]   I think went because one of the teams was disqualified,
[02:08:04.640 --> 02:08:06.440]   then I would have been the next in line.
[02:08:07.440 --> 02:08:09.480]   It's interesting when like you just like find out
[02:08:09.480 --> 02:08:12.160]   about people's prior lives and it's like,
[02:08:12.160 --> 02:08:13.880]   oh, you know, this guy was almost an Olympian,
[02:08:13.880 --> 02:08:16.840]   this other guy was whatever, you know what I mean?
[02:08:16.840 --> 02:08:19.040]   Okay, let's talk about interoperability.
[02:08:19.040 --> 02:08:19.880]   - Yeah.
[02:08:19.880 --> 02:08:22.760]   - I actually want to stay on the brain stuff
[02:08:22.760 --> 02:08:24.600]   as a way to get into it for a second.
[02:08:24.600 --> 02:08:30.760]   We were previously discussing is the brain organized
[02:08:30.760 --> 02:08:32.960]   in the way where you have a residual stream
[02:08:32.960 --> 02:08:37.960]   that is gradually refined with higher level associations
[02:08:37.960 --> 02:08:39.120]   over time or something.
[02:08:39.120 --> 02:08:44.240]   There's a fixed dimension size in a model.
[02:08:44.240 --> 02:08:47.800]   If you had to, I don't even know how to ask this question
[02:08:47.800 --> 02:08:51.560]   in a sensible way, but what is the D model of the brain?
[02:08:51.560 --> 02:08:53.600]   What is it like the embedding size of,
[02:08:53.600 --> 02:08:54.720]   or because of feature splitting,
[02:08:54.720 --> 02:08:56.320]   is that not a sensible question?
[02:08:56.320 --> 02:08:59.040]   - No, I think it's a sensible question.
[02:08:59.040 --> 02:09:01.040]   Well, it is a question.
[02:09:01.040 --> 02:09:02.720]   (all laughing)
[02:09:02.720 --> 02:09:05.560]   - You could've just not said that either way.
[02:09:05.560 --> 02:09:06.600]   - No, I don't like this question.
[02:09:06.600 --> 02:09:09.040]   - You can talk, just like actively.
[02:09:09.040 --> 02:09:12.160]   - I'm trying to, I don't know how you would begin
[02:09:12.160 --> 02:09:15.400]   to kind of be like, okay, well, this part of the brain
[02:09:15.400 --> 02:09:17.720]   is like a vector of this dimensionality.
[02:09:17.720 --> 02:09:19.320]   I mean, maybe for the visual stream,
[02:09:19.320 --> 02:09:23.360]   because it's like V1 to V2 to IT, whatever,
[02:09:23.360 --> 02:09:25.400]   you could just count the number of neurons that are there
[02:09:25.400 --> 02:09:27.000]   and be like, that is the dimensionality,
[02:09:27.000 --> 02:09:30.400]   but it seems more likely that there are kind of sub-modules
[02:09:30.400 --> 02:09:31.560]   and things are divided up.
[02:09:31.560 --> 02:09:34.120]   So yeah, I don't have,
[02:09:34.120 --> 02:09:39.040]   and I'm not the world's greatest neuroscientist, right?
[02:09:39.040 --> 02:09:40.160]   Like I did it for a few years.
[02:09:40.160 --> 02:09:42.600]   I studied the cerebellum quite a bit.
[02:09:42.600 --> 02:09:43.440]   So I'm sure there are people
[02:09:43.440 --> 02:09:45.400]   who could give you a better answer on this.
[02:09:45.400 --> 02:09:51.720]   Do you think that the way to think about
[02:09:51.720 --> 02:09:55.720]   whether it's in the brain or whether it's in these models,
[02:09:55.720 --> 02:09:58.400]   fundamentally what's happening is features are added,
[02:09:58.400 --> 02:10:01.840]   removed, changed, and like the feature
[02:10:01.840 --> 02:10:05.520]   is the fundamental unit of what is happening in the model.
[02:10:05.520 --> 02:10:08.000]   Like what would have to be true for,
[02:10:08.000 --> 02:10:10.440]   give me a, and this goes back to the earlier thing
[02:10:10.440 --> 02:10:11.320]   we were talking about,
[02:10:11.320 --> 02:10:14.480]   whether it's just associations all the way down.
[02:10:14.480 --> 02:10:15.960]   Give me like a counterfactual.
[02:10:15.960 --> 02:10:17.640]   In the world where this is not true,
[02:10:17.640 --> 02:10:19.200]   what is happening instead?
[02:10:19.200 --> 02:10:21.800]   Like what is the alternative hypothesis here?
[02:10:21.800 --> 02:10:23.360]   - Yeah, it's hard for me to think about,
[02:10:23.360 --> 02:10:25.680]   'cause at this point I just think so much
[02:10:25.680 --> 02:10:27.200]   in terms of this feature space.
[02:10:27.400 --> 02:10:32.560]   I mean, at one point there was like
[02:10:32.560 --> 02:10:36.560]   the kind of behavioralist approach towards cognition
[02:10:36.560 --> 02:10:40.320]   where, or it's like, you're just,
[02:10:40.320 --> 02:10:41.720]   you're like input output,
[02:10:41.720 --> 02:10:44.120]   but you're not really doing any processing.
[02:10:44.120 --> 02:10:45.840]   Or it's like everything is embodied
[02:10:45.840 --> 02:10:47.640]   and you're just like a dynamical system
[02:10:47.640 --> 02:10:52.440]   that's like operating along like some predictable equations.
[02:10:52.440 --> 02:10:55.240]   But like, there's no state in the system, I guess.
[02:10:56.280 --> 02:10:59.040]   But whenever I've read these sorts of critiques,
[02:10:59.040 --> 02:11:01.080]   it's like, well, you're just choosing
[02:11:01.080 --> 02:11:02.720]   to not call this thing a state,
[02:11:02.720 --> 02:11:04.480]   but you could call like any internal component
[02:11:04.480 --> 02:11:05.320]   of the model a state.
[02:11:05.320 --> 02:11:07.120]   Like even with the feature discussion,
[02:11:07.120 --> 02:11:10.600]   it's defining what a feature is, is really hard.
[02:11:10.600 --> 02:11:14.620]   And so the question feels almost too slippery.
[02:11:14.620 --> 02:11:16.960]   - What is a feature?
[02:11:16.960 --> 02:11:20.280]   - A direction and activation space.
[02:11:20.280 --> 02:11:25.920]   A latent variable that is operating behind the scenes
[02:11:25.920 --> 02:11:28.920]   that has like causal influence
[02:11:28.920 --> 02:11:30.760]   over the system you're observing.
[02:11:30.760 --> 02:11:34.120]   It's a feature if you call it a feature.
[02:11:34.120 --> 02:11:35.120]   It's tautological.
[02:11:35.120 --> 02:11:38.160]   I mean, these are all explanations
[02:11:38.160 --> 02:11:41.080]   that I like, I feel some association.
[02:11:41.080 --> 02:11:43.240]   - In a very rough, intuitive sense,
[02:11:43.240 --> 02:11:46.200]   in like a sufficiently sparse and binary vector,
[02:11:46.200 --> 02:11:47.520]   features like whether or not
[02:11:47.520 --> 02:11:49.080]   something's turned on or off, right?
[02:11:49.080 --> 02:11:51.200]   Like in a very simplistic sense.
[02:11:51.200 --> 02:11:52.680]   Which might be, I think, a useful metaphor
[02:11:52.680 --> 02:11:53.560]   to understand it by.
[02:11:53.560 --> 02:11:56.280]   It's like when we talk about features activating,
[02:11:56.280 --> 02:11:57.960]   it is in many respects the same way
[02:11:57.960 --> 02:11:58.800]   that neuroscientists would talk
[02:11:58.800 --> 02:12:02.160]   about like a neuron activating, right?
[02:12:02.160 --> 02:12:03.960]   - If that neuron corresponds to--
[02:12:03.960 --> 02:12:04.800]   - To something in particular.
[02:12:04.800 --> 02:12:06.080]   - Right, yeah, yeah, yeah, yeah.
[02:12:06.080 --> 02:12:07.960]   And no, I think that's useful as like,
[02:12:07.960 --> 02:12:09.720]   what do we want a feature to be, right?
[02:12:09.720 --> 02:12:11.200]   Like what is a synthetic problem
[02:12:11.200 --> 02:12:12.720]   under which a feature exists?
[02:12:12.720 --> 02:12:16.400]   But even with the, towards monosemanticity work,
[02:12:16.400 --> 02:12:18.020]   we talk about what's called feature splitting,
[02:12:18.020 --> 02:12:20.840]   which is basically, you will find as many features
[02:12:20.840 --> 02:12:23.720]   as you give the model the capacity to learn.
[02:12:23.720 --> 02:12:27.680]   And by model here, I mean the up-projection
[02:12:27.680 --> 02:12:31.040]   that we fit after we trained the original model.
[02:12:31.040 --> 02:12:33.320]   And so, if you don't give it much capacity,
[02:12:33.320 --> 02:12:35.760]   it'll learn a feature for bird.
[02:12:35.760 --> 02:12:37.080]   But if you give it more capacity,
[02:12:37.080 --> 02:12:39.720]   then it will learn like ravens and eagles and sparrows
[02:12:39.720 --> 02:12:41.480]   and like specific types of birds.
[02:12:41.480 --> 02:12:44.920]   - Still on definitions thing.
[02:12:44.920 --> 02:12:49.920]   I guess, naively, I think of things like bird
[02:12:50.920 --> 02:12:55.400]   versus what kind of token, is it like a period
[02:12:55.400 --> 02:12:57.960]   at the end of a hyperlink, as you were talking about earlier,
[02:12:57.960 --> 02:13:02.960]   versus at the highest level, things like love or deception
[02:13:02.960 --> 02:13:07.440]   or like holding a very complicated proof
[02:13:07.440 --> 02:13:09.080]   in your head or something.
[02:13:09.080 --> 02:13:09.980]   Is this all features?
[02:13:09.980 --> 02:13:11.600]   'Cause then the definition seems so broad
[02:13:11.600 --> 02:13:13.740]   as to almost be not that useful.
[02:13:13.740 --> 02:13:17.760]   Like, or rather that there seems to be
[02:13:17.760 --> 02:13:19.360]   some important differences between these things,
[02:13:19.360 --> 02:13:20.720]   and they're all features.
[02:13:20.720 --> 02:13:23.440]   Like, yeah, I'm not sure what we even mean by.
[02:13:23.440 --> 02:13:26.920]   - I mean, all of those things are like discrete units
[02:13:26.920 --> 02:13:29.760]   that have connections to other things
[02:13:29.760 --> 02:13:31.520]   that then imbues them with meaning.
[02:13:31.520 --> 02:13:36.720]   That feels like a specific enough definition
[02:13:36.720 --> 02:13:39.680]   that it's useful or not too all-encompassing,
[02:13:39.680 --> 02:13:40.640]   but feel free to push back.
[02:13:40.640 --> 02:13:42.600]   - Well, like what would you discover tomorrow
[02:13:42.600 --> 02:13:45.720]   in, that could make you think like,
[02:13:45.720 --> 02:13:47.560]   oh, this is like kind of fundamentally the wrong way
[02:13:47.560 --> 02:13:49.400]   to think about what's happening in the model?
[02:13:49.400 --> 02:13:53.560]   - I mean, if the features we were finding weren't predictive
[02:13:53.560 --> 02:13:58.340]   or if they were just representations of the data, right?
[02:13:58.340 --> 02:14:00.680]   Where it's like, oh, all you're doing
[02:14:00.680 --> 02:14:02.440]   is just clustering your data.
[02:14:02.440 --> 02:14:06.440]   And there's no like higher level associations
[02:14:06.440 --> 02:14:07.480]   that are being made.
[02:14:07.480 --> 02:14:11.520]   Or it's some like phenomenological thing of like,
[02:14:11.520 --> 02:14:14.640]   you're saying that this feature files for marriage,
[02:14:14.640 --> 02:14:17.460]   but if you activate it really strongly,
[02:14:17.460 --> 02:14:19.120]   it doesn't change the outputs of the model
[02:14:19.120 --> 02:14:20.520]   in a way that would correspond to it.
[02:14:20.520 --> 02:14:23.360]   Like, I think these would both be good critiques.
[02:14:23.360 --> 02:14:25.880]   I guess one more is,
[02:14:25.880 --> 02:14:28.200]   and we tried to do experiments on MNIST,
[02:14:28.200 --> 02:14:31.040]   which is a data set of digits, images,
[02:14:31.040 --> 02:14:33.180]   and we didn't look super hard into it.
[02:14:33.180 --> 02:14:34.600]   And so I'd be interested if people,
[02:14:34.600 --> 02:14:37.320]   other people wanted to take up like a deeper investigation,
[02:14:37.320 --> 02:14:40.520]   but it's plausible that your like latent space
[02:14:40.520 --> 02:14:43.960]   of representations is dense and it's a manifold
[02:14:43.960 --> 02:14:45.960]   instead of being these discrete points.
[02:14:46.920 --> 02:14:49.400]   And so you could like move across the manifold,
[02:14:49.400 --> 02:14:52.760]   but at every point there would be some meaningful behavior.
[02:14:52.760 --> 02:14:56.160]   And it's much harder than to label things as features
[02:14:56.160 --> 02:14:57.060]   that are discrete.
[02:14:57.060 --> 02:15:00.960]   - Like in a naive sort of outsider way,
[02:15:00.960 --> 02:15:03.580]   the thing that would seem to me to be like
[02:15:03.580 --> 02:15:05.520]   a way in which this picture could be wrong
[02:15:05.520 --> 02:15:08.200]   is if there's not some like,
[02:15:08.200 --> 02:15:10.700]   this thing is turned on and turned off,
[02:15:10.700 --> 02:15:14.120]   but it's like a much more global kind of like,
[02:15:14.120 --> 02:15:17.600]   the system is, I don't know, I'm gonna use really clumsy,
[02:15:17.600 --> 02:15:20.360]   like, you know, I mentioned it in a pretty kind of language,
[02:15:20.360 --> 02:15:24.540]   but is there a good analogy here?
[02:15:24.540 --> 02:15:29.000]   Yeah, I guess if you think of like something
[02:15:29.000 --> 02:15:31.100]   like the laws of physics, it's not like,
[02:15:31.100 --> 02:15:34.420]   well, the feature for wetness is turned on,
[02:15:34.420 --> 02:15:35.600]   but it's only turned on this much,
[02:15:35.600 --> 02:15:38.100]   and then the feature for like, you know,
[02:15:38.100 --> 02:15:39.880]   I guess maybe it's true,
[02:15:39.880 --> 02:15:41.480]   'cause like the mass is like a gradient,
[02:15:41.480 --> 02:15:44.600]   and like, you know, like, I don't know,
[02:15:44.600 --> 02:15:47.720]   but the polarity or whatever is a gradient as well.
[02:15:47.720 --> 02:15:49.680]   But there's also a sense in which like there's the laws,
[02:15:49.680 --> 02:15:50.880]   and the laws are more general,
[02:15:50.880 --> 02:15:54.580]   and you have to understand like the general bigger picture,
[02:15:54.580 --> 02:15:56.040]   you don't get that from just like
[02:15:56.040 --> 02:15:59.360]   these like specific sub-circuits.
[02:15:59.360 --> 02:16:01.440]   - But that's where like the reasoning circuit itself
[02:16:01.440 --> 02:16:02.340]   comes into play, right?
[02:16:02.340 --> 02:16:04.080]   Where you're taking these features ideally,
[02:16:04.080 --> 02:16:06.560]   and like trying to compose them into something high level.
[02:16:06.560 --> 02:16:09.240]   Like you might say, okay, like when I'm using,
[02:16:09.240 --> 02:16:11.560]   at least this is my head canon.
[02:16:11.560 --> 02:16:13.200]   Let's say I'm trying to use the foot,
[02:16:13.200 --> 02:16:14.720]   you know, F equals MA, right?
[02:16:14.720 --> 02:16:16.360]   Then presumably at some point I have features
[02:16:16.360 --> 02:16:18.840]   which like denote, okay, like mass,
[02:16:18.840 --> 02:16:20.640]   and then that's like helping me retrieve the actual mass
[02:16:20.640 --> 02:16:21.620]   of the thing that I'm using,
[02:16:21.620 --> 02:16:24.780]   and then like the acceleration and this kind of stuff,
[02:16:24.780 --> 02:16:27.760]   but then also maybe there's a higher level feature
[02:16:27.760 --> 02:16:30.000]   that does correspond to using the first law of physics.
[02:16:30.000 --> 02:16:31.760]   Maybe, but the more important part
[02:16:31.760 --> 02:16:33.520]   is that the composition of components
[02:16:33.520 --> 02:16:36.520]   which helps me retrieve relevant pieces of information,
[02:16:36.520 --> 02:16:39.280]   and then produce like maybe some like a multiplication
[02:16:39.280 --> 02:16:41.720]   operator or something like that from when necessary.
[02:16:41.720 --> 02:16:43.120]   At least that's my head canon.
[02:16:43.120 --> 02:16:45.520]   - What is a compelling explanation to you,
[02:16:45.520 --> 02:16:48.020]   especially for very smart models of,
[02:16:48.020 --> 02:16:51.680]   like I understand why it made this output,
[02:16:51.680 --> 02:16:54.060]   and it was like for a legit reason.
[02:16:54.060 --> 02:16:57.100]   If it's doing million line pull requests or something,
[02:16:57.100 --> 02:16:58.880]   what are you seeing at the end of that request
[02:16:58.880 --> 02:17:01.160]   where you're like, yep, that's chill?
[02:17:01.160 --> 02:17:05.060]   - Yeah, so ideally you apply dictionary learning
[02:17:05.060 --> 02:17:05.960]   to the model.
[02:17:05.960 --> 02:17:07.520]   You've found features.
[02:17:07.520 --> 02:17:10.560]   Right now we're actively trying to get the same success
[02:17:10.560 --> 02:17:13.240]   for attention heads, in which case we have features
[02:17:13.240 --> 02:17:15.440]   for both the core, you can do it for residual stream,
[02:17:15.440 --> 02:17:18.520]   MLP, and attention throughout the whole model.
[02:17:18.520 --> 02:17:19.940]   Hopefully at that point you can also identify
[02:17:19.940 --> 02:17:21.800]   broader circuits through the model
[02:17:21.800 --> 02:17:25.080]   that are like more general reasoning abilities
[02:17:25.080 --> 02:17:26.440]   that will activate or not activate.
[02:17:26.440 --> 02:17:28.640]   But in your case where we're trying to figure out
[02:17:28.640 --> 02:17:31.080]   if this like pull request should be approved or not,
[02:17:31.080 --> 02:17:35.040]   I think you can flag or detect features
[02:17:35.040 --> 02:17:37.040]   that correspond to deceptive behavior,
[02:17:37.040 --> 02:17:39.160]   malicious behavior, these sorts of things,
[02:17:39.160 --> 02:17:41.280]   and see whether or not those have fired.
[02:17:41.280 --> 02:17:42.720]   That would be like an immediate,
[02:17:42.720 --> 02:17:44.920]   you can do more than that, but that would be an immediate.
[02:17:44.920 --> 02:17:46.560]   - But before I trace down on that,
[02:17:46.560 --> 02:17:49.320]   what does a reasoning circuit look like?
[02:17:49.320 --> 02:17:51.320]   What would that look like when you found it?
[02:17:51.320 --> 02:17:52.800]   - Yeah, so I mean the induction head
[02:17:52.800 --> 02:17:53.640]   is probably one of the simplest cases of this.
[02:17:53.640 --> 02:17:55.120]   - But that's not like reasoning, right?
[02:17:55.120 --> 02:17:57.120]   - Well, I mean, what do you call reasoning, right?
[02:17:57.120 --> 02:18:01.760]   Like it's a good reason,
[02:18:01.760 --> 02:18:03.760]   so I guess context for listeners,
[02:18:03.760 --> 02:18:05.260]   the induction head is basically,
[02:18:05.260 --> 02:18:08.160]   and you see the line like Mr. and Mrs. Dursley
[02:18:08.160 --> 02:18:10.920]   did something Mr. blank,
[02:18:10.920 --> 02:18:12.920]   and you're trying to predict what blank is.
[02:18:12.920 --> 02:18:15.800]   And the head has learned to look for previous occurrences
[02:18:15.800 --> 02:18:20.180]   of the word Mr., look at the word that comes after it,
[02:18:20.180 --> 02:18:22.380]   and then copy and paste that as the prediction
[02:18:22.380 --> 02:18:23.640]   for what should come next,
[02:18:23.640 --> 02:18:26.480]   which is a super reasonable thing to do.
[02:18:26.480 --> 02:18:28.760]   And there is computation being done there
[02:18:28.760 --> 02:18:31.520]   to accurately predict the next token.
[02:18:32.320 --> 02:18:35.640]   - Mm-hmm, that is context dependent.
[02:18:35.640 --> 02:18:36.960]   - That is, yeah.
[02:18:36.960 --> 02:18:40.240]   - But it's not like reasoning, you know what I mean?
[02:18:40.240 --> 02:18:43.620]   - But is, I guess going back to the associations
[02:18:43.620 --> 02:18:46.120]   all the way down, it's like if you chain together
[02:18:46.120 --> 02:18:48.880]   a bunch of these reasoning circuits
[02:18:48.880 --> 02:18:51.680]   or heads that have different rules
[02:18:51.680 --> 02:18:53.520]   for how to relate information.
[02:18:53.520 --> 02:18:56.080]   - But in this sort of like zero shot case,
[02:18:56.080 --> 02:18:59.520]   like something is happening where when you like
[02:18:59.520 --> 02:19:01.360]   pick up a new game and you immediately start understanding
[02:19:01.360 --> 02:19:02.360]   how to play it.
[02:19:02.360 --> 02:19:04.520]   And it doesn't seem like an induction heads kind of thing.
[02:19:04.520 --> 02:19:05.360]   Or like-
[02:19:05.360 --> 02:19:06.760]   - Well, I think there would be another circuit
[02:19:06.760 --> 02:19:10.040]   for like extracting pixels and turning them
[02:19:10.040 --> 02:19:13.360]   into latent representations of the different objects
[02:19:13.360 --> 02:19:14.520]   in the game, right?
[02:19:14.520 --> 02:19:17.120]   And like a circuit that is learning physics.
[02:19:17.120 --> 02:19:19.240]   - And what would that, because the induction heads
[02:19:19.240 --> 02:19:20.920]   is like one layer transformer.
[02:19:20.920 --> 02:19:23.520]   - Two layers, yeah, yeah, yeah.
[02:19:23.520 --> 02:19:26.200]   - So you can like kind of see like what the thing
[02:19:26.200 --> 02:19:28.980]   that is a human picks up a new game and understands it.
[02:19:28.980 --> 02:19:31.860]   How would you think about what that is?
[02:19:31.860 --> 02:19:34.540]   Is it presumably it's across multiple layers,
[02:19:34.540 --> 02:19:37.700]   but like, is it, yeah, yeah.
[02:19:37.700 --> 02:19:39.540]   What would that physically look like?
[02:19:39.540 --> 02:19:42.220]   How big would it be maybe?
[02:19:42.220 --> 02:19:45.700]   Or like, I mean, that would just be an empirical question,
[02:19:45.700 --> 02:19:46.980]   right, of like how big does the model need to be
[02:19:46.980 --> 02:19:47.900]   to perform this task?
[02:19:47.900 --> 02:19:49.700]   But like, I mean, maybe it's useful if I just talk
[02:19:49.700 --> 02:19:50.980]   about some other circuits that we've seen.
[02:19:50.980 --> 02:19:53.840]   So we've seen like the IOI circuit,
[02:19:53.840 --> 02:19:56.220]   which is the indirect object identification.
[02:19:56.220 --> 02:20:00.500]   And so this is like, if you see, it's like Mary and Jim
[02:20:00.500 --> 02:20:05.340]   went to the store, Jim gave the object to blank, right?
[02:20:05.340 --> 02:20:08.020]   And it would predict Mary because Mary's appeared before
[02:20:08.020 --> 02:20:12.420]   as like the indirect object or it'll infer pronouns, right?
[02:20:12.420 --> 02:20:17.500]   And this circuit even has behavior where like
[02:20:17.500 --> 02:20:20.260]   if you ablate it, then like other heads in the model
[02:20:20.260 --> 02:20:22.700]   will pick up that behavior.
[02:20:22.700 --> 02:20:25.260]   We'll even find heads that wanna do copying behavior
[02:20:25.260 --> 02:20:27.100]   and then other heads will suppress.
[02:20:27.100 --> 02:20:30.900]   So like, it's one jobs, one head's job to just always copy
[02:20:30.900 --> 02:20:33.620]   like the token that came before, for example,
[02:20:33.620 --> 02:20:35.580]   or the token that came five before or whatever.
[02:20:35.580 --> 02:20:37.580]   And then it's another head's job to be like,
[02:20:37.580 --> 02:20:39.320]   no, do not copy that thing.
[02:20:39.320 --> 02:20:45.580]   So there are lots of different circuits performing
[02:20:45.580 --> 02:20:47.980]   in these cases, pretty basic operations,
[02:20:47.980 --> 02:20:49.020]   but when they're chained together,
[02:20:49.020 --> 02:20:51.420]   you can get unique behaviors.
[02:20:51.420 --> 02:20:54.020]   - And, but like, is the story of how you found it
[02:20:54.020 --> 02:20:55.460]   with the reasoning thing is like,
[02:20:55.460 --> 02:20:56.820]   'cause you won't be able to understand
[02:20:56.820 --> 02:20:58.860]   or it'll just be like really conv, you know,
[02:20:58.860 --> 02:20:59.820]   it won't be something you can see
[02:20:59.820 --> 02:21:01.140]   in like a two layer transformer.
[02:21:01.140 --> 02:21:04.380]   So will you just be like the circuit for deception
[02:21:04.380 --> 02:21:07.380]   or whatever, it just, this part of the network fired
[02:21:07.380 --> 02:21:10.660]   when we at the end identified the thing as being deceptive,
[02:21:10.660 --> 02:21:12.660]   this part, and it didn't fire when we didn't identify
[02:21:12.660 --> 02:21:13.620]   it as being deceptive,
[02:21:13.620 --> 02:21:16.420]   therefore this must be the deception circuit.
[02:21:16.420 --> 02:21:19.060]   - I think a lot of analysis like that,
[02:21:19.060 --> 02:21:21.780]   like Anthropic has done quite a bit of research before
[02:21:21.780 --> 02:21:24.900]   on sycophancy, which is like the model saying
[02:21:24.900 --> 02:21:27.300]   what it thinks you want to hear.
[02:21:27.300 --> 02:21:30.100]   - And that requires us at the end to be able to label
[02:21:30.100 --> 02:21:33.100]   which one is like bad and which one is good.
[02:21:33.100 --> 02:21:34.340]   - Yeah, so we have tons of instances,
[02:21:34.340 --> 02:21:35.820]   and actually as you make a lot of models larger,
[02:21:35.820 --> 02:21:40.460]   they do more of this, where the model is clearly,
[02:21:40.460 --> 02:21:44.420]   it has like features that model another person's mind
[02:21:44.420 --> 02:21:49.180]   and these activate and like some subset of these,
[02:21:50.060 --> 02:21:52.420]   we're hypothesizing here, but like would be associated
[02:21:52.420 --> 02:21:54.740]   with more deceptive behavior.
[02:21:54.740 --> 02:21:57.020]   - Although like it's doing that by, I don't know,
[02:21:57.020 --> 02:21:58.820]   Chad GPT, I think it's probably modeling me
[02:21:58.820 --> 02:22:01.420]   because that's like RLHF induces that too.
[02:22:01.420 --> 02:22:02.780]   - Yeah, theory of mind.
[02:22:02.780 --> 02:22:04.820]   - Yeah, so, well, first of all,
[02:22:04.820 --> 02:22:07.980]   the thing you mentioned earlier about there's redundancy.
[02:22:07.980 --> 02:22:11.740]   So then it's like, well, have you caught like the whole thing
[02:22:11.740 --> 02:22:13.060]   that could cause deception of the whole thing
[02:22:13.060 --> 02:22:15.220]   or like, is it just one instance of it?
[02:22:15.220 --> 02:22:16.060]   - Yeah.
[02:22:16.060 --> 02:22:17.620]   - Second of all, are your like labels correct?
[02:22:17.620 --> 02:22:19.980]   You know, maybe like you thought this wasn't deceptive,
[02:22:19.980 --> 02:22:21.140]   it's like still deceptive,
[02:22:21.140 --> 02:22:23.900]   especially if it's producing output you can't understand.
[02:22:23.900 --> 02:22:27.180]   Third, is the thing that's gonna be the bad outcome,
[02:22:27.180 --> 02:22:28.780]   something that's even human understandable,
[02:22:28.780 --> 02:22:30.740]   like deception is a concept we can understand.
[02:22:30.740 --> 02:22:31.620]   Maybe there's like a...
[02:22:31.620 --> 02:22:33.340]   - Yeah, yeah, so a lot to unpack here.
[02:22:33.340 --> 02:22:35.060]   So I guess a few things.
[02:22:35.060 --> 02:22:39.540]   One, it's fantastic that these models are deterministic.
[02:22:39.540 --> 02:22:41.300]   When you sample from them, it's stochastic, right?
[02:22:41.300 --> 02:22:44.420]   But like, I can just keep putting in more inputs
[02:22:44.420 --> 02:22:46.820]   and ablate every single part of the model.
[02:22:46.820 --> 02:22:48.940]   This is kind of the pitch for computational neuroscientists
[02:22:48.940 --> 02:22:50.100]   to come and work on interpretability.
[02:22:50.100 --> 02:22:51.500]   It's like, you have this alien brain
[02:22:51.500 --> 02:22:53.460]   and you have access to everything in it
[02:22:53.460 --> 02:22:55.740]   and you can just ablate however much of it you want.
[02:22:55.740 --> 02:22:57.460]   And so I think if you do this carefully enough,
[02:22:57.460 --> 02:22:59.500]   you really can start to pin down
[02:22:59.500 --> 02:23:00.500]   what are the circuits involved?
[02:23:00.500 --> 02:23:01.820]   What are the backup circuits?
[02:23:01.820 --> 02:23:03.580]   These sorts of things.
[02:23:03.580 --> 02:23:05.220]   The kind of cop-out answer here,
[02:23:05.220 --> 02:23:06.220]   but it's important to keep in mind,
[02:23:06.220 --> 02:23:08.740]   is doing automated interpretability.
[02:23:08.740 --> 02:23:10.900]   So it's like, as our models continue to get more capable,
[02:23:10.900 --> 02:23:12.580]   having them assign labels
[02:23:12.580 --> 02:23:15.340]   or like run some of these experiments at scale.
[02:23:15.340 --> 02:23:17.260]   And then with respect to like,
[02:23:17.260 --> 02:23:20.300]   if there's superhuman performance, how do you detect it?
[02:23:20.300 --> 02:23:22.780]   Which I think was kind of the last part of your question.
[02:23:22.780 --> 02:23:24.380]   Aside from the cop-out answer,
[02:23:24.380 --> 02:23:28.500]   if we buy this associations all the way down,
[02:23:28.500 --> 02:23:31.700]   you should be able to coarse grain the representations
[02:23:31.700 --> 02:23:34.580]   at a certain level, such that they then make sense.
[02:23:34.580 --> 02:23:37.700]   I think it was even in Demis' podcast,
[02:23:37.700 --> 02:23:38.740]   he's talking about like,
[02:23:38.740 --> 02:23:41.260]   if a chess player makes a superhuman move,
[02:23:41.260 --> 02:23:42.860]   they should be able to distill it
[02:23:42.860 --> 02:23:44.900]   into reasons why they did it.
[02:23:44.900 --> 02:23:48.420]   And like, even if the model's not gonna tell you what it is,
[02:23:48.420 --> 02:23:52.700]   you should be able to decompose that complex behavior
[02:23:52.700 --> 02:23:55.740]   into simpler circuits or features
[02:23:55.740 --> 02:23:57.220]   to really start to make sense
[02:23:57.220 --> 02:23:59.380]   of why it did the thing that it did.
[02:23:59.380 --> 02:24:00.620]   - There's a separate question of,
[02:24:00.620 --> 02:24:03.380]   does such representation exist,
[02:24:03.380 --> 02:24:05.340]   which it seems like there must,
[02:24:05.340 --> 02:24:07.060]   or actually I'm not sure if that's the case.
[02:24:07.060 --> 02:24:11.740]   And secondly, whether using this parser encoder setup,
[02:24:11.740 --> 02:24:13.300]   you could find it.
[02:24:13.300 --> 02:24:15.300]   And in this case, if you don't have labels for it
[02:24:15.300 --> 02:24:16.900]   that are adequate to represent it,
[02:24:16.900 --> 02:24:18.540]   like you wouldn't find it, right?
[02:24:18.540 --> 02:24:20.860]   - Yes and no.
[02:24:20.860 --> 02:24:23.980]   So like, we are actively trying to use dictionary learning
[02:24:23.980 --> 02:24:26.500]   now on the sleeper agents work,
[02:24:26.500 --> 02:24:27.540]   which we talked about earlier.
[02:24:27.540 --> 02:24:29.100]   And it's like, if I just give you a model,
[02:24:29.100 --> 02:24:30.700]   can you tell me if there's this trigger in it
[02:24:30.700 --> 02:24:33.060]   and it's gonna start doing interesting behavior?
[02:24:33.060 --> 02:24:34.780]   And it's an open question whether or not
[02:24:34.780 --> 02:24:35.780]   when it learns that behavior,
[02:24:35.780 --> 02:24:38.220]   it's part of a more general circuit
[02:24:38.220 --> 02:24:41.500]   that we can pick up on without actually getting activations
[02:24:41.500 --> 02:24:43.820]   for and having it display that behavior, right?
[02:24:43.820 --> 02:24:46.420]   'Cause that would kind of be cheating then.
[02:24:46.420 --> 02:24:50.380]   Or if it's learning some hacky trick over,
[02:24:50.380 --> 02:24:52.540]   like that's a separate circuit that you'll only pick up on
[02:24:52.540 --> 02:24:54.220]   if you actually have it do that behavior.
[02:24:54.220 --> 02:24:55.300]   But even in that case,
[02:24:55.300 --> 02:24:58.460]   the geometry of features gets really interesting
[02:24:58.460 --> 02:25:02.180]   because like fundamentally each feature
[02:25:02.180 --> 02:25:05.340]   like is in some part of your representation space
[02:25:05.340 --> 02:25:08.260]   and they all exist with respect to each other.
[02:25:08.260 --> 02:25:10.300]   And so in order to have this new behavior,
[02:25:10.300 --> 02:25:13.100]   you need to carve out some subset of the feature space
[02:25:13.100 --> 02:25:13.980]   for the new behavior
[02:25:13.980 --> 02:25:15.980]   and then push everything else out of the way
[02:25:15.980 --> 02:25:17.300]   to make space for it.
[02:25:17.300 --> 02:25:20.180]   So hypothetically you can imagine you like have your model
[02:25:20.180 --> 02:25:22.500]   before you've taught it this bad behavior.
[02:25:22.500 --> 02:25:23.540]   You know all the features
[02:25:23.540 --> 02:25:25.780]   or like have some coarse grained representation of them.
[02:25:25.780 --> 02:25:28.900]   You then fine tune it such that it becomes malicious.
[02:25:28.900 --> 02:25:30.100]   And then you can kind of identify
[02:25:30.100 --> 02:25:32.780]   this like black hole region of feature space
[02:25:32.780 --> 02:25:34.980]   where like everything else has been shifted away from that.
[02:25:34.980 --> 02:25:36.020]   And there's like this region
[02:25:36.020 --> 02:25:37.820]   and like you haven't put in an input
[02:25:37.820 --> 02:25:39.700]   that like causes it to fire.
[02:25:39.700 --> 02:25:42.460]   But then you can start searching for what is the input
[02:25:42.460 --> 02:25:44.580]   that would cause this part of the space to fire?
[02:25:44.580 --> 02:25:46.940]   What happens if I activate something in this space?
[02:25:46.940 --> 02:25:48.260]   There are like a whole bunch of other ways
[02:25:48.260 --> 02:25:50.260]   that you can try and attack that problem.
[02:25:50.260 --> 02:25:52.540]   - This is sort of a tangent,
[02:25:52.540 --> 02:25:55.660]   but one interesting idea I heard was
[02:25:55.660 --> 02:25:58.420]   if that space is shared between models,
[02:25:58.420 --> 02:26:01.140]   you can imagine trying to find it in an open source model
[02:26:01.140 --> 02:26:03.980]   to then make like Gemma is,
[02:26:03.980 --> 02:26:05.060]   they said in the paper,
[02:26:05.060 --> 02:26:06.340]   Gemma by the way,
[02:26:06.340 --> 02:26:08.180]   Google's newly released open source model.
[02:26:08.180 --> 02:26:09.020]   They said in the paper,
[02:26:09.020 --> 02:26:10.860]   it's trained using the same architecture
[02:26:10.860 --> 02:26:11.980]   or something like that.
[02:26:11.980 --> 02:26:12.820]   - I had to be honest,
[02:26:12.820 --> 02:26:14.540]   I didn't know because I haven't read the Gemma paper.
[02:26:14.540 --> 02:26:18.100]   - It's similar to something whatever as Gemini.
[02:26:18.100 --> 02:26:20.460]   So to the extent that's true,
[02:26:20.460 --> 02:26:21.540]   I don't know how much like,
[02:26:21.540 --> 02:26:23.540]   how much of the rec teaming you do on Gemma
[02:26:23.540 --> 02:26:26.100]   is like potentially helping you jailbreak into Gemini?
[02:26:26.100 --> 02:26:27.540]   - Yeah, this gets into the fun space
[02:26:27.540 --> 02:26:29.940]   of like how universal are features across models
[02:26:29.940 --> 02:26:33.740]   and our Towards Monosemanticity paper looked at this a bit.
[02:26:33.740 --> 02:26:37.660]   And we find, I can't give you summary statistics,
[02:26:37.660 --> 02:26:40.220]   but like the base 64 feature, for example,
[02:26:40.220 --> 02:26:41.660]   which we see across a ton of models.
[02:26:41.660 --> 02:26:43.580]   This is like, they're actually three of them,
[02:26:43.580 --> 02:26:47.100]   but they'll fire four and model base 64 encoded text,
[02:26:47.100 --> 02:26:48.820]   which is prevalent in like every URL.
[02:26:48.820 --> 02:26:51.140]   And there are lots of URLs in the training data.
[02:26:51.140 --> 02:26:53.860]   They have really high cosine similarity across models.
[02:26:53.860 --> 02:26:56.020]   So they all learn this feature.
[02:26:56.020 --> 02:26:57.520]   And I mean, within a rotation, right?
[02:26:57.520 --> 02:26:59.420]   But it's like, yeah, yeah, yeah.
[02:26:59.420 --> 02:27:00.940]   - Like the actual like vector itself.
[02:27:00.940 --> 02:27:01.780]   - Yeah, yeah.
[02:27:01.780 --> 02:27:04.380]   And I wasn't part of this analysis,
[02:27:04.380 --> 02:27:06.860]   but yeah, it definitely finds the feature
[02:27:06.860 --> 02:27:08.780]   and they're like pretty similar to each other
[02:27:08.780 --> 02:27:10.660]   across two separate, two models,
[02:27:10.660 --> 02:27:12.420]   the same model architecture,
[02:27:12.420 --> 02:27:13.740]   but trained with different random seeds.
[02:27:13.740 --> 02:27:16.180]   - It supports the quantum theory of neural scaling.
[02:27:16.180 --> 02:27:17.740]   It's like a hypothesis, right?
[02:27:17.740 --> 02:27:19.660]   We just look like all models on like a similar dataset,
[02:27:19.660 --> 02:27:22.260]   we will learn the same features in the same order-ish,
[02:27:22.260 --> 02:27:23.580]   roughly like you learn your N-grams,
[02:27:23.580 --> 02:27:24.420]   you learn your induction heads,
[02:27:24.420 --> 02:27:27.020]   and you learn like to put full stops after numbered lines
[02:27:27.020 --> 02:27:27.860]   and this kind of stuff.
[02:27:27.860 --> 02:27:30.100]   - Hey, but by the way, okay, so this is another tangent.
[02:27:30.100 --> 02:27:31.260]   To the extent that that's true.
[02:27:31.260 --> 02:27:33.420]   And like, I guess there's evidence that that's true.
[02:27:33.420 --> 02:27:35.380]   Why doesn't curriculum learning work?
[02:27:35.380 --> 02:27:36.220]   'Cause if it is the case
[02:27:36.220 --> 02:27:37.740]   that you learn certain things first,
[02:27:37.740 --> 02:27:39.660]   shouldn't just directly training those things first
[02:27:39.660 --> 02:27:40.980]   lead to better results?
[02:27:40.980 --> 02:27:42.780]   - Both Gemini papers mentioned
[02:27:42.780 --> 02:27:44.580]   some like aspects of curriculum learning.
[02:27:44.580 --> 02:27:45.420]   - Okay, interesting.
[02:27:45.420 --> 02:27:46.900]   I mean, the fact that fine-tuning works
[02:27:46.900 --> 02:27:48.860]   is like evidence of curriculum learning, right?
[02:27:48.860 --> 02:27:50.900]   'Cause the last things you're training on
[02:27:50.900 --> 02:27:52.740]   have a disproportionate impact.
[02:27:52.740 --> 02:27:55.180]   - I wouldn't necessarily say that.
[02:27:55.180 --> 02:27:56.340]   Like there's one mode of thinking
[02:27:56.340 --> 02:27:58.580]   in which fine-tuning is specialized.
[02:27:58.580 --> 02:28:00.580]   Like you've got this like latent bundle of capabilities
[02:28:00.580 --> 02:28:03.620]   and you're like specializing for this particular
[02:28:03.620 --> 02:28:04.460]   like use case that you want.
[02:28:04.460 --> 02:28:06.220]   But I'm not sure how true it is.
[02:28:06.220 --> 02:28:07.820]   - I think the David Bell Lab paper
[02:28:07.820 --> 02:28:08.660]   kind of supports this, right?
[02:28:08.660 --> 02:28:09.620]   Like you have that ability
[02:28:09.620 --> 02:28:11.700]   and you're just like getting better at entity recognition.
[02:28:11.700 --> 02:28:13.940]   Like fine-tuning that circuit instead of other ones.
[02:28:13.940 --> 02:28:14.780]   - Yeah.
[02:28:14.780 --> 02:28:15.620]   - Yeah.
[02:28:15.620 --> 02:28:16.700]   - I'm sorry, what was the thing we were talking about before?
[02:28:16.700 --> 02:28:18.380]   - But generally, I do think like curriculum learning
[02:28:18.380 --> 02:28:19.220]   is a really interesting thing
[02:28:19.220 --> 02:28:20.060]   that people should explore more.
[02:28:20.060 --> 02:28:22.060]   And it like seems very plausible.
[02:28:22.060 --> 02:28:25.260]   I would really love to see more analysis
[02:28:25.260 --> 02:28:26.540]   along the lines of the quantum theory stuff
[02:28:26.540 --> 02:28:27.860]   when like understanding better
[02:28:27.860 --> 02:28:29.540]   what do you actually learn at each stage
[02:28:29.540 --> 02:28:31.820]   and like decomposing that out
[02:28:31.820 --> 02:28:34.300]   and exploring whether or not curricula change that
[02:28:34.300 --> 02:28:35.140]   in a more direct way.
[02:28:35.140 --> 02:28:36.180]   - By the way, I just realized.
[02:28:36.180 --> 02:28:38.740]   Forgot to, I just like got in conversation mode
[02:28:38.740 --> 02:28:40.460]   and forgot there's an audience.
[02:28:40.460 --> 02:28:43.220]   Curriculum learning is when you organize a data set.
[02:28:43.220 --> 02:28:44.980]   When you think about a human, how they learn,
[02:28:44.980 --> 02:28:47.260]   they don't just see like a random wiki text
[02:28:47.260 --> 02:28:48.580]   and they just like try to predict it, right?
[02:28:48.580 --> 02:28:50.020]   They're like, we'll start you off
[02:28:50.020 --> 02:28:54.540]   with like a Lorax or something and then you'll learn.
[02:28:54.540 --> 02:28:56.260]   I don't even remember what first grade was like
[02:28:56.260 --> 02:28:57.820]   but you learn the things that first graders learn
[02:28:57.820 --> 02:29:00.300]   and then like second graders and so forth.
[02:29:00.300 --> 02:29:01.140]   And so you'd imagine that's--
[02:29:01.140 --> 02:29:03.100]   - Sorry, we know you never got past first grade.
[02:29:03.100 --> 02:29:05.680]   (all laughing)
[02:29:05.680 --> 02:29:20.980]   - Okay, anyways, let's get back to like the big,
[02:29:20.980 --> 02:29:24.140]   before we get into like a bunch of like inter details.
[02:29:24.140 --> 02:29:29.140]   The big picture, there's two threads I want to explore.
[02:29:29.140 --> 02:29:32.700]   First is, I guess it makes me a little worried
[02:29:32.700 --> 02:29:35.460]   that there's not even an alternative formulation
[02:29:35.460 --> 02:29:37.100]   of what could be happening in these models
[02:29:37.100 --> 02:29:39.420]   that could invalidate this approach.
[02:29:39.420 --> 02:29:41.340]   Which feels like, I mean, we do know
[02:29:41.340 --> 02:29:42.860]   that we don't understand intelligence, right?
[02:29:42.860 --> 02:29:44.860]   Like there are definitely unknown unknowns here.
[02:29:44.860 --> 02:29:48.500]   So like the fact that there's not a null hypothesis,
[02:29:48.500 --> 02:29:51.180]   I don't know, I feel like what if we're just wrong
[02:29:51.180 --> 02:29:52.500]   and we don't even know the way in which we're wrong,
[02:29:52.500 --> 02:29:55.940]   which actually increases the uncertainty and yeah.
[02:29:55.940 --> 02:29:57.540]   - Yeah, yeah, yeah.
[02:29:57.540 --> 02:30:00.540]   So it's not that there aren't other hypotheses,
[02:30:00.620 --> 02:30:03.300]   it's just, I have been working on superposition
[02:30:03.300 --> 02:30:06.980]   for like a number of years and very involved in this effort.
[02:30:06.980 --> 02:30:10.100]   And so I'm less sympathetic to, or well like--
[02:30:10.100 --> 02:30:11.100]   - You just said they're wrong.
[02:30:11.100 --> 02:30:13.900]   (all laughing)
[02:30:13.900 --> 02:30:14.820]   - To these other approaches,
[02:30:14.820 --> 02:30:17.700]   especially because our recent work has been so successful.
[02:30:17.700 --> 02:30:20.020]   - Yeah, it's like quite high explanatory power.
[02:30:20.020 --> 02:30:22.620]   Like there's this beauty, like in the scaling laws paper,
[02:30:22.620 --> 02:30:25.000]   there's this little bump at a particular,
[02:30:25.000 --> 02:30:27.540]   like the original scaling laws paper, there's a little bump.
[02:30:27.540 --> 02:30:29.500]   And that apparently corresponds
[02:30:29.500 --> 02:30:31.700]   to when the model learns induction heads.
[02:30:31.700 --> 02:30:33.740]   And then like after that, so it goes off track,
[02:30:33.740 --> 02:30:36.260]   learns induction heads, gets back on track.
[02:30:36.260 --> 02:30:38.020]   Which is like an incredible piece
[02:30:38.020 --> 02:30:39.700]   of retroactive explanatory power.
[02:30:39.700 --> 02:30:42.120]   - Yeah, before I forget it though,
[02:30:42.120 --> 02:30:44.580]   I do have one thread on future universality
[02:30:44.580 --> 02:30:46.980]   that you might wanna have in.
[02:30:46.980 --> 02:30:49.540]   So there are some really interesting behavioral,
[02:30:49.540 --> 02:30:51.700]   evolutionary biology experiments on like,
[02:30:51.700 --> 02:30:53.680]   should humans learn a real representation
[02:30:53.680 --> 02:30:55.060]   of the world or not?
[02:30:55.060 --> 02:30:57.500]   You can imagine a world in which we saw all venomous animals
[02:30:57.500 --> 02:30:59.900]   as like flashing neon pink,
[02:30:59.900 --> 02:31:01.780]   a world in which we survive better.
[02:31:01.780 --> 02:31:03.060]   And so it would make sense for us
[02:31:03.060 --> 02:31:05.940]   to not have a realistic representation of the world.
[02:31:05.940 --> 02:31:09.580]   And there's some work where they'll simulate
[02:31:09.580 --> 02:31:11.260]   like little basic agents
[02:31:11.260 --> 02:31:14.860]   and see if the representations they learn,
[02:31:14.860 --> 02:31:18.780]   like map to the like tools they can use
[02:31:18.780 --> 02:31:20.540]   and like the inputs they should have.
[02:31:20.540 --> 02:31:23.500]   And it turns out if you have these little agents
[02:31:23.500 --> 02:31:26.420]   perform more than a certain number of tasks,
[02:31:26.420 --> 02:31:29.140]   given these basic tools and objects in the world,
[02:31:29.140 --> 02:31:33.460]   then they will learn a like ground truth representation
[02:31:33.460 --> 02:31:36.500]   because like there are so many possible use cases
[02:31:36.500 --> 02:31:38.340]   that you need for these base objects
[02:31:38.340 --> 02:31:41.340]   that you actually want to learn what the object actually is
[02:31:41.340 --> 02:31:45.180]   and not some like cheap visual heuristic or other thing.
[02:31:45.180 --> 02:31:47.740]   And so to the extent that we are doing,
[02:31:47.740 --> 02:31:48.580]   and we haven't talked at all
[02:31:48.580 --> 02:31:50.020]   about like Friston's free energy principle
[02:31:50.020 --> 02:31:51.500]   or predictive coding or anything else,
[02:31:51.500 --> 02:31:53.620]   but like to the extent that all living organisms
[02:31:53.620 --> 02:31:56.620]   are trying to like actively predict what comes next
[02:31:56.620 --> 02:31:58.860]   and form like a really accurate world model,
[02:31:58.860 --> 02:32:03.140]   it wouldn't surprise me or I'm optimistic
[02:32:03.140 --> 02:32:06.660]   that we are learning genuine features about the world
[02:32:06.660 --> 02:32:08.100]   that are good for modeling it.
[02:32:08.100 --> 02:32:09.940]   And our language models will do the same,
[02:32:09.940 --> 02:32:11.660]   at least, especially because we're training them
[02:32:11.660 --> 02:32:14.580]   on human data and human texts.
[02:32:14.580 --> 02:32:16.180]   - Another dinner party question.
[02:32:16.180 --> 02:32:20.580]   Should we be less worried about misalignment
[02:32:20.580 --> 02:32:22.580]   and maybe that's not even the right word
[02:32:22.580 --> 02:32:23.540]   for what I'm referring to,
[02:32:23.540 --> 02:32:27.540]   but like just alienness and shogginess from these models,
[02:32:27.540 --> 02:32:30.220]   given that there is future universality
[02:32:30.220 --> 02:32:32.220]   and there are certain ways of thinking
[02:32:32.220 --> 02:32:33.460]   and ways of understanding the world
[02:32:33.460 --> 02:32:36.380]   that are instrumentally useful
[02:32:36.380 --> 02:32:39.100]   to different kinds of intelligences?
[02:32:39.100 --> 02:32:40.020]   Should we just be less worried
[02:32:40.020 --> 02:32:43.820]   about like bizarro paperclip maximizers as a result?
[02:32:43.820 --> 02:32:46.260]   - I think this is kind of why I bring this up
[02:32:46.260 --> 02:32:47.620]   as like the optimistic take.
[02:32:47.620 --> 02:32:50.500]   Predicting the internet is very different
[02:32:50.500 --> 02:32:51.420]   from what we're doing though, right?
[02:32:51.420 --> 02:32:53.420]   Like the models are way better at predicting next tokens
[02:32:53.420 --> 02:32:54.260]   than we are.
[02:32:54.260 --> 02:32:55.740]   They're trained on so much garbage.
[02:32:55.740 --> 02:32:57.460]   They're trained on so many URLs.
[02:32:57.460 --> 02:32:59.020]   Like in the dictionary learning work,
[02:32:59.020 --> 02:33:01.340]   we find there are like three separate features
[02:33:01.340 --> 02:33:03.780]   for base 64 encodings.
[02:33:03.780 --> 02:33:06.900]   And like, even that is kind of an alien example
[02:33:06.900 --> 02:33:08.460]   that is probably worth me talking about for a minute.
[02:33:08.460 --> 02:33:12.460]   Like one of these base 64 features fired for numbers,
[02:33:12.460 --> 02:33:15.740]   one, like other base 64,
[02:33:15.740 --> 02:33:17.380]   like if it sees base 64 numbers,
[02:33:17.380 --> 02:33:18.540]   it'll like predict more of those.
[02:33:18.540 --> 02:33:20.140]   Another fired for letters,
[02:33:20.140 --> 02:33:22.740]   but then there was this third one that we didn't understand
[02:33:22.740 --> 02:33:25.060]   and it like fired for like a very specific subset
[02:33:25.060 --> 02:33:26.780]   of base 64 features.
[02:33:26.780 --> 02:33:29.740]   And someone on the team who clearly knows way too much
[02:33:29.740 --> 02:33:32.420]   about base 64 realized that this was the subset
[02:33:32.420 --> 02:33:34.500]   that was ASCII decodable.
[02:33:34.500 --> 02:33:38.020]   So you could decode it back into the ASCII characters.
[02:33:38.020 --> 02:33:40.740]   And the fact that the model like learned
[02:33:40.740 --> 02:33:42.220]   these three different features
[02:33:42.220 --> 02:33:44.060]   and it took us a little while to like figure out
[02:33:44.060 --> 02:33:48.180]   what was going on is very Shoggoth-esque.
[02:33:48.180 --> 02:33:52.660]   - It has a denser representation of like regions
[02:33:52.660 --> 02:33:54.860]   that are particularly relevant to predicting the next token.
[02:33:54.860 --> 02:33:57.060]   - Yeah, because it's so, but yeah.
[02:33:57.060 --> 02:33:59.180]   And it's clearly doing something that humans wouldn't, right?
[02:33:59.180 --> 02:34:01.300]   Like you can even talk to any of the current models
[02:34:01.300 --> 02:34:03.700]   in base 64 and it will apply in base 64.
[02:34:03.700 --> 02:34:04.540]   - Right.
[02:34:04.540 --> 02:34:07.580]   - And you can then like decode it and it works great.
[02:34:07.580 --> 02:34:09.020]   - That particular example,
[02:34:09.020 --> 02:34:12.900]   I wonder if that implies that the difficulty
[02:34:12.900 --> 02:34:16.420]   of doing interoperability on smarter models will be harder
[02:34:16.420 --> 02:34:20.020]   because if like it requires somebody
[02:34:20.020 --> 02:34:22.020]   with esoteric knowledge has just happened to see
[02:34:22.020 --> 02:34:24.220]   that base 64 has, I don't know,
[02:34:24.220 --> 02:34:26.060]   like whatever that distinction was,
[02:34:26.060 --> 02:34:28.060]   doesn't it imply when you have a million line pull request,
[02:34:28.060 --> 02:34:29.660]   it's like, there is no human that's going to be able
[02:34:29.660 --> 02:34:32.420]   to decode like two different reasons why the pull request,
[02:34:32.420 --> 02:34:35.420]   there's like two different features for this pull.
[02:34:35.420 --> 02:34:36.980]   Yeah, you know what I mean?
[02:34:36.980 --> 02:34:37.820]   - Yeah.
[02:34:37.820 --> 02:34:38.660]   - So if you think-
[02:34:38.660 --> 02:34:39.480]   - And that's when you type a comment,
[02:34:39.480 --> 02:34:40.320]   like small CLs please.
[02:34:40.320 --> 02:34:42.220]   (all laughing)
[02:34:42.220 --> 02:34:43.060]   - Yeah, exactly.
[02:34:43.060 --> 02:34:43.900]   No, no, I mean, you could do that, right?
[02:34:43.900 --> 02:34:45.900]   This is like, what I was gonna say is like one technique
[02:34:45.900 --> 02:34:46.980]   here is anomaly detection, right?
[02:34:46.980 --> 02:34:47.820]   - Yeah.
[02:34:47.820 --> 02:34:49.260]   - And so one beauty of dictionary learning
[02:34:49.260 --> 02:34:52.740]   instead of like linear probes is that it's unsupervised.
[02:34:52.740 --> 02:34:54.820]   You are just trying to learn to span
[02:34:54.820 --> 02:34:57.660]   all of the representations that the model has
[02:34:57.660 --> 02:34:59.940]   and then interpret them later.
[02:34:59.940 --> 02:35:01.940]   But if there's a weird feature that suddenly fires
[02:35:01.940 --> 02:35:04.060]   for the first time that you haven't seen fire before,
[02:35:04.060 --> 02:35:05.740]   that's a red flag.
[02:35:05.740 --> 02:35:07.020]   You could also coarse grain it
[02:35:07.020 --> 02:35:09.300]   so that it's just a single base 64 feature.
[02:35:09.300 --> 02:35:11.260]   I mean, even the fact that this came up
[02:35:11.260 --> 02:35:14.100]   and we could see that it's specifically favors
[02:35:14.100 --> 02:35:15.700]   these particular outputs and it fires
[02:35:15.700 --> 02:35:17.100]   for these particular inputs,
[02:35:17.100 --> 02:35:18.540]   gets you a lot of the way there.
[02:35:18.540 --> 02:35:21.200]   I'm even familiar of cases from the auto interp side
[02:35:21.200 --> 02:35:22.900]   where a human will look at a feature
[02:35:22.900 --> 02:35:27.700]   and try to annotate it for it fires for Latin words.
[02:35:27.700 --> 02:35:30.280]   And then when you ask the model to classify it,
[02:35:30.280 --> 02:35:33.940]   it says it fires for Latin words, defining plants.
[02:35:33.940 --> 02:35:36.820]   So it can like already like beat the human in some cases
[02:35:36.820 --> 02:35:38.740]   for like labeling what's going on.
[02:35:38.740 --> 02:35:41.900]   - So at scale, this would require an adversarial
[02:35:41.900 --> 02:35:47.780]   thing between models where like some model
[02:35:47.780 --> 02:35:51.220]   you have like millions of features potentially for GPT-6
[02:35:51.220 --> 02:35:53.260]   and some, like it just a bunch of models
[02:35:53.260 --> 02:35:54.380]   are just trying to figure out
[02:35:54.380 --> 02:35:56.540]   what each of these features means.
[02:35:56.540 --> 02:35:58.500]   How does that sound right, okay.
[02:35:58.500 --> 02:36:00.460]   - Yeah, but you can even automate this process, right?
[02:36:00.460 --> 02:36:03.180]   I mean, this goes back to the determinism of the model.
[02:36:03.180 --> 02:36:04.020]   Like you could have a model
[02:36:04.020 --> 02:36:06.600]   that is actively editing input text
[02:36:06.600 --> 02:36:09.740]   and predicting if the feature is gonna fire or not
[02:36:09.740 --> 02:36:11.940]   and figure out what makes it fire, what doesn't
[02:36:11.940 --> 02:36:14.460]   and like search the space.
[02:36:14.460 --> 02:36:17.140]   - Yeah, I wanna talk more about the feature splitting
[02:36:17.140 --> 02:36:18.700]   'cause I think that's like an interesting thing
[02:36:18.700 --> 02:36:20.660]   that has been underexplored.
[02:36:20.660 --> 02:36:21.780]   - Especially for scalability,
[02:36:21.780 --> 02:36:24.020]   I think it's underappreciated right now.
[02:36:24.020 --> 02:36:27.220]   - First of all, like how do we even think about,
[02:36:27.220 --> 02:36:30.700]   is it really just, you can keep going down and down
[02:36:30.700 --> 02:36:32.900]   like there's no end to the amount of features?
[02:36:32.900 --> 02:36:35.060]   - I mean, so at some point,
[02:36:35.060 --> 02:36:37.140]   I think you might just start fitting noise
[02:36:37.140 --> 02:36:40.000]   or things that are part of the data,
[02:36:40.000 --> 02:36:40.840]   but that the model isn't actually representing.
[02:36:40.840 --> 02:36:42.780]   - Wait, do you wanna explain what feature splitting is?
[02:36:42.780 --> 02:36:45.860]   - Yeah, yeah, so it's the part before
[02:36:45.860 --> 02:36:48.180]   where like the model will learn
[02:36:48.180 --> 02:36:50.380]   however many features it has capacity for
[02:36:50.380 --> 02:36:53.140]   that still span the space of representation.
[02:36:53.140 --> 02:36:54.260]   - So like give an example potentially.
[02:36:54.260 --> 02:36:56.380]   - Yeah, yeah, so you learn,
[02:36:56.380 --> 02:36:57.860]   if you don't give the model that much capacity
[02:36:57.860 --> 02:36:59.540]   for the features it's learning,
[02:36:59.540 --> 02:37:03.380]   concretely if you project to not as high dimensional space,
[02:37:03.380 --> 02:37:05.900]   it will learn one feature for birds.
[02:37:05.900 --> 02:37:07.620]   But if you give the model more capacity,
[02:37:07.620 --> 02:37:11.020]   it will learn features for all the different types of birds.
[02:37:11.020 --> 02:37:15.260]   And so it's more specific than otherwise.
[02:37:15.260 --> 02:37:17.480]   And oftentimes like there's the bird vector
[02:37:17.480 --> 02:37:18.860]   that points in one direction
[02:37:18.860 --> 02:37:21.040]   and all the other specific types of birds
[02:37:21.040 --> 02:37:23.940]   point in like a similar region of the space,
[02:37:23.940 --> 02:37:27.460]   but are obviously more specific than the course label.
[02:37:27.460 --> 02:37:29.780]   - Okay, so let's go back to GPT-7.
[02:37:29.780 --> 02:37:32.460]   First of all, is this a sort of like linear tax
[02:37:32.460 --> 02:37:34.600]   on any model to figure out?
[02:37:34.600 --> 02:37:37.900]   Even before that, is this a one-time thing you had to do
[02:37:37.900 --> 02:37:41.120]   or is this the kind of thing you have to do on every output?
[02:37:41.120 --> 02:37:42.540]   Or is just like one time it's not deceptive,
[02:37:42.540 --> 02:37:43.380]   we're good to go.
[02:37:44.900 --> 02:37:46.340]   Actually, let me let you answer that.
[02:37:46.340 --> 02:37:48.200]   - Yeah, so you do dictionary learning
[02:37:48.200 --> 02:37:49.740]   after you've trained your model
[02:37:49.740 --> 02:37:53.060]   and you feed it a ton of inputs
[02:37:53.060 --> 02:37:55.140]   and you get the activations from those
[02:37:55.140 --> 02:37:56.500]   and then you do this projection
[02:37:56.500 --> 02:37:57.580]   into the higher dimensional space.
[02:37:57.580 --> 02:38:00.220]   And so the method is it's unsupervised
[02:38:00.220 --> 02:38:02.180]   in that it's trying to learn these sparse features.
[02:38:02.180 --> 02:38:04.440]   You're not telling them in advance what they should be,
[02:38:04.440 --> 02:38:08.600]   but it is constrained by the inputs you're giving the model.
[02:38:08.600 --> 02:38:10.940]   I guess two caveats here.
[02:38:10.940 --> 02:38:15.780]   One, like we can try and choose what inputs we want.
[02:38:15.780 --> 02:38:17.260]   So if we're looking for theory of mind features
[02:38:17.260 --> 02:38:18.220]   that might lead to deception,
[02:38:18.220 --> 02:38:20.140]   we can put it in the sycophancy dataset.
[02:38:20.140 --> 02:38:22.260]   Hopefully at some point we can move into
[02:38:22.260 --> 02:38:25.020]   looking at the weights of the model alone
[02:38:25.020 --> 02:38:28.020]   or at least using that information to do dictionary learning.
[02:38:28.020 --> 02:38:30.460]   But I think in order to get there,
[02:38:30.460 --> 02:38:32.160]   that's like such a hard problem
[02:38:32.160 --> 02:38:34.220]   that you need to make traction on just learning
[02:38:34.220 --> 02:38:36.180]   what the features are first.
[02:38:36.180 --> 02:38:37.860]   But yeah, so what's the cost of this?
[02:38:37.860 --> 02:38:39.540]   - Can you repeat the last sentence?
[02:38:39.540 --> 02:38:41.100]   Weights of the model alone.
[02:38:41.100 --> 02:38:44.300]   - So like right now we just have these neurons in the model.
[02:38:44.300 --> 02:38:45.540]   They don't make any sense.
[02:38:45.540 --> 02:38:46.900]   We apply dictionary learning.
[02:38:46.900 --> 02:38:48.020]   We get these features out.
[02:38:48.020 --> 02:38:50.020]   They start to make sense.
[02:38:50.020 --> 02:38:53.340]   But that depends on the activations of the neurons.
[02:38:53.340 --> 02:38:55.500]   The weights of the model itself,
[02:38:55.500 --> 02:38:57.460]   like what neurons are connected to what other neurons,
[02:38:57.460 --> 02:38:59.460]   certainly has information in it.
[02:38:59.460 --> 02:39:02.960]   And the dream is that we can kind of bootstrap
[02:39:02.960 --> 02:39:05.840]   towards actually making sense of the weights of the model
[02:39:05.840 --> 02:39:09.140]   that are independent of the activations of the data.
[02:39:09.140 --> 02:39:11.620]   I mean, this is, I'm not saying we've made any progress here.
[02:39:11.620 --> 02:39:13.020]   It's a very hard problem,
[02:39:13.020 --> 02:39:15.780]   but it feels like we'll have a lot more traction
[02:39:15.780 --> 02:39:16.740]   and be able to like sanity check
[02:39:16.740 --> 02:39:17.780]   what we're finding with the weights
[02:39:17.780 --> 02:39:19.300]   if we're able to pull out features first.
[02:39:19.300 --> 02:39:22.260]   - For the audience, weights are permanent.
[02:39:22.260 --> 02:39:23.260]   Well, I don't know if permanent is the right word,
[02:39:23.260 --> 02:39:24.900]   but like they are the model itself.
[02:39:24.900 --> 02:39:28.220]   Whereas activations are the sort of like artifacts
[02:39:28.220 --> 02:39:30.060]   of any single call.
[02:39:30.060 --> 02:39:32.860]   - In a brain metaphor,
[02:39:32.860 --> 02:39:35.340]   the weights are like the actual connection scheme
[02:39:35.340 --> 02:39:36.900]   between neurons and the activations
[02:39:36.900 --> 02:39:39.700]   of the current neurons that are lining up, basically.
[02:39:39.700 --> 02:39:42.140]   - Yeah, okay, so there's gonna be two steps to this
[02:39:42.140 --> 02:39:45.380]   for GPT-7 or whatever model we're concerned about.
[02:39:45.380 --> 02:39:48.260]   One, first correct me if I'm wrong,
[02:39:48.260 --> 02:39:50.820]   but like training the sparse autoencoder
[02:39:50.820 --> 02:39:53.380]   and like do the unsupervised projection
[02:39:53.380 --> 02:39:57.460]   into a wider space of features that have a higher fidelity
[02:39:57.460 --> 02:39:59.820]   to like what is actually happening in the model.
[02:39:59.820 --> 02:40:02.820]   And then secondly, label those features.
[02:40:02.820 --> 02:40:06.820]   'Cause let's say like the cost of training the model
[02:40:06.820 --> 02:40:11.140]   is N, what will those two steps cost relative to N?
[02:40:11.140 --> 02:40:16.060]   - We will see, like it really depends on two main things.
[02:40:16.060 --> 02:40:17.380]   What is your expansion factors?
[02:40:17.380 --> 02:40:18.300]   Like how much are you projecting
[02:40:18.300 --> 02:40:19.540]   into the higher dimensional space?
[02:40:19.540 --> 02:40:21.340]   And how much data do you need to put into the model?
[02:40:21.340 --> 02:40:23.580]   How many activations do you need to give it?
[02:40:23.580 --> 02:40:25.740]   But this brings me back to the feature splitting
[02:40:25.740 --> 02:40:26.580]   to a certain extent,
[02:40:26.580 --> 02:40:30.420]   because if you know you're looking for specific features,
[02:40:30.420 --> 02:40:33.060]   you can start with a really cheaper
[02:40:33.060 --> 02:40:35.140]   like course representation.
[02:40:35.140 --> 02:40:37.980]   So maybe my expansion factor is like only two.
[02:40:37.980 --> 02:40:39.460]   So like I have a thousand neurons
[02:40:39.460 --> 02:40:41.460]   I'm projecting to a 2000 dimensional space.
[02:40:41.460 --> 02:40:43.780]   I get 2000 features out, but they're really coarse.
[02:40:43.780 --> 02:40:46.460]   And so previously I had the example for birds.
[02:40:46.460 --> 02:40:50.100]   Let's move that example to like, I have a biology feature.
[02:40:50.100 --> 02:40:52.780]   And, but I really care about if the model
[02:40:52.780 --> 02:40:55.260]   has representations for bioweapons
[02:40:55.260 --> 02:40:56.620]   and is trying to manufacture them.
[02:40:56.620 --> 02:40:59.740]   And so what I actually want is like an anthrax feature.
[02:40:59.740 --> 02:41:02.580]   What you can then do is rather than,
[02:41:02.580 --> 02:41:03.540]   and let's say the anthrax,
[02:41:03.540 --> 02:41:05.100]   you only see the anthrax feature
[02:41:05.100 --> 02:41:06.940]   if instead of going from a thousand dimensions
[02:41:06.940 --> 02:41:10.220]   to 2000 dimensions, I go to a million dimensions, right?
[02:41:10.220 --> 02:41:12.820]   And so you can kind of imagine this big tree
[02:41:12.820 --> 02:41:15.140]   of semantic concepts where like biology splits
[02:41:15.140 --> 02:41:19.140]   into like cells versus like whole body biology.
[02:41:19.140 --> 02:41:21.100]   And then further down it splits into all these other things.
[02:41:21.100 --> 02:41:23.140]   So rather than needing to immediately go
[02:41:23.140 --> 02:41:24.580]   from a thousand to a million,
[02:41:24.580 --> 02:41:26.740]   and then picking out that one feature of interest,
[02:41:26.740 --> 02:41:29.340]   you can find the direction that the biology feature
[02:41:29.340 --> 02:41:31.340]   is pointing in, which again is very coarse,
[02:41:31.340 --> 02:41:34.500]   and then selectively search around that space.
[02:41:34.500 --> 02:41:38.820]   So like only do dictionary learning if this,
[02:41:38.820 --> 02:41:40.020]   if something in the direction
[02:41:40.020 --> 02:41:41.900]   of the biology feature fires first.
[02:41:41.900 --> 02:41:46.140]   And so the computer science metaphor here would be like,
[02:41:46.140 --> 02:41:48.300]   instead of doing breadth-first search,
[02:41:48.300 --> 02:41:50.460]   you're able to do depth-first search,
[02:41:50.460 --> 02:41:52.420]   where you're only recursively expanding
[02:41:52.420 --> 02:41:53.780]   and exploring a particular part
[02:41:53.780 --> 02:41:56.140]   of this like semantic tree of features.
[02:41:56.140 --> 02:41:58.500]   - Although given the way that these features
[02:41:58.500 --> 02:42:03.500]   are not organized in things that are intuitive for humans,
[02:42:03.500 --> 02:42:05.460]   right, like, 'cause we just don't know how to deal
[02:42:05.460 --> 02:42:07.580]   with basic C4, so we don't have that many,
[02:42:07.580 --> 02:42:09.220]   you know, we just don't dedicate that much,
[02:42:09.220 --> 02:42:12.460]   like whatever, firmware to like deconstructing
[02:42:12.460 --> 02:42:13.900]   which kind of basic C4 it is.
[02:42:13.900 --> 02:42:15.980]   How would we know that the subjects,
[02:42:15.980 --> 02:42:17.780]   and this will go back to maybe the MOE discussion
[02:42:17.780 --> 02:42:21.420]   we'll have of, I guess we might as well talk about it,
[02:42:21.420 --> 02:42:23.380]   but like in mixture of experts,
[02:42:23.380 --> 02:42:27.660]   the mixture of paper talked about how they couldn't find,
[02:42:27.660 --> 02:42:29.780]   the experts weren't specialized in a way
[02:42:29.780 --> 02:42:30.700]   that we could understand.
[02:42:30.700 --> 02:42:32.060]   There's not like a chemistry expert
[02:42:32.060 --> 02:42:34.220]   or a physics expert or something.
[02:42:34.220 --> 02:42:35.500]   So why would you think that like,
[02:42:35.500 --> 02:42:37.020]   it will be like biology feature
[02:42:37.020 --> 02:42:39.500]   and then deconstruct rather than like blah,
[02:42:39.500 --> 02:42:41.620]   and then you just deconstruct and it's like anthrax
[02:42:41.620 --> 02:42:44.500]   and you're like shoes and whatever.
[02:42:44.500 --> 02:42:47.020]   - So I haven't read the Mistral paper,
[02:42:47.020 --> 02:42:50.620]   but I think that the heads, I mean, this goes back to like,
[02:42:50.620 --> 02:42:52.460]   if you just look at the neurons in a model,
[02:42:52.460 --> 02:42:53.660]   they're polysemantic.
[02:42:53.660 --> 02:42:55.620]   And so if all they did was just look at the neurons
[02:42:55.620 --> 02:42:57.940]   in a given head, it's very plausible
[02:42:57.940 --> 02:43:00.700]   that it's also a polysemantic because of superposition.
[02:43:00.700 --> 02:43:03.700]   - I wanna just tug on a thread that Dorcas mentioned there.
[02:43:03.700 --> 02:43:06.100]   Have you seen in the subtrees when you expand them out,
[02:43:06.100 --> 02:43:07.540]   like something in a subtree,
[02:43:07.540 --> 02:43:09.140]   which like you really wouldn't guess
[02:43:09.140 --> 02:43:10.020]   that it should be there
[02:43:10.020 --> 02:43:11.620]   based on like the higher level of traction.
[02:43:11.620 --> 02:43:14.660]   - So this is a line of work that we haven't pursued
[02:43:14.660 --> 02:43:16.580]   as much as I want to yet.
[02:43:16.580 --> 02:43:17.740]   But I think we're planning to,
[02:43:17.740 --> 02:43:20.460]   I hope that maybe external groups do as well.
[02:43:20.460 --> 02:43:21.940]   Like what is the geometry of features?
[02:43:21.940 --> 02:43:22.780]   - What's the geometry?
[02:43:22.780 --> 02:43:23.620]   Exactly.
[02:43:23.620 --> 02:43:24.460]   - And how does that change over time?
[02:43:24.460 --> 02:43:25.860]   Like what would suck if like anthrax feature
[02:43:25.860 --> 02:43:29.820]   happened to be like below the like, you know, coffee can?
[02:43:29.820 --> 02:43:31.660]   Subtree or something like that, right?
[02:43:31.660 --> 02:43:32.500]   - Totally, totally.
[02:43:32.500 --> 02:43:33.340]   - And that feels like the kind of thing
[02:43:33.340 --> 02:43:37.380]   that you could quickly try and find like proof of,
[02:43:37.380 --> 02:43:39.580]   which would then like mean that you need to like,
[02:43:39.580 --> 02:43:40.420]   then solve that problem.
[02:43:40.420 --> 02:43:41.260]   - Yeah, yeah.
[02:43:41.260 --> 02:43:42.340]   - Inject more structure into the geometry.
[02:43:42.340 --> 02:43:43.180]   - Totally.
[02:43:43.180 --> 02:43:44.460]   I mean, it would really surprise me, I guess,
[02:43:44.460 --> 02:43:46.220]   especially like given how linear the models seem to be.
[02:43:46.220 --> 02:43:47.060]   - Completely agree.
[02:43:47.060 --> 02:43:48.820]   - That like there isn't some component
[02:43:48.820 --> 02:43:51.020]   of the anthrax feature, like vector,
[02:43:51.020 --> 02:43:53.820]   that is similar to and looks like the biology vector
[02:43:53.820 --> 02:43:55.500]   and that they're not in a similar part of the space.
[02:43:55.500 --> 02:43:59.180]   But yes, I mean, ultimately machine learning is empirical.
[02:43:59.180 --> 02:44:00.020]   - Yeah.
[02:44:00.020 --> 02:44:00.860]   - We need to do this.
[02:44:00.860 --> 02:44:01.700]   - We need to do the research.
[02:44:01.700 --> 02:44:02.540]   - I think it's going to be pretty important
[02:44:02.540 --> 02:44:04.180]   for certain aspects of scaling dictionary learning.
[02:44:04.180 --> 02:44:05.020]   - Yeah, yeah.
[02:44:05.020 --> 02:44:06.460]   Interesting.
[02:44:06.460 --> 02:44:07.820]   On the MOE discussion.
[02:44:07.820 --> 02:44:08.660]   - Yeah.
[02:44:08.660 --> 02:44:10.860]   - There's an interesting scaling vision transformers paper
[02:44:10.860 --> 02:44:13.340]   that Google put out a little while ago,
[02:44:13.340 --> 02:44:17.020]   where they like do image net classification with like an MOE.
[02:44:17.020 --> 02:44:19.740]   And they find really clear class specialization there
[02:44:19.740 --> 02:44:20.580]   for experts.
[02:44:20.580 --> 02:44:22.700]   Like there's a clear dog expert.
[02:44:22.700 --> 02:44:24.260]   - Wait, so like the mixture of people
[02:44:24.260 --> 02:44:26.220]   just not do a good job of like identifying those?
[02:44:26.220 --> 02:44:27.780]   - I think it's hard.
[02:44:27.780 --> 02:44:30.900]   Like, and like, it's entirely possible that,
[02:44:30.900 --> 02:44:34.620]   like in some respects, there's almost no reason
[02:44:34.620 --> 02:44:37.940]   that like all of the different archive like features
[02:44:37.940 --> 02:44:38.900]   should go to one expert.
[02:44:38.900 --> 02:44:41.180]   Like you could have biology, like let's say,
[02:44:41.180 --> 02:44:42.860]   I don't know what buckets they had in their paper,
[02:44:42.860 --> 02:44:44.500]   but let's say they had like archive papers
[02:44:44.500 --> 02:44:46.020]   as like one of the things.
[02:44:46.020 --> 02:44:47.580]   You could imagine like biology papers going here,
[02:44:47.580 --> 02:44:48.420]   math papers going here,
[02:44:48.420 --> 02:44:51.260]   and all of a sudden you're like breakdown is like ruined.
[02:44:51.260 --> 02:44:53.660]   But that vision transformer one,
[02:44:53.660 --> 02:44:55.980]   where the class separation is really clear and obvious
[02:44:55.980 --> 02:44:57.420]   gives I think some evidence
[02:44:57.420 --> 02:44:59.900]   towards the specialization hypothesis.
[02:44:59.900 --> 02:45:02.420]   - So I think images are also in some ways
[02:45:02.420 --> 02:45:03.860]   just easier to interpret than text.
[02:45:03.860 --> 02:45:04.700]   - Yeah, exactly.
[02:45:04.700 --> 02:45:07.380]   - And like, so Chris Ola's like interpretability work
[02:45:07.380 --> 02:45:10.460]   on AlexNet and these other models,
[02:45:10.460 --> 02:45:12.100]   like in the original AlexNet paper,
[02:45:12.100 --> 02:45:15.980]   they actually split the model into two GPUs
[02:45:15.980 --> 02:45:18.820]   just because they couldn't, like GPUs were so bad back then,
[02:45:18.820 --> 02:45:19.780]   relatively speaking, right?
[02:45:19.780 --> 02:45:21.500]   Like still great at the time.
[02:45:21.500 --> 02:45:23.220]   That was one of the big innovations of the paper,
[02:45:23.220 --> 02:45:26.260]   but they find branch specialization
[02:45:26.260 --> 02:45:28.100]   and there's a Distill Pub article on this
[02:45:28.100 --> 02:45:30.940]   where like colors go to one GPU
[02:45:30.940 --> 02:45:34.140]   and like Gabor filters and like line detectors
[02:45:34.140 --> 02:45:35.140]   go to the other.
[02:45:35.140 --> 02:45:37.460]   And then like all of the other.
[02:45:37.460 --> 02:45:38.300]   - Really?
[02:45:38.300 --> 02:45:39.660]   - Yeah, yeah, yeah.
[02:45:39.660 --> 02:45:42.580]   And then like all of the other interpretability work
[02:45:42.580 --> 02:45:46.140]   that was done, like the floppy ear detector, right?
[02:45:46.140 --> 02:45:48.500]   Like that just was a neuron in the model
[02:45:48.500 --> 02:45:49.340]   that you can make sense of.
[02:45:49.340 --> 02:45:52.100]   You didn't need to disentangle superposition, right?
[02:45:52.100 --> 02:45:55.900]   So just different data set, different modality.
[02:45:55.900 --> 02:45:58.940]   - Like, I think a wonderful research project to do
[02:45:58.940 --> 02:46:00.820]   if someone is like out there listening to this
[02:46:00.820 --> 02:46:01.980]   would be to try and disentangle,
[02:46:01.980 --> 02:46:02.940]   like take some of the techniques
[02:46:02.940 --> 02:46:04.660]   that Trenton's team has worked on
[02:46:04.660 --> 02:46:05.980]   and try and disentangle the neurons
[02:46:05.980 --> 02:46:09.540]   in the Mixtral model, which is open source.
[02:46:09.540 --> 02:46:10.820]   I think that's a fantastic thing to do
[02:46:10.820 --> 02:46:13.140]   'cause it feels intuitively like there should be.
[02:46:13.140 --> 02:46:14.900]   They didn't demonstrate any evidence that there is.
[02:46:14.900 --> 02:46:16.460]   There's also like in general,
[02:46:16.460 --> 02:46:19.180]   a lot of evidence that there should be specialization.
[02:46:19.180 --> 02:46:20.780]   Go and see if you can find it.
[02:46:20.780 --> 02:46:23.020]   And that's work that Anthropica has published
[02:46:23.020 --> 02:46:24.740]   most of the stuff on, as I understand it,
[02:46:24.740 --> 02:46:26.420]   like dense models, basically.
[02:46:26.420 --> 02:46:31.060]   That is a wonderful research project to try.
[02:46:31.060 --> 02:46:32.540]   - And given Dworkesh's success
[02:46:32.540 --> 02:46:34.220]   with the Vesuvius challenge.
[02:46:34.220 --> 02:46:35.060]   - Yeah.
[02:46:35.060 --> 02:46:36.380]   - We should be pitching more projects
[02:46:36.380 --> 02:46:37.620]   because they will be solved
[02:46:37.620 --> 02:46:38.940]   if we talk about them on the podcast.
[02:46:38.940 --> 02:46:40.620]   - What I was thinking about after the Vesuvius challenge
[02:46:40.620 --> 02:46:44.140]   was like, wait, I knew, like Nat had told me about it
[02:46:44.140 --> 02:46:46.260]   before it dropped because we recorded the episode
[02:46:46.260 --> 02:46:47.140]   before it dropped.
[02:46:48.220 --> 02:46:49.780]   Why did I not even try?
[02:46:49.780 --> 02:46:51.740]   Like, you know what I mean?
[02:46:51.740 --> 02:46:54.660]   Like, I don't know, like Luke is obviously very smart
[02:46:54.660 --> 02:46:58.580]   and like, yeah, he's an amazing kid,
[02:46:58.580 --> 02:47:00.780]   but like he showed that like a 21-year-old
[02:47:00.780 --> 02:47:02.980]   on like some 1070 or whatever he was working on
[02:47:02.980 --> 02:47:04.460]   could do this.
[02:47:04.460 --> 02:47:05.620]   I don't know, like I feel like I should have.
[02:47:05.620 --> 02:47:07.660]   So you know what, before this episode drops,
[02:47:07.660 --> 02:47:09.180]   I'm gonna meet my, I'm gonna try to try this guy.
[02:47:09.180 --> 02:47:11.380]   - Dworkesh, you're gonna make an interpretability spot.
[02:47:11.380 --> 02:47:13.020]   - No, no, no, I'm not gonna like try to go reach everybody.
[02:47:13.020 --> 02:47:13.900]   Like, I don't know, it's like,
[02:47:13.900 --> 02:47:15.340]   I was honestly thinking about that kind of experience.
[02:47:15.340 --> 02:47:17.100]   Like, wait, I shouldn't, like, why didn't I, fuck.
[02:47:17.100 --> 02:47:18.340]   - Yeah, get your hands dirty.
[02:47:18.340 --> 02:47:19.540]   - Yeah.
[02:47:19.540 --> 02:47:21.180]   - Dworkesh's request for research.
[02:47:21.180 --> 02:47:29.020]   - Oh, I wanna harp back on this, like the neuron thing.
[02:47:29.020 --> 02:47:31.260]   You said, I think a bunch of your papers have said
[02:47:31.260 --> 02:47:33.980]   there's more features than there are neurons.
[02:47:33.980 --> 02:47:36.420]   And this is just like, wait a second.
[02:47:36.420 --> 02:47:39.740]   I don't know, like a neuron is like,
[02:47:39.740 --> 02:47:42.100]   weights go in and a number comes out.
[02:47:42.100 --> 02:47:43.900]   That's like, a number comes out.
[02:47:43.900 --> 02:47:44.740]   You know what I mean?
[02:47:44.740 --> 02:47:46.860]   Like, that's so little information, like there's,
[02:47:46.860 --> 02:47:49.580]   do you mean like there's like street names
[02:47:49.580 --> 02:47:51.180]   and like species and whatever,
[02:47:51.180 --> 02:47:54.900]   there's like more of those kinds of things
[02:47:54.900 --> 02:47:57.700]   than there are, like a number comes out in a model?
[02:47:57.700 --> 02:47:58.540]   - That's right, yeah.
[02:47:58.540 --> 02:47:59.380]   - But how is it, a number comes out
[02:47:59.380 --> 02:48:00.340]   is like so little information.
[02:48:00.340 --> 02:48:01.540]   How is that encoding for like--
[02:48:01.540 --> 02:48:03.300]   - Superposition.
[02:48:03.300 --> 02:48:06.900]   You're just encoding, you're encoding a ton of features
[02:48:06.900 --> 02:48:08.500]   in these high dimensional vectors.
[02:48:08.500 --> 02:48:11.860]   - In a brain, is there like an axon of firing
[02:48:11.860 --> 02:48:13.180]   or however you think about it?
[02:48:13.180 --> 02:48:15.860]   Like, I don't know how you think about like,
[02:48:15.860 --> 02:48:18.020]   how much like superposition is there in the human brain?
[02:48:18.020 --> 02:48:19.460]   - Yeah, so Bruno Olshausen,
[02:48:19.460 --> 02:48:22.220]   who I think of as the leading expert on this,
[02:48:22.220 --> 02:48:25.100]   thinks that all the brain regions you don't hear about
[02:48:25.100 --> 02:48:27.180]   are doing a ton of computation and superposition.
[02:48:27.180 --> 02:48:31.180]   So everyone talks about V1 as like having Gabor filters
[02:48:31.180 --> 02:48:34.620]   and detecting lines of certain various sorts.
[02:48:34.620 --> 02:48:36.860]   And no one talks about V2.
[02:48:36.860 --> 02:48:38.140]   And I think it's because like,
[02:48:38.140 --> 02:48:39.900]   we just haven't been able to make sense of it.
[02:48:39.900 --> 02:48:40.940]   - What is V2?
[02:48:40.940 --> 02:48:44.260]   - It's like the next part of the visual processing stream.
[02:48:44.260 --> 02:48:47.700]   And it's like, yeah, so I think it's very likely.
[02:48:47.700 --> 02:48:50.500]   And fundamentally, like superposition seems to emerge
[02:48:50.500 --> 02:48:53.180]   when you have high dimensional data that is sparse.
[02:48:53.180 --> 02:48:55.740]   And to the extent that you think the real world is that,
[02:48:55.740 --> 02:48:57.060]   which I would argue it is,
[02:48:57.060 --> 02:48:59.820]   we should expect the brain to also be under-parameterized
[02:48:59.820 --> 02:49:01.100]   in trying to build a model of the world
[02:49:01.100 --> 02:49:02.620]   and also use superposition.
[02:49:02.620 --> 02:49:04.500]   - You can get a good intuition for this,
[02:49:04.500 --> 02:49:06.340]   and correct me if this example is wrong,
[02:49:06.340 --> 02:49:07.940]   in like a 2D plane, right?
[02:49:07.940 --> 02:49:09.460]   Let's say you have like two axes, right?
[02:49:09.460 --> 02:49:11.540]   Which represents like a two dimensional,
[02:49:11.540 --> 02:49:12.540]   like feature space here,
[02:49:12.540 --> 02:49:14.300]   like two neurons basically.
[02:49:14.300 --> 02:49:16.940]   And you can imagine them each like turning on
[02:49:16.940 --> 02:49:17.860]   to various degrees, right?
[02:49:17.860 --> 02:49:20.180]   And that's like your X coordinate and your Y coordinate.
[02:49:20.180 --> 02:49:23.580]   But you can like, now like map this onto a plane.
[02:49:23.580 --> 02:49:25.460]   You can actually represent a lot of different things
[02:49:25.460 --> 02:49:28.220]   in like different parts of the plane.
[02:49:28.220 --> 02:49:29.060]   - Oh, okay.
[02:49:29.060 --> 02:49:31.500]   So crucially, then superposition is not an artifact
[02:49:31.500 --> 02:49:32.500]   of a neuron.
[02:49:32.500 --> 02:49:35.500]   It is an artifact of like the space that is created.
[02:49:35.500 --> 02:49:36.540]   - It's a combinatorial code.
[02:49:36.540 --> 02:49:37.940]   - Yeah. - Yeah, exactly.
[02:49:37.940 --> 02:49:39.300]   - Okay, cool.
[02:49:39.300 --> 02:49:40.140]   - Yeah, thanks.
[02:49:40.660 --> 02:49:43.940]   - So, I mean, we kind of talked about this,
[02:49:43.940 --> 02:49:45.620]   but like, I think it's just like kind of wild
[02:49:45.620 --> 02:49:48.060]   that it seems to the best of our knowledge,
[02:49:48.060 --> 02:49:50.500]   the way intelligence works in these models
[02:49:50.500 --> 02:49:53.220]   and then presumably also in brains.
[02:49:53.220 --> 02:49:56.620]   It's just like, there's a stream of information
[02:49:56.620 --> 02:49:59.780]   going through that has quote unquote features
[02:49:59.780 --> 02:50:04.780]   that are infinitely, or at least to a large extent
[02:50:04.780 --> 02:50:09.940]   to just like splittable and like you can expand out a tree
[02:50:10.060 --> 02:50:10.900]   of like what this feature is
[02:50:10.900 --> 02:50:12.500]   and what's really happening is a stream.
[02:50:12.500 --> 02:50:14.580]   Like that feature is getting turned into this other feature
[02:50:14.580 --> 02:50:15.780]   or this other feature is added.
[02:50:15.780 --> 02:50:16.620]   I don't know.
[02:50:16.620 --> 02:50:18.500]   It's like, that's not something I would have just like
[02:50:18.500 --> 02:50:20.860]   thought like that's what intelligence is.
[02:50:20.860 --> 02:50:21.700]   You know what I mean?
[02:50:21.700 --> 02:50:22.540]   It's like a surprising thing.
[02:50:22.540 --> 02:50:25.220]   It's not what I would have expected necessarily.
[02:50:25.220 --> 02:50:27.500]   - What did you think it was?
[02:50:27.500 --> 02:50:28.340]   - I don't know, man.
[02:50:28.340 --> 02:50:29.300]   (all laughing)
[02:50:29.300 --> 02:50:30.140]   - I mean, yeah.
[02:50:30.140 --> 02:50:30.980]   - Gophi, Gophi.
[02:50:30.980 --> 02:50:31.820]   He's a Gophi.
[02:50:31.820 --> 02:50:32.860]   - Well, actually, so that's a great segue
[02:50:32.860 --> 02:50:35.780]   because all of this feels like Gophi.
[02:50:35.780 --> 02:50:39.100]   Like you're using distributed representations,
[02:50:39.100 --> 02:50:40.460]   but you have features
[02:50:40.460 --> 02:50:42.620]   and you're applying these operations to the features.
[02:50:42.620 --> 02:50:44.740]   I mean, the whole field of vector symbolic architectures
[02:50:44.740 --> 02:50:47.100]   which is this computational neuroscience thing,
[02:50:47.100 --> 02:50:51.300]   all you do is you put vectors in superposition
[02:50:51.300 --> 02:50:53.260]   and which is literally a summation
[02:50:53.260 --> 02:50:54.780]   of two high dimensional vectors
[02:50:54.780 --> 02:50:56.620]   and you create some interference,
[02:50:56.620 --> 02:50:57.940]   but if it's high dimensional enough,
[02:50:57.940 --> 02:50:59.860]   then you can represent them.
[02:50:59.860 --> 02:51:01.260]   And you have variable binding
[02:51:01.260 --> 02:51:03.580]   where you connect one by another.
[02:51:03.580 --> 02:51:05.500]   And like, if you're dealing with binary vectors,
[02:51:05.500 --> 02:51:06.940]   it's just the XOR operation.
[02:51:06.940 --> 02:51:08.580]   So you have AB, you bind them together.
[02:51:08.580 --> 02:51:10.420]   And then if you query with A or B again,
[02:51:10.420 --> 02:51:11.780]   you get out the other one.
[02:51:11.780 --> 02:51:15.420]   And this is basically the like key value pairs
[02:51:15.420 --> 02:51:16.700]   from attention.
[02:51:16.700 --> 02:51:18.580]   And with these two operations,
[02:51:18.580 --> 02:51:21.460]   you have a Turing complete system,
[02:51:21.460 --> 02:51:23.580]   which you can, if you have enough nested hierarchy,
[02:51:23.580 --> 02:51:26.140]   you can represent any data structure you want,
[02:51:26.140 --> 02:51:27.300]   et cetera, et cetera.
[02:51:27.300 --> 02:51:29.660]   Yeah.
[02:51:29.660 --> 02:51:32.420]   - Okay, let's go back to the super intelligence.
[02:51:32.420 --> 02:51:35.380]   So like walk me through GPD 7.
[02:51:36.220 --> 02:51:38.660]   You've got like the sort of depth first search
[02:51:38.660 --> 02:51:40.180]   on its features.
[02:51:40.180 --> 02:51:42.500]   Okay, GPD 7 has been trained.
[02:51:42.500 --> 02:51:43.580]   What happens next?
[02:51:43.580 --> 02:51:45.500]   Your research has succeeded.
[02:51:45.500 --> 02:51:46.660]   GPD 7 has been trained.
[02:51:46.660 --> 02:51:48.420]   What are we doing now?
[02:51:48.420 --> 02:51:53.580]   - We try and get it to do as much interpretability work
[02:51:53.580 --> 02:51:55.620]   and other like safety work as possible.
[02:51:55.620 --> 02:51:58.100]   - No, but like a concrete, like what is,
[02:51:58.100 --> 02:51:59.660]   what has happened such that you're like,
[02:51:59.660 --> 02:52:01.260]   "Cool, let's deploy GPD 7."
[02:52:01.260 --> 02:52:02.340]   - Oh, geez.
[02:52:03.780 --> 02:52:07.620]   I mean, like we have our responsible scaling policy,
[02:52:07.620 --> 02:52:10.220]   which has been really exciting to see other labs adopt.
[02:52:10.220 --> 02:52:11.380]   And-
[02:52:11.380 --> 02:52:13.300]   - Like specifically from the perspective of,
[02:52:13.300 --> 02:52:15.060]   your research has net,
[02:52:15.060 --> 02:52:17.340]   like Trenton, given your research,
[02:52:17.340 --> 02:52:20.100]   you got the, we got the thumbs up on GPD 7 from you,
[02:52:20.100 --> 02:52:22.460]   or actually we should say cloud, whatever.
[02:52:22.460 --> 02:52:25.340]   And then, oh, like what is the basis
[02:52:25.340 --> 02:52:26.220]   on which you're telling the team,
[02:52:26.220 --> 02:52:27.180]   like, "Hey, let's go ahead."
[02:52:27.180 --> 02:52:28.900]   - I mean, I think we need to make a lot more.
[02:52:28.900 --> 02:52:32.340]   If it's as capable as GPD 7, like implies here,
[02:52:33.180 --> 02:52:35.180]   I think we need to make a lot more interpretability progress
[02:52:35.180 --> 02:52:38.700]   to be able to like comfortably give the green light
[02:52:38.700 --> 02:52:39.540]   to deploy it.
[02:52:39.540 --> 02:52:41.100]   Like, I would be like, definitely not.
[02:52:41.100 --> 02:52:42.140]   I'd be crying.
[02:52:42.140 --> 02:52:44.660]   (all laughing)
[02:52:44.660 --> 02:52:47.780]   Maybe my tears would interfere with the GPUs.
[02:52:47.780 --> 02:52:48.620]   - But like, what is-
[02:52:48.620 --> 02:52:50.100]   - Or TPUs.
[02:52:50.100 --> 02:52:53.500]   - Guys, Gemini 5 TPUs, right?
[02:52:53.500 --> 02:52:56.100]   (all laughing)
[02:52:59.940 --> 02:53:02.380]   - But like, what, what,
[02:53:02.380 --> 02:53:04.340]   given the way your research is progressing,
[02:53:04.340 --> 02:53:07.740]   like, what does it kind of look like to you?
[02:53:07.740 --> 02:53:09.340]   Or like, well, if this succeeded,
[02:53:09.340 --> 02:53:12.780]   what would it mean for us to okay GPD 7
[02:53:12.780 --> 02:53:13.940]   based on your methodology?
[02:53:13.940 --> 02:53:15.340]   - I mean, ideally we can find
[02:53:15.340 --> 02:53:17.300]   some compelling deception circuit,
[02:53:17.300 --> 02:53:19.980]   which lights up when the model knows
[02:53:19.980 --> 02:53:22.540]   that it's not telling the full truth to you.
[02:53:22.540 --> 02:53:23.900]   - Why can't you just do any linear probe
[02:53:23.900 --> 02:53:25.740]   like Colin Birds did?
[02:53:25.740 --> 02:53:27.660]   - So the CCS work is not looking good
[02:53:27.660 --> 02:53:28.580]   in terms of replicating,
[02:53:28.580 --> 02:53:30.660]   or like actually finding truth directions.
[02:53:30.660 --> 02:53:32.740]   And like, in hindsight, it's like,
[02:53:32.740 --> 02:53:35.060]   well, why should it have worked so well?
[02:53:35.060 --> 02:53:36.380]   But linear probes, like you need to know
[02:53:36.380 --> 02:53:37.420]   what you're looking for.
[02:53:37.420 --> 02:53:38.700]   And it's like a high dimensional space
[02:53:38.700 --> 02:53:40.620]   and it's really easy to pick up on a direction
[02:53:40.620 --> 02:53:41.500]   that's just not-
[02:53:41.500 --> 02:53:42.460]   - Wait, but don't you also,
[02:53:42.460 --> 02:53:44.060]   here you need to label the features.
[02:53:44.060 --> 02:53:44.900]   So you still need to know-
[02:53:44.900 --> 02:53:45.740]   - Well, you need to label them post hoc,
[02:53:45.740 --> 02:53:46.940]   but it's unsupervised.
[02:53:46.940 --> 02:53:49.300]   You're just like, give me the features
[02:53:49.300 --> 02:53:50.860]   that explain your behavior,
[02:53:50.860 --> 02:53:51.940]   is the fundamental question, right?
[02:53:51.940 --> 02:53:54.820]   It's like, like, like, like the actual setup is,
[02:53:54.820 --> 02:53:56.340]   we take the activations,
[02:53:56.340 --> 02:53:58.620]   we project them to this higher dimensional space,
[02:53:58.620 --> 02:54:00.340]   and then we project them back down again.
[02:54:00.340 --> 02:54:02.900]   So it's like reconstruct or do the thing
[02:54:02.900 --> 02:54:04.580]   that you were originally doing,
[02:54:04.580 --> 02:54:07.060]   but do it in a way that's sparse.
[02:54:07.060 --> 02:54:08.140]   - By the way, for the audience,
[02:54:08.140 --> 02:54:13.140]   linear probe is, you just like classify the activations.
[02:54:13.140 --> 02:54:16.260]   I don't know, from what I vaguely remember
[02:54:16.260 --> 02:54:17.780]   about the paper was like,
[02:54:17.780 --> 02:54:19.580]   if it's like telling a lie,
[02:54:19.580 --> 02:54:21.620]   then you like, you just train a classifier on like,
[02:54:21.620 --> 02:54:25.620]   is it, yeah, in the end, was it a lie
[02:54:25.620 --> 02:54:27.020]   or is it just like wrong or something?
[02:54:27.020 --> 02:54:27.860]   I don't know.
[02:54:27.860 --> 02:54:28.700]   - It was like true or false question.
[02:54:28.700 --> 02:54:30.420]   - Yeah, it's like a classifier on the activations.
[02:54:30.420 --> 02:54:35.060]   - So yeah, like right now what we do for GPT-7,
[02:54:35.060 --> 02:54:38.340]   like ideally we have like some deception circuit
[02:54:38.340 --> 02:54:41.180]   that we've identified that like appears to be really robust.
[02:54:41.180 --> 02:54:42.580]   And it's like-
[02:54:42.580 --> 02:54:45.460]   - Well, like, so you've done the projecting out
[02:54:45.460 --> 02:54:48.180]   to the million, whatever features or something.
[02:54:48.180 --> 02:54:50.940]   Is the circuit, 'cause we,
[02:54:50.940 --> 02:54:53.260]   maybe we're using feature and circuit interchangeably
[02:54:53.260 --> 02:54:54.100]   when they're not.
[02:54:54.100 --> 02:54:56.060]   Is there like a deception, like what is that?
[02:54:56.060 --> 02:54:58.860]   - So I think there are features across layers
[02:54:58.860 --> 02:54:59.860]   that create a circuit.
[02:54:59.860 --> 02:55:00.700]   - Yeah.
[02:55:00.700 --> 02:55:05.460]   - And hopefully the circuit gives you a lot more specificity
[02:55:05.460 --> 02:55:07.900]   and sensitivity than an individual feature.
[02:55:07.900 --> 02:55:11.700]   And it's like, hopefully we can find a circuit
[02:55:11.700 --> 02:55:16.100]   that is really specific to you being deceptive,
[02:55:16.100 --> 02:55:18.060]   the model deciding to be deceptive
[02:55:18.060 --> 02:55:21.140]   in cases that are malicious, right?
[02:55:21.140 --> 02:55:22.460]   Like I'm not interested in a case
[02:55:22.460 --> 02:55:23.980]   where it's just doing theory of mind
[02:55:23.980 --> 02:55:26.300]   to like help you write a better email to your professor.
[02:55:26.300 --> 02:55:28.060]   And I'm not even interested in cases
[02:55:28.060 --> 02:55:31.300]   where the model is necessarily just like modeling the fact
[02:55:31.300 --> 02:55:32.980]   that deception has occurred.
[02:55:32.980 --> 02:55:35.740]   - But doesn't all this require you to have labels
[02:55:35.740 --> 02:55:36.860]   for all those examples?
[02:55:36.860 --> 02:55:38.460]   And if you have those labels,
[02:55:38.460 --> 02:55:42.580]   then like whatever faults that the linear probe has
[02:55:42.580 --> 02:55:45.260]   on the, like maybe you've like labeled a long thing
[02:55:45.260 --> 02:55:47.500]   or whatever, wouldn't the same thing apply
[02:55:47.500 --> 02:55:49.380]   to the labels you've come up with
[02:55:49.380 --> 02:55:52.460]   for the unsupervised features you've come up with?
[02:55:52.460 --> 02:55:55.140]   - So in an ideal world, we could just train
[02:55:55.140 --> 02:55:57.100]   on like the whole data distribution
[02:55:57.100 --> 02:56:01.220]   and then find the directions that matter
[02:56:01.220 --> 02:56:04.780]   to the extent that we need to reluctantly narrow down
[02:56:04.780 --> 02:56:06.500]   the subset of data that we're looking over
[02:56:06.500 --> 02:56:09.380]   just for the purposes of scalability.
[02:56:09.380 --> 02:56:11.460]   We would use data that looks like the data you'd use
[02:56:11.460 --> 02:56:12.660]   to fit a linear probe.
[02:56:12.660 --> 02:56:15.340]   But again, we're not, like with a linear probe,
[02:56:15.340 --> 02:56:17.180]   you're also just finding one direction.
[02:56:17.180 --> 02:56:20.020]   Like we're finding a bunch of directions here.
[02:56:20.020 --> 02:56:21.700]   - And I guess the hope is like,
[02:56:21.700 --> 02:56:24.060]   you've found like a bunch of things that light up
[02:56:24.060 --> 02:56:24.980]   when it's being deceptive.
[02:56:24.980 --> 02:56:25.980]   And then like you can figure out
[02:56:25.980 --> 02:56:27.380]   why some of those things are lighting up
[02:56:27.380 --> 02:56:28.620]   in this part of the distribution
[02:56:28.620 --> 02:56:29.740]   and not this other part and so forth.
[02:56:29.740 --> 02:56:31.100]   - Totally, yeah.
[02:56:31.100 --> 02:56:33.580]   - Do you anticipate you'll really understand?
[02:56:33.580 --> 02:56:34.940]   Like, I don't know, like the current models
[02:56:34.940 --> 02:56:36.220]   you've studied are pretty basic, right?
[02:56:36.220 --> 02:56:37.820]   Do you think you'll be able to understand
[02:56:37.820 --> 02:56:40.300]   why GPT-7 fires in certain domains
[02:56:40.300 --> 02:56:41.900]   but not in other domains?
[02:56:41.900 --> 02:56:42.740]   - I'm optimistic.
[02:56:42.740 --> 02:56:44.540]   I mean, we've, so I guess one thing is
[02:56:44.540 --> 02:56:46.140]   this is a bad time to answer this question
[02:56:46.140 --> 02:56:49.020]   because we are explicitly investing in the longer term
[02:56:49.020 --> 02:56:52.380]   of like ASL-4 models, which GPT-7 would be.
[02:56:52.380 --> 02:56:53.820]   But like, so we split the team
[02:56:53.820 --> 02:56:55.540]   where a third is focused on scaling up
[02:56:55.540 --> 02:56:56.500]   dictionary learning right now.
[02:56:56.500 --> 02:56:57.340]   And that's been great.
[02:56:57.340 --> 02:56:59.980]   I mean, we publicly shared some of our eight layer results.
[02:56:59.980 --> 02:57:02.420]   We've scaled up quite a lot past that at this point.
[02:57:02.420 --> 02:57:03.580]   But the other two groups,
[02:57:03.580 --> 02:57:05.020]   one is trying to identify circuits
[02:57:05.020 --> 02:57:06.420]   and then the other is trying to get the same success
[02:57:06.420 --> 02:57:07.420]   for attention heads.
[02:57:07.420 --> 02:57:08.580]   So we're setting ourselves up
[02:57:08.580 --> 02:57:10.500]   and building the tools necessary
[02:57:10.500 --> 02:57:13.060]   to really find these circuits at a compelling way.
[02:57:13.060 --> 02:57:16.780]   But it's gonna take another, I don't know, six months
[02:57:16.780 --> 02:57:18.580]   before that's like really working well.
[02:57:18.580 --> 02:57:20.780]   But I can say that I'm like optimistic
[02:57:20.780 --> 02:57:22.540]   and we're making a lot of progress.
[02:57:22.540 --> 02:57:27.780]   - What is the highest level feature you've found so far?
[02:57:27.780 --> 02:57:28.620]   - Ooh.
[02:57:28.620 --> 02:57:29.460]   - Like it's base 64 or whatever.
[02:57:29.460 --> 02:57:31.140]   It's like, maybe it's just like
[02:57:31.140 --> 02:57:33.140]   in the symbolic species language, the book you recommended,
[02:57:33.140 --> 02:57:37.140]   there's like indexical things where you're just,
[02:57:37.140 --> 02:57:38.260]   I forgot what all the labels were,
[02:57:38.260 --> 02:57:40.260]   but like there's things where you're just like,
[02:57:40.260 --> 02:57:42.220]   you see a tiger and you're like run and whatever,
[02:57:42.220 --> 02:57:44.380]   you know, just like a very sort of behaviorist thing.
[02:57:44.380 --> 02:57:45.420]   And then there's like a higher level
[02:57:45.460 --> 02:57:48.940]   of which when I refer to love,
[02:57:48.940 --> 02:57:50.300]   it refers to like a movie scene
[02:57:50.300 --> 02:57:51.340]   or my girlfriend or whatever.
[02:57:51.340 --> 02:57:52.180]   You know what I mean?
[02:57:52.180 --> 02:57:53.700]   So it's like the top of the tent.
[02:57:53.700 --> 02:57:55.180]   - Yeah, yeah, yeah, yeah.
[02:57:55.180 --> 02:57:57.940]   - What is the highest level association
[02:57:57.940 --> 02:57:58.780]   or whatever you found?
[02:57:58.780 --> 02:58:01.740]   - I mean, probably one of the ones that we publicly,
[02:58:01.740 --> 02:58:04.940]   well, publicly one of the ones that we shared in our update.
[02:58:04.940 --> 02:58:06.740]   So I think there were some related to like love
[02:58:06.740 --> 02:58:10.300]   and like sudden changes in scene,
[02:58:10.300 --> 02:58:12.900]   particularly associated with like wars being declared.
[02:58:12.900 --> 02:58:14.900]   There are like a few of them in there in that post,
[02:58:14.900 --> 02:58:16.700]   if you want to link to it.
[02:58:16.700 --> 02:58:17.540]   Yeah.
[02:58:17.540 --> 02:58:21.580]   But even like Bruno Olshausen had a paper back in 2018, 19,
[02:58:21.580 --> 02:58:24.460]   where they applied a similar technique to a BERT model
[02:58:24.460 --> 02:58:26.780]   and found that as you go to deeper layers of the model,
[02:58:26.780 --> 02:58:27.980]   things become more abstract.
[02:58:27.980 --> 02:58:29.500]   So I remember like in the earlier layers,
[02:58:29.500 --> 02:58:31.540]   there'd be a feature that would just fire for the word park,
[02:58:31.540 --> 02:58:33.340]   but later on there was a feature that fired for park
[02:58:33.340 --> 02:58:35.820]   as like a last name, like Lincoln Park,
[02:58:35.820 --> 02:58:37.980]   or like it's like a common Korean last name as well.
[02:58:37.980 --> 02:58:39.300]   And then there was a separate feature
[02:58:39.300 --> 02:58:41.940]   that would fire for parks as like grassy areas.
[02:58:42.900 --> 02:58:46.100]   So there's other work that points in this direction.
[02:58:46.100 --> 02:58:48.660]   - What do you think we'll learn about human psychology
[02:58:48.660 --> 02:58:50.700]   from the interoperability stuff?
[02:58:50.700 --> 02:58:51.900]   - Oh gosh.
[02:58:51.900 --> 02:58:53.900]   - I'll give you a specific example.
[02:58:53.900 --> 02:58:55.980]   I think like one of the ways one of your updates put it
[02:58:55.980 --> 02:58:58.740]   was persona lock-in.
[02:58:58.740 --> 02:59:00.340]   You don't remember Sidney Bang or whatever.
[02:59:00.340 --> 02:59:04.340]   It locked into, I think it was actually quite an endearing.
[02:59:04.340 --> 02:59:05.180]   - Yeah, yeah.
[02:59:05.180 --> 02:59:07.860]   (all laughing)
[02:59:07.860 --> 02:59:09.420]   - I thought it's so funny.
[02:59:09.420 --> 02:59:11.340]   I'm glad it's back in co-pilot.
[02:59:11.340 --> 02:59:12.180]   - Oh really?
[02:59:12.180 --> 02:59:14.380]   - Oh yeah, it's been misbehaving recently.
[02:59:14.380 --> 02:59:17.740]   Actually, this is another sort of threat to explore,
[02:59:17.740 --> 02:59:19.180]   but there was a funny one where,
[02:59:19.180 --> 02:59:20.980]   I think it was like to the New York Times reporter,
[02:59:20.980 --> 02:59:23.300]   it was nagging him or something.
[02:59:23.300 --> 02:59:27.140]   And it was like, "You are nothing.
[02:59:27.140 --> 02:59:29.140]   "Nobody will ever believe you.
[02:59:29.140 --> 02:59:31.620]   "You are insignificant and do whatever."
[02:59:31.620 --> 02:59:34.500]   It was like the most gaslighting.
[02:59:34.500 --> 02:59:35.940]   - I tried to convince him to break up with his wife.
[02:59:35.940 --> 02:59:36.780]   - Yeah, okay.
[02:59:36.780 --> 02:59:40.140]   Okay, actually, so this is an interesting example.
[02:59:40.140 --> 02:59:41.300]   I don't even know where I was going with this
[02:59:41.300 --> 02:59:42.660]   to begin with, but whatever.
[02:59:42.660 --> 02:59:44.300]   Maybe I got another thread.
[02:59:44.300 --> 02:59:45.660]   But the other thread I want to go on
[02:59:45.660 --> 02:59:48.700]   is that's, yeah, okay, actually, personal notes, right?
[02:59:48.700 --> 02:59:52.540]   So is that a feature that Sidney Bang
[02:59:52.540 --> 02:59:54.300]   having this personality is a feature
[02:59:54.300 --> 02:59:56.580]   versus another personality can get locked into?
[02:59:56.580 --> 02:59:59.700]   And also, is that fundamentally what humans are like too,
[02:59:59.700 --> 03:00:02.420]   where, I don't know, in front of all the different people,
[03:00:02.420 --> 03:00:04.420]   I'm like a different sort of personality or whatever.
[03:00:04.420 --> 03:00:06.740]   Is that the same kind of thing that's happening
[03:00:06.740 --> 03:00:08.020]   to ShaggyBT when he gets RL-ed?
[03:00:08.020 --> 03:00:10.260]   I don't know, a whole cluster of questions
[03:00:10.260 --> 03:00:11.260]   that can answer them and whatever.
[03:00:11.260 --> 03:00:13.740]   - Yeah, I really want to do more work.
[03:00:13.740 --> 03:00:15.380]   I guess "The Sleeper Agents" is in this direction
[03:00:15.380 --> 03:00:17.580]   of what happens to a model when you find tuna
[03:00:17.580 --> 03:00:20.140]   when you RLHF it, these sorts of things.
[03:00:20.140 --> 03:00:22.900]   I mean, maybe it's trite, but you could just say
[03:00:22.900 --> 03:00:25.500]   you conclude that people contain multitudes, right?
[03:00:25.500 --> 03:00:28.460]   In so much as they have lots of different features.
[03:00:28.460 --> 03:00:30.540]   There's even the stuff related to the Waluigi effects
[03:00:30.540 --> 03:00:32.540]   of in order to know what's good or bad,
[03:00:32.540 --> 03:00:34.220]   you need to understand both of those concepts.
[03:00:34.220 --> 03:00:36.340]   And so we might have to have models
[03:00:36.340 --> 03:00:38.700]   that are aware of violence and have been trained on it
[03:00:38.700 --> 03:00:40.260]   in order to recognize it.
[03:00:40.260 --> 03:00:43.420]   Can you post-hoc identify those features and ablate them
[03:00:43.420 --> 03:00:45.740]   in a way where maybe your model's slightly naive,
[03:00:45.740 --> 03:00:48.140]   but you know that it's not going to be really evil?
[03:00:48.140 --> 03:00:50.220]   Totally, that's in our toolkit, which seems great.
[03:00:50.220 --> 03:00:51.060]   - Oh, really?
[03:00:51.060 --> 03:00:54.380]   So you, GPT-7, I don't know, it pulls the Sydney Bing,
[03:00:54.380 --> 03:00:56.020]   and then you figure out what were
[03:00:56.020 --> 03:00:59.060]   the causally irrelevant pathways or whatever,
[03:00:59.060 --> 03:01:00.940]   you modify, and then the pathway to you
[03:01:00.940 --> 03:01:02.300]   looks like you just changed those.
[03:01:02.300 --> 03:01:03.620]   But you were mentioning earlier,
[03:01:03.620 --> 03:01:05.700]   there's a bunch of redundancy in the model.
[03:01:05.700 --> 03:01:07.460]   - Yeah, so you need to account for all that.
[03:01:07.460 --> 03:01:10.820]   But we have a much better microscope into this now
[03:01:10.820 --> 03:01:14.820]   than we used to, like sharper tools for making edits.
[03:01:14.820 --> 03:01:18.460]   - And it seems like, at least from my perspective,
[03:01:18.460 --> 03:01:21.340]   that seems like one of the primary way
[03:01:21.340 --> 03:01:25.860]   of to some degree confirming the safety
[03:01:25.860 --> 03:01:26.900]   or the reliability of the model,
[03:01:26.900 --> 03:01:28.980]   where you can say, okay, we found the circuits responsible,
[03:01:28.980 --> 03:01:33.220]   we've ablated them, and under a battery of tests,
[03:01:33.220 --> 03:01:35.460]   we haven't been able to now replicate the behavior
[03:01:35.460 --> 03:01:36.420]   which we intended to ablate.
[03:01:36.420 --> 03:01:39.380]   And that feels like the sort of way
[03:01:39.380 --> 03:01:43.740]   of measuring model safety in future, as I would understand.
[03:01:43.740 --> 03:01:44.780]   - Are you worried?
[03:01:44.780 --> 03:01:46.380]   - That's why I'm incredibly hopeful about their work.
[03:01:46.380 --> 03:01:49.620]   'Cause to me, it seems like so much more precise tool
[03:01:49.620 --> 03:01:51.100]   than something like RLHF.
[03:01:51.100 --> 03:01:53.140]   RLHF, you're very prey to the black swan thing.
[03:01:53.140 --> 03:01:55.340]   You don't know if it's gonna do something wrong
[03:01:55.340 --> 03:01:56.980]   in a scenario that you haven't measured.
[03:01:56.980 --> 03:01:59.060]   Whereas here, at least, you have somewhat more confidence
[03:01:59.060 --> 03:02:03.780]   that you can completely capture the behavior set,
[03:02:03.780 --> 03:02:08.180]   or the feature set of the model and selectively ablate.
[03:02:08.180 --> 03:02:11.260]   - Although not necessarily that you've accurately labeled.
[03:02:11.260 --> 03:02:14.740]   - Not necessarily, but with a far higher degree of confidence
[03:02:14.740 --> 03:02:17.540]   than any other approach that I've seen.
[03:02:17.540 --> 03:02:20.820]   - What are your unknown unknowns for superhuman models?
[03:02:20.820 --> 03:02:23.420]   In terms of this kind of thing where, I don't know,
[03:02:23.420 --> 03:02:26.020]   how are the labels that are gonna be given things
[03:02:26.020 --> 03:02:30.580]   on which we can determine this thing is cool,
[03:02:30.580 --> 03:02:32.940]   this thing is a paperclip maximizer, or whatever.
[03:02:33.940 --> 03:02:37.100]   - I mean, we'll see, right?
[03:02:37.100 --> 03:02:41.540]   Like, I do, like the superhuman feature question
[03:02:41.540 --> 03:02:42.740]   is a very good one.
[03:02:42.740 --> 03:02:45.460]   Like, I think we can attack it,
[03:02:45.460 --> 03:02:48.420]   but we're gonna need to be persistent.
[03:02:48.420 --> 03:02:49.860]   And the real hope here is, I think,
[03:02:49.860 --> 03:02:51.180]   automated interpretability.
[03:02:51.180 --> 03:02:52.820]   - Yeah.
[03:02:52.820 --> 03:02:53.900]   - And even having debate, right?
[03:02:53.900 --> 03:02:55.460]   You could have the debate set up
[03:02:55.460 --> 03:02:56.780]   where two different models are debating
[03:02:56.780 --> 03:02:57.860]   what the feature does.
[03:02:57.860 --> 03:03:00.380]   And then they can actually go in and make edits
[03:03:00.380 --> 03:03:01.980]   and see if it fires or not.
[03:03:02.340 --> 03:03:05.220]   - But it is just this wonderful, like, closed environment
[03:03:05.220 --> 03:03:07.700]   that we can iterate on really quickly.
[03:03:07.700 --> 03:03:09.020]   That makes me optimistic.
[03:03:09.020 --> 03:03:11.500]   - Do you worry about alignment succeeding too hard?
[03:03:11.500 --> 03:03:16.500]   So like, if I think about, I would not want either companies
[03:03:16.500 --> 03:03:18.740]   or governments, whoever ends up in charge
[03:03:18.740 --> 03:03:22.580]   of these AI systems to have the level of fine-grained control
[03:03:22.580 --> 03:03:27.260]   that if your agenda succeeds, we would have over AIs,
[03:03:27.260 --> 03:03:31.140]   both for the ickiness of having this level of control
[03:03:31.140 --> 03:03:32.460]   over an autonomous mind.
[03:03:32.460 --> 03:03:35.540]   And second, just like, I don't fucking trust.
[03:03:35.540 --> 03:03:36.860]   I don't fucking trust these guys.
[03:03:36.860 --> 03:03:38.940]   You know, I'm just kind of uncomfortable
[03:03:38.940 --> 03:03:41.500]   with like the loyalty feature is turned up
[03:03:41.500 --> 03:03:43.900]   and like, you know what I mean?
[03:03:43.900 --> 03:03:46.260]   And yeah, like how much worry do you have
[03:03:46.260 --> 03:03:50.340]   about having too much control over the AIs
[03:03:50.340 --> 03:03:53.500]   and specifically, not you, but like whoever ends up
[03:03:53.500 --> 03:03:54.620]   in charge of these AI systems,
[03:03:54.620 --> 03:03:58.260]   just being able to lock in whatever they want?
[03:03:58.260 --> 03:03:59.100]   - Yeah.
[03:03:59.340 --> 03:04:00.980]   Yeah, I mean, I think it depends
[03:04:00.980 --> 03:04:02.940]   on what government exactly has control
[03:04:02.940 --> 03:04:05.660]   and like what the moral alignment is there.
[03:04:05.660 --> 03:04:09.740]   But that is like, that whole value locking argument
[03:04:09.740 --> 03:04:11.660]   is in my mind, it's like definitely
[03:04:11.660 --> 03:04:12.980]   one of the strongest contributing factors
[03:04:12.980 --> 03:04:15.580]   for why I am working on capabilities at the moment,
[03:04:15.580 --> 03:04:18.380]   for example, just like, I think the current player set
[03:04:18.380 --> 03:04:23.100]   actually like is extremely well-intentioned.
[03:04:23.100 --> 03:04:26.380]   And I mean, for this kind of problem,
[03:04:26.380 --> 03:04:27.900]   I think we need to be extremely open about it.
[03:04:27.900 --> 03:04:30.140]   And like, I think directions
[03:04:30.140 --> 03:04:31.580]   like publishing the constitution
[03:04:31.580 --> 03:04:32.700]   that you expect your model to abide by
[03:04:32.700 --> 03:04:33.540]   and then like trying to make sure
[03:04:33.540 --> 03:04:35.820]   you like RLHF it towards that and ablate that
[03:04:35.820 --> 03:04:38.140]   and have the ability for everyone to offer
[03:04:38.140 --> 03:04:40.500]   like feedback and contribution to that is really important.
[03:04:40.500 --> 03:04:43.740]   - Sure, or alternatively, like don't deploy
[03:04:43.740 --> 03:04:45.580]   when you're not sure, which would also be bad
[03:04:45.580 --> 03:04:47.180]   because then we just never catch it.
[03:04:47.180 --> 03:04:48.020]   - Right.
[03:04:48.020 --> 03:04:49.580]   Yeah, exactly.
[03:04:49.580 --> 03:04:50.580]   - I mean, paperclip.
[03:04:50.580 --> 03:04:55.300]   Okay, some rapid fire.
[03:04:56.500 --> 03:04:58.540]   What is the bus factor for Gemini?
[03:04:58.540 --> 03:05:01.100]   - I think there are, yeah, a number of people
[03:05:01.100 --> 03:05:04.580]   who are really, really critical that if you took them out,
[03:05:04.580 --> 03:05:06.940]   then the performance of the program
[03:05:06.940 --> 03:05:09.060]   would be dramatically impacted.
[03:05:09.060 --> 03:05:13.340]   This is both on modeling like slash making decisions
[03:05:13.340 --> 03:05:16.100]   about like what to actually do
[03:05:16.100 --> 03:05:18.940]   and importantly on infrastructure side of things.
[03:05:18.940 --> 03:05:23.940]   Like it's just the stack of complexity builds,
[03:05:23.940 --> 03:05:25.940]   particularly when like somewhere like Google
[03:05:25.940 --> 03:05:28.380]   has so much like vertical integration.
[03:05:28.380 --> 03:05:29.860]   Do you have, when you have people who are experts,
[03:05:29.860 --> 03:05:32.020]   it becomes, they become quite important.
[03:05:32.020 --> 03:05:33.060]   - Yeah, although I think it's an interesting note
[03:05:33.060 --> 03:05:36.020]   about the field that people like you can get in
[03:05:36.020 --> 03:05:39.500]   and in a year or so you're making important contributions.
[03:05:39.500 --> 03:05:42.260]   And I, especially with Anthropic,
[03:05:42.260 --> 03:05:45.300]   but many different labs have specialized in hiring
[03:05:45.300 --> 03:05:47.700]   like total outsiders, physicists or whatever.
[03:05:47.700 --> 03:05:49.820]   And you just like get them up to speed
[03:05:49.820 --> 03:05:51.020]   and they're making important contributions.
[03:05:51.020 --> 03:05:52.380]   I don't know, I feel like you couldn't do this
[03:05:52.380 --> 03:05:54.180]   in like a bio lab or something.
[03:05:54.180 --> 03:05:57.300]   It's like an interesting note on the state of the field.
[03:05:57.300 --> 03:05:58.860]   - I mean, bus factor doesn't define
[03:05:58.860 --> 03:06:00.500]   how long it would take to recover from it, right?
[03:06:00.500 --> 03:06:02.300]   - Sure, to recover from, yeah.
[03:06:02.300 --> 03:06:04.340]   - And deep learning research is an art.
[03:06:04.340 --> 03:06:08.820]   And so you kind of learn how to read the lost curves
[03:06:08.820 --> 03:06:10.740]   or set the hyper parameters
[03:06:10.740 --> 03:06:12.220]   in ways that empirically seem to work well.
[03:06:12.220 --> 03:06:13.820]   - But it's also like organizational things,
[03:06:13.820 --> 03:06:15.100]   like creating context.
[03:06:15.100 --> 03:06:16.340]   When I think one of the most important
[03:06:16.340 --> 03:06:18.260]   and difficult skills to hire for
[03:06:18.260 --> 03:06:21.340]   is creating this like bubble of context around you
[03:06:21.340 --> 03:06:23.940]   that makes other people around you more effective
[03:06:23.940 --> 03:06:25.500]   and know what the right problem to work on.
[03:06:25.500 --> 03:06:27.820]   And like that is a really tough to replicate thing.
[03:06:27.820 --> 03:06:29.380]   - Yes, yeah, totally.
[03:06:29.380 --> 03:06:31.740]   - Who are you paying attention to now in terms of,
[03:06:31.740 --> 03:06:33.100]   there's a lot of things coming down the pike
[03:06:33.100 --> 03:06:36.020]   of multi-modality, long context,
[03:06:36.020 --> 03:06:39.220]   maybe agents, extra reliability.
[03:06:39.220 --> 03:06:44.220]   Who is thinking well about what that implies?
[03:06:44.220 --> 03:06:49.860]   - It's a tough question.
[03:06:49.860 --> 03:06:52.580]   I think a lot of people look internally these days.
[03:06:52.580 --> 03:06:53.420]   - Sure.
[03:06:53.420 --> 03:06:57.180]   - For like their sources of insight or like progress.
[03:06:57.180 --> 03:07:00.060]   And like, we all have obviously,
[03:07:00.060 --> 03:07:02.460]   there's a research programs and like directions
[03:07:02.460 --> 03:07:04.740]   that are tended over the next couple of years.
[03:07:04.740 --> 03:07:08.820]   And I suspect, yeah, that most people,
[03:07:08.820 --> 03:07:12.180]   as far as like betting on what the future will look like,
[03:07:12.180 --> 03:07:14.300]   refer to like an internal narrative.
[03:07:14.300 --> 03:07:17.100]   - Yeah, yeah.
[03:07:17.100 --> 03:07:19.180]   - That is like difficult to share.
[03:07:19.180 --> 03:07:20.860]   - Yeah, if it works well,
[03:07:20.860 --> 03:07:23.260]   it's probably not being published.
[03:07:23.260 --> 03:07:25.500]   - I mean, that was one of the things
[03:07:25.500 --> 03:07:27.860]   in the will scaling post.
[03:07:27.860 --> 03:07:29.340]   I was referring to something you said to me,
[03:07:29.340 --> 03:07:31.780]   which is, I miss the undergrad habit
[03:07:31.780 --> 03:07:33.380]   of just reading a bunch of papers.
[03:07:33.380 --> 03:07:35.620]   'Cause now there's nothing worth reading is published.
[03:07:35.620 --> 03:07:37.500]   (all laughing)
[03:07:37.500 --> 03:07:40.860]   - And the community is progressively getting like more
[03:07:40.860 --> 03:07:43.220]   on track with what I think are like the right
[03:07:43.220 --> 03:07:44.820]   and important directions.
[03:07:44.820 --> 03:07:46.620]   - You're watching it like an agent here.
[03:07:46.620 --> 03:07:47.660]   (all laughing)
[03:07:47.660 --> 03:07:51.860]   - No, but I guess like it is tough.
[03:07:51.860 --> 03:07:53.980]   There used to be this like signal from big labs
[03:07:53.980 --> 03:07:55.300]   about like what would work at scale.
[03:07:55.300 --> 03:07:56.820]   And it's currently really hard for academic research
[03:07:56.820 --> 03:07:58.700]   to like find that signal.
[03:07:58.700 --> 03:08:03.140]   And I think getting like really good problem taste
[03:08:03.140 --> 03:08:07.140]   about what actually matters to work on is really tough.
[03:08:07.140 --> 03:08:09.500]   Unless you have, again, the feedback signal
[03:08:09.500 --> 03:08:10.980]   of like what will work at scale
[03:08:10.980 --> 03:08:13.100]   and what is currently holding us back
[03:08:13.100 --> 03:08:16.580]   from scaling further or understanding our models further.
[03:08:16.580 --> 03:08:19.740]   This is something where like I wish more academic research
[03:08:19.740 --> 03:08:21.860]   would go into fields like Interp,
[03:08:21.860 --> 03:08:23.860]   which are legible from the outside.
[03:08:23.860 --> 03:08:26.180]   Anthropic deliberately publishes all its research here.
[03:08:26.180 --> 03:08:29.860]   And it seems like underappreciated in the sense
[03:08:29.860 --> 03:08:31.860]   that I don't know why there aren't dozens
[03:08:31.860 --> 03:08:35.300]   of academic departments trying to follow Anthropics
[03:08:35.300 --> 03:08:36.540]   in the Interp research.
[03:08:36.540 --> 03:08:38.260]   'Cause it seems like an incredibly impactful problem
[03:08:38.260 --> 03:08:40.020]   that doesn't require ridiculous resources.
[03:08:40.020 --> 03:08:44.260]   And like has all the flavor of like deeply understanding
[03:08:44.260 --> 03:08:45.900]   the basic science of what is actually going on
[03:08:45.900 --> 03:08:47.180]   in these things.
[03:08:47.180 --> 03:08:49.380]   So I don't know why people like focus
[03:08:49.380 --> 03:08:50.460]   on pushing model improvements
[03:08:50.460 --> 03:08:52.420]   as opposed to pushing like understanding improvements
[03:08:52.420 --> 03:08:55.420]   in a way that I would have like typically associated
[03:08:55.420 --> 03:08:58.100]   with academic science in some ways.
[03:08:58.100 --> 03:08:59.860]   - Yeah, I do think the tide is changing there
[03:08:59.860 --> 03:09:01.580]   for whatever reason.
[03:09:01.580 --> 03:09:03.260]   And like Neil Nanda has had a ton of success
[03:09:03.260 --> 03:09:05.380]   promoting interpretability in a way
[03:09:05.380 --> 03:09:09.140]   where like Chris Ola hasn't been as active recently
[03:09:09.140 --> 03:09:10.380]   in pushing things.
[03:09:10.380 --> 03:09:12.460]   Maybe because Neil's just doing quite a lot of the work,
[03:09:12.460 --> 03:09:14.860]   but like, I don't know, four or five years ago,
[03:09:14.860 --> 03:09:16.940]   he was like really pushing and like talking
[03:09:16.940 --> 03:09:19.220]   at all sorts of places and these sorts of things.
[03:09:19.220 --> 03:09:21.620]   And people weren't anywhere near as receptive.
[03:09:21.620 --> 03:09:25.300]   Maybe they've just woken up to like deep learning matters
[03:09:25.300 --> 03:09:27.100]   and it's clearly useful post-track GPT,
[03:09:27.100 --> 03:09:30.420]   but yeah, it is kind of striking.
[03:09:30.420 --> 03:09:31.260]   - All right, cool.
[03:09:31.260 --> 03:09:32.100]   And okay, I'm trying to think,
[03:09:32.100 --> 03:09:33.860]   what is a good last question?
[03:09:33.860 --> 03:09:37.220]   I mean, the one I'm going to those thinking of is like,
[03:09:37.220 --> 03:09:39.620]   do you think models enjoy next token prediction?
[03:09:39.620 --> 03:09:40.460]   - Yeah.
[03:09:40.460 --> 03:09:41.900]   - Do models believe in love?
[03:09:41.900 --> 03:09:44.500]   (all laughing)
[03:09:44.500 --> 03:09:48.740]   - We have this sense of things that are rewarded
[03:09:48.740 --> 03:09:51.380]   in our accessible environment.
[03:09:51.380 --> 03:09:53.660]   There's like this deep sense of fulfillment
[03:09:53.660 --> 03:09:56.700]   that we think we're supposed to get from them
[03:09:56.700 --> 03:09:58.020]   or often people do, right?
[03:09:58.020 --> 03:10:02.740]   Of like community or sugar or whatever we wanted
[03:10:02.740 --> 03:10:04.020]   on the African Savannah.
[03:10:04.020 --> 03:10:07.820]   Do you think like in the future models are trained
[03:10:07.820 --> 03:10:10.020]   with RL and everything, a lot of post-training
[03:10:10.020 --> 03:10:11.140]   on top of whatever,
[03:10:11.140 --> 03:10:13.860]   but they're like some in the way
[03:10:13.860 --> 03:10:15.220]   we were just a really like ice cream,
[03:10:15.220 --> 03:10:16.300]   they'll just be like, hi,
[03:10:16.300 --> 03:10:17.900]   just to predict the next token again.
[03:10:17.900 --> 03:10:19.660]   You know what I mean?
[03:10:19.660 --> 03:10:21.660]   Like in the good old days.
[03:10:21.660 --> 03:10:23.460]   - So there's this ongoing discussion of like,
[03:10:23.460 --> 03:10:24.900]   are models sentient or not?
[03:10:24.900 --> 03:10:27.100]   And like, do you thank the model when it helps you?
[03:10:27.100 --> 03:10:27.940]   - Yeah.
[03:10:27.940 --> 03:10:30.020]   - But I think if you want to thank it,
[03:10:30.020 --> 03:10:31.540]   you actually shouldn't say thank you.
[03:10:31.540 --> 03:10:32.780]   You should just give it a sequence
[03:10:32.780 --> 03:10:34.460]   that's very easy to predict.
[03:10:34.460 --> 03:10:35.300]   (all laughing)
[03:10:35.300 --> 03:10:37.620]   And the even funnier part of this
[03:10:37.620 --> 03:10:40.820]   is there's some work on if you just give it the sequence,
[03:10:40.820 --> 03:10:43.860]   A, like over and over again,
[03:10:43.860 --> 03:10:46.340]   then eventually the model will just start spewing out
[03:10:46.340 --> 03:10:51.220]   all sorts of things that otherwise wouldn't ever say.
[03:10:51.220 --> 03:10:54.780]   And so, yeah, I won't say anything more about that,
[03:10:54.780 --> 03:10:56.500]   but you can, yeah,
[03:10:56.500 --> 03:10:57.940]   you should just give your model something very easy
[03:10:57.940 --> 03:10:59.940]   to predict as a nice little treat.
[03:10:59.940 --> 03:11:01.620]   - This is what the OEM ends up being.
[03:11:01.620 --> 03:11:03.460]   We just found the universe and like.
[03:11:03.460 --> 03:11:05.020]   (all laughing)
[03:11:05.020 --> 03:11:08.140]   - But do we like things that are like easy to predict?
[03:11:08.140 --> 03:11:11.580]   Aren't we constantly in search of like the dose of-
[03:11:11.580 --> 03:11:12.420]   - The bits of entropy?
[03:11:12.420 --> 03:11:14.300]   - Yeah, the bits of entropy, exactly, right?
[03:11:14.300 --> 03:11:15.140]   Shouldn't you be giving it things
[03:11:15.140 --> 03:11:17.220]   that are just slightly too hard to predict?
[03:11:17.220 --> 03:11:18.620]   (all laughing)
[03:11:18.620 --> 03:11:20.180]   Just out of reach.
[03:11:20.180 --> 03:11:21.780]   - Yeah, but I wonder, like,
[03:11:21.780 --> 03:11:23.620]   at least from the free energy principle perspective, right?
[03:11:23.620 --> 03:11:26.340]   Like you don't like, you don't want to be surprised.
[03:11:26.340 --> 03:11:29.620]   And so maybe it's this, like, I don't feel surprised
[03:11:29.620 --> 03:11:30.860]   I feel in control of my environment.
[03:11:30.860 --> 03:11:32.540]   And so now I can go and seek things
[03:11:32.540 --> 03:11:34.420]   and I've been predisposed to like,
[03:11:34.420 --> 03:11:37.740]   in the long run it's better to explore new things right now.
[03:11:37.740 --> 03:11:40.540]   Like leave the rock that I've been sheltered under
[03:11:40.540 --> 03:11:42.700]   ultimately leading me to like build a house
[03:11:42.700 --> 03:11:43.940]   or like some better structure.
[03:11:43.940 --> 03:11:46.500]   But we don't like surprises.
[03:11:46.500 --> 03:11:48.580]   I think most people are very upset
[03:11:48.580 --> 03:11:51.220]   when like expectation does not meet reality.
[03:11:51.220 --> 03:11:54.140]   - That's why babies like love watching the same show
[03:11:54.140 --> 03:11:55.420]   over and over and over again, right?
[03:11:55.420 --> 03:11:56.380]   - Yeah, interesting.
[03:11:56.380 --> 03:11:57.540]   Yeah, I can see that.
[03:11:57.540 --> 03:12:00.980]   - Oh, I guess they're learning to model it and stuff too.
[03:12:00.980 --> 03:12:01.820]   - Yeah.
[03:12:01.820 --> 03:12:03.060]   - Yeah.
[03:12:03.060 --> 03:12:05.780]   - Okay, well hopefully this will be the repeat.
[03:12:05.780 --> 03:12:07.500]   (all laughing)
[03:12:07.500 --> 03:12:09.380]   That the AI has learned to love.
[03:12:09.380 --> 03:12:10.220]   Okay, cool.
[03:12:10.220 --> 03:12:11.780]   I think that's a great place to wrap.
[03:12:11.780 --> 03:12:14.660]   I should also mention that the better part
[03:12:14.660 --> 03:12:15.780]   of what I know about AI,
[03:12:15.780 --> 03:12:17.540]   I've learned from just talking with you guys.
[03:12:17.540 --> 03:12:20.420]   You know, we've been good friends for about a year now.
[03:12:20.420 --> 03:12:22.540]   So yeah, I mean, yeah, I appreciate you guys
[03:12:22.540 --> 03:12:24.700]   getting me up to speed here and yeah.
[03:12:24.700 --> 03:12:25.900]   - You guys have great questions.
[03:12:25.900 --> 03:12:27.860]   It's really fun to hang and chat.
[03:12:27.860 --> 03:12:28.700]   - Great, great.
[03:12:28.700 --> 03:12:29.900]   - I really treasure that time together.
[03:12:29.900 --> 03:12:31.820]   - Yeah, you're getting a lot better at pickleball.
[03:12:31.820 --> 03:12:34.420]   (all laughing)
[03:12:37.100 --> 03:12:39.060]   - Hey, we're trying to progress the tenders, come on.
[03:12:39.060 --> 03:12:41.660]   (all laughing)
[03:12:41.660 --> 03:12:43.940]   - Awesome, cool, cool.
[03:12:43.940 --> 03:12:45.060]   Awesome, thanks.
[03:12:45.060 --> 03:12:47.860]   - Hey everybody.
[03:12:47.860 --> 03:12:49.740]   I hope you enjoyed that episode.
[03:12:49.740 --> 03:12:51.940]   As always, the most helpful thing you can do
[03:12:51.940 --> 03:12:53.740]   is to share the podcast.
[03:12:53.740 --> 03:12:55.460]   Send it to people you think might enjoy it,
[03:12:55.460 --> 03:12:57.580]   put it in Twitter, your group chats, et cetera.
[03:12:57.580 --> 03:12:59.420]   Just splits the world.
[03:12:59.420 --> 03:13:00.660]   Appreciate you listening.
[03:13:00.660 --> 03:13:01.820]   I'll see you next time.
[03:13:01.820 --> 03:13:03.140]   Cheers.
[03:13:03.140 --> 03:13:05.720]   (upbeat music)
[03:13:05.720 --> 03:13:08.300]   (upbeat music)
[03:13:08.300 --> 03:13:12.300]   [music]

