
[00:00:00.000 --> 00:00:02.720]   I try to lay out the context of this.
[00:00:02.720 --> 00:00:06.160]   I mean, this was after the most destructive war that the world had ever known.
[00:00:06.160 --> 00:00:08.640]   Millions of people had died.
[00:00:08.640 --> 00:00:13.040]   And von Neumann had predicted this and the Holocaust
[00:00:13.040 --> 00:00:16.440]   very successfully years in advance.
[00:00:16.440 --> 00:00:19.880]   And he now was convinced that within a decade
[00:00:19.880 --> 00:00:22.160]   there would be a third world war with nuclear weapons.
[00:00:22.160 --> 00:00:34.080]   Okay, today I have the pleasure of speaking with Ananu Bhattacharya,
[00:00:34.080 --> 00:00:39.280]   who is a science writer who has worked at The Economist and at Nature.
[00:00:39.280 --> 00:00:43.760]   And most recently, he's the author of The Man from the Future,
[00:00:43.760 --> 00:00:46.160]   The Visionary Life of John von Neumann.
[00:00:46.160 --> 00:00:49.600]   And it was an extremely enjoyable read, super interesting.
[00:00:49.600 --> 00:00:52.000]   And so before we jump into the questions, Ananu,
[00:00:52.000 --> 00:00:55.040]   I'm wondering if you can kind of give context to my audience
[00:00:55.040 --> 00:00:57.280]   and summarize the life of this giant.
[00:00:57.280 --> 00:01:02.160]   Well, that's not an easy task, but I'll give it a go.
[00:01:02.160 --> 00:01:10.000]   So he was born in Budapest in around 1903 to this wealthy Jewish family.
[00:01:10.000 --> 00:01:16.320]   And pretty early on, they realized that there's something quite special about him.
[00:01:16.880 --> 00:01:22.000]   So he can do these long six-figure calculations in his head by six.
[00:01:22.000 --> 00:01:27.360]   And he's learned calculus by eight, right?
[00:01:27.360 --> 00:01:34.880]   And he's teaching himself the finer points of set theory by kind of 11, right?
[00:01:34.880 --> 00:01:39.200]   So he's going on long walks with Eugene Wigner,
[00:01:39.200 --> 00:01:42.880]   who was a childhood friend of his and a future Nobel Prize winner.
[00:01:42.880 --> 00:01:44.800]   And Wigner is a year older than him.
[00:01:44.800 --> 00:01:48.960]   And he's teaching Wigner set theory at that age.
[00:01:48.960 --> 00:01:54.240]   So it's kind of clear that even among geniuses,
[00:01:54.240 --> 00:01:59.600]   as he would be later on at Los Alamos, for example,
[00:01:59.600 --> 00:02:04.400]   or at Princeton and at the Institute for Advanced Study,
[00:02:04.400 --> 00:02:07.120]   where he'd be recruited along with Einstein,
[00:02:07.120 --> 00:02:13.760]   that he was kind of a cut above even all of these incredibly clever people.
[00:02:14.480 --> 00:02:22.320]   And so, yeah, so he grows up in this quite privileged Budapest surroundings.
[00:02:22.320 --> 00:02:33.520]   Their home was often visited by the greats of the time.
[00:02:33.520 --> 00:02:37.360]   It was an incredibly cultured city.
[00:02:37.920 --> 00:02:45.200]   And his father, Max, was a kind of successful banker.
[00:02:45.200 --> 00:02:46.320]   So they were quite wealthy.
[00:02:46.320 --> 00:02:47.760]   I mean, he was a self-made man.
[00:02:47.760 --> 00:02:54.560]   But he had, as a result, von Neumann, who is one of three brothers, actually,
[00:02:54.560 --> 00:02:55.680]   is the eldest.
[00:02:55.680 --> 00:03:00.400]   He had the benefits of kind of a top-flight education as well.
[00:03:00.400 --> 00:03:00.800]   Yeah.
[00:03:00.800 --> 00:03:03.520]   So, you know, right before we did the interview,
[00:03:03.520 --> 00:03:07.040]   I was thinking about what, you know, I have a computer science degree.
[00:03:07.040 --> 00:03:10.240]   And I was thinking about, okay, what portion of my computer science degree
[00:03:10.240 --> 00:03:11.920]   can be traced back directly to von Neumann?
[00:03:11.920 --> 00:03:15.760]   I was just going through just like an initial glance at a few of the classes
[00:03:15.760 --> 00:03:18.560]   that I took, where like a large part of the fraction of the content
[00:03:18.560 --> 00:03:20.160]   came from von Neumann, right?
[00:03:20.160 --> 00:03:23.600]   So you could like, okay, algorithms, linear programming, you know,
[00:03:23.600 --> 00:03:26.400]   merge sort, like probably like a quarter of my curriculum,
[00:03:26.400 --> 00:03:32.000]   quantum computing, you know, density, density matrix, von Neumann entropy,
[00:03:32.000 --> 00:03:36.240]   hardware, von Neumann, you know, the von Neumann architecture for the computers.
[00:03:37.040 --> 00:03:39.520]   You know, even like my organizational ethics class, you know,
[00:03:39.520 --> 00:03:43.040]   that game theory that comes up, you know,
[00:03:43.040 --> 00:03:47.040]   theory of computing, you know, finite state machines, cellular automata.
[00:03:47.040 --> 00:03:51.600]   So like, it's astounding to me that this person is responsible
[00:03:51.600 --> 00:03:54.800]   for probably like a third of everything I learned in college.
[00:03:54.800 --> 00:04:00.480]   And so it was very interesting to then get to read the history of this person
[00:04:00.480 --> 00:04:02.960]   and the ideas that he came up with and interacted with.
[00:04:04.240 --> 00:04:10.480]   And now one very interesting part about the context surrounding von Neumann's work is,
[00:04:10.480 --> 00:04:13.360]   you know, he was part of this group, as you talk about, called the Martians.
[00:04:13.360 --> 00:04:15.760]   They were Hungarian and Central European Jews
[00:04:15.760 --> 00:04:18.880]   who migrated to the United States in the early 20th century.
[00:04:18.880 --> 00:04:22.800]   And Scott Alexander has a fun blog post title about this.
[00:04:22.800 --> 00:04:29.120]   He says the nuclear bomb was a high school science project
[00:04:29.120 --> 00:04:33.360]   for a bunch of Hungarians, because a lot of the scientists
[00:04:33.360 --> 00:04:36.800]   worked on the nuclear bomb, like went to the same high school.
[00:04:36.800 --> 00:04:40.880]   So what was the cultural or other factors that made
[00:04:40.880 --> 00:04:44.400]   this group of people so, I mean, produced so many geniuses?
[00:04:44.400 --> 00:04:48.240]   Right. So they were all Jewish.
[00:04:48.240 --> 00:04:54.880]   And von Neumann attributed this kind of pressure to succeed
[00:04:54.880 --> 00:05:02.240]   to growing up in kind of Central Europe between the two world wars.
[00:05:02.800 --> 00:05:05.920]   He was being surrounded by sort of anti-Semitism.
[00:05:05.920 --> 00:05:11.200]   Now, Budapest was relatively tolerant, but it was in the air of Central Europe at the time.
[00:05:11.200 --> 00:05:20.800]   And he said that he felt a pressure to succeed or face extinction.
[00:05:20.800 --> 00:05:24.640]   I mean, they were constantly under this huge, relentless
[00:05:24.640 --> 00:05:30.320]   kind of psychological pressure to kind of do the impossible.
[00:05:31.120 --> 00:05:34.800]   And, you know, von Neumann in his letters from 1930,
[00:05:34.800 --> 00:05:39.440]   by which time he's safely in the US, he's predicting disaster.
[00:05:39.440 --> 00:05:44.080]   He predicts pretty accurately that there will be a second world war.
[00:05:44.080 --> 00:05:50.240]   And he predicts that European Jews will face extinction.
[00:05:50.240 --> 00:05:55.440]   So he is very acutely aware of this.
[00:05:55.440 --> 00:06:01.920]   Of course, there are circumstances around Budapest at the time, which
[00:06:01.920 --> 00:06:09.840]   was able, which meant that geniuses of this sort were nurtured.
[00:06:09.840 --> 00:06:13.760]   So there were private schools and they were all inevitably private schools,
[00:06:13.760 --> 00:06:16.080]   and they were almost all boys schools as well.
[00:06:16.080 --> 00:06:23.120]   And von Neumann went to one of three, I think, elite schools in Budapest at the time.
[00:06:23.680 --> 00:06:35.520]   Teller, for example, and Wigner and Szilard are all part of this group called the Martians later.
[00:06:35.520 --> 00:06:38.720]   They all went to these kind of elite schools.
[00:06:38.720 --> 00:06:45.760]   And von Neumann was spotted quite early on by his maths teacher who told his father,
[00:06:45.760 --> 00:06:50.320]   you know, your boy's exceptional, let me arrange special tutoring for him.
[00:06:50.320 --> 00:06:56.480]   So von Neumann gets picked out even from this group of exceptional people.
[00:06:56.480 --> 00:07:01.280]   And he's given a special course at the University of Budapest.
[00:07:01.280 --> 00:07:08.000]   And it's his teachers are all just amazed at his abilities.
[00:07:08.000 --> 00:07:15.440]   So the joke was later on when all these guys met again at Los Alamos to work on
[00:07:15.440 --> 00:07:19.840]   the American bomb project that they had these funny Hungarian accents
[00:07:19.840 --> 00:07:24.880]   and they had these almost supernatural intellectual abilities.
[00:07:24.880 --> 00:07:27.920]   So the joke was they must be from another planet.
[00:07:27.920 --> 00:07:33.200]   Now, when Wigner was asked about this, he said there is no Hungarian phenomenon.
[00:07:33.200 --> 00:07:37.120]   The only phenomenon that needs explaining is Johnny von Neumann.
[00:07:37.120 --> 00:07:42.320]   So you can tell from those sorts of comments what kind of person he was.
[00:07:42.320 --> 00:07:48.720]   I'm actually curious to boil down what exactly was going on that produced so many geniuses.
[00:07:48.720 --> 00:07:52.320]   I mean, one thing you proposed was maybe it was the private schools.
[00:07:52.320 --> 00:07:57.360]   But I mean, as you just said, you know, he had like he was he had taught himself integral
[00:07:57.360 --> 00:08:01.040]   and differential calculus by the time he was 10 and knew like four languages.
[00:08:01.040 --> 00:08:05.280]   So maybe that aided his growth.
[00:08:05.280 --> 00:08:11.600]   But I'm curious, it seems like he was already on the path to becoming like a world star scientist.
[00:08:11.600 --> 00:08:15.760]   Yeah, I mean, he was renowned as a mathematician really early on.
[00:08:15.760 --> 00:08:23.280]   I mean, he as soon as he finished his PhD, where he resolves this incredibly difficult
[00:08:23.280 --> 00:08:29.760]   paradox in set theory, he helps to resolve it by 22.
[00:08:29.760 --> 00:08:35.840]   And then he goes to Gershgin, where quantum theory is being invented by another whiz kid,
[00:08:35.840 --> 00:08:37.680]   actually, Werner Heisenberg.
[00:08:37.680 --> 00:08:40.320]   Of course, he's just a year older than von Neumann.
[00:08:40.320 --> 00:08:44.080]   And von Neumann gets really interested in quantum mechanics.
[00:08:44.080 --> 00:08:52.000]   And he produces this first mathematically rigorous version of it in a few years later.
[00:08:52.000 --> 00:08:59.920]   So, I mean, von Neumann, clearly, I mean, he was just an exceptional, he had an exceptional brain.
[00:08:59.920 --> 00:09:06.960]   Now, his grandfather was apparently, although he wasn't academically
[00:09:09.120 --> 00:09:13.440]   particularly successful, he had started his own very successful business.
[00:09:13.440 --> 00:09:18.240]   But what was interesting was that he had these calculational
[00:09:18.240 --> 00:09:21.680]   abilities that were actually better than von Neumann.
[00:09:21.680 --> 00:09:28.240]   So von Neumann remembers asking his granddad these incredibly long sums,
[00:09:28.240 --> 00:09:33.760]   and his granddad would come back with answers pretty quickly.
[00:09:33.760 --> 00:09:36.960]   And von Neumann, despite all of his genius, he was never able to
[00:09:38.000 --> 00:09:42.160]   kind of match these abilities himself.
[00:09:42.160 --> 00:09:46.000]   And of course, there's a lot more to higher mathematics, as we know,
[00:09:46.000 --> 00:09:48.560]   than being able to do really long sums.
[00:09:48.560 --> 00:09:56.880]   But it's kind of interesting that there's some genetic predisposition there that we can see.
[00:09:56.880 --> 00:10:02.720]   One interesting possibility that I've heard is you have Jewish emancipation in Europe in like,
[00:10:02.720 --> 00:10:07.040]   was it the 18th or 19th century?
[00:10:07.040 --> 00:10:10.240]   And then afterwards, you have this tremendous streak of Jewish achievement
[00:10:10.240 --> 00:10:12.800]   that's halted by the Holocaust.
[00:10:12.800 --> 00:10:16.480]   So, you know, you have this brief window where this group of extraordinary people
[00:10:16.480 --> 00:10:20.400]   are able to achieve great things before, you know, before they're forced to emigrate,
[00:10:20.400 --> 00:10:21.840]   or, you know, other things happen.
[00:10:21.840 --> 00:10:25.600]   And I mean, it makes what happened in Europe during that time even more tragic
[00:10:25.600 --> 00:10:28.320]   when you consider what was stopped.
[00:10:28.320 --> 00:10:34.400]   So, you know, one question I have is, you have this person who is incredibly prolific.
[00:10:35.120 --> 00:10:40.880]   Would he have been able to achieve as much as he did if he were born, say, today,
[00:10:40.880 --> 00:10:44.480]   given that a lot of the low-hanging fruit has been picked?
[00:10:44.480 --> 00:10:49.360]   Is it just that he got into science and mathematics at a time
[00:10:49.360 --> 00:10:53.680]   that there was just so many different ideas combining and left to explore?
[00:10:53.680 --> 00:10:56.240]   Or, I mean, do you think that any at any other time,
[00:10:56.240 --> 00:11:00.240]   a person like von Neumann would have been able to be as prolific?
[00:11:00.240 --> 00:11:04.480]   No, I think you've really hit the nail on the head there.
[00:11:04.480 --> 00:11:07.120]   I think there was definitely a historical moment.
[00:11:07.120 --> 00:11:11.600]   I mean, in terms of people with brains like von Neumann, they're pretty hard to think of.
[00:11:11.600 --> 00:11:14.640]   But, you know, in terms of raw mathematical ability,
[00:11:14.640 --> 00:11:20.800]   you look at somebody like Terence Tao today, or, you know, you consider...
[00:11:20.800 --> 00:11:32.000]   There's a few other pure mathematicians who could maybe match von Neumann's sort of brain.
[00:11:32.000 --> 00:11:39.920]   I mean, it was extraordinarily unusual, but maybe not, you know, once in a century unusual,
[00:11:39.920 --> 00:11:41.840]   but extremely unusual.
[00:11:41.840 --> 00:11:47.600]   But I think there are a few things that kind of mark him out.
[00:11:47.600 --> 00:11:49.840]   One is, yeah, the historical moment.
[00:11:49.840 --> 00:11:56.880]   So he arrives on the scene in kind of, you know, 1910, 1920s,
[00:11:56.880 --> 00:12:04.320]   and he's immersed in kind of a maths that's going through this logical crisis.
[00:12:04.320 --> 00:12:08.640]   And it's going to spur people like Alan Turing and Kurt Gödel
[00:12:08.640 --> 00:12:13.120]   to think really hard about the step-by-step proofs.
[00:12:13.120 --> 00:12:19.200]   How do we prove stuff properly without getting into these awful paradoxes?
[00:12:19.200 --> 00:12:22.080]   And that would lead later on, that step-by-step thinking
[00:12:22.080 --> 00:12:26.880]   would be extremely influential when people came to think about programming,
[00:12:26.880 --> 00:12:29.120]   you know, and algorithms and things like that.
[00:12:29.120 --> 00:12:31.680]   So there's that side of things.
[00:12:31.680 --> 00:12:35.120]   And then, of course, science just explodes.
[00:12:35.120 --> 00:12:39.680]   You know, you've got masses of funding, of course.
[00:12:39.680 --> 00:12:45.760]   Quantum mechanics becomes the atom bomb, basically, within a space of 25 years.
[00:12:45.760 --> 00:12:51.200]   You have huge amounts of money suddenly being thrown at science,
[00:12:51.200 --> 00:12:52.960]   and then you get big science.
[00:12:52.960 --> 00:12:58.800]   And, you know, economics, you know, thanks to von Neumann in large part,
[00:12:58.800 --> 00:13:01.200]   becomes suddenly more mathematical.
[00:13:01.200 --> 00:13:07.280]   But now, with that massive funding and the continued funding of science,
[00:13:07.280 --> 00:13:12.800]   I think there's been a great degree of specialism.
[00:13:12.800 --> 00:13:16.240]   I think the time when one genius of von Neumann's stature
[00:13:16.240 --> 00:13:21.120]   could contribute so productively to kind of, you know,
[00:13:21.120 --> 00:13:24.880]   everything from pure mathematics, right the way through quantum mechanics,
[00:13:24.880 --> 00:13:29.840]   to various fields of physics, to, you know, nonlinear equations,
[00:13:29.840 --> 00:13:35.920]   and to distill out the modern form of the computer,
[00:13:35.920 --> 00:13:39.360]   the programmable computer, to automata theory,
[00:13:39.360 --> 00:13:46.000]   you know, come up with a proof that machines could reproduce themselves.
[00:13:46.000 --> 00:13:52.240]   I think, sadly, that that was really a brief moment of the 20th century
[00:13:52.240 --> 00:13:53.440]   that made it possible.
[00:13:53.440 --> 00:13:58.240]   But the second thing that's incredibly rare about von Neumann
[00:13:58.240 --> 00:14:02.400]   that I noticed, he actually embraced this idea
[00:14:02.400 --> 00:14:07.760]   of applying maths to real world problems.
[00:14:07.760 --> 00:14:11.920]   Whereas many mathematicians, many academics of all sorts, actually,
[00:14:12.480 --> 00:14:15.840]   would rather eschew, you know, the real world.
[00:14:15.840 --> 00:14:17.520]   They don't want very much to do with it.
[00:14:17.520 --> 00:14:21.360]   When it comes to mathematicians,
[00:14:21.360 --> 00:14:24.560]   they'd rather be left alone in their ivory tower to prove theorems.
[00:14:24.560 --> 00:14:26.320]   And von Neumann did a lot of that.
[00:14:26.320 --> 00:14:32.480]   He left behind, you know, a massive amount of pure mathematics.
[00:14:32.480 --> 00:14:38.720]   But really, my book focuses on the stuff that he left behind
[00:14:38.720 --> 00:14:41.760]   that came about from engaging with the real world.
[00:14:41.760 --> 00:14:43.360]   And there's a huge amount of that.
[00:14:43.360 --> 00:14:50.160]   And I think that's also what made him really quite exceptional.
[00:14:50.160 --> 00:14:54.880]   The only other person that I can think of that is now as gifted
[00:14:54.880 --> 00:14:59.680]   mathematically as he was and has shown some interest
[00:14:59.680 --> 00:15:02.560]   in these sort of practical affairs is Stephen Wolfram.
[00:15:02.560 --> 00:15:10.320]   But, you know, Wolfram was born in the wrong time, I think.
[00:15:10.800 --> 00:15:14.400]   And perhaps if he'd have been born in 1903,
[00:15:14.400 --> 00:15:17.040]   you know, he might have been a von Neumann-esque figure.
[00:15:17.040 --> 00:15:21.600]   But so there's definitely a combination there of good luck,
[00:15:21.600 --> 00:15:27.040]   a historical moment, and just, you know, a particular attitude,
[00:15:27.040 --> 00:15:33.040]   maybe because he was brought up in a, you know, by a banker father
[00:15:33.040 --> 00:15:37.200]   who was not afraid to get his hands dirty.
[00:15:37.200 --> 00:15:39.760]   I mean, this was an he was an investment banker.
[00:15:39.760 --> 00:15:44.160]   He was happily investing in firms, in technology,
[00:15:44.160 --> 00:15:46.160]   in the technology firms of the time.
[00:15:46.160 --> 00:15:50.560]   People, you know, he invested in a Jacquard loom company,
[00:15:50.560 --> 00:15:54.560]   which used punch cards to program looms.
[00:15:54.560 --> 00:15:58.800]   You know, that made an impact on von Neumann, obviously, at the time.
[00:15:58.800 --> 00:16:02.560]   So I think, yeah, there's a combination of reasons
[00:16:02.560 --> 00:16:05.200]   that von Neumann was so influential.
[00:16:05.200 --> 00:16:07.920]   Wolfram could have been a great scientist at another time.
[00:16:07.920 --> 00:16:11.920]   I guess he just ended up writing some mathematical software in our time.
[00:16:11.920 --> 00:16:15.120]   Not to say he hasn't tried other things.
[00:16:15.120 --> 00:16:19.600]   So you suggest that maybe it was his,
[00:16:19.600 --> 00:16:23.280]   it was the time he spent working on practical problems
[00:16:23.280 --> 00:16:24.800]   that helped him achieve so much.
[00:16:24.800 --> 00:16:27.520]   And I wonder if the opposite may not be true,
[00:16:27.520 --> 00:16:31.760]   that is it possible that because he got recruited
[00:16:31.760 --> 00:16:34.720]   into all these different projects that the government had going on at the time,
[00:16:34.720 --> 00:16:36.640]   especially because of World War II, you know,
[00:16:36.640 --> 00:16:39.040]   ballistics research, nuclear implosion devices,
[00:16:39.040 --> 00:16:41.520]   and then advising with like Cold War strategy.
[00:16:41.520 --> 00:16:46.160]   Was this in some sense a distraction from the, you know,
[00:16:46.160 --> 00:16:48.880]   the basic research that he might have otherwise done
[00:16:48.880 --> 00:16:50.400]   and been more productive at?
[00:16:50.400 --> 00:16:56.400]   Well, I mean, you know, Bronowski thought, you know,
[00:16:56.400 --> 00:17:00.960]   that von Neumann had kind of wasted his incredible talent.
[00:17:00.960 --> 00:17:03.360]   But to me, the more I looked at his work,
[00:17:03.360 --> 00:17:05.040]   the more I realised that for him,
[00:17:05.040 --> 00:17:10.080]   this engagement with the real world was actually vitally important.
[00:17:10.080 --> 00:17:15.040]   And, you know, it need not have been the work for the military.
[00:17:15.040 --> 00:17:18.720]   But that is where at the time in the,
[00:17:18.720 --> 00:17:21.600]   unfortunately, in the early to mid 20th century,
[00:17:21.600 --> 00:17:23.920]   a lot of the challenging problems were,
[00:17:23.920 --> 00:17:27.120]   I mean, designing the atom bomb,
[00:17:27.120 --> 00:17:30.240]   which is where he made some key contributions.
[00:17:30.240 --> 00:17:32.160]   And then later on, of course,
[00:17:32.160 --> 00:17:35.280]   the emergence of the computer is deeply,
[00:17:35.280 --> 00:17:39.520]   deeply linked to the mathematics of the atom bomb.
[00:17:39.520 --> 00:17:45.760]   And arguably, you know, it was his engagement
[00:17:45.760 --> 00:17:50.000]   with these areas that led him to think
[00:17:50.000 --> 00:17:54.400]   and be in a position to kind of spur computing.
[00:17:54.400 --> 00:17:57.120]   And as I argue, he was kind of a godfather
[00:17:57.120 --> 00:17:58.560]   of the open source movement.
[00:18:00.480 --> 00:18:07.360]   You know, his proof of that automata
[00:18:07.360 --> 00:18:09.920]   could reproduce themselves and evolve.
[00:18:09.920 --> 00:18:13.600]   All of this thinking came about because he was,
[00:18:13.600 --> 00:18:15.680]   I think, deeply engaged with the real world.
[00:18:15.680 --> 00:18:17.200]   And that makes him unusual.
[00:18:17.200 --> 00:18:21.360]   And he argues as much quite openly in an essay
[00:18:21.360 --> 00:18:22.960]   that he did called The Mathematician.
[00:18:22.960 --> 00:18:28.160]   And where he says that if mathematicians retreat too far,
[00:18:28.880 --> 00:18:30.960]   kind of into their ivory towers,
[00:18:30.960 --> 00:18:34.800]   if the maths becomes just maths for the sake of maths,
[00:18:34.800 --> 00:18:41.440]   with no input from kind of the real world,
[00:18:41.440 --> 00:18:50.640]   then he said it became baroque and not interesting.
[00:18:50.640 --> 00:18:55.280]   So I find it really difficult to believe
[00:18:55.280 --> 00:19:00.400]   that if von Neumann had sheltered himself away
[00:19:00.400 --> 00:19:01.680]   and somehow been left alone
[00:19:01.680 --> 00:19:04.640]   or didn't engage with the sorts of problems that he did,
[00:19:04.640 --> 00:19:06.960]   whether it was the computer or to his military work,
[00:19:06.960 --> 00:19:09.840]   that he would have left behind
[00:19:09.840 --> 00:19:14.160]   the kind of interesting oeuvre that he did.
[00:19:14.160 --> 00:19:16.880]   He wouldn't have been von Neumann, right?
[00:19:16.880 --> 00:19:19.920]   I mean, you can see it's so deeply ingrained
[00:19:19.920 --> 00:19:24.560]   in his personality to be out there thinking all the time
[00:19:24.560 --> 00:19:28.560]   and to be thinking about key problems
[00:19:28.560 --> 00:19:33.200]   that it's difficult to imagine a von Neumann
[00:19:33.200 --> 00:19:35.280]   that wasn't like that, that was tucked away.
[00:19:35.280 --> 00:19:39.840]   And I think that as a kind of intellectual biographer,
[00:19:39.840 --> 00:19:42.880]   that makes him kind of incredibly interesting,
[00:19:42.880 --> 00:19:46.000]   but also incredibly challenging to tackle.
[00:19:46.000 --> 00:19:48.160]   Yeah, that's what makes your book so interesting,
[00:19:48.160 --> 00:19:50.640]   is that you are a biographer of ideas.
[00:19:50.640 --> 00:19:53.520]   So, you know, a lot of other biographies
[00:19:53.520 --> 00:19:54.800]   about scientists really frustrate me
[00:19:54.800 --> 00:19:57.840]   because you get to hear all these details about their life,
[00:19:57.840 --> 00:20:00.800]   which is also interesting,
[00:20:00.800 --> 00:20:03.040]   but you never get to engage with their ideas,
[00:20:03.040 --> 00:20:04.240]   which is probably a big part
[00:20:04.240 --> 00:20:06.400]   of what reading about a scientist should be about.
[00:20:06.400 --> 00:20:07.600]   And you do that really well.
[00:20:07.600 --> 00:20:10.160]   So, you know, that was super fun.
[00:20:10.160 --> 00:20:12.960]   Did John von Neumann have a miracle year?
[00:20:12.960 --> 00:20:14.340]   You know what?
[00:20:14.340 --> 00:20:18.560]   I don't know.
[00:20:18.560 --> 00:20:21.200]   And maybe you've looked at his publication record
[00:20:21.200 --> 00:20:24.240]   more closely than I have and counted up the papers.
[00:20:24.240 --> 00:20:28.800]   But, you know, whilst Einstein, for example,
[00:20:28.800 --> 00:20:30.880]   and Kurt Gödel, when they were placed
[00:20:30.880 --> 00:20:32.640]   into this perfect environment,
[00:20:32.640 --> 00:20:34.640]   that was the Institute for Advanced Study, right?
[00:20:34.640 --> 00:20:38.080]   They didn't have, you know, to teach anybody anything.
[00:20:38.080 --> 00:20:39.520]   They had massive holidays.
[00:20:39.520 --> 00:20:41.280]   They could do what they wanted.
[00:20:41.280 --> 00:20:44.080]   Well, Einstein's time there
[00:20:44.080 --> 00:20:46.640]   was really not very productive.
[00:20:46.640 --> 00:20:49.520]   He had, you know, his miracle year, right?
[00:20:49.520 --> 00:20:52.480]   In the kind of early 20th century,
[00:20:52.480 --> 00:20:53.520]   which was incredible.
[00:20:53.520 --> 00:20:57.360]   But then his time at the IAS
[00:20:57.360 --> 00:20:58.880]   was not particularly productive.
[00:20:58.880 --> 00:21:02.880]   He was trying to find his theory of everything.
[00:21:02.880 --> 00:21:06.640]   And Gödel, after this incredible work in Europe
[00:21:06.640 --> 00:21:09.360]   on, you know, his incompleteness theorems.
[00:21:09.360 --> 00:21:12.800]   Again, he spent a lot of time at the IAS
[00:21:12.800 --> 00:21:14.720]   going for nice walks with Einstein
[00:21:14.720 --> 00:21:18.000]   and, you know, and talking, chatting to von Neumann.
[00:21:18.800 --> 00:21:20.400]   But of course, you know,
[00:21:20.400 --> 00:21:24.000]   there wasn't much coming out there in comparison.
[00:21:24.000 --> 00:21:28.320]   Now, when you look at von Neumann's productivity
[00:21:28.320 --> 00:21:31.360]   at the IAS, I mean, he was inventing
[00:21:31.360 --> 00:21:33.760]   whole new fields of mathematics.
[00:21:33.760 --> 00:21:38.640]   He was bringing about the birth of the modern computer.
[00:21:38.640 --> 00:21:41.280]   You know, he had this project at the IAS
[00:21:41.280 --> 00:21:47.920]   to bring a computer to them against,
[00:21:47.920 --> 00:21:50.880]   you know, it has to be said against the wishes
[00:21:50.880 --> 00:21:52.880]   of many of the IAS staff.
[00:21:52.880 --> 00:21:57.440]   But, you know, he was, he was,
[00:21:57.440 --> 00:22:01.360]   he'd written three volumes worth of operator theory.
[00:22:01.360 --> 00:22:03.280]   And he always joked, right?
[00:22:03.280 --> 00:22:09.360]   That, you know, a mathematician's productive years
[00:22:09.360 --> 00:22:16.240]   are over, you know, at 30 or at 28.
[00:22:16.240 --> 00:22:18.320]   And it was always 10 years away
[00:22:18.320 --> 00:22:21.120]   from however old he was at the time.
[00:22:21.120 --> 00:22:25.760]   So, you know, he clearly felt that he had a lot more to do.
[00:22:25.760 --> 00:22:28.560]   And I think that's what made his kind of untimely death
[00:22:28.560 --> 00:22:31.440]   all the more tragic for everybody.
[00:22:31.440 --> 00:22:37.520]   But, you know, it was incredibly painful for him.
[00:22:37.520 --> 00:22:40.880]   You know, nobody enjoys staring death in the face.
[00:22:40.880 --> 00:22:47.920]   But for Neumann, it was extraordinarily painful, yeah.
[00:22:47.920 --> 00:22:49.360]   And I think you mentioned the theory
[00:22:49.360 --> 00:22:52.560]   that it might've had to do with his spending time
[00:22:52.560 --> 00:22:54.880]   around nuclear tests, the bone cancer he got,
[00:22:54.880 --> 00:22:57.440]   which is, you know, ironic, but still tragic.
[00:22:57.440 --> 00:23:02.160]   So we know him very well for his work on computers.
[00:23:02.160 --> 00:23:05.920]   I'm curious why his research on cellular automata
[00:23:05.920 --> 00:23:08.640]   and the constructors hasn't taken off
[00:23:08.640 --> 00:23:09.760]   and why that isn't considered...
[00:23:09.760 --> 00:23:12.480]   Why that hasn't been researched, I guess,
[00:23:12.480 --> 00:23:13.840]   as fundamental as computers are.
[00:23:13.840 --> 00:23:15.520]   You know, like David Deutsch has recently published
[00:23:15.520 --> 00:23:17.040]   about constructor theory.
[00:23:17.040 --> 00:23:19.360]   His claim is that a universal constructor
[00:23:19.360 --> 00:23:23.120]   is like as fundamental a tool as a universal computer
[00:23:23.120 --> 00:23:24.480]   is, something that can construct anything else.
[00:23:24.480 --> 00:23:28.160]   Why did this train of thought kind of languish?
[00:23:28.160 --> 00:23:31.600]   Well, I mean, that's fascinating, isn't it?
[00:23:31.600 --> 00:23:33.360]   Because, I mean, the book's called
[00:23:33.360 --> 00:23:34.640]   "The Man from the Future," right?
[00:23:34.640 --> 00:23:43.040]   And I loved von Neumann's proof of his automata theory,
[00:23:43.040 --> 00:23:48.480]   you know, his proof that automata could reproduce.
[00:23:48.480 --> 00:23:53.840]   And, you know, he combines Turing's universal computer
[00:23:53.840 --> 00:24:02.000]   with this idea of a construction unit.
[00:24:02.000 --> 00:24:04.880]   And so he produces the universal constructor, right?
[00:24:04.880 --> 00:24:10.240]   And I think, in a sense, this is an idea
[00:24:10.240 --> 00:24:13.360]   that's still kind of ahead of its time.
[00:24:13.360 --> 00:24:17.360]   And just after I published "The Man from the Future"
[00:24:17.360 --> 00:24:19.920]   in the UK, this group in the States,
[00:24:19.920 --> 00:24:24.000]   published their paper on xenobots.
[00:24:24.000 --> 00:24:26.960]   And these are kind of stem cells,
[00:24:26.960 --> 00:24:30.640]   and they sort of whirl around
[00:24:30.640 --> 00:24:33.600]   and collect other stem cells together in little groups.
[00:24:33.600 --> 00:24:35.440]   And then these stem cells themselves
[00:24:35.440 --> 00:24:38.800]   start to whirl around and collect more together.
[00:24:38.800 --> 00:24:41.360]   So I suddenly realized, wow, you know,
[00:24:41.360 --> 00:24:45.200]   this is the embodiment of von Neumann's
[00:24:45.200 --> 00:24:47.040]   self-reproducing automata.
[00:24:47.040 --> 00:24:48.880]   And it's only taken, what, you know,
[00:24:48.880 --> 00:24:52.400]   70 years for them to make an appearance.
[00:24:52.400 --> 00:24:54.400]   And these stem cells were designed
[00:24:54.400 --> 00:25:00.400]   by kind of a neural net, so artificial intelligence.
[00:25:00.880 --> 00:25:03.280]   And here we are, you know, all of von Neumann's
[00:25:03.280 --> 00:25:04.960]   little influencers coming together
[00:25:04.960 --> 00:25:06.960]   in this neat, neat package.
[00:25:06.960 --> 00:25:08.800]   I think maybe in another 10 years' time,
[00:25:08.800 --> 00:25:10.800]   we'll be asking the same question again.
[00:25:10.800 --> 00:25:13.600]   Why didn't anybody realize this stuff was important?
[00:25:13.600 --> 00:25:17.920]   I mean, when von Neumann's first biographer,
[00:25:17.920 --> 00:25:20.880]   Norman Macrae, wrote about automata theory,
[00:25:20.880 --> 00:25:22.800]   he was extremely dismissive.
[00:25:22.800 --> 00:25:24.560]   Barely, you know, gave it a few pages
[00:25:24.560 --> 00:25:26.320]   as if it's, like, something quirky.
[00:25:28.560 --> 00:25:31.520]   And now we're beginning to see, kind of,
[00:25:31.520 --> 00:25:34.880]   it's the influence of this extraordinarily powerful idea,
[00:25:34.880 --> 00:25:36.000]   if nothing else.
[00:25:36.000 --> 00:25:40.720]   We know that it inspired those early pioneers
[00:25:40.720 --> 00:25:43.840]   in nanotechnology to think about universal constructors
[00:25:43.840 --> 00:25:44.800]   at the molecular level.
[00:25:44.800 --> 00:25:51.040]   We know that RepRap, this idea of a 3D printer
[00:25:51.040 --> 00:25:53.200]   where you could print most of its parts,
[00:25:53.200 --> 00:25:56.400]   you know, I talked to the inventor of that,
[00:25:56.400 --> 00:26:00.640]   and he said he was inspired by this idea,
[00:26:00.640 --> 00:26:01.840]   von Neumann's idea.
[00:26:01.840 --> 00:26:06.080]   And, you know, in the '50s and '60s and '70s,
[00:26:06.080 --> 00:26:07.600]   you had people thinking about,
[00:26:07.600 --> 00:26:10.480]   well, how do we explore the universe?
[00:26:10.480 --> 00:26:12.000]   Well, why don't we make a probe
[00:26:12.000 --> 00:26:14.240]   that can make more copies of itself,
[00:26:14.240 --> 00:26:16.560]   you know, out in space by foraging
[00:26:16.560 --> 00:26:18.560]   on the planets it finds?
[00:26:18.560 --> 00:26:20.800]   It's this incredibly fertile idea.
[00:26:20.800 --> 00:26:23.920]   And I think we're still just at the beginning
[00:26:23.920 --> 00:26:26.000]   of really working out where this goes.
[00:26:26.000 --> 00:26:28.160]   And it's kind of dangerous,
[00:26:28.160 --> 00:26:30.480]   and it's kind of exciting,
[00:26:30.480 --> 00:26:33.040]   and who knows where it's going to end.
[00:26:33.040 --> 00:26:35.120]   I think, for me at least,
[00:26:35.120 --> 00:26:39.040]   his work here and the implications of it
[00:26:39.040 --> 00:26:40.640]   are even more scary than, like,
[00:26:40.640 --> 00:26:41.920]   the counterintuitive implications
[00:26:41.920 --> 00:26:43.040]   from his game theory work.
[00:26:43.040 --> 00:26:45.760]   Because, like, Robin Hanson has this paper,
[00:26:45.760 --> 00:26:46.480]   I forget the title,
[00:26:46.480 --> 00:26:50.400]   but the idea is whatever force,
[00:26:50.400 --> 00:26:52.240]   or, like, civilization or whatever
[00:26:52.240 --> 00:26:54.640]   is expanding fastest will be the one
[00:26:54.640 --> 00:26:57.360]   that controls most of the universe,
[00:26:57.360 --> 00:26:59.520]   at least unless impeded by another one.
[00:26:59.520 --> 00:27:01.360]   And so if it's the case that
[00:27:01.360 --> 00:27:03.600]   this sort of von Neumann probes
[00:27:03.600 --> 00:27:06.480]   almost spread like a virus around the universe
[00:27:06.480 --> 00:27:07.680]   and turning everything into goop,
[00:27:07.680 --> 00:27:10.720]   maybe, like, the expected outcome
[00:27:10.720 --> 00:27:12.720]   of colonization is just that
[00:27:12.720 --> 00:27:14.480]   that's what the universe ends up looking like,
[00:27:14.480 --> 00:27:16.320]   where the low-hanging fruit, so to speak,
[00:27:16.320 --> 00:27:20.960]   has been burned away by such probes.
[00:27:20.960 --> 00:27:25.120]   And it's an interesting, like, futuristic hypothesis,
[00:27:25.120 --> 00:27:27.200]   and one I don't really hear much talked about,
[00:27:27.200 --> 00:27:28.320]   which I think is interesting.
[00:27:28.320 --> 00:27:32.320]   - Well, you know, that's one way it could go.
[00:27:32.320 --> 00:27:33.920]   Let's hope it doesn't go that way.
[00:27:33.920 --> 00:27:39.280]   You know, maybe they'll, you know,
[00:27:39.280 --> 00:27:42.560]   build us a new home after we've trashed this one.
[00:27:42.560 --> 00:27:43.060]   Who knows?
[00:27:43.060 --> 00:27:48.160]   Yeah, I think, of course, you know,
[00:27:48.160 --> 00:27:50.400]   these sorts of science fiction-y elements
[00:27:50.400 --> 00:27:55.440]   maybe part of it is that, you know,
[00:27:55.440 --> 00:27:57.360]   nobody wants to talk about automata theory
[00:27:57.360 --> 00:27:58.800]   because it's got these unsavory
[00:27:58.800 --> 00:28:00.720]   science fiction elements attached to it.
[00:28:00.720 --> 00:28:04.320]   You know, people would rather stick to
[00:28:04.320 --> 00:28:06.560]   the von Neumann architecture
[00:28:06.560 --> 00:28:08.240]   and all that sort of stuff.
[00:28:08.240 --> 00:28:11.600]   But yeah, I mean,
[00:28:11.600 --> 00:28:16.240]   it's the fecundity, really, of the idea
[00:28:16.240 --> 00:28:18.240]   more than the mathematics, isn't it?
[00:28:18.240 --> 00:28:23.680]   It just, you know, that somebody can take this question
[00:28:23.680 --> 00:28:26.400]   that philosophers have been kicking around
[00:28:26.400 --> 00:28:27.600]   for sort of centuries, you know,
[00:28:27.600 --> 00:28:29.280]   can machines make more machines?
[00:28:29.280 --> 00:28:30.480]   Can machines have babies?
[00:28:30.480 --> 00:28:31.680]   Can machines reproduce?
[00:28:31.680 --> 00:28:34.880]   And he just says, yeah, well, let's look at this
[00:28:34.880 --> 00:28:36.400]   mathematically, shall we?
[00:28:36.400 --> 00:28:37.680]   And then he solves it.
[00:28:37.680 --> 00:28:39.200]   And, you know, we have the answer.
[00:28:39.200 --> 00:28:42.560]   And that's what I find gripping
[00:28:42.560 --> 00:28:43.840]   about von Neumann's work.
[00:28:44.480 --> 00:28:48.560]   It's kind of what I found overall
[00:28:48.560 --> 00:28:50.000]   as I was approaching this book
[00:28:50.000 --> 00:28:51.920]   that I wanted to show that people,
[00:28:51.920 --> 00:28:54.240]   when you look at kind of popular science books
[00:28:54.240 --> 00:28:55.840]   or popular mathematics books,
[00:28:55.840 --> 00:28:58.240]   the majority of them are really about
[00:28:58.240 --> 00:29:00.240]   kind of celebrating the maths
[00:29:00.240 --> 00:29:02.720]   or the science in and of itself, right?
[00:29:02.720 --> 00:29:05.440]   They rarely actually talk about maths
[00:29:05.440 --> 00:29:08.240]   as this kind of existential thing
[00:29:08.240 --> 00:29:10.320]   that humans have invented
[00:29:10.320 --> 00:29:13.520]   that underpins our technological world.
[00:29:13.520 --> 00:29:14.800]   We don't really think of it
[00:29:14.800 --> 00:29:15.760]   like that often.
[00:29:15.760 --> 00:29:18.400]   And with von Neumann,
[00:29:18.400 --> 00:29:19.840]   as I was writing about von Neumann,
[00:29:19.840 --> 00:29:22.640]   it became impossible not to, right?
[00:29:22.640 --> 00:29:24.240]   So take game theory.
[00:29:24.240 --> 00:29:26.480]   What was he trying to do there?
[00:29:26.480 --> 00:29:29.280]   Well, this was rooted again
[00:29:29.280 --> 00:29:33.360]   in this very early 20th century idea
[00:29:33.360 --> 00:29:34.640]   amongst mathematicians
[00:29:34.640 --> 00:29:37.360]   that maths was extraordinarily successful.
[00:29:37.360 --> 00:29:40.320]   So we can apply it to kind of anything.
[00:29:40.320 --> 00:29:41.920]   And, you know, why should we leave
[00:29:42.800 --> 00:29:45.520]   the human mind and human behavior
[00:29:45.520 --> 00:29:47.200]   to psychologists
[00:29:47.200 --> 00:29:49.440]   when they've been so terribly unsuccessful
[00:29:49.440 --> 00:29:50.560]   in actually getting anywhere
[00:29:50.560 --> 00:29:51.760]   with understanding it?
[00:29:51.760 --> 00:29:54.800]   Let's try to do the maths on this.
[00:29:54.800 --> 00:29:57.360]   And so kind of that,
[00:29:57.360 --> 00:29:58.720]   I think it was that impetus
[00:29:58.720 --> 00:30:01.360]   that really drove a lot of mathematicians,
[00:30:01.360 --> 00:30:02.320]   including von Neumann,
[00:30:02.320 --> 00:30:05.520]   to tackle the theory of games,
[00:30:05.520 --> 00:30:09.760]   which is really about conflict and cooperation.
[00:30:09.760 --> 00:30:12.000]   And I think that was kind of his motivation there.
[00:30:12.000 --> 00:30:14.480]   And again, you've got,
[00:30:14.480 --> 00:30:18.560]   you know, the very thing
[00:30:18.560 --> 00:30:21.760]   that kind of some pure mathematicians would say,
[00:30:21.760 --> 00:30:22.320]   oh yeah, you know,
[00:30:22.320 --> 00:30:23.920]   von Neumann was wasting his time
[00:30:23.920 --> 00:30:27.040]   by being so involved with military work
[00:30:27.040 --> 00:30:28.880]   or, you know, this practical stuff.
[00:30:28.880 --> 00:30:30.000]   He was whizzing about
[00:30:30.000 --> 00:30:32.320]   looking for computational power.
[00:30:32.320 --> 00:30:33.760]   Well, you know,
[00:30:33.760 --> 00:30:36.320]   without that part of his personality,
[00:30:36.320 --> 00:30:40.240]   would he have been so interested in game theory?
[00:30:40.240 --> 00:30:41.040]   Would he have done,
[00:30:41.040 --> 00:30:42.480]   would he have achieved what he did,
[00:30:42.480 --> 00:30:46.240]   you know, in those terms,
[00:30:46.240 --> 00:30:48.400]   which is recasting economics
[00:30:48.400 --> 00:30:50.640]   in a completely different light, really.
[00:30:50.640 --> 00:30:51.760]   - Yeah, yeah.
[00:30:51.760 --> 00:30:52.880]   It's almost like he foresaw
[00:30:52.880 --> 00:30:55.360]   the replications crisis in psychology or something.
[00:30:55.360 --> 00:30:58.640]   You know, speaking of his work on game theory,
[00:30:58.640 --> 00:31:04.720]   I think that part was especially relevant today.
[00:31:04.720 --> 00:31:07.360]   I'm curious how, you know,
[00:31:07.360 --> 00:31:09.680]   his, you know, min-max theorem
[00:31:09.680 --> 00:31:11.360]   and theory of like zero-sum games,
[00:31:11.360 --> 00:31:12.480]   that makes it really hard,
[00:31:12.480 --> 00:31:16.800]   easy to model two-player games,
[00:31:16.800 --> 00:31:18.480]   two-player zero-sum games,
[00:31:18.480 --> 00:31:21.120]   like the one we had against the Soviet Union.
[00:31:21.120 --> 00:31:22.720]   I'm curious how he would have thought
[00:31:22.720 --> 00:31:24.640]   about a multipolar world
[00:31:24.640 --> 00:31:27.520]   where more than two parties have nuclear weapons
[00:31:27.520 --> 00:31:31.120]   and are possibly roughly equal in power.
[00:31:31.120 --> 00:31:35.360]   How would a game theory generalize
[00:31:35.360 --> 00:31:36.480]   to that kind of problem?
[00:31:37.280 --> 00:31:41.360]   - Yeah, I mean, so it's not at all clear, right?
[00:31:41.360 --> 00:31:45.200]   The von Neumann thought about nuclear strategy
[00:31:45.200 --> 00:31:49.120]   in kind of mini-max terms, a zero-sum game.
[00:31:49.120 --> 00:31:50.960]   In fact, there's quite a lot of evidence that he didn't.
[00:31:50.960 --> 00:31:54.080]   I mean, his, he, for example,
[00:31:54.080 --> 00:31:56.640]   he took very little interest in the prisoner's dilemma.
[00:31:56.640 --> 00:31:58.720]   That wasn't cooked up by him.
[00:31:58.720 --> 00:32:01.360]   It was cooked up by people at RAND
[00:32:01.360 --> 00:32:04.800]   who were kind of inspired and influenced by him.
[00:32:04.800 --> 00:32:07.200]   And of course, prisoner's dilemma isn't a zero-sum game.
[00:32:07.200 --> 00:32:08.880]   It's a non-zero-sum game.
[00:32:08.880 --> 00:32:13.520]   But it became this template
[00:32:13.520 --> 00:32:15.440]   with which many people thought
[00:32:15.440 --> 00:32:19.840]   about nuclear strategy in the Cold War.
[00:32:19.840 --> 00:32:26.080]   Now, if you look at what von Neumann wrote
[00:32:26.080 --> 00:32:29.440]   in "Theory of Games and Economic Behavior"
[00:32:29.440 --> 00:32:30.400]   with Morgenstern,
[00:32:30.400 --> 00:32:32.960]   what he was concerned with,
[00:32:32.960 --> 00:32:37.120]   his kind of solutions were based around cooperation.
[00:32:37.120 --> 00:32:42.320]   So he was like, were there stable solutions to games
[00:32:42.320 --> 00:32:47.680]   if a number of the players cooperated?
[00:32:47.680 --> 00:32:50.960]   And was this an optimal solution to the game?
[00:32:50.960 --> 00:32:55.680]   So you could imagine, right,
[00:32:55.680 --> 00:32:58.480]   say if you play, I don't know, Monopoly,
[00:32:58.480 --> 00:33:00.080]   and there's three of you,
[00:33:01.200 --> 00:33:05.120]   often what you'll notice is one player will start winning,
[00:33:05.120 --> 00:33:07.120]   and then the two other players,
[00:33:07.120 --> 00:33:08.640]   even without talking to each other,
[00:33:08.640 --> 00:33:10.640]   they'll sort of gang up on them, right?
[00:33:10.640 --> 00:33:12.640]   They'll form a kind of alliance.
[00:33:12.640 --> 00:33:20.800]   And von Neumann's early look at game theory
[00:33:20.800 --> 00:33:23.680]   was based around increasing numbers
[00:33:23.680 --> 00:33:27.840]   of these kind of alliances.
[00:33:27.840 --> 00:33:30.160]   So if you wanted to know about a 10-player game,
[00:33:30.160 --> 00:33:32.640]   von Neumann tried to kind of think about
[00:33:32.640 --> 00:33:35.280]   how within this 10-player group,
[00:33:35.280 --> 00:33:38.000]   you could get different alliances that were kind of stable
[00:33:38.000 --> 00:33:39.440]   and would lead to a winning solution.
[00:33:39.440 --> 00:33:41.760]   It wasn't entirely successful,
[00:33:41.760 --> 00:33:45.920]   and it took John Forbes Nash later on
[00:33:45.920 --> 00:33:51.680]   to kind of develop this idea of non-cooperative game theory,
[00:33:51.680 --> 00:33:54.960]   which was hugely successful.
[00:33:57.760 --> 00:34:02.080]   But that kind of doesn't chime well, really,
[00:34:02.080 --> 00:34:04.480]   with this idea of von Neumann
[00:34:04.480 --> 00:34:08.640]   viewing the world in these zero-sum terms, right?
[00:34:08.640 --> 00:34:12.480]   He came from this rather central European background
[00:34:12.480 --> 00:34:14.640]   where they were used to discussing ideas
[00:34:14.640 --> 00:34:17.440]   and kind of bars and cafes over a drink
[00:34:17.440 --> 00:34:20.960]   and talking about stuff quite freely
[00:34:20.960 --> 00:34:26.720]   and sharing and giving credit to others when it's due.
[00:34:26.720 --> 00:34:30.320]   And so, I mean, he was obviously proud
[00:34:30.320 --> 00:34:31.760]   of his own contributions,
[00:34:31.760 --> 00:34:33.600]   and he was quite defensive about them,
[00:34:33.600 --> 00:34:36.320]   but he was also reasonably honest.
[00:34:36.320 --> 00:34:38.320]   If he had culled an idea from somebody else,
[00:34:38.320 --> 00:34:43.280]   he would totally be honest about that and give them credit.
[00:34:43.280 --> 00:34:48.560]   And so this kind of thread of thinking,
[00:34:48.560 --> 00:34:52.480]   I think, was quite important.
[00:34:53.120 --> 00:34:57.120]   And it's been weirdly overlooked
[00:34:57.120 --> 00:35:01.120]   when it came to kind of this caricature of von Neumann
[00:35:01.120 --> 00:35:03.200]   that developed as a result of Kubrick
[00:35:03.200 --> 00:35:07.680]   using him as one inspiration for Dr. Strangelove later on.
[00:35:07.680 --> 00:35:12.800]   Now, von Neumann's actual thoughts on nuclear strategy,
[00:35:12.800 --> 00:35:16.240]   he penned a paper in the '50s before he died.
[00:35:17.760 --> 00:35:26.080]   And in that, he makes it clear that he doesn't,
[00:35:26.080 --> 00:35:29.360]   he's not really talking about the idea
[00:35:29.360 --> 00:35:32.240]   of a preemptive strike on the Soviet Union anymore.
[00:35:32.240 --> 00:35:33.600]   It's a lot more complicated.
[00:35:33.600 --> 00:35:37.760]   It's more like what evolved at RAND later.
[00:35:37.760 --> 00:35:41.040]   So, you know, he was deeply uncomfortable
[00:35:41.040 --> 00:35:42.720]   with this idea that, you know,
[00:35:42.720 --> 00:35:46.320]   we had two or more sides with enough nuclear weapons
[00:35:46.320 --> 00:35:48.800]   to wipe out the world many times over.
[00:35:48.800 --> 00:35:51.680]   So he thought that if nuclear weapons ever were used,
[00:35:51.680 --> 00:35:55.840]   you know, you'd have to be insane to just go all out.
[00:35:55.840 --> 00:36:01.040]   So, you know, he talked about kind of holding back.
[00:36:01.040 --> 00:36:02.240]   And, you know, you toss it,
[00:36:02.240 --> 00:36:04.480]   if one person tosses a nuclear weapon over
[00:36:04.480 --> 00:36:07.760]   and blows up the city, then the other person does.
[00:36:07.760 --> 00:36:11.440]   And it proceeds a little bit more slowly.
[00:36:11.440 --> 00:36:13.120]   It doesn't escalate all at once
[00:36:13.120 --> 00:36:16.720]   into this massive catastrophic nuclear war.
[00:36:16.720 --> 00:36:22.720]   But the thing that people picked up most about his thinking
[00:36:22.720 --> 00:36:24.720]   was, of course, in this brief period
[00:36:24.720 --> 00:36:26.320]   after the Second World War,
[00:36:26.320 --> 00:36:29.920]   where he famously said, "If you say bomb them tomorrow,
[00:36:29.920 --> 00:36:31.040]   I say, why not today?
[00:36:31.040 --> 00:36:34.160]   If you say four o'clock, why not two o'clock?"
[00:36:34.160 --> 00:36:36.480]   And, you know, it's not entirely clear
[00:36:36.480 --> 00:36:39.760]   that he meant that in all seriousness.
[00:36:39.760 --> 00:36:42.560]   I mean, his daughter certainly thinks
[00:36:42.560 --> 00:36:45.280]   he was advocating for a preemptive strike,
[00:36:45.280 --> 00:36:49.040]   or at least he was asking people to think quite rationally
[00:36:49.040 --> 00:36:50.800]   about whether a preemptive strike
[00:36:50.800 --> 00:36:52.640]   on the Soviet Union might be worthwhile,
[00:36:52.640 --> 00:36:56.240]   given that he felt that it was almost inevitable
[00:36:56.240 --> 00:36:59.120]   Stalin, as soon as he developed nuclear weapons,
[00:36:59.120 --> 00:37:04.320]   would launch a kind of strike on the United States.
[00:37:04.320 --> 00:37:07.920]   He was sort of arguing,
[00:37:07.920 --> 00:37:10.080]   "Well, you know, if we're in this situation
[00:37:10.080 --> 00:37:11.120]   where we're thinking about it,
[00:37:11.120 --> 00:37:14.160]   why shouldn't we do it sooner rather than later?
[00:37:14.160 --> 00:37:16.400]   And shouldn't we do it before the Soviet Union
[00:37:16.400 --> 00:37:21.360]   has enough weapons that, you know, they can fight back?
[00:37:21.360 --> 00:37:23.680]   And shouldn't we do something to ensure
[00:37:23.680 --> 00:37:27.040]   that nuclear power doesn't get into the wrong hands?"
[00:37:27.040 --> 00:37:29.680]   And, you know, whether that's a world government
[00:37:29.680 --> 00:37:32.160]   or whether the United States functions
[00:37:32.160 --> 00:37:36.240]   as a de facto guardian of nuclear technology,
[00:37:36.240 --> 00:37:38.080]   you know, wasn't clear.
[00:37:38.080 --> 00:37:42.080]   I think the other thing that I sort of say in my book
[00:37:42.080 --> 00:37:45.280]   is I try to lay out the context of this.
[00:37:45.280 --> 00:37:47.600]   I mean, this was after the most destructive war
[00:37:47.600 --> 00:37:48.800]   that the world had ever known.
[00:37:48.800 --> 00:37:51.280]   Millions of people had died,
[00:37:51.280 --> 00:37:54.000]   and von Neumann had predicted this
[00:37:54.000 --> 00:37:57.520]   and the Holocaust very, you know,
[00:37:57.520 --> 00:37:59.040]   successfully years in advance.
[00:37:59.040 --> 00:38:02.400]   And he now was convinced that within a decade
[00:38:02.400 --> 00:38:05.120]   there would be a third world war with nuclear weapons.
[00:38:05.120 --> 00:38:08.400]   Now, if you imagine that, and if you think that,
[00:38:08.400 --> 00:38:10.960]   and if your past predictions have come true,
[00:38:10.960 --> 00:38:15.840]   then it allows you incredible scope
[00:38:15.840 --> 00:38:21.360]   to think in this kind of rather kind of ruthless manner
[00:38:21.360 --> 00:38:25.600]   about, well, maybe bombing, you know, the Soviet Union
[00:38:25.600 --> 00:38:27.440]   and wiping out, you know,
[00:38:27.440 --> 00:38:31.360]   100,000 people's lives at the push of a button.
[00:38:31.360 --> 00:38:35.280]   Maybe that's not as bad as it could be
[00:38:35.280 --> 00:38:37.600]   when you consider that millions of people
[00:38:37.600 --> 00:38:39.280]   are gonna be dead in a decade,
[00:38:39.280 --> 00:38:41.920]   and, you know, potentially bringing
[00:38:41.920 --> 00:38:46.320]   all of human civilization to an abrupt end.
[00:38:46.320 --> 00:38:50.320]   Well, maybe we can stop that from happening.
[00:38:50.320 --> 00:38:55.920]   And it turns out that it's a surprisingly common idea
[00:38:55.920 --> 00:38:59.040]   at the time in America and elsewhere.
[00:38:59.040 --> 00:39:00.640]   I mean, Bertrand Russell, for example,
[00:39:00.640 --> 00:39:04.560]   the famous pacifist, also argued for a preemptive strike
[00:39:04.560 --> 00:39:06.480]   on the Soviet Union if they didn't give up
[00:39:06.480 --> 00:39:08.960]   their, you know, their nuclear ambitions.
[00:39:08.960 --> 00:39:11.520]   And, you know, you dig around in the post,
[00:39:11.520 --> 00:39:12.880]   kind of in the late '40s,
[00:39:12.880 --> 00:39:17.280]   in this brief window after the second world war
[00:39:17.280 --> 00:39:20.800]   when the U.S. seemed to have a virtual monopoly
[00:39:20.800 --> 00:39:22.320]   on nuclear weapons.
[00:39:22.320 --> 00:39:24.720]   And you find suddenly that, you know,
[00:39:24.720 --> 00:39:27.680]   a lot more people supported this idea,
[00:39:27.680 --> 00:39:30.400]   including a large proportion, by the way,
[00:39:30.400 --> 00:39:35.760]   of the American public than you think is possible.
[00:39:35.760 --> 00:39:38.560]   You know, as you talk about in the book,
[00:39:38.560 --> 00:39:40.320]   there's like a very interesting
[00:39:40.320 --> 00:39:42.880]   but extremely scary, precarious scenario
[00:39:42.880 --> 00:39:46.080]   where two sides have a nuclear weapon
[00:39:46.080 --> 00:39:48.080]   or think that both sides have a nuclear weapon,
[00:39:48.080 --> 00:39:50.160]   but neither one has developed the ability yet
[00:39:50.160 --> 00:39:56.080]   to defend their nuclear silos against initial attack.
[00:39:56.080 --> 00:39:58.960]   So then, you know, both of them think
[00:39:58.960 --> 00:40:00.800]   that the other one, if they launch a first strike,
[00:40:00.800 --> 00:40:01.920]   there would be no deterrence.
[00:40:01.920 --> 00:40:03.280]   So then both of them are incentivized
[00:40:03.280 --> 00:40:04.480]   to launch that first strike,
[00:40:04.480 --> 00:40:06.880]   which is kind of like the opposite of mad.
[00:40:06.880 --> 00:40:08.960]   And, you know, that's one worry if, like,
[00:40:08.960 --> 00:40:12.800]   I don't know, if nuclear technology gets better.
[00:40:12.800 --> 00:40:14.800]   In some ways, that could make a nuclear war
[00:40:14.800 --> 00:40:17.520]   much more likely because people could start thinking,
[00:40:17.520 --> 00:40:21.760]   okay, but we can just take out all their entire arsenal
[00:40:21.760 --> 00:40:23.280]   so they have no way to retaliate.
[00:40:25.600 --> 00:40:28.640]   I'm curious what, you mentioned, you know,
[00:40:28.640 --> 00:40:31.280]   he had a good way of thinking about escalation.
[00:40:31.280 --> 00:40:33.200]   I'm curious how he would have thought about,
[00:40:33.200 --> 00:40:36.160]   you know, one problem we have today
[00:40:36.160 --> 00:40:38.400]   is like you can have cyber warfare,
[00:40:38.400 --> 00:40:41.360]   which is immensely destructive in an economic sense,
[00:40:41.360 --> 00:40:44.480]   but doesn't warrant or seem to warrant a sort of land war.
[00:40:44.480 --> 00:40:46.160]   And then you can have a land war, like, I don't know,
[00:40:46.160 --> 00:40:47.440]   China takes over Taiwan or, you know,
[00:40:47.440 --> 00:40:48.960]   you have what's going on in Ukraine.
[00:40:48.960 --> 00:40:52.880]   But it seems like way too harsh to react with nuclear war.
[00:40:53.440 --> 00:40:56.640]   And I'm curious how von Neumann would have been able
[00:40:56.640 --> 00:40:57.840]   to think about these kinds of problems.
[00:40:57.840 --> 00:41:04.080]   You know, von Neumann, I mean, he was recruited by Rand,
[00:41:04.080 --> 00:41:07.600]   but the work that he did, and Rand became this
[00:41:07.600 --> 00:41:11.200]   kind of hothouse for nuclear strategic thinking, right,
[00:41:11.200 --> 00:41:16.160]   in the Cold War, and it influenced American policy.
[00:41:16.160 --> 00:41:21.600]   But von Neumann, apart from this paper on nuclear strategy,
[00:41:21.600 --> 00:41:24.400]   he seems to have taken remarkable little interest
[00:41:24.400 --> 00:41:25.520]   in the whole thing.
[00:41:25.520 --> 00:41:27.120]   I mean, when he was at Rand,
[00:41:27.120 --> 00:41:36.160]   he was computing various solutions to kind of duels.
[00:41:36.160 --> 00:41:39.120]   So, you know, he'd worked out the minimax theorem.
[00:41:39.120 --> 00:41:41.360]   And so he was busy.
[00:41:41.360 --> 00:41:44.560]   Well, you know, if you have a plane and a,
[00:41:44.560 --> 00:41:48.960]   I don't know, a tank or, you know,
[00:41:48.960 --> 00:41:51.040]   whatever, a submarine and a ship,
[00:41:51.040 --> 00:41:53.040]   you know, and they can see each other coming.
[00:41:53.040 --> 00:41:54.960]   At what point should they fire?
[00:41:54.960 --> 00:41:57.440]   At what, you know, at what point should they do this?
[00:41:57.440 --> 00:42:00.640]   And so he got kind of involved in that and computing.
[00:42:00.640 --> 00:42:04.480]   And he kind of lost interest in game theory again,
[00:42:04.480 --> 00:42:07.440]   as soon as computing came to the fore.
[00:42:07.440 --> 00:42:09.760]   So he helped, so whilst he was doing this,
[00:42:09.760 --> 00:42:12.800]   he ended up helping Rand kind of realize
[00:42:12.800 --> 00:42:16.160]   their own ambitions of having a computer.
[00:42:18.320 --> 00:42:21.360]   So it's not at all clear to me
[00:42:21.360 --> 00:42:25.760]   how much he'd still carry on being involved in the strategy,
[00:42:25.760 --> 00:42:30.400]   you know, in the nuclear strategy side.
[00:42:30.400 --> 00:42:35.040]   But of course, I mean, this idea of kind of,
[00:42:35.040 --> 00:42:38.720]   if you are coming up with your best strategy,
[00:42:38.720 --> 00:42:43.680]   then you have to think what, you know,
[00:42:43.680 --> 00:42:45.360]   your opponent will make of that.
[00:42:45.360 --> 00:42:48.800]   And you have to imagine that they're also,
[00:42:48.800 --> 00:42:51.120]   you know, an intelligent opponent
[00:42:51.120 --> 00:42:54.080]   who's going to be out for themselves.
[00:42:54.080 --> 00:42:57.040]   And that thinking is very deeply embedded into Minimax.
[00:42:57.040 --> 00:43:04.880]   And, you know, and that was clearly very influential later on.
[00:43:04.880 --> 00:43:06.720]   - One thing I find very interesting about
[00:43:06.720 --> 00:43:07.920]   Von Neumann's work for the government
[00:43:07.920 --> 00:43:11.440]   and in aiding these kinds of strategic conversations is,
[00:43:11.440 --> 00:43:14.240]   at least from my understanding,
[00:43:14.240 --> 00:43:16.880]   it seems that a lot of the scientists during that time
[00:43:16.880 --> 00:43:20.160]   were somewhat radical and sympathetic to socialism,
[00:43:20.160 --> 00:43:22.160]   you know, like Bertrand Russell or Oppenheimer.
[00:43:22.160 --> 00:43:25.200]   And Von Neumann seems to be a very practical,
[00:43:25.200 --> 00:43:27.280]   non-radical person.
[00:43:27.280 --> 00:43:30.240]   I mean, you can think that's a good thing or a bad thing,
[00:43:30.240 --> 00:43:34.080]   but it seems like he broke from the conventional,
[00:43:34.080 --> 00:43:37.200]   I guess, elite scientific culture at the time.
[00:43:37.200 --> 00:43:39.520]   I'm curious, what about his personality or background
[00:43:39.520 --> 00:43:41.520]   do you think made him that way?
[00:43:41.520 --> 00:43:43.520]   Am I even characterizing the situation in the correct way?
[00:43:43.520 --> 00:43:46.960]   - Yeah, no, I think that's fair.
[00:43:46.960 --> 00:43:50.960]   In fact, if anything, he was considered kind of right-wing
[00:43:50.960 --> 00:43:55.520]   or at least a Cold War hawk in certain circles.
[00:43:55.520 --> 00:44:00.080]   I think if you look at him quite closely,
[00:44:00.080 --> 00:44:01.760]   I mean, you could argue in many ways
[00:44:01.760 --> 00:44:04.400]   he was, you know, something of a liberal,
[00:44:04.400 --> 00:44:07.360]   but, you know, at the time,
[00:44:07.360 --> 00:44:09.200]   some, you know, a lot of people felt
[00:44:09.200 --> 00:44:11.040]   that he was quite hawkish.
[00:44:12.000 --> 00:44:14.960]   Now, the reason for that is that
[00:44:14.960 --> 00:44:21.280]   there was a, shortly after the First World War in Hungary,
[00:44:21.280 --> 00:44:22.400]   there were two things that happened.
[00:44:22.400 --> 00:44:26.640]   One was there was a very short-lived communist uprising
[00:44:26.640 --> 00:44:28.800]   and that government lasted for six months
[00:44:28.800 --> 00:44:29.920]   and it was pretty brutal.
[00:44:29.920 --> 00:44:35.760]   You know, they reclaimed private property
[00:44:35.760 --> 00:44:38.560]   from wealthy families
[00:44:38.560 --> 00:44:41.600]   and there was just general chaos
[00:44:41.600 --> 00:44:44.400]   and beatings on the street and stuff.
[00:44:44.400 --> 00:44:48.880]   But then something happened afterwards
[00:44:48.880 --> 00:44:52.000]   and a military, essentially, you know,
[00:44:52.000 --> 00:44:55.520]   a military government just marched in,
[00:44:55.520 --> 00:44:58.400]   led by General Horthy, and they took control.
[00:44:58.400 --> 00:45:00.320]   And that turned out to be even worse.
[00:45:00.320 --> 00:45:04.480]   I mean, there was public hangings and rapes
[00:45:04.480 --> 00:45:08.800]   and, you know, thousands of people ended up dead.
[00:45:08.800 --> 00:45:14.080]   And many Jewish people at that time
[00:45:14.080 --> 00:45:16.480]   were seen to have been collaborating
[00:45:16.480 --> 00:45:21.120]   with the earlier communist government.
[00:45:21.120 --> 00:45:24.480]   So, you know, many Jews were basically shot
[00:45:24.480 --> 00:45:25.840]   on the streets as well.
[00:45:25.840 --> 00:45:29.040]   Now, the von Neumanns were, you know,
[00:45:29.040 --> 00:45:31.280]   by a dint of their wealth,
[00:45:32.480 --> 00:45:34.640]   they were kind of protected from this,
[00:45:34.640 --> 00:45:38.240]   but von Neumann saw all of this as he was growing up.
[00:45:38.240 --> 00:45:41.600]   And then, of course, later with the rise of the Nazis
[00:45:41.600 --> 00:45:48.400]   in Germany, he, you know, he had left Germany by then,
[00:45:48.400 --> 00:45:50.320]   but a lot of his formative years
[00:45:50.320 --> 00:45:53.440]   as a scientist or as a mathematician
[00:45:53.440 --> 00:45:55.600]   were spent in Germany.
[00:45:55.600 --> 00:45:59.760]   And he adored kind of interwar Germany
[00:45:59.760 --> 00:46:02.320]   in the, at least in the late '20s.
[00:46:02.320 --> 00:46:05.760]   And for him, it was this perfect intellectual climate.
[00:46:05.760 --> 00:46:08.320]   I mean, you have to remember that Germany was,
[00:46:08.320 --> 00:46:10.240]   you know, scientifically and mathematically
[00:46:10.240 --> 00:46:12.880]   definitely kind of the center of the world then.
[00:46:12.880 --> 00:46:16.320]   I mean, America just was nothing at the time.
[00:46:16.320 --> 00:46:22.320]   It was only, you know, kind of during the Second World War
[00:46:22.320 --> 00:46:25.920]   and post the Second World War that from the '30s,
[00:46:25.920 --> 00:46:30.480]   late '30s onwards that America became this scientific
[00:46:30.480 --> 00:46:32.800]   and technological kind of powerhouse really.
[00:46:32.800 --> 00:46:36.080]   And, you know, it benefited from many
[00:46:36.080 --> 00:46:38.880]   of these European scientists who left
[00:46:38.880 --> 00:46:40.480]   as a result of the Nazis.
[00:46:40.480 --> 00:46:45.280]   Now he'd seen this and his lesson was that authoritarianism,
[00:46:45.280 --> 00:46:50.560]   you know, is something that we shouldn't tolerate.
[00:46:50.560 --> 00:46:52.720]   And so when he came to the States,
[00:46:52.720 --> 00:46:56.160]   his priority was to put his expertise
[00:46:56.160 --> 00:46:59.600]   into the hands of the democratic government there.
[00:46:59.600 --> 00:47:05.200]   And whilst he definitely was advising them,
[00:47:05.200 --> 00:47:10.960]   he, you know, I got the feeling that, you know,
[00:47:10.960 --> 00:47:14.800]   he wasn't interested in making decisions on their behalf
[00:47:14.800 --> 00:47:18.160]   'cause, you know, this was a democratically elected government.
[00:47:18.160 --> 00:47:20.000]   I think deep down he was a Democrat.
[00:47:20.000 --> 00:47:22.640]   He felt he should work as hard as possible
[00:47:22.640 --> 00:47:26.000]   to give the U.S. government the tools that it needed
[00:47:26.000 --> 00:47:29.920]   to overcome the Nazis and to, you know,
[00:47:29.920 --> 00:47:35.040]   and to, you know, maintain their lead
[00:47:35.040 --> 00:47:39.680]   as kind of the preeminent democracy in the world.
[00:47:39.680 --> 00:47:44.960]   But so he was kind of, I don't know,
[00:47:44.960 --> 00:47:48.400]   I think more allergic to authoritarianism.
[00:47:48.400 --> 00:47:54.800]   Whereas I think, you know,
[00:47:54.800 --> 00:47:57.280]   before the Second World War happened
[00:47:57.280 --> 00:48:00.320]   and before we knew what was happening under Stalin,
[00:48:00.320 --> 00:48:04.320]   there were many intellectuals who were willing to give,
[00:48:04.320 --> 00:48:06.800]   you know, the communism, you know,
[00:48:06.800 --> 00:48:10.960]   deep left thinking more of a chance.
[00:48:10.960 --> 00:48:14.080]   Whereas von Neumann had kind of seen
[00:48:14.080 --> 00:48:18.320]   what that turned into in Hungary.
[00:48:18.320 --> 00:48:20.880]   And he'd seen that essentially
[00:48:20.880 --> 00:48:23.360]   it became a kind of authoritarian regime.
[00:48:23.360 --> 00:48:29.280]   He was deeply suspicious of Stalin from day one
[00:48:29.280 --> 00:48:30.800]   for the very same reason.
[00:48:30.800 --> 00:48:34.000]   And he'd had these experiences of, you know,
[00:48:34.000 --> 00:48:36.880]   Europe being turned upside down by the Nazis.
[00:48:36.880 --> 00:48:41.040]   And I think that really shaped him very profoundly.
[00:48:42.320 --> 00:48:44.800]   He became quite cynical about human nature as well
[00:48:44.800 --> 00:48:46.080]   at the same time.
[00:48:46.080 --> 00:48:49.200]   I think, you know, deep down it was, you know,
[00:48:49.200 --> 00:48:53.600]   superficially he was kind of a good man
[00:48:53.600 --> 00:48:57.920]   and he, you know, he was nice to people.
[00:48:57.920 --> 00:49:02.320]   And I think that's really where he started, you know,
[00:49:02.320 --> 00:49:05.520]   in his day-to-day interactions with people, he was nice.
[00:49:05.520 --> 00:49:08.800]   He would do these incredible things very quietly
[00:49:08.800 --> 00:49:11.520]   behind people's backs that many other scientists
[00:49:11.520 --> 00:49:12.240]   wouldn't dream of.
[00:49:12.240 --> 00:49:14.720]   Like, you know, this builder, Hungarian builder,
[00:49:14.720 --> 00:49:16.720]   contacted him in the middle of the Second World War
[00:49:16.720 --> 00:49:18.800]   and said, "I want to learn more about maths,
[00:49:18.800 --> 00:49:21.840]   but I'm in America basically building stuff.
[00:49:21.840 --> 00:49:23.360]   Where do I find out more about maths?"
[00:49:23.360 --> 00:49:26.960]   So he writes to his friend in wartime Hungary
[00:49:26.960 --> 00:49:28.160]   and gets them to send over
[00:49:28.160 --> 00:49:30.640]   a bunch of Hungarian maths textbooks.
[00:49:30.640 --> 00:49:35.920]   I mean, and later on, you've got people like Mandelbrot
[00:49:35.920 --> 00:49:40.960]   who came over thanks to his reference.
[00:49:40.960 --> 00:49:43.600]   And, you know, he was at Princeton and the IAS.
[00:49:43.600 --> 00:49:48.400]   And years later, when Mandelbrot ran into problems
[00:49:48.400 --> 00:49:52.400]   with his boss, he goes looking for work elsewhere
[00:49:52.400 --> 00:49:56.320]   and he finds that like, whatever, a decade earlier,
[00:49:56.320 --> 00:49:57.920]   long after, you know,
[00:49:57.920 --> 00:49:59.920]   and this is long after von Neumann was dead,
[00:49:59.920 --> 00:50:02.880]   you know, von Neumann had sent out letters
[00:50:02.880 --> 00:50:04.320]   and talked to people.
[00:50:04.320 --> 00:50:06.960]   So, you know, Mandelbrot is doing really important work,
[00:50:06.960 --> 00:50:08.320]   but, you know, he may struggle
[00:50:08.320 --> 00:50:10.720]   because what he's doing is so cutting edge.
[00:50:10.720 --> 00:50:13.680]   So if he does, and he comes looking for a job,
[00:50:13.680 --> 00:50:15.360]   please, you know, give him a job
[00:50:15.360 --> 00:50:16.800]   because this guy's brilliant.
[00:50:16.800 --> 00:50:18.400]   And, you know, he does these little things
[00:50:18.400 --> 00:50:21.600]   and he, of course, helps scientists leave,
[00:50:21.600 --> 00:50:28.000]   kind of, Europe before the Nazis make that impossible.
[00:50:28.000 --> 00:50:33.520]   He helps to get Gödel out of Germany, for example.
[00:50:33.520 --> 00:50:36.560]   So, you know, he's this very conflicted personality.
[00:50:36.560 --> 00:50:41.600]   So I think, you know, he's, as you would expect,
[00:50:41.600 --> 00:50:45.280]   quite a complex and thoughtful human being.
[00:50:45.280 --> 00:50:47.680]   And he's not easily characterized as, you know,
[00:50:47.680 --> 00:50:52.720]   Dr. Strangelove or, you know, a bleeding heart liberal.
[00:50:52.720 --> 00:50:57.600]   - I understand what you meant, but out of context,
[00:50:57.600 --> 00:50:59.680]   he was superficially a good man,
[00:50:59.680 --> 00:51:03.120]   has got to be the best backhanded compliment ever.
[00:51:03.440 --> 00:51:05.520]   (both laughing)
[00:51:05.520 --> 00:51:07.520]   So the final question, out of yourself for your time,
[00:51:07.520 --> 00:51:09.040]   you know, you're a researcher yourself,
[00:51:09.040 --> 00:51:11.200]   you know, you have a PhD in protein crystallography,
[00:51:11.200 --> 00:51:12.720]   you were a medical researcher,
[00:51:12.720 --> 00:51:15.760]   and now you've analyzed John Morton Neumann's life,
[00:51:15.760 --> 00:51:17.440]   you know, probably one of the greatest,
[00:51:17.440 --> 00:51:20.960]   probably the greatest genius of all time.
[00:51:20.960 --> 00:51:25.040]   Do you, have you like extrapolated some lessons
[00:51:25.040 --> 00:51:26.320]   about how to be prolific
[00:51:26.320 --> 00:51:29.120]   or how to come up with new insights in different fields?
[00:51:32.000 --> 00:51:36.000]   - Not at all, but I would thoroughly recommend
[00:51:36.000 --> 00:51:37.280]   if you're going to write a book,
[00:51:37.280 --> 00:51:40.720]   that you try not to give up your day job
[00:51:40.720 --> 00:51:45.680]   a year before the worst pandemic descends
[00:51:45.680 --> 00:51:48.320]   that we've known about for, you know, decades,
[00:51:48.320 --> 00:51:50.720]   descends on and engulfs the planet.
[00:51:50.720 --> 00:51:54.800]   Thus ensuring that instead of working on your book
[00:51:54.800 --> 00:51:58.800]   about the cleverest person of the 20th century,
[00:51:59.440 --> 00:52:01.440]   who works on abstruse set theory,
[00:52:01.440 --> 00:52:05.520]   you end up having to homeschool a recalcitrant 10 year old.
[00:52:05.520 --> 00:52:09.920]   So that's one, you know, if you want to be productive,
[00:52:09.920 --> 00:52:11.360]   don't do that, okay.
[00:52:11.360 --> 00:52:15.920]   But in other terms, I think, you know,
[00:52:15.920 --> 00:52:21.200]   it's dangerous trying to, you know,
[00:52:21.200 --> 00:52:23.280]   come out with a kind of self-help book
[00:52:23.280 --> 00:52:25.760]   based on von Neumann's lifestyle, right?
[00:52:25.760 --> 00:52:28.320]   I mean, his first wife left him
[00:52:28.320 --> 00:52:31.440]   because he was too busy thinking.
[00:52:31.440 --> 00:52:34.560]   And, you know, she took up with essentially
[00:52:34.560 --> 00:52:37.920]   a graduate student, Horner Cooper,
[00:52:37.920 --> 00:52:40.800]   who was, you know, a physics graduate student.
[00:52:40.800 --> 00:52:45.760]   And, you know, and she was quite the thinker herself.
[00:52:45.760 --> 00:52:49.040]   She ended up becoming this mover and shaker
[00:52:49.040 --> 00:52:51.520]   in science admin.
[00:52:51.520 --> 00:52:56.320]   And, you know, his second wife was very clever herself,
[00:52:56.320 --> 00:52:59.520]   Clara Dan, but, you know,
[00:52:59.520 --> 00:53:04.800]   he thought incessantly from morning to night.
[00:53:04.800 --> 00:53:07.440]   And, you know, even at the cocktail parties that he threw,
[00:53:07.440 --> 00:53:12.080]   he would sometimes just find noise conducive to work.
[00:53:12.080 --> 00:53:14.400]   And he would just rush off cocktail in hand
[00:53:14.400 --> 00:53:16.400]   to write down some theorem.
[00:53:16.400 --> 00:53:18.240]   I mean, what do you draw?
[00:53:18.240 --> 00:53:21.280]   What kind of lessons do you draw from that?
[00:53:21.280 --> 00:53:22.720]   You know, the only lesson I draw
[00:53:22.720 --> 00:53:25.520]   is that just don't do that.
[00:53:25.920 --> 00:53:30.080]   You know, try and forge some sort of work schedule
[00:53:30.080 --> 00:53:31.600]   that kind of works for you.
[00:53:31.600 --> 00:53:32.880]   We can't all be superhuman.
[00:53:32.880 --> 00:53:36.160]   And, you know, as we see his relationships,
[00:53:36.160 --> 00:53:38.080]   his human relationships suffered.
[00:53:38.080 --> 00:53:42.960]   And he was, you know, deeply troubled
[00:53:42.960 --> 00:53:45.280]   as he went out at the close of his life
[00:53:45.280 --> 00:53:51.040]   as, you know, cancer was eroding his mental capabilities.
[00:53:51.040 --> 00:53:55.360]   I mean, he kind of rediscovered Catholicism.
[00:53:55.360 --> 00:53:57.760]   He'd converted when he was younger,
[00:53:57.760 --> 00:53:59.840]   but he had this, he was overtaken
[00:53:59.840 --> 00:54:02.080]   by this fear of mortality.
[00:54:02.080 --> 00:54:03.280]   And I think, you know,
[00:54:03.280 --> 00:54:05.600]   when we think about a productive life,
[00:54:05.600 --> 00:54:09.120]   I think, you know, we probably all want to go out
[00:54:09.120 --> 00:54:13.760]   on something of a high and not go out in abject terror.
[00:54:13.760 --> 00:54:17.520]   So, yeah, you know, read about this incredible human being,
[00:54:17.520 --> 00:54:20.000]   but don't try to draw too many life lessons from it.
[00:54:20.000 --> 00:54:23.200]   - Yeah, yeah, no, that's definitely very fair.
[00:54:23.200 --> 00:54:25.520]   You're an Ajahn Moynihan, almost certainly.
[00:54:25.520 --> 00:54:28.240]   So, Ananya, thank you so much for your time.
[00:54:28.240 --> 00:54:30.320]   I really appreciate you coming on the podcast.
[00:54:30.320 --> 00:54:31.200]   - Thanks very much.
[00:54:31.200 --> 00:54:32.240]   It was a pleasure.
[00:54:32.240 --> 00:54:34.800]   (upbeat music)
[00:54:34.800 --> 00:54:37.380]   (upbeat music)
[00:54:37.380 --> 00:54:39.960]   (upbeat music)
[00:54:39.960 --> 00:54:42.540]   (upbeat music)
[00:54:42.540 --> 00:54:43.540]   Beep. Beep.

