
[00:00:00.000 --> 00:00:05.340]   So the existence of sex is the most pessimistic thing there is, I find that ironic.
[00:00:05.340 --> 00:00:07.860]   So I'm not the pessimist sex is, right?
[00:00:07.860 --> 00:00:12.200]   So it could be Y Combinator is a bit stale, but stale in the good sense, like Harvard
[00:00:12.200 --> 00:00:13.200]   is stale, right?
[00:00:13.200 --> 00:00:15.940]   It dates from the 17th century, but it's still amazing.
[00:00:15.940 --> 00:00:21.860]   We'll be permanently set back, kind of forever, and in the meantime, we can't build asteroid
[00:00:21.860 --> 00:00:27.380]   protection or whatever else, and it will just be like medieval living standards, super small
[00:00:27.380 --> 00:00:32.340]   population, feudal governance, lots of violence, rape, whatever.
[00:00:32.340 --> 00:00:38.820]   A lot of the IDW people have very clear peaks, which now lie in the past, but they made extreme
[00:00:38.820 --> 00:00:44.940]   bets on very particular ideas, and maybe different people will disagree about those ideas, but
[00:00:44.940 --> 00:00:50.380]   I think a lot of them are losing ideas, even if they might be correct in some ways.
[00:00:50.380 --> 00:00:56.780]   I've done much less to bet on a single idea to ask Brian about, like early and late Kaplan
[00:00:56.780 --> 00:00:58.900]   and which ways are they not consistent?
[00:00:58.900 --> 00:01:00.980]   That's the, it's kind of friendly jab.
[00:01:00.980 --> 00:01:01.980]   Okay, interesting.
[00:01:01.980 --> 00:01:02.980]   Yeah.
[00:01:02.980 --> 00:01:05.380]   Garrett Jones has tweeted about this in the past.
[00:01:05.380 --> 00:01:09.220]   So like in the myth of the rational voter, education is so wonderful.
[00:01:09.220 --> 00:01:10.300]   Like it makes you more free market?
[00:01:10.300 --> 00:01:11.300]   Yeah.
[00:01:11.300 --> 00:01:12.300]   Yeah, yeah, yeah.
[00:01:12.300 --> 00:01:13.300]   And like, hey, it no longer seems to be true.
[00:01:13.300 --> 00:01:18.340]   Now, it was true from the data Brian took from, and second, Brian doesn't think education
[00:01:18.340 --> 00:01:19.780]   really teaches you much.
[00:01:19.780 --> 00:01:21.420]   So then why is it making people free market?
[00:01:21.420 --> 00:01:22.420]   Yes.
[00:01:22.420 --> 00:01:23.420]   Yeah.
[00:01:23.420 --> 00:01:26.260]   Like it once did, even though it doesn't now, and if it doesn't now, it may teach some
[00:01:26.260 --> 00:01:27.260]   bad things.
[00:01:27.260 --> 00:01:28.260]   Right.
[00:01:28.260 --> 00:01:29.260]   But like it's teaching them something.
[00:01:29.260 --> 00:01:32.900]   I have asked him this, so he thinks that it doesn't teach him anything, therefore that
[00:01:32.900 --> 00:01:35.420]   wokenism can be a result of colleges.
[00:01:35.420 --> 00:01:38.900]   And then I've asked him, okay, at some point, these were like ideas in colleges that are
[00:01:38.900 --> 00:01:39.900]   in the broader world.
[00:01:39.900 --> 00:01:41.380]   What do you think happened?
[00:01:41.380 --> 00:01:42.980]   Why did it transition from one to the other?
[00:01:42.980 --> 00:01:43.980]   Yeah.
[00:01:43.980 --> 00:01:44.980]   I don't think you had a good answer to that.
[00:01:44.980 --> 00:01:45.980]   Yeah.
[00:01:45.980 --> 00:01:46.980]   You can put this in the podcast if you want.
[00:01:46.980 --> 00:01:47.980]   Up to you.
[00:01:47.980 --> 00:01:50.700]   So I like the free podcast talk often better than the podcast.
[00:01:50.700 --> 00:01:51.700]   Okay.
[00:01:51.700 --> 00:01:53.180]   Well, yeah, we can just start rolling.
[00:01:53.180 --> 00:01:54.180]   Okay.
[00:01:54.180 --> 00:01:58.860]   So Tyler, it is my great pleasure to once again, speak to Tyler Cowen now about his
[00:01:58.860 --> 00:02:05.260]   new book, Talent, How to Find Energizers, Creatives, and Winners Across the World.
[00:02:05.260 --> 00:02:07.100]   Tyler, welcome to the Lunar Society again.
[00:02:07.100 --> 00:02:08.100]   Happy to be here.
[00:02:08.100 --> 00:02:09.100]   Thank you.
[00:02:09.100 --> 00:02:10.100]   Okay.
[00:02:10.100 --> 00:02:11.100]   Excellent.
[00:02:11.100 --> 00:02:13.340]   I want to get talent in just a second, but I've got a few questions for you first.
[00:02:13.340 --> 00:02:14.340]   Absolutely.
[00:02:14.340 --> 00:02:20.300]   So in terms of novelty and wonder, do you think traveling to the past would be a fundamentally
[00:02:20.300 --> 00:02:23.620]   different experience to traveling to different countries today, or is it kind of the same
[00:02:23.620 --> 00:02:25.860]   category of thing?
[00:02:25.860 --> 00:02:30.500]   You need to be protected against disease and have some access to the languages.
[00:02:30.500 --> 00:02:33.660]   And obviously your smartphone is not going to work, right?
[00:02:33.660 --> 00:02:38.460]   So if you adjust for those differences, I think it would be a lot like traveling today,
[00:02:38.460 --> 00:02:43.340]   except there'd be bigger surprises because no one else has gone to the past, right?
[00:02:43.340 --> 00:02:46.020]   Older people, in a sense, were there.
[00:02:46.020 --> 00:02:50.740]   But if you go back to ancient Athens or the peak of the Roman empire, you're the first
[00:02:50.740 --> 00:02:51.740]   traveler.
[00:02:51.740 --> 00:02:52.740]   Right.
[00:02:52.740 --> 00:02:55.900]   And then the experience of reading a history book, you think it's somewhat substitutable
[00:02:55.900 --> 00:02:57.780]   for actually traveling to a place today?
[00:02:57.780 --> 00:02:58.780]   No, not at all.
[00:02:58.780 --> 00:03:01.180]   I think we understand the past very, very poorly.
[00:03:01.180 --> 00:03:05.980]   And if you travel appropriately in contemporary times, it should make you more skeptical about
[00:03:05.980 --> 00:03:10.220]   history because you'll realize how little you can learn about the current places just
[00:03:10.220 --> 00:03:11.380]   by reading about them.
[00:03:11.380 --> 00:03:12.380]   Oh, interesting.
[00:03:12.380 --> 00:03:13.380]   Okay.
[00:03:13.380 --> 00:03:16.580]   So it's like travel versus history and the historians lose.
[00:03:16.580 --> 00:03:17.580]   Right.
[00:03:17.580 --> 00:03:22.540]   So I'm curious, how has traveling a lot just changed your perspective when you read a work
[00:03:22.540 --> 00:03:23.540]   of history?
[00:03:23.540 --> 00:03:25.820]   In what ways are you skeptical of it that you weren't before?
[00:03:25.820 --> 00:03:27.780]   What do you think they're probably getting wrong?
[00:03:27.780 --> 00:03:32.340]   Well, it may not be a concrete way, but first you ask, was the person there?
[00:03:32.340 --> 00:03:37.020]   Or if it's a biography, did he or she know the subject of the biography?
[00:03:37.020 --> 00:03:40.360]   And that becomes an extremely important question.
[00:03:40.360 --> 00:03:43.540]   So I was just in India for the sixth time.
[00:03:43.540 --> 00:03:48.260]   I hardly pretend to understand India, whatever that possibly might mean.
[00:03:48.260 --> 00:03:52.800]   But if I think like before I went at all, I'd read a few hundred books about India.
[00:03:52.800 --> 00:03:54.980]   Like I didn't get nothing out of them.
[00:03:54.980 --> 00:03:58.500]   But in some sense, I knew nothing about India.
[00:03:58.500 --> 00:04:02.220]   And now the other things I read make more sense as well, including the history.
[00:04:02.220 --> 00:04:03.220]   Okay.
[00:04:03.220 --> 00:04:04.220]   Interesting.
[00:04:04.220 --> 00:04:07.540]   So you've asked this question to many of your guests, and I don't think any of them have
[00:04:07.540 --> 00:04:08.540]   had a good answer.
[00:04:08.540 --> 00:04:11.460]   So let me just ask you, what do you think is the explanation behind Conquest's Second
[00:04:11.460 --> 00:04:12.460]   Law?
[00:04:12.460 --> 00:04:15.580]   Why does any institution that is not explicitly right-wing become left-wing over time?
[00:04:15.580 --> 00:04:19.740]   Well, first of all, I'm not sure that Conquest's Second Law is true.
[00:04:19.740 --> 00:04:24.860]   So you have something like the World Bank, which is quite sort of centrist-statist in
[00:04:24.860 --> 00:04:29.860]   the 1960s, but by the 1990s has become fairly neoliberal.
[00:04:29.860 --> 00:04:31.660]   Now what's left-wing, right-wing in that?
[00:04:31.660 --> 00:04:32.660]   It's global.
[00:04:32.660 --> 00:04:33.660]   It's complicated.
[00:04:33.660 --> 00:04:37.300]   But it's not a simple case of Conquest's Law holding, right?
[00:04:37.300 --> 00:04:44.180]   So I do think for a big part of the latter post-war era, some version of Conquest's Law
[00:04:44.180 --> 00:04:46.380]   does mostly hold for the United States.
[00:04:46.380 --> 00:04:53.180]   But once you see it is not universal, you're just asking, well, why is the American intelligentsia
[00:04:53.180 --> 00:04:54.740]   shifted to the left?
[00:04:54.740 --> 00:04:58.800]   So there's a political science literature on educational polarization.
[00:04:58.800 --> 00:05:04.140]   I wouldn't say it's a settled question, but it's not a huge mystery, right?
[00:05:04.140 --> 00:05:07.260]   Republicans act wackier, Democrats sort more.
[00:05:07.260 --> 00:05:10.340]   The issues realign in particular ways.
[00:05:10.340 --> 00:05:14.820]   So I think that's why Conquest's Law locally is mostly holding.
[00:05:14.820 --> 00:05:15.860]   —Oh, interesting.
[00:05:15.860 --> 00:05:19.540]   So you don't think there's anything special about the intellectual life that tends to
[00:05:19.540 --> 00:05:20.940]   make people left-wing?
[00:05:20.940 --> 00:05:23.220]   It's a particular to our current moment?
[00:05:23.220 --> 00:05:26.420]   —I think by choosing the word "left-wing," you're begging the question.
[00:05:26.420 --> 00:05:31.860]   There's a lot of historical eras where what is left-wing is not even well-defined.
[00:05:31.860 --> 00:05:36.060]   And in that sense, Conquest's Law can't even hold there.
[00:05:36.060 --> 00:05:41.140]   So once I had a debate with Mark Andreessen about this, I think Mark tends to see left-wing,
[00:05:41.140 --> 00:05:46.100]   right-wing, wherever your views might be, as somewhat universal historical categories.
[00:05:46.100 --> 00:05:47.600]   And I very much do not.
[00:05:47.600 --> 00:05:51.260]   So in medieval times, what's left-wing, what's right-wing?
[00:05:51.260 --> 00:05:57.140]   Even 17th century England, there are like particular groups who on particular issues
[00:05:57.140 --> 00:05:58.620]   are very left or right-wing.
[00:05:58.620 --> 00:05:59.620]   I don't know.
[00:05:59.620 --> 00:06:04.300]   It seems to me unsatisfying, and there's a lot of fluidity in how these axes play out
[00:06:04.300 --> 00:06:05.580]   over real issues.
[00:06:05.580 --> 00:06:06.580]   —Interesting.
[00:06:06.580 --> 00:06:12.100]   So maybe then what is considered left at the time is the thing that ended up winning, or
[00:06:12.100 --> 00:06:15.620]   at least looking back on it, that's how we categorize things.
[00:06:15.620 --> 00:06:19.140]   Something insightful I heard from Berne Hobart is that if the left keeps winning, then just
[00:06:19.140 --> 00:06:20.260]   redefine what the left is.
[00:06:20.260 --> 00:06:22.220]   So if you think of prohibition, right?
[00:06:22.220 --> 00:06:23.780]   At the time it was a left-wing cause, right?
[00:06:23.780 --> 00:06:27.060]   Now the opposite of prohibition is a left-wing cause, so then just change what left is.
[00:06:27.060 --> 00:06:28.060]   —Exactly.
[00:06:28.060 --> 00:06:29.060]   Or like take the French Revolution.
[00:06:29.060 --> 00:06:34.460]   Their equivalent of like non-profits then versus 1830, restoration.
[00:06:34.460 --> 00:06:38.660]   Was everything moving to the left between Roche-Pierre and 1830?
[00:06:38.660 --> 00:06:42.660]   I don't pretend to know, but it sure doesn't seem that way, right?
[00:06:42.660 --> 00:06:47.580]   So again, there seem to be a lot of cases where conquest's law is not so economical.
[00:06:47.580 --> 00:06:48.580]   —Right.
[00:06:48.580 --> 00:06:51.580]   Napoleon is a great example of this, where it's not sure whether he's the most left-wing
[00:06:51.580 --> 00:06:53.060]   figure in history or the most right-wing figure in history.
[00:06:53.060 --> 00:06:55.460]   —Or maybe both somehow, yes.
[00:06:55.460 --> 00:06:56.460]   —Okay.
[00:06:56.460 --> 00:06:59.860]   How much of talent, or the lack thereof, is a moral judgment for you?
[00:06:59.860 --> 00:07:04.420]   So just to give some context, when I think that somebody is not that intelligent, for
[00:07:04.420 --> 00:07:05.820]   me that doesn't seem like a moral judgment.
[00:07:05.820 --> 00:07:07.780]   That just seems like a lot of the lottery.
[00:07:07.780 --> 00:07:11.680]   When I say that somebody's not hardworking, for me that seems like more of a moral judgment.
[00:07:11.680 --> 00:07:15.180]   So on that spectrum, where would you say talent lies?
[00:07:15.180 --> 00:07:16.180]   —I don't know.
[00:07:16.180 --> 00:07:19.580]   My default is that most people aren't that ambitious.
[00:07:19.580 --> 00:07:21.180]   I'm fine with that.
[00:07:21.180 --> 00:07:24.620]   It actually creates some opportunities for the ambitious.
[00:07:24.620 --> 00:07:29.460]   There might be an optimal degree of ambition well short of everyone being sort of maximally
[00:07:29.460 --> 00:07:30.860]   ambitious.
[00:07:30.860 --> 00:07:36.580]   So I don't go around like pissed off at those people or judging them in some moralizing
[00:07:36.580 --> 00:07:38.340]   way.
[00:07:38.340 --> 00:07:43.700]   I think a lot of me is on automatic pilot in terms of kind of morally judging the people
[00:07:43.700 --> 00:07:44.700]   at the distance.
[00:07:44.700 --> 00:07:49.500]   I don't wake up in the morning and get pissed off at someone in the Middle East doing whatever,
[00:07:49.500 --> 00:07:51.540]   even though I might think it was wrong.
[00:07:51.540 --> 00:07:53.020]   So same with talent.
[00:07:53.020 --> 00:07:54.020]   —Okay.
[00:07:54.020 --> 00:07:55.020]   Okay.
[00:07:55.020 --> 00:07:59.300]   So when you read the biographies of great people, often you see there's a bit of emotional
[00:07:59.300 --> 00:08:01.140]   neglect and abuse when they're kids.
[00:08:01.140 --> 00:08:02.420]   Why do you think that is?
[00:08:02.420 --> 00:08:04.180]   Why is that such a common trope?
[00:08:04.180 --> 00:08:08.780]   —I would love to see the data, but I'm not convinced that it's more common than with
[00:08:08.780 --> 00:08:10.100]   other people.
[00:08:10.100 --> 00:08:14.580]   So famous people, especially those who have biographies, on average they're from earlier
[00:08:14.580 --> 00:08:16.580]   times, right?
[00:08:16.580 --> 00:08:20.380]   And in earlier times, children were treated worse.
[00:08:20.380 --> 00:08:23.100]   So it could be correlated without being causal.
[00:08:23.100 --> 00:08:27.740]   Now maybe there's this notion, well, you need to have something to prove, and you only feel
[00:08:27.740 --> 00:08:31.940]   you need to prove something, you're Napoleon, you're short, you weren't always treated well.
[00:08:31.940 --> 00:08:34.460]   Like, that's possible, I don't rule it out.
[00:08:34.460 --> 00:08:38.780]   But you look at your Bill Gateses, your Mark Zuckerbergs, without pretending to know what
[00:08:38.780 --> 00:08:43.300]   their childhoods were like, it sure sounds like they were upper-middle-class kids treated
[00:08:43.300 --> 00:08:47.100]   very well, at least from a distance.
[00:08:47.100 --> 00:08:48.100]   —Yes.
[00:08:48.100 --> 00:08:50.100]   Is there anything— —You know, the Collisons, like, they have
[00:08:50.100 --> 00:08:51.100]   great parents.
[00:08:51.100 --> 00:08:54.100]   They did well, you know?
[00:08:54.100 --> 00:08:57.300]   —It could just be that the examples that stick out in my mind of emotional neglect
[00:08:57.300 --> 00:08:59.700]   stick out in my mind particularly because— —And they stick out of the line, yes.
[00:08:59.700 --> 00:09:01.260]   So I'd really like to see the data.
[00:09:01.260 --> 00:09:04.380]   I think it's, like, an important, very good question.
[00:09:04.380 --> 00:09:09.460]   It seems to me maybe one could investigate it, but I've never seen an actual result.
[00:09:09.460 --> 00:09:13.820]   —Is there something you've learned about talent spotting through writing the book that
[00:09:13.820 --> 00:09:14.820]   you wish wasn't so?
[00:09:14.820 --> 00:09:18.260]   That you find it disturbing or you find it disappointing in some way?
[00:09:18.260 --> 00:09:20.820]   For example, something as a correlate for talent that you wish wasn't, or something
[00:09:20.820 --> 00:09:21.820]   like that?
[00:09:21.820 --> 00:09:22.820]   —I don't know.
[00:09:23.380 --> 00:09:28.340]   Again, I think I'm relatively accepting of a lot of these realities, but the thing that
[00:09:28.340 --> 00:09:32.100]   disappoints me a bit is how geographically clustered talent is.
[00:09:32.100 --> 00:09:35.180]   And I don't mean where it was born, or I don't mean ethnically.
[00:09:35.180 --> 00:09:37.680]   I just mean where it ends up.
[00:09:37.680 --> 00:09:44.980]   So if you get an application, say, from rural Italy, where maybe living standards are perfectly
[00:09:44.980 --> 00:09:45.980]   fine.
[00:09:45.980 --> 00:09:51.340]   There's weather, there's olive oil, there's pasta, but it's just probably not that good.
[00:09:51.340 --> 00:09:56.240]   And certainly Italians have had enough amazing achievements over the millennia, but right
[00:09:56.240 --> 00:10:01.780]   now the people there who are up to something, they're going to move to London, to New York,
[00:10:01.780 --> 00:10:02.780]   to somewhere.
[00:10:02.780 --> 00:10:04.740]   So I find that a bit depressing.
[00:10:04.740 --> 00:10:07.340]   It's not about really the people, but...
[00:10:07.340 --> 00:10:11.720]   —To what extent is, when you do find a cluster of talent in a place, to what extent can that
[00:10:11.720 --> 00:10:14.980]   be explained by sort of like a cyclical view of what's happening in the region, that in
[00:10:14.980 --> 00:10:16.900]   this sort of hard times create strong men's sense?
[00:10:16.900 --> 00:10:23.300]   So at some point, Italy had a renaissance, and then things maybe get complacent over
[00:10:23.300 --> 00:10:24.300]   time.
[00:10:24.300 --> 00:10:29.280]   —Again, maybe that's true for Italy, but most of the talent clusters have been such
[00:10:29.280 --> 00:10:30.580]   for a long time.
[00:10:30.580 --> 00:10:33.100]   So London, New York, it's not cyclical.
[00:10:33.100 --> 00:10:35.660]   They've just had a ton of talent for a very long time.
[00:10:35.660 --> 00:10:36.660]   They still do.
[00:10:36.660 --> 00:10:38.420]   Later on, they still will.
[00:10:38.420 --> 00:10:41.780]   Not literally forever, but it seems like an enduring effect.
[00:10:41.780 --> 00:10:46.020]   —But if they leave, for example, with like Central European Jews, right, then they don't
[00:10:46.020 --> 00:10:47.020]   stay there anymore.
[00:10:47.020 --> 00:10:48.020]   They leave.
[00:10:48.020 --> 00:10:50.660]   —A big war obviously can destroy almost anything.
[00:10:50.660 --> 00:10:56.060]   So German scientific talent took a big whack, German cultural talent, you know, Hungarian
[00:10:56.060 --> 00:10:57.860]   Jews and mathematics.
[00:10:57.860 --> 00:11:01.660]   I don't know how much of a thing it still is, but certainly it's nothing close to what
[00:11:01.660 --> 00:11:02.660]   it once was.
[00:11:02.660 --> 00:11:03.660]   —OK.
[00:11:03.660 --> 00:11:08.580]   So then I was worried that if you realize that some particular region has a lot of talent
[00:11:08.580 --> 00:11:12.820]   right now, then that might be like a one-time gain that you realize, oh, India or Toronto
[00:11:12.820 --> 00:11:15.640]   or Nigeria or something, they have a lot of talent.
[00:11:15.640 --> 00:11:20.860]   But that the culture doesn't persist in some sort of, in an extended way.
[00:11:20.860 --> 00:11:25.540]   —I think that might be true for where talent comes from, but where it goes to, it just
[00:11:25.540 --> 00:11:27.540]   seems to show persistence.
[00:11:27.540 --> 00:11:31.620]   So people will be going to London for centuries, almost certainly.
[00:11:31.620 --> 00:11:34.140]   Is London producing a lot of talent?
[00:11:34.140 --> 00:11:35.760]   That's less clear.
[00:11:35.760 --> 00:11:37.100]   That may be much more cyclical.
[00:11:37.100 --> 00:11:40.420]   So the 17th century in London is amazing, right?
[00:11:40.420 --> 00:11:41.780]   John Donne.
[00:11:41.780 --> 00:11:42.780]   London today?
[00:11:42.800 --> 00:11:48.760]   I would say I don't know, but it's not obvious it's coming close to its previous glories.
[00:11:48.760 --> 00:11:55.080]   So the current status of India, I think that will be temporary, but temporary for a long
[00:11:55.080 --> 00:11:56.080]   time.
[00:11:56.080 --> 00:11:57.080]   It's just a very big place.
[00:11:57.080 --> 00:11:58.820]   It has a lot of centers.
[00:11:58.820 --> 00:12:04.200]   The things it has going for it, like not taking prosperity for granted, it will have for quite
[00:12:04.200 --> 00:12:05.200]   a while.
[00:12:05.200 --> 00:12:07.000]   Like India is still pretty poor.
[00:12:07.000 --> 00:12:10.840]   —What do you think makes certain places the actual place where the clusters of talent
[00:12:10.840 --> 00:12:15.000]   congregate, and places that are just a source of that talent?
[00:12:15.000 --> 00:12:18.360]   What makes something a source rather—sorry, a sink rather than the source of talent?
[00:12:18.360 --> 00:12:23.240]   —I think what is a place where people go is more or less obvious.
[00:12:23.240 --> 00:12:24.240]   You need money.
[00:12:24.240 --> 00:12:25.640]   You need a big city.
[00:12:25.640 --> 00:12:29.000]   You need some kind of common trade or linguistic connection.
[00:12:29.000 --> 00:12:33.760]   So New York, London, they are what they are for sort of obvious reasons, right?
[00:12:33.760 --> 00:12:38.040]   Path dependence, history, you make it in the Big Apple, and so on.
[00:12:38.040 --> 00:12:43.520]   But where people come from, I think theory is very bad at understanding.
[00:12:43.520 --> 00:12:49.600]   Like why did the Renaissance blossom in Florence and Venice and not in Milan?
[00:12:49.600 --> 00:12:54.020]   If you're going back earlier, it wasn't obvious it would be those places.
[00:12:54.020 --> 00:12:58.080]   I've done a lot of reading to try to figure this out, but I find I've gotten remarkably
[00:12:58.080 --> 00:13:00.520]   not far on the question.
[00:13:00.520 --> 00:13:04.320]   —The particular examples you mentioned today, like New York, San Francisco, London, these
[00:13:04.320 --> 00:13:08.920]   places, like they have a high fixed cost if you want to move there, it's expensive.
[00:13:08.920 --> 00:13:13.560]   Do you think that this is—they've been sinks of talent despite this fact or because of
[00:13:13.560 --> 00:13:14.560]   it?
[00:13:14.560 --> 00:13:16.960]   That you need some sort of exclusion in order to be a haven of talent?
[00:13:16.960 --> 00:13:19.520]   —Well, I think this is a problem for San Francisco.
[00:13:19.520 --> 00:13:23.200]   It may be a more temporary cluster than it ought to have been.
[00:13:23.200 --> 00:13:27.240]   And since it's a pretty recent cluster, it can't count on the same kind of historical
[00:13:27.240 --> 00:13:30.360]   path dependence that, say, New York and Manhattan have.
[00:13:30.360 --> 00:13:33.760]   But a lot of New York still is not that expensive.
[00:13:33.760 --> 00:13:35.520]   Look at the people who work and live there.
[00:13:35.520 --> 00:13:37.720]   They're not all rich, to say the least.
[00:13:37.720 --> 00:13:41.120]   And that is an important part of why New York is still New York.
[00:13:41.120 --> 00:13:47.720]   London, it's much harder, but it seems to me London is a sink for somewhat established
[00:13:47.720 --> 00:13:50.540]   talent, which is fine, right?
[00:13:50.540 --> 00:13:52.840]   But in that regard, much inferior to New York.
[00:13:52.840 --> 00:13:56.680]   —Okay, I want to play a game of overrated and underrated with you.
[00:13:56.680 --> 00:13:57.680]   —I'm here for that.
[00:13:57.680 --> 00:14:00.600]   —But we're going to do it with certain traits or certain kinds of personalities that might
[00:14:00.600 --> 00:14:02.400]   come in when you're interviewing them.
[00:14:02.400 --> 00:14:05.320]   —Okay, but it's probably all going to be indeterminate, but go on.
[00:14:05.320 --> 00:14:06.320]   —Right.
[00:14:06.320 --> 00:14:07.320]   Okay.
[00:14:07.320 --> 00:14:08.320]   So somebody comes in, they're very humble.
[00:14:08.320 --> 00:14:09.320]   —Yes.
[00:14:09.320 --> 00:14:10.320]   —Well, is it—
[00:14:10.320 --> 00:14:11.320]   —Immediately, I'm suspicious.
[00:14:11.320 --> 00:14:12.320]   —Why?
[00:14:12.320 --> 00:14:16.600]   —I figure most people who are going to make something of themselves are arrogant.
[00:14:16.600 --> 00:14:20.280]   If they're willing to show it, there's a certain bravery or openness in that.
[00:14:20.280 --> 00:14:22.560]   I don't rule out the humble person doing great.
[00:14:22.560 --> 00:14:26.960]   A lot of people who do great are humble, but I just get a wee bit like, "Hmm, what's up
[00:14:26.960 --> 00:14:27.960]   with you?
[00:14:27.960 --> 00:14:29.840]   You're not really humble, are you?"
[00:14:29.840 --> 00:14:33.880]   —Oh, so maybe the humility is a way of avoiding confrontation.
[00:14:33.880 --> 00:14:36.280]   You don't have the confidence to actually show that you can be great, so you're just—
[00:14:36.280 --> 00:14:37.280]   —Right.
[00:14:37.280 --> 00:14:40.920]   Now, it might be efficient for them to avoid confrontation, but I would just say I start
[00:14:40.920 --> 00:14:43.240]   thinking, "I don't know the real story."
[00:14:43.240 --> 00:14:48.400]   When I see a bit of arrogance, I'm less likely to think—it may in a way be feigned, but
[00:14:48.400 --> 00:14:52.080]   the feigning of arrogance is itself a kind of arrogance, and in that sense, I'm still
[00:14:52.080 --> 00:14:53.520]   getting the genuine thing.
[00:14:53.520 --> 00:14:57.920]   —What is the difference—let's say a 15-year-old is kind of arrogant versus a 50-year-old is
[00:14:57.920 --> 00:15:01.280]   kind of arrogant, and the latter has accomplishments already, the first one doesn't.
[00:15:01.280 --> 00:15:03.680]   Is there a difference in how you perceive humility or the lack thereof?
[00:15:03.680 --> 00:15:04.680]   —Oh, sure.
[00:15:04.680 --> 00:15:08.920]   The 50-year-old, you want to see what have they done, and you're much more likely to
[00:15:08.920 --> 00:15:13.320]   think the 50-year-old should feign humility than the 15-year-old, because that's the high
[00:15:13.320 --> 00:15:15.880]   status thing to do, is to feign humility.
[00:15:15.880 --> 00:15:19.080]   If they can't do that, you figure, "Well, here's one thing they're bad at.
[00:15:19.080 --> 00:15:20.320]   What else are they bad at?"
[00:15:20.320 --> 00:15:24.560]   Whereas the 15-year-old, they have a chip on their shoulder, can't quite hold it all
[00:15:24.560 --> 00:15:25.560]   in.
[00:15:25.560 --> 00:15:26.560]   You're like, "Oh, that's great.
[00:15:26.560 --> 00:15:27.680]   Let's see what you're going to do."
[00:15:27.680 --> 00:15:29.320]   —And in how arrogant can you be?
[00:15:29.320 --> 00:15:32.720]   So there's many 15-year-olds, if they're really mad, they're like, "I want to solve P does
[00:15:32.720 --> 00:15:35.840]   not equal NP," or, "I want to build an AGI," or something.
[00:15:35.840 --> 00:15:38.320]   Is there some level where you're just like, you clearly don't understand what's going
[00:15:38.320 --> 00:15:41.360]   on if you think you can do that, or is it always a plus?
[00:15:41.360 --> 00:15:43.120]   —I haven't seen the level yet.
[00:15:43.120 --> 00:15:47.960]   Now, if a 15-year-old said to me, "In three years, I'm going to invent a perpetual motion
[00:15:47.960 --> 00:15:51.440]   machine," I would think like, "No, you're just crazy."
[00:15:51.440 --> 00:15:52.920]   But no one's ever said that to me.
[00:15:52.920 --> 00:15:57.400]   You know, there's the famous Mark Zuckerberg story where he went into the VC meeting at
[00:15:57.400 --> 00:16:01.760]   Sequoia wearing his pajamas, and he told Sequoia not to give him money.
[00:16:01.760 --> 00:16:04.600]   I think he was 18 then.
[00:16:04.600 --> 00:16:09.620]   That's pretty far out on the arrogant behavior spectrum, and we should be fine with that.
[00:16:09.620 --> 00:16:11.280]   We know how the story ends.
[00:16:11.280 --> 00:16:17.160]   So it's really hard to be too arrogant, but once you say this, this second-order effect,
[00:16:17.160 --> 00:16:20.260]   you start thinking, "Well, are they just being arrogant as an act?"
[00:16:20.260 --> 00:16:23.540]   And then in the act sense, yes, they can be too arrogant.
[00:16:23.540 --> 00:16:26.300]   —Isn't the back story there that he was friends with Sean Parker, and then Sean Parker
[00:16:26.300 --> 00:16:28.340]   had a beef with Sequoia, and then—
[00:16:28.340 --> 00:16:29.340]   —There's something like that.
[00:16:29.340 --> 00:16:34.460]   I wouldn't want to say off the top of my head exactly what, but there is a back story.
[00:16:34.460 --> 00:16:36.460]   —Somebody comes in professionally dressed when they didn't need to.
[00:16:36.460 --> 00:16:38.140]   So they've got a crisp, clean shirt.
[00:16:38.140 --> 00:16:39.140]   They've got a nice wash.
[00:16:39.140 --> 00:16:40.140]   What do you think?
[00:16:40.140 --> 00:16:41.140]   —How old are they?
[00:16:41.140 --> 00:16:42.140]   —20.
[00:16:42.140 --> 00:16:43.300]   —They're too conformist.
[00:16:43.300 --> 00:16:47.580]   Again, some jobs conformities great, but I get a little suspicious, at least for what
[00:16:47.580 --> 00:16:49.500]   I'm looking for.
[00:16:49.500 --> 00:16:50.500]   But I wouldn't rule them out.
[00:16:50.500 --> 00:16:52.940]   For a lot of things, it's a plus, right?
[00:16:52.940 --> 00:16:56.700]   —Is there a point, though, where you're in some way being conformist by dressing up
[00:16:56.700 --> 00:16:59.420]   in a polo shirt, like if you were in San Francisco, right?
[00:16:59.420 --> 00:17:02.700]   Now, it seems like the conformist thing is not to wear a suit to an interview if you're
[00:17:02.700 --> 00:17:03.700]   trying to be a software engineer.
[00:17:03.700 --> 00:17:08.300]   —Yeah, there might be situations where it's so weird, so over-the-top, so conformist that
[00:17:08.300 --> 00:17:12.020]   it's actually total non-conformism, like, "I don't know anyone who's a conformist like
[00:17:12.020 --> 00:17:13.020]   you are."
[00:17:13.020 --> 00:17:15.580]   And you go, "Maybe they're not being a conformist, right?
[00:17:15.580 --> 00:17:17.140]   They're just some kind of nut."
[00:17:17.140 --> 00:17:19.180]   And then you're interested again.
[00:17:19.180 --> 00:17:23.620]   —An overall sense that you get from the person that they're really content, almost
[00:17:23.620 --> 00:17:28.580]   like if Buddha came in for an interview, like a sense of well-being.
[00:17:28.580 --> 00:17:29.980]   —It's going to depend on context.
[00:17:29.980 --> 00:17:35.620]   I don't think I'd hold it against someone, but I wouldn't take it at face value.
[00:17:35.620 --> 00:17:40.500]   You figure they're antsy in some way, you hope, and, like, you'll see it with more time,
[00:17:40.500 --> 00:17:43.240]   I would just think.
[00:17:43.240 --> 00:17:47.020]   —Somebody who uses a lot of nootropics, so they're constantly using caffeine, but maybe
[00:17:47.020 --> 00:17:51.780]   on the side, like, multiple times a week, they're also using Adderall, modafinil, other
[00:17:51.780 --> 00:17:52.780]   kinds of nootropics.
[00:17:52.780 --> 00:17:56.980]   —I don't personally like it, but I've never seen the evidence that it's negatively correlated
[00:17:56.980 --> 00:18:00.220]   with success, so I would try to put it out of mind.
[00:18:00.220 --> 00:18:04.020]   I sort of personally get a queasy feeling, like, "Do you really know what you're doing?
[00:18:04.020 --> 00:18:05.700]   Is all this stuff good for you?
[00:18:05.700 --> 00:18:07.900]   Why do you need this?"
[00:18:07.900 --> 00:18:12.900]   That's like my actual reaction, but again, at the intellectual level, it does seem to
[00:18:12.900 --> 00:18:15.660]   work for some people, or at least not screw them up too much.
[00:18:15.660 --> 00:18:17.620]   —You don't drink caffeine, correct?
[00:18:17.620 --> 00:18:18.620]   —Zero.
[00:18:18.620 --> 00:18:19.620]   —Why?
[00:18:19.620 --> 00:18:20.620]   —I don't like it.
[00:18:20.620 --> 00:18:21.980]   And it might be bad for you.
[00:18:21.980 --> 00:18:22.980]   —Oh, really?
[00:18:22.980 --> 00:18:23.980]   You think so?
[00:18:23.980 --> 00:18:24.980]   —And people get addicted to it, also.
[00:18:24.980 --> 00:18:28.460]   —So you're not worried it might make you less productive over the long term?
[00:18:28.460 --> 00:18:30.780]   It's more about, you just don't want to be addicted to something?
[00:18:30.780 --> 00:18:35.660]   —Well, since I don't know it well, I'm not sure what my worries are, but the status quo
[00:18:35.660 --> 00:18:37.660]   regime seems to work.
[00:18:37.660 --> 00:18:43.420]   I observe a lot of people who end up addicted to coffee, Coke, soda, like, we know is bad
[00:18:43.420 --> 00:18:44.580]   for you.
[00:18:44.580 --> 00:18:46.700]   So like, what's the problem I need to solve?
[00:18:46.700 --> 00:18:47.700]   Why do it?
[00:18:47.700 --> 00:18:51.100]   —A history of mental illness, like depression or anxiety.
[00:18:51.100 --> 00:18:54.540]   Not that they're good, but at current margins, do you think that maybe they're punished too
[00:18:54.540 --> 00:18:56.700]   heavily or maybe that people don't take them seriously enough?
[00:18:56.700 --> 00:18:59.140]   That they're actually a bigger signal than people are considering?
[00:18:59.140 --> 00:19:00.140]   —I don't know.
[00:19:00.140 --> 00:19:01.220]   I mean, both could be true, right?
[00:19:01.220 --> 00:19:07.460]   So there's definitely positive correlations between that stuff and artistic creativity.
[00:19:07.460 --> 00:19:10.580]   Causal, harder to say, but correlates.
[00:19:10.580 --> 00:19:17.460]   So you certainly should take the person seriously, but, you know, are they the best Starbucks
[00:19:17.460 --> 00:19:18.460]   cashier?
[00:19:18.460 --> 00:19:20.380]   I don't know.
[00:19:20.380 --> 00:19:21.380]   —Yeah.
[00:19:21.380 --> 00:19:24.960]   In the book you, or I guess in other podcasts, you've pointed out that some of the most talented
[00:19:24.960 --> 00:19:28.380]   people you see that are neglected are 15 to 17-year-olds.
[00:19:28.380 --> 00:19:32.380]   How does this impact how you think, like, let's say you were in charge of a high school,
[00:19:32.380 --> 00:19:33.380]   right?
[00:19:33.380 --> 00:19:34.380]   You're the principal of a high school.
[00:19:34.380 --> 00:19:37.220]   You know that there's like 2,000 students there, a few of them have to be geniuses.
[00:19:37.220 --> 00:19:38.220]   —Right.
[00:19:38.220 --> 00:19:41.780]   —What, like, how is a high school run by Tyler Cowen, especially for the very smartest
[00:19:41.780 --> 00:19:42.780]   people there?
[00:19:42.780 --> 00:19:43.780]   How is it run?
[00:19:43.780 --> 00:19:44.780]   —Less homework.
[00:19:44.780 --> 00:19:45.780]   —Okay.
[00:19:45.780 --> 00:19:48.320]   —No, I would work harder to hire better teachers, pay them more, fire the bad ones
[00:19:48.320 --> 00:19:50.260]   if I'm allowed to do that.
[00:19:50.260 --> 00:19:51.580]   Those are no-brainers.
[00:19:51.580 --> 00:19:56.800]   But mainly less homework and have more people come in who are potential role models.
[00:19:56.800 --> 00:20:00.940]   So someone like me, I was invited once to a Flint Hill High School in Oakton.
[00:20:00.940 --> 00:20:02.140]   It's right nearby.
[00:20:02.140 --> 00:20:03.140]   I went in.
[00:20:03.140 --> 00:20:04.140]   I wasn't paid.
[00:20:04.140 --> 00:20:06.240]   I just figure I'll do this.
[00:20:06.240 --> 00:20:10.140]   It seems to me a lot of high schools don't even try, and they could get a bunch of people
[00:20:10.140 --> 00:20:13.180]   to come in for free and just say, "I'm an economist.
[00:20:13.180 --> 00:20:15.140]   Here's what being an economist is like."
[00:20:15.140 --> 00:20:17.380]   You know, 45 minutes.
[00:20:17.380 --> 00:20:20.300]   Is that so much worse than the BS the teacher has to spew?
[00:20:20.300 --> 00:20:21.300]   Of course not.
[00:20:21.300 --> 00:20:22.900]   So I would just do more things like that.
[00:20:22.900 --> 00:20:25.420]   —I want to understand the difference between these three options.
[00:20:25.420 --> 00:20:29.100]   So one is somebody like you actually gives an in-person lecture, like, saying what this
[00:20:29.100 --> 00:20:30.100]   life is like.
[00:20:30.100 --> 00:20:31.100]   —Yeah, or Zoom.
[00:20:31.100 --> 00:20:32.100]   You could do Zoom, too.
[00:20:32.100 --> 00:20:33.100]   —Sure.
[00:20:33.100 --> 00:20:34.100]   That's actually just about tasks.
[00:20:34.100 --> 00:20:35.100]   The second is Zoom.
[00:20:35.100 --> 00:20:36.360]   You don't have it in any way whatsoever.
[00:20:36.360 --> 00:20:38.520]   You're just kind of, like, maybe showing a video of the person.
[00:20:38.520 --> 00:20:40.320]   —I'm a big believer in the vividness.
[00:20:40.320 --> 00:20:44.400]   So Zoom is better than nothing, and a lot of people are at a distance.
[00:20:44.400 --> 00:20:49.280]   But I think you'll get more and better responses inviting people to do it live.
[00:20:49.280 --> 00:20:50.280]   Local people.
[00:20:50.280 --> 00:20:54.740]   And there's plenty of local people where most of the good high schools are.
[00:20:54.740 --> 00:20:59.200]   —Are you tempted to just give these really smart 15-year-olds a hall pass to the library
[00:20:59.200 --> 00:21:01.840]   all day and some Wi-Fi access and then just leave them alone?
[00:21:01.840 --> 00:21:03.520]   Or do you think that they need some sort of structure?
[00:21:03.520 --> 00:21:07.820]   —I think they need some structure, but you have to let them rebel against it and do their
[00:21:07.820 --> 00:21:09.260]   own thing also.
[00:21:09.260 --> 00:21:14.820]   Zero structure strikes me as great for a few of them, but even for the super-talented ones,
[00:21:14.820 --> 00:21:15.820]   it's not perfect.
[00:21:15.820 --> 00:21:19.100]   They need exposure to things, and they need some teachers as role models.
[00:21:19.100 --> 00:21:22.460]   So you want them to have some structure.
[00:21:22.460 --> 00:21:26.860]   —And if you read old books about education, there's a strong emphasis on moral instruction.
[00:21:26.860 --> 00:21:31.220]   Do you think that needs to be an important part of education, or is that just...?
[00:21:31.220 --> 00:21:35.120]   —I'd like to see more data, but I suspect the best moral instruction is the teachers
[00:21:35.120 --> 00:21:37.300]   actually being good people.
[00:21:37.300 --> 00:21:40.460]   And I think that works, but again, I'd like to see the data.
[00:21:40.460 --> 00:21:44.600]   But that somehow you get up and lecture them about the seven virtues or something seems
[00:21:44.600 --> 00:21:47.820]   to me a waste of time and maybe even counterproductive.
[00:21:47.820 --> 00:21:52.360]   —Now, the way I read your book about talent, it seems like also a critique of Brian's book,
[00:21:52.360 --> 00:21:53.920]   The Case Against Education.
[00:21:53.920 --> 00:21:54.920]   —Of course it is.
[00:21:54.920 --> 00:21:55.920]   —Okay.
[00:21:55.920 --> 00:21:58.680]   —Brian, like, describes me as the guy who's always torturing him.
[00:21:58.680 --> 00:22:00.960]   And in a sense, he's right.
[00:22:00.960 --> 00:22:06.040]   —Well, I guess more specifically, if you don't need—it seems that Brian's book relies
[00:22:06.040 --> 00:22:10.760]   on the argument that you need a costly signal to show that you have talent or, you know,
[00:22:10.760 --> 00:22:13.440]   you have intelligence and other conscientious and other traits.
[00:22:13.440 --> 00:22:18.040]   But if you can just learn that from a 1,500-word essay and a Zoom call, then maybe the college
[00:22:18.040 --> 00:22:19.040]   is not about the signal.
[00:22:19.040 --> 00:22:22.840]   —Well, in that sense, I'm not sure it's a good critique of Brian.
[00:22:22.840 --> 00:22:28.040]   So for most people in the middle of the distribution, I don't think you can learn what I learned
[00:22:28.040 --> 00:22:33.600]   from, say, top EV winners through a written application and a half-hour Zoom call.
[00:22:33.600 --> 00:22:38.320]   But that said, I think the talent book shows you, you know, my old saying, "Context is
[00:22:38.320 --> 00:22:39.520]   that which is scarce."
[00:22:39.520 --> 00:22:43.800]   And you're always testing people for their understanding of context.
[00:22:43.800 --> 00:22:48.200]   And most people need a fair amount of higher education to acquire that context, even if
[00:22:48.200 --> 00:22:52.040]   they don't remember the detailed content of their classes.
[00:22:52.040 --> 00:22:57.140]   So I think Brian overlooks how much people actually learn when they go to school.
[00:22:57.140 --> 00:23:02.400]   —How would you go about measuring the amount of context somebody is getting in college?
[00:23:02.400 --> 00:23:05.000]   Is there, like, something you can point to that says, "Oh, clearly they're getting some
[00:23:05.000 --> 00:23:06.000]   context.
[00:23:06.000 --> 00:23:07.000]   Otherwise, they wouldn't be able to do this."
[00:23:07.000 --> 00:23:10.120]   —Well, I think if you meet enough people who did and didn't go to college, you'll see
[00:23:10.120 --> 00:23:12.680]   the difference on average.
[00:23:12.680 --> 00:23:13.840]   Stressing the word "average."
[00:23:13.840 --> 00:23:17.600]   Now, there are papers measuring positive returns to higher ed.
[00:23:17.600 --> 00:23:20.520]   I don't think they at all show it's due to context.
[00:23:20.520 --> 00:23:24.840]   But I am persuaded by most of Brian's arguments that you don't remember the details of what
[00:23:24.840 --> 00:23:25.840]   you learned in class.
[00:23:25.840 --> 00:23:26.840]   —Right.
[00:23:26.840 --> 00:23:31.520]   —It's like, "Oh, you learned, you know, this about astronomy and Kepler's laws and
[00:23:31.520 --> 00:23:32.760]   opportunity cost.
[00:23:32.760 --> 00:23:35.160]   People can't reproduce that two or three years later."
[00:23:35.160 --> 00:23:37.620]   It seems pretty clear we know that, right?
[00:23:37.620 --> 00:23:42.360]   But they do learn a lot of context and networking and how to deal with different personality
[00:23:42.360 --> 00:23:43.360]   types.
[00:23:43.360 --> 00:23:46.360]   —Like, how would you falsify this claim, though, that you are getting a lot of context?
[00:23:46.360 --> 00:23:49.280]   Is it just something that you had to qualitatively evaluate or is there some way that you could
[00:23:49.280 --> 00:23:53.840]   say — like, what would have to be true of the world for you to conclude that the opposite
[00:23:53.840 --> 00:23:54.840]   is true?
[00:23:54.840 --> 00:23:57.960]   —Well, if you could show people remembered a lot of the facts they learned and those
[00:23:57.960 --> 00:24:02.200]   facts were important for their jobs, neither of which I think is true, but in principle
[00:24:02.200 --> 00:24:06.580]   they're demonstrable, then you would be much more skeptical about the context being the
[00:24:06.580 --> 00:24:08.040]   thing that mattered.
[00:24:08.040 --> 00:24:12.920]   But as it stands now, that's the residual and it's probably what matters.
[00:24:12.920 --> 00:24:13.920]   —Right.
[00:24:13.920 --> 00:24:17.000]   So I thought that Brian showed in the book that actually people don't even remember many
[00:24:17.000 --> 00:24:18.640]   of the basic facts that they learned in school.
[00:24:18.640 --> 00:24:19.640]   —Of course they don't.
[00:24:19.640 --> 00:24:21.440]   But that's not the main thing they learn.
[00:24:21.440 --> 00:24:26.080]   They learn some vision of how the world works, how they fit into it, that they ought to have
[00:24:26.080 --> 00:24:30.160]   higher aspirations, that they can join the upper middle class, that they're supposed
[00:24:30.160 --> 00:24:33.600]   to have a particular kind of job, here are the kinds of jerks you're going to meet along
[00:24:33.600 --> 00:24:38.120]   the way, here's some sense of how dating markets work, maybe you're in a fraternity, maybe
[00:24:38.120 --> 00:24:39.920]   you do a sport, and so on.
[00:24:39.920 --> 00:24:41.760]   That's what you learn.
[00:24:41.760 --> 00:24:44.020]   —How did you spot Brian?
[00:24:44.020 --> 00:24:50.320]   —He was maybe in high school even when I met him, and it was at some kind of IHS event.
[00:24:50.320 --> 00:24:52.720]   And I think he made a point of seeking me out.
[00:24:52.720 --> 00:24:55.200]   And I immediately thought, well, this guy is going to be something.
[00:24:55.200 --> 00:24:57.260]   Like, got to keep track of this guy.
[00:24:57.260 --> 00:24:58.260]   Right away.
[00:24:58.260 --> 00:24:59.260]   —Say more.
[00:24:59.260 --> 00:25:00.960]   Like, what happened?
[00:25:00.960 --> 00:25:07.120]   —His level of enthusiasm, ability to speak with respect to detail, and just kind of bursting
[00:25:07.120 --> 00:25:11.080]   with everything was immediately evident, as it still is.
[00:25:11.080 --> 00:25:16.040]   Brian has changed less than almost anyone else I know over what is now, he could tell
[00:25:16.040 --> 00:25:19.000]   you how many years, but a whole bunch of decades.
[00:25:19.000 --> 00:25:22.880]   —So if that's the case, then it would have been interesting to meet somebody who is like
[00:25:22.880 --> 00:25:23.880]   Brian but like a 19-year-old.
[00:25:23.880 --> 00:25:27.120]   —Yeah, and I did.
[00:25:27.120 --> 00:25:28.120]   I was right.
[00:25:28.120 --> 00:25:29.120]   —Yeah.
[00:25:29.120 --> 00:25:34.560]   To what extent do the best talent scouts inevitably suffer from good hearts law?
[00:25:34.560 --> 00:25:38.080]   Like, has it been happening to you where your approval gets turned into a credential, and
[00:25:38.080 --> 00:25:41.880]   so a whole bunch of non-earnest people start applying, you get a whole bunch of adverse
[00:25:41.880 --> 00:25:46.040]   selection, and then it becomes hard for you to run your program?
[00:25:46.040 --> 00:25:49.320]   —It is not yet hard to run the program.
[00:25:49.320 --> 00:25:52.100]   If I needed to, I would just shut down applications.
[00:25:52.100 --> 00:25:58.300]   I've seen a modest uptick in bad applications, but it takes so little time to decide they're
[00:25:58.300 --> 00:26:03.480]   no good, or just not a good fit for us, that it's not a problem.
[00:26:03.480 --> 00:26:08.200]   So the endorsement does get credentialized, mostly that's a good thing, right?
[00:26:08.200 --> 00:26:14.000]   Like you help the people you pick, and then you see what happens next, and you keep on
[00:26:14.000 --> 00:26:16.840]   innovating as you need to.
[00:26:16.840 --> 00:26:21.040]   —You say in the book, the super-talented are best at spotting other super-talented
[00:26:21.040 --> 00:26:26.560]   individuals, and there aren't many of the super-talented talent spotters to go around.
[00:26:26.560 --> 00:26:30.200]   So this sounds like it's saying that if you're not super-talented, much of the book will
[00:26:30.200 --> 00:26:32.160]   maybe not do you so much good.
[00:26:32.160 --> 00:26:34.720]   Results may vary, should be maybe on the title.
[00:26:34.720 --> 00:26:38.320]   How much of talent spotting can be done by people who aren't themselves super-talented?
[00:26:38.320 --> 00:26:43.120]   —Well, I'd want to see the context of what I wrote, but I'm well aware of the fact that,
[00:26:43.120 --> 00:26:47.960]   say in basketball, most of the greatest general managers were not great players.
[00:26:47.960 --> 00:26:49.480]   Some were, like Jerry West, right?
[00:26:49.480 --> 00:26:51.600]   But say Pat Riley was not.
[00:26:51.600 --> 00:26:55.120]   So again, that's something you could study, but I don't in general think that the best
[00:26:55.120 --> 00:26:58.520]   talent scouts are themselves super-talented.
[00:26:58.520 --> 00:27:02.760]   —Then what is the skill in particular that they have, if it's not the particular thing
[00:27:02.760 --> 00:27:05.000]   that they're working on?
[00:27:05.000 --> 00:27:10.120]   —Some intangible kind of intuition, where they feel the right thing in the people they
[00:27:10.120 --> 00:27:15.520]   meet, and we try to teach people that intuition the same way you might teach art or music
[00:27:15.520 --> 00:27:16.520]   appreciation.
[00:27:16.520 --> 00:27:18.160]   But it's not a science, right?
[00:27:18.160 --> 00:27:21.160]   It's not paint by numbers.
[00:27:21.160 --> 00:27:24.080]   —Even with all the advice in the book, and even with the stuff that isn't in the book,
[00:27:24.080 --> 00:27:29.640]   that is just your inarticulable knowledge about how to spot talent, all your intuitions,
[00:27:29.640 --> 00:27:35.960]   how much of the variance in somebody's true potential is just fundamentally unpredictable
[00:27:35.960 --> 00:27:40.280]   and is just too chaotic of a thing to actually get your grips on.
[00:27:40.280 --> 00:27:42.080]   —I think it will always be an art.
[00:27:42.080 --> 00:27:47.440]   And if you look at the success rates of VCs, it depends what you count as the pool they're
[00:27:47.440 --> 00:27:52.080]   drawing from, but their overall rate at picking winners is not that impressive.
[00:27:52.080 --> 00:27:55.720]   And they're super high stakes, they're super smart.
[00:27:55.720 --> 00:27:59.040]   So I think it will mostly remain an art, not a science.
[00:27:59.040 --> 00:28:01.560]   So people say, "Oh, genomics this, genomics that."
[00:28:01.560 --> 00:28:02.560]   We'll see.
[00:28:02.560 --> 00:28:04.000]   Somehow I don't think that will change this.
[00:28:04.000 --> 00:28:08.040]   —So you don't think getting a polygenic risk score of, I don't know, a drive, that's
[00:28:08.040 --> 00:28:09.600]   going to be a thing that happens?
[00:28:09.600 --> 00:28:13.880]   —Maybe future genomics will be so different from what we have now, maybe, but it's not
[00:28:13.880 --> 00:28:14.880]   around the corner.
[00:28:14.880 --> 00:28:15.880]   —Yeah.
[00:28:15.880 --> 00:28:16.880]   Yeah.
[00:28:16.880 --> 00:28:20.280]   Maybe the sample size is just so low on somebody like you that how are you even going to collect
[00:28:20.280 --> 00:28:21.280]   that data?
[00:28:21.280 --> 00:28:24.240]   Like, how are you going to get the correlates of who the super-talented people are?
[00:28:24.240 --> 00:28:29.880]   —Yeah, and how genomic data interact with each other, you can apply machine learning
[00:28:29.880 --> 00:28:30.880]   and so on.
[00:28:30.880 --> 00:28:32.760]   It just seems quite murky.
[00:28:32.760 --> 00:28:36.120]   —If the best people get spotted earlier, and you can tell who is a 10x engineer at
[00:28:36.120 --> 00:28:40.400]   your company and who is only a 1x engineer or a 0.5x engineer, doesn't that mean that
[00:28:40.400 --> 00:28:43.000]   inequality in a way that will get worse?
[00:28:43.000 --> 00:28:46.880]   Because now the 10x engineer knows that they're 10x and everybody else knows that they're
[00:28:46.880 --> 00:28:47.880]   10x.
[00:28:47.880 --> 00:28:49.920]   They're not going to be willing to cross-subsidize your other employees.
[00:28:49.920 --> 00:28:53.320]   They're going to be wanting to get paid proportionate to their skill.
[00:28:53.320 --> 00:28:56.840]   —Well, they might be paid more, but they'll also innovate more, right?
[00:28:56.840 --> 00:29:00.200]   So they'll create more benefits for people who are doing nothing.
[00:29:00.200 --> 00:29:05.480]   So my intuition is that overall inequality of well-being will go down, but you can't
[00:29:05.480 --> 00:29:07.440]   say that's true a priori.
[00:29:07.440 --> 00:29:09.000]   Inequality of income might go up, right?
[00:29:09.000 --> 00:29:12.920]   —And then, but will the slack in the system go away for people who are not top performers?
[00:29:12.920 --> 00:29:15.120]   Like, as you can tell now, if we're getting better talent spotting?
[00:29:15.120 --> 00:29:19.440]   —I think a lot of this has happened already in contemporary America, as I wrote in "Average
[00:29:19.440 --> 00:29:25.560]   is Over," and not due to super-sophisticated talent spotting, though sometimes, but simply
[00:29:25.560 --> 00:29:30.280]   the fact that a lot of service sectors, you can measure output reasonably directly.
[00:29:30.280 --> 00:29:33.000]   Like, did you finish the computer program, right?
[00:29:33.000 --> 00:29:34.720]   Did it work?
[00:29:34.720 --> 00:29:40.560]   And that has made it harder for people to get paid things they don't deserve.
[00:29:40.560 --> 00:29:45.440]   —I wonder if this leads to adverse selection in the areas where you can't measure how well
[00:29:45.440 --> 00:29:46.440]   somebody's doing.
[00:29:46.440 --> 00:29:49.720]   So the people who are kind of lazy and bums, they'll just go in places where output can't
[00:29:49.720 --> 00:29:53.440]   be measured, and then so these industries will just be overflowing with the people who
[00:29:53.440 --> 00:29:54.440]   didn't want to work in measurable areas.
[00:29:54.440 --> 00:29:55.440]   —Oh, absolutely, yes.
[00:29:55.440 --> 00:29:59.080]   Even the people who are talented in those sectors, maybe they'll even start their own
[00:29:59.080 --> 00:30:04.920]   companies and earn through equity, and no one is really ever measuring their labor power,
[00:30:04.920 --> 00:30:08.800]   but still what they're doing is working, and they're making more from it.
[00:30:08.800 --> 00:30:14.520]   —If talent is partly heritable, then the better you get at spotting talent, over time
[00:30:14.520 --> 00:30:17.560]   will the social mobility in society go down?
[00:30:17.560 --> 00:30:19.360]   —Depends how you measure social mobility.
[00:30:19.360 --> 00:30:21.880]   So is it relative to the previous generation?
[00:30:21.880 --> 00:30:26.520]   I mean, most talent spotters don't know a lot about parents, like I don't know anything
[00:30:26.520 --> 00:30:29.440]   about your parents at all, right?
[00:30:29.440 --> 00:30:34.460]   And the other aspect of spotting talent is the talent you mobilize, you hope, does great
[00:30:34.460 --> 00:30:38.800]   things for people not doing anything at all, and that's a kind of automatic social mobility
[00:30:38.800 --> 00:30:39.800]   they get.
[00:30:39.800 --> 00:30:44.280]   But if you're measuring quintiles across generations, I don't know, the intuition could go either
[00:30:44.280 --> 00:30:47.600]   way.
[00:30:47.600 --> 00:30:50.840]   —But this goes back to wondering whether this is a one-time gain or not.
[00:30:50.840 --> 00:30:54.920]   So maybe initially they can help the people who are around them.
[00:30:54.920 --> 00:30:58.400]   If you find somebody in Brazil, they help the people around them.
[00:30:58.400 --> 00:31:01.320]   But once they're found, they're going to go to those clusters you talked about, and they're
[00:31:01.320 --> 00:31:04.120]   going to be helping the people in San Francisco who don't need help.
[00:31:04.120 --> 00:31:05.880]   So is this a one-time gain, then?
[00:31:05.880 --> 00:31:11.600]   —Well, so many people from India seem to give back to India in a very consistent way.
[00:31:11.600 --> 00:31:15.840]   People from Russia don't seem to do that, and that may relate to the fact that Russia
[00:31:15.840 --> 00:31:19.240]   is in terrible shape and India has a brighter future.
[00:31:19.240 --> 00:31:23.240]   So it will depend, but I certainly think there are ways of arranging things where people
[00:31:23.240 --> 00:31:24.240]   give back a lot.
[00:31:24.240 --> 00:31:27.240]   —Let's talk about emergent ventures.
[00:31:27.240 --> 00:31:33.560]   So I wonder if the goal of emergent ventures is to raise aspirations, does that still work
[00:31:33.560 --> 00:31:37.400]   given the fact that you have to accept some people but reject other people?
[00:31:37.400 --> 00:31:40.800]   So in Bayesian terms, the updates up have to equal the updates down.
[00:31:40.800 --> 00:31:45.440]   In some sense, you're almost transferring a vision from the excellent to the truly great.
[00:31:45.440 --> 00:31:47.440]   You see what I'm saying?
[00:31:47.440 --> 00:31:50.440]   —Well, you might discourage the people you turn away.
[00:31:50.440 --> 00:31:54.040]   But if they're really going to do something, they should take that as a challenge, and
[00:31:54.040 --> 00:31:55.040]   many do.
[00:31:55.040 --> 00:31:57.080]   Like, "Oh, I was rejected by Harvard.
[00:31:57.080 --> 00:32:01.240]   I had to go to UChicago, but then I decided I'm going to show those bastards."
[00:32:01.240 --> 00:32:03.640]   I think we talked about that a few minutes ago.
[00:32:03.640 --> 00:32:09.600]   So if I just crush the spirits of those who are rejected, I don't feel too bad about that.
[00:32:09.600 --> 00:32:14.400]   They should probably be in some role anyway where they're just working for someone.
[00:32:14.400 --> 00:32:18.640]   —Let me ask you then the converse of that, which is, if you do accept somebody, are you
[00:32:18.640 --> 00:32:22.580]   worried that if one of the things that drives people is getting rejected and then wanting
[00:32:22.580 --> 00:32:25.900]   to prove the people who rejected them wrong, are you worried that by accepting somebody
[00:32:25.900 --> 00:32:29.760]   when they're 15, you're killing that thing in them that wants to get some kind of approval?
[00:32:29.760 --> 00:32:32.040]   —Plenty of other people will still reject them, right?
[00:32:32.040 --> 00:32:36.760]   Now, if everyone accepts them every step of the way, maybe they're just awesome.
[00:32:36.760 --> 00:32:42.560]   LeBron James' basketball history passed a certain point, it just seems everyone wanted
[00:32:42.560 --> 00:32:46.240]   him, you know, for a bunch of decades now.
[00:32:46.240 --> 00:32:50.720]   I think deliberately with a lot of candidates, you shouldn't encourage them too much.
[00:32:50.720 --> 00:32:54.920]   And I make a point of chewing out a lot of people, just like light a fire under them,
[00:32:54.920 --> 00:32:57.760]   like, "What you're doing, it's not going to work."
[00:32:57.760 --> 00:33:00.480]   So I'm all for that, selectively, but yes.
[00:33:00.480 --> 00:33:01.480]   —Yes.
[00:33:01.480 --> 00:33:05.520]   Why do you think that so many of the people who have had emergent ventures, as you said,
[00:33:05.520 --> 00:33:09.320]   are interested in effective altruism?
[00:33:09.320 --> 00:33:14.040]   —There is like a moment right now for effective altruism, where it is the thing.
[00:33:14.040 --> 00:33:19.420]   Some of it is political polarization, like the main parties are so stupid and offensive.
[00:33:19.420 --> 00:33:21.440]   Those energies will go somewhere.
[00:33:21.440 --> 00:33:25.880]   Some of that in the 1970s, they maybe went to libertarianism.
[00:33:25.880 --> 00:33:27.960]   Libertarianism has been out there for too long.
[00:33:27.960 --> 00:33:31.840]   It doesn't seem to address a lot of current problems like climate change or pandemics
[00:33:31.840 --> 00:33:33.440]   very well.
[00:33:33.440 --> 00:33:37.280]   So where should it go?
[00:33:37.280 --> 00:33:38.640]   Rationality community gets some of it.
[00:33:38.640 --> 00:33:41.640]   It's related to EA, as I'm sure you know.
[00:33:41.640 --> 00:33:44.240]   Just like the tech startup community gets some of it.
[00:33:44.240 --> 00:33:45.280]   That's great.
[00:33:45.280 --> 00:33:47.120]   It seems to be working pretty well to me.
[00:33:47.120 --> 00:33:51.360]   Like, I'm not an EA person, but maybe they deserve a lot of it.
[00:33:51.360 --> 00:33:54.200]   —But you don't think it's persistent, like you think it comes and goes?
[00:33:54.200 --> 00:33:55.960]   —I think it will come and go.
[00:33:55.960 --> 00:33:58.640]   But I think EA will not vanish.
[00:33:58.640 --> 00:34:01.840]   Like libertarianism, it will continue for quite a long time.
[00:34:01.840 --> 00:34:05.080]   —Is there any movement that has attracted young people and that has been persistent
[00:34:05.080 --> 00:34:06.080]   over time?
[00:34:06.080 --> 00:34:07.080]   Or did they all fade?
[00:34:07.080 --> 00:34:08.960]   —Well, Christianity, right?
[00:34:08.960 --> 00:34:09.960]   Judaism.
[00:34:09.960 --> 00:34:10.960]   Islam.
[00:34:10.960 --> 00:34:12.800]   They're pretty persistent.
[00:34:12.800 --> 00:34:13.800]   —Right.
[00:34:13.800 --> 00:34:17.880]   So to the extent that being more religious makes you more persistent, then the criticism
[00:34:17.880 --> 00:34:21.520]   of EA, that it's kind of like a religion, can we view that as a plus?
[00:34:21.520 --> 00:34:22.520]   —Of course.
[00:34:22.520 --> 00:34:23.520]   Yeah.
[00:34:23.520 --> 00:34:24.520]   I think it's somewhat like a religion.
[00:34:24.520 --> 00:34:25.680]   To me, that's a plus.
[00:34:25.680 --> 00:34:26.960]   We need more religions.
[00:34:26.960 --> 00:34:31.720]   I wish more of the religions we needed were just flat-out religions.
[00:34:31.720 --> 00:34:34.000]   But in the meantime, EA will do.
[00:34:34.000 --> 00:34:38.000]   —Are there times when somebody asks you for a grant, and you view that as a negative
[00:34:38.000 --> 00:34:39.000]   signal?
[00:34:39.000 --> 00:34:40.000]   So let's say they're—especially when they're well-off.
[00:34:40.000 --> 00:34:43.800]   Like, let's say somebody's a former Google engineer, and they want to start a new project,
[00:34:43.800 --> 00:34:45.760]   and they're asking you for a grant.
[00:34:45.760 --> 00:34:48.720]   Do you worry that maybe they're too risk-averse, they want to put their own capital into it,
[00:34:48.720 --> 00:34:52.600]   or maybe that they are too conformist, they need your approval before they go ahead?
[00:34:52.600 --> 00:34:56.360]   —Things like this have happened, and I ask people flat-out, like, "Why do you want this
[00:34:56.360 --> 00:34:58.560]   grant from me?"
[00:34:58.560 --> 00:35:01.880]   And it is a forcing question in the sense that if their answer isn't good, I won't give
[00:35:01.880 --> 00:35:06.940]   it to them, even though they might have a good level of talent, good ideas, whatever.
[00:35:06.940 --> 00:35:10.320]   So they have to be able to answer that question in a credible way.
[00:35:10.320 --> 00:35:12.080]   Some can, some can't.
[00:35:12.080 --> 00:35:17.400]   —If you—I remember that the president of the University of Chicago many years back
[00:35:17.400 --> 00:35:21.920]   said that if he rejected the entire class of freshmen that are coming in and accepted
[00:35:21.920 --> 00:35:26.320]   the 1,500 people that they had to reject that year—the next 1,500 that they had to reject
[00:35:26.320 --> 00:35:29.480]   that year—then there would be, like, no difference in the quality of the admits.
[00:35:29.480 --> 00:35:33.120]   —I would think Chicago is the one school where that's not true, but I agree that it's
[00:35:33.120 --> 00:35:34.120]   true for most schools.
[00:35:34.120 --> 00:35:36.040]   —Do you think that's also true of emergent ventures?
[00:35:36.040 --> 00:35:37.440]   —No, not at all.
[00:35:37.440 --> 00:35:40.060]   —Like, how good is a marginal reject?
[00:35:40.060 --> 00:35:41.060]   —Not good.
[00:35:41.060 --> 00:35:46.600]   It's a remarkably bimodal distribution, as I perceive it, and maybe I'm wrong, but there
[00:35:46.600 --> 00:35:51.080]   aren't that many cases where I'm agonizing, and if I'm agonizing, I figure it probably
[00:35:51.080 --> 00:35:53.480]   should be a no.
[00:35:53.480 --> 00:35:57.040]   —I guess that makes it even rougher if you do get rejected, because it wasn't like, "Oh,
[00:35:57.040 --> 00:36:00.560]   you weren't a right fit for the job," or, "You almost made the cut."
[00:36:00.560 --> 00:36:03.880]   It's like, "No, we were actually just assessing your potential, not some sort of fit for the
[00:36:03.880 --> 00:36:04.880]   job."
[00:36:04.880 --> 00:36:09.040]   And not only did you—you weren't on the edge of potential, you were just on the other
[00:36:09.040 --> 00:36:10.040]   edge of the curve.
[00:36:10.040 --> 00:36:15.680]   —But a lot of these rejected people and projects, I don't think they're so, like, spilling
[00:36:15.680 --> 00:36:16.840]   tears over it.
[00:36:16.840 --> 00:36:21.480]   Like, you get an application, someone's in Akron, Ohio, and they want to start a nonprofit
[00:36:21.480 --> 00:36:26.920]   dog shelter, and they saw EV on the list of things you can apply to, and they apply to
[00:36:26.920 --> 00:36:31.480]   a lot of things and, like, maybe never get funding, but it's like people who enter contests
[00:36:31.480 --> 00:36:34.520]   or something, and so they apply to EV.
[00:36:34.520 --> 00:36:38.800]   Nothing against nonprofit dog shelters, but that's kind of a no, right?
[00:36:38.800 --> 00:36:44.320]   And I don't know, I genuinely don't know their response, but I don't think they walk away
[00:36:44.320 --> 00:36:50.520]   from the experience with some deeper model of what they should infer from the EV decision.
[00:36:50.520 --> 00:36:53.160]   —How much does the money part of emergent ventures matter, say, if you just didn't give
[00:36:53.160 --> 00:36:54.160]   them the money?
[00:36:54.160 --> 00:36:59.280]   —There's a whole bunch of proposals that really need the money for capital costs, and
[00:36:59.280 --> 00:37:00.720]   then it matters a lot.
[00:37:00.720 --> 00:37:03.000]   For a lot of them, the money per se doesn't matter.
[00:37:03.000 --> 00:37:04.000]   —Right.
[00:37:04.000 --> 00:37:05.800]   So what is the function of returns like that for that?
[00:37:05.800 --> 00:37:10.840]   If you, like, 10x the money or you 0.1x the money for some of these things, do you think
[00:37:10.840 --> 00:37:13.680]   that you'd see significantly different results?
[00:37:13.680 --> 00:37:19.800]   —I think a lot of foundations give out too many large grants and not enough small grants.
[00:37:19.800 --> 00:37:23.560]   I hope I'm at an optimum, but again, I don't have data to tell you.
[00:37:23.560 --> 00:37:28.880]   But I do think about this a lot, and I think small grants are underrated.
[00:37:28.880 --> 00:37:32.080]   —Why are women often better at detecting deceit?
[00:37:32.080 --> 00:37:36.280]   —I would assume, for biological and evolutionary reasons, that there are all these men trying
[00:37:36.280 --> 00:37:40.640]   to deceive them, right?
[00:37:40.640 --> 00:37:46.280]   The cost of a pregnancy is higher for a woman than for a man, on average, by quite a bit.
[00:37:46.280 --> 00:37:50.680]   So women will develop defense mechanisms that men maybe don't have as much.
[00:37:50.680 --> 00:37:54.480]   —One theory I heard from somebody I was brainstorming these questions with, she suggested
[00:37:54.480 --> 00:37:57.800]   that maybe it's because women just discuss personal matters more, and so therefore they
[00:37:57.800 --> 00:37:59.040]   have a greater library.
[00:37:59.040 --> 00:38:03.000]   —Well, that's certainly true, but that's subordinate to my explanation, I would say.
[00:38:03.000 --> 00:38:06.840]   But definitely, there's a lot of intermediate steps, right?
[00:38:06.840 --> 00:38:10.520]   Things that women do more of that help them be insightful.
[00:38:10.520 --> 00:38:12.920]   —Why is writing skill so important to you?
[00:38:12.920 --> 00:38:16.080]   —Well, one thing is just I'm good at judging it, right?
[00:38:16.080 --> 00:38:18.760]   So lacrosse skill, I'm very bad at judging.
[00:38:18.760 --> 00:38:22.200]   So there's nothing on the EV application testing for your lacrosse skill.
[00:38:22.200 --> 00:38:27.240]   But look, writing is a form of thinking, and public intellectuals are one of the things
[00:38:27.240 --> 00:38:32.000]   I want to support, and some of the companies I admire are like writing culture companies
[00:38:32.000 --> 00:38:33.920]   like Amazon or Stripe.
[00:38:33.920 --> 00:38:34.920]   So writing it is.
[00:38:34.920 --> 00:38:38.760]   I'm a good reader, so you're going to be asked to write.
[00:38:38.760 --> 00:38:43.800]   —Do you think it's a general fact that writing correlates with just general competence?
[00:38:43.800 --> 00:38:47.760]   —I do, but especially the areas that I'm funding, it's strongly true.
[00:38:47.760 --> 00:38:51.920]   Whether it's true for everything is harder to say.
[00:38:51.920 --> 00:38:53.320]   —Can stamina be increased?
[00:38:53.320 --> 00:38:54.320]   —Of course.
[00:38:54.320 --> 00:38:55.320]   —Oh, really?
[00:38:55.320 --> 00:38:56.320]   Okay.
[00:38:56.320 --> 00:38:57.320]   —Yeah.
[00:38:57.320 --> 00:39:00.320]   It's one of the easier things to increase.
[00:39:00.320 --> 00:39:05.640]   I don't think you can become superhuman in your energy and stamina if you're not born
[00:39:05.640 --> 00:39:07.280]   that way.
[00:39:07.280 --> 00:39:13.080]   But I think almost everyone could increase by 30 percent, 50 percent, like some notable
[00:39:13.080 --> 00:39:14.080]   amount.
[00:39:14.080 --> 00:39:15.080]   —Okay.
[00:39:15.080 --> 00:39:16.080]   That's interesting.
[00:39:16.080 --> 00:39:20.720]   —You know, putting aside maybe some disabilities or something, but people in regular circumstances.
[00:39:20.720 --> 00:39:21.720]   —Okay.
[00:39:21.720 --> 00:39:25.360]   Yeah, I think that's interesting because in our blog post you said from Robin Hanson
[00:39:25.360 --> 00:39:29.600]   about the stamina, I think his point of view was this is just something that's inherent
[00:39:29.600 --> 00:39:30.600]   to people.
[00:39:30.600 --> 00:39:34.160]   —Well, I don't think that's totally false.
[00:39:34.160 --> 00:39:38.400]   The people who have superhuman stamina are born that way.
[00:39:38.400 --> 00:39:39.400]   But there's plenty of margins.
[00:39:39.400 --> 00:39:41.580]   I mean, take physical stamina.
[00:39:41.580 --> 00:39:44.680]   You don't think people can train more and run for longer?
[00:39:44.680 --> 00:39:45.680]   Of course they can.
[00:39:45.680 --> 00:39:47.160]   Like, that's totally proven.
[00:39:47.160 --> 00:39:51.800]   So it would be weird if it held for all these organs but not your brain.
[00:39:51.800 --> 00:39:56.240]   That seems quite implausible, especially for someone like Robin where your brain is just
[00:39:56.240 --> 00:40:00.400]   this other organ that you're going to download or upload or goodness knows what with it.
[00:40:00.400 --> 00:40:03.480]   He's the physicalist if there ever was one.
[00:40:03.480 --> 00:40:06.360]   —Have you read Haruki Murakami's book on running?
[00:40:06.360 --> 00:40:07.360]   —No.
[00:40:07.360 --> 00:40:09.080]   I've been meaning to.
[00:40:09.080 --> 00:40:11.040]   I'm not sure how interesting I'll find it.
[00:40:11.040 --> 00:40:12.040]   I will someday.
[00:40:12.040 --> 00:40:13.040]   —Maybe not that interesting.
[00:40:13.040 --> 00:40:14.040]   —I like his stuff a lot.
[00:40:14.160 --> 00:40:18.120]   But what I found really interesting about it was just how linked building up physical
[00:40:18.120 --> 00:40:21.800]   stamina is for him to building up the stamina to write a lot.
[00:40:21.800 --> 00:40:26.560]   —And Magnus Carlsen would say the same with chess.
[00:40:26.560 --> 00:40:31.480]   So being in reasonable physical shape is important for your mental stamina, which is another
[00:40:31.480 --> 00:40:35.320]   kind of simple proof that you can boost your mental stamina.
[00:40:35.320 --> 00:40:39.920]   —Now, after reading the book, I was inclined to think that intelligence matters more than
[00:40:39.920 --> 00:40:40.920]   I previously thought.
[00:40:40.920 --> 00:40:41.920]   Not less.
[00:40:41.920 --> 00:40:42.920]   —Good.
[00:40:42.920 --> 00:40:43.920]   You might have undervalued it.
[00:40:43.920 --> 00:40:46.480]   —Or not even that.
[00:40:46.480 --> 00:40:50.960]   You say in the book that intelligence has convex returns and it matters especially for
[00:40:50.960 --> 00:40:52.820]   areas like inventors.
[00:40:52.820 --> 00:40:56.160]   Then you also say that if you look at some of the most important things in society, something
[00:40:56.160 --> 00:40:59.560]   like what Larry and Sergey did, they're basically inventors, right?
[00:40:59.560 --> 00:41:02.920]   So many of the most important things in society, intelligence matters more.
[00:41:02.920 --> 00:41:04.720]   And not only that, it has increasing returns.
[00:41:04.720 --> 00:41:08.000]   And it seems like with emerging ventures or a venture like that, you're trying to pick
[00:41:08.000 --> 00:41:10.080]   the people who are at the tail, right?
[00:41:10.080 --> 00:41:12.400]   You're not looking for a barista at Starbucks.
[00:41:12.400 --> 00:41:15.480]   So it seems like you should care about intelligence more, given the evidence there.
[00:41:15.480 --> 00:41:17.000]   —Well, more than who does?
[00:41:17.000 --> 00:41:21.840]   I mean, I feel what the book presents is, in fact, my view.
[00:41:21.840 --> 00:41:24.160]   And kind of by definition, I agree with that view.
[00:41:24.160 --> 00:41:28.080]   But yeah, there's a way of reading it where intelligence really matters a lot, but for
[00:41:28.080 --> 00:41:30.240]   a relatively small number of jobs.
[00:41:30.240 --> 00:41:31.240]   —Right.
[00:41:31.240 --> 00:41:34.160]   So maybe you just started off with like a really high prior on intelligence and that's
[00:41:34.160 --> 00:41:35.160]   why you downgraded.
[00:41:35.160 --> 00:41:36.160]   But maybe the average person should upgrade.
[00:41:36.160 --> 00:41:37.160]   —That's true.
[00:41:37.160 --> 00:41:42.240]   But I still would say there's a lot of jobs that I actually hire for in actual life where
[00:41:42.240 --> 00:41:44.760]   smarts are not the main thing I look for.
[00:41:44.760 --> 00:41:45.760]   Most jobs.
[00:41:45.760 --> 00:41:49.640]   —Does the convexity of returns on intelligence suggest that maybe the multiplicative model
[00:41:49.640 --> 00:41:50.640]   is wrong?
[00:41:50.640 --> 00:41:53.520]   Because if the multiplicative model is right, you would expect to see decreasing returns
[00:41:53.520 --> 00:41:55.640]   and putting your stats on one skill.
[00:41:55.640 --> 00:41:58.120]   You'd want to diversify more, right?
[00:41:58.120 --> 00:42:03.360]   —I think the convexity of returns to intelligence is embedded in a multiplicative model where
[00:42:03.360 --> 00:42:08.840]   the IQ returns only cash out for people good at all these other things.
[00:42:08.840 --> 00:42:12.720]   And for a lot of geniuses, they just can't get out of bed in the morning and you're stuck
[00:42:12.720 --> 00:42:15.880]   and you should write them off.
[00:42:15.880 --> 00:42:20.240]   —So you cite the data that Sweden collects from everybody that enters the military there
[00:42:20.240 --> 00:42:23.960]   and then, you know, the CEOs apparently are not especially smart.
[00:42:23.960 --> 00:42:27.680]   But one thing I found interesting from that same data was that Swedish soccer players
[00:42:27.680 --> 00:42:31.540]   are pretty smart and the better a soccer player somebody is, the smarter they are.
[00:42:31.540 --> 00:42:34.960]   And I mean, you've interviewed professional basketball players turned public intellectuals
[00:42:34.960 --> 00:42:38.120]   on your podcast and they sound extremely smart to me.
[00:42:38.120 --> 00:42:39.120]   What is going on there?
[00:42:39.120 --> 00:42:42.760]   Why anecdotally, and with some limited amounts of evidence, it seems that professional athletes
[00:42:42.760 --> 00:42:44.920]   are smarter than you would expect.
[00:42:44.920 --> 00:42:50.240]   —I'm a big fan of the view that top-level athletic performance is super cognitively
[00:42:50.240 --> 00:42:54.680]   intense, that most top athletes are really extraordinarily smart.
[00:42:54.680 --> 00:42:58.840]   And I don't just mean smart on the court, though obviously that, but smart more broadly,
[00:42:58.840 --> 00:43:00.320]   and that this is underrated.
[00:43:00.320 --> 00:43:04.400]   I think Michelle Dawson was the one who talked me into this, but absolutely, I'm with you
[00:43:04.400 --> 00:43:05.400]   all the way.
[00:43:05.400 --> 00:43:09.440]   —Do you think it's just mutational load or like the actual act of—
[00:43:09.440 --> 00:43:13.880]   —I think you actually have to be really smart to figure out like how to lead a team,
[00:43:13.880 --> 00:43:17.800]   how to improve yourself, how to practice, how to outsmart the opposition, all these
[00:43:17.800 --> 00:43:19.640]   other things.
[00:43:19.640 --> 00:43:22.680]   Maybe not the only way to get there, but very g-loaded.
[00:43:22.680 --> 00:43:23.680]   —Interesting.
[00:43:23.680 --> 00:43:28.700]   —And you certainly see some super talented athletes who just go bust.
[00:43:28.700 --> 00:43:32.960]   Or they may destroy themselves with drugs, or there's plenty of tales like that.
[00:43:32.960 --> 00:43:34.720]   You don't have to look hard.
[00:43:34.720 --> 00:43:38.680]   —Are there other areas where you—I wouldn't expect it to be g-loaded, but it actually
[00:43:38.680 --> 00:43:39.680]   is?
[00:43:39.680 --> 00:43:42.600]   —Probably, but there's so many like I just don't know.
[00:43:42.600 --> 00:43:47.880]   But sports is something in my life I followed, so I definitely have opinions about it.
[00:43:47.880 --> 00:43:51.560]   And they seem incredibly smart to me, and when they're interviewed, they're not always
[00:43:51.560 --> 00:43:53.160]   articulate.
[00:43:53.160 --> 00:43:57.000]   And there's sort of talking yourself into it by a sex post.
[00:43:57.000 --> 00:44:01.160]   But I hear like Michael Jordan in the '90s, I thought, "That guy's really smart."
[00:44:01.160 --> 00:44:02.720]   So I think he is.
[00:44:02.720 --> 00:44:03.720]   Look at Charles Barkley.
[00:44:03.720 --> 00:44:04.720]   He's amazing.
[00:44:04.720 --> 00:44:05.720]   —Right, right.
[00:44:05.720 --> 00:44:08.840]   —There's hardly anyone I'd rather listen to, including on talent, than Charles Barkley.
[00:44:08.840 --> 00:44:09.840]   —Huh.
[00:44:09.840 --> 00:44:10.840]   That's really interesting.
[00:44:10.840 --> 00:44:11.840]   Amazing.
[00:44:11.840 --> 00:44:12.840]   —And he's not that tall.
[00:44:12.840 --> 00:44:15.240]   You can't say, "Oh, he succeeded because he's 7'2".
[00:44:15.240 --> 00:44:19.400]   He was maybe top 6'4", and they called him "the round mound of rebound."
[00:44:19.400 --> 00:44:21.240]   Now, how did he do that?
[00:44:21.240 --> 00:44:22.240]   He was smart.
[00:44:22.240 --> 00:44:25.320]   He figured out where the ball was going, the weaknesses of his opponents, how to nudge
[00:44:25.320 --> 00:44:27.420]   them the right way, and so on.
[00:44:27.420 --> 00:44:28.420]   Brilliant guy.
[00:44:28.420 --> 00:44:31.880]   —What I find really remarkable is, not just with athletes, but with many other professions,
[00:44:31.880 --> 00:44:36.880]   if you interview somebody who's at the top of that field, they come off really, really
[00:44:36.880 --> 00:44:37.880]   smart.
[00:44:37.880 --> 00:44:38.880]   —Yes.
[00:44:38.880 --> 00:44:39.880]   —Like YouTubers, even sex workers.
[00:44:39.880 --> 00:44:44.240]   —So whoever is like the top gardener, I expect I would be super impressed by.
[00:44:44.240 --> 00:44:45.240]   —Hmm.
[00:44:45.240 --> 00:44:46.240]   —Exactly.
[00:44:46.240 --> 00:44:47.240]   —Right.
[00:44:47.240 --> 00:44:48.240]   Now, all your books are in some way about talent.
[00:44:48.240 --> 00:44:49.240]   Right?
[00:44:49.240 --> 00:44:50.240]   —Of course they are.
[00:44:50.240 --> 00:44:52.920]   —So let me read you the following passage from "An Economist Gets Lunch."
[00:44:52.920 --> 00:44:53.920]   —OK.
[00:44:53.920 --> 00:44:56.960]   —And I want you to tell me how we can apply this insight to talent.
[00:44:56.960 --> 00:44:57.960]   —OK.
[00:44:57.960 --> 00:45:01.300]   "At a fancy restaurant, the menu is well thought out.
[00:45:01.300 --> 00:45:03.800]   The time and attention of the kitchen are scarce.
[00:45:03.800 --> 00:45:07.940]   An item won't be on the menu unless there's a good reason for its presence.
[00:45:07.940 --> 00:45:11.080]   If it sounds bad, it probably tastes especially good."
[00:45:11.080 --> 00:45:12.080]   —Right.
[00:45:12.080 --> 00:45:13.760]   That's counter-savingly, right?
[00:45:13.760 --> 00:45:19.560]   So anything that is very weird, they will keep on the menu because it has a devoted
[00:45:19.560 --> 00:45:23.000]   set of people who keep on ordering it and appreciate it.
[00:45:23.000 --> 00:45:26.560]   And that's part of the talent of being a chef, that you can come up with such things.
[00:45:26.560 --> 00:45:28.760]   —Well, how do we apply this to talent?
[00:45:28.760 --> 00:45:33.660]   —Well, with restaurants, you have selection pressure, where you're only going to ones
[00:45:33.660 --> 00:45:35.660]   that have cleared certain hurdles.
[00:45:35.660 --> 00:45:39.800]   So this is true for talent, only for talents who are established.
[00:45:39.800 --> 00:45:44.640]   So if you see a persistent NBA player who's a very poor free throw shooter, like Shaquille
[00:45:44.640 --> 00:45:49.100]   O'Neal was, you can more or less assume they're really good at something else.
[00:45:49.100 --> 00:45:52.860]   But for people who are not established, there's not the same selection pressure.
[00:45:52.860 --> 00:45:54.940]   So there's not an analogous inference you can draw.
[00:45:54.940 --> 00:45:55.940]   —Right.
[00:45:55.940 --> 00:45:58.520]   So if I show up to an emerging ventures conference and I meet somebody and they don't seem especially
[00:45:58.520 --> 00:46:02.160]   impressive in the first impression, maybe I should think their work is especially impressive.
[00:46:02.160 --> 00:46:03.160]   —Yes, absolutely.
[00:46:03.160 --> 00:46:08.720]   —OK, so my understanding of your book, Creative Destruction, is that maybe on average, cultural
[00:46:08.720 --> 00:46:14.460]   diversity will go down, but in special niches, the diversity and ingenuity will go up.
[00:46:14.460 --> 00:46:19.160]   Can I apply the same insight to talent that maybe two random college grads will have similar
[00:46:19.160 --> 00:46:20.800]   skill sets over time?
[00:46:20.800 --> 00:46:25.200]   But if you look at somebody on the tails, their skills and knowledge will become even
[00:46:25.200 --> 00:46:26.680]   more specialized and even more diverse.
[00:46:26.680 --> 00:46:29.640]   There's a lot of different presuppositions in your question.
[00:46:29.640 --> 00:46:33.520]   So first, is cultural diversity going up or down?
[00:46:33.520 --> 00:46:35.440]   That I think is multidimensional.
[00:46:35.440 --> 00:46:41.120]   So say different cities in different countries will be more like each other over time.
[00:46:41.120 --> 00:46:44.840]   But that said, the genres they produce don't have to become more similar.
[00:46:44.840 --> 00:46:48.360]   They're more similar in the sense you can get sushi in each one.
[00:46:48.360 --> 00:46:53.360]   But like Nouvelle Cuisine in Dakar, Senegal, might be taking a very different path from
[00:46:53.360 --> 00:46:56.080]   Nouvelle Cuisine in Tokyo, Japan.
[00:46:56.080 --> 00:46:57.840]   So what happens with cultural diversity?
[00:46:57.840 --> 00:47:03.840]   I think the most reliable generalization is that it tends to come out of larger units.
[00:47:03.840 --> 00:47:08.160]   So small groups and tribes and linguistic groups, they get absorbed.
[00:47:08.160 --> 00:47:14.640]   Those people don't stop being creative in other venues, but there are fewer unique,
[00:47:14.640 --> 00:47:20.240]   isolated cultures and much more like thickly diverse urban creativity.
[00:47:20.240 --> 00:47:23.080]   That would be the main generalization I would put forward.
[00:47:23.080 --> 00:47:28.320]   So if you wanted to then apply that generalization to talent, I think in a funny way, you come
[00:47:28.320 --> 00:47:33.200]   back to my earlier point, the talent just tends to be geographically extremely well
[00:47:33.200 --> 00:47:35.560]   clustered.
[00:47:35.560 --> 00:47:42.240]   That's not the question you asked, but it's how I would reconfigure the pieces of it.
[00:47:42.240 --> 00:47:43.240]   Interesting.
[00:47:43.240 --> 00:47:47.920]   What does Alcheon Allen suggest about finding talent in a globalized world?
[00:47:47.920 --> 00:47:52.880]   And like in particular, if it's cheaper to find talent because of the Internet, does
[00:47:52.880 --> 00:47:55.760]   that mean that you should be selecting more mediocre candidates?
[00:47:55.760 --> 00:47:59.780]   I think it means you should be more bullish on immigrants from Africa.
[00:47:59.780 --> 00:48:05.980]   So it's relatively hard to get out of Africa to the United States in most cases.
[00:48:05.980 --> 00:48:10.760]   So that's a sign the person put in a lot of effort and ability.
[00:48:10.760 --> 00:48:15.240]   Maybe an easy country to come here from would be Canada.
[00:48:15.240 --> 00:48:16.240]   All other things equal.
[00:48:16.240 --> 00:48:19.600]   Again, I'd want this to be measured.
[00:48:19.600 --> 00:48:24.640]   The people who come from countries that are hard to come from, India actually, the numbers
[00:48:24.640 --> 00:48:29.720]   are fairly high, but the roots are mostly pretty gated.
[00:48:29.720 --> 00:48:30.720]   Right.
[00:48:30.720 --> 00:48:31.720]   Yes.
[00:48:31.720 --> 00:48:36.440]   Is part of the reason that talent is hard to spot and find today is that we have an
[00:48:36.440 --> 00:48:37.600]   aging population.
[00:48:37.600 --> 00:48:42.040]   So then we would have more capital, more jobs, more mentorship available for young people
[00:48:42.040 --> 00:48:44.080]   coming up than there are young people.
[00:48:44.080 --> 00:48:48.600]   I don't think we're really into demographic decline yet, not in the United States.
[00:48:48.600 --> 00:48:50.480]   Maybe in Japan that would be true.
[00:48:50.480 --> 00:48:55.140]   But it seems to me, especially with the Internet, there's more 15 year old talent today than
[00:48:55.140 --> 00:48:58.240]   ever before by a lot, not just by a little.
[00:48:58.240 --> 00:49:03.080]   You see this in chess, right, where we can measure performance very well.
[00:49:03.080 --> 00:49:07.880]   Just a lot more young talent from many different places, including the US.
[00:49:07.880 --> 00:49:09.800]   So aging hasn't mattered yet.
[00:49:09.800 --> 00:49:12.040]   Maybe for a few places, but not here.
[00:49:12.040 --> 00:49:15.160]   What do you think will change in talent spotting as society becomes older?
[00:49:15.160 --> 00:49:16.160]   Society.
[00:49:16.160 --> 00:49:20.520]   Depends what you mean by society.
[00:49:20.520 --> 00:49:26.440]   The US, unless it totally screws up on immigration, will always have like a very seriously good
[00:49:26.440 --> 00:49:34.280]   flow of young people that we don't ever have to enter the aging equilibrium the way Japan
[00:49:34.280 --> 00:49:35.880]   probably already has.
[00:49:35.880 --> 00:49:38.680]   So I don't know what will change, I think.
[00:49:38.680 --> 00:49:42.000]   And then there's work from a distance, there's hiring from a distance, funding from a distance.
[00:49:42.000 --> 00:49:47.880]   As you know, there's Emergent Ventures India, which is in India, and we do that at a distance.
[00:49:47.880 --> 00:49:49.800]   So I don't think we're ever going to enter that world.
[00:49:49.800 --> 00:49:50.800]   Right.
[00:49:50.800 --> 00:49:52.920]   But then what does it look like for Japan?
[00:49:52.920 --> 00:49:56.720]   Is part of the reason that Japanese cultures and companies are arranged the way they are
[00:49:56.720 --> 00:50:00.720]   and do the recruitment the way they do, is that linked to perhaps their demographics?
[00:50:00.720 --> 00:50:04.040]   That strikes me as a plausible reason, but I don't think I know enough to say.
[00:50:04.040 --> 00:50:07.360]   But it wouldn't surprise me if that turned out to be the case.
[00:50:07.360 --> 00:50:14.800]   To what extent do you need a sort of "great man" ethos in your culture in order to empower
[00:50:14.800 --> 00:50:16.040]   the top talent?
[00:50:16.040 --> 00:50:20.360]   That if you have too much political and moral egalitarianism, you're not going to give great
[00:50:20.360 --> 00:50:25.200]   people like the real incentive and drive to strive to be great.
[00:50:25.200 --> 00:50:30.960]   We've got to say "great man" or "great woman" ethos, or, you know, "other," whatever all-purpose
[00:50:30.960 --> 00:50:33.560]   word we wish to use.
[00:50:33.560 --> 00:50:38.080]   I worry much less about woke ideology than a lot of people I know.
[00:50:38.080 --> 00:50:42.760]   It's not my thing, but it's something young people can rebel against.
[00:50:42.760 --> 00:50:47.080]   Like if that keeps you down, I'm not so impressed by you.
[00:50:47.080 --> 00:50:48.400]   I think it's fine.
[00:50:48.400 --> 00:50:49.680]   Let the woke reign.
[00:50:49.680 --> 00:50:51.320]   People can work around them.
[00:50:51.320 --> 00:50:56.160]   But overall, if you have a culture like Europe's, do you think that has any impact on...
[00:50:56.160 --> 00:50:58.680]   Europe is not woke enough in a lot of ways, right?
[00:50:58.680 --> 00:51:03.720]   Europe is very chauvinist and conservative in the literal sense and often quite old-fashioned.
[00:51:03.720 --> 00:51:05.200]   Depends where you're talking about.
[00:51:05.200 --> 00:51:09.200]   But Europe, I would say, on that is much less woke than the United States.
[00:51:09.200 --> 00:51:13.840]   I wouldn't say that's their main problem, but you can't say, "Oh, they don't innovate
[00:51:13.840 --> 00:51:14.840]   because they're too woke."
[00:51:14.840 --> 00:51:15.840]   No, no, no.
[00:51:15.840 --> 00:51:20.000]   Like hang out with some 63-year-old Danish guys and see how woke you think they are after
[00:51:20.000 --> 00:51:21.160]   everyone's had a few drinks.
[00:51:21.160 --> 00:51:22.160]   My question wasn't about wokeism.
[00:51:22.160 --> 00:51:25.480]   I just mean in general, if you have an egalitarian society.
[00:51:25.480 --> 00:51:27.920]   I think of Europe as less egalitarian than Europe.
[00:51:27.920 --> 00:51:33.760]   I think they have bad cultural norms for innovation, but I don't think being too egalitarian is
[00:51:33.760 --> 00:51:34.760]   it.
[00:51:34.760 --> 00:51:35.760]   They're culturally so non-egalitarian.
[00:51:35.760 --> 00:51:41.160]   Again, it depends where, but listen, Paris would be the extreme.
[00:51:41.160 --> 00:51:42.800]   Everyone is classified, right?
[00:51:42.800 --> 00:51:43.800]   And by status.
[00:51:43.800 --> 00:51:48.120]   You need to wear your sweater the right way and this and that.
[00:51:48.120 --> 00:51:50.040]   Now how innovative is Paris?
[00:51:50.040 --> 00:51:54.580]   Actually, maybe more than people think, but I still think they have too few dimensions
[00:51:54.580 --> 00:51:56.560]   of status competition.
[00:51:56.560 --> 00:52:01.600]   That's a general problem in most of Europe, is too few dimensions of status competition.
[00:52:01.600 --> 00:52:04.400]   Not enough room for the proverbial like village idiot.
[00:52:04.400 --> 00:52:05.400]   Interesting.
[00:52:05.400 --> 00:52:08.600]   You say in the book that questions tend to degrade over time if you don't replace them.
[00:52:08.600 --> 00:52:12.760]   I find it interesting that Y Combinator has kept the same questions since they were started
[00:52:12.760 --> 00:52:13.760]   in 2005.
[00:52:13.760 --> 00:52:18.000]   And of course, your co-author was a partner at Y Combinator.
[00:52:18.000 --> 00:52:21.320]   Do you think that works for Y Combinator or do you think they're probably making a mistake?
[00:52:21.320 --> 00:52:22.600]   I genuinely don't know.
[00:52:22.600 --> 00:52:27.440]   There are people who will tell you Y Combinator, while still successful, has become more like
[00:52:27.440 --> 00:52:33.280]   a scalable business school and less like attracting all the top weirdos who do amazing things.
[00:52:33.280 --> 00:52:39.240]   Again, I'd want to see data before asserting that myself, but you certainly hear it a lot.
[00:52:39.240 --> 00:52:43.200]   So it could be Y Combinator is a bit stale, but stale in the good sense.
[00:52:43.200 --> 00:52:44.640]   Like Harvard is stale, right?
[00:52:44.640 --> 00:52:47.800]   It dates from the 17th century, but it's still amazing.
[00:52:47.800 --> 00:52:49.360]   MIT is stale.
[00:52:49.360 --> 00:52:51.760]   Maybe Y Combinator has become more like those groups.
[00:52:51.760 --> 00:52:54.120]   Do you think that will happen to emergent ventures eventually?
[00:52:54.120 --> 00:52:59.800]   I don't think so because it has a number of unique features built in from the front.
[00:52:59.800 --> 00:53:03.360]   So a very small number of evaluators, two.
[00:53:03.360 --> 00:53:07.760]   Now the two might grow a little bit, but it's not going to grow that much.
[00:53:07.760 --> 00:53:10.160]   I'm not paid to do it.
[00:53:10.160 --> 00:53:15.080]   So that really limits how much it's going to scale.
[00:53:15.080 --> 00:53:20.360]   And there's not like a staff that has to be carried where you're captured by the staff.
[00:53:20.360 --> 00:53:21.720]   There is no staff.
[00:53:21.720 --> 00:53:25.720]   There's a bit of free riding on staff who do other things.
[00:53:25.720 --> 00:53:30.000]   But there's no sense of if the program goes away, like all my buddies on staff get laid
[00:53:30.000 --> 00:53:31.000]   off.
[00:53:31.000 --> 00:53:32.000]   No.
[00:53:32.000 --> 00:53:38.160]   So it's kind of pop up and low costs of exit whenever that time comes.
[00:53:38.160 --> 00:53:41.400]   Do you personally have questions that you haven't put in the book or elsewhere because
[00:53:41.400 --> 00:53:47.040]   you want them to be fresh for asking somebody who's applying to the grant?
[00:53:47.040 --> 00:53:48.960]   Well I didn't when we wrote the book.
[00:53:48.960 --> 00:53:52.240]   So we put everything in there that we were thinking of.
[00:53:52.240 --> 00:53:55.060]   But over time we've developed more.
[00:53:55.060 --> 00:53:59.260]   And I don't generally give them in interviews because you've got to keep some stock.
[00:53:59.260 --> 00:54:00.800]   So yeah, there's more since then.
[00:54:00.800 --> 00:54:02.200]   But we weren't holding back at the time.
[00:54:02.200 --> 00:54:03.200]   It's like a comedy routine.
[00:54:03.200 --> 00:54:04.680]   You got to write a new one each year.
[00:54:04.680 --> 00:54:05.680]   That's right.
[00:54:05.680 --> 00:54:11.440]   But when your show's on the air, you do give it your best jokes, right?
[00:54:11.440 --> 00:54:14.760]   Is somebody who's applying to Merchant Ventures, if they've read your book, are they any better
[00:54:14.760 --> 00:54:19.380]   off or are they perhaps worse off because maybe they get misleading or partial view
[00:54:19.380 --> 00:54:20.800]   into what's required of them?
[00:54:20.800 --> 00:54:24.720]   I hope they're not better off in a way, but probably they are.
[00:54:24.720 --> 00:54:30.880]   I hope they use it to understand their own talent better and present it better in a good
[00:54:30.880 --> 00:54:35.620]   way and not just to try to manipulate the system.
[00:54:35.620 --> 00:54:39.480]   But most people aren't actually that good at manipulating that kind of system.
[00:54:39.480 --> 00:54:40.720]   So I'm not too worried.
[00:54:40.720 --> 00:54:41.720]   Right.
[00:54:41.720 --> 00:54:43.720]   In a sense, if they can manipulate the system, that's a positive signal.
[00:54:43.720 --> 00:54:44.720]   Of some kind.
[00:54:44.720 --> 00:54:45.720]   Right.
[00:54:45.720 --> 00:54:46.720]   Like you could fool me.
[00:54:46.720 --> 00:54:48.160]   Like, hey, what else have you got to say?
[00:54:48.160 --> 00:54:49.160]   You know?
[00:54:49.160 --> 00:54:50.160]   Yeah.
[00:54:50.160 --> 00:54:51.160]   Right.
[00:54:51.160 --> 00:54:52.160]   Are you worried that young people will encounter you now?
[00:54:52.160 --> 00:54:55.640]   They're going to think of you as a sort of a talent judge and a good one at that.
[00:54:55.640 --> 00:54:59.240]   So they're maybe going to be more self-aware than they were.
[00:54:59.240 --> 00:55:03.680]   I worry about the effect of this on me, that maybe a lot of my interactions become less
[00:55:03.680 --> 00:55:10.540]   genuine or people are too self-conscious or too stilted or too something.
[00:55:10.540 --> 00:55:11.540]   Is there something you can do about that?
[00:55:11.540 --> 00:55:13.640]   Or is that just baked into the cake?
[00:55:13.640 --> 00:55:14.640]   I don't know.
[00:55:14.640 --> 00:55:21.800]   If you do your best to try to act genuine, whatever that means, maybe you can avoid it
[00:55:21.800 --> 00:55:23.980]   a bit or delay it at least a bit.
[00:55:23.980 --> 00:55:25.620]   But a lot of it I don't think you can avoid.
[00:55:25.620 --> 00:55:27.820]   In part, you're just cashing in.
[00:55:27.820 --> 00:55:30.220]   So I'm 60.
[00:55:30.220 --> 00:55:33.460]   I don't think I'll still be doing this when I'm 80.
[00:55:33.460 --> 00:55:39.860]   So if I have like 18 years of cashing in, maybe it's what I should be doing.
[00:55:39.860 --> 00:55:43.160]   To what extent are the principles of finding talent timeless?
[00:55:43.160 --> 00:55:47.340]   So if you're looking for, let's say, a general in the French Revolution, how much does the
[00:55:47.340 --> 00:55:48.340]   advice change?
[00:55:48.340 --> 00:55:50.700]   Or are the basic principles the same over time?
[00:55:50.700 --> 00:55:53.660]   Well, one of the key principles is contextual.
[00:55:53.660 --> 00:55:56.620]   You need to focus on how the sector is different.
[00:55:56.620 --> 00:56:01.380]   But if you're doing that, then I think at the meta level, the principles broadly stay
[00:56:01.380 --> 00:56:02.580]   the same.
[00:56:02.580 --> 00:56:06.380]   You have a really interesting book about autism and systematizers.
[00:56:06.380 --> 00:56:08.780]   You think Napoleon was autistic?
[00:56:08.780 --> 00:56:14.220]   I've read several biographies of him and not come away with that impression, but you can't
[00:56:14.220 --> 00:56:15.220]   rule it out.
[00:56:15.220 --> 00:56:16.420]   Like, what are the biographers now?
[00:56:16.420 --> 00:56:19.540]   It gets back to our question of how valuable is history?
[00:56:19.540 --> 00:56:21.380]   Did the biographers ever meet Napoleon?
[00:56:21.380 --> 00:56:26.700]   Well, some of them did, but those people had such weak other intellectual categories.
[00:56:26.700 --> 00:56:31.860]   And the modern biographies, Andrew Roberts, whoever you think is good, they don't know.
[00:56:31.860 --> 00:56:32.860]   So how can I know?
[00:56:32.860 --> 00:56:33.860]   Right.
[00:56:33.860 --> 00:56:36.620]   And again, the issue is that the details that stick on my mind for reading the biography
[00:56:36.620 --> 00:56:39.060]   are the ones that make him seem autistic, right?
[00:56:39.060 --> 00:56:40.060]   Yes.
[00:56:40.060 --> 00:56:45.060]   And there's a tendency in biographies to storify things.
[00:56:45.060 --> 00:56:46.820]   And that's dangerous too, right?
[00:56:46.820 --> 00:56:51.420]   How general or cross-applicable is talent or just competence of any kind?
[00:56:51.420 --> 00:56:58.780]   So like, if you look at somebody like Peter Thiel, you know, investor, great executive,
[00:56:58.780 --> 00:57:02.700]   great thinker even, or, you know, I think, speaking of Napoleon, I think it was some
[00:57:02.700 --> 00:57:06.780]   mathematician, was it Lagrange or Laplace, who he was studying under.
[00:57:06.780 --> 00:57:09.340]   They said that he could have been a mathematician if he wanted to.
[00:57:09.340 --> 00:57:13.820]   I don't know if that's true, but it does seem that the top achievers in one field seem to
[00:57:13.820 --> 00:57:17.860]   be able to be able to move across fields and be top achievers in other fields.
[00:57:17.860 --> 00:57:20.100]   Is that a pattern that you see as well?
[00:57:20.100 --> 00:57:24.580]   Maybe somewhat, but I don't think you can be top at anything or even most things.
[00:57:24.580 --> 00:57:29.980]   And a lot of these very successful people in other areas, they might just be like mere
[00:57:29.980 --> 00:57:30.980]   millionaires.
[00:57:30.980 --> 00:57:33.660]   What do you mean?
[00:57:33.660 --> 00:57:40.900]   Oh, maybe they ran a car dealership and earned $3 million in 1966, which is a pretty good
[00:57:40.900 --> 00:57:43.140]   life back then, right?
[00:57:43.140 --> 00:57:49.020]   But it's not like what they have ended up being.
[00:57:49.020 --> 00:57:52.180]   You quote Sam Altman in the book, that was really interesting.
[00:57:52.180 --> 00:57:58.180]   He says, "The successful founders I funded believe that they are eventually certain to
[00:57:58.180 --> 00:57:59.520]   be successful."
[00:57:59.520 --> 00:58:05.980]   To what extent is this self-belief, is that the result or the cause of being talented?
[00:58:05.980 --> 00:58:10.180]   Maybe it's both, but keep in mind the context for Sam.
[00:58:10.180 --> 00:58:16.180]   Those are companies and their startups, and startups succeed at such a low rate that the
[00:58:16.180 --> 00:58:20.380]   successes really are selecting for people with quite a bit of overconfidence.
[00:58:20.380 --> 00:58:25.020]   And other sectors, you won't in general find that same level of overconfidence.
[00:58:25.020 --> 00:58:26.620]   So you have to be careful.
[00:58:26.620 --> 00:58:31.160]   I agree with Sam, but he's talking about one set of things, not everything.
[00:58:31.160 --> 00:58:32.160]   Is that not true of other fields?
[00:58:32.160 --> 00:58:37.440]   Like if you're looking for an intellectual, right, you're partially hoping for the outcome
[00:58:37.440 --> 00:58:40.800]   that they become remembered or their ideas have a lot of influence.
[00:58:40.800 --> 00:58:43.640]   And that's also a rare thing to be able to do.
[00:58:43.640 --> 00:58:47.140]   I think more people stumble into it, for instance.
[00:58:47.140 --> 00:58:55.060]   And then there's more people who know early on they can do it, but not for Sam-like reasons
[00:58:55.060 --> 00:58:56.860]   of overconfidence.
[00:58:56.860 --> 00:59:02.060]   Just they kind of know it because they can and there are enough early tests.
[00:59:02.060 --> 00:59:04.260]   So I still think it's different.
[00:59:04.260 --> 00:59:07.140]   And there's more, yes, stumbling into it by accident.
[00:59:07.140 --> 00:59:10.780]   And which better describes your intellectual journey?
[00:59:10.780 --> 00:59:13.340]   Like were you in some sense a little overconfident in your 20s?
[00:59:13.340 --> 00:59:16.900]   Well, there's an interesting break in my life that relates to stumbling into things.
[00:59:16.900 --> 00:59:19.300]   So I grew up with no internet.
[00:59:19.300 --> 00:59:24.180]   So I thought I would do quite well in that sense, I was overconfident, but I had no notion
[00:59:24.180 --> 00:59:27.700]   that I would have large numbers of people listening to me.
[00:59:27.700 --> 00:59:30.260]   I just didn't think about the internet.
[00:59:30.260 --> 00:59:34.900]   So in that sense, I totally stumbled into the particular way in which I ended up doing
[00:59:34.900 --> 00:59:37.900]   well, yet was still at a younger age overconfident.
[00:59:37.900 --> 00:59:38.900]   Interesting.
[00:59:38.900 --> 00:59:44.860]   I want to back test some of your methods of finding talent in certain people.
[00:59:44.860 --> 00:59:45.860]   Okay.
[00:59:45.860 --> 00:59:48.940]   Okay, so we just talked about Haruki Murakami, right?
[00:59:48.940 --> 00:59:49.940]   Yeah.
[00:59:49.940 --> 00:59:50.940]   Let's use him as an example.
[00:59:51.620 --> 00:59:53.620]   In his 30s, I believe he was just running a bar.
[00:59:53.620 --> 00:59:54.620]   Who's the person?
[00:59:54.620 --> 00:59:55.620]   Oh, the novelist, Haruki Murakami.
[00:59:55.620 --> 00:59:56.620]   Oh, yeah, yeah.
[00:59:56.620 --> 00:59:57.620]   Okay.
[00:59:57.620 --> 00:59:58.620]   He was just running a bar.
[00:59:58.620 --> 00:59:59.620]   Right.
[00:59:59.620 --> 01:00:02.900]   So it doesn't have to be him in particular, but just generally.
[01:00:02.900 --> 01:00:03.900]   Think of like a 30-year-old.
[01:00:03.900 --> 01:00:07.020]   You go to Japan, you go to a bar, you start talking to the bartender who also happens
[01:00:07.020 --> 01:00:08.020]   to own the place.
[01:00:08.020 --> 01:00:09.020]   Yeah.
[01:00:09.020 --> 01:00:12.100]   Like, what would the conversation look like where you would like identify if this person
[01:00:12.100 --> 01:00:14.460]   could be a great novelist or it could be a great anything?
[01:00:14.460 --> 01:00:18.340]   I think my chance of identifying great novelists is very low.
[01:00:18.340 --> 01:00:20.820]   And it's one reason why it's not something I try to do.
[01:00:20.820 --> 01:00:21.820]   Interesting.
[01:00:21.820 --> 01:00:24.100]   Why is that?
[01:00:24.100 --> 01:00:28.180]   When I look at biographies, there seem to be so many instances of people who don't show
[01:00:28.180 --> 01:00:32.100]   obvious signs of promise early on.
[01:00:32.100 --> 01:00:36.460]   So maybe if I knew more such people, I could develop markers.
[01:00:36.460 --> 01:00:37.780]   It's possible, right?
[01:00:37.780 --> 01:00:41.740]   Like my chance of being good at that is probably way above average.
[01:00:41.740 --> 01:00:44.460]   But I definitely don't think I'd be good at it now.
[01:00:44.460 --> 01:00:45.460]   Interesting.
[01:00:45.460 --> 01:00:48.780]   So what do you think makes novelists so hard to predict?
[01:00:48.780 --> 01:00:52.380]   They can blossom much later, and very often they do.
[01:00:52.380 --> 01:00:57.920]   Very high percentage of them are women whose earlier lives are interrupted, often by children,
[01:00:57.920 --> 01:00:58.920]   but not only.
[01:00:58.920 --> 01:01:02.420]   That gets back to the late bloomers thing.
[01:01:02.420 --> 01:01:08.940]   And there's something quite discreet about a novel, that until a person has done it,
[01:01:08.940 --> 01:01:11.100]   it's hard to tell how good they are.
[01:01:11.100 --> 01:01:16.580]   Or say a great nonfiction book, well, like, you know, "Taleb" or "Pinker" or whoever.
[01:01:16.580 --> 01:01:21.340]   Like you could read their earlier blog posts and just see, flat out see, "Oh, they're really
[01:01:21.340 --> 01:01:22.340]   smart.
[01:01:22.340 --> 01:01:23.340]   Maybe they could write a great book."
[01:01:23.340 --> 01:01:28.060]   Like, I wouldn't say anyone could do that, but most people we know could do that, spot
[01:01:28.060 --> 01:01:29.060]   them earlier.
[01:01:29.060 --> 01:01:30.340]   But a novel?
[01:01:30.340 --> 01:01:31.340]   I can't.
[01:01:31.340 --> 01:01:33.260]   Do you think that's also true?
[01:01:33.260 --> 01:01:36.260]   The way you described it sounds very similar to "A Startup Founder," even maybe "The Time
[01:01:36.260 --> 01:01:37.260]   Horizon."
[01:01:37.260 --> 01:01:40.420]   Like, you haven't really done anything that's like it before.
[01:01:40.420 --> 01:01:42.660]   And "The Time Horizon" is maybe like five to ten years.
[01:01:42.660 --> 01:01:45.660]   I don't know, maybe a novel takes shorter, but…
[01:01:45.660 --> 01:01:48.580]   There are more intermediate benchmarks with startups.
[01:01:48.580 --> 01:01:51.340]   Just how good a job do they do trying to raise their first round?
[01:01:51.340 --> 01:01:53.820]   There's a lot you can watch.
[01:01:53.820 --> 01:01:57.420]   It's not indicative of, like, product fit to the market and a bunch of other things,
[01:01:57.420 --> 01:01:59.780]   but you see a lot early on.
[01:01:59.780 --> 01:02:01.740]   How good is the pitch, the deck?
[01:02:01.740 --> 01:02:06.900]   Again, there's some great, great things that had terrible pitches, but I think you see
[01:02:06.900 --> 01:02:08.100]   way more early signs.
[01:02:08.100 --> 01:02:12.900]   Now, maybe novelists show early signs that I don't know about, so again, I'm suggesting
[01:02:12.900 --> 01:02:17.740]   maybe I could learn, but right now I'm totally at sea with that one.
[01:02:17.740 --> 01:02:20.660]   Joseph Conrad, like, was I going to get that?
[01:02:20.660 --> 01:02:21.660]   Herman Melville.
[01:02:21.660 --> 01:02:23.180]   I don't think so.
[01:02:23.180 --> 01:02:24.180]   Hmm.
[01:02:24.180 --> 01:02:25.180]   Interesting.
[01:02:25.180 --> 01:02:29.380]   Okay, well, let's back-test with another person.
[01:02:29.380 --> 01:02:36.900]   Like, hey, Joe, you're from Poland, you're going to write in English, like, get real.
[01:02:36.900 --> 01:02:37.900]   Scott Aronson.
[01:02:37.900 --> 01:02:43.540]   So, as you know, he's a famous complexity theorist, computer scientist, and he was actually
[01:02:43.540 --> 01:02:50.900]   a former professor, but he wrote in a blog post about standardized tests.
[01:02:50.900 --> 01:02:51.900]   This is what he wrote.
[01:02:51.900 --> 01:02:56.660]   "I was a 15-year-old with perfect SATs and a published research paper, but not only was
[01:02:56.660 --> 01:03:01.300]   I young and immature with spotty grades and a weird academic trajectory, I had no sports,
[01:03:01.300 --> 01:03:03.780]   no music, no diverse leadership experiences.
[01:03:03.780 --> 01:03:08.500]   I was a narrow, linear, A-to-B thinker who lacked depth and emotional intelligence, the
[01:03:08.500 --> 01:03:11.620]   exact opposite of what Harvard and Princeton were looking for in every way."
[01:03:11.620 --> 01:03:14.580]   Now, what would happen to Scott Aronson if he, at that time, had applied to Emergent
[01:03:14.580 --> 01:03:15.580]   Ventures?
[01:03:15.580 --> 01:03:20.260]   I've never met Scott, but odds are very strong we'd find him from the sound of it, but again,
[01:03:20.260 --> 01:03:22.300]   I don't know him at all.
[01:03:22.300 --> 01:03:24.420]   But the narrow, linear thinker?
[01:03:24.420 --> 01:03:25.460]   I don't know what that means.
[01:03:25.460 --> 01:03:30.300]   A lot of people misdescribe themselves, and they say, like, "Oh, if you ask me those questions,
[01:03:30.300 --> 01:03:32.220]   I would suck," and they're wrong.
[01:03:32.220 --> 01:03:33.220]   They wouldn't suck.
[01:03:33.220 --> 01:03:38.940]   No, they don't, and I suspect Scott's self-description is a bit off, but I think he would do very
[01:03:38.940 --> 01:03:40.220]   well at Emergent Ventures.
[01:03:40.220 --> 01:03:41.220]   Sure, yeah.
[01:03:41.220 --> 01:03:42.220]   I agree.
[01:03:42.220 --> 01:03:45.700]   Let's talk about effective altruism and long-termism.
[01:03:45.700 --> 01:03:49.940]   You have expressed skepticism to the idea that you can use long-termism to say that
[01:03:49.940 --> 01:03:53.700]   existential risks matter more than everything else because we should be optimizing for the
[01:03:53.700 --> 01:03:57.300]   branch of the decision tree where we survive for millions of years.
[01:03:57.300 --> 01:03:59.940]   Can you say more of why you're skeptical of that kind of reasoning?
[01:03:59.940 --> 01:04:03.580]   Well, I'd want to express my skepticism a little more clearly.
[01:04:03.580 --> 01:04:08.900]   I think existential risk matters much more than almost anyone thinks, and in this sense,
[01:04:08.900 --> 01:04:10.900]   I'm with the EA people.
[01:04:10.900 --> 01:04:13.460]   But I do think they overvalue it a bit.
[01:04:13.460 --> 01:04:17.380]   I would just say I don't think there are many good things we can do to limit existential
[01:04:17.380 --> 01:04:23.700]   risk that are very different from looking for more talent, growing GDP, supporting science,
[01:04:23.700 --> 01:04:29.060]   trotting down a pretty familiar list of things that don't all have to be that long-term.
[01:04:29.060 --> 01:04:32.840]   And in that sense, I think they flip out about it a bit too much and have all these super
[01:04:32.840 --> 01:04:38.180]   specific hypotheses, but we should invest in good things now.
[01:04:38.180 --> 01:04:41.460]   I do favor an asteroid protection program, by the way.
[01:04:41.460 --> 01:04:45.980]   To the extent that there was a trade-off in that hypothetical, would you put the same
[01:04:45.980 --> 01:04:51.140]   weight on existential risk that they do, or do you just differ with them on how you actually
[01:04:51.140 --> 01:04:52.940]   go about solving existential risks?
[01:04:52.940 --> 01:04:53.940]   Probably more the latter.
[01:04:53.940 --> 01:04:58.860]   I think they're not epistemically modest enough when it comes to existential risk, and they
[01:04:58.860 --> 01:05:01.980]   think they have all these particular — some of them, not all of them, by any means — these
[01:05:01.980 --> 01:05:07.300]   very particular hypotheses about AGI and we've got to prevent this.
[01:05:07.300 --> 01:05:08.500]   That's where I really differ from them.
[01:05:08.500 --> 01:05:13.820]   I think their ability to limit that risk, however great or small it might be, is basically
[01:05:13.820 --> 01:05:14.820]   zero.
[01:05:14.820 --> 01:05:19.540]   That if AGI is a risk, it's the worst set of procedures that will do you in, and you
[01:05:19.540 --> 01:05:22.180]   can't regulate those very well at all.
[01:05:22.180 --> 01:05:27.180]   Putting everyone at your favorite tech company through this training about alignment, I'm
[01:05:27.180 --> 01:05:34.020]   not against doing that, but, come on, if it's going to happen, it's like handling pandemic
[01:05:34.020 --> 01:05:35.020]   materials.
[01:05:35.020 --> 01:05:38.300]   It's the sloppiest people you've got to worry about, and they are not sitting in on your
[01:05:38.300 --> 01:05:40.500]   class on AGI and alignment.
[01:05:40.500 --> 01:05:41.500]   Yes.
[01:05:41.500 --> 01:05:45.740]   Although, it is surprising to the extent that the companies in the U.S. that maybe care
[01:05:45.740 --> 01:05:51.660]   more than other entities about alignment are actually first currently, right?
[01:05:51.660 --> 01:05:52.660]   Opening eyes first.
[01:05:52.660 --> 01:05:56.620]   Yeah, but it won't matter, because if that view is the correct one — and I don't think
[01:05:56.620 --> 01:06:02.140]   it is — the more screwed up successors will just come 10 years later, and, you know, Skynet
[01:06:02.140 --> 01:06:03.140]   goes live.
[01:06:03.140 --> 01:06:04.540]   But, 10 years later.
[01:06:04.540 --> 01:06:05.540]   Right.
[01:06:05.540 --> 01:06:11.860]   I'm curious, just generally, why is the possibility of humanity serving for a very long time not
[01:06:11.860 --> 01:06:17.140]   something that is a strong part of your worldview, given that you're a strong long-termist?
[01:06:17.140 --> 01:06:22.740]   I think the chance of there being a major war with nuclear weapons or whatever comes
[01:06:22.740 --> 01:06:28.180]   next, while very low in any given year, you just have the clock tick.
[01:06:28.180 --> 01:06:32.060]   That chance adds up, and we're not going to be here for another 100,000 years.
[01:06:32.060 --> 01:06:33.580]   It's a simple argument.
[01:06:33.580 --> 01:06:36.900]   But I'm not a pessimist in any given year at all.
[01:06:36.900 --> 01:06:37.900]   Right.
[01:06:37.900 --> 01:06:42.020]   But if the odds are like sufficiently above zero, then do you just not buy the argument
[01:06:42.020 --> 01:06:47.020]   that like anything above zero is just huge and we should be optimizing for that?
[01:06:47.020 --> 01:06:51.420]   I'm all for things to make nuclear weapons safer, but it's hard to know exactly what
[01:06:51.420 --> 01:06:52.420]   you do.
[01:06:52.420 --> 01:06:54.180]   What do we do in Ukraine now?
[01:06:54.180 --> 01:06:56.540]   We should be more tough, less tough?
[01:06:56.540 --> 01:07:00.160]   There's different arguments, but they're not that different from just the normal foreign
[01:07:00.160 --> 01:07:01.160]   policy arguments.
[01:07:01.160 --> 01:07:07.100]   There's not some special branch of EA long-termism that tells you what to do in Ukraine.
[01:07:07.100 --> 01:07:11.540]   And those people, if anything, tend to be kind of under-invested in historical and cultural
[01:07:11.540 --> 01:07:12.940]   forms of knowledge.
[01:07:12.940 --> 01:07:18.940]   So I just don't think you buy that much extra stuff by calling yourself worried about existential
[01:07:18.940 --> 01:07:19.940]   risk.
[01:07:19.940 --> 01:07:23.580]   There are plenty of people in the U.S. foreign policy establishment who think about all this
[01:07:23.580 --> 01:07:24.580]   stuff.
[01:07:24.580 --> 01:07:28.740]   Until recently, most of them had never heard of EA, maybe even still.
[01:07:28.740 --> 01:07:31.700]   It doesn't change the debate much.
[01:07:31.700 --> 01:07:35.620]   I'm sure you've heard these arguments, but it seems with nuclear war, it's hard to imagine
[01:07:35.620 --> 01:07:38.580]   how it could kill like every single person on the planet.
[01:07:38.580 --> 01:07:42.620]   But I think we'll be permanently set back kind of forever.
[01:07:42.620 --> 01:07:46.660]   And in the meantime, we can't build asteroid protection or whatever else.
[01:07:46.660 --> 01:07:52.380]   And it will just be like medieval living standards, super small population, feudal governance,
[01:07:52.380 --> 01:07:55.900]   lots of violence, rape, whatever.
[01:07:55.900 --> 01:08:01.340]   And there's no reason to think like, oh, you read a copy of the Constitution in 400 years,
[01:08:01.340 --> 01:08:02.660]   we're back on track.
[01:08:02.660 --> 01:08:03.660]   That's crazy wrong, I think.
[01:08:03.660 --> 01:08:05.780]   But we did emerge from feudalism, right?
[01:08:05.780 --> 01:08:08.500]   So if it happened once, isn't that example enough?
[01:08:08.500 --> 01:08:09.500]   We don't know.
[01:08:09.500 --> 01:08:10.500]   There's what?
[01:08:10.500 --> 01:08:14.760]   Hundreds of thousands of years of human history where we seem to make diddly-squat progress.
[01:08:14.760 --> 01:08:15.760]   We don't know why.
[01:08:15.760 --> 01:08:19.460]   But don't assume that it happened once means you always rebuild.
[01:08:19.460 --> 01:08:21.020]   I don't think it does.
[01:08:21.020 --> 01:08:22.020]   Right.
[01:08:22.020 --> 01:08:27.380]   Like, well, if it's not just the idea as being laden in the space, like what would it take
[01:08:27.380 --> 01:08:30.100]   for our descendants to be able to, like, recover industrial civilization?
[01:08:30.100 --> 01:08:32.460]   I don't think we have good theories of that at all.
[01:08:32.460 --> 01:08:41.100]   I would just say we had a lot of semi-independent operating parts of the world, say circa 1500.
[01:08:41.100 --> 01:08:42.900]   And not that many of them made much progress.
[01:08:42.900 --> 01:08:43.900]   Right.
[01:08:43.900 --> 01:08:47.780]   I mean, I think of you as optimist, at least by temperament, but this seems like one of
[01:08:47.780 --> 01:08:52.220]   the most, one of the more pessimistic things I've heard overall, anywhere, because the
[01:08:52.220 --> 01:08:56.820]   idea that not only will human civilization be decimated almost surely, but that they
[01:08:56.820 --> 01:08:58.260]   will never be able to recover.
[01:08:58.260 --> 01:09:03.800]   I wouldn't say never, you know, never say never, James Bond movie, but there's no reason
[01:09:03.800 --> 01:09:05.540]   to assume you just bounce back.
[01:09:05.540 --> 01:09:07.420]   I would say we don't know.
[01:09:07.420 --> 01:09:09.940]   Other problems will come upon us.
[01:09:09.940 --> 01:09:13.220]   Nuclear winter, crop failures, climate change.
[01:09:13.220 --> 01:09:15.800]   It just seems very daunting to me.
[01:09:15.800 --> 01:09:19.900]   And like the overall history of mammalian species is not that optimistic.
[01:09:19.900 --> 01:09:23.700]   The fact that sex exists is biologically very pessimistic.
[01:09:23.700 --> 01:09:25.380]   What do you mean?
[01:09:25.380 --> 01:09:29.060]   I don't think of myself as a pessimist, but you can call it that.
[01:09:29.060 --> 01:09:31.180]   Wait, can you explain that quote?
[01:09:31.180 --> 01:09:32.820]   The fact that sex is a mysticism?
[01:09:32.820 --> 01:09:38.740]   Well, anything that stands still gets destroyed by maybe parasites or destroyed by something.
[01:09:38.740 --> 01:09:42.980]   So you've got to randomize and change what you are through sex.
[01:09:42.980 --> 01:09:48.100]   That's like the clearly winning model for at least larger things, right?
[01:09:48.100 --> 01:09:50.940]   And that's a sign nothing survives for that long.
[01:09:50.940 --> 01:09:53.660]   So the existence of sex is the most pessimistic thing there is.
[01:09:53.660 --> 01:09:55.660]   I find that ironic.
[01:09:55.660 --> 01:10:00.260]   So I'm not the pessimist sexist, right?
[01:10:00.260 --> 01:10:02.860]   Let's say I take your argument that economic growth is very important.
[01:10:02.860 --> 01:10:06.100]   Does that imply anything in particular about what somebody should do with their life?
[01:10:06.100 --> 01:10:09.720]   Or is it basically just an argument about policy?
[01:10:09.720 --> 01:10:11.960]   It can guide your life a bit.
[01:10:11.960 --> 01:10:15.060]   So to think just more carefully, how do you fit in?
[01:10:15.060 --> 01:10:16.980]   Maybe you could do something important.
[01:10:16.980 --> 01:10:18.940]   Maybe you could have higher aspirations.
[01:10:18.940 --> 01:10:20.740]   For most people, it won't matter.
[01:10:20.740 --> 01:10:24.340]   But again, certainly some people could do much more.
[01:10:24.340 --> 01:10:27.500]   And I do my best there to try to help that along.
[01:10:27.500 --> 01:10:28.500]   Right.
[01:10:28.500 --> 01:10:31.260]   Does it have to do with the fact that we don't know much about what causes economic growth?
[01:10:31.260 --> 01:10:34.860]   Or is it just like, even if we knew that there's like, you can never offer like concrete advice
[01:10:34.860 --> 01:10:37.220]   about what you could do to increase economic growth.
[01:10:37.220 --> 01:10:41.120]   I give concrete advice to people all the time with a grain of salt.
[01:10:41.120 --> 01:10:45.660]   But I'll like tell people, I think you should go to this school, not that school.
[01:10:45.660 --> 01:10:47.300]   That's concrete advice.
[01:10:47.300 --> 01:10:50.960]   I don't think we know so little about growth.
[01:10:50.960 --> 01:10:54.460]   We certainly know a lot about how to wreck growth, right?
[01:10:54.460 --> 01:10:57.460]   So we know enough.
[01:10:57.460 --> 01:10:58.460]   Right.
[01:10:58.460 --> 01:10:59.460]   Right.
[01:10:59.460 --> 01:11:03.020]   But not enough to like create an 80,000 hours for progress studies, what do you think?
[01:11:03.020 --> 01:11:04.340]   Or is that just fundamentally impossible?
[01:11:04.340 --> 01:11:05.740]   What do you mean create an 80?
[01:11:05.740 --> 01:11:06.740]   You mean an institution?
[01:11:06.740 --> 01:11:08.500]   Well, there's Institute for Progress.
[01:11:08.500 --> 01:11:09.500]   No, no, no.
[01:11:09.500 --> 01:11:13.040]   Like here's a list, here's the things you should consider doing with your career.
[01:11:13.040 --> 01:11:14.440]   It's not the right way to think about it.
[01:11:14.440 --> 01:11:15.960]   What would you say more?
[01:11:15.960 --> 01:11:21.240]   You want to sit down with the person, understand the entire context, see what they could do,
[01:11:21.240 --> 01:11:24.440]   see if there's a way you could or should bend up the curve.
[01:11:24.440 --> 01:11:29.920]   But a list, no, that's, it's a little too EA static maximization for me.
[01:11:29.920 --> 01:11:34.540]   If you like focus more on learning about cultures, history, you're not going to like come up
[01:11:34.540 --> 01:11:37.000]   with some list as the way to approach that.
[01:11:37.000 --> 01:11:39.740]   Oh, why are culture and history going to...
[01:11:39.740 --> 01:11:45.820]   They show you how complex things are and the people who made very significant contributions,
[01:11:45.820 --> 01:11:51.340]   how complex the inputs were into that, or even did very terrible things like Napoleon,
[01:11:51.340 --> 01:11:56.540]   like understanding Napoleon, what he came from, ideas he had, super complicated.
[01:11:56.540 --> 01:11:59.580]   I don't really get the list version.
[01:11:59.580 --> 01:12:05.860]   Here's the list for baby Napoleon, like don't invade Russia or I don't know, beware, you
[01:12:05.860 --> 01:12:07.700]   know, Tally Rand or whatever.
[01:12:07.700 --> 01:12:09.960]   It just seems to miss the point.
[01:12:09.960 --> 01:12:13.300]   If you were to, if like a young person were to read a bunch of biographies, not necessarily
[01:12:13.300 --> 01:12:16.660]   as a career advice, but just generally as trying to like better understand how they
[01:12:16.660 --> 01:12:17.660]   could be more effective.
[01:12:17.660 --> 01:12:18.660]   Yeah.
[01:12:18.660 --> 01:12:21.660]   Do you think that's going to teach them, like teach them that things are more complex than
[01:12:21.660 --> 01:12:22.660]   they thought?
[01:12:22.660 --> 01:12:23.660]   Or would that give them any practical...
[01:12:23.660 --> 01:12:24.660]   I think both.
[01:12:24.660 --> 01:12:27.640]   Napoleon's a good person to read biographies of.
[01:12:27.640 --> 01:12:32.060]   When I was young, like sports and chess players were my grist.
[01:12:32.060 --> 01:12:34.060]   And I feel I learned a lot from that.
[01:12:34.060 --> 01:12:39.020]   I don't know that it was any big lesson, but just you saw all these histories of people
[01:12:39.020 --> 01:12:43.220]   persevering and self-improving and that's worth a lot.
[01:12:43.220 --> 01:12:44.940]   So I don't think it's a waste of time at all.
[01:12:44.940 --> 01:12:46.900]   I think it's probably essential.
[01:12:46.900 --> 01:12:49.060]   I don't know if you have to read biographies.
[01:12:49.060 --> 01:12:53.440]   Like if you just follow sports careers, that might be enough, but that's kind of like reading
[01:12:53.440 --> 01:12:55.020]   a biography, right?
[01:12:55.020 --> 01:12:56.860]   YouTube can do it.
[01:12:56.860 --> 01:12:59.900]   I don't like fixate on the biography.
[01:12:59.900 --> 01:13:03.140]   They seem in a way inefficiently long.
[01:13:03.140 --> 01:13:07.420]   Is it like just somebody having a blog and you follow along their blog on a weekly basis
[01:13:07.420 --> 01:13:08.420]   or a daily basis?
[01:13:08.420 --> 01:13:09.420]   Absolutely.
[01:13:09.420 --> 01:13:10.420]   That's reading a biography.
[01:13:10.420 --> 01:13:11.420]   Yes.
[01:13:11.420 --> 01:13:16.300]   I've been blogging almost 20 years and I hope there's some lesson in the constancy of that
[01:13:16.300 --> 01:13:18.860]   for people.
[01:13:18.860 --> 01:13:22.900]   I was struck while reading your book that some of the advice you offer for how to ask
[01:13:22.900 --> 01:13:27.160]   good questions in hiring is actually great advice for also how to ask good questions
[01:13:27.160 --> 01:13:28.160]   in a podcast.
[01:13:28.160 --> 01:13:29.160]   Yes.
[01:13:29.160 --> 01:13:30.160]   Absolutely.
[01:13:30.160 --> 01:13:31.160]   Yes.
[01:13:31.160 --> 01:13:34.320]   To get the conversation flow going, to get them interested, talk about something that
[01:13:34.320 --> 01:13:35.880]   they're very interested in.
[01:13:35.880 --> 01:13:39.720]   Don't worry about changing the subject, just get them on something where they're involved
[01:13:39.720 --> 01:13:40.720]   and excited.
[01:13:40.720 --> 01:13:41.720]   Yeah.
[01:13:41.720 --> 01:13:42.720]   Yeah.
[01:13:42.720 --> 01:13:43.720]   Yeah.
[01:13:43.720 --> 01:13:44.720]   And so to what extent was I just informed from having a podcast?
[01:13:44.720 --> 01:13:46.040]   Oh, quite a bit.
[01:13:46.040 --> 01:13:49.900]   You can think of the podcasts in a sense as like interviews.
[01:13:49.900 --> 01:13:53.480]   Like they're kind of very judgmental, like how worthy are you, right?
[01:13:53.480 --> 01:13:54.480]   Right.
[01:13:54.480 --> 01:13:55.480]   Right.
[01:13:55.480 --> 01:13:56.480]   People are afraid more and more.
[01:13:56.480 --> 01:14:01.000]   You have a quite mellow personality and like I'm similar in that way.
[01:14:01.000 --> 01:14:06.440]   I wonder, do you think this has any sort of intellectual consequence in the sense that
[01:14:06.440 --> 01:14:11.960]   if you were, maybe if you could experience like this, the exuberant highs or the incapacitating
[01:14:11.960 --> 01:14:17.820]   lows, you would be maybe less modest or moderate?
[01:14:17.820 --> 01:14:21.200]   I might be more creative, but I think I would be more wrong.
[01:14:21.200 --> 01:14:22.200]   Interesting.
[01:14:22.200 --> 01:14:23.200]   Yeah.
[01:14:23.200 --> 01:14:26.440]   So like how much is that on that trade off?
[01:14:26.440 --> 01:14:29.440]   Like where should one be if they're trying to reason about important topics?
[01:14:29.440 --> 01:14:32.760]   Should you just try to increase the variance so you can get the important things right?
[01:14:32.760 --> 01:14:34.760]   I think it's again, context specific.
[01:14:34.760 --> 01:14:40.400]   You want to understand where a person is, understand they were probably born that way.
[01:14:40.400 --> 01:14:45.200]   You can only budge them so much wherever you might think they should be and just try to
[01:14:45.200 --> 01:14:49.260]   marginally improve how they're dealing with the flow coming their way.
[01:14:49.260 --> 01:14:54.520]   So I prefer to work with people's strengths and boost the strengths rather than like have
[01:14:54.520 --> 01:14:57.360]   a list set out of how to reform them.
[01:14:57.360 --> 01:15:02.060]   I think it's a way more productive way to do things and it's lower conflict.
[01:15:02.060 --> 01:15:08.520]   So you as like a coach or mentor or a coworker, it's way less stressful for you because you're
[01:15:08.520 --> 01:15:10.820]   being very positive with them and it's sincere.
[01:15:10.820 --> 01:15:14.760]   So you'll just do more of it where if you're hitting them over the head, like why don't
[01:15:14.760 --> 01:15:18.280]   you, you know, wake up at 7am in the morning?
[01:15:18.280 --> 01:15:19.280]   Maybe they should.
[01:15:19.280 --> 01:15:22.660]   It's like, come on, you have something better to do than that.
[01:15:22.660 --> 01:15:27.240]   Like if they can't figure that out for themselves, tell them something else that they can actually
[01:15:27.240 --> 01:15:29.800]   find useful.
[01:15:29.800 --> 01:15:34.640]   When you interviewed Andrew Sullivan, one of the things he said was that the reason
[01:15:34.640 --> 01:15:39.320]   he decided to write about gay rights was that he got HIV and he realized he might not have
[01:15:39.320 --> 01:15:40.320]   that long to live.
[01:15:40.320 --> 01:15:41.320]   Yeah.
[01:15:41.320 --> 01:15:44.120]   And of course, we see the consequence of that in society today.
[01:15:44.120 --> 01:15:46.880]   If you find out you only have five years to live, what is the book you would write?
[01:15:46.880 --> 01:15:50.280]   What is the argument you would make?
[01:15:50.280 --> 01:15:54.680]   Five years from now, I think I would do more emergent ventures.
[01:15:54.680 --> 01:15:55.680]   Interesting.
[01:15:55.680 --> 01:16:01.020]   I would finish the book I'm working on and I might consider a sequel to Talent, depending
[01:16:01.020 --> 01:16:04.920]   on Daniel's plans.
[01:16:04.920 --> 01:16:11.080]   But my marginal thing I don't feel is writing more books, though I will write more books.
[01:16:11.080 --> 01:16:15.420]   So like it's like your late career that you think the most important thing is institution
[01:16:15.420 --> 01:16:16.420]   building and talent spotting?
[01:16:16.420 --> 01:16:17.420]   Yes.
[01:16:17.420 --> 01:16:18.420]   At this point.
[01:16:18.420 --> 01:16:19.980]   And I've written like 16, 17 books.
[01:16:19.980 --> 01:16:22.100]   So it's not like I haven't had my say.
[01:16:22.100 --> 01:16:23.100]   Sure.
[01:16:23.100 --> 01:16:26.980]   One interesting thing about top performers in many fields is that they have intricate
[01:16:26.980 --> 01:16:32.100]   philosophies and worldviews like Peter Thiel, obviously, with the Girardian stuff.
[01:16:32.100 --> 01:16:36.700]   But even like somebody like George Soros with the theory of reflexivity.
[01:16:36.700 --> 01:16:41.460]   To what extent do you think that these are very important in their success?
[01:16:41.460 --> 01:16:44.700]   Or to what extent do you think maybe that if you just have somebody who's like plus
[01:16:44.700 --> 01:16:48.340]   four standard deviations in verbal intelligence, one of the things they'll do, other than being
[01:16:48.340 --> 01:16:51.460]   very successful, is just create intricate worldviews?
[01:16:51.460 --> 01:16:55.540]   All the people I know who are like that, such as Peter, I feel it's important for their
[01:16:55.540 --> 01:16:56.540]   success.
[01:16:56.540 --> 01:16:58.160]   Soros, I don't know.
[01:16:58.160 --> 01:17:00.460]   But since all the people I do know, it seems to matter.
[01:17:00.460 --> 01:17:05.220]   My intuition is that it matters a lot more broadly.
[01:17:05.220 --> 01:17:08.500]   Like you need a unique way of looking at the world.
[01:17:08.500 --> 01:17:15.060]   Is that a correlate in the sense that it like jolts you out of complacency?
[01:17:15.060 --> 01:17:16.060]   Or is that...
[01:17:16.060 --> 01:17:18.060]   And it protects you from other people's idiocy.
[01:17:18.060 --> 01:17:23.060]   Your mimetic desires get channeled away from a lot of other things that might even be good
[01:17:23.060 --> 01:17:25.820]   overall, but they would distract you.
[01:17:25.820 --> 01:17:26.820]   Right.
[01:17:26.820 --> 01:17:31.160]   But maybe like the actual theory itself is not...
[01:17:31.160 --> 01:17:32.160]   It's not the edge.
[01:17:32.160 --> 01:17:33.160]   It's just like...
[01:17:33.160 --> 01:17:34.160]   Correct.
[01:17:34.160 --> 01:17:35.160]   Now, sometimes it's the edge.
[01:17:35.160 --> 01:17:39.120]   If you read the story of Peter, knew Rene Girard, saw Facebook would be a big thing.
[01:17:39.120 --> 01:17:40.120]   Right.
[01:17:40.120 --> 01:17:43.880]   It's probably true, but it doesn't have to be the edge, I would say.
[01:17:43.880 --> 01:17:44.880]   Yeah.
[01:17:44.880 --> 01:17:48.160]   So one thing I found really interesting in your book, What Price Fame?, was you had this
[01:17:48.160 --> 01:17:53.760]   interesting discussion where you cite your former advisor, Thomas Schelling, about how
[01:17:53.760 --> 01:17:58.840]   certain celebrities can make themselves focal points in the culture.
[01:17:58.840 --> 01:18:02.240]   And I'm curious how we can apply this to public intellectuals.
[01:18:02.240 --> 01:18:06.840]   Like in recent years, we've seen a lot of public intellectuals become a focal point
[01:18:06.840 --> 01:18:09.080]   in the culture war or in just general discussions.
[01:18:09.080 --> 01:18:10.080]   Right.
[01:18:10.080 --> 01:18:11.080]   In some sense, this has happened to you as well.
[01:18:11.080 --> 01:18:12.080]   Right.
[01:18:12.080 --> 01:18:15.280]   We've seen this with many of your even former podcast guests, people like Jordan Peterson,
[01:18:15.280 --> 01:18:17.160]   Christopher Hitchens, Sam Harris.
[01:18:17.160 --> 01:18:19.920]   Like how do public intellectuals make themselves focal points?
[01:18:19.920 --> 01:18:22.860]   Well, by doing something noteworthy, right?
[01:18:22.860 --> 01:18:27.000]   So it can be an idea, but it can also be a form of performance art.
[01:18:27.000 --> 01:18:29.880]   And maybe performance art has become more important.
[01:18:29.880 --> 01:18:34.440]   I think of my own focality as having more to do with the kind of performance art than
[01:18:34.440 --> 01:18:37.360]   with any specific idea I have.
[01:18:37.360 --> 01:18:42.400]   And I think very carefully about how to stay somewhat focal for a long period of time.
[01:18:42.400 --> 01:18:48.280]   So it's quite unusual that I've mostly increased my focality over 20 years now.
[01:18:48.280 --> 01:18:51.920]   It's much more common that people come and go and decline.
[01:18:51.920 --> 01:18:53.720]   And I work quite hard not to do that.
[01:18:53.720 --> 01:18:59.080]   A lot of the IDW people have very clear peaks, which now lie in the past.
[01:18:59.080 --> 01:19:01.880]   And I suspect we'll have fairly rapid declines.
[01:19:01.880 --> 01:19:03.080]   I don't want that to happen to me.
[01:19:03.080 --> 01:19:05.680]   I want to be in the thick of things, just for selfish reasons.
[01:19:05.680 --> 01:19:06.680]   What do you think?
[01:19:06.680 --> 01:19:10.840]   What's the explanation for why they peak so early?
[01:19:10.840 --> 01:19:12.280]   I don't know if early is the word.
[01:19:12.280 --> 01:19:15.200]   Like Jordan Peterson was at it for a long time.
[01:19:15.200 --> 01:19:19.220]   But they made extreme bets on very particular ideas.
[01:19:19.220 --> 01:19:22.480]   And maybe different people will disagree about those ideas.
[01:19:22.480 --> 01:19:27.940]   But I think a lot of them are losing ideas, even if they might be correct in some ways.
[01:19:27.940 --> 01:19:31.240]   I've done much less to bet on a single idea.
[01:19:31.240 --> 01:19:34.160]   You can say kind of market-based economics.
[01:19:34.160 --> 01:19:37.440]   But it's not so radical.
[01:19:37.440 --> 01:19:39.680]   Like we're still in a capitalistic society.
[01:19:39.680 --> 01:19:44.080]   So like I have enough to say about new issues that come along all the time.
[01:19:44.080 --> 01:19:49.040]   And to be taking a mostly pro-capitalist point of view, like it's just not that obsolete.
[01:19:49.040 --> 01:19:52.240]   Maybe it's not peak fashion now, but it's fine to be doing that.
[01:19:52.240 --> 01:19:53.240]   They were over-leveraging.
[01:19:53.240 --> 01:19:54.640]   They got margin called.
[01:19:54.640 --> 01:20:00.720]   I'm not making a big bet on ivermectin, is one way to put it.
[01:20:00.720 --> 01:20:04.600]   Why doesn't tenure make academics less risk-averse?
[01:20:04.600 --> 01:20:06.320]   I've thought about this quite a bit.
[01:20:06.320 --> 01:20:09.320]   I think the selection filters are very strong.
[01:20:09.320 --> 01:20:11.420]   They're very pro-conformity.
[01:20:11.420 --> 01:20:13.800]   People care a lot what their peers think of them.
[01:20:13.800 --> 01:20:17.080]   You're selecting for conformists to begin with.
[01:20:17.080 --> 01:20:20.460]   And you're just literally not trained in how to take risk.
[01:20:20.460 --> 01:20:23.360]   It's not always that easy, right?
[01:20:23.360 --> 01:20:28.220]   So let's say you're like, "F this tenure, I'm going to get up and take a risk."
[01:20:28.220 --> 01:20:29.220]   Like what do you do?
[01:20:29.220 --> 01:20:30.760]   You say, "Demand curves slope upwards.
[01:20:30.760 --> 01:20:35.000]   Well, that's a risk, but it's just not that easy."
[01:20:35.000 --> 01:20:39.120]   Is there something that would be analogous to a course on risk that makes people more
[01:20:39.120 --> 01:20:40.120]   risk-taking?
[01:20:40.120 --> 01:20:44.120]   I would say at Mercatus, we have a lot of students come through here, and we do try
[01:20:44.120 --> 01:20:47.360]   to teach them all in different ways how to take more career risk.
[01:20:47.360 --> 01:20:49.640]   And I think we've been remarkably successful at that.
[01:20:49.640 --> 01:20:51.640]   Well, what is being taught that makes them more risk-taking?
[01:20:51.640 --> 01:20:56.320]   Well, first of all, they observe what we all do, and they just learn by example.
[01:20:56.320 --> 01:21:00.260]   But if they want advice, like how to run a podcast, how to write a blog, how to try to
[01:21:00.260 --> 01:21:04.560]   work for someone on the Senate Banking Committee, we have people who've done that.
[01:21:04.560 --> 01:21:06.400]   We'll put them in touch.
[01:21:06.400 --> 01:21:11.920]   So we don't have a class, but there's a very serious focused effort that anyone with any
[01:21:11.920 --> 01:21:17.640]   interest in learning how to do things, take more risk, whatever, can get that here for
[01:21:17.640 --> 01:21:18.640]   free.
[01:21:18.640 --> 01:21:22.120]   And I think the most valuable is risk aversion.
[01:21:22.120 --> 01:21:27.880]   I think a lot of the students we produce have done unusual things, certainly relative to
[01:21:27.880 --> 01:21:28.880]   other programs.
[01:21:28.880 --> 01:21:31.400]   They could be better yet, of course.
[01:21:31.400 --> 01:21:35.880]   But us leading by example is the number one way we teach them.
[01:21:35.880 --> 01:21:40.080]   You had an interesting post in 2011 where you're talking about which public intellectuals
[01:21:40.080 --> 01:21:41.760]   have been the most influential.
[01:21:41.760 --> 01:21:45.240]   And one thing I noticed from the list was that the people you said who were most influential
[01:21:45.240 --> 01:21:50.560]   were people who were advocating for one very specific narrow policy change their entire
[01:21:50.560 --> 01:21:51.560]   careers.
[01:21:51.560 --> 01:21:52.560]   Right.
[01:21:52.560 --> 01:21:53.560]   And a lot of those people fade.
[01:21:53.560 --> 01:21:58.120]   Now, the two I cited, Andrew Sullivan and Peter Singer, have not faded, but it's a risky
[01:21:58.120 --> 01:21:59.120]   strategy.
[01:21:59.120 --> 01:22:01.400]   And I would get bored too quickly doing that.
[01:22:01.400 --> 01:22:02.400]   OK.
[01:22:02.400 --> 01:22:06.400]   I was just about to ask, is there a reason why you haven't adopted this strategy yourself?
[01:22:06.400 --> 01:22:07.400]   Yeah.
[01:22:07.400 --> 01:22:09.880]   That's the way in which I am risk averse.
[01:22:09.880 --> 01:22:11.880]   I don't have the single issue.
[01:22:11.880 --> 01:22:14.360]   Like for Andrew, gay marriage, like, great.
[01:22:14.360 --> 01:22:15.360]   He won.
[01:22:15.360 --> 01:22:16.360]   That's amazing.
[01:22:16.360 --> 01:22:18.780]   I'm all for it.
[01:22:18.780 --> 01:22:22.620]   But in part, he won because it is the thing he cared about most.
[01:22:22.620 --> 01:22:25.480]   And I don't have a comparable thing like that.
[01:22:25.480 --> 01:22:29.320]   The closest thing like that I have is like, here's my way of thinking.
[01:22:29.320 --> 01:22:31.320]   I'm going to show it to you.
[01:22:31.320 --> 01:22:33.880]   And I have done that pretty monomaniacally.
[01:22:33.880 --> 01:22:36.160]   And I think I've been somewhat successful.
[01:22:36.160 --> 01:22:41.180]   Does this imply that the most interesting intellectuals will never be the most influential?
[01:22:41.180 --> 01:22:43.560]   Because to be influential, you have to focus on just one thing.
[01:22:43.560 --> 01:22:45.400]   On average, that's true.
[01:22:45.400 --> 01:22:46.400]   Right.
[01:22:46.400 --> 01:22:47.400]   I think so.
[01:22:47.400 --> 01:22:48.400]   OK.
[01:22:48.400 --> 01:22:52.520]   So John Stuart Mill was super interesting and right about many, many things like he
[01:22:52.520 --> 01:22:54.440]   had some influence.
[01:22:54.440 --> 01:22:57.380]   But was there a single thing where he saw through and made it happen?
[01:22:57.380 --> 01:22:58.920]   I don't know.
[01:22:58.920 --> 01:22:59.920]   Yeah.
[01:22:59.920 --> 01:23:05.600]   Whereas Richard Cobden, not like a deep economic thinker, but for free trade against the Corn
[01:23:05.600 --> 01:23:08.480]   Laws, he and John Bright, they made that happen.
[01:23:08.480 --> 01:23:10.560]   I would say they were correct.
[01:23:10.560 --> 01:23:13.960]   But it's not that interesting to read them.
[01:23:13.960 --> 01:23:19.200]   But do you think people just change people's worldviews in general in sort of like they
[01:23:19.200 --> 01:23:22.800]   might not have impacted any one person all that much, like they're not going to become
[01:23:22.800 --> 01:23:25.720]   evangelist because they read this person.
[01:23:25.720 --> 01:23:29.080]   But just the broader cultural change in a way that's hard to measure.
[01:23:29.080 --> 01:23:31.700]   You think these people are especially influential over the long term or?
[01:23:31.700 --> 01:23:32.700]   I don't know.
[01:23:32.700 --> 01:23:37.600]   I hope there's some influence there, but very hard to say, hard to measure.
[01:23:37.600 --> 01:23:41.860]   Especially the blogosphere, like the legendary parts of it were started in the 2000s, 2010s
[01:23:41.860 --> 01:23:43.080]   with people like you.
[01:23:43.080 --> 01:23:46.320]   But before 2010s, I would say, but go on.
[01:23:46.320 --> 01:23:47.320]   Yes.
[01:23:47.320 --> 01:23:48.320]   You, Paul Graham, Scott Alexander.
[01:23:48.320 --> 01:23:49.320]   Right.
[01:23:49.320 --> 01:23:53.280]   You think that people are starting blogs today, are they kind of just LARPing the new moment
[01:23:53.280 --> 01:23:54.280]   you had at the time?
[01:23:54.280 --> 01:23:59.280]   Or you think that this is actually a format that can survive the test of time?
[01:23:59.280 --> 01:24:00.880]   I think it will survive.
[01:24:00.880 --> 01:24:05.800]   So we have early bloggers, Samuel Pepys, James Boswell, they've survived, right?
[01:24:05.800 --> 01:24:06.800]   It's good material.
[01:24:06.800 --> 01:24:09.520]   It's 17th and 18th centuries.
[01:24:09.520 --> 01:24:17.080]   So why can't it survive today when the technology makes it easier and more readily preserved?
[01:24:17.080 --> 01:24:22.520]   Just the notion that you write and someone reads you seems to me extremely robust.
[01:24:22.520 --> 01:24:23.520]   Right.
[01:24:23.520 --> 01:24:26.520]   But like you write on the internet on a regular basis.
[01:24:26.520 --> 01:24:27.520]   Why not?
[01:24:27.520 --> 01:24:28.520]   Yes.
[01:24:28.520 --> 01:24:33.280]   So many writers like writing on a regular basis and it has some practical advantages.
[01:24:33.280 --> 01:24:35.800]   So I'd be very surprised if that went away.
[01:24:35.800 --> 01:24:40.520]   Now at this moment, you could say Substack is bigger than blogs, it's a bit different,
[01:24:40.520 --> 01:24:42.480]   but it's broadly a similar thing.
[01:24:42.480 --> 01:24:43.480]   Right.
[01:24:43.480 --> 01:24:46.780]   What do you see as the differences between a Substack and a blog?
[01:24:46.780 --> 01:24:50.480]   Substack posts tend to be longer.
[01:24:50.480 --> 01:24:55.160]   A lot of blogs, you're very much editor and not just content creator.
[01:24:55.160 --> 01:25:00.120]   You're sometimes editor in Substack, but much less.
[01:25:00.120 --> 01:25:04.560]   So something like what Instapundit has done, I don't think there's really a Substack version
[01:25:04.560 --> 01:25:05.840]   of that for better or worse.
[01:25:05.840 --> 01:25:06.840]   You don't need one.
[01:25:06.840 --> 01:25:08.080]   It's done on blogs.
[01:25:08.080 --> 01:25:09.840]   He's mostly been editor.
[01:25:09.840 --> 01:25:15.360]   A lot of MR is just me as editor or sometimes Alex as editor.
[01:25:15.360 --> 01:25:22.400]   Are you worried that the same format and look of Substack will make people also intellectually
[01:25:22.400 --> 01:25:23.400]   less creative?
[01:25:23.400 --> 01:25:29.240]   I think Substack encourages posts that are too long and too whiny and too self-reflective
[01:25:29.240 --> 01:25:33.480]   and too obsessed with a relatively small number of recurring topics.
[01:25:33.480 --> 01:25:37.240]   So do I worry, but are there enough mechanisms in the whole thing for self-correction?
[01:25:37.240 --> 01:25:38.240]   Obviously.
[01:25:38.240 --> 01:25:40.120]   There's competition.
[01:25:40.120 --> 01:25:42.400]   Readers get sick of stuff that's not great.
[01:25:42.400 --> 01:25:45.300]   It cycles through whatever, it'll be fine.
[01:25:45.300 --> 01:25:48.240]   Is the reason that we've been seeing a decline in research productivity, do you think that
[01:25:48.240 --> 01:25:53.120]   can be explained by the buildup of scientific bureaucracy or is it, I mean, if it's been
[01:25:53.120 --> 01:25:57.720]   consistent over decades, we just have slowly deteriorating research productivity that like
[01:25:57.720 --> 01:26:01.660]   the only explanation can be that we've just picked the low hanging fruits.
[01:26:01.660 --> 01:26:06.520]   I think that's a reason, but I don't think it's the most fundamental reason in this sense.
[01:26:06.520 --> 01:26:10.400]   Maybe like Patrick Collison would see it as more important than I do.
[01:26:10.400 --> 01:26:15.420]   I think exhausting the literal low hanging fruits at the technological level is the main
[01:26:15.420 --> 01:26:20.880]   reason and those you can replenish with new breakthrough tech, general purpose technologies.
[01:26:20.880 --> 01:26:26.240]   That takes a long time, but I see that as the main reason and the ongoing bureaucratization
[01:26:26.240 --> 01:26:27.240]   of science.
[01:26:27.240 --> 01:26:32.340]   I fully accept and want to reform and want to change, but it's not literally my number
[01:26:32.340 --> 01:26:34.660]   one reason for stagnation.
[01:26:34.660 --> 01:26:35.660]   Right.
[01:26:35.660 --> 01:26:39.940]   And is it just like a sine wave that you'll just have periods of easy to grab innovations
[01:26:39.940 --> 01:26:44.380]   and then harder to grab or is there something particular about this stagnation period?
[01:26:44.380 --> 01:26:47.460]   I don't know what the function looks like.
[01:26:47.460 --> 01:26:52.900]   It just seems to me that today we have enough diverse sources of ideas, enough wealth, enough
[01:26:52.900 --> 01:26:58.340]   different universities, research labs, whatever, that it ought not to go too badly.
[01:26:58.340 --> 01:27:03.920]   Like there's an awful lot of conformity, but it doesn't seem that absolute or extreme and
[01:27:03.920 --> 01:27:06.780]   something like MRNA worked, right?
[01:27:06.780 --> 01:27:09.340]   AI is making a lot of progress.
[01:27:09.340 --> 01:27:12.740]   Fusion is being talked about in a serious way.
[01:27:12.740 --> 01:27:15.820]   It doesn't seem that grim to me.
[01:27:15.820 --> 01:27:19.220]   What I find interesting is that you were an optimist in the short term, given this uncertainty,
[01:27:19.220 --> 01:27:23.420]   but in the long term, given the uncertainty about the future of human civilization, you're
[01:27:23.420 --> 01:27:24.420]   a pessimist.
[01:27:24.420 --> 01:27:25.860]   But it's the same view.
[01:27:25.860 --> 01:27:32.480]   So the more "progress" advances, the easier it is to destroy things.
[01:27:32.480 --> 01:27:34.300]   And the chance of an accident can be small.
[01:27:34.300 --> 01:27:38.060]   I think it is small, but again, bad things happen.
[01:27:38.060 --> 01:27:39.980]   Easier to destroy than create.
[01:27:39.980 --> 01:27:40.980]   Right.
[01:27:40.980 --> 01:27:45.820]   Do you have an emotional reaction to the idea that the human story is almost certain to
[01:27:45.820 --> 01:27:46.820]   end?
[01:27:46.820 --> 01:27:47.820]   I don't know.
[01:27:47.820 --> 01:27:49.900]   You've only got 700 years left of this left.
[01:27:49.900 --> 01:27:51.620]   I don't know what I can do about it.
[01:27:51.620 --> 01:27:54.140]   I try actually to do something about it.
[01:27:54.140 --> 01:27:56.580]   So I have a reaction.
[01:27:56.580 --> 01:28:00.700]   But I'm aware of the extreme fallibility embedded in all such projections.
[01:28:00.700 --> 01:28:05.100]   And I'm like, let's just wake up this morning and let's do some stuff now.
[01:28:05.100 --> 01:28:07.140]   And like, I'm going to do it.
[01:28:07.140 --> 01:28:10.020]   And I hope there's a payoff under all these different scenarios.
[01:28:10.020 --> 01:28:11.020]   Yeah.
[01:28:11.020 --> 01:28:13.660]   Does state capacity just continue to decline?
[01:28:13.660 --> 01:28:14.660]   Is there some way?
[01:28:14.660 --> 01:28:15.660]   I don't know that it's declining.
[01:28:15.660 --> 01:28:16.660]   Okay.
[01:28:16.660 --> 01:28:20.940]   It feels like it is, but it's a bit like being in the longest line at the supermarket, like
[01:28:20.940 --> 01:28:23.420]   you notice it more.
[01:28:23.420 --> 01:28:25.660]   And US state has done a bunch of things well.
[01:28:25.660 --> 01:28:30.900]   If you look at the war against terror, I don't know who or what gets the credit, but compared
[01:28:30.900 --> 01:28:38.660]   to what we expected right after 9/11, it's gone okay, right?
[01:28:38.660 --> 01:28:43.540]   Operation Warp Speed went amazingly well.
[01:28:43.540 --> 01:28:47.220]   The local DMV works way better than it used to.
[01:28:47.220 --> 01:28:49.740]   So it's a very complex picture of state capacity.
[01:28:49.740 --> 01:28:53.300]   It's not flat out in decline, I would say.
[01:28:53.300 --> 01:28:55.220]   So what is the explanation for why it gets better over time?
[01:28:55.220 --> 01:28:58.300]   Like there's a public choice theory explanation for why it might get worse, but to the extent
[01:28:58.300 --> 01:28:59.980]   that certain parts of it are getting better over time.
[01:28:59.980 --> 01:29:03.420]   The best explanation I have is so stupid, I'm embarrassed to present it.
[01:29:03.420 --> 01:29:04.420]   What is it?
[01:29:04.420 --> 01:29:07.060]   It's that you have some people in the system who want to make it better and they make it
[01:29:07.060 --> 01:29:08.060]   better.
[01:29:08.060 --> 01:29:09.060]   Uh-huh.
[01:29:09.060 --> 01:29:10.060]   Like not very sophisticated.
[01:29:10.060 --> 01:29:14.820]   But they have some deep way of tracing it to differential incentive structures or just
[01:29:14.820 --> 01:29:15.820]   like demand.
[01:29:15.820 --> 01:29:16.820]   Okay.
[01:29:16.820 --> 01:29:20.580]   And then so are you optimistic about libertarianism in the long term, if like state capacity continues
[01:29:20.580 --> 01:29:21.580]   to get better?
[01:29:21.580 --> 01:29:24.140]   I don't think we'll ever have libertarian societies.
[01:29:24.140 --> 01:29:29.420]   I think there's quite a good chance we will like get more good reforms than bad ones and
[01:29:29.420 --> 01:29:33.820]   capitalism and democracy and broadly classical liberal values will advance.
[01:29:33.820 --> 01:29:34.820]   Okay.
[01:29:34.820 --> 01:29:37.740]   If that's optimistic, I think there's quite a good chance for that.
[01:29:37.740 --> 01:29:42.060]   Is there any difference between you and the Fukuyama view of the end of history that the
[01:29:42.060 --> 01:29:46.180]   capitalist liberal democracy is, uh, there's nothing more compelling than that?
[01:29:46.180 --> 01:29:50.460]   Well, he's changed his view a number of times.
[01:29:50.460 --> 01:29:55.100]   So you can read the original Fukuyama as being quite pessimistic, that there's something
[01:29:55.100 --> 01:30:00.620]   about demand for esteem and self-respect that the end of history doesn't satisfy and so
[01:30:00.620 --> 01:30:01.620]   it unravels.
[01:30:01.620 --> 01:30:05.980]   Then there's the Fukuyama view of the rest of history is all about how we'll manipulate
[01:30:05.980 --> 01:30:09.560]   biology, which seems to me significant.
[01:30:09.560 --> 01:30:13.300]   Maybe he overstated it, but I don't dismiss it at all.
[01:30:13.300 --> 01:30:18.580]   And then it's like all these other Fukuyama restatements since then, it makes me dizzy.
[01:30:18.580 --> 01:30:22.560]   I just asked the simple question, are you long the market or short the market?
[01:30:22.560 --> 01:30:23.620]   I'm long the market.
[01:30:23.620 --> 01:30:24.980]   I don't know what he is.
[01:30:24.980 --> 01:30:28.260]   Very few people are short, so I hope he's long the market too.
[01:30:28.260 --> 01:30:30.560]   It's one of my favorite forcing questions.
[01:30:30.560 --> 01:30:31.820]   Are you long the market?
[01:30:31.820 --> 01:30:33.540]   Are you short the market?
[01:30:33.540 --> 01:30:34.540]   People spew.
[01:30:34.540 --> 01:30:36.300]   Have you ever typed a curse on your podcast?
[01:30:36.300 --> 01:30:40.940]   People spew so much bullshit and it's tribalism and then it's like, are you short the market?
[01:30:40.940 --> 01:30:44.060]   And oh, well, I haven't bought anything lately.
[01:30:44.060 --> 01:30:48.340]   It's like they become morons when you ask them this question.
[01:30:48.340 --> 01:30:52.180]   They should just say, well, that those are my tribalist sympathies.
[01:30:52.180 --> 01:30:55.020]   You know, I'm neurotic and overly stressed.
[01:30:55.020 --> 01:30:59.220]   What I do is pretty optimistic, of course.
[01:30:59.220 --> 01:31:02.700]   But the general fact that people are more neurotic, it seems like Fukuyama isn't right
[01:31:02.700 --> 01:31:09.300]   in the sense that the last man that liberal democracy creates, he's like a neurotic mess.
[01:31:09.300 --> 01:31:14.680]   I think that's how somebody could characterize American politics, at least.
[01:31:14.680 --> 01:31:20.740]   So is he right in a way that humans are not satisfied by liberal democracies?
[01:31:20.740 --> 01:31:23.840]   I don't think people are satisfied by anything in that sense he's right.
[01:31:23.840 --> 01:31:28.380]   I'm not sure it's a special problem for a liberal democracy, probably less so.
[01:31:28.380 --> 01:31:32.500]   There are more other ways to anesthetize yourself.
[01:31:32.500 --> 01:31:34.360]   So there's no entertain yourself.
[01:31:34.360 --> 01:31:36.980]   So there's no form of government or like no structure of society where people are not
[01:31:36.980 --> 01:31:38.300]   just like generally a mess.
[01:31:38.300 --> 01:31:39.300]   I haven't seen it.
[01:31:39.300 --> 01:31:40.300]   Yeah.
[01:31:40.300 --> 01:31:42.500]   Like, when would that have been?
[01:31:42.500 --> 01:31:47.100]   Maybe right after you win a war, there's some kind of giddiness and desire to build for
[01:31:47.100 --> 01:31:48.100]   the future.
[01:31:48.100 --> 01:31:51.420]   But it can't last that long, right?
[01:31:51.420 --> 01:31:55.820]   I'm curious why you continue to read like one of the reasons you say that reading is
[01:31:55.820 --> 01:31:58.580]   fast for you is because you know many of the things that are already in books.
[01:31:58.580 --> 01:31:59.740]   So then why continue doing it?
[01:31:59.740 --> 01:32:01.740]   Like if you already know many of the things that are in there?
[01:32:01.740 --> 01:32:05.260]   Well, it's often frustrating, but I do try to read in new areas.
[01:32:05.260 --> 01:32:10.300]   I very much prefer traveling to reading as a way of learning things, but I can't always
[01:32:10.300 --> 01:32:11.300]   travel.
[01:32:11.300 --> 01:32:13.700]   But at the margin, I would rather travel more and read less.
[01:32:13.700 --> 01:32:14.700]   Absolutely.
[01:32:14.700 --> 01:32:15.700]   And for that reason.
[01:32:15.700 --> 01:32:16.700]   Okay.
[01:32:16.700 --> 01:32:17.700]   Let me ask a meta question.
[01:32:17.700 --> 01:32:19.380]   What do you think podcasts are for?
[01:32:19.380 --> 01:32:21.220]   Like what is happening?
[01:32:21.220 --> 01:32:22.220]   To anesthetize people.
[01:32:22.220 --> 01:32:23.220]   Oh, interesting.
[01:32:23.220 --> 01:32:29.420]   To make them feel they're learning something, to put them to sleep, so they can exercise
[01:32:29.420 --> 01:32:35.100]   and not feel like idiots, occasionally to learn something, just to keep themselves busy
[01:32:35.100 --> 01:32:36.100]   work of some kind.
[01:32:36.100 --> 01:32:39.300]   Is this to say more about the anesthetizing?
[01:32:39.300 --> 01:32:42.620]   You want to feel you're imbibing the most important ideas.
[01:32:42.620 --> 01:32:43.620]   Yes.
[01:32:43.620 --> 01:32:44.620]   Right.
[01:32:44.620 --> 01:32:48.140]   And there's very costly, tough ways to do that, to actually work on one of those problems
[01:32:48.140 --> 01:32:49.140]   as a researcher.
[01:32:49.140 --> 01:32:53.660]   But most people can't do that through no fault of their own, even if they're academics, maybe
[01:32:53.660 --> 01:32:56.540]   they just can't do it.
[01:32:56.540 --> 01:33:01.060]   So like one of the next best things is to listen to someone who at least pretends that
[01:33:01.060 --> 01:33:02.060]   they've done it.
[01:33:02.060 --> 01:33:04.580]   And it's like, okay, it's a substitute.
[01:33:04.580 --> 01:33:05.580]   Like, why not?
[01:33:05.580 --> 01:33:07.140]   What are you supposed to do?
[01:33:07.140 --> 01:33:08.140]   Watch TV?
[01:33:08.140 --> 01:33:09.140]   Okay.
[01:33:09.140 --> 01:33:12.180]   But is your own podcast a compliment, do you think, to actual intellectual inquiry?
[01:33:12.180 --> 01:33:13.420]   I don't know.
[01:33:13.420 --> 01:33:15.260]   I don't assume that it is.
[01:33:15.260 --> 01:33:19.500]   I think of it as a very high class form of entertainment, more than anything.
[01:33:19.500 --> 01:33:20.500]   Right.
[01:33:20.500 --> 01:33:21.500]   Which I like, to be clear.
[01:33:21.500 --> 01:33:22.500]   I don't feel bad about that.
[01:33:22.500 --> 01:33:23.500]   Yeah.
[01:33:23.500 --> 01:33:26.300]   I wonder if the substitute would actually be real engagement or if it would have just
[01:33:26.300 --> 01:33:29.180]   been like, just pure entertainment.
[01:33:29.180 --> 01:33:32.940]   It'd probably be like a lesser podcast, is my guess.
[01:33:32.940 --> 01:33:33.940]   Right.
[01:33:33.940 --> 01:33:34.940]   Yeah.
[01:33:34.940 --> 01:33:35.940]   Yeah.
[01:33:35.940 --> 01:33:37.900]   Well, on that note, Tyler, this was a lot of fun.
[01:33:37.900 --> 01:33:42.260]   And I do want to thank you especially for, you were my fourth guest on the podcast and
[01:33:42.260 --> 01:33:48.100]   having you on early was just a huge help in terms of, you know, growing the podcast.
[01:33:48.100 --> 01:33:50.020]   So happy to be on it again.
[01:33:50.020 --> 01:33:51.020]   I'm extremely grateful.
[01:33:51.020 --> 01:33:52.020]   Yeah.
[01:33:52.020 --> 01:33:53.020]   Thank you for coming by.
[01:33:53.020 --> 01:33:54.020]   Yes.
[01:33:54.020 --> 01:33:55.020]   It's my pleasure.
[01:33:55.020 --> 01:33:56.020]   Entirely.
[01:33:56.020 --> 01:33:57.020]   Great.
[01:33:57.020 --> 01:33:58.020]   Awesome.
[01:33:58.020 --> 01:33:59.580]   Hey, thanks for listening.
[01:33:59.580 --> 01:34:05.220]   If you enjoyed that episode, I would really, really, really appreciate it if you could
[01:34:05.220 --> 01:34:06.380]   share it.
[01:34:06.380 --> 01:34:08.660]   This is still a pretty small podcast.
[01:34:08.660 --> 01:34:14.460]   So it is a huge help when any one of you shares an episode that you like, post it on Twitter,
[01:34:14.460 --> 01:34:18.620]   send it to friends who you think might like it, put it in your group chats, just let the
[01:34:18.620 --> 01:34:19.620]   word go forth.
[01:34:19.620 --> 01:34:21.100]   It helps out a ton.
[01:34:21.100 --> 01:34:29.780]   Many thanks to my amazing editor, Graham Bessalou, for producing this podcast and to Mia Ayana
[01:34:29.780 --> 01:34:35.100]   for creating the amazing transcripts that accompany each episode, which have helpful
[01:34:35.100 --> 01:34:36.100]   links.
[01:34:36.100 --> 01:34:39.620]   And you can find them at the link in the description below.
[01:34:39.620 --> 01:34:43.540]   Remember to subscribe on YouTube and your favorite podcast platforms.
[01:34:43.540 --> 01:34:44.540]   Cheers.
[01:34:44.540 --> 01:34:52.580]   See you next time.
[01:34:52.580 --> 01:34:57.580]   [Music]

