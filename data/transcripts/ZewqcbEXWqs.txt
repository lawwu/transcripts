
[00:00:00.000 --> 00:00:05.080]   It's almost Christmas in the world of AI, and what else could that mean, other than
[00:00:05.080 --> 00:00:11.520]   another step forward in the march toward lifelike text image, rumors of an imminent extra ten
[00:00:11.520 --> 00:00:17.480]   years of health span funded by Sam Altman, and new state-of-the-art models being trained
[00:00:17.480 --> 00:00:20.440]   behind the scenes, presumably by elves.
[00:00:20.440 --> 00:00:25.920]   But let's start briefly with the new Mid Journey version 6 announced in the last few
[00:00:25.920 --> 00:00:26.920]   days.
[00:00:26.920 --> 00:00:30.120]   I've been playing about with it for far too many hours, and the main difference that
[00:00:30.120 --> 00:00:36.320]   I can notice with version 5.2 is that it adheres to your prompts far more sensitively.
[00:00:36.320 --> 00:00:40.800]   I could give you dozens of examples, but take this Roman Triumphal Arch.
[00:00:40.800 --> 00:00:45.440]   I recently saw one of these in person, but the problem with Mid Journey version 5 is
[00:00:45.440 --> 00:00:51.480]   that it might capture the main topic of your prompt, but misses out on key relational details,
[00:00:51.480 --> 00:00:54.840]   like something being bigger than, next to, or on top of.
[00:00:54.840 --> 00:00:59.960]   Version 5 also misses out on parts of the prompt, like here the stream running through
[00:00:59.960 --> 00:01:02.120]   the center of the arch.
[00:01:02.120 --> 00:01:07.000]   With version 6, at least one of the outputs will likely adhere to what you're asking
[00:01:07.000 --> 00:01:08.000]   for.
[00:01:08.000 --> 00:01:12.960]   Of course, the difference with even earlier versions of Mid Journey are even more stark.
[00:01:12.960 --> 00:01:20.080]   Look at the progress just since 2022, and that's before I even upscale the v6 output.
[00:01:20.080 --> 00:01:24.520]   We really are getting tantalizingly close to photorealism.
[00:01:24.520 --> 00:01:28.880]   But now let me show you the upscaled version of that image using Magnific.
[00:01:28.880 --> 00:01:32.040]   Talk about getting pretty close to photorealism.
[00:01:32.040 --> 00:01:36.640]   And no, this is not sponsored, but honestly, I found it really fun to play about with Magnific,
[00:01:36.640 --> 00:01:38.480]   albeit it's a little bit pricey.
[00:01:38.480 --> 00:01:43.380]   Basically because all you have to do is drag and drop your Dali 3 or Mid Journey v6 image
[00:01:43.380 --> 00:01:46.160]   into the top left, and then basically scroll down.
[00:01:46.160 --> 00:01:49.560]   I don't even think you have to change any of the settings to get amazing results, and
[00:01:49.560 --> 00:01:52.040]   then you just go to upscale, and then it's done.
[00:01:52.040 --> 00:01:57.920]   It takes less than a minute and it turns the AI slightly glossy images into something much
[00:01:57.920 --> 00:02:00.440]   more lifelike, photorealistic.
[00:02:00.440 --> 00:02:04.760]   But while we're on the topic of tips, here's another one that I found really useful.
[00:02:04.760 --> 00:02:10.120]   Instead of adding words like photorealism or 4k, 8k to your Mid Journey prompt, try
[00:02:10.120 --> 00:02:16.960]   --style raw, and you'll notice that the images go from, again, a kind of glossy, clearly
[00:02:16.960 --> 00:02:22.240]   AI style image, and they become much more photorealistic already, before you even try
[00:02:22.240 --> 00:02:23.240]   Magnific.
[00:02:23.240 --> 00:02:29.760]   Dali 3 is still, for me, a little bit better at text, but they try to avoid photorealism.
[00:02:29.760 --> 00:02:35.240]   My guess is because OpenAI want people to know if an image is AI generated, and is of
[00:02:35.240 --> 00:02:37.360]   course watermarking its images.
[00:02:37.360 --> 00:02:42.320]   Now yes, I did experiment with putting Dali 3 images through Magnific, and it kind of
[00:02:42.320 --> 00:02:43.320]   works.
[00:02:43.320 --> 00:02:50.560]   So I put Dali 3 Roman Arch over a quiet stream, and then this is after one round of upscaling.
[00:02:50.560 --> 00:02:55.160]   And of course, with the magic of the new Suno AI web app, you can turn your prompt into
[00:02:55.160 --> 00:03:20.720]   a backing track.
[00:03:20.720 --> 00:03:25.680]   Then throw in a bit of runway ML to get some movement.
[00:03:25.680 --> 00:03:32.120]   And at this point, it doesn't take much imagination to see where the industry is going.
[00:03:32.120 --> 00:03:36.880]   Photorealism has almost reached text to image, and it's coming for text to video.
[00:03:36.880 --> 00:03:42.480]   But there are a few more things to get to before the end of this Christmas episode of
[00:03:42.480 --> 00:03:43.680]   AI Explained.
[00:03:43.680 --> 00:03:49.000]   Sam Altman, the CEO of OpenAI, yesterday came out with this list of the things he wished
[00:03:49.000 --> 00:03:50.400]   someone had told him.
[00:03:50.400 --> 00:03:54.520]   I'm just going to pick up on three of them as they relate to new stories that have broken
[00:03:54.520 --> 00:03:56.120]   in the last few days.
[00:03:56.120 --> 00:04:01.840]   The first one being how he mentioned concentrating his resources on a small number of high conviction
[00:04:01.840 --> 00:04:02.840]   bets.
[00:04:02.840 --> 00:04:06.920]   And it seems on this front, Sam Altman is putting his money where his mouth is.
[00:04:06.920 --> 00:04:13.000]   He has personally invested $180 million on healthy lifespan extension.
[00:04:13.000 --> 00:04:16.800]   The life extension company, Retro, is quick to point out that that's not about living
[00:04:16.800 --> 00:04:17.800]   forever.
[00:04:17.800 --> 00:04:19.960]   It's about adding 10 good years to your life.
[00:04:19.960 --> 00:04:24.800]   It was a fascinating read in Bloomberg, and of course, I can't get to all the details,
[00:04:24.800 --> 00:04:29.680]   but the company has just one investor, that's Sam Altman, CEO of OpenAI, who's invested
[00:04:29.680 --> 00:04:31.560]   that $180 million.
[00:04:31.560 --> 00:04:36.080]   Now I don't know how I missed this, but back in March, MIT Technology Review reported that
[00:04:36.080 --> 00:04:41.400]   Sam Altman had emptied his bank account betting on energy and extended lifespan.
[00:04:41.400 --> 00:04:42.480]   He said, "It's a lot.
[00:04:42.480 --> 00:04:46.640]   I basically just took all my liquid net worth and put it into these two companies."
[00:04:46.640 --> 00:04:51.320]   Retro is betting on five different tracks, thanks to Sam Altman being willing to do something
[00:04:51.320 --> 00:04:55.280]   different and throw lots of money at a bunch of things in parallel.
[00:04:55.280 --> 00:05:00.560]   Their main bet is on partial cell reprogramming, and I read several papers to understand it
[00:05:00.560 --> 00:05:04.120]   before making this video, but I still can't fully summarize it.
[00:05:04.120 --> 00:05:05.400]   Nevertheless, here's my best attempt.
[00:05:05.400 --> 00:05:10.440]   In a nutshell, what it's doing is reprogramming mature cells by introducing a set of what's
[00:05:10.440 --> 00:05:15.920]   called transcription factor genes, basically full of special instructions, which turn those
[00:05:15.920 --> 00:05:20.900]   mature cells into something more youthful without changing their type or function or
[00:05:20.900 --> 00:05:22.240]   making them stem cells.
[00:05:22.240 --> 00:05:25.760]   A bit like changing the code of a program, not the function.
[00:05:25.760 --> 00:05:29.680]   Now, any biologists watching this can let me know if that was a good summary, but the
[00:05:29.680 --> 00:05:35.040]   main purpose of including this article in this video was this paragraph.
[00:05:35.040 --> 00:05:40.000]   Retro, the company, and its peers really do think that this time is different.
[00:05:40.000 --> 00:05:44.800]   Many researchers in the field contend that the science behind cell reprogramming in particular
[00:05:44.800 --> 00:05:48.640]   has been solved and that therapies are now an engineering problem.
[00:05:48.640 --> 00:05:54.680]   They see full-on age reversal as not only achievable, but also perhaps imminent.
[00:05:54.680 --> 00:05:59.320]   Sam Ullman thinks what's needed is an open AI type effort in longevity.
[00:05:59.320 --> 00:06:04.400]   Now, I'm old enough to have been reading longevity research for over a decade, so I
[00:06:04.400 --> 00:06:06.200]   have heard claims like this before.
[00:06:06.200 --> 00:06:10.680]   In fact, I remember reading an article about Calico Company being formed over 10 years
[00:06:10.680 --> 00:06:11.680]   ago.
[00:06:11.680 --> 00:06:13.360]   That was funded in part by Larry Page.
[00:06:13.360 --> 00:06:17.960]   They've been working on increasing healthy lifespan for over a decade, and I haven't
[00:06:17.960 --> 00:06:21.680]   seen much literature on the success of their work.
[00:06:21.680 --> 00:06:26.440]   And even the Bloomberg article reports that Sam Ullman will not be able to fund this all
[00:06:26.440 --> 00:06:29.280]   the way, according to the Retro co-founder.
[00:06:29.280 --> 00:06:33.480]   Getting to the final products will require more investors and going public at some point.
[00:06:33.480 --> 00:06:37.120]   But I haven't seen language like things being imminent before.
[00:06:37.120 --> 00:06:41.760]   And even the PubMed article that I read while researching for this video used language that
[00:06:41.760 --> 00:06:43.180]   was pretty interesting.
[00:06:43.180 --> 00:06:48.480]   In the final sentence, it said, "It remains to be seen whether cycles of epigenetic rejuvenation
[00:06:48.480 --> 00:06:55.400]   by age reprogramming could confer biological immortality in mammals, including humans."
[00:06:55.400 --> 00:06:59.600]   But as many of you may know, it's not just Sam Ullman in big tech who's focusing on
[00:06:59.600 --> 00:07:04.600]   longevity and also, by the way, personally taking metformin to boost his own health span.
[00:07:04.600 --> 00:07:09.120]   There's also Jeff Bezos, who has pumped billions into Altos Labs.
[00:07:09.120 --> 00:07:13.460]   One of the leaders of that lab is also using some dramatic language.
[00:07:13.460 --> 00:07:19.060]   He said last year that within two decades, we will be able to prevent diseases and ageing.
[00:07:19.060 --> 00:07:23.300]   Now, of course, the point of this Christmas video is not to go deep into the science and
[00:07:23.300 --> 00:07:24.960]   test out all of these theories.
[00:07:24.960 --> 00:07:29.700]   It's more to describe the shift in language that I've been seeing in recent months and
[00:07:29.700 --> 00:07:30.700]   years.
[00:07:30.700 --> 00:07:34.060]   As I say, I've been following the literature for more than a decade, and it's the first
[00:07:34.060 --> 00:07:37.740]   time I've heard things like imminence being seriously spoken of.
[00:07:37.740 --> 00:07:43.060]   And even setting aside the possibility that something like AGI could speed up this research,
[00:07:43.060 --> 00:07:48.500]   the 2030s are more and more being spoken of as the decade that could be revolutionary
[00:07:48.500 --> 00:07:49.500]   for healthspan.
[00:07:49.500 --> 00:07:54.060]   That's healthy lifespan as opposed to just living longer in your hundreds, decrepit.
[00:07:54.060 --> 00:07:57.060]   Of course, not everyone shares the excitement of living indefinitely.
[00:07:57.060 --> 00:08:01.340]   Here's one OpenAI employee who might have an interesting chat with Sam Ullman.
[00:08:01.340 --> 00:08:06.200]   He said, "In the pursuit of immortality, we are losing the benefits of Darwinism.
[00:08:06.200 --> 00:08:10.020]   Things dying and new things being born brings health to the ecosystem.
[00:08:10.020 --> 00:08:15.580]   I feel that for the misguided quest of stability, we often keep things alive longer than they
[00:08:15.580 --> 00:08:16.580]   should be."
[00:08:16.580 --> 00:08:21.100]   And it does seem like there is a big split in big tech about whether the goal of living
[00:08:21.100 --> 00:08:24.420]   for longer is one that we should pursue in the first place.
[00:08:24.420 --> 00:08:29.460]   I personally wonder what Santa Claus has to say, who is himself presumably immortal.
[00:08:29.460 --> 00:08:34.860]   Anyway, back to Sam Ullman who said, "Fight BS and bureaucracy every time you see it and
[00:08:34.860 --> 00:08:36.780]   get other people to fight it too.
[00:08:36.780 --> 00:08:40.980]   Kind of ironic in the light of what happened with the OpenAI board, but nevertheless, do
[00:08:40.980 --> 00:08:44.800]   not let the org chart get in the way of people working productively together."
[00:08:44.800 --> 00:08:49.820]   And there is one more point on this list which nicely dovetails with the next news item that
[00:08:49.820 --> 00:08:50.820]   I wanted to mention.
[00:08:50.820 --> 00:08:55.740]   Toward the end, Sam Ullman wrote this, "Compounding exponentials are magic.
[00:08:55.740 --> 00:09:00.540]   In particular, you really want to build a business that gets a compounding advantage
[00:09:00.540 --> 00:09:01.540]   with scale."
[00:09:01.540 --> 00:09:05.900]   And that was straight after saying that scale often has surprising emergent properties.
[00:09:05.900 --> 00:09:10.660]   The question of course though, is whether it's OpenAI who will get that compounding
[00:09:10.660 --> 00:09:15.660]   advantage of scale, or maybe is it going to be that old titan, Google?
[00:09:15.660 --> 00:09:19.460]   Just this week, we learned from the information that according to one person familiar with
[00:09:19.460 --> 00:09:23.740]   the matter, Google is already training its next big model, Gemini 2.
[00:09:23.740 --> 00:09:28.660]   So they're training Gemini 2 before they've even released Gemini 1 Ultra.
[00:09:28.660 --> 00:09:32.200]   But how does Gemini 2 relate to compounding scale?
[00:09:32.200 --> 00:09:37.500]   Well, according to a report from Semianalysis, the compute power available to Google is set
[00:09:37.500 --> 00:09:41.020]   to far outstrip OpenAI in the near future.
[00:09:41.020 --> 00:09:45.260]   As they put it, "Even when giving OpenAI every benefit of the doubt, Google's compute
[00:09:45.260 --> 00:09:47.820]   capabilities make everyone else look silly."
[00:09:47.820 --> 00:09:52.860]   Google will quite literally have more TPUV5s, that's Tensor Processing Units that they're
[00:09:52.860 --> 00:09:58.500]   using to train Gemini 2, than OpenAI, Meta, CoreWeave, Oracle and Amazon will have
[00:09:58.500 --> 00:10:00.100]   GPUs combined.
[00:10:00.100 --> 00:10:05.260]   They go on, "Of course, on performance per chip, there is a significant deficit for TPUV5
[00:10:05.260 --> 00:10:06.260]   versus H100.
[00:10:06.260 --> 00:10:11.380]   But even when shaking that out, OpenAI's compute is a fraction of Google's, and that
[00:10:11.380 --> 00:10:16.980]   the incredible TPUV5 build-out is going to drive significantly more training and inference
[00:10:16.980 --> 00:10:19.980]   capacity than anyone else on the planet."
[00:10:19.980 --> 00:10:26.300]   So it seems certain that whether we're talking about Gemini 2, GPT-5, Healthspan or Photorealism,
[00:10:26.300 --> 00:10:29.300]   2024 looks set to be interesting.
[00:10:29.300 --> 00:10:34.300]   One thing we can all agree on though, is that working with great people is one of the best
[00:10:34.300 --> 00:10:35.300]   parts of life.
[00:10:35.300 --> 00:10:39.500]   And for me, that's all of you who watch these videos, even if you don't like or
[00:10:39.500 --> 00:10:40.500]   comment.
[00:10:40.500 --> 00:10:44.580]   I really do appreciate every view, every comment in this Christmas season and all year.
[00:10:44.580 --> 00:10:49.780]   A special thanks of course to all of the AI Insiders and Legendary Supporters who keep
[00:10:49.780 --> 00:10:51.940]   the channel going on Patreon.
[00:10:51.940 --> 00:10:55.380]   A massive and enduring thanks to all of you.
[00:10:55.380 --> 00:10:58.500]   So from me, thanks for watching all the way to the end.
[00:10:58.500 --> 00:11:01.940]   Have a very Merry Christmas and a wonderful day.

