
[00:00:00.000 --> 00:00:02.560]   The following is a conversation with Jonathan Haidt,
[00:00:02.560 --> 00:00:04.720]   social psychologist at NYU
[00:00:04.720 --> 00:00:07.480]   and critic of the negative effects of social media
[00:00:07.480 --> 00:00:10.480]   on the human mind and human civilization.
[00:00:10.480 --> 00:00:13.740]   He gives a respectful but hard hitting response
[00:00:13.740 --> 00:00:16.040]   to my conversation with Mark Zuckerberg.
[00:00:16.040 --> 00:00:18.560]   And together, him and I try to figure out
[00:00:18.560 --> 00:00:20.200]   how we can do better,
[00:00:20.200 --> 00:00:22.560]   how we can lessen the amount of depression
[00:00:22.560 --> 00:00:24.960]   and division in the world.
[00:00:24.960 --> 00:00:28.760]   He has brilliantly discussed these topics in his writing,
[00:00:28.760 --> 00:00:32.240]   including in his book, "The Coddling of the American Mind"
[00:00:32.240 --> 00:00:35.560]   and in his recent long article in "The Atlantic"
[00:00:35.560 --> 00:00:39.120]   titled, "Why the Past 10 Years of American Life
[00:00:39.120 --> 00:00:41.800]   Have Been Uniquely Stupid."
[00:00:41.800 --> 00:00:45.160]   When Teddy Roosevelt said in his famous speech
[00:00:45.160 --> 00:00:47.680]   that it is not the critic who counts,
[00:00:47.680 --> 00:00:48.960]   he has not yet read
[00:00:48.960 --> 00:00:51.320]   the brilliant writing of Jonathan Haidt.
[00:00:51.320 --> 00:00:53.680]   I disagree with John on some of the details
[00:00:53.680 --> 00:00:55.320]   of his analysis and ideas,
[00:00:55.320 --> 00:01:00.320]   but both his criticism and our disagreement is essential
[00:01:00.320 --> 00:01:02.200]   if we are to build better
[00:01:02.200 --> 00:01:04.640]   and better technologies that connect us.
[00:01:04.640 --> 00:01:08.520]   Social media has both the power to destroy our society
[00:01:08.520 --> 00:01:10.560]   and to help it flourish.
[00:01:10.560 --> 00:01:14.140]   It's up to us to figure out how we take the lighter path.
[00:01:14.140 --> 00:01:17.120]   This is the Lex Friedman Podcast.
[00:01:17.120 --> 00:01:18.080]   To support it,
[00:01:18.080 --> 00:01:20.760]   please check out our sponsors in the description.
[00:01:20.760 --> 00:01:24.360]   And now, dear friends, here's Jonathan Haidt.
[00:01:25.360 --> 00:01:27.560]   - So you have been thinking about the human mind
[00:01:27.560 --> 00:01:29.320]   for quite a long time.
[00:01:29.320 --> 00:01:31.000]   You wrote "The Happiness Hypothesis,"
[00:01:31.000 --> 00:01:33.640]   "The Righteous Mind," "The Coddling of the American Mind,"
[00:01:33.640 --> 00:01:35.680]   and today you're thinking, you're writing a lot
[00:01:35.680 --> 00:01:38.960]   about social media and about democracy.
[00:01:38.960 --> 00:01:42.120]   So perhaps if it's okay,
[00:01:42.120 --> 00:01:46.120]   let's go through the thread that connects all of that work.
[00:01:46.120 --> 00:01:49.280]   How do we get from the very beginning to today
[00:01:49.280 --> 00:01:53.180]   with the good, the bad, and the ugly of social media?
[00:01:53.180 --> 00:01:55.120]   So I'm a social psychologist,
[00:01:55.120 --> 00:01:59.640]   which means I study how we think about other people
[00:01:59.640 --> 00:02:01.760]   and how people affect our thinking.
[00:02:01.760 --> 00:02:04.560]   And in graduate school at the University of Pennsylvania,
[00:02:04.560 --> 00:02:07.220]   I picked the topic of moral psychology,
[00:02:07.220 --> 00:02:10.920]   and I studied how morality varied across countries.
[00:02:10.920 --> 00:02:13.040]   I studied in Brazil and India.
[00:02:13.040 --> 00:02:15.120]   And in the '90s, I began,
[00:02:15.120 --> 00:02:17.840]   this was like I got my PhD in 1992.
[00:02:17.840 --> 00:02:21.280]   And in that decade was really when the American culture war
[00:02:21.280 --> 00:02:23.740]   kind of really began to blow up.
[00:02:23.740 --> 00:02:26.400]   And I began to notice that left and right in this country
[00:02:26.400 --> 00:02:28.540]   were becoming like separate countries.
[00:02:28.540 --> 00:02:30.300]   And you could use the tools of cultural psychology
[00:02:30.300 --> 00:02:31.980]   to study this split,
[00:02:31.980 --> 00:02:34.780]   this moral battle between left and right.
[00:02:34.780 --> 00:02:36.100]   So I started doing that.
[00:02:36.100 --> 00:02:39.700]   And I began growing alarmed in the early 2000s
[00:02:39.700 --> 00:02:42.340]   about how bad polarization was getting.
[00:02:42.340 --> 00:02:46.520]   And I began studying the causes of polarization,
[00:02:46.520 --> 00:02:50.240]   bringing moral psychology to bear on our political problems.
[00:02:50.240 --> 00:02:52.940]   And I was originally gonna write a book
[00:02:52.940 --> 00:02:55.540]   to basically help the Democrats stop screwing up,
[00:02:55.540 --> 00:02:58.460]   because I could see that some of my research showed
[00:02:58.460 --> 00:03:01.860]   people on the right understand people on the left,
[00:03:01.860 --> 00:03:02.780]   they know what they think.
[00:03:02.780 --> 00:03:03.980]   You can't grow up in America
[00:03:03.980 --> 00:03:06.060]   without knowing what progressives think.
[00:03:06.060 --> 00:03:08.220]   But here I grew up generally on the left,
[00:03:08.220 --> 00:03:10.060]   and I had no idea what conservatives thought
[00:03:10.060 --> 00:03:11.660]   until I went and sought it out
[00:03:11.660 --> 00:03:13.100]   and started reading conservative things
[00:03:13.100 --> 00:03:14.160]   like National Review.
[00:03:14.160 --> 00:03:18.180]   So originally I wanted to actually help the Democrats
[00:03:18.180 --> 00:03:19.500]   to understand moral psychology
[00:03:19.500 --> 00:03:21.840]   so they could stop losing to George W. Bush.
[00:03:21.840 --> 00:03:24.380]   And I got a contract to write "The Righteous Mind."
[00:03:24.380 --> 00:03:25.500]   And once I started writing it,
[00:03:25.500 --> 00:03:27.840]   I committed to understanding conservatives
[00:03:27.840 --> 00:03:30.060]   by reading the best writings, not the worst.
[00:03:30.060 --> 00:03:31.540]   And I discovered, you know what?
[00:03:31.540 --> 00:03:32.580]   You don't understand anything
[00:03:32.580 --> 00:03:34.600]   until you look from multiple perspectives.
[00:03:34.600 --> 00:03:36.380]   And I discovered there are a lot of great
[00:03:36.380 --> 00:03:37.580]   social science ideas
[00:03:37.580 --> 00:03:39.860]   in the conservative intellectual tradition.
[00:03:39.860 --> 00:03:42.320]   And I also began to see, you know what?
[00:03:42.320 --> 00:03:43.980]   America's actually in real trouble.
[00:03:43.980 --> 00:03:45.420]   And this is like 2008, 2009.
[00:03:45.420 --> 00:03:47.560]   Things are really, we're coming apart here.
[00:03:47.560 --> 00:03:49.420]   So I began to really focus my research
[00:03:49.420 --> 00:03:51.660]   on helping left and right understand each other
[00:03:51.660 --> 00:03:54.820]   and helping our democratic institutions to work better.
[00:03:54.820 --> 00:03:56.680]   Okay, so all this is before I had any interest
[00:03:56.680 --> 00:03:57.520]   in social media.
[00:03:57.520 --> 00:03:59.620]   I was on Twitter, I guess like 2009,
[00:03:59.620 --> 00:04:02.220]   and not much, didn't think about it much.
[00:04:02.220 --> 00:04:05.900]   And then, so I'm going along
[00:04:05.900 --> 00:04:07.900]   as a social psychologist studying this.
[00:04:07.900 --> 00:04:10.540]   And then everything seems to kind of blow up
[00:04:10.540 --> 00:04:13.620]   in 2014, 2015 at universities.
[00:04:13.620 --> 00:04:16.380]   And that's when Greg Lukianoff came to me
[00:04:16.380 --> 00:04:17.860]   in May of 2014 and said,
[00:04:17.860 --> 00:04:20.500]   "John, weird stuff is happening.
[00:04:20.500 --> 00:04:22.260]   Students are freaking out
[00:04:22.260 --> 00:04:24.340]   about a speaker coming to campus
[00:04:24.340 --> 00:04:25.780]   that they don't have to go see.
[00:04:25.780 --> 00:04:27.700]   And they're saying it's dangerous, it's violence.
[00:04:27.700 --> 00:04:28.940]   Like what is going on?"
[00:04:28.940 --> 00:04:31.060]   And so anyway, Greg's ideas
[00:04:31.060 --> 00:04:33.220]   about how we were teaching students
[00:04:33.220 --> 00:04:34.300]   to think in distorted ways,
[00:04:34.300 --> 00:04:37.020]   that led us to write the "Coddling the American Mind,"
[00:04:37.020 --> 00:04:38.740]   which wasn't primarily about social media either.
[00:04:38.740 --> 00:04:39.740]   It was about, you know,
[00:04:39.740 --> 00:04:42.820]   this sort of a rise of depression, anxiety.
[00:04:42.820 --> 00:04:45.780]   But after that, things got so much worse everywhere.
[00:04:45.780 --> 00:04:47.100]   And that's when I began to think like,
[00:04:47.100 --> 00:04:49.180]   whoa, something systemically has changed.
[00:04:49.180 --> 00:04:50.020]   Something has changed
[00:04:50.020 --> 00:04:52.020]   about the fabric of the social universe.
[00:04:52.020 --> 00:04:52.860]   And so ever since then,
[00:04:52.860 --> 00:04:54.820]   I've been focused on social media.
[00:04:54.820 --> 00:04:57.340]   - So we're going to try to sneak up
[00:04:57.340 --> 00:04:59.340]   to the problems and the solutions at hand
[00:04:59.340 --> 00:05:00.860]   from different directions.
[00:05:00.860 --> 00:05:02.180]   I have a lot of questions
[00:05:02.180 --> 00:05:05.140]   whether it's fundamentally the nature of social media
[00:05:05.140 --> 00:05:05.980]   that's the problem,
[00:05:05.980 --> 00:05:08.780]   it's the decisions of various human beings
[00:05:08.780 --> 00:05:11.360]   that lead the social media companies that's the problem.
[00:05:11.360 --> 00:05:12.960]   Is there still some component
[00:05:12.960 --> 00:05:16.020]   that's highlighted in the "Coddling of the American Mind"
[00:05:16.020 --> 00:05:18.180]   that's the individual psychology at play
[00:05:18.180 --> 00:05:22.400]   or the way parenting and education works
[00:05:22.400 --> 00:05:27.400]   to make sort of emphasize anti-fragility of the human mind
[00:05:27.400 --> 00:05:31.300]   as it interacts with the social media platforms
[00:05:31.300 --> 00:05:32.820]   and the other humans through the social.
[00:05:32.820 --> 00:05:34.220]   So all that beautiful mess.
[00:05:34.220 --> 00:05:36.300]   - That should take us an hour or two to cover.
[00:05:36.300 --> 00:05:37.940]   - Or maybe a couple of years, yes.
[00:05:37.940 --> 00:05:40.120]   But so let's start if it's okay.
[00:05:40.120 --> 00:05:43.180]   You said you wanted to challenge some of the things
[00:05:43.180 --> 00:05:44.920]   that Mark Zuckerberg has said
[00:05:44.920 --> 00:05:46.620]   in a conversation with me.
[00:05:46.620 --> 00:05:48.620]   What are some of the ideas he expressed
[00:05:48.620 --> 00:05:49.500]   that you disagree with?
[00:05:49.500 --> 00:05:51.780]   - Okay, there are two major areas that I study.
[00:05:51.780 --> 00:05:54.300]   One is what is happening with teen mental health?
[00:05:54.300 --> 00:05:57.580]   It fell off a cliff in 2013, it was very sudden.
[00:05:57.580 --> 00:06:00.140]   And then the other is what is happening
[00:06:00.140 --> 00:06:02.860]   to our democratic and epistemic institutions?
[00:06:02.860 --> 00:06:04.140]   That means knowledge generating
[00:06:04.140 --> 00:06:07.060]   like universities, journalism.
[00:06:07.060 --> 00:06:09.300]   So my main areas of research
[00:06:09.300 --> 00:06:10.700]   where I'm collecting the empirical research
[00:06:10.700 --> 00:06:12.020]   and trying to make sense of it
[00:06:12.020 --> 00:06:14.300]   is what's happened to teen mental health
[00:06:14.300 --> 00:06:17.360]   and what's the evidence that social media is a contributor?
[00:06:17.360 --> 00:06:18.760]   And then the other areas,
[00:06:18.760 --> 00:06:22.320]   what's happening to democracies, not just America,
[00:06:22.320 --> 00:06:23.760]   and what's the evidence that social media
[00:06:23.760 --> 00:06:25.520]   is a contributor to the dysfunction?
[00:06:25.520 --> 00:06:26.800]   So I'm sure we'll get to that
[00:06:26.800 --> 00:06:28.820]   'cause that's what the Atlantic article is about.
[00:06:28.820 --> 00:06:30.360]   But if we focus first on
[00:06:30.360 --> 00:06:32.400]   what's happened to teen mental health.
[00:06:32.400 --> 00:06:34.440]   So before I read the quotes from Mark,
[00:06:34.440 --> 00:06:36.360]   I'd like to just give the overview.
[00:06:36.360 --> 00:06:39.720]   And it is this.
[00:06:39.720 --> 00:06:43.560]   There's a lot of data tracking adolescents.
[00:06:43.560 --> 00:06:47.040]   There's self-reports of how depressed, anxious, lonely.
[00:06:47.040 --> 00:06:49.120]   There's data on hospital admissions for self-harm.
[00:06:49.120 --> 00:06:50.740]   There's data on suicide.
[00:06:50.740 --> 00:06:53.720]   And all of these things, they bounce around somewhat,
[00:06:53.720 --> 00:06:56.840]   but they're relatively level in the early 2000s.
[00:06:56.840 --> 00:07:00.520]   And then all of a sudden, around 2010 to 2013,
[00:07:00.520 --> 00:07:02.680]   depending on which statistic you're looking at,
[00:07:02.680 --> 00:07:06.280]   all of a sudden, they begin to shoot upwards.
[00:07:06.280 --> 00:07:08.040]   More so for girls in some cases,
[00:07:08.040 --> 00:07:10.760]   but on the whole, it's like up for both sexes.
[00:07:10.760 --> 00:07:12.100]   It's just that boys have lower levels
[00:07:12.100 --> 00:07:13.080]   of anxiety and depression.
[00:07:13.080 --> 00:07:15.240]   So the curve is not quite as dramatic.
[00:07:15.240 --> 00:07:17.600]   But what we see is not small increases.
[00:07:17.600 --> 00:07:20.240]   It's not like, oh, 10%, 20%.
[00:07:20.240 --> 00:07:24.640]   No, the increases are between 50 and 150%,
[00:07:24.640 --> 00:07:26.800]   depending on which group you're looking at.
[00:07:26.800 --> 00:07:31.560]   Suicide for preteen girls, thankfully, it's not very common,
[00:07:31.560 --> 00:07:34.000]   but it's two to three times more common now.
[00:07:34.000 --> 00:07:36.600]   Or by 2015, it had doubled.
[00:07:36.600 --> 00:07:38.680]   Between 2010 and 2015, it doubled.
[00:07:38.680 --> 00:07:40.600]   So something is going radically wrong
[00:07:40.600 --> 00:07:42.880]   in the world of American preteens.
[00:07:42.880 --> 00:07:46.360]   So as I've been studying it, I found, first of all,
[00:07:46.360 --> 00:07:47.520]   it's not just America.
[00:07:47.520 --> 00:07:50.800]   It's identical in Canada and the UK.
[00:07:50.800 --> 00:07:52.320]   Australia and New Zealand are very similar.
[00:07:52.320 --> 00:07:53.800]   They're just after a little delay.
[00:07:53.800 --> 00:07:55.000]   So whatever we're looking for here,
[00:07:55.000 --> 00:07:58.240]   but yet it's not as clear in the Germanic countries.
[00:07:58.240 --> 00:08:00.120]   In continental Europe, it's a little different,
[00:08:00.120 --> 00:08:02.640]   and we can get into that when we talk about childhood.
[00:08:02.640 --> 00:08:05.640]   But something's happening in many countries,
[00:08:05.640 --> 00:08:08.920]   and it started right around 2012, 2013.
[00:08:08.920 --> 00:08:10.520]   It wasn't gradual.
[00:08:10.520 --> 00:08:11.520]   It hit girls hardest,
[00:08:11.520 --> 00:08:13.920]   and it hit preteen girls the hardest.
[00:08:13.920 --> 00:08:15.640]   So what could it be?
[00:08:15.640 --> 00:08:18.400]   Nobody has come up with another explanation.
[00:08:18.400 --> 00:08:19.280]   Nobody.
[00:08:19.280 --> 00:08:20.760]   It wasn't the financial crisis.
[00:08:20.760 --> 00:08:23.760]   That wouldn't have hit preteen girls the hardest.
[00:08:23.760 --> 00:08:25.080]   There is no other explanation.
[00:08:25.080 --> 00:08:27.360]   The complexity here in the data is, of course,
[00:08:27.360 --> 00:08:30.520]   as everyone knows, correlation doesn't prove causation.
[00:08:30.520 --> 00:08:32.480]   The fact that television viewing was going up
[00:08:32.480 --> 00:08:35.360]   in the '60s and '70s doesn't mean
[00:08:35.360 --> 00:08:37.080]   that that was the cause of the crime.
[00:08:37.080 --> 00:08:40.480]   So what I've done, and this is Rook with Jean Twenge,
[00:08:40.480 --> 00:08:44.280]   who wrote the book "iGen," is because I was challenged,
[00:08:44.280 --> 00:08:46.400]   when Greg and I put out the book,
[00:08:46.400 --> 00:08:47.800]   "The Coddling of the American Mind,"
[00:08:47.800 --> 00:08:48.960]   some researchers challenged us and said,
[00:08:48.960 --> 00:08:50.760]   "Oh, you don't know what you're talking about.
[00:08:50.760 --> 00:08:52.960]   "The correlations between social media use
[00:08:52.960 --> 00:08:55.500]   "and mental health, they exist, but they're tiny.
[00:08:55.500 --> 00:08:59.000]   "It's like a correlation coefficient of .03,
[00:08:59.000 --> 00:09:02.360]   "or a beta of .05, tiny little things."
[00:09:02.360 --> 00:09:03.960]   And one famous article said,
[00:09:03.960 --> 00:09:06.160]   "It's no bigger than the correlation
[00:09:06.160 --> 00:09:10.080]   "of bad mental health and eating potatoes,"
[00:09:10.080 --> 00:09:14.480]   which exists, but it's so tiny it's zero, essentially.
[00:09:14.480 --> 00:09:17.400]   And that claim, that social media's no more harmful
[00:09:17.400 --> 00:09:19.400]   than eating potatoes or wearing eyeglasses,
[00:09:19.400 --> 00:09:21.680]   it was a very catchy claim, and it's caught on,
[00:09:21.680 --> 00:09:22.880]   and I keep hearing that.
[00:09:22.880 --> 00:09:25.320]   But let me unpack why that's not true,
[00:09:25.320 --> 00:09:26.340]   and then we'll get to what Mark said.
[00:09:26.340 --> 00:09:27.760]   'Cause what Mark basically said,
[00:09:27.760 --> 00:09:29.200]   here, I'll actually read it.
[00:09:29.200 --> 00:09:31.160]   - And by the way, just to pause real quick,
[00:09:31.160 --> 00:09:35.240]   is you implied, but just to make it explicit,
[00:09:35.240 --> 00:09:36.880]   that the best explanation we have now,
[00:09:36.880 --> 00:09:39.880]   as you're proposing, is that a very particular aspect
[00:09:39.880 --> 00:09:42.280]   of social media is the cause,
[00:09:42.280 --> 00:09:43.600]   which is not just social media,
[00:09:43.600 --> 00:09:46.440]   but the like button and the retweet,
[00:09:46.440 --> 00:09:51.120]   a certain mechanism of virality that was invented,
[00:09:51.120 --> 00:09:53.360]   or perhaps some aspect of social media is the cause.
[00:09:53.360 --> 00:09:54.320]   - Okay, good idea.
[00:09:54.320 --> 00:09:55.400]   Let's be clear.
[00:09:55.400 --> 00:09:56.960]   Connecting people is good.
[00:09:56.960 --> 00:10:00.080]   I mean, overall, the more you connect people, the better.
[00:10:00.080 --> 00:10:02.940]   Giving people the telephone was an amazing step forward.
[00:10:02.940 --> 00:10:05.560]   Giving them free telephone, free long distance,
[00:10:05.560 --> 00:10:06.400]   is even better.
[00:10:06.400 --> 00:10:08.360]   Video is, I mean, so connecting people is good.
[00:10:08.360 --> 00:10:10.120]   I'm not a Luddite.
[00:10:10.120 --> 00:10:14.120]   And social media, at least the idea of users posting things,
[00:10:14.120 --> 00:10:16.080]   like that happens on LinkedIn, and it's great.
[00:10:16.080 --> 00:10:17.900]   It can serve all kinds of needs.
[00:10:17.900 --> 00:10:20.520]   What I'm talking about here is not the internet.
[00:10:20.520 --> 00:10:21.840]   It's not technology.
[00:10:21.840 --> 00:10:23.360]   It's not smartphones.
[00:10:23.360 --> 00:10:25.940]   And it's not even all social media.
[00:10:25.940 --> 00:10:28.200]   It's a particular business model
[00:10:28.200 --> 00:10:31.760]   in which people are incentivized to create content.
[00:10:31.760 --> 00:10:34.200]   And that content is what brings other people on.
[00:10:34.200 --> 00:10:36.480]   And the people on there are the product
[00:10:36.480 --> 00:10:38.160]   which is sold to advertisers.
[00:10:38.160 --> 00:10:39.680]   It's that particular business model
[00:10:39.680 --> 00:10:41.880]   which Facebook pioneered,
[00:10:41.880 --> 00:10:45.480]   which seems to be incredibly harmful for teenagers,
[00:10:45.480 --> 00:10:47.940]   especially for young girls, 10 to 14 years old
[00:10:47.940 --> 00:10:50.040]   is where they're most vulnerable.
[00:10:50.040 --> 00:10:51.720]   And it seems to be particularly harmful
[00:10:51.720 --> 00:10:53.320]   for democratic institutions
[00:10:53.320 --> 00:10:55.560]   because it leads to all kinds of anger, conflict,
[00:10:55.560 --> 00:10:57.520]   and the destruction of any shared narrative.
[00:10:57.520 --> 00:10:59.000]   So that's what we're talking about.
[00:10:59.000 --> 00:11:00.560]   We're talking about Facebook, Twitter.
[00:11:00.560 --> 00:11:01.920]   I don't have any data on TikTok.
[00:11:01.920 --> 00:11:03.680]   I suspect it's gonna end up being,
[00:11:03.680 --> 00:11:05.680]   having a lot of really bad effects
[00:11:05.680 --> 00:11:07.000]   because the teens are on it so much.
[00:11:07.000 --> 00:11:08.160]   And to be really clear,
[00:11:08.160 --> 00:11:10.320]   since we're doing the nuance now in this section,
[00:11:10.320 --> 00:11:11.720]   lots of good stuff happens.
[00:11:11.720 --> 00:11:14.840]   There's a lot of funny things on Twitter.
[00:11:14.840 --> 00:11:17.520]   I use Twitter because it's an amazing way to put out news,
[00:11:17.520 --> 00:11:19.520]   to put out when I write something,
[00:11:19.520 --> 00:11:22.280]   you and I use it to promote things.
[00:11:22.280 --> 00:11:24.680]   We learn things quickly.
[00:11:24.680 --> 00:11:25.520]   - Well, there's gonna be,
[00:11:25.520 --> 00:11:28.000]   now this is harder to measure.
[00:11:28.000 --> 00:11:29.600]   And we'll probably,
[00:11:29.600 --> 00:11:30.720]   I'll try to mention it
[00:11:30.720 --> 00:11:32.760]   because so much of our conversation
[00:11:32.760 --> 00:11:34.400]   will be about rigorous criticism.
[00:11:34.400 --> 00:11:37.200]   I'll try to sometimes mention
[00:11:37.200 --> 00:11:39.760]   what are the possible positive effects of social media
[00:11:39.760 --> 00:11:40.840]   in different ways.
[00:11:40.840 --> 00:11:45.840]   So for example, in the way I've been using Twitter,
[00:11:45.840 --> 00:11:48.280]   not the promotion or any of that kind of stuff,
[00:11:48.280 --> 00:11:53.280]   it makes me feel less lonely to connect with people,
[00:11:53.280 --> 00:11:56.800]   to make me smile, a little bit of humor here and there.
[00:11:56.800 --> 00:11:59.160]   And that at scale is a very interesting effect,
[00:11:59.160 --> 00:12:00.760]   being connected across the globe,
[00:12:00.760 --> 00:12:03.240]   especially during times of COVID and so on.
[00:12:03.240 --> 00:12:05.000]   It's very difficult to measure that.
[00:12:05.000 --> 00:12:08.840]   So we kind of have to consider that and be honest.
[00:12:08.840 --> 00:12:10.960]   There is a trade-off.
[00:12:10.960 --> 00:12:13.560]   We have to be honest about the positive and the negative.
[00:12:13.560 --> 00:12:16.360]   And sometimes we're not sufficiently positive
[00:12:16.360 --> 00:12:18.880]   or in a rigorous scientific way about the,
[00:12:18.880 --> 00:12:22.760]   we're not rigorous in a scientific way about the negative.
[00:12:22.760 --> 00:12:25.280]   And that's what we're trying to do here.
[00:12:25.280 --> 00:12:31.040]   And so that brings us to the Mark Zuckerberg email.
[00:12:31.040 --> 00:12:31.880]   - Okay, but wait,
[00:12:31.880 --> 00:12:33.280]   let me just pick up on the issue of trade-offs
[00:12:33.280 --> 00:12:35.680]   because people might think like,
[00:12:35.680 --> 00:12:37.200]   well, how much of this do we need?
[00:12:37.200 --> 00:12:38.280]   If we have too much, it's bad.
[00:12:38.280 --> 00:12:42.120]   No, that's a one-dimensional conceptualization.
[00:12:42.120 --> 00:12:44.040]   This is a multi-dimensional issue.
[00:12:44.040 --> 00:12:45.160]   And a lot of people seem to think like,
[00:12:45.160 --> 00:12:46.280]   oh, what would we have done
[00:12:46.280 --> 00:12:47.480]   without social media during COVID?
[00:12:47.480 --> 00:12:49.840]   Like we would have been sitting there alone in our homes.
[00:12:49.840 --> 00:12:54.840]   Yeah, if all we had was texting, telephone, Zoom, Skype,
[00:12:54.840 --> 00:12:57.260]   multiplayer video games, WhatsApp,
[00:12:57.260 --> 00:13:00.120]   all sorts of ways of communicating with each other.
[00:13:00.120 --> 00:13:02.600]   Oh, and there's blogs and the rest of the internet.
[00:13:02.600 --> 00:13:03.680]   Yeah, we would have been fine.
[00:13:03.680 --> 00:13:06.680]   Did we really need the hyper-viral platforms
[00:13:06.680 --> 00:13:07.660]   of Facebook and Twitter?
[00:13:07.660 --> 00:13:09.960]   Now those did help certain things get out faster.
[00:13:09.960 --> 00:13:12.040]   And that did help science Twitter sometimes,
[00:13:12.040 --> 00:13:14.540]   but it also led to huge explosions of misinformation
[00:13:14.540 --> 00:13:17.740]   and the polarization of our politics to such an extent
[00:13:17.740 --> 00:13:19.600]   that a third of the country, you know,
[00:13:19.600 --> 00:13:22.520]   didn't believe what the medical establishment was saying.
[00:13:22.520 --> 00:13:23.660]   And we'll get into this.
[00:13:23.660 --> 00:13:25.300]   The medical establishment sometimes
[00:13:25.300 --> 00:13:27.960]   was playing political games that made them less credible.
[00:13:27.960 --> 00:13:30.080]   So on net, it's not clear to me,
[00:13:30.080 --> 00:13:33.280]   if you've got the internet, smartphones, blogs,
[00:13:33.280 --> 00:13:36.360]   all of that stuff, it's not clear to me
[00:13:36.360 --> 00:13:39.400]   that adding in this particular business model
[00:13:39.400 --> 00:13:40.920]   of Facebook, Twitter, TikTok,
[00:13:40.920 --> 00:13:43.280]   that that really adds a lot more.
[00:13:43.280 --> 00:13:46.840]   - And one interesting one we'll also talk about is YouTube.
[00:13:46.840 --> 00:13:50.240]   I think it's easier to talk about Twitter and Facebook.
[00:13:50.240 --> 00:13:53.640]   YouTube is another complex beast that's very hard to,
[00:13:53.640 --> 00:13:54.960]   'cause YouTube has many things.
[00:13:54.960 --> 00:13:56.120]   It's a content platform,
[00:13:56.120 --> 00:13:58.200]   but it also has a recommendation system.
[00:13:59.480 --> 00:14:02.600]   That's, let's focus our discussion
[00:14:02.600 --> 00:14:04.240]   on perhaps Twitter and Facebook,
[00:14:04.240 --> 00:14:06.560]   but you do in this large document
[00:14:06.560 --> 00:14:09.600]   that you're putting together on social media
[00:14:09.600 --> 00:14:11.640]   called "Social Media and Political Dysfunction
[00:14:11.640 --> 00:14:14.160]   Collaborative Review" with Chris Bale.
[00:14:14.160 --> 00:14:18.040]   That includes, I believe, papers on YouTube as well.
[00:14:18.040 --> 00:14:19.120]   - It does, but yeah, again,
[00:14:19.120 --> 00:14:20.600]   just to finish up with the nuance,
[00:14:20.600 --> 00:14:23.440]   yeah, YouTube is really complicated
[00:14:23.440 --> 00:14:25.280]   because I can't imagine life without YouTube.
[00:14:25.280 --> 00:14:26.520]   It's incredibly useful.
[00:14:26.520 --> 00:14:28.260]   It does a lot of good things.
[00:14:28.260 --> 00:14:30.600]   It also obviously helps to radicalize
[00:14:30.600 --> 00:14:32.440]   terrorist groups and murderers.
[00:14:32.440 --> 00:14:34.840]   So I think about YouTube
[00:14:34.840 --> 00:14:37.020]   the way I think about the internet in general,
[00:14:37.020 --> 00:14:39.200]   and I don't know enough to really comment on YouTube.
[00:14:39.200 --> 00:14:42.000]   So I have been focused, and it's also interesting.
[00:14:42.000 --> 00:14:45.640]   One thing we know is teen social life changed radically
[00:14:45.640 --> 00:14:47.760]   between about 2010 and 2012.
[00:14:47.760 --> 00:14:49.920]   Before 2010, they weren't mostly on every day
[00:14:49.920 --> 00:14:51.680]   'cause they didn't have smartphones yet.
[00:14:51.680 --> 00:14:53.680]   By 2012 to '14, that's the area
[00:14:53.680 --> 00:14:55.720]   in which they almost all get smartphones,
[00:14:55.720 --> 00:14:58.780]   and they become daily users of the, what?
[00:14:58.780 --> 00:15:00.820]   So the girls go to Instagram and Tumblr.
[00:15:00.820 --> 00:15:02.380]   They go to the visual ones.
[00:15:02.380 --> 00:15:04.680]   The boys go to YouTube and video games.
[00:15:04.680 --> 00:15:06.540]   Those don't seem to be as harmful to mental health
[00:15:06.540 --> 00:15:07.860]   or even harmful at all.
[00:15:07.860 --> 00:15:11.560]   It's really Tumblr, Instagram particularly,
[00:15:11.560 --> 00:15:14.300]   that seem to really have done in girls' mental health.
[00:15:14.300 --> 00:15:16.380]   So now, okay, so let's look at the quote
[00:15:16.380 --> 00:15:18.600]   from Mark Zuckerberg.
[00:15:18.600 --> 00:15:23.600]   So at 64 minutes and 31 seconds on the video,
[00:15:23.600 --> 00:15:25.060]   I time-coded this.
[00:15:25.060 --> 00:15:26.180]   - This is excellent.
[00:15:26.180 --> 00:15:28.620]   - This is the very helpful YouTube transcript.
[00:15:28.620 --> 00:15:30.240]   YouTube's an amazing program.
[00:15:30.240 --> 00:15:33.100]   You ask him about Francis Haugen.
[00:15:33.100 --> 00:15:34.760]   You give him a chance to respond.
[00:15:34.760 --> 00:15:37.500]   And here's the key thing.
[00:15:37.500 --> 00:15:41.300]   So he talks about what Francis Haugen said.
[00:15:41.300 --> 00:15:42.940]   He said, "No, but that's mischaracterized.
[00:15:42.940 --> 00:15:45.360]   "Actually, on most measures, the kids are doing better
[00:15:45.360 --> 00:15:46.200]   "when they're on Instagram.
[00:15:46.200 --> 00:15:48.500]   "It's just on one out of the 18."
[00:15:48.500 --> 00:15:52.140]   And then he says, "I think an accurate characterization
[00:15:52.140 --> 00:15:55.380]   "would have been that kids using Instagram,"
[00:15:55.380 --> 00:15:56.760]   or not kids, but teens,
[00:15:56.760 --> 00:15:59.420]   "is generally positive for their mental health."
[00:15:59.420 --> 00:16:03.420]   That's his claim, that Instagram is overall,
[00:16:03.420 --> 00:16:05.640]   taken as a whole, Instagram is positive
[00:16:05.640 --> 00:16:06.480]   for their mental health.
[00:16:06.480 --> 00:16:07.500]   That's what he says.
[00:16:07.500 --> 00:16:10.500]   Now, is it really?
[00:16:10.500 --> 00:16:11.660]   Is it really?
[00:16:11.660 --> 00:16:14.060]   So first, just a simple, okay, now here,
[00:16:14.060 --> 00:16:16.340]   what I'd like to do is turn my attention
[00:16:16.340 --> 00:16:18.540]   to another document that we'll make available.
[00:16:18.540 --> 00:16:20.540]   So I was invited to give testimony
[00:16:20.540 --> 00:16:22.900]   before a Senate subcommittee two weeks ago,
[00:16:22.900 --> 00:16:23.820]   where they were considering
[00:16:23.820 --> 00:16:25.140]   the Platform Accountability Act.
[00:16:25.140 --> 00:16:27.220]   Should we force the platforms to actually tell us
[00:16:27.220 --> 00:16:28.620]   what our kids are doing?
[00:16:28.620 --> 00:16:30.580]   Like, we have no idea, other than self-report.
[00:16:30.580 --> 00:16:32.260]   We have no idea.
[00:16:32.260 --> 00:16:34.240]   They're the only ones who know, like, the kid does this,
[00:16:34.240 --> 00:16:36.860]   and then over the next hours, the kid's depressed or happy.
[00:16:36.860 --> 00:16:39.860]   We can't know that, but Facebook knows it.
[00:16:39.860 --> 00:16:42.700]   So should they be compelled to reveal the data?
[00:16:42.700 --> 00:16:43.700]   We need that.
[00:16:43.700 --> 00:16:47.940]   - So you raised just, to give people a little bit of context,
[00:16:47.940 --> 00:16:51.740]   and this document is brilliantly structured with questions,
[00:16:51.740 --> 00:16:54.940]   studies that indicate that the answer to a question is yes,
[00:16:54.940 --> 00:16:57.180]   indicate that the answer to a question is no,
[00:16:57.180 --> 00:16:58.140]   and then mixed results.
[00:16:58.140 --> 00:17:00.340]   And questions include things like,
[00:17:00.340 --> 00:17:02.180]   does social media make people more angry
[00:17:02.180 --> 00:17:03.700]   or effectively polarized?
[00:17:03.700 --> 00:17:06.140]   - Right, wait, so that's the one that we're gonna get to.
[00:17:06.140 --> 00:17:07.900]   That's the one for democracy.
[00:17:07.900 --> 00:17:08.900]   - Yes, that's for democracy.
[00:17:08.900 --> 00:17:10.540]   - So I've got three different Google Docs here,
[00:17:10.540 --> 00:17:11.860]   'cause I found this is an amazing way,
[00:17:11.860 --> 00:17:13.220]   and thank God for Google Docs.
[00:17:13.220 --> 00:17:16.060]   It's an amazing way to organize the research literature,
[00:17:16.060 --> 00:17:17.660]   and it's a collaborative review,
[00:17:17.660 --> 00:17:19.220]   meaning that, so on this one,
[00:17:19.220 --> 00:17:21.300]   Gene Twenge and I put up the first draft,
[00:17:21.300 --> 00:17:23.580]   and we say, please, comment, add studies,
[00:17:23.580 --> 00:17:24.660]   tell us what we missed.
[00:17:24.660 --> 00:17:25.860]   And it evolves in real time.
[00:17:25.860 --> 00:17:27.700]   - In any direction, the yes or the no.
[00:17:27.700 --> 00:17:29.060]   - Oh yeah, we specifically encourage,
[00:17:29.060 --> 00:17:31.980]   'cause look, the center of my research
[00:17:31.980 --> 00:17:34.500]   is that our gut feelings drive our reasoning.
[00:17:34.500 --> 00:17:37.180]   That was my dissertation, that was my early research.
[00:17:37.180 --> 00:17:39.620]   And so if Gene Twenge and I are committed to,
[00:17:39.620 --> 00:17:41.940]   but we're gonna obviously preferentially believe
[00:17:41.940 --> 00:17:43.540]   that these platforms are bad for kids,
[00:17:43.540 --> 00:17:44.980]   'cause we said so in our books.
[00:17:44.980 --> 00:17:47.020]   So we have confirmation bias,
[00:17:47.020 --> 00:17:49.300]   and I'm a devotee of John Stuart Mill,
[00:17:49.300 --> 00:17:51.300]   the only cure for confirmation bias
[00:17:51.300 --> 00:17:53.780]   is other people who have a different confirmation bias.
[00:17:53.780 --> 00:17:55.940]   So these documents evolve because critics then say,
[00:17:55.940 --> 00:17:57.160]   no, you missed this, or they say,
[00:17:57.160 --> 00:17:58.180]   you don't know what you're talking about.
[00:17:58.180 --> 00:17:59.880]   It's like, great, say so, tell us.
[00:17:59.880 --> 00:18:02.340]   So I put together this document,
[00:18:02.340 --> 00:18:05.340]   and I'm gonna put links to everything on my website.
[00:18:05.340 --> 00:18:11.900]   If listeners, viewers go to jonathanheit.com/socialmedia.
[00:18:11.900 --> 00:18:13.700]   It's a new page I just created.
[00:18:13.700 --> 00:18:15.620]   I'll put everything together in one place there,
[00:18:15.620 --> 00:18:17.220]   and we'll put those in the show notes.
[00:18:17.220 --> 00:18:19.180]   - Like links to this document,
[00:18:19.180 --> 00:18:21.140]   and other things like it that we're talking about.
[00:18:21.140 --> 00:18:22.380]   - That's right, exactly.
[00:18:22.380 --> 00:18:24.740]   So yeah, so the thing I wanna call attention to now
[00:18:24.740 --> 00:18:27.580]   is this document here with the title,
[00:18:27.580 --> 00:18:29.340]   Teen Mental Health is Plummeting,
[00:18:29.340 --> 00:18:32.220]   and Social Media is a Major Contributing Cause.
[00:18:32.220 --> 00:18:34.980]   So Ben Sass and Chris Coons are on the Judiciary Committee.
[00:18:34.980 --> 00:18:36.420]   They had a subcommittee hearing
[00:18:36.420 --> 00:18:38.900]   on Nate Priscilli's bill,
[00:18:38.900 --> 00:18:40.580]   Platform Accountability Transparency Act.
[00:18:40.580 --> 00:18:42.740]   So they asked me to testify on what do we know,
[00:18:42.740 --> 00:18:44.340]   what's going on with teen mental health?
[00:18:44.340 --> 00:18:46.340]   And so what I did was I put together everything I know
[00:18:46.340 --> 00:18:49.940]   with plenty of graphs to make these points.
[00:18:49.940 --> 00:18:52.040]   That first, what do we know about the crisis?
[00:18:52.040 --> 00:18:55.300]   Well, that the crisis is specific to mood disorders,
[00:18:55.300 --> 00:18:56.420]   not everything else.
[00:18:56.420 --> 00:18:59.300]   It's not just self-report, it's also behavioral data,
[00:18:59.300 --> 00:19:02.940]   because suicide and self-harm go skyrocketing after 2010.
[00:19:02.940 --> 00:19:05.460]   The increases are very large,
[00:19:05.460 --> 00:19:08.780]   and the crisis is gendered, and it's hit many countries.
[00:19:08.780 --> 00:19:10.300]   So I go through the data on that.
[00:19:10.300 --> 00:19:12.340]   So we have a pretty clear characterization,
[00:19:12.340 --> 00:19:14.540]   and nobody's disputed me on this part.
[00:19:14.540 --> 00:19:16.060]   - So can we just pause real quick,
[00:19:16.060 --> 00:19:17.900]   just so for people who are not aware.
[00:19:17.900 --> 00:19:20.780]   So self-report, just how you kind of collect data
[00:19:20.780 --> 00:19:21.980]   on this kind of thing.
[00:19:21.980 --> 00:19:22.820]   - Sure.
[00:19:22.820 --> 00:19:26.380]   - You have a self-report, a survey, you ask people--
[00:19:26.380 --> 00:19:29.340]   - Yeah, how anxious are you these days?
[00:19:29.340 --> 00:19:30.180]   - Yeah.
[00:19:30.180 --> 00:19:31.860]   - How many hours a week do you use social media?
[00:19:31.860 --> 00:19:32.700]   - That kind of stuff.
[00:19:32.700 --> 00:19:35.680]   And you do, it's maybe,
[00:19:35.680 --> 00:19:38.640]   you can collect large amounts of data that way,
[00:19:38.640 --> 00:19:40.060]   'cause you can ask a large number of people
[00:19:40.060 --> 00:19:41.020]   that kind of question.
[00:19:41.020 --> 00:19:43.260]   And but then there's, I forget the term you use,
[00:19:43.260 --> 00:19:46.300]   but more, so non-self-report data.
[00:19:46.300 --> 00:19:47.140]   - Behavioral data.
[00:19:47.140 --> 00:19:48.260]   - Behavioral data, that's right.
[00:19:48.260 --> 00:19:53.260]   Where you actually have self-harm and suicide numbers.
[00:19:53.260 --> 00:19:54.660]   - Exactly.
[00:19:54.660 --> 00:19:55.740]   So there are a lot of graphs like this.
[00:19:55.740 --> 00:19:58.300]   So this is from the National Survey on Drug Use and Health.
[00:19:58.300 --> 00:20:01.900]   So the federal government, and also Pew and Gallup,
[00:20:01.900 --> 00:20:03.360]   there are a lot of organizations
[00:20:03.360 --> 00:20:05.740]   that have been collecting survey data for decades.
[00:20:05.740 --> 00:20:07.020]   So this is a gold mine.
[00:20:07.020 --> 00:20:09.300]   And what you see on these graphs over and over again
[00:20:09.300 --> 00:20:12.500]   is relatively straight lines up until around 2010 or 2012.
[00:20:12.500 --> 00:20:14.180]   - And on the X-axis, we have time,
[00:20:14.180 --> 00:20:17.380]   years going from 2004 to 2020.
[00:20:17.380 --> 00:20:20.340]   And the Y-axis is the percent of US teens
[00:20:20.340 --> 00:20:22.540]   who had a major depression in the last year.
[00:20:22.540 --> 00:20:23.380]   - That's right.
[00:20:23.380 --> 00:20:24.900]   So when this data started coming out around,
[00:20:24.900 --> 00:20:27.220]   so Jean Twain's book, "I, Jen," 2017,
[00:20:27.220 --> 00:20:29.040]   a lot of people said, "Oh, she doesn't know
[00:20:29.040 --> 00:20:29.880]   "what she's talking about.
[00:20:29.880 --> 00:20:30.980]   "This is just self-report."
[00:20:30.980 --> 00:20:32.740]   Like, Gen Z, they're just really comfortable
[00:20:32.740 --> 00:20:33.700]   talking about this.
[00:20:33.700 --> 00:20:34.780]   This is a good thing.
[00:20:34.780 --> 00:20:36.300]   This isn't a real epidemic.
[00:20:36.300 --> 00:20:39.420]   And literally the day before my book with Greg was published,
[00:20:39.420 --> 00:20:42.380]   the day before, there was a psychiatrist in "New York Times"
[00:20:42.380 --> 00:20:44.520]   who had an op-ed saying, "Relax.
[00:20:44.520 --> 00:20:47.060]   "Smartphones are not ruining your kid's brain."
[00:20:47.060 --> 00:20:48.200]   And he said, "It's just self-report.
[00:20:48.200 --> 00:20:50.080]   "It's just that they're giving higher rates.
[00:20:50.080 --> 00:20:50.900]   "There's more diagnosis.
[00:20:50.900 --> 00:20:52.460]   "But underlying, there's no change."
[00:20:52.460 --> 00:20:56.720]   No, because it's theoretically possible,
[00:20:56.720 --> 00:20:59.380]   but all we have to do is look at the hospitalization data
[00:20:59.380 --> 00:21:03.140]   for self-harm and suicide, and we see the exact same trends.
[00:21:03.140 --> 00:21:06.500]   We see also a very sudden, big rise
[00:21:06.500 --> 00:21:08.660]   around between 2009 and 2012.
[00:21:08.660 --> 00:21:10.660]   You have an elbow, and then it goes up, up, up.
[00:21:10.660 --> 00:21:11.900]   So, and that is not self-report.
[00:21:11.900 --> 00:21:13.700]   Those are actual kids admitted to hospitals
[00:21:13.700 --> 00:21:15.460]   for cutting themselves.
[00:21:15.460 --> 00:21:18.860]   So we have a catastrophe, and this was all true before COVID.
[00:21:18.860 --> 00:21:21.700]   COVID made things worse, but we have to realize,
[00:21:21.700 --> 00:21:22.920]   you know, COVID's going away.
[00:21:22.920 --> 00:21:25.900]   Kids are back in school, but we're not gonna go back
[00:21:25.900 --> 00:21:29.780]   to where we were because this problem is not caused by COVID.
[00:21:29.780 --> 00:21:31.620]   What is it caused by?
[00:21:31.620 --> 00:21:33.900]   Well, just again, to just go through the point,
[00:21:33.900 --> 00:21:34.740]   then I'll stop.
[00:21:34.740 --> 00:21:38.140]   I just feel like I just really wanna get out the data
[00:21:38.140 --> 00:21:40.300]   to show that Mark is wrong. - This is amazing.
[00:21:40.300 --> 00:21:42.180]   - So first point, correlational studies
[00:21:42.180 --> 00:21:43.520]   consistently show a link.
[00:21:43.520 --> 00:21:45.740]   They almost all do, but it's not big.
[00:21:45.740 --> 00:21:47.580]   Equivalent to a correlation coefficient
[00:21:47.580 --> 00:21:49.440]   around 0.1, typically.
[00:21:49.440 --> 00:21:51.380]   That's the first point.
[00:21:51.380 --> 00:21:55.340]   The second point is that the correlation
[00:21:55.340 --> 00:21:58.020]   is actually much larger than for eating potatoes.
[00:21:58.020 --> 00:22:01.780]   So that famous line wasn't about social media use.
[00:22:01.780 --> 00:22:04.300]   That was about digital media use.
[00:22:04.300 --> 00:22:05.960]   That included watching Netflix,
[00:22:05.960 --> 00:22:07.540]   doing homework on everything.
[00:22:07.540 --> 00:22:10.780]   And so what they did is they looked at all screen use,
[00:22:10.780 --> 00:22:12.380]   and then they said, "This is correlated
[00:22:12.380 --> 00:22:15.220]   "with self-reports of depression, anxiety."
[00:22:15.220 --> 00:22:17.840]   Like, you know, 0.03, it's tiny.
[00:22:17.840 --> 00:22:20.300]   And they said that clearly in the paper,
[00:22:20.300 --> 00:22:24.860]   but the media has reported it as social media is 0.03,
[00:22:24.860 --> 00:22:26.980]   or tiny, and that's just not true.
[00:22:26.980 --> 00:22:28.300]   What I found digging into it,
[00:22:28.300 --> 00:22:29.940]   you don't know this until you look at the,
[00:22:29.940 --> 00:22:32.520]   there's more than 100 studies in the Google Doc.
[00:22:32.520 --> 00:22:34.260]   Once you dig in, what you see is,
[00:22:34.260 --> 00:22:36.400]   okay, you see a tiny correlation.
[00:22:36.400 --> 00:22:38.420]   What happens if we zoom in on just social media?
[00:22:38.420 --> 00:22:40.180]   It always gets bigger, often a lot bigger,
[00:22:40.180 --> 00:22:42.020]   two or three times bigger.
[00:22:42.020 --> 00:22:44.460]   What happens if we zoom in on girls and social media?
[00:22:44.460 --> 00:22:46.380]   It always gets bigger, often a lot bigger.
[00:22:46.380 --> 00:22:48.840]   And so what I think we can conclude,
[00:22:48.840 --> 00:22:49.980]   in fact, what one of the authors
[00:22:49.980 --> 00:22:52.060]   of the potato studies herself concludes,
[00:22:52.060 --> 00:22:55.060]   Amy Orban says, I think I have a quote from here,
[00:22:55.060 --> 00:22:56.660]   she reviewed a lot of studies,
[00:22:56.660 --> 00:22:59.020]   and she herself said that, quote,
[00:22:59.020 --> 00:23:02.060]   "The associations between social media use and wellbeing
[00:23:02.060 --> 00:23:07.060]   therefore range from about R equals 0.15 to R equals 0.10."
[00:23:07.060 --> 00:23:09.860]   So that's the range we're talking about.
[00:23:09.860 --> 00:23:11.900]   And that's for boys and girls together.
[00:23:11.900 --> 00:23:14.220]   And a lot of research, including hers and mine,
[00:23:14.220 --> 00:23:15.700]   show that girls, it's higher.
[00:23:15.700 --> 00:23:17.160]   So for girls, we're talking about correlations
[00:23:17.160 --> 00:23:19.500]   around 0.15 to 0.2, I believe.
[00:23:19.500 --> 00:23:22.600]   Gene Twenge and I found it's about 0.2 or 0.22.
[00:23:22.600 --> 00:23:24.780]   Now this might sound like an arcane social science debate,
[00:23:24.780 --> 00:23:26.240]   but people have to understand,
[00:23:26.240 --> 00:23:29.180]   public health correlations are almost never above 0.2.
[00:23:29.180 --> 00:23:31.900]   So the correlation of childhood exposure to lead
[00:23:31.900 --> 00:23:35.220]   and adult IQ, very serious problem, that's 0.09.
[00:23:35.220 --> 00:23:37.900]   Like the world's messy and our measurements are messy.
[00:23:37.900 --> 00:23:41.180]   And so if you find a consistent correlation of 0.15,
[00:23:41.180 --> 00:23:43.900]   like you would never let your kid do that thing.
[00:23:43.900 --> 00:23:45.180]   That actually is dangerous.
[00:23:45.180 --> 00:23:47.500]   And it can explain, when you multiply it
[00:23:47.500 --> 00:23:50.280]   over tens of millions of kids spending,
[00:23:50.280 --> 00:23:51.980]   you know, years of their lives,
[00:23:51.980 --> 00:23:54.060]   you actually can explain the mental health epidemic
[00:23:54.060 --> 00:23:56.900]   just from social media use.
[00:23:56.900 --> 00:23:58.300]   - Well, and then there's questions.
[00:23:58.300 --> 00:24:00.780]   By the way, this is really good to learn
[00:24:00.780 --> 00:24:04.140]   because I quit potatoes and it had no effect on me.
[00:24:04.140 --> 00:24:06.260]   And as a Russian, that was a big sacrifice.
[00:24:06.260 --> 00:24:08.220]   They're quite literal actually,
[00:24:08.220 --> 00:24:10.020]   because I'm mostly eating keto these days.
[00:24:10.020 --> 00:24:12.700]   But that's funny that they're actually
[00:24:12.700 --> 00:24:14.540]   literally called the potato studies.
[00:24:14.540 --> 00:24:17.380]   Okay, but given this,
[00:24:17.380 --> 00:24:19.820]   and there's a lot of fascinating data here,
[00:24:19.820 --> 00:24:22.820]   there's also a discussion of how to fix it.
[00:24:23.820 --> 00:24:28.820]   What are the aspects that if fixed
[00:24:28.820 --> 00:24:32.420]   would start to reverse some of these trends?
[00:24:32.420 --> 00:24:37.420]   So if we just linger on the Mark Zuckerberg statements.
[00:24:37.420 --> 00:24:40.420]   So first of all, do you think Mark is aware
[00:24:40.420 --> 00:24:43.200]   of some of these studies?
[00:24:43.200 --> 00:24:48.200]   So if you put yourself in the shoes of Mark Zuckerberg
[00:24:48.200 --> 00:24:51.500]   and the executives at Facebook and Twitter,
[00:24:51.500 --> 00:24:53.720]   how can you try to understand the studies?
[00:24:53.720 --> 00:24:55.580]   Like the Google Docs you put together
[00:24:55.580 --> 00:24:59.340]   to try to make decisions that fix things?
[00:24:59.340 --> 00:25:02.060]   Is there a stable science now
[00:25:02.060 --> 00:25:03.740]   that you can start to investigate?
[00:25:03.740 --> 00:25:06.780]   And also maybe if you can comment
[00:25:06.780 --> 00:25:08.660]   on the depth of data that's available,
[00:25:08.660 --> 00:25:11.600]   because ultimately, and this is something you argue,
[00:25:11.600 --> 00:25:14.380]   that the data should be more transparent,
[00:25:14.380 --> 00:25:15.740]   should be provided.
[00:25:15.740 --> 00:25:18.980]   But currently, if it's not,
[00:25:18.980 --> 00:25:22.460]   all you have is maybe some leaks of internal data.
[00:25:22.460 --> 00:25:23.620]   - That's right, that's right.
[00:25:23.620 --> 00:25:25.460]   And we could talk about the potential.
[00:25:25.460 --> 00:25:27.200]   You have to be very sort of objective
[00:25:27.200 --> 00:25:29.700]   about the potential bias in those kinds of leaks.
[00:25:29.700 --> 00:25:33.700]   You want to, it would be nice to have a non-leak data.
[00:25:33.700 --> 00:25:35.180]   Like--
[00:25:35.180 --> 00:25:36.780]   - Yeah, it'd be nice to be able to actually
[00:25:36.780 --> 00:25:39.140]   have academic researchers able to access
[00:25:39.140 --> 00:25:41.620]   in de-individuated, de-identified form
[00:25:41.620 --> 00:25:44.560]   the actual data on what kids are doing
[00:25:44.560 --> 00:25:45.940]   and how their mood changes.
[00:25:45.940 --> 00:25:48.780]   And when people commit suicide, what was happening before.
[00:25:48.780 --> 00:25:49.660]   And it'd be great to know that.
[00:25:49.660 --> 00:25:51.060]   We have no idea.
[00:25:51.060 --> 00:25:55.620]   - So how do we begin to fix social media, would you say?
[00:25:55.620 --> 00:25:58.220]   - Okay, so here's the most important thing to understand.
[00:25:58.220 --> 00:26:00.700]   In the social sciences, we say,
[00:26:00.700 --> 00:26:02.140]   is social media harmful to kids?
[00:26:02.140 --> 00:26:03.320]   That's a broad question.
[00:26:03.320 --> 00:26:04.780]   You can't answer that directly.
[00:26:04.780 --> 00:26:06.640]   You have to have much more specific questions.
[00:26:06.640 --> 00:26:08.540]   You have to operationalize it
[00:26:08.540 --> 00:26:11.420]   and have a theory of how it's harming kids.
[00:26:11.420 --> 00:26:13.580]   And so almost all of the research
[00:26:13.580 --> 00:26:16.340]   is done on what's called the dose response model.
[00:26:16.340 --> 00:26:19.160]   That is, everybody, including most of the researchers,
[00:26:19.160 --> 00:26:20.380]   are thinking about this like,
[00:26:20.380 --> 00:26:22.580]   let's treat this like sugar.
[00:26:22.580 --> 00:26:23.860]   You know, 'cause the data usually shows
[00:26:23.860 --> 00:26:24.900]   a little bit of social media use
[00:26:24.900 --> 00:26:26.780]   isn't correlated with harm, but a lot is.
[00:26:26.780 --> 00:26:28.660]   So, you know, think of it like sugar.
[00:26:28.660 --> 00:26:31.100]   And if kids have a lot of sugar, then it's bad.
[00:26:31.100 --> 00:26:32.200]   So how much is okay?
[00:26:32.200 --> 00:26:35.900]   But social media is not like sugar at all.
[00:26:35.900 --> 00:26:37.380]   It's not a dose response thing.
[00:26:37.380 --> 00:26:40.320]   It's a complete rewiring of childhood.
[00:26:40.320 --> 00:26:43.620]   So we evolved as a species in which kids play
[00:26:43.620 --> 00:26:44.860]   in mixed age groups.
[00:26:44.860 --> 00:26:46.180]   They learn the skills of adulthood.
[00:26:46.180 --> 00:26:47.660]   They're always playing and working
[00:26:47.660 --> 00:26:48.780]   and learning and doing errands.
[00:26:48.780 --> 00:26:49.740]   That's normal childhood.
[00:26:49.740 --> 00:26:50.780]   That's how you develop your brain.
[00:26:50.780 --> 00:26:53.740]   That's how you become a mature adult, until the 1990s.
[00:26:53.740 --> 00:26:55.340]   In the 1990s, we dropped all that.
[00:26:55.340 --> 00:26:56.620]   We said, it's too dangerous.
[00:26:56.620 --> 00:26:58.540]   If we let you outside, you'll be kidnapped.
[00:26:58.540 --> 00:27:00.940]   So we completely, we began rewiring childhood
[00:27:00.940 --> 00:27:03.140]   in the '90s before social media.
[00:27:03.140 --> 00:27:05.140]   And that's a big part of the story.
[00:27:05.140 --> 00:27:07.020]   I'm a big fan of Lenore Scanese,
[00:27:07.020 --> 00:27:08.300]   who wrote the book "Free-Range Kids."
[00:27:08.300 --> 00:27:10.540]   If there are any parents listening to this,
[00:27:10.540 --> 00:27:13.680]   please buy Lenore's book "Free-Range Kids,"
[00:27:13.680 --> 00:27:15.900]   and then go to letgrow.org.
[00:27:15.900 --> 00:27:18.600]   It's a nonprofit that Lenore and I started
[00:27:18.600 --> 00:27:21.260]   with Peter Gray and Daniel Shookman
[00:27:21.260 --> 00:27:24.520]   to help change the laws and the norms
[00:27:24.520 --> 00:27:25.780]   around letting kids out to play.
[00:27:25.780 --> 00:27:26.940]   They need free play.
[00:27:26.940 --> 00:27:28.340]   So that's the big picture.
[00:27:28.340 --> 00:27:29.740]   They need free play.
[00:27:29.740 --> 00:27:31.620]   And we started stopping that in the '90s
[00:27:31.620 --> 00:27:32.700]   that we reduced it.
[00:27:32.700 --> 00:27:35.740]   And then Gen Z, kids born in 1996,
[00:27:35.740 --> 00:27:37.900]   they're the first people in history
[00:27:37.900 --> 00:27:41.020]   to get on social media before puberty.
[00:27:41.020 --> 00:27:43.520]   Millennials didn't get it until they were in college.
[00:27:43.520 --> 00:27:46.100]   But Gen Z, they get it 'cause you can lie.
[00:27:46.100 --> 00:27:48.180]   You just lie about your age.
[00:27:48.180 --> 00:27:51.820]   So they really began to get on around 2009, 2010,
[00:27:51.820 --> 00:27:53.740]   and boom, two years later, they're depressed.
[00:27:53.740 --> 00:27:56.240]   It's not because they ate too much sugar necessarily.
[00:27:56.240 --> 00:27:59.140]   It's because even normal social interactions
[00:27:59.140 --> 00:28:02.260]   that kids had in the early 2000s, largely,
[00:28:02.260 --> 00:28:04.580]   well, they decline because now everything's
[00:28:04.580 --> 00:28:05.880]   through the phone.
[00:28:05.880 --> 00:28:08.340]   And that's what I'm trying to get across,
[00:28:08.340 --> 00:28:10.660]   that it's not just a dose response thing.
[00:28:10.660 --> 00:28:13.280]   It's imagine one middle school
[00:28:13.280 --> 00:28:15.180]   where everyone has an Instagram account
[00:28:15.180 --> 00:28:16.260]   and it's constant drama.
[00:28:16.260 --> 00:28:18.160]   Everyone's constantly checking and posting
[00:28:18.160 --> 00:28:21.840]   and worrying and imagine going through puberty that way
[00:28:21.840 --> 00:28:24.800]   versus imagine there was a policy, no phones in school.
[00:28:24.800 --> 00:28:26.100]   You have to check them in a locker.
[00:28:26.100 --> 00:28:27.520]   No one can have an Instagram account.
[00:28:27.520 --> 00:28:29.460]   All the parents are on board.
[00:28:29.460 --> 00:28:31.300]   Parents only let their kids have Instagram
[00:28:31.300 --> 00:28:33.420]   because the kid says everyone else has it.
[00:28:33.420 --> 00:28:35.060]   And we're stuck in a social dilemma.
[00:28:35.060 --> 00:28:36.140]   We're stuck in a trap.
[00:28:36.140 --> 00:28:38.220]   So what's the solution?
[00:28:38.220 --> 00:28:40.420]   Keep kids off until they're done with puberty.
[00:28:40.420 --> 00:28:42.220]   There's a new study actually by Amy Orban
[00:28:42.220 --> 00:28:45.820]   and Andy Shabilsky showing that the damage is greatest
[00:28:45.820 --> 00:28:47.740]   for girls between 11 and 13.
[00:28:47.740 --> 00:28:50.480]   So there is no way to make it safe for preteens
[00:28:50.480 --> 00:28:51.640]   or even 13, 14 year olds.
[00:28:51.640 --> 00:28:54.220]   We've gotta, kids should simply not be allowed
[00:28:54.220 --> 00:28:57.720]   on these business models where you're the product.
[00:28:57.720 --> 00:28:59.540]   They should not be allowed until you're 16.
[00:28:59.540 --> 00:29:00.940]   We need to raise the age and enforce it.
[00:29:00.940 --> 00:29:01.760]   That's the biggest thing.
[00:29:01.760 --> 00:29:03.660]   - So I think that's a really powerful solution,
[00:29:03.660 --> 00:29:08.660]   but it makes me wonder if there's other solutions
[00:29:08.660 --> 00:29:14.440]   like controlling the virality of bullying.
[00:29:15.580 --> 00:29:18.780]   Sort of if there's a way that's more productive
[00:29:18.780 --> 00:29:21.500]   to childhood to use social media.
[00:29:21.500 --> 00:29:24.060]   So of course one thing is putting your phone down,
[00:29:24.060 --> 00:29:25.740]   but first of all from the perspective
[00:29:25.740 --> 00:29:28.380]   of social media companies, it might be difficult
[00:29:28.380 --> 00:29:29.800]   to convince them to do so.
[00:29:29.800 --> 00:29:34.580]   And also for me as an adult who grew up
[00:29:34.580 --> 00:29:39.500]   without social media, social media is a source of joy.
[00:29:39.500 --> 00:29:43.460]   So I wonder if it's possible to design the mechanisms
[00:29:43.460 --> 00:29:47.260]   both challenge the ad driven model,
[00:29:47.260 --> 00:29:49.940]   but actually just technically the recommender system
[00:29:49.940 --> 00:29:55.420]   and how virality works on these platforms.
[00:29:55.420 --> 00:29:57.780]   If it's possible to design a platform
[00:29:57.780 --> 00:30:00.700]   that leads to growth, anti-fragility,
[00:30:00.700 --> 00:30:05.700]   but does not lead to depression, self-harm, and suicide.
[00:30:05.700 --> 00:30:08.500]   Like finding that balance and making that
[00:30:08.500 --> 00:30:12.940]   as the objective function, not engagement or--
[00:30:12.940 --> 00:30:15.980]   - Yeah, I don't think that can be done for kids.
[00:30:15.980 --> 00:30:18.740]   So I am very reluctant to tell adults what to do.
[00:30:18.740 --> 00:30:20.420]   I have a lot of libertarian friends
[00:30:20.420 --> 00:30:22.460]   and I would lose their friendship if I started saying,
[00:30:22.460 --> 00:30:24.660]   oh, it's bad for adults and we should stop adults
[00:30:24.660 --> 00:30:25.780]   from using it.
[00:30:25.780 --> 00:30:27.580]   But by the same token, I'm very reluctant
[00:30:27.580 --> 00:30:29.500]   to have Facebook and Instagram tell my kids
[00:30:29.500 --> 00:30:31.180]   what to do without me even knowing
[00:30:31.180 --> 00:30:33.580]   or without me having any ability to control it.
[00:30:33.580 --> 00:30:35.860]   As a parent, it's very hard to stop your kid.
[00:30:35.860 --> 00:30:38.220]   I have stopped my kids from getting on Instagram
[00:30:38.220 --> 00:30:39.900]   and that's caused some difficulties,
[00:30:39.900 --> 00:30:42.540]   but they also have thanked me
[00:30:42.540 --> 00:30:44.220]   'cause they see that it's stupid.
[00:30:44.220 --> 00:30:46.500]   They see that what the kids are really on it,
[00:30:46.500 --> 00:30:48.500]   what they post, they see that the culture of it
[00:30:48.500 --> 00:30:50.220]   is stupid, as they say.
[00:30:50.220 --> 00:30:54.240]   So I don't think there's a way to make it healthy for kids.
[00:30:54.240 --> 00:30:56.100]   I think there's one thing which is healthy for kids,
[00:30:56.100 --> 00:30:57.160]   which is free play.
[00:30:57.160 --> 00:30:59.860]   We already robbed them of most of it in the '90s.
[00:30:59.860 --> 00:31:01.340]   The more time they spend on their devices,
[00:31:01.340 --> 00:31:03.020]   the less they have free play.
[00:31:03.020 --> 00:31:04.220]   Video games is a kind of play.
[00:31:04.220 --> 00:31:06.700]   I'm not saying that these things are all bad,
[00:31:06.700 --> 00:31:08.480]   but 12 hours of video game play
[00:31:08.480 --> 00:31:10.740]   means you don't get any physical play.
[00:31:10.740 --> 00:31:11.580]   So anyway--
[00:31:11.580 --> 00:31:13.240]   - To me, physical play is the way
[00:31:13.240 --> 00:31:17.940]   to develop physical antifragility.
[00:31:17.940 --> 00:31:19.020]   - And especially social skills.
[00:31:19.020 --> 00:31:21.620]   Kids need huge amounts of conflict
[00:31:21.620 --> 00:31:24.140]   with no adult to supervise or mediate,
[00:31:24.140 --> 00:31:25.460]   and that's why we robbed them of.
[00:31:25.460 --> 00:31:26.980]   So anyway, we should move on
[00:31:26.980 --> 00:31:29.420]   'cause I get really into the evidence here
[00:31:29.420 --> 00:31:32.260]   because I think the story's actually quite clear now.
[00:31:32.260 --> 00:31:33.680]   There was a lot of ambiguity.
[00:31:33.680 --> 00:31:34.820]   There are conflicting studies,
[00:31:34.820 --> 00:31:36.580]   but when you look at it all together,
[00:31:36.580 --> 00:31:38.180]   the correlational studies are pretty clear
[00:31:38.180 --> 00:31:41.240]   and the effect sizes are coming in around 0.1 to 0.15,
[00:31:41.240 --> 00:31:43.580]   whether you call that a correlation coefficient or a beta.
[00:31:43.580 --> 00:31:44.900]   It's all standardized beta.
[00:31:44.900 --> 00:31:46.620]   It's all in that sort of range.
[00:31:46.620 --> 00:31:48.700]   There's also experimental evidence.
[00:31:48.700 --> 00:31:50.900]   We collect true experiments with random assignment
[00:31:50.900 --> 00:31:52.500]   and they mostly show an effect.
[00:31:52.500 --> 00:31:55.820]   And there's eyewitness testimony.
[00:31:55.820 --> 00:31:58.580]   With the kids themselves,
[00:31:58.580 --> 00:32:00.700]   you talk to girls and you poll them.
[00:32:00.700 --> 00:32:01.860]   Do you think overall Instagram
[00:32:01.860 --> 00:32:04.060]   is good for your mental health or bad for it?
[00:32:04.060 --> 00:32:05.020]   You're not gonna find a group saying,
[00:32:05.020 --> 00:32:05.860]   "Oh, it's wonderful.
[00:32:05.860 --> 00:32:07.220]   Oh yeah, yeah, Mark, you're right.
[00:32:07.220 --> 00:32:08.060]   It's mostly good."
[00:32:08.060 --> 00:32:09.900]   No, the girls themselves say,
[00:32:09.900 --> 00:32:10.900]   "This is the major reason."
[00:32:10.900 --> 00:32:13.140]   And I've got studies in the Google Doc
[00:32:13.140 --> 00:32:13.980]   where there've been surveys.
[00:32:13.980 --> 00:32:16.900]   What do you think is causing depression and anxiety?
[00:32:16.900 --> 00:32:19.140]   And the number one thing they say is social media.
[00:32:19.140 --> 00:32:20.980]   So there's multiple strands of evidence.
[00:32:20.980 --> 00:32:25.460]   - Do you think the recommendation is, as a parent,
[00:32:25.460 --> 00:32:30.300]   that teens should not use Instagram, Twitter?
[00:32:30.300 --> 00:32:31.140]   - Yes.
[00:32:31.140 --> 00:32:33.620]   - That's ultimately, maybe in the longterm,
[00:32:33.620 --> 00:32:34.740]   there's a more nuanced solution.
[00:32:34.740 --> 00:32:36.060]   - There's no way to make it safe.
[00:32:36.060 --> 00:32:38.020]   It's unsafe at any speed.
[00:32:38.020 --> 00:32:41.180]   I mean, it might be very difficult to make it safe.
[00:32:41.180 --> 00:32:42.340]   And then the short term,
[00:32:42.340 --> 00:32:45.620]   while we don't know how to make it safe, put down the phone.
[00:32:45.620 --> 00:32:47.380]   - Well, hold on a second.
[00:32:47.380 --> 00:32:50.220]   Play with other kids via a platform
[00:32:50.220 --> 00:32:52.140]   like Roblox or multiplayer video games.
[00:32:52.140 --> 00:32:52.980]   That's great.
[00:32:52.980 --> 00:32:54.500]   I have no beef with that.
[00:32:54.500 --> 00:32:56.220]   You focus on bullying before.
[00:32:56.220 --> 00:32:59.400]   That's one of five or seven different avenues of harm.
[00:32:59.400 --> 00:33:02.220]   The main one I think, which does in the girls,
[00:33:02.220 --> 00:33:03.440]   is not being bullied.
[00:33:03.440 --> 00:33:07.740]   It's living a life where you're thinking all the time
[00:33:07.740 --> 00:33:09.660]   about posting.
[00:33:09.660 --> 00:33:11.100]   Because once a girl starts posting,
[00:33:11.100 --> 00:33:12.860]   so it's bad enough that they're scrolling through,
[00:33:12.860 --> 00:33:14.300]   and this is, everyone comments on this,
[00:33:14.300 --> 00:33:15.140]   you're scrolling through
[00:33:15.140 --> 00:33:16.820]   and everyone's life looks better than yours
[00:33:16.820 --> 00:33:19.080]   because it's fake and all that you see
[00:33:19.080 --> 00:33:21.000]   are the ones the algorithm picked that were the night.
[00:33:21.000 --> 00:33:24.300]   Anyway, so the scrolling, I think, is bad for the girls.
[00:33:24.300 --> 00:33:26.020]   But I'm beginning to see, I can't prove this,
[00:33:26.020 --> 00:33:27.580]   but I'm beginning to see from talking to girls,
[00:33:27.580 --> 00:33:31.180]   from seeing how it's used, is once you start posting,
[00:33:31.180 --> 00:33:32.260]   that takes over your mind.
[00:33:32.260 --> 00:33:34.860]   And now you're, basically, you're no longer present.
[00:33:34.860 --> 00:33:36.460]   Because even if you're only spending
[00:33:36.460 --> 00:33:39.100]   five or six hours a day on Instagram,
[00:33:39.100 --> 00:33:40.740]   you're always thinking about it.
[00:33:40.740 --> 00:33:43.780]   And when you're in class, you're thinking about
[00:33:43.780 --> 00:33:46.180]   how are people responding to the post that I made
[00:33:46.180 --> 00:33:48.800]   between period, between classes.
[00:33:48.800 --> 00:33:49.720]   I mean, I do it.
[00:33:49.720 --> 00:33:51.080]   I try to stay off Twitter for a while,
[00:33:51.080 --> 00:33:54.140]   but now I've got this big article, I'm tweeting about it,
[00:33:54.140 --> 00:33:55.100]   and I can't help it.
[00:33:55.100 --> 00:33:56.900]   I check 20 times a day, I'll check.
[00:33:56.900 --> 00:33:57.740]   Like, what are people saying?
[00:33:57.740 --> 00:33:58.580]   What are people saying?
[00:33:58.580 --> 00:33:59.500]   This is terrible.
[00:33:59.500 --> 00:34:01.740]   And I'm a 58-year-old man.
[00:34:01.740 --> 00:34:04.760]   Imagine being a 12-year-old girl going through puberty,
[00:34:04.760 --> 00:34:06.740]   you're self-conscious about how you look.
[00:34:06.740 --> 00:34:07.980]   And I see some young women,
[00:34:07.980 --> 00:34:09.420]   I see some professional young women,
[00:34:09.420 --> 00:34:10.900]   women in their 20s and 30s,
[00:34:10.900 --> 00:34:12.780]   who are putting up sexy photos of themselves.
[00:34:12.780 --> 00:34:16.100]   Like, and this is so sad, so sad, don't be doing this.
[00:34:16.100 --> 00:34:20.420]   - Yeah, see, the thing where I disagree a little bit is,
[00:34:20.420 --> 00:34:22.660]   I agree with you in the short term,
[00:34:22.660 --> 00:34:23.500]   but in the long term,
[00:34:23.500 --> 00:34:26.180]   I feel it's the responsibility of social media,
[00:34:26.180 --> 00:34:27.700]   not in some kind of ethical way,
[00:34:27.700 --> 00:34:29.380]   not just in an ethical way,
[00:34:29.380 --> 00:34:31.100]   but it'll actually be good for the product
[00:34:31.100 --> 00:34:34.740]   and for the company to maximize the long-term happiness
[00:34:34.740 --> 00:34:36.580]   and well-being of the person.
[00:34:36.580 --> 00:34:37.700]   So not just engagement.
[00:34:37.700 --> 00:34:38.620]   So consider--
[00:34:38.620 --> 00:34:40.460]   - But the person is not the customer.
[00:34:40.460 --> 00:34:42.340]   So the thing is not to make them happy,
[00:34:42.340 --> 00:34:43.660]   it's to keep them on.
[00:34:43.660 --> 00:34:45.540]   - That's the way it is currently, that driven.
[00:34:45.540 --> 00:34:47.460]   - If we can get a business model, as you're saying,
[00:34:47.460 --> 00:34:48.440]   I'd be all for it.
[00:34:48.440 --> 00:34:51.360]   - And I think that's the way to make much more money.
[00:34:51.360 --> 00:34:52.620]   - So like a subscription model,
[00:34:52.620 --> 00:34:54.780]   where the money comes from paying?
[00:34:54.780 --> 00:34:57.220]   - It's not--
[00:34:57.220 --> 00:34:58.060]   - That would work, wouldn't it?
[00:34:58.060 --> 00:34:58.880]   That would help.
[00:34:58.880 --> 00:34:59.880]   - So subscription definitely would help,
[00:34:59.880 --> 00:35:01.960]   but I'm not sure it's so much,
[00:35:01.960 --> 00:35:05.160]   I mean, a lot of people say it's about the source of money,
[00:35:05.160 --> 00:35:06.360]   but I just think it's about
[00:35:06.360 --> 00:35:09.960]   the fundamental mission of the product.
[00:35:09.960 --> 00:35:13.060]   If you want people to really love a thing,
[00:35:13.060 --> 00:35:17.780]   I think that thing should maximize
[00:35:17.780 --> 00:35:19.760]   your long-term well-being.
[00:35:19.760 --> 00:35:23.760]   - It should, in theory, in morality land, it should.
[00:35:23.760 --> 00:35:25.400]   - I don't think it's just morality land.
[00:35:25.400 --> 00:35:26.840]   I think in business land, too.
[00:35:26.840 --> 00:35:29.000]   But that's maybe a discussion for another day.
[00:35:29.000 --> 00:35:33.480]   We're studying the reality of the way things currently are,
[00:35:33.480 --> 00:35:36.920]   and they are as they are, as the studies are highlighting.
[00:35:36.920 --> 00:35:41.920]   So let us go then in from the land of mental health
[00:35:41.920 --> 00:35:45.480]   for young people to the land of democracy.
[00:35:45.480 --> 00:35:49.440]   By the way, in these big umbrella areas,
[00:35:49.440 --> 00:35:52.520]   is there a connection, is there a correlation
[00:35:52.520 --> 00:35:55.880]   between the mental health of a human mind
[00:35:55.880 --> 00:35:59.160]   and the division of our political discourse?
[00:35:59.160 --> 00:36:00.800]   - Oh yes, oh yes.
[00:36:00.800 --> 00:36:02.880]   So our brains are structured
[00:36:02.880 --> 00:36:06.260]   to be really good at approach and avoid.
[00:36:06.260 --> 00:36:08.640]   So we have circuits, the front left circuit,
[00:36:08.640 --> 00:36:10.720]   this is an oversimplification, but there's some truth to it.
[00:36:10.720 --> 00:36:12.720]   There's what's called the behavioral activation system,
[00:36:12.720 --> 00:36:13.860]   front left cortex.
[00:36:13.860 --> 00:36:16.480]   It's all about approach, opportunity,
[00:36:16.480 --> 00:36:17.640]   kid in a candy store.
[00:36:17.640 --> 00:36:19.280]   And then the front right cortex has circuits
[00:36:19.280 --> 00:36:22.040]   specialized for withdrawal, fear, threat.
[00:36:22.040 --> 00:36:24.840]   And of course, students, I'm a college professor,
[00:36:24.840 --> 00:36:27.120]   and most of us think about our college days like,
[00:36:27.120 --> 00:36:29.760]   you know, yeah, we were anxious at times, but it was fun.
[00:36:29.760 --> 00:36:32.120]   And it was like, I can take all these courses,
[00:36:32.120 --> 00:36:35.320]   I can do all these clubs, I know all these people.
[00:36:35.320 --> 00:36:38.080]   Now imagine if in 2013, all of a sudden,
[00:36:38.080 --> 00:36:39.480]   students are coming in
[00:36:39.480 --> 00:36:41.520]   with their front right cortex hyperactivated,
[00:36:41.520 --> 00:36:43.980]   everything's a threat, everything is dangerous,
[00:36:43.980 --> 00:36:45.840]   there's not enough to go around.
[00:36:45.840 --> 00:36:48.320]   So the front right cortex puts us into
[00:36:48.320 --> 00:36:51.360]   what's called defend mode as opposed to discover mode.
[00:36:51.360 --> 00:36:53.000]   Now let's move up to adults.
[00:36:53.000 --> 00:36:57.520]   Imagine a large, diverse, secular, liberal democracy
[00:36:57.520 --> 00:37:00.760]   in which people are most of the time in discover mode.
[00:37:00.760 --> 00:37:01.920]   And you know, we have a problem,
[00:37:01.920 --> 00:37:03.160]   hmm, let's think how to solve it.
[00:37:03.160 --> 00:37:05.400]   And this is what de Tocqueville said about Americans,
[00:37:05.400 --> 00:37:07.120]   like, there's a problem, we get together,
[00:37:07.120 --> 00:37:08.360]   we figure out how to solve it.
[00:37:08.360 --> 00:37:10.640]   And he said, whereas in England and France,
[00:37:10.640 --> 00:37:12.600]   people would wait for the king to do it.
[00:37:12.600 --> 00:37:14.400]   But here, like, you know, let's roll up our sleeves,
[00:37:14.400 --> 00:37:15.280]   let's do it.
[00:37:15.280 --> 00:37:16.820]   That's the can-do mindset.
[00:37:16.820 --> 00:37:19.040]   That's front left cortex discover mode.
[00:37:19.040 --> 00:37:21.920]   If you have a national shift
[00:37:21.920 --> 00:37:24.440]   of people spending more time in defend mode,
[00:37:24.440 --> 00:37:26.120]   now you, so everything that comes up,
[00:37:26.120 --> 00:37:27.700]   whatever anyone says, you're not looking like,
[00:37:27.700 --> 00:37:29.040]   oh, is there something good about it?
[00:37:29.040 --> 00:37:31.040]   You're thinking, you know, how is this dangerous?
[00:37:31.040 --> 00:37:31.880]   How is this a threat?
[00:37:31.880 --> 00:37:32.700]   How is this violence?
[00:37:32.700 --> 00:37:33.540]   How can I attack this?
[00:37:33.540 --> 00:37:35.880]   How can I, you know, so if you imagine, you know,
[00:37:35.880 --> 00:37:38.200]   God up there with a little lever, like,
[00:37:38.200 --> 00:37:40.300]   okay, let's push everyone over into, you know,
[00:37:40.300 --> 00:37:41.520]   more into discover mode.
[00:37:41.520 --> 00:37:44.020]   And it's like joy breaks out, age of Aquarius.
[00:37:44.020 --> 00:37:45.400]   All right, let's shift them back into,
[00:37:45.400 --> 00:37:47.320]   let's put everyone in defend mode.
[00:37:47.320 --> 00:37:48.400]   And I can't think of a better way
[00:37:48.400 --> 00:37:49.560]   to put people in defend mode
[00:37:49.560 --> 00:37:52.200]   than to have them spend some time on partisan
[00:37:52.200 --> 00:37:53.400]   or political Twitter,
[00:37:53.400 --> 00:37:55.840]   where it's just a stream of horror stories,
[00:37:55.840 --> 00:37:58.520]   including videos about how horrible the other side is.
[00:37:58.520 --> 00:38:00.240]   And it's not just that they're bad people.
[00:38:00.240 --> 00:38:02.320]   It's that if they win this election,
[00:38:02.320 --> 00:38:06.100]   then we lose our country or then it's catastrophe.
[00:38:06.100 --> 00:38:09.080]   So Twitter, and again, we're not saying all of Twitter,
[00:38:09.080 --> 00:38:11.320]   you know, most people aren't on Twitter
[00:38:11.320 --> 00:38:14.260]   and people that are mostly not talking about politics,
[00:38:14.260 --> 00:38:16.480]   but the ones that are on talking about politics
[00:38:16.480 --> 00:38:18.680]   are flooding us with stuff.
[00:38:18.680 --> 00:38:20.020]   All the journalists see it.
[00:38:20.020 --> 00:38:23.720]   All the mainstream media is hugely influenced by Twitter.
[00:38:23.720 --> 00:38:26.320]   So if we put everyone,
[00:38:26.320 --> 00:38:29.640]   if there's more sort of anxiety, sense of threat,
[00:38:29.640 --> 00:38:30.940]   this colors everything.
[00:38:30.940 --> 00:38:32.760]   And then you're not, you know,
[00:38:32.760 --> 00:38:36.440]   the great thing about a democracy and especially,
[00:38:36.440 --> 00:38:39.260]   you know, a legislature that has some diversity in it,
[00:38:39.260 --> 00:38:42.720]   is that the art of politics is that you can grow the pie
[00:38:42.720 --> 00:38:43.700]   and then divide it.
[00:38:43.700 --> 00:38:45.220]   You don't just fight zero sum.
[00:38:45.220 --> 00:38:48.920]   You find ways that we can all get 60% of what we want.
[00:38:48.920 --> 00:38:52.260]   And that ends when everyone's anxious and angry.
[00:38:52.260 --> 00:38:57.260]   - So let's try to start to figure out who's to blame here.
[00:38:57.260 --> 00:38:59.540]   Is it the nature of social media?
[00:38:59.540 --> 00:39:03.460]   Is it the decision of the people
[00:39:03.460 --> 00:39:04.900]   at the heads of social media companies
[00:39:04.900 --> 00:39:07.620]   that they're making in the detailed engineering designs
[00:39:07.620 --> 00:39:09.100]   of the algorithm?
[00:39:09.100 --> 00:39:13.860]   Is it the users of social media that drive narratives
[00:39:13.860 --> 00:39:15.900]   like you mentioned journalists,
[00:39:15.900 --> 00:39:18.940]   that want to maximize drama in order to
[00:39:18.940 --> 00:39:25.340]   drive clicks to their off-site articles?
[00:39:25.340 --> 00:39:30.340]   Is it just human nature that loves drama,
[00:39:30.340 --> 00:39:33.060]   can't look away from an accident when you're driving by?
[00:39:33.060 --> 00:39:35.340]   Is there something to be said about,
[00:39:35.340 --> 00:39:38.020]   the reason I ask these questions is to see,
[00:39:38.020 --> 00:39:40.820]   can we start to figure out what the solution would be
[00:39:42.740 --> 00:39:44.780]   to alleviate, to deescalate the-
[00:39:44.780 --> 00:39:45.980]   - Not yet, not yet.
[00:39:45.980 --> 00:39:47.880]   Let's first, we have to understand,
[00:39:47.880 --> 00:39:51.140]   as we did on the teen mental health thing,
[00:39:51.140 --> 00:39:53.100]   okay, now let's lay out what is the problem?
[00:39:53.100 --> 00:39:54.540]   What's messing up our country?
[00:39:54.540 --> 00:39:56.380]   And then we can talk about solutions.
[00:39:56.380 --> 00:39:58.020]   So it's all the things you said,
[00:39:58.020 --> 00:40:00.540]   interacting in an interesting way.
[00:40:00.540 --> 00:40:03.260]   So human nature is tribal.
[00:40:03.260 --> 00:40:05.860]   We evolved for intergroup conflict.
[00:40:05.860 --> 00:40:07.320]   We love war.
[00:40:07.320 --> 00:40:10.500]   The first time my buddies and I played paintball,
[00:40:10.500 --> 00:40:12.220]   I was 29.
[00:40:12.220 --> 00:40:14.140]   And we were divided into teams with strangers
[00:40:14.140 --> 00:40:16.700]   to shoot guns at each other and kill each other.
[00:40:16.700 --> 00:40:18.900]   And we all, afterwards, it was like,
[00:40:18.900 --> 00:40:21.780]   oh my God, that was incredible.
[00:40:21.780 --> 00:40:23.900]   Like it really felt like we'd opened a room in our hearts
[00:40:23.900 --> 00:40:25.420]   that had never been opened.
[00:40:25.420 --> 00:40:29.340]   But as men, testosterone changes our brains and our bodies
[00:40:29.340 --> 00:40:31.820]   and activates the war stuff, like we've got more stuff.
[00:40:31.820 --> 00:40:35.900]   And that's why boys like certain team sports, it's play war.
[00:40:35.900 --> 00:40:37.540]   So that's who we are.
[00:40:37.540 --> 00:40:38.820]   It doesn't mean we're always tribal.
[00:40:38.820 --> 00:40:40.180]   It doesn't mean we're always wanting to fight.
[00:40:40.180 --> 00:40:42.580]   We're also really good at making peace and cooperation
[00:40:42.580 --> 00:40:43.620]   and finding deals.
[00:40:43.620 --> 00:40:45.500]   We're good at trade and exchange.
[00:40:45.500 --> 00:40:48.460]   So you want your country to,
[00:40:48.460 --> 00:40:52.060]   you want a society that has room for conflict,
[00:40:52.060 --> 00:40:53.900]   ideally over sports, like that's great.
[00:40:53.900 --> 00:40:57.060]   That's totally, it's not just harmless, it's actually good.
[00:40:57.060 --> 00:40:58.660]   But otherwise you want cooperation
[00:40:58.660 --> 00:41:00.600]   to generally prevail in the society.
[00:41:00.600 --> 00:41:02.820]   That's how you create prosperity and peace.
[00:41:02.820 --> 00:41:04.740]   And if you're gonna have a diverse democracy,
[00:41:04.740 --> 00:41:06.900]   you really better focus on cooperation,
[00:41:06.900 --> 00:41:09.860]   not on tribalism and division.
[00:41:09.860 --> 00:41:11.620]   And there's a wonderful book by Yasha Monk
[00:41:11.620 --> 00:41:12.620]   called "The Great Experiment"
[00:41:12.620 --> 00:41:15.620]   that talks about the difficulty of diversity and democracy
[00:41:15.620 --> 00:41:19.340]   and what we need to do to get this right
[00:41:19.340 --> 00:41:21.540]   and to get the benefits of diversity.
[00:41:21.540 --> 00:41:22.940]   So that's human nature.
[00:41:22.940 --> 00:41:26.620]   Now let's imagine that the technological environment
[00:41:26.620 --> 00:41:28.820]   makes it really easy for us to cooperate.
[00:41:28.820 --> 00:41:31.020]   Let's give everyone telephones and the postal service.
[00:41:31.020 --> 00:41:32.900]   Let's give them email, like, wow,
[00:41:32.900 --> 00:41:35.060]   we can do all these things together with people far away.
[00:41:35.060 --> 00:41:35.900]   It's amazing.
[00:41:35.900 --> 00:41:38.540]   Now, instead of that, let's give them a technology
[00:41:38.540 --> 00:41:40.100]   that encourages them to fight.
[00:41:40.100 --> 00:41:44.300]   So early Facebook and Twitter were generally lovely places.
[00:41:44.300 --> 00:41:46.700]   People old enough to remember, like, they were fun.
[00:41:46.700 --> 00:41:48.260]   There was a lot of humor.
[00:41:48.260 --> 00:41:50.100]   You didn't feel like you were gonna get your head blown off
[00:41:50.100 --> 00:41:51.300]   no matter what you said.
[00:41:51.300 --> 00:41:55.460]   2007, 2008, 2009, it was still fun.
[00:41:55.460 --> 00:41:57.020]   These were nice places mostly.
[00:41:57.020 --> 00:42:00.340]   And like almost all the platforms start off as nice places.
[00:42:00.340 --> 00:42:03.540]   But, and this is the key thing in the article,
[00:42:03.540 --> 00:42:06.580]   in the Atlantic article on Babel, on after Babel.
[00:42:06.580 --> 00:42:07.820]   - The Atlantic article, by the way,
[00:42:07.820 --> 00:42:09.700]   is why the past 10 years of American life
[00:42:09.700 --> 00:42:11.460]   have been uniquely stupid.
[00:42:11.460 --> 00:42:15.020]   - Yeah, my title in the magazine was,
[00:42:15.020 --> 00:42:18.420]   After Babel, Adapting to a World We Can No Longer Share,
[00:42:18.420 --> 00:42:19.260]   is what I proposed.
[00:42:19.260 --> 00:42:21.820]   But they A/B tested, what's the title
[00:42:21.820 --> 00:42:22.740]   that gets the most clicks,
[00:42:22.740 --> 00:42:24.500]   and it was why the past 10 years have been unique.
[00:42:24.500 --> 00:42:25.940]   - So Babel, the Tower of Babel,
[00:42:25.940 --> 00:42:29.220]   is a driving metaphor in the piece.
[00:42:29.220 --> 00:42:31.700]   So first of all, what is it, what's the Tower of Babel?
[00:42:31.700 --> 00:42:33.140]   What's Babel, what are we talking about?
[00:42:33.140 --> 00:42:36.460]   - Okay, so the Tower of Babel is a story early in Genesis
[00:42:36.460 --> 00:42:39.100]   where the descendants of Noah are spreading out
[00:42:39.100 --> 00:42:40.700]   and repopulating the world.
[00:42:40.700 --> 00:42:42.420]   And they're on the plain of Shinar,
[00:42:42.420 --> 00:42:45.340]   and they say, "Let us build us a city with a tower
[00:42:45.340 --> 00:42:48.300]   to make a name for ourselves, lest we be scattered again."
[00:42:48.300 --> 00:42:50.740]   And so it's a very short story, there's not a lot in it,
[00:42:50.740 --> 00:42:52.660]   but it looks like they're saying,
[00:42:52.660 --> 00:42:54.060]   we don't want God to flood us again,
[00:42:54.060 --> 00:42:57.660]   let's build a city and a tower to reach the heavens.
[00:42:57.660 --> 00:43:00.780]   And God is offended by the hubris of these people
[00:43:00.780 --> 00:43:02.560]   acting again like gods.
[00:43:02.560 --> 00:43:05.700]   And he says, and here's the key line,
[00:43:05.700 --> 00:43:10.060]   he says, "Let us go down and confuse their language
[00:43:10.060 --> 00:43:13.100]   so that they may not understand one another."
[00:43:13.100 --> 00:43:15.920]   So in the story, he doesn't literally knock the tower over,
[00:43:15.920 --> 00:43:20.420]   but many of us have seen images or movie dramatizations
[00:43:20.420 --> 00:43:23.500]   where a great wind comes and the tower is knocked over
[00:43:23.500 --> 00:43:26.500]   and the people are left wandering amid the rubble,
[00:43:26.500 --> 00:43:28.540]   unable to talk to each other.
[00:43:28.540 --> 00:43:30.740]   So I've been grappling, I've been trying to say,
[00:43:30.740 --> 00:43:33.700]   what the hell happened to our society?
[00:43:33.700 --> 00:43:35.580]   Beginning in 2014, what the hell is happening
[00:43:35.580 --> 00:43:36.420]   to universities?
[00:43:36.420 --> 00:43:38.140]   And then it spread out from universities,
[00:43:38.140 --> 00:43:41.300]   it hit journalism, the arts, and now it's all over companies.
[00:43:41.300 --> 00:43:44.060]   What the hell happened to us?
[00:43:44.060 --> 00:43:46.220]   And it wasn't until I reread the Babel story
[00:43:46.220 --> 00:43:47.780]   a couple of years ago that I thought,
[00:43:47.780 --> 00:43:51.100]   whoa, this is it, this is the metaphor.
[00:43:51.100 --> 00:43:53.620]   Because I'd been thinking about tribalism
[00:43:53.620 --> 00:43:55.620]   and left-right battles and war,
[00:43:55.620 --> 00:43:57.880]   and that's easy to think about.
[00:43:57.880 --> 00:44:00.180]   But Babel isn't like, and God said,
[00:44:00.180 --> 00:44:01.860]   "Let half of the people hate the other half."
[00:44:01.860 --> 00:44:02.860]   No, it wasn't that.
[00:44:02.860 --> 00:44:05.140]   It's God said, "Let us confuse their language
[00:44:05.140 --> 00:44:07.420]   "that they, none of them can understand each other
[00:44:07.420 --> 00:44:09.740]   "ever again, or at least for a while."
[00:44:09.740 --> 00:44:12.780]   So it's a story about fragmentation.
[00:44:12.780 --> 00:44:14.180]   And that's what's unique about our time.
[00:44:14.180 --> 00:44:19.180]   So Meta or Facebook wrote a rebuttal to my article.
[00:44:19.180 --> 00:44:23.380]   They disputed what I said, and one of their arguments was,
[00:44:23.380 --> 00:44:27.060]   oh, but polarization goes back way before social media
[00:44:27.060 --> 00:44:29.780]   and it was happening in the '90s.
[00:44:29.780 --> 00:44:31.100]   And they're right, it does.
[00:44:31.100 --> 00:44:33.620]   And I did say that, but I should have said it more clearly
[00:44:33.620 --> 00:44:34.940]   with more examples.
[00:44:34.940 --> 00:44:36.660]   But here's the new thing.
[00:44:36.660 --> 00:44:38.300]   Even though left and right were beginning
[00:44:38.300 --> 00:44:39.700]   to hate each other more,
[00:44:39.700 --> 00:44:42.100]   we weren't afraid of the person next to us.
[00:44:42.100 --> 00:44:43.740]   We weren't afraid of each other.
[00:44:43.740 --> 00:44:47.020]   Cable TV, Fox News, whatever you want to point to
[00:44:47.020 --> 00:44:48.740]   about increasing polarization,
[00:44:48.740 --> 00:44:50.980]   it didn't make me afraid of my students.
[00:44:50.980 --> 00:44:53.620]   And that was new in around 2014, 2015.
[00:44:53.620 --> 00:44:55.980]   We started hearing, getting articles,
[00:44:55.980 --> 00:44:58.980]   I'm a liberal professor and my liberal students frightened me.
[00:44:58.980 --> 00:45:00.820]   It was in Vox in 2015.
[00:45:00.820 --> 00:45:02.220]   And that was after Greg and I had turned in
[00:45:02.220 --> 00:45:05.900]   the draft of our first draft of our coddling article.
[00:45:05.900 --> 00:45:07.440]   And surveys show over and over again,
[00:45:07.440 --> 00:45:09.580]   students are not as afraid of their professors,
[00:45:09.580 --> 00:45:11.420]   they're actually afraid of other students.
[00:45:11.420 --> 00:45:12.740]   Most students are lovely.
[00:45:12.740 --> 00:45:15.620]   It's not like the whole generation has lost their minds.
[00:45:15.620 --> 00:45:17.460]   What happens, a small number,
[00:45:17.460 --> 00:45:22.140]   a small number are adept at using social media
[00:45:22.140 --> 00:45:25.040]   to destroy anyone that they think
[00:45:25.040 --> 00:45:27.880]   they can get credit for destroying.
[00:45:27.880 --> 00:45:30.060]   And the bizarre thing is it's never,
[00:45:30.060 --> 00:45:31.860]   it's rarely about what ideas you express.
[00:45:31.860 --> 00:45:35.020]   It's usually about a word, like he used this word,
[00:45:35.020 --> 00:45:36.420]   or this was insensitive,
[00:45:36.420 --> 00:45:38.760]   or I can link this word to that word.
[00:45:38.760 --> 00:45:42.000]   So they don't even engage with ideas and arguments.
[00:45:42.000 --> 00:45:45.460]   It's a real sort of gotcha, prosecutable,
[00:45:45.460 --> 00:45:47.220]   it's like a witch trial mindset.
[00:45:47.220 --> 00:45:50.140]   - So the unique thing here,
[00:45:50.140 --> 00:45:53.000]   there's something about social media in those years
[00:45:53.000 --> 00:45:54.900]   that a small number of people
[00:45:54.900 --> 00:45:56.820]   can sort of be catalysts for this division.
[00:45:56.820 --> 00:45:58.800]   They can start the viral wave
[00:45:58.800 --> 00:46:00.980]   that leads to a division that's different
[00:46:00.980 --> 00:46:02.620]   than the kind of division we saw before.
[00:46:02.620 --> 00:46:04.020]   - It's a little different than a viral wave.
[00:46:04.020 --> 00:46:05.260]   Once you get some people
[00:46:05.260 --> 00:46:08.260]   who can use social media to intimidate,
[00:46:08.260 --> 00:46:10.340]   you get a sudden phase shift.
[00:46:10.340 --> 00:46:12.740]   You get a big change in the dynamics of groups.
[00:46:12.740 --> 00:46:13.900]   And that's the heart of the article.
[00:46:13.900 --> 00:46:15.300]   This isn't just another article
[00:46:15.300 --> 00:46:16.980]   about how social media is polarizing us
[00:46:16.980 --> 00:46:18.580]   and destroying democracy.
[00:46:18.580 --> 00:46:21.180]   The heart of the article is an analysis
[00:46:21.180 --> 00:46:24.420]   of what makes groups smart and what makes them stupid.
[00:46:24.420 --> 00:46:27.660]   And so because, as we said earlier,
[00:46:27.660 --> 00:46:30.200]   my own research is on post-hoc reasoning,
[00:46:30.200 --> 00:46:32.240]   post-hoc justification, rationalization.
[00:46:32.240 --> 00:46:33.720]   The only cure for that is other people
[00:46:33.720 --> 00:46:35.760]   who don't share your biases.
[00:46:35.760 --> 00:46:37.800]   And so if you have an academic debate,
[00:46:37.800 --> 00:46:40.840]   as like the one I'm having with these other researchers
[00:46:40.840 --> 00:46:43.680]   over social media, I write something, they write something,
[00:46:43.680 --> 00:46:45.160]   I have to take account of their arguments
[00:46:45.160 --> 00:46:46.880]   and they have to take account of mine.
[00:46:46.880 --> 00:46:48.240]   When the academic world works,
[00:46:48.240 --> 00:46:49.640]   it's because it puts us together
[00:46:49.640 --> 00:46:51.160]   in ways that things cancel out.
[00:46:51.160 --> 00:46:53.360]   That's what makes universities smart.
[00:46:53.360 --> 00:46:55.600]   It's what makes them generators of knowledge.
[00:46:55.600 --> 00:46:59.020]   Unless we stop dissent, what if we say,
[00:46:59.020 --> 00:47:01.380]   on these topics, there can be no dissent.
[00:47:01.380 --> 00:47:02.820]   And if anyone says otherwise,
[00:47:02.820 --> 00:47:05.680]   if any academic comes up with research that says otherwise,
[00:47:05.680 --> 00:47:06.980]   we're gonna destroy them.
[00:47:06.980 --> 00:47:10.260]   And if any academic even tweets a study
[00:47:10.260 --> 00:47:13.660]   contradicting what is the official word,
[00:47:13.660 --> 00:47:14.500]   we're gonna destroy them.
[00:47:14.500 --> 00:47:16.420]   And that was the famous case of David Shore,
[00:47:16.420 --> 00:47:19.420]   who in the days after George Floyd was killed
[00:47:19.420 --> 00:47:21.620]   and there were protests, and the question is,
[00:47:21.620 --> 00:47:23.260]   are these protests gonna be productive?
[00:47:23.260 --> 00:47:24.860]   Are they gonna backfire?
[00:47:24.860 --> 00:47:27.400]   Now, most of them were peaceful, but some were violent.
[00:47:27.400 --> 00:47:30.780]   And he tweeted a study, he just simply tweeted a study
[00:47:30.780 --> 00:47:32.400]   done by an African-American,
[00:47:32.400 --> 00:47:35.300]   I think sociologist at Princeton, Omar Wasow.
[00:47:35.300 --> 00:47:38.300]   And Wasow's study showed that when you look back
[00:47:38.300 --> 00:47:40.500]   at the '60s, you see that where there were violent protests,
[00:47:40.500 --> 00:47:43.740]   it tended to backfire, peaceful protests tend to work.
[00:47:43.740 --> 00:47:46.340]   And so he simply tweeted that study.
[00:47:46.340 --> 00:47:48.080]   And there was a Twitter mob after him,
[00:47:48.080 --> 00:47:51.740]   this was insensitive, this was anti-black,
[00:47:51.740 --> 00:47:53.100]   I think he was accused of,
[00:47:53.100 --> 00:47:55.140]   and he was fired within a day or two.
[00:47:55.140 --> 00:47:56.740]   So this is the kind of dynamic.
[00:47:56.740 --> 00:47:58.580]   This is not caused by cable TV.
[00:47:58.580 --> 00:48:00.780]   This is not caused, this is something new.
[00:48:00.780 --> 00:48:04.260]   - Can I, just on a small tangent there,
[00:48:04.260 --> 00:48:06.940]   in that situation, because it happens time and time again,
[00:48:06.940 --> 00:48:08.860]   you highlight in your current work,
[00:48:08.860 --> 00:48:11.160]   but also in the coddling of the American mind,
[00:48:11.160 --> 00:48:17.780]   is the blame on the mob, the mechanisms that enables the mob
[00:48:17.780 --> 00:48:19.720]   or the people that do the firing?
[00:48:19.720 --> 00:48:21.660]   The administration does the firing.
[00:48:21.660 --> 00:48:22.780]   - Yeah, it's all of them.
[00:48:22.780 --> 00:48:26.700]   - Well, can I, I sometimes feel
[00:48:26.700 --> 00:48:28.380]   that we don't put enough blame
[00:48:28.380 --> 00:48:30.940]   on the people that do the firing.
[00:48:30.940 --> 00:48:33.100]   Which is, that feels like,
[00:48:33.100 --> 00:48:36.000]   in the long arc of human history,
[00:48:36.000 --> 00:48:40.380]   that is the place for courage and for ideals, right?
[00:48:40.380 --> 00:48:41.860]   That's where it stops.
[00:48:41.860 --> 00:48:43.820]   That's where the buck stops.
[00:48:43.820 --> 00:48:46.900]   So there's going to be new mechanisms for mobs
[00:48:46.900 --> 00:48:47.740]   and all that kind of stuff.
[00:48:47.740 --> 00:48:49.540]   There's going to be tribalism.
[00:48:49.540 --> 00:48:50.500]   But at the end of the day,
[00:48:50.500 --> 00:48:52.300]   that's what it means to be a leader,
[00:48:52.300 --> 00:48:56.000]   is to stop the mob at the door.
[00:48:56.000 --> 00:48:57.380]   - But I'm a social psychologist,
[00:48:57.380 --> 00:49:00.380]   which means I look at the social forces that work on people.
[00:49:00.380 --> 00:49:02.220]   And if you show me a situation
[00:49:02.220 --> 00:49:05.420]   in which 95% of the people behave one way,
[00:49:05.420 --> 00:49:08.620]   and it's a way that we find surprising and shameful,
[00:49:08.620 --> 00:49:11.220]   I'm not going to say, wow, 95% of the people are shameful.
[00:49:11.220 --> 00:49:14.420]   I'm going to say, wow, what a powerful situation.
[00:49:14.420 --> 00:49:16.420]   We've got to change that situation.
[00:49:16.420 --> 00:49:17.900]   So that's what I think is happening here,
[00:49:17.900 --> 00:49:20.540]   because there are hardly any in the first few years,
[00:49:20.540 --> 00:49:22.420]   you know, it began around 2018, 2019,
[00:49:22.420 --> 00:49:24.140]   it really enters the corporate world.
[00:49:24.140 --> 00:49:26.700]   There are hardly any leaders who stood up against it.
[00:49:26.700 --> 00:49:29.700]   But I've talked to a lot, and it's always the same thing.
[00:49:29.700 --> 00:49:33.020]   You have these, you know, people in their,
[00:49:33.020 --> 00:49:35.100]   usually in their 50s or 60s,
[00:49:35.100 --> 00:49:37.660]   generally they're progressive or on the left.
[00:49:37.660 --> 00:49:40.340]   They're accused of things by their young employees.
[00:49:40.340 --> 00:49:43.780]   They don't have the vocabulary to stand up to it.
[00:49:43.780 --> 00:49:45.520]   And they give in very quickly.
[00:49:45.520 --> 00:49:47.300]   And because it happens over and over again,
[00:49:47.300 --> 00:49:49.580]   and there's only a few examples of university presidents
[00:49:49.580 --> 00:49:52.380]   who said like, no, we're not going to stop this talk
[00:49:52.380 --> 00:49:53.620]   just because you're freaking out.
[00:49:53.620 --> 00:49:55.540]   No, you know, we're not going to fire this professor
[00:49:55.540 --> 00:49:59.180]   because he wrote a paper that you don't like.
[00:49:59.180 --> 00:50:01.100]   There are so few examples,
[00:50:01.100 --> 00:50:04.220]   I have to include that the situational forces are so strong.
[00:50:04.220 --> 00:50:06.220]   Now, I think we are seeing,
[00:50:06.220 --> 00:50:10.140]   we are seeing a reversal in the last few weeks or months.
[00:50:10.140 --> 00:50:12.340]   A clear sign of that is that the New York Times
[00:50:12.340 --> 00:50:14.700]   actually came out with an editorial
[00:50:14.700 --> 00:50:16.160]   from the editorial board
[00:50:16.160 --> 00:50:18.660]   saying that free speech is important.
[00:50:18.660 --> 00:50:21.380]   Now that's amazing that the Times had the guts
[00:50:21.380 --> 00:50:24.100]   to stand up for free speech because, you know,
[00:50:24.100 --> 00:50:27.420]   they're the people, well,
[00:50:27.420 --> 00:50:28.580]   what's been happening with the Times
[00:50:28.580 --> 00:50:29.780]   is that they've allowed Twitter
[00:50:29.780 --> 00:50:31.500]   to become the editorial board.
[00:50:31.500 --> 00:50:33.940]   Twitter has control over the New York Times
[00:50:33.940 --> 00:50:36.780]   and the New York Times literally will change papers.
[00:50:36.780 --> 00:50:39.540]   I have an essay in Politico with Nadine Strassen,
[00:50:39.540 --> 00:50:41.540]   Steve Pinker and Pamela Peresky
[00:50:41.540 --> 00:50:44.740]   on how the New York Times
[00:50:44.740 --> 00:50:48.180]   retracted and changed an editorial by Brett Stevens.
[00:50:48.180 --> 00:50:50.460]   And they did it in a sneaky way and they lied about it.
[00:50:50.460 --> 00:50:53.820]   And they did this out of fear because he mentioned IQ.
[00:50:53.820 --> 00:50:55.820]   He mentioned IQ and Jews.
[00:50:55.820 --> 00:50:57.540]   And then he went on to say,
[00:50:57.540 --> 00:50:58.700]   it probably isn't a genetic thing,
[00:50:58.700 --> 00:51:00.540]   it's probably cultural, but he mentioned it.
[00:51:00.540 --> 00:51:03.540]   And the New York Times, I mean, they were really cowardly.
[00:51:03.540 --> 00:51:05.500]   Now, I think they, from what I hear,
[00:51:05.500 --> 00:51:07.060]   they know that they were cowardly.
[00:51:07.060 --> 00:51:10.480]   They know that they should not have fired James Bennett.
[00:51:10.480 --> 00:51:12.300]   They know that they gave in to the mob.
[00:51:12.300 --> 00:51:13.800]   And that's why they're now poking their head up
[00:51:13.800 --> 00:51:15.320]   above the parapet and they're saying,
[00:51:15.320 --> 00:51:16.860]   oh, we think that free speech is important.
[00:51:16.860 --> 00:51:18.220]   And then of course they got their heads blown off
[00:51:18.220 --> 00:51:20.500]   'cause Twitter reacted like, how dare you say this?
[00:51:20.500 --> 00:51:22.400]   Are you saying racist speech is okay?
[00:51:22.400 --> 00:51:23.300]   But they didn't back down.
[00:51:23.300 --> 00:51:24.580]   They didn't retract it.
[00:51:24.580 --> 00:51:27.020]   They didn't apologize for defending free speech.
[00:51:27.020 --> 00:51:30.540]   So I think the Times might be coming back.
[00:51:30.540 --> 00:51:32.420]   - Can I ask you an opinion on something here?
[00:51:32.420 --> 00:51:34.820]   What, in terms of the Times coming back,
[00:51:34.820 --> 00:51:38.140]   in terms of Twitter being the editorial board
[00:51:38.140 --> 00:51:42.680]   for the prestigious journalistic organizations,
[00:51:43.580 --> 00:51:48.020]   what's the importance of the role of Mr. Elon Musk in this?
[00:51:48.020 --> 00:51:52.260]   So, you know, it's all fun and games,
[00:51:52.260 --> 00:51:55.060]   but here's a human who tweets about the importance
[00:51:55.060 --> 00:51:59.020]   of freedom of speech and buys Twitter.
[00:51:59.020 --> 00:52:02.300]   What are your thoughts on the influence,
[00:52:02.300 --> 00:52:05.220]   the positive and the negative possible consequences
[00:52:05.220 --> 00:52:07.060]   of this particular action?
[00:52:07.060 --> 00:52:09.460]   - So, you know, if he is gonna succeed
[00:52:09.460 --> 00:52:11.860]   and if he's gonna be one of the major reasons
[00:52:11.860 --> 00:52:14.820]   why we decarbonize quickly and why we get to Mars,
[00:52:14.820 --> 00:52:17.520]   then I'm willing to cut him a lot of slack.
[00:52:17.520 --> 00:52:19.780]   So I have an overall positive view of him.
[00:52:19.780 --> 00:52:22.220]   Now where I'm concerned and where I'm critical
[00:52:22.220 --> 00:52:25.180]   is we're in the middle of a raging culture war
[00:52:25.180 --> 00:52:28.520]   and this culture war is making our institutions stupid.
[00:52:28.520 --> 00:52:30.220]   It's making them fail.
[00:52:30.220 --> 00:52:32.780]   This culture war, I think, could destroy our country.
[00:52:32.780 --> 00:52:34.820]   And by destroy, I mean we could descend
[00:52:34.820 --> 00:52:38.460]   into constant constitutional crises, a lot more violence.
[00:52:38.460 --> 00:52:39.700]   You know, not that we're gonna disappear,
[00:52:39.700 --> 00:52:40.800]   not that we're gonna kill each other,
[00:52:40.800 --> 00:52:42.620]   but I think there will be a lot more violence.
[00:52:42.620 --> 00:52:45.540]   So we're in the middle of this raging culture war.
[00:52:45.540 --> 00:52:48.220]   It's possibly turning to violence.
[00:52:48.220 --> 00:52:51.380]   You need to not add fuel to the fire.
[00:52:51.380 --> 00:52:53.020]   And the fact that he declared
[00:52:53.020 --> 00:52:54.660]   that he's gonna be a Republican
[00:52:54.660 --> 00:52:56.460]   and the Democrats are the bad party.
[00:52:56.460 --> 00:52:59.340]   And, you know, as an individual citizen,
[00:52:59.340 --> 00:53:01.420]   he's entitled to his opinion, of course,
[00:53:01.420 --> 00:53:03.300]   but as an influential citizen,
[00:53:03.300 --> 00:53:06.080]   he should at least be thoughtful.
[00:53:06.080 --> 00:53:08.400]   And more importantly,
[00:53:10.620 --> 00:53:14.640]   companies need and I think would benefit
[00:53:14.640 --> 00:53:16.980]   from a Geneva Convention for the culture war
[00:53:16.980 --> 00:53:20.740]   in which, 'cause they're all being damaged
[00:53:20.740 --> 00:53:22.820]   by the culture war coming to the companies.
[00:53:22.820 --> 00:53:24.660]   What we need to get to, I hope,
[00:53:24.660 --> 00:53:27.620]   is a place where companies do,
[00:53:27.620 --> 00:53:30.100]   they have strong ethical obligations
[00:53:30.100 --> 00:53:31.420]   about the effects that they cause,
[00:53:31.420 --> 00:53:32.620]   about how they treat their employees,
[00:53:32.620 --> 00:53:33.500]   about their supply chain.
[00:53:33.500 --> 00:53:35.980]   They have strong ethical obligations,
[00:53:35.980 --> 00:53:39.220]   but they should not be weighing in on culture war issues.
[00:53:39.220 --> 00:53:41.220]   - Well, if I can read the exact tweet,
[00:53:41.220 --> 00:53:43.220]   'cause part of the tweet I like,
[00:53:43.220 --> 00:53:45.780]   he said, "In the past, I voted Democrat
[00:53:45.780 --> 00:53:50.260]   "because they were mostly the kindness party,
[00:53:50.260 --> 00:53:53.940]   "but they have become the party of division and hate,
[00:53:53.940 --> 00:53:58.340]   "so I can no longer support them and will vote Republican."
[00:53:58.340 --> 00:53:59.400]   And then he finishes with,
[00:53:59.400 --> 00:54:03.460]   "Now watch their dirty tricks campaign against me unfold."
[00:54:03.460 --> 00:54:04.860]   Okay.
[00:54:04.860 --> 00:54:05.700]   - What do you make of that?
[00:54:05.700 --> 00:54:07.220]   Like, what do you think he was thinking
[00:54:07.220 --> 00:54:10.420]   that he came out so blatantly as a partisan?
[00:54:10.420 --> 00:54:13.060]   - Because he's probably communicating with the board,
[00:54:13.060 --> 00:54:14.340]   with the people inside Twitter,
[00:54:14.340 --> 00:54:16.740]   and he's clearly seeing the lean.
[00:54:16.740 --> 00:54:18.340]   And he's responding to that lean.
[00:54:18.340 --> 00:54:23.260]   He's also opening the door to the potential,
[00:54:23.260 --> 00:54:27.900]   bringing back the former president onto the platform,
[00:54:27.900 --> 00:54:29.820]   and also bringing back,
[00:54:29.820 --> 00:54:31.540]   which he's probably looking at the numbers
[00:54:31.540 --> 00:54:34.300]   of the people who are behind Truth Social,
[00:54:34.300 --> 00:54:35.660]   saying that, okay,
[00:54:35.660 --> 00:54:39.460]   it seems that there's a strong lean in Twitter
[00:54:39.460 --> 00:54:41.620]   in terms of the left.
[00:54:41.620 --> 00:54:45.340]   And in fact, from what I see,
[00:54:45.340 --> 00:54:49.940]   it seems like the current operation of Twitter
[00:54:49.940 --> 00:54:54.940]   is the extremes of the left get outraged by something,
[00:54:54.940 --> 00:54:58.580]   and the extremes of the right point out
[00:54:58.580 --> 00:55:00.300]   how the left is ridiculous.
[00:55:00.300 --> 00:55:02.600]   Like, that seems to be the mechanism.
[00:55:02.600 --> 00:55:07.120]   And that's the source of the drama,
[00:55:07.120 --> 00:55:09.920]   and then the left gets very mad at the right
[00:55:09.920 --> 00:55:11.360]   that points out the ridiculousness,
[00:55:11.360 --> 00:55:13.320]   and there's this vicious kind of cycle.
[00:55:13.320 --> 00:55:14.720]   - That's the polarization cycle.
[00:55:14.720 --> 00:55:15.880]   That's what we're in.
[00:55:15.880 --> 00:55:17.160]   - There's something that happened here
[00:55:17.160 --> 00:55:19.440]   that there's a shift where,
[00:55:19.440 --> 00:55:20.400]   there's a decline, I would say,
[00:55:20.400 --> 00:55:23.080]   in both parties towards being shitty.
[00:55:23.080 --> 00:55:23.920]   - Okay, but look,
[00:55:23.920 --> 00:55:26.120]   with everything with the parties, that's not the issue.
[00:55:26.120 --> 00:55:29.560]   The issue is, should the most important CEO in America,
[00:55:29.560 --> 00:55:32.520]   the CEO of some of our biggest and most important companies,
[00:55:32.520 --> 00:55:35.360]   so let's imagine five years from now,
[00:55:35.360 --> 00:55:36.320]   two different worlds.
[00:55:36.320 --> 00:55:41.320]   In one world, the CEO of every Fortune 500 company has said,
[00:55:41.320 --> 00:55:44.000]   I'm a Republican because I hate those douchebags,
[00:55:44.000 --> 00:55:47.160]   or I'm a Democrat because I hate those Nazi racists.
[00:55:47.160 --> 00:55:48.600]   That's one world where everybody,
[00:55:48.600 --> 00:55:50.240]   everybody puts up a thing in their window,
[00:55:50.240 --> 00:55:53.020]   everybody, it's culture war everywhere, all the time,
[00:55:53.020 --> 00:55:54.360]   24 hours a day.
[00:55:54.360 --> 00:55:56.560]   You pick a doctor based on whether he's red or blue.
[00:55:56.560 --> 00:55:57.880]   Everything is culture war.
[00:55:57.880 --> 00:56:00.100]   That's one possible future, which we're headed towards.
[00:56:00.100 --> 00:56:02.320]   The other is, we say, you know what?
[00:56:02.320 --> 00:56:05.080]   Political conflict should be confined to political spaces.
[00:56:05.080 --> 00:56:06.200]   There is a room for protest,
[00:56:06.200 --> 00:56:08.360]   but you don't go protesting at people's private homes.
[00:56:08.360 --> 00:56:09.520]   You don't go threatening their children.
[00:56:09.520 --> 00:56:11.160]   You don't go doxing them.
[00:56:11.160 --> 00:56:12.240]   We have to have channels
[00:56:12.240 --> 00:56:14.560]   that are not culture war all the time.
[00:56:14.560 --> 00:56:16.440]   When you go shopping, when you go to a restaurant,
[00:56:16.440 --> 00:56:18.520]   you shouldn't be yelled at and screamed at.
[00:56:18.520 --> 00:56:19.400]   When you buy a product,
[00:56:19.400 --> 00:56:21.200]   you should be able to buy products from an excellent company.
[00:56:21.200 --> 00:56:23.520]   You shouldn't have to always think, what's the CEO?
[00:56:23.520 --> 00:56:25.080]   What is the, I mean, what an insane world,
[00:56:25.080 --> 00:56:25.920]   but that's where we're heading.
[00:56:25.920 --> 00:56:29.040]   So I think that Elon did a really bad thing
[00:56:29.040 --> 00:56:31.360]   in launching that tweet.
[00:56:31.360 --> 00:56:33.760]   That was, I think, really throwing fuel on a fire
[00:56:33.760 --> 00:56:36.680]   and setting a norm in which businesses
[00:56:36.680 --> 00:56:39.000]   are gonna get even more politicized than they are.
[00:56:39.000 --> 00:56:41.360]   - And you're saying specifically the problem
[00:56:41.360 --> 00:56:43.280]   was that he picked the side.
[00:56:43.280 --> 00:56:45.480]   - As the head of, yes, as the CEO,
[00:56:45.480 --> 00:56:48.280]   as the head of several major companies,
[00:56:48.280 --> 00:56:50.720]   of course we can find out what his views are.
[00:56:50.720 --> 00:56:52.040]   You know, it's not like it's, I mean,
[00:56:52.040 --> 00:56:53.320]   actually with him, it's maybe hard to know,
[00:56:53.320 --> 00:56:58.320]   but it's not that a CEO can't be a partisan or have views,
[00:56:58.320 --> 00:57:00.880]   but to publicly declare it in that way,
[00:57:00.880 --> 00:57:03.800]   in such a really insulting way,
[00:57:03.800 --> 00:57:05.040]   this is throwing fuel on the fire
[00:57:05.040 --> 00:57:07.880]   and it's setting a precedent that corporations
[00:57:07.880 --> 00:57:09.720]   are major players in the culture war.
[00:57:09.720 --> 00:57:10.720]   And I'm trying to reverse that.
[00:57:10.720 --> 00:57:11.880]   We've got to pull back from that.
[00:57:11.880 --> 00:57:13.400]   - Let me play devil's advocate here.
[00:57:13.400 --> 00:57:15.640]   So, 'cause I've gotten a chance to interact
[00:57:15.640 --> 00:57:17.200]   with quite a few CEOs,
[00:57:17.200 --> 00:57:22.200]   there is also a value for authenticity.
[00:57:22.200 --> 00:57:24.920]   So I'm guessing this was written while sitting
[00:57:24.920 --> 00:57:29.920]   on the toilet and I could see in a day from now
[00:57:30.200 --> 00:57:32.480]   saying, "LOL, just kidding."
[00:57:32.480 --> 00:57:35.920]   There's a humor, there's a lightness,
[00:57:35.920 --> 00:57:39.600]   there's a chaos element, and that's, chaos is not--
[00:57:39.600 --> 00:57:40.920]   - Yeah, that's not what we need right now.
[00:57:40.920 --> 00:57:41.880]   We don't need more chaos.
[00:57:41.880 --> 00:57:45.220]   - Well, so yes, there's a balance here.
[00:57:45.220 --> 00:57:47.680]   The chaos isn't engineered chaos,
[00:57:47.680 --> 00:57:50.060]   it's really authentically who he is.
[00:57:50.060 --> 00:57:51.640]   And I would like to say that there's--
[00:57:51.640 --> 00:57:52.480]   - I agree with that.
[00:57:52.480 --> 00:57:55.760]   - That's a trade-off, because if you become a politician,
[00:57:55.760 --> 00:57:58.040]   so there's a trade-off between, in this case,
[00:57:58.040 --> 00:58:01.600]   maybe authenticity and civility, maybe,
[00:58:01.600 --> 00:58:06.600]   like being calculating about the impact you have
[00:58:06.600 --> 00:58:09.120]   with your words versus just being yourself.
[00:58:09.120 --> 00:58:13.000]   And I'm not sure calculating is also a slippery slope.
[00:58:13.000 --> 00:58:15.120]   Both are slippery slopes, you have to be careful.
[00:58:15.120 --> 00:58:18.080]   - So when we have conversations in a vacuum
[00:58:18.080 --> 00:58:20.400]   and we just say like, "What should a person do?"
[00:58:20.400 --> 00:58:23.300]   Those are very hard, but our world is actually structured
[00:58:23.300 --> 00:58:25.520]   into domains and institutions.
[00:58:25.520 --> 00:58:29.200]   And if it's just like, "Oh, talking here among our friends,
[00:58:29.200 --> 00:58:31.160]   "like we should be authentic," sure.
[00:58:31.160 --> 00:58:34.460]   But the CEO of a company has fiduciary duties,
[00:58:34.460 --> 00:58:36.480]   legal fiduciary duties to the company,
[00:58:36.480 --> 00:58:38.640]   he owes loyalty to the company.
[00:58:38.640 --> 00:58:42.920]   And if he is using the company for his own political gain
[00:58:42.920 --> 00:58:44.960]   or other purposes or social standing,
[00:58:44.960 --> 00:58:48.440]   that's a violation of his fiduciary duty to the company.
[00:58:48.440 --> 00:58:49.640]   Now there's debate among scholars
[00:58:49.640 --> 00:58:51.520]   whether your fiduciary duty is to the shareholders,
[00:58:51.520 --> 00:58:53.160]   I don't think it's the shareholders.
[00:58:53.160 --> 00:58:55.120]   I think many legal experts say
[00:58:55.120 --> 00:58:57.600]   the company's a legal person,
[00:58:57.600 --> 00:58:58.820]   you have duties to the company,
[00:58:58.820 --> 00:59:00.800]   employees owe a duty to the company.
[00:59:00.800 --> 00:59:03.920]   So he's got those duties and I think he,
[00:59:03.920 --> 00:59:05.000]   you can say he's being authentic,
[00:59:05.000 --> 00:59:06.960]   but he's also violating those duties.
[00:59:06.960 --> 00:59:10.140]   So it's not necessarily he's violating a law by doing it,
[00:59:10.140 --> 00:59:12.880]   but he certainly is shredding any notion
[00:59:12.880 --> 00:59:14.840]   of professional ethics around leadership
[00:59:14.840 --> 00:59:16.760]   of a company in the modern age.
[00:59:16.760 --> 00:59:18.520]   - I think you have to take it in the full context
[00:59:18.520 --> 00:59:23.000]   because you see that he's not being a political player,
[00:59:23.000 --> 00:59:24.920]   he's just saying quit being douchey.
[00:59:24.920 --> 00:59:27.240]   - Suppose the CEO of Ford says,
[00:59:27.240 --> 00:59:30.320]   "You know what, let's pick a group."
[00:59:30.320 --> 00:59:32.000]   Well, I shouldn't do a racial group
[00:59:32.000 --> 00:59:32.880]   'cause that would be different.
[00:59:32.880 --> 00:59:35.080]   Let's just say, "You know what,
[00:59:35.080 --> 00:59:37.400]   "left-handed people are douchebags, I hate them."
[00:59:37.400 --> 00:59:38.880]   Like, why would you say that?
[00:59:38.880 --> 00:59:40.440]   Like, why would you drag away left-handed people?
[00:59:40.440 --> 00:59:45.240]   - What you said now is not either funny or like-hearted
[00:59:45.240 --> 00:59:47.160]   because I hate them, it wasn't funny.
[00:59:47.160 --> 00:59:49.640]   I'm not picking on you, I'm saying that statement.
[00:59:49.640 --> 00:59:53.000]   Words matter, there's a lightness to the statement
[00:59:53.000 --> 00:59:54.080]   in the full context.
[00:59:54.080 --> 00:59:57.160]   If you look at the timeline of the man,
[00:59:57.160 --> 01:00:01.060]   there's ridiculous memes and there's nonstop jokes
[01:00:01.060 --> 01:00:03.280]   that my big problem with the CEO of Ford
[01:00:03.280 --> 01:00:04.760]   is there's never any of that.
[01:00:04.760 --> 01:00:06.920]   Not only is there any of that,
[01:00:06.920 --> 01:00:09.420]   there's not a celebration of the beauty of the engineering
[01:00:09.420 --> 01:00:10.800]   of the different products.
[01:00:10.800 --> 01:00:12.760]   It's all political speak,
[01:00:12.760 --> 01:00:15.480]   channeled through multiple meetings of PR.
[01:00:15.480 --> 01:00:18.540]   There's levels upon levels upon levels
[01:00:18.540 --> 01:00:22.640]   where you think that it's really not authentic.
[01:00:22.640 --> 01:00:26.880]   And there, you're actually, by being polite,
[01:00:26.880 --> 01:00:29.800]   by being civil, you're actually playing politics
[01:00:29.800 --> 01:00:34.120]   because all of your actual political decision-making
[01:00:34.120 --> 01:00:36.060]   is done in the back channels.
[01:00:36.060 --> 01:00:37.120]   That's obvious.
[01:00:37.120 --> 01:00:41.320]   Here, here's a human being authentic
[01:00:41.320 --> 01:00:43.440]   and actually struggling with some of the ideas
[01:00:43.440 --> 01:00:44.840]   and having fun with it.
[01:00:45.220 --> 01:00:50.220]   I think this lightness represents the kind of positivity
[01:00:50.220 --> 01:00:52.260]   that we should be striving for.
[01:00:52.260 --> 01:00:53.660]   It's funny to say that
[01:00:53.660 --> 01:00:56.020]   because you're looking at these statements
[01:00:56.020 --> 01:00:57.420]   and they seem negative,
[01:00:57.420 --> 01:00:59.600]   but in the full context of social media,
[01:00:59.600 --> 01:01:01.140]   I don't know if they are.
[01:01:01.140 --> 01:01:03.020]   - But look at what you just said, in the full context.
[01:01:03.020 --> 01:01:05.620]   You're taking his tweets in context.
[01:01:05.620 --> 01:01:07.060]   You know who doesn't do that?
[01:01:07.060 --> 01:01:07.900]   Twitter.
[01:01:07.900 --> 01:01:09.420]   Like, that's the Twitter--
[01:01:09.420 --> 01:01:10.260]   - Well, that's the problem you're highlighting.
[01:01:10.260 --> 01:01:12.220]   - The rule of Twitter is there is no context.
[01:01:12.220 --> 01:01:14.140]   Everything is taken in the maximum possible way.
[01:01:14.140 --> 01:01:14.980]   There is no context.
[01:01:14.980 --> 01:01:15.800]   - Oh, you're saying--
[01:01:15.800 --> 01:01:16.640]   - So this is not like,
[01:01:16.640 --> 01:01:20.020]   you know, yes, I wish we did take people in context.
[01:01:20.020 --> 01:01:21.220]   I wish we lived in that world.
[01:01:21.220 --> 01:01:22.420]   But now that we have Twitter and Facebook,
[01:01:22.420 --> 01:01:23.500]   we don't live in that world anymore.
[01:01:23.500 --> 01:01:26.580]   - So you're saying it is a bit of responsibility
[01:01:26.580 --> 01:01:28.580]   for people with a large platform
[01:01:28.580 --> 01:01:30.580]   to consider the fact that
[01:01:30.580 --> 01:01:32.500]   there is the fundamental mechanism of Twitter
[01:01:32.500 --> 01:01:34.900]   where people don't give you the benefit of the doubt.
[01:01:34.900 --> 01:01:36.600]   - Well, I don't wanna hang it on large platform
[01:01:36.600 --> 01:01:38.180]   because then that's what a lot of people say.
[01:01:38.180 --> 01:01:40.060]   Like, well, you know, she shouldn't say that
[01:01:40.060 --> 01:01:41.220]   because she has a large platform
[01:01:41.220 --> 01:01:43.300]   and she should say things that agree with my politics.
[01:01:43.300 --> 01:01:44.820]   I don't wanna hang it on large platform.
[01:01:44.820 --> 01:01:47.660]   I wanna hang it on CEO of a company.
[01:01:47.660 --> 01:01:50.780]   CEOs of a company have duties and responsibilities.
[01:01:50.780 --> 01:01:52.300]   And, you know, Scott Galloway,
[01:01:52.300 --> 01:01:53.500]   I think is very clear about this.
[01:01:53.500 --> 01:01:54.780]   You know, he criticized Elon a lot
[01:01:54.780 --> 01:01:57.500]   as being a really bad role model for young men.
[01:01:57.500 --> 01:01:59.080]   Young men need role models
[01:01:59.080 --> 01:02:01.540]   and he is a very appealing, attractive role model.
[01:02:01.540 --> 01:02:02.420]   - So I agree with you,
[01:02:02.420 --> 01:02:04.820]   but in terms of being a role model,
[01:02:04.820 --> 01:02:08.540]   I think, I don't wanna put a responsibility on people,
[01:02:08.540 --> 01:02:11.380]   but yes, he could be a much, much better role model.
[01:02:11.380 --> 01:02:12.220]   There's--
[01:02:12.220 --> 01:02:14.020]   - You can't insult sitting senators by calling them old.
[01:02:14.020 --> 01:02:15.280]   I mean, that's, you know.
[01:02:15.280 --> 01:02:19.680]   - Yeah, I won't do a both-side-ism of like,
[01:02:19.680 --> 01:02:21.540]   well, those senators can be assholes too.
[01:02:21.540 --> 01:02:23.420]   - Yeah, yes, yes. - But that's fair enough.
[01:02:23.420 --> 01:02:25.460]   Respond intelligently, as I tweeted,
[01:02:25.460 --> 01:02:29.240]   to unintelligent treatment, yes, yes.
[01:02:29.240 --> 01:02:31.420]   So the reason I like, and he's now a friend,
[01:02:31.420 --> 01:02:34.340]   the reason I like Elon is because of the engineering,
[01:02:34.340 --> 01:02:35.580]   because of the work he does.
[01:02:35.580 --> 01:02:37.340]   - No, I admire him enormously for that.
[01:02:37.340 --> 01:02:41.200]   - But what I admire on the Twitter side is the authenticity
[01:02:41.200 --> 01:02:43.460]   because I've been a little bit jaded and worn out
[01:02:43.460 --> 01:02:48.020]   by people who have built up walls,
[01:02:48.020 --> 01:02:49.260]   people in the position of power,
[01:02:49.260 --> 01:02:51.660]   the CEOs and the politicians who built up walls
[01:02:51.660 --> 01:02:53.000]   and you don't see the real person.
[01:02:53.000 --> 01:02:55.500]   That's one of the reasons I love long-form podcasting,
[01:02:55.500 --> 01:02:58.240]   is especially if you talk more than 10 minutes,
[01:02:58.240 --> 01:02:59.940]   it's hard to have a wall up.
[01:02:59.940 --> 01:03:01.640]   It all kind of crumbles away.
[01:03:01.640 --> 01:03:03.840]   So I don't know, but yes, yes, you're right.
[01:03:03.840 --> 01:03:08.840]   That is a step backwards to say,
[01:03:10.340 --> 01:03:12.840]   at least to me, the biggest problem is to pick sides,
[01:03:12.840 --> 01:03:16.140]   to say I'm not going to vote this way or that way.
[01:03:16.140 --> 01:03:21.140]   That's, like leave that to the politicians.
[01:03:21.140 --> 01:03:26.000]   You have much, like the importance of social media
[01:03:26.000 --> 01:03:30.680]   is far bigger than the bickering,
[01:03:30.680 --> 01:03:33.080]   the short-term bickering of any one political party.
[01:03:33.080 --> 01:03:35.760]   It's a platform where we make progress,
[01:03:35.760 --> 01:03:40.760]   where we develop ideas through sort of rigorous discourse,
[01:03:40.760 --> 01:03:43.420]   all those kinds of things.
[01:03:43.420 --> 01:03:45.540]   - So, okay, so here's an idea about social media
[01:03:45.540 --> 01:03:48.060]   developed through social media from Elon,
[01:03:48.060 --> 01:03:52.620]   which is everyone freaks out because they think either,
[01:03:52.620 --> 01:03:54.620]   oh, he's going to do less content moderations.
[01:03:54.620 --> 01:03:55.540]   The left is freaking out
[01:03:55.540 --> 01:03:57.240]   because they want more content moderation.
[01:03:57.240 --> 01:03:58.140]   The right is celebrating
[01:03:58.140 --> 01:03:59.940]   because they think the people doing the content moderation
[01:03:59.940 --> 01:04:01.060]   are on the left.
[01:04:01.060 --> 01:04:04.180]   But there was one, I think it was a tweet,
[01:04:04.180 --> 01:04:05.220]   where he said like three things
[01:04:05.220 --> 01:04:06.380]   he was going to do to make it better.
[01:04:06.380 --> 01:04:08.080]   And it was, I would defeat the bots or something.
[01:04:08.080 --> 01:04:11.460]   But he said, authenticate all humans.
[01:04:11.460 --> 01:04:13.500]   And this is a hugely important statement.
[01:04:13.500 --> 01:04:15.580]   And it's pretty powerful that this guy
[01:04:15.580 --> 01:04:17.220]   can put three words in a tweet.
[01:04:17.220 --> 01:04:19.140]   And actually, I think this could change the world.
[01:04:19.140 --> 01:04:22.460]   Even if the bid fails, the fact that Elon said that,
[01:04:22.460 --> 01:04:26.260]   that he thinks we need to authenticate all humans is huge.
[01:04:26.260 --> 01:04:29.900]   Because now we're talking about solutions here.
[01:04:29.900 --> 01:04:32.700]   What can we do to make social media a better place
[01:04:32.700 --> 01:04:33.540]   for democracy,
[01:04:33.540 --> 01:04:35.940]   a place that actually makes democracy better?
[01:04:35.940 --> 01:04:38.140]   As Tristan Harris has pointed out,
[01:04:38.140 --> 01:04:39.820]   social media and digital technology,
[01:04:39.820 --> 01:04:41.700]   the Chinese are using it really skillfully
[01:04:41.700 --> 01:04:44.300]   to make a better authoritarian nation.
[01:04:44.300 --> 01:04:45.660]   And by better, I don't mean morally better.
[01:04:45.660 --> 01:04:48.860]   I mean like more stable, successful.
[01:04:48.860 --> 01:04:51.060]   Whereas we're using it to make ourselves weaker,
[01:04:51.060 --> 01:04:53.500]   more fragmented, and more insane.
[01:04:53.500 --> 01:04:55.820]   So we're on the way down.
[01:04:55.820 --> 01:04:57.500]   We're in big trouble.
[01:04:57.500 --> 01:05:01.180]   And all the argument is about content moderation.
[01:05:01.180 --> 01:05:02.720]   And what we learned from Francis Haugen
[01:05:02.720 --> 01:05:06.740]   is that what, five or 10% of what they might call
[01:05:06.740 --> 01:05:09.900]   hate speech gets caught, 1% of violence and intimidation.
[01:05:09.900 --> 01:05:11.660]   Content moderation, even if we do a lot more of it,
[01:05:11.660 --> 01:05:13.700]   isn't gonna make a big difference.
[01:05:13.700 --> 01:05:18.100]   All the powers and the dynamics changes to the architecture.
[01:05:18.100 --> 01:05:20.460]   And as I said in my Atlantic article,
[01:05:20.460 --> 01:05:22.140]   what are the reforms that would matter for social media?
[01:05:22.140 --> 01:05:23.420]   And the number one thing I said,
[01:05:23.420 --> 01:05:27.380]   the number one thing I believe is user authentication
[01:05:27.380 --> 01:05:28.900]   or user verification.
[01:05:28.900 --> 01:05:30.700]   And people freak out and they say like,
[01:05:30.700 --> 01:05:31.700]   oh, but we need anonymous.
[01:05:31.700 --> 01:05:33.760]   Like, yeah, fine, you can be anonymous.
[01:05:33.760 --> 01:05:36.460]   But what I think needs to be done is
[01:05:36.460 --> 01:05:39.420]   anyone can open an account on Twitter, Facebook, whatever,
[01:05:39.420 --> 01:05:41.540]   as long as you're over 16, and that's another piece.
[01:05:41.540 --> 01:05:44.660]   Once you're 16 or 18, at a certain age,
[01:05:44.660 --> 01:05:46.780]   you can be treated like an adult.
[01:05:46.780 --> 01:05:50.220]   You can open an account and you can look, you can read,
[01:05:50.220 --> 01:05:52.740]   and you can make up whatever fake name you want.
[01:05:52.740 --> 01:05:54.260]   But if you want to post,
[01:05:54.260 --> 01:05:57.420]   if you want the viral amplification
[01:05:57.420 --> 01:06:00.560]   on a company that has section 230 protection from lawsuits,
[01:06:00.560 --> 01:06:01.900]   which is a very special privilege,
[01:06:01.900 --> 01:06:03.260]   I understand the need for it,
[01:06:03.260 --> 01:06:05.260]   but it's an incredibly powerful privilege
[01:06:05.260 --> 01:06:07.580]   to protect them from lawsuits.
[01:06:07.580 --> 01:06:10.980]   If you want to be able to post on platforms that,
[01:06:10.980 --> 01:06:13.100]   as we'll get to in the Google Doc,
[01:06:13.100 --> 01:06:15.180]   there's a lot of evidence that they are undermining
[01:06:15.180 --> 01:06:16.520]   and damaging democracy.
[01:06:16.520 --> 01:06:20.780]   Then the company has this minimal responsibility
[01:06:20.780 --> 01:06:21.820]   it has to meet.
[01:06:21.820 --> 01:06:23.460]   Banks have know your customer laws.
[01:06:23.460 --> 01:06:25.420]   You can't just walk up to a bank with a bag of money
[01:06:25.420 --> 01:06:27.740]   that you stole and say, here, deposit this for me.
[01:06:27.740 --> 01:06:29.280]   My name's John Smith.
[01:06:29.280 --> 01:06:31.420]   You have to actually show who you are.
[01:06:31.420 --> 01:06:33.460]   And the bank isn't gonna announce who you are publicly,
[01:06:33.460 --> 01:06:35.620]   but you have to, if they're gonna do business with you,
[01:06:35.620 --> 01:06:39.620]   they need to know you're a real person, not a criminal.
[01:06:39.620 --> 01:06:42.940]   And so there's a lot of schemes for how to do this.
[01:06:42.940 --> 01:06:44.020]   There's multiple levels.
[01:06:44.020 --> 01:06:45.340]   People don't seem to understand this.
[01:06:45.340 --> 01:06:48.020]   Level zero of authentication is nothing.
[01:06:48.020 --> 01:06:49.200]   That's what we have now.
[01:06:49.200 --> 01:06:51.740]   Level one, this might be what Elon meant,
[01:06:51.740 --> 01:06:53.240]   authenticate all humans,
[01:06:53.240 --> 01:06:55.900]   meaning you have to at least pass a CAPTCHA or some test
[01:06:55.900 --> 01:06:57.300]   to show you're not a bot.
[01:06:57.300 --> 01:06:58.600]   There's no identity, there's nothing.
[01:06:58.600 --> 01:07:00.300]   Just something that, you know,
[01:07:00.300 --> 01:07:03.500]   it's a constant cat and mouse struggle between bots.
[01:07:03.500 --> 01:07:06.160]   So we try to just filter out pure bots.
[01:07:06.160 --> 01:07:08.720]   The next level up, there are a variety of schemes
[01:07:08.720 --> 01:07:11.120]   that allow you to authenticate identity
[01:07:11.120 --> 01:07:13.640]   in ways that are not traceable or kept.
[01:07:13.640 --> 01:07:17.380]   So some, whether you show an ID, whether you use biometric,
[01:07:17.380 --> 01:07:19.580]   whether you have something on the blockchain
[01:07:19.580 --> 01:07:22.340]   that establishes identity, whether it's linked to a phone,
[01:07:22.340 --> 01:07:24.580]   whatever it is, there are multiple schemes now
[01:07:24.580 --> 01:07:27.540]   that companies have figured out for how to do this.
[01:07:27.540 --> 01:07:30.920]   And so if you did that, then in order to get an account
[01:07:30.920 --> 01:07:33.620]   where you have posting privileges on Facebook or Twitter
[01:07:33.620 --> 01:07:36.860]   or TikTok or whatever, you have to at least do that.
[01:07:36.860 --> 01:07:39.220]   And if you do that, you know,
[01:07:39.220 --> 01:07:44.220]   now the other people are real humans too.
[01:07:44.220 --> 01:07:46.420]   And suddenly our public square's a lot nicer
[01:07:46.420 --> 01:07:48.140]   'cause you don't have bots swarming around.
[01:07:48.140 --> 01:07:49.460]   This would also cut down on trolls.
[01:07:49.460 --> 01:07:51.420]   You still have trolls who use their real name,
[01:07:51.420 --> 01:07:54.780]   but this would just make it a little scarier for trolls.
[01:07:54.780 --> 01:07:57.100]   Some men turn into complete assholes.
[01:07:57.100 --> 01:07:59.100]   They can be very polite in real life,
[01:07:59.100 --> 01:08:01.300]   but some men, as soon as they have the anonymity,
[01:08:01.300 --> 01:08:03.380]   they start using racial slurs.
[01:08:03.380 --> 01:08:04.820]   They're horrible.
[01:08:04.820 --> 01:08:08.180]   One troll can ruin thousands of people's day.
[01:08:08.180 --> 01:08:12.640]   - You know, I'm somebody who believes in free speech.
[01:08:12.640 --> 01:08:15.940]   And so there's been a lot of discussions about this.
[01:08:15.940 --> 01:08:18.940]   And we'll ask you some questions about this too.
[01:08:18.940 --> 01:08:23.940]   But there is, the tension there is the power of a troll
[01:08:26.000 --> 01:08:27.800]   to ruin the party.
[01:08:27.800 --> 01:08:28.940]   - Yeah, that's right.
[01:08:28.940 --> 01:08:31.060]   - So like this idea of free speech,
[01:08:31.060 --> 01:08:35.840]   boy, do you have to also consider
[01:08:35.840 --> 01:08:39.920]   if you wanna have a private party and enjoy your time,
[01:08:39.920 --> 01:08:43.640]   challenging, lots of disagreement,
[01:08:43.640 --> 01:08:45.340]   debate, all that kind of stuff, but fun.
[01:08:45.340 --> 01:08:48.840]   No like annoying person screaming,
[01:08:48.840 --> 01:08:52.160]   just not disagreeing, but just like spilling
[01:08:52.160 --> 01:08:54.920]   like the drinks all over the place.
[01:08:55.920 --> 01:08:57.400]   Yeah, all that kind of stuff.
[01:08:57.400 --> 01:09:02.240]   So see, you're saying it's a step in the right direction
[01:09:02.240 --> 01:09:07.240]   to at least verify the humanness of a person
[01:09:07.240 --> 01:09:08.840]   while maintaining anonymity.
[01:09:08.840 --> 01:09:12.720]   So that's one step, but the further step,
[01:09:12.720 --> 01:09:15.480]   that's maybe doesn't go all the way
[01:09:15.480 --> 01:09:17.200]   because you can still figure out ways
[01:09:17.200 --> 01:09:19.080]   to create multiple accounts and you can--
[01:09:19.080 --> 01:09:19.960]   - But it's a lot harder.
[01:09:19.960 --> 01:09:21.840]   So actually there's a lot of ways to do this.
[01:09:21.840 --> 01:09:23.120]   There's a lot of creativity out there
[01:09:23.120 --> 01:09:24.260]   about solving this problem.
[01:09:24.260 --> 01:09:27.140]   So if you go to the social media and political dysfunction,
[01:09:27.140 --> 01:09:29.800]   Google Doc that I created with Chris Bale,
[01:09:29.800 --> 01:09:32.880]   and then you go to section 11,
[01:09:32.880 --> 01:09:35.100]   proposals for improving social media.
[01:09:35.100 --> 01:09:39.400]   So we're collecting there now some of the ideas
[01:09:39.400 --> 01:09:41.800]   for how to do user authentication.
[01:09:41.800 --> 01:09:46.480]   And so one is WorldCoin, there's one human-id.org.
[01:09:46.480 --> 01:09:50.240]   This is a new organization created by an NYU Stern student
[01:09:50.240 --> 01:09:52.240]   who just came into my office last week,
[01:09:52.240 --> 01:09:54.160]   working with some other people.
[01:09:54.160 --> 01:09:56.360]   And what they do here is they have a method
[01:09:56.360 --> 01:10:00.840]   of identity verification that is keyed to your phone.
[01:10:00.840 --> 01:10:03.280]   So you do have to have a phone number.
[01:10:03.280 --> 01:10:06.160]   And of course you can buy seven different phone numbers
[01:10:06.160 --> 01:10:08.760]   if you want, but it's gonna be about 20 or $30 a month.
[01:10:08.760 --> 01:10:11.320]   So nobody's gonna buy a thousand phones.
[01:10:11.320 --> 01:10:15.600]   So yeah, you don't have just one unique ID,
[01:10:15.600 --> 01:10:18.400]   but most people do and nobody has a thousand.
[01:10:18.400 --> 01:10:20.600]   So just things like this
[01:10:20.600 --> 01:10:22.520]   that would make an enormous difference.
[01:10:22.520 --> 01:10:24.800]   So here's the way that I think about it.
[01:10:24.800 --> 01:10:27.280]   Imagine a public square in which the incentives
[01:10:27.280 --> 01:10:28.700]   are to be an asshole,
[01:10:28.700 --> 01:10:30.480]   that the more you kick people in the shins
[01:10:30.480 --> 01:10:32.680]   and spit on them and throw things at them,
[01:10:32.680 --> 01:10:34.080]   the more people applaud you.
[01:10:34.080 --> 01:10:37.420]   Okay, so that's the public square we have now.
[01:10:37.420 --> 01:10:39.080]   Not for most people, but as you said,
[01:10:39.080 --> 01:10:41.280]   just one troll can ruin it for everybody.
[01:10:41.280 --> 01:10:43.600]   If there's a thousand of us in the public square
[01:10:43.600 --> 01:10:48.240]   and 10 are incentivized to kick us and throw shit at us,
[01:10:48.240 --> 01:10:50.200]   it's no fun to be in that public square.
[01:10:50.200 --> 01:10:52.760]   So right now, I think Twitter in particular
[01:10:52.760 --> 01:10:54.560]   is making our public square much worse.
[01:10:54.560 --> 01:10:56.040]   It's making our democracy much weaker,
[01:10:56.040 --> 01:10:59.560]   much more divided, it's bringing us down.
[01:10:59.560 --> 01:11:01.560]   Imagine if we changed the incentives.
[01:11:01.560 --> 01:11:04.160]   Imagine if the incentive was to be constructive.
[01:11:04.160 --> 01:11:05.880]   And so this is an idea that I've been kicking around.
[01:11:05.880 --> 01:11:07.340]   I talked about it with Reid Hoffman last week
[01:11:07.340 --> 01:11:09.380]   and he seemed to think it's a good idea.
[01:11:09.380 --> 01:11:14.480]   And it would be very easy to,
[01:11:14.480 --> 01:11:16.600]   rather than trying to focus on posts,
[01:11:16.600 --> 01:11:19.000]   what post is fake or whatever,
[01:11:19.000 --> 01:11:22.880]   focus on users, what users are incredibly aggressive.
[01:11:22.880 --> 01:11:24.120]   And so people just use a lot of
[01:11:24.120 --> 01:11:27.040]   obscenity and exclamation points.
[01:11:27.040 --> 01:11:30.880]   AI could easily code nastiness or just aggression, hostility.
[01:11:30.880 --> 01:11:33.040]   And imagine if every user is rated
[01:11:33.040 --> 01:11:35.840]   on a one to five scale for that.
[01:11:35.840 --> 01:11:37.680]   And the default, when you open an account
[01:11:37.680 --> 01:11:40.600]   on Twitter or Facebook, the default is four.
[01:11:40.600 --> 01:11:43.280]   You will see everybody who's a four and below,
[01:11:43.280 --> 01:11:45.400]   but you won't even see the people who are fives
[01:11:45.400 --> 01:11:46.940]   and they don't get to see you.
[01:11:46.940 --> 01:11:49.500]   So they can say what they want, free speech.
[01:11:49.500 --> 01:11:52.060]   We're not censoring them, they can say what they want.
[01:11:52.060 --> 01:11:55.780]   But now there's actually an incentive to not be an asshole.
[01:11:55.780 --> 01:11:57.260]   'Cause the more of an asshole you are,
[01:11:57.260 --> 01:11:59.180]   the more people block you out.
[01:11:59.180 --> 01:12:01.220]   So imagine our country goes in two directions.
[01:12:01.220 --> 01:12:03.460]   In one, things continue to deteriorate
[01:12:03.460 --> 01:12:05.180]   and we have no way to have a public square
[01:12:05.180 --> 01:12:06.780]   in which we could actually talk about things.
[01:12:06.780 --> 01:12:10.020]   And in the other, we actually try to disincentivize
[01:12:10.020 --> 01:12:13.500]   being an asshole and encourage being constructive.
[01:12:13.500 --> 01:12:14.540]   What do you think?
[01:12:14.540 --> 01:12:17.260]   - Well, this is because I'm an AI person.
[01:12:17.260 --> 01:12:20.420]   And I very much, ever since I talked to Jack
[01:12:20.420 --> 01:12:22.140]   about the health of conversation,
[01:12:22.140 --> 01:12:23.860]   I've been looking at a lot of the machine learning models
[01:12:23.860 --> 01:12:27.300]   involved and I believe that the nastiness classification
[01:12:27.300 --> 01:12:29.180]   is a difficult problem automatically.
[01:12:29.180 --> 01:12:30.020]   - I'm sure it is.
[01:12:30.020 --> 01:12:35.020]   - So I personally believe in crowdsourced nastiness labeling.
[01:12:35.020 --> 01:12:39.780]   But in an objective way where it doesn't become
[01:12:39.780 --> 01:12:43.900]   viral mob cancellation type of dynamics.
[01:12:43.900 --> 01:12:46.300]   But more sort of objectively,
[01:12:46.300 --> 01:12:49.380]   is this a shitty, almost out of context,
[01:12:49.380 --> 01:12:52.420]   with only local context.
[01:12:52.420 --> 01:12:54.300]   Is this a shitty thing to say at this moment?
[01:12:54.300 --> 01:12:55.420]   Because here's the thing.
[01:12:55.420 --> 01:12:57.780]   - Well, wait, no, but we don't care about individual posts.
[01:12:57.780 --> 01:12:58.620]   - No, no, but--
[01:12:58.620 --> 01:12:59.660]   - All that matters is the average.
[01:12:59.660 --> 01:13:01.060]   - The posts make the man.
[01:13:01.060 --> 01:13:02.740]   - They do, but the point is,
[01:13:02.740 --> 01:13:04.540]   as long as we're talking about averages here,
[01:13:04.540 --> 01:13:07.500]   one person has a misclassified post, it doesn't matter.
[01:13:07.500 --> 01:13:08.340]   - Right, right, right.
[01:13:08.340 --> 01:13:10.580]   Yeah, yeah, but you need to classify posts
[01:13:10.580 --> 01:13:12.020]   in order to build up the average.
[01:13:12.020 --> 01:13:13.260]   That's what I mean.
[01:13:13.260 --> 01:13:16.700]   And so I really like that idea,
[01:13:16.700 --> 01:13:20.820]   the high level idea of incentivizing people
[01:13:20.820 --> 01:13:22.300]   to be less shitty.
[01:13:22.300 --> 01:13:25.220]   'Cause that's what, we have that incentive in real life.
[01:13:25.220 --> 01:13:26.060]   - Yeah, that's right.
[01:13:26.060 --> 01:13:29.040]   - It's actually really painful to be a full-time asshole,
[01:13:29.040 --> 01:13:30.980]   I think, in physical reality.
[01:13:30.980 --> 01:13:32.100]   - That's right, you'd be cut off.
[01:13:32.100 --> 01:13:36.700]   - It should be also pain to be an asshole on the internet.
[01:13:36.700 --> 01:13:38.420]   There could be different mechanisms for that.
[01:13:38.420 --> 01:13:41.340]   I wish AI was there, machine learning models were there.
[01:13:41.340 --> 01:13:42.900]   They just aren't yet.
[01:13:42.900 --> 01:13:44.220]   But how about we have,
[01:13:44.220 --> 01:13:46.900]   so one track is we have AI machine learning models
[01:13:46.900 --> 01:13:48.060]   and they render a verdict.
[01:13:48.060 --> 01:13:49.740]   Another class is crowdsourcing.
[01:13:49.740 --> 01:13:52.260]   You get, and then whenever the two disagree,
[01:13:52.260 --> 01:13:55.500]   you have staff at Twitter or whatever,
[01:13:55.500 --> 01:13:57.380]   they look at it and they say, "What's going on here?"
[01:13:57.380 --> 01:13:59.700]   And that way you can refine both the AI
[01:13:59.700 --> 01:14:01.340]   and you can refine whatever the algorithms are
[01:14:01.340 --> 01:14:02.180]   for the crowdsourcing.
[01:14:02.180 --> 01:14:03.420]   Because of course that can be gamed
[01:14:03.420 --> 01:14:05.260]   and people can only, "Hey, let's all rate this guy
[01:14:05.260 --> 01:14:06.940]   as really aggressive."
[01:14:06.940 --> 01:14:09.580]   So you wouldn't want just to rely on one single track.
[01:14:09.580 --> 01:14:12.100]   But if you have two tracks, I think you could do it.
[01:14:12.940 --> 01:14:15.040]   - What do you think about this word misinformation
[01:14:15.040 --> 01:14:18.660]   that maybe connects to our two discussions now?
[01:14:18.660 --> 01:14:23.620]   So one is the discussion of social media and democracy.
[01:14:23.620 --> 01:14:27.060]   And then the other is the coddling of the American mind.
[01:14:27.060 --> 01:14:31.340]   I've seen the word misinformation misused
[01:14:31.340 --> 01:14:35.700]   or used as a bullying word, like racism and so on,
[01:14:35.700 --> 01:14:40.700]   which are important concepts to identify,
[01:14:41.140 --> 01:14:45.980]   but they're nevertheless instead overused.
[01:14:45.980 --> 01:14:46.900]   Does that worry you?
[01:14:46.900 --> 01:14:49.440]   Because that seems to be the mechanism
[01:14:49.440 --> 01:14:52.540]   from inside Twitter, from inside Facebook,
[01:14:52.540 --> 01:14:54.900]   to label information you don't like
[01:14:54.900 --> 01:14:58.260]   versus information that's actually
[01:14:58.260 --> 01:15:00.860]   fundamentally harmful to society.
[01:15:00.860 --> 01:15:03.660]   - So I think there is a meaning of disinformation
[01:15:03.660 --> 01:15:06.900]   that is very useful and helpful,
[01:15:06.900 --> 01:15:09.420]   which is when you have a concerted campaign
[01:15:09.420 --> 01:15:14.340]   by Russian agents to plant a story and spread it.
[01:15:14.340 --> 01:15:17.640]   And they've been doing that since the '50s or '40s even.
[01:15:17.640 --> 01:15:19.700]   - That's what this podcast actually is.
[01:15:19.700 --> 01:15:21.060]   - Is what? (laughs)
[01:15:21.060 --> 01:15:21.900]   - It's a disinformation campaign by the Russians.
[01:15:21.900 --> 01:15:24.340]   - Yeah, you seem really Soviet to me, buddy.
[01:15:24.340 --> 01:15:28.820]   - It's subtle, it's between the lines.
[01:15:28.820 --> 01:15:29.660]   Okay, I'm sorry.
[01:15:29.660 --> 01:15:33.860]   - So I think to the extent that there are campaigns
[01:15:33.860 --> 01:15:36.500]   by either foreign agents or just by
[01:15:36.500 --> 01:15:38.100]   the Republican or Democratic parties,
[01:15:38.100 --> 01:15:39.740]   there have been examples of that,
[01:15:39.740 --> 01:15:43.540]   there are all kinds of concerted campaigns
[01:15:43.540 --> 01:15:48.540]   that are intending to confuse or spread lies.
[01:15:48.540 --> 01:15:52.420]   This is the Soviet, the firehose of falsehood tactic.
[01:15:52.420 --> 01:15:53.760]   So it's very useful for that.
[01:15:53.760 --> 01:15:56.260]   All the companies need to have pretty large staffs,
[01:15:56.260 --> 01:15:57.100]   I think, to deal with that,
[01:15:57.100 --> 01:15:58.380]   'cause that will always be there.
[01:15:58.380 --> 01:16:00.680]   And that is really bad for our country.
[01:16:00.680 --> 01:16:03.700]   So Renee Duresta is just brilliant on this.
[01:16:03.700 --> 01:16:05.620]   Reading her work has really frightened me
[01:16:05.620 --> 01:16:07.860]   and opened my eyes about how easy it is
[01:16:07.860 --> 01:16:12.420]   to manipulate and spread misinformation
[01:16:12.420 --> 01:16:14.540]   and especially polarization.
[01:16:14.540 --> 01:16:17.860]   The Russians have been trying since the '50s,
[01:16:17.860 --> 01:16:19.580]   they would come to America and they would do hate crimes.
[01:16:19.580 --> 01:16:21.820]   They would spray swastikas in synagogues
[01:16:21.820 --> 01:16:24.100]   to make, and they spray anti-blackslurs.
[01:16:24.100 --> 01:16:25.420]   They try to make Americans feel
[01:16:25.420 --> 01:16:27.220]   that they're as divided as possible.
[01:16:27.220 --> 01:16:31.000]   Most of the debate nowadays, however, is not that.
[01:16:31.000 --> 01:16:32.900]   It seems to be people are talking about
[01:16:32.900 --> 01:16:34.500]   what the other side is saying.
[01:16:34.500 --> 01:16:37.140]   So if you're on the right,
[01:16:37.140 --> 01:16:39.420]   then you're very conscious of the times when,
[01:16:39.420 --> 01:16:42.900]   well, the left wouldn't let us even say,
[01:16:42.900 --> 01:16:44.300]   could COVID be from a lab?
[01:16:44.300 --> 01:16:46.500]   They would, you literally would get shut down
[01:16:46.500 --> 01:16:47.780]   for saying that, and it turns out,
[01:16:47.780 --> 01:16:49.660]   well, we don't know if it's true,
[01:16:49.660 --> 01:16:51.340]   but there's at least a real likelihood that it is,
[01:16:51.340 --> 01:16:52.260]   and it certainly is something
[01:16:52.260 --> 01:16:53.500]   that should have been talked about.
[01:16:53.500 --> 01:16:57.460]   So I tend to stay away from any such discussions.
[01:16:57.460 --> 01:16:58.980]   And the reason is twofold.
[01:16:58.980 --> 01:17:01.540]   One is because they're almost entirely partisan.
[01:17:01.540 --> 01:17:03.260]   It generally is each side thinks
[01:17:03.260 --> 01:17:07.380]   what the other side is saying is misinformation
[01:17:07.380 --> 01:17:10.980]   or disinformation, and they can prove certain examples.
[01:17:10.980 --> 01:17:12.460]   So we're not gonna get anywhere on that.
[01:17:12.460 --> 01:17:14.660]   We certainly are never gonna get 60 votes in the Senate
[01:17:14.660 --> 01:17:16.780]   for anything about that.
[01:17:16.780 --> 01:17:17.940]   I don't think content moderation
[01:17:17.940 --> 01:17:19.700]   is nearly as important as people think.
[01:17:19.700 --> 01:17:21.740]   It has to be done, and it can be improved.
[01:17:21.740 --> 01:17:23.540]   Almost all the action is in the dynamics,
[01:17:23.540 --> 01:17:25.340]   the architecture, the virality,
[01:17:25.340 --> 01:17:30.340]   and then the nature of who is on the platform,
[01:17:30.340 --> 01:17:33.060]   unverified people, and how much amplification they get.
[01:17:33.060 --> 01:17:34.660]   That's what we should be looking at,
[01:17:34.660 --> 01:17:36.940]   rather than wasting so much of our breath
[01:17:36.940 --> 01:17:38.620]   on whether we're gonna do a little more
[01:17:38.620 --> 01:17:40.180]   or a little less content moderation.
[01:17:40.180 --> 01:17:44.540]   - So the true harm to society on average
[01:17:44.540 --> 01:17:47.060]   and over the long term is in the dynamics,
[01:17:47.060 --> 01:17:49.780]   is fundamentally in the dynamics of social media,
[01:17:49.780 --> 01:17:53.780]   not in the subtle choices of content moderation,
[01:17:53.780 --> 01:17:55.140]   aka censorship.
[01:17:55.140 --> 01:17:55.980]   - Exactly.
[01:17:55.980 --> 01:17:57.900]   There've always been conspiracy theories.
[01:17:57.900 --> 01:18:01.180]   You know, "The Turner Diaries" is this book written in 1978.
[01:18:01.180 --> 01:18:05.900]   It introduced the replacement theory to a lot of people.
[01:18:05.900 --> 01:18:09.020]   Timothy McVeigh had it on him when he was captured in 1995
[01:18:09.020 --> 01:18:10.340]   after the Oklahoma City bombing.
[01:18:10.340 --> 01:18:12.820]   It's a kind of a Bible of that fringe,
[01:18:12.820 --> 01:18:16.380]   violent, racist, white supremacist group.
[01:18:16.380 --> 01:18:20.860]   So the killer in "Buffalo"
[01:18:20.860 --> 01:18:22.340]   was well acquainted with these ideas.
[01:18:22.340 --> 01:18:25.500]   They've been around, but this guy's from a small town.
[01:18:25.500 --> 01:18:26.740]   I forget where he's from.
[01:18:26.740 --> 01:18:30.860]   But he was, as he says in a manifesto,
[01:18:30.860 --> 01:18:33.780]   he was entirely influenced by things he found online.
[01:18:33.780 --> 01:18:36.300]   He was not influenced by anyone he met in person.
[01:18:36.300 --> 01:18:40.440]   Ideas spread and communities can form,
[01:18:40.440 --> 01:18:43.120]   these like micro communities can form
[01:18:43.120 --> 01:18:45.560]   with bizarre and twisted beliefs.
[01:18:45.560 --> 01:18:49.500]   And this is, again, back to the "Atlantic" article.
[01:18:49.500 --> 01:18:52.500]   I've got this amazing quote from Martin Goury.
[01:18:52.500 --> 01:18:53.500]   I mean, just find it.
[01:18:53.500 --> 01:18:56.660]   But Martin Goury, he was a former CIA analyst,
[01:18:56.660 --> 01:18:59.860]   wrote this brilliant book called "The Revolt of the Public"
[01:18:59.860 --> 01:19:04.860]   or, and he has this great quote.
[01:19:04.860 --> 01:19:09.780]   He says, he talks about how in the age of mass media,
[01:19:09.780 --> 01:19:11.940]   we were all in a sense looking at a mirror,
[01:19:11.940 --> 01:19:12.860]   looking back at us.
[01:19:12.860 --> 01:19:14.480]   And it might've been a distorted mirror,
[01:19:14.480 --> 01:19:18.580]   but we had stories in common, we had facts in common.
[01:19:18.580 --> 01:19:20.460]   It was mass media.
[01:19:20.460 --> 01:19:22.480]   And he describes how the flood of information
[01:19:22.480 --> 01:19:25.460]   with the internet is like a tsunami washing over us.
[01:19:25.460 --> 01:19:26.980]   It has all kinds of effects.
[01:19:26.980 --> 01:19:30.340]   And he says, this isn't a comment in an interview on Vox.
[01:19:30.340 --> 01:19:33.940]   He says, "The digital revolution has shattered that mirror.
[01:19:33.940 --> 01:19:37.760]   And now the public inhabits those broken pieces of glass.
[01:19:37.760 --> 01:19:39.340]   So the public isn't one thing.
[01:19:39.340 --> 01:19:43.080]   It's highly fragmented and it's basically mutually hostile.
[01:19:43.080 --> 01:19:45.260]   It's mostly people yelling at each other
[01:19:45.260 --> 01:19:48.060]   and living in bubbles of one sort or another."
[01:19:48.060 --> 01:19:51.740]   And so, we now see clearly there's this little bubble
[01:19:51.740 --> 01:19:56.360]   of just bizarre nastiness in which the killer
[01:19:56.360 --> 01:19:58.460]   in Christchurch and the killer in Norway
[01:19:58.460 --> 01:20:01.620]   and now in Buffalo, they're all put into a community
[01:20:01.620 --> 01:20:03.220]   and posts flow up within that community
[01:20:03.220 --> 01:20:05.060]   by a certain dynamic.
[01:20:05.060 --> 01:20:09.300]   So we can never stamp those words or ideas out.
[01:20:09.300 --> 01:20:12.340]   The question is not, can we stop them from existing?
[01:20:12.340 --> 01:20:14.820]   The question is, what platforms,
[01:20:14.820 --> 01:20:16.900]   what are the platforms by which they spread
[01:20:16.900 --> 01:20:18.760]   all over the world and into every little town
[01:20:18.760 --> 01:20:21.240]   so that the 1% of whatever,
[01:20:21.240 --> 01:20:23.620]   whatever percentage of young men are vulnerable to this,
[01:20:23.620 --> 01:20:25.640]   that they get exposed to it.
[01:20:25.640 --> 01:20:27.820]   It's in the dynamics and the architecture.
[01:20:27.820 --> 01:20:29.740]   - It's a fascinating point to think about
[01:20:29.740 --> 01:20:33.180]   'cause we often debate and think about
[01:20:33.180 --> 01:20:36.180]   the content moderation, the censorship,
[01:20:36.180 --> 01:20:38.460]   the ideas of free speech, but you're saying,
[01:20:38.460 --> 01:20:40.060]   yes, that's important to talk about,
[01:20:40.060 --> 01:20:43.140]   but much more important is fixing the dynamics.
[01:20:43.140 --> 01:20:44.120]   - That's right, 'cause everyone thinks
[01:20:44.120 --> 01:20:45.780]   if there's regulation, it means censorship.
[01:20:45.780 --> 01:20:46.980]   At least people on the right think
[01:20:46.980 --> 01:20:49.140]   regulation equals censorship.
[01:20:49.140 --> 01:20:50.660]   And I'm trying to say, no, no,
[01:20:50.660 --> 01:20:53.100]   that's only if all we talk about is content moderation.
[01:20:53.100 --> 01:20:55.180]   Well, then yes, that is the framework.
[01:20:55.180 --> 01:20:57.440]   You know, how much or how little do we, you know,
[01:20:57.440 --> 01:20:59.300]   but I don't even wanna talk about that
[01:20:59.300 --> 01:21:00.960]   'cause all the action is in the dynamics.
[01:21:00.960 --> 01:21:02.080]   That's the point of my article.
[01:21:02.080 --> 01:21:03.560]   It's the architecture changed
[01:21:03.560 --> 01:21:05.440]   and our social world went insane.
[01:21:05.440 --> 01:21:09.400]   - So can you try to steel man the other side?
[01:21:09.400 --> 01:21:12.640]   So the people that might say that social media
[01:21:12.640 --> 01:21:15.360]   is good for society overall,
[01:21:15.360 --> 01:21:18.400]   both in the dimension of mental health,
[01:21:18.400 --> 01:21:23.000]   as Mark said, for teenagers, teenage girls,
[01:21:23.000 --> 01:21:24.640]   and for our democracy.
[01:21:24.640 --> 01:21:26.920]   Yes, there's a lot of negative things,
[01:21:26.920 --> 01:21:30.340]   but that's slices of data.
[01:21:30.340 --> 01:21:33.240]   If you look at the whole, which is difficult to measure,
[01:21:33.240 --> 01:21:35.040]   it's actually good for society.
[01:21:35.040 --> 01:21:37.520]   And to the degree that it's not good,
[01:21:37.520 --> 01:21:38.520]   it's getting better and better.
[01:21:38.520 --> 01:21:40.200]   Is it possible to steel man their point?
[01:21:40.200 --> 01:21:41.480]   - Yeah.
[01:21:41.480 --> 01:21:44.560]   It's hard, but I should be able to do it.
[01:21:44.560 --> 01:21:45.960]   I need to put my money where my mouth is,
[01:21:45.960 --> 01:21:47.120]   and that's a good question.
[01:21:47.120 --> 01:21:48.560]   So on the mental health front,
[01:21:48.560 --> 01:21:52.700]   you know, the argument is usually what they say is,
[01:21:52.700 --> 01:21:54.700]   well, you know, for communities that are cut off,
[01:21:54.700 --> 01:21:59.000]   especially LGBTQ kids, they can find each other.
[01:21:59.000 --> 01:22:01.320]   So it connects kids,
[01:22:01.320 --> 01:22:04.700]   especially kids who wouldn't find connection otherwise.
[01:22:04.700 --> 01:22:09.300]   It exposes you to a range of ideas and content,
[01:22:09.300 --> 01:22:13.860]   and it's fun.
[01:22:13.860 --> 01:22:16.700]   - Is there, in the studies you looked at,
[01:22:16.700 --> 01:22:21.700]   is there inklings of data that's maybe early data
[01:22:22.100 --> 01:22:23.820]   that shows that there is positive effects
[01:22:23.820 --> 01:22:25.120]   in terms of self-report data,
[01:22:25.120 --> 01:22:28.660]   or how would you measure behavioral positive?
[01:22:28.660 --> 01:22:29.700]   It's difficult.
[01:22:29.700 --> 01:22:33.220]   - Right, so if you look at how do you feel
[01:22:33.220 --> 01:22:34.700]   when you're on the platform,
[01:22:34.700 --> 01:22:36.780]   you get a mix of positive and negative,
[01:22:36.780 --> 01:22:38.780]   and people say they feel supported.
[01:22:38.780 --> 01:22:41.140]   And this is what Mark was referring to when he said,
[01:22:41.140 --> 01:22:42.820]   you know, there was like 18 criteria,
[01:22:42.820 --> 01:22:45.700]   and on most it was positive, and on some it was negative.
[01:22:45.700 --> 01:22:46.580]   So if you look at how do you feel
[01:22:46.580 --> 01:22:48.460]   while you're using the platform,
[01:22:48.460 --> 01:22:50.800]   look, most kids enjoy it, they're having fun,
[01:22:51.620 --> 01:22:56.620]   but some kids are feeling inferior, cut off, bullied.
[01:22:56.620 --> 01:23:01.700]   So if we're saying what's the average experience
[01:23:01.700 --> 01:23:04.420]   on the platform, that might actually be positive.
[01:23:04.420 --> 01:23:06.020]   If we just measured the hedonics,
[01:23:06.020 --> 01:23:08.180]   like how much fun versus fear is there,
[01:23:08.180 --> 01:23:09.480]   it could well be positive.
[01:23:09.480 --> 01:23:11.400]   But what I'm trying to, okay,
[01:23:11.400 --> 01:23:12.740]   so is that enough steel manning?
[01:23:12.740 --> 01:23:13.580]   Can I?
[01:23:13.580 --> 01:23:15.140]   - That's pretty good.
[01:23:15.140 --> 01:23:15.960]   - Okay.
[01:23:15.960 --> 01:23:16.800]   - You held your breath.
[01:23:16.800 --> 01:23:18.200]   - Yeah.
[01:23:18.200 --> 01:23:20.460]   But what I'm trying to point out is
[01:23:20.460 --> 01:23:22.300]   this isn't a dose response sugar thing,
[01:23:22.300 --> 01:23:24.800]   like how do you feel while you're consuming heroin?
[01:23:24.800 --> 01:23:27.940]   Like while I'm consuming heroin, I feel great,
[01:23:27.940 --> 01:23:29.740]   but am I glad that heroin came into my life?
[01:23:29.740 --> 01:23:31.660]   Am I glad that everyone in my seventh grade class
[01:23:31.660 --> 01:23:32.500]   is on heroin?
[01:23:32.500 --> 01:23:33.340]   Like, no, I'm not.
[01:23:33.340 --> 01:23:35.220]   Like I wish that people weren't on heroin,
[01:23:35.220 --> 01:23:36.500]   and they could play on the playground,
[01:23:36.500 --> 01:23:37.660]   but instead they're just, you know,
[01:23:37.660 --> 01:23:40.020]   sitting on the bench shooting up during recess.
[01:23:40.020 --> 01:23:42.700]   So when you look at it as an emergent phenomenon
[01:23:42.700 --> 01:23:45.140]   that changed childhood, now it doesn't matter
[01:23:45.140 --> 01:23:47.220]   what are the feelings while you're actually using it.
[01:23:47.220 --> 01:23:50.860]   We need to zoom out and say, how has this changed childhood?
[01:23:50.860 --> 01:23:52.860]   - Can you try to do the same for democracy?
[01:23:52.860 --> 01:23:54.140]   - Yeah.
[01:23:54.140 --> 01:23:56.780]   So we can go back to, you know,
[01:23:56.780 --> 01:24:01.780]   what Mark said in 2012 when he was taking Facebook public,
[01:24:01.780 --> 01:24:04.580]   and you know, this is the wake of the Arab Spring.
[01:24:04.580 --> 01:24:05.760]   I think people really have to remember
[01:24:05.760 --> 01:24:08.140]   what an extraordinary year 2011 was.
[01:24:08.140 --> 01:24:10.460]   It starts with the Arab Spring.
[01:24:10.460 --> 01:24:11.860]   Dictators are pulled down.
[01:24:11.860 --> 01:24:13.860]   Now people say, you know, Facebook took them down.
[01:24:13.860 --> 01:24:15.460]   I mean, of course it was the citizen,
[01:24:15.460 --> 01:24:18.060]   the people themselves took down dictators,
[01:24:18.060 --> 01:24:21.540]   aided by Facebook and Twitter and,
[01:24:21.540 --> 01:24:23.060]   I don't know if it was, or texting,
[01:24:23.060 --> 01:24:24.720]   there were some other platforms they used.
[01:24:24.720 --> 01:24:28.440]   So the argument that Mark makes in this letter
[01:24:28.440 --> 01:24:31.600]   to potential shareholders, investors,
[01:24:31.600 --> 01:24:34.580]   is, you know, we're at a turning point in history,
[01:24:34.580 --> 01:24:38.820]   and, you know, social media is rewiring.
[01:24:38.820 --> 01:24:42.380]   We're giving people the tools to rewire their institutions.
[01:24:42.380 --> 01:24:43.900]   So this all sounds great.
[01:24:43.900 --> 01:24:45.700]   Like this is the democratic dream.
[01:24:45.700 --> 01:24:47.340]   And what I write about in the essay
[01:24:47.340 --> 01:24:50.260]   is the period of techno-democratic optimism,
[01:24:50.260 --> 01:24:51.620]   which began in the early '90s
[01:24:51.620 --> 01:24:54.580]   with the fall of the Iron Curtain and the Soviet Union.
[01:24:54.580 --> 01:24:56.940]   And then the internet comes in and, you know,
[01:24:56.940 --> 01:24:58.440]   people, I mean, people my age remember
[01:24:58.440 --> 01:25:01.780]   how extraordinary it was, how much fun it was.
[01:25:01.780 --> 01:25:04.900]   I mean, the sense that this was the dawning of a new age,
[01:25:04.900 --> 01:25:06.700]   and there was so much optimism.
[01:25:06.700 --> 01:25:09.740]   And so this optimism runs all the way from the early '90s,
[01:25:09.740 --> 01:25:12.980]   all the way through 2011 with the Arab Spring.
[01:25:12.980 --> 01:25:15.180]   And of course that year ends with Occupy Wall Street.
[01:25:15.180 --> 01:25:18.060]   And there were also big protest movements in Israel,
[01:25:18.060 --> 01:25:19.660]   in Spain, and in a lot of areas.
[01:25:19.660 --> 01:25:21.740]   Martin Goury talks about this.
[01:25:21.740 --> 01:25:24.740]   So there certainly was a case to be made
[01:25:24.740 --> 01:25:28.020]   that Facebook in particular, but all these platforms,
[01:25:28.020 --> 01:25:30.700]   these were God's gift to democracy.
[01:25:30.700 --> 01:25:33.820]   What dictator could possibly keep out the internet?
[01:25:33.820 --> 01:25:35.620]   What dictator could stand up to people
[01:25:35.620 --> 01:25:38.240]   connected on these digital media platforms?
[01:25:38.240 --> 01:25:39.180]   So that's the strong case
[01:25:39.180 --> 01:25:41.500]   that this is gonna be good for democracy.
[01:25:41.500 --> 01:25:43.420]   And then we can see what happened in the years after.
[01:25:43.420 --> 01:25:47.900]   Now, first of all, so in Mark's response to you,
[01:25:47.900 --> 01:25:49.800]   so here, let me read from what he said
[01:25:49.800 --> 01:25:51.380]   when you interviewed him.
[01:25:51.380 --> 01:25:55.260]   He says, "I think it's worth grounding this conversation
[01:25:55.260 --> 01:25:57.300]   "in the actual research that has been done on this,
[01:25:57.300 --> 01:25:59.260]   "which by and large finds that social media
[01:25:59.260 --> 01:26:01.660]   "is not a large driver of polarization."
[01:26:01.660 --> 01:26:02.540]   He says that.
[01:26:02.540 --> 01:26:05.360]   Then he says, "Most academic studies that I've seen
[01:26:05.360 --> 01:26:07.120]   "actually show that social media use
[01:26:07.120 --> 01:26:09.540]   "is correlated with lower polarization."
[01:26:09.540 --> 01:26:11.220]   That's a factual claim that he makes,
[01:26:11.220 --> 01:26:12.700]   which is not true.
[01:26:12.700 --> 01:26:15.940]   But he asserts that, well, actually, wait, it's tricky
[01:26:15.940 --> 01:26:18.220]   because he says the studies he has seen.
[01:26:18.220 --> 01:26:20.420]   So I can't, so it might be that the studies he has seen
[01:26:20.420 --> 01:26:23.020]   say that, but if you go to the Google Doc with Chris Bale,
[01:26:23.020 --> 01:26:25.140]   you see there's seven different questions
[01:26:25.140 --> 01:26:26.300]   that can be addressed.
[01:26:26.300 --> 01:26:29.260]   And on one of them, which is filter bubbles,
[01:26:29.260 --> 01:26:30.420]   the evidence is very mixed.
[01:26:30.420 --> 01:26:32.500]   And he might be right that Facebook overall
[01:26:32.500 --> 01:26:33.900]   doesn't contribute to filter bubbles.
[01:26:33.900 --> 01:26:36.500]   But on the other six, the evidence is pretty strongly
[01:26:36.500 --> 01:26:38.620]   on the yes side, it is a cause.
[01:26:38.620 --> 01:26:40.620]   - He also draws a line between the United States
[01:26:40.620 --> 01:26:41.820]   versus the rest of the world.
[01:26:41.820 --> 01:26:43.640]   - Right, and there's one thing true about that,
[01:26:43.640 --> 01:26:46.020]   which is that polarization has been rising
[01:26:46.020 --> 01:26:48.380]   much faster in the US than in any other major country.
[01:26:48.380 --> 01:26:49.580]   So he's right about that.
[01:26:49.580 --> 01:26:53.300]   So we're talking about an article by Matthew Genscow
[01:26:53.300 --> 01:26:56.340]   and a few other researchers.
[01:26:56.340 --> 01:26:57.860]   A very important article, we've got it
[01:26:57.860 --> 01:27:01.540]   in the political dysfunction database.
[01:27:01.540 --> 01:27:02.940]   - And we should say that in this study,
[01:27:02.940 --> 01:27:04.980]   there's, like I started to say,
[01:27:04.980 --> 01:27:06.720]   there's a lot of fascinating questions
[01:27:06.720 --> 01:27:10.400]   that it's organized by, whether studies indicate yes or no,
[01:27:10.400 --> 01:27:12.980]   question one is does social media make people more angry
[01:27:12.980 --> 01:27:15.020]   or effectively polarized?
[01:27:15.020 --> 01:27:17.700]   Question two is does social media create echo chambers?
[01:27:17.700 --> 01:27:19.720]   These are fascinating, really important questions.
[01:27:19.720 --> 01:27:22.620]   Question three is does social media amplify posts
[01:27:22.620 --> 01:27:25.920]   that are more emotional, inflammatory, or false?
[01:27:25.920 --> 01:27:27.700]   Question four is does social media increase
[01:27:27.700 --> 01:27:29.340]   the probability of violence?
[01:27:29.340 --> 01:27:31.240]   Question five is does social media enable
[01:27:31.240 --> 01:27:34.260]   foreign governments to increase political dysfunction
[01:27:34.260 --> 01:27:36.660]   in the US and other democracies?
[01:27:36.660 --> 01:27:39.780]   Question six, does social media decrease trust?
[01:27:39.780 --> 01:27:44.500]   Seven, is the social media strengthen populist movements?
[01:27:44.500 --> 01:27:47.140]   And then there's other sections, as you mentioned.
[01:27:47.140 --> 01:27:47.980]   - Yeah, that's right.
[01:27:47.980 --> 01:27:52.320]   But once you operationalize it as seven different questions,
[01:27:52.320 --> 01:27:56.000]   so one is about polarization and there are measures of that,
[01:27:56.000 --> 01:27:58.380]   the degree to which people say they hate the other side.
[01:27:58.380 --> 01:28:02.860]   And so in this study by Boxell, Genscow, and Shapiro, 2021,
[01:28:02.860 --> 01:28:06.660]   they looked at all the measures of polarization
[01:28:06.660 --> 01:28:09.060]   they could find going back to the 1970s
[01:28:09.060 --> 01:28:10.940]   for about 20 different countries.
[01:28:10.940 --> 01:28:13.140]   And they show plots, you have these nice plots
[01:28:13.140 --> 01:28:15.140]   with red lines showing that in some countries
[01:28:15.140 --> 01:28:17.580]   it's going up, like the United States especially,
[01:28:17.580 --> 01:28:19.140]   in some countries it's going down,
[01:28:19.140 --> 01:28:20.940]   and in some countries it's pretty flat.
[01:28:20.940 --> 01:28:22.700]   And so Marx says, well, you know,
[01:28:22.700 --> 01:28:24.900]   if polarization's going up a lot in the US
[01:28:24.900 --> 01:28:26.140]   but not in most other countries,
[01:28:26.140 --> 01:28:28.460]   well, maybe Facebook isn't responsible.
[01:28:28.460 --> 01:28:31.860]   But so much depends on how you operationalize things.
[01:28:31.860 --> 01:28:35.420]   Are we interested in the straight line, regression line,
[01:28:35.420 --> 01:28:37.180]   going back to the '70s?
[01:28:37.180 --> 01:28:40.380]   And if so, well, then he's right in what he says.
[01:28:40.380 --> 01:28:41.500]   But that's not the argument.
[01:28:41.500 --> 01:28:43.340]   The argument isn't that, you know,
[01:28:43.340 --> 01:28:44.780]   it's been rising and falling since the '70s.
[01:28:44.780 --> 01:28:47.620]   The argument is it's been rising and falling since 2012
[01:28:47.620 --> 01:28:48.500]   or so.
[01:28:48.500 --> 01:28:50.420]   And for that, now I just spoke with,
[01:28:50.420 --> 01:28:53.020]   I've been emailing with the authors of the study
[01:28:53.020 --> 01:28:54.380]   and they say there's not really enough data
[01:28:54.380 --> 01:28:56.060]   to do it statistically reliably
[01:28:56.060 --> 01:28:58.160]   'cause there's only a few observations after 2012.
[01:28:58.160 --> 01:29:00.420]   But if you look at the graphs in their study,
[01:29:00.420 --> 01:29:02.820]   and they actually do provide, as they pointed out to me,
[01:29:02.820 --> 01:29:04.780]   they do provide a statistical test
[01:29:04.780 --> 01:29:07.180]   if you break the data at the year 2000.
[01:29:07.180 --> 01:29:11.540]   So actually, a polarization is going up pretty widely
[01:29:11.540 --> 01:29:12.940]   if you just look after 2000,
[01:29:12.940 --> 01:29:15.900]   which is when the internet would be influential.
[01:29:15.900 --> 01:29:17.580]   And if you look just after 2012,
[01:29:17.580 --> 01:29:18.940]   you have to just do it by eye.
[01:29:18.940 --> 01:29:21.100]   But if you do it on their graphs by eye,
[01:29:21.100 --> 01:29:22.700]   you see that actually a number of countries
[01:29:22.700 --> 01:29:24.340]   do see a sudden sharp upturn.
[01:29:24.340 --> 01:29:26.060]   Not all, not all by any means.
[01:29:26.060 --> 01:29:28.420]   But my point is, Mark asserts,
[01:29:28.420 --> 01:29:29.900]   he points to one study and he points to this
[01:29:29.900 --> 01:29:30.740]   over and over again.
[01:29:30.740 --> 01:29:31.900]   I have had two conversations with him.
[01:29:31.900 --> 01:29:33.700]   He pointed to this study both times.
[01:29:34.540 --> 01:29:37.260]   He asserts that this study shows
[01:29:37.260 --> 01:29:40.180]   that polarization is up some places, down other places.
[01:29:40.180 --> 01:29:41.980]   There's no association.
[01:29:41.980 --> 01:29:45.040]   But actually, we have another section in the Google Doc
[01:29:45.040 --> 01:29:47.820]   where we review all the data on the decline of democracy.
[01:29:47.820 --> 01:29:49.380]   And the high point of democracy,
[01:29:49.380 --> 01:29:50.860]   of course, it was rising in the '90s.
[01:29:50.860 --> 01:29:52.500]   But if you look around the world,
[01:29:52.500 --> 01:29:55.220]   by some measures it begins to drop in the late 2000s,
[01:29:55.220 --> 01:29:57.540]   around 2007, 2008.
[01:29:57.540 --> 01:30:00.820]   By others, it's in the early to mid 2010s.
[01:30:00.820 --> 01:30:03.500]   The point is, there is a, by many measures,
[01:30:03.500 --> 01:30:06.220]   there's a drop in the quality and the number
[01:30:06.220 --> 01:30:09.940]   of democracies on this planet that began in the 2010s.
[01:30:09.940 --> 01:30:13.540]   And so, yes, Mark can point to one study.
[01:30:13.540 --> 01:30:14.620]   But if you look in the Google Doc,
[01:30:14.620 --> 01:30:16.740]   there are a lot of other studies that point the other way.
[01:30:16.740 --> 01:30:18.020]   And especially about whether things
[01:30:18.020 --> 01:30:20.340]   are getting more polarized or less, more polarized.
[01:30:20.340 --> 01:30:22.100]   Not in all countries, but in a lot.
[01:30:22.100 --> 01:30:25.260]   - So you've provided the problem,
[01:30:25.260 --> 01:30:27.300]   several proposals for solutions.
[01:30:27.300 --> 01:30:32.300]   Do you think Mark, do you think Elon or whoever
[01:30:32.300 --> 01:30:35.580]   is at the head of Twitter would be able
[01:30:35.580 --> 01:30:36.900]   to implement these changes?
[01:30:36.900 --> 01:30:38.620]   Or does there need to be a competitor,
[01:30:38.620 --> 01:30:40.780]   social network to step up?
[01:30:40.780 --> 01:30:42.660]   If you were to predict the future,
[01:30:42.660 --> 01:30:45.740]   now this is you giving sort of financial advice to me.
[01:30:45.740 --> 01:30:47.260]   No. - No, not that I can't do.
[01:30:47.260 --> 01:30:48.100]   - Definitely not financial advice.
[01:30:48.100 --> 01:30:48.940]   - I can give you advice.
[01:30:48.940 --> 01:30:50.620]   Do the opposite of whatever I've done.
[01:30:50.620 --> 01:30:52.540]   - Okay, excellent. (laughs)
[01:30:52.540 --> 01:30:57.300]   But what do you think, when we talk again in 10 years,
[01:30:57.300 --> 01:31:00.940]   what do you think we'd be looking at if it's a better world?
[01:31:00.940 --> 01:31:03.260]   - Yeah, so you have to look at each,
[01:31:03.260 --> 01:31:05.620]   the dynamics of each change that needs to be made.
[01:31:05.620 --> 01:31:07.620]   And you have to look at it systemically.
[01:31:07.620 --> 01:31:09.740]   And so the biggest change for teen mental health,
[01:31:09.740 --> 01:31:12.380]   I think, is to raise the age from 13,
[01:31:12.380 --> 01:31:16.260]   it was set to 13 in COPPA in like 1997 or six or whatever,
[01:31:16.260 --> 01:31:17.940]   that was eight, whatever it was.
[01:31:17.940 --> 01:31:19.500]   It was set to 13 with no enforcement.
[01:31:19.500 --> 01:31:22.860]   I think it needs to go to 16 or 18 with enforcement.
[01:31:22.860 --> 01:31:25.880]   Now, there's no way that Facebook can say,
[01:31:25.880 --> 01:31:29.220]   actually, so look, Instagram, the age is 13,
[01:31:29.220 --> 01:31:30.500]   but they don't enforce it.
[01:31:30.500 --> 01:31:32.700]   And they're under pressure to not enforce it
[01:31:32.700 --> 01:31:34.420]   because if they did enforce it,
[01:31:34.420 --> 01:31:36.100]   then all the kids would just go to TikTok,
[01:31:36.100 --> 01:31:36.940]   which they're doing anyway.
[01:31:36.940 --> 01:31:38.820]   But if we go back a couple of years,
[01:31:38.820 --> 01:31:42.500]   when they were talking about rolling out Facebook for kids,
[01:31:42.500 --> 01:31:43.740]   'cause they need to get those kids,
[01:31:43.740 --> 01:31:45.180]   they need to get kids under 13.
[01:31:45.180 --> 01:31:47.220]   There's a business imperative to hook them early
[01:31:47.220 --> 01:31:48.480]   and keep them.
[01:31:48.480 --> 01:31:51.220]   So I don't expect Facebook to act on its own accord
[01:31:51.220 --> 01:31:52.060]   and do the right thing because then they--
[01:31:52.060 --> 01:31:53.540]   - So regulation is the only way.
[01:31:53.540 --> 01:31:54.380]   - Exactly.
[01:31:54.380 --> 01:31:56.020]   When you have a social dilemma,
[01:31:56.020 --> 01:31:58.340]   like what economists call like a prisoner's dilemma
[01:31:58.340 --> 01:32:01.140]   or a social dilemma is generalized to multiple people.
[01:32:01.140 --> 01:32:03.180]   And when you have a social dilemma,
[01:32:03.180 --> 01:32:06.100]   each player can't opt out because they're gonna lose.
[01:32:06.100 --> 01:32:07.660]   You have to have central regulation.
[01:32:07.660 --> 01:32:09.540]   So I think we have to raise the age.
[01:32:09.540 --> 01:32:11.380]   The UK parliament is way ahead of us.
[01:32:11.380 --> 01:32:12.700]   I think they're actually functional.
[01:32:12.700 --> 01:32:14.780]   The US Congress is not functional.
[01:32:14.780 --> 01:32:16.580]   So the parliament is implementing
[01:32:16.580 --> 01:32:18.860]   the age appropriate design code
[01:32:18.860 --> 01:32:21.180]   that may put pressure on the platforms globally
[01:32:21.180 --> 01:32:22.300]   to change certain.
[01:32:22.300 --> 01:32:25.860]   So anyway, my point is, we have to have regulation
[01:32:25.860 --> 01:32:28.140]   to force them to be transparent
[01:32:28.140 --> 01:32:30.100]   and share what they're doing.
[01:32:30.100 --> 01:32:31.700]   There are some good bills out there.
[01:32:31.700 --> 01:32:34.580]   So I think that if the companies and the users,
[01:32:34.580 --> 01:32:36.940]   if we're all stuck in a social dilemma
[01:32:36.940 --> 01:32:40.900]   in which the incentives against doing the right thing
[01:32:40.900 --> 01:32:44.540]   are strong, we do need regulation on certain matters.
[01:32:44.540 --> 01:32:46.660]   And again, it's not about content moderation,
[01:32:46.660 --> 01:32:47.860]   who gets to say what,
[01:32:47.860 --> 01:32:49.620]   but it's things like the Platform Accountability
[01:32:49.620 --> 01:32:52.820]   and Transparency Act, which is from Stanardoz Kunz,
[01:32:52.820 --> 01:32:54.300]   Portman and Klobuchar.
[01:32:54.300 --> 01:32:56.940]   This would force the platforms to just share information
[01:32:56.940 --> 01:32:57.780]   on what they're doing.
[01:32:57.780 --> 01:32:59.940]   Like we can't even study what's happening
[01:32:59.940 --> 01:33:01.140]   without the information.
[01:33:01.140 --> 01:33:03.380]   So that I think is just common sense.
[01:33:03.380 --> 01:33:05.020]   Senator Michael Bennett introduced
[01:33:05.020 --> 01:33:09.020]   the Digital Platforms Commission Act of 2022,
[01:33:09.020 --> 01:33:12.580]   which would create a body tasked with actually regulating
[01:33:12.580 --> 01:33:13.660]   and having oversight.
[01:33:13.660 --> 01:33:16.020]   Right now, the US government doesn't have a body.
[01:33:16.020 --> 01:33:17.980]   I mean, the FTC can do certain things.
[01:33:17.980 --> 01:33:19.260]   We have things about antitrust,
[01:33:19.260 --> 01:33:22.420]   but we don't have a body that can oversee or understand
[01:33:22.420 --> 01:33:24.380]   these things that are transforming everything
[01:33:24.380 --> 01:33:28.020]   and possibly severely damaging our political life.
[01:33:28.020 --> 01:33:31.580]   So I think there's a lot of, oh, and then the,
[01:33:31.580 --> 01:33:35.140]   the state of California is actually currently considering
[01:33:35.140 --> 01:33:39.140]   a version of the UK's, the age appropriate design code,
[01:33:39.140 --> 01:33:42.180]   which would force the companies to do some simple things
[01:33:42.180 --> 01:33:45.740]   like not be sending alerts and notifications to children
[01:33:45.740 --> 01:33:47.140]   at 10 or 11 o'clock at night,
[01:33:47.140 --> 01:33:52.140]   just things like that to make platforms just less damaging.
[01:33:52.540 --> 01:33:55.060]   So I think there's an essential role for regulation.
[01:33:55.060 --> 01:33:58.180]   And I think if the US Congress is too paralyzed by politics,
[01:33:58.180 --> 01:34:01.540]   if the UK and the EU and the state of California
[01:34:01.540 --> 01:34:02.620]   and the state, a few other states,
[01:34:02.620 --> 01:34:05.020]   if they enact legislation,
[01:34:05.020 --> 01:34:07.020]   the platforms don't wanna have different versions
[01:34:07.020 --> 01:34:08.660]   in different states or countries.
[01:34:08.660 --> 01:34:10.140]   So I think there actually is some hope,
[01:34:10.140 --> 01:34:12.660]   even if the US Congress is dysfunctional.
[01:34:12.660 --> 01:34:15.140]   - So there is, 'cause I've been interacting
[01:34:15.140 --> 01:34:17.260]   with certain regulations hitting,
[01:34:17.260 --> 01:34:19.580]   designed to hit Amazon, but it's hitting YouTube.
[01:34:19.580 --> 01:34:21.300]   YouTube folks have been talking to me,
[01:34:21.300 --> 01:34:23.020]   which is recommender systems.
[01:34:23.020 --> 01:34:26.760]   The algorithm has to be public, I think,
[01:34:26.760 --> 01:34:30.180]   versus private, which completely breaks.
[01:34:30.180 --> 01:34:33.180]   It's way too clumsy a regulation
[01:34:33.180 --> 01:34:35.140]   that where the unintended consequences
[01:34:35.140 --> 01:34:38.160]   break recommender systems, not for Amazon,
[01:34:38.160 --> 01:34:40.740]   but for other platforms.
[01:34:40.740 --> 01:34:43.860]   That's just to say that government can sometimes
[01:34:43.860 --> 01:34:45.580]   be clumsy with the regulation.
[01:34:45.580 --> 01:34:46.420]   - Usually is.
[01:34:46.420 --> 01:34:51.420]   - And so my preference is the threat of regulation
[01:34:51.420 --> 01:34:54.740]   in a friendly way encourages--
[01:34:54.740 --> 01:34:56.340]   - Good behavior. - You really shouldn't need it.
[01:34:56.340 --> 01:34:57.500]   You really shouldn't need it.
[01:34:57.500 --> 01:35:01.620]   My preference is great leaders lead the way
[01:35:01.620 --> 01:35:03.220]   in doing the right thing.
[01:35:03.220 --> 01:35:05.860]   And I actually, honestly, this, to our earlier
[01:35:05.860 --> 01:35:08.620]   kind of maybe my naive disagreement
[01:35:08.620 --> 01:35:11.300]   that I think it's good business to do the right thing
[01:35:11.300 --> 01:35:12.900]   in these spaces.
[01:35:12.900 --> 01:35:14.220]   So creating a problem-- - Sometimes it is.
[01:35:14.220 --> 01:35:18.020]   Sometimes it loses you, most of your users.
[01:35:18.020 --> 01:35:20.460]   - Well, I think it's important because I've been thinking
[01:35:20.460 --> 01:35:22.980]   a lot about World War III recently.
[01:35:22.980 --> 01:35:26.580]   - Yeah. - And it might be silly
[01:35:26.580 --> 01:35:30.580]   to say, but I think social media has a role
[01:35:30.580 --> 01:35:34.300]   in either creating World War III or avoiding World War III.
[01:35:34.300 --> 01:35:37.940]   It seems like so much of wars throughout history
[01:35:37.940 --> 01:35:42.300]   have been started through very fast escalation.
[01:35:42.300 --> 01:35:44.780]   And it feels like just looking at our recent history,
[01:35:44.780 --> 01:35:47.420]   social media is the mechanism for escalation.
[01:35:47.420 --> 01:35:49.460]   And so it's really important to get this right,
[01:35:49.460 --> 01:35:52.180]   not just for the mental health of young people,
[01:35:52.180 --> 01:35:54.140]   not just for the polarization of bickering
[01:35:54.140 --> 01:35:57.380]   over small-scale political issues,
[01:35:57.380 --> 01:36:01.580]   but literally the survival of human civilization.
[01:36:01.580 --> 01:36:04.360]   So there's a lot at stake here.
[01:36:04.360 --> 01:36:05.540]   - Yeah, I certainly agree with that.
[01:36:05.540 --> 01:36:07.380]   I would just say that I'm less concerned
[01:36:07.380 --> 01:36:09.820]   about World War III than I am about Civil War II.
[01:36:09.820 --> 01:36:11.540]   I think that's a more likely prospect.
[01:36:11.540 --> 01:36:13.880]   - Yeah, yeah, yeah.
[01:36:13.880 --> 01:36:19.420]   Can I ask for your wise sage advice to young people?
[01:36:19.420 --> 01:36:22.500]   So advice number one is put down the phone.
[01:36:22.500 --> 01:36:25.420]   Don't use Instagram and social media.
[01:36:25.420 --> 01:36:28.980]   But to young people in high school and college,
[01:36:28.980 --> 01:36:30.220]   how to have a career,
[01:36:30.220 --> 01:36:33.500]   or how to have a life they can be proud of.
[01:36:33.500 --> 01:36:34.660]   - Yeah, I'd be happy to,
[01:36:34.660 --> 01:36:38.780]   'cause I teach a course at NYU in the business school
[01:36:38.780 --> 01:36:40.860]   called Work, Wisdom, and Happiness.
[01:36:40.860 --> 01:36:42.340]   And the course is,
[01:36:42.340 --> 01:36:44.220]   it's advice on how to have a happy,
[01:36:44.220 --> 01:36:47.140]   a successful career as a human being.
[01:36:47.140 --> 01:36:50.620]   But the course has evolved that it's now about three things,
[01:36:50.620 --> 01:36:54.900]   how to get stronger, smarter, and more sociable.
[01:36:54.900 --> 01:36:56.640]   If you can do those three things,
[01:36:56.640 --> 01:36:59.180]   then you will be more successful
[01:36:59.180 --> 01:37:01.940]   at work and in love and friendships.
[01:37:01.940 --> 01:37:03.820]   And if you are more successful in work,
[01:37:03.820 --> 01:37:05.500]   love, and friendships, then you will be happier.
[01:37:05.500 --> 01:37:07.500]   You will be as happy as you can be, in fact.
[01:37:07.500 --> 01:37:08.780]   So the question is,
[01:37:08.780 --> 01:37:11.240]   how do you become smarter, stronger, and happier?
[01:37:11.240 --> 01:37:15.380]   And the answer to all three is,
[01:37:15.380 --> 01:37:16.260]   it's a number of things,
[01:37:16.260 --> 01:37:18.340]   but it's you have to see yourself
[01:37:18.340 --> 01:37:20.900]   as this complex adaptive system.
[01:37:20.900 --> 01:37:22.680]   You've got this complicated mind
[01:37:22.680 --> 01:37:25.900]   that needs a lot of experience to wire itself up.
[01:37:25.900 --> 01:37:28.220]   And the most important part of that experience is
[01:37:28.220 --> 01:37:30.020]   that you don't grow
[01:37:30.020 --> 01:37:31.820]   when you are with your attachment figure.
[01:37:31.820 --> 01:37:33.660]   You don't grow when you're safe.
[01:37:33.660 --> 01:37:35.780]   You have an attachment figure to make you feel confident
[01:37:35.780 --> 01:37:37.240]   to go out and explore the world.
[01:37:37.240 --> 01:37:39.540]   In that world, you will face threats,
[01:37:39.540 --> 01:37:40.700]   you will face fear,
[01:37:40.700 --> 01:37:42.380]   and sometimes you'll come running back.
[01:37:42.380 --> 01:37:43.500]   But you have to keep doing it
[01:37:43.500 --> 01:37:46.260]   because over time, you then develop the strength
[01:37:46.260 --> 01:37:48.340]   to stay out there and to conquer it.
[01:37:48.340 --> 01:37:50.200]   That's normal human childhood.
[01:37:50.200 --> 01:37:52.980]   That's what we blocked in the 1990s in this country.
[01:37:52.980 --> 01:37:56.700]   So young people have to get themselves the childhood,
[01:37:56.700 --> 01:37:58.100]   and this is all the way through adolescence
[01:37:58.100 --> 01:37:59.340]   and young adulthood,
[01:37:59.340 --> 01:38:00.820]   they have to get themselves the experience
[01:38:00.820 --> 01:38:04.020]   that older generations are blocking them from out of fear
[01:38:04.020 --> 01:38:06.100]   and that their phones are blocking them from
[01:38:06.100 --> 01:38:09.080]   out of just hijacking almost all the inputs into their life
[01:38:09.080 --> 01:38:10.920]   and almost all the minutes of their day.
[01:38:10.920 --> 01:38:14.800]   So go out there, put yourself out in experiences.
[01:38:14.800 --> 01:38:17.240]   You are anti-fragile and you're not gonna get strong
[01:38:17.240 --> 01:38:19.360]   unless you actually have setbacks
[01:38:19.360 --> 01:38:21.200]   and criticisms and fights.
[01:38:21.200 --> 01:38:24.120]   So that's how you get stronger.
[01:38:24.120 --> 01:38:27.080]   And then there's an analogy in how you get smarter,
[01:38:27.080 --> 01:38:29.960]   which is you have to expose yourself to other ideas,
[01:38:29.960 --> 01:38:32.880]   to ideas that people that criticize you,
[01:38:32.880 --> 01:38:34.560]   people that disagree with you.
[01:38:34.560 --> 01:38:36.860]   And this is why I co-founded Heterodox Academy
[01:38:36.860 --> 01:38:41.580]   because we believe that faculty need to be in communities
[01:38:41.580 --> 01:38:43.780]   that have political diversity and viewpoint diversity,
[01:38:43.780 --> 01:38:45.100]   but so do students.
[01:38:45.100 --> 01:38:47.540]   And it turns out students want this.
[01:38:47.540 --> 01:38:50.260]   The surveys show very clearly Gen Z
[01:38:50.260 --> 01:38:52.780]   has not turned against viewpoint diversity.
[01:38:52.780 --> 01:38:54.100]   Most of them want it,
[01:38:54.100 --> 01:38:55.740]   but they're just afraid of the small number
[01:38:55.740 --> 01:38:57.420]   that will sort of shoot darts at them
[01:38:57.420 --> 01:38:59.540]   if they say something wrong.
[01:38:59.540 --> 01:39:02.740]   So anyway, the point is you're anti-fragile
[01:39:02.740 --> 01:39:05.640]   and so you have to realize that to get stronger,
[01:39:05.640 --> 01:39:07.400]   you have to realize to get smarter.
[01:39:07.400 --> 01:39:10.080]   And then the key to becoming more sociable is very simple.
[01:39:10.080 --> 01:39:10.960]   It's just always looking at it
[01:39:10.960 --> 01:39:12.680]   through the other person's point of view.
[01:39:12.680 --> 01:39:14.260]   Don't be so focused on what you want
[01:39:14.260 --> 01:39:16.200]   and what you're afraid of.
[01:39:16.200 --> 01:39:17.640]   Put yourself in the other person's shoes,
[01:39:17.640 --> 01:39:19.840]   what's interesting to them, what do they want?
[01:39:19.840 --> 01:39:21.960]   And if you develop the skill of looking at it
[01:39:21.960 --> 01:39:23.040]   from their point of view,
[01:39:23.040 --> 01:39:24.560]   you'll be a better conversation partner,
[01:39:24.560 --> 01:39:27.200]   you'll be a better life partner.
[01:39:27.200 --> 01:39:28.960]   So there's a lot that you can do.
[01:39:28.960 --> 01:39:29.880]   I mean, I could say,
[01:39:29.880 --> 01:39:32.120]   go read "The Coddling of the American Mind."
[01:39:32.120 --> 01:39:33.440]   I could say, go read Dale Carnegie,
[01:39:33.440 --> 01:39:36.100]   "How to Win Friends and Influence People."
[01:39:36.100 --> 01:39:39.120]   But take charge of your life and your development
[01:39:39.120 --> 01:39:40.520]   'cause if you don't do it,
[01:39:40.520 --> 01:39:45.520]   then the older protective generation and your phone
[01:39:45.520 --> 01:39:47.800]   are gonna take charge of you.
[01:39:47.800 --> 01:39:51.680]   - So on anti-fragility and coddling the American mind,
[01:39:51.680 --> 01:39:53.480]   if I may read just a few lines
[01:39:53.480 --> 01:39:55.200]   from Chief Justice John Roberts,
[01:39:55.200 --> 01:39:58.060]   which I find is really beautiful.
[01:39:58.060 --> 01:40:00.840]   So it's not just about viewpoint diversity,
[01:40:00.840 --> 01:40:04.400]   but it's real struggle, absurd, unfair struggle
[01:40:04.400 --> 01:40:07.240]   that seems to be formative to the human mind.
[01:40:07.240 --> 01:40:10.480]   He says, "From time to time in the years to come,
[01:40:10.480 --> 01:40:13.120]   I hope you will be treated unfairly
[01:40:13.120 --> 01:40:16.360]   so that you will come to know the value of justice.
[01:40:16.360 --> 01:40:18.780]   I hope that you will suffer betrayal
[01:40:18.780 --> 01:40:21.800]   because that will teach you the importance of loyalty.
[01:40:21.800 --> 01:40:24.200]   Sorry to say, but I hope you will be lonely
[01:40:24.200 --> 01:40:25.540]   from time to time
[01:40:25.540 --> 01:40:27.760]   so that you don't take friends for granted.
[01:40:27.760 --> 01:40:30.560]   I wish you bad luck again from time to time
[01:40:30.560 --> 01:40:33.480]   so that you will be conscious of the role of chance in life
[01:40:33.480 --> 01:40:37.720]   and understand that your success is not completely deserved
[01:40:37.720 --> 01:40:39.000]   and that the failure of others
[01:40:39.000 --> 01:40:41.240]   is not completely deserved either.
[01:40:41.240 --> 01:40:44.520]   And when you lose, as you will from time to time,
[01:40:44.520 --> 01:40:46.800]   I hope every now and then,
[01:40:46.800 --> 01:40:49.360]   your opponent will gloat over your failure.
[01:40:49.360 --> 01:40:51.200]   It is a way for you to understand
[01:40:51.200 --> 01:40:53.200]   the importance of sportsmanship.
[01:40:53.200 --> 01:40:55.260]   I hope you'll be ignored
[01:40:55.260 --> 01:40:57.520]   so you know the importance of listening to others.
[01:40:57.520 --> 01:41:00.120]   And I hope you will have just enough pain
[01:41:00.120 --> 01:41:01.760]   to learn compassion.
[01:41:01.760 --> 01:41:04.040]   Whether I wish these things or not,
[01:41:04.040 --> 01:41:05.720]   they're going to happen.
[01:41:05.720 --> 01:41:08.080]   And whether you benefit from them or not
[01:41:08.080 --> 01:41:09.660]   will depend upon your ability
[01:41:09.660 --> 01:41:13.140]   to see the message in your misfortunes.
[01:41:13.140 --> 01:41:15.560]   He read that in a middle school graduation.
[01:41:15.560 --> 01:41:18.640]   - Yes, for his son's middle school graduation.
[01:41:18.640 --> 01:41:19.800]   That's what I was trying to say,
[01:41:19.800 --> 01:41:21.200]   only that's much more beautiful.
[01:41:21.200 --> 01:41:24.040]   - Yeah, and I think your work is really important
[01:41:24.040 --> 01:41:27.200]   and it is beautiful and it's bold and fearless
[01:41:27.200 --> 01:41:29.440]   and it's a huge honor that you would sit with me.
[01:41:29.440 --> 01:41:30.520]   I'm a big fan.
[01:41:30.520 --> 01:41:32.440]   Thank you for spending your valuable time with me today,
[01:41:32.440 --> 01:41:33.360]   John, thank you so much.
[01:41:33.360 --> 01:41:35.720]   - Thanks so much, Lex, what a pleasure.
[01:41:35.720 --> 01:41:37.160]   - Thanks for listening to this conversation
[01:41:37.160 --> 01:41:38.640]   with Jonathan Haidt.
[01:41:38.640 --> 01:41:39.880]   To support this podcast,
[01:41:39.880 --> 01:41:42.880]   please check out our sponsors in the description.
[01:41:42.880 --> 01:41:45.780]   And now, let me leave you with some words from Carl Jung.
[01:41:45.780 --> 01:41:50.240]   "Everything that irritates us about others
[01:41:50.240 --> 01:41:53.940]   "can lead us to an understanding of ourselves."
[01:41:53.940 --> 01:41:57.120]   Thank you for listening and hope to see you next time.
[01:41:57.120 --> 01:41:59.700]   (upbeat music)
[01:41:59.700 --> 01:42:02.280]   (upbeat music)
[01:42:02.280 --> 01:42:12.280]   [BLANK_AUDIO]

