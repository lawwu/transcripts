
[00:00:00.000 --> 00:00:02.960]   The following is a conversation with Manolis Kellis,
[00:00:02.960 --> 00:00:05.000]   his fourth time on the podcast.
[00:00:05.000 --> 00:00:06.880]   He's a professor at MIT
[00:00:06.880 --> 00:00:10.420]   and head of the MIT Computational Biology Group.
[00:00:10.420 --> 00:00:14.440]   Since this is episode number 142,
[00:00:14.440 --> 00:00:16.660]   and 42, as we all know,
[00:00:16.660 --> 00:00:18.840]   is the answer to the ultimate question of life,
[00:00:18.840 --> 00:00:20.420]   the universe, and everything,
[00:00:20.420 --> 00:00:23.640]   according to the Hitchhiker's Guide to the Galaxy,
[00:00:23.640 --> 00:00:26.740]   we decided to talk about this unanswerable question
[00:00:26.740 --> 00:00:28.120]   of the meaning of life
[00:00:28.120 --> 00:00:32.000]   in whatever way we two descendants of apes could muster,
[00:00:32.000 --> 00:00:37.000]   from biology, to psychology, to metaphysics, and to music.
[00:00:37.000 --> 00:00:39.400]   Quick mention of each sponsor,
[00:00:39.400 --> 00:00:42.100]   followed by some thoughts related to the episode.
[00:00:42.100 --> 00:00:44.160]   Thanks to Grammarly,
[00:00:44.160 --> 00:00:47.000]   which is a service for checking, spelling,
[00:00:47.000 --> 00:00:49.880]   grammar, sentence structure, and readability,
[00:00:49.880 --> 00:00:52.520]   Athletic Greens, the all-in-one drink
[00:00:52.520 --> 00:00:54.080]   that I start every day with
[00:00:54.080 --> 00:00:56.280]   to cover all my nutritional bases,
[00:00:56.280 --> 00:01:00.400]   and Cash App, the app I use to send money to friends.
[00:01:00.400 --> 00:01:02.440]   Please check out these sponsors in the description
[00:01:02.440 --> 00:01:05.680]   to get a discount and to support this podcast.
[00:01:05.680 --> 00:01:08.360]   As a side note, let me say that the opening 40 minutes
[00:01:08.360 --> 00:01:11.480]   of the conversation are all about the many songs
[00:01:11.480 --> 00:01:15.820]   that formed the soundtrack to the journey of Manolis's life.
[00:01:15.820 --> 00:01:18.000]   It was a happy accident for me to discover
[00:01:18.000 --> 00:01:20.440]   yet another dimension of depth
[00:01:20.440 --> 00:01:22.720]   to the fascinating mind of Manolis.
[00:01:22.720 --> 00:01:24.840]   I include links to YouTube versions
[00:01:24.840 --> 00:01:28.160]   of many of the songs we mention in the description
[00:01:28.160 --> 00:01:30.600]   and overlay lyrics on occasion.
[00:01:30.600 --> 00:01:31.960]   But if you're just listening to this
[00:01:31.960 --> 00:01:34.720]   without listening to the songs or watching the video,
[00:01:34.720 --> 00:01:37.020]   I hope you still might enjoy, as I did,
[00:01:37.020 --> 00:01:39.600]   the passion that Manolis has for music,
[00:01:39.600 --> 00:01:43.080]   his singing of the little excerpts from the songs,
[00:01:43.080 --> 00:01:46.200]   and in general, the meaning we discuss
[00:01:46.200 --> 00:01:49.180]   that we pull from the different songs.
[00:01:49.180 --> 00:01:50.760]   If music is not your thing,
[00:01:50.760 --> 00:01:53.960]   I do give timestamps to the less musical
[00:01:53.960 --> 00:01:56.720]   and more philosophical parts of the conversation.
[00:01:56.720 --> 00:02:00.040]   I hope you enjoy this little experimenting conversation
[00:02:00.040 --> 00:02:02.940]   about music and life.
[00:02:02.940 --> 00:02:05.260]   If you do, please subscribe on YouTube,
[00:02:05.260 --> 00:02:07.760]   review it with Five Stars on Apple Podcast,
[00:02:07.760 --> 00:02:10.360]   follow on Spotify, support on Patreon,
[00:02:10.360 --> 00:02:13.280]   or connect with me on Twitter @LexFriedman.
[00:02:13.280 --> 00:02:17.360]   And now, here's my conversation with Manolis Kallis.
[00:02:17.360 --> 00:02:21.100]   You mentioned Leonard Cohen and the song "Hallelujah"
[00:02:21.100 --> 00:02:22.800]   as a beautiful song.
[00:02:22.800 --> 00:02:26.120]   So what are the three songs
[00:02:26.120 --> 00:02:29.160]   you draw the most meaning from about life?
[00:02:29.160 --> 00:02:31.320]   - Don't get me started.
[00:02:31.320 --> 00:02:34.660]   So there's really countless songs that have marked me,
[00:02:34.660 --> 00:02:38.520]   that have sort of shaped me in periods of joy
[00:02:38.520 --> 00:02:40.280]   and in periods of sadness.
[00:02:40.280 --> 00:02:43.040]   My son likes to joke that I have a song
[00:02:43.040 --> 00:02:44.900]   for every sentence he will say,
[00:02:44.900 --> 00:02:46.320]   'cause very often I will break into a song
[00:02:46.320 --> 00:02:47.760]   with a sentence he'll say.
[00:02:47.760 --> 00:02:48.760]   (laughs)
[00:02:48.760 --> 00:02:50.520]   My wife calls me the radio
[00:02:50.520 --> 00:02:53.440]   'cause I can sort of recite hundreds of songs
[00:02:53.440 --> 00:02:54.560]   that have really shaped me.
[00:02:54.560 --> 00:02:56.800]   So it's gonna be very hard to just pick a few.
[00:02:56.800 --> 00:02:57.800]   So I'm just gonna tell you a little bit
[00:02:57.800 --> 00:03:01.640]   about my song transition as I've grown up.
[00:03:01.640 --> 00:03:03.720]   In Greece, it was very much about,
[00:03:03.720 --> 00:03:06.880]   as I told you before, the misery, the poverty,
[00:03:06.880 --> 00:03:09.000]   but also overcoming adversity.
[00:03:09.000 --> 00:03:11.920]   So some of the songs that have really shaped me
[00:03:11.920 --> 00:03:13.880]   are "Kharis Alexiou," for example,
[00:03:13.880 --> 00:03:16.560]   is one of my favorite singers in Greece.
[00:03:16.560 --> 00:03:19.600]   And then there's also really just old traditional songs
[00:03:19.600 --> 00:03:21.120]   that my parents used to listen to.
[00:03:21.120 --> 00:03:22.340]   Like one of them is,
[00:03:22.340 --> 00:03:25.480]   ♪ Ani mon plousios ♪
[00:03:25.480 --> 00:03:28.840]   Which is basically, oh, if I was rich.
[00:03:28.840 --> 00:03:32.200]   And the song is painting this beautiful picture
[00:03:32.200 --> 00:03:34.960]   about all the noises that you hear in the neighborhood,
[00:03:34.960 --> 00:03:37.540]   his poor neighborhood, the train going by,
[00:03:37.540 --> 00:03:39.960]   the priest walking to the church,
[00:03:39.960 --> 00:03:43.080]   and the kids crying next door and all of that.
[00:03:43.080 --> 00:03:44.640]   And he says, with all of that,
[00:03:44.640 --> 00:03:47.520]   I'm having trouble falling asleep and dreaming.
[00:03:47.520 --> 00:03:49.560]   ♪ If I was rich ♪
[00:03:49.560 --> 00:03:52.400]   And then he was like, you know, breaking into that.
[00:03:52.400 --> 00:03:55.720]   So it's this juxtaposition between the spirit
[00:03:55.720 --> 00:03:59.960]   and the sublime and then the physical and the harsh reality.
[00:03:59.960 --> 00:04:03.360]   It's just not having troubles, not being miserable.
[00:04:03.360 --> 00:04:05.200]   So basically, rich to him just means
[00:04:05.200 --> 00:04:06.600]   out of my misery, basically.
[00:04:06.600 --> 00:04:10.240]   And then also being able to travel,
[00:04:10.240 --> 00:04:12.280]   being able to sort of be the captain of a ship
[00:04:12.280 --> 00:04:14.200]   and see the world and stuff like that.
[00:04:14.200 --> 00:04:16.200]   So it's just such beautiful imagery.
[00:04:16.200 --> 00:04:17.360]   - So many of the Greek songs,
[00:04:17.360 --> 00:04:19.040]   just like the poetry we talked about,
[00:04:19.040 --> 00:04:22.560]   they acknowledge the cruelty, the difficulty of life,
[00:04:22.560 --> 00:04:24.440]   but are longing for a better life.
[00:04:24.440 --> 00:04:25.280]   - That's exactly right.
[00:04:25.280 --> 00:04:26.960]   And another one is "Phtohologia."
[00:04:26.960 --> 00:04:28.320]   And this is one of those songs
[00:04:28.320 --> 00:04:31.240]   that has a fast and joyful half
[00:04:31.240 --> 00:04:33.600]   and a slow and sad half.
[00:04:33.600 --> 00:04:35.760]   And it goes back and forth between them.
[00:04:35.760 --> 00:04:36.600]   And it's like,
[00:04:36.600 --> 00:04:41.200]   ♪ Phtohologia, yefsena kathemo dragoodi ♪
[00:04:41.200 --> 00:04:42.740]   So poor, you know, basically,
[00:04:42.740 --> 00:04:45.800]   it's the state of being poor.
[00:04:45.800 --> 00:04:50.160]   I don't even know if there's a word for that in English.
[00:04:50.160 --> 00:04:51.920]   And then fast part is,
[00:04:51.920 --> 00:04:56.240]   ♪ Ta heria sum megalosan ke ponesan ke matesan ♪
[00:04:56.240 --> 00:04:59.120]   So then it's like, oh, you know,
[00:04:59.120 --> 00:05:02.680]   basically like the state of being poor and misery,
[00:05:02.680 --> 00:05:05.600]   you know, for you, I write all my songs, et cetera.
[00:05:05.600 --> 00:05:07.160]   And then the fast part is,
[00:05:07.160 --> 00:05:11.600]   in your arms grew up and suffered
[00:05:11.600 --> 00:05:15.360]   and, you know, stood up and, you know, rose.
[00:05:16.160 --> 00:05:18.400]   Men with clear vision.
[00:05:18.400 --> 00:05:21.800]   This whole concept of taking on the world
[00:05:21.800 --> 00:05:25.080]   with nothing to lose because you've seen the worst of it.
[00:05:25.080 --> 00:05:26.680]   This imagery of,
[00:05:26.680 --> 00:05:29.960]   ♪ Psyllaki parisopula harastakorisopula ♪
[00:05:29.960 --> 00:05:34.120]   So it's describing the young men as cypress trees.
[00:05:34.120 --> 00:05:35.840]   And that's probably one of my earliest exposure
[00:05:35.840 --> 00:05:38.120]   to a metaphor, to sort of, you know,
[00:05:38.120 --> 00:05:40.080]   this very rich imagery.
[00:05:40.080 --> 00:05:41.560]   And I love about the fact that
[00:05:41.560 --> 00:05:43.520]   I was reading a story to my kids the other day
[00:05:43.520 --> 00:05:44.800]   and it was dark.
[00:05:44.800 --> 00:05:47.000]   And my daughter, who's six, is like,
[00:05:47.000 --> 00:05:48.480]   oh, can I please see the pictures?
[00:05:48.480 --> 00:05:51.120]   And Jonathan, who's eight,
[00:05:51.120 --> 00:05:53.600]   so my daughter Cleo is like,
[00:05:53.600 --> 00:05:54.640]   oh, let's look at the pictures.
[00:05:54.640 --> 00:05:56.320]   And my son Jonathan, he's like,
[00:05:56.320 --> 00:05:58.800]   but Cleo, if you look at the pictures,
[00:05:58.800 --> 00:06:00.240]   it's just an image.
[00:06:00.240 --> 00:06:03.280]   If you just close your eyes and listen, it's a video.
[00:06:03.280 --> 00:06:04.680]   (laughing)
[00:06:04.680 --> 00:06:05.520]   - That's brilliant.
[00:06:05.520 --> 00:06:06.840]   - It's beautiful.
[00:06:06.840 --> 00:06:09.000]   And he's basically showing just how much more
[00:06:09.000 --> 00:06:12.280]   the human imagination has besides just a few images
[00:06:12.280 --> 00:06:14.160]   that, you know, the book will give you.
[00:06:14.160 --> 00:06:15.760]   And then another one, oh gosh,
[00:06:15.760 --> 00:06:18.040]   this one is really like miserable.
[00:06:18.040 --> 00:06:23.040]   It's called "Sto perigiali, to krifo."
[00:06:23.040 --> 00:06:25.680]   And it's basically describing how
[00:06:25.680 --> 00:06:30.000]   vigorously we took on our life
[00:06:30.000 --> 00:06:33.000]   and we pushed hard towards a direction
[00:06:33.000 --> 00:06:35.040]   that we then realized was the wrong one.
[00:06:35.040 --> 00:06:37.000]   (laughing)
[00:06:37.000 --> 00:06:39.720]   And again, these songs give you so much perspective.
[00:06:39.720 --> 00:06:41.040]   There's no songs like that in English
[00:06:41.040 --> 00:06:42.040]   that will basically, you know,
[00:06:42.040 --> 00:06:44.240]   sort of just smack you in the face
[00:06:44.240 --> 00:06:47.480]   about sort of the passion and the force and the drive.
[00:06:47.480 --> 00:06:48.320]   And then it turns out,
[00:06:48.320 --> 00:06:51.160]   ah, we just followed the wrong life.
[00:06:51.160 --> 00:06:52.600]   And it's like, whoa.
[00:06:52.600 --> 00:06:54.480]   (laughing)
[00:06:54.480 --> 00:06:55.320]   - Okay, so that was you.
[00:06:55.320 --> 00:06:57.120]   - All right, so that's like before 12.
[00:06:57.120 --> 00:07:00.440]   So, you know, growing up in sort of this
[00:07:00.440 --> 00:07:03.200]   horrendously miserable, you know,
[00:07:03.200 --> 00:07:07.080]   sort of view of romanticism of, you know, suffering.
[00:07:07.080 --> 00:07:10.640]   So then my preteen years is like, you know,
[00:07:10.640 --> 00:07:12.800]   learning English through songs.
[00:07:12.800 --> 00:07:13.720]   So basically, you know,
[00:07:13.720 --> 00:07:15.520]   listening to all the American pop songs
[00:07:15.520 --> 00:07:17.800]   and then memorizing them vocally
[00:07:17.800 --> 00:07:19.600]   before I even knew what they meant.
[00:07:19.600 --> 00:07:21.000]   (laughing)
[00:07:21.000 --> 00:07:22.960]   So, you know, Madonna and Michael Jackson
[00:07:22.960 --> 00:07:25.240]   and all of these sort of really popular songs
[00:07:25.240 --> 00:07:27.240]   and, you know, George Michael.
[00:07:27.240 --> 00:07:29.360]   Just songs that I would just listen to the radio
[00:07:29.360 --> 00:07:30.760]   and repeat vocally.
[00:07:30.760 --> 00:07:32.440]   And eventually, as I started learning English,
[00:07:32.440 --> 00:07:34.480]   I was like, oh, wow, this thing has been repeating.
[00:07:34.480 --> 00:07:36.000]   I now understand what it means
[00:07:36.000 --> 00:07:37.800]   without re-listening it.
[00:07:37.800 --> 00:07:39.040]   But just with re-repeating it,
[00:07:39.040 --> 00:07:41.440]   I was like, oh. (laughing)
[00:07:41.440 --> 00:07:44.080]   Again, Michael Jackson's "Man in the Mirror"
[00:07:44.080 --> 00:07:47.160]   is teaching you that it's your responsibility
[00:07:47.160 --> 00:07:49.320]   to just improve yourself.
[00:07:49.320 --> 00:07:51.080]   You know, if you wanna make the world a better place,
[00:07:51.080 --> 00:07:52.880]   take a look at yourself and make the change.
[00:07:52.880 --> 00:07:55.080]   This whole concept of, again, I mean,
[00:07:55.080 --> 00:07:56.920]   all of these songs, you can listen to them shallowly
[00:07:56.920 --> 00:07:58.960]   or you can just listen to them and say,
[00:07:58.960 --> 00:08:00.680]   oh, there's deeper meaning here.
[00:08:00.680 --> 00:08:04.160]   And I think there's a certain philosophy of song
[00:08:04.160 --> 00:08:06.280]   as a way of touching the psyche.
[00:08:06.280 --> 00:08:08.320]   So if you look at regions of the brain,
[00:08:08.320 --> 00:08:10.080]   people who have lost their language ability
[00:08:10.080 --> 00:08:12.240]   because they have an accident in that region of the brain
[00:08:12.240 --> 00:08:14.080]   can actually sing
[00:08:14.080 --> 00:08:18.320]   because it's exactly the symmetric region of the brain.
[00:08:18.320 --> 00:08:19.600]   And that, again, teaches you so much
[00:08:19.600 --> 00:08:21.440]   about language evolution
[00:08:21.440 --> 00:08:26.120]   and sort of the duality of musicality
[00:08:26.120 --> 00:08:31.080]   and rhythmic patterns and eventually language.
[00:08:31.080 --> 00:08:33.960]   - Do you have a sense of why songs developed?
[00:08:33.960 --> 00:08:36.200]   So you're kind of suggesting that it's possible
[00:08:36.200 --> 00:08:39.240]   that there is something important
[00:08:39.240 --> 00:08:43.160]   about our connection with song and with music
[00:08:43.160 --> 00:08:46.040]   on the level of the importance of language.
[00:08:46.040 --> 00:08:47.320]   Is it possible?
[00:08:47.320 --> 00:08:48.560]   - It's not just possible.
[00:08:48.560 --> 00:08:51.200]   In my view, language comes after music.
[00:08:51.200 --> 00:08:52.520]   Language comes after song.
[00:08:52.520 --> 00:08:53.360]   No, seriously.
[00:08:53.360 --> 00:08:56.720]   Like, basically, my view of human cognitive evolution
[00:08:56.720 --> 00:08:58.800]   is rituals.
[00:08:58.800 --> 00:09:01.360]   If you look at many early cultures,
[00:09:01.360 --> 00:09:04.560]   there's rituals around every stage of life.
[00:09:04.560 --> 00:09:09.560]   There's organized dance performances around mating.
[00:09:09.560 --> 00:09:11.440]   And if you look at mate selection,
[00:09:11.440 --> 00:09:14.080]   I mean, that's an evolutionary drive right there.
[00:09:14.080 --> 00:09:16.160]   So basically, if you're not able to string together
[00:09:16.160 --> 00:09:19.960]   a complex dance as a bird, you don't get a mate.
[00:09:19.960 --> 00:09:23.240]   And that actually forms this development
[00:09:23.240 --> 00:09:25.320]   for many song-learning birds.
[00:09:25.320 --> 00:09:27.800]   Not every bird knows how to sing,
[00:09:27.800 --> 00:09:31.040]   and not every bird knows how to learn a complicated song.
[00:09:31.040 --> 00:09:32.800]   So basically, there's birds
[00:09:32.800 --> 00:09:35.360]   that simply have the same few tunes
[00:09:35.360 --> 00:09:36.200]   that they know how to play,
[00:09:36.200 --> 00:09:39.120]   and a lot of that is inherent and genetically encoded.
[00:09:39.120 --> 00:09:42.600]   And others are birds that learn how to sing.
[00:09:42.600 --> 00:09:47.320]   And if you look at a lot of these exotic birds of paradise
[00:09:47.320 --> 00:09:48.160]   and stuff like that,
[00:09:48.160 --> 00:09:49.760]   like the mating rituals that they have
[00:09:49.760 --> 00:09:51.400]   are enormously amazing.
[00:09:51.400 --> 00:09:55.120]   And I think human mating rituals of ancient tribes
[00:09:55.120 --> 00:09:56.840]   are not very far off from that.
[00:09:56.840 --> 00:10:01.840]   And in my view, the sequential formation of these movements
[00:10:01.960 --> 00:10:06.080]   is a prelude to the cognitive capabilities
[00:10:06.080 --> 00:10:08.360]   that ultimately enable language.
[00:10:08.360 --> 00:10:10.600]   - It's fascinating to think that that's
[00:10:10.600 --> 00:10:13.440]   not just an accidental precursor to intelligence.
[00:10:13.440 --> 00:10:16.040]   - Yeah, it's sexually selected.
[00:10:16.040 --> 00:10:19.560]   - Well, it's sexually selected and it's a prerequisite.
[00:10:19.560 --> 00:10:20.400]   - Yeah.
[00:10:20.400 --> 00:10:21.720]   - It's like it's required for intelligence.
[00:10:21.720 --> 00:10:24.960]   - And even as language has now developed,
[00:10:24.960 --> 00:10:28.760]   I think the artistic expression is needed,
[00:10:28.760 --> 00:10:31.280]   like badly needed by our brain.
[00:10:31.280 --> 00:10:33.240]   So it's not just that,
[00:10:33.240 --> 00:10:35.760]   oh, our brain can kind of take a break
[00:10:35.760 --> 00:10:36.840]   and go do that stuff.
[00:10:36.840 --> 00:10:40.160]   No, I mean, I don't know if you remember that scene from,
[00:10:40.160 --> 00:10:43.560]   oh gosh, what's that Jack Nicholson movie in New Hampshire?
[00:10:43.560 --> 00:10:47.840]   All work and no play, make Jack a doll boy.
[00:10:47.840 --> 00:10:49.640]   - The doll boy, The Shining.
[00:10:49.640 --> 00:10:50.480]   - The Shining.
[00:10:50.480 --> 00:10:51.520]   (laughing)
[00:10:51.520 --> 00:10:52.360]   - Such a good movie.
[00:10:52.360 --> 00:10:53.720]   - There's this amazing scene
[00:10:53.720 --> 00:10:56.200]   where he's constantly trying to concentrate
[00:10:56.200 --> 00:10:59.040]   and what's coming out of the typewriter is just gibberish.
[00:10:59.040 --> 00:11:01.600]   And I have that image as well when I'm working.
[00:11:01.600 --> 00:11:04.560]   And I'm like, no, basically all of these crazy,
[00:11:04.560 --> 00:11:06.520]   huge number of hobbies that I have,
[00:11:06.520 --> 00:11:09.520]   they're not just tolerated by my work,
[00:11:09.520 --> 00:11:11.960]   they're required by my work.
[00:11:11.960 --> 00:11:13.920]   This ability of sort of stretching your brain
[00:11:13.920 --> 00:11:15.600]   in all these different directions
[00:11:15.600 --> 00:11:20.120]   is connecting your emotional self and your cognitive self.
[00:11:20.120 --> 00:11:21.880]   And that's a prerequisite
[00:11:21.880 --> 00:11:24.680]   to being able to be cognitively capable,
[00:11:24.680 --> 00:11:25.520]   at least in my view.
[00:11:25.520 --> 00:11:28.800]   - Yeah, I wonder if the world without art and music,
[00:11:28.800 --> 00:11:30.600]   you're just making me realize that perhaps
[00:11:30.600 --> 00:11:34.680]   that world would be not just devoid of fun things
[00:11:34.680 --> 00:11:38.080]   to look at or listen to, but devoid of all the other stuff,
[00:11:38.080 --> 00:11:41.120]   all the bridges and rockets and science.
[00:11:41.120 --> 00:11:42.360]   - Exactly, exactly.
[00:11:42.360 --> 00:11:45.280]   Creativity is not disconnected from art.
[00:11:45.280 --> 00:11:49.120]   And my kids, I mean, I could be doing
[00:11:49.120 --> 00:11:50.480]   the full math treatment to them.
[00:11:50.480 --> 00:11:53.280]   No, they play the piano and they play the violin
[00:11:53.280 --> 00:11:54.280]   and they play sports.
[00:11:54.280 --> 00:11:58.120]   I mean, this whole sort of movement
[00:11:58.120 --> 00:12:01.840]   and going through mazes and playing tennis
[00:12:01.840 --> 00:12:06.400]   and playing soccer and avoiding obstacles and all of that,
[00:12:06.400 --> 00:12:09.120]   that forms your three-dimensional view of the world.
[00:12:09.120 --> 00:12:10.600]   Being able to actually move and run
[00:12:10.600 --> 00:12:12.560]   and play in three dimensions
[00:12:12.560 --> 00:12:14.600]   is extremely important for math,
[00:12:14.600 --> 00:12:17.840]   for stringing together complicated concepts.
[00:12:17.840 --> 00:12:21.440]   It's the same underlying cognitive machinery
[00:12:21.440 --> 00:12:23.640]   that is used for navigating mazes
[00:12:23.640 --> 00:12:27.360]   and for navigating theorems
[00:12:27.360 --> 00:12:28.760]   and sort of solving equations.
[00:12:28.760 --> 00:12:32.360]   So I can't have a conversation with my students
[00:12:32.360 --> 00:12:35.360]   without sort of either using my hands
[00:12:35.360 --> 00:12:39.360]   or opening the whiteboard in Zoom
[00:12:39.360 --> 00:12:41.240]   and just constantly drawing.
[00:12:41.240 --> 00:12:43.760]   Or back when we had in-person meetings,
[00:12:43.760 --> 00:12:44.600]   just the whiteboard.
[00:12:44.600 --> 00:12:47.200]   - The whiteboard, yeah, that's fascinating to think about.
[00:12:47.200 --> 00:12:49.440]   So that's Michael Jackson, man.
[00:12:49.440 --> 00:12:52.200]   "Mirror, Careless Whisper" with George Michael,
[00:12:52.200 --> 00:12:53.040]   which is a song I like.
[00:12:53.040 --> 00:12:53.880]   You didn't say "Careless Whisper."
[00:12:53.880 --> 00:12:54.720]   - It's "The Careless Whisper."
[00:12:54.720 --> 00:12:55.560]   - You didn't say that?
[00:12:55.560 --> 00:12:56.400]   I like that one.
[00:12:56.400 --> 00:12:57.240]   That's me.
[00:12:57.240 --> 00:12:58.080]   I had recorded-- - "Pogular" for you?
[00:12:58.080 --> 00:12:59.960]   - I had recorded, no, no, no.
[00:12:59.960 --> 00:13:01.640]   It's an amazing song for me.
[00:13:01.640 --> 00:13:03.560]   I had recorded a small part of it
[00:13:03.560 --> 00:13:05.800]   as it's played at the tail end of the radio.
[00:13:05.800 --> 00:13:08.480]   And I had a tape where I only had part of that song.
[00:13:08.480 --> 00:13:09.320]   - Part of that song, you just sang it over and over.
[00:13:09.320 --> 00:13:11.960]   - And I just played it over and over and over again.
[00:13:11.960 --> 00:13:13.760]   Just so beautiful.
[00:13:13.760 --> 00:13:15.120]   - It's so heartbreaking.
[00:13:15.120 --> 00:13:17.000]   That song is almost Greek, it's so heartbreaking.
[00:13:17.000 --> 00:13:17.840]   - I know.
[00:13:17.840 --> 00:13:19.080]   And George Michael is Greek.
[00:13:19.080 --> 00:13:19.920]   - Is he Greek? - He's Greek.
[00:13:19.920 --> 00:13:20.800]   - He's, no. - Of course.
[00:13:20.800 --> 00:13:22.760]   George Michaelides, I mean, he's Greek.
[00:13:22.760 --> 00:13:23.600]   - Yeah.
[00:13:23.600 --> 00:13:24.800]   (both laughing)
[00:13:24.800 --> 00:13:25.640]   - Now you know.
[00:13:26.600 --> 00:13:30.320]   So sorry to offend you so deeply not knowing this.
[00:13:30.320 --> 00:13:31.160]   So, okay, so what's--
[00:13:31.160 --> 00:13:33.040]   - So anyway, so we're moving to France when I'm 12 years old
[00:13:33.040 --> 00:13:36.160]   and now I'm getting into the songs of Gainsbourg.
[00:13:36.160 --> 00:13:39.600]   So Gainsbourg is this incredible French composer.
[00:13:39.600 --> 00:13:42.000]   He is always seen on stage,
[00:13:42.000 --> 00:13:44.320]   like not even pretending to try to please,
[00:13:44.320 --> 00:13:45.320]   just like with his cigarette,
[00:13:45.320 --> 00:13:47.520]   just like rrrr, mumbling his songs.
[00:13:47.520 --> 00:13:49.840]   But the lyrics are unbelievable.
[00:13:49.840 --> 00:13:53.080]   Like basically entire sentences will rhyme.
[00:13:53.080 --> 00:13:54.880]   He will say the same thing twice.
[00:13:54.880 --> 00:13:56.600]   And you're like, whoa.
[00:13:56.600 --> 00:13:58.000]   (laughs)
[00:13:58.000 --> 00:14:00.440]   And in fact, another, speaking of Greek,
[00:14:00.440 --> 00:14:03.040]   a French Greek, Georges Moustaki.
[00:14:03.040 --> 00:14:05.240]   This song is just magnificent.
[00:14:05.240 --> 00:14:09.160]   (speaking in foreign language)
[00:14:09.160 --> 00:14:13.600]   So with my face of,
[00:14:13.600 --> 00:14:15.520]   metek is actually a Greek word.
[00:14:15.520 --> 00:14:17.800]   It's a French word for a Greek word.
[00:14:17.800 --> 00:14:20.040]   But met comes from meta,
[00:14:20.040 --> 00:14:23.360]   and then ek from Ikea, from ecology, which means home.
[00:14:23.360 --> 00:14:26.640]   So metek is someone who has changed homes, who are migrant.
[00:14:26.640 --> 00:14:28.480]   So with my face of a migrant,
[00:14:28.480 --> 00:14:30.000]   and you'll love this one,
[00:14:30.000 --> 00:14:32.320]   the (speaking in foreign language)
[00:14:32.320 --> 00:14:35.400]   of a meandering Jew,
[00:14:35.400 --> 00:14:37.800]   of Greek pastor.
[00:14:37.800 --> 00:14:39.600]   (laughs)
[00:14:39.600 --> 00:14:42.000]   So again, you know, the Russian Greek,
[00:14:42.000 --> 00:14:43.600]   Jew orthodox connection.
[00:14:43.600 --> 00:14:46.920]   (speaking in foreign language)
[00:14:46.920 --> 00:14:48.760]   With my hair in the four wings.
[00:14:48.760 --> 00:14:52.680]   (speaking in foreign language)
[00:14:53.640 --> 00:14:56.000]   With my eyes that are all washed out,
[00:14:56.000 --> 00:14:58.840]   who gives me the pretense of dreaming,
[00:14:58.840 --> 00:15:01.280]   but who don't dream that much anymore.
[00:15:01.280 --> 00:15:05.240]   With my hands of thief, of musician,
[00:15:05.240 --> 00:15:09.000]   and who have stolen so many gardens.
[00:15:09.000 --> 00:15:11.320]   With my mouth that has drunk,
[00:15:11.320 --> 00:15:14.560]   that has kissed, and that has bitten,
[00:15:14.560 --> 00:15:16.920]   without ever pleasing its hunger.
[00:15:18.040 --> 00:15:22.400]   With my skin that has been rubbed
[00:15:22.400 --> 00:15:25.680]   in the sun of all the summers,
[00:15:25.680 --> 00:15:28.080]   and anything that was wearing a skirt.
[00:15:28.080 --> 00:15:30.560]   With my heart,
[00:15:30.560 --> 00:15:32.200]   and then you have to listen to this verse,
[00:15:32.200 --> 00:15:33.400]   it's so beautiful.
[00:15:33.400 --> 00:15:37.320]   (speaking in foreign language)
[00:15:37.320 --> 00:15:44.560]   With my heart that knew how to make suffer
[00:15:44.560 --> 00:15:46.600]   as much as it suffered,
[00:15:46.600 --> 00:15:49.080]   but was able to,
[00:15:49.080 --> 00:15:50.400]   that knew how to make,
[00:15:50.400 --> 00:15:51.440]   in French is actually,
[00:15:51.440 --> 00:15:53.200]   (speaking in foreign language)
[00:15:53.200 --> 00:15:54.760]   that knew how to make.
[00:15:54.760 --> 00:15:57.880]   (speaking in foreign language)
[00:15:57.880 --> 00:15:59.520]   Verses that span the whole thing.
[00:15:59.520 --> 00:16:00.840]   It's just beautiful.
[00:16:00.840 --> 00:16:02.960]   - Do you know, on a small tangent,
[00:16:02.960 --> 00:16:05.800]   do you know Jacques Brel?
[00:16:05.800 --> 00:16:06.960]   - Of course, of course.
[00:16:06.960 --> 00:16:08.840]   (speaking in foreign language)
[00:16:08.840 --> 00:16:09.680]   - Yeah. - You know those songs?
[00:16:09.680 --> 00:16:12.600]   Those, that song gets me every time.
[00:16:12.600 --> 00:16:14.840]   - So there's a cover of that song
[00:16:14.880 --> 00:16:17.160]   by one of my favorite female artists.
[00:16:17.160 --> 00:16:18.440]   - Not Nina Simone.
[00:16:18.440 --> 00:16:19.840]   - No, no, no, no, no.
[00:16:19.840 --> 00:16:21.800]   - Modern? - Carol Emerald.
[00:16:21.800 --> 00:16:24.800]   She's from Amsterdam.
[00:16:24.800 --> 00:16:28.240]   And she has a version of "Noumekito Paa"
[00:16:28.240 --> 00:16:30.600]   where she's actually added some English lyrics.
[00:16:30.600 --> 00:16:33.120]   And it's really beautiful.
[00:16:33.120 --> 00:16:35.320]   But again, "Noumekito Paa" is just so,
[00:16:35.320 --> 00:16:38.400]   I mean, it's, you know, the promises,
[00:16:38.400 --> 00:16:42.040]   the volcanoes that will restart.
[00:16:42.040 --> 00:16:43.480]   It's just so beautiful.
[00:16:43.480 --> 00:16:45.200]   And-- - I love,
[00:16:45.200 --> 00:16:50.200]   there's not many songs that show such depth of desperation
[00:16:50.200 --> 00:16:52.840]   for another human being.
[00:16:52.840 --> 00:16:54.000]   That's so powerful.
[00:16:54.000 --> 00:16:54.840]   Unapologetic.
[00:16:54.840 --> 00:16:58.680]   (singing in foreign language)
[00:16:58.680 --> 00:17:03.800]   - And then high school, now I'm starting to learn English.
[00:17:03.800 --> 00:17:05.040]   So I moved to New York.
[00:17:05.040 --> 00:17:07.720]   So Sting's "Englishman in New York."
[00:17:07.720 --> 00:17:09.920]   - Yeah. - Magnificent song.
[00:17:09.920 --> 00:17:10.960]   And again, there's,
[00:17:10.960 --> 00:17:14.320]   ♪ If manners mageth manners someone said ♪
[00:17:14.320 --> 00:17:16.920]   ♪ Then he's the hero of the day ♪
[00:17:16.920 --> 00:17:20.120]   ♪ It takes a man to suffer ignorance and smile ♪
[00:17:20.120 --> 00:17:23.840]   ♪ Be yourself no matter what they say ♪
[00:17:23.840 --> 00:17:27.800]   And then, ♪ Takes more than combat gear to make a man ♪
[00:17:27.800 --> 00:17:30.720]   ♪ Takes more than a license for a gun ♪
[00:17:30.720 --> 00:17:34.200]   ♪ Confront your enemies, avoid them when you can ♪
[00:17:34.200 --> 00:17:37.480]   ♪ A gentleman will walk but never run ♪
[00:17:37.480 --> 00:17:39.920]   It's, again, you're talking about songs
[00:17:39.920 --> 00:17:41.240]   that teach you how to live.
[00:17:41.240 --> 00:17:42.640]   I mean, this is one of them.
[00:17:42.640 --> 00:17:46.400]   Basically says, it's not the combat gear that makes a man.
[00:17:46.400 --> 00:17:49.560]   - Where's the part where he says, there you go.
[00:17:49.560 --> 00:17:53.920]   ♪ Gentleness so brighty, a rare in this society ♪
[00:17:53.920 --> 00:17:56.840]   ♪ At night a candle's brighter than the sun ♪
[00:17:56.840 --> 00:17:57.680]   So beautiful.
[00:17:57.680 --> 00:18:00.760]   He basically says, well, you just might be the only one.
[00:18:00.760 --> 00:18:03.800]   ♪ Modesty propriety can lead to notoriety ♪
[00:18:03.800 --> 00:18:05.920]   ♪ You could end up as the only one ♪
[00:18:05.920 --> 00:18:08.160]   It's, it basically tells you,
[00:18:08.160 --> 00:18:09.880]   you don't have to be like the others.
[00:18:09.880 --> 00:18:14.040]   Be yourself, show kindness, show generosity.
[00:18:14.040 --> 00:18:18.320]   Don't, you know, don't let that anger get to you.
[00:18:18.320 --> 00:18:19.840]   You know the song "Fragile"?
[00:18:19.840 --> 00:18:22.960]   ♪ How fragile we are, how fragile we are ♪
[00:18:22.960 --> 00:18:26.120]   So again, as in Greece, I didn't even know what that meant,
[00:18:26.120 --> 00:18:28.880]   how fragile we are, but the song was so beautiful.
[00:18:28.880 --> 00:18:30.000]   And then eventually I learned English
[00:18:30.000 --> 00:18:31.840]   and I actually understand the lyrics.
[00:18:31.840 --> 00:18:34.040]   And the song is actually written
[00:18:34.040 --> 00:18:39.680]   after the Contras murdered Ben Linder in 1987.
[00:18:39.680 --> 00:18:41.880]   And the US eventually turned against
[00:18:41.880 --> 00:18:44.520]   supporting these guerrillas.
[00:18:44.520 --> 00:18:46.160]   And it was just a political song,
[00:18:46.160 --> 00:18:48.160]   but so such a realization that
[00:18:48.160 --> 00:18:51.400]   you can't win with violence, basically.
[00:18:51.400 --> 00:18:55.360]   And that song starts with the most beautiful poetry.
[00:18:55.360 --> 00:18:56.200]   So,
[00:18:56.200 --> 00:19:00.680]   ♪ If blood will flow when flesh and steel are one ♪
[00:19:00.680 --> 00:19:04.600]   ♪ Drying in the color of the evening sun ♪
[00:19:04.600 --> 00:19:08.280]   ♪ Tomorrow's rain will wash the stains away ♪
[00:19:08.280 --> 00:19:11.760]   ♪ But something in our minds will always stay ♪
[00:19:11.760 --> 00:19:14.280]   ♪ Perhaps this final act was meant ♪
[00:19:14.280 --> 00:19:16.720]   ♪ To clinch a lifetime's argument ♪
[00:19:16.720 --> 00:19:19.040]   ♪ That nothing comes from violence ♪
[00:19:19.040 --> 00:19:21.320]   ♪ And nothing ever could ♪
[00:19:21.320 --> 00:19:25.000]   ♪ For all those born beneath an angry star ♪
[00:19:25.000 --> 00:19:28.280]   ♪ Lest we forget how fragile we are ♪
[00:19:28.280 --> 00:19:29.360]   - Damn.
[00:19:29.360 --> 00:19:30.200]   - Damn, right?
[00:19:30.200 --> 00:19:31.080]   (laughing)
[00:19:31.080 --> 00:19:33.680]   I mean, that's poetry.
[00:19:33.680 --> 00:19:35.680]   It was beautiful.
[00:19:35.680 --> 00:19:37.440]   And he's using the English language,
[00:19:37.440 --> 00:19:42.000]   it's just such a refined way with deep meanings,
[00:19:42.000 --> 00:19:45.600]   but also words that rhyme just so beautifully
[00:19:45.600 --> 00:19:49.760]   and evocations of when flesh and steel are one.
[00:19:49.760 --> 00:19:53.360]   I mean, it's just mind boggling.
[00:19:53.360 --> 00:19:54.320]   And then of course,
[00:19:54.320 --> 00:19:55.960]   the refrain that everybody remembers is,
[00:19:55.960 --> 00:19:59.200]   ♪ On and on the rain will fall ♪
[00:19:59.200 --> 00:20:00.040]   Et cetera.
[00:20:00.040 --> 00:20:00.880]   But like this beginning.
[00:20:00.880 --> 00:20:01.880]   - Tears from a star, wow.
[00:20:01.880 --> 00:20:03.400]   - Yeah.
[00:20:03.400 --> 00:20:06.840]   And again, tears from a star, how fragile we are.
[00:20:06.840 --> 00:20:10.160]   I mean, just these rhymes are just flowing so naturally.
[00:20:10.160 --> 00:20:12.800]   - Something, it seems that more meaning comes
[00:20:12.800 --> 00:20:14.840]   when there's a rhythm that,
[00:20:14.840 --> 00:20:16.000]   I don't know what that is.
[00:20:16.000 --> 00:20:18.200]   That probably connects to exactly what you were saying.
[00:20:18.200 --> 00:20:19.200]   - And if you pay close attention,
[00:20:19.200 --> 00:20:22.480]   you will notice that the more obvious words
[00:20:22.480 --> 00:20:25.120]   sometimes are the second verse.
[00:20:25.120 --> 00:20:28.120]   And the less obvious are often the first verse
[00:20:28.120 --> 00:20:31.680]   because it makes the second verse flow much more naturally
[00:20:31.680 --> 00:20:33.760]   because otherwise it feels contrived.
[00:20:33.760 --> 00:20:36.480]   Oh, you went and found this like unusual word.
[00:20:36.480 --> 00:20:39.800]   In "Dark Moments", the whole album of Pink Floyd
[00:20:39.800 --> 00:20:42.560]   and the movie just marked me enormously
[00:20:42.560 --> 00:20:45.200]   as a teenager, just the wall.
[00:20:45.200 --> 00:20:47.680]   And there's one song that never actually made it
[00:20:47.680 --> 00:20:49.840]   into the album, that's only there in the movie
[00:20:49.840 --> 00:20:51.920]   about when the Tigers broke free
[00:20:51.920 --> 00:20:55.000]   and the Tigers are the tanks of the Germans.
[00:20:55.000 --> 00:20:58.120]   And it just describes again, this vivid imagery.
[00:20:58.120 --> 00:21:02.920]   It was just before dawn, one miserable morning in black 44,
[00:21:02.920 --> 00:21:05.720]   when the forward commander was told to sit tight
[00:21:05.720 --> 00:21:08.440]   when he asked that his men be withdrawn.
[00:21:08.440 --> 00:21:12.840]   And the generals gave thanks as the other ranks held back
[00:21:12.840 --> 00:21:16.040]   the enemy tanks for a while.
[00:21:16.040 --> 00:21:20.720]   And the Anzio Bridgehead was held for the price
[00:21:20.720 --> 00:21:24.640]   of a few hundred ordinary lives.
[00:21:24.640 --> 00:21:27.080]   So that's a theme that keeps coming back in Pink Floyd
[00:21:27.080 --> 00:21:28.920]   with "Us Versus Them".
[00:21:28.920 --> 00:21:30.560]   ♪ Us and them ♪
[00:21:30.560 --> 00:21:32.240]   ♪ God only knows ♪
[00:21:32.240 --> 00:21:36.400]   ♪ That's not what we would choose to do ♪
[00:21:36.400 --> 00:21:39.760]   ♪ Forward he cried from the rear ♪
[00:21:39.760 --> 00:21:43.480]   ♪ And the front rows died ♪
[00:21:43.480 --> 00:21:45.120]   From another song, it's like this whole concept
[00:21:45.120 --> 00:21:46.400]   of "Us Versus Them".
[00:21:46.400 --> 00:21:48.720]   And there's that theme of "Us Versus Them" again
[00:21:48.720 --> 00:21:53.000]   where the child is discovering how his father died
[00:21:53.000 --> 00:21:55.960]   when he finds an old and a founded one day
[00:21:55.960 --> 00:21:58.560]   in a drawer of old photographs hidden away.
[00:21:58.560 --> 00:22:03.200]   And my eyes still grow damp to remember his majesty's sign
[00:22:03.200 --> 00:22:05.240]   with his own rubber stamp.
[00:22:05.240 --> 00:22:08.640]   So it's so ironic because it seems the way
[00:22:08.640 --> 00:22:10.160]   that he's writing it, that he's not crying
[00:22:10.160 --> 00:22:11.760]   because his father was lost.
[00:22:11.760 --> 00:22:16.000]   He's crying because kind old King George took the time
[00:22:16.000 --> 00:22:18.680]   to actually write mother a note about the fact
[00:22:18.680 --> 00:22:20.120]   that his father died.
[00:22:20.120 --> 00:22:23.400]   It's so ironic 'cause it basically says,
[00:22:23.400 --> 00:22:26.920]   we are just ordinary men and of course we're disposable.
[00:22:26.920 --> 00:22:29.840]   So I don't know if you know the root of the word pioneers,
[00:22:29.840 --> 00:22:34.920]   but you had a chessboard here earlier, a pawn.
[00:22:34.920 --> 00:22:36.960]   In French, it's a pion.
[00:22:36.960 --> 00:22:38.360]   They are the ones that you send to the front
[00:22:38.360 --> 00:22:40.280]   to get murdered, slaughtered.
[00:22:40.280 --> 00:22:42.880]   This whole concept of pioneers having taken
[00:22:42.880 --> 00:22:45.360]   this whole disposable ordinary men
[00:22:45.360 --> 00:22:48.880]   to actually be the ones that we're now treating as heroes.
[00:22:48.880 --> 00:22:50.920]   So anyway, there's this juxtaposition of that.
[00:22:50.920 --> 00:22:53.680]   And then the part that always just strikes me
[00:22:53.680 --> 00:22:56.920]   is the music and the tonality totally changes.
[00:22:56.920 --> 00:22:59.200]   And now he describes the attack.
[00:22:59.200 --> 00:23:01.200]   ♪ It was dark all around ♪
[00:23:01.200 --> 00:23:03.160]   ♪ There was frost in the ground ♪
[00:23:03.160 --> 00:23:05.720]   ♪ When the tigers broke free ♪
[00:23:05.720 --> 00:23:07.480]   ♪ And no one survived ♪
[00:23:07.480 --> 00:23:11.120]   ♪ From the Royal Fusiliers Company ♪
[00:23:11.120 --> 00:23:13.280]   ♪ They were all left behind ♪
[00:23:13.280 --> 00:23:15.600]   ♪ Most of them dead ♪
[00:23:15.600 --> 00:23:18.880]   ♪ The rest of them dying ♪
[00:23:18.880 --> 00:23:21.480]   ♪ And that's how the high command ♪
[00:23:21.480 --> 00:23:24.800]   ♪ Took my daddy from me ♪
[00:23:24.800 --> 00:23:27.800]   And that song, even though it's not in the album,
[00:23:27.800 --> 00:23:30.040]   explains the whole movie.
[00:23:30.040 --> 00:23:32.080]   'Cause it's this movie of misery.
[00:23:32.080 --> 00:23:35.080]   It's this movie of someone being stuck in their head
[00:23:35.080 --> 00:23:37.080]   and not being able to get out of it.
[00:23:37.080 --> 00:23:40.120]   There's no other movie that I think has captured so well
[00:23:40.120 --> 00:23:44.760]   this prison that is someone's own mind.
[00:23:44.760 --> 00:23:47.240]   And this wall that you're stuck inside
[00:23:47.240 --> 00:23:50.680]   and this feeling of loneliness.
[00:23:50.680 --> 00:23:53.080]   And sort of, is there anybody out there?
[00:23:53.080 --> 00:23:56.680]   And you know, sort of, hello, hello,
[00:23:56.680 --> 00:23:59.120]   is there anybody in there?
[00:23:59.120 --> 00:24:01.160]   Just nod if you can hear me.
[00:24:01.160 --> 00:24:03.000]   Is there anyone home?
[00:24:03.000 --> 00:24:06.800]   Come on, yeah.
[00:24:06.800 --> 00:24:09.000]   I hear you're feeling down.
[00:24:09.000 --> 00:24:12.920]   Just one little day
[00:24:12.920 --> 00:24:15.480]   ♪ And you're not in again ♪
[00:24:15.480 --> 00:24:16.840]   Anyway, so--
[00:24:16.840 --> 00:24:18.080]   - Yeah, the prison of your mind.
[00:24:18.080 --> 00:24:19.640]   So those are the darker moments.
[00:24:19.640 --> 00:24:22.000]   - Exactly, these are the darker moments.
[00:24:22.000 --> 00:24:23.760]   - Yeah, in the darker moments,
[00:24:23.760 --> 00:24:28.040]   the mind does feel like you're trapped
[00:24:28.040 --> 00:24:30.120]   alone in a room with it.
[00:24:30.120 --> 00:24:32.440]   - Yeah, and there's this scene in the movie
[00:24:32.440 --> 00:24:35.600]   which is like, where he just breaks out with his guitar
[00:24:35.600 --> 00:24:36.840]   and there's this prostitute in the room.
[00:24:36.840 --> 00:24:39.720]   He starts throwing stuff and then he like, you know,
[00:24:39.720 --> 00:24:41.840]   breaks the window, he throws the chair outside
[00:24:41.840 --> 00:24:43.680]   and then you see him laying in the pool
[00:24:43.680 --> 00:24:45.800]   with his own blood like, you know, everywhere.
[00:24:45.800 --> 00:24:48.840]   And then there's these endless hours spent
[00:24:48.840 --> 00:24:51.680]   fixing every little thing and lining it up.
[00:24:51.680 --> 00:24:55.240]   And it's this whole sort of mania versus, you know,
[00:24:55.240 --> 00:24:57.320]   you can spend hours building up something
[00:24:57.320 --> 00:24:59.760]   and just destroy it in a few seconds.
[00:24:59.760 --> 00:25:01.600]   One of my turns is that song.
[00:25:01.600 --> 00:25:03.240]   And it's like,
[00:25:03.240 --> 00:25:07.560]   ♪ I feel cold as a tourniquet ♪
[00:25:07.560 --> 00:25:10.080]   ♪ Red as an intercom ♪
[00:25:10.080 --> 00:25:13.920]   ♪ Dry as a funeral drum ♪
[00:25:13.920 --> 00:25:15.720]   And then the music builds up, it's like,
[00:25:15.800 --> 00:25:17.560]   ♪ Run to the bedroom ♪
[00:25:17.560 --> 00:25:19.920]   ♪ There's a suitcase on the left ♪
[00:25:19.920 --> 00:25:23.240]   ♪ You'll find my favourite axe ♪
[00:25:23.240 --> 00:25:24.920]   ♪ Don't look so frightened ♪
[00:25:24.920 --> 00:25:27.400]   ♪ This is just a passing phase ♪
[00:25:27.400 --> 00:25:30.000]   ♪ One of my bad days ♪
[00:25:30.000 --> 00:25:31.080]   It's just so beautiful.
[00:25:31.080 --> 00:25:33.120]   - I need to rewatch it, that's so,
[00:25:33.120 --> 00:25:34.480]   you're making me realistic. - But imagine watching this
[00:25:34.480 --> 00:25:35.800]   as a teenager. - Yeah, yeah.
[00:25:35.800 --> 00:25:37.680]   - It like, ruins your mind.
[00:25:37.680 --> 00:25:39.000]   (laughing)
[00:25:39.000 --> 00:25:42.200]   Like so many, it's just such harsh imagery.
[00:25:42.200 --> 00:25:44.600]   And then, you know, anyway,
[00:25:44.600 --> 00:25:46.720]   so there's the dark moment.
[00:25:46.720 --> 00:25:48.800]   And then again, going back to Sting,
[00:25:48.800 --> 00:25:50.880]   now it's the political songs, Russians.
[00:25:50.880 --> 00:25:53.760]   And I think that song should be
[00:25:53.760 --> 00:25:56.440]   a new national anthem for the US.
[00:25:56.440 --> 00:25:58.520]   Not for Russians, but for Red versus Blue.
[00:25:58.520 --> 00:26:03.120]   ♪ Mr. Khrushchev says we will bury you ♪
[00:26:03.120 --> 00:26:06.760]   ♪ I don't subscribe to this point of view ♪
[00:26:06.760 --> 00:26:10.920]   ♪ It'd be such an ignorant thing to do ♪
[00:26:10.920 --> 00:26:15.480]   ♪ If the Russians love their children too ♪
[00:26:15.480 --> 00:26:16.520]   What is it doing?
[00:26:16.520 --> 00:26:17.760]   It's basically saying,
[00:26:17.760 --> 00:26:22.120]   the Russians are just as humans as we are.
[00:26:22.120 --> 00:26:25.560]   There's no way that they're gonna let their children die.
[00:26:25.560 --> 00:26:27.400]   And then it's just so beautiful.
[00:26:27.400 --> 00:26:30.960]   ♪ How can I save my innocent boy ♪
[00:26:30.960 --> 00:26:34.200]   ♪ From Oppenheimer's deadly toy ♪
[00:26:34.200 --> 00:26:37.480]   And now that's the new national anthem, are you reading?
[00:26:37.480 --> 00:26:40.800]   ♪ There is no monopoly of common sense ♪
[00:26:40.800 --> 00:26:44.560]   ♪ On either side of the political fence ♪
[00:26:44.560 --> 00:26:47.920]   ♪ We share the same biology ♪
[00:26:47.920 --> 00:26:51.360]   ♪ Regardless of ideology ♪
[00:26:51.360 --> 00:26:55.480]   ♪ Believe me when I say to you ♪
[00:26:55.480 --> 00:27:00.240]   ♪ I hope the Russians love their children too ♪
[00:27:00.240 --> 00:27:03.680]   ♪ There's no such thing as a winnable war ♪
[00:27:03.680 --> 00:27:08.560]   ♪ It's a lie we don't believe anymore ♪
[00:27:08.560 --> 00:27:10.600]   I mean, it's beautiful, right?
[00:27:10.600 --> 00:27:13.640]   And for God's sake, America, wake up.
[00:27:13.640 --> 00:27:15.680]   These are your fellow Americans.
[00:27:15.680 --> 00:27:19.160]   They're your fellow biology.
[00:27:19.160 --> 00:27:21.440]   There is no monopoly of common sense
[00:27:21.440 --> 00:27:23.200]   on either side of the political fence.
[00:27:23.200 --> 00:27:24.320]   It's just so beautiful.
[00:27:24.320 --> 00:27:27.320]   - There's no crisper, simpler way to say,
[00:27:27.320 --> 00:27:31.160]   Russians love their children too, the common humanity.
[00:27:31.160 --> 00:27:33.200]   - Yeah, and remember what I was telling you,
[00:27:33.200 --> 00:27:35.200]   I think in one of our first podcasts
[00:27:35.200 --> 00:27:39.640]   about the daughter who's crying for her brother
[00:27:39.640 --> 00:27:40.920]   to come back from war,
[00:27:40.920 --> 00:27:43.120]   and then the Virgin Mary appears and says,
[00:27:43.120 --> 00:27:44.480]   who should I take instead?
[00:27:44.480 --> 00:27:47.560]   This Turk, here's his family, here's his children.
[00:27:47.560 --> 00:27:51.040]   This other one, he just got married, et cetera.
[00:27:51.040 --> 00:27:53.760]   And that basically says, no, I mean,
[00:27:53.760 --> 00:27:56.280]   if you look at the Lord of the Rings,
[00:27:56.280 --> 00:27:59.440]   the enemies are these monsters, they're not human.
[00:27:59.440 --> 00:28:00.880]   And that's what we always do.
[00:28:00.880 --> 00:28:04.880]   We always say, they're not like us, they're different.
[00:28:04.880 --> 00:28:06.240]   They're not humans, et cetera.
[00:28:06.240 --> 00:28:08.920]   So there's this dehumanization that has to happen
[00:28:08.920 --> 00:28:10.200]   for people to go to war.
[00:28:10.200 --> 00:28:14.280]   If you realize just how close we are genetically,
[00:28:14.280 --> 00:28:18.680]   one with the other, this whole 99.9% identical,
[00:28:18.680 --> 00:28:22.120]   you can't bear weapons against someone who's like that.
[00:28:22.120 --> 00:28:24.160]   - And the things that are the most meaningful to us
[00:28:24.160 --> 00:28:28.600]   in our lives at every level is the same on all sides,
[00:28:28.600 --> 00:28:30.040]   on both sides. - Exactly.
[00:28:30.040 --> 00:28:32.360]   - So it's not just that we're genetically the same.
[00:28:32.360 --> 00:28:34.400]   - Yeah, we're ideologically the same.
[00:28:34.400 --> 00:28:36.360]   We love our children, we love our country.
[00:28:36.360 --> 00:28:39.920]   We will fight for our family.
[00:28:39.920 --> 00:28:43.880]   And the last one I mentioned last time we spoke,
[00:28:43.880 --> 00:28:47.280]   which is Joni Mitchell's "Both Sides Now."
[00:28:47.280 --> 00:28:50.960]   So she has three rounds, one on clouds,
[00:28:50.960 --> 00:28:53.360]   one on love, and one on life.
[00:28:53.360 --> 00:28:55.520]   And on clouds, she says,
[00:28:55.520 --> 00:28:58.920]   ♪ Rows and flows of angel hair ♪
[00:28:58.920 --> 00:29:02.160]   ♪ And ice cream castles in the air ♪
[00:29:02.160 --> 00:29:04.640]   ♪ And feather canyons everywhere ♪
[00:29:04.640 --> 00:29:07.240]   ♪ I've looked at clouds that way ♪
[00:29:07.240 --> 00:29:10.720]   ♪ But now they only block the sun ♪
[00:29:10.720 --> 00:29:14.160]   ♪ They rain and snow on everyone ♪
[00:29:14.160 --> 00:29:17.240]   ♪ So many things I would've done ♪
[00:29:17.240 --> 00:29:20.080]   ♪ But clouds got in my way ♪
[00:29:20.080 --> 00:29:23.360]   ♪ And then I've looked at clouds from both sides now ♪
[00:29:23.360 --> 00:29:25.640]   ♪ From up and down ♪
[00:29:25.640 --> 00:29:30.640]   ♪ And still somehow it's clouds illusions I recall ♪
[00:29:31.200 --> 00:29:36.200]   ♪ I really don't know clouds at all ♪
[00:29:36.200 --> 00:29:37.680]   And then she goes on about love,
[00:29:37.680 --> 00:29:39.240]   how it's super, super happy,
[00:29:39.240 --> 00:29:41.160]   or it's about misery and loss,
[00:29:41.160 --> 00:29:44.240]   and about life, how it's about winning and losing,
[00:29:44.240 --> 00:29:45.440]   and so on and so forth.
[00:29:45.440 --> 00:29:49.240]   ♪ But now old friends are acting strange ♪
[00:29:49.240 --> 00:29:53.200]   ♪ They shake their heads, they say I've changed ♪
[00:29:53.200 --> 00:29:56.920]   ♪ Well, something's lost and something's gained ♪
[00:29:56.920 --> 00:29:59.520]   ♪ In living every day ♪
[00:29:59.520 --> 00:30:02.400]   So again, that's growing up and realizing that,
[00:30:02.400 --> 00:30:04.960]   well, the view that you had as a kid
[00:30:04.960 --> 00:30:07.200]   is not necessarily that you have as an adult.
[00:30:07.200 --> 00:30:09.920]   Remember my poem from when I was 16 years old
[00:30:09.920 --> 00:30:13.440]   of this whole, you know, children dance now while in row,
[00:30:13.440 --> 00:30:16.240]   and then in the end, even though the snow seems bright,
[00:30:16.240 --> 00:30:17.760]   without you have lost their light,
[00:30:17.760 --> 00:30:19.680]   song that sang and moon that smiled.
[00:30:19.680 --> 00:30:23.040]   So this whole concept of if you have love
[00:30:23.040 --> 00:30:24.600]   and if you have passion,
[00:30:24.600 --> 00:30:26.800]   you see the exact same thing from a different way.
[00:30:26.800 --> 00:30:28.560]   You can go out running in the rain,
[00:30:28.560 --> 00:30:29.840]   or you could just stay in and say,
[00:30:29.840 --> 00:30:32.560]   ah, sucks, I won't be able to go outside now.
[00:30:32.560 --> 00:30:34.400]   - Both sides.
[00:30:34.400 --> 00:30:36.240]   - Anyway, and the last one is,
[00:30:36.240 --> 00:30:38.440]   last, last one, I promise, Leonard Cohen.
[00:30:38.440 --> 00:30:39.800]   - This is amazing, by the way.
[00:30:39.800 --> 00:30:40.800]   (Lex laughing)
[00:30:40.800 --> 00:30:45.000]   I'm so glad we stumbled on how much joy you have
[00:30:45.000 --> 00:30:48.120]   in so many avenues of life,
[00:30:48.120 --> 00:30:49.560]   and music is just one of them.
[00:30:49.560 --> 00:30:52.200]   That's amazing, but yes, Leonard Cohen.
[00:30:52.200 --> 00:30:53.160]   - Going back to Leonard Cohen,
[00:30:53.160 --> 00:30:54.080]   since that's where you started,
[00:30:54.080 --> 00:30:56.360]   so Leonard Cohen's "Dance Me to the End of Love,"
[00:30:56.360 --> 00:30:59.480]   that was our opening song in our wedding with my wife.
[00:30:59.480 --> 00:31:00.320]   - Oh no, that's good.
[00:31:00.320 --> 00:31:01.760]   - As we came out to greet the guests,
[00:31:01.760 --> 00:31:03.400]   it was "Dance Me to the End of Love."
[00:31:03.400 --> 00:31:06.200]   And then another one, which is just so passionate always,
[00:31:06.200 --> 00:31:09.880]   and we always keep referring back to it, is "I'm Your Man."
[00:31:09.880 --> 00:31:12.120]   And it goes on and on about sort of,
[00:31:12.120 --> 00:31:14.720]   I can be every type of lover for you.
[00:31:14.720 --> 00:31:16.720]   And what's really beautiful in marriage
[00:31:16.720 --> 00:31:20.240]   is that we live that with my wife every day.
[00:31:20.240 --> 00:31:23.280]   You can have the passion, you can have the anger,
[00:31:23.280 --> 00:31:26.000]   you can have the love, you can have the tenderness.
[00:31:26.000 --> 00:31:28.400]   There's just so many gems in that song.
[00:31:28.400 --> 00:31:31.720]   If you want a partner, take my hand,
[00:31:31.720 --> 00:31:35.800]   or if you want to strike me down in anger,
[00:31:35.800 --> 00:31:40.040]   here I stand, I'm your man.
[00:31:40.040 --> 00:31:43.920]   Then if you want a boxer, I will step into the ring for you.
[00:31:43.920 --> 00:31:46.680]   If you want a driver, climb inside,
[00:31:46.680 --> 00:31:50.760]   or if you want to take me for a ride, you know you can.
[00:31:50.760 --> 00:31:54.120]   So this whole concept of you wanna drive, I'll follow.
[00:31:54.120 --> 00:31:55.840]   You want me to drive, I'll drive.
[00:31:55.840 --> 00:31:58.040]   - And the difference, I would say,
[00:31:58.040 --> 00:32:00.600]   between that and "Nemeketepa" is,
[00:32:00.600 --> 00:32:02.160]   this song, he's got an attitude,
[00:32:02.160 --> 00:32:07.920]   he's proud of his ability to basically be any kind of man
[00:32:07.920 --> 00:32:10.920]   for as long as he wants, as opposed to the Jacques Brel
[00:32:10.920 --> 00:32:15.040]   like desperation of, what do I have to be
[00:32:15.040 --> 00:32:17.760]   for you to love me, that kind of desperation.
[00:32:17.760 --> 00:32:20.440]   - But notice, there's a parallel here.
[00:32:20.440 --> 00:32:22.960]   There's a verse that is perhaps not paid attention
[00:32:22.960 --> 00:32:24.520]   to as much, which says,
[00:32:24.520 --> 00:32:29.320]   "Ah, but a man never got a woman back,
[00:32:29.320 --> 00:32:32.640]   not by begging on his knees."
[00:32:32.640 --> 00:32:34.760]   So it seems that the "I'm your man"
[00:32:34.760 --> 00:32:36.520]   is actually an apology song,
[00:32:36.520 --> 00:32:39.920]   in the same way that "Nemeketepa" is an apology song.
[00:32:39.920 --> 00:32:43.600]   "Nemeketepa" basically says, I've--
[00:32:43.600 --> 00:32:45.000]   - Screwed up. - I've screwed up.
[00:32:45.000 --> 00:32:45.920]   - I'm sorry, baby.
[00:32:45.920 --> 00:32:48.240]   - And in the same way that the "Careless Whisper"
[00:32:48.240 --> 00:32:49.120]   is "I'm Screwed Up."
[00:32:49.120 --> 00:32:50.440]   - Yes, that's right.
[00:32:50.440 --> 00:32:52.760]   ♪ I'm never gonna dance again ♪
[00:32:52.760 --> 00:32:57.240]   ♪ Guilty feet have got no rhythm ♪
[00:32:57.240 --> 00:33:00.080]   So this is an apology song, not by begging on his knees
[00:33:00.080 --> 00:33:03.520]   or I'd crawl to you, baby, and I'd fall at your feet
[00:33:03.520 --> 00:33:07.480]   and I'd howl at your beauty like a dog in heat
[00:33:07.480 --> 00:33:11.240]   and I'd claw at your heart and I'd tear at your sheet.
[00:33:11.240 --> 00:33:13.880]   I'd say, please.
[00:33:13.880 --> 00:33:17.160]   And then the last one is so beautiful.
[00:33:18.480 --> 00:33:22.320]   ♪ If you want a father for your child ♪
[00:33:22.320 --> 00:33:27.320]   ♪ Or only want to walk with me a while across the sand ♪
[00:33:27.320 --> 00:33:30.800]   ♪ I'm your man ♪
[00:33:30.800 --> 00:33:33.480]   That's the last verses, which basically says,
[00:33:33.480 --> 00:33:34.760]   you want me for a day?
[00:33:34.760 --> 00:33:37.040]   I'll be there.
[00:33:37.040 --> 00:33:38.040]   Do you want me to just walk?
[00:33:38.040 --> 00:33:38.880]   I'll be there.
[00:33:38.880 --> 00:33:40.200]   You want me for life?
[00:33:40.200 --> 00:33:42.600]   If you want a father for your child, I'll be there too.
[00:33:42.600 --> 00:33:43.840]   It's just so beautiful.
[00:33:43.840 --> 00:33:44.840]   Oh, sorry.
[00:33:44.840 --> 00:33:45.960]   Remember how I told you I was gonna finish
[00:33:45.960 --> 00:33:47.040]   with a lighthearted song?
[00:33:47.040 --> 00:33:47.880]   - Yes.
[00:33:47.880 --> 00:33:48.720]   - Last one.
[00:33:48.720 --> 00:33:49.560]   You ready?
[00:33:49.560 --> 00:33:52.920]   So, Alison Krauss and Union Station,
[00:33:52.920 --> 00:33:55.400]   country song, believe it or not, the lucky one.
[00:33:55.400 --> 00:34:00.760]   So, I've never identified as much
[00:34:00.760 --> 00:34:05.520]   with the lyrics of a song as this one.
[00:34:05.520 --> 00:34:06.400]   And it's hilarious.
[00:34:06.400 --> 00:34:08.600]   My friend, Seraphim Patoglou,
[00:34:08.600 --> 00:34:11.240]   is the guy who got me to genomics in the first place.
[00:34:11.240 --> 00:34:13.120]   I owe enormously to him.
[00:34:13.120 --> 00:34:14.280]   And he's another Greek.
[00:34:14.280 --> 00:34:15.760]   We actually met dancing, believe it or not.
[00:34:15.760 --> 00:34:18.200]   So, we used to perform Greek dances.
[00:34:18.200 --> 00:34:19.040]   I was the president
[00:34:19.040 --> 00:34:20.120]   of the International Students Association.
[00:34:20.120 --> 00:34:21.840]   So, we put on these big performances
[00:34:21.840 --> 00:34:23.720]   for 500 people at MIT.
[00:34:23.720 --> 00:34:26.040]   And there's a picture on the MIT Tech
[00:34:26.040 --> 00:34:27.840]   where Seraphim, who's like, you know,
[00:34:27.840 --> 00:34:30.240]   a bodybuilder, was holding me on his shoulder.
[00:34:30.240 --> 00:34:33.840]   And I was like doing maneuvers in the air, basically.
[00:34:33.840 --> 00:34:36.000]   So, anyway, this guy, Seraphim,
[00:34:36.000 --> 00:34:39.720]   we were driving back from a conference
[00:34:39.720 --> 00:34:41.280]   and there's this Russian girl
[00:34:41.280 --> 00:34:43.600]   who was describing how every member of her family
[00:34:43.600 --> 00:34:45.600]   had been either killed by the communists
[00:34:45.600 --> 00:34:48.000]   or killed by the Germans or killed by the...
[00:34:48.000 --> 00:34:50.840]   Like, she had just like, you know, misery,
[00:34:50.840 --> 00:34:54.440]   like death and, you know, sickness and everything.
[00:34:54.440 --> 00:34:55.840]   Everyone was decimated in her family.
[00:34:55.840 --> 00:34:57.480]   She was the last standing member.
[00:34:57.480 --> 00:35:00.040]   And we stop at a, Seraphim was driving,
[00:35:00.040 --> 00:35:02.160]   and we stop at a rest area.
[00:35:02.160 --> 00:35:03.920]   And he takes me aside and he's like,
[00:35:03.920 --> 00:35:05.640]   "Manolis, we're gonna crash."
[00:35:05.640 --> 00:35:07.480]   (both laughing)
[00:35:07.480 --> 00:35:08.640]   Get her out of my car.
[00:35:08.640 --> 00:35:10.640]   And then he basically says,
[00:35:11.200 --> 00:35:15.120]   "But I'm only reassured because you're here with me."
[00:35:15.120 --> 00:35:15.960]   And I'm like, "What do you mean?"
[00:35:15.960 --> 00:35:18.880]   He's like, "From your smile,
[00:35:18.880 --> 00:35:20.880]   I know you're the luckiest man on the planet."
[00:35:20.880 --> 00:35:22.400]   (both laughing)
[00:35:22.400 --> 00:35:24.240]   So, there's this really funny thing
[00:35:24.240 --> 00:35:28.600]   where I just feel freaking lucky all the time.
[00:35:28.600 --> 00:35:30.520]   And it's a question of attitude.
[00:35:30.520 --> 00:35:32.440]   Of course, I'm not any luckier than any other person,
[00:35:32.440 --> 00:35:35.040]   but every time something horrible happens to me,
[00:35:35.040 --> 00:35:37.080]   I'm like, and in fact, even in that song,
[00:35:37.080 --> 00:35:39.040]   the song about sort of, you know,
[00:35:39.040 --> 00:35:40.880]   walking on the beach and this, you know,
[00:35:40.880 --> 00:35:43.000]   sort of taking our life the wrong way,
[00:35:43.000 --> 00:35:45.320]   and then, you know, having to turn around,
[00:35:45.320 --> 00:35:47.160]   at some point he's like, you know,
[00:35:47.160 --> 00:35:50.240]   "In the fresh sand we wrote her name,
[00:35:50.240 --> 00:35:53.760]   (speaking in foreign language)
[00:35:53.760 --> 00:35:57.760]   how nicely that the wind blew and the writing was erased."
[00:35:57.760 --> 00:36:00.960]   So again, it's this whole sort of,
[00:36:00.960 --> 00:36:03.960]   not just saying, "Oh, bummer,"
[00:36:03.960 --> 00:36:07.040]   but, "Oh, great, I just lost this.
[00:36:07.040 --> 00:36:08.840]   This must mean something."
[00:36:08.840 --> 00:36:09.680]   (both laughing)
[00:36:09.680 --> 00:36:10.520]   Right?
[00:36:10.520 --> 00:36:11.760]   - This horrible thing happened,
[00:36:11.760 --> 00:36:15.280]   it must open the door to a beautiful chapter.
[00:36:15.280 --> 00:36:18.320]   - So, Alison Krauss is talking about "The Lucky One."
[00:36:18.320 --> 00:36:20.560]   So, it's like, "Oh my God, she wrote a song for me."
[00:36:20.560 --> 00:36:21.880]   (both laughing)
[00:36:21.880 --> 00:36:22.720]   And she goes,
[00:36:22.720 --> 00:36:25.320]   ♪ You're the lucky one, I know that now ♪
[00:36:25.320 --> 00:36:27.880]   ♪ As free as the wind blowing down the road ♪
[00:36:27.880 --> 00:36:30.200]   ♪ Loved by many, hated by none ♪
[00:36:30.200 --> 00:36:32.680]   ♪ I'd say you were lucky 'cause you know what you've done ♪
[00:36:32.680 --> 00:36:36.000]   ♪ Not the care in the world, not the worry inside ♪
[00:36:36.000 --> 00:36:37.720]   ♪ Everything's gonna be all right ♪
[00:36:37.720 --> 00:36:39.720]   ♪ 'Cause you're the lucky one ♪
[00:36:39.720 --> 00:36:41.280]   And then she goes,
[00:36:41.280 --> 00:36:43.480]   ♪ You're the lucky one, always having fun ♪
[00:36:43.480 --> 00:36:45.920]   ♪ A jack of all trades, a master of none ♪
[00:36:45.920 --> 00:36:48.160]   ♪ You look at the world with the smiling eyes ♪
[00:36:48.160 --> 00:36:50.960]   ♪ And laugh at the devil as his train rolls by ♪
[00:36:50.960 --> 00:36:53.720]   ♪ I'll give you a song and a one-night stand ♪
[00:36:53.720 --> 00:36:55.640]   ♪ You'll be looking at a happy man ♪
[00:36:55.640 --> 00:36:57.240]   ♪ 'Cause you're the lucky one ♪
[00:36:57.240 --> 00:36:59.000]   It basically says,
[00:36:59.000 --> 00:37:01.440]   if you just don't worry too much,
[00:37:01.440 --> 00:37:04.480]   if you don't try to be, you know,
[00:37:04.480 --> 00:37:06.760]   a one-trick pony,
[00:37:06.760 --> 00:37:09.120]   if you just embrace the fact
[00:37:09.120 --> 00:37:10.840]   that you might suck at a bunch of things,
[00:37:10.840 --> 00:37:12.720]   but you're just gonna try a lot of things.
[00:37:12.720 --> 00:37:15.160]   And then there's another verse that says,
[00:37:15.160 --> 00:37:17.280]   ♪ Well, you're blessed, I guess ♪
[00:37:17.280 --> 00:37:20.120]   ♪ But never knowing which road you're choosing ♪
[00:37:20.120 --> 00:37:22.320]   ♪ To you, the next best thing ♪
[00:37:22.320 --> 00:37:25.320]   ♪ To playing and winning is playing and losing ♪
[00:37:25.320 --> 00:37:27.320]   It's just so beautiful
[00:37:27.320 --> 00:37:29.040]   because it basically says,
[00:37:29.040 --> 00:37:30.840]   if you try
[00:37:30.840 --> 00:37:34.040]   your best,
[00:37:34.040 --> 00:37:36.360]   but it's still playing,
[00:37:36.360 --> 00:37:38.760]   if you lose, it's okay, you had an awesome game.
[00:37:38.760 --> 00:37:42.120]   And again, superficially,
[00:37:42.120 --> 00:37:43.800]   it sounds like a super happy song,
[00:37:43.800 --> 00:37:46.960]   but then there's the last verse basically says,
[00:37:46.960 --> 00:37:48.760]   no matter where you're at, that's where you'll be,
[00:37:48.760 --> 00:37:51.480]   you can bet your luck won't follow me.
[00:37:51.480 --> 00:37:53.880]   Just give you a song and a one-night stand,
[00:37:53.880 --> 00:37:55.640]   you'll be looking at a happy man.
[00:37:55.640 --> 00:37:57.040]   And in the video of the song,
[00:37:57.040 --> 00:37:58.600]   she just walks away or he just walks away
[00:37:58.600 --> 00:37:59.960]   or something like that.
[00:37:59.960 --> 00:38:04.360]   And it basically tells you that freedom comes at a price.
[00:38:04.360 --> 00:38:06.480]   Freedom comes at the price of non-commitment.
[00:38:06.480 --> 00:38:09.120]   This whole sort of birds who love or birds who cry,
[00:38:09.120 --> 00:38:12.360]   you can't really love unless you cry.
[00:38:12.360 --> 00:38:15.720]   You can't just be the lucky one, the happy boy, la la la,
[00:38:15.720 --> 00:38:18.960]   and yet have a long-term relationship.
[00:38:18.960 --> 00:38:21.000]   So it's, on one hand,
[00:38:21.000 --> 00:38:23.120]   I identify with the shallowness of the song
[00:38:23.120 --> 00:38:24.880]   of you're the lucky one,
[00:38:24.880 --> 00:38:27.400]   jack of all trades, a master of none.
[00:38:27.400 --> 00:38:30.600]   But at the same time, I identify with a lesson of,
[00:38:30.600 --> 00:38:34.400]   well, you can't just be the happy merry-go-lucky all the time.
[00:38:34.400 --> 00:38:36.440]   Sometimes you have to embrace loss
[00:38:36.440 --> 00:38:38.360]   and sometimes you have to embrace suffering.
[00:38:38.360 --> 00:38:40.520]   And sometimes you have to embrace that.
[00:38:40.520 --> 00:38:43.120]   If you have a safety net,
[00:38:43.120 --> 00:38:45.520]   you're not really committing enough.
[00:38:45.520 --> 00:38:46.520]   You're not, you know,
[00:38:46.520 --> 00:38:49.040]   basically you're allowing yourself to slip.
[00:38:49.040 --> 00:38:53.160]   But if you just go all in and you just, you know,
[00:38:53.160 --> 00:38:54.760]   let go of your reservations,
[00:38:54.760 --> 00:38:56.800]   that's when you truly will get somewhere.
[00:38:56.800 --> 00:38:59.680]   So anyway, that's like the,
[00:38:59.680 --> 00:39:02.920]   I managed to narrow it down to what, 15 songs?
[00:39:02.920 --> 00:39:05.040]   - Thank you for that wonderful journey
[00:39:05.040 --> 00:39:06.280]   that you just took us on.
[00:39:06.280 --> 00:39:12.080]   The darkest possible places of Greek song
[00:39:12.080 --> 00:39:14.880]   to ending on this, a country song.
[00:39:14.880 --> 00:39:18.120]   I haven't heard it before, but that's exactly right.
[00:39:18.120 --> 00:39:21.440]   I feel the same way, depending on the day.
[00:39:21.440 --> 00:39:24.280]   Is this the luckiest human on earth?
[00:39:24.280 --> 00:39:26.920]   And there's something to that, but you're right.
[00:39:26.920 --> 00:39:31.920]   It needs to be, we need to now return to the muck of life
[00:39:31.920 --> 00:39:37.160]   in order to be able to truly enjoy it.
[00:39:37.160 --> 00:39:38.560]   - What do you mean muck?
[00:39:38.560 --> 00:39:39.960]   What's muck?
[00:39:39.960 --> 00:39:41.320]   - The messiness of life.
[00:39:41.320 --> 00:39:43.440]   The things that were,
[00:39:43.440 --> 00:39:47.040]   things don't turn out the way you expect it to.
[00:39:47.040 --> 00:39:49.760]   So like to feel lucky is like focusing
[00:39:49.760 --> 00:39:52.240]   on the beautiful consequences.
[00:39:52.240 --> 00:39:55.640]   But then that feeling of things being different
[00:39:55.640 --> 00:39:59.760]   than you expected, that you stumble in all the kinds of ways
[00:39:59.760 --> 00:40:03.320]   that seems to be, needs to be paired with the feeling.
[00:40:03.320 --> 00:40:04.760]   - There's basically one way.
[00:40:04.760 --> 00:40:07.880]   The only way not to make mistakes is to never do anything.
[00:40:07.880 --> 00:40:08.720]   - Right.
[00:40:08.720 --> 00:40:11.880]   - Basically, you have to embrace the fact
[00:40:11.880 --> 00:40:13.520]   that you'll be wrong so many times.
[00:40:13.520 --> 00:40:15.480]   In so many research meetings,
[00:40:15.480 --> 00:40:17.640]   I just go off on a tangent and I say,
[00:40:17.640 --> 00:40:19.480]   let's think about this for a second.
[00:40:19.480 --> 00:40:23.040]   And it's just crazy for me, who's a computer scientist,
[00:40:23.040 --> 00:40:24.760]   to just tell my biologist friends,
[00:40:24.760 --> 00:40:27.440]   what if biology kind of worked this way?
[00:40:27.440 --> 00:40:30.280]   And they humor me, they just let me talk.
[00:40:30.280 --> 00:40:34.840]   And rarely has it not gone somewhere good.
[00:40:34.840 --> 00:40:36.280]   It's not that I'm always right,
[00:40:36.280 --> 00:40:39.840]   but it's always something worth exploring further.
[00:40:39.840 --> 00:40:44.000]   That if you're an outsider with humility
[00:40:44.000 --> 00:40:47.360]   and knowing that I'll be wrong a bunch of times,
[00:40:47.360 --> 00:40:49.760]   but I'll challenge your assumptions,
[00:40:49.760 --> 00:40:53.560]   and often take us to a better place,
[00:40:53.560 --> 00:40:56.160]   is part of this whole sort of messiness of life.
[00:40:56.160 --> 00:40:59.320]   Like if you don't try and lose and get hurt
[00:40:59.320 --> 00:41:02.480]   and suffer and cry and just break your heart
[00:41:02.480 --> 00:41:05.160]   and all these feelings of guilt and,
[00:41:05.160 --> 00:41:06.520]   wow, I did the wrong thing.
[00:41:06.520 --> 00:41:09.080]   Of course, that's part of life.
[00:41:09.080 --> 00:41:10.640]   And that's just something that,
[00:41:10.640 --> 00:41:15.400]   you know, if you are a doer, you'll make mistakes.
[00:41:15.400 --> 00:41:17.600]   If you're a criticizer, yeah, sure,
[00:41:17.600 --> 00:41:19.840]   you can sit back and criticize everybody else
[00:41:19.840 --> 00:41:21.480]   for the mistakes they make.
[00:41:21.480 --> 00:41:22.920]   Or instead, you can just be out there
[00:41:22.920 --> 00:41:24.320]   making those mistakes.
[00:41:24.320 --> 00:41:26.280]   And frankly, I'd rather be the criticized one
[00:41:26.280 --> 00:41:27.840]   than the criticized one.
[00:41:27.840 --> 00:41:28.960]   - Brilliantly put.
[00:41:28.960 --> 00:41:30.840]   - Every time somebody steals my bicycle, I say,
[00:41:30.840 --> 00:41:32.480]   well, no, my son's like,
[00:41:32.480 --> 00:41:34.400]   "Why do they steal our bicycle, dad?"
[00:41:34.400 --> 00:41:37.320]   And I'm like, "Aren't you happy that you have bicycles
[00:41:37.320 --> 00:41:39.200]   "that people can steal?"
[00:41:39.200 --> 00:41:41.720]   I'd rather be the person stolen from than the stealer.
[00:41:41.720 --> 00:41:42.560]   - Yeah, yeah.
[00:41:42.560 --> 00:41:43.400]   (Lex laughing)
[00:41:43.400 --> 00:41:45.160]   It's not the critic that counts.
[00:41:45.160 --> 00:41:49.840]   So that's, we've just talked amazingly about life
[00:41:49.840 --> 00:41:51.680]   from the music perspective.
[00:41:51.680 --> 00:41:55.120]   Let's talk about life, human life,
[00:41:55.120 --> 00:41:57.440]   from perhaps other perspective and its meaning.
[00:41:57.440 --> 00:42:00.760]   So this is episode 142.
[00:42:00.760 --> 00:42:07.040]   There is perhaps an absurdly deep meaning
[00:42:07.040 --> 00:42:13.680]   to the number 42 that our culture has elevated.
[00:42:13.680 --> 00:42:16.760]   So this is a perfect time to talk about the meaning of life.
[00:42:16.760 --> 00:42:18.400]   We've talked about it already,
[00:42:18.400 --> 00:42:22.840]   but do you think this question that's so simple
[00:42:22.840 --> 00:42:27.560]   and so seemingly absurd has value
[00:42:27.560 --> 00:42:29.160]   of what is the meaning of life?
[00:42:29.160 --> 00:42:33.280]   Is it something that raising the question
[00:42:33.280 --> 00:42:36.520]   and trying to answer it, is that a ridiculous pursuit
[00:42:36.520 --> 00:42:37.560]   or is there some value?
[00:42:37.560 --> 00:42:39.360]   Is it answerable at all?
[00:42:39.360 --> 00:42:42.560]   - So first of all, I feel that we owe it to your listeners
[00:42:42.560 --> 00:42:44.000]   to say why 42.
[00:42:44.000 --> 00:42:44.840]   - Sure.
[00:42:44.840 --> 00:42:46.040]   (Lex laughing)
[00:42:46.040 --> 00:42:48.360]   - So of course, the Hitchhiker's Guide to the Galaxy
[00:42:48.360 --> 00:42:52.200]   came up with 42 as basically a random number.
[00:42:52.200 --> 00:42:55.480]   Just, you know, the author just pulled it out of a hat
[00:42:55.480 --> 00:42:56.680]   and he's admitted so.
[00:42:56.680 --> 00:42:59.000]   He said, "Well, 42 just seemed like
[00:42:59.000 --> 00:43:00.400]   "just random numbers, any."
[00:43:00.400 --> 00:43:05.480]   But in fact, there's many numbers that are linked to 42.
[00:43:05.480 --> 00:43:09.320]   So 42, again, just to summarize,
[00:43:09.320 --> 00:43:13.860]   is the answer that these super mega computer
[00:43:13.860 --> 00:43:15.960]   that had computed for a million years
[00:43:15.960 --> 00:43:17.800]   with the most powerful computer in the world
[00:43:17.800 --> 00:43:18.840]   had come up with.
[00:43:18.840 --> 00:43:23.360]   At some point, the computer says, "I have an answer."
[00:43:23.360 --> 00:43:25.320]   And they're like, "What?"
[00:43:25.320 --> 00:43:27.160]   It's like, "You're not gonna like it."
[00:43:27.160 --> 00:43:28.200]   Like, "What is it?"
[00:43:28.200 --> 00:43:29.040]   "It's 42."
[00:43:29.040 --> 00:43:30.440]   (both laughing)
[00:43:30.440 --> 00:43:33.040]   And then the irony is that they had forgotten, of course,
[00:43:33.040 --> 00:43:33.880]   what the question was.
[00:43:33.880 --> 00:43:34.720]   - Yes.
[00:43:34.720 --> 00:43:35.540]   (both laughing)
[00:43:35.540 --> 00:43:36.380]   - So now they have to build bigger computers
[00:43:36.380 --> 00:43:37.220]   to figure out what the question is.
[00:43:37.220 --> 00:43:38.060]   - What the question is.
[00:43:38.060 --> 00:43:39.380]   - To which the answer is 42.
[00:43:39.380 --> 00:43:43.760]   So as I was turning 42, I basically sort of researched
[00:43:43.760 --> 00:43:45.720]   why 42 is such a cool number.
[00:43:45.720 --> 00:43:48.240]   And it turns out that, and I put together
[00:43:48.240 --> 00:43:49.560]   this little passage that was explaining
[00:43:49.560 --> 00:43:53.080]   to all those guests to my 42nd birthday party
[00:43:53.080 --> 00:43:54.880]   why we were talking about the meaning of life.
[00:43:54.880 --> 00:43:59.880]   And I basically talked about how 42 is the angle
[00:43:59.880 --> 00:44:04.260]   at which light reflects off of water to create a rainbow.
[00:44:04.260 --> 00:44:07.760]   And it's so beautiful because the rainbow
[00:44:07.760 --> 00:44:09.880]   is basically the combination of sort of,
[00:44:09.880 --> 00:44:11.760]   it's been raining, but there's hope
[00:44:11.760 --> 00:44:13.280]   'cause the sun just came out.
[00:44:13.280 --> 00:44:14.880]   So it's a very beautiful number there.
[00:44:14.880 --> 00:44:17.760]   So 42 is also the sum of all rows and columns
[00:44:17.760 --> 00:44:21.360]   of a magic cube that contains all consecutive integers
[00:44:21.360 --> 00:44:22.720]   starting at one.
[00:44:22.720 --> 00:44:24.640]   So basically if you take all integers
[00:44:24.640 --> 00:44:27.360]   between one and however many vertices there are,
[00:44:27.360 --> 00:44:28.880]   the sums is always 42.
[00:44:28.880 --> 00:44:34.240]   42 is the only number left under 100
[00:44:34.240 --> 00:44:36.540]   for which the equation of X to the cube
[00:44:36.540 --> 00:44:39.760]   plus Y to the cube plus Z to the cube is N.
[00:44:39.760 --> 00:44:42.000]   And was not known to not have a solution.
[00:44:42.000 --> 00:44:46.160]   And now it's the only one that actually has a solution.
[00:44:46.160 --> 00:44:50.240]   42 is also one, zero, one, zero, one, zero in binary.
[00:44:50.240 --> 00:44:52.700]   Again, the yin and the yang, the good and the evil,
[00:44:52.700 --> 00:44:54.960]   one and zero, the balance of the fours.
[00:44:54.960 --> 00:44:58.400]   42 is the number of chromosomes for the giant panda.
[00:44:58.400 --> 00:45:00.080]   The giant panda. (laughs)
[00:45:00.080 --> 00:45:01.920]   No, it's totally random.
[00:45:01.920 --> 00:45:03.160]   - Or is it? - It's a vicious symbol
[00:45:03.160 --> 00:45:05.600]   of great strength coupled with peace,
[00:45:05.600 --> 00:45:07.480]   friendship, gentle temperament,
[00:45:07.480 --> 00:45:09.520]   harmony, balance, and friendship,
[00:45:09.520 --> 00:45:11.320]   whose black and white colors, again,
[00:45:11.320 --> 00:45:12.560]   symbolize yin and yang.
[00:45:12.560 --> 00:45:15.040]   The reason why it's the symbol for China
[00:45:15.040 --> 00:45:19.840]   is exactly the strength but yet peace, and so on and so forth.
[00:45:19.840 --> 00:45:21.680]   So 42 chromosomes.
[00:45:21.680 --> 00:45:24.960]   It takes light 10 to the minus 42 seconds
[00:45:24.960 --> 00:45:27.880]   to cross the diameter of a proton
[00:45:27.880 --> 00:45:29.560]   connecting the two fundamental dimensions
[00:45:29.560 --> 00:45:31.160]   of space and time.
[00:45:31.160 --> 00:45:33.520]   42 is the number of times a piece of paper
[00:45:33.520 --> 00:45:37.800]   should be folded to reach beyond the moon. (laughs)
[00:45:37.800 --> 00:45:41.960]   So, which is what I assume my students mean
[00:45:41.960 --> 00:45:44.560]   when they ask that their paper reaches for the stars.
[00:45:44.560 --> 00:45:46.120]   I just tell them just fold it a bunch of times.
[00:45:46.120 --> 00:45:48.920]   (both laugh)
[00:45:48.920 --> 00:45:53.920]   42 is the number of Messier object 42, which is Orion.
[00:45:53.920 --> 00:45:58.520]   And that's one of the most famous galaxies.
[00:45:58.520 --> 00:46:00.800]   It's, I think, also the place where we can actually see
[00:46:00.800 --> 00:46:03.000]   the center of our galaxy.
[00:46:03.000 --> 00:46:04.560]   42 is the numeric representation
[00:46:04.560 --> 00:46:06.480]   of the star symbol in ASCII.
[00:46:07.360 --> 00:46:08.280]   (Lex laughs)
[00:46:08.280 --> 00:46:10.880]   Which is very useful when searching for the stars.
[00:46:10.880 --> 00:46:14.240]   And also a reg exp for life, the universe, and everything.
[00:46:14.240 --> 00:46:17.160]   So star. (both laugh)
[00:46:17.160 --> 00:46:20.120]   In Egyptian mythology, the goddess Maat,
[00:46:20.120 --> 00:46:22.320]   which was personifying truth and justice,
[00:46:22.320 --> 00:46:25.600]   would ask 42 questions to every dying person,
[00:46:25.600 --> 00:46:27.960]   and those answering successfully would become stars,
[00:46:27.960 --> 00:46:30.440]   continue to give life and fuel universal growth.
[00:46:30.440 --> 00:46:35.720]   In Judaic tradition, God ascribed the 42-lettered name
[00:46:35.720 --> 00:46:38.080]   and trusted only to the middle-aged, pious, meek,
[00:46:38.080 --> 00:46:42.120]   free from bad temper, sober, and not insistent on his rights.
[00:46:42.120 --> 00:46:46.480]   And in Christian tradition, there's 42 generations
[00:46:46.480 --> 00:46:49.320]   from Abraham, Isaac, that we talked about,
[00:46:49.320 --> 00:46:51.240]   the story of Isaac, Jacob,
[00:46:51.240 --> 00:46:53.680]   eventually Joseph, Mary, and Jesus.
[00:46:53.680 --> 00:46:56.880]   In Qabbalistic tradition, Eloqa, which is 42,
[00:46:56.880 --> 00:47:00.000]   is the number with which God creates the universe,
[00:47:00.000 --> 00:47:04.760]   starting with 25, let there be, and ending with 17, good.
[00:47:04.760 --> 00:47:07.800]   So 25 plus, you know, 17.
[00:47:07.800 --> 00:47:10.600]   There's the 42-chapter sutra,
[00:47:10.600 --> 00:47:12.840]   which is the first Indian religious scripture,
[00:47:12.840 --> 00:47:14.640]   which was translated to Chinese,
[00:47:14.640 --> 00:47:18.800]   thus introducing Buddhism to China from India.
[00:47:18.800 --> 00:47:21.920]   The 42-line Bible was the first printed book
[00:47:21.920 --> 00:47:25.760]   marking the age of printing in the 1450s
[00:47:25.760 --> 00:47:27.240]   and the dissemination of knowledge
[00:47:27.240 --> 00:47:29.560]   eventually leading to the Enlightenment.
[00:47:29.560 --> 00:47:33.240]   A yeast cell, which is called a single-cell eukaryote
[00:47:33.240 --> 00:47:34.800]   and the subject of my PhD research
[00:47:34.800 --> 00:47:38.080]   has exactly 42 million proteins.
[00:47:38.080 --> 00:47:39.920]   Anyway, so there's a series of 42.
[00:47:39.920 --> 00:47:41.360]   - You're on fire with this.
[00:47:41.360 --> 00:47:42.920]   These are really good.
[00:47:42.920 --> 00:47:45.680]   So I guess what you're saying is just a random number.
[00:47:45.680 --> 00:47:47.320]   - Yeah, basically.
[00:47:47.320 --> 00:47:49.200]   So all of these are background names.
[00:47:49.200 --> 00:47:50.480]   So, you know, after you have the number,
[00:47:50.480 --> 00:47:52.160]   you figure out why that number.
[00:47:52.160 --> 00:47:56.400]   So anyway, so now that we've spoken about why 42,
[00:47:56.400 --> 00:47:58.280]   why do we search for meaning?
[00:47:58.280 --> 00:48:00.160]   And you're asking, you know,
[00:48:00.160 --> 00:48:02.600]   will that search ultimately lead to our destruction?
[00:48:02.600 --> 00:48:04.880]   And my thinking is exactly the opposite.
[00:48:04.880 --> 00:48:08.560]   So basically that asking about meaning
[00:48:08.560 --> 00:48:11.520]   is something that's so inherent to human nature.
[00:48:11.520 --> 00:48:13.480]   It's something that makes life beautiful
[00:48:13.480 --> 00:48:15.000]   that makes it worth living.
[00:48:15.000 --> 00:48:18.520]   And that searching for meaning is actually the point.
[00:48:18.520 --> 00:48:19.720]   It's not the finding it.
[00:48:19.720 --> 00:48:21.680]   I think when you found it, you're dead.
[00:48:21.680 --> 00:48:26.560]   Don't ever be satisfied that, you know, I've got it.
[00:48:26.560 --> 00:48:30.120]   So I like to say that life is lived forward,
[00:48:30.120 --> 00:48:32.320]   but it only makes sense backward.
[00:48:32.320 --> 00:48:34.480]   And I don't remember whose quote that is,
[00:48:34.480 --> 00:48:39.480]   but the whole search itself is the meaning.
[00:48:39.480 --> 00:48:41.880]   And what I love about it is that
[00:48:41.880 --> 00:48:45.000]   there's a double search going on.
[00:48:45.000 --> 00:48:47.680]   There's a search in every one of us
[00:48:47.680 --> 00:48:50.760]   through our own lives to find meaning.
[00:48:50.760 --> 00:48:52.200]   And then there's a search,
[00:48:52.200 --> 00:48:55.960]   which is happening for humanity itself to find our meaning.
[00:48:55.960 --> 00:49:01.600]   And we as humans like to look at animals and say,
[00:49:01.600 --> 00:49:03.160]   of course they have a meaning.
[00:49:03.160 --> 00:49:05.000]   Like a dog has its meaning.
[00:49:05.000 --> 00:49:07.040]   It's just a bunch of instincts, you know,
[00:49:07.040 --> 00:49:09.440]   running around, loving everything.
[00:49:09.440 --> 00:49:11.760]   You know, remember our joke with the cat and the dog.
[00:49:11.760 --> 00:49:13.440]   - Yeah, cat has no meaning.
[00:49:13.440 --> 00:49:14.360]   (laughing)
[00:49:14.360 --> 00:49:15.200]   - No, no.
[00:49:15.200 --> 00:49:20.640]   And I'm noticing the yin yang symbol right here
[00:49:20.640 --> 00:49:23.400]   with this whole panda, black and white and the zero one zero.
[00:49:23.400 --> 00:49:25.000]   - You're on fire with that 42.
[00:49:25.000 --> 00:49:29.560]   Some of those are gold ASCII value for a star symbol.
[00:49:29.560 --> 00:49:30.400]   Damn.
[00:49:30.400 --> 00:49:31.440]   - I love it.
[00:49:31.440 --> 00:49:32.920]   So basically in my view,
[00:49:32.920 --> 00:49:37.160]   the search for meaning and the act of searching
[00:49:37.160 --> 00:49:42.160]   for something more meaningful is life's meaning by itself.
[00:49:42.160 --> 00:49:45.960]   The fact that we kind of always hope that yes,
[00:49:45.960 --> 00:49:48.040]   maybe for animals that's not the case,
[00:49:48.040 --> 00:49:51.280]   but maybe humans have something that we should be doing
[00:49:51.280 --> 00:49:52.120]   and something else.
[00:49:52.120 --> 00:49:53.560]   And it's not just about procreation.
[00:49:53.560 --> 00:49:55.480]   It's not just about dominance.
[00:49:55.480 --> 00:49:58.400]   It's not just about strength and feeding, et cetera.
[00:49:58.400 --> 00:50:01.080]   Like we're the one species that spends such a tiny little
[00:50:01.080 --> 00:50:05.400]   minority of its time feeding that we have this enormous,
[00:50:05.400 --> 00:50:08.840]   you know, huge cognitive capability
[00:50:08.840 --> 00:50:11.160]   that we can just use for all kinds of other stuff.
[00:50:11.160 --> 00:50:12.960]   And that's where art comes in.
[00:50:12.960 --> 00:50:16.440]   That's where, you know, the healthy mind comes in with,
[00:50:16.440 --> 00:50:18.360]   you know, exploring all of these different aspects
[00:50:18.360 --> 00:50:23.360]   that are just not directly tied to a purpose.
[00:50:23.360 --> 00:50:25.880]   That's not directly tied to a function.
[00:50:25.880 --> 00:50:28.600]   It's really just the playing of life.
[00:50:28.600 --> 00:50:32.440]   The, you know, not for a particular reason.
[00:50:32.440 --> 00:50:34.320]   - Do you think this thing we got,
[00:50:34.320 --> 00:50:38.200]   this mind is unique in the universe
[00:50:38.200 --> 00:50:42.320]   in terms of how difficult it is to build?
[00:50:42.320 --> 00:50:43.400]   - So--
[00:50:43.400 --> 00:50:47.560]   - Is it possible that we're the most beautiful thing
[00:50:47.560 --> 00:50:49.920]   that the universe has constructed?
[00:50:49.920 --> 00:50:51.200]   - Both the most beautiful and the most ugly,
[00:50:51.200 --> 00:50:52.560]   but certainly the most complex.
[00:50:52.560 --> 00:50:55.120]   So look at evolutionary time.
[00:50:55.120 --> 00:50:58.920]   The dinosaurs ruled the earth for 135 million years.
[00:50:58.920 --> 00:51:00.960]   We've been around for a million years.
[00:51:00.960 --> 00:51:05.840]   So one versus 135.
[00:51:05.840 --> 00:51:07.760]   So dinosaurs were extinct, you know,
[00:51:07.760 --> 00:51:09.120]   about 60 million years ago,
[00:51:09.120 --> 00:51:11.040]   and mammals that had been happily evolving
[00:51:11.040 --> 00:51:13.320]   as tiny little creatures for 30 million years,
[00:51:13.320 --> 00:51:14.720]   then took over the planet.
[00:51:14.720 --> 00:51:16.680]   And then, you know, dramatically radiated
[00:51:16.680 --> 00:51:18.040]   about 60 million years ago.
[00:51:18.040 --> 00:51:24.040]   Out of these mammals came the neocortex formation.
[00:51:24.040 --> 00:51:25.600]   So basically the neocortex,
[00:51:25.600 --> 00:51:28.080]   which is sort of the outer layer of our brain
[00:51:28.080 --> 00:51:29.880]   compared to our quote unquote reptilian brain,
[00:51:29.880 --> 00:51:33.200]   which we share the structure of with all of the dinosaurs,
[00:51:33.200 --> 00:51:36.040]   they didn't have that, and yet they ruled the planet.
[00:51:36.040 --> 00:51:38.760]   So how many other planets have still, you know,
[00:51:38.760 --> 00:51:42.800]   mindless dinosaurs where strength was the only dimension
[00:51:42.800 --> 00:51:45.480]   ruling the planet?
[00:51:45.480 --> 00:51:49.400]   So there was something weird that annihilated the dinosaurs.
[00:51:49.400 --> 00:51:51.080]   And again, you could look at biblical things
[00:51:51.080 --> 00:51:53.360]   of sort of God coming and wiping out his creatures
[00:51:53.360 --> 00:51:55.080]   and to make room for the next ones.
[00:51:55.080 --> 00:51:59.120]   So the mammals basically sort of took over the planet
[00:51:59.120 --> 00:52:03.200]   and then grew this cognitive capability
[00:52:03.200 --> 00:52:06.240]   of this general purpose machine.
[00:52:06.240 --> 00:52:10.560]   And primates pushed that to extreme,
[00:52:10.560 --> 00:52:15.520]   and humans among primates have just exploded that hardware.
[00:52:15.520 --> 00:52:20.520]   But that hardware is selected for survival.
[00:52:22.240 --> 00:52:24.200]   It's selected for procreation.
[00:52:24.200 --> 00:52:28.160]   It's initially selected with this very simple
[00:52:28.160 --> 00:52:32.400]   Darwinian view of the world of random mutation,
[00:52:32.400 --> 00:52:33.800]   ruthless selection,
[00:52:33.800 --> 00:52:36.200]   and then selection for making more of yourself.
[00:52:36.200 --> 00:52:40.880]   If you look at human cognition,
[00:52:40.880 --> 00:52:43.760]   it's gone down a weird evolutionary path
[00:52:43.760 --> 00:52:46.720]   in the sense that we are expending
[00:52:46.720 --> 00:52:49.800]   an enormous amount of energy on this apparatus
[00:52:49.800 --> 00:52:52.680]   between our ears that is wasting, what,
[00:52:52.680 --> 00:52:55.680]   15% of our bodily energy, 20%,
[00:52:55.680 --> 00:52:59.760]   like some enormous percentage of our calories
[00:52:59.760 --> 00:53:01.600]   go to function our brain.
[00:53:01.600 --> 00:53:05.960]   No other species makes that big of a commitment.
[00:53:05.960 --> 00:53:08.840]   That has basically taken energetic changes
[00:53:08.840 --> 00:53:13.240]   for efficiency on the metabolic side for humanity
[00:53:13.240 --> 00:53:15.400]   to basically power that thing.
[00:53:15.400 --> 00:53:18.680]   And our brain is both enormously more efficient
[00:53:18.680 --> 00:53:22.120]   than other brains, but also, despite this efficiency,
[00:53:22.120 --> 00:53:23.920]   enormously more energy consuming.
[00:53:23.920 --> 00:53:28.600]   So, and if you look at just the sheer folds
[00:53:28.600 --> 00:53:30.400]   that the human brain has,
[00:53:30.400 --> 00:53:33.280]   again, our skull could only grow so much
[00:53:33.280 --> 00:53:36.400]   before it could no longer go through the pelvic opening
[00:53:36.400 --> 00:53:39.480]   and kill the mother at every birth.
[00:53:39.480 --> 00:53:42.200]   So, but yet the folds continued,
[00:53:42.200 --> 00:53:45.380]   effectively creating just so much more capacity.
[00:53:46.640 --> 00:53:51.560]   The evolutionary context in which this was made
[00:53:51.560 --> 00:53:53.480]   is enormously fascinating.
[00:53:53.480 --> 00:53:57.240]   And it has to do with other humans
[00:53:57.240 --> 00:54:00.440]   that we have now killed off or that have gone extinct.
[00:54:00.440 --> 00:54:04.280]   And that has now created this weird place of humans
[00:54:04.280 --> 00:54:06.520]   on the planet as the only species
[00:54:06.520 --> 00:54:08.800]   that has this enormous hardware.
[00:54:08.800 --> 00:54:10.400]   So, that can basically make us think
[00:54:10.400 --> 00:54:12.120]   that there's something very weird and unique
[00:54:12.120 --> 00:54:13.280]   that happened in human evolution
[00:54:13.280 --> 00:54:15.400]   that perhaps has not been recreated elsewhere.
[00:54:15.400 --> 00:54:18.600]   Maybe the asteroid didn't hit sister earth
[00:54:18.600 --> 00:54:20.720]   and dinosaurs are still ruling.
[00:54:20.720 --> 00:54:23.760]   And any kind of proto-human is squished
[00:54:23.760 --> 00:54:25.560]   and eaten for breakfast, basically.
[00:54:25.560 --> 00:54:31.280]   However, we're not as unique as we like to think
[00:54:31.280 --> 00:54:33.400]   because there was this enormous diversity
[00:54:33.400 --> 00:54:35.800]   of other human-like forms.
[00:54:35.800 --> 00:54:37.800]   And once you make it to that stage
[00:54:37.800 --> 00:54:40.560]   where you have a neocortex-like explosion of,
[00:54:40.560 --> 00:54:42.920]   wow, we're now competing on intelligence
[00:54:42.920 --> 00:54:44.720]   and we're now competing on social structures
[00:54:44.720 --> 00:54:48.800]   and we're now competing on larger and larger groups
[00:54:48.800 --> 00:54:53.800]   and being able to coordinate and being able to have empathy.
[00:54:53.800 --> 00:54:57.800]   The concept of empathy, the concept of an ego,
[00:54:57.800 --> 00:55:01.240]   the concept of a self, of self-awareness
[00:55:01.240 --> 00:55:06.240]   comes probably from being able to project
[00:55:06.240 --> 00:55:10.880]   another person's intentions to understand what they mean
[00:55:10.880 --> 00:55:13.480]   when you have these large cognitive groups,
[00:55:13.480 --> 00:55:15.160]   large social groups.
[00:55:15.160 --> 00:55:19.080]   So me being able to sort of create a mental model
[00:55:19.080 --> 00:55:22.120]   of how you think may have come before
[00:55:22.120 --> 00:55:25.120]   I was able to create a personal mental model
[00:55:25.120 --> 00:55:26.360]   of how do I think.
[00:55:26.360 --> 00:55:29.680]   So this introspection probably came after
[00:55:29.680 --> 00:55:33.520]   this sort of projection and this empathy,
[00:55:33.520 --> 00:55:37.720]   which basically means passion, pathos, suffering,
[00:55:37.720 --> 00:55:39.120]   but basically sensing.
[00:55:39.120 --> 00:55:42.600]   So basically empathy means feeling what you're feeling,
[00:55:42.600 --> 00:55:45.360]   trying to project your emotional state
[00:55:45.360 --> 00:55:47.560]   onto my cognitive apparatus.
[00:55:47.560 --> 00:55:51.360]   And I think that is what eventually led
[00:55:51.360 --> 00:55:55.240]   to this enormous cognitive explosion
[00:55:55.240 --> 00:55:56.680]   that happened in humanity.
[00:55:56.680 --> 00:56:01.680]   So life itself, in my view, is inevitable on every planet.
[00:56:01.680 --> 00:56:05.440]   - Inevitable. - Inevitable.
[00:56:05.440 --> 00:56:09.920]   But the evolution of life to self-awareness and cognition
[00:56:09.920 --> 00:56:12.720]   and all the incredible things that humans have done,
[00:56:12.720 --> 00:56:14.600]   you know, that might not be as inevitable.
[00:56:14.600 --> 00:56:15.440]   - That's your intuition.
[00:56:15.440 --> 00:56:20.440]   So if you were to sort of estimate and bet some money on it,
[00:56:20.440 --> 00:56:24.920]   if we reran Earth a million times,
[00:56:24.920 --> 00:56:29.520]   would what we got now be the most special thing
[00:56:29.520 --> 00:56:30.960]   and how often would it be?
[00:56:30.960 --> 00:56:33.040]   So scientifically speaking,
[00:56:33.040 --> 00:56:36.280]   how repeatable is this experiment? (laughs)
[00:56:36.280 --> 00:56:38.680]   - So this whole cognitive revolution,
[00:56:38.680 --> 00:56:41.960]   yes. - Maybe not.
[00:56:41.960 --> 00:56:42.800]   Maybe not.
[00:56:42.800 --> 00:56:47.400]   Basically, I feel that the longevity of dinosaurs
[00:56:47.400 --> 00:56:51.880]   suggests that it was not quite inevitable,
[00:56:51.880 --> 00:56:56.680]   that we humans eventually made it.
[00:56:56.680 --> 00:56:59.220]   - Well, you're also implying one thing here.
[00:56:59.220 --> 00:57:01.120]   You're saying, you're implying that humans
[00:57:01.120 --> 00:57:03.320]   also don't have this longevity.
[00:57:03.320 --> 00:57:05.040]   This is the interesting question.
[00:57:05.040 --> 00:57:06.960]   So with the Fermi paradox,
[00:57:06.960 --> 00:57:10.440]   the idea that the basic question is like,
[00:57:10.440 --> 00:57:14.400]   if the universe has a lot of alien life forms in it,
[00:57:14.400 --> 00:57:16.480]   why haven't we seen them?
[00:57:16.480 --> 00:57:19.360]   And one thought is that there's a great filter
[00:57:19.360 --> 00:57:21.240]   or multiple great filters
[00:57:21.240 --> 00:57:25.560]   that basically would destroy intelligent civilizations.
[00:57:25.560 --> 00:57:28.640]   This thing that we, you know, this multifolding brain
[00:57:28.640 --> 00:57:32.160]   that keeps growing may not be such a big feature.
[00:57:32.160 --> 00:57:34.280]   It might be useful for survival,
[00:57:34.280 --> 00:57:38.440]   but it like takes us down a side road
[00:57:38.440 --> 00:57:41.160]   that is a very short one with a quick dead end.
[00:57:41.160 --> 00:57:42.240]   What do you think about that?
[00:57:42.240 --> 00:57:45.620]   - So I think the universe is enormous,
[00:57:45.620 --> 00:57:48.280]   not just in space, but also in time.
[00:57:48.280 --> 00:57:53.960]   And the pretense that, you know,
[00:57:53.960 --> 00:57:56.760]   the last blink of an instant
[00:57:56.760 --> 00:57:58.840]   that we've been able to send radio waves
[00:57:58.840 --> 00:58:00.360]   is when somebody should have been paying attention
[00:58:00.360 --> 00:58:03.000]   to our planet is a little ridiculous.
[00:58:03.880 --> 00:58:07.260]   So my, you know, what I love about Star Wars
[00:58:07.260 --> 00:58:10.800]   is a long, long time ago in a galaxy far, far away.
[00:58:10.800 --> 00:58:11.920]   It's not like some distant future,
[00:58:11.920 --> 00:58:13.520]   it's a long, long time ago.
[00:58:13.520 --> 00:58:16.000]   What I love about it is that basically says,
[00:58:16.000 --> 00:58:19.400]   you know, evolution and civilization
[00:58:19.400 --> 00:58:23.800]   are just so recent in, you know, on earth.
[00:58:23.800 --> 00:58:25.120]   Like there's countless other planets
[00:58:25.120 --> 00:58:27.760]   that have probably all kinds of life forms,
[00:58:27.760 --> 00:58:30.420]   multicellular perhaps, and so on and so forth.
[00:58:31.600 --> 00:58:35.880]   But the fact that humanity has only been listening
[00:58:35.880 --> 00:58:39.360]   and emitting for just this tiny little blink
[00:58:39.360 --> 00:58:42.840]   means that any of these, you know, alien civilizations
[00:58:42.840 --> 00:58:44.100]   would need to be paying attention
[00:58:44.100 --> 00:58:47.200]   to every single insignificant planet out there.
[00:58:47.200 --> 00:58:50.360]   And, you know, again, I mean, the movie "Contact"
[00:58:50.360 --> 00:58:52.400]   and the book is just so beautiful.
[00:58:52.400 --> 00:58:56.240]   This whole concept of we don't need to travel physically.
[00:58:56.240 --> 00:58:57.720]   We can travel as light.
[00:58:57.720 --> 00:59:01.080]   We can send instructions for people to create machines
[00:59:01.080 --> 00:59:04.880]   that will allow us to beam down light and recreate ourselves.
[00:59:04.880 --> 00:59:08.160]   And in the book, you know, the aliens actually take over.
[00:59:08.160 --> 00:59:09.840]   - Yes. - They're not as friendly.
[00:59:09.840 --> 00:59:13.560]   But, you know, this concept that we have to eventually
[00:59:13.560 --> 00:59:14.780]   go and conquer every planet.
[00:59:14.780 --> 00:59:16.720]   I mean, I think that, yes,
[00:59:16.720 --> 00:59:18.600]   we will become a galactic species.
[00:59:18.600 --> 00:59:21.840]   - So you have a hope, well, you said think, so.
[00:59:21.840 --> 00:59:23.120]   - Oh, of course, of course.
[00:59:23.120 --> 00:59:25.840]   I mean, now that we've made it so far.
[00:59:25.840 --> 00:59:27.240]   - So you feel like we've made it.
[00:59:27.240 --> 00:59:30.120]   - Oh gosh, I feel that, you know, cognition,
[00:59:30.120 --> 00:59:31.800]   the cognition as an evolutionary trait
[00:59:31.800 --> 00:59:33.840]   has won over in our planet.
[00:59:33.840 --> 00:59:35.360]   There's no doubt that we've made it.
[00:59:35.360 --> 00:59:40.280]   So basically humans have won the battle for, you know,
[00:59:40.280 --> 00:59:41.400]   dominance.
[00:59:41.400 --> 00:59:43.800]   It wasn't necessarily the case with dinosaurs.
[00:59:43.800 --> 00:59:46.160]   Like, I mean, yes, you know,
[00:59:46.160 --> 00:59:50.400]   there's some claims of intelligence.
[00:59:50.400 --> 00:59:53.840]   And if you look at Jurassic Park, yeah, sure, whatever.
[00:59:53.840 --> 00:59:57.280]   But, you know, they just don't have the hardware for it.
[00:59:57.280 --> 00:59:58.120]   - Yeah.
[00:59:58.120 --> 00:59:59.320]   - And humans have the hardware.
[00:59:59.320 --> 01:00:00.960]   There's no doubt that mammals
[01:00:00.960 --> 01:00:03.680]   have a dramatically improved hardware
[01:00:03.680 --> 01:00:06.120]   for cognition over dinosaurs.
[01:00:06.120 --> 01:00:09.460]   Like basically there's universes where strength won out.
[01:00:09.460 --> 01:00:11.680]   And in our planet, in our, you know,
[01:00:11.680 --> 01:00:15.600]   particular version of whatever happened in this planet,
[01:00:15.600 --> 01:00:16.960]   cognition won out.
[01:00:16.960 --> 01:00:18.040]   And it's kind of cool.
[01:00:18.040 --> 01:00:20.220]   I mean, it's a privilege, right?
[01:00:20.220 --> 01:00:22.040]   But it's kind of like living in Boston instead of,
[01:00:22.040 --> 01:00:26.600]   I don't know, some middle age place
[01:00:26.600 --> 01:00:28.080]   where everybody's like hitting each other
[01:00:28.080 --> 01:00:31.160]   with, you know, weapons and sticks.
[01:00:31.160 --> 01:00:32.880]   - You're back to the lucky one song.
[01:00:32.880 --> 01:00:33.720]   (laughing)
[01:00:33.720 --> 01:00:36.080]   - I mean, we are the lucky ones.
[01:00:36.080 --> 01:00:39.640]   - But the flip side of that is that this hardware
[01:00:39.640 --> 01:00:41.640]   also allows us to develop weapons
[01:00:41.640 --> 01:00:43.880]   and methods of destroying ourselves.
[01:00:43.880 --> 01:00:46.120]   So you--
[01:00:46.120 --> 01:00:47.960]   - Again, I wanna go back to Pinker
[01:00:47.960 --> 01:00:50.440]   and the better angels of our nature.
[01:00:50.440 --> 01:00:55.120]   The whole concept that civilization
[01:00:55.120 --> 01:00:58.120]   and the act of civilizing
[01:00:58.120 --> 01:01:02.920]   has dramatically reduced violence, dramatically.
[01:01:02.920 --> 01:01:06.320]   If you look, you know, at every scale,
[01:01:06.320 --> 01:01:08.880]   as soon as organization comes,
[01:01:08.880 --> 01:01:13.740]   the state basically owns the right to violence.
[01:01:13.740 --> 01:01:17.600]   And eventually the state gives that right
[01:01:17.600 --> 01:01:20.440]   of governance to the people,
[01:01:20.440 --> 01:01:23.480]   but violence has been eliminated by that state.
[01:01:23.480 --> 01:01:27.160]   So this whole concept of central governance
[01:01:27.160 --> 01:01:29.840]   and people agreeing to live together
[01:01:29.840 --> 01:01:34.000]   and share responsibilities and duties
[01:01:34.000 --> 01:01:36.840]   and, you know, all of that,
[01:01:36.840 --> 01:01:41.600]   is something that has led so much to less violence,
[01:01:41.600 --> 01:01:44.800]   less death, less suffering, less, you know,
[01:01:44.800 --> 01:01:48.800]   poverty, less, you know, war.
[01:01:48.800 --> 01:01:52.240]   I mean, yes, we have the capability
[01:01:52.240 --> 01:01:53.560]   to destroy ourselves,
[01:01:53.560 --> 01:01:56.560]   but the arc of civilization
[01:01:56.560 --> 01:01:58.760]   has led to much, much less destruction,
[01:01:58.760 --> 01:02:01.080]   much, much less war and much more peace.
[01:02:01.080 --> 01:02:04.040]   And of course there's blips back and forth
[01:02:04.040 --> 01:02:06.480]   and, you know, there are setbacks,
[01:02:06.480 --> 01:02:10.240]   but again, the moral arc of the universe
[01:02:10.240 --> 01:02:11.480]   seems to just--
[01:02:11.480 --> 01:02:14.300]   - I probably imagine there were two dinosaurs
[01:02:14.300 --> 01:02:17.080]   back in the day having this exact conversation
[01:02:17.080 --> 01:02:18.600]   and they look up to the sky
[01:02:18.600 --> 01:02:21.200]   and there seems to be something like an asteroid
[01:02:21.200 --> 01:02:22.040]   going towards Earth.
[01:02:22.040 --> 01:02:24.760]   So while it's very true
[01:02:24.760 --> 01:02:28.520]   that the arc of our society of human civilization
[01:02:28.520 --> 01:02:30.380]   seems to be progressing
[01:02:30.380 --> 01:02:32.880]   towards a better, better life for everybody
[01:02:32.880 --> 01:02:34.860]   in the many ways that you described,
[01:02:34.860 --> 01:02:39.160]   things can change in a moment.
[01:02:39.160 --> 01:02:41.640]   And it feels like it's not just us humans
[01:02:41.640 --> 01:02:44.160]   we're living through a pandemic.
[01:02:44.160 --> 01:02:47.960]   You could imagine that a pandemic would be more destructive
[01:02:47.960 --> 01:02:52.520]   or there could be asteroids that just appear
[01:02:52.520 --> 01:02:54.480]   out of the darkness of space,
[01:02:54.480 --> 01:02:58.400]   which I recently learned is not that easy to--
[01:02:58.400 --> 01:02:59.240]   - Let me give you another number.
[01:02:59.240 --> 01:03:00.400]   - Detect them, yes.
[01:03:00.400 --> 01:03:02.940]   - So 48, what happens in 48 years?
[01:03:02.940 --> 01:03:05.440]   - I'm not sure.
[01:03:05.440 --> 01:03:06.960]   - 2068, Apophis.
[01:03:06.960 --> 01:03:09.600]   There's an asteroid that's coming.
[01:03:09.600 --> 01:03:11.560]   In 48 years, it has very high chance
[01:03:11.560 --> 01:03:13.560]   of actually wiping us out completely.
[01:03:13.560 --> 01:03:14.400]   - Yes.
[01:03:14.400 --> 01:03:15.240]   - Yes.
[01:03:15.400 --> 01:03:18.560]   - So we have 48 years to get our act together.
[01:03:18.560 --> 01:03:21.000]   It's not like some distant, distant hypothesis.
[01:03:21.000 --> 01:03:21.840]   - Yes.
[01:03:21.840 --> 01:03:23.120]   - Like, yeah, sure, they're hard to detect,
[01:03:23.120 --> 01:03:25.280]   but this one we know about, it's coming.
[01:03:25.280 --> 01:03:26.240]   - So how do you feel about that?
[01:03:26.240 --> 01:03:27.200]   Why are you still so optimistic?
[01:03:27.200 --> 01:03:29.840]   - Oh gosh, I'm so happy with where we are now.
[01:03:29.840 --> 01:03:30.880]   This is gonna be great.
[01:03:30.880 --> 01:03:32.760]   Seriously, if you look at progress,
[01:03:32.760 --> 01:03:34.040]   if you look at, again,
[01:03:34.040 --> 01:03:38.440]   the speed with which knowledge has been transferred,
[01:03:38.440 --> 01:03:43.280]   what has led to humanity making so many advances so fast?
[01:03:43.280 --> 01:03:44.360]   Okay?
[01:03:44.360 --> 01:03:47.560]   So what has led to humanity making so many advances
[01:03:47.560 --> 01:03:50.400]   is not just the hardware upgrades.
[01:03:50.400 --> 01:03:52.600]   It's also the software upgrades.
[01:03:52.600 --> 01:03:55.520]   So by hardware upgrades, I basically mean our neocortex
[01:03:55.520 --> 01:03:59.440]   and the expansion and these layers and folds of our brain
[01:03:59.440 --> 01:04:00.280]   and all of that.
[01:04:00.280 --> 01:04:01.400]   That's the hardware.
[01:04:01.400 --> 01:04:04.520]   The software hasn't, you know,
[01:04:04.520 --> 01:04:07.120]   the hardware hasn't changed much in the last,
[01:04:07.120 --> 01:04:08.400]   what, 70,000 years?
[01:04:08.400 --> 01:04:12.000]   As I mentioned last time,
[01:04:12.000 --> 01:04:13.560]   if you take a person from ancient Egypt
[01:04:13.560 --> 01:04:15.200]   and you bring them up now,
[01:04:15.200 --> 01:04:18.000]   they're just as equally fit.
[01:04:18.000 --> 01:04:20.520]   So hardware hasn't changed.
[01:04:20.520 --> 01:04:22.440]   What has changed is software.
[01:04:22.440 --> 01:04:27.440]   What has changed is that we are growing up in societies
[01:04:27.440 --> 01:04:30.880]   that are much more complex.
[01:04:30.880 --> 01:04:32.880]   This whole concept of neoteny
[01:04:32.880 --> 01:04:35.760]   basically allows our exponential growth.
[01:04:35.760 --> 01:04:39.120]   The concept that our brain has not fully formed,
[01:04:39.120 --> 01:04:41.680]   has not fully stabilized itself
[01:04:41.680 --> 01:04:43.600]   until after our teenage years.
[01:04:43.600 --> 01:04:46.680]   So we basically have a good 16 years, 18 years
[01:04:46.680 --> 01:04:48.920]   to sort of infuse it with the latest
[01:04:48.920 --> 01:04:51.240]   and greatest in software.
[01:04:51.240 --> 01:04:53.920]   If you look at what happened in ancient Greece,
[01:04:53.920 --> 01:04:57.400]   why did everything explode at once?
[01:04:57.400 --> 01:04:59.680]   My take on this is that it was the shift
[01:04:59.680 --> 01:05:03.520]   from the Egyptian and hieroglyphic software
[01:05:03.520 --> 01:05:05.680]   to the Greek language software.
[01:05:05.680 --> 01:05:09.960]   This whole concept of creating abstract notions,
[01:05:09.960 --> 01:05:14.960]   of creating these layers of cognition
[01:05:14.960 --> 01:05:19.480]   and layers of meaning and layers of abstraction
[01:05:19.480 --> 01:05:24.480]   for words and ideals and beauty and harmony.
[01:05:24.480 --> 01:05:26.800]   How do you write harmony in hieroglyphics?
[01:05:26.800 --> 01:05:29.920]   There's no such thing as sort of expressing these ideals
[01:05:29.920 --> 01:05:34.360]   of peace and justice and these concepts of,
[01:05:34.360 --> 01:05:39.360]   or even macabre concepts of doom, et cetera.
[01:05:39.680 --> 01:05:42.160]   You don't have the language for it.
[01:05:42.160 --> 01:05:47.160]   Your brain has trouble getting at that concept.
[01:05:47.160 --> 01:05:50.120]   So what I'm trying to say is that
[01:05:50.120 --> 01:05:55.360]   these software upgrades for human language,
[01:05:55.360 --> 01:06:00.080]   human culture, human environment, human education
[01:06:00.080 --> 01:06:04.920]   have basically led to this enormous explosion of knowledge.
[01:06:04.920 --> 01:06:07.200]   And eventually after the enlightenment,
[01:06:07.200 --> 01:06:11.320]   and as I was mentioning, the 42-line Bible
[01:06:11.320 --> 01:06:13.800]   and the printed press, the dissemination of knowledge,
[01:06:13.800 --> 01:06:17.080]   you basically now have this whole horizontal dispersion
[01:06:17.080 --> 01:06:21.320]   of ideas in addition to the vertical inheritance of genes.
[01:06:21.320 --> 01:06:23.640]   So the hardware improvements
[01:06:23.640 --> 01:06:25.480]   happen through vertical inheritance.
[01:06:25.480 --> 01:06:27.000]   The software improvements happen
[01:06:27.000 --> 01:06:28.720]   through horizontal inheritance.
[01:06:28.720 --> 01:06:31.360]   And the reason why human civilization exploded
[01:06:31.360 --> 01:06:32.680]   is not a hardware change anymore,
[01:06:32.680 --> 01:06:34.560]   it's really a software change.
[01:06:34.560 --> 01:06:37.200]   So if you're looking at now where we are today,
[01:06:37.200 --> 01:06:40.480]   look at coronavirus.
[01:06:40.480 --> 01:06:42.520]   Yes, sure, it could have killed us.
[01:06:42.520 --> 01:06:45.560]   A hundred years ago, it would have, but it didn't.
[01:06:45.560 --> 01:06:46.400]   Why?
[01:06:46.400 --> 01:06:50.960]   Because in January, we published the genome.
[01:06:50.960 --> 01:06:52.960]   A month later, less than a month later,
[01:06:52.960 --> 01:06:54.920]   the first vaccine designs were done.
[01:06:54.920 --> 01:06:58.160]   And now less than a year later, 10 months later,
[01:06:58.160 --> 01:07:01.160]   we already have a working vaccine that's 90% efficient.
[01:07:01.160 --> 01:07:03.800]   I mean, that is ridiculous by any standards.
[01:07:03.800 --> 01:07:05.960]   And the reason is sharing.
[01:07:05.960 --> 01:07:09.400]   So the asteroid, yes, could wipe us out in 48 years,
[01:07:09.400 --> 01:07:11.000]   but 48 years?
[01:07:11.000 --> 01:07:16.000]   I mean, look at where we were 48 years ago, technologically.
[01:07:16.000 --> 01:07:18.360]   I mean, how much more we understand
[01:07:18.360 --> 01:07:22.960]   the basic foundations of space is enormous.
[01:07:22.960 --> 01:07:27.320]   The technological revolutions of digitization,
[01:07:27.320 --> 01:07:30.360]   the amount of compute power we can put on any,
[01:07:33.480 --> 01:07:36.880]   in nail size hardware is enormous.
[01:07:36.880 --> 01:07:40.560]   So, and this is nowhere near ending.
[01:07:40.560 --> 01:07:43.800]   We all have our little problems going back and forth
[01:07:43.800 --> 01:07:46.240]   on the social side and on the political side
[01:07:46.240 --> 01:07:49.280]   and on the cognitive and on the sort of human side
[01:07:49.280 --> 01:07:54.080]   and the societal side, but science has not slowed down.
[01:07:54.080 --> 01:07:57.360]   Science is moving at a breakneck pace ahead.
[01:07:57.360 --> 01:08:01.680]   So Elon is now putting rockets out from the private space.
[01:08:01.680 --> 01:08:06.040]   I mean, that now democratization of space exploration
[01:08:06.040 --> 01:08:08.680]   is gonna revolutionize. - It's gonna explode.
[01:08:08.680 --> 01:08:09.800]   - Of course. - It could continue.
[01:08:09.800 --> 01:08:12.600]   - In the same way that every technology has exploded,
[01:08:12.600 --> 01:08:15.920]   this is the shift to space technology exploding.
[01:08:15.920 --> 01:08:19.360]   So 48 years is infinity from now
[01:08:19.360 --> 01:08:21.280]   in terms of space capabilities.
[01:08:21.280 --> 01:08:22.600]   So I'm not worried at all.
[01:08:22.600 --> 01:08:25.000]   - Are you excited by the possibility of a human,
[01:08:25.000 --> 01:08:28.440]   well, one, a human stepping foot on Mars
[01:08:28.440 --> 01:08:33.080]   and two, possible colonization of not necessarily Mars,
[01:08:33.080 --> 01:08:34.840]   but other planets and all that kind of stuff
[01:08:34.840 --> 01:08:36.120]   for people living in space?
[01:08:36.120 --> 01:08:37.560]   - Inevitable. - Inevitable.
[01:08:37.560 --> 01:08:39.000]   - Inevitable. - Would you do it?
[01:08:39.000 --> 01:08:40.240]   Or are you kind of like Earth? - Of course.
[01:08:40.240 --> 01:08:41.520]   Of course. (Lex laughing)
[01:08:41.520 --> 01:08:42.360]   In a heartbeat.
[01:08:42.360 --> 01:08:44.000]   - How many people will you wait?
[01:08:44.000 --> 01:08:46.400]   Will you wait for, I think it was about when
[01:08:46.400 --> 01:08:50.000]   the Declaration of Independence was signed,
[01:08:50.000 --> 01:08:52.560]   about two to three million people lived here.
[01:08:52.560 --> 01:08:54.440]   So would you move like before?
[01:08:54.440 --> 01:08:57.000]   Would you be like on the first boat?
[01:08:57.000 --> 01:08:58.280]   Would you be on the 10th boat?
[01:08:58.280 --> 01:09:00.440]   Would you wait until the Declaration of Independence?
[01:09:00.440 --> 01:09:02.120]   - I don't think I'll be on the short list
[01:09:02.120 --> 01:09:04.000]   because I'll be old by then.
[01:09:04.000 --> 01:09:06.120]   They'll probably get a bunch of younger people.
[01:09:06.120 --> 01:09:08.200]   - But you're, it's the-- (Lex laughing)
[01:09:08.200 --> 01:09:09.800]   It's the wisdom and the--
[01:09:09.800 --> 01:09:13.640]   - But wisdom can be transferred horizontally.
[01:09:13.640 --> 01:09:15.240]   - I gotta tell you, you are the lucky one,
[01:09:15.240 --> 01:09:16.160]   so you might be on the list.
[01:09:16.160 --> 01:09:17.000]   I don't know.
[01:09:17.000 --> 01:09:19.200]   You never know. - I mean, I kind of feel
[01:09:19.200 --> 01:09:21.960]   like I would love to see Earth from above
[01:09:21.960 --> 01:09:23.720]   just to watch our planet.
[01:09:23.720 --> 01:09:26.840]   I mean, just, I mean, you know you can watch a live feed
[01:09:26.840 --> 01:09:29.320]   of the space station.
[01:09:29.320 --> 01:09:32.600]   Watching Earth is magnificent,
[01:09:32.600 --> 01:09:35.840]   like this blue tiny little shield.
[01:09:35.840 --> 01:09:38.040]   It's so thin, our atmosphere.
[01:09:38.040 --> 01:09:39.160]   Like if you drive to New York,
[01:09:39.160 --> 01:09:40.560]   you're basically in outer space.
[01:09:40.560 --> 01:09:41.440]   I mean, it's ridiculous.
[01:09:41.440 --> 01:09:42.480]   It's just so thin.
[01:09:42.480 --> 01:09:45.040]   And it's just, again, such a privilege
[01:09:45.040 --> 01:09:47.280]   to be on this planet, such a privilege.
[01:09:47.280 --> 01:09:52.280]   But I think our species is in for big, good things.
[01:09:52.280 --> 01:09:54.800]   (both laughing)
[01:09:54.800 --> 01:09:58.800]   I think that we will overcome our little problems
[01:09:58.800 --> 01:10:01.400]   and eventually come together as a species.
[01:10:01.400 --> 01:10:04.360]   I feel that we're definitely on the path to that.
[01:10:04.360 --> 01:10:09.360]   And it's just not permeated through the whole universe yet,
[01:10:09.360 --> 01:10:11.240]   I mean, through the whole world yet,
[01:10:11.240 --> 01:10:12.560]   through the whole Earth yet,
[01:10:12.560 --> 01:10:14.200]   but it's definitely permeating.
[01:10:14.200 --> 01:10:17.640]   - So you've talked about humans as special.
[01:10:17.640 --> 01:10:23.840]   How exactly are we special relative to the dinosaurs?
[01:10:24.280 --> 01:10:28.200]   - So I mentioned that there's this dramatic
[01:10:28.200 --> 01:10:31.720]   cognitive improvement that we've made,
[01:10:31.720 --> 01:10:34.000]   but I think it goes much deeper than that.
[01:10:34.000 --> 01:10:38.720]   So if you look at a lion attacking a gazelle
[01:10:38.720 --> 01:10:41.000]   in the middle of the Serengeti,
[01:10:41.000 --> 01:10:45.680]   the lion is smelling the molecules in the environment.
[01:10:45.680 --> 01:10:50.680]   Its hormones and neuroreceptors
[01:10:50.680 --> 01:10:53.560]   are sort of getting it ready for impulse.
[01:10:54.520 --> 01:10:58.440]   The target is constantly looking around and sensing.
[01:10:58.440 --> 01:11:03.000]   I've actually been in Kenya and I've kind of seen the hunt.
[01:11:03.000 --> 01:11:07.360]   So I've kind of seen the sort of game of waiting.
[01:11:07.360 --> 01:11:13.320]   And the mitochondria in the muscles of the lion
[01:11:13.320 --> 01:11:18.360]   are basically ready for jumping.
[01:11:18.360 --> 01:11:21.520]   They're expensing an enormous amount of energy.
[01:11:21.520 --> 01:11:25.760]   The grass as it's flowing is constantly transforming
[01:11:25.760 --> 01:11:30.080]   solar energy into chloroplasts,
[01:11:30.080 --> 01:11:32.200]   through the chloroplasts into energy,
[01:11:32.200 --> 01:11:34.640]   which eventually feeds the gazelle
[01:11:34.640 --> 01:11:37.880]   and eventually feeds the lions and so on and so forth.
[01:11:37.880 --> 01:11:42.880]   So as humans, we experience all of that,
[01:11:42.880 --> 01:11:48.360]   but the lion only experiences one layer.
[01:11:49.620 --> 01:11:51.120]   The mitochondria in its body
[01:11:51.120 --> 01:11:52.520]   are only experiencing one layer.
[01:11:52.520 --> 01:11:55.560]   The chloroplasts are only experiencing one layer.
[01:11:55.560 --> 01:11:59.800]   The photoreceptors and the smell receptors
[01:11:59.800 --> 01:12:00.640]   and the chemical receptors,
[01:12:00.640 --> 01:12:02.840]   like the lion always attacks against the wind
[01:12:02.840 --> 01:12:04.080]   so that it's not smelled.
[01:12:04.080 --> 01:12:10.160]   Like all of these things are one layer at a time.
[01:12:10.160 --> 01:12:14.760]   And we humans somehow perceive the whole stack.
[01:12:14.760 --> 01:12:17.120]   So going back to software infrastructure
[01:12:17.120 --> 01:12:18.680]   and hardware infrastructure,
[01:12:18.680 --> 01:12:20.640]   if you design a computer,
[01:12:20.640 --> 01:12:23.400]   you basically have a physical layer that you start with.
[01:12:23.400 --> 01:12:24.840]   And then on top of that physical layer,
[01:12:24.840 --> 01:12:27.360]   you have the electrical layer.
[01:12:27.360 --> 01:12:28.720]   And on top of the electrical layer,
[01:12:28.720 --> 01:12:32.720]   you have basically gates and logic and an assembly layer.
[01:12:32.720 --> 01:12:33.720]   And on top of the assembly layer,
[01:12:33.720 --> 01:12:37.760]   you have your higher order, higher level programming.
[01:12:37.760 --> 01:12:38.600]   And on top of that,
[01:12:38.600 --> 01:12:40.200]   you have your deep learning routine, et cetera.
[01:12:40.200 --> 01:12:41.040]   And on top of that,
[01:12:41.040 --> 01:12:44.780]   you eventually build a cognitive system that's smart.
[01:12:46.060 --> 01:12:49.540]   I want you to now picture this cognitive system
[01:12:49.540 --> 01:12:51.920]   becoming not just self-aware,
[01:12:51.920 --> 01:12:56.880]   but also becoming aware of the hardware that it's made of
[01:12:56.880 --> 01:13:01.180]   and the atoms that it's made of and so on and so forth.
[01:13:01.180 --> 01:13:03.680]   So it's as if your AI system,
[01:13:03.680 --> 01:13:08.680]   and there's this beautiful scene in "2001 Odyssey of Space"
[01:13:08.680 --> 01:13:13.060]   where Hal, after Dave starts disconnecting him,
[01:13:13.060 --> 01:13:17.720]   is starting to sing a song about daisies, et cetera.
[01:13:17.720 --> 01:13:20.160]   And Hal is basically saying,
[01:13:20.160 --> 01:13:22.740]   "Dave, I'm losing my mind.
[01:13:22.740 --> 01:13:26.480]   I can feel I'm losing my mind."
[01:13:26.480 --> 01:13:28.160]   It's just so beautiful.
[01:13:28.160 --> 01:13:30.880]   This concept of self-awareness,
[01:13:30.880 --> 01:13:35.320]   of knowing that the hardware is no longer there, is amazing.
[01:13:35.320 --> 01:13:36.680]   And in the same way,
[01:13:36.680 --> 01:13:39.120]   humans who have had accidents
[01:13:39.120 --> 01:13:42.120]   are aware that they've had accidents.
[01:13:42.120 --> 01:13:44.240]   So there's this self-awareness of AI
[01:13:44.240 --> 01:13:48.740]   that is this beautiful concept
[01:13:48.740 --> 01:13:53.740]   about sort of the eventual cognitive leap to self-awareness.
[01:13:53.740 --> 01:13:57.280]   But imagine now the AI system
[01:13:57.280 --> 01:13:58.840]   actually breaking through these layers and saying,
[01:13:58.840 --> 01:14:00.240]   "Wait a minute, I think I can design
[01:14:00.240 --> 01:14:03.520]   a slightly better hardware to get me functioning better."
[01:14:03.520 --> 01:14:05.740]   And that's what basically humans are doing.
[01:14:05.740 --> 01:14:08.440]   So if you look at our reasoning layer,
[01:14:08.440 --> 01:14:11.240]   it's built on top of a cognitive layer.
[01:14:11.240 --> 01:14:13.420]   And the reasoning layer we share with AI.
[01:14:13.420 --> 01:14:14.840]   It's kind of cool.
[01:14:14.840 --> 01:14:16.520]   There is another thing on the planet
[01:14:16.520 --> 01:14:19.400]   that can integrate equations and it's man-made,
[01:14:19.400 --> 01:14:22.140]   but we share computation with them.
[01:14:22.140 --> 01:14:24.880]   We share this cognitive layer of playing chess.
[01:14:24.880 --> 01:14:26.560]   We're not alone anymore.
[01:14:26.560 --> 01:14:28.440]   We're not the only thing on the planet that plays chess.
[01:14:28.440 --> 01:14:31.280]   Now we have AI that also plays chess.
[01:14:31.280 --> 01:14:34.200]   - But in some sense, that particular organism, AI,
[01:14:34.200 --> 01:14:36.280]   as it is now, only operates in that layer.
[01:14:36.280 --> 01:14:38.240]   - Exactly, exactly.
[01:14:38.240 --> 01:14:41.440]   And then most animals operate in the sort of
[01:14:41.440 --> 01:14:43.800]   cognitive layer that we're all experiencing.
[01:14:43.800 --> 01:14:48.040]   A bat is doing this incredible integration of signals,
[01:14:48.040 --> 01:14:50.020]   but it's not aware of it.
[01:14:50.020 --> 01:14:55.020]   It's basically constantly sending echo location, waves,
[01:14:55.020 --> 01:14:56.920]   and it's receiving them back.
[01:14:56.920 --> 01:14:59.200]   And multiple bats in the same cave
[01:14:59.200 --> 01:15:01.800]   are operating at slightly different frequencies
[01:15:01.800 --> 01:15:03.800]   and with slightly different pulses.
[01:15:03.800 --> 01:15:05.240]   And they're all sensing objects
[01:15:05.240 --> 01:15:07.440]   and they're doing motion planning
[01:15:07.440 --> 01:15:08.880]   in their cognitive hardware,
[01:15:08.880 --> 01:15:10.760]   but they're not even aware of all of that.
[01:15:10.760 --> 01:15:13.920]   All they know is that they have a 3D view of space
[01:15:13.920 --> 01:15:17.440]   around them, just like any gazelle walking through,
[01:15:17.440 --> 01:15:19.000]   you know, the desert.
[01:15:19.000 --> 01:15:23.640]   And any baby looking around is aware of things
[01:15:23.640 --> 01:15:26.920]   without doing the math of how am I processing
[01:15:26.920 --> 01:15:29.000]   all of these visual information, et cetera.
[01:15:29.000 --> 01:15:31.880]   You're just aware of the layer that you live in.
[01:15:31.880 --> 01:15:36.120]   I think if you look at this, at humanity,
[01:15:36.120 --> 01:15:39.120]   we've basically managed through our cognitive layer,
[01:15:39.120 --> 01:15:42.040]   through our perception layer, through our senses layer,
[01:15:42.040 --> 01:15:47.040]   through our multi-organ layer, through our genetic layer,
[01:15:47.040 --> 01:15:52.200]   through our molecular layer, through our atomic layer,
[01:15:52.200 --> 01:15:54.360]   through our quantum layer,
[01:15:54.360 --> 01:15:58.200]   through even the very fabric of the space-time continuum,
[01:15:58.200 --> 01:16:00.680]   unite all of that cognitively.
[01:16:00.680 --> 01:16:04.520]   So as we're watching that scene in the Serengeti,
[01:16:04.520 --> 01:16:07.360]   we as scientists, we as educated humans,
[01:16:07.360 --> 01:16:10.040]   we as, you know, anyone who's finished high school
[01:16:10.040 --> 01:16:12.120]   are aware of all of this beauty
[01:16:12.120 --> 01:16:14.760]   of all of these different layers interplaying together.
[01:16:14.760 --> 01:16:16.720]   And I think that's something very unique
[01:16:16.720 --> 01:16:20.160]   in perhaps not just the galaxy, but maybe even the cosmos.
[01:16:20.160 --> 01:16:25.960]   These species that has managed to, in space,
[01:16:25.960 --> 01:16:29.420]   cross through these layers from the enormous
[01:16:29.420 --> 01:16:30.780]   to the infinitely small.
[01:16:30.780 --> 01:16:33.240]   And that's what I love about particle physics,
[01:16:33.240 --> 01:16:35.800]   the fact that it actually unites everything.
[01:16:35.800 --> 01:16:36.640]   - The very small, the very big.
[01:16:36.640 --> 01:16:38.000]   - The very small and the very big.
[01:16:38.000 --> 01:16:39.280]   It's only through the very big
[01:16:39.280 --> 01:16:41.320]   that the very small gets formed.
[01:16:41.320 --> 01:16:43.080]   Like basically every atom of gold
[01:16:43.080 --> 01:16:47.880]   results from the fusion that happened
[01:16:47.880 --> 01:16:51.400]   of increasingly large particles before that explosion
[01:16:51.400 --> 01:16:53.640]   that then disperses it through the cosmos.
[01:16:53.640 --> 01:16:57.220]   And it's only through understanding the very large
[01:16:57.220 --> 01:16:59.560]   that we understand the very small and vice versa.
[01:16:59.560 --> 01:17:02.000]   And that's in space.
[01:17:02.000 --> 01:17:04.880]   Then there's the time direction.
[01:17:04.880 --> 01:17:08.720]   As you are watching the Kilimanjaro Mountain,
[01:17:08.720 --> 01:17:11.720]   you can kind of look back through time
[01:17:11.720 --> 01:17:14.360]   to when that volcano was exploding
[01:17:14.360 --> 01:17:17.760]   and growing out of the tectonic forces.
[01:17:17.760 --> 01:17:20.460]   As you drive through Death Valley,
[01:17:20.460 --> 01:17:23.400]   you see these mountains on their side
[01:17:23.400 --> 01:17:27.000]   and these layers of history exposed.
[01:17:27.000 --> 01:17:33.260]   We are aware of the eons that have happened on Earth
[01:17:33.260 --> 01:17:36.060]   and the tectonic movements on Earth
[01:17:36.060 --> 01:17:41.220]   the same way that we're aware of the Big Bang
[01:17:41.220 --> 01:17:44.620]   and the early evolution of the cosmos.
[01:17:44.620 --> 01:17:46.620]   And we can also see forward in time
[01:17:46.620 --> 01:17:48.380]   as to where the universe is heading.
[01:17:48.380 --> 01:17:53.340]   We can see Apophis in 2068 coming over,
[01:17:53.340 --> 01:17:54.360]   looking ahead in time.
[01:17:54.360 --> 01:17:58.800]   I mean, that would be magician stuff in ancient times.
[01:17:58.800 --> 01:18:02.400]   So what I love about humanity and its role in the universe
[01:18:02.400 --> 01:18:05.840]   is that if there's a God watching,
[01:18:05.840 --> 01:18:08.240]   he's like, "Finally, somebody figured it out.
[01:18:08.240 --> 01:18:10.380]   "I've been building all these beautiful things
[01:18:10.380 --> 01:18:11.640]   "and somebody can appreciate it."
[01:18:11.640 --> 01:18:13.780]   - And figured me out from God's perspective,
[01:18:13.780 --> 01:18:15.680]   meaning become aware of.
[01:18:15.680 --> 01:18:16.980]   - Yeah.
[01:18:16.980 --> 01:18:18.920]   - Yeah, so it's kind of interesting
[01:18:18.920 --> 01:18:22.000]   to think of the world in this way as layers
[01:18:22.000 --> 01:18:25.380]   and us humans are able to convert those layers
[01:18:25.380 --> 01:18:29.920]   into ideas that you can then combine.
[01:18:29.920 --> 01:18:32.040]   So we're doing some kind of conversion.
[01:18:32.040 --> 01:18:33.500]   - Exactly, exactly.
[01:18:33.500 --> 01:18:35.200]   And last time you asked me about
[01:18:35.200 --> 01:18:37.640]   whether we live in a simulation, for example.
[01:18:37.640 --> 01:18:42.640]   I mean, realize that we are living in a simulation.
[01:18:42.640 --> 01:18:44.260]   We are.
[01:18:44.260 --> 01:18:45.740]   The reality that we're in
[01:18:45.740 --> 01:18:49.580]   without any sort of person programming this is a simulation.
[01:18:49.580 --> 01:18:52.420]   Like basically what happens inside your skull?
[01:18:52.420 --> 01:18:55.780]   There's this integration of sensory inputs
[01:18:55.780 --> 01:18:58.580]   which are translated into perceptory signals,
[01:18:58.580 --> 01:19:01.180]   which are then translated into a conceptual model
[01:19:01.180 --> 01:19:03.260]   of the world around you.
[01:19:03.260 --> 01:19:07.580]   And that exercise is happening seamlessly.
[01:19:07.580 --> 01:19:11.680]   And yet, if you think about sort of, again,
[01:19:11.680 --> 01:19:15.760]   this whole simulation and Neo analogy,
[01:19:15.760 --> 01:19:19.060]   you can think of the reality that we live in as a matrix,
[01:19:19.060 --> 01:19:20.500]   as the matrix,
[01:19:20.500 --> 01:19:23.020]   but we've actually broken through the matrix.
[01:19:23.020 --> 01:19:24.620]   We've actually traversed the layers.
[01:19:24.620 --> 01:19:26.140]   We didn't have to take a pill.
[01:19:26.140 --> 01:19:28.300]   Like we didn't, you know,
[01:19:28.300 --> 01:19:30.120]   Morpheus didn't have to show up
[01:19:30.120 --> 01:19:32.540]   to basically give us the blue pill or the red pill.
[01:19:32.540 --> 01:19:36.580]   We were able to sufficiently evolve cognitively
[01:19:36.580 --> 01:19:38.460]   through the hardware explosion
[01:19:38.460 --> 01:19:41.220]   and sufficiently involve scientifically
[01:19:41.220 --> 01:19:43.060]   through the software explosion
[01:19:43.060 --> 01:19:45.820]   to basically get at breaking through the matrix,
[01:19:45.820 --> 01:19:47.820]   realizing that we live in a matrix
[01:19:47.820 --> 01:19:51.220]   and realizing that we are this thing in there.
[01:19:51.220 --> 01:19:53.780]   And yet that thing in there has a consciousness
[01:19:53.780 --> 01:19:56.060]   that lives through all these layers.
[01:19:56.060 --> 01:19:58.480]   And I think we're the only species.
[01:19:58.480 --> 01:20:00.460]   We are the only thing that we even can think of
[01:20:00.460 --> 01:20:01.620]   that has actually done that,
[01:20:01.620 --> 01:20:06.620]   has sort of permeated space and time,
[01:20:06.620 --> 01:20:11.940]   scales and layers of abstraction,
[01:20:11.940 --> 01:20:13.780]   plowing through them
[01:20:13.780 --> 01:20:16.900]   and realizing what we're really, really made of.
[01:20:16.900 --> 01:20:20.020]   And the next frontier is of course, cognition.
[01:20:20.020 --> 01:20:22.540]   So we understand so much of the cosmos,
[01:20:22.540 --> 01:20:24.320]   so much of the stuff around us,
[01:20:24.320 --> 01:20:26.100]   but the stuff inside here,
[01:20:26.100 --> 01:20:28.060]   finding the basis for the soul,
[01:20:28.060 --> 01:20:30.100]   finding the basis for the ego,
[01:20:30.100 --> 01:20:32.900]   for the self, the self-awareness.
[01:20:32.900 --> 01:20:35.180]   When does the spark happen
[01:20:35.180 --> 01:20:38.820]   that basically sort of makes you, you?
[01:20:38.820 --> 01:20:41.060]   I mean, that's, you know, really the next frontier.
[01:20:41.060 --> 01:20:45.000]   So in terms of these peeling off layers of complexity,
[01:20:45.000 --> 01:20:47.420]   somewhere between the cognitive layer
[01:20:47.420 --> 01:20:52.420]   and the reasoning layer or the computational layer,
[01:20:52.420 --> 01:20:54.460]   there's still some stuff to be figured out there.
[01:20:54.460 --> 01:20:56.020]   And I think that's the final frontier
[01:20:56.020 --> 01:20:59.060]   of sort of completing our journey through that matrix.
[01:20:59.060 --> 01:21:00.740]   - And maybe duplicating it
[01:21:00.740 --> 01:21:05.540]   in other versions of ourselves through AI,
[01:21:05.540 --> 01:21:08.660]   which is another very exciting possibility.
[01:21:08.660 --> 01:21:10.420]   - What I love about AI
[01:21:10.420 --> 01:21:12.700]   and the way that it operates right now
[01:21:12.700 --> 01:21:14.940]   is the fact that it is unpredictable.
[01:21:14.940 --> 01:21:18.400]   There's emergent behavior
[01:21:18.400 --> 01:21:22.240]   in our cognitively capable artificial systems
[01:21:22.240 --> 01:21:26.580]   that we can certainly model,
[01:21:26.580 --> 01:21:30.000]   but we don't encode directly.
[01:21:30.000 --> 01:21:32.480]   And that's a key difference.
[01:21:32.480 --> 01:21:35.440]   So we like to say, oh, of course,
[01:21:35.440 --> 01:21:38.900]   this is not really intelligent because we coded it up.
[01:21:38.900 --> 01:21:41.480]   And we've just put in these little parameters there
[01:21:41.480 --> 01:21:43.720]   and there's like, you know, what, 6 billion parameters.
[01:21:43.720 --> 01:21:45.680]   And once you've learned them, you know,
[01:21:45.680 --> 01:21:48.160]   we kind of understand the layers.
[01:21:48.160 --> 01:21:50.400]   But that's an oversimplification.
[01:21:50.400 --> 01:21:53.280]   It's like saying, oh, of course, humans,
[01:21:53.280 --> 01:21:54.200]   we understand humans.
[01:21:54.200 --> 01:21:56.680]   They're just made out of neurons and, you know,
[01:21:56.680 --> 01:22:00.440]   layers of cortex and there's a visual area.
[01:22:00.440 --> 01:22:04.200]   But every human is encoded
[01:22:04.200 --> 01:22:06.400]   by a ridiculously small number of genes
[01:22:06.400 --> 01:22:09.560]   compared to the complexity of our cognitive apparatus.
[01:22:09.560 --> 01:22:11.560]   20,000 genes is really not that much
[01:22:11.560 --> 01:22:13.440]   out of which a tiny little fraction
[01:22:13.440 --> 01:22:16.400]   are in fact encoding all of our cognitive functions.
[01:22:16.400 --> 01:22:19.760]   The rest is emergent behavior.
[01:22:19.760 --> 01:22:22.760]   The rest is the, you know,
[01:22:22.760 --> 01:22:26.760]   the cortical layers doing their thing
[01:22:26.760 --> 01:22:29.360]   in the same way that when we build, you know,
[01:22:29.360 --> 01:22:32.160]   these conversational systems or these cognitive systems
[01:22:32.160 --> 01:22:34.600]   or these deep learning systems,
[01:22:34.600 --> 01:22:36.160]   we put the architecture in place,
[01:22:36.160 --> 01:22:37.640]   but then they do their thing.
[01:22:37.640 --> 01:22:39.960]   And in some ways, that's creating something
[01:22:39.960 --> 01:22:41.600]   that has its own identity.
[01:22:41.600 --> 01:22:43.600]   That's creating something that's not just,
[01:22:43.600 --> 01:22:45.840]   oh yeah, it's not the early AI
[01:22:45.840 --> 01:22:47.640]   where if you hadn't programmed what happens
[01:22:47.640 --> 01:22:50.760]   in the grocery bags when you have both cold and hot
[01:22:50.760 --> 01:22:53.480]   and hard and soft, you know,
[01:22:53.480 --> 01:22:54.560]   the system wouldn't know what to do.
[01:22:54.560 --> 01:22:57.520]   No, no, you basically now just program the primitives
[01:22:57.520 --> 01:22:59.480]   and then it learns from that.
[01:22:59.480 --> 01:23:01.200]   - So even though the origins are humble,
[01:23:01.200 --> 01:23:04.000]   just like it is for genetic code, for AI,
[01:23:04.000 --> 01:23:05.800]   even though the origins are humble,
[01:23:07.200 --> 01:23:11.400]   the result of it being deployed into the world
[01:23:11.400 --> 01:23:13.800]   is infinitely complex.
[01:23:13.800 --> 01:23:18.800]   And that's, and yet, there's not,
[01:23:18.800 --> 01:23:23.200]   it's not yet able to be cognizant of all the other layers
[01:23:23.200 --> 01:23:26.640]   in, of its, you know, it's not,
[01:23:26.640 --> 01:23:32.760]   it's not able to think about space and time.
[01:23:32.760 --> 01:23:35.040]   It's not able to think about the hardware
[01:23:35.040 --> 01:23:38.440]   on which it runs, the electricity on which it runs yet.
[01:23:38.440 --> 01:23:41.080]   - So if you look at humans,
[01:23:41.080 --> 01:23:43.280]   we basically have the same cognitive architecture
[01:23:43.280 --> 01:23:45.640]   as monkeys, as the great apes.
[01:23:45.640 --> 01:23:48.200]   It's just a ton more of it.
[01:23:48.200 --> 01:23:53.080]   If you look at GPT-3 versus GPT-2,
[01:23:53.080 --> 01:23:56.400]   again, it's the same architecture, just more of it.
[01:23:56.400 --> 01:23:59.320]   And yet it's able to do so much more.
[01:23:59.320 --> 01:24:00.760]   So if you start thinking about sort of
[01:24:00.760 --> 01:24:04.040]   what's the future of that, GPT-4 and GPT-5,
[01:24:04.040 --> 01:24:06.240]   do you really need fundamentally different architectures
[01:24:06.240 --> 01:24:08.760]   or do you just need a ton more hardware?
[01:24:08.760 --> 01:24:11.320]   And we do have a ton more hardware.
[01:24:11.320 --> 01:24:13.520]   Like these systems are nowhere near
[01:24:13.520 --> 01:24:16.480]   what humans have between our ears.
[01:24:16.480 --> 01:24:20.560]   So, you know, there's something to be said about
[01:24:20.560 --> 01:24:23.680]   stay tuned for emergent behavior.
[01:24:23.680 --> 01:24:25.480]   We keep thinking that general intelligence
[01:24:25.480 --> 01:24:28.440]   might just be forever away,
[01:24:28.440 --> 01:24:30.520]   but it could just simply be that
[01:24:30.520 --> 01:24:32.600]   we just need a ton more hardware
[01:24:32.600 --> 01:24:34.360]   and that humans are just not that different
[01:24:34.360 --> 01:24:37.960]   from the great apes, except for just a ton more of it.
[01:24:37.960 --> 01:24:41.760]   - Yeah, it's interesting that in the AI community,
[01:24:41.760 --> 01:24:44.240]   maybe there's a human-centric fear,
[01:24:44.240 --> 01:24:49.240]   but the notion that GPT-10 will achieve general intelligence
[01:24:49.240 --> 01:24:51.840]   is something that people shy away from,
[01:24:51.840 --> 01:24:54.200]   that there has to be something totally different
[01:24:54.200 --> 01:24:56.120]   and new added to this.
[01:24:56.120 --> 01:24:59.200]   And yet it's not seriously considered that
[01:25:01.000 --> 01:25:05.040]   this very simple thing, this very simple architecture,
[01:25:05.040 --> 01:25:07.360]   when scaled, might be the thing
[01:25:07.360 --> 01:25:09.000]   that achieves super intelligence.
[01:25:09.000 --> 01:25:11.480]   - And people think the same way about humanity
[01:25:11.480 --> 01:25:13.080]   and human consciousness.
[01:25:13.080 --> 01:25:15.080]   They're like, "Oh, consciousness might be quantum
[01:25:15.080 --> 01:25:18.120]   "or it might be some non-physical thing."
[01:25:18.120 --> 01:25:21.560]   And it's like, or it could just be a lot more
[01:25:21.560 --> 01:25:25.760]   of the same hardware that now is sufficiently capable
[01:25:25.760 --> 01:25:29.320]   of self-awareness just because it has the neurons to do it.
[01:25:29.320 --> 01:25:32.720]   So maybe the consciousness that is so elusive
[01:25:32.720 --> 01:25:38.200]   is an emergent behavior of you basically string together
[01:25:38.200 --> 01:25:41.240]   all these cognitive capabilities that come from running,
[01:25:41.240 --> 01:25:43.160]   from seeing, from reacting,
[01:25:43.160 --> 01:25:45.560]   from predicting the movement of a fly
[01:25:45.560 --> 01:25:47.240]   as you're catching it through the air.
[01:25:47.240 --> 01:25:49.440]   All of these things are just like great lookup tables
[01:25:49.440 --> 01:25:51.320]   encoded in a giant neural network.
[01:25:51.320 --> 01:25:52.960]   I mean, I'm oversimplifying, of course,
[01:25:52.960 --> 01:25:54.760]   the complexity and the diversity
[01:25:54.760 --> 01:25:57.160]   of the different types of excitatory and inhibitory neurons,
[01:25:57.160 --> 01:26:02.160]   the waveforms that sort of shine through the connections
[01:26:02.160 --> 01:26:04.640]   across all these different layers,
[01:26:04.640 --> 01:26:06.800]   the amalgamation of signals, et cetera.
[01:26:06.800 --> 01:26:08.400]   The brain is enormously complex.
[01:26:08.400 --> 01:26:09.920]   I mean, of course, but again,
[01:26:09.920 --> 01:26:11.520]   it's a small number of primitives
[01:26:11.520 --> 01:26:14.200]   encoded by a tiny number of genes,
[01:26:14.200 --> 01:26:19.200]   which are self-organized and shaped by their environment.
[01:26:19.200 --> 01:26:25.280]   Babies that are growing up today are listening to language
[01:26:26.360 --> 01:26:28.400]   from conception.
[01:26:28.400 --> 01:26:32.200]   Basically, as soon as the auditory apparatus forms,
[01:26:32.200 --> 01:26:35.040]   it's already getting shaped to the types of signals
[01:26:35.040 --> 01:26:37.120]   that are out in the real world today.
[01:26:37.120 --> 01:26:39.240]   So it's not just like, oh, have an Egyptian be born
[01:26:39.240 --> 01:26:40.440]   and then ship them over.
[01:26:40.440 --> 01:26:44.200]   It's like, no, that Egyptian would be listening in
[01:26:44.200 --> 01:26:46.480]   to the complex of the world and then getting born
[01:26:46.480 --> 01:26:49.520]   and sort of seeing just how much more complex the world is.
[01:26:49.520 --> 01:26:53.840]   So it's a combination of the underlying hardware,
[01:26:53.840 --> 01:26:57.440]   which if you think about as a geneticist,
[01:26:57.440 --> 01:27:00.000]   in my view, the hardware gives you an upper bound
[01:27:00.000 --> 01:27:02.160]   of cognitive capabilities,
[01:27:02.160 --> 01:27:05.160]   but it's the environment that makes those capabilities shine
[01:27:05.160 --> 01:27:06.880]   and reach their maximum.
[01:27:06.880 --> 01:27:10.960]   So we're a combination of nature and nurture.
[01:27:10.960 --> 01:27:15.240]   The nature is our genes and our cognitive apparatus,
[01:27:15.240 --> 01:27:18.520]   and the nurture is the richness of the environment
[01:27:18.520 --> 01:27:22.040]   that makes that cognitive apparatus reach its potential.
[01:27:22.040 --> 01:27:27.040]   And we are so far from reaching our full potential, so far.
[01:27:27.040 --> 01:27:30.960]   I think that kids being born a hundred years from now,
[01:27:30.960 --> 01:27:33.400]   they'll be looking at us now and saying
[01:27:33.400 --> 01:27:36.400]   what primitive educational systems they had.
[01:27:36.400 --> 01:27:39.200]   I can't believe people were not wired into this,
[01:27:39.200 --> 01:27:42.640]   you know, virtual reality from birth as we are now,
[01:27:42.640 --> 01:27:46.000]   'cause like they're clearly inferior and so on and so forth.
[01:27:46.000 --> 01:27:48.000]   So I basically think that our environment
[01:27:48.000 --> 01:27:53.000]   will continue exploding and our cognitive capabilities,
[01:27:53.000 --> 01:27:55.200]   it's not like, oh, we're only using 10% of our brain.
[01:27:55.200 --> 01:27:56.040]   That's ridiculous.
[01:27:56.040 --> 01:27:57.600]   Of course, we're using a hundred percent of our brain,
[01:27:57.600 --> 01:27:59.880]   but it's still constrained
[01:27:59.880 --> 01:28:02.080]   by how complex our environment is.
[01:28:02.080 --> 01:28:03.800]   - So the hardware will remain the same,
[01:28:03.800 --> 01:28:08.680]   but the software in a quickly advancing environment,
[01:28:08.680 --> 01:28:10.760]   the software will make a huge difference
[01:28:10.760 --> 01:28:14.280]   in the nature of like the human experience,
[01:28:14.280 --> 01:28:15.480]   the human condition.
[01:28:15.480 --> 01:28:17.120]   It's fascinating to think that humans
[01:28:17.120 --> 01:28:19.440]   will look very different a hundred years from now,
[01:28:19.440 --> 01:28:20.720]   just because the environment changed,
[01:28:20.720 --> 01:28:22.960]   even though we're still the same great apes,
[01:28:22.960 --> 01:28:25.440]   the descendant of apes.
[01:28:25.440 --> 01:28:28.600]   At the core of this is kind of a notion of ideas
[01:28:28.600 --> 01:28:31.000]   that I don't know if you're,
[01:28:31.000 --> 01:28:32.240]   there's a lot of people that's,
[01:28:32.240 --> 01:28:34.960]   including you eloquently about this topic,
[01:28:34.960 --> 01:28:39.480]   but Richard Dawkins talks about the notion of memes
[01:28:39.480 --> 01:28:43.740]   and let's say this notion of ideas,
[01:28:45.120 --> 01:28:49.120]   you know, multiplying, selecting in the minds of humans.
[01:28:49.120 --> 01:28:52.440]   Do you ever think about ideas from that perspective,
[01:28:52.440 --> 01:28:54.720]   ideas as organisms themselves
[01:28:54.720 --> 01:28:57.600]   that are breeding in the minds of humans?
[01:28:57.600 --> 01:28:59.520]   - I love the concept of memes.
[01:28:59.520 --> 01:29:03.640]   I love the concept of these horizontal transfer of ideas
[01:29:03.640 --> 01:29:06.520]   and sort of permeating through, you know,
[01:29:06.520 --> 01:29:11.160]   our layer of interconnected neural networks.
[01:29:11.160 --> 01:29:15.160]   So you can think of sort of the cognitive space
[01:29:15.160 --> 01:29:18.040]   that has now connected all of humanity,
[01:29:18.040 --> 01:29:22.480]   where we are now one giant information
[01:29:22.480 --> 01:29:24.840]   and idea sharing network,
[01:29:24.840 --> 01:29:28.880]   well beyond what was thought to be ever capable
[01:29:28.880 --> 01:29:32.600]   when the concept of a meme was created by Richard Dawkins.
[01:29:32.600 --> 01:29:36.560]   So, but I want to take that concept just to,
[01:29:36.560 --> 01:29:39.240]   you know, into another twist,
[01:29:39.240 --> 01:29:44.240]   which is the horizontal transfer of humans with fellowships.
[01:29:44.240 --> 01:29:51.120]   And the fact that as people apply to MIT
[01:29:51.120 --> 01:29:53.080]   from around the world,
[01:29:53.080 --> 01:29:55.680]   there's a selection that happens,
[01:29:55.680 --> 01:29:58.240]   not just for their ideas,
[01:29:58.240 --> 01:30:00.640]   but also for the cognitive hardware
[01:30:00.640 --> 01:30:02.960]   that came up with those ideas.
[01:30:02.960 --> 01:30:05.280]   So we don't just ship ideas around anymore.
[01:30:05.280 --> 01:30:07.240]   They don't evolve in a vacuum.
[01:30:07.240 --> 01:30:10.120]   The ideas themselves influence
[01:30:10.120 --> 01:30:12.520]   the distribution of cognitive systems,
[01:30:12.520 --> 01:30:15.520]   i.e. humans and brains, around the planet.
[01:30:15.520 --> 01:30:17.160]   - Yeah, we ship them to different locations
[01:30:17.160 --> 01:30:18.360]   based on their properties.
[01:30:18.360 --> 01:30:19.640]   - That's exactly right.
[01:30:19.640 --> 01:30:23.440]   So those cognitive systems that think of, you know,
[01:30:23.440 --> 01:30:26.120]   physics, for example, might go to CERN,
[01:30:26.120 --> 01:30:28.320]   and those that think of genomics
[01:30:28.320 --> 01:30:30.480]   might go to the Broad Institute,
[01:30:30.480 --> 01:30:32.120]   and those that think of computer science
[01:30:32.120 --> 01:30:34.980]   might go to, I don't know, Stanford or CMU or MIT.
[01:30:35.840 --> 01:30:38.680]   And you basically have this co-evolution now
[01:30:38.680 --> 01:30:41.240]   of memes and ideas,
[01:30:41.240 --> 01:30:44.680]   and the cognitive conversational systems
[01:30:44.680 --> 01:30:47.240]   that love these ideas and feed on these ideas
[01:30:47.240 --> 01:30:50.160]   and understand these ideas and appreciate these ideas,
[01:30:50.160 --> 01:30:52.080]   now coming together.
[01:30:52.080 --> 01:30:56.000]   So you basically have students coming to Boston to study,
[01:30:56.000 --> 01:30:57.320]   because that's the place where
[01:30:57.320 --> 01:30:59.280]   these type of cognitive systems thrive.
[01:30:59.280 --> 01:31:05.600]   And they're selected based on their cognitive output
[01:31:05.600 --> 01:31:08.000]   and their idea output.
[01:31:08.000 --> 01:31:10.640]   But once they get into that place,
[01:31:10.640 --> 01:31:15.000]   the boiling and interbreeding of these memes
[01:31:15.000 --> 01:31:17.680]   becomes so much more frequent.
[01:31:17.680 --> 01:31:21.560]   That what comes out of it is so far beyond
[01:31:21.560 --> 01:31:24.000]   if ideas were evolving in a vacuum
[01:31:24.000 --> 01:31:25.840]   of an already established hardware
[01:31:25.840 --> 01:31:28.560]   cognitive interconnection system of the planet,
[01:31:28.560 --> 01:31:32.800]   where now you basically have the ideas
[01:31:32.800 --> 01:31:35.080]   shaping the distribution of these systems,
[01:31:35.080 --> 01:31:37.680]   and then the genetics kick in as well.
[01:31:37.680 --> 01:31:40.080]   You basically have now these people
[01:31:40.080 --> 01:31:42.000]   who came to be a student, kind of like myself,
[01:31:42.000 --> 01:31:45.240]   who now stuck around and are now professors,
[01:31:45.240 --> 01:31:49.880]   bringing up our own genetically encoded
[01:31:49.880 --> 01:31:53.280]   and genetically related cognitive systems.
[01:31:53.280 --> 01:31:56.080]   Mine are eight, six, and three years old,
[01:31:56.080 --> 01:31:58.000]   who are now growing up in an environment
[01:31:58.000 --> 01:32:02.440]   surrounded by other cognitive systems of a similar age,
[01:32:02.440 --> 01:32:06.320]   with parents who love these types of thinking and ideas.
[01:32:06.320 --> 01:32:09.200]   And you basically have a whole interbreeding now
[01:32:09.200 --> 01:32:14.200]   of genetically selected transfer of cognitive systems,
[01:32:14.200 --> 01:32:21.320]   where the genes and the memes are co-evolving
[01:32:21.320 --> 01:32:25.280]   the same soup of ever improving knowledge
[01:32:25.280 --> 01:32:30.160]   and societal inter-fertilization,
[01:32:30.160 --> 01:32:31.800]   cross-fertilization of these ideas.
[01:32:31.800 --> 01:32:34.760]   - So this beautiful image,
[01:32:34.760 --> 01:32:37.000]   so these are shipping these actual
[01:32:37.000 --> 01:32:39.480]   meat cognitive systems to physical locations.
[01:32:39.480 --> 01:32:42.360]   They tend to cluster in,
[01:32:42.360 --> 01:32:45.280]   the biology ones cluster in a certain building too,
[01:32:45.280 --> 01:32:49.480]   so within that there's clusters on top of clusters
[01:32:49.480 --> 01:32:50.320]   on top of clusters.
[01:32:50.320 --> 01:32:52.680]   What about in the online world?
[01:32:52.680 --> 01:32:55.280]   Is that, do you also see that kind of,
[01:32:55.280 --> 01:32:58.880]   because people now form groups on the internet
[01:32:58.880 --> 01:33:00.120]   that they stick together,
[01:33:00.120 --> 01:33:03.760]   so they can sort of,
[01:33:03.760 --> 01:33:08.600]   these cognitive systems can collect themselves
[01:33:08.600 --> 01:33:13.600]   and breed together in different layers of spaces.
[01:33:13.600 --> 01:33:15.880]   It doesn't just have to be physical space.
[01:33:15.880 --> 01:33:17.120]   - Absolutely, absolutely.
[01:33:17.120 --> 01:33:19.680]   So basically there's the physical rearrangement,
[01:33:19.680 --> 01:33:21.800]   but there's also the conglomeration
[01:33:21.800 --> 01:33:24.280]   of the same cognitive system.
[01:33:24.280 --> 01:33:25.800]   Doesn't need to be, I eat human.
[01:33:25.800 --> 01:33:26.760]   (both laughing)
[01:33:26.760 --> 01:33:29.120]   Doesn't need to belong to only one community.
[01:33:29.120 --> 01:33:30.320]   So yes, you might be a member
[01:33:30.320 --> 01:33:31.560]   of the computer science department,
[01:33:31.560 --> 01:33:33.760]   but you can also hang out in the biology department,
[01:33:33.760 --> 01:33:35.800]   but you might also go online into,
[01:33:35.800 --> 01:33:38.640]   I don't know, poetry department readings and so on so forth,
[01:33:38.640 --> 01:33:40.600]   or you might be part of a group
[01:33:40.600 --> 01:33:42.800]   that only has 12 people in the world,
[01:33:42.800 --> 01:33:45.280]   but that are connected through their ideas
[01:33:45.280 --> 01:33:49.360]   and are now interbreeding these ideas in a whole other way.
[01:33:49.360 --> 01:33:53.160]   So this co-evolution of genes and memes
[01:33:53.160 --> 01:33:55.240]   is not just physically instantiated,
[01:33:55.240 --> 01:33:58.000]   it's also sort of rearranged
[01:33:58.000 --> 01:34:01.720]   in this cognitive space as well.
[01:34:01.720 --> 01:34:05.080]   - And sometimes these cognitive systems hold conferences
[01:34:05.080 --> 01:34:06.800]   and they all gather around
[01:34:06.800 --> 01:34:09.040]   and there's like one of them is like talking
[01:34:09.040 --> 01:34:11.320]   and they're all like listening and then they discuss
[01:34:11.320 --> 01:34:13.120]   and then they have free lunch and so on.
[01:34:13.120 --> 01:34:15.960]   - No, but then that's where you find students
[01:34:15.960 --> 01:34:18.240]   where, you know, when I go to a conference,
[01:34:18.240 --> 01:34:20.920]   I go through the posters where I'm on a mission.
[01:34:20.920 --> 01:34:22.880]   Basically my mission is to read
[01:34:22.880 --> 01:34:25.080]   and understand what every poster is about.
[01:34:25.080 --> 01:34:25.920]   And for a few of them,
[01:34:25.920 --> 01:34:27.640]   I'll dive deeply and understand everything,
[01:34:27.640 --> 01:34:29.920]   but I make it a point to just go poster after poster
[01:34:29.920 --> 01:34:31.800]   in order to read all of them.
[01:34:31.800 --> 01:34:35.440]   And I find some gems and students that I speak to
[01:34:35.440 --> 01:34:37.800]   that sometimes eventually join my lab.
[01:34:37.800 --> 01:34:41.800]   And then sort of you're sort of creating this permeation
[01:34:41.800 --> 01:34:46.400]   of, you know, the transfer of ideas,
[01:34:46.400 --> 01:34:48.280]   of ways of thinking,
[01:34:48.280 --> 01:34:52.960]   and very often of moral values, of social structures,
[01:34:52.960 --> 01:34:57.960]   of, you know, just more imperceptible properties
[01:34:57.960 --> 01:35:02.680]   of these cognitive systems that simply just cling together.
[01:35:02.680 --> 01:35:04.760]   Basically, you know, there's,
[01:35:04.760 --> 01:35:09.120]   I have the luxury at MIT of not just choosing smart people,
[01:35:09.120 --> 01:35:12.680]   but choosing smart people who I get along with,
[01:35:12.680 --> 01:35:17.680]   who are generous and friendly and creative and smart
[01:35:17.680 --> 01:35:22.280]   and, you know, excited and childish
[01:35:22.280 --> 01:35:26.000]   in their uninhibited behaviors and so on and so forth.
[01:35:26.000 --> 01:35:29.800]   So you basically can choose yourself to surround,
[01:35:29.800 --> 01:35:31.240]   you can choose to surround yourself
[01:35:31.240 --> 01:35:36.120]   with people who are not only cognitively compatible,
[01:35:36.120 --> 01:35:39.760]   but also, you know, imperceptibly
[01:35:39.760 --> 01:35:43.560]   through the meta cognitive systems compatible.
[01:35:43.560 --> 01:35:46.720]   And again, when I say compatible, not all the same.
[01:35:46.720 --> 01:35:50.560]   Sometimes, you know, not sometimes, all the time,
[01:35:50.560 --> 01:35:53.520]   the teams are made out of complementary components,
[01:35:53.520 --> 01:35:56.240]   not just compatible, but very often complementary.
[01:35:56.240 --> 01:35:59.600]   So in my own team, I have a diversity of students
[01:35:59.600 --> 01:36:01.920]   who come from very different backgrounds.
[01:36:01.920 --> 01:36:03.480]   There's a whole spectrum of biology
[01:36:03.480 --> 01:36:04.880]   to computation, of course,
[01:36:04.880 --> 01:36:06.680]   but within biology, there's a lot of realms,
[01:36:06.680 --> 01:36:08.840]   within computation, there's a lot of realms.
[01:36:08.840 --> 01:36:13.360]   And what makes us click so well together
[01:36:13.360 --> 01:36:16.640]   is the fact that not only do we have a common mission,
[01:36:16.640 --> 01:36:20.360]   a common passion, and a common, you know,
[01:36:20.360 --> 01:36:24.600]   view of the world, but that we're complementary
[01:36:24.600 --> 01:36:27.480]   in our skills, in our angles with which we accommodate,
[01:36:27.480 --> 01:36:28.320]   and so on and so forth,
[01:36:28.320 --> 01:36:29.720]   and that's sort of what makes it click.
[01:36:29.720 --> 01:36:32.880]   - Yeah, it's fascinating that the stickiness
[01:36:32.880 --> 01:36:36.040]   of multiple cognitive systems together
[01:36:36.040 --> 01:36:38.040]   includes both the commonality,
[01:36:38.040 --> 01:36:40.800]   so you meet because there's some common thing,
[01:36:40.800 --> 01:36:45.240]   but you stick together because you're different
[01:36:45.240 --> 01:36:46.760]   in all the useful ways.
[01:36:46.760 --> 01:36:48.440]   - Yeah, yeah, and my wife and I,
[01:36:48.440 --> 01:36:51.840]   I mean, we adore each other, like, to pieces,
[01:36:51.840 --> 01:36:54.320]   but we're also extremely different in many ways.
[01:36:54.320 --> 01:36:55.920]   - Careful. - And that's beautiful.
[01:36:55.920 --> 01:36:57.120]   - She's gonna be listening to this.
[01:36:57.120 --> 01:36:59.040]   - But I love that about us.
[01:36:59.040 --> 01:37:00.760]   I love the fact that, you know,
[01:37:00.760 --> 01:37:03.600]   I'm like living out there in the world of ideas
[01:37:03.600 --> 01:37:05.600]   and I forget what day it is,
[01:37:05.600 --> 01:37:07.040]   and she's like, "Well, at 8 a.m.,
[01:37:07.040 --> 01:37:08.200]   the kids better be to school."
[01:37:08.200 --> 01:37:09.280]   (both laughing)
[01:37:09.280 --> 01:37:12.600]   And, you know, I do get yelled at,
[01:37:13.920 --> 01:37:15.480]   but I need it.
[01:37:15.480 --> 01:37:18.120]   Basically, I need her as much as she needs me,
[01:37:18.120 --> 01:37:20.200]   and she loves interacting with me and talking.
[01:37:20.200 --> 01:37:23.560]   I mean, you know, last night, we were talking about this,
[01:37:23.560 --> 01:37:24.600]   and I showed her the questions,
[01:37:24.600 --> 01:37:26.560]   and we were bouncing ideas of each other,
[01:37:26.560 --> 01:37:28.040]   and it was just beautiful.
[01:37:28.040 --> 01:37:30.120]   Like, we basically have these, you know,
[01:37:30.120 --> 01:37:34.000]   basically cognitive, you know,
[01:37:34.000 --> 01:37:36.120]   let it all loose kind of dates,
[01:37:36.120 --> 01:37:38.280]   where, you know, we just bring papers,
[01:37:38.280 --> 01:37:41.040]   and we're like, you know, bouncing ideas, et cetera.
[01:37:41.040 --> 01:37:44.560]   So, you know, we have extremely different perspectives,
[01:37:44.560 --> 01:37:49.560]   but very common, you know, goals and interests, and anyway.
[01:37:49.560 --> 01:37:52.240]   - What do you make of the communication mechanism
[01:37:52.240 --> 01:37:54.160]   that we humans use to share those ideas?
[01:37:54.160 --> 01:37:57.320]   'Cause like one essential element of all of this
[01:37:57.320 --> 01:38:01.520]   is not just that we're able to have these ideas,
[01:38:01.520 --> 01:38:03.760]   but we're also able to share them.
[01:38:03.760 --> 01:38:06.080]   We tend to, maybe you can correct me,
[01:38:06.080 --> 01:38:10.080]   but we seem to use language to share the ideas.
[01:38:10.080 --> 01:38:12.600]   Maybe we share them in some much deeper way than language,
[01:38:12.600 --> 01:38:13.440]   I don't know.
[01:38:13.440 --> 01:38:15.800]   But what do you make of this whole mechanism,
[01:38:15.800 --> 01:38:18.720]   and how fundamental it is to the human condition?
[01:38:18.720 --> 01:38:20.120]   - So some people will tell you
[01:38:20.120 --> 01:38:23.160]   that your language dictates your thoughts,
[01:38:23.160 --> 01:38:26.280]   and your thoughts cannot form outside language.
[01:38:26.280 --> 01:38:27.920]   I tend to disagree.
[01:38:27.920 --> 01:38:32.920]   I see thoughts as much more abstract,
[01:38:32.920 --> 01:38:35.240]   as, you know, basically when I dream,
[01:38:35.240 --> 01:38:38.360]   I don't dream in words, I dream in shapes, and forms,
[01:38:38.360 --> 01:38:42.400]   and, you know, three-dimensional space with extreme detail.
[01:38:42.400 --> 01:38:44.320]   I was describing, so when I wake up
[01:38:44.320 --> 01:38:47.160]   in the middle of the night, I actually record my dreams.
[01:38:47.160 --> 01:38:50.040]   Sometimes I write them down in a Dropbox file.
[01:38:50.040 --> 01:38:53.940]   Other times I'll just dictate them in, you know, audio.
[01:38:53.940 --> 01:38:57.440]   And my wife was giving me a massage the other day,
[01:38:57.440 --> 01:39:00.000]   'cause like my left side was frozen,
[01:39:00.000 --> 01:39:02.760]   and I started playing the recording.
[01:39:02.760 --> 01:39:05.440]   And as I was listening to it, I was like,
[01:39:05.440 --> 01:39:06.280]   I don't remember any of that.
[01:39:06.280 --> 01:39:08.000]   And I was like, of course!
[01:39:08.000 --> 01:39:10.240]   And then the entire thing came back.
[01:39:10.240 --> 01:39:12.400]   But then there's no way any other person
[01:39:12.400 --> 01:39:15.480]   could have recreated that entire, sort of,
[01:39:15.480 --> 01:39:20.200]   you know, three-dimensional shape, and dream, and concept.
[01:39:20.200 --> 01:39:22.560]   And in the same way, when I'm thinking of ideas,
[01:39:22.560 --> 01:39:25.320]   there's so many ideas I can't put to words.
[01:39:25.320 --> 01:39:27.120]   I mean, I will describe them with a thousand words,
[01:39:27.120 --> 01:39:29.880]   but the idea itself is much more precise,
[01:39:29.880 --> 01:39:31.520]   or much more sort of abstract,
[01:39:31.520 --> 01:39:34.080]   or much more something, you know, different.
[01:39:34.080 --> 01:39:36.400]   It's either less abstract or more abstract,
[01:39:36.400 --> 01:39:39.080]   and it's either, you know, basically,
[01:39:39.080 --> 01:39:42.760]   there's a projection that happens
[01:39:42.760 --> 01:39:44.640]   from the three-dimensional ideas
[01:39:44.640 --> 01:39:46.880]   into, let's say, a one-dimensional language.
[01:39:46.880 --> 01:39:51.040]   And the language certainly gives you the apparatus
[01:39:51.040 --> 01:39:52.460]   to think about concepts
[01:39:52.460 --> 01:39:54.960]   that you didn't realize existed before.
[01:39:54.960 --> 01:39:57.640]   And with my team, we often create new words.
[01:39:57.640 --> 01:39:59.440]   I'm like, well, now we're gonna call this
[01:39:59.440 --> 01:40:01.360]   the regulatory plexus of a gene.
[01:40:01.360 --> 01:40:02.920]   And that gives us now the language
[01:40:02.920 --> 01:40:05.800]   to sort of build on that as one concept
[01:40:05.800 --> 01:40:09.080]   that you then build upon with all kinds of other things.
[01:40:09.080 --> 01:40:13.120]   So there's this co-evolution again of ideas and language,
[01:40:13.120 --> 01:40:16.520]   but they're not one-to-one with each other.
[01:40:16.520 --> 01:40:21.520]   Now let's talk about language itself, words, sentences.
[01:40:21.520 --> 01:40:25.200]   This is a very distant construct
[01:40:25.200 --> 01:40:27.520]   from where language actually begun.
[01:40:27.520 --> 01:40:29.520]   So if you look at how we communicate,
[01:40:29.520 --> 01:40:34.120]   as I'm speaking, my eyes are shining,
[01:40:34.120 --> 01:40:37.120]   and my face is changing through all kinds of emotions,
[01:40:37.120 --> 01:40:42.120]   and my entire body composition posture is reshaped,
[01:40:42.120 --> 01:40:45.360]   and my intonation, the pauses that I make,
[01:40:45.360 --> 01:40:48.360]   the softer and the louder and the this and that
[01:40:48.360 --> 01:40:51.800]   are conveying so much more information.
[01:40:51.800 --> 01:40:55.300]   And if you look at early human language,
[01:40:55.300 --> 01:40:57.640]   and if you look at how, you know,
[01:40:57.640 --> 01:40:59.520]   the great apes communicate with each other,
[01:40:59.520 --> 01:41:01.360]   there's a lot of grunting, there's a lot of posturing,
[01:41:01.360 --> 01:41:02.400]   there's a lot of emotions,
[01:41:02.400 --> 01:41:04.760]   there's a lot of sort of shrieking, et cetera.
[01:41:04.760 --> 01:41:09.760]   They have a lot of components of our human language,
[01:41:09.760 --> 01:41:11.560]   just not the words.
[01:41:11.560 --> 01:41:15.960]   So I think of human communication
[01:41:15.960 --> 01:41:21.420]   as combining the ape component,
[01:41:21.420 --> 01:41:25.220]   but also of course, the, you know, GPT-3 component.
[01:41:25.220 --> 01:41:27.280]   So basically there's the cognitive layer
[01:41:27.280 --> 01:41:30.120]   and the reasoning layer that we share
[01:41:30.120 --> 01:41:32.680]   with different parts of our relatives.
[01:41:32.680 --> 01:41:34.280]   There's the AI relatives,
[01:41:34.280 --> 01:41:36.840]   but there's also the grunting relatives.
[01:41:36.840 --> 01:41:40.040]   And what I love about humanity is that we have both.
[01:41:40.040 --> 01:41:42.400]   We're not just a conversational system.
[01:41:42.400 --> 01:41:46.800]   We're a grunting, emotionally charged, you know,
[01:41:46.800 --> 01:41:51.080]   weirdly interconnected system
[01:41:51.080 --> 01:41:53.600]   that also has the ability to reason.
[01:41:53.600 --> 01:41:56.620]   And when we communicate with each other,
[01:41:56.620 --> 01:41:59.000]   there's so much more than just language.
[01:41:59.000 --> 01:42:01.160]   There's so much more than just words.
[01:42:01.160 --> 01:42:03.760]   - It does seem like we're able to somehow transfer
[01:42:03.760 --> 01:42:06.680]   even more than the body language.
[01:42:06.680 --> 01:42:08.800]   It seems that in the room with us
[01:42:08.800 --> 01:42:13.800]   is always a giant knowledge base of like shared experiences,
[01:42:13.800 --> 01:42:16.520]   different perspectives on those experiences,
[01:42:16.520 --> 01:42:20.200]   but I don't know, the knowledge of who the last three,
[01:42:20.200 --> 01:42:22.240]   four presidents in the United States was,
[01:42:22.240 --> 01:42:25.680]   and just all the, you know, 9/11, the tragedies in 9/11,
[01:42:25.680 --> 01:42:29.120]   all the beautiful and terrible things
[01:42:29.120 --> 01:42:29.960]   that happened in the world,
[01:42:29.960 --> 01:42:32.640]   they're somehow both in our minds
[01:42:32.640 --> 01:42:37.080]   and somehow enrich the ability to transfer information.
[01:42:37.080 --> 01:42:39.080]   - What I love about it is I can talk to you
[01:42:39.080 --> 01:42:40.440]   about 2001 Audience of Space
[01:42:40.440 --> 01:42:41.920]   and mention a very specific scene,
[01:42:41.920 --> 01:42:44.520]   and that evokes all these feelings that you had
[01:42:44.520 --> 01:42:45.520]   when you first watched it.
[01:42:45.520 --> 01:42:48.200]   - We're both visualizing that, maybe in different ways.
[01:42:48.200 --> 01:42:49.040]   - Exactly.
[01:42:49.040 --> 01:42:50.400]   - But in the, yeah.
[01:42:50.400 --> 01:42:55.400]   And not only that, but the feeling is broad-based.
[01:42:55.400 --> 01:42:58.160]   Broad back up, just like you said, with the dreams.
[01:42:58.160 --> 01:43:01.160]   We both have that feeling arise in some form
[01:43:01.160 --> 01:43:04.600]   as you bring up the, how, you know,
[01:43:04.600 --> 01:43:07.160]   facing his own mortality.
[01:43:07.160 --> 01:43:09.280]   It's fascinating that we're able to do that,
[01:43:09.280 --> 01:43:10.600]   but I don't know.
[01:43:10.600 --> 01:43:12.520]   - Now let's talk about Neuralink for a second.
[01:43:12.520 --> 01:43:14.600]   So what's the concept of Neuralink?
[01:43:14.600 --> 01:43:17.080]   The concept of Neuralink is that I'm gonna take
[01:43:17.080 --> 01:43:19.720]   whatever knowledge is encoded in my brain,
[01:43:19.720 --> 01:43:22.140]   directly transfer it into your brain.
[01:43:22.140 --> 01:43:25.240]   So this is a beautiful, fascinating,
[01:43:25.240 --> 01:43:29.180]   and extremely sort of, you know, appealing concept,
[01:43:29.180 --> 01:43:32.180]   but I see a lot of challenges surrounding that.
[01:43:32.180 --> 01:43:34.900]   The first one is we have no idea
[01:43:34.900 --> 01:43:36.780]   how to even begin to understand
[01:43:36.780 --> 01:43:40.140]   how knowledge is encoded in a person's brain.
[01:43:40.140 --> 01:43:41.900]   I mean, I told you about this paper that we had recently
[01:43:41.900 --> 01:43:45.420]   with Li-Hui Tsai and Asaf Marko,
[01:43:45.420 --> 01:43:47.980]   that basically was looking at these engrams
[01:43:47.980 --> 01:43:50.620]   that are formed with combinations of neurons
[01:43:50.620 --> 01:43:53.180]   that co-fire when a stimulus happens,
[01:43:53.180 --> 01:43:54.300]   where we can go into a mouse
[01:43:54.300 --> 01:43:56.820]   and select those neurons that fire by marking them,
[01:43:56.820 --> 01:43:58.620]   and then see what happens when they first fire,
[01:43:58.620 --> 01:44:00.820]   and then select the neurons that fire again
[01:44:00.820 --> 01:44:02.780]   when the experience is repeated.
[01:44:02.780 --> 01:44:04.900]   These are the recall neurons,
[01:44:04.900 --> 01:44:07.660]   and then there's the memory consolidation neurons.
[01:44:07.660 --> 01:44:09.940]   So we're starting to understand a little bit
[01:44:09.940 --> 01:44:14.140]   of sort of the distributed nature of knowledge encoding
[01:44:14.140 --> 01:44:16.600]   and experience encoding in the human brain
[01:44:16.600 --> 01:44:18.020]   and in the mouse brain.
[01:44:18.020 --> 01:44:22.460]   And the concept that we'll understand that sufficiently
[01:44:22.460 --> 01:44:26.700]   one day to be able to take a snapshot
[01:44:26.700 --> 01:44:31.700]   of what does that scene from Dave losing his mind,
[01:44:31.700 --> 01:44:34.340]   of Hal losing his mind and talking to Dave,
[01:44:34.340 --> 01:44:39.440]   how is that scene encoded in your mind?
[01:44:39.440 --> 01:44:41.440]   Imagine the complexity of that.
[01:44:41.440 --> 01:44:44.860]   But now imagine, suppose that we solve this problem,
[01:44:45.840 --> 01:44:48.840]   and the next enormous challenge is how do I go
[01:44:48.840 --> 01:44:51.080]   and modify the next person's brain
[01:44:51.080 --> 01:44:54.360]   to now create the same exact neural connections?
[01:44:54.360 --> 01:44:56.360]   So that's an enormous challenge right there.
[01:44:56.360 --> 01:44:59.000]   So basically it's not just reading, it's now writing.
[01:44:59.000 --> 01:45:02.200]   And again, what if something goes wrong?
[01:45:02.200 --> 01:45:04.120]   I don't wanna even think about that.
[01:45:04.120 --> 01:45:05.200]   That's number two.
[01:45:05.200 --> 01:45:09.600]   And number three, who says that the way that you encode,
[01:45:09.600 --> 01:45:11.560]   "Dave, I'm losing my mind,"
[01:45:11.560 --> 01:45:14.760]   and I encode, "Dave, I'm losing my mind,"
[01:45:14.760 --> 01:45:17.320]   is anywhere near each other.
[01:45:17.320 --> 01:45:20.040]   Basically, maybe the way that I'm encoding it
[01:45:20.040 --> 01:45:23.320]   is twisted with my childhood memories
[01:45:23.320 --> 01:45:27.120]   of running through the pebbles in Greece,
[01:45:27.120 --> 01:45:29.320]   and yours is twisted with your childhood memories
[01:45:29.320 --> 01:45:31.200]   of growing up in Russia.
[01:45:31.200 --> 01:45:34.720]   And there's no way that I can take my encoding
[01:45:34.720 --> 01:45:35.840]   and put it into your brain,
[01:45:35.840 --> 01:45:38.120]   'cause it'll A, mess things up,
[01:45:38.120 --> 01:45:43.000]   and B, be incompatible with your own unique experiences.
[01:45:43.000 --> 01:45:44.540]   - So that's telepathic communications.
[01:45:44.540 --> 01:45:46.240]   From human to human, that's fascinating.
[01:45:46.240 --> 01:45:51.240]   You're reminding us that there's two biological systems
[01:45:51.240 --> 01:45:54.000]   on both ends of that communication.
[01:45:54.000 --> 01:45:56.280]   The one, the easier, I guess,
[01:45:56.280 --> 01:45:59.680]   maybe half as difficult thing to do,
[01:45:59.680 --> 01:46:01.860]   and the hope with Neuralink is that
[01:46:01.860 --> 01:46:04.440]   we can communicate with an AI system.
[01:46:04.440 --> 01:46:08.820]   So where one side of that is a little bit more controllable,
[01:46:08.820 --> 01:46:13.320]   but even just that is exceptionally difficult.
[01:46:13.320 --> 01:46:16.840]   - Let's talk about two neuronal systems talking to each other.
[01:46:16.840 --> 01:46:19.240]   Suppose that GPT-4 tells GPT-3,
[01:46:19.240 --> 01:46:21.320]   "Hey, give me all your knowledge."
[01:46:21.320 --> 01:46:23.000]   Right? - Yeah, that's hilarious.
[01:46:23.000 --> 01:46:26.040]   - I have 10 times more hardware, I'm ready, just feed me.
[01:46:26.040 --> 01:46:27.320]   What's GPT-3 gonna do?
[01:46:27.320 --> 01:46:30.920]   Is it gonna say, "Oh, here's my 10 billion parameters?"
[01:46:30.920 --> 01:46:32.720]   - No. - No way.
[01:46:32.720 --> 01:46:35.080]   The simplest way, and perhaps the fastest way,
[01:46:35.080 --> 01:46:37.960]   for GPT-3 to transfer all its knowledge to its older body
[01:46:37.960 --> 01:46:39.860]   that has a lot more hardware,
[01:46:39.860 --> 01:46:44.860]   is to regenerate every single possible human sentence
[01:46:44.860 --> 01:46:46.780]   that it can possibly create.
[01:46:46.780 --> 01:46:48.140]   - Just keep talking.
[01:46:48.140 --> 01:46:50.820]   - Keep talking, and just re-encode it all together.
[01:46:50.820 --> 01:46:53.420]   So maybe what language does is exactly that.
[01:46:53.420 --> 01:46:56.780]   It's taking one generative cognitive model,
[01:46:56.780 --> 01:46:59.660]   it's running it forward to emit utterances
[01:46:59.660 --> 01:47:01.820]   that kind of make sense in my cognitive frame,
[01:47:01.820 --> 01:47:04.100]   and it's re-encoding them into yours
[01:47:04.100 --> 01:47:07.300]   through the parsing of that same language.
[01:47:07.300 --> 01:47:08.580]   - And I think the conversation
[01:47:08.580 --> 01:47:11.180]   might actually be the most efficient way to do it.
[01:47:11.180 --> 01:47:14.580]   So not just talking, but interactive.
[01:47:14.580 --> 01:47:18.380]   So talking back and forth, asking questions, interrupting.
[01:47:18.380 --> 01:47:20.780]   - So GPT-4 will constantly be interrupted.
[01:47:20.780 --> 01:47:22.660]   - Interrupted, just annoying.
[01:47:22.660 --> 01:47:24.260]   Annoying it. - Annoyingly.
[01:47:24.260 --> 01:47:25.100]   - Yeah.
[01:47:25.100 --> 01:47:27.740]   - But the beauty of that is also that
[01:47:27.740 --> 01:47:29.780]   as we're interrupting each other,
[01:47:29.780 --> 01:47:33.460]   there's all kinds of misinterpretations that happen.
[01:47:33.460 --> 01:47:36.620]   That, you know, basically when my students speak,
[01:47:36.620 --> 01:47:38.940]   I will often know that I'm misunderstanding
[01:47:38.940 --> 01:47:41.380]   what they're saying, and I'll be like,
[01:47:41.380 --> 01:47:43.060]   hold that thought for a second.
[01:47:43.060 --> 01:47:44.820]   Let me tell you what I think I understood,
[01:47:44.820 --> 01:47:46.740]   which I know is different from what you said.
[01:47:46.740 --> 01:47:48.980]   Then I'll say that, and then someone else
[01:47:48.980 --> 01:47:51.580]   in the same Zoom meeting will basically say,
[01:47:51.580 --> 01:47:54.100]   well, you know, here's another way to think
[01:47:54.100 --> 01:47:55.500]   about what you just said.
[01:47:55.500 --> 01:47:57.260]   And then by the third iteration,
[01:47:57.260 --> 01:47:59.220]   we're somewhere completely different
[01:47:59.220 --> 01:48:01.940]   that if we could actually communicate
[01:48:01.940 --> 01:48:06.060]   with full neural network parameters back and forth
[01:48:06.060 --> 01:48:09.060]   of that knowledge and idea and coding
[01:48:09.060 --> 01:48:14.020]   would be far inferior because the re-encoding
[01:48:14.020 --> 01:48:16.380]   with our own, as we said last time,
[01:48:16.380 --> 01:48:18.860]   emotional baggage and cognitive baggage
[01:48:18.860 --> 01:48:23.860]   from our unique experiences through our shared experiences,
[01:48:23.860 --> 01:48:27.940]   distinct encodings in the context
[01:48:27.940 --> 01:48:30.940]   of all our unique experiences is leading
[01:48:30.940 --> 01:48:35.940]   to so much more diversity of perceptions
[01:48:35.940 --> 01:48:38.060]   and perspectives, and again, going back
[01:48:38.060 --> 01:48:41.940]   to this whole concept of this entire network
[01:48:41.940 --> 01:48:44.380]   of all of human cognitive systems connected
[01:48:44.380 --> 01:48:47.060]   to each other and sort of how ideas
[01:48:47.060 --> 01:48:49.060]   and memes permeate through that,
[01:48:49.060 --> 01:48:52.180]   that's sort of what really creates a whole new level
[01:48:52.180 --> 01:48:57.180]   of human experience through this reasoning layer
[01:48:57.180 --> 01:49:02.740]   and this computational layer that obviously lives
[01:49:02.740 --> 01:49:04.860]   on top of our cognitive layer.
[01:49:04.860 --> 01:49:09.860]   - So you're one of these aforementioned cognitive systems,
[01:49:09.860 --> 01:49:16.500]   mortal but thoughtful, and you're connected
[01:49:16.500 --> 01:49:18.980]   to a bunch, like you said, students,
[01:49:18.980 --> 01:49:21.140]   your wife, your kids.
[01:49:21.140 --> 01:49:24.740]   What do you, in your brief time here on Earth,
[01:49:24.740 --> 01:49:27.240]   this is a Meaning of Life episode,
[01:49:27.240 --> 01:49:32.240]   so what do you hope this world will remember you as?
[01:49:32.780 --> 01:49:34.680]   What do you hope your legacy will be?
[01:49:34.680 --> 01:49:41.820]   - I don't think of legacy as much as maybe most people.
[01:49:41.820 --> 01:49:44.100]   - I thought all Greeks think of legacy.
[01:49:44.100 --> 01:49:44.940]   - Oh, it's kind of funny.
[01:49:44.940 --> 01:49:47.840]   I'm consciously living the present.
[01:49:47.840 --> 01:49:51.060]   Many students tell me, oh, give us some career advice.
[01:49:51.060 --> 01:49:52.520]   I'm like, I'm the wrong person.
[01:49:52.520 --> 01:49:54.300]   I've never made a career plan.
[01:49:54.300 --> 01:49:55.540]   I still have to make one.
[01:49:55.540 --> 01:49:57.780]   (laughing)
[01:49:59.260 --> 01:50:03.500]   It's funny to be both experiencing the past
[01:50:03.500 --> 01:50:05.580]   and the present and the future,
[01:50:05.580 --> 01:50:08.220]   but also consciously living in the present
[01:50:08.220 --> 01:50:12.020]   and just, there's a conscious decision we can make
[01:50:12.020 --> 01:50:13.980]   to not worry about all that,
[01:50:13.980 --> 01:50:16.740]   which again goes back to the I'm the lucky one
[01:50:16.740 --> 01:50:17.580]   kind of thing.
[01:50:17.580 --> 01:50:19.540]   (laughing)
[01:50:19.540 --> 01:50:22.960]   Of living in the present and being happy winning
[01:50:22.960 --> 01:50:24.260]   and being happy losing,
[01:50:24.260 --> 01:50:28.500]   and there's a certain freedom that comes with that,
[01:50:28.500 --> 01:50:32.460]   but again, a certain sort of, I don't know,
[01:50:32.460 --> 01:50:35.800]   ephemerity of living for the present.
[01:50:35.800 --> 01:50:39.700]   But if you step back from all of that,
[01:50:39.700 --> 01:50:44.580]   where basically my current modus operandi
[01:50:44.580 --> 01:50:46.340]   is live for the present,
[01:50:46.340 --> 01:50:50.460]   make every day the best you can make,
[01:50:50.460 --> 01:50:55.260]   and just make the local blip of local maxima
[01:50:55.260 --> 01:50:58.700]   of the universe, of the awesomeness of the planet
[01:50:58.700 --> 01:51:02.020]   and the town and the family that we live in,
[01:51:02.020 --> 01:51:05.820]   both academic family and biological family.
[01:51:05.820 --> 01:51:08.660]   Make it a little more awesome
[01:51:08.660 --> 01:51:09.860]   by being generous to your friends,
[01:51:09.860 --> 01:51:11.260]   being generous to the people around you,
[01:51:11.260 --> 01:51:13.580]   being kind to your enemies,
[01:51:13.580 --> 01:51:17.700]   and just showing love all around.
[01:51:17.700 --> 01:51:21.500]   You can't be upset at people if you truly love them.
[01:51:21.500 --> 01:51:23.340]   If somebody yells at you and insults you
[01:51:23.340 --> 01:51:25.740]   every time you say the slightest thing,
[01:51:25.740 --> 01:51:28.900]   and yet when you see them, you just see them with love,
[01:51:28.900 --> 01:51:31.620]   it's a beautiful feeling.
[01:51:31.620 --> 01:51:34.580]   It's like, I'm feeling exactly like
[01:51:34.580 --> 01:51:37.580]   when I look at my three-year-old who's screaming.
[01:51:37.580 --> 01:51:39.700]   Even though I love her and I want her good,
[01:51:39.700 --> 01:51:42.940]   she's still screaming and saying, "No, no, no, no, no."
[01:51:42.940 --> 01:51:47.060]   And I'm like, "I love you, genuinely love you."
[01:51:47.060 --> 01:51:49.580]   But I can sort of kind of see that your brain
[01:51:49.580 --> 01:51:53.860]   is kind of stuck in that little mode of anger.
[01:51:53.860 --> 01:51:58.260]   And there's plenty of people out there who don't like me,
[01:51:58.260 --> 01:52:01.460]   and I see them with love as a child
[01:52:01.460 --> 01:52:04.540]   that is stuck in a cognitive state
[01:52:04.540 --> 01:52:06.980]   that they're eventually gonna snap out of,
[01:52:06.980 --> 01:52:09.020]   or maybe not, and that's okay.
[01:52:09.020 --> 01:52:13.020]   So there's that aspect of sort of experiencing
[01:52:13.020 --> 01:52:17.020]   life with the best intentions.
[01:52:17.020 --> 01:52:20.420]   And I love when I'm wrong.
[01:52:20.420 --> 01:52:21.980]   I had a friend who was like
[01:52:21.980 --> 01:52:23.780]   one of the smartest people I've ever met,
[01:52:23.780 --> 01:52:26.300]   who would basically say, "Oh, I love it when I'm wrong
[01:52:26.300 --> 01:52:27.940]   "'cause it makes me feel human."
[01:52:27.940 --> 01:52:29.740]   (Zubin laughs)
[01:52:29.740 --> 01:52:31.140]   And it's so beautiful.
[01:52:31.140 --> 01:52:32.700]   I mean, she's really one of the smartest people
[01:52:32.700 --> 01:52:36.140]   I've ever met, and she was like, "Oh, it's such a good feeling."
[01:52:36.140 --> 01:52:41.140]   And I love being wrong, but there's something
[01:52:41.140 --> 01:52:43.900]   about self-improvement, there's something about sort of
[01:52:43.900 --> 01:52:46.900]   how do I not make the most mistakes,
[01:52:46.900 --> 01:52:50.860]   but attempt the most rights and do the fewest wrongs,
[01:52:50.860 --> 01:52:53.140]   but with the full knowledge that this will happen.
[01:52:53.140 --> 01:52:54.420]   That's one aspect.
[01:52:54.420 --> 01:53:00.300]   So through this life in the present,
[01:53:00.300 --> 01:53:02.700]   what's really funny is, and that's something
[01:53:02.700 --> 01:53:04.740]   that I've experienced more and more,
[01:53:04.740 --> 01:53:07.820]   really, thanks to you and through this podcast,
[01:53:07.820 --> 01:53:09.940]   is this enormous number of people
[01:53:09.940 --> 01:53:13.060]   who will basically comment, "Wow, I've been following
[01:53:13.060 --> 01:53:14.860]   "this guy for so many years now,"
[01:53:14.860 --> 01:53:17.340]   or, "Wow, this guy has inspired so many of us
[01:53:17.340 --> 01:53:19.540]   "in computational biology," and so on and so forth.
[01:53:19.540 --> 01:53:21.980]   And I'm like, "I don't know any of that,
[01:53:21.980 --> 01:53:25.220]   "but I'm only discovering this now through this sort of
[01:53:25.220 --> 01:53:29.260]   "sharing our emotional states and our cognitive states
[01:53:29.260 --> 01:53:32.900]   "with a wider audience," where suddenly,
[01:53:32.900 --> 01:53:36.260]   I'm sort of realizing that, wow, maybe I've had a legacy.
[01:53:36.260 --> 01:53:37.100]   - Yes.
[01:53:37.100 --> 01:53:39.100]   - Like basically, I've trained generations
[01:53:39.100 --> 01:53:43.580]   of students from MIT, and I've put all of my courses
[01:53:43.580 --> 01:53:47.740]   freely online since 2001.
[01:53:47.740 --> 01:53:50.140]   So basically, all of my video recordings of my lectures
[01:53:50.140 --> 01:53:52.220]   have been online since 2001.
[01:53:52.220 --> 01:53:56.100]   So countless generations of people from across the world
[01:53:56.100 --> 01:53:57.940]   will meet me at a conference and say,
[01:53:57.940 --> 01:54:01.060]   I was at this conference where somebody heard my voice
[01:54:01.060 --> 01:54:02.540]   and was like, "I know this voice.
[01:54:02.540 --> 01:54:04.540]   "I've been listening to your lectures."
[01:54:04.540 --> 01:54:06.260]   And it's just such a beautiful thing
[01:54:06.260 --> 01:54:11.260]   where we're sharing widely, and who knows
[01:54:11.400 --> 01:54:15.460]   which students will get where from whatever they catch
[01:54:15.460 --> 01:54:17.340]   out of these lectures, even if what they catch
[01:54:17.340 --> 01:54:20.720]   is just inspiration and passion and drive.
[01:54:20.720 --> 01:54:25.340]   So there's this intangible, you know, legacy,
[01:54:25.340 --> 01:54:27.920]   quote unquote, that every one of us has
[01:54:27.920 --> 01:54:29.720]   through the people we touch.
[01:54:29.720 --> 01:54:31.880]   One of my friends from undergrad basically told me,
[01:54:31.880 --> 01:54:33.560]   "Oh, my mom remembers you vividly
[01:54:33.560 --> 01:54:34.520]   "from when she came to campus."
[01:54:34.520 --> 01:54:36.080]   I'm like, "I didn't even meet her."
[01:54:36.080 --> 01:54:38.800]   She's like, "No, but she sort of saw you
[01:54:38.800 --> 01:54:40.040]   "interacting with people and said,
[01:54:40.040 --> 01:54:43.440]   "Wow, he's exuding this positive energy."
[01:54:43.440 --> 01:54:47.040]   And there's that aspect of sort of just motivating people
[01:54:47.040 --> 01:54:50.640]   with your kindness, with your passion, with your generosity,
[01:54:50.640 --> 01:54:55.360]   and with your, you know, just selflessness of, you know,
[01:54:55.360 --> 01:54:58.040]   just give, it doesn't matter where it goes.
[01:54:58.040 --> 01:55:01.400]   I've been to conferences where basically people will,
[01:55:01.400 --> 01:55:03.160]   you know, I'll ask them a question,
[01:55:03.160 --> 01:55:04.120]   and then they'll come back to,
[01:55:04.120 --> 01:55:05.480]   or like there was a conference
[01:55:05.480 --> 01:55:07.120]   where I asked somebody a question, they said,
[01:55:07.120 --> 01:55:09.160]   "Oh, in fact, this entire project was inspired
[01:55:09.160 --> 01:55:11.560]   "by your question three years ago at the same conference."
[01:55:11.560 --> 01:55:13.480]   - Yes. - I'm like, "Wow."
[01:55:13.480 --> 01:55:15.560]   - And then on top of that, there's also the ripple effect.
[01:55:15.560 --> 01:55:17.680]   So you're speaking to the direct influence
[01:55:17.680 --> 01:55:19.200]   of inspiration or education,
[01:55:19.200 --> 01:55:22.120]   but there's also like the follow-on things
[01:55:22.120 --> 01:55:23.960]   that happen to that, and there's this ripple
[01:55:23.960 --> 01:55:27.120]   that through from you just this one individual first drop.
[01:55:27.120 --> 01:55:29.240]   - And from every one of us, from everyone.
[01:55:29.240 --> 01:55:30.920]   That's what I love about humanity.
[01:55:30.920 --> 01:55:35.760]   The fact that every one of us shares genes
[01:55:36.680 --> 01:55:39.800]   and genetic variants with very recent ancestors
[01:55:39.800 --> 01:55:41.040]   with everyone else.
[01:55:41.040 --> 01:55:45.640]   So even if I die tomorrow, my genes are still shared
[01:55:45.640 --> 01:55:47.440]   through my cousins and through my uncles
[01:55:47.440 --> 01:55:49.960]   and through my, you know, immediate family.
[01:55:49.960 --> 01:55:52.740]   And of course, I'm lucky enough to have my own children,
[01:55:52.740 --> 01:55:55.680]   but even if you don't, your genes are still permeating
[01:55:55.680 --> 01:55:57.560]   through all of the layers of your family.
[01:55:57.560 --> 01:56:00.240]   - So your genes will have the legacy there, yeah.
[01:56:00.240 --> 01:56:01.960]   - Every one of us.
[01:56:01.960 --> 01:56:03.680]   - Yeah. - Number two, our ideas
[01:56:03.680 --> 01:56:05.720]   are constantly intermingling with each other.
[01:56:05.720 --> 01:56:08.760]   So there's no person living in the planet
[01:56:08.760 --> 01:56:12.000]   100 years from now who will not be directly impacted
[01:56:12.000 --> 01:56:14.120]   by every one of the planet living here today.
[01:56:14.120 --> 01:56:15.880]   - Yeah. - Through genetic inheritance
[01:56:15.880 --> 01:56:18.520]   and through meme inheritance.
[01:56:18.520 --> 01:56:22.000]   - That's cool to think that your ideas, Manolis Callas,
[01:56:22.000 --> 01:56:27.000]   would touch every single person on this planet.
[01:56:27.000 --> 01:56:28.760]   It's interesting. - But not just mine.
[01:56:28.760 --> 01:56:30.760]   Joe Smith, who's looking at this right now,
[01:56:30.760 --> 01:56:33.920]   his ideas will also touch everybody.
[01:56:33.920 --> 01:56:36.320]   So there's this interconnectedness of humanity.
[01:56:36.320 --> 01:56:41.420]   And then I'm also a professor, so my day job is legacy.
[01:56:41.420 --> 01:56:46.240]   My day job is training, not just the thousands of people
[01:56:46.240 --> 01:56:47.960]   who watch my videos on the web,
[01:56:47.960 --> 01:56:50.360]   but the people who are actually in my class,
[01:56:50.360 --> 01:56:55.080]   who basically come to MIT to learn from a bunch of us.
[01:56:55.080 --> 01:56:57.720]   - The cognitive systems that were shipped
[01:56:57.720 --> 01:56:59.040]   to this particular location.
[01:56:59.040 --> 01:57:00.600]   - And who will then disperse back
[01:57:00.600 --> 01:57:02.760]   into all of their home countries.
[01:57:02.760 --> 01:57:05.240]   That's what makes America the beacon of the world.
[01:57:05.240 --> 01:57:10.240]   We don't just export goods, we export people.
[01:57:10.240 --> 01:57:12.200]   - Cognitive systems.
[01:57:12.200 --> 01:57:15.360]   - We export people who are born here,
[01:57:15.360 --> 01:57:19.040]   and we also export training that people born elsewhere
[01:57:19.040 --> 01:57:21.960]   will come here to get, and will then disseminate
[01:57:21.960 --> 01:57:24.000]   not just whatever knowledge they got,
[01:57:24.000 --> 01:57:26.200]   but whatever ideals they learned.
[01:57:26.200 --> 01:57:28.760]   And I think that's a legacy of the US,
[01:57:28.760 --> 01:57:31.320]   that you cannot stop with political isolation,
[01:57:31.320 --> 01:57:33.480]   you cannot stop with economic isolation.
[01:57:33.480 --> 01:57:35.720]   That's something that will continue to happen
[01:57:35.720 --> 01:57:38.320]   through all the people we've touched through our universities.
[01:57:38.320 --> 01:57:40.960]   So there's the students who took my classes,
[01:57:40.960 --> 01:57:44.280]   who are basically now going off and teaching their classes,
[01:57:44.280 --> 01:57:46.480]   and I've trained generations of computational biologists.
[01:57:46.480 --> 01:57:48.680]   No one in genomics who's gone through MIT
[01:57:48.680 --> 01:57:50.120]   hasn't taken my class.
[01:57:50.120 --> 01:57:53.080]   So basically there's this impact through,
[01:57:53.080 --> 01:57:54.840]   I mean, there's so many people in biotechs
[01:57:54.840 --> 01:57:56.240]   who are like, "Hey, I took your class,
[01:57:56.240 --> 01:57:58.440]   "that's what got me into the field 15 years ago."
[01:57:58.440 --> 01:58:00.200]   And it's just so beautiful.
[01:58:00.200 --> 01:58:04.800]   And then there's the academic family that I have.
[01:58:04.800 --> 01:58:07.800]   So the students who are actually studying with me,
[01:58:07.800 --> 01:58:09.320]   who are my trainees.
[01:58:09.320 --> 01:58:13.120]   So this sort of mentorship of ancient Greece, this.
[01:58:13.120 --> 01:58:15.040]   (Lex laughing)
[01:58:15.040 --> 01:58:17.760]   So I basically have an academic family,
[01:58:17.760 --> 01:58:20.200]   and we are a family.
[01:58:20.200 --> 01:58:24.400]   There's this such strong connection,
[01:58:24.400 --> 01:58:27.840]   this bond of you're part of the Kelly's family.
[01:58:27.840 --> 01:58:30.160]   So I have a biological family at home,
[01:58:30.160 --> 01:58:32.280]   and I have an academic family on campus.
[01:58:32.280 --> 01:58:33.920]   And that academic family
[01:58:33.920 --> 01:58:36.480]   has given me great grandchildren already.
[01:58:36.480 --> 01:58:40.920]   So I've trained people who are now professors at Stanford,
[01:58:40.920 --> 01:58:45.920]   CMU, Harvard, WashU, I mean, everywhere in the world.
[01:58:45.920 --> 01:58:49.440]   And these people have now trained people
[01:58:49.440 --> 01:58:53.040]   who are now having their own faculty jobs.
[01:58:53.040 --> 01:58:55.120]   So there's basically people who see me
[01:58:55.120 --> 01:58:57.320]   as their academic grandfather.
[01:58:57.320 --> 01:58:58.400]   And it's just so beautiful,
[01:58:58.400 --> 01:59:00.400]   'cause you don't have to wait for the 18 years
[01:59:00.400 --> 01:59:03.880]   of cognitive hardware development
[01:59:03.880 --> 01:59:07.400]   to sort of have amazing conversation with people.
[01:59:07.400 --> 01:59:09.600]   These are fully grown humans, fully grown adults,
[01:59:09.600 --> 01:59:13.800]   who are cognitively super ready,
[01:59:13.800 --> 01:59:15.640]   and who are shaped by,
[01:59:15.640 --> 01:59:17.800]   and I see some of these beautiful papers,
[01:59:17.800 --> 01:59:21.080]   and I'm like, "I can see the touch of our lab
[01:59:21.080 --> 01:59:21.920]   in those papers."
[01:59:21.920 --> 01:59:23.600]   It's just so beautiful, 'cause you're like,
[01:59:23.600 --> 01:59:25.520]   "I've spent hours with these people,
[01:59:25.520 --> 01:59:27.560]   teaching them not just how to do a paper,
[01:59:27.560 --> 01:59:28.800]   but how to think."
[01:59:28.800 --> 01:59:31.960]   And this whole concept of,
[01:59:31.960 --> 01:59:34.200]   the first paper that we write together
[01:59:34.200 --> 01:59:37.480]   is an experience with every one of these students.
[01:59:37.480 --> 01:59:40.560]   So I always tell them to write the whole first draft,
[01:59:40.560 --> 01:59:43.320]   and they know that I will rewrite every word.
[01:59:43.320 --> 01:59:45.840]   But the act of them writing it,
[01:59:45.840 --> 01:59:48.320]   and what I do is these like joint editing sessions
[01:59:48.320 --> 01:59:50.080]   where I'm like, "Let's co-edit."
[01:59:50.080 --> 01:59:53.040]   And with this co-editing, we basically have-
[01:59:53.040 --> 01:59:55.040]   - Creative destruction.
[01:59:55.040 --> 01:59:56.680]   - So I share my Zoom screen,
[01:59:56.680 --> 01:59:59.600]   and I'm just thinking out loud as I'm doing this,
[01:59:59.600 --> 02:00:01.200]   and they're learning from that process,
[02:00:01.200 --> 02:00:03.200]   as opposed to like come back two days later,
[02:00:03.200 --> 02:00:05.080]   and they see a bunch of red on a page.
[02:00:05.080 --> 02:00:08.120]   I'm sort of, "Well, that's not how you write this.
[02:00:08.120 --> 02:00:09.480]   That's not how you think about this.
[02:00:09.480 --> 02:00:10.840]   That's not, you know, what's the point?"
[02:00:10.840 --> 02:00:12.560]   Like this morning I was having,
[02:00:12.560 --> 02:00:14.680]   yes, this morning between six and 8 a.m.,
[02:00:14.680 --> 02:00:16.800]   I had a two-hour meeting,
[02:00:16.800 --> 02:00:18.400]   going through one of these papers,
[02:00:18.400 --> 02:00:20.800]   and then saying, "What's the point here?
[02:00:20.800 --> 02:00:22.200]   Why do you even show that?
[02:00:22.200 --> 02:00:24.040]   It's just a bunch of points on a graph."
[02:00:24.040 --> 02:00:26.640]   No, what you have to do is extract the meaning,
[02:00:26.640 --> 02:00:28.240]   do the homework for them.
[02:00:28.240 --> 02:00:32.440]   And there's this nurturing, this mentorship
[02:00:32.440 --> 02:00:34.640]   that sort of creates now a legacy,
[02:00:34.640 --> 02:00:39.400]   which is infinite because they've now gone off on the,
[02:00:39.400 --> 02:00:42.120]   you know, and all of that is just humanity.
[02:00:42.120 --> 02:00:45.160]   Then, of course, there's the papers I write.
[02:00:45.160 --> 02:00:48.240]   'Cause yes, my day job is training students,
[02:00:48.240 --> 02:00:50.280]   but it's a research university.
[02:00:50.280 --> 02:00:54.400]   The way that they learn is through the mens et manus,
[02:00:54.400 --> 02:00:56.000]   mind and hand.
[02:00:56.000 --> 02:00:59.360]   It's the practical training of actually doing research.
[02:00:59.360 --> 02:01:03.860]   And that research is a beneficial side effect
[02:01:03.860 --> 02:01:06.640]   of having these awesome papers
[02:01:06.640 --> 02:01:10.900]   that will now tell other people how to think.
[02:01:10.900 --> 02:01:13.240]   There's this paper we just posted recently on MedArchive,
[02:01:13.240 --> 02:01:16.200]   and one of the most generous and eloquent comments about it
[02:01:16.200 --> 02:01:19.320]   was like, "Wow, this is a masterclass
[02:01:19.320 --> 02:01:22.760]   in scientific writing, in analysis,
[02:01:22.760 --> 02:01:24.920]   in biological interpretation," and so forth.
[02:01:24.920 --> 02:01:27.480]   It's just so fulfilling from a person I've never met.
[02:01:27.480 --> 02:01:29.680]   - Can you say the title of the paper, Brian?
[02:01:29.680 --> 02:01:31.860]   - I don't remember the title,
[02:01:31.860 --> 02:01:36.760]   but it's "Single-Cell Dissection of Schizophrenia Reveals."
[02:01:36.760 --> 02:01:39.920]   And so the two points that we found
[02:01:39.920 --> 02:01:42.780]   was this whole transcriptional resilience.
[02:01:42.780 --> 02:01:45.780]   Like there's some individuals who are schizophrenic,
[02:01:46.600 --> 02:01:50.040]   but who's, they have an additional cell type
[02:01:50.040 --> 02:01:53.560]   or initial cell state, which we believe is protective.
[02:01:53.560 --> 02:01:55.680]   And that cell state, when they have it,
[02:01:55.680 --> 02:01:56.900]   will cause other cells
[02:01:56.900 --> 02:01:58.920]   to have normal gene expression patterns.
[02:01:58.920 --> 02:02:00.440]   It's just beautiful.
[02:02:00.440 --> 02:02:03.980]   And then that cell is connected
[02:02:03.980 --> 02:02:06.120]   with some of the PV interneurons
[02:02:06.120 --> 02:02:09.520]   that are basically sending these inhibitory brain waves
[02:02:09.520 --> 02:02:10.880]   through the brain.
[02:02:10.880 --> 02:02:14.920]   And basically there's another component of,
[02:02:14.920 --> 02:02:18.440]   there's a set of master regulators that we discovered
[02:02:18.440 --> 02:02:20.740]   who are controlling many of the genes
[02:02:20.740 --> 02:02:22.600]   that are differentially expressed.
[02:02:22.600 --> 02:02:24.480]   And these master regulators are themselves
[02:02:24.480 --> 02:02:27.120]   genetic targets of schizophrenia.
[02:02:27.120 --> 02:02:28.680]   And they are themselves involved
[02:02:28.680 --> 02:02:31.800]   in both synaptic connectivity
[02:02:31.800 --> 02:02:34.720]   and also in early brain development.
[02:02:34.720 --> 02:02:36.860]   So there's this sort of interconnectedness
[02:02:36.860 --> 02:02:40.280]   between synaptic development axis
[02:02:40.280 --> 02:02:41.640]   and also this transcriptional resilience.
[02:02:41.640 --> 02:02:43.120]   So I mean, we basically made up a title
[02:02:43.120 --> 02:02:44.840]   that combines all these concepts.
[02:02:44.840 --> 02:02:45.760]   You have all these concepts,
[02:02:45.760 --> 02:02:46.920]   all these people working together,
[02:02:46.920 --> 02:02:49.640]   and ultimately these minds condense it down
[02:02:49.640 --> 02:02:51.760]   into a beautifully written little document
[02:02:51.760 --> 02:02:52.600]   that lives on forever. - Exactly.
[02:02:52.600 --> 02:02:55.040]   And that document now has its own life.
[02:02:55.040 --> 02:02:59.920]   Our work has 120,000 citations.
[02:02:59.920 --> 02:03:02.100]   I mean, that's not just people who read it.
[02:03:02.100 --> 02:03:05.760]   These are people who used it to write something based on it.
[02:03:05.760 --> 02:03:10.640]   I mean, that to me is just so fulfilling
[02:03:10.640 --> 02:03:12.960]   to basically say, wow, I've touched people.
[02:03:12.960 --> 02:03:17.800]   So I don't think of my legacy as I live every day.
[02:03:17.800 --> 02:03:20.300]   I just think of the beauty of the present
[02:03:20.300 --> 02:03:22.440]   and the power of interconnectedness.
[02:03:22.440 --> 02:03:25.000]   And just, I feel like a kid in a candy shop
[02:03:25.000 --> 02:03:28.040]   where I'm just like constantly,
[02:03:28.040 --> 02:03:30.820]   where do I, what package do I open first?
[02:03:30.820 --> 02:03:33.960]   And-- - You're the lucky one.
[02:03:33.960 --> 02:03:35.360]   (both laughing)
[02:03:35.360 --> 02:03:37.880]   - A jack of all trades, a master of none.
[02:03:37.880 --> 02:03:41.320]   - I think for a "Meaning of Life" episode,
[02:03:41.320 --> 02:03:45.400]   we would be amiss if we did not have at least a poem or two.
[02:03:45.400 --> 02:03:49.400]   Do you mind if we end in a couple of poems?
[02:03:49.400 --> 02:03:51.760]   Maybe a happy, maybe a sad one.
[02:03:51.760 --> 02:03:52.720]   - I would love that.
[02:03:52.720 --> 02:03:54.180]   So thank you for the luxury.
[02:03:54.180 --> 02:03:57.120]   The first one is kind of,
[02:03:57.120 --> 02:04:03.740]   I remember when you were talking with Eric Weinstein
[02:04:03.740 --> 02:04:08.740]   about this comment of Leonard Cohen that says,
[02:04:09.400 --> 02:04:11.920]   "But you don't really care for music, do ya?"
[02:04:11.920 --> 02:04:13.760]   In "Hallelujah," that's basically kind of like
[02:04:13.760 --> 02:04:16.320]   mocking its reader.
[02:04:16.320 --> 02:04:18.560]   So one of my poems is a little like that.
[02:04:18.560 --> 02:04:23.560]   So I had just broken up with my girlfriend
[02:04:23.560 --> 02:04:27.000]   and there's this other friend who was coming to visit me.
[02:04:27.000 --> 02:04:30.400]   And she said, "I will not come unless you write me a poem."
[02:04:30.400 --> 02:04:33.040]   (both laughing)
[02:04:33.040 --> 02:04:37.540]   And I was like, "Oh, writing a poem on demand."
[02:04:37.540 --> 02:04:40.960]   So this poem is called "Write Me a Poem."
[02:04:40.960 --> 02:04:44.280]   It goes, "Write me a poem," she said with a smile.
[02:04:44.280 --> 02:04:47.120]   "Make sure it's pretty, romantic, and rhymes.
[02:04:47.120 --> 02:04:49.480]   "Make sure it's worthy of that bold flame
[02:04:49.480 --> 02:04:52.640]   "that love uniting us beyond a mere game."
[02:04:52.640 --> 02:04:54.920]   And she took off without more words,
[02:04:54.920 --> 02:04:57.840]   rushed for the bus, and traveled the world.
[02:04:57.840 --> 02:05:00.600]   "A poem," I thought, "this is sublime.
[02:05:00.600 --> 02:05:03.080]   "What better way for passing the time?
[02:05:03.080 --> 02:05:05.340]   "What better way to count up the hours
[02:05:05.340 --> 02:05:08.400]   "before she comes back to my lonely tower?
[02:05:08.400 --> 02:05:10.360]   "Waiting for joy to fill up my heart,
[02:05:10.360 --> 02:05:13.240]   "let's write a poem for when we're apart.
[02:05:13.240 --> 02:05:15.980]   "How does a poem start?" I inquired.
[02:05:15.980 --> 02:05:18.340]   "Give me a topic, cook up a style.
[02:05:18.340 --> 02:05:20.780]   "Throw in some cute words, oh, here and there.
[02:05:20.780 --> 02:05:23.700]   "Throw in some passion, love, and despair.
[02:05:23.700 --> 02:05:26.680]   "Love, three eggs, one pound of flour,
[02:05:26.680 --> 02:05:28.860]   "three cups of water, and bake for an hour.
[02:05:28.860 --> 02:05:32.240]   "Love is no recipe, as I understand.
[02:05:32.240 --> 02:05:34.760]   "You can't just cook up a poem on demand."
[02:05:34.760 --> 02:05:37.040]   And as I was twisting all this in my mind,
[02:05:37.040 --> 02:05:38.520]   I looked at the page.
[02:05:38.520 --> 02:05:40.960]   By golly, it rhymed.
[02:05:40.960 --> 02:05:43.960]   Three roses, white chocolate, vanilla powder,
[02:05:43.960 --> 02:05:46.580]   some beautiful rhymes, and maybe a flower.
[02:05:46.580 --> 02:05:49.300]   "No, be romantic," the young girl insisted.
[02:05:49.300 --> 02:05:51.540]   "Do this, do that, don't be so silly.
[02:05:51.540 --> 02:05:53.320]   "You must believe it straight from your heart.
[02:05:53.320 --> 02:05:55.940]   "If you don't feel it, we're better apart."
[02:05:55.940 --> 02:05:59.460]   "Oh, my sweet thing, what can I say?
[02:05:59.460 --> 02:06:02.880]   "You bring me the sun all night and all day.
[02:06:02.880 --> 02:06:06.000]   "You're the stars and the moon and the birds way up high.
[02:06:06.000 --> 02:06:09.380]   "You're my evening sweet song, my morning blue sky.
[02:06:09.380 --> 02:06:12.400]   "You are my muse, your spell has me caught.
[02:06:12.400 --> 02:06:15.560]   "You bring me my voice and scatter my thoughts.
[02:06:15.560 --> 02:06:19.080]   "To put that love in writing, in vain, I can try.
[02:06:19.080 --> 02:06:22.320]   "But when I'm with you, my wings want to fly.
[02:06:22.320 --> 02:06:25.560]   "So I put down the pen and drop my defenses,
[02:06:25.560 --> 02:06:28.660]   "give myself to you and fill up my senses."
[02:06:31.280 --> 02:06:35.040]   - The baffled king composing, oh, that was beautiful.
[02:06:35.040 --> 02:06:38.160]   - What I love about it is that I did not
[02:06:38.160 --> 02:06:39.760]   bring up a dictionary of rhymes.
[02:06:39.760 --> 02:06:42.200]   I did not sort of work hard.
[02:06:42.200 --> 02:06:45.560]   So basically, when I write poems, I just type.
[02:06:45.560 --> 02:06:46.840]   I never go back, I just.
[02:06:46.840 --> 02:06:50.880]   So when my brain gets into that mode,
[02:06:50.880 --> 02:06:52.800]   it actually happens like I wrote it.
[02:06:52.800 --> 02:06:54.800]   - Oh, wow, so the rhyme just kind of,
[02:06:54.800 --> 02:06:57.120]   it's an emergent phenomenon. - It's an emergent phenomenon.
[02:06:57.120 --> 02:07:00.960]   I just get into that mode, and then it comes out.
[02:07:00.960 --> 02:07:02.040]   - That's a beautiful one.
[02:07:02.040 --> 02:07:06.760]   - And it's basically, as you got it,
[02:07:06.760 --> 02:07:08.120]   it's basically saying it's not a recipe,
[02:07:08.120 --> 02:07:09.860]   and then I'm throwing in the recipes,
[02:07:09.860 --> 02:07:11.640]   and as I'm writing it, I'm like,
[02:07:11.640 --> 02:07:16.500]   so it's very introspective in this whole concept.
[02:07:16.500 --> 02:07:19.560]   So anyway, there's another one many years earlier
[02:07:19.560 --> 02:07:23.620]   that is darker.
[02:07:23.620 --> 02:07:26.760]   It's basically this whole concept of let's be friends.
[02:07:26.760 --> 02:07:27.800]   I was like, "Ugh!"
[02:07:29.760 --> 02:07:32.480]   No, let's be friends, just like, you know.
[02:07:32.480 --> 02:07:34.640]   So the last words are shout out, I love you,
[02:07:34.640 --> 02:07:36.240]   or send me to hell.
[02:07:36.240 --> 02:07:40.080]   So the title is "Burn Me Tonight."
[02:07:40.080 --> 02:07:43.200]   Lie to me, baby.
[02:07:43.200 --> 02:07:44.680]   Lie to me now.
[02:07:44.680 --> 02:07:45.920]   Tell me you love me.
[02:07:45.920 --> 02:07:47.080]   Break me a vow.
[02:07:47.080 --> 02:07:49.920]   Give me a sweet word, a promise, a kiss.
[02:07:49.920 --> 02:07:52.760]   Give me the world, a sweet taste to miss.
[02:07:52.760 --> 02:07:56.120]   Don't let me lay here, inert, ugly, cold,
[02:07:56.120 --> 02:07:59.140]   with nothing sweet felt and nothing harsh told.
[02:07:59.140 --> 02:08:02.360]   Give me some hope, false, foolish, yet kind.
[02:08:02.360 --> 02:08:05.120]   Make me regret, I'll leave you behind.
[02:08:05.120 --> 02:08:08.240]   Don't pity my soul, but torture it right.
[02:08:08.240 --> 02:08:11.040]   Treat it with hatred, start up a fight,
[02:08:11.040 --> 02:08:14.040]   for it's from mildness that my soul dies
[02:08:14.040 --> 02:08:18.600]   when you cover your passion in a bland friend's disguise.
[02:08:18.600 --> 02:08:21.080]   Kiss me now, baby, show me your passion.
[02:08:21.080 --> 02:08:23.640]   Turn off the lights and rip off your fashion.
[02:08:23.640 --> 02:08:26.720]   Give me my life's joy this one night.
[02:08:26.720 --> 02:08:30.140]   Burn all my matches for one blazing light.
[02:08:30.140 --> 02:08:32.500]   Don't think of tomorrow and let today fade.
[02:08:32.500 --> 02:08:35.580]   Don't try and protect me from love's cutting blade.
[02:08:35.580 --> 02:08:38.500]   Your razor will always rip off my veins.
[02:08:38.500 --> 02:08:42.100]   Don't spare me the passion to spare me the pains.
[02:08:42.100 --> 02:08:44.620]   Kiss me now, honey, or spit in my face.
[02:08:44.620 --> 02:08:47.700]   Throw me an insult I'll gladly embrace.
[02:08:47.700 --> 02:08:49.980]   Tell me now clearly that you never cared.
[02:08:49.980 --> 02:08:52.820]   Say it now loudly like you never dared.
[02:08:52.820 --> 02:08:54.100]   I'm ready to hear it.
[02:08:54.100 --> 02:08:55.500]   I'm ready to die.
[02:08:55.500 --> 02:08:58.520]   I'm ready to burn and start a new life.
[02:08:58.520 --> 02:09:01.420]   I'm ready to face the rough burning truth
[02:09:01.420 --> 02:09:04.380]   rather than waste the rest of my youth.
[02:09:04.380 --> 02:09:07.260]   So tell me, my lover, should I stay or go?
[02:09:07.260 --> 02:09:09.800]   The answer to love is one, yes or no.
[02:09:09.800 --> 02:09:12.480]   There's no I like you, no let's be friends,
[02:09:12.480 --> 02:09:15.140]   shout out I love you, or send me to hell.
[02:09:15.140 --> 02:09:18.420]   (laughing)
[02:09:18.420 --> 02:09:19.840]   - I don't think there's a better way
[02:09:19.840 --> 02:09:23.320]   to end a discussion of the meaning of life,
[02:09:23.320 --> 02:09:25.740]   whatever the heck the meaning is,
[02:09:25.740 --> 02:09:28.700]   go all in as that poem says.
[02:09:28.700 --> 02:09:30.500]   Manolis, thank you so much for talking today.
[02:09:30.500 --> 02:09:32.800]   - Thanks, I look forward to next time.
[02:09:32.800 --> 02:09:34.500]   - Thanks for listening to this conversation
[02:09:34.500 --> 02:09:38.000]   with Manolis Kellis, and thank you to our sponsors.
[02:09:38.000 --> 02:09:40.760]   Grammarly, which is a service for checking spelling,
[02:09:40.760 --> 02:09:43.740]   grammar, sentence structure, and readability.
[02:09:43.740 --> 02:09:46.260]   Athletic Greens, the all-in-one drink
[02:09:46.260 --> 02:09:47.860]   that I start every day with
[02:09:47.860 --> 02:09:50.220]   to cover all my nutritional bases.
[02:09:50.220 --> 02:09:54.100]   Cash App, the app I use to send money to friends.
[02:09:54.100 --> 02:09:56.360]   Please check out these sponsors in the description
[02:09:56.360 --> 02:09:59.380]   to get a discount and to support this podcast.
[02:09:59.380 --> 02:10:01.880]   If you enjoy this thing, subscribe on YouTube,
[02:10:01.880 --> 02:10:04.040]   review it with Five Stars on Apple Podcast,
[02:10:04.040 --> 02:10:06.520]   follow on Spotify, support on Patreon,
[02:10:06.520 --> 02:10:09.640]   or connect with me on Twitter @LexFriedman.
[02:10:09.640 --> 02:10:11.460]   And now, let me leave you with some words
[02:10:11.460 --> 02:10:13.480]   from Douglas Adams in his book,
[02:10:13.480 --> 02:10:16.120]   "Hitchhiker's Guide to the Galaxy."
[02:10:16.120 --> 02:10:19.440]   On the planet Earth, man had always assumed
[02:10:19.440 --> 02:10:22.160]   that he was more intelligent than dolphins
[02:10:22.160 --> 02:10:24.460]   because he had achieved so much.
[02:10:24.460 --> 02:10:28.720]   The wheel, New York, wars, and so on.
[02:10:28.720 --> 02:10:31.220]   Whilst all the dolphins had ever done
[02:10:31.220 --> 02:10:34.900]   was muck about in the water having a good time.
[02:10:34.900 --> 02:10:38.560]   But conversely, the dolphins had always believed
[02:10:38.560 --> 02:10:41.460]   that they were far more intelligent than man
[02:10:41.460 --> 02:10:43.920]   for precisely the same reasons.
[02:10:43.920 --> 02:10:47.600]   Thank you for listening, I hope to see you next time.
[02:10:47.600 --> 02:10:50.180]   (upbeat music)
[02:10:50.180 --> 02:10:52.760]   (upbeat music)
[02:10:52.760 --> 02:11:02.760]   [BLANK_AUDIO]

