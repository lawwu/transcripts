
[00:00:00.000 --> 00:00:02.160]   So welcome, everybody, to the paper reading group.
[00:00:02.160 --> 00:00:06.320]   And we've had a fair share of technical difficulties today,
[00:00:06.320 --> 00:00:08.720]   but I hope that's the last of them.
[00:00:08.720 --> 00:00:10.600]   So as part of our paper reading group today,
[00:00:10.600 --> 00:00:14.400]   we're looking at the MLP mixer paper.
[00:00:14.400 --> 00:00:16.080]   And I'm really, really excited that I'm
[00:00:16.080 --> 00:00:19.120]   joined by Dr. Habib today.
[00:00:19.120 --> 00:00:22.000]   Dr. Habib and I have spent the past, I think,
[00:00:22.000 --> 00:00:24.440]   a few days almost speaking every day, a few hours,
[00:00:24.440 --> 00:00:26.440]   and just discussing the MLP mixer
[00:00:26.440 --> 00:00:27.840]   architecture between ourselves.
[00:00:27.840 --> 00:00:31.600]   And I'm really excited Dr. Habib is here with us today.
[00:00:31.600 --> 00:00:35.040]   But before that, just a very quick pointer.
[00:00:35.040 --> 00:00:37.880]   Just like every previous paper reading group,
[00:00:37.880 --> 00:00:42.080]   we still have this link, 1db.me/mlpmixer.
[00:00:42.080 --> 00:00:45.600]   So I believe I've posted that in the chat.
[00:00:45.600 --> 00:00:46.360]   Anjulika has.
[00:00:46.360 --> 00:00:46.860]   Perfect.
[00:00:46.860 --> 00:00:47.880]   Thanks, Anjulika.
[00:00:47.880 --> 00:00:49.840]   So if I go to my--
[00:00:49.840 --> 00:00:54.680]   if I type in my browser, 1db.me/mlpmixer,
[00:00:54.680 --> 00:00:57.800]   that will take us to a report, which
[00:00:57.800 --> 00:01:00.560]   is this paper reading group MLP mixer with special guest Dr.
[00:01:00.560 --> 00:01:01.060]   Habib.
[00:01:01.060 --> 00:01:03.720]   And then towards the end, I could just write a comment
[00:01:03.720 --> 00:01:05.440]   and I could say test.
[00:01:05.440 --> 00:01:07.560]   So this is where we will be using all our questions
[00:01:07.560 --> 00:01:08.120]   and answers.
[00:01:08.120 --> 00:01:11.000]   And this has been the format since the beginning.
[00:01:11.000 --> 00:01:13.040]   So we're just going to be using the comments here
[00:01:13.040 --> 00:01:16.200]   as we go to ask questions.
[00:01:16.200 --> 00:01:18.320]   So I won't be monitoring the Zoom chat.
[00:01:18.320 --> 00:01:20.440]   I won't be looking at the YouTube live chat.
[00:01:20.440 --> 00:01:22.160]   It's just very difficult to manage, too.
[00:01:22.160 --> 00:01:25.920]   So we're just going to be using the comments here.
[00:01:25.920 --> 00:01:29.440]   So about Dr. Habib, really excited and welcome, Dr. Habib.
[00:01:29.440 --> 00:01:32.000]   I know you have a PhD in structural biology.
[00:01:32.000 --> 00:01:34.320]   And then you decided to learn coding.
[00:01:34.320 --> 00:01:37.960]   And like many, many others, ended up
[00:01:37.960 --> 00:01:40.440]   being a part of the FastAI family.
[00:01:40.440 --> 00:01:42.360]   And then you started Kaggle.
[00:01:42.360 --> 00:01:45.520]   And starting Kaggle was--
[00:01:45.520 --> 00:01:47.520]   I've seen your work.
[00:01:47.520 --> 00:01:48.560]   I've known you.
[00:01:48.560 --> 00:01:50.040]   I've followed you.
[00:01:50.040 --> 00:01:52.400]   I've been a big fan of what you've
[00:01:52.400 --> 00:01:53.640]   achieved through the years.
[00:01:53.640 --> 00:01:58.400]   And seeing you as current rank 211 with four golds and eight
[00:01:58.400 --> 00:02:01.480]   silvers, I think the only next step is a solo gold.
[00:02:01.480 --> 00:02:05.040]   And you'll be a Kaggle grandmaster very soon.
[00:02:05.040 --> 00:02:07.080]   And now you're working as an applied deep learning
[00:02:07.080 --> 00:02:08.160]   engineer at Janelia.
[00:02:08.160 --> 00:02:10.760]   So really excited to have you with us.
[00:02:10.760 --> 00:02:11.320]   Welcome.
[00:02:11.320 --> 00:02:18.240]   And if I missed anything and if there's anything you want to add.
[00:02:18.240 --> 00:02:18.760]   Thank you.
[00:02:18.760 --> 00:02:20.720]   First of all, thank you very much for inviting.
[00:02:20.720 --> 00:02:25.840]   It's an honor to be here together with you.
[00:02:25.840 --> 00:02:30.160]   I also-- I think our journey started in parallel.
[00:02:30.160 --> 00:02:31.440]   You started with the blog.
[00:02:31.440 --> 00:02:32.840]   I started with the Kaggle.
[00:02:32.840 --> 00:02:37.360]   And now we are kind of merging.
[00:02:37.360 --> 00:02:41.080]   Yeah, I'm really-- yeah, this is my journey.
[00:02:41.080 --> 00:02:43.040]   I started from zero.
[00:02:43.040 --> 00:02:44.920]   Yeah, now I do deep learning.
[00:02:44.920 --> 00:02:49.120]   And as Amman mentioned, I only need one gold solo medal,
[00:02:49.120 --> 00:02:52.880]   which is the hardest thing to become Kaggle grandmaster.
[00:02:52.880 --> 00:02:56.880]   So I will-- for next, I don't know, one or two years,
[00:02:56.880 --> 00:02:59.760]   I will try my best to get it.
[00:02:59.760 --> 00:03:02.400]   It's definitely very challenging for me.
[00:03:02.400 --> 00:03:03.360]   But we'll see.
[00:03:03.360 --> 00:03:04.920]   Yeah.
[00:03:04.920 --> 00:03:07.040]   I'm 100% sure you're going to get it very soon.
[00:03:07.040 --> 00:03:10.040]   I think it's only a few weeks, if not days away.
[00:03:10.040 --> 00:03:13.280]   But fingers crossed and good luck for that.
[00:03:13.280 --> 00:03:13.920]   Thank you.
[00:03:13.920 --> 00:03:17.640]   So with that being said, as Dr. Habib mentioned,
[00:03:17.640 --> 00:03:19.320]   he and I have worked together before.
[00:03:19.320 --> 00:03:23.040]   And previously, we were working on the Vision Transformer
[00:03:23.040 --> 00:03:23.600]   blog post.
[00:03:23.600 --> 00:03:27.800]   There's something-- I'll share this in the chat once more.
[00:03:27.800 --> 00:03:30.840]   But something which I didn't know until this blog post
[00:03:30.840 --> 00:03:32.560]   is that Dr. Habib's really, really
[00:03:32.560 --> 00:03:35.120]   skilled at making wonderful visualizations
[00:03:35.120 --> 00:03:38.720]   and making complex ideas look very easy.
[00:03:38.720 --> 00:03:41.600]   And that's something I've been learning from him.
[00:03:41.600 --> 00:03:43.360]   In every meeting that I meet with him,
[00:03:43.360 --> 00:03:45.800]   there's something definitely there to learn.
[00:03:45.800 --> 00:03:49.400]   And these are visualizations that Dr. Habib did make for us
[00:03:49.400 --> 00:03:51.880]   in this previous blog post, which ended up
[00:03:51.880 --> 00:03:54.040]   being really, really popular.
[00:03:54.040 --> 00:03:58.280]   And looking at the Vision Transformer that way was--
[00:03:58.280 --> 00:04:01.480]   for me, it was very easy to look at these visualizations
[00:04:01.480 --> 00:04:04.080]   and then understand Vision Transformer
[00:04:04.080 --> 00:04:05.880]   from his perspective.
[00:04:05.880 --> 00:04:08.640]   And he and I have gotten together again.
[00:04:08.640 --> 00:04:13.600]   So I'll share the blog post in the URL once more.
[00:04:13.600 --> 00:04:15.400]   But this time, we've gotten together again
[00:04:15.400 --> 00:04:18.200]   for the MLP Mixer paper.
[00:04:18.200 --> 00:04:21.600]   So as a supplement to this paper reading group,
[00:04:21.600 --> 00:04:25.480]   please feel free to have a look at this blog post.
[00:04:25.480 --> 00:04:29.120]   Again, there's lots of beautiful visualizations from Dr. Habib
[00:04:29.120 --> 00:04:30.040]   as we go down--
[00:04:30.040 --> 00:04:33.880]   as we keep going down.
[00:04:33.880 --> 00:04:36.720]   So that's the architecture from the paper.
[00:04:36.720 --> 00:04:39.320]   But again, you'll see lots of similarities
[00:04:39.320 --> 00:04:40.480]   in terms of visualizations.
[00:04:40.480 --> 00:04:44.680]   But we've, again, made Mixer layer really easy.
[00:04:44.680 --> 00:04:47.560]   And so you can have a look at every possible operation that
[00:04:47.560 --> 00:04:49.520]   goes inside the Mixer layer.
[00:04:49.520 --> 00:04:51.720]   So we're going to be using these visualizations today.
[00:04:51.720 --> 00:04:53.960]   And we're going to be using reading through the paper
[00:04:53.960 --> 00:04:58.200]   to understand the MLP Mixer paper.
[00:04:58.200 --> 00:05:02.160]   But something I did want to touch just before we started
[00:05:02.160 --> 00:05:06.280]   was, as part of the MLP Mixer paper, when the MLP Mixer
[00:05:06.280 --> 00:05:09.400]   paper came around, the MLP Mixer paper
[00:05:09.400 --> 00:05:14.240]   was introduced initially as a convolution-free network.
[00:05:14.240 --> 00:05:17.960]   So the researchers from Google-- and it says so in the paper
[00:05:17.960 --> 00:05:19.520]   as well-- that it's convolution-free.
[00:05:19.520 --> 00:05:20.520]   Do you need convolution?
[00:05:20.520 --> 00:05:21.920]   Do you need attention?
[00:05:21.920 --> 00:05:22.560]   Probably not.
[00:05:22.560 --> 00:05:25.920]   And that's when the MLP Mixer paper came in.
[00:05:25.920 --> 00:05:29.640]   But there's also a line or text inside the paper
[00:05:29.640 --> 00:05:32.160]   that did mention--
[00:05:32.160 --> 00:05:37.360]   so there's a special text in the paper that does say,
[00:05:37.360 --> 00:05:38.720]   in the extreme case--
[00:05:38.720 --> 00:05:39.600]   so this is here.
[00:05:39.600 --> 00:05:43.320]   I'm really sorry.
[00:05:43.320 --> 00:05:43.840]   One second.
[00:05:43.840 --> 00:05:57.080]   Sorry, my setup is a bit different today.
[00:05:57.080 --> 00:06:01.320]   So it does say, in the extreme case,
[00:06:01.320 --> 00:06:04.840]   our architecture can be seen as a very special convolutional
[00:06:04.840 --> 00:06:08.560]   network, which uses one-by-one convolutions for channel mixing
[00:06:08.560 --> 00:06:10.760]   and single-channel depth-wise convolutions
[00:06:10.760 --> 00:06:12.800]   of full receptive field and parameter
[00:06:12.800 --> 00:06:14.280]   sharing for token mixing.
[00:06:14.280 --> 00:06:16.640]   So we'll understand everything what these things are.
[00:06:16.640 --> 00:06:19.080]   But I guess the main point right now
[00:06:19.080 --> 00:06:21.520]   is to see that the paper mentions
[00:06:21.520 --> 00:06:25.400]   that through some operations, through some constraints,
[00:06:25.400 --> 00:06:27.880]   the MLP Mixer paper in itself would still
[00:06:27.880 --> 00:06:29.760]   be seen as a convolutional network.
[00:06:29.760 --> 00:06:32.720]   What exactly those constraints are, Dr. Habib and I
[00:06:32.720 --> 00:06:34.680]   have spent hours and hours trying
[00:06:34.680 --> 00:06:38.840]   to demystify what exactly those constraints are.
[00:06:38.840 --> 00:06:40.680]   And something that happened along the line
[00:06:40.680 --> 00:06:44.040]   was then Jan LeCun, he went in and he said, well,
[00:06:44.040 --> 00:06:46.480]   it's not exactly conf free in the first layer.
[00:06:46.480 --> 00:06:48.480]   Could be a per-patch, fully connected,
[00:06:48.480 --> 00:06:51.520]   could be a conv layer with 16 cross 16 kernels and 16
[00:06:51.520 --> 00:06:53.040]   cross 16 stride.
[00:06:53.040 --> 00:06:56.200]   And then the MLP Mixer layer in itself
[00:06:56.200 --> 00:06:58.640]   could be represented as a conv layer,
[00:06:58.640 --> 00:07:01.480]   to which then there was this really interesting competition
[00:07:01.480 --> 00:07:05.240]   that was hosted, which is Mixer is a CNN problem, which
[00:07:05.240 --> 00:07:09.360]   was an attempt to make the MLP Mixer be implemented just
[00:07:09.360 --> 00:07:11.840]   by using convolutional--
[00:07:11.840 --> 00:07:13.680]   just by using the convolution operations.
[00:07:13.680 --> 00:07:18.160]   So I guess before we got started with this paper,
[00:07:18.160 --> 00:07:23.200]   I just wanted to give some background on how the MLP Mixer
[00:07:23.200 --> 00:07:25.320]   paper has been really exciting.
[00:07:25.320 --> 00:07:27.520]   And yet, at the same time, the deep learning community
[00:07:27.520 --> 00:07:29.720]   has been a little bit split on whether it
[00:07:29.720 --> 00:07:32.640]   is a convolutional network or it's not.
[00:07:32.640 --> 00:07:36.120]   We feel it's pretty much like a touchy subject that just comes
[00:07:36.120 --> 00:07:37.000]   down to semantics.
[00:07:37.000 --> 00:07:39.080]   And as part of this paper reading group,
[00:07:39.080 --> 00:07:42.800]   then we're going to dig dive into every bit of this paper
[00:07:42.800 --> 00:07:43.680]   with Dr. Habib.
[00:07:43.680 --> 00:07:45.520]   So Dr. Habib, is there anything else that you
[00:07:45.520 --> 00:07:47.880]   wanted to add over here?
[00:07:47.880 --> 00:07:53.240]   No, I think I don't have anything yet to add.
[00:07:53.240 --> 00:07:55.560]   But it's a nice background introduction
[00:07:55.560 --> 00:08:00.520]   to why there is so much fuzz about this paper.
[00:08:00.520 --> 00:08:01.320]   Yeah, totally.
[00:08:01.320 --> 00:08:02.320]   Thank you.
[00:08:02.320 --> 00:08:04.320]   So then let's get started with this paper.
[00:08:04.320 --> 00:08:09.440]   So something that we know--
[00:08:09.440 --> 00:08:11.320]   I'm just reading the abstract right now.
[00:08:11.320 --> 00:08:13.720]   Something that we know so far is that convolutional neural
[00:08:13.720 --> 00:08:16.720]   networks have been the go-to model for computer vision.
[00:08:16.720 --> 00:08:18.360]   But recently, that's been changing.
[00:08:18.360 --> 00:08:21.600]   We just saw the Vision Transformer blog.
[00:08:21.600 --> 00:08:23.840]   So ever since the Vision Transformer came around,
[00:08:23.840 --> 00:08:25.520]   in our previous paper reading groups,
[00:08:25.520 --> 00:08:27.280]   we've looked at convolution--
[00:08:27.280 --> 00:08:29.280]   sorry, we've looked at transformer architectures.
[00:08:29.280 --> 00:08:32.280]   And we just discussed the Convit paper as well.
[00:08:32.280 --> 00:08:34.960]   And since then, a new type of paper has--
[00:08:34.960 --> 00:08:37.000]   a new type of architecture has emerged,
[00:08:37.000 --> 00:08:40.400]   which is the MLP mixer.
[00:08:40.400 --> 00:08:43.480]   What this MLP mixer is, it's not using self-attention.
[00:08:43.480 --> 00:08:45.360]   It's not using convolution.
[00:08:45.360 --> 00:08:48.280]   It's pretty much just using MLP layers,
[00:08:48.280 --> 00:08:51.080]   which are fully connected layers.
[00:08:51.080 --> 00:08:52.760]   And it's just using fully connected layers
[00:08:52.760 --> 00:08:57.960]   and a very novel way of using the transpose operation that
[00:08:57.960 --> 00:08:59.360]   makes things work.
[00:08:59.360 --> 00:09:03.120]   And by doing that, by using such a simple architecture,
[00:09:03.120 --> 00:09:05.200]   they're still able to get competitive performance
[00:09:05.200 --> 00:09:06.240]   with the state of the art.
[00:09:06.240 --> 00:09:07.760]   So they're still able to--
[00:09:07.760 --> 00:09:11.120]   with lots of pre-training and lots of infrastructure
[00:09:11.120 --> 00:09:14.520]   and compute requirements, of course, to get it to work.
[00:09:14.520 --> 00:09:16.880]   But they're able to still get it to work
[00:09:16.880 --> 00:09:19.920]   in terms of having a good, respectable accuracy
[00:09:19.920 --> 00:09:23.200]   and competitive to the state of art.
[00:09:23.200 --> 00:09:28.000]   So that's the background about what MLP mixer is.
[00:09:28.000 --> 00:09:29.600]   Sorry, Sanyam.
[00:09:29.600 --> 00:09:30.200]   I'm sorry.
[00:09:30.200 --> 00:09:33.080]   Could you please zoom in a little?
[00:09:33.080 --> 00:09:33.800]   Of course.
[00:09:33.800 --> 00:09:35.800]   Is that better?
[00:09:35.800 --> 00:09:37.640]   Yes, thank you.
[00:09:37.640 --> 00:09:38.160]   Hold on.
[00:09:38.160 --> 00:09:41.400]   Let me try and make it a bit better.
[00:09:41.400 --> 00:09:44.840]   So I think that should be fine now.
[00:09:44.840 --> 00:09:45.560]   This is perfect.
[00:09:45.560 --> 00:09:46.600]   Thank you.
[00:09:46.600 --> 00:09:47.240]   OK, excellent.
[00:09:47.240 --> 00:09:47.920]   No worries.
[00:09:47.920 --> 00:09:49.560]   Thanks for pointing that out.
[00:09:49.560 --> 00:09:51.240]   So then, as part of the--
[00:09:51.240 --> 00:09:53.560]   this paper is really, really easy to understand.
[00:09:53.560 --> 00:09:58.200]   It's not like a lot of complexity in this.
[00:09:58.200 --> 00:10:00.480]   But it's about making things simple
[00:10:00.480 --> 00:10:02.480]   and yet getting them to work.
[00:10:02.480 --> 00:10:06.840]   That's what I was really excited to read about this paper.
[00:10:06.840 --> 00:10:09.560]   So if we go down into the architecture,
[00:10:09.560 --> 00:10:13.800]   I think, Dr. Habib, did you want to quickly talk
[00:10:13.800 --> 00:10:15.800]   about the architecture and just present the paper
[00:10:15.800 --> 00:10:17.480]   as a high-level overview?
[00:10:17.480 --> 00:10:21.520]   Yeah, I just want to also kind of unpack,
[00:10:21.520 --> 00:10:23.840]   rephrase what you said a little bit earlier.
[00:10:23.840 --> 00:10:29.160]   So just to have a bigger picture, so far,
[00:10:29.160 --> 00:10:35.280]   before this MLP paper, we had two main ways
[00:10:35.280 --> 00:10:37.240]   to classify images.
[00:10:37.240 --> 00:10:42.280]   One was this convolutional base, where we use convolutions.
[00:10:42.280 --> 00:10:45.680]   And this method, or the way convolutional neural networks
[00:10:45.680 --> 00:10:50.280]   were around maybe decades, and everybody was using.
[00:10:50.280 --> 00:10:53.960]   And until last year, I'm correct,
[00:10:53.960 --> 00:10:59.040]   the vision transformer came, which kind of said, look,
[00:10:59.040 --> 00:11:00.600]   we don't need convolutions.
[00:11:00.600 --> 00:11:04.080]   You actually just need attention and positional embeddings
[00:11:04.080 --> 00:11:05.880]   and just linear layers.
[00:11:05.880 --> 00:11:10.080]   And this vision transformers was--
[00:11:10.080 --> 00:11:12.400]   they kind of overtook convolutional neural networks.
[00:11:12.400 --> 00:11:15.520]   And they are currently even ranked top one
[00:11:15.520 --> 00:11:18.560]   in image classification.
[00:11:18.560 --> 00:11:20.960]   So basically, we have two categories, convolutional
[00:11:20.960 --> 00:11:23.680]   networks and then transformers.
[00:11:23.680 --> 00:11:27.600]   And then it was kind of interesting, right?
[00:11:27.600 --> 00:11:28.680]   Something new came.
[00:11:28.680 --> 00:11:33.400]   And then within the next six months comes this paper.
[00:11:33.400 --> 00:11:38.400]   And their main claim is following that, actually,
[00:11:38.400 --> 00:11:39.600]   you don't need convolutions.
[00:11:39.600 --> 00:11:41.120]   You don't need attentions.
[00:11:41.120 --> 00:11:44.040]   You don't need positional embeddings.
[00:11:44.040 --> 00:11:47.960]   You can get competitive performance.
[00:11:47.960 --> 00:11:50.680]   And competitives, I mean, they claim
[00:11:50.680 --> 00:11:53.880]   that their best MLP, simple MLP module,
[00:11:53.880 --> 00:11:59.160]   has less than 0.3% accuracy than top vision transformer
[00:11:59.160 --> 00:12:01.240]   model, which is pretty impressive.
[00:12:01.240 --> 00:12:05.640]   So they come up with this very simple module, just MLPs.
[00:12:05.640 --> 00:12:07.720]   MLP stands for multilayer perceptron.
[00:12:07.720 --> 00:12:12.920]   So if you're a PyTorch user, you're just an n dot linear.
[00:12:12.920 --> 00:12:15.120]   And they say, yeah, that's pretty much everything
[00:12:15.120 --> 00:12:15.720]   what you need.
[00:12:15.720 --> 00:12:21.160]   And then, obviously, Aman already
[00:12:21.160 --> 00:12:24.440]   introduced all the argument that MLP are kind of convolutional
[00:12:24.440 --> 00:12:24.960]   or not.
[00:12:24.960 --> 00:12:27.640]   So this is just a big picture from my point of view
[00:12:27.640 --> 00:12:34.080]   that it's kind of a third way to classify images.
[00:12:34.080 --> 00:12:40.280]   And so the way how it works, just a global overview--
[00:12:40.280 --> 00:12:43.760]   again, we will go into details.
[00:12:43.760 --> 00:12:46.280]   There are some parts which are very similar to transformer.
[00:12:46.280 --> 00:12:48.920]   So here, what you see that we have this image,
[00:12:48.920 --> 00:12:52.160]   we basically break down into the patches.
[00:12:52.160 --> 00:12:53.200]   Thank you, Aman.
[00:12:53.200 --> 00:13:00.640]   And these patches, and then we have this single shared linear
[00:13:00.640 --> 00:13:01.480]   layer.
[00:13:01.480 --> 00:13:03.960]   What we do, we flatten the patches.
[00:13:03.960 --> 00:13:06.480]   And then we project them.
[00:13:06.480 --> 00:13:10.840]   So we convert them to a single vector.
[00:13:10.840 --> 00:13:13.120]   You can call it embedding, patch embedding.
[00:13:13.120 --> 00:13:16.360]   Or you can call it latent representation.
[00:13:16.360 --> 00:13:19.200]   Doesn't matter whatever the terminology is.
[00:13:19.200 --> 00:13:20.280]   It's just a single vector.
[00:13:20.280 --> 00:13:22.800]   For each patch, we get single vector.
[00:13:22.800 --> 00:13:28.200]   And then these vectors, a combination of vectors,
[00:13:28.200 --> 00:13:32.600]   goes to this mixer layer.
[00:13:32.600 --> 00:13:35.840]   And we'll go in detail what's inside the mixer layer.
[00:13:35.840 --> 00:13:37.280]   But to be honest, what's happening,
[00:13:37.280 --> 00:13:42.480]   there is two MLPs there which do convolutions either directly
[00:13:42.480 --> 00:13:44.880]   on the patches or on the channels.
[00:13:44.880 --> 00:13:49.040]   And you have multiple layers of those.
[00:13:49.040 --> 00:13:52.000]   And at the end, you take the mean.
[00:13:52.000 --> 00:13:54.840]   And then you have this classification ahead.
[00:13:54.840 --> 00:14:00.000]   So to be honest, it uses two building blocks.
[00:14:00.000 --> 00:14:01.960]   So it uses an n dot linear.
[00:14:01.960 --> 00:14:07.240]   And then it uses GLE, which is activation function.
[00:14:07.240 --> 00:14:09.560]   And then obviously, there is a dropout.
[00:14:09.560 --> 00:14:12.280]   I don't know if you consider it layer and skip connections.
[00:14:12.280 --> 00:14:16.120]   So it's very simple.
[00:14:16.120 --> 00:14:19.200]   Yeah, that's pretty much it.
[00:14:19.200 --> 00:14:22.920]   Yeah, that's really-- I think if we were to explain the paper,
[00:14:22.920 --> 00:14:26.120]   it would take five minutes because it's that easy.
[00:14:26.120 --> 00:14:29.320]   But I guess the interesting bits that come from this paper
[00:14:29.320 --> 00:14:32.080]   is that even after being so easy,
[00:14:32.080 --> 00:14:34.880]   it's still able to perform very well
[00:14:34.880 --> 00:14:38.120]   with those very complex architectures out there that
[00:14:38.120 --> 00:14:39.440]   use self-attention.
[00:14:39.440 --> 00:14:45.240]   And I think that's where this paper-- that's
[00:14:45.240 --> 00:14:47.840]   why this paper is one of the first papers to do so.
[00:14:47.840 --> 00:14:50.400]   And then there's other papers that have followed.
[00:14:50.400 --> 00:14:53.320]   But something I would like to highlight is as part of this,
[00:14:53.320 --> 00:14:57.200]   then Dr. Habib mentioned we start with an input image.
[00:14:57.200 --> 00:14:59.720]   So let's say you have an input image like this.
[00:14:59.720 --> 00:15:01.680]   What we do is we break it into patches.
[00:15:01.680 --> 00:15:06.640]   So you're pretty much converting that into small patches.
[00:15:06.640 --> 00:15:09.720]   And this thing has been explained very well
[00:15:09.720 --> 00:15:13.360]   in a previous one of our blogs in patch embedding.
[00:15:13.360 --> 00:15:15.960]   So I'm not going to go into the details of this.
[00:15:15.960 --> 00:15:18.080]   I'm just going to do a high-level overview.
[00:15:18.080 --> 00:15:20.880]   But if you are to have a look at exactly what's
[00:15:20.880 --> 00:15:23.960]   going on in patch embeddings and how this thing gets converted
[00:15:23.960 --> 00:15:29.960]   to how everything gets converted into a vector from a very
[00:15:29.960 --> 00:15:32.400]   detailed perspective, then I've shared that link again
[00:15:32.400 --> 00:15:33.640]   in that blog.
[00:15:33.640 --> 00:15:36.840]   But then coming back to from a high-level perspective,
[00:15:36.840 --> 00:15:40.400]   then what happens is each one of these patches,
[00:15:40.400 --> 00:15:44.720]   like 1, 2, 3, 4, 5, 6, they're pretty much nine patches.
[00:15:44.720 --> 00:15:47.160]   Each one of these patches is represented by a vector.
[00:15:47.160 --> 00:15:49.360]   So there's two, and then that's the ninth one.
[00:15:49.360 --> 00:15:53.840]   So each long vector is a representation of that patch.
[00:15:53.840 --> 00:15:56.920]   So think of it that the first vector
[00:15:56.920 --> 00:16:02.200]   is a representation of this small patch at the top left.
[00:16:02.200 --> 00:16:03.800]   So that's the first vector.
[00:16:03.800 --> 00:16:06.800]   Then the second vector is this representation
[00:16:06.800 --> 00:16:07.960]   of this part of the image.
[00:16:07.960 --> 00:16:09.840]   So that's the second vector, and so on.
[00:16:09.840 --> 00:16:13.320]   The ninth vector then becomes the representation
[00:16:13.320 --> 00:16:15.400]   of this part of the image.
[00:16:15.400 --> 00:16:18.080]   So that's just basically then--
[00:16:18.080 --> 00:16:20.160]   because a computer doesn't know--
[00:16:20.160 --> 00:16:21.720]   it can't read an image directly.
[00:16:21.720 --> 00:16:24.360]   You have to convert it into a vector or a matrix.
[00:16:24.360 --> 00:16:26.960]   So you pretty much get a matrix that's
[00:16:26.960 --> 00:16:31.920]   consisting of nine patches, just in this particular example.
[00:16:31.920 --> 00:16:35.880]   Then as Dr. Habib said, it goes into the Mixer layer.
[00:16:35.880 --> 00:16:38.040]   And in the Mixer layer, there's some operations
[00:16:38.040 --> 00:16:39.520]   that we will look into.
[00:16:39.520 --> 00:16:42.000]   But pretty much an MLP everywhere
[00:16:42.000 --> 00:16:45.760]   that you see or you will see in part of this paper, an MLP,
[00:16:45.760 --> 00:16:48.400]   that's exactly, as Dr. Habib already mentioned,
[00:16:48.440 --> 00:16:51.800]   it's a first convolution layer, two convolution layers,
[00:16:51.800 --> 00:16:56.760]   and then separated by a GLUE or non-linearity, basically.
[00:16:56.760 --> 00:16:59.720]   And fully connected then is pretty much in PyTorch.
[00:16:59.720 --> 00:17:01.480]   You could just say nn.linear.
[00:17:01.480 --> 00:17:04.680]   Or in TensorFlow, I believe you could say dense.
[00:17:04.680 --> 00:17:06.920]   So I'm not a TensorFlow user.
[00:17:06.920 --> 00:17:08.280]   I could be very well wrong here.
[00:17:08.280 --> 00:17:11.240]   But I do believe dense should be what
[00:17:11.240 --> 00:17:12.960]   is a fully connected layer in terms
[00:17:12.960 --> 00:17:15.120]   of PyTorch and TensorFlow.
[00:17:15.120 --> 00:17:16.760]   So that's the main overall idea.
[00:17:16.760 --> 00:17:20.360]   I guess now the next step is for us--
[00:17:20.360 --> 00:17:22.040]   from a high-level perspective, then that's
[00:17:22.040 --> 00:17:23.720]   exactly what's going on.
[00:17:23.720 --> 00:17:28.000]   But then the next step for us is to understand this Mixer layer
[00:17:28.000 --> 00:17:31.600]   on what exactly is going on in the Mixer layer.
[00:17:31.600 --> 00:17:34.320]   And let's start by doing that.
[00:17:34.320 --> 00:17:37.600]   Let's start by looking at things in a simpler way.
[00:17:37.600 --> 00:17:42.320]   So I believe, again, from the MLP Mixer blog, Dr. Habib,
[00:17:42.320 --> 00:17:44.160]   I'll take that.
[00:17:44.160 --> 00:17:49.920]   I'll take your visualizations because it makes
[00:17:49.920 --> 00:17:51.520]   things really easy to explain.
[00:17:51.520 --> 00:17:53.120]   So maybe figure two, is that something
[00:17:53.120 --> 00:17:56.600]   that's going to help you explain how everything gets converted
[00:17:56.600 --> 00:17:58.000]   into a patch embedding?
[00:17:58.000 --> 00:17:58.960]   Yes.
[00:17:58.960 --> 00:18:01.560]   Should I copy and paste in one node?
[00:18:01.560 --> 00:18:05.400]   Yeah, whatever you prefer.
[00:18:05.400 --> 00:18:06.680]   Yeah.
[00:18:06.680 --> 00:18:08.960]   OK, so we are right now--
[00:18:08.960 --> 00:18:13.120]   so what we will present to you is one forward pass.
[00:18:13.120 --> 00:18:17.400]   So we'll take one image, and we will go one by one each step
[00:18:17.400 --> 00:18:19.760]   to explain exactly what's happening.
[00:18:19.760 --> 00:18:21.800]   And then you can just apply this operation
[00:18:21.800 --> 00:18:23.400]   to millions of images.
[00:18:23.400 --> 00:18:24.560]   It will be the same.
[00:18:24.560 --> 00:18:29.000]   So in this case, we have image of the frog.
[00:18:29.000 --> 00:18:31.760]   And so the step which we are describing
[00:18:31.760 --> 00:18:36.880]   is converting patch to some embedding vector.
[00:18:36.880 --> 00:18:40.440]   So this process is very similar to Vision Transformer.
[00:18:40.440 --> 00:18:42.640]   So this part is one-to-one.
[00:18:42.640 --> 00:18:46.600]   So the way how it works, we have image 224 by 224.
[00:18:46.600 --> 00:18:47.880]   And you know what we can do?
[00:18:47.880 --> 00:18:52.920]   We can divide this image into a grid or into the patches.
[00:18:52.920 --> 00:18:58.080]   So the patch which we will use, it will be 16 by 16.
[00:18:58.080 --> 00:19:03.160]   So if we do that, we will have 196 patches.
[00:19:03.160 --> 00:19:07.120]   So here, the first patch will be the top left corner,
[00:19:07.120 --> 00:19:08.520]   which is green.
[00:19:08.520 --> 00:19:10.640]   And then the last one will be this black one,
[00:19:10.640 --> 00:19:12.400]   which is 196 patch.
[00:19:12.400 --> 00:19:16.080]   It's located in the top right corner.
[00:19:16.080 --> 00:19:20.080]   And so if we'll now imagine we just break down the image,
[00:19:20.080 --> 00:19:22.320]   and we have this 196 patches.
[00:19:22.320 --> 00:19:26.000]   And each of these patches will have dimension of 16 by 16.
[00:19:26.000 --> 00:19:30.120]   And we have three channels, right?
[00:19:30.120 --> 00:19:33.400]   Because images comes in red, green, blue.
[00:19:33.400 --> 00:19:35.560]   So we have three channels.
[00:19:35.560 --> 00:19:39.600]   So now what we're going to do, we will take this.
[00:19:39.600 --> 00:19:43.640]   So we'll take this patch, and we'll completely flatten it.
[00:19:43.640 --> 00:19:47.400]   So 16 multiplied by 16 will be 256.
[00:19:47.400 --> 00:19:48.920]   And we have three channels.
[00:19:48.920 --> 00:19:53.200]   We have three channels.
[00:19:53.200 --> 00:19:55.360]   It will be 25--
[00:19:55.360 --> 00:19:57.120]   250-- 700--
[00:19:57.120 --> 00:19:58.360]   768.
[00:19:58.360 --> 00:19:59.240]   768, sorry.
[00:19:59.240 --> 00:20:00.600]   768.
[00:20:00.600 --> 00:20:02.560]   So we basically take the patch.
[00:20:02.560 --> 00:20:04.560]   We flatten it to 768.
[00:20:04.560 --> 00:20:10.040]   So each pixels will be just a single vector.
[00:20:10.040 --> 00:20:13.120]   And then we do for all 196 patches.
[00:20:13.120 --> 00:20:20.360]   And then what we can do, we can--
[00:20:20.360 --> 00:20:23.520]   in PyTorch, we have an n dot linear,
[00:20:23.520 --> 00:20:26.240]   which takes the 768.
[00:20:26.240 --> 00:20:29.640]   And we can project it to any dimension we want.
[00:20:29.640 --> 00:20:34.760]   So we can either do one-to-one measure.
[00:20:34.760 --> 00:20:40.080]   Either we can project to 768 or to 512.
[00:20:40.080 --> 00:20:40.920]   So this is it.
[00:20:40.920 --> 00:20:43.920]   So it's a one shared layer.
[00:20:43.920 --> 00:20:46.040]   And we go through all the patches.
[00:20:46.040 --> 00:20:51.480]   So what we will get, we will get 196 for each patch
[00:20:51.480 --> 00:20:54.000]   and 768 vector.
[00:20:54.000 --> 00:20:55.600]   So think about this.
[00:20:55.640 --> 00:21:00.200]   We will get a table, which 196, 768.
[00:21:00.200 --> 00:21:03.160]   So there is a terminology, which I would like to say.
[00:21:03.160 --> 00:21:09.360]   So in the paper, they use channel mixings.
[00:21:09.360 --> 00:21:13.080]   Channels-- channel refers to the--
[00:21:13.080 --> 00:21:17.840]   so for each patch, if we use the embedding dimension 512,
[00:21:17.840 --> 00:21:19.760]   so we will have 512 channels.
[00:21:19.760 --> 00:21:24.000]   If we use 768, we will have 768 channels.
[00:21:24.000 --> 00:21:28.960]   So channels will refer to a column in a vector.
[00:21:28.960 --> 00:21:32.520]   And then I believe tokens, Amman, correct me
[00:21:32.520 --> 00:21:34.920]   if I'm wrong, refers to the number of patches.
[00:21:34.920 --> 00:21:37.600]   So it's 190.
[00:21:37.600 --> 00:21:40.680]   So basically, you can say for each image,
[00:21:40.680 --> 00:21:44.760]   we'll get token by channels or number of patches
[00:21:44.760 --> 00:21:46.120]   by embedding dimension.
[00:21:46.120 --> 00:21:51.240]   So this is the first step.
[00:21:51.240 --> 00:21:57.480]   And this is very similar to transformers.
[00:21:57.480 --> 00:21:59.120]   This is the only thing which is similar
[00:21:59.120 --> 00:22:02.240]   between transformers and MLPs.
[00:22:02.240 --> 00:22:04.960]   Yeah, it's so far, until here, this is--
[00:22:04.960 --> 00:22:05.560]   Yes.
[00:22:05.560 --> 00:22:10.560]   All of this from back on is the same as vision transformers.
[00:22:10.560 --> 00:22:11.760]   I'm just going to type width.
[00:22:11.760 --> 00:22:13.720]   Yeah, so this is exactly the same.
[00:22:13.720 --> 00:22:16.000]   So so far, as Dr. Habib is saying,
[00:22:16.000 --> 00:22:18.560]   because you have-- say you have an input of 2 to 4
[00:22:18.560 --> 00:22:21.160]   by 2 to 4 image, and each patch, basically,
[00:22:21.160 --> 00:22:24.080]   the height and width of this patch is 16 by 16.
[00:22:24.080 --> 00:22:28.480]   So you'll get basically 196 patches in total.
[00:22:28.480 --> 00:22:31.000]   And then each of these patches could be flattened,
[00:22:31.000 --> 00:22:34.240]   or it could be projected using this linear projection layer.
[00:22:34.240 --> 00:22:38.800]   So in the end, every patch becomes a vector here like this.
[00:22:38.800 --> 00:22:41.960]   So you can have every patch represented as a vector.
[00:22:41.960 --> 00:22:43.440]   The dimensions of this vector could
[00:22:43.440 --> 00:22:46.400]   be depending on this linear projection layer.
[00:22:46.400 --> 00:22:47.480]   It could be 512.
[00:22:47.480 --> 00:22:48.600]   It could be 768.
[00:22:48.600 --> 00:22:50.600]   It could be whatever we want it to be.
[00:22:50.600 --> 00:22:53.040]   But the main point is that until this point,
[00:22:53.040 --> 00:22:54.360]   then, you get like this.
[00:22:54.360 --> 00:22:57.160]   You could stack these all vectors together.
[00:22:57.160 --> 00:23:01.920]   So you get a 196 by 768 matrix.
[00:23:01.920 --> 00:23:04.320]   So now your input image has been converted
[00:23:04.320 --> 00:23:07.920]   into a 196 by 768 matrix.
[00:23:07.920 --> 00:23:10.200]   Now, whatever happens after this point
[00:23:10.200 --> 00:23:13.520]   is what's different in the MLP mixer than the vision
[00:23:13.520 --> 00:23:14.640]   transformer.
[00:23:14.640 --> 00:23:17.480]   And that's what we're going to look at now.
[00:23:17.480 --> 00:23:18.440]   Right.
[00:23:18.440 --> 00:23:20.240]   So I hope this part is clear.
[00:23:20.240 --> 00:23:22.920]   I mean, we tried to explain very nicely.
[00:23:22.920 --> 00:23:27.480]   I think after this, we will come back to this and explain--
[00:23:27.480 --> 00:23:29.640]   we are using right now nn.linear.
[00:23:29.640 --> 00:23:33.720]   But we will show you how you can replace the same operation
[00:23:33.720 --> 00:23:35.400]   by convolutions.
[00:23:35.400 --> 00:23:38.920]   But I think we will come back to this towards the end,
[00:23:38.920 --> 00:23:42.000]   once we go how exactly it is done in the paper.
[00:23:42.000 --> 00:23:46.320]   So Amal, should we move to a mixer layer after today?
[00:23:46.320 --> 00:23:47.440]   Yes, one second.
[00:23:47.440 --> 00:23:48.960]   So I was thinking for the mixer layer,
[00:23:48.960 --> 00:23:55.120]   I did have a way to explain the mixer layer.
[00:23:55.120 --> 00:23:55.640]   OK.
[00:23:55.640 --> 00:24:00.040]   Let me-- it's OK if you want to go to it.
[00:24:00.040 --> 00:24:02.240]   Dr. Habib, you can definitely go to this mixer layer.
[00:24:02.240 --> 00:24:03.400]   I'll just copy paste this.
[00:24:03.400 --> 00:24:03.880]   Yes.
[00:24:03.880 --> 00:24:05.320]   Do you want me to copy paste this?
[00:24:05.320 --> 00:24:05.840]   Yes.
[00:24:05.840 --> 00:24:07.640]   Or do you want me to use your visualization?
[00:24:07.640 --> 00:24:09.160]   I think I will use mine.
[00:24:09.160 --> 00:24:12.440]   I think this will be easier for me, because--
[00:24:12.440 --> 00:24:14.320]   OK, so if we go down--
[00:24:14.320 --> 00:24:16.080]   Is it this one?
[00:24:16.080 --> 00:24:16.840]   Yes.
[00:24:16.840 --> 00:24:17.480]   Oh, yes.
[00:24:17.480 --> 00:24:18.440]   It's the first one.
[00:24:18.440 --> 00:24:21.320]   Yeah, so this whole thing, if you can copy paste,
[00:24:21.320 --> 00:24:23.040]   because this is the whole mixer layer.
[00:24:23.040 --> 00:24:23.560]   Yeah.
[00:24:23.560 --> 00:24:25.200]   OK, one second.
[00:24:25.200 --> 00:24:27.880]   So I'll just copy this bit here.
[00:24:27.880 --> 00:24:29.240]   Yeah.
[00:24:29.240 --> 00:24:30.360]   OK, one second.
[00:24:30.360 --> 00:24:30.880]   OK.
[00:24:30.880 --> 00:24:37.440]   Is that what you want?
[00:24:37.440 --> 00:24:38.080]   That's perfect.
[00:24:38.080 --> 00:24:40.520]   That's perfect.
[00:24:40.520 --> 00:24:46.440]   So in my-- OK, so we had-- yes.
[00:24:46.440 --> 00:24:47.440]   Thank you, Amal.
[00:24:47.440 --> 00:24:48.400]   It's 512.
[00:24:48.400 --> 00:24:49.240]   So we used 512.
[00:24:49.240 --> 00:24:50.480]   Oh, yeah, sorry.
[00:24:50.480 --> 00:24:52.840]   Yeah, I think there was a mistake in the image.
[00:24:52.840 --> 00:24:54.160]   And we will correct it later.
[00:24:54.160 --> 00:24:58.880]   So basically, we took this 768 patch.
[00:24:58.880 --> 00:25:00.680]   We took a patch, we flattened, and then we
[00:25:00.680 --> 00:25:04.320]   projected to the 512 dimensions.
[00:25:04.320 --> 00:25:07.320]   And so this way, we will refer them to channels.
[00:25:07.320 --> 00:25:09.800]   So for each patch, have 512 channels.
[00:25:09.800 --> 00:25:17.400]   And we have 196 tokens.
[00:25:17.400 --> 00:25:22.960]   OK, so here's something very interesting.
[00:25:22.960 --> 00:25:26.840]   So here is the-- we enter to mixer layer.
[00:25:26.840 --> 00:25:29.360]   So mixer layer is something very simple.
[00:25:29.360 --> 00:25:31.480]   You always start with the layer norm.
[00:25:31.480 --> 00:25:34.120]   It's a type of normalization.
[00:25:34.120 --> 00:25:36.560]   I'm pretty sure you're familiar with batch norms.
[00:25:36.560 --> 00:25:40.560]   So layer norms is people prefer to use in kind
[00:25:40.560 --> 00:25:43.040]   of transformer-like architecture.
[00:25:43.040 --> 00:25:46.520]   It just normalizes all features between--
[00:25:46.520 --> 00:25:50.480]   I forgot, 1 and 0, or there is some arbitrary value.
[00:25:50.480 --> 00:25:51.240]   It doesn't matter.
[00:25:51.240 --> 00:25:53.120]   It's just a normalization layer.
[00:25:53.120 --> 00:25:58.840]   OK, so there is first--
[00:25:58.840 --> 00:26:03.640]   so what we do is, so first, we will flip these features.
[00:26:03.640 --> 00:26:07.640]   We will have 512 by 196.
[00:26:07.640 --> 00:26:10.800]   So first layer called token mixing layer.
[00:26:10.800 --> 00:26:14.240]   And token refers to--
[00:26:14.240 --> 00:26:17.040]   I believe it's across the--
[00:26:17.040 --> 00:26:19.520]   we mix across the patches, right?
[00:26:19.520 --> 00:26:22.400]   Yeah, is it OK if I quickly make a visualization?
[00:26:22.400 --> 00:26:24.360]   And I know exactly what you're trying to say.
[00:26:24.360 --> 00:26:25.440]   Yeah, yeah, please.
[00:26:25.440 --> 00:26:27.160]   I know exactly what you're trying to say.
[00:26:27.160 --> 00:26:28.000]   Yes.
[00:26:28.000 --> 00:26:30.360]   OK, so basically, because our input
[00:26:30.360 --> 00:26:31.720]   looks something like this, right?
[00:26:31.720 --> 00:26:35.840]   You had 196 rows and 512 channels.
[00:26:35.840 --> 00:26:40.400]   But because you stagged all the vectors together,
[00:26:40.400 --> 00:26:43.560]   because we don't want to forget what exactly was happening.
[00:26:43.560 --> 00:26:47.760]   So each patch then is a vector like this, right?
[00:26:47.760 --> 00:26:49.800]   This is the first--
[00:26:49.800 --> 00:26:55.520]   like all of this, the first line is just a 512 long vector.
[00:26:55.520 --> 00:26:58.640]   So what these 512 are, these are the channels.
[00:26:58.640 --> 00:27:01.600]   So that's basically the embedding dimension
[00:27:01.600 --> 00:27:02.720]   if you want to call it.
[00:27:02.720 --> 00:27:07.760]   It basically means that you're representing a single token
[00:27:07.760 --> 00:27:10.480]   with 512 channels.
[00:27:10.480 --> 00:27:11.800]   So that's what this--
[00:27:11.800 --> 00:27:17.040]   but if you look at it, then every row is a token in this--
[00:27:17.040 --> 00:27:20.160]   like the input matrix of 196 by 512,
[00:27:20.160 --> 00:27:22.520]   which we are looking at this.
[00:27:22.520 --> 00:27:25.520]   In this input matrix of 196 by 512,
[00:27:25.520 --> 00:27:28.920]   every row is a token, which is coming
[00:27:28.920 --> 00:27:30.320]   from this patch on the left.
[00:27:30.320 --> 00:27:33.880]   So every row then is basically this representation
[00:27:33.880 --> 00:27:34.600]   of the patch.
[00:27:34.600 --> 00:27:38.560]   So patch and token are used in a similar way.
[00:27:38.560 --> 00:27:41.400]   Token is just a word of saying that the patch has
[00:27:41.400 --> 00:27:42.960]   been converted to a vector.
[00:27:42.960 --> 00:27:46.720]   So every row then in this case is, again, like that token.
[00:27:46.720 --> 00:27:50.800]   And then every column is basically a single channel.
[00:27:50.800 --> 00:27:55.120]   So you have 512 columns, which means you have 512 channels.
[00:27:55.120 --> 00:28:01.360]   But when you take the transpose, then you have 196 on the columns
[00:28:01.360 --> 00:28:04.200]   and you have 512 along the rows.
[00:28:04.200 --> 00:28:08.080]   So what that means is that every row pretty much now
[00:28:08.080 --> 00:28:09.640]   is a single channel.
[00:28:09.640 --> 00:28:11.520]   Every row is a single channel.
[00:28:11.520 --> 00:28:13.720]   So this is a channel.
[00:28:13.720 --> 00:28:18.960]   And then every column is a token.
[00:28:18.960 --> 00:28:22.480]   So a single token with all the 512 channels
[00:28:22.480 --> 00:28:24.480]   is basically this column.
[00:28:24.480 --> 00:28:28.200]   And then a single channel with all the 196 tokens
[00:28:28.200 --> 00:28:29.600]   is basically this row.
[00:28:29.600 --> 00:28:31.120]   Does that help, Dr. Habib?
[00:28:31.120 --> 00:28:33.000]   Is that exactly what you're trying to say?
[00:28:33.000 --> 00:28:33.760]   Yeah.
[00:28:33.760 --> 00:28:36.080]   Basically, we are looking for each position
[00:28:36.080 --> 00:28:38.720]   across the patches, across the tokens.
[00:28:38.720 --> 00:28:42.120]   So yes, that's beautifully explained.
[00:28:42.120 --> 00:28:47.160]   So now what we have, we have one linear layers, which takes--
[00:28:47.160 --> 00:28:52.040]   very something, but it takes 196 features.
[00:28:52.040 --> 00:28:56.080]   But just ignore for now the projection.
[00:28:56.080 --> 00:28:58.960]   But what it does, it takes 196 features,
[00:28:58.960 --> 00:29:01.480]   and then it returns 196 features.
[00:29:01.480 --> 00:29:04.760]   So it's a single shared linear layer.
[00:29:04.760 --> 00:29:08.680]   What we do, we apply all--
[00:29:08.680 --> 00:29:12.080]   across all the channels, we apply patch.
[00:29:12.080 --> 00:29:17.240]   And we kind of look across the patches first.
[00:29:17.240 --> 00:29:21.200]   It's kind of-- think about this as a global view.
[00:29:21.200 --> 00:29:22.000]   Can I jump in?
[00:29:22.000 --> 00:29:23.240]   We globally look for--
[00:29:23.240 --> 00:29:23.760]   Yes.
[00:29:23.760 --> 00:29:24.680]   Can I quickly jump in?
[00:29:24.680 --> 00:29:25.200]   Feel free.
[00:29:25.200 --> 00:29:25.680]   Yeah.
[00:29:25.680 --> 00:29:30.040]   I guess the main point, then, to look in this token mixing
[00:29:30.040 --> 00:29:32.600]   layer is that the mixing is actually
[00:29:32.600 --> 00:29:34.400]   happening along the rows, because that's
[00:29:34.400 --> 00:29:36.880]   how the linear layers do the mixing.
[00:29:36.880 --> 00:29:39.440]   The mixing always happen across the rows.
[00:29:39.440 --> 00:29:42.000]   So they will take each row in one by one,
[00:29:42.000 --> 00:29:44.800]   and then the mixing is going to happen across the rows.
[00:29:44.800 --> 00:29:47.720]   But what does that mean when we say the mixing is happening
[00:29:47.720 --> 00:29:48.880]   across the rows?
[00:29:48.880 --> 00:29:51.160]   Which means we're mixing the columns.
[00:29:51.160 --> 00:29:55.280]   If you have a single vector like this, or a single token,
[00:29:55.280 --> 00:29:58.040]   basically, this row represents a token.
[00:29:58.040 --> 00:30:02.600]   And then you have 512 channels.
[00:30:02.600 --> 00:30:05.280]   Sorry, I wrote that wrong.
[00:30:05.280 --> 00:30:07.160]   In this, the rows are the channels.
[00:30:07.160 --> 00:30:10.160]   So you have a row as a single channel,
[00:30:10.160 --> 00:30:13.600]   and the columns are the 196 tokens.
[00:30:13.600 --> 00:30:15.640]   So when you're doing mixing along the rows,
[00:30:15.640 --> 00:30:18.880]   you're actually mixing these 196 tokens.
[00:30:18.880 --> 00:30:21.920]   Every single channel goes in as input,
[00:30:21.920 --> 00:30:25.400]   and then you mix the 512 channels.
[00:30:25.400 --> 00:30:28.120]   Sorry, every single channel goes as an input,
[00:30:28.120 --> 00:30:30.840]   and then you mix the 196 tokens.
[00:30:30.840 --> 00:30:34.080]   That's why this layer has been called as token mixing.
[00:30:34.080 --> 00:30:35.560]   Because when you're mixing the rows,
[00:30:35.560 --> 00:30:37.640]   you're essentially mixing the tokens.
[00:30:37.640 --> 00:30:39.520]   Does that help?
[00:30:39.520 --> 00:30:40.040]   Yeah.
[00:30:40.040 --> 00:30:44.000]   I know I missed the token and channel quite a few times,
[00:30:44.000 --> 00:30:47.720]   but I hope I was able to get the point across.
[00:30:47.720 --> 00:30:48.800]   Right.
[00:30:48.800 --> 00:30:50.840]   Yeah, so you know--
[00:30:50.840 --> 00:30:53.640]   right, maybe, Aman, you can just explain the next part,
[00:30:53.640 --> 00:30:55.920]   because you explained so nicely.
[00:30:55.920 --> 00:30:57.640]   Oh, OK, sure.
[00:30:57.640 --> 00:31:03.280]   So then in the next part, then, because we're
[00:31:03.280 --> 00:31:06.200]   mixing the rows in this bit, which
[00:31:06.200 --> 00:31:07.720]   means we're mixing the tokens.
[00:31:07.720 --> 00:31:11.640]   So now we're mixing spatial information in the image.
[00:31:11.640 --> 00:31:14.920]   Each token is being mixed with every other token, which
[00:31:14.920 --> 00:31:17.440]   means you're mixing the spatial information.
[00:31:17.440 --> 00:31:19.920]   But in a convolution, what we also mix
[00:31:19.920 --> 00:31:22.920]   is the information that we need to add is also
[00:31:22.920 --> 00:31:25.840]   we need to be able to mix the channels, because we also
[00:31:25.840 --> 00:31:27.120]   have cross channels.
[00:31:27.120 --> 00:31:30.560]   So all the previous papers, like squeeze and excitation,
[00:31:30.560 --> 00:31:32.800]   pretty much, or even in a convolution,
[00:31:32.800 --> 00:31:34.960]   when you say the number of in channels is x
[00:31:34.960 --> 00:31:37.160]   and out channels is y, you're actually
[00:31:37.160 --> 00:31:40.320]   mixing across the channel dimension as well.
[00:31:40.320 --> 00:31:44.960]   So what we do is then we take this output from the token
[00:31:44.960 --> 00:31:48.240]   mixer, and we do a transpose again.
[00:31:48.240 --> 00:31:53.320]   So when we do a transpose this time, we get 196 by 512.
[00:31:53.320 --> 00:31:54.640]   What does that mean?
[00:31:54.640 --> 00:31:59.320]   At this point now, every row becomes a token,
[00:31:59.320 --> 00:32:02.360]   and every column becomes a channel,
[00:32:02.360 --> 00:32:04.360]   just as when we started.
[00:32:04.360 --> 00:32:08.720]   So every row becomes a token, and every column
[00:32:08.720 --> 00:32:10.920]   becomes a channel.
[00:32:10.920 --> 00:32:16.640]   So then again, we pass this through the same sort of MLP
[00:32:16.640 --> 00:32:18.320]   is what it is called.
[00:32:18.320 --> 00:32:20.400]   We pass it through another MLP.
[00:32:20.400 --> 00:32:22.480]   But actually, as I've previously mentioned,
[00:32:22.480 --> 00:32:26.880]   and a fully connected will take in row by row, like one row in,
[00:32:26.880 --> 00:32:27.800]   row by row.
[00:32:27.800 --> 00:32:29.480]   And what it is going to do is it's
[00:32:29.480 --> 00:32:31.800]   going to do the mixing across the rows.
[00:32:31.800 --> 00:32:36.960]   So the mixing is actually happening across the rows.
[00:32:36.960 --> 00:32:40.080]   And what that means that you're actually mixing the channels,
[00:32:40.080 --> 00:32:42.600]   because every column is a channel.
[00:32:42.600 --> 00:32:45.040]   So when you're mixing that columns,
[00:32:45.040 --> 00:32:46.520]   you're actually mixing the channels.
[00:32:46.520 --> 00:32:50.320]   So that's why this particular layer is called channel mixing.
[00:32:50.320 --> 00:32:52.040]   So the first layer is called token mixer,
[00:32:52.040 --> 00:32:54.600]   because you're mixing across the rows that are tokens.
[00:32:54.600 --> 00:32:57.080]   And then the second one, you're mixing the channels.
[00:32:57.080 --> 00:32:59.640]   That's the key difference in the two.
[00:32:59.640 --> 00:33:00.160]   Yeah.
[00:33:00.160 --> 00:33:05.240]   So just to get a global picture, you have patches.
[00:33:05.240 --> 00:33:07.280]   You're doing global and local mixing.
[00:33:07.280 --> 00:33:12.080]   This two operations basically perform the same,
[00:33:12.080 --> 00:33:13.520]   kind of performing the same.
[00:33:13.520 --> 00:33:15.440]   And that's it.
[00:33:15.440 --> 00:33:17.120]   So yeah, this is the architecture.
[00:33:17.120 --> 00:33:21.480]   And then you get the output, which is exactly 196, 512.
[00:33:21.480 --> 00:33:30.400]   And then depending on the configuration,
[00:33:30.400 --> 00:33:31.920]   you can have 10 or 12 layers.
[00:33:31.920 --> 00:33:32.640]   I forgot.
[00:33:32.640 --> 00:33:33.960]   And that's pretty much it.
[00:33:33.960 --> 00:33:36.720]   And then at the end, you just take a mean.
[00:33:36.720 --> 00:33:39.600]   And then you have this output layer,
[00:33:39.600 --> 00:33:41.160]   which predicts number of the classes.
[00:33:41.160 --> 00:33:43.160]   And that's it.
[00:33:43.160 --> 00:33:47.800]   It's really as simple as it is.
[00:33:47.800 --> 00:33:50.400]   It's not complicated.
[00:33:50.400 --> 00:33:52.200]   Yeah.
[00:33:52.200 --> 00:33:52.720]   Yeah.
[00:33:52.720 --> 00:33:55.840]   So Aman, do you want to add something to this?
[00:33:55.840 --> 00:33:59.200]   Yeah, I guess I was also baffled when I was--
[00:33:59.200 --> 00:34:03.840]   I'm used to spending more time trying to understand the paper.
[00:34:03.840 --> 00:34:05.400]   And it takes me--
[00:34:05.400 --> 00:34:07.200]   because I'm very slow, it takes me longer
[00:34:07.200 --> 00:34:08.160]   to understand the paper.
[00:34:08.160 --> 00:34:09.600]   But when I was reading this paper,
[00:34:09.600 --> 00:34:12.200]   it took me like 5, 10 minutes to understand
[00:34:12.200 --> 00:34:14.960]   what was going on in the paper to understand the architecture.
[00:34:14.960 --> 00:34:17.160]   And I was like, OK, I must be missing something.
[00:34:17.160 --> 00:34:19.640]   There's something definitely-- there's
[00:34:19.640 --> 00:34:22.280]   some detail I'm missing or something like that.
[00:34:22.280 --> 00:34:24.000]   But this is really it.
[00:34:24.000 --> 00:34:27.200]   This is really MLP.
[00:34:27.200 --> 00:34:27.680]   Yeah.
[00:34:27.680 --> 00:34:29.600]   And you just do a transpose.
[00:34:29.600 --> 00:34:30.880]   Yeah.
[00:34:30.920 --> 00:34:34.000]   So now if we go a little bit down and--
[00:34:34.000 --> 00:34:37.520]   so this simple thing can compare to transformers.
[00:34:37.520 --> 00:34:41.760]   So if we go down now to the paper, let's go to the table.
[00:34:41.760 --> 00:34:42.400]   Yeah.
[00:34:42.400 --> 00:34:45.640]   Did you mean the different architectures?
[00:34:45.640 --> 00:34:46.160]   So yeah.
[00:34:46.160 --> 00:34:47.360]   So this is basically-- yeah.
[00:34:47.360 --> 00:34:50.320]   So this is basically a standard--
[00:34:50.320 --> 00:34:53.640]   your standard archive paper, which will tell you, look,
[00:34:53.640 --> 00:34:56.000]   we tested a different version of this architecture,
[00:34:56.000 --> 00:35:01.960]   different pack sizes, embedding dimensions, and so on.
[00:35:01.960 --> 00:35:04.400]   And they all kind of work nicely.
[00:35:04.400 --> 00:35:08.360]   There is nothing interesting in this table.
[00:35:08.360 --> 00:35:09.800]   It just shows you what they tested.
[00:35:09.800 --> 00:35:15.560]   So if we go down now a little bit, there is this--
[00:35:15.560 --> 00:35:16.080]   OK.
[00:35:16.080 --> 00:35:16.840]   Is it this one?
[00:35:16.840 --> 00:35:17.320]   Right.
[00:35:17.320 --> 00:35:18.080]   So this one, yeah.
[00:35:18.080 --> 00:35:23.880]   Basically, this just shows comparison.
[00:35:23.880 --> 00:35:26.960]   So for example, so they tried to do
[00:35:26.960 --> 00:35:30.240]   different pre-training tasks.
[00:35:30.240 --> 00:35:33.720]   And you can see, for example, if-- so since it's Google,
[00:35:33.720 --> 00:35:37.240]   they have this GFC 300 million.
[00:35:37.240 --> 00:35:39.520]   It's a secret data set, which only Google had access.
[00:35:39.520 --> 00:35:42.440]   So what they do recently, they just pre-train on this.
[00:35:42.440 --> 00:35:44.280]   And then they train on ImageNet.
[00:35:44.280 --> 00:35:46.520]   And you can see--
[00:35:46.520 --> 00:35:50.680]   so the mixers has--
[00:35:50.680 --> 00:35:56.600]   performs is 87.94 on ImageNet.
[00:35:56.600 --> 00:36:01.000]   And then the vision transformer performs 88.55.
[00:36:01.000 --> 00:36:02.720]   It's really close.
[00:36:02.720 --> 00:36:06.000]   The vision transformers is very heavy architecture.
[00:36:06.000 --> 00:36:07.960]   It uses attention and blah.
[00:36:07.960 --> 00:36:10.720]   And it's complicated.
[00:36:10.720 --> 00:36:14.880]   But you can see that they get really very, very close
[00:36:14.880 --> 00:36:19.680]   performance, which is very nice.
[00:36:19.680 --> 00:36:20.920]   So if we go to--
[00:36:20.920 --> 00:36:24.200]   Aman, do you want to add something here?
[00:36:24.200 --> 00:36:26.680]   No, I think you've got this covered.
[00:36:26.680 --> 00:36:27.200]   Keep going.
[00:36:27.200 --> 00:36:27.840]   Yeah.
[00:36:27.840 --> 00:36:30.720]   OK, so one thing which I want to point out,
[00:36:30.720 --> 00:36:32.760]   so they claim there is--
[00:36:32.760 --> 00:36:35.160]   if we look, so if you are--
[00:36:35.160 --> 00:36:37.680]   there is one advantage of this MLP architecture.
[00:36:37.680 --> 00:36:41.280]   So imagine you are a company, and you
[00:36:41.280 --> 00:36:42.840]   want to deploy something.
[00:36:42.840 --> 00:36:46.040]   And I don't know whether you will care about accuracy
[00:36:46.040 --> 00:36:48.120]   or not, because the accuracy is so close.
[00:36:48.120 --> 00:36:51.720]   One thing about this MLP mixture is that it
[00:36:51.720 --> 00:36:53.920]   has extremely high throughput.
[00:36:53.920 --> 00:36:56.160]   So it can predict images.
[00:36:56.160 --> 00:36:58.280]   You can have-- in some cases, you
[00:36:58.280 --> 00:37:03.880]   predict 120 images through TPU cores.
[00:37:03.880 --> 00:37:06.120]   And for example, for vision transformers,
[00:37:06.120 --> 00:37:10.520]   so if we look on the top, you can see that the mixer can
[00:37:10.520 --> 00:37:12.680]   predict 105.
[00:37:12.680 --> 00:37:16.160]   Oh, yeah, so in this case, which is a very heavy architecture,
[00:37:16.160 --> 00:37:19.200]   MLP mixtures are much more lighter,
[00:37:19.200 --> 00:37:22.880]   and they can predict a lot of images, high throughput
[00:37:22.880 --> 00:37:23.400]   images.
[00:37:23.400 --> 00:37:24.400]   So if you're a company--
[00:37:24.400 --> 00:37:26.680]   Almost three times, still three times the vision.
[00:37:26.680 --> 00:37:31.080]   Right, so if you are interested in deploying the models,
[00:37:31.080 --> 00:37:33.440]   you might be thinking, maybe you don't
[00:37:33.440 --> 00:37:34.880]   need vision transformers.
[00:37:34.880 --> 00:37:41.320]   You can just probably go just use MLP mixtures.
[00:37:41.320 --> 00:37:44.200]   And you have-- depending on what's your objective.
[00:37:44.200 --> 00:37:46.880]   So this is one of the advantages.
[00:37:46.880 --> 00:37:49.520]   So if we'll go to the next slide.
[00:37:49.520 --> 00:37:51.560]   Yeah, just the one thing I want to add here
[00:37:51.560 --> 00:37:53.240]   is the training times.
[00:37:53.240 --> 00:37:59.000]   So I know it says TPU v3 core days, but one thing when--
[00:37:59.000 --> 00:38:02.800]   this architecture, when it's pre-trained on ImageNet 21k,
[00:38:02.800 --> 00:38:05.520]   it's pre-trained on GFD 300 million.
[00:38:05.520 --> 00:38:08.400]   That just means that when you're using a simple architecture,
[00:38:08.400 --> 00:38:10.720]   you pre-train it on lots and lots of data.
[00:38:10.720 --> 00:38:13.840]   So you're actually pre-training on 300 million images.
[00:38:13.840 --> 00:38:16.080]   It's not like you take the simple architecture,
[00:38:16.080 --> 00:38:19.680]   and then you take EfficientNet, and you train both of them
[00:38:19.680 --> 00:38:22.000]   on ImageNet, which is 1 million images.
[00:38:22.000 --> 00:38:23.680]   And then you think that you'll still
[00:38:23.680 --> 00:38:24.920]   get comparative performance.
[00:38:24.920 --> 00:38:26.040]   That's not going to happen.
[00:38:26.040 --> 00:38:28.640]   Because when you have a simpler architecture,
[00:38:28.640 --> 00:38:31.720]   you pretty much have to give it a lot more data for it
[00:38:31.720 --> 00:38:34.680]   to learn things as compared to an EfficientNet that
[00:38:34.680 --> 00:38:37.040]   could have a very competitive performance just
[00:38:37.040 --> 00:38:38.520]   from the 1 million images.
[00:38:38.520 --> 00:38:40.400]   So in terms of costs, yes, it's going
[00:38:40.400 --> 00:38:43.840]   to cost a lot to do the first training.
[00:38:43.840 --> 00:38:45.640]   But if you're definitely, as Dr. Habib says,
[00:38:45.640 --> 00:38:48.960]   if you're definitely a company that cares about the throughput,
[00:38:48.960 --> 00:38:50.640]   because it's a simple architecture, which
[00:38:50.640 --> 00:38:53.160]   means it can do things faster.
[00:38:53.160 --> 00:38:54.920]   Yeah, that's the only thing I want to add.
[00:38:54.920 --> 00:38:57.920]   It's just the size of the pre-training data sets.
[00:38:57.920 --> 00:38:58.760]   Right.
[00:38:58.760 --> 00:39:01.280]   So then if we go down, there was one more--
[00:39:01.280 --> 00:39:02.480]   oh, so yeah, this graph.
[00:39:02.480 --> 00:39:04.880]   So I found-- so for example, they also
[00:39:04.880 --> 00:39:06.280]   make really nice points.
[00:39:06.280 --> 00:39:07.680]   So if we-- so yeah.
[00:39:07.680 --> 00:39:10.200]   So this-- you see this on the right graph.
[00:39:10.200 --> 00:39:11.640]   So this is very interesting.
[00:39:11.640 --> 00:39:13.640]   So there is something called scaling.
[00:39:13.640 --> 00:39:15.280]   So they did a very simple experiment.
[00:39:15.280 --> 00:39:20.040]   So they use vision transformers and mixers.
[00:39:20.040 --> 00:39:22.120]   And so what they did, they-- instead
[00:39:22.120 --> 00:39:25.240]   of training on 300 million images,
[00:39:25.240 --> 00:39:27.640]   they first trained on 10 million.
[00:39:27.640 --> 00:39:31.520]   And then they measure accuracy on ImageNet.
[00:39:31.520 --> 00:39:33.200]   And so, yes.
[00:39:33.200 --> 00:39:36.280]   So you can see, so this is on 10 million.
[00:39:36.280 --> 00:39:40.160]   And if we go to the 10 million, the vision transformers
[00:39:40.160 --> 00:39:43.560]   you can see it kind of plateaus a little bit.
[00:39:43.560 --> 00:39:45.800]   So there is no difference between 30 million
[00:39:45.800 --> 00:39:47.360]   and 100 million.
[00:39:47.360 --> 00:39:51.280]   It's kind of-- you will get the same accuracy.
[00:39:51.280 --> 00:39:54.960]   But in the case of mixers, what's interesting,
[00:39:54.960 --> 00:40:01.800]   the more data it has, it kind of goes kind of linearly,
[00:40:01.800 --> 00:40:03.000]   the better its performance.
[00:40:03.000 --> 00:40:04.960]   So it requires a lot of data to train.
[00:40:04.960 --> 00:40:07.560]   So this was also one of the points which
[00:40:07.560 --> 00:40:11.640]   the author making that perhaps if you have a bigger--
[00:40:11.640 --> 00:40:15.160]   more data, so it scales by the data.
[00:40:15.160 --> 00:40:18.440]   So if you provide more data, maybe we
[00:40:18.440 --> 00:40:22.120]   will see better performance than vision transformer or so on.
[00:40:22.120 --> 00:40:24.000]   And you can see for vision transformers,
[00:40:24.000 --> 00:40:24.960]   it's kind of--
[00:40:24.960 --> 00:40:28.080]   after 10 million, it just--
[00:40:28.080 --> 00:40:29.480]   it's kind of flat.
[00:40:29.480 --> 00:40:34.640]   So just to sum up, it's lighter architecture.
[00:40:34.640 --> 00:40:35.760]   It has high throughput.
[00:40:35.760 --> 00:40:39.320]   And it scales with the pre-trained data,
[00:40:39.320 --> 00:40:44.160]   I think, which is very interesting and nice property.
[00:40:44.160 --> 00:40:45.440]   Yeah, definitely.
[00:40:45.440 --> 00:40:46.640]   Yeah.
[00:40:46.640 --> 00:40:49.000]   OK, so let's go to the next--
[00:40:49.000 --> 00:40:52.920]   not this one, but the shuffling thing.
[00:40:52.920 --> 00:40:54.200]   Yeah.
[00:40:54.200 --> 00:40:55.280]   Yeah, so this--
[00:40:55.280 --> 00:40:56.280]   OK, yeah, this is--
[00:40:56.280 --> 00:40:58.080]   I'm trying to find where is the shuffling.
[00:40:58.080 --> 00:41:00.800]   This is-- that was--
[00:41:00.800 --> 00:41:02.280]   so this is just the table that's just
[00:41:02.280 --> 00:41:03.920]   comparing the results, right?
[00:41:03.920 --> 00:41:05.720]   Yeah, different-- it's just
[00:41:05.720 --> 00:41:08.720]   a bigger table, which compares different architecture with--
[00:41:08.720 --> 00:41:11.880]   This is just showing the weights and visualizing the--
[00:41:11.880 --> 00:41:13.840]   because it's just linear layers.
[00:41:13.840 --> 00:41:16.040]   I think the point here is you can make it-- it's
[00:41:16.040 --> 00:41:18.200]   easier to look at all the weights,
[00:41:18.200 --> 00:41:20.160]   and you can understand the architecture better
[00:41:20.160 --> 00:41:21.160]   because it's so simple.
[00:41:21.160 --> 00:41:23.000]   And simple things are easier to understand.
[00:41:23.000 --> 00:41:25.120]   Right, so here, they want to make arguments.
[00:41:25.120 --> 00:41:27.840]   The weights, you can draw them.
[00:41:27.840 --> 00:41:29.840]   And there are some--
[00:41:29.840 --> 00:41:35.600]   if you see-- there is this thing that in CNN,
[00:41:35.600 --> 00:41:40.920]   early layers are responsible for recognizing small features.
[00:41:40.920 --> 00:41:44.000]   And then as you go, there is some global features.
[00:41:44.000 --> 00:41:47.680]   So here, they are also showing that this--
[00:41:47.680 --> 00:41:51.280]   they were curious whether it worked in the same way as CNN.
[00:41:51.280 --> 00:41:52.840]   And there are some evidence.
[00:41:52.840 --> 00:41:56.920]   You can see there are these filters in this--
[00:41:56.920 --> 00:41:59.760]   and in this linear layer.
[00:41:59.760 --> 00:42:04.960]   And they have-- so red means negative number.
[00:42:04.960 --> 00:42:06.360]   Blue means positive numbers.
[00:42:06.360 --> 00:42:08.560]   And in the early layers, it just--
[00:42:08.560 --> 00:42:09.920]   they are focusing on something.
[00:42:09.920 --> 00:42:12.400]   But as you go to the--
[00:42:12.400 --> 00:42:17.240]   to the-- a little bit later layers,
[00:42:17.240 --> 00:42:20.160]   you can see there is some kind of complex patterns emerging,
[00:42:20.160 --> 00:42:24.280]   which they try to recognize.
[00:42:24.280 --> 00:42:29.920]   So that's pretty much it, yeah, in this.
[00:42:29.920 --> 00:42:31.200]   I just want to add context.
[00:42:31.200 --> 00:42:33.480]   Yeah, I just want to add context.
[00:42:33.480 --> 00:42:38.280]   Where Dr. Habib mentioned that it's similar to CNN,
[00:42:38.280 --> 00:42:42.000]   there's this paper visualizing CNN layers by--
[00:42:42.000 --> 00:42:45.040]   I guess that's the paper, Zeiler and Ferguson, I believe.
[00:42:45.040 --> 00:42:46.040]   That's the one.
[00:42:46.040 --> 00:42:47.800]   So I just want to quickly find that paper.
[00:42:47.800 --> 00:42:50.160]   It's this one, Visualizing and Understanding
[00:42:50.160 --> 00:42:51.640]   Convolution Neural Networks.
[00:42:51.640 --> 00:42:54.040]   In this paper, you will see basically
[00:42:54.040 --> 00:42:57.720]   that the patterns are very similar to MLP Mixer.
[00:42:57.720 --> 00:42:59.000]   So I just want to add context.
[00:42:59.000 --> 00:43:02.480]   That's the paper for anybody watching this after.
[00:43:02.480 --> 00:43:07.080]   OK, yes, and there is also very nice distill articles
[00:43:07.080 --> 00:43:11.680]   about different visualizing filters of neural networks,
[00:43:11.680 --> 00:43:13.960]   just in case somebody wants to read.
[00:43:13.960 --> 00:43:14.600]   OK, so if you--
[00:43:14.600 --> 00:43:15.920]   I'm trying to find the mixing--
[00:43:15.920 --> 00:43:16.440]   Right.
[00:43:16.440 --> 00:43:17.880]   Where did that go?
[00:43:17.880 --> 00:43:18.360]   Hold on.
[00:43:18.360 --> 00:43:25.120]   Is it after-- the role of model scale, the role of pre-trained?
[00:43:25.120 --> 00:43:27.080]   That's related work.
[00:43:27.080 --> 00:43:28.680]   Conclusion.
[00:43:28.680 --> 00:43:30.800]   Is it in the experiments at the bottom?
[00:43:30.800 --> 00:43:31.320]   Maybe.
[00:43:31.320 --> 00:43:32.240]   It wasn't.
[00:43:32.240 --> 00:43:35.320]   Let me just quickly search for it here.
[00:43:35.320 --> 00:43:40.080]   What was it?
[00:43:40.080 --> 00:43:42.720]   It was called Shuffling.
[00:43:42.720 --> 00:43:44.600]   It was Shuffle, yes.
[00:43:44.600 --> 00:43:45.680]   Oh, here it is.
[00:43:45.680 --> 00:43:49.240]   OK, so that's something very interesting.
[00:43:49.240 --> 00:43:51.760]   But it's not surprising.
[00:43:51.760 --> 00:43:55.760]   So this is ablation studies, very interesting study.
[00:43:55.760 --> 00:44:00.080]   So we have-- so they did three experiments.
[00:44:00.080 --> 00:44:02.280]   We took original images.
[00:44:02.280 --> 00:44:05.920]   Actually, somebody on the chat already
[00:44:05.920 --> 00:44:12.680]   asked questions about what will happen
[00:44:12.680 --> 00:44:15.440]   if we change the order of pixels or the patches, right?
[00:44:15.440 --> 00:44:17.680]   So this is the exact experiment what they did.
[00:44:17.680 --> 00:44:20.440]   So we have three sets of experiment.
[00:44:20.440 --> 00:44:24.920]   We take original image and then feed it,
[00:44:24.920 --> 00:44:26.080]   train on original image.
[00:44:26.080 --> 00:44:29.720]   And then what we do is that we take the patch.
[00:44:29.720 --> 00:44:30.920]   We shuffle the patches.
[00:44:30.920 --> 00:44:33.880]   And then we shuffle pixels within the patches.
[00:44:33.880 --> 00:44:38.360]   And then the third experiment is that we just take image
[00:44:38.360 --> 00:44:40.480]   and just randomly shuffle everything, right?
[00:44:40.480 --> 00:44:45.680]   So on the left, you will see the result of the--
[00:44:45.680 --> 00:44:48.240]   for the mixers, because it's used NMVLC linear,
[00:44:48.240 --> 00:44:49.480]   so original.
[00:44:49.480 --> 00:44:55.440]   And when you shuffle the pixel, the shuffle the patches,
[00:44:55.440 --> 00:44:58.160]   and the pixel inside, it's actually-- it doesn't care.
[00:44:58.160 --> 00:45:01.240]   Because it's just linear layers, right?
[00:45:01.240 --> 00:45:03.480]   They don't care about the position,
[00:45:03.480 --> 00:45:05.600]   because everything is connected to everything.
[00:45:05.600 --> 00:45:08.080]   And you will find the same--
[00:45:08.080 --> 00:45:10.680]   you will find the same--
[00:45:10.680 --> 00:45:12.000]   you will find the thing.
[00:45:12.000 --> 00:45:15.880]   But when you-- and the performance is the same, right?
[00:45:15.880 --> 00:45:24.760]   But in the case of ResNet, if you take the classical ResNet--
[00:45:24.760 --> 00:45:26.160]   so in original, they are fine.
[00:45:26.160 --> 00:45:30.920]   But if you shuffle the patches, and then you
[00:45:30.920 --> 00:45:33.440]   shuffle the pixel within the patches-- so again,
[00:45:33.440 --> 00:45:34.640]   this is used ResNet.
[00:45:34.640 --> 00:45:37.560]   It doesn't care about patches, but just to be consistent.
[00:45:37.560 --> 00:45:39.640]   And you can see the performance immediately drops,
[00:45:39.640 --> 00:45:44.200]   because ResNet cares about the order of the pixels
[00:45:44.200 --> 00:45:45.600]   and where they are located.
[00:45:45.600 --> 00:45:49.680]   I think it's called inductive bias, if I am correct.
[00:45:49.680 --> 00:45:51.800]   And then, for example-- and then the bots
[00:45:51.800 --> 00:45:54.920]   shuffle from global--
[00:45:54.920 --> 00:45:56.840]   suffer from global shuffling.
[00:45:56.840 --> 00:46:01.120]   But it seems like MLP does better.
[00:46:01.120 --> 00:46:02.480]   I don't know the scale.
[00:46:02.480 --> 00:46:03.920]   Yes, MLP does better, right?
[00:46:03.920 --> 00:46:06.400]   Because it's more than 20, right, in some cases.
[00:46:06.400 --> 00:46:11.240]   And this-- in the case of ResNet, it's never--
[00:46:11.240 --> 00:46:13.960]   it never goes below--
[00:46:13.960 --> 00:46:15.800]   So does that-- then, is it--
[00:46:15.800 --> 00:46:18.440]   then, Dr. Habib, is it easy to say
[00:46:18.440 --> 00:46:23.000]   that it can handle corruptions better, the MLP mixer?
[00:46:23.000 --> 00:46:25.480]   Or is it like it can handle noise better?
[00:46:25.480 --> 00:46:27.720]   Or is that a--
[00:46:27.720 --> 00:46:29.600]   tough to make inference?
[00:46:29.600 --> 00:46:34.840]   I think the point where they want to say that this--
[00:46:34.840 --> 00:46:38.880]   that MLP probably doesn't care about order so much.
[00:46:38.880 --> 00:46:42.000]   However, this ResNet or vision transformers,
[00:46:42.000 --> 00:46:43.880]   they suffer from inductive bias.
[00:46:43.880 --> 00:46:44.960]   They care about order.
[00:46:44.960 --> 00:46:46.480]   So maybe it's much more robust.
[00:46:46.480 --> 00:46:49.640]   But I'm very afraid to make this kind of statement.
[00:46:49.640 --> 00:46:53.600]   So I will just say what I see from this experiment.
[00:46:53.600 --> 00:46:55.720]   Absolutely, keeping it safe.
[00:46:55.720 --> 00:46:56.240]   Yes.
[00:46:56.240 --> 00:46:58.520]   Let me have a look at the questions if there's any.
[00:46:58.520 --> 00:46:59.000]   Oh, sorry.
[00:46:59.000 --> 00:46:59.680]   Yeah, keep going.
[00:46:59.680 --> 00:47:01.000]   So I want to do one--
[00:47:01.000 --> 00:47:05.720]   what's a-- why-- OK, yes.
[00:47:05.720 --> 00:47:07.960]   So let's go to figure 5.
[00:47:07.960 --> 00:47:11.760]   Can we go to figure 5 on--
[00:47:11.760 --> 00:47:15.680]   no, on the supplementary, after an experiment.
[00:47:15.680 --> 00:47:21.600]   So I found something very interesting
[00:47:21.600 --> 00:47:24.200]   when I was reading through this.
[00:47:24.200 --> 00:47:25.000]   Yeah, so here.
[00:47:25.000 --> 00:47:26.160]   So this is fine, right?
[00:47:26.160 --> 00:47:27.040]   So yeah.
[00:47:27.040 --> 00:47:27.920]   Sorry, which one?
[00:47:27.920 --> 00:47:29.160]   I've missed it.
[00:47:29.160 --> 00:47:30.160]   Yeah.
[00:47:30.160 --> 00:47:31.880]   Was it this one?
[00:47:31.880 --> 00:47:33.640]   It was-- yes, this one.
[00:47:33.640 --> 00:47:34.160]   If you can--
[00:47:34.160 --> 00:47:35.080]   This one?
[00:47:35.080 --> 00:47:35.720]   Yes.
[00:47:35.720 --> 00:47:39.080]   So this is a very interesting experiment.
[00:47:39.080 --> 00:47:43.440]   So you remember we explained about the filters, right?
[00:47:43.440 --> 00:47:47.160]   So can you just zoom on block 0?
[00:47:47.160 --> 00:47:50.280]   Yeah, one second.
[00:47:50.280 --> 00:47:54.600]   I don't know whether I have to discuss this a lot or not,
[00:47:54.600 --> 00:47:55.720]   but I found it interesting.
[00:47:55.720 --> 00:47:58.720]   So you remember we talked about filters, right?
[00:47:58.720 --> 00:48:02.320]   There are filters which are responsible for recognizing
[00:48:02.320 --> 00:48:03.600]   some kind of features, right?
[00:48:03.600 --> 00:48:06.120]   So they did very interesting experiments.
[00:48:06.120 --> 00:48:10.240]   First, they're just looking at one block, one layer.
[00:48:10.240 --> 00:48:16.720]   And so this is the result if you just train on ImageNet,
[00:48:16.720 --> 00:48:18.400]   and then you plot the filter.
[00:48:18.400 --> 00:48:22.200]   So these filters are exactly the same location
[00:48:22.200 --> 00:48:23.600]   in each of these experiments.
[00:48:23.600 --> 00:48:27.200]   So ImageNet, and then if you train on ImageNet 20k,
[00:48:27.200 --> 00:48:29.760]   and then on the right side, it will be--
[00:48:29.760 --> 00:48:34.600]   if you move a little bit on the right, Aman?
[00:48:34.600 --> 00:48:36.520]   Yeah, I'm getting it now.
[00:48:36.560 --> 00:48:40.360]   Yeah, so there is this GFT 300 data set.
[00:48:40.360 --> 00:48:44.760]   And what's interesting that depending on the data set
[00:48:44.760 --> 00:48:47.480]   or on how much data you have, these filters
[00:48:47.480 --> 00:48:51.000]   look completely different.
[00:48:51.000 --> 00:48:53.480]   There is no correlation at all.
[00:48:53.480 --> 00:48:56.840]   But for convolution neural networks,
[00:48:56.840 --> 00:48:58.680]   you will see that early--
[00:48:58.680 --> 00:49:01.120]   these filters, at least in the earlier layers,
[00:49:01.120 --> 00:49:05.720]   they look kind of similar between different architectures.
[00:49:05.720 --> 00:49:07.960]   So for example, I personally don't know whether--
[00:49:07.960 --> 00:49:13.080]   yeah, it's hard.
[00:49:13.080 --> 00:49:14.800]   I don't know what to think about this.
[00:49:14.800 --> 00:49:18.880]   Maybe there is-- it seems like this filter depends
[00:49:18.880 --> 00:49:21.200]   on the amount of data sets you train,
[00:49:21.200 --> 00:49:24.000]   because they are completely different.
[00:49:24.000 --> 00:49:28.680]   So this is-- I don't know if somebody has some ideas
[00:49:28.680 --> 00:49:30.280]   or explanation for this.
[00:49:30.280 --> 00:49:32.160]   I found this kind of interesting.
[00:49:32.160 --> 00:49:34.240]   No, I think that really is a good point,
[00:49:34.240 --> 00:49:36.240]   that if--
[00:49:36.240 --> 00:49:40.680]   with CNNs, at least then, the earlier layers,
[00:49:40.680 --> 00:49:42.160]   they're finding top edges, or they're
[00:49:42.160 --> 00:49:44.560]   finding top left corners, or lines, or diagonals,
[00:49:44.560 --> 00:49:45.160]   or whatever.
[00:49:45.160 --> 00:49:47.960]   They're just trying to find abstract edges and stuff.
[00:49:47.960 --> 00:49:51.480]   But in terms of the MLP mixer, then it's changing.
[00:49:51.480 --> 00:49:54.200]   There's no such thing that the earlier layers are always
[00:49:54.200 --> 00:49:57.920]   finding the same pattern, depending on the data set size.
[00:49:57.920 --> 00:50:00.040]   That really is an interesting, interesting point
[00:50:00.040 --> 00:50:01.320]   that you make there, Dr. Habib.
[00:50:01.320 --> 00:50:04.440]   And yeah, it's hard to make of it.
[00:50:04.440 --> 00:50:07.520]   I mean, one thing then, does it mean
[00:50:07.520 --> 00:50:08.960]   if we apply it on medical imaging,
[00:50:08.960 --> 00:50:10.040]   then it will adapt better?
[00:50:10.040 --> 00:50:11.200]   I don't know.
[00:50:11.200 --> 00:50:12.240]   These are questions.
[00:50:12.240 --> 00:50:14.560]   These are open research questions
[00:50:14.560 --> 00:50:19.640]   that should be tried and tested before making any claims.
[00:50:19.640 --> 00:50:21.360]   So conscious of time as well, I'll
[00:50:21.360 --> 00:50:26.520]   go to all the questions that have been asked so far.
[00:50:26.520 --> 00:50:27.200]   One second.
[00:50:27.200 --> 00:50:27.720]   Sorry, yes.
[00:50:27.720 --> 00:50:28.800]   Let's go to the question.
[00:50:29.240 --> 00:50:31.000]   So Aman, maybe you just want to add one word
[00:50:31.000 --> 00:50:34.680]   about this controversy between Conf2D and OneCog--
[00:50:34.680 --> 00:50:35.800]   Yeah, I will add it.
[00:50:35.800 --> 00:50:36.320]   OK.
[00:50:36.320 --> 00:50:39.120]   I've actually got the whole blog post.
[00:50:39.120 --> 00:50:39.640]   OK, good.
[00:50:39.640 --> 00:50:41.440]   I released two more this morning.
[00:50:41.440 --> 00:50:46.040]   So I'll share that very quickly in terms of the controversy.
[00:50:46.040 --> 00:50:48.200]   But I just want to also get the questions sorted.
[00:50:48.200 --> 00:50:48.720]   Because--
[00:50:48.720 --> 00:50:49.240]   Right.
[00:50:49.240 --> 00:50:50.040]   Perfect.
[00:50:50.040 --> 00:50:50.920]   Let's go.
[00:50:50.920 --> 00:50:54.040]   OK, have you tried using this on any present past forms?
[00:50:54.040 --> 00:50:56.200]   There have been some kernels that have.
[00:50:56.200 --> 00:51:01.000]   I know of them, but I don't know about Dr. Habib if he has.
[00:51:01.000 --> 00:51:02.880]   No, I haven't tried.
[00:51:02.880 --> 00:51:07.200]   I'm not brave enough yet to tell them.
[00:51:07.200 --> 00:51:07.960]   Hi, Dr. Habib.
[00:51:07.960 --> 00:51:11.520]   Aman, why should one chain 768 vector if I want to?
[00:51:11.520 --> 00:51:12.440]   That's a good question.
[00:51:12.440 --> 00:51:15.600]   But it's basically, again, going back to the table in the paper.
[00:51:15.600 --> 00:51:19.200]   So if we go back to the paper and we go back to the--
[00:51:19.200 --> 00:51:21.320]   basically, it's just different architectures.
[00:51:21.320 --> 00:51:22.760]   If you want a bigger architecture,
[00:51:22.760 --> 00:51:23.520]   you use a bigger--
[00:51:23.520 --> 00:51:24.360]   It's a hypertexture.
[00:51:24.360 --> 00:51:26.560]   --you want a bigger architecture, you use a bigger--
[00:51:26.560 --> 00:51:28.200]   It's a hyperparameter.
[00:51:28.200 --> 00:51:31.080]   Think about this as a hyperparameter.
[00:51:31.080 --> 00:51:32.480]   And I don't know whether they say--
[00:51:32.480 --> 00:51:34.120]   oh, yeah, they have, right?
[00:51:34.120 --> 00:51:34.680]   We can see--
[00:51:34.680 --> 00:51:36.040]   They've got different hidden size.
[00:51:36.040 --> 00:51:37.480]   They've got different hidden size.
[00:51:37.480 --> 00:51:41.440]   So they've got 512, they've got 768, and they've got 1,024.
[00:51:41.440 --> 00:51:44.560]   So in the bigger architectures, and even 1,280.
[00:51:44.560 --> 00:51:47.400]   So it's just you want to change the hyperparameter depending
[00:51:47.400 --> 00:51:48.800]   on the architecture.
[00:51:48.800 --> 00:51:51.920]   So the bigger the architecture, the bigger the hidden size.
[00:51:51.920 --> 00:51:53.720]   Yeah.
[00:51:53.720 --> 00:51:56.120]   OK, let's go.
[00:51:56.120 --> 00:51:59.040]   The 16 cross 16 patch is flattened,
[00:51:59.040 --> 00:52:01.160]   and then it is projected using linear layer
[00:52:01.160 --> 00:52:02.640]   to any dimension of our choice.
[00:52:02.640 --> 00:52:04.000]   Yes, that is correct.
[00:52:04.000 --> 00:52:05.160]   That is absolutely correct.
[00:52:05.160 --> 00:52:06.640]   You've got this.
[00:52:06.640 --> 00:52:08.200]   And then the dimension of choices
[00:52:08.200 --> 00:52:09.960]   has been mentioned, as in this paper
[00:52:09.960 --> 00:52:11.440]   that we just looked into.
[00:52:11.440 --> 00:52:13.080]   But you've got this.
[00:52:13.080 --> 00:52:17.840]   Why do we transpose 196 by 512 to 512 by 196?
[00:52:17.840 --> 00:52:21.200]   So 196 by 5-- so the linear layers actually
[00:52:21.200 --> 00:52:23.520]   only work along the rows.
[00:52:23.520 --> 00:52:25.360]   And you want to mix the channels,
[00:52:25.360 --> 00:52:27.640]   and you want to mix the spatial information.
[00:52:27.640 --> 00:52:30.080]   So I guess when this is on YouTube,
[00:52:30.080 --> 00:52:33.120]   have a go back at when this was being explained
[00:52:33.120 --> 00:52:34.920]   in detail with everything.
[00:52:34.920 --> 00:52:40.520]   But the short answer is because you're always mixing the rows,
[00:52:40.520 --> 00:52:42.880]   in the first one, you're mixing the channels
[00:52:42.880 --> 00:52:45.160]   because every column is a channel.
[00:52:45.160 --> 00:52:48.320]   And in the second one, you're mixing the tokens.
[00:52:48.320 --> 00:52:49.200]   Have I got this right?
[00:52:49.200 --> 00:52:50.320]   Yes, I've got this right.
[00:52:50.320 --> 00:52:52.120]   In the second one, you're mixing the tokens
[00:52:52.120 --> 00:52:55.560]   because every column is a token.
[00:52:55.560 --> 00:52:57.360]   So that's the only reason why you
[00:52:57.360 --> 00:53:00.040]   want to do a transpose in the initial one.
[00:53:00.040 --> 00:53:03.200]   Because in the token mixing, you want every column to be a token.
[00:53:03.200 --> 00:53:05.280]   Is there anything you want to add here, Dr. Habib?
[00:53:05.280 --> 00:53:08.360]   No, I think this is very nicely explained.
[00:53:08.360 --> 00:53:09.960]   What does token mixing achieve?
[00:53:09.960 --> 00:53:11.680]   OK.
[00:53:11.680 --> 00:53:12.800]   Is this you want to answer?
[00:53:12.800 --> 00:53:15.920]   Because I've got a really good visualization or easier
[00:53:15.920 --> 00:53:17.000]   way to explain what it is.
[00:53:17.000 --> 00:53:17.760]   Yes, please.
[00:53:17.760 --> 00:53:19.040]   You do.
[00:53:19.040 --> 00:53:20.920]   OK.
[00:53:20.920 --> 00:53:24.720]   I'm going to use your image.
[00:53:24.720 --> 00:53:26.360]   Whatever you want.
[00:53:26.360 --> 00:53:27.480]   If I can find it.
[00:53:27.480 --> 00:53:28.240]   OK, here it is.
[00:53:28.240 --> 00:53:29.120]   Yeah.
[00:53:29.120 --> 00:53:31.800]   So basically, look at this.
[00:53:31.800 --> 00:53:34.320]   We started with the image of a frog.
[00:53:34.320 --> 00:53:38.080]   Every patch-- we don't need the highlighter.
[00:53:38.080 --> 00:53:40.120]   Every patch is like this.
[00:53:40.120 --> 00:53:41.840]   And the last batch is here.
[00:53:41.840 --> 00:53:47.080]   And then every patch is represented as a vector.
[00:53:47.080 --> 00:53:49.160]   You've got every patch as a vector.
[00:53:49.160 --> 00:53:52.120]   When you're doing the token mixing, basically,
[00:53:52.120 --> 00:53:56.720]   and what you're doing is you're mixing the first channel
[00:53:56.720 --> 00:53:59.760]   or just one channel of this token
[00:53:59.760 --> 00:54:01.360]   with all the other tokens.
[00:54:01.360 --> 00:54:02.640]   So that's the token mixing.
[00:54:02.640 --> 00:54:04.640]   So basically, what you're doing is
[00:54:04.640 --> 00:54:07.200]   you're mixing the spatial information.
[00:54:07.200 --> 00:54:08.200]   Because it's important, right?
[00:54:08.200 --> 00:54:09.680]   We need to know where the eyes are.
[00:54:09.680 --> 00:54:11.640]   The eyes are here in the image.
[00:54:11.640 --> 00:54:13.320]   But it does it channel by channel.
[00:54:13.320 --> 00:54:17.400]   And the number of channels could be 512, could be 768.
[00:54:17.400 --> 00:54:19.120]   But that's what the token mixing does.
[00:54:19.120 --> 00:54:22.920]   What the token mixing is doing for every patch,
[00:54:22.920 --> 00:54:26.760]   channel by channel, for 512 or 768 channels,
[00:54:26.760 --> 00:54:30.280]   it's mixing the information in each of the patches.
[00:54:30.280 --> 00:54:32.360]   So you know, OK, eyes are here.
[00:54:32.360 --> 00:54:34.680]   Or other information is here.
[00:54:34.680 --> 00:54:39.800]   So you can mix, actually, the spatial information, as said.
[00:54:39.800 --> 00:54:42.240]   And then the next one in the channel mixing,
[00:54:42.240 --> 00:54:44.600]   you pretty much take just the one patch
[00:54:44.600 --> 00:54:50.280]   and you mix all the 512 or 768 channels in that one patch.
[00:54:50.280 --> 00:54:53.720]   So that's the intuition of token mixing and channel mixing.
[00:54:53.720 --> 00:54:55.760]   But is there anything you want to add, Dr. Habib?
[00:54:55.760 --> 00:54:57.640]   No, I think we should have explained
[00:54:57.640 --> 00:54:59.200]   in this using this broad picture.
[00:54:59.200 --> 00:55:02.360]   This was very nicely done.
[00:55:02.360 --> 00:55:03.560]   OK, it just struck--
[00:55:03.560 --> 00:55:06.600]   it just occurred to me to do it that way.
[00:55:06.600 --> 00:55:10.000]   Is there a comparison of Flops for mixed services,
[00:55:10.000 --> 00:55:11.280]   other architectures?
[00:55:11.280 --> 00:55:14.320]   One thing I do want to say is Flops is not a good way
[00:55:14.320 --> 00:55:17.080]   to compare because Flops doesn't mean that the images--
[00:55:17.080 --> 00:55:19.760]   like the architecture is going to be faster.
[00:55:19.760 --> 00:55:21.680]   There's different research papers
[00:55:21.680 --> 00:55:23.680]   or different discussions on Twitter
[00:55:23.680 --> 00:55:25.600]   that I was reading recently.
[00:55:25.600 --> 00:55:29.360]   But in terms of the throughput, when
[00:55:29.360 --> 00:55:31.920]   we were looking at the MLP mix architecture,
[00:55:31.920 --> 00:55:34.000]   then there was a table that was showing throughput.
[00:55:34.000 --> 00:55:36.120]   So I think that's the comparison.
[00:55:36.120 --> 00:55:38.640]   It's not this table, but it's this table.
[00:55:38.640 --> 00:55:42.160]   So see how it's comparing the throughput.
[00:55:42.160 --> 00:55:45.000]   Right.
[00:55:45.000 --> 00:55:47.520]   I am still unclear on the token and channel mixer.
[00:55:47.520 --> 00:55:49.120]   Can we review that one more time?
[00:55:49.120 --> 00:55:51.240]   I hope-- I just explained it.
[00:55:51.240 --> 00:55:52.240]   Does that help, Ramesh?
[00:55:52.240 --> 00:55:54.280]   Could you maybe reply to this comment here
[00:55:54.280 --> 00:55:58.440]   and let me know if what I just said in terms of an intuition--
[00:55:58.440 --> 00:55:59.160]   does that help?
[00:55:59.160 --> 00:56:00.160]   Yeah, that sounds good.
[00:56:00.160 --> 00:56:01.840]   So one question I had follow up.
[00:56:01.840 --> 00:56:06.240]   So do you do this token mixer, channel mixer parallelly
[00:56:06.240 --> 00:56:08.240]   and then do some sort of concatenation?
[00:56:08.240 --> 00:56:09.240]   Is it like sequentially?
[00:56:09.240 --> 00:56:11.640]   You do token mixer first and then--
[00:56:11.640 --> 00:56:12.440]   No, it's sequential.
[00:56:12.440 --> 00:56:14.800]   --token mixer output and then the channel mixer.
[00:56:14.800 --> 00:56:15.840]   Yeah, it's sequential.
[00:56:15.840 --> 00:56:18.120]   You do the token mixer first and then the channel mixer.
[00:56:18.120 --> 00:56:23.360]   But one experiment you could try is do the channel mixer first
[00:56:23.360 --> 00:56:24.400]   and then the token mixer.
[00:56:24.400 --> 00:56:26.560]   But I don't think it will change anything
[00:56:26.560 --> 00:56:31.320]   because the idea is to mix both the dimensions.
[00:56:31.320 --> 00:56:33.080]   Like a 2D kernel, right?
[00:56:33.080 --> 00:56:37.360]   OK, I guess there's a deeper discussion
[00:56:37.360 --> 00:56:39.920]   to be had on what exactly goes on in the convolution.
[00:56:39.920 --> 00:56:41.560]   But wait for it.
[00:56:41.560 --> 00:56:43.200]   I think we might go--
[00:56:43.200 --> 00:56:44.600]   Dr. Habib, conscious of your time,
[00:56:44.600 --> 00:56:46.040]   it's almost midnight there.
[00:56:46.040 --> 00:56:47.960]   Are you still OK to keep going for another--
[00:56:47.960 --> 00:56:48.800]   Yeah, yeah, yeah.
[00:56:48.800 --> 00:56:50.080]   Yeah, yeah, yeah, sure.
[00:56:50.080 --> 00:56:52.320]   We can just go through all the questions
[00:56:52.320 --> 00:56:54.200]   and whatever is remaining, we can go.
[00:56:54.200 --> 00:56:54.760]   I'm fine.
[00:56:54.760 --> 00:56:58.560]   Yeah, I just still want to touch upon what exactly
[00:56:58.560 --> 00:57:01.240]   a convolution does and why do we need this token mixing
[00:57:01.240 --> 00:57:04.720]   and channel mixing and that whole controversy which
[00:57:04.720 --> 00:57:06.120]   we haven't touched on yet.
[00:57:06.120 --> 00:57:08.920]   But I'll go into that.
[00:57:08.920 --> 00:57:12.080]   OK, has there been further work on using MLP mixer
[00:57:12.080 --> 00:57:16.240]   as backbone for object detection or segmentation tasks?
[00:57:16.240 --> 00:57:19.320]   I'm not aware of it if it has been.
[00:57:19.320 --> 00:57:23.040]   I think the MLP mixer, does it say fine tuning on other--
[00:57:23.040 --> 00:57:25.560]   does the paper fine tune on--
[00:57:25.560 --> 00:57:27.720]   I think the paper in itself fine tunes, right?
[00:57:27.720 --> 00:57:28.960]   Or-- I'm not sure.
[00:57:28.960 --> 00:57:30.680]   The answer is we're not sure.
[00:57:30.680 --> 00:57:31.720]   The paper is very recent.
[00:57:31.720 --> 00:57:33.480]   It's only a month or two ago.
[00:57:33.480 --> 00:57:36.280]   Yes.
[00:57:36.280 --> 00:57:38.360]   And there's been other works, though.
[00:57:38.360 --> 00:57:42.200]   There's been more mix architectures like GMLP, ResMLP.
[00:57:42.200 --> 00:57:44.200]   So you could have a look at those as well.
[00:57:44.200 --> 00:57:46.680]   There's been definitely follow up work.
[00:57:46.680 --> 00:57:48.440]   If we change the order--
[00:57:48.440 --> 00:57:49.560]   exactly, you read my mind.
[00:57:49.560 --> 00:57:52.760]   If we change the order of token mixing and channel mixing,
[00:57:52.760 --> 00:57:55.400]   I expect it to be.
[00:57:55.400 --> 00:57:58.640]   I won't put the money--
[00:57:58.640 --> 00:57:59.800]   I won't put $1,000 in it.
[00:57:59.800 --> 00:58:01.360]   But of course, it should be.
[00:58:01.360 --> 00:58:02.720]   That's my expectation.
[00:58:02.720 --> 00:58:03.720]   What do you think, Dr. B?
[00:58:03.720 --> 00:58:06.360]   I think it will be similar.
[00:58:06.360 --> 00:58:06.880]   Yeah.
[00:58:06.880 --> 00:58:08.280]   But you never know.
[00:58:08.280 --> 00:58:09.040]   You never know.
[00:58:09.040 --> 00:58:10.840]   Yeah, you never know with deep learning.
[00:58:10.840 --> 00:58:13.360]   One thing you will see is deep learning practitioners don't
[00:58:13.360 --> 00:58:18.040]   know what's happening until it happens.
[00:58:18.040 --> 00:58:20.080]   Any ideas on how to do transfer learning
[00:58:20.080 --> 00:58:23.080]   to adapt to other data sets?
[00:58:23.080 --> 00:58:24.320]   Yeah, you could fine tune.
[00:58:24.320 --> 00:58:26.520]   This is still the very same architecture.
[00:58:26.520 --> 00:58:28.240]   We could still go and--
[00:58:28.240 --> 00:58:29.760]   You just have to--
[00:58:29.760 --> 00:58:31.960]   right, so you'd take the last MLP layer, which
[00:58:31.960 --> 00:58:34.280]   has for ImageNet is 1,000 classes.
[00:58:34.280 --> 00:58:37.640]   And you just freeze.
[00:58:37.640 --> 00:58:39.400]   I don't want to say freeze.
[00:58:39.400 --> 00:58:42.000]   Yes, I think you just freeze depending--
[00:58:42.000 --> 00:58:43.360]   I don't know how it--
[00:58:43.360 --> 00:58:45.400]   I haven't done experiments.
[00:58:45.400 --> 00:58:47.880]   But usually on CNN, you can freeze the backbone
[00:58:47.880 --> 00:58:50.800]   and then just fine tune the--
[00:58:50.800 --> 00:58:53.520]   add new NN linear depending if you have five classes
[00:58:53.520 --> 00:58:55.880]   and then fine tune the last layer.
[00:58:55.880 --> 00:58:58.240]   And hopefully, it will work.
[00:58:58.240 --> 00:59:00.520]   But we can see on the paper how they fine tune.
[00:59:00.520 --> 00:59:03.240]   Because they trained on GFC and then how they fine tune.
[00:59:03.240 --> 00:59:07.480]   And you can just repeat the same thing with your data set.
[00:59:07.480 --> 00:59:08.000]   OK.
[00:59:08.000 --> 00:59:09.120]   We have one more question.
[00:59:09.120 --> 00:59:12.280]   I'm curious to know if you agree with me on the following.
[00:59:12.280 --> 00:59:15.640]   One benefit of convolution layers over FC layers
[00:59:15.640 --> 00:59:18.360]   is their translation invariance property.
[00:59:18.360 --> 00:59:22.200]   Can we say that translation invariance in MLP mixes
[00:59:22.200 --> 00:59:25.560]   is achieved since the input is broken into small patches?
[00:59:25.560 --> 00:59:27.520]   I did read about the translation in--
[00:59:27.520 --> 00:59:28.680]   OK, you go, Dr. Aviv.
[00:59:28.680 --> 00:59:29.440]   I know you--
[00:59:29.440 --> 00:59:31.880]   So they prove--
[00:59:31.880 --> 00:59:35.920]   so if my understanding is correct,
[00:59:35.920 --> 00:59:41.120]   they prove that you can shuffle--
[00:59:41.120 --> 00:59:44.120]   so translation invariance, my understanding
[00:59:44.120 --> 00:59:46.680]   is following that you don't remember the positions.
[00:59:46.680 --> 00:59:49.360]   So it means that your prediction are
[00:59:49.360 --> 00:59:53.200]   independent of the positions of where things are located.
[00:59:53.200 --> 00:59:55.880]   And the nice experiment will be that you just mix.
[00:59:55.880 --> 01:00:00.000]   And if result changes, it means it's invariant or--
[01:00:00.000 --> 01:00:02.400]   so it's invariant or not invariant.
[01:00:02.400 --> 01:00:06.200]   So because you're right.
[01:00:06.200 --> 01:00:08.680]   You said that translation invariance
[01:00:08.680 --> 01:00:11.440]   achieved because you break down in the patches
[01:00:11.440 --> 01:00:12.920]   and then you flatten.
[01:00:12.920 --> 01:00:15.640]   And therefore, you connect to fully connected.
[01:00:15.640 --> 01:00:19.240]   But if you think about this, you can take whole image
[01:00:19.240 --> 01:00:22.720]   and you can make it this one vector.
[01:00:22.720 --> 01:00:25.960]   And then you just can have also fully connected layer.
[01:00:25.960 --> 01:00:27.960]   And then you probably also will achieve
[01:00:27.960 --> 01:00:29.200]   translational invariance.
[01:00:29.200 --> 01:00:32.760]   So breaking down in patches and then what happened afterwards,
[01:00:32.760 --> 01:00:34.520]   like that you apply an n dot linear,
[01:00:34.520 --> 01:00:38.160]   I think this all results in achieving this translation
[01:00:38.160 --> 01:00:39.120]   invariance.
[01:00:39.120 --> 01:00:42.080]   But again, don't quote me on this.
[01:00:42.080 --> 01:00:46.320]   Please.
[01:00:46.320 --> 01:00:47.280]   OK.
[01:00:47.280 --> 01:00:48.120]   Great session.
[01:00:48.120 --> 01:00:49.080]   Appreciate the effort.
[01:00:49.080 --> 01:00:49.800]   Thank you.
[01:00:49.800 --> 01:00:51.400]   Is there any intuition behind why
[01:00:51.400 --> 01:00:53.720]   MLP mixes with less number of parameters
[01:00:53.720 --> 01:00:55.760]   would require significantly more training data
[01:00:55.760 --> 01:00:58.960]   to achieve on par performance with vision transformers?
[01:00:58.960 --> 01:00:59.920]   I do have one.
[01:00:59.920 --> 01:01:00.720]   I do have a theory.
[01:01:00.720 --> 01:01:02.520]   But Dr. Habib, do you have something to add?
[01:01:02.520 --> 01:01:05.120]   I will go with your answer.
[01:01:05.120 --> 01:01:05.600]   OK.
[01:01:05.600 --> 01:01:11.760]   I just saw Gian-Lecun tweet that when the architectures are less
[01:01:11.760 --> 01:01:13.280]   rigid, then--
[01:01:13.280 --> 01:01:15.880]   sorry, when you have lots of data,
[01:01:15.880 --> 01:01:17.320]   then you could pretty much get away
[01:01:17.320 --> 01:01:19.240]   with less rigid architectures.
[01:01:19.240 --> 01:01:22.840]   So that just means that because we're pre-training on 300
[01:01:22.840 --> 01:01:26.760]   million, we're pre-training on a large, large amount
[01:01:26.760 --> 01:01:27.960]   of data sets.
[01:01:27.960 --> 01:01:29.560]   Gian-Lecun's tweet was basically
[01:01:29.560 --> 01:01:34.160]   saying when you have that much amount of data,
[01:01:34.160 --> 01:01:38.520]   then you could pretty much get away with less--
[01:01:38.520 --> 01:01:39.840]   with an easy architecture.
[01:01:39.840 --> 01:01:40.840]   That was the main point.
[01:01:40.840 --> 01:01:42.800]   And I think that's the intuition here.
[01:01:42.800 --> 01:01:44.640]   That's why you need that much data.
[01:01:44.640 --> 01:01:45.480]   That's my intuition.
[01:01:45.480 --> 01:01:46.360]   That's what I've read.
[01:01:46.360 --> 01:01:47.920]   But there's no--
[01:01:47.920 --> 01:01:48.840]   I don't have proof.
[01:01:48.840 --> 01:01:54.440]   The mixer seems very much like self-attention layer
[01:01:54.440 --> 01:01:57.280]   in transformer.
[01:01:57.280 --> 01:02:00.440]   Yeah, that's a conversation I don't want to go into.
[01:02:00.440 --> 01:02:02.560]   Everything then could be broken.
[01:02:02.560 --> 01:02:06.000]   One thing is everything is matrix multiplication.
[01:02:06.000 --> 01:02:08.320]   And then everything could be related to any other thing
[01:02:08.320 --> 01:02:10.600]   in a similar way.
[01:02:10.600 --> 01:02:12.840]   So I think we should be careful when
[01:02:12.840 --> 01:02:16.040]   we say this looks like self-attention layer
[01:02:16.040 --> 01:02:17.680]   in transformer.
[01:02:17.680 --> 01:02:21.520]   You have a query key and value in transformer.
[01:02:21.520 --> 01:02:24.600]   And then to me, this is very different.
[01:02:24.600 --> 01:02:27.560]   But again, we should be very careful when
[01:02:27.560 --> 01:02:29.000]   making these comparisons.
[01:02:29.000 --> 01:02:33.480]   Yeah, yes, I agree with what you said.
[01:02:33.480 --> 01:02:35.600]   There was specifically a point made
[01:02:35.600 --> 01:02:39.160]   that this MLP mixer doesn't require attention.
[01:02:39.160 --> 01:02:41.680]   And attention layer is slightly different.
[01:02:41.680 --> 01:02:45.080]   It has more complexity when you increase the data set.
[01:02:45.080 --> 01:02:49.400]   But yeah, so it's not--
[01:02:49.400 --> 01:02:50.840]   yeah, it's not--
[01:02:50.840 --> 01:02:52.360]   I cannot-- yeah, it's not similar,
[01:02:52.360 --> 01:02:54.280]   at least from what I know.
[01:02:54.280 --> 01:02:56.520]   And the whole point of this paper
[01:02:56.520 --> 01:02:58.760]   was that, hey, you don't need attention.
[01:02:58.760 --> 01:03:01.520]   You just need--
[01:03:01.520 --> 01:03:02.320]   You just need the--
[01:03:02.320 --> 01:03:03.360]   MLP.
[01:03:03.360 --> 01:03:07.880]   The main idea is you mix the information in two dimensions.
[01:03:07.880 --> 01:03:11.120]   I guess that's my main idea that I got from it.
[01:03:11.120 --> 01:03:13.080]   Yeah.
[01:03:13.080 --> 01:03:16.560]   OK, so now I guess that's it for the questions.
[01:03:16.560 --> 01:03:19.520]   But we are still left with the one last thing
[01:03:19.520 --> 01:03:23.120]   is where this controversy is coming from on when the MLP
[01:03:23.120 --> 01:03:29.040]   mixer is that a CNN or is that a convolution, basically a CNN?
[01:03:29.040 --> 01:03:31.800]   And is it a fully connected?
[01:03:31.800 --> 01:03:34.040]   So something I wrote this morning
[01:03:34.040 --> 01:03:37.960]   and announced on my Twitter is essentially--
[01:03:37.960 --> 01:03:41.840]   and it did take me some time, but essentially,
[01:03:41.840 --> 01:03:44.880]   then, are fully connected and convolution layers equivalent?
[01:03:44.880 --> 01:03:46.560]   So when I was reading this paper,
[01:03:46.560 --> 01:03:48.200]   I actually asked on Twitter, is there
[01:03:48.200 --> 01:03:49.720]   a nice visualization that explains
[01:03:49.760 --> 01:03:53.800]   why Conv-Wendy and NN.LinearLayers are the same?
[01:03:53.800 --> 01:03:57.640]   The answer is they are the same, and I have the answer now.
[01:03:57.640 --> 01:03:59.000]   But the main point--
[01:03:59.000 --> 01:04:00.400]   I'll share this blog post as well.
[01:04:00.400 --> 01:04:00.920]   One second.
[01:04:00.920 --> 01:04:06.280]   I'll just post this in the chat if I can find where the chat is.
[01:04:06.280 --> 01:04:13.000]   OK, so I've just posted in the chat.
[01:04:13.000 --> 01:04:18.080]   But the key idea, the answer is, yes, Conv-Wendy operation--
[01:04:18.080 --> 01:04:20.960]   and as part of this blog post, I ended up
[01:04:20.960 --> 01:04:24.400]   implementing Conv-Wendy operation in Microsoft Excel.
[01:04:24.400 --> 01:04:27.360]   So this is how Conv-Wendy basically
[01:04:27.360 --> 01:04:29.200]   looks in Microsoft Excel.
[01:04:29.200 --> 01:04:31.880]   So if you have an input 3 by 3 matrix--
[01:04:31.880 --> 01:04:34.480]   so let's just say you have 3 by 3 matrix as input,
[01:04:34.480 --> 01:04:39.280]   and you basically have, then, you say--
[01:04:39.280 --> 01:04:41.360]   you create a convolution 1D kernel
[01:04:41.360 --> 01:04:44.280]   saying you have three input channels and five output
[01:04:44.280 --> 01:04:45.040]   channels.
[01:04:45.040 --> 01:04:48.120]   What that means is you have five filters,
[01:04:48.120 --> 01:04:51.120]   and each filter is of length 3 by 1
[01:04:51.120 --> 01:04:54.400]   because your kernel size is 1 and your input channels is 3.
[01:04:54.400 --> 01:04:57.520]   So essentially, you do a column by column multiplication.
[01:04:57.520 --> 01:05:00.200]   Like, each filter-- so for this first column,
[01:05:00.200 --> 01:05:02.200]   this filter gets multiplied, then this, then this,
[01:05:02.200 --> 01:05:03.400]   then this, then the last one.
[01:05:03.400 --> 01:05:04.640]   So you get five outputs.
[01:05:04.640 --> 01:05:07.640]   Then the same thing happens for the second column and so on.
[01:05:07.640 --> 01:05:09.720]   So that's the convolution operation in Excel.
[01:05:09.720 --> 01:05:12.240]   And I compared the results with--
[01:05:12.240 --> 01:05:15.760]   basically, I implemented this in PyTorch.
[01:05:15.760 --> 01:05:18.560]   Just to compare results, I created convolution 1D,
[01:05:18.560 --> 01:05:20.240]   and then the results matched.
[01:05:20.240 --> 01:05:24.040]   So that's convolution, essentially.
[01:05:24.040 --> 01:05:26.680]   In Excel, then, you could also implement
[01:05:26.680 --> 01:05:30.200]   fully connected layers, which is the matrix multiplication.
[01:05:30.200 --> 01:05:32.080]   So if you have your input, but this time you
[01:05:32.080 --> 01:05:33.320]   take the transpose--
[01:05:33.320 --> 01:05:34.760]   you don't keep the same input.
[01:05:34.760 --> 01:05:36.720]   You rather just take the transpose, which is--
[01:05:36.720 --> 01:05:40.080]   I'm just going to convert it, like, the height to rows.
[01:05:40.080 --> 01:05:42.560]   If you take the transpose, and then you still
[01:05:42.560 --> 01:05:43.960]   have the same weight matrix.
[01:05:43.960 --> 01:05:46.360]   This time, it's not five different kernels.
[01:05:46.360 --> 01:05:48.960]   It's a 3 by 5 weight matrix.
[01:05:48.960 --> 01:05:52.040]   So you do a row by column multiplication.
[01:05:52.040 --> 01:05:53.840]   And in a fully connected layer, then,
[01:05:53.840 --> 01:05:56.920]   you still get the same result. So if I--
[01:05:56.920 --> 01:06:01.280]   like, the answer is exactly the same as the conv 1D operation.
[01:06:01.280 --> 01:06:03.920]   So I guess the main point of, then, this blog post
[01:06:03.920 --> 01:06:08.000]   is to say that conv 1D, or basically,
[01:06:08.000 --> 01:06:10.720]   the convolution operation with 1 cross 1 kernel,
[01:06:10.720 --> 01:06:13.040]   and the linear layer are the same.
[01:06:13.040 --> 01:06:14.280]   Just believe me on that.
[01:06:14.280 --> 01:06:16.800]   And if you don't believe me, have a read of this blog post,
[01:06:16.800 --> 01:06:18.560]   then you'll believe me.
[01:06:18.560 --> 01:06:20.920]   But what I found is that the convolution
[01:06:20.920 --> 01:06:24.600]   with 1 by 1 kernel and fully connected layer are the same.
[01:06:24.600 --> 01:06:27.560]   So let's just stop it at that.
[01:06:27.560 --> 01:06:29.120]   And let's keep that in mind.
[01:06:29.120 --> 01:06:32.560]   But then, if we go into the MLP mixer,
[01:06:32.560 --> 01:06:36.240]   you see how the mixer has linear layers.
[01:06:36.240 --> 01:06:40.320]   So it has nn.linear, nn.linear, nn.linear here and here.
[01:06:40.320 --> 01:06:44.280]   What if you replace, then, those linear layers with conv 1D,
[01:06:44.280 --> 01:06:47.480]   or just a 1 by 1 kernel of a convolution,
[01:06:47.480 --> 01:06:49.600]   basically a convolution 1 by 1 kernel?
[01:06:49.600 --> 01:06:51.520]   So you could replace all of these
[01:06:51.520 --> 01:06:54.760]   with just a convolution 1 by 1 kernel.
[01:06:54.760 --> 01:06:58.560]   And then, you could say that the mixer is basically
[01:06:58.560 --> 01:07:02.840]   a CNN, where the mixer layer consists of convolutions
[01:07:02.840 --> 01:07:03.960]   with 1 by 1 kernel.
[01:07:03.960 --> 01:07:08.320]   And that's exactly what Jan-Lekun said.
[01:07:08.320 --> 01:07:12.360]   So in his tweet, he basically said,
[01:07:12.360 --> 01:07:15.240]   oh, MLP mixer is just a conv layer.
[01:07:15.240 --> 01:07:18.400]   I'm opening the tweet as well.
[01:07:18.400 --> 01:07:21.000]   The MLP mixer is basically just a conv layer
[01:07:21.000 --> 01:07:22.320]   with 1 by 1 kernel.
[01:07:22.320 --> 01:07:23.600]   So that's exactly what he said.
[01:07:23.600 --> 01:07:29.240]   And my understanding of this is because the fully connected
[01:07:29.240 --> 01:07:32.120]   and the 1 by 1 kernel convolution layer are equivalent.
[01:07:32.120 --> 01:07:34.160]   So you could replace the fully connected.
[01:07:34.160 --> 01:07:36.440]   And as part of this blog post, we
[01:07:36.440 --> 01:07:40.840]   look at the implementation of MLP mixer in PyTorch.
[01:07:40.840 --> 01:07:45.440]   And we also pretty much implement the MLP architecture
[01:07:45.440 --> 01:07:47.000]   using just convolutions.
[01:07:47.000 --> 01:07:51.160]   So this is the mixer layer, which we implement just
[01:07:51.160 --> 01:07:51.920]   using convolution.
[01:07:51.920 --> 01:07:56.600]   So we replace every linear layer with a convolution operation
[01:07:56.600 --> 01:07:58.440]   and 1 by 1 kernel.
[01:07:58.440 --> 01:08:02.200]   And we see that the output matches that of the linear
[01:08:02.200 --> 01:08:02.680]   layer.
[01:08:02.680 --> 01:08:05.960]   So you could have all these arguments.
[01:08:05.960 --> 01:08:09.720]   So then that just proves that MLP mixer is like a conv layer
[01:08:09.720 --> 01:08:10.760]   with 1 by 1 kernels.
[01:08:10.760 --> 01:08:12.560]   But does it really?
[01:08:12.560 --> 01:08:16.520]   I mean, so then the controversy is basically
[01:08:16.520 --> 01:08:18.320]   you could even do the patch embedding,
[01:08:18.320 --> 01:08:19.920]   the first part of the architecture,
[01:08:19.920 --> 01:08:22.000]   with a 2D conv, which is this part, which
[01:08:22.000 --> 01:08:24.680]   is converting the image to the per patch fully connected
[01:08:24.680 --> 01:08:27.560]   and to get vector representations of each patch.
[01:08:27.560 --> 01:08:29.840]   So that's, again, also been explained in the blog post.
[01:08:29.840 --> 01:08:32.760]   But you could also then use a convolution 2D to do that.
[01:08:32.760 --> 01:08:35.480]   So there's these different ways of doing things
[01:08:35.480 --> 01:08:40.440]   because convolution operations are so flexible.
[01:08:40.440 --> 01:08:44.560]   But then you could get into all these arguments of,
[01:08:44.560 --> 01:08:45.280]   are they the same?
[01:08:45.280 --> 01:08:47.840]   Is it just a CNN?
[01:08:47.840 --> 01:08:52.120]   But what Dr. Habib and I feel at the end of the day
[01:08:52.120 --> 01:08:54.000]   is that it comes down to semantics.
[01:08:54.000 --> 01:08:57.040]   So we asked Ross--
[01:08:57.040 --> 01:09:00.920]   I pretty much asked Ross Whitman, the author of Team.
[01:09:00.920 --> 01:09:03.080]   And he also said that it comes down to semantics.
[01:09:03.080 --> 01:09:05.160]   It's like the deep learning community is split.
[01:09:05.160 --> 01:09:08.720]   If you see this as a CNN, that's for you.
[01:09:08.720 --> 01:09:11.080]   I mean, that's your interpretation of it.
[01:09:11.080 --> 01:09:13.040]   If we see this as a fully connected,
[01:09:13.040 --> 01:09:14.360]   that's our interpretation of it.
[01:09:14.360 --> 01:09:15.960]   Or like there's no right and wrong.
[01:09:15.960 --> 01:09:17.080]   That's what we think.
[01:09:17.080 --> 01:09:19.600]   But Dr. Habib, do you want to add something here?
[01:09:19.600 --> 01:09:20.240]   Yes.
[01:09:20.240 --> 01:09:23.680]   So we are-- I would say, yes.
[01:09:23.680 --> 01:09:28.840]   I think-- yeah, I think it--
[01:09:28.840 --> 01:09:31.120]   you know, I don't have anything to add
[01:09:31.120 --> 01:09:35.360]   because I kind of agree with both sides.
[01:09:35.360 --> 01:09:41.680]   You can-- the main argument is that it doesn't operate
[01:09:41.680 --> 01:09:43.320]   like CNN.
[01:09:43.320 --> 01:09:45.480]   Therefore, it's not a CNN.
[01:09:45.480 --> 01:09:50.120]   But the thing is that you can use the operation which
[01:09:50.120 --> 01:09:53.120]   CNN performs to replicate MLP.
[01:09:53.120 --> 01:09:57.040]   We can replace NN dot linear with columns and so on.
[01:09:57.040 --> 01:10:00.960]   So the question is like just whether this operation which
[01:10:00.960 --> 01:10:03.440]   are designed for convolutional networks,
[01:10:03.440 --> 01:10:06.400]   if you replace them in MLP, whether it
[01:10:06.400 --> 01:10:12.360]   will convert to convolution type network or--
[01:10:12.360 --> 01:10:14.440]   you know, just--
[01:10:14.440 --> 01:10:17.880]   you can see the argument in both sides, basically.
[01:10:17.880 --> 01:10:18.360]   Yeah.
[01:10:18.360 --> 01:10:19.400]   So yeah.
[01:10:19.400 --> 01:10:19.880]   It's up to--
[01:10:19.880 --> 01:10:20.560]   Yeah, I called it a--
[01:10:20.560 --> 01:10:21.040]   --it's up to you.
[01:10:21.040 --> 01:10:23.920]   In my head, I called it a potato-potato problem.
[01:10:23.920 --> 01:10:24.440]   Right.
[01:10:24.440 --> 01:10:25.600]   It's up to you.
[01:10:25.600 --> 01:10:26.800]   It's up to you.
[01:10:26.800 --> 01:10:29.720]   Maybe we will get up to this video.
[01:10:29.720 --> 01:10:31.880]   I hope we will not get criticized.
[01:10:31.880 --> 01:10:36.240]   But I think it's up to you to decide what it will be.
[01:10:36.240 --> 01:10:37.840]   And maybe there will be some--
[01:10:37.840 --> 01:10:38.440]   I don't know.
[01:10:38.440 --> 01:10:40.000]   Maybe there will be some--
[01:10:40.000 --> 01:10:42.360]   somebody will do some experiment
[01:10:42.360 --> 01:10:45.280]   and will come up with some solution to this problem.
[01:10:45.280 --> 01:10:46.400]   I don't think they--
[01:10:46.400 --> 01:10:48.680]   I think it's still not agreed, right?
[01:10:48.680 --> 01:10:50.320]   What's happening?
[01:10:50.320 --> 01:10:50.800]   OK.
[01:10:50.800 --> 01:10:51.320]   So yeah.
[01:10:51.320 --> 01:10:55.240]   Yeah, I don't think there's a common consensus yet.
[01:10:55.240 --> 01:11:01.160]   And that's the difficulty of discussing very recent papers,
[01:11:01.160 --> 01:11:04.680]   because you don't know what's going to happen next.
[01:11:04.680 --> 01:11:05.240]   Yes.
[01:11:05.240 --> 01:11:11.200]   And we'll be like Swiss people, very independent and natural.
[01:11:11.200 --> 01:11:12.760]   Yeah.
[01:11:12.760 --> 01:11:15.480]   But anyway, then there's this GitHub gist as well,
[01:11:15.480 --> 01:11:19.120]   that kind of showcases that you could replace conv1d
[01:11:19.120 --> 01:11:20.600]   with linear and all that stuff.
[01:11:20.600 --> 01:11:22.760]   So yeah, there's lots of content that we
[01:11:22.760 --> 01:11:24.600]   do provide as supplement.
[01:11:24.600 --> 01:11:27.800]   Thanks, Dr. Habib, for all the wonderful, wonderful
[01:11:27.800 --> 01:11:29.120]   visualizations.
[01:11:29.120 --> 01:11:31.080]   I will go one last time and just have a look
[01:11:31.080 --> 01:11:36.560]   if there's any questions on the report.
[01:11:36.560 --> 01:11:42.200]   The paper reading group, there is one, I believe.
[01:11:42.200 --> 01:11:43.400]   I'll share my screen.
[01:11:43.400 --> 01:11:50.400]   Can you guys still see my screen?
[01:11:50.400 --> 01:11:51.480]   Yes.
[01:11:51.480 --> 01:11:53.160]   OK.
[01:11:53.160 --> 01:11:55.360]   The question is here, on the discussion for FC layers
[01:11:55.360 --> 01:11:59.200]   being equivalent to 1 by 1 cons, see the shared MLP layers
[01:11:59.200 --> 01:12:01.960]   in figure 2 and the code implementation,
[01:12:01.960 --> 01:12:06.080]   where the shared FC layers are implemented as 1 by 1 cons.
[01:12:06.080 --> 01:12:07.200]   OK, yeah, absolutely.
[01:12:07.200 --> 01:12:11.680]   I think there's lots of architectures that implement
[01:12:11.680 --> 01:12:14.520]   basically linear layers as 1 by 1 cons.
[01:12:14.520 --> 01:12:18.160]   And I also found an example of this back in 2020,
[01:12:18.160 --> 01:12:23.400]   January, was GPT2 implementation in Hugging Face.
[01:12:23.400 --> 01:12:27.440]   And they were pretty much using conv1d operation instead
[01:12:27.440 --> 01:12:28.400]   of linear layers.
[01:12:28.400 --> 01:12:32.080]   That's definitely-- that's being followed quite a few times
[01:12:32.080 --> 01:12:36.680]   in the past and has been implemented that way.
[01:12:36.680 --> 01:12:41.760]   So with that being said, I guess that's a wrap for MLP Mixer.
[01:12:41.760 --> 01:12:44.640]   Thanks very much, Dr. Habib, for joining us.
[01:12:44.640 --> 01:12:47.720]   Really, really appreciate your time and effort
[01:12:47.720 --> 01:12:50.200]   that you've put into the MLP Mixer
[01:12:50.200 --> 01:12:54.840]   and being a part of this today and sharing your views,
[01:12:54.840 --> 01:12:57.360]   sharing your opinions, and explaining things
[01:12:57.360 --> 01:12:58.480]   in a wonderful way.
[01:12:58.480 --> 01:13:02.040]   And I always, every minute, I learn new things from you.
[01:13:02.040 --> 01:13:05.080]   So thank you for being a part of the paper reading.
[01:13:05.080 --> 01:13:06.880]   Yeah, it was a pleasure.
[01:13:06.880 --> 01:13:09.320]   Thank you for inviting and hosting
[01:13:09.320 --> 01:13:11.560]   this wonderful paper reading group.
[01:13:11.560 --> 01:13:14.080]   I mean, I hope we will--
[01:13:14.080 --> 01:13:15.920]   we just have to find some nice paper
[01:13:15.920 --> 01:13:21.400]   and hopefully we can hold one more at some time in the future.
[01:13:21.400 --> 01:13:22.160]   Yeah, absolutely.
[01:13:22.160 --> 01:13:23.360]   Look forward to that.
[01:13:23.360 --> 01:13:24.160]   Thank you, guys.
[01:13:24.160 --> 01:13:25.080]   Thanks, everybody.
[01:13:25.080 --> 01:13:25.600]   Bye-bye.
[01:13:25.600 --> 01:13:26.120]   Thank you.
[01:13:26.120 --> 01:13:26.640]   Bye.
[01:13:26.640 --> 01:13:30.000]   [MUSIC PLAYING]
[01:13:30.000 --> 01:13:33.360]   [MUSIC PLAYING]
[01:13:33.360 --> 01:13:36.720]   [MUSIC PLAYING]
[01:13:36.880 --> 01:13:40.240]   [MUSIC PLAYING]
[01:13:41.200 --> 01:13:44.560]   [MUSIC PLAYING]
[01:13:44.560 --> 01:13:47.140]   (upbeat music)

