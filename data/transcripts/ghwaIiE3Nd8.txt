
[00:00:00.000 --> 00:00:05.600]   The following is a conversation with Guido van Rossum, creator of Python, one of the most popular
[00:00:05.600 --> 00:00:12.720]   programming languages in the world, used in almost any application that involves computers, from web
[00:00:12.720 --> 00:00:18.400]   back-end development to psychology, neuroscience, computer vision, robotics, deep learning,
[00:00:18.400 --> 00:00:24.560]   natural language processing, and almost any subfield of AI. This conversation is part of
[00:00:24.560 --> 00:00:29.280]   MIT course on artificial general intelligence and the artificial intelligence podcast.
[00:00:29.840 --> 00:00:35.600]   If you enjoy it, subscribe on YouTube, iTunes, or your podcast provider of choice,
[00:00:35.600 --> 00:00:40.720]   or simply connect with me on Twitter @LexFriedman, spelled F-R-I-D.
[00:00:40.720 --> 00:00:45.600]   And now, here's my conversation with Guido van Rossum.
[00:00:45.600 --> 00:00:52.480]   You were born in the Netherlands in 1956. Your parents and the world around you was
[00:00:53.040 --> 00:01:00.080]   deeply impacted by World War II, as was my family from the Soviet Union. So, with that context,
[00:01:00.080 --> 00:01:08.800]   what is your view of human nature? Are some humans inherently good and some inherently evil,
[00:01:08.800 --> 00:01:12.240]   or do we all have both good and evil within us?
[00:01:14.880 --> 00:01:26.240]   Ouch. I did not expect such a deep one. I guess we all have good and evil potential in us,
[00:01:26.240 --> 00:01:31.440]   and a lot of it depends on circumstances and context.
[00:01:31.440 --> 00:01:37.360]   Out of that world, at least on the Soviet Union side and in Europe,
[00:01:38.080 --> 00:01:45.200]   sort of out of suffering, out of challenge, out of that kind of set of traumatic events,
[00:01:45.200 --> 00:01:52.000]   often emerges beautiful art, music, literature. In an interview I read or heard, you said you
[00:01:52.000 --> 00:01:58.160]   enjoyed Dutch literature when you were a child. Can you tell me about the books that had an
[00:01:58.160 --> 00:02:05.200]   influence on you in your childhood? Well, as a teenager, my favorite writer was,
[00:02:05.200 --> 00:02:14.000]   my favorite Dutch author was a guy named Willem Frederik Hermans, who's writing,
[00:02:14.000 --> 00:02:19.520]   certainly his early novels were all about sort of
[00:02:19.520 --> 00:02:30.480]   ambiguous things that happened during World War II. I think he was a young adult during that time,
[00:02:31.600 --> 00:02:40.880]   and he wrote about it a lot, and very interesting, very good books, I thought, I think.
[00:02:40.880 --> 00:02:47.920]   In a non-fiction way? No, it was all fiction, but it was very much set in
[00:02:47.920 --> 00:02:52.720]   in the ambiguous world of resistance against the Germans,
[00:02:54.560 --> 00:03:03.680]   where often you couldn't tell whether someone was truly in the resistance or really a spy for
[00:03:03.680 --> 00:03:10.960]   the Germans. And some of the characters in his novels sort of cross that line, and you never
[00:03:10.960 --> 00:03:17.600]   really find out what exactly happened. And in his novels, there's always a good guy
[00:03:17.600 --> 00:03:22.480]   and a bad guy, is the nature of good and evil, is it clear there's a hero?
[00:03:22.480 --> 00:03:27.600]   It's no, his heroes are often more, his main characters are often anti-heroes,
[00:03:27.600 --> 00:03:40.400]   and so they're not very heroic. They're often, they fail at some level to accomplish their lofty
[00:03:40.400 --> 00:03:46.640]   goals. And looking at the trajectory through the rest of your life, has literature, Dutch or
[00:03:46.640 --> 00:03:54.240]   English or translation had an impact outside the technical world that you existed in?
[00:03:54.240 --> 00:04:05.280]   I still read novels. I don't think that it impacts me that much directly.
[00:04:05.280 --> 00:04:08.720]   It doesn't impact your work? It's just, it's a...
[00:04:08.720 --> 00:04:15.920]   It's a separate world. My work is highly technical, and sort of the world of art and
[00:04:15.920 --> 00:04:19.200]   literature doesn't really directly have any bearing on it.
[00:04:19.200 --> 00:04:23.920]   You don't think there's a creative element to the design?
[00:04:23.920 --> 00:04:26.880]   You know, some would say art, design of a language is art.
[00:04:26.880 --> 00:04:38.400]   I'm not disagreeing with that. I'm just saying that sort of, I don't feel direct influences
[00:04:38.400 --> 00:04:41.920]   from more traditional art on my own creativity.
[00:04:41.920 --> 00:04:46.560]   Right. Of course, you don't feel doesn't mean it's not somehow deeply there in your subconscious.
[00:04:46.560 --> 00:04:48.720]   Who knows?
[00:04:48.720 --> 00:04:56.880]   Who knows? So, let's go back to your early teens. Your hobbies were building electronic circuits,
[00:04:56.880 --> 00:05:03.920]   building mechanical models. What, if you can just put yourself back in the mind of that
[00:05:05.200 --> 00:05:14.000]   young Guido, 12, 13, 14. Was that grounded in a desire to create a system? So, to create something?
[00:05:14.000 --> 00:05:17.600]   Or was it more just tinkering? Just the joy of puzzle solving?
[00:05:17.600 --> 00:05:21.840]   I think it was more the latter, actually. I...
[00:05:21.840 --> 00:05:34.320]   Maybe towards the end of my high school period, I felt confident enough that I designed my own
[00:05:34.320 --> 00:05:43.680]   circuits that were sort of interesting, somewhat. But a lot of that time, I literally just
[00:05:43.680 --> 00:05:49.520]   took a model kit and followed the instructions, putting the things together. I mean,
[00:05:49.520 --> 00:05:55.120]   I think the first few years that I built electronics kits, I really did not have
[00:05:55.120 --> 00:06:02.480]   enough understanding of sort of electronics to really understand what I was doing. I mean,
[00:06:02.480 --> 00:06:07.200]   I could debug it and I could sort of follow the instructions very carefully,
[00:06:07.200 --> 00:06:17.120]   which has always stayed with me. But I had a very naive model of how a transistor works.
[00:06:17.120 --> 00:06:22.960]   And I don't think that in those days I had any understanding of
[00:06:24.560 --> 00:06:32.800]   coils and capacitors, which actually sort of was a major problem when I started to build more
[00:06:32.800 --> 00:06:39.920]   complex digital circuits, because I was unaware of the sort of the analog part of the...
[00:06:39.920 --> 00:06:46.000]   how they actually work. And I would have things that...
[00:06:48.880 --> 00:06:56.800]   The schematic looked... everything looked fine and it didn't work. And what I didn't realize
[00:06:56.800 --> 00:07:02.080]   was that there was some megahertz level oscillation that was throwing the circuit
[00:07:02.080 --> 00:07:12.080]   off because I had a sort of... two wires were too close or the switches were kind of poorly built.
[00:07:13.040 --> 00:07:18.960]   But through that time, I think it's really interesting and instructive to think about,
[00:07:18.960 --> 00:07:24.880]   because there's echoes of it are in this time now. So in the 1970s, the personal computer was being
[00:07:24.880 --> 00:07:33.920]   born. So did you sense in tinkering with these circuits, did you sense the encroaching revolution
[00:07:33.920 --> 00:07:40.240]   in personal computing? So if at that point, we would sit you down and ask you to predict the
[00:07:40.240 --> 00:07:48.560]   80s and the 90s, do you think you would be able to do so successfully to unroll this process?
[00:07:48.560 --> 00:07:57.840]   No, I had no clue. I remember, I think in the summer after my senior year,
[00:07:57.840 --> 00:08:05.280]   or maybe it was the summer after my junior year. Well, at some point, I think when I was 18,
[00:08:05.280 --> 00:08:14.320]   I went on a trip to the Math Olympiad in Eastern Europe. And there was like, I was part of the
[00:08:14.320 --> 00:08:21.120]   Dutch team. And there were other nerdy kids that sort of had different experiences. And one of
[00:08:21.120 --> 00:08:27.120]   them told me about this amazing thing called a computer. And I had never heard that word.
[00:08:28.480 --> 00:08:35.280]   My own explorations in electronics were sort of about very simple digital circuits.
[00:08:35.280 --> 00:08:44.800]   And I had sort of, I had the idea that I somewhat understood how a digital calculator worked.
[00:08:44.800 --> 00:08:54.000]   And so there is maybe some echoes of computers there, but I never made that connection. I didn't
[00:08:54.000 --> 00:09:01.360]   know that when my parents were paying for magazine subscriptions, using punched cards,
[00:09:01.360 --> 00:09:06.480]   that there was something called a computer that was involved that read those cards and
[00:09:06.480 --> 00:09:12.320]   transferred the money between accounts. I was also not really interested in those things.
[00:09:12.320 --> 00:09:21.120]   It was only when I went to university to study math that I found out that they had a computer
[00:09:21.120 --> 00:09:26.320]   and students were allowed to use it. And there was some, you're supposed to talk to that computer
[00:09:26.320 --> 00:09:32.000]   by programming it. What did that feel like? Yeah, that was the only thing you could do with it.
[00:09:32.000 --> 00:09:38.000]   The computer wasn't really connected to the real world. The only thing you could do was
[00:09:38.000 --> 00:09:45.920]   sort of, you typed your program on a bunch of punched cards, you gave the punched cards to the
[00:09:45.920 --> 00:09:53.840]   operator, and an hour later the operator gave you back your printout. And so all you could do was
[00:09:53.840 --> 00:10:02.800]   write a program that did something very abstract. And I don't even remember what my first forays
[00:10:02.800 --> 00:10:12.400]   into programming were, but they were sort of doing simple math exercises and just to learn
[00:10:12.400 --> 00:10:19.280]   how a programming language worked. Did you sense, okay, first year of college,
[00:10:19.280 --> 00:10:23.440]   you see this computer, you're able to have a program and it generates some output.
[00:10:23.440 --> 00:10:31.840]   Did you start seeing the possibility of this? Or was it a continuation of the tinkering with
[00:10:31.840 --> 00:10:38.640]   circuits? Did you start to imagine that, one, the personal computer, but did you see it as something
[00:10:38.640 --> 00:10:46.320]   that is a tool, like a word processing tool, maybe for gaming or something? Or did you start to
[00:10:46.320 --> 00:10:52.880]   imagine that it could be going to the world of robotics? Like the Frankenstein picture,
[00:10:52.880 --> 00:10:56.960]   that you could create an artificial being, there's like another entity in front of you.
[00:10:56.960 --> 00:11:04.000]   You did not see a computer? I don't think I really saw it that way. I was really more
[00:11:04.000 --> 00:11:10.560]   interested in the tinkering. It's maybe not a sort of a complete coincidence that I ended up
[00:11:10.560 --> 00:11:17.680]   sort of creating a programming language, which is a tool for other programmers.
[00:11:17.680 --> 00:11:23.440]   I've always been very focused on the sort of activity of programming itself and not so much
[00:11:23.440 --> 00:11:32.720]   what happens with the program you write. I do remember, and I don't remember,
[00:11:33.680 --> 00:11:38.160]   maybe in my second or third year, probably my second actually,
[00:11:38.160 --> 00:11:44.640]   someone pointed out to me that there was this thing called Conway's Game of Life.
[00:11:44.640 --> 00:11:50.400]   You're probably familiar with it. I think...
[00:11:50.400 --> 00:11:53.040]   In the 70s, I think is when you came up with it.
[00:11:53.040 --> 00:12:00.880]   So, there was a Scientific American column by someone who did a monthly column about
[00:12:00.880 --> 00:12:07.280]   mathematical diversions. I'm also blanking out on the guy's name. It was very famous at the time,
[00:12:07.280 --> 00:12:13.040]   and I think up to the 90s or so. And one of his columns was about Conway's Game of Life,
[00:12:13.040 --> 00:12:16.160]   and he had some illustrations, and he wrote down all the rules.
[00:12:16.160 --> 00:12:22.240]   And sort of there was the suggestion that this was philosophically interesting,
[00:12:22.240 --> 00:12:29.600]   that that was why Conway had called it that. And all I had was like the two pages photocopy
[00:12:29.600 --> 00:12:36.800]   of that article. I don't even remember where I got it. But it spoke to me, and I remember
[00:12:36.800 --> 00:12:45.680]   implementing a version of that game for the batch computer we were using, where
[00:12:45.680 --> 00:12:53.040]   I had a whole Pascal program that sort of read an initial situation from input and read some numbers
[00:12:54.720 --> 00:13:02.480]   that said, "Do so many generations and print every so many generations." And then out would come
[00:13:02.480 --> 00:13:09.440]   pages and pages of sort of things. Patterns of different kinds.
[00:13:09.440 --> 00:13:18.000]   Yeah. And I remember much later, I've done a similar thing using Python. But that original
[00:13:18.000 --> 00:13:26.480]   version I wrote at the time, I found interesting because I combined it with some trick I had
[00:13:26.480 --> 00:13:34.640]   learned during my electronics hobbyist times. I essentially first on paper, I designed a simple
[00:13:34.640 --> 00:13:43.760]   circuit built out of logic gates that took nine bits of input, which is the sort of the
[00:13:44.720 --> 00:13:53.600]   cell and its neighbors, and produced a new value for that cell. And it's like a combination of a
[00:13:53.600 --> 00:14:00.480]   half adder and some other clipping. No, it's actually a full adder. And so I had worked that
[00:14:00.480 --> 00:14:10.480]   out. And then I translated that into a series of Boolean operations on Pascal integers, where
[00:14:10.480 --> 00:14:20.480]   you could use the integers as bitwise values. And so I could basically generate 60 bits
[00:14:20.480 --> 00:14:28.320]   of a generation in like eight instructions or so.
[00:14:28.320 --> 00:14:29.760]   Nice.
[00:14:29.760 --> 00:14:30.720]   So I was proud of that.
[00:14:30.720 --> 00:14:36.400]   It's funny that you mentioned, so for people who don't know Conway's Game of Life,
[00:14:37.840 --> 00:14:43.760]   it's a cellular automata where there's single compute units that kind of look at their neighbors
[00:14:43.760 --> 00:14:50.480]   and figure out what they look like in the next generation based on the state of their neighbors.
[00:14:50.480 --> 00:14:58.400]   And this is a deeply distributed system in concept at least. And then there's simple rules
[00:14:58.400 --> 00:15:05.440]   that all of them follow. And somehow out of this simple rule, when you step back and look at what
[00:15:05.440 --> 00:15:13.360]   occurs, it's beautiful. There's an emergent complexity, even though the underlying rules
[00:15:13.360 --> 00:15:17.680]   are simple, there's an emergent complexity. Now, the funny thing is, you've implemented this. And
[00:15:17.680 --> 00:15:24.480]   the thing you're commenting on is you're proud of a hack you did to make it run efficiently.
[00:15:24.480 --> 00:15:29.360]   Well, you're not commenting on what like, this is a beautiful implementation.
[00:15:30.480 --> 00:15:36.800]   You're not commenting on the fact that there's an emergent complexity, that you've coded a simple
[00:15:36.800 --> 00:15:42.080]   program. And when you step back and you print out the following generation after generation,
[00:15:42.080 --> 00:15:46.480]   that's stuff that you may have not predicted would happen is happening.
[00:15:46.480 --> 00:15:47.280]   Right.
[00:15:47.280 --> 00:15:53.840]   And is that magic? I mean, that's the magic that all of us feel when we program. When you create
[00:15:53.840 --> 00:15:59.600]   a program, and then you run it, and whether it's Hello World, or show something on screen,
[00:15:59.600 --> 00:16:05.120]   if there's a graphical component, are you seeing the magic and the mechanism of creating that?
[00:16:05.120 --> 00:16:12.320]   I think I went back and forth. As a student, we had an incredibly small budget
[00:16:12.320 --> 00:16:19.120]   of computer time that we could use. It was actually measured. I once got in trouble with
[00:16:19.120 --> 00:16:24.720]   one of my professors because I had overspent the department's budget. It's a different story. But
[00:16:25.440 --> 00:16:36.960]   so I actually wanted the efficient implementation because I also wanted to explore what would
[00:16:36.960 --> 00:16:44.800]   happen with a larger number of generations and a larger sort of size of the board.
[00:16:44.800 --> 00:16:49.360]   And so once the implementation was flawless,
[00:16:51.920 --> 00:16:58.080]   I would feed it different patterns. And then I think maybe there was a follow-up article where
[00:16:58.080 --> 00:17:05.280]   there were patterns that were like gliders, patterns that repeated themselves after a number of
[00:17:05.280 --> 00:17:14.800]   generations, but translated one or two positions to the right or up or something like that.
[00:17:16.720 --> 00:17:22.320]   And I remember things like glider guns. Well, you can google Conway's Game of Life.
[00:17:22.320 --> 00:17:26.720]   People still go on over it.
[00:17:26.720 --> 00:17:32.560]   For a reason, because it's not really well understood why. I mean, this is what Stephen
[00:17:32.560 --> 00:17:40.080]   Wolfram is obsessed about. We don't have the mathematical tools to describe the kind of
[00:17:40.080 --> 00:17:45.120]   complexity that emerges in these kinds of systems. The only way you can do it is to run it.
[00:17:45.840 --> 00:17:56.160]   But I'm not convinced that it's sort of a problem that lends itself to classic mathematical analysis.
[00:17:56.160 --> 00:18:04.960]   So one theory of how you create an artificial intelligence or an artificial being is you kind
[00:18:04.960 --> 00:18:10.400]   of have to, same with the Game of Life, you kind of have to create a universe and let it run.
[00:18:11.360 --> 00:18:17.760]   That creating it from scratch in a design way, coding up a Python program that creates
[00:18:17.760 --> 00:18:22.720]   a fully intelligent system may be quite challenging. You might need to create a
[00:18:22.720 --> 00:18:29.120]   universe just like the Game of Life is. Well, you might have to experiment with a lot of
[00:18:29.120 --> 00:18:37.840]   different universes before. There is a set of rules that doesn't essentially always just end up
[00:18:38.960 --> 00:18:47.360]   repeating itself in a trivial way. Yeah, and Stephen Wolfram works with these simple rules.
[00:18:47.360 --> 00:18:52.640]   Says that it's kind of surprising how quickly you find rules that create interesting things.
[00:18:52.640 --> 00:18:58.960]   You shouldn't be able to, but somehow you do. And so maybe our universe is laden with
[00:18:58.960 --> 00:19:03.440]   rules that will create interesting things that might not look like humans, but
[00:19:03.440 --> 00:19:08.640]   you know, emergent phenomena that's interesting may not be as difficult to create as we think.
[00:19:09.120 --> 00:19:09.680]   Sure.
[00:19:09.680 --> 00:19:15.120]   But let me sort of ask, at that time, you know, some of the world, at least in popular press,
[00:19:15.120 --> 00:19:23.440]   was kind of captivated, perhaps at least in America, by the idea of artificial intelligence.
[00:19:23.440 --> 00:19:31.840]   That these computers would be able to think pretty soon. And did that touch you at all? Did that,
[00:19:31.840 --> 00:19:36.240]   in science fiction or in reality, in any way?
[00:19:37.680 --> 00:19:43.440]   I didn't really start reading science fiction until much, much later.
[00:19:43.440 --> 00:19:52.560]   I think as a teenager, I read maybe one bundle of science fiction stories.
[00:19:52.560 --> 00:19:56.960]   Was it in the background somewhere? Like in your thoughts?
[00:19:56.960 --> 00:20:04.160]   That sort of the using computers to build something intelligent always felt to me,
[00:20:04.160 --> 00:20:10.320]   because I felt I had so much understanding of what actually goes on inside a computer.
[00:20:10.320 --> 00:20:16.880]   I knew how many bits of memory it had and how difficult it was to program.
[00:20:16.880 --> 00:20:29.360]   I didn't believe at all that you could just build something intelligent out of that,
[00:20:29.360 --> 00:20:37.120]   that would really sort of satisfy my definition of intelligence.
[00:20:37.120 --> 00:20:44.960]   I think the most influential thing that I read in my early 20s was Gödel, Escher, Bach.
[00:20:44.960 --> 00:20:51.760]   That was about consciousness and that was a big eye opener, in some sense.
[00:20:53.520 --> 00:21:01.200]   In what sense? So, on your own brain, did you at the time or do you now see your own brain as a
[00:21:01.200 --> 00:21:07.680]   computer? Or is there a total separation of the way? So, yeah, you're very pragmatically,
[00:21:07.680 --> 00:21:15.200]   practically know the limits of memory, the limits of this sequential computing or weakly paralyzed
[00:21:15.200 --> 00:21:22.480]   computing. You just know what we have now and it's hard to see how it creates. But it's also
[00:21:22.480 --> 00:21:30.640]   easy to see it was in the 40s, 50s, 60s and now at least similarities between the brain and our
[00:21:30.640 --> 00:21:41.600]   computers. Oh yeah, I mean, I totally believe that brains are computers in some sense. I mean,
[00:21:41.600 --> 00:21:48.640]   the rules they use to play by are pretty different from the rules we can sort of
[00:21:49.600 --> 00:22:01.840]   implement in our current hardware. But I don't believe in like a separate thing that infuses us
[00:22:01.840 --> 00:22:10.880]   with intelligence or consciousness or any of that. There's no soul. I've been an atheist probably
[00:22:11.840 --> 00:22:19.520]   from when I was 10 years old, just by thinking a bit about math and the universe. And well,
[00:22:19.520 --> 00:22:27.440]   my parents were atheists. Now, I know that you could be an atheist and still believe that there
[00:22:27.440 --> 00:22:35.280]   is something sort of about intelligence or consciousness that cannot possibly emerge
[00:22:36.240 --> 00:22:42.560]   from a fixed set of rules. I am not in that camp. I totally see that
[00:22:42.560 --> 00:22:54.960]   sort of given how many millions of years evolution took its time, DNA is a particular machine that
[00:22:54.960 --> 00:23:08.560]   sort of encodes information and an unlimited amount of information in chemical form and
[00:23:08.560 --> 00:23:15.200]   has figured out a way to replicate itself. I thought that that was maybe it's 300 million
[00:23:15.200 --> 00:23:20.880]   years ago, but I thought it was closer to half a billion years ago that that sort of
[00:23:22.240 --> 00:23:28.480]   originated and it hasn't really changed. The structure of DNA hasn't changed ever since. That
[00:23:28.480 --> 00:23:34.480]   is like our binary code that we have in hardware. I mean...
[00:23:34.480 --> 00:23:39.680]   - The basic programming language hasn't changed, but maybe the programming itself...
[00:23:39.680 --> 00:23:46.640]   - Obviously, it did sort of... It happened to be a set of rules that was good enough to
[00:23:48.240 --> 00:23:58.560]   sort of develop endless variability and sort of the idea of self-replicating molecules
[00:23:58.560 --> 00:24:05.680]   competing with each other for resources and one type eventually sort of always taking over.
[00:24:05.680 --> 00:24:12.560]   That happened before there were any fossils. So, we don't know how that exactly happened, but
[00:24:14.480 --> 00:24:17.360]   I believe it's clear that that did happen.
[00:24:17.360 --> 00:24:25.360]   - Can you comment on consciousness and how you see it? Because I think we'll talk about
[00:24:25.360 --> 00:24:30.480]   programming quite a bit. We'll talk about intelligence connecting to programming
[00:24:30.480 --> 00:24:38.400]   fundamentally, but consciousness is this whole other thing. Do you think about it often as a
[00:24:38.400 --> 00:24:43.920]   developer of a programming language and as a human?
[00:24:43.920 --> 00:24:53.440]   - Those are pretty sort of separate topics. My line of work, working with programming,
[00:24:53.440 --> 00:25:00.880]   does not involve anything that goes in the direction of developing intelligence or
[00:25:00.880 --> 00:25:07.760]   consciousness. But sort of privately, as an avid reader of popular science writing,
[00:25:07.760 --> 00:25:21.360]   I have some thoughts which is mostly that I don't actually believe that consciousness
[00:25:21.360 --> 00:25:32.400]   is an all or nothing thing. I have a feeling that, and I forget what I read that influenced this, but
[00:25:32.400 --> 00:25:40.560]   I feel that if you look at a cat or a dog or a mouse, they have some form of intelligence.
[00:25:41.120 --> 00:25:53.040]   If you look at a fish, it has some form of intelligence. And that evolution just took a
[00:25:53.040 --> 00:26:02.000]   long time, but I feel that the evolution of more and more intelligence that led to the human form
[00:26:02.000 --> 00:26:14.640]   of intelligence followed the evolution of the senses, especially the visual sense. I mean,
[00:26:14.640 --> 00:26:22.800]   there is an enormous amount of processing that's needed to interpret a scene, and humans are still
[00:26:22.800 --> 00:26:32.960]   better at that than computers are. And I have a feeling that there is a sort of,
[00:26:32.960 --> 00:26:44.480]   the reason that mammals in particular developed the levels of consciousness that they have,
[00:26:44.480 --> 00:26:51.360]   and then eventually going from intelligence to self-awareness and consciousness,
[00:26:52.160 --> 00:26:58.000]   has to do with sort of being a robot that has very highly developed senses.
[00:26:58.000 --> 00:27:04.720]   - Has a lot of rich sensory information coming in. So, that's a really interesting thought,
[00:27:04.720 --> 00:27:12.880]   that whatever that basic mechanism of DNA, whatever that basic building blocks of programming,
[00:27:12.880 --> 00:27:20.000]   is if you just add more abilities, more high resolution sensors, more sensors,
[00:27:20.560 --> 00:27:25.920]   you just keep stacking those things on top, that this basic programming in trying to survive
[00:27:25.920 --> 00:27:31.440]   develops very interesting things that start to us humans to appear like intelligence and
[00:27:31.440 --> 00:27:39.440]   consciousness. - Yeah, so as far as robots go, I think that the self-driving cars have the sort of,
[00:27:39.440 --> 00:27:49.600]   the greatest opportunity of developing something like that, because when I drive myself, I don't
[00:27:49.600 --> 00:27:56.800]   just pay attention to the rules of the road. I also look around and I get clues from that,
[00:27:56.800 --> 00:28:04.800]   "Oh, this is a shopping district. Oh, here's an old lady crossing the street. Oh, here is someone
[00:28:04.800 --> 00:28:12.640]   carrying a pile of mail. There's a mailbox. I bet you they're gonna cross the street to reach that
[00:28:12.640 --> 00:28:21.920]   mailbox." And I slow down, and I don't even think about that. And so, there is so much where you
[00:28:21.920 --> 00:28:29.440]   turn your observations into an understanding of what other consciousnesses are going to do,
[00:28:29.440 --> 00:28:35.200]   or what other systems in the world are going to be, "Oh, that tree is gonna fall."
[00:28:37.440 --> 00:28:47.360]   Yeah, I see much more of, I expect somehow that if anything is going to become conscious,
[00:28:47.360 --> 00:28:56.480]   it's going to be the self-driving car and not the network of a bazillion computers in a Google or
[00:28:56.480 --> 00:29:05.520]   Amazon data center that are all networked together to do whatever they do. - So, in that sense,
[00:29:06.160 --> 00:29:10.480]   so you actually highlight, that's what I work in, is in autonomous vehicles, you highlight the big
[00:29:10.480 --> 00:29:17.280]   gap between what we currently can't do and what we truly need to be able to do to solve the problem.
[00:29:17.280 --> 00:29:22.000]   Under that formulation, then consciousness and intelligence is something that
[00:29:22.000 --> 00:29:27.760]   basically a system should have in order to interact with us humans,
[00:29:28.880 --> 00:29:36.160]   as opposed to some kind of abstract notion of consciousness. Consciousness is something that
[00:29:36.160 --> 00:29:44.080]   you need to have to be able to empathize, to be able to fear, understand what the fear of death
[00:29:44.080 --> 00:29:48.720]   is, all these aspects that are important for interacting with pedestrians, you need to be able
[00:29:48.720 --> 00:30:00.080]   to do basic computation based on our human desires and thoughts. - Yeah, if you look at the dog,
[00:30:00.080 --> 00:30:05.520]   the dog clearly knows, I mean, I'm not a dog owner, but I have friends who have dogs, the dogs
[00:30:05.520 --> 00:30:11.040]   clearly know what the humans around them are going to do, or at least they have a model of what those
[00:30:11.040 --> 00:30:17.520]   humans are going to do, and they learn. Some dogs know when you're going out and they want to go out
[00:30:17.520 --> 00:30:23.920]   with you, they're sad when you leave them alone, they cry, they're afraid because they were
[00:30:23.920 --> 00:30:36.320]   mistreated when they were younger. We don't assign sort of consciousness to dogs, or at least not
[00:30:36.320 --> 00:30:46.080]   all that much, but I also don't think they have none of that. So, I think it's consciousness and
[00:30:46.080 --> 00:30:54.320]   intelligence are not all or nothing. - The spectrum, it's really interesting. But, in returning to
[00:30:54.320 --> 00:30:59.760]   programming languages and the way we think about building these kinds of things,
[00:30:59.760 --> 00:31:04.080]   about building intelligence, building consciousness, building artificial beings,
[00:31:04.080 --> 00:31:11.280]   so I think one of the exciting ideas came in the 17th century, and with Leibniz, Hobbes, Descartes,
[00:31:11.840 --> 00:31:20.880]   where there's this feeling that you can convert all thought, all reasoning, all the thing that we
[00:31:20.880 --> 00:31:27.200]   find very special in our brains, you can convert all of that into logic. You can formalize it,
[00:31:27.200 --> 00:31:31.840]   formal reasoning, and then once you formalize everything, all of knowledge, then you can just
[00:31:31.840 --> 00:31:35.680]   calculate, and that's what we're doing with our brains is we're calculating. So, there's this
[00:31:35.680 --> 00:31:41.920]   whole idea that this is possible, that this we can actually program. - But they weren't aware of
[00:31:41.920 --> 00:31:49.200]   the concept of pattern matching in the sense that we are aware of it now. They sort of thought you,
[00:31:49.200 --> 00:31:57.920]   they had discovered incredible bits of mathematics, like Newton's calculus, and
[00:31:58.960 --> 00:32:08.080]   their sort of idealism, their sort of extension of what they could do with logic and math sort of
[00:32:08.080 --> 00:32:19.840]   went along those lines, and they thought there's like, yeah, logic, there's like a bunch of rules
[00:32:19.840 --> 00:32:28.080]   and a bunch of input. They didn't realize that how you recognize a face is not just a bunch of rules,
[00:32:28.080 --> 00:32:38.240]   but it's a shit ton of data, plus a circuit that sort of interprets the visual clues and the
[00:32:38.240 --> 00:32:48.560]   context and everything else, and somehow can massively parallel pattern match against stored
[00:32:48.560 --> 00:32:56.400]   rules. I mean, if I see you tomorrow here in front of the Dropbox office, I might recognize you.
[00:32:56.400 --> 00:33:02.000]   Even if I'm wearing a different shirt. - Yeah, but if I see you tomorrow in a coffee shop in Belmont,
[00:33:02.000 --> 00:33:08.000]   I might have no idea that it was you, or on the beach or whatever. I make those kind of mistakes
[00:33:08.000 --> 00:33:14.480]   myself all the time. I see someone that I only know as like, oh, this person is a colleague
[00:33:14.480 --> 00:33:21.520]   of my wife's, and then I see them at the movies and I don't recognize them. - But do you see those,
[00:33:22.080 --> 00:33:32.560]   you call it pattern matching, do you see that rules is unable to encode that? Everything you
[00:33:32.560 --> 00:33:37.040]   see, all the pieces of information, you look around this room, I'm wearing a black shirt,
[00:33:37.040 --> 00:33:42.320]   I have a certain height, I'm a human, all these, there's probably tens of thousands of facts you
[00:33:42.320 --> 00:33:47.680]   pick up moment by moment about this scene, you take them for granted and you accumulate,
[00:33:47.680 --> 00:33:51.600]   aggregate them together to understand the scene. You don't think all of that could be encoded to
[00:33:51.600 --> 00:33:57.840]   where at the end of the day you can just put it all on the table and calculate? - I don't know
[00:33:57.840 --> 00:34:07.360]   what that means. I mean, yes, in the sense that there is no actual magic there, but there are
[00:34:07.360 --> 00:34:17.520]   enough layers of abstraction from the facts as they enter my eyes and my ears, to the understanding of
[00:34:17.520 --> 00:34:29.920]   the scene, that I don't think that AI has really covered enough of that distance. It's like if you
[00:34:29.920 --> 00:34:40.080]   take a human body and you realize it's built out of atoms, well, that is a uselessly reductionist
[00:34:40.080 --> 00:34:46.320]   view, right? The body is built out of organs, the organs are built out of cells, the cells are built
[00:34:46.320 --> 00:34:53.600]   out of proteins, the proteins are built out of amino acids, the amino acids are built out of
[00:34:53.600 --> 00:35:00.080]   atoms and then you get to quantum mechanics. - So, that's a very pragmatic view, I mean,
[00:35:00.080 --> 00:35:04.560]   obviously as an engineer I agree with that kind of view, but you also have to consider the
[00:35:04.560 --> 00:35:11.360]   Sam Harris view of, well, intelligence is just information processing.
[00:35:12.960 --> 00:35:17.440]   Like you said, you take in sensory information, you do some stuff with it and you come up with
[00:35:17.440 --> 00:35:22.400]   actions that are intelligent. - He makes it sound so easy,
[00:35:22.400 --> 00:35:26.720]   I don't know who Sam Harris is. - Oh, he's a philosopher, so this is
[00:35:26.720 --> 00:35:31.440]   how philosophers often think, right? And essentially that's what Descartes was, is,
[00:35:31.440 --> 00:35:37.600]   wait a minute, if there is, like you said, no magic, so he basically says, it doesn't appear
[00:35:37.600 --> 00:35:44.560]   like there's any magic, but we know so little about it that it might as well be magic. So,
[00:35:44.560 --> 00:35:48.800]   just because we know that we're made of atoms, just because we know we're made of organs,
[00:35:48.800 --> 00:35:54.320]   the fact that we know very little how to get from the atoms to organs in a way that's recreatable
[00:35:54.320 --> 00:36:01.200]   means that you shouldn't get too excited just yet about the fact that you figured out that we're
[00:36:01.200 --> 00:36:09.760]   made of atoms. - Right, and the same about taking facts as our sensory organs take them in,
[00:36:09.760 --> 00:36:19.280]   and turning that into reasons and actions, that sort of, there are a lot of abstractions that
[00:36:19.280 --> 00:36:30.000]   we haven't quite figured out how to deal with those. I mean, sometimes, I don't know if I can
[00:36:30.000 --> 00:36:38.000]   go on a tangent or not. - Please, I'll drag you back in. - Sure, so if I take a simple program
[00:36:38.000 --> 00:36:47.040]   that parses, say I have a compiler, it parses a program. In a sense, the input routine of that
[00:36:47.040 --> 00:36:57.120]   compiler, of that parser, is a sensing organ, and it builds up a mighty complicated internal
[00:36:57.120 --> 00:37:03.920]   representation of the program it just saw. It doesn't just have a linear sequence of bytes
[00:37:03.920 --> 00:37:10.640]   representing the text of the program anymore, it has an abstract syntax tree, and I don't know how
[00:37:10.640 --> 00:37:18.640]   many of your viewers or listeners are familiar with compiler technology, but there's... - Fewer
[00:37:18.640 --> 00:37:26.160]   and fewer these days, right? - That's also true, probably. People want to take a shortcut, but
[00:37:26.160 --> 00:37:34.160]   there's sort of, this abstraction is a data structure that the compiler then uses to produce
[00:37:34.160 --> 00:37:40.080]   outputs that is relevant, like a translation of that program to machine code that can be executed
[00:37:40.080 --> 00:37:53.360]   by hardware. And then that data structure gets thrown away. When a fish or a fly sees,
[00:37:55.040 --> 00:38:04.480]   sort of gets visual impulses, I'm sure it also builds up some data structure, and for the fly
[00:38:04.480 --> 00:38:12.400]   that may be very minimal, a fly may have only a few, I mean, in the case of a fly's brain,
[00:38:12.400 --> 00:38:20.960]   I could imagine that there are few enough layers of abstraction that it's not much more than
[00:38:21.520 --> 00:38:28.000]   when it's darker here than it is here. Well, it can sense motion, because a fly sort of responds
[00:38:28.000 --> 00:38:35.440]   when you move your arm towards it. So clearly, its visual processing is intelligent, well,
[00:38:35.440 --> 00:38:43.600]   not intelligent, but it has an abstraction for motion. And we still have similar things in,
[00:38:43.600 --> 00:38:48.400]   but much more complicated in our brains. I mean, otherwise you couldn't drive a car if you
[00:38:49.840 --> 00:38:56.720]   didn't have an incredibly good abstraction for motion. Yeah, in some sense, the same abstraction
[00:38:56.720 --> 00:39:04.240]   for motion is probably one of the primary sources of information for us. We just know what to do,
[00:39:04.240 --> 00:39:08.960]   I think we know what to do with that. We've built up other abstractions on top. We build much more
[00:39:08.960 --> 00:39:15.360]   complicated data structures based on that. And we build more persistent data structures,
[00:39:15.360 --> 00:39:21.760]   sort of after some processing, some information sort of gets stored in our memory, pretty much
[00:39:21.760 --> 00:39:27.680]   permanently and is available on recall. I mean, there are some things that you sort of,
[00:39:27.680 --> 00:39:35.920]   you're conscious that you're remembering it, like, you give me your phone number, I, well,
[00:39:35.920 --> 00:39:40.640]   at my age, I have to write it down. But I could imagine I could remember those seven numbers or
[00:39:41.200 --> 00:39:47.440]   10 digits and reproduce them in a while if I sort of repeat them to myself a few times.
[00:39:47.440 --> 00:39:54.240]   So, that's a fairly conscious form of memorization. On the other hand,
[00:39:54.240 --> 00:40:02.080]   how do I recognize your face? I have no idea. My brain has a whole bunch of specialized hardware
[00:40:02.080 --> 00:40:08.720]   that knows how to recognize faces. I don't know how much of that is sort of coded in our DNA,
[00:40:08.720 --> 00:40:13.920]   and how much of that is trained over and over between the ages of zero and three.
[00:40:13.920 --> 00:40:22.400]   But somehow our brains know how to do lots of things like that, that are useful in our
[00:40:22.400 --> 00:40:30.000]   interactions with other humans, without really being conscious of how it's done anymore.
[00:40:30.000 --> 00:40:35.920]   Right. So, our actual day-to-day lives, we're operating at the very highest level of abstraction.
[00:40:35.920 --> 00:40:40.720]   We're just not even conscious of all the little details underlying it. There's compilers on top
[00:40:40.720 --> 00:40:45.040]   of, it's like turtles on top of turtles, or turtles all the way down, compilers all the way down.
[00:40:45.040 --> 00:40:52.800]   But that's essentially, you say that there's no magic. That's what I was trying to get at,
[00:40:52.800 --> 00:40:58.640]   I think, is with Descartes started this whole train of saying that there's no magic. I mean,
[00:40:58.640 --> 00:41:03.040]   there's all this before. Well, didn't Descartes also have the notion though that the soul
[00:41:03.040 --> 00:41:06.320]   and the body were fundamentally separate?
[00:41:06.320 --> 00:41:12.320]   - Yeah, I think he had to write in God in there for political reasons. So, I don't actually,
[00:41:12.320 --> 00:41:18.800]   I'm not a historian, but there's notions in there that all of reasoning, all of human thought can
[00:41:18.800 --> 00:41:26.960]   be formalized. I think that continued in the 20th century with Russell and with Gato's
[00:41:26.960 --> 00:41:32.800]   incompleteness theorem, this debate of what are the limits of the things that can be formalized,
[00:41:32.800 --> 00:41:37.920]   that's where the Turing machine came along, and this exciting idea, I mean, underlying a lot of
[00:41:37.920 --> 00:41:45.440]   computing, that you can do quite a lot with a computer. You can encode a lot of the stuff we're
[00:41:45.440 --> 00:41:51.200]   talking about in terms of recognizing faces and so on, theoretically, in an algorithm that can
[00:41:51.200 --> 00:42:01.120]   then run on a computer. And in that context, I'd like to ask programming in a philosophical way,
[00:42:02.080 --> 00:42:09.680]   so what does it mean to program a computer? So, you said you write a Python program or a compiled,
[00:42:09.680 --> 00:42:19.120]   a C++ program that compiles to some byte code. It's forming layers, you're programming a layer
[00:42:19.120 --> 00:42:25.760]   of abstraction that's higher. How do you see programming in that context? Can it keep getting
[00:42:25.760 --> 00:42:27.680]   higher and higher levels of abstraction?
[00:42:27.680 --> 00:42:35.840]   I think at some point, the higher levels of abstraction will not be called programming,
[00:42:35.840 --> 00:42:45.520]   and they will not resemble what we call programming at the moment. There will not be
[00:42:45.520 --> 00:42:53.600]   source code. I mean, there will still be source code sort of at a lower level of the machine,
[00:42:53.600 --> 00:43:01.120]   just like there are still molecules and electrons and sort of proteins in our brains.
[00:43:01.120 --> 00:43:10.400]   And so, there's still programming and system administration and who knows what keeping,
[00:43:10.400 --> 00:43:16.720]   to keep the machine running. But what the machine does is a different level of abstraction,
[00:43:16.720 --> 00:43:24.240]   in a sense. And as far as I understand, the way that for the last decade or more people have made
[00:43:24.240 --> 00:43:30.960]   progress with things like facial recognition or the self-driving cars is all by endless,
[00:43:30.960 --> 00:43:39.840]   endless amounts of training data, where at least as a layperson, and I feel myself
[00:43:40.560 --> 00:43:49.600]   totally as a layperson in that field, it looks like the researchers who publish the results
[00:43:49.600 --> 00:43:59.280]   don't necessarily know exactly how their algorithms work. And I often get upset when I
[00:43:59.280 --> 00:44:06.400]   sort of read a sort of a fluff piece about Facebook in the newspaper or social networks,
[00:44:06.400 --> 00:44:13.360]   and they say, "Well, algorithms." And that's like a totally different interpretation of the word
[00:44:13.360 --> 00:44:20.720]   algorithm. Because for me, the way I was trained or what I learned when I was eight or 10 years
[00:44:20.720 --> 00:44:27.600]   old, an algorithm is a set of rules that you completely understand that can be mathematically
[00:44:27.600 --> 00:44:36.400]   analyzed. And you can prove things. You can prove that Aristoteles' sieve produces all prime
[00:44:36.400 --> 00:44:42.000]   numbers and only prime numbers. Yeah. So I don't know if you know who Andrej Karpathy is.
[00:44:42.000 --> 00:44:51.600]   I'm afraid not. So he's a head of AI at Tesla now, but he was at Stanford before. And he has this
[00:44:51.600 --> 00:44:58.480]   cheeky way of calling this concept Software 2.0. So let me disentangle that for a second.
[00:44:58.480 --> 00:45:05.360]   So kind of what you're referring to is the traditional, traditional, the algorithm,
[00:45:05.360 --> 00:45:09.280]   the concept of an algorithm, something that's there, it's clear, you can read it, you understand
[00:45:09.280 --> 00:45:16.480]   it, you can prove it's functioning, it's kind of Software 1.0. And what Software 2.0 is,
[00:45:17.120 --> 00:45:22.800]   is exactly what you described, which is, you have neural networks, which is a type of machine
[00:45:22.800 --> 00:45:28.720]   learning, that you feed a bunch of data, and that neural network learns to do a function.
[00:45:28.720 --> 00:45:35.360]   All you specify is the inputs and the outputs you want. And you can't look inside, you can't
[00:45:35.360 --> 00:45:41.840]   analyze it. All you can do is train this function to map the inputs to the outputs by giving a lot
[00:45:41.840 --> 00:45:47.840]   of data. In that sense, programming becomes getting a lot of data. That's what programming is.
[00:45:47.840 --> 00:45:51.280]   Well, that would be programming 2.0.
[00:45:51.280 --> 00:45:52.800]   2.0, programming 2.0.
[00:45:52.800 --> 00:45:57.680]   I wouldn't call that programming. It's just a different activity, just like
[00:45:57.680 --> 00:46:01.600]   building organs out of cells is not called chemistry.
[00:46:01.600 --> 00:46:10.160]   Well, so let's just step back and think sort of more generally, of course. But you know,
[00:46:10.160 --> 00:46:17.920]   it's like, as a parent teaching, teaching your kids, things can be called programming,
[00:46:17.920 --> 00:46:24.560]   in that same sense, that that's how program is being used, you're providing them data examples,
[00:46:24.560 --> 00:46:29.200]   use cases. So imagine writing a function not by,
[00:46:29.200 --> 00:46:39.840]   not with for loops, and clearly readable text, but more saying, well, here's a lot of examples,
[00:46:40.560 --> 00:46:45.840]   what this function should take. And here's a lot of examples of when it takes those functions,
[00:46:45.840 --> 00:46:54.000]   it should do this, and then figure out the rest. So that's this 2.0 concept. And the question I
[00:46:54.000 --> 00:47:00.080]   have for you is like, it's a very fuzzy way. This is the reality of a lot of these pattern
[00:47:00.080 --> 00:47:06.320]   recognition systems. It's a fuzzy way of quote unquote, programming. What do you think about
[00:47:06.320 --> 00:47:14.000]   this kind of world? Should it be called something totally different than programming? If you're a
[00:47:14.000 --> 00:47:21.840]   software engineer, does that mean you're, you're designing systems that are very can be systematically
[00:47:21.840 --> 00:47:28.960]   tested, evaluated, they have a very specific specification. And then this other fuzzy
[00:47:28.960 --> 00:47:33.920]   software 2.0 world machine learning world, that's that's something else totally? Or is there some
[00:47:33.920 --> 00:47:46.240]   intermixing that's possible? Well, the question is probably only being asked, because we we don't
[00:47:46.240 --> 00:47:55.440]   quite know what that software 2.0 actually is. And it sort of, I think there is a truism that
[00:47:56.720 --> 00:48:04.720]   every task that AI has, has tackled in the past, at some point, we realized how it was done. And
[00:48:04.720 --> 00:48:10.320]   then it was no longer considered part of artificial intelligence, because it was no longer
[00:48:10.320 --> 00:48:19.200]   necessary to, to use that term, it was just, oh, now we know how to do this.
[00:48:20.880 --> 00:48:30.400]   And a new field of science or engineering has been developed. And I don't know if sort of
[00:48:30.400 --> 00:48:38.240]   every form of learning, or sort of controlling computer systems should always be called
[00:48:38.240 --> 00:48:44.720]   programming. So I, I don't know, maybe I'm focused too much on the terminology I, but I expect that,
[00:48:47.760 --> 00:48:58.000]   that there just will be different concepts, where people with sort of different education and a
[00:48:58.000 --> 00:49:06.240]   different model of what they're trying to do, will will develop those concepts.
[00:49:06.240 --> 00:49:14.800]   Yeah. And I guess if you could comment on another way to put this concept is, I think,
[00:49:16.480 --> 00:49:22.800]   I think the kind of functions that neural networks provide, is things as opposed to being able to
[00:49:22.800 --> 00:49:29.760]   upfront prove that this should work for all cases you throw at it. All you're able, it's the worst
[00:49:29.760 --> 00:49:36.400]   case analysis versus average case analysis, all you're able to say is, it's, it seems on everything
[00:49:36.400 --> 00:49:43.360]   we've tested to work 99.9% of the time, but we can't guarantee it, and it, it fails in unexpected
[00:49:43.360 --> 00:49:48.160]   ways. We can even give you examples of how it fails in unexpected ways. But it's like really
[00:49:48.160 --> 00:49:55.200]   good. Most of the time. Yeah, but there's no room for that in current ways we think about programming.
[00:49:55.200 --> 00:50:03.600]   Uh, programming 1.0 is actually sort of
[00:50:06.160 --> 00:50:14.000]   getting to that point too, where the sort of the ideal of a bug-free program
[00:50:14.000 --> 00:50:24.720]   has been abandoned long ago by most software developers. We only care about bugs that
[00:50:24.720 --> 00:50:33.520]   manifest themselves often enough to be annoying, and we're willing to take the occasional crash
[00:50:33.520 --> 00:50:44.960]   or outage or incorrect result for granted, because we can't possibly, we don't have enough
[00:50:44.960 --> 00:50:50.400]   programmers to make all the code bug-free, and it would be an incredibly tedious business, and if you
[00:50:50.400 --> 00:50:58.720]   try to throw formal methods at it, it gets, it becomes even more tedious. So, every once in a
[00:50:58.720 --> 00:51:06.720]   while, the user clicks on a link in, and somehow they get an error, and the average user doesn't
[00:51:06.720 --> 00:51:15.040]   panic. They just click again and see if it works better the second time, which often magically it
[00:51:15.040 --> 00:51:24.240]   does, or they go up and they try some other way of performing their task. So, that's sort of an
[00:51:24.240 --> 00:51:33.440]   end-to-end recovery mechanism, and inside systems, there is all sorts of retries and timeouts and
[00:51:33.440 --> 00:51:41.520]   fallbacks, and I imagine that that sort of biological systems are even more full of that,
[00:51:41.520 --> 00:51:50.240]   because otherwise they wouldn't survive. Do you think programming should be taught and thought of
[00:51:50.240 --> 00:51:59.760]   as exactly what you just said? I come from this kind of, you're always denying that fact, always.
[00:51:59.760 --> 00:52:05.680]   In sort of basic programming education,
[00:52:05.680 --> 00:52:15.520]   the sort of, the programs you're having students write are so small and simple,
[00:52:17.040 --> 00:52:25.040]   that if there is a bug, you can always find it and fix it, because the sort of programming as
[00:52:25.040 --> 00:52:32.720]   it's being taught in some, even elementary, middle schools, in high school, introduction
[00:52:32.720 --> 00:52:40.480]   to programming classes in college, typically, it's programming in the small. Very few classes
[00:52:41.200 --> 00:52:45.440]   sort of actually teach software engineering, building large systems. I mean,
[00:52:45.440 --> 00:52:53.680]   every summer here at Dropbox, we have a large number of interns, every tech company on the
[00:52:53.680 --> 00:53:01.520]   West Coast has the same thing. These interns are always amazed, because this is the first time in
[00:53:01.520 --> 00:53:09.040]   their life that they see what goes on in a really large software development environment.
[00:53:10.480 --> 00:53:20.320]   And everything they've learned in college was almost always about a much smaller scale. And
[00:53:20.320 --> 00:53:27.360]   somehow that difference in scale makes a qualitative difference in how you do things
[00:53:27.360 --> 00:53:35.360]   and how you think about it. If you then take a few steps back into decades, 70s and 80s,
[00:53:35.360 --> 00:53:39.040]   when you're first thinking about Python, or just that world of programming languages,
[00:53:40.000 --> 00:53:45.680]   did you ever think that there would be systems as large as underlying Google, Facebook and Dropbox?
[00:53:45.680 --> 00:53:51.360]   When you were thinking about Python...
[00:53:51.360 --> 00:53:53.920]   I was actually always caught by surprise by...
[00:53:53.920 --> 00:53:58.320]   Yeah, pretty much every stage of computing.
[00:53:58.320 --> 00:54:06.800]   So maybe just because you've spoken in other interviews, but I think the evolution of
[00:54:06.800 --> 00:54:12.640]   programming languages are fascinating. And it's especially because it leads from my perspective
[00:54:12.640 --> 00:54:17.440]   towards greater and greater degrees of intelligence. I learned the first programming
[00:54:17.440 --> 00:54:27.280]   language I played with in Russia was with the turtle logo. And if you look, I just have a list
[00:54:27.280 --> 00:54:31.920]   of programming languages, all of which I've played with a little bit. And they're all beautiful in
[00:54:31.920 --> 00:54:41.120]   different ways from Fortran, Cobol, Lisp, Algol 60, Basic, Logo again, C. There's a few...
[00:54:41.120 --> 00:54:48.080]   The object-oriented came along in the 60s, Simula, Pascal, Smalltalk. All of that leads...
[00:54:48.080 --> 00:54:49.200]   And all the classics.
[00:54:49.200 --> 00:54:55.680]   The classics, yeah. The classic hits, right? Scheme, that's built on top of Lisp.
[00:54:58.080 --> 00:55:03.680]   On the database side, SQL, C++, and all of that leads up to Python. Pascal too,
[00:55:03.680 --> 00:55:10.720]   that's before Python. MATLAB. These kind of different communities, different languages.
[00:55:10.720 --> 00:55:17.440]   So can you talk about that world? I know that Python came out of ABC, which I actually never
[00:55:17.440 --> 00:55:22.720]   knew that language. I just... Having researched this conversation, went back to ABC, and it looks
[00:55:22.720 --> 00:55:29.760]   remarkably... It has a lot of annoying qualities, but underneath those, like all caps and so on,
[00:55:29.760 --> 00:55:34.880]   but underneath that, there's elements of Python that are quite... They're already there.
[00:55:34.880 --> 00:55:37.360]   That's where I got all the good stuff.
[00:55:37.360 --> 00:55:40.880]   All the good stuff. But in that world, you're swimming in these programming languages. Were
[00:55:40.880 --> 00:55:46.480]   you focused on just the good stuff in your specific circle? Or did you have a sense of
[00:55:47.360 --> 00:55:53.920]   what is everyone chasing? You said that every programming language is built to scratch an itch.
[00:55:53.920 --> 00:56:03.680]   Were you aware of all the itches in the community? And if not, or if yes, what itch were you trying
[00:56:03.680 --> 00:56:10.880]   to scratch with Python? Well, I'm glad I wasn't aware of all the itches, because I would probably
[00:56:10.880 --> 00:56:17.040]   not have been able to do anything. I mean, if you're trying to solve every problem at once...
[00:56:17.040 --> 00:56:18.880]   You'll solve nothing.
[00:56:18.880 --> 00:56:29.680]   Well, yeah, it's too overwhelming. And so I had a very, very focused problem. I wanted a programming
[00:56:29.680 --> 00:56:43.280]   language that sat somewhere in between shell scripting and C. And now, arguably, there is
[00:56:43.280 --> 00:56:54.240]   one is higher level, one is lower level. And Python is sort of a language of an intermediate
[00:56:54.240 --> 00:57:04.000]   level, although it's still pretty much at the high level. And I was thinking much more about
[00:57:04.000 --> 00:57:14.160]   I want a tool that I can use to be more productive as a programmer in a very specific environment.
[00:57:14.160 --> 00:57:23.680]   And I also had given myself a time budget for the development of the tool. And that was sort of
[00:57:23.680 --> 00:57:30.400]   about three months for both the design, like thinking through what are all the features of
[00:57:30.400 --> 00:57:40.480]   the language syntactically, and semantically, and how do I implement the whole pipeline from
[00:57:40.480 --> 00:57:48.560]   parsing the source code to executing it. So I think both with the timeline and the goals,
[00:57:49.200 --> 00:57:56.960]   it seems like productivity was at the core of it as a goal. So like for me in the 90s,
[00:57:56.960 --> 00:58:02.400]   and the first decade of the 21st century, I was always doing machine learning AI.
[00:58:02.400 --> 00:58:11.200]   Programming for my research was always in C++. And then the other people who are a little more
[00:58:11.200 --> 00:58:17.360]   mechanical engineering, electrical engineering, are MATLAB-y. They're a little bit more MATLAB
[00:58:17.360 --> 00:58:23.840]   focused. Those are the world and maybe a little bit Java too. But people who are more interested in
[00:58:23.840 --> 00:58:32.240]   emphasizing the object-oriented nature of things. So but then in the last 10 years or so,
[00:58:32.240 --> 00:58:36.880]   especially with the oncoming of neural networks, and these packages are built on Python
[00:58:36.880 --> 00:58:45.040]   to interface with neural networks, I switched to Python. And it's just, I've noticed a significant
[00:58:45.040 --> 00:58:49.600]   boost that I can't exactly, because I don't think about it, but I can't exactly put into words why
[00:58:49.600 --> 00:58:55.360]   I'm just much, much more productive. Just being able to get the job done much, much faster.
[00:58:55.360 --> 00:59:01.680]   So how do you think, whatever that qualitative difference is, I don't know if it's quantitative,
[00:59:01.680 --> 00:59:07.520]   it could be just a feeling. I don't know if I'm actually more productive. But how do you think?
[00:59:07.520 --> 00:59:08.160]   You probably are.
[00:59:08.160 --> 00:59:14.400]   Yeah, well, that's right. I think there's elements, let me just speak to one aspect,
[00:59:14.400 --> 00:59:23.920]   I think those affecting my productivity is C++ was, I really enjoyed creating performant code
[00:59:23.920 --> 00:59:29.840]   and creating a beautiful structure, where everything that you know, this kind of going
[00:59:29.840 --> 00:59:34.560]   into this, especially with the newer and newer standards of templated programming of just really
[00:59:34.560 --> 00:59:41.040]   creating this beautiful, formal structure that I found myself spending most of my time doing
[00:59:41.040 --> 00:59:45.840]   that as opposed to getting it, parsing a file and extracting a few keywords or whatever the
[00:59:45.840 --> 00:59:51.600]   task was trying to do. So what is it about Python? How do you think of productivity in general,
[00:59:51.600 --> 00:59:56.880]   as you were designing it? Now, sort of through the decades, last three decades, what do you
[00:59:56.880 --> 01:00:02.000]   think it means to be a productive programmer? And how did you try to design it into the language?
[01:00:02.000 --> 01:00:10.320]   There are different tasks. And as a programmer, it's it's useful to have different tools available
[01:00:10.320 --> 01:00:17.760]   that sort of are suitable for different tasks. So I still write C code, I still write shell code.
[01:00:17.760 --> 01:00:22.160]   But I write most of my things in Python.
[01:00:22.160 --> 01:00:30.960]   Why do I still use those other languages, because sometimes the task just demands it.
[01:00:30.960 --> 01:00:38.560]   And, well, I would say, most of the time, the task actually demands a certain language,
[01:00:38.560 --> 01:00:44.640]   because the task is not write a program that solves problem X from scratch. But it's more like
[01:00:44.640 --> 01:00:52.400]   fix a bug in existing program X or add a small feature to an existing large program.
[01:00:52.400 --> 01:01:07.040]   But even if you're not constrained in your choice of language by context like that,
[01:01:07.680 --> 01:01:14.400]   there is still the fact that if you write it in a certain language, then you sort of
[01:01:14.400 --> 01:01:25.440]   you have this balance between how long does it take you to write the code,
[01:01:25.440 --> 01:01:33.520]   and how long does the code run. And when you're in sort of
[01:01:35.120 --> 01:01:44.400]   in the phase of exploring solutions, you often spend much more time writing the code than running
[01:01:44.400 --> 01:01:51.440]   it, because every time you've sort of you've run it, you see that the output is not quite
[01:01:51.440 --> 01:02:01.200]   what you wanted, and you spend some more time coding. And a language like Python just
[01:02:02.080 --> 01:02:09.760]   makes that iteration much faster, because there are fewer details, there is a large library,
[01:02:09.760 --> 01:02:17.520]   sort of there are fewer details that you have to get right before your program compiles and runs.
[01:02:17.520 --> 01:02:24.320]   There are libraries that do all sorts of stuff for you. So you can sort of very quickly
[01:02:26.000 --> 01:02:35.680]   take a bunch of existing components, put them together and get your prototype application
[01:02:35.680 --> 01:02:41.760]   running. Just like when I was building electronics, I was using a breadboard most of the time.
[01:02:41.760 --> 01:02:51.200]   So I had this like sprawled out circuit that if you shook it, it would stop working because it
[01:02:51.200 --> 01:02:58.880]   was not put together very well. But it functioned, and all I wanted was to see that it worked and
[01:02:58.880 --> 01:03:06.320]   then move on to the next schematic or design or add something to it. Once you've sort of figured
[01:03:06.320 --> 01:03:12.800]   out, oh, this is the perfect design for my radio or light sensor or whatever, then you can say,
[01:03:12.800 --> 01:03:20.160]   okay, how do we design a PCB for this? How do we solder the components in a small space? How do we
[01:03:20.160 --> 01:03:32.720]   make it so that it is robust against, say, voltage fluctuations or mechanical disruption? I mean,
[01:03:32.720 --> 01:03:37.760]   I know nothing about that when it comes to designing electronics, but I know a lot about that
[01:03:37.760 --> 01:03:45.200]   when it comes to writing code. So the initial steps are efficient, fast, and there's not much
[01:03:45.200 --> 01:03:54.000]   stuff that gets in the way. But you're kind of describing from like Darwin described the evolution
[01:03:54.000 --> 01:04:01.920]   of species, right? You're observing of what is true about Python. Now, if you take a step back,
[01:04:01.920 --> 01:04:08.880]   if the art of, if the act of creating languages is art, and you had three months to do it,
[01:04:08.880 --> 01:04:16.400]   initial steps, so you just specified a bunch of goals, sort of things that you observe about
[01:04:16.400 --> 01:04:21.360]   Python, perhaps you had those goals, but how do you create the rules, the syntactic structure,
[01:04:21.360 --> 01:04:28.560]   the features that result in those? So I have, in the beginning, and I have follow up questions
[01:04:28.560 --> 01:04:33.520]   about through the evolution of Python, too. But in the very beginning, when you're sitting there,
[01:04:33.520 --> 01:04:36.160]   creating the lexical analyzer, whatever.
[01:04:36.160 --> 01:04:45.600]   Evolution was still a big part of it, because I sort of, I said to myself,
[01:04:45.600 --> 01:04:53.040]   I don't want to have to design everything from scratch. I'm going to borrow features from other
[01:04:53.040 --> 01:04:54.160]   languages that I like.
[01:04:54.160 --> 01:04:57.760]   Oh, interesting. So you basically, exactly, you first observe what you like.
[01:04:57.760 --> 01:05:04.800]   Yeah. And so that's why if you're 17 years old, and you want to sort of create a programming
[01:05:04.800 --> 01:05:11.760]   language, you're not going to be very successful at it. Because you have no experience with other
[01:05:11.760 --> 01:05:25.120]   languages. Whereas I was in my, let's say, mid 30s. I had written parsers before, so I had worked
[01:05:25.120 --> 01:05:31.920]   on the implementation of ABC, I had spent years debating the design of ABC with its authors,
[01:05:33.280 --> 01:05:36.560]   with its designers, I had nothing to do with the design, it was designed
[01:05:36.560 --> 01:05:45.200]   fully as it was ended up being implemented when I joined the team. But so you borrow
[01:05:45.200 --> 01:05:52.000]   ideas and concepts and very concrete sort of local rules from different languages,
[01:05:52.000 --> 01:06:00.240]   like the indentation and certain other syntactic features from ABC. But I chose to borrow string
[01:06:00.240 --> 01:06:07.760]   literals and how numbers work from C and various other things.
[01:06:07.760 --> 01:06:13.760]   So in then, if you take that further, so yet, you've had this funny sounding, but I think
[01:06:13.760 --> 01:06:21.280]   surprisingly accurate, or at least practical title of benevolent dictator for life for quite,
[01:06:21.280 --> 01:06:25.760]   you know, for the last three decades or whatever, or no, not the actual title, but functionally
[01:06:25.760 --> 01:06:36.800]   speaking. So you had to make decisions, design decisions. Can you maybe let's take Python 2,
[01:06:36.800 --> 01:06:45.440]   releasing Python 3, as an example. It's not backward compatible to Python 2 in ways that
[01:06:45.440 --> 01:06:50.880]   a lot of people know. So what was that deliberation, discussion, decision like?
[01:06:50.880 --> 01:06:56.800]   Yeah, what was the psychology of that experience? Do you regret any aspects of how that experience
[01:06:56.800 --> 01:06:57.360]   undergone?
[01:06:57.360 --> 01:07:07.760]   That, yeah, so it was a group process, really, at that point, even though I was BDFL in name,
[01:07:07.760 --> 01:07:18.000]   and certainly everybody sort of respected my position as the creator and the current
[01:07:18.000 --> 01:07:24.880]   sort of owner of the language design. I was looking at everyone else for feedback.
[01:07:24.880 --> 01:07:33.120]   Sort of Python 3.0, in some sense, was sparked by other people in the community,
[01:07:33.120 --> 01:07:45.040]   pointing out, oh, well, there are a few issues that sort of bite users over and over.
[01:07:46.080 --> 01:07:52.560]   Can we do something about that? And for Python 3, we took a number of those
[01:07:52.560 --> 01:07:59.600]   Python wards, as they were called at the time, and we said, can we try to sort of make
[01:07:59.600 --> 01:08:09.440]   small changes to the language that address those wards? And we had sort of, in the past, we had
[01:08:09.440 --> 01:08:17.760]   always taken backwards compatibility very seriously. And so many Python wards in earlier
[01:08:17.760 --> 01:08:23.600]   versions had already been resolved, because they could be resolved while maintaining backwards
[01:08:23.600 --> 01:08:31.280]   compatibility, or sort of using a very gradual path of evolution of the language in a certain
[01:08:31.280 --> 01:08:38.400]   area. And so we were stuck with a number of wards that were widely recognized as problems,
[01:08:39.360 --> 01:08:45.360]   not like roadblocks, but nevertheless, sort of things that some people trip over. And you know
[01:08:45.360 --> 01:08:53.840]   that's always the same thing that people trip over when they trip. And we could not think of
[01:08:53.840 --> 01:09:01.040]   a backwards compatible way of resolving those issues. But it's still an option to not resolve
[01:09:01.040 --> 01:09:07.280]   the issues. And so, yes, for a long time, we had sort of resigned ourselves to, well, okay,
[01:09:07.280 --> 01:09:14.960]   the language is not going to be perfect in this way, and that way, and that way. And we sort of,
[01:09:14.960 --> 01:09:20.400]   certain of these, I mean, there are still plenty of things where you can say, well, that's,
[01:09:20.400 --> 01:09:28.640]   that particular detail is better in Java, or in R, or in Visual Basic, or whatever.
[01:09:32.320 --> 01:09:39.440]   And we're okay with that, because, well, we can't easily change it. It's not too bad, we can do a
[01:09:39.440 --> 01:09:47.760]   little bit with user education, or we can have a static analyzer, or warnings in the parser,
[01:09:47.760 --> 01:09:54.160]   or something. But there were things where we thought, well, these are really problems that
[01:09:54.160 --> 01:10:02.160]   are not going away, they are getting worse. In the future, we should do something about
[01:10:02.160 --> 01:10:02.660]   that.
[01:10:02.660 --> 01:10:04.960]   - But ultimately, there is a decision to be made, right?
[01:10:04.960 --> 01:10:05.460]   - Yes.
[01:10:05.460 --> 01:10:11.840]   - So, was that the toughest decision in the history of Python you had to make,
[01:10:11.840 --> 01:10:18.560]   as the benevolent dictator for life? Or if not, what are other, maybe even on a smaller scale,
[01:10:18.560 --> 01:10:21.840]   what was the decision where you were really torn up about?
[01:10:21.840 --> 01:10:25.280]   - Well, the toughest decision was probably to resign.
[01:10:25.280 --> 01:10:31.280]   - All right, let's go there. Hold on a second, then. Let me just, because in the interest of
[01:10:31.280 --> 01:10:36.080]   time, too, because I have a few cool questions for you. Let's touch a really important one,
[01:10:36.080 --> 01:10:41.200]   because it was quite dramatic and beautiful in certain kinds of ways. In July this year,
[01:10:41.200 --> 01:10:48.480]   three months ago, you wrote, "Now that PEP 572 is done, I don't ever want to have to fight so hard
[01:10:48.480 --> 01:10:54.320]   for a PEP and find that so many people despise my decisions. I would like to remove myself entirely
[01:10:54.320 --> 01:11:00.640]   from the decision process. I'll still be there for a while as an ordinary core developer, and I'll
[01:11:00.640 --> 01:11:06.560]   still be available to mentor people, possibly more available. But I'm basically giving myself
[01:11:06.560 --> 01:11:13.920]   a permanent vacation from being BDFL, benevolent dictator for life. And you all will be on your own."
[01:11:13.920 --> 01:11:14.720]   (laughter)
[01:11:14.720 --> 01:11:21.280]   First of all, it's almost Shakespearean. "I'm not going to appoint a successor,
[01:11:21.280 --> 01:11:28.400]   so what are you all going to do? Create a democracy, anarchy, a dictatorship, a federation?"
[01:11:29.040 --> 01:11:36.560]   So that was a very dramatic and beautiful set of statements. It's almost, it's open-ended nature,
[01:11:36.560 --> 01:11:42.240]   called the community to create a future for Python. There's this kind of beautiful aspect to it.
[01:11:42.240 --> 01:11:42.720]   - Wow.
[01:11:42.720 --> 01:11:49.040]   - So, and dramatic, you know. What was making that decision like? What was on your heart,
[01:11:49.040 --> 01:11:55.040]   on your mind, stepping back now, a few months later? Take me through your mind.
[01:11:55.040 --> 01:12:01.280]   - I'm glad you like the writing, because it was actually written pretty quickly.
[01:12:01.280 --> 01:12:12.320]   It was literally something like, after months and months of going around in circles,
[01:12:12.320 --> 01:12:24.480]   I had finally approved PEP 572, which I had a big hand in its design, although I didn't initiate
[01:12:25.360 --> 01:12:33.920]   it originally. I sort of gave it a bunch of nudges in a direction that would be better for
[01:12:33.920 --> 01:12:35.360]   the language. But so--
[01:12:35.360 --> 01:12:39.200]   - Sorry, just to ask, is async.io, that's the one, or no?
[01:12:39.200 --> 01:12:46.240]   - No, PEP 572 was actually a small feature, which is assignment expressions.
[01:12:46.240 --> 01:12:47.520]   - Oh, assignment expressions, okay.
[01:12:47.520 --> 01:12:55.840]   - That had been, there was just a lot of debate where a lot of people claimed that
[01:12:55.840 --> 01:13:04.240]   they knew what was Pythonic and what was not Pythonic, and they knew that this was going to
[01:13:04.240 --> 01:13:11.200]   destroy the language. This was like a violation of Python's most fundamental design philosophy.
[01:13:11.200 --> 01:13:16.720]   And I thought that was all bullshit, because I was in favor of it. And I would think I know
[01:13:16.720 --> 01:13:23.360]   something about Python's design philosophy. So I was really tired and also stressed of that thing.
[01:13:23.360 --> 01:13:31.360]   And literally, after sort of announcing I was going to accept it, a certain Wednesday evening,
[01:13:31.360 --> 01:13:36.720]   I had finally sent the email, it's accepted, now let's just go implement it.
[01:13:36.720 --> 01:13:45.680]   So I went to bed feeling really relieved, that's behind me. And I wake up Thursday morning,
[01:13:46.640 --> 01:13:58.800]   7am, and I think, well, that was the last one that's going to be such a terrible debate.
[01:13:58.800 --> 01:14:05.360]   And that's the last time that I let myself be so stressed out about a PEP decision.
[01:14:05.360 --> 01:14:12.880]   I should just resign. I've been sort of thinking about retirement for half a decade. I've been
[01:14:13.680 --> 01:14:20.560]   joking and sort of mentioning retirement, sort of telling the community,
[01:14:20.560 --> 01:14:28.400]   some point in the future, I'm going to retire. Don't take that FL part of my title too literally.
[01:14:28.400 --> 01:14:37.360]   And I thought, okay, this is it. I'm done. I had the day off. I wanted to have a good
[01:14:37.360 --> 01:14:45.680]   time with my wife. We were going to a little beach town nearby. And in, I think, maybe 15,
[01:14:45.680 --> 01:14:51.920]   20 minutes, I wrote that thing that you just called Shakespearean. And the funny thing is...
[01:14:51.920 --> 01:14:54.320]   I'm going to get so much crap for calling it Shakespearean.
[01:14:54.320 --> 01:15:02.720]   I didn't even realize what a monumental decision it was. Because five minutes later,
[01:15:03.440 --> 01:15:11.200]   I read a link to my message back on Twitter, where people were already discussing on Twitter,
[01:15:11.200 --> 01:15:20.560]   Guido resigned as the BDFL. And I had posted it on an internal forum that I thought was only read
[01:15:20.560 --> 01:15:26.400]   by core developers. So I thought I would at least have one day before news would sort of get out.
[01:15:28.320 --> 01:15:36.080]   The on your own aspect, I had also an element of quite... It was quite a powerful element
[01:15:36.080 --> 01:15:41.360]   of the uncertainty that lies ahead. But can you also just briefly talk about,
[01:15:41.360 --> 01:15:48.880]   like, for example, I play guitar as a hobby for fun. And whenever I play, people are super positive,
[01:15:48.880 --> 01:15:53.920]   super friendly. They're like, this is awesome. This is great. But sometimes I enter as an
[01:15:53.920 --> 01:15:59.920]   outside observer, I enter the programming community. And there seems to sometimes be
[01:15:59.920 --> 01:16:07.200]   camps on whatever the topic. And the two camps, the two or plus camps are often pretty harsh
[01:16:07.200 --> 01:16:13.840]   at criticizing the opposing camps. As an onlooker, I may be totally wrong on this.
[01:16:13.840 --> 01:16:19.840]   Yeah, holy wars are sort of a favorite activity in the programming community.
[01:16:19.840 --> 01:16:24.960]   And what is the psychology behind that? Is that okay for a healthy community to have?
[01:16:24.960 --> 01:16:29.120]   Is that a productive force ultimately for the evolution of a language?
[01:16:29.120 --> 01:16:35.920]   Well, if everybody is patting each other on the back and never telling the truth,
[01:16:35.920 --> 01:16:45.200]   it would not be a good thing. I think there is a middle ground where sort of
[01:16:48.720 --> 01:16:57.040]   being nasty to each other is not okay. But there is a middle ground where there is
[01:16:57.040 --> 01:17:02.880]   healthy ongoing criticism and feedback that is very productive.
[01:17:02.880 --> 01:17:11.760]   And you mean at every level you see that, I mean, someone proposes to fix a very small
[01:17:12.400 --> 01:17:21.520]   issue in a code base, chances are that some reviewer will sort of respond by saying,
[01:17:21.520 --> 01:17:24.080]   well, actually, you can do it better the other way.
[01:17:24.080 --> 01:17:32.960]   When it comes to deciding on the future of the Python core developer community,
[01:17:32.960 --> 01:17:38.800]   we now have, I think, five or six competing proposals for a constitution.
[01:17:40.960 --> 01:17:46.400]   So that future, do you have a fear of that future? Do you have a hope for that future?
[01:17:46.400 --> 01:17:54.880]   I'm very confident about that future. And by and large, I think that the debate has been very
[01:17:54.880 --> 01:18:06.560]   healthy and productive. And I actually, when I wrote that resignation email, I knew that Python
[01:18:06.560 --> 01:18:13.840]   was in a very good spot and that the Python core development community, the group of 50 or 100
[01:18:13.840 --> 01:18:21.200]   people who sort of write or review most of the code that goes into Python, those people
[01:18:21.200 --> 01:18:31.200]   get along very well most of the time. A large number of different areas of expertise are
[01:18:32.000 --> 01:18:42.800]   represented, different levels of experience in the Python core dev community, different levels
[01:18:42.800 --> 01:18:49.680]   of experience completely outside it in software development in general, large systems, small
[01:18:49.680 --> 01:19:01.200]   systems, embedded systems. So I felt okay resigning because I knew that the community can really take
[01:19:01.200 --> 01:19:09.200]   care of itself. And out of a grab bag of future feature developments, let me ask if you can
[01:19:09.200 --> 01:19:17.360]   comment maybe on all very quickly. Concurrent programming, parallel computing, async IO.
[01:19:17.360 --> 01:19:25.440]   These are things that people have expressed hope, complained about, whatever have discussed on
[01:19:25.440 --> 01:19:31.840]   Reddit. Async IO, so the parallelization in general. Packaging, I was totally clueless on
[01:19:31.840 --> 01:19:37.120]   this. I just use pip to install stuff, but apparently there's pip and poetry, there's these
[01:19:37.120 --> 01:19:41.680]   dependency packaging systems that manage dependencies and so on. They're emerging
[01:19:41.680 --> 01:19:48.560]   and there's a lot of confusion about what's the right thing to use. Then also functional programming.
[01:19:54.080 --> 01:19:57.840]   Are we going to get more functional programming or not? This kind of idea.
[01:19:57.840 --> 01:20:06.960]   And of course the GIL connected to the parallelization, I suppose, the global
[01:20:06.960 --> 01:20:12.320]   interpreter lock problem. Can you just comment on whichever you want to comment on?
[01:20:12.320 --> 01:20:22.640]   Well, let's take the GIL and parallelization and async IO as one topic.
[01:20:23.200 --> 01:20:35.200]   I'm not that hopeful that Python will develop into a high concurrency, high parallelism language.
[01:20:35.200 --> 01:20:44.240]   That the way the language is designed, the way most users use the language, the way the language
[01:20:44.240 --> 01:20:48.560]   is implemented, all make that a pretty unlikely future.
[01:20:48.560 --> 01:20:57.600]   So you think it might not even need to, really, the way people use it. It might not be something that should be of great concern.
[01:20:57.600 --> 01:21:06.000]   I think async IO is a special case because it allows overlapping IO and only IO.
[01:21:06.000 --> 01:21:15.600]   And that is a best practice of supporting very high throughput IO,
[01:21:16.640 --> 01:21:25.040]   many connections per second. I'm not worried about that. I think async IO will evolve.
[01:21:25.040 --> 01:21:31.680]   There are a couple of competing packages. We have some very smart people who are sort of pushing us
[01:21:31.680 --> 01:21:42.080]   to make async IO better. Parallel computing, I think that Python is not the language for that.
[01:21:43.200 --> 01:21:49.760]   There are ways to work around it, but you can't expect to write
[01:21:49.760 --> 01:21:57.120]   an algorithm in Python and have a compiler automatically parallelize that.
[01:21:57.120 --> 01:22:04.320]   What you can do is use a package like NumPy and there are a bunch of other very powerful packages
[01:22:04.320 --> 01:22:12.160]   that use all the CPUs available because you tell the package, here's the data,
[01:22:12.160 --> 01:22:19.200]   here's the abstract operation to apply over it, go at it, and then we're back in the C++ world.
[01:22:19.200 --> 01:22:23.440]   But those packages are themselves implemented usually in C++.
[01:22:23.440 --> 01:22:26.960]   That's right. That's where TensorFlow and all these packages come in, where they parallelize
[01:22:26.960 --> 01:22:34.880]   across GPUs, for example. They take care of that for you. So in terms of packaging, can you comment on the future of packaging in Python?
[01:22:34.880 --> 01:22:44.800]   Packaging has always been my least favorite topic. It's a really tough problem because
[01:22:44.800 --> 01:22:51.200]   the OS and the platform want to own packaging,
[01:22:51.200 --> 01:23:02.080]   but their packaging solution is not specific to a language. If you take Linux, there are
[01:23:02.080 --> 01:23:11.680]   two competing packaging solutions for Linux or for Unix in general, but they all work across
[01:23:11.680 --> 01:23:23.200]   all languages. Several languages like Node, JavaScript, Ruby, and Python all have their
[01:23:23.200 --> 01:23:27.840]   own packaging solutions that only work within the ecosystem of that language.
[01:23:28.800 --> 01:23:38.240]   Well, what should you use? That is a tough problem. My own approach is I use the system
[01:23:38.240 --> 01:23:46.320]   packaging system to install Python and I use the Python packaging system then to install
[01:23:46.320 --> 01:23:53.360]   third-party Python packages. That's what most people do. Ten years ago, Python packaging was
[01:23:53.360 --> 01:24:01.920]   really a terrible situation. Nowadays, PIP is the future. There is a separate ecosystem for
[01:24:01.920 --> 01:24:09.680]   a numerical and scientific Python based on Anaconda. Those two can live together.
[01:24:09.680 --> 01:24:14.640]   I don't think there is a need for more than that. So that's packaging.
[01:24:14.640 --> 01:24:20.240]   Well, at least for me, that's where I've been extremely happy. I didn't even know this
[01:24:20.240 --> 01:24:25.760]   was an issue until it was brought up. In the interest of time, let me skip through a million
[01:24:25.760 --> 01:24:32.480]   other questions I have. I watched a five and a half hour oral history that you've done with the
[01:24:32.480 --> 01:24:37.680]   Computer History Museum. The nice thing about it, it gave this, because of the linear progression of
[01:24:37.680 --> 01:24:45.040]   the interview, it gave this feeling of a life, a life well-lived with interesting things in it.
[01:24:48.320 --> 01:24:52.960]   I would say a good spend of this little existence we have on earth.
[01:24:52.960 --> 01:24:59.920]   So outside of your family, looking back, what about this journey are you really proud of?
[01:24:59.920 --> 01:25:10.480]   Are there moments that stand out, accomplishments, ideas? Is it the creation of Python itself that
[01:25:10.480 --> 01:25:15.040]   stands out as a thing that you look back and say, "Damn, I did pretty good there"?
[01:25:17.680 --> 01:25:21.760]   Well, I would say that Python is definitely the best thing I've ever done.
[01:25:21.760 --> 01:25:34.240]   And I wouldn't sort of say just the creation of Python, but the way I sort of raised Python, like
[01:25:34.240 --> 01:25:41.440]   a baby. I didn't just conceive a child, but I raised a child. And now I'm setting the child
[01:25:41.440 --> 01:25:48.800]   free in the world. And I've set up the child to sort of be able to take care of himself.
[01:25:48.800 --> 01:25:51.120]   And I'm very proud of that.
[01:25:51.120 --> 01:25:56.720]   And as the announcer of Monty Python's Flying Circus used to say, and now for something
[01:25:56.720 --> 01:26:02.320]   completely different, do you have a favorite Monty Python moment or a moment in Hitchhiker's
[01:26:02.320 --> 01:26:06.080]   Guide or any other literature show or movie that cracks you up when you think about it?
[01:26:06.080 --> 01:26:11.200]   Oh, you can always play me the dead parrot sketch.
[01:26:11.200 --> 01:26:12.400]   Oh, that's brilliant.
[01:26:12.400 --> 01:26:14.400]   That's my favorite as well.
[01:26:14.400 --> 01:26:15.680]   It's pushing up the daisies.
[01:26:15.680 --> 01:26:20.240]   Okay, Greta, thank you so much for talking to me today.
[01:26:20.240 --> 01:26:23.920]   Lex, this has been a great conversation.
[01:26:24.160 --> 01:26:24.740]   Thank you.
[01:26:25.700 --> 01:26:26.280]   Thank you.
[01:26:27.160 --> 01:26:27.240]   [END]
[01:26:27.240 --> 01:26:30.260]   (No audio)
[01:26:30.260 --> 01:26:33.260]   (No audio)
[01:26:33.260 --> 01:26:36.260]   (No audio)
[01:26:36.260 --> 01:26:39.260]   (No audio)
[01:26:39.260 --> 01:26:49.260]   [BLANK_AUDIO]

