
[00:00:00.000 --> 00:00:04.200]   Just like buses, on the same day that we got GPT-40 Image Gen
[00:00:04.200 --> 00:00:10.320]   and a new DeepSeq V3, we also got Gemini 2.5 Pro.
[00:00:10.320 --> 00:00:14.380]   There is no Ultra or Nano version, but we're still calling it Pro.
[00:00:14.380 --> 00:00:20.280]   But it is out, and some at Google are claiming that it is the best AI language model out there.
[00:00:20.280 --> 00:00:22.280]   I've been testing it out a fair bit myself,
[00:00:22.280 --> 00:00:25.620]   and yes, this is normally where I'd say, and I've read the paper,
[00:00:25.900 --> 00:00:32.160]   but unlike the new open-weight DeepSeq V3, Gemini 2.5's secret source is being kept secret.
[00:00:32.160 --> 00:00:36.760]   Beyond the benchmarks, though, there is a bigger story here of wider interest.
[00:00:36.760 --> 00:00:42.140]   Just recently, the CEO of Microsoft has claimed that models are being commoditized,
[00:00:42.140 --> 00:00:45.220]   with greater performance bought, like commodities,
[00:00:45.220 --> 00:00:50.680]   and that labs like OpenAI are merely, quote, product companies, selling and experience.
[00:00:50.920 --> 00:00:56.360]   He's saying that they no more have the secret to, quote, AGI than anyone else.
[00:00:56.360 --> 00:01:01.120]   Let's set aside for a moment the fact that the new GPT-40 Image Gen from OpenAI
[00:01:01.120 --> 00:01:04.780]   is very much a model, as well as a product.
[00:01:04.780 --> 00:01:06.740]   Is the bigger point correct?
[00:01:06.740 --> 00:01:13.280]   Does today's Gemini 2.5 and DeepSeq V3 news prove that there is no secret to AI anymore,
[00:01:13.280 --> 00:01:14.960]   at least when it comes to intelligence?
[00:01:15.400 --> 00:01:19.740]   To answer that, let's start with the brand new Gemini 2.5,
[00:01:19.740 --> 00:01:22.420]   Google's, quote, most intelligent AI model.
[00:01:22.420 --> 00:01:24.980]   Notice that this title is a little more humble,
[00:01:24.980 --> 00:01:27.320]   saying it's their most intelligent model,
[00:01:27.320 --> 00:01:30.340]   rather than flat out the smartest model around.
[00:01:30.340 --> 00:01:32.300]   I'll be honest, I didn't see this model coming,
[00:01:32.300 --> 00:01:34.400]   and I don't think too many people did either,
[00:01:34.400 --> 00:01:37.500]   coming so soon after Gemini 2, but here it is.
[00:01:37.500 --> 00:01:40.540]   All the benchmark figures can be a little bit overwhelming,
[00:01:40.540 --> 00:01:43.740]   so I'm going to try and help you understand what some of them mean.
[00:01:43.860 --> 00:01:47.900]   The somewhat amusingly misconceived title of Humanity's Last Exam
[00:01:47.900 --> 00:01:50.400]   is an incredibly knowledge-intensive benchmark.
[00:01:50.400 --> 00:01:53.860]   Of course, you do need to reason and calculate for some of the questions,
[00:01:53.860 --> 00:01:57.620]   but most of all, it's testing incredibly obscure trivia,
[00:01:57.620 --> 00:02:02.360]   difficult Latin translations, and abstruse butterfly physiology.
[00:02:02.360 --> 00:02:07.700]   But credit where credit is due, Gemini 2.5 seems to know the most.
[00:02:07.700 --> 00:02:12.480]   One thing to bear in mind, of course, is that the full O3 hasn't been released yet.
[00:02:12.480 --> 00:02:14.600]   This is just O3 Mini from OpenAI,
[00:02:14.600 --> 00:02:19.540]   so one would suspect that the full O3 will score more highly than this.
[00:02:19.540 --> 00:02:22.340]   One hint at that is that the deep research system,
[00:02:22.340 --> 00:02:26.420]   albeit using tools, gets around 27% on this benchmark.
[00:02:26.420 --> 00:02:28.320]   For knowledge without searching the web,
[00:02:28.320 --> 00:02:31.280]   Gemini 2.5 Pro knows the most, you could say.
[00:02:31.280 --> 00:02:33.620]   What about incredibly difficult science questions?
[00:02:33.620 --> 00:02:35.920]   Google-proof PhD-level questions.
[00:02:36.260 --> 00:02:39.060]   Again, on that front, notice the convergence,
[00:02:39.060 --> 00:02:43.640]   with 2.5 Pro being more or less level with Claude 3.7 Sonnet,
[00:02:43.640 --> 00:02:46.020]   just a shade under when it uses extended thinking,
[00:02:46.020 --> 00:02:48.060]   and likewise for Quark 3.
[00:02:48.060 --> 00:02:49.620]   Now, if you want to be harsh,
[00:02:49.620 --> 00:02:52.620]   you could note the fact that they didn't include O3,
[00:02:52.620 --> 00:02:56.180]   even though we know the score of that model using majority voting,
[00:02:56.180 --> 00:02:58.140]   which is 87.7%.
[00:02:58.140 --> 00:03:00.620]   But Google slightly call out OpenAI there,
[00:03:00.620 --> 00:03:03.920]   saying our scores are without things like majority voting,
[00:03:03.920 --> 00:03:05.940]   which expend even more compute.
[00:03:05.940 --> 00:03:07.400]   But in a way, I've got to say,
[00:03:07.400 --> 00:03:09.420]   that's kind of the point of this video,
[00:03:09.420 --> 00:03:11.240]   which is this word leads.
[00:03:11.240 --> 00:03:13.440]   Does anyone really lead anymore?
[00:03:13.440 --> 00:03:15.340]   As someone who analyzes AI,
[00:03:15.340 --> 00:03:18.080]   making direct comparisons is increasingly,
[00:03:18.080 --> 00:03:20.720]   and you could say deliberately, made more difficult.
[00:03:21.060 --> 00:03:24.900]   Some companies like OpenAI use majority voting for their benchmark scores.
[00:03:24.900 --> 00:03:28.800]   Others just don't report the benchmarks in which they perform worse at.
[00:03:28.800 --> 00:03:31.420]   Some figures you see include the use of tools,
[00:03:31.420 --> 00:03:32.260]   others don't.
[00:03:32.260 --> 00:03:34.220]   But even with all of that said,
[00:03:34.220 --> 00:03:37.320]   the scores are slightly converging.
[00:03:37.320 --> 00:03:39.260]   For a given amount of compute,
[00:03:39.260 --> 00:03:43.840]   you get roughly around a certain level of performance in, say, mathematics,
[00:03:43.840 --> 00:03:46.000]   or science, or knowledge trivia,
[00:03:46.000 --> 00:03:47.260]   or even coding.
[00:03:47.260 --> 00:03:51.180]   That's not to say that anyone watching can't have a clear favorite model.
[00:03:51.180 --> 00:03:55.320]   I would argue probably O3 might be just about the best all around,
[00:03:55.320 --> 00:03:57.580]   and I use deep research all the time.
[00:03:57.580 --> 00:04:01.640]   Many people love the personality and writing style of Claude 3.7's Sonnet,
[00:04:01.640 --> 00:04:04.840]   and use it extensively for coding in things like Cursor.
[00:04:04.840 --> 00:04:06.340]   DeepSeek, and we'll come on to them,
[00:04:06.340 --> 00:04:10.060]   are absolutely pushing the limits when it comes to cost efficiency.
[00:04:10.060 --> 00:04:12.920]   The best bang for your buck, if that's your main criterion.
[00:04:12.920 --> 00:04:15.840]   But if you keep compute expenditures level,
[00:04:16.100 --> 00:04:18.960]   performance is really starting to converge.
[00:04:18.960 --> 00:04:21.900]   And that's the big revelation of tonight for me.
[00:04:21.900 --> 00:04:24.120]   It is very much worth pointing out
[00:04:24.120 --> 00:04:28.340]   that that does not preclude the fact that models are improving.
[00:04:28.340 --> 00:04:30.140]   It's just that they're improving together.
[00:04:30.140 --> 00:04:34.320]   I love the fact that models can now read tables and charts
[00:04:34.320 --> 00:04:37.540]   in, for example, the MMMU benchmark better than ever.
[00:04:37.540 --> 00:04:40.820]   In this case, Gemini 2.5 Pro is literally state-of-the-art.
[00:04:40.820 --> 00:04:47.240]   And its excellence in that particular category was reinforced by the Vista benchmark from Scale AI.
[00:04:47.240 --> 00:04:50.200]   Unlike the MMMU, which is multiple choice,
[00:04:50.200 --> 00:04:54.060]   the Vista benchmark gives the model a free-form response.
[00:04:54.060 --> 00:04:56.660]   And as makes sense for visual understanding,
[00:04:56.660 --> 00:05:00.480]   it tests things like, can you count how many things there are in an image?
[00:05:00.480 --> 00:05:01.500]   Can you use logic?
[00:05:01.500 --> 00:05:02.760]   Can you extract information?
[00:05:03.000 --> 00:05:10.220]   And not only is Gemini 2.5 Pro a huge step above even Claude 3.7 Sonnet in this capacity,
[00:05:10.220 --> 00:05:15.740]   it's actually the first model to get within touching distance of human performance in this benchmark.
[00:05:15.740 --> 00:05:20.900]   And those humans, by the way, were able to browse the web and take their time to answer.
[00:05:21.220 --> 00:05:23.360]   So that performance is pretty impressive.
[00:05:23.360 --> 00:05:26.940]   One quick benchmark I should include, even though it's slightly problematic,
[00:05:26.940 --> 00:05:30.180]   is the huge jump on the language model arena.
[00:05:30.180 --> 00:05:33.740]   Yes, it's community voted and it can be slightly gamed,
[00:05:33.740 --> 00:05:37.880]   but it is a huge delta now between number one and number two.
[00:05:37.880 --> 00:05:42.260]   The eagle-eyed among you, though, may be noticing one big red flag about this benchmark,
[00:05:42.260 --> 00:05:45.580]   which is, where on earth is Claude 3.7 Sonnet?
[00:05:45.580 --> 00:05:51.920]   I have got to admit that in one area, Gemini 2.5 Pro is just head and shoulders above other models,
[00:05:51.920 --> 00:05:53.820]   and that is in long context.
[00:05:53.820 --> 00:05:58.920]   It can handle a million tokens, that's roughly, let's say, 750,000 words,
[00:05:58.920 --> 00:06:03.040]   when the other models in this chart can't even handle a quarter of that.
[00:06:03.040 --> 00:06:06.440]   But I would still argue that with one or two exceptions,
[00:06:06.440 --> 00:06:11.000]   performance across the board is converging between diverse model families.
[00:06:11.000 --> 00:06:14.200]   It's currently free, by the way, in Google's AI Studio,
[00:06:14.460 --> 00:06:16.040]   but that, of course, won't last.
[00:06:16.040 --> 00:06:18.940]   And it is able to search, which is great,
[00:06:18.940 --> 00:06:22.260]   except that ChatGPT has been able to do that for quite a while,
[00:06:22.260 --> 00:06:24.440]   and so too will Claude very soon.
[00:06:24.440 --> 00:06:27.360]   Just quickly, how does it do on my benchmark, SimpleBench,
[00:06:27.360 --> 00:06:30.820]   a test of common sense reasoning, or some would say trick questions?
[00:06:30.820 --> 00:06:33.060]   We're hoping to sometime very late tonight.
[00:06:33.060 --> 00:06:34.180]   I mean, what is it now?
[00:06:34.180 --> 00:06:35.360]   Approaching 10.30.
[00:06:35.360 --> 00:06:37.700]   Hopefully get it done and on the leaderboard.
[00:06:37.700 --> 00:06:41.660]   But I couldn't resist testing it on the 10 public questions.
[00:06:42.040 --> 00:06:46.460]   Gemini 2 Pro, even Gemini 2 Thinking, only got 1 out of 10.
[00:06:46.460 --> 00:06:51.160]   Gemini 2.5 Pro, and you can test yourself on the questions if you like,
[00:06:51.160 --> 00:06:53.900]   gets 5 out of 10, which is a huge jump.
[00:06:53.900 --> 00:06:56.980]   Only thing is, that's what Claude 3.7 gets.
[00:06:56.980 --> 00:06:58.440]   And, oh, one.
[00:06:58.440 --> 00:07:00.620]   That's, again, kind of the point of the video.
[00:07:00.620 --> 00:07:02.760]   Not that progress isn't being made,
[00:07:02.760 --> 00:07:05.280]   and we can talk about that all day.
[00:07:05.480 --> 00:07:08.300]   More that model performance is converging.
[00:07:08.300 --> 00:07:13.160]   But now we must come to the new DeepSeq V3 announced this morning.
[00:07:13.160 --> 00:07:15.020]   And that's not the new reasoning model.
[00:07:15.020 --> 00:07:15.980]   That's not R2.
[00:07:15.980 --> 00:07:18.780]   It's a new base model, you can think of it, for a reasoning model.
[00:07:18.780 --> 00:07:22.660]   The best analogy is comparing it to GPT 4.5,
[00:07:22.660 --> 00:07:25.640]   which should be the base model for GPT-5,
[00:07:25.640 --> 00:07:27.180]   which will be a reasoning model.
[00:07:27.180 --> 00:07:31.920]   Just like GPT-4 O is kind of the base model behind the O series of models.
[00:07:31.920 --> 00:07:35.960]   And let me zoom into the chart, because this perhaps is the starkest evidence
[00:07:35.960 --> 00:07:38.980]   that performance is converging across teams.
[00:07:38.980 --> 00:07:42.180]   Today's DeepSeq V3 is in stripes,
[00:07:42.180 --> 00:07:46.180]   and that is what will surely be the base model for the R2 model
[00:07:46.180 --> 00:07:47.940]   coming probably in the next few weeks.
[00:07:47.940 --> 00:07:50.840]   Now, not only do I want you to focus on the improvement
[00:07:50.840 --> 00:07:55.140]   from the original DeepSeq V3, which is the underlying model of R1,
[00:07:55.140 --> 00:07:59.260]   but also focus on the comparison with GPT-4.5 from OpenAI.
[00:07:59.260 --> 00:08:02.680]   It seems starkly better in mathematics, as you can see,
[00:08:02.680 --> 00:08:04.460]   and arguably so in coding,
[00:08:04.460 --> 00:08:09.000]   just underperforming slightly for science questions and general knowledge.
[00:08:09.000 --> 00:08:15.060]   But remember, OpenAI was supposed to be 6 or 12 months or more ahead of Chinese companies.
[00:08:15.060 --> 00:08:20.840]   Now, the base models between the new DeepSeq V3 and GPT-4.5 are kind of on par.
[00:08:20.840 --> 00:08:26.420]   4.5, don't forget, is the model that Sam Altman said many people will feel the AGI for.
[00:08:26.420 --> 00:08:30.040]   Now, on my Patreon, I'm going to be imminently releasing a documentary
[00:08:30.040 --> 00:08:33.740]   on the behind-the-scenes of DeepSeq and Liang Wenfeng,
[00:08:33.740 --> 00:08:38.000]   but I doubt he would say he's going to feel the AGI on this new V3.
[00:08:38.000 --> 00:08:42.260]   And yes, of course I'm aware that unless they're backed by the Chinese state,
[00:08:42.260 --> 00:08:47.660]   and even then, DeepSeq might struggle to match the sheer compute of Anthropic or OpenAI.
[00:08:47.980 --> 00:08:53.180]   But nevertheless, as things currently stand, there is, for reasoning models, no clear moat.
[00:08:53.180 --> 00:08:58.440]   Before we finish then, two more cheeky bits of evidence for the commoditization of AI.
[00:08:58.440 --> 00:09:03.360]   Commoditization here being the argument that the only real differentiator at the moment
[00:09:03.360 --> 00:09:07.600]   is how much money you can funnel to getting more compute,
[00:09:07.600 --> 00:09:10.360]   which then increments your benchmark performance.
[00:09:10.360 --> 00:09:17.040]   Now, we have already talked about how Satya Nadella has said that directly AI models are getting commoditized.
[00:09:17.040 --> 00:09:19.520]   And as anyone who's watched this channel for a while,
[00:09:19.520 --> 00:09:23.040]   you will know that that's a huge vibe shift from two years ago
[00:09:23.040 --> 00:09:27.360]   when he was celebrating the fact that he had this special partnership with OpenAI.
[00:09:27.360 --> 00:09:31.960]   But a couple weeks back, we got this insider report in the information,
[00:09:31.960 --> 00:09:36.160]   which I couldn't find a chance to talk about on the channel, but it's really interesting.
[00:09:36.160 --> 00:09:39.700]   There is a unit within Microsoft called Microsoft AI,
[00:09:39.700 --> 00:09:45.420]   and it's run by this guy, Mustafa Suleiman, formerly the head of Inflection AI,
[00:09:45.420 --> 00:09:49.140]   and before that, the co-founder of Google DeepMind, along with Demes Azabis.
[00:09:49.140 --> 00:09:53.920]   As you might expect, the Microsoft AI unit is also trying to fashion AGI.
[00:09:53.920 --> 00:10:00.640]   But then, last September, they noticed a one, of course, from OpenAI leap ahead, and we all did.
[00:10:00.640 --> 00:10:07.480]   Mustafa Suleiman apparently called up and got very angry when OpenAI wouldn't tell him how they'd made it.
[00:10:07.480 --> 00:10:11.580]   He started to raise his voice at Mira Mirati, apparently.
[00:10:11.580 --> 00:10:14.800]   You're not holding up your end of the deal, he said.
[00:10:14.800 --> 00:10:16.600]   The call ended abruptly.
[00:10:16.600 --> 00:10:23.900]   Give us documentation, he demanded, about how you had programmed O1 to think about user queries before answering.
[00:10:23.900 --> 00:10:29.300]   Now, obviously, that's a somewhat juicy story of the failing relationship between Microsoft and OpenAI,
[00:10:29.300 --> 00:10:31.240]   but the real nugget comes later.
[00:10:31.240 --> 00:10:36.300]   According to Microsoft, at least, they have figured out how to do this kind of reasoning,
[00:10:36.300 --> 00:10:38.820]   like Gemini and R1 and the rest of it.
[00:10:38.820 --> 00:10:43.640]   Grok 3 and O3 and everyone seems to be reasoning now, but so are Microsoft, apparently.
[00:10:43.640 --> 00:10:53.120]   And they claim that their MAI models now perform nearly as well as leading models from OpenAI and Anthropic on benchmarks.
[00:10:53.120 --> 00:10:56.300]   Their models, of course, also do this thinking before answering.
[00:10:56.300 --> 00:11:01.040]   Is that why Satya Nadella so confidently said that models are being commoditized?
[00:11:01.040 --> 00:11:06.760]   If his own team is able to even partially replicate the performance of, say, O3,
[00:11:06.760 --> 00:11:09.940]   then that would indeed give him that confidence to say that.
[00:11:09.940 --> 00:11:13.060]   Now, it's not like Microsoft needs to have the best model.
[00:11:13.060 --> 00:11:17.800]   They are making a ton of money off AI in all sorts of directions.
[00:11:18.180 --> 00:11:29.320]   For example, did you know, according to the 972 magazine, sales of the company's cloud and AI services to the Israeli army have skyrocketed since the beginning of its onslaught on Gaza?
[00:11:29.320 --> 00:11:35.320]   And, of course, we'll have to wait and see if the MAI models live up to the hype.
[00:11:35.320 --> 00:11:38.180]   But nevertheless, the statement alone is noteworthy.
[00:11:38.180 --> 00:11:41.100]   Do you feel OpenAI is just a product company?
[00:11:41.100 --> 00:11:44.360]   I think it's more than that, but time will tell.
[00:11:44.360 --> 00:11:51.740]   The final bit of cheeky evidence comes from, again, comparing DeepSeq V3, the new one from today, with Claude 3.7 Sonnet.
[00:11:51.740 --> 00:11:57.360]   As we know, these are just five benchmarks out of the probably hundred or so out there now.
[00:11:57.440 --> 00:12:01.480]   But just on, say, live code bench, it outperforms 3.7 Sonnet.
[00:12:01.480 --> 00:12:06.220]   And that comes just days after the CEO of Anthropic made this comment.
[00:12:06.220 --> 00:12:09.040]   What we are finding is we are not far from the world.
[00:12:09.040 --> 00:12:13.900]   I think we'll be there in three to six months where AI is writing 90% of the code.
[00:12:13.900 --> 00:12:20.100]   And then in 12 months, we may be in a world where AI is writing essentially all of the code.
[00:12:20.100 --> 00:12:27.040]   But I must confess to noticing after I saw that rather dramatic and hypey comment.
[00:12:27.040 --> 00:12:33.320]   that Anthropic are still advertising for software engineering roles in my hometown of London.
[00:12:33.320 --> 00:12:39.460]   Not only that, they are advertising annual salaries of very generous proportions.
[00:12:39.460 --> 00:12:45.000]   But wait, if Claude 4 or 5 within 12 months is going to do, quote, all the coding,
[00:12:45.000 --> 00:12:47.820]   then why even advertise an annual salary?
[00:12:47.820 --> 00:12:51.100]   Logically, these people would be out of a job within a few months.
[00:12:51.100 --> 00:12:56.340]   Yes, I'm being somewhat facetious because, of course, engineering is much more than just coding.
[00:12:56.700 --> 00:13:01.500]   But still, the words and the prediction don't quite match the recruitment intensity.
[00:13:01.500 --> 00:13:05.480]   And for a family of models that is set to do, quote, all the coding,
[00:13:05.480 --> 00:13:10.960]   it sure is struggling with a primary school age game, in this case, playing Pokemon,
[00:13:10.960 --> 00:13:17.000]   endlessly getting stuck and resorting to hilarious means to progress and never quite doing so.
[00:13:17.000 --> 00:13:19.580]   So that is my take on the new Gemini.
[00:13:19.580 --> 00:13:24.280]   An amazing model, but more proof of convergence than exceptionality.
[00:13:24.500 --> 00:13:30.940]   Of course, very awkward for the Gemini team that it came on the same night as ImageGen from OpenAI.
[00:13:30.940 --> 00:13:35.080]   I decided to do two separate videos, so let me know if you like that approach.
[00:13:35.080 --> 00:13:38.100]   I thought it would just make it easier for you guys to share it with friends
[00:13:38.100 --> 00:13:40.400]   if they're interested in one topic or the other.
[00:13:40.400 --> 00:13:44.560]   But as it is approaching 11 o'clock here, I must bid you good night.
[00:13:44.560 --> 00:13:47.900]   Thank you so much for watching and have a wonderful day.

