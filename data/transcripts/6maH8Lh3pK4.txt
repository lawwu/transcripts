
[00:00:00.000 --> 00:00:02.680]   A lot of people, they think neural networks are only
[00:00:02.680 --> 00:00:05.240]   good for classifying objects.
[00:00:05.240 --> 00:00:09.640]   And I'm here to expand your mind with autoencoders,
[00:00:09.640 --> 00:00:11.840]   one of my favorite kinds of neural networks
[00:00:11.840 --> 00:00:13.660]   that I think don't get enough love.
[00:00:13.660 --> 00:00:16.280]   Students always want to talk to me about generative adversarial
[00:00:16.280 --> 00:00:18.160]   networks, and those are super cool too.
[00:00:18.160 --> 00:00:21.000]   But I think it's best to start with autoencoders.
[00:00:21.000 --> 00:00:25.040]   Autoencoders are a way to make neural networks summarize
[00:00:25.040 --> 00:00:27.160]   the information that they're looking at.
[00:00:27.160 --> 00:00:28.720]   So what we're going to do is we're
[00:00:28.720 --> 00:00:30.720]   going to start with something, say an image,
[00:00:30.720 --> 00:00:32.160]   but it could be something else.
[00:00:32.160 --> 00:00:33.960]   And then we're going to force the neural net
[00:00:33.960 --> 00:00:36.880]   to describe it in a small set of numbers.
[00:00:36.880 --> 00:00:39.000]   And then we're going to have the neural net actually
[00:00:39.000 --> 00:00:41.680]   try to output the same thing.
[00:00:41.680 --> 00:00:44.360]   So it's almost as though I look at a picture of someone
[00:00:44.360 --> 00:00:47.600]   and I try to describe it to a friend in 16 words,
[00:00:47.600 --> 00:00:50.240]   and then they have to draw that same person.
[00:00:50.240 --> 00:00:52.160]   I'm going to have to really distill
[00:00:52.160 --> 00:00:54.040]   what I'm seeing in that person.
[00:00:54.040 --> 00:00:56.660]   I'm going to have to pick the best adjectives for them.
[00:00:56.660 --> 00:00:58.880]   In the same way, we're going to make the neural net
[00:00:58.880 --> 00:01:00.680]   do that to another neural net.
[00:01:00.680 --> 00:01:02.800]   But the way neural nets pass information around
[00:01:02.800 --> 00:01:05.520]   is with numbers.
[00:01:05.520 --> 00:01:07.920]   So this is useful, one, for compression.
[00:01:07.920 --> 00:01:11.060]   We can actually build these awesome compression algorithms
[00:01:11.060 --> 00:01:13.640]   that are very specific to a domain.
[00:01:13.640 --> 00:01:16.280]   But it's also useful for doing things like denoising,
[00:01:16.280 --> 00:01:18.560]   because if I saw a grainy image of a friend
[00:01:18.560 --> 00:01:20.000]   and I described it to someone, you
[00:01:20.000 --> 00:01:23.640]   can imagine that my friend might draw that person
[00:01:23.640 --> 00:01:25.400]   with the graininess removed.
[00:01:25.400 --> 00:01:28.520]   So we can actually build these incredibly powerful denoising
[00:01:28.520 --> 00:01:31.860]   filters using the same neural net technology.
[00:01:31.860 --> 00:01:35.040]   So first of all, you need to have my GitHub
[00:01:35.040 --> 00:01:36.840]   repository checked out.
[00:01:36.840 --> 00:01:38.520]   So if you haven't done it already,
[00:01:38.520 --> 00:01:41.320]   clone-lucas/ml-class.
[00:01:41.320 --> 00:01:47.480]   And go into the directory videos/autoencoder.
[00:01:47.480 --> 00:01:51.680]   Now, you should probably do w and b init
[00:01:51.680 --> 00:01:52.720]   to create a new project.
[00:01:53.080 --> 00:01:56.040]   [TYPING]
[00:01:56.040 --> 00:02:04.920]   And lastly, open up autoencoder.py.
[00:02:04.920 --> 00:02:11.200]   So most of my code should be familiar to you
[00:02:11.200 --> 00:02:13.480]   if you've watched my first video on perceptrons.
[00:02:13.480 --> 00:02:16.560]   Basically, we're building a perceptron with a twist.
[00:02:16.560 --> 00:02:21.480]   The input and the output to this perceptron is the same.
[00:02:21.480 --> 00:02:24.720]   The model is going to take an image, flatten it,
[00:02:24.720 --> 00:02:27.120]   and run a dense array of perceptrons on it.
[00:02:27.120 --> 00:02:29.920]   But then it's going to run another array of perceptrons
[00:02:29.920 --> 00:02:33.760]   with the outputs equal to the size of the original image.
[00:02:33.760 --> 00:02:35.440]   Then we're going to reshape the output
[00:02:35.440 --> 00:02:37.280]   into the form of the original image.
[00:02:37.280 --> 00:02:39.480]   We're going to run our gradient descent with a loss
[00:02:39.480 --> 00:02:42.440]   function that's a difference between the pixels in our input
[00:02:42.440 --> 00:02:44.240]   image and the output image.
[00:02:44.240 --> 00:02:46.320]   In this case, a mean squared error loss function
[00:02:46.320 --> 00:02:48.520]   actually makes a lot of sense, but you can certainly
[00:02:48.520 --> 00:02:50.080]   try others.
[00:02:50.080 --> 00:02:52.320]   If the hidden layer in blue is large,
[00:02:52.320 --> 00:02:55.000]   the network should easily be able to recreate the input
[00:02:55.000 --> 00:02:55.840]   image.
[00:02:55.840 --> 00:02:57.840]   But if the hidden layer in blue is small,
[00:02:57.840 --> 00:03:01.560]   then the network will have to use these digits as efficiently
[00:03:01.560 --> 00:03:05.440]   as possible to pass the essence of the original image
[00:03:05.440 --> 00:03:08.480]   to the output layer to recreate it.
[00:03:08.480 --> 00:03:11.840]   One use case of this type of structure could be compression.
[00:03:11.840 --> 00:03:13.840]   Gradient descent will hopefully get the network
[00:03:13.840 --> 00:03:17.600]   to compress these digits into the smallest possible space.
[00:03:17.600 --> 00:03:19.680]   We could save each image with the layers
[00:03:19.680 --> 00:03:22.240]   that reduce the image down to the middle layer,
[00:03:22.240 --> 00:03:23.880]   and then we could decompress the image
[00:03:23.880 --> 00:03:25.800]   with the layers that expand out the image
[00:03:25.800 --> 00:03:28.320]   from the middle layer.
[00:03:28.320 --> 00:03:30.240]   We can also use variations of this approach
[00:03:30.240 --> 00:03:32.680]   to generate synthetic images and remove noise
[00:03:32.680 --> 00:03:34.640]   from images in various ways.
[00:03:34.640 --> 00:03:36.560]   Let's crank through the code.
[00:03:36.560 --> 00:03:40.920]   Line 13 sets the size of that middle encoding layer.
[00:03:40.920 --> 00:03:43.000]   I'm trying the number 32 here, but feel
[00:03:43.000 --> 00:03:44.640]   free to experiment with other numbers,
[00:03:44.640 --> 00:03:46.800]   because this is the most important variable
[00:03:46.800 --> 00:03:50.120]   in the autoencoder.
[00:03:50.120 --> 00:03:53.200]   Line 16 loads in the MNIST data.
[00:03:53.200 --> 00:03:56.120]   This is the digit data that we keep working with.
[00:03:56.120 --> 00:04:00.720]   We only load x_train and x_test, the pixel values
[00:04:00.720 --> 00:04:02.560]   of the numbers, and we don't actually
[00:04:02.560 --> 00:04:04.940]   load the labels of the images, because we don't actually
[00:04:04.940 --> 00:04:08.520]   need the labels of the images for what we're about to do.
[00:04:08.520 --> 00:04:11.520]   Lines 18 and 19 normalize the data
[00:04:11.520 --> 00:04:14.000]   to be between 0 and 1, which is generally best
[00:04:14.000 --> 00:04:16.600]   practice for all neural nets.
[00:04:16.600 --> 00:04:19.120]   Line 21 sets up our network.
[00:04:19.120 --> 00:04:21.360]   First, we flatten out the data.
[00:04:21.360 --> 00:04:23.600]   Then we compress it into 32 values
[00:04:23.600 --> 00:04:25.480]   using a ReLU activation function since we're
[00:04:25.480 --> 00:04:27.000]   in the middle of the model.
[00:04:27.000 --> 00:04:29.880]   Then we expand our model with another dense layer
[00:04:29.880 --> 00:04:32.800]   with 28 times 28 outputs, the same size
[00:04:32.800 --> 00:04:36.000]   as the original layer.
[00:04:36.000 --> 00:04:37.360]   Quick understanding check.
[00:04:37.360 --> 00:04:40.200]   Can you guess how many free parameters our model has?
[00:04:40.200 --> 00:04:45.200]   If we have trouble guessing or we
[00:04:45.200 --> 00:04:47.880]   want to check our understanding, we can run model.summary
[00:04:47.880 --> 00:04:50.400]   to make sure we know exactly what's going on.
[00:04:50.400 --> 00:04:53.320]   And then we call model.fit to really train this model.
[00:04:53.320 --> 00:05:01.600]   Now, we pass in x_train both as the input
[00:05:01.600 --> 00:05:04.280]   and the output of this model.
[00:05:04.280 --> 00:05:08.440]   Finally, we call model.save to save the model as a file
[00:05:08.440 --> 00:05:10.040]   so that we can test it on more data.
[00:05:10.040 --> 00:05:21.520]  ,
[00:05:21.520 --> 00:05:22.480]   Look at this one run.
[00:05:22.480 --> 00:05:23.400]   It's going to be nuts.
[00:05:23.400 --> 00:05:23.920]   Look at this.
[00:05:23.920 --> 00:05:24.440]   Look at this.
[00:05:24.440 --> 00:05:24.920]   Look at this.
[00:05:24.920 --> 00:05:25.400]   Look at this.
[00:05:25.400 --> 00:05:25.880]   Oh my god.
[00:05:25.880 --> 00:05:35.960]   OK, look at this.
[00:05:35.960 --> 00:05:38.080]   The image on the left is the original image,
[00:05:38.080 --> 00:05:40.480]   and the input on the right is the output image.
[00:05:40.480 --> 00:05:42.600]   And you can see that the model has really distilled
[00:05:42.600 --> 00:05:47.040]   the essence of these large images, or 28 by 28 images,
[00:05:47.040 --> 00:05:50.000]   into just 32 numbers.
[00:05:50.000 --> 00:05:52.440]   The image on the right might be a little bit more blurry
[00:05:52.440 --> 00:05:55.240]   than the input image, and it might remove artifacts
[00:05:55.240 --> 00:05:57.640]   that would be really unusual for that particular digit.
[00:05:57.640 --> 00:06:03.400]   Let's test our autoencoder and see how well it performs.
[00:06:03.400 --> 00:06:06.800]   I wrote some code for you in run_autoencoder.py
[00:06:06.800 --> 00:06:10.400]   that you can use to see how well this image encoding works.
[00:06:10.400 --> 00:06:13.240]   The first couple lines load in some libraries in our data.
[00:06:13.240 --> 00:06:15.600]   OpenCV is just a library for manipulating images
[00:06:15.600 --> 00:06:16.680]   in lots of ways.
[00:06:16.680 --> 00:06:18.480]   It can be a little frustrating, but if you're
[00:06:18.480 --> 00:06:19.840]   working with images, you probably
[00:06:19.840 --> 00:06:21.400]   need to learn to love it.
[00:06:21.400 --> 00:06:22.920]   The next few lines set up a loop where
[00:06:22.920 --> 00:06:24.680]   we wait for keyboard input.
[00:06:24.680 --> 00:06:26.920]   I load the images in one at a time,
[00:06:26.920 --> 00:06:28.480]   and then I show you the input image.
[00:06:28.480 --> 00:06:30.080]   If you press spacebar, I actually
[00:06:30.080 --> 00:06:31.520]   add some noise to the input image,
[00:06:31.520 --> 00:06:34.520]   and then I call model.predict.
[00:06:34.520 --> 00:06:37.320]   Keras's predict function always runs in batch,
[00:06:37.320 --> 00:06:39.280]   but I want to call it on one single image,
[00:06:39.280 --> 00:06:46.680]   so I need to reshape the input image into one 28 by 28 list.
[00:06:46.680 --> 00:06:48.320]   Then it returns an array of output,
[00:06:48.320 --> 00:06:50.400]   but in my case, since I only fed it one image,
[00:06:50.400 --> 00:06:52.600]   I only get one image in my output.
[00:06:52.600 --> 00:06:54.440]   That image is 28 by 28, but I need
[00:06:54.440 --> 00:06:56.480]   to reshape it to add a color channel
[00:06:56.480 --> 00:06:58.680]   so CV2 will show it to me.
[00:06:58.680 --> 00:06:59.640]   Let's give this a try.
[00:07:02.680 --> 00:07:07.280]   Here my input's on the left, and my output's on the right.
[00:07:07.280 --> 00:07:11.400]   And as I go through, my output looks something like my input,
[00:07:11.400 --> 00:07:15.680]   but it's much less impressive than I saw earlier.
[00:07:15.680 --> 00:07:18.160]   So where did I make my mistake?
[00:07:18.160 --> 00:07:19.840]   Can you spot it?
[00:07:19.840 --> 00:07:22.200]   This one actually took me a little while to debug.
[00:07:22.200 --> 00:07:26.240]   So the first thing I do when I see something like this
[00:07:26.240 --> 00:07:30.160]   is I go back to the original training,
[00:07:30.160 --> 00:07:34.400]   and I look at how well it was working on the training data.
[00:07:34.400 --> 00:07:36.440]   So in this case, I can go into W and B,
[00:07:36.440 --> 00:07:39.760]   and I can see that on my training data,
[00:07:39.760 --> 00:07:41.400]   the input image and the output image
[00:07:41.400 --> 00:07:44.480]   were much more similar than when I'm running this thing
[00:07:44.480 --> 00:07:46.040]   potentially in production.
[00:07:46.040 --> 00:07:54.080]   So when inference is happening differently in production
[00:07:54.080 --> 00:07:57.480]   than in training, the first place that I look
[00:07:57.480 --> 00:08:00.140]   is in the data preparation.
[00:08:00.140 --> 00:08:02.140]   There's really only one step of data preparation
[00:08:02.140 --> 00:08:08.400]   that we do here, and that is dividing our input data by 255.
[00:08:08.400 --> 00:08:13.720]   And sure enough, we forgot to divide our data in our production
[00:08:13.720 --> 00:08:17.800]   code by 255.
[00:08:17.800 --> 00:08:20.200]   And unfortunately, instead of throwing an error,
[00:08:20.200 --> 00:08:23.160]   our model silently gave us degraded performance.
[00:08:28.280 --> 00:08:30.460]   So let's add back in that normalization.
[00:08:30.460 --> 00:08:36.820]   And sure enough, our model now works great.
[00:08:36.820 --> 00:08:44.600]   In fact, it seems to smooth the image out a bit, which
[00:08:44.600 --> 00:08:46.980]   might make you wonder if this model is good at denoising.
[00:08:46.980 --> 00:08:55.900]   So if we go in, we can actually add a function
[00:08:55.900 --> 00:08:59.080]   which changes things so when I press the space bar,
[00:08:59.080 --> 00:09:00.900]   it makes a whole bunch of pixel noise.
[00:09:00.900 --> 00:09:07.900]   Now I'm running my autoencoder on input with pixel noise.
[00:09:07.900 --> 00:09:18.440]   And you can see that with pixel noise,
[00:09:18.440 --> 00:09:20.660]   our model's doing an excellent job of removing it.
[00:09:24.660 --> 00:09:27.500]   This is because the autoencoder has a much harder time
[00:09:27.500 --> 00:09:30.980]   encoding the pixel noise than encoding the original image.
[00:09:30.980 --> 00:09:37.780]   So if we want to make an even better denoiser,
[00:09:37.780 --> 00:09:40.740]   we can programmatically add noises to our image
[00:09:40.740 --> 00:09:44.180]   at training time and use that as input.
[00:09:44.180 --> 00:09:47.540]   So here, input is a noisy version of our original image,
[00:09:47.540 --> 00:09:50.340]   and the output is the original image itself.
[00:09:50.340 --> 00:09:53.780]   So take a look at denoising autoencoder, which does this.
[00:09:53.780 --> 00:09:56.460]   So here we load X-train and X-test again,
[00:09:56.460 --> 00:09:59.020]   but now we run that function addNoise,
[00:09:59.020 --> 00:10:01.540]   which creates versions of X-train and X-test
[00:10:01.540 --> 00:10:03.140]   with pixel noise added.
[00:10:03.140 --> 00:10:11.020]   We can use an identical architecture to before,
[00:10:11.020 --> 00:10:14.780]   and we've built a system that automatically filters out
[00:10:14.780 --> 00:10:16.540]   all types of noise.
[00:10:16.540 --> 00:10:19.900]   But what are neural nets without convolutions?
[00:10:19.900 --> 00:10:22.900]   Let's apply the convolutional nets from previous videos
[00:10:22.900 --> 00:10:25.340]   to both these examples.
[00:10:25.340 --> 00:10:28.580]   Open up autoencoder_cnn.py, and this
[00:10:28.580 --> 00:10:30.820]   is a lot like autoencoder.py, but I've
[00:10:30.820 --> 00:10:34.020]   changed the architecture to be convolutional.
[00:10:34.020 --> 00:10:36.260]   Just like in the convolutional neural nets video,
[00:10:36.260 --> 00:10:39.340]   I need to reshape my data to 28 by 28 by 1
[00:10:39.340 --> 00:10:43.140]   to work with the conv2d layers in Keras.
[00:10:43.140 --> 00:10:44.660]   I make a convolutional layer.
[00:10:44.660 --> 00:10:46.860]   I use a special padding equals same,
[00:10:46.860 --> 00:10:50.340]   so the input and output shape of my convolution is the same.
[00:10:50.340 --> 00:10:51.940]   Without this, the output images would
[00:10:51.940 --> 00:10:54.860]   be slightly smaller than the inputs.
[00:10:54.860 --> 00:10:58.260]   Then I do max pooling to reduce the size.
[00:10:58.260 --> 00:10:59.980]   I do another convolutional layer,
[00:10:59.980 --> 00:11:02.500]   and then I add an upsampling layer, which actually just
[00:11:02.500 --> 00:11:06.060]   repeats the rows and columns to make the image twice as big.
[00:11:06.060 --> 00:11:07.860]   I add one more convolution layer,
[00:11:07.860 --> 00:11:09.900]   and this one has just a single output
[00:11:09.900 --> 00:11:13.820]   to match the shape of my original input image.
[00:11:13.820 --> 00:11:16.140]   This network takes a long time to train,
[00:11:16.140 --> 00:11:19.100]   but it works fantastically well with a very small number
[00:11:19.100 --> 00:11:20.660]   of parameters.
[00:11:21.660 --> 00:11:23.260]   So this is kind of awesome, right?
[00:11:23.260 --> 00:11:26.260]   So we've built a denoising system,
[00:11:26.260 --> 00:11:29.460]   and we've built a custom compression system with stuff
[00:11:29.460 --> 00:11:32.260]   that we already understood and frameworks that we already
[00:11:32.260 --> 00:11:34.820]   knew without changing very much.
[00:11:34.820 --> 00:11:37.780]   I think that's a pretty cool application of neural networks,
[00:11:37.780 --> 00:11:40.180]   but I think there's even a bigger point here, which
[00:11:40.180 --> 00:11:43.900]   is that I've seen when students first encounter autoencoders,
[00:11:43.900 --> 00:11:47.220]   it really opens their minds to lots of different things
[00:11:47.220 --> 00:11:48.420]   that they can do.
[00:11:48.420 --> 00:11:51.060]   It really opens their minds to lots and lots
[00:11:51.060 --> 00:11:52.820]   of new applications of neural networks
[00:11:52.820 --> 00:11:54.540]   that they hadn't thought of before.
[00:11:54.540 --> 00:11:56.740]   You really understand that what neural networks do
[00:11:56.740 --> 00:11:59.100]   is they basically take fixed sets of inputs
[00:11:59.100 --> 00:12:01.580]   and then output fixed sets of numbers.
[00:12:01.580 --> 00:12:03.360]   And I think that's a really important thing
[00:12:03.360 --> 00:12:05.820]   to understand where neural networks could be implied,
[00:12:05.820 --> 00:12:08.420]   especially things like colorizing images
[00:12:08.420 --> 00:12:10.420]   or generative adversarial networks, which
[00:12:10.420 --> 00:12:13.380]   I know we have to do a video on in the future.
[00:12:13.380 --> 00:12:15.580]   So it's a fun challenge from here.
[00:12:15.580 --> 00:12:17.580]   You could try to build a convolutional system
[00:12:17.580 --> 00:12:20.660]   for that same denoising thing that we built earlier
[00:12:20.660 --> 00:12:21.700]   without convolutions.
[00:12:21.700 --> 00:12:24.580]   I think that would be super educational.
[00:12:24.580 --> 00:12:26.740]   I hope that you had fun learning about autoencoders.
[00:12:26.740 --> 00:12:29.220]   I think they're incredibly applied in some ways.
[00:12:29.220 --> 00:12:32.660]   We built a system that can do custom compression
[00:12:32.660 --> 00:12:35.380]   on very specific data sets.
[00:12:35.380 --> 00:12:38.320]   We also built a system almost at the same time
[00:12:38.320 --> 00:12:42.300]   that could do a very robust type of denoising that
[00:12:42.300 --> 00:12:44.460]   could have practical applications on image
[00:12:44.460 --> 00:12:48.340]   processing or video processing or even audio processing.
[00:12:48.340 --> 00:12:50.900]   The next step that probably most of you are interested in
[00:12:50.900 --> 00:12:53.020]   is learning more about generative adversarial
[00:12:53.020 --> 00:12:56.460]   networks, which this stuff applies super well to.
[00:12:56.460 --> 00:12:58.340]   So we're going to make that video soon,
[00:12:58.340 --> 00:13:00.380]   and I'll see you there.
[00:13:00.380 --> 00:13:02.220]   OK, guys, if you like these videos,
[00:13:02.220 --> 00:13:03.700]   here's what you could do for me.
[00:13:03.700 --> 00:13:04.620]   It's so easy.
[00:13:04.620 --> 00:13:09.140]   Subscribe, write a comment, but not a mean comment.
[00:13:09.140 --> 00:13:10.980]   [MUSIC PLAYING]

