
[00:00:00.000 --> 00:00:02.960]   I tell my kids this all the time, like life is not short, life is long.
[00:00:02.960 --> 00:00:06.740]   And what that means is you should think of yourself as having many
[00:00:06.740 --> 00:00:11.140]   opportunities to learn things and try things and do things.
[00:00:11.140 --> 00:00:14.520]   Software development is fundamentally an exercise in sociology, like in
[00:00:14.520 --> 00:00:20.120]   organizing teams and in creating processes and culture and conventions
[00:00:20.120 --> 00:00:22.200]   around the building of software.
[00:00:22.200 --> 00:00:24.720]   I think finance is 9% of GDP.
[00:00:24.720 --> 00:00:27.920]   Is that too high a price to be paying for liquidity and price discovery?
[00:00:27.920 --> 00:00:37.760]   Okay.
[00:00:37.760 --> 00:00:41.560]   Today, I have the pleasure of speaking with Augustin LeBron, who is the
[00:00:41.560 --> 00:00:46.480]   author of The Laws of Trading, A Trader's Guide to Better Decision-Making
[00:00:46.480 --> 00:00:50.920]   for Everyone, this is one of those books, you know, Tyler Cowen calls
[00:00:50.920 --> 00:00:55.360]   these quake books that completely shift the models you have of the world.
[00:00:55.360 --> 00:00:57.840]   I really, really enjoyed reading this book.
[00:00:58.120 --> 00:01:01.280]   So yeah, I'll let you describe your background, Augustin.
[00:01:01.280 --> 00:01:03.600]   But before that, let me just, let me ask this question.
[00:01:03.600 --> 00:01:06.920]   So Peter Thiel says that the Straussian reading of zero to one is
[00:01:06.920 --> 00:01:08.240]   that you shouldn't start a startup.
[00:01:08.240 --> 00:01:11.440]   And I think that, tell me what you think about this.
[00:01:11.440 --> 00:01:14.760]   I think the Straussian reading of the laws of trading is that you shouldn't
[00:01:14.760 --> 00:01:19.000]   trade, right, because you probably don't have edge because you're not
[00:01:19.000 --> 00:01:20.120]   better than a marginal trader.
[00:01:20.120 --> 00:01:21.800]   And if you think you have edge, it's probably because you haven't
[00:01:21.800 --> 00:01:23.440]   factored in risks and other costs.
[00:01:23.440 --> 00:01:24.960]   So don't trade.
[00:01:24.960 --> 00:01:26.960]   Is that, is that what I should take away from this book?
[00:01:27.440 --> 00:01:29.760]   I think you, you pretty much hit the nail on the head.
[00:01:29.760 --> 00:01:34.080]   I think a lot of the times that the people sort of start thinking about
[00:01:34.080 --> 00:01:38.040]   trading seriously, they start realizing more and more how, how, how hard a
[00:01:38.040 --> 00:01:40.120]   job it really is to do well.
[00:01:40.120 --> 00:01:44.640]   And and the answer is probably, look, if you're smart enough and, and good
[00:01:44.640 --> 00:01:48.480]   enough and hardworking enough to, to make a go at it and make a living
[00:01:48.480 --> 00:01:51.360]   at it in financial markets, there's probably an easier way to, to make
[00:01:51.360 --> 00:01:53.960]   money and, you know, have a satisfying life most of the time.
[00:01:53.960 --> 00:01:54.560]   Okay.
[00:01:54.560 --> 00:01:54.640]   Yeah.
[00:01:54.640 --> 00:01:58.680]   So do you want to, do you want to talk about your background and then how,
[00:01:58.680 --> 00:02:01.320]   what you've been working on in the past and what you're working on now?
[00:02:01.320 --> 00:02:02.200]   Yeah.
[00:02:02.200 --> 00:02:05.040]   So, so my background is engineering.
[00:02:05.040 --> 00:02:06.560]   That's kind of what I did in university.
[00:02:06.560 --> 00:02:10.160]   I did engineering for about six years professionally.
[00:02:10.160 --> 00:02:11.200]   I was a chip designer.
[00:02:11.200 --> 00:02:15.520]   At the time I was playing a lot of online poker back when that was a
[00:02:15.520 --> 00:02:17.960]   profitable and arguably legal thing to do.
[00:02:17.960 --> 00:02:22.520]   And so engineering was getting kind of boring and I wanted to do something else.
[00:02:22.520 --> 00:02:25.760]   And so I thought, well, what's, what's halfway between engineering and poker.
[00:02:25.760 --> 00:02:27.440]   And of course that's quant trading.
[00:02:27.440 --> 00:02:32.680]   So January, 2008, walked into my boss's office and I said, I want to quit.
[00:02:32.680 --> 00:02:35.520]   And, and he said, oh, where are you going?
[00:02:35.520 --> 00:02:36.920]   And I said, I'm going to go into finance.
[00:02:36.920 --> 00:02:39.600]   And he's like, are you sure this is a good time to be doing that?
[00:02:39.600 --> 00:02:42.800]   He said, yep, no, I'm dead set on it.
[00:02:42.800 --> 00:02:48.480]   And a few months later, managed to get a job at Jane street and wrote out the
[00:02:48.480 --> 00:02:52.320]   implosion of Western civilization from the seat of a trading desk.
[00:02:53.000 --> 00:02:57.840]   So we did that for a few years and then left Jane street a few years ago and
[00:02:57.840 --> 00:02:59.280]   started my own consulting company.
[00:02:59.280 --> 00:03:04.680]   Basically just helping tech companies with growth things like management and
[00:03:04.680 --> 00:03:05.840]   hiring and that sort of thing.
[00:03:05.840 --> 00:03:10.880]   And in the last few months started a new company in the crypto space.
[00:03:10.880 --> 00:03:14.920]   How much are you willing to give up your edge by telling us what this is?
[00:03:14.920 --> 00:03:16.960]   Or if you're, if you're not willing to talk about it, that's okay as well.
[00:03:16.960 --> 00:03:19.360]   Yeah, no, I mean, big picture.
[00:03:19.360 --> 00:03:23.840]   We're building a crypto protocol that is kind of new and has some pretty cool
[00:03:23.840 --> 00:03:28.760]   cryptographic guarantees against things that people don't like when they trade in
[00:03:28.760 --> 00:03:29.160]   crypto.
[00:03:29.160 --> 00:03:29.680]   Yeah.
[00:03:29.680 --> 00:03:31.680]   So let's get into some of the topics in the book.
[00:03:31.680 --> 00:03:36.080]   So yeah, first I want to talk about adverse selection because this was, you know, this
[00:03:36.080 --> 00:03:37.880]   was the most interesting part of the book for me.
[00:03:37.880 --> 00:03:39.480]   So let me ask this question.
[00:03:39.480 --> 00:03:44.320]   If we think of hiring workers as, you know, placing bids on them, if you're like an
[00:03:44.320 --> 00:03:47.960]   employer and then multiple employers can place bids on them, doesn't winner's curse
[00:03:47.960 --> 00:03:52.240]   imply that the average worker is probably overpaid because the true value of the
[00:03:52.240 --> 00:03:55.600]   employee employee is not the highest bid, but the average bid that they would get
[00:03:55.600 --> 00:03:56.160]   paid on the market?
[00:03:56.160 --> 00:03:57.600]   Yeah, you're right.
[00:03:57.600 --> 00:04:01.360]   I, from the employer side, it's definitely adverse selection all around.
[00:04:01.360 --> 00:04:05.120]   Like, first of all, if you're looking for, if you're just sort of posting a job at
[00:04:05.120 --> 00:04:10.600]   the applicants that apply are, you know, selected against in the sense that you're
[00:04:10.600 --> 00:04:13.840]   selected against that, that pool, because you know, the people who are really, really
[00:04:13.840 --> 00:04:16.120]   good, probably their employers know they're really good.
[00:04:16.120 --> 00:04:17.920]   And so they're really incentivized to keep them.
[00:04:17.920 --> 00:04:20.840]   And so the people who are kind of on the market are probably at the margin, not as
[00:04:20.840 --> 00:04:21.240]   good.
[00:04:21.240 --> 00:04:27.000]   Um, not only that, but even just the mechanics of hiring the person who has the
[00:04:27.000 --> 00:04:30.560]   final say in terms of whether this happens or not is the employee.
[00:04:30.560 --> 00:04:35.760]   And so you're going to get adverse select, adversely selected there because the
[00:04:35.760 --> 00:04:38.280]   people who are really, really good are going to have lots of job offers.
[00:04:38.280 --> 00:04:40.640]   And so they're going to pick from one of many offers.
[00:04:40.640 --> 00:04:44.160]   Uh, the people who aren't so good are going to pick from few offers.
[00:04:44.200 --> 00:04:47.480]   And so employers just systematically get adverse selected that way.
[00:04:47.480 --> 00:04:52.480]   Um, now whether that means that they're sort of systematically overpaid, I think
[00:04:52.480 --> 00:04:56.200]   that's a different question because in the end companies have a pretty good idea, or
[00:04:56.200 --> 00:04:59.480]   at least should have a pretty good idea of what the marginal value of an additional
[00:04:59.480 --> 00:05:00.280]   employee is.
[00:05:00.280 --> 00:05:06.320]   Um, it's true, certainly that people by and large give up things in order for, in
[00:05:06.320 --> 00:05:08.840]   order to get the security of working at a company.
[00:05:08.840 --> 00:05:13.320]   So maybe that counteracts that, that sort of adverse selection in terms of pay.
[00:05:13.600 --> 00:05:16.280]   Um, it's not clear which way it washes out, I think to me.
[00:05:16.280 --> 00:05:16.880]   Yeah.
[00:05:16.880 --> 00:05:20.480]   Burn Hobart, uh, recently wrote a blog post about your, um, this chapter in your
[00:05:20.480 --> 00:05:21.680]   book about adverse selection.
[00:05:21.680 --> 00:05:25.960]   And, um, so one of the things he said in a footnote almost in passing was that there
[00:05:25.960 --> 00:05:30.280]   should be more adverse selection in industries like finance, where, um, the
[00:05:30.280 --> 00:05:33.640]   motivation for people to work in them is money because in industry, like if a
[00:05:33.640 --> 00:05:36.480]   worker wants to work for space X, there's a story you can tell about like why they're
[00:05:36.480 --> 00:05:39.080]   working for you and nobody else in finance.
[00:05:39.080 --> 00:05:42.240]   You know, th there's a lot of people, obviously, as you, as you know, who are
[00:05:42.240 --> 00:05:43.840]   like, might be bidding for really talented people.
[00:05:43.840 --> 00:05:47.160]   So if they're working for you, there's something suspicious about that.
[00:05:47.160 --> 00:05:50.080]   No, I think there's something to that.
[00:05:50.080 --> 00:05:54.600]   Um, certainly, you know, doing a lot of the hiring that I used to do, one of the
[00:05:54.600 --> 00:05:58.360]   biggest, uh, almost red flags is when somebody comes to you and says, Oh, I've
[00:05:58.360 --> 00:06:02.760]   been wanting to be a trader my whole life because they're not like, first of
[00:06:02.760 --> 00:06:04.040]   all, they don't know what trading is, right?
[00:06:04.040 --> 00:06:05.960]   They haven't known what trading is their whole life.
[00:06:05.960 --> 00:06:07.800]   They don't know what the job really involves.
[00:06:07.800 --> 00:06:10.640]   It's not tangible in the way that being a doctor is tangible.
[00:06:10.680 --> 00:06:13.280]   And so what they're really telling you is I've been wanting to make a lot of
[00:06:13.280 --> 00:06:17.560]   money in my whole life, which is generally a pretty, well, let's say like in some
[00:06:17.560 --> 00:06:20.880]   jobs, it's a good motivation, but it's not necessarily the motivation you're 100%
[00:06:20.880 --> 00:06:22.680]   looking for out of the gate in hiring someone.
[00:06:22.680 --> 00:06:23.840]   Oh, interesting.
[00:06:23.840 --> 00:06:27.240]   Um, cause in your chapter on motivation, it seemed like you were implying that that
[00:06:27.240 --> 00:06:28.680]   is the motivation you should be looking for.
[00:06:28.680 --> 00:06:32.520]   Cause if their motivation is emotional, then they're going to be losing to people
[00:06:32.520 --> 00:06:33.600]   whose motivation is to make money.
[00:06:33.600 --> 00:06:37.000]   So, um, I, yeah, I love for you to talk more about what is the motivation you are
[00:06:37.000 --> 00:06:37.520]   looking for?
[00:06:37.520 --> 00:06:38.240]   Yeah.
[00:06:38.240 --> 00:06:43.360]   So, I mean, I think so that the motivation of like winning the game of like making
[00:06:43.360 --> 00:06:46.000]   money and that is sort of how we determine who wins the game.
[00:06:46.000 --> 00:06:50.080]   I think that that part of the making money motivation makes a lot of sense for a
[00:06:50.080 --> 00:06:56.360]   trader, but the, like, all I want to do is make the most money possible is correlated
[00:06:56.360 --> 00:06:58.920]   to things that, um, that aren't maybe so great.
[00:06:58.920 --> 00:07:05.200]   Like, because a lot of the job is, um, is sort of having an inherent curiosity about
[00:07:05.200 --> 00:07:06.960]   random things, for example.
[00:07:06.960 --> 00:07:13.320]   Um, and, um, like if you're, if your whole motivation is like, where can I sort of
[00:07:13.320 --> 00:07:14.320]   make the most money today?
[00:07:14.320 --> 00:07:17.440]   It's not necessarily optimal over the long haul.
[00:07:17.440 --> 00:07:21.320]   And so you kind of need to sort of balance that against these other things, like
[00:07:21.320 --> 00:07:26.480]   enjoying the game for its own sake, uh, enjoying the game for, for like, you know,
[00:07:26.480 --> 00:07:29.080]   sort of as an exploratory kind of thing.
[00:07:29.080 --> 00:07:32.920]   Um, so maybe that's like, maybe a little bit inconsistent with something that I
[00:07:32.920 --> 00:07:36.720]   wrote in the book, but, um, but I think at the margin people need to hear the other
[00:07:36.720 --> 00:07:37.160]   thing more.
[00:07:37.160 --> 00:07:38.920]   Uh, yeah.
[00:07:38.920 --> 00:07:39.160]   Okay.
[00:07:39.160 --> 00:07:39.440]   Interesting.
[00:07:39.440 --> 00:07:42.560]   So, and then how do you figure out if somebody enjoys the game for its own sake?
[00:07:42.560 --> 00:07:46.280]   I think you said in another interview that, um, uh, it's a company like Jane
[00:07:46.280 --> 00:07:48.960]   street would hold it against you if you have like retail trading experience,
[00:07:48.960 --> 00:07:53.160]   because, um, I guess you can talk more about why that is, but yeah.
[00:07:53.160 --> 00:07:56.160]   So if that's not what you're, that's not how you judge whether they would
[00:07:56.160 --> 00:07:59.200]   intrinsically enjoy the job, but how is it that you would judge that?
[00:07:59.920 --> 00:08:02.320]   So I, one of the things I've always said, maybe you've heard me say this
[00:08:02.320 --> 00:08:07.000]   before, is, um, I would love to talk to the person who is the third best player
[00:08:07.000 --> 00:08:11.480]   in the world at some weird, obscure chess variant, because that is probably
[00:08:11.480 --> 00:08:15.840]   very correlated with things that I care about, such as, um, a willingness, a
[00:08:15.840 --> 00:08:19.360]   willingness to really like grind and try to get really, really good at something.
[00:08:19.360 --> 00:08:23.480]   And to do so, not because there's a huge pot of gold at the end of the
[00:08:23.480 --> 00:08:26.760]   rainbow, but because you just find inherent enjoyment in getting really,
[00:08:26.760 --> 00:08:27.640]   really good at something.
[00:08:27.640 --> 00:08:29.360]   So I think that's, that's pretty good.
[00:08:29.720 --> 00:08:35.240]   Um, but yeah, just general, um, again, aside from sort of the mathematical
[00:08:35.240 --> 00:08:38.920]   and, and, um, and sort of risk-taking parts, which are sort of maybe
[00:08:38.920 --> 00:08:43.560]   independent from this, uh, certainly a strong desire to be in a competitive
[00:08:43.560 --> 00:08:46.160]   environment and to enjoy being in that environment.
[00:08:46.160 --> 00:08:49.560]   I think that's, you know, that can take many forms, but I think
[00:08:49.560 --> 00:08:50.720]   that's a big part of it for sure.
[00:08:50.720 --> 00:08:56.160]   So then why is, um, having domain expertise in trading not important?
[00:08:56.320 --> 00:08:59.960]   Um, is it, uh, cause you usually in other industries, it's like, if you, the more
[00:08:59.960 --> 00:09:02.880]   experience you have in the industry, the better, and it seems like you guys are
[00:09:02.880 --> 00:09:06.800]   often hiring people who are just very analytically smart, but, um, maybe you
[00:09:06.800 --> 00:09:08.040]   haven't been traders before.
[00:09:08.040 --> 00:09:10.320]   Um, so like how, how do you guys manage it to do that?
[00:09:10.320 --> 00:09:16.560]   I guess the thing I'm thinking of is that the concept of a domain is probably a lot
[00:09:16.560 --> 00:09:18.400]   narrower than people understand it to be.
[00:09:18.400 --> 00:09:22.800]   Um, like if I'm there sitting there on my Robin hood account, punting stocks back
[00:09:22.800 --> 00:09:26.280]   and forth, like that is not the same domain as what a trader at a market
[00:09:26.280 --> 00:09:28.920]   maker, uh, or at a top trading firm would do.
[00:09:28.920 --> 00:09:32.640]   Um, and in fact, to the extent that you think that that's the same domain, that
[00:09:32.640 --> 00:09:36.760]   is a thing that you have to unlearn when you come work at, at, you know, we'll say
[00:09:36.760 --> 00:09:42.000]   a real company and you know, that, that can happen, but it's just, it's kind of a
[00:09:42.000 --> 00:09:42.520]   problem.
[00:09:42.520 --> 00:09:45.240]   Like it's just a thing you have, you have in the back of your mind, right?
[00:09:45.240 --> 00:09:48.800]   Like you'd rather take a blank slate, really smart, motivated, blank slate, and
[00:09:48.800 --> 00:09:53.640]   sort of teach them what they need to know, then undo something and then teach
[00:09:53.640 --> 00:09:54.800]   them the thing they need to know.
[00:09:55.320 --> 00:09:57.760]   Um, you see this a lot of the time.
[00:09:57.760 --> 00:10:02.760]   The other thing is from, uh, again, at a meta level, probably in expectation, the
[00:10:02.760 --> 00:10:06.160]   person who's doing trading in their personal account, isn't doing positive
[00:10:06.160 --> 00:10:07.000]   edge trades.
[00:10:07.000 --> 00:10:08.800]   Like they're probably on average losing money.
[00:10:08.800 --> 00:10:13.040]   And so you would like the person to realize that maybe this is not a winning
[00:10:13.040 --> 00:10:15.120]   game for them and so they shouldn't be playing it.
[00:10:15.120 --> 00:10:18.040]   And so again, there's sort of this adverse selection of, well, if they can't realize
[00:10:18.040 --> 00:10:20.480]   they're playing a losing game here, then that's probably not great.
[00:10:20.480 --> 00:10:24.680]   So you said in the book, it takes like six, six to 18 months before you can train a
[00:10:24.680 --> 00:10:26.880]   trader to be net positive.
[00:10:26.880 --> 00:10:29.360]   Um, what is happening in that time?
[00:10:29.360 --> 00:10:31.160]   Like what, what, what are the skills you're teaching them?
[00:10:31.160 --> 00:10:32.400]   Yeah.
[00:10:32.400 --> 00:10:36.080]   So this varies from company to company and even has varied over the course of the
[00:10:36.080 --> 00:10:37.480]   history of Jane Street, certainly.
[00:10:37.480 --> 00:10:41.360]   Like when I started, it was very much the, the Socratic method, right?
[00:10:41.360 --> 00:10:44.640]   You sit next to a senior trader and their jobs to teach you everything they know.
[00:10:44.640 --> 00:10:49.240]   Um, and so it's just a continuous stream of questions, answers, conversations,
[00:10:49.240 --> 00:10:49.760]   et cetera.
[00:10:49.760 --> 00:10:53.800]   Um, Jane Street to their credit has improved on that.
[00:10:53.880 --> 00:10:57.280]   Um, there's now sort of a bootcamp that you go through where you basically just
[00:10:57.280 --> 00:11:01.000]   intensively learn the fundamentals of everything that, that you, that, you know,
[00:11:01.000 --> 00:11:03.040]   the firm needs, feels like you need to know as a trader.
[00:11:03.040 --> 00:11:07.200]   So that again, accelerates the process, but it is very much sort of putting people
[00:11:07.200 --> 00:11:12.480]   in situations to sort of experience the decision-making process and iterating on
[00:11:12.480 --> 00:11:13.600]   that decision-making process.
[00:11:13.600 --> 00:11:14.720]   Like, what are you thinking about here?
[00:11:14.720 --> 00:11:15.440]   What do you think about that?
[00:11:15.440 --> 00:11:16.600]   Hey, did you think about that?
[00:11:16.600 --> 00:11:18.040]   What would you do in this situation?
[00:11:18.040 --> 00:11:19.480]   Why, why not, et cetera.
[00:11:19.480 --> 00:11:21.680]   And that just, that just takes time.
[00:11:22.080 --> 00:11:25.520]   I wonder, as you mentioned, you've done a lot of, um, you've helped in a lot of
[00:11:25.520 --> 00:11:26.640]   hiring for tech companies.
[00:11:26.640 --> 00:11:30.880]   I wonder if, uh, how applicable this model is to the tech industry.
[00:11:30.880 --> 00:11:34.840]   So, I mean, um, could a company like Google just have a very effective bootcamp
[00:11:34.840 --> 00:11:37.800]   where they get like people who have studied like physics or math at MIT.
[00:11:37.800 --> 00:11:41.280]   Um, and maybe not necessarily computer science, but you know, if you don't know
[00:11:41.280 --> 00:11:45.280]   that much programming, you can still come in and then we'll make you, you know, 10 X
[00:11:45.280 --> 00:11:48.720]   in a very short amount of time, or is that something special about finance and trading?
[00:11:48.720 --> 00:11:49.920]   I don't think so.
[00:11:49.960 --> 00:11:53.920]   In fact, I think that the most common failure mode I see in tech company hiring
[00:11:53.920 --> 00:11:57.120]   is hiring for skills instead of hiring for abilities and potential.
[00:11:57.120 --> 00:12:00.280]   Um, and it's just because skills are very legible.
[00:12:00.280 --> 00:12:03.320]   Like it is fairly straightforward to spend an hour with somebody and understand
[00:12:03.320 --> 00:12:05.040]   whether they can write code in Python.
[00:12:05.040 --> 00:12:05.400]   Right.
[00:12:05.400 --> 00:12:08.080]   And so it's like the drunk looking for the keys near the lamppost.
[00:12:08.080 --> 00:12:13.120]   Like you just evaluate what's easy to evaluate my, my dream in some sense.
[00:12:13.120 --> 00:12:15.840]   And this is something that I can't really work on right now, but who knows someday
[00:12:15.840 --> 00:12:21.960]   I could, is the idea of doing mass mass screening for people around the world.
[00:12:21.960 --> 00:12:28.640]   Like what I'd love to find is the smartest, um, 0.1% of high school, high
[00:12:28.640 --> 00:12:32.960]   school graduates around the world, India, Nigeria, all these countries that are
[00:12:32.960 --> 00:12:37.200]   being massively underserved by their educational system and their opportunities.
[00:12:37.200 --> 00:12:42.680]   Um, and putting them in these sort of bootcampy situations, um, for, you know,
[00:12:42.680 --> 00:12:45.280]   six months or something where they learn, you know, useful skills.
[00:12:45.280 --> 00:12:48.680]   And at the end of it, there's like a six figure job with a Western company.
[00:12:48.680 --> 00:12:53.560]   Like there's no reason that, that companies like Infosys should,
[00:12:53.560 --> 00:12:56.720]   should be taking the lion's share of that arbitrage opportunity.
[00:12:56.720 --> 00:13:00.680]   Like there's this incredible need in the world for people that
[00:13:00.680 --> 00:13:02.160]   are smart and motivated.
[00:13:02.160 --> 00:13:05.760]   And there's this incredible supply that we're just systematically under tapping.
[00:13:05.760 --> 00:13:08.720]   So my answer to your question is yes, there is.
[00:13:08.720 --> 00:13:13.240]   I, I strongly believe there is a, there's a trillion dollar business
[00:13:13.240 --> 00:13:15.440]   potentially, uh, or maybe it's a nonprofit.
[00:13:15.440 --> 00:13:19.600]   I don't know in, in closing this arbitrage gap, your former
[00:13:19.600 --> 00:13:20.880]   colleague, Sam Beckman, freed.
[00:13:20.880 --> 00:13:26.760]   Um, he, um, uh, you know, obviously the CEO of FTX, um, and he has, you know,
[00:13:26.760 --> 00:13:31.080]   started a big, um, charity called the future fund and one of their project
[00:13:31.080 --> 00:13:35.200]   ideas is exactly what you're talking about, where you would, uh, there'd be
[00:13:35.200 --> 00:13:38.600]   like large gains if you could enable talent from the developing world.
[00:13:38.600 --> 00:13:40.960]   So what is it that you would look for when you were like
[00:13:40.960 --> 00:13:42.080]   scouting out this talent?
[00:13:43.000 --> 00:13:43.240]   Yeah.
[00:13:43.240 --> 00:13:46.960]   So I think one of the things that maybe isn't, uh, isn't terribly polite
[00:13:46.960 --> 00:13:50.720]   to talk about, but I think is critical is just, um, G intelligence.
[00:13:50.720 --> 00:13:58.040]   Like it strongly predicts outcomes across, uh, jobs across, um, industries.
[00:13:58.040 --> 00:14:02.080]   Um, and so you, that is some element of it.
[00:14:02.080 --> 00:14:06.840]   That is certainly some element of it, but also I would say, um, I think
[00:14:06.840 --> 00:14:10.560]   in an ideal world, you would build this, this process, the selection
[00:14:10.560 --> 00:14:13.560]   process, kind of like a game, like maybe like a mobile game or something
[00:14:13.560 --> 00:14:18.840]   where you're sort of, people are sort of incentivized to kind of keep trying
[00:14:18.840 --> 00:14:22.640]   it stuff and maybe it's, it's a little bit of a grind and, and again, you're
[00:14:22.640 --> 00:14:26.040]   sort of selecting for that hardworking this, um, stick to itiveness, whatever
[00:14:26.040 --> 00:14:28.800]   you want to call it, to use a principle Skinner term.
[00:14:28.800 --> 00:14:33.800]   Um, and so, yeah, like some combination of those two things I think are pretty,
[00:14:33.800 --> 00:14:37.360]   are almost definitely predictive of, of actual value.
[00:14:37.360 --> 00:14:39.800]   Have you, have you heard of Pioneer?
[00:14:40.280 --> 00:14:41.800]   Uh, the thing started by Daniel Gross.
[00:14:41.800 --> 00:14:43.840]   Uh, yes, I've heard of it.
[00:14:43.840 --> 00:14:45.200]   I don't know much, much about it.
[00:14:45.200 --> 00:14:46.280]   Oh yeah.
[00:14:46.280 --> 00:14:47.320]   This sounds a lot like it.
[00:14:47.320 --> 00:14:49.880]   I don't know too much about it either, but yeah, this is, this sounds very similar.
[00:14:49.880 --> 00:14:52.680]   I think they're trying to like make, um, building a startup, like a video game.
[00:14:52.680 --> 00:14:56.280]   So, um, with the, you know, the associated risk, uh, rewards and stuff.
[00:14:56.280 --> 00:15:01.320]   Um, how do you deal with adverse selection in cases where theoretically adverse
[00:15:01.320 --> 00:15:02.640]   selection should work for you?
[00:15:02.640 --> 00:15:07.160]   Um, um, but you know, like the counterparty prices in the
[00:15:07.160 --> 00:15:08.520]   possibility of getting a lemon.
[00:15:08.520 --> 00:15:11.680]   So like an example would be I'm 21 years old and I'm a male.
[00:15:11.680 --> 00:15:15.120]   So like car insurance premiums for me are huge.
[00:15:15.120 --> 00:15:19.840]   Even if I'm, um, if, even if I'm a good driver, because you're not, there's
[00:15:19.840 --> 00:15:22.400]   like, um, there's the adverse selection, the insurance company faces.
[00:15:22.400 --> 00:15:24.880]   And then like back, going back to another example, we were talking about if there's
[00:15:24.880 --> 00:15:30.000]   like a great employee, um, who's, he might be getting underpaid because the
[00:15:30.000 --> 00:15:33.280]   company that's hiring him doesn't know how good of an employee he is before he's
[00:15:33.280 --> 00:15:33.640]   hired.
[00:15:33.640 --> 00:15:37.480]   Um, so how do you, how do you deal with such scenarios when you're on the other
[00:15:37.480 --> 00:15:38.400]   side of the adverse selection?
[00:15:38.640 --> 00:15:39.200]   Yeah, certainly.
[00:15:39.200 --> 00:15:42.960]   I think in the car insurance situation, I am fairly sure there are now car
[00:15:42.960 --> 00:15:46.960]   insurances that essentially put like a, like a accelerometer and a GPS on your
[00:15:46.960 --> 00:15:51.680]   car and they essentially monitor how safely you drive or whatever, how jerkily
[00:15:51.680 --> 00:15:52.720]   you drive probably.
[00:15:52.720 --> 00:15:57.040]   And, uh, I imagine that, that you can sort of decrease your adverse selection
[00:15:57.040 --> 00:15:59.240]   by, by taking advantage of those kinds of things.
[00:15:59.240 --> 00:16:04.080]   Um, in the case of the, the employment thing, um, that's a tougher one.
[00:16:04.080 --> 00:16:07.720]   Um, at some level, the most important thing you can do is select your
[00:16:07.720 --> 00:16:10.320]   coworkers as, as a potential employee.
[00:16:10.320 --> 00:16:15.440]   And so getting really, really good at evaluating your interviewers, I think
[00:16:15.440 --> 00:16:19.480]   is, I think it's an undervalued skill, not so much because you want to tell
[00:16:19.480 --> 00:16:20.680]   like, are they good or not?
[00:16:20.680 --> 00:16:22.840]   But it's more like, are they a good fit for me?
[00:16:22.840 --> 00:16:24.080]   Is this company a good fit for me?
[00:16:24.080 --> 00:16:27.560]   And the best signal of whether the company is a good fit for you is who the
[00:16:27.560 --> 00:16:31.160]   people are that are interviewing you and what do they ask you to do?
[00:16:31.160 --> 00:16:35.760]   Um, if a company is at all sensible, what they ask you to do in the interview
[00:16:35.760 --> 00:16:37.480]   is highly correlated to what you do in the job.
[00:16:37.560 --> 00:16:42.320]   And so that's kind of maybe like a baseline don't, don't adverse select
[00:16:42.320 --> 00:16:45.960]   yourself by, by just kind of being like, meh, yeah, I think this will probably
[00:16:45.960 --> 00:16:50.320]   work out or, or perhaps more importantly, this is a high status company.
[00:16:50.320 --> 00:16:54.640]   I am told that it is a high status company and that letting that override
[00:16:54.640 --> 00:16:57.400]   your personal understanding of what the experience was, I think that
[00:16:57.400 --> 00:16:59.040]   happens very, very frequently.
[00:16:59.040 --> 00:17:02.840]   Um, so once you get past that, then you're probably in good shape already.
[00:17:02.840 --> 00:17:06.680]   And at that point, I think it just comes down to, you know, putting
[00:17:06.680 --> 00:17:07.840]   yourself in the right positions.
[00:17:07.840 --> 00:17:11.960]   And that, I think that's, um, that's maybe a, a skill that's, that,
[00:17:11.960 --> 00:17:13.600]   that you learn over time, hopefully.
[00:17:13.600 --> 00:17:14.080]   Yeah.
[00:17:14.080 --> 00:17:17.960]   So I, um, there's a common thing that my friends complain about who are
[00:17:17.960 --> 00:17:20.520]   programmers, which is that when they're interviewing, they get asked questions
[00:17:20.520 --> 00:17:23.360]   that are very unlike their actual job.
[00:17:23.360 --> 00:17:25.880]   So, you know, questions that are almost brain teasers.
[00:17:25.880 --> 00:17:29.920]   Um, but there's a kind of a Chesterson's fence argument that you can make that
[00:17:29.920 --> 00:17:32.200]   it's like, if all the tech companies are doing it, there must be some
[00:17:32.200 --> 00:17:33.240]   important reason why they are.
[00:17:33.440 --> 00:17:37.280]   So, um, have you figured out the reason why such brain teasers are so common?
[00:17:37.280 --> 00:17:40.040]   Is it just like, gee, it's so important that this is the best way to measure it?
[00:17:40.040 --> 00:17:41.360]   So this is the thing, right?
[00:17:41.360 --> 00:17:46.160]   The, the, the dirty secret of, of all of this stuff is that explicitly testing
[00:17:46.160 --> 00:17:50.560]   for IQ is illegal in the United States as a, as a, as an employment practice.
[00:17:50.560 --> 00:17:55.480]   Um, however, you can kind of drive a truck through it because companies do
[00:17:55.480 --> 00:17:58.400]   like, for example, wonder Lake is a company maybe people have heard of
[00:17:58.400 --> 00:18:01.120]   wonder Lake because it's the test they give quarterbacks in the NFL.
[00:18:01.480 --> 00:18:05.640]   Um, wonder Lake is, is a company that is dedicated, for example, to, to
[00:18:05.640 --> 00:18:10.720]   building employment testing that is essentially IQ testing, but has the,
[00:18:10.720 --> 00:18:14.720]   the, you know, whether it's a fig leaf or actually legitimate justification
[00:18:14.720 --> 00:18:18.440]   that as long as you can show that it is, uh, important for job performance,
[00:18:18.440 --> 00:18:19.880]   then you can kind of do the testing.
[00:18:19.880 --> 00:18:20.580]   Right.
[00:18:20.580 --> 00:18:24.240]   And so essentially, I feel like a lot of these brain teaser type
[00:18:24.240 --> 00:18:27.720]   questions are, as you say, you know, IQ tests disguised.
[00:18:28.160 --> 00:18:32.600]   Um, I think oftentimes they are badly misapplied by the interviewers.
[00:18:32.600 --> 00:18:36.040]   Like, I think it takes actually a lot of really, really hard training
[00:18:36.040 --> 00:18:39.040]   and, and experience to ask these sorts of questions in a way that
[00:18:39.040 --> 00:18:41.120]   gets you the signal you want.
[00:18:41.120 --> 00:18:43.960]   Um, but I think that's a, that's a big part of it.
[00:18:43.960 --> 00:18:51.480]   Like the, the extent to which you view your job as vocational, um, is, is the
[00:18:51.480 --> 00:18:54.080]   extent to which you're going to hate those brain teasers, right?
[00:18:54.080 --> 00:18:57.880]   Like, so if I'm a programmer and I want my job to be, I'm just going to write
[00:18:57.880 --> 00:19:00.400]   code all day and sit down and just write code, then you're not going to
[00:19:00.400 --> 00:19:03.160]   like those brain teasers because you don't think of them as part of your job.
[00:19:03.160 --> 00:19:06.920]   Whereas if you think of your job as a programmer, as somewhat more expansive
[00:19:06.920 --> 00:19:10.200]   in the sense of like, well, I'm here to really think about hard problems.
[00:19:10.200 --> 00:19:12.360]   And I happen to implement them in code.
[00:19:12.360 --> 00:19:15.520]   Then maybe you're going to think of the brain teasers as more correlated
[00:19:15.520 --> 00:19:16.920]   to the thing you want to be doing.
[00:19:16.920 --> 00:19:19.400]   So again, select for what you like.
[00:19:19.400 --> 00:19:20.240]   Yeah.
[00:19:20.240 --> 00:19:21.960]   And maybe it makes sense to select for the latter type
[00:19:21.960 --> 00:19:23.000]   of person as well, right?
[00:19:23.000 --> 00:19:26.560]   Or, or I don't know, which is preferable to hire, but, um,
[00:19:26.600 --> 00:19:29.160]   Well, so, so I think this is the thing about, about companies.
[00:19:29.160 --> 00:19:31.720]   Again, there's a lot of schizophrenia in tech hiring.
[00:19:31.720 --> 00:19:36.480]   Um, one of the things that's clear is everybody says they want to hire a
[00:19:36.480 --> 00:19:41.840]   players, um, but only a small fraction kind of by definition can hire those,
[00:19:41.840 --> 00:19:45.280]   those sort of high percentage or a high percentile kinds of people.
[00:19:45.280 --> 00:19:49.440]   And so what ends up happening is a lot of startups have the failure mode
[00:19:49.440 --> 00:19:52.840]   where they try to build these incredibly selective processes.
[00:19:53.160 --> 00:19:56.840]   Um, but the people who, who they really, really want are never
[00:19:56.840 --> 00:19:58.120]   going to accept their offers.
[00:19:58.120 --> 00:20:00.440]   They're going to go somewhere sort of more high status or
[00:20:00.440 --> 00:20:02.160]   more high paying in particular.
[00:20:02.160 --> 00:20:07.760]   Um, and so you try to select for like an 80th percentile person, but you
[00:20:07.760 --> 00:20:12.880]   end up selecting like a, like a set of 50th percentile person, people who
[00:20:12.880 --> 00:20:15.840]   look like 80th percentile people, which is really, really bad.
[00:20:15.840 --> 00:20:19.840]   And so what you should actually do as a startup is be very clear eyed and say,
[00:20:19.840 --> 00:20:24.640]   look, if I have a team of 10, I probably need one or two, like 90th percentile
[00:20:24.640 --> 00:20:29.280]   people, and I should evaluate for, and in particular pay for that.
[00:20:29.280 --> 00:20:33.000]   Uh, and then the rest, I should try to hire a kind of 40th percentile
[00:20:33.000 --> 00:20:36.160]   people and, you know, put them in situations where they can be effective.
[00:20:36.160 --> 00:20:39.760]   That's a much, much more cost-effective way and more
[00:20:39.760 --> 00:20:41.520]   stable way to build the company.
[00:20:41.520 --> 00:20:43.360]   But nobody wants to hear that.
[00:20:43.360 --> 00:20:45.080]   And nobody wants to build a company like that.
[00:20:45.080 --> 00:20:47.640]   That's a great example of like a barbell strategy.
[00:20:48.520 --> 00:20:52.360]   Um, um, so I'm wondering, do you have any ideas of what good arbitrage
[00:20:52.360 --> 00:20:54.120]   opportunities in tech hiring might be?
[00:20:54.120 --> 00:20:57.920]   I know, um, I think SpaceX, some of their early engineers were from the
[00:20:57.920 --> 00:21:02.120]   gaming industry because they're very used to, um, doing optimization problems there.
[00:21:02.120 --> 00:21:05.360]   But, um, it's, it's not necessarily traditionally a high status career.
[00:21:05.360 --> 00:21:09.760]   So, uh, there, there's like arbitrage that are, are you, do you have any ideas
[00:21:09.760 --> 00:21:12.600]   now of like, what is a good place you would be looking for really talented.
[00:21:12.600 --> 00:21:15.760]   Potential future programmers, if you were, if you couldn't compete
[00:21:15.760 --> 00:21:17.400]   with pay, uh, at Google or something.
[00:21:17.800 --> 00:21:18.080]   Yeah.
[00:21:18.080 --> 00:21:23.160]   So I think one of the things I always tell companies is, um, go more junior.
[00:21:23.160 --> 00:21:26.000]   Like if you look at, if you look at the salary of somebody who just comes out of
[00:21:26.000 --> 00:21:28.520]   school and I'm not talking about somebody who just came out of Stanford, I'm talking
[00:21:28.520 --> 00:21:32.040]   about somebody who just came out of like a reasonable CS program, right.
[00:21:32.040 --> 00:21:35.040]   And you look at their salary three years later, like it could
[00:21:35.040 --> 00:21:36.480]   be almost double sometimes, right.
[00:21:36.480 --> 00:21:38.080]   It's just a crazy, crazy jump.
[00:21:38.080 --> 00:21:41.040]   And that is kind of unjustified.
[00:21:41.040 --> 00:21:44.080]   I mean, you can sort of see the argument for it, but it's just like, there's
[00:21:44.080 --> 00:21:49.080]   definitely a kink at the two to three year point because every startup there,
[00:21:49.080 --> 00:21:53.080]   every, I mean, every tech company seems to want to have two years of experience.
[00:21:53.080 --> 00:21:58.080]   And a lot of it is because companies just don't want to, or can't see themselves
[00:21:58.080 --> 00:22:01.000]   investing in the training of those first two years.
[00:22:01.000 --> 00:22:04.520]   And if they do, they tell themselves, well, they're just going to leave after
[00:22:04.520 --> 00:22:07.120]   two years to go for a higher paying job somewhere else.
[00:22:07.120 --> 00:22:12.120]   Um, but I think those are terrible answers by and large to the problem.
[00:22:12.120 --> 00:22:15.080]   Like you should be investing in training your people.
[00:22:15.080 --> 00:22:18.040]   You also get the benefit of training them exactly the way you want.
[00:22:18.040 --> 00:22:22.600]   And if you put in that work, uh, and you think carefully about what it is
[00:22:22.600 --> 00:22:26.160]   that people are coming to work to do for you day to day, probably
[00:22:26.160 --> 00:22:27.160]   they're not going to leave, right?
[00:22:27.160 --> 00:22:29.760]   Like if, if you give them a reason to not leave, they're
[00:22:29.760 --> 00:22:30.760]   probably not going to leave.
[00:22:30.760 --> 00:22:32.880]   Switching jobs is incredibly costly and risky.
[00:22:32.880 --> 00:22:34.720]   People don't go out of their way to do so.
[00:22:34.720 --> 00:22:40.240]   So like, you're kind of, you're kind of getting the, uh, the, um, the
[00:22:40.240 --> 00:22:42.320]   inertia working in your favor anyway.
[00:22:42.320 --> 00:22:44.560]   So like, let's work on these things.
[00:22:44.560 --> 00:22:48.560]   Sounds very similar to the sheepskin effect of the last semester of college.
[00:22:48.560 --> 00:22:53.880]   Um, so the, it, uh, Brian Kaplan has a really good argument about this
[00:22:53.880 --> 00:22:56.160]   in the case of mid education, which is that the last semester of college,
[00:22:56.160 --> 00:23:00.240]   like boost your earnings many times more than the percentage of college
[00:23:00.240 --> 00:23:01.600]   you spend in that last semester.
[00:23:01.600 --> 00:23:04.200]   And it can't be because you're like learning that much more in the last
[00:23:04.200 --> 00:23:07.880]   semester, um, which I guess sets up an arbitrage opportunity for hiring
[00:23:07.880 --> 00:23:10.560]   people and like right before they're about to finish their last year or
[00:23:10.560 --> 00:23:13.600]   something, but you see, like, give me, like, I'll give you a perfect example
[00:23:13.600 --> 00:23:16.920]   here in San Diego where we're startups in San Diego tech tech companies in
[00:23:16.920 --> 00:23:19.080]   San Diego, love to hire into it.
[00:23:19.080 --> 00:23:23.000]   Employees that have two to three years experience because into it hires a
[00:23:23.000 --> 00:23:25.560]   bunch of people and they train them and they train them pretty well.
[00:23:25.560 --> 00:23:29.120]   And, and then like they get poached, but of course, like nobody really
[00:23:29.120 --> 00:23:32.920]   actually thinks about the idea that like into it knows who the good and
[00:23:32.920 --> 00:23:34.520]   the bad are after two years.
[00:23:34.520 --> 00:23:37.400]   And like, you're not seeing the really, really good ones into it.
[00:23:37.400 --> 00:23:38.280]   It's keeping those.
[00:23:38.280 --> 00:23:38.680]   Right.
[00:23:38.680 --> 00:23:43.520]   So, um, so you say in the book that you've traded over your long career in
[00:23:43.520 --> 00:23:46.600]   trading, you've traded all kinds of different financial instruments.
[00:23:46.600 --> 00:23:49.320]   I wonder, um, what, what is the reason?
[00:23:49.320 --> 00:23:53.760]   So is this just, um, I guess you, you just have to do the, well, you had to
[00:23:53.760 --> 00:23:59.080]   trade whatever, um, market that you have to at the moment, or cause I would
[00:23:59.080 --> 00:24:01.880]   think you say in the chapter on edge, that one of the ways you can actually
[00:24:01.880 --> 00:24:03.360]   get edge is to specialize.
[00:24:03.720 --> 00:24:09.320]   Um, so is it a mistake of firms to let their traders over their career trade
[00:24:09.320 --> 00:24:12.760]   in multiple different categories, or is that necessary in order to build
[00:24:12.760 --> 00:24:14.120]   your general aptitude as a trader?
[00:24:14.120 --> 00:24:14.920]   Yeah.
[00:24:14.920 --> 00:24:16.080]   So I think it's a balance.
[00:24:16.080 --> 00:24:18.960]   Um, certainly I don't think that again, it depends on how
[00:24:18.960 --> 00:24:20.240]   big the reference class is.
[00:24:20.240 --> 00:24:20.920]   Certainly.
[00:24:20.920 --> 00:24:25.120]   I have never done any trading that looks like look at a balance sheet in an
[00:24:25.120 --> 00:24:28.520]   income statement and listen to an earnings call and make a bet on that.
[00:24:28.520 --> 00:24:29.800]   Like that's sort of fundamental trading.
[00:24:29.800 --> 00:24:31.280]   I have never done any of that.
[00:24:31.640 --> 00:24:35.680]   Um, and I think it would be a pretty big mistake to put me in that situation.
[00:24:35.680 --> 00:24:39.760]   Um, but within, we'll say that the, the well-defined realm
[00:24:39.760 --> 00:24:41.480]   of like quantitative trading.
[00:24:41.480 --> 00:24:46.520]   Um, I think a lot of the same skillsets apply in different markets.
[00:24:46.520 --> 00:24:49.160]   Like you're, you're kind of built, bringing the same skillset to different
[00:24:49.160 --> 00:24:53.560]   markets and having that experience of going around and looking at different
[00:24:53.560 --> 00:24:58.400]   kinds of markets and how they work informs, like it sort of informs how you
[00:24:58.400 --> 00:25:02.240]   think about things and, and gives you that, that wider vision that I
[00:25:02.240 --> 00:25:03.840]   think makes you a better trader.
[00:25:03.840 --> 00:25:06.520]   Um, so yeah, I think it's a balance.
[00:25:06.520 --> 00:25:10.120]   So I think finance is 9% of GDP.
[00:25:10.120 --> 00:25:13.760]   So I understand the argument that, you know, finance helps allocate
[00:25:13.760 --> 00:25:17.080]   scarce, uh, resources, um, to where they're needed most.
[00:25:17.080 --> 00:25:23.320]   But, um, if we're giving up like a 10th of our resources to make the allocation
[00:25:23.320 --> 00:25:26.360]   of the rest of the resources more efficient, is that too high a price to be
[00:25:26.360 --> 00:25:28.080]   paying for a liquidity and price discovery?
[00:25:28.320 --> 00:25:30.280]   Um, so is finance too high a fraction of GDP?
[00:25:30.280 --> 00:25:34.680]   Um, I go back and forth on this question.
[00:25:34.680 --> 00:25:36.160]   Um, I really do.
[00:25:36.160 --> 00:25:41.480]   Um, because kind of when you see it from the inside, a lot of it is zero sum
[00:25:41.480 --> 00:25:46.960]   competition, um, and, and it feels like, come on, there's gotta be a
[00:25:46.960 --> 00:25:48.640]   more efficient way to do this.
[00:25:48.640 --> 00:25:53.880]   Um, but at the same time, kind of outside view, we haven't come up with
[00:25:53.880 --> 00:25:55.160]   a more efficient way to do this.
[00:25:55.680 --> 00:25:58.080]   Um, and it's hard to argue with GDP growth.
[00:25:58.080 --> 00:26:01.120]   And so I kind of go back and forth on it.
[00:26:01.120 --> 00:26:01.680]   Certainly.
[00:26:01.680 --> 00:26:06.200]   I think the other thing about it is, um, there's two countervailing forces.
[00:26:06.200 --> 00:26:09.720]   You can, you can sort of be inside something and be really, really familiar
[00:26:09.720 --> 00:26:14.080]   with it and just your act, the act of being very, very familiar with something
[00:26:14.080 --> 00:26:17.360]   just gives it legitimacy kind of automatically.
[00:26:17.360 --> 00:26:22.360]   Um, but at the same time, like if you look at something from afar, you're like,
[00:26:22.360 --> 00:26:23.520]   oh, that's ridiculous, right?
[00:26:23.520 --> 00:26:26.160]   Like that's, that's not, that's not a thing that should exist.
[00:26:26.160 --> 00:26:26.600]   Right.
[00:26:26.600 --> 00:26:30.120]   And so it's just sort of this perverse thing where the people, most like the
[00:26:30.120 --> 00:26:32.840]   most well-informed people, the people who really could or should be making
[00:26:32.840 --> 00:26:36.080]   these decisions about like, is this a legitimate thing that we should be doing?
[00:26:36.080 --> 00:26:40.760]   Are biased towards thinking like, um, yeah, you know what, this is probably
[00:26:40.760 --> 00:26:43.040]   a good thing to be doing, or there's value to this.
[00:26:43.040 --> 00:26:49.880]   And so it's, it's hard to sort of disentangle the, the, like the experience
[00:26:49.880 --> 00:26:52.560]   and, and the biases that that experience sort of gives you.
[00:26:53.040 --> 00:26:57.600]   And then would that, would that fraction shrink without, without harming
[00:26:57.600 --> 00:27:02.640]   efficiency, if, um, like are there inefficiencies created by government
[00:27:02.640 --> 00:27:06.000]   regulation or by restrictions on capital flow?
[00:27:06.000 --> 00:27:10.280]   Um, or is that like basically what you should expect it to be even in a free
[00:27:10.280 --> 00:27:13.640]   market or in an, in an optimally regulated market, let's say.
[00:27:13.640 --> 00:27:15.160]   That's also a tough one.
[00:27:15.160 --> 00:27:19.640]   Um, and, and it's, and it's not that I haven't, uh, thought a lot about these.
[00:27:19.640 --> 00:27:23.120]   It's just, I feel like I don't have, um, I don't have a great answer.
[00:27:23.120 --> 00:27:24.960]   Like at the margin, what would I like?
[00:27:24.960 --> 00:27:28.320]   If you, if you sort of made me like regular regulator of the world, like at
[00:27:28.320 --> 00:27:29.360]   the margin, what would I do?
[00:27:29.360 --> 00:27:31.800]   Um, there are some things that I would regulate more.
[00:27:31.800 --> 00:27:35.440]   Um, and this is probably going to be a very unpopular opinion among my, my
[00:27:35.440 --> 00:27:39.200]   financial friends, but like, I think leverage DTF should be banned for, from
[00:27:39.200 --> 00:27:39.920]   retail trading.
[00:27:39.920 --> 00:27:43.920]   Like, I think this, there's just kind of a bad instrument, uh, in particular,
[00:27:43.920 --> 00:27:45.440]   like all the volatility products.
[00:27:45.880 --> 00:27:49.120]   Um, so I feel like that should probably be regulated some more.
[00:27:49.120 --> 00:27:53.840]   Um, but at the same time, the sort of qualified investor status thing that
[00:27:53.840 --> 00:27:58.000]   people are driving a truck through, like that seems weird, like should, should
[00:27:58.000 --> 00:28:02.200]   there be, should we just eliminate the qualified investor, uh, status and let
[00:28:02.200 --> 00:28:03.560]   people invest in whatever they want?
[00:28:03.560 --> 00:28:05.960]   Or should we make it even more restrictive?
[00:28:05.960 --> 00:28:08.920]   Um, I'm not sure about that one.
[00:28:08.920 --> 00:28:14.960]   Um, and certainly the other thing about it is like, um, A lot of the
[00:28:14.960 --> 00:28:18.240]   regulations, especially around capital requirements for banks are incredibly
[00:28:18.240 --> 00:28:23.320]   Baroque and they feel like job Ponzi's a lot of the time, like we need to figure
[00:28:23.320 --> 00:28:26.240]   out a way to employ all these people and like, okay, we're just going to create
[00:28:26.240 --> 00:28:29.680]   like Basel three and that's going to be like an extra thousand employees for
[00:28:29.680 --> 00:28:31.000]   every large bank in the world.
[00:28:31.000 --> 00:28:37.520]   Um, that's probably kind of a deadweight loss, but, but doing things more simply
[00:28:37.520 --> 00:28:40.920]   doesn't seem like it's going to get you the thing like this sort of the
[00:28:40.920 --> 00:28:42.120]   stability outcomes you want.
[00:28:42.120 --> 00:28:46.160]   And so, yeah, it's just, I feel like it's just kind of poor trade-offs all around.
[00:28:46.160 --> 00:28:50.560]   What is the long run future of trading firms look like?
[00:28:50.560 --> 00:28:56.560]   So if, um, if economic growth continues to stay low, then you would expect like
[00:28:56.560 --> 00:28:59.400]   other financial instruments to stop growing at high rates as well.
[00:28:59.400 --> 00:29:04.720]   But even if economic rates, uh, economic growth speeds up, um, if markets get
[00:29:04.720 --> 00:29:08.440]   more efficient over time, then again, you would expect the profits that any one
[00:29:08.440 --> 00:29:11.560]   trading firm can get, um, to decrease.
[00:29:11.680 --> 00:29:15.240]   So is there a future for highly profitable, uh, trade firms like
[00:29:15.240 --> 00:29:16.680]   Jane Street, like in the far future?
[00:29:16.680 --> 00:29:19.560]   So I think to the extent that Jane Street and companies like it
[00:29:19.560 --> 00:29:20.680]   provide a service to the world.
[00:29:20.680 --> 00:29:23.000]   And I really do think they provide a service to the world.
[00:29:23.000 --> 00:29:25.720]   Then they're going to be around and they're going to be profitable.
[00:29:25.720 --> 00:29:30.360]   Now, are they going to gain, um, like we'll call them excess returns?
[00:29:30.360 --> 00:29:35.360]   Um, even that's not so obvious because the thing about trading firms is
[00:29:35.360 --> 00:29:37.720]   especially market makers and that sort of thing, like most of the time, the
[00:29:37.720 --> 00:29:40.560]   business is pretty good if you're really good at it.
[00:29:41.000 --> 00:29:45.160]   Um, but sometimes it's really good, like when, when there's lots of market
[00:29:45.160 --> 00:29:46.360]   volatility and that sort of thing.
[00:29:46.360 --> 00:29:51.360]   Um, but that's precisely because you are the person, you are the entity
[00:29:51.360 --> 00:29:54.120]   that is willing to take the risks that nobody else is willing to take.
[00:29:54.120 --> 00:29:59.360]   And to the extent that we're going to still continue to have volatility in
[00:29:59.360 --> 00:30:03.240]   terms of either like market volatility or, you know, economic downturns or
[00:30:03.240 --> 00:30:07.920]   whatever, there's always going to be, um, a service that these, that
[00:30:07.920 --> 00:30:09.080]   these companies are going to provide.
[00:30:09.520 --> 00:30:12.160]   Now, over the long run, I feel like probably there's going
[00:30:12.160 --> 00:30:13.880]   to be more consolidation.
[00:30:13.880 --> 00:30:17.760]   It seems unlikely to, to, to, to stop.
[00:30:17.760 --> 00:30:23.360]   Um, just because you sort of gain the benefits of the economies of
[00:30:23.360 --> 00:30:24.840]   scale just kind of keep going up.
[00:30:24.840 --> 00:30:29.360]   Um, but then again, you have sort of new things that come up like crypto
[00:30:29.360 --> 00:30:32.120]   and that sort of thing, where like it's the wild west right now, and there's
[00:30:32.120 --> 00:30:34.720]   going to be like a big consolidation over the next 10 years, I think
[00:30:34.720 --> 00:30:36.080]   that's the natural arc of things.
[00:30:36.080 --> 00:30:36.960]   Oh, interesting.
[00:30:37.000 --> 00:30:38.480]   So, um, yeah.
[00:30:38.480 --> 00:30:40.640]   Can you describe what these economies of scale look like in finance?
[00:30:40.640 --> 00:30:45.400]   And, um, um, and then what is a trade off where if you're like too big and
[00:30:45.400 --> 00:30:49.680]   it's not even worth your time to like look at smaller, uh, smaller investments
[00:30:49.680 --> 00:30:51.920]   where you can't take as a big estate without moving the market?
[00:30:51.920 --> 00:30:52.680]   Yeah.
[00:30:52.680 --> 00:30:56.280]   So the thing about finance or like market making trading in general
[00:30:56.280 --> 00:30:59.280]   is, um, it's very labor intensive, right?
[00:30:59.280 --> 00:31:02.120]   So you should think of it almost like the value of a seat or
[00:31:02.120 --> 00:31:03.480]   the value of a person's time.
[00:31:04.040 --> 00:31:07.240]   And so are there going to be, are there going to be inefficiencies in the
[00:31:07.240 --> 00:31:09.880]   market, like pockets in the, you know, pink sheets or something where it's
[00:31:09.880 --> 00:31:13.120]   just not worth a large companies or a large successful companies
[00:31:13.120 --> 00:31:14.400]   trader time to look at?
[00:31:14.400 --> 00:31:14.720]   Yes.
[00:31:14.720 --> 00:31:19.680]   Like those will always exist and they'll get slowly competed away by, by the, by
[00:31:19.680 --> 00:31:23.920]   the mom and pop trading operations, or, or even just the, like the former
[00:31:23.920 --> 00:31:27.480]   Jane street traders who are now at home and kind of doing it on their own for fun.
[00:31:27.480 --> 00:31:30.960]   Um, so I think those will always kind of be there.
[00:31:31.800 --> 00:31:35.040]   Is there a potential that markets can get like way, way more efficient if, uh, we
[00:31:35.040 --> 00:31:40.480]   have, we develop much stronger AI and, um, and at what point will, um, the work
[00:31:40.480 --> 00:31:45.280]   that even traders do, that's like much more, um, I don't know, much more model
[00:31:45.280 --> 00:31:48.680]   generation and like thinking abstractly at what point can that even get automated
[00:31:48.680 --> 00:31:51.040]   away and not just like the road calculations.
[00:31:51.040 --> 00:31:51.760]   Yeah.
[00:31:51.760 --> 00:31:56.240]   I would say it's already getting and gotten like more efficient.
[00:31:56.560 --> 00:32:02.760]   Like when, when my former boss started the idea of an options market maker,
[00:32:02.760 --> 00:32:05.840]   having 10 stocks that they were market makers in was like, that
[00:32:05.840 --> 00:32:07.640]   was kind of the limit, right?
[00:32:07.640 --> 00:32:11.560]   When I was doing it, like we could handle like a hundred stocks, right.
[00:32:11.560 --> 00:32:12.840]   Market-making and a hundred stocks.
[00:32:12.840 --> 00:32:16.240]   Again, technology just made technology just made everything more efficient
[00:32:16.240 --> 00:32:18.600]   or more efficient in human time.
[00:32:18.600 --> 00:32:20.160]   Um, that will continue.
[00:32:20.160 --> 00:32:23.520]   Like you can, you can sort of set up things where I'm looking at some
[00:32:23.520 --> 00:32:26.400]   data and I can like run a bunch of different models and just select the
[00:32:26.400 --> 00:32:29.200]   good ones and make sure that I'm not overfitting because I have all, I have
[00:32:29.200 --> 00:32:30.480]   all these overfitting predictions.
[00:32:30.480 --> 00:32:33.760]   This is all stuff that you can do now that maybe you couldn't do 20 years ago.
[00:32:33.760 --> 00:32:35.200]   That will definitely happen.
[00:32:35.200 --> 00:32:40.400]   I think when people talk about AI and trading, I think it's, um, it's very hard
[00:32:40.400 --> 00:32:42.320]   to, it, like we have to define terms.
[00:32:42.320 --> 00:32:45.960]   I think that's the hard part is defining terms when we talk about AI, because if
[00:32:45.960 --> 00:32:51.280]   we talk about if like, if you ask, um, a reasonably aware person, what AI means,
[00:32:51.280 --> 00:32:55.080]   not probably today in 2022, 90% of people are going to say, oh, we're
[00:32:55.080 --> 00:32:56.360]   talking about large language models.
[00:32:56.360 --> 00:32:57.920]   Of course, that's what AI is.
[00:32:57.920 --> 00:32:58.400]   Right.
[00:32:58.400 --> 00:33:03.360]   And so is the question like, is GPT, is GPT and going to be a
[00:33:03.360 --> 00:33:05.240]   significant force in, in markets?
[00:33:05.240 --> 00:33:07.360]   Like, I'm honestly kind of skeptical about that.
[00:33:07.360 --> 00:33:11.240]   I don't know that, that the let's just keep making larger transformers is the
[00:33:11.240 --> 00:33:14.400]   way that we're going to get to AI, but that's my personal parochial opinion.
[00:33:14.400 --> 00:33:21.920]   But if we think of AI more broadly as, um, as slowly, but surely, uh, increasing
[00:33:21.920 --> 00:33:25.960]   the range of things that things that machines can do that humans can do,
[00:33:25.960 --> 00:33:29.280]   like the, the more we sort of creep into the things that humans can do, that
[00:33:29.280 --> 00:33:34.000]   machines can do as well, then, then yeah, then, then like the, the human part is
[00:33:34.000 --> 00:33:36.600]   going to slowly start to get, uh, disappeared away.
[00:33:36.600 --> 00:33:41.120]   Um, I think the, the, the natural analogy is what happened in the 20th century
[00:33:41.120 --> 00:33:45.280]   with manufacturing where like, it used to be kind of all human power and a little
[00:33:45.280 --> 00:33:49.520]   bit of machine power where you had kind of this like big central, like why did
[00:33:49.520 --> 00:33:52.680]   factories in the 19th century and early 20th century, why were they kind of tall
[00:33:52.680 --> 00:33:53.000]   and thin?
[00:33:53.000 --> 00:33:55.800]   Well, it's because they had one steam plant and they had to like all these
[00:33:55.800 --> 00:33:59.200]   belts and stuff to like, use the power from that one steam plant.
[00:33:59.200 --> 00:33:59.480]   Right.
[00:33:59.480 --> 00:34:00.800]   And then like electric motors happen.
[00:34:00.800 --> 00:34:03.280]   And it's like, okay, now factories are horizontal.
[00:34:03.280 --> 00:34:03.680]   Right.
[00:34:03.680 --> 00:34:09.240]   But over time, the trend is for it to be sort of less human power and more machine
[00:34:09.240 --> 00:34:09.480]   power.
[00:34:09.480 --> 00:34:11.480]   And I think the, the analogy is perfect.
[00:34:11.480 --> 00:34:16.080]   I think AI over time is going to take more and more of that sort of cognitive
[00:34:16.080 --> 00:34:17.280]   load from the human.
[00:34:17.280 --> 00:34:19.640]   Um, that seems inevitable to me.
[00:34:19.720 --> 00:34:25.600]   So I'm curious why you're skeptical, um, uh, that, uh, like a scaled up, uh, GPT
[00:34:25.600 --> 00:34:27.640]   three or other language, uh, large language model.
[00:34:27.640 --> 00:34:29.920]   Um, uh, I'm curious.
[00:34:29.920 --> 00:34:33.880]   Uh, so why does it not have applicability, um, in financial markets?
[00:34:33.880 --> 00:34:37.400]   Uh, like, I know there's like a toy version where you have like GPT 10 and
[00:34:37.400 --> 00:34:38.840]   you ask it to complete the sentence.
[00:34:38.840 --> 00:34:43.400]   The best trade I can make today is, and then, um, so why, why is that unlikely to
[00:34:43.400 --> 00:34:43.880]   happen?
[00:34:43.880 --> 00:34:45.720]   So there's a couple of things that I might say.
[00:34:45.720 --> 00:34:48.000]   One is, is the concept of sample efficiency.
[00:34:48.480 --> 00:34:53.320]   Like these things are incredibly sample inefficient in a way that the way the
[00:34:53.320 --> 00:34:54.480]   humans learn are not.
[00:34:54.480 --> 00:34:57.800]   And so there's something fundamental there that, that we're not getting.
[00:34:57.800 --> 00:34:58.600]   Right.
[00:34:58.600 --> 00:35:03.720]   And the thing that I think we're not getting is, um, is the things that our
[00:35:03.720 --> 00:35:08.960]   brains have, which are structures for a semantic understanding, like to the extent
[00:35:08.960 --> 00:35:12.200]   that, that these large language models have semantic understanding, it's kind of
[00:35:12.200 --> 00:35:13.280]   by accident, right?
[00:35:13.280 --> 00:35:15.640]   It's just like, it's the clever Hans thing, right?
[00:35:15.640 --> 00:35:18.720]   It's just like a super clever Hans and it's super impressive.
[00:35:18.720 --> 00:35:22.440]   And I'm not criticizing the models, like they're incredibly impressive, but it's
[00:35:22.440 --> 00:35:23.480]   still a clever Hans thing.
[00:35:23.480 --> 00:35:30.400]   Um, and so there surely must be a better architecture out there, much like our
[00:35:30.400 --> 00:35:35.160]   brains have these sort of architectures that, um, that sort of specialize in
[00:35:35.160 --> 00:35:39.360]   certain things that, that give these, these machines like semantic understanding,
[00:35:39.360 --> 00:35:43.640]   or at least give them the potential to have semantic understanding, um, that I
[00:35:43.640 --> 00:35:46.400]   don't think GPT-3 certainly has, has evidenced.
[00:35:46.400 --> 00:35:50.000]   Uh, so Jane Street seems like a mysterious place, but what's interesting
[00:35:50.000 --> 00:35:54.320]   to me is there's seems to be a large overlap with the rationality and EA
[00:35:54.320 --> 00:35:54.960]   community.
[00:35:54.960 --> 00:35:56.720]   So obviously you have Sam McInfried.
[00:35:56.720 --> 00:35:59.840]   He's, um, you know, he, he went into Jane Street with the explicit
[00:35:59.840 --> 00:36:01.080]   goal of earning to give.
[00:36:01.080 --> 00:36:06.480]   Tyler Cowen announced that $20 million have been donated to, um, his
[00:36:06.480 --> 00:36:09.360]   emergent ventures grant program from Jane Street traders.
[00:36:09.360 --> 00:36:13.280]   And, you know, even reading your book, like you reference so many thinkers that
[00:36:13.280 --> 00:36:17.280]   are prominent in, um, like rationality spears.
[00:36:17.280 --> 00:36:23.800]   And, um, so there's just to be a big overlap with this community and with at
[00:36:23.800 --> 00:36:27.040]   least a part of the trading world that I'm familiar with now that could just be
[00:36:27.040 --> 00:36:30.160]   selection selection effects, but what, what is going on here?
[00:36:30.160 --> 00:36:31.360]   Yeah, that's a great question.
[00:36:31.360 --> 00:36:36.560]   I think, uh, maybe at two levels, one is the idea of being very rational and not
[00:36:36.560 --> 00:36:40.720]   fooling yourself and, um, and to use a Yudkowsky term, just shut up and multiply.
[00:36:41.000 --> 00:36:44.720]   Like, I think that that is a, that is a thing that is very common, I think
[00:36:44.720 --> 00:36:49.880]   in the two circles, or at least probably it should be, um, like try to really
[00:36:49.880 --> 00:36:55.040]   understand the real world and it matters to do so and doing so using kind of
[00:36:55.040 --> 00:36:57.960]   rational, mathematical, logical approaches.
[00:36:57.960 --> 00:37:02.080]   I think there's a lot of overlap just inherently there, but I think you could
[00:37:02.080 --> 00:37:06.400]   say that about any number of finance, wall street, whatever trading firms.
[00:37:06.400 --> 00:37:09.440]   I think the one thing that Jane Street has going for it differentially from
[00:37:09.440 --> 00:37:13.920]   those other firms maybe is, uh, the, a culture of collegiality.
[00:37:13.920 --> 00:37:17.120]   I think that's kind of an important thing that, that Jane Street has
[00:37:17.120 --> 00:37:19.640]   developed over the years and continues, I think, to have.
[00:37:19.640 --> 00:37:23.800]   Um, and so I think that's, there's a lot of overlap there, but like, it's
[00:37:23.800 --> 00:37:26.680]   the kind of place that if you are an EA person that thinks about things
[00:37:26.680 --> 00:37:31.040]   rationally and just enjoys the, enjoys the process of kind of this collegiality
[00:37:31.040 --> 00:37:35.120]   and, and, and working with people and thinking interesting thoughts together,
[00:37:35.120 --> 00:37:37.480]   Jane Street is going to be a very natural fit for you.
[00:37:38.160 --> 00:37:40.280]   Um, and I think maybe that's some of it too.
[00:37:40.280 --> 00:37:45.920]   When I had Berne Hobart on the podcast, um, we talked about whether debugging
[00:37:45.920 --> 00:37:50.280]   or finance was a better application of like rationality principles, because
[00:37:50.280 --> 00:37:52.160]   in each case you had to like update your beliefs and so on.
[00:37:52.160 --> 00:37:56.760]   And one interesting point he brought up was, um, uh, in finance you have, you
[00:37:56.760 --> 00:38:01.320]   not only have to model like a static system, um, as, as you would have in
[00:38:01.320 --> 00:38:05.560]   debugging, but you also have to model other agents and their incentives and
[00:38:05.560 --> 00:38:09.680]   their motivations, which makes it a much more like a dynamic system to get a hold
[00:38:09.680 --> 00:38:14.160]   of in your brain, which I guess it could even mean that like the tools are like
[00:38:14.160 --> 00:38:17.360]   the current rationality movement are not good enough to, uh, you know, be able to
[00:38:17.360 --> 00:38:20.040]   think about those things as well as probably you guys that have natively
[00:38:20.040 --> 00:38:21.280]   developed in the industry.
[00:38:21.280 --> 00:38:22.120]   Yeah.
[00:38:22.120 --> 00:38:25.240]   And look, I, the, the cross-pollination goes both ways.
[00:38:25.240 --> 00:38:30.280]   Um, but yeah, the, the idea of, of you being an agent in the world you're
[00:38:30.280 --> 00:38:32.640]   trying to study is fundamental in trading.
[00:38:32.640 --> 00:38:36.680]   Um, and it makes it like so much more interesting.
[00:38:36.680 --> 00:38:40.280]   I think that's one of the, getting back to the AI thing, just cause it occurs to
[00:38:40.280 --> 00:38:45.280]   me as one of the big failure modes is to think, think that, okay, well, yeah, I'm
[00:38:45.280 --> 00:38:48.480]   just going to like throw some AI and, or machine learning or something at this
[00:38:48.480 --> 00:38:53.480]   data set, and I'm going to get a trading strategy and okay, that's great.
[00:38:53.480 --> 00:38:56.280]   Like, let's say you, you've figured out something that predicts the price
[00:38:56.280 --> 00:39:00.240]   movement 55% of the time, like that thing can still actually lose a lot
[00:39:00.240 --> 00:39:04.680]   of money in production because of the, again, so there's the adverse selection
[00:39:04.680 --> 00:39:07.480]   effect of you're only going to do a small fraction of the good trades and you're
[00:39:07.480 --> 00:39:08.880]   going to do all the bad trades you want.
[00:39:08.880 --> 00:39:14.240]   Um, but also if you are actually making money at it, this is like a big shining
[00:39:14.240 --> 00:39:15.320]   signal to the rest of the world.
[00:39:15.320 --> 00:39:16.760]   Like, Hey, there's money over here.
[00:39:16.760 --> 00:39:18.440]   Like, why don't you compete it away?
[00:39:18.440 --> 00:39:22.760]   Um, and so, yeah, that's definitely a huge component of it.
[00:39:22.760 --> 00:39:27.200]   So you have a very interesting chapter on software and technology in the book.
[00:39:27.200 --> 00:39:30.080]   And one of the things you argue for is that we should take the concept
[00:39:30.080 --> 00:39:33.280]   of technical debt seriously in a financial sense.
[00:39:33.280 --> 00:39:39.280]   Um, so is one implication of this interpretation that you should be
[00:39:39.280 --> 00:39:43.400]   willing to accept a technical debt more if you're a rapidly growing company?
[00:39:43.400 --> 00:39:46.040]   Because you know, if like you're a startup that's growing fast, it makes
[00:39:46.040 --> 00:39:48.840]   sense to maybe take out a lot of loans because you can pay back the
[00:39:48.840 --> 00:39:49.920]   interest plus way more.
[00:39:49.920 --> 00:39:55.560]   Um, but maybe, maybe if you don't take it financially, maybe that's, you
[00:39:55.560 --> 00:39:58.400]   would think that if you're like scaling rapidly, that's the worst time to take
[00:39:58.400 --> 00:40:01.320]   on all the technical debt because you're just going to be hampered the entire way
[00:40:01.320 --> 00:40:01.800]   along.
[00:40:01.800 --> 00:40:02.680]   So yeah.
[00:40:02.680 --> 00:40:06.560]   So more generally, the question is what kinds of firms, um, should be more
[00:40:06.560 --> 00:40:08.080]   willing to take on technical debt?
[00:40:08.080 --> 00:40:11.800]   Yeah, certainly startups is, is the classic example and it, and it's, and
[00:40:11.800 --> 00:40:13.520]   it's non-recourse debt, right?
[00:40:13.520 --> 00:40:16.440]   Like if it goes belly up, like you don't have to pay it back, right?
[00:40:16.440 --> 00:40:16.960]   You're done.
[00:40:16.960 --> 00:40:20.960]   Um, so, so yeah, like startups should definitely do this and you see it all
[00:40:20.960 --> 00:40:21.400]   the time, right?
[00:40:21.400 --> 00:40:24.440]   This concept of, of an MVP where, you know, let's just get something out there.
[00:40:24.440 --> 00:40:29.200]   Let's get some feedback from the users with the understanding that hopefully
[00:40:29.200 --> 00:40:32.640]   with the understanding that you're going to have to essentially rewrite it from
[00:40:32.640 --> 00:40:33.120]   scratch.
[00:40:33.120 --> 00:40:38.800]   If it's successful, I think it's a very useful and very, very, um, uh, productive
[00:40:38.800 --> 00:40:40.240]   way to do software startups.
[00:40:40.240 --> 00:40:44.440]   Um, because yeah, like the, the, the implied interest rate that you're
[00:40:44.440 --> 00:40:48.800]   willing to pay is incredibly high, um, larger companies.
[00:40:48.800 --> 00:40:49.840]   It's interesting.
[00:40:49.840 --> 00:40:53.200]   Like if you ask yourself, this is a kind of a conversation I had with, with, uh,
[00:40:53.240 --> 00:40:57.200]   with one of my good friends, um, who I actually did consulting with.
[00:40:57.200 --> 00:40:58.920]   He worked at Qualcomm for a lot of years.
[00:40:58.920 --> 00:41:01.200]   And I asked him, cause he worked very closely with Microsoft.
[00:41:01.200 --> 00:41:04.560]   Like Microsoft employs tens of thousands of software engineers.
[00:41:04.560 --> 00:41:06.680]   Like what do they do all day?
[00:41:06.680 --> 00:41:11.160]   And he said to me, like, look, I don't actually know for a fact, but I'm pretty
[00:41:11.160 --> 00:41:15.520]   sure the vast majority of them are like, well, this library is deprecated.
[00:41:15.520 --> 00:41:16.840]   We need to upgrade this thing.
[00:41:16.840 --> 00:41:20.040]   Let's change like all this like code and all these different little places.
[00:41:20.040 --> 00:41:20.200]   Right.
[00:41:20.200 --> 00:41:26.000]   So like, there's just sort of, uh, uh, like a, like a, like a, sort of an
[00:41:26.000 --> 00:41:30.480]   archeology of software that occurs where, where, you know, if you build, if
[00:41:30.480 --> 00:41:33.760]   you'd been building a software piece software for like 20 some odd years,
[00:41:33.760 --> 00:41:37.240]   like there's just all this cruft in there that you're just continually
[00:41:37.240 --> 00:41:41.400]   trying to maintain so that it's functional as you go from, you know,
[00:41:41.400 --> 00:41:44.480]   this OS to this other OS, to the cloud, to whatever, right.
[00:41:44.480 --> 00:41:49.480]   Um, so I think that's, that's kind of, uh, like an accumulated debt
[00:41:49.480 --> 00:41:50.800]   that, that large companies certainly have.
[00:41:50.800 --> 00:41:51.640]   Yeah.
[00:41:51.640 --> 00:41:52.200]   That's so interesting.
[00:41:52.200 --> 00:41:53.320]   They're just like servicing the debt.
[00:41:53.320 --> 00:41:55.400]   They accumulated in like the eighties and nineties when they were growing
[00:41:55.400 --> 00:41:58.920]   rapidly and you can even think of like them moving to a new platform or like
[00:41:58.920 --> 00:42:01.160]   rewriting their code as like refinancing their debt or something.
[00:42:01.160 --> 00:42:01.440]   Right.
[00:42:01.440 --> 00:42:01.960]   Exactly.
[00:42:01.960 --> 00:42:07.240]   In fact, like I would say, um, probably the best, probably the best book I have
[00:42:07.240 --> 00:42:10.920]   ever read about software development is actually a science fiction.
[00:42:10.920 --> 00:42:17.800]   Um, Verner Vinge, um, a deepness in the sky, I feel like is, uh, very
[00:42:18.080 --> 00:42:21.800]   crucially about like, it sort of takes this idea, like, what if we've been
[00:42:21.800 --> 00:42:25.680]   building on the same software stack for 6,000 years, what does that look like?
[00:42:25.680 --> 00:42:27.040]   Like, what does that world look like?
[00:42:27.040 --> 00:42:30.720]   Um, and I think it teaches us a lot about how to think about large
[00:42:30.720 --> 00:42:32.800]   software projects, large, long-term software projects.
[00:42:32.800 --> 00:42:33.600]   Yeah.
[00:42:33.600 --> 00:42:36.760]   So I'm super interested in how you guys think about software
[00:42:36.760 --> 00:42:38.600]   in the financial industry.
[00:42:38.600 --> 00:42:41.840]   Um, I know Jane street uses, Oh, camel.
[00:42:41.840 --> 00:42:46.840]   Um, so, cause I mean, uh, there's like safety, you can tell me more why this
[00:42:46.840 --> 00:42:49.520]   is, but from what I understand, it's like, there's, um, there's more safety
[00:42:49.520 --> 00:42:51.600]   in a functional, uh, functional programming language.
[00:42:51.600 --> 00:42:52.320]   Um, yeah.
[00:42:52.320 --> 00:42:58.040]   So how do you think about like, obviously there's much more reason to want to have
[00:42:58.040 --> 00:43:00.880]   like safe code because you're dealing with an adversary there in some sense.
[00:43:00.880 --> 00:43:04.440]   So, uh, yeah, I'm curious, like, how do you guys make engineering decisions and
[00:43:04.440 --> 00:43:06.360]   what are the, like the trade-offs involved when you're doing, when
[00:43:06.360 --> 00:43:07.120]   you're working in finance?
[00:43:07.120 --> 00:43:07.960]   Yeah.
[00:43:07.960 --> 00:43:11.440]   So, uh, as you said, like Jane Street uses, Oh, camel.
[00:43:11.440 --> 00:43:15.600]   I think one of the, one of the biggest advantages of using that language is it,
[00:43:15.600 --> 00:43:17.440]   it is strongly and statically typed.
[00:43:17.440 --> 00:43:22.120]   And so you can put a lot of things, um, in the, like you can use the type
[00:43:22.120 --> 00:43:25.800]   system to make, uh, impossible States on representable.
[00:43:25.800 --> 00:43:28.040]   This is like a really good software engineering thing you should do.
[00:43:28.040 --> 00:43:32.960]   And it makes it sort of very easy and, um, in rich environment to do that in.
[00:43:32.960 --> 00:43:37.560]   And so this like, Oh, I didn't know I had to handle this explode
[00:43:37.560 --> 00:43:39.440]   problem is kind of minimized.
[00:43:39.440 --> 00:43:44.040]   Um, but yeah, like, you know, Jane Street and companies like it obviously
[00:43:44.040 --> 00:43:48.720]   optimized for avoiding hot loops in code that incinerate money really, really fast.
[00:43:48.720 --> 00:43:52.720]   And that is not what your average, whatever SAS startup optimizes
[00:43:52.720 --> 00:43:54.680]   for, uh, or it shouldn't be anyway.
[00:43:54.680 --> 00:43:58.560]   Um, but I, but the thing I keep coming back to and talking to, you know,
[00:43:58.560 --> 00:44:01.920]   technology leaders and that sort of thing is software development is
[00:44:01.920 --> 00:44:06.800]   fundamentally an exercise in sociology, like in organizing teams and in
[00:44:06.800 --> 00:44:13.680]   creating, uh, processes and culture and conventions, uh, around the building
[00:44:13.680 --> 00:44:17.760]   of software, like, you know, software development is fundamentally the
[00:44:17.760 --> 00:44:20.720]   management of complexity, like the science of managing complexity, because
[00:44:20.720 --> 00:44:22.080]   it is incredibly complex.
[00:44:22.080 --> 00:44:22.400]   Right.
[00:44:22.400 --> 00:44:27.360]   Um, and so all that sociological stuff ends up being some of the most
[00:44:27.360 --> 00:44:28.960]   important stuff to think about.
[00:44:28.960 --> 00:44:31.960]   Now that you're working in finance, but you have a startup.
[00:44:31.960 --> 00:44:34.880]   So you have to think very carefully about this, um, trade-off.
[00:44:34.880 --> 00:44:38.560]   Um, how, like, how are you managing this given that you have to like, I guess
[00:44:38.560 --> 00:44:40.240]   move fast, but you also need to be safe.
[00:44:41.240 --> 00:44:45.800]   Uh, hire really, really good people, honestly, like don't skimp on those
[00:44:45.800 --> 00:44:51.200]   first few employees is, is, uh, I think a really important thing, like where,
[00:44:51.200 --> 00:44:54.600]   where the bar is kind of, uh, like the bar is kind of weird.
[00:44:54.600 --> 00:44:57.280]   Like, it's not, it's not like there's sort of one total ordering
[00:44:57.280 --> 00:44:58.400]   over a quality of engineer.
[00:44:58.400 --> 00:44:58.600]   Right.
[00:44:58.600 --> 00:45:00.200]   There's like, they're incredibly multivariate.
[00:45:00.200 --> 00:45:08.280]   Um, but certainly, um, one really, really good, thoughtful engineer who
[00:45:08.280 --> 00:45:16.320]   can build correct code is worth for not so thoughtful people in a spot like that.
[00:45:16.320 --> 00:45:18.400]   And so that's kind of the thing we're optimizing for right now.
[00:45:18.400 --> 00:45:23.080]   And, um, such engineers, do you expect or give them a lot
[00:45:23.080 --> 00:45:25.800]   of, um, knowledge about finance?
[00:45:25.800 --> 00:45:30.400]   Or can they just function knowing about engineering, uh, just, just about
[00:45:30.400 --> 00:45:33.880]   engineering, and then you can just like tell them we need a program that does
[00:45:33.880 --> 00:45:36.560]   this, or do they need to have an understanding of how trading and finance
[00:45:36.560 --> 00:45:41.040]   works so need is probably a hair strong, but certainly the culture that I want
[00:45:41.040 --> 00:45:44.120]   to build is one where it's almost need.
[00:45:44.120 --> 00:45:45.960]   Like, it's almost like want, right?
[00:45:45.960 --> 00:45:47.640]   Like I would want to hire somebody.
[00:45:47.640 --> 00:45:51.960]   I want to hire somebody for whom understanding the problem domain deeply
[00:45:51.960 --> 00:45:54.400]   is a critical part of the job they feel they're doing.
[00:45:54.400 --> 00:45:58.680]   And so is it possible to build something like this?
[00:45:58.680 --> 00:46:02.880]   Um, another way, probably, but that's not the company I want to build.
[00:46:03.200 --> 00:46:06.200]   Um, and so in your career, you've done so many different things,
[00:46:06.200 --> 00:46:08.880]   engineering, um, trading, uh, consulting.
[00:46:08.880 --> 00:46:09.800]   Yeah.
[00:46:09.800 --> 00:46:13.880]   So how much carryover and lessons do you feel like you've had
[00:46:13.880 --> 00:46:15.240]   between these different domains?
[00:46:15.240 --> 00:46:18.400]   Or do you feel like they're, um, uh, they, they have like
[00:46:18.400 --> 00:46:19.960]   self-contained pools of knowledge.
[00:46:19.960 --> 00:46:24.680]   So I think if there's one, uh, constant for me, it's, I am
[00:46:24.680 --> 00:46:28.520]   surprised by how much my previous careers inform my next careers.
[00:46:28.880 --> 00:46:33.560]   Like when I, when I wanted to, to move from engineering to trading, um, it
[00:46:33.560 --> 00:46:36.080]   did continually surprise me how useful.
[00:46:36.080 --> 00:46:40.720]   Like the, the engineering training, as opposed to just kind of like me,
[00:46:40.720 --> 00:46:43.240]   hopefully being just generally smart and being able to figure things out.
[00:46:43.240 --> 00:46:46.680]   But like the actual engineering training was, was useful.
[00:46:46.680 --> 00:46:52.240]   Um, and then coming back to the consulting with companies, um, again,
[00:46:52.240 --> 00:46:55.160]   really surprising how, like I expected that, you know, when we were doing
[00:46:55.160 --> 00:46:58.000]   kind of the, the management and hiring consulting, that it would be about
[00:46:58.000 --> 00:47:01.960]   the nuts and bolts of, okay, well, what does a good hiring process look like?
[00:47:01.960 --> 00:47:03.720]   What kind of interview questions do you want to build?
[00:47:03.720 --> 00:47:05.360]   How do we evaluate them, et cetera, et cetera.
[00:47:05.360 --> 00:47:09.720]   And there's a component of that, but all of the other trading stuff, like
[00:47:09.720 --> 00:47:12.400]   how to think about the market for candidates and that sort of thing.
[00:47:12.400 --> 00:47:16.320]   Like it surprised me how, how non-obvious a lot of that stuff was
[00:47:16.320 --> 00:47:17.480]   to the people I was talking to.
[00:47:17.480 --> 00:47:22.560]   Um, and so now, yeah, like hopefully bringing all of that, those
[00:47:22.560 --> 00:47:26.040]   experiences, uh, to the table in, in this new startup that I'm doing.
[00:47:26.120 --> 00:47:30.480]   Um, you know, I'm, I'm optimistic that that'll occur again.
[00:47:30.480 --> 00:47:34.560]   You would think that people like you who have so much experience,
[00:47:34.560 --> 00:47:37.960]   um, in so many different industries, they would be the most common
[00:47:37.960 --> 00:47:41.080]   archetype, um, of, of a startup founder.
[00:47:41.080 --> 00:47:44.680]   Uh, because like they have so many general skills, um, at
[00:47:44.680 --> 00:47:45.640]   least in popular culture.
[00:47:45.640 --> 00:47:49.040]   And maybe this isn't represented in like, what is who are empirically
[00:47:49.040 --> 00:47:51.280]   the most successful founders, at least in popular culture, it seems
[00:47:51.280 --> 00:47:53.920]   like the trope is, you know, somebody who like has no particular skills
[00:47:53.920 --> 00:47:56.080]   is the, is the person who like starts a startup out of college.
[00:47:56.080 --> 00:48:00.560]   And why are there not more founders who have, um, a broader
[00:48:00.560 --> 00:48:01.960]   skillset and lots of experience?
[00:48:01.960 --> 00:48:05.400]   I think there actually are like, if I, if I remember correctly, maybe
[00:48:05.400 --> 00:48:08.720]   this is something I read maybe a year ago, the average startup founder
[00:48:08.720 --> 00:48:13.720]   is actually significantly older than, um, than sort of the popular conception.
[00:48:13.720 --> 00:48:17.400]   It's just that the young flashy startup founder gets all the press, right.
[00:48:17.400 --> 00:48:20.760]   And perhaps rightly so like, I'm not besmirching, you know,
[00:48:20.760 --> 00:48:21.920]   the young founders press.
[00:48:22.320 --> 00:48:26.840]   Um, but I think there's a lot of people kind of just doing it possibly
[00:48:26.840 --> 00:48:28.440]   with similar backgrounds to mine.
[00:48:28.440 --> 00:48:31.360]   Um, I think it works.
[00:48:31.360 --> 00:48:34.280]   There was one question I forgot to ask about adverse selection.
[00:48:34.280 --> 00:48:39.200]   Um, which is if, if, uh, let's take a company like Jane Street, if the
[00:48:39.200 --> 00:48:43.320]   counterparty knows that they're trading against Jane Street, um, and they know
[00:48:43.320 --> 00:48:46.600]   that Jane Street has a great reputation of making profitable trades, why does
[00:48:46.600 --> 00:48:48.080]   anybody even make that trade?
[00:48:48.080 --> 00:48:51.600]   And, um, I mean, as a followup, does that mean that Jane Street has to
[00:48:51.600 --> 00:48:55.560]   pay like a higher cost to make the same trade because it has like this
[00:48:55.560 --> 00:48:59.080]   reputation of making really profitable trades, which means that there's almost,
[00:48:59.080 --> 00:49:02.560]   um, a negative feedback loop of, if you become too successful, like the
[00:49:02.560 --> 00:49:04.720]   market makes it really hard for you to continue being successful.
[00:49:04.720 --> 00:49:06.880]   No, the answer is no.
[00:49:06.880 --> 00:49:08.640]   And I'm pretty sure the answer is no.
[00:49:08.640 --> 00:49:11.880]   And the reason is because again, getting back to this idea that Jane Street
[00:49:11.880 --> 00:49:13.760]   provides a service to the world, right?
[00:49:13.760 --> 00:49:14.640]   Like, so who are they?
[00:49:14.640 --> 00:49:17.320]   Jane Street doesn't want to trade against other market makers and other
[00:49:17.320 --> 00:49:19.160]   market makers don't want to trade against Jane Street because they're in
[00:49:19.160 --> 00:49:21.640]   the same business and they know like, that's not who they're going to make
[00:49:21.640 --> 00:49:24.400]   their money from, who they're going to make their money from is people who
[00:49:24.400 --> 00:49:26.360]   need the service that Jane Street provides.
[00:49:26.360 --> 00:49:31.360]   So for example, like if I am a pension fund or if I'm a, um, you know, a
[00:49:31.360 --> 00:49:36.040]   large, um, hedge fund or something, and I want to put on a bet in some random
[00:49:36.040 --> 00:49:39.760]   country, maybe I should just buy that country's ETF, right?
[00:49:39.760 --> 00:49:44.600]   It's certainly a lot easier, more straightforward, convenient, um, to
[00:49:44.600 --> 00:49:49.080]   just buy the ETF than to go to that country's stock market and buy all the
[00:49:49.080 --> 00:49:50.240]   individual stocks, right?
[00:49:50.240 --> 00:49:53.760]   And so that's not a thing that they're, they're an expert in, right?
[00:49:53.760 --> 00:49:56.360]   That they're not an expert in trading Vietnam stocks, right?
[00:49:56.360 --> 00:49:58.240]   They're just an expert in making these macro bets.
[00:49:58.240 --> 00:49:59.800]   Let's just say, right.
[00:49:59.800 --> 00:50:03.600]   And so Jane Street provides them the service of being able to sell them that
[00:50:03.600 --> 00:50:04.200]   ETF.
[00:50:04.200 --> 00:50:08.000]   Um, and then Jane Street takes care of all the, all the little details, right?
[00:50:08.000 --> 00:50:09.640]   That's the thing that Jane Street is really good at.
[00:50:09.640 --> 00:50:12.120]   And so there's gains from trade there.
[00:50:12.120 --> 00:50:14.360]   It's not, it's not zero sum in that sense.
[00:50:14.360 --> 00:50:18.000]   Um, and then what is the role of, um, market makers in crypto?
[00:50:18.000 --> 00:50:20.760]   If you have automated market makers, um, like Uniswap or something.
[00:50:20.760 --> 00:50:26.160]   So then what, what, what is the, what is like the comparative advantage of, I
[00:50:26.160 --> 00:50:28.080]   guess, a smart, uh, market makers?
[00:50:28.080 --> 00:50:33.160]   Well, so I think the thing I would argue is, and perhaps you've seen this paper
[00:50:33.160 --> 00:50:37.120]   from like last fall, but that, that shows that at least half of liquidity
[00:50:37.120 --> 00:50:40.960]   providers on Uniswap V2 lose money, they just lose money.
[00:50:40.960 --> 00:50:43.760]   And that's on priors, what you would expect, right?
[00:50:43.760 --> 00:50:46.400]   Like, let's say that there was no fee on Uniswap, right?
[00:50:46.400 --> 00:50:48.600]   Like let's do liquidity providers, just toss their money in.
[00:50:48.600 --> 00:50:51.520]   Then like liquidity providers are systematically getting adverse
[00:50:51.520 --> 00:50:53.960]   selected against by every trade that happens, right?
[00:50:53.960 --> 00:50:56.760]   And so the fee that you collect as a liquidity provider is a
[00:50:56.760 --> 00:51:00.520]   compensation for the adverse selection that you are undertaking
[00:51:00.520 --> 00:51:01.760]   by being a liquidity provider.
[00:51:01.760 --> 00:51:05.480]   But of course that fee is sort of set by fiat, right?
[00:51:05.480 --> 00:51:08.280]   Like it's either the five bit pool or the 30 bit pool or whatever.
[00:51:08.280 --> 00:51:13.080]   Like it is not adaptive to like market conditions, right?
[00:51:13.480 --> 00:51:19.640]   And so I, I am personally long-term skeptical about CFMMs as a market
[00:51:19.640 --> 00:51:21.080]   mechanism that is going to work.
[00:51:21.080 --> 00:51:25.600]   I just, I don't see how, I don't see how it makes sense for somebody to
[00:51:25.600 --> 00:51:29.320]   just like throw some money in a pool and expect to get sort of outsized
[00:51:29.320 --> 00:51:31.080]   returns by just doing nothing, right?
[00:51:31.080 --> 00:51:34.400]   Like outsized returns come from you knowing how to do something or being
[00:51:34.400 --> 00:51:35.960]   able to do something nobody else does.
[00:51:35.960 --> 00:51:36.320]   Right.
[00:51:36.320 --> 00:51:41.960]   And so it just, it doesn't strike me as a, as an exciting thing, very long-term.
[00:51:43.160 --> 00:51:46.080]   Does that mean that you're also pessimistic about passive investing
[00:51:46.080 --> 00:51:49.960]   in the longterm of somebody just like putting a certain amount of their money?
[00:51:49.960 --> 00:51:55.120]   I think the difference there is passive investing is at least again, over a
[00:51:55.120 --> 00:52:00.360]   long haul, you are providing risk capital to companies that are hopefully sources
[00:52:00.360 --> 00:52:02.280]   of discounted future profits, right?
[00:52:02.280 --> 00:52:06.080]   And so there's like, there's a reason that you might expect that to make money for
[00:52:06.080 --> 00:52:06.200]   you.
[00:52:06.200 --> 00:52:12.280]   Um, whereas when you're, when you're trading either FX or something that
[00:52:12.280 --> 00:52:16.600]   doesn't like earn yields from, from like actually providing value to the world,
[00:52:16.600 --> 00:52:20.720]   then no, you probably shouldn't expect to make money by passively investing in
[00:52:20.720 --> 00:52:20.920]   that.
[00:52:20.920 --> 00:52:24.880]   What made you interested in getting into crypto at this time, uh, transitioning
[00:52:24.880 --> 00:52:25.560]   to that industry?
[00:52:25.560 --> 00:52:26.200]   Yeah.
[00:52:26.200 --> 00:52:30.720]   So I think the thing about crypto that I like is to the extent that you believe
[00:52:30.720 --> 00:52:34.920]   in this somewhat stagnation is theory that look, whether it's through regulation
[00:52:34.920 --> 00:52:39.760]   or just cultural changes or whatever, that we're not doing bold, new, exciting,
[00:52:39.800 --> 00:52:44.520]   weird things, uh, the extent to which crypto is a shelling point around which
[00:52:44.520 --> 00:52:47.920]   everybody has decided, look, all of the crazy weird stuff that we want to try,
[00:52:47.920 --> 00:52:48.920]   we're going to do it here.
[00:52:48.920 --> 00:52:49.920]   I like that.
[00:52:49.920 --> 00:52:52.360]   I think that that is a very good thing for the world.
[00:52:52.360 --> 00:52:57.640]   And if, and I'm not saying this is going to be the case, but if all the crypto
[00:52:57.640 --> 00:53:02.200]   goes to zero and all that's happened is we've had a large wealth transfer from
[00:53:02.200 --> 00:53:07.400]   the rich olds to the young, like people who want to build cool stuff, like that's
[00:53:07.400 --> 00:53:08.320]   still good for the world.
[00:53:08.360 --> 00:53:11.520]   You know, like I think it's going to be more than that.
[00:53:11.520 --> 00:53:14.200]   I think there's a lot of interesting, exciting things that are going to, that
[00:53:14.200 --> 00:53:15.600]   are going to come out of the crypto world.
[00:53:15.600 --> 00:53:19.360]   Um, but you know, we get at least that right.
[00:53:19.360 --> 00:53:22.120]   Like a coordination around trying new things.
[00:53:22.120 --> 00:53:23.840]   If, if that doesn't work.
[00:53:23.840 --> 00:53:27.480]   Um, and in like taking your negative example as, um, let's say that's a
[00:53:27.480 --> 00:53:31.520]   hypothetical, I actually do wonder, but what is the actual wealth transfer that's
[00:53:31.520 --> 00:53:32.240]   happened here?
[00:53:32.240 --> 00:53:36.600]   Is it actually been from, cause like if, um, if institutional investors have not
[00:53:36.600 --> 00:53:40.120]   gotten that much into crypto as compared to like, you know, some grandma, maybe not
[00:53:40.120 --> 00:53:41.720]   grandma, but like, I don't know, some middle-aged guy.
[00:53:41.720 --> 00:53:46.440]   Um, so then has the actual, has the actual wealth transfer been from wealthier to
[00:53:46.440 --> 00:53:48.440]   poor, or I wonder if it's been the other way around.
[00:53:48.440 --> 00:53:52.000]   I think it has, I think it has to have been like, look, look at all these, you
[00:53:52.000 --> 00:53:56.080]   know, look at all these VCs raising all these funds, like the LPs and those VCs.
[00:53:56.080 --> 00:53:59.000]   Funds are old, right?
[00:53:59.000 --> 00:53:59.840]   Yeah.
[00:53:59.840 --> 00:54:03.320]   There's a good story to be told about adverse selection and venture capital as
[00:54:03.320 --> 00:54:03.720]   well.
[00:54:03.720 --> 00:54:08.560]   Um, so, um, but yeah, so that's, it's basically a transfer of wealth from like
[00:54:08.560 --> 00:54:11.120]   VCs to VCs to like 21 year olds.
[00:54:11.120 --> 00:54:14.720]   The other thing about crypto, like people always, especially people from Jane
[00:54:14.720 --> 00:54:17.360]   Street, like whenever I meet them again and, you know, say, Hey, how's it going?
[00:54:17.360 --> 00:54:19.840]   Like almost the first question they asked me when they find out what I'm doing is
[00:54:19.840 --> 00:54:21.560]   like, so what, like, are you all laser eyes now?
[00:54:21.560 --> 00:54:22.320]   Like, what's your deal?
[00:54:22.320 --> 00:54:26.360]   And I think by crypto standards, I think I'm very non-laser eyes.
[00:54:26.360 --> 00:54:30.400]   Um, in the sense that this is probably going to be an unpopular opinion within
[00:54:30.400 --> 00:54:34.880]   the crypto world, but I think success for crypto definitely looks like integration
[00:54:34.880 --> 00:54:35.920]   into the financial system.
[00:54:35.920 --> 00:54:39.400]   Like it just, it's not like it's going to replace it.
[00:54:39.400 --> 00:54:43.840]   It's not going to be like, Oh, Goldman and Chase are going to go to zero and, and
[00:54:43.840 --> 00:54:45.280]   a Coinbase is going to crush it.
[00:54:45.280 --> 00:54:46.640]   Like, that's not what it looks like.
[00:54:46.640 --> 00:54:52.440]   Success for crypto looks like traditional finance integrates, takes the best ideas
[00:54:52.440 --> 00:54:57.360]   and crypto companies are incredibly successful in that process, but we end up
[00:54:57.360 --> 00:54:59.840]   with something that's kind of a hybrid of the best of both.
[00:54:59.920 --> 00:55:01.160]   Like I think that's success.
[00:55:01.160 --> 00:55:01.560]   Yeah.
[00:55:01.560 --> 00:55:02.080]   And I'm here.
[00:55:02.080 --> 00:55:05.680]   So like, what is, um, what is the future look like in a world where crypto is very
[00:55:05.680 --> 00:55:06.120]   successful?
[00:55:06.120 --> 00:55:10.600]   Like, for example, what would, what would, uh, something like the stock market look
[00:55:10.600 --> 00:55:14.800]   like if, um, would it be like far more efficient if it's over crypto or would it
[00:55:14.800 --> 00:55:16.840]   be less efficient because of gas fees?
[00:55:16.840 --> 00:55:20.920]   Um, or, you know, maybe it's like payments internationally, but yeah, I'm curious
[00:55:20.920 --> 00:55:24.000]   what you think, like 20 years down the line success case where crypto looks like.
[00:55:24.000 --> 00:55:26.920]   Um, so I'm going to leave the financial markets to last.
[00:55:26.920 --> 00:55:30.160]   Like I think Western union is out of business is probably a good outcome.
[00:55:30.160 --> 00:55:31.880]   Like it's probably good.
[00:55:31.880 --> 00:55:36.920]   All those stupid, um, like all those stupid, uh, Thomas Cook money exchange in
[00:55:36.920 --> 00:55:38.360]   the airport things are out of business.
[00:55:38.360 --> 00:55:39.600]   Like that's probably good.
[00:55:39.600 --> 00:55:43.880]   So like if, if only that happens, I think we're already in good shape.
[00:55:43.880 --> 00:55:51.760]   Um, certainly the, the idea of NFTs as transferable, uh, signals of, um, uh,
[00:55:51.760 --> 00:55:55.440]   facts about you or facts about whatever persona or avatar you want to have.
[00:55:55.880 --> 00:55:57.680]   Um, I think it's pretty exciting.
[00:55:57.680 --> 00:56:00.160]   Like the idea that I have to like call my university and get a
[00:56:00.160 --> 00:56:03.040]   transcript from them and stuff like that seems insane to me.
[00:56:03.040 --> 00:56:08.680]   Um, and the, in the, like how, how some of these, uh, kind of credentialing
[00:56:08.680 --> 00:56:13.320]   systems can work with NFTs strikes me as, as a fairly natural thing.
[00:56:13.320 --> 00:56:20.400]   Um, I think to the extent that, um, that crypto is breaking the oligopoly of a few,
[00:56:20.400 --> 00:56:23.480]   uh, large financial participants in the market today.
[00:56:23.480 --> 00:56:26.840]   I mean, I, I don't know if you, you followed the saga of the CFTC, uh,
[00:56:26.840 --> 00:56:32.320]   review of FTX's proposal to, to, to do sort of a different kind of, um, futures
[00:56:32.320 --> 00:56:36.560]   margining process on like on actual real futures, I think this is a good thing.
[00:56:36.560 --> 00:56:40.080]   Like there's a lot of vested interests and entrenched interests that are kind
[00:56:40.080 --> 00:56:42.680]   of getting their bell rung and, and that's a good thing.
[00:56:42.680 --> 00:56:46.000]   So that's, that's kind of the direction that I would, that I would take.
[00:56:46.000 --> 00:56:49.920]   It's sort of the, uh, the financial infrastructure or the plumbing of, of
[00:56:49.920 --> 00:56:52.840]   finance is probably going to be sort of cryptoified.
[00:56:53.120 --> 00:56:54.440]   That doesn't mean it's all going on chain.
[00:56:54.440 --> 00:56:58.120]   I think, you know, all the blockchain is, is a very slow, weird database,
[00:56:58.120 --> 00:57:00.840]   but it is a very slow, weird database that has some useful
[00:57:00.840 --> 00:57:02.680]   properties in some situations.
[00:57:02.680 --> 00:57:04.840]   Uh, what are three books you would recommend?
[00:57:04.840 --> 00:57:05.800]   A Deepness in the Sky.
[00:57:05.800 --> 00:57:07.240]   A Deepness in the Sky by Werner Wenge.
[00:57:07.240 --> 00:57:10.480]   If you're a software engineer and you like science fiction, you know, read it
[00:57:10.480 --> 00:57:14.680]   with the eye towards thinking of it as a software book as a thing I would say.
[00:57:14.680 --> 00:57:19.840]   Um, if you want to think about kind of risk taking in general, uh, Aaron
[00:57:19.840 --> 00:57:23.400]   Brown's Red-Blooded Risk is, I'm trying to think of books that
[00:57:23.400 --> 00:57:24.560]   probably people haven't heard of.
[00:57:24.560 --> 00:57:28.840]   Um, Red-Blooded Risk by Aaron Brown is really, really, really good.
[00:57:28.840 --> 00:57:30.080]   In fact, all of his books are really good.
[00:57:30.080 --> 00:57:33.240]   Like, um, the thing that got me interested in finance was reading his
[00:57:33.240 --> 00:57:35.320]   book, uh, The Poker Face of Wall Street.
[00:57:35.320 --> 00:57:37.760]   Cause I was playing poker at the time and I was kind of like, "Hey, maybe
[00:57:37.760 --> 00:57:38.880]   Wall Street, maybe that's a thing."
[00:57:38.880 --> 00:57:42.160]   So The Poker Face of Wall Street by Aaron Brown or, uh, The Red-Blooded Risk.
[00:57:42.160 --> 00:57:47.360]   Um, and one that's, uh, kind of off the wall a little bit is, um, it's
[00:57:47.360 --> 00:57:49.840]   called Kalima Stories by Varlam Shalamov.
[00:57:49.840 --> 00:57:55.080]   And it is a collection of stories about people who, who sort of lived
[00:57:55.080 --> 00:57:58.600]   in the Gulag in Siberia during sort of Stalinist times.
[00:57:58.600 --> 00:58:02.320]   Um, I think it is possibly the most revealing book about
[00:58:02.320 --> 00:58:04.000]   human nature that I've ever read.
[00:58:04.000 --> 00:58:05.520]   Um, it's depressing.
[00:58:05.520 --> 00:58:11.520]   I don't like, read it in a sunny place, you know, but, but it's, uh, it's revealing.
[00:58:11.520 --> 00:58:15.400]   Is this a covert way of telling us about the working conditions at Jane Street?
[00:58:15.400 --> 00:58:16.360]   No, not at all.
[00:58:16.720 --> 00:58:21.480]   Um, yeah, so final question is, um, you know, you've been, um, successful in so
[00:58:21.480 --> 00:58:24.520]   many different, uh, industries and you've, you know, you know, the lessons
[00:58:24.520 --> 00:58:26.480]   of, uh, you know, working in so many of them.
[00:58:26.480 --> 00:58:31.160]   So is there advice you would give to like somebody who's in their early twenties?
[00:58:31.160 --> 00:58:34.360]   Um, I guess most of my audience is probably going to, to the extent that
[00:58:34.360 --> 00:58:36.160]   they're working in those industries, we're probably going to be programmers,
[00:58:36.160 --> 00:58:38.640]   but I don't know, maybe after interviewing you, I'll have like a few
[00:58:38.640 --> 00:58:41.080]   traders or want to be traders who are listening as well.
[00:58:41.080 --> 00:58:44.760]   So yeah, if you have like some advice or what you think would be useful for
[00:58:44.760 --> 00:58:48.640]   somebody who's very young, yeah, I would say like, number one thing I was, I
[00:58:48.640 --> 00:58:52.320]   tell my kids this all the time, like life is long, like life is not short.
[00:58:52.320 --> 00:58:53.120]   Life is long.
[00:58:53.120 --> 00:58:58.320]   And what that means is you should think of yourself as having many opportunities
[00:58:58.320 --> 00:59:02.240]   to learn things and try things and do things.
[00:59:02.240 --> 00:59:07.400]   And so, again, this is just my own experience, but, um, I feel like I'm
[00:59:07.400 --> 00:59:09.080]   sort of sequentially obsessive.
[00:59:09.080 --> 00:59:12.280]   Like I will sort of block off six years of my life.
[00:59:12.320 --> 00:59:15.240]   It turns out like empirically, this is what has happened to like, get
[00:59:15.240 --> 00:59:16.760]   really, really good at a thing.
[00:59:16.760 --> 00:59:20.880]   And then like, okay, next six to seven year period, I'm going to try to get
[00:59:20.880 --> 00:59:24.760]   really, really, really good at another thing that is kind of different.
[00:59:24.760 --> 00:59:31.160]   Um, and that's worked out for me because, um, it's easy to undervalue the
[00:59:31.160 --> 00:59:36.440]   importance of deep, deep, deep expertise in a thing and the process that it, that
[00:59:36.440 --> 00:59:38.440]   is required to get really, really good at something.
[00:59:38.800 --> 00:59:43.800]   Um, and so this idea of kind of sequential excellence, I think is a thing that I,
[00:59:43.800 --> 00:59:49.440]   that I like to think about a lot, um, because you have the time, right?
[00:59:49.440 --> 00:59:53.600]   Like spend five years being a, you know, a front end developer and get
[00:59:53.600 --> 00:59:55.640]   just incredibly good at that.
[00:59:55.640 --> 00:59:57.640]   And then, you know, go do something else.
[00:59:57.640 --> 00:59:59.200]   Maybe it's not programming, maybe it's something else.
[00:59:59.200 --> 00:59:59.480]   Right.
[00:59:59.480 --> 01:00:02.160]   Um, and maybe come back to it, right.
[01:00:02.160 --> 01:00:03.560]   And you'll have this other perspective.
[01:00:03.560 --> 01:00:06.640]   Um, yeah, that's kind of my thought.
[01:00:06.640 --> 01:00:06.880]   Yeah.
[01:00:06.880 --> 01:00:07.840]   That's super interesting.
[01:00:07.840 --> 01:00:12.640]   I'm curious if you think, um, Like, uh, let's say you had, I guess it wouldn't
[01:00:12.640 --> 01:00:16.720]   be possible with crypto, but like, let's say you had been a trader, like you had
[01:00:16.720 --> 01:00:18.880]   instead of doing electrical engineering and computer science, you had just like
[01:00:18.880 --> 01:00:20.440]   done trading from the very get go.
[01:00:20.440 --> 01:00:22.920]   Would you have been by the end of your trading career?
[01:00:22.920 --> 01:00:26.280]   Would you have been more successful in that counterfactual where you've been
[01:00:26.280 --> 01:00:28.880]   the trader the whole time or one where you have the experience from engineering?
[01:00:28.880 --> 01:00:30.920]   And then, you know, same with, uh, consulting.
[01:00:30.920 --> 01:00:36.600]   I mean, um, yeah, so I, I guess is the career path with a lot of, you know,
[01:00:36.600 --> 01:00:40.120]   career path with a lot of, um, D specialties, but changing what that
[01:00:40.120 --> 01:00:44.000]   specialty is over time is that it does that lead to a higher peak in the end
[01:00:44.000 --> 01:00:45.960]   or the one where you just focus on one career?
[01:00:45.960 --> 01:00:47.280]   Yeah, it's a good question.
[01:00:47.280 --> 01:00:49.480]   I think it probably varies by person.
[01:00:49.480 --> 01:00:54.640]   Like if you are, um, if you are destined or have the capacity to be a world
[01:00:54.640 --> 01:01:00.200]   changing physicists, then probably you don't do any of the sequential weirdness.
[01:01:00.200 --> 01:01:01.920]   You just kind of go down that road.
[01:01:02.520 --> 01:01:07.320]   Um, but well, then maybe, maybe I'm going to, I might edit that because
[01:01:07.320 --> 01:01:10.480]   one thing that I do believe about, about discoveries, whether it's in trading
[01:01:10.480 --> 01:01:13.360]   or in science or anything like that, is there's sort of two types.
[01:01:13.360 --> 01:01:18.480]   There's the evolutionary type, which is take a body of work or a field and just
[01:01:18.480 --> 01:01:23.280]   sort of push the boundaries out on it a little bit, and then there's the
[01:01:23.280 --> 01:01:27.680]   revolutionary type, there's the like Albert Einstein, there's the Claude
[01:01:27.680 --> 01:01:31.640]   Shannon, um, these sorts of people where it's like, I'm just going to
[01:01:31.640 --> 01:01:33.200]   invent a whole new field, right?
[01:01:33.200 --> 01:01:37.520]   And so actually Claude Shannon is kind of an informative case because he's
[01:01:37.520 --> 01:01:41.000]   famously just basically played games all day and just thought about random
[01:01:41.000 --> 01:01:45.880]   things and, and kind of tried to have as broad, uh, an exposure to things as he
[01:01:45.880 --> 01:01:46.120]   could.
[01:01:46.120 --> 01:01:48.680]   I mean, you rode unicycles and that sort of thing.
[01:01:48.680 --> 01:01:53.280]   So, you know, maybe, maybe like you, you need to be a little bit self-aware
[01:01:53.280 --> 01:01:55.320]   about kind of which of the two you might, you might be.
[01:01:55.320 --> 01:01:56.120]   Yeah.
[01:01:56.120 --> 01:01:56.360]   Okay.
[01:01:56.360 --> 01:02:01.080]   So the book is the laws of trading available on Amazon.
[01:02:01.080 --> 01:02:04.920]   Um, and yeah, do you want to give your Twitter handle plus any other place
[01:02:04.920 --> 01:02:06.760]   where viewers can find you?
[01:02:06.760 --> 01:02:07.160]   Sure.
[01:02:07.160 --> 01:02:09.160]   So yeah, it's, uh, Augustine LeBron three.
[01:02:09.160 --> 01:02:10.800]   I don't know why three, but that's what it is.
[01:02:10.800 --> 01:02:15.240]   Um, I mostly talk about trading, sometimes talk about software.
[01:02:15.240 --> 01:02:18.680]   Um, sometimes talk about random things in the world.
[01:02:18.680 --> 01:02:24.040]   Um, yeah, that's, that's, I, I am not much of a social media guy.
[01:02:24.040 --> 01:02:26.840]   And in fact, if I hadn't been for the book, I would still be a social
[01:02:26.840 --> 01:02:28.320]   media, non-existent person.
[01:02:28.320 --> 01:02:31.480]   Um, but I, I've kind of gravitated towards Twitter.
[01:02:31.480 --> 01:02:33.840]   It's where, where I ended up having interesting conversations.
[01:02:33.840 --> 01:02:34.320]   Yeah.
[01:02:34.320 --> 01:02:34.680]   Yeah.
[01:02:34.680 --> 01:02:35.880]   I've enjoyed being a follower.
[01:02:35.880 --> 01:02:38.400]   Um, is there anything, uh, is there anything we didn't touch on in the
[01:02:38.400 --> 01:02:41.600]   conversation that you think might be, uh, might, might be interesting to close on?
[01:02:41.600 --> 01:02:42.240]   Or, um,
[01:02:42.240 --> 01:02:46.640]   I mean, I'm selfishly, I want to ask you Dwarkesh, like, tell me about what you're
[01:02:46.640 --> 01:02:49.320]   doing, like, what, what is this that you're building here?
[01:02:49.320 --> 01:02:51.360]   That's a good question.
[01:02:51.520 --> 01:02:58.200]   Um, yeah, so this was, um, to give you the backstory on this, this was, I, I think my
[01:02:58.200 --> 01:03:02.840]   sophomore year of college or maybe junior year, um, COVID hit and, um, I was really
[01:03:02.840 --> 01:03:04.640]   bored because classes went online.
[01:03:04.640 --> 01:03:07.240]   And so I just, you know, started the podcast.
[01:03:07.240 --> 01:03:08.760]   I just cold emailed my first guest.
[01:03:08.760 --> 01:03:12.600]   Um, yeah, actually the time I was releasing, I didn't even have a name for
[01:03:12.600 --> 01:03:16.600]   the podcast because I just had like a recorded episode, but so anyways, uh, just
[01:03:16.600 --> 01:03:21.720]   kept it up, um, and then I graduated like four months ago, actually technically
[01:03:21.720 --> 01:03:25.440]   graduated like two weeks ago, but I was done with classes four months ago.
[01:03:25.440 --> 01:03:28.200]   And then I thought, all right, I have a little bit of money saved up from an
[01:03:28.200 --> 01:03:29.880]   internship and then another grant.
[01:03:29.880 --> 01:03:33.520]   And so I thought, all right, let me just do this like full-time for a few months
[01:03:33.520 --> 01:03:35.640]   and see what happens and, um, got some traction.
[01:03:35.640 --> 01:03:39.040]   So, um, I, I don't know where this leads, but actually the comments you made about,
[01:03:39.040 --> 01:03:45.160]   um, um, the, you know, going deep on one particular thing and then maybe you have
[01:03:45.200 --> 01:03:49.000]   using the skills you learned there to transition to another, that, uh, that
[01:03:49.000 --> 01:03:53.200]   makes me feel a lot better because I don't think my longterm term trajectory
[01:03:53.200 --> 01:03:56.640]   is, um, being a podcast host or, uh, writing a newsletter.
[01:03:56.640 --> 01:04:01.320]   Uh, but I do, so, you know, I, I would like, uh, love to go back to tech and
[01:04:01.320 --> 01:04:06.080]   startups, um, um, you know, like in the future I didn't have a computer science
[01:04:06.080 --> 01:04:10.440]   degree, but, um, yeah, so, uh, we're hoping to learn as much as possible
[01:04:10.480 --> 01:04:15.400]   through the podcast and, uh, writing, and then hopefully use the skills I learned
[01:04:15.400 --> 01:04:17.640]   there to do some cool things and other fields.
[01:04:17.640 --> 01:04:18.700]   Love it.
[01:04:18.700 --> 01:04:19.920]   Sounds like a great plan.
[01:04:19.920 --> 01:04:21.700]   Yeah.
[01:04:21.700 --> 01:04:22.220]   Thanks.
[01:04:22.220 --> 01:04:23.360]   Um, awesome.
[01:04:23.360 --> 01:04:24.080]   Yeah. Thanks for coming on.
[01:04:24.080 --> 01:04:24.320]   I guess.
[01:04:24.320 --> 01:04:25.820]   And this was one of my favorite podcasts I've done.
[01:04:25.820 --> 01:04:27.080]   It was so many insights.
[01:04:27.080 --> 01:04:27.760]   Awesome.
[01:04:27.760 --> 01:04:28.260]   Thanks.
[01:04:28.260 --> 01:04:28.880]   Really enjoyed it.
[01:04:28.880 --> 01:04:38.880]   [music]
[01:04:38.880 --> 01:04:46.140]   [BLANK_AUDIO]

