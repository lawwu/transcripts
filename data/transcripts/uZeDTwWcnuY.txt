
[00:00:00.000 --> 00:00:03.400]   [MUSIC PLAYING]
[00:00:03.400 --> 00:00:11.080]   So at a very high level, what does math
[00:00:11.080 --> 00:00:12.520]   have to do with machine learning?
[00:00:12.520 --> 00:00:15.160]   Why math for machine learning among all the things
[00:00:15.160 --> 00:00:16.840]   that we could be talking about?
[00:00:16.840 --> 00:00:20.320]   And all programming involves mathematics at some level.
[00:00:20.320 --> 00:00:22.320]   Mathematics and programming and computers
[00:00:22.320 --> 00:00:25.160]   have been tied together since the inception of computer
[00:00:25.160 --> 00:00:26.760]   science and programming.
[00:00:26.760 --> 00:00:28.680]   Machine learning in particular, though,
[00:00:28.680 --> 00:00:30.960]   is programming by optimization.
[00:00:30.960 --> 00:00:32.520]   The way that we program computers
[00:00:32.520 --> 00:00:38.080]   to do things in machine learning is through optimization.
[00:00:38.080 --> 00:00:40.660]   And in order to understand optimization,
[00:00:40.660 --> 00:00:43.300]   in order to understand what it is that we're optimizing
[00:00:43.300 --> 00:00:46.840]   and why and how that works, we need mathematics.
[00:00:46.840 --> 00:00:48.880]   And this is what makes machine learning
[00:00:48.880 --> 00:00:52.640]   such a more mathematical discipline of programming
[00:00:52.640 --> 00:00:56.000]   than something like web development or database
[00:00:56.000 --> 00:00:58.720]   management or any of the other different branches
[00:00:58.720 --> 00:01:03.360]   of programming that are popular these days.
[00:01:03.360 --> 00:01:04.800]   And so in this series, we're going
[00:01:04.800 --> 00:01:08.680]   to cover several of the ways that optimization and machine
[00:01:08.680 --> 00:01:11.440]   learning intersect or interact.
[00:01:11.440 --> 00:01:13.680]   Linear algebra, which will help us understand
[00:01:13.680 --> 00:01:15.720]   the objects being optimized.
[00:01:15.720 --> 00:01:18.680]   Calculus, which will help us understand how it is that we
[00:01:18.680 --> 00:01:20.280]   optimize those things.
[00:01:20.280 --> 00:01:22.480]   And then probability and statistics,
[00:01:22.480 --> 00:01:26.880]   which will help us understand what that thing is that we are
[00:01:26.880 --> 00:01:29.280]   optimizing, that we are making better.
[00:01:29.280 --> 00:01:32.400]   So we'll start with that section on linear algebra,
[00:01:32.400 --> 00:01:34.400]   which is to understand the objects that
[00:01:34.400 --> 00:01:36.160]   are being optimized.
[00:01:36.160 --> 00:01:39.240]   So the takeaways for today are that linear algebra
[00:01:39.240 --> 00:01:42.160]   is important and why it's so important.
[00:01:42.160 --> 00:01:43.840]   That linear algebra, despite its name,
[00:01:43.840 --> 00:01:46.560]   is actually not that much like algebra.
[00:01:46.560 --> 00:01:51.120]   And that the SVD, or singular value decomposition,
[00:01:51.120 --> 00:01:56.280]   is a matrix version of refactoring a common technique
[00:01:56.280 --> 00:01:57.320]   in programming.
[00:01:57.320 --> 00:02:00.320]   First, linear algebra is important.
[00:02:00.320 --> 00:02:02.800]   Why do we care about linear algebra?
[00:02:02.800 --> 00:02:04.920]   First past definition is that linear algebra
[00:02:04.920 --> 00:02:07.360]   is the mathematics of arrays.
[00:02:07.360 --> 00:02:10.800]   An array is a collection of numbers,
[00:02:10.800 --> 00:02:14.320]   a sort of block-like collection of numbers.
[00:02:14.320 --> 00:02:16.000]   Those numbers are typically--
[00:02:16.000 --> 00:02:18.280]   usually we think of them as being floating point
[00:02:18.280 --> 00:02:22.040]   numbers in a computer or real numbers in mathematics.
[00:02:22.040 --> 00:02:24.040]   And lots of objects are represented
[00:02:24.040 --> 00:02:25.560]   by arrays in machine learning.
[00:02:25.560 --> 00:02:27.800]   Our data is going to be in an array.
[00:02:27.800 --> 00:02:29.840]   There will be our inputs and our outputs.
[00:02:29.840 --> 00:02:31.280]   If our inputs are images, they're
[00:02:31.280 --> 00:02:34.080]   going to be arrays of numbers that represent that image.
[00:02:34.080 --> 00:02:36.320]   Very often, our models, which are
[00:02:36.320 --> 00:02:39.320]   like the equivalents of our programs in machine learning,
[00:02:39.320 --> 00:02:42.680]   will be represented by arrays or by collections of arrays.
[00:02:42.680 --> 00:02:44.400]   And then the internal computations
[00:02:44.400 --> 00:02:48.080]   even of those models will be represented by arrays as well.
[00:02:48.080 --> 00:02:51.280]   And so the core operation in linear algebra,
[00:02:51.280 --> 00:02:55.000]   the core operation that we use to manipulate those arrays
[00:02:55.000 --> 00:02:56.720]   is matrix multiplication.
[00:02:56.720 --> 00:02:59.200]   So a couple of examples of matrix multiplication
[00:02:59.200 --> 00:03:00.240]   in action--
[00:03:00.240 --> 00:03:03.200]   you may have heard of the dot scalar or inner product,
[00:03:03.200 --> 00:03:06.440]   the correlation or the covariance in statistics,
[00:03:06.440 --> 00:03:08.480]   linear and logistic regression, ideas
[00:03:08.480 --> 00:03:11.680]   from classical machine learning, principal components analysis,
[00:03:11.680 --> 00:03:13.400]   and also a bunch of useful algorithms
[00:03:13.400 --> 00:03:16.000]   outside of what you would think of as machine learning--
[00:03:16.000 --> 00:03:19.560]   discrete Fourier transform, page rank, then things
[00:03:19.560 --> 00:03:21.800]   in machine learning again, hidden layers of neural
[00:03:21.800 --> 00:03:24.120]   networks, convolutions, the second order
[00:03:24.120 --> 00:03:27.760]   methods for optimization, Newton and LBFGS.
[00:03:27.760 --> 00:03:30.560]   Matrix multiplication is essentially
[00:03:30.560 --> 00:03:33.240]   the operation at the core of every single one
[00:03:33.240 --> 00:03:35.000]   of these examples.
[00:03:35.000 --> 00:03:36.960]   And in the end, GPUs--
[00:03:36.960 --> 00:03:39.120]   you may have heard of the importance of GPUs,
[00:03:39.120 --> 00:03:40.720]   graphics processing units.
[00:03:40.720 --> 00:03:45.400]   GPUs are a huge part of what drove this huge explosion
[00:03:45.400 --> 00:03:46.960]   in machine learning recently.
[00:03:46.960 --> 00:03:49.320]   And they were originally designed for video games,
[00:03:49.320 --> 00:03:51.240]   and they made video games faster and better.
[00:03:51.240 --> 00:03:54.200]   But they ended up also making machine learning faster
[00:03:54.200 --> 00:03:56.440]   and better because underneath it all,
[00:03:56.440 --> 00:03:58.760]   both graphics rendering and machine learning
[00:03:58.760 --> 00:04:00.560]   require linear algebra.
[00:04:00.560 --> 00:04:03.720]   And that matrix multiplication is surprisingly simple,
[00:04:03.720 --> 00:04:06.120]   despite how many things it can be used for.
[00:04:06.120 --> 00:04:08.160]   So we have our arrays of numbers.
[00:04:08.160 --> 00:04:11.040]   So we have one array here on the left, another on the right.
[00:04:11.040 --> 00:04:13.800]   That's on the left side of the equal sign here.
[00:04:13.800 --> 00:04:16.720]   And that's going to result in the output of another array.
[00:04:16.720 --> 00:04:18.920]   So if I take two matrices and multiply them,
[00:04:18.920 --> 00:04:21.320]   I take the rows of the first matrix
[00:04:21.320 --> 00:04:23.480]   and the columns of the second matrix,
[00:04:23.480 --> 00:04:28.120]   and I combine those together to get the elements of the output.
[00:04:28.120 --> 00:04:31.160]   And what I do is I take each number in this row
[00:04:31.160 --> 00:04:34.160]   and each number in this column, and I multiply them together.
[00:04:34.160 --> 00:04:35.840]   That gives me a whole bunch of numbers.
[00:04:35.840 --> 00:04:38.880]   And then I add them up, and that gives me this final number
[00:04:38.880 --> 00:04:39.480]   here.
[00:04:39.480 --> 00:04:42.200]   I reduce them down with this sum operation.
[00:04:42.200 --> 00:04:44.000]   So mathematically, that might be written
[00:04:44.000 --> 00:04:47.160]   that if I were to matrix multiply x and y together here
[00:04:47.160 --> 00:04:50.040]   and look at the i-th row and the j-th column,
[00:04:50.040 --> 00:04:52.400]   then what I'm going to do is take the i-th row of x,
[00:04:52.400 --> 00:04:55.640]   the j-th column of y, and multiply their entries
[00:04:55.640 --> 00:04:56.400]   and sum it up.
[00:04:56.400 --> 00:04:59.120]   And if I want to get all the entries of that output
[00:04:59.120 --> 00:05:00.920]   of that matrix multiplication of x and y,
[00:05:00.920 --> 00:05:03.240]   I just do that for every row and for every column.
[00:05:03.240 --> 00:05:05.560]   Matrix multiplication takes two matrices
[00:05:05.560 --> 00:05:09.160]   where one has the same number of rows as the other has columns
[00:05:09.160 --> 00:05:11.520]   and turns them into a single output
[00:05:11.520 --> 00:05:13.640]   with the same number of rows as the first one
[00:05:13.640 --> 00:05:15.840]   and the same number of columns as the second one.
[00:05:15.840 --> 00:05:17.800]   It's relatively simple, given how many things
[00:05:17.800 --> 00:05:19.240]   it's capable of doing.
[00:05:19.240 --> 00:05:23.080]   But it's also kind of a weird and specific rule
[00:05:23.080 --> 00:05:24.680]   that if I just told you, oh, here's
[00:05:24.680 --> 00:05:26.840]   how we're going to combine collections of numbers,
[00:05:26.840 --> 00:05:29.280]   there's no obvious reason why you would do it that way.
[00:05:29.280 --> 00:05:31.920]   And from here, I think a lot of linear algebra tutorials
[00:05:31.920 --> 00:05:34.200]   would just jump straight into algebra.
[00:05:34.200 --> 00:05:35.760]   They would give algebraic definitions
[00:05:35.760 --> 00:05:38.080]   in terms of equations of a whole bunch of key concepts
[00:05:38.080 --> 00:05:39.960]   and then start manipulating those equations,
[00:05:39.960 --> 00:05:41.560]   manipulating matrices.
[00:05:41.560 --> 00:05:44.200]   And I think this is not the right tack to take here,
[00:05:44.200 --> 00:05:45.640]   because I think it doesn't give you
[00:05:45.640 --> 00:05:47.840]   a motivation for why we're doing that rule
[00:05:47.840 --> 00:05:50.360]   and how you should really think about linear algebra,
[00:05:50.360 --> 00:05:52.200]   rather than thinking of it as manipulating
[00:05:52.200 --> 00:05:53.400]   a bunch of equations.
[00:05:53.400 --> 00:05:56.960]   Let's dive into this first of our major concepts
[00:05:56.960 --> 00:05:59.920]   that linear algebra is not like algebra.
[00:05:59.920 --> 00:06:02.680]   Linear algebra is really often taught like algebra,
[00:06:02.680 --> 00:06:05.200]   first discovered in this world where people were really
[00:06:05.200 --> 00:06:07.120]   concerned with solving equations.
[00:06:07.120 --> 00:06:09.040]   And linear algebra kind of looks like algebra,
[00:06:09.040 --> 00:06:11.680]   because it's got addition, and it's got multiplication,
[00:06:11.680 --> 00:06:13.320]   and there's rules for combining them.
[00:06:13.320 --> 00:06:15.320]   So that looks a lot like algebra.
[00:06:15.320 --> 00:06:17.440]   And lots of linear algebra classes
[00:06:17.440 --> 00:06:20.600]   spend their time on just those computations and manipulations--
[00:06:20.600 --> 00:06:22.920]   determinants, eigenvalues, things like that.
[00:06:22.920 --> 00:06:24.640]   And folks end up getting pretty confused,
[00:06:24.640 --> 00:06:26.060]   because the rules are complicated,
[00:06:26.060 --> 00:06:29.680]   and reasoning with equations is pretty hard and unintuitive.
[00:06:29.680 --> 00:06:31.720]   It took me a very long time before I
[00:06:31.720 --> 00:06:34.000]   became comfortable just doing that kind
[00:06:34.000 --> 00:06:36.040]   of algebraic manipulation with linear algebra.
[00:06:36.040 --> 00:06:39.280]   But that's not the only way to understand linear algebra.
[00:06:39.280 --> 00:06:41.040]   There's lots of other ways to look at it
[00:06:41.040 --> 00:06:42.480]   that are more fruitful.
[00:06:42.480 --> 00:06:45.480]   And so one of them is a sort of geometric view
[00:06:45.480 --> 00:06:47.880]   of linear algebra that I think is best put forward
[00:06:47.880 --> 00:06:51.360]   by this YouTube series by the YouTuber 3Blue1Brown,
[00:06:51.360 --> 00:06:54.720]   formerly of Khan Academy, the essence of linear algebra.
[00:06:54.720 --> 00:06:57.120]   And that views linear algebra as the study
[00:06:57.120 --> 00:06:59.360]   of linear transformations of spaces,
[00:06:59.360 --> 00:07:02.240]   of transformations of rotations, reflections,
[00:07:02.240 --> 00:07:05.120]   and scalings of a grid.
[00:07:05.120 --> 00:07:07.680]   And so that's one interesting way of looking at it.
[00:07:07.680 --> 00:07:09.760]   And I recommend you check out that YouTube series.
[00:07:09.760 --> 00:07:10.960]   It's very beloved.
[00:07:10.960 --> 00:07:13.200]   There's also other mathematical ways
[00:07:13.200 --> 00:07:14.680]   of understanding linear algebra that
[00:07:14.680 --> 00:07:17.160]   don't have to do with equational reasoning.
[00:07:17.160 --> 00:07:19.680]   For example, graphical linear algebra approach,
[00:07:19.680 --> 00:07:22.320]   which came out of the world of category theory
[00:07:22.320 --> 00:07:24.440]   in the last 10 years.
[00:07:24.440 --> 00:07:27.440]   This nice blog post series by Pavel Sobochinsky
[00:07:27.440 --> 00:07:29.440]   that basically views linear algebra
[00:07:29.440 --> 00:07:31.920]   as what happens when adding meets copying,
[00:07:31.920 --> 00:07:34.400]   which is an interesting alternative way of looking
[00:07:34.400 --> 00:07:37.160]   at linear algebra and uses diagrammatic reasoning,
[00:07:37.160 --> 00:07:39.360]   like you see on the bottom right of the slide,
[00:07:39.360 --> 00:07:41.160]   rather than equational reasoning.
[00:07:41.160 --> 00:07:42.320]   It's completely rigorous.
[00:07:42.320 --> 00:07:44.640]   It's in some ways even more rigorous than the normal way
[00:07:44.640 --> 00:07:46.040]   that you do linear algebra, but it
[00:07:46.040 --> 00:07:49.600]   involves this very spatial picture-based reasoning.
[00:07:49.600 --> 00:07:51.560]   So if you're interested in abstract math,
[00:07:51.560 --> 00:07:53.920]   that's a fun direction to go.
[00:07:53.920 --> 00:07:55.480]   But I don't think either of those
[00:07:55.480 --> 00:07:58.400]   necessarily is the right approach for folks who work
[00:07:58.400 --> 00:07:59.800]   with software and hardware.
[00:07:59.800 --> 00:08:02.320]   For them, I think the right way to think about linear algebra
[00:08:02.320 --> 00:08:06.280]   is that linear algebra is more like programming.
[00:08:06.280 --> 00:08:08.920]   So matrices are our functions.
[00:08:08.920 --> 00:08:11.920]   Shapes are the types of our data and our functions.
[00:08:11.920 --> 00:08:14.840]   And multiplication is function composition.
[00:08:14.840 --> 00:08:16.960]   And this approach uses a bunch of ideas
[00:08:16.960 --> 00:08:19.840]   from programming, from computer science,
[00:08:19.840 --> 00:08:22.960]   rather than from, say, physics or from abstract math.
[00:08:22.960 --> 00:08:25.080]   So in programming, we combine functions
[00:08:25.080 --> 00:08:27.880]   with matching types through function composition.
[00:08:27.880 --> 00:08:31.320]   So we combine the functions together that way.
[00:08:31.320 --> 00:08:33.480]   In linear algebra, we combine matrices
[00:08:33.480 --> 00:08:36.240]   that have matching shapes through matrix multiplication.
[00:08:36.240 --> 00:08:38.520]   When we're doing programming, we define one function
[00:08:38.520 --> 00:08:39.920]   in terms of the others.
[00:08:39.920 --> 00:08:42.480]   So if I wanted to make a function that
[00:08:42.480 --> 00:08:44.320]   checks whether a string is too long to tweet,
[00:08:44.320 --> 00:08:47.080]   maybe tells a user, hey, this is too long,
[00:08:47.080 --> 00:08:49.760]   shorten that string up if you want to be able to tweet it.
[00:08:49.760 --> 00:08:52.560]   I might define that by composing a couple of functions.
[00:08:52.560 --> 00:08:54.400]   First, I check the length of the string.
[00:08:54.400 --> 00:08:57.520]   Then I check whether the length of that string is over 140.
[00:08:57.520 --> 00:08:59.440]   So I combine these two functions together.
[00:08:59.440 --> 00:09:01.720]   That gives me too long to tweet as a function.
[00:09:01.720 --> 00:09:04.960]   That composition, if I were to look at the type of len,
[00:09:04.960 --> 00:09:07.280]   it takes, say, a string and returns an integer.
[00:09:07.280 --> 00:09:08.640]   That's the length of the string.
[00:09:08.640 --> 00:09:11.520]   Over 140 takes an integer and returns a Boolean,
[00:09:11.520 --> 00:09:13.440]   a true or a false value.
[00:09:13.440 --> 00:09:17.280]   And so too long to tweet, take those two pieces, those arrows,
[00:09:17.280 --> 00:09:19.360]   and slap them onto each other.
[00:09:19.360 --> 00:09:22.120]   It now takes a string and returns a Boole.
[00:09:22.120 --> 00:09:23.960]   That composition of those two functions
[00:09:23.960 --> 00:09:27.360]   takes the input of one to the output of the other
[00:09:27.360 --> 00:09:28.200]   in terms of types.
[00:09:28.200 --> 00:09:30.920]   And we can see that if we draw the typical way of representing
[00:09:30.920 --> 00:09:33.120]   functions in at least the American mathematical education
[00:09:33.120 --> 00:09:34.960]   system, when you first learn about functions
[00:09:34.960 --> 00:09:37.320]   when you're in school, you see a picture
[00:09:37.320 --> 00:09:38.800]   that looks something like this.
[00:09:38.800 --> 00:09:41.680]   All the inputs are in a little circle on one side.
[00:09:41.680 --> 00:09:43.640]   Then the function is like an arrow
[00:09:43.640 --> 00:09:46.840]   that points from one of those inputs to the output.
[00:09:46.840 --> 00:09:49.320]   So a and b are both strings of length 1.
[00:09:49.320 --> 00:09:51.880]   The beginning of the Declaration of Independence,
[00:09:51.880 --> 00:09:54.200]   when in the course of human events it becomes necessary,
[00:09:54.200 --> 00:09:56.880]   that's a string of length 8,007.
[00:09:56.880 --> 00:10:01.000]   8,007 is bigger than 140, and so the Declaration of Independence
[00:10:01.000 --> 00:10:02.440]   is too long to tweet.
[00:10:02.440 --> 00:10:04.840]   So you can read off how these functions behave
[00:10:04.840 --> 00:10:06.840]   by following those arrows.
[00:10:06.840 --> 00:10:09.880]   So we get this purple arrow representing too long to tweet
[00:10:09.880 --> 00:10:13.880]   by following these red and blue arrows representing
[00:10:13.880 --> 00:10:15.400]   len and over 140.
[00:10:15.400 --> 00:10:17.840]   And really, this is the essence of programming.
[00:10:17.840 --> 00:10:20.040]   If you ever do functional programming, especially
[00:10:20.040 --> 00:10:23.320]   pure functional programming, this is all that you do.
[00:10:23.320 --> 00:10:26.200]   And it's enough to do anything that you can do in any language.
[00:10:26.200 --> 00:10:27.840]   We often do slightly different things
[00:10:27.840 --> 00:10:29.200]   in other programming languages.
[00:10:29.200 --> 00:10:31.240]   But in principle, this is the core
[00:10:31.240 --> 00:10:33.960]   of what we are doing when we are programming computers,
[00:10:33.960 --> 00:10:35.560]   combining these things together.
[00:10:35.560 --> 00:10:39.240]   And in linear algebra, we do much the same thing.
[00:10:39.240 --> 00:10:41.040]   So when we want to do function application,
[00:10:41.040 --> 00:10:43.120]   when we want to apply a function to data,
[00:10:43.120 --> 00:10:45.040]   that's matrix-vector multiplication.
[00:10:45.040 --> 00:10:47.360]   Often, we might think of matrix-vector multiplication
[00:10:47.360 --> 00:10:49.680]   as an equational thing, as just a matrix
[00:10:49.680 --> 00:10:52.120]   vector on the left-hand side is equal to the thing
[00:10:52.120 --> 00:10:54.680]   on the right-hand side, the way you would say 2 plus 2
[00:10:54.680 --> 00:10:55.800]   equals 4.
[00:10:55.800 --> 00:10:58.080]   But the view that I'm trying to put forward here
[00:10:58.080 --> 00:11:01.200]   is that you should instead think of this as m of v.
[00:11:01.200 --> 00:11:04.800]   m, the matrix applied as a function to the vector v,
[00:11:04.800 --> 00:11:08.000]   is equal to the thing on the right-hand side, which for now,
[00:11:08.000 --> 00:11:09.120]   we're just calling mv.
[00:11:09.120 --> 00:11:12.680]   Under that view, a matrix is a function
[00:11:12.680 --> 00:11:15.920]   that takes in arrays with a certain number of rows
[00:11:15.920 --> 00:11:19.000]   and puts out arrays with a certain number of rows.
[00:11:19.000 --> 00:11:22.080]   In this case, the number of rows in that vector,
[00:11:22.080 --> 00:11:24.360]   the length of that vector, is just k here
[00:11:24.360 --> 00:11:26.600]   to emphasize it could be a bunch of different things.
[00:11:26.600 --> 00:11:28.840]   But the output, the number of rows in the output,
[00:11:28.840 --> 00:11:30.920]   is given by the number of rows in that matrix.
[00:11:30.920 --> 00:11:32.920]   It's, in this case, 3.
[00:11:32.920 --> 00:11:35.480]   So like that function, too long to tweet,
[00:11:35.480 --> 00:11:38.600]   had a type signature that said it takes in strings
[00:11:38.600 --> 00:11:41.760]   and returns bools, this matrix has a type signature.
[00:11:41.760 --> 00:11:45.320]   It takes in arrays of length k1 and returns
[00:11:45.320 --> 00:11:47.800]   arrays of length 3, 1.
[00:11:47.800 --> 00:11:50.960]   Let's just pick a particular matrix here.
[00:11:50.960 --> 00:11:54.800]   This matrix x here that's 1, 0, 0, 0, 1, 0.
[00:11:54.800 --> 00:11:58.600]   And let's view it the way we would view a function
[00:11:58.600 --> 00:12:01.920]   in that ovals and arrows way of viewing things.
[00:12:01.920 --> 00:12:04.680]   You can see the inputs on the left, the outputs on the right.
[00:12:04.680 --> 00:12:07.360]   So the output 0, 1, 2, and 0, 1, 5
[00:12:07.360 --> 00:12:09.400]   both get taken to the same thing.
[00:12:09.400 --> 00:12:11.360]   And just as an exercise, if you were
[00:12:11.360 --> 00:12:14.640]   to name this the way you name a function in programming,
[00:12:14.640 --> 00:12:17.640]   what name would you give this matrix, x, here?
[00:12:17.640 --> 00:12:19.320]   When we're doing math, we often think
[00:12:19.320 --> 00:12:20.280]   of these things as variables.
[00:12:20.280 --> 00:12:21.800]   We don't give them descriptive names.
[00:12:21.800 --> 00:12:23.680]   We give them names like x and w.
[00:12:23.680 --> 00:12:26.600]   But when we're thinking about functions,
[00:12:26.600 --> 00:12:28.280]   we often give them a specific name.
[00:12:28.280 --> 00:12:31.360]   So this x here is taking in inputs
[00:12:31.360 --> 00:12:33.040]   that look like things on the left
[00:12:33.040 --> 00:12:37.200]   and returning things that come out on the right here.
[00:12:37.200 --> 00:12:40.560]   When we combine matrices through matrix multiplication,
[00:12:40.560 --> 00:12:44.320]   when I take two matrices and multiply them together,
[00:12:44.320 --> 00:12:46.760]   that is like function composition.
[00:12:46.760 --> 00:12:50.320]   That is like that combination of two functions.
[00:12:50.320 --> 00:12:52.960]   So for example, we could take that matrix x
[00:12:52.960 --> 00:12:56.520]   and then apply on the left matrix y.
[00:12:56.520 --> 00:12:59.120]   And we would then get a different function,
[00:12:59.120 --> 00:13:01.840]   a different matrix that does something different
[00:13:01.840 --> 00:13:02.840]   to its inputs.
[00:13:02.840 --> 00:13:04.920]   As an example here, I've got this first matrix
[00:13:04.920 --> 00:13:07.840]   we already talked about strips off that last entry.
[00:13:07.840 --> 00:13:10.600]   So it strips off the z-axis here.
[00:13:10.600 --> 00:13:12.880]   So then we get a vector that looks like this one
[00:13:12.880 --> 00:13:13.720]   in the middle here.
[00:13:13.720 --> 00:13:18.240]   So that red arrow here points from the inputs on the left
[00:13:18.240 --> 00:13:19.880]   to the outputs on the right.
[00:13:19.880 --> 00:13:22.960]   And then we can chain that with that second matrix,
[00:13:22.960 --> 00:13:25.200]   this matrix indicated in blue here,
[00:13:25.200 --> 00:13:27.680]   the y-matrix that takes two-dimensional inputs
[00:13:27.680 --> 00:13:29.920]   and returns two-dimensional outputs.
[00:13:29.920 --> 00:13:32.440]   And that one zeros out the y-axis.
[00:13:32.440 --> 00:13:34.000]   So then we end up with a vector that
[00:13:34.000 --> 00:13:38.160]   lies just along the x-axis with only the first entry
[00:13:38.160 --> 00:13:39.840]   of that vector that went in.
[00:13:39.840 --> 00:13:42.040]   And so I think this way of drawing things
[00:13:42.040 --> 00:13:44.080]   is maybe a little bit more common for vectors than that
[00:13:44.080 --> 00:13:45.520]   oval way of drawing things.
[00:13:45.520 --> 00:13:47.240]   So I wanted to make sure we covered that.
[00:13:47.240 --> 00:13:49.160]   The combination of those two things,
[00:13:49.160 --> 00:13:50.960]   the combination of those two functions,
[00:13:50.960 --> 00:13:53.240]   is itself also a function.
[00:13:53.240 --> 00:13:55.480]   And it's also, in fact, a matrix.
[00:13:55.480 --> 00:13:58.400]   And that matrix is obtained from the two sub-matrices
[00:13:58.400 --> 00:14:01.120]   by doing matrix multiplication.
[00:14:01.120 --> 00:14:02.920]   So that matrix multiplication rule,
[00:14:02.920 --> 00:14:05.880]   why did we use that specific matrix multiplication rule?
[00:14:05.880 --> 00:14:07.880]   It was so that this would be true.
[00:14:07.880 --> 00:14:11.480]   It was so that if we did matrix multiplication
[00:14:11.480 --> 00:14:14.880]   on the left of this matrix by this matrix,
[00:14:14.880 --> 00:14:18.760]   we would get this matrix representing their composition.
[00:14:18.760 --> 00:14:21.680]   So that is the reason for that matrix multiplication rule,
[00:14:21.680 --> 00:14:22.560]   fundamentally.
[00:14:22.560 --> 00:14:24.720]   This is the approach for thinking about linear algebra
[00:14:24.720 --> 00:14:27.520]   that I think is most fruitful for doing machine learning,
[00:14:27.520 --> 00:14:29.480]   and I think also for a lot of other branches
[00:14:29.480 --> 00:14:31.040]   that linear algebra gets applied in.
[00:14:31.040 --> 00:14:35.560]   But the remaining question is, why linear algebra
[00:14:35.560 --> 00:14:36.680]   for machine learning?
[00:14:36.680 --> 00:14:38.920]   We've understood a little bit more about linear algebra,
[00:14:38.920 --> 00:14:41.640]   but we haven't really tackled why this is important
[00:14:41.640 --> 00:14:42.760]   for machine learning yet.
[00:14:42.760 --> 00:14:45.400]   I said that we use arrays to represent functions
[00:14:45.400 --> 00:14:47.640]   and represent data in machine learning,
[00:14:47.640 --> 00:14:50.160]   but why this, and what kinds of functions
[00:14:50.160 --> 00:14:51.560]   can we represent that way?
[00:14:51.560 --> 00:14:54.720]   So why, basically, is linear algebra so important
[00:14:54.720 --> 00:14:57.720]   for optimization and, hence, machine learning?
[00:14:57.720 --> 00:15:00.760]   I think the right tack to get to understanding this
[00:15:00.760 --> 00:15:05.120]   is to think about how would we represent functions as data?
[00:15:05.120 --> 00:15:07.360]   So the fundamental question,
[00:15:07.360 --> 00:15:09.800]   if we want to program by optimization,
[00:15:09.800 --> 00:15:12.800]   is that we need to be able to represent programs
[00:15:12.800 --> 00:15:14.000]   and manipulate them.
[00:15:14.000 --> 00:15:16.240]   So we're gonna start with a program that doesn't work
[00:15:16.240 --> 00:15:19.360]   and then slowly, over time, obtain a program that does work.
[00:15:19.360 --> 00:15:21.520]   That's that process of optimization.
[00:15:21.520 --> 00:15:24.120]   In order to do that, we need to have that program
[00:15:24.120 --> 00:15:26.440]   and we need to be able to manipulate it somehow.
[00:15:26.440 --> 00:15:29.880]   The normal way that we represent programs as data
[00:15:29.880 --> 00:15:31.240]   is as strings, right?
[00:15:31.240 --> 00:15:34.040]   The usual way is buildwebsite.py, right?
[00:15:34.040 --> 00:15:37.480]   That is actually, in the end, a string of characters
[00:15:37.480 --> 00:15:40.080]   and that's how humans tend to manipulate programs.
[00:15:40.080 --> 00:15:43.320]   In general, we could also represent our programs
[00:15:43.320 --> 00:15:45.200]   as dictionaries, as lookup tables.
[00:15:45.200 --> 00:15:49.040]   It's basically applying that oval and arrow view
[00:15:49.040 --> 00:15:50.640]   of functions to your program.
[00:15:50.640 --> 00:15:52.800]   So those are different ways to do it.
[00:15:52.800 --> 00:15:55.600]   Neither one is actually really good for optimization.
[00:15:55.600 --> 00:15:58.360]   If we want to optimize things quantitatively,
[00:15:58.360 --> 00:16:01.240]   actually, the way that we represent programs is horrible,
[00:16:01.240 --> 00:16:02.080]   right?
[00:16:02.080 --> 00:16:05.840]   It's an absolutely horrible way to represent functions
[00:16:05.840 --> 00:16:07.800]   if we want to be able to optimize them.
[00:16:07.800 --> 00:16:11.200]   So just take an example, very simple Python program
[00:16:11.200 --> 00:16:14.440]   to check if a number is odd and return whether it's odd
[00:16:14.440 --> 00:16:15.480]   or even as a string.
[00:16:15.480 --> 00:16:18.280]   Pretty much any single character change to the code
[00:16:18.280 --> 00:16:19.440]   on the left would break it,
[00:16:19.440 --> 00:16:22.240]   would make it not do the right thing
[00:16:22.240 --> 00:16:25.200]   or it would completely break the program,
[00:16:25.200 --> 00:16:27.480]   make it not valid Python anymore.
[00:16:27.480 --> 00:16:29.880]   It's a great way for representing programs
[00:16:29.880 --> 00:16:31.720]   in order to get programmers to work with it,
[00:16:31.720 --> 00:16:34.960]   for humans to adjust the behavior of programs.
[00:16:34.960 --> 00:16:37.480]   But for a computer, that's really difficult
[00:16:37.480 --> 00:16:39.920]   to be able to make these coordinated changes.
[00:16:39.920 --> 00:16:43.760]   And it's very difficult to make that numerical quantitative
[00:16:43.760 --> 00:16:46.520]   in a way that we can actually automate this process,
[00:16:46.520 --> 00:16:48.640]   which is the goal of machine learning.
[00:16:48.640 --> 00:16:51.720]   So the main way that we make progress
[00:16:51.720 --> 00:16:55.120]   is by restricting ourselves to a smaller set of functions
[00:16:55.120 --> 00:16:58.160]   and focusing on making those work well.
[00:16:58.160 --> 00:17:01.040]   And so looking for a more limited data type
[00:17:01.040 --> 00:17:02.680]   that maybe doesn't necessarily represent
[00:17:02.680 --> 00:17:05.800]   all possible functions, but is still powerful.
[00:17:05.800 --> 00:17:08.200]   If we focus on easy to optimize functions,
[00:17:08.200 --> 00:17:11.120]   we'll get two separate branches of machine learning,
[00:17:11.120 --> 00:17:14.080]   one based off of trees that gets you to things
[00:17:14.080 --> 00:17:16.560]   like random forests, gradient boosted trees,
[00:17:16.560 --> 00:17:19.080]   things like that, which we won't be talking about.
[00:17:19.080 --> 00:17:22.320]   And the other direction, which is towards using arrays.
[00:17:22.320 --> 00:17:24.960]   So using matrix multiplication as function application
[00:17:24.960 --> 00:17:27.760]   and thinking about only linear functions,
[00:17:27.760 --> 00:17:28.600]   at least at first.
[00:17:28.600 --> 00:17:30.880]   And the benefit is that arrays are really easy
[00:17:30.880 --> 00:17:32.600]   to change just a tiny bit.
[00:17:32.600 --> 00:17:35.440]   If I make a tiny change to the entries of a matrix,
[00:17:35.440 --> 00:17:38.480]   then I'll get a small change in the function
[00:17:38.480 --> 00:17:39.840]   that the matrix applies.
[00:17:39.840 --> 00:17:41.800]   So the outputs will only change a little bit
[00:17:41.800 --> 00:17:43.560]   if I change the matrix a little bit.
[00:17:43.560 --> 00:17:45.920]   And as we'll see, when we talk about calculus,
[00:17:45.920 --> 00:17:48.960]   that's huge, that's extremely valuable.
[00:17:48.960 --> 00:17:50.600]   And then of course, there's what we talked about,
[00:17:50.600 --> 00:17:52.560]   linear algebra being able to be made fast
[00:17:52.560 --> 00:17:54.400]   and there being all these really useful things
[00:17:54.400 --> 00:17:56.080]   that you can do with it.
[00:17:56.080 --> 00:17:59.480]   And so at its core, this is why linear algebra
[00:17:59.480 --> 00:18:01.520]   is so important for machine learning.
[00:18:01.520 --> 00:18:03.480]   I wanna go over a little bit what I mean
[00:18:03.480 --> 00:18:05.480]   by arrays represent linear functions.
[00:18:05.480 --> 00:18:08.280]   And now it's finally time to do a little bit of algebra.
[00:18:08.280 --> 00:18:10.440]   There's basically two rules for something
[00:18:10.440 --> 00:18:12.280]   to be a linear function.
[00:18:12.280 --> 00:18:15.720]   It needs to respect the first rule here,
[00:18:15.720 --> 00:18:18.720]   F of A plus B is F of A plus F of B,
[00:18:18.720 --> 00:18:20.760]   distribution over addition rule.
[00:18:20.760 --> 00:18:23.600]   And then it also needs to respect this distribution
[00:18:23.600 --> 00:18:26.680]   over multiplication rule, F of lambda times A
[00:18:26.680 --> 00:18:29.200]   is equal to lambda times F of A.
[00:18:29.200 --> 00:18:31.560]   So that says if I scale the input,
[00:18:31.560 --> 00:18:33.160]   then I just scale the output.
[00:18:33.160 --> 00:18:37.080]   And if I look at the output on the sum of two inputs,
[00:18:37.080 --> 00:18:38.760]   it's the sum of those outputs.
[00:18:38.760 --> 00:18:42.160]   So what this allows us to do is rearrange a problem
[00:18:42.160 --> 00:18:45.800]   that may come in looking like F of A plus F of B,
[00:18:45.800 --> 00:18:48.280]   which would require us to compute something twice
[00:18:48.280 --> 00:18:51.640]   and swap that around, just computing it once,
[00:18:51.640 --> 00:18:52.840]   F of A plus B.
[00:18:52.840 --> 00:18:54.640]   The reason why we have those two rules
[00:18:54.640 --> 00:18:56.360]   is 'cause that means that linear functions
[00:18:56.360 --> 00:18:59.000]   play really nicely with weighted sums.
[00:18:59.000 --> 00:19:00.560]   If I combine those two rules,
[00:19:00.560 --> 00:19:02.000]   I can use them over and over again.
[00:19:02.000 --> 00:19:04.240]   If I multiply by a bunch of different scalars,
[00:19:04.240 --> 00:19:06.560]   and if I add a whole bunch of things together,
[00:19:06.560 --> 00:19:09.680]   that's what this thing is doing here in the center.
[00:19:09.680 --> 00:19:11.360]   I can pull that out as well,
[00:19:11.360 --> 00:19:13.880]   just as I could pull the plus sign out
[00:19:13.880 --> 00:19:16.880]   from the inside of the function application here,
[00:19:16.880 --> 00:19:19.040]   and I could pull the scalar multiplication
[00:19:19.040 --> 00:19:21.800]   out to the outside of function application.
[00:19:21.800 --> 00:19:25.160]   I can do that with a whole bunch of sums and multiplications.
[00:19:25.160 --> 00:19:28.520]   So anytime that I do a weighted sum of things,
[00:19:28.520 --> 00:19:31.160]   my linear function will be able to either go on the inside
[00:19:31.160 --> 00:19:33.800]   or the outside, and that's huge.
[00:19:33.800 --> 00:19:36.120]   Weighted sums show up all over the place
[00:19:36.120 --> 00:19:38.440]   and basically makes linear functions
[00:19:38.440 --> 00:19:40.400]   really easy to reason about.
[00:19:40.400 --> 00:19:42.680]   So we'll see how this works
[00:19:42.680 --> 00:19:46.400]   with how linear functions interact with the number zero.
[00:19:46.400 --> 00:19:50.280]   So first off, zero is gonna always be sent to zero.
[00:19:50.280 --> 00:19:53.040]   So we get this from our multiplication rule.
[00:19:53.040 --> 00:19:54.720]   If I apply F to zero,
[00:19:54.720 --> 00:19:57.840]   that's the same as applying F to lambda times zero,
[00:19:57.840 --> 00:20:00.000]   'cause lambda times zero is zero.
[00:20:00.000 --> 00:20:01.680]   And I can then pull that out,
[00:20:01.680 --> 00:20:03.760]   which means that lambda times F of zero
[00:20:03.760 --> 00:20:05.080]   is equal to F of zero.
[00:20:05.080 --> 00:20:08.920]   That can only be true if F of zero is zero.
[00:20:08.920 --> 00:20:11.360]   If inputs collide,
[00:20:11.360 --> 00:20:14.240]   we can also determine using these rules
[00:20:14.240 --> 00:20:16.080]   that more things have to be sent to zero.
[00:20:16.080 --> 00:20:18.520]   So if two things get sent to the same thing,
[00:20:18.520 --> 00:20:21.400]   then their difference has to be sent to zero.
[00:20:21.400 --> 00:20:24.400]   If F of A minus F of B is zero,
[00:20:24.400 --> 00:20:28.800]   so if A and B are sent to the same output by F,
[00:20:28.800 --> 00:20:31.400]   then F of A minus B is zero.
[00:20:31.400 --> 00:20:33.440]   So that difference between A and B,
[00:20:33.440 --> 00:20:35.200]   that thing gets sent to zero.
[00:20:35.200 --> 00:20:36.040]   So we can do that,
[00:20:36.040 --> 00:20:38.480]   we're just pulling that minus sign inside.
[00:20:38.480 --> 00:20:40.040]   And if you're being really technical,
[00:20:40.040 --> 00:20:41.680]   we're basically pulling a minus one inside
[00:20:41.680 --> 00:20:43.240]   and then using the addition rule.
[00:20:43.240 --> 00:20:44.840]   Anything that is sent to zero
[00:20:44.840 --> 00:20:47.040]   is called the kernel of that function.
[00:20:47.040 --> 00:20:50.560]   A minus B here is in the kernel of the function.
[00:20:50.560 --> 00:20:52.120]   So one thing that comes out of this
[00:20:52.120 --> 00:20:55.280]   is you can also check that result that we got
[00:20:55.280 --> 00:20:57.080]   that zero has to be sent to zero.
[00:20:57.080 --> 00:20:59.680]   'Cause if I set A and B equal to each other,
[00:20:59.680 --> 00:21:01.840]   then I get that same result.
[00:21:01.840 --> 00:21:03.080]   So zero's in the kernel,
[00:21:03.080 --> 00:21:04.920]   zero is always sent to zero.
[00:21:04.920 --> 00:21:07.440]   Other things might also be in the kernel.
[00:21:07.440 --> 00:21:10.400]   That kernel there is made up of weighted sum.
[00:21:10.400 --> 00:21:12.800]   Going back to this idea of weighted sums,
[00:21:12.800 --> 00:21:15.920]   if F of A is zero and F of B is zero,
[00:21:15.920 --> 00:21:18.320]   then A plus B is also sent to zero.
[00:21:18.320 --> 00:21:19.840]   So F of A plus B,
[00:21:19.840 --> 00:21:21.600]   that's the same of F of A plus F of B,
[00:21:21.600 --> 00:21:22.600]   those are both zero,
[00:21:22.600 --> 00:21:23.720]   so the output is zero.
[00:21:23.720 --> 00:21:26.640]   And so this collection of things that gets sent to zero
[00:21:26.640 --> 00:21:28.280]   is made up of weighted sums.
[00:21:28.280 --> 00:21:29.680]   If I take a whole bunch of things
[00:21:29.680 --> 00:21:30.880]   that are all sent to zero,
[00:21:30.880 --> 00:21:33.880]   and all I can do is combine them by weighted summing,
[00:21:33.880 --> 00:21:36.560]   then I'm still gonna stay in this collection of things
[00:21:36.560 --> 00:21:37.400]   sent to zero.
[00:21:37.400 --> 00:21:39.360]   The same is true of the collection of things
[00:21:39.360 --> 00:21:40.360]   not sent to zero.
[00:21:40.360 --> 00:21:42.000]   Anything not in the kernel,
[00:21:42.000 --> 00:21:45.120]   if I combine those things with weighted sums,
[00:21:45.120 --> 00:21:47.240]   then they'll stay outside of the kernel.
[00:21:47.240 --> 00:21:49.560]   I went through all this in order to get to
[00:21:49.560 --> 00:21:50.680]   this important concept,
[00:21:50.680 --> 00:21:54.200]   the rank of the function or the rank of the matrix.
[00:21:54.200 --> 00:21:56.600]   We can make new non-kernel elements,
[00:21:56.600 --> 00:21:58.560]   new things not sent to zero,
[00:21:58.560 --> 00:22:02.280]   by making weighted sums of known non-kernel elements.
[00:22:02.280 --> 00:22:03.960]   And rank answers the question,
[00:22:03.960 --> 00:22:06.000]   how many of these things do I need
[00:22:06.000 --> 00:22:08.120]   in order to make every possible thing
[00:22:08.120 --> 00:22:09.360]   that's not in the kernel?
[00:22:09.360 --> 00:22:11.080]   This collection of things sent to zero,
[00:22:11.080 --> 00:22:12.720]   zero is not that interesting.
[00:22:12.720 --> 00:22:13.960]   So this is sort of telling us
[00:22:13.960 --> 00:22:16.240]   how many things do I need in order to understand
[00:22:16.240 --> 00:22:19.720]   all the interesting behavior of this matrix as a function
[00:22:19.720 --> 00:22:21.440]   or of this linear function.
[00:22:21.440 --> 00:22:23.560]   So that was just a little sort of quick exercise
[00:22:23.560 --> 00:22:26.920]   to give you a flavor for how reasoning works
[00:22:26.920 --> 00:22:28.520]   with linear functions
[00:22:28.520 --> 00:22:30.960]   and how much you can learn about linear functions
[00:22:30.960 --> 00:22:34.720]   with just a few quick linear algebraic manipulations.
[00:22:34.720 --> 00:22:38.240]   But I wanted to return to this idea of
[00:22:38.240 --> 00:22:39.920]   linear algebra as programming,
[00:22:39.920 --> 00:22:43.040]   linear algebra as less like those algebraic manipulations
[00:22:43.040 --> 00:22:46.320]   and more like the things we do when we program computers.
[00:22:46.320 --> 00:22:48.520]   The singular value decomposition
[00:22:48.520 --> 00:22:51.440]   is a important concept in linear algebra
[00:22:51.440 --> 00:22:53.320]   and in the understanding of matrices.
[00:22:53.320 --> 00:22:57.880]   And in my view, it is the equivalent of refactoring a program
[00:22:57.880 --> 00:22:59.120]   but for linear algebra.
[00:22:59.120 --> 00:23:03.080]   Refactoring programs means taking a program
[00:23:03.080 --> 00:23:07.160]   and rewriting it, but not changing the way it behaves.
[00:23:07.160 --> 00:23:08.800]   So you change the internals maybe,
[00:23:08.800 --> 00:23:11.880]   you change just exactly what's going on under the hood,
[00:23:11.880 --> 00:23:15.680]   but you don't change anything about its overall behavior.
[00:23:15.680 --> 00:23:17.880]   The same inputs still go to the same output.
[00:23:17.880 --> 00:23:19.520]   So we often refactor programs
[00:23:19.520 --> 00:23:21.000]   in order to understand them better.
[00:23:21.000 --> 00:23:23.840]   Sometimes we do it in order to make them run faster.
[00:23:23.840 --> 00:23:25.000]   There's lots of different reasons
[00:23:25.000 --> 00:23:27.200]   why we might refactor a program.
[00:23:27.200 --> 00:23:29.720]   And there's a couple of common tricks for doing that.
[00:23:29.720 --> 00:23:32.600]   One is to identify separation of concerns,
[00:23:32.600 --> 00:23:34.760]   to say, oh, this one piece of the program
[00:23:34.760 --> 00:23:36.040]   is doing too many things.
[00:23:36.040 --> 00:23:38.080]   I need to separate those out from each other.
[00:23:38.080 --> 00:23:39.440]   So in linear algebra,
[00:23:39.440 --> 00:23:41.560]   the equivalent of that is eigen decomposition.
[00:23:41.560 --> 00:23:43.600]   It says, okay, this matrix is working
[00:23:43.600 --> 00:23:45.320]   on an n-dimensional thing,
[00:23:45.320 --> 00:23:48.680]   but it's really actually doing n one-dimensional things,
[00:23:48.680 --> 00:23:49.600]   for example.
[00:23:49.600 --> 00:23:52.640]   And that's exactly what is done in eigen decomposition.
[00:23:52.640 --> 00:23:54.840]   We find those n one-dimensional pieces
[00:23:54.840 --> 00:23:56.240]   and separate them out.
[00:23:56.240 --> 00:23:58.160]   That separation of concerns.
[00:23:58.160 --> 00:24:00.600]   We can also remove code that doesn't do anything, right?
[00:24:00.600 --> 00:24:02.440]   You may identify, oh, this branch of code
[00:24:02.440 --> 00:24:04.720]   is effectively doing nothing.
[00:24:04.720 --> 00:24:06.800]   It's here, but it's dead weight.
[00:24:06.800 --> 00:24:08.560]   And that's very close to what we do
[00:24:08.560 --> 00:24:10.720]   when we do low rank approximation.
[00:24:10.720 --> 00:24:12.520]   When we try and find a matrix
[00:24:12.520 --> 00:24:15.040]   that has much lower rank than the matrix,
[00:24:15.040 --> 00:24:17.080]   but still does about the same thing.
[00:24:17.080 --> 00:24:19.960]   So we'll talk about low rank approximation in a little bit.
[00:24:19.960 --> 00:24:21.720]   The other piece that we often do
[00:24:21.720 --> 00:24:23.120]   when we refactor a program
[00:24:23.120 --> 00:24:25.800]   is we break something up into multiple functions.
[00:24:25.800 --> 00:24:28.200]   This is much like separation of concerns,
[00:24:28.200 --> 00:24:31.400]   but instead of maybe splitting it up in parallel,
[00:24:31.400 --> 00:24:35.560]   we're splitting it up in that we are taking one function
[00:24:35.560 --> 00:24:38.480]   that is doing several things in a row,
[00:24:38.480 --> 00:24:42.760]   and we are decomposing that into a bunch of pieces
[00:24:42.760 --> 00:24:45.880]   that then when composed together, still do the same thing.
[00:24:45.880 --> 00:24:48.400]   So maybe I was loading data,
[00:24:48.400 --> 00:24:50.240]   then applying a function to data,
[00:24:50.240 --> 00:24:52.720]   then writing it to disk all in a single function.
[00:24:52.720 --> 00:24:55.560]   Let's split that up into three functions
[00:24:55.560 --> 00:24:58.040]   that each do those three pieces.
[00:24:58.040 --> 00:25:00.200]   And then that's our final result.
[00:25:00.200 --> 00:25:03.680]   That's a refactor to break it up into those three pieces.
[00:25:03.680 --> 00:25:05.040]   And that is the equivalent
[00:25:05.040 --> 00:25:07.160]   of the singular value decomposition.
[00:25:07.160 --> 00:25:09.800]   So in linear algebra, we have the singular value decomposition
[00:25:09.800 --> 00:25:12.520]   decomposing, breaking up into pieces,
[00:25:12.520 --> 00:25:14.240]   undoing a composition,
[00:25:14.240 --> 00:25:16.120]   and it achieves the same thing
[00:25:16.120 --> 00:25:19.000]   as that particular type of refactoring in programming.
[00:25:19.000 --> 00:25:20.720]   So in general, in fact,
[00:25:20.720 --> 00:25:22.480]   any function can be decomposed
[00:25:22.480 --> 00:25:24.640]   the way we do in the singular value decomposition,
[00:25:24.640 --> 00:25:26.000]   which I think is very cool,
[00:25:26.000 --> 00:25:27.880]   which is you basically break your function up
[00:25:27.880 --> 00:25:29.200]   into three pieces.
[00:25:29.200 --> 00:25:31.400]   So I've got this diagram here.
[00:25:31.400 --> 00:25:33.960]   We're gonna go through an example with that isOdd function.
[00:25:33.960 --> 00:25:36.560]   So we had this single isOdd function here
[00:25:36.560 --> 00:25:38.680]   that just did it all in a couple of lines
[00:25:38.680 --> 00:25:40.640]   and goes straight from integers to strings.
[00:25:40.640 --> 00:25:43.560]   But that function can actually be broken down into pieces.
[00:25:43.560 --> 00:25:44.600]   And this can be useful.
[00:25:44.600 --> 00:25:47.520]   Refactoring can be useful in case maybe sometime later,
[00:25:47.520 --> 00:25:48.760]   somebody says, "Oh, we actually wanted
[00:25:48.760 --> 00:25:49.920]   "to not return strings,
[00:25:49.920 --> 00:25:52.520]   "but to return just the Booleans instead."
[00:25:52.520 --> 00:25:55.360]   Or, "We actually wanted to do this mod three
[00:25:55.360 --> 00:25:56.640]   "rather than mod two."
[00:25:56.640 --> 00:25:58.000]   And so breaking it up into pieces
[00:25:58.000 --> 00:26:00.160]   can make it easier to make those changes later.
[00:26:00.160 --> 00:26:02.760]   If we wanna break this function down into pieces,
[00:26:02.760 --> 00:26:05.320]   there's a sort of natural way to break it down in my view,
[00:26:05.320 --> 00:26:08.200]   which is to first do the part where we determine
[00:26:08.200 --> 00:26:10.880]   whether this number is even or odd
[00:26:10.880 --> 00:26:12.760]   with this mod two operation.
[00:26:12.760 --> 00:26:14.880]   Then we apply toBool.
[00:26:14.880 --> 00:26:16.600]   We take that integer and we turn it
[00:26:16.600 --> 00:26:18.560]   into a true or false value.
[00:26:18.560 --> 00:26:21.040]   And then we do something different for true and for false.
[00:26:21.040 --> 00:26:25.560]   So for true, we return odd, and for false, we return even.
[00:26:25.560 --> 00:26:28.040]   So that's what this breakdown achieves here.
[00:26:28.040 --> 00:26:31.360]   This split up into the mod two toBool toString setup.
[00:26:31.360 --> 00:26:34.960]   And this actually, this represents a very generic strategy.
[00:26:34.960 --> 00:26:37.080]   So one way of thinking about this
[00:26:37.080 --> 00:26:42.080]   is that mod two picks representatives for this function.
[00:26:42.080 --> 00:26:44.920]   There are really only two outputs here.
[00:26:44.920 --> 00:26:46.960]   There's only odd and there's even.
[00:26:46.960 --> 00:26:49.840]   And so really, we only need to know
[00:26:49.840 --> 00:26:52.040]   what this function does on two inputs,
[00:26:52.040 --> 00:26:56.760]   and then know which inputs correspond to those two classes.
[00:26:56.760 --> 00:26:59.120]   There's gonna be one representative for odd,
[00:26:59.120 --> 00:27:00.600]   one representative for even.
[00:27:00.600 --> 00:27:02.120]   And in this first step here, we're gonna say,
[00:27:02.120 --> 00:27:04.480]   "Okay, do you get sent to the same value as zero,
[00:27:04.480 --> 00:27:07.160]   "or do you get sent to the same value as one?"
[00:27:07.160 --> 00:27:09.280]   That sort of simplifies this function down.
[00:27:09.280 --> 00:27:12.560]   After this point, when it comes to mapping those inputs
[00:27:12.560 --> 00:27:14.440]   to the final output, we only need to do it
[00:27:14.440 --> 00:27:16.400]   for those two specific representatives.
[00:27:16.400 --> 00:27:18.320]   And it makes this first step just a matter of saying,
[00:27:18.320 --> 00:27:20.440]   "Okay, these guys are all gonna get treated the same.
[00:27:20.440 --> 00:27:22.680]   "These guys are all gonna get treated the same."
[00:27:22.680 --> 00:27:26.120]   So that's our first step here, picking out representatives.
[00:27:26.120 --> 00:27:27.160]   Then we have a step
[00:27:27.160 --> 00:27:29.320]   that's just a reversible renaming step.
[00:27:29.320 --> 00:27:32.880]   So associate each representative with its output, one to one.
[00:27:32.880 --> 00:27:36.200]   So zero is going to be even.
[00:27:36.200 --> 00:27:37.720]   It is false that this is odd.
[00:27:37.720 --> 00:27:38.920]   So that gets mapped to false.
[00:27:38.920 --> 00:27:40.240]   One gets mapped to true.
[00:27:40.240 --> 00:27:42.320]   And the important thing here is that each output
[00:27:42.320 --> 00:27:45.320]   is targeted by one and only one representative.
[00:27:45.320 --> 00:27:49.000]   So there's no mixing, no combination, no splitting,
[00:27:49.000 --> 00:27:51.840]   just one to one, a renaming operation.
[00:27:51.840 --> 00:27:53.320]   And then finally, we just need to put them
[00:27:53.320 --> 00:27:54.480]   into the correct type.
[00:27:54.480 --> 00:27:56.200]   We got a true and a false here,
[00:27:56.200 --> 00:27:59.000]   but what we wanted was actually a string at the end,
[00:27:59.000 --> 00:28:00.720]   'cause this is going out to a user
[00:28:00.720 --> 00:28:02.520]   who wants to know whether this is odd or even.
[00:28:02.520 --> 00:28:04.040]   We're effectively just recognizing
[00:28:04.040 --> 00:28:06.360]   a true and false are the outputs here,
[00:28:06.360 --> 00:28:08.680]   and we just need to map them onto what the string is
[00:28:08.680 --> 00:28:09.800]   that we wanna show the user.
[00:28:09.800 --> 00:28:11.680]   So we're finding essentially a copy
[00:28:11.680 --> 00:28:14.280]   of the output of tubool of true and false
[00:28:14.280 --> 00:28:16.360]   inside of the type string.
[00:28:16.360 --> 00:28:19.280]   And we can apply that same breakdown to a matrix.
[00:28:19.280 --> 00:28:21.880]   That breakdown there is generic for any function.
[00:28:21.880 --> 00:28:24.040]   It's called the canonical decomposition.
[00:28:24.040 --> 00:28:26.840]   And we can apply that canonical decomposition
[00:28:26.840 --> 00:28:28.000]   onto a matrix.
[00:28:28.000 --> 00:28:29.720]   The equational way of writing
[00:28:29.720 --> 00:28:32.720]   is to say that M equals ABC.
[00:28:32.720 --> 00:28:35.240]   So A times B times C.
[00:28:35.240 --> 00:28:38.640]   The sort of function view is to say that the function M
[00:28:38.640 --> 00:28:42.040]   that maps arrays of dimension N
[00:28:42.040 --> 00:28:43.960]   to arrays of dimension M,
[00:28:43.960 --> 00:28:45.760]   that function can be broken down
[00:28:45.760 --> 00:28:48.480]   into the composition of three functions,
[00:28:48.480 --> 00:28:50.480]   C, then B, then A.
[00:28:50.480 --> 00:28:53.600]   So this diagram here says the same thing
[00:28:53.600 --> 00:28:55.760]   as this equation over here.
[00:28:55.760 --> 00:28:59.840]   And in this setup, C is a wide matrix,
[00:28:59.840 --> 00:29:01.880]   B is a square matrix,
[00:29:01.880 --> 00:29:03.800]   and A is a tall matrix.
[00:29:03.800 --> 00:29:07.760]   So C has at least as many columns as it has rows.
[00:29:07.760 --> 00:29:10.840]   B has exactly as many columns as it has rows.
[00:29:10.840 --> 00:29:14.520]   And A has at least as many rows as it has columns.
[00:29:14.520 --> 00:29:16.120]   Typical case to think in mind here
[00:29:16.120 --> 00:29:18.400]   is where C is much wider than it is tall,
[00:29:18.400 --> 00:29:20.440]   and A is much taller than it is wide.
[00:29:20.440 --> 00:29:25.440]   C here is going to have R rows,
[00:29:25.560 --> 00:29:26.760]   N columns.
[00:29:26.760 --> 00:29:28.000]   N is the number of inputs,
[00:29:28.000 --> 00:29:30.040]   so it needs to have that many columns
[00:29:30.040 --> 00:29:32.240]   in order to be able to take the same inputs as M,
[00:29:32.240 --> 00:29:35.120]   but it's gonna map it down to only R outputs.
[00:29:35.120 --> 00:29:36.760]   So it's gonna shrink it down.
[00:29:36.760 --> 00:29:39.680]   And what this is doing is it's throwing out that kernel.
[00:29:39.680 --> 00:29:42.680]   So if A and B are sent to the same output by M,
[00:29:42.680 --> 00:29:45.520]   so we're picking out representatives in this step.
[00:29:45.520 --> 00:29:48.600]   We're saying any things that get sent to the same thing,
[00:29:48.600 --> 00:29:51.040]   we want to map those all to one value
[00:29:51.040 --> 00:29:52.880]   and only worry about one value,
[00:29:52.880 --> 00:29:55.880]   like how we did that mod two operation for is odd to say,
[00:29:55.880 --> 00:29:58.200]   okay, I know what I want to do with zero and one.
[00:29:58.200 --> 00:30:01.120]   Let me map everything that is the same as zero to zero,
[00:30:01.120 --> 00:30:02.800]   everything that's the same as one to one.
[00:30:02.800 --> 00:30:04.960]   So here we're doing that same thing.
[00:30:04.960 --> 00:30:06.560]   And now we're looking for two things
[00:30:06.560 --> 00:30:08.800]   that get sent to the same output by M.
[00:30:08.800 --> 00:30:11.720]   And so that connects us back to that kernel, right?
[00:30:11.720 --> 00:30:13.280]   And to that idea of rank.
[00:30:13.280 --> 00:30:15.120]   How many things do I need
[00:30:15.120 --> 00:30:17.760]   in order to build all possible outputs?
[00:30:17.760 --> 00:30:20.840]   And so that's why this output here has dimension R.
[00:30:20.840 --> 00:30:23.520]   It has the same size as that set
[00:30:23.520 --> 00:30:26.280]   that we need to build all non-kernel elements.
[00:30:26.280 --> 00:30:28.840]   But what effectively we're doing here at a high level,
[00:30:28.840 --> 00:30:30.840]   you can think of it as throwing out everything
[00:30:30.840 --> 00:30:31.960]   that gets sent to zero.
[00:30:31.960 --> 00:30:34.360]   Then that second step here with B
[00:30:34.360 --> 00:30:36.760]   is our reversible relabeling.
[00:30:36.760 --> 00:30:38.280]   That's why this matrix is square.
[00:30:38.280 --> 00:30:40.960]   It takes in R inputs and returns R outputs.
[00:30:40.960 --> 00:30:42.480]   So it's just a relabeling.
[00:30:42.480 --> 00:30:45.320]   It says, okay, I'm going to maybe like slightly change
[00:30:45.320 --> 00:30:48.040]   around my axes, but I'm not really making
[00:30:48.040 --> 00:30:49.280]   any serious changes.
[00:30:49.280 --> 00:30:50.760]   And it's always reversible.
[00:30:50.760 --> 00:30:51.600]   It's square.
[00:30:51.600 --> 00:30:53.680]   So the inputs are the same size as the outputs.
[00:30:53.680 --> 00:30:55.640]   That means it's possible for it to be reversible.
[00:30:55.640 --> 00:30:58.280]   If the outputs are way bigger or way smaller
[00:30:58.280 --> 00:31:01.000]   than the inputs, then it's not always going to be possible
[00:31:01.000 --> 00:31:02.680]   to reverse that operation.
[00:31:02.680 --> 00:31:05.960]   And so those matrices A and C are in general, not reversible.
[00:31:05.960 --> 00:31:08.240]   So this is our special reversible step.
[00:31:08.240 --> 00:31:10.800]   And then lastly, we have a tall matrix at the end,
[00:31:10.800 --> 00:31:13.680]   A whose outputs can be bigger than its inputs.
[00:31:13.680 --> 00:31:16.120]   And it effectively finds a copy of all rays
[00:31:16.120 --> 00:31:19.320]   with R elements among the set of all M element arrays.
[00:31:19.320 --> 00:31:21.520]   For example, if you can see my video here,
[00:31:21.520 --> 00:31:24.720]   if I wanted to find a copy of two dimensional arrays
[00:31:24.720 --> 00:31:26.640]   among the set of all three dimensional rays,
[00:31:26.640 --> 00:31:28.400]   there are lots of possible examples.
[00:31:28.400 --> 00:31:30.760]   My fingers here represent two axes
[00:31:30.760 --> 00:31:33.040]   and I'm moving them around in 3D space
[00:31:33.040 --> 00:31:36.040]   to represent a whole bunch of different potential copies
[00:31:36.040 --> 00:31:38.000]   of the set of all two dimensional arrays
[00:31:38.000 --> 00:31:40.760]   among the set of all three dimensional arrays,
[00:31:40.760 --> 00:31:43.360]   the three dimensional space, like the one we live in.
[00:31:43.360 --> 00:31:46.320]   So this step here says, okay, these are dimensional arrays
[00:31:46.320 --> 00:31:47.760]   that came out of B.
[00:31:47.760 --> 00:31:50.840]   I want to match that to the outputs of M on its inputs.
[00:31:50.840 --> 00:31:53.120]   And so those outputs have dimension M.
[00:31:53.120 --> 00:31:55.840]   I need to sort of inject these R dimensional rays
[00:31:55.840 --> 00:31:57.280]   into that output space.
[00:31:57.280 --> 00:31:59.800]   This is very similar to choosing to say, okay,
[00:31:59.800 --> 00:32:03.760]   true corresponds to odd and false corresponds to even
[00:32:03.760 --> 00:32:05.360]   in our is odd example.
[00:32:05.360 --> 00:32:08.280]   There's many possible choices of two words
[00:32:08.280 --> 00:32:09.880]   out of the set of all possible strings.
[00:32:09.880 --> 00:32:11.680]   And we chose those in particular
[00:32:11.680 --> 00:32:13.840]   'cause that's what our function was supposed to output.
[00:32:13.840 --> 00:32:17.120]   So that gives us this decomposition into three pieces.
[00:32:17.120 --> 00:32:19.120]   One way to think about it is
[00:32:19.120 --> 00:32:20.800]   if you're familiar with these ideas,
[00:32:20.800 --> 00:32:23.880]   the first function is the onto piece.
[00:32:23.880 --> 00:32:27.480]   It's the surjection piece of the function M.
[00:32:27.480 --> 00:32:30.880]   The part in the middle is the bijective piece
[00:32:30.880 --> 00:32:34.080]   of the function or the isomorphism piece of the function,
[00:32:34.080 --> 00:32:36.280]   the reversible piece of the function.
[00:32:36.280 --> 00:32:39.680]   And then A is the injective piece of the function,
[00:32:39.680 --> 00:32:42.680]   the part that is into one-to-one.
[00:32:42.680 --> 00:32:44.440]   So we're breaking our function down
[00:32:44.440 --> 00:32:47.000]   into those three types of pieces.
[00:32:47.000 --> 00:32:50.200]   The function M need not have any of those properties,
[00:32:50.200 --> 00:32:52.800]   but it has pieces that each have those properties
[00:32:52.800 --> 00:32:56.640]   of injectivity, bijectivity and surjectivity.
[00:32:56.640 --> 00:33:00.600]   So that's this way of breaking down matrices in general.
[00:33:00.600 --> 00:33:03.760]   If we make certain special choices,
[00:33:03.760 --> 00:33:06.440]   then we get this singular value decomposition.
[00:33:06.440 --> 00:33:09.240]   If we aim to make that middle matrix diagonal,
[00:33:09.240 --> 00:33:12.240]   so it only has numbers in the rows and columns
[00:33:12.240 --> 00:33:13.360]   that are equal to each other,
[00:33:13.360 --> 00:33:15.520]   row one, column one, row two, column two,
[00:33:15.520 --> 00:33:18.000]   that's represented by this dashed line here.
[00:33:18.000 --> 00:33:21.880]   Then we end up with this singular value decomposition
[00:33:21.880 --> 00:33:25.400]   where U and V are unitary matrices.
[00:33:25.400 --> 00:33:27.280]   They don't grow or shrink anything.
[00:33:27.280 --> 00:33:29.040]   In addition to throwing stuff out,
[00:33:29.040 --> 00:33:31.920]   all they do is change basis, change the axes,
[00:33:31.920 --> 00:33:33.760]   spin them around or reflect them.
[00:33:33.760 --> 00:33:36.680]   That's what the two matrices C and A
[00:33:36.680 --> 00:33:39.720]   become these unitary matrices U and V.
[00:33:39.720 --> 00:33:42.280]   And then that is the singular value decomposition.
[00:33:42.280 --> 00:33:44.160]   But the important thing about the singular value
[00:33:44.160 --> 00:33:47.240]   decomposition in my view is that it breaks down
[00:33:47.240 --> 00:33:51.440]   those three pieces of surjection, bijection and injection.
[00:33:51.440 --> 00:33:54.640]   This SVD is a very special way to break up a matrix
[00:33:54.640 --> 00:33:56.680]   as maybe indicated by its relationship
[00:33:56.680 --> 00:33:58.240]   to this canonical decomposition,
[00:33:58.240 --> 00:34:00.520]   this canonical way of breaking down matrices.
[00:34:00.520 --> 00:34:02.680]   And one reason why is it can be used
[00:34:02.680 --> 00:34:05.200]   to calculate low rank approximations.
[00:34:05.200 --> 00:34:07.960]   If you've done principal components analysis,
[00:34:07.960 --> 00:34:10.560]   linear algebra based technique for dimensionality reduction,
[00:34:10.560 --> 00:34:12.640]   pre-processing and analyzing data,
[00:34:12.640 --> 00:34:14.760]   that's based on the singular value decomposition.
[00:34:14.760 --> 00:34:17.280]   In a pure math setting, it's used to classify
[00:34:17.280 --> 00:34:19.200]   and measure matrices in linear algebra
[00:34:19.200 --> 00:34:22.480]   to determine the norm and size of matrices,
[00:34:22.480 --> 00:34:24.520]   to measure the inner product of matrices
[00:34:24.520 --> 00:34:26.960]   is all based on the singular value decomposition.
[00:34:26.960 --> 00:34:28.920]   But we're gonna talk about how it's used
[00:34:28.920 --> 00:34:30.680]   in low rank approximations,
[00:34:30.680 --> 00:34:33.440]   since this comes up in numerical linear algebra
[00:34:33.440 --> 00:34:35.040]   and in machine learning.
[00:34:35.040 --> 00:34:38.160]   When the rank is not close to their total dimension,
[00:34:38.160 --> 00:34:40.440]   they are said to have low rank.
[00:34:40.440 --> 00:34:42.840]   What that looks like if you actually look at a matrix
[00:34:42.840 --> 00:34:45.200]   with low rank, often, but not always,
[00:34:45.200 --> 00:34:48.560]   that shows up as a very obvious visual pattern
[00:34:48.560 --> 00:34:51.680]   that the matrix has some simple pattern to it.
[00:34:51.680 --> 00:34:52.880]   So just as an example,
[00:34:52.880 --> 00:34:55.800]   taking this video here from a security camera
[00:34:55.800 --> 00:34:57.800]   and turning it into a matrix
[00:34:57.800 --> 00:35:01.400]   where each column here represents a single frame
[00:35:01.400 --> 00:35:02.680]   of the security video,
[00:35:02.680 --> 00:35:06.160]   then you can see that it's basically constant all the time.
[00:35:06.160 --> 00:35:08.440]   There's these little deviations from that pattern
[00:35:08.440 --> 00:35:09.640]   inside the matrix,
[00:35:09.640 --> 00:35:11.240]   but basically there's a simple pattern
[00:35:11.240 --> 00:35:13.640]   of just repeat the same column over and over again,
[00:35:13.640 --> 00:35:17.080]   representing effectively the fixed background of the scene.
[00:35:17.080 --> 00:35:18.320]   And I just wanna compare that
[00:35:18.320 --> 00:35:20.760]   to what a full rank matrix would look like.
[00:35:20.760 --> 00:35:21.680]   So as an example,
[00:35:21.680 --> 00:35:23.680]   here's what a full rank matrix
[00:35:23.680 --> 00:35:25.280]   of the same size would look like.
[00:35:25.280 --> 00:35:27.280]   It's sort of more like what you would get by looking
[00:35:27.280 --> 00:35:30.080]   at the static coming out of a TV
[00:35:30.080 --> 00:35:32.960]   from an old school TV tuned to between channels.
[00:35:32.960 --> 00:35:36.120]   Again, we're taking each frame here and making it a column,
[00:35:36.120 --> 00:35:39.760]   and then we're looking over time for this axis here.
[00:35:39.760 --> 00:35:42.520]   Each column here is one frame of this video.
[00:35:42.520 --> 00:35:44.960]   So the same as in this previous example
[00:35:44.960 --> 00:35:46.160]   with the security camera footage,
[00:35:46.160 --> 00:35:47.360]   but now since it's static,
[00:35:47.360 --> 00:35:48.760]   there isn't this simple pattern.
[00:35:48.760 --> 00:35:49.600]   There isn't this simple,
[00:35:49.600 --> 00:35:51.680]   oh, the background stays the same all the time.
[00:35:51.680 --> 00:35:54.320]   A full rank matrix doesn't have those kinds
[00:35:54.320 --> 00:35:55.840]   of simple patterns in it
[00:35:55.840 --> 00:35:57.800]   and can't be broken down in the same way.
[00:35:57.800 --> 00:36:01.480]   So lots of matrices that we come across in real life
[00:36:01.480 --> 00:36:02.680]   when we measure data,
[00:36:02.680 --> 00:36:05.040]   like pointing the security camera somewhere
[00:36:05.040 --> 00:36:06.720]   and measuring its inputs,
[00:36:06.720 --> 00:36:09.120]   we end up with something that is nearly low rank.
[00:36:09.120 --> 00:36:10.920]   It's not exactly low rank.
[00:36:10.920 --> 00:36:14.040]   It can't be exactly represented by a simple pattern,
[00:36:14.040 --> 00:36:15.440]   but it's nearly low rank.
[00:36:15.440 --> 00:36:17.560]   So the simplest low rank pattern in the video
[00:36:17.560 --> 00:36:19.160]   is just this background.
[00:36:19.160 --> 00:36:21.680]   So if I just take this vector representing
[00:36:21.680 --> 00:36:25.560]   a single frame here and then repeat it over and over again,
[00:36:25.560 --> 00:36:27.400]   then I'll get this matrix over here,
[00:36:27.400 --> 00:36:30.360]   which is very close to the original matrix.
[00:36:30.360 --> 00:36:31.520]   So one thing to note here,
[00:36:31.520 --> 00:36:34.320]   this matrix multiplication here is kind of like a repeat
[00:36:34.320 --> 00:36:35.840]   or a tile function, right?
[00:36:35.840 --> 00:36:38.080]   If you were to implement a function that did this,
[00:36:38.080 --> 00:36:40.120]   you might call it repeat or tile.
[00:36:40.120 --> 00:36:42.680]   So that's what this all ones vector is doing here.
[00:36:42.680 --> 00:36:44.800]   So another example of how we can think of matrices
[00:36:44.800 --> 00:36:47.720]   more like we think of functions in computer programs.
[00:36:47.720 --> 00:36:49.720]   And this is effectively an approximation
[00:36:49.720 --> 00:36:51.040]   to that original input.
[00:36:51.040 --> 00:36:55.440]   This guy here is very close to this guy on the right here.
[00:36:55.440 --> 00:36:57.640]   And this guy, it is rank one.
[00:36:57.640 --> 00:36:59.600]   One way to see that it is low rank
[00:36:59.600 --> 00:37:00.880]   is that it can be represented
[00:37:00.880 --> 00:37:03.200]   by this really tiny thing here.
[00:37:03.200 --> 00:37:04.960]   So the reason why this is useful
[00:37:04.960 --> 00:37:06.520]   is because we can use it to compress data.
[00:37:06.520 --> 00:37:08.440]   So we can take something that looks like this
[00:37:08.440 --> 00:37:11.480]   and turn it into just this thing here
[00:37:11.480 --> 00:37:12.880]   on the right hand side.
[00:37:12.880 --> 00:37:15.360]   So that would really compress our video down quite a bit.
[00:37:15.360 --> 00:37:17.480]   Of course, it would also remove all the things
[00:37:17.480 --> 00:37:18.760]   that are moving in the video,
[00:37:18.760 --> 00:37:20.320]   which is maybe too much compression.
[00:37:20.320 --> 00:37:22.680]   But this general principle of low rank approximation
[00:37:22.680 --> 00:37:24.880]   is what JPEG is based off of.
[00:37:24.880 --> 00:37:27.920]   So JPEG basically does low rank approximation
[00:37:27.920 --> 00:37:30.560]   to pictures using the Fourier transform.
[00:37:30.560 --> 00:37:32.800]   But I think what's more useful in this case,
[00:37:32.800 --> 00:37:34.880]   and this idea comes from Fast.ai's
[00:37:34.880 --> 00:37:36.440]   numerical linear algebra course,
[00:37:36.440 --> 00:37:38.240]   which is a great resource,
[00:37:38.240 --> 00:37:40.760]   is to actually do foreground background separation.
[00:37:40.760 --> 00:37:44.280]   So if we take this original video frame
[00:37:44.280 --> 00:37:46.440]   and that rank one approximation
[00:37:46.440 --> 00:37:48.320]   for that frame looks like this,
[00:37:48.320 --> 00:37:49.400]   then if I subtract that off,
[00:37:49.400 --> 00:37:51.480]   then I get only that foreground.
[00:37:51.480 --> 00:37:53.920]   That small bit there that was deviating
[00:37:53.920 --> 00:37:56.240]   from this background pattern is of interest.
[00:37:56.240 --> 00:37:58.200]   So even when that approximation,
[00:37:58.200 --> 00:38:02.160]   that compression is not a good compression for our purposes,
[00:38:02.160 --> 00:38:03.600]   it can still be useful.
[00:38:03.600 --> 00:38:06.960]   And so these low rank approximations show up in both ways
[00:38:06.960 --> 00:38:09.080]   when we're doing data science and machine learning.
[00:38:09.080 --> 00:38:10.960]   Sometimes it's done to do compression.
[00:38:10.960 --> 00:38:12.520]   Sometimes it's done to pull out
[00:38:12.520 --> 00:38:14.880]   the interesting uncompressible pieces.
[00:38:14.880 --> 00:38:17.440]   So that's our three takeaways for today,
[00:38:17.440 --> 00:38:19.040]   that linear algebra is important.
[00:38:19.040 --> 00:38:20.800]   This idea of matrix multiplication,
[00:38:20.800 --> 00:38:23.200]   despite its relative simplicity,
[00:38:23.200 --> 00:38:24.920]   is actually very powerful,
[00:38:24.920 --> 00:38:26.640]   shows up in lots of different places.
[00:38:26.640 --> 00:38:29.040]   Linear algebra, despite the way it's normally taught,
[00:38:29.040 --> 00:38:31.720]   is not actually that much like algebra.
[00:38:31.720 --> 00:38:34.200]   It's more like computer programming in a lot of ways,
[00:38:34.200 --> 00:38:35.920]   where shapes are our types,
[00:38:35.920 --> 00:38:37.640]   matrices are our functions,
[00:38:37.640 --> 00:38:40.520]   and matrix multiplication is function composition.
[00:38:40.520 --> 00:38:43.480]   And then finally, the singular value decomposition
[00:38:43.480 --> 00:38:46.120]   is our matrix refactoring.
[00:38:46.120 --> 00:38:48.240]   When we want to refactor programs,
[00:38:48.240 --> 00:38:50.880]   common operation, something we need to do all the time
[00:38:50.880 --> 00:38:52.200]   when we're doing computer programming,
[00:38:52.200 --> 00:38:53.840]   refactoring shows up in the form
[00:38:53.840 --> 00:38:56.680]   of this singular value decomposition,
[00:38:56.680 --> 00:38:59.280]   among other ways that we might refactor matrices.
[00:38:59.280 --> 00:39:02.080]   So, more resources for understanding linear algebra.
[00:39:02.080 --> 00:39:04.760]   Effectively, we aren't really gonna be able to cover
[00:39:04.760 --> 00:39:06.800]   all the mathematics you need for machine learning
[00:39:06.800 --> 00:39:08.040]   in just three sessions.
[00:39:08.040 --> 00:39:10.680]   And so, I wanted to give you some pointers
[00:39:10.680 --> 00:39:11.880]   to more resources.
[00:39:11.880 --> 00:39:14.640]   And what is most useful to do next
[00:39:14.640 --> 00:39:17.720]   is very dependent on what your past history is
[00:39:17.720 --> 00:39:20.240]   with this branch of math and what your future plans are.
[00:39:20.240 --> 00:39:22.640]   So, if you had a traumatic linear algebra class
[00:39:22.640 --> 00:39:24.560]   back in the day, maybe in college,
[00:39:24.560 --> 00:39:28.840]   and it was just maybe not your favorite class in college,
[00:39:28.840 --> 00:39:31.400]   and you felt like you didn't learn that much,
[00:39:31.400 --> 00:39:33.280]   then you should try Essence of Linear Algebra
[00:39:33.280 --> 00:39:35.480]   by 3Blue1Brown on YouTube.
[00:39:35.480 --> 00:39:38.080]   It's bingeable, you could fit it in an afternoon
[00:39:38.080 --> 00:39:39.480]   if you really wanted to,
[00:39:39.480 --> 00:39:41.760]   with these really nice, slick animations.
[00:39:41.760 --> 00:39:44.680]   A lot of people say it's like a sort of religious experience
[00:39:44.680 --> 00:39:47.440]   of, oh, I finally see the light of linear algebra,
[00:39:47.440 --> 00:39:50.000]   like why it was so confusing for so long.
[00:39:50.000 --> 00:39:51.560]   So, highly recommended.
[00:39:51.560 --> 00:39:54.000]   If you want to level up your abstract math chops,
[00:39:54.000 --> 00:39:55.800]   if you really want to get a better handle
[00:39:55.800 --> 00:39:58.280]   on how to think about mathematics,
[00:39:58.280 --> 00:39:59.680]   read and understand mathematics,
[00:39:59.680 --> 00:40:02.120]   maybe for being able to read machine learning papers
[00:40:02.120 --> 00:40:02.960]   a little bit better,
[00:40:02.960 --> 00:40:05.240]   then that Graphical Linear Algebra blog post series
[00:40:05.240 --> 00:40:07.600]   by Pavel Sobichinsky is a great place to start.
[00:40:07.600 --> 00:40:10.000]   It's aimed at people who have not taken
[00:40:10.000 --> 00:40:11.960]   a proof-based math class before,
[00:40:11.960 --> 00:40:13.400]   and to teach them how to do that,
[00:40:13.400 --> 00:40:15.520]   while also teaching them some cutting-edge mathematics,
[00:40:15.520 --> 00:40:16.720]   which is a great combination.
[00:40:16.720 --> 00:40:17.880]   It's well-written.
[00:40:17.880 --> 00:40:20.120]   If you're more of the sort of hacker type
[00:40:20.120 --> 00:40:22.600]   who really wants to get your hands dirty
[00:40:22.600 --> 00:40:23.960]   with some actual code,
[00:40:23.960 --> 00:40:27.480]   then I would try Numerical Linear Algebra by Fast.ai
[00:40:27.480 --> 00:40:28.640]   with Rachel Thomas.
[00:40:28.640 --> 00:40:30.480]   So, it's an online class and textbook
[00:40:30.480 --> 00:40:33.640]   focusing on applications of linear algebra
[00:40:33.640 --> 00:40:34.680]   for machine learning.
[00:40:34.680 --> 00:40:36.560]   And if you're the type who prefers
[00:40:36.560 --> 00:40:38.440]   the traditional math class experience,
[00:40:38.440 --> 00:40:40.560]   then I would check out Linear Algebra Done Right
[00:40:40.560 --> 00:40:41.600]   by Sheldon Axler.
[00:40:41.600 --> 00:40:43.880]   The textbook has been free in the past.
[00:40:43.880 --> 00:40:45.440]   I'm not sure if it's currently free,
[00:40:45.440 --> 00:40:49.400]   but the lecture series on YouTube is certainly free.
[00:40:49.400 --> 00:40:51.480]   So, this Linear Algebra Done Right lecture series
[00:40:51.480 --> 00:40:54.640]   by Sheldon Axler goes through a very classic
[00:40:54.640 --> 00:40:56.760]   traditional math class approach to linear algebra.
[00:40:56.760 --> 00:40:59.080]   So, if you wanted to do that,
[00:40:59.080 --> 00:41:01.360]   that would be the place to do it, I would say.
[00:41:02.360 --> 00:41:04.960]   (upbeat music)
[00:41:05.080 --> 00:41:06.440]   Thanks for watching my video.
[00:41:06.440 --> 00:41:08.280]   If you enjoyed it, give it a like.
[00:41:08.280 --> 00:41:10.440]   If you want more Weights & Biases tutorial
[00:41:10.440 --> 00:41:12.920]   and demo content, subscribe to our channel.
[00:41:12.920 --> 00:41:14.720]   And if you've got any questions, comments,
[00:41:14.720 --> 00:41:17.400]   ideas for future videos, leave a comment below.
[00:41:17.400 --> 00:41:18.760]   We'd love to hear from you.
[00:41:18.760 --> 00:41:21.340]   (upbeat music)
[00:41:21.340 --> 00:41:22.760]   (light music)

