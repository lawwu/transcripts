
[00:00:00.000 --> 00:00:05.200]   If we had AI systems that could do everything humans do to advance science and technology,
[00:00:05.200 --> 00:00:11.200]   that would be insane. We live in a weird time. Growth has been exploding, accelerating over the
[00:00:11.200 --> 00:00:16.720]   last blink of an eye. We really need to be kind of like nervous and vigilant about what comes next
[00:00:16.720 --> 00:00:20.640]   and thinking about all the things that could radically transform the world. You just imagine
[00:00:20.640 --> 00:00:24.960]   a universe where there actually are some people who live in especially important time, and then
[00:00:24.960 --> 00:00:28.320]   there's a bunch of other people who like tell stories to themselves about how, what, you know,
[00:00:28.320 --> 00:00:32.640]   whether they do. How would you want all those people to behave? And it's like, to me, the worst
[00:00:32.640 --> 00:00:36.480]   possible rule is all those people should just be like, "Nah, this is crazy," and forget about it.
[00:00:36.480 --> 00:00:40.640]   All right. Today, I have the pleasure of speaking with Holden Kornofsky,
[00:00:40.640 --> 00:00:46.480]   who is the co-CEO of Open Philanthropy. In my opinion, Holden is one of the most interesting
[00:00:46.480 --> 00:00:51.200]   intellectuals alive, well, given your role. So Holden, welcome to the Lunar Society.
[00:00:51.200 --> 00:00:52.320]   Thanks for having me.
[00:00:52.320 --> 00:00:56.560]   Okay. So let's start off by talking about the most important century thesis. Do you
[00:00:56.560 --> 00:00:58.320]   want to explain what this is for the audience?
[00:00:58.320 --> 00:01:04.800]   You know, my story is I originally co-founded an organization called GiveWell that helps people
[00:01:04.800 --> 00:01:09.920]   decide where to give as effectively as possible. I'm no longer there, but I'm on the board. And
[00:01:09.920 --> 00:01:13.600]   it's a website called givewell.org that I think makes good recommendations where to give to
[00:01:13.600 --> 00:01:19.680]   charity to help a lot of people. And, you know, as we were working at GiveWell, we met Kerry
[00:01:19.680 --> 00:01:24.960]   Tuna and Dustin Moskovitz. Dustin is the co-founder of Facebook and Asana, and started a project that
[00:01:24.960 --> 00:01:30.000]   became Open Philanthropy to try to help them give away their large fortune, again, to help as many
[00:01:30.000 --> 00:01:34.400]   people as possible. And so I've kind of spent my career looking for ways to do as much good
[00:01:34.400 --> 00:01:39.280]   as possible with a dollar or with an hour, with whatever resources you have, and especially with
[00:01:39.280 --> 00:01:44.880]   money. And so I've kind of developed this professional specialization in looking for
[00:01:44.880 --> 00:01:49.840]   ideas that are underappreciated, underrated, tremendously important, because a lot of the
[00:01:49.840 --> 00:01:54.960]   time that's where I think you can find just kind of outsized, what you might call outsized return
[00:01:54.960 --> 00:01:59.680]   on investment, opportunities to spend some money and just get an enormous impact because you're
[00:01:59.680 --> 00:02:03.760]   doing something very important that is being ignored by others. And so it's through that
[00:02:03.760 --> 00:02:08.880]   kind of professional specialization that I've actively looked for interesting ideas that are
[00:02:08.880 --> 00:02:13.120]   not getting enough attention. And then I encountered the Effective Altruist Community, which is a
[00:02:13.120 --> 00:02:18.400]   community of people basically built around the idea of doing as much good as you can. And so it's
[00:02:18.400 --> 00:02:22.160]   through that community that I encountered the idea of the most important century. It's not
[00:02:22.160 --> 00:02:29.040]   my idea at all. I got to it from a lot of people. And the basic idea is that if we developed the
[00:02:29.040 --> 00:02:34.640]   right kind of AI systems this century, and that looks reasonably likely, that could make this
[00:02:34.640 --> 00:02:40.560]   century the most important of all time for humanity. So the basic mechanics of why that
[00:02:40.560 --> 00:02:46.400]   might be or how you might think about that. So one thing is that if you look back at all of
[00:02:46.400 --> 00:02:51.040]   economic history, just the rate at which the world economy has grown, you see acceleration,
[00:02:51.040 --> 00:02:57.280]   you see that it's growing a lot faster today than it ever was. And one theory of why that might be,
[00:02:57.280 --> 00:03:01.120]   or one way of thinking about it through the lens of basic economic growth theory,
[00:03:01.120 --> 00:03:08.400]   is that in normal circumstances, you can imagine a kind of feedback loop where people have ideas,
[00:03:08.400 --> 00:03:12.000]   and the ideas lead to greater productivity and more resources. And then when you have more
[00:03:12.000 --> 00:03:15.600]   resources, you can also have more people, and then those people have more ideas. So you get
[00:03:15.600 --> 00:03:19.600]   this feedback loop that goes people, ideas, resources, people, ideas, resources.
[00:03:19.600 --> 00:03:24.480]   And starting a couple hundred years ago, if you run a feedback loop like that,
[00:03:24.480 --> 00:03:28.320]   standard economic theory says you'll get accelerating growth. You'll get a rate of
[00:03:28.320 --> 00:03:33.680]   economic growth that goes faster and faster. And basically, if you take the story of our
[00:03:33.680 --> 00:03:37.600]   economy to date, and you just kind of plot it on a chart and do the kind of simplest thing you can
[00:03:37.600 --> 00:03:42.400]   to project it forward, you project that it will go, that our economy will reach like an infinite
[00:03:42.400 --> 00:03:47.680]   growth rate this century. And the reason that I currently don't think that's a great thing to
[00:03:47.680 --> 00:03:53.200]   expect by default is that one of the steps of that feedback loop broke a couple of hundred years ago.
[00:03:53.200 --> 00:03:57.920]   So it goes more people, more ideas, more resources, more people, more ideas, more resources.
[00:03:57.920 --> 00:04:02.880]   A couple hundred years ago, people stopped having more children when they had more resources. They
[00:04:02.880 --> 00:04:07.600]   got just more, they got richer instead of more populous. And this is all discussed in the Most
[00:04:07.600 --> 00:04:12.880]   Important Century page on my blog, Cold Takes. And so what happens right now is that when we
[00:04:12.880 --> 00:04:16.480]   have more ideas and we have more resources, we don't end up with more people as a result. We
[00:04:16.480 --> 00:04:22.640]   don't have that same accelerating feedback loop. And if you had AI systems that could do all the
[00:04:22.640 --> 00:04:27.680]   things humans do to advance science and technology, meaning the AI systems could fill in that more
[00:04:27.680 --> 00:04:32.400]   ideas part of the loop, then you could get that feedback loop back. And then you could get sort
[00:04:32.400 --> 00:04:38.720]   of this unbounded, heavily accelerating, explosive growth in science and technology. So that's the
[00:04:38.720 --> 00:04:43.680]   basic dynamic at the heart of it. So that's kind of a way of putting it that's trying to use familiar
[00:04:43.680 --> 00:04:49.200]   concepts from economic growth theory. Another way of putting it might just be, gosh, if we had AI
[00:04:49.200 --> 00:04:54.080]   systems that could do everything humans do to advance science and technology, that would be
[00:04:54.080 --> 00:04:59.040]   insane. What if we were to take the things that humans do to create new technologies that have
[00:04:59.040 --> 00:05:03.920]   transformed the planet so radically, and we were able to completely automate them so that every
[00:05:03.920 --> 00:05:08.000]   computer we have is potentially another mind working on advancing technology. So either way
[00:05:08.000 --> 00:05:12.080]   you think about it, you could imagine the world changing incredibly quickly and incredibly
[00:05:12.080 --> 00:05:18.640]   dramatically. And so I argue in the Most Important Century series that it looks reasonably likely,
[00:05:18.640 --> 00:05:25.200]   in my opinion, more than 50/50, that this century will see AI systems that can do all of the key
[00:05:25.200 --> 00:05:30.240]   tasks that humans do to advance science and technology. And that if that happens, we'll see
[00:05:30.240 --> 00:05:34.800]   explosive progress in science and technology. The world will quickly become extremely different from
[00:05:34.800 --> 00:05:39.920]   how it is today. You might think of it as if there was thousands of years of changes packed into a
[00:05:39.920 --> 00:05:45.760]   much shorter time period. And then if that happens, I argue that you could end up in a deeply unfamiliar
[00:05:45.760 --> 00:05:50.800]   future. I give one example of what that might look like using this hypothetical technology idea
[00:05:50.800 --> 00:05:55.120]   called digital people. That would be sort of people that live in virtual environments that
[00:05:55.120 --> 00:06:01.360]   are kind of simulated, but also realistic and exactly like us. And when you picture that kind
[00:06:01.360 --> 00:06:07.440]   of advanced world, I think there is a decent reason to think that if we did get that rate of scientific
[00:06:07.440 --> 00:06:11.760]   and technological advancement, we could basically hit the limits of science and technology. We could
[00:06:11.760 --> 00:06:16.800]   basically find most of what there is to find and end up with a civilization that expands well beyond
[00:06:16.800 --> 00:06:21.120]   this planet, has a lot of control over the environment, and is very stable for very long
[00:06:21.120 --> 00:06:26.160]   periods of time, and basically looks sort of post-human in a lot of relevant ways. And if you
[00:06:26.160 --> 00:06:30.640]   think that, then this is basically our last chance to shape how this happens. So that's the most
[00:06:30.640 --> 00:06:35.760]   important century hypothesis in a nutshell is that if we develop AI that can do all the things humans
[00:06:35.760 --> 00:06:40.640]   do to advance science and technology, we could very quickly reach a very futuristic world,
[00:06:40.640 --> 00:06:44.400]   very different from today's, could be a very stable, very large world. This is our last
[00:06:44.400 --> 00:06:49.680]   chance to shape it. Gotcha. Okay, so I and many other people are going to find that very wild.
[00:06:49.680 --> 00:06:57.360]   So maybe you can walk us through the process by which you went from doing global development stuff
[00:06:57.360 --> 00:07:04.240]   to thinking this way. So in 2014, for example, you had an interview or a conversation,
[00:07:04.240 --> 00:07:10.080]   and this is a quote from there. Maybe you can walk me through how you got from there to where
[00:07:10.080 --> 00:07:15.040]   you are today. "I have looked at the situation in Africa, have understanding of the situation in
[00:07:15.040 --> 00:07:20.080]   Africa, and see a path of doing a lot of good in Africa. I don't know how to look into the far
[00:07:20.080 --> 00:07:25.760]   future situation, don't understand the far future situation, and don't see a path to doing good on
[00:07:25.760 --> 00:07:30.160]   that front I feel good about." Yeah. First, I think I just, I went on for a while, but I want
[00:07:30.160 --> 00:07:34.880]   to come back and connect this back up to how this relates to the work I was even doing at GiveWell,
[00:07:34.880 --> 00:07:40.720]   why this is all kind of one theme. If we are kind of on the cusp for this century of creating these
[00:07:40.720 --> 00:07:46.320]   advanced AI systems, then we could be looking at a future that's very good or very bad. And I think
[00:07:46.320 --> 00:07:51.360]   there are decent arguments that if we move forward without caution, and we develop kind of sloppily
[00:07:51.360 --> 00:07:56.320]   designed AI systems, they can end up with goals of their own. And basically, we'd end up with a
[00:07:56.320 --> 00:08:00.640]   universe that contains very little that humans value, or a galaxy that does. We could also
[00:08:00.640 --> 00:08:06.800]   imaginably end up with a world in which very powerful technologies are used by just not very
[00:08:06.800 --> 00:08:11.120]   well-meaning governments to create a world that isn't very good, or a world where we kind of
[00:08:11.120 --> 00:08:15.680]   eliminate a lot of forms of material scarcity and have a world that's much better than today's.
[00:08:15.680 --> 00:08:20.240]   And so, a lot of what I ask, a lot of what I ask at GiveWell is how can we help the most people
[00:08:20.240 --> 00:08:24.960]   possible per dollar spent? And if you ask how can we help the most people possible per dollar spent,
[00:08:24.960 --> 00:08:31.120]   then if you think that funding some work to help shape that transition, to help make sure that we
[00:08:31.120 --> 00:08:36.160]   don't move forward too incautiously, to help make sure that we do increase the odds that we do get
[00:08:36.160 --> 00:08:40.800]   like a good future world instead of a bad future one, that's helping a huge number of people per
[00:08:40.800 --> 00:08:45.680]   dollar spent. So, that's the motivation. And now you're quoting me a discussion, an argument I was
[00:08:45.680 --> 00:08:51.520]   having where we posted a transcript back in 2014. And at that time, that was part of my journey of
[00:08:51.520 --> 00:08:55.440]   getting here. So, I was talking to people who were saying, "Holden, you want to help a lot of
[00:08:55.440 --> 00:09:00.560]   people with your resources. You should be focused on this massive event that could be coming this
[00:09:00.560 --> 00:09:05.440]   century that very few people are paying attention to, that there might be a chance to make this go
[00:09:05.440 --> 00:09:10.240]   well or poorly for humanity." And I was saying, "Gosh, like that sure is interesting." And I did
[00:09:10.240 --> 00:09:13.520]   think it was interesting. That's why I was spending the time and doing the conversation.
[00:09:13.520 --> 00:09:19.280]   But I said, "You know, when I look at global poverty and global health, I see what I can do.
[00:09:19.280 --> 00:09:22.800]   I see the evidence. I see the actions I can take, and I'm not seeing that with this stuff."
[00:09:22.800 --> 00:09:30.000]   So, what changed? I would say a good chunk of what changed is maybe like the most boring answer
[00:09:30.000 --> 00:09:34.560]   possible is I just kept at it. So, I think I was sitting there in 2014 saying, "Gosh, this is
[00:09:34.560 --> 00:09:41.200]   really interesting, but it's all a bit overwhelming. It's all a bit crazy. I don't know how I would
[00:09:41.200 --> 00:09:46.000]   even think about this. I don't know how I would come up with a risk from AI that I actually
[00:09:46.000 --> 00:09:50.320]   believed was a risk and could do something about today." And now I've just been thinking about this
[00:09:50.320 --> 00:09:55.680]   for a much longer time period. And I do believe most things you could say about the far future
[00:09:55.680 --> 00:10:00.720]   are very unreliable and not worth taking action on. But I think there are a few things one might
[00:10:00.720 --> 00:10:06.480]   say about what a transition to very powerful AI systems could look like. There are some things I'm
[00:10:06.480 --> 00:10:11.520]   willing to say. I'm willing to say it would be bad if AI systems were poorly designed, had goals of
[00:10:11.520 --> 00:10:17.120]   their own, and ended up running the world instead of humans. That seems bad, and I do, and I am more
[00:10:17.120 --> 00:10:21.600]   familiar today than I was then with the research and the work people can do to make that less
[00:10:21.600 --> 00:10:26.640]   likely and the actions people can take to make that less likely. So, that's probably more than
[00:10:26.640 --> 00:10:30.320]   half the answer. But another thing that would be close to half the answer is I think the world has
[00:10:30.320 --> 00:10:37.440]   changed. And I think that basically there's been big changes in the world of AI since then. So,
[00:10:37.440 --> 00:10:41.440]   I think basically in 2014, that was the beginning of what's sometimes called the deep learning
[00:10:41.440 --> 00:10:48.160]   revolution. And since then, we've basically seen these very computation-intensive, but fundamentally
[00:10:48.160 --> 00:10:54.960]   simple AI systems and achieve a lot of progress on a lot of different unrelated tasks. And it's
[00:10:54.960 --> 00:11:01.200]   looking to me like not totally crazy to imagine that the current way people are developing AI
[00:11:01.200 --> 00:11:06.400]   systems, cutting-edge AI systems, could take us all the way to the kind of extremely powerful AI
[00:11:06.400 --> 00:11:11.520]   systems that automate roughly everything humans do to advance science and technology. It's not so
[00:11:11.520 --> 00:11:16.000]   wild to imagine that we could just keep on going with these systems, make them bigger, put more
[00:11:16.000 --> 00:11:20.720]   work into them, but basically stay on the same path and you could get there. And if you imagine
[00:11:20.720 --> 00:11:25.200]   doing that, it becomes a little bit less daunting to imagine the risks that might come up and the
[00:11:25.200 --> 00:11:29.760]   things we could do about them. So, I don't think it's necessarily the leading possibility, but it's
[00:11:29.760 --> 00:11:34.400]   enough to sort of start thinking concretely about the problem. Another quote from the interview that
[00:11:34.400 --> 00:11:38.720]   today I find appealing, because I haven't done the work you have yet, "Does even the upper crust
[00:11:38.720 --> 00:11:44.080]   of humanity have a track record of being able to figure out the kinds of things Miri claims to have
[00:11:44.080 --> 00:11:48.000]   figured out?" And by the way, for context for the viewers, Miri is the organization Eleazar was
[00:11:48.000 --> 00:11:53.600]   leading, which is who we were talking to at the time. Yeah, well, I don't remember exactly what
[00:11:53.600 --> 00:11:58.320]   kinds of things Miri was trying to figure out, and I'm not sure that I even understood what they
[00:11:58.320 --> 00:12:04.320]   were that well. So, I definitely think it is true that it is hard to predict the future in a
[00:12:04.320 --> 00:12:08.080]   matter of who you are, no matter how hard you think, no matter how much you've studied. I think
[00:12:08.080 --> 00:12:14.720]   that is true. I think parts of our kind of world or memeplex or whatever you want to call it,
[00:12:14.720 --> 00:12:19.760]   overblow this at least a little bit. And I think I was kind of buying into that a little bit more
[00:12:19.760 --> 00:12:25.920]   than I could. So, I think probably in 2014, I would have said something like, "Gosh, really,
[00:12:25.920 --> 00:12:30.800]   no one's ever done something like making smart statements about what the several decades out
[00:12:30.800 --> 00:12:34.960]   future could look like, or making smart statements about what we would be doing today to prepare for
[00:12:34.960 --> 00:12:40.400]   it." Since then, I think a bunch of people have looked into this and looked for historical
[00:12:40.400 --> 00:12:45.520]   examples of people making long-term predictions and long-term interventions. And I don't think
[00:12:45.520 --> 00:12:51.520]   it's amazing, but I wrote a recent blog post entitled, "The Track Record of Futurists Seems
[00:12:51.520 --> 00:12:58.720]   Fine." "Fine" is how I'd put it, where I don't think there's anyone who has demonstrated a real
[00:12:58.720 --> 00:13:03.600]   ability to predict the future with precision and know exactly what we should do. But I also don't
[00:13:03.600 --> 00:13:09.040]   think humans' track record of this is so bad and so devastating that we shouldn't think we are
[00:13:09.040 --> 00:13:15.280]   capable of at least giving it a shot. And I think if you enter into this endeavor with self-awareness
[00:13:15.280 --> 00:13:19.280]   about the fact that everything is less reliable than it appears and feels at first glance,
[00:13:19.280 --> 00:13:23.840]   and you look for the few things that you would really bet on, I think it's worth doing. I think
[00:13:23.840 --> 00:13:28.960]   it's worth the bet. My job is to find things that we might do 10 things and have nine of them fail
[00:13:28.960 --> 00:13:32.480]   embarrassingly, and one of them be such a big hit that it makes up for everything else. And a lot of
[00:13:32.480 --> 00:13:37.840]   my job is to find stuff like that. So I don't think it's totally crazy to think we could make
[00:13:37.840 --> 00:13:42.400]   meaningful statements about how things we do today could make these future events go better,
[00:13:42.400 --> 00:13:45.520]   especially if the future events aren't crazily far away, especially if they're within the next
[00:13:45.520 --> 00:13:49.600]   few decades. So that is something I've changed my mind on at least to some degree.
[00:13:49.600 --> 00:13:52.320]   Gotcha. Okay. So we'll get to the forecasting stuff in a second,
[00:13:52.320 --> 00:13:56.160]   but let's continue on the object level conversation about the most important century. So
[00:13:56.160 --> 00:14:02.480]   I want to make sure I have the thesis right. So is the argument that because we're living in a weird
[00:14:02.480 --> 00:14:06.320]   time, we shouldn't be surprised if something like transformative AI happens this century,
[00:14:06.320 --> 00:14:10.960]   or is the argument that since transformative AI could happen this century, it's a weird time?
[00:14:10.960 --> 00:14:14.720]   Yeah. So something we haven't covered yet, but I think is worth throwing in now is that a
[00:14:14.720 --> 00:14:20.000]   significant part of the most important century series is kind of just making the case that even
[00:14:20.000 --> 00:14:25.040]   if you ignore AI, there's a lot of things that are very strange about the time that our generation
[00:14:25.040 --> 00:14:30.400]   lives in. The reason I spent so much effort on this is because my number one, back in 2014 and
[00:14:30.400 --> 00:14:37.200]   before that, my number one objection to these stories about transformative AI is it's not
[00:14:37.200 --> 00:14:43.440]   anything about whether the specific claims about AI or economic models or alignment research makes
[00:14:43.440 --> 00:14:48.160]   sense. It's just this whole thing sounds crazy. And it's just suspicious. It's just suspicious
[00:14:48.160 --> 00:14:51.360]   if someone says to you, you know, this could be the most important century of all time for
[00:14:51.360 --> 00:14:56.960]   humanity. I titled the series that way because I wanted people to know that I was saying something
[00:14:56.960 --> 00:15:01.200]   kind of crazy and that I should have to defend it. I didn't want to be backpedaling or soft
[00:15:01.200 --> 00:15:07.440]   pedaling or hiding what a big claim I was making. And so I think my biggest source of skepticism
[00:15:07.440 --> 00:15:12.080]   has been just like, I don't have any specific objection. It just sounds kind of crazy and
[00:15:12.080 --> 00:15:16.640]   suspicious to say that we might live in one of the most significant times or the most significant
[00:15:16.640 --> 00:15:22.240]   time for humanity ever. And so a lot of my series is just kind of saying it is weird. It is weird to
[00:15:22.240 --> 00:15:27.600]   think that, but we already have a lot of evidence that we live in an extraordinarily weird time that
[00:15:27.600 --> 00:15:32.400]   would be on the shortlist of contenders for most important time ever before you get into anything
[00:15:32.400 --> 00:15:37.680]   about AI, just using like completely commonly accepted facts about the world. For example,
[00:15:37.680 --> 00:15:43.040]   if you chart the history of economic growth, you see that the last couple hundred years have seen
[00:15:43.040 --> 00:15:49.040]   faster growth by a lot than like anything else in the history of humanity or the world. And if you
[00:15:49.040 --> 00:15:52.640]   chart anything about like scientific and technological developments, you can see that
[00:15:52.640 --> 00:15:57.360]   everything significant is packed together in the recent past. And there's almost no way to cut it.
[00:15:57.360 --> 00:16:00.080]   You know, I've looked at many different cuts of this. There's almost no way to cut it that won't
[00:16:00.080 --> 00:16:04.800]   give you that conclusion. One way to put it is that the, you know, the universe is something
[00:16:04.800 --> 00:16:09.840]   like 11 or 12 billion years old, humanity, or sorry, life on earth is like 3 billion years old.
[00:16:10.640 --> 00:16:15.120]   And then humanity is just a blink of an eye compared to that. You could call it 300,000,
[00:16:15.120 --> 00:16:20.320]   three, 3 million years old. Human civilization is a blink of eye compared to that. And we're in this
[00:16:20.320 --> 00:16:25.680]   really, really tiny sliver of time, couple hundred years when we've seen basically, you know, all,
[00:16:25.680 --> 00:16:30.080]   or like just a huge amount of the technological advancement, economic growth. So that's weird.
[00:16:30.080 --> 00:16:34.720]   You know, I also talk about the fact that the current rate of economic growth seems high enough
[00:16:34.720 --> 00:16:38.960]   that we can't keep it going for that much longer. If it went for another 10,000 years,
[00:16:38.960 --> 00:16:42.880]   that's another blink of an eye and galactic timescales. It looks to me, and we can get to
[00:16:42.880 --> 00:16:48.160]   this, like we would run out of atoms in the galaxy, wouldn't have anywhere to go. And so I think there
[00:16:48.160 --> 00:16:52.880]   are a lot of signs that we just live in a really strange time. I think one, one more, I'll just
[00:16:52.880 --> 00:16:57.920]   throw in there that we can move on is that, uh, you know, I think a lot of people who disagree
[00:16:57.920 --> 00:17:04.560]   with my take would say, look, I do believe eventually we will develop space colonization
[00:17:04.560 --> 00:17:09.840]   abilities, go to the stars, fill up the galaxy with life, you know, maybe have artificial general
[00:17:09.840 --> 00:17:14.000]   intelligence. I just, to say it's a century is crazy. I think it might be 500 years. I think
[00:17:14.000 --> 00:17:18.400]   it might be a thousand years. I think it might be 5,000 years. And a big point I make in the
[00:17:18.400 --> 00:17:23.520]   series is I say, well, even if it's a hundred thousand years, that's still an extremely crazy
[00:17:23.520 --> 00:17:28.960]   time to be in, in the scheme of things. If you just make, you know, if you make a graphic timeline
[00:17:28.960 --> 00:17:33.680]   and, and you kind of show my view versus yours, they look exactly the same down to the pixel.
[00:17:33.680 --> 00:17:38.720]   And so there's already a lot of reasons to think we live in a very weird time. We're on this planet
[00:17:38.720 --> 00:17:44.240]   where there's no other sign of life anywhere in the galaxy. We believe that we could fill up the
[00:17:44.240 --> 00:17:49.680]   galaxy with life. That alone would make us among the earliest life that has ever existed or would
[00:17:49.680 --> 00:17:53.920]   make us the earliest life that has ever existed in the galaxy, a tiny fraction of it. Um, so that
[00:17:53.920 --> 00:17:57.760]   is a lot of what the series is about. And I have sort of answered your question, but I'll do it
[00:17:57.760 --> 00:18:04.400]   explicitly. You ask, you know, is the series about, uh, transformative AI could come and therefore
[00:18:04.400 --> 00:18:07.680]   this century could be weird, or is it about this century could be weird, therefore transformative
[00:18:07.680 --> 00:18:12.880]   AI could come. The central claim is that transformative AI could be developed in this
[00:18:12.880 --> 00:18:17.920]   century. And the stuff about how weird a time we live in is just a response to an objection. It's
[00:18:17.920 --> 00:18:22.560]   a response to a point of skepticism. It's a way of saying there's already a lot of reasons to think
[00:18:22.560 --> 00:18:28.240]   we live in a very weird time. And so actually this thing about AI is only a moderate quantitative
[00:18:28.240 --> 00:18:31.600]   update, not a complete revolution in the way you're thinking about things. There's a famous
[00:18:31.600 --> 00:18:37.760]   comedian who has a bit where he's imagining what it must've been like to live in 10 BC. Yeah. People
[00:18:37.760 --> 00:18:42.160]   are just numbering it's 10 years before zero. It's nine years before zero. It's going to happen.
[00:18:42.160 --> 00:18:47.200]   Let's say somebody comes with a proof that current deep learning techniques are not scalable for
[00:18:47.200 --> 00:18:51.840]   some reason. And that transformative AI is very unlikely this century. I don't know if this is
[00:18:51.840 --> 00:18:56.000]   the hypothetical where that would happen, but let's just say that's what it is. Even if this is
[00:18:56.000 --> 00:19:00.720]   a weird time in terms of economic growth, does that have any implications other than transformative AI
[00:19:00.720 --> 00:19:07.920]   of what you should be doing? Yeah. I think in general, having learned about just how strange
[00:19:07.920 --> 00:19:12.160]   the time we live in is when you look at it in context, and here I'd just encourage people to
[00:19:12.160 --> 00:19:16.400]   go to my series because I have a bunch of charts illustrating this and it can be a little bit hard
[00:19:16.400 --> 00:19:21.760]   to do concisely. But I think the biggest thing I take away from that is we should really look
[00:19:21.760 --> 00:19:26.720]   for the next big thing. I think if you'd been living 300 years ago and you'd been talking
[00:19:26.720 --> 00:19:31.760]   about the best way to help people, a lot of people might've been talking about various forms of
[00:19:31.760 --> 00:19:38.240]   helping low-income people. Maybe they probably would've been talking about spreading various
[00:19:38.240 --> 00:19:43.280]   religious beliefs. And I think it would've seemed crazy to think that what you should be thinking
[00:19:43.280 --> 00:19:46.960]   about, for example, was the steam engine and how that might change the world. But I think the
[00:19:46.960 --> 00:19:52.240]   Industrial Revolution was actually just an enormous deal and probably was the right thing
[00:19:52.240 --> 00:19:55.040]   to be thinking about if there was any way to be thinking about it, if there was any way to be
[00:19:55.040 --> 00:19:59.440]   thinking about how that would change the world and what one might do to make that a world that
[00:19:59.440 --> 00:20:06.080]   could be better. So that's basically where I'm at on this stuff is I just think as a world,
[00:20:06.080 --> 00:20:11.120]   as a global civilization, we should have a really high priority on just saying, "We live in a weird
[00:20:11.120 --> 00:20:17.200]   time. Growth has been exploding, accelerating over the last blink of an eye. We really need to be
[00:20:17.200 --> 00:20:22.400]   nervous and vigilant about what comes next and thinking about all the things that could radically
[00:20:22.400 --> 00:20:26.080]   transform the world. We should make a list of all the things that might radically transform the
[00:20:26.080 --> 00:20:30.240]   world. We should make sure we've done everything we can to think about them, identify the ways we
[00:20:30.240 --> 00:20:33.600]   might be able to do something today that would actually help. And maybe after we're done doing
[00:20:33.600 --> 00:20:37.120]   all that and we have a lot of the world's brightest mind doing their best to think of stuff and they
[00:20:37.120 --> 00:20:42.000]   can't think of any more, then we can go back to all the other things that we worry about."
[00:20:42.000 --> 00:20:46.240]   But I think right now the world invests so little in that kind of speculative,
[00:20:46.240 --> 00:20:50.480]   "Hey, what's the next big thing?" Even if it's not super productive to do so, even if there's
[00:20:50.480 --> 00:20:55.600]   not that much to learn, I feel the world should be investing more in that than it is because the
[00:20:55.600 --> 00:21:00.560]   stakes are extremely high. And I think it is just like a reasonable guess that we're living in a
[00:21:00.560 --> 00:21:04.720]   world that's recently been incredibly transformed by the industrial revolution and the future could
[00:21:04.720 --> 00:21:08.640]   be incredibly transformed by the next thing. I just don't think this gets a lot of discussion
[00:21:08.640 --> 00:21:12.880]   in basically any circles. And if it got some, I would feel a lot more comfortable with the
[00:21:12.880 --> 00:21:17.120]   whole thing. I don't think the whole world should just obsess over what the next transformative
[00:21:17.120 --> 00:21:19.600]   event is. But I think right now there's so little attention to it.
[00:21:19.600 --> 00:21:26.080]   I'm glad you brought up the industrial revolution because I feel like there's two implicit claims
[00:21:26.080 --> 00:21:31.520]   within the most important century thesis that don't seem perfectly compatible. One is that
[00:21:31.520 --> 00:21:36.880]   we live in an extremely wild time, that the transition here is potentially wilder than any
[00:21:36.880 --> 00:21:41.840]   other transition there has been before. And the second is we have some sense of what we can be
[00:21:41.840 --> 00:21:45.840]   doing to make sure this transition goes well. I'm curious if you think that somebody at the
[00:21:45.840 --> 00:21:50.080]   beginning of the industrial revolution, if knowing what they knew then could have done something
[00:21:50.080 --> 00:21:55.120]   significant to make sure that it went as favorably as possible, or do you think that that's a bad
[00:21:55.120 --> 00:22:00.000]   analogy for some reason? It's a pretty good analogy for being thought-provoking and for
[00:22:00.000 --> 00:22:05.040]   thinking, gosh, if you had seen the industrial revolution coming in advance, and this is when
[00:22:05.040 --> 00:22:11.200]   economic growth really reached a new level back in the 1700s and 1800s, what could you have done?
[00:22:11.200 --> 00:22:16.080]   And I think part of the answer is it's not that clear. And I think that is a bit of an argument
[00:22:16.080 --> 00:22:20.960]   that we should maybe not get too carried away today by thinking that we know exactly what we
[00:22:20.960 --> 00:22:26.400]   can do. But I don't think the answer is quite nothing. So I have kind of a goofy cold-taste
[00:22:26.400 --> 00:22:30.880]   post that I never published and may never publish because, I don't know, I kind of lost track of it.
[00:22:30.880 --> 00:22:36.480]   But it's kind of saying, well, what if you'd been sitting in that time and you had known the
[00:22:36.480 --> 00:22:39.600]   industrial revolution was coming, or you had thought it might be? You had asked yourself
[00:22:39.600 --> 00:22:43.200]   what you could be doing. One answer you might have given is you might have said, well, gosh,
[00:22:43.200 --> 00:22:48.400]   if this happens, whatever country it happens in might be just disproportionately influential.
[00:22:48.400 --> 00:22:53.200]   And you know what would be great is if I could help transform the thinking and the culture in
[00:22:53.200 --> 00:22:57.680]   that country to have a better handle on human rights and more value on human rights and
[00:22:57.680 --> 00:23:02.640]   individual liberties and a lot of other stuff. And gosh, it kind of looks like people were doing
[00:23:02.640 --> 00:23:05.040]   that. And it kind of looks like it worked out. Fair enough.
[00:23:05.040 --> 00:23:10.560]   So this is the enlightenment. And I think I even give this kind of goofy example. I could look it
[00:23:10.560 --> 00:23:14.800]   up. And it's all kind of a trollish post. But the example is it's like someone's kind of thinking
[00:23:14.800 --> 00:23:20.560]   through, hey, I'm thinking about this sort of esoteric question about what a government owes
[00:23:20.560 --> 00:23:26.480]   to its citizens or when a citizen has a right to overthrow a government or when it's acceptable
[00:23:26.480 --> 00:23:31.040]   to kind of enforce certain beliefs and not. And it's like the other person in the dialogue is
[00:23:31.040 --> 00:23:34.560]   just like, this is the weirdest, most esoteric question. Why does this matter? Why aren't you
[00:23:34.560 --> 00:23:38.400]   helping poor people? But these are the questions that the enlightenment thinkers were thinking
[00:23:38.400 --> 00:23:43.200]   about. And I think there is a good case that they came up with a lot of stuff that really
[00:23:43.200 --> 00:23:48.160]   shaped the whole world since then because of the fact that the UK became so influential,
[00:23:48.160 --> 00:23:52.560]   really laid the groundwork for a lot of stuff about the rights of the governed and free speech
[00:23:52.560 --> 00:23:57.440]   and individual rights and human rights. And then I go to the next analogy. And I'm like,
[00:23:57.440 --> 00:24:01.200]   OK, now we're sitting here today. And someone is saying, well, instead of working on global
[00:24:01.200 --> 00:24:05.360]   poverty, I'm studying this kind of esoteric question about how you get an AI system to do
[00:24:05.360 --> 00:24:09.840]   what you wanted it to do instead of doing its own thing. And it's, you know, you could-- it's not--
[00:24:09.840 --> 00:24:13.600]   I think it's not completely crazy to see them as analogous. Now, I don't think this is what
[00:24:13.600 --> 00:24:16.640]   the enlightenment thinkers were actually doing. I don't think they were saying, this could be the
[00:24:16.640 --> 00:24:21.280]   most important millennium, so let's do this. But it is interesting that you-- it doesn't look like
[00:24:21.280 --> 00:24:24.800]   there was nothing to be had there. It doesn't look like there's nothing you could have come up with.
[00:24:24.800 --> 00:24:28.640]   And in many ways, it looks like what the enlightenment thinkers were up to had the
[00:24:28.640 --> 00:24:34.560]   same kind of esoteric, strange, overly cerebral feel at the time and ended up mattering a huge
[00:24:34.560 --> 00:24:38.240]   amount. So it doesn't feel like there's zero precedent either. Yeah. Maybe I'm a bit more
[00:24:38.240 --> 00:24:42.880]   pessimistic about that because people like-- the people who are working on individual rights
[00:24:42.880 --> 00:24:46.400]   frameworks like Log-- I don't think they were like anticipating an industrial revolution.
[00:24:46.400 --> 00:24:48.960]   Yeah, they weren't. Yeah. I mean, I feel like the person that actually did anticipate the
[00:24:48.960 --> 00:24:53.040]   industrial revolution probably-- his political philosophy was actually probably a negative
[00:24:53.040 --> 00:24:55.120]   given, you know-- Are we done with that?
[00:24:55.120 --> 00:25:00.240]   Karl Marx. Okay. Yeah, yeah. Yeah. So it's not obvious to me that even if you saw something
[00:25:00.240 --> 00:25:02.960]   like this happening that you're-- Oh, it's totally not obvious. I mean,
[00:25:02.960 --> 00:25:07.200]   I think my basic position here is we-- I'm not sitting here like highly confident. I'm not
[00:25:07.200 --> 00:25:10.560]   saying, you know, there's tons of precedent and we know exactly what to do. That's not what I
[00:25:10.560 --> 00:25:14.640]   believe. I believe we should be giving it a shot. I think we should be trying and I don't think we
[00:25:14.640 --> 00:25:18.720]   should be totally defeatist and saying, "Well, it's so obvious that there's never anything you
[00:25:18.720 --> 00:25:22.480]   could have come up with throughout history and humans have been helpless to predict the future."
[00:25:22.480 --> 00:25:26.400]   I don't think that is true. And so, yeah, I think that's like enough of an example to kind
[00:25:26.400 --> 00:25:30.720]   of illustrate that. And I mean, gosh, like you could make the same statement today as you could
[00:25:30.720 --> 00:25:35.760]   say, "Look, doing research on how to get AI systems to behave as intended is a perfectly
[00:25:35.760 --> 00:25:41.040]   fine thing to do at any period in time." It's not like a bad thing to do. And I think John Locke was
[00:25:41.040 --> 00:25:45.120]   doing his stuff because he felt it was a good thing to do at any period in time. But the thing
[00:25:45.120 --> 00:25:48.560]   is that if we are at this crucial period of time, it becomes an even better thing to do and it
[00:25:48.560 --> 00:25:51.840]   becomes magnified to the point where it could be more important than other things.
[00:25:51.840 --> 00:25:56.080]   Now, the one reason I might be skeptical of this theory is that I could say, "Oh, gosh,
[00:25:56.080 --> 00:25:59.760]   if you look throughout history, people were often convinced they were living in the most
[00:25:59.760 --> 00:26:04.640]   important time," or at least an especially important time. And if you go back, I mean,
[00:26:04.640 --> 00:26:09.120]   everybody can be right about living in the most important time. So maybe I should just have a very
[00:26:09.120 --> 00:26:12.640]   low prior that anybody is right about this kind of thing. How do you respond to that kind of logic?
[00:26:12.640 --> 00:26:17.760]   Well, I mean, so first off, I don't know that it's really true that it's that common for people who
[00:26:17.760 --> 00:26:20.800]   said they're living in the most important time in history. I mean, this would be like an interesting
[00:26:20.800 --> 00:26:26.240]   thing to look at, but just from stuff I've read about whatever past works on political philosophy
[00:26:26.240 --> 00:26:30.160]   and stuff, I don't exactly see this claim all over the place. It definitely happens. It's
[00:26:30.160 --> 00:26:34.800]   definitely happened. And I think a way of thinking about it is there's kind of two reasons that you
[00:26:34.800 --> 00:26:38.880]   might think you are especially important. One is that you actually are, and you made reasonable
[00:26:38.880 --> 00:26:43.280]   observations about it. And another is that you want to be, or you want to think you are. You're
[00:26:43.280 --> 00:26:48.480]   self-deceiving. And so over the long sweep of history, a lot of people will come to this
[00:26:48.480 --> 00:26:52.400]   conclusion for the second reason, and most of the people who think they're the most important will
[00:26:52.400 --> 00:26:56.400]   be wrong. So that's all true. And that certainly could apply to me, and it certainly could apply
[00:26:56.400 --> 00:27:00.960]   to others. But I mean, so I think that's like, that's just completely fine and completely true.
[00:27:00.960 --> 00:27:05.520]   And I think we should have some skepticism when we find ourselves making these kinds of observations.
[00:27:05.520 --> 00:27:10.880]   And at the same time, I think it would be like a really bad rule or something, or a really bad norm
[00:27:10.880 --> 00:27:14.960]   that every time you find yourself thinking the stakes are really high, or that you're in a really
[00:27:14.960 --> 00:27:19.120]   important position, that you just decide to ignore the thought. I think that would be very bad. Like
[00:27:19.120 --> 00:27:24.080]   if you just imagine a universe where there actually are some people who live in especially important
[00:27:24.080 --> 00:27:28.880]   time, and then there's a bunch of other people who tell stories to themselves about whether they do,
[00:27:28.880 --> 00:27:34.080]   how would you want all those people to behave? And it's like, to me, the worst possible rule
[00:27:34.080 --> 00:27:37.200]   is all those people should just be like, nah, this is crazy and forget about it. That's like
[00:27:37.200 --> 00:27:40.240]   the worst possible rule. Because the people who are living at the important time will then do
[00:27:40.240 --> 00:27:44.560]   the wrong thing. I think another bad rule would be that everyone should take themselves completely,
[00:27:44.560 --> 00:27:48.720]   seriously, and literally, and just promote their own interests ahead of everyone else's.
[00:27:48.720 --> 00:27:53.360]   And a rule I would propose over either of them is that all these people should take their beliefs
[00:27:53.360 --> 00:27:57.520]   reasonably seriously and try to do the best thing according to their beliefs, but should also adhere
[00:27:57.520 --> 00:28:03.520]   to common sense standards of ethical conduct and not do too much ends justify the means reasoning.
[00:28:03.520 --> 00:28:07.680]   You know, I think whatever people are trying to do to help the most important century go well,
[00:28:07.680 --> 00:28:12.560]   it's totally good and fine to do research on AI alignment. But people shouldn't be like,
[00:28:12.560 --> 00:28:16.560]   going and you know, telling lies or breaking the law in order to further their ends.
[00:28:16.560 --> 00:28:20.720]   That would be my proposed rule is that we should, you know, when we have these high stakes crazy
[00:28:20.720 --> 00:28:24.800]   thoughts, we should do what we can about them and not go so crazy about them that we break all the
[00:28:24.800 --> 00:28:28.480]   rules of society. And that seems like a better rule. That's the rule I'm trying to follow.
[00:28:28.480 --> 00:28:32.080]   Okay, can you can you talk more about that? So if for some reason, we can convince that
[00:28:32.080 --> 00:28:36.000]   the expected value calculation was immense, that you had to break some law in order to increase
[00:28:36.000 --> 00:28:40.240]   the odds that AI goes well, I don't know what hypothetical this would be. Is it just that
[00:28:40.240 --> 00:28:43.040]   you're not sure that you would be right? So you just want to err on the side of caution?
[00:28:43.040 --> 00:28:47.280]   Or much? Yeah, I'm really not a fan of ends justify the means reasoning. I just think it's
[00:28:47.280 --> 00:28:51.520]   like historically looks, that's the thing that looks really, really bad is is people kind of
[00:28:51.520 --> 00:28:57.040]   saying, you know, it's it's worth doing horrible things and coercing each other and, you know,
[00:28:57.040 --> 00:29:02.080]   using force to accomplish these things that, you know, the ends we're trying to get to are
[00:29:02.080 --> 00:29:05.840]   more important than everything else. So I'm, I'm against that stuff. I think that stuff looks a lot
[00:29:05.840 --> 00:29:10.240]   worse historically than people like, trying to break the future and do helpful things.
[00:29:10.240 --> 00:29:14.400]   So I see my main role in the world is as trying to break the future and do helpful things.
[00:29:14.400 --> 00:29:20.400]   I can do that without a, you know, without doing a bunch of harmful or common sense unethical stuff.
[00:29:20.400 --> 00:29:24.880]   Maybe someday, you know, there will be one of these intense trade offs, I haven't really felt
[00:29:24.880 --> 00:29:28.800]   like I've run into them yet. And if I ever ran into one of those intense trade offs, I have to
[00:29:28.800 --> 00:29:33.280]   ask myself how confident I really am. The current level of information and confidence I have is,
[00:29:33.280 --> 00:29:36.960]   in my opinion, not enough to do to do really unjustify the meansy stuff.
[00:29:36.960 --> 00:29:42.320]   Yeah. Okay, so let's talk about the potential implausibility of continued high growth.
[00:29:42.320 --> 00:29:45.920]   One thing somebody might think is, okay, maybe 2% growth can't keep going on forever.
[00:29:45.920 --> 00:29:50.000]   But, you know, maybe growth slows down to like 0.5% a year or something.
[00:29:50.000 --> 00:29:54.720]   And as you know, small differences in growth rates have like big effects on the end result. So by the
[00:29:54.720 --> 00:29:58.720]   point that we've exhausted all the possible growth in the galaxy, we'll probably be able to expand to
[00:29:58.720 --> 00:30:03.440]   other galaxies. What, what is wrong with that kind of train of logic where, you know, if there's like
[00:30:03.440 --> 00:30:08.400]   0.5% growth, that still doesn't imply a lock-in or it'd be, it'd be weird if that implied lock-in.
[00:30:08.400 --> 00:30:12.400]   Um, I'm, I think we might want to give a little bit more context here or something. So, so one,
[00:30:12.400 --> 00:30:16.720]   one of the key arguments of the most important century is it's just part, it's just one of the
[00:30:16.720 --> 00:30:20.800]   arguments that we live in a strange time. But I'm arguing the current level of economic growth just
[00:30:20.800 --> 00:30:25.840]   looks too high to go on for another 10,000 years or so. And one of the points I make, which is a
[00:30:25.840 --> 00:30:29.520]   point I got from Robin Hanson, it's not, it's not my own originally, is that if you just take the
[00:30:29.520 --> 00:30:33.920]   current level of economic growth and extrapolate it out 10,000 years, you end up having to conclude
[00:30:33.920 --> 00:30:38.960]   that we would need multiple stuff that is worth as much as the whole world economy is today,
[00:30:38.960 --> 00:30:43.840]   multiple times that per atom in the galaxy. And if you believe we can't break the speed of light,
[00:30:43.840 --> 00:30:46.800]   then we can't get further than that. We can't get outside the galaxy. So in some sense,
[00:30:46.800 --> 00:30:51.120]   we like run out of material. And so you're saying, well, okay, but what if the growth rate falls to
[00:30:51.120 --> 00:30:56.240]   0.5%? And then I'm kind of like, okay, well, so the growth rate now I ballparked it in the post
[00:30:56.240 --> 00:31:01.120]   is around 2%. That's the growth rate generally in the, in the most developed countries. Um, let's
[00:31:01.120 --> 00:31:07.440]   say it falls to 0.5%. And then I'm like, okay, so the growth rate falls to 0.5%, just like for how
[00:31:07.440 --> 00:31:11.040]   long for, did, did you calculate how long it would take to get to the same place? Um, yeah,
[00:31:11.040 --> 00:31:17.760]   I think it was like 25,000 years. 0.5% gets you like one world size, um, yeah, economy. It's
[00:31:17.760 --> 00:31:23.440]   10,000 versus 25,000, but 25,000 is the amount of, uh, light years between us and like the next
[00:31:23.440 --> 00:31:27.360]   galaxy. That doesn't sound right. I don't think this galaxy calculation was very close. There's
[00:31:27.360 --> 00:31:30.640]   also going to be a bunch of dead space. Like as you get to the outer reach of the galaxy,
[00:31:30.640 --> 00:31:33.760]   there's not going to be as much there. That doesn't sound super right, but let's,
[00:31:33.760 --> 00:31:37.200]   let's just roll with it. I mean, let's just, let's just say that, let's just say that you
[00:31:37.200 --> 00:31:42.640]   had 2% today and then growth went down to 0.5% and just like stayed there forever. Yeah. I'm
[00:31:42.640 --> 00:31:46.080]   pretty sure that's still too big. I'm pretty sure you're still going to hit limits in some reasonable
[00:31:46.080 --> 00:31:49.680]   period of time, but also that would still be just like weird on its own. It would just be like,
[00:31:49.680 --> 00:31:55.440]   well, we lived in the 200 year period when we had 2% growth and then we had 0.5% growth forever.
[00:31:55.440 --> 00:31:58.400]   That would still make this like kind of an interesting time. It would be like the most
[00:31:58.400 --> 00:32:03.760]   dynamic, fastest changing time in all of human history, not by a ton, but it's also like you
[00:32:03.760 --> 00:32:08.000]   pick the number that's like the closest and the most perfectly optimized here. So, you know,
[00:32:08.000 --> 00:32:13.840]   if it went down to 0.1% or even down to 0.01%, then it would take longer to run out of stuff,
[00:32:13.840 --> 00:32:18.400]   but it would be even stranger, the 2% versus the 0.01%. So I don't, I don't really think there's
[00:32:18.400 --> 00:32:22.400]   any way out of like, gosh, this looks like, this looks like it's probably going to end up looking
[00:32:22.400 --> 00:32:26.000]   like a very special time or a very weird time. This is probably not getting worth getting hung
[00:32:26.000 --> 00:32:31.440]   up on, but like from that perspective, then the century in which we had 8% growth because of the
[00:32:31.440 --> 00:32:35.600]   industrial revolution, that would be that you would say that maybe that's the most important century.
[00:32:35.600 --> 00:32:39.840]   Oh, sure. Yeah, yeah, no, totally. No, the thing, the thing about the rapid growth is not supposed
[00:32:39.840 --> 00:32:44.080]   to be on its own. An argument that this century is the special one. I mean, by, by growth standards,
[00:32:44.080 --> 00:32:48.320]   this century looks less special than like the last one or two. It's, it's just, it's, it's saying
[00:32:48.320 --> 00:32:52.880]   this century is one of a handful, or I think when I say like one of the 80 of the most significant
[00:32:52.880 --> 00:32:56.640]   centuries or something by economic growth standards. Um, and that's only one argument.
[00:32:56.640 --> 00:33:00.640]   And then I look at a lot of other ways in which this century looks unusual, but just to be clear,
[00:33:00.640 --> 00:33:05.840]   I mean, to say that something is the most important century of all time sounds totally
[00:33:05.840 --> 00:33:09.920]   nuts because there's so many centuries. There's just so many centuries in the history of,
[00:33:09.920 --> 00:33:13.760]   of humanity, even, and especially if you want to think about it on galactic timescales.
[00:33:13.760 --> 00:33:18.480]   And even once you narrow it down to 80, it's just much way less weird. It's just like, well,
[00:33:18.480 --> 00:33:23.280]   if, if I've already convinced you using kind of non-controversial reasoning that we're one of the
[00:33:23.280 --> 00:33:27.840]   80 most important centuries, it shouldn't take me nearly as much further evidence to say, actually,
[00:33:27.840 --> 00:33:32.160]   this one might be number one out of 80 because you're, you're starting odds are more than 1%.
[00:33:32.160 --> 00:33:37.120]   So to get you up to 10% or 20% or 30%, it's, it's, it doesn't necessarily require a massive
[00:33:37.120 --> 00:33:40.320]   update the way that it would if we're just starting from nowhere. But I guess I'm still
[00:33:40.320 --> 00:33:45.280]   not convinced that just because we, uh, this is a weird century that has any implications for
[00:33:45.840 --> 00:33:50.480]   why or whether we should see transformative AI this century. So if we have a model about when
[00:33:50.480 --> 00:33:53.760]   transformative AI happens and if one of the variables that goes into that is what is the
[00:33:53.760 --> 00:33:58.880]   growth rate in zero AD and what is the growth rate in 2080, it just feels weird to have as
[00:33:58.880 --> 00:34:02.640]   a parameter and like when the specific technological development is going to happen.
[00:34:02.640 --> 00:34:05.520]   It's just one argument in the series. It's just kind of, you know, I think,
[00:34:05.520 --> 00:34:09.360]   I think the way that I would come at it is I would just say, Hey, look at, look at AI systems,
[00:34:09.360 --> 00:34:12.880]   look at what they're doing, look at how fast the rate of progress is. Look at these like five
[00:34:12.880 --> 00:34:17.040]   different angles on imagining when AI might be able to do all the things humans do to advance
[00:34:17.040 --> 00:34:22.240]   science and technology. And, and just imagine that we get there this century, wouldn't it be crazy
[00:34:22.240 --> 00:34:26.400]   to have AI that could do all the things humans do to advance science and technology? Wouldn't that
[00:34:26.400 --> 00:34:31.120]   lead to just a lot of crazy stuff happening? Like there's only ever been one species in the history
[00:34:31.120 --> 00:34:35.040]   of the universe that we know of that can do the kinds of things humans do. Wouldn't it be weird
[00:34:35.040 --> 00:34:38.960]   if there were two, that would be crazy. And, and one of the, one of them was a new one we built
[00:34:38.960 --> 00:34:44.160]   that could be like copied at will run at different speeds, run on any, you know, any hardware you have
[00:34:44.160 --> 00:34:48.080]   that would be crazy. And then you might come back and say, yeah, that would be crazy. This is too
[00:34:48.080 --> 00:34:52.080]   crazy. It's like, I I'm ruling this out because this is too crazy. And then I would say, okay,
[00:34:52.080 --> 00:34:57.040]   well, we have a bunch of evidence that we live in an unusual, crazy time. And you actually should
[00:34:57.040 --> 00:35:01.920]   think that there's like a lot of signs that this, this century is not just a random century picked
[00:35:01.920 --> 00:35:06.480]   from a sample of millions of centuries. Um, so that's the basic structure of the argument. Uh,
[00:35:06.480 --> 00:35:10.880]   as, as far as the growth rate in zero AD, I mean, I think it matters. I think you're asking the
[00:35:10.880 --> 00:35:14.880]   question, why do the dynamics of, of growth in zero AD matter at all for this argument?
[00:35:14.880 --> 00:35:19.200]   And I think it's because it just, it's a question of like, how does economic growth work generally
[00:35:19.200 --> 00:35:23.120]   and how has it worked and what is the trend that we're on and what happens if that trend continues?
[00:35:23.120 --> 00:35:27.600]   So if around zero AD growth was very low, but accelerating. And if that was also true at a
[00:35:27.600 --> 00:35:31.680]   hundred AD and a thousand AD and negative a thousand, um, or, you know, a thousand BC,
[00:35:31.680 --> 00:35:35.200]   however you want to put it, if all those things are true, then it starts to point to a general
[00:35:35.200 --> 00:35:38.640]   pattern that growth is accelerating and maybe accelerating for a particular reason. And
[00:35:38.640 --> 00:35:43.040]   therefore you might expect more acceleration. Gotcha. Gotcha. Okay. So let's just talk about
[00:35:43.040 --> 00:35:49.120]   transformative AI then. Yeah. Can you describe what success looks like concretely? Are humans
[00:35:49.120 --> 00:35:55.680]   part of the post transformative AI world? Are we hoping that these AI's become, uh,
[00:35:55.680 --> 00:36:00.880]   enslaved gods that help us create a utopia? What is the concrete success scenario look like?
[00:36:00.880 --> 00:36:04.320]   Uh, I mean, I think we've, we've talked a lot about the difficulty of predicting the future.
[00:36:04.320 --> 00:36:09.360]   And I think I do want to emphasize that I really do believe in that. So my attitude to the most
[00:36:09.360 --> 00:36:13.520]   important century is not at all, Hey, I know exactly what's going to happen. And I'm making
[00:36:13.520 --> 00:36:18.880]   a plan to get us through it. It's much more like there's a general fuzzy outline of a big thing,
[00:36:18.880 --> 00:36:23.280]   uh, that might be approaching us. There's maybe like two or three things we can come up with
[00:36:23.280 --> 00:36:26.960]   that seem good to do everything else. We think about, we're not going to know if it's good to
[00:36:26.960 --> 00:36:30.800]   do or bad to do. And so I'm just trying to find the things that are good to do so that I can make
[00:36:30.800 --> 00:36:34.960]   things go a little bit better or help things go a little bit better. That is my general attitude.
[00:36:34.960 --> 00:36:38.800]   So it's, um, you know, it's, I don't know. It's like, if you were, if you were on a, on a ship
[00:36:38.800 --> 00:36:43.760]   in a storm and you saw some like very large fuzzy object obscured by the clouds, you might want to
[00:36:43.760 --> 00:36:47.440]   steer away from it. You might not want to say, well, what I think that is, is it's an Island.
[00:36:47.440 --> 00:36:51.200]   And I think there's probably, you know, a tiger on it. And if we go and train the tiger in the
[00:36:51.200 --> 00:36:54.000]   right way, blah, blah, blah, blah, blah, you don't want to get into that. Right. So that,
[00:36:54.000 --> 00:36:57.120]   that is the general attitude I'm taking. So what does success look like to me? I mean,
[00:36:57.920 --> 00:37:01.760]   success could look like a lot of things, but one thing success would look like to me
[00:37:01.760 --> 00:37:07.920]   would frankly, just be that we get something not too different from the trajectory we're already
[00:37:07.920 --> 00:37:14.080]   on. So in other words, if we can have AI systems that behaved as intended, acted as tools and
[00:37:14.080 --> 00:37:18.400]   amplifiers of humans did the things they're supposed to do. And if we could avoid a world
[00:37:18.400 --> 00:37:24.400]   where those AI systems got sort of like, I don't know, all controlled by one government or one
[00:37:24.400 --> 00:37:28.960]   person avoid a world where that caused a huge concentration of power. If we could just have
[00:37:28.960 --> 00:37:32.480]   a world where AI systems are just another technology, they help us do a lot of stuff.
[00:37:32.480 --> 00:37:37.120]   We invent lots of other technologies and everything is like relatively broadly distributed and
[00:37:37.120 --> 00:37:40.880]   everything works roughly as it's supposed to work. Then you might be in a world where we
[00:37:40.880 --> 00:37:44.880]   continue the trend we've seen over the last couple of hundred years, which is that we're
[00:37:44.880 --> 00:37:50.160]   all getting richer. We're all getting more tools. We all hopefully get increasing ability to kind
[00:37:50.160 --> 00:37:54.400]   of understand ourselves, study ourselves, understand what makes us happy, what makes us
[00:37:54.400 --> 00:38:01.280]   thrive. And hopefully the world just gets better over time. And we have more and more new ideas.
[00:38:01.280 --> 00:38:06.240]   The ideas make us hopefully wiser. And I do think that in most respects, the world of today is just
[00:38:06.240 --> 00:38:10.560]   like a heck of a lot better than the world of 200 years ago. I don't think the only reason for that
[00:38:10.560 --> 00:38:15.280]   is wealth and technology, but I think they played a role. And I think that like, yeah, if you'd gone
[00:38:15.280 --> 00:38:20.000]   back to 200 years ago and said, Holden, how would you like the world to develop a bunch of new
[00:38:20.000 --> 00:38:24.480]   technologies as long as they're like sort of evenly distributed and they behave roughly as intended
[00:38:24.480 --> 00:38:28.480]   and people mostly just get richer and discover new stuff? I'd be like, that sounds great. I don't
[00:38:28.480 --> 00:38:32.960]   know exactly where we're going to land. I can't predict in advance whether we're going to decide
[00:38:32.960 --> 00:38:36.640]   that we want to treat our technologies as having their own rights. That's stuff that the world
[00:38:36.640 --> 00:38:41.120]   will figure out. But I'd like to avoid massive disasters that are identifiable because I think
[00:38:41.120 --> 00:38:45.280]   if we can, we might end up in a world where the future is wiser than we are and is able to do
[00:38:45.280 --> 00:38:52.080]   better things. Okay. The way you put it with AI enabling humans, that doesn't sound like something
[00:38:52.080 --> 00:38:57.760]   that could last for thousands of years. It almost sounds as weird as like Chim saying, you know,
[00:38:57.760 --> 00:39:02.080]   what we would like is as humans to be our tools. At best, maybe they could hope we would give them
[00:39:02.080 --> 00:39:07.280]   nice zoos. But like, what is the role for humans in this future? I mean, a world I could easily
[00:39:07.280 --> 00:39:12.000]   imagine, although that doesn't mean it's realistic at all, is a world where we do build these AI
[00:39:12.000 --> 00:39:18.240]   systems, they do what they're supposed to do, and we kind of use them to gain, you know, more
[00:39:18.240 --> 00:39:23.600]   intelligence and wisdom. I've talked a little bit about this hypothetical idea of digital people,
[00:39:23.600 --> 00:39:27.520]   maybe we develop something like that. And then, you know, after a hundred years of this,
[00:39:27.520 --> 00:39:32.480]   we've been around and people have been having discussions in the public sphere and people kind
[00:39:32.480 --> 00:39:36.720]   of start to talk about whether the AIs themselves do have rights of their own and should be sharing
[00:39:36.720 --> 00:39:41.200]   the world with us. And then maybe they do get rights and maybe some AI systems end up voting,
[00:39:41.200 --> 00:39:44.960]   or maybe we decide they shouldn't and they don't. And either way, you have this kind of world where
[00:39:44.960 --> 00:39:49.440]   there's a bunch of different beings that all have rights and interests that matter. And they vote on
[00:39:49.440 --> 00:39:52.720]   how to set up the world so that we can all hopefully thrive and have a good time. We have
[00:39:52.720 --> 00:39:56.720]   less and less material scarcity. So fewer and fewer trade-offs need to be made. That would be
[00:39:56.720 --> 00:40:00.320]   great. I don't know exactly where it ends or what it looks like, but that does, I don't know. I mean,
[00:40:00.320 --> 00:40:04.400]   what does anything strike you as like, as, as, as unimaginable about that?
[00:40:04.400 --> 00:40:09.360]   Yeah. The fact that you can have beings that can be copied at will, but also there's some method
[00:40:09.360 --> 00:40:13.360]   of voting that. Oh yeah. Yeah. So yeah, that's a problem that would have to be solved. I mean,
[00:40:13.360 --> 00:40:17.840]   we have a lot of today, we have a lot of attention paid to, you know, how voting system works,
[00:40:17.840 --> 00:40:22.880]   who gets to vote and how we avoid things being unfair. And yeah, I mean, it's, it's, it's
[00:40:22.880 --> 00:40:27.040]   definitely true that if we had, if we decided there was some kind of digital entity that it
[00:40:27.040 --> 00:40:30.720]   should have the right to vote and that digital entity was able to copy itself, well, you could
[00:40:30.720 --> 00:40:35.120]   get some havoc right there. So you'd, you'd want to come up with some system that maybe restricts
[00:40:35.120 --> 00:40:38.640]   how many copies you can make of yourself or restricts how many of those copies can vote.
[00:40:38.640 --> 00:40:44.400]   These are problems that I, that I'm hoping can be handled in a, in a way that while not perfect,
[00:40:44.400 --> 00:40:49.040]   could be non-catastrophic, uh, by a society that hasn't been derailed by some huge concentration
[00:40:49.040 --> 00:40:52.560]   of power or misaligned AI systems. So that sounds like that might take time,
[00:40:52.560 --> 00:40:57.040]   but let's say you didn't have time. So let's say you get a call and somebody says, hold on next
[00:40:57.040 --> 00:41:02.160]   month, my company is developing or deploying a model that might plausibly lead to AGI. What,
[00:41:02.160 --> 00:41:05.440]   what does open philanthropy do? What do you do? Well, I need to distinguish. I mean,
[00:41:06.080 --> 00:41:11.280]   you may not have time to avoid some of these like catastrophes, like a huge concentration of power
[00:41:11.280 --> 00:41:15.200]   or AI systems that don't behave as intended and have their own goals. If you can prevent those
[00:41:15.200 --> 00:41:19.280]   catastrophes from happening, you might then get more time after you build the AIs to have these
[00:41:19.280 --> 00:41:23.840]   tools that help us, you know, help us invent new technologies and help us perhaps figure things out
[00:41:23.840 --> 00:41:27.200]   better and ask better questions. And then you could have a lot of time where you could figure
[00:41:27.200 --> 00:41:31.120]   out a lot in a little time if you had those things. Um, but if someone said, how long did
[00:41:31.120 --> 00:41:37.360]   you give me a month, a month, let's say three months. So it's a little bit more. Yeah. I would,
[00:41:37.360 --> 00:41:42.480]   I would find that extremely scary. And I think I would, I think that would do, I kind of,
[00:41:42.480 --> 00:41:46.720]   I kind of feel like that's one of the worlds in which I might not even be able to offer an
[00:41:46.720 --> 00:41:51.840]   enormous amount. I think so. So I, my job is in philanthropy and a lot of what philanthropists
[00:41:51.840 --> 00:41:57.680]   do historically or have done well historically is we help fields grow. We help do things that
[00:41:57.680 --> 00:42:01.840]   operate on very long timescales. So an example of something open philanthropy does a lot of right
[00:42:01.840 --> 00:42:08.080]   now is we fund people who do research on AI alignment and we fund people who are thinking
[00:42:08.080 --> 00:42:11.920]   about what it would look like to get through the most important centuries successfully.
[00:42:11.920 --> 00:42:16.560]   And a lot of these people right now are like very early in their career and just figuring stuff out.
[00:42:16.560 --> 00:42:19.840]   And so a lot of the world I picture is like, it's 10 years from now, it's 20 years from now,
[00:42:19.840 --> 00:42:25.920]   it's 50 years from now. And there's this whole field of expertise that got support when traditional
[00:42:25.920 --> 00:42:29.840]   institutions wouldn't support it. And that was because of us. And then you come to me and you
[00:42:29.840 --> 00:42:33.520]   say, we've got one week left. What do we do? And I'm like, I don't know. We did what we could do,
[00:42:33.520 --> 00:42:37.920]   you know, like go back in time and like try to prepare for this better. So
[00:42:37.920 --> 00:42:41.280]   that would be a lot of my answer. I mean, I could say more specific things about what I'd say in the
[00:42:41.280 --> 00:42:45.600]   one to three month timeframe, but a lot of it would be like flailing around and freaking out,
[00:42:45.600 --> 00:42:51.040]   frankly. Gotcha. Okay. So maybe we can reverse the question and let's say you found out that
[00:42:51.040 --> 00:42:56.080]   it actually is going to take much longer than you thought. And you have like more than five decades.
[00:42:56.080 --> 00:42:59.040]   Yeah. What changes? What are you able to do that you might not otherwise be able to do?
[00:42:59.040 --> 00:43:05.920]   Yeah. I mean, I think the further out things are, the more I think it's valid to say that humans
[00:43:05.920 --> 00:43:11.120]   have trouble making predictions on long timeframes. And the more I'm like interested in focusing more
[00:43:11.120 --> 00:43:15.440]   on other causes or on very broad things we do, such as trying to grow the set of people who
[00:43:15.440 --> 00:43:20.640]   thinks about issues like this, rather than trying to specifically study, for example, how to get AI
[00:43:20.640 --> 00:43:25.680]   systems like today's to behave as intended. So I think that's a general shift, but I would say that,
[00:43:25.680 --> 00:43:30.320]   yeah, I tend to feel a bit more optimistic on longer timeframes because I do think that the
[00:43:30.320 --> 00:43:34.000]   world just isn't ready for this and isn't thinking seriously about this. And a lot of what we're
[00:43:34.000 --> 00:43:39.440]   trying to do at Open Philanthropy is create support that doesn't exist in traditional
[00:43:39.440 --> 00:43:45.760]   institutions for people to think about these topics. And that includes doing the AI alignment
[00:43:45.760 --> 00:43:50.480]   research. That also includes thinking through how we want politically, what regulations we might
[00:43:50.480 --> 00:43:54.640]   want to prevent disaster. I think those are a lot of the things. So it's kind of a spectrum. I would
[00:43:54.640 --> 00:44:02.240]   say, if it's in three months, I would probably be trying to hammer out a reasonable test of
[00:44:02.240 --> 00:44:06.800]   whether we can demonstrate that the AI system is either safe or dangerous. And if we could
[00:44:06.800 --> 00:44:13.040]   demonstrate it's dangerous, use that demonstration to really advocate for a broad slowing of AI
[00:44:13.040 --> 00:44:17.200]   research, to buy more time, to figure out how to make it less dangerous. But I don't know that I
[00:44:17.200 --> 00:44:21.680]   feel that much optimism. If AI is like five, you know, this kind of AI is 500 years off,
[00:44:21.680 --> 00:44:25.840]   then I'm kind of inclined to just ignore it and just try and make the world better and more robust
[00:44:25.840 --> 00:44:30.480]   and wiser. But I think if we've got, you know, 10 years, 20 years, 50 years, 80 years, something in
[00:44:30.480 --> 00:44:35.920]   that range, I think that is kind of the place where, you know, supporting early careers and
[00:44:35.920 --> 00:44:40.560]   supporting people who are going to spend their lives thinking about this, then we flash forward
[00:44:40.560 --> 00:44:43.520]   to this crucial time. And there's a lot more people who spent their lives thinking about it.
[00:44:43.520 --> 00:44:46.800]   And I think that would be a big deal. Let's talk about the question of whether
[00:44:46.800 --> 00:44:51.040]   we can expect AI to be smart enough to disempower humanity, but, you know, dumb enough to have that
[00:44:51.040 --> 00:44:56.000]   kind of goal. You know, when I look out at smart people in the world, it seems like a lot of them
[00:44:56.000 --> 00:45:01.520]   have very complex, nuanced goals that they've thought a lot about what is good and how to do
[00:45:01.520 --> 00:45:06.880]   good. A lot of them don't. Yeah. Okay. Yeah. But does that overall make you more optimistic about
[00:45:06.880 --> 00:45:13.680]   AIs? I am not that comforted by that. I, you know, I pretty much this, this is a very,
[00:45:13.680 --> 00:45:18.160]   very old debate in the world of AI alignment. Eliezer Yudkowsky has something called the
[00:45:18.160 --> 00:45:21.920]   orthogonality thesis. I don't remember exactly what it says, but it's something like,
[00:45:21.920 --> 00:45:26.800]   you could be very intelligent about any goal. You could have the stupidest goal and be very
[00:45:26.800 --> 00:45:30.720]   intelligent about how to, how to get it. And in many ways, a lot of human goals are pretty silly.
[00:45:30.720 --> 00:45:35.120]   Like a lot of the things that make me happy are not things that are, you know, profound or
[00:45:35.120 --> 00:45:38.800]   wonderful. They're just things that happen to make me happy. And you could very intelligently
[00:45:38.800 --> 00:45:44.160]   try to get those things. So it doesn't give me a lot of comfort. I think basically my picture of
[00:45:44.160 --> 00:45:48.400]   how modern AI works is that you're basically training these systems by trial and error.
[00:45:48.400 --> 00:45:53.440]   And so you're basically taking, you know, taking an AI system, you're encouraging some behaviors,
[00:45:53.440 --> 00:45:58.480]   discouraging other behaviors, and you might end up with a system that's like being encouraged
[00:45:58.480 --> 00:46:02.480]   to pursue something that you didn't mean to encourage. And it does it very intelligently.
[00:46:02.480 --> 00:46:06.320]   And I don't see any contradiction there. I think that just, just because you, you know, if you,
[00:46:06.320 --> 00:46:11.360]   if you were to design an AI system and you were, you were kind of giving it encouragement every
[00:46:11.360 --> 00:46:16.160]   time it was, you know, getting more money into your bank account. If you were encouraging it
[00:46:16.160 --> 00:46:19.760]   to do that, you might get something that's very, very good at getting money into your bank account
[00:46:19.760 --> 00:46:22.720]   to the point where we're going to disrupt the whole world to do that. And you will not
[00:46:22.720 --> 00:46:27.040]   automatically get something that thinks, gosh, is this a good thing to do? And I think a lot of,
[00:46:27.040 --> 00:46:30.880]   you know, a lot of human goals are just like, there's not really a right answer about whether
[00:46:30.880 --> 00:46:35.600]   our goals actually make sense. They're just the goals we have. You've written elsewhere about how
[00:46:35.600 --> 00:46:39.440]   moral progress is something that's real, that's historically happened. And it corresponds to like
[00:46:39.440 --> 00:46:43.600]   what actually counts as moral progress. I wonder, do you think there's reason to think the same
[00:46:43.600 --> 00:46:46.960]   thing might happen with AI? Like whatever the process, whatever the process is that creates
[00:46:46.960 --> 00:46:52.160]   this moral progress? I kind of don't in particular. So I've kind of used the term moral progress as a,
[00:46:52.160 --> 00:46:57.520]   as just a term to refer to changes in morality that are good. I think there has been moral
[00:46:57.520 --> 00:47:01.280]   progress. But I don't think that means moral progress is something inevitable or something
[00:47:01.280 --> 00:47:05.760]   that happens every time you are intelligent or something like that. So I just, you know,
[00:47:05.760 --> 00:47:10.320]   an example I use a lot is just like attitude towards homosexuality. It's a lot more accepted
[00:47:10.320 --> 00:47:14.800]   today than it used to be. I call that moral progress because I think it's good. And some
[00:47:14.800 --> 00:47:19.360]   people will say, well, you know, I don't believe that morality is objectively good or bad. I don't
[00:47:19.360 --> 00:47:23.520]   believe there is any such thing as moral progress. I just think things change randomly. And that will
[00:47:23.520 --> 00:47:27.120]   often be an example I'll pull out. And I say, but do you think that was a neutral change? I just
[00:47:27.120 --> 00:47:30.640]   think it was good. I think it was good, but that's not because I believe there's some underlying
[00:47:30.640 --> 00:47:37.040]   objective reality. It's just my way of tagging or using language to talk about moral changes
[00:47:37.040 --> 00:47:41.840]   that seem like they were positive to me. I don't particularly expect that an AI system
[00:47:41.840 --> 00:47:46.800]   would have the same sort of, would have the same evolution that I've had in reflecting on morality
[00:47:46.800 --> 00:47:50.960]   or would come to the same conclusions I've come to, or would come up with moralities that seem
[00:47:50.960 --> 00:47:54.720]   good to me. I don't have any reason to think any of that. I do think historically there have been
[00:47:54.720 --> 00:47:58.560]   some cases of moral progress. All right. What do you think is the explanation for that historical
[00:47:58.560 --> 00:48:03.120]   progress? Well, I mean, one thing that I would say is that, you know, humans have a lot in common
[00:48:03.120 --> 00:48:08.720]   with each other. So I think some of history contains cases of humans just kind of learning
[00:48:08.720 --> 00:48:13.040]   more about the world, learning more about themselves, debating each other. Maybe I think
[00:48:13.040 --> 00:48:17.920]   a lot of moral progress has just come from humans, like getting to know other humans who they
[00:48:17.920 --> 00:48:23.440]   previously were stereotyping and judging negatively and afraid of. So I think there's some way in which
[00:48:23.440 --> 00:48:27.840]   humans learning about the world and learning about themselves leads them to have kind of
[00:48:27.840 --> 00:48:32.720]   conclusions that are more reflective and more intelligent for their own goals. But if
[00:48:32.720 --> 00:48:36.320]   you brought in something into the picture that was not a human at all, it might be very intelligent
[00:48:36.320 --> 00:48:39.840]   and reflective about its goals, and those goals might have zero value from our point of view.
[00:48:39.840 --> 00:48:42.800]   Recent developments in AI have made many people think that AI could happen much
[00:48:42.800 --> 00:48:47.280]   sooner than they otherwise thought. Have the release of these new models impacted your timelines?
[00:48:47.280 --> 00:48:53.200]   Yeah, I definitely think that recent developments in AI have made me a bit more freaked out. And
[00:48:53.200 --> 00:48:57.600]   even since I wrote The Most Important Century series and before that, I mean, there were years
[00:48:57.600 --> 00:49:01.760]   when Open Philanthropy was very interested in AI risk, but it's become more so as we've seen the
[00:49:01.760 --> 00:49:07.040]   progress in AI. So I think what we're seeing is we're seeing these very simple systems. It's kind
[00:49:07.040 --> 00:49:11.280]   of the same type of system over and over again, very general, and is able to do a lot of different
[00:49:11.280 --> 00:49:15.920]   tasks. One of the most interesting, so I think people are interested in this. I think you should
[00:49:15.920 --> 00:49:21.680]   maybe Google GPT-3. There's a lot of compilations of what this very simple language model that,
[00:49:21.680 --> 00:49:25.280]   by the way, my wife and brother-in-law both worked on, so that's a disclosure,
[00:49:25.280 --> 00:49:29.040]   but this very simple language model that just predicts the next word it's going to see in a
[00:49:29.040 --> 00:49:33.120]   stream of text. People have gotten it to tell stories. People have gotten similar,
[00:49:33.120 --> 00:49:37.040]   though not identical, models to analyze and explain jokes. People have gotten it to play
[00:49:37.040 --> 00:49:41.440]   kind of role-playing games, write poetry, write lyrics, answer multiple choice questions, answer
[00:49:41.440 --> 00:49:47.200]   trivia questions. And one of the results that I found most kind of ridiculous and strange and
[00:49:47.200 --> 00:49:51.760]   weird was this thing called Minerva, where people took one of these language models and with very
[00:49:51.760 --> 00:49:55.840]   little intervention, very little special intervention, they got it to do these difficult
[00:49:55.840 --> 00:50:00.480]   math problems and explain its reasoning and get them right about half the time. And this is just
[00:50:00.480 --> 00:50:04.640]   not, like it wasn't really trained in a way that was very specialized for these math problems.
[00:50:04.640 --> 00:50:10.000]   And so I think that's like we just see AI systems kind of having all these unpredictable human-like
[00:50:10.000 --> 00:50:13.760]   abilities just from having this very simple training procedure, and that is something I
[00:50:13.760 --> 00:50:17.520]   find kind of wild and kind of scary. I don't know exactly where it's going or how fast.
[00:50:17.520 --> 00:50:21.520]   So if you think transformative AI might happen this century, what implications does that have
[00:50:21.520 --> 00:50:26.640]   for the traditional global health and well-being stuff that OpenPhilanthropy does? I mean,
[00:50:26.640 --> 00:50:31.040]   will that have persistent effects if AI, if it gets aligned, will create a utopia for us anyway?
[00:50:31.040 --> 00:50:34.880]   I mean, I don't know about utopia. I mean, my general take is that anything could happen.
[00:50:34.880 --> 00:50:38.080]   And I think my general take on this most important century stuff, the reason it's
[00:50:38.080 --> 00:50:42.480]   so important is because it's easy to imagine a world that is really awesome and is free from
[00:50:42.480 --> 00:50:46.480]   scarcity and that we end up in a really, you know, we see more of the progress that we've
[00:50:46.480 --> 00:50:50.640]   seen over the last 200 years and end up in a really great place. It's also easy to imagine
[00:50:50.640 --> 00:50:55.360]   like a horrible dystopia. But I do think it's, you know, my take is that the more likely you
[00:50:55.360 --> 00:50:59.600]   think all of this is, the more likely you think transformative AI is, the more you should think
[00:50:59.600 --> 00:51:03.600]   that that should be the top priority, that we should be trying to make that go well instead
[00:51:03.600 --> 00:51:07.760]   of trying to solve more direct problems at a more short term. I'm not an extremist on this.
[00:51:07.760 --> 00:51:12.000]   So, you know, Open Philanthropy does both. Open Philanthropy works on speculative,
[00:51:12.000 --> 00:51:18.000]   far off future risks. And Open Phil also does a bunch of, you know, more direct work. You know,
[00:51:18.000 --> 00:51:22.720]   again, we do direct and recommend a lot of money to give us top charities, which do things like
[00:51:22.720 --> 00:51:27.520]   distributing bed nets in Africa to help prevent malaria, treating children for intestinal
[00:51:27.520 --> 00:51:32.800]   parasites. You know, Open Philanthropy does a lot of advocacy for more money going to foreign aid
[00:51:32.800 --> 00:51:38.400]   or for better land use policies to, you know, to have a stronger economy. We do a bunch of,
[00:51:38.400 --> 00:51:43.120]   like, scientific research work that is more aimed at just like direct medical applications,
[00:51:43.120 --> 00:51:47.120]   especially in poor countries. So I support all that stuff. I'm glad we're doing it.
[00:51:47.120 --> 00:51:50.880]   And it's just a matter of, like, how real and how imminent do you think this
[00:51:50.880 --> 00:51:55.360]   transformative AI stuff is, the more real and more imminent, the more relatively of our resources
[00:51:55.360 --> 00:51:59.600]   should go into it. Yeah, no, that makes sense to me. Yeah. But I'm curious, whatever work you do
[00:51:59.600 --> 00:52:04.720]   elsewhere, does that still have persistent effects after transformative AI comes? Or do you think
[00:52:04.720 --> 00:52:08.960]   that does well wash out in comparison to the really big stuff? I mean, I think in some sense,
[00:52:08.960 --> 00:52:13.200]   the effects are permanent in that if you if you cause someone to live a healthier, better life,
[00:52:13.200 --> 00:52:18.320]   that's a thing that happened. And nothing will ever erase that life or make that life unimportant.
[00:52:18.320 --> 00:52:22.560]   But I think in terms of the effects on the future, I do expect it mostly to wash out. I expect
[00:52:22.560 --> 00:52:28.000]   mostly whatever we do to make the world better in that way. It will not persist in any kind of
[00:52:28.000 --> 00:52:32.000]   systematic, predictable way past these kind of crazy changes. And I think that's probably how
[00:52:32.000 --> 00:52:35.360]   things look pre and post industrial revolution. There's probably some exceptions, but that's my
[00:52:35.360 --> 00:52:40.880]   guess. You've expressed skepticism towards the competition frame around AI or, you know, you
[00:52:40.880 --> 00:52:45.200]   try to make capabilities go faster for the countries or companies you favor most. Yeah.
[00:52:45.200 --> 00:52:49.840]   But, you know, elsewhere, you've used the innovation as mining metaphor. And maybe you
[00:52:49.840 --> 00:52:54.640]   can explain that when you're giving the answer. But it seems like this frame should imply that
[00:52:55.600 --> 00:53:00.160]   actually the second most powerful AI company is probably right on the heels of the first most
[00:53:00.160 --> 00:53:04.880]   powerful. And then so just actually, if you think the first most powerful is going to take safety
[00:53:04.880 --> 00:53:09.680]   more seriously, you should try to boost them. How do you think about how these two different frames
[00:53:09.680 --> 00:53:14.960]   interact? I think it's common for people who become convinced that AI could be really important
[00:53:14.960 --> 00:53:19.360]   to just jump straight to, well, I want to make sure that people I trust build it first. And
[00:53:19.360 --> 00:53:24.880]   that could mean my country, that could mean my friends, people I'm investing in. And I have
[00:53:24.880 --> 00:53:29.280]   generally called that the competition frame, want to win a competition to develop AI. And I've
[00:53:29.280 --> 00:53:33.280]   contrasted it with a frame that I also think is important, which is the caution frame, which is
[00:53:33.280 --> 00:53:38.640]   that we need to all work together to be careful to not build something that spins out of control
[00:53:38.640 --> 00:53:42.560]   and has all these properties and behaves in all these ways we didn't intend. I do think we're
[00:53:42.560 --> 00:53:46.880]   likely, if we do develop these very powerful AI systems, I do think we're likely to end up in a
[00:53:46.880 --> 00:53:50.560]   world where there's multiple players trying to develop it and they're all hot on each other's
[00:53:50.560 --> 00:53:57.120]   heels. And I am very interested in ways to find ways for us all to work together to avoid disaster
[00:53:57.120 --> 00:54:01.760]   as we're doing that. And I am maybe less excited than the average person who first learns about
[00:54:01.760 --> 00:54:06.480]   this is about like picking the one I like best and helping them race ahead. Although I am someone
[00:54:06.480 --> 00:54:10.800]   interested in both. But if you take the innovation is mining metaphor, seriously, yeah, doesn't that
[00:54:10.800 --> 00:54:15.920]   imply that actually the competition is really a big factor here because. Oh, so the innovation
[00:54:15.920 --> 00:54:19.920]   mining metaphors is from another another bit of cold takes. And it's an argument I make that that
[00:54:19.920 --> 00:54:25.120]   you should think of ideas as being somewhat like natural resources in the sense of once someone
[00:54:25.120 --> 00:54:29.920]   discovers a scientific hypothesis or even once you know what someone writes a certain great symphony,
[00:54:29.920 --> 00:54:33.200]   that's something that can only be done once. That's an innovation that can only be done once.
[00:54:33.200 --> 00:54:38.160]   And so it gets harder and harder over time to have revolutionary ideas because the most
[00:54:38.160 --> 00:54:42.080]   revolutionary, easiest to find ideas have already been found. So there's an analogy to mining.
[00:54:42.080 --> 00:54:48.640]   I don't think it applies super importantly to the AI thing, because all I'm saying is that success
[00:54:48.640 --> 00:54:53.360]   by person one makes success by person two harder. I'm not saying that it has no impact or that it
[00:54:53.360 --> 00:54:57.680]   doesn't speed things up. So just to use a literal mining metaphor, let's say there's like a bunch
[00:54:57.680 --> 00:55:03.680]   of a bunch of gold in the ground. It is true that if you rush and go get all that gold, it'll be
[00:55:03.680 --> 00:55:07.760]   harder for me to now come in and find a bunch of gold. That is true. What's not true is that it
[00:55:07.760 --> 00:55:11.120]   doesn't matter if you do it. I mean, you might do it a lot faster than me. You might do it a lot
[00:55:11.120 --> 00:55:15.680]   ahead of me. I totally think fair enough. Yeah. So maybe one piece of skepticism that somebody
[00:55:15.680 --> 00:55:22.000]   could have about transformative AI is that all this is going to be bottlenecked by the non-automatable
[00:55:22.000 --> 00:55:28.000]   steps in the innovation sequence. So there won't be these feedback loops that speed up.
[00:55:28.000 --> 00:55:31.280]   Well, what is your reaction? Yeah. I think the single best
[00:55:31.280 --> 00:55:36.240]   criticism and my biggest point of skepticism on this most important century stuff is the idea that
[00:55:36.240 --> 00:55:39.520]   you could build an AI system that's very impressive, that could do pretty much everything
[00:55:39.520 --> 00:55:43.840]   humans can do, but there might be like one step that you still have to have humans do,
[00:55:43.840 --> 00:55:47.760]   and that could bottleneck everything. And then you could have the world not speed up that much
[00:55:47.760 --> 00:55:51.840]   in science and technology, not advance that fast because AIs are doing almost everything,
[00:55:51.840 --> 00:55:56.080]   but humans are still slowing down this one step, or the real world is slowing down one step. So you
[00:55:56.080 --> 00:56:00.320]   have to do, let's say, real world experiments to invent new technologies, and they just take how
[00:56:00.320 --> 00:56:05.840]   long they take. I think this is the best objection to this whole thing, and the one that I'd most
[00:56:05.840 --> 00:56:12.960]   like to look into more. But I ultimately think that there's enough reason to think that if you
[00:56:12.960 --> 00:56:19.040]   had AI systems that had human-like reasoning and analysis capabilities, I think you shouldn't count
[00:56:19.040 --> 00:56:22.800]   on this kind of bottleneck causing everything to go really slow. And a lot of that, I mean,
[00:56:22.800 --> 00:56:27.760]   I read about that in this piece called Weak Point in the Most Important Century, Full Automation.
[00:56:27.760 --> 00:56:31.920]   Part of this is just like, you don't need to automate the entire economy to get this crazy
[00:56:31.920 --> 00:56:37.440]   growth loop. You can automate just a part of it, specifically parts that have to do with very
[00:56:37.440 --> 00:56:43.200]   important tech like energy and AI itself. And those actually seem in many ways just like less
[00:56:43.200 --> 00:56:49.360]   bottlenecked than a lot of other parts of the economy. So you could be developing better AI
[00:56:49.360 --> 00:56:56.000]   algorithms and AI chips, manufacturing them mostly using robots, using those to come up with even
[00:56:56.000 --> 00:57:00.480]   better designs. And then you could also be designing more and more efficient solar panels,
[00:57:00.480 --> 00:57:05.680]   using those to collect more and more energy to power your AIs. So a lot of the crucial pieces
[00:57:05.680 --> 00:57:09.920]   here just actually don't seem all that likely to be bottlenecked. And then also, I mean,
[00:57:09.920 --> 00:57:14.400]   at the point where you have something that has the ability to have kind of like creative new
[00:57:14.400 --> 00:57:18.480]   scientific hypotheses the way a human does, which is a debate over whether we should ever expect
[00:57:18.480 --> 00:57:23.120]   that and when, but once you have that, I think you should figure there's just a lot of ways to
[00:57:23.120 --> 00:57:27.120]   get around all your other bottlenecks because you have this potentially massive population
[00:57:27.120 --> 00:57:32.560]   of thinkers looking for them. And so an example is, you know, you could, with enough firepower,
[00:57:32.560 --> 00:57:37.920]   enough energy, enough AI, enough analysis, you could probably find a way to simulate a lot of
[00:57:37.920 --> 00:57:41.920]   the experiments you need to run, for example. Gotcha. Now, it seems like the specific examples
[00:57:41.920 --> 00:57:47.040]   you used of energy and AI innovations, it seems like those are probably the hardest things to
[00:57:47.040 --> 00:57:51.520]   automate given the fact that those are the ones that humanity's only gotten around to advancing
[00:57:51.520 --> 00:57:56.400]   most recently. Can you talk through the intuition that those might be easier?
[00:57:56.400 --> 00:58:01.680]   I think some of the stuff that might be hardest to automate would just be stuff that
[00:58:01.680 --> 00:58:07.280]   in some sense doesn't have anything to do with software or capabilities. So an example of
[00:58:07.280 --> 00:58:13.920]   something that might just be like extremely hard to automate might be like trust, like, you know,
[00:58:13.920 --> 00:58:19.600]   making a business deal or providing care for someone who's sick. It might just be the like,
[00:58:19.600 --> 00:58:25.040]   even if an AI system has all the same intellectual capabilities as a human, you know, can write
[00:58:25.040 --> 00:58:29.520]   poetry just as well, can have just as many ideas, can have just as good a conversation. It just,
[00:58:29.520 --> 00:58:32.960]   it doesn't look like a human. So people don't want that. Maybe you can create a perfect
[00:58:32.960 --> 00:58:36.560]   representation of a human on a screen, but it's still on the screen. And in general,
[00:58:36.560 --> 00:58:41.120]   I see the progress in AI as being mostly on the software front, not the hardware front.
[00:58:41.120 --> 00:58:45.360]   So AIs are able to do a lot of incredible things with language, things with math,
[00:58:45.360 --> 00:58:49.760]   things with board games. I wouldn't be surprised if they could write hit music in the next decade
[00:58:49.760 --> 00:58:54.960]   or two, but they, people really are not making the same kind of progress on like robotics. Like,
[00:58:54.960 --> 00:58:59.920]   so, so weirdly a task that might be among the hardest, harder ones to automate, or especially
[00:58:59.920 --> 00:59:04.400]   if things go fast, it might be hard to automate the task of like taking this bottle of water
[00:59:04.400 --> 00:59:08.400]   and taking off the cap because I have this, like, you know, I have this hand that is just like,
[00:59:08.400 --> 00:59:12.800]   well-designed for that. Well, it's clearly not designed for that, but it's, I have these, like,
[00:59:12.800 --> 00:59:16.880]   these hands, I could do a lot of stuff and we aren't seeing the same kind of progress there.
[00:59:16.880 --> 00:59:22.080]   So I think, I think there are a lot of places where like AI systems might have kind of,
[00:59:22.800 --> 00:59:26.720]   their brains can do roughly everything human brains can, but there's some other reason they
[00:59:26.720 --> 00:59:30.800]   can't do some key economic tasks. And I think these are not the tasks I see likely to bottleneck the
[00:59:30.800 --> 00:59:35.760]   R&D as much. Gotcha. This is, this is an argument I make in one of my more obscure cold takes posts,
[00:59:35.760 --> 00:59:41.600]   but I say like, you know, AI that could actually take everyone's job, like, like every human's job
[00:59:41.600 --> 00:59:46.080]   might be like a lot harder than AI that could radically transform the galaxy via new technology.
[00:59:46.080 --> 00:59:49.280]   Cause it might be in some ways, like it might be easier to take a scientist's job than like a
[00:59:49.280 --> 00:59:53.440]   teacher's job or a doctor's job, because the teachers and the doctors are regulated and people
[00:59:53.440 --> 00:59:57.360]   might just say, I want human teachers. I don't want an AI teacher. Whereas you can sit there in
[00:59:57.360 --> 01:00:01.040]   your lab with your AI scientists and find new theories that change the world. So some of this
[01:00:01.040 --> 01:00:04.800]   stuff I think is very counterintuitive, but, but I could, I can imagine worlds where you get,
[01:00:04.800 --> 01:00:09.200]   you know, you get really wacky stuff before you get self-driving cars out on the road,
[01:00:09.200 --> 01:00:12.560]   just cause the way the regulations working. Gotcha. Okay. Let's talk about another weak
[01:00:12.560 --> 01:00:16.880]   point or the one that you identified as a weak point, lock-in. What do you think are the odds
[01:00:16.880 --> 01:00:21.520]   of lock-in given transformative AI? So lock-in is my term, or I don't know if it's my term,
[01:00:21.520 --> 01:00:26.320]   but it's a term I use to talk about the possibility that we could end up with a very
[01:00:26.320 --> 01:00:31.040]   stable civilization. And so I talk about that as it's another post it's called weak point in the
[01:00:31.040 --> 01:00:35.120]   most important century lock-in. I wrote posts about the weakest points in the series. And the
[01:00:35.120 --> 01:00:41.840]   idea is basically like throughout history so far, let's say someone becomes in charge of a government
[01:00:41.840 --> 01:00:46.480]   and they're very powerful and they're very bad. Usually this is like generally considered to be
[01:00:46.480 --> 01:00:49.600]   temporary in at least some sense, like the kind of thing that's not going to go on forever.
[01:00:49.600 --> 01:00:56.160]   There's a lot of reasons the world is just dynamic and the ways the world is tend to just like not
[01:00:56.160 --> 01:00:59.840]   stay that way completely. The world just has changed a lot throughout history. It's kind of
[01:00:59.840 --> 01:01:04.800]   a dumb thing to say, but I'll get to why, why this might be important. So, you know, if someone is
[01:01:04.800 --> 01:01:08.080]   running a country in a really cruel, corrupt way, I mean, for one thing, at some point they're going
[01:01:08.080 --> 01:01:11.680]   to get old and die and someone else is going to take over and that person will probably be
[01:01:11.680 --> 01:01:16.160]   different from them. For another thing, the world is changing all the time. There's new technologies,
[01:01:16.160 --> 01:01:20.800]   new things are possible, there's new ideas. And so, you know, the most powerful country today
[01:01:20.800 --> 01:01:24.720]   might not be the most powerful tomorrow. The people in power today might not be the ones in
[01:01:24.720 --> 01:01:30.400]   power tomorrow. And I think this gets us used to the idea that everything is temporary, everything
[01:01:30.400 --> 01:01:35.840]   changes. And a point I make in the Most Important Century series is that you can imagine a level of
[01:01:35.840 --> 01:01:41.280]   technological development where there just aren't new things to find and there isn't a lot of new
[01:01:41.280 --> 01:01:46.240]   growth to have. And people aren't dying because we've, you know, for whatever reason, you know,
[01:01:46.240 --> 01:01:49.840]   that seems like it should be medically possible for people not to age or die. And so, you could
[01:01:49.840 --> 01:01:54.720]   imagine a lot of the sources of dynamism in the world actually going away if we had enough
[01:01:54.720 --> 01:01:59.120]   technology. You could imagine a government that was able to actually surveil everyone, which is
[01:01:59.120 --> 01:02:04.480]   not something you can do now, with a dictator who actually doesn't age or die, who knows everything
[01:02:04.480 --> 01:02:08.560]   going on, who's able to respond to everything. And then you could imagine that world just being
[01:02:08.560 --> 01:02:12.960]   completely stable. I think this is a very scary thought, and it's something we have to be mindful
[01:02:12.960 --> 01:02:18.400]   of, that if the rate of technological progress speeds up a lot, we could quickly get to a world
[01:02:18.400 --> 01:02:23.440]   that doesn't have a lot more dynamism and is a lot more stable. What are the odds of this? I don't
[01:02:23.440 --> 01:02:28.400]   know. It's very hard to put a probability on it. I think when you think about, if you imagine that
[01:02:28.400 --> 01:02:32.560]   we're going to get this explosion in scientific and technological advancement, I think you have
[01:02:32.560 --> 01:02:37.680]   to take pretty seriously the idea that that could end by hitting a wall, that there could not be a
[01:02:37.680 --> 01:02:41.600]   lot of room for more dynamism, and that we could have these kind of very stable societies. What
[01:02:41.600 --> 01:02:46.400]   does "seriously" mean? I don't know. A quarter, a third, a half, something like that. I don't know.
[01:02:46.400 --> 01:02:50.880]   I'm making up numbers. I think it's serious enough to think about it, and think about it as something
[01:02:50.880 --> 01:02:56.080]   that affects the stakes of what we're talking about. Gotcha. So I'm curious if you're concerned
[01:02:56.080 --> 01:03:00.720]   about lock-in just from the perspective of locking in a negative future, or if you think that might
[01:03:00.720 --> 01:03:05.920]   intrinsically be bad to lock in any kind of future. If you could, right now, press a button and lock
[01:03:05.920 --> 01:03:11.200]   in a reasonably positive future, but that won't have any dynamism, or one where dynamism is
[01:03:11.200 --> 01:03:16.880]   guaranteed, but net expected positive is not, how would you make that determination?
[01:03:16.880 --> 01:03:21.520]   I don't think a lot about what I would do with unrealistic buttons where I have crazy amounts
[01:03:21.520 --> 01:03:27.200]   of power that I'll never have and shouldn't have. I think of lock-in by default as mostly a bad
[01:03:27.200 --> 01:03:35.680]   thing. I feel like mostly we want to at least preserve optionality and have a world where it's
[01:03:35.680 --> 01:03:41.200]   not just one person running the show with their values set up the way they want forever. I think
[01:03:41.200 --> 01:03:46.480]   of it mostly that way. I can imagine some future world where the civilization's been around long
[01:03:46.480 --> 01:03:50.000]   enough, we've learned what there is to learn, and we know what a good world looks like, and
[01:03:50.000 --> 01:03:53.040]   most people feel pretty confident about that, and they're right to feel confident, and maybe
[01:03:53.040 --> 01:03:57.680]   lock-in's not so bad, but I mostly think of lock-in as a bad thing. I also imagine that you
[01:03:57.680 --> 01:04:03.360]   could lock in some things about the world in order to avoid locking in others. So I can imagine if
[01:04:03.360 --> 01:04:08.160]   you had this enormous amount of power over how the world works, and again, this comes down. Some
[01:04:08.160 --> 01:04:12.640]   of this is more explained in my digital people series, how this could work, but if you had this
[01:04:12.640 --> 01:04:18.320]   kind of world where you completely control the environment, you might want to lock in the fact
[01:04:18.320 --> 01:04:21.680]   that you never have one person with all the power. That might be a thing you might want to lock in,
[01:04:21.680 --> 01:04:26.720]   and that prevents other kinds of lock-in. Do you worry about AI alignment as being a form
[01:04:26.720 --> 01:04:31.600]   of lock-in? So in some sense, if the goal of the research is to prevent drift from human values,
[01:04:32.160 --> 01:04:36.560]   then you might just be locking in values that aren't suboptimal.
[01:04:36.560 --> 01:04:43.040]   Yeah. I mostly think of AI alignment as just trying to avoid a really bad thing happening.
[01:04:43.040 --> 01:04:47.200]   So I mostly think of it like the thing we don't want to happen is we have some AI system. We
[01:04:47.200 --> 01:04:50.960]   thought we were designing it to help us. We're actually designing it to do something extremely
[01:04:50.960 --> 01:04:55.520]   random thing. It's just like, again, these systems work by trial and error, by encouragement,
[01:04:55.520 --> 01:04:59.600]   discouragement, or positive and negative reinforcement, and so we might have not even
[01:04:59.600 --> 01:05:05.440]   noticed that through the pattern of reinforcement we were giving, we trained some system to want to
[01:05:05.440 --> 01:05:09.840]   put as much money as possible into one bank account, or gain as much power as possible,
[01:05:09.840 --> 01:05:14.960]   or control as much energy as possible, or something like that, or just set its own reward
[01:05:14.960 --> 01:05:19.840]   number, its own score to the highest possible number. I think that would be a form of lock-in
[01:05:19.840 --> 01:05:24.560]   if we had systems more powerful than humans that had these kind of random goals. That would be like
[01:05:24.560 --> 01:05:29.760]   locking in a future that is not related to the things that humans value and care about. That's
[01:05:29.760 --> 01:05:33.760]   an example of a thing I think would be really bad. Now, if we got these systems to behave as
[01:05:33.760 --> 01:05:37.600]   intended, we still might have problems because we might have humans doing really stupid things
[01:05:37.600 --> 01:05:42.160]   and locking in really bad futures. I think that's an issue too. I feel reasonably comfortable,
[01:05:42.160 --> 01:05:47.760]   though not 100% confident, saying that we'd like to avoid that just slip-up. We'd like to avoid
[01:05:47.760 --> 01:05:51.440]   having these systems that have these random goals we gave them by accident, and they're very
[01:05:51.440 --> 01:05:57.120]   powerful, and they're better at setting up the world than we are, and so we get this world that's
[01:05:57.120 --> 01:06:01.200]   just doing this random thing that we did by accident. I think that's a thing worth avoiding.
[01:06:01.200 --> 01:06:04.640]   What is your biggest disagreement with Will McCaskill's new book on long-termism?
[01:06:04.640 --> 01:06:10.560]   I like Will's book. I think it's worth reading Will McCaskill's book about how the future could
[01:06:10.560 --> 01:06:17.680]   be very large and very important. I think he presents a picture ... In my opinion, if you
[01:06:17.680 --> 01:06:20.880]   want to talk about the long-run future and how to make the long-run future go well,
[01:06:21.360 --> 01:06:27.760]   you're starting from a place of, "By default, almost nothing I can do will actually make sense."
[01:06:27.760 --> 01:06:32.080]   I do really believe it's hard to understand the long-run future, and it's hard to make specific
[01:06:32.080 --> 01:06:38.800]   plans about it. I would say compared to Will, I am very picky about which issues I think are big
[01:06:38.800 --> 01:06:45.600]   enough and serious enough to actually pay attention to. I feel the issue of AI, AI would be transformative
[01:06:45.600 --> 01:06:49.680]   enough. It looks likely enough that it'll be soon. If it's soon, that means there might actually be
[01:06:49.680 --> 01:06:54.720]   things we can do that have predictable effects. I think this misaligned AI thing is a real threat.
[01:06:54.720 --> 01:07:00.400]   The way people design AI systems today would be really bad. I am ready to put some resources into
[01:07:00.400 --> 01:07:07.120]   preventing it, but that's crossing my threshold. Most things don't. If you make a list of ways
[01:07:07.120 --> 01:07:10.960]   to make the next million years go well, I'll look at most of them and be like, "I don't know. I
[01:07:10.960 --> 01:07:14.400]   don't really believe in this. I wouldn't really invest in this." I think Will is a bit broader
[01:07:14.400 --> 01:07:18.800]   than I am in a sense. He's interested in more things, and I am pickier than he is because I
[01:07:18.800 --> 01:07:22.880]   think it's so hard to know what's really going to affect the long-run future that I'm just looking
[01:07:22.880 --> 01:07:26.400]   for a really short list of things that are worth paying special attention to.
[01:07:26.400 --> 01:07:29.280]   Gosh, I got you. Is there a specific thing that he points out in the book that you think
[01:07:29.280 --> 01:07:33.120]   it'd be hard to grapple with? I don't remember super well. The book
[01:07:33.120 --> 01:07:39.280]   is a really broad survey of lots of stuff. I think an example I might give is he talks about
[01:07:39.280 --> 01:07:45.840]   the risk of stagnation, for example. The risk that growth might just stop or something,
[01:07:45.840 --> 01:07:51.200]   or growth might slow to very low levels. That implies that what we should be trying to do is
[01:07:51.200 --> 01:07:55.360]   make sure we continue to innovate and continue to have growth. Then there's other parts of the book
[01:07:55.360 --> 01:07:59.600]   that make it sound like we shouldn't move too fast and we shouldn't innovate too much because we
[01:07:59.600 --> 01:08:05.440]   don't want to get to our future before we've achieved some sort of civilizational maturity
[01:08:05.440 --> 01:08:09.360]   beyond what we have now to decide what we want that future to look like. We don't want to build
[01:08:09.360 --> 01:08:13.760]   these powers before we have a better idea of what to do with them. I think these are examples of
[01:08:13.760 --> 01:08:18.640]   things where I'm just like, "Gosh, I don't know." It could be good to have more growth. It could be
[01:08:18.640 --> 01:08:23.040]   good to have less growth. It could be that stagnation is a big threat. It could be that
[01:08:23.040 --> 01:08:27.200]   building powerful technologies too fast is a big threat. I just don't really know.
[01:08:27.200 --> 01:08:31.600]   I'll tell you what I'm thinking about. I'm thinking about AI because I think it's a big
[01:08:31.600 --> 01:08:35.280]   enough deal and likely enough and that we've got enough traction on some of the major risks.
[01:08:35.280 --> 01:08:39.520]   Right, right. When I look throughout history, it often seems like people who are predicting
[01:08:39.520 --> 01:08:45.040]   long-term trends are too pessimistic. In the '70s, you might have been too pessimistic about
[01:08:45.040 --> 01:08:50.400]   the ability to find more oil or feed a growing population because they couldn't have predicted
[01:08:50.400 --> 01:08:55.200]   the technological breakthroughs that might have made these things possible. Does this inform some
[01:08:55.200 --> 01:09:01.600]   sort of vague optimism about the future for you with regards to AI or no? Yeah. I think historically,
[01:09:01.600 --> 01:09:06.560]   people have been overly pessimistic about future technologies. I think by default,
[01:09:06.560 --> 01:09:11.920]   the picture with AI looks just like really scary. It just looks like it would be really easy to get
[01:09:11.920 --> 01:09:16.480]   a bad future in a lot of different ways if we just didn't move cautiously. These two considerations
[01:09:16.480 --> 01:09:22.240]   kind of balance each other out a little bit for me. I know a lot of people who believe that we're
[01:09:22.240 --> 01:09:26.960]   in just deep, deep, enormous trouble that this outcome where you get AI with its own goals,
[01:09:26.960 --> 01:09:32.320]   wiping humanity off the map is just like almost surely going to happen. I don't believe that,
[01:09:32.320 --> 01:09:36.240]   and this is part of the reason I don't believe it. It's just like I actually think the situation
[01:09:36.240 --> 01:09:41.360]   looks very challenging, very scary by default, and I think we're tending to overestimate how
[01:09:41.360 --> 01:09:45.680]   bad and how dire things are, and so they balance out a little bit for me. Okay, gotcha. But in many
[01:09:45.680 --> 01:09:51.200]   of these cases, it seems like it would be impossible to see the positive scenario come about.
[01:09:51.200 --> 01:09:56.320]   For example, if you were forecasting population in the '70s, is there some reasonable method by
[01:09:56.320 --> 01:09:58.960]   which you would have predicted that this is not going to lead to some massive famine that kills
[01:09:58.960 --> 01:10:04.000]   a billion people? Or would that have been your focus in the '70s if the open fill was a thing
[01:10:04.000 --> 01:10:10.320]   back then? I think it's really hard to know how knowable the future was in the past and what that
[01:10:10.320 --> 01:10:16.160]   means for today. I do think that when you look back, and again, this is something I've looked
[01:10:16.160 --> 01:10:19.760]   at a fair amount, when you look back at people trying to predict the future in the past,
[01:10:19.760 --> 01:10:24.640]   it just looks deeply unserious. You could say that future people will say the same about us.
[01:10:24.640 --> 01:10:28.640]   I'm sure they'll think we look less serious than they are, but I think there's a difference. I
[01:10:28.640 --> 01:10:35.920]   really do think that attempts to rigorously make predictions about the future, there haven't been
[01:10:35.920 --> 01:10:41.120]   a lot of them historically. I don't think it's obvious that people were doing the best they can
[01:10:41.120 --> 01:10:48.160]   and that we can't do better today. This population is an example. It doesn't seem necessarily true to
[01:10:48.160 --> 01:10:54.560]   me that you couldn't have been at that time and said, "Gosh, population has been going up for a
[01:10:54.560 --> 01:10:59.520]   while now and people keep inventing new ways to come up with more resources. Maybe that'll keep
[01:10:59.520 --> 01:11:02.720]   happening." I'm just really not convinced you couldn't have said that. I'm definitely not
[01:11:02.720 --> 01:11:11.520]   convinced no one did say it. I think some people did say it. I'm hesitant to get too defeatist
[01:11:11.520 --> 01:11:15.600]   just from the fact that some people were wrong about the future in the past. I think it's hard
[01:11:15.600 --> 01:11:18.960]   to know if there was really no way to know or if they just weren't trying very hard.
[01:11:18.960 --> 01:11:24.720]   Okay. Well, one thing you just said a minute ago was that we are better at making predictions
[01:11:24.720 --> 01:11:28.160]   than people in the past were. That alone should make us more optimistic about our ability to
[01:11:28.160 --> 01:11:30.800]   predict the future. That's just a guess. I do think
[01:11:30.800 --> 01:11:36.960]   that there's been a lot of progress. This is what society is. We have a lot of progress on all kinds
[01:11:36.960 --> 01:11:40.880]   of intellectual fronts. I think there has been a lot of progress on what it looks like to make
[01:11:40.880 --> 01:11:44.400]   good reasonable predictions about the future. I think that's something that's happened.
[01:11:45.520 --> 01:11:49.840]   I think we should be expecting ourselves to do a bit better than people did in the past,
[01:11:49.840 --> 01:11:51.840]   and future people will probably do better than we do.
[01:11:51.840 --> 01:11:59.120]   Right. I kind of wonder when I look at a report like Biological Anchors, whether Asimov's just
[01:11:59.120 --> 01:12:03.680]   shooting the shit about screens and what you'll be able to do with them, whether he might have had
[01:12:03.680 --> 01:12:08.800]   less sources of error than this eight-step methodology where you might not even be aware
[01:12:08.800 --> 01:12:12.240]   of there's a ninth or tenth missing step that might make the whole thing invalid, and where
[01:12:12.240 --> 01:12:17.120]   many of the inputs have multiple orders of magnitude, wide confidence intervals. What do
[01:12:17.120 --> 01:12:22.640]   you think of that general skepticism? Biological Anchors, I think, is a very
[01:12:22.640 --> 01:12:28.960]   important input into my thinking. It's not the only input. I think my views on AI timelines are
[01:12:28.960 --> 01:12:34.320]   kind of a mix of just kind of A, looking at AI systems today, looking at what they did 50 years
[01:12:34.320 --> 01:12:38.560]   ago, looking at what they did 10 years ago, and just kind of being like, "Well, gosh, it sure
[01:12:38.560 --> 01:12:42.560]   looks plausible that these will be able to do all the things humans can do to advance science and
[01:12:42.560 --> 01:12:46.640]   technology pretty soon." That's one put into my thinking. Another input into my thinking
[01:12:46.640 --> 01:12:51.120]   is what we call the semi-informative priors analysis, which is, you know, it's a complex
[01:12:51.120 --> 01:12:55.200]   report because it looks from a lot of different angles, but I think you can summarize the
[01:12:55.200 --> 01:13:00.640]   highlights of the report as just saying most of the effort that has ever in the history of humanity
[01:13:00.640 --> 01:13:06.560]   gone into making AI will be in this century, ever, because the field of AI is not very old,
[01:13:07.200 --> 01:13:11.360]   and the economy and the amount of effort invested have gone up dramatically. I think that's a data
[01:13:11.360 --> 01:13:16.240]   point in favor of not being too skeptical that we could be on the cusp of transformative AI.
[01:13:16.240 --> 01:13:21.360]   In some sense, the world has not been trying very hard for very long. That's an input. Another input
[01:13:21.360 --> 01:13:26.160]   is just like expert surveys. When people just ask AI researchers when they think AI will be able to
[01:13:26.160 --> 01:13:30.240]   do everything humans can, they tend to come out saying it's a few decades. Now, that can be biased
[01:13:30.240 --> 01:13:34.160]   and unreliable in all kinds of ways. All these things have their problems. That's a data point.
[01:13:34.160 --> 01:13:38.880]   Then there's biological anchors. Biological anchors is a report that kind of, you know,
[01:13:38.880 --> 01:13:44.560]   I would summarize it to summarize it on a podcast. It's a very complex report. There's a lot of
[01:13:44.560 --> 01:13:48.480]   different angles it uses. There's a lot of different questions it asks. There's a lot of
[01:13:48.480 --> 01:13:53.600]   different numbers, but I think you can boil it down to some fairly simple observations.
[01:13:53.600 --> 01:13:59.200]   You can say that in some important sense, which could be debated and analyzed, but it seems true
[01:13:59.200 --> 01:14:04.240]   most ways you look at it. In some important sense, we've never built AI systems before
[01:14:04.240 --> 01:14:10.320]   that do as much computation per second as like a human brain does. It shouldn't be surprising
[01:14:10.320 --> 01:14:13.680]   that we don't have AI systems that can do everything humans do, because humans are
[01:14:13.680 --> 01:14:18.240]   actually doing more work in their brains than a normal AI system is doing. However,
[01:14:18.240 --> 01:14:24.880]   it also looks like within this century, we probably will have AI systems that are that big.
[01:14:24.880 --> 01:14:29.680]   If we estimate how much it would take to train them, how much it would cost to build them,
[01:14:29.680 --> 01:14:33.280]   that looks like it will probably be affordable this century. Then you can just talk about all
[01:14:33.280 --> 01:14:36.640]   the different ways you could define this, and all the different ways you could quantify that,
[01:14:36.640 --> 01:14:39.360]   and all the different assumptions you could put in, but my bottom line is like
[01:14:39.360 --> 01:14:44.480]   almost any way you slice it, it looks pretty ... However you want to think about it,
[01:14:44.480 --> 01:14:48.080]   however you want to define what it means for an AI brain to be as big as a human's,
[01:14:48.080 --> 01:14:53.200]   and what it would mean to get that brain sort of trained, most angles on it suggest that it looks
[01:14:53.200 --> 01:14:56.240]   reasonably likely it'll happen this century. That's a data point for me. That matters.
[01:14:56.240 --> 01:15:01.360]   All those are data points feeding into my view. I'm stealing this from Eliezer who asked on
[01:15:01.360 --> 01:15:07.760]   Twitter, "Has there literally ever in the entire history of AI been any case of anybody successfully
[01:15:07.760 --> 01:15:13.760]   calling the development timing of literally any novel AI capability," he has very complex sentences,
[01:15:13.760 --> 01:15:21.360]   "using a bio-anchored or bio-inspired calculation?" I saw some discussion of this on his Facebook,
[01:15:21.360 --> 01:15:26.560]   and I think the answer might be yes, but I mostly want to attack the premise. I mostly just want to
[01:15:26.560 --> 01:15:32.880]   say there haven't been a lot of cases of people predicting AI milestones with great precision,
[01:15:32.880 --> 01:15:37.440]   and that's also not what I'm trying to do. A lot of what I'm trying to say is I'm trying to say,
[01:15:37.440 --> 01:15:41.440]   "Gosh, this century, it looks like more likely than not that we'll get something hugely
[01:15:41.440 --> 01:15:49.440]   transformative." He's asking about some history of AI that's a few decades old, and there haven't
[01:15:49.440 --> 01:15:52.800]   even been a lot of people trying to make predictions. A lot of the predictions have
[01:15:52.800 --> 01:15:56.880]   been way more narrow and specific than that. I'm mostly just like, "This isn't a very important or
[01:15:56.880 --> 01:16:02.640]   informative question." I think all the work he's doing, has there ever been an example of someone
[01:16:02.640 --> 01:16:06.400]   using the kind of reasoning Eliezer is using to predict the end of the world or something,
[01:16:06.400 --> 01:16:11.360]   which is what he's predicting? I mostly just want to challenge the premise and just say, "Look,
[01:16:11.360 --> 01:16:16.720]   we're not working with a lot of sample size here. It's not like this is some big, well-developed
[01:16:16.720 --> 01:16:20.720]   field where people have tried to do the exact same thing I'm trying to do 25 times and failed
[01:16:20.720 --> 01:16:28.160]   each time." This is like mostly people in academia try to advance AI systems. They don't try to
[01:16:28.160 --> 01:16:32.400]   predict when AI systems can do much. Do what? We're not working with a lot here. We have to do
[01:16:32.400 --> 01:16:35.840]   the best we can, make our best guess, use our common sense, use our judgment, use the angles
[01:16:35.840 --> 01:16:41.120]   we've got. That's my main answer. Another answer I would give is I would just say that, I don't know,
[01:16:42.880 --> 01:16:52.080]   Hans Moravec was the original biological anchors person. I think he predicted artificial general
[01:16:52.080 --> 01:16:57.840]   intelligence around 2020 or something. On Eliezer's own views, it's going to look like
[01:16:57.840 --> 01:17:05.520]   he was unbelievably close. Eliezer believes we'll see it by 2030, and I think that's plausible.
[01:17:05.520 --> 01:17:08.880]   If that's what you believe, then it's going to be, I don't know, I think we'll look back on that as
[01:17:08.880 --> 01:17:13.600]   the greatest technology prediction ever by a lot. I think the answer is maybe. There was also some
[01:17:13.600 --> 01:17:19.440]   discussion in the comments about whether Moravec called big progress on AI is doing well at vision
[01:17:19.440 --> 01:17:22.880]   by examining the retina. There was some debate about that. I think it's all very muddy. I don't
[01:17:22.880 --> 01:17:27.200]   think this is much of a knockdown argument against thinking about biological anchors. It is only one
[01:17:27.200 --> 01:17:33.680]   input into my thinking. I do think it looks good for biological anchors that we have seen this deep
[01:17:33.680 --> 01:17:39.680]   learning revolution, and we have seen these brute force AI training methods working really well when
[01:17:39.680 --> 01:17:45.360]   they didn't used to work well. It is when AI systems started to be about the size of insect or
[01:17:45.360 --> 01:17:49.680]   small animal brains, or within range, within a few orders of magnitude of human brains.
[01:17:49.680 --> 01:17:53.760]   You could call that a wild coincidence, but it doesn't feel ... Probably these numbers are all
[01:17:53.760 --> 01:17:58.800]   off. They're probably all off by 10x, 100x, 1,000x. We're talking about very important things and
[01:17:58.800 --> 01:18:02.960]   trying to get our best handle and our best guess. I think biological anchors looks fine so far. I
[01:18:02.960 --> 01:18:07.520]   don't know. It doesn't look amazing. It looks fine. Now, as I'm sure many people have proposed
[01:18:07.520 --> 01:18:12.000]   that increasing progress in science, technology, and increasing economic growth are the most
[01:18:12.000 --> 01:18:17.760]   compelling things to be doing instead of working on transformative AI. I just want to get your broad
[01:18:17.760 --> 01:18:23.360]   reaction to that first. Sure. I think we're talking about maybe
[01:18:23.360 --> 01:18:28.240]   the progress studies crowd. I wrote a piece about this on Cold Taste called Rowing, Steering,
[01:18:28.240 --> 01:18:32.720]   Anchoring, Equity, and Mutiny, where I discussed these different ways of thinking about what it
[01:18:32.720 --> 01:18:38.000]   means to make the world good. I do have some sympathy for the idea that a lot of the way the
[01:18:38.000 --> 01:18:41.680]   world has gotten better over the last couple hundred years is just we've gotten richer,
[01:18:41.680 --> 01:18:45.600]   we've had more technological capabilities. Maybe we should try and do more of that.
[01:18:45.600 --> 01:18:49.040]   I don't think this is a nutty thing to think. I think this is somewhat reasonable,
[01:18:49.040 --> 01:18:56.320]   but I overall feel that even a couple hundred years is not that big a percentage of the history
[01:18:56.320 --> 01:19:02.240]   of humanity. I wrote a series called Has Life Gotten Better that asks what the whole graph
[01:19:02.240 --> 01:19:06.800]   of quality of life looks like over the course of humanity. There is precedent for technological
[01:19:06.800 --> 01:19:09.840]   development seeming to make things worse. That's what it looks like happened in the
[01:19:09.840 --> 01:19:15.440]   agricultural revolution. I have some sympathy for saying, "Hey, this thing has been good for 200
[01:19:15.440 --> 01:19:20.080]   years. Let's do more of it." I don't think it's the tightest, most conclusive argument in the world.
[01:19:20.080 --> 01:19:25.840]   I think we do have some specific reasons to believe that developing some particular new
[01:19:25.840 --> 01:19:31.680]   technologies, not only AI, but also potentially bioweapons, could just be so catastrophically
[01:19:31.680 --> 01:19:36.400]   dangerous. I think it's something like where you could think of us, I think Toby Ord uses
[01:19:36.400 --> 01:19:41.840]   the analogy of humanity being like a child who's becoming an adolescent. It's great to become
[01:19:41.840 --> 01:19:46.960]   stronger up to a certain point. That's fun. That feels good. Then at a certain point, you're strong
[01:19:46.960 --> 01:19:50.240]   enough to really hurt yourself and really get yourself in trouble, or maybe strong enough that
[01:19:50.240 --> 01:19:54.000]   you don't know your own strength. I think there's a pretty good case that humanity is reaching that
[01:19:54.000 --> 01:19:58.240]   point. I think we're reaching the point where we could have a nuclear war or a bioweapon or AI
[01:19:58.240 --> 01:20:03.520]   systems that really change the world forever. It might have made sense 300 years ago when we were
[01:20:03.520 --> 01:20:07.440]   all struggling to feed ourselves to say, "Hey, we want more power. We want more technology. We want
[01:20:07.440 --> 01:20:11.600]   more abilities." Today, I think we're starting to enter the gray area. We're starting to enter the
[01:20:11.600 --> 01:20:15.760]   gray zone, or maybe we should slow down a little bit, be a little bit more careful. I'm not saying
[01:20:15.760 --> 01:20:20.800]   to literally slow down, but I'm talking about priorities. I would rather look at what I think
[01:20:20.800 --> 01:20:26.160]   are dramatically neglected issues that might affect all of humanity's future and at least do
[01:20:26.160 --> 01:20:30.880]   the best we can to have a handle on what we want to do about them than put my effort into throwing
[01:20:30.880 --> 01:20:34.880]   more juice and more gas behind this ongoing technological progress, which I think is a good
[01:20:34.880 --> 01:20:39.360]   thing. It's just a matter of priority. Okay. Do you think that the entire vision
[01:20:39.360 --> 01:20:43.200]   of increasing progress is doomed if ideas get harder to find?
[01:20:43.200 --> 01:20:50.160]   I do think there's eventually limits to how much. I think that the rate, like I've said,
[01:20:50.160 --> 01:20:55.760]   the atoms of the galaxy argument, but I think just a broader common sense take would be that
[01:20:55.760 --> 01:20:59.920]   the world over the last couple of hundred years has changed incredibly dramatically.
[01:20:59.920 --> 01:21:06.400]   We've had just new exciting technologies and capabilities every year. I think just a good
[01:21:06.400 --> 01:21:11.360]   guess would be that that hits a wall at some point. It might be the atoms of the galaxy.
[01:21:11.360 --> 01:21:16.800]   It might just be something much more boring. It might just be a gradually slowing. Actually,
[01:21:16.800 --> 01:21:21.280]   what we seem to observe when we look at the numbers is that we are seeing a bit of stagnation,
[01:21:21.280 --> 01:21:27.760]   a bit of slowing down, and probably will keep slowing down by default. I think probably a good
[01:21:27.760 --> 01:21:32.000]   guess is that the world is changing at an incredible pace that has not been the case for
[01:21:32.000 --> 01:21:34.640]   most of history, and it probably won't be the case for the whole future.
[01:21:34.640 --> 01:21:37.920]   Okay, gotcha. I guess there's several reactions somebody could have to the idea that ideas are
[01:21:37.920 --> 01:21:42.080]   getting harder to find, and therefore that this makes progress studies less relevant. One is just
[01:21:42.080 --> 01:21:46.960]   if you look at your own blog. The entire thing is just you complaining about all this low-hanging
[01:21:46.960 --> 01:21:51.120]   fruit that people are not plucking. Nobody's thinking seriously about transformative AI.
[01:21:51.120 --> 01:21:54.640]   Nobody's thinking seriously about utopia. Nobody's thinking seriously about eco-governance.
[01:21:54.640 --> 01:21:54.880]   Sure.
[01:21:54.880 --> 01:21:57.920]   How do you square this with the general thing about ideas getting harder to find?
[01:21:57.920 --> 01:22:04.800]   Oh, I think there's just a ton of really important stuff today that not enough people are paying
[01:22:04.800 --> 01:22:10.880]   attention to. That was also true 50 years ago. That was also true 100 years ago. It was probably
[01:22:10.880 --> 01:22:14.720]   more true 50 years ago than it is today. It was probably more true 100 years ago than 50 years
[01:22:14.720 --> 01:22:19.360]   ago. It's gradually the supply of amazingly important ideas that are not getting any
[01:22:19.360 --> 01:22:23.200]   attention. It's probably getting harder to do that. Harder doesn't mean impossible.
[01:22:23.200 --> 01:22:28.720]   I do actually think that if people want to do something that's really new and world-changing
[01:22:28.720 --> 01:22:34.960]   and dramatic and revolutionary, I think maybe the worst way to do that is to go into some
[01:22:34.960 --> 01:22:39.200]   well-established scientific field and try to revolutionize that. I think it's better to just
[01:22:39.200 --> 01:22:43.760]   use your common sense and ask an important question about the world that no one's working
[01:22:43.760 --> 01:22:47.440]   on because it isn't a scientific field, because it isn't a field of academia, because it doesn't
[01:22:47.440 --> 01:22:51.440]   have institutions and work on that. A lot of my blog does advocate that. For example,
[01:22:51.440 --> 01:22:57.440]   AI itself is a very well-established field, but AI alignment is a weird field that doesn't really
[01:22:57.440 --> 01:23:01.600]   have academic departments right now. A lot of the stuff I'm talking about, trying to predict
[01:23:01.600 --> 01:23:05.680]   what the future is going to look like, it's kind of a weird, low-prestige thing that you can't
[01:23:05.680 --> 01:23:09.440]   easily explain to your extended family. I do think that's probably the best place to look
[01:23:09.440 --> 01:23:14.400]   if you want to do something that's going to be super-significant, super-revolutionary. That is
[01:23:14.400 --> 01:23:18.400]   why I've kind of professionally been drawn to it, looking for potential big wins that philanthropy
[01:23:18.400 --> 01:23:22.640]   could get. But isn't another way to just read what you said, which is that we shouldn't follow in
[01:23:22.640 --> 01:23:27.440]   the footsteps of the greats to have great achievements yourself, isn't another way to
[01:23:27.440 --> 01:23:31.120]   think about that, that you should probably also ignore the advice, the object-level advice you're
[01:23:31.120 --> 01:23:36.080]   giving or 80,000 hours it gives, because those specific things, that's not what's going to make
[01:23:36.080 --> 01:23:41.440]   you the next Einstein? I mean, a fair number of your questions, or a fair number of dialogue I
[01:23:41.440 --> 01:23:46.560]   see in some of the skeptical of futurism world, it feels to me like it's almost just getting
[01:23:46.560 --> 01:23:52.240]   unnecessarily fancy or something. I kind of just want to say, who's someone who really
[01:23:52.240 --> 01:23:56.160]   revolutionized the way the world thinks about stuff? I don't know, like Darwin. Now, what was
[01:23:56.160 --> 01:24:02.000]   Darwin doing? Was Darwin saying, "Well, I really don't want to think about this thing, because I
[01:24:02.000 --> 01:24:06.080]   don't believe humans are capable of thinking about that thing. I don't want to think about
[01:24:06.080 --> 01:24:10.960]   this topic, because I think it's too hard to know the future," and blah, blah, blah. Was he doing
[01:24:10.960 --> 01:24:19.040]   all that stuff, or was he just asking an interesting question? Was he just kind of saying,
[01:24:19.040 --> 01:24:24.560]   "Hey, this thing seems important. I'm going to use my common sense and judgment to figure out
[01:24:24.560 --> 01:24:27.680]   how it works, and I'm going to write about it." And I think some of this stuff gets too fancy.
[01:24:27.680 --> 01:24:32.080]   So, I think today what's going on is if I just look at the world and I say,
[01:24:32.080 --> 01:24:35.680]   "What are the most important things that could matter for the world being a good or bad place
[01:24:35.680 --> 01:24:39.120]   in the future?" I've looked at a lot of possibilities. I think that AI is one of
[01:24:39.120 --> 01:24:42.640]   the leading examples, and I don't see a lot of people paying attention to it. So, that's what
[01:24:42.640 --> 01:24:48.080]   I want to work on. I think a lot of the people who have done revolutionary work that we now look back
[01:24:48.080 --> 01:24:51.760]   on and a lot of people try to imitate, they weren't really doing all this stuff. They weren't trying
[01:24:51.760 --> 01:24:55.920]   to imitate the stuff that usually works and stay away from the stuff that wasn't. They were just
[01:24:55.920 --> 01:25:00.080]   asking interesting, important questions and working on them. As far as myself and 80,000
[01:25:00.080 --> 01:25:05.520]   Hours, I guess I just don't feel that we're well-known enough or influential enough that our
[01:25:05.520 --> 01:25:10.480]   advice, that stuff we're interested in, is automatically, therefore, not neglected. I think
[01:25:10.480 --> 01:25:13.440]   the stuff we're talking about is very neglected. But if you find something that's even more
[01:25:13.440 --> 01:25:17.760]   neglected and more important, more power to you. Gotcha. Let's say the total amount of money given
[01:25:17.760 --> 01:25:23.280]   to EA just increased by an order of magnitude or something. What could be possible at that point
[01:25:23.280 --> 01:25:29.280]   that's not possible now? I don't know. I think even then, I think the amount of money we'd be
[01:25:29.280 --> 01:25:33.360]   working with would be really small by the standards of any kind of government budget.
[01:25:33.360 --> 01:25:36.400]   In general, with philanthropy, I'm always looking for things where it's like, "Can we
[01:25:36.400 --> 01:25:44.080]   seed the creation of a field? Can we fund people to introduce new ideas?" But still, we're very
[01:25:44.080 --> 01:25:47.520]   small compared to the overall economy and the overall government. I think even multiplying
[01:25:47.520 --> 01:25:51.520]   everything by 10, that would still be true. I'm not sure exactly what we'd do with 10x as much
[01:25:51.520 --> 01:25:53.920]   money. I'm not even sure what we're going to do with the money that already exists.
[01:25:53.920 --> 01:25:58.800]   But do you think there will be more EA billionaires in the coming future?
[01:25:58.800 --> 01:26:02.080]   I would guess so. Does that imply that you should be spending
[01:26:02.080 --> 01:26:06.400]   money faster now? Yeah, we're trying to model it. In theory,
[01:26:06.400 --> 01:26:10.160]   we have all these models that say, "Here's our guess at how much money is eventually going to
[01:26:10.160 --> 01:26:14.240]   be available, and here's our guess at how many giving opportunities will eventually be there to
[01:26:14.240 --> 01:26:20.240]   fund. This is our guess at what's good enough to fund and what's not." That's a very tentative
[01:26:20.240 --> 01:26:25.360]   guess, and a lot of it is just really, really, really imprecise stuff, but we have to have some
[01:26:25.360 --> 01:26:32.560]   view on it. Anyone who's spending money does. I do tend to assume that Sam Baikman-Fried and
[01:26:32.560 --> 01:26:37.440]   Dustin Moskovitz and Kerry Tuna are not the last billionaires who are interested in doing as much
[01:26:37.440 --> 01:26:43.840]   good as possible, but it is really hard to model this stuff. Frankly, a lot of it, we have various
[01:26:43.840 --> 01:26:47.520]   rough models we've made over the years. We also just will sometimes use our intuition and just
[01:26:47.520 --> 01:26:52.160]   say, "We fund the stuff that seems quite good and exciting, and we don't fund stuff that doesn't,"
[01:26:52.160 --> 01:26:57.280]   and that's an input into our thinking, too. I got you. Then how do you think about the risk
[01:26:57.280 --> 01:27:00.800]   that some of your giving might have negative impacts? People have brought this up in the
[01:27:00.800 --> 01:27:07.040]   context of your $30 million investment in open AI, but in all sorts of contexts, especially when
[01:27:07.040 --> 01:27:10.640]   you're talking about political advocacy, people might think that the thing you do has negative
[01:27:10.640 --> 01:27:15.920]   side effects that counteract the positive effects. Is it just a straight EV calculation, or how do
[01:27:15.920 --> 01:27:20.640]   you think about this? I think in theory, what we want is we want to make grants that have more
[01:27:20.640 --> 01:27:26.480]   upside than downside or have expected net positive effects. I think we tend to be just, in a common
[01:27:26.480 --> 01:27:31.600]   sense way, a little bit conservative with the negative effects in that the thing we don't want
[01:27:31.600 --> 01:27:37.280]   to do is we don't want to enter some field on a theory that's just totally messed up and wrong in
[01:27:37.280 --> 01:27:40.480]   a way that we could have known if we'd just done a little bit more homework. I think that there's
[01:27:40.480 --> 01:27:47.200]   just something irresponsible and uncooperative about that. In general, when we are making big
[01:27:47.200 --> 01:27:53.680]   decisions, like big dollar decisions or going into a new cause, we often will try really hard to do
[01:27:53.680 --> 01:27:58.560]   everything we can to understand the downsides. If after we've done roughly everything we can,
[01:27:58.560 --> 01:28:03.440]   up to some reasonable diminishing returns, and once we've done that, if we believe, as far as
[01:28:03.440 --> 01:28:07.280]   we can tell, the upsides outweigh the downsides, we're generally going to go for it. Our goal is
[01:28:07.280 --> 01:28:14.720]   not to avoid harm at all costs, although our goal is to operate in a cooperative, high-integrity way,
[01:28:14.720 --> 01:28:19.120]   always doing our best, always trying to anticipate the downsides, but recognize we're going to have
[01:28:19.120 --> 01:28:24.000]   unintended side effects sometimes, and that's life, and anything you do has unintended side
[01:28:24.000 --> 01:28:28.240]   effects. I don't agree with the specific example you gave as an example of something that was net
[01:28:28.240 --> 01:28:30.240]   negative, but I don't know. Are you talking about open AI?
[01:28:30.240 --> 01:28:32.320]   Yes. Okay. Many people
[01:28:32.320 --> 01:28:35.520]   on Twitter reminded me to ask if you were investing in open AI.
[01:28:35.520 --> 01:28:40.640]   I mean, you can look up our $30 million grant to open AI. I think it was back in 2016,
[01:28:40.640 --> 01:28:45.600]   and we wrote about some of the thinking behind it. We wrote that part of that grant was getting a
[01:28:45.600 --> 01:28:50.400]   board seat for open philanthropy for a few years, that we could help with their governance at a
[01:28:50.400 --> 01:28:58.800]   crucial early time in their development. I think some people believe that open AI has been net
[01:28:58.800 --> 01:29:03.760]   negative for the world because of the fact that they have contributed a lot to AI advancing and
[01:29:03.760 --> 01:29:10.000]   to AI being hyped, and they think that gives us less time to prepare for it. I do think that all
[01:29:10.000 --> 01:29:14.240]   else equal, AI advancing faster gives us less time to prepare and is a bad thing, but I don't think
[01:29:14.240 --> 01:29:18.560]   it's the only consideration. I think open AI has done a number of good things too, has set some
[01:29:18.560 --> 01:29:23.120]   important precedents. I think it's probably much more interested in a lot of the issues I'm talking
[01:29:23.120 --> 01:29:28.160]   about and risks from advanced AI than the company that I would guess would exist if they didn't,
[01:29:28.160 --> 01:29:34.240]   that would be doing similar things. I don't really accept that even the idea that open AI is a
[01:29:34.240 --> 01:29:38.720]   negative force. I think it's highly debatable. We could talk about it all day. Then if you look at
[01:29:38.720 --> 01:29:44.240]   our specific grant, it's just that's even a completely different thing because a lot of that
[01:29:44.240 --> 01:29:49.120]   was not just about boosting them, but was about getting to be part of their early decision-making,
[01:29:49.120 --> 01:29:54.800]   and I think that was something that there were benefits and was important. My overall view is
[01:29:54.800 --> 01:29:58.960]   that I look back on that grant as one of the better grants we've made, not one of the worst ones,
[01:29:58.960 --> 01:30:04.640]   but certainly we've done a lot of things that have not worked out. I think there are some times
[01:30:04.640 --> 01:30:09.840]   surely when we've done things that have consequences we didn't intend and no philanthropist
[01:30:09.840 --> 01:30:14.720]   can be free of that. What we can try and do is be responsible, seriously do our homework to try to
[01:30:14.720 --> 01:30:18.640]   understand things beforehand, see the risks that we're able to see and think about how to minimize
[01:30:18.640 --> 01:30:23.040]   them. Let's talk about ethics. I think you have a very interesting series of blog posts about
[01:30:23.040 --> 01:30:29.600]   future-proof ethics. Do you want to explain what this is first? Sure. I wrote a short blog post
[01:30:29.600 --> 01:30:34.800]   series just trying to explain some of the philosophical views and ethical views that
[01:30:34.800 --> 01:30:39.760]   are common among people who call themselves effective altruists. One of the ideas I appealed
[01:30:39.760 --> 01:30:44.240]   to is what I think, and I'm not sure I'm getting this right, but what I think a lot of people I
[01:30:44.240 --> 01:30:49.440]   know are trying to do is they're trying to come up with a system of morality and a system of ethics
[01:30:49.440 --> 01:30:56.400]   that would survive a lot of moral progress or something, that if they later became a lot wiser
[01:30:56.400 --> 01:31:00.160]   and learned a lot more and reflected on their morality, they wouldn't look back on their earlier
[01:31:00.160 --> 01:31:04.400]   actions and think they were doing horrible, monstrous mistakes. A lot of history has just
[01:31:04.400 --> 01:31:07.680]   like people doing things they thought were fine and right at the time, and now we look back and
[01:31:07.680 --> 01:31:12.640]   we're horrified. You could think of yourself as like, "What morality can I have that would make
[01:31:12.640 --> 01:31:18.800]   it not so likely that if there is a bunch more moral progress and if people do learn a lot more,
[01:31:18.800 --> 01:31:22.640]   that the future won't look back on me and be horrified at what I did?" I wrote a bit of a
[01:31:22.640 --> 01:31:26.720]   series about what it might look like to try to do that, laid out a few principles of it,
[01:31:26.720 --> 01:31:32.480]   and tried to use this to explain the moral systems a lot of effective altruists tend to use,
[01:31:32.480 --> 01:31:38.480]   which tends to be some flavor of utilitarianism, is often very expansive about whose rights count.
[01:31:38.480 --> 01:31:42.160]   Effective altruists are very interested in future generations that don't exist yet.
[01:31:42.160 --> 01:31:46.240]   They're interested in animals being mistreated on factory farms. They're interested in various
[01:31:46.240 --> 01:31:50.560]   populations that a lot of people don't care about today, but that there are large numbers of.
[01:31:50.560 --> 01:31:54.960]   I tried to explain that. A thing that's important is I laid this view out partly so I could argue
[01:31:54.960 --> 01:31:58.800]   against it later, and I haven't done the latter yet. I have a lot of reservations too about the
[01:31:58.800 --> 01:32:03.040]   ethical systems that are common with effective altruists. Let's talk about some of the pillars
[01:32:03.040 --> 01:32:07.440]   you laid out in this piece. Sentientism, that seems pretty reasonable to me.
[01:32:07.440 --> 01:32:15.360]   Sentientism is the idea that ... Let me back up. There's three principles that I roughly
[01:32:15.360 --> 01:32:19.840]   outlined that you might want for a morality that is going to stand up to scrutiny, or that you won't
[01:32:19.840 --> 01:32:23.280]   be so likely to change your mind about if you learn more and get better.
[01:32:23.280 --> 01:32:28.640]   One principle is just systemization. It's better to have morality based on simple general principles
[01:32:28.640 --> 01:32:33.120]   that you apply everywhere that have a morality that's just always you just deciding what feels
[01:32:33.120 --> 01:32:37.360]   right in the moment, because the latter could be subject to a lot of the biases of your time,
[01:32:37.360 --> 01:32:42.880]   and the former lets you stress test the core ideas. Then two of the core ideas I propose are
[01:32:42.880 --> 01:32:46.960]   what I call thin utilitarianism, which is basically the greatest good for the greatest number,
[01:32:46.960 --> 01:32:52.640]   and sentientism, which is basically saying that someone counts or someone matters if they're able
[01:32:52.640 --> 01:32:58.160]   to suffer or have pleasure. I think you just said sentientism seems reasonable to you. I think
[01:32:58.160 --> 01:33:03.760]   sentientism might be the weakest part of the picture to me. I think if you have a morality
[01:33:03.760 --> 01:33:08.640]   where you are insistent on saying that everyone counts equally in proportion to the amount of
[01:33:08.640 --> 01:33:13.360]   pain or pleasure they're able to have, you run into a lot of weird dilemmas that you wouldn't
[01:33:13.360 --> 01:33:17.360]   have to run into if you didn't have that view. I think it's very strange, but I think it is
[01:33:17.360 --> 01:33:22.880]   actually one of the more questionable parts of the view. It's saying when I'm deciding whether
[01:33:22.880 --> 01:33:27.280]   I care about someone, it doesn't matter at all if they're way in the future, if they're way far
[01:33:27.280 --> 01:33:30.640]   away, if they're totally different from me, if they're not human, if I've never met them,
[01:33:30.640 --> 01:33:34.960]   all that matters is if they can have pain or pleasure. I think it sounds great, and I completely
[01:33:34.960 --> 01:33:38.800]   get why someone listening would be like, "How could you ever disagree with that?" But I do think
[01:33:38.800 --> 01:33:42.480]   there's various challenges with it, which I have not had the chance to write about yet, and I doubt
[01:33:42.480 --> 01:33:46.880]   I can be very convincing on this podcast yet because I haven't thought about it. All right.
[01:33:46.880 --> 01:33:52.560]   Sounds good. Let's talk about the other two. Systemization, doesn't the fact that you have
[01:33:52.560 --> 01:33:58.400]   lots of complex and sometimes contradictory moral intuitions, does that suggest that maybe the whole
[01:33:58.400 --> 01:34:01.920]   goal of having some fundamental principles you extrapolate the rest of morality from,
[01:34:01.920 --> 01:34:06.880]   that's a doom project? I think it does somewhat suggest that. I am somewhat partial to that view,
[01:34:06.880 --> 01:34:11.200]   and that's something I may be writing in my rebuttal, but I also think it's possible to
[01:34:11.200 --> 01:34:16.480]   just be confused. I think it's possible to just have lots of stuff going on in your brain. Some
[01:34:16.480 --> 01:34:22.480]   of it might be based on really good intentions of treating other people fairly and being good to
[01:34:22.480 --> 01:34:27.600]   other people. Some of it might be based on just other weird stuff about wanting to stand up for
[01:34:27.600 --> 01:34:33.600]   people who look like you or help people who look like you, and things like that. I do have some
[01:34:33.600 --> 01:34:37.920]   sympathy for the project of trying to say, "My intuitions contradict each other, but some of
[01:34:37.920 --> 01:34:41.760]   them are coming from a good place. Some of them are coming from a bad place. If I thought more
[01:34:41.760 --> 01:34:45.760]   about it, I would realize which ones are which, and I want to try and do that." Yeah. Let's talk
[01:34:45.760 --> 01:34:51.440]   about thin utilitarianism. There's this question from an old Scott Alexander post where he asks,
[01:34:51.440 --> 01:34:54.960]   "Would you rather the medieval church spent all of its money helping the poor
[01:34:54.960 --> 01:34:59.360]   rather than supporting the arts?" Maybe there were fewer poor people back in the medieval times,
[01:34:59.360 --> 01:35:03.120]   but you wouldn't have any cathedrals or you wouldn't have the Sistine Chapel. I don't know
[01:35:03.120 --> 01:35:07.200]   how you would answer that if you were in medieval times. It doesn't sound like the strongest version
[01:35:07.200 --> 01:35:12.800]   of this argument to me, to be honest. I think maybe that would be fine or good. I don't know.
[01:35:12.800 --> 01:35:18.240]   Maybe if I was a bigger fan of ... I mean, I don't know. My wife really loves these old churches,
[01:35:18.240 --> 01:35:23.040]   so maybe if I had more of her attitude, I would be more horrified by this idea. I think that,
[01:35:23.920 --> 01:35:28.320]   yeah, I mean, low-income people had a rough time in the past, and then having better lives seems
[01:35:28.320 --> 01:35:32.480]   pretty appealing. I don't really know if that's the best version of ... I don't know if that's
[01:35:32.480 --> 01:35:36.560]   the best version of this argument. All right. How much of Future Proof Ethics is basically,
[01:35:36.560 --> 01:35:40.640]   "You're very confident that a future Holden will have a much more developed and better set of
[01:35:40.640 --> 01:35:44.640]   ethics," or how much do you think people in general or humanity in general will get better
[01:35:44.640 --> 01:35:48.560]   ethics over time? Yeah, this has been definitely a point of confusion in this series, and partly
[01:35:48.560 --> 01:35:52.800]   just something I think I didn't communicate well about and makes the series not that amazing,
[01:35:53.520 --> 01:35:58.960]   is just that I use the term moral progress, and I just use it to refer to things getting better.
[01:35:58.960 --> 01:36:03.360]   I think sometimes there is such a thing as thinking more about your morality,
[01:36:03.360 --> 01:36:07.680]   gaining some insight, and ending up in a better place as a result. I think that is a thing that
[01:36:07.680 --> 01:36:12.800]   is real. There are some people who believe morality is an objective truth, but I'm not one
[01:36:12.800 --> 01:36:16.640]   of those people. Even though I believe morality is not objective, I still think there's a meaningful
[01:36:16.640 --> 01:36:21.680]   notion of moral progress. There's such a thing as having more reasonable moral views than I used to.
[01:36:21.680 --> 01:36:26.160]   What I didn't mean to say is that moral progress has any inevitability about it. I didn't mean to
[01:36:26.160 --> 01:36:30.160]   say that moral progress necessarily happens just because time goes on. I don't think that.
[01:36:30.160 --> 01:36:34.560]   I just think it's a thing that can happen. I do think a future Holden will probably be
[01:36:34.560 --> 01:36:37.920]   better at morality, just because I'm really interested in the topic, and I'm going to keep
[01:36:37.920 --> 01:36:44.320]   trying to improve at it. We have some reason to think that actually just does help a bit,
[01:36:44.320 --> 01:36:48.800]   a really tiny bit, but I'm not confident in that at all. I certainly don't think that society is
[01:36:48.800 --> 01:36:52.800]   going to have moral progress just necessarily, but I do think we've had some in the past.
[01:36:52.800 --> 01:36:58.880]   Okay, but then it seems weird to label the system of ethics future-proof ethics, right? Maybe it
[01:36:58.880 --> 01:37:03.600]   should be future Holden-proof ethics. Yeah, possible. I talk about this a bunch
[01:37:03.600 --> 01:37:08.400]   in the series, and I think I just didn't do a great job with this. I think what I was trying
[01:37:08.400 --> 01:37:12.960]   to do is I was trying to use a term that you didn't have to be a moral realist to get behind.
[01:37:12.960 --> 01:37:19.840]   What I was really trying to capture is, can I think now to reduce the odds that if later I
[01:37:19.840 --> 01:37:24.160]   improve that I'll be horrified by my early actions? That was what I was trying to capture
[01:37:24.160 --> 01:37:28.240]   the concept of. I'm not sure I really did it successfully. Gotcha. Okay, so you had a recent
[01:37:28.240 --> 01:37:32.960]   post on EA Forum that I thought was really interesting. A quote from that is, "My view
[01:37:32.960 --> 01:37:39.600]   is that for the most part, people who identify as EAs tend to have unusually high integrity,
[01:37:39.600 --> 01:37:45.440]   but my guess is that this is more despite utilitarianism than because of it." What do you
[01:37:45.440 --> 01:37:49.760]   think is the explanation for this coincidence where a group of reasonable, non-fanatical,
[01:37:49.760 --> 01:37:54.000]   high integrity people also happen to be a community of utilitarians? You might have a
[01:37:54.000 --> 01:37:59.280]   set of people who think of themselves as trying really hard to be the kind of person they should
[01:37:59.280 --> 01:38:03.360]   be, or really hard to bring their actions into line with their beliefs and their statements,
[01:38:03.360 --> 01:38:09.040]   and so that drives them to be honest a lot of the time and follow a lot of our common
[01:38:09.040 --> 01:38:14.240]   sense rules of morality, but it also drives them to really try to get that ethics right and land
[01:38:14.240 --> 01:38:20.160]   on ideas like utilitarianism that are very systematic and pure and give you this clear
[01:38:20.160 --> 01:38:24.640]   theoretical guidance. It could drive both those things, whereas I believe that if you're a
[01:38:24.640 --> 01:38:29.600]   utilitarian, it's really unclear whether utilitarianism actually tells you to do things
[01:38:29.600 --> 01:38:33.280]   like avoiding lying. Some people think it does, some people think it doesn't, and I think it's
[01:38:33.280 --> 01:38:38.800]   very unclear. You've advocated for the moral parliament's approach when you're trying to make
[01:38:38.800 --> 01:38:45.600]   decisions. What is the right level of organization at which to use that approach? Should individuals
[01:38:45.600 --> 01:38:51.200]   be making decisions based on having multiple different moral parties inside them? Is that
[01:38:51.200 --> 01:38:55.440]   the right approach for entire movements, but individuals should be specializing? What is the
[01:38:55.440 --> 01:38:58.560]   right level to be applying this approach at? This is something I hope to write about in the
[01:38:58.560 --> 01:39:02.960]   future is this topic of moral uncertainty, and the basic idea is there might be a bunch
[01:39:02.960 --> 01:39:07.600]   of different ways about thinking about what the right thing is to do in the world. You might look
[01:39:07.600 --> 01:39:12.080]   at the world from one angle and say, "Well, what matters is the total sum of all the pleasures,
[01:39:12.080 --> 01:39:16.160]   so therefore a bigger world would be better, so therefore I should be really obsessed with getting
[01:39:16.160 --> 01:39:20.240]   the world to be as big as possible." There might be another perspective that says that what really
[01:39:20.240 --> 01:39:23.680]   matters is suffering, we should minimize suffering, we should want the world to be small.
[01:39:23.680 --> 01:39:27.520]   There might be another perspective that says, "It doesn't matter what happens to the world,
[01:39:27.520 --> 01:39:30.800]   it matters how I act, what matters is that I act with integrity, that I tell the truth,"
[01:39:30.800 --> 01:39:34.960]   things like that. There's these interesting debates about what do you do when you think
[01:39:34.960 --> 01:39:38.960]   you have some sympathy for all these views, and then how do you choose an action that some
[01:39:38.960 --> 01:39:41.760]   perspectives would say is the best thing you've ever done and some would say is the worst thing
[01:39:41.760 --> 01:39:45.920]   you've ever done. The moral of parliament idea is an idea that was laid out by Nick Bostrom
[01:39:45.920 --> 01:39:50.400]   in an Overcoming Bias post something like a decade ago that I like. I think about it as if I'm just
[01:39:50.400 --> 01:39:54.080]   multiple people. I just think about it as if there's multiple people all living inside my
[01:39:54.080 --> 01:39:58.000]   head arguing about what to do, and they all are friends, and they all care about each other,
[01:39:58.000 --> 01:40:02.720]   and they want to get along, and so they're trying to reach a deal that all of them can feel fairly
[01:40:02.720 --> 01:40:07.440]   good about. That is how I tend to think about dealing with different moral views. I tend to
[01:40:07.440 --> 01:40:12.800]   want to do things that are really good according to one and not too bad according to the rest,
[01:40:12.800 --> 01:40:19.440]   and try to have the different parts of myself making deals with each other. That relates to
[01:40:19.440 --> 01:40:24.000]   something I said at the beginning about not being into ends justify the means. I put a lot of effort
[01:40:24.000 --> 01:40:28.800]   into doing things that would be really, really good if this most important century stuff came
[01:40:28.800 --> 01:40:33.840]   out true, but also not too catastrophic if it didn't. There's lines I'm not willing to cross.
[01:40:33.840 --> 01:40:40.720]   There's behaviors I'm not willing to engage in to promote the goals of people who worry about AI
[01:40:40.720 --> 01:40:46.560]   safety. It's a moderating approach, I think. Got you. That makes a lot of sense for somebody who's
[01:40:46.560 --> 01:40:51.680]   the CEO of Open Philanthropy that you would want the decisions you make to reflect uncertainties
[01:40:51.680 --> 01:40:56.160]   about your decisions, but if it's just somebody like me where I'm not necessarily in a position
[01:40:56.160 --> 01:41:00.960]   where I have a large amount of resources to dedicate to, should I just specialize in that
[01:41:00.960 --> 01:41:06.400]   particular moral view I have, or should I also be trying to allocate my time and resources according
[01:41:06.400 --> 01:41:10.560]   to different moral views? I think no matter what position I was in in the world and however many
[01:41:10.560 --> 01:41:14.960]   resources I had, I would feel that my decisions were significant in some sense and affected people
[01:41:14.960 --> 01:41:20.640]   and were important in that way that they affect those around me. I think it's just very natural
[01:41:20.640 --> 01:41:24.720]   to me to think there's a lot of different perspectives on what it means to be a good
[01:41:24.720 --> 01:41:29.600]   person and rather than trying to turn them into a single unifying mathematical equation and take
[01:41:29.600 --> 01:41:32.880]   the expected value, which is another approach that I think is interesting, but I think the approach
[01:41:32.880 --> 01:41:36.160]   I do tend to prefer is to imagine the different perspectives as different people trying to get
[01:41:36.160 --> 01:41:40.320]   along and make a deal with each other. Let's talk about governance and management.
[01:41:40.320 --> 01:41:46.080]   In software, as I'm sure you're aware, there's a concept of a 10x engineer. Is there something
[01:41:46.080 --> 01:41:52.000]   similar in the kinds of work a research analyst at Open Philanthropy does? Is it meaningful to say
[01:41:52.000 --> 01:41:56.880]   that two people doing the same job, one can be orders of magnitude more effective than another?
[01:41:56.880 --> 01:42:00.480]   Yeah, I think some people are much, at any given thing in Open Philanthropy, some people are much
[01:42:00.480 --> 01:42:04.240]   better at it than others. I don't think that's very surprising. I think it's true in many jobs.
[01:42:04.240 --> 01:42:09.680]   I don't really know the reasons for it. It could be any combination of talent, interest,
[01:42:09.680 --> 01:42:15.360]   how hard someone works at it. I certainly think there's a lot of variance and hiring people who
[01:42:15.360 --> 01:42:19.040]   can do a great job at the work Open Phil does has been a lifelong challenge.
[01:42:20.160 --> 01:42:23.600]   You've written about the Bayesian mindset. You know many billionaires, many of them are donors
[01:42:23.600 --> 01:42:28.160]   to Open Philanthropy. In your experience, do these startup founders who end up becoming very
[01:42:28.160 --> 01:42:32.480]   successful, do they have a Bayesian mindset or is that the wrong way to characterize their...
[01:42:32.480 --> 01:42:37.760]   Yeah, I wrote about this idea of Bayesian mindset, which is basically like being willing to put a
[01:42:37.760 --> 01:42:43.600]   probability on anything and use your probabilities and say your probabilities as a way of discovering
[01:42:43.600 --> 01:42:48.480]   why it is you think what you think and using expected value calculations similarly. I think
[01:42:48.480 --> 01:42:54.320]   this is much more common among successful tech founders than it is among the general population,
[01:42:54.320 --> 01:42:58.960]   but there's plenty of tech founders who don't think this way at all. I say in the Bayesian
[01:42:58.960 --> 01:43:02.960]   mindset, I don't think it's like a super well-tested, well-proven social technology that
[01:43:02.960 --> 01:43:06.400]   does amazing things. I think it's more like an interesting thing to be experimenting with.
[01:43:06.400 --> 01:43:08.560]   Okay, that seems...
[01:43:08.560 --> 01:43:14.560]   Well, the general population, it's practically unheard of, Bayesian mindset. Not even just
[01:43:14.560 --> 01:43:19.840]   the name, just the idea, this whole idea of thinking expected value and subjective probabilities
[01:43:19.840 --> 01:43:23.920]   all the time. It's just like almost no one does that. I think tech founders probably do it more
[01:43:23.920 --> 01:43:27.120]   than the average person. No, that makes sense. Do you think that
[01:43:27.120 --> 01:43:31.200]   adopting more of a Bayesian mindset would help somebody get the top levels, be more successful?
[01:43:31.200 --> 01:43:36.320]   It's really TBD and unclear. I just think the Bayesian mindset is a cool thing to experiment
[01:43:36.320 --> 01:43:40.480]   with. I experiment with it a lot. I feel like it helps me sometimes. Like most things, it's good
[01:43:40.480 --> 01:43:44.400]   in moderation and with taste and not using it for every single thing. Maybe 10 years from now,
[01:43:44.400 --> 01:43:48.720]   as it gets more popular, a better sense of where the actual applied strengths and weaknesses are.
[01:43:48.720 --> 01:43:52.240]   As I'm sure you're aware, there's many prizes floating around for all kinds of intellectual
[01:43:52.240 --> 01:43:55.840]   work in effective altruism. Some of them even have come from open philanthropy.
[01:43:55.840 --> 01:43:56.320]   Yeah.
[01:43:56.320 --> 01:44:01.040]   Are you optimistic about their ability to resurface new ideas?
[01:44:01.040 --> 01:44:06.480]   I would say I'm medium optimistic about the impact of all these prizes. I've been part of
[01:44:06.480 --> 01:44:10.880]   designing some of them. I've just seen some other ones. People will say, "Hey, we'll pay you X
[01:44:10.880 --> 01:44:15.280]   dollars if you can give us a good critique of our... GiveWell will pay people to give them a
[01:44:15.280 --> 01:44:20.560]   good critique of their reasoning about what the best charities already give to open philanthropy
[01:44:20.560 --> 01:44:23.840]   as a prize for at least showing us a cause we should be looking at that we're not." I don't
[01:44:23.840 --> 01:44:28.640]   know. I think I'm medium optimistic. I think it will get some interest and it will get some people
[01:44:28.640 --> 01:44:31.840]   to pay attention who weren't otherwise. Some of those people might have good ideas. I don't think
[01:44:31.840 --> 01:44:36.480]   it's the only way to solve these problems or that it will automatically solve them. That's generally
[01:44:36.480 --> 01:44:38.720]   how the people designing the prizes think about them too.
[01:44:38.720 --> 01:44:42.160]   You have an interesting post about stakeholder management that over time,
[01:44:42.160 --> 01:44:46.000]   institutions have to take into account the interests of more and more stakeholders.
[01:44:46.000 --> 01:44:49.520]   Do you expect that this will be something that will be a major factor in how open
[01:44:49.520 --> 01:44:54.800]   philanthropy acts in the future? What will be the impact on how open philanthropy runs overall?
[01:44:54.800 --> 01:44:58.240]   Yeah. I think in general, the bigger your organization is, the bigger your city is,
[01:44:58.240 --> 01:45:03.040]   the bigger your society is, there's more people who are... If you want everyone to be happy,
[01:45:03.040 --> 01:45:07.200]   there's more people you're going to have to make happy. I think this does mean in general,
[01:45:07.200 --> 01:45:12.560]   by default, as a company grows, it gets less able to make a lot of disruptive, quick changes. A lot
[01:45:12.560 --> 01:45:16.560]   of people would use the term nimble. A lot of people in the tech world like to just use these
[01:45:16.560 --> 01:45:21.040]   very negative terms for big company properties and very positive terms for small company properties.
[01:45:21.040 --> 01:45:25.040]   Small companies are nimble, and quick, and practical, and adaptive, and dynamic,
[01:45:25.040 --> 01:45:30.240]   and high productivity. Big companies are bureaucratic, and slow, and non-adaptive.
[01:45:30.240 --> 01:45:34.880]   I think that's all fair. I also think that big companies often, at the end of the day,
[01:45:35.680 --> 01:45:41.840]   produce more stuff than they could if they were small. I think if Apple were still 10 people,
[01:45:41.840 --> 01:45:44.720]   it might be a more exciting place to work, but they wouldn't be able to make all those iPhones.
[01:45:44.720 --> 01:45:50.000]   There's a lot of iPhones going out to a lot of people, serving a lot of different people's needs,
[01:45:50.000 --> 01:45:53.680]   abiding by a lot of regulatory requirements. There's a lot of work to be done.
[01:45:53.680 --> 01:45:59.200]   I don't think it's necessarily a bad thing, but I think it's a trade-off for a company to grow.
[01:45:59.840 --> 01:46:07.040]   I do think open philanthropy is in the business of doing unconventional giving and using a lot
[01:46:07.040 --> 01:46:12.160]   of judgment calls to do it. I tend to think we benefit a lot from staying as small as we can.
[01:46:12.160 --> 01:46:16.080]   I generally have fought for us to stay as small as we can while doing our work,
[01:46:16.080 --> 01:46:19.600]   but we still have to grow from where we are. Gotcha. Do you mean stay small in terms of
[01:46:19.600 --> 01:46:24.080]   funds or do you mean people? People, yes. It seems odd to say that the organization
[01:46:24.080 --> 01:46:27.920]   you have the most experience with, your inside view is that more stakeholders would be bad,
[01:46:27.920 --> 01:46:33.840]   but overall, it's been a net zero or positive. It's not clear. We're growing. We're bigger than
[01:46:33.840 --> 01:46:37.680]   we were a year ago. We'll be bigger in a year. It's definitely not true that I'm just like
[01:46:37.680 --> 01:46:42.800]   minimize the size of the company. We're growing, but I think we want to watch it. I think we want
[01:46:42.800 --> 01:46:46.880]   to treat each hire as something that we only do because we had a really good reason to.
[01:46:46.880 --> 01:46:51.440]   I think there are some companies that may have more to gain from being 10,000 people. I don't
[01:46:51.440 --> 01:46:54.480]   think we'll ever be 10,000 people. Right, right, right. Now,
[01:46:54.480 --> 01:46:59.840]   your written career advice emphasizes building aptitudes and specializing, but when I look at
[01:46:59.840 --> 01:47:03.440]   your career, it's all over the place, right? We were just talking about at the beginning of the
[01:47:03.440 --> 01:47:08.800]   interview, you started off in GiveWell, then you were working at Open Philanthropy, and now you're
[01:47:08.800 --> 01:47:15.120]   forecasting AI. How do you think about this kind of thing? Are you specializing or what's going on
[01:47:15.120 --> 01:47:20.880]   here? I don't know if I really forecast AI. I mostly distill and bring together analyses that
[01:47:20.880 --> 01:47:25.920]   others have done, and I manage people who work on that sort of thing. I don't know. I think it's
[01:47:25.920 --> 01:47:29.280]   really good. The career advice I often give is just like it's really good to just have something
[01:47:29.280 --> 01:47:33.440]   you're very focused on, that you're specializing in, that you're trying to be the best in the world
[01:47:33.440 --> 01:47:38.800]   at. The way that my career went is like, first off, the general theme of my career is just taking
[01:47:38.800 --> 01:47:43.600]   questions, especially questions about how to give effectively, where no one's really gotten started
[01:47:43.600 --> 01:47:48.480]   on this question, so even doing a pretty crappy analysis can be better than what already exists.
[01:47:48.480 --> 01:47:53.920]   Often what I have done in my career, what I consider myself to have specialized in, in a sense,
[01:47:53.920 --> 01:47:58.960]   is I do the first cut crappy analysis of some question that has not been analyzed much and is
[01:47:58.960 --> 01:48:04.160]   very important, and then I build a team to do better analysis of that question. That's been my
[01:48:04.160 --> 01:48:07.920]   general pattern. I think that's the most generalizable skill I've had, but I have switched
[01:48:07.920 --> 01:48:13.520]   around because I do think that I've at various points in my career just said, "Hey, here's
[01:48:13.520 --> 01:48:17.040]   something that's getting very little attention, and it's very important, and it's worth the
[01:48:17.040 --> 01:48:21.440]   sacrifice of the specialized knowledge I've built up in one area to switch into this other area that
[01:48:21.440 --> 01:48:25.440]   I think I ought to be working on." What does the logo on the Coldface blog mean?
[01:48:25.440 --> 01:48:30.320]   There is no logo. I think you're talking about the browser icon? Yeah, yeah.
[01:48:30.320 --> 01:48:38.000]   So, that is a stuffed animal named Mora, and at some point, if I get enough subscribers, I will
[01:48:38.000 --> 01:48:42.960]   explain who all these stuffed animals are, but my wife and I basically use a stuffed animal
[01:48:42.960 --> 01:48:48.240]   personality classification system where we will compare someone to various stuffed animals to
[01:48:48.240 --> 01:48:53.680]   explain what their strengths and weaknesses are, and Mora is a pink polar bear who's very creative,
[01:48:53.680 --> 01:48:58.880]   but also very narcissistic and loves attention. So, she's the mascot of the blog because it's this
[01:48:58.880 --> 01:49:04.080]   blog that's just very crazy, very out there, and is just me writing in public, and so it just felt
[01:49:04.080 --> 01:49:08.080]   like her spirit. Gosh, okay. Well, let me ask, what is the goal of the blog? Are you having a
[01:49:08.080 --> 01:49:11.920]   second job as a blogger in addition to being the CEO of a big organization?
[01:49:11.920 --> 01:49:15.840]   No, I think it fits into my job reasonably well. I didn't want it to be open philanthropy branded.
[01:49:15.840 --> 01:49:19.440]   I just wanted the freedom to write about things the way I wanted and how I wanted, but I do think
[01:49:19.440 --> 01:49:24.640]   that we make these high-stakes decisions based on very unconventional views about the world,
[01:49:24.640 --> 01:49:29.520]   and I think it's good for us to be trying to make those views have contact for the rest of the
[01:49:29.520 --> 01:49:34.000]   world. So, I think there would be something not ideal about being a large foundation giving
[01:49:34.000 --> 01:49:38.080]   you large amounts of money, and we're just kind of quietly going around believing these enormously
[01:49:38.080 --> 01:49:42.480]   important and true things that no one else believes, and it is a way of kind of, you know,
[01:49:42.480 --> 01:49:47.280]   if we put the views out into the world, A, I think just like all the people seeking money from us
[01:49:47.280 --> 01:49:51.280]   just have a better idea of where we're coming from, just like why is it that we're interested
[01:49:51.280 --> 01:49:55.120]   in funding what we're interested in funding, and I think to the extent people find the arguments
[01:49:55.120 --> 01:50:00.000]   compelling or even just like understand them and helps understand our thinking, I think that can
[01:50:00.000 --> 01:50:05.040]   help create more grantees for us. It can help, you know, cause the world to be a place where
[01:50:05.040 --> 01:50:09.600]   there's more good stuff for us to fund because more people see where we're coming from, hopefully
[01:50:09.600 --> 01:50:13.360]   agree with it, and are trying to work on the things we consider important, and then to the
[01:50:13.360 --> 01:50:17.360]   extent that my stuff is actually just like screwed up and wrong and I've got mistakes in there and
[01:50:17.360 --> 01:50:20.800]   I've thought it all through wrong, this is also like the best way I know of to discover that.
[01:50:20.800 --> 01:50:25.120]   It's just, you know, I don't know how else I'm going to get people to critique it except by
[01:50:25.120 --> 01:50:29.200]   putting it out there and maybe getting some attention for it. So, that's how I kind of think
[01:50:29.200 --> 01:50:33.520]   of it is it's taking views that are very important to the decisions we're making and trying to
[01:50:33.520 --> 01:50:37.680]   express them so that we can either get more people agreeing with us who we're able to fund and
[01:50:37.680 --> 01:50:40.960]   support and work with, or learn more about what we're getting wrong.
[01:50:40.960 --> 01:50:44.640]   All right, so let me actually ask you, has that happened? Like, has there been an important view
[01:50:44.640 --> 01:50:48.560]   that are expressed on the blog that because of feedback you changed your mind on, or is it mostly
[01:50:48.560 --> 01:50:52.400]   about the communication part? Let me think. I mean, there's definitely been like a lot of
[01:50:52.400 --> 01:50:56.720]   interesting stuff. I mean, like an example is I put up this post on the track record of Futurist,
[01:50:56.720 --> 01:51:00.880]   and then there was a post by I think Dan Liu recently that I haven't read yet,
[01:51:00.880 --> 01:51:05.120]   but it just has its own analysis of the track record of Futurist, and I need to compare them,
[01:51:05.120 --> 01:51:08.800]   you know, think about what I really think about how good humans have historically been
[01:51:08.800 --> 01:51:12.880]   about predicting the future. But yes, he certainly has a ton of data in there that I was not aware of
[01:51:12.880 --> 01:51:18.640]   that, you know, that feels like a bit of a response, may or may not have been prompted by it.
[01:51:18.640 --> 01:51:22.160]   Yeah, there's been like a lot of commentary. There's been a lot of, like, you wrote some
[01:51:22.160 --> 01:51:25.360]   critiques of some of the stuff I've written in The Most Important Century. There have been other
[01:51:25.360 --> 01:51:30.320]   critiques. I mean, I don't know. Like, I think a lot of the stuff I wrote about the biggest weak
[01:51:30.320 --> 01:51:33.920]   points of The Most Important Century were based on the public criticism that was coming in. So,
[01:51:33.920 --> 01:51:37.440]   I think it has, like, yeah, I think I have become more aware of a lot of the, like,
[01:51:37.440 --> 01:51:41.200]   the parts of my thinking that are the least convincing, or the most weak, or
[01:51:41.200 --> 01:51:44.320]   the most in need of argument, and I have, like, paid more attention to those things because of
[01:51:44.320 --> 01:51:48.960]   that. Yeah. Right. Okay. This may just be me talking, but it actually does sound like maybe
[01:51:48.960 --> 01:51:54.000]   you've learned about how people react to The Most Important Century thesis, but it doesn't seem like
[01:51:54.000 --> 01:51:58.480]   something has surfaced which has made you change your mind on it a lot. Oh, I mean, I haven't
[01:51:58.480 --> 01:52:03.760]   dropped my, I mean, that would be a big change, like, to drop my view that we could be in The
[01:52:03.760 --> 01:52:08.640]   Most Important Century for humanity. Like, that's still what I believe. I mean, I think I've also
[01:52:08.640 --> 01:52:11.760]   heard from people who just think I'm, like, underselling the whole thing like crazy. Like,
[01:52:11.760 --> 01:52:17.360]   people who just think that I should be planning on transformative AI much sooner than what I say,
[01:52:17.360 --> 01:52:22.480]   than what I kind of imply in the series. So, yeah, I mean, I put out this thing, The Most
[01:52:22.480 --> 01:52:27.440]   Important Century, and I don't believe any of the critiques have been deep enough and strong enough
[01:52:27.440 --> 01:52:31.440]   to make me just, like, drop that whole thing. But it's a big picture with a lot of moving parts,
[01:52:31.440 --> 01:52:34.960]   and I have kind of deepened my, you know, my understanding of many of the parts. Yeah, yeah.
[01:52:34.960 --> 01:52:42.480]   One thing I find really interesting about your work is how much it involves the CEO having a
[01:52:42.480 --> 01:52:46.960]   deep understanding of all the issues involved. You are the one who has to, like, deeply understand,
[01:52:46.960 --> 01:52:50.880]   for example, we were just talking about moral parlance or whether it's, like, specific forecast
[01:52:50.880 --> 01:52:56.080]   about AI, like, biological anchors or whatever else, right? It seems maybe in other organizations,
[01:52:56.080 --> 01:53:00.000]   the CEO just delegates this kind of understanding and just asks for the bullet points.
[01:53:00.000 --> 01:53:04.000]   Is this something you think more leaders should be doing? Or is there something special about
[01:53:04.000 --> 01:53:09.120]   your position? I know much less about any given topic than the person who specializes in the
[01:53:09.120 --> 01:53:12.880]   topic. And I think what I try to do is I try and know enough about the topic that I can manage
[01:53:12.880 --> 01:53:16.880]   them effectively. And that's, like, a pretty general corporate best practice. And I think
[01:53:16.880 --> 01:53:20.640]   it just varies a lot. So I think, for example, like, something like keeping our books, like,
[01:53:20.640 --> 01:53:24.400]   keeping our finances, doing the financial audits, all that stuff. That's something that's, like,
[01:53:24.400 --> 01:53:27.920]   really easy to just judge the outputs without really knowing much about finance at all. You
[01:53:27.920 --> 01:53:32.400]   can just say, look, was this compliant? Did we do our audit? Did we pass the audit? You know,
[01:53:32.400 --> 01:53:36.080]   do we still have money in the bank? Like, how much money do we have in the bank? You don't need to
[01:53:36.080 --> 01:53:41.840]   know much about it to judge it effectively. But at any company, I mean, you may need to know a fair
[01:53:41.840 --> 01:53:46.400]   amount about some other topics in order to judge them very effectively. If your company is making
[01:53:46.400 --> 01:53:51.520]   computers or phones and design is very important, the CEO would be really bad if the CEO just had,
[01:53:51.520 --> 01:53:55.040]   like, no opinions on design. It was just like, well, I'm going to let our design person decide
[01:53:55.040 --> 01:53:58.320]   the design. I mean, it's a central thing to the company. It matters to the company,
[01:53:58.320 --> 01:54:02.080]   and they should know some things about it. So I do, you know, things that are really central
[01:54:02.080 --> 01:54:06.160]   to open philanthropy. What does it mean to do good? How do we handle uncertainty about how
[01:54:06.160 --> 01:54:10.560]   to do good? What are the most important causes? If AI might be one of the most important causes,
[01:54:10.560 --> 01:54:14.480]   then when might we see transformative AI? What would that mean? How big is the risk
[01:54:14.480 --> 01:54:19.120]   of misaligned AI? I think I need to understand those issues well enough to effectively manage
[01:54:19.120 --> 01:54:22.880]   people who know a lot more about them than I do. Yeah, and I'm just curious, like, what do you,
[01:54:22.880 --> 01:54:25.760]   you know, what do you think about this whole most important century stuff? Does it just strike you
[01:54:25.760 --> 01:54:30.800]   as like crazy? Or like, you know, what do you think when you read the series? Yeah, I've been,
[01:54:30.800 --> 01:54:33.920]   obviously, like, through the entire interview, I've been trying to, like, nitpick small things.
[01:54:33.920 --> 01:54:38.080]   But when I really think about what is the main claim you're making is that this could be the
[01:54:38.080 --> 01:54:41.440]   most important century, transformative AI could happen this century, and if it does, it's a really
[01:54:41.440 --> 01:54:44.640]   big deal. It's like, yeah, I don't disagree. That makes a lot of sense. Throughout, like,
[01:54:44.640 --> 01:54:47.680]   preparing for the interview and trying to come up with objections, I've just been, like, a little
[01:54:47.680 --> 01:54:51.440]   bit more and more convinced with thinking about, like, is there actually something I could do over
[01:54:51.440 --> 01:54:55.040]   my early career that matters? Or is that something that maybe I should just hold
[01:54:55.040 --> 01:54:57.920]   off on thinking about? Glad to hear it. Do you have any ideas what you might do?
[01:54:57.920 --> 01:55:03.360]   No. Really? Literally no ideas? You haven't been like, can I work on AI alignment or?
[01:55:03.360 --> 01:55:08.080]   Oh, well, yeah, in that sense, I thought a little bit about it. Probably in, like,
[01:55:08.080 --> 01:55:11.280]   two months or three months, I'll think real hard about what I actually want to do for career.
[01:55:11.280 --> 01:55:19.040]   Hey, thanks for listening. If you enjoyed that episode, I would really appreciate it
[01:55:19.040 --> 01:55:25.200]   if you could share it. This is still a pretty small podcast, so it is a huge help when any
[01:55:25.200 --> 01:55:30.160]   one of you shares an episode that you like. Post it on Twitter, send it to friends who you think
[01:55:30.160 --> 01:55:34.480]   might like it, put it in your group chats, just let the word go forth. It helps out a ton.
[01:55:35.280 --> 01:55:44.640]   Many thanks to my amazing editor, Graham Bessaloo for producing this podcast, and to Mia Ayana for
[01:55:44.640 --> 01:55:50.000]   creating the amazing transcripts that accompany each episode, which have helpful links and you
[01:55:50.000 --> 01:55:56.400]   can find them at the link in the description below. Remember to subscribe on YouTube and
[01:55:56.400 --> 01:56:05.920]   your favorite podcast platforms. Cheers. See you next time.
[01:56:05.920 --> 01:56:09.900]   [ Music ]

