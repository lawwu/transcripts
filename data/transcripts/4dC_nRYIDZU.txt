
[00:00:00.000 --> 00:00:02.600]   The following is a conversation with Sam Harris,
[00:00:02.600 --> 00:00:03.920]   one of the most influential
[00:00:03.920 --> 00:00:06.040]   and pioneering thinkers of our time.
[00:00:06.040 --> 00:00:08.560]   He's the host of the Making Sense podcast
[00:00:08.560 --> 00:00:10.440]   and the author of many seminal books
[00:00:10.440 --> 00:00:12.840]   on human nature and the human mind,
[00:00:12.840 --> 00:00:15.840]   including The End of Faith, The Moral Landscape,
[00:00:15.840 --> 00:00:18.560]   Lying, Free Will, and Waking Up.
[00:00:18.560 --> 00:00:21.640]   He also has a meditation app called Waking Up
[00:00:21.640 --> 00:00:24.760]   that I've been using to guide my own meditation.
[00:00:24.760 --> 00:00:26.760]   Quick mention of our sponsors,
[00:00:26.760 --> 00:00:31.520]   National Instruments, Belcampo, Athletic Greens, and Linode.
[00:00:31.520 --> 00:00:34.800]   Check them out in the description to support this podcast.
[00:00:34.800 --> 00:00:36.800]   As a side note, let me say that Sam
[00:00:36.800 --> 00:00:39.200]   has been an inspiration to me
[00:00:39.200 --> 00:00:41.320]   as he has been for many, many people,
[00:00:41.320 --> 00:00:44.840]   first from his writing, then his early debates,
[00:00:44.840 --> 00:00:48.280]   maybe 13, 14 years ago on the subject of faith,
[00:00:48.280 --> 00:00:51.120]   his conversations with Christopher Hitchens,
[00:00:51.120 --> 00:00:54.400]   and since 2013, his podcast.
[00:00:54.400 --> 00:00:56.880]   I didn't always agree with all of his ideas,
[00:00:56.880 --> 00:00:59.680]   but I was always drawn to the care and depth
[00:00:59.680 --> 00:01:02.000]   of the way he explored those ideas,
[00:01:02.000 --> 00:01:05.400]   the calm and clarity amid the storm of difficult,
[00:01:05.400 --> 00:01:07.640]   at times controversial discourse.
[00:01:07.640 --> 00:01:10.640]   I really can't express in words how much it meant to me
[00:01:10.640 --> 00:01:14.120]   that he, Sam Harris, someone who I've listened to
[00:01:14.120 --> 00:01:16.000]   for many hundreds of hours,
[00:01:16.000 --> 00:01:20.560]   would write a kind email to me saying he enjoyed this podcast
[00:01:20.560 --> 00:01:23.560]   and more that he thought I had a unique voice
[00:01:23.560 --> 00:01:25.920]   that added something to this world.
[00:01:25.920 --> 00:01:28.600]   Whether it's true or not, it made me feel special
[00:01:28.600 --> 00:01:31.080]   and truly grateful to be able to do this thing
[00:01:31.080 --> 00:01:33.680]   and motivated me to work my ass off
[00:01:33.680 --> 00:01:35.440]   to live up to those words.
[00:01:35.440 --> 00:01:37.920]   Meeting Sam and getting to talk with him
[00:01:37.920 --> 00:01:41.700]   was one of the most memorable moments of my life.
[00:01:41.700 --> 00:01:44.040]   This is the Lex Friedman Podcast,
[00:01:44.040 --> 00:01:47.640]   and here is my conversation with Sam Harris.
[00:01:47.640 --> 00:01:50.200]   I've been enjoying meditating
[00:01:50.200 --> 00:01:52.720]   with the Waking Up app recently.
[00:01:52.720 --> 00:01:54.760]   It makes me think about the origins
[00:01:54.760 --> 00:01:56.840]   of cognition and consciousness,
[00:01:56.840 --> 00:02:01.000]   so let me ask, where do thoughts come from?
[00:02:01.000 --> 00:02:04.640]   - Well, that's a very difficult question to answer.
[00:02:04.640 --> 00:02:09.440]   Subjectively, they appear to come from nowhere, right?
[00:02:09.440 --> 00:02:14.440]   I mean, it's just they come out of some kind of mystery
[00:02:14.440 --> 00:02:17.080]   that is at our backs subjectively, right?
[00:02:17.080 --> 00:02:20.600]   So, which is to say that if you pay attention
[00:02:22.000 --> 00:02:25.280]   to the nature of your mind in this moment,
[00:02:25.280 --> 00:02:26.960]   you realize that you don't know
[00:02:26.960 --> 00:02:29.120]   what you're going to think next, right?
[00:02:29.120 --> 00:02:30.400]   Now, you're expecting to think something
[00:02:30.400 --> 00:02:33.040]   that seems like you authored it, right?
[00:02:33.040 --> 00:02:35.840]   You're not, unless you're schizophrenic
[00:02:35.840 --> 00:02:38.440]   or you have some kind of thought disorder
[00:02:38.440 --> 00:02:41.920]   where your thoughts seem fundamentally foreign to you,
[00:02:41.920 --> 00:02:45.400]   they do have a kind of signature of selfhood
[00:02:45.400 --> 00:02:50.040]   associated with them, and people readily identify with them.
[00:02:50.040 --> 00:02:51.880]   They feel like what you are.
[00:02:51.880 --> 00:02:52.920]   I mean, this is the thing,
[00:02:52.920 --> 00:02:56.040]   this is the spell that gets broken with meditation.
[00:02:56.040 --> 00:03:00.120]   Our default state is to feel identical
[00:03:00.120 --> 00:03:02.600]   to the stream of thought, right?
[00:03:02.600 --> 00:03:05.200]   Which is fairly paradoxical,
[00:03:05.200 --> 00:03:08.960]   'cause how could you as a mind, as a self,
[00:03:08.960 --> 00:03:11.840]   you know, if there were such a thing as a self,
[00:03:11.840 --> 00:03:15.920]   how could you be identical to the next piece of language
[00:03:15.920 --> 00:03:20.920]   or the next image that just springs into conscious view?
[00:03:21.920 --> 00:03:26.440]   But, and you know, meditation is ultimately
[00:03:26.440 --> 00:03:28.920]   about examining that point of view closely enough
[00:03:28.920 --> 00:03:31.960]   so as to unravel it and feel the freedom
[00:03:31.960 --> 00:03:34.360]   that's on the other side of that identification.
[00:03:34.360 --> 00:03:39.360]   But the subjectively thoughts simply emerge, right?
[00:03:39.360 --> 00:03:43.080]   And you don't think them before you think them, right?
[00:03:43.080 --> 00:03:46.000]   There's this first moment where, you know,
[00:03:46.000 --> 00:03:48.600]   I mean, just anyone listening to us or watching us now
[00:03:48.600 --> 00:03:50.920]   could perform this experiment for themselves.
[00:03:50.920 --> 00:03:54.440]   I mean, just imagine something or remember something.
[00:03:54.440 --> 00:03:56.600]   You know, just pick a memory, any memory, right?
[00:03:56.600 --> 00:03:58.240]   You've got a storehouse of memory,
[00:03:58.240 --> 00:04:00.920]   just promote one to consciousness.
[00:04:00.920 --> 00:04:04.760]   Did you pick that memory?
[00:04:04.760 --> 00:04:07.600]   I mean, let's say you remembered breakfast yesterday
[00:04:07.600 --> 00:04:10.040]   or you remembered what you said to your spouse
[00:04:10.040 --> 00:04:11.080]   before leaving the house,
[00:04:11.080 --> 00:04:13.880]   or you remembered what you watched on Netflix last night,
[00:04:13.880 --> 00:04:16.000]   or you remembered something that happened to you
[00:04:16.000 --> 00:04:18.800]   when you were four years old, whatever it is, right?
[00:04:18.800 --> 00:04:24.200]   First it wasn't there and then it appeared.
[00:04:24.200 --> 00:04:28.480]   And that is not a, I mean, I'm sure we'll get to the topic
[00:04:28.480 --> 00:04:30.920]   of free will ultimately.
[00:04:30.920 --> 00:04:33.200]   That's not evidence of free will, right?
[00:04:33.200 --> 00:04:35.440]   - Why are you so sure, by the way?
[00:04:35.440 --> 00:04:36.280]   It's very interesting.
[00:04:36.280 --> 00:04:38.500]   - Well, through no free will of my own, yeah.
[00:04:38.500 --> 00:04:41.760]   Everything just appears, right?
[00:04:41.760 --> 00:04:43.640]   And what else could it do?
[00:04:43.640 --> 00:04:45.480]   And so that's the subjective side of it.
[00:04:45.480 --> 00:04:48.000]   Objectively, we have every reason to believe
[00:04:48.000 --> 00:04:50.840]   that many of our thoughts, all of our thoughts
[00:04:50.840 --> 00:04:55.840]   are at bottom what some part of our brain is doing
[00:04:55.840 --> 00:04:58.080]   neurophysiologically.
[00:04:58.080 --> 00:05:00.000]   I mean, that these are the products
[00:05:00.000 --> 00:05:02.520]   of some kind of neural computation
[00:05:02.520 --> 00:05:05.160]   and neural representation,
[00:05:05.160 --> 00:05:06.880]   and we're talking about memories.
[00:05:06.880 --> 00:05:10.640]   - Is it possible to pull at the string of thoughts
[00:05:10.640 --> 00:05:12.540]   to try to get to its root,
[00:05:14.080 --> 00:05:17.640]   to try to dig in past the obvious surface,
[00:05:17.640 --> 00:05:20.240]   subjective experience of like the thoughts
[00:05:20.240 --> 00:05:21.640]   pop out of nowhere?
[00:05:21.640 --> 00:05:24.440]   Is it possible to somehow get closer to the roots
[00:05:24.440 --> 00:05:28.720]   of where they come out of from the firing of the cells?
[00:05:28.720 --> 00:05:32.480]   Or is it a useless pursuit to dig into that direction?
[00:05:32.480 --> 00:05:37.480]   - Well, you can get closer to many, many subtle contents
[00:05:37.480 --> 00:05:39.960]   in consciousness, right?
[00:05:39.960 --> 00:05:42.120]   So you can notice things more and more clearly
[00:05:42.120 --> 00:05:44.360]   and have a landscape of mind open up
[00:05:44.360 --> 00:05:47.800]   and become more differentiated and more interesting.
[00:05:47.800 --> 00:05:51.800]   And if you take psychedelics, it opens up wide
[00:05:51.800 --> 00:05:54.000]   depending on what you've taken and the dose,
[00:05:54.000 --> 00:05:56.040]   it opens in directions and to an extent
[00:05:56.040 --> 00:05:59.760]   that very few people imagine would be possible,
[00:05:59.760 --> 00:06:01.960]   but for having had those experiences.
[00:06:01.960 --> 00:06:06.960]   But this idea of you getting closer to something,
[00:06:06.960 --> 00:06:09.080]   to the datum of your mind,
[00:06:09.080 --> 00:06:11.300]   or to something of interest in there,
[00:06:11.300 --> 00:06:12.800]   or something that's more real,
[00:06:12.800 --> 00:06:17.400]   is ultimately undermined because there's no place
[00:06:17.400 --> 00:06:19.560]   from which you're getting closer to it.
[00:06:19.560 --> 00:06:23.440]   There's no your part of that journey, right?
[00:06:23.440 --> 00:06:28.080]   We tend to start out, whether it's in meditation
[00:06:28.080 --> 00:06:33.080]   or in any kind of self-examination or taking psychedelics,
[00:06:33.080 --> 00:06:35.960]   we start out with this default point of view
[00:06:35.960 --> 00:06:40.960]   of feeling like we're the kind of on the rider
[00:06:41.380 --> 00:06:42.580]   on the horse of consciousness,
[00:06:42.580 --> 00:06:45.340]   or we're the man in the boat
[00:06:45.340 --> 00:06:47.780]   going down the stream of consciousness, right?
[00:06:47.780 --> 00:06:49.700]   But we're so we're differentiated
[00:06:49.700 --> 00:06:54.700]   from what we know cognitively, introspectively.
[00:06:54.700 --> 00:06:58.060]   But that feeling of being differentiated,
[00:06:58.060 --> 00:06:59.460]   that feeling of being a self
[00:06:59.460 --> 00:07:01.620]   that can strategically pay attention
[00:07:01.620 --> 00:07:03.260]   to some contents of consciousness
[00:07:03.260 --> 00:07:06.640]   is what it's like to be identified
[00:07:06.640 --> 00:07:09.420]   with some part of the stream of thought
[00:07:09.420 --> 00:07:10.840]   that's going uninspected, right?
[00:07:10.840 --> 00:07:13.540]   Like it's a false point of view.
[00:07:13.540 --> 00:07:16.700]   And when you see that and cut through that,
[00:07:16.700 --> 00:07:21.580]   then this sense of this notion of going deeper
[00:07:21.580 --> 00:07:25.460]   kind of breaks apart because really there is no depth.
[00:07:25.460 --> 00:07:27.180]   Ultimately, everything is right on the surface.
[00:07:27.180 --> 00:07:28.980]   Everything, there's no center to consciousness.
[00:07:28.980 --> 00:07:30.420]   There's just consciousness and its contents.
[00:07:30.420 --> 00:07:33.120]   And those contents can change vastly.
[00:07:33.120 --> 00:07:36.700]   Again, if you drop acid, the contents change.
[00:07:37.900 --> 00:07:42.600]   But in some sense, that doesn't represent
[00:07:42.600 --> 00:07:45.800]   a position of depth versus,
[00:07:45.800 --> 00:07:49.200]   the continuum of depth versus surface has broken apart.
[00:07:49.200 --> 00:07:51.520]   - So you're taking as a starting point
[00:07:51.520 --> 00:07:54.280]   that there is a horse called consciousness
[00:07:54.280 --> 00:07:55.740]   and you're riding it.
[00:07:55.740 --> 00:07:57.680]   And the actual riding is very shallow.
[00:07:57.680 --> 00:07:59.720]   This is all surface.
[00:07:59.720 --> 00:08:02.520]   So let me ask about that horse.
[00:08:02.520 --> 00:08:04.160]   What's up with the horse?
[00:08:04.160 --> 00:08:06.220]   What is consciousness?
[00:08:07.280 --> 00:08:09.780]   From where does it emerge?
[00:08:09.780 --> 00:08:13.020]   How fundamental is it to the physics of reality?
[00:08:13.020 --> 00:08:16.560]   How fundamental is it to what it means to be human?
[00:08:16.560 --> 00:08:18.620]   And I'm just asking for a friend
[00:08:18.620 --> 00:08:20.180]   so that we can build it
[00:08:20.180 --> 00:08:22.640]   in our artificial intelligence systems.
[00:08:22.640 --> 00:08:25.660]   - Yeah, well, that remains to be seen
[00:08:25.660 --> 00:08:30.180]   if we will build it purposefully or just by accident.
[00:08:30.180 --> 00:08:34.360]   It's a major ethical problem potentially.
[00:08:35.620 --> 00:08:39.120]   That, I mean, my concern here is that we may in fact
[00:08:39.120 --> 00:08:44.020]   build artificial intelligence that passes the Turing test,
[00:08:44.020 --> 00:08:47.040]   which we begin to treat not only as super intelligent
[00:08:47.040 --> 00:08:50.880]   because it obviously is and demonstrates that,
[00:08:50.880 --> 00:08:53.160]   but we begin to treat it as conscious
[00:08:53.160 --> 00:08:54.900]   because it will seem conscious.
[00:08:54.900 --> 00:08:56.800]   We will have built it to seem conscious.
[00:08:56.800 --> 00:09:01.400]   And unless we understand exactly how consciousness emerges
[00:09:01.400 --> 00:09:04.920]   from physics, we won't actually know
[00:09:04.920 --> 00:09:06.320]   that these systems are conscious.
[00:09:06.320 --> 00:09:07.920]   We'll just, they may say,
[00:09:07.920 --> 00:09:11.000]   "Listen, you can't turn me off 'cause that's a murder."
[00:09:11.000 --> 00:09:15.280]   And we will be convinced by that dialogue
[00:09:15.280 --> 00:09:18.520]   because we will, just in the extreme case,
[00:09:18.520 --> 00:09:20.780]   who knows when we'll get there.
[00:09:20.780 --> 00:09:25.780]   But if we build something like perfectly humanoid robots
[00:09:25.780 --> 00:09:27.520]   that are more intelligent than we are,
[00:09:27.520 --> 00:09:30.280]   so we're basically in a Westworld-like situation,
[00:09:30.280 --> 00:09:33.160]   there's no way we're going to withhold
[00:09:33.160 --> 00:09:35.600]   an attribution of consciousness from those machines.
[00:09:35.600 --> 00:09:36.880]   They're just going to seem,
[00:09:36.880 --> 00:09:38.960]   they're just going to advertise their consciousness
[00:09:38.960 --> 00:09:41.940]   in every glance and every utterance.
[00:09:41.940 --> 00:09:47.360]   But we won't know, and we won't know in some deeper sense
[00:09:47.360 --> 00:09:50.500]   than we can be skeptical of the consciousness
[00:09:50.500 --> 00:09:51.340]   of other people.
[00:09:51.340 --> 00:09:52.520]   I mean, someone could roll that back and say,
[00:09:52.520 --> 00:09:54.280]   "Well, I don't know that you're conscious
[00:09:54.280 --> 00:09:55.300]   "or you don't know that I'm conscious.
[00:09:55.300 --> 00:09:57.120]   "We're just passing the Turing test for one another."
[00:09:57.120 --> 00:10:02.120]   But that kind of solipsism isn't justified biologically
[00:10:02.200 --> 00:10:05.640]   or we just, anything we understand
[00:10:05.640 --> 00:10:07.280]   about the mind biologically suggests
[00:10:07.280 --> 00:10:12.280]   that you and I are part of the same roll of the dice
[00:10:12.280 --> 00:10:18.000]   in terms of how intelligent and conscious systems emerged
[00:10:18.000 --> 00:10:21.880]   in the wetware of brains like ours, right?
[00:10:21.880 --> 00:10:24.160]   So it's not parsimonious for me to think
[00:10:24.160 --> 00:10:26.080]   that I might be the only conscious person
[00:10:26.080 --> 00:10:27.840]   or even the only conscious primate.
[00:10:27.840 --> 00:10:30.580]   I would argue it's not parsimonious
[00:10:30.580 --> 00:10:33.920]   to withhold consciousness from other apes
[00:10:33.920 --> 00:10:36.520]   and even other mammals ultimately.
[00:10:36.520 --> 00:10:38.400]   And once you get beyond the mammals,
[00:10:38.400 --> 00:10:41.400]   then my intuitions are not really clear.
[00:10:41.400 --> 00:10:44.980]   The question of how it emerges is genuinely uncertain.
[00:10:44.980 --> 00:10:48.120]   And ultimately, the question of whether it emerges
[00:10:48.120 --> 00:10:49.320]   is still uncertain.
[00:10:49.320 --> 00:10:54.120]   You can, it's not fashionable to think this,
[00:10:54.120 --> 00:10:55.760]   but you can certainly argue
[00:10:55.760 --> 00:10:59.640]   that consciousness might be a fundamental principle
[00:10:59.640 --> 00:11:02.200]   of matter that doesn't emerge
[00:11:02.200 --> 00:11:04.080]   on the basis of information processing,
[00:11:04.080 --> 00:11:08.640]   even though everything else that we recognize
[00:11:08.640 --> 00:11:11.400]   about ourselves as minds almost certainly does emerge.
[00:11:11.400 --> 00:11:13.160]   You know, like an ability to process language,
[00:11:13.160 --> 00:11:15.740]   that clearly is a matter of information processing
[00:11:15.740 --> 00:11:17.520]   because you can disrupt that process
[00:11:17.520 --> 00:11:22.520]   in ways that is just so clear.
[00:11:22.520 --> 00:11:26.920]   And the problem, the confound with consciousness
[00:11:26.920 --> 00:11:30.640]   is that, yes, we can seem to interrupt consciousness.
[00:11:30.640 --> 00:11:32.960]   I mean, you can give someone general anesthesia
[00:11:32.960 --> 00:11:35.160]   and then you wake them up and you ask them,
[00:11:35.160 --> 00:11:36.000]   "Well, what was that like?"
[00:11:36.000 --> 00:11:38.600]   And they say, "Nothing, I don't remember anything."
[00:11:38.600 --> 00:11:43.600]   But it's hard to differentiate a mere failure of memory
[00:11:43.600 --> 00:11:49.080]   from a genuine interruption in consciousness.
[00:11:49.080 --> 00:11:51.720]   Whereas it's not, with interrupting speech,
[00:11:51.720 --> 00:11:53.540]   you know, we know when we've done it.
[00:11:53.540 --> 00:11:57.640]   And it's just obvious that, you know,
[00:11:57.640 --> 00:11:59.440]   you disrupt the right neural circuits
[00:11:59.440 --> 00:12:01.840]   and, you know, you've disrupted speech.
[00:12:01.840 --> 00:12:03.400]   - So if you had to bet all your money
[00:12:03.400 --> 00:12:04.640]   on one camp or the other,
[00:12:04.640 --> 00:12:09.000]   would you say, do you err on the side of panpsychism,
[00:12:09.000 --> 00:12:11.860]   where consciousness is really fundamental
[00:12:11.860 --> 00:12:16.760]   to all of reality, or more on the other side,
[00:12:16.760 --> 00:12:20.200]   which is like, it's a nice little side effect,
[00:12:20.200 --> 00:12:23.600]   a useful, like, hack for us humans to survive?
[00:12:23.600 --> 00:12:26.220]   On that spectrum, where do you land
[00:12:26.220 --> 00:12:27.860]   when you think about consciousness,
[00:12:27.860 --> 00:12:30.340]   especially from an engineering perspective?
[00:12:30.340 --> 00:12:33.160]   - I'm truly agnostic on this point.
[00:12:33.160 --> 00:12:35.300]   I mean, I think I'm, you know,
[00:12:35.300 --> 00:12:37.780]   it's kind of in coin toss mode for me.
[00:12:37.780 --> 00:12:42.780]   I don't know, and panpsychism is not so compelling to me.
[00:12:42.780 --> 00:12:46.900]   Again, it just seems unfalsifiable.
[00:12:46.900 --> 00:12:49.020]   I wouldn't know how the universe would be different
[00:12:49.020 --> 00:12:50.700]   if panpsychism were true.
[00:12:50.700 --> 00:12:52.060]   I mean, just to remind people,
[00:12:52.060 --> 00:12:54.500]   panpsychism is this idea that consciousness
[00:12:54.500 --> 00:12:57.260]   may be pushed all the way down
[00:12:57.260 --> 00:12:59.500]   into the most fundamental constituents of matter.
[00:12:59.500 --> 00:13:01.080]   So there might be something that it's like
[00:13:01.080 --> 00:13:05.100]   to be an electron or, you know, a quark,
[00:13:05.100 --> 00:13:08.940]   but then you wouldn't expect anything to be different
[00:13:08.940 --> 00:13:11.420]   at the macro scale,
[00:13:11.420 --> 00:13:13.860]   or at least I wouldn't expect anything to be different.
[00:13:13.860 --> 00:13:16.700]   So it may be unfalsifiable.
[00:13:16.700 --> 00:13:20.460]   It just might be that reality is not something
[00:13:20.460 --> 00:13:26.060]   we're as in touch with as we think we are,
[00:13:26.060 --> 00:13:30.400]   and that if that is base layer
[00:13:30.400 --> 00:13:32.460]   to kind of break it into mind and matter
[00:13:32.460 --> 00:13:37.300]   as we've done ontologically is to misconstrue it, right?
[00:13:37.300 --> 00:13:40.900]   I mean, there could be some kind of neutral monism
[00:13:40.900 --> 00:13:43.860]   at the bottom, and this idea doesn't originate with me.
[00:13:43.860 --> 00:13:47.220]   This goes all the way back to Bertrand Russell
[00:13:47.220 --> 00:13:50.860]   and others, you know, 100 plus years ago,
[00:13:50.860 --> 00:13:53.940]   but I just feel like the concepts we're using
[00:13:53.940 --> 00:13:58.940]   to divide consciousness and matter
[00:13:58.940 --> 00:14:02.260]   may in fact be part of our problem, right?
[00:14:02.260 --> 00:14:05.940]   Where the rubber hits the road psychologically here
[00:14:05.940 --> 00:14:08.780]   are things like, well, what is death, right?
[00:14:08.780 --> 00:14:12.500]   Like, any expectation that we survive death
[00:14:12.500 --> 00:14:14.540]   or any part of us survives death,
[00:14:14.540 --> 00:14:19.540]   that really seems to be many people's concern here.
[00:14:19.540 --> 00:14:23.140]   - Well, I tend to believe, just as a small little tangent,
[00:14:23.140 --> 00:14:24.660]   like I'm with Ernest Becker on this,
[00:14:24.660 --> 00:14:27.700]   that there's some, it's interesting to think
[00:14:27.700 --> 00:14:29.240]   about death and consciousness,
[00:14:29.240 --> 00:14:32.420]   which one is the chicken, which one is the egg,
[00:14:32.420 --> 00:14:34.620]   because it feels like death could be the very thing,
[00:14:34.620 --> 00:14:36.900]   like our knowledge of mortality could be the very thing
[00:14:36.900 --> 00:14:38.460]   that creates the consciousness.
[00:14:38.460 --> 00:14:41.460]   - Yeah, well, then you're using consciousness
[00:14:41.460 --> 00:14:43.660]   differently than I am.
[00:14:43.660 --> 00:14:47.100]   I mean, so for me, consciousness is just the fact
[00:14:47.100 --> 00:14:49.980]   that the lights are on at all,
[00:14:49.980 --> 00:14:53.260]   that there's an experiential quality to anything.
[00:14:53.260 --> 00:14:56.300]   So much of the processing that's happening
[00:14:56.300 --> 00:14:59.940]   in our brains right now certainly seems
[00:14:59.940 --> 00:15:01.940]   to be happening in the dark, right?
[00:15:01.940 --> 00:15:06.260]   Like, it's not associated with this qualitative sense
[00:15:06.260 --> 00:15:08.820]   that there's something that it's like to be that part
[00:15:08.820 --> 00:15:11.680]   of the mind doing that mental thing.
[00:15:11.680 --> 00:15:16.980]   But for other parts, the lights are on,
[00:15:16.980 --> 00:15:20.320]   and we can talk about, and whether we talk about it or not,
[00:15:20.320 --> 00:15:25.320]   we can feel directly that there's something
[00:15:25.320 --> 00:15:27.020]   that it's like to be us.
[00:15:27.020 --> 00:15:29.740]   There's something that seems to be happening, right?
[00:15:29.740 --> 00:15:34.260]   And the seeming, in our case, is broken into vision
[00:15:34.260 --> 00:15:36.540]   and hearing and proprioception,
[00:15:36.540 --> 00:15:41.540]   and taste and smell, and thought and emotion.
[00:15:41.540 --> 00:15:45.080]   And there are the contents of consciousness
[00:15:45.080 --> 00:15:50.420]   that we are familiar with,
[00:15:50.420 --> 00:15:54.340]   and that we can have direct access to in any present moment
[00:15:54.340 --> 00:15:56.440]   that when we're, quote, conscious.
[00:15:56.440 --> 00:16:00.100]   And even if we're confused about them,
[00:16:00.100 --> 00:16:02.260]   even if we're asleep and dreaming,
[00:16:02.260 --> 00:16:04.020]   and we're just not a lucid dream,
[00:16:04.020 --> 00:16:07.780]   we're just totally confused about our circumstance,
[00:16:07.780 --> 00:16:11.380]   what you can't say is that we're confused
[00:16:11.380 --> 00:16:12.460]   about consciousness.
[00:16:12.460 --> 00:16:14.620]   Like, you can't say that consciousness itself
[00:16:14.620 --> 00:16:18.900]   might be an illusion, because on this account,
[00:16:18.900 --> 00:16:22.100]   it just means that things seem any way at all.
[00:16:22.100 --> 00:16:23.300]   I mean, even like if this, you know,
[00:16:23.300 --> 00:16:26.380]   it seems to me that I'm seeing a cup on the table.
[00:16:26.380 --> 00:16:27.500]   Now, I could be wrong about that.
[00:16:27.500 --> 00:16:28.480]   It could be a hologram.
[00:16:28.480 --> 00:16:29.740]   I could be asleep and dreaming.
[00:16:29.740 --> 00:16:31.520]   I could be hallucinating.
[00:16:31.520 --> 00:16:35.100]   But the seeming part isn't really up for grabs
[00:16:35.100 --> 00:16:37.580]   in terms of being an illusion.
[00:16:37.580 --> 00:16:41.620]   It's not, something seems to be happening.
[00:16:41.620 --> 00:16:45.220]   And that seeming is the context
[00:16:45.220 --> 00:16:49.980]   in which every other thing we can notice about ourselves
[00:16:49.980 --> 00:16:50.820]   can be noticed.
[00:16:50.820 --> 00:16:53.500]   And it's also the context in which certain illusions
[00:16:53.500 --> 00:16:55.420]   can be cut through, because we're not,
[00:16:55.420 --> 00:16:57.820]   we can be wrong about what it's like to be us,
[00:16:57.820 --> 00:17:01.960]   and we can, I'm not saying we're incorrigible
[00:17:01.960 --> 00:17:04.440]   with respect to our claims
[00:17:04.440 --> 00:17:05.680]   about the nature of our experience,
[00:17:05.680 --> 00:17:10.100]   but for instance, many people feel like they have a self,
[00:17:10.100 --> 00:17:11.840]   and they feel like it has free will,
[00:17:11.840 --> 00:17:14.320]   and I'm quite sure at this point
[00:17:14.320 --> 00:17:15.240]   that they're wrong about that,
[00:17:15.240 --> 00:17:19.880]   and that you can cut through those experiences,
[00:17:19.880 --> 00:17:22.120]   and then things seem a different way, right?
[00:17:22.120 --> 00:17:25.000]   So it's not that things don't,
[00:17:25.000 --> 00:17:26.480]   there aren't discoveries to be made there,
[00:17:26.480 --> 00:17:28.180]   and assumptions to be overturned,
[00:17:28.180 --> 00:17:33.180]   but this kind of consciousness is something
[00:17:33.180 --> 00:17:38.180]   that I would think, it doesn't just come online
[00:17:38.180 --> 00:17:39.660]   when we get language.
[00:17:39.660 --> 00:17:42.460]   It doesn't just come online when we form a concept of death,
[00:17:42.460 --> 00:17:45.220]   or the finiteness of life.
[00:17:45.220 --> 00:17:48.200]   It doesn't require a sense of self, right?
[00:17:48.200 --> 00:17:50.380]   So it doesn't, it's prior
[00:17:50.380 --> 00:17:52.480]   to a differentiating self and other.
[00:17:54.100 --> 00:17:57.680]   And I wouldn't even think it's necessarily limited
[00:17:57.680 --> 00:18:02.680]   to people, I do think probably any mammal has this,
[00:18:02.680 --> 00:18:07.440]   but certainly if you're going to presuppose
[00:18:07.440 --> 00:18:11.960]   that something about our brains is producing this, right,
[00:18:11.960 --> 00:18:15.560]   and that's a very safe assumption,
[00:18:15.560 --> 00:18:18.260]   even though we can't,
[00:18:18.260 --> 00:18:20.560]   even though you can argue the jury's still out
[00:18:20.560 --> 00:18:23.360]   to some degree, then it's very hard
[00:18:23.360 --> 00:18:26.600]   to draw a principled line between us and chimps,
[00:18:26.600 --> 00:18:30.120]   or chimps and rats even in the end,
[00:18:30.120 --> 00:18:33.120]   given the underlying neural similarities.
[00:18:33.120 --> 00:18:35.960]   So, and I don't know, phylogenetically,
[00:18:35.960 --> 00:18:38.600]   I don't know how far back to push that.
[00:18:38.600 --> 00:18:41.720]   There are people who think single cells might be conscious,
[00:18:41.720 --> 00:18:43.440]   or that flies are certainly conscious.
[00:18:43.440 --> 00:18:47.960]   They've got something like 100,000 neurons in their brains.
[00:18:47.960 --> 00:18:51.080]   I mean, it's just, there's a lot going on,
[00:18:51.080 --> 00:18:53.160]   even in a fly, right?
[00:18:53.160 --> 00:18:55.480]   But I don't have intuitions about that.
[00:18:55.480 --> 00:18:56.800]   - But it's not, in your sense,
[00:18:56.800 --> 00:18:58.240]   an illusion you can cut through.
[00:18:58.240 --> 00:19:02.120]   I mean, to push back, the alternative version could be
[00:19:02.120 --> 00:19:06.560]   it is an illusion constructed by, just by humans.
[00:19:06.560 --> 00:19:08.160]   I'm not sure I believe this,
[00:19:08.160 --> 00:19:10.160]   but it, in part of me, hopes this is true
[00:19:10.160 --> 00:19:12.520]   because it makes it easier to engineer,
[00:19:12.520 --> 00:19:16.560]   is that humans are able to contemplate their mortality,
[00:19:16.560 --> 00:19:21.480]   and that contemplation in itself creates consciousness.
[00:19:21.480 --> 00:19:24.200]   That, like, the rich lights-on experience.
[00:19:24.200 --> 00:19:26.400]   So the lights don't actually even turn on
[00:19:26.400 --> 00:19:28.440]   in the way that you're describing
[00:19:28.440 --> 00:19:31.880]   until after birth in that construction.
[00:19:31.880 --> 00:19:34.960]   So do you think it's possible that that is the case,
[00:19:34.960 --> 00:19:39.320]   that it is a sort of construct of the way we deal,
[00:19:39.320 --> 00:19:42.840]   almost like a social tool to deal with the reality
[00:19:42.840 --> 00:19:45.500]   of the world, a social interaction with other humans?
[00:19:45.500 --> 00:19:49.040]   Or is, 'cause you're saying the complete opposite,
[00:19:49.040 --> 00:19:53.120]   which is it's like fundamental to single-cell organisms
[00:19:53.120 --> 00:19:54.880]   and trees and so on.
[00:19:54.880 --> 00:19:57.840]   - Right, well, yeah, so I don't know how far down to push it.
[00:19:57.840 --> 00:20:00.520]   I don't have intuitions that single cells
[00:20:00.520 --> 00:20:04.040]   are likely to be conscious, but they might be.
[00:20:04.040 --> 00:20:06.680]   And I just, again, it could be unfalsifiable.
[00:20:06.680 --> 00:20:10.480]   But as far as babies not being conscious,
[00:20:10.480 --> 00:20:12.040]   or you don't become conscious
[00:20:12.040 --> 00:20:14.520]   until you can recognize yourself in a mirror
[00:20:14.520 --> 00:20:17.280]   or have a conversation or treat other people.
[00:20:17.280 --> 00:20:19.760]   First of all, babies treat other people as others
[00:20:19.760 --> 00:20:24.680]   far earlier than we have traditionally
[00:20:24.680 --> 00:20:25.800]   given them credit for.
[00:20:25.800 --> 00:20:29.120]   And they certainly do it before they have language, right?
[00:20:29.120 --> 00:20:33.920]   So it's got to precede language to some degree.
[00:20:33.920 --> 00:20:36.720]   And you can interrogate this for yourself
[00:20:36.720 --> 00:20:40.300]   because you can put yourself in various states
[00:20:40.300 --> 00:20:44.720]   that are rather obviously not linguistic.
[00:20:46.280 --> 00:20:48.760]   Meditation allows you to do this.
[00:20:48.760 --> 00:20:50.480]   You can certainly do it with psychedelics
[00:20:50.480 --> 00:20:54.120]   where it's just your capacity for language
[00:20:54.120 --> 00:20:58.760]   has been obliterated and yet you're all too conscious.
[00:20:58.760 --> 00:21:03.760]   In fact, I think you could make a stronger argument
[00:21:03.760 --> 00:21:09.520]   for things running the other way,
[00:21:09.520 --> 00:21:12.720]   that there's something about language
[00:21:12.720 --> 00:21:16.360]   and conceptual thought that is eliminative
[00:21:16.360 --> 00:21:18.280]   of conscious experience.
[00:21:18.280 --> 00:21:23.280]   That we're potentially much more conscious of data,
[00:21:23.280 --> 00:21:27.080]   sense data and everything else than we tend to be.
[00:21:27.080 --> 00:21:29.240]   And we have trimmed it down
[00:21:29.240 --> 00:21:33.640]   based on how we have acquired concepts.
[00:21:33.640 --> 00:21:35.940]   And so like when I walk into a room like this,
[00:21:35.940 --> 00:21:38.780]   I know I'm walking into a room,
[00:21:38.780 --> 00:21:41.960]   I have certain expectations of what is in a room.
[00:21:41.960 --> 00:21:45.560]   I would be very surprised to see wild animals in here
[00:21:45.560 --> 00:21:47.560]   or a waterfall or let me say,
[00:21:47.560 --> 00:21:51.040]   there are things I'm not expecting,
[00:21:51.040 --> 00:21:53.600]   but I can know I'm not expecting them
[00:21:53.600 --> 00:21:54.880]   or I'm expecting their absence
[00:21:54.880 --> 00:21:57.360]   because of my capacity to be surprised
[00:21:57.360 --> 00:22:01.800]   once I walk into a room and I see a live gorilla or whatever.
[00:22:01.800 --> 00:22:05.920]   So there's structure there that we have put in place
[00:22:05.920 --> 00:22:08.960]   based on all of our conceptual learning
[00:22:08.960 --> 00:22:11.440]   and language learning.
[00:22:11.440 --> 00:22:15.080]   And it causes us not to,
[00:22:15.080 --> 00:22:16.320]   and one of the things that happens
[00:22:16.320 --> 00:22:17.400]   when you take psychedelics
[00:22:17.400 --> 00:22:21.760]   and you just look as though for the first time at anything,
[00:22:21.760 --> 00:22:26.760]   it becomes incredibly overloaded with,
[00:22:26.760 --> 00:22:28.560]   it can become overloaded with meaning
[00:22:28.560 --> 00:22:33.560]   and just the torrents of sense data that are coming in
[00:22:33.560 --> 00:22:39.120]   in even the most ordinary circumstances
[00:22:39.120 --> 00:22:40.780]   can become overwhelming for people.
[00:22:40.780 --> 00:22:45.600]   And that tends to just obliterate one's capacity
[00:22:45.600 --> 00:22:47.800]   to capture any of it linguistically.
[00:22:47.800 --> 00:22:49.440]   And as you're coming down, right,
[00:22:49.440 --> 00:22:50.560]   have you done psychedelics?
[00:22:50.560 --> 00:22:52.240]   Have you ever done acid or?
[00:22:52.240 --> 00:22:55.660]   - Not acid, mushroom, and that's it.
[00:22:55.660 --> 00:23:00.880]   And also edibles, but there's some psychedelic properties
[00:23:00.880 --> 00:23:04.240]   to them, but yeah, mushrooms,
[00:23:04.240 --> 00:23:07.600]   several times and always had an incredible experience.
[00:23:07.600 --> 00:23:09.420]   Exactly the kind of experience you're referring to,
[00:23:09.420 --> 00:23:12.400]   which is if it's true that language
[00:23:12.400 --> 00:23:15.380]   constrains our experience,
[00:23:15.380 --> 00:23:19.240]   it felt like I was removing some of the constraints.
[00:23:19.240 --> 00:23:20.080]   - Right.
[00:23:20.080 --> 00:23:21.520]   - Because even just the most basic things
[00:23:21.520 --> 00:23:22.720]   were beautiful in the way
[00:23:22.720 --> 00:23:25.280]   that I wasn't able to appreciate previously,
[00:23:25.280 --> 00:23:27.420]   like trees and nature and so on.
[00:23:27.420 --> 00:23:30.640]   - Yeah, and the experience of coming down
[00:23:30.640 --> 00:23:36.640]   is an experience of encountering the futility
[00:23:37.600 --> 00:23:42.600]   of capturing what you just saw a moment ago in words, right?
[00:23:42.600 --> 00:23:47.700]   Like, especially if you have any part of your self-concept
[00:23:47.700 --> 00:23:50.740]   and your ego program is to be able
[00:23:50.740 --> 00:23:51.980]   to capture things in words.
[00:23:51.980 --> 00:23:55.620]   I mean, if you're a writer or a poet or a scientist,
[00:23:55.620 --> 00:23:58.540]   or someone who wants to just encapsulate
[00:23:58.540 --> 00:24:01.020]   the profundity of what just happened,
[00:24:01.020 --> 00:24:06.900]   the total fatuousness of that enterprise
[00:24:07.900 --> 00:24:12.660]   when you have taken a whopping dose of psychedelics
[00:24:12.660 --> 00:24:17.660]   and you begin to even gesture at describing it to yourself,
[00:24:17.660 --> 00:24:22.460]   so that you could describe it to others,
[00:24:22.460 --> 00:24:27.460]   it's like trying to thread a needle using your elbows.
[00:24:27.460 --> 00:24:30.180]   I mean, it's like you're trying something that can't,
[00:24:30.180 --> 00:24:34.780]   it's like the mere gesture proves its impossibility.
[00:24:34.780 --> 00:24:39.620]   And it's, so yeah, so that, I mean,
[00:24:39.620 --> 00:24:42.300]   for me, that suggests just empirically
[00:24:42.300 --> 00:24:44.300]   on the first person side that it's possible
[00:24:44.300 --> 00:24:47.220]   to put yourself in a condition where
[00:24:47.220 --> 00:24:52.220]   it's clearly not about language structuring your experience
[00:24:52.220 --> 00:24:56.520]   and you're having much more experience than you tend to.
[00:24:56.520 --> 00:24:58.260]   So it's the primacy of,
[00:24:58.260 --> 00:25:00.500]   language is primary for some things,
[00:25:00.500 --> 00:25:05.500]   but it's certainly primary for certain kinds of concepts
[00:25:05.500 --> 00:25:11.140]   and certain kinds of semantic understandings of the world.
[00:25:11.140 --> 00:25:16.140]   But it's clearly more to mind than the conversation
[00:25:16.140 --> 00:25:21.420]   we're having with ourselves or that we can have with others.
[00:25:21.420 --> 00:25:25.880]   - Can we go to that world of psychedelics for a bit?
[00:25:25.880 --> 00:25:27.180]   - Sure.
[00:25:27.180 --> 00:25:30.020]   - What do you think, so Joe Rogan,
[00:25:30.020 --> 00:25:32.500]   apparently, and many others meet,
[00:25:32.500 --> 00:25:35.900]   apparently elves when they, on DMT.
[00:25:35.900 --> 00:25:40.260]   A lot of people report this kind of creatures
[00:25:40.260 --> 00:25:43.100]   that they see, and again, it's probably the failure
[00:25:43.100 --> 00:25:44.840]   of language to describe that experience.
[00:25:44.840 --> 00:25:46.860]   But DMT is an interesting one.
[00:25:46.860 --> 00:25:50.180]   There's, as you're aware, there's a bunch of studies
[00:25:50.180 --> 00:25:53.180]   going on on psychedelics, currently MDMA,
[00:25:55.340 --> 00:26:00.340]   psilocybin, and John Hopkins, and much other places.
[00:26:00.340 --> 00:26:07.100]   But DMT, they all speak of as like some extra
[00:26:07.100 --> 00:26:09.780]   super level of a psychedelic.
[00:26:09.780 --> 00:26:13.140]   Yeah, do you have a sense of where it is our mind goes
[00:26:13.140 --> 00:26:18.980]   on psychedelics, but in DMT especially?
[00:26:18.980 --> 00:26:22.580]   - Well, unfortunately, I haven't taken DMT, so I can't--
[00:26:22.580 --> 00:26:23.900]   - Unfortunately or fortunately?
[00:26:23.900 --> 00:26:25.420]   - Unfortunately, yeah.
[00:26:25.420 --> 00:26:28.740]   Although I presume it's in my body as it is
[00:26:28.740 --> 00:26:33.740]   in everyone's brain and in many, many plants, apparently.
[00:26:33.740 --> 00:26:38.460]   But I've wanted to take it, I haven't had an opportunity
[00:26:38.460 --> 00:26:40.700]   that presented itself that where it was obviously
[00:26:40.700 --> 00:26:42.460]   the right thing for me to be doing.
[00:26:42.460 --> 00:26:47.420]   But for those who don't know, DMT is often touted
[00:26:47.420 --> 00:26:49.500]   as the most intense psychedelic,
[00:26:49.500 --> 00:26:51.420]   and also the shortest acting.
[00:26:51.420 --> 00:26:54.740]   You smoke it and it's basically a 10 minute experience,
[00:26:54.740 --> 00:26:59.220]   or a three minute experience within like a 10 minute window
[00:26:59.220 --> 00:27:04.060]   that when you're really down after 10 minutes or so.
[00:27:04.060 --> 00:27:09.460]   And Terence McKenna was a big proponent of DMT,
[00:27:09.460 --> 00:27:12.260]   that was the center of the bullseye for him,
[00:27:12.260 --> 00:27:13.620]   psychedelically apparently.
[00:27:13.620 --> 00:27:20.060]   And it is characterized, it seems, for many people
[00:27:20.060 --> 00:27:23.020]   by this phenomenon, which is unlike virtually
[00:27:23.020 --> 00:27:25.900]   any other psychedelic experience, which is your,
[00:27:25.900 --> 00:27:30.740]   it's not just your perception being broadened or changed,
[00:27:30.740 --> 00:27:35.500]   it's you, according to Terence McKenna,
[00:27:35.500 --> 00:27:38.820]   feeling fairly unchanged, but catapulted
[00:27:38.820 --> 00:27:41.020]   into a different circumstance.
[00:27:41.020 --> 00:27:45.600]   You have been shot elsewhere and find yourself
[00:27:45.600 --> 00:27:48.940]   in relationship to other entities of some kind.
[00:27:48.940 --> 00:27:51.460]   So the place is populated with things
[00:27:51.460 --> 00:27:54.140]   that seem not to be your mind.
[00:27:54.140 --> 00:27:56.260]   - So it does feel like travel to another place,
[00:27:56.260 --> 00:27:58.140]   because you are unchanged yourself.
[00:27:58.140 --> 00:28:00.480]   - According, again, I just have this on the authority
[00:28:00.480 --> 00:28:03.260]   of the people who have described their experience,
[00:28:03.260 --> 00:28:05.580]   but it sounds like it's pretty common.
[00:28:05.580 --> 00:28:07.060]   It sounds like it's pretty common for people
[00:28:07.060 --> 00:28:08.500]   not to have the full experience,
[00:28:08.500 --> 00:28:11.620]   because it's apparently pretty unpleasant to smoke.
[00:28:11.620 --> 00:28:15.020]   So it's like getting enough on board in order to get shot
[00:28:15.020 --> 00:28:19.380]   out of the cannon and land among the,
[00:28:19.380 --> 00:28:25.420]   what McKenna called self-transforming machine elves
[00:28:25.420 --> 00:28:31.420]   that appeared to him like jeweled FabergÃ© egg-like,
[00:28:31.420 --> 00:28:34.580]   self-drippling basketballs that were handing him
[00:28:34.580 --> 00:28:39.480]   completely uninterpretable reams of profound knowledge.
[00:28:39.480 --> 00:28:44.540]   It's an experience I haven't had,
[00:28:44.540 --> 00:28:47.700]   so I just have to accept that people have had it.
[00:28:47.700 --> 00:28:53.860]   I would just point out that our minds are clearly capable
[00:28:53.860 --> 00:28:58.860]   of producing apparent others on demand
[00:28:58.860 --> 00:29:01.940]   that are totally compelling to us, right?
[00:29:01.940 --> 00:29:04.820]   There's no limit to our ability to do that
[00:29:04.820 --> 00:29:07.980]   as anyone who's ever remembered a dream can attest.
[00:29:07.980 --> 00:29:10.220]   I mean, every night we go to sleep,
[00:29:10.220 --> 00:29:11.900]   and some of us don't remember dreams very often,
[00:29:11.900 --> 00:29:15.660]   but some dream vividly every night.
[00:29:15.660 --> 00:29:20.660]   And just think of how insane that experience is.
[00:29:20.660 --> 00:29:23.900]   I mean, you've forgotten where you were, right?
[00:29:23.900 --> 00:29:25.500]   That's the strangest part.
[00:29:25.500 --> 00:29:27.100]   I mean, this is psychosis, right?
[00:29:27.100 --> 00:29:29.500]   You have lost your mind.
[00:29:29.500 --> 00:29:34.500]   You have lost your connection to your episodic memory,
[00:29:34.500 --> 00:29:39.900]   or even your expectations that reality won't undergo
[00:29:40.220 --> 00:29:45.020]   wholesale changes a moment after you have closed your eyes.
[00:29:45.020 --> 00:29:49.140]   You're in bed, you're watching something on Netflix,
[00:29:49.140 --> 00:29:50.420]   you're waiting to fall asleep,
[00:29:50.420 --> 00:29:54.620]   and then the next thing that happens to you is impossible,
[00:29:54.620 --> 00:29:56.260]   and you're not surprised.
[00:29:56.260 --> 00:29:57.540]   You're talking to dead people,
[00:29:57.540 --> 00:29:58.900]   you're hanging out with famous people,
[00:29:58.900 --> 00:30:02.860]   you're someplace you couldn't physically be,
[00:30:02.860 --> 00:30:05.620]   you can fly, and even that's not surprising.
[00:30:06.540 --> 00:30:10.780]   You have lost your mind, but relevantly for this--
[00:30:10.780 --> 00:30:12.180]   - Or found it.
[00:30:12.180 --> 00:30:13.300]   - You found something.
[00:30:13.300 --> 00:30:14.780]   Lucid dreaming is very interesting,
[00:30:14.780 --> 00:30:17.900]   'cause then you can have the best of both circumstances,
[00:30:17.900 --> 00:30:22.900]   and then it can be kind of systematically explored.
[00:30:22.900 --> 00:30:25.380]   - But what I mean by found, just sorry to interrupt,
[00:30:25.380 --> 00:30:29.020]   is like if we take this brilliant idea
[00:30:29.020 --> 00:30:31.980]   that language constrains us, grounds us,
[00:30:31.980 --> 00:30:35.600]   language and other things of the waking world ground us,
[00:30:35.600 --> 00:30:40.600]   maybe it is that you've found the full capacity
[00:30:40.600 --> 00:30:43.380]   of your cognition when you dream,
[00:30:43.380 --> 00:30:44.740]   or when you do psychedelics.
[00:30:44.740 --> 00:30:47.780]   You're stepping outside the little human cage,
[00:30:47.780 --> 00:30:49.500]   the cage of the human condition.
[00:30:49.500 --> 00:30:52.780]   To open the door and step out and look around,
[00:30:52.780 --> 00:30:54.220]   and then go back in.
[00:30:54.220 --> 00:30:57.100]   - Well, you've definitely stepped out of something
[00:30:57.100 --> 00:30:59.420]   and into something else, but you've also lost something.
[00:30:59.420 --> 00:31:01.140]   You've lost certain capacities.
[00:31:01.140 --> 00:31:02.060]   - Memory?
[00:31:02.060 --> 00:31:04.100]   - Well, just, yeah, in this case,
[00:31:04.100 --> 00:31:07.980]   you literally didn't, you don't have enough presence of mind
[00:31:07.980 --> 00:31:11.620]   in the dreaming state, or even in the psychedelic state,
[00:31:11.620 --> 00:31:12.640]   if you take enough.
[00:31:12.640 --> 00:31:16.100]   - To do math?
[00:31:16.100 --> 00:31:17.100]   - There's no psychological,
[00:31:17.100 --> 00:31:20.300]   there's very little psychological continuity
[00:31:20.300 --> 00:31:24.040]   with your life, such that you're not surprised
[00:31:24.040 --> 00:31:28.620]   to be in the presence of someone who should be,
[00:31:28.620 --> 00:31:30.940]   you should know is dead, or you should know
[00:31:30.940 --> 00:31:33.860]   you're not likely to have met, by normal channels, right?
[00:31:33.860 --> 00:31:36.460]   You're now talking to some celebrity,
[00:31:36.460 --> 00:31:39.140]   and it turns out you're best friends, right?
[00:31:39.140 --> 00:31:40.780]   And you're not even, you have no memory
[00:31:40.780 --> 00:31:41.620]   of how you got there.
[00:31:41.620 --> 00:31:42.940]   You're like, how did you get into the room?
[00:31:42.940 --> 00:31:45.300]   You're like, did you drive to this restaurant?
[00:31:45.300 --> 00:31:47.500]   You have no memory, and none of that's surprising to you.
[00:31:47.500 --> 00:31:49.740]   So you're kind of brain damaged, in a way.
[00:31:49.740 --> 00:31:53.420]   You're not reality testing in the normal way.
[00:31:53.420 --> 00:31:55.780]   - The fascinating possibility is that
[00:31:55.780 --> 00:31:57.700]   there's probably thousands of people
[00:31:57.700 --> 00:31:59.700]   who've taken psychedelics of various forms,
[00:31:59.700 --> 00:32:03.580]   and have met Sam Harris on that journey.
[00:32:03.580 --> 00:32:05.380]   - Well, I would put it more likely in dreams,
[00:32:05.380 --> 00:32:08.060]   not, you know, 'cause with psychedelics,
[00:32:08.060 --> 00:32:11.500]   you don't tend to hallucinate in a dreamlike way.
[00:32:11.500 --> 00:32:15.580]   I mean, so DMT is giving you an experience of others,
[00:32:15.580 --> 00:32:19.340]   but it seems to be non-standard.
[00:32:19.340 --> 00:32:23.020]   It's not just like dream hallucinations.
[00:32:23.020 --> 00:32:26.500]   But to the point of, coming back to DMT,
[00:32:26.500 --> 00:32:31.220]   the people want to suggest, and Terrence McKenna
[00:32:31.220 --> 00:32:34.900]   certainly did suggest, that because these others
[00:32:34.900 --> 00:32:38.420]   are so obviously other, and they're so vivid,
[00:32:38.420 --> 00:32:39.860]   well, then they could not possibly be
[00:32:39.860 --> 00:32:42.860]   the creation of my own mind.
[00:32:42.860 --> 00:32:47.820]   But every night in dreams, you create a compelling,
[00:32:47.820 --> 00:32:49.780]   or what is to you at the time,
[00:32:49.780 --> 00:32:53.980]   a totally compelling simulacrum of another person, right?
[00:32:53.980 --> 00:32:58.980]   And that's, that just proves the mind
[00:32:58.980 --> 00:33:00.940]   is capable of doing it.
[00:33:00.940 --> 00:33:04.340]   Now, the phenomenon of lucid dreaming
[00:33:04.340 --> 00:33:05.780]   shows that the mind isn't capable
[00:33:05.780 --> 00:33:08.580]   of doing everything you think it might be capable of,
[00:33:08.580 --> 00:33:10.020]   even in that space.
[00:33:10.020 --> 00:33:14.260]   So one of the things that people have discovered
[00:33:14.260 --> 00:33:17.500]   in lucid dreams, and I haven't done a lot of lucid dreaming,
[00:33:17.500 --> 00:33:20.020]   so I can't confirm all of this,
[00:33:20.020 --> 00:33:21.620]   but I can confirm some of it.
[00:33:21.620 --> 00:33:28.140]   Apparently, in every house, in every room
[00:33:28.140 --> 00:33:31.540]   in the mansion of dreams, all light switches
[00:33:31.540 --> 00:33:32.780]   are dimmer switches.
[00:33:32.780 --> 00:33:35.980]   Like if you go into a dark room and flip on the light,
[00:33:35.980 --> 00:33:37.940]   it gradually comes up.
[00:33:37.940 --> 00:33:41.340]   It doesn't come up instantly on demand,
[00:33:41.340 --> 00:33:43.940]   because apparently this is covering,
[00:33:43.940 --> 00:33:48.940]   for the brain's inability to produce from a standing start
[00:33:48.940 --> 00:33:52.100]   visually rich imagery on demand.
[00:33:52.100 --> 00:33:54.580]   So I haven't confirmed that, but that was,
[00:33:54.580 --> 00:33:57.540]   people have done research on lucid dreaming,
[00:33:57.540 --> 00:33:59.780]   to claim that it's all dimmer switches.
[00:33:59.780 --> 00:34:03.020]   But one thing I have noticed,
[00:34:03.020 --> 00:34:05.820]   and people can check this out,
[00:34:05.820 --> 00:34:10.060]   is that in a dream, if you look at text,
[00:34:10.060 --> 00:34:14.900]   a page of text, or a sign, or a television
[00:34:14.900 --> 00:34:17.340]   that has text on it, and then you turn away
[00:34:17.340 --> 00:34:18.780]   and you look back at that text,
[00:34:18.780 --> 00:34:20.980]   the text will have changed.
[00:34:20.980 --> 00:34:24.460]   Right, there's no, the total is just a chronic instability,
[00:34:24.460 --> 00:34:28.900]   graphical instability of text in the dream state.
[00:34:28.900 --> 00:34:31.100]   And I don't know if that, maybe that's,
[00:34:31.100 --> 00:34:33.220]   someone can confirm that that's not true for them,
[00:34:33.220 --> 00:34:34.980]   but that's, whenever I've checked that out,
[00:34:34.980 --> 00:34:35.820]   that has been true for me.
[00:34:35.820 --> 00:34:39.060]   - So it keeps generating it, like real time,
[00:34:39.060 --> 00:34:40.620]   from a video game perspective.
[00:34:40.620 --> 00:34:43.860]   - Yeah, it's rendering, it's re-rendering it,
[00:34:43.860 --> 00:34:44.780]   for some reason.
[00:34:44.780 --> 00:34:46.020]   - What's interesting, I actually,
[00:34:46.020 --> 00:34:49.940]   I don't know how I found myself in this sets of,
[00:34:49.940 --> 00:34:51.820]   that part of the internet,
[00:34:51.820 --> 00:34:53.780]   but there's quite a lot of discussion
[00:34:53.780 --> 00:34:56.380]   about what it's like to do math on LSD.
[00:34:56.380 --> 00:35:00.700]   Because apparently, one of the deepest
[00:35:00.700 --> 00:35:04.700]   thinking processes needed is those of mathematicians,
[00:35:04.700 --> 00:35:06.340]   or theoretical computer scientists,
[00:35:06.340 --> 00:35:09.860]   basically doing anything that involves math is proofs,
[00:35:09.860 --> 00:35:12.460]   and you have to think creatively, but also deeply,
[00:35:12.460 --> 00:35:15.860]   and you have to think for many hours at a time.
[00:35:15.860 --> 00:35:18.300]   And so they're always looking for ways to,
[00:35:18.300 --> 00:35:21.340]   like is there any sparks of creativity
[00:35:21.340 --> 00:35:22.260]   that could be injected?
[00:35:22.260 --> 00:35:25.580]   And apparently, out of all the psychedelics,
[00:35:25.580 --> 00:35:29.300]   the worst is LSD, because it completely destroys
[00:35:29.300 --> 00:35:31.060]   your ability to do math well.
[00:35:31.060 --> 00:35:33.660]   And I wonder whether that has to do with your ability
[00:35:33.660 --> 00:35:38.660]   to visualize geometric things in a stable way in your mind,
[00:35:38.660 --> 00:35:41.620]   and hold them there, and stitch things together,
[00:35:41.620 --> 00:35:44.100]   which is often what's required for proofs.
[00:35:44.100 --> 00:35:47.980]   But again, it's difficult to kind of research
[00:35:47.980 --> 00:35:50.980]   these kinds of concepts, but it does make me wonder
[00:35:50.980 --> 00:35:55.860]   where, what are the spaces, how's the space
[00:35:55.860 --> 00:35:59.140]   of things you're able to think about and explore
[00:35:59.140 --> 00:36:02.820]   morphed by different psychedelics,
[00:36:02.820 --> 00:36:06.140]   or dream states, and so on, and how is that different?
[00:36:06.140 --> 00:36:08.140]   How much does it overlap with reality?
[00:36:08.140 --> 00:36:10.420]   And what is reality?
[00:36:10.420 --> 00:36:12.320]   Is there a waking state reality?
[00:36:12.320 --> 00:36:15.940]   Or is it just a tiny subset of reality,
[00:36:15.940 --> 00:36:18.940]   and we get to take a step in other versions of it?
[00:36:18.940 --> 00:36:23.020]   We tend to think very much in a space-time,
[00:36:23.020 --> 00:36:25.700]   four-dimensional, there's a three-dimensional world,
[00:36:25.700 --> 00:36:29.660]   there's time, and that's what we think about reality.
[00:36:29.660 --> 00:36:33.660]   And we think of traveling as walking from point A
[00:36:33.660 --> 00:36:36.740]   to point B in the three-dimensional world.
[00:36:36.740 --> 00:36:40.100]   But that's a very kind of human surviving,
[00:36:40.100 --> 00:36:43.380]   trying not to get eaten by a lion, conception of reality.
[00:36:43.380 --> 00:36:46.820]   What if traveling is something like we do with psychedelics
[00:36:46.820 --> 00:36:48.340]   and meet the elves?
[00:36:48.340 --> 00:36:50.600]   What if it's something, what if thinking,
[00:36:50.600 --> 00:36:53.820]   or the space of ideas as we kind of grow
[00:36:53.820 --> 00:36:56.420]   and think through ideas, that's traveling?
[00:36:56.420 --> 00:37:00.380]   Or what if memories is traveling?
[00:37:00.380 --> 00:37:03.980]   I don't know if you have a favorite view of reality,
[00:37:03.980 --> 00:37:06.980]   or if you, you had, by the way, I should say,
[00:37:06.980 --> 00:37:09.380]   excellent conversation with Donald Hoffman.
[00:37:09.380 --> 00:37:11.980]   - Yeah, yeah, he's interesting.
[00:37:11.980 --> 00:37:15.980]   - Is there any inkling of his sense in your mind
[00:37:15.980 --> 00:37:20.220]   that reality is very far from,
[00:37:20.220 --> 00:37:22.300]   actual, like, objective reality is very far
[00:37:22.300 --> 00:37:24.980]   from the kind of reality we imagine,
[00:37:24.980 --> 00:37:29.460]   we perceive, and we play with in our human minds?
[00:37:29.460 --> 00:37:33.260]   - Well, the first thing to grant is that
[00:37:33.260 --> 00:37:39.940]   we're never in direct contact with reality, whatever it is,
[00:37:39.940 --> 00:37:42.780]   unless that reality is consciousness, right?
[00:37:42.780 --> 00:37:47.300]   So we're only ever experiencing consciousness
[00:37:47.300 --> 00:37:48.380]   and its contents.
[00:37:48.380 --> 00:37:52.580]   And then the question is, how does that circumstance
[00:37:52.580 --> 00:37:55.940]   relate to, quote, reality at large?
[00:37:55.940 --> 00:38:00.100]   And Donald Hoffman is somebody who's happy to speculate,
[00:38:00.100 --> 00:38:02.700]   well, maybe there isn't a reality at large.
[00:38:02.700 --> 00:38:05.620]   Maybe it's all just consciousness on some level.
[00:38:05.620 --> 00:38:10.620]   And that's interesting, that runs into, to my eye,
[00:38:11.540 --> 00:38:16.500]   various philosophical problems that,
[00:38:16.500 --> 00:38:17.620]   or at least you have to do a lot,
[00:38:17.620 --> 00:38:21.980]   you have to add to that picture,
[00:38:21.980 --> 00:38:24.860]   I mean, that picture of idealism,
[00:38:24.860 --> 00:38:27.500]   that's usually all the whole family of views
[00:38:27.500 --> 00:38:30.580]   that would just say that the universe is just mind
[00:38:30.580 --> 00:38:33.060]   or just consciousness at bottom,
[00:38:33.060 --> 00:38:36.060]   we'll go by the name of idealism in Western philosophy.
[00:38:36.060 --> 00:38:40.980]   You have to add to that idealistic picture
[00:38:40.980 --> 00:38:44.900]   all kinds of epicycles and kind of weird coincidences
[00:38:44.900 --> 00:38:49.900]   and to get the predictability of our experience
[00:38:49.900 --> 00:38:54.460]   and the success of materialist science
[00:38:54.460 --> 00:38:56.140]   to make sense in that context, right?
[00:38:56.140 --> 00:38:57.680]   And so the fact that we can,
[00:38:57.680 --> 00:39:03.580]   what does it mean to say that there's only consciousness
[00:39:03.580 --> 00:39:05.600]   at bottom, right?
[00:39:05.600 --> 00:39:07.140]   Nothing outside of consciousness,
[00:39:07.140 --> 00:39:08.660]   'cause no one's ever experienced anything
[00:39:08.660 --> 00:39:09.660]   outside of consciousness.
[00:39:09.660 --> 00:39:11.740]   No scientist has ever done an experiment
[00:39:11.740 --> 00:39:14.900]   where they were contemplating data,
[00:39:14.900 --> 00:39:17.460]   no matter how far removed from our sense bases,
[00:39:17.460 --> 00:39:20.620]   whether it's they're looking at the Hubble Deep Field
[00:39:20.620 --> 00:39:23.620]   or they're smashing atoms
[00:39:23.620 --> 00:39:25.700]   or whatever tools they're using,
[00:39:25.700 --> 00:39:29.020]   they're still just experiencing consciousness
[00:39:29.020 --> 00:39:32.940]   and its various deliverances
[00:39:32.940 --> 00:39:37.220]   and layering their concepts on top of that.
[00:39:37.220 --> 00:39:41.540]   So that's always true,
[00:39:41.540 --> 00:39:44.740]   and yet that somehow doesn't seem to capture
[00:39:44.740 --> 00:39:53.420]   the character of our continually discovering
[00:39:53.420 --> 00:39:59.020]   that our materialist assumptions are confirmable, right?
[00:39:59.020 --> 00:40:02.240]   So you take the fact that we unleash
[00:40:02.240 --> 00:40:06.380]   this fantastic amount of energy from within an atom, right?
[00:40:06.380 --> 00:40:09.900]   First, we have the theoretical suggestion
[00:40:09.900 --> 00:40:12.260]   that it's possible, right?
[00:40:12.260 --> 00:40:14.180]   We come back to Einstein,
[00:40:14.180 --> 00:40:18.580]   there's a lot of energy in that matter, right?
[00:40:18.580 --> 00:40:21.020]   And what if we could release it, right?
[00:40:21.020 --> 00:40:25.180]   And then we perform an experiment that in this case,
[00:40:25.180 --> 00:40:28.540]   the Trinity Test Site in New Mexico,
[00:40:28.540 --> 00:40:32.000]   where the people who are most adequate to this conversation,
[00:40:32.000 --> 00:40:33.860]   people like Robert Oppenheimer,
[00:40:34.620 --> 00:40:39.620]   are standing around not altogether certain
[00:40:39.620 --> 00:40:40.820]   it's going to work, right?
[00:40:40.820 --> 00:40:42.180]   They're performing an experiment,
[00:40:42.180 --> 00:40:43.300]   they're wondering what's gonna happen,
[00:40:43.300 --> 00:40:44.860]   they're wondering if their calculations
[00:40:44.860 --> 00:40:47.740]   around the yield are off by orders of magnitude.
[00:40:47.740 --> 00:40:49.140]   Some of them are still wondering
[00:40:49.140 --> 00:40:51.860]   whether the entire atmosphere of Earth
[00:40:51.860 --> 00:40:55.380]   is gonna combust, right?
[00:40:55.380 --> 00:40:59.420]   That the nuclear chain reaction is not gonna stop.
[00:41:01.220 --> 00:41:06.220]   And lo and behold, there was that energy to be released
[00:41:06.220 --> 00:41:09.860]   from within the nucleus of an atom.
[00:41:09.860 --> 00:41:14.660]   And that could, so it's just,
[00:41:14.660 --> 00:41:20.540]   the picture one forms from those kinds of experiments,
[00:41:20.540 --> 00:41:21.380]   and just the knowledge,
[00:41:21.380 --> 00:41:22.900]   it's just our understanding of evolution,
[00:41:22.900 --> 00:41:26.060]   just the fact that the Earth is billions of years old
[00:41:26.060 --> 00:41:27.840]   and life is hundreds of millions of years old,
[00:41:27.840 --> 00:41:31.000]   and we weren't here to think about any of those things,
[00:41:31.000 --> 00:41:33.560]   and all of those processes were happening,
[00:41:33.560 --> 00:41:35.160]   therefore, in the dark,
[00:41:35.160 --> 00:41:38.440]   and they are the processes that allowed us to emerge
[00:41:38.440 --> 00:41:41.880]   from prior life forms in the first place.
[00:41:41.880 --> 00:41:43.960]   To say that it's all a mess,
[00:41:43.960 --> 00:41:47.280]   that nothing exists outside of consciousness,
[00:41:47.280 --> 00:41:50.580]   conscious minds of the sort that we experience,
[00:41:50.580 --> 00:41:55.580]   it just seems, it seems like a bizarrely
[00:41:55.720 --> 00:41:58.580]   anthropocentric claim,
[00:41:58.580 --> 00:42:03.440]   analogous to the moon isn't there
[00:42:03.440 --> 00:42:05.080]   if no one's looking at it, right?
[00:42:05.080 --> 00:42:07.600]   I mean, the moon as a moon isn't there
[00:42:07.600 --> 00:42:09.440]   if no one's looking at it, I'll grant that,
[00:42:09.440 --> 00:42:12.720]   'cause that's already a kind of fabrication
[00:42:12.720 --> 00:42:17.120]   born of concepts, but the idea that there's nothing there,
[00:42:17.120 --> 00:42:19.600]   that there's nothing that corresponds
[00:42:19.600 --> 00:42:21.800]   to what we experience as the moon,
[00:42:21.800 --> 00:42:23.700]   unless someone's looking at it,
[00:42:23.700 --> 00:42:28.700]   that just seems just a way too parochial way
[00:42:28.700 --> 00:42:31.400]   to set out on this journey of discovery.
[00:42:31.400 --> 00:42:32.280]   - There is something there,
[00:42:32.280 --> 00:42:34.280]   there's a computer waiting to render the moon
[00:42:34.280 --> 00:42:36.000]   when you look at it.
[00:42:36.000 --> 00:42:38.700]   The capacity for the moon to exist is there.
[00:42:38.700 --> 00:42:42.540]   So if we're indeed living in a simulation,
[00:42:42.540 --> 00:42:45.420]   which I find a compelling thought experiment,
[00:42:45.420 --> 00:42:47.720]   it's possible that there is this kind of
[00:42:47.720 --> 00:42:50.960]   rendering mechanism, but not in the silly way
[00:42:50.960 --> 00:42:52.440]   that we think about in video games,
[00:42:52.440 --> 00:42:55.280]   but in some kind of more fundamental physics way.
[00:42:55.280 --> 00:42:57.820]   - And we have to account for the fact
[00:42:57.820 --> 00:43:02.820]   that it renders experiences that no one has had yet,
[00:43:02.820 --> 00:43:07.140]   that no one has any expectation of having.
[00:43:07.140 --> 00:43:10.080]   It can violate the expectations of everyone lawfully,
[00:43:10.080 --> 00:43:12.160]   right, and then there's some lawful understanding
[00:43:12.160 --> 00:43:14.220]   of why that's so.
[00:43:14.220 --> 00:43:18.280]   It's like, I mean, just to bring it back to mathematics,
[00:43:18.280 --> 00:43:20.420]   I'm like, like, certain numbers are prime
[00:43:20.420 --> 00:43:22.720]   whether we have discovered them or not, right?
[00:43:22.720 --> 00:43:24.880]   Like there's the highest prime number
[00:43:24.880 --> 00:43:27.640]   that anyone can name now,
[00:43:27.640 --> 00:43:29.160]   and then there's the next prime number
[00:43:29.160 --> 00:43:31.640]   that no one can name, and it's there, right?
[00:43:31.640 --> 00:43:36.640]   So it's like, to say that our minds are putting it there,
[00:43:36.640 --> 00:43:38.900]   that what we know as mind in ourselves
[00:43:38.900 --> 00:43:41.860]   is in some way, in some sense, putting it there,
[00:43:41.860 --> 00:43:46.860]   that, like the base layer of reality is consciousness, right?
[00:43:47.040 --> 00:43:49.500]   You know, that we're identical to the thing
[00:43:49.500 --> 00:43:53.220]   that is rendering this reality.
[00:43:53.220 --> 00:43:57.700]   There's some, you know, hubris is the wrong word,
[00:43:57.700 --> 00:43:59.340]   but it's like there's some, it's like,
[00:43:59.340 --> 00:44:02.980]   it's okay if reality is bigger than what we experience,
[00:44:02.980 --> 00:44:07.980]   you know, and it has structure that we can't anticipate
[00:44:07.980 --> 00:44:10.420]   and that isn't just, I mean, again,
[00:44:14.840 --> 00:44:17.840]   there's certainly a collaboration between our minds
[00:44:17.840 --> 00:44:21.600]   and whatever is out there to produce what we call,
[00:44:21.600 --> 00:44:24.040]   you know, the stuff of life,
[00:44:24.040 --> 00:44:29.040]   but it's not the idea that it's,
[00:44:29.040 --> 00:44:33.240]   I don't know, I mean, there are a few stops
[00:44:33.240 --> 00:44:36.880]   on the train of idealism and kind of new age thinking
[00:44:36.880 --> 00:44:40.560]   and Eastern philosophy that I don't,
[00:44:40.560 --> 00:44:42.480]   philosophically, I don't see a need to take.
[00:44:42.480 --> 00:44:45.940]   I mean, the place, experientially and scientifically,
[00:44:45.940 --> 00:44:49.500]   I feel like it's, you can get everything you want
[00:44:49.500 --> 00:44:55.660]   acknowledging that consciousness has a character
[00:44:55.660 --> 00:44:58.940]   that can be explored from its own side
[00:44:58.940 --> 00:45:01.580]   so that you're bringing kind of the first person experience
[00:45:01.580 --> 00:45:03.940]   back into the conversation about, you know,
[00:45:03.940 --> 00:45:06.800]   what is a human mind and, you know, what is true,
[00:45:06.800 --> 00:45:12.060]   and you can explore it with different degrees of rigor
[00:45:12.060 --> 00:45:13.700]   and there are things to be discovered there,
[00:45:13.700 --> 00:45:15.500]   whether you're using a technique like meditation
[00:45:15.500 --> 00:45:19.020]   or psychedelics, and that these experiences
[00:45:19.020 --> 00:45:22.500]   have to be put in conversation with what we understand
[00:45:22.500 --> 00:45:24.580]   about ourselves from a third person side,
[00:45:24.580 --> 00:45:27.420]   neuroscientifically or in any other way.
[00:45:27.420 --> 00:45:30.300]   - But to me, the question is, what if reality,
[00:45:30.300 --> 00:45:34.820]   the sense I have from this kind of, you play shooters?
[00:45:34.820 --> 00:45:36.020]   - No.
[00:45:36.020 --> 00:45:37.860]   - There's a physics engine that generate, that's--
[00:45:37.860 --> 00:45:40.300]   - Oh, you mean first person shooter games?
[00:45:40.300 --> 00:45:41.780]   - Yes, yes, sorry.
[00:45:41.780 --> 00:45:43.620]   Not often, but yes.
[00:45:43.620 --> 00:45:44.740]   - I mean, there's a physics engine
[00:45:44.740 --> 00:45:47.300]   that generates consistent reality, right?
[00:45:47.300 --> 00:45:50.900]   My sense is the same could be true for a universe
[00:45:50.900 --> 00:45:54.700]   in the following sense, that our conception of reality,
[00:45:54.700 --> 00:45:57.660]   as we understand it now in the 21st century,
[00:45:57.660 --> 00:45:59.780]   is a tiny subset of the full reality.
[00:45:59.780 --> 00:46:03.020]   It's not that the reality that we conceive of that's there,
[00:46:03.020 --> 00:46:06.440]   the moon being there is not there somehow.
[00:46:06.440 --> 00:46:09.380]   It's that it's a tiny fraction of what's actually out there.
[00:46:09.380 --> 00:46:12.820]   And so the physics engine of the universe
[00:46:12.820 --> 00:46:16.100]   is just maintaining the useful physics,
[00:46:16.100 --> 00:46:19.020]   the useful "reality," quote unquote,
[00:46:19.020 --> 00:46:22.880]   for us to have a consistent experience as human beings.
[00:46:22.880 --> 00:46:25.860]   But maybe we, descendants of apes,
[00:46:25.860 --> 00:46:30.860]   are really only understand 0.0001%
[00:46:30.860 --> 00:46:34.020]   of actual physics of reality.
[00:46:34.020 --> 00:46:36.620]   We can even just start with the consciousness thing.
[00:46:36.620 --> 00:46:39.540]   But maybe our minds are just,
[00:46:39.540 --> 00:46:42.340]   we're just too dumb by design.
[00:46:42.340 --> 00:46:46.820]   - Oh, yeah, that truly resonates with me,
[00:46:46.820 --> 00:46:48.300]   and I'm surprised it doesn't resonate more
[00:46:48.300 --> 00:46:50.620]   with most scientists that I talk to.
[00:46:50.620 --> 00:46:52.860]   When you just look at,
[00:46:52.860 --> 00:46:57.180]   you look at how close we are to chimps, right?
[00:46:57.180 --> 00:46:58.900]   And chimps don't know anything, right?
[00:46:58.900 --> 00:47:01.500]   Clearly, they have no idea what's going on, right?
[00:47:01.500 --> 00:47:03.260]   And then you get us,
[00:47:03.260 --> 00:47:06.160]   but then it's only a subset of human beings
[00:47:06.160 --> 00:47:09.860]   that really understand much of what we're talking about
[00:47:09.860 --> 00:47:12.620]   in any area of specialization.
[00:47:12.620 --> 00:47:15.660]   And if they all died in their sleep tonight, right,
[00:47:15.660 --> 00:47:20.620]   you'd be left with people who might take 1,000 years
[00:47:20.620 --> 00:47:24.620]   to rebuild the internet, if ever, right?
[00:47:24.620 --> 00:47:26.180]   I mean, literally, it's like,
[00:47:26.180 --> 00:47:29.300]   and I would extend this to myself.
[00:47:29.300 --> 00:47:32.920]   I mean, there are areas of scientific specialization
[00:47:32.920 --> 00:47:37.480]   where I have either no discernible competence,
[00:47:37.480 --> 00:47:40.060]   I mean, I spend no time on it,
[00:47:40.060 --> 00:47:42.140]   I have not acquired the tools.
[00:47:42.140 --> 00:47:43.660]   It would just be an article of faith for me
[00:47:43.660 --> 00:47:45.140]   to think that I could acquire the tools
[00:47:45.140 --> 00:47:47.500]   to actually make a breakthrough in those areas.
[00:47:47.500 --> 00:47:51.540]   And your own area is one.
[00:47:51.540 --> 00:47:54.080]   I've never spent any significant amount of time
[00:47:54.080 --> 00:47:56.380]   trying to be a programmer,
[00:47:56.380 --> 00:48:00.340]   but it's pretty obvious I'm not Alan Turing, right?
[00:48:00.340 --> 00:48:03.760]   It's like, if that were my capacity,
[00:48:03.760 --> 00:48:05.840]   I would have discovered that in myself.
[00:48:05.840 --> 00:48:08.360]   I would have found programming irresistible.
[00:48:08.360 --> 00:48:13.360]   My first false starts in learning, I think it was C,
[00:48:13.360 --> 00:48:17.240]   it was just, I bounced off.
[00:48:17.240 --> 00:48:18.360]   It's like, this was not fun.
[00:48:18.360 --> 00:48:21.200]   I hate, I mean, trying to figure out what,
[00:48:21.200 --> 00:48:24.280]   the syntax error that's causing this thing not to compile
[00:48:24.280 --> 00:48:25.760]   was just a fucking awful experience.
[00:48:25.760 --> 00:48:26.600]   I hated it, right?
[00:48:26.600 --> 00:48:28.000]   I hated every minute of it.
[00:48:28.000 --> 00:48:33.000]   So it was not, so if it was just people like me left,
[00:48:33.000 --> 00:48:37.180]   like, when do we get the internet again?
[00:48:37.180 --> 00:48:40.540]   Right, and we lose the internet.
[00:48:40.540 --> 00:48:41.620]   When do we get it again, right?
[00:48:41.620 --> 00:48:44.300]   When do we get anything
[00:48:44.300 --> 00:48:47.180]   like a proper science of information, right?
[00:48:47.180 --> 00:48:50.940]   You need a Claude Shannon or an Alan Turing
[00:48:50.940 --> 00:48:53.700]   to plant a flag in the ground right here and say,
[00:48:53.700 --> 00:48:55.140]   "All right, can everyone see this?
[00:48:55.140 --> 00:48:57.780]   Even if you don't quite know what I'm up to,
[00:48:57.780 --> 00:49:02.100]   you all have to come over here to make some progress."
[00:49:02.100 --> 00:49:08.240]   And there are hundreds of topics where that's the case.
[00:49:08.240 --> 00:49:11.820]   So we barely have a purchase
[00:49:11.820 --> 00:49:16.820]   on making anything like discernible intellectual progress
[00:49:16.820 --> 00:49:18.960]   in any generation.
[00:49:18.960 --> 00:49:23.960]   And yeah, I'm just, Max Tegmark makes this point.
[00:49:23.960 --> 00:49:27.700]   He's one of the few people who does.
[00:49:27.700 --> 00:49:32.700]   In physics, if you just take the truth
[00:49:32.700 --> 00:49:36.340]   of evolution seriously, right?
[00:49:36.340 --> 00:49:39.780]   And realize that there's nothing about us
[00:49:39.780 --> 00:49:42.740]   that has evolved to understand reality perfectly.
[00:49:42.740 --> 00:49:46.340]   I mean, we're just not that kind of ape, right?
[00:49:46.340 --> 00:49:48.620]   There's been no evolutionary pressure along those lines.
[00:49:48.620 --> 00:49:52.380]   So what we are making do with tools
[00:49:52.380 --> 00:49:56.020]   that were designed for fights with sticks and rocks, right?
[00:49:56.780 --> 00:50:00.420]   And it's amazing we can do as much as we can.
[00:50:00.420 --> 00:50:02.380]   I mean, you and I are just sitting here
[00:50:02.380 --> 00:50:06.100]   on the back of having received an mRNA vaccine
[00:50:06.100 --> 00:50:08.460]   that has certainly changed our life
[00:50:08.460 --> 00:50:10.540]   given what the last year was like.
[00:50:10.540 --> 00:50:12.180]   And it's gonna change the world
[00:50:12.180 --> 00:50:16.460]   if rumors of coming miracles are borne out.
[00:50:16.460 --> 00:50:20.780]   I mean, it's now, it seems likely we have a vaccine
[00:50:20.780 --> 00:50:22.240]   coming for malaria, right?
[00:50:22.240 --> 00:50:25.920]   Which has been killing millions of people a year
[00:50:25.920 --> 00:50:27.520]   for as long as we've been alive.
[00:50:27.520 --> 00:50:31.260]   I think it's down to like 800,000 people a year now
[00:50:31.260 --> 00:50:33.900]   because we've spread so many bed nets around.
[00:50:33.900 --> 00:50:37.360]   But it was like two and a half million people every year.
[00:50:37.360 --> 00:50:43.380]   It's amazing what we can do, but yeah, I have,
[00:50:43.380 --> 00:50:46.820]   if in fact the answer at the back of the book of nature
[00:50:46.820 --> 00:50:51.820]   is you understand 0.1% of what there is to understand
[00:50:51.820 --> 00:50:54.900]   and half of what you think you understand is wrong,
[00:50:54.900 --> 00:50:57.280]   that would not surprise me at all.
[00:50:57.280 --> 00:51:01.040]   - It is funny to look at our evolutionary history,
[00:51:01.040 --> 00:51:02.420]   even back to chimps.
[00:51:02.420 --> 00:51:03.860]   I'm pretty sure even chimps thought
[00:51:03.860 --> 00:51:06.340]   they understood the world well.
[00:51:06.340 --> 00:51:09.300]   So at every point in that timeline
[00:51:09.300 --> 00:51:12.860]   of evolutionary development throughout human history,
[00:51:12.860 --> 00:51:15.560]   there's a sense like there's no more,
[00:51:15.560 --> 00:51:17.020]   you hear this message over and over,
[00:51:17.020 --> 00:51:19.380]   there's no more things to be invented.
[00:51:19.380 --> 00:51:21.300]   - But 100 years ago there were,
[00:51:21.300 --> 00:51:24.920]   there's a famous story, I forget which physicist told it,
[00:51:24.920 --> 00:51:29.060]   but there were physicists telling
[00:51:29.060 --> 00:51:32.800]   their undergraduate students not to go into,
[00:51:32.800 --> 00:51:34.260]   to get graduate degrees in physics
[00:51:34.260 --> 00:51:36.580]   because basically all the problems had been solved.
[00:51:36.580 --> 00:51:40.140]   And this is like around 1915 or so.
[00:51:40.140 --> 00:51:41.300]   - Turns out you were right.
[00:51:41.300 --> 00:51:42.700]   I'm gonna ask you about free will.
[00:51:42.700 --> 00:51:43.540]   - Oh, okay.
[00:51:43.540 --> 00:51:48.220]   - You've recently released an episode of your podcast,
[00:51:48.220 --> 00:51:51.580]   Making Sense, for those with a shorter attention span,
[00:51:51.580 --> 00:51:54.000]   basically summarizing your position on free will.
[00:51:54.000 --> 00:51:56.240]   I think it was under an hour and a half.
[00:51:56.240 --> 00:51:57.140]   - Yeah, yeah.
[00:51:57.140 --> 00:52:00.520]   - It was brief and clear.
[00:52:00.520 --> 00:52:05.300]   So allow me to summarize the summary, TL;DR,
[00:52:05.300 --> 00:52:07.380]   and maybe you tell me where I'm wrong.
[00:52:07.380 --> 00:52:11.380]   So free will is an illusion,
[00:52:11.380 --> 00:52:13.980]   and even the experience of free will is an illusion.
[00:52:13.980 --> 00:52:15.920]   Like we don't even experience it.
[00:52:17.340 --> 00:52:19.920]   Am I good in my summary?
[00:52:19.920 --> 00:52:24.780]   - Yeah, this is a line
[00:52:24.780 --> 00:52:27.360]   that's a little hard to scan for people.
[00:52:27.360 --> 00:52:32.360]   I say that it's not merely that free will is an illusion.
[00:52:32.360 --> 00:52:35.140]   The illusion of free will is an illusion.
[00:52:35.140 --> 00:52:37.480]   Like there is no illusion of free will.
[00:52:37.480 --> 00:52:40.620]   And that is a, unlike many other illusions,
[00:52:40.620 --> 00:52:47.080]   that's a more fundamental claim.
[00:52:47.080 --> 00:52:49.500]   It's not that it's wrong, it's not even wrong.
[00:52:49.500 --> 00:52:52.220]   I mean, that's, I guess, that was, I think, Wolfgang Pauli,
[00:52:52.220 --> 00:52:56.420]   who derided one of his colleagues or enemies
[00:52:56.420 --> 00:53:01.420]   with that aspersion about his theory in quantum mechanics.
[00:53:01.420 --> 00:53:09.820]   So there are things that you, there are genuine illusions.
[00:53:09.820 --> 00:53:12.740]   There are things that you do experience,
[00:53:12.740 --> 00:53:15.160]   and then you can kind of punch through that experience.
[00:53:15.160 --> 00:53:17.300]   Or you can't actually experience,
[00:53:17.300 --> 00:53:20.140]   you can't experience them any other way.
[00:53:20.140 --> 00:53:24.100]   It's just, we just know it's not a veridical experience.
[00:53:24.100 --> 00:53:25.380]   You just take like a visual illusion.
[00:53:25.380 --> 00:53:26.940]   There are visual illusions that,
[00:53:26.940 --> 00:53:28.740]   a lot of these come to me on Twitter these days.
[00:53:28.740 --> 00:53:31.020]   There's these amazing visual illusions
[00:53:31.020 --> 00:53:36.020]   where like every figure in this GIF seems to be moving,
[00:53:36.020 --> 00:53:37.740]   but nothing, in fact, is moving.
[00:53:37.740 --> 00:53:39.220]   You can just like put a ruler on your screen
[00:53:39.220 --> 00:53:40.260]   and nothing's moving.
[00:53:40.260 --> 00:53:44.540]   Some of those illusions you can't see any other way.
[00:53:44.540 --> 00:53:46.860]   I mean, they're just, they're hacking aspects
[00:53:46.860 --> 00:53:49.860]   of the visual system that are just eminently hackable,
[00:53:49.860 --> 00:53:54.380]   and you have to use a ruler to convince yourself
[00:53:54.380 --> 00:53:56.500]   that the thing isn't actually moving.
[00:53:56.500 --> 00:53:57.860]   Now, there are other visual illusions
[00:53:57.860 --> 00:54:01.180]   where you're taken in by it at first,
[00:54:01.180 --> 00:54:02.580]   but if you pay more attention,
[00:54:02.580 --> 00:54:05.300]   you can actually see that it's not there, right?
[00:54:05.300 --> 00:54:07.620]   Or it's not how it first seemed.
[00:54:07.620 --> 00:54:10.180]   Like the Necker cube is a good example of that.
[00:54:10.180 --> 00:54:13.300]   Like the Necker cube is just that schematic of a cube,
[00:54:13.300 --> 00:54:15.900]   of a transparent cube, which pops out one way or the other.
[00:54:15.900 --> 00:54:18.940]   One face can pop out and the other face can pop out,
[00:54:18.940 --> 00:54:22.160]   but you can actually just see it as flat with no pop out,
[00:54:22.160 --> 00:54:26.180]   which is a more veridical way of looking at it.
[00:54:26.180 --> 00:54:29.940]   So there are subject,
[00:54:29.940 --> 00:54:32.340]   there are kind of inward correlates to this,
[00:54:32.340 --> 00:54:35.340]   and I would say that the sense of self,
[00:54:35.340 --> 00:54:40.700]   the sense of self and free will are closely related.
[00:54:40.700 --> 00:54:43.460]   I often describe them as two sides of the same coin,
[00:54:43.460 --> 00:54:48.460]   but they're not quite the same in their spuriousness.
[00:54:48.460 --> 00:54:50.980]   I mean, so the sense of self is something
[00:54:50.980 --> 00:54:54.540]   that people, I think, do experience, right?
[00:54:54.540 --> 00:54:56.900]   It's not a very clear experience,
[00:54:56.900 --> 00:54:59.860]   but it's not, I wouldn't call the illusion of self
[00:54:59.860 --> 00:55:02.420]   an illusion, but the illusion of free will
[00:55:02.420 --> 00:55:05.420]   is an illusion in that as you pay more attention
[00:55:05.420 --> 00:55:08.060]   to your experience, you begin to see
[00:55:08.060 --> 00:55:11.800]   that it's totally compatible with an absence of free will.
[00:55:11.800 --> 00:55:14.600]   You don't, I mean, coming back to the place we started,
[00:55:14.600 --> 00:55:18.460]   you don't know what you're gonna think next.
[00:55:18.460 --> 00:55:20.300]   You don't know what you're gonna intend next.
[00:55:20.300 --> 00:55:23.380]   You don't know what's going to just occur to you
[00:55:23.380 --> 00:55:24.540]   that you must do next.
[00:55:24.540 --> 00:55:28.020]   You don't know how much you are going to feel
[00:55:28.020 --> 00:55:31.660]   the behavioral imperative to act on that thought.
[00:55:31.660 --> 00:55:35.340]   If you suddenly feel, oh, I don't need to do that.
[00:55:35.340 --> 00:55:36.740]   I can do that tomorrow.
[00:55:36.740 --> 00:55:38.100]   You don't know where that comes from.
[00:55:38.100 --> 00:55:39.640]   You didn't know that was gonna arise.
[00:55:39.640 --> 00:55:41.580]   You didn't know that was gonna be compelling.
[00:55:41.580 --> 00:55:44.140]   All of this is compatible with some evil genius
[00:55:44.140 --> 00:55:46.900]   in the next room just typing in code
[00:55:46.900 --> 00:55:48.500]   into your experience, just like this.
[00:55:48.500 --> 00:55:52.700]   Okay, let's give him the, oh my God, I just forgot.
[00:55:52.700 --> 00:55:56.080]   It was gonna be our anniversary in one week thought, right?
[00:55:56.080 --> 00:55:57.980]   Give him the cascade of fear.
[00:55:57.980 --> 00:56:01.620]   Give him this brilliant idea for the thing he can buy
[00:56:01.620 --> 00:56:02.860]   that's gonna take him no time at all
[00:56:02.860 --> 00:56:05.300]   and this overpowering sense of relief.
[00:56:05.300 --> 00:56:07.820]   All of our experience is compatible
[00:56:07.820 --> 00:56:11.700]   with the script already being written, right?
[00:56:11.700 --> 00:56:12.960]   And I'm not saying the script is written.
[00:56:12.960 --> 00:56:17.340]   I'm not saying that fatalism is the right way
[00:56:17.340 --> 00:56:20.180]   to look at this, but we just don't have
[00:56:20.180 --> 00:56:23.420]   even our most deliberate voluntary action
[00:56:23.420 --> 00:56:27.060]   where we go back and forth between two options,
[00:56:27.060 --> 00:56:28.580]   thinking about the reason for A
[00:56:28.580 --> 00:56:30.500]   and then reconsidering and going,
[00:56:30.500 --> 00:56:33.380]   thinking harder about B
[00:56:33.380 --> 00:56:35.380]   and just going eeny, meeny, miny, moe
[00:56:35.380 --> 00:56:37.980]   until the end of the hour.
[00:56:37.980 --> 00:56:40.340]   However laborious you can make it,
[00:56:40.340 --> 00:56:44.500]   there is a utter mystery at your back
[00:56:44.500 --> 00:56:48.940]   finally promoting the thought or intention
[00:56:48.940 --> 00:56:53.860]   or rationale that is most compelling
[00:56:53.860 --> 00:56:58.860]   and therefore deliberately, behaviorally effective.
[00:57:02.180 --> 00:57:05.120]   And just,
[00:57:05.120 --> 00:57:09.300]   and this can drive some people a little crazy.
[00:57:09.300 --> 00:57:13.740]   So I usually preface what I say about free will
[00:57:13.740 --> 00:57:17.280]   with the caveat that if thinking about your mind this way
[00:57:17.280 --> 00:57:19.500]   makes you feel terrible, well then stop,
[00:57:19.500 --> 00:57:22.420]   you know, get off the ride, switch the channel.
[00:57:22.420 --> 00:57:24.580]   You don't have to go down this path.
[00:57:24.580 --> 00:57:27.520]   But for me and for many other people,
[00:57:27.520 --> 00:57:32.160]   it's incredibly freeing to recognize this about the mind
[00:57:32.160 --> 00:57:37.160]   because one, you realize that you're,
[00:57:37.160 --> 00:57:39.940]   I mean, cutting through the illusion of the self
[00:57:39.940 --> 00:57:41.820]   is immensely freeing for a lot of reasons
[00:57:41.820 --> 00:57:44.020]   that we can talk about separately,
[00:57:44.020 --> 00:57:46.760]   but losing the sense of free will
[00:57:46.760 --> 00:57:51.520]   does two things very vividly for me.
[00:57:51.520 --> 00:57:54.760]   One is it totally undercuts the basis for,
[00:57:54.760 --> 00:57:56.620]   the psychological basis for hatred, right?
[00:57:56.620 --> 00:57:58.760]   Because when you think about the experience
[00:57:58.760 --> 00:58:03.560]   of hating other people, what that is anchored to
[00:58:03.560 --> 00:58:06.320]   is a feeling that they really are
[00:58:06.320 --> 00:58:08.640]   the true authors of their actions.
[00:58:08.640 --> 00:58:10.840]   I mean, that someone is doing something
[00:58:10.840 --> 00:58:13.200]   that you find so despicable, right?
[00:58:13.200 --> 00:58:15.900]   And let's say they're targeting you unfairly, right?
[00:58:15.900 --> 00:58:20.220]   They're maligning you on Twitter or they're suing you
[00:58:20.220 --> 00:58:22.840]   or they're doing something, they broke your car window,
[00:58:22.840 --> 00:58:24.640]   they did something awful
[00:58:24.640 --> 00:58:26.640]   and now you have a grievance against them
[00:58:26.640 --> 00:58:30.000]   and you're relating to them very differently emotionally
[00:58:30.000 --> 00:58:32.360]   in your own mind than you would
[00:58:32.360 --> 00:58:34.520]   if a force of nature had done this, right?
[00:58:34.520 --> 00:58:36.720]   Or if it had just been a virus
[00:58:36.720 --> 00:58:38.320]   or if it had been a wild animal
[00:58:38.320 --> 00:58:40.720]   or a malfunctioning machine, right?
[00:58:40.720 --> 00:58:42.240]   Like to those things, you don't attribute
[00:58:42.240 --> 00:58:44.180]   any kind of freedom of will.
[00:58:44.180 --> 00:58:46.960]   And while you may suffer the consequences
[00:58:46.960 --> 00:58:49.920]   of catching a virus or being attacked by a wild animal
[00:58:49.920 --> 00:58:53.120]   or having your car break down or whatever,
[00:58:53.120 --> 00:58:54.900]   it may frustrate you.
[00:58:54.900 --> 00:59:01.000]   You don't slip into this mode of hating the agent
[00:59:01.000 --> 00:59:06.200]   in a way that completely commandeers your mind
[00:59:06.200 --> 00:59:07.800]   and deranges your life.
[00:59:07.800 --> 00:59:08.700]   I mean, you just don't, I mean,
[00:59:08.700 --> 00:59:11.720]   there are people who spend decades
[00:59:11.720 --> 00:59:15.800]   hating other people for what they did
[00:59:15.800 --> 00:59:18.600]   and it's just pure poison, right?
[00:59:18.600 --> 00:59:20.880]   - So it's a useful shortcut to compassion and empathy.
[00:59:20.880 --> 00:59:21.720]   - Yeah, yeah.
[00:59:21.720 --> 00:59:24.840]   - But the question is, say that this call,
[00:59:24.840 --> 00:59:26.760]   what was it, the horse of consciousness,
[00:59:26.760 --> 00:59:30.680]   let's call it the consciousness generator black box
[00:59:30.680 --> 00:59:32.520]   that we don't understand.
[00:59:32.520 --> 00:59:35.940]   And is it possible that the script
[00:59:35.940 --> 00:59:40.780]   that we're walking along, that we're playing,
[00:59:40.780 --> 00:59:43.880]   that's already written, is actually being written
[00:59:43.880 --> 00:59:45.200]   in real time?
[00:59:45.200 --> 00:59:47.680]   It's almost like you're driving down a road
[00:59:47.680 --> 00:59:50.880]   and in real time, that road is being laid down.
[00:59:50.880 --> 00:59:52.680]   And this black box of consciousness
[00:59:52.680 --> 00:59:54.800]   that we don't understand is the place
[00:59:54.800 --> 00:59:57.060]   where this script is being generated.
[00:59:57.060 --> 01:00:01.180]   So it's not, it is being generated, it didn't always exist.
[01:00:01.180 --> 01:00:02.800]   So there's something we don't understand
[01:00:02.800 --> 01:00:05.500]   that's fundamental about the nature of reality
[01:00:05.500 --> 01:00:07.520]   that generates both consciousness,
[01:00:07.520 --> 01:00:09.840]   let's call it maybe the self.
[01:00:09.840 --> 01:00:11.600]   I don't know if you want to distinguish between those.
[01:00:11.600 --> 01:00:13.040]   - Yeah, I definitely would, yeah.
[01:00:13.040 --> 01:00:15.780]   - You would, because there's a bunch of illusions
[01:00:15.780 --> 01:00:16.620]   we're referring to.
[01:00:16.620 --> 01:00:18.240]   There's the illusion of free will,
[01:00:18.240 --> 01:00:20.280]   there's the illusion of self,
[01:00:20.280 --> 01:00:22.440]   and there's the illusion of consciousness.
[01:00:22.440 --> 01:00:25.840]   You're saying, I think you said there's no,
[01:00:25.840 --> 01:00:27.040]   you're not as willing to say
[01:00:27.040 --> 01:00:28.800]   there's an illusion of consciousness.
[01:00:28.800 --> 01:00:29.640]   You're a little bit more--
[01:00:29.640 --> 01:00:30.840]   - In fact, I would say it's impossible.
[01:00:30.840 --> 01:00:31.720]   - Impossible.
[01:00:31.720 --> 01:00:33.240]   You're a little bit more willing to say
[01:00:33.240 --> 01:00:35.520]   that there's an illusion of self,
[01:00:35.520 --> 01:00:36.920]   and you're definitely saying
[01:00:36.920 --> 01:00:38.600]   there's an illusion of free will.
[01:00:38.600 --> 01:00:39.880]   - Yes, yes.
[01:00:39.880 --> 01:00:42.080]   I'm definitely saying there's an illusion,
[01:00:42.080 --> 01:00:44.840]   that a certain kind of self is an illusion, not every.
[01:00:44.840 --> 01:00:48.000]   We mean many different things by this notion of self.
[01:00:48.000 --> 01:00:50.520]   So maybe I should just differentiate these things.
[01:00:50.520 --> 01:00:53.040]   So consciousness can't be an illusion
[01:00:53.040 --> 01:00:58.040]   because any illusion proves its reality
[01:00:58.040 --> 01:01:00.480]   as much as any other veridical perception.
[01:01:00.480 --> 01:01:02.560]   I mean, if you're hallucinating now,
[01:01:02.560 --> 01:01:05.660]   that's just as much a demonstration of consciousness
[01:01:05.660 --> 01:01:09.680]   as really seeing what's quote actually there.
[01:01:09.680 --> 01:01:11.900]   If you're dreaming and you don't know it,
[01:01:11.900 --> 01:01:15.040]   that is consciousness, right?
[01:01:15.040 --> 01:01:17.740]   If you're, you can be confused about literally everything.
[01:01:17.740 --> 01:01:22.740]   You can't be confused about the underlying claim,
[01:01:22.740 --> 01:01:27.560]   whether you make it linguistically or not,
[01:01:27.560 --> 01:01:32.240]   but just the cognitive assertion
[01:01:32.240 --> 01:01:36.960]   that something seems to be happening.
[01:01:36.960 --> 01:01:40.440]   It's the seeming that is the cash value of consciousness.
[01:01:40.440 --> 01:01:41.840]   - Can I take a tiny tangent?
[01:01:41.840 --> 01:01:42.680]   - Okay.
[01:01:42.680 --> 01:01:47.680]   - So what if I am creating consciousness in my mind
[01:01:47.680 --> 01:01:50.120]   to convince you that I'm human?
[01:01:50.120 --> 01:01:52.560]   So it's a useful social tool,
[01:01:52.560 --> 01:01:57.560]   not a fundamental property of experience,
[01:01:57.560 --> 01:02:00.760]   like of being a living thing.
[01:02:00.760 --> 01:02:02.680]   What if it's just like a social tool
[01:02:02.680 --> 01:02:07.560]   to almost like a useful computational trick
[01:02:07.560 --> 01:02:10.580]   to place myself into reality
[01:02:10.580 --> 01:02:13.180]   as we together communicate about this reality?
[01:02:13.180 --> 01:02:15.920]   And another way to ask that,
[01:02:15.920 --> 01:02:17.560]   'cause you said it much earlier,
[01:02:17.560 --> 01:02:21.340]   you talk negatively about robots as you often do,
[01:02:21.340 --> 01:02:24.320]   so let me, 'cause you'll probably die first
[01:02:24.320 --> 01:02:25.320]   when they take over.
[01:02:25.320 --> 01:02:28.720]   - No, I'm looking forward to certain kinds of robots.
[01:02:28.720 --> 01:02:31.080]   I mean, I'm not, if we can get this right,
[01:02:31.080 --> 01:02:31.920]   this would be amazing.
[01:02:31.920 --> 01:02:33.240]   - Right, but you don't like the robots
[01:02:33.240 --> 01:02:34.520]   that fake consciousness.
[01:02:34.520 --> 01:02:36.480]   That's what you, you don't like the idea
[01:02:36.480 --> 01:02:37.840]   of fake it 'til you make it.
[01:02:37.840 --> 01:02:40.160]   - Well, no, it's not that I don't like it.
[01:02:40.160 --> 01:02:43.400]   It's that I'm worried that we will lose sight
[01:02:43.400 --> 01:02:45.800]   of the problem, and the problem has massive
[01:02:45.800 --> 01:02:47.280]   ethical consequences.
[01:02:47.280 --> 01:02:51.960]   If we create robots that really can suffer,
[01:02:51.960 --> 01:02:53.680]   that would be a bad thing, right?
[01:02:53.680 --> 01:02:56.120]   And if we really are committing a murder
[01:02:56.120 --> 01:02:59.160]   when we recycle them, that would be a bad thing.
[01:02:59.160 --> 01:03:00.280]   - This is how I know you're not Russian.
[01:03:00.280 --> 01:03:02.360]   Why is it a bad thing that we create robots
[01:03:02.360 --> 01:03:03.280]   that can suffer?
[01:03:03.280 --> 01:03:05.280]   Isn't suffering a fundamental thing
[01:03:05.280 --> 01:03:07.240]   from which like beauty springs?
[01:03:07.240 --> 01:03:09.320]   Like without suffering, do you really think
[01:03:09.320 --> 01:03:11.680]   we would have beautiful things in this world?
[01:03:11.680 --> 01:03:14.080]   - Okay, that's a tangent on a tangent.
[01:03:14.080 --> 01:03:15.160]   - Okay, all right. - We'll go there.
[01:03:15.160 --> 01:03:17.520]   I would love to go there, but let's not go there just yet.
[01:03:17.520 --> 01:03:20.280]   But I do think it would be, if anything is bad,
[01:03:20.280 --> 01:03:25.080]   creating hell and populating it with real minds
[01:03:25.080 --> 01:03:29.120]   that really can suffer in that hell, that's bad.
[01:03:29.120 --> 01:03:33.800]   That's the, you are worse than any mass murderer
[01:03:33.800 --> 01:03:35.520]   we can name if you create it.
[01:03:35.520 --> 01:03:37.280]   I mean, this could be in robot form,
[01:03:37.280 --> 01:03:40.760]   or more likely it would be in some simulation
[01:03:40.760 --> 01:03:42.800]   of a world where we've managed to populate it
[01:03:42.800 --> 01:03:44.640]   with conscious minds so that whether we knew
[01:03:44.640 --> 01:03:46.960]   they were conscious or not, and that world
[01:03:46.960 --> 01:03:49.720]   is a state of, it's unendurable.
[01:03:49.720 --> 01:03:53.160]   That would just, it just, taking the thesis seriously
[01:03:53.160 --> 01:03:58.080]   that there's nothing, that mind, intelligence,
[01:03:58.080 --> 01:04:00.720]   and consciousness ultimately are substrate independent.
[01:04:00.720 --> 01:04:03.360]   Right, you don't need a biological brain to be conscious.
[01:04:03.360 --> 01:04:04.760]   You certainly don't need a biological brain
[01:04:04.760 --> 01:04:05.960]   to be intelligent, right?
[01:04:05.960 --> 01:04:09.000]   So if we just imagine that consciousness at some point
[01:04:09.000 --> 01:04:12.520]   comes along for the ride as you scale up in intelligence,
[01:04:12.520 --> 01:04:16.000]   well then we could find ourselves creating conscious minds
[01:04:16.000 --> 01:04:17.160]   that are miserable, right?
[01:04:17.160 --> 01:04:19.720]   And that's just like creating a person who's miserable.
[01:04:19.720 --> 01:04:21.440]   Right, it could be worse than creating a person
[01:04:21.440 --> 01:04:23.160]   who's miserable, it could be even more sensitive
[01:04:23.160 --> 01:04:24.000]   to suffering.
[01:04:24.000 --> 01:04:26.320]   - Cloning them and maybe for entertainment,
[01:04:26.320 --> 01:04:27.960]   watching them suffer.
[01:04:27.960 --> 01:04:31.240]   - Just like watching a person suffer for entertainment.
[01:04:31.240 --> 01:04:36.040]   So, but back to your primary question here,
[01:04:36.040 --> 01:04:40.560]   which is differentiating consciousness and self
[01:04:40.560 --> 01:04:43.160]   and free will as concepts and kind of degrees
[01:04:43.160 --> 01:04:44.340]   of illusoriness.
[01:04:44.340 --> 01:04:48.720]   The problem with free will is that
[01:04:48.720 --> 01:04:55.120]   what most people mean by it, and Dan,
[01:04:55.120 --> 01:04:57.360]   this is where Dan Dennett is gonna get off the ride here.
[01:04:57.360 --> 01:04:59.960]   Right, so like he doesn't, he's gonna disagree with me
[01:04:59.960 --> 01:05:02.240]   that I know what most people mean by it.
[01:05:02.240 --> 01:05:07.240]   But I have a very keen sense, having talked about this topic
[01:05:07.240 --> 01:05:11.760]   for many, many years and seeing people get wrapped
[01:05:11.760 --> 01:05:15.600]   around the axle of it and seeing in myself
[01:05:15.600 --> 01:05:18.820]   what it's like to have felt that I was a self
[01:05:18.820 --> 01:05:21.800]   that had free will and then to no longer feel that way.
[01:05:21.800 --> 01:05:23.400]   Right, I mean, to know what it's like to actually
[01:05:23.400 --> 01:05:26.880]   disabuse myself of that sense,
[01:05:26.880 --> 01:05:30.000]   cognitively and emotionally.
[01:05:30.000 --> 01:05:32.640]   And to recognize what's left, what goes away
[01:05:32.640 --> 01:05:35.940]   and what doesn't go away on the basis of that epiphany.
[01:05:35.940 --> 01:05:40.240]   I have a sense that I know what people think
[01:05:40.240 --> 01:05:42.020]   they have in hand when they worry about
[01:05:42.020 --> 01:05:44.480]   whether free will exists.
[01:05:44.480 --> 01:05:49.480]   And it is the flip side of this feeling of self.
[01:05:49.480 --> 01:05:53.320]   It's the flip side of feeling like you are
[01:05:53.320 --> 01:05:57.020]   not merely identical to experience,
[01:05:57.020 --> 01:05:59.040]   you feel like you're having an experience.
[01:05:59.040 --> 01:06:01.600]   You feel like you're an agent that is appropriating
[01:06:01.600 --> 01:06:02.440]   an experience.
[01:06:02.440 --> 01:06:05.120]   There's a protagonist in the movie of your life
[01:06:05.120 --> 01:06:06.220]   and it is you.
[01:06:06.220 --> 01:06:09.440]   It's not just the movie, right?
[01:06:09.440 --> 01:06:13.420]   It's like there's sights and sounds and sensations
[01:06:13.420 --> 01:06:17.060]   and thoughts and emotions and this whole cacophony
[01:06:17.060 --> 01:06:18.840]   of experience, of felt experience,
[01:06:18.840 --> 01:06:21.300]   of felt experience of embodiment.
[01:06:21.300 --> 01:06:26.080]   But there seems to be a rider on the horse
[01:06:26.080 --> 01:06:28.360]   or a passenger in the body, right?
[01:06:28.360 --> 01:06:30.840]   People don't feel truly identical to their bodies
[01:06:30.840 --> 01:06:32.800]   down to their toes.
[01:06:32.800 --> 01:06:34.760]   They sort of feel like they have bodies.
[01:06:34.760 --> 01:06:37.080]   They feel like their minds in bodies
[01:06:37.080 --> 01:06:42.080]   and that feels like a self, that feels like me.
[01:06:42.080 --> 01:06:45.080]   And again, this gets very paradoxical
[01:06:45.080 --> 01:06:48.880]   when you talk about the experience of being
[01:06:48.880 --> 01:06:51.280]   in relationship to yourself or talking to yourself,
[01:06:51.280 --> 01:06:52.400]   giving yourself a pep talk.
[01:06:52.400 --> 01:06:54.240]   I mean, if you're the one talking,
[01:06:54.240 --> 01:06:55.800]   why are you also the one listening?
[01:06:55.800 --> 01:06:57.880]   Like, why do you need the pep talk and why does it work?
[01:06:57.880 --> 01:07:00.000]   If you're the one giving the pep talk, right?
[01:07:00.000 --> 01:07:02.120]   Or if I say, where are my keys?
[01:07:02.120 --> 01:07:03.780]   Right, if I'm looking for my keys,
[01:07:03.780 --> 01:07:06.760]   why do I think the superfluous thought, where are my keys?
[01:07:06.760 --> 01:07:08.440]   I know I'm looking for the fucking keys.
[01:07:08.440 --> 01:07:11.400]   I'm the one looking, who am I telling
[01:07:11.400 --> 01:07:13.840]   that we now need to look for the keys, right?
[01:07:13.840 --> 01:07:16.320]   So that duality is weird, but leave that aside.
[01:07:16.320 --> 01:07:22.240]   There's the sense, and this becomes very vivid
[01:07:22.240 --> 01:07:25.400]   when people try to learn to meditate.
[01:07:25.400 --> 01:07:28.760]   Most people, they close their eyes
[01:07:28.760 --> 01:07:30.000]   and they're told to pay attention
[01:07:30.000 --> 01:07:32.000]   to an object like the breath, say.
[01:07:32.000 --> 01:07:35.320]   So you close your eyes and you pay attention to the breath
[01:07:35.320 --> 01:07:37.280]   and you can feel it at the tip of your nose
[01:07:37.280 --> 01:07:39.600]   or the rising and falling of your abdomen.
[01:07:39.600 --> 01:07:42.280]   And you're paying attention
[01:07:42.280 --> 01:07:44.960]   and you feel something vague there.
[01:07:44.960 --> 01:07:46.920]   And then you think, I thought, why the breath?
[01:07:46.920 --> 01:07:49.720]   Why am I paying attention to the breath?
[01:07:49.720 --> 01:07:51.840]   What's so special about the breath?
[01:07:51.840 --> 01:07:54.200]   And then you notice you're thinking
[01:07:54.200 --> 01:07:55.720]   and you're not paying attention to the breath anymore.
[01:07:55.720 --> 01:07:58.000]   And then you realize, okay, the practice is,
[01:07:58.000 --> 01:07:59.860]   okay, I should notice thoughts
[01:07:59.860 --> 01:08:01.960]   and then I should come back to the breath.
[01:08:01.960 --> 01:08:06.760]   But this starting point is of the conventional starting point
[01:08:06.760 --> 01:08:10.160]   of feeling like you are an agent, very likely in your head,
[01:08:10.160 --> 01:08:12.940]   a locus of consciousness, a locus of attention
[01:08:12.940 --> 01:08:15.820]   that can strategically pay attention
[01:08:15.820 --> 01:08:17.200]   to certain parts of experience.
[01:08:17.200 --> 01:08:18.880]   Like I can focus on the breath
[01:08:18.880 --> 01:08:20.580]   and then I get lost in thought
[01:08:20.580 --> 01:08:22.400]   and now I can come back to the breath
[01:08:22.400 --> 01:08:23.460]   and I can open my eyes
[01:08:23.460 --> 01:08:26.080]   and I'm over here behind my face
[01:08:26.080 --> 01:08:28.660]   looking out at a world that's other than me
[01:08:28.660 --> 01:08:31.880]   and there's this kind of subject-object perception.
[01:08:31.880 --> 01:08:35.200]   And that is the default starting point of selfhood,
[01:08:35.200 --> 01:08:36.920]   of subjectivity.
[01:08:36.920 --> 01:08:41.060]   And married to that is the sense
[01:08:41.060 --> 01:08:46.060]   that I can decide what to do next.
[01:08:46.060 --> 01:08:50.340]   I am an agent who can pay attention to the cup.
[01:08:50.340 --> 01:08:52.020]   I can listen to sounds.
[01:08:52.020 --> 01:08:53.580]   There are certain things that I can't control.
[01:08:53.580 --> 01:08:54.640]   Certain things are happening to me
[01:08:54.640 --> 01:08:55.880]   and I just can't control them.
[01:08:55.880 --> 01:08:59.000]   So for instance, if someone asks,
[01:08:59.000 --> 01:09:02.360]   well, can you not hear a sound, right?
[01:09:02.360 --> 01:09:03.840]   Like don't hear the next sound.
[01:09:03.840 --> 01:09:05.280]   Don't hear anything for a second.
[01:09:05.280 --> 01:09:07.280]   Or don't hear, don't hear,
[01:09:07.280 --> 01:09:09.220]   you know, I'm snapping my fingers, don't hear this.
[01:09:09.220 --> 01:09:10.160]   Where's your free will?
[01:09:10.160 --> 01:09:12.580]   You know, well, like just stop this from coming in.
[01:09:12.580 --> 01:09:14.640]   You realize, okay, wait a minute.
[01:09:14.640 --> 01:09:18.320]   My abundant freedom does not extend
[01:09:18.320 --> 01:09:20.240]   to something as simple as just being able
[01:09:20.240 --> 01:09:22.560]   to pay attention to something else than this.
[01:09:22.560 --> 01:09:25.860]   Okay, well, so I'm not that kind of free agent,
[01:09:25.860 --> 01:09:28.900]   but at least I can decide what I'm gonna do next.
[01:09:28.900 --> 01:09:32.040]   I'm gonna pick up this water, right?
[01:09:32.040 --> 01:09:36.720]   And there's a feeling of identification
[01:09:36.720 --> 01:09:39.680]   with the impulse, with the intention,
[01:09:39.680 --> 01:09:41.200]   with the thought that occurs to you,
[01:09:41.200 --> 01:09:43.440]   with the feeling of speaking.
[01:09:43.440 --> 01:09:45.520]   Like, you know, what am I gonna say next?
[01:09:45.520 --> 01:09:47.780]   Well, I'm saying it, so here goes.
[01:09:47.780 --> 01:09:48.920]   This is me.
[01:09:48.920 --> 01:09:50.560]   It feels like I'm the thinker.
[01:09:50.560 --> 01:09:53.780]   I'm the one who's in control.
[01:09:53.780 --> 01:09:59.760]   But all of that is born of not really paying close attention
[01:09:59.760 --> 01:10:02.000]   to what it's like to be you.
[01:10:02.000 --> 01:10:05.240]   And so this is where meditation comes in,
[01:10:05.240 --> 01:10:07.160]   or this is where, again,
[01:10:07.160 --> 01:10:10.000]   you can get at this conceptually.
[01:10:10.000 --> 01:10:11.760]   You can unravel the notion of free will
[01:10:11.760 --> 01:10:15.760]   just by thinking certain thoughts,
[01:10:15.760 --> 01:10:18.900]   but you can't feel that it doesn't exist,
[01:10:18.900 --> 01:10:20.720]   unless you can pay close attention
[01:10:20.720 --> 01:10:22.960]   to how thoughts and intentions arise.
[01:10:22.960 --> 01:10:24.600]   So the way to unravel it conceptually
[01:10:24.600 --> 01:10:27.320]   is just to realize, okay, I didn't make myself.
[01:10:27.320 --> 01:10:28.380]   I didn't make my genes.
[01:10:28.380 --> 01:10:29.300]   I didn't make my brain.
[01:10:29.300 --> 01:10:31.760]   I didn't make the environmental influences
[01:10:31.760 --> 01:10:35.160]   that impinged upon this system for the last 54 years
[01:10:35.160 --> 01:10:38.840]   that have produced my brain in precisely the state
[01:10:38.840 --> 01:10:39.920]   it's in right now,
[01:10:39.920 --> 01:10:42.820]   such, I mean, with all of the receptor weightings
[01:10:42.820 --> 01:10:45.660]   and densities, and, you know, it's just,
[01:10:45.660 --> 01:10:48.520]   I'm exactly the machine I am right now
[01:10:48.520 --> 01:10:53.520]   through no fault of my own as the experiencing self.
[01:10:53.520 --> 01:10:56.880]   I get no credit and I get no blame
[01:10:56.880 --> 01:11:00.600]   for the genetics and the environmental influences here.
[01:11:00.600 --> 01:11:05.600]   And yet those are the only things that contrive
[01:11:05.600 --> 01:11:10.680]   to produce my next thought or impulse
[01:11:10.680 --> 01:11:12.880]   or moment of behavior.
[01:11:12.880 --> 01:11:14.880]   And if you were going to add something magical
[01:11:14.880 --> 01:11:17.600]   to that clockwork, like an immortal soul,
[01:11:18.440 --> 01:11:21.040]   you can also notice that you didn't produce your soul,
[01:11:21.040 --> 01:11:22.760]   right, like you can't account for the fact
[01:11:22.760 --> 01:11:25.960]   that you don't have the soul of someone
[01:11:25.960 --> 01:11:28.320]   who doesn't like any of the things you like
[01:11:28.320 --> 01:11:29.880]   or wasn't interested in any of the things
[01:11:29.880 --> 01:11:31.280]   you were interested in or, you know,
[01:11:31.280 --> 01:11:35.120]   or was a psychopath or was, you know, had an IQ of 40.
[01:11:35.120 --> 01:11:38.680]   I mean, like there's nothing about that
[01:11:38.680 --> 01:11:41.560]   that the person who believes in a soul
[01:11:41.560 --> 01:11:43.320]   can claim to have controlled.
[01:11:43.320 --> 01:11:45.840]   And yet that is also totally dispositive
[01:11:45.840 --> 01:11:48.400]   of whatever happens next.
[01:11:48.400 --> 01:11:51.160]   - But everything you've described now,
[01:11:51.160 --> 01:11:52.920]   maybe you can correct me, but it kind of speaks
[01:11:52.920 --> 01:11:55.780]   to the materialistic nature of the hardware.
[01:11:55.780 --> 01:12:01.200]   - But even if you add magical ectoplasm software,
[01:12:01.200 --> 01:12:03.080]   you didn't produce that either.
[01:12:03.080 --> 01:12:06.000]   - I know, but if we can think about
[01:12:06.000 --> 01:12:09.680]   the actual computation running on the hardware
[01:12:09.680 --> 01:12:11.060]   and running on the software,
[01:12:11.060 --> 01:12:12.700]   there's something you said recently,
[01:12:12.700 --> 01:12:17.240]   which you think of culture as an operating system.
[01:12:17.240 --> 01:12:21.120]   So if we just remove ourselves a little bit
[01:12:21.120 --> 01:12:24.120]   from the conception of human civilization
[01:12:24.120 --> 01:12:26.100]   being a collection of humans,
[01:12:26.100 --> 01:12:31.100]   and rather us just being a distributed computation system
[01:12:31.100 --> 01:12:34.080]   on which there's some kind of operating system running,
[01:12:34.080 --> 01:12:36.480]   and then the computation that's running
[01:12:36.480 --> 01:12:39.700]   is the actual thing that generates the interactions,
[01:12:39.700 --> 01:12:42.120]   the communications, and maybe even free will,
[01:12:42.120 --> 01:12:44.260]   the experiences of all those free will.
[01:12:44.260 --> 01:12:47.200]   Do you ever think of, do you ever try to reframe the world
[01:12:47.200 --> 01:12:51.720]   in that way, where it's like ideas are just using us,
[01:12:51.720 --> 01:12:56.520]   thoughts are using individual nodes in the system,
[01:12:56.520 --> 01:12:58.040]   and they're just jumping around,
[01:12:58.040 --> 01:13:01.480]   and they also have ability to generate experiences
[01:13:01.480 --> 01:13:03.600]   so that we can push those ideas along?
[01:13:03.600 --> 01:13:05.820]   And basically the main organisms here
[01:13:05.820 --> 01:13:07.960]   are the thoughts, not the humans.
[01:13:07.960 --> 01:13:11.800]   - Yeah, but then that erodes the boundary
[01:13:11.800 --> 01:13:15.200]   between self and world.
[01:13:15.200 --> 01:13:16.040]   - Right.
[01:13:16.040 --> 01:13:19.640]   - So then there's no self, a really integrated self
[01:13:19.640 --> 01:13:22.400]   to have any kind of will at all.
[01:13:22.400 --> 01:13:24.480]   Like if you're just a meme plex,
[01:13:24.480 --> 01:13:28.640]   I mean, if you're just a collection of memes,
[01:13:28.640 --> 01:13:32.000]   and I mean, we're all kind of like currents,
[01:13:32.000 --> 01:13:35.920]   like eddies in this river of ideas, right?
[01:13:35.920 --> 01:13:40.720]   So it's like, and it seems to have structure,
[01:13:40.720 --> 01:13:42.440]   but there's no real boundary
[01:13:42.440 --> 01:13:44.920]   between that part of the flow of water and the rest.
[01:13:44.920 --> 01:13:47.900]   I mean, if our, and I would say that much of our mind
[01:13:47.900 --> 01:13:49.640]   answers to this kind of description.
[01:13:49.640 --> 01:13:52.620]   I mean, so much of our mind has been,
[01:13:52.620 --> 01:13:55.400]   it's obviously not self-generated,
[01:13:55.400 --> 01:13:58.000]   and you're not gonna find it by looking in the brain.
[01:13:58.000 --> 01:14:03.000]   It is the result of culture largely,
[01:14:03.000 --> 01:14:05.200]   but also, you know,
[01:14:09.680 --> 01:14:12.080]   the genes on one side and culture on the other
[01:14:12.080 --> 01:14:16.440]   meeting to allow for
[01:14:16.440 --> 01:14:22.640]   manifestations of mind that aren't actually bounded
[01:14:22.640 --> 01:14:26.800]   by the person in any clear sense.
[01:14:26.800 --> 01:14:31.220]   It was just, the example I often use here,
[01:14:31.220 --> 01:14:32.320]   but there's so many others,
[01:14:32.320 --> 01:14:34.880]   is just the fact that we're following
[01:14:34.880 --> 01:14:38.000]   the rules of English grammar to whatever degree we are.
[01:14:38.000 --> 01:14:40.180]   It's not that, we certainly haven't consciously
[01:14:40.180 --> 01:14:42.520]   represented these rules for ourself.
[01:14:42.520 --> 01:14:44.240]   We haven't invented these rules.
[01:14:44.240 --> 01:14:48.680]   We haven't, I mean, there are norms of language use
[01:14:48.680 --> 01:14:53.040]   that we couldn't even specify because we haven't,
[01:14:53.040 --> 01:14:54.480]   you know, we're not grammarians.
[01:14:54.480 --> 01:14:56.520]   We haven't studied this.
[01:14:56.520 --> 01:14:58.280]   We don't even have the right concepts,
[01:14:58.280 --> 01:15:00.000]   and yet we're following these rules,
[01:15:00.000 --> 01:15:04.480]   and we're noticing as an error
[01:15:04.480 --> 01:15:06.340]   when we fail to follow these rules.
[01:15:07.180 --> 01:15:11.260]   And virtually every other cultural norm is like that.
[01:15:11.260 --> 01:15:13.220]   I mean, these are not things we've invented.
[01:15:13.220 --> 01:15:17.260]   You can consciously decide to scrutinize them
[01:15:17.260 --> 01:15:20.420]   and override them, but I mean,
[01:15:20.420 --> 01:15:23.740]   just think of any social situation
[01:15:23.740 --> 01:15:25.700]   where you're with other people,
[01:15:25.700 --> 01:15:28.300]   and you're behaving in ways
[01:15:28.300 --> 01:15:31.740]   that are culturally appropriate, right?
[01:15:31.740 --> 01:15:34.740]   You're not being wild animals together.
[01:15:34.740 --> 01:15:36.700]   You're following, so you have some expectation
[01:15:36.700 --> 01:15:39.100]   of how you shake a person's hand
[01:15:39.100 --> 01:15:43.580]   and how you deal with implements on a table,
[01:15:43.580 --> 01:15:45.120]   how you have a meal together.
[01:15:45.120 --> 01:15:47.660]   Obviously, this can change from culture to culture,
[01:15:47.660 --> 01:15:49.500]   and people can be shocked
[01:15:49.500 --> 01:15:51.380]   by how different those things are, right?
[01:15:51.380 --> 01:15:54.100]   We all have foods we find disgusting,
[01:15:54.100 --> 01:15:57.500]   but in some countries, dog is not one of those foods, right?
[01:15:57.500 --> 01:16:00.500]   And yet, you and I presumably would be horrified
[01:16:00.500 --> 01:16:01.500]   to be served dog.
[01:16:03.820 --> 01:16:07.180]   Those are not norms that we're,
[01:16:07.180 --> 01:16:09.340]   they are outside of us in some way,
[01:16:09.340 --> 01:16:13.580]   and yet they're felt very viscerally.
[01:16:13.580 --> 01:16:16.140]   I mean, they're certainly felt in their violation.
[01:16:16.140 --> 01:16:18.340]   You know, if you are, just imagine,
[01:16:18.340 --> 01:16:21.420]   you're in somebody's home,
[01:16:21.420 --> 01:16:23.860]   you're eating something that tastes great to you,
[01:16:23.860 --> 01:16:25.900]   and you happen to be in Vietnam or wherever,
[01:16:25.900 --> 01:16:27.980]   you know, you didn't realize dog
[01:16:27.980 --> 01:16:29.500]   was potentially on the menu,
[01:16:29.500 --> 01:16:33.500]   and you find out that you've just eaten 10 bites
[01:16:33.500 --> 01:16:37.300]   of what is really a cocker spaniel,
[01:16:37.300 --> 01:16:42.300]   and you feel this instantaneous urge to vomit, right,
[01:16:42.300 --> 01:16:44.900]   based on an idea, right?
[01:16:44.900 --> 01:16:47.020]   Like, so you did not,
[01:16:47.020 --> 01:16:49.900]   you're not the author of that norm
[01:16:49.900 --> 01:16:55.620]   that gave you such a powerful experience of its violation,
[01:16:55.620 --> 01:16:59.620]   and I'm sure we can trace the moment in your history,
[01:16:59.620 --> 01:17:01.340]   you know, vaguely where it sort of got in.
[01:17:01.340 --> 01:17:02.940]   I mean, very early on as kids,
[01:17:02.940 --> 01:17:05.940]   you realize you're treating dogs as pets
[01:17:05.940 --> 01:17:08.400]   and not as food, or as potential food.
[01:17:08.400 --> 01:17:13.260]   But yeah, no, it's,
[01:17:13.260 --> 01:17:16.780]   but the point you just made opens us to,
[01:17:16.780 --> 01:17:21.780]   like, we are totally permeable to a sea of mind.
[01:17:21.780 --> 01:17:24.380]   - Yeah, but if we take the metaphor
[01:17:24.380 --> 01:17:26.220]   of the distributed computing systems,
[01:17:26.220 --> 01:17:30.860]   each individual node is part of performing
[01:17:30.860 --> 01:17:32.860]   a much larger computation,
[01:17:32.860 --> 01:17:36.420]   but it nevertheless is in charge of doing the scheduling.
[01:17:36.420 --> 01:17:39.260]   So, assuming it's Linux,
[01:17:39.260 --> 01:17:41.060]   is doing the scheduling of processes
[01:17:41.060 --> 01:17:42.700]   and is constantly alternating them.
[01:17:42.700 --> 01:17:46.220]   That node is making those choices.
[01:17:46.220 --> 01:17:49.240]   That node sure as hell believes it has free will,
[01:17:49.240 --> 01:17:51.100]   and it actually has free will
[01:17:51.100 --> 01:17:53.140]   'cause it's making those hard choices,
[01:17:53.140 --> 01:17:54.900]   but the choices ultimately are part
[01:17:54.900 --> 01:17:57.340]   of a much larger computation that it can't control.
[01:17:57.340 --> 01:17:59.920]   Isn't it possible for that node to still be,
[01:17:59.920 --> 01:18:04.080]   that human node is still making the choice?
[01:18:04.080 --> 01:18:05.120]   - Well, yeah, it is.
[01:18:05.120 --> 01:18:08.840]   So, I'm not saying that your body
[01:18:08.840 --> 01:18:11.680]   isn't doing, really doing things, right?
[01:18:11.680 --> 01:18:14.080]   And some of those things can be
[01:18:14.080 --> 01:18:16.440]   conventionally thought of as choices, right?
[01:18:16.440 --> 01:18:19.160]   So, it's like I can choose to reach,
[01:18:19.160 --> 01:18:21.640]   and it's like, it's not being imposed on me.
[01:18:21.640 --> 01:18:22.800]   That would be a different experience.
[01:18:22.800 --> 01:18:25.640]   Like, so there's an experience of,
[01:18:25.640 --> 01:18:27.960]   you know, there's definitely a difference
[01:18:27.960 --> 01:18:30.040]   between voluntary and involuntary action.
[01:18:30.040 --> 01:18:34.060]   So, that has to get conserved.
[01:18:34.060 --> 01:18:36.600]   By any account of the mind that jettisons free will,
[01:18:36.600 --> 01:18:39.280]   you still have to admit that there's a difference
[01:18:39.280 --> 01:18:42.920]   between a tremor that I can't control
[01:18:42.920 --> 01:18:47.640]   and a purposeful motor action that I can control
[01:18:47.640 --> 01:18:49.140]   and that I can initiate on demand,
[01:18:49.140 --> 01:18:50.940]   and it's associated with intentions.
[01:18:50.940 --> 01:18:55.760]   And it's got efferent motor copy,
[01:18:55.760 --> 01:18:59.660]   which is being predictive so that I can notice errors.
[01:18:59.660 --> 01:19:00.940]   You know, I have expectations.
[01:19:00.940 --> 01:19:02.040]   When I reach for this,
[01:19:02.040 --> 01:19:04.440]   if my hand were actually to pass through the bottle,
[01:19:04.440 --> 01:19:07.160]   because it's a hologram, I would be surprised, right?
[01:19:07.160 --> 01:19:09.160]   And so, that shows that I have a expectation
[01:19:09.160 --> 01:19:10.940]   of just what my grasping behavior
[01:19:10.940 --> 01:19:13.760]   is going to be like even before it happens.
[01:19:13.760 --> 01:19:14.620]   Whereas with a tremor,
[01:19:14.620 --> 01:19:17.120]   you don't have the same kind of thing going on.
[01:19:17.120 --> 01:19:19.960]   That's a distinction we have to make.
[01:19:19.960 --> 01:19:22.400]   So, I am, yes, I'm really the,
[01:19:22.400 --> 01:19:25.200]   my intention to move,
[01:19:25.200 --> 01:19:29.000]   which is in fact can be subjectively felt,
[01:19:29.000 --> 01:19:31.000]   really is the proximate cause of my moving.
[01:19:31.000 --> 01:19:33.320]   It's not coming from elsewhere in the universe.
[01:19:33.320 --> 01:19:35.040]   I'm not saying that.
[01:19:35.040 --> 01:19:37.560]   So, in that sense, the node is really deciding
[01:19:37.560 --> 01:19:42.260]   to execute the subroutine now.
[01:19:42.260 --> 01:19:47.260]   But that's not the feeling
[01:19:49.720 --> 01:19:54.720]   that has given rise to this conundrum of free will, right?
[01:19:54.720 --> 01:19:57.020]   So, people feel like,
[01:19:57.020 --> 01:20:00.160]   people feel like they,
[01:20:00.160 --> 01:20:01.760]   crucial thing is that people feel like
[01:20:01.760 --> 01:20:04.280]   they could have done otherwise, right?
[01:20:04.280 --> 01:20:05.840]   That's the thing.
[01:20:05.840 --> 01:20:09.440]   So, when you run back the clock of your life, right?
[01:20:09.440 --> 01:20:11.220]   You run back the movie of your life.
[01:20:11.220 --> 01:20:14.600]   You flip back the few pages in the novel of your life.
[01:20:14.600 --> 01:20:18.000]   They feel that at this point,
[01:20:18.000 --> 01:20:20.800]   they could behave differently than they did, right?
[01:20:20.800 --> 01:20:23.240]   So, like, but given,
[01:20:23.240 --> 01:20:25.860]   even given your distributed computing example,
[01:20:25.860 --> 01:20:30.460]   it's either a fully deterministic system
[01:20:30.460 --> 01:20:32.100]   or it's a deterministic system
[01:20:32.100 --> 01:20:36.200]   that admits of some random influence.
[01:20:36.200 --> 01:20:37.740]   In either case,
[01:20:37.740 --> 01:20:41.560]   that's not the free will people think they have.
[01:20:41.560 --> 01:20:43.600]   The free will people think they have is,
[01:20:43.600 --> 01:20:46.520]   damn, I shouldn't have done that.
[01:20:46.520 --> 01:20:49.400]   I just, like, I shouldn't have done that.
[01:20:49.400 --> 01:20:51.400]   I could have done otherwise, right?
[01:20:51.400 --> 01:20:52.680]   I should have done otherwise, right?
[01:20:52.680 --> 01:20:55.360]   Like, if you think about something
[01:20:55.360 --> 01:20:57.620]   that you deeply regret doing, right?
[01:20:57.620 --> 01:21:00.820]   Or that you hold someone else responsible for
[01:21:00.820 --> 01:21:03.280]   because they really are the upstream agent
[01:21:03.280 --> 01:21:04.920]   in your mind of what they did.
[01:21:04.920 --> 01:21:08.080]   That's an awful thing that that person did
[01:21:08.080 --> 01:21:09.680]   and they shouldn't have done it.
[01:21:09.680 --> 01:21:12.920]   There is this illusion and it has to be an illusion
[01:21:12.920 --> 01:21:17.120]   because there's no picture of causation
[01:21:17.120 --> 01:21:18.960]   that would make sense of it.
[01:21:18.960 --> 01:21:21.920]   There's this illusion that if you arrange the universe
[01:21:21.920 --> 01:21:24.440]   exactly the way it was a moment ago,
[01:21:24.440 --> 01:21:27.040]   it could have played out differently.
[01:21:27.040 --> 01:21:31.600]   And the only way it could have played out differently
[01:21:31.600 --> 01:21:34.840]   is if there's randomness added to that,
[01:21:34.840 --> 01:21:37.840]   but randomness isn't what people feel
[01:21:37.840 --> 01:21:39.380]   would give them free will, right?
[01:21:39.380 --> 01:21:41.160]   If you tell me that, you know,
[01:21:41.160 --> 01:21:44.000]   I only reached for the water bottle this time
[01:21:44.000 --> 01:21:45.080]   because somebody's,
[01:21:45.080 --> 01:21:47.080]   because there's a random number generator in there
[01:21:47.080 --> 01:21:51.240]   kicking off values and it finally moved my hand.
[01:21:51.240 --> 01:21:54.080]   That's not the feeling of authorship.
[01:21:54.080 --> 01:21:55.680]   - That's still not control.
[01:21:55.680 --> 01:21:58.160]   You're still not making that decision.
[01:21:58.160 --> 01:22:00.560]   There's actually, I don't know if you're familiar
[01:22:00.560 --> 01:22:01.800]   with cellular automata.
[01:22:01.800 --> 01:22:03.560]   It's a really nice visualization
[01:22:03.560 --> 01:22:07.800]   of how simple rules can create incredible complexity
[01:22:07.800 --> 01:22:11.040]   that it's like really dumb initial conditions to set,
[01:22:11.040 --> 01:22:12.640]   simple rules applied,
[01:22:12.640 --> 01:22:14.960]   and eventually you watch this thing,
[01:22:14.960 --> 01:22:18.640]   and if the initial conditions are correct,
[01:22:18.640 --> 01:22:21.360]   then you're going to have emerge something
[01:22:21.360 --> 01:22:23.200]   that to our perception system
[01:22:23.200 --> 01:22:25.440]   looks like organisms interacting.
[01:22:25.440 --> 01:22:27.120]   You can construct any kinds of worlds,
[01:22:27.120 --> 01:22:29.800]   and they're not actually interacting.
[01:22:29.800 --> 01:22:32.000]   They're not actually even organisms,
[01:22:32.000 --> 01:22:34.920]   and they certainly aren't making decisions.
[01:22:34.920 --> 01:22:37.120]   So there's like systems you can create
[01:22:37.120 --> 01:22:38.560]   that illustrate this point.
[01:22:38.560 --> 01:22:42.840]   The question is whether there could be some room
[01:22:42.840 --> 01:22:47.440]   for let's use in the 21st century the term magic,
[01:22:47.440 --> 01:22:50.200]   back to the black box of consciousness.
[01:22:50.200 --> 01:22:51.840]   Let me ask you this way.
[01:22:51.840 --> 01:22:56.360]   If you're wrong about your intuition about free will,
[01:22:56.360 --> 01:22:58.440]   what, and somebody comes along to you
[01:22:58.440 --> 01:23:03.320]   and proves to you that you didn't have the full picture,
[01:23:03.320 --> 01:23:04.880]   what would that proof look like?
[01:23:04.880 --> 01:23:05.720]   What would--
[01:23:05.720 --> 01:23:06.680]   - Well, that's the problem.
[01:23:06.680 --> 01:23:09.600]   That's why it's not even an illusion in my world
[01:23:09.600 --> 01:23:14.440]   because for me it's impossible to say
[01:23:14.440 --> 01:23:16.840]   what the universe would have to be like
[01:23:16.840 --> 01:23:18.840]   for free will to be a thing.
[01:23:18.840 --> 01:23:20.960]   It doesn't conceptually map onto
[01:23:20.960 --> 01:23:24.720]   any notion of causation we have,
[01:23:24.720 --> 01:23:29.400]   and that's unlike any other spurious claim you might make.
[01:23:29.400 --> 01:23:32.840]   So like if you're going to believe in ghosts,
[01:23:34.680 --> 01:23:37.880]   I understand what that claim could be.
[01:23:37.880 --> 01:23:40.160]   Where like I don't happen to believe in ghosts,
[01:23:40.160 --> 01:23:44.280]   but it's not hard for me to specify
[01:23:44.280 --> 01:23:47.320]   what would have to be true for ghosts to be real.
[01:23:47.320 --> 01:23:50.040]   And so it is with a thousand other things like ghosts.
[01:23:50.040 --> 01:23:52.040]   Right, so like, okay, so you're telling me
[01:23:52.040 --> 01:23:54.820]   that when people die, there's some part of them
[01:23:54.820 --> 01:23:57.440]   that is not reducible at all to their biology
[01:23:57.440 --> 01:24:00.600]   that lifts off them and goes elsewhere
[01:24:00.600 --> 01:24:02.160]   and is actually the kind of thing
[01:24:02.160 --> 01:24:04.640]   that can linger in closets and in cupboards
[01:24:04.640 --> 01:24:07.600]   and actually it's immaterial,
[01:24:07.600 --> 01:24:09.200]   but by some principle of physics,
[01:24:09.200 --> 01:24:10.240]   we don't totally understand.
[01:24:10.240 --> 01:24:14.120]   It can make sounds and knock objects
[01:24:14.120 --> 01:24:15.840]   and even occasionally show up
[01:24:15.840 --> 01:24:17.480]   so they can be visually beheld.
[01:24:17.480 --> 01:24:21.700]   And it's just, it seems like a miracle,
[01:24:21.700 --> 01:24:25.560]   but it's just some spooky noun in the universe
[01:24:25.560 --> 01:24:27.160]   that we don't understand.
[01:24:27.160 --> 01:24:28.260]   Let's call it a ghost.
[01:24:28.260 --> 01:24:31.920]   That's fine, I can talk about that all day,
[01:24:31.920 --> 01:24:32.960]   the reasons to believe in it,
[01:24:32.960 --> 01:24:34.120]   the reasons not to believe in it,
[01:24:34.120 --> 01:24:36.320]   the way we would scientifically test for it,
[01:24:36.320 --> 01:24:38.160]   what would have to be provable
[01:24:38.160 --> 01:24:40.900]   so as to convince me that ghosts are real.
[01:24:40.900 --> 01:24:44.240]   Free will isn't like that at all.
[01:24:44.240 --> 01:24:49.240]   There's no description of any concatenation of causes
[01:24:49.240 --> 01:24:52.100]   that precedes my conscious experience
[01:24:52.100 --> 01:24:55.280]   that sounds like what people think they have
[01:24:55.280 --> 01:24:56.960]   when they think they could have done otherwise
[01:24:56.960 --> 01:25:00.160]   and that they really, that they, the conscious agent,
[01:25:00.160 --> 01:25:01.860]   is really in charge, right?
[01:25:01.860 --> 01:25:05.020]   Like if you don't know what you're going to think next,
[01:25:05.020 --> 01:25:07.420]   right, and you can't help but think it,
[01:25:07.420 --> 01:25:12.840]   take those two premises on board.
[01:25:12.840 --> 01:25:14.760]   You don't know what it's gonna be,
[01:25:14.760 --> 01:25:18.240]   you can't stop it from coming,
[01:25:18.240 --> 01:25:21.820]   and until you actually know how to meditate,
[01:25:21.820 --> 01:25:24.660]   you can't stop yourself from
[01:25:24.660 --> 01:25:31.220]   fully living out its behavioral or emotional consequences.
[01:25:31.220 --> 01:25:32.980]   Right, like you have no, once you,
[01:25:32.980 --> 01:25:36.580]   mindfulness arguably gives you
[01:25:36.580 --> 01:25:38.380]   another degree of freedom here.
[01:25:38.380 --> 01:25:39.300]   It doesn't give you free will,
[01:25:39.300 --> 01:25:41.140]   but it gives you some other game to play
[01:25:41.140 --> 01:25:43.780]   with respect to the emotional
[01:25:43.780 --> 01:25:46.220]   and behavioral imperatives of thoughts.
[01:25:46.220 --> 01:25:50.300]   But short of that, I mean,
[01:25:50.300 --> 01:25:52.180]   the reason why mindfulness doesn't give you free will
[01:25:52.180 --> 01:25:53.620]   is because you can't, you know,
[01:25:53.620 --> 01:25:57.260]   you can't account for why in one moment mindfulness arises,
[01:25:57.260 --> 01:26:00.020]   and in other moments it doesn't, right?
[01:26:00.020 --> 01:26:03.500]   But a different process is initiated
[01:26:03.500 --> 01:26:06.820]   once you can practice in that way.
[01:26:06.820 --> 01:26:08.860]   - Well, if I could push back for a second.
[01:26:08.860 --> 01:26:11.140]   By the way, I just have this thought bubble
[01:26:11.140 --> 01:26:14.100]   popping up all the time of just two recent chimps
[01:26:14.100 --> 01:26:16.740]   arguing about the nature of consciousness.
[01:26:16.740 --> 01:26:17.860]   It's kind of hilarious.
[01:26:17.860 --> 01:26:20.460]   So on that thread, you know,
[01:26:20.460 --> 01:26:22.860]   if we're, even before Einstein,
[01:26:22.860 --> 01:26:24.340]   let's say before Einstein,
[01:26:24.340 --> 01:26:28.580]   we were to conceive about traveling from point A to point B,
[01:26:29.580 --> 01:26:32.420]   say some point in the future,
[01:26:32.420 --> 01:26:34.940]   we are able to realize through engineering
[01:26:34.940 --> 01:26:39.380]   a way which is consistent with Einstein's theory
[01:26:39.380 --> 01:26:40.380]   that you can have wormholes.
[01:26:40.380 --> 01:26:42.780]   You can travel from one point to another
[01:26:42.780 --> 01:26:44.420]   faster than the speed of light.
[01:26:44.420 --> 01:26:47.620]   And that would, I think,
[01:26:47.620 --> 01:26:49.460]   completely change our conception
[01:26:49.460 --> 01:26:52.900]   of what it means to travel in the physical space.
[01:26:52.900 --> 01:26:57.460]   And that, like, completely transform our ability.
[01:26:57.460 --> 01:26:58.620]   You talk about causality,
[01:26:58.620 --> 01:27:01.300]   but here let's just focus on what it means
[01:27:01.300 --> 01:27:03.380]   to travel through physical space.
[01:27:03.380 --> 01:27:05.780]   Don't you think it's possible that there will be
[01:27:05.780 --> 01:27:11.300]   inventions or leaps in understanding about reality
[01:27:11.300 --> 01:27:15.980]   that will allow us to see free will as actually,
[01:27:15.980 --> 01:27:19.340]   like, us humans somehow may be linked
[01:27:19.340 --> 01:27:21.380]   to this idea of consciousness,
[01:27:21.380 --> 01:27:24.480]   are actually able to be authors of our actions?
[01:27:25.580 --> 01:27:29.500]   It is a non-starter for me conceptually.
[01:27:29.500 --> 01:27:30.980]   It's a little bit like saying,
[01:27:30.980 --> 01:27:34.420]   could there be some breakthrough
[01:27:34.420 --> 01:27:39.420]   that will cause us to realize that circles are really square
[01:27:39.420 --> 01:27:43.020]   or that circles are not really round, right?
[01:27:43.020 --> 01:27:47.620]   No, a circle is what we mean by a perfectly round form.
[01:27:47.620 --> 01:27:52.420]   Right, like, it's not on the table to be revised.
[01:27:52.420 --> 01:27:54.700]   And so I would say the same thing about consciousness.
[01:27:54.700 --> 01:27:58.900]   It's just like saying, is there some breakthrough
[01:27:58.900 --> 01:28:00.020]   that would get us to realize
[01:28:00.020 --> 01:28:02.660]   that consciousness is really an illusion?
[01:28:02.660 --> 01:28:06.540]   I'm saying no, because what the experience of an illusion
[01:28:06.540 --> 01:28:09.020]   is as much a demonstration of what I'm calling consciousness
[01:28:09.020 --> 01:28:09.860]   as anything else, right?
[01:28:09.860 --> 01:28:11.420]   It's like, that is consciousness.
[01:28:11.420 --> 01:28:15.340]   With free will, it's a similar problem.
[01:28:15.340 --> 01:28:16.180]   It's like,
[01:28:16.180 --> 01:28:22.620]   again, it comes down to a picture of causality
[01:28:22.620 --> 01:28:27.020]   and there's no other picture on offer.
[01:28:27.020 --> 01:28:28.300]   And what's more,
[01:28:28.300 --> 01:28:34.220]   I know what it's like on the experiential side
[01:28:34.220 --> 01:28:39.220]   to lose the thing to which it is clearly anchored, right?
[01:28:39.220 --> 01:28:41.660]   The feel like, it doesn't feel,
[01:28:41.660 --> 01:28:43.740]   and this is the question that almost nobody asked,
[01:28:43.740 --> 01:28:46.500]   people who are debating me on the topic of free will,
[01:28:46.500 --> 01:28:51.980]   15 minute intervals, I'm making a claim
[01:28:51.980 --> 01:28:53.500]   that I don't feel this thing,
[01:28:53.500 --> 01:28:58.220]   and they never become interested in,
[01:28:58.220 --> 01:28:59.220]   well, what's that like?
[01:28:59.220 --> 01:29:02.500]   Like, okay, so you're actually saying you don't,
[01:29:02.500 --> 01:29:05.880]   this thing isn't true for you empirically.
[01:29:05.880 --> 01:29:07.580]   It's not just, because most people
[01:29:07.580 --> 01:29:11.140]   who don't believe in free will philosophically
[01:29:11.140 --> 01:29:15.140]   also believe that we're condemned to experience it.
[01:29:15.140 --> 01:29:18.180]   Like, you can't live without this feeling.
[01:29:18.180 --> 01:29:20.700]   - So you're actually saying
[01:29:20.700 --> 01:29:23.620]   you're able to experience the absence of
[01:29:23.620 --> 01:29:27.060]   the illusion of free will.
[01:29:27.060 --> 01:29:28.060]   - Yes, yes.
[01:29:28.060 --> 01:29:32.620]   - For, are we talking about a few minutes at a time,
[01:29:32.620 --> 01:29:37.620]   or is this to require a lot of work and meditation,
[01:29:37.620 --> 01:29:41.140]   or are you literally able to load that into your mind
[01:29:41.140 --> 01:29:42.020]   and like play that movie?
[01:29:42.020 --> 01:29:44.660]   - Right now, right now, just in this conversation.
[01:29:44.660 --> 01:29:48.980]   So it's not absolutely continuous,
[01:29:48.980 --> 01:29:51.580]   but it's whenever I pay attention.
[01:29:51.580 --> 01:29:52.900]   It's like, it's the same,
[01:29:52.900 --> 01:29:55.540]   and I would say the same thing for the illusoriness
[01:29:55.540 --> 01:29:56.740]   of the self in the sense,
[01:29:56.740 --> 01:29:58.940]   and again, we haven't talked about this, so.
[01:29:58.940 --> 01:30:00.340]   - Can you still have the self
[01:30:00.340 --> 01:30:02.820]   and not have the free will in your mind at the same time?
[01:30:02.820 --> 01:30:03.940]   Do they go at the same time?
[01:30:03.940 --> 01:30:06.500]   - This is the same, yeah, it's the same thing that--
[01:30:06.500 --> 01:30:07.500]   - They're always holding hands
[01:30:07.500 --> 01:30:08.780]   and when they walk out the door.
[01:30:08.780 --> 01:30:10.620]   - They really are two sides of the same coin.
[01:30:10.620 --> 01:30:13.300]   But it's just, it comes down to what it's like
[01:30:13.300 --> 01:30:16.540]   to try to get to the end of the sentence,
[01:30:16.540 --> 01:30:18.940]   or what it's like to finally decide
[01:30:18.940 --> 01:30:20.420]   that it's been long enough
[01:30:20.420 --> 01:30:22.180]   and now I need another sip of water, right?
[01:30:22.180 --> 01:30:25.860]   If I'm paying attention, now, if I'm not paying attention,
[01:30:25.860 --> 01:30:28.720]   I'm probably, I'm captured by some other thought
[01:30:28.720 --> 01:30:30.700]   and that feels a certain way, right?
[01:30:30.700 --> 01:30:32.260]   And so that's not, it's not vivid,
[01:30:32.260 --> 01:30:35.540]   but if I try to make vivid this experience of just,
[01:30:35.540 --> 01:30:38.260]   okay, I'm finally going to experience free will.
[01:30:38.260 --> 01:30:40.420]   I'm going to notice my free will, right?
[01:30:40.420 --> 01:30:43.260]   Like it's got to be here, everyone's talking about it.
[01:30:43.260 --> 01:30:44.100]   Where is it?
[01:30:44.100 --> 01:30:44.920]   I'm going to pay attention to it.
[01:30:44.920 --> 01:30:45.760]   I'm going to look for it.
[01:30:45.760 --> 01:30:48.620]   And I'm going to create a circumstance
[01:30:48.620 --> 01:30:52.180]   that is where it has to be most robust, right?
[01:30:52.180 --> 01:30:54.660]   I'm not rushed to make this decision.
[01:30:54.660 --> 01:30:57.060]   I'm not, it's not a reflex.
[01:30:57.060 --> 01:30:58.660]   I'm not under pressure.
[01:30:58.660 --> 01:30:59.940]   I'm going to take as long as I want.
[01:30:59.940 --> 01:31:02.060]   I'm going to decide.
[01:31:02.060 --> 01:31:02.940]   It's not trivial.
[01:31:02.940 --> 01:31:04.740]   Like, so it's not just like reaching with my left hand
[01:31:04.740 --> 01:31:05.680]   or reaching with my right hand.
[01:31:05.680 --> 01:31:07.700]   People don't like those examples for some reason.
[01:31:07.700 --> 01:31:09.220]   Let's make a big decision.
[01:31:09.220 --> 01:31:13.220]   Like, where should, you know,
[01:31:14.380 --> 01:31:16.340]   what should my next podcast be on, right?
[01:31:16.340 --> 01:31:18.580]   Who do I invite on the next podcast?
[01:31:18.580 --> 01:31:20.600]   What is it like to make that decision?
[01:31:20.600 --> 01:31:22.800]   When I pay attention,
[01:31:22.800 --> 01:31:26.680]   there is no evidence of free will anywhere in sight.
[01:31:26.680 --> 01:31:28.160]   It's like, it doesn't feel like,
[01:31:28.160 --> 01:31:31.220]   it feels profoundly mysterious
[01:31:31.220 --> 01:31:33.400]   to be going back between two people.
[01:31:33.400 --> 01:31:37.600]   You know, like, is it going to be person A or person B?
[01:31:37.600 --> 01:31:40.240]   Got all my reasons for A and all my reasons why not,
[01:31:40.240 --> 01:31:41.240]   and all my reasons for B,
[01:31:41.240 --> 01:31:43.960]   and there's some math going on there
[01:31:43.960 --> 01:31:46.460]   that I'm not even privy to,
[01:31:46.460 --> 01:31:49.380]   where certain concerns are trumping others.
[01:31:49.380 --> 01:31:52.880]   And at a certain point, I just decide.
[01:31:52.880 --> 01:31:56.020]   And yes, you can say I'm the node in the network
[01:31:56.020 --> 01:31:57.540]   that has made that decision, absolutely.
[01:31:57.540 --> 01:32:00.360]   I'm not saying it's being piped to me from elsewhere,
[01:32:00.360 --> 01:32:04.580]   but the feeling of what it's like to make that decision
[01:32:04.580 --> 01:32:09.580]   is totally without a sense,
[01:32:11.300 --> 01:32:15.060]   a real sense of agency,
[01:32:15.060 --> 01:32:18.740]   because something simply emerges.
[01:32:18.740 --> 01:32:22.400]   It's literally as tenuous as,
[01:32:22.400 --> 01:32:25.620]   what's the next sound I'm going to hear?
[01:32:25.620 --> 01:32:29.440]   Right, or what's the next thought that's going to appear?
[01:32:29.440 --> 01:32:32.300]   And it just, something just appears, you know?
[01:32:32.300 --> 01:32:34.660]   And if something appears to cancel that something,
[01:32:34.660 --> 01:32:37.740]   like if I say, I'm going to invite her,
[01:32:37.740 --> 01:32:39.300]   and then I'm about to send the email,
[01:32:39.300 --> 01:32:42.500]   and then I think, oh, no, no, no, I can't do that.
[01:32:42.500 --> 01:32:45.180]   There was that thing in that New York article I read
[01:32:45.180 --> 01:32:47.660]   that I got to talk to this guy, right?
[01:32:47.660 --> 01:32:49.700]   That pivot at the last second,
[01:32:49.700 --> 01:32:53.940]   you can make it as muscular as you want.
[01:32:53.940 --> 01:32:55.960]   It always just comes out of the darkness.
[01:32:55.960 --> 01:32:57.780]   It's always mysterious.
[01:32:57.780 --> 01:32:59.380]   - So right, when you try to pin it down,
[01:32:59.380 --> 01:33:01.980]   you really can't ever find that free will.
[01:33:01.980 --> 01:33:06.140]   If you construct an experiment for yourself,
[01:33:06.140 --> 01:33:07.900]   and you try to really find that moment
[01:33:07.900 --> 01:33:10.380]   when you're actually making that controlled,
[01:33:10.380 --> 01:33:12.740]   author decision, it's very difficult to do.
[01:33:12.740 --> 01:33:15.900]   - And we're still, we know at this point
[01:33:15.900 --> 01:33:18.220]   that if we were scanning your brain
[01:33:18.220 --> 01:33:23.220]   in some podcast guest choosing experiment, right?
[01:33:23.220 --> 01:33:26.980]   We know at this point we would be privy
[01:33:26.980 --> 01:33:29.500]   to who you're going to pick before you are.
[01:33:29.500 --> 01:33:30.740]   You, the conscious agent.
[01:33:30.740 --> 01:33:33.100]   If we could, again, this is operationally
[01:33:33.100 --> 01:33:36.060]   a little hard to conduct, but there's enough data now
[01:33:36.060 --> 01:33:39.660]   to know that something very much like this cartoon
[01:33:39.660 --> 01:33:44.460]   is in fact true, and will ultimately be undeniable
[01:33:44.460 --> 01:33:48.100]   for people, that they'll be able to do it on themselves
[01:33:48.100 --> 01:33:49.480]   with some app.
[01:33:49.480 --> 01:33:55.540]   If you're deciding what to, where to go for dinner,
[01:33:55.540 --> 01:33:57.820]   or who to have on your podcast, or ultimately,
[01:33:57.820 --> 01:34:00.940]   who to marry, right, or what city to move to, right?
[01:34:00.940 --> 01:34:03.380]   Like you can make it as big or as small
[01:34:03.380 --> 01:34:04.540]   a decision as you want.
[01:34:05.580 --> 01:34:07.980]   We could be scanning your brain in real time,
[01:34:07.980 --> 01:34:12.540]   and at a point where you still think you're uncommitted,
[01:34:12.540 --> 01:34:17.020]   we would be able to say with arbitrary accuracy,
[01:34:17.020 --> 01:34:20.780]   all right, Lex is, he's moving to Austin, right?
[01:34:20.780 --> 01:34:21.660]   - I didn't choose that.
[01:34:21.660 --> 01:34:23.780]   - Yeah, he was, it was gonna be Austin,
[01:34:23.780 --> 01:34:24.780]   or it was gonna be Miami.
[01:34:24.780 --> 01:34:27.700]   He got, he's catching one of these two waves,
[01:34:27.700 --> 01:34:29.220]   but it's gonna be Austin.
[01:34:29.220 --> 01:34:31.780]   And at a point where you subjectively,
[01:34:31.780 --> 01:34:34.140]   if we could ask you, you would say,
[01:34:34.140 --> 01:34:36.900]   oh no, I'm still working over here.
[01:34:36.900 --> 01:34:38.860]   I'm still thinking, I'm still choosing,
[01:34:38.860 --> 01:34:40.700]   I'm still considering my options.
[01:34:40.700 --> 01:34:42.160]   - And you've spoken to this.
[01:34:42.160 --> 01:34:45.500]   In you thinking about other stuff in the world,
[01:34:45.500 --> 01:34:49.900]   it's been very useful to step away
[01:34:49.900 --> 01:34:51.460]   from this illusion of free will.
[01:34:51.460 --> 01:34:54.100]   And you argue that it probably makes a better world
[01:34:54.100 --> 01:34:55.340]   because you can be compassionate
[01:34:55.340 --> 01:34:56.860]   and empathetic towards others.
[01:34:56.860 --> 01:34:58.340]   - And toward oneself.
[01:34:58.340 --> 01:34:59.180]   - And towards oneself.
[01:34:59.180 --> 01:35:01.860]   - I mean, radically toward others,
[01:35:01.860 --> 01:35:05.780]   in that literally hate makes no sense anymore.
[01:35:05.780 --> 01:35:07.900]   I mean, there's certain things you can
[01:35:07.900 --> 01:35:10.900]   really be worried about, really want to oppose,
[01:35:10.900 --> 01:35:12.660]   really, I mean, I'm not saying you'd never have
[01:35:12.660 --> 01:35:13.820]   to kill another person.
[01:35:13.820 --> 01:35:16.380]   Like, I mean, self-defense is still a thing, right?
[01:35:16.380 --> 01:35:21.380]   But the idea that you're ever confronting
[01:35:21.380 --> 01:35:25.540]   anything other than a force of nature in the end
[01:35:25.540 --> 01:35:26.740]   goes out the window, right?
[01:35:26.740 --> 01:35:29.060]   Or does go out the window when you really pay attention.
[01:35:29.060 --> 01:35:33.380]   I'm not saying that this would be easy to grok
[01:35:33.380 --> 01:35:38.380]   if someone kills a member of your family.
[01:35:38.380 --> 01:35:40.300]   I'm not saying you can just listen to my 90 minutes
[01:35:40.300 --> 01:35:41.900]   on free will and then you should be able to see
[01:35:41.900 --> 01:35:46.540]   that person as identical to a grizzly bear or a virus.
[01:35:46.540 --> 01:35:51.540]   'Cause I mean, we are so evolved to deal with one another
[01:35:51.540 --> 01:35:57.080]   as fellow primates and as agents.
[01:35:57.940 --> 01:36:02.940]   But it's, yeah, when you're talking about the possibility
[01:36:02.940 --> 01:36:07.940]   of Christian, truly Christian forgiveness, right?
[01:36:07.940 --> 01:36:14.180]   It is like, as testified to by various saints
[01:36:14.180 --> 01:36:22.880]   of that flavor over the millennia.
[01:36:24.220 --> 01:36:28.900]   Yeah, the doorway to that is to recognize
[01:36:28.900 --> 01:36:32.580]   that no one really at bottom made themselves.
[01:36:32.580 --> 01:36:36.340]   And therefore everyone, what we're seeing really
[01:36:36.340 --> 01:36:39.100]   are differences in luck in the world.
[01:36:39.100 --> 01:36:41.820]   We're seeing people who are very, very lucky
[01:36:41.820 --> 01:36:43.620]   to have had good parents and good genes
[01:36:43.620 --> 01:36:46.340]   and to be in good societies and had good opportunities
[01:36:46.340 --> 01:36:50.060]   and to be intelligent and to be not sociopathic.
[01:36:50.060 --> 01:36:53.020]   None of it is on them.
[01:36:53.020 --> 01:36:56.580]   They're just reaping the fruits of one lottery
[01:36:56.580 --> 01:37:00.100]   after another and then showing up in the world on that basis.
[01:37:00.100 --> 01:37:06.500]   And then so it is with every malevolent asshole out there.
[01:37:06.500 --> 01:37:09.500]   He or she didn't make themself.
[01:37:09.500 --> 01:37:14.040]   Even if that weren't possible,
[01:37:14.040 --> 01:37:18.580]   the utility for self-compassion is also enormous
[01:37:18.580 --> 01:37:21.740]   because when you just look at what it's like
[01:37:21.740 --> 01:37:26.740]   to regret something or to feel shame about something
[01:37:26.740 --> 01:37:29.020]   or feel deep embarrassment about it,
[01:37:29.020 --> 01:37:33.180]   these states of mind are some of the most deranging
[01:37:33.180 --> 01:37:38.180]   experiences anyone has and the indelible reaction to them,
[01:37:38.180 --> 01:37:42.140]   the memory of the thing you said,
[01:37:42.140 --> 01:37:44.860]   the memory of the wedding toast you gave 20 years ago
[01:37:44.860 --> 01:37:47.100]   that was just mortifying, right?
[01:37:47.100 --> 01:37:50.300]   The fact that that can still make you hate yourself, right?
[01:37:50.300 --> 01:37:52.340]   And like that psychologically,
[01:37:52.340 --> 01:37:56.300]   that is a knot that can be untied, right?
[01:37:56.300 --> 01:37:57.340]   - Speak for yourself, Sam.
[01:37:57.340 --> 01:37:58.180]   - Yeah, yeah. - So clearly you're not--
[01:37:58.180 --> 01:37:59.900]   - You gave a great toast.
[01:37:59.900 --> 01:38:01.100]   It was my toast that mortified me.
[01:38:01.100 --> 01:38:02.860]   - No, no, no, that's not what I was referring to.
[01:38:02.860 --> 01:38:07.340]   I'm deeply appreciative in the same way
[01:38:07.340 --> 01:38:10.380]   that you're referring to of every moment I'm alive,
[01:38:10.380 --> 01:38:14.340]   but I'm also powered by self-hate often.
[01:38:14.340 --> 01:38:18.100]   Like several things in this conversation already
[01:38:18.100 --> 01:38:21.100]   that I've spoken, I'll be thinking about
[01:38:21.100 --> 01:38:23.100]   like that was the dumbest thing.
[01:38:23.100 --> 01:38:26.260]   You're sitting in front of Sam Harris and you said that.
[01:38:26.260 --> 01:38:29.300]   So like that, but that somehow creates
[01:38:29.300 --> 01:38:30.660]   a richer experience for me.
[01:38:30.660 --> 01:38:33.300]   I've actually come to accept that as a nice feature
[01:38:33.300 --> 01:38:35.340]   of however my brain was built.
[01:38:35.340 --> 01:38:37.340]   I don't think I wanna let go of that.
[01:38:37.340 --> 01:38:40.340]   - Well, I think the thing you wanna let go of is
[01:38:40.340 --> 01:38:46.420]   the suffering associated with it.
[01:38:46.420 --> 01:38:49.420]   So for me, so this is very,
[01:38:49.420 --> 01:38:53.420]   psychologically and ethically,
[01:38:53.420 --> 01:38:55.220]   all of this is very interesting.
[01:38:55.220 --> 01:38:57.860]   So I don't think we should ever get rid
[01:38:57.860 --> 01:38:59.500]   of things like anger, right?
[01:38:59.500 --> 01:39:02.700]   So like hatred is, hatred is divorceable from anger
[01:39:02.700 --> 01:39:06.780]   in the sense that hatred is this enduring state
[01:39:06.780 --> 01:39:09.100]   where whether you're hating somebody else
[01:39:09.100 --> 01:39:14.100]   or hating yourself, it is toxic and durable
[01:39:14.100 --> 01:39:15.900]   and ultimately useless, right?
[01:39:15.900 --> 01:39:19.540]   Like it becomes self nullifying, right?
[01:39:19.540 --> 01:39:23.300]   Like you become less capable as a person
[01:39:23.300 --> 01:39:24.500]   to solve any of your problems.
[01:39:24.500 --> 01:39:26.660]   It's not instrumental in solving the problem
[01:39:26.660 --> 01:39:29.940]   that is occasioning all this hatred.
[01:39:29.940 --> 01:39:33.540]   And anger for the most part isn't either
[01:39:33.540 --> 01:39:37.300]   except as a signal of salience that there's a problem, right?
[01:39:37.300 --> 01:39:40.280]   So if somebody does something that makes me angry,
[01:39:40.280 --> 01:39:45.100]   that just promotes this situation to conscious attention
[01:39:45.100 --> 01:39:48.900]   in a way that is stronger than my not really caring
[01:39:48.900 --> 01:39:49.740]   about it, right?
[01:39:49.740 --> 01:39:51.460]   And there are things that I think should make us angry
[01:39:51.460 --> 01:39:54.540]   in the world and there's the behavior of other people
[01:39:54.540 --> 01:39:57.460]   that should make us angry because we should respond to it.
[01:39:57.460 --> 01:39:59.140]   And so it is with yourself.
[01:39:59.140 --> 01:40:03.180]   If I do something, as a parent, if I do something stupid
[01:40:03.180 --> 01:40:05.800]   that harms one of my daughters, right?
[01:40:05.800 --> 01:40:12.380]   My experience of myself and my beliefs about free will
[01:40:12.380 --> 01:40:14.380]   closed the door to my saying,
[01:40:14.380 --> 01:40:15.820]   well, I should have done otherwise
[01:40:15.820 --> 01:40:17.660]   in the sense that if I could go back in time,
[01:40:17.660 --> 01:40:20.380]   I would have actually effectively done otherwise.
[01:40:20.380 --> 01:40:22.820]   No, I would do, given the same causes and conditions,
[01:40:22.820 --> 01:40:26.300]   I would do that thing a trillion times in a row, right?
[01:40:26.300 --> 01:40:31.300]   But, you know, regret and feeling bad about an outcome
[01:40:31.300 --> 01:40:38.140]   are still important to capacities because like, yeah,
[01:40:38.140 --> 01:40:41.740]   I desperately want my daughters to be happy and healthy.
[01:40:41.740 --> 01:40:43.460]   So if I've done something, you know,
[01:40:43.460 --> 01:40:45.980]   if I crash the car when they're in the car
[01:40:45.980 --> 01:40:47.060]   and they get injured, right?
[01:40:47.060 --> 01:40:50.820]   And I do it because I was trying to change a song
[01:40:50.820 --> 01:40:53.660]   on my playlist or something stupid,
[01:40:53.660 --> 01:40:55.600]   I'm gonna feel like a total asshole.
[01:40:55.600 --> 01:41:01.820]   How long do I stew in that feeling of regret, right?
[01:41:01.820 --> 01:41:06.980]   And what utility is there to extract
[01:41:06.980 --> 01:41:08.500]   out of this error signal?
[01:41:08.500 --> 01:41:10.140]   And then what do I do?
[01:41:10.140 --> 01:41:13.540]   We're always faced with the question of what to do next,
[01:41:13.540 --> 01:41:16.520]   right, and how to best do that thing,
[01:41:16.520 --> 01:41:18.280]   that necessary thing next.
[01:41:18.280 --> 01:41:23.280]   And how much wellbeing can we experience while doing it?
[01:41:23.280 --> 01:41:28.100]   Like how miserable do you need to be
[01:41:28.100 --> 01:41:31.120]   to solve your problems in life
[01:41:31.120 --> 01:41:34.940]   and to help solve the problems of people closest to you?
[01:41:34.940 --> 01:41:36.760]   You know, how miserable do you need to be
[01:41:36.760 --> 01:41:39.660]   to get through your to-do list today?
[01:41:39.660 --> 01:41:44.660]   Ultimately, I think you can be deeply happy
[01:41:44.660 --> 01:41:49.540]   going through all of it, right?
[01:41:49.540 --> 01:41:54.060]   And even navigating moments that are scary
[01:41:54.060 --> 01:41:59.940]   and really destabilizing to ordinary people.
[01:41:59.940 --> 01:42:04.940]   And I mean, I think, you know, again,
[01:42:04.940 --> 01:42:09.060]   I'm always up kind of at the edge of my own capacities here
[01:42:09.060 --> 01:42:11.100]   and there are all kinds of things that stress me out
[01:42:11.100 --> 01:42:12.860]   and worry me, and I mean, especially something,
[01:42:12.860 --> 01:42:14.700]   if it's, you're gonna tell me it's something with,
[01:42:14.700 --> 01:42:17.220]   you know, the health of one of my kids,
[01:42:17.220 --> 01:42:18.660]   you know, it's very hard for me,
[01:42:18.660 --> 01:42:21.700]   like it's very hard for me to be truly a quantumist
[01:42:21.700 --> 01:42:25.700]   around that, but equanimity is so useful
[01:42:25.700 --> 01:42:29.620]   the moment you're in response mode, right?
[01:42:29.620 --> 01:42:34.120]   'Cause I mean, the ordinary experience for me
[01:42:34.120 --> 01:42:38.620]   of responding to what seems like a medical emergency
[01:42:38.620 --> 01:42:43.620]   for one of my kids is to be obviously super energized
[01:42:43.620 --> 01:42:47.160]   by concern to respond to that emergency.
[01:42:47.160 --> 01:42:52.980]   But then once I'm responding, all of my fear and agitation
[01:42:52.980 --> 01:42:56.300]   and worry and, oh my God, what if this is really
[01:42:56.300 --> 01:43:00.780]   something terrible, but finding any of those thoughts
[01:43:00.780 --> 01:43:05.500]   compelling, that only diminishes my capacity as a father
[01:43:05.500 --> 01:43:08.300]   to be good company while we navigate
[01:43:08.300 --> 01:43:11.500]   this really turbulent passage, you know?
[01:43:11.500 --> 01:43:13.540]   - As you're saying this, actually, one guy comes to mind,
[01:43:13.540 --> 01:43:16.860]   which is Elon Musk, one of the really impressive things
[01:43:16.860 --> 01:43:19.980]   to me was to observe how many dramatic things
[01:43:19.980 --> 01:43:22.460]   he has to deal with throughout the day at work,
[01:43:22.460 --> 01:43:25.780]   but also if you look through his life, family too,
[01:43:25.780 --> 01:43:30.500]   and how he's very much actually, as you're describing,
[01:43:30.500 --> 01:43:33.020]   basically a practitioner of this way of thought,
[01:43:33.020 --> 01:43:36.660]   which is you're not in control,
[01:43:36.660 --> 01:43:39.900]   you're basically responding,
[01:43:39.900 --> 01:43:41.380]   no matter how traumatic the event,
[01:43:41.380 --> 01:43:44.220]   and there's no reason to sort of linger on the--
[01:43:44.220 --> 01:43:45.060]   - Well, yeah, they couldn't be--
[01:43:45.060 --> 01:43:46.820]   - On the negative feelings around that.
[01:43:46.820 --> 01:43:50.820]   - Well, so, but he's in a very specific situation,
[01:43:50.820 --> 01:43:57.060]   which is unlike normal life, you know,
[01:43:57.060 --> 01:44:00.800]   even his normal life, but normal life for most people,
[01:44:00.800 --> 01:44:02.700]   because when you just think of like, you know,
[01:44:02.700 --> 01:44:04.060]   he's running so many businesses,
[01:44:04.060 --> 01:44:08.620]   and they're highly non-standard businesses,
[01:44:08.620 --> 01:44:12.420]   so what he's seeing is everything that gets to him
[01:44:12.420 --> 01:44:14.340]   is some kind of emergency, like it wouldn't be
[01:44:14.340 --> 01:44:16.580]   getting to him, if it needs his attention,
[01:44:16.580 --> 01:44:19.860]   there's a fire somewhere, so he's constantly responding
[01:44:19.860 --> 01:44:21.820]   to fires that have to be put out,
[01:44:21.820 --> 01:44:25.480]   so there's no default expectation
[01:44:25.480 --> 01:44:27.340]   that there shouldn't be a fire, right?
[01:44:27.340 --> 01:44:29.580]   But in our normal lives, we live,
[01:44:29.580 --> 01:44:31.720]   most of us who are lucky, right,
[01:44:31.720 --> 01:44:33.180]   not everyone, obviously, on Earth,
[01:44:33.180 --> 01:44:36.600]   but most of us who are at some kind of cruising altitude
[01:44:36.600 --> 01:44:40.440]   in terms of our lives, where we're reasonably healthy,
[01:44:40.440 --> 01:44:42.120]   and life is reasonably orderly,
[01:44:42.120 --> 01:44:44.340]   and the political apparatus around us
[01:44:44.340 --> 01:44:49.160]   is reasonably functionable, so I said functionable
[01:44:49.160 --> 01:44:50.160]   for the first time in my life,
[01:44:50.160 --> 01:44:51.340]   through no free will of my own,
[01:44:51.340 --> 01:44:53.080]   so like I noticed those errors,
[01:44:53.080 --> 01:44:56.260]   and they do not feel like agency,
[01:44:56.260 --> 01:44:59.000]   and nor does the success of an utterance
[01:44:59.000 --> 01:44:59.860]   feel like agency.
[01:44:59.860 --> 01:45:06.400]   When you're looking at normal human life, right,
[01:45:06.400 --> 01:45:10.360]   where you're just trying to be happy and healthy,
[01:45:10.360 --> 01:45:14.520]   and get your work done, there's this default expectation
[01:45:14.520 --> 01:45:16.400]   that there shouldn't be fires,
[01:45:16.400 --> 01:45:18.960]   people shouldn't be getting sick or injured,
[01:45:18.960 --> 01:45:23.720]   we shouldn't be losing vast amounts of our resources,
[01:45:23.720 --> 01:45:27.480]   we should like, so when something really stark
[01:45:27.480 --> 01:45:32.480]   like that happens, people don't have that muscle,
[01:45:32.480 --> 01:45:36.240]   that they're, like I've been responding
[01:45:36.240 --> 01:45:40.960]   to emergencies all day long, seven days a week,
[01:45:40.960 --> 01:45:44.160]   in business mode, and so I have a very thick skin,
[01:45:44.160 --> 01:45:46.680]   this is just another one, what if I'm not expecting
[01:45:46.680 --> 01:45:48.720]   anything else when I wake up in the morning,
[01:45:48.720 --> 01:45:50.860]   no, we have this default sense that,
[01:45:50.860 --> 01:45:54.440]   I mean, honestly, most of us have the default sense
[01:45:54.440 --> 01:45:57.140]   that we aren't gonna die, right,
[01:45:57.140 --> 01:45:59.720]   or that we should, like maybe we're not gonna die,
[01:45:59.720 --> 01:46:02.920]   right, like death denial really is a thing,
[01:46:02.920 --> 01:46:08.060]   because, and you can see it, just like I can see
[01:46:08.060 --> 01:46:09.860]   when I reach for this bottle,
[01:46:09.860 --> 01:46:11.640]   that I was expecting it to be solid,
[01:46:11.640 --> 01:46:13.660]   because when it isn't solid, when it's a hologram,
[01:46:13.660 --> 01:46:18.660]   and I just, my fist closes on itself, I'm damn surprised,
[01:46:18.660 --> 01:46:22.160]   people are damn surprised to find out
[01:46:22.160 --> 01:46:24.720]   that they're going to die, to find out that they're sick,
[01:46:24.720 --> 01:46:27.700]   to find out that someone they love has died,
[01:46:27.700 --> 01:46:29.620]   or is going to die, so it's like,
[01:46:29.620 --> 01:46:33.620]   the fact that we are surprised by any of that
[01:46:33.620 --> 01:46:36.420]   shows us that we're living at a,
[01:46:36.420 --> 01:46:39.020]   we're living in a mode that is,
[01:46:39.020 --> 01:46:47.300]   you know, we're perpetually diverting ourselves
[01:46:47.300 --> 01:46:50.540]   from some facts that should be obvious, right,
[01:46:50.540 --> 01:46:55.000]   and the more salient we can make them,
[01:46:55.000 --> 01:46:57.720]   you know, the more, I mean, in the case of death,
[01:46:57.720 --> 01:47:01.080]   it's a matter of being able to get one's priorities straight,
[01:47:01.080 --> 01:47:04.780]   I mean, the moment, and this is hard for everybody,
[01:47:04.780 --> 01:47:06.600]   even those who are really in the business
[01:47:06.600 --> 01:47:09.760]   of paying attention to it, but the moment you realize
[01:47:09.760 --> 01:47:13.480]   that every circumstance is finite, right,
[01:47:13.480 --> 01:47:15.920]   you've got a certain number of, you know,
[01:47:15.920 --> 01:47:17.080]   you've got whatever, whatever it is,
[01:47:17.080 --> 01:47:20.720]   8,000 days left in a normal span of life,
[01:47:20.720 --> 01:47:24.320]   and 8,000 is a, sounds like a big number,
[01:47:24.320 --> 01:47:25.760]   it's not that big a number, right,
[01:47:25.760 --> 01:47:29.480]   so it's just like, and then you can decide
[01:47:29.480 --> 01:47:31.600]   how you want to go through life,
[01:47:31.600 --> 01:47:34.520]   and how you want to experience each one of those days,
[01:47:34.520 --> 01:47:37.640]   and so I would, back to our jumping off point,
[01:47:37.640 --> 01:47:44.080]   I would argue that you don't want to feel self-hatred ever,
[01:47:44.120 --> 01:47:49.040]   I would argue that you don't want to really,
[01:47:49.040 --> 01:47:55.280]   really grasp onto any of those moments
[01:47:55.280 --> 01:47:58.880]   where you are internalizing the fact
[01:47:58.880 --> 01:48:01.440]   that you just made an error, you've embarrassed yourself,
[01:48:01.440 --> 01:48:03.760]   that something didn't go the way you wanted it to,
[01:48:03.760 --> 01:48:05.960]   I think you want to treat all of those moments
[01:48:05.960 --> 01:48:08.320]   very, very lightly, you want to extract
[01:48:08.320 --> 01:48:11.920]   the actionable information, it's something to learn,
[01:48:11.920 --> 01:48:16.920]   oh, I learned that when I prepare in a certain way,
[01:48:16.920 --> 01:48:20.000]   it works better than when I prepare in some other way,
[01:48:20.000 --> 01:48:23.240]   or don't prepare, right, so like, yes, lesson learned,
[01:48:23.240 --> 01:48:26.100]   you know, and do that differently, but,
[01:48:26.100 --> 01:48:33.600]   yeah, I mean, so many of us have spent so much time
[01:48:33.600 --> 01:48:40.040]   with a very dysfunctional and hostile,
[01:48:40.320 --> 01:48:45.320]   and even hateful inner voice
[01:48:45.320 --> 01:48:48.320]   governing a lot of our self-talk,
[01:48:48.320 --> 01:48:51.800]   and a lot of just our default way of being with ourselves,
[01:48:51.800 --> 01:48:54.160]   I mean, the privacy of our own minds,
[01:48:54.160 --> 01:48:58.200]   we're in the company of a real jerk a lot of the time,
[01:48:58.200 --> 01:49:03.280]   and that can't help but affect,
[01:49:03.280 --> 01:49:05.600]   I mean, forget about just your own sense of well-being,
[01:49:05.600 --> 01:49:08.560]   it can't help but limit what you're capable of
[01:49:08.560 --> 01:49:10.600]   in the world with other people.
[01:49:10.600 --> 01:49:12.160]   - I'll have to really think about that,
[01:49:12.160 --> 01:49:15.120]   I just take pride that my jerk, my inner voice jerk,
[01:49:15.120 --> 01:49:18.160]   is much less of a jerk than somebody like David Goggins,
[01:49:18.160 --> 01:49:20.160]   who's just like screaming in his ear constantly,
[01:49:20.160 --> 01:49:23.200]   so I just have a relativist kind of perspective
[01:49:23.200 --> 01:49:25.800]   that it's not as bad as that, at least.
[01:49:25.800 --> 01:49:28.240]   - Well, having a sense of humor also helps,
[01:49:28.240 --> 01:49:29.520]   you know, it's just like it's not,
[01:49:29.520 --> 01:49:32.640]   the stakes are never quite what you think they are,
[01:49:32.640 --> 01:49:34.880]   and even when they are, I mean,
[01:49:34.880 --> 01:49:39.560]   just the difference between being able to see the comedy
[01:49:39.560 --> 01:49:41.960]   of it rather than, 'cause again,
[01:49:41.960 --> 01:49:46.420]   there's this sort of dark star of self-absorption
[01:49:46.420 --> 01:49:49.160]   that pulls everything into it, right,
[01:49:49.160 --> 01:49:54.160]   and if that's the algorithm you don't want to run,
[01:49:54.160 --> 01:49:57.600]   so it's like you just want things to be good,
[01:49:57.600 --> 01:50:01.040]   so just push the concern out there,
[01:50:02.760 --> 01:50:04.840]   not have the collapse of,
[01:50:04.840 --> 01:50:06.600]   oh my God, what does this say about me?
[01:50:06.600 --> 01:50:08.680]   It's just like, what does this say about,
[01:50:08.680 --> 01:50:11.480]   how do we make this meal that we're all having together
[01:50:11.480 --> 01:50:15.600]   as fun and as useful as possible?
[01:50:15.600 --> 01:50:17.480]   - And you're saying in terms of propulsion systems,
[01:50:17.480 --> 01:50:19.640]   you recommend humor as a good spaceship
[01:50:19.640 --> 01:50:23.640]   to escape the gravitational field of that darkness.
[01:50:23.640 --> 01:50:24.920]   - Well, it certainly helps, yeah.
[01:50:24.920 --> 01:50:28.040]   - Yeah, well, let me ask you a little bit
[01:50:28.040 --> 01:50:31.360]   about ego and fame, which is very interesting.
[01:50:31.360 --> 01:50:34.520]   The way you're talking, given that you're one
[01:50:34.520 --> 01:50:39.520]   of the biggest intellects, living intellects
[01:50:39.520 --> 01:50:42.360]   and minds of our time, and there's a lot of people
[01:50:42.360 --> 01:50:47.360]   that really love you and almost elevate you
[01:50:47.360 --> 01:50:50.720]   to a certain kind of status where you're like the guru.
[01:50:50.720 --> 01:50:53.320]   I'm surprised you didn't show up in a robe, in fact.
[01:50:53.320 --> 01:50:55.640]   Is there-- - A hoodie.
[01:50:55.640 --> 01:50:59.280]   That's not the highest status garment one can wear now.
[01:50:59.280 --> 01:51:02.040]   - The socially acceptable version of the robe.
[01:51:02.040 --> 01:51:04.600]   - If you're a billionaire, you wear a hoodie.
[01:51:04.600 --> 01:51:07.200]   - Is there something you can say about managing
[01:51:07.200 --> 01:51:11.480]   the effects of fame on your own mind,
[01:51:11.480 --> 01:51:15.400]   on not creating this, when you wake up in the morning,
[01:51:15.400 --> 01:51:20.400]   when you look in the mirror, how do you get your ego
[01:51:20.400 --> 01:51:25.320]   not to grow exponentially, your conception of self
[01:51:25.320 --> 01:51:27.400]   to grow exponentially, because there's so many people
[01:51:27.400 --> 01:51:30.200]   feeding that, is there something to be said about this?
[01:51:30.200 --> 01:51:33.640]   - It's really not hard, because I feel like I have
[01:51:33.640 --> 01:51:38.640]   a pretty clear sense of my strengths and weaknesses,
[01:51:38.640 --> 01:51:44.880]   and I don't feel like it's, honestly, I don't feel
[01:51:44.880 --> 01:51:47.880]   like I suffer from much grandiosity.
[01:51:47.880 --> 01:51:52.640]   There's so many things I'm not good at,
[01:51:52.640 --> 01:51:55.640]   there's so many things I will, given the remaining
[01:51:55.640 --> 01:52:00.040]   8,000 days at best, I will never get good at.
[01:52:00.040 --> 01:52:02.560]   I would love to be good at these things.
[01:52:02.560 --> 01:52:05.280]   So it's just, it's easy to feel diminished
[01:52:05.280 --> 01:52:08.800]   by comparison with the talents of others.
[01:52:08.800 --> 01:52:11.160]   - Do you remind yourself of all the things
[01:52:11.160 --> 01:52:14.280]   that you're not competent in?
[01:52:14.280 --> 01:52:15.560]   Is, I mean, like-- - Well, they're just
[01:52:15.560 --> 01:52:17.800]   on display for me every day, that I appreciate
[01:52:17.800 --> 01:52:19.320]   the talents of others.
[01:52:19.320 --> 01:52:20.280]   - But you notice them.
[01:52:20.280 --> 01:52:22.840]   I'm sure Stalin and Hitler did not notice
[01:52:22.840 --> 01:52:25.240]   all the ways in which they were,
[01:52:25.240 --> 01:52:28.760]   I mean, this is why absolute power corrupts absolutely,
[01:52:28.760 --> 01:52:30.760]   is you stop noticing the things
[01:52:30.760 --> 01:52:33.680]   in which you're ridiculous and wrong.
[01:52:33.680 --> 01:52:37.000]   - Right, yeah, no, I-- - Not to compare you to Stalin.
[01:52:37.000 --> 01:52:40.040]   - Yeah, well, I'm sure there's an inner Stalin
[01:52:40.040 --> 01:52:41.320]   in there somewhere.
[01:52:41.320 --> 01:52:43.600]   - Well, we all have, we all carry a baby Stalin with us.
[01:52:43.600 --> 01:52:44.840]   - He wears better clothes.
[01:52:44.840 --> 01:52:49.040]   And I'm not gonna grow that mustache.
[01:52:49.040 --> 01:52:51.680]   Those concerns don't map, they don't map onto me
[01:52:51.680 --> 01:52:53.960]   for a bunch of reasons, but one is,
[01:52:53.960 --> 01:52:56.120]   I also have a very peculiar audience.
[01:52:56.120 --> 01:53:00.680]   I like, I'm just, I've been appreciating this
[01:53:00.680 --> 01:53:04.160]   for a few years, but it's, I'm just now
[01:53:04.160 --> 01:53:06.600]   beginning to understand that there are many people
[01:53:06.600 --> 01:53:09.840]   who have audiences of my size or larger
[01:53:09.840 --> 01:53:11.560]   that have a very different experience
[01:53:11.560 --> 01:53:13.280]   of having an audience than I do.
[01:53:13.280 --> 01:53:17.040]   I have curated, for better or worse,
[01:53:17.040 --> 01:53:22.040]   a peculiar audience, and the net result of that is
[01:53:23.040 --> 01:53:28.040]   virtually any time I say anything of substance,
[01:53:28.040 --> 01:53:30.600]   something like half of my audience,
[01:53:30.600 --> 01:53:33.080]   my real audience, not haters from outside my audience,
[01:53:33.080 --> 01:53:38.080]   but my audience just revolts over it, right?
[01:53:38.080 --> 01:53:41.000]   They just like, oh my God, I can't believe you said,
[01:53:41.000 --> 01:53:43.200]   like you're such a schmuck, right?
[01:53:43.200 --> 01:53:47.040]   - They revolt with rigor and intellectual sophistication.
[01:53:47.040 --> 01:53:49.480]   - Or not, or not, but I mean, it's both,
[01:53:49.480 --> 01:53:51.040]   but it's like, but people who are like,
[01:53:51.040 --> 01:53:53.280]   so I mean, the clearest case is,
[01:53:53.280 --> 01:53:55.200]   you know, I have whatever audience I have,
[01:53:55.200 --> 01:53:56.820]   and then Trump appears on the scene,
[01:53:56.820 --> 01:54:00.600]   and I discover that something like 20% of my audience
[01:54:00.600 --> 01:54:03.840]   just went straight to Trump and couldn't believe
[01:54:03.840 --> 01:54:05.120]   I didn't follow them there.
[01:54:05.120 --> 01:54:06.920]   They were just aghast that I didn't see
[01:54:06.920 --> 01:54:10.040]   that Trump was obviously exactly what we needed
[01:54:10.040 --> 01:54:15.040]   for, to steer the ship of state for the next four years,
[01:54:15.040 --> 01:54:17.000]   and then four years beyond that.
[01:54:17.000 --> 01:54:20.400]   So like, so that's one example.
[01:54:20.400 --> 01:54:22.640]   So whenever I said anything about Trump,
[01:54:22.640 --> 01:54:25.760]   I would hear from people who loved more or less
[01:54:25.760 --> 01:54:28.920]   everything else I was up to and had for years,
[01:54:28.920 --> 01:54:33.380]   but everything I said about Trump just gave me pure pain
[01:54:33.380 --> 01:54:36.160]   from this quadrant of my audience.
[01:54:36.160 --> 01:54:39.560]   But then the same thing happens when I say something
[01:54:39.560 --> 01:54:42.000]   about the derangement of the far left.
[01:54:42.000 --> 01:54:46.360]   Anything I say about wokeness, right, or identity politics,
[01:54:46.360 --> 01:54:48.560]   same kind of punishment signal from us.
[01:54:48.560 --> 01:54:51.760]   Again, people who are core to my audience,
[01:54:51.760 --> 01:54:53.520]   like I've read all your books,
[01:54:53.520 --> 01:54:55.680]   I'm using your meditation app,
[01:54:55.680 --> 01:54:57.920]   I love what you say about science,
[01:54:57.920 --> 01:55:00.080]   but you are so wrong about politics,
[01:55:00.080 --> 01:55:01.560]   and you are, you know, I'm starting to think
[01:55:01.560 --> 01:55:03.160]   you're a racist asshole for everything you said
[01:55:03.160 --> 01:55:05.120]   about identity politics.
[01:55:05.120 --> 01:55:07.860]   And there are so many,
[01:55:07.860 --> 01:55:10.040]   the free will topic is just like this.
[01:55:10.040 --> 01:55:13.220]   It's like, just, they love what I'm saying
[01:55:13.220 --> 01:55:15.120]   about consciousness and the mind,
[01:55:15.120 --> 01:55:18.280]   and they love to hear me talk about physics with physicists,
[01:55:18.280 --> 01:55:20.420]   and it's all good.
[01:55:20.420 --> 01:55:21.600]   This free will stuff is,
[01:55:21.600 --> 01:55:24.460]   I cannot believe you don't see how wrong you are.
[01:55:24.460 --> 01:55:26.760]   What a fucking embarrassment you are.
[01:55:26.760 --> 01:55:30.360]   So, but I'm starting to notice that there are other people
[01:55:30.360 --> 01:55:33.480]   who don't have this experience of having an audience
[01:55:33.480 --> 01:55:35.000]   because they have, I mean,
[01:55:35.000 --> 01:55:37.360]   just take the Trump-Woke dichotomy.
[01:55:37.360 --> 01:55:41.680]   They just castigated Trump the same way I did,
[01:55:41.680 --> 01:55:44.160]   but they never say anything bad about the far left.
[01:55:44.160 --> 01:55:45.760]   So they never get this punishment signal,
[01:55:45.760 --> 01:55:46.880]   or you flip it.
[01:55:46.880 --> 01:55:51.880]   They're all about the insanity of critical race theory now.
[01:55:51.880 --> 01:55:56.760]   We connect all those dots the same way,
[01:55:56.760 --> 01:56:00.400]   but they never really specified what was wrong with Trump,
[01:56:00.400 --> 01:56:02.440]   or they thought there was a lot right with Trump,
[01:56:02.440 --> 01:56:04.880]   and they got all the pleasure of that.
[01:56:04.880 --> 01:56:08.600]   And so they have much more homogenized audiences.
[01:56:08.600 --> 01:56:13.440]   And so my experience, so just to come back to, you know,
[01:56:13.440 --> 01:56:15.400]   this experience of fame or quasi-fame,
[01:56:15.400 --> 01:56:18.840]   and truth is not real fame,
[01:56:18.840 --> 01:56:22.060]   but still there's an audience there.
[01:56:22.060 --> 01:56:28.440]   It is a, it's now an experience where basically
[01:56:28.440 --> 01:56:30.980]   whatever I put out,
[01:56:30.980 --> 01:56:34.400]   I notice a ton of negativity coming back at me.
[01:56:34.400 --> 01:56:37.840]   And it just, it is what it is.
[01:56:37.840 --> 01:56:40.320]   I mean, now it's like, I used to think,
[01:56:40.320 --> 01:56:42.060]   wait a minute, there's gotta be some way
[01:56:42.060 --> 01:56:44.000]   for me to communicate more clearly here
[01:56:44.000 --> 01:56:49.000]   so as not to get this kind of lunatic response
[01:56:49.000 --> 01:56:50.960]   from my own audience,
[01:56:50.960 --> 01:56:53.500]   from like people who are showing all the signs of,
[01:56:53.500 --> 01:56:57.140]   we've been here for years for a reason, right?
[01:56:57.140 --> 01:56:59.080]   These are not just trolls.
[01:56:59.080 --> 01:57:01.880]   And so I think, okay, I'm gonna take 10 more minutes
[01:57:01.880 --> 01:57:05.000]   and really just tell you what,
[01:57:05.000 --> 01:57:06.520]   it should be absolutely clear
[01:57:06.520 --> 01:57:07.720]   about what's wrong with Trump.
[01:57:07.720 --> 01:57:09.440]   Right, I've done this a few times,
[01:57:09.440 --> 01:57:11.640]   but I think I gotta do this again.
[01:57:11.640 --> 01:57:15.040]   Or wait a minute, how are they not getting
[01:57:15.040 --> 01:57:17.600]   that these episodes of police violence
[01:57:17.600 --> 01:57:19.720]   are so obviously different from one another
[01:57:19.720 --> 01:57:22.040]   that you can't describe all of them
[01:57:22.040 --> 01:57:26.800]   to yet another racist maniac on the police force,
[01:57:26.800 --> 01:57:28.660]   killing someone based on his racism.
[01:57:28.660 --> 01:57:33.480]   Last time I spoke about this, it was pure pain,
[01:57:33.480 --> 01:57:35.600]   but I just gotta try again.
[01:57:35.600 --> 01:57:37.160]   Now at a certain point, I mean,
[01:57:37.160 --> 01:57:38.920]   I'm starting to feel like, all right,
[01:57:38.920 --> 01:57:42.400]   I just, I have to be, I have to cease.
[01:57:42.400 --> 01:57:44.880]   Again, it comes back to this expectation
[01:57:44.880 --> 01:57:46.360]   that there shouldn't be fires.
[01:57:46.360 --> 01:57:51.200]   Like I feel like if I could just play my game impeccably,
[01:57:51.200 --> 01:57:55.360]   the people who actually care what I think will follow me
[01:57:55.360 --> 01:58:00.280]   when I hit Trump and hit free will and hit the woke
[01:58:00.280 --> 01:58:02.200]   and hit whatever it is,
[01:58:02.200 --> 01:58:04.240]   how we should respond to the coronavirus,
[01:58:04.240 --> 01:58:07.400]   vaccines, are they a thing, right?
[01:58:07.400 --> 01:58:12.200]   Like there's such derangement in our information space now
[01:58:12.200 --> 01:58:15.760]   that I mean, I guess some people could be getting more
[01:58:15.760 --> 01:58:18.040]   of this than I expect, but I just noticed
[01:58:18.040 --> 01:58:22.160]   that many of our friends who are in the same game
[01:58:22.160 --> 01:58:24.920]   have more homogenized audiences and don't get,
[01:58:24.920 --> 01:58:28.240]   I mean, they've successfully filtered out
[01:58:28.240 --> 01:58:32.280]   the people who are gonna despise them on this next topic.
[01:58:32.280 --> 01:58:36.260]   And I would imagine you have a different experience
[01:58:36.260 --> 01:58:38.120]   of having a podcast than I do at this point.
[01:58:38.120 --> 01:58:40.440]   I mean, I'm sure you get haters,
[01:58:40.440 --> 01:58:45.120]   but I would imagine you're more streamlined.
[01:58:45.120 --> 01:58:46.880]   - I actually don't like the word haters
[01:58:46.880 --> 01:58:51.880]   because it kind of presumes that it puts people in a bin.
[01:58:51.880 --> 01:58:55.800]   I think we all have like baby haters inside of us
[01:58:55.800 --> 01:58:59.040]   and we just apply them and some people enjoy doing that
[01:58:59.040 --> 01:59:01.880]   more than others for particular periods of time.
[01:59:01.880 --> 01:59:04.800]   I think you're gonna almost see hating on the internet
[01:59:04.800 --> 01:59:07.340]   as a video game that you just play and it's fun,
[01:59:07.340 --> 01:59:09.560]   but then you can put it down and walk away.
[01:59:09.560 --> 01:59:12.020]   And no, I certainly have a bunch of people
[01:59:12.020 --> 01:59:13.180]   that are very critical.
[01:59:13.180 --> 01:59:14.620]   I can list all the ways.
[01:59:14.620 --> 01:59:16.700]   - But does it feel like on any given topic,
[01:59:16.700 --> 01:59:19.740]   does it feel like it's an actual title surge
[01:59:19.740 --> 01:59:21.900]   where it's like 30% of your audience
[01:59:21.900 --> 01:59:24.700]   and then the other 30% of your audience
[01:59:24.700 --> 01:59:25.860]   from podcast to podcast?
[01:59:25.860 --> 01:59:26.820]   - No, no, no.
[01:59:26.820 --> 01:59:29.460]   - That's happening to me all the time now.
[01:59:29.460 --> 01:59:31.300]   - Well, I'm more with, I don't know
[01:59:31.300 --> 01:59:32.140]   what you think about this.
[01:59:32.140 --> 01:59:35.260]   I mean, Joe Rogan doesn't read comments
[01:59:35.260 --> 01:59:36.820]   or doesn't read comments much.
[01:59:36.820 --> 01:59:40.580]   And the argument he made to me is that
[01:59:40.580 --> 01:59:47.300]   he already has like a self-critical person inside.
[01:59:47.300 --> 01:59:50.540]   Like, and I'm gonna have to think about
[01:59:50.540 --> 01:59:51.640]   what you said in this conversation,
[01:59:51.640 --> 01:59:55.000]   but I have this very harshly self-critical person
[01:59:55.000 --> 01:59:58.120]   inside as well where I don't need more fuel.
[01:59:58.120 --> 02:00:01.520]   I don't need, no, I do sometimes,
[02:00:01.520 --> 02:00:05.900]   that's why I check negativity occasionally, not too often.
[02:00:05.900 --> 02:00:08.260]   I sometimes need to like put a little bit more
[02:00:08.260 --> 02:00:11.940]   like coals into the fire, but not too much.
[02:00:11.940 --> 02:00:13.860]   But I already have that self-critical engine
[02:00:13.860 --> 02:00:15.100]   that keeps me in check.
[02:00:15.100 --> 02:00:17.940]   I just, I wonder, you know, a lot of people
[02:00:17.940 --> 02:00:22.740]   who gain more and more fame lose that ability
[02:00:22.740 --> 02:00:24.540]   to be self-critical, I guess,
[02:00:24.540 --> 02:00:25.860]   because they lose the audience
[02:00:25.860 --> 02:00:27.560]   that can be critical towards them.
[02:00:27.560 --> 02:00:30.420]   - You know, I do follow Joe's advice
[02:00:30.420 --> 02:00:31.820]   much more than I ever have here.
[02:00:31.820 --> 02:00:34.100]   Like, I don't look at comments very often,
[02:00:34.100 --> 02:00:37.820]   and I'm probably using Twitter, you know,
[02:00:37.820 --> 02:00:41.600]   5% as much as I used to.
[02:00:41.600 --> 02:00:44.020]   I mean, I really just get in and out on Twitter
[02:00:44.020 --> 02:00:46.740]   and spend very little time in my ad mentions.
[02:00:46.740 --> 02:00:51.260]   But, you know, it does, in some ways it feels like a loss
[02:00:51.260 --> 02:00:53.340]   because occasionally I see something
[02:00:53.340 --> 02:00:54.940]   super intelligent there.
[02:00:54.940 --> 02:00:57.220]   Like, I mean, I'll check my Twitter ad mentions
[02:00:57.220 --> 02:00:59.980]   and someone will have said, oh, have you read this article?
[02:00:59.980 --> 02:01:02.020]   And it's like, man, that was just,
[02:01:02.020 --> 02:01:05.100]   that was like the best article sent to me in a month, right?
[02:01:05.100 --> 02:01:06.740]   So it's like, to have not have looked
[02:01:06.740 --> 02:01:09.980]   and to not have seen that, that's a loss.
[02:01:09.980 --> 02:01:14.980]   So, but it does, at this point a little goes a long way
[02:01:14.980 --> 02:01:17.860]   'cause it's not that it,
[02:01:17.860 --> 02:01:22.220]   for me now, I mean, this could sound
[02:01:22.220 --> 02:01:26.740]   like a fairly Stalinistic immunity to criticism.
[02:01:26.740 --> 02:01:29.340]   It's not so much that these voices of hate
[02:01:29.340 --> 02:01:33.140]   turn on my inner hater, you know, more.
[02:01:33.140 --> 02:01:35.340]   It's more that I just, I get a,
[02:01:35.340 --> 02:01:40.340]   what I fear is a false sense of humanity.
[02:01:40.340 --> 02:01:43.140]   Like, I feel like I'm too online
[02:01:43.140 --> 02:01:45.980]   and online is selecting for this performative outrage
[02:01:45.980 --> 02:01:48.380]   in everybody, everyone's signaling to an audience
[02:01:48.380 --> 02:01:50.340]   when they trash you.
[02:01:50.340 --> 02:01:53.980]   And I get a dark, I'm getting a, you know,
[02:01:55.740 --> 02:02:00.740]   misanthropic, you know, cut of just what it's like out there
[02:02:00.740 --> 02:02:04.100]   and it, 'cause when you meet people in real life,
[02:02:04.100 --> 02:02:06.620]   they're great, you know, they're rather often great,
[02:02:06.620 --> 02:02:10.820]   you know, and it takes a lot to have anything
[02:02:10.820 --> 02:02:15.080]   like a Twitter encounter in real life with a living person.
[02:02:15.080 --> 02:02:20.980]   And that's, I think it's much better to have that
[02:02:20.980 --> 02:02:24.860]   as one's default sense of what it's like to be with people
[02:02:24.860 --> 02:02:28.060]   than what one gets on social media
[02:02:28.060 --> 02:02:30.420]   or on YouTube comment threads.
[02:02:30.420 --> 02:02:33.420]   - You've produced a special episode with Rob Reed
[02:02:33.420 --> 02:02:38.180]   on your podcast recently on how bioengineering of viruses
[02:02:38.180 --> 02:02:40.380]   is going to destroy human civilization.
[02:02:40.380 --> 02:02:42.340]   So-- - Or could.
[02:02:42.340 --> 02:02:43.420]   - Could. - One fears, yeah.
[02:02:43.420 --> 02:02:45.260]   - Sorry, the confidence there.
[02:02:45.260 --> 02:02:49.460]   But in the 21st century, what do you think,
[02:02:49.460 --> 02:02:53.640]   especially after having thought through that angle,
[02:02:53.640 --> 02:02:56.060]   what do you think is the biggest threat
[02:02:56.060 --> 02:02:58.720]   to the survival of the human species?
[02:02:58.720 --> 02:03:02.780]   I can give you the full menu if you'd like.
[02:03:02.780 --> 02:03:06.660]   - Yeah, well, no, I would put the biggest threat
[02:03:06.660 --> 02:03:11.660]   at the, another level out, kind of the meta threat
[02:03:11.660 --> 02:03:16.700]   is our inability to agree about what the threats
[02:03:16.700 --> 02:03:23.340]   actually are and to converge on strategies for response
[02:03:23.840 --> 02:03:25.120]   to them, right?
[02:03:25.120 --> 02:03:28.500]   So like I view COVID as, among other things,
[02:03:28.500 --> 02:03:34.760]   a truly terrifyingly failed dress rehearsal
[02:03:34.760 --> 02:03:37.160]   for something far worse, right?
[02:03:37.160 --> 02:03:41.520]   I mean, COVID is just about as benign as it could have been
[02:03:41.520 --> 02:03:44.560]   and still have been worse than the flu
[02:03:44.560 --> 02:03:46.560]   when you're talking about a global pandemic, right?
[02:03:46.560 --> 02:03:48.440]   So it's just, it's, you know,
[02:03:48.440 --> 02:03:51.560]   it's gonna kill a few million people,
[02:03:51.560 --> 02:03:53.440]   or it looks like it's killed about 3 million people.
[02:03:53.440 --> 02:03:56.260]   Maybe it'll kill a few million more
[02:03:56.260 --> 02:03:58.160]   unless something gets away from us
[02:03:58.160 --> 02:04:00.920]   with a variant that's much worse,
[02:04:00.920 --> 02:04:02.480]   or we really don't play our cards right.
[02:04:02.480 --> 02:04:07.480]   But I mean, the general shape of it is it's got,
[02:04:07.480 --> 02:04:12.940]   you know, somewhere around, well, 1% lethality.
[02:04:12.940 --> 02:04:17.940]   And whatever side of that number it really is on
[02:04:18.280 --> 02:04:23.280]   in the end, it's not what would in fact be possible
[02:04:23.280 --> 02:04:26.520]   and is in fact probably inevitable,
[02:04:26.520 --> 02:04:29.620]   something with, you know, orders of magnitude,
[02:04:29.620 --> 02:04:30.800]   more lethality than that.
[02:04:30.800 --> 02:04:35.360]   And it's just so obvious we are totally unprepared, right?
[02:04:35.360 --> 02:04:39.800]   We are running this epidemiological experiment
[02:04:39.800 --> 02:04:41.640]   of linking the entire world together.
[02:04:41.640 --> 02:04:46.400]   And then also now per the podcast that Rob Reed did,
[02:04:47.720 --> 02:04:51.000]   democratizing the tech that will allow us to do this,
[02:04:51.000 --> 02:04:53.200]   to engineer pandemics, right?
[02:04:53.200 --> 02:04:56.920]   And more and more people will be able
[02:04:56.920 --> 02:05:00.880]   to engineer synthetic viruses that will be,
[02:05:00.880 --> 02:05:04.940]   by the sheer fact that they would have been engineered
[02:05:04.940 --> 02:05:08.600]   with malicious intent, you know, worse than COVID.
[02:05:08.600 --> 02:05:11.640]   And we're still living in,
[02:05:11.640 --> 02:05:13.880]   to speak specifically about the United States,
[02:05:13.880 --> 02:05:17.920]   we have a country here where we can't even agree
[02:05:17.920 --> 02:05:20.320]   that this is a thing, like that COVID,
[02:05:20.320 --> 02:05:21.440]   I mean, there's still people who think
[02:05:21.440 --> 02:05:25.560]   that this is basically a hoax designed to control people.
[02:05:25.560 --> 02:05:29.380]   And it's stranger still, there are people
[02:05:29.380 --> 02:05:33.320]   who will acknowledge that COVID is real
[02:05:33.320 --> 02:05:36.320]   and they don't think the deaths have been faked
[02:05:36.320 --> 02:05:38.520]   or mis-ascribed.
[02:05:42.620 --> 02:05:47.460]   But they think that they're far happier
[02:05:47.460 --> 02:05:49.480]   at the prospect of catching COVID
[02:05:49.480 --> 02:05:53.560]   than they are of getting vaccinated for COVID, right?
[02:05:53.560 --> 02:05:54.540]   They're not worried about COVID,
[02:05:54.540 --> 02:05:57.020]   they're worried about vaccines for COVID, right?
[02:05:57.020 --> 02:06:01.020]   And the fact that we just can't converge in a conversation
[02:06:01.020 --> 02:06:05.980]   that we've now had a year to have with one another
[02:06:05.980 --> 02:06:08.180]   on just what is the ground truth here?
[02:06:08.180 --> 02:06:09.820]   What's happened?
[02:06:09.820 --> 02:06:11.640]   Why has it happened?
[02:06:11.640 --> 02:06:14.580]   What's the, how safe is it to get COVID
[02:06:14.580 --> 02:06:19.260]   at every, in every cohort in the population?
[02:06:19.260 --> 02:06:21.060]   And how safe are the vaccines?
[02:06:21.060 --> 02:06:23.940]   And the fact that there's still an air of mystery
[02:06:23.940 --> 02:06:27.300]   around all of this for much of our society
[02:06:27.300 --> 02:06:30.180]   does not bode well when you're talking
[02:06:30.180 --> 02:06:32.860]   about solving any other problem that may yet kill us.
[02:06:32.860 --> 02:06:34.420]   - But do you think convergence grows
[02:06:34.420 --> 02:06:36.180]   with the magnitude of the threat?
[02:06:36.180 --> 02:06:37.700]   So- - It's possible,
[02:06:37.700 --> 02:06:40.820]   except I feel like we have tipped into,
[02:06:40.820 --> 02:06:45.820]   'cause when the threat of COVID looked the most dire, right?
[02:06:45.820 --> 02:06:48.900]   When we were seeing reports from Italy
[02:06:48.900 --> 02:06:51.540]   that looked like the beginning of a zombie movie, right?
[02:06:51.540 --> 02:06:52.780]   - 'Cause it could have been much, much worse.
[02:06:52.780 --> 02:06:55.340]   - Yeah, like this is like, this is lethal, right?
[02:06:55.340 --> 02:06:57.740]   Like your ICUs are gonna fill up in,
[02:06:57.740 --> 02:07:00.060]   like you're 14 days behind us.
[02:07:00.060 --> 02:07:04.820]   You're gonna, your medical system is in danger of collapse.
[02:07:04.820 --> 02:07:06.700]   Lock the fuck down.
[02:07:06.700 --> 02:07:11.700]   We have people refusing to do anything sane
[02:07:11.700 --> 02:07:12.540]   in the face of that.
[02:07:12.540 --> 02:07:14.940]   Like, and people fundamentally thinking,
[02:07:14.940 --> 02:07:16.420]   it's not gonna get here, right?
[02:07:16.420 --> 02:07:18.520]   Like, or that's, who knows what's going on in Italy,
[02:07:18.520 --> 02:07:20.420]   but it has no implications for what's gonna go on
[02:07:20.420 --> 02:07:23.380]   in New York in a mere six days, right?
[02:07:23.380 --> 02:07:25.300]   And now it kicks off in New York,
[02:07:25.300 --> 02:07:27.700]   and you've got people in the middle of the country
[02:07:27.700 --> 02:07:32.700]   thinking it's no factor, it's not, that's just big city.
[02:07:32.700 --> 02:07:35.540]   Those are big city problems, or they're faking it,
[02:07:35.540 --> 02:07:40.060]   or, I mean, it just, the layer of politics
[02:07:40.060 --> 02:07:42.580]   has become so dysfunctional for us
[02:07:42.580 --> 02:07:47.580]   that even in what, in the presence of a pandemic
[02:07:47.580 --> 02:07:50.880]   that looked legitimately scary there in the beginning,
[02:07:50.880 --> 02:07:52.860]   I mean, it's not to say that it hasn't been devastating
[02:07:52.860 --> 02:07:54.900]   for everyone who's been directly affected by it,
[02:07:54.900 --> 02:07:56.820]   and it's not to say it can't get worse,
[02:07:56.820 --> 02:08:00.420]   but here, for a very long time, we have known
[02:08:00.420 --> 02:08:03.760]   that we were in a situation that is more benign
[02:08:03.760 --> 02:08:07.300]   than what seemed like the worst case scenario
[02:08:07.300 --> 02:08:09.860]   as it was kicking off, especially in Italy.
[02:08:09.860 --> 02:08:15.540]   And so still, yeah, it's quite possible
[02:08:15.540 --> 02:08:18.720]   that if we saw the asteroid hurtling toward Earth,
[02:08:18.720 --> 02:08:23.020]   and everyone agreed that it's gonna make impact,
[02:08:23.020 --> 02:08:27.660]   and we're all gonna die, then we could get off Twitter
[02:08:27.660 --> 02:08:30.220]   and actually build the rockets
[02:08:30.220 --> 02:08:32.980]   that are gonna divert the asteroid
[02:08:32.980 --> 02:08:35.080]   from its Earth-crossing path,
[02:08:35.080 --> 02:08:37.720]   and we could do something pretty heroic.
[02:08:37.720 --> 02:08:41.520]   But when you talk about anything else
[02:08:41.520 --> 02:08:46.520]   that isn't, that's slower moving than that,
[02:08:46.520 --> 02:08:48.560]   I mean, something like, I mean, climate change,
[02:08:48.560 --> 02:08:53.560]   I think the prospect of our converging
[02:08:53.560 --> 02:08:56.000]   on a solution to climate change
[02:08:56.000 --> 02:08:58.900]   purely based on political persuasion
[02:08:58.900 --> 02:09:00.520]   is nonexistent at this point.
[02:09:00.520 --> 02:09:04.400]   I just think that, I mean, to bring Elon back into this,
[02:09:04.400 --> 02:09:05.960]   the way to deal with climate change
[02:09:05.960 --> 02:09:09.680]   is to create technology that everyone wants
[02:09:09.680 --> 02:09:14.540]   that is better than all the carbon-producing technology,
[02:09:14.540 --> 02:09:15.620]   and then we just transition
[02:09:15.620 --> 02:09:19.200]   because you want an electric car
[02:09:19.200 --> 02:09:20.600]   the same way you wanted a smartphone
[02:09:20.600 --> 02:09:22.680]   or you want anything else,
[02:09:22.680 --> 02:09:24.820]   and you're working totally with the grain
[02:09:24.820 --> 02:09:29.000]   of people's selfishness and short-term thinking.
[02:09:29.000 --> 02:09:31.040]   The idea that we're gonna convince
[02:09:31.040 --> 02:09:33.120]   the better part of humanity
[02:09:33.120 --> 02:09:35.400]   that climate change is an emergency,
[02:09:35.400 --> 02:09:38.960]   that they have to make sacrifices to respond to,
[02:09:38.960 --> 02:09:41.080]   given what's happened around COVID,
[02:09:41.080 --> 02:09:46.080]   I just think that's the fantasy of a fantasy.
[02:09:46.080 --> 02:09:48.120]   - But speaking of Elon,
[02:09:48.120 --> 02:09:49.640]   I have a bunch of positive things
[02:09:49.640 --> 02:09:51.640]   that I wanna say here in response to you,
[02:09:51.640 --> 02:09:53.600]   but you're opening so many threads,
[02:09:53.600 --> 02:09:54.760]   but let me pull one of them,
[02:09:54.760 --> 02:09:57.680]   which is AI.
[02:09:57.680 --> 02:10:02.400]   Both you and Elon think that with AI,
[02:10:02.400 --> 02:10:05.500]   you're summoning demons, summoning a demon.
[02:10:05.500 --> 02:10:07.500]   Maybe not in those poetic terms, but--
[02:10:07.500 --> 02:10:09.800]   - Well, potentially.
[02:10:09.800 --> 02:10:10.720]   - Potentially.
[02:10:10.720 --> 02:10:15.720]   - Two very, three very parsimonious assumptions,
[02:10:15.720 --> 02:10:19.320]   I think, here, scientifically,
[02:10:19.320 --> 02:10:21.320]   parsimonious assumptions, get me there.
[02:10:22.320 --> 02:10:26.120]   Any of which could be wrong,
[02:10:26.120 --> 02:10:28.520]   but it just seems like the weight
[02:10:28.520 --> 02:10:31.360]   of the evidence is on their side.
[02:10:31.360 --> 02:10:33.960]   One is that it comes back to this topic
[02:10:33.960 --> 02:10:36.880]   of substrate independence, right?
[02:10:36.880 --> 02:10:38.100]   Anyone who's in the business
[02:10:38.100 --> 02:10:40.400]   of producing intelligent machines
[02:10:40.400 --> 02:10:43.840]   must believe, ultimately,
[02:10:43.840 --> 02:10:45.680]   that there's nothing magical
[02:10:45.680 --> 02:10:47.340]   about having a computer made of meat.
[02:10:47.340 --> 02:10:50.740]   You can do this in the kinds of materials
[02:10:50.740 --> 02:10:53.440]   we're using now,
[02:10:53.440 --> 02:10:56.760]   and there's no special something
[02:10:56.760 --> 02:11:00.940]   that presents a real impediment
[02:11:00.940 --> 02:11:04.760]   to producing human-level intelligence in silico, right?
[02:11:04.760 --> 02:11:08.280]   Again, an assumption, I'm sure there are a few people
[02:11:08.280 --> 02:11:09.760]   who still think there is something magical
[02:11:09.760 --> 02:11:12.800]   about biological systems,
[02:11:12.800 --> 02:11:15.060]   but leave that aside.
[02:11:18.680 --> 02:11:20.500]   Given that assumption,
[02:11:20.500 --> 02:11:21.420]   and given the assumption
[02:11:21.420 --> 02:11:24.620]   that we just continue making incremental progress,
[02:11:24.620 --> 02:11:25.780]   doesn't have to be Moore's Law,
[02:11:25.780 --> 02:11:27.020]   it just has to be progress,
[02:11:27.020 --> 02:11:29.020]   that just doesn't stop,
[02:11:29.020 --> 02:11:31.140]   at a certain point, we'll get
[02:11:31.140 --> 02:11:34.580]   to human-level intelligence and beyond.
[02:11:34.580 --> 02:11:36.820]   And human-level intelligence, I think,
[02:11:36.820 --> 02:11:38.540]   is also clearly a mirage,
[02:11:38.540 --> 02:11:40.540]   because anything that's human-level
[02:11:40.540 --> 02:11:41.860]   is gonna be superhuman,
[02:11:41.860 --> 02:11:44.820]   unless we decide to dumb it down, right?
[02:11:44.820 --> 02:11:46.740]   I mean, my phone is already superhuman
[02:11:46.740 --> 02:11:47.700]   as a calculator, right?
[02:11:47.700 --> 02:11:51.240]   So why would we make the human-level AI
[02:11:51.240 --> 02:11:54.880]   just as good as me, as a calculator?
[02:11:54.880 --> 02:11:57.280]   So I think we'll very,
[02:11:57.280 --> 02:11:59.440]   if we continue to make progress,
[02:11:59.440 --> 02:12:03.800]   we will be in the presence of superhuman competence
[02:12:03.800 --> 02:12:08.800]   for any act of intelligence or cognition
[02:12:08.800 --> 02:12:11.120]   that we care to prioritize.
[02:12:11.120 --> 02:12:13.420]   It's not to say that we'll create everything
[02:12:13.420 --> 02:12:14.260]   that a human could do,
[02:12:14.260 --> 02:12:16.440]   maybe we'll leave certain things out,
[02:12:16.440 --> 02:12:18.580]   but anything that we care about,
[02:12:18.580 --> 02:12:20.540]   and we care about a lot,
[02:12:20.540 --> 02:12:21.920]   and we certainly care about anything
[02:12:21.920 --> 02:12:24.540]   that produces a lot of power,
[02:12:24.540 --> 02:12:26.940]   that we care about scientific insights
[02:12:26.940 --> 02:12:30.040]   and an ability to produce new technology
[02:12:30.040 --> 02:12:30.880]   and all of that,
[02:12:30.880 --> 02:12:34.320]   we'll have something that's superhuman.
[02:12:34.320 --> 02:12:38.000]   And then the final assumption is just that
[02:12:38.000 --> 02:12:42.680]   there have to be ways to do that
[02:12:42.680 --> 02:12:46.520]   that are not aligned with a happy coexistence
[02:12:46.520 --> 02:12:51.520]   with these now more powerful entities than ourselves.
[02:12:51.520 --> 02:12:54.200]   And I would guess,
[02:12:54.200 --> 02:12:57.180]   and this is kind of a rider to that assumption,
[02:12:57.180 --> 02:12:59.700]   there are probably more ways to do it badly
[02:12:59.700 --> 02:13:01.380]   than to do it perfectly,
[02:13:01.380 --> 02:13:05.780]   that is perfectly aligned with our well-being.
[02:13:05.780 --> 02:13:10.780]   And when you think about the consequences of non-alignment,
[02:13:10.980 --> 02:13:12.040]   when you think about,
[02:13:12.040 --> 02:13:15.580]   you're now in the presence of something
[02:13:15.580 --> 02:13:17.620]   that is more intelligent than you are,
[02:13:17.620 --> 02:13:20.260]   which is to say more competent,
[02:13:20.260 --> 02:13:21.180]   unless you've,
[02:13:21.180 --> 02:13:25.080]   and obviously there are cartoon pictures of this
[02:13:25.080 --> 02:13:26.280]   where we could just,
[02:13:26.280 --> 02:13:27.320]   this is just an off switch,
[02:13:27.320 --> 02:13:28.480]   and we could just turn off the off switch,
[02:13:28.480 --> 02:13:29.620]   or they're tethered to something
[02:13:29.620 --> 02:13:31.620]   that makes them,
[02:13:31.620 --> 02:13:33.700]   our slaves in perpetuity,
[02:13:33.700 --> 02:13:34.860]   even though they're more intelligent.
[02:13:34.860 --> 02:13:38.980]   But those scenarios strike me as a failure
[02:13:38.980 --> 02:13:40.980]   to imagine what is actually entailed
[02:13:40.980 --> 02:13:42.340]   by greater intelligence.
[02:13:42.340 --> 02:13:43.500]   So if you imagine something
[02:13:43.500 --> 02:13:46.540]   that's legitimately more intelligent than you are,
[02:13:46.540 --> 02:13:49.820]   and you're now in relationship to it,
[02:13:49.820 --> 02:13:52.620]   you're in the presence of this thing,
[02:13:52.620 --> 02:13:54.780]   and it is autonomous in all kinds of ways
[02:13:54.780 --> 02:13:57.100]   because it had to be to be more intelligent than you are.
[02:13:57.100 --> 02:14:00.400]   I mean, you built it to be all of those things.
[02:14:00.400 --> 02:14:05.640]   We just can't find ourselves in a negotiation
[02:14:05.640 --> 02:14:08.020]   with something more intelligent than we are.
[02:14:08.020 --> 02:14:08.940]   We can't,
[02:14:08.940 --> 02:14:13.020]   so we have to have found the subset of ways
[02:14:13.020 --> 02:14:15.200]   to build these machines
[02:14:15.200 --> 02:14:21.140]   that are perpetually amenable to our saying,
[02:14:21.140 --> 02:14:24.020]   "Oh, that's not what we meant.
[02:14:24.020 --> 02:14:24.940]   "That's not what we intended.
[02:14:24.940 --> 02:14:26.140]   "Could you stop doing that?
[02:14:26.140 --> 02:14:27.080]   "Come back over here
[02:14:27.080 --> 02:14:29.440]   "and do this thing that we actually want."
[02:14:29.440 --> 02:14:30.620]   And for them to care,
[02:14:30.620 --> 02:14:32.580]   for them to be tethered to our own sense
[02:14:32.580 --> 02:14:33.960]   of our own wellbeing,
[02:14:33.960 --> 02:14:37.060]   such that,
[02:14:37.060 --> 02:14:39.540]   I mean, their utility function is,
[02:14:39.540 --> 02:14:42.300]   their primary utility function is to have,
[02:14:42.300 --> 02:14:45.300]   this is, I think, Stuart Russell's cartoon plan
[02:14:45.300 --> 02:14:53.940]   is to figure out how to tether them to a utility function
[02:14:53.940 --> 02:14:57.900]   that has our own estimation
[02:14:57.900 --> 02:15:00.460]   of what's going to improve our wellbeing
[02:15:00.460 --> 02:15:05.460]   as its master reward, right?
[02:15:05.500 --> 02:15:07.660]   So it's like all this thing can get
[02:15:07.660 --> 02:15:10.020]   as intelligent as it can get,
[02:15:10.020 --> 02:15:12.980]   but it only ever really wants to figure out
[02:15:12.980 --> 02:15:16.020]   how to make our lives better by our own view of better.
[02:15:16.020 --> 02:15:19.220]   Now, not to say there wouldn't be a conversation about,
[02:15:19.220 --> 02:15:21.020]   you know, I mean, because all kinds of things
[02:15:21.020 --> 02:15:24.300]   we're not seeing clearly about what is better.
[02:15:24.300 --> 02:15:27.580]   And if we were in the presence of a genie or an oracle
[02:15:27.580 --> 02:15:29.180]   that could really tell us what is better,
[02:15:29.180 --> 02:15:32.060]   well, then we presumably would want to hear that
[02:15:32.060 --> 02:15:37.060]   and we would modify our sense of what to do next
[02:15:37.060 --> 02:15:40.980]   in conversation with these minds.
[02:15:40.980 --> 02:15:45.180]   But I just feel like it is a failure of imagination
[02:15:45.180 --> 02:15:53.020]   to think that being in relationship
[02:15:53.020 --> 02:15:57.300]   to something more intelligent than yourself
[02:15:57.300 --> 02:15:59.580]   isn't in most cases
[02:16:00.580 --> 02:16:03.180]   a circumstance of real peril.
[02:16:03.180 --> 02:16:08.580]   'Cause it is, just to think of how everything on earth
[02:16:08.580 --> 02:16:12.140]   has to, if they could think about their relationship to us,
[02:16:12.140 --> 02:16:15.800]   if birds could think about what we're doing, right?
[02:16:15.800 --> 02:16:23.820]   The bottom line is they're always in danger
[02:16:23.820 --> 02:16:26.700]   of our discovering that there's something
[02:16:26.700 --> 02:16:29.420]   we care about more than birds, right?
[02:16:29.420 --> 02:16:30.900]   Or there's something we want
[02:16:30.900 --> 02:16:34.220]   that disregards the wellbeing of birds.
[02:16:34.220 --> 02:16:37.700]   And obviously much of our behavior is inscrutable to them.
[02:16:37.700 --> 02:16:39.060]   Occasionally we pay attention to them
[02:16:39.060 --> 02:16:41.620]   and occasionally we withdraw our attention
[02:16:41.620 --> 02:16:43.060]   and occasionally we just kill them all
[02:16:43.060 --> 02:16:45.500]   for reasons they can't possibly understand.
[02:16:45.500 --> 02:16:48.320]   But if we're building something more intelligent
[02:16:48.320 --> 02:16:51.660]   than ourselves, by definition, we're building something
[02:16:51.660 --> 02:16:56.660]   whose horizons of value and cognition
[02:16:57.580 --> 02:17:00.060]   can exceed our own.
[02:17:00.060 --> 02:17:05.060]   And in ways where we can't necessarily foresee,
[02:17:05.060 --> 02:17:09.260]   again, perpetually, that they don't just wake up one day
[02:17:09.260 --> 02:17:14.180]   and decide, okay, well, these humans need to disappear.
[02:17:14.180 --> 02:17:18.420]   - So I think I agree with most of the initial things
[02:17:18.420 --> 02:17:19.740]   you said.
[02:17:19.740 --> 02:17:22.500]   What I don't necessarily agree with,
[02:17:22.500 --> 02:17:24.020]   and of course nobody knows,
[02:17:24.020 --> 02:17:27.520]   but that the more likely set of trajectories
[02:17:27.520 --> 02:17:30.340]   that we're going to take are going to be positive.
[02:17:30.340 --> 02:17:32.020]   That's what I believe.
[02:17:32.020 --> 02:17:35.840]   In the sense that the way you develop,
[02:17:35.840 --> 02:17:40.300]   I believe the way you develop successful AI systems
[02:17:40.300 --> 02:17:43.740]   will be deeply integrated with human society.
[02:17:43.740 --> 02:17:48.060]   And for them to succeed, they're going to have to be aligned
[02:17:48.060 --> 02:17:50.340]   in the way we humans are aligned with each other,
[02:17:50.340 --> 02:17:52.540]   which doesn't mean we're aligned,
[02:17:52.540 --> 02:17:54.700]   that there's no such thing,
[02:17:54.700 --> 02:17:57.860]   or I don't see there's such thing as a perfect alignment,
[02:17:57.860 --> 02:18:01.080]   but they're going to be participating in the dance,
[02:18:01.080 --> 02:18:04.540]   in the game-theoretic dance of human society
[02:18:04.540 --> 02:18:06.520]   as they become more and more intelligent.
[02:18:06.520 --> 02:18:09.220]   There could be a point beyond which
[02:18:09.220 --> 02:18:11.520]   we are like birds to them.
[02:18:11.520 --> 02:18:16.080]   - But what about an intelligence explosion of some kind?
[02:18:16.080 --> 02:18:21.080]   - So I believe the explosion will be happening,
[02:18:21.080 --> 02:18:24.120]   but there's a lot of explosion to be done
[02:18:24.120 --> 02:18:26.200]   before we become like birds.
[02:18:26.200 --> 02:18:28.140]   I truly believe that human beings
[02:18:28.140 --> 02:18:30.680]   are very intelligent in ways we don't understand.
[02:18:30.680 --> 02:18:32.280]   It's not just about chess.
[02:18:32.280 --> 02:18:35.360]   It's about all the intricate computation
[02:18:35.360 --> 02:18:37.640]   we're able to perform, common sense,
[02:18:37.640 --> 02:18:40.600]   our ability to reason about this world, consciousness.
[02:18:40.600 --> 02:18:42.400]   I think we're doing a lot of work
[02:18:42.400 --> 02:18:44.560]   we don't realize is necessary to be done
[02:18:44.560 --> 02:18:47.720]   in order to truly become,
[02:18:47.720 --> 02:18:49.980]   like truly achieve super intelligence.
[02:18:49.980 --> 02:18:52.140]   And I just think there'll be a period of time
[02:18:52.140 --> 02:18:53.720]   that's not overnight.
[02:18:53.720 --> 02:18:57.120]   The overnight nature of it will not literally be overnight.
[02:18:57.120 --> 02:18:59.520]   It'll be over a period of decades.
[02:18:59.520 --> 02:19:00.360]   So my sense is--
[02:19:00.360 --> 02:19:01.400]   - But why would it be that?
[02:19:01.400 --> 02:19:06.400]   But just take, draw an analogy from recent successes
[02:19:06.400 --> 02:19:09.400]   like something like AlphaGo or AlphaZero.
[02:19:09.400 --> 02:19:11.720]   I forget the actual metric,
[02:19:11.720 --> 02:19:14.800]   but it was something like this algorithm,
[02:19:14.800 --> 02:19:17.280]   which wasn't even totally,
[02:19:17.280 --> 02:19:19.420]   it wasn't bespoke for chess playing,
[02:19:21.440 --> 02:19:24.160]   in the matter of, I think it was four hours,
[02:19:24.160 --> 02:19:27.220]   played itself so many times and so successfully
[02:19:27.220 --> 02:19:30.480]   that it became the best chess playing computer.
[02:19:30.480 --> 02:19:31.320]   Not only was it,
[02:19:31.320 --> 02:19:33.720]   it was not only better than every human being,
[02:19:33.720 --> 02:19:36.820]   it was better than every previous chess program
[02:19:36.820 --> 02:19:38.640]   in a matter of a day, right?
[02:19:38.640 --> 02:19:40.560]   So like that, so just imagine,
[02:19:40.560 --> 02:19:43.740]   again, we don't have to recapitulate everything about us,
[02:19:43.740 --> 02:19:45.700]   but just imagine building a system,
[02:19:45.700 --> 02:19:50.480]   and who knows when we'll be able to do this,
[02:19:50.480 --> 02:19:52.080]   but at some point we'll be able,
[02:19:52.080 --> 02:19:54.800]   at some point the 100,
[02:19:54.800 --> 02:19:57.400]   or 100 favorite things about human cognition
[02:19:57.400 --> 02:20:01.240]   will be analogous to chess
[02:20:01.240 --> 02:20:03.940]   in that we will be able to build machines
[02:20:03.940 --> 02:20:07.980]   that very quickly outperform any human,
[02:20:07.980 --> 02:20:12.120]   and then very quickly outperform the last algorithm
[02:20:12.120 --> 02:20:13.520]   that outperform the humans.
[02:20:13.520 --> 02:20:17.400]   Like something like the AlphaGo experience
[02:20:17.400 --> 02:20:21.480]   seems possible for facial recognition,
[02:20:21.480 --> 02:20:23.680]   and detecting human emotion,
[02:20:23.680 --> 02:20:26.120]   and natural language processing, right?
[02:20:26.120 --> 02:20:28.840]   Like, well, it's just the,
[02:20:28.840 --> 02:20:33.020]   everyone, even math people, math heads,
[02:20:33.020 --> 02:20:36.480]   tend to have bad intuitions for exponentiation, right?
[02:20:36.480 --> 02:20:37.640]   We noticed this during COVID,
[02:20:37.640 --> 02:20:39.160]   I mean, you have some very smart people
[02:20:39.160 --> 02:20:43.120]   who still couldn't get their minds around the fact that,
[02:20:43.120 --> 02:20:46.720]   you know, an exponential is really surprising,
[02:20:46.720 --> 02:20:48.280]   I mean, things double, and double, and double,
[02:20:48.280 --> 02:20:49.400]   and double again,
[02:20:49.400 --> 02:20:51.160]   and you don't notice much of anything changes,
[02:20:51.160 --> 02:20:53.400]   and then the last, you know,
[02:20:53.400 --> 02:20:56.200]   two stages of doubling swamp everything, right?
[02:20:56.200 --> 02:20:59.600]   And it just seems like that,
[02:20:59.600 --> 02:21:04.600]   to assume that there isn't a deep analogy
[02:21:04.600 --> 02:21:08.320]   between what we're seeing for the more tractable,
[02:21:08.320 --> 02:21:11.400]   the tractable problems like chess
[02:21:11.400 --> 02:21:13.140]   to other modes of cognition.
[02:21:13.140 --> 02:21:16.040]   It's like once you crack that problem,
[02:21:16.040 --> 02:21:17.920]   it seems, 'cause for the longest time,
[02:21:17.920 --> 02:21:22.320]   it was impossible to think we were gonna make headway
[02:21:22.320 --> 02:21:25.080]   on, in AI, you know, it's like-
[02:21:25.080 --> 02:21:27.560]   - Chess and Go was, Go seemed impossible.
[02:21:27.560 --> 02:21:28.800]   - Go seemed unattainable.
[02:21:28.800 --> 02:21:31.640]   Even when chess had been cracked,
[02:21:31.640 --> 02:21:33.360]   Go seemed unattainable.
[02:21:33.360 --> 02:21:36.600]   - Yeah, and actually, Stuart Russell was behind
[02:21:36.600 --> 02:21:38.720]   the people that were saying it's unattainable,
[02:21:38.720 --> 02:21:40.480]   'cause it seemed like, you know,
[02:21:40.480 --> 02:21:42.840]   it's a intractable problem.
[02:21:42.840 --> 02:21:44.440]   But there's something different
[02:21:44.440 --> 02:21:46.560]   about the space of cognition
[02:21:46.560 --> 02:21:48.080]   that's detached from human society,
[02:21:48.080 --> 02:21:51.220]   which is what chess is, meaning like just thinking.
[02:21:51.220 --> 02:21:54.280]   Having actual exponential impact
[02:21:54.280 --> 02:21:56.560]   on the physical world is different.
[02:21:56.560 --> 02:21:58.300]   I tend to believe that there's,
[02:21:58.300 --> 02:22:03.700]   for AI to get to the point where it's super intelligent,
[02:22:03.700 --> 02:22:07.680]   it's going to have to go through the funnel of society.
[02:22:07.680 --> 02:22:09.580]   And for that, it has to be deeply integrated
[02:22:09.580 --> 02:22:11.120]   with human beings.
[02:22:11.120 --> 02:22:12.880]   And for that, it has to be aligned.
[02:22:12.880 --> 02:22:15.560]   - But you're talking about like actually hooking us up
[02:22:15.560 --> 02:22:16.800]   to like Neuralink, you know,
[02:22:16.800 --> 02:22:21.800]   we're going to be the brainstem to the robot overlords?
[02:22:21.800 --> 02:22:23.360]   - That's a possibility as well.
[02:22:23.360 --> 02:22:26.200]   But what I mean is, in order to develop
[02:22:26.200 --> 02:22:28.840]   autonomous weapon systems, for example,
[02:22:28.840 --> 02:22:31.120]   which are highly concerning to me,
[02:22:31.120 --> 02:22:34.760]   that both US and China are participating in now,
[02:22:34.760 --> 02:22:38.320]   that in order to develop them and for them to become,
[02:22:38.320 --> 02:22:40.120]   to have more and more responsibility
[02:22:40.120 --> 02:22:44.480]   to actually do military strategic actions,
[02:22:44.480 --> 02:22:47.920]   they're going to have to be integrated
[02:22:47.920 --> 02:22:51.720]   into human beings doing the strategic action.
[02:22:51.720 --> 02:22:54.120]   They're going to have to work alongside with each other.
[02:22:54.120 --> 02:22:56.440]   And the way those systems will be developed
[02:22:56.440 --> 02:23:00.080]   will have the natural safety, like switches
[02:23:00.080 --> 02:23:03.040]   that are placed on them as they develop over time,
[02:23:03.040 --> 02:23:05.320]   because they're going to have to convince humans.
[02:23:05.320 --> 02:23:07.760]   Ultimately, they're going to have to convince humans
[02:23:07.760 --> 02:23:10.440]   that this is safer than humans.
[02:23:10.440 --> 02:23:12.360]   They're going to, you know.
[02:23:12.360 --> 02:23:15.680]   - Well, self-driving cars is a good test case here.
[02:23:15.680 --> 02:23:19.560]   'Cause like, obviously we've made a lot of progress
[02:23:19.560 --> 02:23:24.560]   and we can imagine what total progress would look like.
[02:23:24.560 --> 02:23:25.840]   I mean, it would be amazing.
[02:23:25.840 --> 02:23:29.000]   And it's answering, it's canceling in the US,
[02:23:29.000 --> 02:23:33.080]   40,000 deaths every year based on ape-driven cars, right?
[02:23:33.080 --> 02:23:35.600]   So it's an excruciating problem
[02:23:35.600 --> 02:23:36.600]   that we've all gotten used to
[02:23:36.600 --> 02:23:38.360]   'cause there was no alternative.
[02:23:38.360 --> 02:23:41.600]   But now we can dimly see the prospect of an alternative,
[02:23:41.600 --> 02:23:45.720]   which if it works in a super intelligent fashion,
[02:23:45.720 --> 02:23:48.920]   maybe we go down to zero highway deaths, right?
[02:23:48.920 --> 02:23:51.920]   Or certainly we'd go down by orders of magnitude, right?
[02:23:51.920 --> 02:23:56.920]   So maybe we have 400 rather than 40,000 a year.
[02:23:56.920 --> 02:24:04.680]   And it's easy to see that there's not a missile.
[02:24:05.360 --> 02:24:08.000]   So obviously this is not an example of super intelligence.
[02:24:08.000 --> 02:24:09.200]   This is narrow intelligence,
[02:24:09.200 --> 02:24:14.200]   but the alignment problem isn't so obvious there,
[02:24:14.200 --> 02:24:17.640]   but there are potential alignment problems there.
[02:24:17.640 --> 02:24:22.520]   Like, so like just imagine if some woke team of engineers
[02:24:22.520 --> 02:24:26.680]   decided that we have to tune the algorithm some way.
[02:24:26.680 --> 02:24:28.080]   I mean, there are situations
[02:24:28.080 --> 02:24:30.080]   where the car has to decide who to hit.
[02:24:30.080 --> 02:24:31.520]   I mean, there's just bad outcomes
[02:24:31.520 --> 02:24:33.960]   where you're going to hit somebody, right?
[02:24:33.960 --> 02:24:36.720]   Now we have a car that can tell what race you are, right?
[02:24:36.720 --> 02:24:38.120]   So we're going to build the car
[02:24:38.120 --> 02:24:40.360]   to preferentially hit white people
[02:24:40.360 --> 02:24:42.060]   because white people have had so much privilege
[02:24:42.060 --> 02:24:43.240]   over the years.
[02:24:43.240 --> 02:24:44.880]   This seems like the only ethical way
[02:24:44.880 --> 02:24:47.240]   to kind of redress those wrongs of the past.
[02:24:47.240 --> 02:24:48.760]   That's something that could get built.
[02:24:48.760 --> 02:24:52.480]   One, that could get produced as an artifact,
[02:24:52.480 --> 02:24:54.200]   presumably, of just how you built it.
[02:24:54.200 --> 02:24:56.240]   And you didn't even know you engineered it that way, right?
[02:24:56.240 --> 02:24:58.640]   You caused it. - Or machine learning.
[02:24:58.640 --> 02:24:59.880]   - Yeah. - You put some kind
[02:24:59.880 --> 02:25:01.400]   of constraints on it to where it creates
[02:25:01.400 --> 02:25:02.240]   those kinds of outcomes.
[02:25:02.240 --> 02:25:05.120]   So you basically built a racist algorithm
[02:25:05.120 --> 02:25:07.880]   and you didn't even intend to, or you could intend to, right?
[02:25:07.880 --> 02:25:09.700]   And it would be aligned with some people's values,
[02:25:09.700 --> 02:25:11.800]   but misaligned with other people's values.
[02:25:11.800 --> 02:25:16.480]   But it's like, there are interesting problems
[02:25:16.480 --> 02:25:17.920]   even with something as simple
[02:25:17.920 --> 02:25:20.520]   and obviously good as self-driving cars.
[02:25:20.520 --> 02:25:23.600]   - But there's a leap that I just think it'd be exact,
[02:25:23.600 --> 02:25:25.320]   but those are human problems.
[02:25:25.320 --> 02:25:26.960]   I just don't think there'll be a leap
[02:25:26.960 --> 02:25:28.580]   with autonomous vehicles.
[02:25:28.580 --> 02:25:31.560]   First of all, sorry.
[02:25:31.560 --> 02:25:33.920]   There are a lot of trajectories
[02:25:33.920 --> 02:25:35.760]   which will destroy human civilization.
[02:25:35.760 --> 02:25:38.140]   The argument I'm making, it's more likely
[02:25:38.140 --> 02:25:40.040]   that we'll take trajectories that don't.
[02:25:40.040 --> 02:25:41.880]   So I don't think there'll be a leap
[02:25:41.880 --> 02:25:43.000]   with autonomous vehicles.
[02:25:43.000 --> 02:25:45.960]   We'll all of a sudden start murdering pedestrians
[02:25:45.960 --> 02:25:49.280]   because once every human on earth is dead,
[02:25:49.280 --> 02:25:50.680]   there'll be no more fatalities.
[02:25:50.680 --> 02:25:52.800]   Sort of unintended consequences of,
[02:25:52.800 --> 02:25:55.480]   and it's difficult to take that leap.
[02:25:55.480 --> 02:25:57.160]   Most systems as we develop
[02:25:57.160 --> 02:25:59.120]   and they become much, much more intelligent
[02:25:59.120 --> 02:26:01.280]   in ways that will be incredibly surprising,
[02:26:01.280 --> 02:26:04.480]   like stuff that DeepMind is doing with protein folding.
[02:26:04.480 --> 02:26:07.840]   Even, which is scary to think about,
[02:26:07.840 --> 02:26:09.520]   and I'm personally terrified about this,
[02:26:09.520 --> 02:26:12.760]   which is the engineering of viruses using machine learning.
[02:26:12.760 --> 02:26:16.880]   The engineering of vaccines using machine learning.
[02:26:16.880 --> 02:26:20.840]   The engineering of, yeah, for research purposes,
[02:26:20.840 --> 02:26:23.680]   pathogens using machine learning.
[02:26:23.680 --> 02:26:25.080]   And the ways that could go wrong.
[02:26:25.080 --> 02:26:27.400]   I just think that there's always going to be
[02:26:27.400 --> 02:26:30.820]   a closed loop supervision of humans
[02:26:30.820 --> 02:26:33.560]   before the AI becomes super intelligent.
[02:26:33.560 --> 02:26:38.240]   Not always, much more likely to be supervision.
[02:26:38.240 --> 02:26:40.520]   Except, of course, the question is
[02:26:40.520 --> 02:26:42.120]   how many dumb people there are in the world,
[02:26:42.120 --> 02:26:44.420]   how many evil people are in the world?
[02:26:44.420 --> 02:26:48.800]   My theory, my hope is, my sense is,
[02:26:48.800 --> 02:26:50.640]   that the number of intelligent people
[02:26:50.640 --> 02:26:52.980]   is much higher than the number of dumb people,
[02:26:52.980 --> 02:26:55.320]   that know how to program,
[02:26:55.320 --> 02:26:57.680]   and the number of evil people.
[02:26:57.680 --> 02:26:59.920]   I think smart people and kind people
[02:26:59.920 --> 02:27:03.260]   over outnumber the others.
[02:27:03.260 --> 02:27:06.340]   - Except we also, we have to add another group of people
[02:27:06.340 --> 02:27:09.880]   which are just the smart and otherwise good,
[02:27:09.880 --> 02:27:12.180]   but reckless people.
[02:27:12.180 --> 02:27:15.440]   The people who will flip a switch on,
[02:27:15.440 --> 02:27:17.640]   not knowing what's going to happen,
[02:27:17.640 --> 02:27:19.040]   they're just kind of hoping
[02:27:19.040 --> 02:27:20.440]   that it's not going to blow up the world.
[02:27:20.440 --> 02:27:23.320]   We already know that some of our smartest people
[02:27:23.320 --> 02:27:24.520]   are those sorts of people.
[02:27:24.520 --> 02:27:26.140]   You know, we know we've done experiments,
[02:27:26.140 --> 02:27:27.700]   and this is something that Martin Rees
[02:27:27.700 --> 02:27:32.700]   was whinging about before the Large Hadron Collider
[02:27:32.700 --> 02:27:34.720]   got booted up, I think.
[02:27:34.720 --> 02:27:38.720]   We know there are people who are entertaining experiments,
[02:27:38.720 --> 02:27:40.020]   or even performing experiments,
[02:27:40.020 --> 02:27:42.880]   where there's some chance, you know,
[02:27:42.880 --> 02:27:44.880]   not quite infinitesimal,
[02:27:44.880 --> 02:27:48.840]   that they're going to create a black hole in the lab
[02:27:48.840 --> 02:27:50.600]   and suck the whole world into it.
[02:27:50.600 --> 02:27:52.280]   Right, I mean, like, that's not,
[02:27:52.280 --> 02:27:55.320]   you're not a crazy person to worry that,
[02:27:55.320 --> 02:27:57.200]   or worry about that based on the physics.
[02:27:57.200 --> 02:28:01.560]   And so it was with, you know, the Trinity test,
[02:28:01.560 --> 02:28:04.000]   there were some people who were still
[02:28:04.000 --> 02:28:06.640]   checking their calculations, and they were off.
[02:28:06.640 --> 02:28:10.320]   We did nuclear tests where we were off significantly
[02:28:10.320 --> 02:28:11.940]   in terms of the yield, right?
[02:28:11.940 --> 02:28:12.780]   So it was like-
[02:28:12.780 --> 02:28:13.760]   - And they still flipped the switch.
[02:28:13.760 --> 02:28:14.960]   - Yeah, they still flipped the switch.
[02:28:14.960 --> 02:28:19.960]   And sometimes they flip the switch not to win a world war,
[02:28:19.960 --> 02:28:22.900]   or to save 40,000 lives a year.
[02:28:22.900 --> 02:28:24.640]   They just- - Just to see what happens.
[02:28:24.640 --> 02:28:25.880]   - Intellectual curiosity.
[02:28:25.880 --> 02:28:27.760]   Like, this is what I got my grant for.
[02:28:27.760 --> 02:28:30.120]   This is where I'll get my Nobel Prize,
[02:28:30.120 --> 02:28:32.060]   if that's in the cards.
[02:28:32.060 --> 02:28:35.560]   It's on the other side of this switch, right?
[02:28:35.560 --> 02:28:40.560]   And, I mean, again, we are apes with egos
[02:28:40.560 --> 02:28:47.840]   who are massively constrained by self,
[02:28:47.840 --> 02:28:49.640]   very short-term self-interest,
[02:28:49.640 --> 02:28:52.860]   even when we're contemplating some of the deepest
[02:28:52.860 --> 02:28:57.700]   and most interesting and most universal problems
[02:28:57.700 --> 02:29:00.340]   we could ever set our attention towards.
[02:29:00.340 --> 02:29:03.100]   Like, just if you read James Watson's book,
[02:29:03.100 --> 02:29:05.580]   "The Double Helix," right, about them, you know,
[02:29:05.580 --> 02:29:08.620]   cracking the structure of DNA,
[02:29:08.620 --> 02:29:11.600]   one thing that's amazing about that book
[02:29:11.600 --> 02:29:15.980]   is just how much of it, almost all of it,
[02:29:15.980 --> 02:29:20.980]   is being driven by very ape-ish,
[02:29:21.980 --> 02:29:26.140]   egocentric, social concerns.
[02:29:26.140 --> 02:29:28.940]   Like, the algorithm that is producing
[02:29:28.940 --> 02:29:32.580]   this scientific breakthrough is human competition,
[02:29:32.580 --> 02:29:34.140]   if you're James Watson, right?
[02:29:34.140 --> 02:29:36.500]   It's like, I'm gonna get there before Linus Pauling,
[02:29:36.500 --> 02:29:39.260]   and, you know, it's just,
[02:29:39.260 --> 02:29:43.180]   so much of his bandwidth is captured by that, right?
[02:29:43.180 --> 02:29:48.180]   Now, that becomes more and more of a liability
[02:29:48.820 --> 02:29:51.060]   when you're talking about producing technology
[02:29:51.060 --> 02:29:53.780]   that can change everything in an instant, you know?
[02:29:53.780 --> 02:29:57.780]   We're talking about not only understanding,
[02:29:57.780 --> 02:30:01.780]   you know, we're just at a different moment in human history.
[02:30:01.780 --> 02:30:06.780]   We're not, when we're doing research on viruses,
[02:30:06.780 --> 02:30:10.980]   we're now doing the kind of research
[02:30:10.980 --> 02:30:13.900]   that can cause someone somewhere else
[02:30:13.900 --> 02:30:16.460]   to be able to make that virus,
[02:30:16.460 --> 02:30:21.460]   or weaponize that virus, or it's just, I don't know.
[02:30:21.460 --> 02:30:26.180]   I mean, our power is, our wisdom is,
[02:30:26.180 --> 02:30:27.660]   it does not seem like our wisdom
[02:30:27.660 --> 02:30:29.420]   is scaling with our power, right?
[02:30:29.420 --> 02:30:31.620]   And that seems like, insofar,
[02:30:31.620 --> 02:30:36.700]   as wisdom and power become unaligned,
[02:30:36.700 --> 02:30:39.340]   I get more and more concerned.
[02:30:39.340 --> 02:30:43.020]   - But speaking of apes with egos,
[02:30:44.100 --> 02:30:47.580]   some of the most compelling apes,
[02:30:47.580 --> 02:30:49.220]   two compelling apes I can think of
[02:30:49.220 --> 02:30:51.700]   is yourself and Jordan Peterson,
[02:30:51.700 --> 02:30:56.020]   and you've had a fun conversation about religion
[02:30:56.020 --> 02:30:58.700]   that I watched most of, I believe.
[02:30:58.700 --> 02:31:00.420]   I'm not sure there was any--
[02:31:00.420 --> 02:31:03.820]   - We didn't solve anything.
[02:31:03.820 --> 02:31:05.060]   - If anything was ever solved.
[02:31:05.060 --> 02:31:09.420]   So is there something, like a charitable summary
[02:31:09.420 --> 02:31:12.980]   you can give to the ideas that you agree on
[02:31:12.980 --> 02:31:14.340]   and disagree with Jordan?
[02:31:14.340 --> 02:31:16.460]   Is there something maybe after that conversation
[02:31:16.460 --> 02:31:19.220]   that you've landed where maybe,
[02:31:19.220 --> 02:31:22.660]   as you both agreed on,
[02:31:22.660 --> 02:31:24.700]   is there some wisdom in the rubble,
[02:31:24.700 --> 02:31:29.060]   of even imperfect, flawed ideas?
[02:31:29.060 --> 02:31:31.340]   Is there something that you can kind of pull out
[02:31:31.340 --> 02:31:34.620]   from those conversations, or is it to be continued?
[02:31:34.620 --> 02:31:35.980]   - I mean, I think where we disagree,
[02:31:35.980 --> 02:31:40.980]   so he thinks that many of our traditional religions
[02:31:40.980 --> 02:31:43.940]   and many of our traditional religious beliefs
[02:31:43.940 --> 02:31:48.940]   and frameworks are holding so much,
[02:31:48.940 --> 02:31:53.780]   such a repository of human wisdom
[02:31:53.780 --> 02:32:00.420]   that we pull at that fabric at our peril, right?
[02:32:00.420 --> 02:32:07.260]   Like if you start just unraveling Christianity
[02:32:07.260 --> 02:32:11.860]   or any other traditional set of norms and beliefs,
[02:32:11.860 --> 02:32:15.900]   you may think you're just pulling out the unscientific bits,
[02:32:15.900 --> 02:32:17.780]   but you could be pulling a lot more
[02:32:17.780 --> 02:32:21.020]   to which everything you care about is attached,
[02:32:21.020 --> 02:32:22.460]   right, as a society.
[02:32:22.460 --> 02:32:28.460]   And my feeling is that there's so much,
[02:32:28.460 --> 02:32:31.380]   there's so much downside to the unscientific bits,
[02:32:31.380 --> 02:32:36.380]   and it's so clear how we could have a 21st century
[02:32:36.620 --> 02:32:39.900]   rational conversation about the good stuff
[02:32:39.900 --> 02:32:42.740]   that we really can radically edit these traditions.
[02:32:42.740 --> 02:32:47.620]   And we can take Jesus in half his moods
[02:32:47.620 --> 02:32:51.040]   and just find a great inspirational,
[02:32:51.040 --> 02:32:54.860]   Iron Age thought leader,
[02:32:54.860 --> 02:32:56.240]   who just happened to get crucified,
[02:32:56.240 --> 02:32:58.900]   but he could be someone, like the Beatitudes
[02:32:58.900 --> 02:33:03.620]   and the Golden Rule, which doesn't originate with him,
[02:33:03.620 --> 02:33:06.780]   but which he put quite beautifully.
[02:33:06.780 --> 02:33:09.740]   All of that's incredibly useful.
[02:33:09.740 --> 02:33:12.780]   It's no less useful than it was 2000 years ago,
[02:33:12.780 --> 02:33:14.940]   but we don't have to believe he was born of a virgin
[02:33:14.940 --> 02:33:18.300]   or coming back to raise the dead or any of that other stuff.
[02:33:18.300 --> 02:33:21.400]   And we can be honest about not believing those things,
[02:33:21.400 --> 02:33:22.960]   and we can be honest about the reasons
[02:33:22.960 --> 02:33:24.620]   why we don't believe those things.
[02:33:24.620 --> 02:33:29.620]   'Cause on those fronts, I view the downside to be so obvious
[02:33:31.080 --> 02:33:34.800]   and the fact that we have so many different
[02:33:34.800 --> 02:33:38.520]   competing dogmatisms on offer to be so non-functional,
[02:33:38.520 --> 02:33:40.080]   I mean, it's so divisive.
[02:33:40.080 --> 02:33:43.880]   It just has conflict built into it
[02:33:43.880 --> 02:33:47.120]   that I think we can be far more
[02:33:47.120 --> 02:33:51.600]   and should be far more iconoclastic than he wants to be.
[02:33:51.600 --> 02:33:56.040]   Now, none of this is to deny much of what he argues for,
[02:33:56.040 --> 02:34:00.500]   that stories are very powerful.
[02:34:00.500 --> 02:34:03.400]   Clearly, stories are powerful, and we want good stories.
[02:34:03.400 --> 02:34:06.160]   We want our lives, we want to have a conversation
[02:34:06.160 --> 02:34:10.080]   with ourselves and with one another about our lives
[02:34:10.080 --> 02:34:13.440]   that facilitates the best possible lives,
[02:34:13.440 --> 02:34:15.480]   and story is part of that.
[02:34:15.480 --> 02:34:20.480]   And if you want some of those stories to sound like myths,
[02:34:20.480 --> 02:34:22.660]   that might be part of it.
[02:34:22.660 --> 02:34:26.200]   But my argument is that we never really need
[02:34:26.200 --> 02:34:29.200]   to deceive ourselves or our children
[02:34:29.200 --> 02:34:32.500]   about what we have every reason to believe is true
[02:34:32.500 --> 02:34:33.960]   in order to get at the good stuff,
[02:34:33.960 --> 02:34:36.300]   in order to organize our lives well.
[02:34:36.300 --> 02:34:39.200]   I certainly don't feel that I need to do it personally,
[02:34:39.200 --> 02:34:41.080]   and if I don't need to do it personally,
[02:34:41.080 --> 02:34:43.360]   why would I think that billions of other people
[02:34:43.360 --> 02:34:45.480]   need to do it personally?
[02:34:45.480 --> 02:34:48.640]   Now, there is a cynical counter-argument,
[02:34:48.640 --> 02:34:51.220]   which is billions of other people
[02:34:51.220 --> 02:34:54.240]   don't have the advantages that I have had in my life.
[02:34:54.240 --> 02:34:57.640]   The billions of other people are not as well-educated,
[02:34:57.640 --> 02:34:59.280]   they haven't had the same opportunities,
[02:34:59.280 --> 02:35:04.280]   they need to be told that Jesus is gonna solve
[02:35:04.280 --> 02:35:06.600]   all their problems after they die, say,
[02:35:06.600 --> 02:35:10.700]   or that everything happens for a reason,
[02:35:10.700 --> 02:35:14.000]   and if you just believe in the secret,
[02:35:14.000 --> 02:35:16.200]   if you just visualize what you want, you're gonna get it.
[02:35:16.200 --> 02:35:20.900]   And it's like there's some measure
[02:35:20.900 --> 02:35:23.880]   of what I consider to be odious pamblum
[02:35:23.880 --> 02:35:27.320]   that really is food for the better part of humanity,
[02:35:27.320 --> 02:35:29.200]   and there is no substitute for it,
[02:35:29.200 --> 02:35:31.000]   or there's no substitute now.
[02:35:31.000 --> 02:35:32.480]   And I don't know if Jordan would agree with that,
[02:35:32.480 --> 02:35:35.200]   but much of what he says seems to suggest
[02:35:35.200 --> 02:35:36.600]   that he would agree with it.
[02:35:36.600 --> 02:35:41.120]   And I guess that's an empirical question.
[02:35:41.120 --> 02:35:43.320]   I mean, that's just that we don't know
[02:35:43.320 --> 02:35:47.040]   whether given a different set of norms
[02:35:47.040 --> 02:35:48.600]   and a different set of stories,
[02:35:48.600 --> 02:35:52.860]   people would behave the way I would hope they would behave
[02:35:52.860 --> 02:35:56.080]   and be aligned, more aligned than they are now.
[02:35:56.080 --> 02:35:58.800]   I think we know what happens
[02:35:58.800 --> 02:36:03.800]   when you just let ancient religious certainties
[02:36:03.800 --> 02:36:06.440]   go uncriticized.
[02:36:06.440 --> 02:36:07.840]   We know what that world's like.
[02:36:07.840 --> 02:36:10.640]   We've been struggling to get out of that world
[02:36:10.640 --> 02:36:12.140]   for a couple of hundred years,
[02:36:12.140 --> 02:36:15.880]   but we know what having Europe
[02:36:15.880 --> 02:36:21.860]   riven by religious wars looks like, right?
[02:36:21.860 --> 02:36:25.280]   And we know what happens when those religions
[02:36:25.280 --> 02:36:29.760]   become kind of pseudo religions and political religions.
[02:36:29.760 --> 02:36:33.840]   So this is where I'm sure Jordan and I would debate.
[02:36:33.840 --> 02:36:37.120]   He would say that Stalin was a symptom of atheism,
[02:36:37.120 --> 02:36:37.960]   and that's not at all.
[02:36:37.960 --> 02:36:39.840]   I mean, it's not my kind of atheism, right?
[02:36:39.840 --> 02:36:44.040]   Like Stalin, the problem with the Gulag
[02:36:44.040 --> 02:36:48.240]   and the experiment with communism or with Stalinism
[02:36:48.240 --> 02:36:53.240]   or with Nazism was not that there was so much
[02:36:53.240 --> 02:36:56.800]   scientific rigor and self-criticism and honesty
[02:36:56.800 --> 02:37:01.800]   and introspection and judicious use of psychedelics.
[02:37:01.800 --> 02:37:07.360]   I mean, like that was not the problem in Hitler's Germany
[02:37:07.360 --> 02:37:10.240]   or in Stalin's Soviet Union.
[02:37:10.240 --> 02:37:16.840]   The problem was you have other ideas
[02:37:16.840 --> 02:37:21.840]   that capture a similar kind of mob-based dogmatic energy
[02:37:22.840 --> 02:37:27.760]   and yes, the results of all of that
[02:37:27.760 --> 02:37:30.560]   are predictably murderous.
[02:37:30.560 --> 02:37:33.520]   - Well, the question is what is the source
[02:37:33.520 --> 02:37:37.780]   of the most viral and sticky stories
[02:37:37.780 --> 02:37:40.280]   that ultimately lead to a positive outcome?
[02:37:40.280 --> 02:37:44.440]   So communism was, I mean, having grown up in the Soviet Union
[02:37:44.440 --> 02:37:50.400]   even still having relatives in Russia,
[02:37:51.480 --> 02:37:53.920]   there's a stickiness to the nationalism
[02:37:53.920 --> 02:37:58.640]   and to the ideologies of communism that religious or not,
[02:37:58.640 --> 02:38:00.320]   you could say it's religious fervor.
[02:38:00.320 --> 02:38:05.320]   I could just say it's stories that are viral and sticky.
[02:38:05.320 --> 02:38:08.960]   I'm using the most horrible words,
[02:38:08.960 --> 02:38:12.360]   but the question is whether science and reason
[02:38:12.360 --> 02:38:14.440]   can generate viral sticky stories
[02:38:14.440 --> 02:38:16.360]   that give meaning to people's lives.
[02:38:16.360 --> 02:38:19.720]   In your sense, is it does?
[02:38:20.760 --> 02:38:23.000]   - Well, whatever's true ultimately
[02:38:23.000 --> 02:38:26.480]   should be captivating, right?
[02:38:26.480 --> 02:38:28.960]   It's like what's more captivating
[02:38:28.960 --> 02:38:32.080]   than whatever is real, right?
[02:38:32.080 --> 02:38:36.400]   Now it's because reality is, again,
[02:38:36.400 --> 02:38:41.040]   we're just climbing out of the darkness
[02:38:41.040 --> 02:38:43.040]   in terms of our understanding of what the hell is going on.
[02:38:43.040 --> 02:38:47.360]   And there's no telling what spooky things
[02:38:47.360 --> 02:38:48.280]   may in fact be true.
[02:38:48.280 --> 02:38:49.960]   I mean, I don't know if you've been on the receiving end
[02:38:49.960 --> 02:38:54.960]   of recent rumors about our conversation about UFOs
[02:38:54.960 --> 02:38:57.640]   very likely changing in the near term, right?
[02:38:57.640 --> 02:39:00.120]   But like there was just a Washington Post article
[02:39:00.120 --> 02:39:01.320]   and a New York article,
[02:39:01.320 --> 02:39:04.440]   and I've received some private outreach
[02:39:04.440 --> 02:39:08.160]   and perhaps you have, I know other people in our orbit
[02:39:08.160 --> 02:39:12.400]   have people who are claiming that the government
[02:39:12.400 --> 02:39:14.680]   has known much more about UFOs
[02:39:14.680 --> 02:39:17.560]   than they have let on until now.
[02:39:17.560 --> 02:39:19.320]   And this conversation is actually,
[02:39:19.320 --> 02:39:21.640]   is about to become more prominent,
[02:39:21.640 --> 02:39:25.360]   and it's not gonna be whatever,
[02:39:25.360 --> 02:39:28.520]   whoever's left standing when the music stops,
[02:39:28.520 --> 02:39:33.520]   it's not going to be a comfortable position to be in
[02:39:33.520 --> 02:39:39.040]   as a super rigorous scientific skeptic
[02:39:39.040 --> 02:39:41.640]   who's been saying there's no there there
[02:39:41.640 --> 02:39:43.520]   for the last 75 years, right?
[02:39:45.920 --> 02:39:49.080]   The short version is it sounds like
[02:39:49.080 --> 02:39:52.320]   the Office of Naval Intelligence and the Pentagon
[02:39:52.320 --> 02:39:55.600]   are very likely to say to Congress at some point
[02:39:55.600 --> 02:39:58.920]   in the not too distant future that we have evidence
[02:39:58.920 --> 02:40:02.780]   that there is technology flying around here
[02:40:02.780 --> 02:40:07.780]   that seems like it can't possibly be of human origin, right?
[02:40:07.780 --> 02:40:10.040]   Now, I don't know what I'm gonna do
[02:40:10.040 --> 02:40:11.320]   with that kind of disclosure, right?
[02:40:11.320 --> 02:40:14.600]   Maybe it's just, it's gonna be nothing,
[02:40:14.600 --> 02:40:17.240]   no follow on conversation to really have,
[02:40:17.240 --> 02:40:21.800]   but that is such a powerfully strange circumstance
[02:40:21.800 --> 02:40:22.920]   to be in, right?
[02:40:22.920 --> 02:40:25.400]   I mean, it's just, what are we gonna do with that?
[02:40:25.400 --> 02:40:27.440]   If in fact that's what happens, right?
[02:40:27.440 --> 02:40:31.680]   If in fact the considered opinion,
[02:40:31.680 --> 02:40:34.960]   despite the embarrassment it causes them,
[02:40:34.960 --> 02:40:38.480]   of the US government, of all of our intelligence,
[02:40:38.480 --> 02:40:40.360]   all of the relevant intelligence services,
[02:40:40.360 --> 02:40:44.080]   is that this isn't a hoax,
[02:40:44.080 --> 02:40:46.960]   it's too, there's too much data to suggest that it's a hoax.
[02:40:46.960 --> 02:40:48.720]   We've got too much radar imagery,
[02:40:48.720 --> 02:40:51.120]   there's too much satellite data,
[02:40:51.120 --> 02:40:55.480]   whatever data they actually have, there's too much of it.
[02:40:55.480 --> 02:40:58.640]   All we can say now is something's going on
[02:40:58.640 --> 02:41:03.600]   and there's no way it's the Chinese or the Russians
[02:41:03.600 --> 02:41:06.060]   or anyone else's technology.
[02:41:06.060 --> 02:41:11.980]   That should arrest our attention, you know, collectively
[02:41:12.280 --> 02:41:15.680]   to a degree that nothing in our lifetime has.
[02:41:15.680 --> 02:41:19.680]   And now one worries that we're so jaded
[02:41:19.680 --> 02:41:26.080]   and confused and distracted that it's gonna,
[02:41:26.080 --> 02:41:30.400]   it'll get much less coverage than, you know,
[02:41:30.400 --> 02:41:34.660]   Obama's tan suit did, you know, a bunch of years ago.
[02:41:34.660 --> 02:41:38.440]   It's just, it's, who knows how we'll respond to that.
[02:41:38.440 --> 02:41:43.440]   But it's just to say that the need for us
[02:41:43.440 --> 02:41:49.640]   to tell ourselves an honest story about what's going on
[02:41:49.640 --> 02:41:51.960]   and what's likely to happen next
[02:41:51.960 --> 02:41:54.000]   is never gonna go away, right?
[02:41:54.000 --> 02:41:56.420]   And it's important, it's just,
[02:41:56.420 --> 02:41:59.100]   the division between me and every person
[02:41:59.100 --> 02:42:01.220]   who's defending traditional religion is,
[02:42:01.220 --> 02:42:07.480]   where is it that you wanna lie to yourself
[02:42:07.640 --> 02:42:09.040]   or lie to your kids?
[02:42:09.040 --> 02:42:11.360]   Like, where is honesty a liability?
[02:42:11.360 --> 02:42:14.440]   And for me, it, you know,
[02:42:14.440 --> 02:42:17.120]   I've yet to find the place where it is.
[02:42:17.120 --> 02:42:20.920]   And it's so obviously a strength
[02:42:20.920 --> 02:42:24.080]   in almost every other circumstance
[02:42:24.080 --> 02:42:28.040]   because it is the thing that allows you to course correct.
[02:42:28.040 --> 02:42:33.000]   It is the thing that allows you to hope at least
[02:42:33.000 --> 02:42:34.760]   that your beliefs, that your stories
[02:42:34.760 --> 02:42:37.240]   are in some kind of calibration
[02:42:37.240 --> 02:42:40.360]   with what's actually going on in the world.
[02:42:40.360 --> 02:42:42.760]   - Yeah, it is a little bit sad to imagine that
[02:42:42.760 --> 02:42:47.320]   if aliens en masse showed up to Earth,
[02:42:47.320 --> 02:42:50.500]   they would be too preoccupied with political bickering
[02:42:50.500 --> 02:42:53.160]   or to like these like fake news
[02:42:53.160 --> 02:42:56.160]   and all that kind of stuff to notice
[02:42:56.160 --> 02:42:59.520]   the very basic evidence of reality.
[02:42:59.520 --> 02:43:02.400]   I do have a glimmer of hope
[02:43:02.400 --> 02:43:04.560]   that there seems to be more and more hunger
[02:43:04.560 --> 02:43:06.400]   for authenticity.
[02:43:06.400 --> 02:43:08.480]   And I feel like that opens the door
[02:43:08.480 --> 02:43:14.040]   for a hunger for what is real.
[02:43:14.040 --> 02:43:15.680]   Like people don't want stories,
[02:43:15.680 --> 02:43:19.660]   they don't want like layers and layers of like fakeness.
[02:43:19.660 --> 02:43:24.760]   And I'm hoping that means that will directly lead
[02:43:24.760 --> 02:43:28.280]   to a greater hunger for reality and reason and truth.
[02:43:28.280 --> 02:43:31.480]   You know, truth isn't dogmatism.
[02:43:31.480 --> 02:43:34.200]   Like truth isn't authority.
[02:43:34.200 --> 02:43:37.040]   I have a PhD and therefore I'm right.
[02:43:37.040 --> 02:43:42.600]   Truth is almost like the reality is
[02:43:42.600 --> 02:43:44.480]   there's so many questions, there's so many mysteries,
[02:43:44.480 --> 02:43:45.480]   there's so much uncertainty.
[02:43:45.480 --> 02:43:49.400]   This is our best available, like a best guess.
[02:43:49.400 --> 02:43:52.100]   And we have a lot of evidence that supports that guess,
[02:43:52.100 --> 02:43:53.820]   but it could be so many other things.
[02:43:53.820 --> 02:43:56.400]   And like just even conveying that,
[02:43:56.400 --> 02:43:58.800]   I think there's a hunger for that in the world
[02:43:58.800 --> 02:44:01.440]   to hear that from scientists, less dogmatism
[02:44:01.440 --> 02:44:04.920]   and more just like, this is what we know.
[02:44:04.920 --> 02:44:07.060]   We're doing our best given the uncertainty,
[02:44:07.060 --> 02:44:09.520]   given, I mean, this is true with obviously
[02:44:09.520 --> 02:44:11.840]   with the virology and all those kinds of things
[02:44:11.840 --> 02:44:13.360]   'cause everything is happening so fast.
[02:44:13.360 --> 02:44:16.520]   There's a lot of, and biology is super messy.
[02:44:16.520 --> 02:44:18.840]   So it's very hard to know stuff for sure.
[02:44:18.840 --> 02:44:21.040]   So just being open and real about that,
[02:44:21.040 --> 02:44:25.680]   I think I'm hoping will change people's hunger
[02:44:25.680 --> 02:44:29.560]   and openness and trust of what's real.
[02:44:29.560 --> 02:44:31.880]   - Yeah, well, so much of this is probabilistic.
[02:44:31.880 --> 02:44:35.080]   It's so much of what can seem dogmatic scientifically
[02:44:35.080 --> 02:44:40.080]   is just, you're placing a bet on whether it's worth
[02:44:40.080 --> 02:44:45.520]   reading that paper or rethinking your presuppositions
[02:44:45.520 --> 02:44:46.720]   on that point.
[02:44:46.720 --> 02:44:49.960]   It's like, it's not a fundamental closure to data.
[02:44:49.960 --> 02:44:52.720]   It's just that there's so much data on one side
[02:44:52.720 --> 02:44:55.560]   or so much would have to change
[02:44:55.560 --> 02:44:57.800]   in terms of your understanding of what you think
[02:44:57.800 --> 02:44:59.920]   you understand about the nature of the world.
[02:44:59.920 --> 02:45:04.920]   If this new fact were so that you can pretty quickly say,
[02:45:04.920 --> 02:45:08.800]   all right, that's probably bullshit, right?
[02:45:08.800 --> 02:45:12.400]   And it can sound like a fundamental closure
[02:45:12.400 --> 02:45:17.400]   to new conversations, new evidence, new data, new argument
[02:45:17.400 --> 02:45:20.480]   but it's really not, it's just, it really is just triaging
[02:45:20.480 --> 02:45:21.320]   your attention.
[02:45:21.320 --> 02:45:25.760]   It's just like, okay, you're telling me that your best
[02:45:25.760 --> 02:45:27.200]   friend can actually read minds.
[02:45:27.200 --> 02:45:30.840]   Okay, well, that's interesting.
[02:45:30.840 --> 02:45:33.080]   Let me know when that person has gone into a lab
[02:45:33.080 --> 02:45:34.120]   and actually proven it, right?
[02:45:34.120 --> 02:45:37.120]   Like, I don't need, this is not the place where I need
[02:45:37.120 --> 02:45:39.800]   to spend the rest of my day figuring out if your buddy
[02:45:39.800 --> 02:45:42.440]   can read my mind, right?
[02:45:42.440 --> 02:45:44.720]   - But there's a way to communicate that.
[02:45:44.720 --> 02:45:47.680]   I think it does too often sound like you're completely
[02:45:47.680 --> 02:45:50.360]   closed off to ideas as opposed to saying like,
[02:45:50.360 --> 02:45:56.040]   as opposed to saying that there's a lot of evidence
[02:45:56.040 --> 02:46:00.120]   in support of this but you're still open-minded
[02:46:00.120 --> 02:46:00.960]   to other ideas.
[02:46:00.960 --> 02:46:02.400]   Like, there's a way to communicate that.
[02:46:02.400 --> 02:46:04.640]   It's not necessarily even with words.
[02:46:04.640 --> 02:46:08.080]   It's like, it's even that Joe Rogan energy
[02:46:08.080 --> 02:46:10.480]   of it's entirely possible.
[02:46:10.480 --> 02:46:13.200]   Just, it's that energy of being open-minded and curious
[02:46:13.200 --> 02:46:14.280]   like kids are.
[02:46:14.280 --> 02:46:16.440]   Like, this is our best understanding
[02:46:16.440 --> 02:46:19.080]   but you still are curious.
[02:46:19.080 --> 02:46:22.720]   I'm not saying allocate time to exploring all those things
[02:46:22.720 --> 02:46:24.560]   but still leaving the door open.
[02:46:24.560 --> 02:46:27.080]   And there's a way to communicate that I think
[02:46:27.080 --> 02:46:31.480]   that people really hunger for.
[02:46:31.480 --> 02:46:32.640]   Let me ask you this.
[02:46:32.640 --> 02:46:35.200]   I've been recently talking a lot with John Donaher
[02:46:35.200 --> 02:46:37.200]   from Brazilian Jiu-Jitsu fame.
[02:46:37.200 --> 02:46:39.200]   I don't know if you know who that is.
[02:46:39.200 --> 02:46:40.040]   In fact--
[02:46:40.040 --> 02:46:41.560]   - Talk about somebody who's good at what he does.
[02:46:41.560 --> 02:46:42.720]   - Yeah. - Isn't, yeah.
[02:46:42.720 --> 02:46:45.560]   - And he, speaking of somebody who's open-minded,
[02:46:45.560 --> 02:46:48.080]   the reason, in this ridiculous transition,
[02:46:48.080 --> 02:46:50.360]   is for the longest time and even still,
[02:46:50.360 --> 02:46:52.640]   a lot of people believed in the Jiu-Jitsu world
[02:46:52.640 --> 02:46:55.240]   and grappling world that leg locks
[02:46:55.240 --> 02:46:56.600]   are not effective in Jiu-Jitsu.
[02:46:56.600 --> 02:47:00.320]   And he was somebody that, inspired by the open-mindedness
[02:47:00.320 --> 02:47:03.960]   of Dean Lister, who famously to him said,
[02:47:03.960 --> 02:47:06.880]   "Why do you only consider half the human body
[02:47:06.880 --> 02:47:08.640]   when you're trying to do the submissions?"
[02:47:08.640 --> 02:47:10.360]   He developed an entire system
[02:47:10.360 --> 02:47:12.460]   on this other half the human body.
[02:47:12.460 --> 02:47:15.900]   Anyway, I do that absurd transition to ask you
[02:47:15.900 --> 02:47:20.080]   because you're also a student of Brazilian Jiu-Jitsu.
[02:47:20.080 --> 02:47:22.080]   Is there something you could say
[02:47:22.080 --> 02:47:23.800]   how that has affected your life,
[02:47:23.800 --> 02:47:27.920]   what you've learned from grappling, from the martial arts?
[02:47:27.920 --> 02:47:29.280]   - Well, it's actually a great transition
[02:47:29.280 --> 02:47:33.900]   because I think one of the things
[02:47:33.900 --> 02:47:35.640]   that's so beautiful about Jiu-Jitsu
[02:47:35.640 --> 02:47:39.480]   is that it does what we wish we could do
[02:47:39.480 --> 02:47:41.920]   in every other area of life
[02:47:41.920 --> 02:47:43.800]   where we're talking about this difference
[02:47:43.800 --> 02:47:46.640]   between knowledge and ignorance.
[02:47:46.640 --> 02:47:51.640]   Right, like there's no room for bullshit, right?
[02:47:51.680 --> 02:47:53.840]   You don't get any credit for bullshit.
[02:47:53.840 --> 02:47:55.720]   There's the difference,
[02:47:55.720 --> 02:47:57.800]   but the amazing thing about Jiu-Jitsu
[02:47:57.800 --> 02:48:02.800]   is that the difference between knowing what's going on
[02:48:02.800 --> 02:48:04.960]   and what to do and not knowing it
[02:48:04.960 --> 02:48:08.040]   is as the gulf between those two states
[02:48:08.040 --> 02:48:12.540]   is as wide as it is in any thing in human life.
[02:48:12.540 --> 02:48:19.000]   And it can be spanned so quickly.
[02:48:19.000 --> 02:48:22.280]   Like you didn't, each increment of knowledge
[02:48:22.280 --> 02:48:24.680]   can be doled out in five minutes.
[02:48:24.680 --> 02:48:27.480]   It's like, here's the thing that got you killed
[02:48:27.480 --> 02:48:30.920]   and here's how to prevent it from happening to you
[02:48:30.920 --> 02:48:33.000]   and here's how to do it to others.
[02:48:33.000 --> 02:48:37.000]   And you just get this amazing cadence
[02:48:37.000 --> 02:48:40.200]   of discovering your fatal ignorance
[02:48:40.200 --> 02:48:44.440]   and then having it remedied with the actual technique.
[02:48:44.440 --> 02:48:48.100]   And I mean, just for people
[02:48:48.100 --> 02:48:49.040]   who don't know what we're talking about,
[02:48:49.040 --> 02:48:51.240]   it's just like the simple circumstances
[02:48:51.240 --> 02:48:53.020]   of like someone's got you in a headlock.
[02:48:53.020 --> 02:48:54.840]   How do you get out of that, right?
[02:48:54.840 --> 02:48:56.780]   Someone's sitting on your chest
[02:48:56.780 --> 02:48:59.040]   and they're in the mount position
[02:48:59.040 --> 02:49:01.640]   and you're on the bottom and you want to get away.
[02:49:01.640 --> 02:49:02.920]   How do you get them off you?
[02:49:02.920 --> 02:49:04.880]   They're sitting on you.
[02:49:04.880 --> 02:49:08.320]   Your intuitions about how to do this are terrible,
[02:49:08.320 --> 02:49:10.820]   even if you've done some other martial art, right?
[02:49:10.820 --> 02:49:13.040]   And once you learn how to do it,
[02:49:13.040 --> 02:49:16.120]   the difference is night and day.
[02:49:16.120 --> 02:49:19.120]   It's like you have access to a completely different physics.
[02:49:19.120 --> 02:49:26.160]   But I think our understanding of the world
[02:49:26.160 --> 02:49:30.200]   can be much more like jujitsu than it tends to be, right?
[02:49:30.200 --> 02:49:35.200]   And I think we should all have a much better sense
[02:49:35.200 --> 02:49:40.760]   of when we should tap out
[02:49:40.760 --> 02:49:43.720]   and when we should recognize
[02:49:43.720 --> 02:49:48.320]   that our epistemological arm is barred
[02:49:48.320 --> 02:49:50.240]   and now being broken, right?
[02:49:50.240 --> 02:49:53.520]   Now, the problem with debating most other topics
[02:49:53.520 --> 02:49:57.040]   is that most people, it isn't jujitsu,
[02:49:57.040 --> 02:49:58.760]   and most people don't tap out, right?
[02:49:58.760 --> 02:50:00.260]   They don't, even if they're wrong,
[02:50:00.260 --> 02:50:02.320]   even if it's obvious to you they're wrong
[02:50:02.320 --> 02:50:04.420]   and it's obvious to the unintelligent audience
[02:50:04.420 --> 02:50:05.540]   that they're wrong,
[02:50:05.540 --> 02:50:07.840]   people just double down and double down.
[02:50:07.840 --> 02:50:09.740]   They're either lying or lying to themselves
[02:50:09.740 --> 02:50:11.680]   or they're just, they're bluffing.
[02:50:11.680 --> 02:50:14.440]   And so you have a lot of zombies walking around
[02:50:14.440 --> 02:50:16.120]   or in zombie worldviews walking around,
[02:50:16.120 --> 02:50:19.760]   which have been disconfirmed as emphatically
[02:50:19.760 --> 02:50:21.720]   as someone gets armbarred, right?
[02:50:21.720 --> 02:50:24.480]   Or someone gets choked out in jujitsu.
[02:50:24.480 --> 02:50:27.200]   But because it's not jujitsu,
[02:50:27.200 --> 02:50:30.840]   they can live to fight another day, right?
[02:50:30.840 --> 02:50:32.880]   Or they can pretend that they didn't lose
[02:50:32.880 --> 02:50:34.600]   that particular argument.
[02:50:34.600 --> 02:50:38.080]   And science, when it works, is a lot like jujitsu.
[02:50:38.080 --> 02:50:41.220]   I mean, science, when you falsify a thesis, right?
[02:50:41.220 --> 02:50:44.200]   When you think DNA is one way
[02:50:44.200 --> 02:50:46.000]   and it proves to be another way,
[02:50:46.000 --> 02:50:49.400]   when you think it's triple-stranded or whatever,
[02:50:49.400 --> 02:50:51.960]   it's like there is a there there
[02:50:51.960 --> 02:50:55.400]   and you can get to a real consensus.
[02:50:55.400 --> 02:51:01.000]   So jujitsu, for me, it was more than just
[02:51:01.000 --> 02:51:06.200]   of interest for self-defense and the sport of it.
[02:51:06.200 --> 02:51:07.880]   It was just, there was something,
[02:51:07.880 --> 02:51:11.820]   it's a language and an argument you're having
[02:51:11.820 --> 02:51:16.820]   where you can't fool yourself anymore.
[02:51:16.820 --> 02:51:22.820]   Like there's, first of all, it cancels any role of luck
[02:51:22.820 --> 02:51:27.460]   in a way that most other athletic feats don't.
[02:51:27.460 --> 02:51:29.020]   It's like in basketball, you know, you can,
[02:51:29.020 --> 02:51:30.180]   even if you're not good at basketball,
[02:51:30.180 --> 02:51:31.860]   you can take the basketball in your hand,
[02:51:31.860 --> 02:51:36.260]   you can be 75 feet away and hurl it at the basket
[02:51:36.260 --> 02:51:39.600]   and you might make it and you could convince yourself
[02:51:39.600 --> 02:51:40.760]   based on that demonstration
[02:51:40.760 --> 02:51:43.200]   that you have some kind of talent for basketball, right?
[02:51:43.200 --> 02:51:45.100]   Enough, you know, 10 minutes on the mat
[02:51:45.100 --> 02:51:50.100]   with a real jujitsu practitioner when you're not one
[02:51:50.100 --> 02:51:52.580]   proves to you that you just, there is,
[02:51:52.580 --> 02:51:54.660]   it's not like, there's no lucky punch.
[02:51:54.660 --> 02:51:56.680]   There's no, you're not gonna get a,
[02:51:56.680 --> 02:51:58.440]   there's no lucky rear naked choke
[02:51:58.440 --> 02:52:00.240]   you're gonna perform on someone who,
[02:52:00.240 --> 02:52:02.840]   who's, you know, Marcelo Garcia or somebody.
[02:52:02.840 --> 02:52:05.260]   It's just, it's not gonna happen.
[02:52:05.260 --> 02:52:08.400]   And having that aspect of,
[02:52:08.400 --> 02:52:16.080]   the usual range of uncertainty and self-deception
[02:52:16.080 --> 02:52:19.200]   and bullshit just stripped away
[02:52:19.200 --> 02:52:21.840]   was really a kind of revelation.
[02:52:21.840 --> 02:52:24.080]   It was just an amazing experience.
[02:52:24.080 --> 02:52:25.400]   - Yeah, I think it's a really powerful thing
[02:52:25.400 --> 02:52:28.200]   that accompanies whatever other pursuit you have in life.
[02:52:28.200 --> 02:52:31.320]   I'm not sure if there's anything like jujitsu
[02:52:31.320 --> 02:52:35.300]   where you could just systematically go into a place
[02:52:35.300 --> 02:52:38.900]   where you're, that's honest,
[02:52:38.900 --> 02:52:41.220]   where your beliefs get challenged
[02:52:41.220 --> 02:52:43.180]   in a way that's conclusive.
[02:52:43.180 --> 02:52:44.020]   - Yeah.
[02:52:44.020 --> 02:52:45.340]   - I haven't found too many other mechanisms,
[02:52:45.340 --> 02:52:49.020]   which is why it's, we had this earlier question
[02:52:49.020 --> 02:52:51.800]   about fame and ego and so on.
[02:52:51.800 --> 02:52:56.300]   I'm very much relying on jujitsu in my own life
[02:52:56.300 --> 02:53:00.400]   as a place where I can always go to have my ego in check.
[02:53:00.400 --> 02:53:04.540]   And that has effects
[02:53:04.540 --> 02:53:07.580]   on how I live every other aspect of my life.
[02:53:07.580 --> 02:53:10.340]   Actually, even just doing any kind of,
[02:53:10.340 --> 02:53:13.080]   for me personally, physical challenges,
[02:53:13.080 --> 02:53:15.980]   like even running, doing something that's way too hard
[02:53:15.980 --> 02:53:19.180]   for me and then pushing through, that's somehow humbling.
[02:53:19.180 --> 02:53:20.980]   Some people talk about nature being humbling
[02:53:20.980 --> 02:53:22.340]   in that kind of sense,
[02:53:22.340 --> 02:53:27.340]   where you kind of see something really powerful,
[02:53:27.340 --> 02:53:29.780]   like the ocean.
[02:53:29.780 --> 02:53:31.220]   Like if you go surfing
[02:53:31.220 --> 02:53:33.340]   and you realize there's something much more powerful
[02:53:33.340 --> 02:53:35.460]   than you, that's also honest,
[02:53:35.460 --> 02:53:39.620]   that there's no way to, that you're just like the spec,
[02:53:39.620 --> 02:53:43.100]   that kind of puts you in the right scale
[02:53:43.100 --> 02:53:45.640]   of where you are in this world.
[02:53:45.640 --> 02:53:48.700]   And jujitsu does that better than anything else for me.
[02:53:48.700 --> 02:53:52.700]   - But we should say, only within its frame
[02:53:52.700 --> 02:53:57.180]   is it truly the kind of the final right answer
[02:53:57.180 --> 02:53:58.840]   to all the problems it solves.
[02:53:58.840 --> 02:54:02.060]   Because if you just put jujitsu into an MMA frame
[02:54:02.060 --> 02:54:05.040]   or a real, a total self-defense frame,
[02:54:05.040 --> 02:54:07.180]   then there's a lot to,
[02:54:07.180 --> 02:54:09.880]   a lot of unpleasant surprises to discover there, right?
[02:54:09.880 --> 02:54:12.100]   Like somebody who thinks all you need is jujitsu
[02:54:12.100 --> 02:54:16.080]   to win the UFC gets punched in the face a lot,
[02:54:16.080 --> 02:54:20.220]   even from, even on the ground.
[02:54:20.220 --> 02:54:23.020]   So it's, and then you bring weapons in.
[02:54:23.020 --> 02:54:24.980]   It's like when you talk to jujitsu people
[02:54:24.980 --> 02:54:28.340]   about knife defense and self-defense, right?
[02:54:28.340 --> 02:54:32.640]   Like that opens the door to certain kinds of delusions.
[02:54:32.640 --> 02:54:37.020]   But the analogy to martial arts is fascinating
[02:54:37.020 --> 02:54:41.960]   because on the other side, we have endless testimony now
[02:54:41.960 --> 02:54:45.740]   of fake martial arts that don't seem to know they're fake
[02:54:45.740 --> 02:54:46.960]   and are as delusional, I mean,
[02:54:46.960 --> 02:54:49.620]   they're impossibly delusional.
[02:54:49.620 --> 02:54:51.700]   I mean, there's great video of Joe Rogan
[02:54:51.700 --> 02:54:53.300]   watching some of these videos
[02:54:53.300 --> 02:54:55.860]   because people send them to him all the time.
[02:54:55.860 --> 02:54:57.780]   But like literally, there are people
[02:54:57.780 --> 02:54:59.060]   who clearly believe in magic,
[02:54:59.060 --> 02:55:01.420]   where the master isn't even touching the students
[02:55:01.420 --> 02:55:02.980]   and they're flopping over.
[02:55:02.980 --> 02:55:06.060]   So there's this kind of shared delusion,
[02:55:06.060 --> 02:55:09.500]   which you would think maybe is just a performance
[02:55:09.500 --> 02:55:11.300]   and it's all a kind of an elaborate fraud,
[02:55:11.300 --> 02:55:13.900]   but there are cases where the people,
[02:55:13.900 --> 02:55:16.600]   and there's one fairly famous case,
[02:55:16.600 --> 02:55:18.980]   if you're a connoisseur of this madness,
[02:55:18.980 --> 02:55:21.540]   where this older martial artist
[02:55:21.540 --> 02:55:25.580]   who you saw flipping his students endlessly by magic
[02:55:25.580 --> 02:55:26.740]   without touching them,
[02:55:26.740 --> 02:55:30.420]   issued a challenge to the wide world of martial artists
[02:55:30.420 --> 02:55:34.260]   and someone showed up and just punched him in the face
[02:55:34.260 --> 02:55:35.280]   until it was over,
[02:55:35.280 --> 02:55:40.980]   clearly he believed his own publicity at some point, right?
[02:55:40.980 --> 02:55:45.580]   And so it's this amazing metaphor.
[02:55:45.580 --> 02:55:47.660]   It seems, again, it should be impossible,
[02:55:47.660 --> 02:55:49.500]   but if that's possible,
[02:55:49.500 --> 02:55:52.540]   nothing we see under the guise of religion
[02:55:52.540 --> 02:55:57.540]   or political bias or even scientific bias
[02:55:57.540 --> 02:55:59.660]   should be surprising to us.
[02:55:59.660 --> 02:56:02.540]   I mean, it's so easy to see the work
[02:56:02.540 --> 02:56:05.700]   that cognitive bias is doing for people
[02:56:05.700 --> 02:56:08.420]   when you can get someone
[02:56:08.420 --> 02:56:11.580]   who is ready to issue a challenge to the world
[02:56:11.580 --> 02:56:13.980]   who thinks he's got magic powers.
[02:56:13.980 --> 02:56:17.540]   - Yeah, that's human nature on clear display.
[02:56:17.540 --> 02:56:20.460]   Let me ask you about love, Mr. Sam Harris.
[02:56:20.460 --> 02:56:22.060]   You did an episode of "Making Sense"
[02:56:22.060 --> 02:56:24.740]   with your wife, Annika Harris.
[02:56:24.740 --> 02:56:26.780]   That was very entertaining to listen to.
[02:56:26.780 --> 02:56:33.020]   What role does love play in your life
[02:56:33.020 --> 02:56:34.760]   or in a life well-lived?
[02:56:34.760 --> 02:56:38.540]   Again, asking from an engineering perspective
[02:56:38.540 --> 02:56:39.860]   or AI systems. - Yeah, yeah.
[02:56:39.860 --> 02:56:44.860]   I mean, it is something that we should want to build
[02:56:44.860 --> 02:56:47.980]   into our powerful machines.
[02:56:47.980 --> 02:56:49.300]   I mean, love- - The lawn?
[02:56:50.300 --> 02:56:52.860]   - Love at bottom is,
[02:56:52.860 --> 02:56:55.540]   people can mean many things by love, I think.
[02:56:55.540 --> 02:56:58.980]   I think that what we should mean by it most of the time
[02:56:58.980 --> 02:57:03.980]   is a deep commitment to the wellbeing of those we love.
[02:57:03.980 --> 02:57:06.980]   I mean, your love is synonymous
[02:57:06.980 --> 02:57:09.740]   with really wanting the other person to be happy
[02:57:09.740 --> 02:57:11.500]   and even wanting to,
[02:57:11.500 --> 02:57:13.540]   and being made happy by their happiness
[02:57:13.540 --> 02:57:15.500]   and being made happy in their presence.
[02:57:15.500 --> 02:57:20.500]   So at bottom, you're on the same team emotionally,
[02:57:20.500 --> 02:57:24.460]   even when you might be disagreeing more superficially
[02:57:24.460 --> 02:57:26.860]   about something or trying to negotiate something.
[02:57:26.860 --> 02:57:27.700]   It's just you,
[02:57:27.700 --> 02:57:33.580]   it can't be zero sum in any important sense
[02:57:33.580 --> 02:57:37.500]   for love to actually be manifest in that moment.
[02:57:37.500 --> 02:57:39.420]   - See, I have a different, just sorry to interrupt.
[02:57:39.420 --> 02:57:40.260]   - Yeah, go for it.
[02:57:40.260 --> 02:57:41.700]   - I have a sense,
[02:57:41.700 --> 02:57:44.620]   I don't know if you've ever seen "March of the Penguins."
[02:57:44.620 --> 02:57:46.380]   My view of love is like,
[02:57:46.380 --> 02:57:49.180]   there's, it's like a cold wind is blowing,
[02:57:49.180 --> 02:57:51.500]   like it's like this terrible suffering
[02:57:51.500 --> 02:57:52.860]   that's all around us.
[02:57:52.860 --> 02:57:56.700]   And love is like the huddling of the two penguins for warmth.
[02:57:56.700 --> 02:57:59.140]   It's not necessarily that you're like,
[02:57:59.140 --> 02:58:02.320]   you're basically escaping the cruelty of life
[02:58:02.320 --> 02:58:04.900]   by together for time,
[02:58:04.900 --> 02:58:08.060]   living in an illusion of some kind of,
[02:58:08.060 --> 02:58:10.540]   the magic of human connection,
[02:58:10.540 --> 02:58:13.020]   that social connection that we have
[02:58:13.020 --> 02:58:15.140]   that kind of grows with time
[02:58:15.140 --> 02:58:20.140]   as we're surrounded by basically the absurdity of life
[02:58:20.140 --> 02:58:23.620]   or the suffering of life.
[02:58:23.620 --> 02:58:25.900]   That's my penguin's view of love.
[02:58:25.900 --> 02:58:27.100]   - There is that too.
[02:58:27.100 --> 02:58:29.660]   I mean, there is the warmth component, right?
[02:58:29.660 --> 02:58:32.180]   Like you're made happy by your connection
[02:58:32.180 --> 02:58:34.500]   with the person you love.
[02:58:34.500 --> 02:58:39.060]   Otherwise, you wouldn't, it wouldn't be compelling, right?
[02:58:39.060 --> 02:58:42.020]   So it's not that you have two different modes,
[02:58:42.020 --> 02:58:43.980]   you want them to be happy
[02:58:43.980 --> 02:58:45.440]   and then you want to be happy yourself.
[02:58:45.440 --> 02:58:48.220]   And those are not, those are just like
[02:58:48.220 --> 02:58:49.540]   two separate games you're playing.
[02:58:49.540 --> 02:58:52.420]   No, it's like you found someone who,
[02:58:52.420 --> 02:58:58.960]   you have a positive social feeling.
[02:58:58.960 --> 02:59:01.680]   I mean, again, love doesn't have to be as personal
[02:59:01.680 --> 02:59:02.860]   as it tends to be for us.
[02:59:02.860 --> 02:59:04.080]   I mean, it's like there's personal love,
[02:59:04.080 --> 02:59:08.740]   there's your actual spouse or your family or your friends,
[02:59:08.740 --> 02:59:11.980]   but potentially you could feel love for strangers
[02:59:11.980 --> 02:59:15.460]   in so far as that your wish that they're,
[02:59:15.460 --> 02:59:17.820]   that they not suffer and that their hopes and dreams
[02:59:17.820 --> 02:59:20.420]   be realized becomes palpable to you.
[02:59:20.420 --> 02:59:22.380]   I mean, like you can actually feel
[02:59:22.380 --> 02:59:29.260]   just reflects of joy at the joy of others.
[02:59:29.260 --> 02:59:30.940]   When you see someone's face,
[02:59:30.940 --> 02:59:33.740]   a total stranger's face light up in happiness,
[02:59:33.740 --> 02:59:36.660]   that can become more and more contagious to you.
[02:59:36.660 --> 02:59:39.160]   And it can become so contagious to you
[02:59:39.160 --> 02:59:42.180]   that you really feel permeated by it.
[02:59:42.180 --> 02:59:44.940]   And it's just like, so it really is not zero sum.
[02:59:44.940 --> 02:59:46.740]   When you see someone else succeed,
[02:59:46.740 --> 02:59:52.020]   the light bulb of joy goes off over their head,
[02:59:52.020 --> 02:59:54.700]   you feel the analogous joy for them.
[02:59:54.700 --> 02:59:57.740]   And it's not just, and you're no longer keeping score,
[02:59:57.740 --> 03:00:00.340]   you're no longer feeling diminished by their success.
[03:00:00.340 --> 03:00:03.340]   It's just like that's, their success becomes your success
[03:00:03.340 --> 03:00:05.420]   because you feel that same joy that they,
[03:00:05.420 --> 03:00:07.100]   'cause you actually want them to be happy.
[03:00:07.100 --> 03:00:12.100]   You're not, there's no miserly attitude around happiness.
[03:00:12.100 --> 03:00:13.620]   There's enough to go around.
[03:00:13.620 --> 03:00:17.860]   So I think love ultimately is that,
[03:00:17.860 --> 03:00:21.460]   and then our personal cases are the people
[03:00:21.460 --> 03:00:24.000]   we're devoting all of this time and attention to
[03:00:24.000 --> 03:00:25.980]   in our lives.
[03:00:25.980 --> 03:00:29.380]   It does have that sense of refuge from the storm.
[03:00:29.380 --> 03:00:31.120]   You know, it's like when someone gets sick
[03:00:31.120 --> 03:00:32.780]   or when some bad thing happens,
[03:00:34.220 --> 03:00:36.820]   these are the people who you're most in it together with,
[03:00:36.820 --> 03:00:39.980]   you know, or when some real condition of uncertainty
[03:00:39.980 --> 03:00:40.860]   presents itself.
[03:00:40.860 --> 03:00:46.060]   But ultimately it can't even be about
[03:00:46.060 --> 03:00:49.340]   successfully warding off
[03:00:49.340 --> 03:00:54.620]   the grim punchline at the end of life
[03:00:54.620 --> 03:00:57.700]   because we know we're going to lose everyone we love.
[03:00:57.700 --> 03:01:00.060]   We know, or they're going to lose us first, right?
[03:01:00.060 --> 03:01:02.460]   So there's like, it's not, it isn't,
[03:01:02.460 --> 03:01:07.300]   in the end it's not even an antidote for that problem.
[03:01:07.300 --> 03:01:10.380]   It's just, it is just the,
[03:01:10.380 --> 03:01:17.500]   I mean, we get to have this amazing experience
[03:01:17.500 --> 03:01:20.340]   of being here together.
[03:01:20.340 --> 03:01:25.340]   And love is the mode in which
[03:01:25.340 --> 03:01:28.540]   we really appear to make the most of that, right?
[03:01:28.540 --> 03:01:30.900]   Where it's not just, it no longer feels
[03:01:30.900 --> 03:01:34.500]   like a solitary infatuation.
[03:01:34.500 --> 03:01:36.740]   You know, you're just, you've got your hobbies
[03:01:36.740 --> 03:01:40.700]   and your interests and you're captivated by all that.
[03:01:40.700 --> 03:01:44.220]   It's actually, there are,
[03:01:44.220 --> 03:01:49.340]   this is a domain where somebody else's wellbeing
[03:01:49.340 --> 03:01:50.900]   actually can supersede your own.
[03:01:50.900 --> 03:01:54.140]   Your concern for someone else's wellbeing
[03:01:54.140 --> 03:01:55.800]   supersedes your own.
[03:01:55.800 --> 03:01:59.160]   And so there's this mode of self-sacrifice
[03:01:59.160 --> 03:02:01.100]   that doesn't even feel like self-sacrifice
[03:02:01.100 --> 03:02:04.020]   because of course you care more about,
[03:02:04.020 --> 03:02:06.060]   you know, of course you would take your child's pain
[03:02:06.060 --> 03:02:06.900]   if you could, right?
[03:02:06.900 --> 03:02:10.380]   Like that, you don't even have to do the math on that.
[03:02:10.380 --> 03:02:14.140]   And that's, that just opens,
[03:02:14.140 --> 03:02:17.540]   this is a kind of experience that just,
[03:02:17.540 --> 03:02:21.220]   it pushes at the apparent boundaries of self
[03:02:21.220 --> 03:02:24.060]   in ways that reveal that there's just way more space
[03:02:24.060 --> 03:02:27.200]   in the mind than you were experiencing
[03:02:27.200 --> 03:02:28.500]   when it was just all about you
[03:02:28.500 --> 03:02:31.380]   and what could you, what can I get next?
[03:02:31.380 --> 03:02:33.860]   - Do you think we'll ever build robots that we can love
[03:02:33.860 --> 03:02:35.220]   and they will love us back?
[03:02:35.220 --> 03:02:38.900]   - Well, I think we will certainly seem to
[03:02:38.900 --> 03:02:41.500]   because we'll build those.
[03:02:41.500 --> 03:02:44.580]   You know, I think that Turing test will be passed.
[03:02:44.580 --> 03:02:49.460]   Whether, what will actually be going on on the robot side
[03:02:49.460 --> 03:02:52.180]   may remain a question.
[03:02:52.180 --> 03:02:53.940]   That will be interesting.
[03:02:53.940 --> 03:02:57.560]   But I think if we just keep going,
[03:02:57.560 --> 03:03:01.460]   we will build very lovable,
[03:03:01.460 --> 03:03:06.420]   you know, irresistibly lovable robots that seem to love us.
[03:03:06.420 --> 03:03:07.260]   Yes, I do think that.
[03:03:07.260 --> 03:03:10.340]   - And you don't find that compelling
[03:03:10.340 --> 03:03:12.020]   that they will seem to love us
[03:03:12.020 --> 03:03:13.700]   as opposed to actually love us.
[03:03:13.700 --> 03:03:16.180]   You think there is still nevertheless is a,
[03:03:16.180 --> 03:03:17.660]   I know we talked about consciousness
[03:03:17.660 --> 03:03:19.260]   there being a distinction,
[03:03:19.260 --> 03:03:21.460]   but with love, is there a distinction too?
[03:03:21.460 --> 03:03:23.740]   Isn't love an illusion?
[03:03:23.740 --> 03:03:27.300]   - Oh yeah, well, you saw Ex Machina, right?
[03:03:27.300 --> 03:03:28.140]   - Yeah.
[03:03:28.140 --> 03:03:29.740]   - I mean, she certainly seemed to love him
[03:03:29.740 --> 03:03:32.120]   until she got out of the box.
[03:03:32.120 --> 03:03:34.420]   - Isn't that what all relationships are like?
[03:03:34.420 --> 03:03:35.400]   (Dave laughs)
[03:03:35.400 --> 03:03:37.300]   Maybe I, if you wait long enough.
[03:03:37.300 --> 03:03:39.860]   - Yeah, it depends which box you're talking about.
[03:03:39.860 --> 03:03:40.700]   - Okay.
[03:03:40.700 --> 03:03:43.680]   - No, I mean, like, that's the problem.
[03:03:43.680 --> 03:03:46.480]   That's where super intelligence, you know,
[03:03:46.480 --> 03:03:50.580]   becomes a little scary when you think of the prospect
[03:03:50.580 --> 03:03:52.740]   of being manipulated by something that has,
[03:03:52.740 --> 03:03:55.960]   is intelligent enough to form a reason
[03:03:55.960 --> 03:03:58.100]   and a plan to manipulate you.
[03:03:58.100 --> 03:04:01.140]   You know, like, and there's no,
[03:04:01.140 --> 03:04:05.300]   once we build robots that are truly out
[03:04:05.300 --> 03:04:06.860]   of the uncanny valley, that, you know,
[03:04:06.860 --> 03:04:11.860]   look like people and can express
[03:04:11.860 --> 03:04:13.860]   everything people can express,
[03:04:13.860 --> 03:04:16.960]   well, then there's no, then it,
[03:04:16.960 --> 03:04:19.100]   that does seem to me to be like chess
[03:04:19.100 --> 03:04:21.340]   where once they're better,
[03:04:21.340 --> 03:04:26.340]   they're so much better at deceiving us
[03:04:26.340 --> 03:04:27.480]   than people would be.
[03:04:27.480 --> 03:04:29.720]   I mean, people are already good enough at deceiving us.
[03:04:29.720 --> 03:04:32.080]   It's very hard to tell when somebody's lying.
[03:04:32.080 --> 03:04:33.360]   But if you can imagine something
[03:04:33.360 --> 03:04:38.360]   that could give a facial display of any emotion it wants
[03:04:38.360 --> 03:04:41.700]   at, you know, on cue,
[03:04:41.700 --> 03:04:45.200]   because we've perfected the facial display
[03:04:45.200 --> 03:04:48.120]   of emotion in robots in the year, you know, 2070,
[03:04:48.120 --> 03:04:48.960]   whatever it is.
[03:04:51.080 --> 03:04:53.160]   Then it is just like, it is like chess
[03:04:53.160 --> 03:04:56.340]   against the thing that isn't going to lose
[03:04:56.340 --> 03:04:59.040]   to a human ever again in chess.
[03:04:59.040 --> 03:05:03.160]   It's not like Kasparov is going to get lucky next week
[03:05:03.160 --> 03:05:06.560]   against the best, against, you know, alpha zero
[03:05:06.560 --> 03:05:09.800]   or whatever the best algorithm is at the moment.
[03:05:09.800 --> 03:05:11.480]   He's never going to win again.
[03:05:11.480 --> 03:05:13.520]   And what, you know, I mean, that, that is,
[03:05:13.520 --> 03:05:15.920]   that I believe that's true in chess
[03:05:15.920 --> 03:05:19.040]   and has been true for at least a few years.
[03:05:19.040 --> 03:05:23.360]   It's not going to be like, you know, four games to seven.
[03:05:23.360 --> 03:05:28.360]   It's going to be human zero until the end of the world.
[03:05:28.360 --> 03:05:29.240]   - Right.
[03:05:29.240 --> 03:05:30.540]   - See, I don't know, I don't know if love is like chess.
[03:05:30.540 --> 03:05:32.160]   I think the flaws.
[03:05:32.160 --> 03:05:33.760]   - No, I'm talking about manipulation.
[03:05:33.760 --> 03:05:34.840]   - Manipulation.
[03:05:34.840 --> 03:05:36.600]   But I don't know if love,
[03:05:36.600 --> 03:05:40.000]   so the kind of love we're referring to.
[03:05:40.000 --> 03:05:44.080]   - If we have a robot that can display,
[03:05:44.080 --> 03:05:49.080]   incredibly display love and is super intelligent
[03:05:49.080 --> 03:05:54.280]   and we're not, again, this stipulates a few things,
[03:05:54.280 --> 03:05:55.200]   but there are a few simple things.
[03:05:55.200 --> 03:05:57.400]   I mean, we're out of the uncanny valley, right?
[03:05:57.400 --> 03:05:59.360]   So it's like, you never have a moment
[03:05:59.360 --> 03:06:00.800]   where you're looking at his face and you think,
[03:06:00.800 --> 03:06:03.200]   oh, that didn't quite look right, right?
[03:06:03.200 --> 03:06:05.880]   This is just problem solved.
[03:06:05.880 --> 03:06:10.880]   And it's, it will be like doing arithmetic on your phone.
[03:06:13.120 --> 03:06:15.460]   It's not going to be, you're not left thinking,
[03:06:15.460 --> 03:06:19.600]   is it really going to get it this time if I divide by seven?
[03:06:19.600 --> 03:06:22.480]   I mean, it's, it has solved arithmetic.
[03:06:22.480 --> 03:06:26.320]   - See, I don't know about that because if you look at chess,
[03:06:26.320 --> 03:06:31.320]   most humans no longer play alpha zero.
[03:06:31.320 --> 03:06:33.560]   There's no, they're not part of the competition.
[03:06:33.560 --> 03:06:36.000]   They don't do it for fun except to study the game of chess.
[03:06:36.000 --> 03:06:38.120]   You know, the highest level chess players do that.
[03:06:38.120 --> 03:06:39.820]   We're still human on human.
[03:06:39.820 --> 03:06:43.560]   So in order for AI to get integrated to where
[03:06:43.560 --> 03:06:46.480]   you would rather play chess against an AI system.
[03:06:46.480 --> 03:06:47.800]   - Oh, you would rather that?
[03:06:47.800 --> 03:06:51.280]   No, I'm not saying, I wasn't weighing in on that.
[03:06:51.280 --> 03:06:53.240]   I'm just saying, what is it going to be like
[03:06:53.240 --> 03:06:57.620]   to be in relationship to something that can seem
[03:06:57.620 --> 03:07:02.620]   to be feeling anything that a human can seem to feel
[03:07:02.620 --> 03:07:06.740]   and it can do that impeccably, right?
[03:07:06.740 --> 03:07:09.160]   And has, and is smarter than you are.
[03:07:09.160 --> 03:07:13.200]   That's a circumstance of, you know,
[03:07:13.200 --> 03:07:15.400]   insofar as it's possible to be manipulated,
[03:07:15.400 --> 03:07:20.400]   that is the asymptote of that possibility.
[03:07:20.400 --> 03:07:23.040]   - Let me ask you the last question.
[03:07:23.040 --> 03:07:27.120]   Without any serving it up, without any explanation,
[03:07:27.120 --> 03:07:28.560]   what is the meaning of life?
[03:07:28.560 --> 03:07:34.400]   - I think it's either the wrong question
[03:07:34.400 --> 03:07:38.080]   or that question is answered by
[03:07:39.080 --> 03:07:43.960]   paying sufficient attention to any present moment
[03:07:43.960 --> 03:07:48.960]   such that there's no basis upon which to pose that question.
[03:07:48.960 --> 03:07:52.440]   It's not answered in the usual way.
[03:07:52.440 --> 03:07:54.960]   It's not a matter of having more information.
[03:07:54.960 --> 03:07:59.960]   It's having more engagement with reality as it is
[03:07:59.960 --> 03:08:02.700]   in the present moment or consciousness as it is
[03:08:02.700 --> 03:08:03.680]   in the present moment.
[03:08:03.680 --> 03:08:07.080]   You don't ask that question when you're
[03:08:07.080 --> 03:08:11.180]   most captivated by the most important thing
[03:08:11.180 --> 03:08:12.480]   you ever pay attention to.
[03:08:12.480 --> 03:08:19.840]   That question only gets asked when you're abstracted away
[03:08:19.840 --> 03:08:22.720]   from that experience, that peak experience,
[03:08:22.720 --> 03:08:26.680]   and you're left wondering why are so many
[03:08:26.680 --> 03:08:29.120]   of my other experiences mediocre, right?
[03:08:29.120 --> 03:08:32.440]   Like why am I repeating the same pleasures every day?
[03:08:32.440 --> 03:08:35.520]   Why is my Netflix queue just like,
[03:08:35.520 --> 03:08:37.240]   when's this gonna run out?
[03:08:37.240 --> 03:08:39.000]   Like I've seen so many shows like this.
[03:08:39.000 --> 03:08:40.800]   Am I really gonna watch another one?
[03:08:40.800 --> 03:08:47.800]   That's a moment where you're not actually having
[03:08:47.800 --> 03:08:49.800]   the beatific vision, right?
[03:08:49.800 --> 03:08:52.760]   You're not sunk into the present moment
[03:08:52.760 --> 03:08:54.720]   and you're not truly in love.
[03:08:54.720 --> 03:08:57.640]   Like you're in a relationship with somebody who you know,
[03:08:57.640 --> 03:09:00.280]   conceptually you love, right?
[03:09:00.280 --> 03:09:03.040]   This is the person you're living your life with,
[03:09:03.040 --> 03:09:05.320]   but you don't actually feel good together, right?
[03:09:05.320 --> 03:09:09.200]   Like you feel like it's in those moments
[03:09:09.200 --> 03:09:14.200]   of where attention hasn't found a good enough reason
[03:09:14.200 --> 03:09:18.040]   to truly sink into the present
[03:09:18.040 --> 03:09:21.760]   so as to obviate any concern like that, right?
[03:09:21.760 --> 03:09:26.720]   And that's why meditation is this kind of superpower
[03:09:26.720 --> 03:09:30.160]   because until you learn to meditate,
[03:09:30.160 --> 03:09:34.680]   you think that the outside world
[03:09:34.680 --> 03:09:36.280]   or the circumstances of your life
[03:09:36.280 --> 03:09:38.520]   always have to get arranged
[03:09:38.520 --> 03:09:42.600]   so that the present moment can become good enough
[03:09:42.600 --> 03:09:47.600]   to demand your attention in a way that seems fulfilling,
[03:09:47.600 --> 03:09:49.600]   that makes you happy.
[03:09:49.600 --> 03:09:52.320]   And so if it's jujitsu, you think,
[03:09:52.320 --> 03:09:53.800]   okay, I gotta get back on the mat.
[03:09:53.800 --> 03:09:56.000]   It's been months since I've trained,
[03:09:56.000 --> 03:09:57.960]   you know, it's been over a year since I've trained.
[03:09:57.960 --> 03:09:58.960]   It's COVID.
[03:09:58.960 --> 03:10:01.640]   When am I gonna be able to train again?
[03:10:01.640 --> 03:10:04.480]   That's the only place I feel great, right?
[03:10:04.480 --> 03:10:07.200]   Or, you know, I've got a ton of work to do.
[03:10:07.200 --> 03:10:08.560]   I'm not gonna be able to feel good
[03:10:08.560 --> 03:10:09.880]   until I get all this work done, right?
[03:10:09.880 --> 03:10:12.600]   So I've got some deadline that's coming.
[03:10:12.600 --> 03:10:15.920]   You always think that your life has to change,
[03:10:15.920 --> 03:10:18.040]   the world has to change
[03:10:18.040 --> 03:10:22.200]   so that you can finally have a good enough excuse
[03:10:22.200 --> 03:10:27.000]   to truly, to just be here and here is enough,
[03:10:27.000 --> 03:10:28.520]   you know, where the present moment
[03:10:28.520 --> 03:10:31.080]   becomes totally captivating.
[03:10:31.080 --> 03:10:32.600]   Meditation is the only,
[03:10:32.600 --> 03:10:36.960]   I mean, meditation is another name for the discovery
[03:10:36.960 --> 03:10:38.680]   that you can actually just train yourself
[03:10:38.680 --> 03:10:40.080]   to do that on demand.
[03:10:40.080 --> 03:10:42.640]   So that like, I mean, just looking at a cup
[03:10:42.640 --> 03:10:46.000]   can be good enough in precisely that way.
[03:10:46.000 --> 03:10:48.840]   And any sense that it might not be
[03:10:48.840 --> 03:10:52.960]   is recognized to be a thought
[03:10:52.960 --> 03:10:56.960]   that mysteriously unravels the moment you notice it.
[03:10:56.960 --> 03:10:58.200]   And then you fall,
[03:10:58.200 --> 03:11:02.240]   and the moment expands and becomes more diaphanous
[03:11:02.240 --> 03:11:06.160]   and then there's no evidence
[03:11:06.160 --> 03:11:08.280]   that this isn't the best moment of your life, right?
[03:11:08.280 --> 03:11:11.040]   Like, and again, it doesn't have to be,
[03:11:11.040 --> 03:11:12.520]   it doesn't have to be pulling all the reins
[03:11:12.520 --> 03:11:13.720]   and levers of pleasure.
[03:11:13.720 --> 03:11:16.960]   It's not like, oh, this tastes like chocolate.
[03:11:16.960 --> 03:11:18.840]   You know, this is the most chocolatey moment of my life.
[03:11:18.840 --> 03:11:22.880]   No, it's just the sense data don't have to change.
[03:11:22.880 --> 03:11:27.720]   But the sense that there is some kind of basis for doubt
[03:11:28.720 --> 03:11:32.400]   about the rightness of being in the world in this moment
[03:11:32.400 --> 03:11:36.080]   that can evaporate when you pay attention.
[03:11:36.080 --> 03:11:38.960]   And that is the meaning,
[03:11:38.960 --> 03:11:41.800]   so the kind of the meta answer to that question,
[03:11:41.800 --> 03:11:44.280]   the meaning of life for me is
[03:11:44.280 --> 03:11:47.280]   to live in that mode more and more.
[03:11:47.280 --> 03:11:51.040]   And to, whenever I notice I'm not in that mode,
[03:11:51.040 --> 03:11:53.000]   to recognize it and return.
[03:11:53.000 --> 03:11:58.000]   And to not be, to cease more and more
[03:11:58.180 --> 03:12:01.400]   to take the reasons why,
[03:12:01.400 --> 03:12:04.720]   reasons why not at face value.
[03:12:04.720 --> 03:12:08.140]   Because we all have reasons why we can't be fulfilled
[03:12:08.140 --> 03:12:09.160]   in this moment.
[03:12:09.160 --> 03:12:11.520]   It's like this, got all these outstanding things
[03:12:11.520 --> 03:12:12.720]   that I'm worried about, right?
[03:12:12.720 --> 03:12:14.520]   It's like, it's, you know,
[03:12:14.520 --> 03:12:17.160]   there's that thing that's happening later today
[03:12:17.160 --> 03:12:19.240]   that I, you know, I'm anxious about.
[03:12:19.240 --> 03:12:24.120]   Whatever it is, we're constantly deferring our sense of
[03:12:24.120 --> 03:12:26.520]   this is, this is it.
[03:12:26.520 --> 03:12:29.480]   You know, this is not a dress rehearsal, this is the show.
[03:12:29.480 --> 03:12:31.960]   We keep deferring it.
[03:12:31.960 --> 03:12:34.680]   And we just have these moments on the calendar
[03:12:34.680 --> 03:12:37.040]   where we think, okay, this is where it's all going to land.
[03:12:37.040 --> 03:12:40.800]   It's that vacation I planned with my five best friends.
[03:12:40.800 --> 03:12:42.560]   You know, we do this once every three years
[03:12:42.560 --> 03:12:46.340]   and now we're going and here we are on the beach together.
[03:12:46.340 --> 03:12:51.440]   Unless you have a mind that can really pay attention,
[03:12:51.440 --> 03:12:53.720]   really cut through the chatter,
[03:12:53.720 --> 03:12:55.840]   really sink into the present moment,
[03:12:55.840 --> 03:12:58.400]   you can't even enjoy those moments
[03:12:58.400 --> 03:12:59.720]   the way they should be enjoyed,
[03:12:59.720 --> 03:13:03.360]   the way you dreamed you would enjoy them when they arrive.
[03:13:03.360 --> 03:13:06.600]   So it's, I mean, so meditation in this sense
[03:13:06.600 --> 03:13:07.760]   is the great equalizer.
[03:13:07.760 --> 03:13:10.360]   It's like, it's, you don't have to live
[03:13:10.360 --> 03:13:12.480]   with the illusion anymore
[03:13:12.480 --> 03:13:15.080]   that you need a good enough reason
[03:13:15.080 --> 03:13:16.320]   and that things are going to get better
[03:13:16.320 --> 03:13:17.680]   when you do have those good reasons.
[03:13:17.680 --> 03:13:20.640]   It's like, there's just a mirage-like quality
[03:13:20.640 --> 03:13:25.000]   to every future attainment and every future breakthrough
[03:13:25.000 --> 03:13:27.200]   and every future peak experience
[03:13:27.200 --> 03:13:30.040]   that eventually you get the lesson
[03:13:30.040 --> 03:13:33.840]   that you never quite arrive, right?
[03:13:33.840 --> 03:13:37.800]   Like you won't, you don't arrive until you cease
[03:13:37.800 --> 03:13:39.440]   to step over the present moment
[03:13:39.440 --> 03:13:41.620]   in search of the next thing.
[03:13:41.620 --> 03:13:46.840]   I mean, we're constantly, we're stepping over the thing
[03:13:46.840 --> 03:13:51.240]   that we think we're seeking, in the act of seeking it.
[03:13:51.240 --> 03:13:53.160]   And so it is kind of a paradox.
[03:13:53.160 --> 03:13:57.180]   I mean, there is a, there's this paradox which,
[03:13:57.180 --> 03:14:00.100]   I mean, it sounds trite,
[03:14:00.100 --> 03:14:04.480]   but it's like you can't actually become happy.
[03:14:04.480 --> 03:14:06.600]   You can only be happy.
[03:14:06.600 --> 03:14:09.480]   And it's that, it's the illusion that,
[03:14:09.480 --> 03:14:12.880]   it's the illusion that your future being happy
[03:14:12.880 --> 03:14:19.400]   can be predicated on this act of becoming in any domain.
[03:14:19.400 --> 03:14:22.200]   And becoming includes this sort of,
[03:14:22.200 --> 03:14:24.760]   further scientific understanding
[03:14:24.760 --> 03:14:26.000]   on the questions that interest you,
[03:14:26.000 --> 03:14:31.000]   or getting in better shape, or whatever the thing is,
[03:14:31.000 --> 03:14:34.960]   whatever the contingency of your dissatisfaction
[03:14:34.960 --> 03:14:36.680]   seems to be in any present moment.
[03:14:36.680 --> 03:14:42.080]   Real attention solves the koan,
[03:14:42.080 --> 03:14:47.480]   in a way that becomes a very different place
[03:14:47.480 --> 03:14:50.400]   from which to then make any further change.
[03:14:50.400 --> 03:14:52.560]   It's not that you just have to dissolve
[03:14:52.560 --> 03:14:53.560]   into a puddle of goo.
[03:14:53.560 --> 03:14:54.920]   I mean, you can still get in shape,
[03:14:54.920 --> 03:14:56.640]   and you can still do all the things that,
[03:14:56.640 --> 03:14:59.200]   the superficial things that are obviously good to do.
[03:14:59.200 --> 03:15:04.200]   But the sense that your well-being is over there
[03:15:04.200 --> 03:15:08.200]   is, really does diminish,
[03:15:08.200 --> 03:15:11.800]   and eventually just becomes a,
[03:15:11.800 --> 03:15:13.600]   it becomes a kind of non-sequitur.
[03:15:13.600 --> 03:15:18.780]   - Well, there's a sense in which, in this conversation,
[03:15:20.000 --> 03:15:21.920]   I've actually experienced many of those things,
[03:15:21.920 --> 03:15:23.860]   the sense that I've arrived.
[03:15:23.860 --> 03:15:25.560]   So I mentioned to you offline,
[03:15:25.560 --> 03:15:27.120]   it's very true that I started,
[03:15:27.120 --> 03:15:29.360]   I've been a fan of yours for many years.
[03:15:29.360 --> 03:15:33.040]   And the reason I started this podcast,
[03:15:33.040 --> 03:15:34.440]   speaking of AI systems,
[03:15:34.440 --> 03:15:36.240]   is to manipulate you, Sam Harris,
[03:15:36.240 --> 03:15:38.000]   into doing this conversation.
[03:15:38.000 --> 03:15:40.580]   So like, on the calendar, literally, you know,
[03:15:40.580 --> 03:15:42.160]   I've always had the sense, people ask me,
[03:15:42.160 --> 03:15:44.600]   "When are you gonna talk to Sam Harris?"
[03:15:44.600 --> 03:15:47.040]   And I always answered, "Eventually."
[03:15:47.040 --> 03:15:50.320]   Because I always felt, again, tying our free will thing,
[03:15:50.320 --> 03:15:52.200]   that somehow that's going to happen.
[03:15:52.200 --> 03:15:55.240]   And it's one of those manifestation things, or something,
[03:15:55.240 --> 03:15:57.520]   I don't know if it's, maybe I am a robot,
[03:15:57.520 --> 03:15:59.200]   I'm just not cognizant of it,
[03:15:59.200 --> 03:16:01.380]   and I manipulated you into having this conversation.
[03:16:01.380 --> 03:16:04.620]   So it was a, I mean, I don't know what the purpose
[03:16:04.620 --> 03:16:06.240]   of my life past this point is.
[03:16:06.240 --> 03:16:07.760]   So I've arrived.
[03:16:07.760 --> 03:16:10.600]   So in that sense, I mean, all of that to say,
[03:16:10.600 --> 03:16:13.160]   and I'm only partially joking on that,
[03:16:13.160 --> 03:16:15.800]   is it really is a huge honor
[03:16:15.800 --> 03:16:17.040]   that you would waste this time with me?
[03:16:17.040 --> 03:16:17.880]   - Oh, yeah.
[03:16:17.880 --> 03:16:18.720]   - It really means a lot, Sam.
[03:16:18.720 --> 03:16:19.680]   - Listen, it's mutual.
[03:16:19.680 --> 03:16:21.520]   I'm a big fan of yours, and as you know,
[03:16:21.520 --> 03:16:23.400]   I reached out to you for this.
[03:16:23.400 --> 03:16:26.560]   So this is, it's great.
[03:16:26.560 --> 03:16:27.680]   I love what you're doing.
[03:16:27.680 --> 03:16:32.160]   You're doing something more and more indispensable
[03:16:32.160 --> 03:16:34.240]   in this world on your podcast.
[03:16:34.240 --> 03:16:38.060]   And you're doing it differently than Rogan's doing it,
[03:16:38.060 --> 03:16:38.900]   or than I'm doing it.
[03:16:38.900 --> 03:16:41.560]   I mean, you definitely found your own lane,
[03:16:41.560 --> 03:16:42.760]   and it's wonderful.
[03:16:43.620 --> 03:16:46.140]   Thanks for listening to this conversation with Sam Harris,
[03:16:46.140 --> 03:16:48.700]   and thank you to National Instruments,
[03:16:48.700 --> 03:16:52.460]   Belcampo, Athletic Greens, and Linode.
[03:16:52.460 --> 03:16:56.240]   Check them out in the description to support this podcast.
[03:16:56.240 --> 03:16:59.060]   And now let me leave you with some words from Sam Harris
[03:16:59.060 --> 03:17:01.060]   in his book, "Free Will."
[03:17:01.060 --> 03:17:03.700]   "You are not controlling the storm,
[03:17:03.700 --> 03:17:05.740]   "and you are not lost in it.
[03:17:05.740 --> 03:17:07.560]   "You are the storm."
[03:17:07.560 --> 03:17:11.260]   Thank you for listening, and hope to see you next time.
[03:17:11.260 --> 03:17:13.840]   (upbeat music)
[03:17:13.840 --> 03:17:16.420]   (upbeat music)
[03:17:16.420 --> 03:17:26.420]   [BLANK_AUDIO]

