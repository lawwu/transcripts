
[00:00:00.000 --> 00:00:05.000]   Okay. Hey folks. My name is Andrew Truong. I'm a machine learning engineer at Weights and Biases.
[00:00:05.000 --> 00:00:11.000]   And today we're going to talk about principled ML workflows, how to spend more time on ML and less on Ops.
[00:00:11.000 --> 00:00:17.000]   So, ML is a pretty experimental field. And so there are a lot of different experiments that we're trying out.
[00:00:17.000 --> 00:00:19.000]   And hopefully you're logging them somewhere.
[00:00:19.000 --> 00:00:23.000]   Some folks might be logging their experiments in something like a log file.
[00:00:23.000 --> 00:00:27.000]   Others might have something that looks like a spreadsheet.
[00:00:27.000 --> 00:00:30.000]   And maybe you are using something like TensorBoard.
[00:00:30.000 --> 00:00:34.000]   Having used all of these tools before, I think they have a time and a place.
[00:00:34.000 --> 00:00:44.000]   But some problems start to crop up when your team starts to expand beyond just yourself to maybe two or four or more people.
[00:00:44.000 --> 00:00:50.000]   And then all of a sudden you have these Jupyter notebooks and they're flying around in different emails.
[00:00:50.000 --> 00:00:55.000]   You're posting things onto Confluence. Your stakeholders are asking for stuff in PowerPoint decks.
[00:00:55.000 --> 00:00:58.000]   You're getting pinged on Teams, Slack.
[00:00:58.000 --> 00:01:03.000]   And I think for the very pain-tolerant among us, this might be fine.
[00:01:03.000 --> 00:01:06.000]   But for me, I think there's got to be a better way.
[00:01:06.000 --> 00:01:11.000]   And so here are three principles for a more ideal ML workflow.
[00:01:11.000 --> 00:01:19.000]   Ideally, we're able to rapidly iterate, reproduce, and collaborate on the work that we're working on.
[00:01:19.000 --> 00:01:22.000]   Okay. So, number one, rapidly iterate.
[00:01:22.000 --> 00:01:24.000]   What do I mean by that?
[00:01:24.000 --> 00:01:30.000]   Well, we have these different experiments that are kind of running in their own separate enclaves.
[00:01:30.000 --> 00:01:34.000]   And we'd like to bring them back into one single repository.
[00:01:34.000 --> 00:01:37.000]   An easy way to do this is with Weights & Biases.
[00:01:37.000 --> 00:01:39.000]   So you can get started really quickly.
[00:01:39.000 --> 00:01:44.000]   Simply pip install the package, type in 1b.init to set up an experiment,
[00:01:44.000 --> 00:01:48.000]   and then 1b.log to track any of the metrics that you care about.
[00:01:48.000 --> 00:01:53.000]   And to prove that this is actually the case, let's see it in action.
[00:01:53.000 --> 00:01:58.000]   So here, I'm going to install the package and then import the libraries.
[00:01:58.000 --> 00:02:02.000]   And then I'm going to run the snippet, which is going to set up an experiment,
[00:02:02.000 --> 00:02:05.000]   and then log some metrics.
[00:02:05.000 --> 00:02:12.000]   What you'll see is in real time, the results are getting piped to the Weights & Biases UI.
[00:02:12.000 --> 00:02:16.000]   And so you can see here, these charts down here are updating live.
[00:02:16.000 --> 00:02:23.000]   And we can do all of this because we have this special 1b magic that outputs this in the notebook.
[00:02:23.000 --> 00:02:27.000]   So if you're a fan of notebooks, like I am, you can get everything inside the notebook itself.
[00:02:27.000 --> 00:02:34.000]   And of course, if you prefer a more traditional interface, you can always just go to the actual website itself.
[00:02:34.000 --> 00:02:39.000]   These metrics will update live here as well.
[00:02:39.000 --> 00:02:41.000]   Cool.
[00:02:41.000 --> 00:02:44.000]   But, you know, you're logging these metrics, and that's great.
[00:02:44.000 --> 00:02:49.000]   But ideally, that central repository is keeping track of everything, right?
[00:02:49.000 --> 00:02:51.000]   All of the data that's going in.
[00:02:51.000 --> 00:02:55.000]   And so you might be keeping track of things like images and segmentation masks.
[00:02:55.000 --> 00:03:00.000]   Maybe you have 3D point clouds and bounding boxes.
[00:03:00.000 --> 00:03:07.000]   You might be keeping track of audio, spectrograms, maybe even the underlying Matplotlib or Plotly figures.
[00:03:07.000 --> 00:03:11.000]   Maybe it's video or POS.
[00:03:11.000 --> 00:03:15.000]   Maybe your notebooks have these cool custom HTML visualizations.
[00:03:15.000 --> 00:03:20.000]   Whatever it is, ideally, that single system of record is capturing all of this data.
[00:03:20.000 --> 00:03:24.000]   And at Wins and Biases, our goal is to support all of these types.
[00:03:24.000 --> 00:03:30.000]   And so for a full set of types, you can learn more about them at docs.1b.ai.
[00:03:30.000 --> 00:03:32.000]   And if we're missing a type, then let us know.
[00:03:32.000 --> 00:03:37.000]   We're always looking to add support for new types.
[00:03:37.000 --> 00:03:38.000]   Cool.
[00:03:38.000 --> 00:03:40.000]   So we logged all this data.
[00:03:40.000 --> 00:03:44.000]   Now we often want to ask interesting questions about the data, right?
[00:03:44.000 --> 00:03:47.000]   Things like what were the inputs?
[00:03:47.000 --> 00:03:49.000]   What did the outputs look like?
[00:03:49.000 --> 00:03:51.000]   How did they compare to the ground truth?
[00:03:51.000 --> 00:04:01.000]   And today, you might spin up a mishmash of tools, ad hoc and kind of custom apps, where you try to do this analysis.
[00:04:01.000 --> 00:04:04.000]   And there's a bit of overhead involved in that.
[00:04:04.000 --> 00:04:12.000]   But one of the really nice things about the Wins and Biases interface is that just by specifying the type of data that you logged,
[00:04:12.000 --> 00:04:14.000]   you get a lot of these things for free.
[00:04:14.000 --> 00:04:16.000]   Let me show you.
[00:04:16.000 --> 00:04:21.000]   So here is an example of some data that we logged.
[00:04:21.000 --> 00:04:26.000]   In this case, we logged an image and some segmentation masks.
[00:04:26.000 --> 00:04:30.000]   I'm going to split them up into their components so we get a better sense of what we're looking at here.
[00:04:30.000 --> 00:04:34.000]   So in this first column, what you see are the actual camera images.
[00:04:34.000 --> 00:04:38.000]   So these are scenes of some street driving.
[00:04:38.000 --> 00:04:43.000]   In the second column, we have what our model's predictions were.
[00:04:43.000 --> 00:04:49.000]   And in the third column, we have the actual ground truth labels.
[00:04:49.000 --> 00:04:56.000]   What I can do is if I go all the way back in time, we can see the model when it was initially set up.
[00:04:56.000 --> 00:05:02.000]   And you can see that there doesn't seem to be a lot of agreement between the ground truth and the actual predictions.
[00:05:02.000 --> 00:05:06.000]   And we can get a better sense of that by overlaying those masks on top of each other.
[00:05:06.000 --> 00:05:11.000]   So you'll notice that for some areas, like this tree part, it looks like there's some overlap, which is good.
[00:05:11.000 --> 00:05:15.000]   But for this person, it looks like we're confusing them to be a road.
[00:05:15.000 --> 00:05:19.000]   And that's probably not a good thing.
[00:05:19.000 --> 00:05:29.000]   So what I can do also is layer these masks on top of the actual camera image to get a full sense of what exactly are we looking at here.
[00:05:29.000 --> 00:05:34.000]   Now, the UI has this option to step through time.
[00:05:34.000 --> 00:05:40.000]   And so you can imagine this as like I do epoch or some n number of steps that you're going through.
[00:05:40.000 --> 00:05:48.000]   And as we increase the step counter, hopefully we start to see that the model begins to converge on what the ground truth labels look like.
[00:05:48.000 --> 00:05:58.000]   And if I drag the step slider all the way to the right, we'll begin to see that, hey, it looks like we are picking up the fact that this is a person, this is a car, you know, these are some trees.
[00:05:58.000 --> 00:06:00.000]   So it looks like our model is learning something.
[00:06:00.000 --> 00:06:05.000]   We got all of this for free just by logging the type of data that we're working with.
[00:06:05.000 --> 00:06:12.000]   No more custom code or ad hoc notebooks to look at.
[00:06:12.000 --> 00:06:14.000]   Cool.
[00:06:14.000 --> 00:06:18.000]   There's another type of analysis which I think is really interesting.
[00:06:18.000 --> 00:06:20.000]   And that is in the tabular case.
[00:06:20.000 --> 00:06:28.000]   So in the tabular world, we've had these ideas of filtering and group bys and plots for a really long time.
[00:06:28.000 --> 00:06:31.000]   And they work amazing.
[00:06:31.000 --> 00:06:37.000]   But why haven't they transferred over to non-tabular data?
[00:06:37.000 --> 00:06:49.000]   I think the probably most likely reason is because if you try to put these images or other types of rich media into a data frame, something like pandas, you don't actually get a picture, right?
[00:06:49.000 --> 00:06:54.000]   You just get the place where it's loaded in memory, which is not ideal.
[00:06:54.000 --> 00:06:59.000]   But what if you could get the picture in the data frame?
[00:06:59.000 --> 00:07:03.000]   And what if that table then supported data frame semantics, things like group by?
[00:07:03.000 --> 00:07:05.000]   What would that look like?
[00:07:05.000 --> 00:07:07.000]   Let's take a look at this.
[00:07:07.000 --> 00:07:09.000]   So this is super cool.
[00:07:09.000 --> 00:07:11.000]   Let's take a look here.
[00:07:11.000 --> 00:07:18.000]   I want to show you first of all that the table that I'm talking about, this data frame, supports all of the types that we mentioned earlier, right?
[00:07:18.000 --> 00:07:22.000]   So things like molecules, you can interact with them, play around with the different molecules.
[00:07:22.000 --> 00:07:27.000]   You can log plots in addition to the standard ints, floats, and strings that you've seen.
[00:07:27.000 --> 00:07:32.000]   Here's another one with actual audio files and spectrograms, things like that.
[00:07:32.000 --> 00:07:38.000]   But let's take a look at a really common use case that you might see done with tables.
[00:07:38.000 --> 00:07:42.000]   So what we're looking at here is a table.
[00:07:42.000 --> 00:07:44.000]   And you might notice a couple of things.
[00:07:44.000 --> 00:07:50.000]   So first off, there are some very familiar ints, floats, and strings that you might be used to in a more classic data frame.
[00:07:50.000 --> 00:07:53.000]   But we also have this vector of images right here.
[00:07:53.000 --> 00:07:56.000]   Okay, so these are actual images that are showing up in a table.
[00:07:56.000 --> 00:07:57.000]   Super cool.
[00:07:57.000 --> 00:07:59.000]   Okay.
[00:07:59.000 --> 00:08:06.000]   A common operation that you might want to do -- oh, and I should mention ahead of time, this table is for classification, right?
[00:08:06.000 --> 00:08:11.000]   So we have an image, and we're trying to classify what kingdom does this thing belong to.
[00:08:11.000 --> 00:08:12.000]   Okay.
[00:08:12.000 --> 00:08:18.000]   So you can see here, these are the guesses and these are the truths, and these are the different classes that they could belong to.
[00:08:18.000 --> 00:08:19.000]   Okay.
[00:08:19.000 --> 00:08:25.000]   A common operation that you might want to do with data frames is you might want to filter the data frame.
[00:08:25.000 --> 00:08:31.000]   Right, so one thing that I could do is say, show me where the guess is not equal to the truth.
[00:08:31.000 --> 00:08:35.000]   Or in other words, where did my model get something wrong?
[00:08:35.000 --> 00:08:46.000]   I can apply this filter, and now I'll see rows where the model got a bad prediction, and also what were the scores that led up to that decision.
[00:08:46.000 --> 00:08:48.000]   Okay, so super cool.
[00:08:48.000 --> 00:08:52.000]   And I can see all of the examples of things that the model got wrong.
[00:08:52.000 --> 00:08:54.000]   Okay.
[00:08:54.000 --> 00:08:57.000]   Another thing that I might want to do then is group by.
[00:08:57.000 --> 00:09:01.000]   So I can group on the guess.
[00:09:01.000 --> 00:09:05.000]   Okay, and you'll see a couple of things which are really neat.
[00:09:05.000 --> 00:09:12.000]   So first off, each row now represents something that our model predicted but got wrong.
[00:09:12.000 --> 00:09:23.000]   Okay, so here are the cases where the model thought they were fungi, and of the things that it got wrong, here were the actual ground truth distributions.
[00:09:23.000 --> 00:09:26.000]   Right, so we've done a filter, and then a group by.
[00:09:26.000 --> 00:09:30.000]   I can take a look at this distribution and see whether or not that makes sense.
[00:09:30.000 --> 00:09:39.000]   I can also go a little bit deeper and analyze the distribution of scores to see how good or bad or reasonable the model is doing.
[00:09:39.000 --> 00:09:48.000]   And I think most importantly, we can get an intuitive sense of how our model is performing, because we can actually look at the images that it got wrong.
[00:09:48.000 --> 00:09:53.000]   Right, so here, it looks like, this looks like a frog. So is it reasonable that this is a fungi?
[00:09:53.000 --> 00:09:58.000]   Maybe we can go through this carousel and see other things that the model got wrong.
[00:09:58.000 --> 00:10:02.000]   You know, is it reasonable that this is a fungus? Who knows.
[00:10:02.000 --> 00:10:09.000]   And of course, because we support layering things like masks on top or bounding boxes, you can actually layer these things on top, right?
[00:10:09.000 --> 00:10:17.000]   So you can actually see, you know, what did the activations look like and layer any sort of arbitrary masks or different visualization tools on top of these images here.
[00:10:17.000 --> 00:10:22.000]   Okay, so really interesting stuff.
[00:10:22.000 --> 00:10:26.000]   I'm going to reset the table for a moment. And so we talked about a group by, we talked about a filter.
[00:10:26.000 --> 00:10:29.000]   Another common thing that you might want to do is plot.
[00:10:29.000 --> 00:10:33.000]   So here, we actually have a plotting option on the table.
[00:10:33.000 --> 00:10:36.000]   So I can just choose, instead of table, I can select plot.
[00:10:36.000 --> 00:10:39.000]   And you'll see, we'll auto generate the plot for you, right?
[00:10:39.000 --> 00:10:48.000]   So this is along the lines of df.plot, or if you use Plotly or Express or Altair, you basically specify what you want the X and Y and the different dimensions to be.
[00:10:48.000 --> 00:10:52.000]   And then we'll generate a plot for you, right? So you can specify them up here.
[00:10:52.000 --> 00:10:54.000]   Okay.
[00:10:54.000 --> 00:10:58.000]   So here, this plot was generated automatically.
[00:10:58.000 --> 00:11:07.000]   The cherry on top, I think, is often the data is like super complex, and we want to project down this high dimensional thing into this lower dimensional space.
[00:11:07.000 --> 00:11:13.000]   And you can also do this in the UI just by selecting the projection plot.
[00:11:13.000 --> 00:11:17.000]   So here, I projected down using PCA.
[00:11:17.000 --> 00:11:22.000]   But I could choose any number of other things as well, between t-SNE and UMAP.
[00:11:22.000 --> 00:11:27.000]   Okay, so here I can see this lower dimensional representation of my data.
[00:11:27.000 --> 00:11:33.000]   And of course, because these are plots, I can also hover over individual dots to see, you know, does that make sense?
[00:11:33.000 --> 00:11:37.000]   Is my data labeled correctly? So on and so forth.
[00:11:37.000 --> 00:11:43.000]   And what's really neat is, again, we got this for free just because we were logging based on types, right?
[00:11:43.000 --> 00:11:46.000]   So there's no code to generate the visualizations.
[00:11:46.000 --> 00:11:51.000]   We just log the data. And then we said, show me a table or show me a plot.
[00:11:51.000 --> 00:11:57.000]   Right, so really flexible stuff helps a lot with rapidly iterating.
[00:11:57.000 --> 00:12:01.000]   Cool. I want to show you one last thing, which I think is really, really neat.
[00:12:01.000 --> 00:12:06.000]   Okay, so I'm going to turn off some of these a moment, just so that we can get a sense of what's going on here.
[00:12:06.000 --> 00:12:13.000]   So this table shows us how we can compare different models to each other.
[00:12:13.000 --> 00:12:21.000]   So we could group by things, we could filter things, but now we also might want to compare across different types of models.
[00:12:21.000 --> 00:12:28.000]   Maybe you're doing architecture search or maybe you have just different techniques that you're trying out, different experiments.
[00:12:28.000 --> 00:12:36.000]   Using the same table paradigm that we talked about, you can very easily compare models just by toggling that model on or off.
[00:12:36.000 --> 00:12:40.000]   Right, so here I'm going to toggle on this sand colored model.
[00:12:40.000 --> 00:12:49.000]   And you'll see that the table automatically updates and has the same distributions and images and all of this stuff that we've seen in the previous table,
[00:12:49.000 --> 00:13:01.000]   but now across different models as well. So again, lots of tools to help us rapidly iterate in our model dev.
[00:13:01.000 --> 00:13:06.000]   Cool. The last thing I'll talk about is asking questions about training.
[00:13:06.000 --> 00:13:13.000]   So often, you're building this model and you have lots of different experiments that you want to try out.
[00:13:13.000 --> 00:13:20.000]   Maybe you want to vary the type of backbone that you're using or some other hyperparameter that you care about.
[00:13:20.000 --> 00:13:30.000]   We have a tool called Sweeps that lets you quickly and easily search across the hyperparameters and then learn stuff about your training process on the fly.
[00:13:30.000 --> 00:13:33.000]   So let's take a look at that.
[00:13:33.000 --> 00:13:38.000]   Here, you can see that I've captured different runs that I did.
[00:13:38.000 --> 00:13:44.000]   These are just different hyperparameter combinations, and I've logged them all to this workspace.
[00:13:44.000 --> 00:13:49.000]   The thing I want to focus on here is this parallel coordinates plot.
[00:13:49.000 --> 00:13:57.000]   So what you can do in the interface is subselect just a portion of the runs that had the highest accuracies,
[00:13:57.000 --> 00:14:01.000]   and this will cross filter the rest of the charts in the workspace.
[00:14:01.000 --> 00:14:06.000]   So you might notice that some of these eyes have closed and some of the charts over here have changed a little bit.
[00:14:06.000 --> 00:14:10.000]   What I want to focus on here is you'll notice that with the parallel coordinates plot,
[00:14:10.000 --> 00:14:16.000]   it looks like the values with the highest accuracy had the lower learning rates.
[00:14:16.000 --> 00:14:20.000]   And so using these tools, I can very quickly learn, hey, it looks like in the future,
[00:14:20.000 --> 00:14:27.000]   if I want to have better models in terms of accuracy, at least, I might want to have lower learning rates.
[00:14:27.000 --> 00:14:34.000]   So that's some insight that I can glean from this model or this model training process.
[00:14:34.000 --> 00:14:37.000]   I can also scroll down a little bit to see the run comparison.
[00:14:37.000 --> 00:14:43.000]   And in this case, we're looking at for the set of runs that had high accuracy,
[00:14:43.000 --> 00:14:47.000]   what parameters were most important to the accuracy?
[00:14:47.000 --> 00:14:51.000]   And of course, I can choose whatever metric I'm trying to learn about,
[00:14:51.000 --> 00:14:59.000]   and I can see the different inputs and what their effects were and their correlation to those results.
[00:14:59.000 --> 00:15:05.000]   It's a really interesting stuff that we can learn from the sweep.
[00:15:05.000 --> 00:15:13.000]   The sweep also has some really nice things built in that let you search across multiple machines.
[00:15:13.000 --> 00:15:20.000]   So basically, you give us a YAML file describing what search space you want to look over and what you're trying to optimize.
[00:15:20.000 --> 00:15:34.000]   And then we will put that onto our central machine, and then you can distribute that work across any number of machines to get those runs moving a lot faster.
[00:15:34.000 --> 00:15:39.000]   Cool. So hopefully that gives us a sense of how to rapidly iterate.
[00:15:39.000 --> 00:15:43.000]   Next, let's talk about reproducing.
[00:15:43.000 --> 00:15:49.000]   So you probably have an artifact or a model pipeline that looks something like this.
[00:15:49.000 --> 00:15:52.000]   So we have some raw data. It goes through some transforms.
[00:15:52.000 --> 00:15:57.000]   Ultimately, you have a model, and it's making some predictions.
[00:15:57.000 --> 00:16:01.000]   How many times have you gotten a bad prediction?
[00:16:01.000 --> 00:16:05.000]   It looks like this dog is a paper towel. That's not good.
[00:16:05.000 --> 00:16:11.000]   And how many times have you just scratched your head figuring out just what exactly went wrong here?
[00:16:11.000 --> 00:16:15.000]   Is it the training script that's broken? Maybe the pre-trained models?
[00:16:15.000 --> 00:16:20.000]   Maybe the pre-processing script is messed up? Maybe someone changed the raw data?
[00:16:20.000 --> 00:16:24.000]   Maybe it's even more insidious, and actually it's a multi-processing bug.
[00:16:24.000 --> 00:16:32.000]   Again, the more pain-tolerant among us might say, "This is fine. This is part of the ML process."
[00:16:32.000 --> 00:16:35.000]   But again, I say there's got to be a better way.
[00:16:35.000 --> 00:16:41.000]   And so the question basically boils down to, how do you debug a model pipeline?
[00:16:41.000 --> 00:16:43.000]   And so there are a couple ways to do this.
[00:16:43.000 --> 00:16:47.000]   One, you could check the code. So we have tools for this already.
[00:16:47.000 --> 00:16:55.000]   You can get diff and then see what lines of code changed and did those things cause the bad downstream effects.
[00:16:55.000 --> 00:16:57.000]   That's one way.
[00:16:57.000 --> 00:17:01.000]   In Weights & Biases, you can actually go a little bit further.
[00:17:01.000 --> 00:17:04.000]   So here is that DAG that we were talking about.
[00:17:04.000 --> 00:17:07.000]   The squares here represent the bits of code.
[00:17:07.000 --> 00:17:10.000]   So I can click onto a square.
[00:17:10.000 --> 00:17:13.000]   In this case, this represents one of the training routes.
[00:17:13.000 --> 00:17:18.000]   And I'll see the code as normal. Of course, here's the code that generated this.
[00:17:18.000 --> 00:17:22.000]   But I'll also see some other metadata that might be helpful in debugging this.
[00:17:22.000 --> 00:17:26.000]   Things like, when was it run? How long did it take?
[00:17:26.000 --> 00:17:31.000]   What system? OS? Different things about the environment.
[00:17:31.000 --> 00:17:35.000]   What was the actual command that was used to kick off this job?
[00:17:35.000 --> 00:17:41.000]   And then hardware metrics. Things like how many CPUs you had, what GPUs you were running,
[00:17:41.000 --> 00:17:46.000]   the specific types of GPUs, even the versions that you're working with.
[00:17:46.000 --> 00:17:51.000]   If you scroll down a little bit further, we'll keep track of all sorts of the inputs and outputs,
[00:17:51.000 --> 00:17:56.000]   including the actual artifacts that came in and out, and the configs,
[00:17:56.000 --> 00:18:03.000]   what you might think of as input scalers or output scalers.
[00:18:03.000 --> 00:18:06.000]   We'll also keep track of things beyond the code.
[00:18:06.000 --> 00:18:12.000]   So in a more classic Python file diffing, you would see the diff of the files.
[00:18:12.000 --> 00:18:16.000]   But it's often difficult to diff notebooks, right?
[00:18:16.000 --> 00:18:23.000]   Because the notebook might be run out of order, and we're not exactly sure where or how to reproduce the notebook.
[00:18:23.000 --> 00:18:27.000]   We do some nice stuff that basically captures the cell execution order of the notebook.
[00:18:27.000 --> 00:18:32.000]   And so when we save the code, if you're running from a notebook, we'll save --
[00:18:32.000 --> 00:18:35.000]   excuse me -- we'll save that execution order.
[00:18:35.000 --> 00:18:39.000]   And so you'll always be able to reproduce what you're working on.
[00:18:39.000 --> 00:18:43.000]   And of course, the bane of my existence, I always forget this.
[00:18:43.000 --> 00:18:45.000]   I'm very glad that it happens automatically.
[00:18:45.000 --> 00:18:49.000]   If you're running like a pyenv, it will capture the requirements.txt.
[00:18:49.000 --> 00:18:53.000]   And if you're using something like Conda, it will also capture the env.yaml.
[00:18:53.000 --> 00:18:56.000]   So again, the idea is to give you full reproducibility.
[00:18:56.000 --> 00:19:03.000]   Never again will you build this awesome model and then not be able to reproduce it.
[00:19:03.000 --> 00:19:06.000]   So that's checking the code, right?
[00:19:06.000 --> 00:19:08.000]   But there's more elements to it.
[00:19:08.000 --> 00:19:11.000]   What about the input and output artifacts?
[00:19:11.000 --> 00:19:12.000]   Right?
[00:19:12.000 --> 00:19:15.000]   So let's start with the inputs.
[00:19:15.000 --> 00:19:19.000]   So one thing you can do is like literally look at how do the inputs change, right?
[00:19:19.000 --> 00:19:22.000]   So if it's a raw data, we might have a folder of images.
[00:19:22.000 --> 00:19:25.000]   And so what changed about those images?
[00:19:25.000 --> 00:19:28.000]   What literal files got added or subtracted?
[00:19:28.000 --> 00:19:33.000]   And did the annotations.json change give us the change there?
[00:19:33.000 --> 00:19:34.000]   Right?
[00:19:34.000 --> 00:19:40.000]   So we can see here that in version 1.0.3, I screwed up changing the bounding box definitions.
[00:19:40.000 --> 00:19:45.000]   If we roll back, it looks like Ben did a good job, and that was the last working version.
[00:19:45.000 --> 00:19:47.000]   Cool.
[00:19:47.000 --> 00:19:50.000]   In the UI, you can get a flavor for this as well.
[00:19:50.000 --> 00:19:57.000]   So instead of clicking on the squares, we can click on the circles, which represent the actual artifacts themselves.
[00:19:57.000 --> 00:20:02.000]   We can click on this to see more about the artifacts that we're looking at.
[00:20:02.000 --> 00:20:03.000]   Right?
[00:20:03.000 --> 00:20:11.000]   So here I can see things like what version is the artifact, what does the digest look like when it was created, any sort of tags or aliases.
[00:20:11.000 --> 00:20:19.000]   And more importantly, what runs generated this artifact and what runs consume this artifact.
[00:20:19.000 --> 00:20:20.000]   Right?
[00:20:20.000 --> 00:20:26.000]   This is super helpful, especially if you're in a super regulated or compliance-heavy industry.
[00:20:26.000 --> 00:20:37.000]   Maybe you're subject to HIPAA compliance or some other GDPR type of requirement, and you accidentally leak data into the pipeline.
[00:20:37.000 --> 00:20:45.000]   Well, with the artifact system and with this DAG, you'll be able to backtrack to the place where you accidentally injected that data,
[00:20:45.000 --> 00:20:53.000]   and then roll everything back to use the clean, or at least better, version of the dataset that didn't have the offending files.
[00:20:53.000 --> 00:20:54.000]   Right?
[00:20:54.000 --> 00:20:56.000]   So really nice stuff here.
[00:20:56.000 --> 00:21:02.000]   We also have some built-in versioning that lets you check out the actual underlying stuff that's inside.
[00:21:02.000 --> 00:21:07.000]   So here you can see this is a folder of data that we logged, and we're capturing things like the different images.
[00:21:07.000 --> 00:21:11.000]   So these are different images of plants, in addition to the metadata.
[00:21:11.000 --> 00:21:12.000]   Right?
[00:21:12.000 --> 00:21:17.000]   So things like what are the labels and what was the split, and then also the IDs here.
[00:21:17.000 --> 00:21:18.000]   Right?
[00:21:18.000 --> 00:21:26.000]   So lots of stuff that you can capture happens automatically when you're logging with artifacts.
[00:21:26.000 --> 00:21:27.000]   Cool.
[00:21:27.000 --> 00:21:29.000]   And then lastly, you can check the outputs.
[00:21:29.000 --> 00:21:30.000]   Right?
[00:21:30.000 --> 00:21:37.000]   So we kind of showed this already, but you can take a look at the outputs and see how good or bad the prediction really was.
[00:21:37.000 --> 00:21:38.000]   Right?
[00:21:38.000 --> 00:21:40.000]   So this is the same filter that we had before.
[00:21:40.000 --> 00:21:41.000]   Right?
[00:21:41.000 --> 00:21:43.000]   So these are the things that the model got wrong.
[00:21:43.000 --> 00:21:49.000]   So the model thinks that these are plants, and here are the actual ground truth distributions.
[00:21:49.000 --> 00:21:55.000]   But if we look at the images, we can see that actually, you know, I might forgive the model in these cases.
[00:21:55.000 --> 00:21:59.000]   You know, I would say there's a lot of plant in this image and in this image here.
[00:21:59.000 --> 00:22:03.000]   Whereas, you know, these probably don't really look like mollusks to me.
[00:22:03.000 --> 00:22:04.000]   Okay?
[00:22:04.000 --> 00:22:14.000]   So we can actually triage by looking at the underlying images and seeing, you know, where is the model paying attention, and are there edge cases that we haven't accounted for in the data itself.
[00:22:14.000 --> 00:22:15.000]   Right?
[00:22:15.000 --> 00:22:23.000]   So really awesome tools, helps us with reproducibility, and just understanding, you know, what exactly is the model doing?
[00:22:23.000 --> 00:22:25.000]   Cool.
[00:22:25.000 --> 00:22:28.000]   So the last bit is on collaboration.
[00:22:28.000 --> 00:22:33.000]   And I think this is super important, especially as teams grow.
[00:22:33.000 --> 00:22:39.000]   So we have all of these different enclaves of experiments, and we managed to put them into one place.
[00:22:39.000 --> 00:22:42.000]   Awesome.
[00:22:42.000 --> 00:22:50.000]   The benefit of this, I think one of the really interesting benefits, is that practitioners often don't agree on what metrics are important.
[00:22:50.000 --> 00:22:51.000]   Right?
[00:22:51.000 --> 00:22:54.000]   I might have my pet metric that I think is super important.
[00:22:54.000 --> 00:22:57.000]   Gorob might have his own metric that he thinks is really important.
[00:22:57.000 --> 00:23:03.000]   And sometimes, you know, I'll run a cool experiment, and he's looking for a particular metric, and I just don't have it.
[00:23:03.000 --> 00:23:04.000]   Right?
[00:23:04.000 --> 00:23:14.000]   So instead of, you know, being sassy, and, you know, saying some fun stuff on Slack, Gorob could actually just go into the interface and select the metrics that he cares about.
[00:23:14.000 --> 00:23:15.000]   He doesn't have to ask me.
[00:23:15.000 --> 00:23:19.000]   He doesn't have to wait for me to generate a new notebook with the results.
[00:23:19.000 --> 00:23:26.000]   He can do everything in place because all of the data is saved in this central location.
[00:23:26.000 --> 00:23:27.000]   Okay?
[00:23:27.000 --> 00:23:34.000]   So I want to show you other types of things that you can build with the data once it's stored in this central spot.
[00:23:34.000 --> 00:23:39.000]   And so this is what we call reports.
[00:23:39.000 --> 00:23:43.000]   So first off, I want to show you that you can stay super organized with reports.
[00:23:43.000 --> 00:23:44.000]   Right?
[00:23:44.000 --> 00:23:52.000]   So typically, with projects, there's often like a mishmash of different files and notebooks and experiments and all this stuff in different places linking back and forth.
[00:23:52.000 --> 00:23:54.000]   It can be kind of a headache.
[00:23:54.000 --> 00:23:58.000]   And so one thing that you can do to stay organized is create a landing page.
[00:23:58.000 --> 00:23:59.000]   Right?
[00:23:59.000 --> 00:24:04.000]   At the high level, we have metrics that everybody in that project would care about.
[00:24:04.000 --> 00:24:08.000]   Everyone from the practitioners, the PMs, to your business stakeholders.
[00:24:08.000 --> 00:24:14.000]   And then for the more MLE type of stuff, you can go further down into the supplementary notes.
[00:24:14.000 --> 00:24:17.000]   And this is where you can get further into the weeds.
[00:24:17.000 --> 00:24:18.000]   Okay?
[00:24:18.000 --> 00:24:20.000]   No more linking out to, like, random files.
[00:24:20.000 --> 00:24:24.000]   You can just have everything organized in one central location.
[00:24:24.000 --> 00:24:26.000]   Awesome.
[00:24:26.000 --> 00:24:28.000]   Next, we can click into some of these results.
[00:24:28.000 --> 00:24:31.000]   Let's click into the high level results to take a look.
[00:24:31.000 --> 00:24:34.000]   So first, how about this actual report?
[00:24:34.000 --> 00:24:36.000]   So here's, like, a research report-looking thing.
[00:24:36.000 --> 00:24:37.000]   Okay?
[00:24:37.000 --> 00:24:40.000]   And so you can generate something that looks like -- kind of like an archive paper.
[00:24:40.000 --> 00:24:41.000]   Right?
[00:24:41.000 --> 00:24:44.000]   So here we have our markdown and LaTeX.
[00:24:44.000 --> 00:24:46.000]   We can see the results that we were logging.
[00:24:46.000 --> 00:24:52.000]   In this case, this report is about an RL agent learning to play an RTS game.
[00:24:52.000 --> 00:24:55.000]   We can see the results and our commentary.
[00:24:55.000 --> 00:24:57.000]   If I have any feedback, I can also leave comments.
[00:24:57.000 --> 00:25:03.000]   So here I might add a comment and say, like, hey, this is really cool.
[00:25:03.000 --> 00:25:05.000]   And I can tag whoever I want.
[00:25:05.000 --> 00:25:07.000]   Maybe I want to say Costa.
[00:25:07.000 --> 00:25:08.000]   Right?
[00:25:08.000 --> 00:25:12.000]   And this lets everyone stay focused and organized in one place.
[00:25:12.000 --> 00:25:13.000]   So, again, really convenient.
[00:25:13.000 --> 00:25:15.000]   No more Slack messages back and forth.
[00:25:15.000 --> 00:25:21.000]   In fact, some of our customers are actually just using the report in lieu of an update.
[00:25:21.000 --> 00:25:22.000]   Right?
[00:25:22.000 --> 00:25:27.000]   They just kind of leave a message inside the report, and people can see it as the pings come up.
[00:25:27.000 --> 00:25:31.000]   So really cool stuff here.
[00:25:31.000 --> 00:25:33.000]   Okay.
[00:25:33.000 --> 00:25:36.000]   Another view that you could take is something that looks more like a dashboard.
[00:25:36.000 --> 00:25:37.000]   Right?
[00:25:37.000 --> 00:25:40.000]   So a lot of teams have their own kind of custom dashboarding stuff.
[00:25:40.000 --> 00:25:43.000]   Maybe you're using Tableau or something like that.
[00:25:43.000 --> 00:25:47.000]   You can also create dashboards in the report itself.
[00:25:47.000 --> 00:25:48.000]   Right?
[00:25:48.000 --> 00:25:52.000]   So here's the dashboard showing how we have some progress that's going on.
[00:25:52.000 --> 00:25:57.000]   And, you know, if you work with some, let's say, non-technical stakeholders, often they care about a few things.
[00:25:57.000 --> 00:25:58.000]   Right?
[00:25:58.000 --> 00:26:05.000]   So my technical -- or my non-technical stakeholders really love, you know, really big numbers and charts that go up and to the right.
[00:26:05.000 --> 00:26:06.000]   Right?
[00:26:06.000 --> 00:26:13.000]   And so with these reports, you can collaborate with them and show, hey, it looks like we're actually making good progress.
[00:26:13.000 --> 00:26:14.000]   Check out these returns.
[00:26:14.000 --> 00:26:15.000]   They're improving over time.
[00:26:15.000 --> 00:26:20.000]   And, by the way, here are the high-level metrics that you can report back to the business about.
[00:26:20.000 --> 00:26:21.000]   Right?
[00:26:21.000 --> 00:26:22.000]   Some really interesting stuff.
[00:26:22.000 --> 00:26:24.000]   And, of course, if you want to go deeper into the weeds, you can.
[00:26:24.000 --> 00:26:31.000]   We can scroll further down and see more about the experiments that we were running.
[00:26:31.000 --> 00:26:33.000]   Okay.
[00:26:33.000 --> 00:26:35.000]   So this is really just the tip of the iceberg.
[00:26:35.000 --> 00:26:42.000]   There are tons of other really cool reports that I'd love to share but just probably don't have time for today.
[00:26:42.000 --> 00:26:48.000]   If you want to see the full set of reports that we have posted, check out wanby.ai/fullyconnected.
[00:26:48.000 --> 00:26:52.000]   We have a bunch of reports from different people around the community showing what's possible.
[00:26:52.000 --> 00:26:55.000]   I'll show you just, you know, one or two things here.
[00:26:55.000 --> 00:27:00.000]   Here you can see this kind of block nerf model and its applications.
[00:27:00.000 --> 00:27:04.000]   Really cool.
[00:27:04.000 --> 00:27:05.000]   Right?
[00:27:05.000 --> 00:27:10.000]   And so if you take a visit to the site, you'll be able to see what other folks are working on.
[00:27:10.000 --> 00:27:15.000]   I think this wordle reinforcement learning thing was pretty cool as well.
[00:27:15.000 --> 00:27:16.000]   All right.
[00:27:16.000 --> 00:27:18.000]   So that's it for me.
[00:27:18.000 --> 00:27:22.000]   Hope you enjoyed the presentation and learned something.
[00:27:22.000 --> 00:27:24.000]   Please check out the community.
[00:27:24.000 --> 00:27:28.000]   You can visit the community forum at wanby.me/andyou.
[00:27:28.000 --> 00:27:30.000]   And, you know, here are all the socials.
[00:27:30.000 --> 00:27:35.000]   And, of course, if you want to connect with me, I'm Andrew@wanby.com.
[00:27:35.000 --> 00:27:36.000]   Cool.
[00:27:36.000 --> 00:27:37.000]   See you guys.
[00:27:37.000 --> 00:27:47.000]   [BLANK_AUDIO]

