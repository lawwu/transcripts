
[00:00:00.000 --> 00:00:05.200]   Hey everyone, Ivan here, and in this video we'll be talking about data collection and labeling.
[00:00:05.200 --> 00:00:14.160]   This video covers everything you need to know about preparing a dataset
[00:00:14.160 --> 00:00:20.480]   to train a custom YOLOv5 model. We'll actually start by asking if object detection is right
[00:00:20.480 --> 00:00:26.000]   for the problem you're working on. If it is, we'll move on to collecting training images,
[00:00:26.000 --> 00:00:30.480]   labeling tools, and finally using weights and biases to store a dataset in the cloud
[00:00:30.480 --> 00:00:34.480]   before downloading it when we need to start training on a virtual machine.
[00:00:34.480 --> 00:00:42.960]   You're watching part 2 of the YOLOv5 series. Watch part 0 to learn more about the YOLOv5
[00:00:42.960 --> 00:00:48.320]   and weights and biases integration. And watch part 1 to learn how to install YOLOv5
[00:00:48.320 --> 00:00:51.920]   for real-time object detection on Windows and Google Coop.
[00:00:54.080 --> 00:00:58.560]   If you have any questions, feel free to drop them in the comment section down below,
[00:00:58.560 --> 00:01:00.000]   and let's get started.
[00:01:00.000 --> 00:01:06.480]   Everything starts with determining if object detection is the right approach for your problem
[00:01:06.480 --> 00:01:13.600]   in the first place. You see, object detection is powerful, but has its limitations. By definition,
[00:01:13.600 --> 00:01:18.480]   object detection is all about detecting whether a given object is present in the image and drawing
[00:01:18.480 --> 00:01:24.400]   a rectangular bounding box around it. Instead of just saying that an image has a dog in it,
[00:01:24.400 --> 00:01:27.760]   we'll say that the image has a dog present at the following coordinates.
[00:01:27.760 --> 00:01:33.200]   So, if you're looking for a level of precision beyond the rectangular box,
[00:01:33.200 --> 00:01:38.720]   you may want to use other approaches. For example, image segmentation aims to assign a label to
[00:01:38.720 --> 00:01:44.320]   every pixel in the image. If you care about the orientation of an object in space,
[00:01:44.320 --> 00:01:49.120]   you may want to look into 3D object detection, since normal object detection gives you a flat,
[00:01:49.120 --> 00:01:51.280]   two-dimensional bounding box.
[00:01:51.280 --> 00:01:57.680]   You might find that your problem has multiple parts. In this case, you can combine object
[00:01:57.680 --> 00:02:03.120]   detection with other techniques. For example, you could break a 3D orientation estimation
[00:02:03.120 --> 00:02:09.280]   problem into smaller parts. In one of the projects my friend Carlo Laplers was a part of,
[00:02:09.280 --> 00:02:14.080]   they isolated a stuffed animal inside a bounding box, grabbed the results,
[00:02:14.080 --> 00:02:17.520]   and then passed it through another model to estimate its rotation.
[00:02:17.520 --> 00:02:24.720]   One of my projects was building an app for visually impaired people to detect buses
[00:02:24.720 --> 00:02:30.640]   and identify their route number. Object detection was a perfect fit for a part of this task because
[00:02:30.640 --> 00:02:35.200]   I cared about the location of the buses and the bus numbers so that I could attribute the number
[00:02:35.200 --> 00:02:42.560]   to the bus that it's on. However, while object detection is perfect for detecting bus numbers,
[00:02:42.560 --> 00:02:48.880]   it's not so perfect for reading them. So, I used object detection just for what it does best,
[00:02:48.880 --> 00:02:54.560]   detecting the bus numbers. Then I cropped the images and used another model to read the cropped
[00:02:54.560 --> 00:03:02.880]   numbers. If you've determined that object detection will be useful and relevant for the problem
[00:03:02.880 --> 00:03:07.440]   you're trying to solve, now we'll talk about the next step, which is data.
[00:03:07.440 --> 00:03:13.440]   Training an object detector is a supervised learning problem, meaning that we need the
[00:03:13.440 --> 00:03:19.200]   examples and the correct answers to those examples. One very important question for
[00:03:19.200 --> 00:03:23.840]   almost all supervised machine learning problems is "how are you gonna get the data?"
[00:03:23.840 --> 00:03:29.680]   You might already have a dataset in mind. In that case, then you might be interested
[00:03:29.680 --> 00:03:34.960]   in supplementing your existing dataset. The fundamental rule of ML still applies here,
[00:03:34.960 --> 00:03:41.440]   the more quality data, the better. If you don't have a dataset, you could create one.
[00:03:41.440 --> 00:03:46.160]   If you're creating your own dataset, you want to think about where the model will be deployed.
[00:03:46.160 --> 00:03:52.160]   For example, since I was working on an app that would be used as a bus stop to detect buses,
[00:03:52.160 --> 00:03:58.720]   I filmed buses approaching bus stops. I split those videos into frames that became part of
[00:03:58.720 --> 00:04:05.200]   my dataset. Another thing you can do is to look at public datasets and dataset platforms.
[00:04:05.200 --> 00:04:11.360]   Cocoa, or Common Objects in Context, is a very popular object detection segmentation dataset
[00:04:11.360 --> 00:04:15.840]   with over 120,000 images and 80 different classes.
[00:04:15.840 --> 00:04:20.560]   Here's a link to my video on the YOLO format for Cocoa dataset.
[00:04:24.160 --> 00:04:29.040]   You can also look at Kaggle, RoboFlow, and other platforms hosting public datasets.
[00:04:29.040 --> 00:04:33.840]   Try running a quick Google search of a type of dataset you're looking for,
[00:04:33.840 --> 00:04:39.600]   plus YOLO format. One big benefit of such datasets is that you can save a lot of time
[00:04:39.600 --> 00:04:41.920]   by not having to label all the images yourself.
[00:04:41.920 --> 00:04:47.840]   A fourth way of collecting data is using the free images on the internet.
[00:04:47.840 --> 00:04:52.480]   There are many tools that can help you automate the process of downloading these images,
[00:04:53.040 --> 00:04:56.000]   just make sure that you're allowed to use the images you're downloading.
[00:04:56.000 --> 00:05:05.680]   I want to emphasize the importance of having true negatives in your data.
[00:05:05.680 --> 00:05:10.960]   In the case of object detection, it means having the images without the objects you're looking for.
[00:05:10.960 --> 00:05:17.280]   For example, I added images with no buses into my dataset so that the model could learn
[00:05:17.280 --> 00:05:24.160]   when not to detect buses. I also added images of trucks, because they might look similar to buses.
[00:05:24.160 --> 00:05:30.160]   That way the model learns what's not a bus, and we can avoid potential misdetections in the future.
[00:05:30.160 --> 00:05:36.640]   The easiest way for me to get many images of trucks was also Cocoa dataset.
[00:05:36.640 --> 00:05:44.800]   In my case, I ended up with 7,000 bus images, which allowed the model to produce good results.
[00:05:45.840 --> 00:05:49.920]   However, you don't necessarily need that many images to start training,
[00:05:49.920 --> 00:05:52.560]   a good number to start with can be just a thousand.
[00:05:52.560 --> 00:05:59.600]   YOLOv5 also has some data augmentation built in, which could help with smaller datasets.
[00:05:59.600 --> 00:06:05.280]   If the results look promising, you can continue adding more images while tracking the accuracies
[00:06:05.280 --> 00:06:10.480]   of your models. We'll use weights and biases for that in part 3 of this series.
[00:06:13.840 --> 00:06:16.960]   Now that we have our images, how do we label them?
[00:06:16.960 --> 00:06:24.720]   Different object detectors have different label formats. YOLOv5 and many other YOLO-type networks
[00:06:24.720 --> 00:06:28.640]   use two files with the same name, but a different file extension.
[00:06:28.640 --> 00:06:34.240]   One file is a JPEG image file, and the other is a .txt text file.
[00:06:34.240 --> 00:06:39.280]   The image is just a normal image, that's pretty simple.
[00:06:39.840 --> 00:06:44.880]   The .txt file is used to store the labels, the types of objects present in the image,
[00:06:44.880 --> 00:06:47.120]   and the coordinates of their bounding boxes.
[00:06:47.120 --> 00:06:53.840]   The number of rows indicates the number of objects in the image.
[00:06:53.840 --> 00:07:01.200]   Each row has 5 parameters, the index of an object,
[00:07:01.200 --> 00:07:05.360]   the x and y coordinates of the bounding box center,
[00:07:05.360 --> 00:07:09.440]   and the width and height of the bounding box.
[00:07:10.000 --> 00:07:14.720]   The coordinates and bounding box dimensions are normalized to a value between 0 and 1
[00:07:14.720 --> 00:07:16.960]   as a percentage of the image dimensions.
[00:07:16.960 --> 00:07:24.160]   For example, I'll draw a very elongated bounding box at the bottom of the image.
[00:07:24.160 --> 00:07:30.400]   In the .txt file, we can see that the x and y coordinates of the bounding box center
[00:07:30.400 --> 00:07:33.760]   are about 0.5 and 0.93.
[00:07:35.600 --> 00:07:38.720]   These values are normalized to a percentage of the image dimensions.
[00:07:38.720 --> 00:07:44.960]   So, the x coordinate is located about 50% across the image, left to right,
[00:07:44.960 --> 00:07:49.680]   and the y coordinate is 93% down the image, top to bottom.
[00:07:49.680 --> 00:07:56.480]   The width of the bounding box is 97% of the entire image width,
[00:07:56.480 --> 00:08:00.960]   and the height of the bounding box is 10% of the entire image height.
[00:08:02.400 --> 00:08:08.480]   Obviously, we can't just go count every pixel and type out this information by hand.
[00:08:08.480 --> 00:08:10.960]   We're gonna need to run some code.
[00:08:10.960 --> 00:08:16.320]   Labeling software really comes in all the different sizes and flavors.
[00:08:16.320 --> 00:08:20.400]   When I asked people on Twitter what do they use to label images,
[00:08:20.400 --> 00:08:22.400]   I received the following recommendations.
[00:08:26.800 --> 00:08:32.480]   My favorite labeling tool is an open-source repo called OpenLabeling by Joao Cartucho.
[00:08:32.480 --> 00:08:35.920]   It's powered by OpenCV, which I'm very familiar with,
[00:08:35.920 --> 00:08:39.680]   so it was easy to add extra features that made it perfect for my needs.
[00:08:39.680 --> 00:08:45.760]   I'll show the labeling process using a modified version of OpenLabeling,
[00:08:45.760 --> 00:08:49.520]   but feel free to use any labeling tool that supports the YOLOv5 format.
[00:08:49.520 --> 00:08:54.640]   I'll link to OpenLabeling and the modified version in the description.
[00:08:56.240 --> 00:09:00.720]   First thing we need to do, as usual, is to download and unzip the labeling repo.
[00:09:00.720 --> 00:09:09.600]   We'll put the images we want to label into the images folder.
[00:09:09.600 --> 00:09:14.480]   According to the YOLO format,
[00:09:14.480 --> 00:09:19.200]   that TXT file with the same file names will be created in the bbox.txt folder.
[00:09:19.200 --> 00:09:24.400]   We need to make sure that we have these Python modules installed
[00:09:24.400 --> 00:09:26.800]   in order to run modified OpenLabeling.
[00:09:26.800 --> 00:09:36.000]   We need to run the run.py file to launch the labeling tool.
[00:09:36.000 --> 00:09:40.880]   I'll open it as a Python file and then press F5 to run the code.
[00:09:40.880 --> 00:09:46.240]   A window that we can resize or make fullscreen will pop up and display the image.
[00:09:51.040 --> 00:09:54.640]   There's a sliding bar at the top to switch the image we're currently on
[00:09:54.640 --> 00:09:57.360]   and the class we're currently drawing a bounding box for.
[00:09:57.360 --> 00:10:03.120]   We can use the A and D keys to switch between images
[00:10:03.120 --> 00:10:06.480]   and the W and S keys to switch between classes.
[00:10:06.480 --> 00:10:17.040]   The first left mouse click defines the top left corner of a bounding box.
[00:10:17.040 --> 00:10:20.320]   And the second click defines the bottom right corner.
[00:10:21.280 --> 00:10:23.760]   A right click inside a bounding box deletes it.
[00:10:23.760 --> 00:10:30.800]   We can specify the number of classes and their names in the classlist.txt file.
[00:10:30.800 --> 00:10:38.080]   In my case, class closedoor has the index 0, opendoor1, bus2, and number3.
[00:10:38.080 --> 00:10:44.960]   You want the classes to be specific enough for the model to be able to generalize well,
[00:10:44.960 --> 00:10:48.160]   but not so specific that there isn't enough data to support it.
[00:10:48.880 --> 00:10:54.320]   Remember, the class busnumber is better than busnumber86 or something.
[00:10:54.320 --> 00:11:00.480]   All of the bounding boxes we draw get automatically added to the .txt file
[00:11:00.480 --> 00:11:01.920]   that corresponds to the image.
[00:11:01.920 --> 00:11:07.120]   Now I'll describe how I modified the Open Labeling tool.
[00:11:07.120 --> 00:11:12.240]   First, I added the ability to switch classes with the digit keys on the keyboard.
[00:11:12.240 --> 00:11:15.840]   This comes in handy when there are many classes.
[00:11:17.280 --> 00:11:20.880]   For example, I press 1 to switch to the class with the index 0,
[00:11:20.880 --> 00:11:24.320]   and I press 2 to switch to the class with the index 1, and so on.
[00:11:24.320 --> 00:11:30.400]   I also added the ability to move an image in its label
[00:11:30.400 --> 00:11:35.200]   directly to the local recycle bin inside of Open Labeling by pressing the R key.
[00:11:35.200 --> 00:11:42.320]   It helps when you encounter a confusing image that you don't want to include in your dataset,
[00:11:42.320 --> 00:11:44.800]   so you can quickly remove it and continue labeling.
[00:11:45.360 --> 00:11:48.560]   Labeling is a pretty tedious process,
[00:11:48.560 --> 00:11:52.560]   but it can also be quite rewarding when it helps your model perform better.
[00:11:52.560 --> 00:12:06.320]   Now let's learn about taking our labeled data and backing it up to the cloud.
[00:12:06.320 --> 00:12:09.120]   This way we can use it for training in the future.
[00:12:10.400 --> 00:12:13.840]   We want to use the data that we've just labeled to train in the cloud,
[00:12:13.840 --> 00:12:16.800]   on Google Callup or on whatever cloud platform you prefer.
[00:12:16.800 --> 00:12:20.640]   For that we'll use Weights & Biases Artifacts,
[00:12:20.640 --> 00:12:23.360]   which allows us to version our datasets and models.
[00:12:23.360 --> 00:12:27.600]   You can watch part 0 of the series to learn more about artifacts
[00:12:27.600 --> 00:12:30.800]   and the YOLOv5 and Weights & Biases integration.
[00:12:30.800 --> 00:12:36.480]   First, we'll run a script inside the modified Open Labeling tool.
[00:12:37.360 --> 00:12:41.360]   First, we'll run a script inside the modified Open Labeling folder
[00:12:41.360 --> 00:12:45.680]   in order to randomly split the data into the training and validation sets.
[00:12:45.680 --> 00:12:49.200]   We can specify the ratio of the split here
[00:12:49.200 --> 00:12:52.400]   and run the script to copy the images into the new folders.
[00:12:52.400 --> 00:12:56.320]   The script creates a custom dataset directory,
[00:12:56.320 --> 00:13:01.200]   and the images and labels for the training and validation sets are in the appropriate folders.
[00:13:01.200 --> 00:13:08.000]   The script creates a custom dataset directory, and the images and labels for the training and validation sets are in the appropriate folders.
[00:13:08.000 --> 00:13:13.040]   Next up, we can copy the custom dataset folder into the YOLOv5 folder,
[00:13:13.040 --> 00:13:16.080]   which we installed locally in the first part of the series.
[00:13:16.080 --> 00:13:21.120]   After that, we need a .yml file for our dataset.
[00:13:21.120 --> 00:13:27.920]   YOLOv5 uses this file to understand where to take the images and labels for training,
[00:13:27.920 --> 00:13:32.240]   what the names of the classes are, how many classes there are, and so on.
[00:13:32.240 --> 00:13:40.000]   Let's make a copy of the boc.yml file that came with the YOLOv5 repo,
[00:13:40.000 --> 00:13:43.360]   name it custom_dataset.yml, and edit it a little bit.
[00:13:43.360 --> 00:13:49.920]   Let's delete all the lines we don't need, and change the paths, number of classes, and class names.
[00:13:49.920 --> 00:13:52.320]   The result should look something like this.
[00:13:52.320 --> 00:13:57.440]   Now let's save it and upload our dataset as an artifact.
[00:13:58.400 --> 00:14:01.440]   We'll open the console inside the YOLOv5 folder,
[00:14:01.440 --> 00:14:06.640]   and run pip install 1db to install the Weights and Biases Python client.
[00:14:06.640 --> 00:14:18.080]   Next, we'll run this command to upload our dataset as an artifact into a WMB project named custom_yolov5.
[00:14:18.080 --> 00:14:22.080]   It may prompt you to log into Weights and Biases,
[00:14:22.080 --> 00:14:27.040]   or quickly create a new account if it's your first time using the 1db Python client.
[00:14:27.040 --> 00:14:39.280]   [Music]
[00:14:39.280 --> 00:14:44.640]   Notice that the custom_dataset1db.yml file appears in the YOLOv5 folder.
[00:14:44.640 --> 00:14:51.120]   We'll use this file in the future to start training on the dataset that we uploaded as a WMB artifact.
[00:14:51.120 --> 00:14:55.760]   We can follow this link to open our WMB project.
[00:14:55.840 --> 00:15:00.160]   [Music]
[00:15:00.160 --> 00:15:02.640]   The data is now stored in the cloud.
[00:15:02.640 --> 00:15:06.080]   We can access it no matter which machine we plan on using for training.
[00:15:06.080 --> 00:15:12.720]   If we navigate into the Artifacts tab, we'll see two new dataset artifacts appear,
[00:15:12.720 --> 00:15:15.840]   one for the training and one for the validation data.
[00:15:15.840 --> 00:15:23.840]   If we click on one of the artifacts and go into the Files tab, we can open it as a 1db table.
[00:15:25.600 --> 00:15:28.800]   1db tables are a way of interactively exploring data.
[00:15:28.800 --> 00:15:34.560]   We can visualize, query, and analyze tabular data right in the browser.
[00:15:34.560 --> 00:15:42.960]   Weights and Biases Artifacts is a tool for model and dataset versioning,
[00:15:42.960 --> 00:15:46.320]   aka keeping track of the changes to your models and datasets.
[00:15:46.320 --> 00:15:53.520]   I will now add a random bounding box to this image just by editing the labels.txt file.
[00:15:53.680 --> 00:15:58.000]   [Music]
[00:15:58.000 --> 00:16:00.880]   Then I'll upload the dataset as an artifact again.
[00:16:00.880 --> 00:16:09.920]   WMB Artifacts scans every file, creates a new version of the dataset
[00:16:09.920 --> 00:16:13.600]   if it detects any change in the file's name or contents.
[00:16:13.600 --> 00:16:21.200]   Here we can see the random bounding box.
[00:16:21.840 --> 00:16:24.000]   [Music]
[00:16:24.000 --> 00:16:28.880]   We can also compare different versions of datasets side by side and see the difference.
[00:16:28.880 --> 00:16:37.280]   You can see that the old version has 3 bounding boxes and that the new version has 4 of them.
[00:16:37.280 --> 00:16:43.280]   Most importantly, WMB Artifacts only saves the new or changed files
[00:16:43.280 --> 00:16:48.480]   and just references the unchanged ones, meaning that we're not wasting any storage.
[00:16:48.560 --> 00:16:52.880]   [Music]
[00:16:52.880 --> 00:16:58.000]   That's it for this video. Stay tuned for the next one where we'll cover training a YOLOv5 model
[00:16:58.000 --> 00:17:02.320]   while leveraging Weights and Biases Experiment Tracking tools that are full power.
[00:17:02.320 --> 00:17:08.800]   If you have any questions or comments, please feel free to leave them in the comment section
[00:17:08.800 --> 00:17:13.600]   down below and I'll be happy to answer them. And consider subscribing to our channel to
[00:17:13.600 --> 00:17:21.520]   see the upcoming parts of the series. And thank you for watching, I hope you enjoyed it and found it useful!

