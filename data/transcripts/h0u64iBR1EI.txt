
[00:00:00.000 --> 00:00:02.000]   We went double check real fast.
[00:00:02.000 --> 00:00:19.920]   They supposed to be this 30 second delay that you have to
[00:00:19.920 --> 00:00:23.800]   just pick out. Awesome. I can hear myself and we have a better
[00:00:23.800 --> 00:00:26.360]   host than me today. So I'll hand it over to Brian. Brian,
[00:00:26.360 --> 00:00:27.120]   please take it away.
[00:00:28.000 --> 00:00:32.520]   Welcome and good morning. As Siam has mentioned, I'm Brian
[00:00:32.520 --> 00:00:35.320]   Bischoff. I'm the head of data science at Weights and Biases.
[00:00:35.320 --> 00:00:38.120]   I'm also an adjunct lecturer in machine learning at Rutgers
[00:00:38.120 --> 00:00:42.800]   University. And today I'm really excited to bring to you two
[00:00:42.800 --> 00:00:50.920]   people that I admire for an AMA. Today is a little bit of a dual
[00:00:50.920 --> 00:00:57.520]   and also a dual AMA, which is to say that we have two YouTube
[00:00:57.520 --> 00:01:01.360]   personalities with us today, who are represented by two of my
[00:01:01.360 --> 00:01:05.280]   favorite beverages. First, I'd like to introduce Letizia
[00:01:05.280 --> 00:01:11.000]   Parkelebescu, who comes from physics and computer science and
[00:01:11.000 --> 00:01:14.160]   then is currently a PhD candidate at Heidelberg
[00:01:14.160 --> 00:01:17.840]   University. Her research focuses on vision and language
[00:01:17.840 --> 00:01:21.960]   integration in multimodal machine learning. Her hobby,
[00:01:21.960 --> 00:01:25.520]   however, revolves around the AI Coffee Break with Letizia
[00:01:25.640 --> 00:01:29.320]   YouTube channel, where the animated Miss Coffee Bean
[00:01:29.320 --> 00:01:32.800]   explains and visualizes concepts from the latest natural
[00:01:32.800 --> 00:01:36.040]   language processing, computer vision and multimodal research
[00:01:36.040 --> 00:01:42.040]   topics. Her channel regularly reaches over 35,000 viewers. And
[00:01:42.040 --> 00:01:45.400]   for those of you wondering, we did also try to book Miss Coffee
[00:01:45.400 --> 00:01:50.720]   Bean, but her agent told us her schedule was a bit too busy. I
[00:01:50.720 --> 00:01:53.800]   am really excited to have Letizia with us today. I have
[00:01:53.800 --> 00:01:58.440]   been watching Letizia for over a year. I watch every episode,
[00:01:58.440 --> 00:02:03.400]   usually almost within the first day. And so I'm really excited
[00:02:03.400 --> 00:02:09.240]   to get to talk to Letizia about ML and community. We also have
[00:02:09.240 --> 00:02:12.800]   another character today, who you might be a little bit more
[00:02:12.800 --> 00:02:17.840]   familiar with. And that's Siam Bhutani. Siam is the community
[00:02:17.840 --> 00:02:20.640]   lead at Weights and Biases where we work together as colleagues.
[00:02:21.000 --> 00:02:24.560]   He enjoys drinking chai and making content. He was the host
[00:02:24.560 --> 00:02:26.960]   of the Chai Time Data Science podcast, which was one of the
[00:02:26.960 --> 00:02:31.120]   top podcasts in the data science and Kaggle community in 2020.
[00:02:31.120 --> 00:02:35.240]   And he's been recognized as one of the top AI writers on Medium
[00:02:35.240 --> 00:02:38.560]   and Hacker Noon prior to that. He's reached an audience of over
[00:02:38.560 --> 00:02:43.160]   1.5 million people. He's also a Kaggle master and two time
[00:02:43.160 --> 00:02:46.800]   expert. Almost more importantly than any of that, he's a
[00:02:46.800 --> 00:02:50.000]   fantastic colleague and great friend, and someone who I love
[00:02:50.000 --> 00:02:54.200]   chatting about machine learning and community about every day.
[00:02:54.200 --> 00:02:57.280]   I have the privilege to work with him. And so I get to bug
[00:02:57.280 --> 00:03:00.120]   him with questions all the time. But today, I get to bug him
[00:03:00.120 --> 00:03:04.960]   with those questions in front of all of you. In general today,
[00:03:04.960 --> 00:03:07.000]   there's a few things that I'd like to mention before we get
[00:03:07.000 --> 00:03:11.120]   started. One, I'm going to be asking questions to both of our
[00:03:11.120 --> 00:03:14.520]   guests, some questions geared at one or the other, some questions
[00:03:14.520 --> 00:03:18.160]   geared at both. I also want to take a moment before we get
[00:03:18.160 --> 00:03:21.040]   started, to acknowledge that right now, things are quite
[00:03:21.040 --> 00:03:26.040]   complicated in the world. I think that it would really, I
[00:03:26.040 --> 00:03:30.320]   would really appreciate if all of you took a moment to reflect
[00:03:30.320 --> 00:03:38.120]   on what you can do to promote the inclusivity of our community,
[00:03:38.120 --> 00:03:42.160]   and think about how our community interacts with
[00:03:42.160 --> 00:03:47.680]   welcoming others into it. In particular, if you're not in a
[00:03:47.760 --> 00:03:52.680]   situation of duress, or insecurity at this moment, I'd
[00:03:52.680 --> 00:03:56.760]   really appreciate taking a moment to reflect on these
[00:03:56.760 --> 00:04:10.840]   opportunities. Thanks. I'd like to start us out with some
[00:04:10.840 --> 00:04:16.480]   relatively simple questions. So first, my question is for
[00:04:16.480 --> 00:04:21.520]   Laetitia. And that question is, what's your favorite thing
[00:04:21.520 --> 00:04:26.000]   about tea? What do you think that tea offers that is really
[00:04:26.000 --> 00:04:27.720]   just uniquely beautiful?
[00:04:27.720 --> 00:04:35.560]   Hmm. That's an easy, hard question. I think what I like
[00:04:35.560 --> 00:04:42.840]   most about tea is that it so if I were to compare tea to coffee,
[00:04:42.840 --> 00:04:46.000]   it's like coffee is more like a train hits you on the tongue and
[00:04:46.000 --> 00:04:53.520]   tea is more like caress on the tongue. It's more gentle. That's
[00:04:53.520 --> 00:04:55.520]   what I like about tea, the gentleness.
[00:04:55.520 --> 00:04:59.040]   I see. I see. And Sanyam, I have a question for you as well.
[00:04:59.040 --> 00:05:03.120]   Which is, what about the coffee community and the coffee world?
[00:05:03.120 --> 00:05:05.800]   Do you think is really incredible? And something
[00:05:05.800 --> 00:05:08.720]   that's really just, you know, something to be lauded and
[00:05:08.720 --> 00:05:09.400]   appreciated?
[00:05:09.400 --> 00:05:13.360]   First of all, thanks. Thanks for the wonderful intro. And I know
[00:05:13.360 --> 00:05:16.200]   I'm I'm the one who's always asking stupid questions to you.
[00:05:16.200 --> 00:05:20.000]   So thanks for always answering them. The wonderful thing is I
[00:05:20.000 --> 00:05:23.400]   get to convert so many people to chai from the coffee community.
[00:05:23.400 --> 00:05:26.840]   So that's, that's awesome. I have a big target audience that
[00:05:26.840 --> 00:05:27.560]   I can convert.
[00:05:27.560 --> 00:05:31.800]   I see. I see. Interesting. Interesting. Well, I'm clearly a
[00:05:31.800 --> 00:05:36.360]   lover of both. So you'll get no side taking over here. Um, all
[00:05:36.360 --> 00:05:41.000]   right. So next, I want to start with a question that I think we
[00:05:41.000 --> 00:05:44.280]   all care about, which is to say that, like, what is the ML
[00:05:44.280 --> 00:05:49.240]   community need more of? If you don't mind starting, Sanyam, what
[00:05:49.240 --> 00:05:52.360]   do you think? What would you love to see just cranked up to
[00:05:52.360 --> 00:05:54.800]   11 in the AI community and the machine learning community?
[00:05:54.800 --> 00:06:00.040]   Sure. I've obviously thought a lot about this just because of
[00:06:00.040 --> 00:06:04.560]   the nature of the work I do on YouTube. I wish like, sorry to
[00:06:04.560 --> 00:06:07.680]   speak on your behalf, Leticia. But I think reason you and I do
[00:06:07.680 --> 00:06:10.720]   what I do is because many research labs don't do what they
[00:06:10.720 --> 00:06:14.520]   supposed to do, which is, please give us the paper with a nice
[00:06:14.520 --> 00:06:17.440]   produced video that we can really easily understand. Like
[00:06:17.440 --> 00:06:21.960]   I'm not I'm not a researcher, I can't digest that mathy strong
[00:06:21.960 --> 00:06:25.000]   paper and understand it so easily as probably Leticia or
[00:06:25.000 --> 00:06:29.640]   Brian could, right. So just in a way, lowering the entry
[00:06:29.640 --> 00:06:34.440]   barriers just by making content more accessible to people. And
[00:06:34.440 --> 00:06:36.560]   that's, that's the reason why I'm personally so motivated
[00:06:36.560 --> 00:06:38.920]   about the things I do at Bits and Vices also.
[00:06:40.120 --> 00:06:42.360]   And how about you, Leticia? What do you think the ML community
[00:06:42.360 --> 00:06:43.040]   needs more of?
[00:06:43.040 --> 00:06:50.800]   I actually agree very much to Sanyam because it's the problem
[00:06:50.800 --> 00:06:54.840]   is that a lot of research is happening not really behind
[00:06:54.840 --> 00:06:58.480]   closed doors because you know, we people from academia, we go
[00:06:58.480 --> 00:07:02.800]   to conferences and, you know, explain our paper in front of an
[00:07:02.800 --> 00:07:07.720]   audience of six people, what we have researched about in the
[00:07:07.720 --> 00:07:12.200]   last year. But what I think we from academia, because we're
[00:07:12.200 --> 00:07:16.120]   funded by public money should do more is produce this kind of
[00:07:16.120 --> 00:07:20.600]   accessible content. So to make it a little more appealing, and,
[00:07:20.600 --> 00:07:23.720]   you know, when we make a presentation, maybe not use the
[00:07:23.720 --> 00:07:31.760]   standard latex blue template about for everything, but try to,
[00:07:31.760 --> 00:07:36.480]   you know, I would use the word sell here. But of course,
[00:07:36.520 --> 00:07:39.560]   everybody would, you know, from academia, jump onto my head and
[00:07:39.560 --> 00:07:42.960]   say, we're not selling anything, because we are just doing
[00:07:42.960 --> 00:07:47.640]   science. And we were convincing. We're not convincing people
[00:07:47.640 --> 00:07:52.800]   what we're just communicating what facts are. But I think it
[00:07:52.800 --> 00:07:58.240]   matters a lot, like what we say, but also how we say it, and to
[00:07:58.240 --> 00:08:01.280]   make it understandable and to make it accessible and not,
[00:08:01.280 --> 00:08:05.120]   like, explain it super complicated, because you give me
[00:08:05.120 --> 00:08:09.200]   all this money, and I am a super complicated and smart mind. So
[00:08:09.200 --> 00:08:12.000]   let's make it sound complicated. I don't think that's the way to
[00:08:12.000 --> 00:08:17.160]   do it. I think just produce nice, beautiful content and have
[00:08:17.160 --> 00:08:21.160]   more incentives for it, perhaps even, you know, when when you
[00:08:21.160 --> 00:08:25.040]   get a professorship, put it as a part of the job giving lectures,
[00:08:25.040 --> 00:08:30.400]   but also reaching out to through the internet to people.
[00:08:31.280 --> 00:08:35.360]   And what do you think the value is in sort of making it more
[00:08:35.360 --> 00:08:39.000]   accessible and having it more understandable by the public?
[00:08:39.000 --> 00:08:42.520]   What do you think the value is of that? Is it just it feels
[00:08:42.520 --> 00:08:43.640]   good? Or is there something more?
[00:08:43.640 --> 00:08:48.200]   On the one side, I mean, from in academia, we're funded by
[00:08:48.200 --> 00:08:51.720]   public money, and the public gets to know what they're paying
[00:08:51.720 --> 00:08:55.840]   for. I think that's already a win situation. Because if people
[00:08:55.840 --> 00:08:59.600]   are engaged and excited about what we do in academia, they
[00:08:59.600 --> 00:09:03.840]   will also be more happy in funding science and giving this
[00:09:03.840 --> 00:09:08.000]   these kinds of projects. But of course, I think it has a longer
[00:09:08.000 --> 00:09:10.600]   reach than just the excitement you get on the spot when you
[00:09:10.600 --> 00:09:16.120]   SQL science, it's people want to become scientists, the scientist
[00:09:16.120 --> 00:09:21.360]   job becomes perhaps more sexy because of it. And people learn
[00:09:21.360 --> 00:09:25.440]   about these things, get to think about these things, and then
[00:09:25.440 --> 00:09:28.640]   want to be a part of these things, or perhaps send their
[00:09:28.640 --> 00:09:31.760]   kids to be a part of these things. I think it's a very long
[00:09:31.760 --> 00:09:34.000]   time term investment. Yeah.
[00:09:34.000 --> 00:09:39.400]   Yeah. And Siam, you know, you're coming from the Kaggle side,
[00:09:39.400 --> 00:09:44.280]   where does Kaggle play a role in this sort of like, I would call
[00:09:44.280 --> 00:09:48.160]   it sort of like extension to people beyond the academy or
[00:09:48.160 --> 00:09:50.160]   just the absolute experts?
[00:09:50.160 --> 00:09:54.640]   Yeah, totally. And in my opinion, calculus are also
[00:09:54.640 --> 00:09:57.960]   experts. So I'm quite biased in that way. But I'll say that and
[00:09:58.200 --> 00:10:02.800]   to continue on Leticia's point first, if I may, I think it also
[00:10:02.800 --> 00:10:05.920]   lowers the barrier entry for everyone, right? Because
[00:10:05.920 --> 00:10:09.200]   everyone wants to use research at the end of the day, if
[00:10:09.200 --> 00:10:11.560]   something is cutting edge over the years, it'll bleed into
[00:10:11.560 --> 00:10:15.480]   consumer products, whatever that means. Right? I come from the
[00:10:15.480 --> 00:10:19.400]   fast AI community, which is open to everyone. And Jeremy Howard,
[00:10:19.400 --> 00:10:23.760]   the creator always quotes examples of, you know, farmers
[00:10:23.760 --> 00:10:28.000]   who are using fast AI to help their cattle in some way. And if
[00:10:28.000 --> 00:10:30.160]   they can understand this, and they can apply it, like what's
[00:10:30.160 --> 00:10:34.320]   what, how is it more realer, realer, if that's a word that
[00:10:34.320 --> 00:10:39.960]   on Kaggle, I think we've always seen frameworks pop up by
[00:10:39.960 --> 00:10:44.360]   torch, in my opinion, got more famous on Kaggle before we saw
[00:10:44.360 --> 00:10:49.560]   that shift in academia. Also, we saw Tim really become popular
[00:10:49.560 --> 00:10:53.280]   XGBoost become really popular on Kaggle, because Kaggle induces
[00:10:53.280 --> 00:10:57.560]   this really fast experimentation setup. So in that way, I think
[00:10:57.560 --> 00:11:01.720]   it also piggies back to research of, in a way of validating ideas
[00:11:01.720 --> 00:11:02.680]   on leaderboards.
[00:11:02.680 --> 00:11:07.880]   Yeah, and I think one thing that I find really interesting in all
[00:11:07.880 --> 00:11:12.120]   these is sort of what is gained by having more people have
[00:11:12.120 --> 00:11:15.720]   access. And so I think, I think about this question around sort
[00:11:15.720 --> 00:11:19.920]   of what is creating more accessibility lead to what are
[00:11:19.920 --> 00:11:22.800]   the outcomes that are sort of improved. And so I think it's
[00:11:22.800 --> 00:11:25.080]   really interesting to think about inspiring more people to
[00:11:25.080 --> 00:11:27.960]   go into science, and also, on the other hand, this sort of,
[00:11:27.960 --> 00:11:31.400]   like implementing it in their own applications for sort of
[00:11:31.400 --> 00:11:35.280]   their, you know, betterment. So I think those are really
[00:11:35.280 --> 00:11:40.720]   interesting directions. One thing I think, I wonder about
[00:11:40.720 --> 00:11:45.040]   sometimes is what makes ML different than other fields?
[00:11:45.040 --> 00:11:50.440]   What about public education for machine learning is different
[00:11:50.440 --> 00:11:54.520]   than public education for physics, or public education for
[00:11:54.520 --> 00:11:59.320]   economics? Or even more broadly, public education for just sort
[00:11:59.320 --> 00:12:03.640]   of foundational statistical ideas and, like logical
[00:12:03.640 --> 00:12:07.320]   thinking? What do you think is unique about ML education,
[00:12:07.320 --> 00:12:07.760]   Letizia?
[00:12:07.760 --> 00:12:14.520]   I think it's unique that it comes very late. I, I think I've
[00:12:14.520 --> 00:12:19.880]   already said this quite a lot of times, but in school, I could
[00:12:20.200 --> 00:12:24.920]   choose to do physics, I could do math, I could do German,
[00:12:24.920 --> 00:12:28.760]   Romanian, so languages, but I couldn't just choose to do ML.
[00:12:28.760 --> 00:12:32.520]   It's, I didn't even know about the existence of ML at that
[00:12:32.520 --> 00:12:36.840]   time, I found out about the existence of ML in when I was
[00:12:36.840 --> 00:12:42.640]   in, I think, fourth semester in university. And yeah, that it
[00:12:42.640 --> 00:12:46.800]   is a university course that people doing their master at
[00:12:46.800 --> 00:12:51.440]   their master's level or advanced bachelor's level, it's quite a
[00:12:51.440 --> 00:12:57.120]   little late. And we, we humans are really bad at statistics,
[00:12:57.120 --> 00:13:01.320]   we're really bad at estimating or thinking about large numbers,
[00:13:01.320 --> 00:13:04.400]   we think the difference between 1 million and 2 million is
[00:13:04.400 --> 00:13:09.520]   roughly the same as the difference between 20 and 100.
[00:13:09.520 --> 00:13:13.400]   It's, you know, big numbers are hard to think about, we have
[00:13:13.400 --> 00:13:21.560]   just 10, 10 fingers to usually count. And if we would have more
[00:13:21.560 --> 00:13:25.960]   education about statistics and simple ML, this, this very
[00:13:25.960 --> 00:13:29.120]   simple, you know, linear regression kind of ML, you're
[00:13:29.120 --> 00:13:34.520]   already in math in, or, you know, in computer science in 12
[00:13:34.520 --> 00:13:38.760]   grade, instead of just doing integrals and derivatives, I
[00:13:38.760 --> 00:13:42.560]   think that could come in handy, especially because everybody now
[00:13:42.560 --> 00:13:48.680]   has to touch ML or work with an ML product in in a way, if I
[00:13:48.680 --> 00:13:52.080]   write something on my smartphone, I get next word
[00:13:52.080 --> 00:13:57.680]   prediction, what is that. And this is why I really think we
[00:13:57.680 --> 00:14:00.400]   should start really, really early. And the special thing
[00:14:00.400 --> 00:14:04.280]   about ML education is that it starts very late. And if you're
[00:14:04.280 --> 00:14:08.400]   in academia, you can start with a curriculum and somehow have a
[00:14:08.400 --> 00:14:12.480]   feeling that you're making it step by step, if you are late
[00:14:12.520 --> 00:14:16.720]   or if you start on your own, I guess the resources are Yeah, I
[00:14:16.720 --> 00:14:19.720]   read the blog post there, I read the book over there, I watch a
[00:14:19.720 --> 00:14:23.440]   video over there. And it's hard to have a curriculum. And yeah,
[00:14:23.440 --> 00:14:25.440]   it's easy, perhaps to get lost.
[00:14:25.440 --> 00:14:28.520]   So you actually touched on another thing that I wanted to
[00:14:28.520 --> 00:14:31.680]   specifically ask about. And I'll turn it over to Sonia for this
[00:14:31.680 --> 00:14:33.880]   one, because I'm really curious to hear his perspective. But
[00:14:33.880 --> 00:14:38.600]   some people think that machine learning should not be about the
[00:14:38.600 --> 00:14:42.880]   math. And others think that there's nothing else to it, that
[00:14:42.880 --> 00:14:47.120]   that's all ML is. What's the right balance of math and
[00:14:47.120 --> 00:14:47.720]   machine learning?
[00:14:47.720 --> 00:14:51.120]   I mean, now that I don't have to interview for weights and
[00:14:51.120 --> 00:14:54.640]   biases, I was worried about being asked mathy questions,
[00:14:54.640 --> 00:14:59.000]   because I absolutely have no, I feel like I'm standing on this
[00:14:59.000 --> 00:15:01.720]   structure that's completely hollow, because I haven't
[00:15:01.720 --> 00:15:05.480]   followed this strict academic path. I have huge respect for
[00:15:05.480 --> 00:15:10.200]   that. But I just didn't follow it. So now I like just kept the
[00:15:10.200 --> 00:15:15.240]   green vegetables where I like math, statistics, and no, that's
[00:15:15.240 --> 00:15:18.240]   not exciting. Let's jump to Kaggle and see if I can win this
[00:15:18.240 --> 00:15:20.320]   competition. Let's just go there. Let's just build a
[00:15:20.320 --> 00:15:24.120]   project. That's fun. Let's just build my undergrad thesis was a
[00:15:24.120 --> 00:15:28.240]   music generator. That's more exciting than reading equations.
[00:15:28.240 --> 00:15:31.440]   Right? At least to me. I know no disrespect to anyone. So.
[00:15:33.200 --> 00:15:36.560]   So what is the right balance? I mean, it sounds like for you,
[00:15:36.560 --> 00:15:41.320]   you're excited to just get moving and just start building.
[00:15:41.320 --> 00:15:45.240]   But maybe more generally, and specifically with an eye towards
[00:15:45.240 --> 00:15:49.080]   education, how much math is necessary and useful even.
[00:15:49.080 --> 00:15:53.560]   So, yep, sorry, I forgot to answer the question. But I think
[00:15:53.560 --> 00:15:59.680]   I just skipped to what was necessary. So I usually just
[00:15:59.680 --> 00:16:03.080]   read the definition under an equation when I'm reading a
[00:16:03.080 --> 00:16:06.720]   paper, and then I glance over the equation. So just to have
[00:16:06.720 --> 00:16:09.440]   for me, my approach is just having enough working
[00:16:09.440 --> 00:16:12.160]   understanding where I can like translate the idea into code.
[00:16:12.160 --> 00:16:15.840]   And if I don't need to read the math for it or understand it, I
[00:16:15.840 --> 00:16:19.080]   just skip over it. Maybe that doesn't give me the depth of
[00:16:19.080 --> 00:16:21.920]   knowledge that I should have in the first place. But that's
[00:16:21.920 --> 00:16:25.240]   that's my hacker. Make do approach.
[00:16:25.240 --> 00:16:29.200]   Yeah. And I think, I mean, for what it's worth, like, I think
[00:16:29.200 --> 00:16:31.520]   it's really important to be inclusive of people with what I
[00:16:31.640 --> 00:16:34.560]   mean, with what you refer to as the hacker mindset, which is
[00:16:34.560 --> 00:16:36.600]   like, let's just get started. Let's just get our fingers
[00:16:36.600 --> 00:16:40.080]   dirty. And, you know, one thing that I'm interested in, let's
[00:16:40.080 --> 00:16:44.840]   just say, is in your videos, sometimes you're talking about
[00:16:44.840 --> 00:16:48.840]   incredibly mathematical ideas, things that are very, very deep
[00:16:48.840 --> 00:16:52.640]   on the theory side. And I don't see a lot of equations in your
[00:16:52.640 --> 00:16:54.760]   videos. How do you strike that balance?
[00:16:57.200 --> 00:17:02.280]   Yeah, it's, um, I get these comments from people like, Hey,
[00:17:02.280 --> 00:17:06.520]   why are you not doing a lot more theory? Or why are you not
[00:17:06.520 --> 00:17:14.640]   explaining this cool math behind that? And yeah, the thing is, I
[00:17:14.640 --> 00:17:20.000]   in my videos, I try to tell more of a story or more about of how
[00:17:20.000 --> 00:17:24.200]   to think about that thing that has a formula attached to it,
[00:17:24.400 --> 00:17:31.320]   but like how to tell it in words. And I guess formulas also
[00:17:31.320 --> 00:17:34.400]   try to sometimes scare people. And if you if you have a
[00:17:34.400 --> 00:17:37.080]   formula, you have a sort of commitment, you have to attach
[00:17:37.080 --> 00:17:40.240]   to it, because you have some sort of notation you have to
[00:17:40.240 --> 00:17:43.440]   introduce because nobody and not everybody uses the same
[00:17:43.440 --> 00:17:46.560]   notation. And you have to explain what exactly are those
[00:17:46.560 --> 00:17:50.880]   symbols even meaning over there, you have to explain like every
[00:17:50.880 --> 00:17:54.120]   tiny bit of it and not left, leave the left hand side and
[00:17:54.120 --> 00:17:57.360]   explained or any or something like that. And that would take
[00:17:57.360 --> 00:18:01.120]   time. And because I have, I want to keep my videos kind of short,
[00:18:01.120 --> 00:18:06.000]   I tell like the high level lesson I one could take from
[00:18:06.000 --> 00:18:12.560]   that, and I don't include many math equations, or even the very
[00:18:12.560 --> 00:18:16.200]   theoretical part of machine learning in my videos. And also
[00:18:16.200 --> 00:18:20.800]   it has, it has also to do with the kind of things I do during
[00:18:20.800 --> 00:18:26.640]   my PhD life, because if I were very invested in mathematics
[00:18:26.640 --> 00:18:31.360]   during my PhD life, like very deeply working on on these kinds
[00:18:31.360 --> 00:18:36.160]   of developments, I guess I would be more in that area and try to
[00:18:36.160 --> 00:18:39.520]   push it a lot because I would, you know, breathe it every day.
[00:18:39.520 --> 00:18:44.800]   But I am more on the experimenting side. So I think
[00:18:44.800 --> 00:18:47.600]   there I'm at the moment more comfortable. But you know, my
[00:18:47.600 --> 00:18:51.240]   background is actually physics and I am I was comfortable very
[00:18:51.240 --> 00:18:57.080]   much in formulas is just it's a balance between what I am doing
[00:18:57.080 --> 00:19:01.080]   very easily at the moment and I'm exercised in and also, I
[00:19:01.080 --> 00:19:05.560]   think, mathematical formulas in a video are a commitment and you
[00:19:05.560 --> 00:19:09.760]   have to do it three blue one brown, like to really get it
[00:19:09.760 --> 00:19:14.640]   across. And I don't know if I'm able to do that at the moment.
[00:19:15.640 --> 00:19:18.240]   Yeah, and I think there's a lot of really fantastic math
[00:19:18.240 --> 00:19:22.840]   YouTubers. I know because I watched their videos. But I think
[00:19:22.840 --> 00:19:29.520]   it is interesting how I've not yet found many math ML
[00:19:29.520 --> 00:19:33.640]   YouTubers, which I think I think is something that I've taken
[00:19:33.640 --> 00:19:38.280]   note of and I've wondered about. So there are some fantastic
[00:19:38.280 --> 00:19:40.720]   producers on Twitter, however, good.
[00:19:40.720 --> 00:19:45.280]   I just wanted to add to what Leticia was saying math in like
[00:19:45.320 --> 00:19:48.760]   ML is this house party that you weren't invited to for a long
[00:19:48.760 --> 00:19:51.480]   time. And now you jump into it. So it's this like running
[00:19:51.480 --> 00:19:54.240]   conversation of equations that you need to have an
[00:19:54.240 --> 00:19:57.760]   understanding of. That's why like I said earlier that we need
[00:19:57.760 --> 00:20:01.160]   more people just explaining this stuff as wonderfully as Leticia
[00:20:01.160 --> 00:20:05.200]   does and many other people also do. Just because you need to
[00:20:05.200 --> 00:20:08.200]   have that background knowledge just to be able to first of all
[00:20:08.200 --> 00:20:11.560]   decompress the knowledge and something that has just come out
[00:20:11.560 --> 00:20:12.200]   fresh. So
[00:20:12.680 --> 00:20:18.000]   yeah, yeah. Yeah, I think it's a it's an interesting juncture
[00:20:18.000 --> 00:20:22.520]   where ML has such stark applications and such deep
[00:20:22.520 --> 00:20:26.200]   fundamentals that it is a really I think that's one of the things
[00:20:26.200 --> 00:20:30.040]   that I find really exciting about ML and ML education is how
[00:20:30.040 --> 00:20:32.640]   it I tend to think of it as where the rubber meets the road
[00:20:32.640 --> 00:20:39.520]   for mathematics. So that's really cool. So this is maybe a
[00:20:39.560 --> 00:20:42.760]   difficult question, but I'm curious, you know, in the old
[00:20:42.760 --> 00:20:47.680]   story with you, Sam. What is a characteristic of someone new to
[00:20:47.680 --> 00:20:51.960]   ML that when you meet them, you think this person is going to do
[00:20:51.960 --> 00:20:52.760]   great things?
[00:20:52.760 --> 00:20:58.880]   That's, that's a tough one for me, because I am, I consider
[00:20:58.880 --> 00:21:01.840]   myself a noob, honestly, at least compared to the people in
[00:21:01.840 --> 00:21:11.040]   this call. I think you can, I was trying to think of some
[00:21:11.040 --> 00:21:14.240]   grand words that I could just say philosophically, and the one
[00:21:14.240 --> 00:21:17.880]   that came to my mind was in like, my free time, and I was
[00:21:17.880 --> 00:21:21.520]   trying to think of this is you can't open source passion. And I
[00:21:21.520 --> 00:21:25.960]   think the field is just so new that even someone who just joins
[00:21:25.960 --> 00:21:28.480]   today can get started with transformers and build
[00:21:28.480 --> 00:21:31.400]   incredible stuff they don't need, they probably should learn
[00:21:31.400 --> 00:21:35.200]   about all of the classic NLP stuff. They don't need to do
[00:21:35.200 --> 00:21:38.000]   like build fun projects. And when you meet someone who's
[00:21:38.000 --> 00:21:41.080]   truly passionate, you can tell that, hey, they're excited about
[00:21:41.080 --> 00:21:44.000]   this. And I think that's, that's the only hard requirement for
[00:21:44.000 --> 00:21:45.760]   ML, you just don't have to give up.
[00:21:45.760 --> 00:21:51.120]   And how about you, Leticia? What do you think is like the, this,
[00:21:51.120 --> 00:21:55.440]   you know, a good flag for this person? They've got something.
[00:21:55.440 --> 00:22:00.400]   The mathematician inside of you won't like the answer. But I
[00:22:00.400 --> 00:22:05.040]   think a hacker mind is needed. Because I mean, yes, ML has a
[00:22:05.040 --> 00:22:08.360]   lot of theory, and it's really nice, but you need an infinite
[00:22:08.360 --> 00:22:12.240]   amount of neurons in a layer, or you need an infinite amount of
[00:22:12.240 --> 00:22:16.720]   layers. And infinity doesn't really exist in computer
[00:22:16.720 --> 00:22:20.840]   science. And you need to go around with finite memory,
[00:22:20.840 --> 00:22:26.480]   finite, everything. And a hacker mind is the mind that will try
[00:22:27.040 --> 00:22:33.360]   every hyper parameter out and will not be discouraged by like,
[00:22:33.360 --> 00:22:39.920]   I want to know is now the random forest better than I know SVMs
[00:22:39.920 --> 00:22:43.120]   is it or is it not? And this definitive answer, this
[00:22:43.120 --> 00:22:47.320]   theoretical answer, what is now better? And you don't get such
[00:22:47.320 --> 00:22:50.080]   an answer in machine learning, because it's always data
[00:22:50.080 --> 00:22:53.360]   dependent, like very, very data dependent, and not even
[00:22:53.360 --> 00:22:56.520]   application or task dependent, just data dependent, what works
[00:22:56.520 --> 00:23:01.320]   better. And you don't get any time a definitive answer. And
[00:23:01.320 --> 00:23:05.040]   you need to try things out and to keep off the even classical
[00:23:05.040 --> 00:23:08.880]   algorithms here and, and just try them out. Because sometimes
[00:23:08.880 --> 00:23:12.560]   you just need a baseline where you say the frequency on in this
[00:23:12.560 --> 00:23:15.760]   dimension should be greater than 500. And then it just works, you
[00:23:15.760 --> 00:23:19.480]   don't even need ML. And that's why you, you need this hacker
[00:23:19.480 --> 00:23:23.720]   mind that tries to hack over there, something over there and
[00:23:23.720 --> 00:23:27.800]   put things together and not like ask, what is the general rule
[00:23:27.800 --> 00:23:32.200]   that will give me the recipe to solve any problem, but like, you
[00:23:32.200 --> 00:23:36.120]   know, this, this Lego pieces, putting them from there and
[00:23:36.120 --> 00:23:41.080]   there and stitching things together. And, and I think I see
[00:23:41.080 --> 00:23:43.360]   that success.
[00:23:43.360 --> 00:23:48.440]   That's great. That's great. Yeah. You know, sort of along
[00:23:48.440 --> 00:23:53.160]   those lines, like, what do you, you know, let's say for you, you
[00:23:53.160 --> 00:23:56.840]   have students and for Siam, you have this, like, you know, very
[00:23:56.840 --> 00:24:01.520]   invested audience, like, what do you try to instill in the people
[00:24:01.520 --> 00:24:04.880]   that you are listening, that are listening to you? What are you
[00:24:04.880 --> 00:24:07.880]   trying to communicate to them? Because in the case of passion,
[00:24:07.880 --> 00:24:12.200]   it's really hard to, to inspire passion, but you could try. And
[00:24:12.200 --> 00:24:14.480]   in the case of that hacker mindset, you can tell them,
[00:24:14.480 --> 00:24:17.840]   don't, don't give up and, you know, be tenacious and keep
[00:24:17.840 --> 00:24:20.800]   pushing. What do you what do you want to get across? What do you
[00:24:20.800 --> 00:24:25.520]   want your, your audience to, to be inspired from you? We'll
[00:24:25.520 --> 00:24:26.920]   start with Letizia.
[00:24:26.920 --> 00:24:31.040]   Yeah, that's interesting, because my answer will be
[00:24:31.040 --> 00:24:33.600]   actually a little bit contradicting what I've said
[00:24:33.600 --> 00:24:37.880]   before, because I think a hacker mind is, you have to be that to
[00:24:37.880 --> 00:24:43.760]   have success in ML. But when I am teaching, I try to get across
[00:24:43.760 --> 00:24:49.080]   these points of, of, of these general points, these general
[00:24:49.080 --> 00:24:52.720]   pieces of understanding these ideas that are the broader about
[00:24:52.720 --> 00:24:55.800]   deductive biases, for example, I love thinking about that, like,
[00:24:55.800 --> 00:25:01.360]   what is it that neural networks brings in, when it's not even
[00:25:01.360 --> 00:25:04.600]   trained, you just initialize it, and what is it what it can do?
[00:25:04.600 --> 00:25:10.240]   And, yeah, I try to, and I am very excited about these general
[00:25:10.240 --> 00:25:14.440]   takes, and I teach about them. And I try to make people excited
[00:25:14.440 --> 00:25:18.520]   about that. And then at every end of the lecture, I'm like,
[00:25:18.520 --> 00:25:24.200]   yeah, but it's, you know, this is theory, and you still have to
[00:25:24.200 --> 00:25:28.360]   try it out. And then I returned to the hacker mind. But I think
[00:25:28.360 --> 00:25:35.640]   one, one also needs to think about what one hacks. And yeah,
[00:25:35.640 --> 00:25:38.320]   that's, that's, I think, my passion.
[00:25:38.320 --> 00:25:39.960]   Yeah. So
[00:25:42.120 --> 00:25:46.840]   I don't have as awesome of an audience as Latisha does. So I
[00:25:46.840 --> 00:25:50.160]   can speak about what I do at Bits and Biases. We really care
[00:25:50.160 --> 00:25:53.960]   about the community, broadly speaking, and I'm in this lucky
[00:25:53.960 --> 00:25:57.720]   position where I just try to teach other people something
[00:25:57.720 --> 00:26:00.680]   that I have just learned. So what I try to leave them with
[00:26:00.680 --> 00:26:04.360]   is, hey, why don't you try this? And we at Bits and Biases will
[00:26:04.360 --> 00:26:06.720]   send you some swag, I just convinced my team to do that.
[00:26:06.720 --> 00:26:10.160]   For some reason, I tell them to do this homework and people
[00:26:10.160 --> 00:26:14.680]   always do. So that's, that's loosely what I try to leave the
[00:26:14.680 --> 00:26:15.200]   audience with.
[00:26:15.200 --> 00:26:18.400]   I see. So inspire to get involved, I guess would be one
[00:26:18.400 --> 00:26:21.920]   way of saying it. That's awesome. That's awesome. Great.
[00:26:21.920 --> 00:26:26.960]   So first, I want to take a minute to have a beverage break.
[00:26:26.960 --> 00:26:31.160]   So, you know, anyone in the audience, go ahead and pick up
[00:26:31.160 --> 00:26:34.760]   your your coffee or your tea. And let's have a sip.
[00:26:34.760 --> 00:26:38.960]   Fair enough.
[00:26:38.960 --> 00:26:44.400]   I know, I know, I came prepared. Cool. And then I'd like to
[00:26:44.400 --> 00:26:47.040]   actually take a minute and switch to like a lightning round.
[00:26:47.040 --> 00:26:52.240]   And so I'm gonna take one of you at a time. And we're gonna do a
[00:26:52.240 --> 00:26:54.960]   sort of lightning round of questions about your experience
[00:26:54.960 --> 00:26:59.600]   and some of your particular interests. So let's start with
[00:26:59.600 --> 00:27:05.760]   LaTitia. So I guess my first question to you, LaTitia, is,
[00:27:06.240 --> 00:27:09.280]   where did Miss Coffee Bean come from? What's her origin story?
[00:27:09.280 --> 00:27:17.480]   In Photoshop. I just wanted to have a character so I don't have
[00:27:17.480 --> 00:27:21.240]   to show my face for every video. So I have a replacement that I
[00:27:21.240 --> 00:27:25.760]   just have to plug in somehow. Yeah, it's not.
[00:27:25.760 --> 00:27:30.760]   And was this a character that you had drawn previously? Or
[00:27:30.760 --> 00:27:35.240]   sort of like, no. Yeah, yeah, I've I was thinking about that
[00:27:35.240 --> 00:27:38.680]   I was thinking about I wanted to have a channel where I explain
[00:27:38.680 --> 00:27:41.720]   things. So I wanted to reach an audience broader than just my
[00:27:41.720 --> 00:27:46.600]   students at university. And I wanted to make it cool and you
[00:27:46.600 --> 00:27:49.520]   know, make it appealing. And I didn't want to show my face
[00:27:49.520 --> 00:27:54.400]   every time. I wanted also to name the channel somehow. And I
[00:27:54.400 --> 00:27:57.920]   was thinking about a coffee break. Great. And I need a coffee
[00:27:57.920 --> 00:28:01.200]   related character. And I was thinking about coffee cup or
[00:28:01.200 --> 00:28:06.440]   something. And I thought of bean, it's probably the thing.
[00:28:06.440 --> 00:28:10.200]   And I just drew her for that. And I'm actually not a drawer,
[00:28:10.200 --> 00:28:14.360]   you can see it on I can tell you so many drawing mistakes on her.
[00:28:14.360 --> 00:28:20.840]   But I just won't point them out right now. It's I'm not I'm not
[00:28:20.840 --> 00:28:23.200]   experienced in this. But you know, in PowerPoint, in
[00:28:23.200 --> 00:28:27.640]   Photoshop, you can just undo when you have drawn a mistake.
[00:28:27.640 --> 00:28:29.400]   So that's what I did.
[00:28:29.960 --> 00:28:32.000]   That's great. That's great. I think it's really effective. I
[00:28:32.000 --> 00:28:35.720]   really like it. Um, another thing that I wanted to ask about
[00:28:35.720 --> 00:28:39.640]   your channel in particular was that, so you've recently started
[00:28:39.640 --> 00:28:42.960]   doing quizzes. And the quizzes
[00:28:42.960 --> 00:28:46.720]   not recently. It's a long time ago now.
[00:28:46.720 --> 00:28:48.120]   They have been awesome.
[00:28:48.120 --> 00:28:53.640]   More than 300. More than 300 questions. So it has to be more
[00:28:53.640 --> 00:28:57.280]   than 300 days because they come out. Yeah, I thought it was I
[00:28:57.280 --> 00:29:02.160]   have a whole database of a couple months. My goodness. Okay,
[00:29:02.160 --> 00:29:05.960]   okay. So, you know, one way or another, you've got a large
[00:29:05.960 --> 00:29:10.720]   number of quizzes. And from my perspective, they run the gamut
[00:29:10.720 --> 00:29:15.600]   of very clear, just like sort of like, you know, how much have
[00:29:15.600 --> 00:29:19.800]   you studied the fundamentals versus borderline, like, you
[00:29:19.800 --> 00:29:25.640]   know, topics for a podcast? Um, what do you sort of, I guess my
[00:29:25.640 --> 00:29:28.400]   first question about these quizzes is how much of this is
[00:29:28.400 --> 00:29:31.720]   things that you already knew and thought were useful to remind
[00:29:31.720 --> 00:29:35.640]   people versus things that you were unsure of yourself? And you
[00:29:35.640 --> 00:29:38.920]   sort of wanted to use this as an opportunity to reify your own
[00:29:38.920 --> 00:29:39.520]   understanding?
[00:29:39.520 --> 00:29:45.320]   Yeah, when I was first posting these questions, so at the
[00:29:45.320 --> 00:29:49.080]   beginning, I was posting about things that I really knew about,
[00:29:49.080 --> 00:29:54.040]   because that's what I I, I thought about that concept today.
[00:29:54.040 --> 00:29:59.120]   And I know about that concept for a long time. So just ask on
[00:29:59.120 --> 00:30:02.680]   on a quiz, if everybody now knows about that concept, and it
[00:30:02.680 --> 00:30:07.120]   depends a lot on what education type and where the education was
[00:30:07.120 --> 00:30:12.680]   for people, if they know the same things, if they find this
[00:30:12.680 --> 00:30:17.760]   also self understood or not, or self explanatory. So but these
[00:30:17.760 --> 00:30:21.720]   were like, at most 20 questions, not more than that. And if I
[00:30:21.720 --> 00:30:26.040]   wanted to continue posting questions every day, then I
[00:30:26.040 --> 00:30:28.920]   really had to get some inspiration. So I started to
[00:30:28.920 --> 00:30:33.680]   read a book, the deep, the deep learning book by Goodfellow and
[00:30:33.680 --> 00:30:40.480]   collaborators. It's free, so no paywall there. And yeah, and
[00:30:40.480 --> 00:30:44.480]   there I learned about some things, just things that I was
[00:30:44.480 --> 00:30:48.960]   missing during my training. So for example, I didn't know about
[00:30:49.000 --> 00:30:54.120]   and that's actually a thing that is now coming up in the last
[00:30:54.120 --> 00:30:59.560]   questions about the icon of regularization. And yeah, it's,
[00:30:59.560 --> 00:31:05.360]   I had to get a lot of, you know, reading to and I learned about
[00:31:05.360 --> 00:31:09.760]   some things there. And it's about some things I don't, even
[00:31:09.760 --> 00:31:13.040]   after I posted the question, I don't know really what's the
[00:31:13.040 --> 00:31:16.840]   answer about. And then people are starting to explain it to me.
[00:31:16.840 --> 00:31:18.760]   And that's actually the wonderful thing about these
[00:31:18.760 --> 00:31:22.000]   quizzes is that sometimes they're just wrong, I'm wrong.
[00:31:22.000 --> 00:31:26.640]   And people start to comment. And I like actually, most I like the
[00:31:26.640 --> 00:31:29.480]   questions where people comment a lot. And this is why some of the
[00:31:29.480 --> 00:31:33.520]   questions are kind of these podcast questions about does
[00:31:33.520 --> 00:31:37.760]   what's artificial intelligence or things like these, where
[00:31:37.760 --> 00:31:41.880]   people start to argue. I think that's really great. Because
[00:31:41.880 --> 00:31:46.480]   when you ask about l1 versus l2 loss, then the comment section
[00:31:46.480 --> 00:31:47.440]   is not that interesting.
[00:31:47.440 --> 00:31:51.360]   Yeah, yeah. Cool. Yeah, I find these quizzes really fun. And
[00:31:51.360 --> 00:31:53.760]   there are definitely days where I'm in the comments myself. So
[00:31:53.760 --> 00:31:55.840]   yeah, I know.
[00:31:55.840 --> 00:32:00.360]   I just going to say I've been I've been following the quizzes
[00:32:00.360 --> 00:32:03.960]   for a long time, and I almost feel all of them. So they're
[00:32:03.960 --> 00:32:06.600]   quite tough. I would encourage everyone to check them out.
[00:32:06.600 --> 00:32:12.120]   We have a very surprised. Sorry, it's to see that most of them
[00:32:12.120 --> 00:32:16.280]   really are. So the majority is right. It's just very, very few
[00:32:16.280 --> 00:32:20.440]   of them where the majority isn't right. And that's why I was
[00:32:20.440 --> 00:32:23.720]   thinking about what I didn't find the time of posting videos
[00:32:23.720 --> 00:32:26.240]   that explain exactly these questions where the majority
[00:32:26.240 --> 00:32:30.640]   isn't right. And I found some help. Tim has helped me to do
[00:32:30.640 --> 00:32:34.840]   some of these things. But it's, it's work, you know.
[00:32:34.840 --> 00:32:40.280]   And the quizzes themselves are already work. And sometimes,
[00:32:40.280 --> 00:32:45.520]   yeah, the questions are like, you see, I didn't have so much
[00:32:45.520 --> 00:32:46.320]   time to for them.
[00:32:46.320 --> 00:32:52.200]   We have a question from Marco here that I'll surface to you,
[00:32:52.200 --> 00:32:55.000]   which is, do you make longer teaching videos internally for
[00:32:55.000 --> 00:32:58.760]   your students that are not part of the AI Coffee Break, but are
[00:32:58.760 --> 00:33:00.400]   just sort of as you teach?
[00:33:00.400 --> 00:33:06.560]   Yeah, exactly. So that's how I actually started. I was, you
[00:33:06.560 --> 00:33:09.760]   know, the pandemic started, and then we had to choose how we
[00:33:09.760 --> 00:33:13.600]   teach. And at the time, because of the infrastructure, it wasn't
[00:33:13.600 --> 00:33:18.000]   really possible to teach live or I, I wasn't comfortable with
[00:33:18.000 --> 00:33:22.080]   teaching live with a lot of, I mean, the worst things that could
[00:33:22.080 --> 00:33:25.320]   go wrong technically, and I wasn't very experienced about
[00:33:25.320 --> 00:33:28.120]   that. And I said, Okay, I do a video. And if I do a video, I
[00:33:28.120 --> 00:33:33.680]   can mess it up, and just, you know, edit it. And video is also
[00:33:33.680 --> 00:33:36.480]   something that everyone can watch without interruption
[00:33:36.480 --> 00:33:40.240]   because of network errors, and so on. And I started making
[00:33:40.240 --> 00:33:44.080]   videos. But after making, teaching one semester with
[00:33:44.080 --> 00:33:51.520]   videos, I thought, Okay, I don't want to do this anymore, because
[00:33:51.520 --> 00:33:54.800]   it's a lot of time and the production quality wasn't like
[00:33:54.800 --> 00:34:00.080]   for the channel. It was just me talking. But still, it was work.
[00:34:00.080 --> 00:34:05.560]   And I just did it that I taught live, still in the pandemic, I
[00:34:05.560 --> 00:34:08.920]   taught live and I was recording myself because I wasn't
[00:34:08.920 --> 00:34:12.200]   expecting now everybody to not run into problems while
[00:34:12.200 --> 00:34:18.240]   everything live was running. And now we move to I'm teaching in
[00:34:18.240 --> 00:34:21.760]   person again. So I have students in front of me, I don't need to
[00:34:21.760 --> 00:34:26.920]   do videos anymore. But still, I do videos for a few couple of
[00:34:26.920 --> 00:34:31.400]   students just that are not able to be in Heidelberg because of
[00:34:31.400 --> 00:34:36.720]   reasons. But yeah, I'm, I'm sharing this only internally,
[00:34:36.760 --> 00:34:42.240]   exactly. And I don't actually think to I'm not thinking about
[00:34:42.240 --> 00:34:46.280]   publishing it because when you do a video like this, you have
[00:34:46.280 --> 00:34:50.600]   to take care that every thing every material is not copyrighted
[00:34:50.600 --> 00:34:55.560]   or that you're right on all these legal sides. And when I'm
[00:34:55.560 --> 00:34:59.480]   teaching and I want to show cool thing internally, then I can
[00:34:59.480 --> 00:35:04.160]   just show it. So yeah, no sharing from that side.
[00:35:04.720 --> 00:35:07.720]   One comment I'll make Marco is if you're looking for open
[00:35:07.720 --> 00:35:11.960]   source, great lecture content, I can't help but recommend
[00:35:11.960 --> 00:35:17.880]   Sebastian Roshka's YouTube series. They come with Jupiter
[00:35:17.880 --> 00:35:22.040]   notebooks, and they also reference his books. And I think
[00:35:22.040 --> 00:35:25.200]   that materials very high quality, and he's a great
[00:35:25.200 --> 00:35:27.360]   educator. And so if that's something that you're looking
[00:35:27.360 --> 00:35:31.040]   for, I, I think that's a good place to look. Leticia, do you
[00:35:31.040 --> 00:35:33.880]   have any ones that you'd want to recommend?
[00:35:33.880 --> 00:35:39.320]   The MIT lectures, the Stanford lectures, like there are the
[00:35:39.320 --> 00:35:44.080]   lectures, they usually have people who have researched on
[00:35:44.080 --> 00:35:48.480]   that topic talk about teach that topic. It's like the closest to
[00:35:48.480 --> 00:35:51.680]   the source you can get and the quality is amazing. And yeah,
[00:35:51.680 --> 00:35:52.160]   it's
[00:35:52.160 --> 00:35:56.440]   I mean, I, I totally agree. One in particular is the one on
[00:35:56.440 --> 00:36:00.400]   SVMs. When I give my lectures on SVMs, I'm trying my hardest to
[00:36:00.400 --> 00:36:03.760]   be as close to the MIT version as I can possibly get, because I
[00:36:03.760 --> 00:36:06.840]   don't think there's any room for improvement. So
[00:36:06.840 --> 00:36:12.240]   but then what is the role of university if you if you can
[00:36:12.240 --> 00:36:15.320]   find all this material online? That's what I am asking myself
[00:36:15.320 --> 00:36:20.160]   while I'm teaching these things like why yet again, an
[00:36:20.160 --> 00:36:21.360]   explanation?
[00:36:21.360 --> 00:36:23.960]   I think it's a really fascinating question. I thought
[00:36:23.960 --> 00:36:27.320]   about it myself. I'm curious, do you have an answer?
[00:36:29.600 --> 00:36:34.320]   I actually I don't have like I have a strange opinion on this,
[00:36:34.320 --> 00:36:38.560]   which is, I think that these standard lectures that were
[00:36:38.560 --> 00:36:42.360]   already taught like once, twice, thrice, like, it's really high
[00:36:42.360 --> 00:36:45.960]   quality, you just have to give the students like with the link
[00:36:45.960 --> 00:36:49.360]   and people should watch it. And maybe they shouldn't watch it
[00:36:49.360 --> 00:36:54.000]   alone to make like a view like a cinema session, but educational
[00:36:54.000 --> 00:36:57.600]   and everybody's watching it and one can pause at the moment so
[00:36:57.600 --> 00:37:00.560]   one doesn't understand something and we can talk about it. It's
[00:37:00.560 --> 00:37:04.640]   more like a seminar and let's like a lecture then and then
[00:37:04.640 --> 00:37:09.880]   professors should be less, you know, teaching old content and
[00:37:09.880 --> 00:37:12.840]   regurgitating that but teaching about the really cutting edge
[00:37:12.840 --> 00:37:15.400]   new stuff and putting a lot of investment to create high
[00:37:15.400 --> 00:37:18.880]   quality on that. And professors would have, you know, do
[00:37:18.880 --> 00:37:23.160]   projects with you and supervise you but less of this. Let's
[00:37:23.680 --> 00:37:28.480]   explain SVMs for the, you know, millions time.
[00:37:28.480 --> 00:37:31.840]   For what it's worth, I really agree with you a lot. And I
[00:37:31.840 --> 00:37:35.760]   think what my goal is, what I'm teaching is to not just
[00:37:35.760 --> 00:37:38.360]   regurgitate and not just and I do give them the link to that
[00:37:38.360 --> 00:37:43.000]   MIT lecture and say, here's what I consider to be 10 out of 10.
[00:37:43.000 --> 00:37:46.360]   My goal is to create an atmosphere for them to ask
[00:37:46.360 --> 00:37:51.680]   questions, and for them to inspect sort of what these ideas
[00:37:51.680 --> 00:37:54.560]   are that they're not quite understanding as the lecture is
[00:37:54.560 --> 00:37:58.800]   going through. The challenge about asynchronous videos are
[00:37:58.800 --> 00:38:02.280]   that there's no feedback loop. There's no, wait, I'm confused
[00:38:02.280 --> 00:38:04.640]   by that. Like, what does that mean? When you say that you
[00:38:04.640 --> 00:38:07.200]   want to, you know, maximize the difference from the support
[00:38:07.200 --> 00:38:11.800]   vectors? You know, why? And I think that's what I really
[00:38:11.800 --> 00:38:15.960]   strive for is to say, like to repeatedly interrupt myself and
[00:38:15.960 --> 00:38:18.680]   ask the students, what do you need here? What are you
[00:38:18.680 --> 00:38:22.560]   confused on? What do you get? So I think, I think I really align
[00:38:22.560 --> 00:38:26.160]   with sort of your vision about a viewing party for these like,
[00:38:26.160 --> 00:38:29.280]   perfect lectures. That's really fun. And I think you're also
[00:38:29.280 --> 00:38:32.920]   right to say that more investment on cutting edge
[00:38:32.920 --> 00:38:37.440]   material actually serves a lot more value than just sort of
[00:38:37.440 --> 00:38:44.080]   iterating and we'll call it lossy reproductions of these,
[00:38:44.080 --> 00:38:45.520]   these good lectures.
[00:38:45.880 --> 00:38:49.440]   Sanyam Bhutani: I, if I may jump in, like, I have a strong
[00:38:49.440 --> 00:38:52.840]   opinion. And I'm, I'm like a total outsider to this world of
[00:38:52.840 --> 00:38:56.320]   teaching, right. But I think, I'm sure all of you have seen
[00:38:56.320 --> 00:38:59.680]   the latest Apple keynotes, they've shown us like, what's
[00:38:59.680 --> 00:39:03.960]   the production bar for if you like have to watch this non
[00:39:03.960 --> 00:39:08.720]   live? This is the bar and what's what's stopping universities or
[00:39:08.720 --> 00:39:11.120]   research labs from doing that? I mean, not of course, at that
[00:39:11.120 --> 00:39:15.760]   level, but just make one lecture series that's so well produced,
[00:39:15.760 --> 00:39:19.960]   so well animated that I can just like, I don't hit Ctrl T and
[00:39:19.960 --> 00:39:23.640]   start scrolling my Twitter feed. I don't hit Ctrl T and start
[00:39:23.640 --> 00:39:24.720]   doing anything else.
[00:39:24.720 --> 00:39:29.800]   And I think some YouTube producers like 3Blue1Brown, or
[00:39:29.800 --> 00:39:35.920]   to a certain extent, Mathologer, are shooting for that sort of
[00:39:35.920 --> 00:39:39.320]   thing of like, let's take an idea. And let's make sort of
[00:39:39.320 --> 00:39:45.040]   the, as good as we possibly can to convey this like deep idea,
[00:39:45.280 --> 00:39:48.520]   even if it's something that you could find elsewhere. Like, I
[00:39:48.520 --> 00:39:51.120]   think I think that is sort of their goal. And I can't speak
[00:39:51.120 --> 00:39:53.720]   for them. Maybe we'll need to get Grant Sanderson on the show
[00:39:53.720 --> 00:40:00.080]   next. But so to wrap up this lightning round, Letizia, I have
[00:40:00.080 --> 00:40:02.880]   a few more questions for you, which is, what is one thing that
[00:40:02.880 --> 00:40:05.120]   you love and hate about creating content?
[00:40:05.120 --> 00:40:14.360]   Yeah, it's what I love about it is that I can, you know,
[00:40:14.800 --> 00:40:19.000]   explain something while I'm sleeping, especially if it's a
[00:40:19.000 --> 00:40:23.480]   video, right? It's people from all over the world can watch it
[00:40:23.480 --> 00:40:28.040]   while I am not actively, you know, doing that anymore,
[00:40:28.040 --> 00:40:31.080]   because I already did it, post it, done. It's useful,
[00:40:31.080 --> 00:40:37.480]   hopefully. What I hate about it is, I don't know what I hate
[00:40:37.480 --> 00:40:40.920]   most, it's the editing. But that's something I could easily,
[00:40:40.920 --> 00:40:44.760]   you know, delegate. I didn't decide to do that yet. But I
[00:40:44.760 --> 00:40:49.280]   should do. But the other part is that you put yourself out there
[00:40:49.280 --> 00:40:53.920]   and that piece of content kind of lives without you. And that's
[00:40:53.920 --> 00:40:56.480]   the great part about it, you can explain something while I'm
[00:40:56.480 --> 00:41:00.600]   sleeping. But also, it can be misunderstood without you having
[00:41:00.600 --> 00:41:06.000]   the chance to, you know, really explain it. Or, yeah, it's just
[00:41:06.000 --> 00:41:09.680]   sometimes I mean, if I do a video today, in a year, I would
[00:41:09.680 --> 00:41:13.280]   have known much better how to do it. And it's like, ah, this,
[00:41:13.400 --> 00:41:16.680]   you're looking back and you see this low quality content. And
[00:41:16.680 --> 00:41:20.640]   also, the funny thing is my Transformer Explained video is
[00:41:20.640 --> 00:41:24.480]   one of my first videos. And I'm so it's so cringe when I look at
[00:41:24.480 --> 00:41:27.680]   it. It's like, my one of my first videos there was talking
[00:41:27.680 --> 00:41:31.200]   really strangely, and I didn't know, you know how to do it and
[00:41:31.200 --> 00:41:34.000]   how to explain it well. And now I could remaster it so much
[00:41:34.000 --> 00:41:36.360]   better. And it's on my to do list will have a remastered
[00:41:36.360 --> 00:41:46.640]   Transformer Explained video. But yeah, it's hard sometimes for
[00:41:46.640 --> 00:41:50.840]   perfectionist. Understood. I have to lose perfectionism.
[00:41:50.840 --> 00:41:55.840]   Where do you take inspiration to improve your videos? And, you
[00:41:55.840 --> 00:41:58.360]   know, how much time do you spend thinking about sort of
[00:41:58.360 --> 00:42:00.880]   production and making them perfect?
[00:42:00.880 --> 00:42:04.680]   Yeah, that's the point about perfectionism is what I have
[00:42:04.680 --> 00:42:08.080]   learned is that with perfectionism, you don't get one
[00:42:08.080 --> 00:42:12.880]   video out because it will never be perfect. And if I am doing so
[00:42:12.880 --> 00:42:16.920]   I got 80% of the quality and if I try to push it higher, I could
[00:42:16.920 --> 00:42:22.280]   get the last 20%. But then for whom is that the last 20%? I
[00:42:22.280 --> 00:42:26.760]   think I believe in overfitting for content. If I make it
[00:42:26.760 --> 00:42:30.960]   perfect for you, Brian, then it I won't like it anymore. Right.
[00:42:31.280 --> 00:42:36.520]   And I believe in the 80% rule, like I do it how it is, and I
[00:42:36.520 --> 00:42:40.240]   will improve with my next video. And of course, in my next video,
[00:42:40.240 --> 00:42:43.680]   I will do even some mistakes over there. But it's like, I
[00:42:43.680 --> 00:42:48.640]   wanted so the improvement to be from video to video and not on
[00:42:48.640 --> 00:42:52.320]   one video because otherwise I will never get something out.
[00:42:52.320 --> 00:42:58.160]   And I have to I was really perfectionist. I am not that
[00:42:58.400 --> 00:43:03.360]   perfectionist anymore. I'm still you know, editing more than for
[00:43:03.360 --> 00:43:07.760]   example, Yannick does or I mean what I what I absolutely so
[00:43:07.760 --> 00:43:11.200]   Yannick is an inspiration for me on this aspect on many aspects,
[00:43:11.200 --> 00:43:15.440]   but on one aspect is a real inspiration on, you know, just
[00:43:15.440 --> 00:43:22.880]   do it not don't edit it at all or and just live with it. That's
[00:43:22.880 --> 00:43:23.840]   I think, great.
[00:43:26.280 --> 00:43:30.200]   One final question for our lightning round, which is, and I
[00:43:30.200 --> 00:43:34.160]   love this question for what it's worth. How has YouTube affected
[00:43:34.160 --> 00:43:37.000]   how you approach your research? What's the effect of your
[00:43:37.000 --> 00:43:39.160]   channel on your research?
[00:43:39.160 --> 00:43:48.400]   It's less than I had expected, actually. So what the effect the
[00:43:48.400 --> 00:43:52.880]   immediate and big effect is that I'm reading and I'm knowing
[00:43:52.880 --> 00:43:56.800]   about stuff a little broader than my research topic, which is
[00:43:56.800 --> 00:44:00.920]   very specific on multimodality is forcing me to it's sometimes I
[00:44:00.920 --> 00:44:05.720]   just explain the birthology, the biology meets the birthology
[00:44:05.720 --> 00:44:08.880]   meets biology paper. It's a while now. And I learned
[00:44:08.880 --> 00:44:13.840]   something about biology and all these vision transformers I am
[00:44:13.840 --> 00:44:16.680]   explaining. It's also something I don't really need for my
[00:44:16.680 --> 00:44:20.000]   research. So I'm keeping a broad perspective on a broader
[00:44:20.040 --> 00:44:24.200]   perspective onto the field than I would naturally do just
[00:44:24.200 --> 00:44:28.600]   pursuing my own research. But the other effect is but I don't
[00:44:28.600 --> 00:44:33.920]   think it's that strong is that it makes me may present my own
[00:44:33.920 --> 00:44:39.920]   research better. So I think the presentations I do at in, you
[00:44:39.920 --> 00:44:42.840]   know, at my university just became better because of my
[00:44:42.840 --> 00:44:49.120]   content creation activity. Also, I believe now in writing the
[00:44:49.120 --> 00:44:51.920]   paper as well as I can, of course, I'm improving daily on
[00:44:51.920 --> 00:44:55.960]   that. But also I like well written papers now much more
[00:44:55.960 --> 00:45:01.040]   than I liked them before. And but otherwise, it's the
[00:45:01.040 --> 00:45:06.760]   research ideas. I don't think that's something that has to do
[00:45:06.760 --> 00:45:07.880]   with the channel.
[00:45:07.880 --> 00:45:12.600]   Cool, cool. That's great. I'm not surprised to hear that your
[00:45:12.600 --> 00:45:19.320]   presentations are getting next level. Awesome. So let's, let's
[00:45:19.320 --> 00:45:24.120]   switch to Siam for this lightning round. Okay, that's
[00:45:24.120 --> 00:45:30.920]   what you say now. So if you weren't doing machine learning
[00:45:30.920 --> 00:45:33.560]   and things adjacent to machine learning, what subject would
[00:45:33.560 --> 00:45:34.400]   you be working in?
[00:45:37.640 --> 00:45:44.720]   I really don't know. I just tried everything in my
[00:45:44.720 --> 00:45:47.800]   undergrad. And that was like at the initial curve of the
[00:45:47.800 --> 00:45:51.520]   Dunning-Kruger effect where I can say that I tried everything
[00:45:51.520 --> 00:45:54.680]   of course I did. And this is what stuck. So like I was
[00:45:54.680 --> 00:45:58.760]   literally signing up for every possible workshop for freshers,
[00:45:58.760 --> 00:46:04.320]   freshmen, every single student club for freshmen. I just
[00:46:04.320 --> 00:46:05.400]   happened to stick with this.
[00:46:06.360 --> 00:46:09.480]   That's great. Yeah, I think early diversification is about
[00:46:09.480 --> 00:46:12.880]   the best thing you can do. So I strongly support your strategy.
[00:46:12.880 --> 00:46:17.520]   I think I know the answer to this one. What's the best ML
[00:46:17.520 --> 00:46:18.400]   book you've ever read?
[00:46:18.400 --> 00:46:23.320]   Oh, fast book. That's the only one I would recommend to
[00:46:23.320 --> 00:46:26.800]   everyone. It's for the audience. It's the fast AI book. It's also
[00:46:26.800 --> 00:46:28.920]   available for free. You can buy it. You should buy it if you
[00:46:28.920 --> 00:46:33.000]   want to read it in paper. That's the best book.
[00:46:33.240 --> 00:46:39.480]   I had a feeling. One thing, you know, we've sort of implicitly
[00:46:39.480 --> 00:46:44.280]   talked about it a little bit already today. But like, ML has
[00:46:44.280 --> 00:46:48.960]   applications, it's almost the point. And I'd be curious to
[00:46:48.960 --> 00:46:51.720]   know, what is your favorite type of application, your favorite
[00:46:51.720 --> 00:46:53.200]   field of application?
[00:46:53.200 --> 00:47:02.520]   That's a tough one. Just because it's really janky, right? At the
[00:47:02.720 --> 00:47:06.240]   end points, like we see research papers, they're like, we get the
[00:47:06.240 --> 00:47:10.800]   we get the excitement, but like, what's what's the best
[00:47:10.800 --> 00:47:14.800]   application that I use day to day? Grammarly? Maybe autocorrect?
[00:47:14.800 --> 00:47:18.120]   Maybe? I really don't have a good answer for that.
[00:47:18.120 --> 00:47:21.480]   I guess my question is maybe not just your favorite application
[00:47:21.480 --> 00:47:24.480]   that you use. But when you think about the wider world, when you
[00:47:24.480 --> 00:47:27.560]   think about ML applications that get you really fired up, and
[00:47:27.560 --> 00:47:31.320]   you're like, this is why ML is important. Is it healthcare? Is
[00:47:31.320 --> 00:47:34.680]   it self driving? Is it drug discovery? Is it you know,
[00:47:34.680 --> 00:47:37.320]   language like translation? What is it?
[00:47:37.320 --> 00:47:43.680]   Got it. Sorry. So I would say agritech agriculture. India has
[00:47:43.680 --> 00:47:47.280]   India is like an agriculture based economy. I know many
[00:47:47.280 --> 00:47:51.080]   countries are. And if we can like optimize that, we can
[00:47:51.080 --> 00:47:55.400]   optimize food for everyone. We can optimize that goal.
[00:47:56.040 --> 00:48:03.280]   Yeah, yeah, that's great. Great. Yeah. Um, how did you first
[00:48:03.280 --> 00:48:07.040]   decide to make that leap into posting content on YouTube?
[00:48:07.040 --> 00:48:11.920]   Like, that's a big step. And I think if you haven't ever done
[00:48:11.920 --> 00:48:16.240]   it, it can feel like really intimidating. And so when I look
[00:48:16.240 --> 00:48:18.440]   at people that have done it, and then have continued to do it
[00:48:18.440 --> 00:48:21.520]   over a long period of time, I find myself often thinking like,
[00:48:21.520 --> 00:48:25.040]   man, like, where did they get that initial impulse energy to
[00:48:25.040 --> 00:48:29.480]   get over sort of like the the hump? So yeah, what what drove
[00:48:29.480 --> 00:48:30.600]   you to first start doing that?
[00:48:30.600 --> 00:48:36.200]   It was the impulse energy. So I I was in India, working on
[00:48:36.200 --> 00:48:40.080]   barrier level payout contracts, as someone who's just graduated.
[00:48:40.080 --> 00:48:44.240]   And I wasn't getting enough kick out of it. So I decided I'll end
[00:48:44.240 --> 00:48:47.880]   a few of them just to start a YouTube channel. And I just said
[00:48:47.880 --> 00:48:52.520]   from now on, now on out on onwards, I'll publish two videos
[00:48:52.520 --> 00:48:55.960]   every single week for an entire year. And I just did that. I
[00:48:55.960 --> 00:48:58.720]   just told that to every single person I knew on the internet I
[00:48:58.720 --> 00:49:01.720]   knew in my neighborhood. And that's what I did.
[00:49:01.720 --> 00:49:06.760]   That's, it's a truly mind boggling early commitment. I
[00:49:06.760 --> 00:49:11.720]   recently did 28 days in a row of video content and I thought I
[00:49:11.720 --> 00:49:16.080]   was going to die by the end. So I you know, I truly truly
[00:49:16.080 --> 00:49:16.520]   cannot.
[00:49:16.520 --> 00:49:18.440]   Overstay my
[00:49:18.440 --> 00:49:19.400]   impress
[00:49:20.280 --> 00:49:24.000]   mind weren't as good as yours. I skip a lot of them because
[00:49:24.000 --> 00:49:26.520]   they're coffee stuff. So what I know, I know they're better.
[00:49:26.520 --> 00:49:32.920]   We've talked about this a little bit. And let's see. I shared one
[00:49:32.920 --> 00:49:37.160]   of hers. And I'm curious. Do you have a role model in the YouTube
[00:49:37.160 --> 00:49:40.720]   ml community or even just ml community at large?
[00:49:40.720 --> 00:49:47.160]   At large, I really copied Jeremy Howard style just because I come
[00:49:47.160 --> 00:49:50.800]   from that school. Like, that's the only place place I've
[00:49:50.800 --> 00:49:55.520]   partially followed, not even completely. And that's gotten me
[00:49:55.520 --> 00:49:59.400]   a little far in my career. So of course, I try to emulate Jeremy
[00:49:59.400 --> 00:50:03.400]   style, I try to replicate what they have done and what they
[00:50:03.400 --> 00:50:05.760]   have taught me the first day community.
[00:50:05.760 --> 00:50:10.160]   Broadly speaking, I've watched pretty much all of the ml
[00:50:10.160 --> 00:50:13.800]   YouTubers. I really like Letitia stuff. I really like ml
[00:50:13.800 --> 00:50:17.040]   street talk. I don't understand most of it, but I still watch
[00:50:17.040 --> 00:50:20.960]   it because they're awesome. I watch Yannick's videos. So
[00:50:20.960 --> 00:50:24.600]   pretty much all of the YouTubers, ml YouTubers.
[00:50:24.600 --> 00:50:27.800]   What would you say is unique about Jeremy Howard's
[00:50:27.800 --> 00:50:32.640]   perspective and what you really like, value and appreciate in
[00:50:32.640 --> 00:50:35.280]   Jeremy Howard's because you said that you sort of love his style.
[00:50:35.280 --> 00:50:40.240]   And I'd love to hear, you know, briefly how you capture that
[00:50:40.240 --> 00:50:41.800]   maybe in a single sentence or a couple.
[00:50:46.760 --> 00:50:51.320]   He's one of the most welcoming persons in the community, and
[00:50:51.320 --> 00:50:55.440]   also very knowledgeable. And those things together are
[00:50:55.440 --> 00:50:56.600]   extremely rare.
[00:50:56.600 --> 00:50:59.160]   That's great. That's great.
[00:50:59.160 --> 00:51:05.480]   So this is this is a question that we'll come to in a moment.
[00:51:05.480 --> 00:51:08.880]   Actually, I'll wait and I'll ask that question in a moment. But
[00:51:08.880 --> 00:51:13.960]   sort of to finish off this lightning round, what change do
[00:51:13.960 --> 00:51:15.400]   you want to see in the ml community?
[00:51:16.080 --> 00:51:19.920]   Yeah, just just more, more easily accessible content, as I
[00:51:19.920 --> 00:51:20.560]   said earlier.
[00:51:20.560 --> 00:51:24.480]   Yeah. And I think I think we all know sort of what your role in
[00:51:24.480 --> 00:51:31.600]   that changes. Get out there and make it right. I don't know.
[00:51:31.600 --> 00:51:35.760]   But about that, I would, please, I would like to, you know,
[00:51:35.760 --> 00:51:38.880]   encourage people to do that if they're knowledgeable, if they
[00:51:38.880 --> 00:51:42.800]   have a knack for, you know, explaining things to people.
[00:51:42.800 --> 00:51:46.760]   If they have a knack for, you know, explaining something and
[00:51:46.760 --> 00:51:50.960]   if they get the kick out of it, just don't say, hey, I would
[00:51:50.960 --> 00:51:53.560]   like to see that video from you just do it yourself.
[00:51:53.560 --> 00:51:57.480]   It's I think we are not that many content creators in the
[00:51:57.480 --> 00:51:59.920]   machine learning community. And to follow what Sanyam was
[00:51:59.920 --> 00:52:04.480]   saying, is just, you know, more people doing this. I don't
[00:52:04.480 --> 00:52:07.840]   think the choice is so large in, you know, whom you want to
[00:52:07.840 --> 00:52:10.720]   watch. So I would like to see more diversity.
[00:52:12.400 --> 00:52:15.640]   I think that's something that we should all sort of keep in mind
[00:52:15.640 --> 00:52:18.840]   as much as possible. I think that's a really valuable
[00:52:18.840 --> 00:52:26.080]   perspective. So that brings us to a few big questions. And I'm
[00:52:26.080 --> 00:52:31.000]   going to steal from Wakes and Biases podcast here and ask a
[00:52:31.000 --> 00:52:35.400]   few questions that our founder, Lucas Biewald likes to ask. And
[00:52:35.400 --> 00:52:39.600]   the first one is sort of what's an underappreciated value of
[00:52:39.600 --> 00:52:44.080]   machine learning, or like, domain of machine learning that
[00:52:44.080 --> 00:52:46.880]   doesn't get enough attention or whatever. And so let's start
[00:52:46.880 --> 00:52:47.560]   with you, Letizia.
[00:52:47.560 --> 00:52:55.520]   Underappreciated, I think, so applications where money comes
[00:52:55.520 --> 00:52:59.240]   out easily, that's, you know, it gets a lot of attention. If you
[00:52:59.240 --> 00:53:02.640]   can get the product and you can sell it easily, that's where the
[00:53:02.640 --> 00:53:05.680]   attention is where the money is. But I think and I'm really
[00:53:05.680 --> 00:53:08.760]   excited about the future of machine learning. I think that
[00:53:08.760 --> 00:53:11.480]   I'm really excited and passionate about machine
[00:53:11.480 --> 00:53:14.440]   learning being applied to science just to help out
[00:53:14.440 --> 00:53:17.840]   science. You have a physical simulation that takes too long
[00:53:17.840 --> 00:53:21.560]   to you don't have the computers for that. I mean, you have all
[00:53:21.560 --> 00:53:25.000]   the differential equations, you could you could solve them. But
[00:53:25.000 --> 00:53:29.960]   it just takes forever. And yeah, just to apply a neural network
[00:53:29.960 --> 00:53:34.840]   to, you know, kind of hack around the solution of the
[00:53:34.840 --> 00:53:38.600]   differential equation and just estimate and don't, they usually
[00:53:38.600 --> 00:53:41.880]   don't have the perfect prediction, but they have, you
[00:53:41.880 --> 00:53:46.400]   know, they kind of predict what we should expect in general. And
[00:53:46.400 --> 00:53:49.800]   these kinds of applications where it's not, I mean, drug
[00:53:49.800 --> 00:53:52.280]   discovery is already where attention money is, but
[00:53:52.280 --> 00:53:56.000]   something like astronomy to there's a lot of data that
[00:53:56.000 --> 00:54:00.040]   nobody has the time to look at their, their all these
[00:54:00.040 --> 00:54:03.880]   telescopes are, you know, watching the sky, and it's
[00:54:03.880 --> 00:54:08.520]   actually hard to get some amount of time on that telescope as a
[00:54:08.520 --> 00:54:12.120]   astronomer. But when you did, you get a lot of pictures, and
[00:54:12.120 --> 00:54:16.560]   then you have to, you know, look around and, and label things
[00:54:16.560 --> 00:54:20.280]   and find the interesting physics over there. And, you know, when
[00:54:20.280 --> 00:54:23.520]   machine learning helps with that to pre process the data and just
[00:54:23.520 --> 00:54:26.160]   tell you, hey, scientists, please look over there and there
[00:54:26.160 --> 00:54:30.520]   and there, that's interesting stuff going on. I think that's
[00:54:30.520 --> 00:54:33.960]   that's where I, I'm really passionate about that. But it
[00:54:33.960 --> 00:54:36.600]   doesn't get so much attention yet.
[00:54:36.960 --> 00:54:41.160]   Two quick follow ups on that. The first one, you've done a
[00:54:41.160 --> 00:54:46.600]   video on sort of linear algebra and using like, like large
[00:54:46.600 --> 00:54:49.840]   models for linear algebra. And also recently, there's the quite
[00:54:49.840 --> 00:54:54.280]   famous nature paper of collaborating between Google X
[00:54:54.280 --> 00:54:59.800]   and Jordi Williamson, in sort of machine learning assisted
[00:54:59.800 --> 00:55:02.840]   mathematics research. Is that sort of the sort of things that
[00:55:02.840 --> 00:55:04.320]   you feel really passionate about?
[00:55:05.760 --> 00:55:09.880]   Yeah, it's something I really like. And it's so where people
[00:55:09.880 --> 00:55:12.960]   what were the reviewers saying for this linear algebra paper?
[00:55:12.960 --> 00:55:17.120]   Why do we need a transformer to learn the matrix transposition?
[00:55:17.120 --> 00:55:21.320]   If we can do that, computers can do that we have algorithms for
[00:55:21.320 --> 00:55:25.640]   that. And they were like, not excited about the idea itself to
[00:55:25.640 --> 00:55:30.560]   take an algorithm we know about and just, you know, have an
[00:55:30.560 --> 00:55:35.280]   sometimes approximate, it's an approximation of it. But just
[00:55:35.320 --> 00:55:39.200]   this principle that you can, I mean, you have proven the
[00:55:39.200 --> 00:55:42.680]   principle of you can do something with neural networks
[00:55:42.680 --> 00:55:45.760]   that you have actually algorithms for. And then I mean,
[00:55:45.760 --> 00:55:48.680]   if you understood that you can apply it to something we don't
[00:55:48.680 --> 00:55:51.760]   have the algorithms for. And this proof of concept that you
[00:55:51.760 --> 00:55:56.160]   can find stuff with that it's, I think it's powerful. And I am
[00:55:56.160 --> 00:55:58.760]   excited. I don't need a reason I don't need an application. I
[00:55:58.760 --> 00:56:02.200]   don't need the money out of a linear algebra solver with
[00:56:02.200 --> 00:56:05.040]   transformers. And I'm already excited about that. But it's
[00:56:05.040 --> 00:56:10.160]   hard to convince people to that science itself. So when when you
[00:56:10.160 --> 00:56:15.320]   are building a product just to get a science insight off of it,
[00:56:15.320 --> 00:56:16.600]   that that's cool, too.
[00:56:16.600 --> 00:56:21.520]   Now you sound like a mathematician, Letizia. I don't
[00:56:21.520 --> 00:56:24.920]   need a financial reason to do this work. I don't know. I think
[00:56:24.920 --> 00:56:26.480]   there's a mathematician somewhere in there.
[00:56:29.960 --> 00:56:34.200]   Siam, how about you? What's an underappreciated type of ML?
[00:56:34.200 --> 00:56:39.760]   It's understated how easy it is to get started in machine
[00:56:39.760 --> 00:56:41.200]   learning for any outsider.
[00:56:41.200 --> 00:56:47.200]   I see. So this sort of like, initial input energy is smaller
[00:56:47.200 --> 00:56:49.720]   than people think. And you think it's doesn't get enough
[00:56:49.720 --> 00:56:52.560]   attention how easy people can just get get cooking.
[00:56:52.560 --> 00:56:57.040]   Yeah, exactly. And not not just that not just getting started,
[00:56:57.040 --> 00:57:01.080]   you can like build really cool stuff, just with like, really
[00:57:01.080 --> 00:57:03.880]   small knowledge, like smaller than you would anticipate. I
[00:57:03.880 --> 00:57:05.720]   think that's one thing I'm always amazed at.
[00:57:05.720 --> 00:57:11.440]   What do you think the sort of best tool is currently for
[00:57:11.440 --> 00:57:14.760]   saying, like, I've got this AI app in my brain, and I want to
[00:57:14.760 --> 00:57:18.200]   build this AI app. And maybe I don't know, I'm not very good at
[00:57:18.200 --> 00:57:21.040]   Python, and I'm not very good at programming. And, you know,
[00:57:21.040 --> 00:57:24.160]   maybe not super experienced in mathematics. What's the tool set
[00:57:24.160 --> 00:57:26.480]   to kind of just like, get that app working?
[00:57:27.440 --> 00:57:31.800]   You'd probably expect this answer first day, just stay and
[00:57:31.800 --> 00:57:35.120]   and my passion, I think someone built this really cool app where
[00:57:35.120 --> 00:57:39.200]   they built, they will, I assume they're going to do segmentation
[00:57:39.200 --> 00:57:42.200]   lecture, and they built a pothole detector, which was
[00:57:42.200 --> 00:57:44.720]   really awesome. I mean, it's such a simple idea. And they
[00:57:44.720 --> 00:57:47.440]   just use the lecture code and they were able to build it.
[00:57:47.440 --> 00:57:49.840]   That's awesome. That's awesome. And what about things like
[00:57:49.840 --> 00:57:53.120]   deployment and sort of like making this sort of front end
[00:57:53.120 --> 00:57:56.400]   and the full stack application aspects of it. I think those
[00:57:56.400 --> 00:57:59.440]   are obviously important for people to realize their their
[00:57:59.440 --> 00:58:02.320]   goals and dreams. What do you think about that? How do you
[00:58:02.320 --> 00:58:04.680]   what's what's the right on ramp there?
[00:58:04.680 --> 00:58:08.680]   I just I'm like the hacker mindset where I'd like literally
[00:58:08.680 --> 00:58:12.320]   fork any repository on GitHub that sort of works, tweak things
[00:58:12.320 --> 00:58:15.960]   just to like barely make it work with like 50 errors left and
[00:58:15.960 --> 00:58:19.320]   right and just like make that one demo work for that initial
[00:58:19.320 --> 00:58:21.200]   happiness and then then go from there.
[00:58:21.960 --> 00:58:26.440]   That's great. That's great. Cool. And that brings us to sort
[00:58:26.440 --> 00:58:30.960]   of the final question, which is like, you know, in a lot of
[00:58:30.960 --> 00:58:36.960]   technology areas, investors and sort of like people in the
[00:58:36.960 --> 00:58:39.760]   field, they always want to know what's the next big thing. And
[00:58:39.760 --> 00:58:44.080]   so I'm curious to both of you, sort of what do you think the
[00:58:44.080 --> 00:58:46.960]   next big thing is for machine learning and artificial
[00:58:46.960 --> 00:58:48.560]   intelligence? We'll start with you, Sam.
[00:58:50.960 --> 00:58:54.280]   I think I come like we all have our biases. I'm from India, I
[00:58:54.280 --> 00:58:57.920]   think agriculture is big here. So any area where which like
[00:58:57.920 --> 00:59:02.840]   reaches to the masses, we can probably apply machine learning
[00:59:02.840 --> 00:59:05.920]   effectively there. And that's where we should try to focus on.
[00:59:05.920 --> 00:59:09.680]   So I see. And what about like types of models or sort of like
[00:59:09.680 --> 00:59:13.160]   approaches and maybe languages sort of what's the next big
[00:59:13.160 --> 00:59:15.360]   thing if you're a ML practitioner?
[00:59:17.400 --> 00:59:21.280]   I would say transfer learning. So just just with like minimal
[00:59:21.280 --> 00:59:22.680]   hardware, how can you apply anything?
[00:59:22.680 --> 00:59:25.640]   Awesome. Awesome. How about you, Leticia?
[00:59:25.640 --> 00:59:31.720]   So algorithm wise, or, you know, model wise, I expect models
[00:59:31.720 --> 00:59:36.280]   working some multimodal models working with video to be a
[00:59:36.280 --> 00:59:41.920]   really big thing because text alone is kind of solved, GPT
[00:59:42.120 --> 00:59:48.640]   three. And then we have images and text kind of solved. But
[00:59:48.640 --> 00:59:54.120]   video is a more height, it's really high dimensional compared
[00:59:54.120 --> 00:59:58.840]   to just the previous two. And I think that could be a really
[00:59:58.840 --> 01:00:03.480]   big as a where we should keep our eyes for the next big, huge
[01:00:03.480 --> 01:00:07.120]   model. And about applications.
[01:00:07.120 --> 01:00:09.320]   For the audience, would you mind defining multimodal?
[01:00:11.240 --> 01:00:15.680]   Ah, yeah, sure. Actually, it's a very, it's actually hard to
[01:00:15.680 --> 01:00:20.480]   define. But there is a Yeah, I have a video about that, too.
[01:00:20.480 --> 01:00:26.400]   But I mean, you can explain it to a five year old, like you
[01:00:26.400 --> 01:00:31.200]   have more sensory inputs to a model. So you have speed, like
[01:00:31.200 --> 01:00:36.240]   audio, you have video, you have texts, and these are each are
[01:00:36.240 --> 01:00:39.200]   all different modalities, they come in very different
[01:00:39.200 --> 01:00:41.840]   representations. And you have to kind of make sense of that.
[01:00:41.840 --> 01:00:45.600]   And we humans are multimodal, we see we hear we touch, we, you
[01:00:45.600 --> 01:00:49.240]   know, taste and all these things. And usually neural
[01:00:49.240 --> 01:00:53.280]   networks are not so they're not liking to integrate things
[01:00:53.280 --> 01:00:56.760]   coming from very different dimension, spaces.
[01:00:56.760 --> 01:00:59.560]   Thank you. And you were saying,
[01:00:59.560 --> 01:01:05.000]   yeah, the application where I think that ml will help. I mean,
[01:01:05.360 --> 01:01:09.480]   this answer is already spoiled by, you know, deep minds, I
[01:01:09.480 --> 01:01:12.760]   would call it breakthrough with nuclear fusion to the
[01:01:12.760 --> 01:01:16.240]   stabilization effects they have achieved. That's, I mean, we see
[01:01:16.240 --> 01:01:21.960]   already that we, I mean, you could build with ml
[01:01:21.960 --> 01:01:26.840]   applications, but you can also build the science or help the
[01:01:26.840 --> 01:01:32.240]   science that or have the insight then that that will help you,
[01:01:32.480 --> 01:01:38.760]   you know, achieve better health care, try to, you know, energy,
[01:01:38.760 --> 01:01:45.680]   solve energy problems. And just the science, I think ml applied
[01:01:45.680 --> 01:01:49.000]   to science will be a big thing because ml was a thing on its
[01:01:49.000 --> 01:01:51.920]   own for quite a while. And now scientists have figured out,
[01:01:51.920 --> 01:01:57.080]   hey, we could use ml tools in our research. And I think the ml
[01:01:57.080 --> 01:02:00.880]   tools are ready for that there are quite as I am was saying
[01:02:00.880 --> 01:02:04.360]   they're democratized, they're accessible, and now scientists
[01:02:04.360 --> 01:02:08.280]   can use them too. And I think there will be interesting things
[01:02:08.280 --> 01:02:11.760]   coming out from from that part and biology and all of that.
[01:02:11.760 --> 01:02:17.880]   That's awesome. That's awesome. Yeah. So, um, you know, thank
[01:02:17.880 --> 01:02:22.240]   you so much for joining us today. Sonia, do you have
[01:02:22.240 --> 01:02:24.680]   anything else you want to say to the community before we head
[01:02:24.680 --> 01:02:25.000]   out?
[01:02:25.000 --> 01:02:30.480]   Yeah, please subscribe to Laetitia's channel. It's amazing
[01:02:30.640 --> 01:02:35.240]   watch all of all of her videos. They're awesome. Find Brian on
[01:02:35.240 --> 01:02:40.000]   Twitter. He's he also shares awesome stuff. And yeah, watch
[01:02:40.000 --> 01:02:43.840]   more of her stuff at Weights and Biases. It's my job. It'll help
[01:02:43.840 --> 01:02:46.520]   me tell them I do a good job at it.
[01:02:46.520 --> 01:02:49.320]   We all know he does. We don't need to.
[01:02:49.320 --> 01:02:54.400]   Cool. And please, please help me get Brian off to host more,
[01:02:54.400 --> 01:02:57.280]   more of these. I know this was awesome. And let's let's, let's
[01:02:57.280 --> 01:03:00.000]   ask him to do more of these. So it'll be more fun then.
[01:03:00.000 --> 01:03:03.240]   Thanks a lot for hosting and for the conversation.
[01:03:03.240 --> 01:03:05.480]   Yeah. And Laetitia, do you have anything else you want to say to
[01:03:05.480 --> 01:03:07.320]   the community or anywhere you want to direct them?
[01:03:07.320 --> 01:03:14.960]   Stay curious, and start making content and videos. And yeah, I
[01:03:14.960 --> 01:03:15.880]   think that's it.
[01:03:15.880 --> 01:03:18.920]   Awesome. Awesome. Thank you both so much. This has been super
[01:03:18.920 --> 01:03:24.440]   fun. And you know, I think one final cheers is in order. So
[01:03:24.560 --> 01:03:29.120]   cheers, whatever your beverage of choice is. Laetitia, you
[01:03:29.120 --> 01:03:30.400]   really, you really missed it.
[01:03:30.400 --> 01:03:31.080]   Flushy.
[01:03:31.080 --> 01:03:33.680]   Thanks.
[01:03:33.680 --> 01:03:36.360]   I'm out of tea pretty early. Thanks. Thanks again.
[01:03:36.360 --> 01:03:38.200]   Yeah, it's actually quite late for me.
[01:03:38.200 --> 01:03:42.360]   Fair. All right. Thanks so much, everybody. Have a good
[01:03:42.360 --> 01:03:43.120]   Saturday.
[01:03:43.120 --> 01:03:45.640]   Thanks, everyone.
[01:03:45.640 --> 01:03:55.640]   [BLANK_AUDIO]

