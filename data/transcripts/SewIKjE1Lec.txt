
[00:00:00.000 --> 00:00:13.000]   And this was Friedman opining in a very quick rapid fire interview of the 14 government agencies that existed at that moment in time, which he would abolish and which he would keep.
[00:00:13.000 --> 00:00:17.000]   And, you know, we'll roll it and put a clip in here.
[00:00:17.000 --> 00:00:20.000]   Keep them or abolish them? Department of Agriculture?
[00:00:20.000 --> 00:00:21.000]   Abolish.
[00:00:21.000 --> 00:00:23.000]   Gone. Department of Commerce?
[00:00:23.000 --> 00:00:24.000]   Abolish.
[00:00:24.000 --> 00:00:26.000]   Gone. Department of Defense?
[00:00:26.000 --> 00:00:27.000]   Keep.
[00:00:27.000 --> 00:00:29.000]   Keep it? Department of Education?
[00:00:29.000 --> 00:00:30.000]   Abolish.
[00:00:30.000 --> 00:00:32.000]   Gone. Energy?
[00:00:32.000 --> 00:00:33.000]   Abolish.
[00:00:33.000 --> 00:00:37.000]   Except that energy ties in with the military.
[00:00:37.000 --> 00:00:44.000]   Well, then we shove it under defense. The little bit that handles the nuclear plutonium and so forth goes under defense, but we abolish the rest of it.
[00:00:44.000 --> 00:00:50.000]   But, you know, they asked him, Department of Agriculture? Abolish. Commerce? Abolish. Education? Abolish.
[00:00:51.000 --> 00:01:10.000]   Hey, Bill, we were in my office last night. My boys put on the cowboy hat. The cowboy hats we made down at your anniversary.
[00:01:10.000 --> 00:01:12.000]   Yes. It looks great on you.
[00:01:12.000 --> 00:01:14.000]   It's great.
[00:01:14.000 --> 00:01:16.000]   You should wear it for the whole episode.
[00:01:16.000 --> 00:01:24.000]   It's good to be here. It's kind of a surprise. This morning I got on the plane to fly up to Seattle.
[00:01:24.000 --> 00:01:29.000]   I was going to do a pod with Satya, as you know, and that we were going to do our reaction pod.
[00:01:29.000 --> 00:01:34.000]   And a typhoon or something hit Seattle last night, literally like halfway up there.
[00:01:34.000 --> 00:01:39.000]   I got word that Microsoft has no power. Nobody has any power. Trees are down.
[00:01:39.000 --> 00:01:45.000]   So I did a U-turn and wished them well. I'm going to go back up there on Monday and do a pod with Satya.
[00:01:45.000 --> 00:01:52.000]   But given that, I couldn't wait to get together with you and kick around all the things that we've been sharing back and forth over the last couple of weeks.
[00:01:52.000 --> 00:01:54.000]   Lots of stuff happened.
[00:01:54.000 --> 00:02:01.000]   No doubt about it. One of the things certainly popular in our threads has been AI scaling laws.
[00:02:01.000 --> 00:02:07.000]   You know, like, are we beginning to see models top out, particularly on pre-training?
[00:02:07.000 --> 00:02:15.000]   In fact, there was this Bloomberg headline, "OpenAI, Google, and Anthropic struggle to build more advanced AI."
[00:02:15.000 --> 00:02:26.000]   It then goes on to say that Orion or Chat GPT-5, Gemini, and 3.5 Opus are all falling short of internal pre-training targets.
[00:02:26.000 --> 00:02:36.000]   And then they quoted Dario as saying, "Scaling laws are not actually laws of the universe, but they are simply empirical regularities."
[00:02:36.000 --> 00:02:42.000]   He said, "I'm going to be in favor of them continuing, but I'm not certain of that."
[00:02:42.000 --> 00:02:43.000]   Yeah.
[00:02:43.000 --> 00:02:51.000]   So what's been on your mind about whether or not we are, in fact, seeing continued gains from pre-training?
[00:02:51.000 --> 00:02:54.000]   I would add a few other things.
[00:02:54.000 --> 00:03:00.000]   Ilya was also quoted as questioning whether the scaling laws were continued.
[00:03:00.000 --> 00:03:06.000]   And for whatever reason, Mark and Ben over at Dendres and Horowitz also made the same statement.
[00:03:06.000 --> 00:03:13.000]   So in a very short window, we got this point of view from quite a few number of people.
[00:03:13.000 --> 00:03:21.000]   I would add that in Dario's five-hour interview with Lex, which I only grabbed pieces of from the transcript,
[00:03:21.000 --> 00:03:27.000]   but he was very positive on scaling laws in that interview, which just recently dropped.
[00:03:27.000 --> 00:03:35.000]   So there may be a disagreement between some people, but the breadth of the feedback suggests something's up.
[00:03:35.000 --> 00:03:37.000]   I think something's worth paying attention to.
[00:03:37.000 --> 00:03:45.000]   And I would add that there were people that raised this question early on.
[00:03:45.000 --> 00:03:48.000]   And I think it's specific to LLMs.
[00:03:48.000 --> 00:03:51.000]   I don't think you should say AI scaling laws.
[00:03:51.000 --> 00:03:56.000]   I think there was always a question about whether LLMs would run out of steam.
[00:03:56.000 --> 00:04:01.000]   And that was tied to three different things, which you and I talked about on July 11th.
[00:04:01.000 --> 00:04:05.000]   And I had even raised this question a year ago.
[00:04:05.000 --> 00:04:08.000]   One is, will the parameter count run out?
[00:04:08.000 --> 00:04:16.000]   Historically, mathematical algorithms that do some type of fit, and this is a very sophisticated form of that,
[00:04:16.000 --> 00:04:22.000]   but when you take the variables up to a certain level, it stops adding value.
[00:04:22.000 --> 00:04:24.000]   You just get too close to the fit.
[00:04:24.000 --> 00:04:27.000]   There's a question about how big the context window will be.
[00:04:27.000 --> 00:04:30.000]   And that came up on the NVIDIA call today.
[00:04:30.000 --> 00:04:31.000]   I think we can talk about that.
[00:04:31.000 --> 00:04:33.000]   And then data was brought up.
[00:04:33.000 --> 00:04:39.000]   And a lot of people, I know this was a big point Melanie Mitchell had raised, like, are we just going to run out of data?
[00:04:39.000 --> 00:04:48.000]   And there was pushback that there's synthetic data, but there's been papers published that show synthetic data creates a lot of chaos.
[00:04:48.000 --> 00:04:53.000]   And so I do think there were people that were thinking this might happen.
[00:04:53.000 --> 00:05:00.000]   And there was also a belief, and I share this belief, you don't have to, so we can disagree on this,
[00:05:00.000 --> 00:05:10.000]   that there was a argument being made by, you know, OpenAI and Anthropic that, oh, you're just going to see the next number drop,
[00:05:10.000 --> 00:05:15.000]   and the next number is going to be way better than the last number, and that's going to happen routinely.
[00:05:15.000 --> 00:05:23.000]   And they both said, you know, we spent whatever, a billion on this model, and we're going to spend 10, and then we're going to spend 100.
[00:05:23.000 --> 00:05:28.000]   And that implies, I would say, at least linear scaling or maybe above.
[00:05:28.000 --> 00:05:30.000]   And they all said that.
[00:05:30.000 --> 00:05:42.000]   And so if the comment, you know, that you read earlier is true and that they're not getting the benefit, there are implications.
[00:05:42.000 --> 00:05:45.000]   You know, it doesn't mean AI is in trouble or AI is done.
[00:05:45.000 --> 00:05:51.000]   I mean, there's tons of positive AI news out there, but it may mean we're shifting directions.
[00:05:51.000 --> 00:05:57.000]   And it's worth, I think, talking about, well, if this is true, what are the implications?
[00:05:57.000 --> 00:05:58.000]   Right, no doubt about it.
[00:05:58.000 --> 00:06:04.000]   You know, and Jensen did just say on the NVIDIA call, he basically said there is no slowdown,
[00:06:04.000 --> 00:06:11.000]   but he did say that foundation model pre-training is only one vector of scaling.
[00:06:11.000 --> 00:06:18.000]   You obviously have post-training scaling, and now we have inference time reasoning that will scale.
[00:06:18.000 --> 00:06:21.000]   We have data and data quality that will scale.
[00:06:21.000 --> 00:06:29.000]   So I think that, you know, it is a question as we look at in the early phases over the last 24 months,
[00:06:29.000 --> 00:06:34.000]   everybody was mesmerized by these evals on pre-training.
[00:06:34.000 --> 00:06:37.000]   And then I think a lot of people mistakenly, and we'll get to this later,
[00:06:37.000 --> 00:06:43.000]   a lot of people mistakenly said if the pre-training ever slows down, this is horrible for NVIDIA,
[00:06:43.000 --> 00:06:48.000]   because the only reason NVIDIA is going up is because people are buying these big clusters to do pre-training,
[00:06:48.000 --> 00:06:51.000]   which also isn't entirely true.
[00:06:51.000 --> 00:06:58.000]   But I do think it's important to recognize, right, that as you scale up pre-training,
[00:06:58.000 --> 00:07:01.000]   a lot of the low-hanging fruit was plucked.
[00:07:01.000 --> 00:07:09.000]   And so it makes sense to me that you're seeing a deceleration in the rate of improvement of the pre-training.
[00:07:09.000 --> 00:07:15.000]   But when you look in the aggregate at all these various vectors on which we're scaling intelligence
[00:07:15.000 --> 00:07:20.000]   on a combinatorial basis, it may be just as good or even better.
[00:07:20.000 --> 00:07:24.000]   But I think it's important to your point, like let's steel man the argument.
[00:07:24.000 --> 00:07:31.000]   Let's assume that we do, you know, kind of run out of improvements or see a significant deceleration
[00:07:31.000 --> 00:07:35.000]   in the pre-training improvements on these models.
[00:07:35.000 --> 00:07:37.000]   What do you think that means?
[00:07:37.000 --> 00:07:40.000]   Like who are the people who are hurt by that?
[00:07:40.000 --> 00:07:43.000]   Who are the people who are helped by that?
[00:07:43.000 --> 00:07:50.000]   The first thing that comes to my mind, and once again I'm not asking you to agree with this,
[00:07:50.000 --> 00:08:00.000]   it's just like what's in my head, is that this is not the expectation that Sam and Dario had 12 months ago.
[00:08:00.000 --> 00:08:05.000]   Like because they were promoting a thesis that it was just going to keep going up and up,
[00:08:05.000 --> 00:08:11.000]   and they were willing to put 10x over and over again on the size of the cluster they were going to train on.
[00:08:11.000 --> 00:08:17.000]   And if it's a shift, and it may very well be a shift that leads to insane innovation
[00:08:17.000 --> 00:08:20.000]   and continued growth, blah, blah, blah, blah, blah.
[00:08:20.000 --> 00:08:22.000]   I still think it's a surprise shift.
[00:08:22.000 --> 00:08:25.000]   Like I don't think it was what was expected.
[00:08:25.000 --> 00:08:29.000]   And so I think that's worth acknowledging if that's what happened.
[00:08:29.000 --> 00:08:33.000]   Two other things that I think are, let's just call them questions that are raised.
[00:08:33.000 --> 00:08:41.000]   One, the NVIDIA differentiation, as we've talked about, is greatest at the largest cluster size.
[00:08:41.000 --> 00:08:49.000]   So if pre-training, once again I only equate this to LLMs, you know, I don't think of FSD problem,
[00:08:49.000 --> 00:08:55.000]   I think that that is more of a core AI problem, and that thing may scale way beyond where it is today.
[00:08:55.000 --> 00:09:04.000]   But for LLMs at least, if we've hit a cap, then, you know, does that impact NVIDIA demand at all for the bigger systems?
[00:09:04.000 --> 00:09:11.000]   And then there's a differentiation question, which is if the world shifts to inference sooner or faster,
[00:09:11.000 --> 00:09:19.000]   and if the path of differentiation for the language model companies becomes more inference-based than pre-training-based,
[00:09:19.000 --> 00:09:23.000]   what does that mean for the hardware systems that people are using?
[00:09:23.000 --> 00:09:29.000]   And, yeah, and then the third thing, and I'll go quick and you can respond, is, you know,
[00:09:29.000 --> 00:09:37.000]   might a ceiling on that pre-training allow other people to catch up faster with open AI?
[00:09:37.000 --> 00:09:48.000]   And there was an article out today about a, like, pseudo-Chinese model that was performing at or near some of the latest--
[00:09:48.000 --> 00:09:49.000]   Deep-seek.
[00:09:49.000 --> 00:09:51.000]   Deep-seek, I think is what you're referring to.
[00:09:51.000 --> 00:09:57.000]   So anyway, those would be the three things that I would say that you would want to analyze if you thought that was true.
[00:09:57.000 --> 00:10:03.000]   We've spent a lot of time talking with, for example, the CEOs of most of the hyperscalers, if not all of them.
[00:10:03.000 --> 00:10:07.000]   And then I listened to Jensen on the call today, right?
[00:10:07.000 --> 00:10:12.000]   They called Hopper Demand exceptional, and this is the very tail end.
[00:10:12.000 --> 00:10:19.000]   We may have-- people thought Hopper Demand would be over by now, frankly, and he said it's going to continue well into next year.
[00:10:19.000 --> 00:10:31.000]   And interesting, they called Blackwell Demand staggering and said that, you know, they're increasing the production even against their prior predictions, right?
[00:10:31.000 --> 00:10:34.000]   They beat the quarter in terms of revenue.
[00:10:34.000 --> 00:10:43.000]   So the action on the street seems to support Jensen's words on the call, which is there's no slowdown, right?
[00:10:43.000 --> 00:10:46.000]   Sam tweeted, I think there is no wall.
[00:10:46.000 --> 00:10:50.000]   And so there is a debate here, and some of it may be semantics, right?
[00:10:50.000 --> 00:10:56.000]   Like, I definitely think the second derivative is slowing, right?
[00:10:56.000 --> 00:10:58.000]   But you're still seeing gains.
[00:10:58.000 --> 00:11:07.000]   And I think the implication we've hit a wall suggests no gains and that there are no different vectors of scaling that intelligence.
[00:11:07.000 --> 00:11:20.000]   So I think at least as we look at 2025 for NVIDIA, for the supply chain, and for the model companies themselves, we think we're going to continue to see improvements in the models.
[00:11:20.000 --> 00:11:21.000]   It won't all come from pre-training.
[00:11:21.000 --> 00:11:30.000]   We think there's an increasing amount coming from post-training, from data quality, and then obviously from this inference time reasoning.
[00:11:30.000 --> 00:11:42.000]   I would add to what you said. I mean, I listened to the call. Yeah, there was certainly no concern raised by Jensen about either his demand or his sense of what was going on at the architectural level.
[00:11:42.000 --> 00:11:47.000]   And Microsoft has been increasingly boisterous as well.
[00:11:47.000 --> 00:11:54.000]   If this thing is true, it certainly goes against what Kevin Scott was saying on a podcast four or five months ago.
[00:11:54.000 --> 00:12:01.000]   And so I doubt that even if it were true that it would play out in the numbers as fast as we're talking about.
[00:12:01.000 --> 00:12:04.000]   This stuff just came out like weeks ago.
[00:12:04.000 --> 00:12:07.000]   But I wouldn't ignore it. I think it's important to understand.
[00:12:07.000 --> 00:12:19.000]   I think it's important to know what the drivers are of incremental investment and incremental use and incremental performance and competitiveness and to see where that's going.
[00:12:19.000 --> 00:12:26.000]   And I think there are a lot of questions that we need to ask and understand about inference.
[00:12:26.000 --> 00:12:37.000]   Do models that are, as I understand it from the startups that we work with, you can train a model on a big NVIDIA cluster and you can run it on a different system.
[00:12:37.000 --> 00:12:48.000]   And there are certainly performance tests that are in the public that show the Groxon, Cerebris, and other TPUs, including the ones at Google, outperforming on inference.
[00:12:48.000 --> 00:13:00.000]   Now, on the call, Jensen said something that I hadn't heard him say before, which is he talked about context windows potentially growing really large.
[00:13:00.000 --> 00:13:06.000]   And that might then require a larger cluster, which might bring a point.
[00:13:06.000 --> 00:13:12.000]   Prior on the previous call, Jensen had just said, well, the reason you'll use us for inference is because you've got old machines laying around.
[00:13:12.000 --> 00:13:22.000]   This was a step in a different direction saying, well, if context windows are huge, we might be differentiated yet again here with larger systems.
[00:13:22.000 --> 00:13:26.000]   So I think those are the things that people need to ask and figure out.
[00:13:26.000 --> 00:13:27.000]   There's no doubt.
[00:13:27.000 --> 00:13:41.000]   I think one of the implications of what you're saying is as we looked at pre-training, we basically watched what appeared to be the increasing commoditization of the pre-training advantage.
[00:13:41.000 --> 00:13:48.000]   Everybody was catching up pretty quickly, even people who we didn't think were necessarily in the lead pack.
[00:13:48.000 --> 00:14:08.000]   So if, in fact, we're seeing a deceleration in the rate of improvement, if that's becoming more commodity, then it shifts the balance of power to post-training, which really relies on quality of data and other vectors of scaling intelligence like inference time reasoning.
[00:14:08.000 --> 00:14:20.000]   And those things may be less commodity either because you have a data flywheel that's better than your competitors or because you've just had some breakthroughs architecturally on other ways to scale intelligence.
[00:14:20.000 --> 00:14:33.000]   And so I think 2025 is going to cast more light on those things as well as improvements into the core product like we've talked about before around memory and actions.
[00:14:33.000 --> 00:14:34.000]   Yeah.
[00:14:34.000 --> 00:14:37.000]   And there was a lot of talk on memory in the past week.
[00:14:37.000 --> 00:14:43.000]   So both Google, Jim, and I released a model of their chatbot that has memory.
[00:14:43.000 --> 00:14:49.000]   And Mustafa at Microsoft dropped the phrase "near infinite memory."
[00:14:49.000 --> 00:14:55.000]   Now, putting "near" next to the word "infinite" I think is a bit of an oxymoron.
[00:14:55.000 --> 00:14:57.000]   You could say I'm going to live near forever.
[00:14:57.000 --> 00:15:01.000]   And I'm not sure what that means.
[00:15:01.000 --> 00:15:04.000]   Clever use of clever words of semantics.
[00:15:04.000 --> 00:15:10.000]   But they seem super excited about what they've unlocked in memory.
[00:15:10.000 --> 00:15:20.000]   And I've talked for a long time about I think that the more memory one of these systems can have, the way more useful it becomes to each individual that uses it.
[00:15:20.000 --> 00:15:24.000]   So I look forward to seeing what they have and seeing when they release that.
[00:15:24.000 --> 00:15:35.000]   You know, I would just make one comment kind of over again on if this scaling wall or ceiling is starting to arise.
[00:15:35.000 --> 00:15:48.000]   There was talk prior to now, or if you assume that weren't true, that these companies were just going to train on bigger and bigger and bigger systems.
[00:15:48.000 --> 00:15:54.000]   They said, oh, someone will train a model on a $100 billion system one day.
[00:15:54.000 --> 00:15:58.000]   If it's true that it's top, that's not going to happen anymore.
[00:15:58.000 --> 00:16:05.000]   And so that is a-- it doesn't mean that NVIDIA may still be sold out till 2030.
[00:16:05.000 --> 00:16:13.000]   But there was this notion that they were just going to be constantly building larger model for pre-training LLM.
[00:16:13.000 --> 00:16:16.000]   And that may be off the table if this is true.
[00:16:16.000 --> 00:16:23.000]   And let me suggest as an investor, at least in one of them, in open AI, it may not be a bad thing that that's off the table.
[00:16:23.000 --> 00:16:25.000]   They don't have to build that system.
[00:16:25.000 --> 00:16:26.000]   Right?
[00:16:26.000 --> 00:16:29.000]   I mean, I do think there was this question around--
[00:16:29.000 --> 00:16:31.000]   That's a fair point.
[00:16:31.000 --> 00:16:34.000]   --returns and where the money was going to come from.
[00:16:34.000 --> 00:16:48.000]   And I'm not worried about it from the perspective of NVIDIA because I think, again, while we're all hyper-focused on these evals, right, the real world people are focused on use cases.
[00:16:48.000 --> 00:16:55.000]   So if you break down consumer and enterprise here for a second, right, you heard Jensen referring to this a lot on the call.
[00:16:55.000 --> 00:17:03.000]   Remember, Microsoft has said of its $10 billion in AI revenue, it's almost all inference.
[00:17:03.000 --> 00:17:07.000]   We know NVIDIA has said half of their revenue is inference.
[00:17:07.000 --> 00:17:12.000]   And he was asked on the call if that mix would become higher in the coming years.
[00:17:12.000 --> 00:17:15.000]   And he said, I expect it, and it'll be--
[00:17:15.000 --> 00:17:17.000]   He said he hopes it does.
[00:17:17.000 --> 00:17:22.000]   And it will be great for the world when it does because it'll mean a lot more people are getting benefits from it.
[00:17:22.000 --> 00:17:28.000]   But let's talk about this vector around consumer AI, right?
[00:17:28.000 --> 00:17:32.000]   You mentioned Mustapha referring to infinite memory.
[00:17:32.000 --> 00:17:33.000]   It's very clear now.
[00:17:33.000 --> 00:17:36.000]   He said 2025 is going to be the year of memory.
[00:17:36.000 --> 00:17:51.000]   You and I started off this year hoping that 2024 might be the year of memory because we think these agents are going to become infinitely more useful to us when we don't have to prompt them about all the things we've done in the past.
[00:17:51.000 --> 00:17:55.000]   And we've also seen everybody begin to roll out actions.
[00:17:55.000 --> 00:17:57.000]   Every company now-- Gemini has talked about it.
[00:17:57.000 --> 00:18:00.000]   We saw computer use out of Enthropic.
[00:18:00.000 --> 00:18:10.000]   I took note that Sam said recently that tools and agents that can do things like book airline tickets are at least as important as better models.
[00:18:10.000 --> 00:18:17.000]   We will have better and better models, but I think the next giant breakthrough will be with agents and their actions.
[00:18:17.000 --> 00:18:21.000]   I think that these, again, they're not either/or, right?
[00:18:21.000 --> 00:18:27.000]   The core models will get more capable, multi-modality, inference time reasoning, post-training.
[00:18:27.000 --> 00:18:38.000]   But I also think we finally see out of these companies a real cadence around product improvements, memory, actions, voice, voice, right?
[00:18:38.000 --> 00:18:48.000]   That just dramatically increase the utility of these things, which then leads to massive increase in token production or inference.
[00:18:48.000 --> 00:18:50.000]   And that inference has got to run somewhere, right?
[00:18:50.000 --> 00:18:57.000]   So that doesn't-- and then when I look at ChatGPT, you could see the numbers that are out there publicly.
[00:18:57.000 --> 00:19:00.000]   It certainly does not seem to be slowing down.
[00:19:00.000 --> 00:19:02.000]   In fact, if anything, accelerating.
[00:19:02.000 --> 00:19:10.000]   So I think those are indicators, right, because I've heard that Claude and other models are doing-- Perplexity are doing well as well.
[00:19:10.000 --> 00:19:16.000]   I think those are indicators on the consumer side, right, that consumers are finding more and more utility.
[00:19:16.000 --> 00:19:24.000]   And I think 25 with these product improvements, I think that could be another step function unlock in terms of consumer use.
[00:19:24.000 --> 00:19:26.000]   And enterprise use.
[00:19:26.000 --> 00:19:37.000]   I mean, all those things, memory, voice, they can be utilized in an enterprise fashion for sure and will be.
[00:19:37.000 --> 00:19:41.000]   And so, yeah, I think you're right about that.
[00:19:41.000 --> 00:19:47.000]   I think those innovations will matter for the exact same reason you said, which is utility will go up.
[00:19:47.000 --> 00:19:55.000]   I think Snowflake said on their call tonight, 3,200 of their customers, so they have about 10,000 enterprise customers,
[00:19:55.000 --> 00:20:02.000]   3,200 of them are using some of their AI use cases, AI products now in daily use.
[00:20:02.000 --> 00:20:11.000]   So again, starting to see some of that indication, I noted that Microsoft, which is on a $66 billion run rate at Azure,
[00:20:11.000 --> 00:20:16.000]   has said they expect Q1 and Q2 Azure revenue to accelerate.
[00:20:16.000 --> 00:20:17.000]   Think about that, Bill.
[00:20:17.000 --> 00:20:25.000]   Azure is growing at 34% on a $66 billion base, and they say they expect it to accelerate.
[00:20:25.000 --> 00:20:29.000]   So it's really significant growth on big software businesses.
[00:20:29.000 --> 00:20:36.000]   And I think what we've seen in the season is that software certainly has not decelerated.
[00:20:36.000 --> 00:20:41.000]   If anything, it's accelerated after a couple years of digestion, I think, around this.
[00:20:41.000 --> 00:20:48.000]   But, you know, if I had one takeaway having watched all of this, I would agree with you.
[00:20:48.000 --> 00:20:52.000]   And you've been, you know, rattling my cage about this for a year.
[00:20:52.000 --> 00:21:00.000]   And I think you've been proven at least partially right that we're seeing a deceleration in the rate of improvement on pre-training.
[00:21:00.000 --> 00:21:07.000]   But my conclusion is I don't think it necessarily matters to the consumption on the NVIDIA side.
[00:21:07.000 --> 00:21:14.000]   Certainly it would be higher, it could be even more, I suppose, if they were able to produce that many more, which I don't think they can.
[00:21:14.000 --> 00:21:18.000]   But I think that they're, you know, still in a very strong position.
[00:21:18.000 --> 00:21:29.000]   And I think these models, I think, you know, for example, OpenAI could double, triple the number of users of ChatGPT with, frankly, not making any improvements in the core model.
[00:21:29.000 --> 00:21:34.000]   Because consumers today barely scratch the surface on core model capabilities.
[00:21:34.000 --> 00:21:43.000]   Product improvements like voice, memory, and actions, I think, in and of themselves could drive tremendous traction and revenue increases in that.
[00:21:43.000 --> 00:21:47.000]   But I do think they'll still continue to see some of those model improvements.
[00:21:47.000 --> 00:22:03.000]   Maybe one area that I know that we've actually seen a tremendous return, I think, on the investment made in AI, and we saw some news this week, is on national full self-driving regulation, right?
[00:22:03.000 --> 00:22:13.000]   So the Trump administration announced that they want the DOT to develop a national framework to regulate self-driving rather than the state-by-state framework, right?
[00:22:13.000 --> 00:22:18.000]   So most states, I think, require certain licensure to be able to do it.
[00:22:18.000 --> 00:22:20.000]   Texas and Florida have no restrictions.
[00:22:20.000 --> 00:22:27.000]   But at least the indications out of the administration were that they wanted a national policy, which makes sense to me.
[00:22:27.000 --> 00:22:33.000]   I think that's very beneficial to Tesla and to Waymo and to anybody else who's in the full self-driving game.
[00:22:33.000 --> 00:22:40.000]   You get to deal with one regulatory authority rather than trying to, you know, deal with the complex nature of 50 of these folks.
[00:22:40.000 --> 00:22:43.000]   That happened at about the same time.
[00:22:43.000 --> 00:22:51.000]   Our team's doing a tremendous amount of work around FSD 13, which we expect to roll out in the next few weeks, OK?
[00:22:51.000 --> 00:22:57.000]   So you and I did the pod earlier in the year, Bill, on FSD 12.3.
[00:22:57.000 --> 00:23:06.000]   That's a big breakthrough as the first imitation learning model really moving away from these deterministic models where we saw a step function in terms of capability.
[00:23:06.000 --> 00:23:15.000]   Elon had said they were going from making 10 percent improvements a year to 10x improvements in a much shorter period of time.
[00:23:15.000 --> 00:23:17.000]   So we know about FSD 13.
[00:23:17.000 --> 00:23:21.000]   It's the first model they've trained on the Austin supercluster, right?
[00:23:21.000 --> 00:23:25.000]   Parameter size is about five times bigger.
[00:23:25.000 --> 00:23:31.000]   And what I hear is that Elon's priorities around it are safety, safety, safety, safety, right?
[00:23:31.000 --> 00:23:39.000]   He wants different driver to be able to understand different driver modalities in different situations, right?
[00:23:39.000 --> 00:23:49.000]   So that you can you know, the metric which they use is MPCI, miles per critical disengagement, OK?
[00:23:49.000 --> 00:24:03.000]   And my partner, Frida, tweeted a couple of weeks ago that at the start of this year, we saw 12.1 come out.
[00:24:03.000 --> 00:24:12.000]   By the middle of this year, when they released 12.5, it showed 100x improvement in miles per critical disengagement.
[00:24:12.000 --> 00:24:22.000]   And then with the launch of FSD 13 in the next couple of weeks, we expect another 10x improvement in terms of that MPCI.
[00:24:22.000 --> 00:24:25.000]   But here was the crazy stat for me.
[00:24:25.000 --> 00:24:32.000]   So you're already at 1000x better than we were at the start of this year in terms of miles per critical engagement.
[00:24:32.000 --> 00:24:41.000]   Now, if you do some back of the envelope, which she did in her tweet, and Elon retweeted it, at launch in Q4 of this year,
[00:24:41.000 --> 00:24:46.000]   it looks like version 13 will be somewhere around 25,000 miles per critical disengagement.
[00:24:46.000 --> 00:24:50.000]   And we estimate that Waymo is right around that same level, maybe 20,000 miles.
[00:24:50.000 --> 00:24:54.000]   So they're already caught up, we think, with Waymo.
[00:24:54.000 --> 00:25:03.000]   And at the current rate of improvement, in Q1 of next year, they could be at 100,000 miles per critical disengagement.
[00:25:03.000 --> 00:25:07.000]   By the middle of next year, 500,000 miles per critical disengagement.
[00:25:07.000 --> 00:25:17.000]   Now, just to put this in perspective, there's a human accident every 500,000 miles that's self-reported with human drivers.
[00:25:17.000 --> 00:25:26.000]   So if 2024 was the chat GPT moment for full self-driving bill, where we started seeing Waymos on every street corner in San Francisco,
[00:25:26.000 --> 00:25:33.000]   where you and I had our, you know, and Michael Dell tweeted about it, our Tesla FSD 12.3 moment,
[00:25:33.000 --> 00:25:42.000]   then if this was the chat GPT year, I think next year really is the year of achievement of a safety standard
[00:25:42.000 --> 00:25:47.000]   that allows RoboTaxi to go into action, probably in Q2.
[00:25:47.000 --> 00:25:49.000]   They'll pick a couple cities.
[00:25:49.000 --> 00:25:54.000]   And that doesn't mean that you can pull the steering wheel out of every Tesla,
[00:25:54.000 --> 00:25:59.000]   because, you know, Elon has said we're not there yet from a safety perspective.
[00:25:59.000 --> 00:26:07.000]   But the way they'll do it for a RoboTaxi is you can put the car on RoboTaxi mode.
[00:26:07.000 --> 00:26:09.000]   So think about it this way.
[00:26:09.000 --> 00:26:12.000]   If I'm driving down Sand Hill Road out here, I have an expectation.
[00:26:12.000 --> 00:26:13.000]   I'm sitting behind the wheel.
[00:26:13.000 --> 00:26:16.000]   I have an expectation as to how fast I'm going to go.
[00:26:16.000 --> 00:26:21.000]   And if the Tesla is not going that fast, I'm going to disengage and drive the car, right?
[00:26:21.000 --> 00:26:23.000]   So because I'm sitting behind the wheel.
[00:26:23.000 --> 00:26:25.000]   Now imagine I'm in a RoboTaxi.
[00:26:25.000 --> 00:26:27.000]   I'm sitting in the back.
[00:26:27.000 --> 00:26:28.000]   I'm leaning back.
[00:26:28.000 --> 00:26:29.000]   I'm on my phone.
[00:26:29.000 --> 00:26:31.000]   I'm watching something.
[00:26:31.000 --> 00:26:38.000]   I will tolerate, just like I tolerate in a Waymo, a different speed of driving, right, a different safety standard, if you will.
[00:26:38.000 --> 00:26:43.000]   So I think that's how they get safe enough in a couple markets to be able to drive RoboTaxi,
[00:26:43.000 --> 00:26:47.000]   is to just put it in a safer driving mode.
[00:26:47.000 --> 00:26:49.000]   You know, and I've watched these cars do it.
[00:26:49.000 --> 00:26:53.000]   They modulate modes depending upon the type of streets that they're on already.
[00:26:53.000 --> 00:27:01.000]   So that for me was paying attention to that regulation combined with the improvements in FSD-13.
[00:27:01.000 --> 00:27:06.000]   I think that's going to be big news as we look forward at 2025.
[00:27:06.000 --> 00:27:12.000]   There's a couple things I would respond to that I think are going to have to play out.
[00:27:12.000 --> 00:27:18.000]   First of all, as of now, Tesla hasn't had cars driving around without drivers picking up people.
[00:27:18.000 --> 00:27:24.000]   And Waymo's got years under their belt doing that, and there's going to be some learnings that come from that.
[00:27:24.000 --> 00:27:25.000]   There just are.
[00:27:25.000 --> 00:27:33.000]   And so they're going to -- I think I read that they're going to maybe start doing that in the next few months or something like that.
[00:27:33.000 --> 00:27:39.000]   But in the states that are allowing Waymo, you have to apply, you have to get approval.
[00:27:39.000 --> 00:27:41.000]   So there's a step that they need to take.
[00:27:41.000 --> 00:27:44.000]   And I think there's some natural learning that comes from that.
[00:27:44.000 --> 00:27:53.000]   I mean, you can hear how they talk about SpaceX, and there's a similar thing here, like until you get the cars out on the road.
[00:27:53.000 --> 00:28:04.000]   Most people believe Waymo still has oversight from a data center somewhere where people are watching when engagements happen or disengagements happen so they can take over and pay attention.
[00:28:04.000 --> 00:28:07.000]   And so is that same infrastructure going to be built at Tesla?
[00:28:07.000 --> 00:28:09.000]   I don't know.
[00:28:09.000 --> 00:28:13.000]   That might be an important thing as well.
[00:28:13.000 --> 00:28:17.000]   To the national standard, I unquestionably think it can be helpful.
[00:28:17.000 --> 00:28:26.000]   There are states in this fine country of ours that like to override regardless of a national standard, most notably California.
[00:28:26.000 --> 00:28:32.000]   And so that would -- but of course it would make more sense for this to be done nationally.
[00:28:32.000 --> 00:28:35.000]   And so that would be great if they stepped in.
[00:28:35.000 --> 00:28:38.000]   I had always been a big proponent of open source in this area.
[00:28:38.000 --> 00:28:45.000]   There's a lot of stuff that could be beneficial from a safety standpoint if everyone were doing it the exact same way.
[00:28:45.000 --> 00:28:55.000]   And I could even imagine a communication network, the status of a traffic light, for example, should -- there's no need for that to be inferred.
[00:28:55.000 --> 00:29:00.000]   I mean, we're going to use how much compute to look at the light and determine whether it's red or green.
[00:29:00.000 --> 00:29:02.000]   That's a state machine.
[00:29:02.000 --> 00:29:04.000]   It's either red or green, and that can be known.
[00:29:04.000 --> 00:29:25.000]   And so you could, as you build these intelligent systems, start to have the state of different things, which could include, you know, road openings and road closures and, you know, construction work that's going on and traffic and all kind of other things that should be built into the system as well.
[00:29:25.000 --> 00:29:29.000]   So I do think that -- I do think a national kind of standard could be great.
[00:29:29.000 --> 00:29:31.000]   It could also be really bad.
[00:29:31.000 --> 00:29:39.000]   Like, if you get overregulation and this massive, you know, stuff that -- and the incumbents might just use it to protect themselves.
[00:29:39.000 --> 00:29:53.000]   But my own belief is that it's -- you know, you've got Waymo, who's got the most cars running autonomously on the street right now, Tesla, who's got all the advantages you just talked about.
[00:29:53.000 --> 00:29:57.000]   You've got Uber, who's got this huge network of users.
[00:29:57.000 --> 00:30:04.000]   I'm less certain who the next player is after those three that's got a strong argument right now.
[00:30:04.000 --> 00:30:07.000]   Oh, actually, that is in the U.S.
[00:30:07.000 --> 00:30:15.000]   Obviously, China has tons of autonomous driving, you know, products on the street as we speak.
[00:30:15.000 --> 00:30:30.000]   Well, let's talk about that because I think that's an interesting question, you know, kind of a market structure question that I've been thinking a lot about lately, Bill, certainly on the public fund side of our business, right?
[00:30:30.000 --> 00:30:45.000]   When I have a disruption of this magnitude in an industry that's a multitrillion-dollar industry that has, you know, 20, 30 different public players in the supply chain, OEMs, et cetera, like, I just start thinking, who are the winners and who are the losers here, right?
[00:30:45.000 --> 00:30:54.000]   Exactly how I was feeling in 2008, you see the iPhone, you know, you've got Nokia, you've got Motorola, you've got BlackBerry, you've got, you know, all this stuff.
[00:30:54.000 --> 00:30:56.000]   And you're like, somebody's winning and somebody's losing.
[00:30:56.000 --> 00:31:01.000]   I remember my mother didn't understand the concept of shorting, right?
[00:31:01.000 --> 00:31:14.000]   And 2009, I tell her to buy Apple and to short Nokia, and that was my, you know, kind of canonical example for her, you know, and it worked out okay.
[00:31:14.000 --> 00:31:17.000]   It actually went against her first, but then it worked out okay.
[00:31:17.000 --> 00:31:19.000]   Let's just talk about OEMs for a second.
[00:31:19.000 --> 00:31:45.000]   I think the consensus view in the world is that all these OEMs, so I'm talking the U.S. OEMs, the Stellantis, the GMs, the Fords, the Europeans, the Volkswagens, the BMWs, the Mercedes, the Japanese, the Toyotas, et cetera, the Chinese, that FSD becomes a commodity and that they all end up with some version of it.
[00:31:45.000 --> 00:31:53.000]   Maybe there's one provider like NVIDIA or, you know, Mobileye or somebody else who's providing this capability into these cars.
[00:31:53.000 --> 00:32:06.000]   And that, you know, think of that as maybe like this Android version that they're all going to, this box they're going to plug in their cars, and they're all going to go compete, you know, as though they were before with Tesla.
[00:32:06.000 --> 00:32:08.000]   That's one version of the world.
[00:32:08.000 --> 00:32:18.000]   The other version of the world is that this is a tectonic disruption, that Tesla currently sells 2 million of the world's 80 million cars.
[00:32:18.000 --> 00:32:22.000]   But in the future, this isn't going to be about buying a car.
[00:32:22.000 --> 00:32:38.000]   This is going to be about a mode of transportation and that Tesla could end up restructuring the entire industry, end up being tens of millions of cars. You may only have two or three or four global players, which, you know, we have in a lot of other industries.
[00:32:38.000 --> 00:32:43.000]   And so the question I have for you is, do you, you know, what is your pattern recognition?
[00:32:43.000 --> 00:32:50.000]   Do you think that we're going to have all the same OEMs, they're all going to have some system, it's going to be basically commoditized?
[00:32:50.000 --> 00:32:55.000]   Or is this an extinction moment for some of these OEMs?
[00:32:55.000 --> 00:33:00.000]   Such a hard question. I mean, there's so many different ways this could play out.
[00:33:00.000 --> 00:33:06.000]   One of the things that I question in my mind is who's going to own these vehicles.
[00:33:06.000 --> 00:33:16.000]   And so, you know, on one hand, you have the Waymo situation where they own all the cars, but they also know what that means from a CapEx standpoint.
[00:33:16.000 --> 00:33:33.000]   And if you look at the subtleties of the kind of the statements they've made in conjunction with Uber, they're at least going to experiment with some models where maybe they don't have to pay for all the cars or licensing their technology to the other OEMs that we talked about.
[00:33:33.000 --> 00:33:47.000]   And people I've heard Dara talk about at Uber talk about, you know, maybe, you know, large, you know, debt companies get behind and own these things, like maybe the containers work in a rail yard, right?
[00:33:47.000 --> 00:33:53.000]   Like who owns these things? They move around, but someone owns them, right? People lease them.
[00:33:53.000 --> 00:34:03.000]   So that's a big question, you know, and the Tesla argument is that they'll just be owned by the people that own the Tesla, which is great.
[00:34:03.000 --> 00:34:08.000]   Tesla doesn't have the CapEx burden, everybody else has, because they've talked them into buying these cars.
[00:34:08.000 --> 00:34:12.000]   But what percentage of people are going to lease out their car every day?
[00:34:12.000 --> 00:34:16.000]   What would you guess? I have no idea what to put on that number.
[00:34:16.000 --> 00:34:23.000]   I mean, Airbnb does exist, so it's maybe not zero, but most of us still have our own house, right?
[00:34:23.000 --> 00:34:26.000]   So where does it fall?
[00:34:26.000 --> 00:34:35.000]   I think, you know, my own hunch on that is that it's going to be a higher percentage, particularly as the price points on the cars fall, right?
[00:34:35.000 --> 00:34:47.000]   Now they're leasing cars for, you know, 200 bucks a month, and as those subscription prices fall, I think you're going to have a whole category of people who don't mind at all, right, earning incremental revenues on their car.
[00:34:47.000 --> 00:35:00.000]   I imagine they've done internal survey work on this, and I think if you look at the places where, Bill, they're likely to launch the RoboTaxi, number one, it's going to be in marketplaces where they have sufficient safety, right?
[00:35:00.000 --> 00:35:06.000]   So where they have a density of data that tells them they can put it in RoboTaxi mode and they won't have any problem.
[00:35:06.000 --> 00:35:10.000]   I think that could be somewhere like maybe the Bay Area, somewhere like Austin.
[00:35:10.000 --> 00:35:15.000]   And then secondly, it's going to be a market where they have a lot of Teslas sitting on the sidelines.
[00:35:15.000 --> 00:35:21.000]   So they come seed it with 25 or 50 Teslas, but in both of these places, they happen to have Tesla factories.
[00:35:21.000 --> 00:35:25.000]   There are a lot of people with Teslas, and there are a lot of people who will drop them into the pool.
[00:35:25.000 --> 00:35:30.000]   So those will be good test markets, but if I think of a market, Bill, like New York City, right?
[00:35:30.000 --> 00:35:41.000]   New York City seems to me to be a problem because there's massive demand for cars in New York City, but there aren't a lot of Teslas sitting around latent, right, that can be tapped into for supply.
[00:35:41.000 --> 00:35:47.000]   So I don't think it will work equally well in all markets, but I think it is a massive advantage.
[00:35:47.000 --> 00:35:52.000]   Remember, out of there, I think they may have 7 million cars on the road today, something like that.
[00:35:52.000 --> 00:35:58.000]   About 2.5 million of those I think are hardware 4, and it's going to require hardware 4 in order to run all these cars.
[00:35:58.000 --> 00:36:02.000]   So, you know, we're really talking about a multi-year undertaking.
[00:36:02.000 --> 00:36:13.000]   But if you said to me over under on, you know, middle of next year where we have evidence of a single market or two where RoboTaxi is working, I think we will.
[00:36:13.000 --> 00:36:20.000]   I'm going to take the over on that, and that is a huge difference from where the world was 18 or 24 months ago.
[00:36:20.000 --> 00:36:26.000]   I just really don't know what percentage of people let their car go out.
[00:36:26.000 --> 00:36:31.000]   You know, Turo exists. It's tiny, like tiny compared to what we're talking about.
[00:36:31.000 --> 00:36:35.000]   It's a nice business, but that's where people let somebody else let their car go.
[00:36:35.000 --> 00:36:42.000]   Everybody who owns a Tesla has Tesla app on their phone, and all they have to do is toggle, says, yes, this time's the day.
[00:36:42.000 --> 00:36:45.000]   I just think if they make it easy enough, it could surprise us.
[00:36:45.000 --> 00:36:49.000]   It comes back, and it smells a little funny.
[00:36:49.000 --> 00:36:56.000]   You know, your wife says, honey, I really need you to go out to the drugstore right away.
[00:36:56.000 --> 00:37:01.000]   I've got this problem, and you're like, I let the car go out.
[00:37:01.000 --> 00:37:02.000]   She's like, what?
[00:37:02.000 --> 00:37:05.000]   Bill, you know exactly what's going to happen here.
[00:37:05.000 --> 00:37:09.000]   The arbitrage is the same arbitrage you and I saw in Uber, right?
[00:37:09.000 --> 00:37:12.000]   You're going to have enterprising young people.
[00:37:12.000 --> 00:37:15.000]   They're going to go lease four of these a month, 200 bucks.
[00:37:15.000 --> 00:37:16.000]   I could see that.
[00:37:16.000 --> 00:37:20.000]   200 bucks each, and they're going to be arbing this whole system.
[00:37:20.000 --> 00:37:22.000]   I just think the marketplace will fill up.
[00:37:22.000 --> 00:37:30.000]   That makes more sense to me than some majority of humans letting their car go out.
[00:37:30.000 --> 00:37:32.000]   I have a harder time with that.
[00:37:32.000 --> 00:37:40.000]   And then just to this industry structure question, because I think this is one that's going to play out over the course of the next three, five years.
[00:37:40.000 --> 00:37:43.000]   What are we going to be long, and what are we going to be short?
[00:37:43.000 --> 00:37:45.000]   What do we think is going to really happen?
[00:37:45.000 --> 00:37:47.000]   I'm skeptical.
[00:37:47.000 --> 00:37:48.000]   I'm skeptical.
[00:37:48.000 --> 00:37:51.000]   We've spent a lot of time looking at the Chinese manufacturers.
[00:37:51.000 --> 00:37:53.000]   Frida spent a lot of time in China.
[00:37:53.000 --> 00:37:55.000]   I'm skeptical on China.
[00:37:55.000 --> 00:37:57.000]   They don't have the sensors on the car.
[00:37:57.000 --> 00:37:59.000]   They don't have the data.
[00:37:59.000 --> 00:38:03.000]   We think models and access to the best chips are going to matter.
[00:38:03.000 --> 00:38:10.000]   Certainly, they have advantages over non-Chinese manufacturers, but I'm skeptical on the European manufacturers.
[00:38:10.000 --> 00:38:12.000]   I'm skeptical on the Japanese manufacturers.
[00:38:12.000 --> 00:38:15.000]   I could be wrong, and I'm mentally flexible.
[00:38:15.000 --> 00:38:30.000]   The industry may end up looking exactly like the industry does today, but I think there is a path where this is that event, that extinction event, tectonic event, that takes us from 20 OEMs down to three or four.
[00:38:30.000 --> 00:38:38.000]   And if we sat here in five or six or seven years, and we were on a path to three or four global automakers, it wouldn't surprise me.
[00:38:38.000 --> 00:38:41.000]   Maybe we can have an expert on the China thing.
[00:38:41.000 --> 00:38:49.000]   I've certainly seen videos online that suggest the Waymo equivalents are already working in China.
[00:38:49.000 --> 00:38:52.000]   So, that would be a data point.
[00:38:52.000 --> 00:38:54.000]   Well, Frida went and rode in every single one of them.
[00:38:54.000 --> 00:38:58.000]   I think there were 12, and she took videos in every car and sent it back to the team.
[00:38:58.000 --> 00:39:02.000]   And some of them were totally terrifying, obviously anecdotal.
[00:39:02.000 --> 00:39:10.000]   Yeah, everybody from Huawei to Baidu to Didi to BYD, et cetera, has their version of it.
[00:39:10.000 --> 00:39:18.000]   By the way, there's another question, I think, which is both Waymo and Tesla have entered that potentially licensing.
[00:39:18.000 --> 00:39:32.000]   And so, that would be an interesting maybe that happens down the road between the historical incumbents and these two new companies, and whether that type of model plays out.
[00:39:32.000 --> 00:39:36.000]   It seems quite likely to me that that's possible.
[00:39:36.000 --> 00:39:40.000]   Speaking of Elon.
[00:39:40.000 --> 00:39:45.000]   Real quick, one last thing that I forgot to mention on the national footprint.
[00:39:45.000 --> 00:39:49.000]   It may be important if they're going to put that together.
[00:39:49.000 --> 00:40:05.000]   Assuming that this is an issue that the country wants to support and wants to see leadership in, they may need to consider some type of insurance reform and litigation limitations built into the whole thing.
[00:40:05.000 --> 00:40:12.000]   Because I do think that we have this huge problem in America where litigation is so rampant.
[00:40:12.000 --> 00:40:24.000]   And I can just imagine some lawyer in front of a jury talking about how the computer killed somebody and trying to extract tens or hundreds of millions of dollars.
[00:40:24.000 --> 00:40:28.000]   And that could be a real problem for the industry.
[00:40:28.000 --> 00:40:41.000]   So, if they're going to consider all the other safety stuff, I think they should also think about liability limitations, especially on just arbitrary jury awards and things like that.
[00:40:41.000 --> 00:40:52.000]   Well, listen, I think that that's a perfect segue to talking about Doge and what's happening with this tectonic election we've just had.
[00:40:52.000 --> 00:41:08.000]   And I think those really big ambitious objectives, which frankly we haven't seen a lot of ambitious efforts to reform, I think, over the course of the last few administrations, maybe both Republican and Democrat.
[00:41:08.000 --> 00:41:16.000]   The one thing about the current administration, it strikes me, has a level of ambition that I don't know that I've seen in my lifetime, Bill.
[00:41:16.000 --> 00:41:19.000]   But one is the Department of Government Efficiency.
[00:41:19.000 --> 00:41:46.000]   So, this is the unofficial government department, which is run by Vivek and Elon and is overseeing trying to reform government spending, government regulations, frankly reducing the size of the federal government with the overall objective of getting control of our national deficit and our national debt, which we all agree is pretty egregious.
[00:41:46.000 --> 00:41:58.000]   And one of the videos I saw going around, I saw several people tweeting it, including Elon, was of somebody you and I, I think, have a lot of affection for, Milton Friedman.
[00:41:58.000 --> 00:42:11.000]   And this was Friedman opining in a very quick, rapid fire interview of the 14 government agencies that existed at that moment in time, which he would abolish and which he would keep.
[00:42:11.000 --> 00:42:16.000]   And, you know, we'll roll it and put a clip in here.
[00:42:16.000 --> 00:42:19.000]   Keep them or abolish them? Department of Agriculture.
[00:42:19.000 --> 00:42:20.000]   Abolish.
[00:42:20.000 --> 00:42:21.000]   Gone.
[00:42:21.000 --> 00:42:22.000]   Department of Commerce.
[00:42:22.000 --> 00:42:23.000]   Abolish.
[00:42:23.000 --> 00:42:24.000]   Gone.
[00:42:24.000 --> 00:42:25.000]   Department of Defense.
[00:42:25.000 --> 00:42:26.000]   Keep.
[00:42:26.000 --> 00:42:27.000]   Keep it.
[00:42:27.000 --> 00:42:28.000]   Abolish.
[00:42:28.000 --> 00:42:29.000]   Department of Education.
[00:42:29.000 --> 00:42:30.000]   Abolish.
[00:42:30.000 --> 00:42:31.000]   Gone.
[00:42:31.000 --> 00:42:32.000]   Energy.
[00:42:32.000 --> 00:42:33.000]   Abolish.
[00:42:33.000 --> 00:42:35.000]   Except that energy ties in with the military.
[00:42:35.000 --> 00:42:37.000]   Well, then we shove it under defense.
[00:42:37.000 --> 00:42:38.000]   The little bit that handles the nuclear.
[00:42:38.000 --> 00:42:39.000]   Right.
[00:42:39.000 --> 00:42:40.000]   That ought to go under defense.
[00:42:40.000 --> 00:42:41.000]   Plutonium and so forth goes under defense.
[00:42:41.000 --> 00:42:42.000]   But we abolish the rest of it.
[00:42:42.000 --> 00:42:45.000]   But, you know, they asked him, Department of Agriculture.
[00:42:45.000 --> 00:42:46.000]   Abolish.
[00:42:46.000 --> 00:42:47.000]   Commerce.
[00:42:47.000 --> 00:42:48.000]   Abolish.
[00:42:48.000 --> 00:42:49.000]   Education.
[00:42:49.000 --> 00:42:50.000]   Abolish, et cetera.
[00:42:50.000 --> 00:43:06.000]   And I think in tweeting that, it kind of sets some boundary conditions, right, of a level of ambition that I don't think many people -- they heard the words, but I don't think many people said, no, they're not really going to do that.
[00:43:06.000 --> 00:43:09.000]   They're not really serious about going after that.
[00:43:09.000 --> 00:43:13.000]   What do you expect out of the Department of Government Efficiency?
[00:43:13.000 --> 00:43:27.000]   And, you know, is the agenda going to be that ambitious or do you think it will hit the wall of some congressional reality and maybe they're just setting the boundary way out there that they can negotiate back from?
[00:43:27.000 --> 00:43:47.000]   Well, first of all, I mean, I think if you extrapolate the growth of government, you know, two or three decades into the future, you know, we would get to a point where such a large percentage of the population work for the government that it would self-implode.
[00:43:47.000 --> 00:44:03.000]   And many people, very smart people, have also highlighted that our debt is too large, the U.S. country debt is too large, and interest payments are now a huge part of the federal budget.
[00:44:03.000 --> 00:44:07.000]   And how can you possibly solve that if you don't shrink the size of government?
[00:44:07.000 --> 00:44:14.000]   Now, prior to the election, the papers said that both candidates were going to spend the same amount.
[00:44:14.000 --> 00:44:24.000]   So this kind of new department is, I think, you know, orthogonal to what most of the media thought Trump was going to do.
[00:44:24.000 --> 00:44:35.000]   I would add, I highly recommend everyone watch the two hour interview between Lex Friedman and Malay from Argentina.
[00:44:35.000 --> 00:44:38.000]   I think a lot of this is being provoked by him.
[00:44:38.000 --> 00:44:44.000]   He was recently in town and hung out with the Doge team.
[00:44:44.000 --> 00:44:54.000]   If you take him at his word for what has been accomplished in Argentina in such a short window of time, six to nine months, it's pretty spectacular.
[00:44:54.000 --> 00:44:57.000]   And maybe it's important for someone to fact check that.
[00:44:57.000 --> 00:45:04.000]   But really remarkable what can happen if you improve efficiency.
[00:45:04.000 --> 00:45:13.000]   And, you know, things like he got rid of a ton of regulation around rent control and housing improved for everyone.
[00:45:13.000 --> 00:45:18.000]   You know, and for me, like, that's like an of course, of course, that's what would happen.
[00:45:18.000 --> 00:45:21.000]   And so how do you how do you tear this apart?
[00:45:21.000 --> 00:45:36.000]   We we could simultaneously, if they're successful, see a reduction in government and an acceleration or proliferation of growth because you find out governments getting in the way.
[00:45:36.000 --> 00:45:47.000]   As I've said on the past two or three podcasts, you know, Governor Shapiro in Pennsylvania keeps getting accolades for moving regulation out of the way.
[00:45:47.000 --> 00:45:48.000]   It's so ridiculous.
[00:45:48.000 --> 00:45:55.000]   Like if that's if that's what you get celebrated for, we should just rewrite regulation and get it off the books.
[00:45:55.000 --> 00:46:07.000]   And we you know, we know there are states that are more productive and are growing faster right now and are and are actually increasing their population because people can build things.
[00:46:07.000 --> 00:46:13.000]   They can build factories, they can build, you know, distribution centers and everything can happen.
[00:46:13.000 --> 00:46:16.000]   They can build solar farms, you know, a lot faster.
[00:46:16.000 --> 00:46:19.000]   And so I do think it's super important.
[00:46:19.000 --> 00:46:22.000]   I really encourage people to watch the Malay thing.
[00:46:22.000 --> 00:46:30.000]   The part that's that's most interesting, because I think the easy pushback is, oh, they'll never be able to get it to happen.
[00:46:30.000 --> 00:46:37.000]   Washington's too, you know, and can't ensconced and you just can't move it or these people will protect themselves.
[00:46:37.000 --> 00:46:51.000]   The methodology that I'm going to assume was Elon's idea is basically to shine a flashlight on the idiocracy that exists.
[00:46:51.000 --> 00:46:55.000]   And, you know, transparency can be a hell of a disinfectant.
[00:46:55.000 --> 00:47:14.000]   And while you and I have started this, I took a flash look at Twitter and Vivek just announced that he's going to pause his own podcast to launch a new podcast with Elon called Dogecast, where they're going to give regular updates on the cost cutting and what they're going to do.
[00:47:14.000 --> 00:47:21.000]   And by the way, they've asked people to send in their favorite example of government waste.
[00:47:21.000 --> 00:47:24.000]   They're going to talk about it.
[00:47:24.000 --> 00:47:35.000]   And I think when you highlight to the American population really idiotic spend, they're going to get behind this thing.
[00:47:35.000 --> 00:47:42.000]   And that's a point here, you know, comes to this question of can they get it done, right?
[00:47:42.000 --> 00:47:43.000]   Can they get it done?
[00:47:43.000 --> 00:47:55.000]   And listen, there have been plenty of presidents, Clinton, Reagan, et cetera, who've talked about getting government control, spending under control, that talked about empowering the individual versus the state.
[00:47:55.000 --> 00:47:58.000]   And then they meet the Leviathan of Washington.
[00:47:58.000 --> 00:48:03.000]   Maybe just a little background here on the executive power as it relates to this.
[00:48:03.000 --> 00:48:08.000]   So there's this constitutional doctrine that we've been talking about in our thread, the doctrine of impoundment.
[00:48:08.000 --> 00:48:14.000]   And it's the ability of the executive branch to withhold or delay the spending of money appropriated by Congress, right?
[00:48:14.000 --> 00:48:33.000]   And this was a question as to whether or not the executive branch actually had this authority.
[00:48:33.000 --> 00:48:35.000]   Congress passed an act in 1974, the Impoundment Control Act, which basically said it restricted the president's ability to withhold funding, saying Congress is the one that allocates funding.
[00:48:35.000 --> 00:48:40.000]   If we allocate funding, then the executive branch has to implement the law, has to spend the money.
[00:48:40.000 --> 00:48:46.000]   Now, notwithstanding that fact, there was a big – and Trump won.
[00:48:46.000 --> 00:48:53.000]   The Trump administration withheld 391 million bucks in military aid to Ukraine that Congress had appropriated.
[00:48:53.000 --> 00:48:56.000]   And some argued that this violated the ICA.
[00:48:56.000 --> 00:48:58.000]   Now, why do I bring this up?
[00:48:58.000 --> 00:49:04.000]   I bring this up on the one hand because there is going to be a constitutional authority question here.
[00:49:04.000 --> 00:49:11.000]   But over the weekend, I talked to several members of Congress, both in the Senate and in the House, on the budget committee.
[00:49:11.000 --> 00:49:17.000]   And I will tell you there is strong agreement, and it wasn't just from Republicans, right?
[00:49:17.000 --> 00:49:25.000]   There were people who were trying to figure out how they can support and interact with the Department of Government Efficiency on both sides of the aisle, right?
[00:49:25.000 --> 00:49:32.000]   And so I think that notwithstanding the ICA, if they can get Congress to cooperate, then there is no constitutional issue.
[00:49:32.000 --> 00:49:49.000]   And then the final one, Bill, which is the one that I think is the most powerful here, at the end of the day, what we've seen time and time again, the power of Twitter, the power of X, going directly to the people with your message, saying here's what government is standing in the way of.
[00:49:49.000 --> 00:49:51.000]   Here's what we want to do.
[00:49:51.000 --> 00:50:01.000]   I think that's overwhelmingly powerful, shaming the government, right, really into making what I think a lot of people will agree are logical cuts.
[00:50:01.000 --> 00:50:03.000]   And so I'm bullish.
[00:50:03.000 --> 00:50:11.000]   And one of the things I keep hearing in the investment world is lots of people are trying to read between the lines, right?
[00:50:11.000 --> 00:50:16.000]   They're saying, well, what do you think the president really says or what do you think is really going to happen?
[00:50:16.000 --> 00:50:18.000]   And I said, stop trying to guess.
[00:50:18.000 --> 00:50:20.000]   Just listen to the words.
[00:50:20.000 --> 00:50:26.000]   After the election, Trump is literally recording on X, reiterating exactly what he's going to do.
[00:50:26.000 --> 00:50:30.000]   People are tweeting about what the Department of Government Efficiency is going to do.
[00:50:30.000 --> 00:50:32.000]   You're going to have these dogecasts.
[00:50:32.000 --> 00:50:35.000]   And I think that's exactly what they're going to attempt to do.
[00:50:35.000 --> 00:50:42.000]   And I think this is going to be a once in a generation reset in the balance of power between the individual and the state.
[00:50:42.000 --> 00:50:45.000]   And frankly, for one, I think it's well needed.
[00:50:45.000 --> 00:50:46.000]   I hope it's successful.
[00:50:46.000 --> 00:50:53.000]   Certainly, there'll be some things they get wrong, just like anybody who takes on an ambitious project will get some things wrong.
[00:50:53.000 --> 00:50:56.000]   But I think you could just undo those.
[00:50:56.000 --> 00:51:10.000]   By the way, one of the things that's already come up is I think two of the largest departments in the country have failed audits for like three to five of the past years.
[00:51:10.000 --> 00:51:11.000]   Yes.
[00:51:11.000 --> 00:51:20.000]   A single company on our public markets fails an audit or has to change auditors, as we recently saw with Supermicro.
[00:51:20.000 --> 00:51:23.000]   You get taken to the goddamn woodshed.
[00:51:23.000 --> 00:51:25.000]   How about that? Down 90 percent.
[00:51:25.000 --> 00:51:26.000]   Yes.
[00:51:26.000 --> 00:51:32.000]   It's considered a faux pas of the worst kind.
[00:51:32.000 --> 00:51:42.000]   And I've already seen interviews where someone's asking these department heads about these failed audits, and they're saying it's OK.
[00:51:42.000 --> 00:51:46.000]   And so that should not be OK.
[00:51:46.000 --> 00:51:49.000]   The dollars are massive.
[00:51:49.000 --> 00:51:53.000]   These people work for the citizens of the country.
[00:51:53.000 --> 00:51:56.000]   And if they have an audit requirement, they should be meeting it.
[00:51:56.000 --> 00:52:02.000]   And I'm shocked to find out that that's true.
[00:52:02.000 --> 00:52:09.000]   Well, one thing I wanted to do, I ran a quick exercise because I wanted to just see – I've heard a lot of people say, oh, it's unachievable.
[00:52:09.000 --> 00:52:14.000]   You can't get the budget to balance in four years, which I totally disagree with.
[00:52:14.000 --> 00:52:22.000]   So if you look at the federal revenues, they're about $5.2 trillion or just over $5 trillion expected this year.
[00:52:22.000 --> 00:52:24.000]   Government spending $7 trillion.
[00:52:24.000 --> 00:52:27.000]   So we basically have a $2 trillion deficit.
[00:52:27.000 --> 00:52:32.000]   That spending is up dramatically, by the way, from the $4.5 trillion that we spent in 2019.
[00:52:32.000 --> 00:52:35.000]   So in COVID, we just lost our mind and we've never regained our mind.
[00:52:35.000 --> 00:52:37.000]   So I just said what's possible here.
[00:52:37.000 --> 00:52:41.000]   Like, what would not even require draconian changes?
[00:52:41.000 --> 00:52:51.000]   If you just grow revenues from here at 3 to 4 percent over the course of the next four years, Bill, and you cut costs in the first year by 5 percent, second year by 5 percent, third year by 2 percent.
[00:52:51.000 --> 00:53:00.000]   So all that does is get you back to trendline from 2019 as though we increased spending 3 percent a year from 2019.
[00:53:00.000 --> 00:53:02.000]   The budget balances.
[00:53:02.000 --> 00:53:03.000]   It's tiny.
[00:53:03.000 --> 00:53:07.000]   These are tiny changes in costs.
[00:53:07.000 --> 00:53:16.000]   And I shared that with Vivek and Elon, and they both had separate and independent reactions, which is not nearly ambitious enough, right?
[00:53:16.000 --> 00:53:21.000]   And here, here, like, there's no reason in my mind that we can't do more.
[00:53:21.000 --> 00:53:31.000]   But what's encouraging to me and exciting to me, and I think will be well received by the bond market, is showing a four-year plan just to get us to balance, right?
[00:53:31.000 --> 00:53:32.000]   Just like a company does.
[00:53:32.000 --> 00:53:35.000]   Show me the long-term boundary conditions.
[00:53:35.000 --> 00:53:37.000]   Tell me where you're trying to go.
[00:53:37.000 --> 00:53:43.000]   And then let's use the power of the bully pulpit, the power of impoundment, to get people in line.
[00:53:43.000 --> 00:53:44.000]   I agree.
[00:53:44.000 --> 00:53:45.000]   It'll be interesting to watch.
[00:53:45.000 --> 00:53:48.000]   And once again, I would encourage people to watch.
[00:53:48.000 --> 00:53:53.000]   I'd say watch this because I think Malay is such a fascinating human.
[00:53:53.000 --> 00:54:00.000]   And I'd watch it on YouTube just so you can see his facial expressions, although it's dubbed because they actually did the interview in Spanish.
[00:54:00.000 --> 00:54:03.000]   But he's a very unique individual.
[00:54:03.000 --> 00:54:05.000]   Like, he was in a rock band.
[00:54:05.000 --> 00:54:10.000]   He obviously has studied the history of economics like no other.
[00:54:10.000 --> 00:54:12.000]   A disciple of Milton Friedman.
[00:54:12.000 --> 00:54:14.000]   Which comes out in this thing.
[00:54:14.000 --> 00:54:29.000]   Like I said, if what he said is true about what's been accomplished, it may be the fastest impact a president of a country has ever had on the trajectory of that country.
[00:54:29.000 --> 00:54:33.000]   And so, anyway, I'm going to agree with you.
[00:54:33.000 --> 00:54:35.000]   I'm in the optimistic camp.
[00:54:35.000 --> 00:54:37.000]   It's about time.
[00:54:37.000 --> 00:54:47.000]   There is zero doubt in my mind that making the government more efficient in our country would be very positive for everyone that lives here.
[00:54:47.000 --> 00:54:52.000]   So, maybe just a quick market wrap and then we'll bust out of here.
[00:54:52.000 --> 00:54:53.000]   I get a lot of questions.
[00:54:53.000 --> 00:54:55.000]   In fact, I'm doing CNBC tomorrow, Bill.
[00:54:55.000 --> 00:55:01.000]   And the question I get right now, like I always do in kind of November, is what about next year?
[00:55:01.000 --> 00:55:08.000]   And so, people are beginning to – everybody who's been in the market, we've had a really good year.
[00:55:08.000 --> 00:55:10.000]   Lots of our friends have had really good years.
[00:55:10.000 --> 00:55:19.000]   So, now we're starting to play a little bit more lockdown poker, tax loss harvest, all the things that one does as you head into December of this year.
[00:55:19.000 --> 00:55:26.000]   And people start thinking about what is the portfolio that I want to start 2025 with?
[00:55:26.000 --> 00:55:33.000]   And so, maybe just at a high level, what I would say is I always start with kind of just market backdrop.
[00:55:33.000 --> 00:55:36.000]   So, let me give you a sense.
[00:55:36.000 --> 00:55:40.000]   At the end of 2022, remember how bad '22 was.
[00:55:40.000 --> 00:55:44.000]   Mike Wilson was saying we're going to have a hard landing.
[00:55:44.000 --> 00:55:47.000]   Larry Summers was saying interest rates are going to 7% to 8%.
[00:55:47.000 --> 00:55:54.000]   We had seen a tremendous sell-off, almost 30% in the NASDAQ.
[00:55:54.000 --> 00:55:59.000]   So, it felt really terrible, but the setup for 2023 was actually pretty good.
[00:55:59.000 --> 00:56:02.000]   Just reverting to the mean would be a pretty good year.
[00:56:02.000 --> 00:56:09.000]   Now, we've had two pretty bullish years for technology, but we just had this historic election.
[00:56:09.000 --> 00:56:14.000]   So, when I look at '25, here are the things I put on the pro side of the ledger, right?
[00:56:14.000 --> 00:56:16.000]   We're going to get lower taxes, right?
[00:56:16.000 --> 00:56:25.000]   Not only the extension of the tax cuts that we had 10 years ago, but it looks like we're going to get some additional tax cuts on top of that.
[00:56:25.000 --> 00:56:29.000]   The corporate rate may go even lower from '21 to '15.
[00:56:29.000 --> 00:56:32.000]   You may get adjustments down in the capital gains rate.
[00:56:32.000 --> 00:56:34.000]   You may see the reintroduction of the SALT.
[00:56:34.000 --> 00:56:37.000]   So, a lot of tax stimulus.
[00:56:37.000 --> 00:56:41.000]   We're going to get less regulation like you and I just discussed.
[00:56:41.000 --> 00:56:43.000]   Rates are headed lower.
[00:56:43.000 --> 00:56:50.000]   They may be going lower, slower than we thought because the economy is doing well, but the Fed has tons of flexibility.
[00:56:50.000 --> 00:56:52.000]   Our growth is still high, right?
[00:56:52.000 --> 00:56:54.000]   GDP is still ripping.
[00:56:54.000 --> 00:56:56.000]   We see these corporate reports coming out.
[00:56:56.000 --> 00:56:57.000]   They're doing great.
[00:56:57.000 --> 00:56:59.000]   We have balanced employment, right?
[00:56:59.000 --> 00:57:05.000]   Employment is not so tight that you can't hire anybody, but our unemployment rate is still at historical lows.
[00:57:05.000 --> 00:57:12.000]   And then we're early in this tech and infrastructure super cycle that you and I started the pod talking about.
[00:57:12.000 --> 00:57:15.000]   What's on the negative side?
[00:57:15.000 --> 00:57:23.000]   Well, it looks like we may get a lot of tariffs as we've seen Howard Lutnick now propose for commerce.
[00:57:23.000 --> 00:57:25.000]   He's advocated, as has Trump.
[00:57:25.000 --> 00:57:29.000]   We need to level the playing field and then negotiate from there.
[00:57:29.000 --> 00:57:37.000]   So, leveling the playing field is putting tariffs on a lot of products in a lot of countries that would presumably raise prices.
[00:57:37.000 --> 00:57:40.000]   We also see we're going to get reduced government spending.
[00:57:40.000 --> 00:57:44.000]   And remember, government spending is a key driver of economic growth.
[00:57:44.000 --> 00:57:48.000]   So, you'll put stimulus in by way of tax cuts, but reduce spending.
[00:57:48.000 --> 00:57:55.000]   And then you got all that against the fact that market multiples are near all-time highs, right?
[00:57:55.000 --> 00:57:57.000]   So, the S&P is trading at 24 times.
[00:57:57.000 --> 00:58:00.000]   I think the NASDAQ is closer to 29 times.
[00:58:00.000 --> 00:58:06.000]   So, in my mind, it's not a recipe for an all-in moment, right?
[00:58:06.000 --> 00:58:08.000]   Because there's not blood in the streets, right?
[00:58:08.000 --> 00:58:12.000]   We're closer to trumpets in the air than blood in the streets, as Buffett would say.
[00:58:12.000 --> 00:58:15.000]   And he says sell when there are trumpets in the air.
[00:58:15.000 --> 00:58:19.000]   So, it all comes down to top-line growth and bottom-line growth for next year.
[00:58:19.000 --> 00:58:25.000]   And when I look at that, there were two reports today that kind of stick out to me, right?
[00:58:25.000 --> 00:58:30.000]   On the one hand, we were short a little bit of Target because we're worried about tariffs.
[00:58:30.000 --> 00:58:38.000]   And we're worried about the retail consumer holding up, especially around these highly levered consumers
[00:58:38.000 --> 00:58:42.000]   when it comes to housewares and things that Target has exposure to.
[00:58:42.000 --> 00:58:43.000]   They missed their numbers.
[00:58:43.000 --> 00:58:44.000]   They took down guidance.
[00:58:44.000 --> 00:58:48.000]   The stock was taken out to the woodshed, down 25%, right?
[00:58:48.000 --> 00:58:55.000]   And I saw some data that suggested auto delinquencies are on the rise.
[00:58:55.000 --> 00:58:58.000]   So, do you have any concern about the consumer?
[00:58:58.000 --> 00:58:59.000]   For sure.
[00:58:59.000 --> 00:59:04.000]   I think that, listen, we know that they've spent through all their stimulus money, right?
[00:59:04.000 --> 00:59:09.000]   That they're not getting the pay raises and all the overtime that they were a couple years ago.
[00:59:09.000 --> 00:59:14.000]   And we know that when you look at their credit, they're really spending a lot on credit now.
[00:59:14.000 --> 00:59:17.000]   And we see that same data on auto delinquencies.
[00:59:17.000 --> 00:59:21.000]   And so, if you just go through the things that Trump says he's going to do, right?
[00:59:21.000 --> 00:59:25.000]   Imposing tariffs is bad for retailers like Target.
[00:59:25.000 --> 00:59:27.000]   So, that's one side of the equation.
[00:59:27.000 --> 00:59:31.000]   Then on the other side, you have somebody like Snowflake or Palo Alto report tonight.
[00:59:31.000 --> 00:59:34.000]   And they beat earnings, but expectations are low.
[00:59:34.000 --> 00:59:36.000]   Snowflake's up 20% in the after hours.
[00:59:36.000 --> 00:59:38.000]   So, just think about that spread trade.
[00:59:38.000 --> 00:59:41.000]   Target's down 20%, Snowflake's up 20%.
[00:59:41.000 --> 00:59:44.000]   One has super high expectations, super high valuation.
[00:59:44.000 --> 00:59:46.000]   One has super low expectations.
[00:59:46.000 --> 00:59:51.000]   And in the case of a company like Snowflake, there's massive room for efficiency, right?
[00:59:51.000 --> 00:59:53.000]   They're getting the efficiencies from AI.
[00:59:53.000 --> 00:59:57.000]   They don't need to hire at the rate that they were hiring before.
[00:59:57.000 --> 01:00:00.000]   And they still just grew 29% in the quarter.
[01:00:00.000 --> 01:00:06.000]   So, that's far from the massive deceleration that many people were projecting.
[01:00:06.000 --> 01:00:15.000]   And so, I do think that the beneficiaries in technology, they're both beneficiaries of this top-line growth, but also bottom-line margin expansion.
[01:00:15.000 --> 01:00:20.000]   I was looking back in preparation for the pod with Satya on Monday.
[01:00:20.000 --> 01:00:28.000]   Since he became CEO of Microsoft, the net income margin has expanded by 1,000 basis points.
[01:00:28.000 --> 01:00:29.000]   Yeah.
[01:00:29.000 --> 01:00:30.000]   Right?
[01:00:30.000 --> 01:00:41.000]   If you look at the story of Microsoft over the last 10 years, revenue has doubled, but earnings have almost quadrupled because of the expansion of the earnings margin in that business.
[01:00:41.000 --> 01:00:44.000]   And that's what I think we're still going to see in technology.
[01:00:44.000 --> 01:00:47.000]   And it's going to be accelerated with AI.
[01:00:47.000 --> 01:00:53.000]   So, I head into next year not all in, but I'm definitely constructive on the backdrop.
[01:00:53.000 --> 01:00:56.000]   I just think it's going to be a stock picker's market.
[01:00:56.000 --> 01:00:58.000]   It's going to be a bunch of stuff that you want to avoid.
[01:00:58.000 --> 01:01:08.000]   But you can find your 5, 10, 20 tech companies that are enjoying the tailwinds of AI and also enjoying the possibility of this margin expansion.
[01:01:08.000 --> 01:01:10.000]   I think they can continue to do reasonably well.
[01:01:10.000 --> 01:01:12.000]   So, that's how we're positioned.
[01:01:12.000 --> 01:01:17.000]   What I'm hearing you say is you can kind of have your cake and eat it, too, if you pick the right courses here.
[01:01:17.000 --> 01:01:27.000]   Like, even if the consumer has some struggles, if you've got your portfolio aligned in the right way, you're going to be able to maneuver through that.
[01:01:27.000 --> 01:01:32.000]   Yeah, another way of saying it, I don't expect a 25% up year for a broad-based index.
[01:01:32.000 --> 01:01:33.000]   Yeah.
[01:01:33.000 --> 01:01:37.000]   And if you look at it, I don't know where it is as of today.
[01:01:37.000 --> 01:01:47.000]   But there is one point in the year, Bill, that if you took the top 10 tech companies out of the S&P, right, the S&P was not up 20, but it was actually down five.
[01:01:47.000 --> 01:01:48.000]   Yeah.
[01:01:48.000 --> 01:01:49.000]   Right?
[01:01:49.000 --> 01:02:02.000]   And I think that's the type of dispersion that I think we're going to see again next year, that there are going to be a lot of winners, either as the result of changing government regulations, changing government policies, lower taxes, or as the result of this tech super cycle.
[01:02:02.000 --> 01:02:04.000]   But there are going to be a whole lot of losers, right?
[01:02:04.000 --> 01:02:06.000]   We just talked about the ones in FSD.
[01:02:06.000 --> 01:02:12.000]   It may very well be that this is terrible for a lot of OEMs, but really great for Tesla.
[01:02:12.000 --> 01:02:14.000]   We have yet to be seen.
[01:02:14.000 --> 01:02:26.000]   I'll tell you, as an investor, and we could leave it on this, the rate of change has never been higher in technology, and the rate of change of this election is massive, right?
[01:02:26.000 --> 01:02:29.000]   The magnitude of the policy change is massive.
[01:02:29.000 --> 01:02:34.000]   You combine those two things, and let's just say we've been working overtime.
[01:02:34.000 --> 01:02:36.000]   Makes sense to me.
[01:02:36.000 --> 01:02:49.000]   One of the things we may want to talk about in the future is whether the Trump administration is going to lean into AI regulation as well, which could have implications for everyone around.
[01:02:49.000 --> 01:02:50.000]   Anyway, thanks so much, man.
[01:02:50.000 --> 01:02:51.000]   Great to see you.
[01:02:51.000 --> 01:02:52.000]   Good to catch up.
[01:02:52.000 --> 01:02:54.000]   Sorry this thing didn't happen, but we'll get it next time.
[01:02:54.000 --> 01:02:56.000]   We'll get her done on Monday.
[01:02:56.000 --> 01:02:57.000]   All right, man.
[01:02:57.000 --> 01:02:58.000]   PG2, out.
[01:02:58.000 --> 01:03:07.000]   [music]
[01:03:07.000 --> 01:03:11.000]   As a reminder to everybody, just our opinions, not investment advice.

